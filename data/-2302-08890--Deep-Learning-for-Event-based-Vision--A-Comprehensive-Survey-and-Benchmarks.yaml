- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:41:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:41:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.08890] Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.08890] 基于事件的视觉深度学习：综合调查与基准测试'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08890](https://ar5iv.labs.arxiv.org/html/2302.08890)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08890](https://ar5iv.labs.arxiv.org/html/2302.08890)
- en: 'Deep Learning for Event-based Vision:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于事件的视觉深度学习：
- en: A Comprehensive Survey and Benchmarks
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 综合调查与基准测试
- en: Xu Zheng^∗, Yexin Liu^∗, Yunfan Lu, Tongyan Hua, Tianbo Pan, Weiming Zhang,
    Dacheng Tao, Lin Wang^†
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 徐征^∗, 刘叶欣^∗, 吕云凡, 华同言, 潘天博, 张伟明, 陶大程, 王林^†
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Event cameras are bio-inspired sensors that capture the per-pixel intensity
    changes asynchronously and produce event streams encoding the time, pixel position,
    and polarity (sign) of the intensity changes. Event cameras possess a myriad of
    advantages over canonical frame-based cameras, such as high temporal resolution,
    high dynamic range, low latency, etc. Being capable of capturing information in
    challenging visual conditions, event cameras have the potential to overcome the
    limitations of frame-based cameras in the computer vision and robotics community.
    In very recent years, deep learning (DL) has been brought to this emerging field
    and inspired active research endeavors in mining its potential. However, there
    is still a lack of taxonomies in DL techniques for event-based vision. We first
    scrutinize the typical event representations with quality enhancement methods
    as they play a pivotal role as inputs to the DL models. We then provide a comprehensive
    taxonomy for existing DL-based methods by structurally grouping them into two
    major categories: 1) image reconstruction and restoration; 2) event-based scene
    understanding and 3D vision. Importantly, we conduct benchmark experiments for
    the existing methods in some representative research directions (e.g., object
    recognition) to identify some critical insights and problems. Finally, we make
    important discussions regarding the challenges and provide new perspectives for
    inspiring more research studies.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 事件摄像头是生物启发的传感器，它们异步捕捉每个像素的强度变化，并生成事件流，编码强度变化的时间、像素位置和极性（符号）。事件摄像头相较于传统的帧摄像头具有众多优势，如高时间分辨率、高动态范围、低延迟等。由于能够在复杂的视觉条件下捕捉信息，事件摄像头有可能克服计算机视觉和机器人学领域中帧摄像头的局限性。近年来，深度学习（DL）被引入到这一新兴领域，激发了对其潜力的积极研究。然而，针对基于事件的视觉的DL技术仍缺乏分类法。我们首先审视了典型事件表示及其质量增强方法，因为它们在DL模型中作为输入发挥了关键作用。接着，我们为现有的DL方法提供了一个全面的分类法，将其结构性地分为两个主要类别：1）图像重建与恢复；2）基于事件的场景理解和3D视觉。重要的是，我们对一些代表性研究方向（如物体识别）的现有方法进行了基准实验，以识别一些关键见解和问题。最后，我们对挑战进行了重要讨论，并提供了新的视角，以激发更多的研究工作。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Event Cameras, Deep Learning, Computer Vision and Robotics, Taxonomy, Survey.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 事件摄像头，深度学习，计算机视觉与机器人学，分类，调查。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The breakthrough in neuromorphic engineering has recently extended the realms
    of sensory perception and initiated a novel paradigm that mimics biological vision.
    The bio-inspired sensors are also called event cameras [[1](#bib.bib1)]. The difference
    is that each pixel in the camera operates independently, triggering a response
    (i.e., event) only when there’s a brightness change, probably caused by motion
    or other visual changes [[2](#bib.bib2)]. In practice, the event streams captured
    by an event camera are sparse and are more concentrated along object boundaries,
    but also present in areas of continuous texture gradients, enabling the camera
    with a merit of low latency. By contrast, canonical frame-based cameras record
    a complete image or video of the scene with a fixed frame rate. Event cameras
    offer some other benefits, such as high temporal resolution and high dynamic range [[3](#bib.bib3)].
    This means that an event camera can capture high-quality data either in extreme
    lighting or high-speed motion conditions [[4](#bib.bib4)]. Therefore, event cameras
    have the potential to overcome the limitations of frame-based cameras in the computer
    vision and robotic fields and have helped the community in solving various downstream
    tasks, e.g., corner tracking[[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)], Simultaneous Localization And Mapping (SLAM)[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)],
    image and video restoration[[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)], object detection and segmentation[[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [4](#bib.bib4)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态工程的突破最近拓展了感知领域，并引入了一种模仿生物视觉的新范式。生物启发的传感器也被称为事件摄像头[[1](#bib.bib1)]。不同之处在于，摄像头中的每个像素独立操作，仅在亮度发生变化（即事件）时触发响应，这种变化可能是由运动或其他视觉变化引起的[[2](#bib.bib2)]。实际上，事件摄像头捕捉到的事件流是稀疏的，主要集中在物体边界上，但也存在于连续纹理梯度区域，使得摄像头具有低延迟的优点。相比之下，经典的基于帧的摄像头以固定帧率记录完整的图像或视频。事件摄像头还具有其他一些优点，如高时间分辨率和高动态范围[[3](#bib.bib3)]。这意味着事件摄像头可以在极端光照或高速运动条件下捕捉高质量数据[[4](#bib.bib4)]。因此，事件摄像头有潜力克服计算机视觉和机器人领域中基于帧的摄像头的局限性，并帮助社区解决各种下游任务，例如角点跟踪[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]，同时定位与地图构建（SLAM）[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)]，图像和视频修复[[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]，以及物体检测和分割[[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [4](#bib.bib4)]。
- en: '![Refer to caption](img/a95743e77b5213160eaf7061d57d402e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a95743e77b5213160eaf7061d57d402e.png)'
- en: 'Figure 1: The structural and hierarchical taxonomy of event-based vision with
    deep learning.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于事件的视觉与深度学习的结构和层次分类。
- en: 'Motivation: The first application of an event camera is in a hardware-based
    Deep Neural Network (DNN) system called CAVIAR [[26](#bib.bib26)], predating the
    first software-based DNN system from the computer vision community by approximately
    a decade. This early development also includes significant contributions from
    solid-state circuit papers that detailed the design and initial applications of
    camera chips [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)]. Recently,
    deep learning (DL) has received great attention in this emerging area with amounts
    of techniques developed based on various purposes. For example, DL-based feature
    trackers [[30](#bib.bib30)] achieve better accuracy than the non-DL-based methods
    (See Tab. [IX](#S4.T9 "TABLE IX ‣ 4.1.2 Feature Tracking ‣ 4.1 Scene Understanding
    ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks")). We explore some fundamental questions
    that have been driving this research area when examining the representative methods.
    For instance, how to feed event data to DNNs as they are designed for image- or
    tensor-like inputs? What makes DL more advantageous than optimization-based methods
    for learning events? Do we really need very deep models to learn visual features
    from events? How can we balance the distinct property of event cameras, e.g.,
    low latency, when applying DL models? Do we really need convolution operations
    to filter events as done for image data?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '动机：事件相机的首个应用是在一个名为 CAVIAR 的硬件基础深度神经网络（DNN）系统中 [[26](#bib.bib26)]，这一系统的出现大约比计算机视觉社区的第一个软件基础
    DNN 系统早十年。这个早期发展还包括来自固态电路论文的重要贡献，这些论文详细描述了相机芯片的设计和初步应用 [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)]。最近，深度学习（DL）在这一新兴领域受到了广泛关注，各种基于不同目的开发的技术也随之出现。例如，基于 DL 的特征跟踪器
    [[30](#bib.bib30)] 相比于非 DL 方法具有更好的准确性（参见表 [IX](#S4.T9 "TABLE IX ‣ 4.1.2 Feature
    Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")）。我们在考察代表性方法时，探讨了一些驱动这一研究领域的基本问题。例如，如何将事件数据输入到
    DNN 中，因为它们是为图像或张量类输入设计的？为什么 DL 相比于基于优化的方法在学习事件时更具优势？我们是否真的需要非常深的模型来从事件中学习视觉特征？在应用
    DL 模型时，如何平衡事件相机的不同特性，例如低延迟？我们是否真的需要卷积操作来过滤事件，就像处理图像数据时那样？'
- en: With these questions being discussed, this paper provides an in-depth survey
    of current trends in DL for event-based vision. We focus on analyzing, categorizing,
    and benchmarking the DL-based methods with a highlight on recent progress. In
    particular, we review and scrutinize this promising area by systematically discussing
    the technical details, challenges, and potential directions. Previously, Gallego
    et al. [[1](#bib.bib1)] provided the first overview of the event-based vision
    with a particular focus on the principles and conventional algorithms. However,
    DL has invigorated almost every field of event-based vision recently, and remarkable
    advancements in methodologies and techniques have been achieved. Therefore, a
    more up-to-date yet insightful survey is urgently needed for capturing the research
    trends while clarifying the challenges and potential directions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些问题，本论文对基于事件的视觉领域的当前趋势进行了深入调查。我们专注于分析、分类和基准测试基于深度学习（DL）的方法，并重点关注最近的进展。特别地，我们通过系统地讨论技术细节、挑战和潜在方向来审视和深入剖析这一有前景的领域。之前，Gallego
    等人 [[1](#bib.bib1)] 提供了基于事件的视觉的首次概述，特别关注原理和传统算法。然而，近年来深度学习几乎重振了每个基于事件的视觉领域，并取得了显著的进展。因此，迫切需要一份更为最新且富有洞见的调查报告，以捕捉研究趋势，同时澄清挑战和潜在方向。
- en: 'We survey the DL-based methods by focusing on three important aspects: 1) How
    to learn events with DNNs—event representation and quality enhancement (Sec. [2](#S2
    "2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")); 2) Current research highlights by typically analyzing
    two hot fields, image restoration and enhancement (Sec.[3](#S3 "3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")), scene understanding and 3D vision (Sec. [4](#S4 "4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks")); 3) Potential future directions, such as event-based neural radiance
    for 3D reconstruction, cross-modal learning, and event-based model pretraining.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过关注三个重要方面来调查基于深度学习的方法：1）如何利用深度神经网络学习事件——事件表示和质量提升（Sec. [2](#S2 "2 Event
    Processing for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")）；2）当前研究亮点，通常分析两个热门领域，图像修复和增强（Sec.[3](#S3 "3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")），场景理解和3D视觉（Sec. [4](#S4 "4 Scene Understanding and 3D Vision
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")）；3）潜在的未来方向，例如用于3D重建的事件基础神经辐射、跨模态学习和基于事件的模型预训练。'
- en: 'Contributions: In summary, the main contributions of this paper are five-fold:
    (I) We provide a comprehensive overview of the existing event representations
    as well as the quality enhancement methods for events. (II) We provide a summary
    of how existing DL-based approaches for event-based vision address challenges
    and offer insights for the computer vision community, including image restoration
    and enhancement and high-level scene understanding tasks. (III) We discuss some
    open problems and challenges on DL with events and identify future research directions,
    providing guidance for future developments in this field. (IV) We ask and discuss
    some broadly focused questions as well as some potential problems to answer the
    concern and dive deeply into the event-based vision. Furthermore, we create an
    open-source repository that provides a taxonomy of all mentioned papers and code
    links. Our open-source repository will be updated regularly with the latest research
    progress, and we hope this work can bring sparks to the research of this field.
    The repository link is [https://github.com/vlislab2022/Event-Deep-Learning-Survey](https://github.com/vlislab2022/Event-Deep-Learning-Survey).
    Meanwhile, we benchmark and highlight some representative event-based and event-guided
    vision tasks, e.g., in Tab. [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video
    Deblurring ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks") and [X](#S4.T10 "TABLE X ‣ 4.1.3
    Object Detection and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks"), to identify the critical insights and problems for future studies.
    Due to the lack of space, some experimental results can be found in the supplementary
    material.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '贡献：总而言之，本文的主要贡献有五点：（I）我们提供了现有事件表示以及事件质量提升方法的全面概述。（II）我们总结了现有基于深度学习的事件视觉方法如何应对挑战，并为计算机视觉社区提供了见解，包括图像修复和增强以及高级场景理解任务。（III）我们讨论了一些基于事件的深度学习的开放问题和挑战，并确定了未来的研究方向，为该领域的未来发展提供指导。（IV）我们提出并讨论了一些广泛关注的问题以及一些潜在问题，以回应关切并深入探讨基于事件的视觉。此外，我们创建了一个开源仓库，提供了所有提及论文的分类和代码链接。我们的开源仓库将定期更新最新的研究进展，希望这项工作能为该领域的研究带来灵感。仓库链接是
    [https://github.com/vlislab2022/Event-Deep-Learning-Survey](https://github.com/vlislab2022/Event-Deep-Learning-Survey)。同时，我们对一些代表性的基于事件和事件引导的视觉任务进行了基准测试和突出显示，例如在表格
    [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video Deblurring ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks") 和 [X](#S4.T10 "TABLE X ‣ 4.1.3 Object Detection and Tracking
    ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks") 中，以识别未来研究的关键见解和问题。由于篇幅限制，一些实验结果可以在补充材料中找到。'
- en: 'Outlines: In the following sections, we discuss and analyze the recent advances
    in DL methods for event-based vision. The structural and hierarchical taxonomy
    of this paper is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"). In Sec. [2](#S2
    "2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we systematically summarize existing event representation
    methods for DL and compare the advantages and disadvantages of different representations
    on different tasks. In this section, the quality enhancement methods are also
    summarized. In Sec. [3](#S3 "3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks"), we review and
    analyze the image restoration and enhancement methods, including image and video
    reconstruction, super-resolution, video frame interpolation, image and video deblurring,
    and high dynamic range (HDR) image and video reconstruction. In Sec. [4](#S4 "4
    Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we summarise existing event-based DL approaches in scene
    understanding tasks, including object classification, object detection and tracking,
    semantic segmentation, feature tracking, optical flow, and depth estimation. In
    this section, we mainly discuss the deep learning pipelines for these computer
    vision tasks with event cameras. In Sec. [5.1](#S5.SS1 "5.1 Discussions ‣ 5 Research
    Trend and Discussions ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks"), we ask and discuss some widely focused questions as well
    as some potential problems to answer the existing concern and dive deeply into
    the event-based vision. And we discuss some open problems and new applications
    on DL with events and identify future research directions in Sec. [5.2](#S5.SS2
    "5.2 New Directions ‣ 5 Research Trend and Discussions ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), providing guidance for future
    developments in this field.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大纲：在接下来的章节中，我们讨论和分析了事件驱动视觉中深度学习（DL）方法的最新进展。本文的结构和层次分类如图[1](#S1.F1 "图 1 ‣ 1 引言
    ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")所示。在第[2](#S2 "2 DNNs 的事件处理 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中，我们系统地总结了现有的事件表示方法，并比较了不同任务上各种表示的优缺点。本节还总结了质量增强方法。在第[3](#S3
    "3 图像恢复与增强 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中，我们回顾并分析了图像恢复和增强方法，包括图像和视频重建、超分辨率、视频帧插值、图像和视频去模糊以及高动态范围（HDR）图像和视频重建。在第[4](#S4
    "4 场景理解与 3D 视觉 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中，我们总结了现有的基于事件的深度学习方法在场景理解任务中的应用，包括物体分类、物体检测与跟踪、语义分割、特征跟踪、光流估计和深度估计。本节主要讨论了使用事件相机进行这些计算机视觉任务的深度学习流程。在第[5.1](#S5.SS1
    "5.1 讨论 ‣ 5 研究趋势与讨论 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中，我们提出并讨论了一些广泛关注的问题以及一些潜在的问题，以回答现有的关注点，并深入探讨基于事件的视觉技术。我们还在第[5.2](#S5.SS2
    "5.2 新方向 ‣ 5 研究趋势与讨论 ‣ 基于事件的视觉中的深度学习：全面调查和基准测试")节中讨论了一些开放问题和新的应用方向，确定了未来的研究方向，为该领域的未来发展提供指导。
- en: 2 Event Processing for DNNs
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 DNNs 的事件处理
- en: 'Event cameras generate events when individual pixels detect the relative logarithm
    intensity change. Consequently, sparse and asynchronous event streams are generated.
    This inherent sparsity offers immediate advantages, such as low latency and low
    computational requirements for postprocessing systems [[31](#bib.bib31)]. Event
    cameras pose a distinctive shift in the imaging paradigm regarding how visual
    information is captured, making it impossible to directly apply the DNN models
    taking the image- or tensor-like inputs. Therefore, we first analyze the event
    representations, which are used as inputs to DNNs (Sec. [2.1](#S2.SS1 "2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks")). Moreover, as event data are often hampered
    by noises (especially in low-light conditions) and limited spatial resolution
    (in particular for DAVIS cameras), we analyze the DL methods that super-resolve
    and denoise the event streams for improving the learning performance (Sec. [2.2](#S2.SS2
    "2.2 Quality Enhancement for Events ‣ 2 Event Processing for DNNs ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '事件相机在单个像素检测到相对对数强度变化时生成事件。因此，会生成稀疏和异步的事件流。这种固有的稀疏性带来了即时的优势，例如低延迟和对后处理系统的低计算要求
    [[31](#bib.bib31)]。事件相机在成像范式中引入了显著的变化，即视觉信息的捕获方式，使得直接应用处理图像或张量输入的DNN模型变得不可能。因此，我们首先分析用于DNN输入的事件表示（第
    [2.1](#S2.SS1 "2.1 Event Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks") 节）。此外，由于事件数据通常受到噪声（特别是在低光照条件下）和有限空间分辨率（特别是对DAVIS相机）的影响，我们分析了可以超分辨率和去噪事件流的深度学习方法，以改善学习性能（第
    [2.2](#S2.SS2 "2.2 Quality Enhancement for Events ‣ 2 Event Processing for DNNs
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    节）。'
- en: 2.1 Event Representation
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 事件表示
- en: 'We first review how an event camera responds asynchronously to each independent
    pixel and generates a stream of events. An event is interpreted as a tuple $(\textbf{u},t,p)$,
    which is triggered whenever a change in the logarithmic intensity $L$ surpasses
    a constant value (threshold) $C$, formulated as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先回顾了事件相机如何异步响应每个独立像素并生成事件流。事件被解释为一个元组 $(\textbf{u},t,p)$，当对数强度 $L$ 的变化超过常数值（阈值）
    $C$ 时触发，如下所示：
- en: '|  | <math   alttext="p=\left\{\begin{aligned} +1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\geq C\\ -1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\leq-C\\'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="p=\left\{\begin{aligned} +1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\geq C\\ -1,&amp;L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\leq-C\\'
- en: 0,&amp;other\\
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 0,&amp;other\\
- en: \end{aligned}\right." display="block"><semantics ><mrow ><mi >p</mi><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mo >+</mo><mn >1</mn></mrow><mo >,</mo></mrow></mtd><mtd  columnalign="left"
    ><mrow ><mrow ><mrow ><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mi >t</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >L</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mrow ><mi
    >t</mi><mo >−</mo><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >≥</mo><mi >C</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="right" ><mrow ><mrow
    ><mo >−</mo><mn >1</mn></mrow><mo >,</mo></mrow></mtd><mtd columnalign="left"
    ><mrow ><mrow ><mrow ><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mi >t</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >L</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mrow ><mi
    >t</mi><mo >−</mo><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >≤</mo><mrow ><mo >−</mo><mi >C</mi></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"
    ><mrow ><mn >0</mn><mo >,</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >h</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >r</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >p=\left\{\begin{aligned} +1,&L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\geq C\\ -1,&L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\leq-C\\ 0,&other\\ \end{aligned}\right.</annotation></semantics></math>
    |  | (1) |
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \end{aligned}\right." display="block"><semantics ><mrow ><mi >p</mi><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mo >+</mo><mn >1</mn></mrow><mo >,</mo></mrow></mtd><mtd  columnalign="left"
    ><mrow ><mrow ><mrow ><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mi >t</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >L</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mrow ><mi
    >t</mi><mo >−</mo><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >≥</mo><mi >C</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="right" ><mrow ><mrow
    ><mo >−</mo><mn >1</mn></mrow><mo >,</mo></mrow></mtd><mtd columnalign="left"
    ><mrow ><mrow ><mrow ><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mi >t</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >L</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mtext >u</mtext><mo >,</mo><mrow ><mi
    >t</mi><mo >−</mo><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >≤</mo><mrow ><mo >−</mo><mi >C</mi></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"
    ><mrow ><mn >0</mn><mo >,</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >h</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >r</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >p=\left\{\begin{aligned} +1,&L(\textbf{u},t)-L(\textbf{u},t-\Delta
    t)\geq C\\ -1,&L(\textbf{u},t)-L(\textbf{u},t-\Delta t)\leq-C\\ 0,&other\\ \end{aligned}\right.</annotation></semantics></math>
    |  | (1) |
- en: 'where $\textbf{u}=(x,y)$ is the pixel location, $t$ is the timestamp and $p\in\{-1,1\}$
    is the polarity, indicating the sign of brightness changes (1 and -1 represent
    positive and negative events, respectively), and $p=0$ means that there are no
    events. $\Delta t$ is a time interval since the last event at pixel $\textbf{u}=(x,y)$.
    A number (or stream) of events are triggered which can be denoted as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{u}=(x,y)$ 是像素位置，$t$ 是时间戳，$p\in\{-1,1\}$ 是极性，表示亮度变化的符号（1 和 -1 分别代表正事件和负事件），而
    $p=0$ 表示没有事件。 $\Delta t$ 是自上一个事件以来在像素 $\textbf{u}=(x,y)$ 处的时间间隔。可以表示为一组（或一串）触发的事件：
- en: '|  | $\mathcal{E}=\{e_{i}\}_{i=1}^{N}=\{\textbf{u}_{i},t_{i},p_{i}\},i\in N,$
    |  | (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{E}=\{e_{i}\}_{i=1}^{N}=\{\textbf{u}_{i},t_{i},p_{i}\},i\in N,$
    |  | (2) |'
- en: 'For more details of the event generation model, we refer readers to [[1](#bib.bib1)].
    In a nutshell, this particular type of data makes it difficult to apply the DNN
    models predominantly designed for frame-based cameras. Therefore, it is pivotal
    to exploit the effective alternative representations of event data for mining
    their visual information and power [[32](#bib.bib32)]. In the following, we review
    the representative event representation methods, which can be divided into six
    categories: image-based, surface-based, learning-based, voxel-based, graph-based,
    and spike-based representations, as shown in Tab. [I](#S2.T1 "TABLE I ‣ 2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '关于事件生成模型的更多细节，我们请读者参考[[1](#bib.bib1)]。简而言之，这种特定类型的数据使得应用主要为帧基相机设计的深度神经网络模型变得困难。因此，利用有效的事件数据替代表示来挖掘其视觉信息和能力是至关重要的[[32](#bib.bib32)]。接下来，我们回顾了具有代表性的事件表示方法，这些方法可以分为六类：基于图像的、基于表面的、基于学习的、基于体素的、基于图的和基于脉冲的表示，如表[I](#S2.T1
    "TABLE I ‣ 2.1 Event Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")所示。'
- en: 'TABLE I: Representative event representations, served as inputs to event-based
    DNN models. (SP: Steering Prediction; OF: Optical Flow Estimation; Cls: Classification;
    CD: Corner Detection; GR: Gesture recognition; Recon: Reconstruction; DE: Depth
    Estimation; N/A: Not Available.)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '表I：代表性的事件表示，作为事件驱动深度神经网络模型的输入。（SP: 驾驶预测; OF: 光流估计; 分类: 分类; 角点检测: 角点检测; GR:
    手势识别; 重建: 重建; DE: 深度估计; 不适用: 不可用。）'
- en: '| Categories | Event Representation | Tasks | Dimensions | Description | Characteristics
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 事件表示 | 任务 | 尺寸 | 描述 | 特性 |'
- en: '| Image | Maqueda et al.[[33](#bib.bib33)] | SP | (2,H,W) | Channels for positive
    and negative events | Characteristics |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | Maqueda et al.[[33](#bib.bib33)] | 驾驶预测 | (2,H,W) | 正负事件的通道 | 特性 |'
- en: '| EV-FlowNet [[34](#bib.bib34)] | OF | (4,H,W) | Image of event counts | Discards
    temporal information |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| EV-FlowNet [[34](#bib.bib34)] | 光流 | (4,H,W) | 事件计数图像 | 丢弃时间信息 |'
- en: '| Event Image [[35](#bib.bib35)] | Cls | (4,H,W) | Image of event counts |
    Four channel event images |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 事件图像 [[35](#bib.bib35)] | 分类 | (4,H,W) | 事件计数图像 | 四通道事件图像 |'
- en: '| AMAE[[36](#bib.bib36)] | Cls | (2,H,W) | Two-channel image used timestamps
    | Indistinguishable semantic information |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| AMAE[[36](#bib.bib36)] | 分类 | (2,H,W) | 使用时间戳的双通道图像 | 语义信息不可区分 |'
- en: '| Bai et al.[[37](#bib.bib37)] | Cls | (3,H,W) | Three-channel event representation
    | Event count channel based on number |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Bai et al.[[37](#bib.bib37)] | 分类 | (3,H,W) | 三通道事件表示 | 基于数量的事件计数通道 |'
- en: '|  | MVF-Net [[38](#bib.bib38)] | Cls | N/A | Multi-view 2D maps | spatial-temporal
    complements |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | MVF-Net [[38](#bib.bib38)] | 分类 | 不适用 | 多视角2D地图 | 空间-时间补充 |'
- en: '| Learning | EST [[39](#bib.bib39)] | Cls&OF | (2,B,H,W) | 4 grid of convolutions
    | Temporally quantized information into B bins |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | EST [[39](#bib.bib39)] | 分类&光流 | (2,B,H,W) | 4网格卷积 | 将时间量化信息分成B个区间 |'
- en: '| Matrix-LSTM [[40](#bib.bib40)] | Cls&OF | (B,H,W) | End-to-end event surfaces
    | Temporally quantized information into B bins |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Matrix-LSTM [[40](#bib.bib40)] | 分类&光流 | (B,H,W) | 端到端事件表面 | 将时间量化信息分成B个区间
    |'
- en: '| Surface | Timestamps Image [[41](#bib.bib41)] | Cls | N/A | Spatial and temporal
    downsampling | N/A |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Surface | 时间戳图像 [[41](#bib.bib41)] | 分类 | 不适用 | 空间和时间下采样 | 不适用 |'
- en: '| SAE [[42](#bib.bib42)] | Cls | (2,H,W) | Image of most recent timestamps
    | Discards all prior timestamps |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SAE [[42](#bib.bib42)] | 分类 | (2,H,W) | 最新时间戳图像 | 丢弃所有先前时间戳 |'
- en: '| Time Surface [[43](#bib.bib43)] | Cls | (2,H,W) | Exponential of newest timestamps
    | Discards all prior timestamps |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 时间表面 [[43](#bib.bib43)] | 分类 | (2,H,W) | 最新时间戳的指数 | 丢弃所有先前时间戳 |'
- en: '| Sorted Time Surface [[7](#bib.bib7)] | Cls | (2,H,W) | Sorted newest timestamps
    | Retains temporal relationship |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Sorted Time Surface [[7](#bib.bib7)] | 分类 | (2,H,W) | 排序后的最新时间戳 | 保留时间关系
    |'
- en: '| Event Histogram [[33](#bib.bib33)] | Cls | (2,H,W) | Image of event counts
    | Discard temporal information |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 事件直方图 [[33](#bib.bib33)] | 分类 | (2,H,W) | 事件计数图像 | 丢弃时间信息 |'
- en: '| HATS [[44](#bib.bib44)] | Cls | (2,H,W) | Aggregated newest timestamps |
    Discards temporal information |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| HATS [[44](#bib.bib44)] | 分类 | (2,H,W) | 聚合最新时间戳 | 丢弃时间信息 |'
- en: '| IETS [[45](#bib.bib45)] | Cls | (3,H,W) | Image of filtered timestamps &
    event count | Discard temporal information |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| IETS [[45](#bib.bib45)] | 分类 | (3,H,W) | 过滤后的时间戳图像与事件计数 | 丢弃时间信息 |'
- en: '| SITS [[46](#bib.bib46)] | CD | (2,H,W) | Speed invariant time surface | Discards
    absolute timestamps |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| SITS [[46](#bib.bib46)] | 角点检测 | (2,H,W) | 速度不变时间表面 | 丢弃绝对时间戳 |'
- en: '| Chain SAE [[32](#bib.bib32)] | Cls | (2,H,W) | Chain Updating Strategy of
    time surface | Retains temporal relationship |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Chain SAE [[32](#bib.bib32)] | Cls | (2,H,W) | 时间表面链更新策略 | 保留时间关系 |'
- en: '| DS [[47](#bib.bib47)] | OP | (H,W) | Image of spatial distance to active
    pixel | Discards temporal/polarity information |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| DS [[47](#bib.bib47)] | OP | (H,W) | 到活动像素的空间距离图像 | 丢弃时间/极性信息 |'
- en: '| DiST [[48](#bib.bib48)] | Cls | (2,H,W) | Sorted discounted timestamps |
    Retains temporal relationship |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| DiST [[48](#bib.bib48)] | Cls | (2,H,W) | 排序的折扣时间戳 | 保留时间关系 |'
- en: '| TORE [[3](#bib.bib3)] | Cls | (2,K,H,W) | 4D grid of last K timestamps |
    Retains all information for last K events |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| TORE [[3](#bib.bib3)] | Cls | (2,K,H,W) | 最后K个时间戳的4D网格 | 保留最后K个事件的所有信息 |'
- en: '| Voxel | Zhu et al.[[49](#bib.bib49)] | OF | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Voxel | Zhu et al.[[49](#bib.bib49)] | 光流 | (B,H,W) | 通过离散化时间域生成 | 时间量化信息分为B个
    bin |'
- en: '| Rebecq et al.[[50](#bib.bib50)] | Recon | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Rebecq et al.[[50](#bib.bib50)] | 重建 | (B,H,W) | 通过离散化时间域生成 | 时间量化信息分为B个
    bin |'
- en: '| Ye et al.[[51](#bib.bib51)] | DE&OF | (B,H,W) | Generated by discretizing
    the time domain | Temporally quantized information into B bins |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Ye et al.[[51](#bib.bib51)] | DE&OF | (B,H,W) | 通过离散化时间域生成 | 时间量化信息分为B个 bin
    |'
- en: '| TORE [[3](#bib.bib3)] | Recon, etc | (2,K,H,W) | 4D grid of last K timestamps
    | Retains all information for last K events |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| TORE [[3](#bib.bib3)] | 重建等 | (2,K,H,W) | 最后K个时间戳的4D网格 | 保留最后K个事件的所有信息 |'
- en: '| Graph | RG-CNN [[52](#bib.bib52)] | Cls | N/A | End-to-end learning with
    graph convolution neural networks | N/A |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Graph | RG-CNN [[52](#bib.bib52)] | Cls | 不适用 | 使用图卷积神经网络的端到端学习 | 不适用 |'
- en: '| EV-VGCNN [[53](#bib.bib53)] | Cls | (H,W,A) | Use voxel-wise vertices rather
    than point-wise inputs | Normalize the time dimension with a compensation coefficient
    A |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| EV-VGCNN [[53](#bib.bib53)] | Cls | (H,W,A) | 使用体素级顶点而非点级输入 | 使用补偿系数A对时间维度进行归一化
    |'
- en: '| Spike | Lee et al.[[54](#bib.bib54)] | Cls | N/A | A novel supervised learning
    method for SNNs | N/A |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Spike | Lee et al.[[54](#bib.bib54)] | Cls | 不适用 | 一种针对SNN的新型监督学习方法 | 不适用
    |'
- en: '| Tactilesgnet [[50](#bib.bib50)] | Cls | N/A | Design a spiking graph convolutional
    network | N/A |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Tactilesgnet [[50](#bib.bib50)] | Cls | 不适用 | 设计脉冲图卷积网络 | 不适用 |'
- en: '| Botzheim et al.[[55](#bib.bib55)] | GR | N/A | Use spiking neural network
    and classification learning | N/A |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Botzheim et al.[[55](#bib.bib55)] | GR | 不适用 | 使用脉冲神经网络和分类学习 | 不适用 |'
- en: '| Amiret al.[[56](#bib.bib56)] | GR | N/A | Fully Event-Based Gesture Recognition
    System | N/A |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Amiret al.[[56](#bib.bib56)] | GR | 不适用 | 完全基于事件的手势识别系统 | 不适用 |'
- en: 2.1.1 Image-based Representation
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 基于图像的表示
- en: The straightforward solution to adopt events to the existing DL methods is to
    stack (or convert) events to synchronous 2D image representations (similar to
    frame-based cameras) as the inputs to DNNs. For example, Moeys et al. [[57](#bib.bib57)]
    proposed the first CNN driven by DVS frames to address the blurring issue in a
    predator-prey robot scenario. This study also marks the initial utilization of
    event count DVS images to guide a DNN using DVS data. The channels of image-based
    representation are often set to preserve polarities, timestamps, and event counts [[33](#bib.bib33),
    [34](#bib.bib34), [39](#bib.bib39), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [57](#bib.bib57)]. Based on how images are formed, we divide the prevailing methods
    into four types.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 采用事件到现有深度学习方法的直接解决方案是将事件堆叠（或转换）为同步的2D图像表示（类似于基于帧的相机），作为深度神经网络（DNN）的输入。例如，Moeys
    et al. [[57](#bib.bib57)] 提出了第一个由DVS帧驱动的CNN，用于解决捕食者-猎物机器人场景中的模糊问题。这项研究还标志着首次使用事件计数DVS图像来指导使用DVS数据的DNN。基于图像的表示的通道通常设置为保留极性、时间戳和事件计数
    [[33](#bib.bib33), [34](#bib.bib34), [39](#bib.bib39), [36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [57](#bib.bib57)]。根据图像的形成方式，我们将现有方法分为四种类型。
- en: 'Stack based on polarity: Maqueda et al.[[33](#bib.bib33)] set up two separate
    channels to evaluate the histograms for positive and negative events to obtain
    two-channel event images, which are finally merged together into synchronous event
    frames.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于极性的堆叠：Maqueda et al.[[33](#bib.bib33)] 建立了两个独立的通道来评估正负事件的直方图，从而获得两个通道的事件图像，最终将这些图像合并为同步事件帧。
- en: 'Stack based on timestamps: To consider the importance of event counts and timestamps
    for holistic information, [[36](#bib.bib36), [37](#bib.bib37), [17](#bib.bib17)]
    take the timestamps of events into consideration.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时间戳的堆叠：为了考虑事件计数和时间戳对整体信息的重要性，[[36](#bib.bib36), [37](#bib.bib37), [17](#bib.bib17)]
    考虑了事件的时间戳。
- en: 'Stack based on the number of events: Due to the uneven triggers of events within
    fixed time intervals, another stacking strategy is proposed to sample and stack
    events in a fixed constant number [[58](#bib.bib58), [59](#bib.bib59), [17](#bib.bib17)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于事件数量的堆叠：由于在固定时间间隔内事件触发的不均匀性，提出了一种新的堆叠策略来在固定常数数量的事件中进行采样和堆叠[[58](#bib.bib58),
    [59](#bib.bib59), [17](#bib.bib17)]。
- en: 'Stack based on timestamps and polarity: In Ev-gait  [[35](#bib.bib35)], event
    streams are converted to frame-like representation with four channels, containing
    the positive or negative polarities in two channels and the temporal characteristics
    in another two channels. Also, some research has focused on exposure time control
    in event-based systems. Liu et al. [[60](#bib.bib60), [61](#bib.bib61)] have explored
    dynamic control of exposure time and inter-slice time interval to optimize the
    quality of slice features. This adaptive control helps to ensure the robustness
    of the model in dynamic scenes with varying motion speeds and scene structures.
    Detailed mathematical formulations can be found in Sec.1.1 of the supplementary
    material.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时间戳和极性的堆叠：在 Ev-gait [[35](#bib.bib35)]中，事件流被转换为类似帧的表示，包含四个通道，其中两个通道包含正或负极性，另外两个通道包含时间特性。此外，一些研究集中于事件驱动系统中的曝光时间控制。刘等人[[60](#bib.bib60),
    [61](#bib.bib61)]探讨了曝光时间和切片间时间间隔的动态控制，以优化切片特征的质量。这种自适应控制有助于确保模型在动态场景中具有鲁棒性，适应不同的运动速度和场景结构。详细的数学公式可见于补充材料的第1.1节。
- en: 2.1.2 Surface-based Representation
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 基于表面的表示
- en: 'The first surface-based representation, i.e., Surface of Active Events (SAE) [[42](#bib.bib42)],
    it maps the event streams to a time-dependent surface and tracks the activity
    around the spatial location of the latest event $e_{i}$. Different from the basic
    image-based representation which utilizes intensity images to provide context
    content, the SAE achieves this through a totally different perspective, i.e.,
    the temporal-spatial perspective. Specifically, the time surface of the $i$ th
    event $e_{i}$ can be formulated as a spatial operator acting on the neighboring
    region of $e_{i}$:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个基于表面的表示，即主动事件表面（SAE）[[42](#bib.bib42)]，它将事件流映射到时间依赖的表面，并跟踪最新事件 $e_{i}$ 的空间位置周围的活动。与利用强度图像提供上下文内容的基本图像表示不同，SAE
    从完全不同的角度，即时间-空间角度来实现这一点。具体而言，第 $i$ 个事件 $e_{i}$ 的时间表面可以被表述为作用于 $e_{i}$ 邻域的空间算子：
- en: '|  | $\tau_{i}([x_{n},y_{n}]^{T},p)=\underset{j\leq i}{max}\{t_{j}&#124;[x_{i}+x_{n},y_{i}+y_{n}],p_{j}=p\}$
    |  | (3) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tau_{i}([x_{n},y_{n}]^{T},p)=\underset{j\leq i}{max}\{t_{j}&#124;[x_{i}+x_{n},y_{i}+y_{n}],p_{j}=p\}$
    |  | (3) |'
- en: 'where $x_{n}\in\{-r,r\}$ is the horizontal coordinate of $e_{i}$, $y_{n}\in\{-r,r\}$
    is the vertical coordinate of $e_{i}$, $p_{j}\in\{-1,1\}$ is the polarity of the
    $j$ th event $e_{j}$, $t_{j}$ is the timestamp of $e_{j}$ and $r$ is the radius
    of the neighboring region used to obtain the time surface. As shown in Eq. [3](#S2.E3
    "In 2.1.2 Surface-based Representation ‣ 2.1 Event Representation ‣ 2 Event Processing
    for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    the time surface $\tau_{i}([x_{n},y_{n}]^{T},p)$ encodes the time context in the
    $(2r+1)\times(2r+1)$ neighborhood region of $e_{i}$, hence maintaining both temporal
    and spatial information for downstream tasks.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{n}\in\{-r,r\}$ 是 $e_{i}$ 的水平坐标，$y_{n}\in\{-r,r\}$ 是 $e_{i}$ 的垂直坐标，$p_{j}\in\{-1,1\}$
    是第 $j$ 个事件 $e_{j}$ 的极性，$t_{j}$ 是 $e_{j}$ 的时间戳，$r$ 是用于获取时间表面的邻域半径。如公式 [3](#S2.E3
    "在 2.1.2 基于表面表示 ‣ 2.1 事件表示 ‣ 2 事件处理用于 DNNs ‣ 深度学习用于事件驱动视觉：全面调查与基准") 所示，时间表面 $\tau_{i}([x_{n},y_{n}]^{T},p)$
    编码了 $e_{i}$ 的 $(2r+1)\times(2r+1)$ 邻域区域中的时间上下文，从而保持了下游任务所需的时间和空间信息。
- en: However, the timestamps of events monotonically increase, which causes the temporal
    values in the surface from 0 to infinity [[32](#bib.bib32)]. Therefore, appropriate
    normalization approaches are required to preserve the temporal-invariant data
    representation from raw SAE by mapping the timestamps to $[0,1]$. Basic normalization
    methods are directly applied to time surfaces [[7](#bib.bib7), [43](#bib.bib43),
    [62](#bib.bib62), [63](#bib.bib63)], such as the min-max [[7](#bib.bib7)], time
    window [[63](#bib.bib63)], etc.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，事件的时间戳单调增加，这导致表面上的时间值从 0 到无穷大 [[32](#bib.bib32)]。因此，需要适当的归一化方法通过将时间戳映射到 $[0,1]$
    来保持原始 SAE 的时间不变数据表示。基本的归一化方法直接应用于时间表面 [[7](#bib.bib7), [43](#bib.bib43), [62](#bib.bib62),
    [63](#bib.bib63)]，例如 min-max [[7](#bib.bib7)]、时间窗口 [[63](#bib.bib63)] 等。
- en: All these above normalization methods rely on empirical parameter tuning, leading
    to additional computational costs. To avoid this problem, sort normalization is
    employed by Alzugaray et al. [[7](#bib.bib7)] to sort all the timestamps within
    an SAE at each pixel. However, though this method alleviates the dependence on
    parameter tuning, the by-product of time complexity impedes the whole procedure’s
    efficiency. To build an efficient SAE and achieve robust speed-invariant characteristics,
    Manderscheid et al. [[46](#bib.bib46)] introduced a normalization scheme to obtain
    the Speed Invariant Time Surface (SITS). The SITS updates the time surface of
    each incoming event according to its neighborhood with the radius $r$. Overall,
    when large $r$ is adopted, the SITS updates the time surface when a new event
    is triggered, thus leading to inefficiency in the on-demand tasks. Lin et al. [[32](#bib.bib32)]
    suggested solving and alleviating this imbalance between normalization and the
    number of events by using a chain update strategy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些归一化方法依赖于经验参数调整，从而导致额外的计算成本。为避免这个问题，Alzugaray 等人 [[7](#bib.bib7)] 采用了排序归一化方法，在每个像素处对
    SAE 中的所有时间戳进行排序。然而，尽管这种方法缓解了对参数调整的依赖，但时间复杂度的副作用妨碍了整个过程的效率。为了构建高效的 SAE 并实现稳健的速度不变特性，Manderscheid
    等人 [[46](#bib.bib46)] 引入了一种归一化方案，以获得速度不变时间表面 (SITS)。SITS 根据其半径 $r$ 的邻域更新每个到达事件的时间表面。总体而言，当采用较大的
    $r$ 时，SITS 在新事件触发时更新时间表面，从而导致按需任务的低效。Lin 等人 [[32](#bib.bib32)] 建议通过使用链式更新策略来解决和缓解归一化与事件数量之间的不平衡。
- en: 2.1.3 Voxel-based Representation
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 基于体素的表示
- en: The voxel-based representations map the raw events into the nearest temporal
    grid within temporal bins. The first spatial-temporal voxel grid is proposed in
     [[49](#bib.bib49)], inserting events into volumes using a linearly weighted accumulation
    to improve the resolution along the temporal domain. This spatial-temporal voxel
    grid is also used in some following works [[64](#bib.bib64), [51](#bib.bib51)].
    More recently, a time-ordered recent event (TORE) volume is proposed in  [[3](#bib.bib3)],
    aiming at compactly maintaining raw spike temporal information with minimal information
    loss.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基于体素的表示将原始事件映射到时间桶内的最近时间网格。第一个时空体素网格在 [[49](#bib.bib49)] 中提出，通过线性加权积累将事件插入体积，以改善时间域的分辨率。这个时空体素网格在一些后续工作中也被使用 [[64](#bib.bib64),
    [51](#bib.bib51)]。最近，[[3](#bib.bib3)] 中提出了一种时间排序最近事件 (TORE) 体积，旨在以最小的信息损失紧凑地保持原始尖峰时间信息。
- en: 2.1.4 Graph-based Representation
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 基于图的表示
- en: Aiming at preserving the sparsity of events, graph-based approaches transform
    the raw event streams within a time window into a set of connected nodes. Bi et
    al. first proposed a residual graph CNN architecture to obtain a compact graph
    representation for object classification [[65](#bib.bib65), [52](#bib.bib52)].
    The graph CNN preserves the spatial-temporal coherence of input events while avoiding
    large computational costs. More recently, Deng et al. proposed a voxel graph CNN
    which aims at exploiting the sparsity of event data [[53](#bib.bib53)]. The proposed
    EV-VGCNN [[53](#bib.bib53)] is a lightweight voxel graph CNN while achieving the
    SOTA classification accuracy with very low model complexity.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 针对保持事件稀疏性的目标，基于图的方法将时间窗口内的原始事件流转换为一组连接的节点。Bi 等人首次提出了一种残差图 CNN 架构，以获得用于对象分类的紧凑图表示 [[65](#bib.bib65),
    [52](#bib.bib52)]。图 CNN 保留了输入事件的时空一致性，同时避免了较大的计算成本。最近，Deng 等人提出了一种体素图 CNN，旨在利用事件数据的稀疏性 [[53](#bib.bib53)]。提出的
    EV-VGCNN [[53](#bib.bib53)] 是一种轻量级体素图 CNN，同时以非常低的模型复杂度实现了 SOTA 分类准确率。
- en: 2.1.5 Spike-based Representation
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5 基于尖峰的表示
- en: 'Due to the sparsity and asynchronous nature of event streams, most of the above
    representations consider the timestamps. Different from the standard DNN models,
    spiking neural networks (SNNs)[[66](#bib.bib66), [67](#bib.bib67), [56](#bib.bib56),
    [50](#bib.bib50), [54](#bib.bib54), [55](#bib.bib55)] are advantageous in that
    they incorporate the concept of time into operational models. Therefore, SNNs
    better fit the biological neuronal mechanism by using signals in the form of pulses
    (discontinuous values) to convey visual information. SNNs are applied to extracting
    features from events asynchronously to solve diverse tasks, such as object classification[[54](#bib.bib54),
    [50](#bib.bib50)] and gesture recognition[[56](#bib.bib56), [55](#bib.bib55)].
    However, due to the complex dynamics and non-differentiable nature of the spikes,
    two challenges exist: 1) Well-established back-propagation methods cannot be applied
    to the training process, leading to a long training time and high costs; 2) Specialized
    and effective hardware and algorithms are lacking. Consequently, their accuracy
    cannot exceed SOTA methods. Future research could explore more in this direction.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于事件流的稀疏性和异步性，上述大部分表示方法考虑了时间戳。不同于标准的 DNN 模型，尖峰神经网络（SNNs）[[66](#bib.bib66), [67](#bib.bib67),
    [56](#bib.bib56), [50](#bib.bib50), [54](#bib.bib54), [55](#bib.bib55)] 的优势在于它们将时间概念融入到操作模型中。因此，SNNs
    通过使用脉冲（不连续值）的形式传递视觉信息，更好地契合了生物神经机制。SNNs 被应用于异步提取事件特征，以解决多种任务，如对象分类[[54](#bib.bib54),
    [50](#bib.bib50)]和手势识别[[56](#bib.bib56), [55](#bib.bib55)]。然而，由于尖峰的复杂动态和不可微性质，存在两个挑战：1）现有的反向传播方法无法应用于训练过程，导致训练时间长和成本高；2）缺乏专门有效的硬件和算法。因此，其准确性无法超过
    SOTA 方法。未来的研究可以在这方面进一步探索。
- en: 2.1.6 Learning-based Representation
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.6 基于学习的表示
- en: The aforementioned event representations are mainly designed for a specific
    task and cannot be generally and flexibly applied to other tasks. To this end,
    Gehrig et al. [[39](#bib.bib39)] proposed the first learning-based approach to
    convert asynchronous raw events into tensor-like inputs, which can be flexibly
    applied to diverse downstream tasks. In particular, a multi-layer perceptron (MLP)
    is adopted to learn the coordinates and timestamps of events to obtain grid-like
    representations. Moreover, some methods [[68](#bib.bib68), [40](#bib.bib40)] extract
    features from events using Long Short-Term Memory (LSTM). A representative approach,
    Matrix-LSTM [[40](#bib.bib40)], utilizes a grid of LSTM cells to integrate information
    in the temporal axis. This approach follows a fully differentiable procedure that
    extracts the most relevant event representations for downstream tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上述事件表示方法主要针对特定任务设计，不能普遍和灵活地应用于其他任务。为此，Gehrig 等人[[39](#bib.bib39)] 提出了第一种基于学习的方法，将异步原始事件转换为类似张量的输入，这些输入可以灵活地应用于各种下游任务。特别是，采用多层感知器（MLP）来学习事件的坐标和时间戳，以获得网格状表示。此外，一些方法[[68](#bib.bib68),
    [40](#bib.bib40)] 使用长短期记忆（LSTM）从事件中提取特征。一种代表性的方法 Matrix-LSTM [[40](#bib.bib40)]，利用
    LSTM 单元的网格在时间轴上整合信息。这种方法遵循完全可微的过程，提取最相关的事件表示用于下游任务。
- en: 2.1.7 Remarks
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.7 备注
- en: 'Table [I](#S2.T1 "TABLE I ‣ 2.1 Event Representation ‣ 2 Event Processing for
    DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    outlines six types of mainstream representation methods for DNNs. Different event
    representations offer unique advantages and considerations when applied to various
    tasks. Image-based representation enables seamless integration with traditional
    deep learning algorithms, allowing for applications in object detection, segmentation,
    and feature extraction. Surface-based representation provides spatial-temporal
    context and preserves temporal information to a certain extent. Voxel-based representation
    enhances resolution and preserves raw spike temporal information by mapping raw
    events into temporal grids. Graph-based representation maintains sparsity and
    coherence while minimizing computational costs, achieving high classification
    accuracy. Spike-based representation, buttressed by SNNs, offers advantages in
    asynchronous processing, efficiency, noise robustness, and compatibility with
    neuromorphic hardware. Learning-based representation aims to discover optimal
    event representations, adapting to task-specific requirements. However, practical
    factors such as computational complexity and data availability should be considered.
    Overall, the selection of event representation should be considered based on the
    requirements of the task, and the trade-offs between complexity, computational
    efficiency, and interpretability. Further research and studies are urgently needed
    to explore more generic event representations for a wider range of tasks.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [I](#S2.T1 "TABLE I ‣ 2.1 Event Representation ‣ 2 Event Processing for
    DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    概述了 DNNs 的六种主流表示方法。不同的事件表示在应用于各种任务时提供了独特的优势和考虑因素。基于图像的表示方法能够与传统深度学习算法无缝集成，应用于目标检测、分割和特征提取。基于表面的表示方法提供了时空背景，并在一定程度上保留了时间信息。基于体素的表示方法通过将原始事件映射到时间网格中来增强分辨率并保留原始脉冲时间信息。基于图的表示方法在保持稀疏性和连贯性的同时，最小化计算成本，实现了高分类准确度。基于脉冲的表示方法在
    SNNs 的支持下，在异步处理、效率、噪声鲁棒性以及与神经形态硬件的兼容性方面具有优势。基于学习的表示方法旨在发现最佳事件表示，适应任务特定要求。然而，实践中的计算复杂性和数据可用性等因素应予以考虑。总体而言，事件表示的选择应根据任务需求进行考虑，并权衡复杂性、计算效率和可解释性之间的利弊。进一步的研究和探索亟需以发现适用于更广泛任务的通用事件表示方法。'
- en: 2.2 Quality Enhancement for Events
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 事件质量提升
- en: Event cameras, e.g., DAVIS346 [[69](#bib.bib69)], are with relatively low resolution—346$\times$240,
    while some event cameras, e.g., Prophesse ¹¹1https://www.prophesee.ai/, show higher
    spatial resolution up to, e.g., 640$\times$480\. These cameras often suffer from
    unexpected noise in the captured event data, especially in challenging visual
    conditions and event representation processes. Also, the spatial resolution is
    still lower than that of the frame-based cameras. These problems often hamper
    applying deep learning to event-based vision. Therefore, research has been recently
    conducted to improve the spatial resolution of events and denoise the events to
    achieve higher quality.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 事件相机，例如 DAVIS346 [[69](#bib.bib69)]，具有相对较低的分辨率—346$\times$240，而一些事件相机，例如 Prophesse
    ¹¹1https://www.prophesee.ai/，显示出更高的空间分辨率，例如 640$\times$480。这些相机通常在捕捉的事件数据中遭遇意外噪声，尤其在具有挑战性的视觉条件和事件表示过程中。此外，其空间分辨率仍低于基于帧的相机。这些问题通常妨碍了深度学习在基于事件的视觉中的应用。因此，近期研究已致力于提高事件的空间分辨率并对事件进行去噪，以实现更高质量的结果。
- en: 2.2.1 Event Super-Resolution
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 事件超分辨率
- en: Different from image super-solution (SR), event SR requires distribution estimation
    in both spatial and temporal dimensions. Li et al. [[70](#bib.bib70)] first proposed
    to solve the spatial-temporal SR problem of the LR event image. Later on, Wang
    et al. [[71](#bib.bib71)] proposed to bridge intensity images and events from
    the Dynamic Vision Sensor (DVS) via joint image filtering, so as to obtain motion-compensated
    event frames with high-resolution (HR) and less noise. The first DL-based approach
    is proposed by Duan et al. [[72](#bib.bib72)], which addresses the joint denoising
    and SR by using a multi-resolution event recording system and a 3D U-Net-based
    framework, called EventZoom. In particular, it incorporates event-to-image reconstruction
    to achieve resolution enhancement. Furthermore, Li et al.[[73](#bib.bib73)] proposed
    an SNN framework with a constraint learning mechanism to simultaneously learn
    the spatial and temporal distributions of event streams. Recently, Weng et al.[[74](#bib.bib74)]
    introduced a Recurrent Neural Network (RNN) that employs temporal propagation
    and spatial-temporal fusion net to ensure the restoration abilities of fine-grained
    event details without any auxiliary high-quality and HR frames.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像超分辨率（SR）不同，事件SR需要在空间和时间维度上进行分布估计。Li等人[[70](#bib.bib70)]首次提出解决LR事件图像的空间-时间SR问题。随后，Wang等人[[71](#bib.bib71)]提出通过联合图像滤波来桥接强度图像和来自动态视觉传感器（DVS）的事件，从而获得运动补偿的高分辨率（HR）事件帧且噪声较少。Duan等人[[72](#bib.bib72)]提出了首个基于DL的方法，通过使用多分辨率事件记录系统和基于3D
    U-Net的框架，称为EventZoom，来解决联合去噪和SR问题。特别地，它结合了事件到图像的重建以实现分辨率增强。此外，Li等人[[73](#bib.bib73)]提出了一种具有约束学习机制的SNN框架，以同时学习事件流的空间和时间分布。最近，Weng等人[[74](#bib.bib74)]引入了一种递归神经网络（RNN），利用时间传播和空间-时间融合网络来确保细粒度事件细节的恢复能力，而无需任何辅助的高质量和HR帧。
- en: 'Remarks: Though these methods achieve plausible SR results, the spatial-temporal
    distribution estimation leads to high latency for large factor SR, e.g., $\times
    16$. Future research could focus on reducing inference latency and lightweight
    network design.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：尽管这些方法取得了可信的SR结果，但空间-时间分布估计会导致大因素SR（例如，$\times 16$）的高延迟。未来的研究可以集中在减少推理延迟和轻量化网络设计上。
- en: 2.2.2 Event Denoising
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 事件去噪
- en: The presence of random noise, such as thermal noise and junction leakage currents,
    leads to background activity (BA) where an event is generated without any log-intensity
    change. To address this issue, various approaches have been developed, including
    filter-based methods, DL-based methods, and simulator-based methods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 随机噪声的存在，如热噪声和结漏电流，会导致背景活动（BA），在这种情况下，事件的生成没有任何对数强度变化。为解决这一问题，已经开发了各种方法，包括基于滤波的方法、基于DL的方法和基于模拟器的方法。
- en: Filter-based methods utilize various types of filters to eliminate background
    activity. These methods include bio-inspired filters[[75](#bib.bib75)], hardware-based
    filters[[76](#bib.bib76), [77](#bib.bib77)], spatial filters[[42](#bib.bib42),
    [78](#bib.bib78), [79](#bib.bib79)], and temporal filters[[45](#bib.bib45), [45](#bib.bib45),
    [71](#bib.bib71)]. However, in complex environments with multiple noise sources,
    these methods often fail to achieve satisfactory results. To mitigate these problems,
    Guo et al.[[80](#bib.bib80)] introduced a novel framework that quantifies denoising
    algorithms more effectively by measuring receiver operating characteristics using
    known mixtures of signal and noise DVS events. For experiment results of different
    filter-based methods, refer to Fig. 6 in the supplementary material.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于滤波的方法利用各种类型的滤波器来消除背景活动。这些方法包括生物启发式滤波器[[75](#bib.bib75)]、基于硬件的滤波器[[76](#bib.bib76),
    [77](#bib.bib77)]、空间滤波器[[42](#bib.bib42), [78](#bib.bib78), [79](#bib.bib79)]和时间滤波器[[45](#bib.bib45),
    [45](#bib.bib45), [71](#bib.bib71)]。然而，在具有多个噪声源的复杂环境中，这些方法往往无法取得令人满意的结果。为解决这些问题，Guo等人[[80](#bib.bib80)]引入了一种新颖的框架，通过使用已知的信号和噪声DVS事件的混合来量化去噪算法的效果。有关不同滤波器方法的实验结果，请参见补充材料中的图6。
- en: DL-based methods have also been introduced in  [[81](#bib.bib81), [82](#bib.bib82)].
    One representative framework is EDnCNN proposed by Baldwin et al.[[81](#bib.bib81)].
    It transforms neighboring events into voxels and differentiates noise using an
    Event Probability Mask (EPM). However, the artificial regularization operation,
    such as voxel transformation, can compromise the inherent properties of event
    data. To address this issue, AEDNet[[82](#bib.bib82)] decomposes DVS signals into
    temporal correlation and spatial affinity, leveraging the properties of temporal
    continuation and spatial discreteness. These signals are then separately processed
    using a unique feature extraction module. For detailed qualitative and quantitative
    experiment results, refer to Fig. 5 and Tab. 4 in the supplementary material.
    Another type of approach, a.w.a., simulator-based methods, focuses on incorporating
    noise effects into event simulators. The knowledge learned from simulated event
    data is then transferred to real data to reduce the influence of noise [[83](#bib.bib83),
    [84](#bib.bib84), [25](#bib.bib25), [85](#bib.bib85)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的方法也已在[[81](#bib.bib81), [82](#bib.bib82)]中介绍。其中一个代表性框架是由Baldwin等人提出的EDnCNN[[81](#bib.bib81)]。该框架将邻近的事件转换为体素，并使用事件概率掩码（EPM）来区分噪声。然而，人工正则化操作，例如体素转换，可能会损害事件数据的固有属性。为解决这个问题，AEDNet[[82](#bib.bib82)]将DVS信号分解为时间相关性和空间亲和性，利用时间延续性和空间离散性的属性。这些信号随后通过独特的特征提取模块分别处理。有关详细的定性和定量实验结果，请参见补充材料中的图5和表4。另一种方法，即基于模拟器的方法，专注于将噪声效应纳入事件模拟器。通过从模拟事件数据中学到的知识，转移到真实数据上，以减少噪声的影响[[83](#bib.bib83),
    [84](#bib.bib84), [25](#bib.bib25), [85](#bib.bib85)]。
- en: 'Remarks: Some event samples have abnormal pixel values in the event-count channels.
    Different abnormal pixels in different feature areas of the same object lead to
    a decline in feature learning performance, affecting subsequent tasks. Thus, denoising
    methods are one of the bases of event-based vision. In future research, a more
    general DL-based denoising pipeline, which can be applied to various event-based
    vision tasks, is worth exploring.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：一些事件样本在事件计数通道中具有异常的像素值。不同特征区域的异常像素会导致特征学习性能下降，影响后续任务。因此，去噪方法是基于事件视觉的基础之一。在未来的研究中，值得探索一种更通用的基于深度学习的去噪流程，可以应用于各种基于事件的视觉任务。
- en: 3 Image Restoration and Enhancement
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 图像恢复与增强
- en: Event cameras hold immense potential for leveraging event cameras in the reconstruction
    and restoration of HDR images and high frame-rate videos. However, their unique
    imaging paradigm presents a challenge when applying vision algorithms designed
    for frame-based cameras. To address this challenge and bridge the gap between
    event-based and standard computer vision, many methods have been proposed to reconstruct
    intensity video frames or images from events.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 事件相机在HDR图像和高帧率视频的重建与恢复中具有巨大的潜力。然而，它们独特的成像模式在应用为帧基础相机设计的视觉算法时带来了挑战。为了解决这个挑战并弥合基于事件的视觉与标准计算机视觉之间的差距，已经提出了许多方法来从事件中重建强度视频帧或图像。
- en: 'In this paper, we group the prevailing methods into two major types: event-based
    image (or video) reconstruction (with only events as inputs) and event-guided
    image restoration (hybrid inputs of events and frames). For the former, the main
    problem is how to fully explore the visual information, e.g., edge, from events
    with DNNs to reconstruct high-quality intensity images or video frames; while
    the latter explores how to fuse frames and events while leveraging the advantages
    of events, e.g., HDR, to benefit the image restoration process. We now review
    the state-of-the-art (SOTA) techniques in the following sections.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将现有方法分为两大类：基于事件的图像（或视频）重建（仅以事件为输入）和事件引导的图像恢复（事件和帧的混合输入）。对于前者，主要问题是如何利用深度神经网络（DNNs）从事件中充分挖掘视觉信息，例如边缘，以重建高质量的强度图像或视频帧；而后者则探讨如何在利用事件优势（例如HDR）的同时融合帧和事件，从而有利于图像恢复过程。我们将在接下来的章节中回顾最新的技术（SOTA）。
- en: 3.1 Event-based Image/Video Reconstruction
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于事件的图像/视频重建
- en: 'Insight: This task learns a mapping from a stream of events to a single intensity
    image or sequence of images (i.e., video). The mapped results allows for applying
    the off-the-shelf DL algorithms—developed for frame-based cameras—to learning
    downstream tasks. From our review, intensive research has been devoted to achieving
    this task, as summarized in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1 Event-based Image/Video
    Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), Fig. [2](#S3.F2 "Figure 2 ‣ 3.1
    Event-based Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣
    Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    and Tab. [II](#S3.T2 "TABLE II ‣ 3.1 Event-based Image/Video Reconstruction ‣
    3 Image Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A
    Comprehensive Survey and Benchmarks").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 洞察：该任务学习从事件流到单个强度图像或图像序列（即视频）的映射。映射结果允许应用针对基于帧的相机开发的DL算法到学习下游任务。根据我们的评论，已经有大量的研究致力于实现这一任务，如图 [3](#S3.F3
    "图 3 ‣ 3.1 事件驱动图像/视频重建 ‣ 3 图像恢复和增强 ‣ 基于事件的视觉的深度学习：综述和基准"), 图 [2](#S3.F2 "图 2 ‣
    3.1 事件驱动图像/视频重建 ‣ 3 图像恢复和增强 ‣ 基于事件的视觉的深度学习：综述和基准"), 和表 [II](#S3.T2 "表 II ‣ 3.1
    事件驱动图像/视频重建 ‣ 3 图像恢复和增强 ‣ 基于事件的视觉的深度学习：综述和基准") 中总结的内容。
- en: 'Early methods rely on the assumption about the scene structure (or motion dynamics) [[86](#bib.bib86),
    [87](#bib.bib87)] or event integration with regularization terms [[88](#bib.bib88)]
    to reconstruct intensity images. However, these methods suffer from artifacts
    due to the direct event integration, and the reconstructed intensity images are
    not photo-realistic enough. DL-based methods, by contrast, bring significant accuracy
    gains. In this paper, we analyze the SOTA deep learning methods based on the challenges:
    1) A lack of large-scale datasets for training deep networks; 2) High computational
    complexity and low latency; 3) The low-quality of reconstructed images or videos,
    e.g., relatively low resolution and blurred images.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的方法依赖于对场景结构（或运动动态）的假设[[86](#bib.bib86), [87](#bib.bib87)]或带有正则项的事件集成[[88](#bib.bib88)]来重建强度图像。然而，这些方法由于直接事件集成而遭受到了伪影，重建的强度图像不够逼真。相比之下，基于DL的方法带来了显著的精度提升。在本文中，我们分析了基于挑战的SOTA深度学习方法：1）缺乏大规模训练深度网络的数据集；2）高计算复杂度和低延迟；3）重建图像或视频的质量较低，例如相对较低的分辨率和模糊图像。
- en: 'TABLE II: Qualitative comparison results of some image reconstruction methods [[89](#bib.bib89)]
    on event dataset [[12](#bib.bib12)].'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：一些图像重建方法[[89](#bib.bib89)]在事件数据集[[12](#bib.bib12)]上的定性比较结果。
- en: '| Method | Type | MSE $\downarrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ |
    Time |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类型 | MSE $\downarrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ | 时间 |'
- en: '| E2VID  [[90](#bib.bib90)] | DL-based | 0.069 | 0.395 | 0.438 | 0.2448 s |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| E2VID  [[90](#bib.bib90)] | 基于DL | 0.069 | 0.395 | 0.438 | 0.2448 秒 |'
- en: '| ECNN [[91](#bib.bib91)] | D-based | 0.056 | 0.416 | 0.442 | 0.2839 s |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ECNN [[91](#bib.bib91)] | 基于D | 0.056 | 0.416 | 0.442 | 0.2839 秒 |'
- en: '| BTEB [[92](#bib.bib92)] | DL-based | 0.090 | 0.357 | 0.520 | 0.4059 s |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| BTEB [[92](#bib.bib92)] | 基于DL | 0.090 | 0.357 | 0.520 | 0.4059 秒 |'
- en: '| Tikhonov  [[89](#bib.bib89)] | Model-based | 0.121 | 0.356 | 0.485 | 0.4401
    s |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Tikhonov  [[89](#bib.bib89)] | 基于模型 | 0.121 | 0.356 | 0.485 | 0.4401 秒 |'
- en: '| TV [[89](#bib.bib89)] | Mode-based | 0.113 | 0.386 | 0.502 | 4.0443 s |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| TV [[89](#bib.bib89)] | 基于模型 | 0.113 | 0.386 | 0.502 | 4.0443 秒 |'
- en: '| CNN [[89](#bib.bib89)] | DL-based | 0.080 | 0.437 | 0.485 | 28.3904 s | ![Refer
    to caption](img/7de71ad3bf8a765ce2bdf974e5df4ebd.png)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '| CNN [[89](#bib.bib89)] | 基于DL | 0.080 | 0.437 | 0.485 | 28.3904 秒 | ![参考说明](img/7de71ad3bf8a765ce2bdf974e5df4ebd.png)'
- en: 'Figure 2: Methods for event-based image/video reconstruction.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：事件驱动图像/视频重建方法。
- en: 'TABLE III: Comparison of the representative event-guided video frame interpolation
    (VFI) methods.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：代表性事件引导的视频帧插值（VFI）方法的比较。
- en: '| Publication | Methods | Highlight | Event Representation | Optical Flow |
    Deblurring | Supervised | Backbone | Dataset |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 出版物 | 方法 | 重点 | 事件表示 | 光流 | 去模糊 | 监督 | 主干 | 数据集 |'
- en: '| CVPR 2021 | TimeLens[[93](#bib.bib93)] | synthesis-based and ﬂow-based |
    voxel grid[[49](#bib.bib49)] | ✓ | ✗ | ✓ | CNN | HQF,Vimeo90k,GoPro,Middlebury,HS-ERGB
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2021 | TimeLens[[93](#bib.bib93)] | 基于合成和基于流 | 体素网格[[49](#bib.bib49)]
    | ✓ | ✗ | ✓ | CNN | HQF,Vimeo90k,GoPro,Middlebury,HS-ERGB |'
- en: '| CVPR 2021 | EFI-Net[[94](#bib.bib94)] | different spatial resolutions | voxel
    grids[[49](#bib.bib49)] | ✗ | ✗ | ✓ | CNN | Samsung GE3 DVS |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2021 | EFI-Net[[94](#bib.bib94)] | 不同的空间分辨率 | 体素网格[[49](#bib.bib49)]
    | ✗ | ✗ | ✓ | CNN | 三星GE3 DVS |'
- en: '| ICCV 2021 | Yu et al.[[95](#bib.bib95)] | weakly supervised | Image-based
    | ✓ | ✗ | ✗ | ViT+CNN | GoPro, SloMo-DVS |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2021 | Yu 等人[[95](#bib.bib95)] | 弱监督 | 基于图像 | ✓ | ✗ | ✗ | ViT+CNN |
    GoPro, SloMo-DVS |'
- en: '| CVPR 2022 | Time Replayer[[96](#bib.bib96)] | unsupervised cycle-consistent
    style | 4-channel frames[[41](#bib.bib41)] | ✓ | ✗ | ✗ | CNN | GoPro, Adobe240,
    Vimeo90k |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2022 | Time Replayer[[96](#bib.bib96)] | 无监督循环一致性风格 | 4通道帧[[41](#bib.bib41)]
    | ✓ | ✗ | ✗ | CNN | GoPro, Adobe240, Vimeo90k |'
- en: '| CVPR 2022 | TimeLens++[[97](#bib.bib97)] | multi-scale feature-level fusion
    | voxel grid[[49](#bib.bib49)] | ✓ | ✗ | ✓ | CNN | BS-ERGB, HS-ERGB |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2022 | TimeLens++[[97](#bib.bib97)] | 多尺度特征级融合 | 体素网格[[49](#bib.bib49)]
    | ✓ | ✗ | ✓ | CNN | BS-ERGB, HS-ERGB |'
- en: '| ECCV 2022 | $A^{2}OF$[[98](#bib.bib98)] | optical flows adjustment | four-channel
    frame[[41](#bib.bib41)] | ✓ | ✗ | ✓ | CNN | Adobe240, GoPro, Middlebury, HS-ERGB,
    HQF |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2022 | $A^{2}OF$[[98](#bib.bib98)] | 光流调整 | 四通道帧[[41](#bib.bib41)] |
    ✓ | ✗ | ✓ | CNN | Adobe240, GoPro, Middlebury, HS-ERGB, HQF |'
- en: '| ECCV2020 | Lin et al.[[99](#bib.bib99)] | physical model inspired | stream
    and frame-based | ✗ | ✓ | ✓ | CNN | GoPro,Blur-DVS |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ECCV2020 | Lin 等人[[99](#bib.bib99)] | 物理模型启发 | 流式和基于帧 | ✗ | ✓ | ✓ | CNN |
    GoPro, Blur-DVS |'
- en: '| CVPR 2022 | E-CIR[[100](#bib.bib100)] | parametric intensity function | polynomial[[100](#bib.bib100)]
    | ✗ | ✓ | ✓ | CNN | REDS |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2022 | E-CIR[[100](#bib.bib100)] | 参数化强度函数 | 多项式[[100](#bib.bib100)]
    | ✗ | ✓ | ✓ | CNN | REDS |'
- en: '| CVPR 2022 | Zhang et al.[[101](#bib.bib101)] | deblurring and frame interpolation
    | event streams | ✗ | ✓ | ✗ | CNN | GoPro, HQF, RBE | ![Refer to caption](img/d7d58c05f9f2268e94f2df983c90f6b9.png)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '| CVPR 2022 | Zhang 等人[[101](#bib.bib101)] | 去模糊和帧插值 | 事件流 | ✗ | ✓ | ✗ | CNN
    | GoPro, HQF, RBE | ![参见说明](img/d7d58c05f9f2268e94f2df983c90f6b9.png)'
- en: 'Figure 3: Visual examples of some SOTA methods for video reconstruction (E2VID
     [[90](#bib.bib90)], EF [[91](#bib.bib91)], RCNN [[102](#bib.bib102)]).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：一些视频重建SOTA方法的视觉示例（E2VID [[90](#bib.bib90)]，EF [[91](#bib.bib91)]，RCNN [[102](#bib.bib102)]）。
- en: '![Refer to caption](img/a7aacc204721616ed7e24c81a0b7b2df.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a7aacc204721616ed7e24c81a0b7b2df.png)'
- en: 'Figure 4: Representative VFI methods, including, e.g., (a) TimeLens [[93](#bib.bib93)],
    the first event-guided VFI method (b) TimeLens++[[97](#bib.bib97)], the SOTA event-based
    VFI method (c) TimeReplayer[[96](#bib.bib96)], the first unsupervised event-guided
    VFI method.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：代表性的VFI方法，包括例如，(a) TimeLens [[93](#bib.bib93)]，第一个事件引导的VFI方法；(b) TimeLens++[[97](#bib.bib97)]，SOTA事件基础VFI方法；(c)
    TimeReplayer[[96](#bib.bib96)]，第一个无监督事件引导的VFI方法。
- en: For the first challenge,  [[103](#bib.bib103)] and  [[104](#bib.bib104)] are
    representative works leveraging generative adversarial networks (GANs) to bridge
    knowledge transfer between events and RGB images to alleviate the scarce labeled
    data problem. wMoreover, Stoffregen et al. [[91](#bib.bib91)] found that the contrast
    threshold is a key factor in synthesizing data to match the real event data well.
    Further, Vallés et al. [[92](#bib.bib92)] explored the theoretical basis of event
    cameras and proposes self-supervised learning to reduce the dependence on the
    ground truth video (including synthetic data) based on the photometric constancy
    of events.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个挑战，[[103](#bib.bib103)] 和 [[104](#bib.bib104)] 是利用生成对抗网络（GANs）弥合事件与RGB图像之间的知识转移的代表性工作，以缓解标注数据稀缺的问题。此外，Stoffregen
    等人[[91](#bib.bib91)] 发现对比度阈值是合成数据与真实事件数据匹配的关键因素。进一步地，Vallés 等人[[92](#bib.bib92)]
    探索了事件相机的理论基础，并提出了自监督学习，以减少对地面真实视频（包括合成数据）的依赖，这基于事件的光度一致性。
- en: For addressing the second challenge, Scheerlinck et al. [[19](#bib.bib19)] employed
    recurrent connections to build a state over time, allowing a much smaller recurrent
    neural network that reuses previously reconstructed results. Interestingly, Duwek
    et al. [[105](#bib.bib105)] combined CNNs with SNNs based on Laplacian prediction
    and Poisson integration to achieve video reconstruction with fewer parameters.
    To solve the third challenge, GANs, the double integral model [[106](#bib.bib106)],
    and RNNs are applied to avoid generating blurred results and obtain high-speed
    and HDR videos from events in [[17](#bib.bib17), [102](#bib.bib102), [90](#bib.bib90),
    [89](#bib.bib89)]. As for generating super-resolution (SR) images/videos from
    events, we divide the prevailing works into three categories, including optimization-based[[70](#bib.bib70)],
    supervised [[72](#bib.bib72), [107](#bib.bib107), [108](#bib.bib108), [71](#bib.bib71)],
    and adversarial learning methods [[109](#bib.bib109), [110](#bib.bib110)]. Optimization-based
    methods, e.g., [[70](#bib.bib70)], adopts a two-stage framework to solve the SR
    image reconstruction problem based on the non-homogeneous Poisson point process.
    Supervised methods either utilize residual connections to prevent the network
    models from the problem of gradients vanishing when generating SR images or estimate
    optical flow and temporal constraints to learn the motion cues, so as to reconstruct
    SR videos. For adversarial learning methods, Wang et al.[[109](#bib.bib109)] propose
    a representative end-to-end SR image reconstruction framework without access to
    the ground truth (GT), i.e., HR images.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第二个挑战，Scheerlinck 等人[[19](#bib.bib19)] 采用了递归连接来构建时间上的状态，这使得一个更小的递归神经网络能够重用先前重建的结果。有趣的是，Duwek
    等人[[105](#bib.bib105)] 结合了基于拉普拉斯预测和泊松积分的 CNNs 和 SNNs 来实现视频重建，并且参数更少。为了解决第三个挑战，GANs、双重积分模型[[106](#bib.bib106)]
    和 RNNs 被应用于避免生成模糊结果，并从事件中获得高速和 HDR 视频[[17](#bib.bib17), [102](#bib.bib102), [90](#bib.bib90),
    [89](#bib.bib89)]。至于从事件中生成超分辨率（SR）图像/视频，我们将现有的工作分为三类，包括基于优化的方法[[70](#bib.bib70)]、监督学习方法[[72](#bib.bib72),
    [107](#bib.bib107), [108](#bib.bib108), [71](#bib.bib71)] 和对抗学习方法[[109](#bib.bib109),
    [110](#bib.bib110)]。基于优化的方法，例如[[70](#bib.bib70)]，采用了一个两阶段框架来解决基于非齐次泊松点过程的 SR 图像重建问题。监督学习方法要么利用残差连接来防止网络模型在生成
    SR 图像时出现梯度消失的问题，要么估计光流和时间约束来学习运动线索，从而重建 SR 视频。对于对抗学习方法，Wang 等人[[109](#bib.bib109)]
    提出了一个代表性的端到端 SR 图像重建框架，无需访问真实值（GT），即 HR 图像。
- en: 'Remarks: In this section, we discuss various techniques for event-based image/video
    reconstruction. However, we acknowledge the need for a brief comparison to determine
    which technique is more suitable for different scenarios. While DL-based methods
    have shown significant accuracy gains in terms of accuracy and photorealism compared
    to early approaches relying on assumptions and regularization, they also come
    with increased computational complexity. On the other hand, traditional methods
    based on direct event integration may suffer from artifacts and produce less photo-realistic
    results. Furthermore, the choice of event representation remains an open question,
    and existing learned models often exhibit limited generalization capability. Noise
    in event data also poses a significant challenge, and the reconstruction of color
    images/videos from events is a particularly difficult problem. Future research
    efforts could focus on addressing these aspects to improve the quality and fidelity
    of reconstructed results.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：在本节中，我们讨论了基于事件的图像/视频重建的各种技术。然而，我们也承认需要进行简要的比较，以确定哪种技术更适合不同的场景。尽管基于深度学习的方法在准确性和真实感方面相比于早期依赖假设和正则化的方法显示了显著的精度提升，但它们也伴随着更高的计算复杂度。另一方面，基于直接事件集成的传统方法可能会遭遇伪影，并且产生较少的照片真实感结果。此外，事件表示的选择仍然是一个未解的问题，现有的学习模型通常表现出有限的泛化能力。事件数据中的噪声也构成了一个重大挑战，而从事件中重建彩色图像/视频是一个特别困难的问题。未来的研究工作可以集中在解决这些方面，以提高重建结果的质量和保真度。
- en: 3.2 Event-guided Image/Video Super-Resolution (SR)
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 事件引导的图像/视频超分辨率（SR）
- en: 'Insight: The goal is to explore the visual information, e.g., edge, and high
    temporal resolution of events, which are fused with the low-resolution (LR) image/video
    to recover the high-resolution (HR) image/video, as shown in Tab. [IV](#S3.T4
    "TABLE IV ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '洞察：目标是探索视觉信息，例如边缘和事件的高时间分辨率，这些信息与低分辨率（LR）图像/视频融合，以恢复高分辨率（HR）图像/视频，如表[IV](#S3.T4
    "TABLE IV ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")所示。'
- en: 'Image SR: eSL-Net [[111](#bib.bib111)] is the first work that introduces events
    for guiding image SR. It proposes a unified event-guided sparse learning framework
    that simultaneously denoises, deblurs, and super-resolves the low-quality active
    pixel sensor (APS) ²²2A type of frame-based sensor, embedded in DAVIS event cameras.
    images to recover high-quality images in an end-to-end learning manner. However,
    due to the limitations of sparse coding, this method performs poorly on more complex
    datasets [[111](#bib.bib111), [112](#bib.bib112)]. EvIntSR [[112](#bib.bib112)]
    achieves the goal of image SR in two steps: 1) Synthesizing a sequence of latent
    frames by combing events and blurry LR frames; 2) Merging latent frames to obtain
    a sharp HR frame. In general, EvIntSR explores the distinctive properties of events
    more directly than eSL and achieves better SR results on the simulation dataset.
    However, this method has two drawbacks: 1) Errors are accumulated in the two-stage
    training procedure; 2) The visual information of events is less explored in the
    second stage.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图像SR：eSL-Net [[111](#bib.bib111)]是第一个引入事件指导图像SR的工作。它提出了一个统一的事件指导的稀疏学习框架，以端到端的学习方式同时对低质量的活跃像素传感器（APS）²²²这种基于帧的传感器嵌入在DAVIS事件相机中的图像进行去噪、去模糊和超分辨。然而，由于稀疏编码的局限性，这种方法在更复杂的数据集上表现不佳
    [[111](#bib.bib111), [112](#bib.bib112)]。EvIntSR [[112](#bib.bib112)]通过两个步骤实现图像SR的目标：1）通过组合事件和模糊的LR帧合成一系列潜在帧；2）合并潜在帧获得清晰的HR帧。总的来说，EvIntSR比eSL更直接地探索了事件的独特属性，并在模拟数据集上取得了更好的SR结果。然而，这种方法有两个缺点：1）误差在两阶段训练过程中累积；2）在第二阶段较少探索事件的视觉信息。
- en: 'Video SR: Compare with image SR, video SR pays more attention to the relationship
    between multiple frames. E-VSR [[108](#bib.bib108)] is the first VSR framework
    with events. Similar to EvIntSR, it also consists of two sub-tasks: video frame
    interpolation and video SR and is limited by the accumulated errors. Recently,
    EG-VSR [[113](#bib.bib113)] employs implicit functions for learning the continuous
    representation of videos. This method enables end-to-end upsampling at arbitrary
    scales, offering advantages in the video SR task.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 视频SR：与图像SR相比，视频SR更加关注多个帧之间的关系。E-VSR [[108](#bib.bib108)]是第一个具有事件的VSR框架。与EvIntSR类似，它也由两个子任务组成：视频帧插值和视频SR，并受到累积误差的限制。最近，EG-VSR
    [[113](#bib.bib113)]采用隐式函数来学习视频的连续表示。该方法使得可以在任意比例下进行端到端的上采样，在视频SR任务中具有优势。
- en: 'TABLE IV: Comparison of the representative event-guided image/video SR methods.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '表IV: 代表性事件指导图像/视频SR方法的比较。'
- en: '| Publication | Methods | Highlight | Backbone |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 出版物 | 方法 | 亮点 | 主干 |'
- en: '| ECCV 2020 | eSL-Net [[111](#bib.bib111)] | sparse learning | CNN |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2020 | eSL-Net [[111](#bib.bib111)] | 稀疏学习 | CNN |'
- en: '| ICCV 2021 | EvIntSR [[112](#bib.bib112)] | two-step methods | CNN |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2021 | EvIntSR [[112](#bib.bib112)] | 两步方法 | CNN |'
- en: '| CVPR 2021 | E-VSR [[108](#bib.bib108)] | two-step methods | CNN |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2021 | E-VSR [[108](#bib.bib108)] | 两步方法 | CNN |'
- en: '| CVPR 2023 | EG-VSR [[113](#bib.bib113)] | SR with arbitrary scales | ViT+CNN
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2023 | EG-VSR [[113](#bib.bib113)] | 任意尺度的SR | ViT+CNN |'
- en: 'Remarks: While significant progress has been made for this task, including
    the ability to perform upsampling at arbitrary scales, there are still areas that
    need further investigation. For example, the research ignores the distinct modality
    differences between events and RGB frames. Therefore, the directly fusing features
    of two modalities might degrade the performance of SR as events are often disturbed
    by unexpected noises, e.g., in low-light scenes. Future research could explore
    more to tackle these problems.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '备注：虽然在这个任务上取得了显著进展，包括能够在任意尺度上进行上采样，但仍然有需要进一步研究的地方。例如，研究忽略了事件和RGB帧之间的明显模态差异。因此，直接融合两种模态的特征可能会降低SR的性能，因为事件常常受到意外噪音的干扰，例如在低光场景中。未来的研究可以更深入地探索以解决这些问题。 '
- en: '![Refer to caption](img/e2554f33da133e0a55171402a123ca86.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e2554f33da133e0a55171402a123ca86.png)'
- en: 'Figure 5: Visual results of VFI by three different methods. (TimeLens[[93](#bib.bib93)],
    TimeReplayer[[96](#bib.bib96)], $A^{2}OF$[[98](#bib.bib98)])'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：三种不同方法的 VFI 视觉结果。（TimeLens[[93](#bib.bib93)]、TimeReplayer[[96](#bib.bib96)]、$A^{2}OF$[[98](#bib.bib98)]）
- en: .
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: '![Refer to caption](img/e16f657c729713bdfd1dc7d80a39e743.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e16f657c729713bdfd1dc7d80a39e743.png)'
- en: 'Figure 6: Representative deblurring methods, including, e.g., (a) Interaction-based
    methods, (b) Event fusion-based methods, e.g., and (c) Event selection-based methods.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：代表性的去模糊方法，包括，例如，（a）基于交互的方法，（b）基于事件融合的方法，以及（c）基于事件选择的方法。
- en: 3.3 Event-guided Video Frame Interpolation (VFI)
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 事件引导的视频帧插值 (VFI)
- en: 'Insight : This task leverages the high temporal resolution of events and aims
    to estimate the non-linear motion information between frames, so as to insert
    latent frames between two consecutive frames. Based on how the VFI frameworks
    are learned, we categorize them into three types: supervised, weakly-supervised,
    and unsupervised methods, as shown in Tab. [III](#S3.T3 "TABLE III ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks"). The VFI results
    of some representative methods are visualized in Fig. [5](#S3.F5 "Figure 5 ‣ 3.2
    Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '见解：该任务利用事件的高时间分辨率，旨在估计帧间的非线性运动信息，以在两个连续帧之间插入潜在帧。根据 VFI 框架的学习方式，我们将其分为三种类型：监督式、弱监督式和无监督式方法，如表格
    [III](#S3.T3 "TABLE III ‣ 3.1 Event-based Image/Video Reconstruction ‣ 3 Image
    Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") 所示。一些代表性方法的 VFI 结果在图 [5](#S3.F5 "Figure 5 ‣ 3.2 Event-guided
    Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks") 中得到了可视化。'
- en: 'Supervised methods: TimeLens [[93](#bib.bib93)] is the first and representative
    work, which employs four modules to fuse features to achieve warping-based and
    synthesis-based interpolation (See Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")(a)). A dataset
    with spatially aligned events and high-speed videos was also released. However,
    it has limitations in that 1) the optical flow estimated from events limits the
    warped frames; 2) the noisy events restrict the quality of optical flow; and 3)
    it is learned sequentially (i.e., not in an end-to-end manner). Therefore, training
    TimeLens is difficult, and errors are accumulated, degrading the performance.
    These problems are better addressed later on by Timelens++[[97](#bib.bib97)],
    $A^{2}OF$ [[98](#bib.bib98)], and EFI-Net[[94](#bib.bib94)].'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '监督式方法：TimeLens [[93](#bib.bib93)] 是第一个且具有代表性的工作，它采用了四个模块来融合特征，以实现基于变形和合成的插值（见图
    [4](#S3.F4 "Figure 4 ‣ 3.1 Event-based Image/Video Reconstruction ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")(a)）。还发布了一个具有空间对齐事件和高速视频的数据集。然而，它有以下局限性：1) 从事件中估计的光流限制了变形帧；2)
    嘈杂的事件限制了光流的质量；3) 它是顺序学习的（即非端到端方式）。因此，训练 TimeLens 较为困难，错误会累积，从而降低性能。这些问题在后来由 Timelens++[[97](#bib.bib97)]、$A^{2}OF$ [[98](#bib.bib98)]
    和 EFI-Net[[94](#bib.bib94)] 得到了更好的解决。'
- en: 'In particular, Timelens++[[97](#bib.bib97)] proposes a framework, comprised
    of four modules including motion estimation, warping encoder, synthesis encoder,
    and fusion module, as depicted in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Event-based
    Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")(b). This method
    introduces multi-scale feature-level fusion and computes one-shot non-linear inter-frame
    motion, which could effectively be sampled for image warping based on events and
    frames. $A^{2}OF$ [[98](#bib.bib98)] focuses on generating the anisotropic optical
    flow from events. However, such an approach cannot model the complicated motion
    in real-world scenes; therefore, $A^{2}OF$ employs the distribution masks for
    optical flow from events to achieve the intricate intermediate motion interpolation.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '特别是，Timelens++[[97](#bib.bib97)] 提出了一个框架，包括四个模块：运动估计、变形编码器、合成编码器和融合模块，如图 [4](#S3.F4
    "Figure 4 ‣ 3.1 Event-based Image/Video Reconstruction ‣ 3 Image Restoration and
    Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks")(b) 所示。该方法引入了多尺度特征级融合，并计算一次性非线性帧间运动，这可以有效地基于事件和帧进行图像变形采样。$A^{2}OF$
    [[98](#bib.bib98)] 关注于从事件生成各向异性光流。然而，这种方法无法建模真实场景中的复杂运动；因此，$A^{2}OF$ 利用事件的分布掩码来实现复杂的中间运动插值。'
- en: It is worth noting that the events used by the aforementioned methods have the
    same spatial resolution as RGB frames. Unfortunately, it is quite expensive to
    match a RGB sensor’s resolution with an event sensor in real scenarios. Therefore,
    EFI-Net[[94](#bib.bib94)] proposes a multi-phase CNN-based framework, which can
    fuse the frames and events with various spatial resolutions. In summary, supervised
    methods rely on paired data with high-frame-rate videos and events. HS-ERGB [[93](#bib.bib93)]
    and BS-ERGB[[97](#bib.bib97)] are representative datasets. However, these datasets
    suffer from strict pixel alignments between events and frames and are expensive
    to collect. Therefore, some weakly-supervised and unsupervised methods have been
    proposed recently.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，上述方法使用的事件具有与 RGB 帧相同的空间分辨率。不幸的是，在实际场景中，将 RGB 传感器的分辨率与事件传感器匹配是相当昂贵的。因此，EFI-Net[[94](#bib.bib94)]
    提出了一个多阶段 CNN 基础框架，可以融合具有不同空间分辨率的帧和事件。总的来说，监督方法依赖于具有高帧率视频和事件的配对数据。HS-ERGB [[93](#bib.bib93)]
    和 BS-ERGB[[97](#bib.bib97)] 是具有代表性的数据集。然而，这些数据集在事件和帧之间的像素对齐要求严格，且采集成本高。因此，最近提出了一些弱监督和无监督方法。
- en: 'Weakly-supervised methods: Yu et al.[[95](#bib.bib95)] proposed the first weakly-supervised
    event-based VFI method. In practice, it extracts complementary information from
    events to correct image appearance and employs an attention mechanism to support
    correspondence searching on the low-resolution feature maps. Meanwhile, a real-world
    dataset, namely SloMo-DVS, is also released.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督方法：Yu 等人[[95](#bib.bib95)] 提出了首个基于事件的弱监督视频帧插值（VFI）方法。在实践中，该方法从事件中提取补充信息以纠正图像外观，并利用注意力机制支持低分辨率特征图上的对应关系搜索。同时，还发布了一个真实世界数据集，即
    SloMo-DVS。
- en: 'Unsupervised methods: TimeReplayer [[96](#bib.bib96)] is the first unsupervised
    method, trained in a cycle-consistent manner, as shown in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1 Event-based Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    (c). It directly estimates the optical flow between the key-frame and the input
    frame, instead of computing intermediate frames as a proportion of the computed
    optical flow between input frames. In this way, the complex motion can be estimated.
    Then, input frames can be reconstructed by key-frame and inverse optical flow.
    Totally, this cycle consistency method not only models complex nonlinear motion
    but also avoids the need for a large amount of paired high-speed frames and events.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '无监督方法：TimeReplayer [[96](#bib.bib96)] 是首个以循环一致的方式训练的无监督方法，如图 [4](#S3.F4 "Figure
    4 ‣ 3.1 Event-based Image/Video Reconstruction ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    (c) 所示。它直接估计关键帧和输入帧之间的光流，而不是计算作为输入帧之间计算的光流比例的中间帧。这样，可以估计复杂的运动。然后，可以通过关键帧和逆光流重建输入帧。总体来说，这种循环一致性方法不仅建模复杂的非线性运动，还避免了对大量配对的高速帧和事件的需求。'
- en: All the above-mentioned methods have the strong assumption that the exposure
    time of RGB frames is very short, and there are no blur artifacts in frames. However,
    this assumption is overly harsh because the practical exposure time can be long
    and results in blur artifacts in frames, particularly in complex lighting scenes.
    When the exposure time is longer, the issue of interpolation needs to be re-examined.
    For this reason, [[99](#bib.bib99), [101](#bib.bib101), [100](#bib.bib100)] jointly
    address the interpolation problem and deblurring. For example, E-CIR [[100](#bib.bib100)]
    transforms a blurry image into a sharp video that is represented as a time-to-intensity
    parametric function with events. Similarly, Zhang et al. [[101](#bib.bib101)]
    employed a learnable double integral network to map blurry frames to sharp latent
    images with event guidance. Lin et al. [[99](#bib.bib99)] emphasized that the
    residuals between a blurry image and a sharp image are event integrals. Based
    on this perspective, they proposed a network that uses events to estimate residuals
    for sharp frame restoration.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法都假设RGB帧的曝光时间非常短，并且帧中没有模糊伪影。然而，这一假设过于严格，因为实际的曝光时间可能很长，并且会导致帧中的模糊伪影，特别是在复杂的光照场景中。当曝光时间较长时，需要重新审视插值问题。因此，[[99](#bib.bib99)、[101](#bib.bib101)、[100](#bib.bib100)]
    共同解决了插值问题和去模糊问题。例如，E-CIR [[100](#bib.bib100)] 将模糊图像转换为清晰的视频，并将其表示为事件的时间-强度参数函数。类似地，Zhang
    等人 [[101](#bib.bib101)] 使用可学习的双重积分网络，通过事件指导将模糊帧映射到清晰的潜在图像。Lin 等人 [[99](#bib.bib99)]
    强调模糊图像与清晰图像之间的残差是事件积分。基于这一观点，他们提出了一个网络，利用事件估计残差以恢复清晰帧。
- en: 'Remarks: From our review, the majority of the methods are based on supervised
    learning, and weakly supervised or unsupervised methods still have a lot of room
    for further research. For example, mutual supervision could be performed through
    the imaging relationship between events and interpolated frames to relieve the
    need for ground truth, i.e., high frame rate video for training. Also, dense optical
    flow estimated by events[[114](#bib.bib114)], could be used as a constraint between
    interpolation results to improve VFI accuracy in an unsupervised manner.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：通过我们的综述，大多数方法基于监督学习，弱监督或无监督方法仍有很大的研究空间。例如，可以通过事件与插值帧之间的成像关系进行相互监督，从而减轻对真实标注数据的需求，即用于训练的高帧率视频。此外，事件估计的稠密光流[[114](#bib.bib114)]
    可以用作插值结果之间的约束，从而以无监督的方式提高VFI的准确性。
- en: 3.4 Event-guided Image/Video Deblurring
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 基于事件的图像/视频去模糊
- en: 'Insight: This task gets inspired by the no-motion-blur property of events and
    aims to restore a sharp image/video from a blurry image/video sequence under the
    guidance of events. Because supervised methods tend to achieve higher PSNR and
    SSIM [[96](#bib.bib96), [93](#bib.bib93)]. Traditional deblurring methods rely
    on the physical event generation model [[71](#bib.bib71)]. In particular, Pan
    et al. [[106](#bib.bib106)] proposed an event-based double integral model for
    recovering latent intensity images. Based on this model, sharp images and videos
    could be generated by solving the non-convex optimization problems under adverse
    visual conditions. However, it suffers from the problem of accumulated error caused
    by noise in the sampling process. By contrast, learning-based methods directly
    explore the relationship between blurry and sharp images with the help of events
    and show more plausible deblurring result. In this paper, we divide the learning-based
    methods into three categories: 1) interaction-based methods; 2) fusion-based methods;
    and 3) selection-based methods (See Fig. [6](#S3.F6 "Figure 6 ‣ 3.2 Event-guided
    Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")). The
    deblur results of some representative methods are shown in Tab. [V](#S3.T5 "TABLE
    V ‣ 3.4 Event-guided Image/Video Deblurring ‣ 3 Image Restoration and Enhancement
    ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '见解：该任务受到事件的无运动模糊特性启发，旨在在事件的指导下从模糊的图像/视频序列中恢复清晰的图像/视频。由于监督方法通常能够实现更高的PSNR和SSIM
    [[96](#bib.bib96), [93](#bib.bib93)]。传统的去模糊方法依赖于物理事件生成模型 [[71](#bib.bib71)]。特别是，Pan等人
    [[106](#bib.bib106)] 提出了一个基于事件的双重积分模型用于恢复潜在的强度图像。基于该模型，通过在恶劣视觉条件下解决非凸优化问题，可以生成清晰的图像和视频。然而，它存在由采样过程中的噪声引起的累积误差问题。相比之下，基于学习的方法直接探索模糊图像与清晰图像之间的关系，借助事件显示出更可信的去模糊结果。在本文中，我们将基于学习的方法分为三类：1）基于交互的方法；2）基于融合的方法；3）基于选择的方法（见图
    [6](#S3.F6 "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3
    Image Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")）。一些代表性方法的去模糊结果显示在表 [V](#S3.T5 "TABLE V ‣ 3.4 Event-guided
    Image/Video Deblurring ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for
    Event-based Vision: A Comprehensive Survey and Benchmarks")。'
- en: 'TABLE V: Qualitative comparison of deblurring methods on GoPro and HQF dataset
    from  [[101](#bib.bib101)]. ‘N/A’ means no results are available.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：基于GoPro和HQF数据集的去模糊方法的定性比较，来自 [[101](#bib.bib101)]。‘N/A’表示没有结果。
- en: '| Method | GoPro | HQF | Param. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GoPro | HQF | 参数 |'
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ | PSNR $\uparrow$ |
    SSIM $\uparrow$ | LPIPS$\downarrow$ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| PSNR$\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ | PSNR $\uparrow$ |
    SSIM $\uparrow$ | LPIPS$\downarrow$ |'
- en: '| LEVS  [[115](#bib.bib115)] | 20.84 | 0.5473 | 0.1111 | 20.08 | 0.5629 | 0.0998
    | 18.21M |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| LEVS  [[115](#bib.bib115)] | 20.84 | 0.5473 | 0.1111 | 20.08 | 0.5629 | 0.0998
    | 18.21M |'
- en: '| EDI  [[106](#bib.bib106)] | 21.29 | 0.6402 | 0.1104 | 19.65 | 0.5909 | 0.1173
    | N/A |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| EDI  [[106](#bib.bib106)] | 21.29 | 0.6402 | 0.1104 | 19.65 | 0.5909 | 0.1173
    | N/A |'
- en: '| eSL-Net  [[111](#bib.bib111)] | 17.80 | 0.5655 | 0.1141 | 21.36 | 0.6659
    | 0.0644 | 0.188M |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| eSL-Net  [[111](#bib.bib111)] | 17.80 | 0.5655 | 0.1141 | 21.36 | 0.6659
    | 0.0644 | 0.188M |'
- en: '| LEDVDI  [[99](#bib.bib99)] | 25.38 | 0.8567 | 0.0280 | 22.58 | 0.7472 | 0.0578
    | 4.996M |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| LEDVDI  [[99](#bib.bib99)] | 25.38 | 0.8567 | 0.0280 | 22.58 | 0.7472 | 0.0578
    | 4.996M |'
- en: '| RED [[116](#bib.bib116)] | 25.14 | 0.8587 | 0.0425 | 24.48 | 0.7572 | 0.0475
    | 9.762M |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| RED [[116](#bib.bib116)] | 25.14 | 0.8587 | 0.0425 | 24.48 | 0.7572 | 0.0475
    | 9.762M |'
- en: '| EVDI [[101](#bib.bib101)] | 30.40 | 0.9058 | 0.0144 | 24.77 | 0.7664 | 0.0423
    | 0.393M |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| EVDI [[101](#bib.bib101)] | 30.40 | 0.9058 | 0.0144 | 24.77 | 0.7664 | 0.0423
    | 0.393M |'
- en: 'Interaction-based methods usually input the blurry image and events into two
    different networks and then carry out information interaction after encoding the
    features in each branch to improve the deblurring effect (See Fig. [6](#S3.F6
    "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks") (a)). For example, in [[116](#bib.bib116)], a self-supervised
    framework was proposed to reduce the domain gap between simulated and real-world
    data. Specifically, they first estimated the optical flow and exploited the blurry
    and photometric consistency to enable self-supervision on the deblurring network.
    Lin et al.[[99](#bib.bib99)] introduced a CNN framework to predict the residual
    between sharp and blurry images for deblurring, and the residual between sharp
    frames for interpolation. Jiang et al. [[117](#bib.bib117)] explored long-term,
    local appearance/motion cues and novel event boundary priors to solve motion deblurring.
    Zhang et al. [[101](#bib.bib101)] utilized low latency of events to alleviate
    motion blur and facilitate the prediction of intermediate frames.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '基于交互的方法通常将模糊图像和事件输入到两个不同的网络中，然后在对每个分支中的特征进行编码后进行信息交互，以改善去模糊效果（见图 [6](#S3.F6
    "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks") (a)）。例如，在 [[116](#bib.bib116)] 中，提出了一个自监督框架，以减少模拟数据和真实世界数据之间的领域差距。具体来说，他们首先估计光流，并利用模糊和光度一致性来实现去模糊网络上的自监督。Lin
    等人[[99](#bib.bib99)] 引入了一个 CNN 框架，用于预测清晰图像和模糊图像之间的残差进行去模糊，以及清晰帧之间的残差用于插值。Jiang
    等人[[117](#bib.bib117)] 探索了长期的、局部的外观/运动线索和新颖的事件边界先验，以解决运动去模糊问题。Zhang 等人[[101](#bib.bib101)]
    利用事件的低延迟来减轻运动模糊，并促进中间帧的预测。'
- en: 'Fusion-based methods aims to design a principled framework for video deblurring
    and event-guided deblurring [[111](#bib.bib111)], as shown in Fig. [6](#S3.F6
    "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration
    and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey
    and Benchmarks")(b). For instance, Shang et al. [[118](#bib.bib118)] proposed
    a two-stream framework to explore the non-consecutively blurry frames and bridge
    the gap between event-guided and video deblurring.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '基于融合的方法旨在设计一个有原则的视频去模糊和事件引导去模糊框架[[111](#bib.bib111)]，如图 [6](#S3.F6 "Figure
    6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and
    Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks")(b) 所示。例如，Shang 等人[[118](#bib.bib118)] 提出了一个双流框架，以探索非连续模糊帧，并弥合事件引导和视频去模糊之间的差距。'
- en: 'Selection-based methods, e.g., [[119](#bib.bib119)], formulate the event-guided
    motion deblurring by considering the unknown exposure and readout time in the
    video frame acquisition process. The main challenge is how to selectively use
    event features by estimating the cross-modal correlation between the blurry frame
    features and the events. Therefore, the proposed event selection module subtly
    selects useful events, and the fusion module fuses the selected event features
    and blur frames effectively, as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.2 Event-guided
    Image/Video Super-Resolution (SR) ‣ 3 Image Restoration and Enhancement ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks") (c).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '基于选择的方法，如 [[119](#bib.bib119)]，通过考虑视频帧获取过程中的未知曝光和读出时间来制定事件引导的运动去模糊。主要挑战是如何通过估计模糊帧特征和事件之间的跨模态相关性来有选择地使用事件特征。因此，提出的事件选择模块巧妙地选择有用的事件，而融合模块有效地融合所选择的事件特征和模糊帧，如图
    [6](#S3.F6 "Figure 6 ‣ 3.2 Event-guided Image/Video Super-Resolution (SR) ‣ 3
    Image Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") (c) 所示。'
- en: 'Remarks: Most of the aforementioned deblurring methods are limited to some
    specific scenes. In some scenes with large or fast motions, the model’s accuracy
    may deteriorate dramatically.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：上述去模糊方法大多局限于特定场景。在一些大幅度或快速运动的场景中，模型的准确性可能会显著下降。
- en: 'TABLE VI: Experiments of representative methods on event object classification.
    N/A means no results available.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：代表性方法在事件目标分类上的实验。N/A 表示没有结果可用。
- en: '| Publication | Methods | Dataset |  | Param. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 发表文章 | 方法 | 数据集 |  | 参数 |'
- en: '| N-MINIST [[120](#bib.bib120)] | MINIST-DVS [[121](#bib.bib121)] | N-Caltech101 [[120](#bib.bib120)]
    | CIFAR10-DVS [[122](#bib.bib122)] | N-Cars [[44](#bib.bib44)] | ASL-DVS [[65](#bib.bib65)]
    | N-ImageNet [[48](#bib.bib48)] |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| N-MINIST [[120](#bib.bib120)] | MINIST-DVS [[121](#bib.bib121)] | N-Caltech101 [[120](#bib.bib120)]
    | CIFAR10-DVS [[122](#bib.bib122)] | N-Cars [[44](#bib.bib44)] | ASL-DVS [[65](#bib.bib65)]
    | N-ImageNet [[48](#bib.bib48)] |'
- en: '| TPAMI 2015 | HFirst [[66](#bib.bib66)] | 0.712 | N/A | 0.054 | N/A | 0.561
    | N/A | N/A | 21.79M |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| TPAMI 2015 | HFirst [[66](#bib.bib66)] | 0.712 | N/A | 0.054 | N/A | 0.561
    | N/A | N/A | 21.79M |'
- en: '| TPAMI 2016 | HOTS [[43](#bib.bib43)] | 0.808 | 0.803 | 0.210 | 0.271 | 0.624
    | N/A | N/A | 21.79M |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| TPAMI 2016 | HOTS [[43](#bib.bib43)] | 0.808 | 0.803 | 0.210 | 0.271 | 0.624
    | N/A | N/A | 21.79M |'
- en: '| CVPR 2018 | HATS [[44](#bib.bib44)] | 0.991 | 0.984 | 0.642 | 0.524 | 0.902
    | N/A | 0.471 | 21.79M |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2018 | HATS [[44](#bib.bib44)] | 0.991 | 0.984 | 0.642 | 0.524 | 0.902
    | N/A | 0.471 | 21.79M |'
- en: '| ICCV 2019 | EST [[39](#bib.bib39)] | N/A | N/A | 0.817 | N/A | 0.925 | N/A
    | 0.489 | 21.79M |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2019 | EST [[39](#bib.bib39)] | N/A | N/A | 0.817 | N/A | 0.925 | N/A
    | 0.489 | 21.79M |'
- en: '| ICCV 2019 | RG-CNNs [[52](#bib.bib52)] | 0.990 | 0.986 | 0.657 | 0.540 |
    0.914 | 0.901 | N/A | 19.46M |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2019 | RG-CNNs [[52](#bib.bib52)] | 0.990 | 0.986 | 0.657 | 0.540 |
    0.914 | 0.901 | N/A | 19.46M |'
- en: '| TPAMI 2019 | DART [[123](#bib.bib123)] | 0.979 | 0.985 | 0.664 | 0.658 |
    N/A | N/A | N/A | N/A |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| TPAMI 2019 | DART [[123](#bib.bib123)] | 0.979 | 0.985 | 0.664 | 0.658 |
    N/A | N/A | N/A | N/A |'
- en: '| ECCV 2020 | Matrix-LSTM [[40](#bib.bib40)] | 0.989 | N/A | 0.843 | N/A |
    0.943 | 0.997 | 0.322 | 25.56M |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2020 | Matrix-LSTM [[40](#bib.bib40)] | 0.989 | N/A | 0.843 | N/A |
    0.943 | 0.997 | 0.322 | 25.56M |'
- en: '| ECCV 2020 | ASCN [[59](#bib.bib59)] | N/A | N/A | 0.745 | N/A | 0.944 | N/A
    | N/A | 9.47M |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2020 | ASCN [[59](#bib.bib59)] | N/A | N/A | 0.745 | N/A | 0.944 | N/A
    | N/A | 9.47M |'
- en: '| ICCV 2021 | EvS  [[124](#bib.bib124)] | N/A | 0.991 | 0.761 | 0.680 | 0.931
    | N/A | N/A | N/A |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2021 | EvS  [[124](#bib.bib124)] | N/A | 0.991 | 0.761 | 0.680 | 0.931
    | N/A | N/A | N/A |'
- en: '| ICCV 2021 | DiST [[48](#bib.bib48)] | N/A | N/A | N/A | N/A | N/A | N/A |
    0.484 | 21.79M |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2021 | DiST [[48](#bib.bib48)] | N/A | N/A | N/A | N/A | N/A | N/A |
    0.484 | 21.79M |'
- en: '| TCSVT 2021 | MVF-Net [[38](#bib.bib38)] | 0.993 | N/A | 0.871 | 0.663 | 0.968
    | 0.996 | N/A | 21.79M |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| TCSVT 2021 | MVF-Net [[38](#bib.bib38)] | 0.993 | N/A | 0.871 | 0.663 | 0.968
    | 0.996 | N/A | 21.79M |'
- en: '| CVPR 2022 | AEGNNs [[125](#bib.bib125)] | N/A | N/A | 0.668 | N/A | 0.945
    | N/A | N/A | N/A |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2022 | AEGNNs [[125](#bib.bib125)] | N/A | N/A | 0.668 | N/A | 0.945
    | N/A | N/A | N/A |'
- en: '| CVPR 2022 | EV-VGCNN [[53](#bib.bib53)] | 0.994 | N/A | 0.748 | N/A | 0.953
    | 0.983 | N/A | 21.79M |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2022 | EV-VGCNN [[53](#bib.bib53)] | 0.994 | N/A | 0.748 | N/A | 0.953
    | 0.983 | N/A | 21.79M |'
- en: '| TPAMI 2022 | TORE [[3](#bib.bib3)] | 0.994 | N/A | 0.798 | N/A | 0.977 |
    0.996 | N/A | 5.94M |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| TPAMI 2022 | TORE [[3](#bib.bib3)] | 0.994 | N/A | 0.798 | N/A | 0.977 |
    0.996 | N/A | 5.94M |'
- en: 3.5 Event-based Deep Image/Video HDR
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 基于事件的深度图像/视频HDR
- en: 'Insight: The HDR of events makes it naturally more advantageous to reconstruct
    an HDR image/video. The predominant methods can be divided into two main categories:
    event-based HDR image/video HDR methods [[90](#bib.bib90), [102](#bib.bib102),
    [17](#bib.bib17)] and event-guided image/video HDR methods (a hybrid of event
    and frame data) [[126](#bib.bib126), [127](#bib.bib127), [18](#bib.bib18)].'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：事件的高动态范围（HDR）使得重建HDR图像/视频自然具有更多优势。主要方法可以分为两大类：基于事件的HDR图像/视频HDR方法 [[90](#bib.bib90),
    [102](#bib.bib102), [17](#bib.bib17)] 和事件引导的图像/视频HDR方法（事件和帧数据的混合） [[126](#bib.bib126),
    [127](#bib.bib127), [18](#bib.bib18)]。
- en: 'Event-based image/video HDR typically employs the idea of event-to-image translation—reconstructing
    HDR images from events, as mentioned in Sec. [3.1](#S3.SS1 "3.1 Event-based Image/Video
    Reconstruction ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"). Representative works are based
    on the recurrent neural networks (RNNs) [[90](#bib.bib90), [102](#bib.bib102)]
    (See Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based Deep Image/Video HDR ‣ 3 Image
    Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") (a)) or generative adversarial networks (GANs) [[17](#bib.bib17)]
    (See Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based Deep Image/Video HDR ‣ 3 Image
    Restoration and Enhancement ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") (b)). However, the reconstructed HDR results intrinsically
    lack textural details, especially in the static scene, as events are sparse and
    motion-dependent.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 基于事件的图像/视频 HDR 通常采用事件到图像的转换思想——从事件重建 HDR 图像，如第 [3.1](#S3.SS1 "3.1 基于事件的图像/视频重建
    ‣ 3 图像恢复与增强 ‣ 基于事件的视觉的深度学习：综合调查与基准") 节中提到的那样。代表性工作基于递归神经网络 (RNNs) [[90](#bib.bib90),
    [102](#bib.bib102)]（见图 [7](#S3.F7 "图 7 ‣ 3.5 基于事件的深度图像/视频 HDR ‣ 3 图像恢复与增强 ‣ 基于事件的视觉的深度学习：综合调查与基准")
    (a)）或生成对抗网络 (GANs) [[17](#bib.bib17)]（见图 [7](#S3.F7 "图 7 ‣ 3.5 基于事件的深度图像/视频 HDR
    ‣ 3 图像恢复与增强 ‣ 基于事件的视觉的深度学习：综合调查与基准") (b)）。然而，由于事件稀疏且依赖于运动，重建的 HDR 结果在静态场景中本质上缺乏纹理细节。
- en: '![Refer to caption](img/43598221bba1b31f9aa472c57b8d8bdc.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/43598221bba1b31f9aa472c57b8d8bdc.png)'
- en: 'Figure 7: Representative DL-based HDR imaging methods. (a) RNN-based methods [[90](#bib.bib90),
    [102](#bib.bib102)] and (b) GAN-based method [[17](#bib.bib17)] that use events
    only, and (c) event-frame fusion [[126](#bib.bib126), [127](#bib.bib127), [18](#bib.bib18)].
    (latent embedding denotes information learned by feature extractor, filter,etc.)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 代表性的基于深度学习的 HDR 成像方法。 (a) 基于 RNN 的方法 [[90](#bib.bib90), [102](#bib.bib102)]
    和 (b) 仅使用事件的 GAN 方法 [[17](#bib.bib17)]，以及 (c) 事件-帧融合 [[126](#bib.bib126), [127](#bib.bib127),
    [18](#bib.bib18)]。 (潜在嵌入表示由特征提取器、滤波器等学到的信息。)'
- en: 'Event-guided image/video HDR: HDR imaging methods are categorized into two
    types: single-exposure HDR and multi-exposure HDR (See [[128](#bib.bib128)] for
    details). Event-guided image/video also follows these two paradigms.  [[126](#bib.bib126),
    [127](#bib.bib127), [18](#bib.bib18)] explore the potential of merging both events
    and frames for this task, as shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Event-based
    Deep Image/Video HDR ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for
    Event-based Vision: A Comprehensive Survey and Benchmarks") (c). In particular,
    Han et al. [[126](#bib.bib126)] proposed the first single-exposure HDR imaging
    framework to recover an HR HDR image by adding the LR intensity map generated
    from events. This framework addresses the gaps in spatial resolution, dynamic
    range, and color representation of the hybrid sensor system to pursue a better
    fusion. By contrast, EHDR  [[18](#bib.bib18)] is the first multi-exposure HDR
    imaging framework that combines bracketed LDR images and synchronized events to
    recover an HDR image. To alleviate the impact of scene motion between exposures,
    EHDR employs events to learn a deformable convolution kernel, which can align
    feature maps from images with different exposure times. By contrast, HDRev-Net [[129](#bib.bib129)]
    implicitly mitigates the misalignment of multi-modal representations by aligning
    them in the shared latent space and fusing them with a confidence-guided fusion
    module.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 事件引导的图像/视频 HDR：HDR 成像方法分为两类：单次曝光 HDR 和多次曝光 HDR（详见 [[128](#bib.bib128)]）。事件引导的图像/视频也遵循这两种模式。
    [[126](#bib.bib126), [127](#bib.bib127), [18](#bib.bib18)] 探索了将事件和帧融合的潜力，如图 [7](#S3.F7
    "图 7 ‣ 3.5 基于事件的深度图像/视频 HDR ‣ 3 图像恢复与增强 ‣ 基于事件的视觉的深度学习：综合调查与基准") (c) 所示。特别地，Han
    等 [[126](#bib.bib126)] 提出了第一个单次曝光 HDR 成像框架，通过添加由事件生成的低分辨率强度图来恢复高分辨率 HDR 图像。该框架解决了混合传感器系统在空间分辨率、动态范围和颜色表示方面的差距，旨在实现更好的融合。相比之下，EHDR
    [[18](#bib.bib18)] 是第一个多次曝光 HDR 成像框架，它结合了分段的 LDR 图像和同步事件来恢复 HDR 图像。为了减轻曝光之间场景运动的影响，EHDR
    利用事件学习可变形卷积核，从而对齐不同曝光时间的图像特征图。相比之下，HDRev-Net [[129](#bib.bib129)] 通过在共享的潜在空间中对齐多模态表示并用信心引导的融合模块融合它们，隐式减轻了多模态表示的错位。
- en: 'Remarks: Based on the review, only two research works have been proposed for
    deep HDR imaging. The most possible reason is that it is practically difficult
    to collect paired datasets for training, especially for multi-exposure HDR imaging.
    Future directions could consider directly fusing LDR images and events and learning
    a unified HDR imaging framework without relying on image/video reconstruction.
    Also, it is promising to explore how to leverage events to guide color image HDR
    imaging.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：根据回顾，目前仅有两个研究工作提出了深度HDR成像。最可能的原因是，收集用于训练的配对数据集在实践中相当困难，特别是对于多曝光HDR成像。未来的方向可以考虑直接融合LDR图像和事件，并学习一个统一的HDR成像框架，而无需依赖图像/视频重建。此外，探索如何利用事件指导彩色图像HDR成像也很有前景。
- en: 4 Scene Understanding and 3D Vision
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 场景理解与3D视觉
- en: 4.1 Scene Understanding
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 场景理解
- en: 4.1.1 Object Classification
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 物体分类
- en: 'Insight: Event-based object classification aims to identify and classify objects
    from an event stream based on their visual characteristics. This allows for real-time
    object classification with high temporal resolution and low latency, making it
    suitable for applications in robotics, autonomous vehicles, and other mobile systems.
    Intuitively, we divide the event-based classification methods into three categories
    according to the input event representations and DNN types: 1) learning-based;
    2) graph-based; and 3) asynchronous model-based methods.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 'Insight: 基于事件的物体分类旨在根据物体的视觉特征识别和分类对象。这允许实时物体分类，具有高时间分辨率和低延迟，使其适用于机器人、自动驾驶车辆和其他移动系统的应用。从直观上讲，我们根据输入事件表示和DNN类型将基于事件的分类方法分为三类：1)
    基于学习的；2) 基于图的；3) 异步模型的。'
- en: Learning-based methods Gehrig et al. [[39](#bib.bib39)] proposed the first end-to-end
    framework to learn event representation for object classification. In particular,
    it converts event streams into grid-like tensors, i.e., Event Spike Tensor (EST),
    through a sequence of differentiable operations. Though EST achieves high accuracy,
    it also brings redundant computation costs and high latency. To tackle this problem,
    Cannici et al. [[40](#bib.bib40)] proposed Matrix-LSTM to adaptively integrate
    and utilize information of events by the memory mechanism of LSTM. This makes
    it possible to efficiently aggregate the temporal information of event data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的方法 Gehrig等人 [[39](#bib.bib39)] 提出了第一个端到端框架，用于学习事件表示以进行物体分类。特别是，它通过一系列可微操作将事件流转换为类似网格的张量，即事件尖峰张量（EST）。尽管EST实现了高准确率，但也带来了冗余计算成本和高延迟。为了解决这个问题，Cannici等人 [[40](#bib.bib40)]
    提出了Matrix-LSTM，通过LSTM的记忆机制自适应地整合和利用事件信息。这使得能够高效地汇聚事件数据的时间信息。
- en: Graph-based methods Some works also utilize graphs for representing events for
    the computational efficiency of the Graph CNNs. Yin et al. [[65](#bib.bib65)]
    proposed a representative approach that represents event data as a graph and introduced
    residual graph CNNs (RG-CNNs).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的方法 一些研究还利用图来表示事件，以提高图卷积神经网络（Graph CNNs）的计算效率。Yin等人 [[65](#bib.bib65)] 提出了一个将事件数据表示为图的代表性方法，并引入了残差图卷积神经网络（RG-CNNs）。
- en: Asynchronous model-based methods Though the learning-based methods obtain plausible
    classification results, they fail to fully explore the inherent asynchronicity
    and sparsity of event data. Consequently, Nico et al. [[59](#bib.bib59)] converted
    the classification models trained on the synchronous frame-like event representations
    into models taking asynchronous events as inputs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 异步模型的方法 尽管基于学习的方法获得了可行的分类结果，但它们未能充分挖掘事件数据的固有异步性和稀疏性。因此，Nico等人 [[59](#bib.bib59)]
    将在同步帧状事件表示上训练的分类模型转换为以异步事件作为输入的模型。
- en: 'TABLE VII: Comparison of existing representative event classification benchmarks.
    MR denotes Monitor Recording. MR is the process of capturing the visual output
    displayed on a computer monitor or screen.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：现有代表性事件分类基准的比较。MR表示监控录制。MR是捕捉计算机显示器或屏幕上显示的视觉输出的过程。
- en: '| Dataset | # of Samples | # of Classes | Sources | Paired RGB Data |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 样本数量 | 类别数量 | 来源 | 配对RGB数据 |'
- en: '| N-Cars [[44](#bib.bib44)] | 24029 | 2 | Real | N/A |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| N-Cars [[44](#bib.bib44)] | 24029 | 2 | 真实 | 无 |'
- en: '| N-Caltech101 [[120](#bib.bib120)] | 8709 | 101 | MR | Caltech101 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| N-Caltech101 [[120](#bib.bib120)] | 8709 | 101 | MR | Caltech101 |'
- en: '| CIFAR10-DVS [[122](#bib.bib122)] | 10000 | 10 | MR | CIFAR10 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR10-DVS [[122](#bib.bib122)] | 10000 | 10 | MR | CIFAR10 |'
- en: '| ASL-DVS [[65](#bib.bib65)] | 100800 | 24 | Real | N/A |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| ASL-DVS [[65](#bib.bib65)] | 100800 | 24 | 真实 | 无 |'
- en: '| N-MNIST [[120](#bib.bib120)] | 70000 | 10 | MR | MNIST |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| N-MNIST [[120](#bib.bib120)] | 70000 | 10 | MR | MNIST |'
- en: '| MNIST-DVS [[121](#bib.bib121)] | 30000 | 10 | MR | MNIST |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| MNIST-DVS [[121](#bib.bib121)] | 30000 | 10 | MR | MNIST |'
- en: '| N-ImageNet [[48](#bib.bib48)] | 1781167 | 1000 | MR | ImageNet |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| N-ImageNet [[48](#bib.bib48)] | 1781167 | 1000 | MR | ImageNet |'
- en: Besides, to fit with the sparse event data, VMV-GCN[[130](#bib.bib130)] first
    considers the relationships between vertices of the graph and then groups the
    vertices according to the proximity both in the original input and feature space.
    Furthermore, for computational efficiency, AEGNN [[125](#bib.bib125)] proposes
    to process events sparsely and asynchronously as temporally evolving graphs. Meanwhile,
    EV-VGCNN[[53](#bib.bib53)] utilizes voxel-wise vertices rather than point-wise
    inputs to explicitly exploit the regional 2D semantics of event streams while
    maintaining the trade-off between accuracy and model complexity.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，为了适应稀疏事件数据，VMV-GCN [[130](#bib.bib130)] 首先考虑图的顶点之间的关系，然后根据原始输入和特征空间中的接近程度对顶点进行分组。此外，为了提高计算效率，AEGNN
    [[125](#bib.bib125)] 提出了将事件以稀疏和异步的方式处理为时间演变图。同时，EV-VGCNN [[53](#bib.bib53)] 利用体素级的顶点而非点级输入，明确利用事件流的区域
    2D 语义，同时保持准确性和模型复杂性之间的平衡。
- en: 'TABLE VIII: Comparison of existing representative event object detection methods.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII: 现有代表性事件物体检测方法的比较。'
- en: '| Publications | Method | Representations | Highlight | Backbone | Frame Images
    | Multi-modal |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 发表论文 | 方法 | 表示 | 突出 | 主干 | 帧图像 | 多模态 |'
- en: '| CVPR 2019 | YOLE [[131](#bib.bib131)] | Surface-based | Event-based neural
    network components | CNN | ✗ | ✗ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | YOLE [[131](#bib.bib131)] | 基于表面的 | 基于事件的神经网络组件 | CNN | ✗ | ✗
    |'
- en: '| WACV 2022 | PointConv [[132](#bib.bib132)] | Image-based | pint-cloud feature
    extractor | CNN | ✗ | ✗ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| WACV 2022 | PointConv [[132](#bib.bib132)] | 基于图像的 | 点云特征提取器 | CNN | ✗ |
    ✗ |'
- en: '| MFI 2022 | GFA-Net [[133](#bib.bib133)] | Image-based | Edge information
    & Temporal information across event frames | CNN & ViT | ✗ | ✗ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| MFI 2022 | GFA-Net [[133](#bib.bib133)] | 基于图像的 | 事件帧间的边缘信息与时间信息 | CNN &
    ViT | ✗ | ✗ |'
- en: '| ECCV 2020 | NGA [[23](#bib.bib23)] | Image-based | Grafting pre-trained deep
    network for novel sensors | CNN | ✗ | ✓ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2020 | NGA [[23](#bib.bib23)] | 基于图像的 | 为新型传感器植入预训练深度网络 | CNN | ✗ |
    ✓ |'
- en: '| NeurIPS 2020 | RED [[23](#bib.bib23)] | Image-based | Recurrent architecture
    and temporal consistency | RNN | ✗ | ✗ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| NeurIPS 2020 | RED [[23](#bib.bib23)] | 基于图像的 | 循环架构和时间一致性 | RNN | ✗ | ✗
    |'
- en: '| TIP 2022 | ASTMNet [[134](#bib.bib134)] | Image-based | Continuous event
    stream with lightweight RNN | RNN | ✗ | ✗ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| TIP 2022 | ASTMNet [[134](#bib.bib134)] | 基于图像的 | 使用轻量级 RNN 的连续事件流 | RNN
    | ✗ | ✗ |'
- en: '| ICRA 2019 | Mixed-Yolo [[135](#bib.bib135)] | Image-based | Mixed APS frame
    and DVS frame | CNN | ✓ | ✗ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ICRA 2019 | Mixed-Yolo [[135](#bib.bib135)] | 基于图像的 | 混合 APS 帧和 DVS 帧 | CNN
    | ✓ | ✗ |'
- en: '| ICME 2019 | JDF [[136](#bib.bib136)] | Spike-based | Joint detection with
    event streams and frames | CNN & SNN | ✓ | ✗ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ICME 2019 | JDF [[136](#bib.bib136)] | 基于脉冲的 | 与事件流和帧的联合检测 | CNN & SNN |
    ✓ | ✗ |'
- en: '| ICRA 2022 | FPN-fusion events [[137](#bib.bib137)] | Image-based | Robust
    detection with RGB and event-based sensors | CNN | ✗ | ✓ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| ICRA 2022 | FPN-融合事件 [[137](#bib.bib137)] | 基于图像的 | 结合 RGB 和基于事件的传感器的鲁棒检测
    | CNN | ✗ | ✓ |'
- en: 'Benchmark datasets are vital foundations for the development of event-based
    vision, given that sufficient event data is barely available due to the novelty
    of event sensors. The existing event datasets can be briefly divided into two
    categories according to the captured scenes, i.e., the real and the simulated
    ones. Gehrig et al. [[25](#bib.bib25)] proposed to convert video datasets into
    event datasets by adaptive upsampling and using an event camera simulator (ESIM) [[85](#bib.bib85)].
    Models trained on the simulated dataset generalize well on the real data. More
    recently, N-ImageNet [[48](#bib.bib48)] serves as the first real large-scale fine-grained
    benchmark, which provides various validation sets to test the robustness of event-based
    object recognition approaches amidst changes in motion or illumination. We summarize
    the existing datasets for event recognition in Tab. [VII](#S4.T7 "TABLE VII ‣
    4.1.1 Object Classification ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") and conduct a benchmark evaluation for the representative event-based
    classification methods in Tab. [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video
    Deblurring ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks").'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '基准数据集是事件驱动视觉发展的重要基础，因为由于事件传感器的创新，实际的事件数据稀缺。现有的事件数据集可以根据捕获的场景大致分为两类，即真实和模拟的。Gehrig
    等人 [[25](#bib.bib25)] 提出了通过自适应上采样和使用事件相机模拟器（ESIM） [[85](#bib.bib85)] 将视频数据集转换为事件数据集。基于模拟数据集训练的模型在真实数据上表现良好。最近，N-ImageNet
    [[48](#bib.bib48)] 成为第一个真实的大规模细粒度基准，提供了各种验证集来测试事件驱动物体识别方法在运动或光照变化中的鲁棒性。我们在表格 [VII](#S4.T7
    "TABLE VII ‣ 4.1.1 Object Classification ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") 中总结了现有的事件识别数据集，并在表格 [VI](#S3.T6 "TABLE VI ‣ 3.4 Event-guided Image/Video
    Deblurring ‣ 3 Image Restoration and Enhancement ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks") 中对具有代表性的事件驱动分类方法进行了基准评估。'
- en: 'Remarks: The accuracy of event-based object classification is always hindered
    by insufficient annotated datasets. Therefore, endeavors have been made to simulate
    event data using ESIM or generate event data from the monitor-displayed image
    observation. It also deserves to adapt the model trained on synthetic data to
    real-world event data [[138](#bib.bib138), [139](#bib.bib139)]. Another research
    direction could focus on leveraging large amounts of unlabeled data or active
    learning, where the classifier can request additional labeled data as needed in
    order to improve its accuracy.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：事件驱动物体分类的准确性总是受到标注数据集不足的限制。因此，已尝试使用 ESIM 模拟事件数据或从显示器显示的图像观察生成事件数据。此外，还需要将基于合成数据训练的模型适应于真实世界的事件数据 [[138](#bib.bib138),
    [139](#bib.bib139)]。另一个研究方向可以集中在利用大量未标记的数据或主动学习上，其中分类器可以根据需要请求额外的标记数据以提高其准确性。
- en: 4.1.2 Feature Tracking
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 特征跟踪
- en: 'Insight: In recent years, researchers have focused on event-based feature tracking
    for its robustness in fast motion capture and extreme lighting conditions  [[30](#bib.bib30),
    [5](#bib.bib5)]. Early event-based feature trackers treat events as point sets
    and use Iterative Closest Point (ICP) [[140](#bib.bib140)] to track features [[141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144)], and there are also
    works use B-splines [[145](#bib.bib145)] and some other techniques to obtain the
    feature trajectories [[5](#bib.bib5), [146](#bib.bib146), [7](#bib.bib7), [147](#bib.bib147),
    [148](#bib.bib148)].'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：近年来，研究人员关注事件驱动特征跟踪，因为其在快速运动捕捉和极端光照条件下的鲁棒性 [[30](#bib.bib30), [5](#bib.bib5)]。早期的事件驱动特征跟踪器将事件视为点集，并使用迭代最近点（ICP） [[140](#bib.bib140)]
    来跟踪特征 [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144)]，也有使用
    B-splines [[145](#bib.bib145)] 和其他技术来获得特征轨迹的研究 [[5](#bib.bib5), [146](#bib.bib146),
    [7](#bib.bib7), [147](#bib.bib147), [148](#bib.bib148)]。
- en: Recently, deep learning-based method, a.k.a., data-driven methods for event-based
    feature tracking method has been proposed. The most representative one is  [[30](#bib.bib30)]
    which serves as the first work of introducing a data-driven feature tracker for
    event cameras, leveraging low-latency events to track features detected in a grayscale
    frame. The data-driven tracker outperforms the existing non-DL-based methods in
    relative feature age by up to 120% while keeping the lowest latency.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于深度学习的方法，即事件驱动特征跟踪的基于数据的方法已被提出。其中最具代表性的是 [[30](#bib.bib30)]，这项工作首次引入了一个针对事件相机的数据驱动特征跟踪器，利用低延迟事件跟踪在灰度帧中检测到的特征。该数据驱动跟踪器在相对特征年龄上比现有的非深度学习方法高出多达
    120%，同时保持最低的延迟。
- en: 'Remarks: From our review, we find that deep learning is just introduced to
    event-based feature tracking recently and it is worth exploring this direction.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '备注: 从我们的审查中，我们发现深度学习刚刚被引入事件驱动的特征跟踪中，这一方向值得进一步探索。'
- en: 'TABLE IX: The quantitative results of the evaluated trackers on the EDS and
    EC dataset are reported in terms of ”Feature Aeg (FA)” of the stable tracks and
    the ”Expected FA”, which is the multiplication of the feature age by the ratio
    of the number of table tracks over the number of initial features. This table
    is from  [[30](#bib.bib30)].'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IX: 评估跟踪器在 EDS 和 EC 数据集上的定量结果以稳定轨迹的“特征年龄 (FA)”和“预期 FA”进行报告，其中“预期 FA”是特征年龄与表中轨迹数量与初始特征数量比率的乘积。该表来源于
    [[30](#bib.bib30)]。'
- en: '| Method | EDS | EC |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | EDS | EC |'
- en: '| Feature Age (FA) | Expected FA | Feature Age (FA) | Expected FA |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 特征年龄 (FA) | 预期 FA | 特征年龄 (FA) | 预期 FA |'
- en: '| ICP [[140](#bib.bib140)] | 0.060 | 0.040 | 0.256 | 0.245 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ICP [[140](#bib.bib140)] | 0.060 | 0.040 | 0.256 | 0.245 |'
- en: '| EM-ICP [[143](#bib.bib143)] | 0.161 | 0.120 | 0.337 | 0.334 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| EM-ICP [[143](#bib.bib143)] | 0.161 | 0.120 | 0.337 | 0.334 |'
- en: '| HASTE [[147](#bib.bib147)] | 0.096 | 0.063 | 0.442 | 0.427 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| HASTE [[147](#bib.bib147)] | 0.096 | 0.063 | 0.442 | 0.427 |'
- en: '| EKLT [[5](#bib.bib5)] | 0.325 | 0.205 | 0.811 | 0.775 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| EKLT [[5](#bib.bib5)] | 0.325 | 0.205 | 0.811 | 0.775 |'
- en: '| DDFT [[30](#bib.bib30)] (zero-shot) | 0.549 | 0.451 | 0.811 | 0.787 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| DDFT [[30](#bib.bib30)] (零样本) | 0.549 | 0.451 | 0.811 | 0.787 |'
- en: '| DDFT [[30](#bib.bib30)] (fine-tuned) | 0.576 | 0.472 | 0.825 | 0.818 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| DDFT [[30](#bib.bib30)] (微调) | 0.576 | 0.472 | 0.825 | 0.818 |'
- en: 4.1.3 Object Detection and Tracking
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 对象检测与跟踪
- en: 'Object Detection: Event cameras bring a new perspective in dealing with the
    challenges in object detection (e.g., motion blur, occlusions, and extreme lighting
    conditions). In reality, the RGB-based detection fails to enable robust perception
    under image corruptions or extreme weather conditions. Meanwhile, auxiliary sensors,
    such as LiDARs, are extremely bulky and costly [[137](#bib.bib137)]. Therefore,
    event-based detectors are introduced to overcome the dilemma, especially in challenging
    visual conditions [[133](#bib.bib133)]. In this work, we divide the event-based
    object detection methods into three categories according to the input data formats
    and data representations, as summarized in Tab. [VIII](#S4.T8 "TABLE VIII ‣ 4.1.1
    Object Classification ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D
    Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks").'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '对象检测：事件相机在处理对象检测中的挑战（如运动模糊、遮挡和极端光照条件）方面带来了新的视角。实际上，基于 RGB 的检测在图像损坏或极端天气条件下无法实现稳健感知。与此同时，辅助传感器如
    LiDAR 体积庞大且昂贵 [[137](#bib.bib137)]。因此，引入事件驱动的检测器以克服这一困境，特别是在具有挑战性的视觉条件下 [[133](#bib.bib133)]。在这项工作中，我们根据输入数据格式和数据表示，将事件驱动的对象检测方法分为三类，如表
    [VIII](#S4.T8 "TABLE VIII ‣ 4.1.1 Object Classification ‣ 4.1 Scene Understanding
    ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks") 中总结的。'
- en: The first category simply converts the raw event data into frame-based images [[22](#bib.bib22),
    [149](#bib.bib149), [131](#bib.bib131), [135](#bib.bib135), [136](#bib.bib136),
    [133](#bib.bib133), [132](#bib.bib132), [150](#bib.bib150)], e.g., the recurrent
    vision transformer (RVT) [[150](#bib.bib150)] which takes 2-channel frames within
    a time duration. However, this kind of method loses the raw spatial-temporal information
    in the event stream. For this reason, event volume and some other formats tailored
    for object detectors are used in the methods of the second category. Some works [[58](#bib.bib58),
    [23](#bib.bib23)] obtain event volumes by taking the linear or convolve kernels
    to integrate the asynchronous events into multiple slices within the equal temporal
    volume.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类方法简单地将原始事件数据转换为基于帧的图像 [[22](#bib.bib22), [149](#bib.bib149), [131](#bib.bib131),
    [135](#bib.bib135), [136](#bib.bib136), [133](#bib.bib133), [132](#bib.bib132),
    [150](#bib.bib150)]，例如，递归视觉变换器（RVT） [[150](#bib.bib150)] 采用时间段内的2通道帧。然而，这种方法会丢失事件流中的原始时空信息。因此，第二类方法中使用了事件体积和一些其他针对目标检测器量身定制的格式。一些工作
    [[58](#bib.bib58), [23](#bib.bib23)] 通过采用线性或卷积核将异步事件整合到相等的时间体积内，从而获得事件体积。
- en: However, the event volume still follows a frame-like 2D representation, and
    critical temporal information is lost. Recently, ASTMNet  [[134](#bib.bib134)]
    exploits the spatial-temporal information by directly processing the asynchronous
    events instead of the 2D frame-like representations. Furthermore, it also serves
    as the first end-to-end pipeline for continuous object detection. Notably, a branch
    of research has introduced temporal hints by integrating recurrent neural network
    layers, resulting in significant enhancements in detection accuracy  [[150](#bib.bib150),
    [23](#bib.bib23), [134](#bib.bib134)]. The third category of attempts combines
    the advantages of event images and RGB images [[135](#bib.bib135), [136](#bib.bib136),
    [151](#bib.bib151), [152](#bib.bib152)]. Tomy et al. [[137](#bib.bib137)] proposed
    a representative framework that fuses the information from the event- and frame-based
    cameras for better detection accuracy in normal conditions and robust performance
    in the presence of extreme scenarios.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，事件体积仍然遵循类似帧的2D表示，且关键信息会丢失。最近，ASTMNet  [[134](#bib.bib134)] 通过直接处理异步事件而不是2D帧状表示来利用时空信息。此外，它也是第一个用于连续目标检测的端到端管道。值得注意的是，一些研究通过整合递归神经网络层引入了时间线索，从而显著提高了检测准确性
    [[150](#bib.bib150), [23](#bib.bib23), [134](#bib.bib134)]。第三类尝试结合了事件图像和RGB图像的优点
    [[135](#bib.bib135), [136](#bib.bib136), [151](#bib.bib151), [152](#bib.bib152)]。Tomy等
    [[137](#bib.bib137)] 提出了一个代表性框架，将事件摄像头和帧摄像头的信息融合，以提高正常情况下的检测准确性和在极端场景中的鲁棒性。
- en: 'Object Tracking: Tracking dynamic objects is an essential task in mobile robots,
    which requires the basic functionality of obstacle avoidance. RGB camera-based
    trackers perform poorly for high-speed and dynamic objects because of motion blur
    and time-delayed transmission. In such cases, introducing event cameras to address
    this problem is of great value.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 目标跟踪：跟踪动态物体是移动机器人中的一项重要任务，它需要障碍物避让的基本功能。基于RGB摄像头的跟踪器对高速和动态物体的表现不佳，因为运动模糊和时间延迟传输。在这种情况下，引入事件摄像头来解决这个问题具有重要价值。
- en: For mobile robots, earlier methods proposed to track moving objects on the conditions
    of geometric priors [[153](#bib.bib153)], known shape [[154](#bib.bib154)], [[154](#bib.bib154)]
    and motion-compensation model [[21](#bib.bib21)], [[155](#bib.bib155)]. More recently,
    many DL-based methods designed for canonical image data have undergone a paradigm
    shift and have been applied successfully to event data. For instance, the widely
    adopted object detector—Yolo and object tracker—Kalman filter, have been applied
    to event data, showing pleasant outcomes [[156](#bib.bib156)]. Many more endeavors
    have been made to enable the onboard inference ability of the deep learning models.
    For instance, EVDodge [[157](#bib.bib157)] specifically decouples the network
    to predict obstacle motion and ego-m otion separately by introducing two event
    cameras facing the ground and front, demonstrating that it outperforms their monocular
    counterparts which use a single event camera. Furthermore, EVReflex  [[158](#bib.bib158)]
    suggests an additional LiDARs sensor instead of an additional event camera exhibiting
    higher accuracy than EVDodge. EV-Catcher [[159](#bib.bib159)] trains a small CNN
    to process single-channel event images, achieving an inference speed of 2ms, which
    is faster than its predecessor by a large margin [[160](#bib.bib160)]. Because
    EV-Catcher only regresses the real-time target position and its uncertainty at
    x-coordinate from the DNNs, further estimation of hitting position and timing
    is based on the linear motion assumptions. Others tend to proceed with this problem
    in an end-to-end manner, showing marginal benefits.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于移动机器人，早期的方法提出了在几何先验条件[[153](#bib.bib153)]、已知形状[[154](#bib.bib154)]、[[154](#bib.bib154)]
    和运动补偿模型[[21](#bib.bib21)]、[[155](#bib.bib155)]下跟踪移动物体。最近，许多为经典图像数据设计的深度学习（DL）方法经历了范式转变，并成功应用于事件数据。例如，广泛采用的目标检测器——Yolo
    和目标跟踪器——卡尔曼滤波器，已被应用于事件数据，并显示出令人满意的结果[[156](#bib.bib156)]。更多的努力被用来提升深度学习模型的车载推理能力。例如，EVDodge
    [[157](#bib.bib157)] 特别地通过引入两个面对地面和前方的事件相机，解耦网络以分别预测障碍物运动和自我运动，展示了它优于其单眼相机的单相机对比方法。此外，EVReflex
    [[158](#bib.bib158)] 提议使用额外的激光雷达传感器，而不是额外的事件相机，展现出比 EVDodge 更高的准确性。EV-Catcher
    [[159](#bib.bib159)] 训练一个小型 CNN 处理单通道事件图像，实现了 2ms 的推理速度，比其前身快了很多[[160](#bib.bib160)]。由于
    EV-Catcher 仅回归 DNNs 的实时目标位置及其 x 坐标的不确定性，进一步的击中位置和时间的估计基于线性运动假设。其他方法倾向于以端到端的方式解决这一问题，显示出边际效益。
- en: 'TABLE X: Experiments of representative methods on event-based optical flow
    estimation from  [[161](#bib.bib161)].'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '表 X: 代表性方法在基于事件的光流估计上的实验来自[[161](#bib.bib161)]。'
- en: 'UL: unsupervised learning. SL: supervised learning. MB: model-based methods.
    ($\cdot$): evaluation on both outdoor_day1 and outdoor_day2 sequences. [$\cdot$]:
    evaluation on outdoor_day2 sequences. N/A means no results are available. The
    results without any brackets mean that they are not trained on any sequence of
    MVSEC.)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'UL: 无监督学习。SL: 监督学习。MB: 基于模型的方法。($\cdot$): 在 outdoor_day1 和 outdoor_day2 序列上的评估。[$\cdot$]:
    在 outdoor_day2 序列上的评估。不适用表示没有结果。没有括号的结果表示没有在 MVSEC 的任何序列上进行训练。'
- en: '| Type | Method | Metric | indoor_flying1 | indoor_flying2 | indoor_flying3
    | outdoor_day1 | Param. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 方法 | 指标 | indoor_flying1 | indoor_flying2 | indoor_flying3 | outdoor_day1
    | 参数 |'
- en: '| EPE | %Out | EPE | %Out | EPE | %Out | EPE | %Out |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| EPE | %丢失 | EPE | %丢失 | EPE | %丢失 | EPE | %丢失 |'
- en: '| UL | Ev-FlowNet [[34](#bib.bib34)] | sparse | (1.03) | (2.2) | (1.72) | (15.1)
    | (1.53) | (11.9) | [0.49] | [0.2] | N/A |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| UL | Ev-FlowNet [[34](#bib.bib34)] | 稀疏 | (1.03) | (2.2) | (1.72) | (15.1)
    | (1.53) | (11.9) | [0.49] | [0.2] | 不适用 |'
- en: '| Zhu et al. [[162](#bib.bib162)] | sparse | (0.58) | (0.0) | (1.02) | (4.0)
    | (0.87) | (3.0) | [0.32] | [0.0] | N/A |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Zhu et al. [[162](#bib.bib162)] | 稀疏 | (0.58) | (0.0) | (1.02) | (4.0) |
    (0.87) | (3.0) | [0.32] | [0.0] | 不适用 |'
- en: '| Matrix-LSTM [[40](#bib.bib40)] | sparse | (0.82) | (0.53) | (1.19) | (5.59)
    | (1.08) | (4.81) | N/A | N/A | N/A |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Matrix-LSTM [[40](#bib.bib40)] | 稀疏 | (0.82) | (0.53) | (1.19) | (5.59) |
    (1.08) | (4.81) | 不适用 | 不适用 | 不适用 |'
- en: '| Spike-FLowNet [[163](#bib.bib163)] | sparse | [0.84] | N/A | [1.28] | N/A
    | [1.11] | N/A | [0.49] | N/A | 13.039M |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Spike-FLowNet [[163](#bib.bib163)] | 稀疏 | [0.84] | 不适用 | [1.28] | 不适用 | [1.11]
    | 不适用 | [0.49] | 不适用 | 13.039M |'
- en: '| Paredes et al. [[92](#bib.bib92)] | sparse | (0.79) | (1.2) | (1.40) | (10.9)
    | (1.18) | (7.4) | [0.92] | [5.4] | N/A |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Paredes et al. [[92](#bib.bib92)] | 稀疏 | (0.79) | (1.2) | (1.40) | (10.9)
    | (1.18) | (7.4) | [0.92] | [5.4] | 不适用 |'
- en: '| LIF-EV-FlowNet [[164](#bib.bib164)] | sparse | 0.71 | 1.41 | 1.44 | 12.75
    | 1.16 | 9.11 | 0.53 | 0.33 | N/A |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| LIF-EV-FlowNet [[164](#bib.bib164)] | 稀疏 | 0.71 | 1.41 | 1.44 | 12.75 | 1.16
    | 9.11 | 0.53 | 0.33 | 不适用 |'
- en: '| Deng et al. [[165](#bib.bib165)] | sparse | (0.89) | (0.66) | (1.31) | (6.44)
    | (1.13) | (3.53) | N/A | N/A | N/A |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Deng et al. [[165](#bib.bib165)] | 稀疏 | (0.89) | (0.66) | (1.31) | (6.44)
    | (1.13) | (3.53) | 不适用 | 不适用 | 不适用 |'
- en: '| Li et al. [[166](#bib.bib166)] | sparse | (0.59) | (0.83) | (0.64) | (2.26)
    | N/A | N/A | [0.31] | [0.03] | N/A |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[166](#bib.bib166)] | 稀疏 | (0.59) | (0.83) | (0.64) | (2.26) |
    不适用 | 不适用 | [0.31] | [0.03] | 不适用 |'
- en: '| STE-FlowNet [[167](#bib.bib167)] | sparse | [0.57] | [0.1] | [0.79] | [1.6]
    | [0.72] | [1.3] | [0.42] | [0.0] | N/A |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| STE-FlowNet [[167](#bib.bib167)] | 稀疏 | [0.57] | [0.1] | [0.79] | [1.6] |
    [0.72] | [1.3] | [0.42] | [0.0] | 不适用 |'
- en: '| SL | Stoffregen et al. [[91](#bib.bib91)] | dense | 0.56 | 1.00 | 0.66 |
    1.00 | 0.59 | 1.00 | 0.68 | 0.99 | N/A |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| SL | Stoffregen et al. [[91](#bib.bib91)] | 密集 | 0.56 | 1.00 | 0.66 | 1.00
    | 0.59 | 1.00 | 0.68 | 0.99 | 不适用 |'
- en: '| EST [[39](#bib.bib39)] | sparse | (0.97) | (0.91) | (1.38) | (8.20) | (1.43)
    | (6.47) | N/A | N/A | N/A |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| EST [[39](#bib.bib39)] | 稀疏 | (0.97) | (0.91) | (1.38) | (8.20) | (1.43)
    | (6.47) | 不适用 | 不适用 | 不适用 |'
- en: '| DCEIFlow [[161](#bib.bib161)] | dense | 0.56 | 0.28 | 0.64 | 0.16 | 0.57
    | 0.12 | 0.91 | 0.71 | N/A |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| DCEIFlow [[161](#bib.bib161)] | 密集 | 0.56 | 0.28 | 0.64 | 0.16 | 0.57 | 0.12
    | 0.91 | 0.71 | 不适用 |'
- en: '| DCEIFlow [[161](#bib.bib161)] | sparse | 0.57 | 0.30 | 0.70 | 0.30 | 0.58
    | 0.15 | 0.74 | 0.29 | N/A |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| DCEIFlow [[161](#bib.bib161)] | 稀疏 | 0.57 | 0.30 | 0.70 | 0.30 | 0.58 | 0.15
    | 0.74 | 0.29 | 不适用 |'
- en: '| MB | Pan et al. [[168](#bib.bib168)] | sparse | 0.93 | 0.48 | 0.93 | 0.48
    | 0.93 | 0.48 | 0.93 | 0.48 | N/A |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| MB | Pan et al. [[168](#bib.bib168)] | 稀疏 | 0.93 | 0.48 | 0.93 | 0.48 | 0.93
    | 0.48 | 0.93 | 0.48 | 不适用 |'
- en: '| Shiba [[169](#bib.bib169)] | sparse | 0.42 | 0.10 | 0.60 | 0.59 | 0.50 |
    0.28 | 0.30 | 0.10 | N/A |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Shiba [[169](#bib.bib169)] | 稀疏 | 0.42 | 0.10 | 0.60 | 0.59 | 0.50 | 0.28
    | 0.30 | 0.10 | 不适用 |'
- en: '| $\text{Fusion-FlowNet}_{Early}$ [[170](#bib.bib170)] | dense | (0.56) | N/A
    | (0.95) | N/A | (0.76) | N/A | [0.59] | N/A | 12.269M |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Fusion-FlowNet}_{Early}$ [[170](#bib.bib170)] | 密集 | (0.56) | 不适用
    | (0.95) | 不适用 | (0.76) | 不适用 | [0.59] | 不适用 | 12.269M |'
- en: '| $\text{Fusion-FlowNet}_{Late}$ [[170](#bib.bib170)] | sparse | (0.57) | N/A
    | (0.99) | N/A | (0.79) | N/A | [0.55] | N/A | 7.549M | ![Refer to caption](img/221954002d9b13c0853cdc1b71b8b042.png)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '| $\text{Fusion-FlowNet}_{Late}$ [[170](#bib.bib170)] | 稀疏 | (0.57) | 不适用 |
    (0.99) | 不适用 | (0.79) | 不适用 | [0.55] | 不适用 | 7.549M | ![参见说明](img/221954002d9b13c0853cdc1b71b8b042.png)'
- en: 'Figure 8: Visualization results of semantic segmentation with events, (a) Events,
    (b) Pseudo Label, (c) Ev-Transfer [[171](#bib.bib171)], (d) Image, (e) ESS [[172](#bib.bib172)],
    (f) E2VID [[90](#bib.bib90)].'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：语义分割的可视化结果，包括 (a) 事件, (b) 伪标签, (c) Ev-Transfer [[171](#bib.bib171)], (d)
    图像, (e) ESS [[172](#bib.bib172)], (f) E2VID [[90](#bib.bib90)]。
- en: 'Remarks: Overall, it is worth exploring how to build effective object detectors
    that leverage event cameras to overcome the deficiency of frame-based detectors
    in extreme visual conditions, e.g., high-speed motion or dark night. Meanwhile,
    it is promising to apply the abundant off-the-shelf DL-based object detectors,
    e.g., recurrent vision transformer [[150](#bib.bib150)], to event cameras. Moreover,
    though event cameras show distinct advantages, RGB-based cameras are still occupied
    by the mainstream object detection tasks. Intuitively, fusing the event and frame
    data could help improve detection accuracy, especially in scenarios where the
    events are temporarily absent, e.g., the static motion scenarios. In object detection
    and tracking, more and more attention is paid to multi-modal sensor fusion. Moreover,
    pure event-based solutions could be explored to exonerate the additional expensive
    sensors, like LiDARs, while achieving comparable or better performance.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：总体而言，值得深入探讨如何构建有效的目标检测器，利用事件相机克服基于帧的检测器在极端视觉条件下的不足，例如高速运动或黑暗夜晚。同时，将现有的基于深度学习的目标检测器（如递归视觉变换器 [[150](#bib.bib150)]）应用于事件相机是有前景的。此外，尽管事件相机展示了明显的优势，但RGB相机仍主导着主流目标检测任务。直观上，将事件和帧数据融合有助于提高检测准确性，尤其是在事件暂时缺失的场景中，例如静态运动场景。在目标检测和跟踪中，多模态传感器融合越来越受到关注。此外，可以探索纯事件基础的解决方案，以免除额外昂贵的传感器，如激光雷达，同时实现可比或更好的性能。
- en: 4.1.4 Semantic Segmentation
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 语义分割
- en: Image segmentation [[173](#bib.bib173)] is a fundamental vision task with many
    pivotal applications [[174](#bib.bib174), [175](#bib.bib175), [176](#bib.bib176)],
    including robotic perception, scene understanding, augmented reality, etc. In
    these practical scenarios, the segmentation models always fail in the non-ideal
    weather and lighting conditions [[4](#bib.bib4)], leading to poor scene perception
    of intelligent systems. Event-based semantic segmentation, which is first proposed
    in Ev-SegNet [[24](#bib.bib24)], achieves a significant improvement by utilizing
    the asynchronous event data. Ev-SegNet also introduces a dataset extended from
    the DDD17 dataset [[177](#bib.bib177)]. However, the resolution and image quality
    is less satisfactory for the semantic segmentation task.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割[[173](#bib.bib173)]是一项基础的视觉任务，具有许多关键应用[[174](#bib.bib174)、[175](#bib.bib175)、[176](#bib.bib176)]，包括机器人感知、场景理解、增强现实等。在这些实际场景中，分割模型常常在非理想的天气和光照条件下失败[[4](#bib.bib4)]，导致智能系统的场景感知效果不佳。基于事件的语义分割，首次在Ev-SegNet[[24](#bib.bib24)]中提出，通过利用异步事件数据实现了显著的改进。Ev-SegNet还引入了从DDD17数据集[[177](#bib.bib177)]扩展的数据集。然而，分割任务的分辨率和图像质量仍然不令人满意。
- en: To address this problem, Gehrig et al. [[25](#bib.bib25)] proposed to convert
    video data to synthetic events. This work unlocks the usage of a large number
    of existing video datasets for event-based semantic segmentation. Inspired by
    this synthetic data source, Wang et al. [[178](#bib.bib178), [179](#bib.bib179)]
    suggested combining the labeled RGB data and unlabeled event data in a cross-modal
    knowledge distillation setting, so as to alleviate the shortage of labeled real
    event data. For higher segmentation results, Zhang et al. [[4](#bib.bib4)] constructed
    a multi-modal segmentation benchmark model by using the complementary information
    in both event and RGB branches. More recently, ESS [[172](#bib.bib172)] proposes
    an unsupervised domain adaptation (UDA) framework that leverages the still images
    without paired events and frames.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Gehrig等人[[25](#bib.bib25)]提出将视频数据转换为合成事件。这项工作解锁了大量现有视频数据集在基于事件的语义分割中的使用。受到这一合成数据源的启发，Wang等人[[178](#bib.bib178)、[179](#bib.bib179)]建议在跨模态知识蒸馏设置中结合有标签的RGB数据和无标签的事件数据，以缓解标注真实事件数据的短缺。为了获得更高的分割结果，Zhang等人[[4](#bib.bib4)]通过利用事件和RGB分支中的互补信息，构建了一个多模态分割基准模型。最近，ESS[[172](#bib.bib172)]提出了一个无监督领域适应（UDA）框架，该框架利用没有配对事件和帧的静态图像。
- en: 'Remarks: Due to the lack of precisely annotated large-scale real-world event
    datasets, existing works mostly focus on generating pseudo labels. However, the
    labels are not precise enough, rendering the learned segmentation models less
    robust, as demonstrated by the visual results in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.3
    Object Detection and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks"). Future work could further explore the multi-modal domain adaption
    from RGB data to event data for semantic segmentation.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '备注：由于缺乏精确注释的大规模真实事件数据集，现有工作大多集中于生成伪标签。然而，这些标签的精确度不足，使得学习到的分割模型不够稳健，如图[8](#S4.F8
    "Figure 8 ‣ 4.1.3 Object Detection and Tracking ‣ 4.1 Scene Understanding ‣ 4
    Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")中的视觉结果所示。未来的工作可以进一步探索从RGB数据到事件数据的多模态领域适应，以提高语义分割的效果。'
- en: 4.1.5 Optical Flow Estimation
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 光流估计
- en: 'Insight: Optical flow estimation is the process of estimating the motion field
    within an image sequence. Conventional RGB-based methods remain unsatisfying in
    extreme lighting conditions, e.g., at night and in high-speed motion. To overcome
    these limitations, event cameras have been introduced. The SOTA event-based methods
    for optical flow estimation can be classified into two categories: traditional
    methods and DL-based methods. DL-based methods further encompass supervised and
    unsupervised approaches. Table. [X](#S4.T10 "TABLE X ‣ 4.1.3 Object Detection
    and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣
    Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")
    presents the results achieved by several representative methods in the field.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '见解：光流估计是估算图像序列中的运动场的过程。传统的基于RGB的方法在极端光照条件下仍然表现不佳，例如在夜间和高速运动中。为克服这些限制，引入了事件相机。光流估计的最先进事件基础方法可以分为两类：传统方法和基于深度学习（DL）的方法。基于DL的方法进一步包括监督和无监督方法。表格 [X](#S4.T10
    "TABLE X ‣ 4.1.3 Object Detection and Tracking ‣ 4.1 Scene Understanding ‣ 4 Scene
    Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")展示了该领域几种代表性方法所取得的结果。'
- en: '![Refer to caption](img/93a78497cba654a26a4ab97183c52846.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/93a78497cba654a26a4ab97183c52846.png)'
- en: 'Figure 9: Representative optical flow estimation methods, including supervised
    methods [[39](#bib.bib39), [44](#bib.bib44), [33](#bib.bib33)], (a) Correlation-based
    methods, (b) Multi-task learning methods, and unsupervised learning methods [[162](#bib.bib162),
    [51](#bib.bib51)] (c) Multi-task learning methods, (d) self-supervised learning
    methods, and (e) SNN-based methods.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：代表性的光流估计方法，包括监督方法 [[39](#bib.bib39), [44](#bib.bib44), [33](#bib.bib33)]，（a）基于相关的方法，（b）多任务学习方法，以及无监督学习方法 [[162](#bib.bib162),
    [51](#bib.bib51)]，（c）多任务学习方法，（d）自监督学习方法，以及（e）基于SNN的方法。
- en: 'Traditional methods: Recent research has delved into understanding the principles
    and characteristics of event data that facilitate the estimation process. These
    studies have particularly focused on leveraging contrast maximization methods
    to estimate optical flow accurately [[180](#bib.bib180), [181](#bib.bib181), [169](#bib.bib169)].
    Furthermore, there is ongoing research aimed at designing innovative event camera
    platforms specifically tailored for the hardware implementation of adaptive block-matching
    optical flow. These platforms serve as practical demonstrations of the effectiveness
    of this approach [[61](#bib.bib61)].'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法：近期的研究深入探讨了事件数据的原理和特征，这些数据有助于估计过程。这些研究特别关注利用对比度最大化方法来准确估计光流 [[180](#bib.bib180),
    [181](#bib.bib181), [169](#bib.bib169)]。此外，还有针对设计创新事件相机平台的持续研究，这些平台专门针对自适应块匹配光流的硬件实现。这些平台作为这种方法有效性的实际展示 [[61](#bib.bib61)]。
- en: 'Supervised methods: In  [[39](#bib.bib39), [44](#bib.bib44), [33](#bib.bib33)],
    event streams are first converted into image-based or surface-based representations
    and then trained via standard convolutional neural networks (CNNs). Kepple et
    al.[[182](#bib.bib182)] proposed to simultaneously generate the region’s local
    flow and the reliability of the prediction. Gehrig et al.[[114](#bib.bib114)]
    proposed an RNN-based framework that utilizes the cost volumes and learns the
    feature correlation of the volumetric voxel grid of events, so as to estimate
    optical ﬂow.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 监督方法：在 [[39](#bib.bib39), [44](#bib.bib44), [33](#bib.bib33)]中，事件流首先被转换为基于图像或基于表面的表示，然后通过标准卷积神经网络（CNN）进行训练。Kepple
    等人[[182](#bib.bib182)] 提出了同时生成区域的局部流和预测的可靠性。Gehrig 等人[[114](#bib.bib114)] 提出了一个基于RNN的框架，该框架利用成本体积并学习事件体素网格的特征相关性，以估计光流。
- en: Some research employs SNNs for optical flow estimation [[183](#bib.bib183),
    [184](#bib.bib184), [163](#bib.bib163)]. For example, Haessig et al. [[183](#bib.bib183)]
    introduced an SNN variant of the Barlow and Levick model utilizing IBM’s TrueNorth
    Neurosynaptic System. However, deep SNNs suffer from spike vanishing problems.
    To this end, [[163](#bib.bib163)] combined SNNs and CNNs in an end-to-end manner
    to estimate optical ﬂow, while [[184](#bib.bib184)] proposed a hierarchical SNN
    architecture for feature extraction and local and global motion perception. Future
    research could explore combining SNNs and transformer [[152](#bib.bib152)] to
    learn the global and local visual information from events.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究利用 SNNs 进行光流估计 [[183](#bib.bib183), [184](#bib.bib184), [163](#bib.bib163)]。例如，Haessig
    等人 [[183](#bib.bib183)] 介绍了一种使用 IBM 的 TrueNorth 神经突触系统的 Barlow 和 Levick 模型的 SNN
    变体。然而，深度 SNNs 遭遇了脉冲消失问题。为此，[[163](#bib.bib163)] 将 SNNs 和 CNNs 以端到端的方式结合在一起估计光流，而
    [[184](#bib.bib184)] 提出了用于特征提取和局部及全局运动感知的分层 SNN 架构。未来的研究可以探索将 SNNs 和 transformer
    [[152](#bib.bib152)] 结合起来，以从事件中学习全局和局部视觉信息。
- en: 'Unsupervised methods: Recent research is focused on unsupervised learning for
    solving the data scarcity problem. Zhu et al. [[162](#bib.bib162)] introduced
    a novel event representation containing two channels for encoding the number of
    positive and negative events and two for the timestamp of the most recent positive
    and negative events. They utilized the grayscale, i.e., APS, images of the event
    camera as the self-supervision signals to train the network. Ye et al. [[51](#bib.bib51)]
    simultaneously predicted the dense depth and optical flow with two corresponding
    neural networks. With the guidance of depth maps, the optical flow is calculated
    based on the poses of neighboring frames and the depth of the middle frame. However,
    these methods are still based on the photo consistency principle, while this assumption
    may not be valid in some adverse visual conditions (e.g., high-speed motion).
    To this end, Zhu et al.[[162](#bib.bib162)] proposed a discretized volumetric
    event representation to maintain the events’ temporal distribution, and the input
    processed event data is used to predict the motions and remove the motion blur.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督方法：近期研究集中在无监督学习以解决数据稀缺问题。Zhu 等人 [[162](#bib.bib162)] 介绍了一种新型事件表示，包含两个通道用于编码正负事件的数量，以及两个通道用于编码最近正负事件的时间戳。他们利用灰度图像（即
    APS 图像）作为自监督信号来训练网络。Ye 等人 [[51](#bib.bib51)] 使用两个相应的神经网络同时预测密集深度和光流。在深度图的指导下，光流是基于邻近帧的姿态和中间帧的深度来计算的。然而，这些方法仍然基于光度一致性原则，而在某些不利的视觉条件下（例如高速运动）这一假设可能不成立。为此，Zhu
    等人 [[162](#bib.bib162)] 提出了离散化的体积事件表示，以保持事件的时间分布，处理后的事件数据用于预测运动并去除运动模糊。
- en: 'Remarks: While optical flow may not have standalone usefulness, it serves as
    a valuable tool in driving other computations or closed-loop control in various
    computer vision tasks. Its ability to estimate motion patterns provides crucial
    information for tasks like object tracking, visual odometry, video stabilization,
    action recognition, and motion-based segmentation.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：虽然光流可能没有独立的实用性，但它在驱动其他计算或计算机视觉任务中的闭环控制方面是一个宝贵的工具。它估计运动模式的能力为目标跟踪、视觉里程计、视频稳定、动作识别和基于运动的分割等任务提供了关键的信息。
- en: 4.1.6 Depth estimation
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6 深度估计
- en: '![Refer to caption](img/bcbc59735cbdb2cc7de3fbd38e920980.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bcbc59735cbdb2cc7de3fbd38e920980.png)'
- en: 'Figure 10: Event-based depth estimation methods, including (a) Monocular event
    only method [[185](#bib.bib185)], (b) Monocular event-frame based method  [[186](#bib.bib186)],
    (c) Stereo event only method [[187](#bib.bib187), [188](#bib.bib188)], (d) Stereo
    event-frame based method  [[189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)].'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：基于事件的深度估计方法，包括 (a) 单目事件方法 [[185](#bib.bib185)]，(b) 单目事件-帧基方法 [[186](#bib.bib186)]，(c)
    立体事件方法 [[187](#bib.bib187), [188](#bib.bib188)]，(d) 立体事件-帧基方法 [[189](#bib.bib189),
    [190](#bib.bib190), [191](#bib.bib191)]。
- en: 'Insight : Events streams reflect abundant edge information, HDR, and high temporal
    resolution, benefiting depth estimation tasks, especially in extreme conditions.
    Depth can be learned from either the monocular (single) input or stereo (multi-view
    of a scene) inputs. Under this outline, we categorize the depth estimation methods
    based on how events are used and learned.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：事件流反映了丰富的边缘信息、高动态范围和高时间分辨率，这对深度估计任务特别是在极端条件下大有裨益。深度可以从单目（单个）输入或立体（场景的多视图）输入中学习。在这个框架下，我们根据事件的使用和学习方式对深度估计方法进行了分类。
- en: 'Monocular depth estimation: Based on whether events are used alone or combined
    with the intensity frames, we divide the monocular depth estimation methods into
    two types. 1) Events-only approaches: [[185](#bib.bib185)] is a representative
    approach that adopts a recurrent network [[64](#bib.bib64)] to learn the temporal
    information from grid-like event inputs, depicted in Fig. [10](#S4.F10 "Figure
    10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (a). However, as monocular depth estimation from events is an ill-posed
    problem, rendering such a learning framework difficult to achieve highly precise
    depth. Moreover, this learning paradigm may fail to predict depth in the static
    scene as events are only triggered by motion. 2) event-plus-frame approaches:
    RAM [[186](#bib.bib186)] employs the same RNN as [[185](#bib.bib185)] but combines
    events and frames (i.e., as complementary to each other) to learn to predict depth
    from the multi-modal inputs asynchronously, as shown in Fig. [10](#S4.F10 "Figure
    10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (b). Nonetheless, the recurrent network is inevitably accompanied
    by long-term memory costs.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '单目深度估计：根据事件是否单独使用或与强度帧结合，我们将单目深度估计方法分为两类。1) 仅事件方法：[[185](#bib.bib185)] 是一种采用递归网络
    [[64](#bib.bib64)] 从网格状事件输入中学习时间信息的代表性方法，如图 [10](#S4.F10 "Figure 10 ‣ 4.1.6 Depth
    estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks") (a) 所示。然而，由于从事件中进行单目深度估计是一个不适定的问题，这使得这样的学习框架难以实现高精度的深度估计。此外，由于事件仅在运动时被触发，这种学习范式可能无法在静态场景中预测深度。2)
    事件加帧方法：RAM [[186](#bib.bib186)] 采用与 [[185](#bib.bib185)] 相同的 RNN，但结合事件和帧（即彼此互补），以异步方式从多模态输入中学习预测深度，如图
    [10](#S4.F10 "Figure 10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4
    Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks") (b) 所示。然而，递归网络不可避免地伴随着长期的记忆成本。'
- en: 'Stereo depth estimation: As the visual cues of the left and right event cameras
    are used in the stereo setting, the model complexity and memory cost of learning
    pipelines become more prohibitive. [[187](#bib.bib187), [188](#bib.bib188)] are
    two pioneering works in stereo depth estimation, as shown in Fig. [10](#S4.F10
    "Figure 10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding
    and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and
    Benchmarks") (c). In particular, DDES [[187](#bib.bib187)] is the first learning-based
    stereo-matching method, and in  [[188](#bib.bib188)], the first unsupervised learning
    framework is proposed. Both methods store events at each position as a First-in
    First-out queue, enabling concurrent time and polarity reservation. To adaptively
    extract features from sparse data, Zhang et al. [[192](#bib.bib192)] proposed
    continuous time convolution and discrete time convolution to encode high dimensional
    spatial-temporal event data.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '立体深度估计：由于在立体设置中使用了左、右事件相机的视觉线索，因此学习管道的模型复杂性和内存成本变得更加高昂。[[187](#bib.bib187),
    [188](#bib.bib188)] 是两个在立体深度估计领域的开创性工作，如图 [10](#S4.F10 "Figure 10 ‣ 4.1.6 Depth
    estimation ‣ 4.1 Scene Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep
    Learning for Event-based Vision: A Comprehensive Survey and Benchmarks") (c) 所示。特别地，DDES
    [[187](#bib.bib187)] 是首个基于学习的立体匹配方法，而 [[188](#bib.bib188)] 提出了第一个无监督学习框架。这两种方法将事件存储在每个位置作为先进先出队列，实现了并发的时间和极性保留。为了从稀疏数据中自适应地提取特征，Zhang
    等 [[192](#bib.bib192)] 提出了连续时间卷积和离散时间卷积，以编码高维的时空事件数据。'
- en: 'By contrast, some research explores multi-modality fusion under different settings
    which serves as a remedy to utilize the benefits of each modality. HDES [[193](#bib.bib193)]
    mitigates the modal-gap between the data from different viewpoints by introducing
    a hybrid pyramid attention module for multi-modal data fusion. EIS [[189](#bib.bib189)]
    is a representative work to combine events and frames with a recycling network,
    as depicted in Fig. [10](#S4.F10 "Figure 10 ‣ 4.1.6 Depth estimation ‣ 4.1 Scene
    Understanding ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks") (d). However, since events are
    sparse, event stacking is an important factor that affects the quality of fusion
    and depth prediction because stacking inappropriate amounts of events can lead
    to information overriding or missing problems. To this end,  [[190](#bib.bib190),
    [191](#bib.bib191)] propose a selection module to filter more useful events. Specifically,
    Nam et al. [[190](#bib.bib190)] concatenated the event stacks with different densities
    and then adaptively learn these stacks to highlight the contribution of the well-stacked
    events. Moreover, considering the constant motion of the cameras, SCSNet [[191](#bib.bib191)]
    introduces a differentiable event selection network to extract more reliable events
    and correlate the feature from a neighbor region of events and images, diminishing
    the disruption of bad alignment intuitively.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，一些研究探讨了在不同设置下的多模态融合，以利用每种模态的优点。HDES[[193](#bib.bib193)]通过引入混合金字塔注意力模块来减轻来自不同视点的数据之间的模态差距，从而实现多模态数据融合。EIS[[189](#bib.bib189)]是一个结合事件和帧的代表性工作，采用了回收网络，如图[10](#S4.F10
    "图 10 ‣ 4.1.6 深度估计 ‣ 4.1 场景理解 ‣ 4 场景理解和3D视觉 ‣ 基于事件的视觉深度学习：全面调查与基准")（d）所示。然而，由于事件是稀疏的，事件堆叠是影响融合和深度预测质量的重要因素，因为堆叠不适当的事件量可能导致信息覆盖或丢失问题。为此，[[190](#bib.bib190)、[191](#bib.bib191)]提出了选择模块，以筛选更多有用的事件。具体而言，Nam等[[190](#bib.bib190)]将具有不同密度的事件堆叠在一起，然后自适应地学习这些堆叠，以突出表现良好的事件的贡献。此外，考虑到摄像机的持续运动，SCSNet[[191](#bib.bib191)]引入了可微分事件选择网络，以提取更可靠的事件，并将特征与事件和图像的邻域区域相关联，从而直观地减少不良对齐的干扰。
- en: 'Remarks: From our review, inter-camera spatial correlation is the key to content
    matching between events and frames. Although [[189](#bib.bib189), [190](#bib.bib190),
    [191](#bib.bib191)] combine events and frames, it still deserves exploring which
    part events contribute most to the multi-modal feature fusion and alignment. Also,
    it is possible to use an event camera and a frame-based camera for stereo depth
    estimation. Future research could consider exploring these directions.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：从我们的审查中来看，摄像机间的空间相关性是事件与帧之间内容匹配的关键。尽管[[189](#bib.bib189)、[190](#bib.bib190)、[191](#bib.bib191)]将事件与帧结合起来，但仍然值得探讨事件在多模态特征融合和对齐中贡献的部分。此外，使用事件摄像机和基于帧的摄像机进行立体深度估计也是可能的。未来的研究可以考虑探索这些方向。
- en: 4.2 3D Vision
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 3D视觉
- en: 4.2.1 Visual SLAM
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 视觉SLAM
- en: 'Insight: It is an essential module for various applications, e.g., robotic
    navigation and virtual reality. Visual SLAM receives the signals, e.g., 2D images,
    as the source for ego-motion estimation and builds 3D maps, which can generally
    be defined as the tracking thread and the mapping thread. Event-based visual SLAM
    shares a similar spirit and benefits from the robustness of event cameras to light-changing
    and fast-moving conditions.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：这是各种应用中的一个关键模块，例如机器人导航和虚拟现实。视觉SLAM接收信号，例如2D图像，作为自我运动估计的来源，并构建3D地图，这通常可以定义为跟踪线程和映射线程。基于事件的视觉SLAM具有类似的精神，并从事件摄像机对光照变化和快速移动条件的鲁棒性中获益。
- en: '![Refer to caption](img/4cc32294b8a129e05a3b2f11a3c472af.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4cc32294b8a129e05a3b2f11a3c472af.png)'
- en: 'Figure 11: (Left) Illustration of the general framework for event-based SLAM
    via deep learning, key elements are retrieved from [[194](#bib.bib194), [195](#bib.bib195),
    [51](#bib.bib51), [162](#bib.bib162)]. The colored point cloud is a map reconstructed
    from event data of different sensor views (green trajectory) grounded by robot
    location (red directional trajectory). (Right) Visualization of 3D reconstruction
    from (a) blurred RGB images and (b) events from  [[196](#bib.bib196)].'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：（左）通过深度学习的事件基础SLAM的通用框架示意图，关键元素摘自[[194](#bib.bib194)、[195](#bib.bib195)、[51](#bib.bib51)、[162](#bib.bib162)]。彩色点云是从不同传感器视角（绿色轨迹）重建的事件数据地图，以机器人位置（红色方向轨迹）为基准。（右）从（a）模糊RGB图像和（b）事件中的3D重建可视化，来自[[196](#bib.bib196)]。
- en: In traditional SLAM, depth and ego-motion could be easily estimated through
    triangulation and the subsequent local pose estimation by, e.g., Perspective-N-Points.
    Recently, the learning-based approaches are also been applied in SLAM, such as,
    [[51](#bib.bib51), [162](#bib.bib162)] propose united frameworks—with an encoder-decoder
    structure—for optical flow, depth, and ego-motion estimation.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的SLAM中，可以通过三角测量和随后的局部姿态估计（例如，通过Perspective-N-Points）来轻松估计深度和自我运动。近年来，学习-based方法也被应用于SLAM，例如，[[51](#bib.bib51)、[162](#bib.bib162)]提出了统一框架——采用编码器-解码器结构——用于光流、深度和自我运动的估计。
- en: In particular, in [[51](#bib.bib51)], a united framework is proposed to estimate
    sparse optical flow, depths, and ego-motion, in which an encoder-decoder structure
    was adopted for sparse depth estimation, The key idea behind this framework is
    that the maximization of event frame contrast through optical flow warping provides
    natural, high-quality edge maps and enables applying a multi-view stereo loss
    to learn metric poses and depth. Two sub-networks under this framework are trained
    for the prediction of optical flow and depths, respectively.By contrast, Zhu et
    al.[[162](#bib.bib162)] proposed to directly learn 6-DOF poses from multi-view
    intensity frames, other than deriving from optical flow and depth, like [[51](#bib.bib51)].
    The recent EAGAN [[197](#bib.bib197)] adopts the vision transformer  [[198](#bib.bib198)]
    to boost the accuracy of optical flow estimation, yet no optimization is done
    to depth estimation. However, EAGAN shows an increase in the number of learnable
    parameters compared to the methods [[162](#bib.bib162), [51](#bib.bib51)], and
    the ego-motion estimation is not considered.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在[[51](#bib.bib51)]中，提出了一个统一框架来估计稀疏光流、深度和自我运动，其中采用了编码器-解码器结构来进行稀疏深度估计。这个框架的核心思想是，通过光流变形最大化事件帧对比度，可以提供自然的高质量边缘图，并使得应用多视角立体损失以学习度量姿态和深度成为可能。该框架下的两个子网络分别用于光流和深度的预测。相比之下，Zhu
    等人[[162](#bib.bib162)]提出了直接从多视角强度帧中学习6自由度姿态的方法，而不是像[[51](#bib.bib51)]那样从光流和深度中推导。近期的EAGAN
    [[197](#bib.bib197)]采用了视觉变换器[[198](#bib.bib198)]来提升光流估计的准确性，但对深度估计没有进行优化。然而，EAGAN在学习参数的数量上相较于[[162](#bib.bib162)、[51](#bib.bib51)]方法有所增加，并且未考虑自我运动估计。
- en: Moreover, a branch of research casts event-based SLAM as a re-localization problem.
    This paradigm proposes to directly learn the camera poses from extracted deep
    features in an end-to-end trainable manner. For example, Nguyen et al.[[199](#bib.bib199)]
    proposed a framework, with CNNs and four LSTM blocks after fully-connected layers
    of PoseNet [[200](#bib.bib200)], to regress the 6-DOF poses from event images
    directly. This, for the first time, reveals the potential of event data to address
    the large-scale re-localization problem. Later on, additional denoising modules
    are introduced in [[201](#bib.bib201)] to further increase the pose estimation
    accuracy.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一类研究将基于事件的SLAM视为重新定位问题。这种范式提出直接从提取的深度特征中以端到端的可训练方式学习相机姿态。例如，Nguyen 等人[[199](#bib.bib199)]提出了一个框架，结合了CNN和在PoseNet
    [[200](#bib.bib200)]完全连接层之后的四个LSTM块，以直接从事件图像中回归6自由度姿态。这首次揭示了事件数据在解决大规模重新定位问题上的潜力。随后，在[[201](#bib.bib201)]中引入了额外的去噪模块，以进一步提高姿态估计的准确性。
- en: 'Remarks: Currently, the event-based SLAM systems employing deep learning are
    generally decoupled as separate modules rather than the cross-event frame and
    pose-map joint estimation as traditionally processed in visual SLAM. Some attempts
    [[162](#bib.bib162), [51](#bib.bib51)] generate frame association (as a form of
    optical flow), pose estimation, and depth map with a unified network architecture.
    Still, global consistency remains an unsolved problem. For the front end, applying
    DNNs for the intra-frame association purpose is a challenging problem. This leaves
    a vacancy for exploration since many frame-based SLAM systems have proved that
    the pose derived by cross-frame feature association is highly precise. Another
    interesting direction to investigate is the dense map or mesh reconstruction from
    event data that served as mapping for SLAM. we have seen mesh reconstruction with
    very limited precision and designed for small-size objects in Sec.[XI](#S4.T11
    "TABLE XI ‣ 4.2.2 3D Reconstruction ‣ 4.2 3D Vision ‣ 4 Scene Understanding and
    3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    leaving learning-based scene reconstruction an open problem.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '备注：目前，采用深度学习的事件驱动 SLAM 系统通常被解耦为独立模块，而不是传统视觉 SLAM 中的跨事件帧和姿态图联合估计。一些尝试 [[162](#bib.bib162),
    [51](#bib.bib51)] 生成帧关联（作为光流的一种形式）、姿态估计和深度图，使用统一的网络架构。然而，全球一致性仍然是一个未解决的问题。对于前端，将
    DNN 应用于帧内关联目的是一项具有挑战性的问题。这留下了探索的空间，因为许多基于帧的 SLAM 系统已经证明，跨帧特征关联得到的姿态是非常精确的。另一个有趣的研究方向是从事件数据中进行稠密地图或网格重建，这些数据用作
    SLAM 的映射。我们已经看到网格重建的精度非常有限，且设计用于小尺寸物体（见 Sec.[XI](#S4.T11 "TABLE XI ‣ 4.2.2 3D
    Reconstruction ‣ 4.2 3D Vision ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks")），因此基于学习的场景重建仍然是一个未解决的问题。'
- en: 4.2.2 3D Reconstruction
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 3D 重建
- en: 'TABLE XI: Comparison of existing representative event-based 3D reconstruction
    methods.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XI：现有代表性事件驱动 3D 重建方法的比较。
- en: '| Publications | Methods | Task | Representations | Frame Input | Real Time
    | Multi Cameras |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 发表文献 | 方法 | 任务 | 表征 | 帧输入 | 实时 | 多摄像头 |'
- en: '| Arxiv 2020 | E3D [[196](#bib.bib196)] | 3D Reconstruction | Image-based |
    ✓ | ✗ | ✗ |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Arxiv 2020 | E3D [[196](#bib.bib196)] | 3D 重建 | 基于图像 | ✓ | ✗ | ✗ |'
- en: '| ECCV 2020 | Stereo-event PTV [[202](#bib.bib202)] | 3D Fluid Flow Reconstruction
    | Stream-based | ✗ | ✗ | ✓ |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2020 | Stereo-event PTV [[202](#bib.bib202)] | 3D 流体流动重建 | 基于流 | ✗ |
    ✗ | ✓ |'
- en: '| ICCV 2021 | EventHands [[203](#bib.bib203)] | 3D Hand Pose Estimation | Surface-based
    | ✗ | ✓ | ✗ |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2021 | EventHands [[203](#bib.bib203)] | 3D 手部姿态估计 | 基于表面 | ✗ | ✓ |
    ✗ |'
- en: '| ECCV 2022 | EvAC3D [[204](#bib.bib204)] | 3D Reconstruction | Surface-based
    | ✗ | ✗ | ✗ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2022 | EvAC3D [[204](#bib.bib204)] | 3D 重建 | 基于表面 | ✗ | ✗ | ✗ |'
- en: 'Insight: Event cameras capture dominant scene features, e.g., edges and silhouettes,
    making them more suitable for some 3D reconstruction methods than the frame-based
    data (See Fig. [11](#S4.F11 "Figure 11 ‣ 4.2.1 Visual SLAM ‣ 4.2 3D Vision ‣ 4
    Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '见解：事件相机捕捉到主要的场景特征，例如边缘和轮廓，使得它们比基于帧的数据更适合某些 3D 重建方法（见图 [11](#S4.F11 "Figure
    11 ‣ 4.2.1 Visual SLAM ‣ 4.2 3D Vision ‣ 4 Scene Understanding and 3D Vision ‣
    Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")）。'
- en: Unlike the traditional frame-based RGB and depth cameras widely explored in
    3D reconstruction [[205](#bib.bib205)], event cameras enjoy inherent benefits,
    such as low latency and HDR [[206](#bib.bib206), [207](#bib.bib207)]. Intuitively,
    approaches designed for prior RGB and depth data can not be directly applied to
    event data due to the distinct data format. Thus, endeavors have been made in
    converting events to sparse, semi-dense point clouds and full-frame depth maps
    for existing RGB-based pipelines [[202](#bib.bib202), [208](#bib.bib208), [209](#bib.bib209),
    [206](#bib.bib206), [186](#bib.bib186)]. These methods are committed to take advantages
    of the off-the-shelf 3D reconstruction pipelines built for RGB-based inputs while
    ignoring the unique strengths of event cameras. In some following research [[14](#bib.bib14),
    [196](#bib.bib196)], event cameras are combined with RGB cameras for the advantages
    of both sensors. For instance, Vidal et al. [[14](#bib.bib14)] proposed to simultaneously
    incorporate event data, intensity images, and inertial measurement unit (IMU)
    data in the SLAM pipeline for achieving superior accuracy than the purely event-based
    methods. Besides, some approaches combine event cameras with other types of sensors,
    such as ELS [[210](#bib.bib210)] that uses a laser point-projector and an event
    camera.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的基于帧的RGB和深度相机在三维重建中广泛探索[[205](#bib.bib205)]不同，事件相机享有固有的优势，如低延迟和HDR[[206](#bib.bib206),
    [207](#bib.bib207)]。直观上，由于数据格式的不同，专为之前的RGB和深度数据设计的方法不能直接应用于事件数据。因此，已经有研究致力于将事件转换为稀疏、半密集的点云和全帧深度图，以适应现有的基于RGB的流程[[202](#bib.bib202),
    [208](#bib.bib208), [209](#bib.bib209), [206](#bib.bib206), [186](#bib.bib186)]。这些方法致力于利用为RGB输入构建的现成三维重建流程，同时忽略了事件相机的独特优势。在一些后续研究中[[14](#bib.bib14),
    [196](#bib.bib196)]，事件相机与RGB相机结合，以利用两种传感器的优势。例如，Vidal等人[[14](#bib.bib14)]提出在SLAM流程中同时结合事件数据、强度图像和惯性测量单元（IMU）数据，以实现比纯事件方法更高的准确性。此外，还有一些方法将事件相机与其他类型的传感器结合，例如使用激光点投影仪和事件相机的ELS[[210](#bib.bib210)]。
- en: 'To directly take advantage of the event data, Wang et al. [[211](#bib.bib211)]
    proposed to generate dimensional flow from the events instead of using the image-based
    reconstruction method [[212](#bib.bib212)] for 3D fluid flow reconstruction. More
    recently, EvAC3D [[204](#bib.bib204)] explores the direct reconstruction of mesh
    from a continuous stream of events while defining the boundaries of the objects
    as apparent contour events and continuously carving out high-fidelity meshes.
    The comparison of existing representative event-based 3D reconstruction methods
    is shown in Tab. [XI](#S4.T11 "TABLE XI ‣ 4.2.2 3D Reconstruction ‣ 4.2 3D Vision
    ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks").'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '为了直接利用事件数据，Wang等人[[211](#bib.bib211)]提出了从事件生成维度流的方法，而不是使用基于图像的重建方法[[212](#bib.bib212)]来进行三维流体流动重建。最近，EvAC3D[[204](#bib.bib204)]探索了从连续事件流中直接重建网格的方法，同时将物体的边界定义为显著轮廓事件，并持续雕刻出高保真度的网格。现有代表性的基于事件的三维重建方法的比较见于表[XI](#S4.T11
    "TABLE XI ‣ 4.2.2 3D Reconstruction ‣ 4.2 3D Vision ‣ 4 Scene Understanding and
    3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")。'
- en: 'Remarks: Recent research provides important insights regarding how events can
    be utilized to understand the 3D world. However, a general and unified pipeline
    is expected, which is left for future research. The fusion of the RGB and event
    cameras is also valuable for achieving better reconstruction results.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：最近的研究提供了有关如何利用事件来理解三维世界的重要见解。然而，期望能够有一个通用的统一流程，这留待未来的研究。将RGB和事件相机融合也有助于实现更好的重建结果。
- en: 4.2.3 3D Human Pose and Shape Estimation
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 三维人体姿态和形状估计
- en: 'Insight: The ability to capture the dynamic motion makes event cameras superior
    for estimating 3D moving objects, especially for 3D human pose and shape estimation
    (3D HPE) (see Fig. [12](#S4.F12 "Figure 12 ‣ 4.2.3 3D Human Pose and Shape Estimation
    ‣ 4.2 3D Vision ‣ 4 Scene Understanding and 3D Vision ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks")).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '见解：捕捉动态运动的能力使事件相机在估计三维移动物体方面优于其他设备，特别是在三维人体姿态和形状估计（3D HPE）方面（见图[12](#S4.F12
    "Figure 12 ‣ 4.2.3 3D Human Pose and Shape Estimation ‣ 4.2 3D Vision ‣ 4 Scene
    Understanding and 3D Vision ‣ Deep Learning for Event-based Vision: A Comprehensive
    Survey and Benchmarks")）。'
- en: In the past few years, 3D HPE has been extensively explored with RGB images
    and videos in the deep learning era [[213](#bib.bib213), [214](#bib.bib214)].
    The most challenging scenario in 3D HPE is always related to the high-speed motion [[215](#bib.bib215)],
    which is essential in many practical applications, such as sports performance
    evaluation. However, the RGB cameras suffer inevitable fundamental problems [[216](#bib.bib216)],
    including unsatisfactory frame rates and data redundancy. By contrast, event cameras
    are more advisable for fast-motion scenarios.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，3D HPE 在深度学习时代通过 RGB 图像和视频得到了广泛探索[[213](#bib.bib213), [214](#bib.bib214)]。3D
    HPE 中最具挑战性的场景通常与高速运动有关[[215](#bib.bib215)]，这一点在许多实际应用中至关重要，例如运动表现评估。然而，RGB 相机存在不可避免的基本问题[[216](#bib.bib216)]，包括帧率不令人满意和数据冗余。相比之下，事件相机在快速运动场景中更为适宜。
- en: DHP19 [[217](#bib.bib217)] is the first DL-based pipeline and provides the first
    dataset for event-based 3D HPE. To utilize the frame-based DL algorithms, the
    event streams are transferred to DVS frames by accumulating a fixed number of
    events in  [[217](#bib.bib217)]. Meanwhile, the DHP19 takes events captured by
    multiple calibrated cameras. More recently, EventCap [[216](#bib.bib216)] is the
    first work to capture high-speed human motions from a single event camera with
    the guidance of gray-scale images. To alleviate the reliance on frame inputs,
    the following research EventHPE [[218](#bib.bib218)], proposes to infer 3D HPE
    from the sole source of event input, given the beginning shape from the first
    frame of the intensity image stream.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: DHP19 [[217](#bib.bib217)] 是第一个基于 DL 的管道，并提供了第一个用于事件驱动 3D HPE 的数据集。为了利用基于帧的
    DL 算法，事件流通过在 [[217](#bib.bib217)] 中积累固定数量的事件转化为 DVS 帧。同时，DHP19 捕捉了由多个校准相机捕获的事件。最近，EventCap
    [[216](#bib.bib216)] 是第一个通过单个事件相机捕捉高速人类运动的工作，并且借助灰度图像的指导。为了减少对帧输入的依赖，随后的研究 EventHPE
    [[218](#bib.bib218)] 提出了从唯一的事件输入源推断 3D HPE，给定来自强度图像流的第一帧的初始形状。
- en: '![Refer to caption](img/09a7f8c0526c36770d00e5cf7c6e7635.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/09a7f8c0526c36770d00e5cf7c6e7635.png)'
- en: 'Figure 12: Qualitative comparison between 3D HPE methods from  [[218](#bib.bib218)].
    (a) VIBE [[215](#bib.bib215)]; (b) EventCap [[216](#bib.bib216)], (c) EventHPE [[218](#bib.bib218)]'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：来自 [[218](#bib.bib218)] 的 3D HPE 方法的定性比较。(a) VIBE [[215](#bib.bib215)];
    (b) EventCap [[216](#bib.bib216)]; (c) EventHPE [[218](#bib.bib218)]
- en: .
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'Remarks: A limitation of these approaches is that they need gray-scale images
    for initialization. For future research, it is worth investigating how to infer
    3D HPE purely from event signals without additional priors. Meanwhile, it also
    promises to combine the advantages of both RGB and event sensors and design multi-modal
    learning frameworks for more robust 3D HPE.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：这些方法的一个局限性是它们需要灰度图像进行初始化。未来的研究值得探讨如何仅从事件信号中推断 3D HPE 而不依赖额外的先验知识。同时，这也有望结合
    RGB 和事件传感器的优点，设计多模态学习框架，以实现更强健的 3D HPE。
- en: 5 Research Trend and Discussions
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 研究趋势与讨论
- en: 5.1 Discussions
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 讨论
- en: Do we really need deep networks for learning events? Motivated by the widespread
    applications and success of DL methods in computer vision using frame-based imagery,
    there is a growing research focus on applying DL to event data. DL-based approaches
    offer significant accuracy improvements compared to conventional event-based vision
    algorithms. For instance, a recent DL-based feature tracker [[30](#bib.bib30)]
    outperforms non-DL methods. Additionally, DL-based methods exhibit superior results
    in 3D HPE compared to multi-view models, showcasing the effectiveness of DL techniques [[217](#bib.bib217)].
    However, the adoption of DL techniques introduces certain challenges. For example,
    SNNs [[43](#bib.bib43), [33](#bib.bib33), [68](#bib.bib68)] naturally accommodate
    event streams and enable asynchronous inference at low computational cost. Nevertheless,
    training SNNs is difficult, and the supporting hardware infrastructure is not
    well-established, limiting their applications in the computer vision community.
    Recently, DNNs have been introduced to address tasks in event-based vision [[39](#bib.bib39)].
    However, existing methods overlook the unique characteristics of event data and
    primarily aim to bridge the domain gaps between RGB and event-based vision. Further
    exploration is needed in the conversion between raw event data and grid-based
    inputs (e.g., various event representations). Moreover, an urgent challenge is
    to determine the suitable learning pipeline for utilizing raw events with DNNs.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真的需要深度网络来学习事件吗？受到深度学习（DL）方法在基于帧的图像计算机视觉中的广泛应用和成功的激励，越来越多的研究关注将 DL 应用于事件数据。与传统的事件视觉算法相比，基于
    DL 的方法在准确性上有显著提高。例如，最近的一个基于 DL 的特征跟踪器[[30](#bib.bib30)] 优于非 DL 方法。此外，基于 DL 的方法在
    3D 人体姿态估计中表现优于多视角模型，展示了 DL 技术的有效性[[217](#bib.bib217)]。然而，采用 DL 技术也带来了一些挑战。例如，SNN[[43](#bib.bib43),
    [33](#bib.bib33), [68](#bib.bib68)] 自然适应事件流，并以低计算成本实现异步推理。然而，训练 SNN 很困难，且支持的硬件基础设施尚未完善，限制了其在计算机视觉领域的应用。最近，DNN
    已被引入处理事件视觉任务[[39](#bib.bib39)]。然而，现有方法忽视了事件数据的独特特性，主要旨在弥合 RGB 和基于事件的视觉之间的领域差距。需要进一步探索原始事件数据与网格化输入之间的转换（例如，各种事件表示）。此外，一个紧迫的挑战是确定适合利用原始事件的
    DNN 学习管道。
- en: Do we really need convolution operations to filter events as done for image
    data? As canonical convolutions only operate through the spatial perspective,
    simply applying the 2D convolutional modules to event streams neglects the temporal
    correlation of events, leading to a sub-optimal network design. Intuitively, we
    suggest 1) using a graph to describe event streams [[53](#bib.bib53)]; 2) introducing
    the self-attention-based transformer for temporal information [[219](#bib.bib219),
    [220](#bib.bib220)]; 3) applying recurrent network [[221](#bib.bib221)].
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否真的需要卷积操作来过滤事件，就像处理图像数据一样？由于经典卷积仅在空间角度上操作，简单地将二维卷积模块应用于事件流忽略了事件的时间相关性，从而导致了次优的网络设计。直观地，我们建议
    1) 使用图形来描述事件流 [[53](#bib.bib53)]; 2) 引入基于自注意力的变换器以处理时间信息 [[219](#bib.bib219),
    [220](#bib.bib220)]; 3) 应用递归网络 [[221](#bib.bib221)]。
- en: 'Low latency of event cameras vs. High computation of DNNs: One of the notable
    superiorities of event cameras is the low latency which enables real-time applications
    of event cameras. However, the computational complexity of the neural network
    is typically enormous, potentially negating the benefits of lower event latency.
    An important research question is how to accelerate the neural network in the
    field of events. We suggest that three angles be taken when conducting this issue.
    1) Make use of a light-weight network architecture, like MobileNet[[222](#bib.bib222),
    [223](#bib.bib223)] Shuffle Net[[224](#bib.bib224), [225](#bib.bib225)]. 2) Make
    use of network quantization and compression techniques[[226](#bib.bib226)]. 3)
    Construct a network based on SNN[[227](#bib.bib227)], which has low delay and
    sparsity.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 事件相机的低延迟与深度神经网络的高计算量：事件相机的一大优点是低延迟，使得事件相机能够进行实时应用。然而，神经网络的计算复杂度通常非常高，可能会抵消事件低延迟的优势。一个重要的研究问题是如何在事件领域加速神经网络。我们建议从三个角度进行研究。1)
    利用轻量级网络架构，如 MobileNet[[222](#bib.bib222), [223](#bib.bib223)] Shuffle Net[[224](#bib.bib224),
    [225](#bib.bib225)]。2) 利用网络量化和压缩技术[[226](#bib.bib226)]。3) 构建基于脉冲神经网络（SNN）[[227](#bib.bib227)]的网络，具有低延迟和稀疏性。
- en: 'How to better deal with noisy events with DNNs? Random noise can be introduced
    throughout the trigger process of events due to many reasons. Thus the denoising
    procedure is necessary for accurate information capture. According to the existing
    denoising methods in Sec. [2.2.2](#S2.SS2.SSS2 "2.2.2 Event Denoising ‣ 2.2 Quality
    Enhancement for Events ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based
    Vision: A Comprehensive Survey and Benchmarks"), we suggest dealing with the noisy
    events with DNNs in three steps: 1) Formulate the spatial and temporal distributions
    of raw events; 2) Separately denoising from both spatial and temporal perspectives,
    e.g., operations in spatial neighbourhoods and current event surfaces; 3) Further
    consider the correlation between spatial and temporal distributions, aiming at
    maintaining spatial-temporal correlation.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '如何更好地处理带噪声的事件？由于多种原因，事件的触发过程可能会引入随机噪声。因此，去噪过程对于准确捕捉信息是必要的。根据现有的去噪方法，见Sec. [2.2.2](#S2.SS2.SSS2
    "2.2.2 Event Denoising ‣ 2.2 Quality Enhancement for Events ‣ 2 Event Processing
    for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks")，我们建议分三步处理带噪声的事件：1)
    制定原始事件的空间和时间分布；2) 从空间和时间两个角度分别去噪，例如在空间邻域和当前事件表面上的操作；3) 进一步考虑空间和时间分布之间的关联，旨在保持时空关联。'
- en: 'Can the high temporal resolution of events be fully reflected by DNNs? No matter
    what kind of event representation, summarized in Sec. [2.1](#S2.SS1 "2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks"), is used as DNN’s input, the grid-like
    tensors always lose some partial information of raw events, such as temporal information
    in the image-based representations [[33](#bib.bib33), [34](#bib.bib34), [39](#bib.bib39),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]. Obviously, the high temporal
    resolution of events cannot be fully reflected by the existing DNNs which are
    proposed to solve frame-based vision. The micro-second level temporal resolutions
    are predominantly fused to frame-like representations for the downstream tasks.
    SNNs can solve this problem according to their architectures, however, there are
    still technical difficulties in bringing SNNs into practical applications. Thus
    specific neural networks directly designed for event data are in the future outlook.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '事件的高时间分辨率能否被深度神经网络（DNNs）充分反映？无论使用什么样的事件表示，概述见于Sec. [2.1](#S2.SS1 "2.1 Event
    Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning for Event-based Vision:
    A Comprehensive Survey and Benchmarks")，作为DNN的输入，网格状张量总是会丢失一些原始事件的部分信息，例如基于图像的表示中的时间信息[[33](#bib.bib33),
    [34](#bib.bib34), [39](#bib.bib39), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]。显然，现有的DNN无法充分反映事件的高时间分辨率，因为这些DNN是为解决基于帧的视觉而提出的。微秒级时间分辨率主要被融合到类似帧的表示中用于下游任务。脉冲神经网络（SNNs）可以根据其架构解决此问题，但在将SNNs应用于实际应用中仍然存在技术难题。因此，直接为事件数据设计的特定神经网络是未来的展望。'
- en: Do events contain sufficient visual information for learning robust DNN models?
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 事件是否包含足够的视觉信息来学习鲁棒的DNN模型？
- en: Less plausible accuracy of event-based vision models in the dark. Low light
    conditions bring noise mainly due to the sufficient intensity changes that do
    not generate an event. The noises can be briefly called ”holes” or false negatives.
    This leads to unreliable scene understanding of event cameras. Employing DL for
    noise cancellation of event data is a promising approach [[71](#bib.bib71)]. In
    addition, multi-sensor fusion is a valuable direction. For example, thermal sensors
    have been widely used in the dark [[228](#bib.bib228)]. Therefore, developing
    DL-based methods that take thermal and event data as inputs is worth exploring.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在黑暗中，基于事件的视觉模型的准确性较低。低光条件下带来的噪声主要由于光强变化不足以生成事件。这些噪声可以简要地称为“孔”或假阴性。这导致了事件相机对场景理解的不可靠。利用深度学习（DL）进行事件数据噪声消除是一种有前景的方法[[71](#bib.bib71)]。此外，多传感器融合也是一个有价值的方向。例如，热传感器在黑暗中已被广泛使用[[228](#bib.bib228)]。因此，开发以热数据和事件数据为输入的基于DL的方法值得探索。
- en: Are DL-based methods more advantageous than optimized-based ones? Optimization-based
    methods are more applicable to edge computing  [[229](#bib.bib229)] but are still
    unlikely to reach global optima—more likely to be trapped in saddle points  [[230](#bib.bib230)]
    ). By contrast, DNNs are superior in that they can flexibly learn the multi-dimension
    data and extract better feature representations. Intuitively, DNNs have the potential
    to learn spatial-temporal information from events. Superior results has been demonstrated
    in exploring the temporal correlation of events with various DNNs in the literature [[219](#bib.bib219),
    [220](#bib.bib220), [221](#bib.bib221)]. Also, owning to the numerical stability
    and efficiency, DNNs allow hidden state encoding for effective prediction in various
    tasks. For example, in the context of event-based SLAM, implicit encoding of event
    streams exhibits higher spatial-temporal consistency than naively tracking fired
    pixels, enabling estimation of ego-motion and 3D scene reconstruction jointly,
    whereas the optimization-based method shows significantly lower accuracy.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的方法是否比基于优化的方法更有优势？基于优化的方法更适用于边缘计算[[229](#bib.bib229)]，但仍然不太可能达到全局最优，更可能陷入鞍点[[230](#bib.bib230)]。相比之下，DNNs在灵活学习多维数据和提取更好特征表示方面表现优越。直观地，DNNs有潜力从事件中学习时空信息。在探索事件的时间相关性方面，文献中的各种DNNs已展示出优越的结果[[219](#bib.bib219),
    [220](#bib.bib220), [221](#bib.bib221)]。此外，由于数值稳定性和效率，DNNs允许隐藏状态编码，以便在各种任务中进行有效预测。例如，在事件基础的SLAM中，事件流的隐式编码比简单跟踪触发像素具有更高的时空一致性，能够共同估计自我运动和3D场景重建，而基于优化的方法显示出显著较低的准确性。
- en: Is focal alignment necessary between RGB and event pixels in event cameras?
    Event cameras have emerged as an efficient alternative for capturing motion information.
    Recent studies highlight the potential of DNNs in leveraging both RGB and event
    data to enhance images, enabling crucial tasks like deblurring and video frame
    interpolation. This capability holds significant practical applications in areas
    such as augmented reality and virtual reality. However, achieving the desired
    image quality mandates RGB characteristics comparable to advanced mobile RGB sensors,
    along with precise focal alignment between RGB and event pixels on the sensor.
    To address these challenges, one promising approach involves the development of
    hybrid-type sensors that seamlessly integrate high-frame-rate event pixels with
    advanced mobile RGB pixels [[231](#bib.bib231), [232](#bib.bib232), [233](#bib.bib233)].
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件相机中，RGB与事件像素之间的焦距对准是否必要？事件相机已成为捕捉运动信息的高效替代方案。最近的研究强调了深度神经网络（DNNs）在利用RGB和事件数据来增强图像方面的潜力，这使得去模糊和视频帧插值等关键任务成为可能。这一能力在增强现实和虚拟现实等领域具有重要的实际应用。然而，实现所需的图像质量需要RGB特性与先进移动RGB传感器相当，并且要求传感器上RGB与事件像素之间的焦距精准对准。为解决这些挑战，一个有前景的方法是开发混合型传感器，将高帧率事件像素与先进的移动RGB像素无缝集成[[231](#bib.bib231),
    [232](#bib.bib232), [233](#bib.bib233)]。
- en: 5.2 New Directions
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 新方向
- en: 'NeRF for Event-based Neural Rendering: NeRF  [[234](#bib.bib234)] is a representative
    neural implicit 3D representation method that synthesizes 3D objects with volume
    rendering techniques. Most existing research in NeRF is investigated based on
    RGB cameras, which suffer from inevitable shortcomings, e.g., low dynamic range
    and motion blur in unfavorable lighting conditions. Thus, recent attention has
    been paid to the usage of event cameras for NeRF [[235](#bib.bib235), [236](#bib.bib236),
    [237](#bib.bib237)]. The first work is EventNeRF [[235](#bib.bib235)], which is
    trained with pure event-based supervision. It demonstrates that the NeRF estimation
    from a single fast-moving event camera in unfavourable scenarios (e.g., fast-moving
    objects, motion blur, or insufficient lighting) is feasible while frame-based
    approaches fail. Moreover, E-NeRF [[236](#bib.bib236)] takes the strengths of
    RGB and event cameras by combining color frames and events to achieve sharp and
    colorize reconstruction results.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 用于事件驱动神经渲染的 NeRF：NeRF [[234](#bib.bib234)] 是一种代表性的神经隐式 3D 表示方法，利用体积渲染技术合成 3D
    对象。现有的大多数 NeRF 研究都是基于 RGB 相机，这些相机在不利的光照条件下存在不可避免的缺点，例如低动态范围和运动模糊。因此，最近的关注已转向事件相机在
    NeRF 中的使用 [[235](#bib.bib235), [236](#bib.bib236), [237](#bib.bib237)]。首个相关工作是
    EventNeRF [[235](#bib.bib235)]，它采用纯事件驱动的监督进行训练。研究表明，从单个快速移动的事件相机在不利情景下（例如，快速移动的物体、运动模糊或光线不足）进行
    NeRF 估计是可行的，而基于帧的方法则失败了。此外，E-NeRF [[236](#bib.bib236)] 通过结合颜色帧和事件，充分利用 RGB 和事件相机的优点，实现了清晰和上色的重建结果。
- en: From the review of recent progress, event-based NeRF methods show superior abilities
    than the frame-based NeRF methods. Since the event-based NeRF is an emerging direction,
    future research could focus on improving the technique pipelines and more lightweight
    DNNs for mobile applications.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 从近期进展的回顾来看，基于事件的 NeRF 方法显示出比基于帧的 NeRF 方法更优越的能力。由于基于事件的 NeRF 是一个新兴方向，未来的研究可以集中于改进技术流程和更轻量的
    DNN 以适应移动应用。
- en: 'Multi- and Cross-modal Learning for Event-based Vision: In practical scenarios,
    event cameras always play an auxiliary role to provide multi-modal guidance in
    many aforementioned computer vision tasks, e.g., image and video SR [[112](#bib.bib112),
    [111](#bib.bib111), [108](#bib.bib108)]. With the development of event cameras,
    event-based vision will occupy an increasingly dominant position in both research
    and industry. Especially in some specific domains, there are already attempts
    to leverage pure event data to facilitate task accuracy, such as reconstructing
    RGB images and videos from pure event data [[17](#bib.bib17), [64](#bib.bib64),
    [102](#bib.bib102), [90](#bib.bib90)], etc. Consequently, how to utilize the domain
    knowledge from the frame-based vision to the emerging event-based vision deserves
    more intensive research. Recently, CTN [[238](#bib.bib238)] is proposed as an
    early attempt of transferring knowledge from frame-based vision to event-based
    vision.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 基于事件的视觉的多模态和跨模态学习：在实际场景中，事件相机通常作为辅助角色，在许多前述计算机视觉任务中提供多模态指导，例如，图像和视频 SR [[112](#bib.bib112),
    [111](#bib.bib111), [108](#bib.bib108)]。随着事件相机的发展，基于事件的视觉将在研究和行业中占据越来越主导的位置。特别是在一些特定领域，已经有尝试利用纯事件数据来促进任务精度，例如从纯事件数据重建
    RGB 图像和视频 [[17](#bib.bib17), [64](#bib.bib64), [102](#bib.bib102), [90](#bib.bib90)]，等。因此，如何将基于帧的视觉领域知识利用到新兴的基于事件的视觉中，值得更深入的研究。最近，CTN [[238](#bib.bib238)]
    被提出作为将知识从基于帧的视觉转移到基于事件的视觉的早期尝试。
- en: 'Event-based model Pretaining: Pre-trained neural networks are the foundations
    of almost all downstream task models in the deep learning era. For the frame-based
    vision, the pre-trained weights on ImageNet are widely utilized for models’ outstanding
    accuracy gains. With the growing interest in event-based vision, a wide range
    of datasets are collected in many downstream tasks. Thus, unified pre-trained
    weights with large-scale data are required for better accuracy. Since the event
    data is a totally distinct format containing spatial-temporal information, a reliable
    and efficient pre-trained method is required for various applications. Recently,
    in  [[239](#bib.bib239)], Yang et al. proposed the first pre-training pipeline
    for dealing with event camera data, including various event data augmentations,
    a masking sample strategy, and a contrastive learning approach. Additionally,
    Hu et al. [[58](#bib.bib58)]  proposed the Network Grafting Algorithm, which replaces
    the front-end network of a pre-trained DNN with a new network that processes unconventional
    visual inputs. Through self-supervised training using synchronously-recorded intensity
    frames and novel sensor data, the algorithm maximizes feature similarity between
    the pretrained network and the grafted network. This enhances the pretrained network’s
    ability to process and extract meaningful features from diverse input sources.
    Future research can focus on how to fully utilize all kinds of event representations,
    summarized in Sec. [2.1](#S2.SS1 "2.1 Event Representation ‣ 2 Event Processing
    for DNNs ‣ Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks"),
    to realize better event-based pre-training. More details of new directions can
    be found in the supplementary material.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '基于事件的模型预训练：在深度学习时代，预训练的神经网络几乎是所有下游任务模型的基础。对于基于帧的视觉，基于 ImageNet 的预训练权重被广泛应用于模型的卓越准确性提升。随着对事件视觉的兴趣不断增长，许多下游任务中收集了各种数据集。因此，需要统一的大规模数据预训练权重以获得更好的准确性。由于事件数据是一种完全不同的包含时空信息的格式，因此需要一种可靠高效的预训练方法来应对各种应用。最近，在
    [[239](#bib.bib239)] 中，Yang 等人提出了第一个处理事件摄像机数据的预训练管道，包括各种事件数据增强、掩蔽样本策略和对比学习方法。此外，Hu
    等人 [[58](#bib.bib58)] 提出了网络嫁接算法，该算法用处理非常规视觉输入的新网络替换预训练深度神经网络的前端网络。通过使用同步记录的强度帧和新传感器数据进行自监督训练，该算法最大化了预训练网络和嫁接网络之间的特征相似性。这增强了预训练网络从多样化输入源中处理和提取有意义特征的能力。未来的研究可以集中在如何充分利用各种事件表示（总结见第
    [2.1](#S2.SS1 "2.1 Event Representation ‣ 2 Event Processing for DNNs ‣ Deep Learning
    for Event-based Vision: A Comprehensive Survey and Benchmarks") 节），以实现更好的基于事件的预训练。更多关于新方向的细节可以在补充材料中找到。'
- en: References
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi,
    S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis *et al.*, “Event-based
    vision: A survey,” *TPAMI*, 2020.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi,
    S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis *等人*, “基于事件的视觉：一项综述，”
    *TPAMI*, 2020。'
- en: '[2] M. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. A. F. Guerra, P. Joshi,
    P. Plank, and S. R. Risbud, “Advancing neuromorphic computing with loihi: A survey
    of results and outlook,” *Proceedings of the IEEE*, 2021.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. A. F. Guerra, P. Joshi,
    P. Plank 和 S. R. Risbud, “通过 Loihi 推进类脑计算：结果和展望的综述，” *IEEE 会议录*, 2021。'
- en: '[3] R. Baldwin, R. Liu, M. M. Almatrafi, V. K. Asari, and K. Hirakawa, “Time-ordered
    recent event volumes for event cameras,” *TPAMI*, 2022.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. Baldwin, R. Liu, M. M. Almatrafi, V. K. Asari 和 K. Hirakawa, “为事件摄像机提供时间排序的近期事件体积，”
    *TPAMI*, 2022。'
- en: '[4] J. Zhang, K. Yang, and R. Stiefelhagen, “ISSAFE: Improving semantic segmentation
    in accidents by fusing event-based data.”   IEEE, 2021.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Zhang, K. Yang 和 R. Stiefelhagen, “ISSAFE：通过融合基于事件的数据来改善事故中的语义分割。” IEEE,
    2021。'
- en: '[5] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, “EKLT: Asynchronous
    photometric feature tracking using events and frames,” *IJCV*, 2020.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. Gehrig, H. Rebecq, G. Gallego 和 D. Scaramuzza, “EKLT：利用事件和帧进行异步光度特征跟踪，”
    *IJCV*, 2020。'
- en: '[6] I. Alzugaray and M. Chli, “Asynchronous corner detection and tracking for
    event cameras in real time,” *RAL*, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] I. Alzugaray 和 M. Chli, “实时事件摄像机的异步角点检测与跟踪，” *RAL*, 2018。'
- en: '[7] Alzugaray, Ignacio and Chli, Margarita, “ACE: An efficient asynchronous
    corner tracker for event cameras,” in *3DV*.   IEEE, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Alzugaray, Ignacio 和 Chli, Margarita, “ACE：一种高效的事件摄像机异步角点跟踪器，” 在 *3DV*
    中。 IEEE, 2018。'
- en: '[8] R. Li, D. Shi, Y. Zhang, K. Li, and R. Li, “FA-Harris: A Fast and Asynchronous
    Corner Detector for Event Cameras,” in *IROS*.   IEEE, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] R. Li, D. Shi, Y. Zhang, K. Li 和 R. Li, “FA-Harris: 一种适用于事件相机的快速和异步角点检测器”，在
    *IROS*。IEEE，2019年。'
- en: '[9] A. Glover, A. Dinale, L. D. S. Rosa, S. Bamford, and C. Bartolozzi, “LuvHarris:
    A Practical Corner Detector for Event-cameras,” *TPAMI*, 2021.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Glover, A. Dinale, L. D. S. Rosa, S. Bamford 和 C. Bartolozzi, “LuvHarris:
    一种适用于事件相机的实用角点检测器”，*TPAMI*，2021年。'
- en: '[10] Henri, Rebecq, Timo, Horstschaefer, Guillermo, Gallego, Davide, and Scaramuzza,
    “EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping
    in Real Time,” *RAL*, 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Henri, Rebecq, Timo, Horstschaefer, Guillermo, Gallego, Davide 和 Scaramuzza,
    “EVO: 一种几何方法用于实时事件基础的6自由度平行跟踪和映射”，*RAL*，2017年。'
- en: '[11] J. Hidalgo-Carrió, G. Gallego, and D. Scaramuzza, “Event-aided direct
    sparse odometry,” in *CVPR*, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Hidalgo-Carrió, G. Gallego 和 D. Scaramuzza, “事件辅助的直接稀疏里程计”，在 *CVPR*，2022年。'
- en: '[12] E. Mueggler, H. Rebecq, G. Gallego, T. Delbruck, and D. Scaramuzza, “The
    event-camera dataset and simulator: Event-based data for pose estimation, visual
    odometry, and SLAM,” *Int J Rob Res.*, 2017.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] E. Mueggler, H. Rebecq, G. Gallego, T. Delbruck 和 D. Scaramuzza, “事件相机数据集和模拟器：用于姿态估计、视觉里程计和SLAM的事件基础数据”，*Int
    J Rob Res.*，2017年。'
- en: '[13] M. Milford, H. Kim, S. Leutenegger, and A. Davison, “Towards visual slam
    with event-based cameras,” in *The problem of mobile sensors workshop in conjunction
    with RSS*, 2015.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Milford, H. Kim, S. Leutenegger 和 A. Davison, “迈向基于事件相机的视觉SLAM”，在 *与RSS联合举办的移动传感器问题研讨会*，2015年。'
- en: '[14] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Ultimate
    SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High
    Speed Scenarios,” *RAL*, 2018.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. R. Vidal, H. Rebecq, T. Horstschaefer 和 D. Scaramuzza, “终极SLAM？结合事件、图像和IMU实现HDR和高速场景下的鲁棒视觉SLAM”，*RAL*，2018年。'
- en: '[15] J. Jiao, H. Huang, L. Li, Z. He, Y. Zhu, and M. Liu, “Comparing Representations
    in Tracking for Event Camera-based SLAM,” 2021.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. Jiao, H. Huang, L. Li, Z. He, Y. Zhu 和 M. Liu, “基于事件相机的SLAM中表征的比较”，2021年。'
- en: '[16] Z. Chen, Q. Zheng, P. Niu, H. Tang, and G. Pan, “Indoor lighting estimation
    using an event camera,” in *CVPR*, 2021.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Z. Chen, Q. Zheng, P. Niu, H. Tang 和 G. Pan, “使用事件相机进行室内照明估计”，在 *CVPR*，2021年。'
- en: '[17] S. M. M. Isfahani, L. Wang, Y.-S. Ho, and K. jin Yoon, “Event-Based High
    Dynamic Range Image and Very High Frame Rate Video Generation Using Conditional
    Generative Adversarial Networks,” *CVPR*, 2018.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. M. M. Isfahani, L. Wang, Y.-S. Ho 和 K. jin Yoon, “基于事件的高动态范围图像和超高帧率视频生成使用条件生成对抗网络”，*CVPR*，2018年。'
- en: '[18] N. Messikommer, S. Georgoulis, D. Gehrig, S. Tulyakov, J. Erbach, A. Bochicchio,
    Y. Li, and D. Scaramuzza, “Multi-Bracket High Dynamic Range Imaging with Event
    Cameras,” in *CVPR*, 2022.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] N. Messikommer, S. Georgoulis, D. Gehrig, S. Tulyakov, J. Erbach, A. Bochicchio,
    Y. Li 和 D. Scaramuzza, “使用事件相机的多支架高动态范围成像”，在 *CVPR*，2022年。'
- en: '[19] C. Scheerlinck, H. Rebecq, D. Gehrig, N. Barnes, R. Mahony, and D. Scaramuzza,
    “Fast image reconstruction with an event camera,” in *WACV*, 2020.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] C. Scheerlinck, H. Rebecq, D. Gehrig, N. Barnes, R. Mahony 和 D. Scaramuzza,
    “使用事件相机进行快速图像重建”，在 *WACV*，2020年。'
- en: '[20] A. Mondal, J. H. Giraldo, T. Bouwmans, A. S. Chowdhury *et al.*, “Moving
    Object Detection for Event-based Vision using Graph Spectral Clustering,” in *ICCV*,
    2021.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Mondal, J. H. Giraldo, T. Bouwmans, A. S. Chowdhury *等人*, “使用图谱谱聚类的事件基础视觉中的移动对象检测”，在
    *ICCV*，2021年。'
- en: '[21] A. Mitrokhin, C. Fermüller, C. Parameshwara, and Y. Aloimonos, “Event-Based
    Moving Object Detection and Tracking,” in *IROS*.   IEEE, 2018.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Mitrokhin, C. Fermüller, C. Parameshwara 和 Y. Aloimonos, “基于事件的移动对象检测和跟踪”，在
    *IROS*。IEEE，2018年。'
- en: '[22] M. Iacono, S. Weber, A. Glover, and C. Bartolozzi, “Towards event-driven
    object detection with off-the-shelf deep learning,” in *IROS*, 2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Iacono, S. Weber, A. Glover 和 C. Bartolozzi, “基于事件的目标检测迈向现成深度学习”，在
    *IROS*，2018年。'
- en: '[23] E. Perot, P. de Tournemire, D. Nitti, J. Masci, and A. Sironi, “Learning
    to detect objects with a 1 megapixel event camera,” *Adv Neural Inf Process Syst.*,
    2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] E. Perot, P. de Tournemire, D. Nitti, J. Masci 和 A. Sironi, “使用1百万像素事件相机学习检测对象”，*Adv
    Neural Inf Process Syst.*，2020年。'
- en: '[24] I. Alonso and A. C. Murillo, “EV-SegNet: Semantic segmentation for event-based
    cameras,” in *CVPRW*, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] I. Alonso 和 A. C. Murillo, “EV-SegNet: 事件基础相机的语义分割”，在 *CVPRW*，2019年。'
- en: '[25] D. Gehrig, M. Gehrig, J. Hidalgo-Carrió, and D. Scaramuzza, “Video to
    events: Recycling video datasets for event cameras,” in *CVPR*, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] D. Gehrig, M. Gehrig, J. Hidalgo-Carrió 和 D. Scaramuzza, “视频转事件：回收视频数据集用于事件相机”，在
    *CVPR*，2020年。'
- en: '[26] R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares-Barranco,
    R. Paz-Vicente, F. Gómez-Rodríguez, L. Camuñas-Mesa, R. Berner, M. Rivas-Pérez,
    T. Delbruck *et al.*, “CAVIAR: A 45k neuron, 5M synapse, 12G connects/s AER hardware
    sensory–processing–learning–actuating system for high-speed visual object recognition
    and tracking,” *IEEE Transactions on Neural networks*, 2009.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares-Barranco,
    R. Paz-Vicente, F. Gómez-Rodríguez, L. Camuñas-Mesa, R. Berner, M. Rivas-Pérez,
    T. Delbruck *等*，“CAVIAR: 一个 45k 神经元，5M 突触，12G 连接/s 的 AER 硬件感知处理学习执行系统，用于高速视觉物体识别和跟踪，”*IEEE
    神经网络汇刊*，2009。'
- en: '[27] T. Delbrück, B. Linares-Barranco, E. Culurciello, and C. Posch, “Activity-driven,
    event-based vision sensors,” in *Proceedings of 2010 IEEE International Symposium
    on Circuits and Systems*, 2010.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] T. Delbrück, B. Linares-Barranco, E. Culurciello, 和 C. Posch, “基于活动的事件视觉传感器，”发表于*2010
    IEEE 国际电路与系统研讨会论文集*，2010。'
- en: '[28] P. Lichtsteiner, C. Posch, and T. Delbruck, “A 128× 128 120 dB 15 us Latency
    Asynchronous Temporal Contrast Vision Sensor,” *IEEE Journal of Solid-State Circuits*,
    2008.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] P. Lichtsteiner, C. Posch, 和 T. Delbruck, “一个 128×128 120 dB 15 us 延迟的异步时间对比视觉传感器，”*IEEE
    固态电路杂志*，2008。'
- en: '[29] P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer, “Fast-classifying,
    high-accuracy spiking deep networks through weight and threshold balancing,” in
    *International joint conference on neural networks*.   IEEE, 2015.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, 和 M. Pfeiffer, “通过权重和阈值平衡实现快速分类的高精度尖峰深度网络，”发表于*国际神经网络联合会议*。IEEE，2015。'
- en: '[30] N. Messikommer, C. Fang, M. Gehrig, and D. Scaramuzza, “Data-driven feature
    tracking for event cameras,” in *CVPR*, 2023.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] N. Messikommer, C. Fang, M. Gehrig, 和 D. Scaramuzza, “基于数据的特征跟踪用于事件相机，”发表于*CVPR*，2023。'
- en: '[31] S.-C. Liu, B. Rueckauer, E. Ceolini, A. Huber, and T. Delbruck, “Event-driven
    sensing for efficient perception: Vision and audition algorithms,” *IEEE Signal
    Processing Magazine*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S.-C. Liu, B. Rueckauer, E. Ceolini, A. Huber, 和 T. Delbruck, “用于高效感知的事件驱动传感：视觉和听觉算法，”*IEEE
    信号处理杂志*，2019。'
- en: '[32] S. Lin, F. Xu, X. Wang, W. Yang, and L. Yu, “Efficient Spatial-Temporal
    Normalization of SAE Representation for Event Camera,” *RAL*, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Lin, F. Xu, X. Wang, W. Yang, 和 L. Yu, “事件相机的高效空间-时间归一化 SAE 表示，”*RAL*，2020。'
- en: '[33] A. I. Maqueda, A. Loquercio, G. Gallego, N. García, and D. Scaramuzza,
    “Event-based vision meets deep learning on steering prediction for self-driving
    cars,” in *CVPR*, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. I. Maqueda, A. Loquercio, G. Gallego, N. García, 和 D. Scaramuzza, “基于事件的视觉与深度学习结合在自驾驶汽车的转向预测中，”发表于*CVPR*，2018。'
- en: '[34] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “EV-FlowNet: Self-supervised
    optical flow estimation for event-based cameras,” in *Robotics: Science and Systems*,
    2018.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Z. Zhu, L. Yuan, K. Chaney, 和 K. Daniilidis, “EV-FlowNet: 自监督光流估计用于基于事件的摄像头，”发表于*机器人学：科学与系统*，2018。'
- en: '[35] Y. Wang, B. Du, Y. Shen, K. Wu, G. Zhao, J. Sun, and H. Wen, “EV-Gait:
    Event-based robust gait recognition using dynamic vision sensors,” in *CVPR*,
    2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Wang, B. Du, Y. Shen, K. Wu, G. Zhao, J. Sun, 和 H. Wen, “EV-Gait: 使用动态视觉传感器的基于事件的鲁棒步态识别，”发表于*CVPR*，2019。'
- en: '[36] Y. Deng, Y. Li, and H. Chen, “AMAE: Adaptive Motion-Agnostic Encoder for
    Event-Based Object Classification,” *RAL*, 2020.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Y. Deng, Y. Li, 和 H. Chen, “AMAE: 自适应运动无关编码器用于基于事件的物体分类，”*RAL*，2020。'
- en: '[37] W. Bai, Y. Chen, R. Feng, and Y. Zheng, “Accurate and Efficient Frame-based
    Event Representation for AER Object Recognition,” in *IJCNN*, 2022.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] W. Bai, Y. Chen, R. Feng, 和 Y. Zheng, “准确且高效的基于帧的事件表示用于AER物体识别，”发表于*IJCNN*，2022。'
- en: '[38] Y. Deng, H. Chen, and Y. Li, “MVF-Net: A Multi-View Fusion Network for
    Event-Based Object Classification,” *TCSVT*, 2021.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Deng, H. Chen, 和 Y. Li, “MVF-Net: 一个用于基于事件的物体分类的多视角融合网络，”*TCSVT*，2021。'
- en: '[39] D. Gehrig, A. Loquercio, K. G. Derpanis, and D. Scaramuzza, “End-to-end
    learning of representations for asynchronous event-based data,” in *ICCV*, 2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] D. Gehrig, A. Loquercio, K. G. Derpanis, 和 D. Scaramuzza, “用于异步基于事件的数据的端到端学习表示，”发表于*ICCV*，2019。'
- en: '[40] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, “A differentiable
    recurrent surface for asynchronous event-based data,” in *ECCV*.   Springer, 2020.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Cannici, M. Ciccone, A. Romanoni, 和 M. Matteucci, “一个用于异步事件数据的可微分递归表面，”发表于*ECCV*。Springer，2020。'
- en: '[41] P. K. Park, B. H. Cho, J. M. Park, K. Lee, H. Y. Kim, H. A. Kang, H. G.
    Lee, J. Woo, Y. Roh, W. J. Lee *et al.*, “Performance improvement of deep learning
    based gesture recognition using spatiotemporal demosaicing technique,” in *ICIP*.   IEEE,
    2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] P. K. Park, B. H. Cho, J. M. Park, K. Lee, H. Y. Kim, H. A. Kang, H. G.
    Lee, J. Woo, Y. Roh, W. J. Lee *等*，“使用时空去马赛克技术提升基于深度学习的手势识别性能，”发表于*ICIP*。IEEE，2016。'
- en: '[42] R. Benosman, C. Clercq, X. Lagorce, S.-H. Ieng, and C. Bartolozzi, “Event-based
    visual flow,” *TNNLS*, 2013.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] R. Benosman, C. Clercq, X. Lagorce, S.-H. Ieng 和 C. Bartolozzi, “基于事件的视觉流，”
    *TNNLS*, 2013。'
- en: '[43] X. Lagorce, F. Orchard, Garrick andc Galluppi, B. E. Shi, and R. B. Benosman,
    “Hots: a hierarchy of event-based time-surfaces for pattern recognition,” *TPAMI*,
    2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Lagorce, F. Orchard, Garrick 和 Galluppi, B. E. Shi 和 R. B. Benosman,
    “Hots：一种事件驱动时间表面的层次结构用于模式识别，” *TPAMI*, 2016。'
- en: '[44] A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benosman, “HATS:
    Histograms of averaged time surfaces for robust event-based object classification,”
    in *CVPR*, 2018.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce 和 R. Benosman, “HATS：用于稳健事件驱动对象分类的平均时间表面直方图，”发表于
    *CVPR*, 2018。'
- en: '[45] R. W. Baldwin, M. Almatrafi, J. R. Kaufman, V. Asari, and K. Hirakawa,
    “Inceptive event time-surfaces for object classification using neuromorphic cameras,”
    in *ICIAR*, 2019.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] R. W. Baldwin, M. Almatrafi, J. R. Kaufman, V. Asari 和 K. Hirakawa, “用于对象分类的初步事件时间表面，基于神经形态相机，”发表于
    *ICIAR*, 2019。'
- en: '[46] J. Manderscheid, A. Sironi, N. Bourdis, D. Migliore, and V. Lepetit, “Speed
    Invariant Time Surface for Learning to Detect Corner Points With Event-Based Cameras,”
    in *CVPR*, 2019.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Manderscheid, A. Sironi, N. Bourdis, D. Migliore 和 V. Lepetit, “用于学习检测角点的速度不变时间表面，基于事件的相机，”发表于
    *CVPR*, 2019。'
- en: '[47] M. Almatrafi, R. Baldwin, K. Aizawa, and K. Hirakawa, “Distance Surface
    for Event-Based Optical Flow,” *TPAMI*, 2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Almatrafi, R. Baldwin, K. Aizawa 和 K. Hirakawa, “事件驱动光流的距离表面，” *TPAMI*,
    2020。'
- en: '[48] J. Kim, J. Bae, G. Park, D. Zhang, and Y. M. Kim, “N-imagenet: Towards
    robust, fine-grained object recognition with event cameras,” in *ICCV*, 2021.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Kim, J. Bae, G. Park, D. Zhang 和 Y. M. Kim, “N-imagenet：朝着利用事件相机进行鲁棒、细粒度对象识别的方向迈进，”发表于
    *ICCV*, 2021。'
- en: '[49] A. Zihao Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “Unsupervised event-based
    optical flow using motion compensation,” in *ECCVW*, 2018.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. Zihao Zhu, L. Yuan, K. Chaney 和 K. Daniilidis, “使用运动补偿的无监督事件驱动光流，”发表于
    *ECCVW*, 2018。'
- en: '[50] F. Gu, W. Sng, T. Taunyazov, and H. Soh, “Tactilesgnet: A spiking graph
    neural network for event-based tactile object recognition,” in *IROS*, 2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] F. Gu, W. Sng, T. Taunyazov 和 H. Soh, “Tactilesgnet：一种用于事件驱动触觉对象识别的脉冲图神经网络，”发表于
    *IROS*, 2020。'
- en: '[51] C. Ye, A. Mitrokhin, C. Fermüller, J. A. Yorke, and Y. Aloimonos, “Unsupervised
    Learning of Dense Optical Flow, Depth and Egomotion with Event-Based Sensors,”
    in *IROS*, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] C. Ye, A. Mitrokhin, C. Fermüller, J. A. Yorke 和 Y. Aloimonos, “通过事件驱动传感器无监督学习密集光流、深度和自运动，”发表于
    *IROS*, 2020。'
- en: '[52] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-based
    spatio-temporal feature learning for neuromorphic vision sensing,” *TIP*, 2020.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze 和 Y. Andreopoulos, “基于图的时空特征学习用于神经形态视觉传感，”
    *TIP*, 2020。'
- en: '[53] Y. Deng, H. Chen, H. Liu, and Y. Li, “A Voxel Graph CNN for Object Classification
    With Event Cameras,” in *CVPR*, 2022.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Deng, H. Chen, H. Liu 和 Y. Li, “用于事件相机的体素图CNN进行对象分类，”发表于 *CVPR*, 2022。'
- en: '[54] J. H. Lee, T. Delbruck, and M. Pfeiffer, “Training deep spiking neural
    networks using backpropagation,” *FRONT NEUROSCI-SWITZ*, 2016.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] J. H. Lee, T. Delbruck 和 M. Pfeiffer, “使用反向传播训练深度脉冲神经网络，” *FRONT NEUROSCI-SWITZ*,
    2016。'
- en: '[55] J. Botzheim, T. Obo, and N. Kubota, “Human gesture recognition for robot
    partners by spiking neural network and classification learning,” in *SCIS*, 2012.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Botzheim, T. Obo 和 N. Kubota, “通过脉冲神经网络和分类学习进行人类手势识别，用于机器人伙伴，”发表于 *SCIS*,
    2012。'
- en: '[56] A. Amir, B. Taba, D. Berg, T. Melano, J. McKinstry, C. Di Nolfo, T. Nayak,
    A. Andreopoulos, G. Garreau, M. Mendoza *et al.*, “A low power, fully event-based
    gesture recognition system,” in *CVPR*, 2017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Amir, B. Taba, D. Berg, T. Melano, J. McKinstry, C. Di Nolfo, T. Nayak,
    A. Andreopoulos, G. Garreau 和 M. Mendoza *et al.*, “一个低功耗、完全基于事件的手势识别系统，”发表于 *CVPR*,
    2017。'
- en: '[57] D. P. Moeys, F. Corradi, E. Kerr, P. Vance, G. Das, D. Neil, D. Kerr,
    and T. Delbrück, “Steering a predator robot using a mixed frame/event-driven convolutional
    neural network,” in *2016 Second international conference on event-based control,
    communication, and signal processing (EBCCSP)*.   IEEE, 2016.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] D. P. Moeys, F. Corradi, E. Kerr, P. Vance, G. Das, D. Neil, D. Kerr 和
    T. Delbrück, “使用混合帧/事件驱动卷积神经网络控制掠食者机器人，”发表于 *2016年第二届事件驱动控制、通信和信号处理国际会议 (EBCCSP)*。IEEE,
    2016。'
- en: '[58] Y. Hu, T. Delbruck, and S.-C. Liu, “Learning to exploit multiple vision
    modalities by using grafted networks,” in *ECCV*.   Springer, 2020.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Hu, T. Delbruck 和 S.-C. Liu, “通过使用移植网络学习利用多种视觉模态，”发表于 *ECCV*。Springer,
    2020。'
- en: '[59] N. Messikommer, D. Gehrig, A. Loquercio, and D. Scaramuzza, “Event-based
    asynchronous sparse convolutional networks,” in *ECCV*, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] N. Messikommer, D. Gehrig, A. Loquercio 和 D. Scaramuzza, “基于事件的异步稀疏卷积网络，”发表于
    *ECCV*, 2020。'
- en: '[60] M. Liu and T. Delbruck, “Adaptive time-slice block-matching optical flow
    algorithm for dynamic vision sensors.”   BMVC, 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] M. Liu 和 T. Delbruck, “用于动态视觉传感器的自适应时间切片块匹配光流算法。” BMVC, 2018。'
- en: '[61] Liu, Min and Delbruck, Tobi, “EDFLOW: Event driven optical flow camera
    with keypoint detection and adaptive block matching,” *IEEE Transactions on Circuits
    and Systems for Video Technology*, 2022.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Liu, Min 和 Delbruck, Tobi, “EDFLOW：具有关键点检测和自适应块匹配的事件驱动光流相机，” *IEEE 视听技术电路与系统期刊*,
    2022。'
- en: '[62] B. R. Pradhan, Y. Bethi, S. Narayanan, A. Chakraborty, and C. S. Thakur,
    “N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory
    Surfaces,” in *ISCAS*, 2019.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] B. R. Pradhan, Y. Bethi, S. Narayanan, A. Chakraborty, 和 C. S. Thakur,
    “N-HAR：一种使用记忆表面的神经形态事件驱动的人类活动识别系统，”在 *ISCAS*, 2019。'
- en: '[63] S. Afshar, T. J. Hamilton, J. Tapson, A. Van Schaik, and G. Cohen, “Investigation
    of event-based surfaces for high-speed detection, unsupervised feature extraction,
    and object recognition,” *Frontiers in Neuroscience*, 2019.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Afshar, T. J. Hamilton, J. Tapson, A. Van Schaik, 和 G. Cohen, “基于事件的表面在高速检测、无监督特征提取和目标识别中的研究，”
    *前沿神经科学*, 2019。'
- en: '[64] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, “Events-to-video:
    Bringing modern computer vision to event cameras,” in *CVPR*, 2019.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] H. Rebecq, R. Ranftl, V. Koltun, 和 D. Scaramuzza, “事件到视频：将现代计算机视觉带到事件相机，”在
    *CVPR*, 2019。'
- en: '[65] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-based
    object classification for neuromorphic vision sensing,” in *ICCV*, 2019.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, 和 Y. Andreopoulos, “用于神经形态视觉传感的图形化目标分类，”在
    *ICCV*, 2019。'
- en: '[66] G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. Thakor, and R. Benosman,
    “HFirst: A temporal approach to object recognition,” *IEEE transactions on pattern
    analysis and machine intelligence*, 2015.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. Thakor, 和 R. Benosman,
    “HFirst：一种时间性物体识别方法，” *IEEE 模式分析与机器智能期刊*, 2015。'
- en: '[67] B. Zhao, R. Ding, S. Chen, B. Linares-Barranco, and H. Tang, “Feedforward
    categorization on aer motion events using cortex-like features in a spiking neural
    network,” *TNNLS*, 2014.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] B. Zhao, R. Ding, S. Chen, B. Linares-Barranco, 和 H. Tang, “在尖峰神经网络中使用类似皮层的特征进行气流运动事件的前馈分类，”
    *TNNLS*, 2014。'
- en: '[68] D. Neil, M. Pfeiffer, and S.-C. Liu, “Phased LSTM: Accelerating recurrent
    network training for long or event-based sequences,” *NeurIPS*, 2016.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] D. Neil, M. Pfeiffer, 和 S.-C. Liu, “相位LSTM：加速长序列或事件驱动序列的递归网络训练，” *NeurIPS*,
    2016。'
- en: '[69] C. Brandli, L. Muller, and T. Delbruck, “Real-time, high-speed video decompression
    using a frame-and event-based davis sensor,” in *Proc. IEEE Int. Symp. Circuits
    Syst.*   IEEE, 2014.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] C. Brandli, L. Muller, 和 T. Delbruck, “使用帧和事件驱动的Davis传感器进行实时高速视频解压，”在
    *Proc. IEEE Int. Symp. Circuits Syst.* IEEE, 2014。'
- en: '[70] H. Li, G. Li, and L. Shi, “Super-resolution of spatiotemporal event-stream
    image,” *Neurocomputing*, 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] H. Li, G. Li, 和 L. Shi, “时空事件流图像的超分辨率，” *神经计算*, 2019。'
- en: '[71] Z. W. Wang, P. Duan, O. Cossairt, A. Katsaggelos, T. Huang, and B. Shi,
    “Joint filtering of intensity images and neuromorphic events for high-resolution
    noise-robust imaging,” in *CVPR*, 2020.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Z. W. Wang, P. Duan, O. Cossairt, A. Katsaggelos, T. Huang, 和 B. Shi,
    “强度图像和神经形态事件的联合滤波用于高分辨率噪声鲁棒成像，”在 *CVPR*, 2020。'
- en: '[72] P. Duan, Z. W. Wang, X. Zhou, Y. Ma, and B. Shi, “Eventzoom: Learning
    to denoise and super resolve neuromorphic events,” in *CVPR*, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] P. Duan, Z. W. Wang, X. Zhou, Y. Ma, 和 B. Shi, “Eventzoom：学习去噪和超分辨神经形态事件，”在
    *CVPR*, 2021。'
- en: '[73] S. Li, Y. Feng, Y. Li, Y. Jiang, C. Zou, and Y. Gao, “Event stream super-resolution
    via spatiotemporal constraint learning,” in *ICCV*, 2021.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] S. Li, Y. Feng, Y. Li, Y. Jiang, C. Zou, 和 Y. Gao, “通过时空约束学习进行事件流超分辨率，”在
    *ICCV*, 2021。'
- en: '[74] W. Weng, Y. Zhang, and Z. Xiong, “Boosting event stream super-resolution
    with a recurrent neural network,” in *ECCV*, 2022.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] W. Weng, Y. Zhang, 和 Z. Xiong, “通过递归神经网络提升事件流超分辨率，”在 *ECCV*, 2022。'
- en: '[75] J. Barrios-Avilés, A. Rosado-Muñoz, L. D. Medus, M. Bataller-Mompeán,
    and J. F. Guerrero-Martínez, “Less data same information for event-based sensors:
    A bioinspired filtering and data reduction algorithm,” *Sensors*, 2018.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] J. Barrios-Avilés, A. Rosado-Muñoz, L. D. Medus, M. Bataller-Mompeán,
    和 J. F. Guerrero-Martínez, “对事件驱动传感器的少量数据同样信息：一种仿生过滤和数据压缩算法，” *传感器*, 2018。'
- en: '[76] A. Khodamoradi and R. Kastner, “$o(n)$-space spatiotemporal filter for
    reducing noise in neuromorphic vision sensors,” *TETC*, 2018.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] A. Khodamoradi 和 R. Kastner, “用于减少神经形态视觉传感器噪声的$o(n)$空间时空滤波器，” *TETC*,
    2018。'
- en: '[77] H. Liu, C. Brandli, C. Li, S.-C. Liu, and T. Delbruck, “Design of a spatiotemporal
    correlation filter for event-based sensors,” in *ISCAS*.   IEEE, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] H. Liu, C. Brandli, C. Li, S.-C. Liu, 和 T. Delbruck，“用于事件相机的时空相关滤波器设计，”
    发表在*ISCAS*。IEEE，2015年。'
- en: '[78] D. Czech and G. Orchard, “Evaluating noise filtering for event-based asynchronous
    change detection image sensors,” in *BioRob*.   IEEE, 2016.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] D. Czech 和 G. Orchard，“评估基于事件的异步变化检测图像传感器的噪声过滤，” 发表在*BioRob*。IEEE，2016年。'
- en: '[79] T. Delbruck, “Frame-free dynamic digital vision,” in *Proceedings of Intl.
    Symp. on Secure-Life Electronics, Advanced Electronics for Quality Life and Society*,
    2008.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T. Delbruck，“无帧动态数字视觉，” 发表在*国际安全电子学研讨会，质量生活和社会的先进电子学*，2008年。'
- en: '[80] S. Guo and T. Delbruck, “Low cost and latency event camera background
    activity denoising,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S. Guo 和 T. Delbruck，“低成本和延迟事件相机背景活动去噪，” 发表在*IEEE Transactions on Pattern
    Analysis and Machine Intelligence*，2022年。'
- en: '[81] R. Baldwin, M. Almatrafi, V. Asari, and K. Hirakawa, “Event probability
    mask (epm) and event denoising convolutional neural network (edncnn) for neuromorphic
    cameras,” in *CVPR*, 2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] R. Baldwin, M. Almatrafi, V. Asari, 和 K. Hirakawa，“事件概率掩膜（epm）和用于神经形态相机的事件去噪卷积神经网络（edncnn），”
    发表在*CVPR*，2020年。'
- en: '[82] H. Fang, J. Wu, L. Li, J. Hou, W. Dong, and G. Shi, “Aednet: Asynchronous
    event denoising with spatial-temporal correlation among irregular data,” in *ACM
    MM*, 2022.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] H. Fang, J. Wu, L. Li, J. Hou, W. Dong, 和 G. Shi，“Aednet：利用不规则数据的时空相关性进行异步事件去噪，”
    发表在*ACM MM*，2022年。'
- en: '[83] S. Lin, Y. Ma, Z. Guo, and B. Wen, “Dvs-voltmeter: Stochastic process-based
    event simulator for dynamic vision sensors,” in *ECCV*.   Springer, 2022.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Lin, Y. Ma, Z. Guo, 和 B. Wen，“Dvs-voltmeter：基于随机过程的事件模拟器，用于动态视觉传感器，”
    发表在*ECCV*。Springer，2022年。'
- en: '[84] Y. Hu, S.-C. Liu, and T. Delbruck, “v2e: From video frames to realistic
    dvs events,” in *CVPR*, 2021.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Hu, S.-C. Liu, 和 T. Delbruck，“v2e：从视频帧到逼真的DVS事件，” 发表在*CVPR*，2021年。'
- en: '[85] H. Rebecq, D. Gehrig, and D. Scaramuzza, “Esim: an open event camera simulator,”
    in *CoRL*, 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] H. Rebecq, D. Gehrig, 和 D. Scaramuzza，“Esim：一个开放的事件相机模拟器，” 发表在*CoRL*，2018年。'
- en: '[86] H. Kim, A. Handa, R. Benosman, S.-H. Ieng, and A. J. Davison, “Simultaneous
    mosaicing and tracking with an event camera,” *J. Solid State Circ*, 2008.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] H. Kim, A. Handa, R. Benosman, S.-H. Ieng, 和 A. J. Davison，“事件相机的同时拼接和跟踪，”
    发表在*J. Solid State Circ*，2008年。'
- en: '[87] M. Cook, L. Gugelmann, F. Jug, C. Krautz, and A. Steger, “Interacting
    maps for fast visual interpretation,” in *IJCNN*, 2011.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Cook, L. Gugelmann, F. Jug, C. Krautz, 和 A. Steger，“快速视觉解释的交互式地图，”
    发表在*IJCNN*，2011年。'
- en: '[88] G. Munda, C. Reinbacher, and T. Pock, “Real-time intensity-image reconstruction
    for event cameras using manifold regularisation,” *IJCV*, 2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] G. Munda, C. Reinbacher, 和 T. Pock，“使用流形正则化的事件相机实时强度图像重建，” 发表在*IJCV*，2018年。'
- en: '[89] Z. Zhang, A. Yezzi, and G. Gallego, “Formulating event-based image reconstruction
    as a linear inverse problem with deep regularization using optical flow,” *TPAMI*,
    2022.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Z. Zhang, A. Yezzi, 和 G. Gallego，“将基于事件的图像重建表述为线性逆问题，使用光流进行深度正则化，” 发表在*TPAMI*，2022年。'
- en: '[90] Rebecq, Henri and Ranftl, René and Koltun, Vladlen and Scaramuzza, Davide,
    “High speed and high dynamic range video with an event camera,” *TPAMI*, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Rebecq, Henri 和 Ranftl, René 和 Koltun, Vladlen 和 Scaramuzza, Davide，“使用事件相机的高速和高动态范围视频，”
    发表在*TPAMI*，2019年。'
- en: '[91] T. Stoffregen, C. Scheerlinck, D. Scaramuzza, T. Drummond, N. Barnes,
    L. Kleeman, and R. Mahony, “Reducing the sim-to-real gap for event cameras,” in
    *ECCV*.   Springer, 2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] T. Stoffregen, C. Scheerlinck, D. Scaramuzza, T. Drummond, N. Barnes,
    L. Kleeman, 和 R. Mahony，“减少事件相机的模拟与现实差距，” 发表在*ECCV*。Springer，2020年。'
- en: '[92] F. Paredes-Vallés and G. C. de Croon, “Back to event basics: Self-supervised
    learning of image reconstruction for event cameras via photometric constancy,”
    in *CVPR*, 2021.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] F. Paredes-Vallés 和 G. C. de Croon，“回到事件基础：通过光度一致性自监督学习图像重建的事件相机，” 发表在*CVPR*，2021年。'
- en: '[93] S. Tulyakov, D. Gehrig, S. Georgoulis, J. Erbach, M. Gehrig, Y. Li, and
    D. Scaramuzza, “Time Lens: Event-based video frame interpolation,” in *CVPR*,
    2021.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Tulyakov, D. Gehrig, S. Georgoulis, J. Erbach, M. Gehrig, Y. Li, 和
    D. Scaramuzza，“时间透镜：基于事件的视频帧插值，” 发表在*CVPR*，2021年。'
- en: '[94] G. Paikin, Y. Ater, R. Shaul, and E. Soloveichik, “EFI-Net: Video Frame
    Interpolation from Fusion of Events and Frames,” in *CVPR*, 2021.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] G. Paikin, Y. Ater, R. Shaul, 和 E. Soloveichik，“EFI-Net：从事件和帧融合的视频帧插值，”
    发表在*CVPR*，2021年。'
- en: '[95] Z. Yu, Y. Zhang, D. Liu, D. Zou, X. Chen, Y. Liu, and J. S. Ren, “Training
    weakly supervised video frame interpolation with events,” in *ICCV*, 2021.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Z. Yu, Y. Zhang, D. Liu, D. Zou, X. Chen, Y. Liu, 和 J. S. Ren，“使用事件训练弱监督视频帧插值，”
    发表在*ICCV*，2021年。'
- en: '[96] W. He, K. You, Z. Qiao, X. Jia, Z. Zhang, W. Wang, H. Lu, Y. Wang, and
    J. Liao, “Timereplayer: Unlocking the potential of event cameras for video interpolation,”
    in *CVPR*, 2022.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] W. He, K. You, Z. Qiao, X. Jia, Z. Zhang, W. Wang, H. Lu, Y. Wang, 和 J.
    Liao，“Timereplayer: 解锁事件相机在视频插值中的潜力，”发表于 *CVPR*，2022年。'
- en: '[97] S. Tulyakov, A. Bochicchio, D. Gehrig, S. Georgoulis, Y. Li, and D. Scaramuzza,
    “Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow
    and Multi-scale Fusion,” in *CVPR*, 2022.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] S. Tulyakov, A. Bochicchio, D. Gehrig, S. Georgoulis, Y. Li, 和 D. Scaramuzza，“Time
    Lens++：基于事件的帧插值与参数非线性流和多尺度融合，”发表于 *CVPR*，2022年。'
- en: '[98] S. Wu, K. You, W. He, C. Yang, Y. Tian, Y. Wang, Z. Zhang, and J. Liao,
    “Video interpolation by event-driven anisotropic adjustment of optical flow,”
    in *ECCV*.   Springer, 2022.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] S. Wu, K. You, W. He, C. Yang, Y. Tian, Y. Wang, Z. Zhang, 和 J. Liao，“通过事件驱动的各向异性调整光流进行视频插值，”发表于
    *ECCV*。Springer，2022年。'
- en: '[99] S. Lin, J. Zhang, J. Pan, Z. Jiang, D. Zou, Y. Wang, J. Chen, and J. Ren,
    “Learning event-driven video deblurring and interpolation,” in *ECCV*.   Springer,
    2020.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] S. Lin, J. Zhang, J. Pan, Z. Jiang, D. Zou, Y. Wang, J. Chen, 和 J. Ren，“学习事件驱动的视频去模糊和插值，”发表于
    *ECCV*。Springer，2020年。'
- en: '[100] C. Song, Q. Huang, and C. Bajaj, “E-cir: Event-enhanced continuous intensity
    recovery,” in *CVPR*, 2022.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] C. Song, Q. Huang, 和 C. Bajaj，“E-cir：事件增强的连续强度恢复，”发表于 *CVPR*，2022年。'
- en: '[101] X. Zhang and L. Yu, “Unifying motion deblurring and frame interpolation
    with events,” in *CVPR*, 2022.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Zhang 和 L. Yu，“用事件统一运动去模糊和帧插值，”发表于 *CVPR*，2022年。'
- en: '[102] Y. Zou, Y. Zheng, T. Takatani, and Y. Fu, “Learning to reconstruct high
    speed and high dynamic range videos from events,” in *CVPR*, 2021.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Y. Zou, Y. Zheng, T. Takatani, 和 Y. Fu，“学习从事件中重建高速和高动态范围视频，”发表于 *CVPR*，2021年。'
- en: '[103] L. Yu, W. Yang *et al.*, “Event-based high frame-rate video reconstruction
    with a novel cycle-event network,” in *ICIP*.   IEEE, 2020.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] L. Yu, W. Yang *等*，“基于事件的高帧率视频重建与新颖的周期事件网络，”发表于 *ICIP*。IEEE，2020年。'
- en: '[104] A. Z. Zhu, Z. Wang, K. Khant, and K. Daniilidis, “Eventgan: Leveraging
    large scale image datasets for event cameras,” in *ICCP*.   IEEE, 2021.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] A. Z. Zhu, Z. Wang, K. Khant, 和 K. Daniilidis，“Eventgan：利用大规模图像数据集进行事件相机研究，”发表于
    *ICCP*。IEEE，2021年。'
- en: '[105] H. C. Duwek, A. Shalumov, and E. E. Tsur, “Image reconstruction from
    neuromorphic event cameras using laplacian-prediction and poisson integration
    with spiking and artificial neural networks,” in *CVPR*, 2021.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] H. C. Duwek, A. Shalumov, 和 E. E. Tsur，“使用拉普拉斯预测和泊松积分与脉冲和人工神经网络进行神经形态事件相机的图像重建，”发表于
    *CVPR*，2021年。'
- en: '[106] L. Pan, C. Scheerlinck, X. Yu, R. Hartley, M. Liu, and Y. Dai, “Bringing
    a blurry frame alive at high frame-rate with an event camera,” in *CVPR*, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] L. Pan, C. Scheerlinck, X. Yu, R. Hartley, M. Liu, 和 Y. Dai，“用事件相机在高帧率下恢复模糊帧，”发表于
    *CVPR*，2019年。'
- en: '[107] J. Choi, K.-J. Yoon *et al.*, “Learning to super resolve intensity images
    from events,” in *CVPR*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] J. Choi, K.-J. Yoon *等*，“学习从事件中超分辨率强度图像，”发表于 *CVPR*，2020年。'
- en: '[108] Y. Jing, Y. Yang, X. Wang, M. Song, and D. Tao, “Turning frequency to
    resolution: Video super-resolution via event cameras,” in *CVPR*, 2021.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Y. Jing, Y. Yang, X. Wang, M. Song, 和 D. Tao，“将频率转化为分辨率：通过事件相机的视频超分辨率，”发表于
    *CVPR*，2021年。'
- en: '[109] L. Wang, T.-K. Kim, and K.-J. Yoon, “EventSR: From Asynchronous Events
    to Image Reconstruction, Restoration, and Super-Resolution via End-to-End Adversarial
    Learning,” in *CVPR*, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] L. Wang, T.-K. Kim, 和 K.-J. Yoon，“EventSR：通过端到端对抗学习从异步事件到图像重建、恢复和超分辨率，”发表于
    *CVPR*，2020年。'
- en: '[110] M. Mostafavi, L. Wang, and K.-J. Yoon, “Learning to reconstruct hdr images
    from events, with applications to depth and flow prediction,” *IJCV*, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] M. Mostafavi, L. Wang, 和 K.-J. Yoon，“从事件中学习重建HDR图像，应用于深度和流预测，” *IJCV*，2021年。'
- en: '[111] B. Wang, J. He, L. Yu, G.-S. Xia, and W. Yang, “Event enhanced high-quality
    image recovery,” in *ECCV*, 2020.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] B. Wang, J. He, L. Yu, G.-S. Xia, 和 W. Yang，“事件增强的高质量图像恢复，”发表于 *ECCV*，2020年。'
- en: '[112] J. Han, Y. Yang, C. Zhou, C. Xu, and B. Shi, “Evintsr-net: Event guided
    multiple latent frames reconstruction and super-resolution,” in *ICCV*, 2021.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Han, Y. Yang, C. Zhou, C. Xu, 和 B. Shi，“Evintsr-net：事件引导的多潜在帧重建与超分辨率，”发表于
    *ICCV*，2021年。'
- en: '[113] Y. Lu, Z. Wang, M. Liu, H. Wang, and L. Wang, “Learning Spatial-Temporal
    Implicit Neural Representations for Event-Guided Video Super-Resolution,” in *CVPR*,
    2023.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Y. Lu, Z. Wang, M. Liu, H. Wang, 和 L. Wang，“为事件引导的视频超分辨率学习时空隐式神经表示，”发表于
    *CVPR*，2023年。'
- en: '[114] M. Gehrig, M. Millhäusler, D. Gehrig, and D. Scaramuzza, “E-raft: Dense
    optical flow from event cameras,” in *3DV*.   IEEE, 2021.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] M. Gehrig, M. Millhäusler, D. Gehrig, 和 D. Scaramuzza，“E-raft：从事件相机获取密集光流，”发表于
    *3DV*。IEEE，2021年。'
- en: '[115] M. Jin, G. Meishvili, and P. Favaro, “Learning to extract a video sequence
    from a single motion-blurred image,” in *CVPR*, 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] M. Jin, G. Meishvili, 和 P. Favaro, “从单个运动模糊图像中学习提取视频序列，” 在 *CVPR*, 2018。'
- en: '[116] F. Xu, L. Yu, B. Wang, W. Yang, G.-S. Xia, X. Jia, Z. Qiao, and J. Liu,
    “Motion deblurring with real events,” in *ICCV*, 2021.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] F. Xu, L. Yu, B. Wang, W. Yang, G.-S. Xia, X. Jia, Z. Qiao, 和 J. Liu,
    “利用真实事件进行运动去模糊处理，” 在 *ICCV*, 2021。'
- en: '[117] Z. Jiang, Y. Zhang, D. Zou, J. Ren, J. Lv, and Y. Liu, “Learning event-based
    motion deblurring,” in *CVPR*, 2020.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Z. Jiang, Y. Zhang, D. Zou, J. Ren, J. Lv, 和 Y. Liu, “基于事件的运动去模糊学习，”
    在 *CVPR*, 2020。'
- en: '[118] W. Shang, D. Ren, D. Zou, J. S. Ren, P. Luo, and W. Zuo, “Bringing events
    into video deblurring with non-consecutively blurry frames,” in *ICCV*, 2021.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] W. Shang, D. Ren, D. Zou, J. S. Ren, P. Luo, 和 W. Zuo, “将事件引入视频去模糊处理中的非连续模糊帧，”
    在 *ICCV*, 2021。'
- en: '[119] T. Kim, J. Lee, L. Wang, and K.-J. Yoon, “Event-guided Deblurring of
    Unknown Exposure Time Videos,” in *ECCV*, 2022.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] T. Kim, J. Lee, L. Wang, 和 K.-J. Yoon, “事件引导的未知曝光时间视频去模糊处理，” 在 *ECCV*,
    2022。'
- en: '[120] G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, “Converting static
    image datasets to spiking neuromorphic datasets using saccades,” *Frontiers in
    Neuroscience*, 2015.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] G. Orchard, A. Jayawant, G. K. Cohen, 和 N. Thakor, “使用眼动将静态图像数据集转换为脉冲神经形态数据集，”
    *Frontiers in Neuroscience*, 2015。'
- en: '[121] T. Serrano-Gotarredona and B. Linares-Barranco, “Poker-DVS and MNIST-DVS.
    Their history, how they were made, and other details,” *Frontiers in Neuroscience*,
    2015.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] T. Serrano-Gotarredona 和 B. Linares-Barranco, “Poker-DVS 和 MNIST-DVS。它们的历史，它们是如何制作的以及其他细节，”
    *Frontiers in Neuroscience*, 2015。'
- en: '[122] H. Li, H. Liu, X. Ji, G. Li, and L. Shi, “Cifar10-dvs: an event-stream
    dataset for object classification,” *Frontiers in Neuroscience*, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] H. Li, H. Liu, X. Ji, G. Li, 和 L. Shi, “Cifar10-dvs: 一种用于物体分类的事件流数据集，”
    *Frontiers in Neuroscience*, 2017。'
- en: '[123] B. Ramesh, H. Yang, G. Orchard, N. A. Le Thi, S. Zhang, and C. Xiang,
    “Dart: distribution aware retinal transform for event-based cameras,” *TPAMI*,
    2019.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] B. Ramesh, H. Yang, G. Orchard, N. A. Le Thi, S. Zhang, 和 C. Xiang, “Dart:
    面向事件的摄像机的分布感知视网膜变换，” *TPAMI*, 2019。'
- en: '[124] Y. Li, H. Zhou, B. Yang, Y. Zhang, Z. Cui, H. Bao, and G. Zhang, “Graph-based
    asynchronous event processing for rapid object recognition,” in *ICCV*, 2021.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Y. Li, H. Zhou, B. Yang, Y. Zhang, Z. Cui, H. Bao, 和 G. Zhang, “基于图的异步事件处理用于快速物体识别，”
    在 *ICCV*, 2021。'
- en: '[125] S. Schaefer, D. Gehrig, and D. Scaramuzza, “AEGNN: Asynchronous Event-based
    Graph Neural Networks,” in *CVPR*, 2022.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] S. Schaefer, D. Gehrig, 和 D. Scaramuzza, “AEGNN: 异步事件驱动的图神经网络，” 在 *CVPR*,
    2022。'
- en: '[126] J. Han, C. Zhou, P. Duan, Y. Tang, C. Xu, C. Xu, T. Huang, and B. Shi,
    “Neuromorphic Camera Guided High Dynamic Range Imaging,” *CVPR*, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Han, C. Zhou, P. Duan, Y. Tang, C. Xu, C. Xu, T. Huang, 和 B. Shi,
    “神经形态相机引导的高动态范围成像，” *CVPR*, 2020。'
- en: '[127] Z. Wang, Y. Ng, C. Scheerlinck, and R. E. Mahony, “An Asynchronous Kalman
    Filter for Hybrid Event Cameras,” *ICCV*, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Z. Wang, Y. Ng, C. Scheerlinck, 和 R. E. Mahony, “一种异步卡尔曼滤波器用于混合事件相机，”
    *ICCV*, 2020。'
- en: '[128] L. Wang and K.-J. Yoon, “Deep Learning for HDR Imaging: State-of-the-Art
    and Future Trends,” *TPAMI*, 2021.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] L. Wang 和 K.-J. Yoon, “HDR成像的深度学习：最新进展与未来趋势，” *TPAMI*, 2021。'
- en: '[129] Y. Yang, J. Han, J. Liang, I. Sato, and B. Shi, “Learning event guided
    high dynamic range video reconstruction,” in *CVPR*, 2023, pp. 13 924–13 934.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. Yang, J. Han, J. Liang, I. Sato, 和 B. Shi, “基于事件引导的高动态范围视频重建，” 在 *CVPR*,
    2023, 第 13,924–13,934 页。'
- en: '[130] B. Xie, Y. Deng, Z. Shao, H. Liu, and Y. Li, “VMV-GCN: Volumetric Multi-View
    Based Graph CNN for Event Stream Classification,” *RAL*, 2022.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] B. Xie, Y. Deng, Z. Shao, H. Liu, 和 Y. Li, “VMV-GCN: 基于体积多视角的图形 CNN 用于事件流分类，”
    *RAL*, 2022。'
- en: '[131] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, “Asynchronous
    convolutional networks for object detection in neuromorphic cameras,” in *CVPRW*,
    2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. Cannici, M. Ciccone, A. Romanoni, 和 M. Matteucci, “用于神经形态相机中的物体检测的异步卷积网络，”
    在 *CVPRW*, 2019。'
- en: '[132] N. Salvatore and J. Fletcher, “Learned Event-based Visual Perception
    for Improved Space Object Detection,” in *WACV*, 2022.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] N. Salvatore 和 J. Fletcher, “基于学习的事件视觉感知用于改进空间物体检测，” 在 *WACV*, 2022。'
- en: '[133] Z. Liang, H. Cao, C. Yang, Z. Zhang, and G. Chen, “Global-local Feature
    Aggregation for Event-based Object Detection on EventKITTI,” in *MFI*, 2022.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Z. Liang, H. Cao, C. Yang, Z. Zhang, 和 G. Chen, “基于事件的物体检测中的全局-局部特征聚合，在
    EventKITTI 上，” 在 *MFI*, 2022。'
- en: '[134] J. Li, J. Li, L. Zhu, X. Xiang, T. Huang, and Y. Tian, “Asynchronous
    Spatio-Temporal Memory Network for Continuous Event-Based Object Detection,” *TIP*,
    2022.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] J. Li, J. Li, L. Zhu, X. Xiang, T. Huang, 和 Y. Tian, “用于连续事件驱动物体检测的异步时空记忆网络，”
    *TIP*, 2022。'
- en: '[135] Z. Jiang, P. Xia, K. Huang, W. Stechele, G. Chen, Z. Bing, and A. Knoll,
    “Mixed frame-/event-driven fast pedestrian detection,” in *ICRA*, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Z. Jiang, P. Xia, K. Huang, W. Stechele, G. Chen, Z. Bing, 和 A. Knoll,
    “混合帧-/事件驱动的快速行人检测，” 收录于 *ICRA*, 2019。'
- en: '[136] J. Li, S. Dong, Z. Yu, Y. Tian, and T. Huang, “Event-based vision enhanced:
    A joint detection framework in autonomous driving,” in *ICME*, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] J. Li, S. Dong, Z. Yu, Y. Tian, 和 T. Huang, “事件增强视觉：自动驾驶中的联合检测框架，” 收录于
    *ICME*, 2019。'
- en: '[137] A. Tomy, A. Paigwar, K. S. Mann, A. Renzaglia, and C. Laugier, “Fusing
    Event-based and RGB camera for Robust Object Detection in Adverse Conditions,”
    in *ICRA*.   IEEE, 2022.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. Tomy, A. Paigwar, K. S. Mann, A. Renzaglia, 和 C. Laugier, “融合基于事件和RGB相机以在恶劣条件下进行鲁棒对象检测，”
    收录于 *ICRA*。IEEE, 2022。'
- en: '[138] J. Kim, I. Hwang, and Y. M. Kim, “Ev-TTA: Test-Time Adaptation for Event-Based
    Object Recognition,” in *CVPR*, 2022.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] J. Kim, I. Hwang, 和 Y. M. Kim, “Ev-TTA: 事件基础对象识别的测试时适应，” 收录于 *CVPR*,
    2022。'
- en: '[139] M. Planamente, C. Plizzari, M. Cannici, M. Ciccone, F. Strada, A. Bottino,
    M. Matteucci, and B. Caputo, “Da4event: towards bridging the sim-to-real gap for
    event cameras using domain adaptation,” *RAL*, 2021.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] M. Planamente, C. Plizzari, M. Cannici, M. Ciccone, F. Strada, A. Bottino,
    M. Matteucci, 和 B. Caputo, “Da4event: 通过领域适应缩小事件相机的模拟到现实差距，” *RAL*, 2021。'
- en: '[140] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,”
    in *Sens. Fusion*.   Spie, 1992.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] P. J. Besl 和 N. D. McKay, “3D形状配准的方法，” 收录于 *Sens. Fusion*。Spie, 1992。'
- en: '[141] B. Kueng, E. Mueggler, G. Gallego, and D. Scaramuzza, “Low-latency visual
    odometry using event-based feature tracks,” in *IROS*.   IEEE, 2016.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] B. Kueng, E. Mueggler, G. Gallego, 和 D. Scaramuzza, “基于事件的特征跟踪的低延迟视觉里程计，”
    收录于 *IROS*。IEEE, 2016。'
- en: '[142] Z. Ni, A. Bolopion, J. Agnus, R. Benosman, and S. Régnier, “Asynchronous
    event-based visual shape tracking for stable haptic feedback in microrobotics,”
    *IEEE Trans. Robot.*, 2012.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Z. Ni, A. Bolopion, J. Agnus, R. Benosman, 和 S. Régnier, “基于异步事件的视觉形状跟踪用于微型机器人中的稳定触觉反馈，”
    *IEEE Trans. Robot.*, 2012。'
- en: '[143] A. Z. Zhu, N. Atanasov, and K. Daniilidis, “Event-based feature tracking
    with probabilistic data association,” in *IEEE ICRA*.   IEEE, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] A. Z. Zhu, N. Atanasov, 和 K. Daniilidis, “基于事件的特征跟踪与概率数据关联，” 收录于 *IEEE
    ICRA*。IEEE, 2017。'
- en: '[144] Y. Dong and T. Zhang, “Standard and Event Cameras Fusion for Feature
    Tracking,” in *Int. Conf. Mach. Vis. Appl.*, 2021.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Dong 和 T. Zhang, “标准与事件相机融合用于特征跟踪，” 收录于 *Int. Conf. Mach. Vis. Appl.*,
    2021。'
- en: '[145] J. Chui, S. Klenk, and D. Cremers, “Event-Based Feature Tracking in Continuous
    Time with Sliding Window Optimization.” [Online]. Available: [http://arxiv.org/abs/2107.04536.](http://arxiv.org/abs/2107.04536.)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Chui, S. Klenk, 和 D. Cremers, “基于事件的特征跟踪在连续时间中的滑动窗口优化。” [在线]。可用链接:
    [http://arxiv.org/abs/2107.04536.](http://arxiv.org/abs/2107.04536.)'
- en: '[146] H. Seok and J. Lim, “Robust feature tracking in dvs event stream using
    bézier mapping,” in *WACV*, 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] H. Seok 和 J. Lim, “使用贝塞尔映射的DVS事件流中的鲁棒特征跟踪，” 收录于 *WACV*, 2020。'
- en: '[147] I. Alzugaray and M. Chli, “Haste: multi-hypothesis asynchronous speeded-up
    tracking of events,” in *BMVC*, 2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] I. Alzugaray 和 M. Chli, “Haste: 多假设异步加速事件跟踪，” 收录于 *BMVC*, 2020。'
- en: '[148] S. Hu, Y. Kim, H. Lim, A. J. Lee, and H. Myung, “eCDT: Event Clustering
    for Simultaneous Feature Detection and Tracking,” in *IROS*.   IEEE, 2022.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] S. Hu, Y. Kim, H. Lim, A. J. Lee, 和 H. Myung, “eCDT: 用于同时特征检测和跟踪的事件聚类，”
    收录于 *IROS*。IEEE, 2022。'
- en: '[149] “Pseudo-labels for supervised learning on dynamic vision sensor data,
    applied to object detection under ego-motion, author=Chen, Nicholas FY,” in *CVPRW*,
    2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] “伪标签用于动态视觉传感器数据的监督学习，应用于自我运动下的对象检测，作者=Chen, Nicholas FY，” 收录于 *CVPRW*,
    2018。'
- en: '[150] M. Gehrig and D. Scaramuzza, “Recurrent vision transformers for object
    detection with event cameras,” in *CVPR*, 2023.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] M. Gehrig 和 D. Scaramuzza, “用于事件相机的对象检测的递归视觉变换器，” 收录于 *CVPR*, 2023。'
- en: '[151] H. Cao, G. Chen, J. Xia, G. Zhuang, and A. Knoll, “Fusion-based feature
    attention gate component for vehicle detection based on event camera,” *IEEE Sensors
    Journal*, 2021.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] H. Cao, G. Chen, J. Xia, G. Zhuang, 和 A. Knoll, “基于事件相机的车辆检测的融合特征注意力门组件，”
    *IEEE Sensors Journal*, 2021。'
- en: '[152] M. Liu, N. Qi, Y. Shi, and B. Yin, “An attention fusion network for event-based
    vehicle object detection,” in *ICIP*.   IEEE, 2021.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] M. Liu, N. Qi, Y. Shi, 和 B. Yin, “一种基于事件的车辆目标检测的注意力融合网络，” 收录于 *ICIP*。IEEE,
    2021。'
- en: '[153] “Event-driven ball detection and gaze fixation in clutter, author=Glover,
    Arren and Bartolozzi, Chiara,” in *IROS*.   IEEE, 2016.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] “基于事件的球体检测与视线固定，作者=Glover, Arren 和 Bartolozzi, Chiara，” 收录于 *IROS*。IEEE,
    2016。'
- en: '[154] V. Vasco, A. Glover, E. Mueggler, D. Scaramuzza, L. Natale, and C. Bartolozzi,
    “Independent motion detection with event-driven cameras,” in *ICAR*.   IEEE, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] V. Vasco, A. Glover, E. Mueggler, D. Scaramuzza, L. Natale, 和 C. Bartolozzi，"使用事件驱动摄像头的独立运动检测"，发表于*ICAR*。IEEE，2017年。'
- en: '[155] T. Stoffregen, G. Gallego, T. Drummond, L. Kleeman, and D. Scaramuzza,
    “Event-Based Motion Segmentation by Motion Compensation,” in *ICCV*, 2019.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] T. Stoffregen, G. Gallego, T. Drummond, L. Kleeman, 和 D. Scaramuzza，"通过运动补偿的基于事件的运动分割"，发表于*ICCV*，2019年。'
- en: '[156] R. Jiang, X. Mou, S. Shi, Y. Zhou, Q. Wang, M. Dong, and S. Chen, “Object
    tracking on event cameras with offline–online learning,” *CAAI Transactions on
    Intelligence Technology*, 2020.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] R. Jiang, X. Mou, S. Shi, Y. Zhou, Q. Wang, M. Dong, 和 S. Chen，"基于事件摄像头的目标跟踪与离线–在线学习"，*CAAI
    智能技术期刊*，2020年。'
- en: '[157] N. J. Sanket, C. M. Parameshwara, C. D. Singh, A. V. Kuruttukulam, C. Fermüller,
    D. Scaramuzza, and Y. Aloimonos, “EVDodgeNet: Deep Dynamic Obstacle Dodging with
    Event Cameras,” in *ICRA*.   IEEE, 2020.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] N. J. Sanket, C. M. Parameshwara, C. D. Singh, A. V. Kuruttukulam, C.
    Fermüller, D. Scaramuzza, 和 Y. Aloimonos，"EVDodgeNet: 使用事件摄像头的深度动态障碍物规避"，发表于*ICRA*。IEEE，2020年。'
- en: '[158] C. Walters and S. Hadfield, “Evreflex: Dense time-to-impact prediction
    for event-based obstacle avoidance,” in *IROS*, 2021.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] C. Walters 和 S. Hadfield，"Evreflex: 基于事件的障碍物规避的密集时间到影响预测"，发表于*IROS*，2021年。'
- en: '[159] Z. Wang, F. C. Ojeda, A. Bisulco, D. Lee, C. J. Taylor, K. Daniilidis,
    M. A. Hsieh, D. D. Lee, and V. Isler, “EV-Catcher: High-Speed Object Catching
    Using Low-Latency Event-Based Neural Networks,” *RAL*, 2022.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Z. Wang, F. C. Ojeda, A. Bisulco, D. Lee, C. J. Taylor, K. Daniilidis,
    M. A. Hsieh, D. D. Lee, 和 V. Isler，"EV-Catcher: 使用低延迟事件驱动神经网络的高速物体捕捉"，*RAL*，2022年。'
- en: '[160] A. Bisulco, F. C. Ojeda, V. Isler, and D. D. Lee, “Fast motion understanding
    with spatiotemporal neural networks and dynamic vision sensors,” in *ICRA*.   IEEE,
    2021.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. Bisulco, F. C. Ojeda, V. Isler, 和 D. D. Lee，"使用时空神经网络和动态视觉传感器进行快速运动理解"，发表于*ICRA*。IEEE，2021年。'
- en: '[161] Z. Wan, Y. Dai, and Y. Mao, “Learning Dense and Continuous Optical Flow
    From an Event Camera,” *TIP*, 2022.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Z. Wan, Y. Dai, 和 Y. Mao，"从事件摄像头学习密集且连续的光流"，*TIP*，2022年。'
- en: '[162] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “Unsupervised event-based
    learning of optical flow, depth, and egomotion,” in *CVPR*, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] A. Z. Zhu, L. Yuan, K. Chaney, 和 K. Daniilidis，"基于事件的光流、深度和自我运动的无监督学习"，发表于*CVPR*，2019年。'
- en: '[163] C. Lee, A. K. Kosta, A. Z. Zhu, K. Chaney, K. Daniilidis, and K. Roy,
    “Spike-flownet: event-based optical flow estimation with energy-efficient hybrid
    neural networks,” in *ECCV*.   Springer, 2020.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] C. Lee, A. K. Kosta, A. Z. Zhu, K. Chaney, K. Daniilidis, 和 K. Roy，"Spike-flownet:
    使用能效混合神经网络的基于事件的光流估计"，发表于*ECCV*。Springer，2020年。'
- en: '[164] J. Hagenaars, F. Paredes-Vallés, and G. De Croon, “Self-supervised learning
    of event-based optical flow with spiking neural networks,” 2021.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] J. Hagenaars, F. Paredes-Vallés, 和 G. De Croon，"基于事件的光流自监督学习与脉冲神经网络"，2021年。'
- en: '[165] Y. Deng, H. Chen, H. Chen, and Y. Li, “Learning from images: A distillation
    learning framework for event cameras,” *TIP*, 2021.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Y. Deng, H. Chen, H. Chen, 和 Y. Li，"从图像中学习：事件摄像头的蒸馏学习框架"，*TIP*，2021年。'
- en: '[166] Z. Li, J. Shen, and R. Liu, “A lightweight network to learn optical flow
    from event data,” in *ICPR*.   IEEE, 2021.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Z. Li, J. Shen, 和 R. Liu，"一个轻量级网络以从事件数据中学习光流"，发表于*ICPR*。IEEE，2021年。'
- en: '[167] Z. Ding, R. Zhao, J. Zhang, T. Gao, R. Xiong, Z. Yu, and T. Huang, “Spatio-temporal
    recurrent networks for event-based optical flow estimation,” in *AAAI*, 2022.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Z. Ding, R. Zhao, J. Zhang, T. Gao, R. Xiong, Z. Yu, 和 T. Huang，"基于事件的光流估计的时空递归网络"，发表于*AAAI*，2022年。'
- en: '[168] L. Pan, M. Liu, and R. Hartley, “Single image optical flow estimation
    with an event camera,” in *CVPR*.   IEEE, 2020.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] L. Pan, M. Liu, 和 R. Hartley，"使用事件摄像头的单幅图像光流估计"，发表于*CVPR*。IEEE，2020年。'
- en: '[169] S. Shiba, Y. Aoki, and G. Gallego, “Secrets of event-based optical flow,”
    in *ECCV*.   Springer, 2022.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] S. Shiba, Y. Aoki, 和 G. Gallego，"基于事件的光流的秘密"，发表于*ECCV*。Springer，2022年。'
- en: '[170] C. Lee, A. K. Kosta, and K. Roy, “Fusion-FlowNet: Energy-efficient optical
    flow estimation using sensor fusion and deep fused spiking-analog network architectures,”
    in *ICRA*.   IEEE, 2022.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] C. Lee, A. K. Kosta, 和 K. Roy，"Fusion-FlowNet: 使用传感器融合和深度融合脉冲模拟网络架构的能效光流估计"，发表于*ICRA*。IEEE，2022年。'
- en: '[171] N. Messikommer, D. Gehrig, M. Gehrig, and D. Scaramuzza, “Bridging the
    gap between events and frames through unsupervised domain adaptation,” *RAL*,
    2022.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] N. Messikommer, D. Gehrig, M. Gehrig, 和 D. Scaramuzza，"通过无监督领域适应弥合事件与帧之间的差距"，*RAL*，2022年。'
- en: '[172] Z. Sun, N. Messikommer, D. Gehrig, and D. Scaramuzza, “ESS: Learning
    Event-Based Semantic Segmentation from Still Images,” in *ECCV*.   Springer, 2022.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Z. Sun, N. Messikommer, D. Gehrig, 和 D. Scaramuzza, “ESS：从静态图像中学习事件驱动的语义分割,”
    在 *ECCV*。   Springer, 2022。'
- en: '[173] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos,
    “Image segmentation using deep learning: A survey,” *CVPR*, 2021.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, 和 D. Terzopoulos,
    “使用深度学习的图像分割：综述,” *CVPR*, 2021。'
- en: '[174] S. Jia, “Event Camera Survey and Extension Application to Semantic Segmentation,”
    in *IPMV*, 2022.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] S. Jia, “事件相机综述及其在语义分割中的扩展应用,” 在 *IPMV*, 2022。'
- en: '[175] A. Deliege, A. Cioppa, S. Giancola, M. J. Seikavandi, J. V. Dueholm,
    K. Nasrollahi, B. Ghanem, T. B. Moeslund, and M. Van Droogenbroeck, “Soccernet-v2:
    A dataset and benchmarks for holistic understanding of broadcast soccer videos,”
    in *CVPR*, 2021.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] A. Deliege, A. Cioppa, S. Giancola, M. J. Seikavandi, J. V. Dueholm,
    K. Nasrollahi, B. Ghanem, T. B. Moeslund, 和 M. Van Droogenbroeck, “Soccernet-v2：用于全方位理解广播足球视频的数据集和基准,”
    在 *CVPR*, 2021。'
- en: '[176] J. Zhang, K. Yang, and R. Stiefelhagen, “Exploring event-driven dynamic
    context for accident scene segmentation,” *TITS*, 2021.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] J. Zhang, K. Yang, 和 R. Stiefelhagen, “探索事件驱动的动态上下文用于事故场景分割,” *TITS*,
    2021。'
- en: '[177] J. Binas, D. Neil, S.-C. Liu, and T. Delbruck, “DDD17: End-to-end DAVIS
    driving dataset,” *arXiv:1711.01458*, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] J. Binas, D. Neil, S.-C. Liu, 和 T. Delbruck, “DDD17：端到端DAVIS驾驶数据集,” *arXiv:1711.01458*,
    2017。'
- en: '[178] L. Wang, Y. Chae, S.-H. Yoon, T.-K. Kim, and K.-J. Yoon, “Evdistill:
    Asynchronous events to end-task learning via bidirectional reconstruction-guided
    cross-modal knowledge distillation,” in *CVPR*, 2021.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] L. Wang, Y. Chae, S.-H. Yoon, T.-K. Kim, 和 K.-J. Yoon, “Evdistill：通过双向重建引导的跨模态知识蒸馏实现异步事件到最终任务学习,”
    在 *CVPR*, 2021。'
- en: '[179] L. Wang, D. Li, Y. Zhu, L. Tian, and Y. Shan, “Dual super-resolution
    learning for semantic segmentation,” in *CVPR*, 2020.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] L. Wang, D. Li, Y. Zhu, L. Tian, 和 Y. Shan, “用于语义分割的双重超分辨率学习,” 在 *CVPR*,
    2020。'
- en: '[180] G. Gallego, H. Rebecq, and D. Scaramuzza, “A unifying contrast maximization
    framework for event cameras, with applications to motion, depth, and optical flow
    estimation,” in *CVPR*, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] G. Gallego, H. Rebecq, 和 D. Scaramuzza, “统一对比度最大化框架用于事件相机，应用于运动、深度和光流估计,”
    在 *CVPR*, 2018。'
- en: '[181] T. Stoffregen and L. Kleeman, “Event cameras, contrast maximization and
    reward functions: An analysis,” in *CVPR*, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] T. Stoffregen 和 L. Kleeman, “事件相机、对比度最大化和奖励函数：分析,” 在 *CVPR*, 2019。'
- en: '[182] D. R. Kepple, D. Lee, C. Prepsius, V. Isler, I. M. Park, and D. D. Lee,
    “Jointly learning visual motion and confidence from local patches in event cameras,”
    in *ECCV*.   Springer, 2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] D. R. Kepple, D. Lee, C. Prepsius, V. Isler, I. M. Park, 和 D. D. Lee,
    “从事件相机中联合学习视觉运动和置信度,” 在 *ECCV*。   Springer, 2020。'
- en: '[183] G. Haessig, A. Cassidy, R. Alvarez, R. Benosman, and G. Orchard, “Spiking
    optical flow for event-based sensors using IBM’s TrueNorth neurosynaptic system,”
    *T BIOMED CIRC S*, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] G. Haessig, A. Cassidy, R. Alvarez, R. Benosman, 和 G. Orchard, “用于事件驱动传感器的脉冲光流，基于IBM的TrueNorth神经突触系统,”
    *T BIOMED CIRC S*, 2018。'
- en: '[184] F. Paredes-Vallés, K. Y. Scheper, and G. C. De Croon, “Unsupervised learning
    of a hierarchical spiking neural network for optical flow estimation: From events
    to global motion perception,” *TPAMI*, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] F. Paredes-Vallés, K. Y. Scheper, 和 G. C. De Croon, “用于光流估计的无监督学习层次脉冲神经网络：从事件到全局运动感知,”
    *TPAMI*, 2019。'
- en: '[185] J. Hidalgo-Carrió, D. Gehrig, and D. Scaramuzza, “Learning Monocular
    Dense Depth from Events,” in *3DV*.   IEEE, 2020.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] J. Hidalgo-Carrió, D. Gehrig, 和 D. Scaramuzza, “从事件中学习单目密集深度,” 在 *3DV*。   IEEE,
    2020。'
- en: '[186] D. Gehrig, M. Rüegg, M. Gehrig, J. Hidalgo-Carrió, and D. Scaramuzza,
    “Combining Events and Frames Using Recurrent Asynchronous Multimodal Networks
    for Monocular Depth Prediction,” *RAL*, 2021.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] D. Gehrig, M. Rüegg, M. Gehrig, J. Hidalgo-Carrió, 和 D. Scaramuzza, “结合事件和帧使用递归异步多模态网络进行单目深度预测,”
    *RAL*, 2021。'
- en: '[187] S. Tulyakov, F. Fleuret, M. Kiefel, P. Gehler, and M. Hirsch, “Learning
    an Event Sequence Embedding for Dense Event-Based Deep Stereo,” *ICCV*, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] S. Tulyakov, F. Fleuret, M. Kiefel, P. Gehler, 和 M. Hirsch, “学习事件序列嵌入用于密集事件驱动的深度立体视觉,”
    *ICCV*, 2019。'
- en: '[188] S. M. N. Uddin, S. H. Ahmed, and Y. J. Jung, “Unsupervised Deep Event
    Stereo for Depth Estimation,” *TCSVT*, 2022.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] S. M. N. Uddin, S. H. Ahmed, 和 Y. J. Jung, “无监督深度事件立体视觉用于深度估计,” *TCSVT*,
    2022。'
- en: '[189] S. M. M. Isfahani, K.-J. Yoon, and J. Choi, “Event-Intensity Stereo:
    Estimating Depth by the Best of Both Worlds,” *ICCV*, 2021.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] S. M. M. Isfahani, K.-J. Yoon, 和 J. Choi, “事件强度立体视觉：通过两者兼得来估计深度,” *ICCV*,
    2021。'
- en: '[190] Y. Nam, S. M. M. Isfahani, K.-J. Yoon, and J. Choi, “Stereo Depth from
    Events Cameras: Concentrate and Focus on the Future,” *CVPR*, 2022.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Y. Nam, S. M. M. Isfahani, K.-J. Yoon 和 J. Choi，“来自事件相机的立体深度：集中并关注未来”，*CVPR*，2022年。'
- en: '[191] H. Cho and K.-J. Yoon, “Selection and Cross Similarity for Event-Image
    Deep Stereo,” in *ECCV*, 2022.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] H. Cho 和 K.-J. Yoon，“事件图像深度立体的选择与交叉相似性”，在 *ECCV*，2022年。'
- en: '[192] K. Zhang, K. Che, J. Zhang, J. Cheng, Z. Zhang, Q. Guo, and L. Leng,
    “Discrete time convolution for fast event-based stereo,” *CVPR*, 2022.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] K. Zhang, K. Che, J. Zhang, J. Cheng, Z. Zhang, Q. Guo 和 L. Leng，“用于快速事件基础立体的离散时间卷积”，*CVPR*，2022年。'
- en: '[193] Y. Zuo, L. Cui, X.-Z. Peng, Y. Xu, S. Gao, X. Wang, and L. Kneip, “Accurate
    depth estimation from a hybrid event-rgb stereo setup,” *IROS*, 2021.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Y. Zuo, L. Cui, X.-Z. Peng, Y. Xu, S. Gao, X. Wang 和 L. Kneip，“来自混合事件-RGB
    立体设置的准确深度估计”，*IROS*，2021年。'
- en: '[194] B. Alsadik and S. Karam, “The simultaneous localization and mapping (SLAM)-An
    overview,” *Surv. Geospat. Eng. J*, 2021.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] B. Alsadik 和 S. Karam，“同时定位与地图构建（SLAM）概述”，*Surv. Geospat. Eng. J*，2021年。'
- en: '[195] Y. Z., G. G., H. R., L. K., H. L., and D. S., “Semi-dense 3D reconstruction
    with a stereo event camera,” in *ECCV*, 2018.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Y. Z., G. G., H. R., L. K., H. L. 和 D. S.，“使用立体事件相机的半稠密 3D 重建”，在 *ECCV*，2018年。'
- en: '[196] A. Baudron, Z. W. Wang, O. Cossairt, and A. K. Katsaggelos, “E3D: Event-Based
    3D Shape Reconstruction,” *arXiv*, 2020.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] A. Baudron, Z. W. Wang, O. Cossairt 和 A. K. Katsaggelos，“E3D：基于事件的 3D
    形状重建”，*arXiv*，2020年。'
- en: '[197] X. Lin, C. Yang, X. Bian, W. Liu, and C. Wang, “EAGAN: Event‐based attention
    generative adversarial networks for optical flow and depth estimation,” 2022.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] X. Lin, C. Yang, X. Bian, W. Liu 和 C. Wang，“EAGAN：用于光流和深度估计的基于事件的注意力生成对抗网络”，2022年。'
- en: '[198] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *NeurIPS*, 2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser 和 I. Polosukhin，“注意力机制就是你所需的一切”，*NeurIPS*，2017年。'
- en: '[199] A. Nguyen, T. T. Do, D. G. Caldwell, and N. G. Tsagarakis, “Real-Time
    6DOF Pose Relocalization for Event Cameras with Stacked Spatial LSTM Networks,”
    2017.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] A. Nguyen, T. T. Do, D. G. Caldwell 和 N. G. Tsagarakis，“基于事件相机的实时 6 自由度姿态重新定位与堆叠空间
    LSTM 网络”，2017年。'
- en: '[200] A. Kendall and R. Cipolla, “Geometric Loss Functions for Camera Pose
    Regression with Deep Learning,” in *CVPR*, 2017.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] A. Kendall 和 R. Cipolla，“用于相机姿态回归的几何损失函数与深度学习”，在 *CVPR*，2017年。'
- en: '[201] Y. Jin, L. Yu, G. Li, and S. Fei, “A 6-DOFs event-based camera relocalization
    system by CNN-LSTM and image denoising,” *EXPERT SYST APPL*, 2020.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Y. Jin, L. Yu, G. Li 和 S. Fei，“基于 CNN-LSTM 和图像去噪的 6 自由度事件相机重新定位系统”，*EXPERT
    SYST APPL*，2020年。'
- en: '[202] L. Steffen, S. Ulbrich, A. Roennau, and R. Dillmann, “Multi-view 3D reconstruction
    with self-organizing maps on event-based data,” in *ICAR*, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] L. Steffen, S. Ulbrich, A. Roennau 和 R. Dillmann，“基于事件数据的自组织映射的多视图 3D
    重建”，在 *ICAR*，2019年。'
- en: '[203] V. Rudnev, V. Golyanik, J. Wang, H. Seidel, F. Mueller, M. Elgharib,
    and C. Theobalt, “EventHands: Real-Time Neural 3D Hand Pose Estimation from an
    Event Stream,” in *ICCV*, 2021.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] V. Rudnev, V. Golyanik, J. Wang, H. Seidel, F. Mueller, M. Elgharib 和
    C. Theobalt，“EventHands：来自事件流的实时神经 3D 手部姿态估计”，在 *ICCV*，2021年。'
- en: '[204] Z. Wang, K. Chaney, and K. Daniilidis, “EvAC3D: From Event-Based Apparent
    Contours to 3D Models via Continuous Visual Hulls,” in *ECCV*, 2022.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Z. Wang, K. Chaney 和 K. Daniilidis，“EvAC3D：通过连续视觉轮廓从基于事件的明显轮廓到 3D 模型”，在
    *ECCV*，2022年。'
- en: '[205] X.-F. Han, H. Laga, and M. Bennamoun, “Image-based 3D object reconstruction:
    State-of-the-art and trends in the deep learning era,” *TPAMI*, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] X.-F. Han, H. Laga 和 M. Bennamoun，“基于图像的 3D 物体重建：深度学习时代的现状与趋势”，*TPAMI*，2019年。'
- en: '[206] H. Rebecq, G. Gallego, and D. Scaramuzza, “EMVS: Event-based multi-view
    stereo,” 2016.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] H. Rebecq, G. Gallego 和 D. Scaramuzza，“EMVS：基于事件的多视图立体”，2016年。'
- en: '[207] H. Kim, S. Leutenegger, and A. J. Davison, “Real-time 3D reconstruction
    and 6-DoF tracking with an event camera,” in *ECCV*, 2016.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] H. Kim, S. Leutenegger 和 A. J. Davison，“使用事件相机的实时 3D 重建和 6 自由度跟踪”，在 *ECCV*，2016年。'
- en: '[208] K. Chaney, A. Zihao Zhu, and K. Daniilidis, “Learning event-based height
    from plane and parallax,” in *CVPR*, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] K. Chaney, A. Zihao Zhu 和 K. Daniilidis，“从平面和视差中学习基于事件的高度”，在 *CVPR*，2019年。'
- en: '[209] Y. Xue, H. Li, S. Leutenegger, and J. Stückler, “Event-based Non-Rigid
    Reconstruction from Contours,” *reconstruction*, 2022.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Y. Xue, H. Li, S. Leutenegger 和 J. Stückler，“基于事件的非刚性轮廓重建”，*reconstruction*，2022年。'
- en: '[210] M. Muglikar, G. Gallego, and D. Scaramuzza, “ESL: Event-based Structured
    Light,” in *3DV*, 2021.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] M. Muglikar, G. Gallego 和 D. Scaramuzza，“ESL：基于事件的结构光”，在 *3DV*，2021年。'
- en: '[211] Y. Wang, R. Idoughi, and W. Heidrich, “Stereo event-based particle tracking
    velocimetry for 3d fluid flow reconstruction,” in *ECCV*, 2020.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Y. Wang, R. Idoughi, 和 W. Heidrich，“基于立体事件的粒子跟踪测速用于 3D 流体流动重建”，见于 *ECCV*，2020。'
- en: '[212] I. Ihrke and M. Magnor, “Image-Based Tomographic Reconstruction of Flames,”
    *ACM*, 2004.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] I. Ihrke 和 M. Magnor，“基于图像的火焰断层重建”，*ACM*，2004。'
- en: '[213] J. Wang, S. Tan, X. Zhen, S. Xu, F. Zheng, Z. He, and L. Shao, “Deep
    3D human pose estimation: A review,” *Comput Vis Image Underst.*, 2021.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] J. Wang, S. Tan, X. Zhen, S. Xu, F. Zheng, Z. He, 和 L. Shao，“深度 3D 人体姿态估计：综述”，*Comput
    Vis Image Underst.*，2021。'
- en: '[214] Y. Desmarais, D. Mottet, P. Slangen, and P. Montesinos, “A review of
    3D human pose estimation algorithms for markerless motion capture,” *CVIU*, 2021.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Y. Desmarais, D. Mottet, P. Slangen, 和 P. Montesinos，“无标记运动捕捉的 3D 人体姿态估计算法综述”，*CVIU*，2021。'
- en: '[215] M. Kocabas, N. Athanasiou, and M. J. Black, “Vibe: Video inference for
    human body pose and shape estimation,” in *CVPR*, 2020.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] M. Kocabas, N. Athanasiou, 和 M. J. Black，“Vibe: 视频推理用于人体姿态和形状估计”，见于 *CVPR*，2020。'
- en: '[216] L. Xu, W. Xu, V. Golyanik, M. Habermann, L. Fang, and C. Theobalt, “Eventcap:
    Monocular 3d capture of high-speed human motions using an event camera,” in *CVPR*,
    2020.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] L. Xu, W. Xu, V. Golyanik, M. Habermann, L. Fang, 和 C. Theobalt，“Eventcap:
    使用事件相机进行高速度人体动作的单目 3D 捕捉”，见于 *CVPR*，2020。'
- en: '[217] E. Calabrese, G. Taverni, C. Awai Easthope, S. Skriabine, F. Corradi,
    L. Longinotti, K. Eng, and T. Delbruck, “DHP19: Dynamic Vision Sensor 3D Human
    Pose Dataset,” in *CVPRW*, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] E. Calabrese, G. Taverni, C. Awai Easthope, S. Skriabine, F. Corradi,
    L. Longinotti, K. Eng, 和 T. Delbruck，“DHP19: 动态视觉传感器 3D 人体姿态数据集”，见于 *CVPRW*，2019。'
- en: '[218] S. Zou, C. Guo, X. Zuo, S. Wang, P. Wang, X. Hu, S. Chen, M. Gong, and
    L. Cheng, “EventHPE: Event-based 3D Human Pose and Shape Estimation,” in *ICCV*,
    2021.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] S. Zou, C. Guo, X. Zuo, S. Wang, P. Wang, X. Hu, S. Chen, M. Gong, 和
    L. Cheng，“EventHPE: 基于事件的 3D 人体姿态和形状估计”，见于 *ICCV*，2021。'
- en: '[219] J. Zhang, B. Dong, H. Zhang, J. Ding, F. Heide, B. Yin, and X. Yang,
    “Spiking Transformers for Event-based Single Object Tracking,” in *CVPR*.   ,
    2022.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] J. Zhang, B. Dong, H. Zhang, J. Ding, F. Heide, B. Yin, 和 X. Yang，“用于基于事件的单对象跟踪的尖峰变换器”，见于
    *CVPR*，2022。'
- en: '[220] W. Weng, Y. Zhang, and Z. Xiong, “Event-based Video Reconstruction Using
    Transformer,” in *ICCV*, 2021.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] W. Weng, Y. Zhang, 和 Z. Xiong，“基于事件的视频重建使用变换器”，见于 *ICCV*，2021。'
- en: '[221] X. Liu, J. Li, X. Fan, and Y. Tian, “Event-based Monocular Dense Depth
    Estimation with Recurrent Transformers,” *CoRR*, 2022.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] X. Liu, J. Li, X. Fan, 和 Y. Tian，“基于事件的单目稠密深度估计与递归变换器”，*CoRR*，2022。'
- en: '[222] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *CVPR*, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, 和 L.-C. Chen，“Mobilenetv2:
    反向残差和线性瓶颈”，见于 *CVPR*，2018。'
- en: '[223] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,
    Y. Zhu, R. Pang, V. Vasudevan *et al.*, “Searching for mobilenetv3,” in *CVPR*,
    2019.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,
    Y. Zhu, R. Pang, V. Vasudevan *等*，“寻找 mobilenetv3”，见于 *CVPR*，2019。'
- en: '[224] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient
    convolutional neural network for mobile devices,” in *CVPR*, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] X. Zhang, X. Zhou, M. Lin, 和 J. Sun，“Shufflenet: 一种极为高效的卷积神经网络用于移动设备”，见于
    *CVPR*，2018。'
- en: '[225] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical guidelines
    for efficient cnn architecture design,” in *ECCV*, 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] N. Ma, X. Zhang, H.-T. Zheng, 和 J. Sun，“Shufflenet v2: 高效 CNN 架构设计的实用指南”，见于
    *ECCV*，2018。'
- en: '[226] R. Krishnamoorthi, “Quantizing deep convolutional networks for efficient
    inference: A whitepaper,” *arXiv*, 2018.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] R. Krishnamoorthi，“量化深度卷积网络以提高推理效率：白皮书”，*arXiv*，2018。'
- en: '[227] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A. Maida,
    “Deep learning in spiking neural networks,” *NN*, 2019.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, 和 A. Maida，“尖峰神经网络中的深度学习”，*NN*，2019。'
- en: '[228] J. Zhang, X. Yang, Y. Fu, X. Wei, B. Yin, and B. Dong, “Object tracking
    by jointly exploiting frame and event domain,” in *ICCV*, 2021.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] J. Zhang, X. Yang, Y. Fu, X. Wei, B. Yin, 和 B. Dong，“通过联合利用帧和事件域进行对象跟踪”，见于
    *ICCV*，2021。'
- en: '[229] R. Battiti, “First-and second-order methods for learning: between steepest
    descent and Newton’s method,” *Neural Comput.*, 1992.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] R. Battiti，“学习的一级和二级方法：在最陡下降与牛顿法之间”，*Neural Comput.*，1992。'
- en: '[230] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*.   MIT Press,
    2016.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] I. Goodfellow, Y. Bengio, 和 A. Courville，*深度学习*。MIT Press，2016。'
- en: '[231] A. Niwa, F. Mochizuki, R. Berner, T. Maruyarma, T. Terano, K. Takamiya,
    Y. Kimura, K. Mizoguchi, T. Miyazaki, S. Kaizu *et al.*, “A 2.97 $\mu$m-pitch
    event-based vision sensor with shared pixel front-end circuitry and low-noise
    intensity readout mode,” in *2023 IEEE International Solid-State Circuits Conference
    (ISSCC)*.   IEEE, 2023.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] A. Niwa, F. Mochizuki, R. Berner, T. Maruyarma, T. Terano, K. Takamiya,
    Y. Kimura, K. Mizoguchi, T. Miyazaki, S. Kaizu *等人*，“一种具有共享像素前端电路和低噪声强度读出模式的2.97
    $\mu$m像素间距事件驱动视觉传感器”，发表于 *2023 IEEE国际固态电路会议（ISSCC）*。IEEE, 2023。'
- en: '[232] M. Guo, S. Chen, Z. Gao, W. Yang, P. Bartkovjak, Q. Qin, X. Hu, D. Zhou,
    M. Uchiyama, S. Fukuoka *et al.*, “A 3-Wafer-Stacked hybrid 15MPixel CIS+ 1 MPixel
    EVS with 4.6 GEvent/s readout, In-Pixel TDC and On-Chip ISP and ESP function,”
    in *2023 IEEE International Solid-State Circuits Conference (ISSCC)*.   IEEE,
    2023.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] M. Guo, S. Chen, Z. Gao, W. Yang, P. Bartkovjak, Q. Qin, X. Hu, D. Zhou,
    M. Uchiyama, S. Fukuoka *等人*，“一种具有4.6 GEvent/s读出、像素内TDC和片上ISP及ESP功能的3片叠层混合15MPixel
    CIS+1 MPixel EVS”，发表于 *2023 IEEE国际固态电路会议（ISSCC）*。IEEE, 2023。'
- en: '[233] K. Kodama, Y. Sato, Y. Yorikado, R. Berner, K. Mizoguchi, T. Miyazaki,
    M. Tsukamoto, Y. Matoba, H. Shinozaki, A. Niwa *et al.*, “1.22 $\mu$m 35.6 Mpixel
    RGB hybrid event-based vision sensor with 4.88 $\mu$m-pitch event pixels and up
    to 10K event frame rate by adaptive control on event sparsity,” in *2023 IEEE
    International Solid-State Circuits Conference (ISSCC)*.   IEEE, 2023.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] K. Kodama, Y. Sato, Y. Yorikado, R. Berner, K. Mizoguchi, T. Miyazaki,
    M. Tsukamoto, Y. Matoba, H. Shinozaki, A. Niwa *等人*，“1.22 $\mu$m 35.6 Mpixel RGB混合事件驱动视觉传感器，具有4.88
    $\mu$m像素间距和通过事件稀疏性自适应控制的最高10K事件帧率”，发表于 *2023 IEEE国际固态电路会议（ISSCC）*。IEEE, 2023。'
- en: '[234] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    *Communications of the ACM*, 2021.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    和 R. Ng，“Nerf: 作为神经辐射场表示场景以进行视图合成”，*ACM通讯*，2021。'
- en: '[235] V. Rudnev, M. Elgharib, C. Theobalt, and V. Golyanik, “EventNeRF: Neural
    Radiance Fields from a Single Colour Event Camera,” *CVPR*, 2022.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] V. Rudnev, M. Elgharib, C. Theobalt, 和 V. Golyanik，“EventNeRF: 来自单一彩色事件摄像机的神经辐射场”，*CVPR*，2022。'
- en: '[236] S. Klenk, L. Koestler, D. Scaramuzza, and D. Cremers, “E-NeRF: Neural
    Radiance Fields from a Moving Event Camera,” *RAL*, 2022.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] S. Klenk, L. Koestler, D. Scaramuzza, 和 D. Cremers，“E-NeRF: 来自移动事件摄像机的神经辐射场”，*RAL*，2022。'
- en: '[237] I. Hwang, J. Kim, and Y. M. Kim, “Ev-NeRF: Event Based Neural Radiance
    Field,” *WACV*, 2022.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] I. Hwang, J. Kim, 和 Y. M. Kim，“Ev-NeRF: 基于事件的神经辐射场”，*WACV*，2022。'
- en: '[238] J. Zhao, S. Zhang, and T. Huang, “Transformer-Based Domain Adaptation
    for Event Data Classification,” in *ICASSP*, 2022.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] J. Zhao, S. Zhang, 和 T. Huang，“基于变换器的事件数据分类领域适应”，发表于 *ICASSP*，2022。'
- en: '[239] Y. Yang, L. Pan, and L. Liu, “Event Camera Data Pre-training.” [Online].
    Available: [http://arxiv.org/abs/2301.01928.](http://arxiv.org/abs/2301.01928.)'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Y. Yang, L. Pan, 和 L. Liu，“事件摄像机数据预训练。” [在线]. 可用: [http://arxiv.org/abs/2301.01928.](http://arxiv.org/abs/2301.01928.)'
- en: '| ![[Uncaptioned image]](img/83ab03d1b95826d31d0140ecafc11779.png) | Xu Zheng
    is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include event-based vision, 3D vision, etc.
    |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/83ab03d1b95826d31d0140ecafc11779.png) | 徐政是香港科技大学广州分校（HKUST-GZ）人工智能方向视觉学习与智能系统实验室的博士生。他的研究兴趣包括基于事件的视觉、3D视觉等。
    |'
- en: '| ![[Uncaptioned image]](img/2a6e293cb71c8f46fda139b658a01ca0.png) | Yexin
    Liu is a Mphil. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include infrared- and event-based vision, and
    unsupervised domain adaptation. |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/2a6e293cb71c8f46fda139b658a01ca0.png) | 刘业欣是香港科技大学广州分校（HKUST-GZ）人工智能方向视觉学习与智能系统实验室的硕士研究生。他的研究兴趣包括红外和基于事件的视觉，以及无监督领域适应。
    |'
- en: '| ![[Uncaptioned image]](img/97cba95341b311a800aad8e65cdf35dd.png) | Yunfan
    Lu is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include low-level vision (event camera, deblurring,
    SR), pattern recognition (image classification, object detection), and DL (transfer
    learning, unsupervised learning). |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/97cba95341b311a800aad8e65cdf35dd.png) | 吕云凡是香港科技大学（广州）人工智能方向视觉学习与智能系统实验室的博士生。他的研究兴趣包括低级视觉（事件相机、去模糊、超分辨率）、模式识别（图像分类、目标检测）和深度学习（迁移学习、无监督学习）。
    |'
- en: '| ![[Uncaptioned image]](img/7b5e9f866be8d4a883613cc4adb7fbd7.png) | Tongyan
    Hua is a research assistant in the Visual Learning and Intelligent Systems Lab,
    Artificial Intelligence Thrust, The Hong Kong University of Science and Technology,
    Guangzhou (HKUST-GZ). Her research interests include robotics vision, Simultaneous
    localization and mapping (SLAM), Deep Learning, etc. |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/7b5e9f866be8d4a883613cc4adb7fbd7.png) | 华童燕是香港科技大学（广州）人工智能方向视觉学习与智能系统实验室的研究助理。她的研究兴趣包括机器人视觉、同时定位与地图构建（SLAM）、深度学习等。
    |'
- en: '| ![[Uncaptioned image]](img/8f87c6ef36e63b2ec2311c28282503ab.png) | Tianbo
    Pan is a Mphil student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou
    (HKUST-GZ). His research interests include event-based vision, Simultaneous localization
    and mapping (SLAM), 3D vision, etc. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/8f87c6ef36e63b2ec2311c28282503ab.png) | 潘天博是香港科技大学（广州）人工智能方向视觉学习与智能系统实验室的硕士生。他的研究兴趣包括事件驱动视觉、同时定位与地图构建（SLAM）、3D视觉等。
    |'
- en: '| ![[Uncaptioned image]](img/d7595ede50696b0b7f080915ab66b9fa.png) | Weiming
    Zhang is a research assistant in the Visual Learning and Intelligent Systems Lab,
    Artificial Intelligence Thrust, The Hong Kong University of Science and Technology,
    Guangzhou (HKUST-GZ). His research interests include event-based vision, Deep
    Learning, etc. |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/d7595ede50696b0b7f080915ab66b9fa.png) | 张伟明是香港科技大学（广州）人工智能方向视觉学习与智能系统实验室的研究助理。他的研究兴趣包括事件驱动视觉、深度学习等。
    |'
- en: '| ![[Uncaptioned image]](img/7d35cabf58fc086d1889176113ac995b.png) | Dacheng
    Tao (Fellow IEEE) the Inaugural Director of the JD Explore Academy and a Senior
    Vice President of JD.com. He is also an advisor and chief scientist of the digital
    sciences initiative at the University of Sydney. He mainly applies statistics
    and mathematics to AI and data science, and his research is detailed in one monograph
    and over 200 publications in prestigious journals and proceedings at leading conferences.
    He received the 2015 Australian Scopus-Eureka Prize, the 2018 IEEE ICDM Research
    Contributions Award, and the 2021 IEEE Computer Society McCluskey Technical Achievement
    Award. He is a fellow of the Australian Academy of Science, AAAS, ACM and IEEE.
    |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/7d35cabf58fc086d1889176113ac995b.png) | 陶大程（IEEE会士）是京东探索学院的首任院长及京东的高级副总裁。他还是悉尼大学数字科学计划的顾问及首席科学家。他主要将统计学和数学应用于人工智能和数据科学，他的研究成果详细记载在一本专著和200多篇发表在著名期刊及领先会议论文集中。他获得了2015年澳大利亚Scopus-Eureka奖、2018年IEEE
    ICDM研究贡献奖和2021年IEEE计算机学会McCluskey技术成就奖。他是澳大利亚科学院、美国科学促进会、ACM和IEEE的会士。'
- en: '| ![[Uncaptioned image]](img/2057381ce4adb7b6d18493949a93f485.png) | Lin Wang
    (IEEE Member) is an assistant professor in the AI Thrust, HKUST-GZ, HKUST FYTRI,
    and an affiliate assistant professor in the Dept. of CSE, HKUST. He did his Postdoc
    at the Korea Advanced Institute of Science and Technology (KAIST). He got his
    Ph.D. (with honors) and M.S. from KAIST, Korea. He had rich cross-disciplinary
    research experience, covering mechanical, industrial, and computer engineering.
    His research interests lie in computer and robotic vision, machine learning, intelligent
    systems (XR, vision for HCI), etc. |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/2057381ce4adb7b6d18493949a93f485.png) | 王林（IEEE会员）是香港科技大学（广州）人工智能方向的助理教授，香港科技大学FYTRI的助理教授，并在香港科技大学计算机科学与工程系担任副教授。他在韩国科学技术院（KAIST）完成了博士后研究，并获得了KAIST的博士（荣誉）和硕士学位。他有丰富的跨学科研究经验，涵盖了机械、工业和计算机工程。他的研究兴趣包括计算机和机器人视觉、机器学习、智能系统（XR、HCI视觉）等。
    |'
