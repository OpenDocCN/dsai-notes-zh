- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:31:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:31:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2407.08865] Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2407.08865] 使用深度学习进行单图像阴影去除：一项全面的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08865](https://ar5iv.labs.arxiv.org/html/2407.08865)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08865](https://ar5iv.labs.arxiv.org/html/2407.08865)
- en: \WarningFilter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \WarningFilter
- en: latexFont shape \WarningFilterlatexfontFont shape
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: latexFont形状 \WarningFilterlatexfontFont形状
- en: 'Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习进行单图像阴影去除：一项全面的综述
- en: Lanqing Guo, Chong Wang, Yufei Wang, Yi Yu, Siyu Huang, Wenhan Yang, Member,
    IEEE,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 郭兰青、王冲、王雨飞、余毅、黄思宇、杨文瀚，IEEE会员，
- en: 'Alex C. Kot, Life Fellow, IEEE, Bihan Wen, Senior Member, IEEE Lanqing Guo,
    Chong Wang, Yufei Wang, Yi Yu, Alex C. Kot, and Bihan Wen are with the School
    of Electrical & Electronic Engineering, Nanyang Technological University, Singapore
    639798\. E-mail: {lanqing001, wang1711, yufei001, yuyi0010, eackot, bihan.wen}@ntu.edu.sg.
    Siyu Huang is with the Visual Computing Division, School of Computing, Clemson
    University, Clemson, SC 29631 USA. E-mail: siyuh@clemson.edu. Wenhan Yang is with
    PengCheng Laboratory, Shenzhen, China. E-mail: yangwh@pcl.ac.cn.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Alex C. Kot, IEEE终身会员，Bihan Wen，IEEE高级会员，郭兰青、王冲、王雨飞、余毅、Alex C. Kot和Bihan Wen均在新加坡南洋理工大学电气与电子工程学院工作，邮政编码：639798\.
    电子邮件：{lanqing001, wang1711, yufei001, yuyi0010, eackot, bihan.wen}@ntu.edu.sg。黄思宇在克莱姆森大学计算机学院视觉计算部工作，邮政编码：29631
    美国南卡罗来纳州克莱姆森市。电子邮件：siyuh@clemson.edu。杨文瀚在中国深圳鹏城实验室工作。电子邮件：yangwh@pcl.ac.cn。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Shadow removal aims at restoring the image content within shadow regions, pursuing
    a uniform distribution of illumination that is consistent between shadow and non-shadow
    regions. Comparing to other image restoration tasks, there are two unique challenges
    in shadow removal: 1) The patterns of shadows are arbitrary, varied, and often
    have highly complex trace structures, making “trace-less” image recovery difficult.
    2) The degradation caused by shadows is spatially non-uniform, resulting in inconsistencies
    in illumination and color between shadow and non-shadow areas. Recent developments
    in this field are primarily driven by deep learning-based solutions, employing
    a variety of learning strategies, network architectures, loss functions, and training
    data. Nevertheless, a thorough and insightful review of deep learning-based shadow
    removal techniques is still lacking. In this paper, we are the first to provide
    a comprehensive survey to cover various aspects ranging from technical details
    to applications. We highlight the major advancements in deep learning-based single-image
    shadow removal methods, thoroughly review previous research across various categories,
    and provide insights into the historical progression of these developments. Additionally,
    we summarize performance comparisons both quantitatively and qualitatively. Beyond
    the technical aspects of shadow removal methods, we also explore potential future
    directions for this field. The related repository is released at: [https://github.com/GuoLanqing/Awesome-Shadow-Removal](https://github.com/GuoLanqing/Awesome-Shadow-Removal).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影去除旨在恢复阴影区域内的图像内容，追求阴影与非阴影区域之间的一致光照分布。与其他图像恢复任务相比，阴影去除面临两个独特的挑战：1）阴影的模式是任意的、多变的，并且通常具有高度复杂的痕迹结构，使得“无痕迹”图像恢复变得困难。2）阴影造成的退化在空间上是不均匀的，导致阴影和非阴影区域之间的光照和颜色不一致。该领域的最新进展主要依赖于基于深度学习的解决方案，采用了各种学习策略、网络架构、损失函数和训练数据。然而，关于基于深度学习的阴影去除技术的全面和深入的综述仍然缺乏。在本文中，我们首次提供了一项全面的综述，涵盖了从技术细节到应用的各个方面。我们重点介绍了基于深度学习的单图像阴影去除方法的主要进展，全面回顾了各个类别的先前研究，并提供了这些发展的历史进程的见解。此外，我们还总结了性能的定量和定性比较。除了阴影去除方法的技术方面，我们还探讨了该领域的潜在未来方向。相关的代码库已发布于：[https://github.com/GuoLanqing/Awesome-Shadow-Removal](https://github.com/GuoLanqing/Awesome-Shadow-Removal)。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Shadow Removal, Low-Level Vision, Deep Learning, Image Enhancement, Computational
    Photography
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影去除、低级视觉、深度学习、图像增强、计算摄影
- en: I Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Shadowing is a natural occurrence observed when certain regions of a surface
    receive less illumination compared to their neighboring areas. This happens when
    an opaque object obstructs the direct path of light between the surface and the
    light source. Shadows in images and videos hinder both the human perception [[1](#bib.bib1),
    [2](#bib.bib2)] as well as many subsequent vision tasks, e.g., object detection,
    tracking, and semantic segmentation [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影是自然现象，当表面某些区域收到的光照少于其邻近区域时会发生。这发生在不透明物体阻挡了表面和光源之间的光线直接路径时。图像和视频中的阴影阻碍了人类感知[[1](#bib.bib1),
    [2](#bib.bib2)]以及许多后续视觉任务，例如对象检测、跟踪和语义分割[[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]。
- en: '![Refer to caption](img/264ad576fbcfb126a5d7c0e2a22062a9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/264ad576fbcfb126a5d7c0e2a22062a9.png)'
- en: 'Figure 1: Milestones in deep learning-based single-image shadow removal methods
    include the exploration of various technologies over time, such as deep CNNs,
    GANs, RNNs, unrolling, transformers, and diffusion models.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：基于深度学习的单图像阴影去除方法的里程碑包括随着时间的推移探索各种技术，如深度卷积神经网络（**CNNs**）、生成对抗网络（**GANs**）、递归神经网络（**RNNs**）、展开、变换器和扩散模型。
- en: Shadow degradations manifest in diverse types and exhibit highly non-uniform
    properties, presenting unique challenges for shadow removal compared to other
    image restoration tasks. Traditional shadow removal methods leverage hand-crafted
    prior information such as illumination [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)], gradient [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)],
    and color [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)] to restore illumination in shadowed areas. Typically, some methods [[13](#bib.bib13),
    [23](#bib.bib23)] eliminate the image gradient along shadow edges and then reintegrate
    the modified gradient to produce a shadow-free image. However, these approaches
    are based on ideal assumptions, resulting in noticeable shadow boundary artifacts
    in real-world scenarios. They always require accurate and fine-grained shadow
    edges in the detection output as the strong prior for shadow removal, which is
    impractical in applications; another group [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)] treat shadow removal as a
    relighting problem, seeking a factor to enhance the brightness of shadow pixels.
    The main challenge is to determine different scale factors for umbra and penumbra
    shadows, along with addressing shadow boundary corrections due to non-uniform
    illumination degradation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影退化表现出多种类型，并具有高度非均匀的特性，这为阴影去除带来了独特的挑战，与其他图像恢复任务相比。传统的阴影去除方法利用手工制作的先验信息，如光照[[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]，梯度[[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)]和颜色[[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]来恢复阴影区域的光照。通常，一些方法[[13](#bib.bib13),
    [23](#bib.bib23)]通过去除阴影边缘的图像梯度，然后重新整合修改后的梯度以生成无阴影图像。然而，这些方法基于理想的假设，导致实际场景中明显的阴影边界伪影。它们总是需要在检测输出中准确细致的阴影边缘作为阴影去除的强先验，这是在应用中不切实际的；另一类方法[[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]将阴影去除视为重光照问题，寻求一种因素来增强阴影像素的亮度。主要挑战是确定伪影和半影的不同尺度因子，并解决由于非均匀光照退化导致的阴影边界校正问题。
- en: 'In recent years, deep learning-based shadow removal methods have demonstrated
    superior performance, owing to the availability of extensive training data. Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey") presents a concise overview of the milestones achieved
    by deep learning-based shadow removal methods in the past years. In 2017, Qu *et
    al.* [[24](#bib.bib24)] first proposed a multi-branch fusion framework as the
    pioneering work to adopt a deep convolutional neural networks (CNNs) [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)] to this field. Subsequently, starting from
    2018, the advancement of Generative Adversarial Networks (GANs) [[28](#bib.bib28)]
    in the realm of low-level vision has led to widespread adoption of GAN-based networks
    in shadow removal tasks. These networks are not only effective in addressing amplified
    artifacts but also enable the development of unsupervised learning methods using
    unpaired shadow and shadow-free data. During this period, some methods tried to
    inject constraints, e.g., physical illumination model and mask information, into
    networks to learn more effective features, by unrolling [[29](#bib.bib29), [30](#bib.bib30)]
    or recurrent [[31](#bib.bib31)] strategies. In the most recent developments, novel
    techniques such as the transformer mechanism [[32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)] and diffusion model [[36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)] have shown remarkable advancements
    in shadow removal. The transformer mechanism exploits long-range dependencies
    within contexts, facilitating more effective extraction of context information
    from non-shadow regions. Meanwhile, the diffusion model offers a potent diffusive
    generative prior for generating natural shadow-free images.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，基于深度学习的阴影去除方法由于大量训练数据的可用性，表现出卓越的性能。图 [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey") 展示了过去几年基于深度学习的阴影去除方法所取得的里程碑式进展。2017年，Qu
    *et al.* [[24](#bib.bib24)] 首次提出了一个多分支融合框架，作为将深度卷积神经网络（CNNs）[[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)] 引入这一领域的开创性工作。随后，从2018年开始，生成对抗网络（GANs）[[28](#bib.bib28)]
    在低级视觉领域的进展导致了GAN-based网络在阴影去除任务中的广泛应用。这些网络不仅在处理放大伪影方面有效，还支持使用未配对的阴影和无阴影数据开发无监督学习方法。在这一期间，一些方法尝试将约束条件（例如，物理照明模型和掩码信息）注入网络中，通过展开
    [[29](#bib.bib29), [30](#bib.bib30)] 或递归 [[31](#bib.bib31)] 策略来学习更有效的特征。在最新的进展中，诸如变换器机制
    [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)] 和扩散模型
    [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]
    等新技术在阴影去除方面表现出了显著的进展。变换器机制利用上下文中的长程依赖关系，从非阴影区域中更有效地提取上下文信息。与此同时，扩散模型提供了一种强大的扩散生成先验，用于生成自然的无阴影图像。'
- en: 'Despite the dominance of deep learning in shadow removal research, there is
    a lack of an in-depth and comprehensive survey on deep learning-based solutions.
    The existing surveys for shadow removal [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]
    only cover traditional methods based on earlier publications, but omit the recent
    works via deep learning approaches. In this paper, we are the first to review
    and summarize deep learning-based single-image shadow removal methods, aiming
    to offer a well-structured and comprehensive knowledge base to support and advance
    the field of shadow removal. The rest of the paper is organized as follows. Section [I](#S1
    "I Introduction ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") introduces the shadow formulation model and the problem definition for
    the shadow removal task. Section [II](#S2 "II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey") presents a detailed
    shadow formation model and outlines the various shadow categories, highlighting
    the different challenges associated with each. Section [III](#S3 "III Deep Learning
    Based Shadow Removal ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") provides a detailed survey of deep learning based single-image shadow
    removal of different learning strategies, including supervised, semi-supervised,
    unsupervised, and zero-shot learning, as well as summarises the challenges of
    analysis of shadow removal. Subsequently, Section [IV](#S4 "IV Technical Review
    and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") reviews the related technical aspects and explores solutions to the aforementioned
    challenges through various technical designs and the integration of priors, including
    network architectures, basic blocks, framework designs, loss functions, datasets,
    and evaluation metrics. Section [V](#S5 "V Benchmarking and Empirical Analysis
    ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey") summarizes
    the quantitative and qualitative comparisons of a number of single-image shadow
    removal methods among various benchmarks. Section [VI](#S6 "VI Shadow Removal
    Applications ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") presents the related shadow removal applications, including shadow generation
    and shadow-related attack in practice. Section [VII](#S7 "VII Future Research
    Directions ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") provides detailed discussions of the future research directions on generalized
    shadow removal and interactive shadow removal. Finally, the paper is concluded
    in Section [VIII](#S8 "VIII Conclusion ‣ Single-Image Shadow Removal Using Deep
    Learning: A Comprehensive Survey").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管深度学习在阴影去除研究中占据主导地位，但目前还缺乏对基于深度学习的解决方案的深入和全面的调查。现有的阴影去除调查[[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]仅涵盖了基于早期出版物的传统方法，却忽略了通过深度学习方法的最新研究。在本文中，我们首次回顾并总结了基于深度学习的单图像阴影去除方法，旨在提供一个结构化且全面的知识基础，以支持和推动阴影去除领域的发展。本文的其余部分组织如下。第[I](#S1
    "I Introduction ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")节介绍了阴影建模模型和阴影去除任务的问题定义。第[II](#S2 "II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")节详细介绍了阴影形成模型，并概述了各种阴影类别，突出了每种类别所面临的不同挑战。第[III](#S3
    "III Deep Learning Based Shadow Removal ‣ Single-Image Shadow Removal Using Deep
    Learning: A Comprehensive Survey")节提供了基于深度学习的单图像阴影去除的详细调查，包括监督学习、半监督学习、无监督学习和零样本学习等不同学习策略，并总结了阴影去除分析的挑战。随后，第[IV](#S4
    "IV Technical Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey")节回顾了相关技术方面，并通过各种技术设计和先验整合探索解决上述挑战的方案，包括网络架构、基本模块、框架设计、损失函数、数据集和评估指标。第[V](#S5
    "V Benchmarking and Empirical Analysis ‣ Single-Image Shadow Removal Using Deep
    Learning: A Comprehensive Survey")节总结了多个单图像阴影去除方法在各种基准中的定量和定性比较。第[VI](#S6 "VI
    Shadow Removal Applications ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey")节介绍了相关的阴影去除应用，包括实际中的阴影生成和阴影相关攻击。第[VII](#S7 "VII Future
    Research Directions ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")节详细讨论了关于通用阴影去除和交互式阴影去除的未来研究方向。最后，第[VIII](#S8 "VIII Conclusion ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")节对论文进行了总结。'
- en: II Analysis on Shadow Formation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 阴影形成分析
- en: II-A Shadow Formation Model
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 阴影形成模型
- en: According to the Retinex theory [[44](#bib.bib44)], the formation process of
    a shadow-free image $\mathbf{I}_{sf}$ with normal lightness can be formulated
    as
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Retinex理论[[44](#bib.bib44)]，无阴影图像 $\mathbf{I}_{sf}$ 的形成过程可以表示为
- en: '|  | $\mathbf{I}_{sf}=\mathbf{L}_{sf}\circ\mathbf{R}$ |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{I}_{sf}=\mathbf{L}_{sf}\circ\mathbf{R}$ |  | (1) |'
- en: where $\mathbf{L}$ denotes the illumination, $\mathbf{R}$ denotes the reflectance,
    and $\circ$ denotes element-wise multiplication. Shadows occur when an occluder
    blocks light between the surface and the light source, causing partial light degradations.
    Thus, the corresponding shadow image $\mathbf{I}_{s}$ with degraded illumination
    $\mathbf{L}_{s}$ can be formulated as
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{L}$ 表示光照，$\mathbf{R}$ 表示反射率，$\circ$ 表示逐元素相乘。当遮挡物阻挡了表面与光源之间的光线时，会产生阴影，导致部分光照衰减。因此，具有衰减光照
    $\mathbf{L}_{s}$ 的对应阴影图像 $\mathbf{I}_{s}$ 可以表示为
- en: '|  | $\mathbf{I}_{s}=\mathbf{L}_{s}\circ\mathbf{R}=\mathbf{A}\circ\mathbf{L}_{sf}\circ\mathbf{R},$
    |  | (2) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{I}_{s}=\mathbf{L}_{s}\circ\mathbf{R}=\mathbf{A}\circ\mathbf{L}_{sf}\circ\mathbf{R},$
    |  | (2) |'
- en: where $\mathbf{A}$ denotes the spatially variant shadow degradation degrees
    at each location.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{A}$ 表示每个位置的空间变异阴影衰减程度。
- en: II-B Shadow Category
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 阴影分类
- en: 'Shadow degradations have diverse types and highly non-uniform properties, which
    present unique challenges for shadow removal compared to other image restoration
    tasks. Shadows can be classified as self shadows if they are part of an object
    or as cast shadows if they belong to the background of the scene as shown in Figure [2](#S2.F2
    "Figure 2 ‣ II-B Shadow Category ‣ II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(a). For a non-point
    source of light, a cast shadow is further sub-divided into soft and hard shadows
    based on the illumination intensity (i.e., darkness) as shown in Figure [2](#S2.F2
    "Figure 2 ‣ II-B Shadow Category ‣ II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(b). Specifically,
    shadows produced by the obstruction of a non-point light source by an object exhibit
    two distinct regions: the umbra and the penumbra. The umbra denotes the high-density
    region, located toward the inner shadow area, whereas the penumbra represents
    the low-density region, situated toward the outer shadow area. Hard shadows exhibit
    umbra regions with a high density, often resulting in nearly obliterated surface
    texture. In contrast, soft shadows display penumbra regions towards the outer
    shadow area, with lower density, and their boundaries typically blend into the
    non-shadowed surroundings. The distance between the occluder and receiver, as
    well as the type of light source, determines whether hard or soft shadows are
    produced, as depicted in Figure [2](#S2.F2 "Figure 2 ‣ II-B Shadow Category ‣
    II Analysis on Shadow Formation ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey")(b).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '阴影衰减有多种类型和高度非均匀的特性，这使得阴影去除相比其他图像恢复任务面临独特的挑战。如果阴影是物体的一部分，则可将其分类为自阴影；如果阴影属于场景的背景，则可分类为投射阴影，如图 [2](#S2.F2
    "Figure 2 ‣ II-B Shadow Category ‣ II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(a)所示。对于非点光源，投射阴影根据光照强度（即暗度）进一步分为软阴影和硬阴影，如图 [2](#S2.F2
    "Figure 2 ‣ II-B Shadow Category ‣ II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(b)所示。具体而言，由物体遮挡的非点光源产生的阴影具有两个不同的区域：本影和半影。本影表示高密度区域，位于阴影的内部，而半影表示低密度区域，位于阴影的外部。硬阴影具有高密度的本影区域，通常导致几乎完全消失的表面纹理。相比之下，软阴影在阴影的外部区域显示出低密度的半影区域，其边界通常与未遮蔽的周围环境融为一体。遮挡物与接收器之间的距离以及光源的类型决定了产生硬阴影还是软阴影，如图 [2](#S2.F2
    "Figure 2 ‣ II-B Shadow Category ‣ II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(b)所示。'
- en: 'Shadow degradations vary widely in type and exhibit highly non-uniform characteristics,
    requiring better generalization ability to handle varying cases in the real world.
    Different shadow types present distinct challenges: hard shadows with sharp boundaries
    can leave noticeable traces, while soft shadows and self-shadows, with ambiguous
    boundaries, are more difficult to detect and mask accurately.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影衰减类型多样且具有高度非均匀的特性，这要求更好的泛化能力以处理现实世界中的各种情况。不同类型的阴影面临不同的挑战：具有锐利边界的硬阴影可能留下明显的痕迹，而模糊边界的软阴影和自阴影更难以准确检测和遮蔽。
- en: '![Refer to caption](img/46659b9203e6982aa96b1f43d111316b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/46659b9203e6982aa96b1f43d111316b.png)'
- en: 'Figure 2: How a shadow is formed? (a) Shadows are then classified as cast shadows
    if they belong to the background of the scene or as self shadows if they are part
    of an occluder object. (b) The cast shadows can be further classified as hard
    shadows and soft shadows. A crisp edged one (hard shadow) formed by a point light
    source, a rather more fuzzy one (soft shadow) that is formed by the area light,
    and otherwise the occluder is very close to the receiver.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：阴影是如何形成的？（a）阴影被分类为投射阴影（如果它们属于场景背景）或自阴影（如果它们是遮挡物的一部分）。（b）投射阴影可以进一步分类为硬阴影和软阴影。硬阴影是由点光源形成的清晰边缘的阴影，软阴影则是由面积光源形成的模糊阴影，或者遮挡物距离接收器非常近。
- en: III Deep Learning Based Shadow Removal
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基于深度学习的阴影去除
- en: In this section, we first provide the problem definition of single-image shadow
    removal. Then, we review and discuss existing shadow removal methods according
    to the categories of general learning strategies. After that, we summarize the
    challenges and provide an analysis of existing methods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先提供单图像阴影去除的问题定义。然后，我们根据一般学习策略的类别回顾并讨论现有的阴影去除方法。之后，我们总结了挑战并对现有方法进行了分析。
- en: III-A Problem Definition
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 问题定义
- en: 'We begin by presenting the common formulation of the deep learning-based image
    shadow removal problem. For a shadow image $\mathbf{I}_{s}\in\mathbb{R}^{H\times
    W\times 3}$ with width $W$ and height $H$, the process can be modeled as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍基于深度学习的图像阴影去除问题的常见表述。对于一个阴影图像$\mathbf{I}_{s}\in\mathbb{R}^{H\times W\times
    3}$，其宽度为$W$，高度为$H$，该过程可以建模如下：
- en: '|  | $\hat{\mathbf{I}}_{sf}=\mathcal{G}(\mathbf{I}_{s};\theta),$ |  | (3) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{I}}_{sf}=\mathcal{G}(\mathbf{I}_{s};\theta),$ |  | (3) |'
- en: 'where $\hat{\mathbf{I}}_{sf}\in\mathbb{R}^{H\times W\times 3}$ is the restored
    shadow-free result and $\mathcal{G}$ represents the shadow removal network with
    trainable parameters $\theta$. Different from other image restoration tasks, shadow
    removal is a partial corruption problem that requires identifying the shadow regions
    and restoring them. To aid this process, an optional shadow mask $\mathbf{M}\in\mathbb{R}^{H\times
    W}$ can be used as auxiliary information, indicating shadow regions either manually
    annotated or detected by a pre-trained shadow detector. The uniqueness property
    motivates numerous explorations into how shadow mask information can assist in
    shadow removal, facilitating the development of shadow mask-guided removal process
    as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\hat{\mathbf{I}}_{sf}\in\mathbb{R}^{H\times W\times 3}$是恢复后的无阴影结果，而$\mathcal{G}$代表具有可训练参数$\theta$的阴影去除网络。与其他图像恢复任务不同，阴影去除是一个部分损坏问题，需要识别阴影区域并恢复它们。为辅助这一过程，可以使用可选的阴影掩码$\mathbf{M}\in\mathbb{R}^{H\times
    W}$作为辅助信息，指示阴影区域，这些区域可以是人工标注的或由预训练的阴影检测器检测的。独特性促使了许多探索，研究阴影掩码信息如何协助阴影去除，从而促进了阴影掩码引导的去除过程的发展，如下所示：
- en: '|  | $\hat{\mathbf{I}}_{sf}=\mathcal{G}(\mathbf{I}_{s},\mathbf{M};\theta)\;.$
    |  | (4) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{I}}_{sf}=\mathcal{G}(\mathbf{I}_{s},\mathbf{M};\theta)\;.$
    |  | (4) |'
- en: III-B General Learning Strategies
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 一般学习策略
- en: 'Based on different learning strategies, we generally categorize existing image
    shadow removal methods into supervised learning, unsupervised learning, zero-shot
    learning, and semi-supervised learning. Figure [3](#S3.F3 "Figure 3 ‣ III-B General
    Learning Strategies ‣ III Deep Learning Based Shadow Removal ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey") illustrates the brief pipeline
    of different learning strategies. In the following section, we review some representative
    methods for each strategy.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '根据不同的学习策略，我们通常将现有的图像阴影去除方法分为监督学习、无监督学习、零样本学习和半监督学习。图 [3](#S3.F3 "Figure 3 ‣
    III-B General Learning Strategies ‣ III Deep Learning Based Shadow Removal ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey") 说明了不同学习策略的简要流程。接下来的部分，我们回顾了每种策略的一些代表性方法。'
- en: Supervised Learning (SL) For shadow removal, supervised learning refers to the
    model training with paired shadow and shadow-free images captured in the same
    scene, but under different illumination conditions. The pioneering deep learning-based
    work DeShadowNet [[24](#bib.bib24)] propose an automatic and end-to-end deep neural
    network to unify shadow detection, umbra/penumbra region classification, and shadow
    removal, which directly learns the mapping function between the shadow image and
    its shadow matte. Subsequently, Wang *et al.* [[45](#bib.bib45)] developed a stacked
    conditional generative adversarial network (ST-CGAN) to simultaneously learn shadow
    detection and shadow removal. The ST-CGAN organizes all tasks in a way that allows
    them to focus on one task at a time during different stages, while also benefiting
    from mutual improvements through the exchange of information in both forward and
    backward directions. Hu *et al.* [[46](#bib.bib46)] solve the shadow removal problem
    by a new perspective, which employs the spatial recurrent neural network with
    the direction-aware attention mechanism to exploit the context information. After
    that, Zhang *et al.* [[47](#bib.bib47)] design a coarse-to-fine pipeline with
    four generators as well as three discriminators to explore the residual and illumination
    information separately. A dual hierarchically aggregation network, DHAN [[48](#bib.bib48)],
    is proposed for shadow removal. The DHAN contains a series of growth dilated convolutions
    as the backbone and hierarchically aggregates multi-context features for attention
    and prediction. Lin *et al.* [[49](#bib.bib49)] specifically design BEDSR-Net
    for document image shadow removal and take advantage of specific properties of
    document images, consisting of the global background color and attention map estimation.
    Recently, a two-stage context-aware network [[50](#bib.bib50)] is proposed, which
    transfers contextual information from non-shadow to shadow regions at different
    scales with a contextual patch matching module.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习（SL）在去除阴影方面，指的是使用在相同场景中拍摄的成对阴影图像和无阴影图像进行模型训练，但这些图像是在不同的光照条件下捕获的。开创性的基于深度学习的工作
    DeShadowNet [[24](#bib.bib24)] 提出了一个自动化且端到端的深度神经网络，以统一阴影检测、阴影区域分类和阴影去除，该网络直接学习阴影图像与其阴影遮罩之间的映射函数。随后，Wang
    *et al.* [[45](#bib.bib45)] 开发了一个堆叠条件生成对抗网络（ST-CGAN），以同时学习阴影检测和阴影去除。ST-CGAN 以一种组织所有任务的方式，使其在不同阶段专注于一个任务，同时通过信息在前向和后向方向的交换来实现相互改进。Hu
    *et al.* [[46](#bib.bib46)] 从一个新的角度解决了阴影去除问题，该方法使用空间递归神经网络和方向感知注意机制来利用上下文信息。之后，Zhang
    *et al.* [[47](#bib.bib47)] 设计了一个从粗到细的流程，包括四个生成器和三个鉴别器，以分别探索残差和光照信息。提出了一种双重层次聚合网络
    DHAN [[48](#bib.bib48)]，用于阴影去除。DHAN 包含一系列增长的膨胀卷积作为骨干，并层次聚合多上下文特征以进行注意和预测。Lin *et
    al.* [[49](#bib.bib49)] 专门为文档图像阴影去除设计了 BEDSR-Net，并利用文档图像的特定属性，包括全局背景色和注意力图估计。最近，提出了一种两阶段上下文感知网络
    [[50](#bib.bib50)]，该网络通过上下文补丁匹配模块在不同尺度上将上下文信息从非阴影区域转移到阴影区域。
- en: In comparison to jointly learning shadow detection and shadow removal to explore
    both the semantic and fine-structural levels, some recent methods enjoy better
    performance and more lightweight models by relying on an external pre-trained
    shadow detector or manually annotated mask information. A group of methods explicitly
    combine the physical shadow illumination model into the deep network to increase
    interpretability. For instance, Le *et al.* [[51](#bib.bib51)] propose to integrate
    the linear illumination transformation into the deep network to model the shadow
    effects. This work uses a regression model to estimate the scaling factor and
    additive constant parameters for linear transformation. Following that, Fu *et
    al.* [[52](#bib.bib52)] re-formulate shadow removal as an exposure fusion challenge.
    They reconstruct shadow-free results by intelligently blending over-exposed shadow
    images with shadow images using an adaptive per-pixel kernel weight map. Zhu *et
    al.* [[29](#bib.bib29)] design an iterative algorithm and unfold pipeline to ensure
    the identity mapping among non-shadow regions and adaptively perform the fine-grained
    spatial mapping between shadow regions. Moreover, the shadow images can be divided
    into shadow and non-shadow regions based on the pre-defined shadow mask, which
    is very effective prior for shadow removal. A Mask-ShadowNet [[53](#bib.bib53)]
    is proposed, which includes a masked adaptive instance normalization layer with
    embedded aligners and processes the shadow and non-shadow regions separately by
    different statistics. To ensure the consistency between shadow removal and generation,
    Zhu *et al.* [[54](#bib.bib54)] propose a bijective mapping network (BMNet) to
    recover the underlying background contents during the forward procedure under
    the guidance of a color map and shadow mask. Guo *et al.* [[55](#bib.bib55)] propose
    ShadowFormer to exploit the illumination information from non-shadow regions to
    help shadow regions restoration via the customized transformer blocks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与联合学习阴影检测和阴影去除以探讨语义和细节结构层次相比，一些近期方法通过依赖外部预训练的阴影检测器或手动标注的掩膜信息，实现了更好的性能和更轻量级的模型。一些方法明确地将物理阴影照明模型结合到深度网络中，以增加可解释性。例如，Le
    *et al.* [[51](#bib.bib51)] 提出了将线性照明变换集成到深度网络中以建模阴影效果。这项工作使用回归模型来估计线性变换的缩放因子和加法常数参数。随后，Fu
    *et al.* [[52](#bib.bib52)] 将阴影去除重新定义为曝光融合挑战。他们通过智能地将过度曝光的阴影图像与阴影图像混合，利用自适应的每像素核权重图来重建无阴影结果。Zhu
    *et al.* [[29](#bib.bib29)] 设计了一种迭代算法和展开管道，以确保非阴影区域之间的一致映射，并自适应地在阴影区域之间执行细粒度的空间映射。此外，可以根据预定义的阴影掩膜将阴影图像划分为阴影区域和非阴影区域，这对于阴影去除非常有效。提出了一种
    Mask-ShadowNet [[53](#bib.bib53)]，包括一个带有嵌入对齐器的掩膜自适应实例归一化层，并通过不同的统计信息分别处理阴影区域和非阴影区域。为了确保阴影去除和生成之间的一致性，Zhu
    *et al.* [[54](#bib.bib54)] 提出了一个双射映射网络 (BMNet)，以在前向过程中在颜色图和阴影掩膜的指导下恢复底层背景内容。Guo
    *et al.* [[55](#bib.bib55)] 提出了 ShadowFormer，利用来自非阴影区域的照明信息，通过定制的变换器块帮助阴影区域恢复。
- en: '![Refer to caption](img/f65be8df6ef628777b2a9bc6c64f9312.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f65be8df6ef628777b2a9bc6c64f9312.png)'
- en: 'Figure 3: Illustration of common deep learning strategies for single-image
    shadow removal. Details refer to Section [III-B](#S3.SS2 "III-B General Learning
    Strategies ‣ III Deep Learning Based Shadow Removal ‣ Single-Image Shadow Removal
    Using Deep Learning: A Comprehensive Survey").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：单幅图像阴影去除的常见深度学习策略的示意图。详细信息请参见第 [III-B](#S3.SS2 "III-B 一般学习策略 ‣ III 基于深度学习的阴影去除
    ‣ 使用深度学习的单幅图像阴影去除：全面调查") 节。
- en: Semi-Supervised Learning (SSL) In recent years, semi-supervised learning has
    emerged as an approach to leverage the strengths of both supervised learning and
    unsupervised learning. It leverages both paired data and unpaired data to boost
    the model performance and improve generalization ability. Ding *et al.* [[31](#bib.bib31)]
    propose a semi-supervised deep attentive recurrent generative adversarial network
    (ARGAN) to progressively detect and remove shadows. The generator of ARGAN performs
    multiple progressive steps in a coarse-to-fine manner, serving the dual purpose
    of generating shadow attention maps and recovering the shadow-removal images.
    The semi-supervised strategy is implemented by incorporating a significant number
    of unsupervised shadow images available online during the training process, which
    achieves better robustness and performance to complicated environment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习（SSL）近年来，半监督学习作为一种方法应运而生，它结合了监督学习和无监督学习的优点。它利用配对数据和未配对数据来提升模型性能和改进泛化能力。丁*等人*[[31](#bib.bib31)]
    提出了一个半监督深度注意力递归生成对抗网络（ARGAN），以逐步检测和去除阴影。ARGAN 的生成器以粗到细的方式执行多个渐进步骤，既生成阴影注意力图，又恢复去除阴影后的图像。半监督策略通过在训练过程中引入大量在线获取的无监督阴影图像来实现，这种方法在复杂环境中表现出更好的鲁棒性和性能。
- en: Unsupervised Learning (UL) While supervised learning methods have shown remarkable
    success in various scenarios, they heavily rely on large amounts of paired training
    data, which can be costly and time-consuming to obtain. Besides, training a deep
    model on paired data is typically designed to solve specific tasks, which struggle
    to adapt to new or out-of-distribution cases without retraining. To address this
    issue, inspired by unsupervised image translation methods, Hu *et al.* [[56](#bib.bib56)]
    propose the mask guided cycle Generative Adversarial Network (GAN) using dual-generator,
    named Mask-ShadowGAN, to employ the unpaired data for shadow removal. Similarly,
    Liu *et al.* [[57](#bib.bib57)] also follow the cycle consistency pipeline while
    integrate the lightness information into the generator. Different from the previous
    works requiring the pre-defined shadow mask, the DC-ShadowNet [[58](#bib.bib58)]
    is proposed to employ the shadow/shadow-free domain classifier as the guidance
    and a series of loss functions to enable the generator focus on the shadow regions.
    He *et al.* [[59](#bib.bib59)] focus on portrait shadow removal instead of general
    one, which proposes to solve shadow removal task as a layer decomposition problem
    and leverages the generative facial priors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习（UL）虽然监督学习方法在各种场景中取得了显著成功，但它们严重依赖大量的配对训练数据，这些数据获取成本高且耗时。此外，在配对数据上训练深度模型通常是为了解决特定任务，这使得模型在面对新情况或分布外的情况时难以适应。为了解决这个问题，受无监督图像翻译方法的启发，胡*等人*[[56](#bib.bib56)]
    提出了使用双生成器的掩模引导循环生成对抗网络（GAN），命名为Mask-ShadowGAN，利用未配对数据进行阴影去除。同样，刘*等人*[[57](#bib.bib57)]
    也遵循循环一致性流程，同时将亮度信息集成到生成器中。与之前需要预定义阴影掩模的工作不同，DC-ShadowNet[[58](#bib.bib58)] 提出采用阴影/无阴影领域分类器作为指导，并结合一系列损失函数，使生成器专注于阴影区域。何*等人*[[59](#bib.bib59)]
    专注于肖像阴影去除，而非一般的阴影去除，提出将阴影去除任务作为层次分解问题来解决，并利用生成面部先验。
- en: Zero-Shot Learning (ZSL) Supervised learning, and unsupervised learning methods
    exhibit limitations in terms of their ability to generalize or undergo stable
    training. To address these challenges, the concept of zero-shot learning has been
    introduced in low-level vision tasks. In this context, zero-shot learning refers
    to the approach of learning image enhancement solely from testing images, without
    the need for paired or unpaired training data. It is important to note that the
    definition of zero-shot learning in low-level vision tasks differs from its definition
    in high-level visual tasks. The goal is to emphasize that the method does not
    rely on specific training data arrangements, aiming to overcome the limitations
    and instability associated with other learning methods in low-level vision tasks [[60](#bib.bib60),
    [61](#bib.bib61)]. Shadows, in fact, only form partial degradation in images,
    while their non-shadow regions provide rich structural and illumination information
    for shadow regions’ restoration. Based on that, Le *et al.* [[62](#bib.bib62)]
    propose to crop image patches with and without shadows from shadow images as the
    unpaired training samples, which can be trained using shadow images themselves.
    Then they introduce a set of physics-based constraints that enables the adversarial
    training. After that, Liu *et al.* [[63](#bib.bib63)] propose the G2R-ShadowNet
    to leverage shadow generation to simulate shadow degradation in non-shadow regions,
    leading to paired data for training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习（ZSL）监督学习和无监督学习方法在泛化能力或稳定训练方面存在局限性。为了解决这些挑战，引入了零样本学习的概念，特别是在低级视觉任务中。在这种情况下，零样本学习指的是仅从测试图像中学习图像增强，而不需要成对或非成对的训练数据。值得注意的是，零样本学习在低级视觉任务中的定义与在高级视觉任务中的定义有所不同。目标是强调该方法不依赖于特定的训练数据安排，旨在克服其他学习方法在低级视觉任务中存在的局限性和不稳定性
    [[60](#bib.bib60), [61](#bib.bib61)]。事实上，阴影仅在图像中形成部分退化，而其非阴影区域提供了丰富的结构和照明信息，用于恢复阴影区域。基于这一点，Le
    *et al.* [[62](#bib.bib62)] 提出了从阴影图像中裁剪有阴影和无阴影的图像块作为非配对训练样本，这些样本可以使用阴影图像本身进行训练。随后，他们引入了一组基于物理的约束来实现对抗训练。之后，Liu
    *et al.* [[63](#bib.bib63)] 提出了 G2R-ShadowNet，通过阴影生成来模拟非阴影区域的阴影退化，从而生成配对数据用于训练。
- en: '![Refer to caption](img/95c38babdffc11a0c321bd7fcc96169a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/95c38babdffc11a0c321bd7fcc96169a.png)'
- en: 'Figure 4: Illustration of the three major artifacts/challenges in deep learning-based
    shadow removal, i.e., boundary trace (top), color and illumination inconsistency
    (middle), and structure distortion (bottom). Results by the representative method
    for each challenge are shown, i.e., BMNet [[54](#bib.bib54)], ShadowDiffusion [[30](#bib.bib30)],
    and DC-ShadowNet [[58](#bib.bib58)], respectively.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：深度学习基础的阴影去除中的三大主要伪影/挑战的示意图，即边界痕迹（上）、颜色和照明不一致（中）以及结构失真（下）。每个挑战的代表性方法的结果如图所示，即
    BMNet [[54](#bib.bib54)]、ShadowDiffusion [[30](#bib.bib30)] 和 DC-ShadowNet [[58](#bib.bib58)]。
- en: III-C Challenges and Analysis
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 挑战与分析
- en: 'The goal of single-image shadow removal is to eliminate shadows from images
    while maintaining or restoring the original appearance of the scene. However,
    there are a few challenges to accomplishing the goal (refer to Figure [4](#S3.F4
    "Figure 4 ‣ III-B General Learning Strategies ‣ III Deep Learning Based Shadow
    Removal ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey")):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 单图像阴影去除的目标是去除图像中的阴影，同时保持或恢复场景的原始外观。然而，实现这一目标面临一些挑战（参见图 [4](#S3.F4 "图4 ‣ III-B
    一般学习策略 ‣ III 基于深度学习的阴影去除 ‣ 使用深度学习的单图像阴影去除：全面调查")）：
- en: '![Refer to caption](img/58fcf5786e1dbc11af6f0b1d6fb3412a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/58fcf5786e1dbc11af6f0b1d6fb3412a.png)'
- en: (a) Learning Strategy
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 学习策略
- en: '![Refer to caption](img/23c6057d474be5fe564a8eb4398b12f0.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/23c6057d474be5fe564a8eb4398b12f0.png)'
- en: (b) Network Structure
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 网络结构
- en: '![Refer to caption](img/4ae3562ee5bec741348ecefdd2182638.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/4ae3562ee5bec741348ecefdd2182638.png)'
- en: (c) Mask Input
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 掩模输入
- en: '![Refer to caption](img/24a82972d45092b4113ba3b9b884ee41.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/24a82972d45092b4113ba3b9b884ee41.png)'
- en: (d) Physic Model
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 物理模型
- en: '![Refer to caption](img/e03dd0fc4f6be51f1d9b9c0514c6022d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/e03dd0fc4f6be51f1d9b9c0514c6022d.png)'
- en: (e) Training Dataset
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 训练数据集
- en: '![Refer to caption](img/b5c234f8ed0c0cf6912497ac4598550e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/b5c234f8ed0c0cf6912497ac4598550e.png)'
- en: (f) Testing Dataset
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 测试数据集
- en: '![Refer to caption](img/12bbb381b33a10de8eb07461dc9014e4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/12bbb381b33a10de8eb07461dc9014e4.png)'
- en: (g) Loss Function
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 损失函数
- en: '![Refer to caption](img/3502ed5342267dbc5e9a8f7dac44368e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3502ed5342267dbc5e9a8f7dac44368e.png)'
- en: (h) Evaluation Metric
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: （h）评估指标
- en: 'Figure 5: A statistical analysis of the number of deep learning-based shadow
    removal methods, according to their learning strategy, network characteristic,
    mask input, physic model, training dataset, testing dataset, loss function, and
    evaluation metric.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于深度学习的阴影去除方法的数量统计分析，包括他们的学习策略、网络特征、掩膜输入、物理模型、训练数据集、测试数据集、损失函数和评估指标。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Trace-less result. The patterns of shadows are characterized by their arbitrary
    nature, diversity, and occasionally, by their highly intricate trace structures,
    which present substantial challenges in achieving “trace-less” image restoration.
    Consequently, the fabrication of a shadow mask that accurately and meticulously
    captures the shadow patterns proves to be a formidable task. Moreover, the variations
    in illumination around the shadow boundaries are pronounced, complicating the
    modeling of the sophisticated and abrupt changes in these boundary regions. As
    a consequence of these complexities, the removal of shadows, particularly those
    with hard, sharp edges, is prone to the generation of artifacts along the edges
    where the shadow interfaces with the non-shadowed areas. In addressing this challenge,
    conventional approaches [[16](#bib.bib16), [17](#bib.bib17)] typically involve
    smoothing the gradient around the boundary. Similarly, deep learning-based methods [[58](#bib.bib58),
    [64](#bib.bib64)] have employed customized Total Variation (TV) loss functions
    in boundary regions to mitigate boundary artifacts.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无痕效果。阴影的阴影模式特点是它们的任意性、多样性和有时高度复杂的轨迹结构，这在实现“无痕”图像恢复中提出了重大挑战。因此，制作一张准确且详细捕捉阴影模式的阴影掩膜被证明是一个艰巨的任务。此外，阴影边界周围的照明变化很明显，使得对这些边界区域中复杂而突然的变化建模变得复杂。由于这些复杂性，去除阴影，特别是那些具有硬、锐利边缘的阴影，容易在阴影接口与非阴影区域相交的边缘产生伪影。在应对这一挑战时，传统方法[[16](#bib.bib16)，[17](#bib.bib17)]通常涉及平滑边界周围的梯度。同样，基于深度学习的方法[[58](#bib.bib58)，[64](#bib.bib64)]在边界区域中采用定制的总变差（Total
    Variation，TV）损失函数来减少边界伪影。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Illumination and color consistency. Shadows represent more than just areas of
    darker pixels; they encompass complex visual phenomena characterized by variations
    in color, texture, and brightness. These characteristics are influenced by the
    properties of both the light source and the surfaces they interact with. Furthermore,
    shadows, despite their impact on image quality, constitute only partial degradation
    within images. Conversely, the non-shadowed regions offer rich and accurate color
    and illumination information, serving as a reference standard. As a result, the
    goal of shadow removal is not only to enhance the shadow regions but also to pursue
    consistent illumination and color compared to non-corrupted (non-shadow) regions.
    Several existing deep learning-based methodologies have endeavored to expand the
    receptive field to incorporate valuable information from non-shadowed regions
    as guidance. This has been achieved through various techniques, including the
    fusion of multi-level features [[24](#bib.bib24)], employment of dilated convolutions [[48](#bib.bib48)],
    utilization of patch matching algorithms [[50](#bib.bib50)], and incorporation
    of transformer mechanisms [[55](#bib.bib55)].
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 照明和色彩一致性。阴影不仅仅代表了较暗像素的区域；它们还包括了由颜色、纹理和亮度变化所特征化的复杂视觉现象。这些特征受到光源和它们所交互的表面的属性的影响。此外，尽管影响图像质量，但阴影仅构成图像内部的部分退化。相反，非阴影区域提供了丰富准确的色彩和照明信息，作为参考标准。因此，去除阴影的目标不仅是增强阴影区域，而且是追求与非污染（非阴影）区域相比的一致的照明和色彩。一些现有的基于深度学习的方法学在扩大感受野以综合来自非阴影区域的有价值信息方面进行了努力。这一目标通过各种技术实现，包括多层特征融合[[24](#bib.bib24)]，扩张卷积的使用[[48](#bib.bib48)]，补丁匹配算法的利用[[50](#bib.bib50)]和转换机制的结合[[55](#bib.bib55)]。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Structure distortion. Data fidelity is a prevalent concern in various image
    restoration tasks, including shadow removal, where the aim is to produce high-fidelity
    results that closely resemble the original scene. Particularly within the context
    of shadow removal, unique challenges arise. On the one hand, objects with intricate
    textures or reflective surfaces can exhibit complex shadow patterns that are challenging
    to remove without affecting the underlying structure. The removal process must
    distinguish between shadow-related texture and genuine object details to avoid
    distortion. Besides, the boundaries between shadowed and non-shadowed areas can
    be ambiguous or ill-defined, particularly in scenes with diffuse lighting or complex
    geometry. Errors in delineating these boundaries can lead to inaccuracies in shadow
    removal and subsequent structure distortion. On the other hand, some works [[45](#bib.bib45),
    [56](#bib.bib56)] applied adversarial losses for training shadow removal networks
    and tried to model the image distribution explicitly. However, these models usually
    hallucinate unrealistic image contents and structures. To mitigate this issue,
    some existing methods have adopted perceptual loss techniques to ensure structural
    consistency between input shadow images and the resulting outputs.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结构扭曲。数据保真度是各种图像恢复任务中的普遍关注点，包括阴影去除，其目标是生成高保真度的结果，尽可能接近原始场景。特别是在阴影去除的背景下，会出现独特的挑战。一方面，具有复杂纹理或反射表面的物体可能会表现出复杂的阴影模式，这些模式难以去除而不影响基础结构。去除过程必须区分阴影相关纹理和真正的物体细节，以避免扭曲。此外，阴影区和非阴影区之间的边界可能模糊或界定不清，尤其是在光线散射或几何形状复杂的场景中。划定这些边界的错误可能导致阴影去除不准确和随后的结构扭曲。另一方面，一些工作[[45](#bib.bib45),
    [56](#bib.bib56)]应用了对抗性损失来训练阴影去除网络，并尝试显式建模图像分布。然而，这些模型通常会产生不现实的图像内容和结构。为了解决这个问题，一些现有方法已经采用了感知损失技术，以确保输入阴影图像与结果输出之间的结构一致性。
- en: IV Technical Review and Discussion
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 技术审查与讨论
- en: 'In this section, we review and discuss the technical details and employed priors
    designed to address the aforementioned challenges in Section [III-C](#S3.SS3 "III-C
    Challenges and Analysis ‣ III Deep Learning Based Shadow Removal ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey") from different perspectives,
    such as network structure, integrating physical illumination models, exploiting
    shadow detection for removal, and using deep generative models for shadow removal.
    We then summarize the widely used loss functions, training and testing datasets,
    and evaluation metrics.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们从不同的角度审查和讨论了技术细节和采用的先验，旨在解决第[III-C](#S3.SS3 "III-C Challenges and Analysis
    ‣ III Deep Learning Based Shadow Removal ‣ Single-Image Shadow Removal Using Deep
    Learning: A Comprehensive Survey")节中提到的挑战，例如网络结构、整合物理照明模型、利用阴影检测进行去除，以及使用深度生成模型进行阴影去除。随后，我们总结了广泛使用的损失函数、训练和测试数据集以及评估指标。'
- en: IV-A Network Structure
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 网络结构
- en: 'As deep learning techniques advance, there’s a noticeable trend in newly proposed
    methods to incorporate more sophisticated basic blocks with enhanced modeling
    capabilities. These blocks are subsequently integrated to build more complex shadow
    removal networks. In the following section, we provide detailed designs and rationales
    for various basic blocks employed in deep learning-based shadow removal networks,
    as illustrated in Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '随着深度学习技术的发展，新提出的方法中有明显的趋势，即采用更复杂的基础模块来增强建模能力。这些模块随后被整合以构建更复杂的阴影去除网络。在以下部分，我们提供了详细的设计和理由，说明在深度学习基础的阴影去除网络中采用的各种基础模块，如图[6](#S4.F6
    "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and Discussion ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")所示。'
- en: 'U-Net and Multi-Scale Addressing shadow removal as a partial corruption problem
    brings forth its own set of unique challenges, prompting us to emphasize the importance
    of maximizing the receptive field of the model to encompass as much non-corrupted
    context information as possible. The widely adopted U-Net-like structure [[65](#bib.bib65)]
    (Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and
    Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")(a)) serves as a fundamental backbone in shadow removal tasks, representing
    approximately 69% of the approaches, as depicted in Figure [5](#S3.F5 "Figure
    5 ‣ III-C Challenges and Analysis ‣ III Deep Learning Based Shadow Removal ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(b). This architecture
    has garnered widespread usage due to its ability to preserve both low-level structural
    details and high-level semantic information, facilitating high-quality restoration
    of shadow-free images while accurately identifying shadow regions. Moreover, alternative
    structural designs incorporate multi-scale information using various dilated kernels
    (Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and
    Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")(b)) or by leveraging different feature maps extracted from pre-trained
    models, such as VGG16 (Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣
    IV Technical Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey")(f)). This approach also enables the capture of diverse
    information across multiple levels and effectively enlarging the receptive field.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net和多尺度解决阴影去除作为部分腐败问题带来了独特的挑战，促使我们强调最大化模型的接收场以涵盖尽可能多的未腐败上下文信息的重要性。广泛采用的类似U-Net的结构
    [[65](#bib.bib65)]（图 [6](#S4.F6 "图 6 ‣ IV-A 网络结构 ‣ IV 技术评审和讨论 ‣ 使用深度学习的单幅图像阴影去除：综合调查")(a)）作为阴影去除任务的基础骨架，代表了约69%的方法，如图
    [5](#S3.F5 "图 5 ‣ III-C 挑战与分析 ‣ III 基于深度学习的阴影去除 ‣ 使用深度学习的单幅图像阴影去除：综合调查")(b) 所示。该结构因其能够保留低层次的结构细节和高层次的语义信息，促进了高质量的无阴影图像恢复，同时准确识别阴影区域，因此获得了广泛使用。此外，替代的结构设计通过使用各种膨胀卷积核（图
    [6](#S4.F6 "图 6 ‣ IV-A 网络结构 ‣ IV 技术评审和讨论 ‣ 使用深度学习的单幅图像阴影去除：综合调查")(b)）或利用从预训练模型中提取的不同特征图（如VGG16，图
    [6](#S4.F6 "图 6 ‣ IV-A 网络结构 ‣ IV 技术评审和讨论 ‣ 使用深度学习的单幅图像阴影去除：综合调查")(f)）来整合多尺度信息。这种方法还能够捕获跨多个层次的多样化信息，并有效扩大接收场。
- en: One-Stage v.s. Multi-Stage A few methods incorporate the use of two-stage or
    multi-stage pipelines for shadow removal. Such a strategy enables the application
    of fine-grained constraints at different stages of the pipeline, facilitating
    more precise and effective restoration. By dividing the whole restoration process
    into multiple stages, each stage can conduct its specific set of constraints.
    For instance, certain techniques [[51](#bib.bib51), [52](#bib.bib52)] integrate
    an additional refinement module following the initial processing stage. This refinement
    step aims to further enhance the structural details or mitigate any artifacts
    present in the output produced by the preceding stage. Nevertheless, in contrast
    to single-stage methods, they often demand double or even multiple times the number
    of parameters and inference time.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 单阶段 vs 多阶段 一些方法采用两阶段或多阶段管道进行阴影去除。这种策略能够在管道的不同阶段应用细粒度约束，促进更精确和有效的恢复。通过将整个恢复过程划分为多个阶段，每个阶段可以执行其特定的约束集。例如，某些技术
    [[51](#bib.bib51), [52](#bib.bib52)] 在初步处理阶段后集成了额外的精细化模块。这个精细化步骤旨在进一步增强结构细节或减少前一阶段产生的输出中的任何伪影。然而，与单阶段方法相比，它们通常需要双倍甚至多倍的参数和推断时间。
- en: '![Refer to caption](img/7f604e22ffb9e82f121750a851930a73.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7f604e22ffb9e82f121750a851930a73.png)'
- en: 'Figure 6: The network architectures of the building blocks that are widely
    used in deep shadow removal algorithms.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在深度阴影去除算法中广泛使用的构建模块的网络结构。
- en: 'CNN v.s. Transformer The era of deep learning-based shadow removal began in
    the year 2017 with the work of Qu *et al.* [[24](#bib.bib24)]. They introduced
    a pioneering multi-branch fusion architecture (Figure [6](#S4.F6 "Figure 6 ‣ IV-A
    Network Structure ‣ IV Technical Review and Discussion ‣ Single-Image Shadow Removal
    Using Deep Learning: A Comprehensive Survey")(f)) designed to independently extract
    both local and global information using cascaded CNNs across different layers.
    Following this, a series of CNN-based networks [[51](#bib.bib51), [62](#bib.bib62),
    [29](#bib.bib29), [48](#bib.bib48), [58](#bib.bib58)] are constructed to improve
    the performance of shadow removal. After that, the transformer mechanism [[32](#bib.bib32)]
    capitalizes on long-range dependencies within contexts, leading to notable enhancements
    across various vision tasks, including image restoration and enhancement. In scenarios
    involving partial corruption, such as shadow removal, transformer-based networks
    exhibit greater potential due to the limited availability of corrupted contextual
    information to guide restoration. As a result, transformer-based models (Figure [6](#S4.F6
    "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and Discussion ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(c)) are becoming
    increasingly prevalent in the field of shadow removal [[66](#bib.bib66), [55](#bib.bib55)].'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'CNN 与 Transformer 深度学习时代的阴影去除始于2017年，由Qu *et al.* [[24](#bib.bib24)]的工作开创了这一领域。他们引入了一种开创性的多分支融合架构（图[6](#S4.F6
    "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and Discussion ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(f)），旨在通过不同层次的级联CNNs独立提取局部和全局信息。随后，构建了一系列基于CNN的网络[[51](#bib.bib51),
    [62](#bib.bib62), [29](#bib.bib29), [48](#bib.bib48), [58](#bib.bib58)]以提升阴影去除的性能。之后，transformer机制[[32](#bib.bib32)]利用上下文中的长距离依赖性，显著提升了包括图像恢复和增强在内的各种视觉任务的效果。在部分损坏的场景中，如阴影去除，由于受损的上下文信息有限，transformer-based模型展现出更大的潜力（图[6](#S4.F6
    "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and Discussion ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(c)）。因此，基于transformer的模型在阴影去除领域变得越来越普遍[[66](#bib.bib66),
    [55](#bib.bib55)]。'
- en: 'Recurrent and Recursive Recurrent neural networks (RNNs) can be used in an
    iterative manner to refine the restoration results over multiple iterations. This
    iterative refinement process allows RNN-based models to effectively address complex
    restoration tasks that require fine-grained adjustments and corrections. Certain
    existing methods [[31](#bib.bib31)] have employed recurrent networks, depicted
    in Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review
    and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")(d), to gradually detect and eliminate shadows using a sequential process.
    Hence, employing a recurrent unit like Long Short-Term Memory (LSTM) can ensure
    the preservation of valuable and detailed information, thereby enhancing the accuracy
    of detected shadow regions and producing increasingly realistic shadow-removal
    images. By dividing the process into multiple stages, each with its specific set
    of constraints, the system can better address various aspects of shadow removal,
    resulting in enhanced performance and accuracy.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '循环和递归循环神经网络（RNNs）可以以迭代的方式使用，以在多个迭代中细化恢复结果。这个迭代细化过程使得基于RNN的模型能够有效处理那些需要细致调整和修正的复杂恢复任务。某些现有方法[[31](#bib.bib31)]已经采用了循环网络，如图[6](#S4.F6
    "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and Discussion ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(d)所示，通过顺序过程逐渐检测和消除阴影。因此，使用如长短期记忆（LSTM）这样的循环单元可以确保保留有价值和详细的信息，从而提高检测到的阴影区域的准确性，并产生越来越逼真的去阴影图像。通过将过程分成多个阶段，每个阶段都有其特定的约束条件，系统可以更好地处理阴影去除的各个方面，从而提升性能和准确性。'
- en: 'Attention Mechanism Shadows often occur in specific regions of an image, and
    the attention mechanism allows the model to selectively focus on these shadowed
    area. By attending to relevant parts of the image, the model can effectively target
    shadow regions for removal while preserving non-shadowed areas. Some methods [[45](#bib.bib45),
    [56](#bib.bib56)] directly serve the shadow mask as the attention for the removal.
    Others predict a learnable attentive map as the auxiliary module by considering
    the direction (Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")(g)) or recurrent attention [[67](#bib.bib67)].'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制 阴影通常发生在图像的特定区域，注意力机制使模型能够选择性地关注这些阴影区域。通过关注图像中的相关部分，模型可以有效地定位阴影区域进行去除，同时保留非阴影区域。一些方法 [[45](#bib.bib45),
    [56](#bib.bib56)] 直接将阴影遮罩作为去除的注意力。其他方法通过考虑方向（图 [6](#S4.F6 "图 6 ‣ IV-A 网络结构 ‣ IV
    技术审查与讨论 ‣ 使用深度学习进行单图像阴影去除：全面综述")(g)）或递归注意力 [[67](#bib.bib67)] 预测一个可学习的注意力图作为辅助模块。
- en: 'TABLE I: Summary of key characteristics of notable deep learning-based methods.
    “Retinex” denotes whether the models are based on the Retinex theory. “Simulated”
    indicates whether the testing data are simulated in the same manner as the synthetic
    training data. “Mask” shows whether the models require a mask. “#P” represents
    the number of trainable parameters.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：著名深度学习方法的关键特征总结。“Retinex”表示模型是否基于 Retinex 理论。“Simulated”表示测试数据是否以与合成训练数据相同的方式进行模拟。“Mask”显示模型是否需要遮罩。“#P”表示可训练参数的数量。
- en: '|  | Method | Learning | Network Structure | Loss Function | Training Data
    | Testing Data | Evaluation Metric | Platform | Physic | Mask |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 学习 | 网络结构 | 损失函数 | 训练数据 | 测试数据 | 评估指标 | 平台 | 物理 | 遮罩 |'
- en: '| 2017 | DeShadowNet [[24](#bib.bib24)] | SL |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | DeShadowNet [[24](#bib.bib24)] | SL |'
- en: '&#124; multi-branch fusion &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多分支融合 &#124;'
- en: '&#124; semantic network, end-to-end &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语义网络，端到端 &#124;'
- en: '| MSE loss (log space) | SRD | UIUC LRSS SRD | RMSE SSIM (LAB space) | Caffe
    |  |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MSE 损失（对数空间） | SRD | UIUC LRSS SRD | RMSE SSIM（LAB 空间） | Caffe |  |  |'
- en: '| 2018 | ST-CGAN [[45](#bib.bib45)] | SL |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | ST-CGAN [[45](#bib.bib45)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类 U-Net 网络 &#124;'
- en: '&#124; joint learning &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 联合学习 &#124;'
- en: '&#124; GAN, end-to-end &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN，端到端 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; adversarial loss &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 &#124;'
- en: '&#124; detection loss &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测损失 &#124;'
- en: '| ISTD SBU | SBU UCF ISTD | RMSE (LAB space) | PyTorch |  |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ISTD SBU | SBU UCF ISTD | RMSE（LAB 空间） | PyTorch |  |  |'
- en: '| 2019 | SP+M-Net [[51](#bib.bib51)] | SL |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | SP+M-Net [[51](#bib.bib51)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类 U-Net 网络 &#124;'
- en: '&#124; regression model(ResNeXt) &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回归模型（ResNeXt）&#124;'
- en: '&#124; two subnetworks &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两个子网络 &#124;'
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; regression loss &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回归损失 &#124;'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 &#124;'
- en: '| ISTD SBU | SBU UCF ISTD | RMSE (LAB space) | PyTorch | ✓ | ✓ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ISTD SBU | SBU UCF ISTD | RMSE（LAB 空间） | PyTorch | ✓ | ✓ |'
- en: '|  | DSC [[46](#bib.bib46)] | SL |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | DSC [[46](#bib.bib46)] | SL |'
- en: '&#124; spatial RNN &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间 RNN &#124;'
- en: '&#124; direction-aware attention &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方向感知注意力 &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; colour loss &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 颜色损失 &#124;'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 &#124;'
- en: '| ISTD SBU | SBU UCF ISTD | RMSE (LAB space) | Caffe |  |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ISTD SBU | SBU UCF ISTD | RMSE（LAB 空间） | Caffe |  |  |'
- en: '|  | Mask-ShadowGAN [[56](#bib.bib56)] | UL |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | Mask-ShadowGAN [[56](#bib.bib56)] | UL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类 U-Net 网络 &#124;'
- en: '&#124; GAN &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN &#124;'
- en: '&#124; dual-generator &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 双生成器 &#124;'
- en: '|'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; cycle-consistency loss &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环一致性损失 &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '&#124; identity loss &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 识别损失 &#124;'
- en: '| SRD ISTD USR | SRD ISTD USR |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SRD ISTD USR | SRD ISTD USR |'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE（LAB 空间）&#124;'
- en: '&#124; User Study &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | ARGAN [[31](#bib.bib31)] | SSL |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | ARGAN [[31](#bib.bib31)] | SSL |'
- en: '&#124; GAN &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN &#124;'
- en: '&#124; recursive network &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 递归网络 &#124;'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; detection loss &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测损失 &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '| SRD ISTD | SRD ISTD |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| SRD ISTD | SRD ISTD |'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE（LAB 空间）&#124;'
- en: '| TensorFlow |  |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow |  |  |'
- en: '| 2020 | RIS-GAN [[47](#bib.bib47)] | SL |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | RIS-GAN [[47](#bib.bib47)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类 U-Net 网络 &#124;'
- en: '&#124; four subnetworks &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四个子网络 &#124;'
- en: '&#124; GAN joint discriminator &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN 联合判别器 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; perceptual loss &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 ($\ell_{1}$) &#124;'
- en: '&#124; residual loss illumination loss &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 残差损失 照明损失 &#124;'
- en: '&#124; cross loss adversarial loss &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交叉损失 对抗损失 &#124;'
- en: '| ISTD SRD | ISTD SRD |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ISTD SRD | ISTD SRD |'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB空间) &#124;'
- en: '&#124; User Study &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 &#124;'
- en: '| TensorFlow |  |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow |  |  |'
- en: '|  | DHAN [[48](#bib.bib48)] | SL |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | DHAN [[48](#bib.bib48)] | SL |'
- en: '&#124; dilated convolution, GAN &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 膨胀卷积, GAN &#124;'
- en: '&#124; hierarchical attention &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分层注意力 &#124;'
- en: '&#124; spatial pooling pyramid &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间池化金字塔 &#124;'
- en: '|'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; perceptual loss &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; binary cross entropy loss &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 二元交叉熵损失 &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD SRD &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD SRD &#124;'
- en: '&#124; simulated &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟 &#124;'
- en: '|'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD SRD &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD SRD &#124;'
- en: '&#124; simulated &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟 &#124;'
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LPIPS SSIM &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LPIPS SSIM &#124;'
- en: '&#124; PSNR RMSE &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR RMSE &#124;'
- en: '| TensorFlow |  |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow |  |  |'
- en: '|  | Le *et al.* [[62](#bib.bib62)] | ZSL |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | Le *et al.* [[62](#bib.bib62)] | ZSL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类网络 &#124;'
- en: '&#124; regression network &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回归网络 &#124;'
- en: '&#124; GAN &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN &#124;'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; matting loss smoothness loss &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 抠图损失 平滑损失 &#124;'
- en: '&#124; boundary loss &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边界损失 &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '|'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD &#124;'
- en: '&#124; Video Dataset &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频数据集 &#124;'
- en: '|'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD &#124;'
- en: '&#124; Video Dataset &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频数据集 &#124;'
- en: '| RMSE (LAB space) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| RMSE (LAB空间) |'
- en: '&#124; TensorFlow &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow &#124;'
- en: '| ✓ | ✓ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ |'
- en: '| 2021 | Fu *et al.* [[52](#bib.bib52)] | SL |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Fu *et al.* [[52](#bib.bib52)] | SL |'
- en: '&#124; regression model &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回归模型 &#124;'
- en: '&#124; multi-stage, fusion network &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多阶段，融合网络 &#124;'
- en: '&#124; U-Net like network &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类网络 &#124;'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 ($\ell_{1}$) &#124;'
- en: '&#124; boundary gradient loss &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边界梯度损失 &#124;'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SRD ISTD ISTD+ &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SRD ISTD ISTD+ &#124;'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SRD ISTD ISTD+ &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SRD ISTD ISTD+ &#124;'
- en: '|'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB空间) &#124;'
- en: '| PyTorch | ✓ | ✓ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | ✓ | ✓ |'
- en: '|  | Mask-ShadowNet [[53](#bib.bib53)] | SL |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | Mask-ShadowNet [[53](#bib.bib53)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类网络 &#124;'
- en: '&#124; adaptive instance normalization &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自适应实例归一化 &#124;'
- en: '&#124; end-to-end &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 端到端 &#124;'
- en: '|'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; perceptual loss &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '|'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD &#124;'
- en: '| ISTD | RMSE (LAB space) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ISTD | RMSE (LAB空间) |'
- en: '&#124; PyTorch &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '|  | ✓ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ |'
- en: '|  | CANet [[50](#bib.bib50)] | SL |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | CANet [[50](#bib.bib50)] | SL |'
- en: '&#124; DenseNet &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DenseNet &#124;'
- en: '&#124; patch matching mechanism &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区块匹配机制 &#124;'
- en: '&#124; contextual feature transfer &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文特征传输 &#124;'
- en: '&#124; DenseUNet &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DenseUNet &#124;'
- en: '&#124; multi-stage &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多阶段 &#124;'
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; regression loss &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回归损失 &#124;'
- en: '&#124; reconstruction loss ($L_{2}$) &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 ($L_{2}$) &#124;'
- en: '&#124; cross-entropy loss &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交叉熵损失 &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; gradient loss &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 梯度损失 &#124;'
- en: '| SRD ISTD | SRD ISTD | RMSE (LAB space) | PyTorch |  |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| SRD ISTD | SRD ISTD | RMSE (LAB空间) | PyTorch |  |  |'
- en: '|  | G2R-ShadowNet [[63](#bib.bib63)] | ZSL |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | G2R-ShadowNet [[63](#bib.bib63)] | ZSL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类网络 &#124;'
- en: '&#124; GAN &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN &#124;'
- en: '&#124; three subnetworks &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三个子网络 &#124;'
- en: '|'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; adversarial loss &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '&#124; identity loss &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 身份损失 &#124;'
- en: '&#124; cycle-consistency loss &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环一致性损失 &#124;'
- en: '&#124; refinement loss area loss &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精细化损失 区域损失 &#124;'
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD &#124;'
- en: '&#124; Video Dataset &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频数据集 &#124;'
- en: '|'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD &#124;'
- en: '&#124; Video Dataset &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频数据集 &#124;'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB空间) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | LG-ShadowNet [[57](#bib.bib57)] | UL |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | LG-ShadowNet [[57](#bib.bib57)] | UL |'
- en: '&#124; multi-stage &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多阶段 &#124;'
- en: '&#124; GAN &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN &#124;'
- en: '&#124; U-Net like network &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类网络 &#124;'
- en: '|'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; identity loss &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 身份损失 &#124;'
- en: '&#124; cycle-consistency loss &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环一致性损失 &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '&#124; colour loss &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 颜色损失 &#124;'
- en: '| ISTD ISTD+ USR | ISTD ISTD+ USR |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| ISTD ISTD+ USR | ISTD ISTD+ USR |'
- en: '&#124; PSNR SSIM User Study &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM 用户研究 &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB空间) &#124;'
- en: '&#124; SSEQ NIQE DBCNN &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSEQ NIQE DBCNN &#124;'
- en: '&#124; Runtime #P FLOPs &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运行时 #P FLOPs &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | DC-ShadowNet [[58](#bib.bib58)] | UL |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | DC-ShadowNet [[58](#bib.bib58)] | UL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类似网络 &#124;'
- en: '&#124; GAN &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN &#124;'
- en: '|'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; chromaticity loss &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 色度损失 &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; classification loss &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类损失 &#124;'
- en: '&#124; boundary smoothness loss &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边界平滑损失 &#124;'
- en: '&#124; adversarial loss identity loss &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗性损失 身份损失 &#124;'
- en: '&#124; reconstruction consistency loss &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建一致性损失 &#124;'
- en: '|'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SRD ISTD+ ISTD &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SRD ISTD+ ISTD &#124;'
- en: '&#124; USR LRSS &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; USR LRSS &#124;'
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SRD ISTD+ ISTD &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SRD ISTD+ ISTD &#124;'
- en: '&#124; USR LRSS &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; USR LRSS &#124;'
- en: '|'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR RMSE (LAB space) &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR RMSE (LAB 空间) &#124;'
- en: '| PyTorch |  |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  |  |'
- en: '| 2022 | Zhu *et al.* [[29](#bib.bib29)] | SL |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Zhu *et al.* [[29](#bib.bib29)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类似网络 &#124;'
- en: '&#124; unfolding &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 展开 &#124;'
- en: '&#124; recursive network &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 递归网络 &#124;'
- en: '|'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 &#124;'
- en: '&#124; regularization loss &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正则化损失 &#124;'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD SRD &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD SRD &#124;'
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD SRD &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD SRD &#124;'
- en: '|'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB 空间) &#124;'
- en: '&#124; Runtime #P FLOPs &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运行时 #P FLOPs &#124;'
- en: '| PyTorch | ✓ | ✓ |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | ✓ | ✓ |'
- en: '|  | BMNet [[54](#bib.bib54)] | SL | invertible network |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | BMNet [[54](#bib.bib54)] | SL | 可逆网络 |'
- en: '&#124; colour loss &#124;'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 颜色损失 &#124;'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 ($\ell_{1}$) &#124;'
- en: '|'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB 空间) &#124;'
- en: '|'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PyTorch &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '|  | ✓ |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ |'
- en: '|  | CNSNet [[66](#bib.bib66)] | SL |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | CNSNet [[66](#bib.bib66)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类似网络 &#124;'
- en: '&#124; transformer &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Transformer &#124;'
- en: '&#124; adaptive normalization &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自适应归一化 &#124;'
- en: '|'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 ($\ell_{1}$) &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; boundary gradient loss &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边界梯度损失 &#124;'
- en: '&#124; soft mask loss &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软掩膜损失 &#124;'
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB 空间) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | SG-ShadowNet [[68](#bib.bib68)] | SL |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | SG-ShadowNet [[68](#bib.bib68)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类似网络 &#124;'
- en: '&#124; style transfer &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 风格迁移 &#124;'
- en: '&#124; prototypical normalization &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原型归一化 &#124;'
- en: '|'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 ($\ell_{1}$) &#124;'
- en: '&#124; area loss &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面积损失 &#124;'
- en: '&#124; spatial consistency loss &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间一致性损失 &#124;'
- en: '|'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ SRD &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ SRD &#124;'
- en: '|'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ SRD &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ SRD &#124;'
- en: '&#124; Video Dataset &#124;'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频数据集 &#124;'
- en: '|'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LPIPS RMSE (LAB space) &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LPIPS RMSE (LAB 空间) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '| 2023 | BA-ShadowNet [[69](#bib.bib69)] | SL |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | BA-ShadowNet [[69](#bib.bib69)] | SL |'
- en: '&#124; encoder-decoder &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器-解码器 &#124;'
- en: '&#124; multi-branch &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多分支 &#124;'
- en: '|'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; boundary loss &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边界损失 &#124;'
- en: '&#124; similarity loss &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相似性损失 &#124;'
- en: '&#124; shadow-free consistency loss &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无阴影一致性损失 &#124;'
- en: '|'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ SRD &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ SRD &#124;'
- en: '|'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ SRD &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ SRD &#124;'
- en: '|'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB 空间) &#124;'
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PyTorch &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '|  | ✓ |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ |'
- en: '|  | DMTN [[70](#bib.bib70)] | SL |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | DMTN [[70](#bib.bib70)] | SL |'
- en: '&#124; encoder-decoder &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器-解码器 &#124;'
- en: '&#124; multi-branch &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多分支 &#124;'
- en: '|'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; perceptual loss &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 ($\ell_{1}$) &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗性损失 &#124;'
- en: '|'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB 空间) &#124;'
- en: '| PyTorch | ✓ |  |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | ✓ |  |'
- en: '|  | ShadowFormer [[55](#bib.bib55)] | SL |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | ShadowFormer [[55](#bib.bib55)] | SL |'
- en: '&#124; transformer &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Transformer &#124;'
- en: '&#124; encoder-decoder &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器-解码器 &#124;'
- en: '|'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 ($\ell_{1}$) &#124;'
- en: '|'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; #P RMSE (LAB space) &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; #P RMSE (LAB 空间) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | SHARDS [[71](#bib.bib71)] | SL |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | SHARDS [[71](#bib.bib71)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net 类似网络 &#124;'
- en: '&#124; two-stage &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; two-stage &#124;'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; adversarial loss &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; adversarial loss &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; perceptual loss &#124;'
- en: '| ISTD SFHQ | ISTD SFHQ | RMSE (LAB space) | PyTorch |  | ✓ |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| ISTD SFHQ | ISTD SFHQ | RMSE (LAB space) | PyTorch |  | ✓ |'
- en: '|  | LRA&LDRA [[72](#bib.bib72)] | SL |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  | LRA&LDRA [[72](#bib.bib72)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net like network &#124;'
- en: '&#124; residual learning &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; residual learning &#124;'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
- en: '| ISTD+ SRD | ISTD+ SRD |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| ISTD+ SRD | ISTD+ SRD |'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB space) &#124;'
- en: '&#124; Runtime #P FLOPs &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Runtime #P FLOPs &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | ShadowDiffusion [[30](#bib.bib30)] | SL |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | ShadowDiffusion [[30](#bib.bib30)] | SL |'
- en: '&#124; diffusion model &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; diffusion model &#124;'
- en: '&#124; unfolding &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; unfolding &#124;'
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; reconstruction loss &#124;'
- en: '&#124; mask loss &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; mask loss &#124;'
- en: '|'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB space) &#124;'
- en: '| PyTorch | ✓ | ✓ |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | ✓ | ✓ |'
- en: '|  | FSR-Net [[73](#bib.bib73)] | SL |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  | FSR-Net [[73](#bib.bib73)] | SL |'
- en: '&#124; multi-stage &#124;'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; multi-stage &#124;'
- en: '&#124; U-Net like network &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net like network &#124;'
- en: '|'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; reconstruction loss &#124;'
- en: '&#124; mask loss &#124;'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; mask loss &#124;'
- en: '|'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB space) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | Inpaint4Shadow [[74](#bib.bib74)] | SL |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | Inpaint4Shadow [[74](#bib.bib74)] | SL |'
- en: '&#124; fusion network &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; fusion network &#124;'
- en: '&#124; Resnet, inpainting network &#124;'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Resnet, inpainting network &#124;'
- en: '|'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; reconstruction loss &#124;'
- en: '&#124; mask loss &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; mask loss &#124;'
- en: '|'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB space) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | BCDiff [[64](#bib.bib64)] | ZSL |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  | BCDiff [[64](#bib.bib64)] | ZSL |'
- en: '&#124; diffusion model &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; diffusion model &#124;'
- en: '&#124; decomposition model &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; decomposition model &#124;'
- en: '|'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; reconstruction loss &#124;'
- en: '&#124; boundary loss &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; boundary loss &#124;'
- en: '|'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ Video &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ Video &#124;'
- en: '|'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ Video &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ Video &#124;'
- en: '|'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB space) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '| 2024 | Liu *et al.* [[75](#bib.bib75)] | SL |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 2024 | Liu *et al.* [[75](#bib.bib75)] | SL |'
- en: '&#124; encoder-decoder &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; encoder-decoder &#124;'
- en: '&#124; multi-branch &#124;'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; multi-branch &#124;'
- en: '|'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; boundary loss &#124;'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; boundary loss &#124;'
- en: '&#124; similarity loss &#124;'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; similarity loss &#124;'
- en: '&#124; shadow-free consistency loss &#124;'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; shadow-free consistency loss &#124;'
- en: '|'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ SRD &#124;'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ SRD &#124;'
- en: '|'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ SRD &#124;'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ SRD &#124;'
- en: '|'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB space) &#124;'
- en: '|'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PyTorch &#124;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '|  | ✓ |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ |'
- en: '|  | DeS3 [[76](#bib.bib76)] | SL |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '|  | DeS3 [[76](#bib.bib76)] | SL |'
- en: '&#124; diffusion model &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; diffusion model &#124;'
- en: '&#124; attention &#124;'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; attention &#124;'
- en: '&#124; classifier guidance &#124;'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; classifier guidance &#124;'
- en: '|'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; perceptual loss &#124;'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; perceptual loss &#124;'
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; adversarial loss &#124;'
- en: '|'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '&#124; LRSS UCF UIUC &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRSS UCF UIUC &#124;'
- en: '|'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM User Study &#124;'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM User Study &#124;'
- en: '&#124; RMSE (LAB space) &#124;'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RMSE (LAB space) &#124;'
- en: '| PyTorch |  |  |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  |  |'
- en: '|  | Mei *et al.* [[77](#bib.bib77)] | SL |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | Mei *et al.* [[77](#bib.bib77)] | SL |'
- en: '&#124; latent diffusion &#124;'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; latent diffusion &#124;'
- en: '&#124; multi-stage &#124;'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; multi-stage &#124;'
- en: '|'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; reconstruction loss &#124;'
- en: '|'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ SRD &#124;'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ SRD &#124;'
- en: '|'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD ISTD+ &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD ISTD+ &#124;'
- en: '&#124; SRD DeSOBA &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SRD DeSOBA &#124;'
- en: '|'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM Runtime &#124;'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM Runtime &#124;'
- en: '&#124; #P RMSE (LAB space) &#124;'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; #P RMSE (LAB space) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: '|  | HomoFormer [[78](#bib.bib78)] | SL |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '|  | HomoFormer [[78](#bib.bib78)] | SL |'
- en: '&#124; transformer &#124;'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; transformer &#124;'
- en: '&#124; U-Net like network &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net like network &#124;'
- en: '|'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; reconstruction loss &#124;'
- en: '|'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ SRD &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ SRD &#124;'
- en: '|'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISTD+ SRD &#124;'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISTD+ SRD &#124;'
- en: '|'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM Runtime &#124;'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM Runtime &#124;'
- en: '&#124; #P RMSE (LAB space) &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; #P RMSE (LAB space) &#124;'
- en: '| PyTorch |  | ✓ |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch |  | ✓ |'
- en: IV-B Integrating Physical Illumination Model
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 物理光照模型集成
- en: The physical properties of shadows are effective prior to shadow removal, which
    guarantees that the networks would learn physically plausible transformations.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在阴影去除之前，阴影的物理特性是有效的，这保证了网络能够学习到物理上合理的变换。
- en: 'Parameter estimation. Some studies [[51](#bib.bib51), [62](#bib.bib62)] employ
    a linear function to model the relationship between illuminated and shadowed pixels.
    They assume this linear relationship remains consistent during the camera’s color
    acquisition process [[79](#bib.bib79)]. Consequently, the illuminated image can
    be represented as a linear function of its shadowed values:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 参数估计。一些研究[[51](#bib.bib51), [62](#bib.bib62)]采用线性函数来建模被照亮像素和阴影像素之间的关系。他们假设这种线性关系在相机的颜色采集过程中保持一致[[79](#bib.bib79)]。因此，被照亮的图像可以表示为其阴影值的线性函数：
- en: '|  | $\displaystyle\mathbf{I}_{lit}=\omega\circ\mathbf{I}_{s}+b,$ |  | (5)
    |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{I}_{lit}=\omega\circ\mathbf{I}_{s}+b,$ |  | (5)
    |'
- en: '|  | $\displaystyle{\color[rgb]{0,0,0}{\hat{\mathbf{I}}_{sf}}}=\bm{\alpha}\circ\mathbf{I}_{s}+(\mathbf{1}-\bm{\alpha})\circ\mathbf{I}_{lit},$
    |  | (6) |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\color[rgb]{0,0,0}{\hat{\mathbf{I}}_{sf}}}=\bm{\alpha}\circ\mathbf{I}_{s}+(\mathbf{1}-\bm{\alpha})\circ\mathbf{I}_{lit},$
    |  | (6) |'
- en: where $\omega=\left[\omega_{R},\omega_{G},\omega_{B}\right],b=\left[b_{R},b_{G},b_{B}\right]$
    for different color channels. $b$ represents the camera’s response to direct illumination,
    while $\omega$ denotes the attenuation factor of ambient illumination for a given
    pixel in a particular color channel. With this assumption in mind, they employ
    a deep regression model to estimate the parameters $\omega$ and $b$ based on input
    data comprising both shadow and shadow-free images. Subsequently, an additional
    network, specialized for matte prediction, is applied to generate the pixel-wise
    shadow matte $\bm{\alpha}$. This matte effectively combines information from the
    illuminated and shadowed regions of the images.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\omega=\left[\omega_{R},\omega_{G},\omega_{B}\right],b=\left[b_{R},b_{G},b_{B}\right]$
    用于不同的颜色通道。$b$ 代表相机对直接照明的响应，而 $\omega$ 表示特定颜色通道中给定像素的环境照明衰减因子。基于这一假设，他们利用深度回归模型来估计参数
    $\omega$ 和 $b$，输入数据包括阴影图像和无阴影图像。随后，应用一个专门用于生成阴影掩膜的附加网络，生成逐像素的阴影掩膜 $\bm{\alpha}$。该掩膜有效地结合了图像中被照亮区域和阴影区域的信息。
- en: 'Deep unrolling. Another group of works [[29](#bib.bib29), [30](#bib.bib30)]
    employs deep unrolling to enhance the interpretability of models. This popular
    and powerful approach incorporates known physical priors into deep networks, achieving
    superior performance across various image restoration problems [[80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)].
    The shadow formation model ([2](#S2.E2 "In II-A Shadow Formation Model ‣ II Analysis
    on Shadow Formation ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")) can be re-formulated as'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 深度展开。另一组研究[[29](#bib.bib29), [30](#bib.bib30)]采用深度展开来增强模型的可解释性。这种流行且强大的方法将已知的物理先验纳入深度网络中，在各种图像恢复问题上取得了优异的性能[[80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)]。阴影形成模型（[2](#S2.E2
    "在 II-A 阴影形成模型 ‣ II 关于阴影形成的分析 ‣ 使用深度学习的单图像阴影去除：综合调查")）可以重新表述为
- en: '|  | $\mathbf{I}_{s}=\mathbf{A}\circ\mathbf{I}_{sf}\;.$ |  | (7) |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{I}_{s}=\mathbf{A}\circ\mathbf{I}_{sf}\;.$ |  | (7) |'
- en: 'Based on that, the shadow removal can be formulated as a degradation prior
    guided model, where regularization terms $\mathcal{R}(\cdot)$ are inferred by
    the learnable conditional deep CNN networks [[29](#bib.bib29)] or generative diffusion
    model [[30](#bib.bib30)] as follows:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，阴影去除可以被表述为一个退化先验引导的模型，其中正则化项 $\mathcal{R}(\cdot)$ 由可学习的条件深度卷积神经网络[[29](#bib.bib29)]或生成扩散模型[[30](#bib.bib30)]推断如下：
- en: '|  | $\hat{\mathbf{I}}_{sf}=\operatorname*{argmin}_{\mathbf{I}_{sf}}\&#124;\mathbf{I}_{s}-\mathbf{A}\circ\mathbf{I}_{sf}\&#124;_{F}^{2}+\mathcal{R}(\mathbf{I}_{sf})\;.$
    |  | (8) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{I}}_{sf}=\operatorname*{argmin}_{\mathbf{I}_{sf}}\&#124;\mathbf{I}_{s}-\mathbf{A}\circ\mathbf{I}_{sf}\&#124;_{F}^{2}+\mathcal{R}(\mathbf{I}_{sf})\;.$
    |  | (8) |'
- en: IV-C Exploiting Shadow Detection for Removal
  id: totrans-531
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 利用阴影检测进行去除
- en: 'Shadow removal often relies on accurate shadow detection. Before removing shadows
    from an image, it’s essential to identify where the shadows are present. Shadow
    detection algorithms help locate regions of shadow within an image, providing
    the necessary information for subsequent removal processes. Most existing shadow
    removal methods leverage rich semantic and structural information learnt from
    shadow detection as prior knowledge, which can roughly be divided into three widely
    used frameworks as shown in Figure [7](#S4.F7 "Figure 7 ‣ IV-C Exploiting Shadow
    Detection for Removal ‣ IV Technical Review and Discussion ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey"):'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '去除阴影通常依赖于准确的阴影检测。在从图像中去除阴影之前，识别阴影存在的位置至关重要。阴影检测算法帮助定位图像中的阴影区域，为后续去除过程提供必要的信息。现有的大多数阴影去除方法利用从阴影检测中学习到的丰富语义和结构信息作为先验知识，这些方法大致可以分为三种广泛使用的框架，如图[7](#S4.F7
    "Figure 7 ‣ IV-C Exploiting Shadow Detection for Removal ‣ IV Technical Review
    and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")所示：'
- en: '1.'
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Separate learning: Most shadow removal techniques (refer to the methods that
    require mask input in Table [I](#S4.T1 "TABLE I ‣ IV-A Network Structure ‣ IV
    Technical Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey")) incorporate a pre-trained shadow detection model before
    the removal stage. This model generates a corresponding shadow mask for the input
    shadow image, which serves as a conditional input for the shadow removal network.
    The shadow detection model remains fixed and operates as a pre-processing step
    for the shadow removal process.'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '分开学习：大多数阴影去除技术（参见表[I](#S4.T1 "TABLE I ‣ IV-A Network Structure ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")中需要掩膜输入的方法）在去除阶段之前包含一个预训练的阴影检测模型。该模型为输入的阴影图像生成相应的阴影掩膜，作为阴影去除网络的条件输入。阴影检测模型保持固定，并作为阴影去除过程的预处理步骤。'
- en: '2.'
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Joint learning: The adopted frozen shadow detection model may encounter a domain
    gap when tested on cases with distinct distributions from the training data. Inaccurate
    detection of shadows could misguide the subsequent shadow removal network. To
    address this, some approaches [[45](#bib.bib45)] have employed a joint training
    strategy, optimizing both the shadow detection and removal networks simultaneously.
    This allows for fine-tuning the shadow detector to the current data domain, effectively
    mitigating the domain gap issue.'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合学习：所采用的冻结阴影检测模型在处理具有与训练数据不同分布的情况时可能会遇到领域间隙。阴影检测不准确可能会误导后续的阴影去除网络。为了解决这个问题，一些方法[[45](#bib.bib45)]采用了联合训练策略，同时优化阴影检测和去除网络。这允许将阴影检测器微调到当前数据领域，有效缓解领域间隙问题。
- en: '3.'
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Multi-task learning: Another category of shadow removal methods [[48](#bib.bib48)]
    has adopted the multi-task learning framework to exploit shared representations
    and simultaneously output shadow mask and shadow-free results. This approach minimizes
    redundancy in feature representation learning and parameter sharing, resulting
    in more efficient utilization of computational resources and decreased inference
    time.'
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多任务学习：另一类阴影去除方法[[48](#bib.bib48)]采用多任务学习框架，以利用共享表示并同时输出阴影掩膜和去阴影结果。这种方法最小化了特征表示学习和参数共享中的冗余，从而更有效地利用计算资源并减少推理时间。
- en: '![Refer to caption](img/f83b2e1e8e5644a1d8b81de50357d88e.png)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f83b2e1e8e5644a1d8b81de50357d88e.png)'
- en: 'Figure 7: Illustration of the three strategies of combining shadow removal
    and detection, i.e., separate learning, joint learning, and multi-task learning
    strategies.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：结合阴影去除和检测的三种策略的示意图，即分开学习、联合学习和多任务学习策略。
- en: IV-D Deep Generative Model for Shadow Removal
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 深度生成模型用于阴影去除
- en: 'Generative models, by design, learn the underlying data distribution of training
    images. This allows them to generate or reconstruct images that are close to the
    real shadow-free data distribution, effectively filling in missing textures or
    colors and avoiding amplified artifacts, such as over-smoothness. This is particularly
    useful in shadow removal, where the model must understand the natural image prior
    to effectively removing shadows without leaving traces. Here, we primarily discuss
    two types of generative models that are widely used in shadow removal: Generative
    Adversarial Networks (GANs) [[86](#bib.bib86)] and diffusion models [[37](#bib.bib37)].'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型从设计上来说，通过学习训练图像的底层数据分布，使其能够生成或重建接近真实无阴影数据分布的图像，有效地填补缺失的纹理或颜色，并避免放大的伪影，例如过度平滑。这在阴影去除中尤其有用，因为模型必须理解自然图像才能有效地去除阴影而不留痕迹。这里，我们主要讨论两种广泛用于阴影去除的生成模型：生成对抗网络（GANs）[[86](#bib.bib86)]和扩散模型[[37](#bib.bib37)]。
- en: GAN-based methods. GAN-based models for shadow removal leverage the adversarial
    training framework to distinguish between shadow and non-shadow regions, aiming
    to generate shadow-free images that are indistinguishable from real, shadowless
    photos. While the integration of adversarial loss in shadow removal models can
    mitigate some issues such as boundary artifacts, such as those proposed by Wang *et
    al.* [[45](#bib.bib45)] and Hu *et al.* [[56](#bib.bib56)] necessitate careful
    adjustment during training. Furthermore, these methods may exhibit tendencies
    to overfit specific visual features or data distributions, potentially leading
    to the generation of hallucinated content and artifacts.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GAN 的方法。基于 GAN 的阴影去除模型利用对抗训练框架来区分阴影区域和非阴影区域，旨在生成与真实无阴影照片无法区分的无阴影图像。尽管将对抗损失集成到阴影去除模型中可以减轻一些问题，如边界伪影，但如
    Wang*等人*[[45](#bib.bib45)] 和 Hu*等人*[[56](#bib.bib56)] 提出的那些方法在训练过程中需要小心调整。此外，这些方法可能会表现出过拟合特定视觉特征或数据分布的倾向，可能导致生成的内容和伪影出现幻觉。
- en: 'Diffusion-based methods. Recently, numerous diffusion models, including the
    widely recognized diffusion denoising probability model (DDPM)[[37](#bib.bib37)],
    have garnered significant interest in the field of low-level vision[[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)]. Guo *et al.* [[30](#bib.bib30)]
    pioneered the integration of diffusion models into the domain of shadow removal.
    Subsequently, a series of methods [[76](#bib.bib76), [64](#bib.bib64), [77](#bib.bib77)]
    tend to employ the diffusion model as the backbone. While diffusion-based methods
    have demonstrated their efficacy in producing realistic shadow-free results, they
    are often accompanied by a significant drawback: the time-consuming nature of
    the inference process, primarily attributable to the iterative sampling procedures
    involved. In some applications, such as real-time processing, there may be strict
    requirements on the latency of inference.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩散的方法。最近，众多扩散模型，包括广泛认可的扩散去噪概率模型（DDPM）[[37](#bib.bib37)]，在低级视觉领域引起了极大的关注[[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)]。郭*等人*[[30](#bib.bib30)]率先将扩散模型融入了阴影去除领域。随后，一系列方法[[76](#bib.bib76),
    [64](#bib.bib64), [77](#bib.bib77)]倾向于将扩散模型作为骨干。虽然基于扩散的方法在生成逼真的无阴影结果方面表现出了其有效性，但它们通常伴随一个显著的缺点：推理过程耗时较长，这主要归因于涉及的迭代采样过程。在某些应用中，如实时处理，可能对推理延迟有严格的要求。
- en: IV-E Loss Function
  id: totrans-545
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 损失函数
- en: The commonly adopted loss functions in shadow removal methods are the full-reference
    loss, e.g., reconstruction loss (${\color[rgb]{0,0,0}{\ell}}_{1}$, ${\color[rgb]{0,0,0}{\ell}}_{2}$),
    and perceptual loss. Additionally, depending on specific settings and formulations,
    some non-reference loss functions like boundary loss, adversarial loss, and color
    loss are also utilized. We provide a detailed explanation of some representative
    loss functions as follows.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的阴影去除损失函数包括全参考损失，例如重建损失（${\color[rgb]{0,0,0}{\ell}}_{1}$，${\color[rgb]{0,0,0}{\ell}}_{2}$），以及感知损失。此外，根据具体设置和公式，还会使用一些非参考损失函数，如边界损失、对抗损失和颜色损失。我们将详细解释一些代表性的损失函数如下。
- en: Reconstruction Loss. Reconstruction loss refers to the measure of dissimilarity
    between the ground truth image and the restored image produced by the model, which
    is typically computed by comparing the pixel values of the restored image with
    the corresponding pixel values of the clean image. Different reconstruction losses
    have their advantages and disadvantages [[87](#bib.bib87), [88](#bib.bib88)].
    ${\color[rgb]{0,0,0}{\ell}}_{2}$ loss, also known as mean squared error (MSE)
    loss, penalizes larger errors more than smaller errors. While ${\color[rgb]{0,0,0}{\ell}}_{2}$
    loss is effective at capturing global trends and minimizing overall reconstruction
    errors, it can sometimes blur or smooth out fine details due to its emphasis on
    larger errors. On the other hand, ${\color[rgb]{0,0,0}{\ell}}_{1}$ loss, also
    known as mean absolute error (MAE) loss, treats all errors equally regardless
    of their magnitude. It is less sensitive to outliers and can preserve colors and
    illuminance well, as it does not disproportionately penalize local deviations
    in the image structure. Consequently, ${\color[rgb]{0,0,0}{\ell}}_{1}$ loss can
    result in sharper edges and better preservation of high-frequency details. Structural
    Similarity Index (SSIM) loss is designed to preserve the structure and texture
    of the image, leading to visually pleasing results.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 重建损失。重建损失是指真实图像与模型生成的恢复图像之间的不相似度，通常通过比较恢复图像的像素值与干净图像的相应像素值来计算。不同的重建损失各有优缺点[[87](#bib.bib87),
    [88](#bib.bib88)]。${\color[rgb]{0,0,0}{\ell}}_{2}$ 损失，也称为均方误差 (MSE) 损失，对较大的错误给予比小错误更多的惩罚。尽管
    ${\color[rgb]{0,0,0}{\ell}}_{2}$ 损失在捕捉全局趋势和最小化总体重建错误方面有效，但由于其对较大错误的强调，有时会模糊或平滑细节。另一方面，${\color[rgb]{0,0,0}{\ell}}_{1}$
    损失，也称为平均绝对误差 (MAE) 损失，对所有错误给予相同的处理，不论其大小。它对离群点不太敏感，能够很好地保留颜色和光照，因为它不会不成比例地惩罚图像结构中的局部偏差。因此，${\color[rgb]{0,0,0}{\ell}}_{1}$
    损失能够产生更锐利的边缘和更好的高频细节保留。结构相似性指数 (SSIM) 损失旨在保留图像的结构和纹理，产生视觉上令人愉悦的结果。
- en: 'Boundary Loss. The illumination surrounding the shadow boundary of shadow images
    often exhibits abrupt variations, easily leading to boundary artifacts in the
    restored results. In order to address this issue, a boundary-aware smoothness
    loss is intuitively employed to encourage smoother transitions along the boundaries
    in methods that require shadow masks. Typically, the boundary-aware smoothness
    loss focuses on minimizing the gradients in the horizontal and vertical directions
    within the boundary regions during the training process [[58](#bib.bib58), [64](#bib.bib64)],
    which is a type of non-reference loss and does not require ground truth shadow-free
    image as follows:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 边界损失。阴影图像的阴影边界周围的光照通常会出现突变，容易导致恢复结果中的边界伪影。为了应对这个问题，直观地使用了边界感知平滑损失，以鼓励在需要阴影掩模的方法中沿边界进行更平滑的过渡。通常，边界感知平滑损失专注于在训练过程中最小化边界区域内的水平和垂直方向的梯度[[58](#bib.bib58),
    [64](#bib.bib64)]，这是一种非参考损失，不需要真实的无阴影图像，具体如下：
- en: '|  | $\mathcal{L}_{\text{boundary }}=\left\&#124;\mathrm{~{}B}\left(\mathbf{M}\right)\circ\left&#124;\nabla\left(\hat{\mathbf{I}}_{sf}\right)\right&#124;\right\&#124;_{1},$
    |  | (9) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{boundary }}=\left\&#124;\mathrm{~{}B}\left(\mathbf{M}\right)\circ\left&#124;\nabla\left(\hat{\mathbf{I}}_{sf}\right)\right&#124;\right\&#124;_{1},$
    |  | (9) |'
- en: where $\nabla$ is the gradient operation, $\mathrm{B}$ is a noise-robust function [[89](#bib.bib89),
    [90](#bib.bib90)] to compute the boundaries of the shadow regions from the corresponding
    shadow mask $\mathbf{M}$.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nabla$ 是梯度操作，$\mathrm{B}$ 是一个抗噪声函数[[89](#bib.bib89), [90](#bib.bib90)]，用于从相应的阴影掩模
    $\mathbf{M}$ 中计算阴影区域的边界。
- en: Adversarial Loss. Adversarial learning is a technique that involves solving
    a maximization-minimization optimization problem [[86](#bib.bib86)] to foster
    enhanced results that are perceptually indistinguishable from reference images.
    It leverages the interplay between a generator and a discriminator to generate
    enhanced results that closely resemble the reference images, enabling the generation
    of high-quality and realistic outputs.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失。对抗学习是一种技术，它涉及解决一个最大化-最小化优化问题[[86](#bib.bib86)]，以促进生成的结果在感知上与参考图像无差别。它利用生成器和判别器之间的相互作用生成与参考图像非常相似的增强结果，从而生成高质量和逼真的输出。
- en: Perceptual Loss. Perceptual loss is a technique used to encourage the generated
    results to closely resemble the ground truth in the feature space. The loss enhances
    the visual quality of the outputs by considering high-level features. It is computed
    as the Euclidean distance between the feature representations of an enhanced result
    and the corresponding ground truth. Typically, these feature representations are
    extracted from pre-trained networks, such as VGG [[91](#bib.bib91)], which enables
    the model to leverage the knowledge learned from a large dataset like ImageNet [[92](#bib.bib92)],
    making the perceptual loss more effective in capturing meaningful features.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 感知损失。感知损失是一种鼓励生成结果在特征空间中与真实数据相似的技术。该损失通过考虑高级特征来提高输出的视觉质量。它是通过增强结果和相应真实数据的特征表示之间的欧几里得距离来计算的。通常，这些特征表示是从预训练网络中提取的，如
    VGG [[91](#bib.bib91)]，这使得模型能够利用从大型数据集（如 ImageNet [[92](#bib.bib92)]）中学到的知识，从而使感知损失在捕捉有意义的特征时更为有效。
- en: '|  | $\mathcal{L}_{\text{perceptual }}=\&#124;\phi(\mathbf{I}_{sf})-\phi(\hat{\mathbf{I}}_{sf})\&#124;_{2}^{2}\;.$
    |  | (10) |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{perceptual }}=\&#124;\phi(\mathbf{I}_{sf})-\phi(\hat{\mathbf{I}}_{sf})\&#124;_{2}^{2}\;.$
    |  | (10) |'
- en: 'TABLE II: Summary of shadow datasets. ‘Syn’ represents Synthetic.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 阴影数据集汇总。‘合成’代表 Synthetic。'
- en: '| Name | Number | Resolution | Format | Real/Syn | Mask | Paired | Type |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 数量 | 分辨率 | 格式 | 真实/合成 | 遮罩 | 配对 | 类型 |'
- en: '| Simulated by Physic Model [[17](#bib.bib17), [93](#bib.bib93), [94](#bib.bib94)]
    | +$\infty$ | $\infty$ | RGB | Syn |  |  |  |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 由物理模型模拟 [[17](#bib.bib17), [93](#bib.bib93), [94](#bib.bib94)] | +$\infty$
    | $\infty$ | RGB | 合成 |  |  |  |'
- en: '| Simulated by GAN [[51](#bib.bib51), [56](#bib.bib56), [48](#bib.bib48)] |
    +$\infty$ | $\infty$ | RGB | Syn |  |  |  |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| 由GAN模拟 [[51](#bib.bib51), [56](#bib.bib56), [48](#bib.bib48)] | +$\infty$
    | $\infty$ | RGB | 合成 |  |  |  |'
- en: '| UIUC [[10](#bib.bib10)] | 108 | $640\times 425$ | RGB | Real | ✓ | ✓ | soft,
    hard, self |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| UIUC [[10](#bib.bib10)] | 108 | $640\times 425$ | RGB | 真实 | ✓ | ✓ | 软, 硬,
    自身 |'
- en: '| LRSS [[17](#bib.bib17)] | 137 | $5184\times 3456$ | raw | Real |  | ✓ | soft
    |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| LRSS [[17](#bib.bib17)] | 137 | $5184\times 3456$ | 原始 | 真实 |  | ✓ | 软 |'
- en: '| SRD [[24](#bib.bib24)] | 3,088 | $800\times 640$ | RGB | Real |  | ✓ | soft,
    hard |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| SRD [[24](#bib.bib24)] | 3,088 | $800\times 640$ | RGB | 真实 |  | ✓ | 软, 硬
    |'
- en: '| ISTD & ISTD+ [[45](#bib.bib45), [51](#bib.bib51)] | 1,870 | $640\times 480$
    | RGB | Real | ✓ | ✓ | hard |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| ISTD & ISTD+ [[45](#bib.bib45), [51](#bib.bib51)] | 1,870 | $640\times 480$
    | RGB | 真实 | ✓ | ✓ | 硬 |'
- en: '| USR [[56](#bib.bib56)] | 2,445 | $600\times 450$ | RGB | Real |  |  | soft,
    hard |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| USR [[56](#bib.bib56)] | 2,445 | $600\times 450$ | RGB | 真实 |  |  | 软, 硬
    |'
- en: '| SBU [[95](#bib.bib95)] | 5,000 | Various | RGB | Real | ✓ |  | soft, self,
    hard |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| SBU [[95](#bib.bib95)] | 5,000 | 各种 | RGB | 真实 | ✓ |  | 软, 自身, 硬 |'
- en: 'Color Loss. Some methods [[96](#bib.bib96), [57](#bib.bib57)] encourage the
    colour in restored result $\hat{\mathbf{I}}_{sf}$ to be the same with the shadow-free
    reference $\mathbf{I}_{sf}$ according to the extra colour loss as follows:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色损失。一些方法 [[96](#bib.bib96), [57](#bib.bib57)] 鼓励恢复结果 $\hat{\mathbf{I}}_{sf}$
    的颜色与无阴影参考 $\mathbf{I}_{sf}$ 一致，根据以下额外颜色损失：
- en: '|  | $\displaystyle\mathcal{L}_{\text{color }}=\sum_{p}\left(1-\cos<(\mathbf{I}_{sf}^{LAB})_{p},(\hat{\mathbf{I}}_{sf}^{LAB})_{p}>\right),$
    |  | (11) |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{color }}=\sum_{p}\left(1-\cos<(\mathbf{I}_{sf}^{LAB})_{p},(\hat{\mathbf{I}}_{sf}^{LAB})_{p}>\right),$
    |  | (11) |'
- en: where $(\cdot)_{p}$ denotes the pixel with index $p$ and and $\cos<\cdot,\cdot>$
    represents the cosine angle between vectors. Each pixel in the restored image
    or input image is treated as a 3D vector representing the Lab color space. The
    cosine similarity between two color vectors equals $1$ when the vectors are perfectly
    aligned.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\cdot)_{p}$ 表示索引为 $p$ 的像素，$\cos<\cdot,\cdot>$ 表示向量间的余弦角度。恢复图像或输入图像中的每个像素被视为表示Lab颜色空间的3D向量。当向量完全对齐时，两个颜色向量之间的余弦相似度为
    $1$。
- en: IV-F Training and Testing Datasets
  id: totrans-567
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 训练和测试数据集
- en: UIUC [[10](#bib.bib10)] comprises 108 images, each featuring a shadow image
    and its corresponding ground truth shadow-free image. Of these, 72 images are
    designated for training, and the remaining 76 for testing. The dataset includes
    various shadow types, such as soft shadows and self-shadows. Among the 76 test
    image pairs, 46 pairs are created by removing the shadow source while keeping
    the light source unchanged. In the remaining 30 pairs, shadows are cast by objects
    within the scene, with the shadow-free image generated by blocking the light source,
    enveloping the entire scene in shadow. The corresponding shadow mask is generated
    by thresholding the ratio between the paired shadow and shadow-free images.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: UIUC [[10](#bib.bib10)] 包含108张图片，每张图片都有一个阴影图像及其对应的真实阴影去除图像。其中72张图片用于训练，其余76张用于测试。数据集包含各种阴影类型，如软阴影和自阴影。在这76对测试图像中，有46对是通过去除阴影源而保持光源不变创建的。在剩下的30对中，阴影由场景中的物体投射，阴影去除图像是通过阻挡光源来生成的，将整个场景笼罩在阴影中。相应的阴影掩码是通过对配对的阴影图像和阴影去除图像之间的比例进行阈值化生成的。
- en: 'LRSS [[17](#bib.bib17)] is a collection of real soft shadow test photographs.
    It includes 137 images, 37 of which have corresponding ground truth shadow-free
    versions (along with mattes) to benchmark shadow removal methods. These ground
    truth images were produced by setting up a camera on a tripod and taking two photos:
    one with the shadow and one without, by removing the object casting the shadow.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: LRSS [[17](#bib.bib17)] 是一个真实软阴影测试照片的集合。它包含137张图像，其中37张有对应的真实阴影去除版本（以及遮罩）用于基准测试阴影去除方法。这些真实图像是通过将相机架设在三脚架上拍摄两张照片：一张有阴影，一张没有阴影，通过去除投射阴影的物体来实现的。
- en: SRD [[24](#bib.bib24)] comprises 2,680 training and 408 testing pairs of shadow
    and shadow-free images. This dataset does not feature manually annotated masks.
    Like ISTD, SRD includes shadows cast by various objects, then removing the shadow
    source to capture the corresponding shadow-free image. Shadow images are captured
    under different illumination conditions, such as varied weather conditions or
    different times of the day, to incorporate both hard and soft shadows into the
    dataset.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: SRD [[24](#bib.bib24)] 包含2,680对训练和408对测试的阴影和阴影去除图像。这一数据集没有手动标注的掩码。与 ISTD 类似，SRD
    包括由各种物体投射的阴影，然后去除阴影源以捕捉相应的阴影去除图像。阴影图像在不同的光照条件下捕获，如不同的天气条件或一天中的不同时间，以将硬阴影和软阴影都纳入数据集中。
- en: ISTD and ISTD+ [[45](#bib.bib45), [51](#bib.bib51)] The ISTD dataset [[45](#bib.bib45)]
    comprises 1330 training and 540 testing triplets, consisting of shadow images,
    corresponding manually annotated masks, and shadow-free images. This dataset primarily
    comprises outdoor scenes and predominantly contains hard shadows. Shadows are
    cast by various objects not present in the scene, and to capture the corresponding
    shadow-free image, the shadow source is removed. The Adjusted ISTD (ISTD+) dataset [[51](#bib.bib51)]
    addresses illumination inconsistencies between the shadow and shadow-free images
    present in the original ISTD dataset.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: ISTD 和 ISTD+ [[45](#bib.bib45), [51](#bib.bib51)] ISTD 数据集 [[45](#bib.bib45)]
    包含1330对训练和540对测试三联图像，包括阴影图像、对应的手动标注掩码和阴影去除图像。该数据集主要包含户外场景，并主要包含硬阴影。阴影由场景中不存在的各种物体投射，为了捕捉相应的阴影去除图像，需要移除阴影源。调整后的
    ISTD (ISTD+) 数据集 [[51](#bib.bib51)] 解决了原始 ISTD 数据集中阴影和阴影去除图像之间的光照不一致问题。
- en: USR [[56](#bib.bib56)] is an unpaired shadow removal dataset consisting of 2,445
    shadow images and 1,770 shadow-free images. It features a diverse range of scenes,
    encompassing over a thousand different environments where shadows are cast by
    various objects such as trees, buildings, traffic signs, people, umbrellas, railings,
    and more. The dataset includes 1,956 images for training and 489 for testing,
    utilizing all 1,770 shadow-free images exclusively for training.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: USR [[56](#bib.bib56)] 是一个无配对阴影去除数据集，包含2,445张阴影图像和1,770张阴影去除图像。它涵盖了各种场景，包括超过一千个不同环境，阴影由各种物体如树木、建筑物、交通标志、人、伞、栏杆等投射。数据集包括1,956张用于训练的图像和489张用于测试的图像，所有1,770张阴影去除图像仅用于训练。
- en: SBU [[95](#bib.bib95)] comprises 5,000 images depicting scenes with shadows,
    spanning diverse environments such as urban areas, beaches, mountains, roads,
    parks, snowscapes, animals, vehicles, and houses. The collection includes various
    types of photographs, including aerial shots, landscapes, close-ups, and selfies.
    Approximately 85% of the images are allocated to the training set, with the remaining
    15% reserved for testing. Among the 700 test images, detailed annotations of shadow
    masks ensure pixel accuracy. During training image annotation, a quick method
    involving sparse strokes on shadow areas was employed for efficient labeling of
    a large image set.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: SBU [[95](#bib.bib95)] 包含 5000 张描绘有阴影的场景图像，涵盖了城市、海滩、山脉、道路、公园、雪景、动物、车辆和房屋等各种环境。该集合包括多种类型的照片，包括航拍图、风景照、特写和自拍照。大约
    85% 的图像分配给训练集，其余 15% 用于测试。在 700 张测试图像中，详细的阴影掩码注释确保了像素级的准确性。在训练图像注释过程中，采用了快速方法，通过在阴影区域上进行稀疏笔触以高效标注大量图像集。
- en: Video [[62](#bib.bib62)] shadow removal dataset comprises 8 videos featuring
    static scenes with unchanging backgrounds. Each video includes a corresponding
    $V_{max}$ frame, which serves as the pseudo shadow-free reference, created by
    capturing the maximum intensity values at each pixel location throughout the video.
    The mask for moving shadows includes pixels found in both shadowed and non-shadowed
    areas, defining the evaluation region. The moving shadow mask is generated by
    applying a threshold of 80.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: Video [[62](#bib.bib62)] 阴影去除数据集包含 8 个视频，这些视频展示了静态场景和不变的背景。每个视频包括一个相应的 $V_{max}$
    帧，这个帧作为伪阴影去除参考，通过捕捉每个像素位置的最大强度值创建。移动阴影的掩码包括阴影区域和非阴影区域的像素，定义了评估区域。移动阴影掩码是通过应用 80
    的阈值生成的。
- en: 'TABLE III: Quantitative comparisons of the shadow removal results on SRD dataset [[24](#bib.bib24)]
    in terms of RMSE, PSNR (in dB), and SSIM. Highlighted in red, yellow, and green
    cells are the best, second-best, and third-best results, respectively. ^§ indicates
    that the results are directly quoted from the original paper as their code implementation
    was not made available.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：SRD 数据集 [[24](#bib.bib24)](CVPR’17) 上阴影去除结果的定量比较，依据 RMSE、PSNR（以 dB 为单位）和
    SSIM。红色、黄色和绿色单元格分别突出显示最佳、第二最佳和第三最佳结果。^§ 表示结果直接引用自原始论文，因为其代码实现未公开。
- en: '| Learning | Method | Shadow Region (S) | Non-Shadow Region (NS) | All Image
    (ALL) |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| Learning | 方法 | 阴影区域 (S) | 非阴影区域 (NS) | 全图像 (ALL) |'
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | RMSE$\downarrow$
    | LPIPS$\downarrow$ |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | RMSE$\downarrow$
    | LPIPS$\downarrow$ |'
- en: '|  | input | 18.96 | 0.871 | 36.69 | 31.47 | 0.975 | 4.83 | 18.19 | 0.830 |
    14.05 | 0.1899 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | 18.96 | 0.871 | 36.69 | 31.47 | 0.975 | 4.83 | 18.19 | 0.830 | 14.05
    | 0.1899 |'
- en: '| TR | Guo *et al.* [[10](#bib.bib10)](TPAMI’12) | - | - | 29.89 | - | - |
    6.47 | - | - | 12.60 | - |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| TR | Guo *et al.* [[10](#bib.bib10)](TPAMI’12) | - | - | 29.89 | - | - |
    6.47 | - | - | 12.60 | - |'
- en: '| SL | DeshadowNet [[24](#bib.bib24)](CVPR’17) | - | - | 11.78 | - | - | 4.84
    | - | - | 6.64 | - |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| SL | DeshadowNet [[24](#bib.bib24)](CVPR’17) | - | - | 11.78 | - | - | 4.84
    | - | - | 6.64 | - |'
- en: '| DSC [[46](#bib.bib46)](TPAMI’19) | 30.65 | 0.960 | 8.62 | 31.94 | 0.965 |
    4.41 | 27.76 | 0.903 | 5.71 | 0.1141 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| DSC [[46](#bib.bib46)](TPAMI’19) | 30.65 | 0.960 | 8.62 | 31.94 | 0.965 |
    4.41 | 27.76 | 0.903 | 5.71 | 0.1141 |'
- en: '| DHAN [[48](#bib.bib48)](AAAI’21) | 33.67 | 0.978 | 8.94 | 34.79 | 0.979 |
    4.80 | 30.51 | 0.949 | 5.67 | 0.0666 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| DHAN [[48](#bib.bib48)](AAAI’21) | 33.67 | 0.978 | 8.94 | 34.79 | 0.979 |
    4.80 | 30.51 | 0.949 | 5.67 | 0.0666 |'
- en: '| AEF [[52](#bib.bib52)](CVPR’21) | 32.26 | 0.966 | 9.55 | 31.87 | 0.945 |
    5.74 | 28.40 | 0.893 | 6.50 | 0.0894 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| AEF [[52](#bib.bib52)](CVPR’21) | 32.26 | 0.966 | 9.55 | 31.87 | 0.945 |
    5.74 | 28.40 | 0.893 | 6.50 | 0.0894 |'
- en: '| EMDN [[29](#bib.bib29)](AAAI’21) | 34.94 | 0.980 | 7.44 | 35.85 | 0.982 |
    3.74 | 31.72 | 0.952 | 4.79 | 0.0980 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| EMDN [[29](#bib.bib29)](AAAI’21) | 34.94 | 0.980 | 7.44 | 35.85 | 0.982 |
    3.74 | 31.72 | 0.952 | 4.79 | 0.0980 |'
- en: '| BMNet [[54](#bib.bib54)](CVPR’22) | 35.05 | 0.981 | 6.61 | 36.02 | 0.982
    | 3.61 | 31.69 | 0.956 | 4.46 | 0.0549 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| BMNet [[54](#bib.bib54)](CVPR’22) | 35.05 | 0.981 | 6.61 | 36.02 | 0.982
    | 3.61 | 31.69 | 0.956 | 4.46 | 0.0549 |'
- en: '| ShadowFormer [[55](#bib.bib55)](AAAI’23) | 35.55 | 0.982 | 6.14 | 36.82 |
    0.983 | 3.54 | 32.46 | 0.957 | 4.28 | 0.0572 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| ShadowFormer [[55](#bib.bib55)](AAAI’23) | 35.55 | 0.982 | 6.14 | 36.82 |
    0.983 | 3.54 | 32.46 | 0.957 | 4.28 | 0.0572 |'
- en: '| ShadowDiffusion [[30](#bib.bib30)](CVPR’23) | 38.72 | 0.987 | 4.98 | 37.78
    | 0.985 | 3.44 | 34.73 | 0.970 | 3.63 | 0.0359 |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| ShadowDiffusion [[30](#bib.bib30)](CVPR’23) | 38.72 | 0.987 | 4.98 | 37.78
    | 0.985 | 3.44 | 34.73 | 0.970 | 3.63 | 0.0359 |'
- en: '| Inpaint4Shadow [[74](#bib.bib74)](ICCV’23) | 36.73 | 0.985 | 5.70 | 36.70
    | 0.985 | 3.27 | 33.27 | 0.967 | 3.81 | 0.0475 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| Inpaint4Shadow [[74](#bib.bib74)](ICCV’23) | 36.73 | 0.985 | 5.70 | 36.70
    | 0.985 | 3.27 | 33.27 | 0.967 | 3.81 | 0.0475 |'
- en: '| DeS3 [[76](#bib.bib76)](AAAI’24) | 37.91 | 0.986 | 5.27 | 37.45 | 0.984 |
    3.03 | 34.11 | 0.968 | 3.56 | 0.0338 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| DeS3 [[76](#bib.bib76)](AAAI’24) | 37.91 | 0.986 | 5.27 | 37.45 | 0.984 |
    3.03 | 34.11 | 0.968 | 3.56 | 0.0338 |'
- en: '| HomoFormer [[78](#bib.bib78)](CVPR’24) | 38.81 | 0.987 | 4.25 | 39.45 | 0.988
    | 2.85 | 35.37 | 0.972 | 3.33 | 0.0419 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| HomoFormer [[78](#bib.bib78)](CVPR’24) | 38.81 | 0.987 | 4.25 | 39.45 | 0.988
    | 2.85 | 35.37 | 0.972 | 3.33 | 0.0419 |'
- en: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 34.00 | 0.975 | 7.70 | 35.53
    | 0.981 | 3.65 | 31.53 | 0.955 | 4.65 | 0.1285 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 34.00 | 0.975 | 7.70 | 35.53
    | 0.981 | 3.65 | 31.53 | 0.955 | 4.65 | 0.1285 |'
- en: '| SSL | ARGAN^§ [[31](#bib.bib31)](ICCV’19) | - | - | 6.35 | - | - | 4.46 |
    - | - | 5.31 | - |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| SSL | ARGAN^§ [[31](#bib.bib31)](ICCV’19) | - | - | 6.35 | - | - | 4.46 |
    - | - | 5.31 | - |'
- en: 'TABLE IV: Quantitative comparisons of the shadow removal results on ISTD+ dataset [[45](#bib.bib45),
    [51](#bib.bib51)] in terms of RMSE, PSNR (in dB), SSIM, and LPIPS. ^† indicates
    that the methods use the ground truth shadow mask provided by ISTD+ dataset as
    input.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 在 ISTD+ 数据集 [[45](#bib.bib45), [51](#bib.bib51)] 上的阴影去除结果的定量比较，涉及 RMSE、PSNR（以
    dB 为单位）、SSIM 和 LPIPS。^† 表示方法使用了 ISTD+ 数据集提供的真实阴影掩模作为输入。'
- en: '| Learning | Method | Shadow Region (S) | Non-Shadow Region (NS) | All Image
    (ALL) |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | 方法 | 阴影区域 (S) | 非阴影区域 (NS) | 全图 (ALL) |'
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | RMSE$\downarrow$
    | LPIPS$\downarrow$ |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | RMSE$\downarrow$
    | LPIPS$\downarrow$ |'
- en: '|  | input | 20.83 | 0.93 | 39.01 | 37.46 | 0.985 | 2.40 | 20.46 | 0.894 |
    8.40 | 0.1431 |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | 20.83 | 0.93 | 39.01 | 37.46 | 0.985 | 2.40 | 20.46 | 0.894 | 8.40
    | 0.1431 |'
- en: '| TR | Guo *et al.* [[10](#bib.bib10)](TPAMI’12) | 26.89 | 0.960 | 21.38 |
    35.56 | 0.976 | 3.09 | 25.52 | 0.925 | 6.09 | 0.0966 |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| TR | Guo *et al.* [[10](#bib.bib10)](TPAMI’12) | 26.89 | 0.960 | 21.38 |
    35.56 | 0.976 | 3.09 | 25.52 | 0.925 | 6.09 | 0.0966 |'
- en: '| SL | SP+M-Net [[51](#bib.bib51)](ICCV’19) | 37.60 | 0.990 | 5.91 | 36.02
    | 0.976 | 2.99 | 32.94 | 0.962 | 3.46 | 0.0586 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| SL | SP+M-Net [[51](#bib.bib51)](ICCV’19) | 37.60 | 0.990 | 5.91 | 36.02
    | 0.976 | 2.99 | 32.94 | 0.962 | 3.46 | 0.0586 |'
- en: '| Param+M+D-Net [[62](#bib.bib62)] | 33.09 | 0.983 | 9.67 | 35.35 | 0.978 |
    2.82 | 30.15 | 0.951 | 3.94 | 0.0662 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| Param+M+D-Net [[62](#bib.bib62)] | 33.09 | 0.983 | 9.67 | 35.35 | 0.978 |
    2.82 | 30.15 | 0.951 | 3.94 | 0.0662 |'
- en: '| AEF [[52](#bib.bib52)](CVPR’21) | 36.04 | 0.978 | 6.55 | 31.16 | 0.892 |
    3.77 | 29.45 | 0.861 | 4.23 | 0.0615 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| AEF [[52](#bib.bib52)](CVPR’21) | 36.04 | 0.978 | 6.55 | 31.16 | 0.892 |
    3.77 | 29.45 | 0.861 | 4.23 | 0.0615 |'
- en: '| BMNet^† [[54](#bib.bib54)](CVPR’22) | 37.87 | 0.991 | 5.62 | 37.51 | 0.985
    | 2.45 | 33.98 | 0.972 | 2.97 | 0.0305 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| BMNet^† [[54](#bib.bib54)](CVPR’22) | 37.87 | 0.991 | 5.62 | 37.51 | 0.985
    | 2.45 | 33.98 | 0.972 | 2.97 | 0.0305 |'
- en: '| SG-ShadowNet [[68](#bib.bib68)](ECCV’22) | 36.80 | 0.990 | 5.93 | 35.57 |
    0.978 | 2.92 | 32.46 | 0.962 | 3.41 | 0.0430 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| SG-ShadowNet [[68](#bib.bib68)](ECCV’22) | 36.80 | 0.990 | 5.93 | 35.57 |
    0.978 | 2.92 | 32.46 | 0.962 | 3.41 | 0.0430 |'
- en: '| Inpaint4Shadow [[74](#bib.bib74)](CVPR’23) | 38.10 | 0.990 | 6.09 | 37.66
    | 0.981 | 2.82 | 34.16 | 0.967 | 3.35 | 0.0675 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| Inpaint4Shadow [[74](#bib.bib74)](CVPR’23) | 38.10 | 0.990 | 6.09 | 37.66
    | 0.981 | 2.82 | 34.16 | 0.967 | 3.35 | 0.0675 |'
- en: '| ShadowFormer^† [[55](#bib.bib55)](AAAI’23) | 39.48 | 0.992 | 5.23 | 38.82
    | 0.983 | 2.30 | 35.46 | 0.971 | 2.78 | 0.0260 |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| ShadowFormer^† [[55](#bib.bib55)](AAAI’23) | 39.48 | 0.992 | 5.23 | 38.82
    | 0.983 | 2.30 | 35.46 | 0.971 | 2.78 | 0.0260 |'
- en: '| ShadowDiffusion^† [[30](#bib.bib30)](CVPR’23) | 39.69 | 0.992 | 4.97 | 38.89
    | 0.987 | 2.28 | 35.67 | 0.975 | 2.72 | 0.0234 |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| ShadowDiffusion^† [[30](#bib.bib30)](CVPR’23) | 39.69 | 0.992 | 4.97 | 38.89
    | 0.987 | 2.28 | 35.67 | 0.975 | 2.72 | 0.0234 |'
- en: '| HomoFormer^† [[78](#bib.bib78)](CVPR’24) | 39.49 | 0.993 | 4.73 | 38.75 |
    0.984 | 2.23 | 35.35 | 0.975 | 2.64 | 0.0259 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| HomoFormer^† [[78](#bib.bib78)](CVPR’24) | 39.49 | 0.993 | 4.73 | 38.75 |
    0.984 | 2.23 | 35.35 | 0.975 | 2.64 | 0.0259 |'
- en: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 31.06 | 0.976 | 12.62 | 27.03
    | 0.961 | 6.82 | 25.03 | 0.926 | 7.77 | 0.1420 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 31.06 | 0.976 | 12.62 | 27.03
    | 0.961 | 6.82 | 25.03 | 0.926 | 7.77 | 0.1420 |'
- en: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)](CVPR’21) | 33.58 | 0.979 | 8.82 |
    35.52 | 0.976 | 2.89 | 30.52 | 0.944 | 3.86 | 0.0662 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)](CVPR’21) | 33.58 | 0.979 | 8.82 |
    35.52 | 0.976 | 2.89 | 30.52 | 0.944 | 3.86 | 0.0662 |'
- en: '| BCDiff [[64](#bib.bib64)](ICCV’23) | 35.71 | 0.986 | 7.61 | 36.39 | 0.981
    | 2.66 | 32.11 | 0.959 | 3.47 | 0.0518 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| BCDiff [[64](#bib.bib64)](ICCV’23) | 35.71 | 0.986 | 7.61 | 36.39 | 0.981
    | 2.66 | 32.11 | 0.959 | 3.47 | 0.0518 |'
- en: 'TABLE V: Quantitative comparisons on LRSS dataset [[17](#bib.bib17)] and UIUC [[10](#bib.bib10)]
    in terms of RMSE, PSNR (in dB), SSIM, and LPIPS.'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 在 LRSS 数据集 [[17](#bib.bib17)] 和 UIUC [[10](#bib.bib10)] 上的定量比较，涉及 RMSE、PSNR（以
    dB 为单位）、SSIM 和 LPIPS。'
- en: '| Learning | Method | LRSS | UIUC |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | 方法 | LRSS | UIUC |'
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ |'
- en: '|  | input | 16.43 | 0.886 | 15.98 | 20.85 | 0.803 | 13.97 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '|  | input | 16.43 | 0.886 | 15.98 | 20.85 | 0.803 | 13.97 |'
- en: '|  | SP+M-Net [[51](#bib.bib51)](ICCV’19) | 21.77 | 0.927 | 11.18 | 28.23 |
    0.866 | 7.13 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '|  | SP+M-Net [[51](#bib.bib51)](ICCV’19) | 21.77 | 0.927 | 11.18 | 28.23 |
    0.866 | 7.13 |'
- en: '|  | EMDN [[29](#bib.bib29)](AAAI’22) | 19.26 | 0.882 | 15.71 | 25.56 | 0.775
    | 13.07 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '|  | EMDN [[29](#bib.bib29)](AAAI’22) | 19.26 | 0.882 | 15.71 | 25.56 | 0.775
    | 13.07 |'
- en: '|  | BMNet [[54](#bib.bib54)](CVPR’22) | 14.19 | 0.048 | 50.79 | 16.86 | 0.679
    | 45.68 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '|  | BMNet [[54](#bib.bib54)](CVPR’22) | 14.19 | 0.048 | 50.79 | 16.86 | 0.679
    | 45.68 |'
- en: '| SL | Inpaint4Shadow [[74](#bib.bib74)](CVPR’23) | 19.59 | 0.805 | 13.79 |
    27.77 | 0.849 | 7.82 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| SL | Inpaint4Shadow [[74](#bib.bib74)](CVPR’23) | 19.59 | 0.805 | 13.79 |
    27.77 | 0.849 | 7.82 |'
- en: '|  | ShadowFormer [[55](#bib.bib55)](AAAI’23) | 27.01 | 0.957 | 6.37 | 28.96
    | 0.874 | 6.57 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '|  | ShadowFormer [[55](#bib.bib55)](AAAI’23) | 27.01 | 0.957 | 6.37 | 28.96
    | 0.874 | 6.57 |'
- en: '|  | ShadowDiffusion [[30](#bib.bib30)](CVPR’23) | 28.37 | 0.955 | 6.01 | 29.02
    | 0.880 | 7.05 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '|  | ShadowDiffusion [[30](#bib.bib30)](CVPR’23) | 28.37 | 0.955 | 6.01 | 29.02
    | 0.880 | 7.05 |'
- en: '|  | HomoFormer [[78](#bib.bib78)](CVPR’24) | 25.86 | 0.951 | 6.98 | 29.08
    | 0.870 | 6.81 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '|  | HomoFormer [[78](#bib.bib78)](CVPR’24) | 25.86 | 0.951 | 6.98 | 29.08
    | 0.870 | 6.81 |'
- en: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 20.89 | 0.902 | 12.55 | 24.85
    | 0.849 | 9.51 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 20.89 | 0.902 | 12.55 | 24.85
    | 0.849 | 9.51 |'
- en: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)](CVPR’21) | 20.90 | 0.901 | 9.99 |
    27.56 | 0.858 | 7.43 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)](CVPR’21) | 20.90 | 0.901 | 9.99 |
    27.56 | 0.858 | 7.43 |'
- en: '| BCDiff [[64](#bib.bib64)](ICCV’23) | 22.13 | 0.922 | 10.68 | 26.81 | 0.852
    | 7.96 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| BCDiff [[64](#bib.bib64)](ICCV’23) | 22.13 | 0.922 | 10.68 | 26.81 | 0.852
    | 7.96 |'
- en: IV-G Evaluation Metrics
  id: totrans-624
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-G 评估指标
- en: •
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: PSNR and RMSE. PSNR measures the ratio between the maximum signal power and
    the power of noise affecting image fidelity, while RMSE calculates the square
    root of the average squared difference between corresponding pixels of the reference
    and reconstructed images. Both metrics quantify the discrepancy between images,
    with lower values indicating higher distortion or error.
  id: totrans-626
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PSNR 和 RMSE。PSNR 测量最大信号功率与影响图像保真度的噪声功率之间的比率，而 RMSE 计算参考图像和重建图像之间对应像素的平方差的平均值的平方根。这两种指标量化图像之间的差异，较低的值表示较高的失真或误差。
- en: •
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MAE measures the average magnitude of errors between predicted shadow-free image
    and ground truth. It calculates the absolute difference between each predicted
    value and its corresponding ground truth value, then averages these absolute differences
    to provide a single score.
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MAE 衡量预测的无阴影图像与真实值之间误差的平均幅度。它计算每个预测值与其对应的真实值之间的绝对差异，然后对这些绝对差异进行平均，以提供一个单一的分数。
- en: •
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SSIM [[97](#bib.bib97)] is a metric evaluating similarity between images, considering
    perceptual differences, and treating image degradation as a perceived alteration
    in structural information. It measures luminance, contrast, and structure, providing
    a score between -1 and 1, with 1 indicating perfect structural similarity.
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SSIM [[97](#bib.bib97)] 是一种评估图像相似性的指标，考虑感知差异，并将图像退化视为结构信息的感知变化。它测量亮度、对比度和结构，提供一个介于
    -1 和 1 之间的分数，1 表示完美的结构相似性。
- en: •
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LPIPS [[98](#bib.bib98)] is a perceptual similarity metric that evaluates differences
    between images based on their perceived similarity by a deep neural network. It
    measures the distance between feature representations of images, capturing both
    low-level and high-level features. Lower LPIPS scores indicate greater perceptual
    similarity, making it effective for tasks where human judgment is crucial, like
    image generation and restoration.
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LPIPS [[98](#bib.bib98)] 是一种感知相似性指标，评估图像之间的差异，基于深度神经网络对其感知的相似性。它测量图像特征表示之间的距离，捕捉低级和高级特征。较低的
    LPIPS 分数表示更大的感知相似性，这使其在需要人工判断的任务中，如图像生成和修复中非常有效。
- en: •
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: NIQE [[99](#bib.bib99)] is a no-reference metric for estimating image quality.
    It analyzes local statistics like luminance and contrast, comparing them to those
    of natural images to compute a quality score. Higher NIQE scores indicate lower
    image quality. It’s useful for tasks like compression and denoising but may not
    perform as well on structured or synthetic images.
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NIQE [[99](#bib.bib99)] 是一种无参考图像质量评估指标。它分析局部统计信息，如亮度和对比度，并将其与自然图像的统计信息进行比较，以计算质量分数。更高的
    NIQE 分数表示较低的图像质量。它在压缩和去噪等任务中很有用，但在结构化或合成图像上可能表现不佳。
- en: V Benchmarking and Empirical Analysis
  id: totrans-635
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 基准测试与实证分析
- en: 'This section provides empirical analysis and highlights some key challenges
    in deep learning based shadow removal methods. We conduct extensive evaluations
    on several public benchmarks. We select a number of recent shadow removal algorithms
    from different categories to be evaluated:'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了实证分析，并强调了基于深度学习的阴影去除方法中的一些关键挑战。我们对几个公共基准进行了广泛的评估。我们选择了若干种不同类别的最新阴影去除算法进行评估：
- en: '1.'
  id: totrans-637
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Multi-context Embedding, DeShadowNet [[24](#bib.bib24)]
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多上下文嵌入，DeShadowNet [[24](#bib.bib24)]
- en: '2.'
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Attentive Recurrent GAN, ARGAN [[31](#bib.bib31)]
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力递归GAN，ARGAN [[31](#bib.bib31)]
- en: '3.'
  id: totrans-641
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Direction-aware Spatial Context, DSC [[46](#bib.bib46)]
  id: totrans-642
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方向感知空间上下文，DSC [[46](#bib.bib46)]
- en: '4.'
  id: totrans-643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Shadow Parameter Estimation and Matte Prediction Network, SP+M-Net [[51](#bib.bib51)]
  id: totrans-644
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阴影参数估计与遮罩预测网络，SP+M-Net [[51](#bib.bib51)]
- en: '5.'
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Dual Hierarchical Aggregation Network, DHAN [[48](#bib.bib48)]
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双层次聚合网络，DHAN [[48](#bib.bib48)]
- en: '6.'
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Generation to Removal, G2R-ShadowNet [[63](#bib.bib63)]
  id: totrans-648
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成到去除，G2R-ShadowNet [[63](#bib.bib63)]
- en: '7.'
  id: totrans-649
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Domain Classifier Guided Net, DC-ShadowNet [[58](#bib.bib58)]
  id: totrans-650
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 域分类器引导网，DC-ShadowNet [[58](#bib.bib58)]
- en: '8.'
  id: totrans-651
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: Auto-Exposure Fusion, AEF [[52](#bib.bib52)]
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动曝光融合，AEF [[52](#bib.bib52)]
- en: '9.'
  id: totrans-653
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: Efficient Model-Driven Network, EMDN [[29](#bib.bib29)]
  id: totrans-654
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效的模型驱动网络，EMDN [[29](#bib.bib29)]
- en: '10.'
  id: totrans-655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: Bijective Mapping Network, BMNet [[54](#bib.bib54)]
  id: totrans-656
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双射映射网络，BMNet [[54](#bib.bib54)]
- en: '11.'
  id: totrans-657
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: Context Transformer, ShadowFormer [[55](#bib.bib55)]
  id: totrans-658
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上下文变换器，ShadowFormer [[55](#bib.bib55)]
- en: '12.'
  id: totrans-659
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '12.'
- en: Unrolling-inspired Diffusion, ShadowDiffusion [[30](#bib.bib30)]
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灵感来源于Unrolling的Diffusion，ShadowDiffusion [[30](#bib.bib30)]
- en: '13.'
  id: totrans-661
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '13.'
- en: Boundary-aware Conditional Diffusion, BCDiff [[64](#bib.bib64)]
  id: totrans-662
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 边界感知条件扩散，BCDiff [[64](#bib.bib64)]
- en: '14.'
  id: totrans-663
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '14.'
- en: Diffusion-based Soft & Self Shadow Removal, DeS3 [[76](#bib.bib76)]
  id: totrans-664
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于Diffusion的软与自我阴影去除，DeS3 [[76](#bib.bib76)]
- en: '15.'
  id: totrans-665
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '15.'
- en: Homogenized Transformer, HomoFormer [[78](#bib.bib78)]
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同质化变换器，HomoFormer [[78](#bib.bib78)]
- en: including eleven supervised learning-based methods (DeShadowNet, DSC, SP+M-Net,
    DHAN, AEF, EMDN, BMNet, ShadowFormer, ShadowDiffusion, SeS3, HomoFormer), two
    zero-shot learning-based methods (G2R-ShadowNet, BCDiff), one unsupervised learning-based
    method (DC-ShadowNet), one semi-supervised learning-based method (ARGAN). To ensure
    fair comparisons, we utilize publicly available code for all the methods under
    consideration to generate their results. We leverage full-reference metrics including
    PSNR, SSIM, RMSE, and LPIPS for those testsets with ground truth shadow-free images,
    i.e., ISTD+ [[45](#bib.bib45), [51](#bib.bib51)], SRD [[24](#bib.bib24)], LRSS [[17](#bib.bib17)],
    and UIUC [[10](#bib.bib10)]. The results of shadow removal from all methods are
    either sourced from their original papers or replicated using their official implementations.
    We standardize the evaluation of shadow removal results to a resolution of $256\times
    256$ following most shadow removal methods [[52](#bib.bib52), [29](#bib.bib29),
    [54](#bib.bib54), [30](#bib.bib30)].
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 包括十一种基于监督学习的方法（DeShadowNet，DSC，SP+M-Net，DHAN，AEF，EMDN，BMNet，ShadowFormer，ShadowDiffusion，SeS3，HomoFormer），两种基于零样本学习的方法（G2R-ShadowNet，BCDiff），一种基于无监督学习的方法（DC-ShadowNet），一种基于半监督学习的方法（ARGAN）。为了确保公平比较，我们利用所有考虑方法的公开代码生成结果。我们使用包括PSNR，SSIM，RMSE和LPIPS在内的全参考指标来评估具有真实阴影去除图像的测试集，即ISTD+ [[45](#bib.bib45)，[51](#bib.bib51)]，SRD [[24](#bib.bib24)]，LRSS [[17](#bib.bib17)]，和UIUC [[10](#bib.bib10)]。所有方法的阴影去除结果均来自其原始论文或使用其官方实现进行复制。我们将阴影去除结果的评估标准化为分辨率$256\times
    256$，以符合大多数阴影去除方法 [[52](#bib.bib52)，[29](#bib.bib29)，[54](#bib.bib54)，[30](#bib.bib30)]。
- en: '![Refer to caption](img/36061119af2bd87986f0171df380187f.png)'
  id: totrans-668
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/36061119af2bd87986f0171df380187f.png)'
- en: (a) Input
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输入
- en: '![Refer to caption](img/1a9fb15d9808f2e280a35797cb21f031.png)'
  id: totrans-670
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1a9fb15d9808f2e280a35797cb21f031.png)'
- en: (b) Mask (Residual)
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 遮罩（残差）
- en: '![Refer to caption](img/e6c7b2dcf9c15fa0e9798c06d3fc545e.png)'
  id: totrans-672
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e6c7b2dcf9c15fa0e9798c06d3fc545e.png)'
- en: (c) Mask (DHAN [[48](#bib.bib48)])
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 遮罩（DHAN [[48](#bib.bib48)])
- en: '![Refer to caption](img/335fae97ca2ada978ab5eeb146777950.png)'
  id: totrans-674
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/335fae97ca2ada978ab5eeb146777950.png)'
- en: (d) DSC [[46](#bib.bib46)]
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: (d) DSC [[46](#bib.bib46)]
- en: '![Refer to caption](img/7cd9439435e1f01cf8ffcc65f2cd7d38.png)'
  id: totrans-676
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7cd9439435e1f01cf8ffcc65f2cd7d38.png)'
- en: (e) DHAN [[48](#bib.bib48)]
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: (e) DHAN [[48](#bib.bib48)]
- en: '![Refer to caption](img/0d50ae3a84eab2dbf89ac18da7a021cc.png)'
  id: totrans-678
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0d50ae3a84eab2dbf89ac18da7a021cc.png)'
- en: (f) AEF [[52](#bib.bib52)]
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: (f) AEF [[52](#bib.bib52)]
- en: '![Refer to caption](img/ed9d7429f286b397568fd3c8baeb44d7.png)'
  id: totrans-680
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ed9d7429f286b397568fd3c8baeb44d7.png)'
- en: (g) EMDN [[29](#bib.bib29)]
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: (g) EMDN [[29](#bib.bib29)]
- en: '![Refer to caption](img/0d9b065bb29dd27be32160147834c073.png)'
  id: totrans-682
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0d9b065bb29dd27be32160147834c073.png)'
- en: (h) BMNet [[54](#bib.bib54)]
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: (h) BMNet [[54](#bib.bib54)]
- en: '![Refer to caption](img/6cc8695e044d7d0a83af85af848e131f.png)'
  id: totrans-684
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6cc8695e044d7d0a83af85af848e131f.png)'
- en: (i) DC-ShadowNet [[58](#bib.bib58)]
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: (i) DC-ShadowNet [[58](#bib.bib58)]
- en: '![Refer to caption](img/3cb5a823c67c2d1636720c575c4327b9.png)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3cb5a823c67c2d1636720c575c4327b9.png)'
- en: (j) SG-ShadowNet [[68](#bib.bib68)]
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: (j) SG-ShadowNet [[68](#bib.bib68)]
- en: '![Refer to caption](img/d6601046b57ab63380bc2bd4faf4dd86.png)'
  id: totrans-688
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d6601046b57ab63380bc2bd4faf4dd86.png)'
- en: (k) Inpaint4Shadow [[74](#bib.bib74)]
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: (k) Inpaint4Shadow [[74](#bib.bib74)]
- en: '![Refer to caption](img/a00979bcd3c75bcdb31de6a3e75b400a.png)'
  id: totrans-690
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a00979bcd3c75bcdb31de6a3e75b400a.png)'
- en: (l) ShadowDiffusion [[30](#bib.bib30)]
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: (l) ShadowDiffusion [[30](#bib.bib30)]
- en: '![Refer to caption](img/1f899686566d05a9a1a15951b4002d6e.png)'
  id: totrans-692
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1f899686566d05a9a1a15951b4002d6e.png)'
- en: (m) DeS3 [[76](#bib.bib76)]
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: (m) DeS3 [[76](#bib.bib76)]
- en: '![Refer to caption](img/f367682b39686cf551da671986cd9c46.png)'
  id: totrans-694
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f367682b39686cf551da671986cd9c46.png)'
- en: (n) HomoFormer [[78](#bib.bib78)]
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: (n) HomoFormer [[78](#bib.bib78)]
- en: '![Refer to caption](img/278f852c85b7786ff16eab0e14be7fc0.png)'
  id: totrans-696
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/278f852c85b7786ff16eab0e14be7fc0.png)'
- en: (o) Reference
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: (o) 参考
- en: 'Figure 8: Visual comparison with different shadow removal methods sampled from
    test set of SRD [[24](#bib.bib24)].'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：与从 SRD 测试集采样的不同阴影去除方法的视觉比较 [[24](#bib.bib24)]。
- en: V-A Benchmarking Results
  id: totrans-699
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 基准测试结果
- en: 'Tables [III](#S4.T3 "TABLE III ‣ IV-F Training and Testing Datasets ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")&[IV](#S4.T4 "TABLE IV ‣ IV-F Training and Testing Datasets ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") show the quantitative results on the testsets over ISTD+, and SRD, respectively.
    When the distribution of testing data is very similar to that of training data,
    supervised methods achieve better performance. Typically, some recent transformer
    or diffusion based methods achieved state-of-the-art over ISTD+ and SRD datasets.
    To further demonstrate the advantages and disadvantages of different methods,
    Figures [8](#S5.F8 "Figure 8 ‣ V Benchmarking and Empirical Analysis ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")&[9](#S5.F9 "Figure
    9 ‣ V-A Benchmarking Results ‣ V Benchmarking and Empirical Analysis ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey") present the visual
    examples of the shadow removal results on SRD dataset. For hard shadow removal,
    the results often exhibit noticeable boundary artifacts due to the difficulty
    in distinguishing sharp shadow edges from structural edges in the original scene,
    as illustrated in the lower right corner of Figure [8](#S5.F8 "Figure 8 ‣ V Benchmarking
    and Empirical Analysis ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey"). It also highlights that in complex scenarios involving multiple shadow
    types, such as self-shadows and cast shadows, certain shadow regions are often
    overlooked by shadow detectors. This oversight results in significant residual
    shadows evident in the outputs of most methods. Inaccurate masks can readily yield
    incorrect results across all types of shadows. Existing methods have utilized
    a range of techniques, including joint mask refinement [[30](#bib.bib30)], regional
    classification [[76](#bib.bib76)], shadow matte prediction [[51](#bib.bib51)],
    to tackle the issue of mask inaccuracy, which enable better processing of ignored
    regions.'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [III](#S4.T3 "TABLE III ‣ IV-F Training and Testing Datasets ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")&[IV](#S4.T4 "TABLE IV ‣ IV-F Training and Testing Datasets ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") 显示了在 ISTD+ 和 SRD 测试集上的定量结果。当测试数据的分布与训练数据非常相似时，监督方法表现更好。通常，一些近期的基于变换器或扩散的方法在
    ISTD+ 和 SRD 数据集上达到了**最先进的**水平。为了进一步展示不同方法的优缺点，图形 [8](#S5.F8 "Figure 8 ‣ V Benchmarking
    and Empirical Analysis ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")&[9](#S5.F9 "Figure 9 ‣ V-A Benchmarking Results ‣ V Benchmarking and
    Empirical Analysis ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") 展示了在 SRD 数据集上阴影去除结果的视觉示例。对于困难的阴影去除，结果通常会显示出明显的边界伪影，因为在原始场景中难以区分锐利的阴影边缘和结构边缘，如图 [8](#S5.F8
    "Figure 8 ‣ V Benchmarking and Empirical Analysis ‣ Single-Image Shadow Removal
    Using Deep Learning: A Comprehensive Survey") 右下角所示。这也突出了在涉及多种阴影类型的复杂场景中，如自阴影和投射阴影，某些阴影区域常被阴影检测器忽视。这种忽视导致大多数方法的输出中存在显著的残余阴影。不准确的掩膜很容易导致所有类型阴影的错误结果。现有方法已经采用了多种技术，包括联合掩膜细化 [[30](#bib.bib30)]、区域分类 [[76](#bib.bib76)]、阴影底色预测 [[51](#bib.bib51)]，以解决掩膜不准确的问题，从而更好地处理被忽视的区域。'
- en: '![Refer to caption](img/b994192fd425fc75566b112671a2fd34.png)'
  id: totrans-701
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b994192fd425fc75566b112671a2fd34.png)'
- en: (a) Input
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输入
- en: '![Refer to caption](img/a2a87d05ce9a328c010a92384c950b4f.png)'
  id: totrans-703
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a2a87d05ce9a328c010a92384c950b4f.png)'
- en: (b) Mask (Residual)
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 掩码 (残差)
- en: '![Refer to caption](img/65cb4062256870d7cde50476c29aadae.png)'
  id: totrans-705
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/65cb4062256870d7cde50476c29aadae.png)'
- en: (c) Mask (DHAN [[48](#bib.bib48)])
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 掩码 (DHAN [[48](#bib.bib48)])
- en: '![Refer to caption](img/d20d4ac7256608947e25afccc9275978.png)'
  id: totrans-707
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d20d4ac7256608947e25afccc9275978.png)'
- en: (d) DSC [[46](#bib.bib46)]
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: (d) DSC [[46](#bib.bib46)]
- en: '![Refer to caption](img/5ed15cfdbb056c48ef38f8250ab74c90.png)'
  id: totrans-709
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ed15cfdbb056c48ef38f8250ab74c90.png)'
- en: (e) DHAN [[48](#bib.bib48)]
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: (e) DHAN [[48](#bib.bib48)]
- en: '![Refer to caption](img/402f09795c139d27f18e030e99d760b6.png)'
  id: totrans-711
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/402f09795c139d27f18e030e99d760b6.png)'
- en: (f) AEF [[52](#bib.bib52)]
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: (f) AEF [[52](#bib.bib52)]
- en: '![Refer to caption](img/94c14e387cd259507e99c0f8654c97bf.png)'
  id: totrans-713
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94c14e387cd259507e99c0f8654c97bf.png)'
- en: (g) EMDN [[29](#bib.bib29)]
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: (g) EMDN [[29](#bib.bib29)]
- en: '![Refer to caption](img/118f72ed1ffc838019186b42f3de95be.png)'
  id: totrans-715
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/118f72ed1ffc838019186b42f3de95be.png)'
- en: (h) BMNet [[54](#bib.bib54)]
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: (h) BMNet [[54](#bib.bib54)]
- en: '![Refer to caption](img/a50621aef090d75f6d6077f3263dfa8c.png)'
  id: totrans-717
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a50621aef090d75f6d6077f3263dfa8c.png)'
- en: (i) DC-ShadowNet [[58](#bib.bib58)]
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: (i) DC-ShadowNet [[58](#bib.bib58)]
- en: '![Refer to caption](img/ec4983532a016f01cd55d1071f40fb89.png)'
  id: totrans-719
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec4983532a016f01cd55d1071f40fb89.png)'
- en: (j) SG-ShadowNet [[68](#bib.bib68)]
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: (j) SG-ShadowNet [[68](#bib.bib68)]
- en: '![Refer to caption](img/17d94b44740597088d6d2b756ab8ee87.png)'
  id: totrans-721
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17d94b44740597088d6d2b756ab8ee87.png)'
- en: (k) Inpaint4Shadow [[74](#bib.bib74)]
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: (k) Inpaint4Shadow [[74](#bib.bib74)]
- en: '![Refer to caption](img/bdaea7cf04154e1225941b39948656e6.png)'
  id: totrans-723
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bdaea7cf04154e1225941b39948656e6.png)'
- en: (l) ShadowDiffusion [[30](#bib.bib30)]
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: (l) ShadowDiffusion [[30](#bib.bib30)]
- en: '![Refer to caption](img/1af1ef7cc4bca9c3d301090ec84690a3.png)'
  id: totrans-725
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1af1ef7cc4bca9c3d301090ec84690a3.png)'
- en: (m) DeS3 [[76](#bib.bib76)]
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: (m) DeS3 [[76](#bib.bib76)]
- en: '![Refer to caption](img/c07e13b646f97149e52ecc9d88bfe675.png)'
  id: totrans-727
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c07e13b646f97149e52ecc9d88bfe675.png)'
- en: (n) HomoFormer [[78](#bib.bib78)]
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: (n) HomoFormer [[78](#bib.bib78)]
- en: '![Refer to caption](img/f05b845c94f1563d2f222fc9fa9525b1.png)'
  id: totrans-729
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f05b845c94f1563d2f222fc9fa9525b1.png)'
- en: (o) Reference
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: (o) 参考文献
- en: 'Figure 9: Visual comparison with different shadow removal methods sampled from
    test set of SRD [[24](#bib.bib24)].'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：与 SRD [[24](#bib.bib24)] 测试集中的不同阴影去除方法的视觉比较。
- en: V-B Generalization Capability
  id: totrans-732
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 泛化能力
- en: 'To further assess the generalization capabilities of current methods, we utilize
    the LRSS and UIUC datasets as our evaluation sets. These datasets are specifically
    chosen because they lack large-scale paired training data, enabling us to rigorously
    test how well the methods perform in scenarios where there is a distinct distribution
    difference between training and testing data. For the LRSS dataset, we binarize
    the residuals of shadow-free images and shadow images to generate mask inputs
    for methods that require a shadow mask. For the UIUC dataset, we utilize the ground
    truth masks provided by the dataset. To ensure a fair comparison, we consistently
    use the ISTD+ training set to train all methods. The performances of most shadow
    removal methods significantly deteriorate when transferring to testing data that
    differs markedly from the training data. For instance, models trained on pairs
    from ISTD+ encounter difficulties in effectively handling shadow images from the
    soft shadow dataset LRSS, as illustrated in Table [V](#S4.T5 "TABLE V ‣ IV-F Training
    and Testing Datasets ‣ IV Technical Review and Discussion ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey"). This is because the ISTD+
    dataset predominantly consists of hard shadow types and lacks soft shadow and
    self-shadow samples. On the contrary, those unsupervised or zero-shot learning
    methods may have better generalization ability to the unseen scenarios. Some methods,
    like DC-ShadowNet, considered the classification of shadow types can have a better
    robustness cross various shadow types. ShadowDiffusion employed the mask refinement
    as the auxiliary branch to achieve a better robustness to soft shadow processing.'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步评估当前方法的泛化能力，我们使用了LRSS和UIUC数据集作为评估集。这些数据集被特别选择是因为它们缺乏大规模的配对训练数据，使我们能够严格测试这些方法在训练数据和测试数据之间存在明显分布差异的场景中的表现。对于LRSS数据集，我们将无阴影图像和阴影图像的残差二值化，以生成需要阴影掩码的方法的掩码输入。对于UIUC数据集，我们利用数据集提供的真实掩码。为了确保公平比较，我们始终使用ISTD+训练集来训练所有方法。大多数阴影去除方法在转移到与训练数据明显不同的测试数据时表现显著下降。例如，表格[V](#S4.T5
    "TABLE V ‣ IV-F Training and Testing Datasets ‣ IV Technical Review and Discussion
    ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey")中所示，基于ISTD+数据集训练的模型在有效处理来自软阴影数据集LRSS的阴影图像时遇到困难。这是因为ISTD+数据集主要包含硬阴影类型，缺乏软阴影和自阴影样本。相反，那些无监督或零样本学习方法可能对未见过的场景具有更好的泛化能力。一些方法，如DC-ShadowNet，考虑到阴影类型的分类，可能在各种阴影类型中具有更好的鲁棒性。ShadowDiffusion则利用掩码细化作为辅助分支，以实现对软阴影处理的更好鲁棒性。'
- en: V-C Computational Complexity
  id: totrans-734
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 计算复杂度
- en: 'In order to provide a comprehensive assessment of deep learning-based shadow
    removal techniques, we conduct a comparative analysis of the computational complexity
    of various state-of-the-art methods, as detailed in Table [VI](#S5.T6 "TABLE VI
    ‣ V-C Computational Complexity ‣ V Benchmarking and Empirical Analysis ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey"), including runtime,
    training parameters, and FLOPs over 100 images of size $256\times 256$ using one
    NVIDIA RTX A5000 GPU. Inpaint4Shadow [[74](#bib.bib74)] and DC-ShadowNet [[58](#bib.bib58)]
    have the shortest runtimes. BMNet [[54](#bib.bib54)] employs a highly efficient
    architecture based on lightweight invertible blocks, resulting in significantly
    fewer model parameters and lower FLOPs. Some methods use multiple sub-networks
    to perform multi-stage processing, which inevitably requires multiples of the
    parameters and computational load, such as SP+M-Net [[51](#bib.bib51)]. Specially,
    diffusion-based methods, such as ShadowDiffusion [[30](#bib.bib30)] and BCDiff [[64](#bib.bib64)],
    require iterative denoising steps. Although multi-stage methods can achieve great
    performance, they require significantly increased computational complexity and
    processing time, making them impractical for many applications.'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提供对基于深度学习的阴影去除技术的全面评估，我们对各种最先进的方法的计算复杂度进行了比较分析，如表[VI](#S5.T6 "TABLE VI ‣
    V-C Computational Complexity ‣ V Benchmarking and Empirical Analysis ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")所示，包括运行时间、训练参数和每100张$256\times
    256$尺寸图像的FLOPs，使用一台NVIDIA RTX A5000 GPU。Inpaint4Shadow [[74](#bib.bib74)] 和 DC-ShadowNet
    [[58](#bib.bib58)] 的运行时间最短。BMNet [[54](#bib.bib54)] 采用了基于轻量级可逆块的高效架构，从而显著减少了模型参数和FLOPs。一些方法使用多个子网络进行多阶段处理，这不可避免地需要更多的参数和计算负担，例如
    SP+M-Net [[51](#bib.bib51)]。特别是，基于扩散的方法，如 ShadowDiffusion [[30](#bib.bib30)] 和
    BCDiff [[64](#bib.bib64)]，需要迭代去噪步骤。尽管多阶段方法可以实现优异的性能，但它们显著增加了计算复杂度和处理时间，使得它们在许多应用中不够实用。'
- en: 'TABLE VI: Quantitative comparisons of the algorithms’ computational efficiency
    in terms of runtime (in second), number of trainable parameters (#Parameters)
    (in M), and FLOPs (in G).'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：算法在运行时间（以秒为单位）、可训练参数数量（以M为单位）和FLOPs（以G为单位）方面的计算效率定量比较。
- en: '| Learning | Method | RunTime$\downarrow$ | #Parameters $\downarrow$ | FLOPs$\downarrow$
    | Platform |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | 方法 | 运行时间$\downarrow$ | #参数 $\downarrow$ | FLOPs$\downarrow$ | 平台 |'
- en: '|  | SP+M-Net [[51](#bib.bib51)] | 0.023 | 141.18 | 61.52 | PyTorch |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '|  | SP+M-Net [[51](#bib.bib51)] | 0.023 | 141.18 | 61.52 | PyTorch |'
- en: '|  | DHAN [[48](#bib.bib48)] | 0.283 | 21.75 | 262.87 | TensorFlow |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '|  | DHAN [[48](#bib.bib48)] | 0.283 | 21.75 | 262.87 | TensorFlow |'
- en: '|  | EMDN [[29](#bib.bib29)] | 0.015 | 10.06 | 56.29 | PyTorch |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '|  | EMDN [[29](#bib.bib29)] | 0.015 | 10.06 | 56.29 | PyTorch |'
- en: '|  | BMNet [[54](#bib.bib54)] | 0.036 | 0.37 | 10.99 | PyTorch |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '|  | BMNet [[54](#bib.bib54)] | 0.036 | 0.37 | 10.99 | PyTorch |'
- en: '| SL | ShadowFormer [[55](#bib.bib55)] | 0.031 | 11.35 | 64.60 | PyTorch |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '| SL | ShadowFormer [[55](#bib.bib55)] | 0.031 | 11.35 | 64.60 | PyTorch |'
- en: '|  | Inpaint4Shadow [[74](#bib.bib74)] | 0.006 | 14.98 | 81.18 | PyTorch |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '|  | Inpaint4Shadow [[74](#bib.bib74)] | 0.006 | 14.98 | 81.18 | PyTorch |'
- en: '|  | ShadowDiffusion [[30](#bib.bib30)] | 0.723 | 60.74 | 937.15 | PyTorch
    |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '|  | ShadowDiffusion [[30](#bib.bib30)] | 0.723 | 60.74 | 937.15 | PyTorch
    |'
- en: '|  | HomoFormer [[78](#bib.bib78)] | 0.042 | 17.81 | 35.63 | PyTorch |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '|  | HomoFormer [[78](#bib.bib78)] | 0.042 | 17.81 | 35.63 | PyTorch |'
- en: '| UL | DC-ShadowNet [[58](#bib.bib58)] | 0.008 | 10.59 | 52.55 | PyTorch |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '| UL | DC-ShadowNet [[58](#bib.bib58)] | 0.008 | 10.59 | 52.55 | PyTorch |'
- en: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)] | 0.010 | 22.76 | 113.93 | PyTorch
    |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)] | 0.010 | 22.76 | 113.93 | PyTorch
    |'
- en: '| BCDiff [[64](#bib.bib64)] | 31.446 | 276.41 | 139218.75 | PyTorch |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| BCDiff [[64](#bib.bib64)] | 31.446 | 276.41 | 139218.75 | PyTorch |'
- en: VI Shadow Removal Applications
  id: totrans-749
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 阴影去除应用
- en: This section provides an overview of applications related to shadow removal
    and discusses its necessity, including shadow generation and shadow-related attacks.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了与阴影去除相关的应用，并讨论了其必要性，包括阴影生成和与阴影相关的攻击。
- en: VI-A Shadow Generation
  id: totrans-751
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 阴影生成
- en: 'Shadow generation and synthesis typically involve the process of creating or
    simulating shadows in digital images or computer graphics. Similarly to shadow
    removal, shadow generation serves as an important application, which can primarily
    be divided into the following two aspects according to their subsequent applications:
    (1) Incorporating a foreground object into a background image to create a composite
    image relies heavily on the presence of shadows. Shadows are pivotal in rendering
    scenes realistically as they convey depth, perspective, and illumination. (2)
    Additionally, shadow generation can aid in simulating large-scale paired images
    that include both shadows and shadow-free counterparts. This process is achieved
    at a minimal expense, contributing significantly to data augmentation.'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影生成和合成通常涉及在数字图像或计算机图形中创建或模拟阴影的过程。类似于阴影去除，阴影生成也是一个重要的应用，根据其后续应用，主要可以分为以下两个方面：（1）将前景对象合成到背景图像中以创建合成图像，这一过程在很大程度上依赖于阴影的存在。阴影在真实地呈现场景时起着至关重要的作用，因为它们传达了深度、透视和光照。（2）此外，阴影生成还可以帮助模拟包含阴影和无阴影对应物的大规模配对图像。这个过程以最低的成本实现，显著有助于数据增强。
- en: '![Refer to caption](img/3f7516bbc4a939f578e53ea1c76cd806.png)'
  id: totrans-753
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3f7516bbc4a939f578e53ea1c76cd806.png)'
- en: (a) Input
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输入
- en: '![Refer to caption](img/425ff3a45c7ea92ffe2fda4fc085d3e5.png)'
  id: totrans-755
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/425ff3a45c7ea92ffe2fda4fc085d3e5.png)'
- en: (b) Mask (Residual)
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 掩膜（残差）
- en: '![Refer to caption](img/5234ad42fcdd98827c094cf1836261a2.png)'
  id: totrans-757
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5234ad42fcdd98827c094cf1836261a2.png)'
- en: (c) SP+M-Net [[51](#bib.bib51)]
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: (c) SP+M-Net [[51](#bib.bib51)]
- en: '![Refer to caption](img/6144d8908091f102d432d1a803cfbb8e.png)'
  id: totrans-759
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6144d8908091f102d432d1a803cfbb8e.png)'
- en: (d) EMDN [[29](#bib.bib29)]
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: (d) EMDN [[29](#bib.bib29)]
- en: '![Refer to caption](img/9acc7b3689f16a9514f6ec609cb532b5.png)'
  id: totrans-761
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9acc7b3689f16a9514f6ec609cb532b5.png)'
- en: (e) G2R-ShadowNet [[63](#bib.bib63)]
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: (e) G2R-ShadowNet [[63](#bib.bib63)]
- en: '![Refer to caption](img/259eec93654474500547f34b310b76f4.png)'
  id: totrans-763
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/259eec93654474500547f34b310b76f4.png)'
- en: (f) Inpaint4Shadow [[74](#bib.bib74)]
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Inpaint4Shadow [[74](#bib.bib74)]
- en: '![Refer to caption](img/4997729c4fa6337457df728976f31676.png)'
  id: totrans-765
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4997729c4fa6337457df728976f31676.png)'
- en: (g) ShadowFormer [[55](#bib.bib55)]
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: (g) ShadowFormer [[55](#bib.bib55)]
- en: '![Refer to caption](img/fdd9464b462094761d3e5551e8cc8a3c.png)'
  id: totrans-767
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fdd9464b462094761d3e5551e8cc8a3c.png)'
- en: (h) ShadowDiffusion [[30](#bib.bib30)]
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: (h) ShadowDiffusion [[30](#bib.bib30)]
- en: '![Refer to caption](img/29daff33996d1b1b6b6e9e1ff677890f.png)'
  id: totrans-769
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/29daff33996d1b1b6b6e9e1ff677890f.png)'
- en: (i) HomoFormer [[78](#bib.bib78)]
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: (i) HomoFormer [[78](#bib.bib78)]
- en: '![Refer to caption](img/ac1faea8eb45e1153f6379aa5eab5beb.png)'
  id: totrans-771
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac1faea8eb45e1153f6379aa5eab5beb.png)'
- en: (j) GT
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: (j) GT
- en: 'Figure 10: Visual comparison with different shadow removal methods sampled
    from test set of LRSS [[17](#bib.bib17)].'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 使用不同阴影去除方法的视觉比较，取自LRSS测试集 [[17](#bib.bib17)]。'
- en: '![Refer to caption](img/c878251d70059c596d2d71c2f39cd206.png)'
  id: totrans-774
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c878251d70059c596d2d71c2f39cd206.png)'
- en: (a) Input
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输入
- en: '![Refer to caption](img/5ddf34b6e63a38b1093f14d9d1fa5f3a.png)'
  id: totrans-776
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ddf34b6e63a38b1093f14d9d1fa5f3a.png)'
- en: (b) Mask (Residual)
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 掩膜（残差）
- en: '![Refer to caption](img/4f95b2bf03a88638cce663ed6fcb6626.png)'
  id: totrans-778
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4f95b2bf03a88638cce663ed6fcb6626.png)'
- en: (c) SP+M-Net [[51](#bib.bib51)]
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: (c) SP+M-Net [[51](#bib.bib51)]
- en: '![Refer to caption](img/0041e9bbbbdec59d9604f0daabd9f382.png)'
  id: totrans-780
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0041e9bbbbdec59d9604f0daabd9f382.png)'
- en: (d) EMDN [[29](#bib.bib29)]
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: (d) EMDN [[29](#bib.bib29)]
- en: '![Refer to caption](img/8008f2550ed31bacc032421c5bd1741b.png)'
  id: totrans-782
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8008f2550ed31bacc032421c5bd1741b.png)'
- en: (e) G2R-ShadowNet [[63](#bib.bib63)]
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: (e) G2R-ShadowNet [[63](#bib.bib63)]
- en: '![Refer to caption](img/f51f1149f7b8defd20f6ea25bdb0ac12.png)'
  id: totrans-784
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f51f1149f7b8defd20f6ea25bdb0ac12.png)'
- en: (f) Inpaint4Shadow [[74](#bib.bib74)]
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Inpaint4Shadow [[74](#bib.bib74)]
- en: '![Refer to caption](img/dc211fb205c50ac3ff753b5ca0616c68.png)'
  id: totrans-786
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dc211fb205c50ac3ff753b5ca0616c68.png)'
- en: (g) ShadowFormer [[55](#bib.bib55)]
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: (g) ShadowFormer [[55](#bib.bib55)]
- en: '![Refer to caption](img/cdf7e58149dc8ffade68c2cbdd4fa6cd.png)'
  id: totrans-788
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cdf7e58149dc8ffade68c2cbdd4fa6cd.png)'
- en: (h) ShadowDiffusion [[30](#bib.bib30)]
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: (h) ShadowDiffusion [[30](#bib.bib30)]
- en: '![Refer to caption](img/c721281b51e61b80f1912556b5a18827.png)'
  id: totrans-790
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c721281b51e61b80f1912556b5a18827.png)'
- en: (i) HomoFormer [[78](#bib.bib78)]
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: (i) HomoFormer [[78](#bib.bib78)]
- en: '![Refer to caption](img/e4400568717577552733dde8036bd94f.png)'
  id: totrans-792
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4400568717577552733dde8036bd94f.png)'
- en: (j) GT
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: (j) GT
- en: 'Figure 11: Visual comparison of the results using different shadow removal
    methods, sampled from test set of UIUC [[10](#bib.bib10)].'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 使用不同阴影去除方法的结果视觉比较，取自UIUC测试集 [[10](#bib.bib10)]。'
- en: Image composition. Image composition involves inserting a foreground object
    into a background image to create a composite image. However, the quality of composite
    images can be greatly affected by inconsistencies between the foreground and background,
    including appearance, geometric, and semantic disparities. Shadow discrepancies
    represent a common issue in image composition, contributing to appearance inconsistencies.
    Therefore, shadow generation becomes essential to produce plausible shadows for
    foreground objects, enhancing the realism of the composite image. Hong *et al.* [[100](#bib.bib100)]
    introduced a manually annotated dataset alongside a shadow synthesis technique
    capable of predicting shadow masks. This method leverages illumination information
    from the background and employs an illumination model to estimate shadow parameters.
    However, manually removing shadows from scenes entails considerable cost. Subsequently,
    Tao *et al.* [[101](#bib.bib101)] proposed utilizing rendering techniques for
    3D scenes to generate large-scale paired shadow/shadow-free images. They developed
    a two-stage network incorporating decomposed mask prediction and attentive shadow
    filling.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 图像合成。图像合成涉及将前景物体插入背景图像中以创建合成图像。然而，合成图像的质量可能会受到前景和背景之间的不一致性，包括外观、几何形状和语义差异的影响。阴影不一致是图像合成中的常见问题，导致外观上的不一致。因此，生成阴影成为关键，以为前景物体生成逼真的阴影，增强合成图像的真实感。Hong
    *等* [[100](#bib.bib100)] 引入了一个手动注释的数据集以及一种能够预测阴影掩码的阴影合成技术。这种方法利用背景的照明信息，并采用照明模型来估计阴影参数。然而，从场景中手动去除阴影涉及相当大的成本。随后，Tao
    *等* [[101](#bib.bib101)] 提出了利用渲染技术为3D场景生成大规模配对的阴影/无阴影图像。他们开发了一个包含分解掩码预测和注意力阴影填充的两阶段网络。
- en: Dataset augmentation. Due to the difficulty in obtaining extensive datasets
    for shadow removal, numerous methods in the field have opted to use the generation
    or synthesis process to supplement their training data. Gryka *et al.* [[17](#bib.bib17)]
    utilized graphics tools to produce shadow images, while Sidorov *et al.* [[93](#bib.bib93)]
    constructed a shadow dataset by manipulating lighting within a video game. Nevertheless,
    computer-generated scenes and shadows often exhibit notable disparities from natural
    scenes [[51](#bib.bib51)]. To bridge this disparity, several methodologies [[51](#bib.bib51),
    [56](#bib.bib56), [48](#bib.bib48)] have suggested employing Generative Adversarial
    Networks (GANs) to produce adversarial shadow images, aiming to enhance the realism
    and authenticity of synthesized shadows. This approach leverages the adversarial
    training process to refine the quality and fidelity of synthesized shadows, ultimately
    enhancing their realism and alignment with natural scenes. Another category of
    methods (referenced in [[94](#bib.bib94), [64](#bib.bib64)]) has utilized physical
    illumination models to simulate shadows with varying intensities, operating under
    the practical assumption that all occluding objects are situated outside the camera’s
    field of view. This non-learning approach can be seamlessly integrated into existing
    training paradigms on-the-fly, offering a computationally inexpensive solution.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集增强。由于获得大量阴影去除数据集的困难，该领域的许多方法选择使用生成或合成过程来补充其训练数据。Gryka *等* [[17](#bib.bib17)]
    利用图形工具生成阴影图像，而 Sidorov *等* [[93](#bib.bib93)] 通过在视频游戏中操控光照来构建一个阴影数据集。然而，计算机生成的场景和阴影通常与自然场景存在明显差异
    [[51](#bib.bib51)]。为了弥合这种差距，一些方法 [[51](#bib.bib51), [56](#bib.bib56), [48](#bib.bib48)]
    提出了使用生成对抗网络（GANs）生成对抗性阴影图像，以提高合成阴影的真实感和真实性。这种方法利用对抗训练过程来改进合成阴影的质量和保真度，最终增强其真实感和与自然场景的一致性。另一类方法（参考
    [[94](#bib.bib94), [64](#bib.bib64)]) 利用了物理照明模型来模拟不同强度的阴影，假设所有遮挡物体都位于相机的视野之外。这种非学习的方法可以无缝地集成到现有的训练模式中，提供一种计算上廉价的解决方案。
- en: VI-B Shadow-related Attacks
  id: totrans-797
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 与阴影相关的攻击
- en: With deep learning-based methods becoming the predominant solution for shadow
    removal, concerns regarding model-related robustness, such as adversarial robustness,
    have increasingly emerged within the community.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于深度学习的方法成为阴影去除的主要解决方案，与模型相关的鲁棒性问题，如对抗鲁棒性，越来越受到社区的关注。
- en: More recently, adversarial robustness in the physical domain has gained popularity
    due to its practical applications. However, existing adversarial examples are
    generated in a “sticker-pasting” strategy, which often falls short in producing
    imperceptible perturbation. In contrast, shadow, as a ubiquitous natural phenomenon,
    has the potential to serve as a non-invasive adversarial perturbation. Zhong *et
    al.* [[102](#bib.bib102)] propose a novel optical adversarial attack that generates
    naturalistic shadow patterns on real-world images under a black box setting. As
    a consequence, they successfully demonstrated that shadow could achieve stealthy
    attacks that significantly misled the prediction of deep networks in both digital-
    and physical- settings. Another line of research focuses on evaluating the robustness
    of existing deep shadow removal networks. Due to the significant inconsistency
    of illumination between shadow and non-shadow regions, traditional adversarial
    attacks such as PGD [[103](#bib.bib103)] fail to generate visually imperceptible
    noise, especially in the shadow region. Motivated by this, Wang *et al.* [[104](#bib.bib104)]
    propose a shadow-adaptive attack where the attack budget is adaptively aligned
    with each pixel’s intensity. Thus, the generated adversarial perturbation could
    be more aggressive in non-shadow regions while remaining stealthy in the shadow
    region. With the merit of such shadow-adaptive attacks, they further establish
    the first benchmark evaluating the adversarial robustness of existing deep shadow
    removal networks.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，由于其实际应用，物理领域中的对抗性鲁棒性越来越受到关注。然而，现有的对抗性示例是通过“贴纸粘贴”策略生成的，这种方法往往难以产生不可察觉的扰动。相反，阴影作为一种普遍存在的自然现象，具有作为非侵入性对抗性扰动的潜力。Zhong
    *等* [[102](#bib.bib102)] 提出了一个新颖的光学对抗攻击方法，该方法在真实世界图像上生成自然的阴影模式，适用于黑箱设置。因此，他们成功演示了阴影可以实现隐蔽攻击，显著误导深度网络在数字和物理环境下的预测。另一项研究重点是评估现有深度阴影去除网络的鲁棒性。由于阴影和非阴影区域之间照明的不一致性，传统的对抗性攻击，如PGD
    [[103](#bib.bib103)]，难以在阴影区域生成视觉上不可察觉的噪声。受到此启发，Wang *等* [[104](#bib.bib104)] 提出了一个阴影自适应攻击，其中攻击预算根据每个像素的强度自适应调整。因此，生成的对抗性扰动在非阴影区域可能更加激进，同时在阴影区域保持隐蔽。凭借这种阴影自适应攻击的优势，他们进一步建立了第一个评估现有深度阴影去除网络对抗性鲁棒性的基准。
- en: VII Future Research Directions
  id: totrans-800
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 未来研究方向
- en: 'As observed from the experimental results presented in Section [V](#S5 "V Benchmarking
    and Empirical Analysis ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey"), shadow removal remains a challenging research topic, particularly concerning
    generalization ability. There remains considerable potential for improvement.
    This section suggests potential future research directions as follows:'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '从第 [V](#S5 "V Benchmarking and Empirical Analysis ‣ Single-Image Shadow Removal
    Using Deep Learning: A Comprehensive Survey") 节中的实验结果可以观察到，阴影去除仍然是一个具有挑战性的研究主题，特别是在泛化能力方面。仍有相当大的改进潜力。本节建议了以下潜在的未来研究方向：'
- en: VII-A Generalized Shadow Removal
  id: totrans-802
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 泛化阴影去除
- en: '1.'
  id: totrans-803
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: From supervised to unsupervised/zero-shot learning. While supervised learning-based
    shadow removal methods have demonstrated superior performance within specific
    datasets, their effectiveness diminishes significantly when applied to testing
    cases with distributions distinct from the training data. However, unsupervised/zero-shot
    learning methods offer a promising alternative. By harnessing the ability to capture
    the underlying structure and characteristics of data without necessitating labeled
    examples, these methods facilitate enhanced generalization to unseen, real-world
    scenarios.
  id: totrans-804
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从监督学习到无监督/零样本学习。尽管基于监督学习的阴影去除方法在特定数据集内表现优异，但当应用于分布与训练数据不同的测试案例时，其效果显著下降。然而，无监督/零样本学习方法提供了一种有前景的替代方案。通过利用捕捉数据潜在结构和特征的能力，而无需标记样本，这些方法有助于增强对未见的真实世界场景的泛化能力。
- en: '2.'
  id: totrans-805
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Exploiting knowledge from pre-trained model. Leveraging knowledge from pre-trained
    models, such as large language models [[32](#bib.bib32), [105](#bib.bib105)],
    stable diffusion [[106](#bib.bib106), [107](#bib.bib107)], and SAM (Spatially
    Adaptive Denoising Network) [[108](#bib.bib108)], holds immense potential for
    advancing shadow removal techniques. These models, having been trained on vast
    amounts of diverse data, encode rich representations of the underlying structure
    and semantics of the input data. By fine-tuning or incorporating these pre-trained
    models into shadow removal frameworks, researchers can tap into this wealth of
    knowledge to improve the quality and robustness of shadow removal algorithms.
  id: totrans-806
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 利用预训练模型中的知识。利用预训练模型中的知识，例如大型语言模型 [[32](#bib.bib32), [105](#bib.bib105)]，稳定扩散 [[106](#bib.bib106),
    [107](#bib.bib107)]，以及 SAM（空间自适应去噪网络） [[108](#bib.bib108)]，在推进阴影去除技术方面具有巨大的潜力。这些模型已经在大量多样的数据上进行了训练，编码了输入数据的潜在结构和语义的丰富表示。通过微调或将这些预训练模型纳入阴影去除框架中，研究人员可以利用这丰富的知识来提高阴影去除算法的质量和鲁棒性。
- en: VII-B Interactive Shadow Removal
  id: totrans-807
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 交互式阴影去除
- en: Interactive shadow removal involves users actively participating in removing
    shadows from images through a user-friendly interface, typically utilizing tools
    or algorithms enabling real-time adjustments and feedback. This method grants
    users enhanced control and customization over the shadow removal process, allowing
    them to tailor results to their specific preferences and requirements.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式阴影去除涉及用户通过用户友好的界面积极参与图像的阴影去除，通常使用能够实现实时调整和反馈的工具或算法。这种方法赋予用户对阴影去除过程的更高控制和自定义能力，使他们能够根据自己的特定偏好和需求调整结果。
- en: '1.'
  id: totrans-809
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: User-friendly input. Existing methods have overly stringent requirements for
    the input mask, or they rely on the off-the-shelf shadow detectors to generate
    the mask. However, using a pre-trained shadow detector may lead to a domain gap
    when applied to cases with different distributions from the training data, resulting
    in inaccurate shadow detection that could mislead the subsequent shadow removal
    network. Therefore, by utilizing simple user inputs such as clicks, strokes, or
    painted masks as prompts, the shadow removal network can effectively guide the
    process.
  id: totrans-810
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户友好的输入。现有方法对输入掩码的要求过于严格，或依赖现成的阴影检测器生成掩码。然而，使用预训练的阴影检测器可能会导致与训练数据分布不同的情况时出现领域差距，从而导致不准确的阴影检测，可能会误导后续的阴影去除网络。因此，通过利用简单的用户输入，例如点击、笔画或绘制掩码作为提示，阴影去除网络可以有效地引导处理过程。
- en: '2.'
  id: totrans-811
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Real-time adjustment. The shadow removal process is optimized for real-time
    performance, ensuring that the results are generated quickly enough to be displayed
    without noticeable delay. This may involve leveraging hardware acceleration, parallel
    processing, or optimized algorithms to achieve fast computation speeds. Additionally,
    in interactive shadow removal systems, users may provide feedback or make adjustments
    in real-time to refine the shadow removal results.
  id: totrans-812
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实时调整。阴影去除过程经过优化以实现实时性能，确保结果能够迅速生成以便显示而不会有明显的延迟。这可能涉及利用硬件加速、并行处理或优化算法以实现快速计算速度。此外，在交互式阴影去除系统中，用户可能会提供反馈或实时调整以完善阴影去除结果。
- en: '3.'
  id: totrans-813
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Multiple shadows process. In real-world scenarios, it’s common to encounter
    multiple shadow regions, stemming from various sources such as multiple occluders
    or diverse light sources. When tackling these scenarios, users often seek the
    flexibility to selectively remove portions of these shadows based on user-defined
    input cues. This could involve removing shadows cast by specific objects or adjusting
    the intensity of shadows in certain areas, empowering users to tailor the shadow
    removal process to their specific needs and preferences.
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多重阴影处理。在现实世界场景中，常常会遇到多个阴影区域，这些阴影来自各种来源，例如多个遮挡物或不同的光源。在处理这些场景时，用户通常寻求灵活性，以根据用户定义的输入提示选择性地去除这些阴影的某些部分。这可能涉及去除特定物体投射的阴影或调整某些区域阴影的强度，使用户能够根据自己的具体需求和偏好调整阴影去除过程。
- en: VII-C More Comprehensive Benchmark
  id: totrans-815
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 更全面的基准测试
- en: Currently, the field of shadow removal faces a significant challenge due to
    the absence of a standardized evaluation benchmark. Existing test datasets for
    shadow removal may not adequately represent the diversity and complexity of real-world
    shadow scenarios. Some test data may be relatively straightforward to handle,
    whose distributions are distinct from real-world cases. This discrepancy further
    complicates the evaluation process and undermines the reliability of comparative
    studies. Hence, there is a clear need for a comprehensive benchmark that encompasses
    large-scale test samples, representing a diverse range of scenes including indoor
    and outdoor environments, as well as challenging lighting conditions such as various
    types of light sources and multiple shadows. Establishing such a benchmark would
    enable researchers to conduct fair and rigorous evaluations of shadow removal
    methods, facilitating advancements in the field.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，阴影去除领域面临着一个重大挑战，即缺乏标准化的评估基准。现有的阴影去除测试数据集可能无法充分代表现实世界阴影场景的多样性和复杂性。一些测试数据可能相对简单，分布与现实情况有所不同。这种差异进一步复杂化了评估过程，削弱了比较研究的可靠性。因此，显然需要一个涵盖大规模测试样本的综合基准，代表各种场景，包括室内和室外环境，以及挑战性的光照条件，如各种光源和多个阴影。建立这样一个基准将使研究人员能够公平而严格地评估阴影去除方法，促进该领域的发展。
- en: VII-D Non-Reference Evaluation Metric
  id: totrans-817
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 非参考评估指标
- en: Collecting paired shadow and shadow-free images poses a significant challenge
    in practical scenarios, mainly due to the complexities involved in capturing images
    under diverse lighting conditions. Furthermore, prevailing benchmarks heavily
    rely on reference metrics such as PSNR, RMSE, and SSIM for evaluation, which may
    not adequately account for scenarios where ground truth data or paired images
    are scarce or inaccessible. In instances where large-scale shadow datasets featuring
    unpaired images, or those primarily collected for shadow detection purposes (e.g.,
    SBU [[56](#bib.bib56)] and USR [[95](#bib.bib95)]), are available, leveraging
    them to the fullest extent for benchmarking becomes crucial. However, in such
    cases, the necessity for new non-reference metrics becomes apparent. These metrics
    offer a vital solution to the limitations posed by traditional evaluation methods,
    providing a more holistic and adaptable approach to image assessment. Introducing
    novel non-reference metrics addresses the shortcomings of traditional evaluation
    methods, offering a more comprehensive and adaptable approach to image assessment.
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 收集成对的阴影和无阴影图像在实际场景中面临着重大挑战，主要由于在多种光照条件下捕捉图像的复杂性。此外，现有的基准严重依赖于参考指标，如PSNR、RMSE和SSIM进行评估，而这些指标可能无法充分考虑缺乏真实数据或成对图像的情况。在有大规模的阴影数据集（例如，SBU [[56](#bib.bib56)]和USR [[95](#bib.bib95)]）可用的情况下，充分利用这些数据进行基准测试变得至关重要。然而，在这种情况下，新型非参考指标的必要性变得显而易见。这些指标为传统评估方法带来的局限性提供了关键解决方案，提供了更全面和适应性的图像评估方法。引入新型非参考指标解决了传统评估方法的不足，提供了更全面和适应性的图像评估方法。
- en: VII-E Extension to Video Shadow Removal
  id: totrans-819
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-E 视频阴影去除的扩展
- en: Current video shadow removal methods typically involve processing each frame
    of a video independently using image shadow removal algorithms, as mentioned by [[63](#bib.bib63),
    [64](#bib.bib64)]. This approach, however, tends to neglect the temporal dimension
    of the video data. By treating each frame in isolation, these methods may fail
    to capture and exploit the temporal coherence present in video sequences. When
    image-based algorithms are directly applied to videos, they often produce artifacts
    and inconsistencies across frames. This is because they do not account for the
    temporal context and variations in lighting conditions that occur over time. As
    a result, the shadow removal process may introduce flickering or other temporal
    artifacts, leading to unsatisfactory results. Therefore, there remains a significant
    gap in the field of video shadow removal, calling for further research and development.
    Efforts are needed to explore novel algorithms that can effectively leverage the
    temporal dimension of video data.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的视频阴影去除方法通常涉及使用图像阴影去除算法独立处理视频的每一帧，如[[63](#bib.bib63), [64](#bib.bib64)]所提到的。然而，这种方法往往忽视了视频数据的时间维度。通过将每一帧孤立处理，这些方法可能无法捕捉和利用视频序列中的时间一致性。当图像处理算法直接应用于视频时，往往会在帧间产生伪影和不一致现象。这是因为它们没有考虑时间上下文和随时间变化的光照条件。因此，阴影去除过程可能会引入闪烁或其他时间伪影，从而导致结果不尽如人意。因此，视频阴影去除领域仍然存在显著的差距，需要进一步的研究和开发。需要探索能够有效利用视频数据时间维度的新算法。
- en: VII-F Large-Scale Training Dataset
  id: totrans-821
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-F 大规模训练数据集
- en: 'While numerous training datasets are available for shadow removal, their scale
    and diversity in representing various shadow categories do not fully capture the
    complexity of real shadow degradations, as indicated in Table [II](#S4.T2 "TABLE
    II ‣ IV-E Loss Function ‣ IV Technical Review and Discussion ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey"). Current deep shadow removal
    models, as discussed in Section [V](#S5 "V Benchmarking and Empirical Analysis
    ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey"), encounter
    challenges in achieving satisfactory performance when confronted with shadow images
    captured in real-world scenarios. Moreover, the resolution of existing datasets
    is typically low, such as $480\times 640$, whereas real-world scenarios often
    involve high-resolution imagery, such as $2k$ or $4k$. Models trained on low-resolution
    data often struggle to generalize directly to high-resolution scenarios. Therefore,
    additional efforts are needed to explore the acquisition of large-scale, high-resolution,
    and diverse real-world paired shadow removal training datasets.'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管有大量的训练数据集用于阴影去除，但它们在表示各种阴影类别的规模和多样性并未完全捕捉到真实阴影退化的复杂性，如表[II](#S4.T2 "TABLE
    II ‣ IV-E Loss Function ‣ IV Technical Review and Discussion ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey")所示。当前的深度阴影去除模型，如第[V](#S5
    "V Benchmarking and Empirical Analysis ‣ Single-Image Shadow Removal Using Deep
    Learning: A Comprehensive Survey")节所讨论的，面临在真实场景中捕捉阴影图像时达到令人满意的性能的挑战。此外，现有数据集的分辨率通常较低，如$480\times
    640$，而真实世界的场景通常涉及高分辨率图像，如$2k$或$4k$。在低分辨率数据上训练的模型通常难以直接推广到高分辨率场景。因此，需要更多努力探索获取大规模、高分辨率和多样化的真实世界配对阴影去除训练数据集。'
- en: VIII Conclusion
  id: totrans-823
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: In this paper, we present a comprehensive survey of deep learning-based single-image
    shadow removal approaches. We first discuss the formation of shadows in the real
    world and summarize the challenges of the shadow removal task. Building upon this,
    we explain how existing methods address these challenges from various perspectives,
    including training strategies, model architecture design, integration of physical
    models, and leveraging knowledge from detection models. We also conduct a comprehensive
    evaluation of existing methods with different learning strategies to explore their
    advantages and disadvantages. Based on the experimental results, we identify several
    open problems that still require further development, such as improving generalization
    to real-world cases, interactive shadow removal, more comprehensive benchmarks,
    video shadow removal, and large-scale training datasets.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了基于深度学习的单图像阴影去除方法的全面综述。我们首先讨论了现实世界中阴影的形成，并总结了阴影去除任务的挑战。在此基础上，我们解释了现有方法如何从各种角度应对这些挑战，包括训练策略、模型架构设计、物理模型的整合以及利用检测模型的知识。我们还对现有方法进行了全面评估，探索不同学习策略的优缺点。根据实验结果，我们确定了几个仍需进一步发展的开放问题，例如改进对实际情况的泛化、交互式阴影去除、更全面的基准、视频阴影去除和大规模训练数据集。
- en: References
  id: totrans-825
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati, “Detecting moving objects,
    ghosts, and shadows in video streams,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 25, no. 10, pp. 1337–1342, 2003.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. Cucchiara, C. Grana, M. Piccardi, 和 A. Prati，“在视频流中检测运动物体、鬼影和阴影，” *IEEE
    计算机视觉与模式识别汇刊*，第25卷，第10期，第1337–1342页，2003年。'
- en: '[2] S. Nadimi and B. Bhanu, “Physical models for moving shadow and object detection
    in video,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 26,
    no. 8, pp. 1079–1087, 2004.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Nadimi 和 B. Bhanu，“视频中移动阴影和物体检测的物理模型，” *IEEE 计算机视觉与模式识别汇刊*，第26卷，第8期，第1079–1087页，2004年。'
- en: '[3] C. R. Jung, “Efficient background subtraction and shadow removal for monochromatic
    video sequences,” *IEEE Transactions on Multimedia*, vol. 11, no. 3, pp. 571–577,
    2009.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. R. Jung，“高效的背景减除和单色视频序列的阴影去除，” *IEEE 多媒体汇刊*，第11卷，第3期，第571–577页，2009年。'
- en: '[4] A. Sanin, C. Sanderson, and B. C. Lovell, “Improved shadow removal for
    robust person tracking in surveillance scenarios,” in *2010 20th International
    Conference on Pattern Recognition*.   IEEE, 2010, pp. 141–144.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Sanin, C. Sanderson, 和 B. C. Lovell，“改进的阴影去除用于监控场景中的鲁棒人员跟踪，” 在 *2010年第20届国际模式识别大会*。
    IEEE，2010年，第141–144页。'
- en: '[5] W. Zhang, X. Zhao, J.-M. Morvan, and L. Chen, “Improving shadow suppression
    for illumination robust face recognition,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 41, no. 3, pp. 611–624, 2018.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] W. Zhang, X. Zhao, J.-M. Morvan, 和 L. Chen，“提高对光照鲁棒的面部识别的阴影抑制，” *IEEE 计算机视觉与模式识别汇刊*，第41卷，第3期，第611–624页，2018年。'
- en: '[6] E. Arbel and H. Hel-Or, “Texture-preserving shadow removal in color images
    containing curved surfaces,” in *2007 IEEE Conference on Computer Vision and Pattern
    Recognition*.   IEEE, 2007, pp. 1–8.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] E. Arbel 和 H. Hel-Or，“在包含曲面彩色图像中保持纹理的阴影去除，” 在 *2007年IEEE计算机视觉与模式识别会议*。
    IEEE，2007年，第1–8页。'
- en: '[7] C. Fredembach and G. Finlayson, “Simple shadow remova,” in *18th International
    Conference on Pattern Recognition (ICPR’06)*, vol. 1.   IEEE, 2006, pp. 832–835.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Fredembach 和 G. Finlayson，“简单阴影去除，” 在 *第18届国际模式识别大会（ICPR’06）*，第1卷。 IEEE，2006年，第832–835页。'
- en: '[8] N. Salamati, A. Germain, and S. Siisstrunk, “Removing shadows from images
    using color and near-infrared,” in *2011 18th IEEE International Conference on
    Image Processing*.   IEEE, 2011, pp. 1713–1716.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] N. Salamati, A. Germain, 和 S. Siisstrunk，“利用颜色和近红外去除图像中的阴影，” 在 *2011年第18届IEEE国际图像处理会议*。
    IEEE，2011年，第1713–1716页。'
- en: '[9] L. Zhang, Q. Zhang, and C. Xiao, “Shadow remover: Image shadow removal
    based on illumination recovering optimization,” *IEEE Transactions on Image Processing*,
    vol. 24, no. 11, pp. 4623–4636, 2015.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. Zhang, Q. Zhang, 和 C. Xiao，“阴影去除器：基于光照恢复优化的图像阴影去除，” *IEEE 图像处理汇刊*，第24卷，第11期，第4623–4636页，2015年。'
- en: '[10] R. Guo, Q. Dai, and D. Hoiem, “Paired regions for shadow detection and
    removal,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 35,
    no. 12, pp. 2956–2967, 2012.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Guo, Q. Dai, 和 D. Hoiem，“用于阴影检测和去除的配对区域，” *IEEE 计算机视觉与模式识别汇刊*，第35卷，第12期，第2956–2967页，2012年。'
- en: '[11] T. F. Y. Vicente and D. Samaras, “Single image shadow removal via neighbor-based
    region relighting,” in *European Conference on Computer Vision*.   Springer, 2014,
    pp. 309–320.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] T. F. Y. Vicente 和 D. Samaras，“通过邻域基础区域重光照进行单幅图像阴影去除”，收录于*欧洲计算机视觉会议*。Springer，2014年，页
    309–320。'
- en: '[12] G. D. Finlayson, S. D. Hordley, and M. S. Drew, “Removing shadows from
    images,” in *Computer Vision—ECCV 2002: 7th European Conference on Computer Vision
    Copenhagen, Denmark, May 28–31, 2002 Proceedings, Part IV 7*.   Springer, 2002,
    pp. 823–836.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] G. D. Finlayson、S. D. Hordley 和 M. S. Drew，“从图像中去除阴影”，收录于*计算机视觉——ECCV
    2002：第 7 届欧洲计算机视觉会议，哥本哈根，丹麦，2002年 5 月 28–31 日论文集，第 IV 部分*。Springer，2002年，页 823–836。'
- en: '[13] G. D. Finlayson, S. D. Hordley, C. Lu, and M. S. Drew, “On the removal
    of shadows from images,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 28, no. 1, pp. 59–68, 2005.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] G. D. Finlayson、S. D. Hordley、C. Lu 和 M. S. Drew，“图像阴影去除研究”，*IEEE 模式分析与机器智能汇刊*，第
    28 卷，第 1 期，页 59–68，2005年。'
- en: '[14] G. D. Finlayson, S. D. Hordley, and M. S. Drew, “Removing shadows from
    images using retinex,” in *Color and imaging conference*, vol. 10.   Society of
    Imaging Science and Technology, 2002, pp. 73–79.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] G. D. Finlayson、S. D. Hordley 和 M. S. Drew，“使用 Retinex 从图像中去除阴影”，收录于*色彩与成像会议*，第
    10 卷。成像科学与技术学会，2002年，页 73–79。'
- en: '[15] C. Fredembach and G. Finlayson, “Hamiltonian path-based shadow removal,”
    in *Proceedings of the 16th British Machine Vision Conference (BMVC)*, vol. 2,
    2005, pp. 502–511.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. Fredembach 和 G. Finlayson，“基于哈密顿路径的阴影去除”，收录于*第 16 届英国机器视觉会议（BMVC）论文集*，第
    2 卷，2005年，页 502–511。'
- en: '[16] C. Fredembach and G. D. Finlayson, “Fast re-integration of shadow free
    images,” in *12th Color Imaging Conference: Color Science and Engineering Systems,
    Technologies, and Applications*, 2004.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. Fredembach 和 G. D. Finlayson，“快速重新整合去除阴影的图像”，收录于*第 12 届色彩成像会议：色彩科学与工程系统、技术与应用*，2004年。'
- en: '[17] M. Gryka, M. Terry, and G. J. Brostow, “Learning to remove soft shadows,”
    *ACM Transactions on Graphics*, vol. 34, no. 5, pp. 1–15, 2015.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Gryka、M. Terry 和 G. J. Brostow，“学习去除柔性阴影”，*ACM 图形学汇刊*，第 34 卷，第 5 期，页
    1–15，2015年。'
- en: '[18] T.-P. Wu and C.-K. Tang, “A bayesian approach for shadow extraction from
    a single image,” in *Tenth IEEE International Conference on Computer Vision (ICCV’05)
    Volume 1*, vol. 1.   IEEE, 2005, pp. 480–487.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T.-P. Wu 和 C.-K. Tang，“一种基于贝叶斯的方法从单幅图像中提取阴影”，收录于*第十届 IEEE 国际计算机视觉会议（ICCV’05）第
    1 卷*，第 1 卷。IEEE，2005年，页 480–487。'
- en: '[19] Y. Shor and D. Lischinski, “The shadow meets the mask: Pyramid-based shadow
    removal,” in *Computer Graphics Forum*, vol. 27, no. 2.   Wiley Online Library,
    2008, pp. 577–586.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Y. Shor 和 D. Lischinski，“阴影遇上掩膜：基于金字塔的阴影去除”，收录于*计算机图形学论坛*，第 27 卷，第 2 期。Wiley
    在线图书馆，2008年，页 577–586。'
- en: '[20] C. Xiao, R. She, D. Xiao, and K.-L. Ma, “Fast shadow removal using adaptive
    multi-scale illumination transfer,” in *Computer Graphics Forum*, vol. 32.   Wiley
    Online Library, 2013, pp. 207–218.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. Xiao、R. She、D. Xiao 和 K.-L. Ma，“使用自适应多尺度光照转移的快速阴影去除”，收录于*计算机图形学论坛*，第
    32 卷。Wiley 在线图书馆，2013年，页 207–218。'
- en: '[21] C. Xiao, D. Xiao, L. Zhang, and L. Chen, “Efficient shadow removal using
    subregion matching illumination transfer,” in *Computer Graphics Forum*, vol. 32,
    no. 7.   Wiley Online Library, 2013, pp. 421–430.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. Xiao、D. Xiao、L. Zhang 和 L. Chen，“使用子区域匹配光照转移的高效阴影去除”，收录于*计算机图形学论坛*，第
    32 卷，第 7 期。Wiley 在线图书馆，2013年，页 421–430。'
- en: '[22] S. H. Khan, M. Bennamoun, F. Sohel, and R. Togneri, “Automatic shadow
    detection and removal from a single image,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 38, no. 3, pp. 431–446, 2015.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] S. H. Khan、M. Bennamoun、F. Sohel 和 R. Togneri，“自动从单幅图像中检测和去除阴影”，*IEEE
    模式分析与机器智能汇刊*，第 38 卷，第 3 期，页 431–446，2015年。'
- en: '[23] G. D. Finlayson, M. S. Drew, and C. Lu, “Entropy minimization for shadow
    removal,” *International Journal of Computer Vision*, vol. 85, no. 1, pp. 35–57,
    2009.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] G. D. Finlayson、M. S. Drew 和 C. Lu，“用于阴影去除的熵最小化”，*计算机视觉国际期刊*，第 85 卷，第
    1 期，页 35–57，2009年。'
- en: '[24] L. Qu, J. Tian, S. He, Y. Tang, and R. W. Lau, “Deshadownet: A multi-context
    embedding deep network for shadow removal,” in *CVPR*, 2017, pp. 4067–4075.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Qu、J. Tian、S. He、Y. Tang 和 R. W. Lau，“Deshadownet：一种多上下文嵌入深度网络用于阴影去除”，收录于*CVPR*，2017年，页
    4067–4075。'
- en: '[25] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Handwritten digit recognition with a back-propagation network,”
    *Advances in neural information processing systems*, vol. 2, pp. 396–404, 1989.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. LeCun、B. Boser、J. S. Denker、D. Henderson、R. E. Howard、W. Hubbard 和
    L. D. Jackel，“利用反向传播网络进行手写数字识别”，*神经信息处理系统进展*，第 2 卷，页 396–404，1989年。'
- en: '[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Advances in neural information processing
    systems*, vol. 25, pp. 1097–1105, 2012.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络的Imagenet分类，” *神经信息处理系统进展*，第25卷，页码1097–1105，2012年。'
- en: '[27] R. Venkatesan and B. Li, *Convolutional neural networks in visual computing:
    a concise guide*.   CRC Press, 2017.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] R. Venkatesan 和 B. Li，*视觉计算中的卷积神经网络：简明指南*。 CRC出版社，2017年。'
- en: '[28] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” *Communications
    of the ACM*, vol. 63, no. 11, pp. 139–144, 2020.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络，” *ACM通讯*，第63卷，第11期，页码139–144，2020年。'
- en: '[29] Y. Zhu, Z. Xiao, Y. Fang, X. Fu, Z. Xiong, and Z.-J. Zha, “Efficient model-driven
    network for shadow removal,” in *Proceedings of the AAAI conference on artificial
    intelligence*, vol. 36, no. 3, 2022, pp. 3635–3643.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Zhu, Z. Xiao, Y. Fang, X. Fu, Z. Xiong, 和 Z.-J. Zha，“高效的模型驱动网络用于阴影去除，”
    载于 *AAAI人工智能会议论文集*，第36卷，第3期，2022年，页码3635–3643。'
- en: '[30] L. Guo, C. Wang, W. Yang, S. Huang, Y. Wang, H. Pfister, and B. Wen, “Shadowdiffusion:
    When degradation prior meets diffusion model for shadow removal,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 14 049–14 058.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] L. Guo, C. Wang, W. Yang, S. Huang, Y. Wang, H. Pfister, 和 B. Wen，“Shadowdiffusion:
    当退化先验遇到扩散模型用于阴影去除，” 载于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2023年，页码14 049–14 058。'
- en: '[31] B. Ding, C. Long, L. Zhang, and C. Xiao, “Argan: Attentive recurrent generative
    adversarial network for shadow detection and removal,” in *Proceedings of the
    IEEE/CVF international conference on computer vision*, 2019, pp. 10 213–10 222.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] B. Ding, C. Long, L. Zhang, 和 C. Xiao，“Argan: 用于阴影检测和去除的注意力递归生成对抗网络，”
    载于 *IEEE/CVF国际计算机视觉会议论文集*，2019年，页码10 213–10 222。'
- en: '[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017, pp. 5998–6008.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力机制才是关键，” 2017年，页码5998–6008。'
- en: '[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” in *International Conference
    on Learning Representations*, 2020.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“图像价值16x16词：用于大规模图像识别的变换器，”
    载于 *国际学习表示会议*，2020年。'
- en: '[34] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
    transformer: Hierarchical vision transformer using shifted windows,” in *Proceedings
    of the IEEE/CVF international conference on computer vision*, 2021, pp. 10 012–10 022.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, 和 B. Guo，“Swin
    transformer: Hierarchical vision transformer using shifted windows，” 载于 *IEEE/CVF国际计算机视觉会议论文集*，2021年，页码10 012–10 022。'
- en: '[35] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, “Swinir:
    Image restoration using swin transformer,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 1833–1844.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, 和 R. Timofte，“Swinir:
    使用Swin变换器的图像恢复，” 载于 *IEEE/CVF国际计算机视觉会议论文集*，2021年，页码1833–1844。'
- en: '[36] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep
    unsupervised learning using nonequilibrium thermodynamics,” in *International
    Conference on Machine Learning*.   PMLR, 2015, pp. 2256–2265.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, 和 S. Ganguli，“使用非平衡热力学的深度无监督学习，”
    载于 *国际机器学习会议*。 PMLR, 2015年，页码2256–2265。'
- en: '[37] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    *Advances in Neural Information Processing Systems*, vol. 33, pp. 6840–6851, 2020.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Ho, A. Jain, 和 P. Abbeel，“去噪扩散概率模型，” *神经信息处理系统进展*，第33卷，页码6840–6851，2020年。'
- en: '[38] B. Kawar, M. Elad, S. Ermon, and J. Song, “Denoising diffusion restoration
    models,” *Advances in Neural Information Processing Systems*, vol. 35, pp. 23 593–23 606,
    2022.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] B. Kawar, M. Elad, S. Ermon, 和 J. Song，“去噪扩散恢复模型，” *神经信息处理系统进展*，第35卷，页码23 593–23 606，2022年。'
- en: '[39] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi,
    “Image super-resolution via iterative refinement,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 2022.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, 和 M. Norouzi，“通过迭代优化实现图像超分辨率，”
    *IEEE模式分析与机器智能汇刊*，2022年。'
- en: '[40] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, and L. Van Gool,
    “Repaint: Inpainting using denoising diffusion probabilistic models,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 11 461–11 471.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, 和 L. Van Gool，“Repaint：使用去噪扩散概率模型的修复”，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页码11 461–11 471。'
- en: '[41] S. Murali, V. Govindan, and S. Kalady, “A survey on shadow removal techniques
    for single image,” *International Journal of Image, Graphics and Signal Processing*,
    vol. 8, no. 12, p. 38, 2016.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Murali, V. Govindan, 和 S. Kalady，“单张图像阴影去除技术的调查”，*国际图像、图形与信号处理杂志*，第8卷，第12期，页码38，2016年。'
- en: '[42] A. Tiwari, P. K. Singh, and S. Amin, “A survey on shadow detection and
    removal in images and video sequences,” in *2016 6th International Conference-Cloud
    System and Big Data Engineering (Confluence)*.   IEEE, 2016, pp. 518–523.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Tiwari, P. K. Singh, 和 S. Amin，“图像和视频序列中阴影检测与去除的调查”，发表于 *2016年第六届国际会议-云系统与大数据工程（Confluence）*。IEEE，2016，页码518–523。'
- en: '[43] S. Das and A. Aery, “A review: shadow detection and shadow removal from
    images,” *International Journal of Engineering Trends and Technology (IJETT)*,
    vol. 4, no. 5, pp. 1764–1767, 2013.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] S. Das 和 A. Aery，“综述：图像中的阴影检测与去除”，*国际工程趋势与技术杂志（IJETT）*，第4卷，第5期，页码1764–1767，2013年。'
- en: '[44] E. H. Land, “The retinex theory of color vision,” *Scientific american*,
    vol. 237, no. 6, pp. 108–129, 1977.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] E. H. Land，“视网膜理论的颜色视觉”，*科学美国人*，第237卷，第6期，页码108–129，1977年。'
- en: '[45] J. Wang, X. Li, and J. Yang, “Stacked conditional generative adversarial
    networks for jointly learning shadow detection and shadow removal,” in *CVPR*,
    2018, pp. 1788–1797.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Wang, X. Li, 和 J. Yang，“用于联合学习阴影检测和阴影去除的堆叠条件生成对抗网络”，发表于 *CVPR*，2018年，页码1788–1797。'
- en: '[46] X. Hu, C.-W. Fu, L. Zhu, J. Qin, and P.-A. Heng, “Direction-aware spatial
    context features for shadow detection and removal,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 42, no. 11, pp. 2795–2808, 2019.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] X. Hu, C.-W. Fu, L. Zhu, J. Qin, 和 P.-A. Heng，“用于阴影检测与去除的方向感知空间上下文特征”，*IEEE模式分析与机器智能汇刊*，第42卷，第11期，页码2795–2808，2019年。'
- en: '[47] L. Zhang, C. Long, X. Zhang, and C. Xiao, “Ris-gan: Explore residual and
    illumination with generative adversarial networks for shadow removal,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 07, 2020, pp.
    12 829–12 836.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] L. Zhang, C. Long, X. Zhang, 和 C. Xiao，“Ris-gan：通过生成对抗网络探索残差和光照以实现阴影去除”，发表于
    *AAAI人工智能会议论文集*，第34卷，第07期，2020年，页码12 829–12 836。'
- en: '[48] X. Cun, C.-M. Pun, and C. Shi, “Towards ghost-free shadow removal via
    dual hierarchical aggregation network and shadow matting gan.” in *AAAI*, 2020,
    pp. 10 680–10 687.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] X. Cun, C.-M. Pun, 和 C. Shi，“通过双层次聚合网络和阴影抠图gan实现无鬼影的阴影去除”，发表于 *AAAI*，2020年，页码10 680–10 687。'
- en: '[49] Y.-H. Lin, W.-C. Chen, and Y.-Y. Chuang, “Bedsr-net: A deep shadow removal
    network from a single document image,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 12 905–12 914.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y.-H. Lin, W.-C. Chen, 和 Y.-Y. Chuang，“Bedsr-net：一种从单张文档图像中深度去除阴影的网络”，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页码12 905–12 914。'
- en: '[50] Z. Chen, C. Long, L. Zhang, and C. Xiao, “Canet: A context-aware network
    for shadow removal,” in *Proceedings of the IEEE/CVF international conference
    on computer vision*, 2021, pp. 4743–4752.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. Chen, C. Long, L. Zhang, 和 C. Xiao，“Canet：一种用于阴影去除的上下文感知网络”，发表于 *IEEE/CVF国际计算机视觉会议论文集*，2021年，页码4743–4752。'
- en: '[51] H. Le and D. Samaras, “Shadow removal via shadow image decomposition,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2019, pp. 8578–8587.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Le 和 D. Samaras，“通过阴影图像分解进行阴影去除”，发表于 *IEEE/CVF国际计算机视觉会议论文集*，2019年，页码8578–8587。'
- en: '[52] L. Fu, C. Zhou, Q. Guo, F. Juefei-Xu, H. Yu, W. Feng, Y. Liu, and S. Wang,
    “Auto-exposure fusion for single-image shadow removal,” in *Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition*, 2021, pp. 10 571–10 580.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] L. Fu, C. Zhou, Q. Guo, F. Juefei-Xu, H. Yu, W. Feng, Y. Liu, 和 S. Wang，“单张图像阴影去除的自动曝光融合”，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，页码10 571–10 580。'
- en: '[53] S. He, B. Peng, J. Dong, and Y. Du, “Mask-shadownet: Toward shadow removal
    via masked adaptive instance normalization,” *IEEE Signal Processing Letters*,
    vol. 28, pp. 957–961, 2021.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] S. He, B. Peng, J. Dong, 和 Y. Du，“Mask-shadownet：通过掩码自适应实例归一化实现阴影去除”，*IEEE信号处理通讯*，第28卷，页码957–961，2021年。'
- en: '[54] Y. Zhu, J. Huang, X. Fu, F. Zhao, Q. Sun, and Z.-J. Zha, “Bijective mapping
    network for shadow removal,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 5627–5636.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Y. Zhu、J. Huang、X. Fu、F. Zhao、Q. Sun 和 Z.-J. Zha，“用于阴影去除的双射映射网络”，见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第5627–5636页。'
- en: '[55] L. Guo, S. Huang, D. Liu, H. Cheng, and B. Wen, “Shadowformer: Global
    context helps shadow removal,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 37, no. 1, 2023, pp. 710–718.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L. Guo、S. Huang、D. Liu、H. Cheng 和 B. Wen，“Shadowformer：全球上下文有助于阴影去除”，见于
    *AAAI人工智能会议论文集*，第37卷，第1期，2023年，第710–718页。'
- en: '[56] X. Hu, Y. Jiang, C.-W. Fu, and P.-A. Heng, “Mask-shadowgan: Learning to
    remove shadows from unpaired data,” in *Proceedings of the IEEE/CVF international
    conference on computer vision*, 2019, pp. 2472–2481.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] X. Hu、Y. Jiang、C.-W. Fu 和 P.-A. Heng，“Mask-shadowgan：从无配对数据中学习去除阴影”，见于
    *IEEE/CVF国际计算机视觉会议论文集*，2019年，第2472–2481页。'
- en: '[57] Z. Liu, H. Yin, Y. Mi, M. Pu, and S. Wang, “Shadow removal by a lightness-guided
    network with training on unpaired data,” *IEEE Transactions on Image Processing*,
    vol. 30, pp. 1853–1865, 2021.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Z. Liu、H. Yin、Y. Mi、M. Pu 和 S. Wang，“通过在无配对数据上训练的亮度引导网络进行阴影去除”，*IEEE图像处理汇刊*，第30卷，第1853–1865页，2021年。'
- en: '[58] Y. Jin, A. Sharma, and R. T. Tan, “Dc-shadownet: Single-image hard and
    soft shadow removal using unsupervised domain-classifier guided network,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 5027–5036.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Jin、A. Sharma 和 R. T. Tan，“Dc-shadownet：通过无监督领域分类器引导网络进行单图像硬阴影和软阴影去除”，见于
    *IEEE/CVF国际计算机视觉会议论文集*，2021年，第5027–5036页。'
- en: '[59] Y. He, Y. Xing, T. Zhang, and Q. Chen, “Unsupervised portrait shadow removal
    via generative priors,” in *ACM International Conference on Multimedia (ACM MM)*,
    2021.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. He、Y. Xing、T. Zhang 和 Q. Chen，“通过生成先验进行无监督肖像阴影去除”，见于 *ACM国际多媒体会议（ACM
    MM）*，2021年。'
- en: '[60] A. Shocher, N. Cohen, and M. Irani, ““zero-shot” super-resolution using
    deep internal learning,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 3118–3126.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] A. Shocher、N. Cohen 和 M. Irani，“通过深度内部学习进行‘零样本’超分辨率”，见于 *IEEE计算机视觉与模式识别会议论文集*，2018年，第3118–3126页。'
- en: '[61] C. Guo, C. Li, J. Guo, C. C. Loy, J. Hou, S. Kwong, and R. Cong, “Zero-reference
    deep curve estimation for low-light image enhancement,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020, pp.
    1780–1789.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] C. Guo、C. Li、J. Guo、C. C. Loy、J. Hou、S. Kwong 和 R. Cong，“零参考深度曲线估计用于低光图像增强”，见于
    *IEEE计算机视觉与模式识别会议（CVPR）论文集*，2020年，第1780–1789页。'
- en: '[62] H. Le and D. Samaras, “From shadow segmentation to shadow removal,” in
    *ECCV*, 2020.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] H. Le 和 D. Samaras，“从阴影分割到阴影去除”，见于 *ECCV*，2020年。'
- en: '[63] Z. Liu, H. Yin, X. Wu, Z. Wu, Y. Mi, and S. Wang, “From shadow generation
    to shadow removal,” in *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*, 2021, pp. 4927–4936.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Z. Liu、H. Yin、X. Wu、Z. Wu、Y. Mi 和 S. Wang，“从阴影生成到阴影去除”，见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第4927–4936页。'
- en: '[64] L. Guo, C. Wang, W. Yang, Y. Wang, and B. Wen, “Boundary-aware divide
    and conquer: A diffusion-based solution for unsupervised shadow removal,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2023, pp. 13 045–13 054.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. Guo、C. Wang、W. Yang、Y. Wang 和 B. Wen，“边界感知的分而治之：一种基于扩散的无监督阴影去除解决方案”，见于
    *IEEE/CVF国际计算机视觉会议论文集*，2023年，第13 045–13 054页。'
- en: '[65] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] O. Ronneberger、P. Fischer 和 T. Brox，“U-net：用于生物医学图像分割的卷积网络”，见于 *医学图像计算与计算机辅助干预国际会议*。Springer，2015年，第234–241页。'
- en: '[66] Q. Yu, N. Zheng, J. Huang, and F. Zhao, “Cnsnet: A cleanness-navigated-shadow
    network for shadow removal,” in *Computer Vision–ECCV 2022 Workshops: Tel Aviv,
    Israel, October 23–27, 2022, Proceedings, Part II*.   Springer, 2023, pp. 221–238.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Q. Yu、N. Zheng、J. Huang 和 F. Zhao，“Cnsnet：用于阴影去除的洁净度导航阴影网络”，见于 *计算机视觉–ECCV
    2022研讨会：以色列特拉维夫，2022年10月23–27日，论文集，第二部分*。Springer，2023年，第221–238页。'
- en: '[67] L. Zhu, Z. Deng, X. Hu, C.-W. Fu, X. Xu, J. Qin, and P.-A. Heng, “Bidirectional
    feature pyramid network with recurrent attention residual modules for shadow detection,”
    in *ECCV*, 2018, pp. 121–136.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] L. Zhu、Z. Deng、X. Hu、C.-W. Fu、X. Xu、J. Qin 和 P.-A. Heng，“带有递归注意力残差模块的双向特征金字塔网络用于阴影检测”，见于
    *ECCV*，2018年，第121–136页。'
- en: '[68] J. Wan, H. Yin, Z. Wu, X. Wu, Y. Liu, and S. Wang, “Style-guided shadow
    removal,” in *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
    October 23–27, 2022, Proceedings, Part XIX*.   Springer, 2022, pp. 361–378.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Wan, H. Yin, Z. Wu, X. Wu, Y. Liu, 和 S. Wang，“风格引导的阴影去除”，发表于 *计算机视觉–ECCV
    2022: 第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第XIX部分*。 Springer，2022年，第361–378页。'
- en: '[69] K. Niu, Y. Liu, E. Wu, and G. Xing, “A boundary-aware network for shadow
    removal,” *IEEE Transactions on Multimedia*, 2022.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] K. Niu, Y. Liu, E. Wu, 和 G. Xing，“一个边界感知网络用于阴影去除”，*IEEE多媒体学报*，2022年。'
- en: '[70] J. Liu, Q. Wang, H. Fan, W. Li, L. Qu, and Y. Tang, “A decoupled multi-task
    network for shadow removal,” *IEEE Transactions on Multimedia*, 2023.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Liu, Q. Wang, H. Fan, W. Li, L. Qu, 和 Y. Tang，“用于阴影去除的解耦多任务网络”，*IEEE多媒体学报*，2023年。'
- en: '[71] M. Sen, S. P. Chermala, N. N. Nagori, V. Peddigari, P. Mathur, B. Prasad,
    and M. Jeong, “Shards: Efficient shadow removal using dual stage network for high-resolution
    images,” in *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*, 2023, pp. 1809–1817.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Sen, S. P. Chermala, N. N. Nagori, V. Peddigari, P. Mathur, B. Prasad,
    和 M. Jeong，“Shards: 使用双阶段网络进行高分辨率图像的高效阴影去除”，发表于 *IEEE/CVF冬季计算机视觉应用会议论文集*，2023年，第1809–1817页。'
- en: '[72] M. K. Yücel, V. Dimaridou, B. Manganelli, M. Ozay, A. Drosou, and A. Saà-Garriga,
    “Lra&ldra: Rethinking residual predictions for efficient shadow detection and
    removal,” in *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*, 2023, pp. 4925–4935.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. K. Yücel, V. Dimaridou, B. Manganelli, M. Ozay, A. Drosou, 和 A. Saà-Garriga，“Lra&ldra:
    重新思考残差预测以实现高效阴影检测和去除”，发表于 *IEEE/CVF冬季计算机视觉应用会议论文集*，2023年，第4925–4935页。'
- en: '[73] J. Yu, P. He, and Z. Peng, “Fsr-net: Deep fourier network for shadow removal,”
    in *Proceedings of the 31st ACM International Conference on Multimedia*, 2023,
    pp. 2335–2343.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. Yu, P. He, 和 Z. Peng，“Fsr-net: 深度傅里叶网络用于阴影去除”，发表于 *第31届ACM国际多媒体会议论文集*，2023年，第2335–2343页。'
- en: '[74] X. Li, Q. Guo, R. Abdelfattah, D. Lin, W. Feng, I. Tsang, and S. Wang,
    “Leveraging inpainting for single-image shadow removal,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2023, pp. 13 055–13 064.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] X. Li, Q. Guo, R. Abdelfattah, D. Lin, W. Feng, I. Tsang, 和 S. Wang，“利用修复技术进行单图像阴影去除”，发表于
    *IEEE/CVF国际计算机视觉会议论文集*，2023年，第13 055–13 064页。'
- en: '[75] Y. Liu, Z. Ke, K. Xu, F. Liu, Z. Wang, and R. W. Lau, “Recasting regional
    lighting for shadow removal,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 38, no. 4, 2024, pp. 3810–3818.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Liu, Z. Ke, K. Xu, F. Liu, Z. Wang, 和 R. W. Lau，“重新定义区域光照以去除阴影”，发表于
    *AAAI人工智能会议论文集*，第38卷，第4期，2024年，第3810–3818页。'
- en: '[76] Y. Jin, W. Ye, W. Yang, Y. Yuan, and R. T. Tan, “Des3: Adaptive attention-driven
    self and soft shadow removal using vit similarity,” in *Proceedings of the AAAI
    Conference on Artificial Intelligence*, vol. 38, no. 3, 2024, pp. 2634–2642.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Jin, W. Ye, W. Yang, Y. Yuan, 和 R. T. Tan，“Des3: 自适应注意力驱动的自我和软阴影去除使用vit相似性”，发表于
    *AAAI人工智能会议论文集*，第38卷，第3期，2024年，第2634–2642页。'
- en: '[77] K. Mei, L. Figueroa, Z. Lin, Z. Ding, S. Cohen, and V. M. Patel, “Latent
    feature-guided diffusion models for shadow removal,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2024, pp. 4313–4322.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] K. Mei, L. Figueroa, Z. Lin, Z. Ding, S. Cohen, 和 V. M. Patel，“潜在特征引导的扩散模型用于阴影去除”，发表于
    *IEEE/CVF冬季计算机视觉应用会议论文集*，2024年，第4313–4322页。'
- en: '[78] J. Xiao, X. Fu, Y. Zhu, D. Li, J. Huang, K. Zhu, and Z.-J. Zha, “Homoformer:
    Homogenized transformer for image shadow removal,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2024, pp.
    25 617–25 626.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. Xiao, X. Fu, Y. Zhu, D. Li, J. Huang, K. Zhu, 和 Z.-J. Zha，“Homoformer:
    用于图像阴影去除的同质变换器”，发表于 *IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，2024年6月，第25 617–25 626页。'
- en: '[79] G. Finlayson, M. M. Darrodi, and M. Mackiewicz, “Rank-based camera spectral
    sensitivity estimation,” *JOSA A*, vol. 33, no. 4, pp. 589–599, 2016.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] G. Finlayson, M. M. Darrodi, 和 M. Mackiewicz，“基于排名的相机光谱灵敏度估计”，*JOSA A*，第33卷，第4期，第589–599页，2016年。'
- en: '[80] K. Zhang, L. V. Gool, and R. Timofte, “Deep unfolding network for image
    super-resolution,” in *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*, 2020, pp. 3217–3226.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] K. Zhang, L. V. Gool, 和 R. Timofte，“用于图像超分辨率的深度展开网络”，发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第3217–3226页。'
- en: '[81] C. Wang, L. Guo, Y. Wang, H. Cheng, Y. Yu, and B. Wen, “Progressive divide-and-conquer
    via subsampling decomposition for accelerated mri,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2024, pp. 25 128–25 137.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] C. Wang, L. Guo, Y. Wang, H. Cheng, Y. Yu, 和 B. Wen, “通过子采样分解的渐进式分而治之用于加速MRI，”
    见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2024年，页码25 128–25 137。'
- en: '[82] H. Zheng, H. Yong, and L. Zhang, “Unfolded deep kernel estimation for
    blind image super-resolution,” in *European Conference on Computer Vision*.   Springer,
    2022, pp. 502–518.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] H. Zheng, H. Yong, 和 L. Zhang, “盲图像超分辨率的展开深度核估计，” 见 *欧洲计算机视觉会议*。 施普林格，2022年，页码502–518。'
- en: '[83] C. Wang, R. Zhang, G. Maliakal, S. Ravishankar, and B. Wen, “Deep reinforcement
    learning based unrolling network for mri reconstruction,” in *2023 IEEE 20th International
    Symposium on Biomedical Imaging (ISBI)*.   IEEE, 2023, pp. 1–5.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] C. Wang, R. Zhang, G. Maliakal, S. Ravishankar, 和 B. Wen, “基于深度强化学习的展开网络用于MRI重建，”
    见 *2023 IEEE第20届生物医学成像国际研讨会（ISBI）*。 IEEE，2023年，页码1–5。'
- en: '[84] S. Kong, W. Wang, X. Feng, and X. Jia, “Deep red unfolding network for
    image restoration,” *IEEE Transactions on Image Processing*, vol. 31, pp. 852–867,
    2021.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S. Kong, W. Wang, X. Feng, 和 X. Jia, “用于图像修复的深度红色展开网络，” *IEEE图像处理汇刊*，第31卷，页码852–867，2021年。'
- en: '[85] C. Mou, Q. Wang, and J. Zhang, “Deep generalized unfolding networks for
    image restoration,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 17 399–17 410.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] C. Mou, Q. Wang, 和 J. Zhang, “用于图像修复的深度广义展开网络，” 见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页码17 399–17 410。'
- en: '[86] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio, “生成对抗网络，” 见 *神经信息处理系统进展*，2014年，页码2672–2680。'
- en: '[87] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image
    restoration with neural networks,” *IEEE Transactions on computational imaging*,
    vol. 3, no. 1, pp. 47–57, 2016.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. Zhao, O. Gallo, I. Frosio, 和 J. Kautz, “用于图像修复的神经网络损失函数，” *IEEE计算成像汇刊*，第3卷，第1期，页码47–57，2016年。'
- en: '[88] C. Li, C. Guo, L. Han, J. Jiang, M.-M. Cheng, J. Gu, and C. C. Loy, “Low-light
    image and video enhancement using deep learning: A survey,” *IEEE transactions
    on pattern analysis and machine intelligence*, vol. 44, no. 12, pp. 9396–9416,
    2021.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. Li, C. Guo, L. Han, J. Jiang, M.-M. Cheng, J. Gu, 和 C. C. Loy, “利用深度学习进行低光照图像和视频增强：综述，”
    *IEEE模式分析与机器智能汇刊*，第44卷，第12期，页码9396–9416，2021年。'
- en: '[89] L. Xu, Q. Yan, Y. Xia, and J. Jia, “Structure extraction from texture
    via relative total variation,” *ACM transactions on graphics (TOG)*, vol. 31,
    no. 6, pp. 1–10, 2012.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] L. Xu, Q. Yan, Y. Xia, 和 J. Jia, “通过相对总变差从纹理中提取结构，” *ACM图形学汇刊（TOG）*，第31卷，第6期，页码1–10，2012年。'
- en: '[90] A. Sharma and L.-F. Cheong, “Into the twilight zone: Depth estimation
    using joint structure-stereo optimization,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 103–118.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. Sharma 和 L.-F. Cheong, “进入暮光区：使用联合结构-立体优化进行深度估计，” 见 *欧洲计算机视觉会议（ECCV）论文集*，2018年，页码103–118。'
- en: '[91] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *3rd International Conference on Learning Representations,
    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*, 2015\.
    [Online]. Available: [http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556)'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络，” 见 *第3届国际学习表征会议，ICLR
    2015，美国加州圣地亚哥，2015年5月7-9日，会议记录*，2015年。 [在线]. 可用：[http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556)'
- en: '[92] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE Conference on Computer
    Vision and Pattern Recognition*.   Ieee, 2009, pp. 248–255.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei, “Imagenet：一个大规模层次化图像数据库，”
    见 *2009 IEEE计算机视觉与模式识别会议*。 Ieee，2009年，页码248–255。'
- en: '[93] O. Sidorov, “Conditional gans for multi-illuminant color constancy: Revolution
    or yet another approach?” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops*, 2019, pp. 0–0.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] O. Sidorov, “用于多光源色彩恒常性的条件性GAN：革命还是另一个方法？” 见 *IEEE/CVF计算机视觉与模式识别会议工作坊论文集*，2019年，页码0–0。'
- en: '[94] N. Inoue and T. Yamasaki, “Learning from synthetic shadows for shadow
    detection and removal,” *IEEE Transactions on Circuits and Systems for Video Technology*,
    vol. 31, no. 11, pp. 4187–4197, 2020.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] N. Inoue 和 T. Yamasaki，“基于合成阴影的阴影检测和去除学习，” *IEEE 视频技术电路与系统汇刊*，第 31 卷，第
    11 期，页码 4187–4197，2020 年。'
- en: '[95] T. F. Y. Vicente, L. Hou, C.-P. Yu, M. Hoai, and D. Samaras, “Large-scale
    training of shadow detectors with noisily-annotated shadow examples,” in *Computer
    Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October
    11-14, 2016, Proceedings, Part VI 14*.   Springer, 2016, pp. 816–832.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] T. F. Y. Vicente, L. Hou, C.-P. Yu, M. Hoai 和 D. Samaras，“使用噪声标注的阴影样本的大规模阴影检测器训练，”
    见于 *计算机视觉–ECCV 2016: 第 14 届欧洲会议，荷兰阿姆斯特丹，2016 年 10 月 11-14 日，论文集，第 VI 部分*。 Springer，2016
    年，页码 816–832。'
- en: '[96] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, and J. Jia, “Underexposed
    photo enhancement using deep illumination estimation,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019, pp. 6849–6857.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng 和 J. Jia，“使用深度照明估计的欠曝光照片增强，”
    见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2019 年，页码 6849–6857。'
- en: '[97] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Z. Wang, A. C. Bovik, H. R. Sheikh 和 E. P. Simoncelli，“图像质量评估：从错误可见性到结构相似性，”
    *IEEE 图像处理汇刊*，第 13 卷，第 4 期，页码 600–612，2004 年。'
- en: '[98] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2018, pp. 586–595.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] R. Zhang, P. Isola, A. A. Efros, E. Shechtman 和 O. Wang，“深度特征作为感知度量的非凡效果，”
    见于 *IEEE 计算机视觉与模式识别会议论文集*，2018 年，页码 586–595。'
- en: '[99] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely blind”
    image quality analyzer,” *IEEE Signal processing letters*, vol. 20, no. 3, pp.
    209–212, 2012.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Mittal, R. Soundararajan 和 A. C. Bovik，“制作一个‘完全盲’的图像质量分析器，” *IEEE 信号处理快报*，第
    20 卷，第 3 期，页码 209–212，2012 年。'
- en: '[100] Y. Hong, L. Niu, and J. Zhang, “Shadow generation for composite image
    in real-world scenes,” in *Proceedings of the AAAI conference on artificial intelligence*,
    vol. 36, no. 1, 2022, pp. 914–922.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Y. Hong, L. Niu 和 J. Zhang，“在现实场景中生成复合图像阴影，” 见于 *AAAI 人工智能会议论文集*，第 36
    卷，第 1 期，2022 年，页码 914–922。'
- en: '[101] X. Tao, J. Cao, Y. Hong, and L. Niu, “Shadow generation with decomposed
    mask prediction and attentive shadow filling,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 38, no. 6, 2024, pp. 5198–5206.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Tao, J. Cao, Y. Hong 和 L. Niu，“基于分解的掩膜预测和关注阴影填充的阴影生成，” 见于 *AAAI 人工智能会议论文集*，第
    38 卷，第 6 期，2024 年，页码 5198–5206。'
- en: '[102] Y. Zhong, X. Liu, D. Zhai, J. Jiang, and X. Ji, “Shadows can be dangerous:
    Stealthy and effective physical-world adversarial attack by natural phenomenon,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 15 345–15 354.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Y. Zhong, X. Liu, D. Zhai, J. Jiang 和 X. Ji，“阴影可能是危险的：自然现象引发的隐蔽且有效的物理世界对抗攻击，”
    见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022 年，页码 15 345–15 354。'
- en: '[103] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
    deep learning models resistant to adversarial attacks,” in *International Conference
    on Learning Representations*, 2018.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] A. Madry, A. Makelov, L. Schmidt, D. Tsipras 和 A. Vladu，“朝着抗对抗攻击的深度学习模型发展，”
    见于 *国际学习表示会议*，2018 年。'
- en: '[104] C. Wang, Y. Yu, L. Guo, and B. Wen, “Benchmarking adversarial robustness
    of image shadow removal with shadow-adaptive attacks,” in *ICASSP 2024-2024 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE,
    2024, pp. 13 126–13 130.'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] C. Wang, Y. Yu, L. Guo 和 B. Wen，“利用阴影适应攻击对图像阴影去除的对抗鲁棒性进行基准测试，” 见于 *ICASSP
    2024-2024 IEEE 国际声学、语音与信号处理会议（ICASSP）*。 IEEE，2024 年，页码 13 126–13 130。'
- en: '[105] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer,
    U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier *et al.*, “Chatgpt for good?
    on opportunities and challenges of large language models for education,” *Learning
    and individual differences*, vol. 103, p. 102274, 2023.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer,
    U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier *等*，“Chatgpt 为善？关于大语言模型在教育中机会与挑战的探讨，”
    *学习与个体差异*，第 103 卷，页码 102274，2023 年。'
- en: '[106] S. Diffusion, “Stable diffusion 2-1 base,” [https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt),
    2022.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] S. Diffusion，“Stable diffusion 2-1 base，” [https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt)，2022年。'
- en: '[107] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller,
    J. Penna, and R. Rombach, “SDXL: Improving latent diffusion models for high-resolution
    image synthesis,” in *The Twelfth International Conference on Learning Representations*.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller,
    J. Penna, 和 R. Rombach，“SDXL：改进高分辨率图像合成的潜在扩散模型，”收录于*第十二届国际学习表征会议*。'
- en: '[108] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo *et al.*, “Segment anything,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2023, pp. 4015–4026.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T.
    Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo *等*，“Segment anything，”收录于*IEEE/CVF国际计算机视觉会议论文集*，2023年，页4015–4026。'
