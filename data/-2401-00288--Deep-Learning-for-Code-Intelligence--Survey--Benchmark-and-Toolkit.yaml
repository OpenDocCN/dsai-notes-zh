- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:35:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:35:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2401.00288] Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2401.00288] 深度学习与代码智能：综述、基准测试和工具包'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.00288](https://ar5iv.labs.arxiv.org/html/2401.00288)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.00288](https://ar5iv.labs.arxiv.org/html/2401.00288)
- en: \newcites
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newcites
- en: secondaryAPPENDIX REFERENCES
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 次要附录参考文献
- en: 'Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与代码智能：综述、基准测试和工具包
- en: Yao Wan [wanyao@hust.edu.cn](mailto:wanyao@hust.edu.cn) National Engineering
    Research Center for Big Data Technology and System, Services Computing Technology
    and System Lab, Cluster and Grid Computing Lab, School of Computer Science and
    TechnologyHuazhong University of Science and TechnologyWuhanChina ,  Yang He Simon
    Fraser UniversityVancouverCanada [yha244@sfu.ca](mailto:yha244@sfu.ca) ,  Zhangqian
    Bi School of Computer Science and TechnologyHuazhong University of Science and
    TechnologyWuhanChina [zqbi@hust.edu.cn](mailto:zqbi@hust.edu.cn) ,  Jianguo Zhang
    Salesforce ResearchUSA [jianguozhang@salesforce.com](mailto:jianguozhang@salesforce.com)
    ,  Hongyu Zhang Chongqing UniversityChina [hyzhang@cqu.edu.cn](mailto:hyzhang@cqu.edu.cn)
    ,  Yulei Sui University of New South WalesAustralia [y.sui@unsw.edu.au](mailto:y.sui@unsw.edu.au)
    ,  Guandong Xu University of Technology SydneyAustralia [guandong.xu@uts.edu.au](mailto:guandong.xu@uts.edu.au)
    ,  Hai Jin [hjin@hust.edu.cn](mailto:hjin@hust.edu.cn) National Engineering Research
    Center for Big Data Technology and System, Services Computing Technology and System
    Lab, Cluster and Grid Computing Lab, School of Computer Science and TechnologyHuazhong
    University of Science and TechnologyWuhanChina  and  Philip S. Yu University of
    Illinois at ChicagoChicagoUSA [psyu@uic.edu](mailto:psyu@uic.edu)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 姚婉 [wanyao@hust.edu.cn](mailto:wanyao@hust.edu.cn) 华中科技大学国家大数据技术与系统工程研究中心、服务计算技术与系统实验室、集群与网格计算实验室，计算机科学与技术学院武汉中国，杨赫
    西蒙弗雷泽大学温哥华加拿大 [yha244@sfu.ca](mailto:yha244@sfu.ca)，张千毕 华中科技大学计算机科学与技术学院武汉中国 [zqbi@hust.edu.cn](mailto:zqbi@hust.edu.cn)，姜国章
    Salesforce Research美国 [jianguozhang@salesforce.com](mailto:jianguozhang@salesforce.com)，洪宇
    张 重庆大学中国 [hyzhang@cqu.edu.cn](mailto:hyzhang@cqu.edu.cn)，隋雨磊 新南威尔士大学澳大利亚 [y.sui@unsw.edu.au](mailto:y.sui@unsw.edu.au)，徐光东
    迪肯大学澳大利亚 [guandong.xu@uts.edu.au](mailto:guandong.xu@uts.edu.au)，金海 [hjin@hust.edu.cn](mailto:hjin@hust.edu.cn)
    华中科技大学国家大数据技术与系统工程研究中心、服务计算技术与系统实验室、集群与网格计算实验室，计算机科学与技术学院武汉中国 以及 菲利普·S·于 伊利诺伊大学芝加哥分校芝加哥美国
    [psyu@uic.edu](mailto:psyu@uic.edu)
- en: Abstract.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Code intelligence leverages machine learning techniques to extract knowledge
    from extensive code corpora, with the aim of developing intelligent tools to improve
    the quality and productivity of computer programming. Currently, there is already
    a thriving research community focusing on code intelligence, with efforts ranging
    from software engineering, machine learning, data mining, natural language processing,
    and programming languages. In this paper, we conduct a comprehensive literature
    review on deep learning for code intelligence, from the aspects of code representation
    learning, deep learning techniques, and application tasks. We also benchmark several
    state-of-the-art neural models for code intelligence, and provide an open-source
    toolkit tailored for the rapid prototyping of deep-learning-based code intelligence
    models. In particular, we inspect the existing code intelligence models under
    the basis of code representation learning, and provide a comprehensive overview
    to enhance comprehension of the present state of code intelligence. Furthermore,
    we publicly release the source code and data resources to provide the community
    with a ready-to-use benchmark, which can facilitate the evaluation and comparison
    of existing and future code intelligence models ([https://xcodemind.github.io](https://xcodemind.github.io)).
    At last, we also point out several challenging and promising directions for future
    research.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 代码智能利用机器学习技术从大量代码库中提取知识，旨在开发智能工具以提高计算机编程的质量和生产力。目前，已经有一个蓬勃发展的研究社区专注于代码智能，研究领域涵盖了软件工程、机器学习、数据挖掘、自然语言处理和编程语言等。在本文中，我们从代码表示学习、深度学习技术和应用任务等方面对深度学习在代码智能中的应用进行了全面的文献综述。我们还对几种最先进的神经网络模型进行了基准测试，并提供了一个开源工具包，旨在快速原型化基于深度学习的代码智能模型。特别是，我们基于代码表示学习检查了现有的代码智能模型，并提供了全面的概述，以增强对当前代码智能状态的理解。此外，我们公开发布了源代码和数据资源，为社区提供了一个现成的基准，这可以促进现有和未来代码智能模型的评估和比较（[https://xcodemind.github.io](https://xcodemind.github.io)）。最后，我们还指出了几个具有挑战性和前景的未来研究方向。
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'Software is eating the world (Andreessen, [2011](#bib.bib15)). With the advancement
    of Artificial Intelligence (AI), it is time to expand that maxim: software ate
    the world, and AI is eating the software. As the software is primarily composed
    of code, we define the emerging concept of code intelligence as the application
    of machine learning techniques to extract knowledge from large-scale code repositories,
    with the aim of developing intelligent tools to improve the quality and productivity
    of computer programming (Lu et al., [2021](#bib.bib160)). This concept is fueled
    by the ever-expanding reservoir of source code, often referred to as “Big Code” (Allamanis
    et al., [2018a](#bib.bib7)), which is harvested from platforms such as GitHub (git,
    [2019](#bib.bib2)) and StackOverflow (sta, [2019](#bib.bib3)). In this paper,
    our research scope is confined to code intelligence, with a particular focus on
    the application of deep learning techniques.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 软件正在吞噬世界（Andreessen，[2011](#bib.bib15)）。随着人工智能（AI）的进步，是时候扩展这一格言了：软件已经吞噬了世界，而AI正在吞噬软件。由于软件主要由代码组成，我们定义了新兴的代码智能概念，即应用机器学习技术从大规模代码库中提取知识，旨在开发智能工具以提高计算机编程的质量和生产力（Lu
    et al., [2021](#bib.bib160)）。这一概念得到了不断扩展的源代码库的推动，这些源代码库通常被称为“大代码”（Allamanis et
    al., [2018a](#bib.bib7)），它们来自于GitHub（git，[2019](#bib.bib2)）和StackOverflow（sta，[2019](#bib.bib3)）等平台。本文的研究范围限定在代码智能上，特别关注深度学习技术的应用。
- en: Achieving code intelligence necessitates a collaborative synergy in research
    across the domains of software engineering, machine learning, Natural Language
    Processing (NLP), programming language, and security. From our investigation,
    precise and reliable code representation learning or code embedding, which aims
    to efficiently and effectively encode the semantics of source code into distributed
    vector representations, is the foundation for code intelligence. Such embedding
    vectors are then used in many downstream tasks, such as code completion (Raychev
    et al., [2014](#bib.bib198); Svyatkovskiy et al., [2019](#bib.bib223); Kim et al.,
    [2021](#bib.bib118); Liu et al., [2020b](#bib.bib150)), code search (Gu et al.,
    [2018](#bib.bib78); Wan et al., [2019](#bib.bib235); Husain et al., [2019](#bib.bib107)),
    code summarization (Allamanis et al., [2016](#bib.bib11); Iyer et al., [2016](#bib.bib108);
    Wan et al., [2018](#bib.bib238); Hu et al., [2018b](#bib.bib104); Zhang et al.,
    [2020c](#bib.bib285)), type inference (Hellendoorn et al., [2018](#bib.bib99);
    Wei et al., [2020a](#bib.bib254); Pradel et al., [2020](#bib.bib189); Allamanis
    et al., [2020](#bib.bib8)), etc. In terms of code embedding, notable advancements
    have been achieved by employing deep learning and NLP techniques to encode source
    code.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实现代码智能需要软件工程、机器学习、自然语言处理（NLP）、编程语言和安全等领域的协同研究。从我们的调查中可以看出，精确可靠的代码表示学习或代码嵌入，旨在将源代码的语义高效有效地编码成分布式向量表示，是代码智能的基础。这些嵌入向量随后被用于许多下游任务，如代码补全 (Raychev
    et al., [2014](#bib.bib198); Svyatkovskiy et al., [2019](#bib.bib223); Kim et al.,
    [2021](#bib.bib118); Liu et al., [2020b](#bib.bib150))、代码搜索 (Gu et al., [2018](#bib.bib78);
    Wan et al., [2019](#bib.bib235); Husain et al., [2019](#bib.bib107))、代码总结 (Allamanis
    et al., [2016](#bib.bib11); Iyer et al., [2016](#bib.bib108); Wan et al., [2018](#bib.bib238);
    Hu et al., [2018b](#bib.bib104); Zhang et al., [2020c](#bib.bib285))、类型推断 (Hellendoorn
    et al., [2018](#bib.bib99); Wei et al., [2020a](#bib.bib254); Pradel et al., [2020](#bib.bib189);
    Allamanis et al., [2020](#bib.bib8))等。在代码嵌入方面，通过深度学习和NLP技术对源代码进行编码，已取得了显著进展。
- en: Analogous to word2vec (Mikolov et al., [2013](#bib.bib167)) in NLP, Alon et al.
    ([2019](#bib.bib14)) proposed code2vec, a distributed representation of code,
    based on a collection of paths extracted from the Abstract Syntax Tree (AST) of
    code. In recent years, a multitude of neural networks tailored for specific tasks
    have been proposed and trained using supervised methods. As pre-trained language
    models (e.g., BERT (Devlin et al., [2019](#bib.bib61)) and GPT-3 (Brown et al.,
    [2020](#bib.bib27))) have been widely applied to NLP, many pre-trained language
    models for code have been proposed (Kanade et al., [2020](#bib.bib116); Feng et al.,
    [2020](#bib.bib67); Guo et al., [2021](#bib.bib84)) to better represent the semantics
    of code. More recently, the emergence of Large Language Models (LLMs), exemplified
    by ChatGPT, has illuminated the pathway for further advancement of pre-trained
    language models, with a notable trend of increasing model sizes. This trend has
    extended to the domain of code intelligence, resulting in the development of various
    LLMs tailored for code, including but not limited to CodeT5 (Wang et al., [2023](#bib.bib247)),
    StarCoder (Li et al., [2023a](#bib.bib131)), and Code Llama (Roziere et al., [2023](#bib.bib199)).
    In this paper, we examine code intelligence through the lenses of code representation
    learning, deep learning methods, and their applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于NLP中的word2vec (Mikolov et al., [2013](#bib.bib167))，Alon et al. ([2019](#bib.bib14))
    提出了code2vec，一种基于从代码的抽象语法树（AST）中提取的路径集合的代码分布表示。近年来，针对特定任务的众多神经网络已经被提出并通过监督方法进行训练。由于预训练语言模型（如BERT (Devlin
    et al., [2019](#bib.bib61)) 和 GPT-3 (Brown et al., [2020](#bib.bib27))) 已广泛应用于NLP，许多针对代码的预训练语言模型也已被提出 (Kanade
    et al., [2020](#bib.bib116); Feng et al., [2020](#bib.bib67); Guo et al., [2021](#bib.bib84))，以更好地表示代码的语义。最近，大型语言模型（LLMs）的出现，如ChatGPT，照亮了预训练语言模型进一步发展的路径，模型规模逐渐增大的趋势尤为明显。这一趋势也扩展到了代码智能领域，导致了针对代码的各种LLM的开发，包括但不限于CodeT5 (Wang
    et al., [2023](#bib.bib247))、StarCoder (Li et al., [2023a](#bib.bib131))和Code
    Llama (Roziere et al., [2023](#bib.bib199))。在本文中，我们通过代码表示学习、深度学习方法及其应用的视角来审视代码智能。
- en: Table 1\. Comparison of the current work with previous survey efforts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 当前工作与之前调查工作的比较
- en: '| Paper | Artifact | Technique | Survey | Benchmark | Toolkit |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 产物 | 技术 | 调查 | 基准 | 工具包 |'
- en: '| Allamanis et al. ([2018a](#bib.bib7)) | Software | Machine Learning | ✓ |
    $\times$ | $\times$ |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| Allamanis et al. ([2018a](#bib.bib7)) | 软件 | 机器学习 | ✓ | $\times$ | $\times$
    |'
- en: '| Watson et al. ([2020](#bib.bib250)) | Software | Deep Learning | ✓ | $\times$
    | $\times$ |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Watson 等人 ([2020](#bib.bib250)) | 软件 | 深度学习 | ✓ | $\times$ | $\times$ |'
- en: '| Wang et al. ([2020a](#bib.bib242)) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([2020a](#bib.bib242)) |'
- en: '| Yang et al. ([2022c](#bib.bib270)) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等人 ([2022c](#bib.bib270)) |'
- en: '| Devanbu et al. ([2020](#bib.bib60)) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Devanbu 等人 ([2020](#bib.bib60)) |'
- en: '| Lu et al. ([2021](#bib.bib160)) | Software | Deep Learning | $\times$ | ✓
    | $\times$ |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Lu 等人 ([2021](#bib.bib160)) | 软件 | 深度学习 | $\times$ | ✓ | $\times$ |'
- en: '| Ours | Code | Deep Learning | ✓ | ✓ | ✓ |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 代码 | 深度学习 | ✓ | ✓ | ✓ |'
- en: Related Surveys and Differences. Within our literature review, we identified
    several surveys related to ours. Notably, Allamanis et al. ([2018a](#bib.bib7))
    conducted an exhaustive examination of machine learning approaches for modeling
    the naturalness of programming language. They primarily emphasize machine learning
    algorithms, with a specific focus on probabilistic models, as opposed to those
    based on deep learning. Recently, Watson et al. ([2020](#bib.bib250)), Wang et al.
    ([2020a](#bib.bib242)) and Yang et al. ([2022c](#bib.bib270)) conducted a thorough
    review of the literature on applications of deep learning in software engineering
    research. They investigated mostly software engineering and artificial intelligence
    conferences and journals, focusing on various software engineering tasks (not
    limited to the source code) that are based on deep learning. (Devanbu et al.,
    [2020](#bib.bib60)) is a report that summarizes the current status of research
    on the subject of the intersection between deep learning and software engineering,
    as well as suggests several future directions. In (Lu et al., [2021](#bib.bib160)),
    the authors established a benchmark dataset called CodeXGLUE for code representation
    and generation. In addition, several benchmark results especially based on pre-trained
    language models (i.e., CodeBERT) are presented.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相关调查与差异。在我们的文献综述中，我们识别出了一些与我们相关的调查。特别是，Allamanis 等人 ([2018a](#bib.bib7)) 对用于建模编程语言自然性的机器学习方法进行了详尽的检查。他们主要强调机器学习算法，尤其关注概率模型，而非基于深度学习的方法。最近，Watson
    等人 ([2020](#bib.bib250))、Wang 等人 ([2020a](#bib.bib242)) 和 Yang 等人 ([2022c](#bib.bib270))
    对深度学习在软件工程研究中的应用进行了全面回顾。他们主要研究了软件工程和人工智能的会议和期刊，关注于基于深度学习的各种软件工程任务（不限于源代码）。(Devanbu
    等人， [2020](#bib.bib60)) 是一份总结了深度学习与软件工程交叉领域研究现状的报告，并提出了若干未来方向。在 (Lu 等人， [2021](#bib.bib160))
    中，作者建立了一个名为 CodeXGLUE 的基准数据集，用于代码表示和生成。此外，还展示了几个特别基于预训练语言模型（即 CodeBERT）的基准结果。
- en: 'Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") summarizes the differences between our paper when
    compared with several related surveys in code intelligence. In contrast to (Allamanis
    et al., [2018a](#bib.bib7)) that focuses on traditional machine learning approaches,
    this paper places greater emphasis on leveraging deep learning techniques for
    code intelligence. In contrast to (Watson et al., [2020](#bib.bib250)), (Wang
    et al., [2020a](#bib.bib242)), (Yang et al., [2022c](#bib.bib270)), and (Devanbu
    et al., [2020](#bib.bib60)) that cover various tasks in broad software engineering,
    our study narrows its focus to tasks associated with source code, examining them
    specifically from the perspective of deep learning. In addition, we survey papers
    from various fields including software engineering, programming languages, machine
    learning, NLP, and security. Note that, as code intelligence based on deep learning
    is an emerging and active research topic, we also include several high-quality
    unpublished papers that have been made available on arXiv. This is because these
    unpublished works in arXiv can be seen as an indicator of future research. Furthermore,
    existing surveys do not provide comprehensive benchmark evaluation results, nor
    do they develop an open-source toolkit to facilitate further research. This paper
    addresses this gap by presenting an open-source toolkit, referred to as NaturalCC
    (standards for Natural Code Comprehension) (Wan et al., [2022a](#bib.bib234)).
    The toolkit is designed to streamline the prototyping of code intelligence models
    and to serve as a benchmarking platform for evaluating various state-of-the-art
    models. In complement to CodeXGLUE (Lu et al., [2021](#bib.bib160)), which aims
    to establish a benchmark dataset for code understanding and generation, particularly
    leveraging pre-trained code models, our focus lies in the construction of infrastructures
    that support diverse model implementations and provide users with the ability
    to conduct rapid prototyping. Compared to CodeXGLUE, our toolkit contains a more
    extensive array of tools designed for the entire pipeline involved in constructing
    code intelligence models, offering heightened flexibility.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit")总结了我们论文与几项相关的代码智能调查之间的差异。与侧重于传统机器学习方法的(Allamanis
    et al., [2018a](#bib.bib7))相比，本文更强调利用深度学习技术进行代码智能研究。与涵盖广泛软件工程任务的(Watson et al.,
    [2020](#bib.bib250)), (Wang et al., [2020a](#bib.bib242)), (Yang et al., [2022c](#bib.bib270)),
    和 (Devanbu et al., [2020](#bib.bib60))相比，我们的研究将焦点缩小到与源代码相关的任务，特别从深度学习的角度进行考察。此外，我们还调查了来自软件工程、编程语言、机器学习、自然语言处理和安全等领域的文献。请注意，由于基于深度学习的代码智能是一个新兴的活跃研究领域，我们还包括了几篇在arXiv上发布的高质量未发表论文。这是因为这些未发表的arXiv论文可以被视为未来研究的一个指示。此外，现有的调查没有提供全面的基准评估结果，也没有开发开源工具包以促进进一步研究。本文通过介绍一个名为NaturalCC（自然代码理解标准）(Wan
    et al., [2022a](#bib.bib234))的开源工具包来填补这一空白。该工具包旨在简化代码智能模型的原型设计，并作为评估各种先进模型的基准平台。与旨在建立代码理解和生成基准数据集的CodeXGLUE
    (Lu et al., [2021](#bib.bib160))不同，我们的重点在于构建支持多种模型实现的基础设施，并为用户提供快速原型设计的能力。与CodeXGLUE相比，我们的工具包包含更广泛的工具，旨在支持构建代码智能模型的整个流程，提供更高的灵活性。'
- en: Our Contributions. This paper is targeted at researchers and practitioners intrigued
    by the convergence of code intelligence and deep learning, with a specific emphasis
    on intelligent software engineering, NLP, and programming languages. In this paper,
    we begin by providing a thorough review of existing research on deep learning
    for code intelligence. Subsequently, we advance our contribution by developing
    an open-source toolkit, referred to as NaturalCC, that incorporates state-of-the-art
    models across various downstream tasks. Employing NaturalCC, we conduct a comprehensive
    performance benchmark of each model across 4 downstream tasks, including code
    summarization, code search, code completion, and type inference. The major contributions
    of this paper are summarized as follows.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献。本论文针对对代码智能和深度学习的融合感兴趣的研究人员和从业者，特别强调智能软件工程、NLP 和编程语言。本论文首先对现有的深度学习在代码智能方面的研究进行了详细回顾。随后，我们通过开发一个名为
    NaturalCC 的开源工具包，推动了我们的贡献，该工具包包含了各种下游任务的最先进模型。通过使用 NaturalCC，我们对每个模型在包括代码摘要、代码搜索、代码补全和类型推断在内的
    4 个下游任务上进行了全面的性能基准测试。本文的主要贡献总结如下。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct a comprehensive review on deep learning for code intelligence. Specifically,
    we have collected 269 papers from various top-tier venues and arXiv, covering
    multiple domains including software engineering, artificial intelligence, NLP,
    programming languages, and security.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对深度学习在代码智能方面的研究进行了全面回顾。具体而言，我们从多个顶级会议和 arXiv 上收集了 269 篇论文，涵盖了包括软件工程、人工智能、NLP、编程语言和安全在内的多个领域。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We benchmark the performance of 13 leading models across four different tasks
    (i.e., code summarization, code search, code completion, and type inference).
    All the resources, datasets and source code are publicly available.¹¹1[http://xcodemind.github.io](http://xcodemind.github.io)
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对 13 个领先模型在四个不同任务（即代码摘要、代码搜索、代码补全和类型推断）上的性能进行了基准测试。所有资源、数据集和源代码都可以公开获取。¹¹1[http://xcodemind.github.io](http://xcodemind.github.io)
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce NaturalCC, an open-source toolkit that has integrated many state-of-the-art
    baselines on different tasks, in order to facilitate research on code intelligence.
    Researchers in the fields of software engineering, NLP, and other fields can benefit
    from the toolkit for quick prototyping and replication.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了 NaturalCC，这是一款开源工具包，集成了许多不同任务上的最先进基准，以促进代码智能研究。软件工程、自然语言处理（NLP）及其他领域的研究人员可以利用该工具包快速原型设计和复制实验。
- en: 2\. Survey Methodology
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 调查方法
- en: 2.1\. A Unified View from Code Representation Learning
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 从代码表示学习的统一视角
- en: '![Refer to caption](img/81860a9045e46ac2b6dc8c5dd6881057.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/81860a9045e46ac2b6dc8c5dd6881057.png)'
- en: Figure 1. Code intelligence tasks based on code representation learning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 基于代码表示学习的代码智能任务。
- en: 'We propose to summarize existing deep-learning-based approaches to code intelligence
    from the lens of code representation learning in this paper. As shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2.1\. A Unified View from Code Representation Learning ‣ 2\. Survey
    Methodology ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit"),
    for code representation learning, researchers first extract features that potentially
    describe the semantics of code, and then design various neural networks to encode
    them into distributed vectors. Code representation learning can be viewed as the
    foundation for different downstream applications. Based on the characteristics
    of each application, the downstream applications can be divided into three groups:
    (1) Classification-based. In these tasks (e.g., code classification, vulnerability
    detection, and type inference), a classifier layer (e.g., softmax) is used to
    map the code embeddings to labels/classes. (2) Similarity-based. In these tasks
    (e.g., code search and code clone detection), Siamese neural network structure (Chicco,
    [2021](#bib.bib48)) is often adopted, where dual encoders are used to encode the
    source code and natural-language query into embedding vectors. Based on the two
    embeddings of code and query, a constraint (such as a triplet loss function) is
    always used to regularize the similarity between them. Note that, in several approaches
    to code search and code clone detection, the two embeddings of code and query
    are also concatenated, and the task is reformulated as a classification task to
    determine whether the code and query are related (Feng et al., [2020](#bib.bib67)).
    (3) Generation-based. In these tasks (e.g., code completion, code summarization,
    program translation, program synthesis, and program repair), the objective is
    to generate source code, natural language descriptions, or programs in another
    programming language from a given code snippet. These tasks usually follow the
    encoder-decoder paradigm, where an encoder network is used to represent the semantics
    of code, and a decoder network (e.g., RNN) is designed to generate sequences,
    e.g., natural-language descriptions or source code. Additionally, we categorize
    the learning paradigms into four groups: supervised learning, unsupervised learning,
    self-supervised learning, and reinforcement learning.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '本文提议从代码表示学习的角度总结现有的基于深度学习的代码智能方法。如图[1](#S2.F1 "Figure 1 ‣ 2.1\. A Unified View
    from Code Representation Learning ‣ 2\. Survey Methodology ‣ Deep Learning for
    Code Intelligence: Survey, Benchmark and Toolkit")所示，对于代码表示学习，研究人员首先提取可能描述代码语义的特征，然后设计各种神经网络将这些特征编码成分布式向量。代码表示学习可以被视为不同下游应用的基础。根据每个应用的特点，下游应用可以分为三类：（1）基于分类的。在这些任务中（例如代码分类、漏洞检测和类型推断），使用分类器层（例如softmax）将代码嵌入映射到标签/类别上。（2）基于相似性的。在这些任务中（例如代码搜索和代码克隆检测），通常采用Siamese神经网络结构（Chicco，[2021](#bib.bib48)），其中使用双编码器将源代码和自然语言查询编码成嵌入向量。根据代码和查询的两个嵌入，通常使用一个约束（例如三元组损失函数）来规范它们之间的相似性。需要注意的是，在一些代码搜索和代码克隆检测方法中，代码和查询的两个嵌入也会被拼接起来，任务被重新表述为分类任务，以确定代码和查询是否相关（Feng
    et al., [2020](#bib.bib67)）。（3）基于生成的。在这些任务中（例如代码补全、代码摘要、程序翻译、程序合成和程序修复），目标是从给定的代码片段生成源代码、自然语言描述或其他编程语言的程序。这些任务通常遵循编码器-解码器范式，其中使用编码器网络来表示代码的语义，设计解码器网络（例如RNN）来生成序列，例如自然语言描述或源代码。此外，我们将学习范式分为四类：监督学习、无监督学习、自监督学习和强化学习。'
- en: 2.2\. Paper Selection
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 论文选择
- en: Deep learning for code intelligence has been studied in many related research
    communities. In this paper, we review high-quality papers selected from top-tier
    conferences and journals, ranging from software engineering, programming languages,
    NLP, and artificial intelligence, to security. Overall, we have identified 32
    publication venues, as shown in the Supplementary Materials. We first manually
    check the publication list of the venues and obtain an initial collection of papers.
    Particularly, we systematically query the aforementioned venue names within the
    DBLP database²²2[https://dblp.uni-trier.de](https://dblp.uni-trier.de) and examine
    the associated proceedings. Subsequently, two authors, both possessing over five
    years of expertise in deep learning for code intelligence, collaboratively undertake
    the task of manually refining the results. This involves meticulous scrutiny of
    titles and a brief review of abstracts to identify and filter out papers that
    are potentially relevant to code intelligence. For those large conferences (e.g.,
    AAAI and IJCAI) that accept thousands of papers per year, we first filter out
    those papers whose titles contain the keywords of “code” or “program”, and then
    manually check them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 代码智能的深度学习在许多相关研究社区中都有研究。在本文中，我们回顾了从顶级会议和期刊中筛选出的高质量论文，涵盖软件工程、编程语言、自然语言处理、人工智能和安全等领域。总体而言，我们确定了32个出版场所，如补充材料所示。我们首先手动检查这些场所的出版列表，并获得初步的论文集合。特别是，我们在DBLP数据库中系统地查询前述场所的名称²²2[https://dblp.uni-trier.de](https://dblp.uni-trier.de)，并检查相关的会议记录。随后，两位在代码智能深度学习领域拥有超过五年经验的作者，协作进行手动细化。这包括对标题的细致审查和对摘要的简要回顾，以识别和筛选出可能与代码智能相关的论文。对于那些每年接受数千篇论文的大型会议（例如AAAI和IJCAI），我们首先筛选出标题中包含“code”或“program”关键词的论文，然后进行人工检查。
- en: 'Based on this initial collection of papers, we start to augment it through
    keyword searching. We systematically search DBLP and Google Scholar using the
    following keywords: “code representation”, “program comprehension”, “code embedding”,
    “code classification”, “vulnerability detection”, “bug finding”, “code completion”,
    “type inference”, “code search/retrieval”, “code clone detection”, “code summarization”,
    “program translation”, “program synthesis”, and “program repair”, with a combination
    of “deep”, “learning”, “neural”, and “network”.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些初步论文集合，我们开始通过关键词搜索来扩充它。我们使用以下关键词系统地搜索DBLP和Google Scholar：“code representation”，“program
    comprehension”，“code embedding”，“code classification”，“vulnerability detection”，“bug
    finding”，“code completion”，“type inference”，“code search/retrieval”，“code clone
    detection”，“code summarization”，“program translation”，“program synthesis”和“program
    repair”，以及“deep”，“learning”，“neural”和“network”的组合。
- en: 'It is worth noting that, in addition to accepted papers from the aforementioned
    venues, we also consider some recent publications from the pre-print archive,
    as they reflect the most current research outputs. We choose publications from
    arXiv based on two criteria: paper quality, author reputation. The quality of
    a pre-printed paper can be assessed based on the number of citations it has garnered
    in recent months. The reputations of authors can be indicated by their Google
    Scholar citations. If a paper satisfies either of these selection criteria, we
    include it for consideration. Having obtained this collection of papers, we then
    filter out the irrelevant papers by manual checking. Finally, we obtained a collection
    of 269 papers. To ensure transparency and accessibility, a comprehensive table
    of the surveyed papers and the source of papers is maintained online.³³3[https://github.com/CGCL-codes/awesome-code-intelligence](https://github.com/CGCL-codes/awesome-code-intelligence)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，除了前面提到的会议接受的论文，我们还考虑了一些来自预印本档案的近期出版物，因为它们反映了最新的研究成果。我们根据两个标准选择arXiv上的出版物：论文质量和作者声誉。预印本论文的质量可以通过最近几个月的引用次数来评估。作者的声誉可以通过他们的Google
    Scholar引用量来指示。如果论文满足其中一个选择标准，我们就将其纳入考虑范围。获取这批论文后，我们通过人工检查筛选出不相关的论文。最终，我们得到了269篇论文的集合。为了确保透明性和可访问性，我们在网上维护了一张全面的调查论文表格和论文来源。³³3[https://github.com/CGCL-codes/awesome-code-intelligence](https://github.com/CGCL-codes/awesome-code-intelligence)
- en: 2.3\. Publication Trends of Code Intelligence
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 代码智能的出版趋势
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.3\. Publication Trends of Code Intelligence
    ‣ 2\. Survey Methodology ‣ Deep Learning for Code Intelligence: Survey, Benchmark
    and Toolkit") provides statistics of the surveyed papers to reveal the publication
    trend and research topic trend. Figure [2(a)](#S2.F2.sf1 "In Figure 2 ‣ 2.3\.
    Publication Trends of Code Intelligence ‣ 2\. Survey Methodology ‣ Deep Learning
    for Code Intelligence: Survey, Benchmark and Toolkit") shows the collected papers
    on deep learning for code intelligence, from January 2014 to December 2022. It
    is noteworthy that, 11 papers about LLMs for code intelligence have been included,
    all published in 2023. Each year, we analyze the publication trends across various
    communities and venues, including NLP, artificial intelligence, software engineering,
    programming language, security, preprint, and others. The surveyed venues for
    each community are detailed in Table 1 of the Supplementary Materials. According
    to our statistical findings, the top five most popular publication venues are
    ICSE, ASE, ACL, ICLR, and FSE, from the software engineering, NLP, and AI communities.
    Notably, we can also observe that code intelligence has garnered increasing attention
    from the AI and NLP communities since 2018. Although deep learning was first proposed
    in 2006 (Hinton et al., [2006](#bib.bib101)), it is initially used for source
    code modeling in 2014. From Figure [2(a)](#S2.F2.sf1 "In Figure 2 ‣ 2.3\. Publication
    Trends of Code Intelligence ‣ 2\. Survey Methodology ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit"), we can also see that the number
    of relevant papers for code intelligence has increased significantly since 2018,
    indicating that deep learning has significantly advanced code intelligence research
    since then. This development can be attributed to the widespread use of deep learning
    in NLP since 2018, which has sparked a lot of studies on using NLP methods for
    tasks involving source code.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S2.F2 "图 2 ‣ 2.3\. 代码智能的出版趋势 ‣ 2\. 调查方法 ‣ 代码智能的深度学习：调查、基准和工具包")提供了调查论文的统计数据，以揭示出版趋势和研究主题趋势。图[2(a)](#S2.F2.sf1
    "图 2 ‣ 2.3\. 代码智能的出版趋势 ‣ 2\. 调查方法 ‣ 代码智能的深度学习：调查、基准和工具包")展示了从2014年1月到2022年12月关于代码智能的深度学习的论文。值得注意的是，11篇关于代码智能的LLM的论文被包含在内，均发表于2023年。每年，我们分析各种社区和会议的出版趋势，包括NLP、人工智能、软件工程、编程语言、安全、预印本等。各社区的调查场所详细信息见附录材料的表1。根据我们的统计结果，五个最受欢迎的出版场所是ICSE、ASE、ACL、ICLR和FSE，来自软件工程、NLP和AI社区。特别地，我们还可以观察到，自2018年以来，代码智能受到了AI和NLP社区的越来越多关注。尽管深度学习最初在2006年提出（Hinton等，[2006](#bib.bib101)），但最初用于源代码建模是在2014年。从图[2(a)](#S2.F2.sf1
    "图 2 ‣ 2.3\. 代码智能的出版趋势 ‣ 2\. 调查方法 ‣ 代码智能的深度学习：调查、基准和工具包")，我们还可以看到，自2018年以来，代码智能相关论文的数量显著增加，这表明自那时以来，深度学习在代码智能研究中取得了重大进展。这一发展可以归因于自2018年以来深度学习在NLP中的广泛应用，这激发了大量关于使用NLP方法处理源代码任务的研究。
- en: '][c].42 ![Refer to caption](img/3ca3c2bddfab1355c6772b95a32aa3b6.png)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '][c].42 ![参考说明](img/3ca3c2bddfab1355c6772b95a32aa3b6.png)'
- en: (a) Number of publications in different years.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同年份的出版数量。
- en: '][c].56 ![Refer to caption](img/e004f71258840ebe566afbf156c50cb8.png)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '][c].56 ![参考说明](img/e004f71258840ebe566afbf156c50cb8.png)'
- en: (b) Publication in each application
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 各应用领域的出版情况
- en: Figure 2. Statistics of the surveyed papers to reveal the publication trend
    and research topic trend.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 调查论文的统计数据，以揭示出版趋势和研究主题趋势。
- en: 'Figure [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2.3\. Publication Trends of Code Intelligence
    ‣ 2\. Survey Methodology ‣ Deep Learning for Code Intelligence: Survey, Benchmark
    and Toolkit") shows the distribution of papers across applications, including
    code classification, vulnerability detection, type inference, code search, code
    clone detection, code completion, code summarization, program translation, program
    synthesis, and program repair. This figure shows a burgeoning interest in recent
    years surrounding topics of code summarization, program synthesis, program repair,
    vulnerability detection, and code search.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2(b)](#S2.F2.sf2 "图 2 ‣ 2.3\. 代码智能的出版趋势 ‣ 2\. 调查方法 ‣ 代码智能的深度学习：调查、基准和工具包")展示了各应用领域的论文分布，包括代码分类、漏洞检测、类型推断、代码搜索、代码克隆检测、代码补全、代码总结、程序翻译、程序合成和程序修复。该图显示了近年来对代码总结、程序合成、程序修复、漏洞检测和代码搜索等主题的日益关注。
- en: 3\. Literature Review
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 文献综述
- en: 3.1\. Taxonomy
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 分类
- en: Figure 3. The taxonomy of deep learning for code intelligence.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 深度学习代码智能的分类。
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ 3.1\. Taxonomy ‣ 3\. Literature Review ‣ Deep
    Learning for Code Intelligence: Survey, Benchmark and Toolkit") illustrates the
    taxonomy of current studies on deep learning for code intelligence that we have
    surveyed in this paper. From our observation, the research in this field can be
    broken down into three distinct aspects: i.e., code features, deep learning techniques,
    and applications. (1) Code Features. As the foundation of deep-learning-based
    code intelligence, code representation seeks to represent source code as distributed
    vectors. We categorize the current code representation approaches by the features
    of input code that they use, such as code tokens, Intermediate Representations
    (IRs), Application Programming Interfaces (APIs), Abstract Syntax Trees (ASTs)
    and code graphs (e.g., graphs that illustrate control flow and data flow). (2)
    In the realm of deep learning techniques, we initially delve into various types
    of neural networks, i.e., RNNs, CNNs, Transformers, and GNNs. Subsequently, we
    examine the diverse learning paradigms employed for modeling source code, i.e.,
    supervised learning, unsupervised learning, self-supervised learning, and reinforcement
    learning. (3) We investigate multiple downstream applications that are based on
    code representation and deep learning techniques, including code classification,
    vulnerability detection and bug finding, type inference, code search, code clone
    detection, code completion, code summarization, program translation, program synthesis,
    and program repair.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S3.F3 "Figure 3 ‣ 3.1\. Taxonomy ‣ 3\. Literature Review ‣ Deep Learning
    for Code Intelligence: Survey, Benchmark and Toolkit")展示了我们在本文中调查的关于代码智能深度学习的当前研究分类。从我们的观察来看，该领域的研究可以分为三个不同的方面：即代码特征、深度学习技术和应用。（1）代码特征。作为深度学习代码智能的基础，代码表示旨在将源代码表示为分布式向量。我们根据输入代码所使用的特征对当前的代码表示方法进行分类，例如代码标记、中间表示（IRs）、应用程序编程接口（APIs）、抽象语法树（ASTs）和代码图（例如，控制流和数据流图）。（2）在深度学习技术领域，我们首先深入探讨各种类型的神经网络，即RNNs、CNNs、Transformers和GNNs。随后，我们研究用于建模源代码的多种学习范式，即监督学习、无监督学习、自监督学习和强化学习。（3）我们调查了基于代码表示和深度学习技术的多个下游应用，包括代码分类、漏洞检测和错误查找、类型推断、代码搜索、代码克隆检测、代码补全、代码总结、程序翻译、程序合成和程序修复。'
- en: 3.2\. Code Features
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 代码特征
- en: '![Refer to caption](img/083c580783c3d1da625dab26cb03aa3a.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/083c580783c3d1da625dab26cb03aa3a.png)'
- en: Figure 4. A detailed C code snippet with its corresponding tokens, IR, AST,
    and IR-based ﬂow graphs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 一个详细的C代码片段及其对应的标记、IR、AST和基于IR的流图。
- en: 'To represent source code, we need to first determine what to represent. Numerous
    works have proposed to extract the code features from diverse perspectives, including
    code tokens, IRs, ASTs, and various types of code graphs. Figure [4](#S3.F4 "Figure
    4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") shows a detailed code snippet written in C, with
    its corresponding code tokens, IR, AST, control-flow graph, data-flow graph, code
    property graph, and IR-based flow graphs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '要表示源代码，我们首先需要确定要表示的内容。许多研究提出从不同角度提取代码特征，包括代码标记、IRs、ASTs 和各种类型的代码图。图[4](#S3.F4
    "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit")展示了一个详细的C代码片段及其对应的代码标记、IR、AST、控制流图、数据流图、代码属性图和基于IR的流图。'
- en: 3.2.1\. Code Tokens
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 代码标记
- en: Code tokens, shaping the textual appearance of source code, are composed of
    function name, keywords, and various variable identifiers. These tokens are simple
    yet effective in representing the semantics of programs. The majority of approaches
    for processing code involve breaking the program down into a sequence of tokens
    based on specific delimiters, such as spaces or the capitalization patterns in
    identifiers (for identifiers like SortList and intArray). Cummins et al. ([2017](#bib.bib56))
    introduced a character-level LSTM network to represent the sequence of code characters
    for program synthesis. Since the set of characters to form a program is always
    a limited size, the character-level code representation does not have the problem
    of out-of-vocabulary. However, this tokenization process at the character level
    breaks down the meaning of the original words and also increases the length of
    the code sequence, which can make it challenging to understand the overall semantics
    of the program.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代码标记，塑造源代码的文本外观，由函数名称、关键字和各种变量标识符组成。这些标记简单但有效地表示了程序的语义。处理代码的大多数方法涉及将程序分解为基于特定分隔符（如空格或标识符中的大小写模式（例如
    SortList 和 intArray））的标记序列。Cummins 等人 ([2017](#bib.bib56)) 引入了一种字符级 LSTM 网络来表示程序合成的代码字符序列。由于构成程序的字符集始终是有限的，字符级代码表示没有词汇外的问题。然而，这种字符级的标记化过程会破坏原始词汇的含义，并且增加了代码序列的长度，这可能使理解程序的整体语义变得困难。
- en: More coarsely, many word-level approaches are proposed to tokenize source code
    into words by separators. For example, White et al. ([2015](#bib.bib258)) and
    Iyer et al. ([2016](#bib.bib108)) proposed to tokenize the program into words
    by whitespace, and designed RNNs to represent them for code summarization and
    code completion. Allamanis et al. ([2016](#bib.bib11)) designed a CNN with an
    attention mechanism to better represent the hierarchical structure of code over
    the subtokens that are simply tokenized by camel cases, to predict the function
    name.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 更粗略地说，许多词级方法被提出，通过分隔符将源代码标记化为词。例如，White 等人 ([2015](#bib.bib258)) 和 Iyer 等人 ([2016](#bib.bib108))
    提出了通过空白字符将程序标记化为词，并设计了 RNN 来表示这些词以进行代码摘要和代码补全。Allamanis 等人 ([2016](#bib.bib11))
    设计了一种具有注意机制的 CNN，以更好地表示代码的层次结构，基于简单的驼峰式标记的子标记，来预测函数名称。
- en: Out-of-Vocabulary (OOV) Issue. Since the variables and function names are always
    defined by developers without constraints, the size of vocabulary will explosively
    increase with the increasing training data, resulting in the out-of-vocabulary
    issue, which is more severe than that in NLP. To mitigate this issue, Cvitkovic
    et al. ([2019](#bib.bib57)) proposed a graph–structured cache, which introduces
    additional nodes for the encountered new words, and connects those nodes with
    edges based on where they occur in the code. Recently, Chirkova and Troshin ([2021b](#bib.bib50))
    offered a straightforward yet effective solution to mitigate the OOV issue by
    using identifier anonymization, and observed promising performance improvement.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇外（OOV）问题。由于变量和函数名称总是由开发者定义而没有约束，因此随着训练数据的增加，词汇量会爆炸性地增长，导致词汇外问题，这比自然语言处理中的问题更为严重。为缓解这一问题，Cvitkovic
    等人 ([2019](#bib.bib57)) 提出了图结构缓存的方法，引入了额外的节点来处理遇到的新词，并根据它们在代码中的出现位置连接这些节点。最近，Chirkova
    和 Troshin ([2021b](#bib.bib50)) 提供了一种简单而有效的解决方案，通过使用标识符匿名化来缓解 OOV 问题，并观察到有希望的性能提升。
- en: 'Another effective approach is to tokenize the source code at a sub-word level,
    such as using techniques like Byte Pair Encoding (BPE), which aims to construct
    a set of sub-words that can be combined to represent the entire code corpus. Figure [4](#S3.F4
    "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit") (b) shows the source tokens obtained
    by the strategy of word tokenization and BPE tokenization. For the input variable
    number, the word tokenization will maintain the original word and consider it
    as a rare word, while the BPE tokenization will split it into two common sub-words,
    i.e., num and ber. In the recent pre-trained language models of source code, e.g.,
    CuBERT (Kanade et al., [2020](#bib.bib116)) and CodeBERT (Feng et al., [2020](#bib.bib67)),
    BPE has commonly been adopted for reducing the vocabulary size. Karampatsis et al.
    ([2020](#bib.bib117)) conducted an empirical study on the granularity of word
    segmentation, and showed that tokenizing code by BPE can significantly reduce
    the vocabulary size.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种有效的方法是对源代码进行子词级别的标记化，例如使用像字节对编码（BPE）这样的技术，旨在构建一组可以组合以表示整个代码语料库的子词。图[4](#S3.F4
    "Figure 4 ‣ 3.2. Code Features ‣ 3. Literature Review ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit") (b) 展示了通过词标记化和BPE标记化策略获得的源代码标记。对于输入变量数量，词标记化将保持原始词汇并视其为稀有词汇，而BPE标记化则会将其拆分为两个常见的子词，即num和ber。在最近的源代码预训练语言模型中，例如CuBERT（Kanade
    et al., [2020](#bib.bib116)）和CodeBERT（Feng et al., [2020](#bib.bib67)），BPE通常被用于减少词汇表大小。Karampatsis
    et al. ([2020](#bib.bib117)) 进行了一项关于词分割粒度的实证研究，并显示BPE标记化代码可以显著减少词汇表大小。'
- en: 3.2.2\. Application Programming Interfaces (API)
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2. 应用程序编程接口（API）
- en: There have been multiple methods proposed to analyze the API sequences in programs.
    One line of work is about mining API usage patterns from a large code corpus to
    demonstrate how to use an API. For example, Moreno et al. ([2015](#bib.bib169))
    proposed a novel approach, named Muse, to demonstrate API usage by mining and
    ranking the code examples in usage. Another line of work is API recommendation,
    which aims to recommend or generate a sequence of APIs for users. Jiang et al.
    ([2017](#bib.bib111)) proposed to discover relevant tutorial fragments for APIs
    by calculating the correlation score based on PageRank and topic relevance. Gu
    et al. ([2016](#bib.bib79)) proposed a language model named DeepAPI, under the
    framework of sequence-to-sequence learning, to produce API sequences in response
    to a given natural language description. Different from DeepAPI, Nguyen et al.
    ([2017](#bib.bib177)) proposed API2Vec to represent the contextual information
    of API elements within an API sequence. Likewise, they also developed a tool called
    API2API based on API2Vec to migrate the APIs across different programming languages,
    i.e., from Java to C#, to validate the learned API embedding. Ling et al. ([2021b](#bib.bib144))
    introduced a method that integrated API call interactions and project structure
    into a single graph, and used this graph to design a graph-based collaborative
    filtering for making API usage recommendations. Bui et al. ([2019b](#bib.bib30))
    proposed a cross-language API mapping approach to map APIs from Java to C# with
    much less prior knowledge, through transfer learning across multiple domains.
    Hu et al. ([2018b](#bib.bib104)) suggested that incorporating API information
    as supplementary knowledge could improve code summarization. To improve the representation
    of semantics in natural-language queries and API sequences, Wei et al. ([2022](#bib.bib255))
    proposed a contrastive learning approach for API recommendation, and Hadi et al.
    ([2022](#bib.bib91)) investigated the effectiveness of pre-trained models for
    generating API sequences from natural language queries.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了多种方法来分析程序中的 API 序列。一种方法是从大量代码语料库中挖掘 API 使用模式，以展示如何使用 API。例如，Moreno 等人（[2015](#bib.bib169)）提出了一种新方法，名为
    Muse，通过挖掘和排序使用中的代码示例来展示 API 使用。另一种方法是 API 推荐，旨在为用户推荐或生成一系列 API。Jiang 等人（[2017](#bib.bib111)）提出了通过基于
    PageRank 和主题相关性计算相关性得分来发现与 API 相关的教程片段。Gu 等人（[2016](#bib.bib79)）提出了一种名为 DeepAPI
    的语言模型，在序列到序列学习框架下生成响应给定自然语言描述的 API 序列。与 DeepAPI 不同，Nguyen 等人（[2017](#bib.bib177)）提出了
    API2Vec，用于表示 API 序列中 API 元素的上下文信息。同样，他们还基于 API2Vec 开发了一个名为 API2API 的工具，用于迁移不同编程语言之间的
    API，即从 Java 迁移到 C#，以验证学习到的 API 嵌入。Ling 等人（[2021b](#bib.bib144)）介绍了一种将 API 调用交互和项目结构整合到一个图中的方法，并使用该图设计基于图的协同过滤，以进行
    API 使用推荐。Bui 等人（[2019b](#bib.bib30)）提出了一种跨语言 API 映射方法，通过跨多个领域的迁移学习，将 Java 的 API
    映射到 C#，所需的先验知识大大减少。Hu 等人（[2018b](#bib.bib104)）建议，将 API 信息作为补充知识纳入可以改善代码总结。为了改进自然语言查询和
    API 序列中的语义表示，Wei 等人（[2022](#bib.bib255)）提出了一种用于 API 推荐的对比学习方法，而 Hadi 等人（[2022](#bib.bib91)）研究了预训练模型在从自然语言查询生成
    API 序列中的有效性。
- en: 3.2.3\. Abstract Syntax Tree (AST)
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 抽象语法树（AST）
- en: 'The AST is a tree-structured intermediate representation of code that describes
    the syntactic structure of a program. As shown in Figure [4](#S3.F4 "Figure 4
    ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") (d), in an AST, the leaf nodes (e.g., number,
    Fib) typically correspond to the tokens of variables and method names in the source
    code, while the non-leaf nodes (e.g., FuncName, SwitchStmt) represent the syntactic
    structure of code, like function definition, branch functions. As a result, this
    representation allows ASTs to be useful for both capturing the lexical information
    (e.g., variable number) and the syntactic structure of the source code. In practice,
    we can extract ASTs using several open source tools, e.g., ANTLR⁴⁴4[https://www.antlr.org](https://www.antlr.org)
    parser, tree-sitter⁵⁵5[https://tree-sitter.github.io/tree-sitter](https://tree-sitter.github.io/tree-sitter)
    parser, and LLVM Clang⁶⁶6[https://clang.llvm.org](https://clang.llvm.org). To
    represent the ASTs, Mou et al. ([2016](#bib.bib170)) proposed a tree structure-based
    CNN, and verified it in a code classification task. In order to handle long-distance
    dependencies between nodes in an AST, Liu et al. ([2020e](#bib.bib152)) proposed
    an improved LSTM by introducing operations such as PUSH and POP, and verified
    it in the tasks of code completion, code classification, and code summarization.
    To better process an AST, Zhang et al. ([2019](#bib.bib286)) divided an AST into
    sentence-based subtrees and represented them using a two-way loop network. Recently,
    Kim et al. ([2021](#bib.bib118)) proposed using a relative position embedding
    for code completion to feed the AST to Transformers. Niu et al. ([2022](#bib.bib179))
    introduced a pre-trained model of source code by integrating AST information.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'AST 是一种树状结构的代码中间表示，描述了程序的语法结构。如图 [4](#S3.F4 "Figure 4 ‣ 3.2\. Code Features
    ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence: Survey, Benchmark
    and Toolkit") (d) 所示，在 AST 中，叶节点（例如，number、Fib）通常对应于源代码中变量和方法名称的标记，而非叶节点（例如，FuncName、SwitchStmt）表示代码的语法结构，如函数定义、分支函数。因此，这种表示方式使得
    AST 对于捕捉词汇信息（例如，变量 number）和源代码的语法结构都非常有用。在实际操作中，我们可以使用几个开源工具来提取 AST，例如 ANTLR⁴⁴4[https://www.antlr.org](https://www.antlr.org)
    解析器、tree-sitter⁵⁵5[https://tree-sitter.github.io/tree-sitter](https://tree-sitter.github.io/tree-sitter)
    解析器和 LLVM Clang⁶⁶6[https://clang.llvm.org](https://clang.llvm.org)。为了表示 AST，Mou
    等人 ([2016](#bib.bib170)) 提出了基于树结构的 CNN，并在代码分类任务中进行了验证。为了处理 AST 中节点之间的长距离依赖，Liu
    等人 ([2020e](#bib.bib152)) 提出了通过引入如 PUSH 和 POP 等操作的改进版 LSTM，并在代码补全、代码分类和代码摘要任务中进行了验证。为了更好地处理
    AST，Zhang 等人 ([2019](#bib.bib286)) 将 AST 划分为基于句子的子树，并使用双向循环网络进行表示。最近，Kim 等人 ([2021](#bib.bib118))
    提出了使用相对位置嵌入来进行代码补全，将 AST 输入到 Transformers 中。Niu 等人 ([2022](#bib.bib179)) 通过整合
    AST 信息引入了源代码的预训练模型。'
- en: Another line of work (Hu et al., [2018a](#bib.bib103); Alon et al., [2019](#bib.bib14),
    [2018](#bib.bib12)) is to represent ASTs indirectly by traversing or path sampling.
    Hu et al. ([2018a](#bib.bib103)) suggested traversing an AST to transform it into
    a linear series of nodes, and then using RNNs to represent the AST sequences for
    the task of code summarization. Alon et al. ([2019](#bib.bib14)) performed path
    sampling on the ASTs, and then used word2vec to represent the semantics of a program.
    Furthermore, Alon et al. ([2018](#bib.bib12)) also applied a similar idea to the
    task of code summarization. Similarly, Alon et al. ([2020](#bib.bib13)) proposed
    a structured code language model for code completion, by sampling paths from an
    incomplete AST.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类研究 (Hu 等人，[2018a](#bib.bib103)；Alon 等人，[2019](#bib.bib14)，[2018](#bib.bib12))
    是通过遍历或路径采样间接表示 AST。Hu 等人 ([2018a](#bib.bib103)) 建议遍历 AST，将其转换为线性节点序列，然后使用 RNN
    来表示 AST 序列以进行代码摘要任务。Alon 等人 ([2019](#bib.bib14)) 对 AST 进行路径采样，然后使用 word2vec 来表示程序的语义。此外，Alon
    等人 ([2018](#bib.bib12)) 还将类似的思路应用于代码摘要任务。类似地，Alon 等人 ([2020](#bib.bib13)) 提出了一个结构化代码语言模型用于代码补全，通过从不完整的
    AST 中采样路径。
- en: In program synthesis, an AST is also incorporated to guide the synthesis of
    programs. Yin and Neubig ([2017](#bib.bib277)) proposed an encoder-decoder framework
    for code generation, in which the encoder first encodes the natural language,
    then the decoder generates an AST of code, and finally, the AST is converted into
    source code. Chen et al. ([2018](#bib.bib44)) proposed a Tree2Tree model for program
    translation, which first uses a TreeLSTM to represent the source program, and
    another TreeLSTM to generate the target program written in another programming
    language.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在程序合成中，AST 也被纳入用于指导程序的合成。Yin 和 Neubig ([2017](#bib.bib277)) 提出了一个编码器-解码器框架用于代码生成，其中编码器首先对自然语言进行编码，然后解码器生成代码的
    AST，最后将 AST 转换为源代码。Chen et al. ([2018](#bib.bib44)) 提出了一个 Tree2Tree 模型用于程序翻译，该模型首先使用
    TreeLSTM 表示源程序，然后使用另一个 TreeLSTM 生成用另一种编程语言编写的目标程序。
- en: 3.2.4\. Intermediate Representation (IR)
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 中间表示（IR）
- en: 'The IR is a well-formed structure that is independent of programming languages
    and machine architectures. It is used by compilers to accurately represent the
    source code during the translation process from the source code to low-level machine
    code. The IR can express the operations of the target machine. It is natural to
    enhance the code embeddings via utilizing IRs (Li et al., [2022c](#bib.bib140)),
    with the benefit of limited vocabulary to significantly alleviate the OOV issue.
    In this paper, we employ LLVM-IR, which is used in the LLVM infrastructure (Lattner
    and Adve, [2004](#bib.bib120)), as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2\.
    Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit") (c). To represent IRs, Ben-Nun et al. ([2018](#bib.bib21))
    proposed inst2vec, which first compiles a program using LLVM Clang to obtain the
    LLVM intermediate representation, and then adopts skip-gram to represent the instructions.
    VenkataKeerthy et al. ([2020](#bib.bib233)) proposed IR2Vec, which regards the
    intermediate code representation as triples in the knowledge graph, and then explores
    several knowledge graph representation methods. Cummins et al. ([2021](#bib.bib55))
    introduced ProGraML, a novel graph-based code representation based on IR. This
    code graph provides new opportunities to represent the semantics of source code
    at a low level using machine learning techniques (e.g., GNNs), for complex downstream
    tasks such as program optimization and analysis. Peng et al. ([2021](#bib.bib186))
    proposed to represent the augmented IR of source code based on pre-training and
    contrastive learning techniques, guided by compiler optimization. Interestingly,
    Gui et al. ([2022](#bib.bib81)) studied a new problem of matching binary code
    and source code across languages by transforming both of them into LLVM-IRs.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'IR 是一个结构良好的表示形式，它独立于编程语言和机器架构。编译器在将源代码翻译为低级机器代码的过程中使用 IR 以准确表示源代码。IR 可以表达目标机器的操作。通过利用
    IR（Li et al., [2022c](#bib.bib140)）来增强代码嵌入是自然的，这样的好处是有限的词汇量显著缓解了 OOV 问题。在本文中，我们使用
    LLVM-IR，该 IR 在 LLVM 基础设施中使用（Lattner and Adve, [2004](#bib.bib120)），如图[4](#S3.F4
    "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit")（c）所示。为了表示 IR，Ben-Nun et al. ([2018](#bib.bib21))
    提出了 inst2vec，该方法首先使用 LLVM Clang 编译程序以获取 LLVM 中间表示，然后采用 skip-gram 来表示指令。VenkataKeerthy
    et al. ([2020](#bib.bib233)) 提出了 IR2Vec，该方法将中间代码表示视为知识图谱中的三元组，然后探索几种知识图谱表示方法。Cummins
    et al. ([2021](#bib.bib55)) 引入了 ProGraML，这是一种基于 IR 的新型图形代码表示。这种代码图提供了使用机器学习技术（例如，GNNs）在低级别表示源代码语义的新机会，用于复杂的下游任务，如程序优化和分析。Peng
    et al. ([2021](#bib.bib186)) 提出了基于预训练和对比学习技术、以编译器优化为指导的源代码增强 IR 表示。有趣的是，Gui et
    al. ([2022](#bib.bib81)) 研究了通过将二进制代码和源代码都转换为 LLVM-IR 来跨语言匹配这两个问题。'
- en: 3.2.5\. Code Graphs
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5\. 代码图
- en: 'Currently, many approaches have been proposed to convert programs into graphs
    to better represent the rich structural information within the programs, including
    Control-Flow Graph (CFG), Data-Flow Graph (DFG) and Code Property Graph (CPG).
    As shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature
    Review ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit")
    (e), the CFG represents the computation and control flow of a program. In this
    representation, each node represents a basic block and each edge represents the
    transitions of control flow in the program. As shown in Figure [4](#S3.F4 "Figure
    4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") (f), the DFG is a directed graph that illustrates
    data relationships among various functions. Each node in the DFG has input and
    output data ports, and each edge links an output port to an input port on another
    node. To represent multiple structural information of code using a joint data
    structure, Yamaguchi et al. ([2014](#bib.bib268)) proposed an innovative CPG to
    merge the structural information of code, including AST, CFG and Program Dependence
    Graph (PDG), into a single graph, as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2\.
    Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit") (g). In practice, we can build CFGs and DFGs using LLVM
    Clang, and build CPGs using Plume⁷⁷7[https://plume-oss.github.io/plume-docs](https://plume-oss.github.io/plume-docs).
    Recently, Cummins et al. ([2021](#bib.bib55)) built a unified graph, termed ProGraML,
    which includes the CFG, DFG and call-graph, as shown in Figure [4](#S3.F4 "Figure
    4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") (h).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '目前，已经提出了许多将程序转换为图形的办法，以更好地表示程序中丰富的结构信息，包括控制流图（CFG）、数据流图（DFG）和代码属性图（CPG）。如图[4](#S3.F4
    "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit")（e）所示，CFG 表示程序的计算和控制流。在这种表示中，每个节点表示一个基本块，每个边表示程序中控制流的转移。如图[4](#S3.F4
    "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit")（f）所示，DFG 是一个有向图，说明了各种函数之间的数据关系。DFG
    中的每个节点都有输入和输出数据端口，每条边将一个输出端口连接到另一个节点的输入端口。为了使用联合数据结构表示代码的多种结构信息，Yamaguchi 等人（[2014](#bib.bib268)）提出了一种创新的
    CPG，将代码的结构信息（包括 AST、CFG 和程序依赖图（PDG））合并为一个图，如图[4](#S3.F4 "Figure 4 ‣ 3.2\. Code
    Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit")（g）所示。实际上，我们可以使用 LLVM Clang 构建 CFG 和 DFG，使用 Plume⁷⁷7[https://plume-oss.github.io/plume-docs](https://plume-oss.github.io/plume-docs)
    构建 CPG。最近，Cummins 等人（[2021](#bib.bib55)）构建了一个统一的图，称为 ProGraML，其中包括 CFG、DFG 和调用图，如图[4](#S3.F4
    "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit")（h）所示。'
- en: To represent these code graphs, Allamanis et al. ([2018b](#bib.bib10)) introduced
    the data flow on the top of ASTs and formed a code graph. Then, a Gated Graph
    Neural Network (GGNN) (Li et al., [2016](#bib.bib134)) was developed to learn
    the data dependencies among this code graph. Allamanis and Brockschmidt ([2017](#bib.bib9))
    built the data flow among variables and considered the contextual information
    of variables for the task of automated pasting in programming. Brockschmidt et al.
    ([2019](#bib.bib25)) expanded the incomplete code into a graph, and then proposed
    a graph neural network for code completion. Sui et al. ([2020](#bib.bib214)) made
    the code representation more accurate by using the value-flow graph of a program.
    Shi et al. ([2022c](#bib.bib211)) resorted to converting the code graphs (e.g.,
    CFG and DFG) into sequences through traversing for the task of code search. Chen
    et al. ([2021a](#bib.bib45)) introduced a general method for transforming a code
    graph into a sequence of tokens and pointers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示这些代码图，Allamanis 等人（[2018b](#bib.bib10)）在抽象语法树（ASTs）之上引入了数据流，并形成了代码图。随后，开发了一种门控图神经网络（GGNN）（Li
    等人，[2016](#bib.bib134)），用于学习该代码图中的数据依赖关系。Allamanis 和 Brockschmidt（[2017](#bib.bib9)）构建了变量之间的数据流，并在自动粘贴编程任务中考虑了变量的上下文信息。Brockschmidt
    等人（[2019](#bib.bib25)）将不完整的代码扩展为图形，然后提出了一种用于代码完成的图神经网络。Sui 等人（[2020](#bib.bib214)）通过使用程序的值流图，使代码表示更加准确。Shi
    等人（[2022c](#bib.bib211)）采用将代码图（例如，CFG 和 DFG）通过遍历转换为序列的方法，用于代码搜索任务。Chen 等人（[2021a](#bib.bib45)）介绍了一种将代码图转换为令牌和指针序列的一般方法。
- en: 3.2.6\. Other Features of Code
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.6. 代码的其他特性
- en: In addition to the aforementioned features of code that have already been widely
    explored, there also exist several kinds of features that are used in some specific
    scenarios. For example, Henkel et al. ([2018](#bib.bib100)) introduced a novel
    feature for code representation learning based on abstractions of traces collected
    from the symbolic execution of a program. Hoang et al. ([2020](#bib.bib102)) proposed
    using deep learning to learn distributed representations of code changes/edits
    that may be used to generate software patches. In terms of code changes, several
    related works are also proposed to represent or predict them. Tufano et al. ([2019](#bib.bib228))
    proposed to automate code editing through sequence-to-sequence-based neural machine
    translation. Brody et al. ([2020](#bib.bib26)) proposed to represent the code
    edits first, and then iteratively generate tree edits over the AST.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了已经广泛探索的代码特性，还有一些特性用于特定场景。例如，Henkel 等人 ([2018](#bib.bib100)) 引入了一种基于程序符号执行中收集的跟踪抽象的代码表示学习新特性。Hoang
    等人 ([2020](#bib.bib102)) 提出了使用深度学习来学习代码更改/编辑的分布式表示，这些表示可能用于生成软件补丁。在代码更改方面，也提出了几种相关的工作来表示或预测这些更改。Tufano
    等人 ([2019](#bib.bib228)) 提出了通过基于序列到序列的神经机器翻译来自动化代码编辑。Brody 等人 ([2020](#bib.bib26))
    提出了先表示代码编辑，然后在抽象语法树（AST）上迭代生成树编辑。
- en: 3.2.7\. Hybrid Representation
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.7\. 混合表示
- en: To leverage multiple code features, several approaches to representing source
    code in a hybrid fashion have been developed. For instance, Gu et al. ([2018](#bib.bib78))
    explored using three separate RNNs for representing function names, code tokens,
    as well as API sequences of code, respectively. It has also been evaluated in
    the code search task. White et al. ([2016](#bib.bib257)) considered both the code
    tokens and AST node sequences, and used two different RNNs to represent these
    two sequences respectively, for the task of code cloning detection. Zhao and Huang
    ([2018](#bib.bib290)) proposed to represent the source code by incorporating the
    flow graphs of code into a semantic matrix. They also developed a neural network
    model to assess the functional similarity between the representations of two code
    snippets. Similarly, Wan et al. ([2018](#bib.bib238)) and Wan et al. ([2019](#bib.bib235))
    developed a hybrid network consisting of an LSTM representing the code tokens,
    a GGNN representing the CFG of code, and a TreeLSTM representing the AST of code,
    for the task of code summarization and code search. Chakraborty and Ray ([2021](#bib.bib41))
    suggested leveraging three modalities of information (e.g., edit location, edit
    code context, and commit messages) to represent the context of programming and
    generate code patches automatically.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用多种代码特性，已经开发了几种以混合方式表示源代码的方法。例如，Gu 等人 ([2018](#bib.bib78)) 探索了使用三个独立的RNN来分别表示函数名称、代码标记以及代码的API序列。该方法也在代码搜索任务中进行了评估。White
    等人 ([2016](#bib.bib257)) 考虑了代码标记和AST节点序列，并使用两个不同的RNN分别表示这两个序列，用于代码克隆检测任务。Zhao
    和 Huang ([2018](#bib.bib290)) 提出了通过将代码的流图纳入语义矩阵来表示源代码。他们还开发了一个神经网络模型来评估两个代码片段表示之间的功能相似性。类似地，Wan
    等人 ([2018](#bib.bib238)) 和 Wan 等人 ([2019](#bib.bib235)) 开发了一个混合网络，包括一个表示代码标记的LSTM，一个表示代码控制流图（CFG）的GGNN，以及一个表示代码AST的TreeLSTM，用于代码总结和代码搜索任务。Chakraborty
    和 Ray ([2021](#bib.bib41)) 建议利用三种信息模态（例如，编辑位置、编辑代码上下文和提交消息）来表示编程上下文并自动生成代码补丁。
- en: 3.3\. Deep Learning Techniques
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 深度学习技术
- en: 'We investigate the types of neural networks and classify the learning paradigms
    into four groups: supervised learning, unsupervised learning, self-supervised
    learning, and reinforcement learning.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了神经网络的类型，并将学习范式分为四组：监督学习、无监督学习、自监督学习和强化学习。
- en: 3.3.1\. Neural Networks
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 神经网络
- en: It is natural to model source code as sequential text, and directly apply NLP
    techniques to represent it. Simply, RNN (Raychev et al., [2014](#bib.bib198);
    Liu et al., [2016b](#bib.bib149); Gu et al., [2018](#bib.bib78); Alon et al.,
    [2018](#bib.bib12); Hellendoorn et al., [2018](#bib.bib99); Malik et al., [2019](#bib.bib163);
    Svyatkovskiy et al., [2019](#bib.bib223); White et al., [2016](#bib.bib257); Hu
    et al., [2018a](#bib.bib103); Zhang et al., [2020c](#bib.bib285); Gupta et al.,
    [2017](#bib.bib90)) and CNN (Allamanis et al., [2016](#bib.bib11); Sun et al.,
    [2019](#bib.bib219)) neural networks can be easily applied to represent the sequential
    structure of source code. In order to capture the syntax structure, especially
    the AST of source code, many tree-structured neural networks (Wan et al., [2018](#bib.bib238);
    Mou et al., [2016](#bib.bib170); Chen et al., [2018](#bib.bib44)) have also been
    designed. Furthermore, to represent the semantic structures (e.g., CFG and DFG)
    of source code, GNNs (Allamanis et al., [2018b](#bib.bib10); Zhou et al., [2019](#bib.bib294);
    Wang and Li, [2021](#bib.bib248); Brockschmidt et al., [2019](#bib.bib25); Wei
    et al., [2020a](#bib.bib254); Allamanis et al., [2020](#bib.bib8); Liu et al.,
    [2020a](#bib.bib155)) have been introduced to represent the source code. Recently,
    the Transformer architecture has been utilized to represent the source code (Kim
    et al., [2021](#bib.bib118); Svyatkovskiy et al., [2020](#bib.bib221)). Chirkova
    and Troshin ([2021a](#bib.bib49)) conducted a comprehensive empirical study of
    how well Transformers can leverage syntactic information in source code for various
    tasks. More preliminaries about the mentioned neural networks are referred to
    the Supplementary Materials.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将源代码建模为序列文本并直接应用自然语言处理（NLP）技术来表示它是自然的。简单来说，RNN（Raychev et al., [2014](#bib.bib198);
    Liu et al., [2016b](#bib.bib149); Gu et al., [2018](#bib.bib78); Alon et al.,
    [2018](#bib.bib12); Hellendoorn et al., [2018](#bib.bib99); Malik et al., [2019](#bib.bib163);
    Svyatkovskiy et al., [2019](#bib.bib223); White et al., [2016](#bib.bib257); Hu
    et al., [2018a](#bib.bib103); Zhang et al., [2020c](#bib.bib285); Gupta et al.,
    [2017](#bib.bib90)) 和 CNN（Allamanis et al., [2016](#bib.bib11); Sun et al., [2019](#bib.bib219)）神经网络可以轻松应用于表示源代码的序列结构。为了捕捉语法结构，特别是源代码的抽象语法树（AST），许多树结构神经网络（Wan
    et al., [2018](#bib.bib238); Mou et al., [2016](#bib.bib170); Chen et al., [2018](#bib.bib44)）也被设计出来。此外，为了表示源代码的语义结构（例如，控制流图（CFG）和数据流图（DFG）），已经引入了图神经网络（GNNs）（Allamanis
    et al., [2018b](#bib.bib10); Zhou et al., [2019](#bib.bib294); Wang and Li, [2021](#bib.bib248);
    Brockschmidt et al., [2019](#bib.bib25); Wei et al., [2020a](#bib.bib254); Allamanis
    et al., [2020](#bib.bib8); Liu et al., [2020a](#bib.bib155)）来表示源代码。最近，Transformer
    架构被用于表示源代码（Kim et al., [2021](#bib.bib118); Svyatkovskiy et al., [2020](#bib.bib221)）。Chirkova
    和 Troshin（[2021a](#bib.bib49)）进行了全面的实证研究，探讨了 Transformers 在各种任务中如何利用源代码中的语法信息。有关上述神经网络的更多初步资料请参阅补充材料。
- en: 3.3.2\. Supervised Learning
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 监督学习
- en: Supervised learning aims to learn a function that maps an input to an output
    based on a set of input-output pairs as training data. It is a widely used learning
    paradigm in deep learning. From our investigation, current deep learning approaches
    for code intelligence are mainly based on supervised learning. For each specific
    code intelligence task, such as code classification (Mou et al., [2016](#bib.bib170);
    Bui et al., [2019a](#bib.bib29)), vulnerability detection and bug finding (Li
    et al., [2018b](#bib.bib142); Zhou et al., [2019](#bib.bib294); Cheng et al.,
    [2021](#bib.bib47); Li et al., [2019](#bib.bib139)), code completion (Raychev
    et al., [2014](#bib.bib198); Liu et al., [2016b](#bib.bib149); Li et al., [2018a](#bib.bib129);
    Svyatkovskiy et al., [2019](#bib.bib223); Alon et al., [2020](#bib.bib13); Svyatkovskiy
    et al., [2020](#bib.bib221)), type inference (Hellendoorn et al., [2018](#bib.bib99);
    Malik et al., [2019](#bib.bib163); Wei et al., [2020a](#bib.bib254); Allamanis
    et al., [2020](#bib.bib8)), code search (Gu et al., [2018](#bib.bib78); Wan et al.,
    [2019](#bib.bib235); Haldar et al., [2020](#bib.bib92)), code clone detection (White
    et al., [2016](#bib.bib257); Wei and Li, [2017](#bib.bib253); Zhao and Huang,
    [2018](#bib.bib290); Wu et al., [2020](#bib.bib262); Zhang et al., [2019](#bib.bib286)),
    code summarization (Allamanis et al., [2016](#bib.bib11); Iyer et al., [2016](#bib.bib108);
    Hu et al., [2018a](#bib.bib103); Alon et al., [2018](#bib.bib12); Wan et al.,
    [2018](#bib.bib238)), program translation (Chen et al., [2018](#bib.bib44); Gu
    et al., [2017](#bib.bib80)), program synthesis (Dong and Lapata, [2016](#bib.bib65);
    Liu et al., [2016a](#bib.bib148); Sun et al., [2019](#bib.bib219); Zhong et al.,
    [2017](#bib.bib292); Cai et al., [2018](#bib.bib33)), and program repair (Gupta
    et al., [2017](#bib.bib90); Vasic et al., [2018](#bib.bib231); Dinella et al.,
    [2020](#bib.bib63); Chakraborty et al., [2020](#bib.bib40); Zhu et al., [2021](#bib.bib297);
    Tufano et al., [2018a](#bib.bib229); Li et al., [2020](#bib.bib135)), a set of
    paired input-output data is collected first. For each task, supervised learning
    is guided by a specific loss function. One limitation of this kind of approach
    is that it relies on lots of well-labeled input-output pairs, which are always
    expensive to collect in some scenarios.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习旨在学习一个函数，该函数根据一组输入-输出对作为训练数据将输入映射到输出。这是一种在深度学习中广泛使用的学习范式。根据我们的调查，目前用于代码智能的深度学习方法主要基于监督学习。对于每个特定的代码智能任务，如代码分类 (Mou
    等人，[2016](#bib.bib170); Bui 等人，[2019a](#bib.bib29))，漏洞检测和错误查找 (Li 等人，[2018b](#bib.bib142);
    Zhou 等人，[2019](#bib.bib294); Cheng 等人，[2021](#bib.bib47); Li 等人，[2019](#bib.bib139))，代码补全 (Raychev
    等人，[2014](#bib.bib198); Liu 等人，[2016b](#bib.bib149); Li 等人，[2018a](#bib.bib129);
    Svyatkovskiy 等人，[2019](#bib.bib223); Alon 等人，[2020](#bib.bib13); Svyatkovskiy
    等人，[2020](#bib.bib221))，类型推断 (Hellendoorn 等人，[2018](#bib.bib99); Malik 等人，[2019](#bib.bib163);
    Wei 等人，[2020a](#bib.bib254); Allamanis 等人，[2020](#bib.bib8))，代码搜索 (Gu 等人，[2018](#bib.bib78);
    Wan 等人，[2019](#bib.bib235); Haldar 等人，[2020](#bib.bib92))，代码克隆检测 (White 等人，[2016](#bib.bib257);
    Wei 和 Li，[2017](#bib.bib253); Zhao 和 Huang，[2018](#bib.bib290); Wu 等人，[2020](#bib.bib262);
    Zhang 等人，[2019](#bib.bib286))，代码摘要 (Allamanis 等人，[2016](#bib.bib11); Iyer 等人，[2016](#bib.bib108);
    Hu 等人，[2018a](#bib.bib103); Alon 等人，[2018](#bib.bib12); Wan 等人，[2018](#bib.bib238))，程序翻译 (Chen
    等人，[2018](#bib.bib44); Gu 等人，[2017](#bib.bib80))，程序合成 (Dong 和 Lapata，[2016](#bib.bib65);
    Liu 等人，[2016a](#bib.bib148); Sun 等人，[2019](#bib.bib219); Zhong 等人，[2017](#bib.bib292);
    Cai 等人，[2018](#bib.bib33))，以及程序修复 (Gupta 等人，[2017](#bib.bib90); Vasic 等人，[2018](#bib.bib231);
    Dinella 等人，[2020](#bib.bib63); Chakraborty 等人，[2020](#bib.bib40); Zhu 等人，[2021](#bib.bib297);
    Tufano 等人，[2018a](#bib.bib229); Li 等人，[2020](#bib.bib135))，首先会收集一组配对的输入-输出数据。对于每个任务，监督学习都由特定的损失函数指导。这种方法的一个限制是它依赖于大量标记良好的输入-输出对，在某些情况下这些对的收集成本总是很高。
- en: 3.3.3\. Unsupervised Learning
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 无监督学习
- en: As opposed to supervised learning, unsupervised learning seeks to identify patterns
    from a dataset without labels. One representative work is TransCoder (Rozière
    et al., [2020](#bib.bib200)), in which a fully unsupervised neural source-to-source
    translator is trained based on unsupervised machine translation. This kind of
    learning paradigm is challenging for code intelligence and more research work
    is still required.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，无监督学习试图从没有标签的数据集中识别模式。一个代表性的工作是 TransCoder (Rozière 等人，[2020](#bib.bib200))，其中训练了一个完全无监督的神经源到源翻译器，该翻译器基于无监督机器翻译。这种学习范式对代码智能来说具有挑战性，仍需要更多的研究工作。
- en: 3.3.4\. Self-Supervised Learning
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. 自监督学习
- en: Self-supervised learning can be thought of as a blend of supervised learning
    and unsupervised learning. Different from supervised learning where data labels
    are available for training, self-supervised learning obtains the supervisory signals
    directly from the data itself, usually the underlying structure of the data. One
    common practice used by self-supervised learning is to predict any unobserved
    (or masked) part of input from the part that can be observed. As a representative
    technique of self-supervised learning, language model pre-training has been widely
    studied in source code (Kanade et al., [2020](#bib.bib116); Feng et al., [2020](#bib.bib67);
    Guo et al., [2021](#bib.bib84)). Kanade et al. ([2020](#bib.bib116)) proposed
    to train a CuBERT on the Python code corpus, and verified the pre-trained model
    on multiple downstream tasks such as variable misuse, operator classification,
    and function-document matching. CodeBERT (Feng et al., [2020](#bib.bib67)) is
    yet another pre-trained model that deals with the two different modalities of
    source code and natural language descriptions. It is based on masked language
    modeling, and has achieved promising results in tasks such as code search and
    code completion. Based on CodeBERT, GraphCodeBERT (Guo et al., [2021](#bib.bib84)),
    SPT-Code (Niu et al., [2022](#bib.bib179)), and TreeBERT (Jiang et al., [2021b](#bib.bib114))
    are proposed to digest the structural information from source code. Lachaux et al.
    ([2021](#bib.bib119)) presented a pre-training objective based on deobfuscation
    as an alternative criterion. Inspired by BART (Lewis et al., [2020](#bib.bib125))
    which is a pre-trained deep model specially designed towards natural language
    understanding and generation, Ahmad et al. ([2021](#bib.bib4)) trained a similar
    pre-trained model PLBART for tasks that are related to code generation as well
    as code understanding. Zhang et al. ([2022a](#bib.bib284)) trained a model named
    CoditT5 on large amounts of source code and natural-language comments, for software-related
    editing tasks, e.g., comment updating, bug fixing, and automated code review.
    Wang et al. ([2021b](#bib.bib245)) and Guo et al. ([2022b](#bib.bib83)) proposed
    to train a model by unifying the modality of source code and natural language
    with contrastive learning, to improve the representation of the semantics of source
    code. Mastropaolo et al. ([2021](#bib.bib164)) and Wang et al. ([2021a](#bib.bib249))
    explored building pre-trained models based on the T5 (Text-To-Text Transfer Transformer)
    architecture, which has attained state-of-the-art results in NLP tasks. Bui et al.
    ([2021a](#bib.bib31)) proposed InferCode, a self-supervised learning method through
    predicting subtrees that are identified from the context of ASTs. Jain et al.
    ([2021](#bib.bib110)) proposed a contrastive learning approach for task-agnostic
    code representation based on program transformations in the compiler. ERNIE-Code (Chai
    et al., [2022a](#bib.bib38)) is a unified pre-trained model based on 116 natural
    languages and 6 programming languages, with the aim of bridging the gap between
    multilingual natural languages and multilingual programming languages. Given that
    LLMs are pre-trained on a dataset that may have a different distribution from
    the testing dataset, Wang et al. ([2022b](#bib.bib240)) explored the fine-tuning
    of pre-trained code models to facilitate adaptation to downstream tasks through
    curriculum learning.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习可以被视为监督学习和无监督学习的结合。不同于监督学习中数据标签用于训练，自监督学习直接从数据本身获取监督信号，通常是数据的潜在结构。自监督学习的一个常见做法是根据可观察的部分预测任何未观察到（或被掩盖）的输入部分。作为自监督学习的代表技术，语言模型预训练已在源代码中被广泛研究（Kanade
    et al., [2020](#bib.bib116); Feng et al., [2020](#bib.bib67); Guo et al., [2021](#bib.bib84)）。Kanade
    et al. ([2020](#bib.bib116)) 提出了在 Python 代码语料库上训练 CuBERT，并在多个下游任务中验证了预训练模型，如变量误用、运算符分类和函数-文档匹配。CodeBERT
    (Feng et al., [2020](#bib.bib67)) 是另一个处理源代码和自然语言描述两种不同模态的预训练模型。它基于掩蔽语言建模，并在代码搜索和代码完成等任务中取得了令人鼓舞的结果。基于
    CodeBERT，GraphCodeBERT (Guo et al., [2021](#bib.bib84))、SPT-Code (Niu et al.,
    [2022](#bib.bib179)) 和 TreeBERT (Jiang et al., [2021b](#bib.bib114)) 被提出以消化源代码中的结构信息。Lachaux
    et al. ([2021](#bib.bib119)) 提出了基于去混淆的预训练目标作为替代标准。受到 BART (Lewis et al., [2020](#bib.bib125))
    的启发，BART 是专门为自然语言理解和生成设计的预训练深度模型，Ahmad et al. ([2021](#bib.bib4)) 训练了一个类似的预训练模型
    PLBART，用于与代码生成和代码理解相关的任务。Zhang et al. ([2022a](#bib.bib284)) 在大量源代码和自然语言注释上训练了一个名为
    CoditT5 的模型，用于软件相关的编辑任务，如注释更新、错误修复和自动化代码审查。Wang et al. ([2021b](#bib.bib245))
    和 Guo et al. ([2022b](#bib.bib83)) 提出了通过对比学习统一源代码和自然语言的模态，以改善源代码语义的表示。Mastropaolo
    et al. ([2021](#bib.bib164)) 和 Wang et al. ([2021a](#bib.bib249)) 探索了基于 T5 (Text-To-Text
    Transfer Transformer) 架构的预训练模型，该模型在 NLP 任务中已获得最先进的结果。Bui et al. ([2021a](#bib.bib31))
    提出了 InferCode，一种通过预测从 AST 上下文中识别的子树进行的自监督学习方法。Jain et al. ([2021](#bib.bib110))
    提出了基于编译器中的程序转换的任务无关代码表示的对比学习方法。ERNIE-Code (Chai et al., [2022a](#bib.bib38)) 是一个统一的预训练模型，基于
    116 种自然语言和 6 种编程语言，旨在弥合多语言自然语言和多语言编程语言之间的差距。鉴于 LLMs 在与测试数据集可能有不同分布的数据集上进行预训练，Wang
    et al. ([2022b](#bib.bib240)) 探索了预训练代码模型的微调，以通过课程学习促进适应下游任务。
- en: Instead of improving the capability of code embedding, Wan et al. ([2022c](#bib.bib237))
    investigated the explainability of pre-trained models for code intelligence, i.e.,
    what kind of information do these models capture, through structural analysis.
    Zhang et al. ([2022b](#bib.bib289)) and Shi et al. ([2022b](#bib.bib209)) suggested
    compressing pre-trained models of code, as to accelerate their efficiency in practice.
    Zhou et al. ([2021](#bib.bib293)) carried out an empirical study to assess the
    generalizability of CodeBERT when applied to various datasets and downstream tasks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与其提高代码嵌入的能力，万等人（[2022c](#bib.bib237)）通过结构分析研究了预训练模型在代码智能方面的可解释性，即这些模型捕捉了什么信息。张等人（[2022b](#bib.bib289)）和石等人（[2022b](#bib.bib209)）建议压缩预训练的代码模型，以加快其在实际应用中的效率。周等人（[2021](#bib.bib293)）进行了实证研究，以评估CodeBERT在应用于各种数据集和下游任务时的泛化能力。
- en: 'Large Language Models (LLMs) of Code. The aforementioned pre-trained code models
    have demonstrated promising capabilities in comprehending the semantics of source
    code. More recently, with the remarkable performance achievements of LLMs in text
    generation and conversational dialogs, exemplified by the success of ChatGPT,
    a diverse array of LLMs has been specifically trained for code-related tasks,
    notably, code generation. Nijkamp et al. ([2022](#bib.bib178)) introduced a novel
    code generation task that enables users to progressively express their intentions
    through multi-turn interactions, and further trained a family of LLMs with up
    to 16.1 billion parameters, called CodeGen, for this task. PaLM (Chowdhery et al.,
    [2022](#bib.bib51)) is a general-purpose LLM developed by Google, which is pre-trained
    on a substantial dataset comprising both text and code corpora, boasting a vast
    parameter size of up to 540 billion. Derived from PaLM, PaLM-Coder is a model
    specifically fine-tuned for code-related tasks, such as code generation and program
    translation. InCoder (Fried et al., [2022](#bib.bib69)) is a LLM developed by
    Meta, which employs a causal masking objective for the purpose of infilling code
    blocks based on arbitrary left and right contexts. Pangu-Coder (Christopoulou
    et al., [2022](#bib.bib52)), introduced by Huawei, is a LLM specifically developed
    for code generation. Its training follows a two-stage strategy: in the first stage,
    it undergoes pre-training using Causal Language Modeling (CLM) on raw code corpora.
    In the second stage, it employs a combination of CLM and Masked Language Modeling
    (MLM) training objectives, with a focus on the downstream task of code generation
    from text. CodeGeeX (Zheng et al., [2023](#bib.bib291)) is a multilingual model
    with 13 billion parameters for code generation, pre-trained on 850 billion tokens
    of 23 programming languages. StarCoder (Li et al., [2023a](#bib.bib131)) is a
    LLM for code, up to 15.5 billion parameters, which is trained on an extensive
    dataset consisting of 1 trillion tokens sourced from a vast collection of permissively
    licensed GitHub repositories with inspection tools and an opt-out process. Code
    Llama (Roziere et al., [2023](#bib.bib199)) is a family of large language models
    for code, released by Meta, built upon the foundation of Llama 2. These models
    are distinguished by their advanced infilling capabilities, extensive long-context
    fine-tuning, and precise instruction fine-tuning. CodeT5+ (Wang et al., [2023](#bib.bib247)),
    released by Salesforce, represents a novel family of encoder-decoder-based LLMs
    explicitly tailored for a broad spectrum of tasks related to both code comprehension
    and code generation. This model introduces innovative pre-training objectives,
    including text-code contrastive learning, matching, and CLM tasks on text-code
    data. phi-1 (Gunasekar et al., [2023](#bib.bib82)) is a comparatively smaller
    LLM for code, consisting of 1.3 billion parameters, achieved through data set
    refinement, while maintaining competitive performance in code generation.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的大型语言模型（LLMs）。前述的预训练代码模型已经展示了在理解源代码语义方面的良好能力。最近，随着LLMs在文本生成和对话交流中的出色表现，例如ChatGPT的成功，各种LLMs被特别训练用于代码相关任务，特别是代码生成。Nijkamp等人（[2022](#bib.bib178)）介绍了一种新颖的代码生成任务，使用户能够通过多轮互动逐步表达他们的意图，并进一步训练了一家名为CodeGen的LLMs系列，其参数量高达16.1亿。PaLM（Chowdhery等人，[2022](#bib.bib51)）是Google开发的通用LLM，预训练于包含文本和代码语料的大规模数据集，参数量高达5400亿。PaLM-Coder是基于PaLM的模型，专门为代码相关任务如代码生成和程序翻译进行微调。InCoder（Fried等人，[2022](#bib.bib69)）是Meta开发的LLM，采用因果掩码目标，用于根据任意左侧和右侧上下文填充代码块。Pangu-Coder（Christopoulou等人，[2022](#bib.bib52)）是华为推出的专为代码生成开发的LLM。其训练遵循两阶段策略：第一阶段，使用因果语言建模（CLM）在原始代码语料上进行预训练；第二阶段，结合CLM和掩码语言建模（MLM）训练目标，重点关注从文本生成代码的下游任务。CodeGeeX（Zheng等人，[2023](#bib.bib291)）是一个拥有130亿参数的多语言代码生成模型，预训练于8500亿个标记的23种编程语言上。StarCoder（Li等人，[2023a](#bib.bib131)）是一个用于代码的LLM，拥有高达155亿个参数，训练于一个包含1万亿个标记的大规模数据集，该数据集来自于大量许可的GitHub仓库，配有检查工具和选择退出过程。Code
    Llama（Roziere等人，[2023](#bib.bib199)）是Meta发布的一系列用于代码的大型语言模型，建立在Llama 2的基础上。这些模型以其先进的填充能力、广泛的长上下文微调和精确的指令微调而著称。CodeT5+（Wang等人，[2023](#bib.bib247)），由Salesforce发布，代表了一系列新的基于编码器-解码器的LLMs，明确针对代码理解和生成相关任务的广泛范围。这一模型引入了创新的预训练目标，包括文本-代码对比学习、匹配和文本-代码数据上的CLM任务。phi-1（Gunasekar等人，[2023](#bib.bib82)）是一个相对较小的代码LLM，拥有13亿参数，通过数据集优化实现，同时在代码生成中保持竞争力的性能。
- en: Different from traditional pre-trained code models that are designed for specific
    tasks, the LLMs for code are distinguished by their strong capabilities in zero-shot
    learning. To unleash the zero-shot capabilities of LLMs, many techniques such
    as prompt tuning, in-context learning, chain-of-thought, and instruction tuning,
    have been developed. Recently, numerous studies have explored the potential of
    LLMs in tasks such as code generation (Liu et al., [2023](#bib.bib147)), code
    summarization (Geng et al., [2024](#bib.bib74)), and code repair (Xia et al.,
    [2023](#bib.bib264)), all achieved through the design of textual prompts. As a
    specific prompting, in-context learning seeks to bolster the capabilities of LLMs
    by furnishing them with contextual information or illustrative examples. Li et al.
    ([2023c](#bib.bib127)) explored in-context learning for better code generation
    based on LLMs. The chain-of-thought is designed to ensure the outputs of LLMs
    follow a logical chain. Li et al. ([2023b](#bib.bib126)) explored chain-of-thought
    for better code generation based on LLMs. The instruction tuning is initially
    designed to enhance the generalization capabilities of LLMs across different tasks.
    WizardCoder (Luo et al., [2023](#bib.bib161)) is crafted to augment the capabilities
    of StarCoder by creating sophisticated code instruction data via the code-specific
    Evol-Instruct approach.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与为特定任务设计的传统预训练代码模型不同，代码的LLMs（大型语言模型）以其强大的零样本学习能力而有所区别。为了发挥LLMs的零样本能力，已经开发了许多技术，如提示调优、上下文学习、思维链和指令调优。最近，众多研究探讨了LLMs在代码生成（刘等，[2023](#bib.bib147)）、代码摘要（耿等，[2024](#bib.bib74)）和代码修复（夏等，[2023](#bib.bib264)）等任务中的潜力，这些都是通过设计文本提示实现的。作为一种特定的提示方式，上下文学习旨在通过提供上下文信息或示例来增强LLMs的能力。李等（[2023c](#bib.bib127)）探讨了基于LLMs的更好代码生成的上下文学习。思维链旨在确保LLMs的输出遵循逻辑链。李等（[2023b](#bib.bib126)）探讨了基于LLMs的更好代码生成的思维链。指令调优最初旨在提高LLMs在不同任务中的泛化能力。WizardCoder（罗等，[2023](#bib.bib161)）通过创建通过代码特定的Evol-Instruct方法生成的复杂代码指令数据，来增强StarCoder的能力。
- en: 3.3.5\. Reinforcement Learning
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5\. 强化学习
- en: Reinforcement learning aims to learn an agent through interacting with the environment
    without input-output pairs. This kind of learning paradigm has been used in code
    summarization (Wan et al., [2018](#bib.bib238)), code search (Yao et al., [2019](#bib.bib273)),
    program repair (Gupta et al., [2018](#bib.bib88)), and program synthesis (Zhong
    et al., [2017](#bib.bib292)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习旨在通过与环境的互动来训练代理，而无需输入-输出对。这种学习范式已被应用于代码摘要（万等，[2018](#bib.bib238)）、代码搜索（姚等，[2019](#bib.bib273)）、程序修复（古普塔等，[2018](#bib.bib88)）和程序合成（钟等，[2017](#bib.bib292)）。
- en: 3.4\. Classification-based Applications
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 基于分类的应用
- en: Classification-based applications, such as code categorization, vulnerability
    detection, and type inference, seek to train a classifier with the objective of
    mapping the source code to specific labels or classes, such as identifying vulnerability
    status or variable types.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分类的应用，例如代码分类、漏洞检测和类型推断，旨在训练分类器，以将源代码映射到特定的标签或类别，如识别漏洞状态或变量类型。
- en: 3.4.1\. Code Classification
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 代码分类
- en: Classifying source code into different classes (e.g., different functionalities
    and programming languages), is important for many tasks such as code categorization,
    programming language identification, code prediction, and vulnerability detection.
    Various studies have been conducted to classify code snippets into categories
    based on their functionalities. To represent programs in the form of ASTs, Mou
    et al. ([2016](#bib.bib170)) developed a Tree-Based Convolutional Neural Network
    (TBCNN), which was then verified on code classification. In the wider realm of
    software categorization, LeClair et al. ([2018](#bib.bib122)) devised a series
    of adaptations, incorporating techniques such as word embedding and neural architectures,
    to tailor NLP methods for text classification specifically to the domain of source
    code. Bui et al. ([2019a](#bib.bib29)) presented a bilateral neural network for
    the cross-language algorithm classification task, where each sub-network is used
    to encode the semantics of code in a specific language, and an additional classification
    module is designed to model the connection of those bilateral programs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将源代码分类为不同类别（例如，不同的功能和编程语言）对于许多任务都很重要，如代码分类、编程语言识别、代码预测和漏洞检测。已有多项研究对代码片段进行了功能分类。为了将程序表示为AST的形式，Mou等人（[2016](#bib.bib170)）开发了一种基于树的卷积神经网络（TBCNN），并在代码分类上进行了验证。在更广泛的软件分类领域，LeClair等人（[2018](#bib.bib122)）设计了一系列改进，结合了如词嵌入和神经网络架构等技术，以将NLP方法特别定制为源代码的文本分类。Bui等人（[2019a](#bib.bib29)）提出了一种双向神经网络用于跨语言算法分类任务，其中每个子网络用于编码特定语言中代码的语义，并设计了一个额外的分类模块以建模这些双向程序的连接。
- en: 3.4.2\. Vulnerability Detection and Bug Finding
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. 漏洞检测与错误查找
- en: Detecting vulnerabilities or bugs in programs is essential for assuring the
    quality of software, as well as saves much effort and time for software development.
    Although many tools have been developed for vulnerability detection, e.g., Clang
    Static Analyzer⁸⁸8[https://clang-analyzer.llvm.org/scan-build.html](https://clang-analyzer.llvm.org/scan-build.html),
    Coverity⁹⁹9[https://scan.coverity.com](https://scan.coverity.com), Fortify^(10)^(10)10[https://www.hpfod.com](https://www.hpfod.com),
    Flawfinder^(11)^(11)11[https://dwheeler.com/flawfinder](https://dwheeler.com/flawfinder),
    Infer^(12)^(12)12[https://fbinfer.com](https://fbinfer.com), and SVF (Sui and
    Xue, [2016](#bib.bib215)), most of them are based on static analysis. Recently,
    a growing number of works employ deep learning to discover vulnerabilities. Wang
    et al. ([2016](#bib.bib243)) made an early attempt at applying deep learning,
    specifically deep belief network, to predict the defects of software, which learns
    the semantic features of programs based on AST. Dam et al. ([2018](#bib.bib58))
    proposed an LSTM-based method to exploit both the syntactic and semantic aspects
    of source code, and apply the embeddings for both within-project and cross-project
    vulnerability detection. VulDeePecker (Li et al., [2018b](#bib.bib142)), $\mu$VulDeePecker (Zou
    et al., [2019](#bib.bib299)) and SySeVR (Li et al., [2021d](#bib.bib141)) are
    a series of works that preserve the semantics of program by extracting API function
    calls and program slices for vulnerability detection. Le et al. ([2018](#bib.bib121))
    presented a maximal divergence sequential auto-encoder network to find vulnerabilities
    in binary files. The network is designed so that the embeddings of vulnerable
    code and invulnerable code are encouraged to be maximally divergent. Zhou et al.
    ([2019](#bib.bib294)) proposed Devign for vulnerability detection, which first
    represents a program by fusing its AST, CFG and DFG into a unified CPG, and then
    designs a graph neural network to represent the CPG of code. Similarly, Wang et al.
    ([2020c](#bib.bib241)) and Cao et al. ([2022](#bib.bib35)) proposed a flow-sensitive
    framework for vulnerability detection, which leverages a GNN to represent the
    control, data, and call dependencies of a program. Cheng et al. ([2021](#bib.bib47))
    introduced DeepWukong, a GNN-based model for vulnerability detection of C/C++
    programs, in which the flow information of programs is preserved. Liu et al. ([2021b](#bib.bib156))
    introduced a GNN model with expert knowledge for detecting vulnerabilities in
    smart contracts, which incorporates the flow information of programs. Inspired
    by image processing, Wu et al. ([2022b](#bib.bib263)) proposed a method to enhance
    the scalability of vulnerability detection by transforming code into an image
    with semantics preserved, and implementing a CNN to capture them effectively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 检测程序中的漏洞或缺陷对于保证软件质量至关重要，同时也能为软件开发节省大量精力和时间。虽然已经开发了许多用于漏洞检测的工具，例如 Clang Static
    Analyzer⁸⁸8[https://clang-analyzer.llvm.org/scan-build.html](https://clang-analyzer.llvm.org/scan-build.html)、Coverity⁹⁹9[https://scan.coverity.com](https://scan.coverity.com)、Fortify^(10)^(10)10[https://www.hpfod.com](https://www.hpfod.com)、Flawfinder^(11)^(11)11[https://dwheeler.com/flawfinder](https://dwheeler.com/flawfinder)、Infer^(12)^(12)12[https://fbinfer.com](https://fbinfer.com)以及
    SVF (Sui and Xue, [2016](#bib.bib215))，大多数这些工具都基于静态分析。近年来，越来越多的研究开始利用深度学习来发现漏洞。Wang
    等人 ([2016](#bib.bib243)) 早期尝试将深度学习，特别是深度信念网络，应用于预测软件缺陷，基于 AST 学习程序的语义特征。Dam 等人
    ([2018](#bib.bib58)) 提出了基于 LSTM 的方法，利用源代码的语法和语义方面，并应用于项目内和跨项目的漏洞检测。VulDeePecker
    (Li 等人, [2018b](#bib.bib142))、$\mu$VulDeePecker (Zou 等人, [2019](#bib.bib299))
    和 SySeVR (Li 等人, [2021d](#bib.bib141)) 是一系列通过提取 API 函数调用和程序切片来保留程序语义进行漏洞检测的工作。Le
    等人 ([2018](#bib.bib121)) 提出了最大离散序列自编码器网络，用于在二进制文件中发现漏洞。该网络的设计目的是使漏洞代码和非漏洞代码的嵌入尽可能地不同。Zhou
    等人 ([2019](#bib.bib294)) 提出了用于漏洞检测的 Devign 方法，该方法首先通过将程序的 AST、CFG 和 DFG 融合成统一的
    CPG 来表示程序，然后设计了一个图神经网络来表示代码的 CPG。同样，Wang 等人 ([2020c](#bib.bib241)) 和 Cao 等人 ([2022](#bib.bib35))
    提出了用于漏洞检测的流敏感框架，利用 GNN 来表示程序的控制、数据和调用依赖关系。Cheng 等人 ([2021](#bib.bib47)) 介绍了 DeepWukong，这是一种基于
    GNN 的 C/C++ 程序漏洞检测模型，其中保留了程序的流信息。Liu 等人 ([2021b](#bib.bib156)) 介绍了一种具有专家知识的 GNN
    模型，用于检测智能合约中的漏洞，结合了程序的流信息。受图像处理启发，Wu 等人 ([2022b](#bib.bib263)) 提出了一种方法，通过将代码转换为保留语义的图像来增强漏洞检测的可扩展性，并实施
    CNN 以有效捕捉这些信息。
- en: Recently, several works have attempted to explain the results of deep learning
    models for vulnerability detection. Li et al. ([2021c](#bib.bib137)) introduced
    a GNN model for vulnerability detection that allows for interpretability, by providing
    users with parts of the Program Dependency Graph (PDG) that may contain the vulnerability.
    Additionally, Zou et al. ([2021](#bib.bib300)) proposed an interpretable deep-learning-based
    model based on heuristic searching for vulnerability detection.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些研究尝试解释深度学习模型在漏洞检测中的结果。Li et al. ([2021c](#bib.bib137)) 引入了一种GNN模型用于漏洞检测，提供了可解释性，向用户展示可能包含漏洞的程序依赖图
    (PDG) 的部分。此外，Zou et al. ([2021](#bib.bib300)) 提出了一个基于启发式搜索的可解释深度学习模型，用于漏洞检测。
- en: In contrast to vulnerability detection which only classifies a program as vulnerable
    or non-vulnerable, another line of work is bug finding, which aims to pinpoint
    the buggy location. DeepBugs (Pradel and Sen, [2018](#bib.bib190)) is an approach
    for name-based bug detection, which trains a classifier to distinguish buggy or
    non-buggy code, based on deep learning. To enhance the accuracy of bug detection,
    Li et al. ([2019](#bib.bib139)) suggested a fusion method by exploiting both the
    PDG and DFG for better representation. Larger weights are assigned to the buggy
    paths using the attention mechanism to identify the possible vulnerability. Gupta
    et al. ([2019](#bib.bib89)) developed a tree-structured CNN to identify the vulnerabilities
    or faults in a flawed program with respect to a failed test. Li et al. ([2021b](#bib.bib136))
    defined the fault localization problem as image recognition, and provided a deep-learning-based
    approach that integrates code coverage, data dependencies between statements,
    and source code representations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅将程序分类为易受攻击或不易受攻击的漏洞检测相比，另一项工作是漏洞定位，旨在准确找出存在错误的位置。DeepBugs (Pradel and Sen,
    [2018](#bib.bib190)) 是一种基于名称的错误检测方法，它通过深度学习训练分类器来区分有错误或没有错误的代码。为了提高错误检测的准确性，Li
    et al. ([2019](#bib.bib139)) 提出了一种融合方法，利用PDG和DFG进行更好的表示。使用注意机制对错误路径分配更大的权重，以识别可能的漏洞。Gupta
    et al. ([2019](#bib.bib89)) 开发了一种树状结构的CNN，以识别在失败测试中存在缺陷的程序中的漏洞或故障。Li et al. ([2021b](#bib.bib136))
    将故障定位问题定义为图像识别，并提供了一种基于深度学习的方法，结合了代码覆盖率、语句间的数据依赖关系和源代码表示。
- en: 3.4.3\. Type Inference
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3\. 类型推断
- en: 'Programming languages with dynamic typing, like Python and JavaScript, allow
    for rapid prototyping for developers and can save the time of software development
    dramatically. However, without the type information, unexpected run-time errors
    are prone to occur, which may introduce bugs and produce low-quality code. Current
    works on type inference, with the aim of automatically inferring variable types,
    mainly fall into two categories: static-analysis-based and learning-based. Traditional
    static-analysis approaches (Hassan et al., [2018](#bib.bib96); Salib, [2004](#bib.bib202))
    are often imprecise since the behavior of programs is always over-approximated.
    In addition, static-analysis-based approaches typically analyze the dependencies
    of an entire program, resulting in relatively low efficiency.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 动态类型语言，如Python和JavaScript，为开发人员提供了快速原型设计的能力，并能显著节省软件开发时间。然而，缺乏类型信息可能会导致意外的运行时错误，这可能引入漏洞并产生低质量代码。目前的类型推断工作，旨在自动推断变量类型，主要分为两类：静态分析基础和基于学习的。传统的静态分析方法 (Hassan
    et al., [2018](#bib.bib96); Salib, [2004](#bib.bib202)) 通常不够精确，因为程序的行为总是被过度近似。此外，静态分析方法通常分析整个程序的依赖关系，导致相对较低的效率。
- en: Recently, many deep learning techniques have been introduced for type inference.
    To the best of our knowledge, Hellendoorn et al. ([2018](#bib.bib99)) was the
    first to employ deep learning for type inference. They proposed a neural network
    based on sequence-to-sequence architecture, named DeepTyper, which uses GRUs to
    represent the program context and predict the type annotations for TypeScript.
    Furthermore, Malik et al. ([2019](#bib.bib163)) proposed NL2Type to predict type
    annotations by leveraging the natural-language information of programs. Based
    on NL2Type, Pradel et al. ([2020](#bib.bib189)) further proposed TypeWriter, which
    utilizes both the natural-language information and programming context (e.g.,
    arguments usage a function). Wei et al. ([2020a](#bib.bib254)) proposed LambdaNet
    for type inference based on GNNs, which first represents the code in the form
    of a type dependency graph, where typed variables and logical constraints among
    them are preserved. Then a GNN is proposed to propagate and aggregate features
    along related type variables, and eventually, predict the type annotations. Pandi
    et al. ([2020](#bib.bib183)) presented OptTyper, which first extracts relevant
    logical constraints, and shapes type inference as an optimization problem. Allamanis
    et al. ([2020](#bib.bib8)) proposed Typilus for type inference in Python, which
    expands ASTs into a graph structure and predicts type annotations over this graph
    using GNNs. To cope with large-scale type vocabulary, Mir et al. ([2022](#bib.bib168))
    presented Type4Py, a similarity-based deep learning model with type clusters,
    which can support the inference of rare types and user-defined classes. Recently,
    Huang et al. ([2022](#bib.bib106)) formulated the type inference task as a cloze-style
    fill-in-blank problem and then trained a CodeBERT model based on prompt tuning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多深度学习技术被引入用于类型推断。据我们所知，Hellendoorn 等人 ([2018](#bib.bib99)) 是第一个采用深度学习进行类型推断的。他们提出了一种基于序列到序列架构的神经网络，命名为
    DeepTyper，该网络使用 GRU 来表示程序上下文并预测 TypeScript 的类型注释。此外，Malik 等人 ([2019](#bib.bib163))
    提出了 NL2Type，通过利用程序的自然语言信息来预测类型注释。基于 NL2Type，Pradel 等人 ([2020](#bib.bib189)) 进一步提出了
    TypeWriter，该方法结合了自然语言信息和编程上下文（例如，函数中的参数使用）。Wei 等人 ([2020a](#bib.bib254)) 提出了 LambdaNet
    基于 GNN 进行类型推断，该方法首先将代码表示为类型依赖图，其中保留了类型变量及其间的逻辑约束。然后提出了 GNN 来传播和聚合相关类型变量的特征，最终预测类型注释。Pandi
    等人 ([2020](#bib.bib183)) 提出了 OptTyper，该方法首先提取相关逻辑约束，并将类型推断表述为优化问题。Allamanis 等人
    ([2020](#bib.bib8)) 提出了 Typilus 用于 Python 的类型推断，该方法将 AST 扩展为图结构，并使用 GNN 在此图上预测类型注释。为了应对大规模类型词汇，Mir
    等人 ([2022](#bib.bib168)) 提出了 Type4Py，这是一种基于相似性的深度学习模型，具有类型簇，可以支持稀有类型和用户定义类的推断。最近，Huang
    等人 ([2022](#bib.bib106)) 将类型推断任务表述为填空式问题，然后基于提示调整训练了 CodeBERT 模型。
- en: 3.5\. Similarity-based Applications
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 基于相似性的应用
- en: Similarity-based applications, such as code search and code clone detection,
    aim to assess the likeness between a query (in either natural language or programming
    language) and a candidate code snippet. It is important to note that several approaches
    propose to reframe these tasks as a classification problem, where both the code
    and query are concatenated, and the goal is to determine their relatedness (Feng
    et al., [2020](#bib.bib67)). In this paper, we differentiate between similarity-based
    and classification-based applications by the objects they address, namely, the
    query and candidate code snippet. Specifically, similarity-based applications
    center on tasks involving two objects.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 基于相似性的应用，如代码搜索和代码克隆检测，旨在评估查询（无论是自然语言还是编程语言）与候选代码片段之间的相似度。需要注意的是，几种方法提议将这些任务重新表述为分类问题，其中代码和查询被串联在一起，目标是确定它们的相关性
    (Feng et al., [2020](#bib.bib67))。在本文中，我们通过它们所涉及的对象，即查询和候选代码片段，来区分基于相似性和基于分类的应用。具体来说，基于相似性的应用集中在涉及两个对象的任务上。
- en: 3.5.1\. Code Search
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1\. 代码搜索
- en: Code search aims to retrieve a code snippet by a natural-language query (nl-to-code)
    or code query (code-to-code). The nl-to-code search refers to searching code fragments
    that have similar semantics to the natural-language query from a codebase. As
    the first solution for code search using deep learning, Gu et al. ([2018](#bib.bib78))
    proposed DeepCS, which simultaneously learns the source code representation (e.g.,
    function name, parameters and API usage) and the natural-language query in a shared
    feature vector space, with triplet criterion as the objective function. On the
    basis of DeepCS, Wan et al. ([2019](#bib.bib235)) and Deng et al. ([2022](#bib.bib59))
    included more structural information of source code, including the ASTs and CFGs,
    under a multi-modal neural network equipped with an attention mechanism for better
    explainability. Ling et al. ([2021a](#bib.bib146)) first converted code fragments
    and natural-language descriptions into two different graphs, and presented a matching
    technique for better source code and natural-language description matching. Furthermore,
    Shi et al. ([2022c](#bib.bib211)) suggested an improved code search method by
    converting code graphs (e.g., CFGs and PDGs) into sequences through traversing.
    Haldar et al. ([2020](#bib.bib92)) proposed a multi-perspective matching method
    to calculate the similarities among source code and natural-language query from
    multiple perspectives. Cambronero et al. ([2019](#bib.bib34)) empirically evaluated
    the architectures and training techniques when applying deep learning to code
    search. Bui et al. ([2021b](#bib.bib32)) and Li et al. ([2022b](#bib.bib132))
    leveraged contrastive learning with semantics-preserving code transformations
    for better code representation in code search.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 代码搜索的目标是通过自然语言查询（nl-to-code）或代码查询（code-to-code）检索代码片段。nl-to-code 搜索指的是从代码库中搜索与自然语言查询语义相似的代码片段。作为使用深度学习进行代码搜索的首个解决方案，Gu
    等人 ([2018](#bib.bib78)) 提出了 DeepCS，该方法在共享特征向量空间中同时学习源代码表示（例如函数名称、参数和 API 使用）和自然语言查询，目标函数为三元组准则。在
    DeepCS 的基础上，Wan 等人 ([2019](#bib.bib235)) 和 Deng 等人 ([2022](#bib.bib59)) 在配备了注意力机制的多模态神经网络中加入了更多源代码的结构信息，包括
    AST 和 CFG，以提高可解释性。Ling 等人 ([2021a](#bib.bib146)) 首先将代码片段和自然语言描述转换为两种不同的图形，并提出了一种匹配技术，以改善源代码和自然语言描述的匹配。此外，Shi
    等人 ([2022c](#bib.bib211)) 提出了通过遍历将代码图（例如 CFG 和 PDG）转换为序列的改进代码搜索方法。Haldar 等人 ([2020](#bib.bib92))
    提出了多角度匹配方法，以从多个角度计算源代码和自然语言查询之间的相似性。Cambronero 等人 ([2019](#bib.bib34)) 实证评估了将深度学习应用于代码搜索时的架构和训练技术。Bui
    等人 ([2021b](#bib.bib32)) 和 Li 等人 ([2022b](#bib.bib132)) 利用对比学习和语义保留代码转换来改进代码搜索中的代码表示。
- en: Similar but different to the DeepCS framework, several more works have been
    proposed as complements for code search. Yao et al. ([2019](#bib.bib273)) proposed
    using reinforcement learning to first generate the summary of code snippet and
    then use the summary for better code search. Sun et al. ([2022a](#bib.bib216))
    suggested parsing source code to machine instructions, then mapping them into
    natural-language descriptions based on several predefined rules, followed by an
    LSTM-based code search model like DeepCS. Zhu et al. ([2020](#bib.bib296)) considered
    the overlapped substrings between natural-language query and source code, and
    developed a neural network component to represent the overlap matrix for code
    search.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与 DeepCS 框架相似但不同的是，提出了几个补充性的代码搜索方法。Yao 等人 ([2019](#bib.bib273)) 提出了使用强化学习首先生成代码片段的摘要，然后利用该摘要进行更好的代码搜索。Sun
    等人 ([2022a](#bib.bib216)) 建议将源代码解析为机器指令，然后根据几个预定义规则将其映射到自然语言描述中，接着使用类似 DeepCS
    的基于 LSTM 的代码搜索模型。Zhu 等人 ([2020](#bib.bib296)) 考虑了自然语言查询和源代码之间的重叠子字符串，并开发了一个神经网络组件来表示重叠矩阵以进行代码搜索。
- en: Recently, Chai et al. ([2022b](#bib.bib39)) suggested a transfer learning method
    for domain-specific code search, with the aim of transferring knowledge from Python
    to SQL. Wan et al. ([2022b](#bib.bib236)) examined the robustness of different
    neural code search models, and showed that some of them are vulnerable to data-poisoning-based
    backdoor attacks. Gu et al. ([2022](#bib.bib77)) proposed to optimize code search
    by deep hashing techniques.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Chai 等人 ([2022b](#bib.bib39)) 提出了一个用于领域特定代码搜索的迁移学习方法，旨在将知识从 Python 迁移到 SQL。Wan
    等人 ([2022b](#bib.bib236)) 研究了不同神经代码搜索模型的鲁棒性，显示其中一些模型容易受到数据中毒攻击的威胁。Gu 等人 ([2022](#bib.bib77))
    提出了通过深度哈希技术优化代码搜索的方法。
- en: In contrast to nl-to-code search, the input of code-to-code search is source
    code, rather than natural-language description. The objective of the code-to-code
    search is to find code snippets that are semantically related to an input code
    from a codebase. The core technique of code-to-code search is to measure the similarity
    index between two code snippets, which is identical to the process of identifying
    code clones. More related work will be investigated in the code clone detection
    section.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与 nl-to-code 搜索不同，code-to-code 搜索的输入是源代码，而不是自然语言描述。code-to-code 搜索的目标是从代码库中找到与输入代码在语义上相关的代码片段。code-to-code
    搜索的核心技术是测量两个代码片段之间的相似度，这与识别代码克隆的过程是相同的。更多相关的工作将在代码克隆检测部分进行探讨。
- en: 3.5.2\. Code Clone Detection
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2\. 代码克隆检测
- en: 'Numerous software engineering activities, including code reuse, vulnerability
    detection, and code search, rely on detecting similar code snippets (or code clones).
    There are basically four main types of code clones: Type-1 code clones are ones
    that are identical except for spaces, blanks, and comments. Type-2 code clones
    denote identical code snippets except for the variable, type, literal, and function
    names. Type-3 code clones denote two code snippets that are almost identical except
    for a few statements that have been added or removed. Type-4 code clones denote
    heterogeneous code snippets with similar functionality but differing code structures
    or syntax. To handle different types of code clones, various works have been proposed.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 许多软件工程活动，包括代码重用、漏洞检测和代码搜索，都依赖于检测相似的代码片段（或代码克隆）。代码克隆主要有四种类型：Type-1 代码克隆是除了空格、空白和注释外完全相同的克隆。Type-2
    代码克隆是除了变量、类型、字面量和函数名称外完全相同的代码片段。Type-3 代码克隆是两段几乎完全相同的代码片段，只是添加或删除了少数几条语句。Type-4
    代码克隆是功能相似但代码结构或语法不同的异质代码片段。为了处理不同类型的代码克隆，已提出了各种方法。
- en: Recently, several deep-learning-based approaches have been designed for the
    semantics representation of a pair of code snippets for the task of clone detection.
    The core of these approaches lies in representing the source code as distributed
    vectors, in which the semantics are preserved. As an example, White et al. ([2016](#bib.bib257))
    proposed DLC, which comprehends the semantics of source code by considering its
    lexical and syntactic information, and then designs RNNs for representation. To
    improve the representation of the syntactic structure of code, Wei and Li ([2017](#bib.bib253))
    applied TreeLSTM to incorporate AST information of source code. Zhao and Huang
    ([2018](#bib.bib290)) proposed encoding the CFG and DFG of code into a semantic
    matrix, and introduced a deep learning model to match similar code representations.
    Zhang et al. ([2019](#bib.bib286)) and Büch and Andrzejak ([2019](#bib.bib28))
    designed approaches to better represent the ASTs of the program, and applied them
    for code clone detection task. Furthermore, Wang et al. ([2020b](#bib.bib244)),
    Nair et al. ([2020](#bib.bib173)) and Mehrotra et al. ([2021](#bib.bib165)) proposed
    to convert source code into graphs (e.g., CFG), represent the code graphs via
    GNN, and then measure the similarities between them. Instead of using GNN, Wu
    et al. ([2020](#bib.bib262))and Hu et al. ([2022](#bib.bib105)) introduced a centrality
    analysis approach on the flow graph (e.g., CFG) of code for clone detection, inspired
    by social network analysis. Wu et al. ([2022a](#bib.bib260)) considered the nodes
    of an AST as distinct states and constructed a model based on Markov chain to
    convert the tree structure into Markov state transitions. Then, for code clone
    detection, a classifier model is trained on the state transitions. Tufano et al.
    ([2018b](#bib.bib230)) empirically evaluated the effectiveness of learning representation
    from diverse perspectives for code clone detection, including identifiers, ASTs,
    CFGs, and bytecode. Recently, Ding et al. ([2022](#bib.bib64)) and Tao et al.
    ([2022](#bib.bib225)) utilized program transformation techniques to augment the
    training data, and then applied pre-training and contrastive learning techniques
    for clone detection. Gui et al. ([2022](#bib.bib81)) studied a new problem of
    cross-language binary-source code matching by transforming both source and binary
    into LLVM-IRs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，已经设计了几种基于深度学习的方法来表示一对代码片段的语义，以进行克隆检测。这些方法的核心在于将源代码表示为分布式向量，其中保留了语义。例如，White
    等人 ([2016](#bib.bib257)) 提出了DLC，通过考虑源代码的词汇和句法信息来理解其语义，然后设计了RNNs进行表示。为了改进代码的句法结构表示，Wei
    和 Li ([2017](#bib.bib253)) 应用了TreeLSTM来结合源代码的AST信息。Zhao 和 Huang ([2018](#bib.bib290))
    提出了将代码的CFG和DFG编码为语义矩阵，并引入了深度学习模型来匹配相似的代码表示。Zhang 等人 ([2019](#bib.bib286)) 和 Büch
    和 Andrzejak ([2019](#bib.bib28)) 设计了更好地表示程序AST的方法，并将其应用于代码克隆检测任务。此外，Wang 等人 ([2020b](#bib.bib244))、Nair
    等人 ([2020](#bib.bib173)) 和 Mehrotra 等人 ([2021](#bib.bib165)) 提出了将源代码转换为图（例如CFG），通过GNN表示代码图，然后测量它们之间的相似性。Wu
    等人 ([2020](#bib.bib262)) 和 Hu 等人 ([2022](#bib.bib105)) 引入了一种基于社交网络分析的中心性分析方法，应用于代码的流图（例如CFG）以进行克隆检测。Wu
    等人 ([2022a](#bib.bib260)) 将AST的节点视为不同的状态，并基于Markov链构建了一个模型，将树结构转换为Markov状态转移。然后，为了代码克隆检测，基于状态转移训练了一个分类器模型。Tufano
    等人 ([2018b](#bib.bib230)) 从多个角度实证评估了从多样化的表示中学习的有效性，包括标识符、AST、CFG和字节码。最近，Ding 等人
    ([2022](#bib.bib64)) 和 Tao 等人 ([2022](#bib.bib225)) 利用程序转换技术来扩充训练数据，然后应用预训练和对比学习技术进行克隆检测。Gui
    等人 ([2022](#bib.bib81)) 研究了一个新的跨语言二进制源代码匹配问题，通过将源代码和二进制代码都转换为LLVM-IRs。
- en: 3.6\. Generation-based Applications
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 基于生成的应用
- en: Generation-based applications, including code completion, code summarization,
    program translation, program synthesis, and program repair, are designed to produce
    source code, natural-language descriptions, or programs in an alternative programming
    language, in response to specific requirements presented in either natural language
    or (partial) code.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成的应用，包括代码补全、代码摘要、程序翻译、程序合成和程序修复，旨在生成源代码、自然语言描述或另一种编程语言中的程序，以响应以自然语言或（部分）代码形式呈现的具体需求。
- en: 3.6.1\. Code Completion
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1\. 代码补全
- en: Code completion is a core feature of most modern IDEs. It offers the developers
    a list of possible code hints based on available information. Raychev et al. ([2014](#bib.bib198))
    made the first attempt to combine the program analysis with neural language models
    for better code completion. It first extracts the abstract histories of programs
    through program analysis, and then learns the probabilities of histories via an
    RNN-based neural language model. Similarly, various works (Liu et al., [2016b](#bib.bib149);
    Li et al., [2018a](#bib.bib129); Svyatkovskiy et al., [2019](#bib.bib223)) resort
    to inferring the next code token over the partial AST, by first traversing the
    AST in a depth-first order, and then introducing an RNN-based neural language
    model. To better represent the structure of code, Kim et al. ([2021](#bib.bib118))
    suggested predicting the missing partial code by feeding the ASTs to Transformers.
    Alon et al. ([2020](#bib.bib13)) presented a structural model for code completion,
    which represents code by sampling paths from an incomplete AST. Furthermore, Wang
    and Li ([2021](#bib.bib248)) suggested a GNN-based approach for code completion,
    which parses the flattened sequence of an AST into a graph, and represents it
    using Gated Graph Neural Networks (GGNNs) (Li et al., [2016](#bib.bib134)). Guo
    et al. ([2022c](#bib.bib85)) modeled the problem of code completion as filling
    in a hole, and developed a Transformer model guided by the grammar file of a specified
    programming language. Brockschmidt et al. ([2019](#bib.bib25)) expanded incomplete
    code into a graph representation, and then proposed a GNN for code completion.
    Svyatkovskiy et al. ([2020](#bib.bib221)) proposed IntelliCode Compose, a pre-trained
    language model of code based on GPT-2, providing instant code completion across
    different programming languages. Liu et al. ([2020b](#bib.bib150), [c](#bib.bib151))
    proposed a multi-task learning framework that unifies the code completion and
    type inference tasks into one overall framework. Lu et al. ([2022](#bib.bib159))
    suggested a retrieval-augmented code completion method that retrieves similar
    code snippets from a code corpus and then uses them as external context.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 代码补全是大多数现代集成开发环境（IDEs）的核心功能。它根据可用信息为开发者提供可能的代码提示列表。Raychev 等人（[2014](#bib.bib198)）首次尝试将程序分析与神经语言模型结合，以实现更好的代码补全。该方法首先通过程序分析提取程序的抽象历史，然后通过基于
    RNN 的神经语言模型学习这些历史的概率。同样，各种研究（Liu 等人，[2016b](#bib.bib149)；Li 等人，[2018a](#bib.bib129)；Svyatkovskiy
    等人，[2019](#bib.bib223)）通过首先以深度优先顺序遍历抽象语法树（AST），然后引入基于 RNN 的神经语言模型，来推断下一个代码符号。为了更好地表示代码结构，Kim
    等人（[2021](#bib.bib118)）建议通过将 AST 输入 Transformers 来预测缺失的部分代码。Alon 等人（[2020](#bib.bib13)）提出了一种结构化的代码补全模型，该模型通过从不完整的
    AST 中采样路径来表示代码。此外，Wang 和 Li（[2021](#bib.bib248)）建议了一种基于 GNN 的代码补全方法，该方法将 AST 的扁平化序列解析为图，并使用门控图神经网络（GGNNs）（Li
    等人，[2016](#bib.bib134)）进行表示。Guo 等人（[2022c](#bib.bib85)）将代码补全问题建模为填补空白，并开发了一个由指定编程语言的语法文件引导的
    Transformer 模型。Brockschmidt 等人（[2019](#bib.bib25)）将不完整的代码扩展为图表示，然后提出了一个用于代码补全的
    GNN。Svyatkovskiy 等人（[2020](#bib.bib221)）提出了 IntelliCode Compose，这是一个基于 GPT-2 的预训练代码语言模型，提供跨编程语言的即时代码补全。Liu
    等人（[2020b](#bib.bib150)，[c](#bib.bib151)）提出了一个多任务学习框架，将代码补全和类型推断任务统一为一个整体框架。Lu
    等人（[2022](#bib.bib159)）建议了一种检索增强的代码补全方法，该方法从代码库中检索类似的代码片段，然后将其用作外部上下文。
- en: Since instant code completion is desired, several studies aim to improve the
    efficiency and flexibility of code completion. Svyatkovskiy et al. ([2021](#bib.bib222))
    suggested improving the efficiency of neural network models for code completion
    by reshaping the problem from generation to ranking the candidates from static
    analysis. Additionally, Shrivastava et al. ([2020](#bib.bib212)) proposed a code
    completion approach that supports fast adaption to an unseen file based on meta-learning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对即时代码补全的需求，几项研究旨在提高代码补全的效率和灵活性。Svyatkovskiy 等人（[2021](#bib.bib222)）建议通过将问题从生成转变为从静态分析中对候选项进行排序来提高神经网络模型的效率。此外，Shrivastava
    等人（[2020](#bib.bib212)）提出了一种支持基于元学习对未见文件进行快速适应的代码补全方法。
- en: 3.6.2\. Code Summarization
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2\. 代码摘要
- en: Inspired by the text generation work in NLP, many approaches have been put forward
    to systematically generate a description or function name to summarize the semantics
    of source code. To the best of our knowledge, Allamanis et al. ([2016](#bib.bib11))
    were the first to use deep learning for code summarization. They designed a CNN
    to represent the code and applied a hybrid breath-first search and beam search
    to predict the tokens of function name. Concurrently, Iyer et al. ([2016](#bib.bib108))
    proposed an LSTM-based sequence-to-sequence network with an attention mechanism
    for generating descriptions for source code. The sequence-to-sequence network (Iyer
    et al., [2016](#bib.bib108)) inspired a line of works for code summarization,
    distinguished in code representation learning. To represent the AST information,
    Hu et al. ([2018a](#bib.bib103)), Alon et al. ([2018](#bib.bib12)), and LeClair
    et al. ([2019](#bib.bib124)) proposed to linearize the ASTs via traversing or
    path sampling, and used RNNs to represent the sequential AST traversals/paths
    for code summarization. Likewise, Fernandes et al. ([2019](#bib.bib68)), LeClair
    et al. ([2020](#bib.bib123)) and Jin et al. ([2022](#bib.bib115)) investigated
    representing the structure of source code via a GNN, and verified it in code summarization.
    Guo et al. ([2022a](#bib.bib86)) designed the triplet position to model hierarchies
    in the syntax structure of source code for better code summarization. Recently,
    several works (Ahmad et al., [2020](#bib.bib5); Wu et al., [2021](#bib.bib259);
    Gong et al., [2022](#bib.bib75); Tang et al., [2022](#bib.bib224)) proposed to
    improve code summarization by designing enhanced Transformers to better capture
    the structural information of code (i.e., ASTs). Wan et al. ([2018](#bib.bib238)),
    Shi et al. ([2021](#bib.bib208)), Yang et al. ([2021](#bib.bib271)), Gao and Lyu
    ([2022](#bib.bib71)), and Wang et al. ([2022a](#bib.bib246)) proposed a hybrid
    representation approach by combining the embeddings of sequential code tokens
    and structured ASTs, and feeding them into a decoder network to generate summaries.
    As a complement, Haque et al. ([2020](#bib.bib94)) and Bansal et al. ([2021](#bib.bib17))
    advanced the performance of code summarization by integrating the context of summarized
    code, which contains important hints for comprehending subroutines of code. Shahbazi
    et al. ([2021](#bib.bib206)) leveraged the API documentation as a knowledge resource
    for better code summarization. Instead of generating a sequence of summary tokens
    at once, Ciurumelea et al. ([2020](#bib.bib53)) resorted to suggesting code comment
    completions based on neural language modeling. Lin et al. ([2021](#bib.bib143))
    proposed to improve the code summarization by splitting the AST under the guidance
    of CFG, which can decrease the AST size and make model training easier.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 受 NLP 中文本生成工作的启发，许多方法被提出系统地生成描述或函数名称，以总结源代码的语义。据我们所知，Allamanis 等人（[2016](#bib.bib11)）是首批使用深度学习进行代码总结的研究者。他们设计了一个
    CNN 来表示代码，并应用了混合的广度优先搜索和束搜索来预测函数名称的标记。同时，Iyer 等人（[2016](#bib.bib108)）提出了一种基于 LSTM
    的序列到序列网络，结合注意力机制来生成源代码的描述。Iyer 等人（[2016](#bib.bib108)）提出的序列到序列网络启发了一系列代码总结的工作，在代码表示学习中表现突出。为了表示
    AST 信息，Hu 等人（[2018a](#bib.bib103)）、Alon 等人（[2018](#bib.bib12)）和 LeClair 等人（[2019](#bib.bib124)）建议通过遍历或路径采样将
    AST 线性化，并使用 RNNs 表示顺序 AST 遍历/路径用于代码总结。同样，Fernandes 等人（[2019](#bib.bib68)）、LeClair
    等人（[2020](#bib.bib123)）和 Jin 等人（[2022](#bib.bib115)）研究了通过 GNN 表示源代码结构，并在代码总结中进行了验证。Guo
    等人（[2022a](#bib.bib86)）设计了三元组位置来建模源代码语法结构中的层次关系，以实现更好的代码总结。最近，一些研究（Ahmad 等人，[2020](#bib.bib5)；Wu
    等人，[2021](#bib.bib259)；Gong 等人，[2022](#bib.bib75)；Tang 等人，[2022](#bib.bib224)）通过设计增强型
    Transformer 来更好地捕捉代码的结构信息（即 ASTs）来改进代码总结。Wan 等人（[2018](#bib.bib238)）、Shi 等人（[2021](#bib.bib208)）、Yang
    等人（[2021](#bib.bib271)）、Gao 和 Lyu（[2022](#bib.bib71)）以及 Wang 等人（[2022a](#bib.bib246)）提出了一种混合表示方法，通过将顺序代码标记和结构化
    AST 的嵌入结合起来，并将其输入到解码器网络中生成摘要。作为补充，Haque 等人（[2020](#bib.bib94)）和 Bansal 等人（[2021](#bib.bib17)）通过整合总结代码的上下文来提高代码总结的性能，这些上下文包含理解代码子例程的重要提示。Shahbazi
    等人（[2021](#bib.bib206)）利用 API 文档作为知识资源，以实现更好的代码总结。Ciurumelea 等人（[2020](#bib.bib53)）没有一次生成一系列摘要标记，而是基于神经语言建模建议代码注释补全。Lin
    等人（[2021](#bib.bib143)）提出通过在 CFG 指导下拆分 AST 来改进代码总结，这可以减少 AST 大小并简化模型训练。
- en: Another line of work aims to utilize code search to enhance the quality of code
    summaries generated by deep learning models. For example, Zhang et al. ([2020c](#bib.bib285)),
    Wei et al. ([2020b](#bib.bib252)), Liu et al. ([2020a](#bib.bib155)) and Li et al.
    ([2021a](#bib.bib128)) suggested augmenting the provided code snippet by searching
    similar source code snippets together with their comments, for better code summarization.
    Instead of acquiring the retrieved samples in advance, Zhu et al. ([2022](#bib.bib298))
    suggested a simple retrieval-based method for the task of code summarization,
    which estimates a probability distribution for generating each token given the
    current translation context.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作旨在利用代码搜索来增强深度学习模型生成的代码摘要的质量。例如，张等人（[2020c](#bib.bib285)）、魏等人（[2020b](#bib.bib252)）、刘等人（[2020a](#bib.bib155)）和李等人（[2021a](#bib.bib128)）建议通过搜索类似的源代码片段及其注释来增强所提供的代码片段，以获得更好的代码摘要。朱等人（[2022](#bib.bib298)）建议了一个简单的基于检索的方法用于代码摘要任务，该方法估计了在当前翻译上下文中生成每个标记的概率分布，而不是事先获取检索到的样本。
- en: Apart from the above approaches, several works (Hu et al., [2018b](#bib.bib104);
    Xie et al., [2022](#bib.bib265); Wei et al., [2019](#bib.bib251); Yang et al.,
    [2022a](#bib.bib269); Ye et al., [2020](#bib.bib275)) are also worthy to be mentioned.
    Hu et al. ([2018b](#bib.bib104)) transferred the code API information as additional
    knowledge to the code summarization task. Xie et al. ([2022](#bib.bib265)) studied
    a new task of project-specific code summarization with limited historical code
    summaries via meta-transfer learning. Wei et al. ([2019](#bib.bib251)) and Yang
    et al. ([2022a](#bib.bib269)) viewed the code generation task as a dual of code
    summarization, and incorporated dual learning for a better summary generation.
    Similarly, Ye et al. ([2020](#bib.bib275)) leveraged code generation for code
    search and code summarization through dual learning as well. Mu et al. ([2022](#bib.bib171))
    introduced a multi-pass deliberation framework for code summarization, inspired
    by human cognitive processes. Xie et al. ([2021](#bib.bib266)) proposed a multi-task
    learning framework by leveraging method name suggestion as an auxiliary task to
    improve code summarization. Haque et al. ([2021](#bib.bib93)) emphasized that
    predicting the action word (always the first word) is an important intermediate
    problem in order to generate improved code summaries. Recently, the consistency
    between source code and comments has also attracted much attention, which is critical
    to ensure the quality of software. Liu et al. ([2019](#bib.bib153)), Panthaplackel
    et al. ([2021](#bib.bib184)), and Nguyen et al. ([2020](#bib.bib176)) trained
    a deep-learning-based classifier to determine whether or not the function body
    and function name are consistent. Panthaplackel et al. ([2020](#bib.bib185)) and
    Liu et al. ([2020d](#bib.bib157)) proposed automatically updating an existing
    comment when the related code is modified, as revealed in the commit histories.
    Gao et al. ([2021](#bib.bib72)) proposed to automate the removal of obsolete TODO
    comments by representing the semantic features of TODO comments, code changes,
    and commit messages using neural networks. Li et al. ([2022e](#bib.bib130)) proposed
    to generate review comments automatically based on pre-trained code models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述方法，还有几项工作（胡等人，[2018b](#bib.bib104)；谢等人，[2022](#bib.bib265)；魏等人，[2019](#bib.bib251)；杨等人，[2022a](#bib.bib269)；叶等人，[2020](#bib.bib275)）也值得一提。胡等人（[2018b](#bib.bib104)）将代码
    API 信息作为附加知识传递给了代码摘要任务。谢等人（[2022](#bib.bib265)）通过元迁移学习研究了具有有限历史代码摘要的项目特定代码摘要的新任务。魏等人（[2019](#bib.bib251)）和杨等人（[2022a](#bib.bib269)）将代码生成任务视为代码摘要的对偶，并采用对偶学习来获得更好的摘要生成。同样，叶等人（[2020](#bib.bib275)）也利用对偶学习来进行代码搜索和代码摘要的代码生成。穆等人（[2022](#bib.bib171)）引入了受人类认知过程启发的代码摘要多通道思考框架。谢等人（[2021](#bib.bib266)）提出了一个多任务学习框架，通过利用方法名称建议作为辅助任务来改进代码摘要。哈克等人（[2021](#bib.bib93)）强调，预测动作词（通常是第一个词）是生成改进的代码摘要的重要中间问题。最近，源代码和注释之间的一致性也引起了很多关注，这对于确保软件质量至关重要。刘等人（[2019](#bib.bib153)）、潘塔普拉克等人（[2021](#bib.bib184)）和阮等人（[2020](#bib.bib176)）训练了一个基于深度学习的分类器，用于确定函数体和函数名称是否一致。潘塔普拉克等人（[2020](#bib.bib185)）和刘等人（[2020d](#bib.bib157)）提出了在相关代码被修改时自动更新现有注释的方法，这可以从提交历史中得知。高等人（[2021](#bib.bib72)）提出利用神经网络来表示TODO注释、代码更改和提交消息的语义特征，从而自动删除过时的TODO注释。李等人（[2022e](#bib.bib130)）提出基于预训练代码模型自动生成评论的方法。
- en: 3.6.3\. Program Translation
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.3\. 程序翻译
- en: Translating programs from a deprecated programming language to a modern one
    is important for software maintenance. Many neural machine translation-based methods
    have been proposed for program translation. In order to utilize the AST structure
    of code, Chen et al. ([2018](#bib.bib44)) proposed Tree2Tree, a neural network
    with structural information preserved. It first converts ASTs into binary trees
    following the left-child right-sibling rule, and then feeds them into an encoder-decoder
    model equipped with TreeLSTM. Gu et al. ([2017](#bib.bib80)) presented DeepAM,
    which can extract API mappings among programming languages without the need of
    bilingual projects. Recently, Rozière et al. ([2020](#bib.bib200)) proposed TransCoder,
    a neural program translator based on unsupervised machine translation. Furthermore,
    Rozière et al. ([2022](#bib.bib201)) leveraged the automated unit tests to filter
    out invalid translations for unsupervised program translation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将过时的编程语言转换为现代编程语言对于软件维护非常重要。许多基于神经机器翻译的方法已经被提出用于程序翻译。为了利用代码的AST结构，Chen等人（[2018](#bib.bib44)）提出了Tree2Tree，这是一种保留结构信息的神经网络。它首先将AST转换为遵循左孩子右兄弟规则的二叉树，然后将其输入到配备TreeLSTM的编码器-解码器模型中。Gu等人（[2017](#bib.bib80)）提出了DeepAM，该方法可以在不需要双语项目的情况下提取编程语言之间的API映射。最近，Rozière等人（[2020](#bib.bib200)）提出了TransCoder，这是一种基于无监督机器翻译的神经程序翻译器。此外，Rozière等人（[2022](#bib.bib201)）利用自动化单元测试来筛选无监督程序翻译中的无效翻译。
- en: 3.6.4\. Program Synthesis
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.4\. 程序合成
- en: Program synthesis is a task for generating source code using high-level specifications
    (e.g., program descriptions or input-output samples). Given the natural-language
    inputs, current approaches resort to generating programs through machine translation.
    For semantic parsing, Dong and Lapata ([2016](#bib.bib65)) proposed an attention-based
    encoder-decoder model, which first encodes input natural language into a vector
    representation using an RNN, and then incorporates another tree-based RNN to generate
    programs. Liu et al. ([2016a](#bib.bib148)) proposed latent attention for the
    If-Then program synthesis, which can effectively learn the importance of words
    in natural-language descriptions. Beltagy and Quirk ([2016](#bib.bib20)) modeled
    the generation of If-Then programs from natural-language descriptions as a structure
    prediction problem, and investigated both neural network and logistic regression
    models for this problem.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 程序合成是一项任务，旨在使用高级规格（例如程序描述或输入-输出示例）生成源代码。对于自然语言输入，目前的方法依赖于通过机器翻译生成程序。对于语义解析，Dong和Lapata（[2016](#bib.bib65)）提出了一种基于注意力的编码器-解码器模型，该模型首先使用RNN将输入自然语言编码为向量表示，然后结合另一种基于树的RNN来生成程序。Liu等人（[2016a](#bib.bib148)）提出了用于If-Then程序合成的潜在注意力机制，这可以有效地学习自然语言描述中单词的重要性。Beltagy和Quirk（[2016](#bib.bib20)）将从自然语言描述生成If-Then程序建模为一个结构预测问题，并研究了神经网络和逻辑回归模型在此问题上的表现。
- en: Unlike synthesizing simple If-Then programs, Yin and Neubig ([2017](#bib.bib277))
    proposed a syntax-preserving model for general-purpose programming languages,
    which generates Python code from pseudo code, powered by a grammar model that
    explicitly captures the compilation rules. Maddison and Tarlow ([2014](#bib.bib162))
    proposed a probabilistic model based on probabilistic context-free grammars (PCFGs)
    for capturing the structure of code for code generation. Ling et al. ([2016](#bib.bib145))
    collected two datasets (i.e., Hearthstone and Magic the Gathering) for code generation
    in trading card games, and proposed a probabilistic neural network with multiple
    predictors. On the basis of (Ling et al., [2016](#bib.bib145)), Rabinovich et al.
    ([2017](#bib.bib193)) proposed to incorporate the structural constraints on outputs
    into a decoder network for executable code generation. Similarly, Sun et al. ([2019](#bib.bib219))
    and Sun et al. ([2020](#bib.bib220)) designed a tree-based CNN and Transformer,
    respectively, for code generation and semantic parsing tasks based on the sequence-to-sequence
    framework. Hayati et al. ([2018](#bib.bib98)) suggested using a neural code generation
    model to retrieve action subtrees at test time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与合成简单的 If-Then 程序不同，Yin 和 Neubig（[2017](#bib.bib277)）提出了一种用于通用编程语言的语法保留模型，该模型通过一个明确捕捉编译规则的语法模型从伪代码生成
    Python 代码。Maddison 和 Tarlow（[2014](#bib.bib162)）提出了一种基于概率上下文无关文法（PCFGs）的概率模型，用于捕捉代码的结构以进行代码生成。Ling
    等人（[2016](#bib.bib145)）收集了两个数据集（即 Hearthstone 和 Magic the Gathering），用于交易卡片游戏中的代码生成，并提出了一种具有多个预测器的概率神经网络。在
    (Ling 等人，[2016](#bib.bib145)) 的基础上，Rabinovich 等人（[2017](#bib.bib193)）提出将输出的结构约束纳入可执行代码生成的解码器网络中。同样，Sun
    等人（[2019](#bib.bib219)）和 Sun 等人（[2020](#bib.bib220)）分别设计了基于树的 CNN 和 Transformer，用于基于序列到序列框架的代码生成和语义解析任务。Hayati
    等人（[2018](#bib.bib98)）建议使用神经代码生成模型在测试时检索动作子树。
- en: Instead of synthesizing programs from natural-language descriptions, several
    works resort to generating programs from the (pseudo) program in another format
    or language. Iyer et al. ([2018](#bib.bib109)) proposed to synthesize the AST
    derivation of source code given descriptions as well as the programmatic contexts.
    The above approaches are driven by well-labeled training examples, while Nan et al.
    ([2020](#bib.bib174)) proposed a novel approach to program synthesis without using
    any training example, inspired by how humans learn to program.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与从自然语言描述合成程序不同，多个研究工作采用从另一种格式或语言中的（伪）程序生成程序。Iyer 等人（[2018](#bib.bib109)）提出了在给定描述和程序上下文的情况下合成源代码的抽象语法树（AST）推导。这些方法依赖于标注良好的训练示例，而
    Nan 等人（[2020](#bib.bib174)）提出了一种不使用任何训练示例的新颖程序合成方法，这一方法受到人类学习编程方式的启发。
- en: Recently, various pre-trained code models also achieved significant progress
    in code generation. CodeGPT (Lu et al., [2021](#bib.bib160)) is a Transformer-based
    model that is trained using corpus for program synthesis, following the same architecture
    of GPT-2. CodeT5 (Wang et al., [2021a](#bib.bib249)) is a pre-trained code model
    in eight programming languages based on T5 (Raffel et al., [2020](#bib.bib194)),
    which incorporates an identifier-aware objective during its pre-training phase.
    Xu et al. ([2020](#bib.bib267)) endeavored to integrate external knowledge into
    the pre-training phase to enhance code generation from natural-language input.
    Codex (Chen et al., [2021b](#bib.bib43)) is a GPT model trained on a code corpus
    sourced from GitHub. This model has played a pivotal role as the underpinning
    framework for Copilot^(13)^(13)13[https://github.com/features/copilot](https://github.com/features/copilot).
    Li et al. ([2022a](#bib.bib133)) introduced AlphaCode, a code generation system
    designed to produce distinctive solutions for complex problems that demand profound
    cognitive engagement. Poesia et al. ([2022](#bib.bib187)) introduced a constrained
    semantic decoding mechanism into a pre-trained model, as to constrain outputs
    of the model in a set of valid programs. More recently, the code generation has
    been dominated by the LLMs, including CodeGen (Nijkamp et al., [2022](#bib.bib178)),
    CodeT5+ (Wang et al., [2023](#bib.bib247)), InCoder (Fried et al., [2022](#bib.bib69)),
    GPT-3.5 (OpenAI, [2022](#bib.bib181)), StarCoder (Li et al., [2023a](#bib.bib131)),
    Code Llama (Roziere et al., [2023](#bib.bib199)), and WizardCoder (Luo et al.,
    [2023](#bib.bib161)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，各种预训练代码模型在代码生成方面也取得了显著进展。CodeGPT (Lu et al., [2021](#bib.bib160)) 是一种基于 Transformer
    的模型，使用程序合成语料库进行训练，遵循 GPT-2 的相同架构。CodeT5 (Wang et al., [2021a](#bib.bib249)) 是一个基于
    T5 (Raffel et al., [2020](#bib.bib194)) 的预训练代码模型，涵盖八种编程语言，并在其预训练阶段引入了识别符感知目标。Xu
    et al. ([2020](#bib.bib267)) 努力将外部知识整合到预训练阶段，以增强从自然语言输入生成代码的能力。Codex (Chen et al.,
    [2021b](#bib.bib43)) 是一个基于 GitHub 的代码语料库训练的 GPT 模型。该模型作为 Copilot^(13)^(13)13[https://github.com/features/copilot](https://github.com/features/copilot)
    的基础框架发挥了关键作用。Li et al. ([2022a](#bib.bib133)) 介绍了 AlphaCode，这是一个设计用于产生针对复杂问题的独特解决方案的代码生成系统，这些问题要求深刻的认知参与。Poesia
    et al. ([2022](#bib.bib187)) 在预训练模型中引入了一种受限语义解码机制，以约束模型输出在一组有效程序中。最近，代码生成领域由大型语言模型主导，包括
    CodeGen (Nijkamp et al., [2022](#bib.bib178))、CodeT5+ (Wang et al., [2023](#bib.bib247))、InCoder (Fried
    et al., [2022](#bib.bib69))、GPT-3.5 (OpenAI, [2022](#bib.bib181))、StarCoder (Li
    et al., [2023a](#bib.bib131))、Code Llama (Roziere et al., [2023](#bib.bib199))
    和 WizardCoder (Luo et al., [2023](#bib.bib161))。
- en: Programming by example is another flourishing direction for program synthesis.
    Shu and Zhang ([2017](#bib.bib213)) proposed a Neural Programming By Example (NPBE)
    model, which learns to solve string manipulation problems through inducting from
    input-output strings. Balog et al. ([2017](#bib.bib16)) proposed DeepCoder, which
    trains a model to predict possible functions useful in the program space, as to
    guide the conventional search-based synthesizer. Devlin et al. ([2017](#bib.bib62))
    proposed RobustFill, which is an end-to-end neural network for synthesising programs
    from input-output examples. Nye et al. ([2019](#bib.bib180)) developed a neuro-symbolic
    program synthesis system called SketchAdapt, which can build programs from input-output
    samples and code descriptions by intermediate sketch. Bavishi et al. ([2019](#bib.bib19))
    proposed a program candidate generator, backed by GNNs, for program synthesis
    in large real-world API.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以示例编程为另一种蓬勃发展的程序合成方向。Shu 和 Zhang ([2017](#bib.bib213)) 提出了 Neural Programming
    By Example (NPBE) 模型，该模型通过从输入输出字符串中推导，学习解决字符串操作问题。Balog et al. ([2017](#bib.bib16))
    提出了 DeepCoder，该模型训练以预测在程序空间中有用的可能函数，从而指导传统的基于搜索的合成器。Devlin et al. ([2017](#bib.bib62))
    提出了 RobustFill，这是一个端到端神经网络，用于从输入输出示例中合成程序。Nye et al. ([2019](#bib.bib180)) 开发了一个称为
    SketchAdapt 的神经符号程序合成系统，该系统可以通过中间草图从输入输出样本和代码描述中构建程序。Bavishi et al. ([2019](#bib.bib19))
    提出了一个程序候选生成器，依靠 GNNs，旨在大型实际 API 中进行程序合成。
- en: It is worth mentioning that there are many works on generating code from natural
    language for specific domain-specific programming languages, e.g., Bash and SQL.
    WikiSQL (Zhong et al., [2017](#bib.bib292)), Spider (Yu et al., [2018b](#bib.bib280)),
    SparC (Yu et al., [2019b](#bib.bib281)), and CoSQL (Yu et al., [2019a](#bib.bib279))
    are four datasets with human annotations for the task of text-to-SQL. Based on
    these datasets, many works (Yu et al., [2018a](#bib.bib278), [2019b](#bib.bib281),
    [2019a](#bib.bib279)) have been proposed. For example, Seq2SQL (Zhong et al.,
    [2017](#bib.bib292)) is a neural machine translation model to generate SQL queries
    from natural-language descriptions with reinforcement learning. Cai et al. ([2018](#bib.bib33))
    further proposed an encoder-decoder framework to translate natural language into
    SQL queries, which integrates the grammar structure of SQL for better generation.
    Yu et al. ([2018a](#bib.bib278)) proposed a neural network SyntaxSQLNet, with
    syntax tree preserved, for the task of text-to-SQL translation across different
    domains, which takes the syntax tree of SQL into account during generation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，有许多研究致力于从自然语言生成特定领域的编程语言代码，例如 Bash 和 SQL。WikiSQL (Zhong et al., [2017](#bib.bib292))、Spider (Yu
    et al., [2018b](#bib.bib280))、SparC (Yu et al., [2019b](#bib.bib281)) 和 CoSQL (Yu
    et al., [2019a](#bib.bib279)) 是四个针对文本到 SQL 任务的人工注释数据集。基于这些数据集，提出了许多研究 (Yu et al.,
    [2018a](#bib.bib278)、[2019b](#bib.bib281)、[2019a](#bib.bib279))。例如，Seq2SQL (Zhong
    et al., [2017](#bib.bib292)) 是一个神经机器翻译模型，通过强化学习从自然语言描述生成 SQL 查询。Cai et al. ([2018](#bib.bib33))
    进一步提出了一种编码器-解码器框架来将自然语言翻译成 SQL 查询，集成了 SQL 的语法结构以优化生成。Yu et al. ([2018a](#bib.bib278))
    提出了一个神经网络 SyntaxSQLNet，保留了语法树，用于跨领域的文本到 SQL 翻译任务，在生成过程中考虑了 SQL 的语法树。
- en: 3.6.5\. Program Repair
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.5\. 程序修复
- en: Automatically localizing and repairing bugs in programs can save much manual
    effort in software development (Jiang et al., [2019](#bib.bib112)). One line of
    work is to learn the patterns of how programmers edit the source code, which can
    be used to check syntax errors while compiling. Bhatia and Singh ([2016](#bib.bib23))
    and Santos et al. ([2018](#bib.bib203)) proposed RNN-based language models for
    correcting syntax errors in programs. DeepFix (Gupta et al., [2017](#bib.bib90))
    and SequenceR (Chen et al., [2019](#bib.bib46)) are two sequence-to-sequence models
    for syntax error correction, by translating the erroneous programs into fixed
    ones. Furthermore, Gupta et al. ([2018](#bib.bib88)) improved program repair by
    reinforcement learning. Vasic et al. ([2018](#bib.bib231)) proposed multi-headed
    pointer networks (one head each for localization and repair) for jointly localizing
    and repairing misused variables in code. Dinella et al. ([2020](#bib.bib63)) presented
    Hoppity to jointly detect and fix bugs based on neural Turing machine (Graves
    et al., [2014](#bib.bib76)), where a GNN-based memory unit is designed for buggy
    program representation, and an LSTM-based central controller is designed to predict
    the operations of bug fixing, e.g., patch generation and type prediction. Tarlow
    et al. ([2020](#bib.bib226)) proposed Graph2Diff, which designs a GNN for representing
    the graph structure of programs, and a pointer network to localize the initial
    AST to be edited. Mesbah et al. ([2019](#bib.bib166)) and Chakraborty et al. ([2020](#bib.bib40))
    proposed to model the modifications of ASTs, and designed a neural machine translation
    model to generate correct patches. Zhu et al. ([2021](#bib.bib297)) presented
    a syntax-directed decoder network with placeholder generation for program repair,
    which aims to generate program modifications rather than the target code. Yasunaga
    and Liang ([2020](#bib.bib274)) proposed DrRepair, which first builds a program-feedback
    graph to align the corresponding symbols and diagnostic feedback, and then designs
    a GNN to generate repaired code. Li et al. ([2022d](#bib.bib138)) introduced a
    novel deep learning-based method for fixing general bugs, which combines spectrum-based
    fault localization with deep learning and flow analysis.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 自动定位和修复程序中的错误可以节省软件开发中大量的人力。其中一种工作方法是学习程序员如何编辑源代码的模式，这可以用于在编译时检查语法错误。Bhatia
    和 Singh ([2016](#bib.bib23)) 以及 Santos 等人 ([2018](#bib.bib203)) 提出了基于 RNN 的语言模型，用于矫正程序中的语法错误。DeepFix (Gupta
    等人，[2017](#bib.bib90)) 和 SequenceR (Chen 等人，[2019](#bib.bib46)) 是用于语法错误校正的两个序列到序列模型，它们会将错误的程序翻译成正确的程序。此外，Gupta
    等人 ([2018](#bib.bib88)) 通过强化学习改进了程序修复。Vasic 等人 ([2018](#bib.bib231)) 提出了多头指针网络（一个用于定位，一个用于修复）来联合定位和修复代码中被误用的变量。Dinella
    等人 ([2020](#bib.bib63)) 提出了一种名为 Hoppity 的方法，基于神经图灵机 (Graves 等人，[2014](#bib.bib76))，来共同检测和修复
    bug。其中，一个基于 GNN 的记忆单元用于表示错误的程序，一个基于 LSTM 的中央控制器用于预测 bug 修复的操作，例如补丁生成和类型预测。Tarlow
    等人 ([2020](#bib.bib226)) 提出了 Graph2Diff，该方法设计了一个 GNN 来表示程序的图结构，并设计了一个指针网络来定位需要编辑的初始
    AST。Mesbah 等人 ([2019](#bib.bib166)) 以及 Chakraborty 等人 ([2020](#bib.bib40)) 提出了对
    AST 修改的建模方法，并设计了一个神经机器翻译模型来生成正确的补丁。Zhu 等人 ([2021](#bib.bib297)) 提出了一个面向语法的解码器网络，其中包括了生成程序修改而不是目标代码的占位符。Yasunaga
    和 Liang ([2020](#bib.bib274)) 提出了 DrRepair，首先构建了一个程序反馈图，以对齐相应的符号和诊断反馈，然后设计了一个
    GNN 来生成修复的代码。Li 等人 ([2022d](#bib.bib138)) 提出了一种用于修复一般 bug 的新型基于深度学习的方法，将频谱故障定位与深度学习和流分析相结合。
- en: Benefiting from the pre-training techniques in NLP, TFix (Berabi et al., [2021](#bib.bib22))
    and VulRepair (Fu et al., [2022](#bib.bib70)) directly posed program repair as
    a text-to-text problem and utilized a model named T5 (Raffel et al., [2020](#bib.bib194)).
    Specifically, it digests the error message and directly outputs the correct code.
    Jiang et al. ([2021a](#bib.bib113)) proposed CURE for program repair, which is
    composed of a pre-trained language model, a code-aware search method, and a sub-word
    tokenization technique.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 受益于自然语言处理中的预训练技术，TFix (Berabi 等人，[2021](#bib.bib22)) 和 VulRepair (Fu 等人，[2022](#bib.bib70))
    直接将程序修复作为文本到文本问题，并利用了一种名为 T5 (Raffel 等人，[2020](#bib.bib194)) 的模型。具体而言，它会分析错误消息并直接输出正确的代码。Jiang
    等人 ([2021a](#bib.bib113)) 提出了用于程序修复的 CURE，该方法由一个预训练语言模型、一个代码感知搜索方法和一个子词标记化技术组成。
- en: Another line of work is focusing on repairing programs by generating patches.
    Tufano et al. ([2018a](#bib.bib229)) carried out an empirical study to evaluate
    the viability of applying machine translation to generate patches for program
    repair in real-world scenarios. Different from (Tufano et al., [2018a](#bib.bib229))
    which targets at function-level small code snippets, Hata et al. ([2018](#bib.bib97))
    trained a neural machine translation model, targeting at statements, by learning
    from the corresponding pre- and post-correction code in previous commits. Harer
    et al. ([2018](#bib.bib95)) proposed to generate the input buggy code via generative
    adversarial networks so that the correction model can be trained without labeled
    pairs. Gupta et al. ([2020](#bib.bib87)) embedded execution traces in order to
    predict a sequence of edits for repairing Karel programs. Li et al. ([2020](#bib.bib135))
    treated the program repair as code transformation and introduced two neural networks,
    a tree-based RNN for learning the context of a bug patch, and another one designed
    to learn the code transformation of fixing bugs. White et al. ([2019](#bib.bib256))
    introduced a novel approach for selecting and transforming program repair patches
    using deep-learning-based code similarities. Empirically, Tian et al. ([2020](#bib.bib227))
    studied the practicality of patch generation through representation learning of
    code changes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作是通过生成补丁来修复程序。Tufano 等人（[2018a](#bib.bib229)）进行了一项实证研究，评估了在现实世界场景中应用机器翻译生成程序修复补丁的可行性。与（Tufano
    等，[2018a](#bib.bib229)）不同，后者针对函数级的小代码片段，Hata 等人（[2018](#bib.bib97)）训练了一种神经机器翻译模型，目标是语句，通过学习先前提交中的相应前后修正代码。Harer
    等人（[2018](#bib.bib95)）提出通过生成对抗网络生成输入的有缺陷代码，以便无需标记对进行训练。Gupta 等人（[2020](#bib.bib87)）嵌入执行跟踪，以预测修复
    Karel 程序的编辑序列。Li 等人（[2020](#bib.bib135)）将程序修复视为代码转换，并引入了两个神经网络，一个是基于树的 RNN 用于学习错误修复的上下文，另一个旨在学习修复错误的代码转换。White
    等人（[2019](#bib.bib256)）引入了一种新方法，通过基于深度学习的代码相似性选择和转换程序修复补丁。从实证上看，Tian 等人（[2020](#bib.bib227)）研究了通过代码变化的表示学习生成补丁的实际性。
- en: 4\. Benchmark
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 基准
- en: Even though significant progress has been made in code intelligence with deep
    learning, two limitations remain obstacles to the development of this field. (1)
    Lack of standardized implementation for reproducing the results. It has become
    a common issue that deep-learning-based models are difficult to reproduce due
    to the sensitivity to data and hyperparameter tuning. From our investigation,
    most of them are implemented independently using different toolkits (i.e., PyTorch
    and TensorFlow). There is a need for a unified framework that enables developers
    to easily evaluate their models by utilizing some shared components. Actually,
    in the artificial intelligence area (e.g., NLP and computer vision), many toolkits
    such as Fairseq (Ott et al., [2019](#bib.bib182)), AllenNLP (Gardner et al., [2018](#bib.bib73)),
    Detectron2 (Wu et al., [2019](#bib.bib261)) have been developed, which significantly
    advance the progress of their corresponding research areas. (2) Lack of benchmarks
    for fair comparisons. Currently, many approaches have been proposed and each of
    them claims that the proposed approach has outperformed other ones. To identify
    where the performance improvements come from, it is essential to create a benchmark
    for fair comparisons.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在代码智能方面取得了显著进展，但仍有两个限制阻碍了该领域的发展。 (1) 缺乏标准化实现以复现结果。由于对数据和超参数调整的敏感性，基于深度学习的模型往往难以复现。我们的调查显示，大多数模型都是使用不同的工具包（即
    PyTorch 和 TensorFlow）独立实现的。需要一个统一的框架，使开发者能够通过利用一些共享组件轻松评估他们的模型。实际上，在人工智能领域（例如
    NLP 和计算机视觉），许多工具包如 Fairseq（Ott 等，[2019](#bib.bib182)）、AllenNLP（Gardner 等，[2018](#bib.bib73)）、Detectron2（Wu
    等，[2019](#bib.bib261)）已被开发，这些工具包显著推动了其对应研究领域的进展。 (2) 缺乏公平比较的基准。目前，已经提出了许多方法，每种方法都声称其方法优于其他方法。为了确定性能改进的来源，创建公平比较的基准是至关重要的。
- en: 'Based on these motivations, we propose NaturalCC (standards for Natural Code
    Comprehension), a thorough platform for evaluating source code models using deep
    learning techniques. Under this platform, we also benchmark four specific application
    tasks, including code summarization, code search, code completion, and type inference.
    The implementation and usage of NaturalCC will be introduced in Section [5](#S5
    "5\. Toolkit and Demonstration ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit").'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些动机，我们提出了 NaturalCC（自然代码理解标准），这是一个使用深度学习技术评估源代码模型的全面平台。在这个平台下，我们还对四个特定的应用任务进行了基准测试，包括代码总结、代码搜索、代码完成和类型推断。NaturalCC
    的实现和使用将在第 [5](#S5 "5\. 工具包和演示 ‣ 代码智能的深度学习：调查、基准和工具包")节中介绍。
- en: 4.1\. Code Summarization
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 代码总结
- en: Table 2. Performance of our model and baseline methods for code summarization
    over Python-Doc dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. 我们的模型和基线方法在 Python-Doc 数据集上的代码总结性能。
- en: '|  | BLEU | METEOR | ROUGE-L | Time Cost |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | BLEU | METEOR | ROUGE-L | 时间成本 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Seq2Seq+Attn | 25.57 | 14.40 | 39.41 | 0.09s/Batch |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Seq2Seq+Attn | 25.57 | 14.40 | 39.41 | 0.09s/Batch |'
- en: '| Tree2Seq+Attn | 23.35 | 12.59 | 36.49 | 0.48s/Batch |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Tree2Seq+Attn | 23.35 | 12.59 | 36.49 | 0.48s/Batch |'
- en: '| Transformer | 30.64 | 17.65 | 44.59 | 0.26s/Batch |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 30.64 | 17.65 | 44.59 | 0.26s/Batch |'
- en: '| PLBART | 32.71 | 18.13 | 46.05 | 0.26s/Batch |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| PLBART | 32.71 | 18.13 | 46.05 | 0.26s/Batch |'
- en: 4.1.1\. Approaches
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 方法
- en: Currently, most deep-learning-based code summarization methods use the encoder-decoder
    architecture. An encoder network is used to convert the input source code into
    an embedding vector, and the decoder network is used to generate output summaries
    from the encoded vector. In this paper, we benchmark the following representative
    methods for code summarization, including three different encoders (i.e., LSTM,
    TreeLSTM, and Transformer) as well as a pre-training-based model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数基于深度学习的代码总结方法使用编码器-解码器架构。编码器网络用于将输入的源代码转换为嵌入向量，而解码器网络用于从编码向量生成输出摘要。在本文中，我们对以下具有代表性的方法进行了基准测试，包括三种不同的编码器（即
    LSTM、TreeLSTM 和 Transformer）以及一个基于预训练的模型。
- en: •
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Seq2Seq+Attn (Iyer et al., [2016](#bib.bib108); Wan et al., [2018](#bib.bib238))
    is a vanilla model following sequence-to-sequence architecture with attention
    mechanism. It is a famous method for neural machine translation. Unlike works
    that only represent the source code as token embedding (Iyer et al., [2016](#bib.bib108)),
    we represent the source code via an LSTM network and generate the summary via
    another LSTM network.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Seq2Seq+Attn（Iyer 等，[2016](#bib.bib108); Wan 等，[2018](#bib.bib238)）是一个遵循序列到序列架构的普通模型，并具有注意力机制。这是一个著名的神经机器翻译方法。不同于仅将源代码表示为标记嵌入的工作（Iyer
    等，[2016](#bib.bib108)），我们通过 LSTM 网络表示源代码，并通过另一个 LSTM 网络生成摘要。
- en: •
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tree2Seq+Attn (Wan et al., [2018](#bib.bib238)) also follows the structure of
    Seq2Seq. The difference is that it uses TreeLSTM as the encoder network for syntax-aware
    modeling of code. Moreover, an attention module is also designed to attend to
    different nodes of the syntax tree of code.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Tree2Seq+Attn（Wan 等，[2018](#bib.bib238)）也遵循 Seq2Seq 的结构。不同之处在于它使用 TreeLSTM 作为编码器网络，以对代码进行语法感知建模。此外，还设计了一个注意力模块，用于关注代码的语法树的不同节点。
- en: •
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transformer (Ahmad et al., [2020](#bib.bib5)) is currently considered the leading
    approach for code summarization, which has also achieved significant improvement
    in neural machine translation. In Transformer, a relative position embedding,
    rather than absolute position embedding, is introduced for modeling the positions
    of code tokens.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformer（Ahmad 等，[2020](#bib.bib5)）目前被认为是代码总结的领先方法，它在神经机器翻译中也取得了显著的改进。在 Transformer
    中，引入了相对位置嵌入，而不是绝对位置嵌入，以建模代码标记的位置。
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: PLBART (Ahmad et al., [2021](#bib.bib4)) is built on the top of BART (Lewis
    et al., [2020](#bib.bib125)), which is originally designed for text understanding
    and generation. PLBART can be seen as a specific BART model pre-trained on code
    corpus.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PLBART（Ahmad 等，[2021](#bib.bib4)）是在 BART（Lewis 等，[2020](#bib.bib125)）基础上构建的，BART
    最初设计用于文本理解和生成。PLBART 可以被视为在代码语料库上进行预训练的特定 BART 模型。
- en: 4.1.2\. Results
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 结果
- en: 'We evaluate the performance of each model on the Python-Doc (Barone and Sennrich,
    [2017](#bib.bib18); Wan et al., [2018](#bib.bib238)) dataset using the BLEU, METEOR,
    and ROUGE metrics as in (Wan et al., [2018](#bib.bib238)). The overall performance
    is summarized in Table [2](#S4.T2 "Table 2 ‣ 4.1\. Code Summarization ‣ 4\. Benchmark
    ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit"). This table
    shows that PLBART, which utilizes the Transformer architecture and pre-training
    techniques, achieves the highest performance. It is interesting to see that the
    simple Seq2Seq+Attn outperforms the Tree2Seq+Attn that considers the AST of code.
    For Transformer, we find that the relative position embedding can indeed represent
    the relative relationships among code tokens.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 Python-Doc（Barone and Sennrich, [2017](#bib.bib18); Wan et al., [2018](#bib.bib238)）数据集上使用
    BLEU、METEOR 和 ROUGE 指标评估每个模型的性能，如（Wan et al., [2018](#bib.bib238)）所示。总体性能总结见表
    [2](#S4.T2 "Table 2 ‣ 4.1\. Code Summarization ‣ 4\. Benchmark ‣ Deep Learning
    for Code Intelligence: Survey, Benchmark and Toolkit")。该表显示，利用 Transformer 架构和预训练技术的
    PLBART 达到了最高性能。有趣的是，简单的 Seq2Seq+Attn 超过了考虑代码 AST 的 Tree2Seq+Attn。对于 Transformer，我们发现相对位置嵌入确实能够表示代码标记之间的相对关系。'
- en: 4.2\. Code Search
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 代码搜索
- en: Table 3. MRR of our model and baseline methods for code search over CodeSearchNet
    dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 我们的模型与基线方法在 CodeSearchNet 数据集上的代码搜索 MRR。
- en: '|  | Go | Java | JavaScript | PHP | Python | Ruby | Time Cost |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | Go | Java | JavaScript | PHP | Python | Ruby | 时间成本 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| NBOW | 66.59 | 59.92 | 47.15 | 54.75 | 63.33 | 42.86 | 0.16s/Batch |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| NBOW | 66.59 | 59.92 | 47.15 | 54.75 | 63.33 | 42.86 | 0.16s/Batch |'
- en: '| 1D-CNN | 70.87 | 60.49 | 38.81 | 61.92 | 67.29 | 36.53 | 0.30s/Batch |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 1D-CNN | 70.87 | 60.49 | 38.81 | 61.92 | 67.29 | 36.53 | 0.30s/Batch |'
- en: '| biRNN | 65.80 | 48.60 | 23.23 | 51.36 | 48.28 | 19.35 | 0.74s/Batch |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| biRNN | 65.80 | 48.60 | 23.23 | 51.36 | 48.28 | 19.35 | 0.74s/Batch |'
- en: '| SelfAtt | 78.45 | 66.55 | 50.38 | 65.78 | 79.09 | 47.96 | 0.25s/Batch |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| SelfAtt | 78.45 | 66.55 | 50.38 | 65.78 | 79.09 | 47.96 | 0.25s/Batch |'
- en: 4.2.1\. Approaches
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 方法
- en: CodeSearchNet Challenge (Husain et al., [2019](#bib.bib107)) is an open challenge
    designed to assess the current state of code search. In (Husain et al., [2019](#bib.bib107)),
    the authors have benchmarked four code search methods. The fundamental idea of (Husain
    et al., [2019](#bib.bib107)) is to learn a joint embedding of code and natural-language
    query in a shared vector space. That is, two encoders are used for representing
    the source code and query, respectively. A loss function is then designed to maximize
    the weighted sum for paired embeddings of source code and natural-language query.
    Based on different encoder networks, we have implemented the following four variant
    models.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: CodeSearchNet 挑战（Husain et al., [2019](#bib.bib107)）是一个旨在评估当前代码搜索状态的公开挑战。在（Husain
    et al., [2019](#bib.bib107)）中，作者对四种代码搜索方法进行了基准测试。其基本思想是学习代码和自然语言查询在共享向量空间中的联合嵌入。即，使用两个编码器分别表示源代码和查询。然后设计一个损失函数，以最大化源代码和自然语言查询的配对嵌入的加权和。基于不同的编码器网络，我们实现了以下四种变体模型。
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Neural Bag of Words (NBOW) (Husain et al., [2019](#bib.bib107)) is a naive approach
    by representing the input sequences by a bag of words. For a given code snippet
    or some specified query written in natural language, it represents tokens into
    a collection of word embeddings before feeding them into a max pooling layer for
    creating a sentence-level representation.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经词袋（NBOW）（Husain et al., [2019](#bib.bib107)）是一种通过词袋表示输入序列的简单方法。对于用自然语言编写的代码片段或某些指定的查询，它将标记表示为词嵌入集合，然后将其送入最大池化层以创建句子级别的表示。
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Bidirectional RNN models (biRNN) (Husain et al., [2019](#bib.bib107)) proposes
    to represent the semantics of source code and query via RNN models. Specially,
    we adopt the two-layer bidirectional LSTM network.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双向 RNN 模型（biRNN）（Husain et al., [2019](#bib.bib107)）建议通过 RNN 模型表示源代码和查询的语义。特别地，我们采用了两层双向
    LSTM 网络。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 1D Convolutional Neural Network (1D-CNN) (Husain et al., [2019](#bib.bib107))
    employs convolutional neural layers for code and query representation, and builds
    a residual connection at each layer.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1D 卷积神经网络（1D-CNN）（Husain et al., [2019](#bib.bib107)）使用卷积神经层进行代码和查询表示，并在每层构建一个残差连接。
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Self-Attention (SelfAtt) (Husain et al., [2019](#bib.bib107)) adopts self-attention
    layers to capture the semantic information of sequential source code and query.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自注意力（SelfAtt）（Husain et al., [2019](#bib.bib107)）采用自注意力层来捕捉顺序源代码和查询的语义信息。
- en: 4.2.2\. Implementation Details
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 实现细节
- en: We employ word-level BPE to tokenize both code snippets and natural-language
    descriptions in the considered methods. Subsequently, a shared vocabulary of size
    $50,000$ is constructed based on the sorted token frequency. All models undergo
    training on a singular Nvidia RTX V100 GPU, utilizing a learning rate of $5\times
    10^{-4}$. The gradient norm is maintained at $1.0$, and a batch size of $1,000$
    is specified to expedite training. The optimization process for all models is
    executed using the Adam optimizer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用词级 BPE 对所考虑的方法中的代码片段和自然语言描述进行分词。随后，基于排序的 token 频率构建了一个大小为 $50,000$ 的共享词汇表。所有模型在单个
    Nvidia RTX V100 GPU 上进行训练，学习率设置为 $5\times 10^{-4}$。梯度范数保持在 $1.0$，并指定了 $1,000$
    的批量大小以加速训练。所有模型的优化过程使用 Adam 优化器进行。
- en: 4.2.3\. Results
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 结果
- en: 'We evaluate the performance of each model on the CodeSearchNet corpus using
    the MRR metric, as described in (Husain et al., [2019](#bib.bib107)). The overall
    performance of each model is summarized in Table [3](#S4.T3 "Table 3 ‣ 4.2\. Code
    Search ‣ 4\. Benchmark ‣ Deep Learning for Code Intelligence: Survey, Benchmark
    and Toolkit"). As shown in the table, it is clear that the NBOW model with the
    simplest architecture achieves a comparable performance, at the lowest cost. Moreover,
    we can also observe that the performance of biRNN is poor, in both effectiveness
    and efficiency. The recurrent characteristic of RNN makes it time-consuming. The
    SelfAttn model obtains the best results, which may be attributed to its use of
    the self-attention mechanism.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 MRR 指标评估每个模型在 CodeSearchNet 语料库上的性能，如 (Husain et al., [2019](#bib.bib107))
    所述。每个模型的整体性能总结在表 [3](#S4.T3 "表 3 ‣ 4.2\. 代码搜索 ‣ 4\. 基准测试 ‣ 深度学习代码智能：调查、基准和工具包")
    中。如表中所示，结构最简单的 NBOW 模型在成本最低的情况下实现了相当的性能。此外，我们还可以观察到 biRNN 的性能在效果和效率上都较差。RNN 的递归特性使其耗时。SelfAttn
    模型获得了最佳结果，这可能归因于其使用的自注意力机制。
- en: 4.3\. Code Completion
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 代码补全
- en: Table 4. MRR of our model and baseline methods for code completion over Py150
    dataset.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4. 我们的模型与基线方法在 Py150 数据集上的代码补全的 MRR。
- en: '|  | Attribute | Number | Identifier | Parameter | All Tokens | Time Cost |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | 属性 | 数量 | 标识符 | 参数 | 所有 Token | 时间成本 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LSTM | 51.67 | 47.45 | 46.52 | 66.06 | 73.73 | 0.31s/Batch |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 51.67 | 47.45 | 46.52 | 66.06 | 73.73 | 0.31s/Batch |'
- en: '| GPT-2 | 70.37 | 62.20 | 63.84 | 73.54 | 82.17 | 0.43s/Batch |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | 70.37 | 62.20 | 63.84 | 73.54 | 82.17 | 0.43s/Batch |'
- en: '| TravTrans | 72.08 | 68.55 | 76.33 | 71.08 | 83.17 | 0.43s/Batch |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| TravTrans | 72.08 | 68.55 | 76.33 | 71.08 | 83.17 | 0.43s/Batch |'
- en: 4.3.1\. Approaches
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 方法
- en: The code completion task aims to generate the completion text based on the given
    partial code. In this paper, we investigate three representative approaches.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 代码补全任务旨在根据给定的部分代码生成完成文本。在本文中，我们探讨了三种具有代表性的方法。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LSTM (Kim et al., [2021](#bib.bib118)) denotes the model that represents the
    partial code by LSTM, and then predicts the missing token via a softmax layer.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LSTM (Kim et al., [2021](#bib.bib118)) 表示通过 LSTM 表示部分代码的模型，然后通过 softmax 层预测缺失的
    token。
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-2 (Kim et al., [2021](#bib.bib118)) is a pre-trained language model based
    on Transformer. It refers to the Transformer model that is trained by iteratively
    predicting the next code token.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-2 (Kim et al., [2021](#bib.bib118)) 是一种基于 Transformer 的预训练语言模型。它指的是通过迭代预测下一个代码
    token 来训练的 Transformer 模型。
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: TravTrans (Kim et al., [2021](#bib.bib118)) is designed to preserve the syntax
    structure of source code while predicting the missing token. It first linearizes
    the code ASTs into a sequence of tokens using depth-first traversing, and afterward
    feeds the traversal into Transformer for representation. It also uses a softmax
    layer to predict the missing token.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TravTrans (Kim et al., [2021](#bib.bib118)) 旨在在预测缺失的 token 时保留源代码的语法结构。它首先通过深度优先遍历将代码
    AST 转换为 token 序列，然后将遍历结果输入 Transformer 进行表示。它还使用 softmax 层来预测缺失的 token。
- en: 4.3.2\. Implementation Details
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 实施细节
- en: For acquiring high-quality code tokens, we perform preprocessing on the code
    snippets by parsing them into ASTs and extracting their leaf nodes as code tokens.
    We establish a unified vocabulary comprising $50,000$ tokens, organized based
    on token frequency. All models undergo training utilizing four Nvidia RTX V100
    GPUs, employing a learning rate of $1\times 10^{-3}$, and a batch size of $32$.
    The optimization of all models is executed using the Adam optimizer.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取高质量的代码标记，我们对代码片段进行预处理，将它们解析成抽象语法树（AST），并提取其叶节点作为代码标记。我们建立了一个包含$50,000$个标记的统一词汇表，基于标记频率进行组织。所有模型使用四个Nvidia
    RTX V100 GPU进行训练，采用$1\times 10^{-3}$的学习率和$32$的批量大小。所有模型的优化使用Adam优化器。
- en: 4.3.3\. Results
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 结果
- en: 'We evaluate each model on the Py150 (Raychev et al., [2016](#bib.bib197)) dataset
    using the MRR metric as used in (Kim et al., [2021](#bib.bib118)). We divide the
    prediction tokens into five categories, namely attributes, numeric constants,
    identifier names, function parameters and all tokens. We summarize the performance
    of each model in Table [4](#S4.T4 "Table 4 ‣ 4.3\. Code Completion ‣ 4\. Benchmark
    ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit"). From this
    table, when comparing GPT-2 with LSTM, we can observe that the Transformer architecture
    outperforms other models in representing the semantics of code, thus, resulting
    in better performance for code completion. Furthermore, when comparing TravTrans
    with GPT-2, we can see that the TravTrans that incorporates the syntax structure
    information achieves better performance, showing that the syntax information is
    useful for code completion.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用MRR指标对每个模型在Py150 (Raychev et al., [2016](#bib.bib197))数据集上的表现进行评估，指标参考了(Kim
    et al., [2021](#bib.bib118))中的用法。我们将预测标记分为五类，即属性、数值常量、标识符名称、函数参数和所有标记。我们在表[4](#S4.T4
    "Table 4 ‣ 4.3\. Code Completion ‣ 4\. Benchmark ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit")中总结了每个模型的性能。从该表中，可以观察到在比较GPT-2与LSTM时，Transformer架构在表示代码语义方面优于其他模型，从而在代码补全方面表现更佳。此外，当将TravTrans与GPT-2进行比较时，我们可以看到，结合语法结构信息的TravTrans实现了更好的性能，表明语法信息对代码补全是有用的。'
- en: 4.4\. Type Inference
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 类型推断
- en: Table 5. Accuracy of our model and baseline methods for type inference over
    Py150 dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表5. 我们的模型和基线方法在Py150数据集上的类型推断准确率。
- en: '|  | Accuracy@1 | Accuracy@5 | Accuracy@1 | Accuracy@5 | Time Cost |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | Accuracy@1 | Accuracy@5 | Accuracy@1 | Accuracy@5 | 时间成本 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | All types | Any types |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | 所有类型 | 任意类型 |'
- en: '| --- | --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| DeepTyper | 0.52 | 0.67 | 0.43 | 0.67 | 0.42s/Batch |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| DeepTyper | 0.52 | 0.67 | 0.43 | 0.67 | 0.42s/Batch |'
- en: '| Transformer | 0.34 | 0.64 | 0.37 | 0.75 | 0.85s/Batch |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 0.34 | 0.64 | 0.37 | 0.75 | 0.85s/Batch |'
- en: 4.4.1\. Approaches
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 方法
- en: Similar to code completion, the type inference task aims to predict the types
    of variables based on contextual information. It first represents the contextual
    code into a vector, and then predicts the missing types by a softmax layer. In
    our work, we employ two state-of-the-art methods for this task.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 与代码补全类似，类型推断任务旨在根据上下文信息预测变量类型。它首先将上下文代码表示成向量，然后通过softmax层预测缺失的类型。在我们的工作中，我们采用了两种最先进的方法来完成此任务。
- en: •
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DeepTyper (Hellendoorn et al., [2018](#bib.bib99)) proposes to represent the
    contextual code by a two-layer biGRU, and then predicts the missing variable types
    via a softmax layer.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DeepTyper (Hellendoorn et al., [2018](#bib.bib99)) 提出了通过双层biGRU表示上下文代码，然后通过softmax层预测缺失的变量类型。
- en: •
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transformer (Ahmad et al., [2020](#bib.bib5)) proposes to represent the contextual
    code by a Transformer encoder network, and then predicts the missing variable
    types via a softmax layer.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformer (Ahmad et al., [2020](#bib.bib5)) 提出了通过Transformer编码网络表示上下文代码，然后通过softmax层预测缺失的变量类型。
- en: 4.4.2\. Implementation Details
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 实现细节
- en: We initially tokenize both the code snippets and natural-language descriptions.
    Subsequently, we establish a common vocabulary comprising $40,000$ tokens, determined
    by sorting them based on frequency. The hardware configuration for training and
    the optimizer employed remains consistent with the aforementioned specifications.
    A batch size of $16$ and a learning rate of $1\times 10^{-4}$ are utilized.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对代码片段和自然语言描述进行标记化。随后，我们建立了一个包含$40,000$个标记的通用词汇表，通过根据频率对其进行排序来确定。训练的硬件配置和使用的优化器与前述规格一致。使用$16$的批量大小和$1\times
    10^{-4}$的学习率。
- en: 4.4.3\. Results
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 结果
- en: 'We evaluate each model on the Py150 (Raychev et al., [2016](#bib.bib197)),
    by using the Accuracy metric as in (Jain et al., [2021](#bib.bib110)). In particular,
    we measure the performance under the settings of all types and any types. The
    performance of different models is summarized in Table [5](#S4.T5 "Table 5 ‣ 4.4\.
    Type Inference ‣ 4\. Benchmark ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit"). From this table, it is interesting to see that the simple
    LSTM-based DeepTyper outperforms the Transformer-based approach, especially under
    the all types setting, at a lower time cost.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用Accuracy度量（如 (Jain et al., [2021](#bib.bib110))所示）对Py150 (Raychev et al.,
    [2016](#bib.bib197))上的每个模型进行评估。特别地，我们测量了所有类型和任意类型设置下的性能。不同模型的性能总结在表[5](#S4.T5
    "Table 5 ‣ 4.4\. Type Inference ‣ 4\. Benchmark ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit")中。从表中可以看出，简单的基于LSTM的DeepTyper在所有类型设置下的表现优于基于Transformer的方法，尤其是在时间成本较低的情况下。'
- en: 5\. Toolkit and Demonstration
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 工具包和演示
- en: 'This section introduces the design of NaturalCC and its user interface. Figure [5](#S5.F5
    "Figure 5 ‣ 5.1\. Data Preprocessing Module ‣ 5\. Toolkit and Demonstration ‣
    Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit") (left) shows
    the code structure of NaturalCC. The dataset folder contains data preprocessing
    code. The ncc folder is the core module. The third_party folder holds model evaluation
    packages. The gui folder contains graphical user interface files and assets. As
    shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.1\. Data Preprocessing Module ‣ 5\. Toolkit
    and Demonstration ‣ Deep Learning for Code Intelligence: Survey, Benchmark and
    Toolkit") (right), NaturalCC is composed of four components, i.e., data preprocessing,
    code representation, downstream tasks, and their corresponding evaluations. At
    the stage of data preprocessing, we process the source code with a series of steps,
    including word tokenization, building vocabulary, and feature extraction. Additionally,
    a data loader is used to iteratively yield batches of code samples with their
    features. The resulting batches are then sent into the code representation models,
    which facilitate a variety of downstream tasks, including code summarization,
    code search, code completion, and type inference. To evaluate the performance
    of each task, we also implement several corresponding metrics that have been widely
    adopted previously.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '本节介绍了NaturalCC的设计及其用户界面。图[5](#S5.F5 "Figure 5 ‣ 5.1\. Data Preprocessing Module
    ‣ 5\. Toolkit and Demonstration ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit")（左）展示了NaturalCC的代码结构。数据集文件夹包含数据预处理代码。ncc文件夹是核心模块。third_party文件夹包含模型评估包。gui文件夹包含图形用户界面文件和资源。如图[5](#S5.F5
    "Figure 5 ‣ 5.1\. Data Preprocessing Module ‣ 5\. Toolkit and Demonstration ‣
    Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit")（右）所示，NaturalCC由四个组件组成，即数据预处理、代码表示、下游任务及其对应的评估。在数据预处理阶段，我们通过一系列步骤处理源代码，包括词汇标记化、构建词汇表和特征提取。此外，我们使用数据加载器来迭代生成带有特征的代码样本批次。生成的批次随后送入代码表示模型，这些模型促进了包括代码总结、代码搜索、代码补全和类型推断等各种下游任务。为了评估每个任务的性能，我们还实现了几个广泛采用的相应度量。'
- en: 5.1\. Data Preprocessing Module
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 数据预处理模块
- en: 'In NaturalCC, we have collected and processed four datasets including CodeSearchNet (Husain
    et al., [2019](#bib.bib107)), Python-Doc (Wan et al., [2018](#bib.bib238)), Py150 (Raychev
    et al., [2016](#bib.bib197)), and DeepTyper (Hellendoorn et al., [2018](#bib.bib99)).
    First, we tokenize the input source code, and then build a vocabulary to map the
    code tokens into indexes. Currently, we support two types of tokenizations: space
    tokenizer and BPE tokenizer (Karampatsis et al., [2020](#bib.bib117)). Along with
    code tokens, we also explore different features of code, such as AST, IR, CFGs,
    and DFGs. All the related scripts for data preprocessing have been put in the
    data and dataset folders.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在NaturalCC中，我们收集并处理了四个数据集，包括CodeSearchNet (Husain et al., [2019](#bib.bib107))、Python-Doc (Wan
    et al., [2018](#bib.bib238))、Py150 (Raychev et al., [2016](#bib.bib197))和DeepTyper (Hellendoorn
    et al., [2018](#bib.bib99))。首先，我们对输入源代码进行标记化，然后构建词汇表以将代码标记映射到索引。目前，我们支持两种类型的标记化：空格标记化和BPE标记化 (Karampatsis
    et al., [2020](#bib.bib117))。除了代码标记，我们还探索了代码的不同特征，如AST、IR、CFG和DFG。所有与数据预处理相关的脚本都放在了data和dataset文件夹中。
- en: '![Refer to caption](img/95a5275b210c31db91f96e05e2b62c5d.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/95a5275b210c31db91f96e05e2b62c5d.png)'
- en: Figure 5. The source code hierarchy and pipeline of NaturalCC.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. NaturalCC的源代码层次结构和管道。
- en: 5.2\. Code Representation Module
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 代码表示模块
- en: As the core component of NaturalCC, we have implemented several encoders that
    are widely used in state-of-the-art approaches for source code representation,
    including RNN, GNN, and Transformer. For example, we have implemented LSTM, TreeLSTM
    and Transformer networks for sequential tokens and (linearized) ASTs. We have
    also implemented a GNN, i.e., GGNN, to represent the control-flow graph of source
    code. It is worth mentioning that in NaturalCC, we have also incorporated the
    pre-training approaches for source code. We have implemented several state-of-the-art
    pre-trained code models, including CodeBERT (Feng et al., [2020](#bib.bib67)),
    PLBART (Ahmad et al., [2021](#bib.bib4)), and GPT-2 (Lu et al., [2021](#bib.bib160)).
    The models and modules folders contain all the implemented networks for code representation.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 作为NaturalCC的核心组件，我们实现了几种在最新的源代码表示方法中广泛使用的编码器，包括RNN、GNN和Transformer。例如，我们为顺序标记和（线性化的）AST实现了LSTM、TreeLSTM和Transformer网络。我们还实现了一个GNN，即GGNN，用于表示源代码的控制流图。值得一提的是，在NaturalCC中，我们还结合了源代码的预训练方法。我们实现了几个最先进的预训练代码模型，包括CodeBERT (Feng
    et al., [2020](#bib.bib67))、PLBART (Ahmad et al., [2021](#bib.bib4))和GPT-2 (Lu
    et al., [2021](#bib.bib160))。模型和模块文件夹包含了所有实现的代码表示网络。
- en: 5.3\. Tool Implementation
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. 工具实现
- en: NaturalCC is mainly implemented by PyTorch, and builds upon other successful
    open-source toolkits in NLP, such as Fairseq, and AllenNLP.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: NaturalCC主要由PyTorch实现，并且基于其他成功的开源NLP工具包，如Fairseq和AllenNLP。
- en: Registry Mechanism. To be flexible, NaturalCC is expected to be easily extended
    to different tasks and model implementations, with minimum modification. Similar
    to Fairseq, we design a register decorator on instantiating a new task or model,
    the implementation of which is in the corresponding __init__.py in each folder.
    The registry mechanism is to create a global variable to store all the available
    tasks, models, and objects at the initialization stage, so that users can easily
    access them throughout the whole project.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注册机制。为了灵活性，NaturalCC预计能够轻松扩展到不同的任务和模型实现，只需最少的修改。与Fairseq类似，我们在实例化新任务或模型时设计了一个注册装饰器，其实现位于每个文件夹的__init__.py中。注册机制旨在创建一个全局变量，在初始化阶段存储所有可用的任务、模型和对象，以便用户可以在整个项目中轻松访问它们。
- en: Efficient Training. NaturalCC supports efficient training of models in a distributed
    way through torch.distributed. It can utilize multiple GPUs across different servers.
    Furthermore, NaturalCC can support calculation in mixed precision to further increase
    the training speed, including both FP32 and FP16 training. Typically, the gradients
    are updated in FP16 while the parameters are saved in FP32.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 高效训练。NaturalCC通过torch.distributed支持模型的高效分布式训练。它可以利用不同服务器上的多个GPU。此外，NaturalCC还支持混合精度计算，以进一步提高训练速度，包括FP32和FP16训练。通常，梯度在FP16中更新，而参数则保存在FP32中。
- en: Flexible Configuration. Instead of employing argparse for managing command-line
    options within Fairseq, we advocate the adoption of individual yaml configuration
    files for each model’s configuration. We contend that the flexibility offered
    by modifying these yaml configuration files is better suited for model exploration.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活配置。我们提倡为每个模型的配置采用单独的yaml配置文件，而不是在Fairseq中使用argparse来管理命令行选项。我们认为，通过修改这些yaml配置文件所提供的灵活性更适合模型的探索。
- en: 5.4\. Graphical User Interface
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4. 图形用户界面
- en: 'We also design a Web system as a graphical user interface to help users explore
    the results of trained models. The design is based on the open-source demonstration
    of AllenNLP (Gardner et al., [2018](#bib.bib73)). Figure [6(a)](#S5.F6.sf1 "In
    Figure 6 ‣ 5.5\. Leaderboard ‣ 5\. Toolkit and Demonstration ‣ Deep Learning for
    Code Intelligence: Survey, Benchmark and Toolkit") shows the screenshot of our
    demonstration system. Currently, we have implemented three tasks that are related
    to code intelligence, i.e., code summarization, code search, and code completion.
    We leave the integration of other related tasks to our future work.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设计了一个Web系统作为图形用户界面，以帮助用户探索训练模型的结果。该设计基于AllenNLP的开源演示（Gardner et al., [2018](#bib.bib73)）。图 [6(a)](#S5.F6.sf1
    "在图6 ‣ 5.5. 排行榜 ‣ 5. 工具包和演示 ‣ 代码智能的深度学习：综述、基准测试和工具包") 显示了我们演示系统的截图。目前，我们实现了三个与代码智能相关的任务，即代码总结、代码搜索和代码补全。其他相关任务的集成留待我们未来的工作中完成。
- en: 5.5\. Leaderboard
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5. 排行榜
- en: 'We also develop a leaderboard so that researchers can report the results of
    their own models and compete with others, as shown in Figure [6(b)](#S5.F6.sf2
    "In Figure 6 ‣ 5.5\. Leaderboard ‣ 5\. Toolkit and Demonstration ‣ Deep Learning
    for Code Intelligence: Survey, Benchmark and Toolkit"). Currently, we only support
    researchers and developers who use NaturalCC to implement their approach and update
    the experimental results via pull requests in GitHub. In our future work, we will
    build a web-based service, which allows users to upload their predicted results
    and evaluate the model performance automatically using the ground-truth labels
    as a reference.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还开发了一个排行榜，以便研究人员可以报告他们自己模型的结果并与他人竞争，如图[6(b)](#S5.F6.sf2 "在图 6 ‣ 5.5\. 排行榜
    ‣ 5\. 工具包和演示 ‣ 代码智能的深度学习：综述、基准和工具包")所示。目前，我们仅支持使用NaturalCC实现其方法的研究人员和开发者，并通过GitHub上的拉取请求更新实验结果。在未来的工作中，我们将建立一个基于Web的服务，允许用户上传他们预测的结果，并使用真实标签作为参考来自动评估模型性能。
- en: '![Refer to caption](img/19eccc44d99d16eb9b63a0e4af30d705.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/19eccc44d99d16eb9b63a0e4af30d705.png)'
- en: (a) Demonstration
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 演示
- en: '![Refer to caption](img/26b412cf3018d1a551daf07d7f231a41.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/26b412cf3018d1a551daf07d7f231a41.png)'
- en: (b) Leaderboard
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 排行榜
- en: Figure 6. Screenshots of GUI and leaderboard of NaturalCC.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. NaturalCC 的GUI和排行榜截图
- en: 6\. Challenges and Opportunities
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 挑战与机遇
- en: Although much effort has been made into deep learning for code intelligence,
    this area of research is still in its infancy with many open challenges and opportunities.
    To inspire future research, this section suggests several potential directions
    that are worth pursuing.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在代码智能的深度学习方面投入了大量努力，但这一研究领域仍处于起步阶段，面临许多开放的挑战和机会。为了激发未来的研究，本节建议了几个值得追求的潜在方向。
- en: Comprehensive Code Representation. Designing a representation approach to effectively
    and efficiently preserve the semantics of programs has always been a fundamental
    problem in code intelligence. Despite much effort on code representation, as mentioned
    in this paper, there are still three main obstacles to be overcome. (a) Open Vocabulary.
    Building a vocabulary to index the textual tokens of code is the first step toward
    applying deep learning models for code intelligence. Since the unambiguous characteristic
    of code, the vocabulary in code is much more open and complicated than the vocabulary
    in natural languages. The vocabulary of programming languages often consists of
    keywords, identifiers, customized method names, and variable names. The large
    vocabulary contains much “noise”, making it difficult to comprehend the code.
    Although many attempts (Cvitkovic et al., [2019](#bib.bib57); Karampatsis et al.,
    [2020](#bib.bib117); Chirkova and Troshin, [2021b](#bib.bib50)) have been made
    towards mitigating the OOV issue, it remains a challenge to design a simple yet
    effective approach to map the source code into indexes while preserving the semantics.
    (b) Complex Structure of Program. Unlike natural language, code is written with
    strict grammar. The computations described by code can be executed in an order
    that is different from the order in which the code was written. This is often
    seen in operations such as loops, recursions, and pointer manipulation. Although
    many attempts to capture the structure of code from different modalities, as we
    surveyed in this paper, we believe that the structures of code are not sufficiently
    preserved, and more effort is needed here. Inspired by the GNNs, there is potential
    to design specific GNNs to better represent the structure of programs. For example,
    from our analysis, ASTs, CFGs, DFGs and CPGs all have high heterogeneity. It is
    desirable to design some heterogeneous-information-network-based approaches (Sun
    et al., [2022b](#bib.bib217)) to represent the heterogeneous code graph. (c) Big
    Models of Code. Despite the significant progress made by pre-trained code models
    in code intelligence, pre-training on a large-scale code corpus is still computationally
    expensive and very costly. Recently, Zhang et al. ([2022b](#bib.bib289)) and Shi
    et al. ([2022b](#bib.bib209)) proposed to improve the efficiency of the training
    process by model compressing. It is a promising research direction to reduce the
    computational resources of pre-trained code models.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 综合代码表示。设计一种有效且高效地保留程序语义的表示方法一直是代码智能中的一个基本问题。尽管在代码表示方面做了很多努力，如本文所述，仍然存在三个主要障碍需要克服。(a)
    开放词汇。建立一个词汇表来索引代码的文本标记是将深度学习模型应用于代码智能的第一步。由于代码的明确特性，代码中的词汇表比自然语言中的词汇表要开放和复杂得多。编程语言的词汇通常包括关键字、标识符、自定义方法名称和变量名称。大词汇表包含许多“噪音”，使得理解代码变得困难。尽管许多尝试（Cvitkovic
    et al., [2019](#bib.bib57); Karampatsis et al., [2020](#bib.bib117); Chirkova
    and Troshin, [2021b](#bib.bib50)）旨在缓解OOV问题，但设计一种简单而有效的方法将源代码映射到索引，同时保留语义仍然是一个挑战。(b)
    程序的复杂结构。与自然语言不同，代码是用严格的语法编写的。代码描述的计算可以以不同于代码编写顺序的顺序执行。这通常出现在循环、递归和指针操作等操作中。尽管我们在本文中调查了许多捕捉代码结构的不同方法，但我们认为代码结构尚未得到充分保留，这里需要更多的努力。受到GNN的启发，有潜力设计特定的GNN以更好地表示程序结构。例如，通过我们的分析，AST、CFG、DFG和CPG都具有高度的异质性。设计一些基于异质信息网络的方法（Sun
    et al., [2022b](#bib.bib217)）以表示异质代码图是值得期望的。(c) 代码的大模型。尽管预训练代码模型在代码智能方面取得了显著进展，但在大规模代码语料库上进行预训练仍然计算开销大且非常昂贵。最近，Zhang
    et al. ([2022b](#bib.bib289)) 和 Shi et al. ([2022b](#bib.bib209)) 提出了通过模型压缩来提高训练过程的效率。这是减少预训练代码模型计算资源的一个有前景的研究方向。
- en: Data Hungry and Data Quality. Despite much progress achieved in deep-learning-based
    approaches for code intelligence, we argue that existing approaches still suffer
    from the data-hungry issue. In other words, the effectiveness of cutting-edge
    techniques significantly depends on the availability of vast quantities of expensive
    and labor-intensive well-labeled training data. Training the model on a small
    qualified dataset will result in far less imprecise results, especially for new
    programming languages or languages with an inadequate number of labeled samples.
    Therefore, it is important to design approaches to reduce the reliance on a large
    quantity of labeled data. A similar problem exists in the field of machine learning.
    One promising solution for this dilemma is transfer learning, which has achieved
    great success in transferring knowledge to alleviate the data-hungry issue in
    computer vision and NLP. Similarly, to model an emerging programming language
    with limited data, it is desirable to mitigate the data-hungry issue by leveraging
    models trained in programming languages with sufficient labeled training data (Chai
    et al., [2022b](#bib.bib39); Cui et al., [2022](#bib.bib54); Chen et al., [2022](#bib.bib42)).
    Data quality is also a crucial issue for code intelligence, which may exacerbate
    the data-hungry problem. From our analysis, the collected datasets from online
    resources, like GitHub and StackOverflow, are not quality ensured. Sun et al.
    ([2022c](#bib.bib218)) and Shi et al. ([2022a](#bib.bib210)) investigated the
    importance of data quality and verified it on the tasks of code search and code
    summarization, respectively.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 数据饥渴与数据质量。尽管在基于深度学习的代码智能方法上取得了很多进展，但我们认为现有的方法仍然面临数据饥渴的问题。换句话说，前沿技术的有效性在很大程度上依赖于大量昂贵且劳动密集型的标注训练数据的可用性。在小规模合格的数据集上训练模型将导致结果不够精准，尤其是对于新编程语言或标注样本不足的语言。因此，设计减少对大量标注数据依赖的方法至关重要。在机器学习领域也存在类似问题。一种有前景的解决方案是迁移学习，它在计算机视觉和自然语言处理领域成功地转移了知识，以缓解数据饥渴的问题。类似地，为了用有限的数据建模新兴编程语言，利用在拥有足够标注训练数据的编程语言上训练的模型来减轻数据饥渴问题是理想的（Chai等，[2022b](#bib.bib39)；Cui等，[2022](#bib.bib54)；Chen等，[2022](#bib.bib42)）。数据质量也是代码智能中的一个关键问题，可能会加剧数据饥渴问题。根据我们的分析，从在线资源（如GitHub和StackOverflow）收集的数据集的质量没有得到保证。Sun等（[2022c](#bib.bib218)）和Shi等（[2022a](#bib.bib210)）研究了数据质量的重要性，并在代码搜索和代码总结任务上验证了这一点。
- en: Multi-Lingual and Cross-Language. The codebase written in multiple programming
    languages is can be considered a multi-lingual corpus, as in NLP. However, the
    multi-lingual problem in programming languages has not been well investigated.
    Different from the multi-lingual problems studied in NLP, the corpus of multiple
    programming languages will bring more opportunities and challenges to future research.
    Recently, several attempts have been made to learn the common knowledge shared
    among multiple programming languages, and transfer the knowledge across different
    programming languages. For example, Zhang et al. ([2021b](#bib.bib283)) proposed
    obtaining better interpretability and generalizability by disentangling the semantics
    of source code from multiple programming languages based on variational autoencoders.
    Zügner et al. ([2021](#bib.bib301)) introduced a language-agnostic code representation
    based on the features directly extracted from the AST. Ahmed and Devanbu ([2022](#bib.bib6))
    conducted an exploratory study and revealed the evidence that multilingual property
    indeed exists in the source code corpora. For example, it is more likely that
    programs that solve the same problem in different languages make use of the same
    or similar identifier names. They also investigate the effect of multilingual
    (pre-)training for code summarization and code search. Nafi et al. ([2019](#bib.bib172))
    proposed CLCDSA, a cross-language clone detector with syntactical features and
    API documentation. Bui et al. ([2019a](#bib.bib29)) proposed a bilateral neural
    network for the task of cross-language algorithm classification. Bui et al. ([2019b](#bib.bib30))
    proposed SAR, which can learn cross-language API mappings with minimal knowledge.
    Recently, Chai et al. ([2022b](#bib.bib39)) proposed a novel approach termed CDCS
    for domain-specific code search through transfer learning across programming languages.
    Gui et al. ([2022](#bib.bib81)) proposed an approach that matches source code
    and binary code across different languages based on intermediate representation.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言与跨语言。用多种编程语言编写的代码库可以视为多语言语料库，就像自然语言处理中的多语言语料库一样。然而，编程语言中的多语言问题尚未得到充分研究。与自然语言处理中的多语言问题不同，多种编程语言的语料库将为未来的研究带来更多的机遇和挑战。最近，已经有几次尝试学习多种编程语言之间共享的共同知识，并在不同编程语言之间转移这些知识。例如，张等人
    ([2021b](#bib.bib283)) 提出了通过变分自编码器将多种编程语言的源代码语义解耦，从而获得更好的可解释性和泛化能力。Zügner 等人 ([2021](#bib.bib301))
    介绍了一种基于直接从 AST 提取特征的语言无关代码表示。Ahmed 和 Devanbu ([2022](#bib.bib6)) 进行了一项探索性研究，揭示了多语言特性在源代码语料库中确实存在的证据。例如，在不同语言中解决相同问题的程序更可能使用相同或相似的标识符名称。他们还调查了多语言（预）训练对代码摘要和代码搜索的影响。Nafi
    等人 ([2019](#bib.bib172)) 提出了 CLCDSA，一种具有语法特征和 API 文档的跨语言克隆检测器。Bui 等人 ([2019a](#bib.bib29))
    提出了用于跨语言算法分类任务的双向神经网络。Bui 等人 ([2019b](#bib.bib30)) 提出了 SAR，该方法可以在最少知识的情况下学习跨语言
    API 映射。最近，Chai 等人 ([2022b](#bib.bib39)) 提出了一个新方法，称为 CDCS，通过跨编程语言的迁移学习进行领域特定代码搜索。Gui
    等人 ([2022](#bib.bib81)) 提出了一个基于中间表示匹配不同语言源代码和二进制代码的方法。
- en: Model Interpretability. Lack of interpretability is a common challenge for most
    deep learning-based techniques for code intelligence, as deep learning is a black-box
    method. New methods and studies on interpreting the working mechanisms of deep
    neural networks should be a potential research direction. Recently, several efforts
    have been made toward increasing the interpretability of deep-learning-based models.
    As an example, Li et al. ([2021c](#bib.bib137)) presented a novel approach to
    explain predicted results for GNN-based vulnerability detection by extracting
    sub-graphs in the program dependency graph. In addition, Zou et al. ([2021](#bib.bib300))
    proposed interpreting a deep-learning-based model for vulnerability detection
    by identifying a limited number of tokens that play a significant role in the
    final prediction of the detectors. Zhang et al. ([2021a](#bib.bib287)) proposed
    interpretable program synthesis that allows users to see the synthesis process
    and have control over the synthesizer. Pornprasit et al. ([2021](#bib.bib188))
    proposed a local rule-based model-agnostic approach, termed PyExplainer, to explain
    the predictions of just-in-time defect models. Rabin et al. ([2021](#bib.bib192))
    proposed a model-agnostic explainer based on program simplification, inspired
    by the delta debugging algorithms. Wan et al. ([2022c](#bib.bib237)), López et al.
    ([2022](#bib.bib158)), and Sharma et al. ([2022](#bib.bib207)) investigated the
    explainability of pre-trained code models through probing the code attention and
    hidden representations. We believe that it is essential to enhance the interpretability
    of current deep-learning-based approaches for code intelligence.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可解释性。缺乏可解释性是大多数基于深度学习的代码智能技术面临的普遍挑战，因为深度学习是一种黑箱方法。解释深度神经网络工作机制的新方法和研究应成为一个潜在的研究方向。最近，已经有一些努力致力于提高基于深度学习的模型的可解释性。例如，Li
    等人 ([2021c](#bib.bib137)) 提出了一种新颖的方法，通过提取程序依赖图中的子图来解释 GNN 基于的漏洞检测的预测结果。此外，Zou
    等人 ([2021](#bib.bib300)) 提出了通过识别在检测器最终预测中发挥重要作用的有限数量的标记来解释基于深度学习的漏洞检测模型。Zhang
    等人 ([2021a](#bib.bib287)) 提出了可解释的程序合成，使用户能够查看合成过程并控制合成器。Pornprasit 等人 ([2021](#bib.bib188))
    提出了一个局部规则基础的模型无关方法，称为 PyExplainer，用于解释及时缺陷模型的预测。Rabin 等人 ([2021](#bib.bib192))
    提出了一个基于程序简化的模型无关解释器，灵感来自于增量调试算法。Wan 等人 ([2022c](#bib.bib237))，López 等人 ([2022](#bib.bib158))
    和 Sharma 等人 ([2022](#bib.bib207)) 通过探测代码注意力和隐藏表示，研究了预训练代码模型的可解释性。我们认为，增强当前基于深度学习的方法在代码智能领域的可解释性是至关重要的。
- en: Robustness and Security. Despite significant progress being made in the training
    of accurate models for code intelligence, the robustness and security of these
    models have rarely been explored. As seen in the fields of NLP and CV, deep neural
    networks are frequently not robust (Carlini and Wagner, [2017](#bib.bib37)). Specifically,
    current deep learning models can be easily deceived by adversarial examples, which
    are created by making small changes to the inputs of the model that it would consider
    as benign. There are many different ways to produce adversarial samples in the
    computer vision and NLP communities, particularly for image classification (Eykholt
    et al., [2018](#bib.bib66); Carlini and Wagner, [2017](#bib.bib37); Carlini et al.,
    [2019](#bib.bib36)) and sentiment classification (Zhang et al., [2020b](#bib.bib288)).
    Similarly, for source code models, the adversarial attack also exists. Recently,
    there have been several efforts to investigate the robustness and security of
    deep-learning-based models for code intelligence. For example, Ramakrishnan et al.
    ([2020](#bib.bib196)) and Yefet et al. ([2020](#bib.bib276)) investigated how
    to improve the robustness of source code models through adversarial training.
    Nguyen et al. ([2021](#bib.bib175)) empirically investigated the use of adversarial
    learning techniques for API recommendation. Bielik and Vechev ([2020](#bib.bib24))
    introduced a novel method that incorporates adversarial training and representation
    refinement to create precise and robust models of source code. Zhou et al. ([2022](#bib.bib295)),
    Yang et al. ([2022b](#bib.bib272)) and Zhang et al. ([2020a](#bib.bib282)) proposed
    a black-box attack for neural code models by generating adversarial examples while
    preserving the semantics of source code. Based on semantics-preserving code transformations,
    Quiring et al. ([2019](#bib.bib191)) and Liu et al. ([2021a](#bib.bib154)) developed
    a novel attack against authorship attribution of source code. Ramakrishnan and
    Albarghouthi ([2022](#bib.bib195)) investigated the possibility of injecting a
    number of common backdoors into deep-learning-based models, and developed a protection
    approach based on spectral signatures. Schuster et al. ([2021](#bib.bib204)) and
    Wan et al. ([2022b](#bib.bib236)) proposed attacking the neural code models through
    data poisoning, and verified it in code completion and code search, respectively.
    Severi et al. ([2021](#bib.bib205)) suggested an explanation-guided backdoor approach
    to attack the malware classifiers. Overall, exploring the robustness and security
    of code intelligence models is an interesting and important research direction.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 稳健性与安全性。尽管在代码智能的准确模型训练方面取得了显著进展，但这些模型的稳健性和安全性仍然鲜有探索。正如在自然语言处理（NLP）和计算机视觉（CV）领域所见，深度神经网络经常表现出不够稳健的特性（Carlini
    和 Wagner，[2017](#bib.bib37)）。具体而言，当前的深度学习模型容易受到对抗样本的欺骗，这些样本通过对模型输入进行微小的修改，使得模型将其视为正常输入。在计算机视觉和NLP社区中，有许多方法可以生成对抗样本，尤其是在图像分类（Eykholt
    et al., [2018](#bib.bib66); Carlini 和 Wagner，[2017](#bib.bib37); Carlini et al.,
    [2019](#bib.bib36)）和情感分类（Zhang et al., [2020b](#bib.bib288)）中。同样，对于源代码模型，对抗攻击也存在。近期，有多项研究致力于探讨基于深度学习的代码智能模型的稳健性和安全性。例如，Ramakrishnan
    et al. ([2020](#bib.bib196)) 和 Yefet et al. ([2020](#bib.bib276)) 研究了如何通过对抗训练提高源代码模型的稳健性。Nguyen
    et al. ([2021](#bib.bib175)) 通过实证研究了对抗学习技术在API推荐中的应用。Bielik 和 Vechev ([2020](#bib.bib24))
    提出了一种新方法，将对抗训练和表示精炼相结合，创建精确且稳健的源代码模型。Zhou et al. ([2022](#bib.bib295))、Yang et
    al. ([2022b](#bib.bib272)) 和 Zhang et al. ([2020a](#bib.bib282)) 提出了针对神经代码模型的黑盒攻击，通过生成对抗样本而保持源代码的语义。基于语义保持的代码转换，Quiring
    et al. ([2019](#bib.bib191)) 和 Liu et al. ([2021a](#bib.bib154)) 开发了一种新的攻击方法，针对源代码的作者归属进行攻击。Ramakrishnan
    和 Albarghouthi ([2022](#bib.bib195)) 研究了将多种常见后门注入深度学习模型的可能性，并开发了一种基于谱签名的保护方法。Schuster
    et al. ([2021](#bib.bib204)) 和 Wan et al. ([2022b](#bib.bib236)) 提出了通过数据中毒攻击神经代码模型，并在代码完成和代码搜索中进行了验证。Severi
    et al. ([2021](#bib.bib205)) 提出了一个基于解释的后门攻击方法，针对恶意软件分类器。总体而言，探索代码智能模型的稳健性和安全性是一个有趣且重要的研究方向。
- en: 7\. Conclusion
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: In this paper, we study deep learning for code intelligence by conducting a
    comprehensive survey, establishing a benchmark, as well as developing an open-source
    toolkit. We begin by providing a thorough literature review on deep learning for
    code intelligence, from the perspectives of code representations, deep learning
    techniques, application tasks, and public datasets. We then present an open-source
    toolkit for code intelligence, termed NaturalCC. On top of NaturalCC, we have
    benchmarked four popular application tasks about code intelligence, i.e., code
    summarization, code search, code completion, and type inference. We hope that
    our study contributes to a better understanding of the current status of code
    intelligence. We also hope that our toolkit and benchmark will contribute to the
    development of better code intelligence models.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们通过进行全面的调查、建立基准以及开发开源工具包来研究代码智能的深度学习。我们首先从代码表示、深度学习技术、应用任务和公共数据集的角度提供深入的文献回顾。然后，我们介绍了一个名为
    NaturalCC 的开源工具包。在 NaturalCC 基础上，我们对四个流行的代码智能应用任务进行了基准测试，即代码摘要、代码搜索、代码补全和类型推断。我们希望我们的研究能有助于更好地理解代码智能的现状。我们也希望我们的工具包和基准测试能促进更好的代码智能模型的发展。
- en: References
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: git (2019) 2019. GitHub. [https://www.github.com](https://www.github.com). [Online;
    accessed 1-May-2019].
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: git（2019）2019。GitHub。 [https://www.github.com](https://www.github.com)。[在线；访问日期：2019年5月1日]。
- en: sta (2019) 2019. StackOverflow. [https://www.stackoverflow.com](https://www.stackoverflow.com).
    [Online; accessed 1-May-2019].
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sta（2019）2019。StackOverflow。 [https://www.stackoverflow.com](https://www.stackoverflow.com)。[在线；访问日期：2019年5月1日]。
- en: Ahmad et al. (2021) Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei
    Chang. 2021. Unified Pre-training for Program Understanding and Generation. In
    *NAACL*. 2655–2668.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad 等人（2021）Wasi Ahmad、Saikat Chakraborty、Baishakhi Ray 和 Kai-Wei Chang。2021。用于程序理解和生成的统一预训练。发表于
    *NAACL*。2655–2668。
- en: Ahmad et al. (2020) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
    Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization.
    In *ACL*. 4998–5007.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad 等人（2020）Wasi Uddin Ahmad、Saikat Chakraborty、Baishakhi Ray 和 Kai-Wei Chang。2020。一种基于
    Transformer 的源代码摘要方法。发表于 *ACL*。4998–5007。
- en: Ahmed and Devanbu (2022) Toufique Ahmed and Premkumar Devanbu. 2022. Multilingual
    training for Software Engineering. In *ICSE*.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed 和 Devanbu（2022）Toufique Ahmed 和 Premkumar Devanbu。2022。软件工程的多语言训练。发表于
    *ICSE*。
- en: Allamanis et al. (2018a) Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,
    and Charles Sutton. 2018a. A survey of machine learning for big code and naturalness.
    *ACM Computing Surveys (CSUR)* 51, 4 (2018), 1–37.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis 等人（2018a）Miltiadis Allamanis、Earl T Barr、Premkumar Devanbu 和 Charles
    Sutton。2018a。大规模代码和自然性的机器学习调查。*ACM Computing Surveys (CSUR)* 51, 4 (2018), 1–37。
- en: 'Allamanis et al. (2020) Miltiadis Allamanis, Earl T Barr, Soline Ducousso,
    and Zheng Gao. 2020. Typilus: neural type hints. In *PLDI*. 91–105.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis 等人（2020）Miltiadis Allamanis、Earl T Barr、Soline Ducousso 和 Zheng Gao。2020。Typilus：神经类型提示。发表于
    *PLDI*。91–105。
- en: 'Allamanis and Brockschmidt (2017) Miltiadis Allamanis and Marc Brockschmidt.
    2017. Smartpaste: Learning to adapt source code. *arXiv:1705.07867* (2017).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis 和 Brockschmidt（2017）Miltiadis Allamanis 和 Marc Brockschmidt。2017。Smartpaste：学习适应源代码。*arXiv:1705.07867*（2017）。
- en: Allamanis et al. (2018b) Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud
    Khademi. 2018b. Learning to Represent Programs with Graphs. In *ICLR*.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis 等人（2018b）Miltiadis Allamanis、Marc Brockschmidt 和 Mahmoud Khademi。2018b。学习用图表示程序。发表于
    *ICLR*。
- en: Allamanis et al. (2016) Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016.
    A convolutional attention network for extreme summarization of source code. In
    *ICML*. 2091–2100.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis 等人（2016）Miltiadis Allamanis、Hao Peng 和 Charles Sutton。2016。一种用于极端摘要源代码的卷积注意力网络。发表于
    *ICML*。2091–2100。
- en: 'Alon et al. (2018) Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018.
    code2seq: Generating Sequences from Structured Representations of Code. In *ICLR*.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 等人（2018）Uri Alon、Shaked Brody、Omer Levy 和 Eran Yahav。2018。code2seq：从结构化代码表示生成序列。发表于
    *ICLR*。
- en: Alon et al. (2020) Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural
    language models of code. In *ICML*. 245–256.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 等人（2020）Uri Alon、Roy Sadaka、Omer Levy 和 Eran Yahav。2020。代码的结构语言模型。发表于 *ICML*。245–256。
- en: 'Alon et al. (2019) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2019. code2vec: Learning distributed representations of code. *POPL* 3 (2019),
    1–29.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 等人（2019）Uri Alon、Meital Zilberstein、Omer Levy 和 Eran Yahav。2019。code2vec：学习代码的分布式表示。*POPL*
    3 (2019), 1–29。
- en: Andreessen (2011) Marc Andreessen. 2011. Why software is eating the world. *Wall
    Street Journal* 20, 2011 (2011), C2.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andreessen（2011）Marc Andreessen。2011。为什么软件正在吞噬世界。*华尔街日报* 20, 2011 (2011), C2。
- en: 'Balog et al. (2017) Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian
    Nowozin, and Daniel Tarlow. 2017. DeepCoder: Learning to Write Programs. In *ICLR*.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Balog 等人（2017）Matej Balog、Alexander L. Gaunt、Marc Brockschmidt、Sebastian Nowozin
    和 Daniel Tarlow。2017。《DeepCoder: 学习编写程序》。发表于 *ICLR*。'
- en: Bansal et al. (2021) Aakash Bansal, Sakib Haque, and Collin McMillan. 2021.
    Project-Level Encoding for Neural Source Code Summarization of Subroutines. In
    *ICPC*. IEEE, 253–264.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal 等人（2021）Aakash Bansal、Sakib Haque 和 Collin McMillan。2021。《面向子程序的神经源代码摘要的项目级编码》。发表于
    *ICPC*。IEEE，253–264。
- en: Barone and Sennrich (2017) Antonio Valerio Miceli Barone and Rico Sennrich.
    2017. A Parallel Corpus of Python Functions and Documentation Strings for Automated
    Code Documentation and Code Generation. In *IJCNLP*. 314–319.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barone 和 Sennrich（2017）Antonio Valerio Miceli Barone 和 Rico Sennrich。2017。《用于自动代码文档生成和代码生成的
    Python 函数及文档字符串的平行语料库》。发表于 *IJCNLP*。314–319。
- en: 'Bavishi et al. (2019) Rohan Bavishi, Caroline Lemieux, Roy Fox, Koushik Sen,
    and Ion Stoica. 2019. AutoPandas: neural-backed generators for program synthesis.
    *OOPSLA* 3 (2019), 1–27.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bavishi 等人（2019）Rohan Bavishi、Caroline Lemieux、Roy Fox、Koushik Sen 和 Ion Stoica。2019。《AutoPandas:
    基于神经网络的程序合成生成器》。*OOPSLA* 3（2019），1–27。'
- en: Beltagy and Quirk (2016) Islam Beltagy and Chris Quirk. 2016. Improved semantic
    parsers for if-then statements. In *ACL*. 726–736.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy 和 Quirk（2016）Islam Beltagy 和 Chris Quirk。2016。《改进的语义解析器用于 if-then 语句》。发表于
    *ACL*。726–736。
- en: 'Ben-Nun et al. (2018) Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler.
    2018. Neural Code Comprehension: A Learnable Representation of Code Semantics.
    In *NeurIPS*. 3589–3601.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben-Nun 等人（2018）Tal Ben-Nun、Alice Shoshana Jakobovits 和 Torsten Hoefler。2018。《神经代码理解:
    可学习的代码语义表示》。发表于 *NeurIPS*。3589–3601。'
- en: 'Berabi et al. (2021) Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin
    Vechev. 2021. TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer.
    In *ICML*. 780–791.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Berabi 等人（2021）Berkay Berabi、Jingxuan He、Veselin Raychev 和 Martin Vechev。2021。《TFix:
    使用文本到文本转换器修复编码错误》。发表于 *ICML*。780–791。'
- en: Bhatia and Singh (2016) Sahil Bhatia and Rishabh Singh. 2016. Automated correction
    for syntax errors in programming assignments using recurrent neural networks.
    *arXiv:1603.06129* (2016).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatia 和 Singh（2016）Sahil Bhatia 和 Rishabh Singh。2016。《使用递归神经网络自动修正编程作业中的语法错误》。*arXiv:1603.06129*（2016）。
- en: Bielik and Vechev (2020) Pavol Bielik and Martin Vechev. 2020. Adversarial robustness
    for code. In *ICML*. 896–907.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bielik 和 Vechev（2020）Pavol Bielik 和 Martin Vechev。2020。《代码的对抗性鲁棒性》。发表于 *ICML*。896–907。
- en: Brockschmidt et al. (2019) Marc Brockschmidt, Miltiadis Allamanis, Alexander L.
    Gaunt, and Oleksandr Polozov. 2019. Generative Code Modeling with Graphs. In *ICLR*.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockschmidt 等人（2019）Marc Brockschmidt、Miltiadis Allamanis、Alexander L. Gaunt
    和 Oleksandr Polozov。2019。《使用图的生成性代码建模》。发表于 *ICLR*。
- en: Brody et al. (2020) Shaked Brody, Uri Alon, and Eran Yahav. 2020. A structural
    model for contextual code changes. *OOPSLA* 4 (2020), 1–28.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brody 等人（2020）Shaked Brody、Uri Alon 和 Eran Yahav。2020。《用于上下文代码更改的结构模型》。*OOPSLA*
    4（2020），1–28。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, et al. 2020. Language models are few-shot learners.
    *NeurIPS* 33 (2020), 1877–1901.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal 等。2020。《语言模型是少量样本学习者》。*NeurIPS* 33（2020），1877–1901。
- en: Büch and Andrzejak (2019) Lutz Büch and Artur Andrzejak. 2019. Learning-based
    recursive aggregation of abstract syntax trees for code clone detection. In *SANER*.
    95–104.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Büch 和 Andrzejak（2019）Lutz Büch 和 Artur Andrzejak。2019。《基于学习的递归聚合抽象语法树用于代码克隆检测》。发表于
    *SANER*。95–104。
- en: Bui et al. (2019a) Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2019a. Bilateral
    dependency neural networks for cross-language algorithm classification. In *SANER*.
    422–433.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bui 等人（2019a）Nghi DQ Bui、Yijun Yu 和 Lingxiao Jiang。2019a。《用于跨语言算法分类的双向依赖神经网络》。发表于
    *SANER*。422–433。
- en: 'Bui et al. (2019b) Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2019b. SAR: learning
    cross-language API mappings with little knowledge. In *ESEC/FSE*. 796–806.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bui 等人（2019b）Nghi DQ Bui、Yijun Yu 和 Lingxiao Jiang。2019b。《SAR: 学习跨语言 API 映射的方法》。发表于
    *ESEC/FSE*。796–806。'
- en: 'Bui et al. (2021a) Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2021a. InferCode:
    Self-Supervised Learning of Code Representations by Predicting Subtrees. In *ICSE*.
    1186–1197.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bui 等人（2021a）Nghi DQ Bui、Yijun Yu 和 Lingxiao Jiang。2021a。《InferCode: 通过预测子树进行自监督代码表示学习》。发表于
    *ICSE*。1186–1197。'
- en: Bui et al. (2021b) Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021b. Self-Supervised
    Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving
    Transformations. In *SIGIR*. ACM, 511–521.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bui 等人（2021b）Nghi D. Q. Bui、Yijun Yu 和 Lingxiao Jiang。2021b。《通过语义保持变换进行代码检索和摘要的自监督对比学习》。发表于
    *SIGIR*。ACM，511–521。
- en: Cai et al. (2018) Ruichu Cai, Boyan Xu, Zhenjie Zhang, Xiaoyan Yang, Zijian
    Li, and Zhihao Liang. 2018. An Encoder-Decoder Framework Translating Natural Language
    to Database Queries. In *IJCAI*. 3977–3983.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai等人（2018）蔡瑞初、徐博然、张震杰、杨晓艳、李自坚和梁志浩。2018年。一个将自然语言翻译为数据库查询的编码器-解码器框架。在*IJCAI*。3977–3983。
- en: Cambronero et al. (2019) Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen,
    and Satish Chandra. 2019. When deep learning met code search. In *ESEC/FSE*. 964–974.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cambronero等人（2019）José Cambronero、李宏宇、金书贤、Sen Koushik和Chandra Satish。2019年。当深度学习遇上代码搜索。在*ESEC/FSE*。964–974。
- en: 'Cao et al. (2022) Sicong Cao, Xiaobing Sun, Lili Bo, Rongxin Wu, Bin Li, and
    Chuanqi Tao. 2022. MVD: Memory-Related Vulnerability Detection Based on Flow-Sensitive
    Graph Neural Networks. In *ICSE*. 1456–1468.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人（2022）曹思聪、孙晓冰、博莉莉、吴荣鑫、李斌和陶川奇。2022年。MVD：基于流敏感图神经网络的内存相关漏洞检测。在*ICSE*。1456–1468。
- en: Carlini et al. (2019) Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland
    Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and
    Alexey Kurakin. 2019. On evaluating adversarial robustness. *arXiv:1902.06705*
    (2019).
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini等人（2019）Nicholas Carlini、Anish Athalye、Nicolas Papernot、Wieland Brendel、Jonas
    Rauber、Dimitris Tsipras、Ian Goodfellow、Aleksander Madry和Alexey Kurakin。2019年。评估对抗鲁棒性的研究。*arXiv:1902.06705*（2019）。
- en: Carlini and Wagner (2017) Nicholas Carlini and David Wagner. 2017. Towards evaluating
    the robustness of neural networks. In *S&P*. 39–57.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini和Wagner（2017）Nicholas Carlini和David Wagner。2017年。评估神经网络鲁棒性的研究。在*S&P*。39–57。
- en: 'Chai et al. (2022a) Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian,
    and Hua Wu. 2022a. ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining
    for Programming Languages. *arXiv preprint arXiv:2212.06742* (2022).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai等人（2022a）柴叶坤、王硕欢、庞超、孙宇、田昊和吴华。2022a。ERNIE-Code：超越以英语为中心的跨语言预训练编程语言。*arXiv
    preprint arXiv:2212.06742*（2022）。
- en: Chai et al. (2022b) Yitian Chai, Hongyu Zhang, Beijun Shen, and Xiaodong Gu.
    2022b. Cross-Domain Deep Code Search with Meta Learning. In *ICSE*. 487–498.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai等人（2022b）柴一田、张宏宇、沈贝军和顾晓东。2022b。跨领域深度代码搜索与元学习。在*ICSE*。487–498。
- en: 'Chakraborty et al. (2020) Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis,
    and Baishakhi Ray. 2020. Codit: Code editing with tree-based neural models. *TSE*
    (2020).'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakraborty等人（2020）Saikat Chakraborty、丁杨瑞博、米尔提亚迪斯·阿拉马尼斯和柴莎琪。2020年。Codit：基于树的神经模型进行代码编辑。*TSE*（2020）。
- en: Chakraborty and Ray (2021) Saikat Chakraborty and Baishakhi Ray. 2021. On Multi-Modal
    Learning of Editing Source Code. In *ASE*. IEEE, 443–455.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakraborty和Ray（2021）Saikat Chakraborty和Baishakhi Ray。2021年。关于编辑源代码的多模态学习。在*ASE*。IEEE，443–455。
- en: Chen et al. (2022) Fuxiang Chen, Fatemeh H. Fard, David Lo, and Timofey Bryksin.
    2022. On the transferability of pre-trained language models for low-resource programming
    languages. In *ICPC*. ACM, 401–412.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2022）陈福祥、Fatemeh H. Fard、大卫·洛和蒂莫费·布里克辛。2022年。预训练语言模型在低资源编程语言中的可迁移性。在*ICPC*。ACM，401–412。
- en: Chen et al. (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, et al. 2021b. Evaluating
    large language models trained on code. *arXiv preprint arXiv:2107.03374* (2021).
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2021b）马克·陈、杰瑞·特沃雷克、金慧宇、袁启明、亨里克·庞德·德·奥利维拉·平托、贾里德·卡普兰、哈里·爱德华兹等人。2021b。评估在代码上训练的大型语言模型。*arXiv
    preprint arXiv:2107.03374*（2021）。
- en: Chen et al. (2018) Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree
    Neural Networks for Program Translation. In *NeurIPS*. 2552–2562.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2018）陈新云、刘畅和宋曙光。2018年。树到树神经网络用于程序翻译。在*NeurIPS*。2552–2562。
- en: 'Chen et al. (2021a) Zimin Chen, Vincent Hellendoorn, Pascal Lamblin, Petros
    Maniatis, Pierre-Antoine Manzagol, et al. 2021a. PLUR: A Unifying, Graph-Based
    View of Program Learning, Understanding, and Repair. *NeurIPS* 34 (2021).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2021a）陈子敏、文森特·赫伦多恩、帕斯卡尔·兰布林、彼得罗斯·马尼亚提斯、皮埃尔-安托万·曼扎戈尔等人。2021a。PLUR：程序学习、理解和修复的统一图形视图。*NeurIPS*
    34（2021）。
- en: 'Chen et al. (2019) Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël
    Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-sequence
    learning for end-to-end program repair. *TSE* (2019).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2019）陈子敏、史蒂夫·詹姆斯·科姆鲁施、米歇尔·图法诺、路易斯-诺埃尔·普歇、Denys Poshyvanyk和马丁·蒙佩鲁斯。2019年。Sequencer：端到端程序修复的序列到序列学习。*TSE*（2019）。
- en: 'Cheng et al. (2021) Xiao Cheng, Haoyu Wang, Jiayi Hua, Guoai Xu, and Yulei
    Sui. 2021. DeepWukong: Statically detecting software vulnerabilities using deep
    graph neural network. *TOSEM* 30, 3 (2021), 1–33.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程等人（2021）程晓、王浩宇、华佳毅、徐国爱和隋雨雷。2021年。DeepWukong：使用深度图神经网络静态检测软件漏洞。*TOSEM* 30, 3（2021），1–33。
- en: 'Chicco (2021) Davide Chicco. 2021. Siamese neural networks: An overview. *Artificial
    Neural Networks* (2021), 73–94.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chicco（2021） Davide Chicco。2021。Siamese 神经网络：概述。*Artificial Neural Networks*（2021），73–94。
- en: Chirkova and Troshin (2021a) Nadezhda Chirkova and Sergey Troshin. 2021a. Empirical
    study of transformers for source code. In *ESEC/FSE*. 703–715.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chirkova 和 Troshin（2021a） Nadezhda Chirkova 和 Sergey Troshin。2021a。对源代码变换器的实证研究。发表于
    *ESEC/FSE*，703–715。
- en: Chirkova and Troshin (2021b) Nadezhda Chirkova and Sergey Troshin. 2021b. A
    Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for
    Source Code. In *NAACL*. 278–288.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chirkova 和 Troshin（2021b） Nadezhda Chirkova 和 Sergey Troshin。2021b。处理源代码深度学习中词汇外标识符的简单方法。发表于
    *NAACL*，278–288。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等（2022） Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav
    Mishra、Adam Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann
    等。2022。Palm：通过路径扩展语言建模。*arXiv 预印本 arXiv:2204.02311*（2022）。
- en: 'Christopoulou et al. (2022) Fenia Christopoulou, Gerasimos Lampouras, Milan
    Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin
    Li, et al. 2022. Pangu-coder: Program synthesis with function-level language modeling.
    *arXiv preprint arXiv:2207.11280* (2022).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christopoulou 等（2022） Fenia Christopoulou、Gerasimos Lampouras、Milan Gritta、Guchun
    Zhang、Yinpeng Guo、Zhongqi Li、Qi Zhang、Meng Xiao、Bo Shen、Lin Li 等。2022。Pangu-coder：使用函数级语言建模的程序合成。*arXiv
    预印本 arXiv:2207.11280*（2022）。
- en: Ciurumelea et al. (2020) Adelina Ciurumelea, Sebastian Proksch, and Harald C
    Gall. 2020. Suggesting comment completions for python using neural language models.
    In *SANER*. 456–467.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciurumelea 等（2020） Adelina Ciurumelea、Sebastian Proksch 和 Harald C Gall。2020。使用神经语言模型为
    Python 提供注释补全建议。发表于 *SANER*，456–467。
- en: Cui et al. (2022) Nan Cui, Yuze Jiang, Xiaodong Gu, and Beijun Shen. 2022. Zero-shot
    program representation learning. In *ICPC*. ACM, 60–70.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等（2022） Nan Cui、Yuze Jiang、Xiaodong Gu 和 Beijun Shen。2022。零样本程序表示学习。发表于
    *ICPC*，ACM，60–70。
- en: 'Cummins et al. (2021) Chris Cummins, Zacharias Fisches, Tal Ben-Nun, Torsten
    Hoefler, Michael O’Boyle, and Hugh Leather. 2021. ProGraML: A Graph-based Program
    Representation for Data Flow Analysis and Compiler Optimizations. In *ICML*.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cummins 等（2021） Chris Cummins、Zacharias Fisches、Tal Ben-Nun、Torsten Hoefler、Michael
    O’Boyle 和 Hugh Leather。2021。ProGraML：一种基于图的程序表示用于数据流分析和编译器优化。发表于 *ICML*。
- en: Cummins et al. (2017) Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh
    Leather. 2017. Synthesizing benchmarks for predictive modeling. In *CGO*. 86–99.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cummins 等（2017） Chris Cummins、Pavlos Petoumenos、Zheng Wang 和 Hugh Leather。2017。为预测建模合成基准。发表于
    *CGO*，86–99。
- en: Cvitkovic et al. (2019) Milan Cvitkovic, Badal Singh, and Animashree Anandkumar.
    2019. Open vocabulary learning on source code with a graph-structured cache. In
    *ICML*. 1475–1485.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cvitkovic 等（2019） Milan Cvitkovic、Badal Singh 和 Animashree Anandkumar。2019。使用图结构缓存进行源代码的开放词汇学习。发表于
    *ICML*，1475–1485。
- en: Dam et al. (2018) Hoa Khanh Dam, Truyen Tran, Trang Thi Minh Pham, Shien Wee
    Ng, John Grundy, and Aditya Ghose. 2018. Automatic feature learning for predicting
    vulnerable software components. *TSE* (2018).
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dam 等（2018） Hoa Khanh Dam、Truyen Tran、Trang Thi Minh Pham、Shien Wee Ng、John
    Grundy 和 Aditya Ghose。2018。自动特征学习用于预测易受攻击的软件组件。*TSE*（2018）。
- en: Deng et al. (2022) Zhongyang Deng, Ling Xu, Chao Liu, Meng Yan, Zhou Xu, and
    Yan Lei. 2022. Fine-grained Co-Attentive Representation Learning for Semantic
    Code Search. In *SANER*. 396–407.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2022） Zhongyang Deng、Ling Xu、Chao Liu、Meng Yan、Zhou Xu 和 Yan Lei。2022。用于语义代码搜索的细粒度共注意表示学习。发表于
    *SANER*，396–407。
- en: 'Devanbu et al. (2020) Prem Devanbu, Matthew B. Dwyer, Sebastian G. Elbaum,
    Michael Lowry, Kevin Moran, et al. 2020. Deep Learning & Software Engineering:
    State of Research and Future Directions. *CoRR* abs/2009.08525 (2020).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devanbu 等（2020） Prem Devanbu、Matthew B. Dwyer、Sebastian G. Elbaum、Michael Lowry、Kevin
    Moran 等。2020。深度学习与软件工程：研究现状与未来方向。*CoRR* abs/2009.08525（2020）。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *NAACL*. 4171–4186.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2019） Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019。BERT：深度双向变换器的预训练用于语言理解。发表于
    *NAACL*，4171–4186。
- en: 'Devlin et al. (2017) Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh
    Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017. Robustfill: Neural program
    learning under noisy i/o. In *ICML*. 990–998.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2017） Jacob Devlin、Jonathan Uesato、Surya Bhupatiraju、Rishabh Singh、Abdel-rahman
    Mohamed 和 Pushmeet Kohli。2017。Robustfill：在嘈杂的输入/输出下的神经程序学习。发表于 *ICML*，990–998。
- en: 'Dinella et al. (2020) Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik,
    Le Song, and Ke Wang. 2020. Hoppity: Learning graph transformations to detect
    and fix bugs in programs. In *ICLR*.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinella et al. (2020) 伊丽莎白·迪内拉、韩俊·戴、紫阳·李、马尤尔·奈克、乐松和柯·王。2020。Hoppity：学习图转换以检测和修复程序中的错误。发表于*ICLR*。
- en: Ding et al. (2022) Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari,
    Baishakhi Ray, and Saikat Chakraborty. 2022. Towards Learning (Dis)-Similarity
    of Source Code from Program Contrasts. In *ACL*. 6300–6312.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2022) 杨瑞博·丁、卢卡·布拉提、索拉布·普贾尔、亚历山德罗·莫拉里、贝莎基·雷和赛卡特·查克拉博提。2022。基于程序对比的源代码（不）相似性学习。发表于*ACL*。6300–6312。
- en: Dong and Lapata (2016) Li Dong and Mirella Lapata. 2016. Language to Logical
    Form with Neural Attention. In *ACL*.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong and Lapata (2016) 李·董和米雷拉·拉帕塔。2016。通过神经注意力将语言转化为逻辑形式。发表于*ACL*。
- en: Eykholt et al. (2018) Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li,
    Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018.
    Robust physical-world attacks on deep learning visual classification. In *CVPR*.
    1625–1634.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eykholt et al. (2018) 凯文·艾克霍特、伊万·叶夫蒂莫夫、厄尔伦斯·费尔南德斯、博·李、阿米尔·拉赫马提、肖维·肖、阿图尔·普拉卡什、塔达约希·小野和道恩·宋。2018。对深度学习视觉分类的鲁棒性物理世界攻击。发表于*CVPR*。1625–1634。
- en: 'Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
    Feng, et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages.
    In *Findings of EMNLP*. 1536–1547.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2020) 张银·风、大雅·郭、杜宇·唐、南端·段、肖成·风等。2020。CodeBERT：一种用于编程和自然语言的预训练模型。发表于*EMNLP发现*。1536–1547。
- en: Fernandes et al. (2019) Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt.
    2019. Structured Neural Summarization. In *ICLR*.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandes et al. (2019) 帕特里克·费尔南德斯、米尔提亚迪斯·阿拉马尼斯和马克·布洛克施密特。2019。结构化神经摘要。发表于*ICLR*。
- en: 'Fried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
    2022. Incoder: A generative model for code infilling and synthesis. *arXiv preprint
    arXiv:2204.05999* (2022).'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fried et al. (2022) 丹尼尔·弗里德、阿门·阿哈贾尼扬、杰西·林、斯达·王、埃里克·沃勒斯、弗蕾达·石、瑞奇·钟、温涛·易、卢克·泽特尔梅耶和迈克·刘易斯。2022。Incoder：用于代码填充和合成的生成模型。*arXiv预印本
    arXiv:2204.05999*（2022）。
- en: 'Fu et al. (2022) Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen,
    and Dinh Q. Phung. 2022. VulRepair: a T5-based automated software vulnerability
    repair. In *ESEC/FSE*. 935–947.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2022) 迈克尔·富、查克里特·坦提塔姆塔沃恩、郑乐、范·阮和丁Q·冯。2022。VulRepair：基于T5的自动化软件漏洞修复。发表于*ESEC/FSE*。935–947。
- en: 'Gao and Lyu (2022) Yuexiu Gao and Chen Lyu. 2022. M2TS: multi-scale multi-modal
    approach based on transformer for source code summarization. In *ICPC*. ACM, 24–35.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao and Lyu (2022) 岳秀·高和陈·吕。2022。M2TS：基于变换器的多尺度多模态源代码总结方法。发表于*ICPC*。ACM，24–35。
- en: Gao et al. (2021) Zhipeng Gao, Xin Xia, David Lo, John Grundy, and Thomas Zimmermann.
    2021. Automating the removal of obsolete TODO comments. In *ESEC/FSE*. 218–229.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2021) 直鹏·高、辛·夏、戴维·洛、约翰·格伦迪和托马斯·齐默曼。2021。自动化移除过时的TODO注释。发表于*ESEC/FSE*。218–229。
- en: 'Gardner et al. (2018) Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord,
    et al. 2018. AllenNLP: A Deep Semantic Natural Language Processing Platform. In
    *Proceedings of Workshop for NLP Open Source Software (NLP-OSS)*. 1–6.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gardner et al. (2018) 马特·加德纳、乔尔·格鲁斯、马克·诺伊曼、奥伊文德·塔福约德等。2018。AllenNLP：一个深度语义自然语言处理平台。发表于*自然语言处理开源软件研讨会（NLP-OSS）*。1–6。
- en: 'Geng et al. (2024) Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang,
    Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao. 2024. Large Language Models are
    Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning.
    (2024).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng et al. (2024) 明阳·耿、尚文·王、德尊·董、昊天·王、葛·李、智金、肖光·毛和向克·廖。2024。大型语言模型是少量样本总结器：通过上下文学习生成多意图评论。（2024）。
- en: Gong et al. (2022) Zi Gong, Cuiyun Gao, Yasheng Wang, Wenchao Gu, Yun Peng,
    and Zenglin Xu. 2022. Source Code Summarization with Structural Relative Position
    Guided Transformer. In *SANER*. 13–24.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. (2022) 兹·龚、崔云·高、雅生·王、温超·顾、云·彭和曾林·徐。2022。带有结构相对位置引导变换器的源代码总结。发表于*SANER*。13–24。
- en: Graves et al. (2014) Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural
    turing machines. *arXiv:1410.5401* (2014).
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves et al. (2014) 亚历克斯·格雷夫斯、格雷格·韦恩和伊沃·丹尼赫卡。2014。神经图灵机。*arXiv:1410.5401*（2014）。
- en: Gu et al. (2022) Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei
    Zhang, and Michael R. Lyu. 2022. Accelerating Code Search with Deep Hashing and
    Code Classification. In *ACL*. 2534–2544.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2022) 温超·顾、燕林·王、伦·杜、洪宇·张、石汉、董梅·张和迈克尔·R·吕。2022。通过深度哈希和代码分类加速代码搜索。发表于*ACL*。2534–2544。
- en: Gu et al. (2018) Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code
    search. In *ICSE*. 933–944.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2018) 肖东·顾、洪宇·张和成勋·金。2018。深度代码搜索。发表于*ICSE*。933–944。
- en: Gu et al. (2016) Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim.
    2016. Deep API learning. In *Proceedings of the 2016 24th ACM SIGSOFT International
    Symposium on Foundations of Software Engineering*. 631–642.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2016）Xiaodong Gu, Hongyu Zhang, Dongmei Zhang 和 Sunghun Kim. 2016. 深度 API
    学习。在 *2016 年第 24 届 ACM SIGSOFT 软件工程基础国际研讨会论文集*。631–642。
- en: 'Gu et al. (2017) Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim.
    2017. DeepAM: Migrate APIs with Multi-Modal Sequence to Sequence Learning. In
    *IJCAI*. 3675–3681.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2017）Xiaodong Gu, Hongyu Zhang, Dongmei Zhang 和 Sunghun Kim. 2017. DeepAM：通过多模态序列到序列学习迁移
    API。在 *IJCAI*。3675–3681。
- en: Gui et al. (2022) Yi Gui, Yao Wan, Hongyu Zhang, Huifang Huang, Yulei Sui, Guandong
    Xu, Zhiyuan Shao, and Hai Jin. 2022. Cross-Language Binary-Source Code Matching
    with Intermediate Representations. In *SANER*.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gui 等（2022）Yi Gui, Yao Wan, Hongyu Zhang, Huifang Huang, Yulei Sui, Guandong
    Xu, Zhiyuan Shao 和 Hai Jin. 2022. 基于中间表示的跨语言二进制源代码匹配。在 *SANER*。
- en: Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. *arXiv preprint
    arXiv:2306.11644* (2023).
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunasekar 等（2023）Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi 等。2023. 教科书就是你所需要的一切。*arXiv 预印本 arXiv:2306.11644*（2023）。
- en: 'Guo et al. (2022b) Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and
    Jian Yin. 2022b. UniXcoder: Unified Cross-Modal Pre-training for Code Representation.
    In *ACL*. 7212–7225.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2022b）Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou 和 Jian Yin.
    2022b. UniXcoder：用于代码表示的统一跨模态预训练。在 *ACL*。7212–7225。
- en: 'Guo et al. (2021) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    Liu, Long Zhou, et al. 2021. GraphCodeBERT: Pre-training Code Representations
    with Data Flow. In *ICLR*.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2021）Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu,
    Long Zhou 等。2021. GraphCodeBERT：通过数据流预训练代码表示。在 *ICLR*。
- en: Guo et al. (2022c) Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc Brockschmidt,
    and Miltiadis Allamanis. 2022c. Learning to Complete Code with Sketches. In *ICLR*.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2022c）Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc Brockschmidt
    和 Miltiadis Allamanis. 2022c. 学习用草图完成代码。在 *ICLR*。
- en: Guo et al. (2022a) Juncai Guo, Jin Liu, Yao Wan, Li Li, and Pingyi Zhou. 2022a.
    Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization.
    In *ACL*. 486–500.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2022a）Juncai Guo, Jin Liu, Yao Wan, Li Li 和 Pingyi Zhou. 2022a. 使用三元组位置建模源代码的层次语法结构。在
    *ACL*。486–500。
- en: 'Gupta et al. (2020) Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn
    Song. 2020. Synthesize, Execute and Debug: Learning to Repair for Neural Program
    Synthesis. In *NeurIPS*.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2020）Kavi Gupta, Peter Ebert Christensen, Xinyun Chen 和 Dawn Song. 2020.
    综合、执行和调试：神经程序合成的修复学习。在 *NeurIPS*。
- en: Gupta et al. (2018) Rahul Gupta, Aditya Kanade, and Shirish Shevade. 2018. Deep
    reinforcement learning for programming language correction. *arXiv:1801.10467*
    (2018).
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2018）Rahul Gupta, Aditya Kanade 和 Shirish Shevade. 2018. 用于编程语言纠正的深度强化学习。*arXiv:1801.10467*（2018）。
- en: Gupta et al. (2019) R Gupta, A Kanade, and S Shevade. 2019. Neural attribution
    for semantic bug-localization in student programs. *NeurIPS* 32 (2019).
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2019）R Gupta, A Kanade 和 S Shevade. 2019. 用于学生程序的语义错误定位的神经归因。*NeurIPS*
    32（2019）。
- en: 'Gupta et al. (2017) Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade.
    2017. Deepfix: Fixing common c language errors by deep learning. In *AAAI*.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2017）Rahul Gupta, Soham Pal, Aditya Kanade 和 Shirish Shevade. 2017.
    Deepfix：通过深度学习修复常见 C 语言错误。在 *AAAI*。
- en: Hadi et al. (2022) Mohammad Abdul Hadi, Imam Nur Bani Yusuf, Ferdian Thung,
    Kien Gia Luong, Lingxiao Jiang, Fatemeh H. Fard, and David Lo. 2022. On the effectiveness
    of pretrained models for API learning. In *ICPC*. ACM, 309–320.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadi 等（2022）Mohammad Abdul Hadi, Imam Nur Bani Yusuf, Ferdian Thung, Kien Gia
    Luong, Lingxiao Jiang, Fatemeh H. Fard 和 David Lo. 2022. 预训练模型在 API 学习中的有效性。在
    *ICPC*。ACM，309–320。
- en: Haldar et al. (2020) Rajarshi Haldar, Lingfei Wu, JinJun Xiong, and Julia Hockenmaier.
    2020. A Multi-Perspective Architecture for Semantic Code Search. In *ACL*. 8563–8568.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haldar 等（2020）Rajarshi Haldar, Lingfei Wu, JinJun Xiong 和 Julia Hockenmaier.
    2020. 一种用于语义代码搜索的多视角架构。在 *ACL*。8563–8568。
- en: Haque et al. (2021) Sakib Haque, Aakash Bansal, Lingfei Wu, and Collin McMillan.
    2021. Action Word Prediction for Neural Source Code Summarization. In *SANER*.
    IEEE, 330–341.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haque 等（2021）Sakib Haque, Aakash Bansal, Lingfei Wu 和 Collin McMillan. 2021.
    神经源代码总结的动作词预测。在 *SANER*。IEEE，330–341。
- en: Haque et al. (2020) Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan.
    2020. Improved automatic summarization of subroutines via attention to file context.
    In *MSR*. 300–310.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haque 等人（2020）Sakib Haque、Alexander LeClair、Lingfei Wu 和 Collin McMillan。2020年。通过关注文件上下文改进子例程的自动摘要。在
    *MSR*。300–310。
- en: Harer et al. (2018) Jacob Harer, Onur Ozdemir, Tomo Lazovich, Christopher P.
    Reale, Rebecca L. Russell, Louis Y. Kim, and Sang Peter Chin. 2018. Learning to
    Repair Software Vulnerabilities with Generative Adversarial Networks. In *NeurIPS*.
    7944–7954.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harer 等人（2018）Jacob Harer、Onur Ozdemir、Tomo Lazovich、Christopher P. Reale、Rebecca
    L. Russell、Louis Y. Kim 和 Sang Peter Chin。2018年。使用生成对抗网络学习修复软件漏洞。在 *NeurIPS*。7944–7954。
- en: Hassan et al. (2018) Mostafa Hassan, Caterina Urban, Marco Eilers, and Peter
    Müller. 2018. MaxSMT-based type inference for Python 3\. In *International Conference
    on Computer Aided Verification*. 12–19.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassan 等人（2018）Mostafa Hassan、Caterina Urban、Marco Eilers 和 Peter Müller。2018年。基于
    MaxSMT 的 Python 3 类型推断。在 *International Conference on Computer Aided Verification*。12–19。
- en: Hata et al. (2018) Hideaki Hata, Emad Shihab, and Graham Neubig. 2018. Learning
    to generate corrective patches using neural machine translation. *arXiv:1812.07170*
    (2018).
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hata 等人（2018）Hideaki Hata、Emad Shihab 和 Graham Neubig。2018年。使用神经机器翻译学习生成纠正补丁。*arXiv:1812.07170*（2018年）。
- en: Hayati et al. (2018) Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru,
    Pengcheng Yin, Anthony Tomasic, and Graham Neubig. 2018. Retrieval-Based Neural
    Code Generation. In *EMNLP*. 925–930.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hayati 等人（2018）Shirley Anugrah Hayati、Raphael Olivier、Pravalika Avvaru、Pengcheng
    Yin、Anthony Tomasic 和 Graham Neubig。2018年。基于检索的神经代码生成。在 *EMNLP*。925–930。
- en: Hellendoorn et al. (2018) Vincent J Hellendoorn, Christian Bird, Earl T Barr,
    and Miltiadis Allamanis. 2018. Deep learning type inference. In *ESEC/FSE*. 152–162.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hellendoorn 等人（2018）Vincent J Hellendoorn、Christian Bird、Earl T Barr 和 Miltiadis
    Allamanis。2018年。深度学习类型推断。在 *ESEC/FSE*。152–162。
- en: 'Henkel et al. (2018) Jordan Henkel, Shuvendu K Lahiri, Ben Liblit, and Thomas
    Reps. 2018. Code vectors: Understanding programs through embedded abstracted symbolic
    traces. In *ESEC/FSE*. 163–174.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henkel 等人（2018）Jordan Henkel、Shuvendu K Lahiri、Ben Liblit 和 Thomas Reps。2018年。代码向量：通过嵌入的抽象符号痕迹理解程序。在
    *ESEC/FSE*。163–174。
- en: Hinton et al. (2006) Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006.
    A fast learning algorithm for deep belief nets. *Neural computation* 18, 7 (2006),
    1527–1554.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等人（2006）Geoffrey E Hinton、Simon Osindero 和 Yee-Whye Teh。2006年。一种快速学习算法用于深度置信网络。*Neural
    computation* 18, 7（2006年），1527–1554。
- en: 'Hoang et al. (2020) Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall.
    2020. Cc2vec: Distributed representations of code changes. In *ICSE*. 518–529.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoang 等人（2020）Thong Hoang、Hong Jin Kang、David Lo 和 Julia Lawall。2020年。Cc2vec：代码变更的分布式表示。在
    *ICSE*。518–529。
- en: Hu et al. (2018a) Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018a. Deep
    code comment generation. In *ICPC*. 200–20010.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2018a）Xing Hu、Ge Li、Xin Xia、David Lo 和 Zhi Jin。2018a年。深度代码注释生成。在 *ICPC*。200–20010。
- en: Hu et al. (2018b) Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin.
    2018b. Summarizing source code with transferred api knowledge.(2018). In *IJCAI*,
    Vol. 19\. 2269–2275.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2018b）Xing Hu、Ge Li、Xin Xia、David Lo、Shuai Lu 和 Zhi Jin。2018b年。利用转移的 API
    知识总结源代码。（2018年）。在 *IJCAI*，第19卷。2269–2275。
- en: 'Hu et al. (2022) Yutao Hu, Deqing Zou, Junru Peng, Yueming Wu, Junjie Shan,
    and Hai Jin. 2022. TreeCen: Building Tree Graph for Scalable Semantic Code Clone
    Detection. In *ASE*. ACM, 109:1–109:12.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2022）Yutao Hu、Deqing Zou、Junru Peng、Yueming Wu、Junjie Shan 和 Hai Jin。2022年。TreeCen：构建用于可扩展语义代码克隆检测的树图。在
    *ASE*。ACM，109:1–109:12。
- en: Huang et al. (2022) Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming
    Zhu, and Qinghua Lu. 2022. Prompt-tuned Code Language Model as a Neural Knowledge
    Base for Type Inference in Statically-Typed Partial Code. In *ASE*. ACM, 79:1–79:13.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2022）Qing Huang、Zhiqiang Yuan、Zhenchang Xing、Xiwei Xu、Liming Zhu 和
    Qinghua Lu。2022年。Prompt-tuned 代码语言模型作为静态类型部分代码类型推断的神经知识库。在 *ASE*。ACM，79:1–79:13。
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of
    semantic code search. *arXiv:1909.09436* (2019).'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Husain 等人（2019）Hamel Husain、Ho-Hsiang Wu、Tiferet Gazit、Miltiadis Allamanis 和
    Marc Brockschmidt。2019年。Codesearchnet 挑战：评估语义代码搜索的现状。*arXiv:1909.09436*（2019年）。
- en: Iyer et al. (2016) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2016. Summarizing source code using a neural attention model. In
    *ACL*. 2073–2083.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer 等人（2016）Srinivasan Iyer、Ioannis Konstas、Alvin Cheung 和 Luke Zettlemoyer。2016年。使用神经注意力模型总结源代码。在
    *ACL*。2073–2083。
- en: Iyer et al. (2018) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2018. Mapping Language to Code in Programmatic Context. In *EMNLP*.
    1643–1652.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer 等人（2018）Srinivasan Iyer、Ioannis Konstas、Alvin Cheung 和 Luke Zettlemoyer。2018年。在程序化上下文中映射语言到代码。在
    *EMNLP*。1643–1652。
- en: Jain et al. (2021) Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph
    Gonzalez, and Ion Stoica. 2021. Contrastive Code Representation Learning. In *EMNLP*.
    5954–5971.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain et al. (2021) Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph
    Gonzalez, 和 Ion Stoica。2021年。《对比代码表示学习》。发表于*EMNLP*，5954–5971。
- en: Jiang et al. (2017) He Jiang, Jingxuan Zhang, Zhilei Ren, and Tao Zhang. 2017.
    An unsupervised approach for discovering relevant tutorial fragments for APIs.
    In *ICSE*. 38–48.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2017) He Jiang, Jingxuan Zhang, Zhilei Ren, 和 Tao Zhang。2017年。《一种发现API相关教程片段的无监督方法》。发表于*ICSE*，38–48。
- en: Jiang et al. (2019) Jiajun Jiang, Yingfei Xiong, and Xin Xia. 2019. A manual
    inspection of Defects4J bugs and its implications for automatic program repair.
    *Sci. China Inf. Sci.* 62, 10 (2019), 200102:1–200102:16.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2019) Jiajun Jiang, Yingfei Xiong, 和 Xin Xia。2019年。《Defects4J漏洞的手动检查及其对自动程序修复的影响》。*Sci.
    China Inf. Sci.* 62, 10 (2019)，200102:1–200102:16。
- en: 'Jiang et al. (2021a) Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021a. CURE:
    Code-Aware Neural Machine Translation for Automatic Program Repair. In *ICSE*.
    1161–1173.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2021a) Nan Jiang, Thibaud Lutellier, 和 Lin Tan。2021a年。《CURE:
    面向自动程序修复的代码感知神经机器翻译》。发表于*ICSE*，1161–1173。'
- en: 'Jiang et al. (2021b) Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei
    Lyu. 2021b. TreeBERT: A tree-based pre-trained model for programming language.
    In *Uncertainty in Artificial Intelligence*. 54–63.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2021b) Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, 和 Lei Lyu。2021b年。《TreeBERT:
    一种基于树的编程语言预训练模型》。发表于*Uncertainty in Artificial Intelligence*，54–63。'
- en: Jin et al. (2022) Dun Jin, Peiyu Liu, and Zhenfang Zhu. 2022. Automatically
    Generating Code Comment Using Heterogeneous Graph Neural Networks. In *SANER*.
    1078–1088.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. (2022) Dun Jin, Peiyu Liu, 和 Zhenfang Zhu。2022年。《使用异构图神经网络自动生成代码注释》。发表于*SANER*，1078–1088。
- en: Kanade et al. (2020) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. 2020. Learning and Evaluating Contextual Embedding of Source Code.
    In *ICML*. 5110–5121.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanade et al. (2020) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, 和 Kensen
    Shi。2020年。《学习和评估源代码的上下文嵌入》。发表于*ICML*，5110–5121。
- en: 'Karampatsis et al. (2020) Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes,
    Charles Sutton, and Andrea Janes. 2020. Big code!= big vocabulary: Open-vocabulary
    models for source code. In *ICSE*. 1073–1085.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Karampatsis et al. (2020) Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes,
    Charles Sutton, 和 Andrea Janes。2020年。《大代码!=大词汇量: 开放词汇模型用于源代码》。发表于*ICSE*，1073–1085。'
- en: Kim et al. (2021) Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra.
    2021. Code prediction by feeding trees to transformers. In *ICSE*. 150–162.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2021) Seohyun Kim, Jinman Zhao, Yuchi Tian, 和 Satish Chandra。2021年。《通过将树结构输入到变换器中进行代码预测》。发表于*ICSE*，150–162。
- en: 'Lachaux et al. (2021) Marie-Anne Lachaux, Baptiste Rozière, Marc Szafraniec,
    and Guillaume Lample. 2021. DOBF: A Deobfuscation Pre-Training Objective for Programming
    Languages. In *NeurIPS*. 14967–14979.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lachaux et al. (2021) Marie-Anne Lachaux, Baptiste Rozière, Marc Szafraniec,
    和 Guillaume Lample。2021年。《DOBF: 一种针对编程语言的去混淆预训练目标》。发表于*NeurIPS*，14967–14979。'
- en: 'Lattner and Adve (2004) Chris Lattner and Vikram Adve. 2004. LLVM: A compilation
    framework for lifelong program analysis & transformation. In *CGO*. 75–86.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lattner and Adve (2004) Chris Lattner 和 Vikram Adve。2004年。《LLVM: 一个终身程序分析与转换的编译框架》。发表于*CGO*，75–86。'
- en: Le et al. (2018) Tue Le, Tuan Nguyen, Trung Le, Dinh Phung, Paul Montague, Olivier
    De Vel, and Lizhen Qu. 2018. Maximal divergence sequential autoencoder for binary
    software vulnerability detection. In *ICLR*.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le et al. (2018) Tue Le, Tuan Nguyen, Trung Le, Dinh Phung, Paul Montague, Olivier
    De Vel, 和 Lizhen Qu。2018年。《用于二进制软件漏洞检测的最大散度序列自编码器》。发表于*ICLR*。
- en: LeClair et al. (2018) Alexander LeClair, Zachary Eberhart, and Collin McMillan.
    2018. Adapting neural text classification for improved software categorization.
    In *ICSME*. 461–472.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair et al. (2018) Alexander LeClair, Zachary Eberhart, 和 Collin McMillan。2018年。《适应神经文本分类以改进软件分类》。发表于*ICSME*，461–472。
- en: LeClair et al. (2020) Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin
    McMillan. 2020. Improved code summarization via a graph neural network. In *ICPC*.
    184–195.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair et al. (2020) Alexander LeClair, Sakib Haque, Lingfei Wu, 和 Collin McMillan。2020年。《通过图神经网络改进代码摘要》。发表于*ICPC*，184–195。
- en: LeClair et al. (2019) Alexander LeClair, Siyuan Jiang, and Collin McMillan.
    2019. A neural model for generating natural language summaries of program subroutines.
    In *ICSE*. 795–806.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair et al. (2019) Alexander LeClair, Siyuan Jiang, 和 Collin McMillan。2019年。《生成程序子例程自然语言摘要的神经模型》。发表于*ICSE*，795–806。
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    et al. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
    Generation, Translation, and Comprehension. In *ACL*. 7871–7880.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    等。2020年。《BART: 适用于自然语言生成、翻译和理解的去噪序列到序列预训练》。发表于*ACL*，7871–7880。'
- en: Li et al. (2023b) Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023b. Enabling Programming
    Thinking in Large Language Models Toward Code Generation. *arXiv preprint arXiv:2305.06599*
    (2023).
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023b）Jia Li, Ge Li, Yongmin Li 和 Zhi Jin。2023b。使大型语言模型能够进行编程思维以生成代码。*arXiv
    预印本 arXiv:2305.06599* (2023)。
- en: Li et al. (2023c) Jia Li, Ge Li, Chongyang Tao, Huangzhao Zhang, Fang Liu, and
    Zhi Jin. 2023c. Large Language Model-Aware In-Context Learning for Code Generation.
    *arXiv preprint arXiv:2310.09748* (2023).
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023c）Jia Li, Ge Li, Chongyang Tao, Huangzhao Zhang, Fang Liu 和 Zhi Jin。2023c。大型语言模型感知的上下文学习用于代码生成。*arXiv
    预印本 arXiv:2310.09748* (2023)。
- en: 'Li et al. (2021a) Jia Li, Yongmin Li, Ge Li, Xing Hu, Xin Xia, and Zhi Jin.
    2021a. EditSum: A Retrieve-and-Edit Framework for Source Code Summarization. In
    *ASE*. 155–166.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021a）Jia Li, Yongmin Li, Ge Li, Xing Hu, Xin Xia 和 Zhi Jin。2021a。EditSum：一种用于源代码摘要的检索与编辑框架。发表于
    *ASE*。155–166。
- en: Li et al. (2018a) Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018a.
    Code Completion with Neural Attention and Pointer Networks. In *IJCAI*. 4159–4165.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2018a）Jian Li, Yue Wang, Michael R. Lyu 和 Irwin King。2018a。使用神经注意力和指针网络进行代码补全。发表于
    *IJCAI*。4159–4165。
- en: 'Li et al. (2022e) Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan
    Hua, Geng Liang, and Chun Zuo. 2022e. AUGER: automatically generating review comments
    with pre-training models. In *ESEC/FSE*. 1009–1021.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022e）Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan Hua,
    Geng Liang 和 Chun Zuo。2022e。AUGER：使用预训练模型自动生成评审评论。发表于 *ESEC/FSE*。1009–1021。
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, et al. 2023a. StarCoder:
    may the source be with you! *arXiv preprint arXiv:2305.06161* (2023).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023a）Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis
    Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki 等。2023a。StarCoder：愿源代码与你同在！*arXiv
    预印本 arXiv:2305.06161* (2023)。
- en: 'Li et al. (2022b) Xiaonan Li, Yeyun Gong, Yelong Shen, et al. 2022b. CodeRetriever:
    Unimodal and Bimodal Contrastive Learning. In *EMNLP*.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022b）Xiaonan Li, Yeyun Gong, Yelong Shen 等。2022b。CodeRetriever：单模态和双模态对比学习。发表于
    *EMNLP*。
- en: Li et al. (2022a) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, et al. 2022a. Competition-Level Code
    Generation with AlphaCode. *Science* 378, 6624 (2022), 1092–1097.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022a）Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
    Rémi Leblond, Tom Eccles 等。2022a。竞争级代码生成与 AlphaCode。*Science* 378, 6624 (2022),
    1092–1097。
- en: Li et al. (2016) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S.
    Zemel. 2016. Gated Graph Sequence Neural Networks. In *ICLR*.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2016）Yujia Li, Daniel Tarlow, Marc Brockschmidt 和 Richard S. Zemel。2016。门控图序列神经网络。发表于
    *ICLR*。
- en: 'Li et al. (2020) Yi Li, Shaohua Wang, and Tien N Nguyen. 2020. Dlfix: Context-based
    code transformation learning for automated program repair. In *ICSE*. 602–614.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Yi Li, Shaohua Wang 和 Tien N Nguyen。2020。Dlfix：基于上下文的代码转换学习用于自动程序修复。发表于
    *ICSE*。602–614。
- en: Li et al. (2021b) Yi Li, Shaohua Wang, and Tien N Nguyen. 2021b. Fault Localization
    with Code Coverage Representation Learning. In *ICSE*. 661–673.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021b）Yi Li, Shaohua Wang 和 Tien N Nguyen。2021b。利用代码覆盖表示学习进行故障定位。发表于 *ICSE*。661–673。
- en: Li et al. (2021c) Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021c. Vulnerability
    detection with fine-grained interpretations. In *ESEC/FSE*. 292–303.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021c）Yi Li, Shaohua Wang 和 Tien N. Nguyen。2021c。利用细粒度解释进行漏洞检测。发表于 *ESEC/FSE*。292–303。
- en: 'Li et al. (2022d) Yi Li, Shaohua Wang, and Tien N. Nguyen. 2022d. DEAR: A Novel
    Deep Learning-based Approach for Automated Program Repair. In *ICSE*. 511–523.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022d）Yi Li, Shaohua Wang 和 Tien N. Nguyen。2022d。DEAR：一种基于深度学习的自动程序修复新方法。发表于
    *ICSE*。511–523。
- en: Li et al. (2019) Yi Li, Shaohua Wang, Tien N Nguyen, and Son Van Nguyen. 2019.
    Improving bug detection via context-based code representation learning and attention-based
    neural networks. *OOPSLA* 3 (2019), 1–30.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019）Yi Li, Shaohua Wang, Tien N Nguyen 和 Son Van Nguyen。2019。通过基于上下文的代码表示学习和注意力机制神经网络来改善错误检测。*OOPSLA*
    3 (2019), 1–30。
- en: Li et al. (2022c) Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang,
    Sen Nie, and Shi Wu. 2022c. Unleashing the Power of Compiler Intermediate Representation
    to Enhance Neural Program Embeddings. In *ICSE*. 2253–2265.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022c）Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen
    Nie 和 Shi Wu。2022c。释放编译器中间表示的力量以增强神经程序嵌入。发表于 *ICSE*。2253–2265。
- en: 'Li et al. (2021d) Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and
    Zhaoxuan Chen. 2021d. SySeVR: A framework for using deep learning to detect software
    vulnerabilities. *TDSC* (2021).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021d）Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu 和 Zhaoxuan Chen。2021d。SySeVR：一个利用深度学习检测软件漏洞的框架。*TDSC*
    (2021)。
- en: 'Li et al. (2018b) Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan
    Wang, Zhijun Deng, and Yuyi Zhong. 2018b. VulDeePecker: A Deep Learning-Based
    System for Vulnerability Detection. In *NDSS*.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2018b) 珍李、德庆邹、寿怀徐、辛雨欧、海锦、苏娟王、志军邓和瑜依钟。2018b。VulDeePecker：一种基于深度学习的漏洞检测系统。发表于*NDSS*。
- en: Lin et al. (2021) Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen,
    Hui Li, and Rongxin Wu. 2021. Improving Code Summarization with Block-wise Abstract
    Syntax Tree Splitting. In *ICPC*. IEEE, 184–195.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2021) 陈林、志超欧阳、俊青庄、建强陈、惠李和荣鑫吴。2021。通过块级抽象语法树拆分改进代码总结。发表于*ICPC*。IEEE，184–195。
- en: Ling et al. (2021b) Chunyang Ling, Yanzhen Zou, and Bing Xie. 2021b. Graph Neural
    Network Based Collaborative Filtering for API Usage Recommendation. In *SANER*.
    IEEE, 36–47.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling et al. (2021b) 春阳Ling、燕珍邹和冰谢。2021b。基于图神经网络的协同过滤用于API使用推荐。发表于*SANER*。IEEE，36–47。
- en: Ling et al. (2016) Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz
    Hermann, Tomáš Kočiskỳ, Fumin Wang, and Andrew Senior. 2016. Latent Predictor
    Networks for Code Generation. In *ACL*. 599–609.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling et al. (2016) 王玲、菲尔·布伦森、爱德华·格雷芬斯特、卡尔·莫里茨·赫尔曼、托马斯·科西基、冯敏和安德鲁·高级。2016。用于代码生成的潜在预测网络。发表于*ACL*。599–609。
- en: Ling et al. (2021a) Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei
    Ma, Fangli Xu, Alex X. Liu, Chunming Wu, and Shouling Ji. 2021a. Deep Graph Matching
    and Searching for Semantic Code Retrieval. *TKDD* 15, 5 (2021), 88:1–88:21.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling et al. (2021a) 项玲、凌飞吴、赛卓王、高宁潘、腾飞马、芳丽徐、亚历克斯·X·刘、春明吴和寿灵吉。2021a。深度图匹配与搜索用于语义代码检索。*TKDD*
    15, 5（2021），88:1–88:21。
- en: Liu et al. (2023) Chao Liu, Xuanlin Bao, Hongyu Zhang, Neng Zhang, Haibo Hu,
    Xiaohong Zhang, and Meng Yan. 2023. Improving ChatGPT Prompt for Code Generation.
    *arXiv preprint arXiv:2305.08360* (2023).
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) 超刘、宣林包、洪宇张、能张、海波胡、晓红张和孟岩。2023。改进ChatGPT提示以进行代码生成。*arXiv预印本arXiv:2305.08360*（2023）。
- en: Liu et al. (2016a) Chang Liu, Xinyun Chen, Eui Chul Shin, Mingcheng Chen, and
    Dawn Song. 2016a. Latent attention for if-then program synthesis. *NeurIPS* 29
    (2016), 4574–4582.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2016a) 常刘、辛云陈、恩熙·贞、明成陈和道恩·宋。2016a。用于if-then程序合成的潜在注意力。*NeurIPS*
    29 (2016)，4574–4582。
- en: Liu et al. (2016b) Chang Liu, Xin Wang, Richard Shin, Joseph E Gonzalez, and
    Dawn Song. 2016b. Neural code completion. (2016).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2016b) 常刘、辛王、理查德·申、约瑟夫·E·冈萨雷斯和道恩·宋。2016b。神经代码补全。（2016）。
- en: Liu et al. (2020b) Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin.
    2020b. A Self-Attentional Neural Architecture for Code Completion with Multi-Task
    Learning. In *ICPC*. 37–47.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020b) 方刘、葛李、博林·魏、辛夏、志毅·傅和智金。2020b。具有多任务学习的自注意神经结构用于代码补全。发表于*ICPC*。37–47。
- en: Liu et al. (2020c) Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020c. Multi-task
    learning based pre-trained language model for code completion. In *ASE*. 473–485.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020c) 方刘、葛李、云飞赵和智金。2020c。基于多任务学习的预训练语言模型用于代码补全。发表于*ASE*。473–485。
- en: Liu et al. (2020e) Fang Liu, Lu Zhang, and Zhi Jin. 2020e. Modeling programs
    hierarchically with stack-augmented LSTM. *Journal of Systems and Software* 164
    (2020), 110547.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020e) 方刘、陆张和智金。2020e。使用堆栈增强LSTM对程序进行层次建模。*系统与软件期刊* 164（2020），110547。
- en: Liu et al. (2019) Kui Liu, Dongsun Kim, Tegawendé F Bissyandé, Taeyoung Kim,
    Kisub Kim, Anil Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning to spot
    and refactor inconsistent method names. In *ICSE*. 1–12.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) 崔刘、东善金、特戈温德·F·比萨安德、太永金、基舒金、安尼尔·科云库、孙泰金和伊夫·勒·特朗。2019。学习识别和重构不一致的方法名称。发表于*ICSE*。1–12。
- en: Liu et al. (2021a) Qianjun Liu, Shouling Ji, Changchang Liu, and Chunming Wu.
    2021a. A Practical Black-box Attack on Source Code Authorship Identification Classifiers.
    *TIFS* (2021).
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021a) 钱军刘、寿灵吉、常常刘和春明吴。2021a。针对源代码作者识别分类器的实际黑盒攻击。*TIFS*（2021）。
- en: Liu et al. (2020a) Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang
    Liu. 2020a. Retrieval-Augmented Generation for Code Summarization via Hybrid GNN.
    In *ICLR*.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020a) 尚庆刘、余陈、肖飞谢、景凯肖和杨刘。2020a。通过混合GNN进行代码总结的检索增强生成。发表于*ICLR*。
- en: Liu et al. (2021b) Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin
    Qiu, and Xun Wang. 2021b. Combining Graph Neural Networks with Expert Knowledge
    for Smart Contract Vulnerability Detection. *TKDE* (2021).
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021b) 郑光刘、彭钱、肖阳王、袁庄、林秋和勋王。2021b。将图神经网络与专家知识结合用于智能合约漏洞检测。*TKDE*（2021）。
- en: Liu et al. (2020d) Zhongxin Liu, Xin Xia, Meng Yan, and Shanping Li. 2020d.
    Automating just-in-time comment updating. In *ASE*. 585–597.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020d) 中心刘、辛夏、孟岩和单平李。2020d。自动化即时注释更新。发表于*ASE*。585–597。
- en: 'López et al. (2022) José Antonio Hernández López, Martin Weyssow, Jesús Sánchez
    Cuadrado, and Houari A. Sahraoui. 2022. AST-Probe: Recovering abstract syntax
    trees from hidden representations of pre-trained language models. In *ASE*.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: López 等 (2022) José Antonio Hernández López, Martin Weyssow, Jesús Sánchez Cuadrado
    和 Houari A. Sahraoui。2022年。AST-Probe：从预训练语言模型的隐藏表示中恢复抽象语法树。在 *ASE*。
- en: 'Lu et al. (2022) Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang,
    and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval-Augmented Code Completion Framework.
    In *ACL*. 6227–6240.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等 (2022) Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang 和 Alexey
    Svyatkovskiy。2022年。ReACC：一种检索增强的代码补全框架。在 *ACL*。6227–6240。
- en: 'Lu et al. (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    et al. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
    and Generation. In *NeurIPS Datasets and Benchmarks*.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等 (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy
    等。2021年。CodeXGLUE：用于代码理解和生成的机器学习基准数据集。在 *NeurIPS Datasets and Benchmarks*。
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering
    Code Large Language Models with Evol-Instruct. *arXiv preprint arXiv:2306.08568*
    (2023).'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等 (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin 和 Daxin Jiang。2023年。WizardCoder：通过 Evol-Instruct
    赋能代码大型语言模型。*arXiv preprint arXiv:2306.08568* (2023)。
- en: Maddison and Tarlow (2014) Chris Maddison and Daniel Tarlow. 2014. Structured
    generative models of natural source code. In *ICML*. 649–657.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maddison 和 Tarlow (2014) Chris Maddison 和 Daniel Tarlow。2014年。自然源代码的结构生成模型。在
    *ICML*。649–657。
- en: 'Malik et al. (2019) Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019.
    NL2Type: inferring JavaScript function types from natural language information.
    In *ICSE*. 304–315.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malik 等 (2019) Rabee Sohail Malik, Jibesh Patra 和 Michael Pradel。2019年。NL2Type：从自然语言信息中推断
    JavaScript 函数类型。在 *ICSE*。304–315。
- en: Mastropaolo et al. (2021) Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper,
    David Nader Palacio, Denys Poshyvanyk, et al. 2021. Studying the usage of text-to-text
    transfer transformer to support code-related tasks. In *ICSE*. 336–347.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mastropaolo 等 (2021) Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper,
    David Nader Palacio, Denys Poshyvanyk 等。2021年。研究文本到文本的转换变换器在支持与代码相关的任务中的使用。在 *ICSE*。336–347。
- en: Mehrotra et al. (2021) Nikita Mehrotra, Navdha Agarwal, Piyush Gupta, Saket
    Anand, David Lo, and Rahul Purandare. 2021. Modeling Functional Similarity in
    Source Code with Graph-Based Siamese Networks. *TSE* (2021).
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehrotra 等 (2021) Nikita Mehrotra, Navdha Agarwal, Piyush Gupta, Saket Anand,
    David Lo 和 Rahul Purandare。2021年。使用基于图的孪生网络建模源代码中的功能相似性。*TSE* (2021)。
- en: 'Mesbah et al. (2019) Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso,
    and Edward Aftandilian. 2019. DeepDelta: learning to repair compilation errors.
    In *ESEC/FSE*. 925–936.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesbah 等 (2019) Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso 和 Edward
    Aftandilian。2019年。DeepDelta：学习修复编译错误。在 *ESEC/FSE*。925–936。
- en: Mikolov et al. (2013) Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. Efficient Estimation of Word Representations in Vector Space. In *ICLR*.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等 (2013) Tomás Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean。2013年。高效的词向量表示估计。在
    *ICLR*。
- en: 'Mir et al. (2022) Amir M. Mir, Evaldas Latoskinas, Sebastian Proksch, and Georgios
    Gousios. 2022. Type4Py: Practical Deep Similarity Learning-Based Type Inference
    for Python. In *ICSE*. 2241–2252.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mir 等 (2022) Amir M. Mir, Evaldas Latoskinas, Sebastian Proksch 和 Georgios Gousios。2022年。Type4Py：基于深度相似性学习的
    Python 类型推断实用方法。在 *ICSE*。2241–2252。
- en: Moreno et al. (2015) Laura Moreno, Gabriele Bavota, Massimiliano Di Penta, Rocco
    Oliveto, and Andrian Marcus. 2015. How can I use this method?. In *ICSE*, Vol. 1\.
    880–890.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moreno 等 (2015) Laura Moreno, Gabriele Bavota, Massimiliano Di Penta, Rocco
    Oliveto 和 Andrian Marcus。2015年。我该如何使用这种方法？。在 *ICSE*，第1卷。880–890。
- en: Mou et al. (2016) Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional
    neural networks over tree structures for programming language processing. In *AAAI*,
    Vol. 30.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mou 等 (2016) Lili Mou, Ge Li, Lu Zhang, Tao Wang 和 Zhi Jin。2016年。基于树结构的卷积神经网络用于编程语言处理。在
    *AAAI*，第30卷。
- en: Mu et al. (2022) Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang. 2022.
    Automatic Comment Generation via Multi-Pass Deliberation. In *ASE*. ACM, 14:1–14:12.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等 (2022) Fangwen Mu, Xiao Chen, Lin Shi, Song Wang 和 Qing Wang。2022年。通过多轮审议自动生成注释。在
    *ASE*。ACM，14:1–14:12。
- en: 'Nafi et al. (2019) Kawser Wazed Nafi, Tonny Shekha Kar, Banani Roy, Chanchal K
    Roy, and Kevin A Schneider. 2019. Clcdsa: cross language code clone detection
    using syntactical features and api documentation. In *ASE*. 1026–1037.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nafi 等 (2019) Kawser Wazed Nafi, Tonny Shekha Kar, Banani Roy, Chanchal K Roy
    和 Kevin A Schneider。2019年。Clcdsa：利用语法特征和 API 文档进行跨语言代码克隆检测。在 *ASE*。1026–1037。
- en: 'Nair et al. (2020) Aravind Nair, Avijit Roy, and Karl Meinke. 2020. funcGNN:
    A Graph Neural Network Approach to Program Similarity. In *ESEM*. 1–11.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 等（2020）Aravind Nair、Avijit Roy 和 Karl Meinke。2020年。《funcGNN：一种用于程序相似性的图神经网络方法》。发表于
    *ESEM*，1–11。
- en: 'Nan et al. (2020) Zifan Nan, Hui Guan, and Xipeng Shen. 2020. HISyn: human
    learning-inspired natural language programming. In *ESEC/FSE*. 75–86.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nan 等（2020）Zifan Nan、Hui Guan 和 Xipeng Shen。2020年。《HISyn：受人类学习启发的自然语言编程》。发表于
    *ESEC/FSE*，75–86。
- en: 'Nguyen et al. (2021) Phuong T Nguyen, Claudio Di Sipio, Juri Di Rocco, Massimiliano
    Di Penta, and Davide Di Ruscio. 2021. Adversarial Attacks to API Recommender Systems:
    Time to Wake Up and Smell the Coffee?. In *ASE*. 253–265.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等（2021）Phuong T Nguyen、Claudio Di Sipio、Juri Di Rocco、Massimiliano Di
    Penta 和 Davide Di Ruscio。2021年。《API 推荐系统的对抗攻击：是时候醒来并嗅嗅咖啡了？》。发表于 *ASE*，253–265。
- en: Nguyen et al. (2020) Son Nguyen, Hung Phan, Trinh Le, and Tien N Nguyen. 2020.
    Suggesting natural method names to check name consistencies. In *ICSE*. 1372–1384.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等（2020）Son Nguyen、Hung Phan、Trinh Le 和 Tien N Nguyen。2020年。《建议自然方法名称以检查名称一致性》。发表于
    *ICSE*，1372–1384。
- en: Nguyen et al. (2017) Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and
    Tien N Nguyen. 2017. Exploring API embedding for API usages and applications.
    In *ICSE*. 438–449.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等（2017）Trong Duc Nguyen、Anh Tuan Nguyen、Hung Dang Phan 和 Tien N Nguyen。2017年。《探索
    API 嵌入用于 API 使用和应用》。发表于 *ICSE*，438–449。
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等（2022）Erik Nijkamp、Bo Pang、Hiroaki Hayashi、Lifu Tu、Huan Wang、Yingbo
    Zhou、Silvio Savarese 和 Caiming Xiong。2022年。《Codegen：一个开放的大型语言模型用于多轮程序合成》。*arXiv
    预印本 arXiv:2203.13474*（2022）。
- en: 'Niu et al. (2022) Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang,
    and Bin Luo. 2022. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source
    Code Representations. In *ICSE*. 1–13.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 等（2022）Changan Niu、Chuanyi Li、Vincent Ng、Jidong Ge、Liguo Huang 和 Bin Luo。2022年。《SPT-Code：用于学习源代码表示的序列到序列预训练》。发表于
    *ICSE*，1–13。
- en: Nye et al. (2019) Maxwell Nye, Luke Hewitt, Joshua Tenenbaum, and Armando Solar-Lezama.
    2019. Learning to infer program sketches. In *ICML*. 4861–4870.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nye 等（2019）Maxwell Nye、Luke Hewitt、Joshua Tenenbaum 和 Armando Solar-Lezama。2019年。《学习推断程序草图》。发表于
    *ICML*，4861–4870。
- en: OpenAI (2022) OpenAI. 2022. ChatGPT. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2022）OpenAI。2022年。《ChatGPT》。[https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)。
- en: 'Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam
    Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A Fast, Extensible
    Toolkit for Sequence Modeling. In *NAACL-HLT: Demonstrations*.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ott 等（2019）Myle Ott、Sergey Edunov、Alexei Baevski、Angela Fan、Sam Gross、Nathan
    Ng、David Grangier 和 Michael Auli。2019年。《fairseq：一个快速、可扩展的序列建模工具包》。发表于 *NAACL-HLT:
    Demonstrations*。'
- en: 'Pandi et al. (2020) Irene Vlassi Pandi, Earl T Barr, Andrew D Gordon, and Charles
    Sutton. 2020. OptTyper: Probabilistic Type Inference by Optimising Logical and
    Natural Constraints. *arXiv:2004.00348* (2020).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandi 等（2020）Irene Vlassi Pandi、Earl T Barr、Andrew D Gordon 和 Charles Sutton。2020年。《OptTyper：通过优化逻辑和自然约束进行概率类型推断》。*arXiv:2004.00348*（2020）。
- en: Panthaplackel et al. (2021) Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric,
    and Raymond J Mooney. 2021. Deep Just-In-Time Inconsistency Detection Between
    Comments and Source Code. In *AAAI*, Vol. 35\. 427–435.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panthaplackel 等（2021）Sheena Panthaplackel、Junyi Jessy Li、Milos Gligoric 和 Raymond
    J Mooney。2021年。《深度即时不一致检测：评论与源代码之间》。发表于 *AAAI*，第35卷，427–435。
- en: Panthaplackel et al. (2020) Sheena Panthaplackel, Pengyu Nie, Milos Gligoric,
    Junyi Jessy Li, and Raymond Mooney. 2020. Learning to Update Natural Language
    Comments Based on Code Changes. In *ACL*. 1853–1868.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panthaplackel 等（2020）Sheena Panthaplackel、Pengyu Nie、Milos Gligoric、Junyi Jessy
    Li 和 Raymond Mooney。2020年。《基于代码变更学习更新自然语言评论》。发表于 *ACL*，1853–1868。
- en: Peng et al. (2021) Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and
    Tie-Yan Liu. 2021. How could Neural Networks understand Programs?. In *ICML*,
    Vol. 139\. 8476–8486.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2021）Dinglan Peng、Shuxin Zheng、Yatao Li、Guolin Ke、Di He 和 Tie-Yan Liu。2021年。《神经网络如何理解程序？》。发表于
    *ICML*，第139卷，8476–8486。
- en: 'Poesia et al. (2022) Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo
    Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable Code
    Generation from Pre-trained Language Models. In *ICLR*.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poesia 等（2022）Gabriel Poesia、Alex Polozov、Vu Le、Ashish Tiwari、Gustavo Soares、Christopher
    Meek 和 Sumit Gulwani。2022年。《Synchromesh：来自预训练语言模型的可靠代码生成》。发表于 *ICLR*。
- en: 'Pornprasit et al. (2021) Chanathip Pornprasit, Chakkrit Tantithamthavorn, Jirayus
    Jiarpakdee, Michael Fu, and Patanamon Thongtanunam. 2021. PyExplainer: Explaining
    the Predictions of Just-In-Time Defect Models. In *ASE*. 407–418.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pornprasit et al. (2021) Chanathip Pornprasit, Chakkrit Tantithamthavorn, Jirayus
    Jiarpakdee, Michael Fu, 和 Patanamon Thongtanunam. 2021. PyExplainer: 解释即时缺陷模型的预测。发表于
    *ASE*。407–418。'
- en: 'Pradel et al. (2020) Michael Pradel, Georgios Gousios, Jason Liu, and Satish
    Chandra. 2020. Typewriter: Neural type prediction with search-based validation.
    In *ESEC/FSE*. 209–220.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pradel et al. (2020) Michael Pradel, Georgios Gousios, Jason Liu, 和 Satish
    Chandra. 2020. Typewriter: 基于搜索的验证神经类型预测。发表于 *ESEC/FSE*。209–220。'
- en: 'Pradel and Sen (2018) Michael Pradel and Koushik Sen. 2018. Deepbugs: A learning
    approach to name-based bug detection. *OOPSLA* 2 (2018), 1–25.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pradel and Sen (2018) Michael Pradel 和 Koushik Sen. 2018. Deepbugs: 一种基于名称的错误检测学习方法。*OOPSLA*
    2 (2018)，1–25。'
- en: Quiring et al. (2019) Erwin Quiring, Alwin Maier, and Konrad Rieck. 2019. Misleading
    authorship attribution of source code using adversarial learning. In *USENIX Security
    19*. 479–496.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quiring et al. (2019) Erwin Quiring, Alwin Maier, 和 Konrad Rieck. 2019. 使用对抗学习的误导性代码作者归属。发表于
    *USENIX Security 19*。479–496。
- en: Rabin et al. (2021) Md. Rafiqul Islam Rabin, Vincent J. Hellendoorn, and Mohammad Amin
    Alipour. 2021. Understanding neural code intelligence through program simplification.
    In *ESEC/FSE*. ACM, 441–452.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabin et al. (2021) Md. Rafiqul Islam Rabin, Vincent J. Hellendoorn, 和 Mohammad
    Amin Alipour. 2021. 通过程序简化理解神经代码智能。发表于 *ESEC/FSE*。ACM，441–452。
- en: Rabinovich et al. (2017) Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017.
    Abstract Syntax Networks for Code Generation and Semantic Parsing. In *ACL*. 1139–1149.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabinovich et al. (2017) Maxim Rabinovich, Mitchell Stern, 和 Dan Klein. 2017.
    用于代码生成和语义解析的抽象语法网络。发表于 *ACL*。1139–1149。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, et al. 2020. Exploring the Limits of Transfer Learning
    with a Unified Text-to-Text Transformer. *JMLR* 21 (2020), 1–67.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, 等等. 2020. 通过统一的文本到文本转换器探索迁移学习的极限。*JMLR* 21 (2020)，1–67。
- en: Ramakrishnan and Albarghouthi (2022) Goutham Ramakrishnan and Aws Albarghouthi.
    2022. Backdoors in Neural Models of Source Code. In *ICPR*. IEEE, 2892–2899.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramakrishnan and Albarghouthi (2022) Goutham Ramakrishnan 和 Aws Albarghouthi.
    2022. 神经源代码模型中的后门。发表于 *ICPR*。IEEE，2892–2899。
- en: Ramakrishnan et al. (2020) Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws
    Albarghouthi, Somesh Jha, and Thomas Reps. 2020. Semantic robustness of models
    of source code. *arXiv:2002.03043* (2020).
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramakrishnan et al. (2020) Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws
    Albarghouthi, Somesh Jha, 和 Thomas Reps. 2020. 源代码模型的语义鲁棒性。*arXiv:2002.03043*
    (2020)。
- en: Raychev et al. (2016) Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016.
    Probabilistic model for code with decision trees. *ACM SIGPLAN Notices* 51, 10
    (2016), 731–747.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raychev et al. (2016) Veselin Raychev, Pavol Bielik, 和 Martin Vechev. 2016.
    使用决策树的代码概率模型。*ACM SIGPLAN Notices* 51, 10 (2016)，731–747。
- en: Raychev et al. (2014) Veselin Raychev, Martin Vechev, and Eran Yahav. 2014.
    Code completion with statistical language models. In *ICPC*. 419–428.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raychev et al. (2014) Veselin Raychev, Martin Vechev, 和 Eran Yahav. 2014. 使用统计语言模型的代码补全。发表于
    *ICPC*。419–428。
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950* (2023).'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, 等等. 2023. Code llama: 开放基础模型用于代码。*arXiv preprint arXiv:2308.12950* (2023)。'
- en: Rozière et al. (2020) Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot,
    and Guillaume Lample. 2020. Unsupervised Translation of Programming Languages.
    In *NeurIPS*.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozière et al. (2020) Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot,
    和 Guillaume Lample. 2020. 无监督编程语言翻译。发表于 *NeurIPS*。
- en: Rozière et al. (2022) Baptiste Rozière, Jie Zhang, François Charton, Mark Harman,
    Gabriel Synnaeve, and Guillaume Lample. 2022. Leveraging Automated Unit Tests
    for Unsupervised Code Translation. In *ICLR*.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozière et al. (2022) Baptiste Rozière, Jie Zhang, François Charton, Mark Harman,
    Gabriel Synnaeve, 和 Guillaume Lample. 2022. 利用自动化单元测试进行无监督代码翻译。发表于 *ICLR*。
- en: 'Salib (2004) Michael Salib. 2004. Faster than C: Static type inference with
    Starkiller. *PyCon Proceedings, Washington DC* 3 (2004).'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salib (2004) Michael Salib. 2004. 比C更快：使用Starkiller的静态类型推断。*PyCon Proceedings,
    Washington DC* 3 (2004)。
- en: 'Santos et al. (2018) Eddie Antonio Santos, Joshua Charles Campbell, Dhvani
    Patel, Abram Hindle, and José Nelson Amaral. 2018. Syntax and sensibility: Using
    language models to detect and correct syntax errors. In *SANER*. 311–322.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santos 等人（2018）Eddie Antonio Santos、Joshua Charles Campbell、Dhvani Patel、Abram
    Hindle 和 José Nelson Amaral。2018。语法与敏感性：利用语言模型检测和修正语法错误。发表于 *SANER*。311–322 页。
- en: 'Schuster et al. (2021) Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly
    Shmatikov. 2021. You autocomplete me: Poisoning vulnerabilities in neural code
    completion. In *USENIX Security*.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuster 等人（2021）Roei Schuster、Congzheng Song、Eran Tromer 和 Vitaly Shmatikov。2021。你自动完成我：神经代码补全中的投毒漏洞。发表于
    *USENIX Security*。
- en: Severi et al. (2021) Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea.
    2021. Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers.
    In *USENIX Security*.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Severi 等人（2021）Giorgio Severi、Jim Meyer、Scott Coull 和 Alina Oprea。2021。基于解释引导的后门投毒攻击对抗恶意软件分类器。发表于
    *USENIX Security*。
- en: 'Shahbazi et al. (2021) Ramin Shahbazi, Rishab Sharma, and Fatemeh H. Fard.
    2021. API2Com: On the Improvement of Automatically Generated Code Comments Using
    API Documentations. In *ICPC*. IEEE, 411–421.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shahbazi 等人（2021）Ramin Shahbazi、Rishab Sharma 和 Fatemeh H. Fard。2021。API2Com：基于
    API 文档改善自动生成代码注释。发表于 *ICPC*。IEEE，第 411–421 页。
- en: Sharma et al. (2022) Rishab Sharma, Fuxiang Chen, Fatemeh H. Fard, and David
    Lo. 2022. An exploratory study on code attention in BERT. In *ICPC*. ACM, 437–448.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人（2022）Rishab Sharma、Fuxiang Chen、Fatemeh H. Fard 和 David Lo。2022。关于
    BERT 中代码关注的探索性研究。发表于 *ICPC*。ACM，第 437–448 页。
- en: 'Shi et al. (2021) Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han,
    et al. 2021. CAST: Enhancing Code Summarization with Hierarchical Splitting and
    Reconstruction of Abstract Syntax Trees. In *EMNLP*. 4053–4062.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2021）Ensheng Shi、Yanlin Wang、Lun Du、Hongyu Zhang、Shi Han 等人。2021。CAST：通过抽象语法树的层次分割和重建来增强代码摘要。发表于
    *EMNLP*。4053–4062 页。
- en: Shi et al. (2022b) Jieke Shi, Zhou Yang, Bowen Xu, Hong Jin Kang, and David
    Lo. 2022b. Compressing Pre-trained Models of Code into 3 MB. In *ASE*. ACM, 24:1–24:12.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2022b）Jieke Shi、Zhou Yang、Bowen Xu、Hong Jin Kang 和 David Lo。2022b。将预训练模型压缩至
    3 MB。发表于 *ASE*。ACM，第 24 卷，第 1–12 页。
- en: Shi et al. (2022a) Lin Shi, Fangwen Mu, Xiao Chen, Song Wang, Junjie Wang, Ye
    Yang, Ge Li, Xin Xia, and Qing Wang. 2022a. Are we building on the rock? on the
    importance of data preprocessing for code summarization. In *ESEC/FSE*. ACM, 107–119.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2022a）Lin Shi、Fangwen Mu、Xiao Chen、Song Wang、Junjie Wang、Ye Yang、Ge Li、Xin
    Xia 和 Qing Wang。2022a。我们是在基石上构建吗？数据预处理在代码摘要中的重要性。发表于 *ESEC/FSE*。ACM，第 107–119
    页。
- en: Shi et al. (2022c) Yucen Shi, Ying Yin, Zhengkui Wang, David Lo, Tao Zhang,
    Xin Xia, Yuhai Zhao, and Bowen Xu. 2022c. How to better utilize code graphs in
    semantic code search?. In *ESEC/FSE*. 722–733.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2022c）Yucen Shi、Ying Yin、Zhengkui Wang、David Lo、Tao Zhang、Xin Xia、Yuhai
    Zhao 和 Bowen Xu。2022c。如何更好地利用代码图进行语义代码搜索？发表于 *ESEC/FSE*。722–733 页。
- en: Shrivastava et al. (2020) Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow.
    2020. On-the-Fly Adaptation of Source Code Models using Meta-Learning. *arXiv:2003.11768*
    (2020).
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shrivastava 等人（2020）Disha Shrivastava、Hugo Larochelle 和 Daniel Tarlow。2020。利用元学习对源代码模型进行即时适应。*arXiv:2003.11768*（2020）。
- en: Shu and Zhang (2017) Chengxun Shu and Hongyu Zhang. 2017. Neural Programming
    by Example. In *AAAI*. 1539–1545.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 和 Zhang（2017）Chengxun Shu 和 Hongyu Zhang。2017。通过示例进行神经编程。发表于 *AAAI*。1539–1545
    页。
- en: 'Sui et al. (2020) Yulei Sui, Xiao Cheng, Guanqin Zhang, and Haoyu Wang. 2020.
    Flow2Vec: value-flow-based precise code embedding. *OOPSLA* 4 (2020), 1–27.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sui 等人（2020）Yulei Sui、Xiao Cheng、Guanqin Zhang 和 Haoyu Wang。2020。Flow2Vec：基于值流的精确代码嵌入。*OOPSLA*
    第 4 卷（2020），第 1–27 页。
- en: 'Sui and Xue (2016) Yulei Sui and Jingling Xue. 2016. SVF: interprocedural static
    value-flow analysis in LLVM. In *Proceedings of the 25th international conference
    on compiler construction*. 265–266.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sui 和 Xue（2016）Yulei Sui 和 Jingling Xue。2016。SVF：LLVM 中的跨过程静态值流分析。发表于 *第 25
    届国际编译器构造会议论文集*。265–266 页。
- en: Sun et al. (2022a) Weisong Sun, Chunrong Fang, Yuchen Chen, Guanhong Tao, Tingxu
    Han, and Quanjun Zhang. 2022a. Code Search based on Context-aware Code Translation.
    In *ICSE*. 388–400.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2022a）Weisong Sun、Chunrong Fang、Yuchen Chen、Guanhong Tao、Tingxu Han 和
    Quanjun Zhang。2022a。基于上下文感知代码翻译的代码搜索。发表于 *ICSE*。388–400 页。
- en: 'Sun et al. (2022b) Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi
    Wu. 2022b. Heterogeneous Information Networks: the Past, the Present, and the
    Future. *Proc. VLDB Endow.* 15, 12 (2022), 3807–3811.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2022b）Yizhou Sun、Jiawei Han、Xifeng Yan、Philip S. Yu 和 Tianyi Wu。2022b。异质信息网络：过去、现在与未来。发表于
    *Proc. VLDB Endow.* 15，第 12 期（2022），第 3807–3811 页。
- en: Sun et al. (2022c) Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022c.
    On the Importance of Building High-quality Training Datasets for Neural Code Search.
    In *ICSE*. ACM, 1609–1620.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2022c) Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, 和 Li Li. 2022c.
    关于构建高质量训练数据集对神经代码搜索重要性的探讨。发表于 *ICSE*，ACM，1609–1620。
- en: Sun et al. (2019) Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu
    Zhang. 2019. A grammar-based structural cnn decoder for code generation. In *AAAI*,
    Vol. 33\. 7055–7062.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, 和 Lu
    Zhang. 2019. 一种基于语法的结构化 CNN 解码器用于代码生成。发表于 *AAAI*，第 33 卷，7055–7062。
- en: 'Sun et al. (2020) Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou,
    and Lu Zhang. 2020. TreeGen: A Tree-Based Transformer Architecture for Code Generation.
    In *AAAI*. 8984–8991.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2020) Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou,
    和 Lu Zhang. 2020. TreeGen: 一种用于代码生成的基于树的 Transformer 架构。发表于 *AAAI*，8984–8991。'
- en: 'Svyatkovskiy et al. (2020) Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
    and Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer.
    In *ESEC/FSE*. 1433–1443.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Svyatkovskiy et al. (2020) Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
    和 Neel Sundaresan. 2020. Intellicode compose: 使用 Transformer 进行代码生成。发表于 *ESEC/FSE*，1433–1443。'
- en: Svyatkovskiy et al. (2021) Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi,
    Maik Riechert, Juliana Vicente Franco, and Miltiadis Allamanis. 2021. Fast and
    memory-efficient neural code completion. In *MSR*. 329–340.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Svyatkovskiy et al. (2021) Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi,
    Maik Riechert, Juliana Vicente Franco, 和 Miltiadis Allamanis. 2021. 快速且内存高效的神经代码补全。发表于
    *MSR*，329–340。
- en: 'Svyatkovskiy et al. (2019) Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and
    Neel Sundaresan. 2019. Pythia: Ai-assisted code completion system. In *SIGKDD*.
    2727–2735.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Svyatkovskiy et al. (2019) Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, 和 Neel
    Sundaresan. 2019. Pythia: AI 辅助代码补全系统。发表于 *SIGKDD*，2727–2735。'
- en: 'Tang et al. (2022) Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang,
    Zheling Zhu, and Bin Luo. 2022. AST-Trans: Code Summarization with Efficient Tree-Structured
    Attention. In *ICSE*.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang et al. (2022) Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang,
    Zheling Zhu, 和 Bin Luo. 2022. AST-Trans: 使用高效树结构注意力进行代码总结。发表于 *ICSE*。'
- en: 'Tao et al. (2022) Chenning Tao, Qi Zhan, Xing Hu, and Xin Xia. 2022. C4: contrastive
    cross-language code clone detection. In *ICPC*. ACM, 413–424.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tao et al. (2022) Chenning Tao, Qi Zhan, Xing Hu, 和 Xin Xia. 2022. C4: 对比跨语言代码克隆检测。发表于
    *ICPC*，ACM，413–424。'
- en: Tarlow et al. (2020) Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen,
    Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian. 2020. Learning
    to fix build errors with graph2diff neural networks. In *ICSE Workshops*. 19–20.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarlow et al. (2020) Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen,
    Pierre-Antoine Manzagol, Charles Sutton, 和 Edward Aftandilian. 2020. 使用 graph2diff
    神经网络学习修复构建错误。发表于 *ICSE Workshops*，19–20。
- en: Tian et al. (2020) Haoye Tian, Kui Liu, Abdoul Kader Kaboré, Anil Koyuncu, Li
    Li, et al. 2020. Evaluating representation learning of code changes for predicting
    patch correctness in program repair. In *ASE*. 981–992.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2020) Haoye Tian, Kui Liu, Abdoul Kader Kaboré, Anil Koyuncu, Li
    Li, 等. 2020. 评估代码变更的表示学习以预测程序修复中的补丁正确性。发表于 *ASE*，981–992。
- en: Tufano et al. (2019) Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele
    Bavota, and Denys Poshyvanyk. 2019. On learning meaningful code changes via neural
    machine translation. In *ICSE*. 25–36.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tufano et al. (2019) Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele
    Bavota, 和 Denys Poshyvanyk. 2019. 通过神经机器翻译学习有意义的代码变更。发表于 *ICSE*，25–36。
- en: Tufano et al. (2018a) Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
    Di Penta, et al. 2018a. An empirical investigation into learning bug-fixing patches
    in the wild via neural machine translation. In *ASE*. 832–837.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tufano et al. (2018a) Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
    Di Penta, 等. 2018a. 通过神经机器翻译对野外的错误修复补丁进行实证研究。发表于 *ASE*，832–837。
- en: Tufano et al. (2018b) Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
    Di Penta, Martin White, and Denys Poshyvanyk. 2018b. Deep learning similarities
    from different representations of source code. In *MSR*. 542–553.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tufano et al. (2018b) Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
    Di Penta, Martin White, 和 Denys Poshyvanyk. 2018b. 从源代码的不同表示中学习深度相似性。发表于 *MSR*，542–553。
- en: Vasic et al. (2018) Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber,
    and Rishabh Singh. 2018. Neural Program Repair by Jointly Learning to Localize
    and Repair. In *ICLR*.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vasic et al. (2018) Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber,
    和 Rishabh Singh. 2018. 通过联合学习定位和修复的神经程序修复。发表于 *ICLR*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *NeurIPS*. 5998–6008.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力机制是你所需的一切。发表于
    *NeurIPS*，5998–6008。
- en: 'VenkataKeerthy et al. (2020) S VenkataKeerthy, Rohit Aggarwal, Shalini Jain,
    Maunendra Sankar Desarkar, Ramakrishna Upadrasta, and YN Srikant. 2020. Ir2vec:
    Llvm ir based scalable program embeddings. *TACO* 17, 4 (2020), 1–27.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'VenkataKeerthy 等人 (2020) S VenkataKeerthy、Rohit Aggarwal、Shalini Jain、Maunendra
    Sankar Desarkar、Ramakrishna Upadrasta 和 YN Srikant. 2020. Ir2vec: 基于 LLVM IR 的可扩展程序嵌入.
    *TACO* 17, 4 (2020)，1–27。'
- en: 'Wan et al. (2022a) Yao Wan, Yang He, Zhangqian Bi, Jianguo Zhang, Yulei Sui,
    Hongyu Zhang, et al. 2022a. NaturalCC: An Open-Source Toolkit for Code Intelligence.
    In *ICSE, Companion Volume*.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wan 等人 (2022a) Yao Wan、Yang He、Zhangqian Bi、Jianguo Zhang、Yulei Sui、Hongyu
    Zhang 等. 2022a. NaturalCC: 一个开源代码智能工具包. 见 *ICSE, Companion Volume*。'
- en: Wan et al. (2019) Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao,
    Jian Wu, and Philip S. Yu. 2019. Multi-modal Attention Network Learning for Semantic
    Source Code Retrieval. In *ASE*. 13–25.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 (2019) Yao Wan、Jingdong Shu、Yulei Sui、Guandong Xu、Zhou Zhao、Jian Wu 和
    Philip S. Yu. 2019. 用于语义源代码检索的多模态注意力网络学习. 见 *ASE*，13–25。
- en: 'Wan et al. (2022b) Yao Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, Guandong
    Xu, Dezhong Yao, Hai Jin, and Lichao Sun. 2022b. You see what I want you to see:
    poisoning vulnerabilities in neural code search. In *ESEC/FSE*. 1233–1245.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 (2022b) Yao Wan、Shijie Zhang、Hongyu Zhang、Yulei Sui、Guandong Xu、Dezhong
    Yao、Hai Jin 和 Lichao Sun. 2022b. 你看到的是我想让你看到的：神经代码搜索中的毒化漏洞. 见 *ESEC/FSE*，1233–1245。
- en: Wan et al. (2022c) Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu,
    and Hai Jin. 2022c. What Do They Capture? - A Structural Analysis of Pre-Trained
    Language Models for Source Code. In *ICSE*. 2377–2388.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 (2022c) Yao Wan、Wei Zhao、Hongyu Zhang、Yulei Sui、Guandong Xu 和 Hai Jin.
    2022c. 他们捕获了什么？——对源代码的预训练语言模型的结构分析. 见 *ICSE*，2377–2388。
- en: Wan et al. (2018) Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian
    Wu, and Philip S Yu. 2018. Improving automatic source code summarization via deep
    reinforcement learning. In *ASE*. 397–407.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 (2018) Yao Wan、Zhou Zhao、Min Yang、Guandong Xu、Haochao Ying、Jian Wu 和
    Philip S Yu. 2018. 通过深度强化学习改进自动源代码摘要. 见 *ASE*，397–407。
- en: Wang et al. (2022c) Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu
    Zhang, and Michael R. Lyu. 2022c. No more fine-tuning? an experimental evaluation
    of prompt tuning in code intelligence. In *ESEC/FSE*. 382–394.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2022c) Chaozheng Wang、Yuanhang Yang、Cuiyun Gao、Yun Peng、Hongyu Zhang
    和 Michael R. Lyu. 2022c. 不再微调？代码智能中提示微调的实验评估. 见 *ESEC/FSE*，382–394。
- en: Wang et al. (2022b) Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong,
    Wei Dong, and Xiangke Liao. 2022b. Bridging Pre-trained Models and Downstream
    Tasks for Source Code Understanding. In *ICSE*. 287–298.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2022b) Deze Wang、Zhouyang Jia、Shanshan Li、Yue Yu、Yun Xiong、Wei Dong
    和 Xiangke Liao. 2022b. Bridging Pre-trained Models and Downstream Tasks for Source
    Code Understanding. 见 *ICSE*，287–298。
- en: Wang et al. (2020c) Huanting Wang, Guixin Ye, Zhanyong Tang, Shin Hwei Tan,
    et al. 2020c. Combining graph-based learning with automated data collection for
    code vulnerability detection. *TIFS* 16 (2020), 1943–1958.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2020c) Huanting Wang、Guixin Ye、Zhanyong Tang、Shin Hwei Tan 等. 2020c.
    结合图基学习与自动化数据收集用于代码漏洞检测. *TIFS* 16 (2020)，1943–1958。
- en: 'Wang et al. (2020a) Simin Wang, Liguo Huang, Jidong Ge, Tengfei Zhang, Haitao
    Feng, Ming Li, He Zhang, and Vincent Ng. 2020a. Synergy between Machine/Deep Learning
    and Software Engineering: How Far Are We? *arXiv:2008.05515* (2020).'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2020a) Simin Wang、Liguo Huang、Jidong Ge、Tengfei Zhang、Haitao Feng、Ming
    Li、He Zhang 和 Vincent Ng. 2020a. 机器/深度学习与软件工程之间的协同效应：我们还差多远？ *arXiv:2008.05515*
    (2020)。
- en: Wang et al. (2016) Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning
    semantic features for defect prediction. In *ICSE*. 297–308.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2016) Song Wang、Taiyue Liu 和 Lin Tan. 2016. 自动学习语义特征以进行缺陷预测. 见 *ICSE*，297–308。
- en: Wang et al. (2020b) Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020b.
    Detecting code clones with graph neural network and flow-augmented abstract syntax
    tree. In *SANER*. 261–271.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2020b) Wenhan Wang、Ge Li、Bo Ma、Xin Xia 和 Zhi Jin. 2020b. 使用图神经网络和流增强抽象语法树检测代码克隆.
    见 *SANER*，261–271。
- en: 'Wang et al. (2021b) Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao
    Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang. 2021b. SynCoBERT: Syntax-Guided Multi-Modal
    Contrastive Pre-Training for Code Representation. *arXiv:2108.04556* (2021).'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2021b) Xin Wang、Yasheng Wang、Fei Mi、Pingyi Zhou、Yao Wan、Xiao Liu、Li
    Li、Hao Wu、Jin Liu 和 Xin Jiang. 2021b. SynCoBERT: 基于语法的多模态对比预训练代码表示. *arXiv:2108.04556*
    (2021)。'
- en: 'Wang et al. (2022a) Yu Wang, Yu Dong, Xuesong Lu, and Aoying Zhou. 2022a. GypSum:
    learning hybrid representations for code summarization. In *ICPC*. ACM, 12–23.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2022a) Yu Wang、Yu Dong、Xuesong Lu 和 Aoying Zhou. 2022a. GypSum: 学习混合表示以进行代码摘要.
    见 *ICPC*，ACM，12–23。'
- en: 'Wang et al. (2023) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui,
    Junnan Li, and Steven CH Hoi. 2023. Codet5+: Open code large language models for
    code understanding and generation. *arXiv preprint arXiv:2305.07922* (2023).'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023）王悦、洪乐、阿基莱什·迪帕克·戈特梅尔、阮DQ·布伊、李俊楠和史蒂文·CH·霍伊。2023。Codet5+：用于代码理解和生成的开放代码大型语言模型。*arXiv预印本
    arXiv:2305.07922*（2023）。
- en: Wang and Li (2021) Yanlin Wang and Hui Li. 2021. Code completion by modeling
    flattened abstract syntax trees as graphs. In *AAAI*, Vol. 35\. 14015–14023.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王和李（2021）王延林和李辉。2021。通过将扁平化抽象语法树建模为图形进行代码补全。在*AAAI*，第35卷。14015–14023。
- en: 'Wang et al. (2021a) Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H.
    Hoi. 2021a. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models
    for Code Understanding and Generation. In *EMNLP*. 8696–8708.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2021a）王悦、王伟诗、沙菲克·R·乔蒂和史蒂文·CH·霍伊。2021a。CodeT5：识别符感知的统一预训练编码器-解码器模型用于代码理解和生成。在*EMNLP*。8696–8708。
- en: Watson et al. (2020) Cody Watson, Ncthan Cooper, David Nader Palacio, Kevin
    Moran, and Denys Poshyvanyk. 2020. A Systematic Literature Review on the Use of
    Deep Learning in Software Engineering Research. *arXiv:2009.06520* (2020).
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沃森等人（2020）科迪·沃森、奈克桑·库珀、大卫·纳德尔·帕拉西奥、凯文·莫兰和德尼斯·波希瓦尼克。2020。关于深度学习在软件工程研究中应用的系统文献综述。*arXiv:2009.06520*（2020）。
- en: Wei et al. (2019) Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code
    Generation as a Dual Task of Code Summarization. In *NeurIPS*. 6559–6569.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人（2019）魏博林、李戈、夏鑫、傅智怡和金智。2019。将代码生成作为代码总结的双重任务。在*NeurIPS*。6559–6569。
- en: 'Wei et al. (2020b) Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020b.
    Retrieve and refine: exemplar-based neural comment generation. In *ASE*. 349–360.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人（2020b）魏博林、李永敏、李戈、夏鑫和金智。2020b。检索和细化：基于示例的神经评论生成。在*ASE*。349–360。
- en: Wei and Li (2017) Huihui Wei and Ming Li. 2017. Supervised Deep Features for
    Software Functional Clone Detection by Exploiting Lexical and Syntactical Information
    in Source Code.. In *IJCAI*. 3034–3040.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏和李（2017）魏慧慧和李明。2017。通过利用源代码中的词汇和句法信息进行软件功能克隆检测的监督深度特征。在*IJCAI*。3034–3040。
- en: 'Wei et al. (2020a) Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig.
    2020a. LambdaNet: Probabilistic Type Inference using Graph Neural Networks. In
    *ICLR*.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人（2020a）魏佳怡、马鲁斯·戈亚尔、格雷格·杜雷特和伊希尔·迪利格。2020a。LambdaNet：使用图神经网络的概率类型推断。在*ICLR*。
- en: 'Wei et al. (2022) Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang,
    and Song Wang. 2022. CLEAR: contrastive learning for API recommendation. In *ICSE*.
    376–387.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人（2022）魏莫石、尼玛·希里·哈泽维利、黄玉超、王俊杰和王松。2022。CLEAR：用于API推荐的对比学习。在*ICSE*。376–387。
- en: White et al. (2019) Martin White, Michele Tufano, Matias Martinez, Martin Monperrus,
    and Denys Poshyvanyk. 2019. Sorting and transforming program repair ingredients
    via deep learning code similarities. In *SANER*. 479–490.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 怀特等人（2019）马丁·怀特、米歇尔·图法诺、马蒂亚斯·马丁内斯、马丁·蒙佩鲁斯和德尼斯·波希瓦尼克。2019。通过深度学习代码相似性对程序修复成分进行排序和转换。在*SANER*。479–490。
- en: White et al. (2016) Martin White, Michele Tufano, Christopher Vendome, and Denys
    Poshyvanyk. 2016. Deep learning code fragments for code clone detection. In *ASE*.
    87–98.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 怀特等人（2016）马丁·怀特、米歇尔·图法诺、克里斯托弗·文多梅和德尼斯·波希瓦尼克。2016。用于代码克隆检测的深度学习代码片段。在*ASE*。87–98。
- en: White et al. (2015) Martin White, Christopher Vendome, Mario Linares-Vásquez,
    and Denys Poshyvanyk. 2015. Toward deep learning software repositories. In *MSR*.
    334–345.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 怀特等人（2015）马丁·怀特、克里斯托弗·文多梅、马里奥·利纳雷斯-瓦斯克斯和德尼斯·波希瓦尼克。2015。迈向深度学习软件库。在*MSR*。334–345。
- en: Wu et al. (2021) Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization
    with Structure-induced Transformer. In *Findings of ACL*. 1078–1090.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人（2021）吴红秋、赵海和张敏。2021。基于结构引导的变换器进行代码总结。在*Findings of ACL*。1078–1090。
- en: Wu et al. (2022a) Yueming Wu, Siyue Feng, Deqing Zou, and Hai Jin. 2022a. Detecting
    Semantic Code Clones by Building AST-based Markov Chains Model. In *ASE*. ACM,
    34:1–34:13.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人（2022a）吴月明、冯思悦、邹德庆和金海。2022a。通过构建基于AST的马尔可夫链模型检测语义代码克隆。在*ASE*。ACM，34:1–34:13。
- en: Wu et al. (2019) Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo,
    and Ross Girshick. 2019. Detectron2. [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2).
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人（2019）吴宇鑫、亚历山大·基里洛夫、弗朗西斯科·马萨、罗万-燕·罗和罗斯·吉尔希克。2019。Detectron2。 [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2)。
- en: 'Wu et al. (2020) Yueming Wu, Deqing Zou, Shihan Dou, Siru Yang, Wei Yang, Feng
    Cheng, Hong Liang, and Hai Jin. 2020. SCDetector: Software Functional Clone Detection
    Based on Semantic Tokens Analysis. In *ASE*. 821–833.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人（2020）吴月明、邹德庆、窦世涵、杨思如、杨伟、程峰、梁洪和金海。2020。SCDetector：基于语义标记分析的软件功能克隆检测。在*ASE*。821–833。
- en: 'Wu et al. (2022b) Yueming Wu, Deqing Zou, Shihan Dou, Wei Yang, Duo Xu, and
    Hai Jin. 2022b. VulCNN: An Image-inspired Scalable Vulnerability Detection System.
    In *ICSE*. 2365–2376.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2022b）**岳铭·吴**、**德青·邹**、**士翰·杜**、**魏·杨**、**多·徐** 和 **海·金**。2022b。《VulCNN：一种受图像启发的可扩展漏洞检测系统》。在
    *ICSE* 会议中，2365–2376。
- en: Xia et al. (2023) Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023.
    Automated program repair in the era of large pre-trained language models. In *Proceedings
    of the 45th International Conference on Software Engineering (ICSE 2023). Association
    for Computing Machinery*.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等（2023）**春秋·史蒂文·夏**、**于翔·魏** 和 **玲铭·张**。2023。《大规模预训练语言模型时代的自动程序修复》。在 *第45届国际软件工程会议（ICSE
    2023）* 中，计算机协会。
- en: Xie et al. (2022) Rui Xie, Tianxiang Hu, Wei Ye, and Shikun Zhang. 2022. Low-Resources
    Project-Specific Code Summarization. In *ASE*. ACM, 68:1–68:12.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2022）**瑞·谢**、**田祥·胡**、**魏·叶** 和 **士坤·张**。2022。《低资源项目特定的代码总结》。在 *ASE* 会议中，ACM，68:1–68:12。
- en: 'Xie et al. (2021) Rui Xie, Wei Ye, Jinan Sun, and Shikun Zhang. 2021. Exploiting
    Method Names to Improve Code Summarization: A Deliberation Multi-Task Learning
    Approach. In *ICPC*. IEEE, 138–148.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2021）**瑞·谢**、**魏·叶**、**济南·孙** 和 **士坤·张**。2021。《利用方法名改进代码总结：一种深思多任务学习方法》。在
    *ICPC* 会议中，IEEE，138–148。
- en: Xu et al. (2020) Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu,
    and Graham Neubig. 2020. Incorporating External Knowledge through Pre-training
    for Natural Language to Code Generation. In *ACL*. 6045–6052.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2020）**弗兰克·F·徐**、**郑宝·姜**、**彭程·尹**、**博格丹·瓦西列斯库** 和 **格雷厄姆·纽比格**。2020。《通过预训练将外部知识融入自然语言到代码生成》。在
    *ACL* 会议中，6045–6052。
- en: Yamaguchi et al. (2014) Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad
    Rieck. 2014. Modeling and discovering vulnerabilities with code property graphs.
    In *S&P*. 590–604.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamaguchi 等（2014）**法比安·山口**、**尼科·戈尔德**、**丹尼尔·阿普** 和 **孔拉德·里克**。2014。《使用代码属性图建模和发现漏洞》。在
    *S&P* 会议中，590–604。
- en: 'Yang et al. (2022a) Guang Yang, Xiang Chen, Yanlin Zhou, and Chi Yu. 2022a.
    DualSC: Automatic Generation and Summarization of Shellcode via Transformer and
    Dual Learning. In *SANER*. 361–372.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2022a）**光阳**、**向辰**、**燕霖·周** 和 **迟玉**。2022a。《DualSC：通过 Transformer 和双重学习自动生成和总结
    Shellcode》。在 *SANER* 会议中，361–372。
- en: Yang et al. (2022c) Yanming Yang, Xin Xia, David Lo, and John Grundy. 2022c.
    A Survey on Deep Learning for Software Engineering. *ACM Comput. Surv.* 54, 10s,
    Article 206 (sep 2022), 73 pages.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2022c）**燕明·杨**、**辛·夏**、**大卫·洛** 和 **约翰·格伦迪**。2022c。《深度学习在软件工程中的应用调查》。*ACM
    Comput. Surv.* 54, 10s, 文章 206（2022年9月），73 页。
- en: Yang et al. (2021) Zhen Yang, Jacky Keung, Xiao Yu, Xiaodong Gu, Zhengyuan Wei,
    Xiaoxue Ma, and Miao Zhang. 2021. A Multi-Modal Transformer-based Code Summarization
    Approach for Smart Contracts. In *ICPC*. IEEE, 1–12.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2021）**振阳**、**杰克·克昂**、**肖宇**、**晓东·顾**、**正元·魏**、**晓雪·马** 和 **苗·张**。2021。《基于多模态
    Transformer 的智能合约代码总结方法》。在 *ICPC* 会议中，IEEE，1–12。
- en: Yang et al. (2022b) Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022b. Natural
    Attack for Pre-trained Models of Code. In *ICSE*. ACM, 1482–1493.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2022b）**周阳**、**杰克·施**、**军达·何** 和 **大卫·洛**。2022b。《对预训练代码模型的自然攻击》。在 *ICSE*
    会议中，ACM，1482–1493。
- en: 'Yao et al. (2019) Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019.
    Coacor: Code annotation for code retrieval with reinforcement learning. In *The
    World Wide Web Conference*. 2203–2214.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2019）**子瑜·姚**、**贾亚瓦尔丹·雷迪·佩达梅尔** 和 **欢·孙**。2019。《Coacor：一种用于代码检索的代码注释方法，基于强化学习》。在
    *全球互联网会议* 中，2203–2214。
- en: Yasunaga and Liang (2020) Michihiro Yasunaga and Percy Liang. 2020. Graph-based,
    self-supervised program repair from diagnostic feedback. In *ICML*. 10799–10808.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yasunaga 和 Liang（2020）**弥次郎·安永** 和 **珀西·梁**。2020。《基于图的自监督程序修复来自诊断反馈》。在 *ICML*
    会议中，10799–10808。
- en: Ye et al. (2020) Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang,
    and Shikun Zhang. 2020. Leveraging code generation to improve code retrieval and
    summarization via dual learning. In *Proceedings of The Web Conference 2020*.
    2309–2319.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等（2020）**魏·叶**、**瑞·谢**、**京雷·张**、**田祥·胡**、**小尹·王** 和 **士坤·张**。2020。《利用代码生成通过双重学习改进代码检索和总结》。在
    *Proceedings of The Web Conference 2020* 会议中，2309–2319。
- en: Yefet et al. (2020) Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial
    examples for models of code. *OOPSLA* 4 (2020), 1–30.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yefet 等（2020）**诺姆·耶费特**、**乌里·阿隆** 和 **埃兰·亚哈夫**。2020。《代码模型的对抗样本》。*OOPSLA* 4（2020），1–30。
- en: Yin and Neubig (2017) Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural
    Model for General-Purpose Code Generation. In *ACL*. 440–450.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 和 Neubig（2017）**彭程·尹** 和 **格雷厄姆·纽比格**。2017。《一种用于通用代码生成的语法神经模型》。在 *ACL* 会议中，440–450。
- en: 'Yu et al. (2018a) Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang,
    Zifan Li, and Dragomir R. Radev. 2018a. SyntaxSQLNet: Syntax Tree Networks for
    Complex and Cross-Domain Text-to-SQL Task. In *EMNLP*. 1653–1663.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2018a）Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang, Zifan
    Li, 和 Dragomir R. Radev. 2018a. SyntaxSQLNet: 用于复杂和跨领域文本到 SQL 任务的语法树网络. 见 *EMNLP*.
    1653–1663.'
- en: 'Yu et al. (2019a) Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang,
    Xi Victoria Lin, et al. 2019a. CoSQL: A Conversational Text-to-SQL Challenge Towards
    Cross-Domain Natural Language Interfaces to Databases. In *EMNLP*. 1962–1979.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2019a）Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria
    Lin, 等. 2019a. CoSQL: 面向跨领域自然语言数据库接口的对话式文本到 SQL 挑战. 见 *EMNLP*. 1962–1979.'
- en: 'Yu et al. (2018b) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, et al. 2018b. Spider: A Large-Scale Human-Labeled Dataset for Complex
    and Cross-Domain Semantic Parsing and Text-to-SQL Task. In *EMNLP*. 3911–3921.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2018b）Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan
    Li, 等. 2018b. Spider: 用于复杂和跨领域语义解析及文本到 SQL 任务的大规模人工标注数据集. 见 *EMNLP*. 3911–3921.'
- en: 'Yu et al. (2019b) Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria
    Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, et al. 2019b. SParC: Cross-Domain
    Semantic Parsing in Context. In *ACL*. 4511–4523.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2019b）Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria
    Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, 等. 2019b. SParC: 跨领域语义解析上下文.
    见 *ACL*. 4511–4523.'
- en: Zhang et al. (2020a) Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and
    Zhi Jin. 2020a. Generating adversarial examples for holding robustness of source
    code processing models. In *AAAI*, Vol. 34\. 1169–1176.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020a）Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, 和 Zhi Jin.
    2020a. 生成对抗样本以保持源代码处理模型的鲁棒性. 见 *AAAI*, 第34卷. 1169–1176.
- en: Zhang et al. (2021b) Jingfeng Zhang, Haiwen Hong, Yin Zhang, Yao Wan, Ye Liu,
    and Yulei Sui. 2021b. Disentangled Code Representation Learning for Multiple Programming
    Languages. In *Findings of ACL*. 4454–4466.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2021b）Jingfeng Zhang, Haiwen Hong, Yin Zhang, Yao Wan, Ye Liu, 和 Yulei
    Sui. 2021b. 多编程语言的解耦代码表示学习. 见 *ACL 发现*. 4454–4466.
- en: 'Zhang et al. (2022a) Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy
    Li, and Milos Gligoric. 2022a. CoditT5: Pretraining for Source Code and Natural
    Language Editing. In *37th IEEE/ACM International Conference on Automated Software
    Engineering, ASE 2022, Rochester, MI, USA, October 10-14, 2022*. ACM, 22:1–22:12.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022a）Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li,
    和 Milos Gligoric. 2022a. CoditT5: 针对源代码和自然语言编辑的预训练. 见 *第37届 IEEE/ACM 自动化软件工程国际会议,
    ASE 2022, 罗切斯特, MI, USA, 2022年10月10-14日*. ACM, 22:1–22:12.'
- en: Zhang et al. (2020c) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2020c. Retrieval-based neural source code summarization. In *ICSE*. 1385–1397.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020c）Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, 和 Xudong Liu.
    2020c. 基于检索的神经源代码摘要. 见 *ICSE*. 1385–1397.
- en: Zhang et al. (2019) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan
    Wang, and Xudong Liu. 2019. A novel neural source code representation based on
    abstract syntax tree. In *ICSE*. 783–794.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2019）Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, 和
    Xudong Liu. 2019. 一种基于抽象语法树的新型神经源代码表示. 见 *ICSE*. 783–794.
- en: Zhang et al. (2021a) Tianyi Zhang, Zhiyang Chen, Yuanli Zhu, Priyan Vaithilingam,
    Xinyu Wang, and Elena L Glassman. 2021a. Interpretable Program Synthesis. In *Proceedings
    of the 2021 CHI Conference on Human Factors in Computing Systems*. 1–16.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2021a）Tianyi Zhang, Zhiyang Chen, Yuanli Zhu, Priyan Vaithilingam, Xinyu
    Wang, 和 Elena L Glassman. 2021a. 可解释程序合成. 见 *2021年 CHI 会议计算机系统人因学会议论文集*. 1–16.
- en: 'Zhang et al. (2020b) Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang
    Li. 2020b. Adversarial attacks on deep-learning models in natural language processing:
    A survey. *TIST* 11, 3 (2020), 1–41.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2020b）Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, 和 Chenliang Li.
    2020b. 对自然语言处理中的深度学习模型进行对抗攻击: 一项综述. *TIST* 11, 3（2020），1–41.'
- en: 'Zhang et al. (2022b) Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong
    Gu. 2022b. Diet code is healthy: simplifying programs for pre-trained models of
    code. In *ESEC/FSE*. 1073–1084.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022b）Zhaowei Zhang, Hongyu Zhang, Beijun Shen, 和 Xiaodong Gu. 2022b.
    Diet code 是健康的: 为预训练代码模型简化程序. 见 *ESEC/FSE*. 1073–1084.'
- en: 'Zhao and Huang (2018) Gang Zhao and Jeff Huang. 2018. Deepsim: deep learning
    code functional similarity. In *ESEC/FSE*. 141–151.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 和 Huang（2018）Gang Zhao 和 Jeff Huang. 2018. Deepsim: 深度学习代码功能相似性. 见 *ESEC/FSE*.
    141–151.'
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A
    pre-trained model for code generation with multilingual evaluations on humaneval-x.
    *arXiv preprint arXiv:2303.17568* (2023).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng 等人 (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei
    Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li 等。2023。Codegeex: 一个用于代码生成的预训练模型，并在
    humaneval-x 上进行了多语言评估。*arXiv 预印本 arXiv:2303.17568* (2023)。'
- en: 'Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017.
    Seq2sql: Generating structured queries from natural language using reinforcement
    learning. *arXiv:1709.00103* (2017).'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等人 (2017) Victor Zhong, Caiming Xiong 和 Richard Socher。2017。Seq2sql：使用强化学习从自然语言生成结构化查询。*arXiv:1709.00103*
    (2017)。
- en: Zhou et al. (2021) Xin Zhou, DongGyun Han, and David Lo. 2021. Assessing Generalizability
    of CodeBERT. In *ICSME*. IEEE, 425–436.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 (2021) Xin Zhou, DongGyun Han 和 David Lo。2021。评估 CodeBERT 的泛化能力。发表于
    *ICSME*。IEEE, 425–436。
- en: 'Zhou et al. (2019) Yaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du, and
    Yang Liu. 2019. Devign: Effective Vulnerability Identification by Learning Comprehensive
    Program Semantics via Graph Neural Networks. In *NeurIPS*. 10197–10207.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 (2019) Yaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du 和 Yang
    Liu。2019。Devign: 通过图神经网络学习全面程序语义进行有效漏洞识别。发表于 *NeurIPS*。10197–10207。'
- en: Zhou et al. (2022) Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue
    Chen, and Harald C. Gall. 2022. Adversarial Robustness of Deep Code Comment Generation.
    *TOSEM* 31, 4 (2022), 60:1–60:30.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 (2022) Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue
    Chen 和 Harald C. Gall。2022。深度代码注释生成的对抗鲁棒性。*TOSEM* 31, 4 (2022), 60:1–60:30。
- en: 'Zhu et al. (2020) Qihao Zhu, Zeyu Sun, Xiran Liang, Yingfei Xiong, and Lu Zhang.
    2020. OCoR: an overlapping-aware code retriever. In *ASE*. 883–894.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人 (2020) Qihao Zhu, Zeyu Sun, Xiran Liang, Yingfei Xiong 和 Lu Zhang。2020。OCoR:
    一个重叠感知代码检索器。发表于 *ASE*。883–894。'
- en: Zhu et al. (2021) Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan,
    Yingfei Xiong, and Lu Zhang. 2021. A syntax-guided edit decoder for neural program
    repair. In *ESEC/FSE*. 341–353.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2021) Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei
    Xiong 和 Lu Zhang。2021。用于神经程序修复的语法引导编辑解码器。发表于 *ESEC/FSE*。341–353。
- en: Zhu et al. (2022) Xiaoning Zhu, Chaofeng Sha, and Junyu Niu. 2022. A Simple
    Retrieval-based Method for Code Comment Generation. In *SANER*. 1089–1100.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2022) Xiaoning Zhu, Chaofeng Sha 和 Junyu Niu。2022。用于代码注释生成的简单检索方法。发表于
    *SANER*。1089–1100。
- en: 'Zou et al. (2019) Deqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li, and Hai Jin.
    2019. $\mu$VulDeePecker: A deep learning-based system for multiclass vulnerability
    detection. *TDSC* (2019).'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人 (2019) Deqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li 和 Hai Jin。2019。$\mu$VulDeePecker：一个基于深度学习的多类别漏洞检测系统。*TDSC*
    (2019)。
- en: Zou et al. (2021) Deqing Zou, Yawei Zhu, Shouhuai Xu, Zhen Li, Hai Jin, and
    Hengkai Ye. 2021. Interpreting deep learning-based vulnerability detector predictions
    based on heuristic searching. *TOSEM* 30, 2 (2021), 1–31.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人 (2021) Deqing Zou, Yawei Zhu, Shouhuai Xu, Zhen Li, Hai Jin 和 Hengkai
    Ye。2021。基于启发式搜索的深度学习漏洞检测器预测解析。*TOSEM* 30, 2 (2021), 1–31。
- en: Zügner et al. (2021) Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure
    Leskovec, and Stephan Günnemann. 2021. Language-Agnostic Representation Learning
    of Source Code from Structure and Context. In *ICLR*.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zügner 等人 (2021) Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec
    和 Stephan Günnemann。2021。基于结构和上下文的无语言表示学习。发表于 *ICLR*。
- en: Supplementary Materials
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 补充材料
- en: Appendix A Surveyed Venues
  id: totrans-554
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 调查的场所
- en: 'Table [6](#A2.T6 "Table 6 ‣ Appendix B Neural Network Modules and Techniques
    ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit") summarizes
    a list of top-tier conferences and journals surveyed in this paper.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [6](#A2.T6 "表格 6 ‣ 附录 B 神经网络模块和技术 ‣ 代码智能的深度学习：调查、基准和工具包") 总结了本文调查的顶级会议和期刊列表。
- en: Appendix B Neural Network Modules and Techniques
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 神经网络模块和技术
- en: Benefiting from the powerful representation ability of deep learning, recently
    deep neural networks have been widely used to represent source code as distributed
    vector representations. Here we quickly review several major neural network modules
    and techniques, e.g., RNN, CNN, attention mechanism, Transformer, graph neural
    network, and model pre-training. Let $\mathcal{C}=\{c_{1},c_{2},\ldots,c_{n}\}$
    denote a code corpus, which is composed of a collection of code snippets. For
    each code snippet $c_{i}$, it is composed of a sequence of code tokens, i.e.,
    $c_{i}=\{x_{1},x_{2},\ldots,x_{|c_{i}|}\}$, where $|\cdot|$ denotes the length
    of the sequence. Let $\mathbf{x}_{i}=w(x_{i})$ denote the word embedding corresponding
    to the $i$-th token in the code snippet.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 受益于深度学习强大的表示能力，近年来深度神经网络被广泛应用于将源代码表示为分布式向量表示。这里我们快速回顾几种主要的神经网络模块和技术，例如 RNN、CNN、注意力机制、Transformer、图神经网络和模型预训练。设
    $\mathcal{C}=\{c_{1},c_{2},\ldots,c_{n}\}$ 表示一个代码语料库，它由一系列代码片段组成。对于每个代码片段 $c_{i}$，它由一系列代码标记组成，即
    $c_{i}=\{x_{1},x_{2},\ldots,x_{|c_{i}|}\}$，其中 $|\cdot|$ 表示序列的长度。设 $\mathbf{x}_{i}=w(x_{i})$
    表示对应于代码片段中第 $i$ 个标记的词嵌入。
- en: 'RNN. Recurrent Neural Networks (RNNs) are neural networks designed to handle
    the sequential inputs with variable lengths. At time step $t$, the hidden state
    $\mathbf{h}_{t}$ of RNNs is updated as follows:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: RNN。递归神经网络（RNN）是设计用来处理具有可变长度序列输入的神经网络。在时间步 $t$，RNN 的隐藏状态 $\mathbf{h}_{t}$ 更新如下：
- en: '| (1) |  | $\mathbf{h}_{t}=f(\mathbf{x}_{t},\mathbf{h}_{t-1})\,,$ |  |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\mathbf{h}_{t}=f(\mathbf{x}_{t},\mathbf{h}_{t-1})\,,$ |  |'
- en: where $f$ is the non-linear mapping function, which is usually a hyperbolic
    tangent, i.e., $f(\mathbf{x}_{t},\mathbf{h}_{t})=\operatorname{tanh}(\mathbf{W}_{x}\mathbf{x}_{t}+\mathbf{W}_{h}\mathbf{h}_{t-1}+\mathbf{b})$.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 是非线性映射函数，通常为双曲正切函数，即 $f(\mathbf{x}_{t},\mathbf{h}_{t})=\operatorname{tanh}(\mathbf{W}_{x}\mathbf{x}_{t}+\mathbf{W}_{h}\mathbf{h}_{t-1}+\mathbf{b})$。
- en: There have been many variants of RNNs. To alleviate the gradient vanishing issue
    of RNNs, the Long Short-Term Memory (LSTM) \citesecondaryhochreiter1997long2 technology
    with a gate mechanism is proposed to determine the information accumulation, where
    the input gate, forget gate and output gate control the input, output and forget
    part of the entire network through weights and activation function. GRU (Gate
    Recurrent Unit) \citesecondarychung2015gated2 is a simplified version of LSTM
    with fewer parameters by combining the forget and input gates into a single update
    gate by merging the cell state and hidden state. Furthermore, bi-directional RNNs
    are used to represent the sequential contents in both forward and backward directions,
    with two parallel RNNs and combined outputs.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 已经出现了许多变体。为了缓解 RNN 的梯度消失问题，提出了具有门控机制的长短期记忆（LSTM）\citesecondaryhochreiter1997long2
    技术来决定信息的积累，其中输入门、遗忘门和输出门通过权重和激活函数控制整个网络的输入、输出和遗忘部分。GRU（门控递归单元）\citesecondarychung2015gated2
    是 LSTM 的简化版本，通过将遗忘门和输入门合并为一个更新门，并合并细胞状态和隐藏状态，减少了参数数量。此外，双向 RNN 被用于表示前向和后向的序列内容，使用两个并行的
    RNN 和结合输出。
- en: Although RNNs are effective in representing sequential texts, it is difficult
    to be parallelized since sequential computation inhibits parallelization. That
    means, the computation of the current time step is dependent on the output of
    the previous time step. Furthermore, the RNNs are also difficult to handle long-range
    dependency when processing long sequences.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 RNN 在表示序列文本方面非常有效，但由于序列计算限制了并行化，因此很难进行并行处理。这意味着当前时间步的计算依赖于前一个时间步的输出。此外，RNN
    在处理长序列时也很难处理长期依赖问题。
- en: CNN. Convolutional Neural Networks (CNNs) which are originally designed to process
    the pixels of images, have also been introduced to model the sequential texts.
    A CNN is composed of convolutional layers and pooling layers. The convolutional
    layer uses convolution operation to extract meaningful local patterns of inputs,
    and the pooling layer reduces the parameters and computation to make the networks
    deeper. A CNN first concatenates the word embedding of each code token as $\mathbf{x}_{1:n}=\mathbf{x}_{1}\oplus\mathbf{x}_{2}\oplus\cdots\oplus\mathbf{x}_{n}$,
    where $\oplus$ is the concatenation operator. For a window of words $\mathbf{x}_{i:i+h-1}$,
    a feature $\mathbf{h}_{i}$ is calculated by the filter $f$ as $\mathbf{h}_{i}=f(\mathbf{W}\mathbf{x}_{i:i+h-1}+\mathbf{b})$,
    where $\mathbf{b}$ is a bias term and $f$ is a non-linear function like hyperbolic
    tangent. After applying the filter to all possible windows of words, a feature
    map will be obtained, i.e., $\mathbf{h}=[\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{n-h+1}]$.
    Then a pooling layer can be applied to obtain the final representation of code,
    i.e., $\hat{\mathbf{h}}=\operatorname{Pooling}(\mathbf{h})$.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: CNN。卷积神经网络（CNNs）最初设计用于处理图像的像素，但也被引入到序列文本建模中。CNN由卷积层和池化层组成。卷积层使用卷积操作来提取输入的有意义的局部模式，而池化层减少参数和计算，以使网络更深。CNN首先将每个代码标记的词嵌入连接为
    $\mathbf{x}_{1:n}=\mathbf{x}_{1}\oplus\mathbf{x}_{2}\oplus\cdots\oplus\mathbf{x}_{n}$，其中
    $\oplus$ 是连接操作符。对于窗口中的词 $\mathbf{x}_{i:i+h-1}$，通过滤波器 $f$ 计算特征 $\mathbf{h}_{i}$
    为 $\mathbf{h}_{i}=f(\mathbf{W}\mathbf{x}_{i:i+h-1}+\mathbf{b})$，其中 $\mathbf{b}$
    是偏置项，$f$ 是像双曲正切这样的非线性函数。将滤波器应用于所有可能的词窗口后，将获得特征图，即 $\mathbf{h}=[\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{n-h+1}]$。然后，可以应用池化层以获得代码的最终表示，即
    $\hat{\mathbf{h}}=\operatorname{Pooling}(\mathbf{h})$。
- en: The CNNs that exploit local dependencies using convolution operations are easy
    to parallelize. However, they require many layers to model the long-distance dependencies
    when handling long sequences.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积操作来利用局部依赖的卷积神经网络（CNNs）易于并行化。然而，当处理长序列时，它们需要许多层来建模长距离依赖关系。
- en: Attention Mechanism. The attention mechanism is a simple yet effective module
    in deep neural networks. It was first introduced by \citesecondarybahdanau2014neural2
    in Neural Machine Translation (NMT) to mitigate the issue of information loss
    in compressing long sentences into a fixed-length vector, as well as the alignment
    between input and output sequences. Since then, the attention mechanism has become
    a widely used component to improve the performance of various models in NLP and
    computer vision. In NMT, the attention mechanism is designed to include an additional
    context vector that allows the decoder to access the entire encoded input sequence
    $\{\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{m}\}$. Especially, it aims
    to learn the attention weights $\alpha_{ij}$, which captures the relevance between
    the encoder hidden state $\mathbf{h}_{i}$ and decoder hidden state $\mathbf{s}_{j}$.
    Consequently, the context vector can be formulated as $c_{j}=\sum_{i=1}^{T}\alpha_{ij}\mathbf{h}_{i}$.
    In general, the attention can also be seen as a mapping of keys $\mathbf{K}$ to
    an attention distribution $\alpha$ according to query $\mathbf{q}$, i.e., $\alpha(\mathbf{q},\mathbf{K})=\operatorname{softmax}(g(\mathbf{q},\mathbf{K}))$,
    where $g$ is the attention score function which measures the similarity between
    the query and key. In some cases, there is also an additional input of values
    $V$ on which the attention distribution is applied. Hence a generalized attention
    with a set of key-value pairs $(\mathbf{K},\mathbf{V})$ and query $\mathbf{Q}$
    is formulated as $\alpha(\mathbf{Q},\mathbf{K},\mathbf{V})=\operatorname{softmax}(g(\mathbf{Q},\mathbf{K}))\cdot\mathbf{V}$.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制。注意机制是深度神经网络中的一个简单但有效的模块。它首次由 \citesecondarybahdanau2014neural2 在神经机器翻译（NMT）中引入，以缓解将长句压缩为固定长度向量时的信息丢失问题，以及输入和输出序列之间的对齐问题。从那时起，注意机制成为了提高各种自然语言处理（NLP）和计算机视觉模型性能的广泛使用的组件。在
    NMT 中，注意机制被设计为包含一个额外的上下文向量，使解码器能够访问整个编码输入序列 $\{\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{m}\}$。特别地，它旨在学习注意权重
    $\alpha_{ij}$，捕捉编码器隐藏状态 $\mathbf{h}_{i}$ 与解码器隐藏状态 $\mathbf{s}_{j}$ 之间的相关性。因此，上下文向量可以表示为
    $c_{j}=\sum_{i=1}^{T}\alpha_{ij}\mathbf{h}_{i}$。通常，注意机制也可以看作是将键 $\mathbf{K}$ 映射到注意分布
    $\alpha$，依据查询 $\mathbf{q}$，即 $\alpha(\mathbf{q},\mathbf{K})=\operatorname{softmax}(g(\mathbf{q},\mathbf{K}))$，其中
    $g$ 是衡量查询和键之间相似性的注意得分函数。在某些情况下，还会有一个额外的值输入 $V$，注意分布应用于这些值。因此，具有一组键值对 $(\mathbf{K},\mathbf{V})$
    和查询 $\mathbf{Q}$ 的广义注意机制被表示为 $\alpha(\mathbf{Q},\mathbf{K},\mathbf{V})=\operatorname{softmax}(g(\mathbf{Q},\mathbf{K}))\cdot\mathbf{V}$。
- en: Table 6. A list of top-tier conferences and journals surveyed in this paper.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6. 本文调查的顶级会议和期刊列表。
- en: '|  | No. | Venue | Venue (Full name) |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '|  | No. | 会议地点 | 会议地点（全名） |'
- en: '| SE | 1 | ICSE | ACM/IEEE International Conference on Software Engineering
    |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| SE | 1 | ICSE | ACM/IEEE 软件工程国际会议 |'
- en: '| 2 | ASE | IEEE/ACM International Conference Automated Software Engineering
    |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ASE | IEEE/ACM 自动化软件工程国际会议 |'
- en: '| 3 | FSE/ESEC | ACM SIGSOFT Symposium on the Foundation of Software Engineering/European
    Software Engineering Conference |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| 3 | FSE/ESEC | ACM SIGSOFT 软件工程基础研讨会/欧洲软件工程会议 |'
- en: '| 4 | SANER | IEEE International Conference on Software Analysis, Evolution
    and Reengineering |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| 4 | SANER | IEEE 软件分析、演变与重构国际会议 |'
- en: '| 5 | ICSME | IEEE International Conference on Software Maintenance and Evolution
    |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| 5 | ICSME | IEEE 软件维护与演进国际会议 |'
- en: '| 6 | ICPC | IEEE International Conference on Program Comprehension |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| 6 | ICPC | IEEE 国际程序理解会议 |'
- en: '| 7 | ESEM | International Symposium on Empirical Software Engineering and
    Measurement |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| 7 | ESEM | 实证软件工程与测量国际研讨会 |'
- en: '| 8 | MSR | International Conference on Mining Software Repositories |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| 8 | MSR | 国际软件仓库挖掘会议 |'
- en: '| 9 | TSE | IEEE Transactions on Software Engineering |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 9 | TSE | IEEE 软件工程汇刊 |'
- en: '|  | 10 | TOSEM | ACM Transactions on Software Engineering and Methodology
    |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '|  | 10 | TOSEM | ACM 软件工程与方法学汇刊 |'
- en: '| PL/Compiler | 1 | POPL | ACM SIGPLAN-SIGACT Symposium on Principles of Programming
    Languages |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| PL/Compiler | 1 | POPL | ACM SIGPLAN-SIGACT 编程语言原理研讨会 |'
- en: '| 2 | PLDI | ACM SIGPLAN Conference on Programming Language Design & Implementation
    |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| 2 | PLDI | ACM SIGPLAN 编程语言设计与实现会议 |'
- en: '| 3 | OOPSLA | Conference on Object-Oriented Programming Systems, Languages,
    and Applications |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| 3 | OOPSLA | 面向对象编程系统、语言及应用大会 |'
- en: '| 4 | CGO | Code Generation and Optimization |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| 4 | CGO | 代码生成与优化 |'
- en: '| 5 | PACT | International Conference on Parallel Architectures and Compilation
    Techniques |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| 5 | PACT | 国际并行架构与编译技术会议 |'
- en: '| 6 | TOPLAS | ACM Transactions on Programming Languages & Systems |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| 6 | TOPLAS | ACM 编程语言与系统汇刊 |'
- en: '| 7 | TACO | ACM Transactions on Architecture and Code Optimization |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| 7 | TACO | ACM 架构与代码优化汇刊 |'
- en: '| AI | 1 | NeurIPS | Annual Conference on Neural Information Processing Systems
    |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| AI | 1 | NeurIPS | 神经信息处理系统年度会议 |'
- en: '| 2 | ICML | International Conference on Machine Learning |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ICML | 国际机器学习会议 |'
- en: '| 3 | ICLR | International Conference on Learning Representations |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| 3 | ICLR | 国际学习表示会议 |'
- en: '| 4 | AAAI | AAAI Conference on Artificial Intelligence |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| 4 | AAAI | AAAI 人工智能会议 |'
- en: '| 5 | IJCAI | International Joint Conference on Artificial Intelligence |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| 5 | IJCAI | 国际人工智能联合会议 |'
- en: '| 6 | SIGKDD | ACM Knowledge Discovery and Data Mining |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| 6 | SIGKDD | ACM 知识发现与数据挖掘 |'
- en: '| 7 | WWW | The Web Conference |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| 7 | WWW | 网络会议 |'
- en: '| NLP | 1 | ACL | Annual Meeting of the Association for Computational Linguistics
    |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| NLP | 1 | ACL | 计算语言学协会年会 |'
- en: '| 2 | EMNLP | Conference on Empirical Methods in Natural Language Processing
    |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| 2 | EMNLP | 自然语言处理经验方法会议 |'
- en: '| 3 | NAACL | The Annual Conference of the North American Chapter of the Association
    for Computational Linguistics |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| 3 | NAACL | 北美计算语言学协会年会 |'
- en: '| Security | 1 | CCS | ACM Conference on Computer and Communications Security
    |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| Security | 1 | CCS | ACM 计算机与通信安全会议 |'
- en: '| 2 | S&P | IEEE Symposium on Security and Privacy |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| 2 | S&P | IEEE 安全与隐私研讨会 |'
- en: '| 3 | USENIX Security | Usenix Security Symposium |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| 3 | USENIX Security | Usenix 安全研讨会 |'
- en: '| 4 | NDSS | ISOC Network and Distributed System Security Symposium |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| 4 | NDSS | ISOC 网络与分布式系统安全研讨会 |'
- en: '| 5 | TIFS | IEEE Transactions on Information Forensics and Security |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| 5 | TIFS | IEEE 信息取证与安全汇刊 |'
- en: 'Transformer. Since sequential computation in RNNs inhibits parallelization,
    recently, a new state-of-the-art network Transformer has been designed for parallel
    processing of sequences (Vaswani et al., [2017](#bib.bib232)). The Transformer
    is mainly composed of multiple self-attention blocks. When feeding a sequence
    of code tokens $c_{i}=\{x_{1},x_{2},\ldots,x_{|c_{i}|}\}$ into Transformer, the
    Transformer block at layer $l$ will produce a sequence of hidden states for each
    code token, i.e., $\mathbf{H}^{l}=[\mathbf{h}_{1}^{l},\mathbf{h}_{2}^{l},\ldots,\mathbf{h}_{|c_{i}|}^{l}]$.
    For each layer, the layer representation $\mathbf{H}^{l}$ is computed by the $l$-th
    layer Transformer block $\mathbf{H}^{l}=\mathrm{Transformer}_{l}(\mathbf{H}^{l-1})$,
    where $l\in\{1,2,\ldots,L\}$. In each Transformer block, multiple self-attention
    heads are used to aggregate the output vectors of the previous layer. A general
    attention mechanism can be formulated as the weighted sum of the value vector
    $\mathbf{V}$, using the query vector $\mathbf{Q}$ and the key vector $\mathbf{K}$:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer。由于 RNN 的顺序计算抑制了并行化，最近设计了一种新的最先进网络 Transformer 以实现序列的并行处理（Vaswani
    等， [2017](#bib.bib232)）。Transformer 主要由多个自注意力块组成。当将代码标记序列 $c_{i}=\{x_{1},x_{2},\ldots,x_{|c_{i}|}\}$
    输入到 Transformer 时，层 $l$ 的 Transformer 块将为每个代码标记生成一个隐藏状态序列，即 $\mathbf{H}^{l}=[\mathbf{h}_{1}^{l},\mathbf{h}_{2}^{l},\ldots,\mathbf{h}_{|c_{i}|}^{l}]$。对于每一层，层表示
    $\mathbf{H}^{l}$ 是通过第 $l$ 层 Transformer 块计算的 $\mathbf{H}^{l}=\mathrm{Transformer}_{l}(\mathbf{H}^{l-1})$，其中
    $l\in\{1,2,\ldots,L\}$。在每个 Transformer 块中，多个自注意力头用于汇总前一层的输出向量。一般的注意力机制可以表述为值向量
    $\mathbf{V}$ 的加权和，使用查询向量 $\mathbf{Q}$ 和键向量 $\mathbf{K}$：
- en: '| (2) |  | $\alpha(\mathbf{Q},\mathbf{K},\mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{\text{model}}}}\right)\cdot\mathbf{V}\,,$
    |  |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\alpha(\mathbf{Q},\mathbf{K},\mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{\text{model}}}}\right)\cdot\mathbf{V}\,,$
    |  |'
- en: where $d_{\rm model}$ represents the dimension of hidden representations. For
    self-attention, $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are mappings of previous
    hidden representations by different linear functions, i.e., $\mathbf{Q}=\mathbf{H}^{l-1}\mathbf{W}_{Q}^{l}$,
    $\mathbf{K}=\mathbf{H}^{l-1}\mathbf{W}_{K}^{l}$, and $\mathbf{V}=\mathbf{H}^{l-1}\mathbf{W}_{V}^{l}$,
    respectively. At last, the encoder produces a final contextual representation
    $\mathbf{H}^{L}=[\mathbf{h}^{L}_{1},\ldots,\mathbf{h}^{L}_{n}]$, which is obtained
    from the last Transformer block.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{\rm model}$ 表示隐藏表示的维度。对于自注意力，$\mathbf{Q}$、$\mathbf{K}$ 和 $\mathbf{V}$
    是通过不同线性函数对先前隐藏表示的映射，即 $\mathbf{Q}=\mathbf{H}^{l-1}\mathbf{W}_{Q}^{l}$、$\mathbf{K}=\mathbf{H}^{l-1}\mathbf{W}_{K}^{l}$
    和 $\mathbf{V}=\mathbf{H}^{l-1}\mathbf{W}_{V}^{l}$。最后，编码器生成最终的上下文表示 $\mathbf{H}^{L}=[\mathbf{h}^{L}_{1},\ldots,\mathbf{h}^{L}_{n}]$，这是从最后一个
    Transformer 块中获得的。
- en: 'Graph Neural Network. The Graph Neural Networks (GNNs) take a graph as input
    and the goal is to learn the representations of each node by recursively updating
    until convergence, which aggregates the information of each neighborhood node.
    Let $\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{A})$ denote a graph, where $\mathcal{V}$
    is a set of nodes, $\mathcal{E}$ is a set of edges, and $\mathbf{A}$ is a adjacency
    matrix. In a graph, let $v_{i}\in\mathcal{V}$ denote a node and $e_{ij}=(v_{i},v_{j})\in\mathcal{E}$
    denote an edge. For graph $\mathcal{G}$, the hidden representation $\mathbf{h}_{v}$
    for each node $v$ can be updated as follows:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络。图神经网络（GNNs）以图作为输入，目标是通过递归更新直到收敛来学习每个节点的表示，这将聚合每个邻域节点的信息。令 $\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{A})$
    表示一个图，其中 $\mathcal{V}$ 是节点集合，$\mathcal{E}$ 是边集合，$\mathbf{A}$ 是邻接矩阵。在图中，令 $v_{i}\in\mathcal{V}$
    表示一个节点，$e_{ij}=(v_{i},v_{j})\in\mathcal{E}$ 表示一条边。对于图 $\mathcal{G}$，每个节点 $v$ 的隐藏表示
    $\mathbf{h}_{v}$ 可以按如下方式更新：
- en: '| (3) |  | $\mathbf{h}_{v,t+1}=f(\ell_{v},\ell_{e},\mathbf{h}_{v^{\prime},t},\ell_{v^{\prime}})\,,$
    |  |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\mathbf{h}_{v,t+1}=f(\ell_{v},\ell_{e},\mathbf{h}_{v^{\prime},t},\ell_{v^{\prime}})\,,$
    |  |'
- en: where $f$ is a local transition function (shared among all nodes), which updates
    the node state according to the input neighborhood. $\ell_{v}$ denotes the label
    attributes of node $v$, $\ell_{e}$ denotes the label attributes of the corresponding
    edges of node $v$, $\mathbf{h}_{v^{\prime},t}$ denotes the hidden representations
    of node $v$’s neighbors at time step $t$, and $\ell_{v^{\prime}}$ denotes the
    label attributes of node $v$’s neighbors. For the options of $f$, there are several
    approaches attempting to use the gate mechanism like GRU \citesecondarychung2015gated
    or LSTM \citesecondaryhochreiter1997long in the propagation step to diminish the
    restrictions in the former GNN models, thereby improving the long-term propagation
    of information across the graph structure. If the local transition function $f$
    is GRU, the model is called Gated Graph Neural Network (GGNN) \citesecondaryli2015gated.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 是一个局部转移函数（在所有节点之间共享），它根据输入的邻域更新节点状态。$\ell_{v}$ 表示节点 $v$ 的标签属性，$\ell_{e}$
    表示节点 $v$ 的相应边的标签属性，$\mathbf{h}_{v^{\prime},t}$ 表示节点 $v$ 的邻居在时间步 $t$ 的隐藏表示，$\ell_{v^{\prime}}$
    表示节点 $v$ 的邻居的标签属性。对于 $f$ 的选项，有几种方法尝试在传播步骤中使用门控机制，如 GRU \citesecondarychung2015gated
    或 LSTM \citesecondaryhochreiter1997long，以减少前 GNN 模型中的限制，从而改善图结构中信息的长期传播。如果局部转移函数
    $f$ 是 GRU，则模型称为门控图神经网络（GGNN） \citesecondaryli2015gated。
- en: Appendix C Public Datasets
  id: totrans-606
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 公共数据集
- en: Table 7. A summary of datasets that have been used for code intelligence.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7. 用于代码智能的 datasets 摘要。
- en: '| Dataset | Language | Train | Valid | Test | Doc. | Granularity | AST |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 语言 | 训练 | 验证 | 测试 | 文档 | 细粒度 | AST |'
- en: '| CSN-Java | Java | 454,451 | 15,328 | 26,909 | Y | Function | Y |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| CSN-Java | Java | 454,451 | 15,328 | 26,909 | Y | Function | Y |'
- en: '| CSN-JavaScript | JavaScript | 123,889 | 8,253 | 6,483 | Y | Function | Y
    |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| CSN-JavaScript | JavaScript | 123,889 | 8,253 | 6,483 | Y | Function | Y
    |'
- en: '| CSN-Python | Python | 412,178 | 23,107 | 22,176 | Y | Function | Y |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| CSN-Python | Python | 412,178 | 23,107 | 22,176 | Y | Function | Y |'
- en: '| CSN-PHP | PHP | 523,712 | 26,015 | 28,391 | Y | Function | Y |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| CSN-PHP | PHP | 523,712 | 26,015 | 28,391 | Y | Function | Y |'
- en: '| CSN-Go | Go | 317,832 | 14,242 | 14,291 | Y | Function | Y |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| CSN-Go | Go | 317,832 | 14,242 | 14,291 | Y | Function | Y |'
- en: '| CSN-Ruby | Ruby | 48,791 | 2,209 | 2279 | Y | Function | Y |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| CSN-Ruby | Ruby | 48,791 | 2,209 | 2279 | Y | Function | Y |'
- en: '| CoSQA-QA | Java, Python | 20,000 | 604 | - | Y | Function | N |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| CoSQA-QA | Java, Python | 20,000 | 604 | - | Y | Function | N |'
- en: '| CoSQA-Search | Java, Python | 19,604 | 500 | 500 | Y | Function | N |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| CoSQA-Search | Java, Python | 19,604 | 500 | 500 | Y | Function | N |'
- en: '| Py150 | Python | 100,000 | - | 50,000 | N | File | Y |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| Py150 | Python | 100,000 | - | 50,000 | N | 文件 | Y |'
- en: '| python-code-docstring | Python | 55,538 | 18,505 | 18,502 | Y | Function
    | Y |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| python-code-docstring | Python | 55,538 | 18,505 | 18,502 | Y | 功能 | Y |'
- en: '| DeepCom-Java | Java | 69,708 | 8,714 | 8,714 | Y | Function | Y |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| DeepCom-Java | Java | 69,708 | 8,714 | 8,714 | Y | 功能 | Y |'
- en: '| CCSD | C | 1,275,937 | 425,312 | 425,312 | Y | Function | N |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| CCSD | C | 1,275,937 | 425,312 | 425,312 | Y | 功能 | N |'
- en: '| Typescript | Typescript | 49,850 | 7,854 | 4,650 | N | File | N |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| Typescript | Typescript | 49,850 | 7,854 | 4,650 | N | 文件 | N |'
- en: '| CodeNet | mainly C++ | 1,235,000 | - | - | N | File | N |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| CodeNet | 主要是 C++ | 1,235,000 | - | - | N | 文件 | N |'
- en: '| JuICe | Python | 1,518,049 | 1,744 | 1,981 | Y | File | N |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| JuICe | Python | 1,518,049 | 1,744 | 1,981 | Y | 文件 | N |'
- en: '| ProGraML | C, C++, Fortran, Haskell, Swift | 250,428 | - | - | N | File |
    N |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| ProGraML | C, C++, Fortran, Haskell, Swift | 250,428 | - | - | N | 文件 | N
    |'
- en: '| FunCom | Java | 1,937,136 | 106,153 | 105,832 | Y | Function | N |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| FunCom | Java | 1,937,136 | 106,153 | 105,832 | Y | 功能 | N |'
- en: '| CoDesc | Java | 3,369,218 | 421,149 | 421,149 | Y | Function | N |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| CoDesc | Java | 3,369,218 | 421,149 | 421,149 | Y | 功能 | N |'
- en: '| APPS | Python | 117,232 | - | 115,212 | N | File | N |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| APPS | Python | 117,232 | - | 115,212 | N | 文件 | N |'
- en: '| AVATAR | Python, Java | 40,423 | 848 | 1,699 | N | File | N |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| AVATAR | Python, Java | 40,423 | 848 | 1,699 | N | 文件 | N |'
- en: '| PyTorrent | Python | 2,273,157 | 284,145 | 284,144 | Y | Function | Y |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| PyTorrent | Python | 2,273,157 | 284,145 | 284,144 | Y | 功能 | Y |'
- en: '| StaQC | Python, SQL | 267,065 | - | - | N | File | N |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| StaQC | Python, SQL | 267,065 | - | - | N | 文件 | N |'
- en: '| CodeQA-Python | Python | 56,084 | 6,999 | 6,999 | N | Function | N |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| CodeQA-Python | Python | 56,084 | 6,999 | 6,999 | N | 功能 | N |'
- en: '| CodeQA-Java | Java | 95,777 | 11,999 | 11,999 | N | Function | N |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| CodeQA-Java | Java | 95,777 | 11,999 | 11,999 | N | 功能 | N |'
- en: '| CodeXGLUE-BCB | Java | 901,028 | 415,416 | 415,416 | N | Function | N |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| CodeXGLUE-BCB | Java | 901,028 | 415,416 | 415,416 | N | 功能 | N |'
- en: '| CodeXGLUE-POJ104 | C/C++ | 32,000 | 8,000 | 12,000 | N | Function | N |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| CodeXGLUE-POJ104 | C/C++ | 32,000 | 8,000 | 12,000 | N | 功能 | N |'
- en: '| CodeXGLUE-Devign | C | 21,854 | 2,732 | 2,732 | N | Function | N |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| CodeXGLUE-Devign | C | 21,854 | 2,732 | 2,732 | N | 功能 | N |'
- en: '| CodeXGLUE-Bugs2Fix | Java | 52,364 | 6,545 | 6,545 | N | Function | N |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| CodeXGLUE-Bugs2Fix | Java | 52,364 | 6,545 | 6,545 | N | 功能 | N |'
- en: '| CodeXGLUE-CodeTrans | Java/C# | 10,295 | 499 | 1,000 | N | Function | N |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| CodeXGLUE-CodeTrans | Java/C# | 10,295 | 499 | 1,000 | N | 功能 | N |'
- en: '| CodeXGLUE-WebQuery | Python | 251,820 | 9,604 | - | Y | Function | N |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| CodeXGLUE-WebQuery | Python | 251,820 | 9,604 | - | Y | 功能 | N |'
- en: '| CodeXGLUE-Concode | Java | 100,000 | 2,000 | 2,000 | Y | Function | N |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| CodeXGLUE-Concode | Java | 100,000 | 2,000 | 2,000 | Y | 功能 | N |'
- en: '| CodeXGLUE-MicoDocs | mainly English | 155,926 | 4,000 | 4,00 | Y | Docs |
    N |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| CodeXGLUE-MicoDocs | 主要是英语 | 155,926 | 4,000 | 4,000 | Y | 文档 | N |'
- en: Many datasets have been collected for different tasks of code intelligence.
    In the literature, many pre-processing steps have been conducted before using
    the dataset for evaluation.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据集已被收集用于不同的代码智能任务。在文献中，使用数据集进行评估之前进行了许多预处理步骤。
- en: •
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CodeSearchNet (CSN)^(14)^(14)14https://github.com/github/CodeSearchNet \citesecondaryhusain2019codesearchnet2
    is a dataset collected from GitHub, originally for code search. It contains about
    6 million functions, where 2 millions of them are annotated with natural language
    descriptions, covering six programming languages (i.e., Go, Java, JavaScript,
    PHP, Python, and Ruby). Since the code snippets in this dataset are paired with
    natural language descriptions and have the characteristic of multi-linguality,
    this dataset can also be used for code summarization and cross-language-related
    tasks. In addition, this dataset has been used for large-scale pre-training, attributed
    to its large scale.
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeSearchNet (CSN)^(14)^(14)14https://github.com/github/CodeSearchNet \citesecondaryhusain2019codesearchnet2
    是一个从 GitHub 收集的数据集，最初用于代码搜索。它包含大约 600 万个函数，其中 200 万个函数带有自然语言描述，涵盖了六种编程语言（即 Go、Java、JavaScript、PHP、Python
    和 Ruby）。由于此数据集中的代码片段与自然语言描述配对，并具有多语言特征，因此也可用于代码总结和跨语言相关任务。此外，鉴于其大规模，这个数据集也被用于大规模预训练。
- en: •
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CoSQA^(15)^(15)15https://github.com/Jun-jie-Huang/CoCLR \citesecondaryhuang2021cosqa2
    is a dataset of paired natural-language queries and code snippets designed for
    code search and question answering, where the queries are from the real search
    logs of Microsoft Bing and the code snippets are from GitHub. It consists of 20,604
    pairs of data samples and each pair is annotated by at least 3 crowd-sourced workers.
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoSQA^(15)^(15)15https://github.com/Jun-jie-Huang/CoCLR \citesecondaryhuang2021cosqa2
    是一个用于代码搜索和问答的配对自然语言查询和代码片段的数据集，其中查询来自 Microsoft Bing 的真实搜索日志，代码片段来自 GitHub。它包含
    20,604 对数据样本，每对样本由至少 3 名众包工作者标注。
- en: •
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Py150^(16)^(16)16https://eth-sri.github.io/py150 \citesecondaryraychev2016probabilistic2
    is a collection of 150$k$ Python source code files from GitHub, which has been
    widely used for evaluating code completion.
  id: totrans-647
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Py150^(16)^(16)16https://eth-sri.github.io/py150 \citesecondaryraychev2016probabilistic2
    是一个包含来自 GitHub 的 150$k$ 个 Python 源代码文件的集合，已被广泛用于评估代码补全。
- en: •
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: python-code-docstring^(17)^(17)17https://github.com/wanyao1992/code_summarization_public \citesecondarybarone2017parallel2,wan2018improving2
    is a dataset of parallel Python code snippets with corresponding descriptions,
    which has been widely adopted for code summarization.
  id: totrans-649
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: python-code-docstring^(17)^(17)17https://github.com/wanyao1992/code_summarization_public \citesecondarybarone2017parallel2,wan2018improving2
    是一个包含对应描述的并行 Python 代码片段的数据集，已被广泛用于代码总结。
- en: •
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DeepCom-Java^(18)^(18)18https://github.com/xing-hu/DeepCom \citesecondaryhu2018deep2
    is a dataset collected from the open source repositories in GitHub. In this dataset,
    only repositories are implemented in Java, and those with at least 10 stars are
    considered. Finally, 588,108 pairs of Java methods and their corresponding Javadoc
    are obtained.
  id: totrans-651
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DeepCom-Java^(18)^(18)18https://github.com/xing-hu/DeepCom \citesecondaryhu2018deep2
    是一个从 GitHub 上的开源代码库中收集的数据集。在此数据集中，仅包含 Java 实现的代码库，并且只有至少 10 个星标的代码库才被考虑。最终，获得了
    588,108 对 Java 方法及其相应的 Javadoc。
- en: •
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CCSD-benchmark-for-code-summarization^(19)^(19)19https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization \citesecondaryliu2020retrieval2
    is a C benchmark dataset for code summarization, which is crawled from GitHub,
    and contains 95$k$+ functions as well as natural-language descriptions.
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CCSD-benchmark-for-code-summarization^(19)^(19)19https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization \citesecondaryliu2020retrieval2
    是一个 C 语言基准数据集，用于代码总结，来自 GitHub，包含 95$k$+ 个函数及其自然语言描述。
- en: •
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Typescript^(20)^(20)20https://github.com/DeepTyper/DeepTyper \citesecondaryhellendoorn2018deep2
    is a dataset of Typescript code snippets, which contain the type information of
    variables and can be used for type inference evaluation.
  id: totrans-655
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Typescript^(20)^(20)20https://github.com/DeepTyper/DeepTyper \citesecondaryhellendoorn2018deep2
    是一个 Typescript 代码片段的数据集，包含变量的类型信息，可用于类型推断评估。
- en: •
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CodeNet^(21)^(21)21https://github.com/IBM/Project_CodeNet \citesecondarypuri2021project2
    is a large-scale dataset released by IBM for code intelligence, consisting of
    14 million code samples and about 500 million lines of code in 55 different programming
    languages.
  id: totrans-657
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeNet^(21)^(21)21https://github.com/IBM/Project_CodeNet \citesecondarypuri2021project2
    是 IBM 发布的一个大规模数据集，用于代码智能，包含 1400 万个代码样本和约 5 亿行代码，覆盖 55 种不同的编程语言。
- en: •
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: JuICe^(22)^(22)22https://github.com/rajasagashe/juice \citesecondaryagashe2019juice2
    is large-scale dataset for open-domain, context-based code generation, which consists
    of 1.5 million examples with a curated test set of 3.7$k$ instances from online
    programming assignments.
  id: totrans-659
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JuICe^(22)^(22)22https://github.com/rajasagashe/juice \citesecondaryagashe2019juice2
    是一个用于开放领域、基于上下文的代码生成的大规模数据集，包含 150 万个示例，经过精心策划的测试集包含来自在线编程作业的 3.7$k$ 实例。
- en: •
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ProGraML^(23)^(23)23https://github.com/ChrisCummins/ProGraML \citesecondarycummins2020programl2
    is a benchmark dataset that converts programs into graphs based on LLVM-IRs, consisting
    of 250$k$ LLVM-IR files covering six programming languages. It can be used for
    evaluating GNNs for program optimization and analysis.
  id: totrans-661
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ProGraML^(23)^(23)23https://github.com/ChrisCummins/ProGraML \citesecondarycummins2020programl2
    是一个基准数据集，将程序转换为基于 LLVM-IR 的图形，包含 250$k$ 个 LLVM-IR 文件，覆盖六种编程语言。它可用于评估用于程序优化和分析的
    GNNs。
- en: •
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: FunCom^(24)^(24)24http://leclair.tech/data/funcom/ \citesecondaryleclair2019recommendations2
    provides a curated dataset for code summarization, which excludes auto-generated
    source code files, as well all functions that are lack of associated comments.
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FunCom^(24)^(24)24http://leclair.tech/data/funcom/ \citesecondaryleclair2019recommendations2
    提供了一个用于代码总结的精心策划的数据集，排除了自动生成的源代码文件以及缺乏相关注释的所有函数。
- en: •
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CoDesc^(25)^(25)25https://github.com/code-desc/CoDesc \citesecondaryhasan2021codesc2
    is a large-scale parallel dataset composed of 4.2 million Java methods and natural
    language descriptions, based on CodeSearchNet, DeepCom, CONCODE and FunCom, with
    comprehensive data cleaning processes and manually check.
  id: totrans-665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoDesc^(25)^(25)25https://github.com/code-desc/CoDesc \citesecondaryhasan2021codesc2
    是一个大规模的并行数据集，由 420 万个 Java 方法和自然语言描述组成，基于 CodeSearchNet、DeepCom、CONCODE 和 FunCom，经过全面的数据清洗和人工检查。
- en: •
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: APPS^(26)^(26)26https://github.com/hendrycks/apps \citesecondaryhendrycks2021measuring
    is a benchmark dataset for code generation in Python based on natural language
    specification, consisting of 10,000 problems at various levels of difficulty,
    covering simple introductory problems, interview-level problems, and coding competition
    challenges.
  id: totrans-667
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: APPS^(26)^(26)26https://github.com/hendrycks/apps \citesecondaryhendrycks2021measuring
    是一个基于自然语言规范的 Python 代码生成基准数据集，包含 10,000 个各种难度的问题，包括简单的入门问题、面试级问题和编码竞赛挑战。
- en: •
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: AVATAR^(27)^(27)27https://github.com/wasiahmad/AVATAR \citesecondaryahmad2021avatar2
    is a parallel corpus for Java-Python program translation. It is composed of 8,475
    programming problems and their solutions implemented in Java and Python, collected
    from open source programming contest sites (e.g., AtCoder, Google Code Jam, and
    Codeforces), and online platforms (e.g., GeeksforGeeks, LeetCode, and Project
    Euler).
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: AVATAR^(27)^(27)27https://github.com/wasiahmad/AVATAR \citesecondaryahmad2021avatar2
    是一个用于 Java-Python 程序翻译的平行语料库。它由 8,475 个编程问题及其用 Java 和 Python 实现的解决方案组成，这些问题和解决方案来自开源编程竞赛网站（例如
    AtCoder、Google Code Jam 和 Codeforces）以及在线平台（例如 GeeksforGeeks、LeetCode 和 Project
    Euler）。
- en: •
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: PyTorrent \citesecondarybahrami2021pytorrent2 is a Python dataset of paired
    source code and natural-language comment, which is similar to the CodeSearchNet
    dataset. The difference is that it is collected from Python libraries such as
    PyPI and Anaconda packages, rather than GitHub projects.
  id: totrans-671
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PyTorrent \citesecondarybahrami2021pytorrent2 是一个 Python 数据集，包含配对的源代码和自然语言注释，类似于
    CodeSearchNet 数据集。不同之处在于，它是从 PyPI 和 Anaconda 包等 Python 库中收集的，而不是 GitHub 项目。
- en: •
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: StaQC \citesecondaryyao2018staqc2 is a collection of about 148$k$ Python and
    120$k$ SQL question-code pairs, mined from Stack Overflow.
  id: totrans-673
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: StaQC \citesecondaryyao2018staqc2 是一个包含约 148$k$ Python 和 120$k$ SQL 问题-代码对的集合，来自
    Stack Overflow。
- en: •
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CodeQA^(28)^(28)28https://github.com/jadecxliu/CodeQA \citesecondaryliu2021codeqa2
    is a free-form question-answering dataset for code comprehension, in which a code
    snippet and a question are given, and a textual answer is required to be generated.
    It is composed of a Java dataset with 119,778 question-answer pairs and a Python
    dataset with 70,085 question-answer pairs.
  id: totrans-675
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeQA^(28)^(28)28https://github.com/jadecxliu/CodeQA \citesecondaryliu2021codeqa2
    是一个自由格式的代码理解问答数据集，其中给定一个代码片段和一个问题，需要生成一个文本答案。它由一个包含 119,778 个问题-答案对的 Java 数据集和一个包含
    70,085 个问题-答案对的 Python 数据集组成。
- en: •
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CodeXGLUE^(29)^(29)29https://github.com/microsoft/CodeXGLUE \citesecondarylu2021codexglue2
    is a benchmark dataset for code understanding and generation, which can support
    multiple code intelligence tasks, such as code summarization, code completion,
    code search, and program translation.
  id: totrans-677
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeXGLUE^(29)^(29)29https://github.com/microsoft/CodeXGLUE \citesecondarylu2021codexglue2
    是一个用于代码理解和生成的基准数据集，支持多种代码智能任务，如代码总结、代码补全、代码搜索和程序翻译。
- en: Appendix D Evaluation Metrics
  id: totrans-678
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 评估指标
- en: We have evaluated the performance of our proposed model based on four widely-used
    evaluation criteria in the area of neural machine translation, i.e., BLEU \citesecondarypapineni2002bleu,
    METEOR \citesecondarybanerjee2005meteor, and ROUGE \citesecondarylin2004rouge.
    BLEU measures the average n-gram precision on a set of reference sentences, with
    a penalty for short sentences. METEOR is recall-oriented and measures how well
    our model captures the contents from the references in our output. ROUGE considers
    sentence-level structure similarity and identifies the longest co-occurring in
    sequence n-grams automatically.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据神经机器翻译领域四个广泛使用的评估标准评估了我们提出的模型的性能，即 BLEU \citesecondarypapineni2002bleu、METEOR
    \citesecondarybanerjee2005meteor 和 ROUGE \citesecondarylin2004rouge。BLEU 衡量一组参考句子的平均
    n-gram 准确率，并对短句施加惩罚。METEOR 是以召回为导向的，衡量我们的模型在输出中捕捉到参考内容的效果。ROUGE 考虑句子级结构相似性，并自动识别最长的共同序列
    n-gram。
- en: BLEU (Bilingual Evaluation Understudy) is a classical evaluation metric in neural
    machine translation, which measures the average n-gram precision on a set of reference
    sentences, with a penalty for short sentences.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（Bilingual Evaluation Understudy）是神经机器翻译中的经典评估指标，衡量一组参考句子的平均 n-gram 准确率，并对短句施加惩罚。
- en: '| (4) |  | $p_{n}=\frac{\sum_{w_{n}\in a}\min\left(c_{a}(w_{n}),\underset{j=1,\cdots,&#124;n&#124;}{\max}c_{b_{j}}(w_{n})\right)}{\sum_{w_{n}\in
    a}c_{a}(w_{n})},$ |  |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $p_{n}=\frac{\sum_{w_{n}\in a}\min\left(c_{a}(w_{n}),\underset{j=1,\cdots,|n|}{\max}c_{b_{j}}(w_{n})\right)}{\sum_{w_{n}\in
    a}c_{a}(w_{n})},$ |  |'
- en: 'where $a$ and $b$ represent a candidate sentence and its corresponding reference,
    respectively. $w_{n}$ represents the n-gram word, $c_{a}(w_{n})$ represents the
    count of n-gram $w_{n}$ in sentence $a$. Since $p_{n}$ encourages to prediction
    of short sentences, thus, we introduce a Brevity Penalty (BP) defined as follows:'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a$ 和 $b$ 分别表示候选句子及其对应的参考句子。$w_{n}$ 代表 n-gram 词，$c_{a}(w_{n})$ 表示句子 $a$ 中
    n-gram $w_{n}$ 的计数。由于 $p_{n}$ 鼓励对短句子的预测，因此，我们引入了如下定义的简洁惩罚（BP）：
- en: '| (5) |  | $\operatorname{BP}=\left\{\begin{matrix}1&amp;if\ c>r\\ e^{(1-\frac{r}{c})}&amp;if\
    c\leq r,\end{matrix}\right.$ |  |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\operatorname{BP}=\left\{\begin{matrix}1&amp; \text{如果}\ c>r\\
    e^{(1-\frac{r}{c})}&amp; \text{如果}\ c\leq r,\end{matrix}\right.$ |  |'
- en: 'where $r$ is the reference sentence length, $c$ the length of the candidate
    sentence. Finally, the BLEU is calculated as follows:'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r$ 是参考句子的长度，$c$ 是候选句子的长度。最后，BLEU 计算如下：
- en: '| (6) |  | $\operatorname{BLEU}=\operatorname{BP}*\exp\left(\sum_{n=1}^{N}\alpha_{n}\log
    p_{n}\right),$ |  |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\operatorname{BLEU}=\operatorname{BP}*\exp\left(\sum_{n=1}^{N}\alpha_{n}\log
    p_{n}\right),$ |  |'
- en: where $N=1,2,3,4$, $p_{n}$ is the n-gram precision of n-grams up to $N$, $\alpha_{n}$
    is positive weight for each gram.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N=1,2,3,4$，$p_{n}$ 是 n-gram 的 n-gram 精度，$\alpha_{n}$ 是每个 gram 的正权重。
- en: 'METEOR is recall-oriented and measures how well our model captures content
    from the references in our output. METEOR matches candidate sentences and reference
    sentence words one by one, and then calculates the harmonic average of their accuracy
    and recall rates between the corresponding candidate sentence and the reference
    sentence. It is calculated as follows:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: METEOR 是以召回为导向的，并衡量我们的模型在输出中捕捉参考内容的效果。METEOR 按一一匹配候选句子和参考句子中的词，然后计算它们之间准确率和召回率的调和平均数。其计算公式如下：
- en: '| (7) |  | $\operatorname{METEOR}=\underset{j=1,\cdots,&#124;b&#124;}{\max}\left(\frac{10PR}{R+9P}\right)\left(1-\frac{1}{2}\left(\frac{\#\text{chunks}}{\#\text{matched\
    uni-gram}}\right)\right),$ |  |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\operatorname{METEOR}=\underset{j=1,\cdots,&#124;b&#124;}{\max}\left(\frac{10PR}{R+9P}\right)\left(1-\frac{1}{2}\left(\frac{\#\text{chunks}}{\#\text{matched\
    uni-gram}}\right)\right),$ |  |'
- en: where $P$ = uni-gram precision, $R$ =uni-gram recall, $\#chunks$ is the number
    of matched chunks between two sentences. $\#\operatorname{matched\ unigram}$ is
    the number of matched uni-grams between the candidate sentence and the reference
    sentence.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P$ = uni-gram 精度，$R$ = uni-gram 召回率，$\#chunks$ 是两个句子之间匹配的块的数量。$\#\operatorname{matched\
    unigram}$ 是候选句子和参考句子之间匹配的 uni-gram 数量。
- en: 'ROUGE is one of the general metrics in the automatic text summarization, it
    is a measurement based on the longest common sub-sequence (Longest Common Sub-sequence,
    LCS). ROUGE takes into account sentence-level structure similarity naturally and
    identifies the longest co-occurring in sequence n-grams automatically. It is calculated
    as follows:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE 是自动文本摘要中的一种通用度量，它基于最长公共子序列（Longest Common Sub-sequence，LCS）进行测量。ROUGE
    自然考虑了句子级结构的相似性，并自动识别最长的共现序列 n-grams。其计算公式如下：
- en: '| (8) |  | $\operatorname{ROUGE}=\frac{\sum_{j=1}^{&#124;b&#124;}\sum_{w_{n}\in
    b_{j}}\min\left(c_{a}(w_{n}),c_{b_{j}}(w_{n})\right)}{\sum_{j=1}^{&#124;b&#124;}\sum_{w_{n}\in
    b_{j}}c_{b_{j}}(w_{n})},$ |  |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\operatorname{ROUGE}=\frac{\sum_{j=1}^{&#124;b&#124;}\sum_{w_{n}\in
    b_{j}}\min\left(c_{a}(w_{n}),c_{b_{j}}(w_{n})\right)}{\sum_{j=1}^{&#124;b&#124;}\sum_{w_{n}\in
    b_{j}}c_{b_{j}}(w_{n})},$ |  |'
- en: 'MRR (Mean Reciprocal Rank) serves as a metric for assessing processes that
    generate lists of potential responses to a set of queries, arranged based on their
    likelihood of correctness. This metric computes the average reciprocal ranks for
    the outcomes associated with a given set of queries. The reciprocal rank of a
    response to a query corresponds to the inverse of the rank assigned to the first
    correct answer within the list. The MRR is formulated as follows:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: MRR（均值倒排等级）作为一个度量标准，用于评估生成一系列潜在回应的过程，这些回应是基于其正确性可能性进行排序的。该度量计算给定查询集合的结果的平均倒排等级。一个查询回应的倒排等级对应于列表中第一个正确答案的排名的倒数。MRR
    的公式如下：
- en: '| (9) |  | $\operatorname{MRR}=\frac{1}{Q}\sum_{i=1}^{Q}\frac{1}{rank_{i}}\,,$
    |  |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\operatorname{MRR}=\frac{1}{Q}\sum_{i=1}^{Q}\frac{1}{rank_{i}}\,,$
    |  |'
- en: where $Q$ is the number of queries, and $rank_{i}$ is the rank position of the
    first correct answer for the $i$-th query.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q$ 是查询的数量，$rank_{i}$ 是第 $i$ 个查询的第一个正确答案的排名位置。
- en: 'Accuracy stands as a pivotal metric for evaluating the efficacy of a classification
    model. This metric is delineated by the ratio of correct predictions to the total
    number of predictions generated by the model, as follows:'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性是评估分类模型有效性的关键指标。这个指标由模型生成的正确预测数量与总预测数量的比值来描述，如下所示：
- en: '| (10) |  | $\operatorname{Accuracy}=\frac{\text{Number of Correct Predictions}}{\text{Total
    Number of Predictions Made}}\,.$ |  |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\operatorname{Accuracy}=\frac{\text{Number of Correct Predictions}}{\text{Total
    Number of Predictions Made}}\,.$ |  |'
- en: \bibliographystylesecondary
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: \bibliographystylesecondary
- en: unsrt \bibliographysecondaryrefappendix
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: unsrt \bibliographysecondaryrefappendix
