- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:56:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:56:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2103.09316] Are deep learning models superior for missing data imputation
    in surveys? Evidence from an empirical comparison'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2103.09316] 深度学习模型在调查中缺失数据插补方面是否优越？来自实证比较的证据'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.09316](https://ar5iv.labs.arxiv.org/html/2103.09316)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2103.09316](https://ar5iv.labs.arxiv.org/html/2103.09316)
- en: Are deep learning models superior for missing data imputation in surveys? Evidence
    from an empirical comparison
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型在调查中缺失数据插补方面是否优越？来自实证比较的证据
- en: 'Zhenhua Wang, Olanrewaju Akande, Jason Poulos and Fan Li ¹¹1Zhenhua Wang is
    PhD student in the Department of Statistics, University of Missouri, Columbia,
    MO 65211 (E-mail: [zhenhua.wang@mail.missouri.edu](mailto:zhenhua.wang@mail.missouri.edu));
    Olanrewaju Akande is research scientist at Meta Platforms, Inc. (E-mail: [akandelanre13@gmail.com](mailto:akandelanre13@gmail.com));
    Jason Poulos is Postdoctoral Associate in the Department of Health Care Policy,
    Harvard Medical School, Boston, MA (E-mail: [poulos@hcp.med.harvard.edu](mailto:poulos@hcp.med.harvard.edu));
    and Fan Li is Professor in the Department of Statistical Science, Box 90251, Duke
    University, Durham, NC 27708 (E-mail: [fl35@duke.edu](mailto:fl35@duke.edu)).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 王振华，奥兰雷瓦朱·阿坎德，杰森·普洛斯和范莉 ¹¹1 王振华是密苏里大学统计系的博士生，地址：Columbia, MO 65211（电子邮件：[zhenhua.wang@mail.missouri.edu](mailto:zhenhua.wang@mail.missouri.edu)）；奥兰雷瓦朱·阿坎德是Meta
    Platforms, Inc.的研究科学家（电子邮件：[akandelanre13@gmail.com](mailto:akandelanre13@gmail.com)）；杰森·普洛斯是哈佛医学院卫生政策系的博士后（电子邮件：[poulos@hcp.med.harvard.edu](mailto:poulos@hcp.med.harvard.edu)）；范莉是杜克大学统计科学系的教授，地址：Box
    90251, Durham, NC 27708（电子邮件：[fl35@duke.edu](mailto:fl35@duke.edu)）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Multiple imputation (MI) is a popular approach for dealing with missing data
    arising from non-response in sample surveys. Multiple imputation by chained equations
    (MICE) is one of the most widely used MI algorithms for multivariate data, but
    it lacks theoretical foundation and is computationally intensive. Recently, missing
    data imputation methods based on deep learning models have been developed with
    encouraging results in small studies. However, there has been limited research
    on evaluating their performance in realistic settings compared to MICE, particularly
    in big surveys. We conduct extensive simulation studies based on a subsample of
    the American Community Survey to compare the repeated sampling properties of four
    machine learning based MI methods: MICE with classification trees, MICE with random
    forests, generative adversarial imputation networks, and multiple imputation using
    denoising autoencoders. We find the deep learning imputation methods are superior
    to MICE in terms of computational time. However, with the default choice of hyperparameters
    in the common software packages, MICE with classification trees consistently outperforms,
    often by a large margin, the deep learning imputation methods in terms of bias,
    mean squared error, and coverage under a range of realistic settings.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多重插补（MI）是一种处理样本调查中缺失数据的流行方法。链式方程的多重插补（MICE）是多变量数据中最广泛使用的MI算法之一，但它缺乏理论基础且计算量大。最近，基于深度学习模型的缺失数据插补方法取得了小规模研究中的鼓舞人心的结果。然而，与MICE相比，特别是在大规模调查中，这些方法的实际表现研究仍然有限。我们基于美国社区调查的一个子样本进行了广泛的模拟研究，以比较四种基于机器学习的MI方法的重复采样特性：MICE与分类树，MICE与随机森林，生成对抗插补网络，以及使用去噪自编码器的多重插补。我们发现，深度学习插补方法在计算时间方面优于MICE。然而，使用常见软件包中的默认超参数选择时，MICE与分类树在偏差、均方误差和覆盖率方面通常大幅度优于深度学习插补方法，适用于多种现实设置。
- en: '*Key words:*: Deep learning; hyperparameter selection; missing data; multiple
    imputation by chained equations; simulation studies; survey data.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词:* 深度学习；超参数选择；缺失数据；链式方程的多重插补；模拟研究；调查数据。'
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Many sample surveys suffer from missing data, arising from unit nonresponse,
    where a subset of participants do not complete the survey, or item nonresponse,
    where missing values are concentrated on particular questions. In opinion polls,
    nonresponse may reflect either refusal to reveal a preference or lack of a preference
    [[11](#bib.bibx11)]. If not properly handled, missing data patterns can lead to
    biased statistical analyses, especially when there are systematic differences
    between the observed data and the missing data [[41](#bib.bibx41), [31](#bib.bibx31)].
    Complete case analysis on units with completely observed data is often infeasible
    and may lead to large bias in most situations [[31](#bib.bibx31)]. As a result,
    many analysts account for the missing data by imputing missing values and then
    proceeding as if the imputed values are true values.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 许多样本调查因缺失数据而受到影响，这些缺失数据可能源于单位非响应，即部分参与者未完成调查，或项目非响应，即缺失值集中在特定问题上。在民意调查中，非响应可能反映了拒绝透露偏好或缺乏偏好[[11](#bib.bibx11)]。如果处理不当，缺失数据模式可能导致统计分析偏差，尤其是在观察数据和缺失数据之间存在系统性差异时[[41](#bib.bibx41),
    [31](#bib.bibx31)]。对完全观察到数据的单位进行完整案例分析通常不可行，并且在大多数情况下可能导致较大偏差[[31](#bib.bibx31)]。因此，许多分析师通过对缺失值进行插补来处理缺失数据，然后按插补值为真实值进行分析。
- en: Multiple imputation (MI) [[42](#bib.bibx42)] is a popular approach for handling
    missing values. In MI, an analyst creates $L>1$ completed datasets by replacing
    the missing values in the sample data with plausible draws generated from the
    predictive distribution of probabilistic models based on the observed data. In
    each completed dataset, the analyst can then compute sample estimates for population
    estimands of interest, and combine the sample estimates across all $L$ datasets
    using MI inference methods developed by [[42](#bib.bibx42)], and more recently,
    [[43](#bib.bibx43), [3](#bib.bibx3), [39](#bib.bibx39)], and [[20](#bib.bibx20)].
    In MI, the estimated variance of an estimand consists of both within-imputation
    and between-imputation variances, and thus takes into account the inherent variability
    of the imputed values. Note that in survey studies, single imputation, e.g. via
    matching or regression, remains to be common for dealing with missing data, where
    the variance is estimated via the delta method or resampling methods [[10](#bib.bibx10),
    [22](#bib.bibx22)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多重插补（MI）[[42](#bib.bibx42)]是一种处理缺失值的常用方法。在MI中，分析师通过用从基于观察数据的概率模型预测分布生成的合理值替换样本数据中的缺失值，创建$L>1$个完成的数据集。在每个完成的数据集中，分析师可以计算感兴趣的总体估计量的样本估计，并使用[[42](#bib.bibx42)]开发的MI推断方法，最近还有[[43](#bib.bibx43),
    [3](#bib.bibx3), [39](#bib.bibx39)]和[[20](#bib.bibx20)]，将所有$L$个数据集中的样本估计进行合并。在MI中，估计量的方差包括插补内部和插补之间的方差，从而考虑了插补值的固有变异性。请注意，在调查研究中，单次插补，例如通过匹配或回归，仍然是处理缺失数据的常见方法，其中方差通过德尔塔方法或重抽样方法估计[[10](#bib.bibx10),
    [22](#bib.bibx22)]。
- en: 1.1 Model-based imputation
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 基于模型的插补
- en: There are two general modeling strategies for MI. The first strategy, known
    as *joint modeling* (JM), is to specify a joint distribution for all variables
    in the data, and then generate imputations from the implied conditional (predictive)
    distributions of the variables with missing values [[44](#bib.bibx44)]. The JM
    strategy aligns with the theoretical foundation of [[42](#bib.bibx42)], but it
    can be challenging to specify a joint model with high-dimensional variables of
    different types. Indeed, most popular JM approaches, such as “PROC MI” in SAS
    [[59](#bib.bibx59)], and “AMELIA” [[23](#bib.bibx23)] and “norm” in R [[44](#bib.bibx44)],
    make a simplifying assumption that the data follow multivariate Gaussian distributions,
    even for categorical variables, which can lead to bias [[24](#bib.bibx24)]. Recent
    research developed flexible JM models based on advanced Bayesian nonparametric
    models such as Dirichlet Process mixtures [[34](#bib.bibx34), [37](#bib.bibx37)].
    However, these methods are computationally expensive, and often struggle to scale
    up to high-dimensional cases.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MI 有两种一般建模策略。第一种策略，称为 *联合建模* (JM)，是为数据中的所有变量指定一个联合分布，然后从隐含的条件（预测）分布中生成缺失值变量的填补值
    [[44](#bib.bibx44)]。JM 策略与 [[42](#bib.bibx42)] 的理论基础一致，但指定具有不同类型高维变量的联合模型可能具有挑战性。确实，大多数流行的
    JM 方法，如 SAS 中的 “PROC MI” [[59](#bib.bibx59)]，以及 R 中的 “AMELIA” [[23](#bib.bibx23)]
    和 “norm” [[44](#bib.bibx44)]，都做出了一个简化假设，即数据遵循多元高斯分布，即使对于分类变量也是如此，这可能导致偏差 [[24](#bib.bibx24)]。最近的研究开发了基于先进的贝叶斯非参数模型（如
    Dirichlet 过程混合模型）[[34](#bib.bibx34), [37](#bib.bibx37)] 的灵活 JM 模型。然而，这些方法计算开销大，通常难以扩展到高维情况。
- en: The second strategy is called *fully conditional specification* (FCS, [[51](#bib.bibx51)]),
    where one separately specifies a univariate conditional distribution for each
    variable with missing values given all the other variables and imputes the missing
    values variable-by-variable iteratively, akin to a Gibbs sampler. The most popular
    FCS method is multiple imputation by chained equations (MICE) [[52](#bib.bibx52)],
    usually implemented with specifying generalized linear models (GLMs) for the univariate
    conditional distributions [[38](#bib.bibx38), [40](#bib.bibx40), [47](#bib.bibx47)].
    Recent research indicates that specifying the conditional models by classification
    and regression trees (CART, [[6](#bib.bibx6), [7](#bib.bibx7)]) comprehensively
    outperforms MICE with GLM [[1](#bib.bibx1)]. A natural extension of MICE with
    CART is to use ensemble tree methods such as random forests, rather than a single
    tree [[5](#bib.bibx5), [12](#bib.bibx12)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略称为 *完全条件指定* (FCS, [[51](#bib.bibx51)]), 在这种策略中，为每个缺失值变量单独指定一个单变量条件分布，给定所有其他变量，并逐个变量迭代填补缺失值，类似于
    Gibbs 采样器。最流行的 FCS 方法是链式方程的多重填补 (MICE) [[52](#bib.bibx52)]，通常通过指定单变量条件分布的广义线性模型
    (GLMs) 实现 [[38](#bib.bibx38), [40](#bib.bibx40), [47](#bib.bibx47)]。最近的研究表明，通过分类和回归树
    (CART, [[6](#bib.bibx6), [7](#bib.bibx7)]) 指定条件模型在综合表现上优于使用 GLM 的 MICE [[1](#bib.bibx1)]。MICE
    与 CART 的自然扩展是使用集成树方法，如随机森林，而不是单一树 [[5](#bib.bibx5), [12](#bib.bibx12)]。
- en: MICE is appealing in large-scale survey data because it is simple and flexible
    in imputing different types of variables. However, MICE has a key theoretical
    drawback that the specified conditional distributions may be incompatible, that
    is, they do not correspond to a joint distribution [[2](#bib.bibx2), [15](#bib.bibx15),
    [27](#bib.bibx27)]. Despite this drawback, MICE works remarkably well in real
    applications and numerous simulations have demonstrated it outperforms many theoretically
    sound JM-based methods; see [[50](#bib.bibx50)] for case studies. However, MICE
    is also computationally intensive [[55](#bib.bibx55)] and generally cannot be
    parallelized. Moreover, popular software packages for implementing MICE with GLMs,
    e.g. mice in R [[52](#bib.bibx52)], often crash in settings with high dimensional
    non-continuous variables, e.g., categorical variables with many categories [[1](#bib.bibx1)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MICE 在大规模调查数据中具有吸引力，因为它在填补不同类型的变量时简单而灵活。然而，MICE 有一个关键的理论缺陷，即指定的条件分布可能不兼容，即它们与联合分布不对应
    [[2](#bib.bibx2), [15](#bib.bibx15), [27](#bib.bibx27)]。尽管有这个缺陷，MICE 在实际应用中表现异常出色，许多模拟研究表明它优于许多理论上可靠的基于
    JM 的方法；有关案例研究请参见 [[50](#bib.bibx50)]。然而，MICE 计算密集 [[55](#bib.bibx55)]，且通常无法并行化。此外，流行的用于实现带有
    GLMs 的 MICE 的软件包，例如 R 中的 mice [[52](#bib.bibx52)]，在高维非连续变量的设置中，如具有多个类别的分类变量 [[1](#bib.bibx1)]，经常崩溃。
- en: 1.2 Imputation with deep learning models
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 使用深度学习模型进行填补
- en: Recent advances in deep learning greatly expand the scope of complex models
    for high-dimensional data. This advancement brings the hope that a new generation
    of missing data imputation methods based on deep learning models may address the
    theoretical and computational limitations of existing statistical methods. For
    example, deep generative models such as generative adversarial networks (GANs,
    [[18](#bib.bibx18)]) are naturally suitable for producing multiple imputations
    because they are designed to generate data that resemble the observed data as
    much as possible. A method in this stream is the generative adversarial imputation
    network (GAIN) of [[57](#bib.bibx57)]. Multiple imputation using denoising autoencoders
    (MIDA, [[17](#bib.bibx17), [32](#bib.bibx32)]), is another generative method based
    on deep neural networks trained on corrupted input data in order to force the
    networks to learn a useful low-dimensional representation of the input data, rather
    than its identity function [[53](#bib.bibx53), [54](#bib.bibx54)]. Several methods
    have been proposed for missing value imputation in time-series data using variational
    autoencoders [[14](#bib.bibx14)] or recurrent neural networks [[29](#bib.bibx29),
    [36](#bib.bibx36), [8](#bib.bibx8), [9](#bib.bibx9), [58](#bib.bibx58)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的最新进展大大拓展了高维数据复杂模型的范围。这一进展带来了希望，即基于深度学习模型的新一代缺失数据插补方法可能解决现有统计方法的理论和计算限制。例如，生成对抗网络（GANs，[[18](#bib.bibx18)]）等深度生成模型天生适合生成多个插补，因为它们被设计为生成尽可能类似于观测数据的数据。这一领域的一种方法是生成对抗插补网络（GAIN，[[57](#bib.bibx57)]）。另一种基于深度神经网络的生成方法是使用去噪自编码器（MIDA，[[17](#bib.bibx17)，[32](#bib.bibx32)]），该方法通过对损坏的输入数据进行训练，强迫网络学习输入数据的有用低维表示，而不是其身份函数[[53](#bib.bibx53)，[54](#bib.bibx54)]。已经提出几种方法用于使用变分自编码器[[14](#bib.bibx14)]或递归神经网络[[29](#bib.bibx29)，[36](#bib.bibx36)，[8](#bib.bibx8)，[9](#bib.bibx9)，[58](#bib.bibx58)]对时间序列数据进行缺失值插补。
- en: Deep learning based MI methods have several advantages, at least theoretically,
    over the traditional statistical models, including (i) they avoid making distributional
    assumptions; (ii) can readily handle mixed data types; (iii) can model nonlinear
    relationships between variables; (iv) are expected to perform well in high-dimensional
    settings; and (v) can leverage graphics processing unit (GPU) power for faster
    computation. Several papers report encouraging performance of deep learning based
    MI methods compared to MICE [[57](#bib.bibx57), e.g.]. However, such conclusions
    are made based on limited evidence. First, the studies are usually based on small
    simulations or several well-studied public “benchmark” datasets, such as those
    described in Section [5](#S5 "5 Evaluation based on “benchmark” datasets ‣ Are
    deep learning models superior for missing data imputation in surveys? Evidence
    from an empirical comparison"), which do not resemble survey data. Second, the
    evaluations are usually based on a few overall performance metric, e.g., the overall
    predictive mean squared error or accuracy. Such metrics may not give a full picture
    of the comparisons and sometimes can be even misleading, as will be illustrated
    later. Third, given the uncertainty of the missing data process, it is crucial
    to examine the repeated sampling properties of imputation methods, but these have
    been rarely evaluated. Finally, hyperparameter tuning is crucial for machine learning
    models and different tuning can result in dramatically different results, but
    few details are provided on hyperparameter tuning and its consequences on the
    performance of imputation methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的缺失数据插补（MI）方法在理论上相对于传统统计模型具有几个优势，包括（i）避免做分布假设；（ii）能够处理混合数据类型；（iii）能够建模变量之间的非线性关系；（iv）预计在高维环境中表现良好；（v）可以利用图形处理单元（GPU）加速计算。几篇论文报告了基于深度学习的缺失数据插补方法相对于MICE的令人鼓舞的表现[[57](#bib.bibx57)，例如]。然而，这些结论是基于有限的证据得出的。首先，这些研究通常基于小规模模拟或几个经过充分研究的公共“基准”数据集，如第[5](#S5
    "5 Evaluation based on “benchmark” datasets ‣ Are deep learning models superior
    for missing data imputation in surveys? Evidence from an empirical comparison")节中描述的，这些数据集与调查数据不相似。其次，评估通常基于几个总体性能指标，例如总体预测均方误差或准确度。这些指标可能无法全面反映比较情况，有时甚至可能误导，如后文所示。第三，考虑到缺失数据过程的不确定性，检验插补方法的重复抽样特性至关重要，但这些特性很少被评估。最后，超参数调整对机器学习模型至关重要，不同的调整可能导致截然不同的结果，但关于超参数调整及其对插补方法性能影响的细节很少提供。
- en: 'Motivated by these limitations, in this paper we carry out extensive simulations
    based on real survey data to evaluate MI methods with a range performance metrics.
    Specifically, we conduct simulations based on a subsample from the American Community
    Survey to compare repeated sampling properties of four aforementioned MI methods:
    MICE with CART (MICE-CART), MICE with random forests (MICE-RF), GAIN, and MIDA.
    We find that deep learning based MI methods are superior to MICE in terms of computational
    time. However, MICE-CART consistently outperforms, often by a large margin, the
    deep learning methods in terms of bias, mean squared error, and coverage, under
    a range of realistic settings. This contradicts previous findings in the machine
    learning literature, and raises questions on the appropriate metrics for evaluating
    imputation methods. It also highlights the importance of assessing repeated-sampling
    properties of imputation methods. Though we focus on multiple imputation in this
    paper, we note that the aforementioned MI methods are readily applicable to generate
    single imputation when $L$ is set to 1\. Extensive empirical evidences suggest
    that the within-imputation variance usually dominates the between-imputation variance
    in MI. As such, we expect the patterns between different imputation methods observed
    here also stand if these methods are used for single imputation.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些局限性，本文基于真实调查数据进行广泛的模拟，以评估各种性能指标下的MI方法。具体而言，我们基于美国社区调查的一个子样本进行模拟，以比较四种前述MI方法的重复抽样特性：使用CART的MICE（MICE-CART）、使用随机森林的MICE（MICE-RF）、GAIN和MIDA。我们发现，基于深度学习的MI方法在计算时间上优于MICE。然而，MICE-CART在偏差、均方误差和覆盖率方面通常大幅超越深度学习方法，在各种现实设置下都表现优异。这与机器学习文献中的先前发现相矛盾，并引发了关于评估插补方法的适当指标的问题。它还强调了评估插补方法重复抽样特性的重要性。虽然本文专注于多重插补，但我们指出，上述MI方法在$L$设置为1时也可以生成单次插补。大量实证证据表明，在MI中，插补内的方差通常主导插补间的方差。因此，我们预计在单次插补时观察到的不同插补方法之间的模式也将保持不变。
- en: The remainder of this article is organized as follows. In Section [2](#S2 "2
    Missing Data Imputation Methods ‣ Are deep learning models superior for missing
    data imputation in surveys? Evidence from an empirical comparison"), we review
    the four MI methods used in our evaluation. In Section [3](#S3 "3 Simulation-based
    evaluation of imputation methods ‣ Are deep learning models superior for missing
    data imputation in surveys? Evidence from an empirical comparison"), we describe
    a framework with several metrics for evaluating imputation methods. In Section
    [4](#S4 "4 Evaluation based on ACS ‣ Are deep learning models superior for missing
    data imputation in surveys? Evidence from an empirical comparison"), we describe
    the simulation design and results with large-scale survey data, and in Section
    [5](#S5 "5 Evaluation based on “benchmark” datasets ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison")
    we summarize evaluation results on the benchmark datasets used in machine learning
    literature. Finally, in Section [6](#S6 "6 Conclusion ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison"),
    we conclude with a practical guide for implementation in real applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。在第[2](#S2 "2 Missing Data Imputation Methods ‣ Are deep learning
    models superior for missing data imputation in surveys? Evidence from an empirical
    comparison")节，我们回顾了我们评估中使用的四种MI方法。在第[3](#S3 "3 Simulation-based evaluation of
    imputation methods ‣ Are deep learning models superior for missing data imputation
    in surveys? Evidence from an empirical comparison")节，我们描述了一个具有多个指标的框架，用于评估插补方法。在第[4](#S4
    "4 Evaluation based on ACS ‣ Are deep learning models superior for missing data
    imputation in surveys? Evidence from an empirical comparison")节，我们描述了使用大规模调查数据的模拟设计和结果，在第[5](#S5
    "5 Evaluation based on “benchmark” datasets ‣ Are deep learning models superior
    for missing data imputation in surveys? Evidence from an empirical comparison")节中，我们总结了在机器学习文献中使用的基准数据集上的评估结果。最后，在第[6](#S6
    "6 Conclusion ‣ Are deep learning models superior for missing data imputation
    in surveys? Evidence from an empirical comparison")节中，我们总结了在实际应用中的实施指南。
- en: 2 Missing Data Imputation Methods
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**缺失数据插补方法**'
- en: We first introduce notation. Consider a sample with $n$ units, each of which
    is associated with $p$ variables. Let $Y_{ij}$ be the value of variable $j$ for
    individual $i$, where $j=1,\dots,p$ and $i=1,\dots,n$. Here, $Y$ can be continuous,
    binary, categorical or mixed binary-continuous. For each individual $i$, let ${\bf
    Y}_{i}=(Y_{i1},\dots,Y_{ip})$. For each variable $j$, let ${\bf Y}_{j}=(Y_{1j},\dots,Y_{nj})$.
    Let ${\bf Y}=({\bf Y}_{1},\ldots,{\bf Y}_{n})$ be the $n\times p$ matrix comprising
    the data for all records included in the sample. We write ${\bf Y}=({\bf Y}_{\textrm{obs}},{\bf
    Y}_{\textrm{mis}})$, where ${\bf Y}_{\textrm{obs}}$ and ${\bf Y}_{\textrm{mis}}$
    are respectively the observed and missing parts of ${\bf Y}$. We write ${\bf Y}_{\textrm{mis}}=({\bf
    Y}_{\textrm{mis},1},\dots,{\bf Y}_{\textrm{mis},p})$, where ${\bf Y}_{\textrm{mis},j}$
    represents all missing values for variable $j$, with $j=1,\dots,p$. Similarly,
    we write ${\bf Y}_{\textrm{obs}}=({\bf Y}_{\textrm{obs},1},\dots,{\bf Y}_{\textrm{obs},p})$
    for the corresponding observed data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍符号。考虑一个包含 $n$ 单位的样本，每个单位关联 $p$ 个变量。设 $Y_{ij}$ 为个体 $i$ 的变量 $j$ 的值，其中 $j=1,\dots,p$
    且 $i=1,\dots,n$。这里，$Y$ 可以是连续的、二元的、类别的或混合的二元-连续型。对于每个个体 $i$，设 ${\bf Y}_{i}=(Y_{i1},\dots,Y_{ip})$。对于每个变量
    $j$，设 ${\bf Y}_{j}=(Y_{1j},\dots,Y_{nj})$。设 ${\bf Y}=({\bf Y}_{1},\ldots,{\bf
    Y}_{n})$ 为包含样本中所有记录的数据的 $n\times p$ 矩阵。我们写 ${\bf Y}=({\bf Y}_{\textrm{obs}},{\bf
    Y}_{\textrm{mis}})$，其中 ${\bf Y}_{\textrm{obs}}$ 和 ${\bf Y}_{\textrm{mis}}$ 分别是
    ${\bf Y}$ 的观测部分和缺失部分。我们写 ${\bf Y}_{\textrm{mis}}=({\bf Y}_{\textrm{mis},1},\dots,{\bf
    Y}_{\textrm{mis},p})$，其中 ${\bf Y}_{\textrm{mis},j}$ 表示变量 $j$ 的所有缺失值，$j=1,\dots,p$。类似地，我们将
    ${\bf Y}_{\textrm{obs}}=({\bf Y}_{\textrm{obs},1},\dots,{\bf Y}_{\textrm{obs},p})$
    用于对应的观测数据。
- en: In MI, the analyst generates values of the missing data ${\bf Y}_{\textrm{mis}}$
    using pre-specified models estimated with ${\bf Y}_{\textrm{obs}}$, resulting
    in a completed dataset. The analyst then repeats the process to generate $L$ completed
    datasets, $\{{\bf Y}^{(l)}:l=1,\dots,L\}$, that are available for inference or
    dissemination. For inference, the analyst can compute sample estimates for population
    estimands in each completed dataset $\textbf{Y}^{(l)}$, and combine them using
    MI inference rules developed by [[42](#bib.bibx42)], which will be reviewed in
    Section [3](#S3 "3 Simulation-based evaluation of imputation methods ‣ Are deep
    learning models superior for missing data imputation in surveys? Evidence from
    an empirical comparison").
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MI 中，分析师使用以 ${\bf Y}_{\textrm{obs}}$ 估计的预设模型生成缺失数据 ${\bf Y}_{\textrm{mis}}$
    的值，从而得到一个完整的数据集。然后，分析师重复这一过程以生成 $L$ 个完整的数据集 $\{{\bf Y}^{(l)}:l=1,\dots,L\}$，这些数据集可用于推断或传播。对于推断，分析师可以计算每个完整数据集
    $\textbf{Y}^{(l)}$ 中的样本估计，并使用 [[42](#bib.bibx42)] 发展起来的 MI 推断规则进行合并，这将在第 [3](#S3
    "3 Simulation-based evaluation of imputation methods ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison")
    节中进行回顾。
- en: 2.1 MICE with classification tree models
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 使用分类树模型的 MICE
- en: Under MICE, the analyst begins by specifying a separate univariate conditional
    model for each variable with missing values. The analyst then specifies an order
    to iterate through the sequence of the conditional models, when doing imputation.
    We write the ordered list of the variables as $(\textbf{Y}_{(1)},\dots,\textbf{Y}_{(p)})$.
    Next, the analyst initializes each ${\bf Y}_{\textrm{mis},(j)}$. The most popular
    options are to sample from (i) the marginal distribution of the corresponding
    ${\bf Y}_{\textrm{obs},(j)}$, or (ii) the conditional distribution of ${\bf Y}_{(j)}$
    given all the other variables, constructed using only available cases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MICE 方法下，分析师首先为每个缺失值变量指定一个单变量条件模型。然后，分析师指定一个顺序，以便在进行填补时依次迭代这些条件模型。我们将这些变量的有序列表写作
    $(\textbf{Y}_{(1)},\dots,\textbf{Y}_{(p)})$。接下来，分析师初始化每个 ${\bf Y}_{\textrm{mis},(j)}$。最常见的选项是从
    (i) 相应的 ${\bf Y}_{\textrm{obs},(j)}$ 的边际分布中抽样，或 (ii) 在仅使用可用样本构建的 ${\bf Y}_{(j)}$
    条件分布中抽样。
- en: After initialization, the MICE algorithm follows an iterative process that cycles
    through the sequence of univariate models. For each variable $j$ at each iteration
    $t$, one fits the conditional model $(\textbf{Y}_{(j)}|\textbf{Y}_{\textrm{obs},(j)},\{\textbf{Y}_{(k)}^{(t)}:k<j\},\{\textbf{Y}_{(k)}^{(t-1)}:k>j\})$.
    Next, one replaces ${\bf Y}_{\textrm{mis},(j)}^{(t)}$ with draws from the implied
    model $({\bf Y}_{\textrm{mis},(j)}^{(t)}|\textbf{Y}_{\textrm{obs},(j)},\{\textbf{Y}_{(k)}^{(t)}:k<j\},\{\textbf{Y}_{(k)}^{(t-1)}:k>j\})$.
    The iterative process continues for $T$ total iterations until convergence, and
    the values at the final iteration make up a completed dataset ${\bf Y}^{(l)}=({\bf
    Y}_{\textrm{obs}},{\bf Y}_{\textrm{mis}}^{(T)})$. The entire process is then repeated
    $L$ times to create the $L$ completed datasets. We provide pseudocode detailing
    each step of the MICE algorithm in the supplementary material.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，MICE算法遵循一个迭代过程，该过程循环通过单变量模型序列。对于每个变量$j$的每次迭代$t$，拟合条件模型$(\textbf{Y}_{(j)}|\textbf{Y}_{\textrm{obs},(j)},\{\textbf{Y}_{(k)}^{(t)}:k<j\},\{\textbf{Y}_{(k)}^{(t-1)}:k>j\})$。接下来，用来自隐含模型$({\bf
    Y}_{\textrm{mis},(j)}^{(t)}|\textbf{Y}_{\textrm{obs},(j)},\{\textbf{Y}_{(k)}^{(t)}:k<j\},\{\textbf{Y}_{(k)}^{(t-1)}:k>j\})$的抽样替换${\bf
    Y}_{\textrm{mis},(j)}^{(t)}$。迭代过程持续$T$次，直到收敛，最终迭代的值构成一个完整的数据集${\bf Y}^{(l)}=({\bf
    Y}_{\textrm{obs}},{\bf Y}_{\textrm{mis}}^{(T)})$。然后整个过程重复$L$次以创建$L$个完整的数据集。我们在补充材料中提供了详细描述MICE算法每一步的伪代码。
- en: Under MICE-CART, the analyst uses CART [[6](#bib.bibx6)] for the univariate
    conditional models in the MICE algorithm. CART follows a decision tree structure
    that uses recursive binary splits to partition the predictor space into distinct
    non-overlapping regions. The top of the tree often represents its root and each
    successive binary split divides the predictor space into two new branches as one
    moves down the tree. The splitting criterion at each leaf is usually chosen to
    minimize an information theoretic entropy measure. Splits that do not decrease
    the lack of fit by an reasonable amount based on a set threshold are pruned off.
    The tree is then built until a stopping criterion is met; e.g., minimum number
    of observations in each leaf.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在MICE-CART中，分析师使用CART [[6](#bib.bibx6)]来处理MICE算法中的单变量条件模型。CART遵循一种决策树结构，使用递归二分裂将预测空间划分为不同的非重叠区域。树的顶部通常代表根节点，每一次二分裂将预测空间划分为两个新的分支，随着树的向下遍历，每个叶子的分裂标准通常是选择以最小化信息理论熵度量的方式。那些基于设定阈值未能合理减少拟合不足的分裂会被剪枝。然后树会继续构建，直到满足停止标准，例如，每个叶子中的最小观察数量。
- en: Once the tree has been fully constructed, one generates ${\bf Y}_{\textrm{mis},(j)}^{(t)}$
    by traversing down the tree to the appropriate leaf using the combinations in
    $(\{\textbf{Y}_{k}^{(t)}:k<j\},\{\textbf{Y}_{k}^{(t-1)}:k>j\})$, and then sampling
    from the $\textbf{Y}_{(j)}^{\textrm{obs}}$ values in that leaf. That is, given
    any combination in $(\{\textbf{Y}_{k}^{(t)}:k<j\},\{\textbf{Y}_{k}^{(t-1)}:k>j\})$,
    one uses the proportion of values of $\textbf{Y}_{j}^{\textrm{obs}}$ in the corresponding
    leaf to approximate the conditional distribution $(\textbf{Y}_{(j)}|\textbf{Y}_{\textrm{obs},(j)},\{\textbf{Y}_{(k)}^{(t)}:k<j\},\{\textbf{Y}_{(k)}^{(t-1)}:k>j\})$.
    The iterative process again continues for $T$ total iterations, and the values
    at the final iteration make up a completed dataset.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦树被完全构建后，通过使用$(\{\textbf{Y}_{k}^{(t)}:k<j\},\{\textbf{Y}_{k}^{(t-1)}:k>j\})$中的组合，沿着树遍历到适当的叶子，从该叶子中的$\textbf{Y}_{(j)}^{\textrm{obs}}$值中进行采样，从而生成${\bf
    Y}_{\textrm{mis},(j)}^{(t)}$。也就是说，给定$(\{\textbf{Y}_{k}^{(t)}:k<j\},\{\textbf{Y}_{k}^{(t-1)}:k>j\})$中的任何组合，使用对应叶子中$\textbf{Y}_{j}^{\textrm{obs}}$值的比例来近似条件分布$(\textbf{Y}_{(j)}|\textbf{Y}_{\textrm{obs},(j)},\{\textbf{Y}_{(k)}^{(t)}:k<j\},\{\textbf{Y}_{(k)}^{(t-1)}:k>j\})$。迭代过程再次持续$T$次，总迭代次数为$T$，最终迭代的值构成了一个完整的数据集。
- en: MICE-RF instead uses random forests for the univariate conditional models in
    MICE [[46](#bib.bibx46), [45](#bib.bibx45), e.g.,]. Random forests [[49](#bib.bibx49),
    [5](#bib.bibx5)] is an ensemble tree method which builds multiple decision trees
    to the data, instead of a single tree like CART. Specifically, random forests
    constructs multiple decision trees using bootstrapped samples of the original,
    and only uses a sample of the predictors for the recursive partitions in each
    tree. This approach can reduce the prevalence of unstable trees as well as the
    correlation among individual trees significantly, since it prevents the same variables
    from dominating the partitioning process across all trees. Theoretically, this
    decorrelation should result in predictions with less variance [[21](#bib.bibx21)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MICE-RF则使用随机森林来处理MICE中的单变量条件模型[[46](#bib.bibx46), [45](#bib.bibx45), 例如]。随机森林[[49](#bib.bibx49),
    [5](#bib.bibx5)]是一种集成树方法，通过构建多个决策树来处理数据，而不是像CART那样使用单一树。具体而言，随机森林使用原始数据的自助样本构建多个决策树，并且在每棵树的递归划分中仅使用一部分预测变量。这种方法可以显著减少不稳定树的出现以及树与树之间的相关性，因为它防止了相同变量在所有树中主导划分过程。从理论上讲，这种去相关性应当会使预测结果的方差减少[[21](#bib.bibx21)]。
- en: For imputation, the analyst first trains a random forests model for each $\textbf{Y}_{(j)}$
    using available cases, given all other variables. Next, the analyst generates
    predictions for ${\bf Y}_{\textrm{mis},j}$ under that model. Specifically, for
    any categorical $\textbf{Y}_{(j)}$, and given any particular combination in $(\{\textbf{Y}_{k}^{(t)}:k<j\},\{\textbf{Y}_{k}^{(t-1)}:k>j\})$,
    the analyst first generates predictions for each tree based on the values $\textbf{Y}_{j}^{\textrm{obs}}$
    in the corresponding leaf for that tree, and then uses the most commonly occurring
    majority level of among all predictions from all the trees. For a continuous $\textbf{Y}_{(j)}$,
    the analyst instead uses the average of all the predictions from all the trees.
    The iterative process again cycles through all the variables, for $T$ total iterations,
    and the values at the final iteration make up a completed dataset. A particularly
    important hyperparameter in random forests is the maximum number of trees $d$.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于插补，分析师首先使用所有其他变量对每个$\textbf{Y}_{(j)}$训练一个随机森林模型。接下来，分析师根据该模型生成${\bf Y}_{\textrm{mis},j}$的预测。具体而言，对于任何类别的$\textbf{Y}_{(j)}$，在给定$(\{\textbf{Y}_{k}^{(t)}:k<j\},\{\textbf{Y}_{k}^{(t-1)}:k>j\})$的特定组合下，分析师首先基于该树对应叶节点中的$\textbf{Y}_{j}^{\textrm{obs}}$的值为每棵树生成预测，然后使用所有树中预测结果中最常见的多数水平。对于连续的$\textbf{Y}_{(j)}$，分析师则使用所有树中预测值的平均值。迭代过程再次遍历所有变量，共进行$T$次迭代，最终迭代的值构成一个完整的数据集。随机森林中的一个特别重要的超参数是最大树数$d$。
- en: For our evaluations, we use the mice R package to implement both MICE-CART and
    MICE-RF, and retain the default hyperparameter setting in the package to mimic
    the common practice in real world applications. Specifically, we set the minimum
    number of observations in each terminal leaf to 5 and the pruning threshold to
    0.0001 in MICE-CART. In MICE-RF, the maximum number of trees $d$ is set to be
    10.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的评估中，我们使用mice R包来实现MICE-CART和MICE-RF，并保留包中的默认超参数设置，以模拟现实世界应用中的常见做法。具体而言，我们在MICE-CART中将每个终端叶节点的最小观察数设置为5，将剪枝阈值设置为0.0001。在MICE-RF中，最大树数$d$设置为10。
- en: 2.2 Generative Adversarial Imputation Network (GAIN)
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 生成对抗插补网络（GAIN）
- en: GAIN [[57](#bib.bibx57)] is an imputation method based on GANs [[18](#bib.bibx18)],
    which consist of a generator function G and a discriminator function D. For any
    data matrix ${\bf Y}=({\bf Y}_{\textrm{obs}},{\bf Y}_{\textrm{mis}})$, we replace
    ${\bf Y}_{\textrm{mis}}$ with random noise, $Z_{ij}$, sampled from a uniform distribution.
    The generator G inputs this initialized data and a mask matrix ${\bf M}$, with
    $M_{ij}\in\{0,1\}$ indicating observed values of ${\bf Y}$, and outputs predicted
    values for both the observed data and missing data, $\hat{\bf Y}$. The discriminator
    D utilizes $\hat{\bf Y}$ = (${\bf Y}_{\textrm{obs}}$, $\hat{\bf Y}_{\textrm{mis}}$)
    and a hint matrix ${\bf H}$ of the same dimension to identify which values are
    observed or imputed by G, which results in a predicted mask matrix $\hat{\bf M}$.
    The hint matrix, sampled from the Bernoulli distribution with $p$ equal to a “hint
    rate” hyperparameter, reveals to D partial information about ${\bf M}$ in order
    to help guide G to learn the underlying distribution of ${\bf Y}$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: GAIN [[57](#bib.bibx57)] 是一种基于GAN的插补方法 [[18](#bib.bibx18)]，它由生成器函数G和判别器函数D组成。对于任何数据矩阵${\bf
    Y}=({\bf Y}_{\textrm{obs}},{\bf Y}_{\textrm{mis}})$，我们用从均匀分布中抽样的随机噪声$Z_{ij}$替换${\bf
    Y}_{\textrm{mis}}$。生成器G将这些初始化的数据和一个掩码矩阵${\bf M}$作为输入，其中$M_{ij}\in\{0,1\}$表示${\bf
    Y}$的观测值，并输出观测数据和缺失数据的预测值$\hat{\bf Y}$。判别器D利用$\hat{\bf Y}$ = (${\bf Y}_{\textrm{obs}}$,
    $\hat{\bf Y}_{\textrm{mis}}$) 和一个同样维度的提示矩阵${\bf H}$来识别哪些值是G观测到的还是插补的，这将导致一个预测的掩码矩阵$\hat{\bf
    M}$。提示矩阵是从伯努利分布中抽样的，其$p$等于“提示率”超参数，向D揭示了关于${\bf M}$的部分信息，以帮助引导G学习${\bf Y}$的潜在分布。
- en: 'We first train D to minimize the loss function, ${L}_{D}({\bf M},\hat{\bf M})$,
    for each mini-batch of size $n_{i}$:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先训练D以最小化损失函数${L}_{D}({\bf M},\hat{\bf M})$，对于每个大小为$n_{i}$的迷你批次：
- en: '|  | ${L}_{D}({\bf M},\hat{\bf M})=\sum_{i=1}^{n_{i}}\sum_{j=1}^{J}M_{ij}\,\text{log}(\hat{M}_{ij})+(1-M_{ij})\,\text{log}(1-\hat{M}_{ij}).$
    |  | (2.1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | ${L}_{D}({\bf M},\hat{\bf M})=\sum_{i=1}^{n_{i}}\sum_{j=1}^{J}M_{ij}\,\text{log}(\hat{M}_{ij})+(1-M_{ij})\,\text{log}(1-\hat{M}_{ij}).$
    |  | (2.1) |'
- en: 'Next, G is trained to minimize the loss function ([2.2](#S2.E2 "In 2.2 Generative
    Adversarial Imputation Network (GAIN) ‣ 2 Missing Data Imputation Methods ‣ Are
    deep learning models superior for missing data imputation in surveys? Evidence
    from an empirical comparison")), which is composed of a generator loss, ${L}_{G}({\bf
    M},\hat{\bf M})$, and a reconstruction loss, ${L}_{M}({\bf Y},\hat{\bf Y},{\bf
    M})$. The generator loss ([2.3](#S2.E3 "In 2.2 Generative Adversarial Imputation
    Network (GAIN) ‣ 2 Missing Data Imputation Methods ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison"))
    is minimized when D incorrectly identifies imputed values as being observed. The
    reconstruction loss ([2.4](#S2.E4 "In 2.2 Generative Adversarial Imputation Network
    (GAIN) ‣ 2 Missing Data Imputation Methods ‣ Are deep learning models superior
    for missing data imputation in surveys? Evidence from an empirical comparison"))
    is minimized when the predicted values are similar to the observed values, and
    is weighted by the hyperparameter $\beta$:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，G被训练以最小化损失函数 ([2.2](#S2.E2 "在2.2生成对抗插补网络(GAIN) ‣ 2 缺失数据插补方法 ‣ 深度学习模型是否在调查中对缺失数据插补更具优势？来自实证比较的证据"))，该函数由生成器损失${L}_{G}({\bf
    M},\hat{\bf M})$和重建损失${L}_{M}({\bf Y},\hat{\bf Y},{\bf M})$组成。生成器损失 ([2.3](#S2.E3
    "在2.2生成对抗插补网络(GAIN) ‣ 2 缺失数据插补方法 ‣ 深度学习模型是否在调查中对缺失数据插补更具优势？来自实证比较的证据")) 在D错误地将插补值识别为观测值时最小化。重建损失
    ([2.4](#S2.E4 "在2.2生成对抗插补网络(GAIN) ‣ 2 缺失数据插补方法 ‣ 深度学习模型是否在调查中对缺失数据插补更具优势？来自实证比较的证据"))
    在预测值与观测值相似时最小化，并由超参数$\beta$加权：
- en: '|  | $\displaystyle{L}({\bf Y},\hat{\bf Y},{\bf M},\hat{\bf M})$ | $\displaystyle={L}_{G}({\bf
    M},\hat{\bf M})+\beta{L}_{M}({\bf Y},\hat{\bf Y},{\bf M}),$ |  | (2.2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{L}({\bf Y},\hat{\bf Y},{\bf M},\hat{\bf M})$ | $\displaystyle={L}_{G}({\bf
    M},\hat{\bf M})+\beta{L}_{M}({\bf Y},\hat{\bf Y},{\bf M}),$ |  | (2.2) |'
- en: '|  | $\displaystyle{L}_{G}({\bf M},\hat{\bf M})$ | $\displaystyle=\sum_{i=1}^{n_{i}}\sum_{j=1}^{J}M_{ij}\,\text{log}(1-\hat{M}_{ij}),$
    |  | (2.3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{L}_{G}({\bf M},\hat{\bf M})$ | $\displaystyle=\sum_{i=1}^{n_{i}}\sum_{j=1}^{J}M_{ij}\,\text{log}(1-\hat{M}_{ij}),$
    |  | (2.3) |'
- en: '|  | $\displaystyle{L}_{M}({\bf Y},\hat{\bf Y},{\bf M})$ | $\displaystyle=\sum_{i=1}^{n_{i}}\sum_{j=1}^{J}(1-M_{ij})\,L_{\textrm{rec}}(Y_{ij},\hat{Y}_{ij}),$
    |  | (2.4) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{L}_{M}({\bf Y},\hat{\bf Y},{\bf M})$ | $\displaystyle=\sum_{i=1}^{n_{i}}\sum_{j=1}^{J}(1-M_{ij})\,L_{\textrm{rec}}(Y_{ij},\hat{Y}_{ij}),$
    |  | (2.4) |'
- en: where
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $L_{\textrm{rec}}(Y_{ij},\hat{Y}_{ij})=\begin{cases}(\hat{Y}_{ij}-Y_{ij})^{2}&amp;\text{if
    $Y_{ij}$ is continuous}\\ -Y_{ij}\,\text{log}\hat{Y}_{ij}&amp;\text{if $Y_{ij}$
    is categorical}.\end{cases}$ |  | (2.5) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\textrm{rec}}(Y_{ij},\hat{Y}_{ij})=\begin{cases}(\hat{Y}_{ij}-Y_{ij})^{2}&amp;\text{如果
    $Y_{ij}$ 是连续的}\\ -Y_{ij}\,\text{log}\hat{Y}_{ij}&amp;\text{如果 $Y_{ij}$ 是类别型的}.\end{cases}$
    |  | (2.5) |'
- en: 'In our experiments, we model both G and D as fully-connected neural networks,
    each with three hidden layers, and $\theta$ hidden units per hidden layer. The
    hidden layer weights are initialized uniformly at random with the Xavier initialization
    method [[16](#bib.bibx16)]. We use leaky ReLU activation function [[33](#bib.bibx33)]
    for each hidden layer, and a softmax activation function for the output layer
    for G in the case of categorical variables, or a sigmoid activation function in
    the case of numerical variables and for the output of D. We facilitate this choice
    of output layer for numerical variables by transforming all continuous variables
    to be within range (0, 1) using the MinMax normalization: $Y_{ij}^{*}=\{Y_{ij}-\text{min}(Y_{\cdot
    j})\}/\{\text{max}(Y_{\cdot j})-\text{min}(Y_{\cdot j})\}$, where $\text{min}(Y_{\cdot
    j})$ and $\text{max}(Y_{\cdot j})$ are the minimum and maximum of variable $j$,
    respectively. After imputation, we transform each value back to its original scale.
    We generate multiple imputations using several runs of the model with varying
    initial imputation of the missing values.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将G和D建模为全连接神经网络，每个网络有三个隐藏层，每个隐藏层有$\theta$个隐藏单元。隐藏层的权重使用Xavier初始化方法[[16](#bib.bibx16)]均匀随机初始化。我们对每个隐藏层使用leaky
    ReLU激活函数[[33](#bib.bibx33)]，对G的输出层（如果是类别变量）使用softmax激活函数，或者在数值变量的情况下以及D的输出中使用sigmoid激活函数。我们通过将所有连续变量转化为(0,
    1)范围内来简化数值变量的输出层选择，使用MinMax归一化：$Y_{ij}^{*}=\{Y_{ij}-\text{min}(Y_{\cdot j})\}/\{\text{max}(Y_{\cdot
    j})-\text{min}(Y_{\cdot j})\}$，其中$\text{min}(Y_{\cdot j})$和$\text{max}(Y_{\cdot
    j})$分别是变量$j$的最小值和最大值。插补后，我们将每个值转换回其原始尺度。我们通过多次运行模型并对缺失值进行不同的初始插补来生成多个插补。
- en: To implement GAIN in our evaluations, we use the same architecture as the one
    in [[57](#bib.bibx57)]. We set $\beta=100$, $\theta$ equal to the number of features
    of the input data, and tune the hint rate on a single simulation. Following the
    common practice in the GAN literature [[4](#bib.bibx4), [19](#bib.bibx19)], we
    track the evolution of GAIN’s generator and discriminator losses, and manually
    tune the hint rate so that the two losses are qualitatively similar. Specifically,
    we first coarsely select the hint rate among {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
    0.8, 0.9 }. Then we determine the final value by an additional fine tuning step.
    In the MAR scenario, for example, after observing that the optimal value is in
    the range (0.1, 0.2), we perform a search among { 0.11, 0.12, 0.13, 0.14, 0.15,
    0.16, 0.17, 0.18, 0.19 }. Finally, we set the optimal hint rate for MCAR and MAR
    scenarios to be 0.3 and 0.13, respectively. We train the networks for 200 epochs
    using stochastic gradient descent (SGD) and mini-batches of size 512 to learn
    the parameter weights. We use the Adam optimizer to adapt the learning rate, with
    an initial rate of 0.001 [[26](#bib.bibx26)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的评估中实施GAIN，我们使用与[[57](#bib.bibx57)]中相同的架构。我们设置$\beta=100$，$\theta$等于输入数据的特征数量，并在单次模拟中调整提示率。按照GAN文献中的常见做法[[4](#bib.bibx4),
    [19](#bib.bibx19)]，我们跟踪GAIN生成器和判别器的损失演变，并手动调整提示率，使得两者的损失在质上相似。具体来说，我们首先在{0.1,
    0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}中粗略选择提示率。然后通过额外的微调步骤确定最终值。例如，在MAR情景下，观察到最佳值在(0.1,
    0.2)范围内后，我们在{0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19}中进行搜索。最后，我们将MCAR和MAR情景下的最佳提示率分别设置为0.3和0.13。我们使用随机梯度下降（SGD）和512大小的迷你批次训练网络200个周期，以学习参数权重。我们使用Adam优化器来调整学习率，初始学习率为0.001[[26](#bib.bibx26)]。
- en: 2.3 Multiple Imputation using Denoising Autoencoders (MIDA)
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 使用去噪自编码器的多重插补（MIDA）
- en: MIDA [[17](#bib.bibx17), [32](#bib.bibx32)] extends a class of neural networks,
    denoising autoencoders, for MI. An autoencoder is a neural network model trained
    to learn the identity function of the input data. Denoising autoencoders intentionally
    corrupt the input data in order to prevent the networks from learning the identity
    function, but rather a useful low-dimensional representation of the input data.
    The MIDA architecture consists of an encoder and decoder, each modeled as a fully-connected
    neural network with three hidden layers, with $\theta$ hidden units per hidden
    layer. We first perform an initial imputation on missing values using the mean
    for continuous variables and the most frequent label for categorical variables,
    which results in a completed data ${\bf Y}_{0}$. The encoder inputs ${\bf Y}_{0}$,
    and corrupts the input data by randomly dropping out half of the variables. The
    corrupted input data is mapped to a higher dimensional representation by adding
    $\Theta$ hidden units to each successive hidden layer of the encoder. The decoder
    receives output from the encoder, and symmetrically scales the encoding back to
    the original input dimension. All hidden layers use a hyperbolic tangent (tanh)
    activation function, while the output layer of the decoder uses a softmax (sigmoid)
    activation function in the case of categorical (numerical) variables. Multiple
    imputations are generated by using multiple runs with the hidden layer weights
    initialized as a Gaussian random variable.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: MIDA [[17](#bib.bibx17), [32](#bib.bibx32)] 扩展了一类神经网络，即去噪自编码器，用于 MI。自编码器是一个神经网络模型，训练目的是学习输入数据的恒等函数。去噪自编码器故意破坏输入数据，以防止网络学习恒等函数，而是学习输入数据的有用低维表示。MIDA
    架构包括一个编码器和解码器，每个都是一个具有三个隐藏层的全连接神经网络，每层有 $\theta$ 个隐藏单元。我们首先对缺失值进行初步插补，对于连续变量使用均值，对于分类变量使用最频繁的标签，这样得到一个完成的数据
    ${\bf Y}_{0}$。编码器接收 ${\bf Y}_{0}$ 作为输入，并通过随机丢弃一半的变量来破坏输入数据。破坏后的输入数据通过在编码器的每个连续隐藏层中添加
    $\Theta$ 个隐藏单元来映射到更高维的表示。解码器接收编码器的输出，并将编码重新缩放回原始输入维度。所有隐藏层使用双曲正切（tanh）激活函数，而解码器的输出层在处理分类（数值）变量时使用
    softmax（sigmoid）激活函数。通过使用多个运行生成多个插补，每次运行的隐藏层权重初始化为高斯随机变量。
- en: 'Following [[32](#bib.bibx32)], we train MIDA in two phases: a primary phase
    and fine-tuning phase. In the primary phase, we feed the initially imputed data
    to MIDA and train for $N_{\textrm{prime}}$ epochs. In the fine-tuning phase, MIDA
    is trained for $N_{\textrm{tune}}$ epochs on the output in the primary phase,
    and produces the outcome. The loss function is used in both phases and closely
    resembles the reconstruction loss in GAIN:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 [[32](#bib.bibx32)] 的方法，我们在两个阶段训练 MIDA：主要阶段和微调阶段。在主要阶段，我们将最初插补的数据输入 MIDA，并训练
    $N_{\textrm{prime}}$ 个周期。在微调阶段，MIDA 对主要阶段的输出进行 $N_{\textrm{tune}}$ 个周期的训练，并生成结果。损失函数在两个阶段中使用，并且与
    GAIN 中的重建损失非常相似。
- en: '|  | $L({Y}_{{ij}_{0}},\hat{Y}_{ij},M_{ij})=\begin{cases}(1-M_{ij})({Y}_{{ij}_{0}}-\hat{Y}_{ij})^{2}&amp;\text{if
    $Y_{ij}$ is continuous}\\ -(1-M_{ij}){Y}_{{ij}_{0}}\,\text{log}\hat{\bf Y}_{{ij}}&amp;\text{if
    $Y_{ij}$ is categorical}.\end{cases}$ |  | (2.6) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $L({Y}_{{ij}_{0}},\hat{Y}_{ij},M_{ij})=\begin{cases}(1-M_{ij})({Y}_{{ij}_{0}}-\hat{Y}_{ij})^{2}&amp;\text{如果
    $Y_{ij}$ 是连续的}\\ -(1-M_{ij}){Y}_{{ij}_{0}}\,\text{log}\hat{\bf Y}_{{ij}}&amp;\text{如果
    $Y_{ij}$ 是分类的}.\end{cases}$ |  | (2.6) |'
- en: To implement MIDA in our evaluations, we use the same architecture and tune
    the hyperparameters in a single simulation as in [[32](#bib.bibx32)]. We plot
    the evolution of loss function $L$, and select the number of additional units
    $\Theta$ among {1, 2, 3, 4, 5, 6, 7 ,8, 9, 10 } to reduce the loss. In our experiments,
    we set $\theta$ equal to the number of features of the input data and add $\Theta=7$
    hidden units to each of the three hidden layers of the encoder. We train the model
    for $N_{\textrm{prime}}=100$ epochs in the primary phase and $N_{\textrm{tune}}=2$
    epochs in the fine-tuning phase. Similar as in GAIN, we learn the model parameters
    using SGD with mini-batches of size 512, and use the Adam optimizer to adapt the
    learning rate with the initial rate being 0.001.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的评估中实现 MIDA，我们使用与 [[32](#bib.bibx32)] 相同的架构，并在单次模拟中调整超参数。我们绘制损失函数 $L$ 的演变图，并从
    {1, 2, 3, 4, 5, 6, 7 ,8, 9, 10 } 中选择附加单元的数量 $\Theta$ 以减少损失。在我们的实验中，我们将 $\theta$
    设置为输入数据的特征数量，并向编码器的三个隐藏层中的每个层添加 $\Theta=7$ 个隐藏单元。我们在主要阶段训练模型 $N_{\textrm{prime}}=100$
    个周期，在微调阶段训练 $N_{\textrm{tune}}=2$ 个周期。与 GAIN 类似，我们使用 mini-batch 大小为 512 的 SGD
    来学习模型参数，并使用 Adam 优化器来调整学习率，初始学习率为 0.001。
- en: 3 Simulation-based evaluation of imputation methods
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于模拟的插补方法评估
- en: Methods for missing data imputation are usually evaluated via real-data based
    simulations [[50](#bib.bibx50)]. Namely, one creates missing values from a complete
    dataset according to a missing data mechanism [[30](#bib.bibx30)], imputes the
    missing values by a specific method, and then compares these imputed values with
    the original “true” values based on some metrics.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据插补方法通常通过基于真实数据的模拟进行评估[[50](#bib.bibx50)]。具体而言，从完整数据集中创建缺失值，根据缺失数据机制[[30](#bib.bibx30)]插补这些缺失值，然后根据一些指标将这些插补值与原始的“真实”值进行比较。
- en: We first give a quick review of Rubin’s MI combination rules. Let $Q$ be the
    target estimand in the population, and $q^{(l)}$ and $u^{(l)}$ be the point and
    variance estimate of $Q$ based on the $l$th imputed dataset, respectively. The
    MI point estimate of $Q$ is $\bar{q}_{L}=\sum_{l=1}^{L}q^{(l)}/L$, and the corresponding
    estimate of the variance is equal to $T_{L}=(1+1/L)b_{L}+\bar{u}_{L}$, where $b_{L}=\sum_{l=1}^{L}(q^{(l)}-\bar{q}_{L})^{2}/(L-1)$,
    and $\bar{u}_{L}=\sum_{l=1}^{L}u^{(l)}/L$. The confidence interval of $Q$ is constructed
    using $(\bar{q}_{L}-Q)\sim t_{\nu}(0,T_{L})$, where $t_{v}$ is a $t$-distribution
    with $\nu=(L-1)(1+\bar{u}_{L}/[(1+1/L)b_{L}])^{2}$ degrees of freedom.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先简要回顾 Rubin 的 MI 组合规则。设 $Q$ 为总体中的目标估计量，$q^{(l)}$ 和 $u^{(l)}$ 分别为基于第 $l$ 个插补数据集的
    $Q$ 的点估计和方差估计。$Q$ 的 MI 点估计为 $\bar{q}_{L}=\sum_{l=1}^{L}q^{(l)}/L$，相应的方差估计为 $T_{L}=(1+1/L)b_{L}+\bar{u}_{L}$，其中
    $b_{L}=\sum_{l=1}^{L}(q^{(l)}-\bar{q}_{L})^{2}/(L-1)$，$\bar{u}_{L}=\sum_{l=1}^{L}u^{(l)}/L$。$Q$
    的置信区间使用 $(\bar{q}_{L}-Q)\sim t_{\nu}(0,T_{L})$ 构建，其中 $t_{v}$ 是具有 $\nu=(L-1)(1+\bar{u}_{L}/[(1+1/L)b_{L}])^{2}$
    自由度的 $t$-分布。
- en: The first step in our simulation-based evaluation procedure is choosing a dataset
    with all values observed, which is taken as the “population.” We then choose a
    set of target estimands $Q$ and compute their values from this population data,
    which are taken as the “ground truth.” The estimands are usually summary statistics
    of the variables or parameters in a down-stream analysis model, e.g., a coefficient
    in a regression model [[48](#bib.bibx48), [25](#bib.bibx25)]. Second, we randomly
    draw without replacement $H$ samples of size $n$ from the population data, and
    in each of sample ($h=1,...,H$) create missing data according to a specific missing
    data mechanism and pre-fixed proportion of missingness. Third, for each simulated
    sample with missing data, we create $L$ imputed datasets using the imputation
    method under consideration and construct the point and interval estimate of each
    estimand using Rubin’s rules. Lastly, we compute performance metrics of each estimand
    from the quantities obtained in the previous step.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于模拟的评估程序的第一步是选择一个所有值都被观测的数据集，该数据集被视为“总体”。然后，我们选择一组目标估计量 $Q$，并从该总体数据中计算它们的值，这些值被视为“真实值”。这些估计量通常是下游分析模型中变量或参数的汇总统计量，例如回归模型中的系数[[48](#bib.bibx48),
    [25](#bib.bibx25)]。其次，我们从总体数据中随机抽取 $H$ 个样本，每个样本的大小为 $n$，在每个样本（$h=1,...,H$）中根据特定的缺失数据机制和预设的缺失比例创建缺失数据。第三，对于每个具有缺失数据的模拟样本，我们使用考虑中的插补方法创建
    $L$ 个插补数据集，并使用 Rubin 规则构建每个估计量的点估计和区间估计。最后，我们根据前一步获得的量计算每个估计量的性能指标。
- en: In the empirical application, we select a large complete subsample from the
    American Community Survey (ACS) — a national survey that bears the hallmarks of
    many big survey data — as our population. Since discrete variables are prevalent
    in the ACS, as well as in most survey data, we focus on the marginal probabilities
    of binary and categorical variables; e.g., a categorical variable with $K$ categories
    has $K-1$ estimands. To evaluate how well the imputation methods preserve the
    multivariate distributional properties, similar to [[1](#bib.bibx1)], we also
    consider the bivariate probabilities of all two-way combinations of categories
    in binary and categorical variables. Another useful metric is the finite-sample
    pairwise correlations between continuous variables. For continuous variables,
    the common estimands are mean, median or variance. To facilitate meaningful comparisons
    of the results between the categorical and continuous variables, we propose to
    discretize each continuous variable into $K$ categories based on the sample quantiles.
    We then evaluate these binned continuous variables as categorical variables based
    on the aforementioned estimands of marginal and bivariate probabilities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们从美国社区调查（ACS）中选择了一个大规模完整的子样本——一个具有许多大型调查数据特征的国家调查——作为我们的总体。由于离散变量在ACS以及大多数调查数据中普遍存在，我们关注二元和分类变量的边际概率；例如，具有$K$个类别的分类变量有$K-1$个估计量。为了评估插补方法在保留多变量分布特性方面的效果，类似于[[1](#bib.bibx1)]，我们还考虑了二元和分类变量中所有两两组合的联合概率。另一个有用的指标是连续变量之间的有限样本配对相关性。对于连续变量，常见的估计量包括均值、中位数或方差。为了方便对分类变量和连续变量结果的有意义比较，我们建议将每个连续变量根据样本分位数离散化为$K$个类别。然后，我们根据上述边际和二元概率的估计量将这些离散化的连续变量作为分类变量进行评估。
- en: 'For each estimand $Q$, we consider three metrics. The first metric focuses
    on bias. To accommodate close-to-zero estimands that are prevalent in probabilities
    of categorical variables, we consider the absolute standardized bias (ASB) of
    each estimand $Q$:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个估计量$Q$，我们考虑三种指标。第一个指标关注偏差。为了适应在分类变量的概率中普遍存在的接近零的估计量，我们考虑每个估计量$Q$的绝对标准化偏差（ASB）：
- en: '|  | $\text{ASB}={\sum_{h=1}^{H}&#124;\bar{q}^{(h)}_{L}-Q}&#124;/{(H\cdot Q)},$
    |  | (3.1) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{ASB}={\sum_{h=1}^{H}|\bar{q}^{(h)}_{L}-Q|/(H\cdot Q)},$ |  | (3.1)
    |'
- en: where $\bar{q}^{(h)}_{L}$ is the MI point estimate of $Q$ in simulation $h$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bar{q}^{(h)}_{L}$是模拟$h$中$Q$的MI点估计。
- en: 'The second metric is the relative mean squared error (Rel.MSE), which is the
    ratio between the MSE of estimating $Q$ from the imputed data and that from the
    sampled data before introducing the missing data:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个指标是相对均方误差（Rel.MSE），即从插补数据估计$Q$的MSE与在引入缺失数据之前从采样数据中估计$Q$的MSE之间的比率：
- en: '|  | $\text{Rel.MSE}=\frac{\sum_{h=1}^{H}(\bar{q}^{(h)}_{L}-Q)^{2}}{\sum_{h=1}^{H}(\widetilde{Q}^{(h)}-Q)^{2}},$
    |  | (3.2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Rel.MSE}=\frac{\sum_{h=1}^{H}(\bar{q}^{(h)}_{L}-Q)^{2}}{\sum_{h=1}^{H}(\widetilde{Q}^{(h)}-Q)^{2}},$
    |  | (3.2) |'
- en: where $\bar{q}^{(h)}_{L}$ is defined earlier, and $\widetilde{Q}^{(h)}$ is the
    prototype estimator of $Q$, i.e. the point estimate from the complete sampled
    data in simulation $h$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bar{q}^{(h)}_{L}$如前所述定义，$\widetilde{Q}^{(h)}$是$Q$的原型估计，即来自模拟$h$中完整采样数据的点估计。
- en: 'The third metric is coverage rate, which is the proportion of the $\alpha\%$
    (e.g. 95%) confidence intervals, denoted by $\mbox{CI}_{h}^{\alpha}$ ($h=1,...,H$),
    in the $H$ simulations that contain the true $Q$:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个指标是覆盖率，即$H$次模拟中$\alpha\%$（例如95%）置信区间$\mbox{CI}_{h}^{\alpha}$（$h=1,...,H$）包含真实$Q$的比例：
- en: '|  | $\text{Coverage}=\sum_{h=1}^{H}\mathbf{1}\{Q\in\mbox{CI}_{h}^{\alpha}\}/H.$
    |  | (3.3) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Coverage}=\sum_{h=1}^{H}\mathbf{1}\{Q\in\mbox{CI}_{h}^{\alpha}\}/H.$
    |  | (3.3) |'
- en: 'We recommend conducting a large number of simulations (e.g. $H\geq 100$) to
    obtain reliable estimates of MSE and coverage. This would not be a problem for
    deep learning algorithms, which can be typically completed in seconds even with
    large sample sizes. However, it can be computationally prohibitive for the MICE
    algorithms when each of the simulated data is large (e.g. $n=100,000$ in some
    of our simulations). In the situation that one has to rely on only a few or even
    a single simulation for evaluation, we propose a modified metric of bias. Specifically,
    for each categorical variable or binned continuous variable $j$ , we define the
    weighted absolute bias (WAB) as the sum of the absolute bias weighted by the true
    marginal probability in each category:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议进行大量模拟（例如 $H\geq 100$），以获得可靠的均方误差（MSE）和覆盖率估计。这对深度学习算法来说不会成为问题，因为即使在大样本量的情况下，这些算法通常也能在几秒钟内完成。然而，对于MICE算法来说，当每个模拟数据量较大（例如在我们的某些模拟中
    $n=100,000$）时，这可能会导致计算上的困难。在只能依赖少数几个或甚至单个模拟进行评估的情况下，我们提出了一种修订的偏差度量。具体来说，对于每个分类变量或分箱的连续变量
    $j$，我们将加权绝对偏差（WAB）定义为按每个类别的真实边际概率加权的绝对偏差之和：
- en: '|  | $\text{Weighted absolute bias}=\sum_{k=1}^{K}Q_{jk}\left&#124;\bar{q}^{(h)}_{jk}-Q_{jk}\right&#124;,$
    |  | (3.4) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{加权绝对偏差}=\sum_{k=1}^{K}Q_{jk}\left\|\bar{q}^{(h)}_{jk}-Q_{jk}\right\|,$
    |  | (3.4) |'
- en: where $K$ is the total number of categories, $Q_{jk}$ is the population marginal
    probability of category $k$ in variable $j$, and $\bar{q}^{(h)}_{jk}$ is its corresponding
    point estimate in simulation $h$. We can also average the weighted absolute bias
    over a number of repeatedly simulated samples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $K$ 是类别的总数，$Q_{jk}$ 是变量 $j$ 中类别 $k$ 的总体边际概率，而 $\bar{q}^{(h)}_{jk}$ 是在模拟 $h$
    中的相应点估计。我们还可以对重复模拟的多个样本进行加权绝对偏差的平均。
- en: The above procedure and metrics differ from the common practice in the machine
    learning literature. For example, many machine learning papers on missing data
    imputation conduct simulations on benchmark datasets, but these data often have
    vastly different structure and features from survey data and thus are less informative
    for the goal of this paper. One such dataset is the Breast Cancer dataset in the
    UCI Machine Learning Repository [[13](#bib.bibx13)], which has only 569 sample
    units and no categorical variables. Also, these simulations are usually based
    on randomly creating missing values of a single dataset repeatedly rather than
    on drawing repeated samples from a population, and thus fails to account for the
    sampling mechanism. Moreover, these evaluations often use metrics focusing on
    accuracy of individual predictions rather than distributional features. Specifically,
    the most commonly used metrics are the root mean squared error (RMSE) and accuracy
    [[17](#bib.bibx17), [57](#bib.bibx57), [32](#bib.bibx32)]. Both metrics can be
    defined in an overall or variable-specific fashion, but the machine learning literature
    usually focuses on the overall version. The overall RMSE is defined as
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上述程序和度量方法与机器学习文献中的常见做法有所不同。例如，许多关于缺失数据插补的机器学习论文在基准数据集上进行模拟，但这些数据往往与调查数据的结构和特征差异较大，因此对本文的目标信息量较少。一个这样的数据集是UCI机器学习库中的乳腺癌数据集[[13](#bib.bibx13)]，该数据集仅有569个样本单位，并且没有分类变量。此外，这些模拟通常是基于随机创建单一数据集的缺失值，而不是从总体中提取重复样本，因此未能考虑采样机制。此外，这些评估往往使用专注于个体预测准确性的度量指标，而不是分布特征。具体来说，最常用的度量指标是均方根误差（RMSE）和准确率[[17](#bib.bibx17),
    [57](#bib.bibx57), [32](#bib.bibx32)]。这两个指标可以以总体或变量特定的方式定义，但机器学习文献通常关注总体版本。总体RMSE定义为
- en: '|  | $\text{RMSE}=\sqrt{\frac{\sum^{n}_{i=1}\sum_{j}M_{ij}(\hat{Y}_{ij}-Y_{ij})^{2}}{\sum^{n}_{i=1}\sum_{j}M_{ij}}},$
    |  | (3.5) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{均方根误差}=\sqrt{\frac{\sum^{n}_{i=1}\sum_{j}M_{ij}(\hat{Y}_{ij}-Y_{ij})^{2}}{\sum^{n}_{i=1}\sum_{j}M_{ij}}},$
    |  | (3.5) |'
- en: 'where $Y_{ij}$ is the value of continuous variable $j$ for individual $i$ in
    the complete data before introducing missing data, and $\hat{Y}_{ij}$ is the corresponding
    imputed value. For non-missing values (i.e. $M_{ij}=1$), $Y_{ij}=\hat{Y}_{ij}$.
    The (overall) accuracy is defined for categorical variables, namely it is the
    proportion of the imputed values being equal to the corresponding original “true”
    value:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Y_{ij}$ 是在引入缺失数据之前的完整数据中个体 $i$ 对于连续变量 $j$ 的值，$\hat{Y}_{ij}$ 是相应的插补值。对于非缺失值（即
    $M_{ij}=1$），$Y_{ij}=\hat{Y}_{ij}$。对于分类变量，总体准确率定义为，即插补值等于相应的原始“真实”值的比例：
- en: '|  | $\text{Accuracy}=\frac{\sum^{n}_{i=1}\sum_{j\in S_{cat}}M_{ij}\mathbbm{1}(\hat{Y}_{ij}=Y_{ij})}{\sum^{n}_{i=1}\sum_{j\in
    S_{cat}}M_{ij}},$ |  | (3.6) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Accuracy}=\frac{\sum^{n}_{i=1}\sum_{j\in S_{cat}}M_{ij}\mathbbm{1}(\hat{Y}_{ij}=Y_{ij})}{\sum^{n}_{i=1}\sum_{j\in
    S_{cat}}M_{ij}},$ |  | (3.6) |'
- en: where $S_{cat}$ is the set of categorical variables.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$S_{cat}$ 是分类变量的集合。
- en: A number of caveats are in order for the RMSE and accuracy metrics. First, they
    are usually computed on a single imputed sample as an overall measure of an imputation
    method, but this ignores the uncertainty of imputations. Second, both RMSE and
    accuracy are single value summaries and do not capture the multivariate distributional
    feature of data. Third, RMSE does not adjust for the different scale of variables
    and can be be easily dominated by a few outliers; also, it is often computed without
    differentiating between continuous and categorical variables. Lastly, when there
    are multiple ($L$) imputed data, a common way is to use the mean of the $L$ imputed
    value as $\hat{Y}_{ij}$ in ([3.5](#S3.E5 "In 3 Simulation-based evaluation of
    imputation methods ‣ Are deep learning models superior for missing data imputation
    in surveys? Evidence from an empirical comparison")), but the statistical meaning
    of the resulting metrics is opaque. This is particularly problematic for categorical
    variables. For these reasons, we warn against using the overall RMSE and accuracy
    as the only metrics for comparing imputation methods, and one should exercise
    caution when interpreting them.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RMSE和准确率指标，有一些注意事项。首先，它们通常在单个填补样本上计算，作为填补方法的总体度量，但这忽略了填补的不确定性。其次，RMSE和准确率都是单一值总结，无法捕捉数据的多元分布特征。第三，RMSE没有调整变量的不同尺度，容易被几个异常值主导；此外，通常在计算时不区分连续变量和分类变量。最后，当有多个($L$)填补数据时，常见的做法是将$L$个填补值的均值作为$\hat{Y}_{ij}$在([3.5](#S3.E5
    "In 3 Simulation-based evaluation of imputation methods ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison"))中使用，但结果指标的统计意义不明。这对于分类变量尤其是问题。因此，我们警告不要仅使用整体RMSE和准确率作为比较填补方法的唯一指标，并且在解释这些指标时应谨慎。
- en: 4 Evaluation based on ACS
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于ACS的评估
- en: In this section, we evaluate the four imputation methods described in Section
    [2](#S2 "2 Missing Data Imputation Methods ‣ Are deep learning models superior
    for missing data imputation in surveys? Evidence from an empirical comparison")
    following the procedure and metrics described in Section [3](#S3 "3 Simulation-based
    evaluation of imputation methods ‣ Are deep learning models superior for missing
    data imputation in surveys? Evidence from an empirical comparison"). For simplicity,
    in the following discussions we use CART and RF to denote MICE-CART and MICE-RF,
    respectively.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了第[2](#S2 "2 Missing Data Imputation Methods ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison")节中描述的四种填补方法，按照第[3](#S3
    "3 Simulation-based evaluation of imputation methods ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison")节中描述的程序和指标进行评估。为简便起见，在以下讨论中，我们用CART和RF来代指MICE-CART和MICE-RF。
- en: 4.1 The “population” data
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 “人口”数据
- en: We use the one-year Public Use Microdata Sample from the 2018 ACS to construct
    our population. The 2018 ACS data contains both household-level variables — for
    example, whether or not a house is owned or rented — and individual-level variables
    — for example, age, income and sex of the individuals within each household. Since
    individuals nested within a household are often dependent, and the imputation
    methods we evaluate generally assume independence across all observations, we
    set our unit of observation at the household-level, where independence is more
    likely to hold. We first remove units corresponding to vacant houses. Next, we
    delete units with any missing values, so that we only keep the complete cases.
    Within each household, we also retain individual-level data corresponding only
    to the household head and merge them with the household-level variables, resulting
    in a rich set of variables with potentially complex joint relationships.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用2018年ACS的为公众使用的微观数据样本来构建我们的人口数据。2018年ACS数据包含了家庭层面的变量——例如，房屋是否拥有或租赁——以及个人层面的变量——例如，家庭中每个人的年龄、收入和性别。由于个体在家庭内部通常是相互依赖的，而我们评估的填补方法通常假设所有观察值之间是独立的，因此我们将观察单位设置为家庭层面，在这里独立性更有可能成立。我们首先移除对应于空置房屋的单位。接下来，我们删除任何有缺失值的单位，只保留完整案例。在每个家庭内，我们还保留仅对应于家庭主人的个人数据，并将其与家庭层面的变量合并，形成一个具有潜在复杂联合关系的丰富变量集。
- en: It is often challenging to generate plausible imputations for ordinal variables
    with many levels when there is very low mass at the highest levels, as is the
    case for some variables in the ACS data. Following [[28](#bib.bibx28)], we treat
    ordinal variables with more than 10 levels as continuous variables. We also follow
    the approach in [[1](#bib.bibx1)] to exclude binary variables where the marginal
    probabilities violate $np>10$ or $n(1-p)>10$; this eliminates estimands where
    the central limit theorem is not likely to hold. For each categorical variable
    with more than two levels but less than 10 levels where this might also be a problem,
    we merge the levels with a small number of observations in the population data.
    For example, for the household language variable, we recode the levels from five
    to three (English, Spanish, and other), because the probability of speaking neither
    English nor Spanish in the full population is less than 8.8%.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当序数变量的最高水平几乎没有质量时，生成合理的填补数据通常具有挑战性，这在ACS数据中的一些变量中尤为如此。按照[[28](#bib.bibx28)]，我们将具有超过10个水平的序数变量视为连续变量。我们还遵循[[1](#bib.bibx1)]的方法，排除边际概率违反$np>10$或$n(1-p)>10$的二元变量；这排除了中央极限定理可能不适用的估计量。对于每个有超过两个水平但少于10个水平的类别变量，如果这也可能是一个问题，我们会合并人口数据中观察数较少的水平。例如，对于家庭语言变量，我们将水平从五个重新编码为三个（英语、西班牙语和其他），因为在总体中既不讲英语也不讲西班牙语的概率小于8.8%。
- en: The final population data contains 1,257,501 units, with 18 binary variables,
    20 categorical variables with 3 to 9 levels, and 8 continuous variables. We describe
    the variables in more detail in the supplementary material. We compute the population
    values of the estimands $Q$ described in Section [3](#S3 "3 Simulation-based evaluation
    of imputation methods ‣ Are deep learning models superior for missing data imputation
    in surveys? Evidence from an empirical comparison"), including all marginal and
    bivariate probabilities of discrete and binned continuous variables. We vary the
    size of the simulated samples from 10,000 to 100,000, and simulate missing data
    according to either missing completely at random (MCAR) or missing at random (MAR)
    mechanisms in each of these scenarios.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的总体数据包含1,257,501个单元，包含18个二元变量、20个类别变量（每个变量有3到9个水平）以及8个连续变量。我们在补充材料中对这些变量进行了更详细的描述。我们计算了第[3](#S3
    "3 Simulation-based evaluation of imputation methods ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison")节中描述的估计量$Q$的总体值，包括所有离散变量和分箱连续变量的边际和双变量概率。我们将模拟样本的大小从10,000变化到100,000，并在这些情况下按照完全随机缺失（MCAR）或随机缺失（MAR）机制模拟缺失数据。
- en: 4.2 Simulations with n=10,000
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 样本量为$n=10,000$的模拟
- en: We first randomly draw $H=100$ samples of size $n=10,000$, and set $30\%$ of
    each sample to be missing under either MCAR or MAR. CART or RF takes around 2.8
    and 9.2 hours, respectively, to create $L=10$ imputed datasets with default parameters
    on a standard desktop computer with a single central processing unit (CPU). The
    deep learning methods are much faster because they leverage GPU computing power
    when implemented on the GPU-enabled TensorFlow software framework [[35](#bib.bibx35)].
    GAIN takes roughly 1.5 minutes and MIDA takes roughly 4 minutes to create $L=10$
    completed datasets using a GeForce GTX 1660 Ti GPU. Note that it is infeasible
    to manually tune the hyperparameter in each of the 100 simulations in each scenario
    for the deep learning models. So for each scenario, we have randomly selected
    one simulation, and tune the hyperparameters using the procedure described in
    Section [2](#S2 "2 Missing Data Imputation Methods ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison").
    We then apply these selected hyperparameters to all simulations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先随机抽取$H=100$个样本，每个样本的大小为$n=10,000$，并将每个样本的$30\%$设置为缺失值，缺失机制为MCAR或MAR。CART或RF在默认参数下分别需要大约2.8小时和9.2小时，在配备单个中央处理单元（CPU）的标准桌面计算机上生成$L=10$个填补数据集。深度学习方法则要快得多，因为它们在实现于支持GPU的TensorFlow软件框架时可以利用GPU计算能力[[35](#bib.bibx35)]。使用GeForce
    GTX 1660 Ti GPU，GAIN大约需要1.5分钟，而MIDA大约需要4分钟来生成$L=10$个完整的数据集。请注意，在每种情况下对100个模拟中的每一个进行超参数的手动调整是不可行的。因此，对于每种情况，我们随机选择了一个模拟，并使用第[2](#S2
    "2 Missing Data Imputation Methods ‣ Are deep learning models superior for missing
    data imputation in surveys? Evidence from an empirical comparison")节中描述的程序调整超参数。然后，我们将这些选定的超参数应用于所有模拟。
- en: 4.2.1 MCAR scenario
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 MCAR场景
- en: To create the MCAR scenario, we randomly set 30% of the values of each variable
    to be missing independently. Table [4.1](#S4.T1 "Table 4.1 ‣ 4.2.1 MCAR scenario
    ‣ 4.2 Simulations with n=10,000 ‣ 4 Evaluation based on ACS ‣ Are deep learning
    models superior for missing data imputation in surveys? Evidence from an empirical
    comparison") displays the distributions of the estimated ASB and relative MSE
    of all the marginal and bivariate probabilities in the imputed data by the four
    imputation methods.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建MCAR场景，我们随机设置每个变量的30%值为缺失值。表格[4.1](#S4.T1 "表格 4.1 ‣ 4.2.1 MCAR场景 ‣ 4.2 模拟（n=10,000）
    ‣ 4 基于ACS的评估 ‣ 深度学习模型在调查缺失数据插补中的优越性？来自实证比较的证据")展示了四种插补方法在插补数据中估计的ASB和相对均方误差的分布情况。
- en: 'Table 4.1: Distributions of absolute standardized bias $(\times 100)$ and relative
    mean squared error of all marginal and bivariate probabilities based on the imputations
    by the four MI methods, when $n=10,000$ and 30% values MCAR. “Cat.” means categorical
    variables and “B.Cont.” means binned continuous variables.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4.1：在$n=10,000$且30%值为MCAR时，四种MI方法的插补结果中所有边际和双变量概率的绝对标准化偏差$(\times 100)$和相对均方误差的分布情况。
    “Cat.”表示分类变量，“B.Cont.”表示分箱连续变量。
- en: '| Quantiles | Marginal |  | Bivariate |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 分位数 | 边际 |  | 双变量 |'
- en: '| CART | RF | GAIN | MIDA |  | CART | RF | GAIN | MIDA |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| CART | RF | GAIN | MIDA |  | CART | RF | GAIN | MIDA |'
- en: '| ASB $(\times 100)$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ASB $(\times 100)$ |'
- en: '|  | 10% | 0.05 | 0.47 | 0.76 | 0.98 |  | 0.15 | 1.14 | 1.21 | 1.54 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | 10% | 0.05 | 0.47 | 0.76 | 0.98 |  | 0.15 | 1.14 | 1.21 | 1.54 |'
- en: '|  | 25% | 0.13 | 1.25 | 1.48 | 2.22 |  | 0.40 | 2.83 | 3.08 | 3.93 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 25% | 0.13 | 1.25 | 1.48 | 2.22 |  | 0.40 | 2.83 | 3.08 | 3.93 |'
- en: '| Cat. | 50% | 0.27 | 2.80 | 3.22 | 4.69 |  | 1.05 | 6.74 | 7.14 | 8.47 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Cat. | 50% | 0.27 | 2.80 | 3.22 | 4.69 |  | 1.05 | 6.74 | 7.14 | 8.47 |'
- en: '|  | 75% | 0.64 | 5.86 | 7.18 | 8.86 |  | 2.51 | 13.59 | 17.03 | 15.23 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 75% | 0.64 | 5.86 | 7.18 | 8.86 |  | 2.51 | 13.59 | 17.03 | 15.23 |'
- en: '|  | 90% | 1.14 | 10.01 | 19.55 | 14.41 |  | 5.34 | 22.33 | 26.92 | 21.90 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 90% | 1.14 | 10.01 | 19.55 | 14.41 |  | 5.34 | 22.33 | 26.92 | 21.90 |'
- en: '|  | 10% | 0.06 | 0.24 | 7.25 | 2.73 |  | 0.19 | 1.30 | 6.05 | 4.80 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 10% | 0.06 | 0.24 | 7.25 | 2.73 |  | 0.19 | 1.30 | 6.05 | 4.80 |'
- en: '|  | 25% | 0.10 | 1.05 | 12.86 | 8.36 |  | 0.43 | 3.24 | 17.61 | 12.01 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 25% | 0.10 | 1.05 | 12.86 | 8.36 |  | 0.43 | 3.24 | 17.61 | 12.01 |'
- en: '| B.Cont. | 50% | 0.21 | 3.59 | 27.30 | 18.51 |  | 1.02 | 6.61 | 34.29 | 24.07
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| B.Cont. | 50% | 0.21 | 3.59 | 27.30 | 18.51 |  | 1.02 | 6.61 | 34.29 | 24.07
    |'
- en: '|  | 75% | 0.43 | 5.43 | 30.21 | 26.84 |  | 1.90 | 11.76 | 49.38 | 39.54 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 75% | 0.43 | 5.43 | 30.21 | 26.84 |  | 1.90 | 11.76 | 49.38 | 39.54 |'
- en: '|  | 90% | 0.81 | 8.49 | 46.41 | 31.36 |  | 3.42 | 20.79 | 90.90 | 64.65 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 90% | 0.81 | 8.49 | 46.41 | 31.36 |  | 3.42 | 20.79 | 90.90 | 64.65 |'
- en: '| Rel.MSE |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 相对均方误差 |'
- en: '|  | 10% | 1.05 | 1.67 | 2.50 | 3.38 |  | 0.96 | 1.11 | 2.75 | 2.98 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 10% | 1.05 | 1.67 | 2.50 | 3.38 |  | 0.96 | 1.11 | 2.75 | 2.98 |'
- en: '|  | 25% | 1.16 | 2.40 | 4.97 | 9.03 |  | 1.08 | 1.61 | 4.33 | 4.75 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | 25% | 1.16 | 2.40 | 4.97 | 9.03 |  | 1.08 | 1.61 | 4.33 | 4.75 |'
- en: '| Cat. | 50% | 1.37 | 5.99 | 10.37 | 14.89 |  | 1.25 | 3.35 | 7.40 | 8.16 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Cat. | 50% | 1.37 | 5.99 | 10.37 | 14.89 |  | 1.25 | 3.35 | 7.40 | 8.16 |'
- en: '|  | 75% | 1.49 | 10.25 | 27.73 | 26.16 |  | 1.48 | 9.07 | 14.87 | 15.80 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 75% | 1.49 | 10.25 | 27.73 | 26.16 |  | 1.48 | 9.07 | 14.87 | 15.80 |'
- en: '|  | 90% | 1.62 | 16.22 | 97.33 | 40.16 |  | 1.89 | 23.91 | 36.37 | 27.92 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 90% | 1.62 | 16.22 | 97.33 | 40.16 |  | 1.89 | 23.91 | 36.37 | 27.92 |'
- en: '|  | 10% | 1.19 | 1.50 | 44.06 | 4.35 |  | 0.82 | 0.86 | 7.40 | 2.05 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 10% | 1.19 | 1.50 | 44.06 | 4.35 |  | 0.82 | 0.86 | 7.40 | 2.05 |'
- en: '|  | 25% | 1.30 | 1.77 | 74.42 | 13.82 |  | 0.92 | 1.11 | 14.80 | 4.90 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 25% | 1.30 | 1.77 | 74.42 | 13.82 |  | 0.92 | 1.11 | 14.80 | 4.90 |'
- en: '| B.Cont. | 50% | 1.44 | 3.31 | 139.24 | 72.57 |  | 1.07 | 1.90 | 32.26 | 13.76
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| B.Cont. | 50% | 1.44 | 3.31 | 139.24 | 72.57 |  | 1.07 | 1.90 | 32.26 | 13.76
    |'
- en: '|  | 75% | 1.55 | 6.71 | 284.00 | 150.35 |  | 1.26 | 4.09 | 88.78 | 47.56 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | 75% | 1.55 | 6.71 | 284.00 | 150.35 |  | 1.26 | 4.09 | 88.78 | 47.56 |'
- en: '|  | 90% | 1.64 | 19.69 | 603.38 | 451.44 |  | 1.54 | 10.80 | 282.29 | 127.15
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 90% | 1.64 | 19.69 | 603.38 | 451.44 |  | 1.54 | 10.80 | 282.29 | 127.15
    |'
- en: Overall, for the estimands of marginal and bivariate probabilities of the categorical
    and binned continuous variables, MICE with CART significantly outperforms all
    other three methods, with consistently yielding the smallest ASB and relative
    MSE. RF is the second best, also consistently outperforming the deep learning
    methods. The advantage of the MICE algorithms is particularly pronounced in the
    upper (e.g. 75% and 90%) quantiles, indicating that GAIN and MIDA imputations
    have large variations over repeated samples and variables. Indeed, MIDA and GAIN
    lead to ultra long tails in estimating the summary statistics of the variables.
    For example, for bivariate probabilities of binned continuous variables, the 90%
    percentile of the ASB from MIDA and GAIN is approximately 20 and 27 times, respectively,
    of that from CART. The discrepancy is even bigger for relative MSE. There is no
    consistent pattern in comparing MIDA and GAIN. Specifically, for continuous variables,
    MIDA generally outperforms GAIN, but the difference is small except for the upper
    percentiles, where GAIN tends to produce very large bias and relative MSE. For
    categorical variables, GAIN outperforms MIDA half of the time, but again leads
    to the largest variation in imputations across the variables. Moreover, an interesting
    and somewhat surprising observation is that MICE with CART consistently outperforms
    RF—sometimes by a large magnitude—regardless of the choice of estimand or metric.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，对于分类变量和分箱连续变量的边际概率和双变量概率的估计，使用CART的MICE显著优于其他三种方法，始终产生最小的ASB和相对MSE。RF是第二好，始终优于深度学习方法。MICE算法的优势在上分位数（如75%和90%）中尤为明显，表明GAIN和MIDA插补在重复样本和变量上有较大变异。实际上，MIDA和GAIN在估计变量的汇总统计时会导致超长尾巴。例如，对于分箱连续变量的双变量概率，MIDA和GAIN的ASB的90%分位数分别约为CART的20倍和27倍。相对MSE的差异更大。MIDA和GAIN的比较没有一致的模式。具体而言，对于连续变量，MIDA通常优于GAIN，但除上分位数外，差异较小，而GAIN在上分位数时往往产生非常大的偏差和相对MSE。对于分类变量，GAIN在一半的情况下优于MIDA，但也导致了变量之间插补的最大变异。此外，一个有趣且稍微令人惊讶的观察是，使用CART的MICE在所有情况下始终优于RF——有时差距很大——无论估计量或指标的选择如何。
- en: All methods generally yield less biased estimates (i.e. smaller ASB) of the
    marginal probabilities than the bivariate probabilities. This illustrates preserving
    multivariate distributional features is more challenging than univariate ones.
    The advantage of CART over the other methods is comparatively larger when estimating
    bivariate estimands than univariate ones. Interestingly, the relative MSE tends
    to be higher for the marginal probabilities than the bivariate probabilities.
    This is likely due to the fact that the denominator in the definition of relative
    MSE in ([3.2](#S3.E2 "In 3 Simulation-based evaluation of imputation methods ‣
    Are deep learning models superior for missing data imputation in surveys? Evidence
    from an empirical comparison")) is the MSE from the sampled data before introducing
    missing data, which tends to be smaller for marginal probabilities than bivariate
    probabilities. CART yields MSEs that are very close to the corresponding MSEs
    from the sampled data before introducing missing data; i.e., the relative MSE
    is close to 1\. On the contrary, both deep learning methods, and GAIN in particular,
    can result in exceedingly large relative MSE for many estimands.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所有方法通常对边际概率的估计（即较小的ASB）比对双变量概率的估计更少偏差。这说明保持多变量分布特征比保持单变量特征更具挑战性。CART在估计双变量估计量时相较于其他方法的优势比较大。有趣的是，边际概率的相对MSE往往高于双变量概率。这可能是因为相对MSE的定义中的分母（见[3.2](#S3.E2
    "在3 模拟基础的插补方法评估 ‣ 深度学习模型在调查缺失数据插补中是否优越？来自实证比较的证据")）是引入缺失数据之前样本数据的MSE，相比于双变量概率，边际概率的MSE往往较小。CART产生的MSE非常接近于引入缺失数据之前的样本数据的MSE；即，相对MSE接近1。相反，深度学习方法，尤其是GAIN，对于许多估计量可能导致极大的相对MSE。
- en: '![Refer to caption](img/f4062de7ffe73be405a1c58ad3a9dee1.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f4062de7ffe73be405a1c58ad3a9dee1.png)'
- en: 'Figure 4.1: Coverage rate of the 95% confidence interval for all marginal and
    bivariate probabilities obtained from the four imputation methods in the simulations
    with $n=10,000$ and 30% values MCAR. The red dashed line is 0.95.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：在模拟中使用四种插补方法获得的所有边际和双变量概率的95%置信区间覆盖率，其中$n=10,000$且30%的值为MCAR。红色虚线为0.95。
- en: Figures [4.1](#S4.F1 "Figure 4.1 ‣ 4.2.1 MCAR scenario ‣ 4.2 Simulations with
    n=10,000 ‣ 4 Evaluation based on ACS ‣ Are deep learning models superior for missing
    data imputation in surveys? Evidence from an empirical comparison") displays the
    estimated coverage rates of the $95\%$ confidence intervals for the marginal and
    bivariate probabilities. The patterns on coverage between different methods is
    similar to those on bias and MSE. Specifically, CART tends to result in coverage
    rates that are close to the nominal $95\%$ level, with the median consistently
    being around 95% and tight interquartile range. In contrast, RF, GAIN and MIDA
    all result in coverage rates that are much farther off from the nominal $95\%$
    level. For example, the median coverage rates under both GAIN and MIDA are all
    under 0.60, and are even less than 0.30 for continuous variables. A closer look
    into the prediction accuracy of each variable reveals that GAIN and MIDA tend
    to generate imputations that are biased toward the most frequent levels, and GAIN
    in particular generally produces narrower intervals than the other methods. This
    once again provides evidence of significant bias under the deep learning methods.
    All methods tend to result in higher median coverage rates for the bivariate probabilities
    than the marginal probabilities, although the left tails are generally longer
    for the former than the latter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4.1](#S4.F1 "Figure 4.1 ‣ 4.2.1 MCAR scenario ‣ 4.2 Simulations with n=10,000
    ‣ 4 Evaluation based on ACS ‣ Are deep learning models superior for missing data
    imputation in surveys? Evidence from an empirical comparison") 显示了边际和双变量概率的$95\%$置信区间的估计覆盖率。不同方法之间的覆盖模式与偏差和均方误差的模式类似。具体而言，CART通常产生接近名义$95\%$水平的覆盖率，中位数始终保持在95%左右，并且四分位数范围较窄。相比之下，RF、GAIN和MIDA的覆盖率远远低于名义$95\%$水平。例如，GAIN和MIDA下的中位覆盖率都低于0.60，对于连续变量甚至低于0.30。对每个变量的预测准确性进行更详细的观察显示，GAIN和MIDA倾向于生成偏向最常见水平的填充值，特别是GAIN通常产生比其他方法更窄的区间。这再次提供了深度学习方法下显著偏差的证据。所有方法在双变量概率上的中位覆盖率通常高于边际概率，尽管前者的左尾通常比后者长。
- en: 4.2.2 MAR scenario
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 MAR场景
- en: We also consider a MAR scenario, which is more plausible than MCAR in practice.
    We set six variables — age, gender, marital status, race, educational attainment
    and class of worker — to be fully observed. It would be cumbersome to specify
    different MAR mechanism for each of the remaining 40 variables, so we randomly
    divide them into three groups, consisting of 10, 15, and 15 variables. We then
    specify a separate nonresponse model by which to generate the missing data for
    the variables in each group. Specifically, we postulate a logistic model per group,
    conditional on the fully observed six variables, based on which we then generate
    binary missing data indicators for each variable in that group. This process results
    in approximately 30% missing rate for each of the 40 variables. We describe the
    models in more detail in the supplementary material.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还考虑了一个MAR场景，这在实践中比MCAR更为可信。我们设定了六个变量——年龄、性别、婚姻状况、种族、教育程度和工作类别——为完全观察变量。为每个剩余的40个变量指定不同的MAR机制将会非常繁琐，因此我们将它们随机分为三个组，分别由10个、15个和15个变量组成。接着，我们为每组变量指定了一个单独的缺失数据模型。具体来说，我们假设每组一个基于完全观察到的六个变量的逻辑回归模型，然后基于该模型生成每个变量的二元缺失数据指示符。这个过程导致每40个变量中的每一个缺失率大约为30%。我们在补充材料中对这些模型进行了更详细的描述。
- en: 'Table 4.2: Distributions of absolute standardized bias $(\times 100)$ and relative
    mean squared error for all methods, when $n=10,000$ and 30% values MAR, over all
    possible marginal and bivariate probabilities. “Cat.” means categorical variables
    and “B.Cont.” means binned continuous variables.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.2：所有方法的绝对标准化偏差$(\times 100)$和相对均方误差的分布，当$n=10,000$且30%值为MAR时，针对所有可能的边际和双变量概率。“Cat.”表示分类变量，“B.Cont.”表示分箱连续变量。
- en: '| Quantiles | Marginal |  | Bivariate |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 分位数 | 边际 |  | 双变量 |'
- en: '| CART | RF | GAIN | MIDA |  | CART | RF | GAIN | MIDA |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CART | RF | GAIN | MIDA |  | CART | RF | GAIN | MIDA |'
- en: '| ASB $(\times 100)$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ASB $(\times 100)$ |'
- en: '|  | 10% | 0.05 | 0.13 | 0.15 | 0.14 |  | 0.15 | 0.71 | 0.76 | 0.89 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 10% | 0.05 | 0.13 | 0.15 | 0.14 |  | 0.15 | 0.71 | 0.76 | 0.89 |'
- en: '|  | 25% | 0.11 | 0.44 | 0.62 | 0.61 |  | 0.40 | 2.23 | 2.55 | 3.20 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | 25% | 0.11 | 0.44 | 0.62 | 0.61 |  | 0.40 | 2.23 | 2.55 | 3.20 |'
- en: '| Cat. | 50% | 0.29 | 2.13 | 3.05 | 4.55 |  | 1.08 | 6.06 | 6.85 | 8.14 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Cat. | 50% | 0.29 | 2.13 | 3.05 | 4.55 |  | 1.08 | 6.06 | 6.85 | 8.14 |'
- en: '|  | 75% | 1.04 | 4.98 | 6.63 | 10.22 |  | 2.49 | 13.43 | 16.78 | 16.19 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 75% | 1.04 | 4.98 | 6.63 | 10.22 |  | 2.49 | 13.43 | 16.78 | 16.19 |'
- en: '|  | 90% | 1.80 | 10.49 | 18.91 | 17.00 |  | 5.68 | 24.06 | 28.04 | 25.36 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | 90% | 1.80 | 10.49 | 18.91 | 17.00 |  | 5.68 | 24.06 | 28.04 | 25.36 |'
- en: '|  | 10% | 0.07 | 0.29 | 0.33 | 0.33 |  | 0.27 | 1.17 | 10.87 | 6.18 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | 10% | 0.07 | 0.29 | 0.33 | 0.33 |  | 0.27 | 1.17 | 10.87 | 6.18 |'
- en: '|  | 25% | 0.17 | 1.07 | 9.64 | 3.13 |  | 0.69 | 3.49 | 23.67 | 16.26 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | 25% | 0.17 | 1.07 | 9.64 | 3.13 |  | 0.69 | 3.49 | 23.67 | 16.26 |'
- en: '| B.Cont. | 50% | 0.67 | 3.14 | 32.86 | 23.85 |  | 1.58 | 7.83 | 38.52 | 31.17
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| B.Cont. | 50% | 0.67 | 3.14 | 32.86 | 23.85 |  | 1.58 | 7.83 | 38.52 | 31.17
    |'
- en: '|  | 75% | 1.20 | 6.95 | 39.57 | 36.09 |  | 3.40 | 15.20 | 53.59 | 47.34 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 75% | 1.20 | 6.95 | 39.57 | 36.09 |  | 3.40 | 15.20 | 53.59 | 47.34 |'
- en: '|  | 90% | 3.40 | 12.39 | 63.45 | 41.99 |  | 5.94 | 25.16 | 97.47 | 85.44 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | 90% | 3.40 | 12.39 | 63.45 | 41.99 |  | 5.94 | 25.16 | 97.47 | 85.44 |'
- en: '| Rel.MSE |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Rel.MSE |'
- en: '|  | 10% | 1.00 | 1.00 | 1.00 | 1.00 |  | 0.97 | 1.00 | 1.53 | 1.93 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 10% | 1.00 | 1.00 | 1.00 | 1.00 |  | 0.97 | 1.00 | 1.53 | 1.93 |'
- en: '|  | 25% | 1.08 | 1.82 | 2.56 | 4.75 |  | 1.04 | 1.39 | 3.78 | 4.03 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | 25% | 1.08 | 1.82 | 2.56 | 4.75 |  | 1.04 | 1.39 | 3.78 | 4.03 |'
- en: '| Cat. | 50% | 1.33 | 4.33 | 19.03 | 15.13 |  | 1.25 | 3.00 | 10.42 | 8.38
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Cat. | 50% | 1.33 | 4.33 | 19.03 | 15.13 |  | 1.25 | 3.00 | 10.42 | 8.38
    |'
- en: '|  | 75% | 1.72 | 13.08 | 55.07 | 33.36 |  | 1.59 | 9.56 | 27.45 | 16.95 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | 75% | 1.72 | 13.08 | 55.07 | 33.36 |  | 1.59 | 9.56 | 27.45 | 16.95 |'
- en: '|  | 90% | 2.27 | 18.70 | 101.91 | 48.44 |  | 2.23 | 27.44 | 64.01 | 32.85
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | 90% | 2.27 | 18.70 | 101.91 | 48.44 |  | 2.23 | 27.44 | 64.01 | 32.85
    |'
- en: '|  | 10% | 1.00 | 1.00 | 1.00 | 1.00 |  | 0.88 | 0.90 | 11.19 | 2.96 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | 10% | 1.00 | 1.00 | 1.00 | 1.00 |  | 0.88 | 0.90 | 11.19 | 2.96 |'
- en: '|  | 25% | 1.38 | 1.83 | 90.98 | 8.49 |  | 1.00 | 1.16 | 20.15 | 6.87 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | 25% | 1.38 | 1.83 | 90.98 | 8.49 |  | 1.00 | 1.16 | 20.15 | 6.87 |'
- en: '| B.Cont. | 50% | 1.70 | 4.57 | 207.58 | 96.08 |  | 1.18 | 2.29 | 45.25 | 21.33
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| B.Cont. | 50% | 1.70 | 4.57 | 207.58 | 96.08 |  | 1.18 | 2.29 | 45.25 | 21.33
    |'
- en: '|  | 75% | 2.12 | 11.47 | 692.67 | 239.69 |  | 1.50 | 6.95 | 125.39 | 70.90
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | 75% | 2.12 | 11.47 | 692.67 | 239.69 |  | 1.50 | 6.95 | 125.39 | 70.90
    |'
- en: '|  | 90% | 3.12 | 50.56 | 1342.23 | 806.43 |  | 2.12 | 18.07 | 459.78 | 205.14
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | 90% | 3.12 | 50.56 | 1342.23 | 806.43 |  | 2.12 | 18.07 | 459.78 | 205.14
    |'
- en: Table [4.2](#S4.T2 "Table 4.2 ‣ 4.2.2 MAR scenario ‣ 4.2 Simulations with n=10,000
    ‣ 4 Evaluation based on ACS ‣ Are deep learning models superior for missing data
    imputation in surveys? Evidence from an empirical comparison") displays the distributions
    of the ASB and relative MSE of all the marginal and bivariate probabilities from
    the four methods. All methods yield larger ASB and relative MSE under the MAR
    scenario than the previous MCAR scenario. This is expected because MAR is a stronger
    assumption than MCAR that requires conditioning on more information. Nonetheless,
    the overall patterns of relative performance between the methods remain the same
    as those under MCAR. Specifically, CART once again produces estimates with the
    least ASB and relative MSE — by an even larger margin then under MCAR — among
    the four methods, followed by RF, and then MIDA and GAIN. One notable observation
    is the deteriorating performance of the deep learning methods, particularly GAIN,
    in imputing continuous variables, sometimes resulting in several hundreds fold
    of relative MSE than CART. This indicates the huge uncertainties associated with
    GAIN in imputing continuous variables.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4.2](#S4.T2 "Table 4.2 ‣ 4.2.2 MAR scenario ‣ 4.2 Simulations with n=10,000
    ‣ 4 Evaluation based on ACS ‣ Are deep learning models superior for missing data
    imputation in surveys? Evidence from an empirical comparison") 显示了四种方法下所有边际和双变量概率的
    ASB 和相对 MSE 的分布。在 MAR 情境下，所有方法的 ASB 和相对 MSE 都比之前的 MCAR 情境更大。这是因为 MAR 是比 MCAR 更强的假设，需要基于更多信息。然而，方法间相对表现的总体模式与
    MCAR 下的情况保持一致。具体来说，CART 再次产生了四种方法中 ASB 和相对 MSE 最小的估计——比 MCAR 下的差距更大——其次是 RF，然后是
    MIDA 和 GAIN。一个显著的观察是，深度学习方法，特别是 GAIN，在填补连续变量时性能下降，有时相对 MSE 甚至比 CART 高出几百倍。这表明
    GAIN 在填补连续变量时存在巨大的不确定性。
- en: '![Refer to caption](img/3683d387833d781e2e8e0bf98ea57e4d.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3683d387833d781e2e8e0bf98ea57e4d.png)'
- en: 'Figure 4.2: Coverage rates of the 95% confidence intervals for all marginal
    and bivariate probabilities obtained from four methods in the simulations with
    $n=10,000$ and 30% values MAR. The red dashed line is 0.95.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：在$n=10,000$和30%值 MAR 的模拟中，四种方法获得的所有边际和双变量概率的 95% 置信区间的覆盖率。红色虚线为 0.95。
- en: Figures [4.2](#S4.F2 "Figure 4.2 ‣ 4.2.2 MAR scenario ‣ 4.2 Simulations with
    n=10,000 ‣ 4 Evaluation based on ACS ‣ Are deep learning models superior for missing
    data imputation in surveys? Evidence from an empirical comparison") displays the
    estimated coverage rates of the $95\%$ confidence intervals for the marginal and
    bivariate probabilities, under each method. Similar as the case of bias and MSE,
    all methods generally result in lower coverage rates under MAR than MCAR, with
    visibly longer left tails in some cases, but the overall patterns comparing between
    the methods remain the same. Specifically, CART still tends to result in coverage
    rates that are above 90%, while the other three methods have consistently lower
    coverage rate. In particular, both GAIN and MIDA result in extremely low—below
    7%– median coverage rates for continuous variables. This is closely related to
    the previous observation of the large uncertainty of the deep learning methods
    in imputing continuous variables.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4.2](#S4.F2 "Figure 4.2 ‣ 4.2.2 MAR scenario ‣ 4.2 Simulations with n=10,000
    ‣ 4 Evaluation based on ACS ‣ Are deep learning models superior for missing data
    imputation in surveys? Evidence from an empirical comparison")展示了每种方法下边际和双变量概率的$95\%$置信区间的估计覆盖率。与偏差和MSE的情况类似，所有方法在MAR下的覆盖率通常低于MCAR，并且在某些情况下可见的左尾更长，但方法间的整体模式保持不变。具体来说，CART仍然倾向于产生超过90%的覆盖率，而其他三种方法的覆盖率持续较低。特别是，GAIN和MIDA在连续变量上的中位数覆盖率极低——低于7%。这与之前观察到的深度学习方法在插补连续变量时的不确定性大有关。
- en: Finally, to illustrate that evaluating only the overall RMSE and accuracy metrics
    may be misleading, we display the mean and empirical standard errors of the overall
    RMSE and accuracy over the 100 simulations in Table [4.3](#S4.T3 "Table 4.3 ‣
    4.2.2 MAR scenario ‣ 4.2 Simulations with n=10,000 ‣ 4 Evaluation based on ACS
    ‣ Are deep learning models superior for missing data imputation in surveys? Evidence
    from an empirical comparison"), where MCAR is in the top panel and MAR is in the
    bottom panel. Under both missing data mechanisms, for the continuous variables,
    MIDA leads to the smallest overall RMSE, followed by CART, and with RF and GAIN
    being last. For the categorical variables, CART and GAIN lead to the highest overall
    accuracy, with MIDA being closely behind and RF last. These patterns, not surprisingly,
    differ from those reported earlier based on marginal and bivariate probabilities
    and different metrics. As discussed in Section [3](#S3 "3 Simulation-based evaluation
    of imputation methods ‣ Are deep learning models superior for missing data imputation
    in surveys? Evidence from an empirical comparison"), overall RMSE and accuracy
    do not capture the distributional features of multivariate data or the repeated
    sampling properties of the imputation methods.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了说明仅评估整体RMSE和准确率指标可能会产生误导，我们在表格[4.3](#S4.T3 "Table 4.3 ‣ 4.2.2 MAR scenario
    ‣ 4.2 Simulations with n=10,000 ‣ 4 Evaluation based on ACS ‣ Are deep learning
    models superior for missing data imputation in surveys? Evidence from an empirical
    comparison")中展示了100次模拟中整体RMSE和准确率的均值和经验标准误，其中MCAR在上面板，MAR在下面板。在这两种缺失数据机制下，对于连续变量，MIDA的整体RMSE最小，其次是CART，RF和GAIN最后。对于分类变量，CART和GAIN的整体准确率最高，MIDA紧随其后，RF最后。这些模式与之前基于边际和双变量概率以及不同指标的报告不一致。正如第[3](#S3
    "3 Simulation-based evaluation of imputation methods ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison")节中讨论的，整体RMSE和准确率未能捕捉多变量数据的分布特征或插补方法的重复采样属性。
- en: 'Table 4.3: Overall RMSE on continuous variables and overall accuracy on categorical
    variables averaged over 100 simulations, with the empirical standard errors in
    the parenthesis. The top panel is under MCAR and the bottom panel is under MAR,
    all with 30% missing data.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '表4.3: 在100次模拟中，连续变量的整体RMSE和分类变量的整体准确率的平均值，括号中为经验标准误。上面板为MCAR，下面板为MAR，所有情况下缺失数据为30%。'
- en: '| Mechanism | Metric | CART | RF | GAIN | MIDA |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 机制 | 指标 | CART | RF | GAIN | MIDA |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | RMSE | 0.128 (0.002) | 0.159 (0.003) | 0.161 (0.008) | 0.112 (0.002) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | RMSE | 0.128 (0.002) | 0.159 (0.003) | 0.161 (0.008) | 0.112 (0.002) |'
- en: '| MCAR | Accuracy | 0.785 (0.001) | 0.658 (0.003) | 0.782 (0.002) | 0.752 (0.004)
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| MCAR | 准确率 | 0.785 (0.001) | 0.658 (0.003) | 0.782 (0.002) | 0.752 (0.004)
    |'
- en: '|  | RMSE | 0.130 (0.003) | 0.154 (0.004) | 0.145 (0.009) | 0.110 (0.002) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | RMSE | 0.130 (0.003) | 0.154 (0.004) | 0.145 (0.009) | 0.110 (0.002) |'
- en: '| MAR | Accuracy | 0.819 (0.001) | 0.704 (0.003) | 0.820 (0.002) | 0.780 (0.007)
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| MAR | 准确率 | 0.819 (0.001) | 0.704 (0.003) | 0.820 (0.002) | 0.780 (0.007)
    |'
- en: 4.3 Simulations with n=100,000 and 30% MCAR
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 具有n=100,000和30% MCAR的模拟
- en: Deep learning models usually require a large sample size to train. Therefore,
    to give MIDA and GAIN a more favorable setting as well as to investigate the sensitivity
    of our results to variations in sample size, we generate a simulation scenario
    of $H=10$ samples with $n=100,000$ under MCAR. That is, we randomly set 30% of
    the values of each variable to be missing independently. Here we only generate
    10 simulations due to the huge computational cost of MICE for samples with this
    size. In this scenario, we omit RF because the previous results in Section [4.2](#S4.SS2
    "4.2 Simulations with n=10,000 ‣ 4 Evaluation based on ACS ‣ Are deep learning
    models superior for missing data imputation in surveys? Evidence from an empirical
    comparison") have shown that RF is consistently inferior to CART in terms of performance
    and computation. We use CART, GAIN, and MIDA to create $L=10$ completed datasets.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常需要大量样本来进行训练。因此，为了给MIDA和GAIN提供更有利的环境，并调查我们结果对样本大小变化的敏感性，我们生成了一个$H=10$的样本模拟场景，其中$n=100,000$，在MCAR条件下。也就是说，我们随机将每个变量的30%值独立地设置为缺失。由于样本大小带来的计算成本巨大，这里我们只生成了10个模拟。在这个场景中，我们省略了RF，因为在[4.2](#S4.SS2
    "4.2 Simulations with n=10,000 ‣ 4 Evaluation based on ACS ‣ Are deep learning
    models superior for missing data imputation in surveys? Evidence from an empirical
    comparison")节中的结果表明，RF在性能和计算方面始终不如CART。我们使用CART、GAIN和MIDA来创建$L=10$个完整的数据集。
- en: Because it usually requires a much larger number of simulations to reliably
    calculate MSE and coverage, here we focus on the weighted absolute bias metric
    ([3.4](#S3.E4 "In 3 Simulation-based evaluation of imputation methods ‣ Are deep
    learning models superior for missing data imputation in surveys? Evidence from
    an empirical comparison")). Table [4.4](#S4.T4 "Table 4.4 ‣ 4.3 Simulations with
    n=100,000 and 30% MCAR ‣ 4 Evaluation based on ACS ‣ Are deep learning models
    superior for missing data imputation in surveys? Evidence from an empirical comparison")
    displays the distributions of the estimated weighted absolute bias, averaged over
    10 simulations, of the marginal probabilities of the categorical and binned continuous
    variables. Overall, the patterns comparing between the four methods remain consistent
    with those observed in Section [4.2](#S4.SS2 "4.2 Simulations with n=10,000 ‣
    4 Evaluation based on ACS ‣ Are deep learning models superior for missing data
    imputation in surveys? Evidence from an empirical comparison"). Specifically,
    CART again results in the smallest weighted absolute difference in both categorical
    and continuous variables, and the advantage is particularly pronounced with continuous
    variables. For example, for categorical variables, MIDA and GAIN result in a median
    of weighted absolute bias at least 9 and 11 times, respectively, larger than CART.
    The advantage of CART grows to about 30 and 60 times over MIDA and GAIN, respectively,
    for continuous variables. Moreover, CART performs robustly across variables, evident
    from the small variation in the weighted absolute bias, e.g. 0.07 for 10% percentile
    and 0.33 for 90% percentile among the categorical variables. In contrast, both
    deep learning models result in much larger variation across variables; e.g., 0.57
    for 10% percentile and 2.92 for 90% percentile among the categorical variables
    under MIDA, and even larger for GAIN. In summary, other than computational time,
    MICE with CART significantly outperforms MIDA and GAIN in terms of bias and variance
    regardless of the sample size.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于通常需要更多的模拟来可靠地计算MSE和覆盖率，这里我们专注于加权绝对偏差指标（[3.4](#S3.E4 "In 3 Simulation-based
    evaluation of imputation methods ‣ Are deep learning models superior for missing
    data imputation in surveys? Evidence from an empirical comparison")）。表[4.4](#S4.T4
    "Table 4.4 ‣ 4.3 Simulations with n=100,000 and 30% MCAR ‣ 4 Evaluation based
    on ACS ‣ Are deep learning models superior for missing data imputation in surveys?
    Evidence from an empirical comparison")显示了估计的加权绝对偏差的分布，这些分布是对10次模拟的平均值，涵盖了分类变量和分箱连续变量的边际概率。总体而言，四种方法之间的比较模式与[4.2](#S4.SS2
    "4.2 Simulations with n=10,000 ‣ 4 Evaluation based on ACS ‣ Are deep learning
    models superior for missing data imputation in surveys? Evidence from an empirical
    comparison")节中观察到的模式保持一致。具体来说，CART在分类和连续变量中都再次表现出最小的加权绝对差异，这一优势在连续变量中尤为明显。例如，对于分类变量，MIDA和GAIN的加权绝对偏差的中位数分别比CART大至少9倍和11倍。对于连续变量，CART的优势分别增大到MIDA和GAIN的约30倍和60倍。此外，CART在各变量间表现稳健，这从加权绝对偏差的较小变化中可以看出，例如在分类变量中10%分位点为0.07，90%分位点为0.33。相比之下，两个深度学习模型在变量间表现出更大的变化；例如，在MIDA下，分类变量的10%分位点为0.57，90%分位点为2.92，而GAIN下的变化更大。总之，除了计算时间外，无论样本大小如何，MICE与CART相比在偏差和方差方面显著优于MIDA和GAIN。
- en: 'Table 4.4: Distributions of the weighted absolute bias $(\times 100)$ averaged
    over 10 simulated samples, each with $n=100,000$ and 30% values MCAR.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4.4: 平均绝对偏差 $(\times 100)$ 的分布，基于 10 个模拟样本，每个样本的 $n=100,000$ 和 30% 的值为 MCAR。'
- en: '| Quantiles | Categorical |  | Binned Continuous |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 分位数 | 类别变量 |  | 分箱连续变量 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| CART | GAIN | MIDA |  | CART | GAIN | MIDA |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| CART | GAIN | MIDA |  | CART | GAIN | MIDA |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 10% | 0.07 | 0.43 | 0.57 |  | 0.10 | 5.52 | 1.98 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 0.07 | 0.43 | 0.57 |  | 0.10 | 5.52 | 1.98 |'
- en: '| 25% | 0.11 | 1.11 | 1.02 |  | 0.11 | 6.65 | 2.78 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 25% | 0.11 | 1.11 | 1.02 |  | 0.11 | 6.65 | 2.78 |'
- en: '| 50% | 0.15 | 1.74 | 1.40 |  | 0.12 | 7.36 | 4.04 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 0.15 | 1.74 | 1.40 |  | 0.12 | 7.36 | 4.04 |'
- en: '| 75% | 0.24 | 3.77 | 2.07 |  | 0.13 | 9.40 | 6.50 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 75% | 0.24 | 3.77 | 2.07 |  | 0.13 | 9.40 | 6.50 |'
- en: '| 90% | 0.33 | 4.63 | 2.92 |  | 0.15 | 11.31 | 7.72 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 90% | 0.33 | 4.63 | 2.92 |  | 0.15 | 11.31 | 7.72 |'
- en: 4.4 Role of hyperparameters in tree-based MICE
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 基于树的 MICE 中超参数的作用
- en: 'The pattern that CART outperforms RF is surprising, because the common knowledge
    is that ensemble methods are usually superior to single tree methods. But the
    same pattern was also observed in another recent study [[56](#bib.bibx56)]. We
    investigate the role of the key hyperparameter in RF—the maximum number of trees
    $d$—in the simulations. We randomly selected a simulated data of size $n=10,000$
    and 30% of entries being MCAR. We use the mice package to fit RF with different
    number of trees: $d=2,5,10,15,20$, where $d=10$ is the default setting. The relative
    MSE of the imputed categorical variables fitted using each $d$ value, as well
    as that using CART, is shown as trajectories in Figure [4.3](#S4.F3 "Figure 4.3
    ‣ 4.4 Role of hyperparameters in tree-based MICE ‣ 4 Evaluation based on ACS ‣
    Are deep learning models superior for missing data imputation in surveys? Evidence
    from an empirical comparison"), which reveals a consistent pattern: the upper
    quantiles —particularly those above 50%—of the relative MSE deteriorates rapidly
    as the maximum number of trees in RF increases, while the lower quantiles, e.g.,
    10%, 25%, remain stable. We found a similar pattern with the standardized bias
    metric and continuous variables, and thus the results are omitted here. This suggests
    that larger number of trees in RF—at least as implemented in the mice package—leads
    to much longer tail in the distribution of the bias and MSEs. This is likely due
    to overfitting. We cannot exclude the possibility that a more customized hyperparameter
    tuning of RF may outperform CART in some applications. However, such case-specific
    fine-tuning of the MICE algorithm is generally not available for the vast majority
    of MI consumers who relies on the default setting of popular packages like mice.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: CART 优于 RF 的模式令人惊讶，因为普遍认为集成方法通常优于单一决策树方法。但在另一项最近的研究中也观察到了相同的模式[[56](#bib.bibx56)]。我们在模拟中调查了
    RF 的关键超参数——最大树木数量 $d$。我们随机选择了一个大小为 $n=10,000$ 的模拟数据，其中 30% 的条目是 MCAR。我们使用 mice
    包拟合不同树木数量的 RF：$d=2,5,10,15,20$，其中 $d=10$ 是默认设置。图 [4.3](#S4.F3 "Figure 4.3 ‣ 4.4
    Role of hyperparameters in tree-based MICE ‣ 4 Evaluation based on ACS ‣ Are deep
    learning models superior for missing data imputation in surveys? Evidence from
    an empirical comparison") 显示了每个 $d$ 值拟合的类别变量的相对 MSE，以及使用 CART 的结果，这揭示了一致的模式：相对
    MSE 的上分位数——尤其是 50% 以上的分位数——随着 RF 中最大树木数量的增加而迅速恶化，而下分位数，例如 10%、25%，保持稳定。我们发现标准化偏差度量和连续变量也有类似的模式，因此这些结果在这里省略。这表明
    RF 中的树木数量越多——至少在 mice 包中实现的——偏差和 MSE 的分布尾部越长，这可能是由于过拟合。我们不能排除 RF 的更定制化超参数调优在某些应用中优于
    CART 的可能性。然而，这种特定案例的 MICE 算法微调通常不适用于大多数依赖于 mice 等流行包默认设置的 MI 用户。
- en: '![Refer to caption](img/c8456985090b23bea055d2c03cfd308c.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8456985090b23bea055d2c03cfd308c.png)'
- en: 'Figure 4.3: Quantiles of the relative mean squared error over all marginal
    and bivariate probabilities of categorical variables, under CART and RF with various
    number of trees, for a simulation sample with $n=10,000$ and $30\%$ values MCAR.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.3: 所有边际和双变量概率的类别变量的相对均方误差的分位数，在 CART 和具有不同树木数量的 RF 下，基于 $n=10,000$ 和 $30\%$
    值为 MCAR 的模拟样本。'
- en: 5 Evaluation based on “benchmark” datasets
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基于“基准”数据集的评估
- en: To verify the evaluations in the GAIN and MIDA papers [[17](#bib.bibx17), [57](#bib.bibx57),
    [32](#bib.bibx32)], we also compared the two deep learning models with CART based
    on the five benchmark datasets and simulation procedure (different from our proposed
    framework) used in these papers. Details of these datasets and simulations are
    presented in the supplementary material. The sample sizes of these data are generally
    not large enough to be considered as population data from which we can repeatedly
    sample from without replacement, so we are unable to evaluate them in a meaningful
    way using absolute standardized bias, relative MSE or coverage. We therefore evaluate
    the methods primarily on the weighted absolute bias metric. In summary, CART again
    consistently and significantly outperforms MIDA and GAIN in terms of weighted
    absolute bias for both categorical and continuous variables, across all five benchmark
    datasets. The difference in performance is particularly pronounced with continuous
    variables. We also calculated the overall MSE and accuracy as those papers did.
    Except for one dataset, we could not reproduce the results reported in these papers,
    even with the authors’ code. One possible reason is that the process of tuning
    and selecting model hyperparameters may not be clearly documented, which is true
    in the present case. More details are provided in the online supplementary material.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证GAIN和MIDA论文中的评估[[17](#bib.bibx17), [57](#bib.bibx57), [32](#bib.bibx32)]，我们还将这两种深度学习模型与基于CART的模型进行了比较，使用了这些论文中的五个基准数据集和模拟程序（与我们提出的框架不同）。这些数据集和模拟的详细信息见补充材料。这些数据的样本量通常不足以被视为可以反复抽样的总体数据，因此我们无法使用绝对标准偏差、相对均方误差或覆盖率以有意义的方式进行评估。因此，我们主要通过加权绝对偏差指标来评估这些方法。总的来说，CART在所有五个基准数据集中，在分类变量和连续变量方面的加权绝对偏差上都
    consistently 和显著优于MIDA和GAIN。特别是在连续变量方面，性能差异尤为明显。我们还计算了总体均方误差和准确率，正如这些论文所做的那样。除了一个数据集外，即使使用作者提供的代码，我们也未能重现这些论文中报告的结果。一个可能的原因是模型超参数的调整和选择过程可能没有被明确记录，这在当前情况下确实如此。更多细节见在线补充材料。
- en: 6 Conclusion
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Recent years have seen the development of many machine learning based methods
    for imputing missing data, raising the hope of improving over the more traditional
    imputation methods such as MICE. However, efforts in evaluating these methods
    in real world situations remain scarce. In this paper, we adopt an evaluation
    framework real-data-based simulations. We conduct extensive simulation studies
    based on the American Community Survey to compare repeated sampling properties
    of two MICE methods and two deep learning imputation methods based on GAN (GAIN)
    and denoising autoencoders (MIDA).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，许多基于机器学习的数据缺失填补方法得到了发展，这让人们对比传统的填补方法如MICE有了更多的期望。然而，评估这些方法在现实世界中的表现的努力仍然很少。在本文中，我们采用了基于真实数据的模拟评估框架。我们通过基于美国社区调查的广泛模拟研究，比较了两种MICE方法和两种基于GAN（GAIN）以及去噪自编码器（MIDA）的深度学习填补方法的重复抽样特性。
- en: We find that the deep learning models hold a vast computational advantage over
    MICE methods, partially because they can leverage GPU power for high-performance
    computing. However, our simulations as well as evaluation on several “benchmark"
    data suggest that MICE with CART specification of the conditional models consistently
    outperforms, usually by a substantial margin, the deep learning models in terms
    of bias, mean squared error, and coverage under a wide range of realistic settings.
    In particular, GAIN and MIDA tend to generate unstable imputations with enormous
    variations over repeated samples compared with MICE. One possible explanation
    is that deep neural networks excel at detecting complex sub-structures of big
    data, but may not suit for data with simple structure, such as the simulated data
    used here. Another possibility is that the sample sizes in our simulations are
    not adequate to train deep neural networks, which usually required much more data
    compared to traditional statistical models.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，深度学习模型在计算上相较于MICE方法具有巨大的优势，部分原因是它们可以利用GPU的高性能计算能力。然而，我们的模拟和在几个“基准”数据上的评估表明，使用CART规格的MICE模型在偏差、均方误差和覆盖范围方面通常比深度学习模型表现更好，通常差距显著，适用于各种现实环境。特别是，与MICE相比，GAIN和MIDA倾向于产生不稳定的插补，重复样本之间变异巨大。一个可能的解释是，深度神经网络擅长于检测大数据的复杂子结构，但可能不适合结构简单的数据，例如这里使用的模拟数据。另一种可能性是，我们的模拟中的样本量不足以训练深度神经网络，这些网络通常需要比传统统计模型更多的数据。
- en: These results contradict previous findings based on the single performance metric
    of overall mean squared error in the machine learning literature [[17](#bib.bibx17),
    [57](#bib.bibx57), [32](#bib.bibx32), e.g.]. This discrepancy highlights the pitfalls
    of the common practice in the machine learning literature of evaluating imputation
    methods. It also demonstrates the importance of assessing repeated-sampling properties
    on multiple estimands of MI methods. An interesting finding is that ensemble trees
    (e.g. RF) do not improve over a single tree (e.g. CART) in the context of MICE,
    which matches the findings in another recent study [[56](#bib.bibx56)]. Combined
    with the fact that the former is more computationally intensive than the latter,
    we recommend using MICE with CART instead of RF in practice.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与基于机器学习文献中的总体均方误差这一单一性能指标的先前研究结果相矛盾[[17](#bib.bibx17), [57](#bib.bibx57),
    [32](#bib.bibx32), 例如]。这一差异突显了机器学习文献中评估插补方法的常见做法的陷阱。它还展示了评估多重插补（MI）方法在多个估计量上的重复抽样特性的必要性。一个有趣的发现是，在MICE的背景下，集成树（例如RF）并未优于单一树（例如CART），这一点与另一项近期研究中的发现相匹配[[56](#bib.bibx56)]。结合前者计算复杂度高于后者的事实，我们建议在实际操作中使用CART而非RF的MICE。
- en: 'Our study has a few limitations. First, there are many deep learning methods
    that can be adapted to missing data imputation and all may have different operating
    characteristics. We choose GAIN and MIDA because both generative adversarial network
    and denoising autoencoders are immensely popular deep learning methods, and the
    imputation methods based on them have been advertised as superior to MICE. Nonetheless,
    it would be desirable to examine other deep learning based imputation methods
    in future research. Second, performance of machine learning methods is highly
    dependent on hyperparameter selection. So it can be argued that the inferior performance
    of GAIN and MIDA may be at least partially due to sub-optimal hyperparameter selection.
    However, practitioners would most likely rely on default hyperparameter values
    for any machine learning based imputation methods, which is indeed what we have
    adopted in our simulations and thus represents the real practice. Third, we did
    not consider the joint distribution between any categorical and continuous variables,
    but our evaluations within categorical and continuous variables have yielded consistent
    conclusions. Lastly, as any simulation study, one should exercise caution in generalizing
    the conclusions. By carefully selecting the data and metrics, we have attempted
    to closely mimic the settings representative of real survey data so that our conclusions
    are informative for practitioners who deal with similar situations. Additional
    evaluation studies based on different data are desired to shed more insights on
    the operating characteristics and comparative performances of different missing
    data imputation methods. Data, code, and supplementary material for the paper
    are available at: [https://github.com/zhenhua-wang/MissingData_DL](https://github.com/zhenhua-wang/MissingData_DL).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究有一些局限性。首先，许多深度学习方法可以用于缺失数据插补，且它们可能具有不同的操作特性。我们选择了GAIN和MIDA，因为生成对抗网络和去噪自编码器是非常流行的深度学习方法，而基于这些方法的插补技术被宣传为优于MICE。然而，未来的研究中考察其他基于深度学习的插补方法是有益的。其次，机器学习方法的性能高度依赖于超参数选择。因此，可以认为GAIN和MIDA表现较差可能至少部分由于超参数选择不佳。然而，实践者可能会依赖于默认的超参数值来进行任何基于机器学习的插补方法，这也是我们在模拟中所采用的，代表了实际操作。第三，我们没有考虑任何分类变量与连续变量之间的联合分布，但我们在分类和连续变量中的评估得出了相一致的结论。最后，像任何模拟研究一样，应谨慎推广结论。通过仔细选择数据和指标，我们试图紧密模拟代表真实调查数据的设置，以便我们的结论对处理类似情况的实践者有参考价值。基于不同数据的附加评估研究是必要的，以进一步了解不同缺失数据插补方法的操作特性和比较性能。数据、代码和论文的补充材料可在：[https://github.com/zhenhua-wang/MissingData_DL](https://github.com/zhenhua-wang/MissingData_DL)
    获取。
- en: Acknowledgements
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Poulos and Li’s research is supported by the National Science Foundation under
    Grant DMS-1638521 to the Statistical and Applied Mathematical Sciences Institute.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Poulos 和 Li 的研究得到国家科学基金会的资助，资助号 DMS-1638521，资助给统计与应用数学科学研究所。
- en: References
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Olanrewaju Akande, Fan Li and Jerome Reiter “An empirical comparison of
    multiple imputation methods for categorical data” In *The American Statistician*
    71.2 Taylor & Francis, 2017, pp. 162–170'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Olanrewaju Akande, Fan Li 和 Jerome Reiter “多重插补方法在分类数据中的经验比较” 见 *The American
    Statistician* 71.2 Taylor & Francis, 2017, 页162–170'
- en: '[2] B.. Arnold and S.. Press “Compatible Conditional Distributions” In *Journal
    of the American Statistical Association* 84, 1989, pp. 152–156'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] B. Arnold 和 S. Press “兼容条件分布” 见 *Journal of the American Statistical Association*
    84, 1989, 页152–156'
- en: '[3] John Barnard and Xiao-Li Meng “Applications of multiple imputation in medical
    studies: from AIDS to NHANES” In *Statistical Methods in Medical Research* 8.1
    Sage Publications Sage CA: Thousand Oaks, CA, 1999, pp. 17–36'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] John Barnard 和 Xiao-Li Meng “多重插补在医学研究中的应用：从艾滋病到NHANES” 见 *Statistical
    Methods in Medical Research* 8.1 Sage Publications Sage CA: Thousand Oaks, CA,
    1999, 页17–36'
- en: '[4] David Berthelot, Tom Schumm and Luke Metz “BEGAN: Boundary Equilibrium
    Generative Adversarial Networks” In *CoRR* abs/1703.10717, 2017 arXiv: [http://arxiv.org/abs/1703.10717](http://arxiv.org/abs/1703.10717)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] David Berthelot, Tom Schumm 和 Luke Metz “BEGAN：边界平衡生成对抗网络” 见 *CoRR* abs/1703.10717,
    2017 arXiv: [http://arxiv.org/abs/1703.10717](http://arxiv.org/abs/1703.10717)'
- en: '[5] L. Breiman “Random forests” In *Machine Learning* 45, 2001, pp. 5–32'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] L. Breiman “随机森林” 见 *Machine Learning* 45, 2001, 页5–32'
- en: '[6] L. Breiman, J.. Friedman, R.. Olshen and C.. Stone “Classification and
    Regression Trees” Belmont, CA: Wadsworh, Inc., 1984'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Breiman, J.. Friedman, R.. Olshen 和 C.. Stone “分类与回归树” Belmont, CA:
    Wadsworh, Inc.，1984'
- en: '[7] L. Burgette and J.. Reiter “Multiple imputation via sequential regression
    trees” In *American Journal of Epidemiology* 172, 2010, pp. 1070–1076'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] L. Burgette 和 J.. Reiter “通过顺序回归树的多重插补” 见 *美国流行病学杂志* 172，2010，第 1070–1076
    页'
- en: '[8] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li and Yitan Li “BRITS: Bidirectional
    recurrent imputation for time series” In *Advances in Neural Information Processing
    Systems*, 2018, pp. 6775–6785'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li 和 Yitan Li “BRITS：双向递归插补时间序列”
    见 *神经信息处理系统进展*，2018，第 6775–6785 页'
- en: '[9] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag and Yan
    Liu “Recurrent neural networks for multivariate time series with missing values”
    In *Scientific Reports* 8.1 Nature Publishing Group, 2018, pp. 1–12'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag 和 Yan Liu
    “用于多变量时间序列缺失值的递归神经网络” 见 *科学报告* 8.1 Nature Publishing Group，2018，第 1–12 页'
- en: '[10] Sixia Chen and David Haziza “Recent developments in dealing with item
    non-response in surveys: a critical review” In *International Statistical Review*
    87 Wiley Online Library, 2019, pp. S192–S218'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Sixia Chen 和 David Haziza “处理调查中项目非响应的近期进展：批判性回顾” 见 *国际统计评论* 87 Wiley
    Online Library，2019，第 S192–S218 页'
- en: '[11] Edith D De Leeuw, Joop Hox and Mark Huisman “Prevention and treatment
    of item nonresponse” In *Journal of Official Statistics-Stockholm* 19.2 ALMQVIST
    & WIKSELL INTERNATIONAL, 2003, pp. 153–176'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Edith D De Leeuw, Joop Hox 和 Mark Huisman “项目缺失的预防与治疗” 见 *官方统计学期刊-斯德哥尔摩*
    19.2 ALMQVIST & WIKSELL INTERNATIONAL，2003，第 153–176 页'
- en: '[12] L.L. Doove, S. Van Buuren and E. Dusseldorp “Recursive partitioning for
    missing data imputation in the presence of interaction effects” In *Computational
    Statistics & Data Analysis* 72, 2014, pp. 92–104'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L.L. Doove, S. Van Buuren 和 E. Dusseldorp “在存在交互效应的情况下，缺失数据插补的递归分区” 见
    *计算统计与数据分析* 72，2014，第 92–104 页'
- en: '[13] Dheeru Dua and Casey Graff “UCI Machine Learning Repository”, 2017 URL:
    [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Dheeru Dua 和 Casey Graff “UCI 机器学习库”，2017 网址: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)'
- en: '[14] Vincent Fortuin, Dmitry Baranchuk, Gunnar Rätsch and Stephan Mandt “GP-VAE:
    Deep probabilistic time series imputation” In *International Conference on Artificial
    Intelligence and Statistics*, 2020, pp. 1651–1661 PMLR'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Vincent Fortuin, Dmitry Baranchuk, Gunnar Rätsch 和 Stephan Mandt “GP-VAE：深度概率时间序列插补”
    见 *国际人工智能与统计会议*，2020，第 1651–1661 页 PMLR'
- en: '[15] A. Gelman and T. Speed “Characterizing a joint probability distribution
    by conditionals” In *Journal of the Royal Statistical Society Series B: Statistical
    Methodology* 55, 1993, pp. 185–188'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Gelman 和 T. Speed “通过条件来表征联合概率分布” 见 *皇家统计学会B系列：统计方法学期刊* 55，1993，第 185–188
    页'
- en: '[16] Xavier Glorot and Yoshua Bengio “Understanding the Difficulty of Training
    Deep Feedforward Neural Networks.” In *Artificial Intelligence and Statistics*
    9, 2010, pp. 249–256'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Xavier Glorot 和 Yoshua Bengio “理解训练深度前馈神经网络的困难” 见 *人工智能与统计学* 9，2010，第
    249–256 页'
- en: '[17] Lovedeep Gondara and Ke Wang “MIDA: Multiple imputation using denoising
    autoencoders” In *Pacific-Asia Conference on Knowledge Discovery and Data Mining*,
    2018, pp. 260–272 Springer'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Lovedeep Gondara 和 Ke Wang “MIDA：使用去噪自编码器的多重插补” 见 *太平洋亚洲知识发现与数据挖掘会议*，2018，第
    260–272 页 Springer'
- en: '[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville and Yoshua Bengio “Generative adversarial nets”
    In *Advances in Neural Information Processing Systems*, 2014, pp. 2672–2680'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville 和 Yoshua Bengio “生成对抗网络” 见 *神经信息处理系统进展*，2014，第
    2672–2680 页'
- en: '[19] Hyungrok Ham, Tae Joon Jun and Daeyoung Kim “Unbalanced gans: Pre-training
    the generator of generative adversarial network using variational autoencoder”
    In *arXiv preprint arXiv:2002.02112*, 2020'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Hyungrok Ham, Tae Joon Jun 和 Daeyoung Kim “不平衡生成对抗网络：使用变分自编码器预训练生成器” 见
    *arXiv 预印本 arXiv:2002.02112*，2020'
- en: '[20] Ofer Harel and Xiao-Hua Zhou “Multiple imputation: Review of theory, implementation
    and software” In *Statistics in Medicine* 26.16 Wiley Online Library, 2007, pp.
    3057–3077'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Ofer Harel 和 Xiao-Hua Zhou “多重插补：理论、实现与软件综述” 见 *医学统计学* 26.16 Wiley Online
    Library，2007，第 3057–3077 页'
- en: '[21] Trevor Hastie, Robert Tibshirani and Jerome Friedman “The elements of
    statistical learning: data mining, inference and prediction” Springer, 2009'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Trevor Hastie, Robert Tibshirani 和 Jerome Friedman “统计学习的要素：数据挖掘、推断与预测”
    Springer，2009'
- en: '[22] David Haziza and Audrey-Anne Vallée “Variance estimation procedures in
    the presence of singly imputed survey data: a critical review” In *Japanese Journal
    of Statistics and Data Science* 3.2 Springer, 2020, pp. 583–623'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] David Haziza 和 Audrey-Anne Vallée “在单次填补调查数据存在的情况下的方差估计程序：一项批判性回顾” 见 *Japanese
    Journal of Statistics and Data Science* 3.2 Springer，2020年，第583–623页'
- en: '[23] James Honaker, Gary King and Matthew Blackwell “Amelia II: A program for
    missing data” In *Journal of Statistical Software* 45.7, 2011, pp. 1–47'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] James Honaker, Gary King 和 Matthew Blackwell “Amelia II：一种处理缺失数据的程序” 见
    *Journal of Statistical Software* 45.7，2011年，第1–47页'
- en: '[24] Nicholas J Horton, Stuart R Lipsitz and Michael Parzen “A potential for
    bias when rounding in multiple imputation” In *The American Statistician* 57.4
    Taylor & Francis, 2003, pp. 229–232'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Nicholas J Horton, Stuart R Lipsitz 和 Michael Parzen “在多重填补中舍入时的偏差潜力”
    见 *The American Statistician* 57.4 Taylor & Francis，2003年，第229–232页'
- en: '[25] Md Hamidul Huque, John B Carlin, Julie A Simpson and Katherine J Lee “A
    comparison of multiple imputation methods for missing data in longitudinal studies”
    In *BMC medical research methodology* 18.1 BioMed Central, 2018, pp. 1–16'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Md Hamidul Huque, John B Carlin, Julie A Simpson 和 Katherine J Lee “长期研究中缺失数据的多重填补方法比较”
    见 *BMC medical research methodology* 18.1 BioMed Central，2018年，第1–16页'
- en: '[26] Diederik Kingma and Jimmy Ba “Adam: A Method for Stochastic Optimization”
    In *arXiv:1412.6980*, 2014'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Diederik Kingma 和 Jimmy Ba “Adam：一种用于随机优化的方法” 见 *arXiv:1412.6980*，2014年'
- en: '[27] F Li, Y Yu and DB Rubin “Imputing missing data by fully conditional models:
    Some cautionary examples and guidelines”, 2012, pp. 1–35'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] F Li, Y Yu 和 DB Rubin “通过完全条件模型填补缺失数据：一些警示性示例和指导方针”，2012年，第1–35页'
- en: '[28] Fan Li, Michela Baccini, Fabrizia Mealli, Elizabeth R. Zell, Constantine
    E. Frangakis and Donald B. Rubin “Multiple Imputation by Ordered Monotone Blocks
    With Application to the Anthrax Vaccine Research Program” In *Journal of Computational
    and Graphical Statistics* 23.3 Taylor & Francis, 2014, pp. 877–892'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Fan Li, Michela Baccini, Fabrizia Mealli, Elizabeth R. Zell, Constantine
    E. Frangakis 和 Donald B. Rubin “通过有序单调块进行多重填补，并应用于炭疽疫苗研究计划” 见 *Journal of Computational
    and Graphical Statistics* 23.3 Taylor & Francis，2014年，第877–892页'
- en: '[29] Zachary C Lipton, David C Kale and Randall Wetzel “Modeling missing data
    in clinical time series with RNNs” In *Machine Learning for Healthcare* 56, 2016'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Zachary C Lipton, David C Kale 和 Randall Wetzel “使用RNNs对临床时间序列中的缺失数据建模”
    见 *Machine Learning for Healthcare* 56，2016年'
- en: '[30] Roderick JA Little and Donald B Rubin “Statistical Analysis with Missing
    Data” Hoboken, NJ: John Wiley & Sons, 2014'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Roderick JA Little 和 Donald B Rubin “缺失数据的统计分析” Hoboken, NJ: John Wiley
    & Sons，2014年'
- en: '[31] Roderick JA Little and Donald B Rubin “Statistical Analysis with Missing
    Data, 3rd Edition” New York: John Wiley & Sons, 2019'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Roderick JA Little 和 Donald B Rubin “缺失数据的统计分析，第3版” New York: John Wiley
    & Sons，2019年'
- en: '[32] Haw-minn Lu, Giancarlo Perrone and José Unpingco “Multiple imputation
    with denoising autoencoder using metamorphic truth and imputation feedback” In
    *arXiv preprint arXiv:2002.08338*, 2020'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Haw-minn Lu, Giancarlo Perrone 和 José Unpingco “使用变换真实值和填补反馈的去噪自编码器进行多重填补”
    见 *arXiv预印本 arXiv:2002.08338*，2020年'
- en: '[33] Andrew L Maas, Awni Y Hannun and Andrew Y Ng “Rectifier nonlinearities
    improve neural network acoustic models” In *Proc. ICML*, 30 1, 2013, pp. 3'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Andrew L Maas, Awni Y Hannun 和 Andrew Y Ng “整流非线性改进神经网络声学模型” 见 *Proc.
    ICML*，30 1，2013年，第3页'
- en: '[34] D Manrique-Vallier and J Reiter “Bayesian estimation of discrete multivariate
    truncated latent structure models” In *Journal of Computational and Graphical
    Statistics* 23, 2014, pp. 1061–1079'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D Manrique-Vallier 和 J Reiter “离散多元截断潜变量结构模型的贝叶斯估计” 见 *Journal of Computational
    and Graphical Statistics* 23，2014年，第1061–1079页'
- en: '[35] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S., Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,
    Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
    Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat
    Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens,
    Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
    Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
    Martin Wicke, Yuan Yu and Xiaoqiang Zheng “TensorFlow: Large-Scale Machine Learning
    on Heterogeneous Systems” Software available from tensorflow.org, 2015 URL: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S., Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,
    Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
    Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat
    Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens,
    Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
    Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
    Martin Wicke, Yuan Yu 和 Xiaoqiang Zheng “TensorFlow：在异构系统上进行大规模机器学习” 软件可从 tensorflow.org
    获取，2015年 URL: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
- en: '[36] Federico Monti, Michael Bronstein and Xavier Bresson “Geometric matrix
    completion with recurrent multi-graph neural networks” In *Advances in Neural
    Information Processing Systems*, 2017, pp. 3697–3707'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Federico Monti, Michael Bronstein 和 Xavier Bresson “使用递归多图神经网络的几何矩阵完成”
    发表在 *神经信息处理系统进展*，2017年，第3697–3707页'
- en: '[37] Jared S Murray and Jerome P Reiter “Multiple imputation of missing categorical
    and continuous values via Bayesian mixture models with local dependence” In *Journal
    of the American Statistical Association* 111.516 Taylor & Francis, 2016, pp. 1466–1479'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Jared S Murray 和 Jerome P Reiter “通过具有局部依赖性的贝叶斯混合模型对缺失的分类和连续值进行多重插补” 发表在
    *美国统计协会杂志* 111.516 Taylor & Francis，2016年，第1466–1479页'
- en: '[38] Trivellore E Raghunathan, James M Lepkowski, John Van Hoewyk and Peter
    Solenberger “A multivariate technique for multiply imputing missing values using
    a sequence of regression models” In *Survey Methodology* 27.1, 2001, pp. 85–96'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Trivellore E Raghunathan, James M Lepkowski, John Van Hoewyk 和 Peter Solenberger
    “使用回归模型序列进行多重插补的多变量技术” 发表在 *调查方法学* 27.1，2001年，第85–96页'
- en: '[39] Jerome P Reiter and Trivellore E Raghunathan “The multiple adaptations
    of multiple imputation” In *Journal of the American Statistical Association* 102.480
    Taylor & Francis, 2007, pp. 1462–1471'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Jerome P Reiter 和 Trivellore E Raghunathan “多重插补的多重适应” 发表在 *美国统计协会杂志*
    102.480 Taylor & Francis，2007年，第1462–1471页'
- en: '[40] Patrick Royston and Ian R White “Multiple imputation by chained equations
    (MICE): implementation in Stata” In *J Statistical Software* 45.4, 2011, pp. 1–20'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Patrick Royston 和 Ian R White “链式方程的多重插补（MICE）：在 Stata 中的实现” 发表在 *统计软件杂志*
    45.4，2011年，第1–20页'
- en: '[41] D.. Rubin “Inference and Missing data (with discussion)” In *Biometrika*
    63, 1976, pp. 581–592'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] D.. Rubin “推断与缺失数据（附讨论）” 发表在 *生物统计学* 63，1976年，第581–592页'
- en: '[42] D.. Rubin “Multiple Imputation for Nonresponse in Surveys” New York: John
    Wiley & Sons, 1987, pp. 258'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] D.. Rubin “调查中的非响应多重插补” 纽约：John Wiley & Sons，1987年，第258页'
- en: '[43] Donald B Rubin “Multiple imputation after 18+ years” In *Journal of the
    American Statistical Association* 91.434 Taylor & Francis Group, 1996, pp. 473–489'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Donald B Rubin “多重插补18+年后” 发表在 *美国统计协会杂志* 91.434 Taylor & Francis Group，1996年，第473–489页'
- en: '[44] Joseph L Schafer “Analysis of Incomplete Multivariate Data” Chapman &
    Hall: London, 1997'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Joseph L Schafer “不完整多变量数据分析” Chapman & Hall：伦敦，1997年'
- en: '[45] Anoop Shah, Jonathan Bartlett, James Carpenter, Owen Nicholas and Harry
    Hemingway “Comparison of random forest and parametric imputation models for imputing
    missing data using MICE: A CALIBER study” In *American Journal of Epidemiology*
    179, 2014, pp. 764–74'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Anoop Shah, Jonathan Bartlett, James Carpenter, Owen Nicholas 和 Harry
    Hemingway “使用 MICE 对缺失数据进行插补时随机森林与参数插补模型的比较：CALIBER 研究” 发表在 *美国流行病学杂志* 179，2014年，第764–74页'
- en: '[46] Daniel J Stekhoven and Peter Bühlmann “MissForest—non-parametric missing
    value imputation for mixed-type data” In *Bioinformatics* 28.1 Oxford University
    Press, 2012, pp. 112–118'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Daniel J Stekhoven 和 Peter Bühlmann “MissForest—用于混合类型数据的非参数缺失值插补” 发表在
    *生物信息学* 28.1 牛津大学出版社，2012年，第112–118页'
- en: '[47] Yu-Sung Su, Andrew E Gelman, Jennifer Hill and Masanao Yajima “Multiple
    imputation with diagnostics (mi) in R: Opening windows into the black box” In
    *Journal of Statistical Software* 45, 2011'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Yu-Sung Su, Andrew E Gelman, Jennifer Hill 和 Masanao Yajima “R 中的多重插补与诊断（mi）：打开黑箱的窗户”
    发表在 *统计软件杂志* 45，2011年'
- en: '[48] Lingqi Tang, Juwon Song, Thomas R Belin and Jürgen Unützer “A comparison
    of imputation methods in a longitudinal randomized clinical trial” In *Statistics
    in medicine* 24.14 Wiley Online Library, 2005, pp. 2111–2128'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Lingqi Tang, Juwon Song, Thomas R Belin 和 Jürgen Unützer 《在一项纵向随机临床试验中插补方法的比较》
    载于 *医学统计学* 24.14 Wiley Online Library，2005年，第2111–2128页'
- en: '[49] Tin Kam Ho “Random decision forests” In *Proceedings of 3rd International
    Conference on Document Analysis and Recognition* 1, 1995, pp. 278–282 vol.1'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Tin Kam Ho 《随机决策森林》 载于 *第三届国际文档分析与识别会议论文集* 1，1995年，第278–282页，第1卷'
- en: '[50] S. Buuren “Flexible Imputation of Missing Data”, Chapman & Hall/CRC Interdisciplinary
    Statistics CRC Press LLC, 2018'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Buuren 《缺失数据的灵活插补》，Chapman & Hall/CRC Interdisciplinary Statistics
    CRC Press LLC，2018'
- en: '[51] S. Buuren, J… Brand, C… Groothuis-Oudshoorn and D.. Rubin “Fully conditional
    specification in multivariate imputation” In *Journal of Statistical Computation
    and Simulation* 76.12 Taylor & Francis, 2006, pp. 1049–1064'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. Buuren, J… Brand, C… Groothuis-Oudshoorn 和 D.. Rubin 《多变量插补中的完全条件规范》
    载于 *统计计算与模拟杂志* 76.12 Taylor & Francis，2006年，第1049–1064页'
- en: '[52] Stef Buuren and Karin Groothuis-Oudshoorn “mice: Multivariate Imputation
    by Chained Equations in R” In *Journal of Statistical Software* 45.3, 2011, pp.
    1–67'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Stef Buuren 和 Karin Groothuis-Oudshoorn 《mice：R中的多变量链式方程插补》 载于 *统计软件杂志*
    45.3，2011年，第1–67页'
- en: '[53] Pascal Vincent, Hugo Larochelle, Yoshua Bengio and Pierre-Antoine Manzagol
    “Extracting and composing robust features with denoising autoencoders” In *Proceedings
    of the 25th international conference on Machine learning*, 2008, pp. 1096–1103'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Pascal Vincent, Hugo Larochelle, Yoshua Bengio 和 Pierre-Antoine Manzagol
    《使用去噪自编码器提取和组合鲁棒特征》 载于 *第25届国际机器学习大会论文集*，2008年，第1096–1103页'
- en: '[54] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine
    Manzagol and Léon Bottou “Stacked denoising autoencoders: Learning useful representations
    in a deep network with a local denoising criterion.” In *Journal of Machine Learning
    Research* 11.12, 2010'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine
    Manzagol 和 Léon Bottou 《堆叠去噪自编码器：在具有局部去噪标准的深度网络中学习有用的表示》 载于 *机器学习研究杂志* 11.12，2010年'
- en: '[55] Ian R. White, Patrick Royston and Angela M. Wood “Multiple imputation
    using chained equations: Issues and guidance for practice” In *Statistics in Medicine*
    30.4 John Wiley & Sons, Ltd., 2011, pp. 377–399'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Ian R. White, Patrick Royston 和 Angela M. Wood 《使用链式方程的多重插补：问题与实践指导》 载于
    *医学统计学* 30.4 John Wiley & Sons, Ltd.，2011年，第377–399页'
- en: '[56] Chayut Wongkamthong and Olanrewaju Akande “A Comparative Study of Imputation
    Methods for Multivariate Ordinal Data” In *Journal of Survey Statistics and Methodology*
    in press, 2021'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Chayut Wongkamthong 和 Olanrewaju Akande 《多变量有序数据插补方法的比较研究》 载于 *调查统计与方法学杂志*，已接受，2021年'
- en: '[57] Jinsung Yoon, James Jordon and Mihaela Schaar “Gain: Missing data imputation
    using generative adversarial nets” In *International conference on machine learning*,
    2018, pp. 5689–5698 PMLR'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Jinsung Yoon, James Jordon 和 Mihaela Schaar 《Gain：使用生成对抗网络的缺失数据插补》 载于
    *国际机器学习大会*，2018年，第5689–5698页 PMLR'
- en: '[58] Jinsung Yoon, William R Zame and Mihaela Schaar “Estimating missing data
    in temporal data streams using multi-directional recurrent neural networks” In
    *IEEE Transactions on Biomedical Engineering* 66.5 IEEE, 2018, pp. 1477–1490'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Jinsung Yoon, William R Zame 和 Mihaela Schaar 《使用多方向递归神经网络估计时间数据流中的缺失数据》
    载于 *IEEE生物医学工程汇刊* 66.5 IEEE，2018年，第1477–1490页'
- en: '[59] Yang Yuan “Multiple imputation using SAS software” In *Journal of Statistical
    Software* 45.6, 2011, pp. 1–25'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Yang Yuan 《使用SAS软件的多重插补》 载于 *统计软件杂志* 45.6，2011年，第1–25页'
