- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:36:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:36:30'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2310.09411] Surveying the Landscape of Text Summarization with Deep Learning:
    A Comprehensive Review'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2310.09411] 深度学习下的文本摘要现状调查：全面回顾'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.09411](https://ar5iv.labs.arxiv.org/html/2310.09411)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.09411](https://ar5iv.labs.arxiv.org/html/2310.09411)
- en: \catchline
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \catchline
- en: 'Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive
    Review'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习下的文本摘要现状调查：全面回顾
- en: Guanghua Wang    Weili Wu [guanghua.wang@utdallas.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 王光华    吴伟力 [guanghua.wang@utdallas.edu](mailto:guanghua.wang@utdallas.edu)
- en: weiliwu@utdallas.edu](mailto:guanghua.wang@utdallas.edu) Computer Science Department,
    The University of Texas at Dallas, 800 W, Campbell Road
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: weiliwu@utdallas.edu](mailto:guanghua.wang@utdallas.edu) 德克萨斯大学达拉斯分校计算机科学系，800
    W，坎贝尔路
- en: Richardson, 75080-3021, United States (Day Month Year; Day Month Year; Day Month
    Year)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 理查森，75080-3021，美国（日期；日期；日期）
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, deep learning has revolutionized natural language processing
    (NLP) by enabling the development of models that can learn complex representations
    of language data, leading to significant improvements in performance across a
    wide range of NLP tasks. Deep learning models for NLP typically use large amounts
    of data to train deep neural networks, allowing them to learn the patterns and
    relationships in language data. This is in contrast to traditional NLP approaches,
    which rely on hand-engineered features and rules to perform NLP tasks. The ability
    of deep neural networks to learn hierarchical representations of language data,
    handle variable-length input sequences, and perform well on large datasets makes
    them well-suited for NLP applications. Driven by the exponential growth of textual
    data and the increasing demand for condensed, coherent, and informative summaries,
    text summarization has been a critical research area in the field of NLP. Applying
    deep learning to text summarization refers to the use of deep neural networks
    to perform text summarization tasks. In this survey, we begin with a review of
    fashionable text summarization tasks in recent years, including extractive, abstractive,
    multi-document, and so on. Next, we discuss most deep learning-based models and
    their experimental results on these tasks. The paper also covers datasets and
    data representation for summarization tasks. Finally, we delve into the opportunities
    and challenges associated with summarization tasks and their corresponding methodologies,
    aiming to inspire future research efforts to advance the field further. A goal
    of our survey is to explain how these methods differ in their requirements as
    understanding them is essential for choosing a technique suited for a specific
    setting. This survey aims to provide a comprehensive review of existing techniques,
    evaluation methodologies, and practical applications of automatic text summarization.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习通过使得可以学习复杂的语言数据表示的模型的发展，彻底改变了自然语言处理（NLP）领域，从而在各种NLP任务的性能上取得了显著的改进。用于NLP的深度学习模型通常使用大量的数据来训练深度神经网络，使其能够学习语言数据中的模式和关系。这与传统的NLP方法形成了对比，后者依赖于手工设计的特征和规则来执行NLP任务。深度神经网络能够学习语言数据的层次表示、处理可变长度的输入序列，并在大数据集上表现良好的能力，使其非常适合NLP应用。由于文本数据的指数增长以及对凝练、连贯和信息丰富的摘要的需求不断增加，文本摘要已成为NLP领域的重要研究领域。将深度学习应用于文本摘要指的是使用深度神经网络来执行文本摘要任务。在这项调查中，我们首先回顾了近年来流行的文本摘要任务，包括抽取式、生成式、多文档等。接下来，我们讨论了大多数基于深度学习的模型及其在这些任务上的实验结果。本文还涵盖了摘要任务的数据集和数据表示。最后，我们深入探讨了与摘要任务及其相应方法相关的机遇和挑战，旨在激发未来研究工作，进一步推动该领域的发展。我们调查的目标是解释这些方法在需求上的差异，因为理解这些差异对于选择适合特定场景的技术至关重要。该调查旨在提供对现有技术、评估方法和自动文本摘要实际应用的全面回顾。
- en: 'keywords:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '关键词:'
- en: Natural language processing; Deep neural network; Text Summarization; Extractive
    Summarization; Abstractive Summarization; Multi-document Summarization{history}\published
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理；深度神经网络；文本摘要；抽取式摘要；生成式摘要；多文档摘要{历史}\已发布
- en: Day Month Year
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 日期
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 1.1 Overview of Deep Learning for Text Summarization
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 深度学习在文本摘要中的概述
- en: Text summarization is the process of reducing a text or multiple texts to their
    essential meaning or main points while preserving its overall meaning and tone
    [[106](#bib.bib106)]. It has a wide range of applications, from creating news
    headlines and abstracts to summarizing legal documents and scientific papers.
    One common application of summarization is in news aggregation, where summaries
    of news articles are provided to users to consume news content quickly and efficiently
    [[12](#bib.bib12), [88](#bib.bib88)]. Another important application of summarization
    is in the legal industry [[77](#bib.bib77), [55](#bib.bib55)], where lawyers may
    need to quickly review large amounts of legal documents to identify relevant information.
    Summarization can also be used in healthcare to summarize medical records [[3](#bib.bib3),
    [6](#bib.bib6)], which can help doctors and other healthcare professionals make
    more informed decisions. In the meanwhile, summarization is able to summarize
    social media posts for busy readers who want to stay informed but do not have
    time to read the entire document [[33](#bib.bib33), [122](#bib.bib122)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要是将文本或多个文本简化为其核心意义或要点的过程，同时保持其整体意义和语调 [[106](#bib.bib106)]。它有广泛的应用范围，从创建新闻标题和摘要到总结法律文件和科学论文。总结的一个常见应用是在新闻聚合中，向用户提供新闻文章的摘要，以便快速有效地获取新闻内容
    [[12](#bib.bib12), [88](#bib.bib88)]。摘要的另一个重要应用是在法律行业 [[77](#bib.bib77), [55](#bib.bib55)]，律师可能需要快速审查大量法律文件以识别相关信息。摘要还可以用于医疗保健领域，总结病历
    [[3](#bib.bib3), [6](#bib.bib6)]，这有助于医生和其他医疗专业人员做出更明智的决策。同时，摘要能够总结社交媒体帖子，以便忙碌的读者在没有时间阅读完整文件的情况下保持信息更新
    [[33](#bib.bib33), [122](#bib.bib122)]。
- en: Traditional summarization models typically include rule-based ones or statistical
    techniques that focus on identifying key phrases and sentences from the source
    text without relying on deep learning or complex language models. Traditional
    methods have been widely used and provide a foundation for understanding the summarization
    task. One of them is the keyword-based method, which focuses on identifying keywords
    within the text and using them to select or rank sentences. Common techniques
    contain Term Frequency-Inverse Document Frequency (TF-IDF) weighting [[32](#bib.bib32),
    [85](#bib.bib85)], where sentences with a high concentration of important keywords
    are considered more relevant. Another approach is the Heuristic method, which
    relies on predefined rules or heuristics, such as considering sentence position,
    length, or similarity to the title, to determine the important sentences. For
    example, the Lead method [[17](#bib.bib17), [183](#bib.bib183)] selects the first
    few sentences of a document, assuming that they contain the most critical information.
    Besides, graph-based systems represent the document as a graph, where nodes correspond
    to sentences and edges represent the relationships or similarities between them.
    Algorithms like PageRank [[115](#bib.bib115), [18](#bib.bib18)] or LexRank [[46](#bib.bib46)]
    are used to identify the most important nodes (sentences) in the graph, which
    are then included in the summary. Latent Semantic Analysis (LSA) [[139](#bib.bib139),
    [164](#bib.bib164)], on the other hand, is a statistical method that aims to capture
    the underlying semantic structure of a document by reducing its dimensionality.
    LSA is applied to a term-sentence matrix, and Singular Value Decomposition (SVD)
    is used to identify the most significant concepts or topics. Sentences that best
    represent these concepts are selected for the summary. The SumBasic algorithm
    [[132](#bib.bib132)] calculates the probability of a word appearing in a summary
    based on its frequency in the document. Sentences are scored by averaging the
    probabilities of their words, and the highest-scoring sentences are chosen for
    the summary. This method is simple but can yield reasonably good results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的总结模型通常包括基于规则的方法或统计技术，这些方法专注于从源文本中识别关键短语和句子，而不依赖于深度学习或复杂的语言模型。传统方法被广泛使用，并为理解总结任务提供了基础。其中一种是基于关键词的方法，专注于识别文本中的关键词，并使用它们来选择或排名句子。常见技术包括词频-逆文档频率（TF-IDF）加权[[32](#bib.bib32)、[85](#bib.bib85)]，其中关键词浓度高的句子被认为更相关。另一种方法是启发式方法，它依赖于预定义的规则或启发式方法，如考虑句子位置、长度或与标题的相似性，以确定重要句子。例如，首段法[[17](#bib.bib17)、[183](#bib.bib183)]选择文档的前几句，假设它们包含最关键的信息。此外，基于图的系统将文档表示为一个图，其中节点对应句子，边表示它们之间的关系或相似性。像PageRank[[115](#bib.bib115)、[18](#bib.bib18)]或LexRank[[46](#bib.bib46)]这样的算法用于识别图中最重要的节点（句子），然后将其包含在总结中。另一方面，潜在语义分析（LSA）[[139](#bib.bib139)、[164](#bib.bib164)]是一种统计方法，旨在通过减少文档的维度来捕捉文档的潜在语义结构。LSA应用于词汇-句子矩阵，并使用奇异值分解（SVD）来识别最重要的概念或主题。最能代表这些概念的句子被选入总结中。SumBasic算法[[132](#bib.bib132)]基于词在文档中的频率计算其出现在总结中的概率。通过平均句子中单词的概率来评分，得分最高的句子被选择为总结内容。这种方法简单，但可以产生相当不错的结果。
- en: However, the capacity of traditional methods to produce organized and smooth
    summaries or adjust to diverse fields or dialects is frequently deficient. These
    methods are often simpler and faster than modern approaches, but they may not
    be as effective or accurate in capturing the nuances of the source text. Most
    of them are mainly focused on extractive summarization, which involves selecting
    the most important and relevant sentences or phrases from the original document
    to create a concise summary. Due to the complexity of generating new text, traditional
    methods are less common in abstractive summarization, which aims to generate a
    condensed version of the source text by rephrasing and reorganizing the original
    content instead of merely extracting existing sentences.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，传统方法在生成有组织和流畅的总结或适应不同领域或方言方面的能力常常不足。这些方法通常比现代方法更简单、更快捷，但可能无法有效或准确地捕捉源文本的细微差别。它们大多专注于抽取式总结，即从原始文档中选择最重要和相关的句子或短语以创建简洁的总结。由于生成新文本的复杂性，传统方法在生成式总结中较少见，生成式总结旨在通过重新表述和重组原始内容而不是仅仅提取现有句子来生成源文本的浓缩版本。
- en: Nowadays, neural networks with multiple layers [[125](#bib.bib125), [31](#bib.bib31),
    [161](#bib.bib161)] enable the development of models that can understand, generate,
    and manipulate natural language from language data. Deep neural networks have
    demonstrated significant improvement in the performance of summarization tasks
    [[39](#bib.bib39), [91](#bib.bib91)], especially when compared to traditional
    statistical and traditional machine learning approaches. Deep learning models
    can learn from large amounts of data and generate more accurate predictions by
    capturing complex patterns and relationships in language data [[151](#bib.bib151)].
    It can also handle the complexity and variability of natural language inputs,
    such as variable-length sequences of words and sentences. This allows the model
    to capture long-range dependencies and context, which are critical for understanding
    the meaning of a sentence or document. On the other hand, deep learning can also
    learn representations of language data end-to-end [[167](#bib.bib167)], without
    relying on hand-engineered features or rules. This approach enables the same model
    to be used for different tasks with minor modifications. With the large, general-purpose
    datasets [[125](#bib.bib125)] and high-performance computation ability [[34](#bib.bib34)]
    in recent years, deep learning can use pre-trained models as a starting point
    for a new summarization task with limited annotated data. The rapid advancements
    in deep learning with new architectures and techniques have led to a steady stream
    of innovations in summarization, which has pushed the state-of-the-art in language
    understanding and generation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，多层神经网络[[125](#bib.bib125), [31](#bib.bib31), [161](#bib.bib161)]使得可以开发能够理解、生成和处理自然语言的模型。深度神经网络在摘要任务的性能上显示出了显著的改进[[39](#bib.bib39),
    [91](#bib.bib91)]，尤其是与传统的统计方法和机器学习方法相比。深度学习模型可以从大量数据中学习，通过捕捉语言数据中的复杂模式和关系来生成更准确的预测[[151](#bib.bib151)]。它还可以处理自然语言输入的复杂性和变异性，如可变长度的单词和句子序列。这使得模型能够捕捉长程依赖和上下文，这对理解句子或文档的意义至关重要。另一方面，深度学习还可以端到端地学习语言数据的表示[[167](#bib.bib167)]，无需依赖手工设计的特征或规则。这种方法使得同一模型可以在不同任务中进行小幅修改而使用。凭借近年来大规模的通用数据集[[125](#bib.bib125)]和高性能计算能力[[34](#bib.bib34)]，深度学习可以利用预训练模型作为新的摘要任务的起点，即使只有有限的标注数据。深度学习在新架构和技术的快速发展中推动了摘要的不断创新，这也推动了语言理解和生成的最先进水平。
- en: 'Numerous research papers have been published on the subject of text summarization
    in conjunction with deep learning. However, these papers vary in scope and focus:
    some primarily address prevalent models [[170](#bib.bib170), [103](#bib.bib103),
    [74](#bib.bib74), [5](#bib.bib5), [64](#bib.bib64), [15](#bib.bib15)], while others
    discuss the applications of summarization tasks [[2](#bib.bib2), [58](#bib.bib58),
    [123](#bib.bib123), [190](#bib.bib190)]. Some papers cover both aspects without
    delving into the datasets associated with text summarization [[128](#bib.bib128),
    [56](#bib.bib56)]. Additionally, certain papers only review a specific sub-field
    of summarization [[4](#bib.bib4), [130](#bib.bib130), [66](#bib.bib66), [49](#bib.bib49),
    [120](#bib.bib120), [97](#bib.bib97), [78](#bib.bib78), [181](#bib.bib181)]. This
    paper aims to provide a comprehensive overview of deep learning techniques for
    text summarization. This encompasses the key text summarization tasks and their
    historical context, widely adopted models, and beneficial techniques. Furthermore,
    a comparative analysis of performance across various models will be presented,
    followed by the prospects for their application. These aspects will be discussed
    in the subsequent sections.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究论文已经发表，涉及深度学习与文本摘要的主题。然而，这些论文在范围和重点上各不相同：一些主要讨论流行的模型[[170](#bib.bib170),
    [103](#bib.bib103), [74](#bib.bib74), [5](#bib.bib5), [64](#bib.bib64), [15](#bib.bib15)]，而另一些则探讨了摘要任务的应用[[2](#bib.bib2),
    [58](#bib.bib58), [123](#bib.bib123), [190](#bib.bib190)]。还有一些论文同时涵盖了这两个方面，但没有深入探讨与文本摘要相关的数据集[[128](#bib.bib128),
    [56](#bib.bib56)]。此外，某些论文仅回顾了摘要的特定子领域[[4](#bib.bib4), [130](#bib.bib130), [66](#bib.bib66),
    [49](#bib.bib49), [120](#bib.bib120), [97](#bib.bib97), [78](#bib.bib78), [181](#bib.bib181)]。本文旨在提供关于深度学习技术在文本摘要中的全面概述。这包括关键的文本摘要任务及其历史背景、广泛采用的模型和有益的技术。此外，将对不同模型的性能进行比较分析，并展望其应用前景。这些内容将在后续章节中讨论。
- en: 1.2 Paper Structure
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 论文结构
- en: 'The rest of the paper is structured as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分结构如下：
- en: $\blacktriangleright$
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: Section 2 presents a comprehensive review of different tasks and their brief
    histories in text summarization.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2节对文本摘要中的不同任务及其简要历史进行了全面回顾。
- en: $\blacktriangleright$
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: Section 3 describes some of the most popular deep neural network models and
    related techniques.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3节描述了一些最流行的深度神经网络模型及相关技术。
- en: $\blacktriangleright$
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: Section 4 reviews a recipe of datasets and shows how to quantify efficiency,
    and what factors to consider during the evaluation of each task.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4节回顾了数据集的使用方法，并展示了如何量化效率，以及在评估每项任务时需要考虑的因素。
- en: $\blacktriangleright$
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: Section 5 discusses the main challenges and future directions for text summarization
    with deep learning techniques.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5节讨论了使用深度学习技术进行文本摘要的主要挑战和未来方向。
- en: 2 Tasks in Summarization
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 摘要任务
- en: Summarization tasks can be classified into several categories based on different
    criteria, such as summarization method, source document quantity, source document
    length, summary length, and so on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要任务可以根据不同的标准进行分类，例如摘要方法、源文档数量、源文档长度、摘要长度等。
- en: '2.1 Summarization Method: Extractive vs Abstractive'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 摘要方法：抽取式与生成式
- en: Extractive and abstractive summarization are two primary approaches to text
    summarization. Extractive summarization aims to identify and select the most pertinent
    sentences or phrases from the original text [[108](#bib.bib108)], whereas abstractive
    summarization creates novel sentences that rephrase and consolidate the key concepts
    of the source document [[42](#bib.bib42)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 抽取式和生成式总结是文本摘要的两种主要方法。抽取式总结旨在从原文中识别并选择最相关的句子或短语[[108](#bib.bib108)]，而生成式总结则创造新的句子，重新表述并整合源文档的关键概念[[42](#bib.bib42)]。
- en: '![Refer to caption](img/a0a7e565049de19ead02a1d5b421d19f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a0a7e565049de19ead02a1d5b421d19f.png)'
- en: 'Figure 1: The generated summary presented is the output of a ”unilm-base-cased”
    model [[11](#bib.bib11)] that has been fine-tuned, whereas the extractive summary
    provided is the output of a ”distilbert-base-uncased” model [[157](#bib.bib157)]
    that has also undergone fine-tuning. Both models were trained on the CNN/Daily
    Mail dataset. [[38](#bib.bib38)]'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：生成的摘要是经过微调的”unilm-base-cased”模型[[11](#bib.bib11)]的输出，而提供的抽取式摘要是经过微调的”distilbert-base-uncased”模型[[157](#bib.bib157)]的输出。两个模型都在CNN/Daily
    Mail数据集上进行训练。[[38](#bib.bib38)]
- en: The underlying assumption of extractive summarization is that the original text
    contains sentences that are sufficiently informative and well-formed to be included
    in the summary directly. Extractive summarization typically relies on techniques
    such as scoring sentences based on their relevance, importance, or position within
    the source text. A graph-based ranking model for text processing, called TextRank
    [[115](#bib.bib115)], is inspired by the PageRank algorithm used in web search.
    The model represents the input as a graph, where nodes are sentences or words,
    and edges represent the similarity between nodes. The algorithm iteratively scores
    nodes based on their connections, with higher-scoring nodes considered more important.
    Furthermore, LexRank [[46](#bib.bib46)], which is based on the concept of eigenvector
    centrality in a graph representation of the original text, is another unsupervised
    method for extractive summarization. On the other hand, Maximal Marginal Relevance
    (MMR) [[23](#bib.bib23)] addresses the problem of redundancy of extractive summaries
    by selecting sentences that are both relevant to the query and diverse from the
    already-selected sentences in the summary. The MMR algorithm iteratively selects
    sentences based on the combination of query relevance and novelty, considering
    the content of previously chosen sentences.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 抽取式总结的基本假设是原文中包含足够信息且表达良好的句子，这些句子可以直接纳入摘要中。抽取式总结通常依赖于基于相关性、重要性或在源文本中的位置对句子进行评分等技术。一种用于文本处理的基于图的排名模型称为TextRank[[115](#bib.bib115)]，其灵感来源于用于网页搜索的PageRank算法。该模型将输入表示为图，其中节点是句子或单词，边表示节点之间的相似性。算法根据节点的连接情况进行迭代评分，得分较高的节点被认为更重要。此外，LexRank[[46](#bib.bib46)]基于图表示中特征向量中心性的概念，是另一种无监督的抽取式总结方法。另一方面，最大边际相关性（MMR）[[23](#bib.bib23)]通过选择与查询相关且与已选择摘要句子不同的句子，解决了抽取式摘要的冗余问题。MMR算法基于查询相关性和新颖性的组合迭代选择句子，考虑之前选择的句子的内容。
- en: Instead of merely selecting existing sentences, abstractive summarization creates
    new sentences that convey the key ideas in a more natural and fluid manner. This
    approach requires a deeper understanding of the text and more advanced natural
    language generation capabilities. Goldstein et al. [[59](#bib.bib59)] propose
    a sentence extraction method based on a linear combination of several feature
    functions, followed by a sentence fusion step to generate abstractive summaries.
    Banko et al. [[10](#bib.bib10)] also use statistical models for content selection
    and surface realization to generate more succinct summaries. With probabilities
    calculated for candidate summary terms and their likely sequencing, a search algorithm
    is used to find a near-optimal summary in their system.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象总结与其仅仅选择现有句子的不同之处在于，它创造了新的句子，以更自然和流畅的方式传达关键思想。这种方法需要对文本有更深入的理解，并且需要更先进的自然语言生成能力。Goldstein
    等人 [[59](#bib.bib59)] 提出了基于多个特征函数的线性组合的句子提取方法，然后进行句子融合步骤以生成抽象总结。Banko 等人 [[10](#bib.bib10)]
    也使用统计模型进行内容选择和表面实现，以生成更简洁的总结。通过计算候选总结词条及其可能的排序的概率，他们的系统使用搜索算法来寻找一个近似最优的总结。
- en: While extractive summarization can produce coherent and accurate summaries,
    it may be limited in terms of fluency and flexibility, as the selected sentences
    are directly taken from the source text and may not always fit together seamlessly.
    Abstractive summarization with traditional methods can produce more creative and
    tailored summaries but faces challenges of flexibility and expressiveness. With
    the advent of deep learning, neural network-based models like sequence-to-sequence
    (Seq2Seq) models [[31](#bib.bib31)], attention mechanisms [[155](#bib.bib155)],
    and transformers [[173](#bib.bib173), [91](#bib.bib91)] have significantly improved
    the performance of abstractive summarization by capturing complex patterns and
    semantic relationships within the original text, as well as extractive summarization.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提取式总结可以产生连贯准确的总结，但它可能在流畅性和灵活性方面存在局限，因为所选择的句子直接来自源文本，并且可能并不总是无缝地结合在一起。传统方法的抽象总结可以产生更具创意和定制的总结，但面临灵活性和表现力的挑战。随着深度学习的出现，基于神经网络的模型如序列到序列（Seq2Seq）模型
    [[31](#bib.bib31)]、注意力机制 [[155](#bib.bib155)] 和变换器 [[173](#bib.bib173), [91](#bib.bib91)]
    通过捕捉原始文本中的复杂模式和语义关系，显著提高了抽象总结的性能，以及提取式总结。
- en: '2.2 Source Document Quantity: Single-document vs Multi-document'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 源文档数量：单文档与多文档
- en: Single-document summarization and multi-document summarization are two distinct
    tasks within text summarization domain based on source document quantity. Single-document
    summarization focuses on generating a summary from a single input document, while
    multi-document summarization aims to create a summary by aggregating information
    from multiple related documents [[111](#bib.bib111), [149](#bib.bib149), [41](#bib.bib41)].
    Within the single-document system, the objective is to condense the main ideas
    and essential information contained in that specific document. On the other hand,
    multi-document summarization tasks require identifying and combining the most
    relevant and non-redundant information from a set of documents, often covering
    the same topic or event. This means multi-document summarization has additional
    challenges, such as maintaining cross-document coherence, efficiently handling
    larger volumes of information, and processing redundancy across documents. These
    complexities make multi-document summarization generally more difficult than single-document
    summarization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 单文档总结和多文档总结是基于源文档数量的文本总结领域中的两个不同任务。单文档总结侧重于从单个输入文档中生成总结，而多文档总结则旨在通过聚合来自多个相关文档的信息来创建总结
    [[111](#bib.bib111), [149](#bib.bib149), [41](#bib.bib41)]。在单文档系统中，目标是浓缩该特定文档中包含的主要思想和基本信息。另一方面，多文档总结任务需要识别和结合一组文档中最相关和不冗余的信息，通常涵盖相同的主题或事件。这意味着多文档总结具有额外的挑战，例如保持跨文档的一致性、高效处理较大的信息量以及处理文档间的冗余。这些复杂性使得多文档总结通常比单文档总结更困难。
- en: Traditional approaches usually employ extractive techniques both in the context
    of single-document and multi-document summarization. Graph-based methods can be
    applied to both single-document and multi-document summarization by representing
    the relationships between sentences in one document or several documents as a
    graph, with sentences as nodes and the edges as the similarity between the sentences.
    The systems [[171](#bib.bib171), [96](#bib.bib96), [141](#bib.bib141), [46](#bib.bib46)]
    then use algorithms like PageRank, HITS, or LexRank to identify the most important
    sentences in the graph, which are then extracted and combined to form the summary.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法通常在单文档和多文档摘要的背景下使用抽取技术。基于图的方法可以应用于单文档和多文档摘要，通过将一个文档或多个文档中句子之间的关系表示为图，其中句子为节点，边缘表示句子之间的相似度。系统
    [[171](#bib.bib171), [96](#bib.bib96), [141](#bib.bib141), [46](#bib.bib46)] 然后使用
    PageRank、HITS 或 LexRank 等算法来识别图中最重要的句子，这些句子被提取并组合成摘要。
- en: '![Refer to caption](img/82858d0072777c8f5a27241d48c45759.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/82858d0072777c8f5a27241d48c45759.png)'
- en: 'Figure 2: The weighted cosine similarity graph [[46](#bib.bib46)] was generated
    for the cluster, based on the subset of d1003t from DUC 2004, which is a dataset
    for multi-document summarization tasks. The notation used in the figure is as
    follows: ’d’ represents document and ’s’ represents sentence. For instance, d2s3
    denotes the third sentence of document 2.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：加权余弦相似度图 [[46](#bib.bib46)] 是基于 DUC 2004 的 d1003t 子集为集群生成的，DUC 2004 是一个用于多文档摘要任务的数据集。图中使用的符号如下：’d’
    代表文档，’s’ 代表句子。例如，d2s3 表示文档 2 的第三句。
- en: For single-document summarization, position-based methods [[44](#bib.bib44),
    [82](#bib.bib82), [95](#bib.bib95)] exploit the position of sentences within the
    document to identify important content. Additionally, the TF-IDF approach [[60](#bib.bib60),
    [115](#bib.bib115), [156](#bib.bib156)] weighs the importance of words in the
    document based on their frequency and rarity. Sentences with high TF-IDF scores,
    which indicate a higher presence of significant words, are considered more important
    and are selected for the summary. Besides, Latent Semantic Analysis (LSA) [[164](#bib.bib164),
    [60](#bib.bib60), [139](#bib.bib139)] is a mathematical technique that reduces
    the dimensionality of the term-document matrix and uncovers the underlying semantic
    structure of the document. By identifying the principal components or latent topics,
    LSA can rank sentences according to their relevance to these topics, and the top-ranked
    sentences are extracted for the summary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单文档摘要，基于位置的方法 [[44](#bib.bib44), [82](#bib.bib82), [95](#bib.bib95)] 利用句子在文档中的位置来识别重要内容。此外，TF-IDF
    方法 [[60](#bib.bib60), [115](#bib.bib115), [156](#bib.bib156)] 根据词语的频率和稀有度来衡量词语在文档中的重要性。具有高
    TF-IDF 分数的句子，表示显著词语的出现频率较高，被认为更重要，并被选择用于摘要。此外，潜在语义分析（LSA） [[164](#bib.bib164),
    [60](#bib.bib60), [139](#bib.bib139)] 是一种数学技术，通过减少词项-文档矩阵的维度并揭示文档的潜在语义结构来进行分析。通过识别主要成分或潜在主题，LSA
    可以根据这些主题的相关性对句子进行排序，排名最高的句子被提取用于摘要。
- en: Regarding the task of multi-document summarization, Centroid-based methods [[149](#bib.bib149),
    [45](#bib.bib45)] calculate a central point for each document by considering the
    average term frequency of words in the document. Then centroids of all documents
    are used to compute an overall centroid, and sentences from different documents
    that are closest to this overall centroid are selected for the final summary.
    In the clustering [[178](#bib.bib178), [112](#bib.bib112), [57](#bib.bib57)] approach,
    the documents are grouped into clusters based on their similarity to represent
    a common theme or topic. Sentences from each cluster are selected as representatives,
    based on features in the document. The final summary is generated by concatenating
    these representative sentences from each cluster. MMR [[23](#bib.bib23)] is another
    technique that balances the relevance of the extracted information to the query
    and the diversity of the information to avoid redundancy in the summary of multi-document.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 关于多文档摘要的任务，基于中心点的方法[[149](#bib.bib149), [45](#bib.bib45)] 通过考虑文档中词语的平均词频来计算每个文档的中心点。然后，利用所有文档的中心点来计算总体中心点，选择与该总体中心点最接近的不同文档中的句子作为最终摘要。在聚类[[178](#bib.bib178),
    [112](#bib.bib112), [57](#bib.bib57)]方法中，根据文档之间的相似性将文档分组，以代表共同的主题或话题。根据文档中的特征，从每个聚类中选择句子作为代表。最终的摘要是通过将这些代表性句子连接起来生成的。MMR[[23](#bib.bib23)]
    是另一种技术，它平衡了提取信息的相关性与信息的多样性，以避免多文档摘要中的冗余。
- en: Recently, deep learning techniques have made significant advancements in both
    single-document and multi-document summarization. They employ various architectures,
    such as RNNs [[124](#bib.bib124), [22](#bib.bib22)], Transformers [[200](#bib.bib200),
    [102](#bib.bib102)], and pre-trained language models [[99](#bib.bib99), [189](#bib.bib189)],
    to generate coherent and informative summaries from single or multiple documents.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习技术在单文档和多文档摘要方面都取得了显著进展。它们采用各种架构，如RNNs[[124](#bib.bib124), [22](#bib.bib22)]、Transformers[[200](#bib.bib200),
    [102](#bib.bib102)] 和预训练语言模型[[99](#bib.bib99), [189](#bib.bib189)]，以从单个或多个文档中生成连贯且信息丰富的摘要。
- en: '2.3 Source Document Length: Short vs Long'
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 源文档长度：短篇与长篇
- en: Short document summarization and long document summarization are two categories
    of text summarization, which differ based on the length and complexity of the
    input documents. Short document summarization focuses on generating summaries
    for relatively shorter documents, such as news articles [[125](#bib.bib125), [63](#bib.bib63)],
    blog posts [[70](#bib.bib70)], or single research papers [[20](#bib.bib20)]. Due
    to the limited length of the input, short document summarization often requires
    less context and background knowledge to produce coherent summaries. The main
    challenge is to effectively identify the most important information and convey
    it in a concise manner while maintaining coherence and readability. Meanwhile,
    long document summarization deals with generating summaries for more extensive
    and complex documents, such as books [[199](#bib.bib199), [81](#bib.bib81)], lengthy
    reports [[71](#bib.bib71)], or collections of research papers [[35](#bib.bib35)].
    The primary challenge is to capture the overall theme and essential details while
    managing a large volume of information. This often needs advanced techniques to
    handle and process lengthy texts, maintain coherence, and produce summaries that
    effectively convey the critical points. Presently, a benchmark dataset whose source
    document length averages over 3,000 lexical tokens can be classified as long documents,
    given that most current state-of-the-art summarization systems (e.g., pre-trained
    models) are restricted to processing only 512 to 1,024 lexical tokens [[13](#bib.bib13)].
    These constraints cannot be easily overcome without new techniques that aid in
    enabling current architectures to process extensive textual inputs [[78](#bib.bib78)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 短文档摘要和长文档摘要是文本摘要的两种类别，其区别在于输入文档的长度和复杂性。短文档摘要侧重于为相对较短的文档生成摘要，例如新闻文章 [[125](#bib.bib125),
    [63](#bib.bib63)]、博客帖子 [[70](#bib.bib70)] 或单篇研究论文 [[20](#bib.bib20)]。由于输入长度有限，短文档摘要通常需要较少的背景和上下文知识来生成连贯的摘要。主要挑战是有效识别最重要的信息，并以简洁的方式传达，同时保持连贯性和可读性。与此同时，长文档摘要涉及为更广泛和复杂的文档生成摘要，例如书籍
    [[199](#bib.bib199), [81](#bib.bib81)]、长报告 [[71](#bib.bib71)] 或研究论文集 [[35](#bib.bib35)]。主要挑战是捕捉整体主题和重要细节，同时管理大量信息。这通常需要先进的技术来处理和加工冗长的文本，保持连贯性，并生成有效传达关键点的摘要。目前，一个源文档长度平均超过3,000个词汇单位的基准数据集可以被归类为长文档，因为大多数当前最先进的摘要系统（例如，预训练模型）仅能处理512到1,024个词汇单位
    [[13](#bib.bib13)]。这些限制在没有新技术的帮助下难以克服，这些新技术有助于使当前架构能够处理大量文本输入 [[78](#bib.bib78)]。
- en: Before the rise of deep learning, short document summarization primarily relied
    on algorithms like TF-IDF [[115](#bib.bib115)], centroid [[45](#bib.bib45)], LSA
    [[164](#bib.bib164)], and graph-based models [[171](#bib.bib171)], which are also
    used for the single-document summarization task discussed earlier. Due to limitations
    in system and algorithm capabilities, long document summarization was largely
    neglected until recent years. However, the emergence of deep neural networks has
    led to advancements in long document summarization task [[61](#bib.bib61), [35](#bib.bib35),
    [188](#bib.bib188), [13](#bib.bib13)], as well as improvements to short document
    summarization [[39](#bib.bib39), [91](#bib.bib91), [157](#bib.bib157)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习兴起之前，短文档摘要主要依赖于TF-IDF [[115](#bib.bib115)]、质心 [[45](#bib.bib45)]、LSA [[164](#bib.bib164)]
    和基于图模型 [[171](#bib.bib171)] 等算法，这些算法也用于之前讨论的单文档摘要任务。由于系统和算法能力的限制，长文档摘要在过去几年里基本被忽视。然而，深度神经网络的出现推动了长文档摘要任务的进展
    [[61](#bib.bib61), [35](#bib.bib35), [188](#bib.bib188), [13](#bib.bib13)]，以及短文档摘要的改进
    [[39](#bib.bib39), [91](#bib.bib91), [157](#bib.bib157)]。
- en: '2.4 Summary Length: Headline vs Short vs Long'
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 摘要长度：标题 vs 短摘要 vs 长摘要
- en: The output summary length of summarization tasks can vary, depending on the
    desired level of detail and the intended use case. Based on summary length, there
    are three distinct text summarization tasks, which include headline summarization,
    short summary summarization, and long summary summarization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要任务的输出摘要长度可以有所不同，这取决于所需的细节程度和预期的使用场景。根据摘要长度，有三种不同的文本摘要任务，包括标题摘要、短摘要和长摘要。
- en: The aim of headline summarization is to generate a very brief and concise summary
    that captures the main theme or topic of the source text [[206](#bib.bib206),
    [165](#bib.bib165), [207](#bib.bib207), [92](#bib.bib92)]. The output is usually
    a single sentence or a short phrase. Headline summarization is often used for
    news articles, where the goal is to provide readers with an immediate understanding
    of the main topic or event without diving into the details. This type of summarization
    task requires the model to extract or generate the most crucial information and
    convey it in a limited number of words.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 标题摘要的目的是生成一个非常简洁的摘要，捕捉源文本的主要主题或话题。输出通常是一个句子或一个短语。标题摘要通常用于新闻文章，目标是让读者立即理解主要话题或事件，而无需深入细节。这种类型的总结任务要求模型提取或生成最关键的信息，并用有限的字数传达。
- en: Short summary summarization [[36](#bib.bib36), [197](#bib.bib197), [125](#bib.bib125)]
    aims to produce a slightly longer summary that provides more context and details
    than a headline. Short summaries typically consist of a few sentences or a short
    paragraph. These summaries are useful for readers who want a quick overview of
    the source text without reading the entire document. Short summarization tasks
    require the model to identify and extract key points, main ideas, and essential
    information while maintaining the overall coherence and informativeness of the
    input text.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 短摘要总结的目标是生成稍长的摘要，比标题提供更多的背景和细节。短摘要通常由几句话或一小段组成。这些摘要对希望快速了解源文本内容的读者很有用，而无需阅读整个文档。短摘要任务要求模型识别和提取关键点、主要思想和重要信息，同时保持输入文本的整体连贯性和信息量。
- en: The target of long summary summarization task [[197](#bib.bib197), [113](#bib.bib113),
    [107](#bib.bib107)] is to generate more comprehensive summaries that cover a wider
    range of topics, subtopics, or details from the source text. These summaries can
    be several paragraphs or even longer, depending on the length and complexity of
    the original document. This sort of task is suitable for situations where readers
    want to gain a deeper understanding of the source material without reading it
    in its entirety. It needs the model to not only extract key information but also
    maintain the logical structure and relationships between different ideas, making
    it a more challenging task.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 长摘要总结任务的目标是生成更全面的摘要，涵盖来源文本的更多主题、子主题或细节。这些摘要可以是几段甚至更长，具体取决于原始文档的长度和复杂性。这种任务适用于读者希望深入理解源材料而不必通读全文的情况。它要求模型不仅提取关键信息，还要保持逻辑结构和不同思想之间的关系，使得任务更具挑战性。
- en: To produce concise summary headlines, Banko et al. [[10](#bib.bib10)] present
    a traditional approach that can generate summaries shorter than a sentence by
    building statistical models for content selection and surface realization. The
    approach is similar to statistical machine translation and uses statistical models
    of term selection and term ordering. Content selection and summary structure generation
    can be combined to rank possible summary candidates against each other using an
    overall weighting scheme. The Viterbi beam search [[72](#bib.bib72)] can be used
    to find a near-optimal summary. For short summary summarization, a trainable summarization
    program based on a statistical framework [[82](#bib.bib82)] was developed, focusing
    on document extracts as a type of computed document summary. Features such as
    sentence length, fixed phrases, paragraph information, thematic words, and uppercase
    words are used to score each sentence. By employing a Bayesian classification
    function, the paper estimates the probability that a sentence will be included
    in a summary.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成简洁的摘要标题，Banko 等人提出了一种传统方法，该方法可以通过构建内容选择和表面实现的统计模型生成比一句话更短的摘要。该方法类似于统计机器翻译，使用术语选择和排序的统计模型。内容选择和摘要结构生成可以结合起来，使用整体加权方案对可能的摘要候选进行排名。可以使用维特比束搜索找到接近最佳的摘要。对于短摘要总结，开发了一种基于统计框架的可训练摘要程序，专注于文档摘录作为一种计算文档摘要类型。利用句子长度、固定短语、段落信息、主题词和大写词等特征来评分每个句子。通过使用贝叶斯分类函数，论文估计句子被纳入摘要的概率。
- en: In recent times, deep learning methods for headline summarization and short
    summary summarization, include sequence-to-sequence models, attention mechanisms
    [[169](#bib.bib169)], and transformers like BERT [[39](#bib.bib39)], GPT [[150](#bib.bib150)],
    and T5 [[152](#bib.bib152)], have emerged. These models are trained on large corpora
    to generate concise and informative headlines by learning the most important and
    relevant information within the input documents. Long summary summarization, along
    with long document summarization tasks, was overlooked for a significant period
    of time due to limitations in hardware and algorithm capabilities. Nevertheless,
    the development of deep neural networks has brought about significant progress
    in the field of long summary summarization techniques [[113](#bib.bib113), [107](#bib.bib107)],
    like long document summarization tasks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习方法用于标题总结和短摘要总结，包括序列到序列模型、注意力机制[[169](#bib.bib169)]，以及如BERT[[39](#bib.bib39)]、GPT[[150](#bib.bib150)]和T5[[152](#bib.bib152)]等变换器。这些模型通过学习输入文档中的最重要和相关的信息，训练在大型语料库上以生成简明和信息丰富的标题。由于硬件和算法能力的限制，长摘要总结以及长文档总结任务曾被忽视。然而，深度神经网络的发展在长摘要总结技术领域带来了显著的进展[[113](#bib.bib113),
    [107](#bib.bib107)]。
- en: '2.5 Language: Single-Language vs Multi-Language vs Cross-Lingual'
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 语言：单语言与多语言与跨语言
- en: Summarization tasks can be categorized based on the language involved, resulting
    in single-language, multi-language, and cross-lingual summarization. In single-language
    summarization, both the input documents and the generated summaries are in the
    same language. This is the most common type of summarization task, and the majority
    of research has focused on this area. Multi-language summarization involves generating
    summaries for documents in various languages, but the output summary is in the
    same language as the input document. For instance, if the input is in French,
    the summary will be in French, if the input is in Japanese in the same model,
    the summary will be in Japanese [[142](#bib.bib142)]. Cross-lingual summarization
    refers to the task of generating a summary in a target language for a source document
    written in a different language [[90](#bib.bib90), [134](#bib.bib134)]. This type
    of summarization task requires models to not only understand and extract the main
    ideas from the source document but also translate the extracted information into
    the target language.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总结任务可以根据涉及的语言进行分类，包括单语言、多语言和跨语言总结。在单语言总结中，输入文档和生成的摘要都是同一种语言。这是最常见的总结任务类型，大多数研究集中在这一领域。多语言总结涉及为各种语言的文档生成摘要，但输出摘要与输入文档使用相同的语言。例如，如果输入是法语，摘要将是法语；如果输入是日语，则摘要也将是日语[[142](#bib.bib142)]。跨语言总结指的是为用不同语言书写的源文档生成目标语言的摘要[[90](#bib.bib90),
    [134](#bib.bib134)]。这种类型的总结任务要求模型不仅要理解和提取源文档中的主要思想，还要将提取的信息翻译成目标语言。
- en: \tbl
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \tbl
- en: One example of cross-lingual summarization involves generating summaries of
    the same content in multiple languages [[8](#bib.bib8)]. \topruleSource text in
    English Crude oil futures climbed 2% on Friday to a 28-month high, as the United
    States and Russia are in a deadlock over the Syrian issue, related concerns intensified.
    In October, the New York Mercantile Exchange’s light sweet crude oil futures settlement
    price rose 2.16 US dollars to 110.53 US dollars per barrel. Source text in Chinese
    原油期货周五攀升2%，至28个月高点，因美俄两国在叙利亚问题上陷入僵局，相关担忧愈演愈烈。纽商所十月轻质低硫原油期货结算价涨2.16美元，至每桶110.53美元。
    Summary in English Oil prices hit a 28-month high as tensions in Syria escalated.
    Summary in Chinese 油价创28个月新高因叙利亚紧张局势升级。 \botrule
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 跨语言总结的一个例子涉及在多种语言中生成相同内容的摘要[[8](#bib.bib8)]。 \toprule英文原文：原油期货周五攀升2%，至28个月高点，因美俄两国在叙利亚问题上陷入僵局，相关担忧愈演愈烈。纽商所十月轻质低硫原油期货结算价涨2.16美元，至每桶110.53美元。
    中文原文：原油期货周五攀升2%，至28个月高点，因美俄两国在叙利亚问题上陷入僵局，相关担忧愈演愈烈。纽商所十月轻质低硫原油期货结算价涨2.16美元，至每桶110.53美元。
    英文摘要：油价创28个月新高因叙利亚紧张局势升级。 中文摘要：油价创28个月新高因叙利亚紧张局势升级。 \botrule
- en: 'Traditional methods such as keyword extraction [[147](#bib.bib147), [65](#bib.bib65)],
    Hidden Markov Model [[54](#bib.bib54)], and graph-based algorithms [[7](#bib.bib7)]
    were commonly used for multi-language summarization before the widespread adoption
    of deep learning. These methods were adapted to work across various languages
    and often employed language-specific resources such as stop-words and stemming
    tools [[142](#bib.bib142)] or language-agnostic features like TF-IDF [[65](#bib.bib65)]
    to improve generalization. Cross-lingual summarization, on the other hand, typically
    depends on machine translation or bilingual dictionaries to comprehend and extract
    content from the source language, followed by producing a summary in the target
    language. Two pipeline methods were commonly employed: one approach is first summarizing
    the source document and then translating the summary to the target language [[90](#bib.bib90),
    [16](#bib.bib16), [176](#bib.bib176), [191](#bib.bib191), [195](#bib.bib195)],
    while the other approach entails translating the source document to the target
    language and then generating a summary [[134](#bib.bib134), [177](#bib.bib177)].'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法如关键词提取[[147](#bib.bib147), [65](#bib.bib65)]、隐马尔可夫模型[[54](#bib.bib54)]和基于图的算法[[7](#bib.bib7)]在深度学习广泛应用之前常用于多语言总结。这些方法被调整以适用于各种语言，并且通常使用语言特定的资源，如停用词和词干提取工具[[142](#bib.bib142)]或语言无关的特征，如TF-IDF[[65](#bib.bib65)]来提高泛化能力。另一方面，跨语言总结通常依赖于机器翻译或双语词典来理解和提取源语言的内容，然后生成目标语言的总结。常用的两种流程方法是：一种是首先总结源文档，然后将总结翻译成目标语言[[90](#bib.bib90),
    [16](#bib.bib16), [176](#bib.bib176), [191](#bib.bib191), [195](#bib.bib195)]，另一种方法是将源文档翻译成目标语言，然后生成总结[[134](#bib.bib134),
    [177](#bib.bib177)]。
- en: Nowadays, both multi-language and cross-lingual summarization tasks have benefited
    from advanced deep-learning techniques. These deep neural models [[29](#bib.bib29),
    [204](#bib.bib204), [135](#bib.bib135), [43](#bib.bib43)] can learn representations
    for multiple languages in a shared embedding space, which makes it possible to
    perform multi-language and cross-lingual transfer learning. By fine-tuning these
    models, researchers have achieved impressive results in generating summaries across
    different languages without the need for extensive parallel data or explicit translation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，多语言和跨语言总结任务都受益于先进的深度学习技术。这些深度神经模型[[29](#bib.bib29), [204](#bib.bib204), [135](#bib.bib135),
    [43](#bib.bib43)]能够在共享的嵌入空间中学习多种语言的表示，这使得多语言和跨语言迁移学习成为可能。通过对这些模型进行微调，研究人员在生成不同语言的总结方面取得了令人瞩目的成果，而无需大量的平行数据或明确的翻译。
- en: '2.6 Domain: General vs Specific domain'
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 领域：一般 vs 特定领域
- en: General summarization pertains to the process of generating summaries for any
    type of text or domain, without focusing on any particular subject matter. Conversely,
    specific domain summarization is designed to produce summaries for texts within
    a specific domain or subject matter, such as legal documents, scientific articles,
    or news stories.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一般总结涉及为任何类型的文本或领域生成总结，而不专注于任何特定的主题。相反，特定领域总结旨在为特定领域或主题的文本生成总结，例如法律文件、科学文章或新闻故事。
- en: Specific domain summarization tasks typically demand domain-specific knowledge
    and may integrate specialized language models, ontologies, or rules to better
    capture the nuances and significant aspects of the target domain [[83](#bib.bib83),
    [192](#bib.bib192), [89](#bib.bib89), [26](#bib.bib26)]. These systems may also
    consider the distinctive structure or format of texts in the domain, leading to
    better summarization results. Prior to deep learning, traditional specific domain
    summarization methods often incorporated domain-specific knowledge in the form
    of rules [[48](#bib.bib48), [172](#bib.bib172)], or templates [[208](#bib.bib208),
    [182](#bib.bib182), [138](#bib.bib138)]. Additionally, feature-based methods were
    employed to identify important sentences or segments in the domain-specific summarization,
    such as the occurrence of certain keywords, phrases, or named entities relevant
    to the domain [[148](#bib.bib148), [27](#bib.bib27), [146](#bib.bib146)]. Graph-based
    algorithms, such as LexRank [[46](#bib.bib46)] or TextRank [[115](#bib.bib115)],
    were also commonly utilized for specific domain summarization. Domain-specific
    information could be incorporated into the graph representation or used to weigh
    the edges, making the algorithms more suited to the targeted domain.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域总结任务通常需要领域特定的知识，并可能集成专业语言模型、本体或规则，以更好地捕捉目标领域的细微差别和重要方面[[83](#bib.bib83),
    [192](#bib.bib192), [89](#bib.bib89), [26](#bib.bib26)]。这些系统还可能考虑到领域中文本的独特结构或格式，从而获得更好的总结结果。在深度学习之前，传统的特定领域总结方法通常采用规则[[48](#bib.bib48),
    [172](#bib.bib172)]或模板[[208](#bib.bib208), [182](#bib.bib182), [138](#bib.bib138)]形式的领域特定知识。此外，基于特征的方法被用来识别领域特定总结中的重要句子或段落，如某些关键词、短语或与领域相关的命名实体的出现[[148](#bib.bib148),
    [27](#bib.bib27), [146](#bib.bib146)]。基于图的算法，如LexRank [[46](#bib.bib46)]或TextRank
    [[115](#bib.bib115)]，也常用于特定领域总结。领域特定信息可以被纳入图表示中或用于加权边缘，使得算法更适合目标领域。
- en: With the advent of deep learning, specific domain summarization has significantly
    evolved and improved. Domain adaptation is a technique that leverages pre-trained
    language models [[83](#bib.bib83), [192](#bib.bib192), [201](#bib.bib201)], which
    have been trained on vast amounts of text data and fine-tunes them for specific
    domain summarization tasks. To better capture the domain-specific knowledge, deep
    learning models also can be trained with domain-specific word embeddings or contextualized
    embeddings, such as BioBERT [[89](#bib.bib89)] for the biomedical domain or Legalbert
    [[26](#bib.bib26)] for the Legal documents. These techniques offer several advantages
    over traditional approaches, including better representation learning, more effective
    handling of domain-specific knowledge, and the ability to adapt to new domains
    more easily. They have shown promising results in various specific domains and
    continue to push the boundaries of what is possible in domain-specific text summarization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的出现，特定领域的总结已经显著演变和改进。领域适应是一种利用预训练语言模型[[83](#bib.bib83), [192](#bib.bib192),
    [201](#bib.bib201)]的技术，这些模型在大量文本数据上进行训练，并针对特定领域总结任务进行微调。为了更好地捕捉领域特定的知识，深度学习模型还可以使用领域特定的词嵌入或上下文化嵌入进行训练，例如用于生物医学领域的BioBERT
    [[89](#bib.bib89)]或用于法律文档的Legalbert [[26](#bib.bib26)]。这些技术相较于传统方法具有若干优势，包括更好的表示学习、更有效地处理领域特定知识以及更容易适应新领域。它们在各种特定领域中表现出良好的结果，并不断推动领域特定文本总结的极限。
- en: '2.7 Level of Abstraction: Generic vs Query-focused'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7 抽象层次：通用与聚焦查询
- en: Query-focused summarization aims to generate a summary that addresses a specific
    user query or topic. The summary should contain the most relevant information
    from the source text with respect to the given query [[203](#bib.bib203), [25](#bib.bib25),
    [1](#bib.bib1)]. This type of summarization is useful for readers who have a specific
    question or interest and want a summary tailored to their needs. In contrast,
    the goal of generic summarization is to create a concise and coherent summary
    that captures the most important information from the source text. This type of
    summarization is not focused on any specific query or topic. Instead, it aims
    to provide an overview of the entire document or set of documents, which can be
    helpful for readers who want to quickly grasp the key points without going through
    the entire text.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以查询为重点的摘要旨在生成一个针对特定用户查询或主题的摘要。该摘要应包含源文本中与给定查询相关的最重要信息[[203](#bib.bib203), [25](#bib.bib25),
    [1](#bib.bib1)]。这种类型的摘要对于那些有特定问题或兴趣并希望获取量身定制的摘要的读者非常有用。相比之下，通用摘要的目标是创建一个简洁连贯的摘要，捕捉源文本中最重要的信息。这种类型的摘要不针对任何特定的查询或主题，而是旨在提供整个文档或文档集的概述，这对那些希望快速掌握关键点而无需通读整个文本的读者很有帮助。
- en: For query-focused, the relevance of the information is determined by its relation
    to the given query. Important information in a generic summary may be excluded
    if it is not directly relevant to the query, while less significant information
    related to the query may be included. Therefore, query-focused summarization typically
    requires additional processing to account for the given query. This may involve
    incorporating the query into the representation of the text, using query-specific
    features, or applying query-based attention mechanisms to guide the extraction
    or generation of the summary. These techniques are more specialized and require
    a query as input along with the source text.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以查询为重点的摘要，信息的相关性取决于其与给定查询的关系。如果通用摘要中的重要信息与查询不直接相关，则可能会被排除，而与查询相关的较不重要的信息可能会被包含在内。因此，以查询为重点的摘要通常需要额外的处理来考虑给定的查询。这可能涉及将查询纳入文本的表示中，使用查询特定的特征，或应用基于查询的注意机制来指导摘要的提取或生成。这些技术更加专业，需要将查询作为输入与源文本一起使用。
- en: '![Refer to caption](img/a75a003cab76e1a9addce77fe279cd4d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a75a003cab76e1a9addce77fe279cd4d.png)'
- en: 'Figure 3: (a) Pre-training the BERTSUM [[101](#bib.bib101)] model on a generic
    abstractive summarization corpus, such as XSUM [[126](#bib.bib126)]. (b) Fine-tuning
    the pre-trained BERTSUM model on the target domain, which in this case is Debatepedia
    [[129](#bib.bib129), [84](#bib.bib84)].'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图3： (a) 在通用抽象摘要语料库上对BERTSUM[[101](#bib.bib101)]模型进行预训练，例如XSUM[[126](#bib.bib126)]。
    (b) 在目标领域上对预训练的BERTSUM模型进行微调，在此情况下为Debatepedia[[129](#bib.bib129), [84](#bib.bib84)]。
- en: One of the simplest traditional approaches to query-focused summarization is
    to match the keywords in the query with those in the source text [[203](#bib.bib203),
    [37](#bib.bib37)]. The sentences containing a higher number of matched keywords
    are considered more relevant and are included in the summary. Besides, query expansion
    methods [[25](#bib.bib25), [24](#bib.bib24)] expand the initial query using various
    techniques, such as synonym extraction or related term discovery, to improve the
    recall of relevant information. The expanded query is then used to match and rank
    sentences in the source text. This method helps to identify relevant information
    that may not have been captured by the original query. Furthermore, query-based
    weighting techniques incorporate the query by utilizing term weighting schemes
    like TF-IDF [[51](#bib.bib51), [37](#bib.bib37)]. In these methods, query terms
    are given higher weights, which enhances the relevance score of sentences containing
    those terms. The sentences are then ranked based on their scores, and the top-ranked
    sentences are selected for the summary. Graph-based algorithms, like LexRank [[46](#bib.bib46)]
    or TextRank [[116](#bib.bib116)], can also be adapted for query-focused summarization
    by incorporating the query into the graph representation of the text. On the other
    hand, supervised machine learning like Support Vector Machines (SVM) [[52](#bib.bib52),
    [163](#bib.bib163)] or logistic regression [[137](#bib.bib137)], can be trained
    to rank the sentences in new, unseen documents based on their relevance to the
    query for query-focused summarization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重点摘要的最简单传统方法之一是将查询中的关键词与源文本中的关键词进行匹配[[203](#bib.bib203), [37](#bib.bib37)]。包含更多匹配关键词的句子被认为更相关，并被纳入摘要中。此外，查询扩展方法[[25](#bib.bib25),
    [24](#bib.bib24)]使用各种技术，如同义词提取或相关术语发现，来扩展初始查询，以提高相关信息的召回率。扩展的查询随后用于匹配和排名源文本中的句子。这种方法有助于识别可能未被原始查询捕获的相关信息。此外，基于查询的加权技术通过利用像TF-IDF[[51](#bib.bib51),
    [37](#bib.bib37)]这样的术语加权方案来结合查询。在这些方法中，查询术语被赋予更高的权重，这增强了包含这些术语的句子的相关性得分。然后根据得分对句子进行排名，并选择排名靠前的句子作为摘要。基于图的算法，如LexRank[[46](#bib.bib46)]或TextRank[[116](#bib.bib116)]，也可以通过将查询融入文本的图表示中来适应查询重点摘要。另一方面，监督学习方法如支持向量机（SVM）[[52](#bib.bib52),
    [163](#bib.bib163)]或逻辑回归[[137](#bib.bib137)]，可以训练以根据与查询的相关性对新未见文档中的句子进行排名，以进行查询重点摘要。
- en: With the appearance of deep learning techniques, such as Seq2Seq models [[175](#bib.bib175)],
    pointer generator networks [[67](#bib.bib67)], pre-trained language models [[1](#bib.bib1)],
    and reinforcement learning [[21](#bib.bib21)], query-focused summarization could
    better understand the input and query context to generate more accurate and relevant
    summaries that address the specific user query or topic.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习技术的出现，如Seq2Seq模型[[175](#bib.bib175)]、指针生成网络[[67](#bib.bib67)]、预训练语言模型[[1](#bib.bib1)]和强化学习[[21](#bib.bib21)]，查询重点摘要可以更好地理解输入和查询上下文，从而生成更准确、更相关的摘要，以解决特定用户查询或主题。
- en: 3 Deep learning techniques
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习技术
- en: This section will review the most popular deep learning models and techniques
    utilized with deep neural networks, such as attention mechanisms, copy mechanisms,
    dictionary probabilities, etc. Each type of model has the potential to significantly
    improve various summarization tasks, which may be a blend of different categories,
    such as long abstractive legal document summarization, cross-lingual headline
    summarization, and extractive query-focused summarization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将回顾在深度神经网络中使用的最流行的深度学习模型和技术，如注意力机制、复制机制、词典概率等。每种模型都有可能显著提升各种摘要任务，这些任务可能是不同类别的混合，例如长篇抽象法律文档摘要、跨语言标题摘要和提取式查询重点摘要。
- en: 3.1 Plain neural network
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 普通神经网络
- en: Plain neural network, also known as feed-forward neural network [[14](#bib.bib14),
    [86](#bib.bib86)], is a type of artificial neural network that consists of several
    layers of interconnected nodes or neurons. The input is passed through one or
    more hidden layers, where the weights of each neuron are adjusted based on the
    error generated by the network’s output. This process, called back-propagation,
    allows the network to learn how to classify or predict outputs based on the input
    data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 普通神经网络，也称为前馈神经网络 [[14](#bib.bib14), [86](#bib.bib86)]，是一种由多个互连的节点或神经元层组成的人工神经网络。输入通过一个或多个隐藏层传递，在这些层中，每个神经元的权重会根据网络输出产生的误差进行调整。这个过程称为反向传播，使网络能够学习如何根据输入数据进行分类或预测输出。
- en: Plain neural network is usually used in learning vector representation of words
    or sentences in summarization tasks. Word2vec models [[117](#bib.bib117), [118](#bib.bib118)],
    which learns continuous vector representations of words from large amounts of
    text data, was developed with plain neural network. After these models, Glove
    [[144](#bib.bib144)] was proposed to combine the advantages of matrix factorization
    and local context window methods to create a more efficient and accurate model.
    The learned word vectors can capture various semantic and syntactic regularities,
    and can be used as features for different summarization tasks. On the other hand,
    Paragraph Vector [[68](#bib.bib68)] learns fixed-length distributed representations
    of variable-length pieces of text by jointly predicting the words in the text
    and a separate paragraph-specific vector.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 普通神经网络通常用于在摘要任务中学习词语或句子的向量表示。Word2vec 模型 [[117](#bib.bib117), [118](#bib.bib118)]，通过从大量文本数据中学习连续的词向量表示，是基于普通神经网络开发的。在这些模型之后，提出了
    Glove [[144](#bib.bib144)]，旨在结合矩阵分解和局部上下文窗口方法的优点，以创建一个更高效、更准确的模型。学习到的词向量能够捕捉各种语义和句法规律，并可以作为不同摘要任务的特征。另一方面，Paragraph
    Vector [[68](#bib.bib68)] 通过联合预测文本中的词和一个单独的段落特定向量，学习固定长度的分布式表示。
- en: '![Refer to caption](img/ac5b579a1773bc6c5201f65739b04872.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac5b579a1773bc6c5201f65739b04872.png)'
- en: 'Figure 4: The current word was predicted based on the context by CBOW, and
    the surrounding words were predicted based on the current word by the Skip-gram.[[117](#bib.bib117)]'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：当前词是基于上下文通过 CBOW 预测的，而周围的词是基于当前词通过 Skip-gram 预测的。[ [117](#bib.bib117) ]
- en: 3.2 Recurrent neural network
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 循环神经网络
- en: Recurrent neural network (RNN) [[154](#bib.bib154), [159](#bib.bib159)] is specifically
    designed to handle sequential data, making them highly suitable for NLP tasks.
    RNN can process variable-length sequences of inputs and maintain an internal state,
    which allows the model to remember information from previous time steps. However,
    RNN is prone to vanishing and exploding gradient problems, which makes it challenging
    to learn long-range dependencies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN） [[154](#bib.bib154), [159](#bib.bib159)] 专门设计用于处理序列数据，使其非常适合自然语言处理任务。RNN
    可以处理变长的输入序列并保持内部状态，这使得模型能够记住之前时间步的信息。然而，RNN 容易出现梯度消失和梯度爆炸问题，这使得学习长程依赖关系变得具有挑战性。
- en: 'To address these issues, Long Short-Term Memory (LSTM) [[69](#bib.bib69)] and
    Gated Recurrent Unit (GRU) [[30](#bib.bib30)] networks were developed to selectively
    remember or forget information over time. Each memory cell in an LSTM network
    has three main components: an input gate, a forget gate, and an output gate. The
    input gate determines how much of the new input information should be stored in
    the memory cell, while the forget gate determines how much of the previous memory
    state should be forgotten. The output gate controls how much of the current memory
    state should be used to generate the output. As opposed to LSTM, GRU uses a simpler
    gating mechanism that has only two gates: an update gate controls how much of
    the previous memory state should be retained and how much of the new input should
    be added to the memory cell, a reset gate determines how much of previous memory
    state should be ignored in favor of the new input.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，长短期记忆（LSTM）[[69](#bib.bib69)]和门控循环单元（GRU）[[30](#bib.bib30)]网络被开发出来以选择性地记住或遗忘信息。LSTM网络中的每个记忆单元有三个主要组件：输入门、遗忘门和输出门。输入门决定多少新的输入信息应存储在记忆单元中，而遗忘门决定多少之前的记忆状态应被遗忘。输出门控制当前记忆状态中多少部分应被用于生成输出。与LSTM相对的是，GRU使用更简单的门控机制，只有两个门：更新门控制多少之前的记忆状态应被保留，以及多少新输入应被添加到记忆单元中，重置门决定多少之前的记忆状态应被忽略以适应新的输入。
- en: For extractive multi-document summarization, SummaRuNNer [[124](#bib.bib124)]
    is a GRU-based RNN model, which allows visualization of its predictions based
    on abstract features such as information content, salience, and novelty. It is
    an extractive model using abstractive training, which can train on human-generated
    reference summaries alone, removing the need for sentence-level extractive labels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于抽取式多文档摘要，SummaRuNNer [[124](#bib.bib124)] 是一个基于GRU的RNN模型，它允许根据信息内容、显著性和新颖性等抽象特征可视化其预测结果。它是一个使用抽象训练的抽取式模型，可以仅通过人类生成的参考摘要进行训练，去除了对句子级抽取标签的需求。
- en: The attention mechanism is a technique that allows models to focus on different
    parts of the input when producing an output. Both Chopra et al. [[31](#bib.bib31)]
    and Nallapati et al. [[125](#bib.bib125)] have investigated the utilization of
    Attentional RNN Decoder for enhancing abstractive summarization performance. In
    the former study, a Convolutional attention-based network is employed as the encoder,
    furnishing a conditioning input to guide the decoder in focusing on relevant portions
    of the input sentence during word generation. On the other hand, the latter study
    employs an RNN-based encoder and incorporates keyword modeling to capture the
    hierarchical structure between sentences and words, effectively addressing the
    challenge of rare or unseen words. To tackle this issue, they propose a novel
    switching decoder/pointer architecture that enables the model to make decisions
    between generating a word and indicating its location within the source document.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制是一种技术，它允许模型在生成输出时关注输入的不同部分。Chopra等人 [[31](#bib.bib31)] 和Nallapati等人 [[125](#bib.bib125)]
    都研究了使用注意力RNN解码器来增强抽象摘要性能。在前者的研究中，采用了基于卷积的注意力网络作为编码器，为解码器提供条件输入，以引导其在生成单词时关注输入句子的相关部分。另一方面，后者的研究采用了基于RNN的编码器，并结合了关键词建模以捕捉句子和单词之间的层次结构，有效解决了稀有或未见单词的问题。为了解决这个问题，他们提出了一种新颖的切换解码器/指针架构，使模型能够在生成单词和指示其在源文档中的位置之间做出决策。
- en: Furthermore, See et al. [[161](#bib.bib161)] presents a hybrid pointer network
    that copies words from the source text to reproduce accurate information and handle
    out-of-vocabulary words. They also developed a coverage architecture to avoid
    repetition in the summary. This aids the attention mechanism to avoid repeatedly
    attending to the same locations, reducing the generation of repetitive text. Additionally,
    they utilize beam search, which is a heuristic search algorithm that explores
    the search space in a breadth-first manner to find the most likely output sequence.
    It extends the search to the top ’B’ candidates at each step, where ’B’ is a predefined
    beam width. The beam width determines the number of alternatives (branches) to
    explore at each step. At each time step, it keeps track of the top ’B’ sequences
    based on the probabilities of the sequences. The final output sequence is the
    one that has the highest overall score. Beam search offers a good trade-off between
    the quality of output and computational efficiency.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，See等人[[161](#bib.bib161)]提出了一种混合指针网络，该网络从源文本中复制单词以再现准确的信息并处理词汇表外的单词。他们还开发了一种覆盖架构，以避免摘要中的重复。这有助于注意力机制避免重复关注相同位置，从而减少重复文本的生成。此外，他们利用了束搜索，这是一种启发式搜索算法，以宽度优先的方式探索搜索空间，以找到最可能的输出序列。它在每一步扩展到前
    ’B’ 个候选项，其中 ’B’ 是预定义的束宽。束宽决定了每一步要探索的替代方案（分支）的数量。在每个时间步，它根据序列的概率跟踪前 ’B’ 个序列。最终的输出序列是具有最高总体得分的序列。束搜索在输出质量和计算效率之间提供了良好的折衷。
- en: '![Refer to caption](img/958bfa54a7f4cee63a52d6c83ecff287.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/958bfa54a7f4cee63a52d6c83ecff287.png)'
- en: 'Figure 5: Pointer-generator model combines an RNN with attention and copy network.
    [[161](#bib.bib161)]'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Pointer-generator 模型将 RNN 与注意力机制和复制网络结合起来。[[161](#bib.bib161)]
- en: Zheng et al. [[205](#bib.bib205)] propose a new method for multi-document summarization
    called Subtopic-Driven Summarization. The authors argue that each underlying topic
    of the documents can be seen as a collection of different subtopics of varying
    importance. The proposed model uses these subtopics and generates the underlying
    topic representation from a document and subtopic view. The goal is to minimize
    the difference between the representations learned from the two views. The model
    uses a hierarchical RNN to encode contextual information and estimates sentence
    importance hierarchically considering subtopic importance and relative sentence
    importance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 郑等人[[205](#bib.bib205)]提出了一种新的多文档摘要方法，称为子主题驱动的摘要。作者认为，文档中的每个潜在主题可以视为不同子主题的集合，这些子主题的重要性各不相同。该模型利用这些子主题，从文档和子主题的视角生成潜在主题表示。目标是最小化从两个视角学习到的表示之间的差异。该模型使用层次RNN来编码上下文信息，并通过考虑子主题重要性和相对句子重要性来层次化地估计句子重要性。
- en: 3.3 Convolutional neural network
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 卷积神经网络
- en: In NLP, Convolutional neural networks (CNN) [[53](#bib.bib53), [87](#bib.bib87),
    [80](#bib.bib80)], which originally developed for image processing and computer
    vision tasks, are used to capture local patterns and structures within a text,
    making them particularly effective for tasks that involve extracting meaningful
    features from a sequence of words or characters. In a typical CNN architecture
    for NLP, the input text is first represented as a sequence of word or character
    embeddings, forming a matrix where each row corresponds to a word or character
    embedding. The embeddings are learned during the training process, allowing the
    model to capture meaningful representations of words or characters. The main building
    block of CNN is the convolutional layer, which consists of multiple filters. Each
    filter is applied to sliding windows of fixed size across the input text, capturing
    local patterns within the text. The filter’s output, or feature map, is then passed
    through a non-linear activation function, such as the Rectified Linear Unit, to
    introduce non-linearity into the model. After the convolutional layers, the feature
    maps are typically passed through a pooling layer, such as max-pooling or average
    pooling, which reduces the spatial dimensions and extracts the most salient features
    from the feature maps. This process helps to reduce the computational complexity
    of the model and makes it more robust to variations in the input. The final layers
    of a CNN for NLP usually consist of one or more fully connected layers, which
    combine the extracted features and perform the specific NLP task, such as text
    classification or sequence tagging. These layers are often followed by a softmax
    layer for multi-class tasks to produce probability distributions over the possible
    output classes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，卷积神经网络（CNN）[[53](#bib.bib53), [87](#bib.bib87), [80](#bib.bib80)]，最初是为图像处理和计算机视觉任务开发的，用于捕捉文本中的局部模式和结构，使其在提取有意义的特征方面特别有效。典型的NLP卷积神经网络架构中，输入文本首先被表示为词或字符嵌入的序列，形成一个矩阵，其中每一行对应一个词或字符的嵌入。嵌入在训练过程中被学习，使模型能够捕捉到词或字符的有意义表示。CNN的主要构建块是卷积层，由多个滤波器组成。每个滤波器应用于输入文本的固定大小的滑动窗口，捕捉文本中的局部模式。滤波器的输出或特征图，然后通过非线性激活函数，如修正线性单元，引入模型中的非线性。在卷积层之后，特征图通常通过池化层，如最大池化或平均池化，减少空间维度并从特征图中提取最显著的特征。这一过程有助于减少模型的计算复杂性，并使其对输入的变化更加鲁棒。NLP卷积神经网络的最终层通常由一个或多个全连接层组成，这些层结合了提取的特征并执行特定的NLP任务，如文本分类或序列标注。这些层通常后接一个softmax层，用于多分类任务，产生对可能输出类别的概率分布。
- en: Narayan et al. [[126](#bib.bib126)] introduce a new concept called ”extreme
    summarization”, which aims to generate a single sentence summary that can answer
    the question ”What is the article about?”. The researchers proposed a novel abstractive
    model that is conditioned on the article’s topics and based entirely on CNN. This
    model was found to effectively capture long-range dependencies and identify pertinent
    content in a document. They also incorporate topic-sensitive embeddings to enhance
    word context with their topical relevance to the documents.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Narayan等人[[126](#bib.bib126)]引入了一个新的概念，称为“极端摘要”，旨在生成一个可以回答“这篇文章讲了什么？”的问题的单句摘要。研究人员提出了一种新颖的抽象模型，该模型基于CNN，并以文章的主题为条件。该模型被发现能有效捕捉长距离的依赖关系并识别文档中的相关内容。他们还结合了主题敏感的嵌入，以增强词汇上下文与文档的主题相关性。
- en: '![Refer to caption](img/493e9dc5487bf38bb90a1fe42b71dd33.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/493e9dc5487bf38bb90a1fe42b71dd33.png)'
- en: 'Figure 6: A convolutional model conditioned on the topic for extreme summarization.
    [[126](#bib.bib126)]'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一个基于主题的极端摘要卷积模型。[[126](#bib.bib126)]
- en: Liu et al. [[105](#bib.bib105)] discuss a new approach for generating summaries
    of user-defined lengths using CNN. The proposed approach modifies a convolutional
    sequence-to-sequence model to include a length constraint in each convolutional
    block of the initial layer of the model. This is achieved by feeding the desired
    length as a parameter into the decoder during the training phase. At test time,
    any desired length can be provided to generate a summary of approximately that
    length. This research contributes a potentially effective method for producing
    summaries of arbitrary lengths, which holds promise for diverse applications across
    various tasks requiring summaries of different lengths.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Liu等人[[105](#bib.bib105)]讨论了一种使用CNN生成用户定义长度摘要的新方法。所提方法修改了卷积序列到序列模型，以在模型初始层的每个卷积块中包括长度约束。这是通过在训练阶段将所需长度作为参数输入到解码器来实现的。在测试时，可以提供任何所需长度，以生成大致该长度的摘要。这项研究提出了一种可能有效的方法来生成任意长度的摘要，这对各种需要不同长度摘要的任务具有广泛的应用前景。
- en: 3.4 Graph neural networks
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 图神经网络
- en: Although Graph Neural Networks (GNN) [[158](#bib.bib158), [114](#bib.bib114),
    [145](#bib.bib145), [62](#bib.bib62)] are primarily used in domains where data
    naturally exhibits graph structures, such as social networks, molecular structures,
    and knowledge graphs, they can also be adapted for NLP tasks. In NLP, GNNs are
    typically used to model relationships between words, sentences, or documents by
    representing them as nodes in a graph, with edges representing the relationships
    between these nodes. A GNN model learns to propagate information through the graph
    by iteratively updating the node representations based on the information from
    their neighbors. The core building blocks of GNNs are graph convolutional layers,
    which are designed to aggregate information from neighboring nodes and update
    the node features.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图神经网络（GNN）[[158](#bib.bib158), [114](#bib.bib114), [145](#bib.bib145), [62](#bib.bib62)]主要用于数据自然呈现图结构的领域，如社交网络、分子结构和知识图谱，但它们也可以被适应用于NLP任务。在NLP中，GNN通常用于通过将单词、句子或文档表示为图中的节点，并用边表示这些节点之间的关系，从而建模它们之间的关系。GNN模型通过基于邻居信息迭代更新节点表示来学习在图中传播信息。GNN的核心构建模块是图卷积层，它们设计用来聚合来自邻居节点的信息并更新节点特征。
- en: Jing et al. [[75](#bib.bib75)] present a novel Multiplex Graph Convolutional
    Network (Multi-GCN) approach for extractive summarization. Multi-GCN learns node
    embedding of different relations among sentences and words separately and combines
    them to produce a final embedding. This approach helps to mitigate the over-smoothing
    and vanishing gradient problems of the original GCN.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Jing等人[[75](#bib.bib75)]提出了一种新颖的多重图卷积网络（Multi-GCN）方法，用于抽取式总结。Multi-GCN分别学习句子和词之间不同关系的节点嵌入，并将它们组合以产生最终的嵌入。这种方法有助于缓解原始GCN的过度平滑和梯度消失问题。
- en: A heterogeneous GNN, HETERSUMGRAPH [[179](#bib.bib179)] is introduced for extractive
    document summarization. This network includes nodes of different granularity levels
    apart from sentences, which act as intermediaries and enrich cross-sentence relations.
    This approach allows different sentences to interact considering overlapping word
    information. Moreover, the graph network can accommodate additional node types,
    such as document nodes for multi-document summarization.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 引入了一种异构GNN，HETERSUMGRAPH [[179](#bib.bib179)]，用于抽取式文档总结。该网络包括除了句子之外的不同粒度级别的节点，这些节点充当中介，并丰富句子之间的关系。这种方法允许不同的句子在考虑重叠的词信息时进行互动。此外，图网络可以容纳额外的节点类型，例如用于多文档总结的文档节点。
- en: Doan et al. [[40](#bib.bib40)] propose a method for long document summarization
    by applying Heterogeneous Graph Neural Networks (HeterGNN) and introducing a homogeneous
    graph structure (HomoGNN). The HomoGNN focuses on sentence-level nodes to create
    a graph structure, enriching inter-sentence connections. Simultaneously, the HeterGNN
    explores the complex relationships between words and sentences, tackling intra-sentence
    connections. Both networks are constructed and updated using a Graph Attention
    Network model. In the HomoGNN, a BERT model is used for the initial encoding of
    sentences, while the HeterGNN uses a combination of CNN and BiLSTM for node feature
    extraction. After processing, the outputs of both networks are concatenated to
    form the final representation of sentences.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Doan 等人 [[40](#bib.bib40)] 提出了一种通过应用异构图神经网络 (HeterGNN) 和引入同质图结构 (HomoGNN) 来进行长文档摘要的方法。HomoGNN
    侧重于句子级节点以创建图结构，丰富句子间的连接。同时，HeterGNN 探索词与句子之间的复杂关系，处理句子内的连接。两个网络都使用图注意力网络模型进行构建和更新。在
    HomoGNN 中，使用 BERT 模型进行句子的初始编码，而 HeterGNN 则使用 CNN 和 BiLSTM 的组合进行节点特征提取。处理后，两个网络的输出被连接在一起，以形成句子的最终表示。
- en: '![Refer to caption](img/5226b6099939f214bbf6f76d807c0674.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5226b6099939f214bbf6f76d807c0674.png)'
- en: 'Figure 7: A two-phase pipeline model for extreme summarization [[40](#bib.bib40)].
    In the first phase, sentences are encoded using pre-trained BERT, and the [CLS]
    token information is passed through a graph attention layer. In the second phase,
    both word and sentence nodes are encoded and fed into a heterogeneous graph layer.
    The outputs from the two phases are concatenated and inputted into a multi-layer
    perceptron (MLP) layer for sentence label classification.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 极端摘要的两阶段管道模型 [[40](#bib.bib40)]。在第一阶段，使用预训练的 BERT 对句子进行编码，并将 [CLS] 标记的信息传递到图注意力层。在第二阶段，词节点和句子节点都被编码并输入到异构图层。两个阶段的输出被连接在一起，并输入到多层感知器
    (MLP) 层中进行句子标签分类。'
- en: 3.5 Transformer
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 Transformer
- en: Transformers models [[173](#bib.bib173), [39](#bib.bib39), [150](#bib.bib150)],
    which are the most popular deep learning architecture, have been a revolutionary
    force in the field of NLP, especially in text summarization. Unlike RNN or LSTM,
    Transformers use the self-attention mechanism, allowing them to capture dependencies
    regardless of their distance in the input text. This is particularly useful in
    text summarization tasks, where understanding the full context of a document is
    crucial.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型 [[173](#bib.bib173), [39](#bib.bib39), [150](#bib.bib150)]，作为最流行的深度学习架构，在
    NLP 领域，特别是在文本摘要中，已成为一种革命性力量。与 RNN 或 LSTM 不同，Transformer 使用自注意力机制，使其能够捕捉到输入文本中无论距离多远的依赖关系。这在文本摘要任务中尤为重要，因为理解文档的完整上下文至关重要。
- en: Transformer [[173](#bib.bib173)] follows a Seq2Seq architecture and consists
    of an encoder to map the input sequence into a latent space, and a decoder to
    map the latent representation back into an output sequence. At the heart of the
    Transformer model is the self-attention mechanism, which allows the model to weigh
    the significance of words in the input sequence when generating each word in the
    output sequence.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer [[173](#bib.bib173)] 遵循 Seq2Seq 架构，包括一个编码器，将输入序列映射到潜在空间中，以及一个解码器，将潜在表示映射回输出序列。Transformer
    模型的核心是自注意力机制，它使模型能够在生成输出序列中的每个单词时，权衡输入序列中单词的意义。
- en: BERT [[39](#bib.bib39)], which is Bidirectional Encoder Representations Transformers,
    pre-trains deep bidirectional representations from the unlabeled text by conditioning
    on both left and right context in all layers. After pre-training, the BERT model
    can be fine-tuned with an additional output layer to create state-of-the-art models
    for a wide range of tasks, including summarization, without task-specific architecture
    modifications. During the fine-tuned phase, the model is initialized with the
    pre-trained parameters, and all parameters are fine-tuned using labeled data from
    the downstream tasks. To use BERT for text summarization, a common method is to
    fine-tune it on a summarization task. BERTSUM [[101](#bib.bib101)] is an approach
    to utilize BERT for extractive summarization by adding an interval segment embedding
    and a positional embedding to the pre-trained BERT model, allowing the model to
    recognize sentences and their orders. These embeddings are learned during the
    fine-tuning process. During inference, the most important sentences are selected
    based on their scores to form the final summary.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: BERT [[39](#bib.bib39)]，即双向编码器表示变换器，通过在所有层中同时考虑左右上下文，从未标记文本中进行深度双向表示的预训练。预训练后，BERT
    模型可以通过附加输出层进行微调，创建适用于各种任务的最先进模型，包括摘要，而无需任务特定的架构修改。在微调阶段，模型使用预训练参数进行初始化，所有参数都使用来自下游任务的标记数据进行微调。要使用
    BERT 进行文本摘要，一种常见方法是对其进行摘要任务的微调。BERTSUM [[101](#bib.bib101)] 是一种利用 BERT 进行抽取式摘要的方法，通过向预训练的
    BERT 模型添加区间段嵌入和位置嵌入，使模型能够识别句子及其顺序。这些嵌入在微调过程中学习。在推理过程中，根据句子的分数选择最重要的句子，以形成最终摘要。
- en: '![Refer to caption](img/b7c7411f20401614c9e9da288e301dc0.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b7c7411f20401614c9e9da288e301dc0.png)'
- en: 'Figure 8: The overview architecture of the BERTSUM model [[101](#bib.bib101)].'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：BERTSUM 模型的概述架构 [[101](#bib.bib101)]。
- en: T5 [[153](#bib.bib153)], short for ”Text-to-Text Transfer Transformer”, is a
    unified model that treats every NLP problem as a text generation task, enabling
    the model to learn multiple tasks simultaneously and to learn shared representations
    across these tasks. In the case of text summarization, the model is trained to
    predict the summarized text given the original text prefixed with a task-specific
    instruction, like ”summarize:”, so the model learns to generate the summary based
    on the context and the given task. T5 is trained using a denoising auto-encoding
    objective, which is essentially a causal language modeling task with some noise
    in the input data. The model has to learn to predict the original clean text from
    the noisy version. This method forces the model to learn to understand and generate
    grammatically correct and contextually relevant text, a skill that’s very useful
    in generating coherent and relevant summaries.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: T5 [[153](#bib.bib153)]，即“文本到文本转换变换器”的缩写，是一个统一模型，将每个自然语言处理（NLP）问题视为文本生成任务，使模型能够同时学习多个任务，并在这些任务之间学习共享的表示。在文本摘要的情况下，模型被训练为在给定以特定任务说明开头的原始文本下预测摘要文本，比如“summarize:”，因此模型学习根据上下文和给定任务生成摘要。T5
    使用去噪自编码目标进行训练，这本质上是一个带有输入数据噪声的因果语言建模任务。模型必须学习从噪声版本预测原始干净文本。这种方法迫使模型学习理解和生成语法正确且上下文相关的文本，这一技能在生成连贯且相关的摘要时非常有用。
- en: BART (Bidirectional and Auto-Regressive Transformers) [[91](#bib.bib91)], is
    a denoising auto-encoder used for pertaining Seq2Seq models. The system works
    by corrupting text with an arbitrary noising function and then training the model
    to reconstruct the original text, using a standard Transformer-based neural machine
    translation architecture. This architecture generalizes the approach used with
    a bidirectional encoder and a left-to-right decoder and is particularly effective
    when fine-tuned for text generation tasks like summarization. Several different
    noising strategies were tested in BART, with the best performance achieved by
    randomly shuffling sentence order and using an innovative in-filling scheme, where
    segments of text are replaced with a single mask token. This forces the model
    to consider overall sentence length and make longer-range transformations to the
    input.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: BART（双向自回归变换器）[[91](#bib.bib91)]，是一个用于训练Seq2Seq模型的去噪自编码器。该系统通过用任意噪声函数破坏文本，然后训练模型重建原始文本，使用标准的基于变换器的神经机器翻译架构。这种架构在具有双向编码器和从左到右的解码器的情况下，通用化了该方法，并且在针对文本生成任务（如总结）进行微调时特别有效。在BART中测试了几种不同的噪声策略，其中随机打乱句子顺序和使用创新的填充方案表现最佳，该方案将文本段替换为一个单一的掩码标记。这迫使模型考虑整体句子长度，并对输入进行更长范围的转换。
- en: Pegasus [[196](#bib.bib196)], which stands for Pre-training with Extracted gap
    sentences for Abstractive Summarization, is a model that specifically focuses
    on abstractive text summarization. Pegasus’s main novelty lies in its pre-training
    strategy, which simulates summarization by masking certain sentences in the document.
    Instead of masking individual words or phrases, Pegasus masks entire sentences,
    treating the task as a sentence-level extraction problem. During this phase, the
    model learns to predict the ’masked’ sentences based on the rest of the text,
    developing a strong sense of sentence-level importance and relevance skills that
    are vital for text summarization. The model was tested on diverse domains including
    news, science, stories, and legislative bills, and showed strong performance on
    all tested datasets, as well as low-resource summarization.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Pegasus [[196](#bib.bib196)]，即“利用提取的缺失句子进行抽象总结的预训练”，是一个专注于抽象文本总结的模型。Pegasus的主要创新在于其预训练策略，它通过在文档中屏蔽某些句子来模拟总结。与其屏蔽单个单词或短语不同，Pegasus屏蔽整个句子，将任务视为一个句子级的提取问题。在这个阶段，模型通过根据其余文本预测被“屏蔽”的句子，培养了对句子级重要性和相关性的强烈感知能力，这对于文本总结至关重要。该模型在新闻、科学、故事和立法草案等多个领域进行了测试，并在所有测试数据集上表现出色，同时在低资源总结任务中也表现良好。
- en: 'BIGBIRD [[194](#bib.bib194)], a sparse attention mechanism designed to tackle
    the quadratic dependency of the sequence length, which is a limitation of Transformer-based
    models like BART. The BIGBIRD mechanism reduces this quadratic dependency to linear,
    meaning it can handle sequences up to 8 times longer than previously possible
    on the same hardware. This means that BIGBIRD can understand and generate much
    longer pieces of text, making it potentially useful for long document summarization.
    The model consists of three parts: a set of global tokens attending to all parts
    of the sequence, all tokens attending to a set of local neighboring tokens, and
    all tokens attending to a set of random tokens. BIGBIRD retains all the known
    theoretical properties of full transformers and extends the application of the
    attention-based model to tasks where long contexts are beneficial.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: BIGBIRD [[194](#bib.bib194)]，是一种稀疏注意力机制，旨在解决序列长度的二次依赖，这是像BART这样的基于变换器模型的限制。BIGBIRD机制将这种二次依赖减少到线性，这意味着它可以处理比以前在相同硬件上可能的序列长8倍的文本。这意味着BIGBIRD可以理解和生成更长的文本片段，使其在长文档总结中具有潜在的应用价值。该模型由三部分组成：一组全局标记关注序列的所有部分、所有标记关注一组局部邻近标记，以及所有标记关注一组随机标记。BIGBIRD保留了全变换器的所有已知理论属性，并将基于注意力的模型应用扩展到长上下文有利的任务中。
- en: Longformer [[13](#bib.bib13)] is another modification of the Transformer model
    designed to handle long sequences. The Longformer offers a linearly scaling attention
    mechanism to make it possible to process documents with thousands of tokens or
    more. Longformer’s attention mechanism is a combination of local windowed self-attention
    and task-motivated global attention. This drop-in replacement for the standard
    self-attention could achieve state-of-the-art results on character-level language
    modeling tasks. Additionally, Longformer-Encoder-Decoder (LED) is introduced as
    a variant of Longformer designed for long document generative Seq2Seq tasks. This
    model is also tested and proven effective on the long document summarization dataset.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer [[13](#bib.bib13)] 是另一种Transformer模型的改进，旨在处理长序列。Longformer提供了一种线性扩展的注意力机制，使处理包含数千个标记或更多的文档成为可能。Longformer的注意力机制结合了局部窗口自注意力和任务驱动的全局注意力。这种替代标准自注意力的机制可以在字符级语言建模任务中实现最先进的结果。此外，Longformer-Encoder-Decoder（LED）作为Longformer的变体被引入，专为长文档生成的Seq2Seq任务设计。该模型在长文档摘要数据集上经过测试并证明有效。
- en: '![Refer to caption](img/517620e7b9fb1aa928f58eea1bb5a6cf.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/517620e7b9fb1aa928f58eea1bb5a6cf.png)'
- en: 'Figure 9: By comparing the complete self-attention pattern and the attention
    pattern configuration in the Longformer model, we can observe the differences.
    [[13](#bib.bib13)].'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：通过比较完整的自注意力模式和Longformer模型中的注意力模式配置，我们可以观察到它们之间的差异。 [[13](#bib.bib13)]。
- en: GPT-1 [[150](#bib.bib150)] (Generative Pre-trained Transformer) is an auto-regressive
    language model, which can generate human-like by predicting the next word in a
    sequence. In the context of text summarization, GPT-1 is able to create summaries
    that are not just extracts of the original text but can rephrase or reframe the
    content in novel ways, capturing the essence of the document while potentially
    reducing its length significantly. However, since GPT-1 does not explicitly model
    the structure of the document beyond the sequence of words, it might not always
    maintain the coherence and relevance of the summary, especially for long or complex
    documents. GPT-2 [[151](#bib.bib151)] is an improvement over GPT-1, having more
    parameters and trained on a larger corpus of data. One important advantage of
    GPT-2 for summarization is its ability to generate fluent and coherent text due
    to its training objective and architecture. It can capture long-range dependencies
    in the text, rephrase the original text, and even generate novel sentences that
    were not in the original document but are consistent with its content. Furthermore,
    GPT-2 can be employed for summarization in a few-shot/zero-shot manner where the
    model is designed to make accurate predictions given only a few or no examples.
    GPT-3 [[19](#bib.bib19)] follows the design of its predecessor and has been found
    to generate exceptionally fluent and coherent text based on a larger model size-
    175 billion parameters. GPT-3 has a larger context window, meaning it can consider
    a significant portion of a document when generating a summary. This allows for
    more holistic and comprehensive summaries, especially when compared to models
    with smaller context windows that might not capture all necessary information.
    Apart from the typical summarization tasks, GPT-3 can be leveraged for a range
    of different summary types. Whether you’re interested in producing extractive
    or abstractive summaries, single-document or multi-document summaries, GPT-3 can
    be utilized to generate them. InstructGPT [[136](#bib.bib136)], a recent development,
    centers around training large language models to comprehend and follow instructions
    with the aid of human feedback. The authors employ a fine-tuning approach on GPT-3,
    utilizing a dataset of labeler demonstrations that outline the desired model behavior,
    starting from labeler-written prompts and responses. They initially fine-tuned
    the model using supervised learning and subsequently employed reinforcement learning
    techniques with human feedback for further fine-tuning. The authors’ findings
    indicate that fine-tuning models with human feedback hold promise in aligning
    language models with human intent, highlighting a fruitful direction for future
    research. Recently, a substantial multi-modal model known as GPT-4 [[133](#bib.bib133)]
    has emerged. It has the ability to process both image and text inputs and generate
    text outputs. In human exam evaluations, this model demonstrates exceptional performance,
    consistently outscoring the majority of human test takers. Despite the lack of
    specific information regarding the model’s structure, hyper-parameters, and training
    methodology in the paper, the results of GPT-4 exhibit remarkable advancements
    across diverse tasks, including summarization. Even though GPT-4 might introduce
    details or points that were not part of the original document, leading to ”hallucinations”,
    the development of this model marks a significant milestone in the advancement
    of AI systems that are both widely applicable and safely deployable.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1 [[150](#bib.bib150)]（生成预训练变换器）是一个自回归语言模型，通过预测序列中的下一个单词来生成类似人类的文本。在文本摘要的背景下，GPT-1
    能够生成不仅仅是原文的提取摘要，还可以以新颖的方式重新措辞或重新构架内容，捕捉文档的精髓，同时显著缩短其长度。然而，由于 GPT-1 不明确建模文档的结构，而仅基于词序列，它可能无法始终保持摘要的连贯性和相关性，特别是对于长篇或复杂的文档。GPT-2
    [[151](#bib.bib151)] 是 GPT-1 的改进版，拥有更多的参数并在更大的数据语料库上进行训练。GPT-2 在摘要生成中的一个重要优势是由于其训练目标和架构，能够生成流畅且连贯的文本。它可以捕捉文本中的长程依赖关系，重新措辞原始文本，甚至生成原文中没有但与其内容一致的新句子。此外，GPT-2
    可以以少量示例或无需示例的方式进行摘要生成，即使只有少量或没有示例，模型也能进行准确的预测。GPT-3 [[19](#bib.bib19)] 继承了其前身的设计，并且由于模型规模扩大——1750亿参数，已经发现生成的文本异常流畅且连贯。GPT-3
    具有更大的上下文窗口，这意味着它在生成摘要时可以考虑文档的显著部分。这使得摘要更为全面和完整，尤其是与上下文窗口较小的模型相比，后者可能无法捕捉所有必要的信息。除了典型的摘要任务外，GPT-3
    还可以用于各种不同类型的摘要。无论是生成提取式还是抽象式摘要，单文档还是多文档摘要，GPT-3 都可以用来生成它们。近期发展出的 InstructGPT [[136](#bib.bib136)]
    以训练大型语言模型理解和遵循指令为核心，辅以人工反馈。作者在 GPT-3 上采用了微调方法，利用标签者示范的数据集来概述期望的模型行为，从标签者编写的提示和响应开始。他们最初使用监督学习对模型进行了微调，随后采用了人工反馈的强化学习技术进行进一步微调。作者的研究结果表明，使用人工反馈对模型进行微调在使语言模型与人类意图对齐方面具有前景，强调了未来研究的一个富有成效的方向。最近，出现了一个重要的多模态模型，称为
    GPT-4 [[133](#bib.bib133)]。它能够处理图像和文本输入并生成文本输出。在人类考试评估中，这个模型表现出色，一直超越大多数人类考生。尽管论文中缺乏关于模型结构、超参数和训练方法的具体信息，但
    GPT-4 的结果在各种任务中展现了显著的进步，包括摘要生成。即使 GPT-4 可能引入了原文中不存在的细节或观点，导致“幻觉”，这一模型的开发标志着 AI
    系统在广泛应用和安全部署方面的一个重要里程碑。
- en: '![Refer to caption](img/4bae508295a2246be3bb2e3fe91e6bf9.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4bae508295a2246be3bb2e3fe91e6bf9.png)'
- en: 'Figure 10: The diagram showcases the three sequential steps of InstructGPT:
    (1) supervised fine-tuning (SFT), (2) reward model (RM) training, and (3) reinforcement
    learning through proximal policy optimization (PPO) using this reward model. The
    blue arrows indicate the utilization of this data to train InstructGPT. [[136](#bib.bib136)].'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：该图展示了 InstructGPT 的三个连续步骤：（1）监督微调（SFT），（2）奖励模型（RM）训练，以及（3）通过该奖励模型的近端策略优化（PPO）进行强化学习。蓝色箭头表示利用这些数据来训练
    InstructGPT。 [[136](#bib.bib136)]。
- en: 3.6 Reinforcement learning
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 强化学习
- en: Reinforcement Learning (RL) [[76](#bib.bib76), [119](#bib.bib119)] is a type
    of Machine Learning where an agent learns to behave in an environment, by performing
    certain actions and receiving rewards (or punishments) in return.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）[[76](#bib.bib76), [119](#bib.bib119)] 是一种机器学习类型，其中代理通过执行某些操作并获得奖励（或惩罚）来学习在环境中的行为。
- en: In the context of text summarization, the environment consists of the input
    document that needs to be summarized. The state could be the current part of the
    document being considered for summarization, along with the portion of the summary
    that has already been generated. The action might involve selecting a sentence
    from the document to include in the summary (for extractive summarization) or
    generating a sentence or phrase to add to the summary (for abstractive summarization).
    The reward is a measure of the quality of the generated summary. This could be
    based on a variety of factors, such as how well the summary represents the main
    points of the document, how grammatically correct and fluent it is, and so on.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本摘要的背景下，环境包括需要被摘要的输入文档。状态可以是当前正在考虑摘要的文档部分，以及已经生成的摘要部分。行动可能包括选择文档中的一句话以纳入摘要（对于抽取式摘要）或生成一个句子或短语以添加到摘要中（对于抽象式摘要）。奖励是生成摘要质量的度量，这可能基于各种因素，如摘要对文档主要观点的代表性、语法正确性和流畅性等。
- en: One of the key benefits of using RL for text summarization is that it allows
    for a more flexible and adaptive approach to summarization, compared to traditional
    supervised learning methods. RL can learn to adapt its summarization strategy
    based on the specific characteristics of each document and can optimize for long-term
    rewards (like the overall coherence and quality of the summary), rather than just
    short-term gains (like the accuracy of the next sentence).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RL 进行文本摘要的一个主要好处是，它比传统的监督学习方法提供了更灵活和自适应的摘要方法。RL 可以根据每个文档的具体特征调整其摘要策略，并可以优化长期奖励（如摘要的整体连贯性和质量），而不仅仅是短期收益（如下一句的准确性）。
- en: Narayan et al. [[127](#bib.bib127)] proposes a novel method for single document
    summarization using extractive summarization as a sentence ranking task, globally
    optimizing the ROUGE evaluation metric through a reinforcement learning objective.
    The authors argue that current cross-entropy training is sub-optimal for extractive
    summarization, tending to generate verbose summaries with unnecessary information.
    Their method improves this by learning to rank sentences for summary generation.
    The approach involves viewing the neural summarization model as an ”agent” in
    a reinforcement learning paradigm, which reads a document and predicts a relevance
    score for each sentence. The agent is then rewarded based on how well the extract
    resembles the gold-standard summary. The REINFORCE algorithm is used to update
    the agent, optimizing the final evaluation metric directly instead of maximizing
    the likelihood of the ground-truth labels, making the model more capable of discriminating
    among sentences.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Narayan 等人 [[127](#bib.bib127)] 提出了一个用于单文档摘要的新方法，将抽取式摘要作为一个句子排名任务，通过强化学习目标全局优化
    ROUGE 评估指标。作者认为，目前的交叉熵训练对于抽取式摘要来说并不理想，容易生成包含不必要信息的冗长摘要。他们的方法通过学习对句子进行排序来改进这一点。该方法将神经网络摘要模型视为强化学习范式中的“代理”，该代理阅读文档并预测每个句子的相关性得分。然后，代理会根据抽取与黄金标准摘要的相似程度进行奖励。REINFORCE
    算法被用来更新代理，直接优化最终评估指标，而不是最大化真实标签的可能性，使模型更能区分句子。
- en: '![Refer to caption](img/b2ed9384396e0a89411bb96954f8b34f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2ed9384396e0a89411bb96954f8b34f.png)'
- en: 'Figure 11: The extractive summarization model with reinforcement learning [[127](#bib.bib127)]
    employs a hierarchical encoder-decoder architecture to rank sentences based on
    their extract worthiness. A candidate summary is then formed by assembling the
    top-ranked sentences. The REWARD generator assesses the candidate summary against
    the gold summary, providing a reward signal that is utilized in the REINFORCE
    algorithm [[185](#bib.bib185)] to update the model.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：带有强化学习[[127](#bib.bib127)]的抽取式摘要模型采用层次编码器-解码器结构来根据句子的抽取价值进行排序。然后，通过组装排名最高的句子来形成候选摘要。REWARD生成器将候选摘要与黄金摘要进行比较，提供奖励信号，该信号在REINFORCE算法[[185](#bib.bib185)]中用于更新模型。
- en: Hyun et al. [[73](#bib.bib73)] introduce another model for unsupervised abstractive
    sentence summarization using reinforcement learning (RL). Unlike previous methods,
    which mainly utilize extractive summarization (removing words from texts), this
    method is abstractive, allowing for the generation of new words not found in the
    original text, thereby increasing flexibility and versatility. The approach involves
    developing a multi-summary learning mechanism that creates multiple summaries
    of varying lengths from a given text, with these summaries enhancing each other.
    The RL-based model used assesses the quality of summaries using rewards, considering
    factors such as the semantic similarity between the summary and the original text
    and the fluency of the generated summaries. The model also involves a pre-training
    task to achieve well-initialized model parameters for RL training.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Hyun等人[[73](#bib.bib73)]提出了一种用于无监督抽象句子摘要的模型，该模型使用强化学习（RL）。与以往主要利用抽取式摘要（从文本中去除词汇）的方法不同，该方法是抽象式的，允许生成在原文中未出现的新词，从而增加了灵活性和多样性。该方法涉及开发一种多摘要学习机制，该机制从给定文本中创建多种不同长度的摘要，这些摘要相互增强。所使用的基于RL的模型通过奖励评估摘要的质量，考虑了如摘要与原文之间的语义相似性和生成摘要的流畅性等因素。该模型还涉及一个预训练任务，以实现针对RL训练的良好初始化模型参数。
- en: 4 Data and Experimental Performance Analysis
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据和实验性能分析
- en: 4.1 Techniques for data processing
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据处理技术
- en: This sub-section will delve into the key techniques utilized for data processing.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节将**深入探讨**用于数据处理的关键技术。
- en: 4.1.1 Pre-training
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 预训练
- en: In the domain of text summarization, pre-training refers to the initial training
    phase of a model on a large, diverse corpus of text data, prior to fine-tuning
    it on a more specific summarization task. This strategy capitalized on the capabilities
    of extensive language models such as GPT [[150](#bib.bib150)], BART [[91](#bib.bib91)],
    and T5 [[153](#bib.bib153)], which have been pre-trained on vast quantities of
    text data to comprehend syntactic and semantic patterns within a language.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本摘要领域，预训练是指在对模型进行更具体的摘要任务微调之前，在大量多样的文本数据语料库上进行的初步训练阶段。这种策略利用了如GPT [[150](#bib.bib150)]、BART
    [[91](#bib.bib91)] 和 T5 [[153](#bib.bib153)] 等广泛语言模型的能力，这些模型已经在大量文本数据上进行预训练，以理解语言中的句法和语义模式。
- en: 4.1.2 Few-shot, zero-shot learning
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 少量样本、零样本学习
- en: Few-shot and zero-shot learning are terms used to describe scenarios where a
    model is required to make accurate predictions for new classes that were either
    minimally represented (few-shot) or completely absent (zero-shot) during the training
    phase [[187](#bib.bib187), [166](#bib.bib166)]. Few-shot learning in summarization
    usually implies a scenario where the model is trained on many examples from a
    few categories and is then expected to generalize to summarizing examples from
    new categories after seeing only a few examples from these new categories. Zero-shot
    learning in summarization, on the other hand, refers to a scenario where the model
    is expected to generalize to entirely new categories without seeing any examples
    from these categories during training. The idea behind these methods is to provide
    the model with a few examples or a description of the task at inference time,
    allowing it to adjust its predictions based on this new information. This often
    involves formulating the summarization task as a type of prompt that the model
    is designed to complete.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习和零样本学习是描述模型在训练阶段对新类别的准确预测能力的术语，这些新类别在训练阶段要么代表性较少（少样本），要么完全不存在（零样本）[[187](#bib.bib187),
    [166](#bib.bib166)]。在摘要任务中，少样本学习通常意味着模型在几个类别的众多示例上进行训练，然后期望模型在仅看到几个新类别的示例后能够总结这些新类别的示例。另一方面，零样本学习指的是模型在没有看到这些新类别示例的情况下，期望其能够推广到完全新类别的场景。这些方法的核心思想是在推理时为模型提供几个示例或任务描述，使其能够根据这些新信息调整预测。这通常涉及将摘要任务表述为模型需要完成的一种提示。
- en: 4.1.3 Prompting
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 提示设置
- en: Prompts play a crucial role in the current generation of language models, particularly
    those that are trained in a transformer-based architecture like GPT-4 [[133](#bib.bib133)]
    or T5 [[153](#bib.bib153)]. The term ”prompt” in the context of these models refers
    to the input given to the model to indicate the task it should perform. For text
    summarization, the prompt is typically the text that needs to be summarized. However,
    in the case of GPT-4 [[133](#bib.bib133)], T5 [[153](#bib.bib153)], and similar
    models, the prompt can also include a task description or examples to guide the
    model’s generation. This is especially important in few-shot and zero-shot learning
    scenarios. Choosing effective prompts is a bit of an art and can significantly
    impact the performance of the model. The best practices for creating prompts are
    still an active area of research, but a well-designed prompt often includes clear
    instructions and, when possible, an example of the desired output.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 提示在当前一代语言模型中发挥着关键作用，特别是那些基于转换器架构训练的模型，如GPT-4 [[133](#bib.bib133)]或T5 [[153](#bib.bib153)]。在这些模型的背景下，“提示”一词指的是提供给模型的输入，以指示其应该执行的任务。对于文本摘要，提示通常是需要被总结的文本。然而，在GPT-4
    [[133](#bib.bib133)]、T5 [[153](#bib.bib153)]及类似模型中，提示还可以包括任务描述或示例以指导模型生成。这在少样本和零样本学习场景中尤为重要。选择有效的提示有点像艺术，可能会显著影响模型的表现。创建提示的最佳实践仍然是一个活跃的研究领域，但一个设计良好的提示通常包括清晰的指令，并且在可能的情况下，提供期望输出的示例。
- en: '![Refer to caption](img/09e094ab530a8ecce39545c05eb109ed.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/09e094ab530a8ecce39545c05eb109ed.png)'
- en: 'Figure 12: The figure highlights the encoder layer of BART and provides annotated
    examples and a comparison between prefix-merging on the two auxiliary tasks (top,
    mid) and the application of the merged prefix on the Few-shot Query-Focused Summarization
    task using prefix-tuning (bottom). [[193](#bib.bib193)]'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：该图突出了BART的编码器层，并提供了带注释的示例以及在两个辅助任务上（顶部、中部）进行前缀合并的比较，并将合并后的前缀应用于使用前缀调优的少样本查询聚焦摘要任务（底部）。
    [[193](#bib.bib193)]
- en: 4.1.4 Domain adaptation
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 领域适应
- en: Domain adaptation in the context of text summarization pertains to the process
    of adapting a summarization model, initially trained on a specific domain (e.g.,
    news articles), to perform effectively on a different yet related domain (e.g.,
    scientific papers or legal documents) [[110](#bib.bib110), [121](#bib.bib121)].
    Fine-tuning the model on a smaller dataset from the target domain is a commonly
    employed approach. Additionally, transfer learning is utilized to capitalize on
    the knowledge acquired from one domain and apply it to another.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本摘要的背景下，领域适应涉及将一个初步在特定领域（例如新闻文章）上训练的摘要模型，调整以在另一个相关领域（例如科学论文或法律文档）上有效运行[[110](#bib.bib110),
    [121](#bib.bib121)]。对目标领域的小型数据集进行微调是常用的方法。此外，迁移学习被用来利用从一个领域获得的知识，并将其应用于另一个领域。
- en: 4.1.5 Tokenization, embedding and Decoding strategies
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 分词、嵌入和解码策略
- en: Tokenization is the initial step of dividing the text into individual units
    known as tokens, which serve as fundamental input elements for most natural language
    processing (NLP) models [[184](#bib.bib184), [109](#bib.bib109)]. Following tokenization,
    the tokens are converted into continuous vector representations through an embedding
    layer to create the input embeddings [[144](#bib.bib144)]. These embeddings are
    then fed into the model for further processing. Once the model generates an output
    sequence, such as a summary, the reverse process takes place, where each token
    is mapped back to its corresponding word in a vocabulary, and the words are subsequently
    joined together using spaces to produce human-readable text.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将文本分成称为令牌的单独单元的初步步骤，这些令牌作为大多数自然语言处理（NLP）模型的基本输入元素[[184](#bib.bib184), [109](#bib.bib109)]。分词之后，令牌通过嵌入层转换为连续的向量表示，以创建输入嵌入[[144](#bib.bib144)]。这些嵌入随后输入到模型中进行进一步处理。一旦模型生成了输出序列，例如摘要，反向过程会发生，其中每个令牌映射回词汇表中的对应单词，然后将单词用空格连接起来，生成可读的文本。
- en: 4.2 Dataset and Evaluation Metrics
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据集和评估指标
- en: In the field of text summarization, various datasets and evaluation metrics
    are used to train models and assess their performance. Each evaluation metric
    has its strengths and weaknesses. While some metrics are easy to compute, they
    might not accurately reflect the human judgment of quality as they are based solely
    on n-gram overlap. Some other metrics attempt to address this issue by taking
    semantic similarity into account. However, manual evaluation by human judges is
    still considered the gold standard, despite its scalability challenges. We will
    first discuss some commonly used datasets, and then we’ll talk about evaluation
    metrics.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本摘要领域，使用各种数据集和评估指标来训练模型并评估其性能。每个评估指标都有其优缺点。虽然有些指标计算容易，但它们可能无法准确反映人类对质量的判断，因为它们仅基于n-gram重叠。其他一些指标尝试通过考虑语义相似性来解决这个问题。然而，尽管人工评估仍然被认为是最终标准，但它的可扩展性挑战仍然存在。我们将首先讨论一些常用的数据集，然后讨论评估指标。
- en: 4.2.1 Dataset
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 数据集
- en: 'Table [4.2.1](#S4.SS2.SSS1 "4.2.1 Dataset ‣ 4.2 Dataset and Evaluation Metrics
    ‣ 4 Data and Experimental Performance Analysis ‣ Surveying the Landscape of Text
    Summarization with Deep Learning: A Comprehensive Review") presents a compilation
    of notable datasets in the field of text summarization. The ”Size” column indicates
    the respective counts of training, validation, and test documents available in
    each dataset.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表[4.2.1](#S4.SS2.SSS1 "4.2.1 数据集 ‣ 4.2 数据集和评估指标 ‣ 4 数据和实验性能分析 ‣ 通过深度学习审视文本摘要的全景综述")展示了文本摘要领域中一些显著的数据集。
    “大小”列表示每个数据集中可用的训练、验证和测试文档的数量。
- en: \tbl
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \tbl
- en: A comparison of different Datasets \topruleDataset Domain Tasks Size Document
    Length Summary Length DUC2004 [[36](#bib.bib36)] News single-document multi-document
    cross-lingual query-focused Abstractive 500 - $<$= 75 bytes (single) $<$= 665
    bytes (multi) CNN/Daily Mail [[125](#bib.bib125)] News single-document Headline
    query-focused Abstractive Extractive 286,817 13,368 11,487 766 words average 53
    words average XSum [[126](#bib.bib126)] News single-document cross-lingual Abstractive
    204,045 11,332 11,334 431 words average 23 words average WikiSum [[100](#bib.bib100)]
    Wiki multi-document Abstractive 1,865,750 233,252 232,998 - - Multi-News [[47](#bib.bib47)]
    News multi-document Abstractive Extractive 44,972 5,622 5,622 2,103 words average
    263 words average BillSum [[79](#bib.bib79)] Legal single-document Long-document
    Abstractive 18,949 1,237 3,269 1,592 words average 197 words average PubMed [[35](#bib.bib35)]
    Medical single-document Long-document Abstractive 119,924 6,633 6,658 3,016 words
    average 203 words average arXiv [[35](#bib.bib35)] Scientific single-document
    Long-document Abstractive 203,037 6,436 6,440 4,938 words average 220 words average
    XGLUE [[93](#bib.bib93)] News cross-lingual Headline Abstractive 300,000 50,000
    50,000 - - BIGPATENT [[162](#bib.bib162)] Patent single-document Long-document
    Extractive Abstractive 1,207,222 67,068 67,072 3,572 words average 116 words average
    Newsroom [[63](#bib.bib63)] News single-document Headline Extractive Abstractive
    995,041 108,837 108,862 658 words average 26 words average MLSUM [[160](#bib.bib160)]
    News single-document cross-lingual Multi-lingual Abstractive 287,096 11,400 10,700
    790 words average 55 words average \botrule
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数据集的比较 \toprule数据集 域 任务 规模 文档长度 摘要长度 DUC2004 [[36](#bib.bib36)] 新闻 单文档 多文档
    跨语言 查询导向 抽象 500 - $<$= 75 字节（单文档） $<$= 665 字节（多文档） CNN/Daily Mail [[125](#bib.bib125)]
    新闻 单文档 标题 查询导向 抽象 抽取 286,817 13,368 11,487 平均 766 字词 平均 53 字词 XSum [[126](#bib.bib126)]
    新闻 单文档 跨语言 抽象 204,045 11,332 11,334 平均 431 字词 平均 23 字词 WikiSum [[100](#bib.bib100)]
    维基 多文档 抽象 1,865,750 233,252 232,998 - - Multi-News [[47](#bib.bib47)] 新闻 多文档 抽象
    抽取 44,972 5,622 5,622 平均 2,103 字词 平均 263 字词 BillSum [[79](#bib.bib79)] 法律 单文档
    长文档 抽象 18,949 1,237 3,269 平均 1,592 字词 平均 197 字词 PubMed [[35](#bib.bib35)] 医疗 单文档
    长文档 抽象 119,924 6,633 6,658 平均 3,016 字词 平均 203 字词 arXiv [[35](#bib.bib35)] 科学 单文档
    长文档 抽象 203,037 6,436 6,440 平均 4,938 字词 平均 220 字词 XGLUE [[93](#bib.bib93)] 新闻 跨语言
    标题 抽象 300,000 50,000 50,000 - - BIGPATENT [[162](#bib.bib162)] 专利 单文档 长文档 抽取 抽象
    1,207,222 67,068 67,072 平均 3,572 字词 平均 116 字词 Newsroom [[63](#bib.bib63)] 新闻 单文档
    标题 抽取 抽象 995,041 108,837 108,862 平均 658 字词 平均 26 字词 MLSUM [[160](#bib.bib160)]
    新闻 单文档 跨语言 多语言 抽象 287,096 11,400 10,700 平均 790 字词 平均 55 字词 \botrule
- en: 4.2.2 Evaluation Metrics
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 评估指标
- en: 'Evaluating the quality of generated summaries is crucial in the text summarization
    task. Here is an assortment of evaluation metrics commonly employed in summarization:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 评估生成摘要的质量在文本摘要任务中至关重要。以下是常用于摘要生成的评估指标：
- en: $\blacktriangleright$
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: 'Rouge (Recall-Oriented Understudy for Gisting Evaluation) [[94](#bib.bib94)]:
    is a set of evaluation metrics used for evaluating automatic summarization. It
    compares the system-generated output with a set of reference summaries. ROUGE-N
    measures the overlap of N-grams (a contiguous sequence of N items from a given
    sample of text or speech) between the system and reference summaries. It includes
    metrics like ROUGE-1 (for unigrams), ROUGE-2 (for bigrams), and so on. ROUGE-L
    metric measures the Longest Common Subsequence (LCS) between the system and reference
    summaries. LCS takes into account sentence-level structure similarity naturally
    and identifies the longest co-occurring in sequence n-grams automatically.'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Rouge（Recall-Oriented Understudy for Gisting Evaluation）[[94](#bib.bib94)]：是一组用于评估自动摘要的指标。它将系统生成的输出与一组参考摘要进行比较。ROUGE-N
    衡量系统摘要和参考摘要之间的 N-gram（来自文本或语音样本的连续 N 项）的重叠。包括 ROUGE-1（针对单字词），ROUGE-2（针对双字词）等指标。ROUGE-L
    指标衡量系统摘要和参考摘要之间的最长公共子序列（LCS）。LCS 自然考虑句子级结构相似性，并自动识别最长的顺序 n-gram。
- en: $\blacktriangleright$
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: 'BLEU (Bilingual Evaluation Understudy) [[140](#bib.bib140)]: is an evaluation
    metric initially developed for assessing the quality of machine-translated text,
    but it has also been used in text summarization tasks. It is a precision-based
    metric that compares the system-generated summary with one or more reference summaries.
    BLEU operates at the n-gram level to measure the overlap of n-grams between the
    generated output and the reference texts. It calculates the precision for each
    n-gram size (usually from 1-gram to 4-gram) and takes a weighted geometric mean
    to compute the final score.'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BLEU（双语评估替代指标）[[140](#bib.bib140)]: 是一种最初为评估机器翻译文本质量而开发的评估指标，但也用于文本摘要任务。它是一种基于精确度的指标，通过将系统生成的摘要与一个或多个参考摘要进行比较来工作。BLEU在n-gram层面上测量生成的输出与参考文本之间n-gram的重叠情况。它计算每个n-gram大小的精确度（通常从1-gram到4-gram），并取加权几何平均值来计算最终得分。'
- en: $\blacktriangleright$
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: 'METEOR (Metric for Evaluation of Translation with Explicit Ordering) [[9](#bib.bib9)]:
    is an evaluation metric initially designed for machine translation tasks but also
    used in text summarization evaluations. Unlike the previously mentioned metrics
    like ROUGE and BLEU that mainly focus on recall and precision at the n-gram level,
    METEOR incorporates more linguistic features and tries to align the generated
    text and the reference at the semantic level, thus potentially capturing the quality
    of the output more accurately.'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'METEOR（显式排序翻译评估指标）[[9](#bib.bib9)]: 是一种最初为机器翻译任务设计的评估指标，但也用于文本摘要评估。与之前提到的主要关注n-gram层面的召回率和精确度的ROUGE和BLEU等指标不同，METEOR结合了更多的语言特征，并尝试在语义层面对齐生成的文本和参考文本，从而可能更准确地捕捉输出的质量。'
- en: $\blacktriangleright$
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: 'Pyramid Score [[131](#bib.bib131)]: is based on the principle that a perfect
    summary could include any of several valid points from the source text, and as
    such, it would not be fair to penalize a summary for not including specific points.
    In the Pyramid Score method, human assessors identify Summary Content Units (SCUs)
    in a set of model summaries, which are essentially nuggets of information. Each
    SCU is assigned a weight based on how many model summaries it appears in. The
    Pyramid Score is then computed for a system-generated summary by adding up the
    weights of the SCUs it contains and normalizing this sum by the maximum possible
    score achievable by any summary of the same length. Pyramid scoring acknowledges
    the potential variation in content selection across different acceptable summaries.
    However, this method is quite labor-intensive because it requires human assessors
    to perform detailed content analysis on the model summaries.'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Pyramid Score [[131](#bib.bib131)]: 基于一个原则，即一个完美的摘要可以包含源文本中的多个有效点，因此不公平地惩罚一个未包含特定点的摘要。在Pyramid
    Score方法中，人类评估者在一组模型摘要中识别摘要内容单元（SCU），这些单元本质上是信息的片段。每个SCU根据其在多少个模型摘要中出现的情况分配权重。然后，通过将生成的摘要包含的SCU权重相加，并通过任何相同长度摘要可以达到的最大可能得分来归一化这个总和，从而计算Pyramid
    Score。Pyramid评分承认不同可接受摘要之间内容选择的潜在差异。然而，这种方法非常费力，因为它需要人类评估者对模型摘要进行详细的内容分析。'
- en: $\blacktriangleright$
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: 'CIDEr (Consensus-based Image Description Evaluation) [[174](#bib.bib174)]:
    is an evaluation metric primarily designed for assessing the quality of image
    captions in the context of image captioning tasks. It also gets used in text summarization
    to some extent. The fundamental idea behind CIDEr is that words that are more
    important to a description should have higher weights.'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CIDEr（共识基础图像描述评估）[[174](#bib.bib174)]: 是一种主要设计用于评估图像描述任务中图像字幕质量的评估指标。在一定程度上，它也被用于文本摘要。CIDEr的基本思想是，对描述更重要的词应该具有更高的权重。'
- en: $\blacktriangleright$
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: 'BERTScore [[198](#bib.bib198)]: is an automatic evaluation metric for natural
    language generation tasks, including text summarization. Unlike traditional metrics
    like ROUGE and BLEU which rely on n-gram overlaps, BERTScore leverages the contextual
    embeddings from the pre-trained BERT model to evaluate the generated text.'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BERTScore [[198](#bib.bib198)]: 是一种用于自然语言生成任务（包括文本摘要）的自动评估指标。与传统的依赖n-gram重叠的ROUGE和BLEU等指标不同，BERTScore利用预训练BERT模型中的上下文嵌入来评估生成的文本。'
- en: $\blacktriangleright$
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\blacktriangleright$
- en: 'Moverscore [[202](#bib.bib202)]: is based on two fundamental principles: the
    use of contextualized embeddings and the Earth Mover’s Distance (EMD), also known
    as the Wasserstein distance. Contextualized embeddings, such as BERT embeddings,
    represent words or phrases within the context they appear, providing a more meaningful
    representation of the text. The Earth Mover’s Distance is a measure of the distance
    between two probability distributions over a region, and it’s used here to measure
    the distance between the embeddings of the generated summary and the reference
    summary.'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Moverscore [[202](#bib.bib202)]：基于两个基本原则：使用上下文化的嵌入和地球搬运工距离（EMD），也称为 Wasserstein
    距离。上下文化的嵌入，如 BERT 嵌入，在其出现的上下文中表示词或短语，提供了文本的更有意义的表示。地球搬运工距离是衡量区域内两个概率分布之间距离的度量，这里用来衡量生成的摘要和参考摘要的嵌入之间的距离。
- en: 'Table [4.2.2](#S4.SS2.SSS2 "4.2.2 Evaluation Metrics ‣ 4.2 Dataset and Evaluation
    Metrics ‣ 4 Data and Experimental Performance Analysis ‣ Surveying the Landscape
    of Text Summarization with Deep Learning: A Comprehensive Review") shows some
    experimental results of popular models on CNN/Daily Mail. Although automatic metrics
    are widely used, they do not always align well with human judgments of summary
    quality. Human evaluation is considered the gold standard, but it’s time-consuming
    and costly. Thus, a combination of automatic and human evaluation is often used
    in practice.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [4.2.2](#S4.SS2.SSS2 "4.2.2 Evaluation Metrics ‣ 4.2 Dataset and Evaluation
    Metrics ‣ 4 Data and Experimental Performance Analysis ‣ Surveying the Landscape
    of Text Summarization with Deep Learning: A Comprehensive Review") 显示了一些流行模型在
    CNN/Daily Mail 上的实验结果。尽管自动指标被广泛使用，但它们并不总是与人类对摘要质量的判断一致。人类评估被认为是黄金标准，但它耗时且成本高。因此，实际应用中常常结合自动评估和人工评估。'
- en: \tbl
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: \tbl
- en: A comparison of different Models on CNN/Daily Mail [[125](#bib.bib125)] \topruleModels
    Model Rouge-1 Rouge-2 Rouge-L Attentional RNN [[125](#bib.bib125)] RNN 35.46 13.30
    32.65 Pointer-Generator [[161](#bib.bib161)] RNN 39.53 17.28 36.38 DynamicConv
    [[186](#bib.bib186)] CNN 39.84 16.25 36.73 TaLK Convolution [[98](#bib.bib98)]
    CNN 40.59 18.97 36.81 RL with intra-attention [[143](#bib.bib143)] RL 41.16 15.75
    39.08 RNN-ext+abs+RL+rerank [[28](#bib.bib28)] RNN+RL 39.66 15.85 37.34 BILSTM+GNN+LSTM+POINTER
    [[50](#bib.bib50)] GNN+LSTM 38.10 16.10 33.20 Graph-Based Attentional LSTM [[168](#bib.bib168)]
    GNN+LSTM 38.10 13.90 34.00 Transformer [[173](#bib.bib173)] Transformer 39.50
    16.06 36.63 PEGASUS [[196](#bib.bib196)] Transformer 44.17 21.47 41.11 BART [[91](#bib.bib91)]
    Transformer 44.16 21.28 40.90 SEASON [[180](#bib.bib180)] Transformer 46.38 22.83
    43.18 BART.GPT-4 [[104](#bib.bib104)] Transformer 63.22 44.70 - \botrule
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNN/Daily Mail 上不同模型的比较 [[125](#bib.bib125)] \toprule模型 Model Rouge-1 Rouge-2
    Rouge-L Attentional RNN [[125](#bib.bib125)] RNN 35.46 13.30 32.65 Pointer-Generator
    [[161](#bib.bib161)] RNN 39.53 17.28 36.38 DynamicConv [[186](#bib.bib186)] CNN
    39.84 16.25 36.73 TaLK Convolution [[98](#bib.bib98)] CNN 40.59 18.97 36.81 RL
    with intra-attention [[143](#bib.bib143)] RL 41.16 15.75 39.08 RNN-ext+abs+RL+rerank
    [[28](#bib.bib28)] RNN+RL 39.66 15.85 37.34 BILSTM+GNN+LSTM+POINTER [[50](#bib.bib50)]
    GNN+LSTM 38.10 16.10 33.20 Graph-Based Attentional LSTM [[168](#bib.bib168)] GNN+LSTM
    38.10 13.90 34.00 Transformer [[173](#bib.bib173)] Transformer 39.50 16.06 36.63
    PEGASUS [[196](#bib.bib196)] Transformer 44.17 21.47 41.11 BART [[91](#bib.bib91)]
    Transformer 44.16 21.28 40.90 SEASON [[180](#bib.bib180)] Transformer 46.38 22.83
    43.18 BART.GPT-4 [[104](#bib.bib104)] Transformer 63.22 44.70 - \botrule
- en: 5 Summary
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 摘要
- en: 5.1 Challenge and Future
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 挑战与未来
- en: Text summarization is an intriguing and challenging task in the realm of natural
    language processing. In recent years, significant advancements have been made
    with the aid of deep learning (DL) models. Novel concepts such as neural embedding,
    attention mechanism, self-attention, Transformer, BERT, and GPT-4 have propelled
    the field forward, resulting in rapid progress over the past decade. However,
    despite these advancements, there are still notable challenges that need to be
    addressed. This section aims to highlight some of the remaining challenges in
    text summarization and explore potential research directions that can contribute
    to further advancements in the field. By addressing these challenges and exploring
    new avenues, we can continue to push the boundaries of text summarization and
    unlock its full potential.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要是在自然语言处理领域中一个有趣且具有挑战性的任务。近年来，在深度学习（DL）模型的帮助下取得了显著进展。神经嵌入、注意机制、自注意力、Transformer、BERT
    和 GPT-4 等新概念推动了该领域的发展，使得过去十年进展迅速。然而，尽管取得了这些进展，仍然存在需要解决的显著挑战。本节旨在突出一些文本摘要中尚未解决的挑战，并探索可能的研究方向，以推动该领域的进一步发展。通过解决这些挑战和探索新的途径，我们可以继续突破文本摘要的界限，释放其全部潜力。
- en: One critical aspect is the need to understand the context of the document, encompassing
    semantics, syntactic structure, and discourse organization. Deep learning models
    often struggle with complex or ambiguous language, idiomatic expressions, and
    domain-specific jargon, making it difficult to achieve accurate and meaningful
    summaries.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键方面是理解文档的上下文，包括语义、句法结构和话语组织。深度学习模型常常难以处理复杂或模糊的语言、习语表达和领域特定术语，从而使得生成准确且有意义的摘要变得困难。
- en: A well-crafted summary should exhibit coherence and cohesion, ensuring that
    ideas logically connect and the text flows smoothly. Models must generate summaries
    that preserve the integrity of the original text’s meaning without introducing
    inconsistencies or redundancies. This requires a deep understanding of the main
    ideas, supporting details, and their interrelationships. Identifying important
    content poses a significant challenge, as it necessitates discerning the relevance
    and significance of various elements in the document.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一份精心制作的摘要应展现连贯性和一致性，确保思想逻辑连接，文本流畅。模型必须生成保留原文意义的摘要，而不引入不一致性或冗余。这需要对主要思想、支持细节及其相互关系有深刻理解。识别重要内容是一个重大挑战，因为这需要判断文档中各种元素的相关性和重要性。
- en: Summarizing long documents, such as legal or research papers, presents additional
    hurdles. These documents often contain complex sentence structures, advanced vocabulary,
    and important information distributed throughout the text. Creating concise summaries
    that capture the key points while maintaining accuracy becomes a daunting task.
    Furthermore, the lack of labeled training data exacerbates the challenges. Supervised
    learning approaches for text summarization rely on substantial amounts of annotated
    data, which can be expensive and time-consuming to create.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 总结长篇文档，如法律或研究论文，面临额外的难题。这些文档通常包含复杂的句子结构、高级词汇，以及分散在文本中的重要信息。创建简洁的摘要，以捕捉关键点，同时保持准确性，成为一项艰巨的任务。此外，缺乏标注的训练数据使挑战加剧。文本摘要的监督学习方法依赖于大量的注释数据，而这些数据的创建既昂贵又耗时。
- en: Evaluating the quality of generated summaries is another ongoing challenge.
    Automatic evaluation metrics, such as ROUGE, BLEU, or BERTScore, do not always
    align perfectly with human judgment. Manual evaluation, while providing more accurate
    insights, is a labor-intensive process. Overcoming these challenges requires the
    development of better evaluation metrics that align more closely with human perceptions
    of summary quality.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 评估生成的摘要质量是另一个持续的挑战。自动评估指标，如ROUGE、BLEU或BERTScore，往往与人工判断不完全一致。虽然手动评估提供了更准确的见解，但这是一个劳动密集的过程。克服这些挑战需要开发更好的评估指标，使其与人类对摘要质量的感知更加一致。
- en: Summarization tasks become more intricate when they are domain-specific, such
    as in medical or legal contexts. These domains often employ specialized language,
    requiring a higher level of understanding and accuracy. Additionally, summarizing
    information from multiple documents introduces further complexities. Models must
    eliminate redundant information, handle potentially conflicting details, and synthesize
    the most relevant content from various sources.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要任务在特定领域（如医疗或法律领域）变得更加复杂。这些领域通常使用专业术语，要求更高的理解和准确性。此外，从多个文档中总结信息会引入更多复杂性。模型必须消除冗余信息，处理可能冲突的细节，并从各种来源中综合最相关的内容。
- en: As the field progresses, advancements in deep learning models, particularly
    transformer-based architectures like BERT, GPT-4, and T5, offer promising opportunities
    for improved performance in text summarization tasks. Fine-tuning pre-trained
    models on specific summarization objectives has shown great potential and is expected
    to continue. Furthermore, as we become increasingly interconnected globally, there
    will be a growing demand for models capable of summarizing text in different languages
    or even across languages.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 随着领域的发展，深度学习模型的进步，特别是像BERT、GPT-4和T5这样的基于变换器的架构，提供了提高文本摘要任务性能的良好机会。对特定摘要目标的预训练模型进行微调显示了巨大的潜力，并预计将继续。此此外，随着全球化的日益加深，对能够用不同语言甚至跨语言总结文本的模型的需求将不断增加。
- en: Addressing the challenges of explainability, transparency, bias, data efficiency,
    multi-modal summarization, and personalized summarization are areas that will
    likely receive significant attention in future research. Explainable and transparent
    AI models are becoming increasingly important, and efforts to develop models that
    can provide reasoning for their decisions are expected. The development of better
    evaluation metrics, mitigating biases, exploring more data-efficient methods,
    handling multi-modal information, and catering to personalized summarization needs
    are all potential avenues for further advancement in the field.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 解决可解释性、透明度、偏见、数据效率、多模态总结和个性化总结等挑战是未来研究中可能会受到重大关注的领域。可解释和透明的AI模型变得越来越重要，预计将会努力开发能够为其决策提供推理的模型。发展更好的评估指标、减轻偏见、探索更具数据效率的方法、处理多模态信息以及满足个性化总结需求，都是该领域进一步发展的潜在方向。
- en: 5.2 Conclusion
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结论
- en: This article presents a comprehensive survey of over 100 deep learning models
    developed in the last decade, highlighting their significant advancements in various
    text summarization tasks. Additionally, we provide an overview of popular summarization
    datasets and conduct a quantitative analysis to assess the performance of these
    models on several public benchmarks. Furthermore, we address open challenges in
    the field and propose potential future research directions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了过去十年开发的100多个深度学习模型的综合调查，突出显示了这些模型在各种文本总结任务中的重大进展。此外，我们提供了流行总结数据集的概述，并进行了定量分析，以评估这些模型在多个公共基准上的表现。此外，我们还讨论了该领域的开放挑战，并提出了潜在的未来研究方向。
- en: References
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Deen Mohammad Abdullah and Yllias Chali. Towards generating query to perform
    query focused abstractive summarization using pre-trained model. In Proceedings
    of the 13th International Conference on Natural Language Generation, pages 80–85,
    Dublin, Ireland, December 2020. Association for Computational Linguistics.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Deen Mohammad Abdullah 和 Yllias Chali. 生成查询以执行查询聚焦的抽象总结：基于预训练模型的研究。在第十三届国际自然语言生成会议论文集，页80–85，爱尔兰都柏林，2020年12月。计算语言学协会。'
- en: '[2] Laith Abualigah, Mohammad Qassem Bashabsheh, Hamzeh Alabool, and Mohammad
    Shehab. Text summarization: A brief review. Recent Advances in NLP: The Case of
    Arabic Language, 874:1, 2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Laith Abualigah、Mohammad Qassem Bashabsheh、Hamzeh Alabool 和 Mohammad Shehab.
    文本总结：简要回顾。近期NLP进展：阿拉伯语案例，874：1，2019。'
- en: '[3] Stergos Afantenos, Vangelis Karkaletsis, and Panagiotis Stamatopoulos.
    Summarization from medical documents: a survey. Artificial intelligence in medicine,
    33(2):157–177, 2005.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Stergos Afantenos、Vangelis Karkaletsis 和 Panagiotis Stamatopoulos. 医疗文档中的总结：一项调查。医学中的人工智能，33(2)：157–177，2005。'
- en: '[4] Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D
    Trippe, Juan B Gutierrez, and Krys Kochut. Text summarization techniques: a brief
    survey. arXiv preprint arXiv:1707.02268, 2017.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Mehdi Allahyari、Seyedamin Pouriyeh、Mehdi Assefi、Saeid Safaei、Elizabeth
    D Trippe、Juan B Gutierrez 和 Krys Kochut. 文本总结技术：简要调查。arXiv预印本 arXiv:1707.02268，2017。'
- en: '[5] Narendra Andhale and Laxmi A Bewoor. An overview of text summarization
    techniques. In 2016 international conference on computing communication control
    and automation (ICCUBEA), pages 1–7\. IEEE, 2016.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Narendra Andhale 和 Laxmi A Bewoor. 文本总结技术概述。在2016年国际计算通信控制与自动化会议（ICCUBEA），第1–7页。IEEE，2016。'
- en: '[6] Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike, Tomoko Ohkuma, Hiroshi
    Masuichi, and Kazuhiko Ohe. Text2table: Medical text summarization system based
    on named entity recognition and modality identification. In Proceedings of the
    BioNLP 2009 Workshop, pages 185–192, 2009.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Eiji Aramaki、Yasuhide Miura、Masatsugu Tonoike、Tomoko Ohkuma、Hiroshi Masuichi
    和 Kazuhiko Ohe. Text2table：基于命名实体识别和模态识别的医疗文本总结系统。在BioNLP 2009研讨会论文集，页185–192，2009。'
- en: '[7] Aqil M Azmi and Suha Al-Thanyyan. A text summarizer for arabic. Computer
    Speech & Language, 26(4):260–273, 2012.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Aqil M Azmi 和 Suha Al-Thanyyan. 阿拉伯语文本总结器。计算机语音与语言，26(4)：260–273，2012。'
- en: '[8] Yu Bai, Yang Gao, and Heyan Huang. Cross-lingual abstractive summarization
    with limited parallel resources. In Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 6910–6924,
    Online, August 2021\. Association for Computational Linguistics.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 余白、杨高和何彦黄。《使用有限的平行资源进行跨语言抽象摘要》。在第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第1卷：长篇论文）上发表，页面6910–6924，线上，2021年8月。计算语言学协会。'
- en: '[9] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation
    with improved correlation with human judgments. In Proceedings of the ACL Workshop
    on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or
    Summarization, pages 65–72, Ann Arbor, Michigan, June 2005\. Association for Computational
    Linguistics.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 萨坦吉夫·巴纳吉和阿隆·拉维。《METEOR：一种改进与人工评判相关性的自动MT评估指标》。在ACL机器翻译和/或摘要评估措施研讨会论文集中，页面65–72，安娜堡，密歇根州，2005年6月。计算语言学协会。'
- en: '[10] Michele Banko, Vibhu O Mittal, and Michael J Witbrock. Headline generation
    based on statistical translation. In Proceedings of the 38th Annual Meeting of
    the Association for Computational Linguistics, pages 318–325, 2000.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 米歇尔·班科、维布胡·O·米特尔和迈克尔·J·威特布罗克。《基于统计翻译的标题生成》。在第38届计算语言学协会年会上发表，页面318–325，2000年。'
- en: '[11] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang,
    Songhao Piao, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unilmv2: Pseudo-masked
    language models for unified language model pre-training. In Preprint, 2020.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 杭博·包、李东、傅如伟、温辉·王、南杨、肖栋·刘、余旺、宋浩·廖、简锋·高、明周和萧文·洪。《Unilmv2：用于统一语言模型预训练的伪遮蔽语言模型》。在预印本中，2020年。'
- en: '[12] Regina Barzilay and Kathleen R McKeown. Sentence fusion for multidocument
    news summarization. Computational Linguistics, 31(3):297–328, 2005.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 雷吉娜·巴齐莱和凯瑟琳·R·麦克基恩。《用于多文档新闻摘要的句子融合》。计算语言学，31(3):297–328，2005年。'
- en: '[13] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document
    transformer, 2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 伊兹·贝尔塔吉、马修·E·彼得斯和阿尔曼·科汉。《Longformer：长文档变换器》，2020年。'
- en: '[14] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic
    language model. Advances in neural information processing systems, 13, 2000.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 约书亚·本吉奥、雷让·迪沙姆和帕斯卡尔·文森。《神经概率语言模型》。神经信息处理系统进展，13，2000年。'
- en: '[15] Neelima Bhatia and Arunima Jaiswal. Automatic text summarization and it’s
    methods-a review. In 2016 6th international conference-cloud system and big data
    engineering (Confluence), pages 65–72\. IEEE, 2016.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 尼利玛·巴蒂亚和阿鲁尼玛·贾斯瓦尔。《自动文本摘要及其方法综述》。在2016年第6届国际会议——云系统与大数据工程（Confluence）上发表，页面65–72。IEEE，2016年。'
- en: '[16] Florian Boudin, Stéphane Huet, and Juan-Manuel Torres-Moreno. A graph-based
    approach to cross-language multi-document summarization. Polibits, 43:113–118,
    2011.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 弗洛里安·布丁、斯特凡·惠特和胡安-曼努埃尔·托雷斯-莫雷诺。《基于图的跨语言多文档摘要方法》。Polibits，43:113–118，2011年。'
- en: '[17] Ronald Brandow, Karl Mitze, and Lisa F Rau. Automatic condensation of
    electronic publications by sentence selection. Information Processing & Management,
    31(5):675–685, 1995.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 罗纳德·布兰道、卡尔·米策和莉莎·F·劳。《通过句子选择自动浓缩电子出版物》。信息处理与管理，31(5):675–685，1995年。'
- en: '[18] Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual
    web search engine. Computer networks and ISDN systems, 30(1-7):107–117, 1998.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 谢尔盖·布林和劳伦斯·佩奇。《大规模超文本网络搜索引擎的解剖》。计算机网络与ISDN系统，30(1-7):107–117，1998年。'
- en: '[19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 汤姆·布朗、本杰明·曼、尼克·赖德、梅拉尼·苏比亚、贾雷德·D·卡普兰、普拉夫拉·达里瓦尔、阿尔温德·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔等。《语言模型是少样本学习者》。神经信息处理系统进展，33:1877–1901，2020年。'
- en: '[20] Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel Weld. TLDR: Extreme summarization
    of scientific documents. In Findings of the Association for Computational Linguistics:
    EMNLP 2020, pages 4766–4777, Online, November 2020\. Association for Computational
    Linguistics.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 伊莎贝尔·卡乔拉、凯尔·洛、阿尔曼·科汉和丹尼尔·韦尔德。《TLDR：科学文档的极端摘要》。在计算语言学协会发现：EMNLP 2020上发表，页面4766–4777，线上，2020年11月。计算语言学协会。'
- en: '[21] Xiaoyan Cai and Wenjie Li. Mutually reinforced manifold-ranking based
    relevance propagation model for query-focused multi-document summarization. IEEE
    transactions on audio, speech, and language processing, 20(5):1597–1607, 2012.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Xiaoyan Cai 和 Wenjie Li。基于相互强化流形排序的查询重点多文档总结相关性传播模型。IEEE音频、语音与语言处理交易，20(5):1597–1607，2012年。'
- en: '[22] Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming Zhou. Ranking with
    recursive neural networks and its application to multi-document summarization.
    In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,
    AAAI’15, page 2153–2159\. AAAI Press, 2015.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Ziqiang Cao，Furu Wei，Li Dong，Sujian Li 和 Ming Zhou。使用递归神经网络进行排名及其在多文档总结中的应用。在第29届AAAI人工智能会议论文集中，AAAI’15，页码2153–2159。AAAI出版社，2015年。'
- en: '[23] Jaime Carbonell and Jade Goldstein. The use of mmr, diversity-based reranking
    for reordering documents and producing summaries. In Proceedings of the 21st annual
    international ACM SIGIR conference on Research and development in information
    retrieval, pages 335–336, 1998.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jaime Carbonell 和 Jade Goldstein。使用MMR，基于多样性的重新排序用于重新排列文档和生成摘要。在第21届国际ACM
    SIGIR信息检索研究与发展年会上，页码335–336，1998年。'
- en: '[24] Yllias Chali and Sadid A. Hasan. On the effectiveness of using sentence
    compression models for query-focused multi-document summarization. In Proceedings
    of COLING 2012, pages 457–474, Mumbai, India, December 2012\. The COLING 2012
    Organizing Committee.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Yllias Chali 和 Sadid A. Hasan。使用句子压缩模型进行查询重点多文档总结的有效性。在COLING 2012会议论文集中，页码457–474，印度孟买，2012年12月。COLING
    2012组织委员会。'
- en: '[25] Yllias Chali and Sadid A Hasan. Query-focused multi-document summarization:
    Automatic data annotations and supervised learning approaches. Natural Language
    Engineering, 18(1):109–145, 2012.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Yllias Chali 和 Sadid A Hasan。以查询为重点的多文档总结：自动数据注释和监督学习方法。自然语言工程，18(1):109–145，2012年。'
- en: '[26] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras,
    and Ion Androutsopoulos. LEGAL-BERT: The muppets straight out of law school. In
    Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898–2904,
    Online, November 2020\. Association for Computational Linguistics.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ilias Chalkidis，Manos Fergadiotis，Prodromos Malakasiotis，Nikolaos Aletras
    和 Ion Androutsopoulos。LEGAL-BERT：从法学院直接出来的小丑。在计算语言学协会会议论文集：EMNLP 2020中，页码2898–2904，在线，2020年11月。计算语言学协会。'
- en: '[27] Ping Chen and Rakesh Verma. A query-based medical information summarization
    system using ontology knowledge. In 19th IEEE Symposium on Computer-Based Medical
    Systems (CBMS’06), pages 37–42\. IEEE, 2006.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Ping Chen 和 Rakesh Verma。基于本体知识的查询医学信息总结系统。在第19届IEEE计算机医学系统研讨会（CBMS’06）上，页码37–42。IEEE，2006年。'
- en: '[28] Yen-Chun Chen and Mohit Bansal. Fast abstractive summarization with reinforce-selected
    sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 675–686, Melbourne,
    Australia, July 2018\. Association for Computational Linguistics.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yen-Chun Chen 和 Mohit Bansal。通过强化选择句子重写进行快速抽象总结。在第56届计算语言学协会年会（卷1：长篇论文）会议论文集中，页码675–686，澳大利亚墨尔本，2018年7月。计算语言学协会。'
- en: '[29] Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, and Jingjing Liu. Distilling
    knowledge learned in BERT for text generation. In Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pages 7893–7905, Online,
    July 2020\. Association for Computational Linguistics.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Yen-Chun Chen，Zhe Gan，Yu Cheng，Jingzhou Liu 和 Jingjing Liu。从BERT中提取知识用于文本生成。在第58届计算语言学协会年会会议论文集中，页码7893–7905，在线，2020年7月。计算语言学协会。'
- en: '[30] Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using RNN encoder–decoder for statistical machine translation. In Proceedings
    of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pages 1724–1734, Doha, Qatar, October 2014\. Association for Computational Linguistics.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Kyunghyun Cho，Bart van Merriënboer，Caglar Gulcehre，Dzmitry Bahdanau，Fethi
    Bougares，Holger Schwenk 和 Yoshua Bengio。使用RNN编码器–解码器学习短语表示用于统计机器翻译。在2014年自然语言处理实证方法会议（EMNLP）论文集中，页码1724–1734，卡塔尔多哈，2014年10月。计算语言学协会。'
- en: '[31] Sumit Chopra, Michael Auli, and Alexander M. Rush. Abstractive sentence
    summarization with attentive recurrent neural networks. In Proceedings of the
    2016 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, pages 93–98, San Diego, California,
    June 2016\. Association for Computational Linguistics.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Sumit Chopra、Michael Auli 和 Alexander M. Rush。基于注意力的递归神经网络的抽象句子总结。发表于第2016届北美计算语言学协会会议：人类语言技术论文集，页码93–98，美国加利福尼亚州圣地亚哥，2016年6月。计算语言学协会。'
- en: '[32] Hans Christian, Mikhael Pramodana Agus, and Derwin Suhartono. Single document
    automatic text summarization using term frequency-inverse document frequency (tf-idf).
    ComTech: Computer, Mathematics and Engineering Applications, 7(4):285–294, 2016.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Hans Christian、Mikhael Pramodana Agus 和 Derwin Suhartono。基于词频-逆文档频率（tf-idf）的单文档自动文本总结。《计算技术：计算机、数学与工程应用》，7(4)：285–294，2016年。'
- en: '[33] Freddy Chua and Sitaram Asur. Automatic summarization of events from social
    media. Proceedings of the International AAAI Conference on Web and Social Media,
    7(1):81–90, Aug. 2021.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Freddy Chua 和 Sitaram Asur。自动总结社交媒体中的事件。发表于国际AAAI网络与社交媒体会议论文集，7(1)：81–90，2021年8月。'
- en: '[34] Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng Andrew.
    Deep learning with cots hpc systems. In International conference on machine learning,
    pages 1337–1345\. PMLR, 2013.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Adam Coates、Brody Huval、Tao Wang、David Wu、Bryan Catanzaro 和 Ng Andrew。使用商用高性能计算系统的深度学习。发表于国际机器学习会议，页码1337–1345。PMLR，2013年。'
- en: '[35] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim,
    Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive
    summarization of long documents. In Proceedings of the 2018 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Volume 2 (Short Papers), pages 615–621, New Orleans, Louisiana,
    June 2018\. Association for Computational Linguistics.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Arman Cohan、Franck Dernoncourt、Doo Soon Kim、Trung Bui、Seokhwan Kim、Walter
    Chang 和 Nazli Goharian。一个关注话语的注意力模型用于长文档的抽象总结。发表于2018年北美计算语言学协会会议：人类语言技术论文集，第2卷（短篇论文），页码615–621，美国路易斯安那州新奥尔良，2018年6月。计算语言学协会。'
- en: '[36] Document Understanding Conference. Duc 2004. https://duc.nist.gov/duc2004/tasks.html,
    2004. Updated: 2011-03-24.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 文档理解会议。Duc 2004。https://duc.nist.gov/duc2004/tasks.html，2004年。更新日期：2011-03-24。'
- en: '[37] Hal Daumé III and Daniel Marcu. Bayesian query-focused summarization.
    In Proceedings of the 21st International Conference on Computational Linguistics
    and 44th Annual Meeting of the Association for Computational Linguistics, pages
    305–312, Sydney, Australia, July 2006. Association for Computational Linguistics.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Hal Daumé III 和 Daniel Marcu。贝叶斯查询导向总结。发表于第21届国际计算语言学会议暨第44届计算语言学协会年会，页码305–312，澳大利亚悉尼，2006年7月。计算语言学协会。'
- en: '[38] Daisy Deng. Bootstrap your text summarization solution with the latest
    release from nlp-recipes. https://techcommunity.microsoft.com/t5/ai-customer-engineering-team/bootstrap-your-text-summarization-solution-with-the-latest/ba-p/1268809,
    2020. Updated: 2020-03-31.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Daisy Deng。利用来自nlp-recipes的最新版本启动你的文本总结解决方案。https://techcommunity.microsoft.com/t5/ai-customer-engineering-team/bootstrap-your-text-summarization-solution-with-the-latest/ba-p/1268809，2020年。更新日期：2020-03-31。'
- en: '[39] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019\. Association
    for Computational Linguistics.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。BERT：用于语言理解的深度双向变换器预训练。发表于2019年北美计算语言学协会会议：人类语言技术论文集，第1卷（长短篇论文），页码4171–4186，美国明尼苏达州明尼阿波利斯，2019年6月。计算语言学协会。'
- en: '[40] Xuan-Dung Doan, Le-Minh Nguyen, and Khac-Hoai Nam Bui. Multi graph neural
    network for extractive long document summarization. In Proceedings of the 29th
    International Conference on Computational Linguistics, pages 5870–5875, Gyeongju,
    Republic of Korea, October 2022\. International Committee on Computational Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Xuan-Dung Doan、Le-Minh Nguyen 和 Khac-Hoai Nam Bui。用于抽取式长文档总结的多图神经网络。发表于第29届国际计算语言学会议论文集中，页码5870–5875，韩国庆州，2022年10月。国际计算语言学委员会。'
- en: '[41] Luobing Dong, Meghana N Satpute, Weili Wu, and Ding-Zhu Du. Two-phase
    multidocument summarization through content-attention-based subtopic detection.
    IEEE Transactions on Computational Social Systems, 8(6):1379–1392, 2021.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Luobing Dong, Meghana N Satpute, Weili Wu, 和 Ding-Zhu Du. 通过基于内容关注的子主题检测进行的两阶段多文档摘要。IEEE
    计算社会系统汇刊，8(6)：1379–1392，2021年。'
- en: '[42] Bonnie Dorr, David Zajic, and Richard Schwartz. Hedge trimmer: A parse-and-trim
    approach to headline generation. In Proceedings of the HLT-NAACL 03 Text Summarization
    Workshop, pages 1–8, 2003.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Bonnie Dorr, David Zajic, 和 Richard Schwartz. Hedge trimmer：一种解析和修剪的标题生成方法。发表于
    HLT-NAACL 03 文本摘要工作坊，页码 1–8，2003年。'
- en: '[43] Xiangyu Duan, Mingming Yin, Min Zhang, Boxing Chen, and Weihua Luo. Zero-shot
    cross-lingual abstractive sentence summarization through teaching generation and
    attention. In Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pages 3162–3172, Florence, Italy, July 2019. Association for Computational
    Linguistics.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Xiangyu Duan, Mingming Yin, Min Zhang, Boxing Chen, 和 Weihua Luo. 零样本跨语言抽象句子摘要通过教学生成和注意力。发表于第57届计算语言学协会年会，页码
    3162–3172，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[44] Harold P Edmundson. New methods in automatic extracting. Journal of the
    ACM (JACM), 16(2):264–285, 1969.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Harold P Edmundson. 自动提取的新方法。ACM 杂志（JACM），16(2)：264–285，1969年。'
- en: '[45] Gunes Erkan and Dragomir Radev. Lexpagerank: Prestige in multi-document
    text summarization. In Proceedings of the 2004 conference on empirical methods
    in natural language processing, pages 365–371, 2004.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Gunes Erkan 和 Dragomir Radev. Lexpagerank：多文档文本摘要中的声望。发表于2004年自然语言处理实证方法会议，页码
    365–371，2004年。'
- en: '[46] Günes Erkan and Dragomir R Radev. Lexrank: Graph-based lexical centrality
    as salience in text summarization. Journal of artificial intelligence research,
    22:457–479, 2004.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Günes Erkan 和 Dragomir R Radev. Lexrank：基于图的词汇中心性作为文本摘要中的显著性。人工智能研究杂志，22：457–479，2004年。'
- en: '[47] Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev.
    Multi-news: A large-scale multi-document summarization dataset and abstractive
    hierarchical model. In Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics, pages 1074–1084, Florence, Italy, July 2019. Association
    for Computational Linguistics.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, 和 Dragomir Radev. Multi-news：一个大规模的多文档摘要数据集和抽象层次模型。发表于第57届计算语言学协会年会，页码
    1074–1084，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[48] James R Faeder, Michael L Blinov, and William S Hlavacek. Rule-based modeling
    of biochemical systems with bionetgen. Systems biology, pages 113–167, 2009.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] James R Faeder, Michael L Blinov, 和 William S Hlavacek. 基于规则的生物化学系统建模与
    bionetgen。系统生物学，页码 113–167，2009年。'
- en: '[49] Xiachong Feng, Xiaocheng Feng, and Bing Qin. A survey on dialogue summarization:
    Recent advances and new frontiers. arXiv preprint arXiv:2107.03175, 2021.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Xiachong Feng, Xiaocheng Feng, 和 Bing Qin. 对话摘要的调查：近期进展与新前沿。arXiv 预印本
    arXiv:2107.03175，2021年。'
- en: '[50] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured
    neural summarization, 2021.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Patrick Fernandes, Miltiadis Allamanis, 和 Marc Brockschmidt. 结构化神经网络摘要，2021年。'
- en: '[51] Seeger Fisher and Brian Roark. Query-focused summarization by supervised
    sentence ranking and skewed word distributions. In Proceedings of the Document
    Understanding Conference, DUC-2006, New York, USA, 2006.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Seeger Fisher 和 Brian Roark. 通过监督句子排序和偏斜词分布进行的查询焦点摘要。发表于文档理解会议，DUC-2006，美国纽约，2006年。'
- en: '[52] Maria Fuentes, Enrique Alfonseca, and Horacio Rodríguez. Support vector
    machines for query-focused summarization trained and evaluated on pyramid data.
    In Proceedings of the 45th Annual Meeting of the Association for Computational
    Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages
    57–60, Prague, Czech Republic, June 2007\. Association for Computational Linguistics.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Maria Fuentes, Enrique Alfonseca, 和 Horacio Rodríguez. 针对查询的支持向量机摘要方法，基于金字塔数据进行训练和评估。发表于第45届计算语言学协会年会附录会议论文集，页码
    57–60，捷克共和国布拉格，2007年6月。计算语言学协会。'
- en: '[53] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model
    for a mechanism of pattern recognition unaffected by shift in position. Biological
    cybernetics, 36(4):193–202, 1980.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Kunihiko Fukushima. Neocognitron：一种自组织神经网络模型，用于对位置变化不敏感的模式识别机制。生物控制论，36(4)：193–202，1980年。'
- en: '[54] Pascale Fung and Grace Ngai. One story, one flow: Hidden markov story
    models for multilingual multidocument summarization. ACM Transactions on Speech
    and Language Processing (TSLP), 3(2):1–16, 2006.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 帕斯卡尔·丰和格蕾丝·黄。一个故事，一个流程：用于多语言多文档摘要的隐马尔可夫故事模型。ACM语音与语言处理（TSLP）事务，3(2)：1–16，2006年。'
- en: '[55] Filippo Galgani, Paul Compton, and Achim Hoffmann. Combining different
    summarization techniques for legal text. In Proceedings of the workshop on innovative
    hybrid approaches to the processing of textual data, pages 115–123, 2012.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 菲利波·加尔加尼、保罗·康普顿和阿基姆·霍夫曼。结合不同的摘要技术用于法律文本。发表于创新混合方法处理文本数据研讨会论文集，115–123页，2012年。'
- en: '[56] Mahak Gambhir and Vishal Gupta. Recent automatic text summarization techniques:
    a survey. Artificial Intelligence Review, 47:1–66, 2017.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 马哈克·甘比尔和维沙尔·古普塔。近期自动文本摘要技术：综述。人工智能评论，47：1–66，2017年。'
- en: '[57] Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. Opinosis: A graph based
    approach to abstractive summarization of highly redundant opinions. In Proceedings
    of the 23rd international conference on computational linguistics (Coling 2010),
    pages 340–348, 2010.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 卡维塔·加纳桑、郑翔霞和贾伟·韩。Opinosis：一种基于图的高度冗余意见抽象摘要方法。发表于第23届国际计算语言学会议（Coling 2010）论文集，340–348页，2010年。'
- en: '[58] Saeedeh Gholamrezazadeh, Mohsen Amini Salehi, and Bahareh Gholamzadeh.
    A comprehensive survey on text summarization systems. In 2009 2nd International
    Conference on Computer Science and its Applications, pages 1–6\. IEEE, 2009.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 萨伊德·戈拉姆雷扎扎德、莫赫森·阿米尼·萨莱希和巴哈雷赫·戈拉姆扎德。关于文本摘要系统的全面调查。发表于2009年第2届计算机科学及其应用国际会议论文集，1–6页。IEEE，2009年。'
- en: '[59] Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell. Summarizing
    text documents: Sentence selection and evaluation metrics. In Proceedings of the
    22nd annual international ACM SIGIR conference on Research and development in
    information retrieval, pages 121–128, 1999.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 贾德·戈尔斯坦、马克·坎特罗维茨、维布·米塔尔和哈梅·卡本内尔。总结文本文档：句子选择和评价指标。发表于第22届国际ACM SIGIR会议论文集，121–128页，1999年。'
- en: '[60] Yihong Gong and Xin Liu. Generic text summarization using relevance measure
    and latent semantic analysis. In Proceedings of the 24th annual international
    ACM SIGIR conference on Research and development in information retrieval, pages
    19–25, 2001.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 易宏·龚和辛·刘。使用相关性度量和潜在语义分析的通用文本摘要。发表于第24届国际ACM SIGIR会议论文集，19–25页，2001年。'
- en: '[61] Quentin Grail, Julien Perez, and Eric Gaussier. Globalizing bert-based
    transformer architectures for long document summarization. In Proceedings of the
    16th conference of the European chapter of the Association for Computational Linguistics:
    Main volume, pages 1792–1810, 2021.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 昆廷·格雷尔、朱利安·佩雷斯和埃里克·高斯耶。全球化BERT基础的变换器架构用于长文档摘要。发表于第16届欧洲计算语言学协会会议：主要卷，1792–1810页，2021年。'
- en: '[62] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for
    networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge
    discovery and data mining, pages 855–864, 2016.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 阿迪提亚·格罗弗和朱尔·莱斯科维奇。node2vec：可扩展的网络特征学习。发表于第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集，855–864页，2016年。'
- en: '[63] Max Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million
    summaries with diverse extractive strategies. In Proceedings of the 2018 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long Papers), pages 708–719, New Orleans,
    Louisiana, June 2018\. Association for Computational Linguistics.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 马克斯·格鲁斯基、莫尔·纳阿曼和约阿夫·阿尔齐。Newsroom：一个包含130万条摘要的多样化抽取策略数据集。发表于2018年北美计算语言学协会年会：人类语言技术卷1（长篇论文），708–719页，美国路易斯安那州新奥尔良，2018年6月。计算语言学协会。'
- en: '[64] Wang Guan, Ivan Smetannikov, and Man Tianxing. Survey on automatic text
    summarization and transformer models applicability. In Proceedings of the 2020
    1st International Conference on Control, Robotics and Intelligent System, pages
    176–184, 2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 王冠、伊万·斯梅塔尼科夫和满天星。自动文本摘要与变换器模型适用性的调查。发表于2020年第1届国际控制、机器人及智能系统会议论文集，176–184页，2020年。'
- en: '[65] Vishal Gupta. Hybrid algorithm for multilingual summarization of hindi
    and punjabi documents. In Mining Intelligence and Knowledge Exploration: First
    International Conference, MIKE 2013, Tamil Nadu, India, December 18-20, 2013.
    Proceedings, pages 717–727\. Springer, 2013.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Vishal Gupta. 印地语和旁遮普语文档的多语言摘要混合算法. 在《智能挖掘与知识探索：第一届国际会议，MIKE 2013》，印度泰米尔纳德邦，2013年12月18-20日.
    论文集, 页717–727\. Springer, 2013.'
- en: '[66] Vishal Gupta and Gurpreet Singh Lehal. A survey of text summarization
    extractive techniques. Journal of emerging technologies in web intelligence, 2(3):258–268,
    2010.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Vishal Gupta 和 Gurpreet Singh Lehal. 文本摘要提取技术的综述. 《网络智能新兴技术期刊》，2(3):258–268,
    2010.'
- en: '[67] Johan Hasselqvist, Niklas Helmertz, and Mikael Kågebäck. Query-based abstractive
    summarization using neural networks. arXiv preprint arXiv:1712.06100, 2017.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Johan Hasselqvist, Niklas Helmertz, 和 Mikael Kågebäck. 基于查询的抽象摘要生成使用神经网络.
    arXiv 预印本 arXiv:1712.06100, 2017.'
- en: '[68] Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations
    of sentences from unlabelled data. In Proceedings of the 2016 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, pages 1367–1377, San Diego, California, June 2016. Association
    for Computational Linguistics.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Felix Hill, Kyunghyun Cho, 和 Anna Korhonen. 从未标记数据中学习句子的分布表示. 在《2016年北美计算语言学协会：人类语言技术会议论文集》，页1367–1377,
    加利福尼亚州圣地亚哥，2016年6月. 计算语言学协会.'
- en: '[69] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Sepp Hochreiter 和 Jürgen Schmidhuber. 长短期记忆. 神经计算, 9(8):1735–1780, 1997.'
- en: '[70] Meishan Hu, Aixin Sun, and Ee-Peng Lim. Comments-oriented document summarization:
    understanding documents with readers’ feedback. In Proceedings of the 31st annual
    international ACM SIGIR conference on Research and development in information
    retrieval, pages 291–298, 2008.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Meishan Hu, Aixin Sun, 和 Ee-Peng Lim. 面向评论的文档摘要：通过读者反馈理解文档. 在《第31届国际ACM
    SIGIR会议论文集》，页291–298, 2008.'
- en: '[71] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient
    attentions for long document summarization. In Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, pages 1419–1436, Online, June 2021\. Association
    for Computational Linguistics.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, 和 Lu Wang. 长文档摘要的高效注意力机制.
    在《2021年北美计算语言学协会：人类语言技术会议论文集》，页1419–1436, 在线, 2021年6月\. 计算语言学协会.'
- en: '[72] X Huang, A Acero, and H Hon. A guide to theory, algorithm, and system
    development. Spoken Language Processing. Prentice-Hall, 2001.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] X Huang, A Acero, 和 H Hon. 理论、算法和系统开发指南. 《口语语言处理》。普伦蒂斯霍尔, 2001.'
- en: '[73] Dongmin Hyun, Xiting Wang, Chayoung Park, Xing Xie, and Hwanjo Yu. Generating
    multiple-length summaries via reinforcement learning for unsupervised sentence
    summarization. In Findings of the Association for Computational Linguistics: EMNLP
    2022, pages 2939–2951, Abu Dhabi, United Arab Emirates, December 2022\. Association
    for Computational Linguistics.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Dongmin Hyun, Xiting Wang, Chayoung Park, Xing Xie, 和 Hwanjo Yu. 通过强化学习生成多长度摘要以进行无监督句子摘要.
    在《计算语言学协会发现：EMNLP 2022》，页2939–2951, 阿布扎比，阿拉伯联合酋长国，2022年12月\. 计算语言学协会.'
- en: '[74] Anubhav Jangra, Sourajit Mukherjee, Adam Jatowt, Sriparna Saha, and Mohammad
    Hasanuzzaman. A survey on multi-modal summarization. ACM Computing Surveys, 2021.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Anubhav Jangra, Sourajit Mukherjee, Adam Jatowt, Sriparna Saha, 和 Mohammad
    Hasanuzzaman. 多模态摘要的综述. ACM 计算调查, 2021.'
- en: '[75] Baoyu Jing, Zeyu You, Tao Yang, Wei Fan, and Hanghang Tong. Multiplex
    graph neural network for extractive text summarization. In Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, pages 133–139,
    Online and Punta Cana, Dominican Republic, November 2021\. Association for Computational
    Linguistics.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Baoyu Jing, Zeyu You, Tao Yang, Wei Fan, 和 Hanghang Tong. 用于提取文本摘要的多重图神经网络.
    在《2021年自然语言处理经验方法会议论文集》，页133–139, 在线和多米尼加共和国蓬塔卡纳，2021年11月\. 计算语言学协会.'
- en: '[76] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement
    learning: A survey. Journal of artificial intelligence research, 4:237–285, 1996.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Leslie Pack Kaelbling, Michael L Littman, 和 Andrew W Moore. 强化学习：综述. 《人工智能研究期刊》，4:237–285,
    1996.'
- en: '[77] Ambedkar Kanapala, Sukomal Pal, and Rajendra Pamula. Text summarization
    from legal documents: a survey. Artificial Intelligence Review, 51:371–402, 2019.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Ambedkar Kanapala、Sukomal Pal 和 Rajendra Pamula。从法律文档中提取文本总结：一项调查。《人工智能评论》，51:371–402，2019年。'
- en: '[78] Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan. An empirical survey
    on long document summarization: Datasets, models, and metrics. ACM computing surveys,
    55(8):1–35, 2022.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Huan Yee Koh、Jiaxin Ju、Ming Liu 和 Shirui Pan。关于长文档总结的实证调查：数据集、模型和指标。《ACM计算调查》，55(8):1–35，2022年。'
- en: '[79] Anastassia Kornilova and Vladimir Eidelman. BillSum: A corpus for automatic
    summarization of US legislation. In Proceedings of the 2nd Workshop on New Frontiers
    in Summarization, pages 48–56, Hong Kong, China, November 2019\. Association for
    Computational Linguistics.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Anastassia Kornilova 和 Vladimir Eidelman。BillSum：用于自动总结美国立法的语料库。在《第2届总结新前沿研讨会论文集》，页码48–56，中国香港，2019年11月。计算语言学协会。'
- en: '[80] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. Communications of the ACM, 60(6):84–90,
    2017.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E Hinton。使用深度卷积神经网络进行ImageNet分类。《ACM通讯》，60(6):84–90，2017年。'
- en: '[81] Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong,
    and Dragomir Radev. Booksum: A collection of datasets for long-form narrative
    summarization. arXiv preprint arXiv:2105.08209, 2021.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Wojciech Kryściński、Nazneen Rajani、Divyansh Agarwal、Caiming Xiong 和 Dragomir
    Radev。Booksum：一个用于长篇叙事总结的数据集集合。arXiv预印本 arXiv:2105.08209，2021年。'
- en: '[82] Julian Kupiec, Jan Pedersen, and Francine Chen. A trainable document summarizer.
    In Proceedings of the 18th annual international ACM SIGIR conference on Research
    and development in information retrieval, pages 68–73, 1995.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Julian Kupiec、Jan Pedersen 和 Francine Chen。一种可训练的文档总结器。在《第18届国际ACM SIGIR信息检索研究与发展年会论文集》，页码68–73，1995年。'
- en: '[83] Moreno La Quatra and Luca Cagliero. End-to-end training for financial
    report summarization. In Proceedings of the 1st Joint Workshop on Financial Narrative
    Processing and MultiLing Financial Summarisation, pages 118–123, Barcelona, Spain
    (Online), December 2020\. COLING.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Moreno La Quatra 和 Luca Cagliero。用于金融报告总结的端到端训练。在《第1届金融叙事处理与多语言金融总结联合研讨会论文集》，页码118–123，西班牙巴塞罗那（在线），2020年12月。COLING。'
- en: '[84] Md Tahmid Rahman Laskar, Enamul Hoque, and Jimmy Huang. Query focused
    abstractive summarization via incorporating query relevance and transfer learning
    with transformer models. In Advances in Artificial Intelligence: 33rd Canadian
    Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May
    13–15, 2020, Proceedings 33, pages 342–348\. Springer, 2020.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Md Tahmid Rahman Laskar、Enamul Hoque 和 Jimmy Huang。通过结合查询相关性和迁移学习与变换器模型进行查询聚焦的抽象总结。在《人工智能进展：第33届加拿大人工智能会议，加拿大AI
    2020》，渥太华，加拿大，2020年5月13日至15日，论文集33，页码342–348。Springer，2020年。'
- en: '[85] Dawn Lawrie, W Bruce Croft, and Arnold Rosenberg. Finding topic words
    for hierarchical summarization. In Proceedings of the 24th annual international
    ACM SIGIR conference on Research and development in information retrieval, pages
    349–357, 2001.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Dawn Lawrie、W Bruce Croft 和 Arnold Rosenberg。寻找用于层次总结的主题词。在《第24届国际ACM
    SIGIR信息检索研究与发展年会论文集》，页码349–357，2001年。'
- en: '[86] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436–444, 2015.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Yann LeCun、Yoshua Bengio 和 Geoffrey Hinton。深度学习。《自然》，521(7553):436–444，2015年。'
- en: '[87] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based
    learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324,
    1998.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Yann LeCun、Léon Bottou、Yoshua Bengio 和 Patrick Haffner。应用于文档识别的基于梯度的学习。《IEEE学报》，86(11):2278–2324，1998年。'
- en: '[88] Chang-Shing Lee, Zhi-Wei Jian, and Lin-Kai Huang. A fuzzy ontology and
    its application to news summarization. IEEE Transactions on Systems, Man, and
    Cybernetics, Part B (Cybernetics), 35(5):859–880, 2005.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Chang-Shing Lee、Zhi-Wei Jian 和 Lin-Kai Huang。模糊本体及其在新闻总结中的应用。《IEEE系统、人类与控制论期刊，B部分（控制论）》，35(5):859–880，2005年。'
- en: '[89] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho
    So, and Jaewoo Kang. BioBERT: a pre-trained biomedical language representation
    model for biomedical text mining. Bioinformatics, 36(4):1234–1240, 09 2019.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Jinhyuk Lee、Wonjin Yoon、Sungdong Kim、Donghyeon Kim、Sunkyu Kim、Chan Ho
    So 和 Jaewoo Kang。BioBERT：一种用于生物医学文本挖掘的预训练生物医学语言表示模型。《生物信息学》，36(4):1234–1240，2019年9月。'
- en: '[90] Anton Leuski, Chin-Yew Lin, Liang Zhou, Ulrich Germann, Franz Josef Och,
    and Eduard Hovy. Cross-lingual c*st*rd: English access to hindi information. ACM
    Transactions on Asian Language Information Processing, 2(3):245–269, sep 2003.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Anton Leuski、Chin-Yew Lin、Liang Zhou、Ulrich Germann、Franz Josef Och 和
    Eduard Hovy。跨语言 c*st*rd：英语访问印地语信息。《ACM亚洲语言信息处理事务》，2(3):245–269，2003年9月。'
- en: '[91] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence
    pre-training for natural language generation, translation, and comprehension.
    In Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pages 7871–7880, Online, July 2020\. Association for Computational
    Linguistics.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Mike Lewis、Yinhan Liu、Naman Goyal、Marjan Ghazvininejad、Abdelrahman Mohamed、Omer
    Levy、Veselin Stoyanov 和 Luke Zettlemoyer。BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练。发表于第58届计算语言学协会年会论文集，页码
    7871–7880，在线，2020年7月。计算语言学协会。'
- en: '[92] Zhengpeng Li, Jiansheng Wu, Jiawei Miao, and Xinmiao Yu. News headline
    generation based on improved decoder from transformer. Scientific Reports, 12(1):11648,
    2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Zhengpeng Li、Jiansheng Wu、Jiawei Miao 和 Xinmiao Yu。基于改进 Transformer 解码器的新闻标题生成。《科学报告》，12(1):11648，2022年。'
- en: '[93] Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming
    Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul
    Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie
    Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. XGLUE:
    A new benchmark dataset for cross-lingual pre-training, understanding and generation.
    In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pages 6008–6018, Online, November 2020\. Association for Computational
    Linguistics.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Yaobo Liang、Nan Duan、Yeyun Gong、Ning Wu、Fenfei Guo、Weizhen Qi、Ming Gong、Linjun
    Shou、Daxin Jiang、Guihong Cao、Xiaodong Fan、Ruofei Zhang、Rahul Agrawal、Edward Cui、Sining
    Wei、Taroon Bharti、Ying Qiao、Jiun-Hung Chen、Winnie Wu、Shuguang Liu、Fan Yang、Daniel
    Campos、Rangan Majumder 和 Ming Zhou。XGLUE：用于跨语言预训练、理解和生成的新基准数据集。发表于2020年自然语言处理经验方法会议（EMNLP）论文集，页码
    6008–6018，在线，2020年11月。计算语言学协会。'
- en: '[94] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries.
    In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004\.
    Association for Computational Linguistics.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Chin-Yew Lin。ROUGE：一个自动评估摘要的软件包。发表于文本摘要扩展研讨会，页码 74–81，西班牙巴塞罗那，2004年7月。计算语言学协会。'
- en: '[95] Chin-Yew Lin and Eduard Hovy. Manual and automatic evaluation of summaries.
    In Proceedings of the ACL-02 workshop on automatic summarization, pages 45–51,
    2002.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Chin-Yew Lin 和 Eduard Hovy。摘要的手动和自动评估。发表于 ACL-02 自动摘要研讨会论文集，页码 45–51，2002年。'
- en: '[96] Hui Lin, Jeff Bilmes, and Shasha Xie. Graph-based submodular selection
    for extractive summarization. In 2009 IEEE Workshop on Automatic Speech Recognition
    & Understanding, pages 381–386\. IEEE, 2009.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Hui Lin、Jeff Bilmes 和 Shasha Xie。基于图的子模选择用于抽取式摘要。发表于2009年 IEEE 自动语音识别与理解研讨会，页码
    381–386。IEEE，2009年。'
- en: '[97] Hui Lin and Vincent Ng. Abstractive summarization: A survey of the state
    of the art. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):9815–9822,
    Jul. 2019.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Hui Lin 和 Vincent Ng。抽象摘要：艺术状态的调查。AAAI 人工智能会议论文集，33(01):9815–9822，2019年7月。'
- en: '[98] Vasileios Lioutas and Yuhong Guo. Time-aware large kernel convolutions.
    In International Conference on Machine Learning, pages 6172–6183\. PMLR, 2020.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Vasileios Lioutas 和 Yuhong Guo。时间感知的大内核卷积。发表于国际机器学习大会，页码 6172–6183。PMLR，2020年。'
- en: '[99] Linqing Liu, Yao Lu, Min Yang, Qiang Qu, Jia Zhu, and Hongyan Li. Generative
    adversarial network for abstractive text summarization. Proceedings of the AAAI
    Conference on Artificial Intelligence, 32(1), Apr. 2018.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Linqing Liu、Yao Lu、Min Yang、Qiang Qu、Jia Zhu 和 Hongyan Li。用于抽象文本摘要的生成对抗网络。AAAI
    人工智能会议论文集，32(1)，2018年4月。'
- en: '[100] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi,
    Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences.
    arXiv preprint arXiv:1801.10198, 2018.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Peter J Liu、Mohammad Saleh、Etienne Pot、Ben Goodrich、Ryan Sepassi、Lukasz
    Kaiser 和 Noam Shazeer。通过总结长序列生成维基百科。arXiv 预印本 arXiv:1801.10198，2018年。'
- en: '[101] Yang Liu. Fine-tune bert for extractive summarization. arXiv preprint
    arXiv:1903.10318, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Yang Liu。微调 BERT 进行抽取式摘要。arXiv 预印本 arXiv:1903.10318，2019年。'
- en: '[102] Yang Liu and Mirella Lapata. Hierarchical transformers for multi-document
    summarization. In Proceedings of the 57th Annual Meeting of the Association for
    Computational Linguistics, pages 5070–5081, Florence, Italy, July 2019. Association
    for Computational Linguistics.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] 刘阳和米雷拉·拉帕塔。用于多文档摘要的层次变换器。发表于第57届计算语言学协会年会论文集，页码5070–5081，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[103] Yike Liu, Tara Safavi, Abhilash Dighe, and Danai Koutra. Graph summarization
    methods and applications: A survey. ACM computing surveys (CSUR), 51(3):1–34,
    2018.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] 刘一珂、塔拉·萨法维、阿比拉什·迪格和达奈·考特拉。图摘要方法与应用：综述。ACM计算调查（CSUR），51(3):1–34，2018年。'
- en: '[104] Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, and Arman
    Cohan. On learning to summarize with large language models as references, 2023.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] 刘一鑫、亚历山大·R·法布里、刘鹏飞、德拉戈米尔·拉德夫和阿尔曼·科汉。利用大型语言模型进行摘要学习的研究，2023年。'
- en: '[105] Yizhu Liu, Zhiyi Luo, and Kenny Zhu. Controlling length in abstractive
    summarization using a convolutional neural network. In Proceedings of the 2018
    Conference on Empirical Methods in Natural Language Processing, pages 4110–4119,
    Brussels, Belgium, October-November 2018\. Association for Computational Linguistics.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] 刘一竹、罗志义和朱肯尼。使用卷积神经网络控制抽象摘要的长度。发表于2018年自然语言处理实证方法会议论文集，页码4110–4119，比利时布鲁塞尔，2018年10月-11月。计算语言学协会。'
- en: '[106] Hans Peter Luhn. The automatic creation of literature abstracts. IBM
    Journal of research and development, 2(2):159–165, 1958.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] 汉斯·彼得·伦。文献摘要的自动生成。IBM研究与开发期刊，2(2):159–165，1958年。'
- en: '[107] Takuya Makino, Tomoya Iwakura, Hiroya Takamura, and Manabu Okumura. Global
    optimization under length constraint for neural text summarization. In Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics, pages
    1039–1048, Florence, Italy, July 2019. Association for Computational Linguistics.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] 牧野拓也、岩仓智也、高村宏也和奥村学。针对神经文本摘要的长度约束下的全局优化。发表于第57届计算语言学协会年会论文集，页码1039–1048，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[108] Inderjeet Mani and Eric Bloedorn. Machine learning of generic and user-focused
    summarization. In AAAI/IAAI, pages 821–826, 1998.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] 印德杰特·马尼和埃里克·布洛多恩。通用和用户聚焦的摘要的机器学习。发表于AAAI/IAAI，页码821–826，1998年。'
- en: '[109] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven
    Bethard, and David McClosky. The Stanford CoreNLP natural language processing
    toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations, pages 55–60, Baltimore, Maryland, June 2014\.
    Association for Computational Linguistics.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] 克里斯托弗·曼宁、米哈伊·苏尔德亚努、约翰·鲍尔、珍妮·芬克尔、斯蒂文·贝瑟德和大卫·麦克洛斯基。斯坦福CoreNLP自然语言处理工具包。发表于第52届计算语言学协会年会：系统演示论文集，页码55–60，美国马里兰州巴尔的摩，2014年6月。计算语言学协会。'
- en: '[110] Yuning Mao, Ming Zhong, and Jiawei Han. CiteSum: Citation text-guided
    scientific extreme summarization and domain adaptation with limited supervision.
    In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing, pages 10922–10935, Abu Dhabi, United Arab Emirates, December 2022\.
    Association for Computational Linguistics.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] 毛玉宁、钟铭和韩家伟。CiteSum：引文文本引导的科学极限摘要与有限监督的领域适应。发表于2022年自然语言处理实证方法会议论文集，页码10922–10935，阿布扎比，阿联酋，2022年12月。计算语言学协会。'
- en: '[111] Kathleen McKeown and Dragomir R Radev. Generating summaries of multiple
    news articles. In Proceedings of the 18th annual international ACM SIGIR conference
    on Research and development in information retrieval, pages 74–82, 1995.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] 凯瑟琳·麦基昂和德拉戈米尔·R·拉德夫。生成多篇新闻文章的摘要。发表于第18届国际ACM SIGIR信息检索研究与开发会议论文集，页码74–82，1995年。'
- en: '[112] Kathleen R McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou,
    Judith L Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman.
    Tracking and summarizing news on a daily basis with columbia’s newsblaster. In
    Proceedings of the human language technology conference, pages 280–285\. San Diego,
    CA, 2002.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] 凯瑟琳·R·麦基昂、雷吉娜·巴齐莱、戴维·埃文斯、瓦西利奥斯·哈齐瓦西洛格卢、朱迪思·L·克拉万斯、安妮·嫩科瓦、卡尔·萨布尔、巴里·施夫曼和谢尔盖·西戈尔曼。通过哥伦比亚大学的新闻机器人进行每日新闻跟踪和摘要。发表于人类语言技术会议论文集，页码280–285。美国加州圣地亚哥，2002年。'
- en: '[113] Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang,
    and Daqing He. Bringing structure into summaries: a faceted summarization dataset
    for long scientific documents. In Proceedings of the 59th Annual Meeting of the
    Association for Computational Linguistics and the 11th International Joint Conference
    on Natural Language Processing (Volume 2: Short Papers), pages 1080–1089, Online,
    August 2021\. Association for Computational Linguistics.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang
    和 Daqing He。将结构引入总结：用于长篇科学文档的多面总结数据集。在第59届计算语言学协会年会和第11届国际联合自然语言处理会议（第2卷：短篇论文）论文集中，页码1080–1089，在线，2021年8月。计算语言学协会。'
- en: '[114] Alessio Micheli. Neural network for graphs: A contextual constructive
    approach. IEEE Transactions on Neural Networks, 20(3):498–511, 2009.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Alessio Micheli。图的神经网络：一种上下文构造方法。IEEE神经网络学报，20(3):498–511，2009年。'
- en: '[115] Rada Mihalcea and Paul Tarau. TextRank: Bringing order into text. In
    Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,
    pages 404–411, Barcelona, Spain, July 2004. Association for Computational Linguistics.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Rada Mihalcea 和 Paul Tarau。TextRank: 将秩序带入文本。在2004年自然语言处理实证方法会议论文集中，页码404–411，西班牙巴塞罗那，2004年7月。计算语言学协会。'
- en: '[116] Rada Mihalcea and Paul Tarau. Textrank: Bringing order into text. In
    Proceedings of the 2004 conference on empirical methods in natural language processing,
    pages 404–411, 2004.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Rada Mihalcea 和 Paul Tarau。Textrank: 将秩序带入文本。在2004年自然语言处理实证方法会议论文集中，页码404–411，2004年。'
- en: '[117] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Tomas Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean。高效的词向量空间表示估计。arXiv预印本arXiv:1301.3781，2013年。'
- en: '[118] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.
    Distributed representations of words and phrases and their compositionality. Advances
    in neural information processing systems, 26, 2013.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado 和 Jeff Dean。词汇和短语的分布式表示及其组合性。神经信息处理系统进展，26，2013年。'
- en: '[119] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel
    Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
    Georg Ostrovski, et al. Human-level control through deep reinforcement learning.
    nature, 518(7540):529–533, 2015.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel
    Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
    Georg Ostrovski 等人。通过深度强化学习实现人类水平的控制。自然，518(7540):529–533，2015年。'
- en: '[120] N Moratanch and S Chitrakala. A survey on extractive text summarization.
    In 2017 international conference on computer, communication and signal processing
    (ICCCSP), pages 1–6\. IEEE, 2017.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] N Moratanch 和 S Chitrakala。提取式文本总结的调查。在2017年国际计算机、通信和信号处理会议（ICCCSP）上，页码1–6。IEEE，2017年。'
- en: '[121] Gianluca Moro and Luca Ragazzi. Semantic self-segmentation for abstractive
    summarization of long documents in low-resource regimes. Proceedings of the AAAI
    Conference on Artificial Intelligence, 36(10):11085–11093, Jun. 2022.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Gianluca Moro 和 Luca Ragazzi。低资源环境下长文档的抽象总结的语义自分割。AAAI人工智能会议论文集，36(10):11085–11093，2022年6月。'
- en: '[122] Mohammed Elsaid Moussa, Ensaf Hussein Mohamed, and Mohamed Hassan Haggag.
    A survey on opinion summarization techniques for social media. Future Computing
    and Informatics Journal, 3(1):82–109, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Mohammed Elsaid Moussa, Ensaf Hussein Mohamed 和 Mohamed Hassan Haggag。社交媒体意见总结技术的调查。未来计算与信息学期刊，3(1):82–109，2018年。'
- en: '[123] Nikita Munot and Sharvari S Govilkar. Comparative study of text summarization
    methods. International Journal of Computer Applications, 102(12), 2014.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Nikita Munot 和 Sharvari S Govilkar。文本总结方法的比较研究。国际计算机应用期刊，102(12)，2014年。'
- en: '[124] Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent
    neural network based sequence model for extractive summarization of documents.
    In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,
    AAAI’17, page 3075–3081\. AAAI Press, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Ramesh Nallapati, Feifei Zhai 和 Bowen Zhou。Summarunner: 基于递归神经网络的序列模型，用于文档的提取式总结。在第三十一届AAAI人工智能会议论文集中，AAAI’17，页码3075–3081。AAAI出版社，2017年。'
- en: '[125] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çağlar Gu̇lçehre, and
    Bing Xiang. Abstractive text summarization using sequence-to-sequence RNNs and
    beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural
    Language Learning, pages 280–290, Berlin, Germany, August 2016. Association for
    Computational Linguistics.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Ramesh Nallapati、Bowen Zhou、Cicero dos Santos、Çağlar Gu̇lçehre 和 Bing
    Xiang. 使用序列到序列 RNN 和其他方法进行抽象文本摘要。发表于第 20 届 SIGNLL 计算自然语言学习会议，页码 280–290，德国柏林，2016年8月。计算语言学协会。'
- en: '[126] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the
    details, just the summary! topic-aware convolutional neural networks for extreme
    summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing, pages 1797–1807, Brussels, Belgium, October-November 2018\.
    Association for Computational Linguistics.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Shashi Narayan、Shay B. Cohen 和 Mirella Lapata. 不要给我细节，只要摘要！用于极端摘要的主题感知卷积神经网络。发表于
    2018 年自然语言处理经验方法会议，页码 1797–1807，比利时布鲁塞尔，2018年10月-11月。计算语言学协会。'
- en: '[127] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Ranking sentences
    for extractive summarization with reinforcement learning. In Proceedings of the
    2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1747–1759,
    New Orleans, Louisiana, June 2018\. Association for Computational Linguistics.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Shashi Narayan、Shay B. Cohen 和 Mirella Lapata. 使用强化学习对提取式摘要进行句子排名。发表于第
    2018 年北美计算语言学协会年会：人类语言技术会议论文集，第 1 卷（长篇论文），页码 1747–1759，路易斯安那州新奥尔良，2018年6月。计算语言学协会。'
- en: '[128] Narges Nazari and MA Mahdavi. A survey on automatic text summarization.
    Journal of AI and Data Mining, 7(1):121–135, 2019.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Narges Nazari 和 MA Mahdavi. 自动文本摘要综述。人工智能与数据挖掘期刊，7(1)：121–135，2019年。'
- en: '[129] Preksha Nema, Mitesh M. Khapra, Anirban Laha, and Balaraman Ravindran.
    Diversity driven attention model for query-based abstractive summarization. In
    Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers), pages 1063–1072, Vancouver, Canada, July 2017\. Association
    for Computational Linguistics.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Preksha Nema、Mitesh M. Khapra、Anirban Laha 和 Balaraman Ravindran. 基于多样性的注意力模型用于查询驱动的抽象摘要。发表于第
    55 届计算语言学协会年会（第 1 卷：长篇论文），页码 1063–1072，加拿大温哥华，2017年7月。计算语言学协会。'
- en: '[130] Ani Nenkova and Kathleen McKeown. A survey of text summarization techniques.
    Mining text data, pages 43–76, 2012.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Ani Nenkova 和 Kathleen McKeown. 文本摘要技术综述。挖掘文本数据，页码 43–76，2012年。'
- en: '[131] Ani Nenkova and Rebecca Passonneau. Evaluating content selection in summarization:
    The pyramid method. In Proceedings of the Human Language Technology Conference
    of the North American Chapter of the Association for Computational Linguistics:
    HLT-NAACL 2004, pages 145–152, Boston, Massachusetts, USA, May 2 - May 7 2004\.
    Association for Computational Linguistics.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Ani Nenkova 和 Rebecca Passonneau. 摘要内容选择评估：金字塔方法。发表于北美计算语言学协会人类语言技术会议：HLT-NAACL
    2004，页码 145–152，美国马萨诸塞州波士顿，2004年5月2日至5月7日。计算语言学协会。'
- en: '[132] Ani Nenkova and Lucy Vanderwende. The impact of frequency on summarization.
    Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR-2005, 101, 2005.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Ani Nenkova 和 Lucy Vanderwende. 频率对摘要的影响。微软研究，华盛顿州雷德蒙德，技术报告 MSR-TR-2005，101，2005年。'
- en: '[133] OpenAI. Gpt-4 technical report, 2023.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] OpenAI. GPT-4 技术报告，2023年。'
- en: '[134] Constantin Orăsan and Oana Andreea Chiorean. Evaluation of a cross-lingual
    Romanian-English multi-document summariser. In Proceedings of the Sixth International
    Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco,
    May 2008\. European Language Resources Association (ELRA).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Constantin Orăsan 和 Oana Andreea Chiorean. 跨语言罗马尼亚语-英语多文档摘要系统的评估。发表于第六届国际语言资源与评估会议（LREC’08），摩洛哥马拉喀什，2008年5月。欧洲语言资源协会（ELRA）。'
- en: '[135] Jessica Ouyang, Boya Song, and Kathy McKeown. A robust abstractive system
    for cross-lingual summarization. In Proceedings of the 2019 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Volume 1 (Long and Short Papers), pages 2025–2031, Minneapolis,
    Minnesota, June 2019\. Association for Computational Linguistics.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Jessica Ouyang、Boya Song 和 Kathy McKeown. 用于跨语言摘要的强健抽象系统。发表于第 2019 年北美计算语言学协会年会：人类语言技术会议论文集，第
    1 卷（长篇和短篇论文），页码 2025–2031，明尼苏达州明尼阿波利斯，2019年6月。计算语言学协会。'
- en: '[136] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models
    to follow instructions with human feedback, 2022.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Christiano, Jan Leike, 和 Ryan Lowe。训练语言模型以根据人类反馈跟随指令，2022。'
- en: '[137] You Ouyang, Wenjie Li, Sujian Li, and Qin Lu. Applying regression models
    to query-focused multi-document summarization. Inf. Process. Manage., 47(2):227–237,
    mar 2011.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] You Ouyang, Wenjie Li, Sujian Li, 和 Qin Lu。应用回归模型进行查询焦点的多文档摘要。信息处理与管理，47(2):227–237，2011年3月。'
- en: '[138] Tatsuro Oya, Yashar Mehdad, Giuseppe Carenini, and Raymond Ng. A template-based
    abstractive meeting summarization: Leveraging summary and source text relationships.
    In Proceedings of the 8th International Natural Language Generation Conference
    (INLG), pages 45–53, Philadelphia, Pennsylvania, U.S.A., June 2014\. Association
    for Computational Linguistics.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Tatsuro Oya, Yashar Mehdad, Giuseppe Carenini, 和 Raymond Ng。基于模板的抽象会议总结：利用摘要和源文本的关系。发表于第8届国际自然语言生成会议论文集，第45–53页，费城，宾夕法尼亚州，美国，2014年6月。计算语言学协会。'
- en: '[139] Makbule Gulcin Ozsoy, Ferda Nur Alpaslan, and Ilyas Cicekli. Text summarization
    using latent semantic analysis. Journal of Information Science, 37(4):405–417,
    2011.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Makbule Gulcin Ozsoy, Ferda Nur Alpaslan, 和 Ilyas Cicekli。使用潜在语义分析的文本摘要。信息科学杂志，37(4):405–417，2011年。'
- en: '[140] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a
    method for automatic evaluation of machine translation. In Proceedings of the
    40th Annual Meeting of the Association for Computational Linguistics, pages 311–318,
    Philadelphia, Pennsylvania, USA, July 2002\. Association for Computational Linguistics.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing Zhu。Bleu：一种自动评估机器翻译的方法。发表于第40届计算语言学协会年会论文集，第311–318页，费城，宾夕法尼亚州，美国，2002年7月。计算语言学协会。'
- en: '[141] Daraksha Parveen, Hans-Martin Ramsl, and Michael Strube. Topical coherence
    for graph-based extractive summarization. In Proceedings of the 2015 conference
    on empirical methods in natural language processing, pages 1949–1954, 2015.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Daraksha Parveen, Hans-Martin Ramsl, 和 Michael Strube。图基抽取摘要的主题连贯性。发表于2015年自然语言处理实证方法会议论文集，第1949–1954页，2015年。'
- en: '[142] Alkesh Patel, Tanveer Siddiqui, and US Tiwary. A language independent
    approach to multilingual text summarization. Large scale semantic access to content
    (text, image, video, and sound), pages 123–132, 2007.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Alkesh Patel, Tanveer Siddiqui, 和 US Tiwary。一种语言独立的多语言文本摘要方法。大规模语义内容访问（文本、图像、视频和声音），第123–132页，2007年。'
- en: '[143] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model
    for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Romain Paulus, Caiming Xiong, 和 Richard Socher。用于抽象总结的深度强化模型。arXiv预印本
    arXiv:1705.04304，2017年。'
- en: '[144] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global
    vectors for word representation. In Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar,
    October 2014\. Association for Computational Linguistics.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Jeffrey Pennington, Richard Socher, 和 Christopher Manning。GloVe：用于词表示的全局向量。发表于2014年自然语言处理实证方法会议论文集，第1532–1543页，多哈，卡塔尔，2014年10月。计算语言学协会。'
- en: '[145] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning
    of social representations. In Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining, pages 701–710, 2014.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Bryan Perozzi, Rami Al-Rfou, 和 Steven Skiena。Deepwalk：社交表征的在线学习。发表于第20届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集，第701–710页，2014年。'
- en: '[146] Seth Polsley, Pooja Jhunjhunwala, and Ruihong Huang. CaseSummarizer:
    A system for automated summarization of legal texts. In Proceedings of COLING
    2016, the 26th International Conference on Computational Linguistics: System Demonstrations,
    pages 258–262, Osaka, Japan, December 2016\. The COLING 2016 Organizing Committee.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Seth Polsley, Pooja Jhunjhunwala, 和 Ruihong Huang。CaseSummarizer：一个用于法律文本自动摘要的系统。发表于COLING
    2016，第26届国际计算语言学会议：系统演示，第258–262页，大阪，日本，2016年12月。COLING 2016组织委员会。'
- en: '[147] Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer,
    Arda Çelebi, Stanko Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu Liu, Jahna
    Otterbacher, Hong Qi, Horacio Saggion, Simone Teufel, Michael Topper, Adam Winkel,
    and Zhu Zhang. MEAD - a platform for multidocument multilingual text summarization.
    In Proceedings of the Fourth International Conference on Language Resources and
    Evaluation (LREC’04), Lisbon, Portugal, May 2004. European Language Resources
    Association (ELRA).'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Dragomir Radev、Timothy Allison、Sasha Blair-Goldensohn、John Blitzer、Arda
    Çelebi、Stanko Dimitrov、Elliott Drabek、Ali Hakim、Wai Lam、Danyu Liu、Jahna Otterbacher、Hong
    Qi、Horacio Saggion、Simone Teufel、Michael Topper、Adam Winkel 和 Zhu Zhang。《MEAD
    - 一个多文档多语言文本摘要平台》。发表于《第四届语言资源与评估国际会议论文集（LREC’04）》，葡萄牙里斯本，2004年5月。欧洲语言资源协会（ELRA）。'
- en: '[148] Dragomir R Radev, Sasha Blair-Goldensohn, Zhu Zhang, and Revathi Sundara
    Raghavan. Newsinessence: A system for domain-independent, real-time news clustering
    and multi-document summarization. In Proceedings of the first international conference
    on Human language technology research, pages 1–4, 2001.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Dragomir R Radev、Sasha Blair-Goldensohn、Zhu Zhang 和 Revathi Sundara Raghavan。《Newsinessence：一种领域无关的实时新闻聚类和多文档摘要系统》。发表于《第一届国际人类语言技术研究会议论文集》，第1–4页，2001年。'
- en: '[149] Dragomir R Radev, Hongyan Jing, Małgorzata Styś, and Daniel Tam. Centroid-based
    summarization of multiple documents. Information Processing & Management, 40(6):919–938,
    2004.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Dragomir R Radev、Hongyan Jing、Małgorzata Styś 和 Daniel Tam。《基于质心的多文档摘要》。信息处理与管理，40(6):919–938，2004年。'
- en: '[150] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever 等。《通过生成预训练提高语言理解》，2018年。'
- en: '[151] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya Sutskever
    等。《语言模型是无监督的多任务学习者》。OpenAI 博客，1(8):9，2019年。'
- en: '[152] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of
    transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res.,
    21(1), jan 2020.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J. Liu。《通过统一的文本到文本转换器探索迁移学习的极限》。机器学习研究期刊，21(1)，2020年1月。'
- en: '[153] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. The Journal of Machine Learning
    Research, 21(1):5485–5551, 2020.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。《通过统一的文本到文本转换器探索迁移学习的极限》。机器学习研究期刊，21(1):5485–5551，2020年。'
- en: '[154] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning
    representations by back-propagating errors. nature, 323(6088):533–536, 1986.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] David E Rumelhart、Geoffrey E Hinton 和 Ronald J Williams。《通过反向传播误差学习表示》。自然，323(6088):533–536，1986年。'
- en: '[155] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention
    model for abstractive sentence summarization. In Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal,
    September 2015\. Association for Computational Linguistics.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Alexander M. Rush、Sumit Chopra 和 Jason Weston。《一种用于抽象句子摘要的神经注意力模型》。发表于《2015年自然语言处理实证方法会议论文集》，第379–389页，葡萄牙里斯本，2015年9月。计算语言学协会。'
- en: '[156] Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic
    text retrieval. Information processing & management, 24(5):513–523, 1988.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Gerard Salton 和 Christopher Buckley。《自动文本检索中的术语加权方法》。信息处理与管理，24(5):513–523，1988年。'
- en: '[157] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter, 2020.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Victor Sanh、Lysandre Debut、Julien Chaumond 和 Thomas Wolf。《Distilbert，bert
    的精简版：更小、更快、更便宜、更轻便》，2020年。'
- en: '[158] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and
    Gabriele Monfardini. The graph neural network model. IEEE transactions on neural
    networks, 20(1):61–80, 2008.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Franco Scarselli、Marco Gori、Ah Chung Tsoi、Markus Hagenbuchner 和 Gabriele
    Monfardini。《图神经网络模型》。IEEE 神经网络交易，20(1):61–80，2008年。'
- en: '[159] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks.
    IEEE transactions on Signal Processing, 45(11):2673–2681, 1997.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Mike Schuster 和 Kuldip K Paliwal。《双向递归神经网络》。IEEE 信号处理交易，45(11):2673–2681，1997年。'
- en: '[160] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski,
    and Jacopo Staiano. MLSUM: The multilingual summarization corpus. In Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pages 8051–8067, Online, November 2020\. Association for Computational Linguistics.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski
    和 Jacopo Staiano。MLSUM：多语言摘要语料库。在2020年自然语言处理经验方法会议（EMNLP）论文集，第8051–8067页，线上，2020年11月。计算语言学协会。'
- en: '[161] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point:
    Summarization with pointer-generator networks. In Proceedings of the 55th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    pages 1073–1083, Vancouver, Canada, July 2017\. Association for Computational
    Linguistics.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Abigail See, Peter J. Liu 和 Christopher D. Manning。直奔要点：使用指针生成网络的摘要。在第55届计算语言学协会年会论文集（第1卷：长篇论文），第1073–1083页，加拿大温哥华，2017年7月。计算语言学协会。'
- en: '[162] Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for
    abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting
    of the Association for Computational Linguistics, pages 2204–2213, Florence, Italy,
    July 2019. Association for Computational Linguistics.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Eva Sharma, Chen Li 和 Lu Wang。BIGPATENT：用于抽象和连贯摘要的大规模数据集。在第57届计算语言学协会年会论文集，第2204–2213页，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[163] Chao Shen and Tao Li. Learning to rank for query-focused multi-document
    summarization. In 2011 IEEE 11th International Conference on Data Mining, pages
    626–634\. IEEE, 2011.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Chao Shen 和 Tao Li。针对查询聚焦的多文档摘要的学习排序。在2011年IEEE第11届国际数据挖掘会议论文集，第626–634页。IEEE，2011年。'
- en: '[164] Josef Steinberger, Karel Jezek, et al. Using latent semantic analysis
    in text summarization and summary evaluation. Proc. ISIM, 4(93-100):8, 2004.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Josef Steinberger, Karel Jezek 等。使用潜在语义分析进行文本摘要和摘要评估。ISIM会议论文，4(93-100)：8，2004年。'
- en: '[165] Milan Straka, Nikita Mediankin, Tom Kocmi, Zdeněk Žabokrtský, Vojtěch
    Hudeček, and Jan Hajič. SumeCzech: Large Czech news-based summarization dataset.
    In Proceedings of the Eleventh International Conference on Language Resources
    and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources
    Association (ELRA).'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Milan Straka, Nikita Mediankin, Tom Kocmi, Zdeněk Žabokrtský, Vojtěch
    Hudeček 和 Jan Hajič。SumeCzech：大型捷克新闻摘要数据集。在第十一届语言资源与评估国际会议（LREC 2018）论文集，日本宫崎，2018年5月。欧洲语言资源协会（ELRA）。'
- en: '[166] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M
    Hospedales. Learning to compare: Relation network for few-shot learning. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 1199–1208,
    2018.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr 和 Timothy
    M Hospedales。学习比较：用于少样本学习的关系网络。在IEEE计算机视觉与模式识别会议论文集，第1199–1208页，2018年。'
- en: '[167] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning
    with neural networks. Advances in neural information processing systems, 27, 2014.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Ilya Sutskever, Oriol Vinyals 和 Quoc V Le。基于神经网络的序列到序列学习。神经信息处理系统进展，27，2014年。'
- en: '[168] Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. Abstractive document summarization
    with a graph-based attentional neural model. In Proceedings of the 55th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    pages 1171–1181, Vancouver, Canada, July 2017\. Association for Computational
    Linguistics.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Jiwei Tan, Xiaojun Wan 和 Jianguo Xiao。基于图的注意力神经模型的抽象文档摘要。在第55届计算语言学协会年会论文集（第1卷：长篇论文），第1171–1181页，加拿大温哥华，2017年7月。计算语言学协会。'
- en: '[169] Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. From neural sentence summarization
    to headline generation: A coarse-to-fine approach. In Proceedings of the 26th
    International Joint Conference on Artificial Intelligence, IJCAI’17, page 4109–4115\.
    AAAI Press, 2017.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Jiwei Tan, Xiaojun Wan 和 Jianguo Xiao。从神经句子摘要到标题生成：一种粗到精的方法。在第26届国际人工智能联合会议，IJCAI’17，第4109–4115页。AAAI出版社，2017年。'
- en: '[170] Oguzhan Tas and Farzad Kiyani. A survey automatic text summarization.
    PressAcademia Procedia, 5(1):205–213, 2007.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Oguzhan Tas 和 Farzad Kiyani。自动文本摘要的调查。PressAcademia Procedia，5(1)：205–213，2007年。'
- en: '[171] Khushboo S Thakkar, Rajiv V Dharaskar, and MB Chandak. Graph-based algorithms
    for text summarization. In 2010 3rd International Conference on Emerging Trends
    in Engineering and Technology, pages 516–519\. IEEE, 2010.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Khushboo S Thakkar, Rajiv V Dharaskar 和 MB Chandak。用于文本摘要的图基算法。在2010年第3届新兴工程与技术国际会议论文集，第516–519页。IEEE，2010年。'
- en: '[172] Gian Lorenzo Thione, Martin van den Berg, Livia Polanyi, and Chris Culy.
    Hybrid text summarization: Combining external relevance measures with structural
    analysis. In Text Summarization Branches Out, pages 51–55, Barcelona, Spain, July
    2004\. Association for Computational Linguistics.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Gian Lorenzo Thione, Martin van den Berg, Livia Polanyi, 和 Chris Culy.
    混合文本摘要：将外部相关性度量与结构分析相结合。在《文本摘要的拓展》一书中，51–55页，西班牙巴塞罗那，2004年7月。计算语言学协会。'
- en: '[173] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 《Attention is all you need》。神经信息处理系统进展，30，2017年。'
- en: '[174] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based
    image description evaluation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 4566–4575, 2015.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Ramakrishna Vedantam, C Lawrence Zitnick, 和 Devi Parikh. Cider：基于共识的图像描述评估。在IEEE计算机视觉与模式识别会议论文集，4566–4575页，2015年。'
- en: '[175] Jesse Vig, Alexander Fabbri, Wojciech Kryscinski, Chien-Sheng Wu, and
    Wenhao Liu. Exploring neural models for query-focused summarization. In Findings
    of the Association for Computational Linguistics: NAACL 2022, pages 1455–1468,
    Seattle, United States, July 2022\. Association for Computational Linguistics.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Jesse Vig, Alexander Fabbri, Wojciech Kryscinski, Chien-Sheng Wu, 和 Wenhao
    Liu. 探索用于查询聚焦摘要的神经模型。在计算语言学协会发现：NAACL 2022，1455–1468页，美国西雅图，2022年7月。计算语言学协会。'
- en: '[176] Xiaojun Wan. Using bilingual information for cross-language document
    summarization. In Proceedings of the 49th Annual Meeting of the Association for
    Computational Linguistics: Human Language Technologies, pages 1546–1555, Portland,
    Oregon, USA, June 2011\. Association for Computational Linguistics.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Xiaojun Wan. 利用双语信息进行跨语言文档摘要。在第49届计算语言学协会年会：人类语言技术会议论文集，1546–1555页，美国俄勒冈州波特兰，2011年6月。计算语言学协会。'
- en: '[177] Xiaojun Wan, Huiying Li, and Jianguo Xiao. Cross-language document summarization
    based on machine translation quality prediction. In Proceedings of the 48th Annual
    Meeting of the Association for Computational Linguistics, pages 917–926, Uppsala,
    Sweden, July 2010. Association for Computational Linguistics.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Xiaojun Wan, Huiying Li, 和 Jianguo Xiao. 基于机器翻译质量预测的跨语言文档摘要。在第48届计算语言学协会年会论文集，917–926页，瑞典乌普萨拉，2010年7月。计算语言学协会。'
- en: '[178] Xiaojun Wan and Jianwu Yang. Multi-document summarization using cluster-based
    link analysis. In Proceedings of the 31st annual international ACM SIGIR conference
    on Research and development in information retrieval, pages 299–306, 2008.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Xiaojun Wan 和 Jianwu Yang. 使用基于聚类的链接分析进行多文档摘要。在第31届国际ACM SIGIR信息检索研究与开发会议论文集，299–306页，2008年。'
- en: '[179] Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, and Xuanjing Huang.
    Heterogeneous graph neural networks for extractive document summarization. In
    Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
    pages 6209–6219, Online, July 2020\. Association for Computational Linguistics.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, 和 Xuanjing Huang.
    用于提取式文档摘要的异质图神经网络。在第58届计算语言学协会年会论文集，6209–6219页，在线，2020年7月。计算语言学协会。'
- en: '[180] Fei Wang, Kaiqiang Song, Hongming Zhang, Lifeng Jin, Sangwoo Cho, Wenlin
    Yao, Xiaoyang Wang, Muhao Chen, and Dong Yu. Salience allocation as guidance for
    abstractive summarization. In Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing, pages 6094–6106, Abu Dhabi, United Arab
    Emirates, December 2022\. Association for Computational Linguistics.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Fei Wang, Kaiqiang Song, Hongming Zhang, Lifeng Jin, Sangwoo Cho, Wenlin
    Yao, Xiaoyang Wang, Muhao Chen, 和 Dong Yu. 作为抽象摘要指导的显著性分配。在2022年自然语言处理经验方法会议论文集，6094–6106页，阿布扎比，阿联酋，2022年12月。计算语言学协会。'
- en: '[181] Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng
    Qu, and Jie Zhou. A survey on cross-lingual summarization. Transactions of the
    Association for Computational Linguistics, 10:1304–1323, 2022.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng
    Qu, 和 Jie Zhou. 跨语言摘要调查。计算语言学协会交易，10:1304–1323，2022年。'
- en: '[182] Lu Wang and Claire Cardie. Domain-independent abstract generation for
    focused meeting summarization. In Proceedings of the 51st Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), pages 1395–1405,
    Sofia, Bulgaria, August 2013\. Association for Computational Linguistics.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Lu Wang 和 Claire Cardie。面向聚焦会议摘要的领域独立抽象生成。在计算语言学协会第51届年会（第1卷：长篇论文）论文集，页码
    1395–1405，保加利亚索非亚，2013年8月。计算语言学协会。'
- en: '[183] Mark Wasson. Using leading text for news summaries: Evaluation results
    and implications for commercial summarization applications. In COLING 1998 Volume
    2: The 17th International Conference on Computational Linguistics, 1998.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Mark Wasson。使用领先文本进行新闻摘要：评估结果及对商业摘要应用的影响。在 COLING 1998 第2卷：第17届国际计算语言学会议，1998。'
- en: '[184] Jonathan J Webster and Chunyu Kit. Tokenization as the initial phase
    in nlp. In COLING 1992 volume 4: The 14th international conference on computational
    linguistics, 1992.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Jonathan J Webster 和 Chunyu Kit。分词作为自然语言处理中的初始阶段。在 COLING 1992 第4卷：第14届国际计算语言学会议，1992。'
- en: '[185] Ronald J Williams. Simple statistical gradient-following algorithms for
    connectionist reinforcement learning. Reinforcement learning, pages 5–32, 1992.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Ronald J Williams。用于连接主义强化学习的简单统计梯度跟随算法。强化学习，页码 5–32，1992。'
- en: '[186] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli.
    Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430,
    2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Felix Wu、Angela Fan、Alexei Baevski、Yann N Dauphin 和 Michael Auli。使用轻量级和动态卷积减少关注。arXiv
    预印本 arXiv:1901.10430，2019。'
- en: '[187] Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot
    learning—a comprehensive evaluation of the good, the bad and the ugly. IEEE transactions
    on pattern analysis and machine intelligence, 41(9):2251–2265, 2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Yongqin Xian、Christoph H Lampert、Bernt Schiele 和 Zeynep Akata。零样本学习——对好的、坏的和丑的全面评估。IEEE
    模式分析与机器智能交易，41(9)：2251–2265，2018。'
- en: '[188] Wen Xiao and Giuseppe Carenini. Extractive summarization of long documents
    by combining global and local context. In Proceedings of the 2019 Conference on
    Empirical Methods in Natural Language Processing and the 9th International Joint
    Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3011–3021, Hong
    Kong, China, November 2019\. Association for Computational Linguistics.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Wen Xiao 和 Giuseppe Carenini。通过结合全局和局部上下文对长文档进行抽取式摘要。在 2019 年自然语言处理经验方法会议和第9届国际联合自然语言处理会议论文集，页码
    3011–3021，中国香港，2019年11月。计算语言学协会。'
- en: '[189] Shusheng Xu, Xingxing Zhang, Yi Wu, and Furu Wei. Sequence level contrastive
    learning for text summarization. Proceedings of the AAAI Conference on Artificial
    Intelligence, 36(10):11556–11565, Jun. 2022.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Shusheng Xu、Xingxing Zhang、Yi Wu 和 Furu Wei。序列级对比学习用于文本摘要。人工智能 AAAI 会议论文集，36(10)：11556–11565，2022年6月。'
- en: '[190] Divakar Yadav, Jalpa Desai, and Arun Kumar Yadav. Automatic text summarization
    methods: A comprehensive review. arXiv preprint arXiv:2204.01849, 2022.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Divakar Yadav、Jalpa Desai 和 Arun Kumar Yadav。自动文本摘要方法：全面回顾。arXiv 预印本
    arXiv:2204.01849，2022。'
- en: '[191] Jin-ge Yao, Xiaojun Wan, and Jianguo Xiao. Phrase-based compressive cross-language
    summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural
    Language Processing, pages 118–127, Lisbon, Portugal, September 2015\. Association
    for Computational Linguistics.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Jin-ge Yao、Xiaojun Wan 和 Jianguo Xiao。基于短语的压缩跨语言摘要。在 2015 年自然语言处理经验方法会议论文集，页码
    118–127，葡萄牙里斯本，2015年9月。计算语言学协会。'
- en: '[192] Tiezheng Yu, Zihan Liu, and Pascale Fung. AdaptSum: Towards low-resource
    domain adaptation for abstractive summarization. In Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, pages 5892–5904, Online, June 2021\. Association
    for Computational Linguistics.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Tiezheng Yu、Zihan Liu 和 Pascale Fung。AdaptSum：面向低资源领域适应的抽象摘要。在 2021 年北美计算语言学协会年会：人类语言技术会议论文集，页码
    5892–5904，在线，2021年6月。计算语言学协会。'
- en: '[193] Ruifeng Yuan, Zili Wang, Ziqiang Cao, and Wenjie Li. Few-shot query-focused
    summarization with prefix-merging. In Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing, pages 3704–3714, Abu Dhabi, United Arab
    Emirates, December 2022\. Association for Computational Linguistics.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] 袁瑞丰、王自力、曹自强和李文杰。少样本查询聚焦总结与前缀合并。载于2022年自然语言处理实证方法会议论文集，页3704–3714，阿布扎比，阿拉伯联合酋长国，2022年12月。计算语言学协会。'
- en: '[194] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie,
    Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
    et al. Big bird: Transformers for longer sequences. Advances in neural information
    processing systems, 33:17283–17297, 2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] 曼齐尔·扎希尔、古鲁·古鲁甘esh、库马尔·阿维纳瓦·杜比、乔舒亚·安斯利、克里斯·阿尔伯提、圣地亚哥·昂塔农、菲利普·范、阿尼鲁德·拉武拉、王启帆、李杨等。Big
    Bird：用于更长序列的变换器。神经信息处理系统进展，33：17283–17297，2020年。'
- en: '[195] Jiajun Zhang, Yu Zhou, and Chengqing Zong. Abstractive cross-language
    summarization via translation model enhanced predicate argument structure fusing.
    IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(10):1842–1853,
    2016.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] 张佳军、周宇和宗成庆。通过增强的谓词论元结构融合的抽象跨语言总结。IEEE/ACM《音频、语音与语言处理事务》，24(10)：1842–1853，2016年。'
- en: '[196] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training
    with extracted gap-sentences for abstractive summarization. In International Conference
    on Machine Learning, pages 11328–11339\. PMLR, 2020.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] 张静清、赵瑶、穆罕默德·萨利赫和彼得·刘。Pegasus：用于抽象总结的提取式间隙句预训练。载于国际机器学习会议论文集，页11328–11339。PMLR，2020年。'
- en: '[197] Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, and Mohit Bansal. EmailSum:
    Abstractive email thread summarization. In Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 6895–6909,
    Online, August 2021\. Association for Computational Linguistics.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] 张诗月、阿斯利·切利基尔马兹、高剑锋和莫希特·班萨尔。EmailSum：抽象电子邮件线程总结。载于第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第一卷：长篇论文）论文集，页6895–6909，在线，2021年8月。计算语言学协会。'
- en: '[198] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav
    Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675,
    2019.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] 张天懿、瓦尔莎·基肖尔、费利克斯·吴、基利安·Q·温伯格和尤阿夫·阿尔齐。Bertscore：使用BERT评估文本生成。arXiv预印本
    arXiv:1904.09675，2019年。'
- en: '[199] Weiwei Zhang, Jackie Chi Kit Cheung, and Joel Oren. Generating character
    descriptions for automatic summarization of fiction. Proceedings of the AAAI Conference
    on Artificial Intelligence, 33(01):7476–7483, Jul. 2019.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] 张伟伟、杰基·奇·基特·张和乔尔·奥伦。生成角色描述以自动总结小说。AAAI人工智能会议论文集，33(01)：7476–7483，2019年7月。'
- en: '[200] Xingxing Zhang, Furu Wei, and Ming Zhou. HIBERT: Document level pre-training
    of hierarchical bidirectional transformers for document summarization. In Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics, pages
    5059–5069, Florence, Italy, July 2019. Association for Computational Linguistics.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] 张兴兴、魏福如和周铭。HIBERT：用于文档总结的层次双向变换器的文档级预训练。载于第57届计算语言学协会年会论文集，页5059–5069，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[201] Xueying Zhang, Yunjiang Jiang, Yue Shang, Zhaomeng Cheng, Chi Zhang,
    Xiaochuan Fan, Yun Xiao, and Bo Long. Dsgpt: Domain-specific generative pre-training
    of transformers for text generation in e-commerce title and review summarization.
    In Proceedings of the 44th International ACM SIGIR Conference on Research and
    Development in Information Retrieval, pages 2146–2150, 2021.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] 张雪莹、蒋云江、尚月、郑兆蒙、张驰、范晓川、肖云和龙博。DSGPT：用于电子商务标题和评论总结的领域特定生成预训练变换器。载于第44届国际ACM
    SIGIR信息检索研究与发展会议论文集，页2146–2150，2021年。'
- en: '[202] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and
    Steffen Eger. MoverScore: Text generation evaluating with contextualized embeddings
    and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP), pages 563–578, Hong Kong, China, November
    2019\. Association for Computational Linguistics.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, 和 Steffen
    Eger。MoverScore：基于上下文嵌入和地球搬运工距离的文本生成评估。发表于2019年自然语言处理实证方法会议和第九届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集，页面563–578，中国香港，2019年11月。计算语言学协会。'
- en: '[203] Xiaojuan Zhao and Jun Tang. Query-focused summarization based on genetic
    algorithm. In 2010 International Conference on Measuring Technology and Mechatronics
    Automation, volume 2, pages 968–971\. IEEE, 2010.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Xiaojuan Zhao 和 Jun Tang。基于遗传算法的查询聚焦摘要。发表于2010国际测量技术与机电一体化自动化会议，卷2，页面968–971。IEEE，2010年。'
- en: '[204] Shaohui Zheng, Zhixu Li, Jiaan Wang, Jianfeng Qu, An Liu, Lei Zhao, and
    Zhigang Chen. Long-document cross-lingual summarization. In Proceedings of the
    Sixteenth ACM International Conference on Web Search and Data Mining, pages 1084–1092,
    2023.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Shaohui Zheng, Zhixu Li, Jiaan Wang, Jianfeng Qu, An Liu, Lei Zhao, 和
    Zhigang Chen。长文档跨语言摘要。发表于第十六届ACM国际网络搜索与数据挖掘会议论文集，页面1084–1092，2023年。'
- en: '[205] Xin Zheng, Aixin Sun, Jing Li, and Karthik Muthuswamy. Subtopic-driven
    multi-document summarization. In Proceedings of the 2019 Conference on Empirical
    Methods in Natural Language Processing and the 9th International Joint Conference
    on Natural Language Processing (EMNLP-IJCNLP), pages 3153–3162, Hong Kong, China,
    November 2019\. Association for Computational Linguistics.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Xin Zheng, Aixin Sun, Jing Li, 和 Karthik Muthuswamy。子主题驱动的多文档摘要。发表于2019年自然语言处理实证方法会议和第九届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集，页面3153–3162，中国香港，2019年11月。计算语言学协会。'
- en: '[206] Liang Zhou and Eduard Hovy. Headline summarization at isi. In Proceedings
    of the HLT-NAACL 2003 text summarization workshop and document understanding conference
    (DUC 2003), pages 174–178\. Citeseer, 2003.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Liang Zhou 和 Eduard Hovy。在isi的标题摘要。发表于HLT-NAACL 2003文本摘要研讨会和文档理解会议（DUC
    2003）论文集，页面174–178。Citeseer，2003年。'
- en: '[207] Liang Zhou and Eduard Hovy. Template-filtered headline summarization.
    In Text Summarization Branches Out, pages 56–60, Barcelona, Spain, July 2004\.
    Association for Computational Linguistics.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Liang Zhou 和 Eduard Hovy。模板过滤的标题摘要。发表于《文本摘要的扩展》论文集，页面56–60，西班牙巴塞罗那，2004年7月。计算语言学协会。'
- en: '[208] Li Zhuang, Feng Jing, and Xiao-Yan Zhu. Movie review mining and summarization.
    In Proceedings of the 15th ACM international conference on Information and knowledge
    management, pages 43–50, 2006.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Li Zhuang, Feng Jing, 和 Xiao-Yan Zhu。电影评论挖掘与摘要。发表于第十五届ACM国际信息与知识管理会议论文集，页面43–50，2006年。'
