- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:30:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:30:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2408.03539] Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2408.03539] 深度强化学习在机器人中的应用：现实世界成功的调研'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.03539](https://ar5iv.labs.arxiv.org/html/2408.03539)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.03539](https://ar5iv.labs.arxiv.org/html/2408.03539)
- en: \jvol
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \jvol
- en: AA \jyearYYYY \UseRawInputEncoding
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AA \jyearYYYY \UseRawInputEncoding
- en: 'Deep Reinforcement Learning for Robotics:'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在机器人中的应用：
- en: A Survey of Real-World Successes
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界成功的调研
- en: 'Chen Tang^(1,∗)    Ben Abbatematteo^(1,∗)    Jiaheng Hu^(1,∗)    Rohan Chandra²
       Roberto Martín-Martín¹    Peter Stone^(1,3) ¹Department of Computer Science,
    The University of Texas at Austin, Austin, Texas 78712, United States; email:
    chen.tang@utexas.edu, abba@cs.utexas.edu, jiahengh@utexas.edu, robertomm@cs.utexas.edu,
    pstone@utexas.edu ²Department of Computer Science, The University of Virginia,
    Charlottesville, Virginia 22904, United States; email: rohanchandra@virginia.edu
    ³Sony AI ^∗Equal Contribution'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 陈唐^(1,∗)    本·阿巴特马特奥^(1,∗)    贾亨·胡^(1,∗)    罗汉·昌德拉²    罗伯托·马丁-马丁¹    彼得·斯通^(1,3)
    ¹德克萨斯大学奥斯汀分校计算机科学系，美国德克萨斯州奥斯汀，邮政编码78712；电子邮件：chen.tang@utexas.edu，abba@cs.utexas.edu，jiahengh@utexas.edu，robertomm@cs.utexas.edu，pstone@utexas.edu
    ²弗吉尼亚大学计算机科学系，美国弗吉尼亚州夏洛茨维尔，邮政编码22904；电子邮件：rohanchandra@virginia.edu ³索尼AI ^∗等贡献
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement learning (RL), particularly its combination with deep neural networks
    referred to as deep RL (DRL), has shown tremendous promise across a wide range
    of applications, suggesting its potential for enabling the development of sophisticated
    robotic behaviors. Robotics problems, however, pose fundamental difficulties for
    the application of RL, stemming from the complexity and cost of interacting with
    the physical world. This article provides a modern survey of DRL for robotics,
    with a particular focus on evaluating the real-world successes achieved with DRL
    in realizing several key robotic competencies. Our analysis aims to identify the
    key factors underlying those exciting successes, reveal underexplored areas, and
    provide an overall characterization of the status of DRL in robotics. We highlight
    several important avenues for future work, emphasizing the need for stable and
    sample-efficient real-world RL paradigms, holistic approaches for discovering
    and integrating various competencies to tackle complex long-horizon, open-world
    tasks, and principled development and evaluation procedures. This survey is designed
    to offer insights for both RL practitioners and roboticists toward harnessing
    RL’s power to create generally capable real-world robotic systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL），特别是与深度神经网络结合的深度强化学习（DRL），在广泛的应用领域中展现了巨大的潜力，表明其能够推动复杂机器人行为的发展。然而，机器人问题为RL的应用带来了根本性的困难，这些困难源于与物理世界交互的复杂性和成本。本文提供了关于DRL在机器人领域应用的现代调研，特别关注了通过DRL实现若干关键机器人能力所取得的现实世界成功。我们的分析旨在识别这些令人兴奋的成功背后的关键因素，揭示尚未深入探索的领域，并对DRL在机器人领域的现状进行总体描述。我们强调了未来工作的几个重要方向，特别是对稳定和样本高效的现实世界RL范式的需求，发现和整合各种能力以解决复杂的长时间跨度开放世界任务的整体方法，以及有原则的发展和评估程序。此调研旨在为RL从业者和机器人专家提供洞见，以利用RL的力量创造具备广泛能力的现实世界机器人系统。
- en: 'doi:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: doi：
- en: 10.1146/((please add article doi))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 10.1146/((please add article doi))
- en: 'keywords:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'robotics, reinforcement learning, deep learning, learning for control, real-world
    applications^†^†journal: Xxxx. Xxx. Xxx. Xxx.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人技术，强化学习，深度学习，控制学习，现实世界应用^†^†期刊：Xxxx. Xxx. Xxx. Xxx.
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Reinforcement learning (RL) [[1](#bib.bib1)] refers to a class of decision-making
    problems in which an agent must learn through trial-and-error to act in such a
    way that maximizes its accumulated *return*, as encoded by a scalar reward function
    that maps the agent’s states and actions to immediate rewards. RL algorithms,
    particularly their combination with deep neural networks referred to as deep RL
    (DRL) [[2](#bib.bib2)], have shown remarkable capabilities in solving complex
    decision-making problems even with high-dimensional observations in domains such
    as board games [[3](#bib.bib3)], video games [[4](#bib.bib4)], healthcare [[5](#bib.bib5)],
    and recommendation systems [[6](#bib.bib6)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）[[1](#bib.bib1)] 指的是一类决策问题，其中代理必须通过反复试错学习行为，以最大化其累积的*回报*，这由一个标量奖励函数编码，将代理的状态和动作映射到即时奖励。RL算法，尤其是它们与深度神经网络的组合，即深度强化学习（DRL）[[2](#bib.bib2)]，已经展示出在解决复杂的决策问题方面具有显著的能力，即使在高维观测的情况下，如棋盘游戏[[3](#bib.bib3)]、电子游戏[[4](#bib.bib4)]、医疗保健[[5](#bib.bib5)]和推荐系统[[6](#bib.bib6)]领域。
- en: These successes underscore the potential of DRL for controlling robotic systems
    with high-dimensional state or observation space and highly nonlinear dynamics
    to perform challenging tasks that conventional decision-making, planning, and
    control approaches (e.g., classical control, optimal control, sampling-based planning)
    cannot handle effectively. Yet, the most notable milestones of DRL so far have
    been achieved in simulation or game environments, where RL agents can learn from
    extensive experience. In contrast, robots need to complete tasks in the *physical
    world*, which presents additional challenges. It is often inefficient and/or unsafe
    for the RL agents to collect trial-and-error samples directly in the physical
    world, and it is usually impossible to create an exact replica of the complex
    real world in simulation. These challenges notwithstanding, recent advances have
    enabled DRL to succeed at some real-world robotic tasks. For instance, DRL has
    enabled champion-level drone racing [[7](#bib.bib7)] and versatile quadruped locomotion
    control integrated into production-level quadruped systems (e.g., ANYbotics¹¹1[https://www.anybotics.com/news/superior-robot-mobility-where-ai-meets-the-real-world/](https://www.anybotics.com/news/superior-robot-mobility-where-ai-meets-the-real-world/),
    Swiss-Mile²²2[https://www.swiss-mile.com/](https://www.swiss-mile.com/), and Boston
    Dynamics³³3[https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/](https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/)).
    However, *the maturity of state-of-the-art DRL solutions varies significantly
    across different robotic applications*. In some domains, such as urban autonomous
    driving, DRL-based solutions remain limited to simulation or strictly confined
    field tests [[8](#bib.bib8)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些成功突显了DRL在控制具有高维状态或观测空间和高度非线性动力学的机器人系统方面的潜力，以执行传统决策、规划和控制方法（例如经典控制、最优控制、基于采样的规划）无法有效处理的挑战性任务。然而，迄今为止，DRL最显著的里程碑成就大多是在模拟或游戏环境中取得的，RL代理可以从丰富的经验中学习。相比之下，机器人需要在*真实世界*中完成任务，这带来了额外的挑战。RL代理直接在物理世界中进行反复试验收集样本通常是低效和/或不安全的，而且通常无法在模拟中创建复杂真实世界的精确副本。尽管存在这些挑战，最近的进展已使DRL成功解决了一些真实世界的机器人任务。例如，DRL已经实现了冠军级的无人机竞速[[7](#bib.bib7)]和集成到生产级四足系统中的灵活四足动作控制（例如ANYbotics¹¹1[https://www.anybotics.com/news/superior-robot-mobility-where-ai-meets-the-real-world/](https://www.anybotics.com/news/superior-robot-mobility-where-ai-meets-the-real-world/)）、Swiss-Mile²²2[https://www.swiss-mile.com/](https://www.swiss-mile.com/)和波士顿动力³³3[https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/](https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/)）。然而，*当前最先进的DRL解决方案在不同机器人应用领域的成熟度差异显著*。在某些领域，例如城市自主驾驶，基于DRL的解决方案仍然局限于模拟或严格限定的现场测试[[8](#bib.bib8)]。
- en: This survey aims to comprehensively evaluate the current progress of DRL in
    real-world robotic applications, identifying key factors behind the most exciting
    successes and open challenges that remain in less mature areas. Specifically,
    we assess the maturity of DRL for a variety of problem domains and contrast the
    DRL literature across domains to pinpoint broadly applicable techniques, under-explored
    areas, and common open challenges that need to be addressed to advance DRL’s applications
    in robotics. We aim for this survey to provide researchers and practitioners with
    a thorough understanding of the status of DRL in robotics, offering valuable insights
    to guide future research and facilitate broadly deployable DRL solutions for real-world
    robotic tasks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查旨在全面评估DRL在现实世界机器人应用中的当前进展，识别最令人兴奋的成功背后的关键因素以及在较不成熟领域中仍然存在的开放挑战。具体而言，我们评估DRL在各种问题领域的成熟度，并对跨领域的DRL文献进行对比，以确定广泛适用的技术、尚未充分探索的领域以及需要解决的共同开放挑战，以推动DRL在机器人中的应用。我们期望本次调查能为研究人员和从业者提供对DRL在机器人领域状态的深入了解，提供宝贵的见解以指导未来研究，并促进广泛可部署的DRL解决方案用于现实世界的机器人任务。
- en: 2 Why Another Survey on RL for Robotics?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 为什么还需要另一项关于机器人领域RL的调查？
- en: 'Although some previous articles have surveyed RL for robotics, we make three
    contributions that provide unique perspectives on the literature and fill gaps
    in knowledge. First, we focus on work that has demonstrated at least *some degree
    of real-world success*, aiming to assess the current state and open challenges
    of DRL for real-world robotic applications. Most existing surveys on RL for robotics
    do not explicitly address this topic, e.g., Dulac-Arnold et al. [[9](#bib.bib9)]
    discuss the general challenges of real-world RL not specific to robotics, and
    Ibarz et al. [[10](#bib.bib10)] list open challenges of DRL unique to real-world
    robotics settings but based on case studies drawn only from their own research.
    In contrast, our discussion is grounded in a comprehensive assessment of the real-world
    successes achieved by DRL in robotics, with one aspect of our evaluation being
    the level of real-world deployment (see Sec. [3.4](#S3.SS4 "3.4 Level of Real-World
    Success ‣ 3 Taxonomy ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes")).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管一些先前的文章已经调查了机器人领域的强化学习（RL），我们做出了三项贡献，提供了对文献的独特视角并填补了知识的空白。首先，我们关注于展示了至少*一定程度的现实世界成功*的工作，旨在评估现实世界机器人应用中的DRL（深度强化学习）的现状和开放挑战。现有关于机器人领域RL的调查大多未明确讨论这一话题，例如，Dulac-Arnold等人[[9](#bib.bib9)]讨论了与机器人无关的现实世界RL的一般挑战，而Ibarz等人[[10](#bib.bib10)]列出了基于他们自己研究的案例研究的现实世界机器人环境中的DRL开放挑战。相反，我们的讨论基于对DRL在机器人领域取得的现实世界成功的全面评估，我们的评价一个方面是现实世界部署的水平（参见第[3.4节](#S3.SS4
    "3.4 Level of Real-World Success ‣ 3 Taxonomy ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes")）。'
- en: 'Second, we present a *novel* and *comprehensive* taxonomy that categorizes
    DRL solutions along multiple axes: robot competencies learned with DRL, problem
    formulation, solution approach, and level of real-world success. Prior surveys
    on RL for robotics and broader robot learning have often focused on specific tasks [[11](#bib.bib11),
    [12](#bib.bib12)] or on particular techniques [[13](#bib.bib13), [14](#bib.bib14)].
    By contrast, our taxonomy allows us to survey the complete landscape of DRL solutions
    that are effective in robotics application domains, in addition to reviewing the
    literature of each application domain separately. Within this framework, we compare
    and contrast solutions and identify *common patterns, broadly applicable approaches,
    under-explored areas, and open challenges* for realizing successful robotic systems.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们提出了一个*新颖的*和*全面的*分类法，该分类法从多个方面对DRL解决方案进行分类：机器人通过DRL学到的能力、问题的表述、解决方案的方法和现实世界成功的水平。之前的机器人领域RL和更广泛的机器人学习调查常常集中于特定任务[[11](#bib.bib11),
    [12](#bib.bib12)]或特定技术[[13](#bib.bib13), [14](#bib.bib14)]。相比之下，我们的分类法使我们能够调查在机器人应用领域中有效的DRL解决方案的完整景观，并单独审查每个应用领域的文献。在这一框架下，我们比较和对比了解决方案，识别*常见模式、广泛适用的方法、尚未充分探索的领域以及开放挑战*，以实现成功的机器人系统。
- en: Third, while some past surveys have shared our motivation to provide a broad
    analysis of the field, the fast and impressive pace of DRL progress has created
    the need for a renewed analysis of the field, its successes, and limitations.
    The seminal survey by Kober et al. [[15](#bib.bib15)] was written before the deep
    learning era, and the general deep learning for robotics survey by Sunderhauf
    et al. [[16](#bib.bib16)] was written when DRL accomplishments were primarily
    in simulation. We provide a refreshed overview of the field by focusing on DRL,
    which is behind the most notable real-world successes of RL in robotics, paying
    particular attention to papers published in the last five years, during which
    most of the successes occurred.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，尽管一些以往的调查分享了我们提供广泛领域分析的动机，但DRL（深度强化学习）进展的快速和令人印象深刻的步伐已经创造了对该领域、新的成功和局限性的重新分析的需求。Kober等人的开创性调查[[15](#bib.bib15)]是在深度学习时代之前撰写的，而Sunderhauf等人的一般性深度学习与机器人学调查[[16](#bib.bib16)]则是在DRL成果主要集中在模拟中的时候撰写的。我们通过关注DRL，提供了一个更新的领域概述，DRL是机器人学中最显著的现实世界成功的背后，特别关注过去五年发表的论文，在此期间大多数成功发生。
- en: 3 Taxonomy
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 分类法
- en: '![Refer to caption](img/4d50ecb6d94ad71f479d218401cc1ac9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4d50ecb6d94ad71f479d218401cc1ac9.png)'
- en: 'Figure 1: The four aspects of our taxonomy: (a) Robot competencies learned
    with DRL; (b) Problem formulation; (c) Solution approach; and (d) Levels of real-world
    success.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们分类法的四个方面：（a）DRL学习的机器人能力；（b）问题表述；（c）解决方案方法；（d）现实世界成功的水平。
- en: 'This section presents the novel taxonomy we introduce to categorize the literature
    on DRL. The unique focus of our survey on the real-world successes of DRL in robotics
    necessitates a new taxonomy to categorize and analyze the literature, which should
    enable us to assess the maturity of DRL solutions across various robotic applications
    and derive valuable lessons from both successes and failures. Specifically, we
    should identify the specific robotic problem addressed in each paper, understand
    how it has been abstracted as an RL problem, and summarize the DRL techniques
    applied to solve it. More importantly, we should evaluate the maturity of these
    DRL solutions, as demonstrated in their experiments. Consequently, we introduce
    a taxonomy spanning four axes: robot competencies learned with DRL, problem formulation,
    solution approach, and the level of real-world success.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了我们提出的新型分类法，用于对DRL文献进行分类。由于我们调查的独特焦点是DRL在机器人学中取得的现实世界成功，这需要一种新的分类法来对文献进行分类和分析，这将使我们能够评估DRL解决方案在各种机器人应用中的成熟度，并从成功和失败中汲取宝贵经验。具体来说，我们应识别每篇论文中解决的具体机器人问题，了解它如何被抽象为一个RL（强化学习）问题，并总结为解决问题应用的DRL技术。更重要的是，我们应评估这些DRL解决方案的成熟度，如实验所示。因此，我们介绍了一种跨越四个轴心的分类法：DRL学习的机器人能力、问题表述、解决方案方法和现实世界成功的水平。
- en: 3.1 Robot Competencies Learned with DRL
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 DRL学习的机器人能力
- en: Our primary axis focuses on the target robotic task studied in each paper. A
    robotic task, especially in open real-world scenarios, may require multiple competencies.
    One may apply DRL to synthesize an end-to-end system to realize all the competencies
    or learn sub-modules to enable a subset of them. Since our focus is DRL, we classify
    papers based on *the specific robot competencies learned and realized with DRL*.
    We first classify the competencies into *single-robot*—competencies required for
    a robot to complete tasks on its own—and *multi-agent*—competencies required to
    interact with other agents sharing the workspace with the robot and affecting
    its task completion.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要轴心关注每篇论文中研究的目标机器人任务。一个机器人任务，特别是在开放的现实世界场景中，可能需要多个能力。可以应用DRL来综合一个端到端系统以实现所有能力，或学习子模块以实现其中的一部分。由于我们关注的是DRL，我们根据*通过DRL学习和实现的具体机器人能力*对论文进行分类。我们首先将能力分类为*单机器人*——机器人完成任务所需的能力——和*多智能体*——与共享工作空间的其他智能体互动并影响任务完成的能力。
- en: 'When a single robot completes a task in a workspace, any competencies it requires
    can be considered as enabling specific ways to *interact with and affect the physical
    world*, which are further divided into mobility—moving in the environment—and
    manipulation—moving or rearranging (e.g., grasping, rotating) objects in the environment [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)]. In the robotics literature, mobility⁴⁴4In
    the robotics literature, both locomotion and navigation have been used to refer
    to the ability to move in an environment. To avoid confusion, mobility is used
    in this survey to refer to the overarching category where DRL enables robot movement.
    is typically split into two problems: locomotion and navigation [[18](#bib.bib18),
    [20](#bib.bib20)]. Locomotion focuses on motor skills that enable robots of various
    morphologies (e.g., quadrupeds, humanoids, wheeled robots, drones) to traverse
    different environments, while navigation focuses on strategies that direct a robot
    to its destination efficiently without collision. Typical navigation policies
    generate *high-level* motion commands, such as desired states at the center of
    mass (CoM), while assuming effective locomotion control to execute them [[18](#bib.bib18)].
    Some works jointly address the locomotion and navigation problems, which is particularly
    useful for tasks in which the navigation strategies are heavily affected by the
    robot’s capability to traverse the environment, as determined by the robot dynamics
    and locomotion control (e.g., navigating through challenging terrains [[20](#bib.bib20)]
    or racing [[21](#bib.bib21)]). We review these papers alongside other navigation
    papers since their ultimate goal is navigation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当单个机器人在工作空间中完成任务时，它所需的任何能力可以被视为使其能够以特定方式*与物理世界互动和影响*，这些能力进一步分为移动能力——在环境中移动——和操作能力——在环境中移动或重新排列（例如，抓取、旋转）物体[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)]。在机器人学文献中，移动能力通常分为两个问题：运动和导航[[18](#bib.bib18),
    [20](#bib.bib20)]。运动侧重于使各种形态的机器人（例如，四足动物、人形机器人、轮式机器人、无人机）能够穿越不同环境的运动技能，而导航侧重于制定策略，将机器人有效地引导到目的地而不发生碰撞。典型的导航策略生成*高层次*的运动指令，如质心（CoM）处的期望状态，同时假设有效的运动控制来执行这些指令[[18](#bib.bib18)]。一些研究共同解决运动和导航问题，这对导航策略受到机器人在环境中穿越能力影响很大的任务特别有用，因为这取决于机器人动力学和运动控制（例如，通过具有挑战性的地形[[20](#bib.bib20)]或比赛[[21](#bib.bib21)]）。我们将这些论文与其他导航论文一起回顾，因为它们的*最终目标*是导航。
- en: In the robotics literature, manipulation is often studied in table-top settings,
    e.g., robotic arms or hands mounted on a stationary base with fixed sensors observing
    the scene. Some other real-world tasks further require robots to interact with
    the environment while moving their base (e.g., household and warehouse robots),
    which necessitates a synergistic integration of manipulation and mobility capabilities.
    We review the former case under the stationary manipulation category and the latter
    under mobile manipulation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人学文献中，操作通常在桌面设置中研究，例如，机器人臂或手臂安装在带有固定传感器观察场景的静态基座上。一些实际任务进一步要求机器人在移动其基座的同时与环境互动（例如，家庭和仓库机器人），这就需要操作能力和移动能力的协同整合。我们在静态操作类别下回顾前者，在移动操作类别下回顾后者。
- en: 'When the task completion is affected by the other agents in the workspace,
    the robot needs to be further equipped with *abilities to interact with other
    agents*, which we place under the heading of *multi-agent* competencies. Note
    that some single-robot competencies may still be required while the robot interacts
    with others, such as crowd navigation or collaborative manipulation. In this category,
    we focus on papers where DRL occurs at the agent-interaction level, i.e., learning
    interaction strategies given certain single-robot competencies or learning policies
    that jointly optimize interaction and single-robot competencies. We further split
    these works into two subcategories based on the types of agents the robot interacts
    with: 1) *Human-robot interaction* concerns a robot’s ability to operate alongside
    humans. The presence of humans introduces additional challenges due to their sophisticated
    behavior and the stringent safety requirements for robots operating around humans. 2)
    *Multi-robot interaction* refers to a robot’s ability to interact with a group
    of robots. A class of RL algorithms, multi-agent RL (MARL), is typically applied
    to solve this problem. In MARL, each robot is a learning agent evolving its policy
    based on its interactions with the environment and other robots, which complicates
    the learning mechanism. Depending on whether the robots’ objectives align, their
    interactions could be cooperative, adversarial, or general-sum. In addition, practical
    scenarios often require decentralized decision-making under partial observability
    and limited communication bandwidth.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务完成受到工作空间中其他智能体的影响时，机器人需要进一步具备*与其他智能体互动的能力*，我们将其归类为*多智能体*能力。请注意，虽然机器人与其他人互动时，仍可能需要一些单机器人能力，例如人群导航或协作操控。在这一类别中，我们关注那些在智能体互动层面进行深度强化学习的论文，即在给定某些单机器人能力的情况下学习互动策略，或学习共同优化互动和单机器人能力的策略。我们进一步根据机器人与之互动的智能体类型将这些工作分为两个子类别：1）*人机互动*关注机器人与人类并肩作战的能力。由于人类的复杂行为和机器人在人体周围操作的严格安全要求，人机互动引入了额外的挑战。2）*多机器人互动*指的是机器人与一组机器人互动的能力。一类强化学习算法——多智能体强化学习（MARL）通常用于解决这个问题。在MARL中，每个机器人都是一个学习智能体，根据与环境和其他机器人互动的情况来演化其策略，这使学习机制变得复杂。根据机器人的目标是否一致，它们的互动可以是合作的、对抗的或一般和的。此外，实际场景通常需要在部分可观测性和有限通信带宽下进行去中心化决策。
- en: 3.2 Problem Formulation
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 问题表述
- en: 'The second axis of our taxonomy is the formulation of the RL problem, which
    specifies the optimal control policy for the targeted robot competency. RL problems
    are typically modeled as Partially Observable Markov Decision Processes (POMDPs)
    for single-agent RL and Decentralized POMDPs (Dec-POMDP) for multi-agent RL. Specifically,
    we categorize the papers based on the following elements of the problem formulation:
    1) *Action space*: whether the actions are *low-level* (i.e., joint or motor commands),
    *mid-level* (i.e., task-space commands), or *high-level* (i.e., temporally extended
    task-space commands or subroutines); 2) *Observation space*: whether the observations
    are *high-dimensional* sensor inputs (e.g., images and/or LiDAR scans) or estimated
    *low-dimensional* state vectors; 3) *Reward function*: whether the reward signals
    are *sparse* or *dense*. Due to space limitations, we provide detailed definitions
    of these terms in the supplementary materials.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类法的第二个轴心是强化学习（RL）问题的表述，它指定了针对目标机器人能力的最优控制策略。强化学习问题通常被建模为单智能体的部分可观测马尔可夫决策过程（POMDPs）和多智能体的去中心化POMDPs（Dec-POMDP）。具体来说，我们根据问题表述的以下元素对论文进行分类：1）*动作空间*：动作是*低级别*（即关节或电机命令）、*中级别*（即任务空间命令），还是*高级别*（即时间扩展任务空间命令或子程序）；2）*观测空间*：观测是*高维*传感器输入（如图像和/或激光雷达扫描），还是估计的*低维*状态向量；3）*奖励函数*：奖励信号是*稀疏*的还是*密集*的。由于篇幅限制，我们在补充材料中提供了这些术语的详细定义。
- en: 3.3 Solution Approach
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 解决方案方法
- en: 'Another axis closely related to the previous one is the solution approach used
    to solve the RL problem, which is composed of the RL algorithm and associated
    techniques that enable a practical solution for the target robotic problem. Specifically,
    we classify the solution approach from the following perspectives: 1) *Simulator
    usage*: whether and how simulators are used, categorized into *zero-shot*, *few-shot
    sim-to-real transfer*, or directly learning offline or in the real world *without
    simulators*; 2) *Model learning*: whether (a part of) the transition dynamics
    model is learned from robot data; 3) *Expert usage*: whether expert (e.g., human
    or oracle policy) data are used to facilitate learning; 4) *Policy optimization*:
    the policy optimization algorithm adopted, including *planning* or *offline*,
    *off-policy*, or *on-policy RL*; 5) *Policy/Model Representation*: Classes of
    neural network architectures used to represent the policy or dynamics model, including
    *MLP*, *CNN*, *RNN*, and *Transformer*. Please refer to the supplementary materials
    for detailed term definitions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述内容紧密相关的另一个维度是用于解决RL问题的解决方案方法，包括RL算法和相关技术，这些技术使得目标机器人问题具有实用性。具体而言，我们从以下几个角度对解决方案方法进行分类：1）*仿真器使用*：是否及如何使用仿真器，分为*零样本*、*少样本仿真到现实转移*，或直接在现实世界中*无仿真器*地离线学习；2）*模型学习*：是否（部分）从机器人数据中学习过渡动态模型；3）*专家使用*：是否使用专家（例如，人类或oracle策略）数据来促进学习；4）*策略优化*：采用的策略优化算法，包括*规划*或*离线*、*离策略*或*在策略RL*；5）*策略/模型表示*：用于表示策略或动态模型的神经网络架构类别，包括*MLP*、*CNN*、*RNN*和*Transformer*。有关术语定义的详细信息，请参阅补充材料。
- en: 3.4 Level of Real-World Success
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 现实世界成功等级
- en: 'To evaluate the practicality of DRL in real-world robotic tasks, we categorize
    the papers based on the maturity of their DRL methods. By comparing the effectiveness
    of DRL across different robotic tasks, we aim to identify domains where the gaps
    between research prototypes and real-world deployment are more or less significant.
    This requires a metric to quantify real-world success across tasks, which, to
    our knowledge, has not been attempted in the DRL for robotics literature. Inspired
    by the levels of autonomous driving [[22](#bib.bib22)] and Technology readiness
    level (TRL) for machine learning [[23](#bib.bib23)], we introduce the concept
    of *levels of real-world success*. We classify the papers into six levels based
    on the scenarios where the proposed methods have been validated: 1) *Level 0*:
    validated only in simulation; 2) *Level 1*: validated in limited lab conditions;
    3) *Level 2*: validated in diverse lab conditions; 4) *Level 3*: validated under
    confined real-world operational conditions; 5) *Level 4*: validated under diverse,
    representative real-world operational conditions; and 6) *Level 5*: deployed on
    commercialized products. We consider Levels 1-5 as achieving at least some degree
    of real-world success. The only information we can use to assess the level of
    real-world success is the experiments reported by the authors. However, many papers
    only described a single real-world trial. While we strive to provide accurate
    estimates, this assessment can be subjective due to limited information. Additionally,
    we use the level of real-world success to quantify the maturity of a solution
    for its target problem, irrespective of its complexity.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估DRL在现实世界机器人任务中的实用性，我们根据其DRL方法的成熟度对论文进行分类。通过比较不同机器人任务中DRL的效果，我们旨在识别研究原型与现实世界部署之间差距较大或较小的领域。这需要一个量化任务中现实世界成功的指标，据我们所知，在机器人领域的DRL文献中尚未尝试过。受到自动驾驶[[22](#bib.bib22)]和机器学习技术准备水平（TRL）[[23](#bib.bib23)]的启发，我们引入了*现实世界成功等级*的概念。我们根据提出的方法经过验证的场景将论文分类为六个等级：1）*等级0*：仅在仿真中验证；2）*等级1*：在有限的实验室条件下验证；3）*等级2*：在多样的实验室条件下验证；4）*等级3*：在受限的现实世界操作条件下验证；5）*等级4*：在多样的、具有代表性的现实世界操作条件下验证；6）*等级5*：在商业化产品上部署。我们认为等级1-5至少达到了某种程度的现实世界成功。我们唯一可以用来评估现实世界成功等级的信息是作者报告的实验。然而，许多论文仅描述了单一的现实世界试验。虽然我们努力提供准确的估计，但由于信息有限，这一评估可能具有主观性。此外，我们使用现实世界成功等级来量化针对目标问题的解决方案的成熟度，而不考虑其复杂性。
- en: 4 Competency-Specific Review
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 能力特定评审
- en: 'This section provides a detailed review of the DRL literature, with each subsection
    focusing on a specific robot competency. In each subsection, we further organize
    the review based on subcategories specific to each type of competency. After discussing
    the papers, we conclude each subsection by summarizing the trends and open challenges
    for learning the competency in question. To aid understanding, each subsection
    includes a table to overview the reviewed papers. Since our main objective is
    to assess the maturity of DRL solutions, we note the level of real-world success
    achieved by each paper in the table. For a comprehensive categorization of the
    papers, please refer to Tables [1](#A2.T1 "Table 1 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")–[6](#A2.T6
    "Table 6 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes") in the supplementary materials.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '本节提供了DRL文献的详细回顾，每个小节专注于特定的机器人能力。在每个小节中，我们进一步根据每种能力的特定子类别组织回顾。在讨论完论文后，我们通过总结学习该能力的趋势和开放挑战来结束每个小节。为了帮助理解，每个小节包括一个表格，以概述所回顾的论文。由于我们的主要目标是评估DRL解决方案的成熟度，我们在表格中注明了每篇论文在现实世界中取得的成功水平。有关论文的全面分类，请参见附录材料中的表格[1](#A2.T1
    "Table 1 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")–[6](#A2.T6 "Table 6 ‣ Appendix B Additional
    Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")。'
- en: 4.1 Locomotion
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 运动
- en: 'Locomotion research aims to develop motor skills for robots to traverse various
    real-world environments. Prior to the deep learning era, several pioneering works
    have explored RL for locomotion control and delivered promising hardware demos,
    e.g., quadruped walking [[24](#bib.bib24)] and helicopter control [[25](#bib.bib25),
    [26](#bib.bib26)]. This subsection reviews DRL solutions for locomotion separately
    from navigation, where the controllers follow high-level navigation commands.
    Since locomotion mainly concerns motor skills, the problem complexity is primarily
    influenced by the system dynamics [[27](#bib.bib27)]. We organize this subsection
    accordingly and review three representative locomotion problems: quadruped and
    biped locomotion, and quadrotor flight control. See Figure [2](#S4.F2 "Figure
    2 ‣ 4.1.1 Quadruped Locomotion ‣ 4.1 Locomotion ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    for an overview of the papers reviewed.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '运动研究旨在开发机器人在各种真实环境中移动的技能。在深度学习时代之前，一些开创性的工作已经探索了用于运动控制的强化学习（RL），并展示了有前景的硬件演示，例如，四足行走[[24](#bib.bib24)]和直升机控制[[25](#bib.bib25),
    [26](#bib.bib26)]。本小节将运动中的深度强化学习（DRL）解决方案与导航问题分开讨论，其中控制器遵循高级导航命令。由于运动主要涉及运动技能，因此问题的复杂性主要受系统动态的影响[[27](#bib.bib27)]。我们将本小节按此组织，并回顾三个代表性的运动问题：四足和双足运动，以及四旋翼飞行控制。有关所回顾论文的概述，请参见图[2](#S4.F2
    "Figure 2 ‣ 4.1.1 Quadruped Locomotion ‣ 4.1 Locomotion ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")。'
- en: 4.1.1 Quadruped Locomotion
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 四足运动
- en: 'Quadruped locomotion is one of the robotic domains where DRL has provided mature
    real-world solutions. Multiple robotics companies, such as ANYbotics, Swiss-Mile,
    and Boston Dynamics, have reported that DRL was integrated into their quadruped
    control for applications including industrial inspection, last-mile delivery,
    and rescue operations. In the literature, DRL methods were first validated for
    *blind quadruped walking*, i.e., relying solely on proprioceptive sensors on flat
    indoor surfaces [[28](#bib.bib28), [29](#bib.bib29)]. These policies were typically
    trained in simulation and deployed zero-shot in the real world. The main challenge
    lies in the sim-to-real gap in quadrupeds’ intrinsic dynamics. Several strategies
    have been explored to bridge the reality gap: 1) learning actuator models, either
    analytical [[28](#bib.bib28)] or neural network-based [[29](#bib.bib29)], from
    robot data to improve simulation fidelity; 2) randomizing dynamics parameters [[28](#bib.bib28),
    [29](#bib.bib29)] and, even further, randomizing morphology [[30](#bib.bib30)],
    which enables generalization to unseen quadrupeds; and 3) adopting a hierarchical
    structure with a low-level, model-based controller to handle dynamics discrepancy
    and external disturbances while facilitating efficient learning. The interface
    between the DRL policy and the model-based controller could be defined at various
    levels, such as joint positions [[31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)],
    leg poses [[28](#bib.bib28)], gait parameters [[34](#bib.bib34), [35](#bib.bib35)],
    or temporally-extended macro actions [[36](#bib.bib36)]. As robots venture beyond
    controlled lab environments, they encounter more challenging terrains such as
    discontinuous, deformable, or slippery surfaces. Four main techniques have been
    used to address the additional challenges. First, the terrain and contact information
    are not directly observable. Privileged learning has been commonly adopted as
    a solution [[34](#bib.bib34), [33](#bib.bib33)], where a policy with privileged
    terrain information is trained first and then distilled into a student policy
    operating on realistic sensor inputs. Alternatively, end-to-end training can be
    achieved with the help of state estimation [[37](#bib.bib37), [38](#bib.bib38)]
    and asymmetric actor-critic [[39](#bib.bib39), [38](#bib.bib38)]. In both cases,
    an extended history of observations is often set as input.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 四足机器人运动是深度强化学习（DRL）提供成熟现实世界解决方案的机器人领域之一。多家机器人公司，如ANYbotics、Swiss-Mile和Boston
    Dynamics，报告称DRL被集成到它们的四足机器人控制中，用于工业检查、最后一公里配送和救援操作等应用。在文献中，DRL方法首先在*盲四足行走*的场景下得到了验证，即仅依赖于平坦室内表面的自体感知传感器[[28](#bib.bib28),
    [29](#bib.bib29)]。这些策略通常在模拟环境中进行训练，并在现实世界中零-shot部署。主要挑战在于四足机器人固有动力学的模拟与现实之间的差距。为了弥合这一现实差距，已经探索了几种策略：1)
    从机器人数据中学习驱动器模型，无论是分析性的[[28](#bib.bib28)]还是基于神经网络的[[29](#bib.bib29)]，以提高模拟的逼真度；2)
    随机化动力学参数[[28](#bib.bib28), [29](#bib.bib29)]，甚至进一步随机化形态[[30](#bib.bib30)]，以实现对未知四足机器人的泛化；3)
    采用具有低级基于模型的控制器的层次结构，以处理动力学差异和外部干扰，同时促进高效学习。DRL策略与基于模型的控制器之间的接口可以在各种层次上定义，如关节位置[[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33)]、腿部姿势[[28](#bib.bib28)]、步态参数[[34](#bib.bib34),
    [35](#bib.bib35)]，或时间扩展的宏动作[[36](#bib.bib36)]。当机器人走出受控实验室环境时，它们会遇到更具挑战性的地形，如不连续、可变形或滑腻的表面。四种主要技术已经被用于应对这些额外的挑战。首先，地形和接触信息并不是直接可观察的。特权学习被常用作为一种解决方案[[34](#bib.bib34),
    [33](#bib.bib33)]，即先训练一个具有特权地形信息的策略，然后将其蒸馏成一个基于实际传感器输入的学生策略。或者，可以借助状态估计[[37](#bib.bib37),
    [38](#bib.bib38)]和非对称演员-评论员[[39](#bib.bib39), [38](#bib.bib38)]实现端到端训练。在这两种情况下，通常会设置一个扩展的观察历史作为输入。
- en: Second, policies should be exposed to diverse conditions during training for
    generalization in the wild. A learning curriculum that progressively increases
    task difficulty is often adopted to facilitate training [[34](#bib.bib34), [33](#bib.bib33),
    [36](#bib.bib36), [38](#bib.bib38), [37](#bib.bib37)]. Advanced terrain models
    can also improve performance on terrains with complex contact dynamics, e.g.,
    deformable surfaces [[37](#bib.bib37)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，为了在实际环境中实现泛化，策略应在训练过程中暴露于多样的条件下。通常采用逐步增加任务难度的学习课程来促进训练[[34](#bib.bib34),
    [33](#bib.bib33), [36](#bib.bib36), [38](#bib.bib38), [37](#bib.bib37)]。先进的地形模型也可以提高在具有复杂接触动力学的地形上的表现，例如，可变形的表面[[37](#bib.bib37)]。
- en: '![Refer to caption](img/12878df0d29508bf4d146c0ee5e715dc.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/12878df0d29508bf4d146c0ee5e715dc.png)'
- en: '| Quadruped | [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 四足 | [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54) |'
- en: '| Biped | [27](#bib.bib27), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 双足 | [27](#bib.bib27), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63) |'
- en: '| Flight | [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 飞行 | [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68) |'
- en: 'Figure 2: Left: An overview of the three locomotion problems reviewed in Sec. [4.1](#S4.SS1
    "4.1 Locomotion ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"), including quadruped [[49](#bib.bib49)]
    and biped [[63](#bib.bib63)] locomotion, and quadrotor flight control [[64](#bib.bib64),
    [67](#bib.bib67)]; Right: Locomotion papers reviewed in Sec. [4.1](#S4.SS1 "4.1
    Locomotion ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes"). The color map indicates the levels of real-world
    success: *Limited Lab*, *Diverse Lab*, *Limited Real*, and *Diverse Real*.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：左侧：第[4.1](#S4.SS1 "4.1 Locomotion ‣ 4 Competency-Specific Review ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes")节中回顾的三种运动问题的概述，包括四足[[49](#bib.bib49)]和双足[[63](#bib.bib63)]运动，以及四旋翼飞行控制[[64](#bib.bib64),
    [67](#bib.bib67)]；右侧：第[4.1](#S4.SS1 "4.1 Locomotion ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")节中回顾的运动论文。颜色图表表示现实世界成功的水平：*有限实验室*、*多样实验室*、*有限现实*和*多样现实*。'
- en: Third, exteroceptive sensors are crucial for traversing risky terrains, as they
    allow the quadruped to adapt to terrains without stepping on them. For example,
    they have fostered more efficient and robust stair traversal [[35](#bib.bib35),
    [43](#bib.bib43)]. Exteroceptive observations are typically in the form of terrain
    height maps [[35](#bib.bib35), [36](#bib.bib36)], depth images [[44](#bib.bib44),
    [45](#bib.bib45)], and RGB images [[43](#bib.bib43)]. Privileged learning is widely
    used to facilitate policy learning from these high-dimensional observations [[44](#bib.bib44),
    [35](#bib.bib35), [45](#bib.bib45)]. To reduce the sim-to-real gap in sensor inputs,
    techniques such as injecting simulated sensor noise [[35](#bib.bib35)], post-processing
    depth images [[50](#bib.bib50)], learning vision encoders from real-world samples [[43](#bib.bib43)]
    are shown effective. Additionally, policies benefit from improved representation
    via self-supervised learning [[36](#bib.bib36), [35](#bib.bib35)], cross-modal
    embedding matching [[44](#bib.bib44), [43](#bib.bib43)], or using models with
    higher capacity, such as transformers [[69](#bib.bib69), [45](#bib.bib45)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，外部感觉传感器对于穿越危险地形至关重要，因为它们使四足机器人能够适应地形而不必踩踏上去。例如，它们促进了更高效和更稳健的楼梯行走[[35](#bib.bib35),
    [43](#bib.bib43)]。外部观察通常以地形高度图[[35](#bib.bib35), [36](#bib.bib36)]、深度图像[[44](#bib.bib44),
    [45](#bib.bib45)] 和RGB图像[[43](#bib.bib43)]的形式存在。特权学习被广泛应用于从这些高维观察中促进策略学习[[44](#bib.bib44),
    [35](#bib.bib35), [45](#bib.bib45)]。为了减少传感器输入中的模拟与现实之间的差距，技术如注入模拟传感器噪声[[35](#bib.bib35)]、后处理深度图像[[50](#bib.bib50)]、从现实世界样本中学习视觉编码器[[43](#bib.bib43)]已被证明有效。此外，策略通过自监督学习[[36](#bib.bib36),
    [35](#bib.bib35)]、跨模态嵌入匹配[[44](#bib.bib44), [43](#bib.bib43)]或使用更高容量的模型，如变压器[[69](#bib.bib69),
    [45](#bib.bib45)]受益于改进的表示。
- en: Fourth, traversing certain complex terrains demands advanced locomotion skills
    beyond regular walking gaits. For example, end-to-end DRL policies typically struggle
    with terrains that have sparse contact regions. Jenelten et al. [[46](#bib.bib46)]
    showed that training an RL policy to track reference footholds provided by trajectory
    optimization results in more accurate and robust foot placement on sparse terrains.
    Jumping further extends the robots’ ability to cross gaps beyond their body length.
    For example, Yang et al. [[47](#bib.bib47)] trained a DRL policy to generate trajectories
    with a model-based tracking controller handling the complex jumping dynamics.
    Fall recovery is another essential skill, especially for automatic reset in real-world
    RL [[48](#bib.bib48), [53](#bib.bib53)]. Several works have trained DRL policies
    for fall recovery [[29](#bib.bib29), [31](#bib.bib31), [48](#bib.bib48), [32](#bib.bib32),
    [41](#bib.bib41)]. However, both jumping and fall recovery have only been validated
    on flat surfaces so far.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，跨越某些复杂地形需要比常规步态更高级的运动技能。例如，端到端DRL策略通常在接触区域稀疏的地形上表现不佳。Jenetlen等人[[46](#bib.bib46)]表明，训练RL策略以跟踪轨迹优化提供的参考足点可以在稀疏地形上提供更准确、更稳健的脚步放置。跳跃进一步扩展了机器人跨越其身体长度的能力。例如，Yang等人[[47](#bib.bib47)]训练了一个DRL策略，以生成具有模型基础跟踪控制器的轨迹，以处理复杂的跳跃动态。跌倒恢复是另一个关键技能，特别是在现实世界的RL中实现自动重置[[48](#bib.bib48),
    [53](#bib.bib53)]。一些工作已经训练了DRL策略用于跌倒恢复[[29](#bib.bib29), [31](#bib.bib31), [48](#bib.bib48),
    [32](#bib.bib32), [41](#bib.bib41)]。然而，跳跃和跌倒恢复迄今仅在平坦表面上得到验证。
- en: To effectively leverage agile locomotion skills for complex downstream tasks
    like parkour [[49](#bib.bib49), [50](#bib.bib50)], it is crucial to develop *multi-skill*
    policies. Learning multiple skills jointly has also been shown effective in fostering
    policy robustness [[63](#bib.bib63)]. One approach is to create a set of RL policies [[51](#bib.bib51),
    [32](#bib.bib32), [50](#bib.bib50)], each tailored to a specific skill, and then
    train a high-level policy to select the optimal skill [[32](#bib.bib32)]. Alternatively,
    a single policy can be distilled from specialized skill policies through BC [[50](#bib.bib50)].
    To avoid the cumbersome procedure of training multiple specialized policies, several
    works explored constructing a unified policy directly. For instance, MoB [[52](#bib.bib52)]
    encoded various locomotion strategies into a single policy conditioned on gait
    parameters. Cheng et al. [[49](#bib.bib49)] used a unified reward consisting of
    waypoint and velocity tracking terms to learn diverse parkour skills. Fu et al. [[42](#bib.bib42)]
    showed that energy minimization led to smooth gait transitions. Motion imitation
    reward is another widely used and unified approach for learning naturalistic and
    diverse locomotion skills [[51](#bib.bib51), [40](#bib.bib40)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地利用敏捷运动技能完成如跑酷这类复杂的下游任务[[49](#bib.bib49), [50](#bib.bib50)]，开发*多技能*策略是至关重要的。联合学习多个技能也已被证明对促进策略的稳健性有效[[63](#bib.bib63)]。一种方法是创建一组RL策略[[51](#bib.bib51),
    [32](#bib.bib32), [50](#bib.bib50)]，每个策略针对特定技能，然后训练一个高层策略以选择最佳技能[[32](#bib.bib32)]。另外，也可以通过BC从专门的技能策略中提炼出一个单一策略[[50](#bib.bib50)]。为了避免训练多个专门策略的繁琐过程，一些工作探索了直接构建统一策略。例如，MoB[[52](#bib.bib52)]将各种运动策略编码到一个以步态参数为条件的单一策略中。Cheng等人[[49](#bib.bib49)]使用了包含路径点和速度跟踪项的统一奖励来学习多样化的跑酷技能。Fu等人[[42](#bib.bib42)]表明，能量最小化有助于平滑的步态过渡。运动模仿奖励是另一种广泛使用的统一方法，用于学习自然且多样的运动技能[[51](#bib.bib51),
    [40](#bib.bib40)]。
- en: Remark on RL algorithms.We conclude the review on quadruped locomotion with
    a remark on the RL algorithms used in the literature. The most mature DRL solutions
    for quadruped locomotion followed the zero-shot sim-to-real transfer scheme, predominantly
    using on-policy model-free RL, e.g., PPO [[70](#bib.bib70)], due to its robustness
    to hyperparameters. Gangapurwala et al. [[36](#bib.bib36)] noted that on-policy
    RL could be less favorable when the action space is temporally extended or deterministic
    control actions are preferred. Meanwhile, researchers have explored few-shot adaptation
    and real-world RL, either model-free [[48](#bib.bib48), [53](#bib.bib53)] or model-based [[54](#bib.bib54)],
    to update policies using real-world rollouts to further generalize policies to
    novel situations without accurate simulation. However, most works along this line
    have only been validated in limited lab settings. The state-of-the-art performance
    for real-world fine-tuning [[48](#bib.bib48)] and learning from scratch [[53](#bib.bib53)]
    were achieved by using off-policy RL to learn walking and fall recovery. However,
    the tested conditions remain limited compared to mature zero-shot solutions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RL算法的备注。我们在关于四足运动的综述中，最后对文献中使用的RL算法进行了备注。最成熟的DRL解决方案采用了零样本的仿真到现实转移方案，主要使用了策略优化的无模型RL，例如PPO [[70](#bib.bib70)]，因为它对超参数具有鲁棒性。Gangapurwala等人[[36](#bib.bib36)]指出，当动作空间在时间上扩展或需要确定性控制动作时，策略优化RL可能不那么有利。同时，研究人员探讨了少样本适应和现实世界RL，无论是无模型[[48](#bib.bib48),
    [53](#bib.bib53)]还是有模型[[54](#bib.bib54)]，以通过真实世界的试验更新策略，进一步将策略推广到新的情况而无需精确的仿真。然而，大多数相关工作仅在有限的实验室环境中得到验证。现实世界的精调[[48](#bib.bib48)]和从头学习[[53](#bib.bib53)]的最先进表现是通过使用离策略RL来学习行走和跌倒恢复。然而，相较于成熟的零样本解决方案，测试条件仍然有限。
- en: 4.1.2 Biped Locomotion
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 双足运动
- en: Compared to the quadruped case, the DRL literature on bipedal locomotion is
    sparser, and the real-world capabilities demonstrated are more limited. We confine
    the discussion to 3D bipedal robots, which can move freely in all spatial dimensions,
    unlike 2D bipeds that are attached to booms and confined to 2D planar motion [[71](#bib.bib71)],
    for their greater practical utility. The literature begins with walking on flat
    indoor surfaces [[55](#bib.bib55), [57](#bib.bib57)] and extends to walking on
    various indoor [[56](#bib.bib56), [58](#bib.bib58), [27](#bib.bib27)] and outdoor
    terrains [[60](#bib.bib60), [62](#bib.bib62)], and under external forces [[27](#bib.bib27),
    [58](#bib.bib58)]. Other demonstrated skills include stair traversal [[59](#bib.bib59)],
    hopping [[57](#bib.bib57)], running [[57](#bib.bib57), [63](#bib.bib63)], jumping [[63](#bib.bib63)],
    and traversing obstacles and gaps [[61](#bib.bib61)]. More advanced skills have
    been showcased by industrial companies⁵⁵5For example, Unitree ([https://t.ly/s1FwW](https://t.ly/s1FwW))
    and Boston Dynamics ([https://t.ly/NaSaO](https://t.ly/NaSaO)), but no technical
    reports are publicly available to reveal if RL was used in their demos. Notably,
    some of these works deployed their locomotion policies on humanoid robots [[56](#bib.bib56),
    [60](#bib.bib60), [62](#bib.bib62)] while others on bipedal robots without upper
    bodies [[55](#bib.bib55), [58](#bib.bib58), [59](#bib.bib59), [27](#bib.bib27),
    [61](#bib.bib61), [63](#bib.bib63)].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与四足运动相比，关于双足运动的DRL文献较少，展示的现实世界能力也更为有限。我们将讨论限制在3D双足机器人上，这些机器人可以在所有空间维度中自由移动，不同于附着在支架上的2D双足机器人，这些机器人仅限于2D平面运动[[71](#bib.bib71)]，因为它们具有更大的实际应用价值。文献从在平坦室内表面行走[[55](#bib.bib55),
    [57](#bib.bib57)]开始，扩展到在各种室内[[56](#bib.bib56), [58](#bib.bib58), [27](#bib.bib27)]和室外地形[[60](#bib.bib60),
    [62](#bib.bib62)]上行走，以及在外力作用下[[27](#bib.bib27), [58](#bib.bib58)]。其他展示的技能包括楼梯行走[[59](#bib.bib59)]、跳跃[[57](#bib.bib57)]、跑步[[57](#bib.bib57),
    [63](#bib.bib63)]、跳跃[[63](#bib.bib63)]以及跨越障碍物和缝隙[[61](#bib.bib61)]。更先进的技能已由工业公司展示⁵⁵5例如，Unitree
    ([https://t.ly/s1FwW](https://t.ly/s1FwW)) 和 Boston Dynamics ([https://t.ly/NaSaO](https://t.ly/NaSaO))，但没有公开的技术报告显示是否在其演示中使用了RL。值得注意的是，其中一些工作将其运动策略应用于类人机器人[[56](#bib.bib56),
    [60](#bib.bib60), [62](#bib.bib62)]，而其他工作则应用于没有上半身的双足机器人[[55](#bib.bib55), [58](#bib.bib58),
    [59](#bib.bib59), [27](#bib.bib27), [61](#bib.bib61), [63](#bib.bib63)]。
- en: The DRL techniques for bipedal locomotion largely overlap with those for quadrupeds
    but show three distinct trends due to the complex and under-actuated dynamics
    of bipeds. First, learning basic standing and walking skills is already challenging
    due to bipeds’ non-statically stable dynamics [[55](#bib.bib55)]. Thus, model-based
    approaches are frequently used to facilitate RL, either by generating reference
    gaits to guide RL [[55](#bib.bib55), [58](#bib.bib58), [63](#bib.bib63)] or handling
    low-level control for high-level RL policies [[60](#bib.bib60)]. Alternatively,
    Siekmann et al. [[57](#bib.bib57)] offered an end-to-end solution with a reference-free
    periodic reward design based on periodic composition. Second, the role of state
    and action *memories* was particularly noted [[55](#bib.bib55)], especially a
    combination of both long- and short-term memories [[63](#bib.bib63)]. Thus, most
    works adopted sequence models in their policy architecture [[63](#bib.bib63),
    [61](#bib.bib61), [55](#bib.bib55), [57](#bib.bib57), [59](#bib.bib59), [62](#bib.bib62)].
    Third, almost all these policies were zero-shot transferred from simulation. One
    exception is GAT [[56](#bib.bib56)], which collected real-world samples to refine
    a simulator iteratively, enabling an NAO to walk on uneven carpets. The limited
    real-world learning examples are likely due to bipeds’ limited recovery capabilities,
    which hinder their resilience in trials, particularly their ability to auto-reset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 双足行走的深度强化学习（DRL）技术在很大程度上与四足动物的技术重叠，但由于双足的复杂和欠驱动动态，表现出三种明显的趋势。首先，由于双足的不静态稳定动态，学习基本的站立和行走技能已经非常具有挑战性 [[55](#bib.bib55)]。因此，模型基础的方法经常用于促进强化学习，要么通过生成参考步态来指导强化学习 [[55](#bib.bib55),
    [58](#bib.bib58), [63](#bib.bib63)]，要么处理高层强化学习策略的低层控制 [[60](#bib.bib60)]。另外，Siekmann等人 [[57](#bib.bib57)]
    提供了一种基于周期组成的无参考周期奖励设计的端到端解决方案。第二，*记忆*在状态和动作中的作用尤为突出 [[55](#bib.bib55)]，尤其是长短期记忆的结合 [[63](#bib.bib63)]。因此，大多数工作在其策略架构中采用了序列模型 [[63](#bib.bib63),
    [61](#bib.bib61), [55](#bib.bib55), [57](#bib.bib57), [59](#bib.bib59), [62](#bib.bib62)]。第三，几乎所有这些策略都从仿真中零-shot
    转移过来。一个例外是 GAT [[56](#bib.bib56)]，它收集了真实世界样本来迭代优化模拟器，使 NAO 能够在不平坦的地毯上行走。有限的真实世界学习示例可能是由于双足的有限恢复能力，这限制了它们在试验中的弹性，尤其是自我重置的能力。
- en: 4.1.3 Quadrotor Flight Control
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 四旋翼飞行控制
- en: 'Flight control for unmanned aerial vehicles (UAVs), in particular quadrotors,
    is another problem where DRL has shown compelling performance. Hwangbo et al. [[64](#bib.bib64)]
    developed the first DRL quadrotor control policy that was successfully validated
    on hardware for waypoint tracking and recovery from harsh initialization. Later
    studies showed that carefully designed simulated dynamics, domain randomization [[65](#bib.bib65)],
    and carefully designed action space, specifically collective thrust and body rates [[66](#bib.bib66)],
    can facilitate policy robustness. Zhang et al. [[67](#bib.bib67)] applied RMA
    to train a robust near-hover position controller adaptable to unseen disturbances.
    Eschmann et al. [[68](#bib.bib68)] introduced the first off-policy RL paradigm
    for quadrotor control, capable of training a deployable control policy within
    18 seconds for waypoint tracking. In summary, DRL has demonstrated better robustness
    than classical feedback controllers (e.g., PID) in hovering control [[65](#bib.bib65),
    [67](#bib.bib67)]. However, DRL policies tend to have larger tracking errors than
    carefully designed optimization-based controllers for waypoint tracking [[64](#bib.bib64),
    [66](#bib.bib66)]. Yet the fundamental advantage of RL over optimal control is
    it enables joint optimization for planning and control [[21](#bib.bib21)], making
    it an ideal candidate for agile navigation such as racing (see Sec. [4.2](#S4.SS2
    "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes")).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '无人机（UAV）的飞行控制，特别是四旋翼，是另一个深度强化学习（DRL）表现出色的问题。Hwangbo等人[[64](#bib.bib64)] 开发了第一个成功在硬件上验证的DRL四旋翼控制策略，用于航点跟踪和从恶劣初始化中恢复。后续研究表明，精心设计的模拟动态、领域随机化[[65](#bib.bib65)]，以及精心设计的动作空间，特别是集体推力和机体速度[[66](#bib.bib66)]，可以促进策略的鲁棒性。Zhang等人[[67](#bib.bib67)]
    应用RMA训练了一个对未见扰动适应的鲁棒接近悬停位置控制器。Eschmann等人[[68](#bib.bib68)] 引入了第一个离政策强化学习（RL）范式，用于四旋翼控制，能够在18秒内训练出可部署的控制策略以实现航点跟踪。总之，DRL在悬停控制上比经典反馈控制器（例如PID）表现出更好的鲁棒性[[65](#bib.bib65),
    [67](#bib.bib67)]。然而，与精心设计的基于优化的控制器相比，DRL策略在航点跟踪上往往会有更大的跟踪误差[[64](#bib.bib64),
    [66](#bib.bib66)]。但RL相对于最优控制的根本优势在于它能够进行规划和控制的联合优化[[21](#bib.bib21)]，使其成为如赛车等敏捷导航的理想候选者（见第[4.2](#S4.SS2
    "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes")节）。'
- en: 4.1.4 Trends and Open Challenges in Locomotion
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 运动趋势和开放挑战
- en: In summary, DRL has shown effectiveness in synthesizing robust and adaptive
    locomotion controllers for challenging conditions. DRL Techniques used for quadruped,
    biped, and flight control heavily overlap. For instance, RMA [[33](#bib.bib33)],
    initially proposed for quadruped locomotion, has been adapted for both biped [[27](#bib.bib27)]
    and quadrotor flight control [[67](#bib.bib67)]. However, the maturity of DRL
    solutions varies across domains. Quadrupeds can traverse various indoor and outdoor
    terrains via DRL, while real-world bipedal locomotion skills achieved by DRL are
    more limited. For quadrotors, most tests remain confined to controlled, obstacle-free
    indoor environments. Hardware accessibility is a contributing factor. The introduction
    of low-cost quadrupeds has spurred quadruped research and led to open-sourced
    and unified software packages. Conversely, the high cost of bipedal hardware limits
    extensive real-world testing, though recent advances in humanoid hardware are
    expected to boost biped research. More importantly, the quadruped dynamics are
    inherently more stable, whereas bipeds and quadrotors are more prone to catastrophic
    failures under control errors, imposing higher requirements on both robustness
    and precision of control [[63](#bib.bib63)]. High-speed quadrotor control in outdoor
    scenarios with complex obstacles further requires the policy to ensure the *long-horizon*
    feasibility of the closed-loop trajectories [[72](#bib.bib72)]. End-to-end RL
    integrating long-horizon planning and short-horizon control shows promise as a
    solution [[7](#bib.bib7)]. In addition to ensuring long-horizon feasibility, integrating
    locomotion with downstream tasks (e.g., loco-manipulation) is an exciting direction
    in general, but how to discover skills necessary for downstream tasks remains
    an open question.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，DRL 在为具有挑战性的条件合成强健且自适应的行走控制器方面表现出了效果。用于四足、双足和飞行控制的 DRL 技术有很大重叠。例如，最初为四足行走提出的
    RMA [[33](#bib.bib33)] 已被调整用于双足 [[27](#bib.bib27)] 和四旋翼飞行控制 [[67](#bib.bib67)]。然而，DRL
    解决方案在不同领域的成熟度有所不同。四足机器人能够通过 DRL 跨越各种室内和室外地形，而 DRL 实现的现实世界双足行走技能则更为有限。对于四旋翼，绝大多数测试仍限于受控、无障碍的室内环境。硬件的可获取性是一个影响因素。低成本四足机器人的引入刺激了四足研究，并导致了开源和统一的软件包。相反，高成本的双足硬件限制了广泛的现实世界测试，尽管近期的人形硬件进展预计将推动双足研究。更重要的是，四足动力学本质上更稳定，而双足和四旋翼在控制错误下更容易发生灾难性故障，这对控制的鲁棒性和精度提出了更高的要求
    [[63](#bib.bib63)]。在复杂障碍的户外场景中，高速四旋翼控制进一步要求策略确保*长视距*闭环轨迹的可行性 [[72](#bib.bib72)]。集成长视距规划和短视距控制的端到端
    RL 显示出了作为解决方案的潜力 [[7](#bib.bib7)]。除了确保长视距可行性，将行走与下游任务（例如，动作操作）结合在一起是一种令人兴奋的方向，但如何发现下游任务所需的技能仍然是一个未解之谜。
- en: '{summary}'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '{总结}'
- en: '[Key Takeaways]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[关键要点]'
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL has enabled mature quadruped locomotion control; yet, the maturity of DRL-based
    solutions for other locomotion problems is lower.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL 已经实现了成熟的四足行走控制；然而，DRL 基于其他行走问题的解决方案的成熟度较低。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hardware accessibility is an important contributing factor. Low-cost and standard
    hardware platforms would facilitate DRL development.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 硬件的可获取性是一个重要的影响因素。低成本和标准硬件平台将促进 DRL 发展。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The inherently complex dynamics of certain locomotion problems present fundamental
    challenges to the reliable deployment of DRL locomotion controllers.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 某些行走问题本质上复杂的动力学对 DRL 行走控制器的可靠部署提出了根本性挑战。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Even in the mature quadruped locomotion domain, open questions remain, such
    as 1) effectively integrating locomotion with downstream tasks via RL, and 2)
    enabling efficient and safe real-world learning.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即使在成熟的四足行走领域，仍然存在一些未解的问题，例如 1）通过 RL 有效集成行走与下游任务，以及 2）实现高效且安全的现实世界学习。
- en: 4.2 Navigation
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 导航
- en: 'Navigation focuses on the decision-making challenge in mobility: transporting
    an agent to a goal location while avoiding collisions, typically assuming effective
    locomotion. As a fundamental mobility capability, navigation has an extensive
    history in robotics research [[18](#bib.bib18)]. “Classical” navigation approaches
    employ mapping, localization, and planning modules to determine and execute a
    path to a goal. Planning is typically decomposed into global planning, which produces
    a coarse path, and local planning, which tracks the global plan and handles collision
    avoidance. In this section, we delineate navigation works by embodiment: wheeled,
    legged, and aerial navigation and identify capabilities enabled by RL in each
    setting. Social navigation, where the robot navigates in the presence of humans,
    is deferred to Sec. [4.5](#S4.SS5 "4.5 Human-Robot Interaction ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes").
    Multi-robot navigation is similarly deferred to Sec. [4.6](#S4.SS6 "4.6 Multi-Robot
    Interaction ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 导航专注于移动性中的决策挑战：将一个代理运输到目标位置，同时避免碰撞，通常假设有效的运动能力。作为一种基本的移动能力，导航在机器人研究中有着悠久的历史 [[18](#bib.bib18)]。
    “经典”导航方法采用映射、定位和规划模块来确定并执行到目标的路径。规划通常被分解为全局规划，生成粗略路径，和局部规划，跟踪全局计划并处理碰撞避免。在这一部分，我们通过具象化来划分导航工作：轮式、腿式和空中导航，并识别在每种环境中由强化学习（RL）实现的能力。社会导航，即机器人在有人存在的情况下进行导航，被推迟到第[4.5节](#S4.SS5
    "4.5 人机交互 ‣ 4 能力专门评审 ‣ 强化学习在机器人中的应用：现实世界成功的调查")。多机器人导航类似地被推迟到第[4.6节](#S4.SS6 "4.6
    多机器人交互 ‣ 4 能力专门评审 ‣ 强化学习在机器人中的应用：现实世界成功的调查")。
- en: 4.2.1 Wheeled Navigation
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 轮式导航
- en: Navigation for wheeled robots, in particular, has a long history in robotics [[18](#bib.bib18)].
    We discuss several common wheeled navigation settings, including geometric navigation,
    visual navigation, and offroad navigation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是轮式机器人导航在机器人领域有着悠久的历史 [[18](#bib.bib18)]。我们讨论了几种常见的轮式导航设置，包括几何导航、视觉导航和越野导航。
- en: Geometric Navigation. Early attempts aimed to verify RL’s capability in solving
    navigation problems typically solved with modular classical approaches [[73](#bib.bib73)].
    These RL policies directly map 2D laser scans to control actions, unlike classical
    methods that construct explicit maps from the laser scans. While showing promise,
    they often did not compare against classical approaches or failed to outperform
    them [[12](#bib.bib12)]. Some recent studies have benchmarked such RL-based approaches
    and found them superior in challenging problems with dense obstacles and narrow
    passages [[74](#bib.bib74)]. Instead of replacing the entire navigation stack
    with an RL policy, modular approaches replace specific components like the local
    planner [[75](#bib.bib75)] or the exploration algorithm [[76](#bib.bib76)] with
    RL, enabling better performance than classical baselines. However, these improvements
    were mainly observed in limited real settings. Most commercially deployed systems
    still primarily adopt classical stacks, owing to the lack of safety, interpretability,
    and generalization of RL-based methods [[12](#bib.bib12), [74](#bib.bib74)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 几何导航。早期尝试旨在验证强化学习（RL）解决通常通过模块化经典方法解决的导航问题的能力 [[73](#bib.bib73)]。这些RL策略直接将2D激光扫描映射到控制动作，与经典方法通过激光扫描构建明确地图不同。虽然显示出一定的前景，但它们通常没有与经典方法进行比较，或未能超越经典方法 [[12](#bib.bib12)]。一些最近的研究对这些基于RL的方法进行了基准测试，发现它们在具有密集障碍物和狭窄通道的挑战性问题中表现更优 [[74](#bib.bib74)]。而不是用RL策略替换整个导航堆栈，模块化方法用RL替换特定组件，如局部规划器 [[75](#bib.bib75)]
    或探索算法 [[76](#bib.bib76)]，从而实现比经典基线更好的性能。然而，这些改进主要是在有限的实际环境中观察到的。大多数商业部署的系统仍主要采用经典堆栈，因为基于RL的方法缺乏安全性、可解释性和泛化能力 [[12](#bib.bib12),
    [74](#bib.bib74)]。
- en: Visual Navigation. Visual navigation refers to problems where agents navigate
    to a goal based on visual observations. The additional input and task complexity
    pose challenges but enable agents to learn common strategies for navigating in
    similar environments (e.g., homes), where structural patterns emerge in visual
    data. Goals are typically specified as a point relative to the agent (termed pointgoal
    navigation) or as an image of a particular object (objectgoal or imagegoal). RL
    is also commonly applied to vision-and-language navigation problems [[77](#bib.bib77)],
    though very little work has demonstrated these capabilities on a real robot. Many
    works [[78](#bib.bib78), [79](#bib.bib79)] map visual observations to actions
    directly without mapping or planning modules. These end-to-end methods have achieved
    near-perfect results on pointgoal tasks in visually realistic simulations [[80](#bib.bib80)].
    However, training such policies is challenging due to the need for scene understanding,
    intelligent exploration, and episodic memory. Their applicability for real-world
    navigation remains unclear, as they have mostly been validated in limited real
    or lab settings. Other works have investigated modular designs, e.g., using RL
    as a global exploration policy together with explicit mapping and local planning [[81](#bib.bib81),
    [82](#bib.bib82)]. They have outperformed both classical and end-to-end learning
    baselines on pointgoal and imagegoal tasks. However, some challenges with such
    modular approaches exist, such as dynamic obstacles, where end-to-end methods
    have shown promise [[83](#bib.bib83)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉导航。视觉导航指的是代理基于视觉观察导航到目标的问题。额外的输入和任务复杂性带来了挑战，但也使代理能够学习在类似环境中导航的通用策略（例如，家庭环境），其中视觉数据中会出现结构性模式。目标通常被指定为相对于代理的一个点（称为pointgoal导航）或特定物体的图像（objectgoal或imagegoal）。RL也常被应用于视觉与语言导航问题[[77](#bib.bib77)]，尽管很少有工作展示了这些能力在真实机器人上的应用。许多工作[[78](#bib.bib78),
    [79](#bib.bib79)]直接将视觉观察映射到动作，而没有使用映射或规划模块。这些端到端方法在视觉现实模拟中的pointgoal任务上取得了接近完美的结果[[80](#bib.bib80)]。然而，训练这样的策略具有挑战性，因为需要场景理解、智能探索和情节记忆。它们在现实世界导航中的适用性仍不明确，因为它们大多在有限的真实或实验室环境中进行了验证。其他工作研究了模块化设计，例如，将RL用作全球探索策略，并结合显式映射和局部规划[[81](#bib.bib81),
    [82](#bib.bib82)]。这些方法在pointgoal和imagegoal任务上超越了经典和端到端学习基准。然而，这些模块化方法也存在一些挑战，例如动态障碍物，而端到端方法在这方面显示出了希望[[83](#bib.bib83)]。
- en: Despite the plethora of RL works on visual navigation, most are limited to simulation.
    While these simulators are typically constructed with real-world scans [[77](#bib.bib77),
    [84](#bib.bib84)], their transferability to the real world remains debatable.
    Some works reported poor transfer due to visual domain differences [[82](#bib.bib82)],
    while others found success through parameter tuning [[85](#bib.bib85)], abstraction
    of dynamics [[86](#bib.bib86)], or employing only depth images rather than RGB-D [[83](#bib.bib83),
    [87](#bib.bib87)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有大量关于视觉导航的RL工作，但大多数仍限于仿真。这些仿真器通常是基于真实世界的扫描数据构建的[[77](#bib.bib77), [84](#bib.bib84)]，但它们在真实世界中的转移性仍有争议。一些工作报告了由于视觉领域差异导致的较差转移[[82](#bib.bib82)]，而其他工作则通过参数调整[[85](#bib.bib85)]、动态抽象[[86](#bib.bib86)]或仅使用深度图像而非RGB-D[[83](#bib.bib83),
    [87](#bib.bib87)]找到了成功的途径。
- en: Off-road navigation. Navigating off-road presents additional challenges due
    to the dynamics and traversability of different terrains. Some methods tackled
    these challenges with model-based RL to learn predictive models of events or disengagements [[88](#bib.bib88)],
    or utilizing demonstration data with offline RL [[89](#bib.bib89)]. Success has
    also been achieved in high-speed, off-road driving with model-based RL [[90](#bib.bib90)]
    and, recently, vision-based model-free RL [[91](#bib.bib91)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 越野导航。越野导航由于不同地形的动态性和可通行性带来了额外的挑战。一些方法通过基于模型的强化学习（RL）来应对这些挑战，以学习事件或脱离的预测模型[[88](#bib.bib88)]，或者利用演示数据进行离线RL[[89](#bib.bib89)]。在高速越野驾驶中，基于模型的RL也取得了成功[[90](#bib.bib90)]，最近，基于视觉的无模型RL也取得了一定进展[[91](#bib.bib91)]。
- en: '![Refer to caption](img/f81fdbd802368950a225662725e49084.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f81fdbd802368950a225662725e49084.png)'
- en: '| Wheeled | [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82), [85](#bib.bib85), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 轮式 | [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82), [85](#bib.bib85), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)
    |'
- en: '| Legged | [20](#bib.bib20), [83](#bib.bib83), [86](#bib.bib86), [87](#bib.bib87),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 四足 | [20](#bib.bib20), [83](#bib.bib83), [86](#bib.bib86), [87](#bib.bib87),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100) |'
- en: '| Aerial | [7](#bib.bib7), [21](#bib.bib21), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 空中 | [7](#bib.bib7), [21](#bib.bib21), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103) |'
- en: 'Figure 3: Left: An overview of the three navigation problems reviewed in Sec. [4.2](#S4.SS2
    "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"), including wheeled navigation [[74](#bib.bib74),
    [88](#bib.bib88), [92](#bib.bib92)], legged navigation [[97](#bib.bib97)], and
    aerial navigation [[21](#bib.bib21)]; Right: Navigation papers reviewed in Sec. [4.2](#S4.SS2
    "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"). The color map indicates the levels
    of real-world success: *Limited Lab*, *Diverse Lab*, *Limited Real*, and *Diverse
    Real*.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3：左侧：第[4.2](#S4.SS2 "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes")节中回顾的三种导航问题的概述，包括轮式导航[[74](#bib.bib74),
    [88](#bib.bib88), [92](#bib.bib92)]，四足导航[[97](#bib.bib97)]，以及空中导航[[21](#bib.bib21)];
    右侧：第[4.2](#S4.SS2 "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")节中回顾的导航论文。颜色图表示现实世界成功的等级：*有限实验室*，*多样实验室*，*有限现实*，和*多样现实*。'
- en: Autonomous Driving. Autonomous driving extends wheeled navigation to full-size
    passenger vehicles operating at higher speeds in more complex and safety-critical
    environments. RL has achieved limited real-world success for autonomous driving [[8](#bib.bib8)]
    with a few examples under specific conditions. Kendall et al. [[92](#bib.bib92)]
    trained a lane-following policy by learning to maximize its progress before the
    safety driver intervenes. More recently, Jang et al. [[93](#bib.bib93)] trained
    a cruise control policy, where the policy command is wrapped by manually specified
    thresholds to ensure safety. They deployed their policy onto 100 vehicles to smooth
    traffic flow in a field test. Their work suggested a pragmatic approach to embed
    RL into self-driving stacks and showed its potential benefits at the fleet level.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶。自动驾驶将轮式导航扩展到在更复杂且安全性关键的环境中以更高速度操作的全尺寸乘用车。强化学习在自动驾驶领域取得了有限的现实成功[[8](#bib.bib8)]，仅在特定条件下有少数成功案例。Kendall等人[[92](#bib.bib92)]通过学习在安全司机介入之前最大化进度来训练车道跟随策略。最近，Jang等人[[93](#bib.bib93)]训练了一种巡航控制策略，其中策略命令被手动指定的阈值包裹以确保安全。他们将其策略部署到100辆车上，在实地测试中平滑交通流量。他们的工作建议了一种将强化学习嵌入自驾系统的实用方法，并展示了其在车队级别的潜在好处。
- en: 4.2.2 Legged Navigation
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 四足导航
- en: Legged navigation shares many challenges with wheeled navigation but also enables
    transversal of more complex terrains. Some have shown that robust visual-legged
    navigation policies can be learned with low-fidelity kinematic-only simulation
    for both indoors [[83](#bib.bib83), [86](#bib.bib86)] and outdoors [[86](#bib.bib86),
    [94](#bib.bib94)]. The policies thus focus on kinematic-level control while assuming
    effective low-level locomotion control during deployment. Truong et al. [[86](#bib.bib86)]
    showed that this approach, in contrast to learning end-to-end policies with high-fidelity
    simulation, facilitates faster simulation and improves policy generalizability.
    With legged locomotion dynamics abstracted away, the approaches are similar to
    the wheeled case, with the main challenge being the visual domain gap. Unsupervised
    representation learning [[83](#bib.bib83)] and pre-trained vision models [[94](#bib.bib94)]
    have been used to facilitate robust visual policies. For outdoor scenes, Truong
    et al. [[87](#bib.bib87)] zero-shot transferred policies trained in well-established
    indoor simulators to outdoors, using goal vector normalization and camera pitch
    randomization to bridge the indoor-to-outdoor domain gap. Sorokin et al. [[94](#bib.bib94)]
    used a high-fidelity autonomous driving simulator and extracted visual features
    from a pre-trained semantic segmentation model for robust sim-to-real transfer
    to sidewalk navigation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 带腿导航与带轮导航面临许多相似的挑战，但也能够穿越更复杂的地形。研究表明，强健的视觉带腿导航策略可以通过低保真度的运动学模拟在室内[[83](#bib.bib83),
    [86](#bib.bib86)]和室外[[86](#bib.bib86), [94](#bib.bib94)]学习。因此，这些策略专注于运动学级别的控制，同时假设在部署过程中具有有效的低级别运动控制。Truong
    等人[[86](#bib.bib86)]表明，与使用高保真度模拟学习端到端策略相比，这种方法能够加快模拟速度并提高策略的通用性。由于腿部运动学动态被抽象化，这些方法与带轮情况类似，主要挑战是视觉领域的差距。无监督表示学习[[83](#bib.bib83)]和预训练的视觉模型[[94](#bib.bib94)]被用于促进强健的视觉策略。对于户外场景，Truong
    等人[[87](#bib.bib87)]将训练于成熟的室内模拟器的零-shot转移策略应用于户外，使用目标向量归一化和相机俯仰随机化来弥合室内到户外的领域差距。Sorokin
    等人[[94](#bib.bib94)]使用高保真度的自动驾驶模拟器，并从预训练的语义分割模型中提取视觉特征，以实现强健的模拟到现实的转移，用于人行道导航。
- en: While abstracting away low-level locomotion has advantages, it limits the system
    from fully utilizing the agile locomotion skills endowed by advanced locomotion
    controllers. Recent research has explored DRL frameworks integrating locomotion
    with navigation, achieving high-speed obstacle avoidance [[100](#bib.bib100)]
    and agile navigation over challenging terrains (e.g., stairs, gaps, and boxes) [[20](#bib.bib20),
    [96](#bib.bib96)] and through confined 3D space [[98](#bib.bib98), [99](#bib.bib99)].
    Particularly, Lee et al. [[97](#bib.bib97)] demonstrated kilometer-scale navigation
    with a wheeled-legged robot in urban scenarios, overcoming challenging terrains
    and dynamic obstacles. The integrated policy network can be end-to-end, taking
    goal coordinates as input and outputting locomotion commands [[20](#bib.bib20),
    [99](#bib.bib99)]. He et al. [[100](#bib.bib100)] further introduced a recovery
    policy coordinated using a learned reach-avoid value network. Alternatively, training
    efficiency can be improved with hierarchical architectures, where a high-level
    policy governs pre-trained low-level locomotion policies [[95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98)]. Despite the potential of integrating locomotion
    with navigation, policy training could be costly and unstable due to the complex
    low-level dynamics together with the long-horizon nature and sparse rewards of
    the navigation tasks [[20](#bib.bib20), [96](#bib.bib96)]. Classical planning
    algorithms are often used for generating local waypoints to reduce the navigation
    horizon and synthesizing feasible paths to guide initial training [[97](#bib.bib97)].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管抽象化低级运动具有优势，但它限制了系统充分利用高级运动控制器赋予的灵活运动技能。最近的研究探索了将运动与导航整合的DRL框架，实现了高速障碍物避让[[100](#bib.bib100)]和在具有挑战性的地形（如楼梯、间隙和箱子）上进行灵活导航[[20](#bib.bib20),
    [96](#bib.bib96)]以及通过受限的三维空间[[98](#bib.bib98), [99](#bib.bib99)]。特别是，Lee等人[[97](#bib.bib97)]展示了在城市场景中使用轮腿机器人进行公里级导航，克服了具有挑战性的地形和动态障碍物。集成的策略网络可以是端到端的，接受目标坐标作为输入，并输出运动命令[[20](#bib.bib20),
    [99](#bib.bib99)]。He等人[[100](#bib.bib100)]进一步引入了使用学习到的到达-避免值网络协调的恢复策略。或者，可以通过层次架构提高训练效率，其中高级策略控制预训练的低级运动策略[[95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)]。尽管将运动与导航整合具有潜力，但由于复杂的低级动态以及导航任务的长期性质和稀疏奖励，策略训练可能代价高昂且不稳定[[20](#bib.bib20),
    [96](#bib.bib96)]。经典的规划算法通常用于生成局部路径点，以减少导航范围，并合成可行路径以指导初始训练[[97](#bib.bib97)]。
- en: 4.2.3 Aerial Navigation
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 空中导航
- en: ompared to wheeled and legged robots, aerial vehicles such as quadrotors are
    more fragile, requiring higher robustness and safety in navigation policies. The
    weight and power constraints of quadrotors also limit the use of sophisticated
    sensors. Several works have explored DRL-based aerial navigation using low-cost
    monocular cameras [[101](#bib.bib101), [102](#bib.bib102)]. Sadeghi et al. [[101](#bib.bib101)]
    leveraged visual domain randomization to achieve zero-shot sim-to-real transfer
    for indoor aerial navigation. Kang et al. [[102](#bib.bib102)] showed the values
    of 1) task-specific pre-training in simulation for learning generalizable visual
    representation and 2) the use of real-world data for learning accurate dynamics [[79](#bib.bib79)].
    Similar to quadruped navigation, DRL has been used to develop end-to-end navigation
    and locomotion policies for agile aerial navigation. Kaufmann et al. [[7](#bib.bib7)]
    achieved human champion-level performance in drone racing. A key recipe behind
    their success was augmenting simulation with data-driven residual models of the
    drone’s perception and dynamics. Their subsequent study [[21](#bib.bib21)] showed
    that RL’s advantage over model-based methods lies in its ability to directly optimize
    the long-horizon racing task objective. However, DRL-based policies are still
    less robust than human pilots, limiting their operational conditions. Integrating
    actor-critic RL with differential MPC has shown promise in enhancing robustness [[103](#bib.bib103)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与轮式和腿式机器人相比，如四旋翼飞行器等空中车辆更脆弱，需要更高的鲁棒性和安全性的导航策略。四旋翼飞行器的重量和功率限制也限制了先进传感器的使用。有几项工作探讨了使用低成本单目摄像头进行基于深度强化学习（DRL）的空中导航[[101](#bib.bib101),
    [102](#bib.bib102)]。Sadeghi等人[[101](#bib.bib101)]利用视觉领域随机性来实现室内空中导航的零样本虚拟到真实转移。Kang等人[[102](#bib.bib102)]展示了1）仿真中的任务特定预训练对于学习可推广视觉表示以及2）使用真实世界数据学习准确动力学的价值[[79](#bib.bib79)]。与四足动物的导航类似，DRL已被用于开发敏捷空中导航的端到端导航和运动策略。Kaufmann等人[[7](#bib.bib7)]在无人机比赛中实现了人类冠军级别的表现。他们成功的关键因素是增加了无人机感知和动力学的数据驱动残差模型的仿真。他们随后的研究[[21](#bib.bib21)]显示，RL比基于模型的方法优势在于其直接优化长视程赛车任务目标的能力。然而，基于DRL的策略仍然不及人类飞行员稳健，限制了其操作条件。将演员-评论家的RL与微分MPC整合显示了提高稳健性的潜力[[103](#bib.bib103)]。
- en: 4.2.4 Trends and Challenges in Navigation
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 导航的趋势与挑战
- en: RL has shown potential for various submodules of navigation systems, such as
    local planning [[75](#bib.bib75), [104](#bib.bib104)] and global exploration [[76](#bib.bib76),
    [81](#bib.bib81), [82](#bib.bib82)], and for constructing end-to-end navigation
    solutions [[74](#bib.bib74)]. However, RL-based solutions for navigation lack
    the generalization, explainability, and safety guarantees of classical systems
    and thus have not seen widespread real-world deployment [[12](#bib.bib12), [74](#bib.bib74)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RL已经显示出在导航系统的各个子模块，例如局部规划[[75](#bib.bib75), [104](#bib.bib104)]和全局探索[[76](#bib.bib76),
    [81](#bib.bib81), [82](#bib.bib82)]，以及构建端到端导航解决方案[[74](#bib.bib74)]方面的潜力。然而，导航的基于RL的解决方案缺乏传统系统的泛化性、可解释性和安全性保证，因此在现实世界中并没有得到广泛的部署[[12](#bib.bib12),
    [74](#bib.bib74)]。
- en: In visual navigation, model-free, end-to-end policies show promise for structured
    indoor environments like homes [[105](#bib.bib105)], while modular architectures
    boost performance without sacrificing guarantees and generalization [[81](#bib.bib81),
    [82](#bib.bib82)]. Striking the right balance between learned and classical modules
    remains an open challenge. Hybrid approaches may be promising, for example, leveraging
    implicit map-like representations learned by end-to-end approaches [[106](#bib.bib106)],
    or using differentiable scene representations [[107](#bib.bib107)] to enable RL
    with algorithmic structure. RL-based vision-and-language navigation [[77](#bib.bib77)]
    is relatively under-explored in real-world settings but promising given the recent
    advances in vision-language models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉导航中，无模型、端到端的策略在家庭等结构化室内环境中显示出很好的性能[[105](#bib.bib105)]，而模块化体系结构能提高性能而不损害保证和泛化[[81](#bib.bib81),
    [82](#bib.bib82)]。在学习模块和经典模块之间取得平衡仍然是一个悬而未决的挑战。混合方法可能是有前途的，例如，利用端到端方法学习的隐式地图样式表示[[106](#bib.bib106)]，或者使用可微分的场景表示[[107](#bib.bib107)]来实现具有算法结构的RL。基于RL的视觉-语言导航在真实世界环境中相对较少被探索，但鉴于近期视觉-语言模型的进展，具有前景。
- en: In legged navigation, abstracting away low-level dynamics has been shown to
    facilitate sim-to-real transfer for navigation [[86](#bib.bib86)]. For agile legged
    and aerial navigation, where low-level complexity is unavoidable, jointly learning
    navigation and locomotion yields promising results [[100](#bib.bib100), [20](#bib.bib20),
    [96](#bib.bib96), [7](#bib.bib7)]. Yet, involving locomotion complicates the training
    of long-horizon navigation policies, which requires future developments to stabilize
    learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在腿部导航中，抽象低层次动态已被证明有助于导航的仿真到现实转移 [[86](#bib.bib86)]。对于敏捷的腿部和空中导航，其中低层次复杂性不可避免，联合学习导航和运动会产生有希望的结果 [[100](#bib.bib100),
    [20](#bib.bib20), [96](#bib.bib96), [7](#bib.bib7)]。然而，涉及运动会使长时间范围的导航策略训练复杂化，这需要未来的发展来稳定学习。
- en: Finally, learning navigation (collision avoidance, in particular) for safety-critical
    systems, like urban autonomous vehicles and drones, is challenging due to stringent
    robustness requirements in perception and control. These domains have seen fewer
    real-world successes as a result. Real-world data can help improve simulation
    fidelity for this purpose [[7](#bib.bib7), [21](#bib.bib21), [103](#bib.bib103)],
    though establishing guarantees on their performance remains difficult.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，学习导航（特别是碰撞避免）对于安全关键系统，如城市自动驾驶汽车和无人机，具有挑战性，因为在感知和控制方面需要严格的鲁棒性要求。因此，这些领域的现实世界成功较少。现实世界的数据可以帮助提高仿真准确性[[7](#bib.bib7),
    [21](#bib.bib21), [103](#bib.bib103)]，但对其性能建立保证仍然困难。
- en: '{summary}'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '{总结}'
- en: '[Key Takeaways]'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[关键要点]'
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While end-to-end RL excels at visual navigation in simulation, most real-world
    successes deploy modular designs and learn components of the navigation stack.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然端到端的强化学习在仿真中的视觉导航表现出色，但大多数现实世界的成功应用都采用了模块化设计，并学习导航栈的各个组件。
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Integrating RL into these modular architectures, e.g., for local planning or
    semantic exploration, is a promising avenue.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将强化学习（RL）融入这些模块化架构，例如用于局部规划或语义探索，是一个有前景的方向。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recent work reasoning jointly about navigation and locomotion enables agile
    legged and aerial navigation, yet how to learn long-horizon navigation stably
    and efficiently with low-level control in the loop remains an open challenge.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近的工作通过联合考虑导航和运动实现了敏捷的腿部和空中导航，但如何在低层控制环中稳定高效地学习长时间范围的导航仍然是一个开放挑战。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Safety-critical applications like urban autonomous driving or outdoor drone
    flight have seen few real-world successes due to the higher requirements for robustness
    and the lack of explainability and generalization on the part of RL algorithms.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像城市自动驾驶或户外无人机飞行这样的安全关键应用，由于对鲁棒性的更高要求以及RL算法在解释性和泛化性方面的不足，现实世界的成功案例很少。
- en: 4.3 Manipulation
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 操作
- en: '![Refer to caption](img/cfde9fe8327d02f485403320af8a52d9.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cfde9fe8327d02f485403320af8a52d9.png)'
- en: '| Pick-and-place | Grasping | [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 拾取与放置 | 抓取 | [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112) |'
- en: '|  | End-to-end Pick-and-place | [54](#bib.bib54), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118),
    [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122),
    [123](#bib.bib123), [124](#bib.bib124), [125](#bib.bib125) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 端到端拾取与放置 | [54](#bib.bib54), [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123),
    [124](#bib.bib124), [125](#bib.bib125) |'
- en: '| Contact-rich | Assembly | [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 高接触性 | 组装 | [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128), [129](#bib.bib129),
    [130](#bib.bib130) |'
- en: '|  | Articulated Objects | [122](#bib.bib122), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | 铰接物体 | [122](#bib.bib122), [131](#bib.bib131), [132](#bib.bib132), [133](#bib.bib133)
    |'
- en: '|  | Deformable Objects | [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | 可变形物体 | [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137)
    |'
- en: '| In-hand | — | [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 手中 | — | [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142) |'
- en: '| Non-prehensile | — | [109](#bib.bib109), [118](#bib.bib118), [143](#bib.bib143),
    [144](#bib.bib144), [145](#bib.bib145) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 非抓握性 | — | [109](#bib.bib109), [118](#bib.bib118), [143](#bib.bib143), [144](#bib.bib144),
    [145](#bib.bib145) |'
- en: 'Figure 4: Top: An overview of the four manipulation problems reviewed in Sec. [4.3](#S4.SS3
    "4.3 Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes"), including pick-and-place [[108](#bib.bib108)],
    contact-rich manipulation [[130](#bib.bib130)], in-hand manipulation [[141](#bib.bib141)],
    and non-prehensile manipulation [[143](#bib.bib143)]; Bottom: Manipulation papers
    reviewed in Sec. [4.3](#S4.SS3 "4.3 Manipulation ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes").
    The color map indicates the levels of real-world success: *Limited Lab*, *Diverse
    Lab*, *Limited Real*, and *Diverse Real*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '图4：顶部：第[4.3](#S4.SS3 "4.3 Manipulation ‣ 4 Competency-Specific Review ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes")节回顾的四个操作问题的概述，包括拾取与放置[[108](#bib.bib108)]、接触丰富的操作[[130](#bib.bib130)]、手内操作[[141](#bib.bib141)]和非抓取操作[[143](#bib.bib143)]；底部：第[4.3](#S4.SS3
    "4.3 Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes")节回顾的操作论文。颜色图示显示了现实世界成功的水平：*有限的实验室*、*多样的实验室*、*有限的现实*和*多样的现实*。'
- en: 'Manipulation refers to an agent’s control of its environment through selective
    contact [[19](#bib.bib19)]. To perform useful work in the world, robots require
    manipulation capabilities such as pick-and-place, mechanical assembly, in-hand
    manipulation, non-prehensile manipulation, and beyond. Manipulation poses several
    challenges for both analytical and learning-based methods [[11](#bib.bib11)],
    as the mechanics of contact are complex and difficult to model, and open-world
    manipulation requires strong generalization and fast online learning. RL is well-suited
    to these challenges, but manipulation poses fundamental difficulties for RL: large
    observation and action spaces make real-world exploration prohibitively time-consuming
    and unsafe; reward function design requires domain knowledge; tasks are often
    long-horizon; and instantaneous environment resets are usually unrealistic in
    real-world tasks. Despite these challenges, DRL has achieved notable successes
    in manipulation recently.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 操作指的是代理通过选择性接触来控制其环境[[19](#bib.bib19)]。为了在世界上执行有用的工作，机器人需要诸如拾取与放置、机械组装、手内操作、非抓取操作等操作能力。操作对分析方法和基于学习的方法都提出了几项挑战[[11](#bib.bib11)]，因为接触的力学复杂且难以建模，而开放世界的操作需要强大的泛化能力和快速的在线学习。强化学习（RL）适合这些挑战，但操作对RL提出了根本性的困难：大规模的观察和动作空间使得现实世界探索耗时且不安全；奖励函数设计需要领域知识；任务通常具有较长的时间跨度；即时环境重置在现实世界任务中通常不切实际。尽管面临这些挑战，深度强化学习（DRL）在操作方面最近取得了显著成功。
- en: 'In this subsection, we review progress in several manipulation capabilities
    enabled by DRL, following the outline from Mason’s seminal review [[19](#bib.bib19)]:
    pick-and-place, contact-rich manipulation, in-hand manipulation, and non-prehensile
    manipulation. See Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Manipulation ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    for an overview of the papers reviewed in this subsection. Note that this subsection
    focuses on stationary manipulators, and we defer mobile manipulation to Sec. [4.4](#S4.SS4
    "4.4 Mobile Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节中，我们回顾了DRL支持的几种操作能力的进展，按照Mason的开创性综述[[19](#bib.bib19)]的提纲：拾取与放置、接触丰富的操作、手内操作和非抓取操作。有关本小节所回顾的论文概述，请参见图[4](#S4.F4
    "Figure 4 ‣ 4.3 Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")。注意，本小节侧重于静态操作器，而移动操作将推迟到第[4.4](#S4.SS4
    "4.4 Mobile Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes")节。'
- en: 4.3.1 Pick-and-place
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 拾取与放置
- en: Picking and placing objects is a longstanding challenge in manipulation, requiring
    the ability to perceive objects, grasp them, determine appropriate placements,
    and generate collision-free motion. Structured pick-and-place, in which the environment
    is engineered to reduce complexity and objects are known a priori, is well-understood
    and widely deployed in manufacturing contexts. Open-world, unstructured pick-and-place—rearranging
    arbitrary objects in the wild—remains a challenge. In recent years, more traditional
    robotic approaches have seen success in industrial applications like fulfillment,
    employing machine learning for object detection and grasping but deferring control
    to analytical methods [[19](#bib.bib19)]. While pick-and-place tasks serve as
    a common testbed for new RL algorithms [[115](#bib.bib115), [116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119), [54](#bib.bib54)],
    end-to-end RL methods still lack the ability to pick and place novel objects in
    the open world with generality. However, modular approaches, such as solving grasping
    with RL, have enabled some real-world successes. We will review RL-based solutions
    to the subproblem of grasping and then discuss end-to-end RL methods, omitting
    a discussion of motion generation for which RL is not commonly used.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 挑选和放置物体是操纵中长期以来的一个挑战，需要能够感知物体、抓取物体、确定合适的放置位置并生成无碰撞运动的能力。结构化的挑选和放置，其中环境被设计为降低复杂性，并且对象是预先知道的，在制造环境中被广泛应用和理解。在野外环境中重新排列任意物体的开放式、非结构化的挑选和放置仍然是一个挑战。近年来，更传统的机器人方法在工业应用中取得了成功，如实现目标的应用，采用机器学习来检测和抓取物体，但将控制交给分析方法[[19](#bib.bib19)].。虽然挑选和放置任务作为新的强化学习算法的常见测试平台[[115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119),
    [54](#bib.bib54)]，但端到端强化学习方法仍然缺乏在开放世界中泛化能力的能力来挑选和放置新颖物体。然而，模块化方法，如利用强化学习解决抓取问题，已实现了一些真实世界的成功。我们将回顾基于强化学习的解决方案的子问题的抓取问题，然后讨论端到端强化学习方法，不讨论通常不使用强化学习进行运动生成的讨论。
- en: 4.3.1.1 Grasping
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.3.1.1 抓取
- en: Grasping objects is a fundamental capability essential for pick-and-place and
    other downstream tasks, such as in-hand manipulation and assembly. Some of the
    first large-scale successes of DRL for manipulation were in grasping objects with
    unknown geometry and appearance [[108](#bib.bib108)]. Where analytical methods
    had achieved grasping of known objects using taxonomies and databases, these works
    leveraged thousands or millions of grasp attempts to learn grasping behaviors
    through interaction. Many works frame grasping as a bandit or classification problem,
    where the action space consists of discrete grasp candidates and the picking motion
    is executed open-loop [[108](#bib.bib108), [109](#bib.bib109)]. These methods
    commonly employ sparse rewards that indicate success when an object is lifted
    and collect data in a self-supervisory manner. Similar systems have been reportedly
    integrated into fulfillment applications⁶⁶6See examples from Ambi Robotics ([https://t.ly/tSds_](https://t.ly/tSds_))
    and Covariant ([https://t.ly/S5pnz](https://t.ly/S5pnz)). with diverse objects.
    Closed-loop grasping—controlling the end-effector pose and/or fingers directly
    to achieve stable grasps—can be formulated as a sequential decision-making problem
    and solved with RL. While some successes have been seen [[110](#bib.bib110), [111](#bib.bib111),
    [112](#bib.bib112)], closed-loop grasping remains challenging due to the additional
    complexity of learning vision-based closed-loop control, and such systems have
    not seen the same level of real-world success as open-loop ones. In both closed-
    and open-loop grasping, while some works exclusively collect real-world data [[109](#bib.bib109),
    [110](#bib.bib110), [112](#bib.bib112)], the common recipe is to use simulation
    for data collection [[108](#bib.bib108)] or policy training [[111](#bib.bib111)],
    often employing domain adaptation to ensure visual similarity between the simulator
    and real world.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 抓取物体是进行抓取与放置以及其他下游任务（如手内操作和组装）的基本能力。一些深度强化学习（DRL）在操控领域的大规模成功最早体现在对未知几何形状和外观的物体进行抓取[[108](#bib.bib108)]。而分析方法通过分类法和数据库实现了对已知物体的抓取，这些研究则利用成千上万次的抓取尝试，通过交互学习抓取行为。许多研究将抓取视为一个赌博问题或分类问题，其中动作空间由离散的抓取候选项组成，抓取动作以开环方式执行[[108](#bib.bib108),
    [109](#bib.bib109)]。这些方法通常使用稀疏奖励来指示成功（当物体被提起时），并以自我监督的方式收集数据。类似的系统据报道已被集成到履行应用中⁶⁶6请参见
    Ambi Robotics 的示例 ([https://t.ly/tSds_](https://t.ly/tSds_)) 和 Covariant ([https://t.ly/S5pnz](https://t.ly/S5pnz))，适用于各种物体。闭环抓取——直接控制末端执行器姿态和/或手指以实现稳定的抓取——可以被公式化为一个序列决策问题，并通过强化学习解决。虽然已取得一些成功[[110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112)]，但由于学习基于视觉的闭环控制的额外复杂性，闭环抓取仍然具有挑战性，这些系统在现实世界中的成功程度尚未达到开环系统的水平。在闭环和开环抓取中，虽然一些研究仅收集真实世界的数据[[109](#bib.bib109),
    [110](#bib.bib110), [112](#bib.bib112)]，但常见的做法是使用模拟进行数据收集[[108](#bib.bib108)]或策略训练[[111](#bib.bib111)]，通常采用领域适应以确保模拟器与现实世界之间的视觉相似性。
- en: 4.3.1.2 End-to-end Pick-and-place
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.3.1.2 端到端的抓取与放置
- en: 'Learning general-purpose pick-and-place in the open world remains daunting
    for end-to-end RL, owing to the sheer variety of objects and tasks and the limited
    generalization of current algorithms. This variety also precludes the common sim-to-real
    recipe successful in other domains like grasping and in-hand manipulation, where
    tasks and objects can be enumerated during training. Nonetheless, some major milestones
    in end-to-end pick-and-place have been observed: Levine et al. [[113](#bib.bib113)]
    demonstrated the potential of deep visuomotor policies; Riedmiller et al. [[119](#bib.bib119)]
    demonstrated pick-and-place manipulation with a hierarchical policy trained in
    the real world; and Lee et al. [[116](#bib.bib116)] achieved stacking of diverse
    objects through sim-to-real transfer. Augmenting the action space with primitives [[123](#bib.bib123)]
    can help in reducing the task horizon and is a natural means to incorporate human
    engineering. Recent work leveraging large vision-language models shows promise
    in handling open-ended diverse objects and task objectives specified by natural
    language [[124](#bib.bib124)]. The potential of RL to solve this longstanding
    challenge is only now coming into focus with emerging large-scale robotic datasets
    and foundation models. Despite not yet achieving widespread success in real-world
    deployments, many important RL innovations have been demonstrated in pick-and-place
    problems, addressing challenges such as multi-task learning [[114](#bib.bib114),
    [115](#bib.bib115), [125](#bib.bib125)], sample efficiency [[54](#bib.bib54)],
    defining and computing reward [[120](#bib.bib120), [121](#bib.bib121)], resetting
    the environment [[117](#bib.bib117)], and utilizing human demonstrations or offline
    data [[122](#bib.bib122), [116](#bib.bib116), [124](#bib.bib124)].'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在开放世界中学习通用的捡放任务对于端到端的强化学习仍然令人畏惧，因为对象和任务的种类繁多，以及当前算法的有限泛化能力。这种多样性也排除了在其他领域（如抓取和手内操控）中成功的常见模拟到现实的配方，在这些领域中，任务和对象可以在训练过程中列举出来。尽管如此，端到端的捡放任务仍然取得了一些重大突破：Levine
    等人[[113](#bib.bib113)] 展示了深度视觉运动政策的潜力；Riedmiller 等人[[119](#bib.bib119)] 展示了在现实世界中通过层级政策进行的捡放操控；Lee
    等人[[116](#bib.bib116)] 通过模拟到现实转移实现了多样物体的堆叠。通过原语[[123](#bib.bib123)] 扩展动作空间可以帮助缩短任务范围，是融入人类工程的自然方式。近期利用大规模视觉语言模型的工作在处理由自然语言指定的开放性多样物体和任务目标方面显示了前景[[124](#bib.bib124)]。随着新兴的大规模机器人数据集和基础模型的出现，强化学习解决这一长期挑战的潜力才刚刚显现。尽管在实际部署中尚未取得广泛成功，但许多重要的强化学习创新已经在捡放问题中得到展示，解决了诸如多任务学习[[114](#bib.bib114)、[115](#bib.bib115)、[125](#bib.bib125)]、样本效率[[54](#bib.bib54)]、奖励定义和计算[[120](#bib.bib120)、[121](#bib.bib121)]、环境重置[[117](#bib.bib117)]以及利用人类示范或离线数据[[122](#bib.bib122)、[116](#bib.bib116)、[124](#bib.bib124)]等挑战。
- en: 4.3.2 Contact-rich Manipulation
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 接触丰富的操控
- en: 'While pick-and-place tasks are often assumed to be strictly kinematic, contact-rich
    tasks like mechanical assembly (e.g., peg insertion), interacting with articulated
    objects (e.g., opening doors), and manipulating deformable objects, require reasoning
    about dynamics and relaxing the rigid-body assumption of the objects. We discuss
    several contact-rich tasks where RL has advanced the state of the art: assembly,
    articulated object manipulation, and deformable object manipulation.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然捡放任务通常被认为是严格的运动学问题，但接触丰富的任务，如机械装配（例如，插入销钉）、与关节物体的互动（例如，开门）以及操控可变形物体，都需要考虑动力学，并放宽物体的刚体假设。我们讨论了几种接触丰富的任务，其中强化学习（RL）推动了技术的进步：装配、关节物体操控和可变形物体操控。
- en: 4.3.2.1 Assembly
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.3.2.1 装配
- en: Assembly tasks are crucial in manufacturing, and automating them is a longstanding
    challenge in robotics. Existing industrial solutions tend to rely on extensive
    engineering of the environment and robot motions, resulting in behaviors sensitive
    to small perturbations and costly to design. Assembly is challenging for RL due
    to the difficulty in controlling contact-rich interactions and the stringent requirements
    for accuracy and precision, coupled with the need to handle diverse object parts.
    While RL has not seen widespread deployment in industrial contexts, some notable
    successes have been observed in recent years. Many approaches employ sim-to-real
    transfer to achieve assembly [[130](#bib.bib130)], though some train policies
    directly in the real world [[126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129)], typically leveraging human demonstrations. Luo et al. [[128](#bib.bib128)]
    notably compare against solutions provided by integrators and find their RL-based
    policies more robust to perturbation. A common strategy among approaches to assembly
    is using residual RL [[126](#bib.bib126)], in which a residual policy is learned
    on top of a reference trajectory. Most works assume that the object is already
    grasped before assembly. By contrast, Tang et al. [[130](#bib.bib130)] present
    a sim-to-real RL framework for the entire assembly pipeline, including object
    detection, grasping, and insertion, achieving diverse assembly tasks by leveraging
    recent advances in contact simulation and developing algorithmic advances for
    sim-to-real transfer.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 组装任务在制造中至关重要，自动化它们是机器人学中一个长期存在的挑战。现有的工业解决方案往往依赖于对环境和机器人动作的广泛工程设计，导致行为对小的扰动敏感且设计成本高昂。由于控制接触丰富的交互和对准确性与精度的严格要求，加上需要处理多样的物体部件，组装对强化学习（RL）来说具有挑战性。虽然RL在工业环境中尚未得到广泛应用，但近年来已观察到一些显著的成功。许多方法采用了从模拟到现实的转移来实现组装[[130](#bib.bib130)]，尽管一些方法直接在现实世界中训练策略[[126](#bib.bib126),
    [127](#bib.bib127), [128](#bib.bib128), [129](#bib.bib129)]，通常利用人类演示。Luo等人[[128](#bib.bib128)]显著地与集成商提供的解决方案进行了比较，发现其基于RL的策略对扰动更具鲁棒性。许多组装方法采用了残差RL[[126](#bib.bib126)]，其中在参考轨迹上学习残差策略。大多数工作假设对象在组装之前已被抓取。相比之下，Tang等人[[130](#bib.bib130)]提出了一种完整组装流程的从模拟到现实的RL框架，包括物体检测、抓取和插入，通过利用接触模拟的最新进展和开发算法进展实现了多样化的组装任务。
- en: 4.3.2.2 Articulated Objects
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.3.2.2 铰接对象
- en: Some limited successes have been observed in constrained manipulation tasks
    like opening drawers. Most commonly, these tasks are used to demonstrate RL capabilities
    without dedicated efforts to realize practical deployment [[122](#bib.bib122),
    [131](#bib.bib131)]. Other works target this class of skills in particular [[132](#bib.bib132),
    [133](#bib.bib133)] with limited success.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在受限操作任务（如打开抽屉）中观察到一些有限的成功。最常见的是，这些任务用于展示RL能力，而没有专门的努力实现实际部署[[122](#bib.bib122),
    [131](#bib.bib131)]。其他工作特别针对这一类技能[[132](#bib.bib132), [133](#bib.bib133)]，但成功有限。
- en: 4.3.2.3 Deformable Objects
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.3.2.3 可变形对象
- en: Deformable objects, such as cloth, present additional challenges owing to the
    difficulty in accurately modeling soft materials. Tasks like cloth folding [[134](#bib.bib134),
    [135](#bib.bib135), [136](#bib.bib136)] and assistive dressing [[137](#bib.bib137)]
    have thus received considerable attention in RL. These works often employ sim-to-real
    transfer [[134](#bib.bib134), [136](#bib.bib136), [137](#bib.bib137)], and often
    simplify the tasks using primitives such as pick-and-place [[135](#bib.bib135)]
    and flinging [[136](#bib.bib136)].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 可变形对象，如布料，由于难以准确建模软材料，带来了额外的挑战。诸如布料折叠[[134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)]和辅助穿衣[[137](#bib.bib137)]等任务因此在RL中得到了相当多的关注。这些工作通常采用从模拟到现实的转移[[134](#bib.bib134),
    [136](#bib.bib136), [137](#bib.bib137)]，并经常使用诸如取放[[135](#bib.bib135)]和抛掷[[136](#bib.bib136)]等原语来简化任务。
- en: In summary, open-world contact-rich manipulation inherits the challenges of
    unstructured pick-and-place (namely, generalization to novel objects and tasks)
    and the additional challenge of controlling contact-rich interactions. Nonetheless,
    some successes have been demonstrated in contact-rich tasks, particularly assembly
    and deformable objects, where tasks are predefined, objects are enumerable, and
    rigid grasps are usually assumed.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，开放世界中的接触丰富操作继承了无结构的取放任务的挑战（即对新对象和任务的泛化能力）以及控制接触丰富交互的额外挑战。尽管如此，已在接触丰富的任务中展示了一些成功，特别是在组装和可变形对象方面，这些任务是预定义的，对象是可枚举的，并且通常假设使用刚性抓取。
- en: 4.3.3 In-hand Manipulation
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 手内操作
- en: Humans exhibit many in-hand manipulation behaviors, re-orienting and re-positioning
    objects to facilitate downstream manipulation. Impressive strides in the development
    of these capabilities have been made with DRL in recent years, allowing agents
    to learn such complex in-hand manipulation behaviors with impressive generalization.
    Several works focused on re-orienting single objects to target configurations [[138](#bib.bib138),
    [139](#bib.bib139)], employing pose estimation modules trained in simulation.
    Nagabandi et al. [[140](#bib.bib140)] similarly demonstrated rotating Baoding
    balls with model-based RL. While showing impressive dexterity, these works focus
    on manipulating known objects (e.g., a given cube) with low-dimensional observations.
    Recent methods leveraging vision and tactile data have demonstrated rotating arbitrary
    objects about arbitrary axes [[141](#bib.bib141)], even against gravity [[142](#bib.bib142)].
    These approaches employ extensive domain randomization and typically leverage
    privileged information (e.g., object shape information, dynamic properties) and
    dense rewards when training in simulation. An open challenge is integrating these
    in-hand manipulation skills with other manipulation abilities (e.g., tool use),
    which require re-orientation to a target configuration suitable for a downstream
    task.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 人类展示了许多手内操作行为，通过重新定位和调整物体以促进后续操作。近年来，深度强化学习（DRL）在这些能力的发展上取得了显著进展，使得代理能够学习这些复杂的手内操作行为，并具有令人印象深刻的泛化能力。若干研究专注于将单个物体重新定位到目标配置 [[138](#bib.bib138),
    [139](#bib.bib139)]，使用在仿真中训练的姿态估计模块。Nagabandi 等人 [[140](#bib.bib140)] 同样展示了使用基于模型的强化学习旋转保定球。尽管展示了令人印象深刻的灵活性，这些工作专注于操作已知物体（例如，给定的立方体）并使用低维观察。近期的方法利用视觉和触觉数据展示了绕任意轴旋转任意物体的能力 [[141](#bib.bib141)]，甚至在抗重力的情况下 [[142](#bib.bib142)]。这些方法采用了广泛的领域随机化，并通常在仿真训练中利用特权信息（例如，物体形状信息、动态属性）和密集奖励。一个开放的挑战是将这些手内操作技能与其他操作能力（例如，工具使用）整合，这需要将物体重新定向到适合下游任务的目标配置。
- en: 4.3.4 Non-prehensile Manipulation
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 非抓取操作
- en: Non-prehensile manipulation, namely moving objects without grasping, is crucial
    when objects are too large to be grasped, grasps are occluded, or in tool use.
    Object pushing abilities have long been demonstrated with RL [[118](#bib.bib118)],
    and studied in connection to grasping [[109](#bib.bib109), [143](#bib.bib143)].
    Recently, general non-prehensile re-orientation of diverse objects has been enabled
    through sim-to-real transfer of RL policies [[144](#bib.bib144), [145](#bib.bib145)].
    Similar to in-hand manipulation, learning with privileged information (i.e., object
    geometry) before distilling a student policy is a common approach. Further work
    is warranted to integrate these skills with prehensile and in-hand behaviors and
    to develop extrinsic dexterity, where the environment is used to facilitate manipulation.
    How to synthesize these capabilities for general-purpose, open-world manipulation
    remains an open question.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 非抓取操作，即在不抓取物体的情况下移动物体，对于物体过大而无法抓取、抓取被遮挡或工具使用时至关重要。物体推动能力已通过强化学习（RL）得到长期验证 [[118](#bib.bib118)]，并与抓取行为进行了研究 [[109](#bib.bib109),
    [143](#bib.bib143)]。近期，通过强化学习策略的仿真到现实转移，已实现了多样物体的一般非抓取重新定位 [[144](#bib.bib144),
    [145](#bib.bib145)]。与手内操作类似，利用特权信息（即，物体几何信息）来学习并在提炼学生策略之前是一种常见的方法。需要进一步的工作来将这些技能与抓取和手内行为整合，并发展外部灵活性，其中环境用于促进操作。如何综合这些能力以实现通用的开放世界操作仍然是一个开放的问题。
- en: 4.3.5 Trends and Open Challenges in Manipulation
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 操作中的趋势和开放挑战
- en: RL is beginning to achieve real-world success in various manipulation problems.
    Generally, RL has been more successful in domains where the space of tasks is
    more constrained—grasping, in-hand manipulation, and assembly—rather than less,
    e.g., end-to-end pick-and-place. These more constrained tasks allow for a priori
    reward design and zero-shot sim-to-real transfer, whereas open-world pick-and-place
    and contact-rich manipulation require generalizing to diverse objects and tasks.
    The limitations of physical simulation may also preclude scaling sim-to-real for
    contact-rich tasks. Differentiable simulation has shown promise for this challenge [[146](#bib.bib146)].
    Open-world manipulation will require several advances, including scaling collections
    of simulated assets and tasks; few-shot sim-to-real [[131](#bib.bib131)]; multi-task
    learning [[114](#bib.bib114), [125](#bib.bib125)]; learning autonomously in the
    real world [[120](#bib.bib120), [117](#bib.bib117), [54](#bib.bib54)]; learning
    reward functions from examples [[120](#bib.bib120)] or human videos [[121](#bib.bib121)];
    and utilizing human demonstrations [[127](#bib.bib127)], offline data [[122](#bib.bib122)]
    and foundation models [[124](#bib.bib124)]. Incorporating priors, such as symmetry [[112](#bib.bib112)]
    and geometry [[147](#bib.bib147)], is promising for improving sample efficiency,
    generalization, and safety. Learning more complex behaviors, e.g. bimanual [[148](#bib.bib148)]
    or dynamic tasks like table tennis [[149](#bib.bib149)], is another important
    avenue for future work [[11](#bib.bib11), [19](#bib.bib19)].
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）开始在各种操作问题中取得现实世界的成功。通常，RL在任务空间更受限的领域中更为成功——抓取、手部操作和装配——而不是那些不受限的领域，例如端到端的捡放。这些更受限的任务允许事先奖励设计和零样本的模拟到现实转移，而开放世界的捡放和接触丰富的操作则需要对多样的物体和任务进行泛化。物理模拟的局限性也可能阻止接触丰富任务的模拟到现实的扩展。可微分模拟在这一挑战中显示出了潜力[[146](#bib.bib146)]。开放世界操作将需要若干进展，包括扩展模拟资产和任务的集合；少样本的模拟到现实[[131](#bib.bib131)]；多任务学习[[114](#bib.bib114),
    [125](#bib.bib125)]；在现实世界中自主学习[[120](#bib.bib120), [117](#bib.bib117), [54](#bib.bib54)]；从示例[[120](#bib.bib120)]或人类视频[[121](#bib.bib121)]中学习奖励函数；以及利用人类演示[[127](#bib.bib127)]、离线数据[[122](#bib.bib122)]和基础模型[[124](#bib.bib124)]。引入先验知识，如对称性[[112](#bib.bib112)]和几何[[147](#bib.bib147)]，在提高样本效率、泛化和安全性方面表现出良好前景。学习更复杂的行为，例如双手操作[[148](#bib.bib148)]或动态任务如乒乓球[[149](#bib.bib149)]，是未来工作中的另一个重要方向[[11](#bib.bib11),
    [19](#bib.bib19)]。
- en: Additionally, action spaces are typically chosen by domain experts to match
    each problem at hand. Open-loop grasping tends to employ an abstraction of motion
    generation for reaching and closing the fingers, whereas closed-loop grasping,
    assembly, and end-to-end pick-and-place methods typically control the end-effector
    Cartesian pose or velocity. Most in-hand manipulation approaches control the fingers
    in configuration space, keeping the end-effector itself in a fixed position. Equipping
    one agent with these various manipulation abilities remains an important challenge
    for deploying capable manipulators in the real world. Moreover, many of these
    real-world successes are demonstrated on short-horizon tasks; further work is
    warranted to build agents that can reason over longer periods of time and compose
    learned abilities together to solve long-horizon tasks [[11](#bib.bib11), [123](#bib.bib123),
    [132](#bib.bib132), [150](#bib.bib150), [151](#bib.bib151)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，动作空间通常由领域专家选择，以匹配每个具体问题。开环抓取倾向于采用运动生成的抽象来实现手指的伸展和闭合，而闭环抓取、装配和端到端的捡放方法通常控制末端执行器的笛卡尔姿态或速度。大多数手部操作方法在配置空间中控制手指，使末端执行器保持固定位置。为一个代理装备这些各种操作能力仍然是将有能力的操作器部署到现实世界中的一个重要挑战。此外，许多现实世界的成功展示是在短期任务上；进一步的工作是构建能够在更长时间内进行推理并将学习到的能力组合起来以解决长期任务的代理[[11](#bib.bib11),
    [123](#bib.bib123), [132](#bib.bib132), [150](#bib.bib150), [151](#bib.bib151)]。
- en: '{summary}'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '{summary}'
- en: '[Key Takeaways]'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[关键要点]'
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RL solutions for manipulation are generally less mature than locomotion, with
    few deployments in the wild, yet there exist many impressive demonstrations in
    representative real-world conditions.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RL在操作领域的解决方案通常不如运动学成熟，实际部署较少，但在具有代表性的现实世界条件下存在许多令人印象深刻的展示。
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Manipulation subproblems where tasks can be enumerated a priori—e.g., grasping,
    in-hand manipulation, assembly—allow for zero-shot sim-to-real transfer, facilitating
    many of the real-world successes.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务可以事先枚举的操作子问题，例如抓取、手部操作、装配，允许零样本的模拟到现实转移，促进了许多现实世界的成功。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Integrating manipulation subfields and connecting with task planning to build
    a generally competent manipulator remains an open challenge.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成操作子领域并与任务规划相连接，以建立一个普遍有效的操作器仍然是一个开放的挑战。
- en: 4.4 Mobile Manipulation
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 移动操作
- en: 'Mobile manipulators are robotic agents combining mobility and manipulation
    competencies, unlocking applications in households, healthcare, and logistics.
    Mobile manipulation (MoMa) problems present unique challenges requiring more than
    a simple concatenation of locomotion and manipulation, including the need to control
    and synchronize many degrees-of-freedom governing multiple body components (e.g.,
    head, arm(s), and base/legs), strong partial observability and tasks with a natural
    long horizon. DRL has been applied to tackle various types of MoMa tasks, including
    1) learning precise, real-time whole-body control; 2) learning object perception
    and interaction in short-horizon interactive tasks; and 3) high-level decision-making
    in long-horizon interactive tasks. In this section, we review works addressing
    these three problems summarized in Figure [5](#S4.F5 "Figure 5 ‣ 4.4.3 Long-horizon
    Interactive Tasks ‣ 4.4 Mobile Manipulation ‣ 4 Competency-Specific Review ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 移动操作器是结合了移动性和操作能力的机器人代理，解锁了家庭、医疗和物流领域的应用。移动操作（MoMa）问题呈现出独特的挑战，需要的不仅仅是简单的运动和操作的串联，包括需要控制和同步多个体部件（如头部、手臂和底座/腿部）所涉及的多个自由度、强烈的部分可观察性和具有自然长远视角的任务。DRL已被应用于解决各种类型的MoMa任务，包括1）学习精确的实时全身控制；2）在短视角互动任务中学习物体感知和互动；3）在长视角互动任务中的高层决策。在本节中，我们回顾了这些问题的相关工作，概述如图[5](#S4.F5
    "图 5 ‣ 4.4.3 长视角互动任务 ‣ 4.4 移动操作 ‣ 4 能力特定评审 ‣ 机器人深度强化学习：现实世界成功案例的调查")所示。
- en: 4.4.1 Learning Whole-Body Control
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 学习全身控制
- en: The common goal in whole-body control (WBC) for mobile manipulators is to determine
    an action or sequence of actions for all degrees of freedom of the body to reach
    a desired configuration, possibly fulfilling additional constraints. Frequently,
    the desired configuration is specified as the desired position or pose of one
    or more of the links of the agent, e.g., the desired pose of the end-effector [[152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155)]. While there exist
    model-based analytical methods for whole-body control in advanced control theory
    literature [[156](#bib.bib156)], DRL has been explored as a powerful alternative
    in situations where either the system dynamics are hard to model (e.g., leg-ground
    contact, slippery wheels, unknown manipulator dynamics), or when the inference-time
    computation is constrained—a frequent problem in MoMa tasks due to the robot embodiment’s
    complexity. For example, Wang et al. [[152](#bib.bib152)] and Fu et al. [[154](#bib.bib154)]
    learned whole-body policies that enable a wheeled mobile manipulator and a quadruped
    with an arm to reach points in 3D space with their end-effector. Ma et al. [[153](#bib.bib153)]
    learned a locomotion policy robust to random wrench perturbation and used an MPC
    planner to control the arm for point reaching.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 移动操作器（WBC）中全身控制的共同目标是确定一个或多个动作的序列，以使身体的所有自由度达到所需的配置，并可能满足额外的约束条件。通常，所需的配置被指定为代理的一条或多条链接的期望位置或姿态，例如，末端执行器的期望姿态[[152](#bib.bib152)、[153](#bib.bib153)、[154](#bib.bib154)、[155](#bib.bib155)]。尽管在先进控制理论文献中存在基于模型的解析方法[[156](#bib.bib156)]，但在系统动态难以建模（如腿部与地面的接触、滑轮、未知操作器动态）或推理时间计算受限的情况下，深度强化学习（DRL）被探索作为一种强有力的替代方案——这种计算受限的情况在由于机器人具身的复杂性，MoMa任务中很常见。例如，Wang等人[[152](#bib.bib152)]和Fu等人[[154](#bib.bib154)]学习了全身策略，使得带轮移动操作器和带有手臂的四足机器人能够用其末端执行器到达3D空间中的点。Ma等人[[153](#bib.bib153)]学习了一种对随机扭矩扰动具有鲁棒性的运动策略，并使用MPC规划器来控制手臂进行点到达。
- en: 'Typically, whole-body control problems focus on precise control of the end-effector
    without taking into account the agent’s surroundings: the policy takes proprioceptive
    sensing as the observation and tries to minimize the difference to the desired
    configuration. Notably, recent works have explored how to integrate low-level
    whole-body control skills into hierarchical RL architectures [[157](#bib.bib157),
    [158](#bib.bib158), [159](#bib.bib159)], where the higher level perceives the
    surroundings and queries a low-level whole-body skill with the right desired pose
    as the goal. This extends the success of DRL in learning WBC to more complex interactive
    MoMa tasks.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，整个身体控制问题侧重于精确控制末端执行器，而不考虑代理的周围环境：策略将本体感觉作为观察，并尝试将其与期望配置的差异最小化。值得注意的是，最近的研究探讨了如何将低级整体身体控制技能集成到层次化强化学习架构中[[157](#bib.bib157),
    [158](#bib.bib158), [159](#bib.bib159)]，其中较高层次的感知周围环境，并以正确的期望姿态作为目标查询低级整体身体技能。这将深度强化学习在学习整体身体控制方面的成功扩展到了更复杂的交互式MoMa任务中。
- en: 4.4.2 Short-horizon Interactive Tasks
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 短期交互任务
- en: 'Short-horizon interactive tasks often focus on learning specific sensorimotor
    skills that require no memory or planning capabilities. Many works have explored
    applying DRL to these short-horizon tasks, including grasping [[160](#bib.bib160),
    [161](#bib.bib161), [159](#bib.bib159)], ball kicking [[162](#bib.bib162), [158](#bib.bib158)],
    collision-free target tracking [[163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165)],
    interactive navigation [[166](#bib.bib166)], and door opening [[167](#bib.bib167),
    [168](#bib.bib168)]. Notably, Ji et al. [[158](#bib.bib158)] used hierarchical
    RL to learn soccer kicking skills, where a high-level policy generates the desired
    end-effector trajectory executed by a low-level policy. Hu et al. [[163](#bib.bib163)]
    improved the training efficiency by deriving a low-variance policy gradient update
    through action space decomposition. Cheng et al. [[169](#bib.bib169)] learned
    separate skills for locomotion and manipulation on a quadruped in simulation and
    chained different skills using a behavior tree. Ji et al. [[162](#bib.bib162)]
    learned a whole-body dribbling policy in simulation, transferring it zero-shot
    to the real world using extensive domain randomization in visual input and simulation
    parameters. Liu et al. [[159](#bib.bib159)] learned grasping policies via hierarchical
    RL and teacher-student distillation, where an image-based student policy is distilled
    from a state-based teacher policy. Interactive tasks require policies to make
    decisions based on sensor observations of their surroundings. Therefore, the policy
    usually takes in high-dimensional observations such as camera or lidar readings
    (Table [2](#A2.T2 "Table 2 ‣ Appendix B Additional Tables ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")). Meanwhile, these tasks
    often involve hard-to-model dynamics such as contact forces or articulated object
    motion, making model-free RL an appealing alternative both to classical methods
    and to model-based RL (Table [4](#A2.T4 "Table 4 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '短期交互任务通常专注于学习不需要记忆或规划能力的特定感官运动技能。许多研究探讨了将深度强化学习应用于这些短期任务，包括抓取[[160](#bib.bib160),
    [161](#bib.bib161), [159](#bib.bib159)]、踢球[[162](#bib.bib162), [158](#bib.bib158)]、无碰撞目标跟踪[[163](#bib.bib163),
    [164](#bib.bib164), [165](#bib.bib165)]、交互式导航[[166](#bib.bib166)]和开门[[167](#bib.bib167),
    [168](#bib.bib168)]。值得注意的是，Ji等人[[158](#bib.bib158)]使用层次化强化学习学习足球踢球技能，其中高层策略生成由低层策略执行的期望末端执行器轨迹。Hu等人[[163](#bib.bib163)]通过动作空间分解推导出低方差策略梯度更新，从而提高了训练效率。Cheng等人[[169](#bib.bib169)]在模拟中学习了四足动物的运动和操控的独立技能，并使用行为树将不同技能链在一起。Ji等人[[162](#bib.bib162)]在模拟中学习了整体身体运球策略，并通过在视觉输入和模拟参数中进行广泛的领域随机化，将其零样本转移到现实世界。Liu等人[[159](#bib.bib159)]通过层次化强化学习和师生蒸馏学习了抓取策略，其中基于图像的学生策略从基于状态的教师策略中蒸馏出来。交互任务要求策略根据对周围环境的传感器观察做出决策。因此，策略通常需要接受高维观察，如相机或激光雷达读数（表[2](#A2.T2
    "Table 2 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")）。同时，这些任务通常涉及难以建模的动态，如接触力或关节对象运动，使得无模型强化学习成为对经典方法和基于模型的强化学习的有吸引力的替代方案（表[4](#A2.T4
    "Table 4 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")）。'
- en: 4.4.3 Long-horizon Interactive Tasks
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3 长期交互任务
- en: For a mobile manipulator to function in unstructured environments such as offices [[170](#bib.bib170)],
    homes, or kitchens [[171](#bib.bib171)], it needs to handle tasks with long horizons
    and strong partial observability. However, end-to-end RL struggles on long-horizon
    tasks due to the difficulty of exploring the state-action space to find successful
    strategies, requiring many samples to train. Partial observability is also challenging
    for DRL as it requires complex network architectures that can encode observation
    history (e.g., RNNs or LSTMs) or some other mechanism to aggregate observations
    and model the environment (e.g., mapping or 3D reconstruction). One possible way
    to mitigate this issue is to make use of expert demonstrations or simulation data
    to bootstrap the learning process. For instance, Herzog et al. [[170](#bib.bib170)]
    exploited simulation data and scripted policies to speed up the training process
    for off-policy RL in a waste sorting task. Another promising direction is to take
    a divide-and-conquer approach by sequentially chaining short-horizon interactive
    skills through planning [[171](#bib.bib171)] or hierarchical RL [[157](#bib.bib157)].
    Overall, solving long-horizon interactive tasks using DRL is an open challenge
    and under-explored area, but solving this type of task is necessary to create
    truly capable household and human-assistant robots.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使移动操控器在办公室[[170](#bib.bib170)]、家庭或厨房[[171](#bib.bib171)]等非结构化环境中正常工作，它需要处理具有长期视野和强部分可观测性的任务。然而，端到端的强化学习在长期视野任务上表现不佳，因为探索状态-动作空间以找到成功策略是困难的，需要大量样本进行训练。部分可观测性对于深度强化学习也是一个挑战，因为它需要复杂的网络架构来编码观察历史（例如，RNNs或LSTMs）或其他机制来汇总观察并建模环境（例如，映射或3D重建）。一种可能的解决办法是利用专家演示或模拟数据来启动学习过程。例如，Herzog等人[[170](#bib.bib170)]利用模拟数据和脚本策略来加速在废物分类任务中的离线策略强化学习的训练过程。另一种有前途的方向是通过规划[[171](#bib.bib171)]或分层强化学习[[157](#bib.bib157)]将短期交互技能顺序链式连接。总体而言，使用深度强化学习解决长期交互任务是一个开放性挑战和未充分探索的领域，但解决此类任务对于创建真正强大的家庭和人类助理机器人是必要的。
- en: '![Refer to caption](img/4838e59944377098eebc1c85507ff9ab.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4838e59944377098eebc1c85507ff9ab.png)'
- en: '| WBC | [152](#bib.bib152),[153](#bib.bib153),[154](#bib.bib154),[155](#bib.bib155)
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| WBC | [152](#bib.bib152),[153](#bib.bib153),[154](#bib.bib154),[155](#bib.bib155)
    |'
- en: '| Short-Horizon | [158](#bib.bib158),[159](#bib.bib159),[160](#bib.bib160),[161](#bib.bib161),[162](#bib.bib162),[163](#bib.bib163),[164](#bib.bib164),[165](#bib.bib165),[166](#bib.bib166),[167](#bib.bib167),[168](#bib.bib168),[169](#bib.bib169)
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 短期视野 | [158](#bib.bib158),[159](#bib.bib159),[160](#bib.bib160),[161](#bib.bib161),[162](#bib.bib162),[163](#bib.bib163),[164](#bib.bib164),[165](#bib.bib165),[166](#bib.bib166),[167](#bib.bib167),[168](#bib.bib168),[169](#bib.bib169)
    |'
- en: '| Long-Horizon | [157](#bib.bib157),[170](#bib.bib170),[171](#bib.bib171) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 长期视野 | [157](#bib.bib157),[170](#bib.bib170),[171](#bib.bib171) |'
- en: 'Figure 5: Top: An overview of the three MoMa challenges discussed in Sec. [4.4](#S4.SS4
    "4.4 Mobile Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes"), including whole-body control
    [[152](#bib.bib152), [154](#bib.bib154)] (WBC) and short- [[161](#bib.bib161),
    [169](#bib.bib169)] and long-horizon [[157](#bib.bib157), [171](#bib.bib171)]
    interactive tasks; Bottom: MoMa papers reviewed in Sec. [4.4](#S4.SS4 "4.4 Mobile
    Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes").Color map indicates levels of real-world
    success: *Limited Lab*, *Diverse Lab*, *Limited Real*, and *Diverse Real*.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5：顶部：概述了第[4.4](#S4.SS4 "4.4 Mobile Manipulation ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")节中讨论的三个MoMa挑战，包括全身控制
    [[152](#bib.bib152), [154](#bib.bib154)] (WBC) 以及短期 [[161](#bib.bib161), [169](#bib.bib169)]
    和长期 [[157](#bib.bib157), [171](#bib.bib171)] 交互任务；底部：第[4.4](#S4.SS4 "4.4 Mobile
    Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes")节中审查的MoMa论文。颜色图表示现实世界成功的级别：*有限实验室*、*多样实验室*、*有限现实*
    和 *多样现实*。'
- en: 4.4.4 Trends and Open Challenges in Mobile Manipulation
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4 移动操控中的趋势和开放挑战
- en: 'Thanks to the generalization of humanoids and other robot embodiments, and
    the advances in locomotion and stationary manipulation, DRL for MoMa is a growing
    field with increasing research attention. Based on our analysis, we infer some
    trends and open questions. First, compared to stationary manipulation, MoMa tasks
    have a significantly larger workspace, making safe real-world exploration challenging.
    As such, existing works mainly perform training in simulations where safety is
    not a concern (Table [3](#A2.T3 "Table 3 ‣ Appendix B Additional Tables ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes")). In the
    rare occurrences of real-world RL, strong domain knowledge, e.g., in the form
    of motion priors [[168](#bib.bib168), [160](#bib.bib160)] and/or demonstrations [[168](#bib.bib168),
    [170](#bib.bib170)], is used to enable safe and efficient exploration. Plus, MoMa’s
    large workspace demands a more sophisticated form of memory and scene representation.
    Representations that work well for navigation often fail to capture the dynamic
    characters in manipulation. While advances in sample efficiency, memory, and safe
    real-world RL promise new opportunities, scaling them to the open-worldness and
    vast workspaces inherent to MoMa remains challenging.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '由于类人机器人和其他机器人形态的概括，以及在运动和静态操作方面的进展，针对MoMa的深度强化学习（DRL）正成为一个日益增长的研究领域。根据我们的分析，我们推测了一些趋势和未解问题。首先，与静态操作相比，MoMa任务拥有显著更大的工作空间，这使得安全的现实世界探索变得具有挑战性。因此，现有的工作主要在安全性不成问题的仿真中进行训练（表 [3](#A2.T3
    "Table 3 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")）。在现实世界RL的少数情况中，强大的领域知识，例如运动先验[[168](#bib.bib168),
    [160](#bib.bib160)]和/或示范[[168](#bib.bib168), [170](#bib.bib170)]，被用来实现安全和高效的探索。此外，MoMa的大工作空间要求更复杂的记忆和场景表示。对于导航效果良好的表示往往无法捕捉操作中的动态特征。尽管样本效率、记忆和安全现实世界RL的进展带来了新的机遇，但将它们扩展到MoMa固有的开放世界性和广阔工作空间仍然具有挑战性。'
- en: Second, mobile manipulators have very diverse morphologies compared to other
    types of robots, including wheeled robots with arms [[170](#bib.bib170), [163](#bib.bib163),
    [167](#bib.bib167), [164](#bib.bib164), [152](#bib.bib152), [160](#bib.bib160),
    [161](#bib.bib161), [171](#bib.bib171), [168](#bib.bib168)], quadrupeds with arms [[153](#bib.bib153),
    [154](#bib.bib154), [159](#bib.bib159), [157](#bib.bib157)], humanoids [[155](#bib.bib155)],
    and even quadrupeds using their legs for both locomotion and manipulation, i.e.,
    loco-manipulation [[158](#bib.bib158), [162](#bib.bib162), [169](#bib.bib169),
    [166](#bib.bib166)]. Each morphology brings unique challenges and opportunities.
    For example, wheeled mobile manipulators are easier to model and generally more
    kinematically stable, facilitating learning only for the manipulation component,
    while legged mobile manipulators can traverse uneven terrains but are harder to
    control, even for simple navigation phases. New research in both morphology-agnostic
    and morphology-specific RL methods is necessary for MoMa.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，与其他类型的机器人相比，移动操作机器人具有非常多样的形态，包括带有机械臂的轮式机器人[[170](#bib.bib170), [163](#bib.bib163),
    [167](#bib.bib167), [164](#bib.bib164), [152](#bib.bib152), [160](#bib.bib160),
    [161](#bib.bib161), [171](#bib.bib171), [168](#bib.bib168)]，带有机械臂的四足机器人[[153](#bib.bib153),
    [154](#bib.bib154), [159](#bib.bib159), [157](#bib.bib157)]，类人机器人[[155](#bib.bib155)]，甚至使用腿部进行运动和操作的四足机器人，即运动-操作机器人[[158](#bib.bib158),
    [162](#bib.bib162), [169](#bib.bib169), [166](#bib.bib166)]。每种形态都带来了独特的挑战和机遇。例如，轮式移动操作机器人更易于建模，通常在运动学上更稳定，仅有操作组件的学习，而腿式移动操作机器人能够穿越不平坦的地形，但即使在简单的导航阶段也更难控制。对于MoMa，形态无关和形态特定的RL方法的新研究是必要的。
- en: 'Third, perhaps due to the diverse morphologies, very diverse choices of action
    spaces are observed in the MoMa literature (Table [1](#A2.T1 "Table 1 ‣ Appendix
    B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes")), including direct joint control [[163](#bib.bib163), [41](#bib.bib41),
    [167](#bib.bib167)], task-space control with classical model-based [[164](#bib.bib164),
    [161](#bib.bib161)], task-space control with learned low-level controllers [[169](#bib.bib169),
    [158](#bib.bib158), [157](#bib.bib157)], and even factored actions that only controls
    a part of the embodiment [[153](#bib.bib153), [164](#bib.bib164)]. Choosing the
    right action space is crucial for performance, as it affects the temporal abstraction
    levels and robot controllability. Yet, there is currently no principled way to
    select the appropriate action space for the diverse set of MoMa tasks. {summary}
    [Key Takeaways]'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '第三，由于形态的多样性，MoMa 文献中观察到非常多样化的动作空间选择（表格 [1](#A2.T1 "Table 1 ‣ Appendix B Additional
    Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")），包括直接关节控制 [[163](#bib.bib163),
    [41](#bib.bib41), [167](#bib.bib167)]，经典模型基的任务空间控制 [[164](#bib.bib164), [161](#bib.bib161)]，基于学习的低级控制器的任务空间控制 [[169](#bib.bib169),
    [158](#bib.bib158), [157](#bib.bib157)]，甚至是仅控制部分体现的分解动作 [[153](#bib.bib153), [164](#bib.bib164)]。选择合适的动作空间对性能至关重要，因为它影响时间抽象水平和机器人可控性。然而，目前没有原则性的方法来选择适合多样化
    MoMa 任务的动作空间。{summary} [关键要点]'
- en: •
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL has achieved initial success in mobile manipulation, in particular on short-horizon
    tasks, especially by leveraging training in simulation.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL 在移动操作方面取得了初步成功，尤其是在短期任务上，特别是通过模拟训练。
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Defining a suitable action space is critical for RL in MoMa, especially given
    the diversity in the morphologies of existing MoMa systems.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 MoMa 中定义合适的动作空间对于 RL 至关重要，特别是考虑到现有 MoMa 系统的形态多样性。
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The successes notwithstanding, existing methods are still insufficient for tackling
    multi-tasking, representing long-term memory, and performing safe exploration
    in the real world, providing opportunities for future improvements.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管取得了一些成功，但现有方法仍不足以应对多任务处理、表示长期记忆和在现实世界中进行安全探索，提供了未来改进的机会。
- en: 4.5 Human-Robot Interaction
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 人机交互
- en: 'In this subsection, we review works where DRL has been applied to human-robot
    interaction (HRI)—on robotic systems for use by or with humans. While HRI tasks
    can have varying objectives and involve robots with distinct morphology, the presence
    of humans introduces shared challenges, including safety, interpretability, and
    human modeling, that distinguish HRI from other robot problems not involving humans.
    Notice that this section focuses on robotic systems with HRI competencies (i.e.,
    interact with humans during task execution), whereas works that only involve humans
    during training are out of the scope of this section. HRI tasks can be broadly
    classified into three main categories: collaborative physical HRI (pHRI), where
    the robot and humans physically collaborate with a shared objective; non-collaborative
    pHRI, where the robot and humans share the same physical space but have distinct
    objectives; and shared autonomy, where humans act as teleoperators, and the robot
    autonomously interprets and executes the teleoperation command. In this section,
    we review works from these three categories. Figure [6](#S4.F6 "Figure 6 ‣ 4.5.2
    Non-collaborative pHRI ‣ 4.5 Human-Robot Interaction ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    summarizes the papers reviewed.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节中，我们回顾了将 DRL 应用于人机交互（HRI）的研究——即用于或与人类一起使用的机器人系统。尽管 HRI 任务可能有不同的目标并涉及具有不同形态的机器人，但人类的存在引入了共享的挑战，包括安全性、可解释性和人类建模，这些挑战使
    HRI 与其他不涉及人类的机器人问题有所不同。请注意，本节专注于具有 HRI 能力的机器人系统（即在任务执行过程中与人类互动），而仅在训练过程中涉及人类的工作超出了本节的范围。HRI
    任务可以大致分为三类：协作物理 HRI (pHRI)，其中机器人和人类在共享目标下进行物理协作；非协作 pHRI，其中机器人和人类共享同一物理空间但具有不同的目标；以及共享自主性，人类作为遥控操作员，机器人自主解释并执行遥控操作命令。在本节中，我们回顾了这三类工作的文献。图 [6](#S4.F6
    "Figure 6 ‣ 4.5.2 Non-collaborative pHRI ‣ 4.5 Human-Robot Interaction ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    总结了所回顾的论文。'
- en: 4.5.1 Collaborative pHRI
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1 协作 pHRI
- en: 'The most intuitive type of HRI arises when a robot and a human physically collaborate
    toward accomplishing a shared goal—a common theme for service robots that assist
    humans in household activities. For example, Ghadirzadeh et al. [[172](#bib.bib172)]
    tackled the collective packaging task, where recurrent Q-learning is combined
    with a behavior tree to minimize the packaging time of a human worker. Christen
    et al. [[173](#bib.bib173), [174](#bib.bib174)] focused on object hand-over from
    a human to a robot, using RL to learn a simulated human hand-over policy and a
    robot policy to grasp the objects handed over by the human. Noticeably, existing
    works for collaborative pHRI share a similar procedure: learning a human model
    from pre-collected data to train a robot policy in simulation. This similarity
    is likely due to the high cost of collecting online interactions for collaborative
    tasks, which require continuous human attention and physical response to the robot’s
    behavior.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观的 HRI 类型出现在机器人和人类在实现共同目标时进行物理协作的情况下——这是服务机器人帮助人类进行家庭活动的共同主题。例如，Ghadirzadeh
    等人 [[172](#bib.bib172)] 处理了集体包装任务，其中将递归 Q 学习与行为树相结合，以最小化人类工人的包装时间。Christen 等人
    [[173](#bib.bib173), [174](#bib.bib174)] 关注从人类到机器人物体传递的过程，使用 RL 学习模拟的人类传递策略和机器人策略来抓取人类传递的物体。值得注意的是，现有的协作性
    pHRI 工作有类似的程序：从预先收集的数据中学习人类模型，以在模拟中训练机器人策略。这种相似性可能是由于收集协作任务的在线交互的高成本，这些任务需要持续的人类关注和对机器人物理行为的反应。
- en: 4.5.2 Non-collaborative pHRI
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2 非协作性 pHRI
- en: In non-collaborative pHRI tasks, a robot operates alongside humans in the same
    physical space but with different objectives. A representative example is social
    navigation， where a robot navigates through crowded environments. Chen et al.
    [[175](#bib.bib175)] trained a robot for social navigation in simulation, where
    a hand-crafted reward is used to encourage socially compliant behavior, and zero-shot
    transferred the policy to a real-world corridor. Everett et al. [[176](#bib.bib176)]
    expanded on this work to incorporate human motion histories into decision-making
    by modeling the value network with an LSTM. Liang et al. [[177](#bib.bib177)]
    developed a high-fidelity simulator of human motions to train navigation policies
    taking lidar scans as inputs, and demonstrated reliable sim-to-real transfer capabilities.
    Hirose et al. [[178](#bib.bib178)] learned navigation policies alongside humans
    in the real world. A residual Q-function is learned on top of an offline pre-trained
    Q-function to generate adaptive behavior on the fly. Unlike collaborative tasks,
    humans do not actively participate in the robot’s activities in non-collaborative
    tasks, making it easier to hard-code human behaviors [[175](#bib.bib175), [176](#bib.bib176),
    [177](#bib.bib177)] or train in the real world [[178](#bib.bib178)], resulting
    in successful real-world implementations. Aside from social navigation, Liu et
    al. [[179](#bib.bib179)] considered manipulation while avoiding collision with
    humans, where an action space transformation is conducted to ensure safe exploration
    in RL.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在非协作性 pHRI 任务中，机器人在人类的物理空间内操作，但目标不同。一个典型的例子是社交导航，在这种情况下，机器人在拥挤的环境中导航。Chen 等人
    [[175](#bib.bib175)] 在模拟中训练了一个用于社交导航的机器人，通过手工设计的奖励来鼓励社会行为，并将策略零-shot 转移到现实世界的走廊中。Everett
    等人 [[176](#bib.bib176)] 在此基础上进行了扩展，通过使用 LSTM 对价值网络建模，将人类运动历史纳入决策制定。Liang 等人 [[177](#bib.bib177)]
    开发了一个高保真度的人体运动模拟器，以 lidar 扫描作为输入训练导航策略，并展示了可靠的仿真到现实转移能力。Hirose 等人 [[178](#bib.bib178)]
    在现实世界中与人类一起学习导航策略。在离线预训练的 Q 函数基础上学习一个残差 Q 函数，以实时生成自适应行为。与协作任务不同，在非协作任务中，人类不会主动参与机器人的活动，这使得更容易硬编码人类行为
    [[175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)] 或在现实世界中进行训练 [[178](#bib.bib178)]，从而实现成功的现实世界应用。除了社交导航，Liu
    等人 [[179](#bib.bib179)] 考虑了在避免与人类碰撞的情况下进行操作，其中进行动作空间转换以确保在 RL 中的安全探索。
- en: '![Refer to caption](img/c87b59339bda5341203f62e5eae394dc.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c87b59339bda5341203f62e5eae394dc.png)'
- en: '| Collaborative pHRI | [173](#bib.bib173), [172](#bib.bib172), [174](#bib.bib174),
    [180](#bib.bib180) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 协作性 pHRI | [173](#bib.bib173), [172](#bib.bib172), [174](#bib.bib174), [180](#bib.bib180)
    |'
- en: '| Non-collaborative pHRI | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178), [179](#bib.bib179) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 非协作性 pHRI | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178),
    [179](#bib.bib179) |'
- en: '| Shared Autonomy | [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 共享自主性 | [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183) |'
- en: 'Figure 6: Top: An overview of the three types of HRI tasks discussed in Sec. [4.5](#S4.SS5
    "4.5 Human-Robot Interaction ‣ 4 Competency-Specific Review ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes"), including collaborative
    [[173](#bib.bib173)] and non-collaborative [[175](#bib.bib175)] pHRI tasks, and
    shared autonomy [[182](#bib.bib182)]; Bottom: Papers reviewed in Sec. [4.5](#S4.SS5
    "4.5 Human-Robot Interaction ‣ 4 Competency-Specific Review ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes"). The color map indicates
    the levels of real-world success: *Sim Only*, *Limited Lab*, *Diverse Lab*, and
    *Limited Real*.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '图6：顶部：概述了第[4.5](#S4.SS5 "4.5 Human-Robot Interaction ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")节中讨论的三种HRI任务类型，包括协作[[173](#bib.bib173)]和非协作[[175](#bib.bib175)]
    pHRI任务，以及共享自主性[[182](#bib.bib182)]；底部：第[4.5](#S4.SS5 "4.5 Human-Robot Interaction
    ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for Robotics: A Survey
    of Real-World Successes")节中回顾的论文。色彩图示了现实世界成功的水平：*仅模拟*、*有限实验室*、*多样实验室*和*有限现实*。'
- en: 4.5.3 Shared Autonomy
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.3 共享自主性
- en: Shared autonomy is an HRI paradigm that does not involve physical contact between
    humans and robots. Instead, the robot takes actions to complete tasks based on
    human instructions such as keyboard control or language commands. In this setting,
    RL can be used to learn a policy that conditions on human inputs and generates
    robot actions that optimize some external task rewards or constraints while aligning
    with the user instructions. For instance, Reddy et al. [[182](#bib.bib182)] tackled
    the quadrotor perching task, where a Q-function is learned based on task reward,
    and the robot chooses actions that are close to the user input and above a preset
    task value threshold. Schaff et al. [[183](#bib.bib183)] formulated shared autonomy
    for simulated quadrotor control as a constrained optimization problem, where a
    residual RL policy is learned to minimally change the human input policy while
    satisfying a set of task-invariant constraints. More recently, advances in NLP
    have opened up the possibility for shared autonomy through natural language instructions.
    For example, Nair et al. [[181](#bib.bib181)] learned a language-conditioned policy
    for table-top manipulation using model-based RL on a pre-collected dataset with
    hand-labeled language instructions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 共享自主性是一种HRI范式，不涉及人类与机器人之间的物理接触。相反，机器人根据人类的指令，如键盘控制或语言命令，采取行动以完成任务。在这种设置下，RL可以用来学习一个依赖于人类输入的策略，并生成优化一些外部任务奖励或约束的机器人动作，同时与用户指令对齐。例如，Reddy等人[[182](#bib.bib182)]解决了四旋翼着陆任务，其中基于任务奖励学习了一个Q函数，机器人选择接近用户输入并高于预设任务值阈值的动作。Schaff等人[[183](#bib.bib183)]将模拟四旋翼控制的共享自主性公式化为一个受限优化问题，其中学习到的残差RL策略最小化地改变了人类输入策略，同时满足一组任务不变约束。最近，NLP的进展使得通过自然语言指令实现共享自主性的可能性变得更加广阔。例如，Nair等人[[181](#bib.bib181)]使用基于模型的RL在一个预先收集的带有手工标注语言指令的数据集上学习了一个语言条件的策略，用于桌面操作。
- en: 4.5.4 Trends and Open Challenges in HRI
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.4 HRI中的趋势与开放挑战
- en: Despite the importance of HRI for household robot applications, RL has seen
    fewer successes in HRI compared to other robotics domains like locomotion and
    manipulation. A primary challenge for applying RL to HRI problems is properly
    incorporating human or human-like priors into the training process, which can
    often be non-markovian, have limited rationality, and are often costly to collect.
    Existing works have primarily tackled this challenge in three ways. First, a straightforward
    approach is to train the policies directly in real-world environments alongside
    humans. However, this approach presents significant challenges to the sample complexity
    of the algorithm since collecting real-world interaction data is costly, especially
    when humans are actively involved. As such, works using this approach either focus
    on simple tasks [[180](#bib.bib180)] or rely on pretraining to derive a good initial
    policy and reduce sample complexity [[178](#bib.bib178)]. Second, an alternative
    to avoid costly real-world learning is to learn a reasonable human model to simulate
    humans during training. This approach is particularly appealing in domains where
    human actions are fairly easy to model, such as shared autonomy, where a human
    policy can be learned by imitating a set of human actors [[183](#bib.bib183),
    [182](#bib.bib182)]. In tasks where human actions are more complex, human models
    have been created using motion capture [[172](#bib.bib172), [179](#bib.bib179)],
    crowd-sourcing [[181](#bib.bib181)], and RL [[173](#bib.bib173)]. Third, when
    human behaviors are simple, human models can be directly hardcoded using domain
    knowledge [[175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)], and be
    incorporated either as parts of the simulation or as behavioral constraints. Although
    this approach is not scalable and inapplicable for many tasks, these simplified
    human models can serve as a useful source for pretraining to improve sample efficiency
    for real-world learning.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管家庭机器人应用中人机交互（HRI）的重要性不容忽视，但与其他机器人领域如移动和操作相比，强化学习（RL）在HRI中的成功案例较少。将RL应用于HRI问题的主要挑战是如何将人类或类人先验知识有效地融入训练过程中，这些先验知识通常是非马尔可夫的，理性有限，并且采集成本较高。现有的研究主要通过三种方式解决这一挑战。首先，一种直接的方法是与人类一起在真实环境中训练策略。然而，这种方法对算法的样本复杂度提出了显著挑战，因为采集真实环境中的交互数据成本高，特别是当人类积极参与时。因此，使用这种方法的研究要么专注于简单任务[[180](#bib.bib180)]，要么依赖于预训练以推导出良好的初始策略并减少样本复杂度[[178](#bib.bib178)]。其次，避免昂贵的真实世界学习的替代方案是学习一个合理的人类模型，以在训练过程中模拟人类。这种方法在那些人类行为较容易建模的领域尤其具有吸引力，如共享自主，其中可以通过模仿一组人类演员来学习人类策略[[183](#bib.bib183),
    [182](#bib.bib182)]。在那些人类行为较复杂的任务中，人类模型已经通过运动捕捉[[172](#bib.bib172), [179](#bib.bib179)]、众包[[181](#bib.bib181)]和RL[[173](#bib.bib173)]等方式创建。第三，当人类行为简单时，人类模型可以通过领域知识直接硬编码[[175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177)]，并作为仿真的一部分或行为约束加以融入。虽然这种方法不具备扩展性，并且不适用于许多任务，但这些简化的人类模型可以作为预训练的有用来源，以提高真实世界学习的样本效率。
- en: 'Overall, two promising future directions emerge: first, developing safe and
    sample-efficient RL algorithms to enable direct real-world RL, possibly by leveraging
    known human behavior models; second, building high-fidelity human behavior simulation
    to bridge sim-to-real gaps for zero-shot sim-to-real transfer. Future advances
    in these directions promise to broaden the application of RL to HRI problems significantly.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，有两个有前景的未来方向：首先，开发安全且样本高效的RL算法，以实现直接的真实世界RL，可能通过利用已知的人类行为模型；其次，构建高保真度的人类行为仿真，以弥合模拟到真实的差距，实现零样本模拟到真实的迁移。这些方向的未来进展有望显著拓宽RL在HRI问题上的应用。
- en: '{summary}'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '{总结}'
- en: '[Key Takeaways]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[关键要点]'
- en: •
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compared to other robotics domains, DRL has achieved limited success in HRI,
    especially on tasks that require the robot to collaborate with humans physically.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与其他机器人领域相比，深度强化学习（DRL）在HRI中取得的成功有限，特别是在需要机器人与人类进行物理合作的任务中。
- en: •
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A key challenge for applying RL to HRI lies in collecting realistic interactive
    experiences with humans, which can, in principle, be obtained by either directly
    training in the real world or by building high-fidelity human models for simulations.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将RL应用于HRI的关键挑战在于收集与人类的真实交互经验，这些经验原则上可以通过在真实世界中直接训练或通过建立高保真度的人类模型进行仿真来获得。
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Existing works have explored both approaches in simple tasks. However, whether
    and how we can scale up these approaches to more difficult tasks remains unclear.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有的研究已经在简单任务中探索了这两种方法。然而，这些方法是否以及如何扩展到更复杂的任务仍然不清楚。
- en: 4.6 Multi-Robot Interaction
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 多机器人交互
- en: 'Multi-robot interaction is often solved as a MARL problem, which, in the most
    general case, is described using a partially observable stochastic game (POSG)
    with distinct reward functions and action and observation spaces, although most
    cooperative real-world problems model the problem as Decentralized POMDPs. We
    highlight three real-world domains where DRL has been successfully applied to
    learn multi-robot interaction: collision avoidance and navigation, multi-agent
    loco-manipulation, and robot soccer.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 多机器人交互通常作为一个MARL问题来解决，在最一般的情况下，这种问题使用带有不同奖励函数以及动作和观察空间的部分可观察随机游戏（POSG）来描述，尽管大多数合作性的实际问题将问题建模为去中心化POMDPs。我们突出了三个实际领域，其中DRL已成功应用于学习多机器人交互：碰撞避免与导航、多智能体运动操控和机器人足球。
- en: 4.6.1 Multi-Agent Collision Avoidance
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1 多智能体碰撞避免
- en: Chen et al. [[184](#bib.bib184)] and Everett et al. [[185](#bib.bib185)] model
    a Dec-MDP in which the policy takes the state vector consisting of positions,
    velocities, and radii of all the robots as input to predict the velocities for
    each robot. The policy is preconditioned via finetuning using ORCA [[186](#bib.bib186)].
    The reward function is sparse, consisting of a goal-reaching reward and collision
    penalties. These works successfully developed collision-avoidance policies in
    simulation and showcased hardware results on aerial and ground robots. The multirotors
    used onboard sensors and controllers to execute maneuvers suggested by the policy.
    The ground robot, equipped with affordable onboard sensors (under $1000$ USD),
    was able to navigate through pedestrian traffic, effectively avoiding collisions
    despite imperfect perception and diverse pedestrian behaviors unseen during training.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[184](#bib.bib184)]和埃弗雷特等人[[185](#bib.bib185)]建模了一个Dec-MDP，其中策略以所有机器人的位置、速度和半径组成的状态向量作为输入，以预测每个机器人的速度。该策略通过使用ORCA[[186](#bib.bib186)]进行微调来预处理。奖励函数是稀疏的，由一个目标到达奖励和碰撞惩罚组成。这些研究成功开发了模拟中的碰撞避免策略，并展示了空中和地面机器人上的硬件结果。使用的多旋翼无人机利用机载传感器和控制器来执行策略建议的机动。配备了价格实惠的机载传感器（低于$1000$美元）的地面机器人能够在行人交通中导航，尽管感知不完美且行人行为多样化，但仍能有效避免碰撞。
- en: '![Refer to caption](img/40ef7a6f713e558e2180840e856b8a42.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/40ef7a6f713e558e2180840e856b8a42.png)'
- en: '| Multi-Robot Collision Avoidance | [184](#bib.bib184), [185](#bib.bib185),
    [187](#bib.bib187), [188](#bib.bib188), [189](#bib.bib189) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 多机器人碰撞避免 | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189) |'
- en: '| Multi-Robot Loco-Manipulation | [190](#bib.bib190) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 多机器人运动操控 | [190](#bib.bib190) |'
- en: '| Robot Soccer | [191](#bib.bib191) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 机器人足球 | [191](#bib.bib191) |'
- en: 'Figure 7: Top: An overview of the three representative multi-robot interaction
    domains reviewed in Sec. [4.6](#S4.SS6 "4.6 Multi-Robot Interaction ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes"),
    including multi-robot collision avoidance [[187](#bib.bib187)], multi-robot manipulation
    via locomotion [[190](#bib.bib190)], and robot soccer [[191](#bib.bib191)]; Bottom:
    Multi-robot interaction papers reviewed in Sec. [4.6](#S4.SS6 "4.6 Multi-Robot
    Interaction ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes"). See the caption of Fig. [2](#S4.F2 "Figure
    2 ‣ 4.1.1 Quadruped Locomotion ‣ 4.1 Locomotion ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    for color map description.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：上方：第[4.6](#S4.SS6 "4.6 多机器人交互 ‣ 4 能力特定评审 ‣ 深度强化学习在机器人中的实际成功")节中回顾的三种代表性的多机器人交互领域的概述，包括多机器人碰撞避免[[187](#bib.bib187)]、通过运动操控的多机器人操控[[190](#bib.bib190)]和机器人足球[[191](#bib.bib191)]；下方：第[4.6](#S4.SS6
    "4.6 多机器人交互 ‣ 4 能力特定评审 ‣ 深度强化学习在机器人中的实际成功")节中回顾的多机器人交互论文。有关色彩图描述，请参见图[2](#S4.F2
    "图 2 ‣ 4.1.1 四足机器人运动 ‣ 4.1 运动 ‣ 4 能力特定评审 ‣ 深度强化学习在机器人中的实际成功")的说明。
- en: Other works [[187](#bib.bib187), [188](#bib.bib188)] have also modeled the problem
    as a Dec-MDP with the objective of time-to-goal minimization. These methods differ
    from the previous approaches in multiple respects. First, the policy takes raw
    lidar scans as input instead of the states of the other agents and thus does not
    depend on precise sensing and perception. Second, they do not precondition or
    finetune the policy using ORCA but instead employ curriculum learning and a dense
    reward function to facilitate training. Third, to deal with more complex multi-agent
    scenarios, it utilizes a hybrid controller to swap out the learned policy with
    a classical controller instead of restricting the other robots’ motion via constant
    linear velocity models.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究[[187](#bib.bib187), [188](#bib.bib188)]也将问题建模为Dec-MDP，目标是最小化时间到达目标。这些方法在多个方面与之前的方法不同。首先，策略以原始激光雷达扫描作为输入，而不是其他智能体的状态，因此不依赖于精确的感知和感测。其次，它们不使用ORCA来预处理或微调策略，而是采用课程学习和密集奖励函数来促进训练。第三，为了处理更复杂的多智能体场景，它利用混合控制器将学习到的策略与经典控制器互换，而不是通过常量线速度模型来限制其他机器人运动。
- en: Finally, Sartoretti et al. [[189](#bib.bib189)] used DRL to prevent agents from
    blocking each other in multi-agent pathfinding problems. A “blocking penalty”
    is applied when an agent reaches its goal but prevents another agent from doing
    the same. This strategy, combined with imitation learning and environment sampling,
    expedites convergence. The algorithm was tested on a small fleet of autonomous
    ground vehicles in a factory floor mock-up.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Sartoretti等人[[189](#bib.bib189)]使用DRL来防止智能体在多智能体路径规划问题中相互阻塞。当一个智能体达到其目标但阻碍了另一个智能体达到目标时，会施加“阻塞惩罚”。这种策略结合模仿学习和环境采样，加快了收敛速度。该算法在工厂车间模型中的一小队自主地面车辆上进行了测试。
- en: 4.6.2 Multi-Agent Loco-Manipulation
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.2 多智能体运动操控
- en: We highlight a recent result [[190](#bib.bib190)] in multi-agent manipulation
    via locomotion (i.e., loco-manipulation). This involves multiple robots using
    movement to manipulate objects or interact with environments. Nachum et al. [[190](#bib.bib190)]
    focus on enabling multiple quadrupeds to perform complex tasks like manipulation
    and coordination using model-free RL. A significant challenge in applying RL to
    coordination or manipulation tasks with multiple legged robots is the complexity
    of interactions between agents or between agents and objects, which usually requires
    extensive real-world trial-and-error learning. To address this, this work employs
    a hierarchical sim2real approach demonstrating zero-shot sim-to-real transfer
    for object avoidance and targeted object pushing. Additionally, the work showcases
    a multi-agent scenario where two quadrupeds coordinate to move a heavy block to
    a specified location and orientation, illustrating the potential of using locomotion
    for coordinated multi-agent manipulation.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调了最近在通过运动进行的多智能体操控中的一项成果[[190](#bib.bib190)]。这涉及多个机器人使用运动来操控物体或与环境互动。Nachum等人[[190](#bib.bib190)]专注于使多个四足机器人利用无模型强化学习执行复杂任务，如操控和协调。将强化学习应用于多足机器人协调或操控任务中的一个重大挑战是智能体之间或智能体与物体之间交互的复杂性，这通常需要大量的现实世界试错学习。为了解决这一问题，该研究采用了分层的sim2real方法，展示了零-shot
    sim-to-real转移在物体避让和目标物体推动中的应用。此外，该研究展示了一个多智能体场景，其中两个四足机器人协调将一个重块移动到指定的位置和方向，展示了使用运动进行协调多智能体操控的潜力。
- en: 4.6.3 Robot Soccer
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.3 机器人足球
- en: 'RL has also been successful in real physical soccer-playing robots in the RoboCup
    Standard Platform League. Many of these works focus on training a policy for a
    single robot, which is then transferred to multiple robots. See Sec. [4.1](#S4.SS1
    "4.1 Locomotion ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes") and Sec. [4.4](#S4.SS4 "4.4 Mobile
    Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes") for discussions on these works focusing
    on single-robot competencies for robot soccer. A recent work [[191](#bib.bib191)]
    further applied RL to learn a variety of dynamic and complex movement skills like
    walking, turning, kicking, and rapid recovery from falls in *1v1 robot soccer
    play*. The agents learn to apply skills appropriately via self-play and showcase
    sophisticated multi-agent competencies such as opponent interception.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 'RL 在 RoboCup 标准平台联赛中的真实物理足球机器人中也取得了成功。许多这些研究集中在为单个机器人训练策略，然后将其转移到多个机器人。有关这些研究的讨论，请参见第
    [4.1](#S4.SS1 "4.1 Locomotion ‣ 4 Competency-Specific Review ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes") 节和第 [4.4](#S4.SS4 "4.4
    Mobile Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes") 节，重点讨论了机器人足球中的单机器人能力。最近的研究 [[191](#bib.bib191)]
    进一步将 RL 应用于学习多种动态和复杂的运动技能，如走路、转弯、踢球和在 *1v1 机器人足球比赛* 中快速恢复。代理通过自我对抗学习适当应用技能，并展示了复杂的多代理能力，例如拦截对手。'
- en: 4.6.4 Trends and Open Challenges in Multi-Robot Interaction
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.4 多机器人互动中的趋势和开放挑战
- en: One of the most significant challenges in multi-agent systems is managing the
    complexity and scalability of the systems as the number of agents increases. This
    challenge is evident in multi-agent manipulation via locomotion and robot soccer,
    where the increase in team size exponentially escalates the complexity of the
    interactions. The transition from controlled, simulated environments to unpredictable
    real-world conditions remains a formidable challenge. Although promising results
    have been shown in domains like collision avoidance, the variability in real-world
    dynamics, such as sensor inaccuracies, unexpected obstacles, and dynamic human
    interactions, often degrades system performance. Next, while RL has provided impressive
    results in learning complex behaviors autonomously, integrating these learned
    behaviors with classical control methods is an increasingly popular area of research.
    Finally, the ability of multi-robot systems to generalize across different tasks
    and environmental conditions presents a substantial opportunity for research.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理系统中的一个重大挑战是随着代理数量的增加，系统的复杂性和可扩展性管理。这一挑战在通过运动和机器人足球进行的多代理操作中尤为明显，其中团队规模的增加会使互动的复杂性呈指数级上升。从受控的模拟环境到不可预测的现实世界条件的过渡仍然是一个艰巨的挑战。尽管在碰撞避免等领域显示了有希望的结果，但现实世界动态中的变异，如传感器不准确、意外障碍物和动态人机互动，通常会降低系统性能。接下来，虽然
    RL 在自主学习复杂行为方面取得了令人印象深刻的成果，但将这些学习到的行为与经典控制方法结合起来正成为一个越来越受欢迎的研究领域。最后，多机器人系统在不同任务和环境条件下进行泛化的能力为研究提供了实质性的机会。
- en: '{summary}'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '{summary}'
- en: '[Key Takeaways]'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[关键要点]'
- en: •
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Current state-of-the-art in RL-based multi-robot interaction is limited to cooperative
    settings with identical reward functions, action spaces, and observation spaces.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前基于 RL 的多机器人互动的最先进技术仅限于具有相同奖励函数、动作空间和观察空间的合作环境。
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Predominantly, DRL in multi-robot settings is applied to collision avoidance
    among ground robots (as compared to manipulation via locomotion and robot soccer).
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在多机器人环境中，DRL 主要应用于地面机器人之间的碰撞避免（与通过运动和机器人足球的操作相比）。
- en: •
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Critical research areas moving forward include dealing with $(i)$ communication
    and networking between agents, $(ii)$ convergence and stability, $(iii)$ scalability,
    $(iv)$ general non-cooperative settings, $(v)$ different robot morphologies and
    applications.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来的关键研究领域包括处理 $(i)$ 代理之间的通信和网络， $(ii)$ 收敛性和稳定性， $(iii)$ 可扩展性， $(iv)$ 一般非合作环境，
    $(v)$ 不同的机器人形态和应用。
- en: 5 General Trends and Open Challenges
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个总体趋势和开放挑战
- en: 'We conclude this survey by summarizing the patterns behind current real-world
    successes in robotics achieved with DRL and the characteristics of those less
    successful cases. Overall, more mature solutions (i.e., L3-4) have often followed
    the zero-shot sim-to-real transfer scheme (Table [3](#A2.T3 "Table 3 ‣ Appendix
    B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes")), which works particularly well for locomotion and navigation. The
    dynamics involved in these competencies, especially terrestrial locomotion and
    navigation, are relatively stable and easy to simulate. Dense and shaped rewards,
    which simplify exploration and improve sample efficiency, have also been effective
    (Table [2](#A2.T2 "Table 2 ‣ Appendix B Additional Tables ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")), leading to the predominant
    use of stable and robust model-free, on-policy algorithms in these domains (Table [5](#A2.T5
    "Table 5 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")). The sim-to-real scheme has been successful
    for manipulation problems in which dense reward functions can be designed a priori
    (e.g., grasping, assembly, in-hand, non-prehensile manipulation), but less so
    in tasks with more diversity (e.g., pick-and-place). The community has been striving
    to explore alternative solutions that do not require simulation (Table [3](#A2.T3
    "Table 3 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")) or reward shaping (Table [4](#A2.T4 "Table
    4 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A
    Survey of Real-World Successes")) and adopt policy optimization algorithms with
    better sample efficiency (Table [5](#A2.T5 "Table 5 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")).
    Human demonstrations (Table [4](#A2.T4 "Table 4 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes"))
    are effective for enabling real-world learning, particularly in manipulation tasks
    that are not prohibitively complex to demonstrate. For competencies where both
    accurate simulation and real-world rollouts are prohibitive (e.g., HRI) or where
    stable, scalable RL algorithms are missing (e.g., multi-robot interaction), successful
    real-world examples are much sparser. In the remainder of this section, we identify
    several concrete open challenges that are opportunities for further extending
    DRL’s applications, in particular for those currently less successful domains.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过总结当前在机器人技术中通过深度强化学习（DRL）实现的现实世界成功模式以及那些不太成功案例的特点来结束这项调查。总体而言，更成熟的解决方案（即L3-4）通常遵循零样本的模拟到现实转移方案（表[3](#A2.T3
    "Table 3 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")），该方案对于运动和导航特别有效。这些能力中涉及的动态，特别是陆地运动和导航，相对稳定且容易模拟。密集和有形奖励，简化了探索并提高了样本效率，也很有效（表[2](#A2.T2
    "Table 2 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")），这导致了这些领域主要使用稳定且强大的无模型、在线算法（表[5](#A2.T5 "Table
    5 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A
    Survey of Real-World Successes")）。模拟到现实方案在可以事先设计密集奖励函数的操作问题中取得了成功（例如，抓取、组装、手中操作、非抓取操作），但在多样性更高的任务中（例如，挑选和放置）效果较差。社区一直在努力探索不需要模拟（表[3](#A2.T3
    "Table 3 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")）或奖励塑造（表[4](#A2.T4 "Table 4 ‣ Appendix B Additional
    Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")）的替代解决方案，并采用具有更好样本效率的策略优化算法（表[5](#A2.T5
    "Table 5 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")）。人类演示（表[4](#A2.T4 "Table 4 ‣ Appendix B Additional
    Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")）在实现现实世界学习方面有效，特别是在那些展示起来并不过于复杂的操作任务中。对于那些准确模拟和现实世界执行都不可行的能力（例如，人机交互）或缺乏稳定、可扩展的强化学习算法的领域（例如，多机器人互动），成功的现实世界案例要稀少得多。在本节剩余部分，我们将确定若干具体的开放挑战，这些挑战为进一步扩展深度强化学习应用提供了机会，特别是对于那些当前不太成功的领域。'
- en: Improving Stability and Sample-Efficiency in RL Algorithms.
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提高强化学习算法的稳定性和样本效率。
- en: While on-policy RL methods are often preferred due to their robustness to hyperparameters,
    collecting large amounts of on-policy data can be prohibitive, especially for
    real-world RL. Even in the predominant zero-shot sim-to-real setting, the sample
    efficiency of on-policy RL is problematic for tasks such as long-horizon mobile
    manipulation [[170](#bib.bib170), [171](#bib.bib171)] and agile legged navigation [[20](#bib.bib20),
    [96](#bib.bib96)], where the long task horizons, large operational spaces, sparse
    rewards, and complex contact dynamics hinder efficient exploration and stable
    learning. Sample efficiency can also be a crucial issue in problems with temporally
    extended action spaces [[32](#bib.bib32), [36](#bib.bib36)]. Fundamental algorithmic
    advances to develop RL algorithms that are at least as robust but more sample-efficient
    than on-policy methods are thus crucial for expanding RL’s applications in robotics.
    An appealing direction is leveraging off-policy or offline samples to complement
    or replace on-policy exploration. However, off-policy and offline RL are often
    less stable due to the distributional shift between behavioral and learning policy
    experiences. Promising efforts have been made to derive scalable and more stable
    off-policy [[110](#bib.bib110)] and offline RL algorithms [[124](#bib.bib124)]
    for manipulation and MoMa [[170](#bib.bib170)]. Fine-tuning offline learned policies
    with online updates can further enhance performance in an efficient manner [[48](#bib.bib48),
    [122](#bib.bib122)]. However, stable online fine-tuning is non-trivial, especially
    for value-based RL [[192](#bib.bib192), [193](#bib.bib193)]. Combining model-free
    and model-based approaches is another promising direction to derive sample-efficient
    RL algorithms [[194](#bib.bib194)]. Lastly, these advances have primarily focused
    on single-robot problems. Multi-robot problems present greater challenges as the
    complexity of multi-robot interaction escalates exponentially with the number
    of robots. The scalability and stability of MARL remain open questions that hinder
    RL’s application for multi-robot interaction.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于对超参数的鲁棒性，通常更倾向于使用在政策的 RL 方法，但收集大量的在政策数据可能会很困难，特别是在实际的 RL 环境中。即使在主要的零-shot
    从模拟到现实设置中，在政策 RL 的样本效率对于诸如长时间跨度的移动操控 [[170](#bib.bib170), [171](#bib.bib171)]
    和灵活的腿部导航 [[20](#bib.bib20), [96](#bib.bib96)] 这样的任务也是一个问题，因为长任务周期、大操作空间、稀疏奖励和复杂的接触动态阻碍了高效的探索和稳定的学习。样本效率在具有时间扩展的动作空间问题中也是一个关键问题
    [[32](#bib.bib32), [36](#bib.bib36)]。因此，开发至少与在政策方法一样鲁棒但更具样本效率的 RL 算法的基本算法进展对于扩展
    RL 在机器人中的应用至关重要。一个有吸引力的方向是利用离政策或离线样本来补充或替代在政策探索。然而，由于行为政策和学习政策之间的分布变化，离政策和离线 RL
    通常稳定性较差。已经有一些有前途的工作致力于推导出可扩展且更稳定的离政策 [[110](#bib.bib110)] 和离线 RL 算法 [[124](#bib.bib124)]，用于操控和
    MoMa [[170](#bib.bib170)]。通过在线更新进一步微调离线学习的策略可以以高效的方式提升性能 [[48](#bib.bib48), [122](#bib.bib122)]。然而，稳定的在线微调并非易事，特别是对于基于价值的
    RL [[192](#bib.bib192), [193](#bib.bib193)]。结合无模型和有模型的方法是推导样本高效 RL 算法的另一个有前景的方向
    [[194](#bib.bib194)]。最后，这些进展主要集中在单机器人问题上。多机器人问题则面临更大的挑战，因为多机器人交互的复杂性随着机器人数量的增加而呈指数级增长。MARL
    的可扩展性和稳定性仍然是开放的问题，阻碍了 RL 在多机器人交互中的应用。
- en: Real-World Learning.
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际世界学习。
- en: 'In our analysis of RL for robot competencies (Sec. [4](#S4 "4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")),
    real-world learning was often mentioned as one of the open challenges. A learning
    process carried out in the real world is crucial for robotic problems where the
    zero-shot sim-to-real transfer procedure is impractical due to the lack of high-fidelity
    simulation, such as open-world and contact-rich manipulation, lightweight quadrotor
    navigation, and physical HRI. Although some progress has been made, particularly
    for manipulation (Table [3](#A2.T3 "Table 3 ‣ Appendix B Additional Tables ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes")), successful
    real-world learning examples are much rarer than zero-shot sim-to-real transfer,
    presenting exciting opportunities for future research. Two main issues need to
    be addressed for real-world RL learning. The first issue is *how to collect many
    useful experiences in a safe manner?* In domains where oracle policies, like humans [[124](#bib.bib124)]
    and scripts [[170](#bib.bib170)], are available, demonstrations can be collected
    for offline learning. However, offline RL faces challenges such as distributional
    shifts, and the demonstration data can be suboptimal and costly to collect for
    human experts. Real-world rollouts require automatic resets [[120](#bib.bib120),
    [117](#bib.bib117), [53](#bib.bib53)] and safe exploration mechanisms [[168](#bib.bib168)]
    to minimize human effort and ensure safety. Such mechanisms are still missing
    in most problem domains and present an opportunity for future development, especially
    for safety-critical applications [[92](#bib.bib92), [102](#bib.bib102)]. To date,
    human-in-the-loop learning (for resets and safety) is currently the only alternative [[92](#bib.bib92)],
    leaving automated real-world learning a desirable future capability. In addition
    to procedural and algorithmic improvements, safe real-world exploration may also
    be facilitated through hardware advances, such as adaptive and less fragile hardware
    and mechanisms that ensure safety passively [[195](#bib.bib195)]. The second issue
    is how do we accelerate training to require fewer experiences? A promising avenue
    is to explore what modules can be updated with real-world samples and how. Instead
    of updating the entire policy with model-free RL, some solutions explore adapting
    vision encoders [[43](#bib.bib43)] or learning (residual) dynamics models [[7](#bib.bib7),
    [102](#bib.bib102), [56](#bib.bib56)] from real-world samples. These alternatives
    improve efficiency; we predict future successful real-world training procedures
    exploring alternative combinations of frozen-trainable modules.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们对机器人能力的强化学习（参见[第4节](#S4 "4 Competency-Specific Review ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes)")的分析中，现实世界的学习常被提及为一个开放性挑战。现实世界中的学习过程对那些由于缺乏高保真模拟而使零-shot模拟到现实转移程序不切实际的机器人问题至关重要，例如开放世界和接触丰富的操作、轻量级四旋翼导航以及物理人机交互。尽管已有一些进展，特别是在操作方面（见[表3](#A2.T3
    "Table 3 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes)")），成功的现实世界学习案例仍然比零-shot模拟到现实转移要稀少得多，为未来的研究提供了激动人心的机会。现实世界强化学习需要解决两个主要问题。第一个问题是*如何安全地收集大量有用的经验？*
    在存在类似人类[[124](#bib.bib124)]和脚本[[170](#bib.bib170)]的oracle策略的领域中，可以收集演示数据用于离线学习。然而，离线强化学习面临着分布转移等挑战，演示数据可能是次优的，并且对于人类专家而言，收集这些数据成本高昂。现实世界的回合需要自动重置[[120](#bib.bib120),
    [117](#bib.bib117), [53](#bib.bib53)]和安全探索机制[[168](#bib.bib168)]来最小化人类的努力并确保安全。这些机制在大多数问题领域中仍然缺失，并为未来的发展提供了机会，特别是在安全关键的应用[[92](#bib.bib92),
    [102](#bib.bib102)]。迄今为止，基于人类的学习（用于重置和安全）目前是唯一的替代方案[[92](#bib.bib92)]，使得自动化现实世界学习成为一种理想的未来能力。除了程序和算法改进外，安全的现实世界探索也可能通过硬件进步来促进，例如适应性更强且不易损坏的硬件以及确保安全的被动机制[[195](#bib.bib195)]。第二个问题是如何加速训练以减少经验需求？一个有前景的方向是探索可以用现实世界样本更新的模块以及如何更新。与其用无模型强化学习更新整个策略，一些解决方案探索了如何调整视觉编码器[[43](#bib.bib43)]或从现实世界样本中学习（残差）动态模型[[7](#bib.bib7),
    [102](#bib.bib102), [56](#bib.bib56)]。这些替代方法提高了效率；我们预测未来成功的现实世界训练程序将探索冻结-可训练模块的不同组合。'
- en: Learning for Long-Horizon Robotic Tasks.
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 长期视角下的机器人任务学习。
- en: 'Long-horizon tasks pose a fundamental challenge to RL algorithms, requiring
    directed exploration and temporal credit assignment over long stretches of time.
    Many such real-world tasks require integrating diverse abilities. By contrast,
    the vast majority of the RL successes we have reviewed are in short-horizon problems,
    e.g., controlling a quadruped to walk at a given velocity or controlling a manipulator
    to rotate an object in hand. A promising avenue for solving long-horizon tasks
    is learning skills and composing them, enabling compositional generalization.
    This approach has seen success in navigation [[96](#bib.bib96), [97](#bib.bib97)],
    manipulation [[11](#bib.bib11), [115](#bib.bib115), [132](#bib.bib132), [150](#bib.bib150)],
    and MoMa [[157](#bib.bib157), [171](#bib.bib171)]. A critical question for future
    work is: what skills should the robot learn?. While some successes have been achieved
    with manually specified skills and reward functions [[32](#bib.bib32), [50](#bib.bib50),
    [96](#bib.bib96), [123](#bib.bib123), [150](#bib.bib150), [114](#bib.bib114)],
    these approaches heavily rely on domain knowledge. Some efforts have been made
    to explore unified reward designs for learning multi-skill locomotion policies [[42](#bib.bib42),
    [51](#bib.bib51), [49](#bib.bib49)]. Formulating skill learning as goal-conditioned [[125](#bib.bib125)]
    or unsupervised RL [[196](#bib.bib196), [197](#bib.bib197)] is promising for more
    general problems. A second critical question is: how should these skills be combined
    to solve long-horizon tasks? Various designs have been explored, including hierarchical
    RL [[32](#bib.bib32), [157](#bib.bib157)], end-to-end training [[123](#bib.bib123),
    [49](#bib.bib49)], and planning [[132](#bib.bib132), [150](#bib.bib150), [171](#bib.bib171)].
    This question will also be central to integrating various competencies toward
    general-purpose robots; recent advances along this line have opened up exciting
    possibilities, including wheel-legged navigation [[97](#bib.bib97)] and loco-manipulation [[51](#bib.bib51),
    [158](#bib.bib158), [162](#bib.bib162), [169](#bib.bib169), [166](#bib.bib166)].'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 长期任务对强化学习（RL）算法提出了根本性的挑战，需要在较长时间段内进行有针对性的探索和时间信用分配。许多这样的现实世界任务需要整合多种能力。相比之下，我们审查的大多数RL成功案例都是在短期任务中，例如，控制四足机器人以特定速度行走或控制操纵器旋转手中的物体。解决长期任务的一个有前景的途径是学习技能并将其组合，从而实现组合泛化。这种方法在导航[[96](#bib.bib96)，[97](#bib.bib97)]、操控[[11](#bib.bib11)，[115](#bib.bib115)，[132](#bib.bib132)，[150](#bib.bib150)]和MoMa[[157](#bib.bib157)，[171](#bib.bib171)]中取得了成功。未来工作的一个关键问题是：机器人应该学习哪些技能？虽然通过手动指定技能和奖励函数[[32](#bib.bib32)，[50](#bib.bib50)，[96](#bib.bib96)，[123](#bib.bib123)，[150](#bib.bib150)，[114](#bib.bib114)]取得了一些成功，这些方法严重依赖于领域知识。已经有一些努力探索了用于学习多技能运动策略的统一奖励设计[[42](#bib.bib42)，[51](#bib.bib51)，[49](#bib.bib49)]。将技能学习制定为目标条件[[125](#bib.bib125)]或无监督RL[[196](#bib.bib196)，[197](#bib.bib197)]对更一般的问题具有前景。第二个关键问题是：这些技能应该如何组合以解决长期任务？已探索了各种设计，包括层次RL[[32](#bib.bib32)，[157](#bib.bib157)]、端到端训练[[123](#bib.bib123)，[49](#bib.bib49)]和规划[[132](#bib.bib132)，[150](#bib.bib150)，[171](#bib.bib171)]。这个问题对于将各种能力整合到通用机器人中也至关重要；沿此方向的近期进展开辟了令人兴奋的可能性，包括轮足导航[[97](#bib.bib97)]和运动操控[[51](#bib.bib51)，[158](#bib.bib158)，[162](#bib.bib162)，[169](#bib.bib169)，[166](#bib.bib166)]。
- en: Designing Principled Approaches for RL Systems.
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计原则化方法用于RL系统。
- en: 'For each robotic task, an RL practitioner must choose among the many alternatives
    that will define its RL system, both in the problem formulation and solution space
    (see Table [1](#A2.T1 "Table 1 ‣ Appendix B Additional Tables ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")–[6](#A2.T6 "Table 6
    ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey
    of Real-World Successes")). Many of these choices are made based on expert knowledge
    and heuristics, which are not necessarily optimal and can even harm performance [[198](#bib.bib198),
    [199](#bib.bib199)]. Principled approaches for RL system design, relying less
    on heuristics and manual efforts, will be essential in the future for scalable
    development and deployment, especially for open-world tasks. Here, we note some
    particularly important examples. First, many real-world successes have been achieved
    with dense and shaped rewards designed with heavy engineering efforts, particularly
    in locomotion and navigation (Table [2](#A2.T2 "Table 2 ‣ Appendix B Additional
    Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")).
    Efforts are being made to explore principled reward designs for specific competencies [[42](#bib.bib42),
    [51](#bib.bib51), [49](#bib.bib49)] and more general problems using goal-conditioned [[125](#bib.bib125)]
    or unsupervised RL [[196](#bib.bib196), [197](#bib.bib197)]. Second, various action
    spaces are used, particularly for manipulation and MoMa (Table [1](#A2.T1 "Table
    1 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A
    Survey of Real-World Successes")). The action space choices affect the temporal
    abstraction levels and robustness of the RL policies. Some studies have attempted
    to benchmark different action spaces [[66](#bib.bib66), [86](#bib.bib86), [199](#bib.bib199)],
    but such principled studies and guidelines are still lacking for many problems.
    Another related design choice is the integration of RL with classical planning
    and control modules. The different levels of integration result in different action
    spaces for the RL policies (i.e., low-, mid-, and high-level). The effectiveness
    of end-to-end versus hybrid modular solutions varies by problem [[82](#bib.bib82),
    [86](#bib.bib86), [100](#bib.bib100), [21](#bib.bib21), [200](#bib.bib200)]. Neither
    approach is universally superior. There are many other dimensions that require
    such principled investigations, which are crucial for advancing DRL’s real-world
    success, in addition to exploring new frontiers in algorithms and applications.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '对于每个机器人任务，RL 从业者必须在将定义其 RL 系统的众多选择中进行选择，包括问题的定义和解决方案空间（参见表[1](#A2.T1 "Table
    1 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A
    Survey of Real-World Successes")–[6](#A2.T6 "Table 6 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")）。许多这些选择是基于专家知识和启发式方法做出的，这些方法不一定是最优的，甚至可能会影响性能[[198](#bib.bib198),
    [199](#bib.bib199)]。未来，基于原则的方法用于 RL 系统设计，将减少对启发式方法和手动努力的依赖，对可扩展的开发和部署尤为重要，特别是对于开放世界任务。在这里，我们指出一些特别重要的例子。首先，许多现实世界的成功是通过密集且经过精心设计的奖励实现的，特别是在运动和导航方面（表[2](#A2.T2
    "Table 2 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")）。正在努力探索针对特定能力[[42](#bib.bib42), [51](#bib.bib51),
    [49](#bib.bib49)]和更一般问题的原则性奖励设计，使用目标条件的[[125](#bib.bib125)]或无监督 RL[[196](#bib.bib196),
    [197](#bib.bib197)]。第二，使用了各种动作空间，特别是用于操作和 MoMa（表[1](#A2.T1 "Table 1 ‣ Appendix
    B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes")）。动作空间的选择影响 RL 策略的时间抽象级别和鲁棒性。一些研究尝试基准测试不同的动作空间[[66](#bib.bib66), [86](#bib.bib86),
    [199](#bib.bib199)]，但许多问题仍然缺乏这种原则性的研究和指导。另一个相关的设计选择是将 RL 与经典规划和控制模块的集成。不同级别的集成会导致
    RL 策略的不同动作空间（即低级、中级和高级）。端到端与混合模块解决方案的有效性因问题而异[[82](#bib.bib82), [86](#bib.bib86),
    [100](#bib.bib100), [21](#bib.bib21), [200](#bib.bib200)]。没有一种方法是普遍优越的。还有许多其他维度需要这种原则性的调查，这对于推动
    DRL 在现实世界中的成功至关重要，除了探索算法和应用的新前沿。'
- en: Benchmarking Real-World Success.
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 现实世界成功的基准测试。
- en: In this survey, we classify papers into six levels of real-world success to
    assess the maturity of DRL-based solutions. However, precisely determining these
    levels can be challenging since the only source of information is the experimental
    results reported by the authors, but the varying testing conditions and evaluation
    metrics make direct comparison difficult. This highlights the need for *standard
    evaluation protocols and benchmarks for real-world performance*. While widely
    adopted, low-cost hardware, as seen with quadrupeds, is helpful by enabling standardized
    experimental platforms, it is not sufficient alone. Test environments and tasks
    must also resemble real-world conditions and, more importantly, be *reproducible*.
    Multiple real-world benchmarks have been established, including those for manipulation [[201](#bib.bib201),
    [202](#bib.bib202)] and domestic service robots [[203](#bib.bib203)]. However,
    when it comes to complex open-world problems, the evaluation procedure must also
    scale up to be realistic and informative [[204](#bib.bib204)]. Overall, developing
    scalable evaluation protocols and benchmarks remains an exciting open research
    direction for many problems.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本综述中，我们将论文分为六个层级以评估基于DRL的解决方案的成熟度。然而，精确确定这些层级可能具有挑战性，因为唯一的信息来源是作者报告的实验结果，但不同的测试条件和评估指标使得直接比较变得困难。这突显了*标准化评估协议和真实世界性能基准*的必要性。虽然广泛采用的低成本硬件，如四足机器人，能够提供标准化实验平台，但仅此并不足够。测试环境和任务还必须类似于现实世界条件，更重要的是，具有*可重复性*。已经建立了多个真实世界的基准，包括操作[[201](#bib.bib201)、[202](#bib.bib202)]和家庭服务机器人[[203](#bib.bib203)]的基准。然而，面对复杂的开放世界问题时，评估程序也必须进行扩展，以确保其现实性和信息性[[204](#bib.bib204)]。总的来说，开发可扩展的评估协议和基准仍然是许多问题中一个令人兴奋的开放研究方向。
- en: Leveraging Foundation Models.
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 利用基础模型。
- en: Lastly, recent advances in large-scale robot dataset [[205](#bib.bib205), [206](#bib.bib206)]
    and robot foundation models [[207](#bib.bib207), [208](#bib.bib208)] present exciting
    open opportunities for RL successes in the real world. Foundation models have
    demonstrated impressive generalization capabilities across domains for reasoning
    and decision-making tasks [[209](#bib.bib209)], showing promise for addressing
    several of the aforementioned challenges of DRL for robotics. For instance, the
    recently introduced DrEureka [[210](#bib.bib210)] algorithm leverages large language
    models (LLMs) to automate reward design and domain randomization configuration
    for sim-to-real transfer without manual tuning. In addition, LLMs and vision-language
    models (VLMs) open up new opportunities to create language-conditioned RL policies
    for novel applications. We refer readers to existing surveys for detailed discussions
    on the opportunities foundation models offer in general [[207](#bib.bib207), [208](#bib.bib208)],
    but we anticipate an increased integration of foundation models into RL solutions
    for real-world robotic tasks.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最近在大规模机器人数据集[[205](#bib.bib205)、[206](#bib.bib206)]和机器人基础模型[[207](#bib.bib207)、[208](#bib.bib208)]的进展，展示了在实际世界中实现RL成功的令人兴奋的开放机会。基础模型在推理和决策任务中展示了令人印象深刻的跨领域泛化能力[[209](#bib.bib209)]，对解决上述DRL在机器人领域面临的一些挑战表现出了前景。例如，最近引入的DrEureka[[210](#bib.bib210)]算法利用大型语言模型（LLMs）来自动化奖励设计和领域随机化配置，从而实现无需手动调节的模拟到现实转移。此外，LLMs和视觉-语言模型（VLMs）为创建语言条件RL策略以应对新应用开辟了新的机会。我们建议读者查阅现有的综述文章，了解基础模型在一般领域所提供的机会[[207](#bib.bib207)、[208](#bib.bib208)]，但我们预期基础模型在现实世界机器人任务中的RL解决方案中的整合会增加。
- en: 6 Conclusion
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Deep reinforcement learning has recently played an important role in the development
    of many robotic capabilities, leading to many real-world successes. Here, we have
    reviewed and categorized these successes, delineating them based on the specific
    robotic competency, problem formulation, and solution approach. Our analysis across
    these axes has revealed general trends and important avenues for future work,
    including algorithmic and procedural improvements, ingredients for real-world
    learning, and holistic approaches toward synthesizing all the competencies discussed
    herein. Harnessing RL’s power to produce capable real-world robotic systems will
    require solving fundamental challenges and innovations in its application; nonetheless,
    we expect that RL will continue to play a central role in the development of generally
    intelligent robots.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习最近在许多机器人能力的发展中发挥了重要作用，取得了许多现实世界的成功。在此，我们对这些成功进行了回顾和分类，按照特定的机器人能力、问题表述和解决方法进行划分。我们在这些轴上的分析揭示了普遍趋势和未来工作的重要途径，包括算法和程序改进、现实世界学习的要素，以及综合讨论的所有能力的整体方法。利用强化学习的力量来生产有能力的现实世界机器人系统将需要解决基本挑战和创新应用；尽管如此，我们预计强化学习将继续在普遍智能机器人发展的过程中发挥核心作用。
- en: Acknowledgements
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Pieter Abbeel, Yuchen Cui, Shivin Dass, George Konidaris, Jan Peters,
    Eric Rosen, Koushil Sreenath, Eugene Vinitsky, and Zhaoming Xie for their feedback
    on the manuscript. We also thank Google DeepMind for permission to use representative
    images from their work on robot soccer. A portion of this work has taken place
    in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory
    at the University of Texas at Austin. LARG research is supported in part by the
    National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research
    (N00014-18-2243), Army Research Office (E2061621), Bosch, Lockheed Martin, and
    Good Systems, a research grand challenge at the University of Texas at Austin.
    The views and conclusions contained in this document are those of the authors
    alone. Peter Stone serves as the Chief Scientist of Sony AI and receives financial
    compensation for this work. The terms of this arrangement have been reviewed and
    approved by the University of Texas at Austin in accordance with its policy on
    objectivity in research.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Pieter Abbeel、Yuchen Cui、Shivin Dass、George Konidaris、Jan Peters、Eric Rosen、Koushil
    Sreenath、Eugene Vinitsky 和 Zhaoming Xie 对稿件的反馈。我们还感谢 Google DeepMind 允许使用他们在机器人足球领域的代表性图像。本工作的一部分在德克萨斯大学奥斯汀分校的人工智能实验室学习代理研究组（LARG）进行。LARG
    研究部分得到了国家科学基金会（FAIN-2019844，NRT-2125858）、海军研究办公室（N00014-18-2243）、陆军研究办公室（E2061621）、博世、洛克希德·马丁和德克萨斯大学奥斯汀分校的Good
    Systems研究挑战的支持。文档中包含的观点和结论仅代表作者个人。Peter Stone 担任 Sony AI 的首席科学家，并为此工作获得经济补偿。此安排的条款已由德克萨斯大学奥斯汀分校根据其研究客观性政策进行审查和批准。
- en: References
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Sutton RS, Barto AG. 2018. Reinforcement learning: An introduction. MIT
    press, 2nd ed.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Sutton RS, Barto AG. 2018. 强化学习：导论。MIT出版社，第2版。'
- en: '[2] François-Lavet V, Henderson P, Islam R, Bellemare MG, Pineau J, et al.
    2018. An introduction to deep reinforcement learning. Found. Trends in Mach. Learn.
    11(3-4):219–354'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] François-Lavet V, Henderson P, Islam R, Bellemare MG, Pineau J, 等. 2018.
    深度强化学习导论。机器学习基础趋势 11(3-4):219–354'
- en: '[3] Schrittwieser J, Antonoglou I, Hubert T, Simonyan K, Sifre L, et al. 2020.
    Mastering atari, go, chess and shogi by planning with a learned model. Nature
    588(7839):604–609'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Schrittwieser J, Antonoglou I, Hubert T, Simonyan K, Sifre L, 等. 2020.
    通过规划与学习模型掌握Atari、围棋、国际象棋和将棋。Nature 588(7839):604–609'
- en: '[4] Wurman PR, Barrett S, Kawamoto K, MacGlashan J, Subramanian K, et al. 2022.
    Outracing champion gran turismo drivers with deep reinforcement learning. Nature
    602(7896):223–228'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Wurman PR, Barrett S, Kawamoto K, MacGlashan J, Subramanian K, 等. 2022.
    通过深度强化学习超越冠军级Gran Turismo驾驶员。Nature 602(7896):223–228'
- en: '[5] Yu C, Liu J, Nemati S, Yin G. 2021. Reinforcement learning in healthcare:
    A survey. ACM Comput. Surv. 55(1):1–36'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Yu C, Liu J, Nemati S, Yin G. 2021. 医疗保健中的强化学习：综述。ACM计算机调查 55(1):1–36'
- en: '[6] Afsar MM, Crump T, Far B. 2022. Reinforcement learning based recommender
    systems: A survey. ACM Comput. Surv. 55(7):1–38'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Afsar MM, Crump T, Far B. 2022. 基于强化学习的推荐系统：综述。ACM计算机调查 55(7):1–38'
- en: '[7] Kaufmann E, Bauersfeld L, Loquercio A, Müller M, Koltun V, Scaramuzza D.
    2023. Champion-level drone racing using deep reinforcement learning. Nature 620(7976):982–987'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Kaufmann E, Bauersfeld L, Loquercio A, Müller M, Koltun V, Scaramuzza D.
    2023. 使用深度强化学习进行冠军级无人机竞速。Nature 620(7976):982–987'
- en: '[8] Kiran BR, Sobh I, Talpaert V, Mannion P, Al Sallab AA, et al. 2021. Deep
    reinforcement learning for autonomous driving: A survey. IEEE Trans. Intell. Transp.
    Syst. 23(6):4909–4926'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Kiran BR, Sobh I, Talpaert V, Mannion P, Al Sallab AA等。2021年。深度强化学习在自主驾驶中的应用：综述。IEEE
    Trans. Intell. Transp. Syst. 23(6):4909–4926'
- en: '[9] Dulac-Arnold G, Levine N, Mankowitz DJ, Li J, Paduraru C, et al. 2021.
    Challenges of real-world reinforcement learning: definitions, benchmarks, and
    analysis. Mach. Learn. 110(9):2419––2468'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Dulac-Arnold G, Levine N, Mankowitz DJ, Li J, Paduraru C等。2021年。现实世界强化学习的挑战：定义、基准和分析。Mach.
    Learn. 110(9):2419––2468'
- en: '[10] Ibarz J, Tan J, Finn C, Kalakrishnan M, Pastor P, Levine S. 2021. How
    to train your robot with deep reinforcement learning: lessons we have learned.
    Int. J. Robot. Res. 40(4-5):698–721'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ibarz J, Tan J, Finn C, Kalakrishnan M, Pastor P, Levine S。2021年。如何使用深度强化学习训练你的机器人：我们学到的教训。Int.
    J. Robot. Res. 40(4-5):698–721'
- en: '[11] Kroemer O, Niekum S, Konidaris G. 2021. A review of robot learning for
    manipulation: Challenges, representations, and algorithms. J. Mach. Learn. Res.
    22(30):1–82'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Kroemer O, Niekum S, Konidaris G。2021年。机器人操作学习综述：挑战、表示和算法。J. Mach. Learn.
    Res. 22(30):1–82'
- en: '[12] Xiao X, Liu B, Warnell G, Stone P. 2022. Motion planning and control for
    mobile robot navigation using machine learning: a survey. Auton. Robots 46(5):569–597'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Xiao X, Liu B, Warnell G, Stone P。2022年。基于机器学习的移动机器人导航中的运动规划和控制：综述。Auton.
    Robots 46(5):569–597'
- en: '[13] Deisenroth MP. 2011. A survey on policy search for robotics. Found. Trends
    Robot. 2(1-2):1–142'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Deisenroth MP。2011年。机器人策略搜索综述。Found. Trends Robot. 2(1-2):1–142'
- en: '[14] Brunke L, Greeff M, Hall AW, Yuan Z, Zhou S, et al. 2022. Safe Learning
    in Robotics: From Learning-Based Control to Safe Reinforcement Learning. Annu.
    Rev. Control Robot. Auton. Syst. 5(1):411–444_eprint: https://doi.org/10.1146/annurev-control-042920-020211'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Brunke L, Greeff M, Hall AW, Yuan Z, Zhou S等。2022年。机器人中的安全学习：从基于学习的控制到安全强化学习。Annu.
    Rev. Control Robot. Auton. Syst. 5(1):411–444_eprint: https://doi.org/10.1146/annurev-control-042920-020211'
- en: '[15] Kober J, Bagnell JA, Peters J. 2013. Reinforcement learning in robotics:
    A survey. Int. J. Robot. Res. 32(11):1238–1274'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Kober J, Bagnell JA, Peters J。2013年。机器人中的强化学习：综述。Int. J. Robot. Res. 32(11):1238–1274'
- en: '[16] Sünderhauf N, Brock O, Scheirer W, Hadsell R, Fox D, et al. 2018. The
    limits and potentials of deep learning for robotics. Int. J. Robot. Res. 37(4-5):405–420'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Sünderhauf N, Brock O, Scheirer W, Hadsell R, Fox D等。2018年。深度学习在机器人领域的限制与潜力。Int.
    J. Robot. Res. 37(4-5):405–420'
- en: '[17] Mason MT. 2001. Mechanics of robotic manipulation. MIT press'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Mason MT。2001年。机器人操作学。MIT出版社'
- en: '[18] Siciliano B, Khatib O, Kröger T. 2008. Springer handbook of robotics,
    vol. 200. Springer'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Siciliano B, Khatib O, Kröger T。2008年。Springer机器人手册，第200卷。Springer'
- en: '[19] Mason MT. 2018. Toward robotic manipulation. Annu. Rev. Control Robot.
    Auton. Syst. 1:1–28'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Mason MT。2018年。迈向机器人操作。Annu. Rev. Control Robot. Auton. Syst. 1:1–28'
- en: '[20] Rudin N, Hoeller D, Bjelonic M, Hutter M. 2022. Advanced skills by learning
    locomotion and local navigation end-to-end. In IEEE/RSJ Int. Conf. Intell. Robots
    Syst., pp. 2497–2503\. IEEE'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Rudin N, Hoeller D, Bjelonic M, Hutter M。2022年。通过端到端学习运动和局部导航的高级技能。在IEEE/RSJ国际智能机器人系统会议，第2497–2503页。IEEE'
- en: '[21] Song Y, Romero A, Müller M, Koltun V, Scaramuzza D. 2023. Reaching the
    limit in autonomous racing: Optimal control versus reinforcement learning. Sci.
    Robot. 8(82):eadg1462'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Song Y, Romero A, Müller M, Koltun V, Scaramuzza D。2023年。在自主赛车中达到极限：最优控制与强化学习。Sci.
    Robot. 8(82):eadg1462'
- en: '[22] On-Road Automated Driving (ORAD) committee. 2018. Taxonomy and definitions
    for terms related to driving automation systems for on-road motor vehicles. Tech.
    rep., SAE International'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] On-Road Automated Driving (ORAD)委员会。2018年。与道路车辆自动驾驶系统相关的术语分类和定义。技术报告，SAE
    International'
- en: '[23] Lavin A, Gilligan-Lee CM, Visnjic A, Ganju S, Newman D, et al. 2022. Technology
    readiness levels for machine learning systems. Nat. Commun. 13(1):6039'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Lavin A, Gilligan-Lee CM, Visnjic A, Ganju S, Newman D等。2022年。机器学习系统的技术成熟度等级。Nat.
    Commun. 13(1):6039'
- en: '[24] Kohl N, Stone P. 2004. Policy gradient reinforcement learning for fast
    quadrupedal locomotion. In IEEE Int. Conf. Robot. Autom., vol. 3, pp. 2619–2624\.
    IEEE'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Kohl N, Stone P。2004年。用于快速四足 locomotion的策略梯度强化学习。在IEEE国际机器人与自动化会议，第3卷，第2619–2624页。IEEE'
- en: '[25] Bagnell JA, Schneider JG. 2001. Autonomous helicopter control using reinforcement
    learning policy search methods. In IEEE Int. Conf. Robot. Autom., vol. 2, pp.
    1615–1620\. IEEE'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Bagnell JA, Schneider JG。2001年。使用强化学习策略搜索方法进行自主直升机控制。在IEEE国际机器人与自动化会议，第2卷，第1615–1620页。IEEE'
- en: '[26] Abbeel P, Coates A, Quigley M, Ng A. 2006. An application of reinforcement
    learning to aerobatic helicopter flight. In Adv. Neural Inf. Process. Syst., vol. 19'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Abbeel P, Coates A, Quigley M, Ng A. 2006. 强化学习在特技直升机飞行中的应用。在神经信息处理系统会议上，第19卷'
- en: '[27] Kumar A, Li Z, Zeng J, Pathak D, Sreenath K, Malik J. 2022. Adapting rapid
    motor adaptation for bipedal robots. In IEEE/RSJ Int. Conf. Intell. Robots Syst.,
    pp. 1161–1168\. IEEE'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Kumar A, Li Z, Zeng J, Pathak D, Sreenath K, Malik J. 2022. 适应双足机器人进行快速运动适应。在IEEE/RSJ国际智能机器人系统会议上，第1161–1168页。IEEE'
- en: '[28] Tan J, Zhang T, Coumans E, Iscen A, Bai Y, et al. 2018. Sim-to-real: Learning
    agile locomotion for quadruped robots. In Robot. Sci. Syst.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Tan J, Zhang T, Coumans E, Iscen A, Bai Y, et al. 2018. 从模拟到现实：为四足机器人学习灵活运动。在机器人科学与系统会议上。'
- en: '[29] Hwangbo J, Lee J, Dosovitskiy A, Bellicoso D, Tsounis V, et al. 2019.
    Learning agile and dynamic motor skills for legged robots. Sci. Robot. 4(26):eaau5872'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Hwangbo J, Lee J, Dosovitskiy A, Bellicoso D, Tsounis V, et al. 2019.
    学习腿部机器人灵活和动态的运动技能。在科学机器人，4(26):eaau5872'
- en: '[30] Feng G, Zhang H, Li Z, Peng XB, Basireddy B, et al. 2023. Genloco: Generalized
    locomotion controllers for quadrupedal robots. In Conference on Robot Learning,
    pp. 1893–1903\. PMLR'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Feng G, Zhang H, Li Z, Peng XB, Basireddy B, et al. 2023. Genloco：四足机器人通用化运动控制器。在机器人学习会议上，第1893–1903页。PMLR'
- en: '[31] Lee J, Hwangbo J, Hutter M. 2019. Robust recovery controller for a quadrupedal
    robot using deep reinforcement learning. arXiv preprint arXiv:1901.07517'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Lee J, Hwangbo J, Hutter M. 2019. 使用深度强化学习的四足机器人鲁棒恢复控制器。arXiv预印本arXiv:1901.07517'
- en: '[32] Yang C, Yuan K, Zhu Q, Yu W, Li Z. 2020. Multi-expert learning of adaptive
    legged locomotion. Sci. Robot. 5(49):eabb2174'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Yang C, Yuan K, Zhu Q, Yu W, Li Z. 2020. 自适应腿部运动的多专家学习。在科学机器人，5(49):eabb2174'
- en: '[33] Kumar A, Fu Z, Pathak D, Malik J. 2021. RMA: Rapid motor adaptation for
    legged robots. In Robot. Sci. Syst.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Kumar A, Fu Z, Pathak D, Malik J. 2021. RMA：适用于腿部机器人快速运动适应。在机器人科学与系统会议上。'
- en: '[34] Lee J, Hwangbo J, Wellhausen L, Koltun V, Hutter M. 2020. Learning quadrupedal
    locomotion over challenging terrain. Sci. Robot. 5(47):eabc5986'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Lee J, Hwangbo J, Wellhausen L, Koltun V, Hutter M. 2020. 学习在具有挑战性的地形上的四足运动。在科学机器人，5(47):eabc5986'
- en: '[35] Miki T, Lee J, Hwangbo J, Wellhausen L, Koltun V, Hutter M. 2022. Learning
    robust perceptive locomotion for quadrupedal robots in the wild. Sci. Robot. 7(62):eabk2822'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Miki T, Lee J, Hwangbo J, Wellhausen L, Koltun V, Hutter M. 2022. 在野外学习四足机器人鲁棒的感知运动。在科学机器人，7(62):eabk2822'
- en: '[36] Gangapurwala S, Geisert M, Orsolino R, Fallon M, Havoutis I. 2022. RLOC:
    Terrain-aware legged locomotion using reinforcement learning and optimal control.
    IEEE Trans. Robot. 38(5):2908–2927'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Gangapurwala S, Geisert M, Orsolino R, Fallon M, Havoutis I. 2022. RLOC：使用强化学习和最优控制的地形感知腿部运动。在IEEE机器人学报，38(5):2908–2927'
- en: '[37] Choi S, Ji G, Park J, Kim H, Mun J, et al. 2023. Learning quadrupedal
    locomotion on deformable terrain. Sci. Robot. 8(74):eade2256'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Choi S, Ji G, Park J, Kim H, Mun J, et al. 2023. 在可变形地形上学习四足运动。在科学机器人，8(74):eade2256'
- en: '[38] Nahrendra IMA, Yu B, Myung H. 2023. DreamWaQ: Learning robust quadrupedal
    locomotion with implicit terrain imagination via deep reinforcement learning.
    In IEEE Int. Conf. Robot. Autom., pp. 5078–5084\. IEEE'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Nahrendra IMA, Yu B, Myung H. 2023. DreamWaQ：通过深度强化学习学习具有隐性地形想象的鲁棒四足运动。在IEEE国际机器人与自动化会议上，第5078–5084页。IEEE'
- en: '[39] Pinto L, Andrychowicz M, Welinder P, Zaremba W, Abbeel P. 2018. Asymmetric
    actor critic for image-based robot learning. In Robot. Sci. Syst.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Pinto L, Andrychowicz M, Welinder P, Zaremba W, Abbeel P. 2018. 用于基于图像的机器人学习的非对称演员评论者。在机器人科学与系统会议上。'
- en: '[40] Escontrela A, Peng XB, Yu W, Zhang T, Iscen A, et al. 2022. Adversarial
    motion priors make good substitutes for complex reward functions. In IEEE/RSJ
    Int. Conf. Intell. Robots Syst., pp. 25–32\. IEEE'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Escontrela A, Peng XB, Yu W, Zhang T, Iscen A, et al. 2022. 对抗性运动先验成为复杂奖励函数的良好替代品。在IEEE/RSJ国际智能机器人系统会议上，第25–32页。IEEE'
- en: '[41] Ma Y, Farshidian F, Hutter M. 2023. Learning arm-assisted fall damage
    reduction and recovery for legged mobile manipulators. In IEEE Int. Conf. Robot.
    Autom., pp. 12149–12155\. IEEE'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Ma Y, Farshidian F, Hutter M. 2023. 为腿部移动操作手学习辅助手臂减轻跌倒损伤和恢复。在IEEE国际机器人与自动化会议上，第12149–12155页。IEEE'
- en: '[42] Fu Z, Kumar A, Malik J, Pathak D. 2022. Minimizing energy consumption
    leads to the emergence of gaits in legged robots. In Conf. Robot Learn., pp. 928–937\.
    PMLR'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Fu Z, Kumar A, Malik J, Pathak D. 2022. 最小化能量消耗导致四足机器人步态的出现。在机器人学习会议上，第928–937页。PMLR'
- en: '[43] Loquercio A, Kumar A, Malik J. 2023. Learning visual locomotion with cross-modal
    supervision. In IEEE Int. Conf. Robot. Autom., pp. 7295–7302\. IEEE'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Loquercio A, Kumar A, Malik J. 2023. 使用跨模态监督学习视觉运动。在IEEE国际机器人与自动化会议上，第7295–7302页。IEEE'
- en: '[44] Agarwal A, Kumar A, Malik J, Pathak D. 2023. Legged locomotion in challenging
    terrains using egocentric vision. In Conf. Robot Learn., pp. 403–415\. PMLR'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Agarwal A, Kumar A, Malik J, Pathak D. 2023. 使用自我中心视觉在挑战性地形中进行腿部运动。发表于机器人学习会议，页码
    403–415。PMLR'
- en: '[45] Yang R, Yang G, Wang X. 2023. Neural volumetric memory for visual locomotion
    control. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 1430–1440\. PMLR'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Yang R, Yang G, Wang X. 2023. 用于视觉运动控制的神经体积记忆。发表于 IEEE/CVF 计算机视觉与模式识别会议，页码
    1430–1440。PMLR'
- en: '[46] Jenelten F, He J, Farshidian F, Hutter M. 2024. DTC: Deep tracking control.
    Sci. Robot. 9(86):eadh5401'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Jenelten F, He J, Farshidian F, Hutter M. 2024. DTC：深度跟踪控制。科学机器人 9(86):eadh5401'
- en: '[47] Yang Y, Shi G, Meng X, Yu W, Zhang T, et al. 2023a. CAJun: Continuous
    adaptive jumping using a learned centroidal controller. In Conf. Robot. Learn.
    PMLR'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Yang Y, Shi G, Meng X, Yu W, Zhang T, 等. 2023a. CAJun：使用学习的中心控制器进行连续自适应跳跃。发表于机器人学习会议。PMLR'
- en: '[48] Smith L, Kew JC, Peng XB, Ha S, Tan J, Levine S. 2022. Legged robots that
    keep on learning: Fine-tuning locomotion policies in the real world. In IEEE Int.
    Conf. Robot. Autom., pp. 1593–1599\. IEEE'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Smith L, Kew JC, Peng XB, Ha S, Tan J, Levine S. 2022. 持续学习的腿部机器人：在现实世界中微调运动策略。发表于
    IEEE 国际机器人与自动化会议，页码 1593–1599。IEEE'
- en: '[49] Cheng X, Shi K, Agarwal A, Pathak D. 2024. Extreme parkour with legged
    robots. In IEEE Int. Conf. Robot. Autom. IEEE'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Cheng X, Shi K, Agarwal A, Pathak D. 2024. 极限跑酷与腿部机器人。发表于 IEEE 国际机器人与自动化会议。IEEE'
- en: '[50] Zhuang Z, Fu Z, Wang J, Atkeson CG, Schwertfeger S, et al. 2023. Robot
    parkour learning. In Conf. Robot. Learn. PMLR'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Zhuang Z, Fu Z, Wang J, Atkeson CG, Schwertfeger S, 等. 2023. 机器人跑酷学习。发表于机器人学习会议。PMLR'
- en: '[51] Vollenweider E, Bjelonic M, Klemm V, Rudin N, Lee J, Hutter M. 2023. Advanced
    skills through multiple adversarial motion priors in reinforcement learning. In
    IEEE Int. Conf. Robot. Autom., pp. 5120–5126\. IEEE'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Vollenweider E, Bjelonic M, Klemm V, Rudin N, Lee J, Hutter M. 2023. 通过多重对抗运动先验在强化学习中掌握高级技能。发表于
    IEEE 国际机器人与自动化会议，页码 5120–5126。IEEE'
- en: '[52] Margolis GB, Agrawal P. 2023. Walk these ways: Tuning robot control for
    generalization with multiplicity of behavior. In Conf. Robot. Learn., pp. 22–31\.
    PMLR'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Margolis GB, Agrawal P. 2023. 以这些方式行走：调整机器人控制以实现行为的泛化。发表于机器人学习会议，页码 22–31。PMLR'
- en: '[53] Smith L, Kostrikov I, Levine S. 2023. Demonstrating a walk in the park:
    Learning to walk in 20 minutes with model-free reinforcement learning. Robot.
    Sci. Syst. 2(3):4'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Smith L, Kostrikov I, Levine S. 2023. 演示公园散步：用无模型强化学习在 20 分钟内学会行走。机器人科学与系统
    2(3):4'
- en: '[54] Wu P, Escontrela A, Hafner D, Abbeel P, Goldberg K. 2023. Daydreamer:
    World models for physical robot learning. In Conf. Robot Learn., pp. 2226–2240\.
    PMLR'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Wu P, Escontrela A, Hafner D, Abbeel P, Goldberg K. 2023. Daydreamer：用于物理机器人学习的世界模型。发表于机器人学习会议，页码
    2226–2240。PMLR'
- en: '[55] Siekmann J, Valluri S, Dao J, Bermillo L, Duan H, et al. 2020. Learning
    memory-based control for human-scale bipedal locomotion. In Robot. Sci. Syst.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Siekmann J, Valluri S, Dao J, Bermillo L, Duan H, 等. 2020. 学习基于记忆的控制用于人类尺度的双足运动。发表于机器人科学与系统。'
- en: '[56] Hanna JP, Desai S, Karnan H, Warnell G, Stone P. 2021. Grounded action
    transformation for sim-to-real reinforcement learning. Mach. Learn. 110(9):2469–2499'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Hanna JP, Desai S, Karnan H, Warnell G, Stone P. 2021. 基于真实场景的强化学习的 grounded
    action transformation。机器学习 110(9):2469–2499'
- en: '[57] Siekmann J, Godse Y, Fern A, Hurst J. 2021a. Sim-to-real learning of all
    common bipedal gaits via periodic reward composition. In IEEE Int. Conf. Robot.
    Autom., pp. 7309–7315\. IEEE'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Siekmann J, Godse Y, Fern A, Hurst J. 2021a. 通过周期奖励组合进行所有常见双足行走模式的仿真到现实学习。发表于
    IEEE 国际机器人与自动化会议，页码 7309–7315。IEEE'
- en: '[58] Li Z, Cheng X, Peng XB, Abbeel P, Levine S, et al. 2021. Reinforcement
    learning for robust parameterized locomotion control of bipedal robots. In IEEE
    Int. Conf. Robot. Autom., pp. 2811–2817\. IEEE'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Li Z, Cheng X, Peng XB, Abbeel P, Levine S, 等. 2021. 强化学习用于双足机器人稳健的参数化运动控制。发表于
    IEEE 国际机器人与自动化会议，页码 2811–2817。IEEE'
- en: '[59] Siekmann J, Green K, Warila J, Fern A, Hurst J. 2021b. Blind Bipedal Stair
    Traversal via Sim-to-Real Reinforcement Learning. In Robot. Sci. Syst.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Siekmann J, Green K, Warila J, Fern A, Hurst J. 2021b. 盲目双足楼梯攀爬通过仿真到现实的强化学习。发表于机器人科学与系统。'
- en: '[60] Castillo GA, Weng B, Zhang W, Hereid A. 2022. Reinforcement learning-based
    cascade motion policy design for robust 3d bipedal locomotion. IEEE Access 10:20135–20148'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Castillo GA, Weng B, Zhang W, Hereid A. 2022. 基于强化学习的级联运动策略设计，用于稳健的 3D
    双足运动。IEEE Access 10:20135–20148'
- en: '[61] Duan H, Pandit B, Gadde MS, van Marum BJ, Dao J, et al. 2024. Learning
    vision-based bipedal locomotion for challenging terrain. In IEEE Int. Conf. Robot.
    Autom.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Duan H, Pandit B, Gadde MS, van Marum BJ, Dao J, 等. 2024. 学习基于视觉的双足运动以应对挑战性地形。发表于
    IEEE 国际机器人与自动化会议。'
- en: '[62] Radosavovic I, Xiao T, Zhang B, Darrell T, Malik J, Sreenath K. 2024.
    Real-world humanoid locomotion with reinforcement learning. Sci. Robot. 9(89):eadi9579'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Radosavovic I, Xiao T, Zhang B, Darrell T, Malik J, Sreenath K. 2024.
    通过强化学习实现现实世界的人形运动。科学机器人，9(89):eadi9579'
- en: '[63] Li Z, Peng XB, Abbeel P, Levine S, Berseth G, Sreenath K. 2024a. Reinforcement
    learning for versatile, dynamic, and robust bipedal locomotion control. arXiv
    preprint arXiv:2401.16889'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Li Z, Peng XB, Abbeel P, Levine S, Berseth G, Sreenath K. 2024a. 用于多功能、动态和鲁棒双足运动控制的强化学习。arXiv预印本
    arXiv:2401.16889'
- en: '[64] Hwangbo J, Sa I, Siegwart R, Hutter M. 2017. Control of a quadrotor with
    reinforcement learning. IEEE Robot. Autom. Lett. 2(4):2096–2103'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Hwangbo J, Sa I, Siegwart R, Hutter M. 2017. 使用强化学习控制四旋翼。IEEE机器人与自动化快报，2(4):2096–2103'
- en: '[65] Molchanov A, Chen T, Hönig W, Preiss JA, Ayanian N, Sukhatme GS. 2019.
    Sim-to-(multi)-real: Transfer of low-level robust control policies to multiple
    quadrotors. In IEEE/RSJ Int. Conf. Intell. Robots Syst., pp. 59–66\. IEEE'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Molchanov A, Chen T, Hönig W, Preiss JA, Ayanian N, Sukhatme GS. 2019.
    从仿真到（多）现实：低级鲁棒控制策略的多四旋翼转移。发表于IEEE/RSJ国际智能机器人系统会议，页码59–66。IEEE'
- en: '[66] Kaufmann E, Bauersfeld L, Scaramuzza D. 2022. A benchmark comparison of
    learned control policies for agile quadrotor flight. In Int. Conf. Robot. Autom.,
    pp. 10504–10510\. IEEE'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Kaufmann E, Bauersfeld L, Scaramuzza D. 2022. 学习控制策略的敏捷四旋翼飞行的基准比较。发表于国际机器人与自动化会议，页码10504–10510。IEEE'
- en: '[67] Zhang D, Loquercio A, Wu X, Kumar A, Malik J, Mueller MW. 2023. Learning
    a single near-hover position controller for vastly different quadcopters. In IEEE
    Int. Conf. Robot. Autom., pp. 1263–1269\. IEEE'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Zhang D, Loquercio A, Wu X, Kumar A, Malik J, Mueller MW. 2023. 学习单一的近悬停位置控制器以适应极其不同的四旋翼。发表于IEEE国际机器人与自动化会议，页码1263–1269。IEEE'
- en: '[68] Eschmann J, Albani D, Loianno G. 2024. Learning to fly in seconds. IEEE
    Robot. Autom. Lett. 9(7):6336–6343'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Eschmann J, Albani D, Loianno G. 2024. 学习在几秒钟内飞行。IEEE机器人与自动化快报，9(7):6336–6343'
- en: '[69] Yang R, Zhang M, Hansen N, Xu H, Wang X. 2021. Learning vision-Guided
    quadrupedal locomotion end-to-end with cross-modal transformers. In Int. Conf.
    Learn. Represent.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Yang R, Zhang M, Hansen N, Xu H, Wang X. 2021. 使用跨模态变换器端到端学习视觉引导的四足运动。发表于国际学习表征会议'
- en: '[70] Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. 2017. Proximal
    policy optimization algorithms. arXiv preprint arXiv:1707.06347'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. 2017. 近端策略优化算法。arXiv预印本
    arXiv:1707.06347'
- en: '[71] Grizzle JW, Hurst J, Morris B, Park HW, Sreenath K. 2009. MABEL, a new
    robotic bipedal walker and runner. In Am. Control Conf., pp. 2030–2036\. IEEE'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Grizzle JW, Hurst J, Morris B, Park HW, Sreenath K. 2009. MABEL，一种新的机器人双足行走和跑步器。发表于美国控制会议，页码2030–2036。IEEE'
- en: '[72] Loquercio A, Kaufmann E, Ranftl R, Müller M, Koltun V, Scaramuzza D. 2021.
    Learning high-speed flight in the wild. Sci. Robot. 6(59):eabg5810'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Loquercio A, Kaufmann E, Ranftl R, Müller M, Koltun V, Scaramuzza D. 2021.
    在野外学习高速飞行。科学机器人，6(59):eabg5810'
- en: '[73] Tai L, Paolo G, Liu M. 2017. Virtual-to-real deep reinforcement learning:
    Continuous control of mobile robots for mapless navigation. In IEEE/RSJ Int. Conf.
    Intell. Robots Syst., pp. 31–36'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Tai L, Paolo G, Liu M. 2017. 虚拟到现实的深度强化学习：无地图导航的移动机器人连续控制。发表于IEEE/RSJ国际智能机器人系统会议，页码31–36'
- en: '[74] Xu Z, Liu B, Xiao X, Nair A, Stone P. 2023. Benchmarking Reinforcement
    Learning Techniques for Autonomous Navigation. In IEEE Int. Conf. Robot. Autom.,
    pp. 9224–9230'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Xu Z, Liu B, Xiao X, Nair A, Stone P. 2023. 基准测试用于自主导航的强化学习技术。发表于IEEE国际机器人与自动化会议，页码9224–9230'
- en: '[75] Chiang HTL, Faust A, Fiser M, Francis A. 2019. Learning navigation behaviors
    end-to-end with autorl. IEEE Robot. Autom. Lett. 4(2):2007–2014'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Chiang HTL, Faust A, Fiser M, Francis A. 2019. 通过自动强化学习端到端学习导航行为。IEEE机器人与自动化快报，4(2):2007–2014'
- en: '[76] Stein GJ, Bradley C, Roy N. 2018. Learning over subgoals for efficient
    navigation of structured, unknown environments. In Conf. Robot Learn., pp. 213–222\.
    PMLR'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Stein GJ, Bradley C, Roy N. 2018. 针对结构化未知环境的高效导航的子目标学习。发表于机器人学习会议，页码213–222。PMLR'
- en: '[77] Anderson P, Wu Q, Teney D, Bruce J, Johnson M, et al. 2018. Vision-and-language
    navigation: Interpreting visually-grounded navigation instructions in real environments.
    In IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3674–3683'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Anderson P, Wu Q, Teney D, Bruce J, Johnson M, 等. 2018. 视觉和语言导航：在真实环境中解释视觉基础的导航指令。发表于IEEE计算机视觉与模式识别会议，页码3674–3683'
- en: '[78] Zhu Y, Mottaghi R, Kolve E, Lim JJ, Gupta A, et al. 2017. Target-driven
    visual navigation in indoor scenes using deep reinforcement learning. In IEEE
    Int. Conf. Robot. Autom., pp. 3357–3364'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Zhu Y, Mottaghi R, Kolve E, Lim JJ, Gupta A, 等. 2017. 使用深度强化学习进行室内场景的目标驱动视觉导航。发表于IEEE国际机器人与自动化会议，页码3357–3364'
- en: '[79] Kahn G, Villaflor A, Ding B, Abbeel P, Levine S. 2018. Self-Supervised
    Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation.
    In IEEE Int. Conf. Robot. Autom., pp. 5129–5136\. IEEE'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Kahn G, Villaflor A, Ding B, Abbeel P, Levine S. 2018. 带有广义计算图的自监督深度强化学习用于机器人导航。在IEEE国际机器人与自动化会议上，第5129–5136页。IEEE'
- en: '[80] Wijmans E, Kadian A, Morcos A, Lee S, Essa I, et al. 2020a. DD-PPO: Learning
    Near-Perfect PointGoal Navigators from 2.5 Billion Frames. ArXiv:1911.00357 [cs]'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Wijmans E, Kadian A, Morcos A, Lee S, Essa I, et al. 2020a. DD-PPO：从25亿帧中学习近乎完美的目标导航。ArXiv:1911.00357
    [cs]'
- en: '[81] Chaplot DS, Gandhi DP, Gupta A, Salakhutdinov RR. 2020. Object goal navigation
    using goal-oriented semantic exploration. Adv. Neural Inf. Process. Syst. 33:4247–4258'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Chaplot DS, Gandhi DP, Gupta A, Salakhutdinov RR. 2020. 使用目标导向的语义探索进行物体目标导航。先进神经信息处理系统
    33:4247–4258'
- en: '[82] Gervet T, Chintala S, Batra D, Malik J, Chaplot DS. 2023. Navigating to
    objects in the real world. Sci. Robot. 8(79)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Gervet T, Chintala S, Batra D, Malik J, Chaplot DS. 2023. 在现实世界中导航至目标物体。Sci.
    Robot. 8(79)'
- en: '[83] Hoeller D, Wellhausen L, Farshidian F, Hutter M. 2021. Learning a State
    Representation and Navigation in Cluttered and Dynamic Environments. IEEE Robot.
    Autom. Lett. 6(3):5081–88'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Hoeller D, Wellhausen L, Farshidian F, Hutter M. 2021. 在拥挤和动态环境中学习状态表示和导航。IEEE机器人与自动化通讯
    6(3):5081–88'
- en: '[84] Savva M, Kadian A, Maksymets O, Zhao Y, Wijmans E, et al. 2019. Habitat:
    A platform for embodied ai research. In IEEE/CVF Int. Conf. Comput. Vis., pp.
    9339–9347'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Savva M, Kadian A, Maksymets O, Zhao Y, Wijmans E, et al. 2019. Habitat：一个用于具身人工智能研究的平台。在IEEE/CVF国际计算机视觉会议上，第9339–9347页'
- en: '[85] Kadian A, Truong J, Gokaslan A, Clegg A, Wijmans E, et al. 2020. Sim2real
    predictivity: Does evaluation in simulation predict real-world performance? IEEE
    Robot. Autom. Lett. 5(4):6670–6677'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Kadian A, Truong J, Gokaslan A, Clegg A, Wijmans E, et al. 2020. Sim2real预测性：在模拟中评估是否能预测现实世界的表现？IEEE机器人与自动化通讯
    5(4):6670–6677'
- en: '[86] Truong J, Rudolph M, Yokoyama NH, Chernova S, Batra D, Rai A. 2023. Rethinking
    sim2real: Lower fidelity simulation leads to higher sim2real transfer in navigation.
    In Conf. Robot Learn., pp. 859–70\. PMLR'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Truong J, Rudolph M, Yokoyama NH, Chernova S, Batra D, Rai A. 2023. 重新思考sim2real：较低保真度的模拟导致更高的sim2real迁移。在机器人学习会议上，第859–870页。PMLR'
- en: '[87] Truong J, Zitkovich A, Chernova S, Batra D, Zhang T, et al. 2024. Indoorsim-to-outdoorreal:
    learning to navigate outdoors without any outdoor experience. IEEE Robot. Autom.
    Lett.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Truong J, Zitkovich A, Chernova S, Batra D, Zhang T, et al. 2024. Indoorsim-to-outdoorreal：学习在没有户外经验的情况下进行户外导航。IEEE机器人与自动化通讯'
- en: '[88] Kahn G, Abbeel P, Levine S. 2021. BADGR: An Autonomous Self-Supervised
    Learning-Based Navigation System. IEEE Robot. Autom. Lett. 6(2):1312–1319'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Kahn G, Abbeel P, Levine S. 2021. BADGR：一个基于自主自我监督学习的导航系统。IEEE机器人与自动化通讯
    6(2):1312–1319'
- en: '[89] Shah D, Bhorkar A, Leen H, Kostrikov I, Rhinehart N, Levine S. 2023. Offline
    Reinforcement Learning for Visual Navigation. In Conf. Robot Learn., pp. 44–54\.
    PMLR'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Shah D, Bhorkar A, Leen H, Kostrikov I, Rhinehart N, Levine S. 2023. 用于视觉导航的离线强化学习。在机器人学习会议上，第44–54页。PMLR'
- en: '[90] Williams G, Wagener N, Goldfain B, Drews P, Rehg JM, et al. 2017. Information
    theoretic mpc for model-based reinforcement learning. In IEEE Int. Conf. Robot.
    Autom., pp. 1714–1721'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Williams G, Wagener N, Goldfain B, Drews P, Rehg JM, et al. 2017. 信息理论模型预测控制用于基于模型的强化学习。在IEEE国际机器人与自动化会议上，第1714–1721页'
- en: '[91] Stachowicz K, Shah D, Bhorkar A, Kostrikov I, Levine S. 2023. FastRLAP:
    A System for Learning High-Speed Driving via Deep RL and Autonomous Practicing.
    In Conf. Robot Learn., pp. 3100–3111\. PMLR'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Stachowicz K, Shah D, Bhorkar A, Kostrikov I, Levine S. 2023. FastRLAP：一个通过深度强化学习和自主练习学习高速驾驶的系统。在机器人学习会议上，第3100–3111页。PMLR'
- en: '[92] Kendall A, Hawke J, Janz D, Mazur P, Reda D, et al. 2019. Learning to
    drive in a day. In Int. Conf. Robot. Autom., pp. 8248–8254\. IEEE'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Kendall A, Hawke J, Janz D, Mazur P, Reda D, et al. 2019. 一天内学习驾驶。在国际机器人与自动化会议上，第8248–8254页。IEEE'
- en: '[93] Jang K, Lichtlé N, Vinitsky E, Shah A, Bunting M, et al. 2024. Reinforcement
    learning based oscillation dampening: Scaling up single-agent rl algorithms to
    a 100 av highway field operational test. arXiv preprint arXiv:2402.17050'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Jang K, Lichtlé N, Vinitsky E, Shah A, Bunting M, et al. 2024. 基于强化学习的振荡减缓：将单智能体强化学习算法扩展到100辆AV高速公路现场操作测试。arXiv预印本
    arXiv:2402.17050'
- en: '[94] Sorokin M, Tan J, Liu CK, Ha S. 2022. Learning to navigate sidewalks in
    outdoor environments. IEEE Robot. Autom. Lett. 7(2):3906–13'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Sorokin M, Tan J, Liu CK, Ha S. 2022. 学习在户外环境中导航人行道。IEEE机器人与自动化通讯 7(2):3906–13'
- en: '[95] Zhang C, Jin J, Frey J, Rudin N, Mattamala Aravena ME, et al. 2024. Resilient
    legged local navigation: Learning to traverse with compromised perception end-to-end.
    In IEEE Int. Conf. Robot. Autom.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Zhang C, Jin J, Frey J, Rudin N, Mattamala Aravena ME, 等. 2024. 弹性的腿式局部导航：学习以受损感知为终点的穿越。在
    IEEE 国际机器人与自动化会议上。'
- en: '[96] Hoeller D, Rudin N, Sako D, Hutter M. 2024. Anymal parkour: Learning agile
    navigation for quadrupedal robots. Sci. Robot. 9(88):eadi7566'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Hoeller D, Rudin N, Sako D, Hutter M. 2024. Anymal parkour：学习四足机器人的灵活导航。Sci.
    Robot. 9(88):eadi7566'
- en: '[97] Lee J, Bjelonic M, Reske A, Wellhausen L, Miki T, Hutter M. 2024. Learning
    robust autonomous navigation and locomotion for wheeled-legged robots. Sci. Robot.
    9(89):eadi9641'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Lee J, Bjelonic M, Reske A, Wellhausen L, Miki T, Hutter M. 2024. 学习轮式-腿式机器人的稳健自主导航和运动。Sci.
    Robot. 9(89):eadi9641'
- en: '[98] Miki T, Lee J, Wellhausen L, Hutter M. 2024. Learning to walk in confined
    spaces using 3d representation. In Int. Conf. Robot. Autom.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Miki T, Lee J, Wellhausen L, Hutter M. 2024. 学习在受限空间中使用 3D 表示步行。在国际机器人与自动化会议上。'
- en: '[99] Xu Z, Raj AH, Xiao X, Stone P. 2024. Dexterous Legged Locomotion in Confined
    3D Spaces with Reinforcement Learning. In Int. Conf. Robot. Autom.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Xu Z, Raj AH, Xiao X, Stone P. 2024. 基于强化学习的受限 3D 空间下的灵巧腿式运动。在国际机器人与自动化会议上。'
- en: '[100] He T, Zhang C, Xiao W, He G, Liu C, Shi G. 2024. Agile but safe: Learning
    collision-free high-speed legged locomotion. Robot.: Sci. Syst.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] He T, Zhang C, Xiao W, He G, Liu C, Shi G. 2024. 灵活但安全：学习无碰撞高速腿式运动。机器人：科学与系统。'
- en: '[101] Sadeghi F, Levine S. 2017. Cad2rl: Real single-image flight without a
    single real image. Robot.: Sci. Syst.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Sadeghi F, Levine S. 2017. Cad2rl：无实际图像的真实单图飞行。机器人：科学与系统。'
- en: '[102] Kang K, Belkhale S, Kahn G, Abbeel P, Levine S. 2019. Generalization
    through simulation: Integrating simulated and real data into deep reinforcement
    learning for vision-based autonomous flight. In Int. Conf. Robot. Autom., pp.
    6008–6014\. IEEE'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Kang K, Belkhale S, Kahn G, Abbeel P, Levine S. 2019. 通过模拟实现通用性：将模拟和真实数据整合到基于视觉的自主飞行的深度强化学习中。在国际机器人与自动化会议上，6008–6014
    页。IEEE'
- en: '[103] Romero A, Song Y, Scaramuzza D. 2024. Actor-critic model predictive control.
    In Int. Conf. Robot. Autom.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Romero A, Song Y, Scaramuzza D. 2024. 演员-评论模型预测控制。在国际机器人与自动化会议上。'
- en: '[104] Xie L, Wang S, Markham A, Trigoni N. 2017. Towards monocular vision based
    obstacle avoidance through deep reinforcement learning. arXiv preprint arXiv:1706.09829'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Xie L, Wang S, Markham A, Trigoni N. 2017. 通过深度强化学习实现基于单目视觉的避障。arXiv
    预印本 arXiv:1706.09829'
- en: '[105] Wijmans E, Kadian A, Morcos A, Lee S, Essa I, et al. 2020b. DD-PPO: Learning
    Near-Perfect PointGoal Navigators from 2.5 Billion Frames. In Int. Conf. Learn.
    Represent.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Wijmans E, Kadian A, Morcos A, Lee S, Essa I, 等. 2020b. DD-PPO：通过 25
    亿帧学习接近完美的点目标导航。在学习表示国际会议上。'
- en: '[106] Wijmans E, Savva M, Essa I, Lee S, Morcos AS, Batra D. 2023. Emergence
    of Maps in the Memories of Blind Navigation Agents. In 11th Int. Conf. Learn.
    Represent.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Wijmans E, Savva M, Essa I, Lee S, Morcos AS, Batra D. 2023. 盲目导航代理的记忆中地图的出现。在第
    11 届国际学习表示会议上。'
- en: '[107] Rosinol A, Leonard JJ, Carlone L. 2023. Nerf-slam: Real-time dense monocular
    slam with neural radiance fields. In 2023 IEEE/RSJ Int. Conf. Intell. Robots Syst.,
    pp. 3437–3444'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Rosinol A, Leonard JJ, Carlone L. 2023. Nerf-slam: 使用神经辐射场进行实时稠密单目 SLAM。在
    2023 年 IEEE/RSJ 国际智能机器人系统会议上，3437–3444 页'
- en: '[108] Mahler J, Matl M, Satish V, Danielczuk M, DeRose B, et al. 2019. Learning
    ambidextrous robot grasping policies. Sci. Robot. 4(26):eaau4984'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Mahler J, Matl M, Satish V, Danielczuk M, DeRose B, 等. 2019. 学习双手机器人抓取策略。Sci.
    Robot. 4(26):eaau4984'
- en: '[109] Zeng A, Song S, Welker S, Lee J, Rodriguez A, Funkhouser T. 2018. Learning
    synergies between pushing and grasping with self-supervised deep reinforcement
    learning. In IEEE/RSJ Int. Conf. Intell. Robots Syst., pp. 4238–4245\. IEEE'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Zeng A, Song S, Welker S, Lee J, Rodriguez A, Funkhouser T. 2018. 通过自监督深度强化学习学习推动和抓取之间的协同作用。在
    IEEE/RSJ 国际智能机器人系统会议上，4238–4245 页。IEEE'
- en: '[110] Kalashnikov D, Irpan A, Pastor P, Ibarz J, Herzog A, et al. 2018. Scalable
    deep reinforcement learning for vision-based robotic manipulation. In Conf. Robot.
    Learn., pp. 651–73'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Kalashnikov D, Irpan A, Pastor P, Ibarz J, Herzog A, 等. 2018. 视觉导向机器人操作的可扩展深度强化学习。在机器人学习会议上，651–73
    页'
- en: '[111] James S, Wohlhart P, Kalakrishnan M, Kalashnikov D, Irpan A, et al. 2019.
    Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical
    adaptation networks. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp.
    12627–12637'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] James S, Wohlhart P, Kalakrishnan M, Kalashnikov D, Irpan A, 等. 2019.
    通过 sim-to-sim 实现 sim-to-real：通过随机到标准适应网络实现数据-高效机器人抓取。在 IEEE/CVF 计算机视觉与模式识别会议文集上，12627–12637
    页'
- en: '[112] Wang D, Jia M, Zhu X, Walters R, Platt R. 2023a. On-Robot Learning With
    Equivariant Models. In Conf. on Robot Learn., pp. 1345–1354\. PMLR'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Wang D, Jia M, Zhu X, Walters R, Platt R. 2023a. 使用等变模型进行机器人学习. 见于 Conf.
    on Robot Learn., 第 1345–1354 页\. PMLR'
- en: '[113] Levine S, Finn C, Darrell T, Abbeel P. 2016. End-to-end training of deep
    visuomotor policies. J. Mach. Learn. Res. 17(1):1334–1373'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Levine S, Finn C, Darrell T, Abbeel P. 2016. 深度视觉运动策略的端到端训练. J. Mach.
    Learn. Res. 17(1):1334–1373'
- en: '[114] Kalashnikov D, Varley J, Chebotar Y, Swanson B, Jonschkowski R, et al.
    2022. Scaling up multi-task robotic reinforcement learning. In Conf. Robot Learn.,
    pp. 557–575\. PMLR'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Kalashnikov D, Varley J, Chebotar Y, Swanson B, Jonschkowski R, 等. 2022.
    扩展多任务机器人强化学习. 见于 Conf. Robot Learn., 第 557–575 页\. PMLR'
- en: '[115] Chebotar Y, Hausman K, Lu Y, Xiao T, Kalashnikov D, et al. 2021. Actionable
    Models: Unsupervised Offline Reinforcement Learning of Robotic Skills. In Int.
    Conf. Mach. Learn., pp. 1518–1528\. PMLR'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Chebotar Y, Hausman K, Lu Y, Xiao T, Kalashnikov D, 等. 2021. 可操作模型：机器人技能的无监督离线强化学习.
    见于 Int. Conf. Mach. Learn., 第 1518–1528 页\. PMLR'
- en: '[116] Lee AX, Devin CM, Zhou Y, Lampe T, Bousmalis K, et al. 2021. Beyond pick-and-place:
    Tackling robotic stacking of diverse shapes. In Conf. on Robot Learn.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Lee AX, Devin CM, Zhou Y, Lampe T, Bousmalis K, 等. 2021. 超越捡取和放置：应对不同形状的机器人堆叠.
    见于 Conf. on Robot Learn.'
- en: '[117] Walke HR, Yang JH, Yu A, Kumar A, Orbik J, et al. 2023. Don’t start from
    scratch: Leveraging prior data to automate robotic reinforcement learning. In
    Conf. Robot Learn., pp. 1652–1662'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Walke HR, Yang JH, Yu A, Kumar A, Orbik J, 等. 2023. 不要从头开始：利用先前数据来自动化机器人强化学习.
    见于 Conf. Robot Learn., 第 1652–1662 页'
- en: '[118] Ebert F, Finn C, Dasari S, Xie A, Lee A, Levine S. 2018. Visual foresight:
    Model-based deep reinforcement learning for vision-based robotic control. arXiv
    preprint arXiv:1812.00568'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Ebert F, Finn C, Dasari S, Xie A, Lee A, Levine S. 2018. 视觉预见：基于模型的深度强化学习用于视觉基础的机器人控制.
    arXiv 预印本 arXiv:1812.00568'
- en: '[119] Riedmiller M, Hafner R, Lampe T, Neunert M, Degrave J, et al. 2018. Learning
    by Playing Solving Sparse Reward Tasks from Scratch. In Int. Conf. Mach. Learn.,
    pp. 4344–4353\. PMLR'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Riedmiller M, Hafner R, Lampe T, Neunert M, Degrave J, 等. 2018. 通过玩耍学习从头解决稀疏奖励任务.
    见于 Int. Conf. Mach. Learn., 第 4344–4353 页\. PMLR'
- en: '[120] Zhu H, Yu J, Gupta A, Shah D, Hartikainen K, et al. 2020. The Ingredients
    of Real World Robotic Reinforcement Learning. In Int. Conf. Learn. Represent.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Zhu H, Yu J, Gupta A, Shah D, Hartikainen K, 等. 2020. 现实世界机器人强化学习的要素.
    见于 Int. Conf. Learn. Represent.'
- en: '[121] Ma YJ, Sodhani S, Jayaraman D, Bastani O, Kumar V, Zhang A. 2022a. VIP:
    Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training.
    In Int. Conf. Learn. Represent.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Ma YJ, Sodhani S, Jayaraman D, Bastani O, Kumar V, Zhang A. 2022a. VIP:
    通过价值隐式预训练实现通用视觉奖励和表示. 见于 Int. Conf. Learn. Represent.'
- en: '[122] Nair A, Gupta A, Dalal M, Levine S. 2020. Awac: Accelerating online reinforcement
    learning with offline datasets. arXiv preprint arXiv:2006.09359'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Nair A, Gupta A, Dalal M, Levine S. 2020. Awac: 通过离线数据集加速在线强化学习. arXiv
    预印本 arXiv:2006.09359'
- en: '[123] Nasiriany S, Liu H, Zhu Y. 2022. Augmenting reinforcement learning with
    behavior primitives for diverse manipulation tasks. In Int. Conf. Robot. Autom.,
    pp. 7477–7484\. IEEE'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Nasiriany S, Liu H, Zhu Y. 2022. 用行为原语增强强化学习以应对多样化的操作任务. 见于 Int. Conf.
    Robot. Autom., 第 7477–7484 页\. IEEE'
- en: '[124] Chebotar Y, Vuong Q, Hausman K, Xia F, Lu Y, et al. 2023. Q-Transformer:
    Scalable offline reinforcement learning via autoregressive Q-functions. In Conf.
    on Robot Learn., pp. 3909–3928\. PMLR'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Chebotar Y, Vuong Q, Hausman K, Xia F, Lu Y, 等. 2023. Q-Transformer:
    通过自回归 Q 函数实现可扩展的离线强化学习. 见于 Conf. on Robot Learn., 第 3909–3928 页\. PMLR'
- en: '[125] Nair AV, Pong V, Dalal M, Bahl S, Lin S, Levine S. 2018. Visual reinforcement
    learning with imagined goals. In Adv. Neural Inf. Process. Syst., vol. 31'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Nair AV, Pong V, Dalal M, Bahl S, Lin S, Levine S. 2018. 具有想象目标的视觉强化学习.
    见于 Adv. Neural Inf. Process. Syst., 卷 31'
- en: '[126] Johannink T, Bahl S, Nair A, Luo J, Kumar A, et al. 2019. Residual reinforcement
    learning for robot control. In Int. Conf. Robot Autom., pp. 6023–6029'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Johannink T, Bahl S, Nair A, Luo J, Kumar A, 等. 2019. 用于机器人控制的残差强化学习.
    见于 Int. Conf. Robot Autom., 第 6023–6029 页'
- en: '[127] Vecerik M, Hester T, Scholz J, Wang F, Pietquin O, et al. 2017. Leveraging
    demonstrations for deep reinforcement learning on robotics problems with sparse
    rewards. arXiv preprint arXiv:1707.08817'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Vecerik M, Hester T, Scholz J, Wang F, Pietquin O, 等. 2017. 利用演示进行深度强化学习以解决稀疏奖励的机器人问题.
    arXiv 预印本 arXiv:1707.08817'
- en: '[128] Luo J, Sushkov O, Pevceviciute R, Lian W, Su C, et al. 2021. Robust multi-modal
    policies for industrial assembly via reinforcement learning and demonstrations:
    A large-scale study. In Robot.: Sci. Syst.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Luo J, Sushkov O, Pevceviciute R, Lian W, Su C, 等. 2021. 通过强化学习和演示进行工业组装的鲁棒多模态策略：大规模研究.
    见于 Robot.: Sci. Syst.'
- en: '[129] Zhao TZ, Luo J, Sushkov O, Pevceviciute R, Heess N, et al. 2022. Offline
    meta-reinforcement learning for industrial insertion. In IEEE Int. Conf. Robot.
    Autom., pp. 6386–6393'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Zhao TZ, Luo J, Sushkov O, Pevceviciute R, Heess N, 等. 2022. 用于工业插入的离线元强化学习。见于IEEE国际机器人与自动化会议，pp.
    6386–6393'
- en: '[130] Tang B, Lin MA, Akinola I, Handa A, Sukhatme GS, et al. 2023. IndustReal:
    Transferring contact-rich assembly tasks from simulation to reality. In Robot.:
    Sci. and Sys.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Tang B, Lin MA, Akinola I, Handa A, Sukhatme GS, 等. 2023. IndustReal：将丰富接触的装配任务从仿真转移到现实。见于机器人：科学与系统'
- en: '[131] Chebotar Y, Handa A, Makoviychuk V, Macklin M, Issac J, et al. 2019.
    Closing the sim-to-real loop: Adapting simulation randomization with real world
    experience. In IEEE Int. Conf. Robot. Autom., pp. 8973–8979\. IEEE'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Chebotar Y, Handa A, Makoviychuk V, Macklin M, Issac J, 等. 2019. 关闭仿真到现实的回路：用现实世界经验调整仿真随机化。见于IEEE国际机器人与自动化会议，pp.
    8973–8979。IEEE'
- en: '[132] Abbatematteo B, Rosen E, Thompson S, Akbulut T, Rammohan S, Konidaris
    G. 2024. Composable interaction primitives: A structured policy class for efficiently
    learning sustained-contact manipulation skills. In IEEE Int. Conf. Robot. Autom.
    IEEE'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Abbatematteo B, Rosen E, Thompson S, Akbulut T, Rammohan S, Konidaris
    G. 2024. 可组合的交互原语：一个结构化策略类，用于高效学习持续接触操作技能。见于IEEE国际机器人与自动化会议。IEEE'
- en: '[133] Wu R, Zhao Y, Mo K, Guo Z, Wang Y, et al. 2022. VAT-Mart: Learning visual
    action trajectory proposals for manipulating 3D articulated objects. In Int. Conf.
    Learn. Represent.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Wu R, Zhao Y, Mo K, Guo Z, Wang Y, 等. 2022. VAT-Mart：学习视觉动作轨迹提议以操控3D关节物体。见于学习表征国际会议'
- en: '[134] Matas J, James S, Davison AJ. 2018. Sim-to-real reinforcement learning
    for deformable object manipulation. In Conf. on Robot Learn., pp. 734–743'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Matas J, James S, Davison AJ. 2018. 用于可变形物体操控的仿真到现实强化学习。见于机器人学习会议，pp.
    734–743'
- en: '[135] Wu Y, Yan W, Kurutach T, Pinto L, Abbeel P. 2020. Learning to manipulate
    deformable objects without demonstrations. In Robot: Sci. Syst.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Wu Y, Yan W, Kurutach T, Pinto L, Abbeel P. 2020. 学习在没有演示的情况下操作可变形物体。见于机器人：科学与系统'
- en: '[136] Avigal Y, Berscheid L, Asfour T, Kröger T, Goldberg K. 2022. Speedfolding:
    Learning efficient bimanual folding of garments. In IEEE/RSJ Int. Conf. Intell.
    Robots Syst., pp. 1–8\. IEEE'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Avigal Y, Berscheid L, Asfour T, Kröger T, Goldberg K. 2022. Speedfolding：学习高效的双手折叠服装。见于IEEE/RSJ国际智能机器人系统会议，pp.
    1–8。IEEE'
- en: '[137] Wang Y, Sun Z, Erickson Z, Held D. 2023b. One policy to dress them all:
    Learning to dress people with diverse poses and garments. In Robot.: Sci. and
    Syst.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Wang Y, Sun Z, Erickson Z, Held D. 2023b. 一种策略适用于所有服装：学习为具有多样姿势和服装的人穿衣。见于机器人：科学与系统'
- en: '[138] Andrychowicz OM, Baker B, Chociej M, Jozefowicz R, McGrew B, et al. 2020.
    Learning dexterous in-hand manipulation. Int. J. Robot. Res. 39(1):3–20'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Andrychowicz OM, Baker B, Chociej M, Jozefowicz R, McGrew B, 等. 2020.
    学习灵巧的手内操作。国际机器人研究杂志 39(1):3–20'
- en: '[139] Handa A, Allshire A, Makoviychuk V, Petrenko A, Singh R, et al. 2023.
    Dextreme: Transfer of agile in-hand manipulation from simulation to reality. In
    IEEE Int. Conf. Robot. and Autom., pp. 5977–5984'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Handa A, Allshire A, Makoviychuk V, Petrenko A, Singh R, 等. 2023. Dextreme：从仿真到现实的灵活手内操作转移。见于IEEE国际机器人与自动化会议，pp.
    5977–5984'
- en: '[140] Nagabandi A, Konolige K, Levine S, Kumar V. 2020. Deep Dynamics Models
    for Learning Dexterous Manipulation. In Proc. Conf. Robot. Learn.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Nagabandi A, Konolige K, Levine S, Kumar V. 2020. 深度动力学模型用于学习灵巧操作。见于机器人学习会议。'
- en: '[141] Qi H, Yi B, Suresh S, Lambeta M, Ma Y, et al. 2023. General in-hand object
    rotation with vision and touch. In Conf. on Robot Learn., pp. 2549–2564\. PMLR'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Qi H, Yi B, Suresh S, Lambeta M, Ma Y, 等. 2023. 通用手内物体旋转结合视觉和触觉。见于机器人学习会议，pp.
    2549–2564。PMLR'
- en: '[142] Chen T, Tippur M, Wu S, Kumar V, Adelson E, Agrawal P. 2023. Visual dexterity:
    In-hand reorientation of novel and complex object shapes. Sci. Robot. 8(84):eadc9244'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Chen T, Tippur M, Wu S, Kumar V, Adelson E, Agrawal P. 2023. 视觉灵巧性：新颖和复杂物体形状的手内重新定位。科学机器人
    8(84):eadc9244'
- en: '[143] Zhou W, Held D. 2023. Learning to grasp the ungraspable with emergent
    extrinsic dexterity. In Conf. Robot Learn., pp. 150–160\. PMLR'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Zhou W, Held D. 2023. 学习用新兴的外在灵巧性抓取不可抓取的物体。见于机器人学习会议，pp. 150–160。PMLR'
- en: '[144] Zhou W, Jiang B, Yang F, Paxton C, Held D. 2023. HACMan: Learning hybrid
    actor-critic maps for 6D non-prehensile manipulation. In Conf. Robot Learn. PMLR'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Zhou W, Jiang B, Yang F, Paxton C, Held D. 2023. HACMan：学习用于6D非抓取操作的混合演员-评论者映射。见于机器人学习会议。PMLR'
- en: '[145] Cho Y, Han J, Cho Y, Kim B. 2024. CORN: Contact-based object representation
    for nonprehensile manipulation of general unseen objects. In Int. Conf. on Learn.
    Represent.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Cho Y, Han J, Cho Y, Kim B. 2024. CORN：基于接触的对象表征用于一般未见物体的非抓取操作。见于学习表征国际会议'
- en: '[146] Lv J, Feng Y, Zhang C, Zhao S, Shao L, Lu C. 2023. SAM-RL: Sensing-Aware
    Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation
    and Rendering. In Robot.: Sci. Sys.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Lv J, Feng Y, Zhang C, Zhao S, Shao L, Lu C. 2023. SAM-RL: 通过可微物理仿真与渲染进行感知感知模型基础的强化学习。机器人学：科学与系统'
- en: '[147] Van Wyk K, Handa A, Makoviychuk V, Guo Y, Allshire A, Ratliff ND. 2024.
    Geometric fabrics: a safe guiding medium for policy learning. arXiv preprint arXiv:2405.02250'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Van Wyk K, Handa A, Makoviychuk V, Guo Y, Allshire A, Ratliff ND. 2024.
    几何织物：一种安全的政策学习引导介质。arXiv 预印本 arXiv:2405.02250'
- en: '[148] Chitnis R, Tulsiani S, Gupta S, Gupta A. 2020. Efficient bimanual manipulation
    using learned task schemas. In IEEE Int. Conf. Robot. Autom., pp. 1149–1155'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Chitnis R, Tulsiani S, Gupta S, Gupta A. 2020. 使用学习任务模型进行高效的双手操作。IEEE
    国际机器人与自动化会议，pp. 1149–1155'
- en: '[149] Büchler D, Guist S, Calandra R, Berenz V, Schölkopf B, Peters J. 2022.
    Learning to play table tennis from scratch using muscular robots. IEEE Trans.
    Robot. 38(6):3850–3860'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Büchler D, Guist S, Calandra R, Berenz V, Schölkopf B, Peters J. 2022.
    从头开始使用肌肉机器人学习打乒乓球。IEEE 机器人学报 38(6):3850–3860'
- en: '[150] Cheng S, Xu D. 2023. League: Guided skill learning and abstraction for
    long-horizon manipulation. IEEE Robot. Autom. Lett.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Cheng S, Xu D. 2023. League: 长期操作的指导性技能学习与抽象。IEEE 机器人与自动化快报。'
- en: '[151] Funk N, Chalvatzaki G, Belousov B, Peters J. 2022. Learn2assemble with
    structured representations and search for robotic architectural construction.
    In Conf. Robot. Learn., pp. 1401–1411'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Funk N, Chalvatzaki G, Belousov B, Peters J. 2022. 使用结构化表示和搜索进行机器人建筑施工的Learn2assemble。机器人学习会议，pp.
    1401–1411'
- en: '[152] Wang C, Zhang Q, Tian Q, Li S, Wang X, et al. 2020. Learning mobile manipulation
    through deep reinforcement learning. Sensors 20(3):939'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Wang C, Zhang Q, Tian Q, Li S, Wang X, et al. 2020. 通过深度强化学习学习移动操控。传感器
    20(3):939'
- en: '[153] Ma Y, Farshidian F, Miki T, Lee J, Hutter M. 2022b. Combining learning-based
    locomotion policy with model-based manipulation for legged mobile manipulators.
    IEEE Robot. Autom. Lett. 7(2):2377–2384'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Ma Y, Farshidian F, Miki T, Lee J, Hutter M. 2022b. 将基于学习的运动策略与基于模型的操作结合用于腿部移动操控。IEEE
    机器人与自动化快报 7(2):2377–2384'
- en: '[154] Fu Z, Cheng X, Pathak D. 2023. Deep whole-body control: learning a unified
    policy for manipulation and locomotion. In Conf. on Robot Learn., pp. 138–149\.
    PMLR'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Fu Z, Cheng X, Pathak D. 2023. 深度全身控制：学习用于操控和运动的统一策略。机器人学习会议，pp. 138–149。PMLR'
- en: '[155] Fu Z, Zhao Q, Wu Q, Wetzstein G, Finn C. 2024. Humanplus: Humanoid shadowing
    and imitation from humans. arXiv preprint arXiv:2406.10454'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Fu Z, Zhao Q, Wu Q, Wetzstein G, Finn C. 2024. Humanplus: 人形机器人模仿与影子学习。arXiv
    预印本 arXiv:2406.10454'
- en: '[156] Sentis L, Khatib O. 2006. A whole-body control framework for humanoids
    operating in human environments. In IEEE Int. Conf. Robot. Autom., pp. 2641–2648\.
    IEEE'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Sentis L, Khatib O. 2006. 用于在人工环境中操作的类人机器人全身控制框架。IEEE 国际机器人与自动化会议，pp.
    2641–2648。IEEE'
- en: '[157] Yokoyama N, Clegg AW, Undersander E, Ha S, Batra D, Rai A. 2023. Adaptive
    skill coordination for robotic mobile manipulation. IEEE Robot. Autom. Lett.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Yokoyama N, Clegg AW, Undersander E, Ha S, Batra D, Rai A. 2023. 机器人移动操控的自适应技能协调。IEEE
    机器人与自动化快报'
- en: '[158] Ji Y, Li Z, Sun Y, Peng XB, Levine S, et al. 2022. Hierarchical reinforcement
    learning for precise soccer shooting skills using a quadrupedal robot. In IEEE/RSJ
    Int. Conf. Intell. Robots Syst., pp. 1479–1486\. IEEE'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Ji Y, Li Z, Sun Y, Peng XB, Levine S, et al. 2022. 使用四足机器人进行精确足球射门技能的分层强化学习。IEEE/RSJ
    国际智能机器人系统会议，pp. 1479–1486。IEEE'
- en: '[159] Liu M, Chen Z, Cheng X, Ji Y, Yang R, Wang X. 2024. Visual whole-body
    control for legged loco-manipulation. arXiv preprint arXiv:2403.16967'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Liu M, Chen Z, Cheng X, Ji Y, Yang R, Wang X. 2024. 用于腿部运动操作的视觉全身控制。arXiv
    预印本 arXiv:2403.16967'
- en: '[160] Sun C, Orbik J, Devin CM, Yang BH, Gupta A, et al. 2022. Fully autonomous
    real-world reinforcement learning with applications to mobile manipulation. In
    Conf. on Robot Learn., pp. 308–319\. PMLR'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Sun C, Orbik J, Devin CM, Yang BH, Gupta A, et al. 2022. 完全自主的现实世界强化学习及其在移动操控中的应用。机器人学习会议，pp.
    308–319。PMLR'
- en: '[161] Jauhri S, Peters J, Chalvatzaki G. 2022. Robot learning of mobile manipulation
    with reachability behavior priors. IEEE Robot. Autom. Lett. 7(3):8399–8406'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Jauhri S, Peters J, Chalvatzaki G. 2022. 具有可达性行为先验的移动操控机器人学习。IEEE 机器人与自动化快报
    7(3):8399–8406'
- en: '[162] Ji Y, Margolis GB, Agrawal P. 2023. Dribblebot: Dynamic legged manipulation
    in the wild. In IEEE Int. Conf. Robot. Autom., pp. 5155–5162\. IEEE'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Ji Y, Margolis GB, Agrawal P. 2023. Dribblebot: 在野外的动态腿部操控。IEEE 国际机器人与自动化会议，pp.
    5155–5162。IEEE'
- en: '[163] Hu J, Stone P, Martín-Martín R. 2023. Causal Policy Gradient for Whole-Body
    Mobile Manipulation. In Robot.: Sci. and Syst.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] 胡俊，斯通，马丁-马丁。2023。全身移动操控的因果策略梯度。机器人：科学与系统'
- en: '[164] Honerkamp D, Welschehold T, Valada A. 2023. N²M²: Learning navigation
    for arbitrary mobile manipulation motions in unseen and dynamic environments.
    IEEE Trans. Robot.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] 霍内坎普，维尔舍霍尔德，瓦拉达。2023。N²M²：在未见和动态环境中学习任意移动操控动作。IEEE机器人学报'
- en: '[165] Uppal S, Agarwal A, Xiong H, Shaw K, Pathak D. 2024. SPIN: Simultaneous
    perception interaction and navigation. In IEEE/CVF Conf. Comput. Vis. Pattern
    Recognit., pp. 18133–18142'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] 乌帕尔，阿加瓦尔，熊辉，肖克，帕塔克。2024。SPIN：同时感知、交互与导航。IEEE/CVF 计算机视觉与模式识别会议论文集，pp.
    18133–18142'
- en: '[166] Kumar KN, Essa I, Ha S. 2023. Cascaded compositional residual learning
    for complex interactive behaviors. IEEE Robot. Autom. Lett. 8(8):4601–4608'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] 库马尔，埃萨，哈施。2023。用于复杂互动行为的级联组成残差学习。IEEE机器人与自动化快报 8(8):4601–4608'
- en: '[167] Yang R, Kim Y, Kembhavi A, Wang X, Ehsani K. 2023b. Harmonic mobile manipulation.
    arXiv preprint arXiv:2312.06639'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] 杨瑞，金瑜，凯姆巴维，王旭，埃赫萨尼。2023b。和谐移动操控。arXiv 预印本 arXiv:2312.06639'
- en: '[168] Xiong H, Mendonca R, Shaw K, Pathak D. 2024. Adaptive mobile manipulation
    for articulated objects in the open world. arXiv preprint arXiv:2401.14403'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] 熊辉，门东卡，肖克，帕塔克。2024。开放世界中关节物体的自适应移动操控。arXiv 预印本 arXiv:2401.14403'
- en: '[169] Cheng X, Kumar A, Pathak D. 2023. Legs as Manipulator: Pushing Quadrupedal
    Agility Beyond Locomotion. In IEEE Int. Conf. Robot. and Autom.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] 程晓，库马尔，帕塔克。2023。作为操控器的腿：将四足灵活性推向超越运动的方向。IEEE国际机器人与自动化会议'
- en: '[170] Herzog A, Rao K, Hausman K, Lu Y, Wohlhart P, et al. 2023. Deep rl at
    scale: Sorting waste in office buildings with a fleet of mobile manipulators.
    In Robot.: Sci. and Syst.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] 赫尔佐格，拉奥，豪斯曼，陆洋，沃尔哈特等。2023。大规模深度强化学习：用一队移动操控器对办公室建筑进行垃圾分类。机器人：科学与系统'
- en: '[171] Wu B, Martin-Martin R, Fei-Fei L. 2023. M-EMBER: Tackling Long-Horizon
    Mobile Manipulation via Factorized Domain Transfer. In IEEE Int. Conf. Robot.
    Autom.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] 吴博，马丁-马丁，费伊-费伊。2023。M-EMBER：通过因子化领域迁移解决长时间跨度的移动操控问题。IEEE国际机器人与自动化会议'
- en: '[172] Ghadirzadeh A, Chen X, Yin W, Yi Z, Björkman M, Kragic D. 2020. Human-centered
    collaborative robots with deep reinforcement learning. IEEE Robot. Autom. Lett.
    6(2):566–571'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] 加迪尔扎德，陈鑫，尹伟，易志，比约克曼，克拉吉克。2020。以深度强化学习为基础的人机协作机器人。IEEE机器人与自动化快报 6(2):566–571'
- en: '[173] Christen S, Feng L, Yang W, Chao YW, Hilliges O, Song J. 2024. Synh2r:
    Synthesizing hand-object motions for learning human-to-robot handovers. Int. Conf.
    Robot. Autom.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] 克里斯滕，冯乐，杨伟，赵云伟，希利格斯，宋佳。2024。Synh2r：为学习人机交接合成手部-物体动作。国际机器人与自动化会议'
- en: '[174] Christen S, Yang W, Pérez-D’Arpino C, Hilliges O, Fox D, Chao YW. 2023.
    Learning human-to-robot handovers from point clouds. In IEEE/CVF Conf. Comput.
    Vis. Pattern Recognit., pp. 9654–9664'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] 克里斯滕，杨伟，佩雷斯-达尔皮诺，希利格斯，福克斯，赵云伟。2023。从点云中学习人机交接。IEEE/CVF 计算机视觉与模式识别会议论文集，pp.
    9654–9664'
- en: '[175] Chen YF, Everett M, Liu M, How JP. 2017a. Socially aware motion planning
    with deep reinforcement learning. In IEEE/RSJ Int. Conf. Intell. Robots Syst.,
    pp. 1343–1350\. IEEE'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] 陈永丰，埃弗雷特，刘明，霍普。2017a。基于深度强化学习的社会感知运动规划。IEEE/RSJ 智能机器人与系统国际会议论文集，pp. 1343–1350。IEEE'
- en: '[176] Everett M, Chen YF, How JP. 2021. Collision avoidance in pedestrian-rich
    environments with deep reinforcement learning. IEEE Access 9:10357–10377'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] 埃弗雷特，陈永丰，霍普。2021。在行人密集环境中的碰撞避免，使用深度强化学习。IEEE Access 9:10357–10377'
- en: '[177] Liang J, Patel U, Sathyamoorthy AJ, Manocha D. 2021. Crowd-steer: Realtime
    smooth and collision-free robot navigation in densely crowded scenarios trained
    using high-fidelity simulation. In Int. Conf. Int. Joint Conf. Artif. Intell.,
    pp. 4221–4228'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] 梁杰，帕特尔，萨蒂亚穆尔提，马诺查。2021。Crowd-steer：在高度拥挤的场景中，通过高保真仿真训练实现实时平滑且无碰撞的机器人导航。国际人工智能联合会议论文集，pp.
    4221–4228'
- en: '[178] Hirose N, Shah D, Stachowicz K, Sridhar A, Levine S. 2024. Selfi: Autonomous
    self-improvement with reinforcement learning for social navigation. arXiv preprint
    arXiv:2403.00991'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] 广濑直，人 Shah，斯塔乔维奇，斯里达尔，莱文。2024。Selfi：通过强化学习实现社会导航的自主自我改进。arXiv 预印本 arXiv:2403.00991'
- en: '[179] Liu P, Zhang K, Tateo D, Jauhri S, Hu Z, et al. 2023. Safe reinforcement
    learning of dynamic high-dimensional robotic tasks: navigation, manipulation,
    interaction. In IEEE Int. Conf. Robot. Autom., pp. 9449–9456\. IEEE'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] 刘鹏，张凯，塔特奥，乔赫里，胡哲等。2023。动态高维机器人任务的安全强化学习：导航、操控、交互。IEEE国际机器人与自动化会议论文集，pp.
    9449–9456。IEEE'
- en: '[180] Dimeas F, Aspragathos N. 2015. Reinforcement learning of variable admittance
    control for human-robot co-manipulation. In IEEE/RSJ Int. Conf. Intell. Robots
    Syst., pp. 1011–1016'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Dimeas F, Aspragathos N. 2015. 人机协作控制的可变顺应性控制的强化学习。在 IEEE/RSJ 国际智能机器人系统会议上，页码1011–1016'
- en: '[181] Nair S, Mitchell E, Chen K, Savarese S, Finn C, et al. 2022. Learning
    language-conditioned robot behavior from offline data and crowd-sourced annotation.
    In Conf. Robot Learn., pp. 1303–1315\. PMLR'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Nair S, Mitchell E, Chen K, Savarese S, Finn C, 等. 2022. 从离线数据和众包标注中学习语言条件的机器人行为。在机器人学习会议上，页码1303–1315。PMLR'
- en: '[182] Reddy S, Dragan AD, Levine S. 2018. Shared autonomy via deep reinforcement
    learning. In Robot.: Sci. and Sys.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Reddy S, Dragan AD, Levine S. 2018. 通过深度强化学习实现共享自主。在机器人科学与系统会议上'
- en: '[183] Schaff C, Walter MR. 2020. Residual policy learning for shared autonomy.
    In Robot. Sci. Syst.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Schaff C, Walter MR. 2020. 用于共享自主的残差策略学习。在机器人科学与系统会议上'
- en: '[184] Chen YF, Liu M, Everett M, How JP. 2017b. Decentralized non-communicating
    multiagent collision avoidance with deep reinforcement learning. In IEEE Int.
    Conf. Robot. Autom., pp. 285–292\. IEEE'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Chen YF, Liu M, Everett M, How JP. 2017b. 基于深度强化学习的去中心化非通信多智能体碰撞避免。在
    IEEE 国际机器人与自动化会议上，页码285–292。IEEE'
- en: '[185] Everett M, Chen YF, How JP. 2018. Motion planning among dynamic, decision-making
    agents with deep reinforcement learning. In IEEE/RSJ Int. Conf. Intell. Robots
    Syst., pp. 3052–3059'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Everett M, Chen YF, How JP. 2018. 基于深度强化学习的动态决策代理的运动规划。在 IEEE/RSJ 国际智能机器人系统会议上，页码3052–3059'
- en: '[186] Van Den Berg J, Guy SJ, Lin M, Manocha D. 2011. Reciprocal n-body collision
    avoidance. In Int. Symp. Robot. Res., pp. 3–19\. Springer'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Van Den Berg J, Guy SJ, Lin M, Manocha D. 2011. 互惠n体碰撞避免。在国际机器人研究研讨会上，页码3–19。Springer'
- en: '[187] Fan T, Long P, Liu W, Pan J. 2020. Distributed multi-robot collision
    avoidance via deep reinforcement learning for navigation in complex scenarios.
    Int. J. Robot. Res. 39(7):856–892'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Fan T, Long P, Liu W, Pan J. 2020. 通过深度强化学习在复杂场景中进行的分布式多机器人碰撞避免。国际机器人研究期刊
    39(7):856–892'
- en: '[188] Han R, Chen S, Wang S, Zhang Z, Gao R, et al. 2022. Reinforcement learned
    distributed multi-robot navigation with reciprocal velocity obstacle shaped rewards.
    IEEE Robot. Autom. Lett. 7(3):5896–5903'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Han R, Chen S, Wang S, Zhang Z, Gao R, 等. 2022. 基于强化学习的分布式多机器人导航与相互速度障碍形状奖励。IEEE
    机器人与自动化快报 7(3):5896–5903'
- en: '[189] Sartoretti G, Kerr J, Shi Y, Wagner G, Kumar TS, et al. 2019. Primal:
    Pathfinding via reinforcement and imitation multi-agent learning. IEEE Robot.
    and Autom. Lett. 4(3):2378–2385'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Sartoretti G, Kerr J, Shi Y, Wagner G, Kumar TS, 等. 2019. Primal: 通过强化和模仿多智能体学习的路径寻找。IEEE
    机器人与自动化快报 4(3):2378–2385'
- en: '[190] Nachum O, Ahn M, Ponte H, Gu S, Kumar V. 2020. Multi-agent manipulation
    via locomotion using hierarchical sim2real. In Conf. Robot Learn., vol. 100, pp.
    110–121\. PMLR'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Nachum O, Ahn M, Ponte H, Gu S, Kumar V. 2020. 通过分层sim2real进行的多智能体操控。在机器人学习会议上，卷100，页码110–121。PMLR'
- en: '[191] Haarnoja T, Moran B, Lever G, Huang SH, Tirumala D, et al. 2024. Learning
    agile soccer skills for a bipedal robot with deep reinforcement learning. Sci.
    Robot. 9(89):eadi8022'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Haarnoja T, Moran B, Lever G, Huang SH, Tirumala D, 等. 2024. 通过深度强化学习为双足机器人学习灵活的足球技能。科学机器人
    9(89):eadi8022'
- en: '[192] Uchendu I, Xiao T, Lu Y, Zhu B, Yan M, et al. 2023. Jump-start reinforcement
    learning. In Int. Conf. Mach. Learn., pp. 34556–34583\. PMLR'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Uchendu I, Xiao T, Lu Y, Zhu B, Yan M, 等. 2023. 跳跃启动强化学习。在机器学习国际会议上，页码34556–34583。PMLR'
- en: '[193] Li C, Tang C, Nishimura H, Mercat J, Tomizuka M, Zhan W. 2023. Residual
    Q-learning: offline and online policy customization without value. In Adv. Neural
    Inf. Process. Syst., vol. 36, pp. 61857–61869'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Li C, Tang C, Nishimura H, Mercat J, Tomizuka M, Zhan W. 2023. 残差Q学习:
    无需价值的离线和在线策略定制。在神经信息处理系统进展会议上，卷36，页码61857–61869'
- en: '[194] Hansen N, Su H, Wang X. 2023. TD-MPC2: Scalable, robust world models
    for continuous control. In Conf. Learn. Represent.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Hansen N, Su H, Wang X. 2023. TD-MPC2: 可扩展的、鲁棒的连续控制世界模型。在学习表示会议上'
- en: '[195] Jeong GC, Bahety A, Pedraza G, Deshpande AD, Martín-Martín R. 2023. Bariflex:
    A robotic gripper with versatility and collision robustness for robot learning.
    arXiv preprint arXiv:2312.05323'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Jeong GC, Bahety A, Pedraza G, Deshpande AD, Martín-Martín R. 2023. Bariflex:
    一种具有多功能性和碰撞鲁棒性的机器人夹持器用于机器人学习。arXiv 预印本 arXiv:2312.05323'
- en: '[196] Eysenbach B, Gupta A, Ibarz J, Levine S. 2019. Diversity is all you need:
    Learning skills without a reward function. In Int. Conf. Learn. Represent.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Eysenbach B, Gupta A, Ibarz J, Levine S. 2019. 多样性就是你所需要的一切: 无需奖励函数的技能学习。在学习表示国际会议上'
- en: '[197] Schwarke C, Klemm V, Van der Boon M, Bjelonic M, Hutter M. 2023. Curiosity-driven
    learning of joint locomotion and manipulation tasks. In Conf. Robot Learn., vol.
    229, pp. 2594–2610'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Schwarke C, Klemm V, Van der Boon M, Bjelonic M, Hutter M. 2023. 基于好奇心的联合运动与操作任务学习。发表于机器人学习会议，第229卷，页码2594–2610'
- en: '[198] Xie Z, Da X, Van de Panne M, Babich B, Garg A. 2021. Dynamics randomization
    revisited: A case study for quadrupedal locomotion. In IEEE Int. Conf. Robot.
    Autom., pp. 4955–4961'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Xie Z, Da X, Van de Panne M, Babich B, Garg A. 2021. 动力学随机化再探：以四足运动为例。发表于
    IEEE 国际机器人与自动化会议，页码4955–4961'
- en: '[199] Martín-Martín R, Lee MA, Gardner R, Savarese S, Bohg J, Garg A. 2019.
    Variable impedance control in end-effector space: An action space for reinforcement
    learning in contact-rich tasks. In IEEE/RSJ Int. Conf. Intell. Robots Syst., pp.
    1010–1017\. IEEE'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Martín-Martín R, Lee MA, Gardner R, Savarese S, Bohg J, Garg A. 2019.
    末端效应器空间中的变阻抗控制：用于接触丰富任务的强化学习行动空间。发表于 IEEE/RSJ 国际智能机器人系统会议，页码1010–1017\. IEEE'
- en: '[200] Xia F, Li C, Martín-Martín R, Litany O, Toshev A, Savarese S. 2021. ReLMoGen:
    Integrating motion generation in reinforcement learning for mobile manipulation.
    In IEEE Conf. Robot. Autom., pp. 4583–4590'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Xia F, Li C, Martín-Martín R, Litany O, Toshev A, Savarese S. 2021. ReLMoGen:
    将运动生成集成到用于移动操作的强化学习中。发表于 IEEE 机器人与自动化会议，页码4583–4590'
- en: '[201] Luo J, Xu C, Liu F, Tan L, Lin Z, et al. 2024. Fmb: A functional manipulation
    benchmark for generalizable robotic learning. Int. J. Robot. Res.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Luo J, Xu C, Liu F, Tan L, Lin Z, 等. 2024. Fmb: 一种通用机器人学习的功能性操作基准。国际机器人研究期刊'
- en: '[202] Heo M, Lee Y, Lee D, Lim JJ. 2023. FurnitureBench: Reproducible Real-World
    Benchmark for Long-Horizon Complex Manipulation. In Robot. Sci. Syst.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Heo M, Lee Y, Lee D, Lim JJ. 2023. FurnitureBench: 可重复的实际世界复杂操作基准。发表于机器人科学与系统'
- en: '[203] van der Zant T, Iocchi L. 2011. Robocup@ home: Adaptive benchmarking
    of robot bodies and minds. In Int. Conf. Soc. Robot., pp. 214–225\. Springer'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] van der Zant T, Iocchi L. 2011. Robocup@ home: 机器人身体和智能的自适应基准测试。发表于国际社会机器人会议，页码214–225\.
    Springer'
- en: '[204] Li X, Hsu K, Gu J, Pertsch K, Mees O, et al. 2024b. Evaluating real-world
    robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Li X, Hsu K, Gu J, Pertsch K, Mees O, 等. 2024b. 在模拟中评估实际世界机器人操作策略。arXiv
    预印本 arXiv:2405.05941'
- en: '[205] Padalkar A, Pooley A, Jain A, Bewley A, Herzog A, et al. 2023. Open x-embodiment:
    Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Padalkar A, Pooley A, Jain A, Bewley A, Herzog A, 等. 2023. Open x-embodiment:
    机器人学习数据集和 rt-x 模型。arXiv 预印本 arXiv:2310.08864'
- en: '[206] Khazatsky A, Pertsch K, Nair S, Balakrishna A, Dasari S, et al. 2024.
    Droid: A large-scale in-the-wild robot manipulation dataset. In Robot.: Sci. and
    Sys.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Khazatsky A, Pertsch K, Nair S, Balakrishna A, Dasari S, 等. 2024. Droid:
    大规模野外机器人操作数据集。发表于机器人科学与系统'
- en: '[207] Firoozi R, Tucker J, Tian S, Majumdar A, Sun J, et al. 2023. Foundation
    models in robotics: Applications, challenges, and the future. arXiv preprint arXiv:2312.07843'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Firoozi R, Tucker J, Tian S, Majumdar A, Sun J, 等. 2023. 机器人中的基础模型：应用、挑战与未来。arXiv
    预印本 arXiv:2312.07843'
- en: '[208] Hu Y, Xie Q, Jain V, Francis J, Patrikar J, et al. 2023. Toward general-purpose
    robots via foundation models: A survey and meta-analysis. arXiv preprint arXiv:2312.08782'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Hu Y, Xie Q, Jain V, Francis J, Patrikar J, 等. 2023. 通过基础模型实现通用机器人：调查与元分析。arXiv
    预印本 arXiv:2312.08782'
- en: '[209] Yang S, Nachum O, Du Y, Wei J, Abbeel P, Schuurmans D. 2023c. Foundation
    models for decision making: Problems, methods, and opportunities. arXiv preprint
    arXiv:2303.04129'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Yang S, Nachum O, Du Y, Wei J, Abbeel P, Schuurmans D. 2023c. 用于决策的基础模型：问题、方法与机遇。arXiv
    预印本 arXiv:2303.04129'
- en: '[210] Ma YJ, Liang W, Wang HJ, Wang S, Zhu Y, et al. 2024. DrEureka: Language
    Model Guided Sim-To-Real Transfer. In Robot.: Sci. Sys.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Ma YJ, Liang W, Wang HJ, Wang S, Zhu Y, 等. 2024. DrEureka: 语言模型引导的模拟到现实转移。发表于机器人科学与系统'
- en: Appendix A Term Definition
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 术语定义
- en: 'As presented in Sec. [3](#S3 "3 Taxonomy ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes") of the main article, we classify
    the literature based on a taxonomy consisting of four axes: robot competencies
    learned with DRL, problem formulation, solution approach, and the level of real-world
    success. In this section, we provide a detailed definition and discussion of the
    elements along the problem formulation and solution approach axes.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '如主要文章第[3](#S3 "3 Taxonomy ‣ Deep Reinforcement Learning for Robotics: A Survey
    of Real-World Successes")节所述，我们根据由四个轴组成的分类法对文献进行了分类：通过深度强化学习学习的机器人能力、问题定义、解决方法和实际世界成功的程度。在本节中，我们对问题定义和解决方法轴上的元素进行了详细定义和讨论。'
- en: A.1 Problem Formulation
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 问题定义
- en: 'As discussed in Sec. [3](#S3 "3 Taxonomy ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"), we categorize the papers based on
    the following elements of the problem formulation: 1) *Action space*: whether
    the actions are *low-level* (i.e., joint or motor commands), *mid-level* (i.e.,
    task-space commands), or *high-level* (i.e., temporally extended task-space commands
    or subroutines); 2) *Observation space*: whether the observations are *high-dimensional*
    sensor inputs (e.g., images and/or LiDAR scans) or estimated *low-dimensional*
    state vectors; 3) *Reward function*: whether the reward signals are *sparse* or
    *dense*. This subsection provides detailed definitions and a discussion of these
    terms.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第[3](#S3 "3 分类 ‣ 机器人强化学习：现实世界成功案例的调查")节中讨论的，我们根据问题公式化的以下要素对论文进行分类：1）*动作空间*：动作是*低级*（即关节或电机命令）、*中级*（即任务空间命令）还是*高级*（即时间扩展的任务空间命令或子例程）；2）*观察空间*：观察是*高维*传感器输入（例如图像和/或激光雷达扫描）还是估计的*低维*状态向量；3）*奖励函数*：奖励信号是*稀疏*还是*密集*。本小节提供了这些术语的详细定义和讨论。
- en: A.1.1 Action Space
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 动作空间
- en: 'Low-level Actions: We define low-level actions as those that directly operate
    in the robot’s joint space, such as controlling torques of individual joints in
    a robot arm or velocities of individual wheels in a mobile robot. A low-level
    action space requires minimal domain knowledge and allows the policy to have fine-grained
    control over the robot’s behavior. However, performing learning in low-level action
    spaces presents several challenges: 1) exploration with low-level actions is difficult,
    as random joint actions often result in trivial behaviors; 2) the action space
    scales linearly with the robot’s degrees of freedom, often resulting in high-dimensional
    action spaces; and 3) joints are often controlled at a high frequency, resulting
    in extended task horizons and inference-time constraints.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 低级动作：我们将低级动作定义为那些直接在机器人的关节空间中操作的动作，例如控制机器人手臂中各个关节的扭矩或移动机器人中各个轮子的速度。低级动作空间需要的领域知识最少，允许策略对机器人的行为进行细粒度控制。然而，在低级动作空间中进行学习存在几个挑战：1）低级动作的探索困难，因为随机的关节动作通常会导致琐碎的行为；2）动作空间随着机器人自由度的增加而线性扩展，通常导致高维动作空间；3）关节通常以高频率控制，导致任务时间跨度延长和推理时间约束。
- en: 'Mid-level Actions: Mid-level actions control the robot in its workspace, such
    as adjusting the end-effector pose of a robot arm or controlling the velocity
    of the center of mass of a mobile robot. Once the policies generate these mid-level
    actions, they are often executed by an external controller, such as an inverse
    kinematics (IK) controller, to produce the joint-level torques. As such, operating
    in a mid-level action space requires domain knowledge to define an appropriate
    operational space and to design and implement the external controller effectively.
    When chosen correctly, mid-level action spaces strike a balance between incorporating
    domain knowledge and maintaining generality for various tasks. This approach is
    a popular choice in many RL applications for robotics, as it leverages specific
    expertise while allowing flexibility across different robotic functions.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 中级动作：中级动作控制机器人在其工作空间中的操作，例如调整机器人手臂末端执行器的姿态或控制移动机器人质心的速度。一旦策略生成了这些中级动作，它们通常会由外部控制器（如逆向运动学（IK）控制器）执行，以产生关节级扭矩。因此，在中级动作空间中操作需要领域知识来定义适当的操作空间，并有效设计和实现外部控制器。当选择得当时，中级动作空间在融入领域知识和保持各种任务的通用性之间取得了平衡。这种方法在许多机器人强化学习应用中很受欢迎，因为它利用了特定的专业知识，同时允许在不同的机器人功能之间灵活应用。
- en: 'High-level Actions: High-level actions control the robot through temporally
    extended “skills” that can realize certain short-horizon behaviors, such as “grasping
    certain objects” or “moving to certain rooms.” A well-designed high-level action
    space can greatly enhance the efficiency of the RL agent’s exploration by drastically
    shortening the task horizon and ensuring that the robot performs task-relevant
    actions most of the time. However, designing an appropriate set of skills for
    the high-level action space is a complex problem, often requiring each skill to
    be formulated as an RL problem in itself. Additionally, these skills may not always
    be transferable across tasks, posing challenges to their scalability.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 高级动作：高级动作通过时间扩展的“技能”控制机器人，可以实现某些短期行为，例如“抓取特定物体”或“移动到特定房间”。 设计良好的高级动作空间可以通过大幅缩短任务范围并确保机器人大多数时间执行与任务相关的动作，从而大大提高RL智能体的探索效率。然而，为高级动作空间设计合适的技能集是一个复杂的问题，通常需要将每个技能单独制定为一个RL问题。此外，这些技能可能不会总是能够在任务之间转移，从而对其可扩展性提出挑战。
- en: A.1.2 Observation Space
  id: totrans-459
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 观察空间
- en: 'Low-dimensional Observations: The robot’s observations are represented as a
    compact, low-dimensional vector, which can include proprioceptive information,
    object locations, and task information.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 低维观察：机器人的观察表示为紧凑的低维向量，可以包括本体感受信息、物体位置和任务信息。
- en: 'High-dimensional Observations: The robot’s observations include high-dimensional
    sensor data for exteroceptive information, which can be in the form of lidar readings,
    camera images, and/or point clouds.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 高维观察：机器人的观察包括用于外部感知信息的高维传感器数据，可以是激光雷达读数、相机图像和/或点云的形式。
- en: A.1.3 Reward Function
  id: totrans-462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.3 奖励函数
- en: 'Sparse Reward: A sparse reward signal means the agent receives trivial reward
    signals for most of the potential transitions in a (PO)MDP and only receives non-trivial
    reward signals sparsely. One natural way of defining a sparse reward for a task
    is to have +1 for all transitions into a success termination state, -1 for all
    transitions into a failure termination state, and 0 for any other transitions.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏奖励：稀疏奖励信号意味着智能体在（PO）MDP中的大多数潜在转换中接收到微不足道的奖励信号，仅在稀疏情况下接收到非微不足道的奖励信号。为任务定义稀疏奖励的一种自然方式是对所有转移到成功终止状态的过渡给予+1，对所有转移到失败终止状态的过渡给予-1，对其他任何过渡给予0。
- en: 'Dense Reward: A dense reward means the reward signal is abundant, providing
    rich feedback to the agent. In certain tasks, such as locomotion, the reward is
    naturally dense (e.g., the error between the robot’s current forward velocity
    and the instructed velocity). In other scenarios where the task reward is inherently
    sparse, such as navigation tasks, a dense reward can be defined by adding shaping
    components to the sparse reward (e.g., the distance between the robot and the
    navigation target). Such kind of shaped and dense rewards are often used to facilitate
    learning efficiency, especially for long-horizon tasks.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密奖励：稠密奖励意味着奖励信号丰富，为智能体提供了丰富的反馈。在某些任务中，例如运动控制，奖励自然是稠密的（例如，机器人当前前进速度与指令速度之间的误差）。在其他任务奖励本质上稀疏的场景中，例如导航任务，通过将塑造组件添加到稀疏奖励中可以定义稠密奖励（例如，机器人与导航目标之间的距离）。这种经过塑造的稠密奖励常用于提高学习效率，尤其是对于长期任务。
- en: A.2 Solution Approach
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 解决方案方法
- en: 'As introduced in Sec. [3](#S3 "3 Taxonomy ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"), we classify the solution approach
    from the following perspectives: 1) *Simulator usage*: whether and how simulators
    are used, categorized into *zero-shot*, *few-shot sim-to-real transfer*, or directly
    learning offline or in the real world *without simulators*; 2) *Model learning*:
    whether (a part of) the transition dynamics model is learned from robot data;
    3) *Expert usage*: whether expert (e.g., human or oracle policy) data are used
    to facilitate learning; 4) *Policy optimization*: the policy optimization algorithm
    adopted, including *planning* or *offline*, *off-policy*, or *on-policy RL*; 5)
    *Policy/Model Representation*: Classes of neural network architectures used to
    represent the policy or dynamics model, including *MLP*, *CNN*, *RNN*, and *Transformer*.
    This subsection provides detailed definitions of these terms.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[3](#S3 "3 Taxonomy ‣ Deep Reinforcement Learning for Robotics: A Survey
    of Real-World Successes")节介绍，我们从以下几个方面对解决方案方法进行分类：1）*模拟器使用*：是否以及如何使用模拟器，分类为*零-shot*、*少-shot
    sim-to-real 转移*，或直接在离线或现实世界中*无模拟器*地学习；2）*模型学习*：是否（部分）过渡动态模型从机器人数据中学习；3）*专家使用*：是否使用专家（例如，人类或神谕策略）数据来促进学习；4）*策略优化*：采用的策略优化算法，包括*规划*或*离线*、*离策略*或*在线
    RL*；5）*策略/模型表示*：用于表示策略或动态模型的神经网络架构类别，包括*MLP*、*CNN*、*RNN* 和 *Transformer*。本小节提供了这些术语的详细定义。'
- en: A.2.1 Simulator Usage
  id: totrans-467
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 模拟器使用
- en: 'Zero-shot sim2real: The training is performed entirely in a simulator, where
    the trained policy is deployed directly in the real world without additional learning.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot sim2real：训练完全在模拟器中进行，经过训练的策略直接在现实世界中部署，而无需额外学习。
- en: 'Few-shot sim2real: The robot is pre-trained in the simulator and fine-tuned
    in the real world with limited additional real-world interactions.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 少-shot sim2real：机器人在模拟器中预训练，并在现实世界中经过有限的额外真实世界交互进行微调。
- en: 'No Simulator: The training is conducted in the real world without using a simulator.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 无模拟器：训练在现实世界中进行，没有使用模拟器。
- en: A.2.2 Model Learning
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 模型学习
- en: 'RL algorithms can be broadly classified into two categories: model-free RL
    and model-based RL, based on whether they learn a dynamics model. In model-free
    RL algorithms, such as PPO and SAC, the robot directly learns a policy or value
    function without explicitly modeling the environment’s dynamics. Model-free RL
    is often easier to implement and superior when learning a good policy is simpler
    than learning a good model. In contrast, model-based RL algorithms, such as TD-MPC
    and Dreamer, involve the robot learning a world model that can predict the consequences
    of its actions. This world model can be used either for model-based planning and
    control or for generating experiences for a model-free RL agent, potentially increasing
    the agent’s sample efficiency. Instead of learning the full dynamics, some methods
    learn a residual or a part of the dynamics model (e.g., actuator dynamics model)
    to complement the simulation for model-free RL, which we also mark as involving
    model learning in our categorization.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: RL 算法可以大致分为两类：基于模型的 RL 和无模型 RL，这取决于它们是否学习动态模型。在无模型 RL 算法中，例如 PPO 和 SAC，机器人直接学习策略或价值函数，而不显式建模环境的动态。无模型
    RL 通常更易于实现，并且在学习一个好的策略比学习一个好的模型更简单时，表现更优。相比之下，基于模型的 RL 算法，例如 TD-MPC 和 Dreamer，涉及机器人学习一个能够预测其行为后果的世界模型。这个世界模型可以用于基于模型的规划和控制，或用于为无模型
    RL 代理生成经验，从而可能提高代理的样本效率。与学习完整的动态模型不同，一些方法学习一个残差或动态模型的部分（例如，执行器动态模型），以补充无模型 RL
    的模拟，这些方法在我们的分类中也被标记为涉及模型学习。
- en: A.2.3 Expert Usage
  id: totrans-473
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.3 专家使用
- en: Tabula rasa RL begins with random initialization, training entirely through
    trial and error. However, in robotics, it is sometimes possible to utilize an
    external expert to expedite the learning process. These experts may include human
    demonstrations, trajectory planners, oracle actions, and so on. In this survey,
    we classify all works that utilize an external expert, either offline or online,
    to facilitate learning as works “with experts”, which gives them an advantage
    over methods that do not assume access to experts.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: Tabula rasa RL 从随机初始化开始，完全通过试错法进行训练。然而，在机器人技术中，有时可以利用外部专家来加快学习过程。这些专家可能包括人类演示、轨迹规划器、神谕动作等。在本调查中，我们将所有利用外部专家（无论是离线还是在线）来促进学习的工作分类为“有专家”的工作，这使得这些工作相对于那些不假设访问专家的方法具有优势。
- en: A.2.4 Policy Optimization
  id: totrans-475
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.4 策略优化
- en: 'Planning: The robot’s policy is derived by solving an optimal control problem
    online using a learned world model. Representative algorithms include A* and MPPI
    (Model Predictive Path Integral).'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '规划: 机器人的政策通过在线解决一个最优控制问题来获得，使用的是学习到的世界模型。代表性算法包括 A* 和 MPPI（Model Predictive
    Path Integral）。'
- en: 'Offline: The robot does not interact with the environment during learning.
    Instead, it learns a policy and, optionally, a value function directly from offline
    data. Representative algorithms include CQL (Conservative Q-Learning) and DT (Decision
    Transformer).'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 'Offline: 机器人在学习过程中不与环境进行交互。相反，它直接从离线数据中学习一个政策，并可选地学习一个价值函数。代表性算法包括 CQL（Conservative
    Q-Learning）和 DT（Decision Transformer）。'
- en: 'On-policy: The robot interacts with the environment during learning and only
    updates the policy with transitions collected by the current policy. Representative
    algorithms include PPO (Proximal Policy Optimization) and TRPO (Trust Region Policy
    Optimization).'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 'On-policy: 机器人在学习过程中与环境进行交互，并仅通过当前政策收集的过渡来更新政策。代表性算法包括 PPO（Proximal Policy
    Optimization）和 TRPO（Trust Region Policy Optimization）。'
- en: 'Off-policy: The robot interacts with the environment during learning and updates
    the policy with transitions collected by both the current policy and other/previous
    policies. Representative algorithms include SAC (Soft Actor-Critic) and DQN (Deep
    Q-Network).'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 'Off-policy: 机器人在学习过程中与环境进行交互，并通过当前政策和其他/之前的政策收集的过渡来更新政策。代表性算法包括 SAC（Soft Actor-Critic）和
    DQN（Deep Q-Network）。'
- en: A.2.5 Policy/Model Representation
  id: totrans-480
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.5 政策/模型表示
- en: 'MLP Only: Multi-layer Perceptron (MLP) models take 1D vector inputs and consist
    solely of fully connected layers. They are widely used for processing low-dimensional
    observations.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '仅 MLP: 多层感知机（MLP）模型接受 1D 向量输入，并仅由全连接层组成。它们广泛用于处理低维观察数据。'
- en: 'CNN: Convolutional Neural Networks (CNNs) are a specialized type of MLP that
    preserves local spatial coherence, initially designed for image processing. Later
    works have extended CNNs to process 1D data like lidar readings and observation
    memory, as well as 3D data such as point clouds.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 'CNN: 卷积神经网络（CNNs）是一种专门的多层感知机（MLP），用于保留局部空间一致性，最初设计用于图像处理。后来的研究扩展了 CNNs 以处理如激光雷达读数和观测记忆等
    1D 数据，以及点云等 3D 数据。'
- en: 'RNN: Recurrent Neural Networks (RNNs), including LSTM and GRU, are bidirectional
    neural networks with internal memory. They are suitable for processing time-series
    data, such as trajectories over time.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 'RNN: 循环神经网络（RNNs），包括 LSTM 和 GRU，是具有内部记忆的双向神经网络。它们适用于处理时间序列数据，例如随时间变化的轨迹。'
- en: 'Transformer: Transformers take a sequence of vectors (tokens) as input and
    use multi-head self-attention to generate outputs. Recently, transformers have
    been widely used to process time-series data, natural language instructions, and
    visual information. They have also proven powerful in fusing tokenized multi-modal
    information.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer: Transformers 接受一系列向量（令牌）作为输入，并使用多头自注意力机制生成输出。近年来，transformers
    已被广泛用于处理时间序列数据、自然语言指令和视觉信息。它们在融合令牌化的多模态信息方面也显示出强大的能力。'
- en: Appendix B Additional Tables
  id: totrans-485
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外表格
- en: 'This section contains tables that present the complete categorization of the
    reviewed papers along all four axes of our taxonomy. Tables [1](#A2.T1 "Table
    1 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A
    Survey of Real-World Successes")-[2](#A2.T2 "Table 2 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    categorize the papers based on problem formulation for each robot competency,
    while Tables [3](#A2.T3 "Table 3 ‣ Appendix B Additional Tables ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")-[6](#A2.T6 "Table 6
    ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey
    of Real-World Successes") categorize the papers based on solution approach for
    each robot competency. As in the tables in the main article, the color map indicates
    the levels of real-world success: *Sim Only*, *Limited Lab*, *Diverse Lab*, *Limited
    Real*, and *Diverse Real*.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了表格，这些表格展示了我们分类体系中所有四个轴的评审论文的完整分类。表格 [1](#A2.T1 "表格 1 ‣ 附录 B 额外表格 ‣ 深度强化学习在机器人领域的真实世界成功案例调研")-[2](#A2.T2
    "表格 2 ‣ 附录 B 额外表格 ‣ 深度强化学习在机器人领域的真实世界成功案例调研") 基于每种机器人能力的问题表述对论文进行了分类，而表格 [3](#A2.T3
    "表格 3 ‣ 附录 B 额外表格 ‣ 深度强化学习在机器人领域的真实世界成功案例调研")-[6](#A2.T6 "表格 6 ‣ 附录 B 额外表格 ‣ 深度强化学习在机器人领域的真实世界成功案例调研")
    则基于每种机器人能力的解决方案方法对论文进行了分类。与主文中的表格一样，颜色图示表示了真实世界成功的水平：*仅仿真*、*有限实验室*、*多样实验室*、*有限真实*
    和 *多样真实*。
- en: In these tables, we add a superscript ^∗ to papers that appear in multiple columns,
    which means they adopt two different elements jointly (e.g., a hierarchical policy
    that outputs both low-level and mid-level actions, a policy network consists of
    both CNN and RNN).
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些表格中，我们为出现在多个列中的论文添加了上标 ^∗，这意味着它们同时采用了两个不同的元素（例如，输出低级和中级动作的层次策略，包含 CNN 和 RNN
    的策略网络）。
- en: '|  | Action Space |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|  | 行为空间 |'
- en: '| Application | Low-Level | Mid-Level | High-Level |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 低级 | 中级 | 高级 |'
- en: '| Locomotion | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31)^∗, [32](#bib.bib32)^∗, [33](#bib.bib33), [36](#bib.bib36)^∗,
    [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65), [68](#bib.bib68) | [31](#bib.bib31)^∗, [32](#bib.bib32)^∗, [34](#bib.bib34),
    [35](#bib.bib35), [47](#bib.bib47), [66](#bib.bib66), [67](#bib.bib67) | [36](#bib.bib36)^∗,
    [60](#bib.bib60) |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 运动 | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31)^∗, [32](#bib.bib32)^∗, [33](#bib.bib33), [36](#bib.bib36)^∗,
    [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65), [68](#bib.bib68) | [31](#bib.bib31)^∗, [32](#bib.bib32)^∗, [34](#bib.bib34),
    [35](#bib.bib35), [47](#bib.bib47), [66](#bib.bib66), [67](#bib.bib67) | [36](#bib.bib36)^∗,
    [60](#bib.bib60) |'
- en: '| Navigation | [20](#bib.bib20), [90](#bib.bib90), [96](#bib.bib96)^∗, [97](#bib.bib97)^∗,
    [99](#bib.bib99), [100](#bib.bib100), | [7](#bib.bib7), [21](#bib.bib21), [73](#bib.bib73),
    [74](#bib.bib74), [75](#bib.bib75), [78](#bib.bib78), [83](#bib.bib83), [85](#bib.bib85),
    [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95)^∗, [98](#bib.bib98), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103) | [76](#bib.bib76), [81](#bib.bib81), [82](#bib.bib82), [86](#bib.bib86),
    [87](#bib.bib87), [95](#bib.bib95)^∗, [96](#bib.bib96)^∗, [97](#bib.bib97)^∗ |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| 导航 | [20](#bib.bib20), [90](#bib.bib90), [96](#bib.bib96)^∗, [97](#bib.bib97)^∗,
    [99](#bib.bib99), [100](#bib.bib100), | [7](#bib.bib7), [21](#bib.bib21), [73](#bib.bib73),
    [74](#bib.bib74), [75](#bib.bib75), [78](#bib.bib78), [83](#bib.bib83), [85](#bib.bib85),
    [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95)^∗, [98](#bib.bib98), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103) | [76](#bib.bib76), [81](#bib.bib81), [82](#bib.bib82), [86](#bib.bib86),
    [87](#bib.bib87), [95](#bib.bib95)^∗, [96](#bib.bib96)^∗, [97](#bib.bib97)^∗ |'
- en: '| Manipulation | [113](#bib.bib113), [122](#bib.bib122), [127](#bib.bib127),
    [131](#bib.bib131), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142) | [54](#bib.bib54), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119),
    [120](#bib.bib120), [121](#bib.bib121), [124](#bib.bib124), [125](#bib.bib125),
    [126](#bib.bib126), [128](#bib.bib128), [129](#bib.bib129), [130](#bib.bib130),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [137](#bib.bib137),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145) | [108](#bib.bib108),
    [109](#bib.bib109), [123](#bib.bib123), [135](#bib.bib135), [136](#bib.bib136)
    |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | [113](#bib.bib113), [122](#bib.bib122), [127](#bib.bib127), [131](#bib.bib131),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142) | [54](#bib.bib54), [110](#bib.bib110), [111](#bib.bib111),
    [112](#bib.bib112), [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [124](#bib.bib124), [125](#bib.bib125), [126](#bib.bib126),
    [128](#bib.bib128), [129](#bib.bib129), [130](#bib.bib130), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [137](#bib.bib137), [143](#bib.bib143),
    [144](#bib.bib144), [145](#bib.bib145) | [108](#bib.bib108), [109](#bib.bib109),
    [123](#bib.bib123), [135](#bib.bib135), [136](#bib.bib136) |'
- en: '| MoMa | [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155), [163](#bib.bib163),
    [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162),
    [159](#bib.bib159), [166](#bib.bib166) | [152](#bib.bib152), [164](#bib.bib164),
    [160](#bib.bib160), [161](#bib.bib161), [171](#bib.bib171), [157](#bib.bib157),
    [170](#bib.bib170), [165](#bib.bib165) | [168](#bib.bib168) |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| MoMa | [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155), [163](#bib.bib163),
    [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162),
    [159](#bib.bib159), [166](#bib.bib166) | [152](#bib.bib152), [164](#bib.bib164),
    [160](#bib.bib160), [161](#bib.bib161), [171](#bib.bib171), [157](#bib.bib157),
    [170](#bib.bib170), [165](#bib.bib165) | [168](#bib.bib168) |'
- en: '| HRI | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178),
    [179](#bib.bib179), [180](#bib.bib180) | [173](#bib.bib173), [174](#bib.bib174),
    [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183) | [172](#bib.bib172)
    |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 人机交互 (HRI) | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180) | [173](#bib.bib173),
    [174](#bib.bib174), [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)
    | [172](#bib.bib172) |'
- en: '| Multi-Robot Interaction | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187),
    [188](#bib.bib188), [190](#bib.bib190), [191](#bib.bib191) | [189](#bib.bib189)
    |  |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 多机器人互动 | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187), [188](#bib.bib188),
    [190](#bib.bib190), [191](#bib.bib191) | [189](#bib.bib189) |  |'
- en: 'Table 1: Categorizing Literature based on Problem Formulation'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 基于问题定义的文献分类'
- en: '|  | Observation Space | Reward Function |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '|  | 观察空间 | 奖励函数 |'
- en: '| Application | High-dim | Low-dim | Sparse | Dense |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 高维 | 低维 | 稀疏 | 密集 |'
- en: '| Locomotion | [35](#bib.bib35), [36](#bib.bib36), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [49](#bib.bib49), [50](#bib.bib50), [54](#bib.bib54), [61](#bib.bib61)
    | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [37](#bib.bib37), [38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [56](#bib.bib56) | [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68) |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| 运动方式 | [35](#bib.bib35), [36](#bib.bib36), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [49](#bib.bib49), [50](#bib.bib50), [54](#bib.bib54), [61](#bib.bib61)
    | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [37](#bib.bib37), [38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [56](#bib.bib56) | [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68) |'
- en: '| Navigation | [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91),
    [92](#bib.bib92), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99), [101](#bib.bib101), [102](#bib.bib102) | [7](#bib.bib7),
    [20](#bib.bib20), [21](#bib.bib21), [90](#bib.bib90), [93](#bib.bib93), [100](#bib.bib100),
    [103](#bib.bib103) | [78](#bib.bib78), [96](#bib.bib96)^∗ | [7](#bib.bib7), [20](#bib.bib20),
    [21](#bib.bib21), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96)^∗,
    [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101),
    [102](#bib.bib102), [103](#bib.bib103) |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 导航 | [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91),
    [92](#bib.bib92), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99), [101](#bib.bib101), [102](#bib.bib102) | [7](#bib.bib7),
    [20](#bib.bib20), [21](#bib.bib21), [90](#bib.bib90), [93](#bib.bib93), [100](#bib.bib100),
    [103](#bib.bib103) | [78](#bib.bib78), [96](#bib.bib96)^∗ | [7](#bib.bib7), [20](#bib.bib20),
    [21](#bib.bib21), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96)^∗,
    [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101),
    [102](#bib.bib102), [103](#bib.bib103) |'
- en: '| Manipulation | [54](#bib.bib54), [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113),
    [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [124](#bib.bib124), [125](#bib.bib125), [128](#bib.bib128), [133](#bib.bib133),
    [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137),
    [141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [145](#bib.bib145) | [122](#bib.bib122), [123](#bib.bib123), [126](#bib.bib126),
    [127](#bib.bib127), [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131),
    [132](#bib.bib132), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140)
    | [54](#bib.bib54), [108](#bib.bib108), [110](#bib.bib110), [111](#bib.bib111),
    [112](#bib.bib112), [114](#bib.bib114), [115](#bib.bib115), [117](#bib.bib117),
    [118](#bib.bib118), [122](#bib.bib122), [124](#bib.bib124), [128](#bib.bib128),
    [129](#bib.bib129), [134](#bib.bib134) | [109](#bib.bib109), [113](#bib.bib113),
    [116](#bib.bib116), [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [123](#bib.bib123), [125](#bib.bib125), [126](#bib.bib126),
    [127](#bib.bib127), [130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140) [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143) [144](#bib.bib144), [145](#bib.bib145)
    |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| Manipulation | [54](#bib.bib54)、[108](#bib.bib108)、[109](#bib.bib109)、[110](#bib.bib110)、[111](#bib.bib111)、[112](#bib.bib112)、[113](#bib.bib113)、[114](#bib.bib114)、[115](#bib.bib115)、[116](#bib.bib116)、[117](#bib.bib117)、[118](#bib.bib118)、[119](#bib.bib119)、[120](#bib.bib120)、[121](#bib.bib121)、[124](#bib.bib124)、[125](#bib.bib125)、[128](#bib.bib128)、[133](#bib.bib133)、[134](#bib.bib134)、[135](#bib.bib135)、[136](#bib.bib136)、[137](#bib.bib137)、[141](#bib.bib141)、[142](#bib.bib142)、[143](#bib.bib143)、[144](#bib.bib144)、[145](#bib.bib145)
    | [122](#bib.bib122)、[123](#bib.bib123)、[126](#bib.bib126)、[127](#bib.bib127)、[129](#bib.bib129)、[130](#bib.bib130)、[131](#bib.bib131)、[132](#bib.bib132)、[138](#bib.bib138)、[139](#bib.bib139)、[140](#bib.bib140)
    | [54](#bib.bib54)、[108](#bib.bib108)、[110](#bib.bib110)、[111](#bib.bib111)、[112](#bib.bib112)、[114](#bib.bib114)、[115](#bib.bib115)、[117](#bib.bib117)、[118](#bib.bib118)、[122](#bib.bib122)、[124](#bib.bib124)、[128](#bib.bib128)、[129](#bib.bib129)、[134](#bib.bib134)
    | [109](#bib.bib109)、[113](#bib.bib113)、[116](#bib.bib116)、[118](#bib.bib118)、[119](#bib.bib119)、[120](#bib.bib120)、[121](#bib.bib121)、[123](#bib.bib123)、[125](#bib.bib125)、[126](#bib.bib126)、[127](#bib.bib127)、[130](#bib.bib130)、[131](#bib.bib131)、[132](#bib.bib132)、[133](#bib.bib133)、[135](#bib.bib135)、[136](#bib.bib136)、[137](#bib.bib137)、[138](#bib.bib138)、[139](#bib.bib139)、[140](#bib.bib140)
    [141](#bib.bib141)、[142](#bib.bib142)、[143](#bib.bib143) [144](#bib.bib144)、[145](#bib.bib145)
    |'
- en: '| MoMa | [163](#bib.bib163), [167](#bib.bib167), [164](#bib.bib164), [160](#bib.bib160),
    [168](#bib.bib168), [165](#bib.bib165), [159](#bib.bib159), [171](#bib.bib171),
    [157](#bib.bib157), [170](#bib.bib170) | [153](#bib.bib153), [154](#bib.bib154),
    [152](#bib.bib152), [155](#bib.bib155), [169](#bib.bib169), [158](#bib.bib158),
    [162](#bib.bib162), [161](#bib.bib161), [166](#bib.bib166) | [168](#bib.bib168),
    [171](#bib.bib171), [170](#bib.bib170) | [153](#bib.bib153), [154](#bib.bib154),
    [152](#bib.bib152), [155](#bib.bib155), [163](#bib.bib163), [167](#bib.bib167),
    [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162), [164](#bib.bib164),
    [160](#bib.bib160), [161](#bib.bib161), [165](#bib.bib165), [159](#bib.bib159),
    [166](#bib.bib166), [157](#bib.bib157) |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| MoMa | [163](#bib.bib163)、[167](#bib.bib167)、[164](#bib.bib164)、[160](#bib.bib160)、[168](#bib.bib168)、[165](#bib.bib165)、[159](#bib.bib159)、[171](#bib.bib171)、[157](#bib.bib157)、[170](#bib.bib170)
    | [153](#bib.bib153)、[154](#bib.bib154)、[152](#bib.bib152)、[155](#bib.bib155)、[169](#bib.bib169)、[158](#bib.bib158)、[162](#bib.bib162)、[161](#bib.bib161)、[166](#bib.bib166)
    | [168](#bib.bib168)、[171](#bib.bib171)、[170](#bib.bib170) | [153](#bib.bib153)、[154](#bib.bib154)、[152](#bib.bib152)、[155](#bib.bib155)、[163](#bib.bib163)、[167](#bib.bib167)、[169](#bib.bib169)、[158](#bib.bib158)、[162](#bib.bib162)、[164](#bib.bib164)、[160](#bib.bib160)、[161](#bib.bib161)、[165](#bib.bib165)、[159](#bib.bib159)、[166](#bib.bib166)、[157](#bib.bib157)
    |'
- en: '| HRI | [173](#bib.bib173), [174](#bib.bib174), [177](#bib.bib177), [178](#bib.bib178),
    [181](#bib.bib181) | [172](#bib.bib172), [175](#bib.bib175), [176](#bib.bib176),
    [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182), [183](#bib.bib183)
    | [172](#bib.bib172) | [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179),
    [180](#bib.bib180), [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)
    |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| HRI | [173](#bib.bib173)、[174](#bib.bib174)、[177](#bib.bib177)、[178](#bib.bib178)、[181](#bib.bib181)
    | [172](#bib.bib172)、[175](#bib.bib175)、[176](#bib.bib176)、[179](#bib.bib179)、[180](#bib.bib180)、[182](#bib.bib182)、[183](#bib.bib183)
    | [172](#bib.bib172) | [173](#bib.bib173)、[174](#bib.bib174)、[175](#bib.bib175)、[176](#bib.bib176)、[177](#bib.bib177)、[178](#bib.bib178)、[179](#bib.bib179)、[180](#bib.bib180)、[181](#bib.bib181)、[182](#bib.bib182)、[183](#bib.bib183)
    |'
- en: '| Multi-Robot Interaction |  | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)
    | [184](#bib.bib184), [185](#bib.bib185) | [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191) |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| 多机器人互动 |  | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191) | [184](#bib.bib184),
    [185](#bib.bib185) | [187](#bib.bib187), [188](#bib.bib188), [189](#bib.bib189),
    [190](#bib.bib190), [191](#bib.bib191) |'
- en: 'Table 2: Categorizing Literature based on Problem Formulation (Cont.)'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于问题描述的文献分类（续）
- en: '|  | Simulator Usage |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | 模拟器使用 |'
- en: '| Application | Zero-shot Sim-to-Real | Few-shot Sim-to-Real | No Simulator
    |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | Zero-shot Sim-to-Real | Few-shot Sim-to-Real | 无模拟器 |'
- en: '| Locomotion | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [55](#bib.bib55),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [43](#bib.bib43), [48](#bib.bib48), [56](#bib.bib56)
    | [53](#bib.bib53), [54](#bib.bib54) |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 运动 | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [55](#bib.bib55),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [43](#bib.bib43), [48](#bib.bib48), [56](#bib.bib56)
    | [53](#bib.bib53), [54](#bib.bib54) |'
- en: '| Navigation | [20](#bib.bib20), [21](#bib.bib21), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [103](#bib.bib103) |
    [7](#bib.bib7), [102](#bib.bib102) | [88](#bib.bib88), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92) |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 导航 | [20](#bib.bib20), [21](#bib.bib21), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [103](#bib.bib103) |
    [7](#bib.bib7), [102](#bib.bib102) | [88](#bib.bib88), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92) |'
- en: '| Manipulation | [108](#bib.bib108), [111](#bib.bib111), [123](#bib.bib123),
    [130](#bib.bib130), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145)
    | [116](#bib.bib116), [131](#bib.bib131) | [54](#bib.bib54), [109](#bib.bib109),
    [110](#bib.bib110), [112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [132](#bib.bib132), [136](#bib.bib136), [140](#bib.bib140)
    |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | [108](#bib.bib108), [111](#bib.bib111), [123](#bib.bib123), [130](#bib.bib130),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139), [141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145) | [116](#bib.bib116),
    [131](#bib.bib131) | [54](#bib.bib54), [109](#bib.bib109), [110](#bib.bib110),
    [112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122), [124](#bib.bib124), [125](#bib.bib125),
    [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128), [129](#bib.bib129),
    [132](#bib.bib132), [136](#bib.bib136), [140](#bib.bib140) |'
- en: '| MoMa | [153](#bib.bib153), [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155),
    [163](#bib.bib163), [167](#bib.bib167), [169](#bib.bib169), [162](#bib.bib162),
    [164](#bib.bib164), [161](#bib.bib161), [165](#bib.bib165), [159](#bib.bib159),
    [166](#bib.bib166), [171](#bib.bib171), [157](#bib.bib157) | [158](#bib.bib158),
    [170](#bib.bib170) | [160](#bib.bib160), [168](#bib.bib168) |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| MoMa | [153](#bib.bib153), [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155),
    [163](#bib.bib163), [167](#bib.bib167), [169](#bib.bib169), [162](#bib.bib162),
    [164](#bib.bib164), [161](#bib.bib161), [165](#bib.bib165), [159](#bib.bib159),
    [166](#bib.bib166), [171](#bib.bib171), [157](#bib.bib157) | [158](#bib.bib158),
    [170](#bib.bib170) | [160](#bib.bib160), [168](#bib.bib168) |'
- en: '| HRI | [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177), [179](#bib.bib179) | [182](#bib.bib182)
    | [178](#bib.bib178), [181](#bib.bib181), [180](#bib.bib180) |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 人机互动（HRI） | [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177), [179](#bib.bib179) | [182](#bib.bib182)
    | [178](#bib.bib178), [181](#bib.bib181), [180](#bib.bib180) |'
- en: '| Multi-Robot Interaction | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)
    |  |  |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 多机器人互动 | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191) |  |  |'
- en: 'Table 3: Categorizing Literature based on Solution Approach'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于解决方案方法的文献分类
- en: '|  | Model Learning | Expert Usage |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型学习 | 专家使用 |'
- en: '| Application | with Model Learning | No Model   Learning | No Expert | with
    Expert |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 有模型学习 | 无模型学习    | 无专家 | 有专家 |'
- en: '| Locomotion | [29](#bib.bib29), [31](#bib.bib31), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [41](#bib.bib41), [51](#bib.bib51), [52](#bib.bib52), [54](#bib.bib54),
    [56](#bib.bib56) | [27](#bib.bib27), [28](#bib.bib28), [30](#bib.bib30), [32](#bib.bib32),
    [33](#bib.bib33), [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [53](#bib.bib53), [55](#bib.bib55),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [28](#bib.bib28), [29](#bib.bib29), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [37](#bib.bib37),
    [38](#bib.bib38), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54), [56](#bib.bib56), [57](#bib.bib57), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [27](#bib.bib27), [30](#bib.bib30), [36](#bib.bib36),
    [40](#bib.bib40), [46](#bib.bib46), [48](#bib.bib48), [51](#bib.bib51), [55](#bib.bib55),
    [58](#bib.bib58), [63](#bib.bib63), [64](#bib.bib64) |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 运动学 | [29](#bib.bib29), [31](#bib.bib31), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [41](#bib.bib41), [51](#bib.bib51), [52](#bib.bib52), [54](#bib.bib54),
    [56](#bib.bib56) | [27](#bib.bib27), [28](#bib.bib28), [30](#bib.bib30), [32](#bib.bib32),
    [33](#bib.bib33), [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [53](#bib.bib53), [55](#bib.bib55),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [28](#bib.bib28), [29](#bib.bib29), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [37](#bib.bib37),
    [38](#bib.bib38), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54), [56](#bib.bib56), [57](#bib.bib57), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [27](#bib.bib27), [30](#bib.bib30), [36](#bib.bib36),
    [40](#bib.bib40), [46](#bib.bib46), [48](#bib.bib48), [51](#bib.bib51), [55](#bib.bib55),
    [58](#bib.bib58), [63](#bib.bib63), [64](#bib.bib64) |'
- en: '| Navigation | [7](#bib.bib7), [20](#bib.bib20), [74](#bib.bib74)*, [88](#bib.bib88),
    [90](#bib.bib90), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [102](#bib.bib102) | [21](#bib.bib21), [73](#bib.bib73), [74](#bib.bib74)^∗ [75](#bib.bib75),
    [76](#bib.bib76), [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [89](#bib.bib89), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [99](#bib.bib99), [100](#bib.bib100),
    [101](#bib.bib101), [103](#bib.bib103) | [7](#bib.bib7), [20](#bib.bib20), [21](#bib.bib21),
    [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [78](#bib.bib78), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96),
    [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103) | [76](#bib.bib76), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [97](#bib.bib97) |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| 导航 | [7](#bib.bib7)、[20](#bib.bib20)、[74](#bib.bib74)*、[88](#bib.bib88)、[90](#bib.bib90)、[95](#bib.bib95)、[96](#bib.bib96)、[97](#bib.bib97)、[98](#bib.bib98)、[102](#bib.bib102)
    | [21](#bib.bib21)、[73](#bib.bib73)、[74](#bib.bib74)^* [75](#bib.bib75)、[76](#bib.bib76)、[78](#bib.bib78)、[81](#bib.bib81)、[82](#bib.bib82)、[83](#bib.bib83)、[85](#bib.bib85)、[86](#bib.bib86)、[87](#bib.bib87)、[89](#bib.bib89)、[91](#bib.bib91)、[92](#bib.bib92)、[93](#bib.bib93)、[94](#bib.bib94)、[99](#bib.bib99)、[100](#bib.bib100)、[101](#bib.bib101)、[103](#bib.bib103)
    | [7](#bib.bib7)、[20](#bib.bib20)、[21](#bib.bib21)、[73](#bib.bib73)、[74](#bib.bib74)、[75](#bib.bib75)、[78](#bib.bib78)、[81](#bib.bib81)、[82](#bib.bib82)、[83](#bib.bib83)、[85](#bib.bib85)、[86](#bib.bib86)、[87](#bib.bib87)、[88](#bib.bib88)、[93](#bib.bib93)、[94](#bib.bib94)、[95](#bib.bib95)、[96](#bib.bib96)、[98](#bib.bib98)、[99](#bib.bib99)、[100](#bib.bib100)、[101](#bib.bib101)、[102](#bib.bib102)、[103](#bib.bib103)
    | [76](#bib.bib76)、[89](#bib.bib89)、[90](#bib.bib90)、[91](#bib.bib91)、[92](#bib.bib92)、[97](#bib.bib97)
    |'
- en: '| Manipulation | [54](#bib.bib54), [113](#bib.bib113), [118](#bib.bib118),
    [140](#bib.bib140) | [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145)
    | [54](#bib.bib54), [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116),
    [118](#bib.bib118), [119](#bib.bib119), [123](#bib.bib123), [125](#bib.bib125),
    [130](#bib.bib130), [131](#bib.bib131), [133](#bib.bib133), [135](#bib.bib135),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [145](#bib.bib145) | [112](#bib.bib112), [113](#bib.bib113), [117](#bib.bib117),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [124](#bib.bib124),
    [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128), [129](#bib.bib129),
    [132](#bib.bib132), [134](#bib.bib134), [136](#bib.bib136) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | [54](#bib.bib54)、[113](#bib.bib113)、[118](#bib.bib118)、[140](#bib.bib140)
    | [108](#bib.bib108)、[109](#bib.bib109)、[110](#bib.bib110)、[111](#bib.bib111)、[112](#bib.bib112)、[114](#bib.bib114)、[115](#bib.bib115)、[116](#bib.bib116)、[117](#bib.bib117)、[119](#bib.bib119)、[120](#bib.bib120)、[121](#bib.bib121)、[122](#bib.bib122)、[123](#bib.bib123)、[124](#bib.bib124)、[125](#bib.bib125)、[126](#bib.bib126)、[127](#bib.bib127)、[128](#bib.bib128)、[129](#bib.bib129)、[130](#bib.bib130)、[131](#bib.bib131)、[132](#bib.bib132)、[133](#bib.bib133)、[134](#bib.bib134)、[135](#bib.bib135)、[136](#bib.bib136)、[137](#bib.bib137)、[138](#bib.bib138)、[139](#bib.bib139)、[141](#bib.bib141)、[142](#bib.bib142)、[143](#bib.bib143)、[144](#bib.bib144)、[145](#bib.bib145)
    | [54](#bib.bib54)、[108](#bib.bib108)、[109](#bib.bib109)、[110](#bib.bib110)、[111](#bib.bib111)、[114](#bib.bib114)、[115](#bib.bib115)、[116](#bib.bib116)、[118](#bib.bib118)、[119](#bib.bib119)、[123](#bib.bib123)、[125](#bib.bib125)、[130](#bib.bib130)、[131](#bib.bib131)、[133](#bib.bib133)、[135](#bib.bib135)、[137](#bib.bib137)、[138](#bib.bib138)、[139](#bib.bib139)、[140](#bib.bib140)、[141](#bib.bib141)、[142](#bib.bib142)、[143](#bib.bib143)、[144](#bib.bib144)、[145](#bib.bib145)
    | [112](#bib.bib112)、[113](#bib.bib113)、[117](#bib.bib117)、[120](#bib.bib120)、[121](#bib.bib121)、[122](#bib.bib122)、[124](#bib.bib124)、[126](#bib.bib126)、[127](#bib.bib127)、[128](#bib.bib128)、[129](#bib.bib129)、[132](#bib.bib132)、[134](#bib.bib134)、[136](#bib.bib136)
    |'
- en: '| MoMa |  | [153](#bib.bib153), [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155),
    [163](#bib.bib163), [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158),
    [162](#bib.bib162), [164](#bib.bib164), [160](#bib.bib160), [161](#bib.bib161),
    [168](#bib.bib168), [165](#bib.bib165), [159](#bib.bib159), [166](#bib.bib166),
    [171](#bib.bib171), [157](#bib.bib157), [170](#bib.bib170) | [153](#bib.bib153),
    [154](#bib.bib154), [152](#bib.bib152), [163](#bib.bib163), [167](#bib.bib167),
    [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162), [164](#bib.bib164),
    [160](#bib.bib160), [161](#bib.bib161), [165](#bib.bib165), [159](#bib.bib159),
    [166](#bib.bib166), [171](#bib.bib171) | [155](#bib.bib155), [168](#bib.bib168),
    [157](#bib.bib157), [170](#bib.bib170) |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| MoMa |  | [153](#bib.bib153), [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155),
    [163](#bib.bib163), [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158),
    [162](#bib.bib162), [164](#bib.bib164), [160](#bib.bib160), [161](#bib.bib161),
    [168](#bib.bib168), [165](#bib.bib165), [159](#bib.bib159), [166](#bib.bib166),
    [171](#bib.bib171), [157](#bib.bib157), [170](#bib.bib170) | [153](#bib.bib153),
    [154](#bib.bib154), [152](#bib.bib152), [163](#bib.bib163), [167](#bib.bib167),
    [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162), [164](#bib.bib164),
    [160](#bib.bib160), [161](#bib.bib161), [165](#bib.bib165), [159](#bib.bib159),
    [166](#bib.bib166), [171](#bib.bib171) | [155](#bib.bib155), [168](#bib.bib168),
    [157](#bib.bib157), [170](#bib.bib170) |'
- en: '| HRI | [173](#bib.bib173), [174](#bib.bib174), [181](#bib.bib181) | [172](#bib.bib172),
    [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178),
    [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182), [183](#bib.bib183)
    | [173](#bib.bib173), [174](#bib.bib174), [177](#bib.bib177), [179](#bib.bib179),
    [180](#bib.bib180), [182](#bib.bib182), [183](#bib.bib183) | [172](#bib.bib172),
    [175](#bib.bib175), [176](#bib.bib176), [178](#bib.bib178), [181](#bib.bib181)
    |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| 人机互动 (HRI) | [173](#bib.bib173), [174](#bib.bib174), [181](#bib.bib181) |
    [172](#bib.bib172), [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182),
    [183](#bib.bib183) | [173](#bib.bib173), [174](#bib.bib174), [177](#bib.bib177),
    [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182), [183](#bib.bib183)
    | [172](#bib.bib172), [175](#bib.bib175), [176](#bib.bib176), [178](#bib.bib178),
    [181](#bib.bib181) |'
- en: '| Multi-Robot Interaction |  | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)
    | [187](#bib.bib187), [188](#bib.bib188), [190](#bib.bib190) | [184](#bib.bib184),
    [185](#bib.bib185), [189](#bib.bib189), [191](#bib.bib191) |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 多机器人互动 |  | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191) | [187](#bib.bib187),
    [188](#bib.bib188), [190](#bib.bib190) | [184](#bib.bib184), [185](#bib.bib185),
    [189](#bib.bib189), [191](#bib.bib191) |'
- en: 'Table 4: Categorizing Literature based on Solution Approach (Cont.)'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：基于解决方案方法的文献分类（续）
- en: '|  | Policy Optimization |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  | 策略优化 |'
- en: '| Application | Planning | Offline | Off-Policy | On-Policy |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 规划 | 离线 | 脱离策略 | 在线策略 |'
- en: '| Locomotion |  |  | [32](#bib.bib32), [36](#bib.bib36)^∗, [48](#bib.bib48),
    [53](#bib.bib53), [54](#bib.bib54), [68](#bib.bib68) | [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)^∗, [37](#bib.bib37), [38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67)
    |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 运动 |  |  | [32](#bib.bib32), [36](#bib.bib36)^∗, [48](#bib.bib48), [53](#bib.bib53),
    [54](#bib.bib54), [68](#bib.bib68) | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)^∗, [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40),
    [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63),
    [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67) |'
- en: '| Navigation | [74](#bib.bib74)^∗, [76](#bib.bib76)^∗, [88](#bib.bib88), [90](#bib.bib90),
    [102](#bib.bib102), [103](#bib.bib103)^∗ | [76](#bib.bib76)^∗, [89](#bib.bib89),
    [91](#bib.bib91)^∗, | [73](#bib.bib73), [74](#bib.bib74)^∗, [75](#bib.bib75),
    [78](#bib.bib78), [91](#bib.bib91)^∗, [92](#bib.bib92), [94](#bib.bib94), [101](#bib.bib101)
    | [7](#bib.bib7), [20](#bib.bib20), [21](#bib.bib21), [81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [93](#bib.bib93),
    [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100), [103](#bib.bib103)^∗ |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 导航 | [74](#bib.bib74)^∗, [76](#bib.bib76)^∗, [88](#bib.bib88), [90](#bib.bib90),
    [102](#bib.bib102), [103](#bib.bib103)^∗ | [76](#bib.bib76)^∗, [89](#bib.bib89),
    [91](#bib.bib91)^∗, | [73](#bib.bib73), [74](#bib.bib74)^∗, [75](#bib.bib75),
    [78](#bib.bib78), [91](#bib.bib91)^∗, [92](#bib.bib92), [94](#bib.bib94), [101](#bib.bib101)
    | [7](#bib.bib7), [20](#bib.bib20), [21](#bib.bib21), [81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [93](#bib.bib93),
    [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100), [103](#bib.bib103)^∗ |'
- en: '| Manipulation | [118](#bib.bib118), [140](#bib.bib140) | [108](#bib.bib108),
    [115](#bib.bib115), [121](#bib.bib121), [122](#bib.bib122), [124](#bib.bib124),
    [129](#bib.bib129) | [54](#bib.bib54), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [114](#bib.bib114), [116](#bib.bib116),
    [117](#bib.bib117), [119](#bib.bib119), [120](#bib.bib120), [123](#bib.bib123),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136), [137](#bib.bib137), [143](#bib.bib143), [144](#bib.bib144)
    | [113](#bib.bib113), [130](#bib.bib130), [131](#bib.bib131), [138](#bib.bib138),
    [139](#bib.bib139), [141](#bib.bib141), [142](#bib.bib142), [145](#bib.bib145)
    |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | [118](#bib.bib118), [140](#bib.bib140) | [108](#bib.bib108), [115](#bib.bib115),
    [121](#bib.bib121), [122](#bib.bib122), [124](#bib.bib124), [129](#bib.bib129)
    | [54](#bib.bib54), [109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111),
    [112](#bib.bib112), [114](#bib.bib114), [116](#bib.bib116), [117](#bib.bib117),
    [119](#bib.bib119), [120](#bib.bib120), [123](#bib.bib123), [125](#bib.bib125),
    [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137), [143](#bib.bib143), [144](#bib.bib144) | [113](#bib.bib113),
    [130](#bib.bib130), [131](#bib.bib131), [138](#bib.bib138), [139](#bib.bib139),
    [141](#bib.bib141), [142](#bib.bib142), [145](#bib.bib145) |'
- en: '| MoMa |  |  | [158](#bib.bib158)^∗, [164](#bib.bib164), [160](#bib.bib160),
    [161](#bib.bib161), [171](#bib.bib171), [170](#bib.bib170) | [153](#bib.bib153),
    [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155), [163](#bib.bib163),
    [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158)^∗, [162](#bib.bib162),
    [168](#bib.bib168), [165](#bib.bib165), [159](#bib.bib159), [166](#bib.bib166),
    [157](#bib.bib157) |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| MoMa |  |  | [158](#bib.bib158)^∗, [164](#bib.bib164), [160](#bib.bib160),
    [161](#bib.bib161), [171](#bib.bib171), [170](#bib.bib170) | [153](#bib.bib153),
    [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155), [163](#bib.bib163),
    [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158)^∗, [162](#bib.bib162),
    [168](#bib.bib168), [165](#bib.bib165), [159](#bib.bib159), [166](#bib.bib166),
    [157](#bib.bib157) |'
- en: '| HRI | [181](#bib.bib181) | [178](#bib.bib178)^∗ | [172](#bib.bib172), [173](#bib.bib173),
    [174](#bib.bib174), [178](#bib.bib178)^∗, [179](#bib.bib179), [180](#bib.bib180),
    [182](#bib.bib182) | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [183](#bib.bib183) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| HRI | [181](#bib.bib181) | [178](#bib.bib178)^∗ | [172](#bib.bib172), [173](#bib.bib173),
    [174](#bib.bib174), [178](#bib.bib178)^∗, [179](#bib.bib179), [180](#bib.bib180),
    [182](#bib.bib182) | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [183](#bib.bib183) |'
- en: '| Multi-Robot Interaction |  |  | [191](#bib.bib191) | [184](#bib.bib184),
    [185](#bib.bib185), [187](#bib.bib187), [188](#bib.bib188), [189](#bib.bib189),
    [190](#bib.bib190) |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| 多机器人互动 |  |  | [191](#bib.bib191) | [184](#bib.bib184), [185](#bib.bib185),
    [187](#bib.bib187), [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190)
    |'
- en: 'Table 5: Categorizing Literature based on Solution Approach (Cont.)'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5：基于解决方案方法的文献分类（续） '
- en: '|  | Policy/Model Representation |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|  | 策略/模型表示 |'
- en: '| Application | MLP Only | CNN | RNN | Transformer |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 仅 MLP | CNN | RNN | Transformer |'
- en: '| Locomotion | [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [56](#bib.bib56), [58](#bib.bib58), [60](#bib.bib60), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [68](#bib.bib68) | [27](#bib.bib27), [33](#bib.bib33), [34](#bib.bib34),
    [36](#bib.bib36), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)^∗, [45](#bib.bib45),
    [49](#bib.bib49)^∗, [54](#bib.bib54)^∗, [63](#bib.bib63), [67](#bib.bib67) | [35](#bib.bib35),
    [37](#bib.bib37), [44](#bib.bib44)^∗, [49](#bib.bib49), [50](#bib.bib50), [54](#bib.bib54)^∗,
    [55](#bib.bib55), [57](#bib.bib57), [59](#bib.bib59), [61](#bib.bib61) | [62](#bib.bib62)
    |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| 运动 | [28](#bib.bib28)、[29](#bib.bib29)、[30](#bib.bib30)、[31](#bib.bib31)、[32](#bib.bib32)、[38](#bib.bib38)、[40](#bib.bib40)、[41](#bib.bib41)、[46](#bib.bib46)、[47](#bib.bib47)、[48](#bib.bib48)、[51](#bib.bib51)、[52](#bib.bib52)、[53](#bib.bib53)、[56](#bib.bib56)、[58](#bib.bib58)、[60](#bib.bib60)、[64](#bib.bib64)、[65](#bib.bib65)、[66](#bib.bib66)、[68](#bib.bib68)
    | [27](#bib.bib27)、[33](#bib.bib33)、[34](#bib.bib34)、[36](#bib.bib36)、[42](#bib.bib42)、[43](#bib.bib43)、[44](#bib.bib44)^\*、[45](#bib.bib45)、[49](#bib.bib49)^\*、[54](#bib.bib54)^\*、[63](#bib.bib63)、[67](#bib.bib67)
    | [35](#bib.bib35)、[37](#bib.bib37)、[44](#bib.bib44)^\*、[49](#bib.bib49)、[50](#bib.bib50)、[54](#bib.bib54)^\*、[55](#bib.bib55)、[57](#bib.bib57)、[59](#bib.bib59)、[61](#bib.bib61)
    | [62](#bib.bib62) |'
- en: '| Navigation | [7](#bib.bib7), [20](#bib.bib20), [21](#bib.bib21), [73](#bib.bib73),
    [74](#bib.bib74)^∗, [75](#bib.bib75), [76](#bib.bib76), [90](#bib.bib90), [93](#bib.bib93),
    [99](#bib.bib99), [100](#bib.bib100), [103](#bib.bib103)^§ | [78](#bib.bib78),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83)^∗, [85](#bib.bib85), [86](#bib.bib86)^∗,
    [87](#bib.bib87)^∗, [89](#bib.bib89), [91](#bib.bib91), [92](#bib.bib92), [94](#bib.bib94),
    [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)^∗, [101](#bib.bib101), [102](#bib.bib102)^∗
    | [74](#bib.bib74)^∗, [83](#bib.bib83)^∗, [85](#bib.bib85), [86](#bib.bib86)^∗,
    [87](#bib.bib87)^∗, [95](#bib.bib95), [98](#bib.bib98)^∗, [102](#bib.bib102)^∗
    | [74](#bib.bib74)^∗, |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 导航 | [7](#bib.bib7)、[20](#bib.bib20)、[21](#bib.bib21)、[73](#bib.bib73)、[74](#bib.bib74)^\*、[75](#bib.bib75)、[76](#bib.bib76)、[90](#bib.bib90)、[93](#bib.bib93)、[99](#bib.bib99)、[100](#bib.bib100)、[103](#bib.bib103)^\§
    | [78](#bib.bib78)、[81](#bib.bib81)、[82](#bib.bib82)、[83](#bib.bib83)^\*、[85](#bib.bib85)、[86](#bib.bib86)^\*、[87](#bib.bib87)^\*、[89](#bib.bib89)、[91](#bib.bib91)、[92](#bib.bib92)、[94](#bib.bib94)、[96](#bib.bib96)、[97](#bib.bib97)、[98](#bib.bib98)^\*、[101](#bib.bib101)、[102](#bib.bib102)^\*
    | [74](#bib.bib74)^\*、[83](#bib.bib83)^\*、[85](#bib.bib85)、[86](#bib.bib86)^\*、[87](#bib.bib87)^\*、[95](#bib.bib95)、[98](#bib.bib98)^\*、[102](#bib.bib102)^\*
    | [74](#bib.bib74)^\*、 |'
- en: '| Manipulation | [123](#bib.bib123), [129](#bib.bib129), [131](#bib.bib131),
    [132](#bib.bib132), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [143](#bib.bib143) | [54](#bib.bib54)^∗, [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113),
    [114](#bib.bib114), [115](#bib.bib115), [117](#bib.bib117), [118](#bib.bib118)^∗
    [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137), [142](#bib.bib142), [144](#bib.bib144) | [54](#bib.bib54)^∗,
    [118](#bib.bib118)^∗, [130](#bib.bib130) | [116](#bib.bib116), [124](#bib.bib124),
    [141](#bib.bib141), [145](#bib.bib145) |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | [123](#bib.bib123)、[129](#bib.bib129)、[131](#bib.bib131)、[132](#bib.bib132)、[138](#bib.bib138)、[139](#bib.bib139)、[140](#bib.bib140)、[143](#bib.bib143)
    | [54](#bib.bib54)^\*、[108](#bib.bib108)、[109](#bib.bib109)、[110](#bib.bib110)、[111](#bib.bib111)、[112](#bib.bib112)、[113](#bib.bib113)、[114](#bib.bib114)、[115](#bib.bib115)、[117](#bib.bib117)、[118](#bib.bib118)^\*、[119](#bib.bib119)、[120](#bib.bib120)、[121](#bib.bib121)、[122](#bib.bib122)、[125](#bib.bib125)、[126](#bib.bib126)、[127](#bib.bib127)、[128](#bib.bib128)、[133](#bib.bib133)、[134](#bib.bib134)、[135](#bib.bib135)、[136](#bib.bib136)、[137](#bib.bib137)、[142](#bib.bib142)、[144](#bib.bib144)
    | [54](#bib.bib54)^\*、[118](#bib.bib118)^\*、[130](#bib.bib130) | [116](#bib.bib116)、[124](#bib.bib124)、[141](#bib.bib141)、[145](#bib.bib145)
    |'
- en: '| MoMa | [153](#bib.bib153), [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155),
    [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162), [161](#bib.bib161),
    [166](#bib.bib166) | [163](#bib.bib163), [167](#bib.bib167)^∗, [164](#bib.bib164),
    [160](#bib.bib160), [168](#bib.bib168), [165](#bib.bib165)^∗, [159](#bib.bib159),
    [171](#bib.bib171), [157](#bib.bib157), [170](#bib.bib170)^∗ | [167](#bib.bib167)^∗,
    [165](#bib.bib165)^∗, [170](#bib.bib170)^∗ |  |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| MoMa | [153](#bib.bib153)、[154](#bib.bib154)、[152](#bib.bib152)、[155](#bib.bib155)、[169](#bib.bib169)、[158](#bib.bib158)、[162](#bib.bib162)、[161](#bib.bib161)、[166](#bib.bib166)
    | [163](#bib.bib163)、[167](#bib.bib167)^\*、[164](#bib.bib164)、[160](#bib.bib160)、[168](#bib.bib168)、[165](#bib.bib165)^\*、[159](#bib.bib159)、[171](#bib.bib171)、[157](#bib.bib157)、[170](#bib.bib170)^\*
    | [167](#bib.bib167)^\*、[165](#bib.bib165)^\*、[170](#bib.bib170)^\* |  |'
- en: '| HRI | [175](#bib.bib175), [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182),
    [183](#bib.bib183) | [173](#bib.bib173), [174](#bib.bib174), [177](#bib.bib177),
    [178](#bib.bib178), [181](#bib.bib181)^∗ | [172](#bib.bib172), [176](#bib.bib176)
    | [181](#bib.bib181)^∗ |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 人机交互 | [175](#bib.bib175), [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182),
    [183](#bib.bib183) | [173](#bib.bib173), [174](#bib.bib174), [177](#bib.bib177),
    [178](#bib.bib178), [181](#bib.bib181)^∗ | [172](#bib.bib172), [176](#bib.bib176)
    | [181](#bib.bib181)^∗ |'
- en: '| Multi-Robot Interaction | [184](#bib.bib184), [187](#bib.bib187), [190](#bib.bib190),
    [191](#bib.bib191) |  | [185](#bib.bib185), [188](#bib.bib188), [189](#bib.bib189)
    |  |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 多机器人交互 | [184](#bib.bib184), [187](#bib.bib187), [190](#bib.bib190), [191](#bib.bib191)
    |  | [185](#bib.bib185), [188](#bib.bib188), [189](#bib.bib189) |  |'
- en: 'Table 6: Categorizing Literature based on Solution Approach (Cont.)'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：基于解决方案方法的文献分类（续）
