- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:36:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:36:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2309.16398] Recent Advances of Differential Privacy in Centralized Deep Learning:
    A Systematic Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2309.16398] 《集中式深度学习中差分隐私的最新进展：系统性调查》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16398](https://ar5iv.labs.arxiv.org/html/2309.16398)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16398](https://ar5iv.labs.arxiv.org/html/2309.16398)
- en: '¹¹institutetext: Graz University of Technology (Austria)²²institutetext: Know-Center
    GmbH (Austria)³³institutetext: University of Graz (Austria)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构：格拉茨理工大学（奥地利）²²机构：Know-Center GmbH（奥地利）³³机构：格拉茨大学（奥地利）
- en: ldemelius@know-center.at
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ldemelius@know-center.at
- en: rkern@know-center.at
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: rkern@know-center.at
- en: atruegler@know-center.at
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: atruegler@know-center.at
- en: 'Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《集中式深度学习中差分隐私的最新进展：系统性调查》
- en: Lea Demelius 1122    Roman Kern 1122    Andreas Trügler 112233
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Lea Demelius 1122    Roman Kern 1122    Andreas Trügler 112233
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Differential Privacy has become a widely popular method for data protection
    in machine learning, especially since it allows formulating strict mathematical
    privacy guarantees. This survey provides an overview of the state-of-the-art of
    differentially private centralized deep learning, thorough analyses of recent
    advances and open problems, as well as a discussion of potential future developments
    in the field. Based on a systematic literature review, the following topics are
    addressed: auditing and evaluation methods for private models, improvements of
    privacy-utility trade-offs, protection against a broad range of threats and attacks,
    differentially private generative models, and emerging application domains.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私已成为机器学习中数据保护的广泛受欢迎的方法，特别是因为它允许制定严格的数学隐私保障。本调查概述了差分隐私集中式深度学习的最新进展，深入分析了近期的进展和未解决的问题，并讨论了该领域未来潜在的发展。基于系统的文献综述，涉及以下主题：私密模型的审计和评估方法、隐私-效用权衡的改进、针对各种威胁和攻击的保护、差分隐私生成模型以及新兴应用领域。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Deep learning is the state-of-the-art of machine learning used to solve complex
    tasks in various fields, including computer vision and natural language processing,
    as well as applications in different domains ranging from healthcare to finance.
    As the performance of these models relies on a high amount of training data, the
    rise of deep learning comes with an increased interest in collecting and analysing
    more and more data. However, data often includes personal or confidential information,
    making privacy a pressing concern. This development is also reflected in legislation
    that was recently put in place to protect personal information and avoid identification
    of individuals, e.g., the General Data Protection Regulation (GDPR) in the European
    Union or the California Consumer Privacy Act (CCPA).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是当前最先进的机器学习技术，用于解决计算机视觉、自然语言处理等多个领域的复杂任务，以及医疗保健、金融等不同领域的应用。由于这些模型的性能依赖于大量的训练数据，深度学习的兴起伴随着对收集和分析更多数据的兴趣增加。然而，数据往往包含个人或机密信息，这使得隐私成为一个紧迫的问题。这一发展也体现在最近出台的保护个人信息和避免识别个人的立法中，例如欧洲联盟的《通用数据保护条例》（GDPR）或加利福尼亚州的《消费者隐私法案》（CCPA）。
- en: In response, privacy-enhancing technologies are growing in popularity. Cryptographic
    techniques [[93](#bib.bib93)] like homomorphic encryption and secure multi-party
    computation are used to protect against direct information leakage during data
    analysis. However, sensitive information can still be leaked indirectly via the
    output of the analysis and compromise privacy. For example, a machine learning
    model trained to diagnose a disease might make it possible to reconstruct specific
    information about individuals in the training dataset, even when the computation
    is encrypted. To avoid this kind of privacy leaks, output privacy-preserving techniques
    can be applied. One common approach is to use anonymization techniques like k-anonymity
    [[99](#bib.bib99)], l-diversity [[74](#bib.bib74)] or t-closeness [[67](#bib.bib67)].
    However, it is now well known that anonymization often cannot prevent re-identification
    [[19](#bib.bib19), [49](#bib.bib49), [81](#bib.bib81)] and the privacy risk is
    not quantifiable. Hence, differential privacy (DP) [[42](#bib.bib42)] was proposed
    to provide output privacy guarantees. DP is a mathematical probabilistic definition
    that makes it possible to hide information about each single datapoint (e.g.,
    about each individual) while allowing inquiries about the whole dataset (e.g.,
    a population) by adding curated noise. While DP provides a good utility-privacy
    trade-off in many cases (especially statistical queries and traditional machine
    learning methods like logistic regression or support vector machines), the combination
    of differential privacy and deep learning poses many challenges arising from the
    high-dimensionality, the high number of training steps, and the non-convex objective
    functions in deep learning. As a response, considerable progress has been made
    in the past few years addressing the difficulties and opportunities of differentially
    private deep learning (DP-DL).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回应，隐私增强技术越来越受欢迎。加密技术[[93](#bib.bib93)]如同态加密和安全多方计算被用来防止数据分析过程中的直接信息泄露。然而，敏感信息仍然可能通过分析结果间接泄露，从而危及隐私。例如，一个用于诊断疾病的机器学习模型可能会使得在训练数据集中关于个体的特定信息得以重建，即使计算是加密的。为避免这种隐私泄露，可以应用输出隐私保护技术。一种常见的方法是使用如k-匿名性[[99](#bib.bib99)]、l-多样性[[74](#bib.bib74)]或t-接近性[[67](#bib.bib67)]的匿名化技术。然而，现在已知匿名化通常无法防止重新识别[[19](#bib.bib19)、[49](#bib.bib49)、[81](#bib.bib81)]，且隐私风险不可量化。因此，提出了差分隐私（DP）[[42](#bib.bib42)]来提供输出隐私保障。DP是一种数学概率定义，可以在允许查询整个数据集（如人口）的同时，通过添加精心设计的噪声隐藏关于每个单独数据点（如每个个体）的信息。虽然DP在许多情况下（尤其是统计查询和传统机器学习方法如逻辑回归或支持向量机）提供了良好的效用-隐私权衡，但差分隐私与深度学习的结合带来了许多挑战，这些挑战源于深度学习中的高维度、高训练步骤数量和非凸目标函数。因此，近年来在解决差分隐私深度学习（DP-DL）困难和机遇方面取得了显著进展。
- en: 1.1 Contributions of this survey
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 本调查的贡献
- en: This survey provides a comprehensive analysis of recent advances in differentially
    private deep learning (DP-DL), focusing on centralized deep learning. Distributed,
    federated and collaborative deep learning methods and applications have their
    unique characteristics and challenges, and deserve a separate survey. We also
    specifically focus on methods that do not presume convex objective functions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查提供了对差分隐私深度学习（DP-DL）最新进展的全面分析，重点关注集中式深度学习。分布式、联邦式和协作式深度学习方法及应用具有其独特特征和挑战，值得另行调查。我们还特别关注那些不假定凸目标函数的方法。
- en: 'Our main contributions are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献包括：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: the thorough systematic literature review of DP-DL
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对DP-DL的彻底系统文献回顾
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'the identification of the research focuses of the last years (2019-2023): 1)
    evaluating and auditing DP-DL models, 2) improving the privacy-utility trade-off
    of DP-DL, 3) DP-DL against threats other than membership and attribute inference,
    4) differentially private generative models, and 5) DP-DL for specific applications'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 识别了近年来（2019-2023）的研究重点：1）评估和审计DP-DL模型，2）改善DP-DL的隐私-效用权衡，3）DP-DL应对除了成员身份和属性推断之外的威胁，4）差分隐私生成模型，以及5）DP-DL在特定应用中的应用
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: the analysis and contextualization of the different research trends including
    their potential future development
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对不同研究趋势的分析和背景化，包括它们潜在的未来发展
- en: 'Previous reviews of deep learning and differential privacy (see Table [1](#S1.T1
    "Table 1 ‣ 1.1 Contributions of this survey ‣ 1 Introduction ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")) primarily
    cover the privacy threats specific to deep learning and the most common differential
    privacy methods used for protection. In contrast, this survey goes beyond the
    basic methods and systematically investigates advances and new paths explored
    in the field since 2019\. We provide the reader with advanced understanding of
    the state-of-the-art methods, the challenges, and opportunities of differentially
    private deep learning.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以前关于深度学习和差分隐私的综述（见表[1](#S1.T1 "表 1 ‣ 1.1 本调查的贡献 ‣ 1 引言 ‣ 集中式深度学习中的差分隐私最新进展：系统性综述")）主要涵盖了深度学习特有的隐私威胁以及用于保护的最常见的差分隐私方法。相比之下，本调查超越了基本方法，系统性地研究了自2019年以来该领域的进展和新路径。我们为读者提供了对最先进方法的深入理解、差分隐私深度学习的挑战和机会。
- en: 'Table 1: Reviews on differentially private deep learning (DL-DP).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：关于差分隐私深度学习（DL-DP）的综述。
- en: 'Review Year Systematic Contribution Zhao et al. [[128](#bib.bib128)] 2019 -
    privacy attacks on DL, basic DP-DL methods Ha et al. [[55](#bib.bib55)] 2019 -
    privacy attacks on DL, basic DP-DL methods Shen and Zhong [[96](#bib.bib96)] 2021
    - comparison of basic DP-DL models Ouadrhiri and Abdelhadi [[84](#bib.bib84)]
    2022 - DP variants and mechanisms for deep and federated learning This survey
    2023 ✓ recent advances in DP-DL: evaluation and auditing, privacy-utility trade-off,
    threats and attacks, synthetic data and generative models, open problems'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 综述 年份 系统性贡献 Zhao 等[[128](#bib.bib128)] 2019 - 深度学习的隐私攻击，基础差分隐私深度学习方法 Ha 等[[55](#bib.bib55)]
    2019 - 深度学习的隐私攻击，基础差分隐私深度学习方法 Shen 和 Zhong[[96](#bib.bib96)] 2021 - 基础差分隐私深度学习模型的比较
    Ouadrhiri 和 Abdelhadi[[84](#bib.bib84)] 2022 - 深度学习和联邦学习的差分隐私变体和机制 本调查 2023 ✓
    差分隐私深度学习的最新进展：评估和审计，隐私-效用权衡，威胁和攻击，合成数据和生成模型，开放问题
- en: Additional to the reviews mentioned above, a range of broader reviews exist,
    e.g., about differential private machine learning (without focusing on deep learning)
    [[53](#bib.bib53), [131](#bib.bib131), [21](#bib.bib21)], privacy-preserving deep
    learning (without focusing on differential privacy) [[28](#bib.bib28), [100](#bib.bib100),
    [56](#bib.bib56), [76](#bib.bib76), [23](#bib.bib23), [106](#bib.bib106)], or
    privacy-preserving machine learning in general [[70](#bib.bib70), [117](#bib.bib117)].
    These papers give a good overview, while our survey follows a more detailed approach
    focusing on recent developments of differential privacy in centralized deep learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述综述外，还有一些更广泛的综述，例如关于差分隐私机器学习（未专注于深度学习）的[[53](#bib.bib53)、[131](#bib.bib131)、[21](#bib.bib21)]，隐私保护深度学习（未专注于差分隐私）的[[28](#bib.bib28)、[100](#bib.bib100)、[56](#bib.bib56)、[76](#bib.bib76)、[23](#bib.bib23)、[106](#bib.bib106)]，或一般的隐私保护机器学习[[70](#bib.bib70)、[117](#bib.bib117)]。这些论文提供了良好的概述，而我们的调查则采取了更详细的方法，专注于差分隐私在集中式深度学习中的最新进展。
- en: 2 Survey methodology
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 调查方法
- en: 'We conducted a systematic literature search on Scopus using a predefined search
    query, and subsequent automatic and manual assessment according to specific inclusion/exclusion
    criteria. The full process is depicted in Figure [1](#S2.F1 "Figure 1 ‣ 2 Survey
    methodology ‣ Recent Advances of Differential Privacy in Centralized Deep Learning:
    A Systematic Survey").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Scopus 上进行了系统的文献搜索，使用了预定义的搜索查询，并根据特定的纳入/排除标准进行了自动和手动评估。完整过程如图[1](#S2.F1
    "图 1 ‣ 2 调查方法 ‣ 集中式深度学习中的差分隐私最新进展：系统性综述")所示。
- en: The Scopus query filtered for documents with the keywords "differential privacy,"
    "differentially private," or "differential private" combined with "neural network,"
    "deep learning," or "machine learning," but without mention of "federated learning,"
    "collaborative," "distributed," or "edge" in their title or abstract. The documents
    were restricted to articles, conference papers, reviews, or book chapters published
    in journals, proceedings, or books in computer science or engineering. Only documents
    published in English between 2019 and March 2023 were considered.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Scopus 查询筛选了包含关键词“差分隐私（differential privacy）”、“差分隐私的（differentially private）”或“差分隐私（differential
    private）”与“神经网络（neural network）”、“深度学习（deep learning）”或“机器学习（machine learning）”结合的文档，但其标题或摘要中未提及“联邦学习（federated
    learning）”、“协作的（collaborative）”、“分布式的（distributed）”或“边缘（edge）”。文档限制为在计算机科学或工程领域的期刊、会议论文、综述或书籍中发表的文章。只考虑了2019年至2023年3月间以英语发表的文献。
- en: Additionally, documents were only included if they were cited at least ten times
    or were in the top 10% of most cited papers of the corresponding year. This inclusion
    criterion allowed manual review of the most influential works in the field. The
    second part of the criterion was added to mitigate the bias towards older papers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，只有当文档至少被引用十次或位于相应年份被引用最多的前10%时，才会被纳入文献。这一纳入标准允许对领域内最有影响力的工作进行手动审查。标准的第二部分旨在减轻对较旧文献的偏见。
- en: 'In the last step, the remaining documents were manually reviewed. Whether a
    document was included in this survey was decided based on the following criteria:
    First, differential privacy must be a key topic of the study. Second, the methods
    must include neural networks with results relevant for deep learning. Third, we
    only included works about centralized learning in contrast to distributed learning.
    Fourth, we excluded reinforcement learning and focused on the supervised and unsupervised
    learning paradigms typical for centralized deep learning. Last, one document was
    excluded because of retraction.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，剩余的文档被手动审查。是否纳入此调查是根据以下标准决定的：首先，差分隐私必须是研究的关键主题。其次，方法必须包括与深度学习相关的神经网络结果。第三，我们只包括关于集中式学习的工作，而不是分布式学习。第四，我们排除了强化学习，专注于典型于集中式深度学习的监督学习和无监督学习范式。最后，一篇文档因撤稿而被排除。
- en: '![Refer to caption](img/441a63604279f3e313af9cff07123417.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/441a63604279f3e313af9cff07123417.png)'
- en: 'Figure 1: Flow diagram of the paper selection process.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：论文选择过程的流程图。
- en: 3 Preliminaries
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步工作
- en: 3.1 Differential Privacy (DP)
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 差分隐私（DP）
- en: Differential Privacy (DP) [[42](#bib.bib42)] is a mathematical definition of
    privacy that aims at protecting details about individuals while still allowing
    general learnings from their data. A randomized algorithm $\mathcal{M}:\mathcal{D}\rightarrow\mathcal{R}$
    with domain $\mathcal{D}$ and range $\mathcal{R}$ is $\epsilon$-differentially
    private if for any two datasets $x,y\in\mathcal{D}$ differing on at most one datapoint,
    and any subset of outputs $\mathcal{S}\subseteq\mathcal{R}$, it holds that
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私（DP）[[42](#bib.bib42)]是对隐私的数学定义，旨在保护个人的详细信息，同时仍允许从其数据中进行一般学习。一个随机算法$\mathcal{M}:\mathcal{D}\rightarrow\mathcal{R}$，其定义域为$\mathcal{D}$，值域为$\mathcal{R}$，如果对任意两个数据集$x,y\in\mathcal{D}$（最多只在一个数据点上不同）和任何输出子集$\mathcal{S}\subseteq\mathcal{R}$，都有
- en: '|  | $Pr[\mathcal{M}(x)\in\mathcal{S}]\leq e^{\epsilon}Pr[\mathcal{M}(y)\in\mathcal{S}]$
    |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $Pr[\mathcal{M}(x)\in\mathcal{S}]\leq e^{\epsilon}Pr[\mathcal{M}(y)\in\mathcal{S}]$
    |  | (1) |'
- en: where $Pr[]$ denotes the probability, and $\epsilon$ is the privacy risk (also
    referred to as the privacy budget). Simply put, a single datapoint (usually corresponding
    to an individual) has only a limited impact on the output of the analysis. $\epsilon$
    quantifies the maximum amount of information the output of the algorithm can disclose
    about the datapoint. Therefore, a lower $\epsilon$ results in stronger privacy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Pr[]$表示概率，$\epsilon$是隐私风险（也称为隐私预算）。简而言之，单个数据点（通常对应于个人）对分析结果的影响是有限的。$\epsilon$量化了算法输出可以泄露关于数据点的最大信息量。因此，较低的$\epsilon$意味着更强的隐私保护。
- en: DP is achieved by computing the sensitivity of the algorithm in question to
    the absence/presence of a datapoint and adding random noise accordingly. The most
    common noise distributions used for DP are Laplace, Gaussian, and Exponential
    distribution. The amount of added noise relative to the sensitivity determines
    the privacy risk.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: DP通过计算算法对数据点的缺失/存在的敏感度并相应地添加随机噪声来实现。最常用的DP噪声分布是拉普拉斯分布、高斯分布和指数分布。添加的噪声量相对于敏感度决定了隐私风险。
- en: 'Differential privacy is not only quantifiable but also has a number of other
    benefits: For one, it is independent of both the input data and auxiliary information
    such as publicly available data or future knowledge. Moreover, it is immune to
    post-processing, and composable, i.e., the sequential or parallel application
    of two $\epsilon$-differentially private algorithms is at least $2\epsilon$-DP.
    Advanced composition theorems can even prove lower overall privacy bounds. Another
    advantage of DP is its flexibility: The noise can be applied at different points
    in the data flow, for example on the input data, on the output of the algorithm,
    or somewhere in between (e.g., during training of a deep learning model). It can
    also be combined with other privacy-enhancing technologies like homomorphic encryption,
    secure multi-party computation, or federated learning.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私不仅是可量化的，还有许多其他好处：首先，它独立于输入数据和辅助信息，如公开数据或未来知识。此外，它对后处理免疫，并且具有可组合性，即两个$\epsilon$-差分隐私算法的顺序或并行应用至少是$2\epsilon$-DP。高级组合定理甚至可以证明更低的整体隐私界限。差分隐私的另一个优点是其灵活性：噪声可以应用于数据流中的不同点，例如在输入数据上、算法输出上或介于两者之间的地方（例如，在深度学习模型训练期间）。它还可以与其他隐私增强技术结合使用，如同态加密、安全多方计算或联邦学习。
- en: While these advantages lead to the widespread acceptance of differential privacy
    as the gold standard for privacy protection, it also comes with its challenges.
    Firstly, the unitless and probabilistic privacy parameter $\epsilon$ is difficult
    to interpret, and thus choosing an appropriate value is challenging. The interested
    reader is referred to [[38](#bib.bib38)] for a list of values used in real-world
    applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些优点导致了差分隐私被广泛接受为隐私保护的**黄金标准**，但它也面临一些挑战。首先，无单位和概率性的隐私参数$\epsilon$难以解读，因此选择合适的值具有挑战性。感兴趣的读者可以参考[[38](#bib.bib38)]，了解实际应用中使用的值列表。
- en: Another challenge of DP is its influence on the algorithm’s outputs and, consequently,
    its properties. The dominant issue is the privacy-utility trade-off. Nevertheless,
    it can also influence the performance, usability, fairness, or robustness of the
    algorithm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私的另一个挑战是其对算法输出的影响，进而影响其属性。主要问题是隐私与效用的权衡。然而，它也可能影响算法的性能、可用性、公平性或稳健性。
- en: 'Additionally, DP is not easy to understand for laypeople: 1) It is important
    to note that DP does not offer perfect privacy. DP provides a specific interpretation
    of privacy, but depending on the context, other interpretations might be expected
    or more relevant. Besides, the privacy protection depends both on the privacy
    parameter $\epsilon$ and the unit/granularity of privacy, i.e., on what is considered
    as one record. That is to say, merely stating that an algorithm is DP does not
    ensure a substantial guarantee. 2) As DP is a mathematical definition that only
    defines the constraints but does not mandate an implementation, there exist many
    different algorithms that satisfy DP, from simple statistical analyses to machine
    learning algorithms. Some assume a central trusted party that has access to all
    the data (global DP), others apply noise before data aggregation (local DP). 3)
    There exist a wide range of DP variants and extensions. Just within the years
    2008 and 2021, over 250 new notions were introduced [[39](#bib.bib39)]. They differ,
    for example, in how they quantify privacy loss, which properties are protected,
    and what capabilities the attacker is assumed to have. The most common extension
    is approximate DP (often simply referred to as $(\epsilon,\delta)$-DP), where
    the algorithm is $\epsilon$-DP with a probability of $1-\delta$. The failure probability
    $\delta$ is typically set to less than the inverse of the dataset size. Other
    common relaxations include zero-concentrated DP (zCDP) [[43](#bib.bib43)] and
    Rényi DP [[77](#bib.bib77)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DP 对于外行人来说并不容易理解：1）需要注意的是，DP 并不能提供完美的隐私保护。DP 提供了一种特定的隐私解释，但根据上下文，可能会有其他期望的或更相关的解释。此外，隐私保护依赖于隐私参数
    $\epsilon$ 和隐私的单位/粒度，即对什么被视为一个记录。也就是说，仅仅声明一个算法是 DP 并不能确保实质性的保证。2）由于 DP 是一个数学定义，仅定义了约束条件而不要求具体实现，因此存在许多不同的算法满足
    DP，从简单的统计分析到机器学习算法。某些算法假设有一个中心可信方可以访问所有数据（全局 DP），而其他算法在数据汇总之前应用噪声（局部 DP）。3）存在广泛的
    DP 变体和扩展。仅在 2008 年至 2021 年间，就有超过 250 个新概念被引入 [[39](#bib.bib39)]。它们在隐私损失的量化方式、保护的属性以及假定攻击者的能力方面存在差异。最常见的扩展是近似
    DP（通常简单地称为 $(\epsilon,\delta)$-DP），其中算法是以概率 $1-\delta$ 的 $\epsilon$-DP。失败概率 $\delta$
    通常设置为小于数据集大小的倒数。其他常见的放宽条件包括零集中 DP（zCDP） [[43](#bib.bib43)] 和 Rényi DP [[77](#bib.bib77)]。
- en: 3.2 Deep neural networks and stochastic gradient descent (SGD)
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 深度神经网络和随机梯度下降（SGD）
- en: 'Deep neural networks consist of connected nodes organised in layers, namely
    an input layer, multiple hidden layers, and an output layer. Each node applies
    a set of weights to its inputs and passes its sum through a non-linear activation
    function. The network can learn to perform different tasks, e.g., classification
    on complex data by minimizing the loss (i.e., the difference between the predictions
    of the model and the desired outputs). As the objective function of deep neural
    networks are often non-convex, it is generally not feasible to solve this optimization
    problem analytically. Instead, iterative optimization algorithms are applied,
    most commonly variants of stochastic gradient descent (SGD): At each time step
    $j$, the model weights $w$ are updated according to the gradients of the objective
    function $\mathcal{L}$ computed on a randomly selected subset of the training
    dataset (=batch) $\mathcal{B}$. In Equation [2](#S3.E2 "Equation 2 ‣ 3.2 Deep
    neural networks and stochastic gradient descent (SGD) ‣ 3 Preliminaries ‣ Recent
    Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey"),
    $x_{i}$ denotes a record from the training set with the corresponding target output
    $y_{i}$, $\eta$ is the learning rate, and $|\mathcal{B}|$ the batch size.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '深度神经网络由组织在层中的连接节点组成，即输入层、多个隐藏层和输出层。每个节点对其输入应用一组权重，并通过非线性激活函数传递其总和。网络可以学习执行不同的任务，例如，通过最小化损失（即模型的预测与期望输出之间的差异）来对复杂数据进行分类。由于深度神经网络的目标函数通常是非凸的，因此通常无法通过解析方法解决这个优化问题。相反，应用了迭代优化算法，最常见的是随机梯度下降（SGD）的变体：在每个时间步
    $j$，模型权重 $w$ 根据在随机选择的训练数据子集（即批次）$\mathcal{B}$ 上计算的目标函数 $\mathcal{L}$ 的梯度进行更新。在方程
    [2](#S3.E2 "Equation 2 ‣ 3.2 Deep neural networks and stochastic gradient descent
    (SGD) ‣ 3 Preliminaries ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey") 中，$x_{i}$ 表示来自训练集的记录及其对应的目标输出 $y_{i}$，$\eta$
    是学习率，$|\mathcal{B}|$ 是批次大小。'
- en: '|  | $w_{j}=w_{j-1}-\eta\frac{1}{&#124;\mathcal{B}&#124;}\sum_{i\in\mathcal{B}}\nabla\mathcal{L}(w_{j-1},x_{i},y_{i})$
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{j}=w_{j-1}-\eta\frac{1}{\| \mathcal{B} \|}\sum_{i\in\mathcal{B}}\nabla\mathcal{L}(w_{j-1},x_{i},y_{i})$
    |  | (2) |'
- en: After training the model, e.g., with SGD, neural networks can be used to make
    predictions for previously unseen data. Models that perform well on new data are
    well-generalized, while models that perform well only on the training data are
    considered overfitted.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完模型之后，例如使用SGD，神经网络可以用于对先前未见的数据进行预测。对新数据表现良好的模型被认为是良好泛化的，而仅在训练数据上表现良好的模型被认为是过拟合的。
- en: Deep learning models are usually trained with a supervised or an unsupervised
    learning paradigm. In supervised learning, the training set is labeled, while
    in unsupervised learning the model learns to identify patterns without access
    to a ground truth. Apart from the fully-connected layers described above, neural
    networks often include other types of architectures and layers. One popular example
    are convolutional layers, which use filters to extract local patterns, for example
    edges in images.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常使用监督学习或无监督学习范式进行训练。在监督学习中，训练集是带标签的，而在无监督学习中，模型在没有访问真实标签的情况下学习识别模式。除了上述的全连接层，神经网络通常还包括其他类型的架构和层。一个流行的例子是卷积层，它使用滤波器提取局部模式，例如图像中的边缘。
- en: 3.3 Privacy threats for deep learning models
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度学习模型的隐私威胁
- en: 'Privacy attacks on deep learning models target either the training data or
    the model itself. They can take place in the training or the inference phase.
    During training, the adversary can not only be a passive observer but also actively
    change the training process. Moreover, the attacker can have access to the the
    whole model, i.e., its architecture, parameters, gradients and outputs (white-box
    access), or only the model’s outputs (black-box access). Typical privacy attacks
    include:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型上的隐私攻击目标是训练数据或模型本身。它们可以发生在训练阶段或推断阶段。在训练过程中，攻击者不仅可以是一个被动的观察者，还可以积极地改变训练过程。此外，攻击者可以访问整个模型，即其架构、参数、梯度和输出（白箱访问），或者仅访问模型的输出（黑箱访问）。典型的隐私攻击包括：
- en: 'Membership inference attack (MIA): The adversary attempts to infer whether
    a specific record was part of the training dataset. This type of attack exists
    both in the white- and black-box setting, where in the latter case one distinguishes
    further between access to the prediction vector and access to the label. The first
    (and still widely used) MIA on machine learning models was proposed by Shokri
    et al. [[97](#bib.bib97)]. They train shadow models that imitate the target model
    and based on their outputs on training and non-training data, a classification
    model learns to identify which records are members of the training data. The attack
    performs best on overfitted models [[122](#bib.bib122), [97](#bib.bib97)].'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 成员推断攻击（MIA）：攻击者试图推断特定记录是否是训练数据集的一部分。这种攻击在白箱和黑箱设置中都存在，其中在黑箱情况下进一步区分了对预测向量的访问和对标签的访问。第一个（并且仍然广泛使用的）MIA是在机器学习模型上由Shokri等人提出的[[97](#bib.bib97)]。他们训练模仿目标模型的影子模型，并基于这些影子模型在训练数据和非训练数据上的输出，分类模型学习识别哪些记录是训练数据的成员。该攻击在过拟合模型上表现最佳[[122](#bib.bib122),
    [97](#bib.bib97)]。
- en: 'Model inversion attack: The adversary tries to infer sensitive information
    about the training data, for example an attribute of a specific datapoint or features
    that characterize a certain class.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 模型反演攻击：攻击者试图推断关于训练数据的敏感信息，例如特定数据点的属性或特征，这些特征用于表征某个类别。
- en: 'Property inference attack: The attacker seeks to extract information about
    the training data that is not related to the training task. For example, they
    might be interested in statistical properties like the ratio of training samples
    that have a certain property.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 属性推断攻击：攻击者试图提取与训练任务无关的训练数据的信息。例如，他们可能对具有某种属性的训练样本的比例等统计属性感兴趣。
- en: 'Model extraction attack: The adversary learns a model that approximates the
    target model. While this threat only targets the model and not the training data,
    it can still increase the privacy risk as a successful model extraction can facilitate
    follow-up attacks like model inversion.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取攻击：攻击者学习一个近似目标模型的模型。虽然这种威胁仅针对模型而不是训练数据，但它仍然可能增加隐私风险，因为成功的模型提取可以促进后续攻击，如模型反演。
- en: In is important to note that especially in the case of model inversion and property
    inference the terms are not used consistently. For example, De Cristofaro [[36](#bib.bib36)]
    uses the term property inference for inferring features of a class, while others
    [[94](#bib.bib94), [87](#bib.bib87), [127](#bib.bib127)] (including us) refer
    to this as a type of model inversion.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在模型反演和属性推断的情况下，术语使用并不一致。例如，De Cristofaro [[36](#bib.bib36)] 使用“属性推断”来推断一个类别的特征，而其他文献
    [[94](#bib.bib94), [87](#bib.bib87), [127](#bib.bib127)]（包括我们）则将其称为模型反演的一种类型。
- en: 'Additionally, there are attacks not specific to privacy that could still be
    a threat to deep learning models:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些不特定于隐私的攻击可能仍对深度学习模型构成威胁：
- en: 'Adversarial attack: During the inference phase, the attacker manipulates inputs
    purposefully so that the model misclassifies them.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击：在推断阶段，攻击者故意操控输入，使模型将其误分类。
- en: 'Poisoning attack: The adversary perturbs the training samples so that they
    manipulate the model with the goal to reduce its accuracy or trigger specific
    misclassifications.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 中毒攻击：对手扰动训练样本，以操控模型从而降低其准确性或触发特定的误分类。
- en: For a more detailed description, we refer the reader to surveys that focus on
    privacy attacks [[36](#bib.bib36), [94](#bib.bib94), [95](#bib.bib95)]. A selection
    of attack implementations can, e.g., be found in the Adversarial Robustness Toolbox
    [[83](#bib.bib83)].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更详细的描述，我们建议读者参考关注隐私攻击的调查 [[36](#bib.bib36), [94](#bib.bib94), [95](#bib.bib95)]。攻击实现的选择可以在对抗鲁棒性工具箱中找到，例如
    [[83](#bib.bib83)]。
- en: 3.4 DP algorithms for deep learning
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 深度学习的 DP 算法
- en: 'The most popular algorithm for DP-DL is differentially private stochastic gradient
    descent (DP-SGD) by Abadi et al. [[11](#bib.bib11)]. It adapts classical SGD by
    perturbing the gradients with Gaussian noise. As the gradients norms are unbounded,
    they are clipped before to ensure a finite sensitivity. Equation [2](#S3.E2 "Equation
    2 ‣ 3.2 Deep neural networks and stochastic gradient descent (SGD) ‣ 3 Preliminaries
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey") thus becomes Equation [3](#S3.E3 "Equation 3 ‣ 3.4 DP algorithms for
    deep learning ‣ 3 Preliminaries ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey"), where $clip[]$ denotes the clipping function
    that clips the per-example gradients so that their norm does not exceed the clipping
    norm $C$, $\chi$ is a random vector drawn from a standard Gaussian distribution,
    and $\sigma$ is the standard deviation of the added Gaussian noise.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: DP-DL 最受欢迎的算法是 Abadi 等人提出的差分隐私随机梯度下降（DP-SGD） [[11](#bib.bib11)]。它通过用高斯噪声扰动梯度来改编经典的
    SGD。由于梯度范数是无界的，因此在裁剪之前确保有限的敏感性。因此，方程 [2](#S3.E2 "方程 2 ‣ 3.2 深度神经网络和随机梯度下降 (SGD)
    ‣ 3 初步知识 ‣ 差分隐私在集中式深度学习中的最新进展：系统性调查") 变成了方程 [3](#S3.E3 "方程 3 ‣ 3.4 深度学习的 DP 算法
    ‣ 3 初步知识 ‣ 差分隐私在集中式深度学习中的最新进展：系统性调查")，其中 $clip[]$ 表示裁剪函数，用于裁剪每例梯度，使其范数不超过裁剪范数
    $C$，$\chi$ 是从标准高斯分布中抽取的随机向量，$\sigma$ 是附加高斯噪声的标准差。
- en: '|  | $w_{j}=w_{j-1}-\eta\frac{1}{&#124;\mathcal{B}&#124;}\sum_{i\in\mathcal{B}}clip\left[\nabla\mathcal{L}(w_{j-1},x_{i},y_{i}),C\right]+\frac{\sigma
    C}{&#124;\mathcal{B}&#124;}\chi$ |  | (3) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{j}=w_{j-1}-\eta\frac{1}{&#124;\mathcal{B}&#124;}\sum_{i\in\mathcal{B}}clip\left[\nabla\mathcal{L}(w_{j-1},x_{i},y_{i}),C\right]+\frac{\sigma
    C}{&#124;\mathcal{B}&#124;}\chi$ |  | (3) |'
- en: While the privacy-utility trade-off inherent to DP still applies (i.e., information
    is lost), DP-SGD can improve generalization and thus improve accuracy on the validation
    dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DP 固有的隐私-效用权衡仍然适用（即信息丢失），DP-SGD 可以改善泛化能力，从而提高验证数据集上的准确性。
- en: It is important to note that the per-example gradient clipping in DP-SGD used
    to bind each record’s sensitivity differs from the the batch-wise gradient clipping
    sometimes used to improve stability and convergence of the training process [[125](#bib.bib125)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，DP-SGD 中用于绑定每个记录的敏感性的每例梯度裁剪与有时用于提高训练过程稳定性和收敛性的批量梯度裁剪不同 [[125](#bib.bib125)]。
- en: Another popular method for training a deep learning model in a differentially
    private manner is PATE (Private aggregation of teacher ensembles) [[85](#bib.bib85)].
    PATE is a semi-supervised approach that needs public data. However, in contrast
    to DP-SGD, PATE can be applied to every machine learning technique. First, several
    teacher models are trained on separate private datasets. Next, the public data
    is labeled based on the differentially private aggregation of the predictions
    of the teacher models. Finally, a student model is trained on the noisily labeled
    public data, which can be made public afterwards.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的以差分隐私方式训练深度学习模型的方法是 PATE（教师集的私人聚合）[[85](#bib.bib85)]。PATE 是一种半监督方法，需要公共数据。然而，与
    DP-SGD 不同的是，PATE 可以应用于所有机器学习技术。首先，训练多个教师模型，每个模型在不同的私人数据集上进行训练。接下来，根据教师模型预测的差分隐私聚合对公共数据进行标注。最后，使用带有噪声的标注公共数据训练学生模型，该模型可以在之后公开。
- en: An alternative to rendering the deep learning model itself differentially private
    is output perturbation, where noise is added to the model’s predictions. However,
    with this method, the privacy budget increases with every prediction made making
    it necessary to limit the number of inferences that can be made with the model.
    For an example of output perturbation in deep learning see Ye et al. [[121](#bib.bib121)].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习模型自身实现差分隐私的另一种替代方法是输出扰动，其中向模型的预测结果中添加噪声。然而，采用这种方法时，隐私预算会随着每次预测的增加而增加，因此需要限制模型的推断次数。有关深度学习中输出扰动的示例，请参见
    Ye 等人 [[121](#bib.bib121)]。
- en: 4 Auditing and Evaluation
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 审计与评估
- en: 'Auditing and evaluating deep learning models is important to ensure that they
    provide effective privacy preservation while maintaining utility. As differential
    privacy is always a trade-off between privacy and utility, privacy evaluation
    helps choosing a suitable privacy budget: high enough to protect sensitive information
    but low enough to provide sufficient accuracy. Which level of accuracy is sufficient
    is application-dependent. In some cases, it might also be important to evaluate
    not only the utility overall but also on different subgroups present in the training
    data to detect potential unfairness and biases. In this section, we first discuss
    the attack-based evaluation of empirical privacy risks, and then the evaluation
    of accuracy on subgroups.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 审计和评估深度学习模型对于确保它们在保持效用的同时有效保护隐私至关重要。由于差分隐私始终是在隐私和效用之间的权衡，隐私评估有助于选择合适的隐私预算：足够高以保护敏感信息，但足够低以提供足够的准确性。什么水平的准确性足够取决于应用情况。在某些情况下，评估不仅要考虑整体效用，还要对训练数据中的不同子群体进行评估，以检测潜在的不公平和偏见。在本节中，我们首先讨论基于攻击的实证隐私风险评估，然后讨论对子群体的准确性评估。
- en: 4.1 Attack-based Evaluation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于攻击的评估
- en: 'While the differential privacy parameter $\epsilon$ provides an upper bound
    on the privacy loss, attack-based evaluation can give a lower bound. Even though
    empirical privacy evaluation cannot provide any guarantee, it can be useful to
    answer questions regarding the practical meaning of different privacy budgets,
    or how different aspects (e.g., the applied differential privacy notion) influence
    the actual privacy risk. Moreover, the considerable gap that was repeatedly observed
    between the lower and upper bound lead to the assumption that the worst-case setting
    is too pessimistic and actual privacy is much lower. As a consequence, high privacy
    budgets (i.e., high $\epsilon$) were often chosen in practical applications [[38](#bib.bib38)].
    While this choice improved the models’ utility, the question remained if the gap
    between lower and upper privacy bounds implies that the DP guarantee is too pessimistic,
    or if the applied attacks were just weak and future stronger attacks would be
    able to fully exploit the privacy budget. Additionally, attacks can be used to
    evaluate whether models preserve privacy in specific settings, e.g., in cases
    where a class consists of instances by a single individual. The papers reviewed
    in this section give insights into these matters. Table [2](#S4.T2 "Table 2 ‣
    4.1 Attack-based Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances of Differential
    Privacy in Centralized Deep Learning: A Systematic Survey") provides an overview
    of the reviewed works, their applied attacks and evaluation metrics. Table [3](#S4.T3
    "Table 3 ‣ 4.1 Attack-based Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey") shows
    which datasets were used for auditing.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然差分隐私参数$\epsilon$提供了隐私损失的上界，但基于攻击的评估可以给出下界。尽管经验性隐私评估不能提供任何保证，但它可以用于回答关于不同隐私预算的实际意义的问题，或者不同方面（例如应用的差分隐私概念）如何影响实际隐私风险。此外，观察到的下界和上界之间的显著差距导致了这样一个假设：最坏情况下的设置过于悲观，实际隐私要低得多。因此，在实际应用中通常选择较高的隐私预算（即较高的$\epsilon$）[[38](#bib.bib38)]。虽然这种选择提高了模型的效用，但问题仍然存在：下界和上界隐私界限之间的差距是否意味着差分隐私保证过于悲观，或者应用的攻击只是较弱，未来更强的攻击是否能够充分利用隐私预算。此外，攻击可以用来评估模型在特定设置下是否保留了隐私，例如，在一个类由单一个体的实例组成的情况下。本节回顾的文献提供了这些问题的见解。表[2](#S4.T2
    "Table 2 ‣ 4.1 Attack-based Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")概述了回顾的工作、其应用的攻击和评估指标。表[3](#S4.T3
    "Table 3 ‣ 4.1 Attack-based Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")显示了用于审计的数据集。'
- en: 'Table 2: Summary of attack-based evaluation methods. The table includes the
    works reviewed in this chapter and states the used attack(s), if they assume black-
    or white-box access to the target model, and how the target model’s vulnerability
    was measured. MIA is the abbreviation for membership inference attack. The attacker’s
    advantage is a measure of privacy leakage proposed by Yeom et al. [[122](#bib.bib122)].
    $\epsilon_{LB}$ and $(\epsilon,\delta)_{LB}$ respectively refers to the empirical
    lower bound of the privacy budget.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基于攻击的评估方法总结。该表包括本章回顾的工作，并说明使用的攻击，是否假设对目标模型具有黑箱或白箱访问，以及如何测量目标模型的脆弱性。MIA是成员推断攻击的缩写。攻击者的优势是Yeom等人提出的隐私泄露度量[[122](#bib.bib122)]。$\epsilon_{LB}$和$(\epsilon,\delta)_{LB}$分别指代隐私预算的经验下界。
- en: Work Attack Black- or white-box Evaluation metric(s) Jayaraman and Evans, 2019
    [[59](#bib.bib59)] MIA & model inversion black- & white-box accuracy loss, attacker’s
    advantage Chen et al., 2020 [[31](#bib.bib31)] MIA white-box accuracy Leino and
    Fredrikson, 2020 [[66](#bib.bib66)] MIA white-box accuracy, recall, precision,
    attacker’s advantage Jagielski et al., 2020 [[58](#bib.bib58)] poisoning attack
    black-box $\epsilon_{LB}$ Nasr et al., 2021 [[82](#bib.bib82)] MIA & poisoning
    attacks black- & white-box $(\epsilon,\delta)_{LB}$ Park et al., 2019 [[87](#bib.bib87)]
    model inversion black-box success rate, impact of the attack Zhang et al., 2020
    [[127](#bib.bib127)] model inversion white-box accuracy
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 工作攻击黑盒或白盒评估度量 Jayaraman 和 Evans, 2019 [[59](#bib.bib59)] MIA和模型反转黑盒和白盒准确率损失，攻击者的优势
    Chen 等, 2020 [[31](#bib.bib31)] MIA 白盒准确率 Leino 和 Fredrikson, 2020 [[66](#bib.bib66)]
    MIA 白盒准确率，召回率，精确率，攻击者的优势 Jagielski 等，2020 [[58](#bib.bib58)] 污染攻击黑盒 $\epsilon_{LB}$
    Nasr 等，2021 [[82](#bib.bib82)] MIA和污染攻击黑盒和白盒 $(\epsilon,\delta)_{LB}$ Park 等，2019
    [[87](#bib.bib87)] 模型反转黑盒成功率，对攻击的影响 Zhang 等, 2020 [[127](#bib.bib127)] 模型反转白盒准确率
- en: 'Table 3: Datasets used for auditing DP-DL in the reviewed papers.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表格3：审计DP-DL所使用的数据集。
- en: '| Dataset | Data type | Paper |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 数据类型 | 论文 |'
- en: '| --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Adult Data Set [[6](#bib.bib6)] | tabular | [[66](#bib.bib66)] |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 人口普查数据集 [[6](#bib.bib6)] | 表格 | [[66](#bib.bib66)] |'
- en: '| AT&T Database of Faces [[1](#bib.bib1)] | images | [[87](#bib.bib87)] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| AT&T 人脸数据库 [[1](#bib.bib1)] | 图像 | [[87](#bib.bib87)] |'
- en: '| Breast Cancer Wisconsin Data Set [[7](#bib.bib7)] | tabular | [[66](#bib.bib66)]
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 威斯康星乳腺癌数据集 [[7](#bib.bib7)] | 表格 | [[66](#bib.bib66)] |'
- en: '| CIFAR-10 [[63](#bib.bib63)] | images | [[58](#bib.bib58), [120](#bib.bib120),
    [66](#bib.bib66), [82](#bib.bib82)] |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-10 [[63](#bib.bib63)] | 图像 | [[58](#bib.bib58), [120](#bib.bib120),
    [66](#bib.bib66), [82](#bib.bib82)] |'
- en: '| CIFAR-100 [[63](#bib.bib63)] | images | [[59](#bib.bib59), [66](#bib.bib66)]
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 [[63](#bib.bib63)] | 图像 | [[59](#bib.bib59), [66](#bib.bib66)]
    |'
- en: '| Fashion-MNIST [[113](#bib.bib113)] | images | [[58](#bib.bib58)] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Fashion-MNIST [[113](#bib.bib113)] | 图像 | [[58](#bib.bib58)] |'
- en: '| German Credit Data [[10](#bib.bib10)] | tabular | [[66](#bib.bib66)] |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 德国信用数据 [[10](#bib.bib10)] | 表格 | [[66](#bib.bib66)] |'
- en: '| Hepatitis Data Set [[9](#bib.bib9)] | tabular | [[66](#bib.bib66)] |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 肝炎数据集 [[9](#bib.bib9)] | 表格 | [[66](#bib.bib66)] |'
- en: '| Labeled Faces in the Wild [[5](#bib.bib5)] | images | [[66](#bib.bib66)]
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Labeled Faces in the Wild 人脸数据集 [[5](#bib.bib5)] | 图像 | [[66](#bib.bib66)]
    |'
- en: '| MNIST [[64](#bib.bib64)] | images | [[66](#bib.bib66), [82](#bib.bib82),
    [127](#bib.bib127)] |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| MNIST [[64](#bib.bib64)] | 图像 | [[66](#bib.bib66), [82](#bib.bib82), [127](#bib.bib127)]
    |'
- en: '| Pima Diabetes Data Set [[8](#bib.bib8)] | tabular | [[66](#bib.bib66)] |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Pima 糖尿病数据集 [[8](#bib.bib8)] | 表格 | [[66](#bib.bib66)] |'
- en: '| Purchase-100 [[4](#bib.bib4)] | tabular | [[59](#bib.bib59), [58](#bib.bib58),
    [82](#bib.bib82)] |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Purchase-100 [[4](#bib.bib4)] | 表格 | [[59](#bib.bib59), [58](#bib.bib58),
    [82](#bib.bib82)] |'
- en: '| VGGFace2 dataset [[3](#bib.bib3)] | images | [[87](#bib.bib87)] |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| VGGFace2 数据集 [[3](#bib.bib3)] | 图像 | [[87](#bib.bib87)] |'
- en: '| Yeast genomic dataset [[22](#bib.bib22)] | genomic | [[31](#bib.bib31)] |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 酵母基因组数据集 [[22](#bib.bib22)] | 基因 | [[31](#bib.bib31)] |'
- en: Jayaraman and Evans [[59](#bib.bib59)] analyzed how different relaxed notions
    of differential privacy influence the attack success. They use both membership
    inference and model inversion attacks, and study DP with advanced composition,
    zero-concentrated DP and Rényi DP. They conclude that relaxed DP definitions go
    hand in hand with increased attack success, i.e., privacy loss. That means that
    using modified privacy analysis can narrow the gap between theoretical privacy
    guarantee and empirical lower bound.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Jayaraman 和 Evans [[59](#bib.bib59)] 分析了不同松弛的差分隐私概念对攻击成功的影响。他们使用了成员推断和模型反转攻击，并研究了先进组合的DP，零集中的DP和Rényi
    DP。他们的结论是，松弛的DP定义与攻击成功（即隐私损失）密切相关。这意味着使用修改后的隐私分析可以缩小理论隐私保证和实验下限之间的差距。
- en: Chen et al. [[31](#bib.bib31)] confirmed the effectiveness of differential privacy
    also in case of high-dimensional training data using the example of genomic data.
    They evaluated a differentially private convolutional neural network (CNN) model
    with and without sparsity by launching a membership inference attack. Even though
    model sparsity can improve the model’s accuracy in the non-private setting (by
    mitigating overfitting), they showed that in the private setting it has a negative
    effect (but improves privacy).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等 [[31](#bib.bib31)] 通过使用基因组数据的例子，确认了差分隐私在高维训练数据的情况下的有效性。他们通过进行成员推断攻击来评估带有和不带有稀疏性的差分隐私卷积神经网络（CNN）模型。尽管模型稀疏性可以改善非隐私设置中模型的准确性（通过减少过拟合），但他们表明在隐私设置中，模型稀疏性具有负面效果（但可以改善隐私性）。
- en: While the previous papers used existing attacks to study different settings,
    the following works propose novel, stronger attacks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的论文使用了现有的攻击方法来研究不同的设置，但以下研究提出了新型、更强的攻击。
- en: 'Leino and Fredrikson [[66](#bib.bib66)] argued that memorization of sensitive
    information can not only be apparent in the model’s predictions but also in how
    it uses (externally given or internally learned) features. Features that are only
    predictive for the training dataset but not for the target data distribution can
    leak information even in well-generalized models. To evaluate the resulting privacy
    risk, they proposed a new white-box membership inference attack that also leverages
    the intermediate representations of the target model. As normal shadow model training
    does not lead to the same interpretation of internal features (even with identical
    model architecture, hyperparameters and training data), they linearly approximated
    each layer, launched a separate attack on each layer and trained a meta-model
    that combines the outputs of the layer-wise attacks. This novel attack was shown
    to be more effective than previous attacks. Training the target model with DP-SGD
    in general decreased the model’s vulnerability, but high $\epsilon$ (here: $\epsilon=16$)
    lead to privacy leakage comparable to the non-private setting.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Leino 和 Fredrikson [[66](#bib.bib66)] 认为，敏感信息的记忆不仅可以在模型的预测中显现出来，还可以体现在模型如何使用（外部给定或内部学习的）特征上。即使在良好泛化的模型中，仅对训练数据集预测有效但对目标数据分布无用的特征也可能泄露信息。为了评估由此产生的隐私风险，他们提出了一种新的白盒成员推断攻击，该攻击还利用了目标模型的中间表示。由于普通的阴影模型训练不会导致内部特征的相同解释（即使具有相同的模型架构、超参数和训练数据），他们对每一层进行了线性近似，对每一层发起了单独的攻击，并训练了一个结合了层级攻击输出的元模型。证明这种新型攻击比以前的攻击更为有效。一般来说，使用
    DP-SGD 训练目标模型会降低模型的脆弱性，但高 $\epsilon$（此处为 $\epsilon=16$）会导致隐私泄露，与非私密设置相当。
- en: Jagielski et al. [[58](#bib.bib58)] explicitly analyzed the gap between theoretical
    privacy guarantee (upper bound) and empirical evaluation (lower bound). The gap
    can be narrowed either by providing tighter privacy analyses (as discussed in
    context with Jayaraman and Evans [[59](#bib.bib59)]), or by developing stronger
    attacks (e.g., like Leino and Fredrikson [[66](#bib.bib66)]). Jagielski et al.
    [[58](#bib.bib58)] followed the latter approach and showed that the former is
    approaching its limits. Their novel attack is based on data poisoning, where the
    attacker purposefully perturbs some training samples to change the parameter distribution
    (e.g., by adding a white patch in the corner of an image). As this approach is
    made effective by using poisoning samples that induce large gradients, the gradient
    clipping of DP-SGD degrades the attack. Their proposed adjustment includes perturbing
    training samples so that parameters are changed in the direction of their lowest
    variance. Their estimated lower bound of the privacy loss $\epsilon_{LB}$ was
    only an order of magnitude below the theoretical upper bound. This suggested the
    end of continuously tighter privacy guarantees by refined analyses that in the
    past brought improvements by a thousandfold.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Jagielski 等人 [[58](#bib.bib58)] 明确分析了理论隐私保障（上界）和实证评估（下界）之间的差距。这个差距可以通过提供更紧密的隐私分析（如与
    Jayaraman 和 Evans [[59](#bib.bib59)] 的讨论中提到的那样）来缩小，或者通过开发更强的攻击（例如，Leino 和 Fredrikson
    [[66](#bib.bib66)] 的研究）。Jagielski 等人 [[58](#bib.bib58)] 采取了后者的方法，并展示了前者正在接近其极限。他们的新型攻击基于数据中毒，其中攻击者故意扰动一些训练样本以改变参数分布（例如，通过在图像的角落添加白色补丁）。由于这种方法通过使用引发大梯度的中毒样本来有效，因此
    DP-SGD 的梯度裁剪降低了攻击效果。他们提出的调整包括扰动训练样本，使得参数沿着其最低方差的方向发生变化。他们估计的隐私损失下界 $\epsilon_{LB}$
    仅比理论上界低一个数量级。这表明，过去通过精细分析带来的隐私保障不断增强的趋势已接近尾声。
- en: 'Nasr et al. [[82](#bib.bib82)] extended this study further by testing how different
    assumptions about the attacker’s capabilities influence the empirical lower bound.
    The investigated capabilities include: access to the parameters of the final and
    all intermediate models, and manipulation of inputs and gradients. In contrast
    to Jagielski et al. [[58](#bib.bib58)], they considered approximate DP. Their
    evaluations revealed that the strongest adversary can exploit the full privacy
    budget determined by theoretic analysis. They confirmed thereby that the possibilities
    to improve the privacy-utility trade-off via more advanced privacy analyses are
    exhausted. However, assuming limited capabilities of the attacker may reduce the
    upper bound further.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Nasr 等人 [[82](#bib.bib82)] 进一步扩展了这项研究，测试了关于攻击者能力的不同假设如何影响经验下限。研究的能力包括：访问最终模型及所有中间模型的参数，及操控输入和梯度。与
    Jagielski 等人 [[58](#bib.bib58)] 相比，他们考虑了近似DP。他们的评估揭示了最强的对手可以利用理论分析确定的全部隐私预算。他们确认了通过更先进的隐私分析来改善隐私-效用权衡的可能性已被耗尽。然而，假设攻击者能力有限可能进一步降低上限。
- en: 'The attacks applied in these papers are all considered typical assessments
    of DP: Membership inference is the most obvious test of DP arising from its definition;
    the model inversion attack applied by Jayaraman and Evans [[59](#bib.bib59)] tries
    to infer attributes of specific individuals - which is not possible if the whole
    record of the individual cannot be recovered; and poisoning attacks exploit the
    fact that DP has to work even with the worst-case dataset. However, model inversion
    attacks can also mean the inference of class attributes (instead of individual
    attributes). In some tasks (e.g., face recognition), a class refers to exactly
    one individual. The following works [[87](#bib.bib87), [127](#bib.bib127)] analyzed
    whether DP-DL also protects against this kind of model inversion attack.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些论文中应用的攻击都被认为是典型的差分隐私（DP）评估：成员推断是最明显的DP测试，来源于其定义；Jayaraman 和 Evans [[59](#bib.bib59)]
    应用的模型反演攻击尝试推断特定个体的属性——如果不能恢复个体的完整记录，这是不可能的；而中毒攻击则利用了DP必须在最坏情况下的数据集上仍然有效的事实。然而，模型反演攻击也可以意味着对类别属性的推断（而非个体属性）。在一些任务中（例如面部识别），一个类别指的正是一个个体。以下工作
    [[87](#bib.bib87), [127](#bib.bib127)] 分析了DP-DL 是否也能防御这种模型反演攻击。
- en: Park et al. [[87](#bib.bib87)] evaluated the privacy loss of a face recognition
    model by reconstructing the training images from the model’s predictions, and
    automatically measuring the attack success based on the performance of an evaluation
    model. Their results showed that even high privacy budgets (e.g., $\epsilon=8$)
    can provide protection against this model inversion attack compared to the non-private
    setting.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Park 等人 [[87](#bib.bib87)] 通过从模型的预测中重建训练图像来评估面部识别模型的隐私损失，并根据评估模型的性能自动测量攻击成功率。他们的结果表明，即使是较高的隐私预算（例如，$\epsilon=8$）也能在此模型反演攻击下提供保护，相比于非隐私设置。
- en: 'Zhang et al. [[127](#bib.bib127)] proposed a generative model inversion (GMI)
    attack also in the face recognition setting. They first trained a GAN (Generative
    Adversarial Network; see detailed explanation in Section [7](#S7 "7 Differentially
    private generative models ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey")) on public data to generate realistic images,
    and then reconstructed the sensitive face regions by finding the values that maximize
    the likelihood. They showed that DP-SGD could not prevent their attack. They also
    argued that higher predictive power of the target models goes hand in hand with
    increased vulnerability to the attack.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [[127](#bib.bib127)] 在面部识别设置中提出了生成模型反演（GMI）攻击。他们首先在公共数据上训练了一个 GAN（生成对抗网络；详细解释见第
    [7](#S7 "7 Differentially private generative models ‣ Recent Advances of Differential
    Privacy in Centralized Deep Learning: A Systematic Survey") 节），以生成逼真的图像，然后通过寻找最大化似然的值来重建敏感面部区域。他们展示了DP-SGD无法防御他们的攻击。他们还认为，目标模型的预测能力越强，对攻击的脆弱性也越大。'
- en: 'Further discussions about the relevance of this kind of model inversion attack
    can be found in Section [9](#S9 "9 Discussion and future directions ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey").'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '关于这种模型反演攻击相关性的进一步讨论可以在第 [9](#S9 "9 Discussion and future directions ‣ Recent
    Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey")
    节中找到。'
- en: '4.2 Evaluation of subgroups: bias and fairness'
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 子组评估：偏见与公平性
- en: The evaluation of accuracy on subgroups is a standard approach to identify biases.
    In general, models should avoid the unfair treatment of different groups especially
    those based on legally protected attributes like gender, religion and ethnicity.
    Bagdasaryan et al. [[20](#bib.bib20)] showed that applying DP-SGD leads not only
    to an overall accuracy loss but underrepresented classes are disparately affected.
    While a greater imbalance in the training data seems to increase the accuracy
    gap, Farrand et al. [[44](#bib.bib44)] showed that it is significant even in cases
    of a 30-70% split, and also for loose privacy guarantees. The observed effect
    is believed to mainly arise from the clipping of the gradients, which penalizes
    those samples that result in bigger gradients, i.e., outliers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对子组的准确性评估是识别偏差的标准方法。通常，模型应避免对不同群体的不公平对待，特别是那些基于法律保护属性的群体，如性别、宗教和种族。Bagdasaryan等人[[20](#bib.bib20)]显示，应用DP-SGD不仅导致整体准确性下降，而且对代表性不足的类别影响特别大。虽然训练数据中的不平衡性似乎会增加准确性差距，Farrand等人[[44](#bib.bib44)]却显示，即使在30-70%的拆分情况下，这种差距也是显著的，并且在松散隐私保障的情况下也存在。这种观察到的效应被认为主要源于梯度的剪辑，这会惩罚那些导致更大梯度的样本，即离群值。
- en: 5 Improving the privacy-utility trade-off of DP-DL
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 改善DP-DL的隐私-效用权衡
- en: 'The main challenge of DP-DL is that by setting meaningful privacy guarantees,
    utility often deteriorates strongly. In recent years, many propositions for improved
    DP-DL (mostly DP-SGD) were made that can increase accuracy at the same privacy.
    Table [4](#S5.T4 "Table 4 ‣ 5 Improving the privacy-utility trade-off of DP-DL
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey") gives an overview of the proposed approaches. An alternative method would
    be to provide tighter theoretical bounds for the privacy loss without influencing
    the learning algorithm. An example of such an approach evaluated on deep learning
    can be found in Ding et la. [[40](#bib.bib40)]. However, as we already established
    in Section [4](#S4 "4 Auditing and Evaluation ‣ Recent Advances of Differential
    Privacy in Centralized Deep Learning: A Systematic Survey"), this line of research
    seems to have reached its limit.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'DP-DL的主要挑战在于，通过设置有意义的隐私保障，效用往往会大幅下降。近年来，许多改进DP-DL（主要是DP-SGD）的建议被提出，这些方法可以在相同隐私条件下提高准确性。表[4](#S5.T4
    "Table 4 ‣ 5 Improving the privacy-utility trade-off of DP-DL ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")概述了这些建议的方法。另一种方法是提供更紧密的理论界限，以减少隐私损失而不影响学习算法。一个评估在深度学习中应用的示例可以在Ding等人[[40](#bib.bib40)]的研究中找到。然而，正如我们在第[4](#S4
    "4 Auditing and Evaluation ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey")节中已经确定的，这条研究路线似乎已达到其极限。'
- en: 'Table 4: Summary of approaches for improving DP-DL.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：改进DP-DL的方法总结。
- en: '| Approach | Paper |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 论文 |'
- en: '| --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Adapting the model architecture | [[86](#bib.bib86)] |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 适应模型架构 | [[86](#bib.bib86)] |'
- en: '| Improving the hyperparameter selection | [[86](#bib.bib86), [71](#bib.bib71)]
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 改进超参数选择 | [[86](#bib.bib86), [71](#bib.bib71)] |'
- en: '| Applying feature engineering and transfer learning | [[102](#bib.bib102)]
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 应用特征工程和迁移学习 | [[102](#bib.bib102)] |'
- en: '| Mitigating the clipping bias (to improve convergence) | [[34](#bib.bib34)]
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 减轻剪辑偏差（以改善收敛性） | [[34](#bib.bib34)] |'
- en: '| Pruning the model | [[51](#bib.bib51), [15](#bib.bib15), [92](#bib.bib92)]
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝模型 | [[51](#bib.bib51), [15](#bib.bib15), [92](#bib.bib92)] |'
- en: '| Adding heterogeneous noise | [[124](#bib.bib124), [112](#bib.bib112), [116](#bib.bib116),
    [16](#bib.bib16), [52](#bib.bib52)] |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 添加异质噪声 | [[124](#bib.bib124), [112](#bib.bib112), [116](#bib.bib116), [16](#bib.bib16),
    [52](#bib.bib52)] |'
- en: 'One approach to increase the accuracy of DP-SGD at the same privacy is to adapt
    the architecture of the deep learning model to better suit differentially private
    learning. Papernot et al. [[86](#bib.bib86)] observed that rendering SGD differentially
    private as proposed by Abadi et al. [[11](#bib.bib11)] leads to exploding gradients.
    The larger the gradients, the more information is lost during clipping, which
    in turn hurts the model’s accuracy. To mitigate this effect, Papernot et al. [[86](#bib.bib86)]
    proposed to use bounded activation functions instead of the unbounded ones commonly
    used in non-private training (e.g., ReLU). They introduced tempered sigmoid activation
    functions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 提高 DP-SGD 准确性的一个方法是在相同隐私条件下调整深度学习模型的架构，以更好地适应差分隐私学习。Papernot 等人 [[86](#bib.bib86)]
    观察到，正如 Abadi 等人 [[11](#bib.bib11)] 所提出的那样，使 SGD 满足差分隐私会导致梯度爆炸。梯度越大，裁剪过程中丢失的信息就越多，这反过来会损害模型的准确性。为了减轻这种影响，Papernot
    等人 [[86](#bib.bib86)] 提议使用有界激活函数，而不是在非私有训练中常用的无界函数（例如 ReLU）。他们引入了温和 sigmoid 激活函数：
- en: '|  | $\Phi(x)=\frac{s}{1+e^{-Tx}}-o$ |  | (4) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi(x)=\frac{s}{1+e^{-Tx}}-o$ |  | (4) |'
- en: 'where the parameter $s$ controls the scale of the activation, the inverse temperature
    $T$ regulates the gradient norms and $o$ is the offset (see Figure [2](#S5.F2
    "Figure 2 ‣ 5 Improving the privacy-utility trade-off of DP-DL ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")).
    The setting [$s=2$, $T=2$, $o=1$] results in the hyperbolic tangent (tanh) function.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，参数 $s$ 控制激活的尺度，逆温度 $T$ 调节梯度范数，$o$ 是偏移量（见图 [2](#S5.F2 "Figure 2 ‣ 5 Improving
    the privacy-utility trade-off of DP-DL ‣ Recent Advances of Differential Privacy
    in Centralized Deep Learning: A Systematic Survey")）。设置 [$s=2$，$T=2$，$o=1$] 结果为双曲正切（tanh）函数。'
- en: '![Refer to caption](img/9a263468de7b73e99cacdf619c079cd4.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9a263468de7b73e99cacdf619c079cd4.png)'
- en: 'Figure 2: Examples of tempered sigmoid functions in comparison with the ReLU
    function. Tempered sigmoid functions with their parameters $s$, $T$ and $o$ are
    bounded activation functions proposed by Papernot et al. [[86](#bib.bib86)] to
    improve private deep learning. ReLU is an unbounded activation function commonly
    used in deep learning.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：与 ReLU 函数比较的温和 sigmoid 函数示例。温和 sigmoid 函数及其参数 $s$、$T$ 和 $o$ 是 Papernot 等人提出的有界激活函数
    [[86](#bib.bib86)]，旨在改进私密深度学习。ReLU 是一种在深度学习中常用的无界激活函数。
- en: Papernot et al. [[86](#bib.bib86)] showed that tempered sigmoids can increase
    the model’s accuracy. For the MNIST and Fashion-MNIST datasets the tanh function
    performed best.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Papernot 等人 [[86](#bib.bib86)] 显示温和 sigmoid 函数可以提高模型的准确性。对于 MNIST 和 Fashion-MNIST
    数据集，tanh 函数表现最佳。
- en: Another important aspect when tuning deep learning models for improved performance
    is hyperparameter selection (e.g., choosing the learning rate). Even though it
    might be tempting to transfer the choice of hyperparameters from non-private to
    private model, Papernot et al. [[86](#bib.bib86)] showed that not only the model’s
    architecture but also the hyperparameters should be chosen explicitly for the
    private model in contrast to using what worked well in the non-private setting.
    As this can result in additional privacy leakage, one should consider private
    selection of hyperparameters, as, for example, proposed by Liu et al. [[71](#bib.bib71)].
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 调整深度学习模型以提高性能时，另一个重要方面是超参数选择（例如，选择学习率）。尽管从非私有模型转移超参数的选择可能很诱人，但 Papernot 等人 [[86](#bib.bib86)]
    表明，除了模型架构外，超参数也应为私有模型明确选择，而不是使用在非私有设置中效果良好的超参数。由于这可能导致额外的隐私泄漏，因此应考虑私有超参数选择，例如
    Liu 等人 [[71](#bib.bib71)] 所提出的方案。
- en: Similar to non-private deep learning, DP-SGD can benefit from feature engineering,
    additional data, and transfer learning. Tramèr and Boneh [[102](#bib.bib102)]
    showed that handcrafted features can significantly improve the private model’s
    utility compared to end-to-end learning. The comparable increase in accuracy for
    private end-to-end learning can be achieved by using an order of magnitude more
    training data or by transferring features learned from public data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与非私有深度学习类似，DP-SGD 可以从特征工程、额外数据和迁移学习中受益。Tramèr 和 Boneh [[102](#bib.bib102)] 表明，与端到端学习相比，手工制作的特征可以显著提高私有模型的效用。通过使用数量级更多的训练数据或迁移从公开数据中学习到的特征，可以实现与私有端到端学习相当的准确性提升。
- en: 'While the preceding techniques are already known from non-private deep learning,
    DP-SGD introduces two new steps that offer opportunities for improvement: gradient
    clipping and noise addition.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前述技术在非私有深度学习中已经很成熟，但DP-SGD引入了两个新的步骤，提供了改进的机会：梯度裁剪和噪声添加。
- en: 'Chen et al. [[34](#bib.bib34)] found that the bias introduced by gradient clipping
    can cause convergence issues. They discovered a relationship between the symmetry
    of the gradient distribution and convergence: Symmetric gradient distributions
    lead to convergence even if a large fraction of gradients are heavily scaled down.
    Based on this finding, Chen et al. [[34](#bib.bib34)] proposed to introduce additional
    noise before clipping when the gradient distribution is non-symmetric. It is important
    to note that this approach may lead to better but slower convergence due to the
    additional variance, and therefore only improves the privacy-utility trade-off
    in specific use-cases.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人 [[34](#bib.bib34)] 发现梯度裁剪引入的偏差可能导致收敛问题。他们发现了梯度分布对称性与收敛之间的关系：即使大量梯度被大幅度缩放，对称的梯度分布也能导致收敛。基于这一发现，Chen
    等人 [[34](#bib.bib34)] 提出了在梯度分布不对称时，在裁剪前引入额外的噪声。需要注意的是，由于额外的方差，这种方法可能导致更好但更慢的收敛，因此仅在特定的使用场景中改善隐私-效用权衡。
- en: Another observation specific to DP-SGD is that the privacy-utility trade-off
    worsens with growing model size. A higher number of model parameters results in
    a higher gradient norm, meaning that clipping the gradient to the same norm leads
    to a higher impact. If the clipping norm is also increased, then more noise has
    to be added to achieve the same privacy guarantee. This effect can be mitigated
    by either reducing the number of model parameters (parameter pruning) [[51](#bib.bib51),
    [15](#bib.bib15)] or compressing the gradients (gradient pruning) [[15](#bib.bib15),
    [92](#bib.bib92)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DP-SGD的另一个观察是，随着模型规模的增大，隐私-效用权衡会恶化。更多的模型参数会导致更高的梯度范数，这意味着将梯度裁剪到相同的范数会带来更大的影响。如果裁剪范数也增加，那么为了实现相同的隐私保证，就需要添加更多的噪声。这个效应可以通过减少模型参数的数量（参数剪枝）[[51](#bib.bib51)，[15](#bib.bib15)]
    或压缩梯度（梯度剪枝）[[15](#bib.bib15)，[92](#bib.bib92)]来缓解。
- en: The work by Gondara et al. [[51](#bib.bib51)] is based on the lottery ticket
    hypothesis [[47](#bib.bib47), [48](#bib.bib48)], which says that there exist sub-networks
    in large neural networks that when trained separately achieve comparable accuracy
    as the full network. The term "lottery ticket" refers to the pruned networks and
    comes from the idea that finding a well-performing sub-network is like winning
    the lottery. Gondara et al. [[51](#bib.bib51)] altered the original lottery ticket
    hypothesis to comply with differential privacy. First, the lottery tickets are
    created non-privately using a public dataset. Next, the accuracy of each lottery
    ticket is evaluated on a private validation set, and the best sub-network is selected
    while preserving DP via the Exponential Mechanism. Finally, the winning ticket
    is trained using DP-SGD.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Gondara 等人 [[51](#bib.bib51)] 的工作基于彩票票假设 [[47](#bib.bib47)，[48](#bib.bib48)]，该假设认为在大型神经网络中存在一些子网络，这些子网络在单独训练时可以达到与完整网络相当的准确性。术语“彩票票”指的是剪枝后的网络，源于找到表现良好的子网络就像赢得彩票一样。Gondara
    等人 [[51](#bib.bib51)] 修改了原始的彩票票假设以符合差分隐私。首先，彩票票是在公共数据集上非私有地创建的。接下来，在私有验证集上评估每个彩票票的准确性，并在通过指数机制保留DP的同时选择最佳子网络。最后，使用DP-SGD训练中奖的票据。
- en: Adamczewski and Park [[15](#bib.bib15)] proposed DP-SSGD (differentially private
    sparse stochastic gradient descent) that too relies on model pruning. They experimented
    with both parameter freezing, where just a subset of parameters are trained, and
    parameter selection, where a different subset of parameters are updated each iteration.
    The updated parameters were either chosen randomly or based on their magnitude.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Adamczewski 和 Park [[15](#bib.bib15)] 提出了DP-SSGD（差分隐私稀疏随机梯度下降），也依赖于模型剪枝。他们实验了参数冻结，即仅训练部分参数，以及参数选择，即每次迭代更新不同的参数子集。更新的参数要么是随机选择的，要么是基于其大小选择的。
- en: Both of these model pruning approaches rely on publicly available data that
    should be as similar as possible to the private data. In contrast, Phong and Phuong
    [[92](#bib.bib92)] proposed a gradient pruning method that works without public
    data. Additionally to making the gradients sparse, they use memorization to maintain
    the direction of the gradient descent.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种模型剪枝方法都依赖于应尽可能与私有数据相似的公开数据。相比之下，Phong 和 Phuong [[92](#bib.bib92)] 提出了无需公共数据的梯度剪枝方法。除了使梯度稀疏外，他们还使用记忆来保持梯度下降的方向。
- en: A further line of research deals with adapting the noise that is added to the
    differentially private model during training. While the original DP-SGD algorithm
    adds the same amount of noise to each gradient coordinate independent of the training
    progress, this line of work adds noise either based on the learning progress (e.g.,
    number of executed epochs) [[124](#bib.bib124)] or based on the coordinates’ impact
    on the model [[112](#bib.bib112), [116](#bib.bib116), [16](#bib.bib16), [52](#bib.bib52)].
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项研究方向涉及在训练过程中调整添加到不同隐私模型的噪声。虽然原始的 DP-SGD 算法对每个梯度坐标添加相同量的噪声，不考虑训练进展，但这一研究方向基于学习进展（例如，执行的轮次）[[124](#bib.bib124)]
    或基于坐标对模型的影响 [[112](#bib.bib112), [116](#bib.bib116), [16](#bib.bib16), [52](#bib.bib52)]
    来添加噪声。
- en: Yu et al. [[124](#bib.bib124)] argues that with training progress and therefore
    convergence to the local optimum the model profits more from smaller noise. This
    "dynamic privacy budget allocation" is similar to the idea behind adaptive learning
    rates, which is a common technique in non-private learning and can also be applied
    in private learning (see for example [[116](#bib.bib116), [115](#bib.bib115)]).
    Yu et al. compared different variants of dynamic schemes to allocate privacy budgets,
    including predefined decay schedules like exponential decay, or noise scaling
    based on the validation accuracy on a public dataset. They showed that all dynamic
    schemes outperform the uniform noise allocation to a similar extent.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Yu 等人 [[124](#bib.bib124)] 认为，随着训练进展并因此收敛到局部最优，模型从较小噪声中获益更多。这种“动态隐私预算分配”类似于自适应学习率的理念，这在非隐私学习中是常见技术，也可以应用于隐私学习（例如，参见
    [[116](#bib.bib116), [115](#bib.bib115)]）。Yu 等人比较了不同的动态方案用于分配隐私预算，包括预定义的衰减计划，如指数衰减，或基于公共数据集上的验证准确率的噪声缩放。他们展示了所有动态方案在很大程度上优于均匀噪声分配。
- en: Xiang et al. [[112](#bib.bib112)] treated the privacy-utility trade-off as an
    optimization problem, namely minimizing the accuracy loss while satisfying the
    privacy constraints. Consequently, less noise is added to those gradient coordinates
    that have a high impact on the model’s output. While the model’s utility was improved
    for a range of privacy budgets and model architectures, the method is computationally
    expensive due to the high dimensionality of the optimization problem.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Xiang 等人 [[112](#bib.bib112)] 将隐私-效用权衡视为优化问题，即在满足隐私约束的同时最小化准确性损失。因此，添加的噪声较少影响模型输出的梯度坐标。尽管在一系列隐私预算和模型架构下，模型的效用有所改善，但由于优化问题的高维度，该方法计算开销较大。
- en: Xu et al. [[116](#bib.bib116)] advanced the approach further not only reducing
    the computational demand but also improving convergence (and therefore decreasing
    the privacy budget). The improved version called AdaDP (adaptive and fast convergent
    approach to differentially private deep learning) replaces the computationally
    expensive optimization with a heuristic approach to compute the gradient coordinates’
    impact on the model’s output. The added noise is not only adaptive with regards
    to the coordinates’ sensitivity but also decreases with the number of training
    iterations. Faster convergence is achieved by incorporating an adaptive learning
    rate that is larger for less frequently updated coordinates.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人 [[116](#bib.bib116)] 进一步改进了这种方法，不仅减少了计算需求，还提高了收敛速度（从而减少了隐私预算）。改进版称为 AdaDP（自适应和快速收敛的不同隐私深度学习方法），用启发式方法替代了计算开销大的优化，以计算梯度坐标对模型输出的影响。添加的噪声不仅根据坐标的敏感性自适应调整，还随着训练迭代次数的增加而减少。通过引入自适应学习率（对更新不频繁的坐标较大），实现了更快的收敛。
- en: A related research direction is the usage of explainable AI methods to calibrate
    the noise. Both Gong et al. [[52](#bib.bib52)] and Adesuyi et al. [[16](#bib.bib16)]
    use layer-wise relevance propagation (LRP) [[78](#bib.bib78)] to determine the
    importance of the different parameters. Gong et al. [[52](#bib.bib52)] proposed
    the ADPPL (adaptive differential privacy preserving learning) framework that adds
    adaptive Laplace noise [[91](#bib.bib91)] to the gradient coordinates according
    to their relevance. In contrast, the approach by Adesuyi et al. [[16](#bib.bib16)]
    is based on loss function perturbation. As deep learning models have non-convex
    loss functions, the polynomial approximation of the loss function is computed
    before adding Laplace noise. LRP is used to classify the parameters either as
    high or low relevance, adding small and large noise accordingly.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的研究方向是使用可解释的 AI 方法来校准噪声。Gong 等人 [[52](#bib.bib52)] 和 Adesuyi 等人 [[16](#bib.bib16)]
    都使用逐层相关传播（LRP） [[78](#bib.bib78)] 来确定不同参数的重要性。Gong 等人 [[52](#bib.bib52)] 提出了 ADPPL（自适应差分隐私保护学习）框架，该框架根据参数的相关性向梯度坐标添加自适应拉普拉斯噪声
    [[91](#bib.bib91)]。相比之下，Adesuyi 等人 [[16](#bib.bib16)] 的方法基于损失函数扰动。由于深度学习模型具有非凸损失函数，因此在添加拉普拉斯噪声之前计算了损失函数的多项式近似。LRP
    被用来将参数分类为高相关性或低相关性，从而分别添加小噪声和大噪声。
- en: Explainable AI-based approaches can also be applied in the local DP setting,
    for example Wang et al. [[109](#bib.bib109)] uses feature importance to decide
    how much noise to add to the training data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基于可解释 AI 的方法也可以应用于局部 DP 设置，例如 Wang 等人 [[109](#bib.bib109)] 使用特征重要性来决定向训练数据添加多少噪声。
- en: 'A list of examples for results that the discussed works reported can be found
    in Table [5](#S5.T5 "Table 5 ‣ 5 Improving the privacy-utility trade-off of DP-DL
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey"). For comparison, the results for the original DP-SGD by Abadi et al.
    [[11](#bib.bib11)] were included as well. The different accuracies for the three
    different reported privacy levels from the original DP-SGD paper clearly show
    the privacy-utility trade-off typical for differentially private algorithms. Direct
    comparison between the methods is difficult due to different network architectures
    and hyperparameters, the different evaluation datasets, and the different privacy
    levels measured according to different DP notions (e.g., $\epsilon$-DP, $(\epsilon,\delta)$-DP,
    $\rho$-zCDP (zero-concentrated DP)).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的工作的结果示例列表可以在表 [5](#S5.T5 "表 5 ‣ 5 改善 DP-DL 的隐私-效用权衡 ‣ 集中深度学习中差分隐私的最新进展：系统综述")
    中找到。为了比较，还包括了 Abadi 等人 [[11](#bib.bib11)] 提出的原始 DP-SGD 的结果。原始 DP-SGD 论文中报告的三个不同隐私水平的不同准确性清楚地展示了差分隐私算法的隐私-效用权衡。由于网络架构和超参数的不同、评估数据集的不同以及根据不同
    DP 概念（例如，$\epsilon$-DP、$(\epsilon,\delta)$-DP、$\rho$-zCDP（零集中 DP））测量的不同隐私水平，方法之间的直接比较是困难的。
- en: 'Table 5: Selected examples of results for the improved DP methods in comparison
    with the original DP-SGD by Abadi et al. [[11](#bib.bib11)]. The used deep learning
    models include fully-connected neural networks (FCNN) and convolutional neural
    networks (CNN). Some include a principal component analysis (PCA) layer in front.
    We reported the model, evaluation dataset, accuracy and privacy budget for all
    methods that state exact values, preferably on the MNIST dataset.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：改进的 DP 方法与 Abadi 等人 [[11](#bib.bib11)] 提出的原始 DP-SGD 的比较结果的选定示例。所用的深度学习模型包括全连接神经网络（FCNN）和卷积神经网络（CNN）。其中一些在前面包含了主成分分析（PCA）层。我们报告了所有说明确切值的方法的模型、评估数据集、准确性和隐私预算，优选
    MNIST 数据集。
- en: '*This result was achieved by parameter freezing. Parameter selection performed
    slightly worse.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个结果是通过参数冻结实现的。参数选择的效果稍微差一些。'
- en: '**This result was achieved by a polynomial decay schedule. Other budget allocation
    schemes performed comparably.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个结果是通过多项式衰减计划实现的。其他预算分配方案表现相当。'
- en: '***$\rho$ is the privacy parameter for zero-concentrated DP.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '***$\rho$ 是零集中差分隐私（zero-concentrated DP）的隐私参数。'
- en: 'Method Model Dataset Accuracy Privacy Original [[11](#bib.bib11)] PCA + FCNN
    (1 hidden layer) MNIST 90% $\epsilon=0.5,\delta=10^{-5}$ Original [[11](#bib.bib11)]
    PCA + FCNN (1 hidden layer) MNIST 95% $\epsilon=2,\delta=10^{-5}$ Original [[11](#bib.bib11)]
    PCA + FCNN (1 hidden layer) MNIST 97% $\epsilon=8,\delta=10^{-5}$ Tanh [[86](#bib.bib86)]
    CNN (2 convolutional layers) MNIST $98.1\%$ $\epsilon=2.93,\delta=10^{-5}$ DPLTH
    [[51](#bib.bib51)] FCNN (3 hidden layers) public: MNIST; private: Fashion-MNIST
    $76\%$ $\epsilon=0.4$ DP-SSGD [[15](#bib.bib15)] CNN (2 convolutional layers)
    MNIST $97.02\%$* $\epsilon=2$ Gradient compression [[92](#bib.bib92)] CNN (2 convolutional
    layers) MNIST $98.52\%$ $\epsilon=1.71,\delta=10^{-5}$ Dynamic privacy budget
    allocation** [[124](#bib.bib124)] PCA + FCNN (1 hidden layer) MNIST $93.2\%$**
    $\rho=0.78$*** Adaptive noise [[112](#bib.bib112)] CNN (2 convolutional layers)
    MNIST $94.69\%$ $\epsilon=1,\delta=10^{-5}$ AdaDP [[116](#bib.bib116)] PCA + FCNN
    (1 hidden layer) MNIST $96\%$ $\epsilon=1.4,\delta=10^{-4}$ Noise acc. to LRP
    [[16](#bib.bib16)] FCNN (3 hidden layers) Winsconsin Diagnosis Breast Cancer (WDBC)
    $94\%$ $\epsilon=1.1$ ADPPL framework [[52](#bib.bib52)] CNN (2 convolutional
    layers) MNIST $94\%$ $\epsilon=1$'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 模型 数据集 准确率 隐私 原始 [[11](#bib.bib11)] PCA + FCNN（1隐藏层） MNIST 90% $\epsilon=0.5,\delta=10^{-5}$
    原始 [[11](#bib.bib11)] PCA + FCNN（1隐藏层） MNIST 95% $\epsilon=2,\delta=10^{-5}$ 原始
    [[11](#bib.bib11)] PCA + FCNN（1隐藏层） MNIST 97% $\epsilon=8,\delta=10^{-5}$ Tanh
    [[86](#bib.bib86)] CNN（2卷积层） MNIST $98.1\%$ $\epsilon=2.93,\delta=10^{-5}$ DPLTH
    [[51](#bib.bib51)] FCNN（3隐藏层） 公开：MNIST；私有：Fashion-MNIST $76\%$ $\epsilon=0.4$
    DP-SSGD [[15](#bib.bib15)] CNN（2卷积层） MNIST $97.02\%$* $\epsilon=2$ 梯度压缩 [[92](#bib.bib92)]
    CNN（2卷积层） MNIST $98.52\%$ $\epsilon=1.71,\delta=10^{-5}$ 动态隐私预算分配** [[124](#bib.bib124)]
    PCA + FCNN（1隐藏层） MNIST $93.2\%$** $\rho=0.78$*** 自适应噪声 [[112](#bib.bib112)] CNN（2卷积层）
    MNIST $94.69\%$ $\epsilon=1,\delta=10^{-5}$ AdaDP [[116](#bib.bib116)] PCA + FCNN（1隐藏层）
    MNIST $96\%$ $\epsilon=1.4,\delta=10^{-4}$ 根据LRP的噪声 [[16](#bib.bib16)] FCNN（3隐藏层）
    威斯康星诊断乳腺癌（WDBC） $94\%$ $\epsilon=1.1$ ADPPL框架 [[52](#bib.bib52)] CNN（2卷积层） MNIST
    $94\%$ $\epsilon=1$
- en: '6 Beyond membership and attribute inference: Applying DP-DL to protect against
    other threats'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 超越成员身份和属性推断：应用DP-DL防护其他威胁
- en: 'Differential privacy is usually applied to protect against re-identification.
    In the context of deep learning this mainly includes membership inference and
    model inversion attacks. This section reviews cases in which differential privacy
    can protect against other threats deep learning models exhibit. Table [6](#S6.T6
    "Table 6 ‣ 6 Beyond membership and attribute inference: Applying DP-DL to protect
    against other threats ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey") lists the discussed threats and literature.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私通常用于保护防止重新识别。在深度学习的背景下，这主要包括成员身份推断和模型逆转攻击。本节回顾了差分隐私如何保护深度学习模型对其他威胁的防护。表[6](#S6.T6
    "表6 ‣ 超越成员身份和属性推断：应用DP-DL防护其他威胁 ‣ 差分隐私在集中式深度学习中的最新进展：系统性综述")列出了讨论的威胁和文献。
- en: 'Table 6: Summary of threats other than membership and attribute inference against
    which DP-DL was applied.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：DP-DL应用于防范除成员身份和属性推断之外的威胁的总结。
- en: '*These two approaches are both based on threat identification via anomaly detection.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*这两种方法均基于通过异常检测进行威胁识别。'
- en: '| Threat | Paper |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 威胁 | 论文 |'
- en: '| --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Model extraction attacks | [[129](#bib.bib129), [130](#bib.bib130), [118](#bib.bib118)]
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 模型提取攻击 | [[129](#bib.bib129), [130](#bib.bib130), [118](#bib.bib118)] |'
- en: '| Adversarial attacks | [[65](#bib.bib65), [90](#bib.bib90)] |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 对抗攻击 | [[65](#bib.bib65), [90](#bib.bib90)] |'
- en: '| Privacy risk of interpretable DL | [[57](#bib.bib57)] |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 可解释深度学习的隐私风险 | [[57](#bib.bib57)] |'
- en: '| Privacy risk of machine unlearning | [[32](#bib.bib32)] |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 机器去学习的隐私风险 | [[32](#bib.bib32)] |'
- en: '| Backdoor poisoning attacks* | [[41](#bib.bib41)] |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 后门中毒攻击* | [[41](#bib.bib41)] |'
- en: '| Network intrusion* | [[119](#bib.bib119)] |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 网络入侵* | [[119](#bib.bib119)] |'
- en: One threat against which differential privacy can be applied even though it
    was not originally intended for that purpose are model extraction attacks. Model
    extraction is primarily a security and confidentiality issue but it can also compromise
    privacy as it facilitates membership and attribute inference.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私可以应用于的一个威胁是模型提取攻击，尽管最初并不是为了这个目的。模型提取主要是安全性和保密性问题，但它也可能危害隐私，因为它促进了成员身份和属性推断。
- en: Zheng et al. [[129](#bib.bib129), [130](#bib.bib130)] observed that most model
    extraction attacks infer the decision boundary of the target model via nearby
    inputs. They introduced the notion of boundary differential privacy ($\epsilon$-BDP)
    and proposed to append a BDP layer to the machine learning model that 1) determines
    which outputs are close to the decision boundary (a question not straightforward
    for deep learning models as the decision boundary has no closed form) and 2) adds
    noise to them. Previous input-output pairs are cached to ensure that the same
    input results in the same noisy output. This method guarantees that the attacker
    cannot learn the decision boundary with more than a predetermined level of precision
    (controlled by $\epsilon$).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Zheng 等人 [[129](#bib.bib129), [130](#bib.bib130)] 观察到，大多数模型提取攻击通过邻近输入推断目标模型的决策边界。他们引入了边界差分隐私
    ($\epsilon$-BDP) 的概念，并建议在机器学习模型中附加一个 BDP 层，该层 1) 确定哪些输出接近决策边界（这是一个对深度学习模型而言并不简单的问题，因为决策边界没有封闭形式）并且
    2) 对这些输出添加噪声。缓存先前的输入输出对，以确保相同输入产生相同的噪声输出。这种方法保证了攻击者无法以超过预定精度水平（由 $\epsilon$ 控制）学习决策边界。
- en: 'A subsequent study by Yan et al. [[118](#bib.bib118)] showed that without caching
    the BDP layer cannot protect against their novel model extraction attack. They
    proposed an alternative to caching: monitoring the privacy leakage and adapting
    the privacy budget accordingly.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Yan 等人 [[118](#bib.bib118)] 的后续研究表明，如果没有缓存，BDP 层无法保护免受他们新型模型提取攻击的影响。他们提出了一种替代缓存的方法：监测隐私泄露并相应调整隐私预算。
- en: 'Differential privacy can also be adapted to protect deep learning models against
    adversarial attacks. While originally DP is defined on a record level, feature-level
    DP can achieve robustness against adversarial examples. For example, Lecuyer et
    al. [[65](#bib.bib65)] proposed PixelDP, a pixel-level DP layer that can be added
    to any type of deep learning model. As this approach only guarantees robustness
    but not differential privacy in the original sense, Phan et al. [[90](#bib.bib90)]
    developed the method further by combining it with DP-SGD. To deal with the trade-off
    between robustness, privacy and utility, they relied on heterogeneous noise: More
    noise is added to more vulnerable coordinates.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私也可以适应于保护深度学习模型免受对抗性攻击。虽然原始的差分隐私是在记录级别上定义的，但特征级差分隐私可以在对抗样本中实现鲁棒性。例如，Lecuyer
    等人 [[65](#bib.bib65)] 提出了 PixelDP，这是一种可以添加到任何类型深度学习模型中的像素级差分隐私层。由于这种方法只保证鲁棒性而非原始意义上的差分隐私，Phan
    等人 [[90](#bib.bib90)] 通过将其与 DP-SGD 结合进一步发展了该方法。为了应对鲁棒性、隐私和效用之间的权衡，他们依赖于异质噪声：对更易受攻击的坐标添加更多噪声。
- en: Another line of research is the relationship between interpretability/explainability
    and privacy. Even though interpretable/explainable AI methods are not a threat
    scenario on their own, they can facilitate privacy attacks. Harder et al. [[57](#bib.bib57)]
    looked at how models can both be interpretable and guarantee differential privacy.
    Models trained with DP-SGD are not vulnerable to gradient-based interpretable
    AI methods due to the post-processing property of DP. However, gradient-based
    methods can only provide local explanations, i.e., about how relevant a specific
    input is for the model’s decision, but Harder et al. [[57](#bib.bib57)] was specifically
    interested in methods that can also give global explanations, i.e., about how
    the model works overall. They introduced differentially private locally linear
    maps (DP-LLM), which can approximate deep learning models and are inherently interpretable.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类研究是可解释性/解释性与隐私之间的关系。即使可解释的/解释的 AI 方法本身不是威胁情景，但它们可以促进隐私攻击。Harder 等人 [[57](#bib.bib57)]
    研究了模型如何既可解释又能保证差分隐私。由于差分隐私的后处理特性，使用 DP-SGD 训练的模型不会受到基于梯度的可解释 AI 方法的影响。然而，基于梯度的方法只能提供局部解释，即关于特定输入对模型决策的相关性，而
    Harder 等人 [[57](#bib.bib57)] 特别关注那些也能提供全局解释的方法，即关于模型整体运作的解释。他们介绍了差分隐私局部线性映射 (DP-LLM)，这可以近似深度学习模型，并且本质上是可解释的。
- en: Chen et al. [[32](#bib.bib32)] investigated the privacy risk of machine unlearning.
    Machine unlearning [[26](#bib.bib26), [107](#bib.bib107)] is the process of removing
    the impact one or more datapoints have on a trained model. Common methods are
    retraining from scratch and SISA (Sharded, Isolated, Sliced, and Aggregated) [[24](#bib.bib24)],
    where the original model consists of $k$ models each trained on a subset of the
    training set and therefore only one submodel is retrained for unlearning. While
    the main idea is to be able to comply with privacy regulations like the right
    to be forgotten in the European General Data Protection Regulation (GDPR), the
    unlearning can disclose additional information about the removed datapoint(s).
    Chen et al. [[32](#bib.bib32)] proposed a new black-box membership inference attack
    that exploits both the original and the unlearned model. Their attack was more
    powerful than classical membership inference attacks, and also worked for well-generalized
    models, when several datapoints were removed, when the attacker missed several
    intermediate unlearned models, and when the model was updated with new inputs.
    They showed that DP-SGD is an effective defense against the privacy risk of machine
    unlearning.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[32](#bib.bib32)]研究了机器“遗忘”的隐私风险。机器“遗忘”[[26](#bib.bib26), [107](#bib.bib107)]是指去除一个或多个数据点对已训练模型的影响的过程。常见的方法包括从头开始重新训练和SISA（分片、隔离、切片和聚合）[[24](#bib.bib24)]，其中原始模型由$k$个模型组成，每个模型在训练集的一个子集上训练，因此只对一个子模型进行重新训练以实现遗忘。尽管主要目的是能够遵守隐私法规，如欧洲通用数据保护条例（GDPR）中的被遗忘权，但遗忘过程可能会泄露关于已删除数据点的附加信息。陈等人[[32](#bib.bib32)]提出了一种新的黑箱成员推断攻击，利用了原始模型和被遗忘模型。他们的攻击比经典的成员推断攻击更强大，而且对于泛化良好的模型也有效，包括当多个数据点被删除、攻击者错过了几个中间的被遗忘模型以及模型随着新输入的更新时。他们展示了DP-SGD是应对机器“遗忘”隐私风险的有效防御手段。
- en: 'Instead of protecting the model directly, differential privacy can also be
    used to improve the detection of attacks [[41](#bib.bib41), [119](#bib.bib119)].
    This approach can be viewed as a kind of anomaly detection, where the attack scenario
    is the outlier/novelty. Anomaly detection with deep learning models (e.g., autoencoders,
    CNNs) is based on the model’s tendency to underfit on underrepresented subgroups,
    i.e., the model’s error is expected to be higher for atypical inputs. Training
    the model with differential privacy amplifies this effect (see Section [4.2](#S4.SS2
    "4.2 Evaluation of subgroups: bias and fairness ‣ 4 Auditing and Evaluation ‣
    Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey")). While this leads to negative consequences in the context of fairness
    and bias, here it can be used to improve the performance of anomaly detection.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私不仅可以直接保护模型，还可以用于提高攻击检测的能力[[41](#bib.bib41), [119](#bib.bib119)]。这种方法可以视为一种异常检测，其中攻击场景被视为异常/新颖。使用深度学习模型（例如，自编码器，卷积神经网络）的异常检测基于模型对低代表性子组的拟合不足，即对于不典型输入，模型的错误预计会更高。使用差分隐私训练模型会放大这种效应（见[4.2节](#S4.SS2
    "4.2 评估子组：偏见与公平 ‣ 4 审计与评估 ‣ 差分隐私在集中式深度学习中的近期进展：系统性调查")）。虽然这在公平性和偏见的背景下会带来负面影响，但在这里可以用来提高异常检测的性能。
- en: Du et al. [[41](#bib.bib41)] applied this approach on crowdsourcing data, that
    is, data stemming from many individuals. These individuals could launch a backdoor
    poisoning attack by maliciously adapting their contributed samples. To protect
    the target model, the poisoned samples need to be identified and removed from
    the training set. They showed that differential privacy can improve the performance
    of anomaly detection in this context. Based on the same reasoning, Yang et al.
    [[119](#bib.bib119)] proposed Griffin, a network intrusion detection system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 杜等人[[41](#bib.bib41)]将这种方法应用于众包数据，即来自许多个体的数据。这些个体可以通过恶意调整他们贡献的样本来发起后门中毒攻击。为了保护目标模型，需要识别并从训练集中移除这些中毒样本。他们展示了差分隐私可以在这种情况下提高异常检测的性能。基于相同的理由，杨等人[[119](#bib.bib119)]提出了Griffin，一个网络入侵检测系统。
- en: 7 Differentially private generative models
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 差分隐私生成模型
- en: 'Generative models are a class of machine learning models that aim to generate
    new data samples similar to the data samples from the training set. The synthetic
    data created with generative models are often seen as privacy-preserving as they
    are not directly linked to real entities or individuals. However, similarly to
    other machine learning models, generative models can memorize sensitive information
    and be vulnerable to privacy attacks. This section gives an overview of recent
    works regarding the generation of differentially private synthetic data with generative
    models (see Table [7](#S7.T7 "Table 7 ‣ 7 Differentially private generative models
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey")).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '生成模型是一类机器学习模型，旨在生成类似于训练集数据样本的新数据样本。用生成模型创建的合成数据通常被视为隐私保护，因为它们不会直接与真实实体或个人相关联。然而，与其他机器学习模型类似，生成模型也可能记住敏感信息，并易受隐私攻击。该部分概述了有关使用生成模型生成差分隐私合成数据的最新工作（见表
    [7](#S7.T7 "Table 7 ‣ 7 Differentially private generative models ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")）。'
- en: 'Table 7: Summary of differentially private generative models. The methods are
    either based on (variational) autoencoders or Generative Adversarial Networks
    (GAN). The DP algorithm DP-EM refers to differentially private expectation maximization
    [[88](#bib.bib88)].'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：差分隐私生成模型的总结。这些方法基于（变分）自编码器或生成对抗网络（GAN）。DP 算法 DP-EM 指的是差分隐私期望最大化 [[88](#bib.bib88)]。
- en: '| Method | Type of generative model(s) | DP algorithm |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 生成模型类型 | DP 算法 |'
- en: '| --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| DPGM [[14](#bib.bib14)] | variational autoencoders | DP k-means & DP-SGD
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DPGM [[14](#bib.bib14)] | 变分自编码器 | DP k-means & DP-SGD |'
- en: '| DP-SYN [[12](#bib.bib12)] | autoencoders | DP-SGD & DP-EM |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| DP-SYN [[12](#bib.bib12)] | 自编码器 | DP-SGD & DP-EM |'
- en: '| PPGAN [[72](#bib.bib72)] | GAN | DP-SGD |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| PPGAN [[72](#bib.bib72)] | GAN | DP-SGD |'
- en: '| GANobfuscator [[115](#bib.bib115)] | GAN | DP-SGD |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| GANobfuscator [[115](#bib.bib115)] | GAN | DP-SGD |'
- en: '| GS-WGAN [[30](#bib.bib30)] | GAN | DP-SGD |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| GS-WGAN [[30](#bib.bib30)] | GAN | DP-SGD |'
- en: '| RDP-CGAN [[101](#bib.bib101)] | GAN | DP-SGD |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| RDP-CGAN [[101](#bib.bib101)] | GAN | DP-SGD |'
- en: '| PATE-GAN [[61](#bib.bib61)] | GAN | PATE |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| PATE-GAN [[61](#bib.bib61)] | GAN | PATE |'
- en: 7.1 Autoencoders
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 自编码器
- en: 'An autoencoder is a deep learning model consisting of two parts: an encoder
    and a decoder. The encoder learns to map the input to a lower-dimensional space,
    while the decoder learns to reconstruct the input from this intermediate representation.
    Variational autoencoders (VAEs) work according to the same principle but learn
    the probability distribution of the intermediate (encoded) representations.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种深度学习模型，由两个部分组成：编码器和解码器。编码器学习将输入映射到较低维度的空间，而解码器学习从这个中间表示中重构输入。变分自编码器（VAEs）遵循相同的原则，但学习中间（编码）表示的概率分布。
- en: 'Acs et al. [[14](#bib.bib14)] proposed DPGM (Differentially Private Generative
    Model) based on a mixture of variational autoencoders (VAEs). First, the training
    data are privately clustered using differentially private version of k-means.
    Next, one VAE per cluster is trained with DP-SGD. The data distributions learned
    by the VAEs are used to generate synthetic data. Splitting the input data into
    clusters and learning separate models has two advantages: Firstly, the models
    learn faster as they are trained on similar datapoints so less noise is added
    and the models achieve higher accuracy. Secondly, unrealistic combinations of
    clusters are avoided.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Acs 等人 [[14](#bib.bib14)] 提出了基于变分自编码器（VAEs）混合的 DPGM（差分隐私生成模型）。首先，使用差分隐私版本的 k-means
    对训练数据进行私密聚类。接着，为每个簇训练一个 VAE，使用 DP-SGD。VAEs 学到的数据分布用于生成合成数据。将输入数据划分为簇并学习单独的模型具有两个优点：首先，模型在类似数据点上训练时学习更快，因此添加的噪声较少，模型准确度更高。其次，避免了不现实的簇组合。
- en: Abay et al. [[12](#bib.bib12)] used a similar approach also based on partitioning
    the training data. In contrast to DPGM [[14](#bib.bib14)], they assume a supervised
    setting and split the data according to their labels. Each class is then used
    to train a separate autoencoder via DP-SGD. Synthetic data is generated by sampling
    the intermediate representations with differentially private expectation maximization
    (DP-EM) [[88](#bib.bib88)], and applying the decoder on the new representations.
    This method (called DP-SYN) outperformed DPGM for most tested datasets.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Abay 等人 [[12](#bib.bib12)] 采用了一种类似的方法，也基于对训练数据的划分。与 DPGM [[14](#bib.bib14)]
    相比，他们假设了一个有监督的设置，并根据标签划分数据。然后，每个类别用于通过 DP-SGD 训练一个独立的自动编码器。合成数据是通过使用不同ially private
    expectation maximization (DP-EM) [[88](#bib.bib88)] 采样中间表示，并在新表示上应用解码器生成的。这种方法（称为
    DP-SYN）在大多数测试数据集上优于 DPGM。
- en: 7.2 GANs
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 GANs
- en: 'Generative Adversarial Networks (GANs) consists of two neural networks that
    play an adversarial game: The generator tries to create synthetic samples similar
    to the training data, while the discriminator attempts to distinguish between
    the artificially generated and the real samples. After training, the generator
    can be used to create synthetic data.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络 (GANs) 由两个神经网络组成，它们进行对抗游戏：生成器试图创建与训练数据相似的合成样本，而鉴别器则尝试区分人工生成的样本和真实样本。训练完成后，生成器可以用来创建合成数据。
- en: In the original GAN, the generator is trained based on the Jensen-Shannon distance
    as a distance measure between the two data distributions. This can lead to instability
    issues, in particular vanishing gradients, where the generator learns too slowly
    compared to the discriminator. To mitigate this problem, the Wasserstein GAN (WGAN)
    [[18](#bib.bib18)] was introduced, which relies on the Wasserstein distance instead
    of the Jensen-Shannon distance. For this variant, the weights are clipped after
    each update to make the Wasserstein distance applicable in this context. A differentially
    private version of WGAN (called DPGAN) was first introduced in 2018 by Xie et
    al. [[114](#bib.bib114)] exploiting the fact that WGAN already has bounded gradients.
    Therefore, differential privacy can be achieved by adding noise to the discriminator’s
    gradients without the need to clip them. As the generator has only access to information
    about the training data via the (now private) discriminator, it is not necessary
    to train the generator with DP-SGD. Like DPGAN, the following works [[72](#bib.bib72),
    [115](#bib.bib115), [30](#bib.bib30), [101](#bib.bib101)] are private variants
    of WGAN.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 GAN 中，生成器是基于 Jensen-Shannon 距离作为两个数据分布之间的距离度量进行训练的。这可能导致不稳定问题，特别是梯度消失，即生成器学习速度远慢于鉴别器。为了解决这个问题，引入了
    Wasserstein GAN (WGAN) [[18](#bib.bib18)]，它依赖于 Wasserstein 距离而不是 Jensen-Shannon
    距离。对于这种变体，权重在每次更新后被裁剪，以使 Wasserstein 距离在此上下文中适用。WGAN 的一个不同ially private 版本（称为
    DPGAN）首次由 Xie 等人 [[114](#bib.bib114)] 在 2018 年引入，利用了 WGAN 已经具有有限梯度的事实。因此，通过向鉴别器的梯度添加噪声而无需裁剪，就可以实现差分隐私。由于生成器仅能通过（现在是隐私的）鉴别器访问有关训练数据的信息，因此不需要使用
    DP-SGD 训练生成器。像 DPGAN 一样，以下工作 [[72](#bib.bib72), [115](#bib.bib115), [30](#bib.bib30),
    [101](#bib.bib101)] 是 WGAN 的私有变体。
- en: The PPGAN by Liu et al. [[72](#bib.bib72)] is similar to DPGAN but uses a different
    optimization algorithm (mini-batch SGD instead of RMSprop). Xu et al. [[115](#bib.bib115)]
    and Chen et al. [[30](#bib.bib30)] both proposed differentially private GANs based
    on the improved WGAN by Gulrajani et al. [[54](#bib.bib54)]. This version of WGAN
    adds a penalty on the gradient norm to the objective function instead of clipping
    the weights to improve the GANs stability. While the GANobfuscator by Xu et al.
    [[115](#bib.bib115)] trains the whole discriminator with DP-SGD, the GS-WGAN by
    Chen et al. [[30](#bib.bib30)] only sanitizes the gradients that propagate information
    from the discriminator back to the generator, resulting in a more selective gradient
    perturbation. Moreover, the GANobfuscator includes an adaptive clipping norm (based
    on average gradient norm on public data) and an adaptive learning rate (based
    on gradients’ magnitudes).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人提出的PPGAN [[72](#bib.bib72)] 类似于DPGAN，但使用了不同的优化算法（迷你批量SGD而非RMSprop）。徐等人 [[115](#bib.bib115)]
    和陈等人 [[30](#bib.bib30)] 都提出了基于Gulrajani等人 [[54](#bib.bib54)] 改进版WGAN的不同ially private
    GAN。这一版本的WGAN在目标函数中增加了对梯度范数的惩罚，而不是裁剪权重，以提高GAN的稳定性。虽然徐等人的GANobfuscator [[115](#bib.bib115)]
    训练了整个鉴别器使用DP-SGD，但陈等人的GS-WGAN [[30](#bib.bib30)] 仅对从鉴别器传播到生成器的信息的梯度进行清洗，从而导致更具选择性的梯度扰动。此外，GANobfuscator包括一个自适应裁剪范数（基于公共数据上的平均梯度范数）和一个自适应学习率（基于梯度的大小）。
- en: 'Torfi et al. [[101](#bib.bib101)] introduced RDP-CGAN (Rényi DP and Convolutional
    GAN) addressing two challenges that can make generating synthetic data difficult:
    1) mixed discrete-continuous data, and 2) correlated data (temporal correlations
    or correlated features). These properties are specially prevalent in health data.
    RDP-CGAN handles discrete data by adding an unsupervised feature learning step
    in the form of a convolutional autoencoder to the WGAN. This autoencoder is trained
    using DP-SGD to map the (discrete) input space to a continuous space. Correlations
    in the data are considered using one-dimensional convolutional layers.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Torfi等人 [[101](#bib.bib101)] 提出了RDP-CGAN（Rényi DP和卷积GAN），解决了生成合成数据时可能遇到的两个挑战：1）混合离散-连续数据，2）相关数据（时间相关性或相关特征）。这些特性在健康数据中尤为常见。RDP-CGAN通过在WGAN中添加一种无监督特征学习步骤，即卷积自编码器，来处理离散数据。该自编码器使用DP-SGD进行训练，将（离散的）输入空间映射到连续空间。数据中的相关性通过一维卷积层进行考虑。
- en: Jordon et al. [[61](#bib.bib61)] took another approach and proposed a differentially
    private GAN based on the PATE framework. $k$ teacher discriminators are trained
    to distinguish real and synthetic data samples. The teachers are then used to
    label the training data for the student discriminator (by noisy aggregation of
    their predictions). A main contribution is that PATE-GAN sidesteps the need for
    public data of the original PATE method. This is essential in this setting as
    the generation of synthetic data is usually necessary exactly because no public
    data is available. They could show that their approach outperforms DPGAN.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Jordon等人 [[61](#bib.bib61)] 采取了另一种方法，提出了一种基于PATE框架的不同ially private GAN。$k$个教师鉴别器被训练用来区分真实和合成数据样本。然后，教师被用来为学生鉴别器标记训练数据（通过对其预测的嘈杂聚合）。一个主要贡献是PATE-GAN避免了原始PATE方法对公共数据的需求。在这种情况下，这一点至关重要，因为合成数据的生成通常正是因为没有公共数据。他们能够证明他们的方法优于DPGAN。
- en: 8 Specific applications of DP-DL
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 特定的DP-DL应用
- en: 'Differentially private deep learning (DP-DL) has attracted increased interest
    in a wide range of application areas. Table [8](#S8.T8 "Table 8 ‣ 8 Specific applications
    of DP-DL ‣ Recent Advances of Differential Privacy in Centralized Deep Learning:
    A Systematic Survey") lists the application fields and corresponding papers discussed
    in this survey.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私深度学习（DP-DL）在广泛的应用领域中引起了越来越多的关注。表[8](#S8.T8 "表 8 ‣ 8 特定的DP-DL应用 ‣ 差分隐私在集中式深度学习中的最新进展：系统性调查")列出了应用领域和本调查中讨论的相关论文。
- en: 'Table 8: Summary of application areas of DP-DL.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：DP-DL应用领域的总结。
- en: '| Application | Paper |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 论文 |'
- en: '| --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Image publishing | [[123](#bib.bib123), [110](#bib.bib110)] |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 图像发布 | [[123](#bib.bib123), [110](#bib.bib110)] |'
- en: '| Medical image analysis | [[80](#bib.bib80), [111](#bib.bib111)] |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 医学图像分析 | [[80](#bib.bib80), [111](#bib.bib111)] |'
- en: '| Face recognition | [[69](#bib.bib69), [27](#bib.bib27)] |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 人脸识别 | [[69](#bib.bib69), [27](#bib.bib27)] |'
- en: '| Video analysis | [[25](#bib.bib25), [50](#bib.bib50)] |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 视频分析 | [[25](#bib.bib25), [50](#bib.bib50)] |'
- en: '| Natural language processing | [[68](#bib.bib68), [73](#bib.bib73), [45](#bib.bib45),
    [46](#bib.bib46), [17](#bib.bib17)] |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言处理 | [[68](#bib.bib68), [73](#bib.bib73), [45](#bib.bib45), [46](#bib.bib46),
    [17](#bib.bib17)] |'
- en: '| Smart grid | [[105](#bib.bib105), [13](#bib.bib13)] |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 智能电网 | [[105](#bib.bib105), [13](#bib.bib13)] |'
- en: '| Recommender systems | [[126](#bib.bib126), [29](#bib.bib29)] |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 推荐系统 | [[126](#bib.bib126), [29](#bib.bib29)] |'
- en: '| Mobile devices | [[108](#bib.bib108), [69](#bib.bib69), [33](#bib.bib33),
    [60](#bib.bib60)] |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 移动设备 | [[108](#bib.bib108), [69](#bib.bib69), [33](#bib.bib33), [60](#bib.bib60)]
    |'
- en: 8.1 Image publishing
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 图像发布
- en: Nowadays, a high amount of images is getting published on a daily basis via
    the internet. In particularly privacy-sensitive cases, de-identification techniques
    like blurring and pixelation are used to protect certain objects (e.g., faces
    and license plates). However, these methods compromise the images’ quality and
    usability. Yu et al. [[123](#bib.bib123)] and Wen et al. [[110](#bib.bib110)]
    proposed to replace the sensitive image content with synthetic data in a differentially
    private manner. They both use GANs and introduce Laplace noise into the latent
    representations. While Yu et al. [[123](#bib.bib123)] first applied a CNN to detect
    the sensitive image regions, Wen et al. [[110](#bib.bib110)] focused specifically
    on face anonymization making this step redundant. Instead, they concentrated on
    preserving the visual traits by encoding the attribute and identity information
    separately and only perturbing the latter.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，通过互联网每天发布大量图像。在隐私特别敏感的情况下，通常使用去标识化技术，如模糊和像素化，以保护某些对象（例如，面孔和车牌）。然而，这些方法会影响图像的质量和可用性。Yu等人
    [[123](#bib.bib123)] 和Wen等人 [[110](#bib.bib110)] 提议以差分隐私的方式用合成数据替代敏感的图像内容。他们都使用了GAN，并在潜在表示中引入了拉普拉斯噪声。虽然Yu等人
    [[123](#bib.bib123)] 首先应用CNN来检测敏感图像区域，Wen等人 [[110](#bib.bib110)] 专注于面部匿名化，使这一步骤变得多余。他们集中在通过分别编码属性和身份信息来保护视觉特征，只对后者进行扰动。
- en: 8.2 Medical image analysis
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 医学图像分析
- en: Privacy is of particular relevance in the health domain. One application area
    where DP was recently applied is medical image analysis, e.g., for COVID-19 diagnoses
    from chest X-rays [[80](#bib.bib80)] and for classification of histology images
    [[111](#bib.bib111)]. The former applied PATE on a convolutional deep neural network.
    The latter introduced P3SGD (patient privacy preserving SGD), a variant of DP-SGD
    that protects patient-level instead of image-level privacy. They showed that differential
    privacy can not only preserve privacy but also mitigate overfitting in cases of
    small numbers of training records.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私在健康领域尤为重要。最近，差分隐私（DP）被应用于医学图像分析，例如从胸部X光片中诊断COVID-19 [[80](#bib.bib80)] 和分类组织学图像
    [[111](#bib.bib111)]。前者在卷积深度神经网络上应用了PATE。后者引入了P3SGD（患者隐私保护SGD），这是DP-SGD的一个变种，保护患者级别的隐私而不是图像级别的隐私。他们展示了差分隐私不仅可以保护隐私，还可以在训练记录较少的情况下缓解过拟合。
- en: 8.3 Face recognition
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 人脸识别
- en: Face recognition is a prevalent technology for security, e.g., for unlocking
    smartphones and for surveillance. For face recognition algorithms, not only privacy
    but also performance is critical as they are often deployed on devices with limited
    resources and the analyses should be carried out in real-time. To this end, methods
    that decrease the size of the models while preserving differential privacy were
    proposed.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸识别是一种广泛使用的安全技术，例如用于解锁智能手机和监控。对于人脸识别算法，隐私和性能都至关重要，因为它们通常部署在资源有限的设备上，分析应实时进行。为此，提出了在保持差分隐私的同时减小模型大小的方法。
- en: Li et al. [[69](#bib.bib69)] introduced LightFace, a lightweight deep learning
    model designed for private face recognition on mobile devices. The approach uses
    depth-wise separable convolutions for model size reduction, and a Bayesian GAN
    and ensemble learning for privacy preservation. They were able to decrease the
    model size and the computational demand while still outperforming both DP-SGD
    and PATE. Chamikara et al. [[27](#bib.bib27)] proposed PEEP (privacy using eigenface
    perturbation), where the dimensionality of the images for training the deep learning
    model is reduced with differentially private PCA (Principal Component Analysis).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 [[69](#bib.bib69)] 引入了 LightFace，这是一种轻量级深度学习模型，旨在用于移动设备上的私人面部识别。该方法使用深度可分离卷积来减少模型大小，并使用贝叶斯
    GAN 和集成学习来保护隐私。他们能够在减少模型大小和计算需求的同时，仍然超越了 DP-SGD 和 PATE。Chamikara 等人 [[27](#bib.bib27)]
    提出了 PEEP（使用特征脸扰动的隐私保护），通过差分隐私 PCA（主成分分析）来减少用于训练深度学习模型的图像维度。
- en: 8.4 Video analysis
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 视频分析
- en: With the prevalence of CCTV cameras, automatic video analyses are on the rise,
    including traffic monitoring and security surveillance. Compared to image analysis,
    video analysis is more complex due to the additional time dimension. Private traffic
    monitoring tries to answer questions like how many people or cars were passing
    in a certain time period or how long people or cars were visible on average, while
    disclosing no information about individuals (e.g., if a specific person or car
    was observed). To this end, Cangialosi et al. [[25](#bib.bib25)] introduced the
    differentially private video analytics system Privid. Privacy-preserving video
    analysis is also relevant in security surveillance, e.g., for crime and threat
    detection. Giorgi et al. [[50](#bib.bib50)] proposed training an autoencoder with
    DP-SGD for detecting anomalies (e.g., vandalism, robbery, assault) in CCTV footage.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 CCTV 摄像头的普及，自动视频分析正在增加，包括交通监控和安全监控。与图像分析相比，视频分析由于额外的时间维度而更加复杂。私人交通监控试图回答诸如某一时间段内有多少人或汽车经过，或人或汽车平均可见多长时间的问题，同时不披露有关个人的信息（例如，是否观察到特定的人或汽车）。为此，Cangialosi
    等人 [[25](#bib.bib25)] 引入了差分隐私视频分析系统 Privid。隐私保护的视频分析在安全监控中也很相关，例如，用于犯罪和威胁检测。Giorgi
    等人 [[50](#bib.bib50)] 提出了使用 DP-SGD 训练自动编码器，以检测 CCTV 视频中的异常（例如，破坏、抢劫、袭击）。
- en: 8.5 Natural language processing
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5 自然语言处理
- en: 'Differential privacy is also increasingly applied in Natural language processing
    (NLP), protecting against different threat scenarios like membership inference
    of a phrase or word, author attribute inference (e.g., age or gender), authorship
    identification, or disclosure of sensitive content. Depending on the use-case
    and threat scenario, the unit of privacy can be, e.g., a token, a word, a sentence,
    a document, or a user. Differentially private NLP models can be achieved in two
    ways: 1) by training the model with DP-SGD [[68](#bib.bib68)], or 2) by perturbing
    the text representations further used for training [[73](#bib.bib73), [45](#bib.bib45),
    [46](#bib.bib46)].'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私也越来越多地应用于自然语言处理（NLP），以保护免受不同威胁场景的影响，例如短语或单词的成员推断、作者属性推断（例如年龄或性别）、作者身份识别或敏感内容披露。根据使用情况和威胁场景，隐私单位可以是，例如，令牌、单词、句子、文档或用户。差分隐私
    NLP 模型可以通过两种方式实现：1）通过使用 DP-SGD 进行模型训练 [[68](#bib.bib68)]，或者 2）通过进一步扰动用于训练的文本表示
    [[73](#bib.bib73), [45](#bib.bib45), [46](#bib.bib46)]。
- en: Applying the original DP-SGD on large language models can lead to deficient
    accuracy and unreasonably high computational and memory overhead. Pretraining,
    refined hyperparameter selection and finetuning can improve the performance significantly
    [[68](#bib.bib68)]. Moreover, Li et al. [[68](#bib.bib68)] proposed "ghost clipping",
    which avoids computing the per-example gradients explicitly and infers the per-example
    gradient norms in a less memory-demanding way.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始 DP-SGD 应用于大型语言模型可能会导致准确性不足和不合理的计算及内存开销。预训练、精细的超参数选择和微调可以显著提高性能 [[68](#bib.bib68)]。此外，Li
    等人 [[68](#bib.bib68)] 提出了“鬼影裁剪”，它避免了明确计算每个样本的梯度，并以较少的内存需求推断每个样本的梯度范数。
- en: The perturbation of text representations is a local DP method. Even though it
    can be used in centralized deep learning, it is primarily applied in the distributed
    setting where the server training the model is untrusted. Additionally to pure
    text processing, text representation perturbation can also be applied to related
    tasks, e.g., speech transcription [[17](#bib.bib17)].
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 文本表示的扰动是一种局部差分隐私方法。尽管它可以用于集中式深度学习，但主要应用于分布式环境，其中训练模型的服务器是不可信的。除了纯文本处理，文本表示扰动还可以应用于相关任务，例如语音转录
    [[17](#bib.bib17)]。
- en: 8.6 Smart energy networks
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 智能能源网络
- en: Smart energy networks, also called smart grids, use digital technologies to
    optimize the generation, distribution and usage of electricity. Key components
    are smart meters, which allow real-time monitoring of energy consumption and,
    thereby, load forecasting. However, smart meter data can disclose personal information,
    e.g., daily habits of residents.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 智能能源网络，也称为智能电网，使用数字技术来优化电力的生成、分配和使用。关键组件是智能电表，它们允许实时监测能源消耗，从而进行负荷预测。然而，智能电表数据可能会泄露个人信息，例如居民的日常习惯。
- en: Abdalzaher et al. [[13](#bib.bib13)] provided an overview of privacy-related
    issues of smart meters and possible defense mechanisms including differential
    privacy. While they focused on private data release, Ustundag Soykan et al. [[105](#bib.bib105)]
    proposed a private load forecasting method on smart meter data using DP-SGD.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Abdalzaher 等人 [[13](#bib.bib13)] 提供了智能电表隐私相关问题及可能的防御机制的概述，包括差分隐私。虽然他们专注于私人数据发布，Ustundag
    Soykan 等人 [[105](#bib.bib105)] 提出了基于 DP-SGD 的智能电表数据私人负荷预测方法。
- en: 8.7 Recommender systems
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 推荐系统
- en: Recommender systems are programs that provide personalized recommendations to
    users, based on their past behaviors and contextual information (so called user
    features). Similar to other machine learning models, recommender systems are at
    risk of leaking personal information.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是根据用户的过去行为和上下文信息（即所谓的用户特征）向用户提供个性化推荐的程序。与其他机器学习模型类似，推荐系统也有泄露个人信息的风险。
- en: Zhang et al. [[126](#bib.bib126)] proposed a recommender system based on a graph
    convolutional network that protects both the user-item interactions (modelled
    as a graph) and the additionally used user features. The former is achieved by
    perturbation of the model’s (polynomially approximated) loss function; the latter
    by adding noise directly to the user features.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人 [[126](#bib.bib126)] 提出了一个基于图卷积网络的推荐系统，该系统保护了用户-物品交互（建模为图）和附加使用的用户特征。前者是通过扰动模型的（多项式近似）损失函数来实现的；后者则是通过直接向用户特征添加噪声来实现的。
- en: Chen et al. [[29](#bib.bib29)] focused on cross-domain recommendation, where
    knowledge is transferred from one domain to another, usually because the target
    domain lacks enough data. The information transfer introduces a privacy risk for
    the users of the source domain. Chen et al. proposed a solution based on differentially
    private rating publishing and subsequent recommendation modeling with deep neural
    networks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人 [[29](#bib.bib29)] 关注了跨领域推荐，其中知识从一个领域转移到另一个领域，通常是因为目标领域缺乏足够的数据。信息转移对源领域的用户引入了隐私风险。陈等人提出了一种基于差分隐私评级发布和随后的深度神经网络推荐建模的解决方案。
- en: 8.8 Mobile devices
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8 移动设备
- en: 'The combination of differential privacy and deep learning was also studied
    in the context of mobile devices. On the one hand, studies looked at how to reduce
    the model’s size and computational demand to allow the deployment and/or training
    of deep neural networks on devices with limited resources while still preserving
    utility and training privacy. For example, Wang et al. [[108](#bib.bib108)] proposed
    an architecture-independent approach based on hint learning and knowledge distillation,
    and Li et al. [[69](#bib.bib69)] introduced the lightweight face recognition algorithm
    LightFace already discussed in Section [8.3](#S8.SS3 "8.3 Face recognition ‣ 8
    Specific applications of DP-DL ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey"). On the other hand, differential privacy
    was applied on location-based services - a typical application class for mobile
    devices [[33](#bib.bib33), [60](#bib.bib60)].'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '差分隐私与深度学习的结合也在移动设备的背景下进行了研究。一方面，研究探讨了如何减少模型的大小和计算需求，以允许在资源有限的设备上部署和/或训练深度神经网络，同时保持效用和训练隐私。例如，Wang等人[[108](#bib.bib108)]提出了一种基于提示学习和知识蒸馏的架构无关方法，而Li等人[[69](#bib.bib69)]介绍了在第[8.3](#S8.SS3
    "8.3 Face recognition ‣ 8 Specific applications of DP-DL ‣ Recent Advances of
    Differential Privacy in Centralized Deep Learning: A Systematic Survey")节讨论的轻量级面部识别算法LightFace。另一方面，差分隐私被应用于基于位置的服务——这是移动设备的典型应用类别[[33](#bib.bib33),
    [60](#bib.bib60)]。'
- en: 9 Discussion and future directions
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 讨论与未来方向
- en: 'This study reviews the latest developments on differential privacy in centralized
    deep learning. The main research focuses of the last years were: 1) auditing and
    evaluation, 2) improvements of the privacy-utility trade-off, 3) applications
    of DP-DL to protect against threats other than membership and attribute inference,
    4) differentially private generative models, and 5) specific application domains.
    For each subtopic, we provided a comprehensive summary of recent advances. In
    this last section, we discuss the key points, interconnections and expected future
    directions of the respective topics and differentially private centralized deep
    learning in general.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究回顾了差分隐私在集中式深度学习中的最新进展。过去几年的主要研究焦点包括：1）审计和评估，2）隐私效用权衡的改进，3）DP-DL在保护免受除成员资格和属性推断以外的威胁方面的应用，4）差分隐私生成模型，和5）具体应用领域。对于每个子主题，我们提供了最近进展的全面总结。在最后一节中，我们讨论了各个主题及差分隐私集中式深度学习的一些关键点、相互联系以及预期的未来方向。
- en: 'Auditing and evaluating deep learning models is a key research topic not only
    but especially for differentially private models. We expect the trend of novel
    attacks to continue, analyzing new threat scenarios and improving our understanding
    of which aspects influence the attack success. An important element of evaluation
    is the used dataset. Interestingly, many commonly used datasets for auditing DP-DL
    (e.g., MNIST, CIFAR-10 or CIFAR-100; see Table [3](#S4.T3 "Table 3 ‣ 4.1 Attack-based
    Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances of Differential Privacy
    in Centralized Deep Learning: A Systematic Survey")) are not "relevant to the
    privacy problem" [[35](#bib.bib35)]. There is a need for more realistic benchmark
    datasets that include private features. Another point to consider is that with
    the rise of continual learning, auditing is not only relevant once before deployment
    but should be carried out repeatedly whenever the training set changes [[35](#bib.bib35)].
    This is also the case when machine unlearning is applied (as discussed in Section
    [6](#S6 "6 Beyond membership and attribute inference: Applying DP-DL to protect
    against other threats ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey")).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '审计和评估深度学习模型不仅是一个关键研究课题，尤其是对于差分隐私模型。我们预期新型攻击的趋势将持续，分析新的威胁场景并提高我们对影响攻击成功的各个方面的理解。评估的一个重要因素是所使用的数据集。有趣的是，许多用于审计DP-DL的常用数据集（例如MNIST、CIFAR-10或CIFAR-100；参见表[3](#S4.T3
    "Table 3 ‣ 4.1 Attack-based Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")）与“隐私问题”并不“相关”[[35](#bib.bib35)]。需要更多包括隐私特征的现实基准数据集。另一个需要考虑的点是，随着持续学习的兴起，审计不仅在部署前相关，而且应在每次训练集更改时重复进行[[35](#bib.bib35)]。这也是应用机器卸载时的情况（如第[6](#S6
    "6 Beyond membership and attribute inference: Applying DP-DL to protect against
    other threats ‣ Recent Advances of Differential Privacy in Centralized Deep Learning:
    A Systematic Survey")节讨论）。'
- en: The accuracy disparity between subgroups is one of many biases that are studied
    in the field of fair AI - with the goal of avoiding discriminatory behavior of
    machine learning models. Its amplification by differential privacy, the underlying
    causes and possible mitigation strategies are actively researched. For example,
    de Oliviera et al. [[37](#bib.bib37)] suggested that they are preventable by better
    hyperparameter selection.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 子组之间的准确性差异是公平AI领域研究的许多偏差之一，旨在避免机器学习模型的歧视性行为。差分隐私对这一问题的放大效应、其根本原因以及可能的缓解策略正受到积极研究。例如，de
    Oliviera等人[[37](#bib.bib37)]建议通过更好的超参数选择来预防这些问题。
- en: Additionally to empirical assessment, theoretical privacy analysis might be
    able to provide more realistic upper bounds by including additional assumptions
    (e.g., about the attacker’s capabilities) or features (e.g., the clipping norm
    or initial randomness [[58](#bib.bib58)]). This could also improve the privacy-utility
    trade-off.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 除了经验评估，理论隐私分析可能通过包含额外的假设（例如，攻击者的能力）或特征（例如，裁剪范数或初始随机性[[58](#bib.bib58)]）来提供更现实的上限。这也可能改善隐私与效用的权衡。
- en: 'We also anticipate further research on novel strategies or advancement on existing
    approaches to improve the privacy-utility trade-off. Some of the mentioned methods
    could be combined in the future, for example, the differentially private lottery
    ticket hypothesis approach by Gondara et al. [[51](#bib.bib51)] can be combined
    with tempered sigmoid activation functions [[86](#bib.bib86)]. Additional effort
    should be made to compare different methods to identify the best performing method(s).
    Simply summarizing the reported results, as we did in Table [5](#S5.T5 "Table
    5 ‣ 5 Improving the privacy-utility trade-off of DP-DL ‣ Recent Advances of Differential
    Privacy in Centralized Deep Learning: A Systematic Survey"), can not provide sufficient
    insight. Fair comparison would require testing the methods on the same model (i.e.,
    same architecture and hyperparameters) with the same evaluation dataset for the
    same privacy level.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还预计会有对新颖策略的进一步研究或对现有方法的改进，以提高隐私与效用的权衡。一些提到的方法在未来可能会结合使用，例如，Gondara等人提出的**差分隐私彩票票假设方法**[[51](#bib.bib51)]可以与**温和的Sigmoid激活函数**[[86](#bib.bib86)]结合使用。还应进一步比较不同的方法，以识别表现最佳的方法。仅仅总结报告结果，如我们在表[5](#S5.T5
    "Table 5 ‣ 5 Improving the privacy-utility trade-off of DP-DL ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")中所做的，无法提供足够的洞察。公平的比较需要在相同模型（即相同架构和超参数）上使用相同的评估数据集进行测试，以保持相同的隐私水平。'
- en: When comparing the different approaches, it is also important to note that some
    rely on public data [[102](#bib.bib102), [51](#bib.bib51), [15](#bib.bib15), [124](#bib.bib124)].
    On the one hand, public data might not be available and, therefore, prevent the
    application of those methods. On the other hand, it is debatable whether public
    availability justifies disregarding all privacy considerations (see [[35](#bib.bib35),
    [103](#bib.bib103)] for further information).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较不同方法时，还需要注意一些方法依赖于公开数据[[102](#bib.bib102), [51](#bib.bib51), [15](#bib.bib15),
    [124](#bib.bib124)]。一方面，公开数据可能不可用，因此可能阻碍这些方法的应用。另一方面，是否公开可用是否能证明忽略所有隐私考虑是值得商榷的（参见[[35](#bib.bib35),
    [103](#bib.bib103)]获取更多信息）。
- en: 'Section [6](#S6 "6 Beyond membership and attribute inference: Applying DP-DL
    to protect against other threats ‣ Recent Advances of Differential Privacy in
    Centralized Deep Learning: A Systematic Survey") showed that the concept of differential
    privacy can be beneficial in diverse threat scenarios. Especially the connection
    to robustness and explainability might gain importance through the growing interest
    in trustworthy AI.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '部分[6](#S6 "6 Beyond membership and attribute inference: Applying DP-DL to protect
    against other threats ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey")显示，差分隐私的概念在不同的威胁场景中可能是有益的。尤其是与鲁棒性和可解释性的联系可能会因对可信AI的日益关注而变得更加重要。'
- en: The increasing awareness of privacy concerns in combination with the many open
    questions regarding ethical, legal and methodical aspects make using synthetic
    data a tempting alternative to applying privacy-enhancing technologies to private
    data. However, it is important to spread the knowledge that synthetic data alone
    is not by default privacy-preserving [[98](#bib.bib98)]. Additional protection
    might be necessary. Even though differentially private synthetic data can be a
    viable solution, future research is needed, for example, to find good ways to
    evaluate the usefulness of the data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对隐私问题的日益关注，加上对伦理、法律和方法方面众多未解之谜，使得使用合成数据成为一种吸引人的替代方案，而不是对私有数据应用隐私增强技术。然而，重要的是要传播一个认识，即合成数据本身并不是默认的隐私保护[[98](#bib.bib98)]。可能需要额外的保护。尽管差分隐私合成数据可以是一个可行的解决方案，但仍需要进一步的研究，例如寻找评估数据有效性的好方法。
- en: This survey demonstrated how diverse the methods and applications of differential
    privacy can be on the example of deep learning models. While this flexibility
    is one of the strengths of differential privacy, it can also be a hindrance to
    its broad deployment due to insufficient understanding. The efforts to make DP
    more accessible to a wider audience and to promote its (correct) application should
    continue. That includes not only discussions about how to choose the method, the
    unit of privacy, and the privacy budget, but also how to verify that implementations
    are correct (see [[62](#bib.bib62)] and references therein for more information).
    An example where an implementation error lead to privacy issues even though theoretically
    DP was proven can be found in [[104](#bib.bib104)].
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查展示了差分隐私在深度学习模型中的方法和应用的多样性。虽然这种灵活性是差分隐私的一个优势，但由于理解不足，它也可能成为其广泛应用的障碍。应继续努力使差分隐私对更广泛的受众更易接触，并促进其（正确的）应用。这不仅包括关于如何选择方法、隐私单位和隐私预算的讨论，还包括如何验证实现是否正确（有关更多信息，请参见[[62](#bib.bib62)]及其中的参考文献）。一个实现错误导致隐私问题的例子，即使理论上差分隐私已被证明，可以在[[104](#bib.bib104)]中找到。
- en: 'Additional to the properties inherent to differential privacy that make it
    hard to understand for laypeople (see Section [3.1](#S3.SS1 "3.1 Differential
    Privacy (DP) ‣ 3 Preliminaries ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey")), names that are used ambiguously in the
    research field can add to the confusion. For example, model inversion can refer
    to inferring 1) the attribute of a single record, or 2) the attribute of a class.
    While the first obviously implies a privacy concern, the second is primarily problematic
    if a class consists of instances of one individual, e.g., as is the case in face
    recognition. As a result of this ambiguous meaning, contradicting conclusions
    emerged about whether differential privacy protects against model inversion attacks.
    Some (e.g., [[122](#bib.bib122)]) used the first interpretation and concluded
    that DP naturally also protects against model inversion. Others (e.g., [[127](#bib.bib127)])
    used the second interpretation and showed that DP does not always do so. Interestingly,
    Park et al. [[87](#bib.bib87)] also uses the second interpretation but concluded
    that DP mitigates model inversion attacks. Maybe this is because DP decreases
    the accuracy of the target model, and, as Zhang et al. [[127](#bib.bib127)] argued,
    predictive power and vulnerability to model inversion go hand in hand. Future
    research should pay attention to accurately define the used terms, and, ideally,
    the research community should agree on a coherent taxonomy.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '除了差分隐私固有的特性使其对外行人难以理解（见第[3.1节](#S3.SS1 "3.1 Differential Privacy (DP) ‣ 3 Preliminaries
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey")），在研究领域中模糊使用的名称也会增加混淆。例如，模型反演可以指推断1) 单个记录的属性，或2) 一个类别的属性。虽然第一种情况显然涉及隐私问题，但第二种情况主要在类别由一个个体的实例组成时才会成为问题，例如在面部识别中。由于这种含糊的意义，关于差分隐私是否保护免受模型反演攻击的结论产生了相互矛盾的看法。一些研究（例如[[122](#bib.bib122)]）使用了第一种解释，并得出结论认为差分隐私自然也保护免受模型反演。其他研究（例如[[127](#bib.bib127)]）使用了第二种解释，并表明差分隐私并不总是能做到这一点。有趣的是，Park等人[[87](#bib.bib87)]也使用了第二种解释，但得出结论认为差分隐私能减轻模型反演攻击。这可能是因为差分隐私降低了目标模型的准确性，正如Zhang等人[[127](#bib.bib127)]所说，预测能力和对模型反演的脆弱性往往是相辅相成的。未来的研究应关注准确定义所使用的术语，并且理想情况下，研究界应就一个连贯的分类法达成一致。'
- en: With the rise of deep learning used in real-world scenarios, new challenges
    arise. For example, real-world datasets often contain various dependencies, where
    domain knowledge is required for their correct interpretation. Causal models [[89](#bib.bib89)]
    may help to capture and model this domain knowledge and additionally improve interpretability
    [[79](#bib.bib79)] and act as guide to avoid biases [[75](#bib.bib75)]. However,
    they may have new implications on privacy. Growing interest is not only coming
    from the research and industrial community, but also the public is actively engaging
    in discussions about the impact of AI applications on society. Most recently large
    language models like ChatGPT [[2](#bib.bib2)] are in the spotlight - among other
    things due to privacy concerns. The future will show whether differential privacy
    will be part of the next generation of deep learning deployments.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习在现实世界中的应用增加，新挑战也随之而来。例如，现实世界的数据集往往包含各种依赖关系，需要领域知识来正确解释。因果模型[[89](#bib.bib89)]可能有助于捕捉和建模这些领域知识，并进一步提高可解释性[[79](#bib.bib79)]，并作为避免偏见的指南[[75](#bib.bib75)]。然而，它们可能对隐私产生新的影响。增长的兴趣不仅来自研究和工业界，公众也积极参与有关AI应用对社会影响的讨论。最近，大型语言模型如ChatGPT[[2](#bib.bib2)]成为焦点——其中之一是由于隐私问题。未来将显示差分隐私是否会成为下一代深度学习部署的一部分。
- en: All in all, differentially private deep learning achieved significant progress
    in recent years but open questions are still numerous. We expect the interest
    in the topic to increase further, especially as new standards and legal frameworks
    arise. On the way to trustworthy AI, we need not only technical innovations but
    also legal and ethical discussions about what privacy preservation means in the
    digital age.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，差分隐私深度学习在近年来取得了显著进展，但仍有许多未解的问题。我们期待这一主题的兴趣进一步增加，特别是随着新的标准和法律框架的出现。在通向可信AI的道路上，我们不仅需要技术创新，还需要关于数字时代隐私保护的法律和伦理讨论。
- en: 10 Conclusion
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 结论
- en: 'This survey provides a comprehensive overview of recent trends and developments
    of differential privacy in centralized deep learning. Throughout the paper, we
    highlight the different research focuses of the last years, including auditing
    and evaluating differentially private models, improving the trade-off between
    privacy and utility, applying differential privacy methods to threats beyond membership
    and attribute inference, generating private synthetic data, and applying differentially
    private deep learning models to different application fields. A total of six insights
    have been derived from literature: (1) A need for more realistic benchmark datasets
    with private features. (2) The necessity for repeated auditing. (3) More realistic
    upper privacy bounds would be possible by including additional attack assumptions
    and model features. (4) Privacy-utility trade-offs can be improved by better comparison
    of existing methods and a possible combination of best approaches. (5) By default
    synthetic data is not privacy-preserving and differentially private synthetic
    data requires more research. (6) Ambiguously used terms lead to confusion in the
    research field and a coherent taxonomy is needed.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查提供了集中深度学习中差分隐私的最新趋势和发展全面概述。在整篇论文中，我们强调了近年来不同的研究重点，包括审计和评估差分隐私模型、改善隐私与实用性之间的权衡、将差分隐私方法应用于超出成员资格和属性推断的威胁、生成私密的合成数据，以及将差分隐私深度学习模型应用于不同的应用领域。从文献中得出了六个见解：(1)
    需要具有私密特征的更现实的基准数据集。(2) 需要重复审计。(3) 通过包含额外的攻击假设和模型特征，可以实现更现实的隐私上界。(4) 通过更好地比较现有方法和可能的最佳方法组合，可以改善隐私-效用权衡。(5)
    默认情况下，合成数据不具备隐私保护功能，差分隐私合成数据需要更多的研究。(6) 模糊使用的术语导致了研究领域的混乱，需要一个连贯的分类法。
- en: In summary, we explore the advancements, remaining challenges, and future prospects
    of integrating mathematical privacy guarantees into deep learning models. By shedding
    light on the current state of the field and emphasizing its potential, we hope
    to inspire further research and real-world applications of differentially private
    deep learning.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们探讨了将数学隐私保证集成到深度学习模型中的进展、现存挑战和未来前景。通过阐明该领域的现状并强调其潜力，我们希望激发进一步的研究和差分隐私深度学习的实际应用。
- en: Acknowledgments
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'The research leading to these results has received funding from the AI for
    Green programme (Grant: 4352956). AI for Green is a research, technology and innovation
    funding programme of the Republic of Austria, Ministry of Climate Action (BMK).
    The Austrian Research Promotion Agency (FFG) has been authorised for the programme
    management. Additionally, this work was supported by the "DDAI" COMET Module within
    the COMET – Competence Centers for Excellent Technologies Programme, funded by
    the Austrian Federal Ministry (BMK and BMDW), the Austrian Research Promotion
    Agency (FFG), the province of Styria (SFG) and partners from industry and academia.
    The COMET Programme is also managed by FFG.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果的研究得到了“AI for Green”计划（资助编号：4352956）的资助。AI for Green 是奥地利共和国气候行动部（BMK）的研究、技术和创新资助计划。奥地利研究促进局（FFG）被授权负责该计划的管理。此外，这项工作得到了“DDAI”
    COMET 模块在 COMET – 卓越技术中心计划中的支持，该计划由奥地利联邦部（BMK 和 BMDW）、奥地利研究促进局（FFG）、斯蒂利亚省（SFG）以及来自工业和学术界的合作伙伴资助。COMET
    计划也由 FFG 管理。
- en: References
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] At&t laboratories cambridge: The database of faces. Last accessed May 31,
    2023 from [https://cam-orl.co.uk/facedatabase.html/](https://cam-orl.co.uk/facedatabase.html/).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] At&t 实验室剑桥：人脸数据库。最后访问时间为 2023年5月31日，网址为 [https://cam-orl.co.uk/facedatabase.html/](https://cam-orl.co.uk/facedatabase.html/)。'
- en: '[2] Chatgpt. Last accessed June 7, 2023 from [https://openai.com/chatgpt](https://openai.com/chatgpt).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Chatgpt。最后访问时间为 2023年6月7日，网址为 [https://openai.com/chatgpt](https://openai.com/chatgpt)。'
- en: '[3] Github: Vggface2 dataset for face recognition. Last accessed May 31, 2023
    from [https://github.com/ox-vgg/vgg_face2](https://github.com/ox-vgg/vgg_face2).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Github: Vggface2 数据集用于人脸识别。最后访问时间为 2023年5月31日，网址为 [https://github.com/ox-vgg/vgg_face2](https://github.com/ox-vgg/vgg_face2)。'
- en: '[4] Kaggle: Acquire valued shoppers challenge. Last accessed May 31, 2023 from
    [https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data](https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Kaggle: 价值购物者挑战。最后访问时间为 2023年5月31日，网址为 [https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data](https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data)。'
- en: '[5] Labeled faces in the wild. Last accessed May 31, 2023 from [https://vis-www.cs.umass.edu/lfw/](https://vis-www.cs.umass.edu/lfw/).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 野外标记的人脸。最后访问时间为 2023年5月31日，网址为 [https://vis-www.cs.umass.edu/lfw/](https://vis-www.cs.umass.edu/lfw/)。'
- en: '[6] Uci machine learning repository: Adult data set. Last accessed May 31,
    2023 from [https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult).'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Uci 机器学习库：成人数据集。最后访问时间为 2023年5月31日，网址为 [https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult)。'
- en: '[7] Uci machine learning repository: Breast cancer wisconsin (diagnostic) data
    set. Last accessed May 31, 2023 from [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Uci 机器学习库：乳腺癌威斯康星（诊断）数据集。最后访问时间为 2023年5月31日，网址为 [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))。'
- en: '[8] Uci machine learning repository: Diabetes data set. Last accessed May 31,
    2023 from [https://archive.ics.uci.edu/ml/datasets/Diabetes](https://archive.ics.uci.edu/ml/datasets/Diabetes).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Uci 机器学习库：糖尿病数据集。最后访问时间为 2023年5月31日，网址为 [https://archive.ics.uci.edu/ml/datasets/Diabetes](https://archive.ics.uci.edu/ml/datasets/Diabetes)。'
- en: '[9] Uci machine learning repository: Hepatitis data set. Last accessed May
    31, 2023 from [https://archive.ics.uci.edu/ml/datasets/Hepatitis](https://archive.ics.uci.edu/ml/datasets/Hepatitis).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Uci 机器学习库：肝炎数据集。最后访问时间为 2023年5月31日，网址为 [https://archive.ics.uci.edu/ml/datasets/Hepatitis](https://archive.ics.uci.edu/ml/datasets/Hepatitis)。'
- en: '[10] Uci machine learning repository: Statlog (german credit data) data set.
    Last accessed May 31, 2023 from [https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Uci 机器学习库：Statlog（德国信用数据）数据集。最后访问时间为 2023年5月31日，网址为 [https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)。'
- en: '[11] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar,
    K., and Zhang, L. Deep Learning with Differential Privacy. In Proceedings of the
    2016 ACM SIGSAC Conference on Computer and Communications Security (Vienna Austria,
    Oct. 2016), ACM, pp. 308–318.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar,
    K., 和 Zhang, L. 深度学习与差分隐私。在2016年ACM SIGSAC计算机与通信安全会议（奥地利维也纳，2016年10月）的论文集中，ACM，第308-318页。'
- en: '[12] Abay, N. C., Zhou, Y., Kantarcioglu, M., Thuraisingham, B., and Sweeney,
    L. Privacy Preserving Synthetic Data Release Using Deep Learning. In Machine Learning
    and Knowledge Discovery in Databases (Cham, 2019), M. Berlingerio, F. Bonchi,
    T. Gärtner, N. Hurley, and G. Ifrim, Eds., Lecture Notes in Computer Science,
    Springer International Publishing, pp. 510–526.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Abay, N. C., Zhou, Y., Kantarcioglu, M., Thuraisingham, B., 和 Sweeney,
    L. 利用深度学习进行隐私保护的合成数据发布。发表于数据库中的机器学习和知识发现（Cham, 2019），M. Berlingerio, F. Bonchi,
    T. Gärtner, N. Hurley, 和 G. Ifrim 编，计算机科学讲义，Springer International Publishing，第510–526页。'
- en: '[13] Abdalzaher, M. S., Fouda, M. M., and Ibrahem, M. I. Data Privacy Preservation
    and Security in Smart Metering Systems. Energies 15, 19 (Jan. 2022), 7419.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Abdalzaher, M. S., Fouda, M. M., 和 Ibrahem, M. I. 智能计量系统中的数据隐私保护和安全。能源
    15, 19 (2022年1月), 7419。'
- en: '[14] Acs, G., Melis, L., Castelluccia, C., and De Cristofaro, E. Differentially
    Private Mixture of Generative Neural Networks. IEEE Transactions on Knowledge
    and Data Engineering 31, 6 (June 2019), 1109–1121.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Acs, G., Melis, L., Castelluccia, C., 和 De Cristofaro, E. 差分隐私生成神经网络的混合模型。IEEE知识与数据工程汇刊
    31, 6 (2019年6月), 1109–1121。'
- en: '[15] Adamczewski, K., and Park, M. Differential privacy meets neural network
    pruning. arXiv preprint arXiv:2303.04612 (2023).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Adamczewski, K., 和 Park, M. 差分隐私与神经网络剪枝相遇。arXiv预印本 arXiv:2303.04612 (2023)。'
- en: '[16] Adesuyi, T. A., and Kim, B. M. A layer-wise Perturbation based Privacy
    Preserving Deep Neural Networks. In 2019 International Conference on Artificial
    Intelligence in Information and Communication (ICAIIC) (Feb. 2019), pp. 389–394.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Adesuyi, T. A., 和 Kim, B. M. 基于逐层扰动的隐私保护深度神经网络。发表于2019年国际人工智能与信息通信大会（ICAIIC）（2019年2月），第389–394页。'
- en: '[17] Ahmed, S., Chowdhury, A. R., Fawaz, K., and Ramanathan, P. Preech: A system
    for privacy-preserving speech transcription. In Proceedings of the 29th USENIX
    Conference on Security Symposium (2020), pp. 2703–2720.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Ahmed, S., Chowdhury, A. R., Fawaz, K., 和 Ramanathan, P. Preech：一种隐私保护的语音转录系统。发表于第29届USENIX安全研讨会（2020年），第2703–2720页。'
- en: '[18] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein Generative Adversarial
    Networks. In Proceedings of the 34th International Conference on Machine Learning
    (July 2017), PMLR, pp. 214–223.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Arjovsky, M., Chintala, S., 和 Bottou, L. Wasserstein 生成对抗网络。发表于第34届国际机器学习大会（2017年7月），PMLR，第214–223页。'
- en: '[19] Backstrom, L., Dwork, C., and Kleinberg, J. Wherefore art thou r3579x?
    anonymized social networks, hidden patterns, and structural steganography. In
    Proceedings of the 16th international conference on World Wide Web (New York,
    NY, USA, May 2007), WWW ’07, Association for Computing Machinery, pp. 181–190.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Backstrom, L., Dwork, C., 和 Kleinberg, J. 你在哪里，r3579x？匿名社交网络、隐藏模式和结构隐写。发表于第16届国际万维网大会（纽约，NY，USA，2007年5月），WWW
    ’07，计算机协会，第181–190页。'
- en: '[20] Bagdasaryan, E., Poursaeed, O., and Shmatikov, V. Differential Privacy
    Has Disparate Impact on Model Accuracy. In Advances in Neural Information Processing
    Systems (2019), vol. 32, Curran Associates, Inc.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Bagdasaryan, E., Poursaeed, O., 和 Shmatikov, V. 差分隐私对模型准确性的不同影响。发表于神经信息处理系统进展（2019年），第32卷，Curran
    Associates, Inc.'
- en: '[21] Blanco-Justicia, A., Sanchez, D., Domingo-Ferrer, J., and Muralidhar,
    K. A Critical Review on the Use (and Misuse) of Differential Privacy in Machine
    Learning. ACM Computing Surveys 55, 8 (Aug. 2023), 1–16.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Blanco-Justicia, A., Sanchez, D., Domingo-Ferrer, J., 和 Muralidhar, K.
    关于差分隐私在机器学习中使用（及误用）的批判性回顾。ACM计算调查 55, 8 (2023年8月), 1–16。'
- en: '[22] Bloom, J. S., Kotenko, I., Sadhu, M. J., Treusch, S., Albert, F. W., and
    Kruglyak, L. Genetic interactions contribute less than additive effects to quantitative
    trait variation in yeast. Nature communications 6, 1 (2015), 8712.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Bloom, J. S., Kotenko, I., Sadhu, M. J., Treusch, S., Albert, F. W., 和
    Kruglyak, L. 基因互作对酵母中定量性状变异的贡献少于加性效应。自然通讯 6, 1 (2015), 8712。'
- en: '[23] Boulemtafes, A., Derhab, A., and Challal, Y. A review of privacy-preserving
    techniques for deep learning. Neurocomputing 384 (Apr. 2020), 21–45.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Boulemtafes, A., Derhab, A., 和 Challal, Y. 深度学习隐私保护技术的综述。神经计算 384 (2020年4月),
    21–45。'
- en: '[24] Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C. A., Jia, H., Travers,
    A., Zhang, B., Lie, D., and Papernot, N. Machine Unlearning. In 2021 IEEE Symposium
    on Security and Privacy (SP) (May 2021), pp. 141–159.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C. A., Jia, H., Travers,
    A., Zhang, B., Lie, D., 和 Papernot, N. 机器遗忘。发表于2021年IEEE安全与隐私研讨会（SP）（2021年5月），第141–159页。'
- en: '[25] Cangialosi, F., Agarwal, N., Arun, V., Narayana, S., Sarwate, A., and
    Netravali, R. Privid: Practical,$\{$Privacy-Preserving$\}$ video analytics queries.
    In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI
    22) (2022), pp. 209–228.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Cangialosi, F., Agarwal, N., Arun, V., Narayana, S., Sarwate, A., 和 Netravali,
    R. Privid: 实用的,$\{$隐私保护$\}$ 视频分析查询。在第19届USENIX网络系统设计与实施研讨会（NSDI 22）（2022年），第209–228页。'
- en: '[26] Cao, Y., and Yang, J. Towards Making Systems Forget with Machine Unlearning.
    In 2015 IEEE Symposium on Security and Privacy (May 2015), pp. 463–480.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Cao, Y., 和 Yang, J. 朝着通过机器遗忘实现系统遗忘。在2015年IEEE安全与隐私研讨会（2015年5月），第463–480页。'
- en: '[27] Chamikara, M. A. P., Bertok, P., Khalil, I., Liu, D., and Camtepe, S.
    Privacy Preserving Face Recognition Utilizing Differential Privacy. Computers
    & Security 97 (Oct. 2020), 101951.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Chamikara, M. A. P., Bertok, P., Khalil, I., Liu, D., 和 Camtepe, S. 利用差分隐私的隐私保护面部识别。计算机与安全
    97 (2020年10月)，101951。'
- en: '[28] Chang, S., and Li, C. Privacy in Neural Network Learning: Threats and
    Countermeasures. IEEE Network 32, 4 (July 2018), 61–67.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Chang, S., 和 Li, C. 神经网络学习中的隐私：威胁与对策。IEEE网络 32, 4 (2018年7月)，第61–67页。'
- en: '[29] Chen, C., Wu, H., Su, J., Lyu, L., Zheng, X., and Wang, L. Differential
    Private Knowledge Transfer for Privacy-Preserving Cross-Domain Recommendation.
    In Proceedings of the ACM Web Conference 2022 (New York, NY, USA, Apr. 2022),
    WWW ’22, Association for Computing Machinery, pp. 1455–1465.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Chen, C., Wu, H., Su, J., Lyu, L., Zheng, X., 和 Wang, L. 差分隐私知识转移用于隐私保护的跨域推荐。在ACM
    Web会议2022的论文集中（纽约，NY，USA，2022年4月），WWW ’22，计算机协会，第1455–1465页。'
- en: '[30] Chen, D., Orekondy, T., and Fritz, M. GS-WGAN: A Gradient-Sanitized Approach
    for Learning Differentially Private Generators. In Advances in Neural Information
    Processing Systems (2020), vol. 33, Curran Associates, Inc., pp. 12673–12684.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Chen, D., Orekondy, T., 和 Fritz, M. GS-WGAN：一种用于学习差分隐私生成器的梯度净化方法。在神经信息处理系统进展（2020年），第33卷，Curran
    Associates, Inc.，第12673–12684页。'
- en: '[31] Chen, J., Wang, W. H., and Shi, X. Differential Privacy Protection Against
    Membership Inference Attack on Machine Learning for Genomic Data. In Biocomputing
    2021. WORLD SCIENTIFIC, Oct. 2020, pp. 26–37.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Chen, J., Wang, W. H., 和 Shi, X. 针对基因组数据的机器学习中的会员推断攻击的差分隐私保护。在2021年生物计算会议。WORLD
    SCIENTIFIC, 2020年10月，第26–37页。'
- en: '[32] Chen, M., Zhang, Z., Wang, T., Backes, M., Humbert, M., and Zhang, Y.
    When Machine Unlearning Jeopardizes Privacy. In Proceedings of the 2021 ACM SIGSAC
    Conference on Computer and Communications Security (New York, NY, USA, Nov. 2021),
    CCS ’21, Association for Computing Machinery, pp. 896–911.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Chen, M., Zhang, Z., Wang, T., Backes, M., Humbert, M., 和 Zhang, Y. 当机器遗忘危及隐私时。在2021年ACM
    SIGSAC计算机与通信安全会议（纽约，NY，USA，2021年11月），CCS ’21，计算机协会，第896–911页。'
- en: '[33] Chen, S., Fu, A., Shen, J., Yu, S., Wang, H., and Sun, H. RNN-DP: A new
    differential privacy scheme base on Recurrent Neural Network for Dynamic trajectory
    privacy protection. Journal of Network and Computer Applications 168 (Oct. 2020),
    102736.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Chen, S., Fu, A., Shen, J., Yu, S., Wang, H., 和 Sun, H. RNN-DP：一种基于递归神经网络的新差分隐私方案，用于动态轨迹隐私保护。网络与计算机应用期刊
    168 (2020年10月)，102736。'
- en: '[34] Chen, X., Wu, S. Z., and Hong, M. Understanding Gradient Clipping in Private
    SGD: A Geometric Perspective. In Advances in Neural Information Processing Systems
    (2020), vol. 33, Curran Associates, Inc., pp. 13773–13782.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Chen, X., Wu, S. Z., 和 Hong, M. 从几何角度理解私有SGD中的梯度裁剪。在神经信息处理系统进展（2020年），第33卷，Curran
    Associates, Inc.，第13773–13782页。'
- en: '[35] Cummings, R., Desfontaines, D., Evans, D., Geambasu, R., Jagielski, M.,
    Huang, Y., Kairouz, P., Kamath, G., Oh, S., Ohrimenko, O., et al. Challenges towards
    the next frontier in privacy. arXiv preprint arXiv:2304.06929 (2023).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Cummings, R., Desfontaines, D., Evans, D., Geambasu, R., Jagielski, M.,
    Huang, Y., Kairouz, P., Kamath, G., Oh, S., Ohrimenko, O., 等。面向隐私的下一个前沿挑战。arXiv预印本
    arXiv:2304.06929 (2023年)。'
- en: '[36] De Cristofaro, E. A Critical Overview of Privacy in Machine Learning.
    IEEE Security & Privacy 19, 4 (July 2021), 19–27.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] De Cristofaro, E. 机器学习中的隐私批判性概述。IEEE安全与隐私 19, 4 (2021年7月)，第19–27页。'
- en: '[37] de Oliveira, A. S., Kaplan, C., Mallat, K., and Chakraborty, T. An empirical
    analysis of fairness notions under differential privacy. arXiv preprint arXiv:2302.02910
    (2023).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] de Oliveira, A. S., Kaplan, C., Mallat, K., 和 Chakraborty, T. 差分隐私下公平性概念的实证分析。arXiv预印本
    arXiv:2302.02910 (2023年)。'
- en: '[38] Desfontaines, D. A list of real-world uses of differential privacy, 2021.
    Last accessed May 31, 2023 from https://desfontain.es/privacy/real-world-differential-privacy.html.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Desfontaines, D. 差分隐私的现实世界应用列表，2021年。最后访问于2023年5月31日，网址为 https://desfontain.es/privacy/real-world-differential-privacy.html。'
- en: '[39] Desfontaines, D., and Pejó, B. Sok: differential privacies. Proceedings
    on privacy enhancing technologies 2020, 2 (2020), 288–313.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Desfontaines, D., 和 Pejó, B. Sok：差分隐私。隐私增强技术会议记录2020，2（2020），第288–313页。'
- en: '[40] Ding, J., Errapotu, S. M., Guo, Y., Zhang, H., Yuan, D., and Pan, M. Private
    Empirical Risk Minimization With Analytic Gaussian Mechanism for Healthcare System.
    IEEE Transactions on Big Data 8, 4 (Aug. 2022), 1107–1117.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Ding, J., Errapotu, S. M., Guo, Y., Zhang, H., Yuan, D., 和 Pan, M. 使用分析高斯机制的医疗系统私有经验风险最小化。IEEE大数据学报8，4（2022年8月），第1107–1117页。'
- en: '[41] Du, M., Jia, R., and Song, D. Robust anomaly detection and backdoor attack
    detection via differential privacy. arXiv preprint arXiv:1911.07116 (2019).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Du, M., Jia, R., 和 Song, D. 通过差分隐私进行鲁棒异常检测和后门攻击检测。arXiv预印本arXiv:1911.07116（2019）。'
- en: '[42] Dwork, C., and Roth, A. The Algorithmic Foundations of Differential Privacy.
    Foundations and Trends® in Theoretical Computer Science 9, 3–4 (Aug. 2014), 211–407.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Dwork, C., 和 Roth, A. 差分隐私的算法基础。理论计算机科学基础与趋势® 9，3–4（2014年8月），第211–407页。'
- en: '[43] Dwork, C., and Rothblum, G. N. Concentrated differential privacy. arXiv
    preprint arXiv:1603.01887 (2016).'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Dwork, C., 和 Rothblum, G. N. 集中差分隐私。arXiv预印本arXiv:1603.01887（2016）。'
- en: '[44] Farrand, T., Mireshghallah, F., Singh, S., and Trask, A. Neither Private
    Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy.
    In Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in
    Practice (New York, NY, USA, Nov. 2020), PPMLP’20, Association for Computing Machinery,
    pp. 15–19.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Farrand, T., Mireshghallah, F., Singh, S., 和 Trask, A. 既不私密也不公平：数据不平衡对差分隐私中效用和公平性的影响。见于2020年隐私保护机器学习实践研讨会（纽约，NY，美国，2020年11月），PPMLP’20，计算机协会，第15–19页。'
- en: '[45] Fernandes, N., Dras, M., and McIver, A. Generalised differential privacy
    for text document processing. In Principles of Security and Trust: 8th International
    Conference, POST 2019, Held as Part of the European Joint Conferences on Theory
    and Practice of Software, ETAPS 2019, Prague, Czech Republic, April 6–11, 2019,
    Proceedings 8 (2019), Springer International Publishing, pp. 123–148.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Fernandes, N., Dras, M., 和 McIver, A. 文本文档处理中的广义差分隐私。见于安全与信任原则：第八届国际会议POST
    2019，作为欧洲联合会议ETAPS 2019的一部分，捷克共和国布拉格，2019年4月6–11日，论文集8（2019），Springer国际出版，第123–148页。'
- en: '[46] Feyisetan, O., Diethe, T., and Drake, T. Leveraging Hierarchical Representations
    for Preserving Privacy and Utility in Text. In 2019 IEEE International Conference
    on Data Mining (ICDM) (Nov. 2019), pp. 210–219.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Feyisetan, O., Diethe, T., 和 Drake, T. 利用层次表示来保护文本的隐私和效用。见于2019年IEEE数据挖掘国际会议（ICDM）（2019年11月），第210–219页。'
- en: '[47] Frankle, J., and Carbin, M. The lottery ticket hypothesis: Finding sparse,
    trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Frankle, J., 和 Carbin, M. 彩票票假说：寻找稀疏的、可训练的神经网络。arXiv预印本arXiv:1803.03635（2018）。'
- en: '[48] Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Linear mode connectivity
    and the lottery ticket hypothesis. In International Conference on Machine Learning
    (2020), PMLR, pp. 3259–3269.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Frankle, J., Dziugaite, G. K., Roy, D., 和 Carbin, M. 线性模式连接性和彩票票假说。见于2020年国际机器学习会议（2020），PMLR，第3259–3269页。'
- en: '[49] Ganta, S. R., Kasiviswanathan, S. P., and Smith, A. Composition attacks
    and auxiliary information in data privacy. In Proceedings of the 14th ACM SIGKDD
    international conference on Knowledge discovery and data mining (New York, NY,
    USA, Aug. 2008), KDD ’08, Association for Computing Machinery, pp. 265–273.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Ganta, S. R., Kasiviswanathan, S. P., 和 Smith, A. 数据隐私中的组合攻击和辅助信息。见于第14届ACM
    SIGKDD国际知识发现与数据挖掘会议（纽约，NY，美国，2008年8月），KDD ’08，计算机协会，第265–273页。'
- en: '[50] Giorgi, G., Abbasi, W., and Saracino, A. Privacy-Preserving Analysis for
    Remote Video Anomaly Detection in Real Life Environments. Journal of Wireless
    Mobile Networks, Ubiquitous Computing, and Dependable Applications 13, 1 (Mar.
    2022), 112–136.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Giorgi, G., Abbasi, W., 和 Saracino, A. 现实环境中远程视频异常检测的隐私保护分析。无线移动网络、无处不在的计算和可靠应用学报13，1（2022年3月），第112–136页。'
- en: '[51] Gondara, L., Carvalho, R. S., and Wang, K. Training Differentially Private
    Neural Networks with Lottery Tickets. In Computer Security – ESORICS 2021 (Cham,
    2021), E. Bertino, H. Shulman, and M. Waidner, Eds., Lecture Notes in Computer
    Science, Springer International Publishing, pp. 543–562.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Gondara, L., Carvalho, R. S., 和 Wang, K. 使用彩票票训练差分隐私神经网络。见于计算机安全 – ESORICS
    2021（Cham，2021），E. Bertino, H. Shulman 和 M. Waidner 编，计算机科学讲义，Springer国际出版，第543–562页。'
- en: '[52] Gong, M., Pan, K., Xie, Y., Qin, A. K., and Tang, Z. Preserving differential
    privacy in deep neural networks with relevance-based adaptive noise imposition.
    Neural Networks 125 (May 2020), 131–141.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Gong, M., Pan, K., Xie, Y., Qin, A. K., 和 Tang, Z. 在深度神经网络中保持差分隐私的相关性自适应噪声施加。神经网络125卷（2020年5月），131–141页。'
- en: '[53] Gong, M., Xie, Y., Pan, K., Feng, K., and Qin, A. A Survey on Differentially
    Private Machine Learning [Review Article]. IEEE Computational Intelligence Magazine
    15, 2 (May 2020), 49–64.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Gong, M., Xie, Y., Pan, K., Feng, K., 和 Qin, A. 关于差分隐私机器学习的调查[综述文章]。IEEE计算智能杂志15卷，2期（2020年5月），49–64页。'
- en: '[54] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C.
    Improved Training of Wasserstein GANs. In Advances in Neural Information Processing
    Systems (2017), vol. 30, Curran Associates, Inc.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., 和 Courville, A.
    C. 改进Wasserstein GANs的训练。发表于神经信息处理系统进展（2017年），第30卷，Curran Associates, Inc.'
- en: '[55] Ha, T., Dang, T. K., Dang, T. T., Truong, T. A., and Nguyen, M. T. Differential
    Privacy in Deep Learning: An Overview. In 2019 International Conference on Advanced
    Computing and Applications (ACOMP) (Nov. 2019), pp. 97–102.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Ha, T., Dang, T. K., Dang, T. T., Truong, T. A., 和 Nguyen, M. T. 深度学习中的差分隐私：概述。发表于2019年国际先进计算与应用会议（ACOMP）（2019年11月），第97–102页。'
- en: '[56] Ha, T., Dang, T. K., Le, H., and Truong, T. A. Security and Privacy Issues
    in Deep Learning: A Brief Review. SN Computer Science 1, 5 (Aug. 2020), 253.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Ha, T., Dang, T. K., Le, H., 和 Truong, T. A. 深度学习中的安全性和隐私问题：简要回顾。SN计算机科学1卷，5期（2020年8月），253页。'
- en: '[57] Harder, F., Bauer, M., and Park, M. Interpretable and Differentially Private
    Predictions. Proceedings of the AAAI Conference on Artificial Intelligence 34,
    04 (Apr. 2020), 4083–4090.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Harder, F., Bauer, M., 和 Park, M. 可解释和差分隐私预测。AAAI人工智能会议论文集34卷，04期（2020年4月），4083–4090页。'
- en: '[58] Jagielski, M., Ullman, J., and Oprea, A. Auditing Differentially Private
    Machine Learning: How Private is Private SGD? In Advances in Neural Information
    Processing Systems (2020), vol. 33, Curran Associates, Inc., pp. 22205–22216.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Jagielski, M., Ullman, J., 和 Oprea, A. 审计差分隐私机器学习：私有SGD有多私密？发表于神经信息处理系统进展（2020年），第33卷，Curran
    Associates, Inc.，第22205–22216页。'
- en: '[59] Jayaraman, B., and Evans, D. Evaluating differentially private machine
    learning in practice. In USENIX Security Symposium (2019).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Jayaraman, B., 和 Evans, D. 实践中评估差分隐私机器学习。在USENIX安全研讨会（2019年）上。'
- en: '[60] Jiang, H., Wang, M., Zhao, P., Xiao, Z., and Dustdar, S. A Utility-Aware
    General Framework With Quantifiable Privacy Preservation for Destination Prediction
    in LBSs. IEEE/ACM Transactions on Networking 29, 5 (Oct. 2021), 2228–2241.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Jiang, H., Wang, M., Zhao, P., Xiao, Z., 和 Dustdar, S. 一种关注效用的通用框架，用于LBS的目的地预测的可量化隐私保护。IEEE/ACM网络交易29卷，5期（2021年10月），2228–2241页。'
- en: '[61] Jordon, J., Yoon, J., and Van Der Schaar, M. Pate-gan: Generating synthetic
    data with differential privacy guarantees. In International conference on learning
    representations (2019).'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Jordon, J., Yoon, J., 和 Van Der Schaar, M. Pate-gan：生成具有差分隐私保证的合成数据。发表于国际学习表征会议（2019年）。'
- en: '[62] Kifer, D., Messing, S., Roth, A., Thakurta, A., and Zhang, D. Guidelines
    for implementing and auditing differentially private systems. arXiv preprint arXiv:2002.04049
    (2020).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Kifer, D., Messing, S., Roth, A., Thakurta, A., 和 Zhang, D. 实施和审计差分隐私系统的指南。arXiv预印本arXiv:2002.04049（2020年）。'
- en: '[63] Krizhevsky, A. Cifar-10 and cifar-100 datasets. Last accessed May 31,
    2023 from [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html).'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Krizhevsky, A. Cifar-10和cifar-100数据集。最后访问于2023年5月31日，网址为[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)。'
- en: '[64] LeCun, Y. Mnist. Last accessed May 31, 2023 from [yann.lecun.com/exdb/mnist/](yann.lecun.com/exdb/mnist/).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] LeCun, Y. Mnist。最后访问于2023年5月31日，网址为[yann.lecun.com/exdb/mnist/](yann.lecun.com/exdb/mnist/)。'
- en: '[65] Lecuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., and Jana, S. Certified
    Robustness to Adversarial Examples with Differential Privacy. In 2019 IEEE Symposium
    on Security and Privacy (SP) (May 2019), pp. 656–672.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Lecuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., 和 Jana, S. 具有差分隐私的对抗样本认证鲁棒性。发表于2019年IEEE安全与隐私研讨会（SP）（2019年5月），第656–672页。'
- en: '[66] Leino, K., and Fredrikson, M. Stolen memories: Leveraging model memorization
    for calibrated white-box membership inference. In 29th USENIX Security Symposium
    (2020).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Leino, K., 和 Fredrikson, M. 被盗记忆：利用模型记忆进行校准的白盒成员推断。发表于第29届USENIX安全研讨会（2020年）。'
- en: '[67] Li, N., Li, T., and Venkatasubramanian, S. t-Closeness: Privacy Beyond
    k-Anonymity and l-Diversity. In 2007 IEEE 23rd International Conference on Data
    Engineering (Apr. 2007), pp. 106–115.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Li, N., Li, T., and Venkatasubramanian, S. t-接近性：超越k-匿名性和l-多样性。在2007年IEEE第23届国际数据工程会议（2007年4月），页码106–115。'
- en: '[68] Li, X., Tramer, F., Liang, P., and Hashimoto, T. Large language models
    can be strong differentially private learners. arXiv preprint arXiv:2110.05679
    (2021).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Li, X., Tramer, F., Liang, P., and Hashimoto, T. 大型语言模型可以是强大的差分隐私学习者。arXiv预印本
    arXiv:2110.05679 (2021)。'
- en: '[69] Li, Y., Wang, Y., and Li, D. Privacy-preserving lightweight face recognition.
    Neurocomputing 363 (Oct. 2019), 212–222.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Li, Y., Wang, Y., and Li, D. 隐私保护的轻量级人脸识别。Neurocomputing 363 (2019年10月),
    212–222。'
- en: '[70] Liu, B., Ding, M., Shaham, S., Rahayu, W., Farokhi, F., and Lin, Z. When
    Machine Learning Meets Privacy: A Survey and Outlook. ACM Computing Surveys 54,
    2 (Mar. 2021), 31:1–31:36.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Liu, B., Ding, M., Shaham, S., Rahayu, W., Farokhi, F., and Lin, Z. 当机器学习遇上隐私：调查与展望。ACM计算调查
    54, 2 (2021年3月), 31:1–31:36。'
- en: '[71] Liu, J., and Talwar, K. Private selection from private candidates. In
    Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (New
    York, NY, USA, June 2019), STOC 2019, Association for Computing Machinery, pp. 298–309.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Liu, J., and Talwar, K. 从私人候选人中选择。发表于第51届年度ACM SIGACT计算理论研讨会（美国纽约，2019年6月），STOC
    2019，计算机机械协会，页码298–309。'
- en: '[72] Liu, Y., Peng, J., Yu, J. J., and Wu, Y. PPGAN: Privacy-Preserving Generative
    Adversarial Network. In 2019 IEEE 25th International Conference on Parallel and
    Distributed Systems (ICPADS) (Dec. 2019), pp. 985–989.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Liu, Y., Peng, J., Yu, J. J., and Wu, Y. PPGAN：隐私保护生成对抗网络。在2019年IEEE第25届国际并行与分布式系统会议（ICPADS）（2019年12月），页码985–989。'
- en: '[73] Lyu, L., Li, Y., He, X., and Xiao, T. Towards Differentially Private Text
    Representations. In Proceedings of the 43rd International ACM SIGIR Conference
    on Research and Development in Information Retrieval (New York, NY, USA, July
    2020), SIGIR ’20, Association for Computing Machinery, pp. 1813–1816.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Lyu, L., Li, Y., He, X., and Xiao, T. 朝向差分隐私文本表示。发表于第43届国际ACM SIGIR信息检索研究与开发会议（美国纽约，2020年7月），SIGIR
    ’20，计算机机械协会，页码1813–1816。'
- en: '[74] Machanavajjhala, A., Kifer, D., Gehrke, J., and Venkitasubramaniam, M.
    L-diversity: Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery
    from Data 1, 1 (Mar. 2007), 3–es.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Machanavajjhala, A., Kifer, D., Gehrke, J., and Venkitasubramaniam, M.
    L-多样性：超越k-匿名性。ACM知识发现数据的交易 1, 1 (2007年3月), 3–es。'
- en: '[75] Makhlouf, K., Zhioua, S., and Palamidessi, C. Survey on causal-based machine
    learning fairness notions. arXiv preprint arXiv:2010.09553 (2020).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Makhlouf, K., Zhioua, S., and Palamidessi, C. 基于因果的机器学习公平性概念调查。arXiv预印本
    arXiv:2010.09553 (2020)。'
- en: '[76] Mireshghallah, F., Taram, M., Vepakomma, P., Singh, A., Raskar, R., and
    Esmaeilzadeh, H. Privacy in Deep Learning: A Survey, Nov. 2020.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Mireshghallah, F., Taram, M., Vepakomma, P., Singh, A., Raskar, R., and
    Esmaeilzadeh, H. 深度学习中的隐私：调查，2020年11月。'
- en: '[77] Mironov, I. Rényi Differential Privacy. In 2017 IEEE 30th Computer Security
    Foundations Symposium (CSF) (Aug. 2017), pp. 263–275.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Mironov, I. Rényi差分隐私。在2017年IEEE第30届计算机安全基础研讨会（CSF）（2017年8月），页码263–275。'
- en: '[78] Montavon, G., Binder, A., Lapuschkin, S., Samek, W., and Müller, K.-R.
    Layer-Wise Relevance Propagation: An Overview. In Explainable AI: Interpreting,
    Explaining and Visualizing Deep Learning, W. Samek, G. Montavon, A. Vedaldi, L. K.
    Hansen, and K.-R. Müller, Eds., Lecture Notes in Computer Science. Springer International
    Publishing, Cham, 2019, pp. 193–209.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Montavon, G., Binder, A., Lapuschkin, S., Samek, W., and Müller, K.-R.
    层次相关传播：概述。在《可解释的AI：解释、说明和可视化深度学习》中，W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen,
    和 K.-R. Müller, 编，计算机科学讲义系列。Springer国际出版公司，Cham, 2019年，页码193–209。'
- en: '[79] Moraffah, R., Karami, M., Guo, R., Raglin, A., and Liu, H. Causal interpretability
    for machine learning-problems, methods and evaluation. ACM SIGKDD Explorations
    Newsletter 22, 1 (2020), 18–33.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Moraffah, R., Karami, M., Guo, R., Raglin, A., and Liu, H. 机器学习的因果可解释性：问题、方法和评估。ACM
    SIGKDD探索通讯 22, 1 (2020), 18–33。'
- en: '[80] Müftüoğlu, Z., Kizrak, M. A., and Yildlnm, T. Differential Privacy Practice
    on Diagnosis of COVID-19 Radiology Imaging Using EfficientNet. In 2020 International
    Conference on INnovations in Intelligent SysTems and Applications (INISTA) (Aug.
    2020), pp. 1–6.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Müftüoğlu, Z., Kizrak, M. A., and Yildlnm, T. 利用EfficientNet在COVID-19放射影像诊断中的差分隐私实践。在2020年智能系统与应用创新国际会议（INISTA）（2020年8月），页码1–6。'
- en: '[81] Narayanan, A., and Shmatikov, V. Robust De-anonymization of Large Sparse
    Datasets. In 2008 IEEE Symposium on Security and Privacy (sp 2008) (May 2008),
    pp. 111–125.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Narayanan, A., 和 Shmatikov, V. 大规模稀疏数据集的鲁棒去匿名化。发表于2008年IEEE安全与隐私研讨会（sp
    2008）（2008年5月），第111–125页。'
- en: '[82] Nasr, M., Songi, S., Thakurta, A., Papernot, N., and Carlin, N. Adversary
    Instantiation: Lower Bounds for Differentially Private Machine Learning. In 2021
    IEEE Symposium on Security and Privacy (SP) (May 2021), pp. 866–882.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Nasr, M., Songi, S., Thakurta, A., Papernot, N., 和 Carlin, N. 对抗者实例化：差分隐私机器学习的下界。发表于2021年IEEE安全与隐私研讨会（SP）（2021年5月），第866–882页。'
- en: '[83] Nicolae, M.-I., Sinn, M., Tran, M. N., Buesser, B., Rawat, A., Wistuba,
    M., Zantedeschi, V., Baracaldo, N., Chen, B., Ludwig, H., Molloy, I., and Edwards,
    B. Adversarial robustness toolbox v1.2.0. CoRR 1807.01069 (2018).'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Nicolae, M.-I., Sinn, M., Tran, M. N., Buesser, B., Rawat, A., Wistuba,
    M., Zantedeschi, V., Baracaldo, N., Chen, B., Ludwig, H., Molloy, I., 和 Edwards,
    B. 对抗鲁棒性工具箱 v1.2.0。CoRR 1807.01069（2018年）。'
- en: '[84] Ouadrhiri, A. E., and Abdelhadi, A. Differential Privacy for Deep and
    Federated Learning: A Survey. IEEE Access 10 (2022), 22359–22380.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Ouadrhiri, A. E., 和 Abdelhadi, A. 深度学习和联邦学习中的差分隐私：综述。IEEE Access 10（2022年），22359–22380。'
- en: '[85] Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., and Talwar, K.
    Semi-supervised knowledge transfer for deep learning from private training data.
    arXiv preprint arXiv:1610.05755 (2016).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., 和 Talwar, K.
    从私有训练数据中进行半监督知识迁移的深度学习。arXiv预印本 arXiv:1610.05755（2016年）。'
- en: '[86] Papernot, N., Thakurta, A., Song, S., Chien, S., and Erlingsson, U. Tempered
    Sigmoid Activations for Deep Learning with Differential Privacy, July 2020.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Papernot, N., Thakurta, A., Song, S., Chien, S., 和 Erlingsson, U. 温和的Sigmoid激活函数用于差分隐私的深度学习，2020年7月。'
- en: '[87] Park, C., Hong, D., and Seo, C. An Attack-Based Evaluation Method for
    Differentially Private Learning Against Model Inversion Attack. IEEE Access 7
    (2019), 124988–124999.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Park, C., Hong, D., 和 Seo, C. 针对模型反演攻击的差分隐私学习的攻击基评估方法。IEEE Access 7（2019年），124988–124999。'
- en: '[88] Park, M., Foulds, J., Choudhary, K., and Welling, M. DP-EM: Differentially
    Private Expectation Maximization. In Proceedings of the 20th International Conference
    on Artificial Intelligence and Statistics (Apr. 2017), PMLR, pp. 896–904.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Park, M., Foulds, J., Choudhary, K., 和 Welling, M. DP-EM：差分隐私期望最大化。发表于第20届国际人工智能与统计会议（2017年4月），PMLR，第896–904页。'
- en: '[89] Pearl, J., and Mackenzie, D. The book of why: the new science of cause
    and effect. Basic books, 2018.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Pearl, J., 和 Mackenzie, D. 为什么的书：因果关系的新科学。基本书籍，2018年。'
- en: '[90] Phan, N., Vu, M., Liu, Y., Jin, R., Dou, D., Wu, X., and Thai, M. T. Heterogeneous
    gaussian mechanism: Preserving differential privacy in deep learning with provable
    robustness. arXiv preprint arXiv:1906.01444 (2019).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Phan, N., Vu, M., Liu, Y., Jin, R., Dou, D., Wu, X., 和 Thai, M. T. 异质高斯机制：在深度学习中保持差分隐私的可证明鲁棒性。arXiv预印本
    arXiv:1906.01444（2019年）。'
- en: '[91] Phan, N., Wu, X., Hu, H., and Dou, D. Adaptive laplace mechanism: Differential
    privacy preservation in deep learning. In 2017 IEEE international conference on
    data mining (ICDM) (2017), IEEE, pp. 385–394.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Phan, N., Wu, X., Hu, H., 和 Dou, D. 自适应拉普拉斯机制：深度学习中的差分隐私保护。发表于2017年IEEE国际数据挖掘会议（ICDM）（2017年），IEEE，第385–394页。'
- en: '[92] Phong, L. T., and Phuong, T. T. Differentially private stochastic gradient
    descent via compression and memorization. Journal of Systems Architecture 135
    (Feb. 2023), 102819.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Phong, L. T., 和 Phuong, T. T. 通过压缩和记忆进行差分隐私随机梯度下降。系统架构杂志 135（2023年2月），102819。'
- en: '[93] Rechberger, C., and Walch, R. Privacy-Preserving Machine Learning Using
    Cryptography. In Security and Artificial Intelligence: A Crossdisciplinary Approach,
    L. Batina, T. Bäck, I. Buhan, and S. Picek, Eds., Lecture Notes in Computer Science.
    Springer International Publishing, Cham, 2022, pp. 109–129.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Rechberger, C., 和 Walch, R. 使用密码学进行隐私保护机器学习。发表于《安全与人工智能：跨学科方法》，L. Batina,
    T. Bäck, I. Buhan, 和 S. Picek 编，计算机科学讲义系列。施普林格国际出版，Cham，2022年，第109–129页。'
- en: '[94] Rigaki, M., and Garcia, S. A survey of privacy attacks in machine learning.
    arXiv preprint arXiv:2007.07646 (2020).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Rigaki, M., 和 Garcia, S. 机器学习中的隐私攻击调查。arXiv预印本 arXiv:2007.07646（2020年）。'
- en: '[95] Shafee, A., and Awaad, T. A. Privacy attacks against deep learning models
    and their countermeasures. Journal of Systems Architecture 114 (Mar. 2021), 101940.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Shafee, A., 和 Awaad, T. A. 针对深度学习模型的隐私攻击及其对策。系统架构杂志 114（2021年3月），101940。'
- en: '[96] Shen, Z., and Zhong, T. Analysis of Application Examples of Differential
    Privacy in Deep Learning. Computational Intelligence and Neuroscience 2021 (Oct.
    2021), e4244040.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Shen, Z. 和 Zhong, T. 深度学习中差分隐私应用示例分析。《计算智能与神经科学》2021 (2021年10月)，e4244040。'
- en: '[97] Shokri, R., Stronati, M., Song, C., and Shmatikov, V. Membership Inference
    Attacks Against Machine Learning Models. In 2017 IEEE Symposium on Security and
    Privacy (SP) (May 2017), pp. 3–18.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Shokri, R.、Stronati, M.、Song, C. 和 Shmatikov, V. 针对机器学习模型的成员推断攻击。发表于 2017
    IEEE 安全与隐私研讨会 (SP) (2017年5月)，第3–18页。'
- en: '[98] Stadler, T., Oprisanu, B., and Troncoso, C. Synthetic data–anonymisation
    groundhog day. In 31st USENIX Security Symposium (USENIX Security 22) (2022),
    pp. 1451–1468.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Stadler, T.、Oprisanu, B. 和 Troncoso, C. 合成数据——匿名化的“土拨鼠日”。发表于第31届 USENIX
    安全研讨会 (USENIX Security 22) (2022)，第1451–1468页。'
- en: '[99] Sweeney, L. k-anonymity: A model for protecting privacy. International
    journal of uncertainty, fuzziness and knowledge-based systems 10, 05 (2002), 557–570.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Sweeney, L. k-匿名性：一种保护隐私的模型。《不确定性、模糊性与知识系统国际期刊》10, 05 (2002)，557–570。'
- en: '[100] Tanuwidjaja, H. C., Choi, R., and Kim, K. A Survey on Deep Learning Techniques
    for Privacy-Preserving. In Machine Learning for Cyber Security (Cham, 2019), X. Chen,
    X. Huang, and J. Zhang, Eds., Lecture Notes in Computer Science, Springer International
    Publishing, pp. 29–46.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Tanuwidjaja, H. C.、Choi, R. 和 Kim, K. 关于隐私保护的深度学习技术的调查。发表于《网络安全的机器学习》
    (Cham, 2019)，X. Chen、X. Huang 和 J. Zhang 编，计算机科学讲义笔记，Springer 国际出版，第29–46页。'
- en: '[101] Torfi, A., Fox, E. A., and Reddy, C. K. Differentially private synthetic
    medical data generation using convolutional GANs. Information Sciences 586 (Mar.
    2022), 485–500.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Torfi, A.、Fox, E. A. 和 Reddy, C. K. 使用卷积 GANs 生成差分隐私合成医学数据。《信息科学》586
    (2022年3月)，485–500。'
- en: '[102] Tramer, F., and Boneh, D. Differentially private learning needs better
    features (or much more data). arXiv preprint arXiv:2011.11660 (2020).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Tramer, F. 和 Boneh, D. 差分隐私学习需要更好的特征（或更多的数据）。arXiv 预印本 arXiv:2011.11660
    (2020)。'
- en: '[103] Tramèr, F., Kamath, G., and Carlini, N. Considerations for differentially
    private learning with large-scale public pretraining. arXiv preprint arXiv:2212.06470
    (2022).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Tramèr, F.、Kamath, G. 和 Carlini, N. 大规模公共预训练下的差分隐私学习考虑因素。arXiv 预印本 arXiv:2212.06470
    (2022)。'
- en: '[104] Tramer, F., Terzis, A., Steinke, T., Song, S., Jagielski, M., and Carlini,
    N. Debugging differential privacy: A case study for privacy auditing. arXiv preprint
    arXiv:2202.12219 (2022).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Tramer, F.、Terzis, A.、Steinke, T.、Song, S.、Jagielski, M. 和 Carlini, N.
    调试差分隐私：隐私审计案例研究。arXiv 预印本 arXiv:2202.12219 (2022)。'
- en: '[105] Ustundag Soykan, E., Bilgin, Z., Ersoy, M. A., and Tomur, E. Differentially
    Private Deep Learning for Load Forecasting on Smart Grid. In 2019 IEEE Globecom
    Workshops (GC Wkshps) (Dec. 2019), pp. 1–6.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Ustundag Soykan, E.、Bilgin, Z.、Ersoy, M. A. 和 Tomur, E. 针对智能电网负荷预测的差分隐私深度学习。发表于
    2019 IEEE Globecom 工作坊 (GC Wkshps) (2019年12月)，第1–6页。'
- en: '[106] Vasa, J., and Thakkar, A. Deep Learning: Differential Privacy Preservation
    in the Era of Big Data. Journal of Computer Information Systems 0, 0 (July 2022),
    1–24.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Vasa, J. 和 Thakkar, A. 深度学习：大数据时代的差分隐私保护。《计算机信息系统杂志》0, 0 (2022年7月)，1–24。'
- en: '[107] Villaronga, E. F., Kieseberg, P., and Li, T. Humans forget, machines
    remember: Artificial intelligence and the Right to Be Forgotten. Computer Law
    & Security Review 34, 2 (Apr. 2018), 304–313.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Villaronga, E. F.、Kieseberg, P. 和 Li, T. 人类忘记，机器记忆：人工智能与被遗忘权。《计算机法律与安全评论》34,
    2 (2018年4月)，304–313。'
- en: '[108] Wang, J., Bao, W., Sun, L., Zhu, X., Cao, B., and Yu, P. S. Private Model
    Compression via Knowledge Distillation. Proceedings of the AAAI Conference on
    Artificial Intelligence 33, 01 (July 2019), 1190–1197.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Wang, J.、Bao, W.、Sun, L.、Zhu, X.、Cao, B. 和 Yu, P. S. 通过知识蒸馏进行隐私模型压缩。《美国人工智能会议论文集》33,
    01 (2019年7月)，1190–1197。'
- en: '[109] Wang, Y., Gu, M., Ma, J., and Jin, Q. DNN-DP: Differential Privacy Enabled
    Deep Neural Network Learning Framework for Sensitive Crowdsourcing Data. IEEE
    Transactions on Computational Social Systems 7, 1 (Feb. 2020), 215–224.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Wang, Y.、Gu, M.、Ma, J. 和 Jin, Q. DNN-DP：为敏感众包数据提供差分隐私的深度神经网络学习框架。IEEE
    计算社会系统交易 7, 1 (2020年2月)，215–224。'
- en: '[110] Wen, Y., Liu, B., Ding, M., Xie, R., and Song, L. IdentityDP: Differential
    private identification protection for face images. Neurocomputing 501 (Aug. 2022),
    197–211.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Wen, Y.、Liu, B.、Ding, M.、Xie, R. 和 Song, L. IdentityDP：面部图像的差分隐私识别保护。神经计算
    501 (2022年8月)，197–211。'
- en: '[111] Wu, B., Zhao, S., Sun, G., Zhang, X., Su, Z., Zeng, C., and Liu, Z. P3sgd:
    Patient privacy preserving sgd for regularizing deep cnns in pathological image
    classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (2019), pp. 2099–2108.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Wu, B., Zhao, S., Sun, G., Zhang, X., Su, Z., Zeng, C., 和 Liu, Z. P3sgd：用于病理图像分类的隐私保护SGD。在IEEE/CVF计算机视觉与模式识别会议论文集（2019年），pp.
    2099–2108。'
- en: '[112] Xiang, L., Yang, J., and Li, B. Differentially-Private Deep Learning
    from an optimization Perspective. In IEEE INFOCOM 2019 - IEEE Conference on Computer
    Communications (Apr. 2019), pp. 559–567.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Xiang, L., Yang, J., 和 Li, B. 从优化角度探讨差分隐私深度学习。在2019年IEEE计算机通信大会（IEEE
    INFOCOM 2019）（2019年4月），pp. 559–567。'
- en: '[113] Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset
    for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747
    (2017).'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Xiao, H., Rasul, K., 和 Vollgraf, R. Fashion-mnist：一个用于基准测试机器学习算法的新图像数据集。arXiv预印本arXiv:1708.07747（2017年）。'
- en: '[114] Xie, L., Lin, K., Wang, S., Wang, F., and Zhou, J. Differentially private
    generative adversarial network. arXiv preprint arXiv:1802.06739 (2018).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Xie, L., Lin, K., Wang, S., Wang, F., 和 Zhou, J. 差分隐私生成对抗网络。arXiv预印本arXiv:1802.06739（2018年）。'
- en: '[115] Xu, C., Ren, J., Zhang, D., Zhang, Y., Qin, Z., and Ren, K. GANobfuscator:
    Mitigating Information Leakage Under GAN via Differential Privacy. IEEE Transactions
    on Information Forensics and Security 14, 9 (Sept. 2019), 2358–2371.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Xu, C., Ren, J., Zhang, D., Zhang, Y., Qin, Z., 和 Ren, K. GANobfuscator：通过差分隐私减轻GAN下的信息泄露。《IEEE信息取证与安全汇刊》14,
    9（2019年9月），2358–2371。'
- en: '[116] Xu, Z., Shi, S., Liu, A. X., Zhao, J., and Chen, L. An Adaptive and Fast
    Convergent Approach to Differentially Private Deep Learning. In IEEE INFOCOM 2020
    - IEEE Conference on Computer Communications (July 2020), pp. 1867–1876.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Xu, Z., Shi, S., Liu, A. X., Zhao, J., 和 Chen, L. 一种自适应且快速收敛的差分隐私深度学习方法。在2020年IEEE计算机通信大会（IEEE
    INFOCOM 2020）（2020年7月），pp. 1867–1876。'
- en: '[117] Xue, M., Yuan, C., Wu, H., Zhang, Y., and Liu, W. Machine Learning Security:
    Threats, Countermeasures, and Evaluations. IEEE Access 8 (2020), 74720–74742.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Xue, M., Yuan, C., Wu, H., Zhang, Y., 和 Liu, W. 机器学习安全：威胁、对策和评估。《IEEE
    Access》8（2020年），74720–74742。'
- en: '[118] Yan, H., Li, X., Li, H., Li, J., Sun, W., and Li, F. Monitoring-Based
    Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack.
    IEEE Transactions on Dependable and Secure Computing 19, 4 (July 2022), 2680–2694.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Yan, H., Li, X., Li, H., Li, J., Sun, W., 和 Li, F. 基于监控的差分隐私机制应对基于查询泛洪的模型提取攻击。《IEEE可靠与安全计算汇刊》19,
    4（2022年7月），2680–2694。'
- en: '[119] Yang, L., Song, Y., Gao, S., Hu, A., and Xiao, B. Griffin: Real-Time
    Network Intrusion Detection System via Ensemble of Autoencoder in SDN. IEEE Transactions
    on Network and Service Management 19, 3 (Sept. 2022), 2269–2281.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Yang, L., Song, Y., Gao, S., Hu, A., 和 Xiao, B. Griffin：通过SDN中自编码器集成的实时网络入侵检测系统。《IEEE网络与服务管理汇刊》19,
    3（2022年9月），2269–2281。'
- en: '[120] Yao, Y. Exploration of Membership Inference Attack on Convolutional Neural
    Networks and Its Defenses. In 2022 International Conference on Image Processing,
    Computer Vision and Machine Learning (ICICML) (Oct. 2022), pp. 604–610.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Yao, Y. 对卷积神经网络的成员推断攻击及其防御的探索。在2022年国际图像处理、计算机视觉与机器学习会议（ICICML）（2022年10月），pp.
    604–610。'
- en: '[121] Ye, D., Shen, S., Zhu, T., Liu, B., and Zhou, W. One Parameter Defense—Defending
    Against Data Inference Attacks via Differential Privacy. IEEE Transactions on
    Information Forensics and Security 17 (2022), 1466–1480.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Ye, D., Shen, S., Zhu, T., Liu, B., 和 Zhou, W. 单参数防御——通过差分隐私防御数据推断攻击。《IEEE信息取证与安全汇刊》17（2022年），1466–1480。'
- en: '[122] Yeom, S., Giacomelli, I., Fredrikson, M., and Jha, S. Privacy Risk in
    Machine Learning: Analyzing the Connection to Overfitting. In 2018 IEEE 31st Computer
    Security Foundations Symposium (CSF) (July 2018), pp. 268–282.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Yeom, S., Giacomelli, I., Fredrikson, M., 和 Jha, S. 机器学习中的隐私风险：分析与过拟合的联系。在2018年IEEE第31届计算机安全基础研讨会（CSF）（2018年7月），pp.
    268–282。'
- en: '[123] Yu, J., Xue, H., Liu, B., Wang, Y., Zhu, S., and Ding, M. GAN-Based Differential
    Private Image Privacy Protection Framework for the Internet of Multimedia Things.
    Sensors 21, 1 (Jan. 2021), 58.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Yu, J., Xue, H., Liu, B., Wang, Y., Zhu, S., 和 Ding, M. 基于GAN的差分隐私图像隐私保护框架用于多媒体物联网。《传感器》21,
    1（2021年1月），58。'
- en: '[124] Yu, L., Liu, L., Pu, C., Gursoy, M. E., and Truex, S. Differentially
    Private Model Publishing for Deep Learning. In 2019 IEEE Symposium on Security
    and Privacy (SP) (May 2019), pp. 332–349.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Yu, L., Liu, L., Pu, C., Gursoy, M. E., 和 Truex, S. 深度学习的差分隐私模型发布。在2019年IEEE安全与隐私研讨会（SP）（2019年5月），pp.
    332–349。'
- en: '[125] Zhang, J., He, T., Sra, S., and Jadbabaie, A. Why gradient clipping accelerates
    training: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881
    (2019).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Zhang, J., He, T., Sra, S., 和 Jadbabaie, A. 为什么梯度裁剪加速训练：适应性的理论依据。arXiv
    预印本 arXiv:1905.11881（2019）。'
- en: '[126] Zhang, S., Yin, H., Chen, T., Huang, Z., Cui, L., and Zhang, X. Graph
    Embedding for Recommendation against Attribute Inference Attacks. In Proceedings
    of the Web Conference 2021 (New York, NY, USA, June 2021), WWW ’21, Association
    for Computing Machinery, pp. 3002–3014.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Zhang, S., Yin, H., Chen, T., Huang, Z., Cui, L., 和 Zhang, X. 针对属性推断攻击的推荐图嵌入。见《网络会议论文集
    2021》（纽约，NY，美国，2021 年 6 月），WWW ’21，计算机协会，第 3002–3014 页。'
- en: '[127] Zhang, Y., Jia, R., Pei, H., Wang, W., Li, B., and Song, D. The secret
    revealer: Generative model-inversion attacks against deep neural networks. In
    Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
    (2020), pp. 253–261.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Zhang, Y., Jia, R., Pei, H., Wang, W., Li, B., 和 Song, D. 秘密揭示者：对深度神经网络的生成模型反演攻击。见《IEEE/CVF
    计算机视觉与模式识别会议论文集》（2020），第 253–261 页。'
- en: '[128] Zhao, J., Chen, Y., and Zhang, W. Differential Privacy Preservation in
    Deep Learning: Challenges, Opportunities and Solutions. IEEE Access 7 (2019),
    48901–48911.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Zhao, J., Chen, Y., 和 Zhang, W. 深度学习中的差分隐私保护：挑战、机会与解决方案。IEEE Access 7（2019），第
    48901–48911 页。'
- en: '[129] Zheng, H., Ye, Q., Hu, H., Fang, C., and Shi, J. BDPL: A Boundary Differentially
    Private Layer Against Machine Learning Model Extraction Attacks. In Computer Security
    – ESORICS 2019 (Cham, 2019), K. Sako, S. Schneider, and P. Y. A. Ryan, Eds., Lecture
    Notes in Computer Science, Springer International Publishing, pp. 66–83.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Zheng, H., Ye, Q., Hu, H., Fang, C., 和 Shi, J. BDPL: 一种针对机器学习模型提取攻击的边界差分隐私层。见《计算机安全
    – ESORICS 2019》（Cham, 2019），K. Sako, S. Schneider, 和 P. Y. A. Ryan 主编，《计算机科学讲义笔记》，Springer
    International Publishing，第 66–83 页。'
- en: '[130] Zheng, H., Ye, Q., Hu, H., Fang, C., and Shi, J. Protecting Decision
    Boundary of Machine Learning Model With Differentially Private Perturbation. IEEE
    Transactions on Dependable and Secure Computing 19, 3 (May 2022), 2007–2022.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Zheng, H., Ye, Q., Hu, H., Fang, C., 和 Shi, J. 通过差分隐私扰动保护机器学习模型的决策边界。IEEE
    可靠与安全计算汇刊 19, 3（2022 年 5 月），第 2007–2022 页。'
- en: '[131] Zhu, T., Ye, D., Wang, W., Zhou, W., and Yu, P. S. More Than Privacy:
    Applying Differential Privacy in Key Areas of Artificial Intelligence. IEEE Transactions
    on Knowledge and Data Engineering 34, 6 (June 2022), 2824–2843.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Zhu, T., Ye, D., Wang, W., Zhou, W., 和 Yu, P. S. 超越隐私：在人工智能关键领域应用差分隐私。IEEE
    知识与数据工程汇刊 34, 6（2022 年 6 月），第 2824–2843 页。'
