- en: 'Deep Learning 2: Part 2 Lesson 14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第2部分第14课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从*[*fast.ai课程*](http://www.fast.ai/) *中得到的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*[*Jeremy*](https://twitter.com/jeremyphoward)
    *和*[*Rachel*](https://twitter.com/math_rachel) *给了我这个学习的机会。*'
- en: '[Forum](http://forums.fast.ai/t/part-2-lesson-14-wiki/15650/1) / [Video](https://youtu.be/nG3tT31nPmQ)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[论坛](http://forums.fast.ai/t/part-2-lesson-14-wiki/15650/1) / [视频](https://youtu.be/nG3tT31nPmQ)'
- en: Show and tell from last week
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上周的展示
- en: Alena Harley did something really interesting which was she tried finding out
    what would happen if you did cycle GAN on just three or four hundred images and
    I really like these projects where people just go to Google Image Search using
    the API or one of the libraries out there. Some of our students have created some
    very good libraries for interacting with Google images API to download a bunch
    of stuff they are interested in, in this case some photos and some stained glass
    windows. With 300~400 photos of that, she trained a few different model — this
    is what I particularly liked. As you can see, with quite a small number of images,
    she gets very nice stained-glass effects. So I thought that was an interesting
    example of using pretty small amounts of data that was readily available that
    she was able to download pretty quickly. There is more information about that
    on the forum if you are interested.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Alena Harley做了一些非常有趣的事情，她尝试找出如果只对三四百张图片进行循环GAN会发生什么，我真的很喜欢这些项目，人们只需使用API或其中一个库去谷歌图片搜索。我们的一些学生已经创建了一些非常好的库，用于与谷歌图片API进行交互，下载一些他们感兴趣的东西，比如一些照片和一些彩色玻璃窗。有了300~400张照片，她训练了几个不同的模型——这是我特别喜欢的。正如你所看到的，用相当少量的图片，她得到了非常漂亮的彩色玻璃效果。所以我认为这是一个有趣的例子，使用相当少量的数据，她能够很快地下载到的数据。如果你感兴趣，论坛上有更多信息。
- en: It’s interesting to wonder about what kinds of things people will come up with
    with this kind of generative model. It’s clearly a great artistic medium. It’s
    clearly a great medium for forgeries and fakeries. I wonder what other kinds of
    things people will realize they can do with these kind of generative models. I
    think audio is going to be the next big area. Also very interactive type stuff.
    Nvidia just released a paper showing a interactive kind of photo repair tool where
    you just brush over an object and it replaces it with a deep learning generated
    replacement very nicely. Those kinds of interactive tools, I think would be very
    interesting too.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人们会用这种生成模型想出什么样的东西是很有趣的。这显然是一个很好的艺术媒介。显然也是一个很好的伪造和欺骗媒介。我想知道人们会意识到他们可以用这种生成模型做什么其他类型的事情。我认为音频将成为下一个重要领域。还有非常互动的类型。英伟达刚刚发布了一篇论文，展示了一种互动的照片修复工具，你只需刷过一个物体，它就会用深度学习生成的替代品替换得很好。我认为这种互动工具也会很有趣。
- en: Super-Resolution [[2:06](https://youtu.be/nG3tT31nPmQ?t=2m6s)]
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超分辨率[[2:06](https://youtu.be/nG3tT31nPmQ?t=2m6s)]
- en: '[Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[实时风格转移和超分辨率的感知损失](https://arxiv.org/abs/1603.08155)'
- en: Last time, we looked at doing style transfer by actually directly optimizing
    pixels. Like with most of the things in part two, it’s not so much that I’m wanting
    you to understand style transfer per se, but the kind of idea of optimizing your
    input directly and using activations as part of a loss function is really the
    key takeaway here.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 上次，我们看了通过直接优化像素来进行风格转移。就像第二部分的大部分内容一样，我并不是想让你理解风格转移本身，而是直接优化输入并使用激活作为损失函数的一种想法，这才是真正的关键点。
- en: So it’s interesting then to see effectively the follow-up paper, not from the
    same people but the paper that came next in the sequence of these vision generative
    models with this one from Justin Johnson and folks at Stanford. It actually does
    the same thing — style transfer, but does it in a different way. Rather than optimizing
    the pixels, we are going to go back to something much more familiar and optimize
    some weights. So specifically, we are going to train a model which learns to take
    a photo and translate it into a photo in the style of a particular artwork. So
    each conv net will learn to produce one kind of style.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有趣的是看到接下来的论文，不是来自同一组人，而是在这些视觉生成模型序列中接下来的一篇来自斯坦福大学的Justin Johnson和他的同事。它实际上做了同样的事情——风格转移，但是用了不同的方法。与其优化像素，我们将回到更熟悉的东西，优化一些权重。具体来说，我们将训练一个模型，学习将一张照片转换成某种艺术作品风格的照片。因此，每个卷积网络将学习产生一种风格。
- en: Now it turns out that getting to that point, there is an intermediate point
    which (I actually think more useful and takes us half way there) is something
    called super resolution. So we are actually going to start with super resolution
    [[3:55](https://youtu.be/nG3tT31nPmQ?t=3m55s)]. Because then we’ll build on top
    of super resolution to finish off the conv net based style transfer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，事实证明，要达到那一点，有一个中间点（我认为更有用，可以让我们走一半的路）叫做超分辨率。所以我们实际上要从超分辨率开始[[3:55](https://youtu.be/nG3tT31nPmQ?t=3m55s)]。因为然后我们将在超分辨率的基础上构建卷积神经网络风格转移的最后部分。
- en: Super resolution is where we take a low resolution image (we are going to take
    72 by 72) and upscale it to a larger image (288 by 288 in our case) trying to
    create a higher res image that looks as real as possible. This is a challenging
    thing to do because at 72 by 72, there’s not that much information about a lot
    of the details. The cool thing is that we are going to do it in a way as we tend
    to do with vision models which is not tied to the input size so you could totally
    then take this model and apply it to a 288 by 288 image and get something that’s
    four times bigger on each side so 16 times bigger than the original. Often it
    even works better at that level because you’re really introducing a lot of detail
    into the finer details and you could really print out a high resolution print
    of something which earlier on was pretty pixelated.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 超分辨率是指我们将一个低分辨率图像（我们将采用72x72）放大到一个更大的图像（在我们的情况下是288x288），试图创建一个看起来尽可能真实的高分辨率图像。这是一件具有挑战性的事情，因为在72x72的情况下，关于很多细节的信息并不多。很酷的是，我们将以一种与视觉模型相似的方式来做，这种方式不受输入大小的限制，因此您完全可以将这个模型应用于288x288的图像，得到每边都大四倍的东西，比原始图像大16倍。通常在那个级别甚至效果更好，因为您真的在更细节的地方引入了很多细节，您可以真正打印出一个高分辨率的打印品，而之前它看起来相当像素化。
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/enhance.ipynb)
    [[5:06](https://youtu.be/nG3tT31nPmQ?t=5m6s)]'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/enhance.ipynb)'
- en: 'It is a lot like that kind of CSI style enhancement where we’re going to take
    something that appears the information is just not there and we kind of invent
    it — but the conv net is going to learn to invent it in a way that’s consistent
    with the information that is there, so hopefully it’s inventing the right information.
    One of the really nice things about this kind of problem is that we can create
    our own dataset as big as we like without any labeling requirements because we
    can easily create a low res image from a high res image just by down sampling
    our images. So something I would love some of you to try this week would be to
    do other types of image-to-image translation where you can invent “labels” (your
    dependent variables). For example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这很像CSI风格的增强，我们将拿出一些看起来信息不在那里的东西，我们会发明它——但是卷积网络将学会以与已有信息一致的方式发明它，所以希望它发明正确的信息。这种问题的一个非常好的地方是，我们可以创建自己的数据集，而不需要任何标签要求，因为我们可以通过对图像进行降采样轻松地从高分辨率图像创建低分辨率图像。所以我希望你们中的一些人这周尝试做其他类型的图像到图像的转换，你可以发明“标签”（你的因变量）。例如：
- en: '**Deskewing**: Either recognize things that have been rotated by 90 degrees
    or better still that have been rotated by 5 degrees and straighten them.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去斜**：识别已经旋转了90度或更好的是旋转了5度并将其拉直的东西。'
- en: '**Colorization**: Make a bunch of images into black-and-white and learn to
    put the color back again.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**着色**：将一堆图像变成黑白，然后学会重新加上颜色。'
- en: '**Noise-reduction**: Maybe do a really low quality JPEG save, and learn to
    put it back to how it should have been.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降噪**：也许做一个质量很低的JPEG保存，然后学会将其恢复到应该有的样子。'
- en: Maybe taking something that’s in a 16 color palette and put it back to a higher
    color palette.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也许将一个16色调色板的东西放回到更高的色调色板。
- en: I think these things are all interesting because they can be used to take pictures
    that you may have taken back on crappy old digital cameras before there are high
    resolution or you may have scanned in some old photos that are now faded, etc.
    I think it’s really useful thing to be able to do and it’s a good project because
    it’s really similar to what we are doing here but different enough that you come
    across some interesting challenges on the way, I’m sure.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这些东西都很有趣，因为它们可以用来处理您以前用糟糕的旧数码相机拍摄的照片，或者您可能已经扫描了一些现在已经褪色的旧照片等。我认为这是一件非常有用的事情，也是一个很好的项目，因为它与我们在这里所做的非常相似，但又有足够的不同，让您在途中遇到一些有趣的挑战，我相信。
- en: I’m going to use ImageNet again [[7:19](https://youtu.be/nG3tT31nPmQ?t=7m19s)].
    You don’t need to use all of the ImageNet at all, I just happen to have it lying
    around. You can download the one percent sample of ImageNet from files.fast.ai.
    You can use any set of pictures you have lying around honestly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我将再次使用ImageNet。您根本不需要使用所有的ImageNet，我只是碰巧有它。您可以从files.fast.ai下载ImageNet的百分之一样本。您可以使用您手头上任何一组图片。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Super resolution data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超分辨率数据
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, as I say we don’t really have labels per se, so I’m just going
    to give everything a label of zero just so we can use it with our existing infrastructure
    more easily.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，正如我所说，我们实际上没有标签，所以我只是给每样东西都标上零，这样我们就可以更容易地与我们现有的基础设施一起使用。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, because I’m pointing at a folder that contains all of ImageNet, I certainly
    don’t want to wait for all of ImageNet to finish to run an epoch. So here, I’m
    just, most of the time, I would set “keep percent” ( `keep_pct` ) to 1 or 2%.
    And then I just generate a bunch of random numbers and then I just keep those
    which are less than 0.02 and so that lets me quickly subsample my rows.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，因为我指向一个包含所有ImageNet的文件夹，我当然不想等待所有ImageNet完成一个周期才运行。所以在这里，我通常会将“保留百分比”（`keep_pct`）设置为1或2%。然后我只生成一堆随机数，然后只保留那些小于0.02的数，这样让我快速地对行进行子采样。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So we are going to use VGG16 [[8:21](https://youtu.be/nG3tT31nPmQ?t=8m21s)]
    and VGG16 is something that we haven’t really looked at in this class but it’s
    a very simple model where we take our normal presumably 3 channel input, and we
    basically run it through a number of 3x3 convolutions, and then from time to time,
    we put it through a 2x2 maxpool and then we do a few more 3x3 convolutions, maxpool,
    so on so forth. And this is our backbone.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将使用VGG16，VGG16是我们在这门课程中还没有真正研究过的东西，但它是一个非常简单的模型，我们将采用我们通常的预计是3通道输入，然后基本上通过一系列3x3的卷积运行它，然后不时地，我们将它通过一个2x2的最大池化，然后我们再做一些3x3的卷积，最大池化，依此类推。这就是我们的骨干。
- en: Then we don’t do an adaptive average pooling layer. After a few of these, we
    end up with this 7x7x512 grid as usual (or something similar). So rather than
    average pooling, we do something different which is we flatten the whole thing
    — so that spits out a very long vector of activations of size 7x7x512 if memory
    serves correctly. Then that gets fed into two fully connected layers each one
    of which has 4096 activations, and one more fully connected layer which has however
    many classes. So if you think about it, the weight matrix here, it’s HUGE 7x7x512x4096\.
    It’s because of that weight matrix really that VGG went out of favor pretty quickly
    — because it takes a lot of memory and takes a lot of computation and it’s really
    slow. And there’s a lot of redundant stuff going on here because really those
    512 activations are not that specific to which of those 7x7 grid cells they are
    in. But when you have this entire weight matrix here of every possible combination,
    it treats all of them uniquely. So that can also lead to generalization problems
    because there’s just a lot of weights and so forth.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们不再使用自适应平均池化层。经过几次操作后，我们像往常一样得到了一个7x7x512的网格（或类似的东西）。所以我们不再进行平均池化，而是做一些不同的事情，即将整个东西展平
    - 这样就会输出一个大小为7x7x512的非常长的激活向量。然后将其馈送到两个全连接层，每个全连接层有4096个激活，并且还有一个具有多少类别的全连接层。所以如果你考虑一下，这里的权重矩阵是巨大的7x7x512x4096。正是因为这个权重矩阵，VGG很快就不受欢迎了
    - 因为它占用了大量内存，需要大量计算，速度非常慢。这里有很多冗余的东西，因为实际上这512个激活并不特定于它们在哪个7x7网格单元中。但是当你有这里的整个权重矩阵，包含了每种可能的组合，它会将它们都视为独特的。这也可能导致泛化问题，因为有很多权重等等。
- en: My view is that the approach that is used in every modern network which is here
    we do an adaptive average pooling (in Keras it’s known as a global average pooling,
    in fast.ai, we do an AdaptiveConcatPool) which spits it straight down to a 512
    long activation [[11:06](https://youtu.be/nG3tT31nPmQ?t=11m6s)]. I think that’s
    throwing away too much geometry. So to me, probably the correct answer is somewhere
    in between and will involve some kind of factored convolution or some kind tensor
    decomposition which maybe some of us can think about in the coming months. So
    for now, anyway, we’ve gone from one extreme which is the adaptive average pooling
    to the other extreme which is this huge flattened fully connected layer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为现代网络中使用的方法是进行自适应平均池化（在Keras中被称为全局平均池化，在fast.ai中我们使用自适应连接池），这将直接输出一个512维的激活。我认为这样做丢失了太多的几何信息。所以对我来说，可能正确的答案在两者之间，并且可能涉及某种因子卷积或张量分解，也许我们中的一些人可以在未来几个月考虑一下。所以目前，我们已经从自适应平均池化这个极端转向了另一个极端，即这个巨大的扁平化全连接层。
- en: A couple of things which are interesting about VGG that make it still useful
    today [[11:59](https://youtu.be/nG3tT31nPmQ?t=11m59s)]. The first one is that
    there’s more interesting layers going on here with most modern networks including
    the ResNet family, the very first layer generally is a 7x7 conv with stride 2
    or something similar. Which means we throw away half the grid size straight away
    and so there is little opportunity to use the fine detail because we never do
    any computation with it. So that’s a bit of a problem for things like segmentation
    or super resolution models because the fine details matters. We actually want
    to restore it. Then the second problem is that the adaptive pooling layer entirely
    throws away the geometry in the last few sections which means that the rest of
    the model doesn’t really have as much interesting kind of learning that geometry
    as it otherwise might. Therefore for things which are dependent on position, any
    kind of localization based approach to anything that requires generative model
    is going to be less effective. So one of the things I’m hoping you are hearing
    as I describe this is that probably none of the existing architectures are actually
    ideal. We can invent a new one. Actually, I just tried inventing a new one over
    the week which was to take the VGG head and attach it to a ResNet backbone. Interestingly,
    I found I actually got a slightly better classifier than a normal ResNet but it
    also was something with a little bit more useful information in it. It took 5
    or 10% longer to train but nothing worth worrying about. Maybe we could, in ResNet,
    replace this (7x7 conv stride 2) as we’ve talked about briefly before. This very
    early convolution with something more like an Inception stem which has a bit more
    computation. I think there’s definitely room for some nice little tweaks to these
    architectures so that we can build some models which are maybe more versatile.
    At the moment, people tend to build architectures that just do one thing. They
    don’t really think what am I throwing away in terms of opportunity because that’s
    how publishing works. You published “I’ve got state of the art of this one thing
    rather than you have created something that’s good at a lots of things.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关于VGG有一些有趣的事情，使它至今仍然有用[[11:59](https://youtu.be/nG3tT31nPmQ?t=11m59s)]。第一件事是这里有更多有趣的层，大多数现代网络包括ResNet系列，第一层通常是一个7x7的卷积，步幅为2或类似的。这意味着我们立即丢弃了一半的网格大小，因此几乎没有机会使用细节，因为我们从不对其进行任何计算。这对于分割或超分辨率模型等需要细节的问题是一个问题。我们实际上想要恢复它。然后第二个问题是自适应池化层完全丢弃了最后几个部分的几何信息，这意味着模型的其余部分实际上没有太多有趣的几何学习。因此，对于依赖位置的事物，任何需要生成模型的定位方法都会不太有效。所以我希望你在我描述这些内容时能听到的一件事是，也许现有的架构都不是理想的。我们可以发明一个新的。实际上，我在这一周尝试了发明一个新的，就是将VGG头部连接到ResNet骨干上。有趣的是，我发现我实际上得到了一个稍微更好的分类器，比普通的ResNet好一点，但它也包含了一些更有用的信息。训练时间长了5到10%，但没有什么值得担心的。也许我们可以在ResNet中，用我们之前简要讨论过的方式，将这个（7x7卷积步幅2）替换为更像Inception
    stem的东西，这样有更多的计算。我认为这些架构肯定有一些小的调整空间，这样我们可以构建一些可能更多功能的模型。目前，人们倾向于构建只能做一件事的架构。他们并没有真正考虑到机会的丢失，因为这就是出版的工作方式。你发表“我在这一件事上达到了最新水平”而不是你创造了一些在很多方面都很擅长的东西。
- en: For these reasons, we are going to use VGG today even though it’s ancient and
    it’s missing lots of great stuff [[14:42](https://youtu.be/nG3tT31nPmQ?t=14m42s)].
    One thing we are going to do though is use a slightly more modern version which
    is a version of VGG where batch norm has been added after all the convolutions.
    In fast.ai when you ask for a VGG network, you always get the batch norm one because
    that’s basically always what you want. So this is VGG with batch norm. There is
    16 and 19, the 19 is way bigger and heavier, and doesn’t really do any better,
    so no one really uses it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，今天我们将使用VGG，尽管它已经过时并且缺少很多很棒的东西[[14:42](https://youtu.be/nG3tT31nPmQ?t=14m42s)]。不过，我们要做的一件事是使用一个稍微更现代的版本，这是一个在所有卷积层之后添加了批量归一化的VGG版本。在fast.ai中，当你请求一个VGG网络时，你总是得到批量归一化的版本，因为那基本上总是你想要的。所以这是带有批量归一化的VGG。有16和19，19更大更重，但实际上并没有做得更好，所以没有人真的使用它。
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are going to go from 72 by 72 LR (`sz_lr`: size low resolution) input. We
    are going to initially scale it up by times 2 with the batch size of 64 to get
    2 * 72 so 144 by 144 output. That is going to be our stage one.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从72x72的LR（`sz_lr`：低分辨率大小）输入开始。我们将首先通过64的批次大小将其放大2倍，以获得2 * 72，即144x144的输出。这将是我们的第一阶段。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’ll create our own dataset for this and it’s very worthwhile looking inside
    the fastai.dataset module and seeing what’s there [[15:45](https://youtu.be/nG3tT31nPmQ?t=15m45s)].
    Because just about anything you’d want, we probably have something that’s almost
    what you want. So in this case, I want a dataset where my *x*’s are images and
    my *y*’s are also images. There’s already a files dataset we can inherit from
    where the *x*’s are images and then I just inherit from that and I just copied
    and pasted the `get_x` and turn that into `get_y` so it just opens an image. Now
    I’ve got something where the *x* is an image and the *y* is an image, and in both
    cases, what we’re passing in is an array of files names.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为此创建自己的数据集，值得查看fastai.dataset模块的内部并看看那里有什么[[15:45](https://youtu.be/nG3tT31nPmQ?t=15m45s)]。因为几乎任何你想要的东西，我们可能都有几乎符合你要求的东西。所以在这种情况下，我想要一个数据集，其中我的*x*是图像，我的*y*也是图像。已经有一个文件数据集，我们可以继承其中的*x*是图像，然后我只需继承自那个，并且我只是复制并粘贴了`get_x`并将其转换为`get_y`，这样它就打开了一个图像。现在我有了一个*x*是图像，*y*也是图像的东西，在这两种情况下，我们传入的都是文件名数组。
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: I’m going to do some data augmentation [[16:32](https://youtu.be/nG3tT31nPmQ?t=16m32s)].
    Obviously with all of ImageNet, we don’t really need it but this is mainly here
    for anybody who is using smaller datasets to make the most of it. `RandomDihedral`
    is referring to every possible 90 degree rotation plus optional left/right flipping
    so they are dihedral group of eight symmetries. Normally we don’t use this transformation
    for ImageNet pictures because you don’t normally flip dogs upside down but in
    this case, we are not trying to classify whether it’s a dog or a cat, we are just
    trying to keep the general structure of it. So actually every possible flip is
    a reasonably sensible thing to do for this problem.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我将进行一些数据增强。显然，对于所有的ImageNet，我们并不真正需要它，但这主要是为了任何使用较小数据集的人能够充分利用它。`RandomDihedral`指的是每个可能的90度旋转加上可选的左/右翻转，因此它们是八个对称的二面角群。通常我们不会对ImageNet图片使用这种转换，因为你通常不会把狗颠倒过来，但在这种情况下，我们并不是试图分类它是狗还是猫，我们只是试图保持它的一般结构。因此，实际上对于这个问题来说，每个可能的翻转都是一个相当明智的事情。
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create a validation set in the usual way [[17:19](https://youtu.be/nG3tT31nPmQ?t=17m19s)].
    You can see I’m using a few more slightly lower level functions — generally speaking,
    I just copy and paste them out of the fastai source code to find the bits I want.
    So here is the bit which takes an array of validation set indexes and one or more
    arrays of variables, and simply splits. In this case, this (`np.array(fnames)`)
    into a training and validation set, and this (the second `np.array(fnames)`) into
    a training and validation set to give us our *x*’s and our *y*’s. In this case,
    the *x* and the *y* are the same. Our input image and our output image are the
    same. We are going to use transformations to make one of them lower resolution.
    That’s why these are the same thing .
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以通常的方式创建一个验证集。你可以看到我使用了一些更低级别的函数——一般来说，我只是从fastai源代码中复制和粘贴它们，找到我想要的部分。这里有一个部分，它接受一个验证集索引数组和一个或多个变量数组，然后简单地分割。在这种情况下，这个（`np.array(fnames)`）分成一个训练和验证集，这个（第二个`np.array(fnames)`）分成一个训练和验证集，给我们我们的*x*和*y*。在这种情况下，*x*和*y*是相同的。我们的输入图像和输出图像是相同的。我们将使用转换使它们中的一个分辨率较低。这就是为什么它们是相同的东西。
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The next thing that we need to do is to create our transformations as per usual
    [[18:13](https://youtu.be/nG3tT31nPmQ?t=18m13s)]. We are going to use `tfm_y`
    parameter like we did for bounding boxes but rather than use `TfmType.COORD` we
    are going to use `TfmType.PIXEL`. That tells our transformations framework that
    your *y* values are images with normal pixels in them, so anything you do to the
    *x*, you also need to do the same thing to the *y*. You need to make sure any
    data augmentation transformations you use have the same parameter as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要像往常一样创建我们的转换。我们将使用`tfm_y`参数，就像我们为边界框所做的那样，但我们不是使用`TfmType.COORD`，而是使用`TfmType.PIXEL`。这告诉我们的转换框架，你的*y*值是带有正常像素的图像，所以任何你对*x*做的事情，你也需要对*y*做同样的事情。你需要确保你使用的任何数据增强转换也具有相同的参数。
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can see the possible transform types you got:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到你得到的可能的转换类型：
- en: 'CLASS: classification which we are about to use the segmentation in the second
    half of today'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类：我们将在今天的下半部分使用分割
- en: 'COORD: coordinates — no transformation at all'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坐标：坐标——没有任何转换
- en: PIXEL
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素
- en: Once we have `Dataset` class and some *x* and *y* training and validation sets.
    There is a handy little method called get datasets (`get_ds`) which basically
    runs that constructor over all the different things that you have to return all
    the datasets you need in exactly the right format to pass to a ModelData constructor
    (in this case the `ImageData` constructor). So we are kind of going back under
    the covers of fastai a little bit and building it up from scratch. In the next
    few weeks, this will all be wrapped up and refactored into something that you
    can do in a single step in fastai. But the point of this class is to learn a bit
    about going under the covers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`Dataset`类和一些*x*和*y*的训练和验证集。有一个方便的小方法叫做获取数据集(`get_ds`)，它基本上运行构造函数，返回你需要的所有数据集，以恰好正确的格式传递给ModelData构造函数（在这种情况下是`ImageData`构造函数）。所以我们有点回到了fastai的内部，从头开始构建。在接下来的几周里，这一切都将被整合和重构成你可以在fastai中一步完成的东西。但这个类的目的是为了学习一些关于内部的知识。
- en: 'Something we’ve briefly seen before is that when we take images in, we transform
    them not just with data augmentation but we also move the channel dimension up
    to the start, we subtract the mean divided by the standard deviation etc [[20:08](https://youtu.be/nG3tT31nPmQ?t=20m8s)].
    So if we want to be able to display those pictures that have come out of our datasets
    or data loaders, we need to de-normalize them. So the model data object’s (`md`)
    dataset (`val_ds`) has denorm function that knows how to do that. I’m just going
    to give that a short name for convenience:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前简要看到的是，当我们输入图像时，我们不仅要进行数据增强，还要将通道维度移到开头，我们要减去平均值除以标准差等。所以如果我们想要显示那些从我们的数据集或数据加载器中出来的图片，我们需要对它们进行反归一化。所以模型数据对象（`md`）的数据集（`val_ds`）有一个denorm函数，知道如何做到这一点。我只是为了方便给它一个简短的名字：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So now I’m going to create a function that can show an image from a dataset
    and if you pass in something saying this is a normalized image, then we’ll denorm
    it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我要创建一个函数，可以显示数据集中的图像，如果你传入一个说这是一个归一化图像的东西，那么我们将对它进行反归一化。
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You’ll see here we’ve passed in size low res (`sz_lr`) as our size for the transforms
    and size high res (`sz_hr`) as, this is something new, the size y parameter (`sz_y`)
    [[20:58](https://youtu.be/nG3tT31nPmQ?t=20m58s)]. So the two bits are going to
    get different sizes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到我们传入了低分辨率大小（`sz_lr`）作为我们的转换大小，高分辨率大小（`sz_hr`）作为，这是新的东西，大小y参数（`sz_y`）。所以这两部分将得到不同的大小。
- en: Here you can see the two different resolutions of our *x* and our *y* for a
    whole bunch of fish.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到我们的*x*和*y*的两种不同分辨率，用于一大堆鱼。
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As per usual, `plt.subplots` to create our two plots and then we can just use
    the different axes that came back to put stuff next to each other.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，使用`plt.subplots`创建我们的两个图，然后我们可以使用返回的不同轴将东西放在一起。
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can then have a look at a few different versions of the data transformation
    [[21:37](https://youtu.be/nG3tT31nPmQ?t=21m37s)]. There you can see them being
    flipped in all different directions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以看一下数据转换的几个不同版本[[21:37](https://youtu.be/nG3tT31nPmQ?t=21m37s)]。在那里，您可以看到它们被以各种不同方向翻转。
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Model [[21:48](https://youtu.be/nG3tT31nPmQ?t=21m48s)]
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型[[21:48](https://youtu.be/nG3tT31nPmQ?t=21m48s)]
- en: 'Let’s create our model. We are going to have a small image coming in, and we
    want to have a big image coming out. So we need to do some computation between
    those two to calculate what the big image would look like. Essentially there’re
    two ways of doing that computation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的模型。我们将有一个小图像输入，并且我们希望有一个大图像输出。因此，我们需要在这两者之间进行一些计算，以计算大图像会是什么样子。基本上有两种方法来进行这种计算：
- en: We could first of all do some upsampling and then do a few stride one layers
    to do lots of computation.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先可以进行一些上采样，然后进行一些步幅为1的层来进行大量计算。
- en: We could first do lots of stride one layers to do all the computation and then
    at the end do some upsampling.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以首先进行大量步幅为1的层来进行所有计算，然后最后进行一些上采样。
- en: 'We are going to pick the second approach because we want to do lots of computation
    on something smaller because it’s much faster to do it that way. Also, all that
    computation we get to leverage during the upsampling process. Upsampling, we know
    a couple of possible ways to do that. We can use:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择第二种方法，因为我们想在较小的东西上进行大量计算，因为这样做速度更快。此外，在上采样过程中，我们可以利用所有这些计算。上采样，我们知道有几种可能的方法可以做到这一点。我们可以使用：
- en: Transposed or fractionally strided convolutions
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转置或分数步幅卷积
- en: Nearest neighbor upsampling followed by a 1x1 conv
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近邻上采样，然后是1x1卷积
- en: And in “do lots of computation” section, we could just have a whole bunch of
    3x3 convs. But in this case particular, it seems likely that ResNet blocks are
    going to be better because really the output and the input are very very similar.
    So we really want a flow through path that allows as little fussing around as
    possible except a minimal amount necessary to do our super resolution. If we use
    ResNet blocks, then they have an identity path already. So you can imagine those
    simple version where it does a bilinear sampling approach or something it could
    just go through identity block all the way through and then in the upsampling
    blocks, just learn to take the averages of the inputs and get something that’s
    not too terrible.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在“进行大量计算”部分，我们可以只进行大量的3x3卷积。但在这种特殊情况下，ResNet块似乎更好，因为输出和输入非常相似。因此，我们真的希望有一个流经路径，允许尽可能少地烦扰，除了必要的最小量来进行我们的超分辨率。如果我们使用ResNet块，那么它们已经有一个身份路径。因此，您可以想象那些简单版本，它采用双线性采样方法或其他方法，它可以直接通过身份块，然后在上采样块中，只需学习获取输入的平均值，并得到一些不太糟糕的东西。
- en: So that’s what we are going to do. We are going to create something with five
    ResNet blocks and then for each 2x scale up we have to do, we’ll have one upsampling
    block.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们要做的。我们将创建一个具有五个ResNet块的模型，然后对于每个2倍的缩放，我们将有一个上采样块。
- en: They are all going to consist of, as per usual, convolution layers possibly
    with activation functions after many of them [[24:37](https://youtu.be/nG3tT31nPmQ?t=24m37s)].
    I like to put my standard convolution block into a function so I can refactor
    it more easily. I won’t worry about passing in padding and just calculate it directly
    as kernel size over two.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都将由通常的卷积层组成，可能在其中的许多之后带有激活函数[[24:37](https://youtu.be/nG3tT31nPmQ?t=24m37s)]。我喜欢将我的标准卷积块放入一个函数中，这样我可以更容易地重构它。我不会担心传递填充，并直接计算它作为内核大小的一半。
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: One interesting thing about our little conv block is that there is no batch
    norm which is pretty unusual for ResNet type models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们小卷积块的一个有趣之处在于没有批量归一化，这对于ResNet类型的模型来说是非常不寻常的。
- en: '[https://arxiv.org/abs/1707.02921](https://arxiv.org/abs/1707.02921)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1707.02921](https://arxiv.org/abs/1707.02921)'
- en: 'The reason there is no batch norm is because I’m stealing ideas from this fantastic
    recent paper which actually won a recent competition in super resolution performance.
    To see how good this paper is, SRResNet is the previous state of the art and what
    they’ve done here is they’ve zoomed way in to an upsampled mesh/fence. HR is the
    original. You can see in the previous best approach, there’s a whole lot of distortion
    and blurring going on. Or else, in their approach, it’s nearly perfect. So this
    paper was a really big step-up. They call their model EDSR ( Enhanced Deep Super-Resolution
    network) and they did two things differently to the previous standard approaches:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 没有批量归一化的原因是因为我从这篇最近的出色论文中窃取了一些想法，这篇论文实际上赢得了最近的超分辨率性能比赛。要看看这篇论文有多好，SRResNet是之前的最先进技术，他们在这里所做的是他们已经放大到了一个上采样的网格/围栏。HR是原始的。您可以看到在以前的最佳方法中，存在大量的失真和模糊。或者，在他们的方法中，几乎完美。因此，这篇论文是一个真正的重大进步。他们称其模型为EDSR（增强深度超分辨率网络），并且他们与以前的标准方法有两点不同：
- en: Take the ResNet blocks and throw away the batch norms. Why would they throw
    away the batch norm? The reason is because batch norm changes stuff and we want
    a nice straight through path that doesn’t change stuff. So the idea here is if
    you don’t want to fiddle with the input more than you have to, then don’t force
    it to have to calculate things like batch norm parameters — so throw away the
    batch norm.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拿起ResNet块并丢弃批量归一化。为什么要丢弃批量归一化？原因是因为批量归一化会改变东西，而我们希望有一个不改变东西的良好直通路径。因此，这里的想法是，如果您不想对输入进行更多操作，那么就不要强迫它计算诸如批量归一化参数之类的东西-所以丢弃批量归一化。
- en: Scaling factor (we will see shortly).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放因子（我们很快会看到）。
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: So we are going to create a residual block containing two convolutions. As you
    see in their approach, they don’t even have a ReLU after their second conv. So
    that’s why I’ve only got activation on the first one.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将创建一个包含两个卷积的残差块。正如你在他们的方法中看到的那样，他们甚至在第二个卷积后没有ReLU。这就是为什么我只在第一个上有激活。
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A couple of interesting things here [[27:10](https://youtu.be/nG3tT31nPmQ?t=27m10s)].
    One is that this idea of having some kind of a main ResNet path (conv, ReLU, conv)
    and then turning that into a ReLU block by adding it back to the identity — it’s
    something we do so often that I factored it out into a tiny little module called
    ResSequential. It simply takes a bunch of layers that you want to put into your
    residual path, turns that into a sequential model, runs it, and then adds it back
    to the input. With this little module, we can now turn anything, like conv activation
    conv, into a ResNet block just by wrapping in ResSequential.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个有趣的地方[27:10]。一个是这个想法，即有一种主要的ResNet路径（卷积，ReLU，卷积），然后通过将其添加回到身份来将其转换为ReLU块——我们经常这样做，以至于我将其提取出来成为一个名为ResSequential的小模块。它简单地将您想要放入残差路径的一堆层转换为顺序模型，运行它，然后将其添加回输入。有了这个小模块，我们现在可以通过将其包装在ResSequential中，将任何东西，比如卷积激活卷积，转换为一个ResNet块。
- en: But that’s not quite all I’m doing because normally a Res block just has `x
    + self.m(x)` in its `forward`. But I’ve also got `* self.res_scale`. What’s `res_scale`?
    `res_scale` is the number 0.1\. Why is it there? I’m not sure anybody quite knows.
    But the short answer is that the guy who invented batch norm also somewhat more
    recently did a paper in which he showed for (I think) the first time the ability
    to train ImageNet in under an hour. The way he did it was fire up lots and lots
    of machines and have them work in parallel to create really large batch sizes.
    Now generally when you increase the batch size by order *N*, you also increase
    the learning rate by order *N* to go with it. So generally a very large batch
    size training means very high learning rate training as well. He found that with
    these very large batch sizes of 8,000+ or even up to 32,000, at the start of training,
    his activations would basicall y go straight to infinity. And a lot of other people
    have found that. We actually found that when we were competing in DAWN bench both
    on the CIFAR and ImageNet competitions that we really struggled to make the most
    of even the eight GPUs that we were trying to take advantage of because of these
    challenges with these larger batch sizes and taking advantage of them. Something
    Christian found was that in the ResNet blocks, if he multiplied them by some number
    smaller than 1, something like .1 or .2, it really helped stabilize training at
    the start. That’s kind of weird because mathematically, it’s identical. Because
    obviously whatever I’m multiplying it by here, I could just scale the weights
    by the opposite amount and have the same number. But we are not dealing with abstract
    math — we are dealing with real optimization problems, different initializations,
    learning rates, and whatever else. So the problem of weights disappearing off
    into infinity, I guess generally is really about the discrete and finite nature
    of computers in practice partly. So often these kind of little tricks can make
    the difference.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是我正在做的全部，因为通常一个Res块在它的`forward`中只有`x + self.m(x)`。但我还加上了`* self.res_scale`。什么是`res_scale`？`res_scale`是数字0.1。为什么要有它？我不确定有人完全知道。但简短的答案是，发明批量归一化的那个人最近还发表了一篇论文，他在其中首次展示了在不到一个小时内训练ImageNet的能力。他是如何做到的呢？他启动了大量的机器，并让它们并行工作，以创建非常大的批量大小。通常情况下，当你将批量大小增加*N*倍时，你也会相应地增加*N*倍的学习率。所以通常情况下，非常大的批量大小训练也意味着非常高的学习率训练。他发现，当使用这些非常大的批量大小，如8000+甚至高达32000时，在训练开始时，他的激活基本上会直接变为无穷大。很多其他人也发现了这一点。我们在DAWN
    bench上参加CIFAR和ImageNet比赛时也发现了这一点，我们很难充分利用我们试图利用的八个GPU，因为这些更大批量大小和利用它们的挑战。Christian发现的一件事是，在ResNet块中，如果他将它们乘以小于1的某个数字，比如0.1或0.2，这确实有助于在开始时稳定训练。这有点奇怪，因为从数学上讲，它是相同的。因为显然，无论我在这里乘以什么，我只需按相反的数量缩放权重，就可以得到相同的数字。但我们不是在处理抽象的数学——我们在处理真实的优化问题，不同的初始化、学习率和其他因素。所以权重消失到无穷大的问题，我想通常主要是关于计算机在实践中的离散和有限性质的一部分。因此，通常这种小技巧可以起到关键作用。
- en: In this case, we are just toning things down based on our initial initialization.
    So there are probably other ways to do this. For example, one approach from some
    folks at Nvidia called LARS which I briefly mentioned last week is an approach
    which uses discriminative learning rates calculated in real time. Basically looking
    at the ration between the gradients and the activations to scale learning rates
    by layer. So they found that they didn’t need this trick to scale up the batch
    sizes a lot. Maybe a different initialization would be all that’s necessary. The
    reason I mentioned this is not so much because I think a lot of you are likely
    to want to train on massive clusters of computers but rather that I think a lot
    of you want to train models quickly and that means using high learning rates and
    ideally getting super convergence. I think these kinds of tricks are the tricks
    that we’ll need to be able to get super convergence across more different architectures
    and so forth. Other than Leslie Smith, no one else is really working on super
    convergence other than some fastai students nowadays. So these kind of things
    about how do we train at very very high learning rates, we’re going to have to
    be the ones who figure it out because as far as I can tell, nobody else cares
    yet. So looking at the literature around training ImageNet in one hour, or more
    recently there’s now train ImageNet in 15 minutes, these papers actually, I think,
    have some of the tricks to allow us to train things at high learning rates. So
    here is one of them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们只是根据我们的初始初始化来调整事物。所以可能还有其他方法可以做到这一点。例如，Nvidia的一些人提出的一种叫做LARS的方法，我上周简要提到过，这是一种实时计算的判别学习率方法。基本上是通过查看梯度和激活之间的比率来按层缩放学习率。因此，他们发现他们不需要这个技巧来大幅增加批量大小。也许只需要不同的初始化就足够了。我提到这一点的原因并不是因为我认为你们中很多人可能想要在大型计算机集群上进行训练，而是因为我认为你们中很多人想要快速训练模型，这意味着使用高学习率，并且理想情况下实现超级收敛。我认为这些技巧是我们需要能够在更多不同的架构等方面实现超级收敛的技巧。除了Leslie
    Smith之外，没有其他人真正致力于超级收敛，现在只有一些fastai学生在做这些事情。因此，关于如何以非常非常高的学习率进行训练的问题，我们将不得不自己去解决，因为据我所知，其他人还没有关心这个问题。因此，查看围绕在一个小时内训练ImageNet的文献，或者最近有现在在15分钟内训练ImageNet的文献，我认为，这些论文实际上有一些技巧可以让我们以高学习率训练事物。这就是其中之一。
- en: Interestingly, other than the train ImageNet in one hour paper, the only other
    place I’ve seen this mentioned was in this EDSR paper. It’s really cool because
    people who win competitions, I find them to be very pragmatic and well-read. They
    actually have to get things to work. So this paper describes an approach which
    actually worked better than anybody else’s approach and they did these pragmatic
    things like throw away batch norm and use this little scaling factor which almost
    nobody seems to know about. So that’s where .1 comes from.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，除了在一个小时内训练ImageNet的论文中提到过之外，我唯一看到这个提到的地方是在这篇EDSR论文中。这真的很酷，因为赢得比赛的人，我发现他们非常务实和博学。他们实际上必须让事情运转起来。因此，这篇论文描述了一种方法，实际上比任何其他方法都要好，他们做了这些务实的事情，比如放弃批量归一化，使用几乎没有人知道的这个小缩放因子。所以这就是0.1的来源。
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So basically our super-resolution ResNet (`SrResnet`) is going to do a convolution
    to go from our three channels to 64 channels just to richen up the space a little
    bit [[33:25](https://youtu.be/nG3tT31nPmQ?t=33m25s)]. Then also we’ve got actually
    8 not 5 Res blocks. Remember, every one of these Res block is stride 1 so the
    grid size doesn’t change, the number of filters doesn’t change. It’s just 64 all
    the way through. We’ll do one more convolution, and then we’ll do our upsampling
    by however much scale we asked for. Then something I’ve added which is one batch
    norm here because it felt like it might be helpful just to scale the last layer.
    Then finally conv to go back to the three channels we want. So you can see that
    here’s lots and lots of computation and then a little bit of upsampling just like
    we described.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的超分辨率ResNet（`SrResnet`）将进行卷积，从我们的三个通道到64个通道，只是为了稍微丰富一下空间。然后我们实际上有8个而不是5个Res块。请记住，每个Res块的步幅都是1，因此网格大小不会改变，滤波器的数量也不会改变。一直都是64。我们将再做一次卷积，然后根据我们要求的比例进行上采样。然后我添加了一个批量归一化，因为感觉可能有帮助，只是为了缩放最后一层。最后再进行卷积，回到我们想要的三个通道。因此，你可以看到这里有大量的计算，然后稍微进行一些上采样，就像我们描述的那样。
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Just to mention, as I’m tending to do now, this whole thing is done by creating
    a list with layers and then at the end, turning into a sequential model so my
    forward function is as simple as can be.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 只是提一下，就像我现在倾向于做的那样，整个过程是通过创建一个带有层的列表，然后在最后将其转换为一个顺序模型，因此我的前向函数尽可能简单。
- en: Here is our upsampling and upsampling is a bit interesting because it is not
    doing either of two things (transposed or fractionally strided convolutions or
    nearest neighbor upsampling followed by a 1x1 conv). So let’s talk a bit about
    upsampling.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的上采样，上采样有点有趣，因为它既不是转置卷积也不是分数步长卷积，也不是最近邻上采样后跟着1x1卷积。所以让我们稍微谈谈上采样。
- en: Here is the picture from the paper (Perceptual Losses for Real-Time Style Transfer
    and Super Resolution). So they are saying “hey, our approach is so much better”
    but look at their approach. It’s got artifacts in it. These just pop up everywhere,
    don’t they. One of the reason for this is that they use transposed convolutions
    and we all know don’t use transposed convolutions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自论文《用于实时风格转移和超分辨率的感知损失》的图片。所以他们说“嘿，我们的方法好得多”，但看看他们的方法。里面有一些瑕疵。这些瑕疵到处都是，不是吗。其中一个原因是他们使用了转置卷积，我们都知道不要使用转置卷积。
- en: Here are transposed convolutions [[35:39](https://youtu.be/nG3tT31nPmQ?t=35m39s)].
    This is from this fantastic convolutional arithmetic paper that was shown also
    in the Theano docs. If we are going from (blue is the original image) 3x3 image
    up to a 5x5 image (6x6 if we added a layer of padding), then all a transpose convolution
    does is it uses a regular 3x3 conv but it sticks white zero pixels between every
    pair of pixels. That makes the input image bigger and when we run this convolution
    over it, therefore gives us a larger output. But that’s obviously stupid because
    when we get here, for example, of the nine pixels coming in, eight of them are
    zero. So we are just wasting a whole a lot of computation. On the other hand,
    if we are slightly off then four of our nine are non-zero. But yet, we only have
    one filter/kernel to use so it can’t change depending on how many zeros are coming
    in. So it has to be suitable for both and it’s just not possible so we end up
    with these artifacts.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是转置卷积[[35:39](https://youtu.be/nG3tT31nPmQ?t=35m39s)]。这是来自这篇出色的卷积算术论文，也在Theano文档中展示过。如果我们从（蓝色是原始图像）3x3图像升级到5x5图像（如果我们添加了一层填充则为6x6），那么转置卷积所做的就是使用常规的3x3卷积，但它在每对像素之间插入白色零像素。这使得输入图像变大，当我们在其上运行这个卷积时，因此会给我们一个更大的输出。但这显然很愚蠢，因为当我们到达这里时，例如，从九个像素中进入的八个是零。所以我们只是浪费了大量的计算。另一方面，如果我们稍微偏离，那么我们九个中有四个是非零的。但是，我们只有一个滤波器/核来使用，所以它不能根据进入的零的数量而改变。所以它必须适用于两者，这是不可能的，所以我们最终得到这些伪像。
- en: '[http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)'
- en: 'One approach we’ve learnt to make it a bit better is to not put white things
    here but instead to copy the pixel’s value to each of these three locations [[36:53](https://youtu.be/nG3tT31nPmQ?t=36m53s)].
    So that’s a nearest neighbor upsampling. That’s certainly a bit better, but it’s
    still pretty crappy because now when we get to these nine (as shown above), 4
    of them are exactly the same number. And when we move across one, then now we’ve
    got a different situation entirely. So depending on where we are, in particular,
    if we are here, there’s going to be a lot less repetition:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到的一种方法是不要在这里放白色的东西，而是将像素的值复制到这三个位置中的每一个[[36:53](https://youtu.be/nG3tT31nPmQ?t=36m53s)]。所以这是最近邻上采样。这当然好一点，但仍然相当糟糕，因为现在当我们到达这九个（如上所示）时，其中有4个是完全相同的数字。当我们移动一个时，现在我们有了完全不同的情况。所以取决于我们在哪里，特别是，如果我们在这里，重复会少得多：
- en: So again, we have this problem where there’s wasted computation and too much
    structure in the data, and it’s going to lead to artifacts again. So upsampling
    is better than transposed convolutions — it’s better to copy them rather than
    replace them with zero. But it’s still not quite good enough.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以再次，我们有这样一个问题，即存在浪费的计算和数据中的太多结构，这将再次导致伪像。因此，上采样比转置卷积更好——最好复制它们而不是用零替换它们。但这仍然不够好。
- en: So instead, we are going to do the pixel shuffle [[37:56](https://youtu.be/nG3tT31nPmQ?t=37m56s)].
    Pixel shuffle is an operation in this sub-pixel convolutional neural network and
    it’s a little bit mind-bending but it’s kind of fascinating.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将进行像素洗牌[[37:56](https://youtu.be/nG3tT31nPmQ?t=37m56s)]。像素洗牌是这个次像素卷积神经网络中的一个操作，有点令人费解，但却很迷人。
- en: '[**Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel
    Convolutional Neural Network**](https://arxiv.org/abs/1609.05158)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用高效的次像素卷积神经网络进行实时单图像和视频超分辨率**](https://arxiv.org/abs/1609.05158)'
- en: We start with our input, we go through some convolutions to create some feature
    maps for a while until eventually we get to layer *n[i-1]* which has n[i-1] feature
    maps. We are going to do another 3x3 conv and our goal here is to go from a 7x7
    grid cell (we’re going to do a 3x3 upscaling) so we are going to go up to a 21x21
    grid cell. So what’s another way we could do that? To make it simpler, let’s just
    pick one face/layer- so let’s take the top most filter and just do a convolution
    over that just to see what happens. What we are going to do is we are going to
    use a convolution where the kernel size (the number of filters) is nine times
    bigger than we need (strictly speaking). So if we needed 64 filters, we are actually
    going to do 64 times 9 filters. Why? Here, r is the the scale factor so 3² is
    9, so here are the nine filters to cover one of these input layers/slices. But
    what we can do is we started with 7x7, and we turned it into 7x7x9\. The output
    that we want is equal to 7 times 3 by 7 times 3\. In other words, there is an
    equal number of pixels/activations here as there are activations in the previous
    step. So we can literally re-shuffle these 7x7x9 activations to create this 7x3
    by 7x3 map [[40:16](https://youtu.be/nG3tT31nPmQ?t=40m16s)]. So what we are going
    to do is we’re going to take one little tube here (all the top left hand of each
    grid) and we are going to put the purple one up in the top left, then the blue
    one one to the right, and light blue one on to the right of that, then the slightly
    darker one in the middle of the far left, the green one in the middle, and so
    forth. So each of these nine cells in the top left, they are going to end up in
    the little 3x3 section of our grid. Then we are going to take (2, 1) and take
    all of those 9 and more them to these 3x3 part of the grid and so on. So we are
    going to end up having every one of these 7x7x9 activations inside the 7x3 by
    7x3 image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从输入开始，经过一些卷积一段时间，直到最终到达第*n[i-1]*层，其中有n[i-1]个特征图。我们将进行另一个3x3卷积，我们的目标是从一个7x7的网格单元（我们将进行一个3x3的放大），所以我们将扩展到一个21x21的网格单元。那么我们还有另一种方法可以做到这一点吗？为了简化，让我们只选择一个面/层-所以让我们取最顶部的滤波器，只对其进行卷积，看看会发生什么。我们要做的是使用一个卷积，其中卷积核大小（滤波器数量）比我们需要的大九倍（严格来说）。所以如果我们需要64个滤波器，实际上我们要做的是64乘以9个滤波器。为什么？这里，r是比例因子，所以3²是9，这里有九个滤波器来覆盖这些输入层/切片中的一个。但我们可以做的是，我们从7x7开始，然后将其转换为7x7x9。我们想要的输出等于7乘以3乘以7乘以3。换句话说，这里的像素/激活数量与上一步的激活数量相同。所以我们可以重新洗牌这些7x7x9的激活，以创建这个7x3乘以7x3的地图。所以我们要做的是，我们要取这里的一个小管道（所有网格的左上角），我们要把紫色的放在左上角，然后把蓝色的放在右边，淡蓝色的放在右边，稍微深一点的放在最左边的中间，绿色的放在中间，依此类推。所以这些九个单元中的每一个在左上角，它们最终会出现在我们网格的小3x3部分中。然后我们要取（2,1）并将所有这9个移动到网格的这个3x3部分，依此类推。所以我们最终会在7x3乘以7x3的图像中有每一个这些7x7x9的激活。
- en: 'So the first thing to realize is yes of course this works under some definition
    of works because we have a learnable convolution here and it’s going to get some
    gradients which is going to do the best job it can of filling in the correct activation
    such that this output is the thing we want. So the first step is to realize there’s
    nothing particularly magical here. We can create any architecture we like. We
    can move things around anyhow we want to and our weights in the convolution will
    do their best to do all we asked. The real question is — is it good idea? Is this
    an easier thing for it to do and a more flexible thing for it to do than the transposed
    convolution or the upsampling followed by one by one conv? The short answer is
    yes it is, and the reason it’s better in short is that the convolution here is
    happening in the low resolution 7x7 space which is quite efficient. Or else, if
    we first of all upsampled and then did our conv then our conv would be happening
    in the 21 by 21 space which is a lot of computation. Furthermore, as we discussed,
    there’s a lot of replication and redundancy in the nearest neighbor upsample version.
    They actually show in this paper, in fact, I think they have a follow-up technical
    note where they provide some more mathematical details as to exactly what work
    is being done and show that the work really is more efficient this way. So that’s
    what we are going to do. For our upsampling, we have two steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先要意识到的是，当然这在某种定义下是有效的，因为我们这里有一个可学习的卷积，它将得到一些梯度，这些梯度将尽力填充正确的激活，使得输出是我们想要的东西。所以第一步是意识到这里没有什么特别神奇的地方。我们可以创建任何我们喜欢的架构。我们可以随意移动事物，我们想要的方式，我们的卷积中的权重将尽力做到我们要求的一切。真正的问题是——这是一个好主意吗？这是一个更容易做的事情，也是一个更灵活的事情，比转置卷积或上采样后再进行一对一卷积更好吗？简短的答案是是的，原因很简单，因为这里的卷积发生在低分辨率的7x7空间中，这是相当高效的。否则，如果我们首先进行上采样，然后再进行卷积，那么我们的卷积将发生在21x21的空间中，这是很多计算。此外，正如我们讨论过的，最近邻上采样版本中存在很多复制和冗余。实际上，他们在这篇论文中展示了这一点，事实上，我认为他们有一个后续的技术说明，其中提供了更多关于正在进行的工作的数学细节，并展示了这种方式确实更有效。所以这就是我们要做的。对于我们的上采样，我们有两个步骤：
- en: 3x3 conv with *r*² times more channels than we originally wanted
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3x3卷积，比我们最初想要的通道数多*r*²倍
- en: Then a pixel shuffle operation which moves everything in each grid cell into
    the little *r* by *r* grids that are located through out here.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后是一个像素洗牌操作，将每个网格单元中的所有内容移动到遍布其中的小*r*乘以*r*的网格中。
- en: 'So here it is:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是：
- en: It’s one line of code. Here is a conv with number of in to number of filters
    out times four because we are doing a scale two upsample (2²=4). That’s our convolution
    and then here is our pixel shuffle it’s built into PyTorch. Pixel shuffle is the
    thing that moves each thing into its right spot. So that will upsample by a scale
    factor of 2\. So we need to do that log base 2 scale times. If scale is four,
    then we’ll do two times to go two times two. So that’s what this upsample here
    does.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一行代码。这是一个卷积，输入数量到输出数量乘以四，因为我们正在进行一个比例为2的上采样（2²=4）。这是我们的卷积，然后这里是我们的像素洗牌，它内置在PyTorch中。像素洗牌是将每个东西移动到正确位置的东西。因此，这将通过一个比例因子为2进行上采样。所以我们需要做对数以2为底的比例次数。如果比例是四，那么我们将做两次，以便两次两次。这就是这里的上采样所做的事情。
- en: Checkerboard pattern [[44:19](https://youtu.be/nG3tT31nPmQ?t=44m19s)]
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 棋盘格模式[[44:19](https://youtu.be/nG3tT31nPmQ?t=44m19s)]
- en: Great. Guess what. That does not get rid of the checkerboard patterns. We still
    have checkerboard patterns. So I’m sure in great fury and frustration, the same
    team from Twitter I think this is back when they used to be a startup called magic
    pony that Twitter bought came back again with another paper saying okay, this
    time we’ve got rid of the checkerboard.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了。猜猜看。这并没有消除棋盘格模式。我们仍然有棋盘格模式。所以我相信在极度愤怒和沮丧的情况下，来自Twitter团队的同一团队，我认为这是在他们被Twitter收购之前的一个创业公司叫做魔术小马，他们再次回来，发表了另一篇论文，说好吧，这次我们消除了棋盘格。
- en: '[https://arxiv.org/abs/1707.02937](https://arxiv.org/abs/1707.02937)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1707.02937](https://arxiv.org/abs/1707.02937)'
- en: Why do we still have a checkerboard? The reason we still have a checkerboard
    even after doing this is that when we randomly initialize this convolutional kernel
    at the start, it means that each of these 9 pixels in this little 3x3 grid over
    here are going to be totally randomly different. But then the next set of 3 pixels
    will be randomly different to each other but will be very similar to their corresponding
    pixel in the previous 3x3 section. So we are going to have repeating 3x3 things
    all the way across. Then as we try to learn something better, it’s starting from
    this repeating 3x3 starting point which is not what we want. What we actually
    would want is for these 3x3 pixels to be the same to start with. To make these
    3x3 pixels the same, we would need to make these 9 channels the same here for
    each filter. So the solution in this paper is very simple. It’s that when we initialize
    this convolution at start when we randomly initialize it, we don’t totally randomly
    initialize it. We randomly initialize one of the *r*² sets of channels then we
    copy that to the other *r*² so they are all the same. That way, initially, each
    of these 3x3 will be the same. So that is called ICNR and that’s what we are going
    to use in a moment.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们仍然有棋盘格？即使在这样做之后，我们仍然有棋盘格的原因是，当我们在开始时随机初始化这个卷积核时，这意味着这里这个小的3x3网格中的每个9个像素将会完全随机不同。但接下来的3个像素集将彼此随机不同，但将与前一个3x3部分中的相应像素非常相似。所以我们将一直有重复的3x3东西。然后当我们尝试学习更好的东西时，它是从这个重复的3x3起点开始的，这不是我们想要的。实际上，我们想要的是这些3x3像素最初是相同的。为了使这些3x3像素相同，我们需要使每个滤波器的这9个通道在这里相同。因此，这篇论文中的解决方案非常简单。就是当我们在开始时初始化这个卷积时，我们不是完全随机初始化它。我们随机初始化*r*²组通道中的一个，然后将其复制到其他*r*²中，使它们都相同。这样，最初，这些3x3将是相同的。这就是所谓的ICNR，这就是我们马上要使用的。
- en: Pixel loss [[46:41](https://youtu.be/nG3tT31nPmQ?t=46m41s)]
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 像素损失[[46:41](https://youtu.be/nG3tT31nPmQ?t=46m41s)]
- en: Before we do, let’s take a quick look. So we’ve got this super resolution ResNet
    which just does lots of computation with lots of ResNet blocks and then it does
    some upsampling and gets our final three channels out.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们快速看一下。所以我们有这个超分辨率的ResNet，它只是用很多ResNet块进行大量计算，然后进行一些上采样，得到我们最终的三个通道输出。
- en: Then to make life faster, we are going to run tins in parallel. One reason we
    want to run it in parallel is because Gerardo told us that he has 6 GPUs and this
    is what his computer looks like right now.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后为了让生活更快，我们将并行运行这些东西。我们想要并行运行的一个原因是因为Gerardo告诉我们他有6个GPU，这就是他的电脑现在的样子。
- en: So I’m sure anybody who has more than one GPU has had this experience before.
    So how do we get these men working together? All you need to do is to take your
    PyTorch module and wrap it with `nn.DataParallel`. Once you’ve done that, it copies
    it to each of your GPUs and will automatically run it in parallel. It scales pretty
    well to two GPUs, okay to three GPUs, better than nothing to four GPUs and beyond
    that, performance does go backwards. By default, it will copy it to all of your
    GPUs — you can add an array of GPUs otherwise if you want to avoid getting in
    trouble, for example, I have to share our box with Yannet and if I didn’t put
    this here, then she would be yelling at me right now or boycotting my class. So
    this is how you avoid getting into trouble with Yannet.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我相信任何拥有多个GPU的人以前都有过这种经历。那么我们如何让这些设备一起工作呢？你所需要做的就是将你的PyTorch模块包装在`nn.DataParallel`中。一旦你这样做了，它会将它复制到每个GPU，并自动并行运行。它在两个GPU上表现得相当好，三个GPU还可以，四个GPU及以上，性能就会下降。默认情况下，它会将其复制到所有GPU上
    - 你可以添加一个GPU数组，否则如果你想避免麻烦，例如，我必须与Yannet共享我们的盒子，如果我没有把这个放在这里，那么她现在会对我大喊大叫或抵制我的课程。这就是你如何避免与Yannet发生麻烦。
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'One thing to be aware of here is that once you do this, it actually modifies
    your module [[48:21](https://youtu.be/nG3tT31nPmQ?t=48m21s)]. So if you now print
    out your module, let’s say previously it was just an endless sequential, now you’ll
    find it’s an `nn.Sequential` embedded inside a module called `Module`. In other
    words, if you save something which you had `nn.DataParallel` and then tried and
    load it back into something you haven’t `nn.DataParallel`, it’ll say it doesn’t
    match up because one of them is embedded inside this Module attribute and the
    other one isn’t. It may also depend even on which GPU IDs you have had it copy
    to. Two possible solutions:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一件事是，一旦你这样做了，它实际上会修改你的模块[[48:21](https://youtu.be/nG3tT31nPmQ?t=48m21s)]。所以如果你现在打印出你的模块，比如以前它只是一个无限的顺序，现在你会发现它是一个嵌入在一个名为`Module`的模块内部的`nn.Sequential`。换句话说，如果你保存了一个`nn.DataParallel`的东西，然后尝试将其加载到一个没有`nn.DataParallel`的东西中，它会说它不匹配，因为其中一个嵌入在这个Module属性内部，而另一个没有。甚至可能取决于你将其复制到的GPU
    ID。两种可能的解决方案：
- en: Don’t save the module `m` but instead save the module attribute `m.module` because
    that’s actually the non data parallel bit.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要保存模块`m`，而是保存模块属性`m.module`，因为那实际上是非数据并行位。
- en: Always put it on the same GPU IDs and then use data parallel and load and save
    that every time. That’s what I was using.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 始终将其放在相同的GPU ID上，然后使用数据并行，并每次加载和保存。这就是我使用的方法。
- en: This is an easy thing for me to fix automatically in fast.ai and I’ll do it
    pretty soon so it will look for that module attribute and deal with it automatically.
    But for now, we have to do it manually. It’s probably useful to know what’s going
    on behind the scenes anyway.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我来说很容易在fast.ai中自动修复，我很快就会做到，这样它就会自动查找那个模块属性并自动处理。但是现在，我们必须手动操作。了解背后发生的事情可能很有用。
- en: So we’ve got our module [[49:46](https://youtu.be/nG3tT31nPmQ?t=49m46s)]. I
    find it’ll run 50 or 60% faster on a 1080Ti if you are running on volta, it actually
    parallelize a bit better. There are much faster ways to parallelize but this is
    a super easy way.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有了我们的模块[[49:46](https://youtu.be/nG3tT31nPmQ?t=49m46s)]。我发现如果你在1080Ti上运行，它会比较快50%或60%，如果你在volta上运行，它实际上会并行化得更好。有更快的并行化方式，但这是一个超级简单的方式。
- en: We create our learner in the usual way. We can use MSE loss here so that’s just
    going to compare the pixels of the output to the pixels that we expected. We can
    run our learning rate finder and we can train it for a while.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以通常的方式创建我们的学习器。我们可以在这里使用MSE损失，这样就可以比较输出的像素与我们期望的像素。我们可以运行我们的学习率查找器，然后训练一段时间。
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is our input:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输入：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: And here is our output.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输出。
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: And you can see that what we’ve managed to do is to train a very advanced residual
    convolutional network that’s learnt to blue things. Why is that? Well, because
    it’s what we asked for. We said to minimize MSE loss. MSE loss between pixels
    really the best way to do that is just average the pixel i.e. to blur it. So that’s
    why pixel loss is no good. So we want to use our perceptual loss.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们已经成功训练了一个非常先进的残差卷积网络，学会了将事物变蓝。为什么呢？因为这是我们要求的。我们说要最小化MSE损失。像素之间的MSE损失真的最好的方法就是对像素求平均，即模糊化。所以像素损失不好。所以我们要使用我们的感知损失。
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Perceptual loss [50:57]
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知损失[50:57]
- en: With perceptual loss, we are basically going to take our VGG network and just
    like we did last week, we are going to find the block index just before we get
    a maxpool.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用感知损失，我们基本上要拿出我们的VGG网络，就像我们上周做的那样，找到在我们得到最大池之前的块索引。
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: So here are the ends of each block of the same grid size. If we just print them
    out, as we’d expect, every one of those is a ReLU module and so in this case these
    last two blocks are less interesting to us. The grid size there is small enough,
    and course enough that it’s not as useful for super resolution. So we are just
    going to use the first three. Just to save unnecessary computation, we are just
    going to use those first 23 layers of VGG and we’ll throw away the rest. We’ll
    stick it on the GPU. We are not going to be training this VGG model at all — we
    are just using it to compare activations. So we’ll stick it in eval mode and we
    will set it to not trainable.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个相同网格大小块的末尾。如果我们将它们打印出来，正如我们所期望的那样，每一个都是一个ReLU模块，所以在这种情况下，这最后两个块对我们来说不太有趣。那里的网格大小足够小，当然足够小，对于超分辨率来说并不那么有用。所以我们只会使用前三个。为了节省不必要的计算，我们只会使用VGG的前23层，然后丢弃其余的。我们会把它放在GPU上。我们不会训练这个VGG模型——我们只是用它来比较激活。所以我们会将其设置为评估模式，并设置为不可训练。
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Just like last week, we will use `SaveFeatures` class to do a forward hook which
    saves the output activations at each of those layers [[52:07](https://youtu.be/nG3tT31nPmQ?t=52m7s)].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 就像上周一样，我们将使用`SaveFeatures`类来做一个前向钩子，保存每个层的输出激活[[52:07](https://youtu.be/nG3tT31nPmQ?t=52m7s)]。
- en: '[PRE31]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: So now we have everything we need to create our perceptual loss or as I call
    it here `FeatureLoss` class. We are going to pass in a list of layer IDs, the
    layers where we want the content loss to be calculated, and a list of weights
    for each of those layers. We can go through each of those layer IDs and create
    an object which has the forward hook function to store the activations. So in
    our forward, then we can just go ahead and call the forward pass of our model
    with the target (high res image we are trying to create). The reason we do that
    is because that is going to then call that hook function and store in `self.sfs`
    (self dot save features) the activations we want. Now we are going to need to
    do that for our conv net output as well. So we need to clone these because otherwise
    the conv net output is going to go ahead and just clobber what I already had.
    So now we can do the same thing for the conv net output which is the input to
    the loss function. And so now we’ve got those two things we can zip them all together
    along with the weights so we’ve got inputs, targets, and weights. Then we can
    do the L1 loss between the inputs and the targets and multiply by the layer weights.
    The only other thing I do is I also grab the pixel loss, but I weight it down
    quite a bit. Most people don’t do this. I haven’t seen papers that do this, but
    in my opinion, it’s maybe a little bit better because you’ve got the perceptual
    content loss activation stuff but the really finest level it also cares about
    the individual pixels. So that’s our loss function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了创建我们的感知损失或者我在这里称之为`FeatureLoss`类所需的一切。我们将传入一个层ID列表，我们希望计算内容损失的层，以及每个层的权重列表。我们可以遍历每个层ID并创建一个具有前向钩子函数来存储激活的对象。所以在我们的前向传播中，我们可以直接调用模型的前向传播，使用目标（我们试图创建的高分辨率图像）。我们这样做的原因是因为这将调用那个钩子函数并将我们想要的激活存储在`self.sfs`（self点保存特征）中。现在我们还需要对我们的卷积网络输出进行相同的操作。所以我们需要克隆这些，否则卷积网络输出将继续覆盖我已经有的内容。所以现在我们可以对卷积网络输出执行相同的操作，这是损失函数的输入。所以现在我们有了这两个东西，我们可以将它们与权重一起压缩在一起，所以我们有了输入、目标和权重。然后我们可以计算输入和目标之间的L1损失，并乘以层权重。我还做的另一件事是我也获取了像素损失，但我将其权重降低了很多。大多数人不这样做。我没有看到有论文这样做，但在我看来，这可能更好一点，因为你有感知内容损失激活的东西，但在最细微的层面上，它也关心个别像素。所以这就是我们的损失函数。
- en: '[PRE32]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We create our super resolution ResNet telling it how much to scale up by.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建我们的超分辨率ResNet，告诉它要放大多少倍。
- en: '[PRE33]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And then we are going to do our `icnr` initialization of that pixel shuffle
    convolution [[54:27](https://youtu.be/nG3tT31nPmQ?t=54m27s)]. This is very boring
    code, I actually stole it from somebody else. Literally all it does is just say
    okay, you’ve got some weight tensor `x` that you want to initialize so we are
    going to treat it as if it has shape (i.e. number of features) divided by scale
    squared features in practice. So this might be 2² = 4 because we actually want
    to just keep one set of then and then copy them four times, so we divide it by
    four and we create something of that size and we initialize that with, by default,
    `kaiming_normal` initialization. Then we just make scale² copies of it. And the
    rest of it is just kind of moving axes around a little bit. So that’s going to
    return a new weight matrix where each initialized sub kernel is repeated r² or
    `scale`² times. So that details don’t matter very much. All that matters here
    is that I just looked through to find what was the actual conv layer just before
    the pixel shuffle and store it away and then I called `icnr` on its weight matrix
    to get my new weight matrix. And then I copied that new weight matrix back into
    that layer.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将对像素混洗卷积进行`icnr`初始化[[54:27](https://youtu.be/nG3tT31nPmQ?t=54m27s)]。这是非常无聊的代码，实际上我是从别人那里抄的。它实际上只是说，好吧，你有一些权重张量`x`，你想要初始化，所以我们将把它视为具有形状（即特征数量）除以比例平方特征的实际特征。所以这可能是2²
    = 4，因为我们实际上只想保留一组然后将它们复制四次，所以我们除以四并创建一个相同大小的东西，我们用默认的`kaiming_normal`初始化它。然后我们只需复制它的scale²份。其余部分只是稍微移动一下轴。所以这将返回一个新的权重矩阵，其中每个初始化的子核被重复r²或`scale`²次。所以细节并不重要。这里重要的是我只是查找了一下，在像素混洗之前实际的卷积层，并将其存储起来，然后我调用`icnr`来获得我的新权重矩阵。然后我将这个新的权重矩阵复制回那一层。
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, I went to quite a lot of trouble in this exercise to really
    try to implement all the best practices [[56:13](https://youtu.be/nG3tT31nPmQ?t=56m13s)].
    I tend to do things a bit one extreme or the other. I show you a really hacky
    version that only slightly works or I go to the *n*th degree to make it work really
    well. So this is a version where I’m claiming that this is pretty much a state
    of the art implementation. It’s a competition winning or at least my re-implementation
    of a competition winning approach. The reason I’m doing that is because I think
    this is one of those rare papers where they actually get a lot of the details
    right and I want you to get a feel of what it feels like to get all the details
    right. Remember, getting the details right is the difference between the hideous
    blurry mess and the pretty exquisite result.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我在这个练习中费了很大的劲，真的尽力去实现所有最佳实践[[56:13](https://youtu.be/nG3tT31nPmQ?t=56m13s)]。我倾向于做事情有点极端。我向你展示了一个只能勉强工作的非常粗糙的版本，或者我会尽最大努力让它真正运行良好。所以这个版本是我声称这几乎是一个最先进的实现。这是一个获奖的竞赛，或者至少是我重新实现的一个获奖方法。我这样做的原因是因为我认为这是那些实际上把很多细节做对的罕见论文之一，我希望你能感受到把所有细节做对的感觉。记住，把细节做对是区分丑陋模糊混乱和漂亮精致结果之间的区别。
- en: '[PRE35]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: So we are going do DataParallel on that again [[57:14](https://youtu.be/nG3tT31nPmQ?t=57m14s)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们再次对其进行DataParallel[[57:14](https://youtu.be/nG3tT31nPmQ?t=57m14s)]。
- en: '[PRE36]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We are going to set our criterion to be FeatureLoss using our VGG model, grab
    the first few blocks and these are sets of layer weights that I found worked pretty
    well.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们的标准设置为使用我们的VGG模型的FeatureLoss，获取前几个块，这些是我发现效果非常好的一组层权重。
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Do a learning rate finder.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 进行学习率查找。
- en: '[PRE38]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Fit it for a while
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 适应一段时间
- en: '[PRE39]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: And I fiddled around for a while trying to get some of these details right.
    But here is my favorite part of the paper is what happens next. Now that we’ve
    done it for scale equals 2 — progressive resizing. So progressive resizing is
    the trick that let us get the best best single computer result for ImageNet training
    on DAWN bench. It’s this idea of starting small gradually making bigger. I only
    know of two papers that have used this idea. One is the progressive resizing of
    GANs paper which allows training a very high resolution GANs and the other one
    is the EDSR paper. And the cool thing about progressive resizing is not only are
    your earlier epochs, assuming you’ve got 2x2 smaller, four times faster. You can
    also make the batch size maybe 3 or 4 times bigger. But more importantly, they
    are going to generalize better because you are feeding in your model different
    sized images during training. So we were able to train half as many epochs for
    ImageNet as most people. Our epochs were faster and there were fewer of them.
    So progressive resizing is something that, particularly if you are training from
    scratch (I’m not so sure if it’s useful for fine-tuning transfer learning, but
    if you are training from scratch), you probably want to do nearly all the time.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我花了一段时间来尝试弄清楚一些细节。但这里是我最喜欢的论文部分，接下来会发生什么。现在我们已经为尺度等于2做好了准备——渐进式调整大小。渐进式调整大小是让我们在DAWN基准上对ImageNet训练获得最佳单台计算机结果的技巧。这个想法是从小开始逐渐变大。我只知道有两篇论文使用了这个想法。一篇是GANs渐进式调整大小的论文，允许训练非常高分辨率的GANs，另一篇是EDSR论文。渐进式调整大小的酷之处不仅在于，假设你的前几个时期是2x2更小，速度快了四倍。你也可以让批量大小可能增加3或4倍。但更重要的是，它们将更好地泛化，因为在训练过程中你会向模型输入不同尺寸的图像。因此，我们能够为ImageNet训练使用一半的时代，比大多数人快。我们的时代更快，而且数量更少。因此，渐进式调整大小是一种特别适合从头开始训练的东西（我不确定它是否对微调迁移学习有用，但如果你是从头开始训练），你可能几乎想一直这样做。
- en: Progressive resizing [[59:07](https://youtu.be/nG3tT31nPmQ?t=59m7s)]
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 渐进式调整大小
- en: So the next step is to go all the way back to the top and change to 4 scale,
    32 batch size, restart. I saved the model before I do that.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是回到顶部，将尺度改为4，批量大小为32，重新启动。在这样做之前，我保存了模型。
- en: Go back and that’s why there’s a little bit of fussing around in here with reloading
    because what I needed to do now is I needed to load my saved model back in.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 回去，这就是为什么在这里重新加载时会有一点混乱，因为现在我需要做的是重新加载我的保存模型。
- en: But there’s a slight issue which is I now have one more upsampling layer than
    I used to have to go from 2x2 to 4x4\. My loop here is now looping through twice,
    not once. Therefore, it’s added an extra conv net and an extra pixel shuffle.
    So how am I going to load in weights for a different network?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个小问题，就是现在我有一个比以前多的上采样层，从2x2到4x4。我的循环现在循环两次，而不是一次。因此，它添加了一个额外的卷积网络和一个额外的像素混洗。那么我要如何为不同的网络加载权重呢？
- en: The answer is that I use a very handy thing in PyTorch `load_state_dict`. This
    is what `lean.load` calls behind the scenes. If I pass this parameter `strict=False`
    then it says “okay, if you can’t fill in all of the layers, just fill in the layers
    you can.” So after loading the model back in this way, we are going to end up
    with something where it’s loaded in all the layers that it can and that one conv
    layer that’s new is going to be randomly initialized.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是我在PyTorch中使用一个非常方便的东西`load_state_dict`。这就是`lean.load`在幕后调用的内容。如果我传递这个参数`strict=False`，那么它会说“好吧，如果你不能填充所有的层，就填充你能填充的层。”因此，在这种方式下加载模型后，我们将得到一个加载了所有可能层的模型，而那个新的卷积层将被随机初始化。
- en: Then I freeze all my layers and then unfreeze that upsampling part [[1:00:45](https://youtu.be/nG3tT31nPmQ?t=1h45s)]
    Then use `icnr` on my newly added extra layer. Then I can go ahead and learn again.
    So then the rest is the same.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我冻结所有的层，然后解冻那个上采样部分。然后在我新添加的额外层上使用`icnr`。然后我可以继续学习。所以接下来的步骤是一样的。
- en: If you are trying to replicate this, don’t just run this top to bottom. Realize
    it involves a bit of jumping around.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你试图复制这个过程，不要只是从头到尾运行。要意识到这需要有一些跳跃。
- en: '[PRE40]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The longer you train, the better it gets [[1:01:18](https://youtu.be/nG3tT31nPmQ?t=1h1m18s)].
    I ended up training it for about 10 hours, but you’ll still get very good results
    much more quickly if you’re less patient. So we can try it out and and here is
    the result. On the left is my pixelated bird and on the right is the upsampled
    version. It literally invented coloration. But it figured out what kind of bird
    it is, and it knows what these feathers are meant to look like. So it has imagined
    a set of feathers which are compatible with these exact pixels which is genius.
    Same for the back of its head. There is no way you can tell what these blue dots
    are meant to represent. But if you know that this kind of bird has an array of
    feathers here, you know that’s what they must be. Then you can figure out whether
    the feathers would have to be such that when they were pixelated they would end
    up in these spots. So it literally reverse engineered given its knowledge of this
    exact species of bird, how it would have to have looked to create this output.
    This is so amazing. It also knows from all the signs around it that this area
    here (background) was almost certainly blurred out. So it actually reconstructed
    blurred vegetation. If it hadn’t have done all of those things, it wouldn’t have
    gotten such a good loss function. Because in the end, it had to match the activations
    saying “oh, there’s a feather over here and it’s kind of fluffy looking and it’s
    in this direction” and all that.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间越长，效果就越好。我最终训练了大约10个小时，但如果你不那么耐心，仍然可以更快地获得非常好的结果。所以我们可以试一试，这里是结果。左边是我的像素化鸟，右边是放大版本。它实际上发明了着色。但它弄清楚了这是什么鸟，知道这些羽毛应该是什么样子的。因此，它想象出了一组与这些确切像素兼容的羽毛，这是天才。同样适用于头部后面。你无法告诉这些蓝点代表什么。但如果你知道这种鸟在这里有一排羽毛，你就知道它们必须是这样的。然后你可以推断出羽毛必须是这样的，以至于当它们被像素化时它们会出现在这些位置。因此，它根据对这种确切鸟类的了解，逆向工程出了它必须看起来像这样才能创建这个输出。这太神奇了。它还知道周围所有的迹象表明这里（背景）几乎肯定被模糊处理了。因此，它实际上重建了模糊的植被。如果它没有做所有这些事情，它就不会得到如此好的损失函数。因为最终，它必须匹配激活，说“哦，这里有一根羽毛，看起来有点蓬松，朝这个方向”，等等。
- en: '[PRE43]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Well, that brings us to the end of super resolution [[1:03:18](https://youtu.be/nG3tT31nPmQ?t=1h3m18s)].
    Don’t forget to check out the [ask Jeremy anything](http://forums.fast.ai/t/ask-jeremy-anything/15646/1)
    thread.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这就是超分辨率的结束。别忘了查看[向Jeremy提问任何问题](http://forums.fast.ai/t/ask-jeremy-anything/15646/1)的帖子。
- en: Ask Jeremy Anything
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向Jeremy提问
- en: '**Question**: What are the future plans for fast.ai and this course? Will there
    be a part 3? If there is a part 3, I would really love to take it [[1:04:11](https://youtu.be/nG3tT31nPmQ?t=1h4m11s)].'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：fast.ai和这门课程的未来计划是什么？会有第3部分吗？如果有第3部分，我真的很想参加。'
- en: '**Jeremy**: I’m not quite sure. It’s always hard to guess. I hope there will
    be some kind of follow-up. Last year, after part 2, one of the students started
    up a weekly book club going through the Ian Goodfellow deep learning book, and
    Ian actually came in and presented quite a few of the chapters and there was somebody,
    an expert, who presented every chapter. That was a really cool part 3\. To a large
    extent, it will depend on you, the community, to come up with ideas and help make
    them happen, and I’m definitely keen to help. I’ve got a bunch of ideas but I’m
    nervous about saying them because I’m not sure which ones will happen and which
    ones won’t. But the more support I have in making things happen that you want
    to happen from you, the more likely they are to happen.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**：我不太确定。猜测总是很困难的。我希望会有某种后续。去年，在第2部分之后，有一名学生发起了一个每周读书俱乐部，通过Ian Goodfellow的深度学习书籍，Ian实际上进来并介绍了很多章节，还有一个专家，每章节都有人介绍。那是一个非常酷的第3部分。在很大程度上，这将取决于你们社区，提出想法并帮助实现它们，我肯定愿意帮助。我有很多想法，但我对说出来感到紧张，因为我不确定哪些会发生，哪些不会。但如果你们支持我，让你们想要发生的事情发生，那么它们发生的可能性就更大。'
- en: '**Question**: What was your experience like starting down the path of entrepreneurship?
    Have you always been an entrepreneur or did you start at a big company and transition
    to a startup? Did you go from academia to startups or startups to academia? [[1:05:13](https://youtu.be/nG3tT31nPmQ?t=1h5m13s)]'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你创业的经历是怎样的？你一直是创业者吗，还是从大公司开始，然后转向创业公司？你是从学术界转向创业公司，还是从创业公司转向学术界的？'
- en: '**Jeremy**: No, I was definitely not an academia. I am totally a fake academic.
    I started at McKinsey and company which is a strategy firm when I was 18 which
    meant I couldn’t really go to university so it didn’t really turn up. Then spent
    8 years in business helping really big companies on strategic questions. I always
    wanted to be an entrepreneur, planned to only spend two years in McKinsey, only
    thing I really regret in my life was not sticking to that plan and wasting eight
    years instead. So two years would have been perfect. But then I went into entrepreneurship,
    started two companies in Australia. The best part about that was that I didn’t
    get any funding so all the money that I made was mine or the decisions were mine
    and my partner’s. I focused entirely on profit and product and customer and service.
    Whereas I find in San Francisco, I’m glad I came here and so the two of us came
    here for Kaggle, Anthony and I, and raised ridiculous amount of money 11 million
    dollar for this really new company. That was really interesting but it’s also
    really distracting trying to worry about scaling and VC’s wanting to see what
    your business development plans are and also just not having any real need to
    actually make a profit. So I had a bit of the same problem at Enlitic where I
    again raised a lot of money 15 million dollars pretty quickly and a lot of distractions.
    I think trying to bootstrap your own company and focus on making money by selling
    something at a profit and then plowing that back into the company, it worked really
    well. Because within five years, we were making a profit from 3 months in and
    within 5 years, we were making enough for profit not just to pay all of us and
    our own wages but also to see my bank account growing and after 10 years sold
    it for a big chunk of money, not enough that a VC would be excited but enough
    that I didn’t have to worry about money again. So I think bootstrapping a company
    is something which people in the Bay Area at least don’t seem to appreciate how
    good of an idea that is.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**：不，我绝对不是学术界的。我完全是一个假学者。我18岁时在麦肯锡公司开始工作，那是一家战略公司，这意味着我不能真正去大学，所以我也没有去。然后在商界度过了8年，帮助一些大公司解决战略问题。我一直想成为一名企业家，计划只在麦肯锡待两年，我生命中唯一后悔的事情就是没有坚持那个计划，而是浪费了八年。所以两年本来就够了。然后我进入了创业领域，在澳大利亚创办了两家公司。最好的部分是我没有得到任何资金支持，所以我赚的钱都是我的，决策也是我和我的合作伙伴的。我完全专注于利润、产品、客户和服务。而我发现在旧金山，我很高兴来到这里，我和安东尼一起来到这里为Kaggle工作，为这家全新的公司筹集了1100万美元的资金。这真的很有趣，但也很分散注意力，要担心扩张和风险投资者想看到你的业务发展计划，而且根本没有真正需要实现利润。所以在Enlitic，我又遇到了同样的问题，我很快又筹集了1500万美元，分散了很多注意力。我认为尝试自己创业，专注于通过销售盈利并将利润再投入公司，效果非常好。因为在五年内，我们从第三个月开始盈利，五年内，我们的利润足够不仅支付我们所有人的工资，还能看到我的银行账户在增长，十年后以一大笔钱出售，虽然不足以让风险投资者兴奋，但足以让我不再为钱担心。所以我认为自己创业是一个好主意，至少在旧金山的人似乎不太欣赏这个主意。'
- en: '**Question**: If you were 25 years old today and still know what you know where
    would you be looking to use AI? What are you working on right now or looking to
    work on in the next 2 years [[1:08:10](https://youtu.be/nG3tT31nPmQ?t=1h8m10s)]?'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如果你今天25岁，仍然知道你所知道的，你会在哪里寻找使用人工智能的机会？你现在正在做什么，或者在接下来的两年里打算做什么？'
- en: '**Jeremy**: You should ignore the last part of that. I won’t even answer it.
    Doesn’t matter where I’m looking. What you should do is leverage your knowledge
    about your domain. So one of the main reasons we do this is to get people who
    have backgrounds in recruiting, oil field surveys, journalism, activism, whatever
    and solve your problems. It’ll be really obvious to you what real problems are
    and it will be really obvious to you what data you have and where to find it.
    Those are all the bits that for everybody else that’s really hard. So people who
    start out with “oh, I know deep learning now I’ll go and find something to apply
    it to” basically never succeed where else people who are like “oh, I’ve been spending
    25 years doing specialized recruiting for legal firms and I know that the key
    issue is this thing and I know that this piece of data totally solves it and so
    I’m just going to do that now and I already know who to call or actually start
    selling it to”. They are the ones who tend to win. If you’ve done nothing but
    academic stuff, then it’s more maybe about your hobbies and interests. So everybody
    has hobbies. The main thing I would say is please don’t focus on building tools
    for data scientists to use or for software engineers to use because every data
    scientist knows about the market of data scientists whereas only you know about
    the market for analyzing oil survey world or understanding audiology studies or
    whatever it is that you do.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**：你应该忽略那个问题的最后部分。我甚至不会回答它。我在哪里寻找并不重要。你应该利用你对领域的知识。我们这样做的主要原因之一是为了让那些在招聘、油田调查、新闻业、活动主义等领域有背景的人解决问题。对你来说，真正的问题会很明显，你拥有的数据在哪里找也会很明显。对其他人来说，这些都是非常困难的部分。所以那些开始时说“哦，我现在懂深度学习了，我会找一些东西来应用它”的人基本上从来没有成功，而那些像“哦，我已经花了25年专门为法律公司招聘，我知道关键问题是什么，我知道这个数据完全解决了它，所以我现在就去做，我已经知道该打电话给谁或者开始销售了”的人往往会成功。如果你除了学术研究什么都没做过，那可能更多是关于你的爱好和兴趣。每个人都有爱好。我想说的主要是，请不要专注于为数据科学家或软件工程师构建工具，因为每个数据科学家都了解数据科学家的市场，而只有你了解分析油田调查世界或理解听力学研究等你所做的市场。'
- en: '**Question**: Given what you’ve shown us about applying transfer learning from
    image recognition to NLP, there looks to be a lot of value in paying attention
    to all of the developments that happen across the whole ML field and that if you
    were to focus in one area you might miss out on some great advances in other concentrations.
    How do you stay aware of all of the advancements across the field while still
    having time to dig in deep to your specific domains [[1:10:19](https://youtu.be/nG3tT31nPmQ?t=1h10m19s)]?'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：鉴于您向我们展示了如何将迁移学习从图像识别应用到NLP，看起来值得关注整个机器学习领域发生的所有发展，如果您专注于某一领域，可能会错过其他领域的一些重大进展。在深入研究您特定领域的同时，如何保持对整个领域的所有进展的了解？'
- en: '**Jeremy**: Yeah, that’s awesome. I mean that’s one of the key messages of
    this course. Lots of good work’s being done in different places and people are
    so specialized and most people don’t know about it. If I can get state of the
    art results in NLP within six months of starting to look at NLP and I think that
    says more about NLP than it does about me, frankly. It’s kind of like the entrepreneurship
    thing. You pick the areas you see that you know about and kind of transfer stuff
    like “oh, we could use deep learning to solve this problem” or in this case, we
    could use this idea of computer vision to solve that problem. So things like transfer
    learning, I’m sure there’s like a thousand opportunities for you to do in other
    field to do what Sebastian and I did in NLP with NLP classification. So the short
    answer to your question is the way to stay ahead of what’s going on would be to
    follow my feed of Twitter favorites and my approach is to then follow lots and
    lots of people on Twitter and put them into the Twitter favorites for you. Literally,
    every time I come across something interesting, I click favorite. There are two
    reasons I do it. The first is that when the next course comes along, I go through
    my favorites to find which things I want to study. The second is so that you can
    do the same thing. And then which you go deep into, it almost doesn’t matter.
    I find every time I look at something it turns out to be super interesting and
    important. So pick something which you feel like solving that problem would be
    actually useful for some reason and it doesn’t seem to be very popular which is
    kind of the opposite of what everybody else does. Everybody else works on the
    problems which everybody else is already working on because they are the ones
    that seem popular. I can’t quite understand this train of thinking but it seems
    to be very common.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**：是的，这太棒了。我是说这门课程的关键信息之一。在不同地方做了很多好工作，人们都很专业，大多数人都不知道。如果我在开始研究NLP六个月后就能获得最先进的结果，我认为这更多地反映了NLP而不是我。这有点像创业的事情。你选择你了解的领域，然后转移类似“哦，我们可以使用深度学习来解决这个问题”或者在这种情况下，我们可以使用计算机视觉的这个想法来解决那个问题。所以像迁移学习这样的东西，我敢肯定在其他领域有成千上万的机会让你像Sebastian和我在NLP中做NLP分类那样做。所以回答你的问题的简短答案是保持对正在发生的事情的了解的方法是关注我的Twitter收藏夹，我的方法是在Twitter上关注很多人，然后将他们放入你的Twitter收藏夹。每当我遇到有趣的东西时，我都会点击收藏。我这样做的原因有两个。第一个是当下一门课程出现时，我会浏览我的收藏夹，找出我想学习的东西。第二个是为了让你也可以做同样的事情。然后你深入研究的东西几乎无关紧要。我发现每次我看某件事情时，它都会变得非常有趣和重要。所以选择一些你觉得解决那个问题会真正有用的东西，而且似乎并不很受欢迎，这与其他人的做法恰恰相反。其他人都在解决已经受欢迎的问题，因为它们似乎很受欢迎。我无法完全理解这种思维方式，但它似乎非常普遍。'
- en: '**Question**: Is Deep Learning an overkill to use on Tabular data? When is
    it better to use DL instead of ML on tabular data [[1:12:46](https://youtu.be/nG3tT31nPmQ?t=1h12m46s)]?'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在表格数据上使用深度学习是否过度？什么时候最好在表格数据上使用DL而不是ML？'
- en: '**Jeremy**: Is that a real question or did you just put that there so that
    I would point out that Rachel Thomas just wrote an article? [http://www.fast.ai/2018/04/29/categorical-embeddings/](http://www.fast.ai/2018/04/29/categorical-embeddings/)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**：这是一个真正的问题，还是你只是放在那里让我指出Rachel Thomas刚写了一篇文章？[http://www.fast.ai/2018/04/29/categorical-embeddings/](http://www.fast.ai/2018/04/29/categorical-embeddings/)'
- en: So Rachel has just written about this and Rachel and I spent a long time talking
    about it and the short answer is we think it’s great to use deep learning on tabular
    data. Actually, of all the rich complex important and interesting things that
    appear in Rachel’s Twitter stream covering everything from the genocide of Rohingya
    through to latest ethics violations in AI companies, the one by far that got the
    most attention and engagement from the community was the question about is it
    called tabular data or structured data. So yeah, ask computer people how to name
    things and you’ll get plenty of interest. There are some really good links here
    to stuff from Instacart and Pinterest and other folks who have done some good
    work in this area. Any of you that went to the Data Institute conference would
    have seen Jeremy Stanley’s presentation about the really cool work they did at
    Instacart.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 所以Rachel刚刚写了这篇文章，Rachel和我花了很长时间讨论这个问题，简短的答案是我们认为在表格数据上使用深度学习是很棒的。实际上，在Rachel的Twitter流中出现的所有丰富复杂重要和有趣的事情中，从罗兴亚种族灭绝到AI公司最新的伦理违规行为，迄今为止引起社区最多关注和参与的是有关表格数据或结构化数据的问题。所以是的，问计算机人如何命名事物，你会得到很多兴趣。这里有一些来自Instacart和Pinterest以及其他一些在这一领域做出了一些出色工作的人的链接。如果你们中有人参加了数据研究所的会议，就会看到Jeremy
    Stanley在Instacart做的非常酷的工作的演示。
- en: '**Rachel**: I relied heavily on lessons 3 and 4 from part 1 in writing this
    post so much of that may be familiar to you.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**Rachel**：我在撰写这篇文章时主要依赖于第1部分的第3和第4课，因此其中的许多内容可能对您来说很熟悉。'
- en: '**Jeremy**: Rachel asked me during the post like how to tell whether you should
    use the decision tree ensemble like GBM or random forest or neural net and my
    answer is I still don’t know. Nobody I’m aware of has done that research in any
    particularly meaningful way. So there’s a question to be answered there, I guess.
    My approach has been to try to make both of those things as accessible as possible
    through fast.ai library so you can try them both and see what works. That’s what
    I do.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**: Rachel在后面问我如何判断是否应该使用决策树集成，如GBM或随机森林，还是神经网络，我的答案是我仍然不知道。据我所知，没有人以任何特别有意义的方式进行过这方面的研究。所以这里有一个需要回答的问题，我想。我的方法是尽可能通过fast.ai库使这两种方法都尽可能易于使用，这样你就可以尝试它们并看看哪种方法有效。这就是我做的。'
- en: '**Question**: Reinforcement Learning popularity has been on a gradual rise
    in the recent past. What’s your take on Reinforcement Learning? Would fast.ai
    consider covering some ground in popular RL techniques in the future [[1:15:21](https://youtu.be/nG3tT31nPmQ?t=1h15m21s)]?'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**: 强化学习在最近逐渐受到关注。你对强化学习有什么看法？fast.ai是否考虑在未来涵盖一些流行的强化学习技术？'
- en: '**Jeremy**: I’m still not a believer in reinforcement learning. I think it’s
    an interesting problem to solve but it’s not at all clear that we have a good
    way of solving this problem. So the problem, it really is the delayed credit problem.
    So I want to learn to play pong, I’ve moved up or down and three minutes later
    I find out whether I won the game of pong — which actions I took were actually
    useful? So to me, the idea of calculating the gradients of the output with respect
    to those inputs, the credit is so delayed that those derivatives don’t seem very
    interesting. I get this question quite regularly in every one of these four courses
    so far. I’ve always said the same thing. I’m rather pleased that finally recently
    there’s been some results showing that actually basically random search often
    does better than reinforcement learning so basically what’s happened is very well-funded
    companies with vast amounts of computational power throw all of it at reinforcement
    learning problems and get good results and people then say “oh it’s because of
    the reinforcement learning” rather than the vast amounts of compute power. Or
    they use extremely thoughtful and clever algorithms like a combination of convolutional
    neural nets and Monte Carlo tree search like they did with the Alpha Go stuff
    to get great results and people incorrectly say “oh that’s because of reinforcement
    learning” when it wasn’t really reinforcement learning at all. So I’m very interested
    in solving these kind of more generic optimization type problems rather than just
    prediction problems and that’s what these delayed credit problems tend to look
    like. But I don’t think we’ve yet got good enough best practices that I have anything
    on, ready to teach and say I’ve got to teach you this thing because I think it’s
    still going to be useful next year. So we’ll keep watching and see what happens.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**: 我仍然不相信强化学习。我认为解决这个问题是一个有趣的问题，但我们并没有一个很好的解决这个问题的方法。问题实际上是延迟奖励问题。所以我想学会玩乒乓球，我向上或向下移动，三分钟后我才知道我是否赢得了乒乓球比赛——我采取的哪些行动实际上是有用的？对我来说，计算输出相对于这些输入的梯度，奖励是如此延迟，以至于这些导数似乎并不那么有趣。到目前为止，在我所教授的四门课程中，我经常被问到这个问题。我总是说同样的话。我很高兴最近终于有一些结果表明，实际上基本上随机搜索往往比强化学习做得更好，所以基本上发生的情况是，拥有大量计算能力的资金充裕的公司将所有资源投入到强化学习问题中，并取得了良好的结果，然后人们就会说“这是因为强化学习”，而不是因为大量的计算资源。或者他们使用非常周到和聪明的算法，比如卷积神经网络和蒙特卡洛树搜索的组合，就像他们在Alpha
    Go项目中所做的那样取得了很好的结果，人们错误地说“这是因为强化学习”，而实际上根本不是强化学习。所以我对解决这些更通用的优化问题非常感兴趣，而不仅仅是预测问题，这些延迟奖励问题看起来就是这样。但我认为我们还没有得到足够好的最佳实践，我没有任何准备好教授的东西，也没有说我必须教你这个东西，因为我认为明年它仍然会有用。所以我们将继续观察并看看会发生什么。'
- en: Super resolution network to a style transfer network [[1:17:57](https://youtu.be/nG3tT31nPmQ?t=1h17m57s)]
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超分辨率网络转换为风格转移网络[[1:17:57](https://youtu.be/nG3tT31nPmQ?t=1h17m57s)]
- en: We are going to now turn the super resolution network into a style transfer
    network. And we’ll do this pretty quickly. We basically already have something.
    *x* is my input image and I’m going to have some loss function and I’ve got some
    neural net again. Instead of a neural net that does a whole a lot of compute and
    then does upsampling at the end, our input this time is just as big as our output.
    So we are going to do some downsampling first. Then our computer, and then our
    upsampling. So that’s the first change we are going to make — we are going to
    add some downsampling so some stride 2 convolution layers to the front of our
    network. The second is rather than just comparing *yc* and *x* are the same thing
    here. So we are going to basically say our input image should look like itself
    by the end. Specifically we are going to compare it by chucking it through VGG
    and comparing it at one of the activation layers. And then its style should look
    like some painting which we’ll do just like we did with the Gatys’ approach by
    looking at the Gram matrix correspondence at a number of layers. So that’s basically
    it. So that ought to be super straight forward. It’s really combining two things
    we’ve already done.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要把超分辨率网络转换为风格转移网络。我们会很快地完成这个过程。我们基本上已经有了一些东西。*x*是我的输入图像，我将有一些损失函数和一些神经网络。这次我们的输入和输出大小是一样的，所以我们要先做一些下采样。然后是计算，最后是上采样。这是我们要做的第一个改变——我们要在网络的前面添加一些下采样，也就是一些步幅为2的卷积层。第二个改变是，不再只是比较*yc*和*x*是否相同。我们基本上要说我们的输入图像应该在最后看起来像它自己。具体来说，我们将通过VGG将其传递并在其中一个激活层进行比较。然后它的风格应该看起来像一幅画，我们将像我们用Gatys的方法那样通过查看多个层的Gram矩阵对应来实现。基本上就是这样。这应该非常简单明了。这实际上是将我们已经做过的两件事结合在一起。
- en: Style transfer net [[1:19:19](https://youtu.be/nG3tT31nPmQ?t=1h19m19s)]
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风格转移网络
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/style-transfer-net.ipynb)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/style-transfer-net.ipynb)'
- en: So all this code starts identical, except we don’t have high res and low res,
    we just have one size 256.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 所以所有这些代码都是相同的，除了我们没有高分辨率和低分辨率，我们只有一个尺寸为256。
- en: '[PRE44]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Model [[1:19:30](https://youtu.be/nG3tT31nPmQ?t=1h19m30s)]
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: My model is the same. One thing I did here is I did not do any kind of fancy
    best practices for this one at all. Partly because there doesn’t seem to be any.
    There’s been very little follow up in this approach compared to the super resolution
    stuff. We’ll talk about why in a moment. So you’ll see, this is much more normal
    looking.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我的模型是一样的。这里我做的一件事是我没有使用任何花哨的最佳实践。部分原因是因为似乎没有。与超分辨率的研究相比，对这种方法的跟进非常少。我们稍后会讨论原因。所以你会看到，这看起来更加正常。
- en: '[PRE45]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: I’ve got batch norm layers. I don’t have scaling factor here.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我有批量归一化层。这里没有缩放因子。
- en: '[PRE46]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: I don’t have a pixel shuffle — it’s just using a normal upsampling followed
    by 1x1 conf. So it’s just more normal.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有像素混洗 —— 只是使用正常的上采样，然后是1x1的卷积。所以这只是更正常的。
- en: '[PRE47]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: One thing they mentioned in the paper is they had a lot of problems with zero
    padding creating artifacts and the way they solved that was by adding 40 pixel
    of reflection padding at the start. So I did the same thing and then they used
    zero padding in their convolutions in their Res blocks. Now if you’ve got zero
    padding in your convolutions in your Res blocks, then that means that the two
    parts of your ResNet won’t add up anymore because you’ve lost a pixel from each
    side on each of your two convolutions. So my `ResSequential` has become `ResSequentialCenter`
    and I’ve removed the last 2 pixels on each side of those good cells. Other than
    that, this is basically the same as what we had before.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在论文中提到的一件事是他们在零填充中遇到了很多问题，他们解决这个问题的方法是在开始时添加40像素的反射填充。所以我也做了同样的事情，然后他们在他们的Res块中的卷积中使用了零填充。现在如果你的Res块中的卷积中有零填充，那么你的ResNet的两部分将不再相加，因为你在每个卷积的每一侧都失去了一个像素。所以我的`ResSequential`变成了`ResSequentialCenter`，我去掉了那些好细胞的每一侧的最后2个像素。除此之外，这基本上和以前一样。
- en: '[PRE48]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Style Image [[1:21:02](https://youtu.be/nG3tT31nPmQ?t=1h21m2s)]
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风格图像
- en: So then we can bring in our starry night picture.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以引入我们的星夜图片。
- en: '[PRE49]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can resize it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整大小。
- en: '[PRE51]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We can throw it through our transformations
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过我们的变换
- en: '[PRE52]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Just to make the method a little bit easier for my brain to handle, I took our
    transform style image which after transformations of 3 x 256 x 256, and I made
    a mini batch. My batch size is 24 — 24 copies of it. It just maeks it a little
    bit easier to do the kind of batch arithmetic without worrying about some of the
    broadcasting. They are not really 24 copies. I used `np.broadcast` to basically
    fake 24 pieces.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我的大脑更容易处理这种方法，我拿出了我们的变换风格图像，经过3 x 256 x 256的变换后，我制作了一个小批量。我的批量大小是24 — 有24个副本。这样做可以更容易地进行批量算术，而不用担心一些广播问题。它们实际上并不是24个副本。我使用`np.broadcast`基本上伪造了24个部分。
- en: '[PRE53]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Perceptual loss [[1:21:51](https://youtu.be/nG3tT31nPmQ?t=1h21m51s)]
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知损失
- en: So just like before, we create a VGG, grab the last block. This time we are
    going to use all of these layers so we keep everything up to the 43rd layer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 所以就像以前一样，我们创建了一个VGG，抓住了最后一个块。这一次我们要使用所有这些层，所以我们保留了所有直到第43层的内容。
- en: '[PRE54]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: So now our combined loss is going to add together a content loss for the third
    block plus the Gram loss for all of our blocks with different weights. Again,
    going back to everything being as normal as possible, I’ve gone back to using
    MSE above. Basically what happened was I had a lot of trouble getting this to
    train properly. So I gradually removed trick after trick and eventually just went
    “ok, I’m just gonna make it as bland as possible”.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们的组合损失将加上第三个块的内容损失，再加上所有块的Gram损失，使用不同的权重。再次回到尽可能正常的一切，我又回到了使用均方误差。基本上发生的事情是我在训练这个模型时遇到了很多困难。所以我逐渐去掉了一个又一个技巧，最终只是说“好吧，我只会让它尽可能平淡”。
- en: Last week’s Gram matrix was wrong, by the way [[1:22:37](https://youtu.be/nG3tT31nPmQ?t=1h22m37s)].
    It only worked for a batch size of one and we only had a batch size of one so
    that was fine. I was using matrix multiply which meant that every batch was being
    compared to every other batch. You actually need to use batch matrix multiple
    (`torch.bmm`) which does a matrix multiply per batch. So that’s something to be
    aware of there.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 上周的Gram矩阵是错误的。它只适用于批量大小为1，而我们只有一个批量大小，所以没问题。我使用的是矩阵乘法，这意味着每个批次都与其他每个批次进行比较。实际上，你需要使用批量矩阵乘法（`torch.bmm`），它对每个批次执行矩阵乘法。所以这是需要注意的一点。
- en: '[PRE55]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: So I’ve got Gram matrices, I do my MSE loss between the Gram matrices, I weight
    them by style weights, so I create that ResNet.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我有Gram矩阵，我在Gram矩阵之间进行均方误差损失，我用风格权重对它们进行加权，所以我创建了那个ResNet。
- en: '[PRE56]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: I create my combined loss passing in the VGG network, passing in the block IDs,
    passing in the transformed starry night image, and you’ll see the the very start
    here, I do a forward pass through my VGG model with that starry night image in
    order that I can save the features for it. Notice, it’s really important now that
    I don’t do any data augmentation because I’ve saved the style features for a particular
    non-augmented version. So if I augmented it, it might make some minor problems.
    But that’s fine because I’ve got all of ImageNet to deal with. I don’t really
    need to do data augmentation anyway.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了我的组合损失，传入VGG网络，传入块ID，传入变换后的星夜图像，你会看到这里的开始，我通过我的VGG模型进行了前向传递，以保存其特征。请注意，现在非常重要的是我不做任何数据增强，因为我保存了特定未增强版本的风格特征。所以如果我增强它，可能会出现一些小问题。但没关系，因为我有所有的ImageNet要处理。我实际上不需要做数据增强。
- en: '[PRE57]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: So I’ve got my loss function and I can go ahead and fit [[1:24:06](https://youtu.be/nG3tT31nPmQ?t=1h24m6s)].
    And there is nothing clever here at all.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我有我的损失函数，我可以继续拟合[[1:24:06](https://youtu.be/nG3tT31nPmQ?t=1h24m6s)]。这里一点聪明的地方都没有。
- en: '[PRE59]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: At the end, I have my `sum_layers=False` so I can see what each part looks like
    and see they are balanced. And I can finally pop it out
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我有我的`sum_layers=False`，这样我就可以看到每个部分的样子，看到它们是平衡的。然后我终于可以弹出它
- en: '[PRE60]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: So I mentioned that should be pretty easy and yet it took me about 4 days because
    I just found this incredibly fiddly to actually get it to work [[1:24:26](https://youtu.be/nG3tT31nPmQ?t=1h24m26s)].
    So when I finally got up in the morning I said to Rachel “guess what, it trained
    correctly.” Rachel said “I never thought that was going to happen.” It just looked
    awful all the time and it’s really about getting the exact right mix of content
    loss and a style loss and the mix of the layers of the style loss. The worst part
    was it takes a really long time to train the darn CNN and I didn’t really know
    how long to train it before I decided it wasn’t doing well. Should I just train
    it for longer? And I don’t know all the little details didn’t seem to slightly
    change it but just it would totally fall apart all the time. So I kind of mentioned
    this partly to say just remember the final answer you see here is after me driving
    myself crazy all week of nearly always not working until finally the last minute
    it finally does. Even for things which just seemed like they couldn’t possibly
    be difficult because that is combining two things we already have working. The
    other is to be careful about how we interpret what authors claim.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我提到这应该很容易，但实际上花了我大约4天，因为我发现这个真的很麻烦，才让它正常工作[[1:24:26](https://youtu.be/nG3tT31nPmQ?t=1h24m26s)]。所以当我终于早上起床时，我对Rachel说“猜猜，它训练正确了。”
    Rachel说“我从来没想过会发生这种事。” 它看起来一直很糟糕，真的是关于得到精确的内容损失和风格损失的混合以及风格损失的层次的混合。最糟糕的部分是训练这个该死的CNN需要很长时间，我真的不知道应该训练多久才能确定它表现不佳。我应该只是继续训练吗？我不知道所有这些细节似乎都没有稍微改变它，但它总是会完全崩溃。所以我提到这部分是为了提醒大家，最终你在这里看到的答案是在我整整一周把自己逼疯几乎总是不起作用，直到最后一刻它终于起作用。即使对于那些看起来不可能困难的事情，因为那是将两个我们已经有的工作结合在一起。另一个是要小心解释作者声称的内容。
- en: It was so fiddly getting this style transfer to work [[1:26:10](https://youtu.be/nG3tT31nPmQ?t=1h26m10s)].
    After doing it, it left me thinking why did I bother because now I’ve got something
    that takes hours to create a network that can turn any kind of photo into one
    specific style. It just seems very unlikely I would want that for anything. The
    only reason I could think that being useful would be to do some art-y stuff on
    a video where I wanted to turn every frame into some style. It’s incredibly niche
    thing to want to do. But when I looked at the paper, the table is saying “oh,
    we are a thousand times faster than the Gatys’ approach which is just such an
    obviously meaningless thing to say. Such an incredibly misleading thing to say
    because it ignores all the hours of training for each individual style and I find
    this frustrating because groups like this Stanford group clearly know better or
    ought to know better, but still I guess the academic community encourages people
    to make these ridiculously grand claims. It also completely ignores this incredibly
    sensitive fiddly training process so this paper was just so well accepted when
    it came out. I remember everybody getting on Twitter and saying “wow, you know
    these Stanford people have found this way of doing style transfer a thousand times
    faster.” And clearly people saying this were top researchers in the field, clearly
    none of them actually understood it because nobody said “I don’t see why this
    is remotely useful, and also I tried it and it was incredibly fiddly to get it
    all to work.” It’s not until 18 months later I finally coming back to it and kind
    of thinking like “wait a minute, this is kind of stupid.” So this is the answer,
    I think, to the question of why haven’t people done follow ups on this to create
    really amazing best practices and better approaches like with a super resolution
    part of the paper. And I think the answer is because it’s dumb. So I think super
    resolution part of the paper is clearly not dumb. And it’s been improved and improved
    and now we have great super resolution. And I think we can derive from that great
    noise reduction, great colorization, great slant removal, great interactive artifact
    removal, etc. So I think there’s a lot of really cool techniques here. It’s also
    leveraging a lot of stuff that we’ve been learning and getting better and better
    at.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让这个风格转移起作用真的很麻烦[[1:26:10](https://youtu.be/nG3tT31nPmQ?t=1h26m10s)]。做完之后，我想为什么我要费这个劲，因为现在我有了一个需要花几个小时来创建一个可以将任何类型的照片转换为一个特定风格的网络。我觉得我很少会想要这样做。我能想到这有用的唯一原因是在视频上做一些艺术性的东西，我想把每一帧都转换成某种风格。这是一个极其狭隘的想法。但当我看了论文后，表格上写着“哦，我们比Gatys的方法快一千倍”，这种说法显然毫无意义。这是一个极其误导人的说法，因为它忽略了为每种风格进行的所有训练时间，我发现这很令人沮丧，因为像斯坦福这样的团体显然更清楚或应该更清楚，但仍然我猜学术界鼓励人们提出这些荒谬的夸大宣称。它也完全忽视了这个极其敏感的繁琐的训练过程，所以这篇论文一出来就被如此广泛接受。我记得每个人都在推特上说“哇，你知道这些斯坦福的人找到了这种方式可以让风格转移快一千倍。”
    显然说这话的人是该领域的顶尖研究人员，显然他们中没有人真正理解，因为没有人说“我不明白为什么这有任何用处，而且我尝试过，让它正常工作真的很麻烦。” 直到18个月后，我最终回头看，有点想“等一下，这有点愚蠢。”
    所以我认为这就是为什么人们没有对此进行后续研究，以创造真正令人惊叹的最佳实践和更好的方法，就像论文中的超分辨率部分一样。我认为答案是因为这很愚蠢。所以我认为论文中的超分辨率部分显然不愚蠢。它已经得到改进，现在我们有了很棒的超分辨率。我认为我们可以从中得到很棒的降噪、很棒的着色、很棒的倾斜去除、很棒的交互式伪影去除等等。所以我认为这里有很多很酷的技术。它还利用了我们一直在学习和不断进步的许多东西。
- en: Segmentation [[1:29:13](https://youtu.be/nG3tT31nPmQ?t=1h29m13s)]
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割[[1:29:13](https://youtu.be/nG3tT31nPmQ?t=1h29m13s)]
- en: Finally, let’s talk about segmentation. This is from the famous CamVid dataset
    which is a classic example of an academic segmentation dataset. Basically you
    can see what we do is we start with a picture (they are actually video frames
    in this dataset) and we have some labels where they are not actually colors —
    each one has an ID and the IDs are mapped to colors. So red might be 1, purple
    might be 2, light pink might be 3 and so all the buildings are one class, all
    the cars are another class, all the people are another class, all the road is
    another class, and so on. So what we are actually doing here is multi-class classification
    for every pixel. You can see, sometimes that multi-class classification really
    is quite tricky — like these branches. Although, sometimes the labels are really
    not that great. This is very coarse as you can see. So that’s what we are going
    to do.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们谈谈分割。这来自著名的CamVid数据集，这是一个学术分割数据集的经典示例。基本上你可以看到我们的做法是从一幅图片开始（实际上在这个数据集中是视频帧），我们有一些标签，它们实际上不是颜色
    - 每个标签都有一个ID，这些ID映射到颜色。所以红色可能是1，紫色可能是2，浅粉色可能是3，等等。所以所有建筑物属于一类，所有汽车属于另一类，所有人属于另一类，所有道路属于另一类，依此类推。所以我们实际上在这里为每个像素进行多类分类。你可以看到，有时多类分类确实非常棘手
    - 就像这些分支。尽管有时标签实际上并不是那么好。正如你所看到的，这非常粗糙。这就是我们要做的。
- en: We are going to do segmentation and so it’s a lot like bounding boxes. But rather
    than just finding a box around each thing, we are actually going to label every
    single pixel with its class. Really, it’s actually a lot easier because it fits
    our CNN style so nicely that we can create any CNN where the output is an N by
    M grid containing the integers from 0 to C where there are C categories. And then
    we can use cross-entropy loss with a softmax activation and we are done. I could
    actually stop the class there and you can go and use exactly the same approaches
    you’ve learnt in lesson 1 and 2 and you’ll get a perfectly okay result. So the
    first thing to say is this is not actually a terribly hard thing to do. But we
    are going to try and do it really well.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行分割，所以这很像边界框。但与其只是找到每个物体周围的框，我们实际上要为每个像素标记其类别。实际上，这实际上要容易得多，因为它非常适合我们的CNN风格，我们可以创建任何输出为N乘以M网格的CNN，其中包含从0到C的整数，其中C是类别数。然后我们可以使用softmax激活的交叉熵损失，然后就完成了。我实际上可以在这里停止课程，你可以使用在第1和第2课中学到的完全相同的方法，你会得到一个完全可以接受的结果。所以首先要说的是，这实际上并不是一件非常困难的事情。但我们将尽力做得更好。
- en: Doing it the simple way [[1:31:26](https://youtu.be/nG3tT31nPmQ?t=1h31m26s)]
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以简单的方式进行[[1:31:26](https://youtu.be/nG3tT31nPmQ?t=1h31m26s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/carvana.ipynb)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/carvana.ipynb)'
- en: Let’s start by doing it the really simple way. And we are going to use Kaggle
    [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge) competition
    and you can download it with Kaggle API as usual.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的方式开始。我们将使用Kaggle [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge)比赛，你可以像往常一样使用Kaggle
    API下载它。
- en: '[PRE61]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Setup
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: There is a train folder containing bunch of images which is the independent
    variable and a train_masks folder there’s the dependent variable and they look
    like below.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个包含一堆图像的训练文件夹，这是自变量，还有一个train_masks文件夹，这是因变量，它们看起来像下面这样。
- en: In this case, just like cats and dogs, we are going simple rather than doing
    multi-class classification, we are going to do binary classification. But of course
    multi-class is just the more general version — categorical cross entropy or binary
    class entropy. There is no differences conceptually, so the dependent variable
    is just zeros and ones, where else the independent variable is a regular image.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，就像猫和狗一样，我们选择简单的方式，而不是进行多类分类，我们将进行二元分类。但当然，多类分类只是更一般的版本 - 分类交叉熵或二元分类熵。在概念上没有区别，因此因变量只是零和一，而自变量是常规图像。
- en: In order to do this well, it would really help to know what cars look like.
    Because really what we want to do is to figure out this is a car and its orientation
    and put white pixels where we expect the car to be based on the picture and their
    understanding of what cars look like.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，真的很有帮助知道汽车是什么样子的。因为我们真正想做的是弄清楚这是一辆车，以及它的方向，并根据图片和他们对汽车外观的理解，在我们期望汽车出现的地方放置白色像素。
- en: '[PRE62]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The original dataset came with these CSV files as well [[1:32:44](https://youtu.be/nG3tT31nPmQ?t=1h32m44s)].
    I don’t really use them for very much other than getting the list of images from
    them.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集还附带了这些CSV文件[[1:32:44](https://youtu.be/nG3tT31nPmQ?t=1h32m44s)]。我实际上并没有用它们做很多其他事情，只是从中获取图像列表。
- en: '[PRE63]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Each image after the car ID has a 01, 02, etc of which I’ve printed out all
    16 of them for one car and as you can see basically those numbers are the 16 orientations
    of one car [[1:32:58](https://youtu.be/nG3tT31nPmQ?t=1h32m58s)]. I don’t think
    anybody in this competition actually used these orientation information. I believe
    they all kept the car’s images just treated them separately.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图片在车辆ID之后都有一个01、02等，我已经打印出其中一个车辆的所有16个方向，正如你所看到的，基本上这些数字是一个车辆的16个方向[[1:32:58](https://youtu.be/nG3tT31nPmQ?t=1h32m58s)]。我认为在这个比赛中没有人实际上使用这些方向信息。我相信他们都保留了车辆的图像，只是单独处理它们。
- en: '[PRE66]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Resize and convert [[1:33:27](https://youtu.be/nG3tT31nPmQ?t=1h33m27s)]
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整大小和转换[[1:33:27](https://youtu.be/nG3tT31nPmQ?t=1h33m27s)]
- en: These images are pretty big — over 1000 by 1000 in size and just opening the
    JPEGs and resizing them is slow. So I processed them all. Also OpenCV can’t handle
    GIF files so I converted them.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像非常大 - 大小超过1000乘以1000，只是打开JPEG并调整它们的大小很慢。所以我对它们进行了处理。此外，OpenCV无法处理GIF文件，因此我对它们进行了转换。
- en: '**Question**: How would somebody get these masks for training initially? [Mechanical
    turk](https://www.mturk.com/) or something [[1:33:48](https://youtu.be/nG3tT31nPmQ?t=1h33m48s)]?
    Yeah, just a lot of boring work. Probably there are some tools that help you with
    a bit of edge snapping so that the human can do it roughly and then just fine
    tune the bits it gets wrong. These kinds of labels are expensive. So one of the
    things I really want to work on is deep learning enhanced interactive labeling
    tools because that’s clearly something that would help a lot of people.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：有人最初如何获得这些用于训练的蒙版？[Mechanical turk](https://www.mturk.com/)或其他什么[[1:33:48](https://youtu.be/nG3tT31nPmQ?t=1h33m48s)]？是的，只是很多无聊的工作。可能有一些工具可以帮助你进行一些边缘捕捉，这样人类可以粗略地完成，然后只需微调它错误的部分。这种标签是昂贵的。所以我真正想要做的事情之一是增强深度学习交互式标注工具，因为这显然是可以帮助很多人的事情。'
- en: I’ve got a little section here that you can run if you want to. You probably
    want to. It converts the GIFs into PNGs so just open int up with PIL and then
    save it as PNG because OpenCV doesn’t have GIF support. As per usual for this
    kind of stuff, I do it with a ThreadPool so I can take advantage of parallel processing.
    And then also create a separate directory `train-128` and `train_masks-128` which
    contains the 128 by 128 resized versions of them.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我这里有一个小节，如果你想的话可以运行。你可能想要。它将GIF转换为PNG，所以只需用PIL打开它，然后保存为PNG，因为OpenCV不支持GIF。像往常一样，对于这种类型的东西，我使用线程池，这样我就可以利用并行处理。然后创建一个单独的目录`train-128`和`train_masks-128`，其中包含它们的128x128调整大小版本。
- en: This is the kind of stuff that keeps you sane if you do it early in the process.
    So anytime you get a new dataset, seriously think about creating a smaller version
    to make life fast. Anytime you find yourself waiting on your computer, try and
    think of a way to create a smaller version.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在过程早期进行的工作，可以让你保持理智的工作。所以每当你获得新的数据集时，认真考虑创建一个较小的版本以加快速度。每当你发现自己在电脑上等待时，尝试想出一种创建较小版本的方法。
- en: '[PRE67]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: So after you grab it from Kaggle, you probably want to run this stuff, go away,
    have lunch, come back and when you are done, you’ll have these smaller directories
    which we are going to use below 128 by 128 to start with.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在你从Kaggle获取它之后，你可能想要运行这些东西，离开，吃午餐，回来时，当你完成时，你将拥有这些较小的目录，我们将从128x128开始使用。
- en: Dataset [[1:35:33](https://youtu.be/nG3tT31nPmQ?t=1h35m33s)]
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集[[1:35:33](https://youtu.be/nG3tT31nPmQ?t=1h35m33s)]
- en: '[PRE68]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: So here is a cool trick. If you use the same axis object (`ax`) to plot an image
    twice and the second time you use alpha which you might know means transparency
    in the computer vision world, then you can actually plot the mask over the top
    of the photo. So here is a nice way to see all the masks on top of the photos
    for all of the cars in one group.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个很酷的技巧。如果你使用相同的轴对象（`ax`）两次绘制图像，第二次使用alpha，你可能知道在计算机视觉世界中意味着透明度，那么你实际上可以在照片的顶部绘制蒙版。这是一个很好的方法，可以看到所有车辆组中所有照片顶部的所有蒙版。
- en: '[PRE69]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This is the same MatchedFilesDataset we’ve seen twice already. This is all the
    same code. Here is something important though. If we had something that was in
    the training set the one on the left, and then the validation had the image on
    the right, that would be kind of cheating because it’s the same car.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们已经看过两次的相同的MatchedFilesDataset。这是相同的代码。这里有一些重要的东西。如果我们在训练集中有左边的图像，然后验证集中有右边的图像，那将是一种作弊，因为它是相同的车辆。
- en: '[PRE70]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: So we use a continuous set of car IDs and since each set is a set of 16, we
    make sure that’s evenly divisible by 16\. So we make sure that our validation
    set contains different car IDs to our training set. This is the kind of stuff
    which you’ve got to be careful of. On Kaggle, it’s not so bad — you’ll know about
    it because you’ll submit your result and you’ll get a very different result on
    your leaderboard compared to your validation set. But in the real world. you won’t
    know until you put it in production and send your company bankrupt and lose your
    job. So you might want to think carefully about your validation set in that case.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们使用一系列连续的汽车ID，由于每个集合是一组16个，我们确保可以被16整除。因此，我们确保我们的验证集包含与训练集不同的汽车ID。这是你必须小心的事情。在Kaggle上，情况并不那么糟糕
    - 你会知道，因为你会提交你的结果，你的排行榜上的结果会与你的验证集有很大不同。但在现实世界中，你不会知道，直到你投入生产并让公司破产并失去工作。所以在这种情况下，你可能需要仔细考虑你的验证集。
- en: '[PRE71]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Here we are going to use transform type classification (`TfmType.CLASS`) [[1:37:03](https://youtu.be/nG3tT31nPmQ?t=1h37m3s)].
    It’s basically the same as transform type pixel (`TfmType.PIXEL`) but if you think
    about it, with a pixel version if we rotate a little bit then we probably want
    to average the pixels in between the two, but the classification, obviously we
    don’t. We use nearest neighbor. So there’s slight difference there. Also for classification,
    lighting doesn’t kick in, normalization doesn’t kick in to the dependent variable.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用转换类型分类（`TfmType.CLASS`）[[1:37:03](https://youtu.be/nG3tT31nPmQ?t=1h37m3s)]。这基本上与转换类型像素（`TfmType.PIXEL`）相同，但是如果你考虑一下，对于像素版本，如果我们旋转一点，那么我们可能希望在两者之间平均像素，但是分类，显然我们不需要。我们使用最近邻。所以这里有一点不同。此外，对于分类，光照不起作用，归一化不起作用于因变量。
- en: '[PRE72]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: They are already square images, so we don’t have to do any cropping.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 它们已经是方形图像，所以我们不必进行任何裁剪。
- en: '[PRE73]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: So here you can see different versions of the augmented images — they are moving
    around a bit, and they are rotating a bit, and so forth.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里你可以看到增强图像的不同版本 - 它们在移动一点，旋转一点，等等。
- en: '[PRE74]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: I get a lot of questions during our study group about how do I debug things
    and fix things that aren’t working. I never have a great answer other than every
    time I fix a problem is because of stuff like this that I do all the time. I just
    always print out everything as I go and then the one thing that I screw up always
    turns out to be the one thing that I forgot to check along the way. The more of
    this kind of thing you can do the better. If you are not looking at all of your
    intermediate results, you are going to have troubles.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的学习小组中，我经常被问到如何调试和修复不起作用的东西。我从来没有一个很好的答案，除了每次我解决问题都是因为我经常做这样的事情。我总是在进行过程中打印出所有内容，然后我搞砸的那一件事总是最后发现是我忘记检查的那一件事。你能做这种事情的越多越好。如果你不看所有的中间结果，你会遇到麻烦。
- en: Model [[1:38:30](https://youtu.be/nG3tT31nPmQ?t=1h38m30s)]
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型[[1:38:30](https://youtu.be/nG3tT31nPmQ?t=1h38m30s)]
- en: '[PRE75]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Given that we want something that knows what cars look like, we probably want
    to start with a pre-trained ImageNet network. So we are going to start with ResNet34\.
    With `ConvnetBuilder`, we can grab our ResNet34 and we can add a custom head.
    The custom head is going to be something that upsamples a bunch of times and we
    are going to do things really dumb for now which is we’re just going to do a ConvTranspose2d,
    batch norm, ReLU.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们想要一个知道汽车长什么样的东西，我们可能想要从一个预训练的ImageNet网络开始。所以我们将从ResNet34开始。使用`ConvnetBuilder`，我们可以获取我们的ResNet34并添加一个自定义头部。自定义头部将是一些上采样的东西，现在我们将做一些非常愚蠢的事情，就是我们只是做一个ConvTranspose2d，批量规范化，ReLU。
- en: This is what I am saying — any of you could have built this without looking
    at any of this notebook or at least you have the information from previous classes.
    There is nothing new at all. So at the very end, we have a single filter. Now
    that’s going to give us something which is batch size by 1 by 128 by 128\. But
    we want something which is batch size by 128 by 128\. So we have to remove that
    unit axis so I’ve got a lambda layer here. Lambda layers are incredibly helpful
    because without the lambda layer here, which is simply removing that unit axis
    by just indexing it with a 0, without a lambda layer, I would have to have created
    a custom class with a custom forward method and so forth. But by creating a lambda
    layer that does the one custom bit, I can now just chuck it in the Sequential
    and so that makes life easier.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我说的 - 任何人都可以在不看任何笔记本的情况下构建这个，或者至少你有来自以前课程的信息。这里没有任何新东西。所以最后，我们有一个单一的过滤器。现在这将给我们一个批量大小为1乘以128乘以128。但我们想要的是批量大小为128乘以128。所以我们必须去掉那个单元轴，所以我在这里有一个lambda层。Lambda层非常有帮助，因为没有这个lambda层，它只是通过索引0来删除那个单元轴，没有lambda层，我将不得不创建一个自定义类，具有自定义的前向方法等等。但通过创建一个lambda层来执行一个自定义操作，我现在可以将其放入Sequential中，这样就更容易了。
- en: PyTorch people are kind of snooty about this approach. Lambda layer is actually
    something that’s a part of the fastai library not part of the PyTorch library.
    And literally people on PyTorch discussion board say “yes, we could give people
    this”, “yes it is only a single line of code” but they never encourage them to
    use sequential too often. So there you go.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的人们对这种方法有点傲慢。Lambda层实际上是fastai库的一部分，而不是PyTorch库的一部分。而且PyTorch讨论板上的人们说“是的，我们可以给人们这个”，“是的，这只是一行代码”，但他们从不鼓励他们过于频繁地使用Sequential。所以你看。
- en: So this is our custom head [[1:40:36](https://youtu.be/nG3tT31nPmQ?t=1h40m36s)].
    So we are going to have a ResNet 34 that goes downsample and then a really simple
    custom head that very quickly upsamples, and that hopefully will do something.
    And we are going to use accuracy with a threshold of 0.5 and print out metrics.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的自定义头部[[1:40:36](https://youtu.be/nG3tT31nPmQ?t=1h40m36s)]。所以我们将有一个ResNet
    34进行下采样，然后一个非常简单的自定义头部，非常快速地上采样，希望这样做一些事情。我们将使用阈值为0.5的准确度并打印出指标。
- en: '[PRE76]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: After a few epochs, we’ve got 96 percent accurate. Is that good [[1:40:56](https://youtu.be/nG3tT31nPmQ?t=1h40m56s)]?
    Is 96% accurate good? And hopefully the answer to that question is it depends.
    What’s it for? The answer is Carvana wanted this because they wanted to be able
    to take their car image and cut them out and paste them on exotic Monte Carlo
    backgrounds or whatever (that’s Monte Carlo the place and not the simulation).
    To do that, you you need a really good mask. You don’t want to leave the rearview
    mirrors behind, have one wheel missing, or include a little bit of background
    or something. That would look stupid. So you would need something very good. So
    only having 96% of the pixels correct doesn’t sound great. But we won’t really
    know until we look at it. So let’s look at it.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几个时代，我们得到了96%的准确率。这好吗[[1:40:56](https://youtu.be/nG3tT31nPmQ?t=1h40m56s)]？96%的准确率好吗？希望对这个问题的答案是取决于。这是为了什么？答案是Carvana想要这个，因为他们想要能够拍摄他们的汽车图像并将它们剪切并粘贴到异国情调的蒙特卡洛背景或其他地方（这是蒙特卡洛的地方，而不是模拟）。为了做到这一点，你需要一个非常好的蒙版。你不想留下后视镜，缺少一个车轮，或者包括一点背景之类的东西。那看起来很愚蠢。所以你需要一些非常好的东西。所以只有96%的像素正确并不听起来很好。但我们真的不知道直到我们看到它。所以让我们看看。
- en: '[PRE78]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: So there is the correct version that we want to cut out [[1:41:54](https://youtu.be/nG3tT31nPmQ?t=1h41m54s)]
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我们想要剪切的正确版本[[1:41:54](https://youtu.be/nG3tT31nPmQ?t=1h41m54s)]
- en: '[PRE79]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: That’s the 96% accurate version. So when you look at it you realize “oh yeah,
    getting 96% of the pixel accurate is actually easy because all the outside bit
    is not car, and all the inside bit is a car, and really interesting bit is the
    edge. So we need to do better.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这是96%准确的版本。所以当你看到它时，你会意识到“哦，是的，准确地获取96%的像素实际上很容易，因为所有外部部分都不是汽车，所有内部部分都是汽车，而真正有趣的部分是边缘。所以我们需要做得更好。
- en: '[PRE80]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Let’s unfreeze because all we’ve done so far is train the custom head. Let’s
    do more.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解冻，因为到目前为止我们只训练了自定义头部。让我们做更多。
- en: '[PRE81]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: After a bit more, we’ve got 99.1%. Is that good? I don’t know. Let’s take a
    look.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 再经过一段时间，我们得到了99.1%。这好吗？我不知道。让我们看看。
- en: '[PRE82]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Actually no. It’s totally missed the rearview vision mirror on the left and
    missed a lot of it on the right. And it’s clearly got an edge wrong on the bottom.
    And these things are totally going to matter when we try to cut it out, so it’s
    still not good enough.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上不是。它完全错过了左侧的后视镜，右侧也错过了很多。底部的边缘明显错了。当我们尝试剪裁时，这些事情完全会影响到，所以还不够好。
- en: '[PRE83]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 512x512 [[1:42:50](https://youtu.be/nG3tT31nPmQ?t=1h42m50s)]
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 512x512
- en: Let’s try upscaling. And the nice thing is that when we upscale to 512 by 512,
    (make sure you decrease the batch size because you’ll run out of memory), it’s
    quite a lot more information there for it to go on so our accuracy increases to
    99.4% and things keep getting better.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试放大。很好的一点是，当我们将其放大到512x512时（确保减少批量大小，因为你会耗尽内存），有更多的信息供其使用，因此我们的准确性提高到99.4%，事情一直在变得更好。
- en: '[PRE85]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Here is the true ones.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这是真实的。
- en: '[PRE86]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Things keep getting better but we’ve still got quite a few little black blocky
    bits. so let’s go to 1024 by 1024.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 事情一直在变得更好，但我们仍然有一些小黑色块状物。所以让我们调整到1024x1024。
- en: 1024x1024 [[1:43:17](https://youtu.be/nG3tT31nPmQ?t=1h43m17s)]
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1024x1024
- en: So let’s go to 1024 by 1024, batch size down to 4\. This is pretty high res
    now, and train a bit more, 99.6, 99.8%!
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们调整到1024x1024，批量大小减少到4。现在这是相当高分辨率的了，再训练一段时间，99.6%，99.8%！
- en: '[PRE90]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Now if we look at the masks, , they are actually looking not bad. That’s looking
    pretty good. So can we do better? And the answer is yes, we can.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们看一下掩模，它们实际上看起来不错。这看起来相当不错。那么我们能做得更好吗？答案是肯定的。
- en: U-Net [[1:43:45](https://youtu.be/nG3tT31nPmQ?t=1h43m45s)]
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: U-Net
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/carvana-unet.ipynb)
    / [Paper](https://arxiv.org/abs/1505.04597)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/carvana-unet.ipynb)
    / [论文](https://arxiv.org/abs/1505.04597)'
- en: U-Net network is quite magnificent. With that previous approach, our pre-trained
    ImageNet network was being squished down all the way down to 7x7 and then expand
    it out all the way back up to 224x224 (1024 gets squished down to quite a bit
    bigger than 7x7). And then expanded out again all this way which means it has
    to somehow store all the information about the much bigger version in the small
    version. And actually most of the information about the bigger version was really
    in the original picture anyway. So it doesn’t seem like a great approach — this
    squishing and un-squishing.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net网络非常了不起。使用之前的方法，我们的预训练ImageNet网络被压缩到7x7，然后再扩展到224x224（1024被压缩到比7x7大得多）。然后再次扩展出来，这意味着它必须以某种方式在小版本中存储关于更大版本的所有信息。实际上，关于更大版本的大部分信息实际上已经在原始图片中。因此，这种压缩和解压似乎不是一个很好的方法。
- en: So the U-Net idea comes from this fantastic paper where it was literally invented
    in this very domain-specific area of biomedical image segmentation. But in fact,
    basically every Kaggle winner in anything even vaguely related to segmentation
    has end up using U-Net. It’s one of these things that everybody in Kaggle knows
    it is the best practice, but in more of academic circles, this has been around
    for a couple of years at least, a lot of people still don’t realize this is by
    far the best approach.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，U-Net的想法来自于这篇出色的论文，在这篇论文中，它实际上是在生物医学图像分割这个非常特定的领域中发明的。但事实上，基本上每一个与分割有关的Kaggle获胜者最终都使用了U-Net。这是每个Kaggle参与者都知道的最佳实践之一，但在更多的学术圈中，这已经存在至少几年了，很多人仍然没有意识到这是迄今为止最好的方法。
- en: Here is the basic idea [[1:45:10](https://youtu.be/nG3tT31nPmQ?t=1h45m10s)].
    On the left is the downward path where we start at 572x572 in this case then halve
    the grid size 4 times, then on the right is the upward path where we double the
    grid size 4 times. But the thing that we also do is, at every point where we halve
    the grid size, we actually copy those activations over to the upward path and
    concatenate them together.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是基本的想法。在左侧是向下路径，我们从572x572开始，然后将网格大小减半4次，然后在右侧是向上路径，我们将网格大小扩大4次。但我们还做的一件事是，在每个减半网格大小的点，我们实际上将这些激活复制到向上路径，并将它们连接在一起。
- en: You can see on the bottom right, these red arrows are max pooling operation,
    these green arrows are upward sampling, and then these gray arrows are copying.
    So we copy and concat. In other words, the input image after a couple of convs
    is copied over to the output, concatenated together, and so now we get to use
    all of the informations gone through all of the informations gone through all
    the down and all the up, plus also a slightly modified version of the input pixels.
    And slightly modified version of one thing down from the input pixels because
    they came up through here. So we have all of the richness of going all the way
    down and up, but also a slightly less coarse version and a slightly less coarse
    version and then the really simple version, and they can all be combined together.
    So that’s U-Net. It’s such a cool idea.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在右下角可以看到，这些红色箭头是最大池化操作，这些绿色箭头是向上采样，然后这些灰色箭头是复制。所以我们复制并连接。换句话说，经过几次卷积后的输入图像被复制到输出中，连接在一起，现在我们可以使用所有经过所有向下和向上的信息，还有输入像素的略微修改版本。以及输入像素的略微修改版本，因为它们是通过这里上来的。所以我们拥有所有向下和向上的丰富性，但也有一个略微不那么粗糙的版本，然后是一个略微不那么粗糙的版本，然后是一个真正简单的版本，它们都可以组合在一起。这就是U-Net。这是一个很酷的想法。
- en: Here we are in the carvana-unet notebook. All this is the same code as before.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在carvana-unet笔记本中。所有这些与之前的代码相同。
- en: '[PRE96]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Data
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: '[PRE97]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Simple upsample
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的上采样
- en: And at the start, I’ve got a simple upsample version just to show you again
    the non U-net version. This time, I’m going to add in something called the dice
    metric. Dice is very similar, as you see, to Jaccard or I over U. It’s just a
    minor difference. It’s basically intersection over union with a minor tweak. The
    reason we are going to use dice is that’s the metric that Kaggle competition used
    and it’s a little bit harder to get a high dice score than a high accuracy because
    it’s really looking at what the overlap of the correct pixels are with your pixels.
    But it’s pretty similar.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，我有一个简单的上采样版本，只是为了再次向你展示非U-net版本。这次，我将加入一个称为dice指标的东西。Dice非常类似，如你所见，与Jaccard或I
    over U非常相似。只是有一点小差别。基本上是交集除以并集，稍微调整了一下。我们要使用dice的原因是Kaggle竞赛使用了这个指标，而且要获得高dice分数比获得高准确度要困难一些，因为它真的在看正确像素与你的像素的重叠部分。但它非常相似。
- en: So in the Kaggle competition, people that were doing okay were getting about
    99.6 dice and the winners were about 99.7 dice.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，表现良好的人得到了大约99.6点，而获胜者得到了大约99.7点。
- en: '[PRE98]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Here is our standard upsample.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的标准上采样。
- en: '[PRE99]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: This all as before.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切和以前一样。
- en: '[PRE100]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Now we can check our dice metric [[1:48:00](https://youtu.be/nG3tT31nPmQ?t=1h48m)].
    So you can see on dice metric, we are getting around 96.8 at 128x128\. So that’s
    not great.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查我们的dice指标[[1:48:00](https://youtu.be/nG3tT31nPmQ?t=1h48m)]。所以你可以看到在dice指标上，我们在128x128处得到了大约96.8。所以这不太好。
- en: '[PRE102]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: U-net (ish) [[1:48:16](https://youtu.be/nG3tT31nPmQ?t=1h48m16s)]
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U-net（ish）[[1:48:16](https://youtu.be/nG3tT31nPmQ?t=1h48m16s)]
- en: So let’s try U-Net. I’m calling it U-net(ish) because as per usual I’m creating
    my own somewhat hacky version — trying to keep things as similar to what you’re
    used to as possible and doing things that I think makes sense. So there should
    be plenty of opportunity for you to at least make this more authentically U-net
    by looking at the exact grid sizes and see how here (the top left convs) the size
    is going down a little bit. So they are obviously not adding any padding and then
    there are some cropping going on — there’s a few differences. But one of the things
    is because I want to take advantage of transfer learning — that means I can’t
    quite use U-Net.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们尝试U-Net。我称之为U-net(ish)，因为通常我正在创建自己的有点hacky版本——尽量保持与你习惯的东西尽可能相似，并做我认为有意义的事情。所以至少有很多机会让你至少通过查看确切的网格大小来使其更加真实地成为U-net，看看这里（左上角的卷积）大小有点下降。所以显然他们没有添加任何填充，然后有一些裁剪——有一些差异。但其中一件事是因为我想利用迁移学习——这意味着我不能完全使用U-Net。
- en: So here is another big opportunity is what if you create the U-Net down path
    and then add a classifier on the end and then train that on ImageNet. You’ve now
    got an ImageNet trained classifier which is specifically designed to be a good
    backbone for U-Net. Then you should be able to now come back and get pretty closed
    to winning this old competition (it’s actually not that old — it’s fairly recent
    competition). Because that pre-trained network didn’t exist before. But if you
    think about what YOLO v3 did, it’s basically that. They created a DarkNet, they
    pre-trained it on ImageNet, and then they used it as the basis for their bounding
    boxes. So again, this idea of pre-training things which are designed not just
    for classification but designed for other things — it’s just something that nobody
    has done yet. But as we’ve shown, you can train ImageNet for $25 in three hours
    now. And if people in the community are interested in doing this, hopefully I’ll
    have credits I can help you with as well so if you do, the work to get it set
    up and give me a script, I can probably run it for you. For now though, we don’t
    have that yet. So we are going to use ResNet.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 所以另一个重要的机会是，如果你创建了U-Net的下行路径，然后在末尾添加一个分类器，然后在ImageNet上训练它。现在你有了一个在ImageNet上训练过的分类器，专门设计为U-Net的良好骨干。然后你应该能够回来并接近赢得这个旧竞赛（实际上并不是很旧——是一个相当新的竞赛）。因为以前不存在这种预训练网络。但是如果你想一下YOLO
    v3是如何做的，基本上就是这样。他们创建了一个DarkNet，他们在ImageNet上预训练了它，然后他们将其用作边界框的基础。所以，再次强调这种不仅为分类而设计而且为其他事物而设计的预训练的想法——这是迄今为止没有人做过的事情。但正如我们所展示的，你现在可以用25美元在三小时内训练ImageNet。如果社区中的人们对此感兴趣，希望我也能提供帮助，如果你愿意，我可以帮助你设置并给我一个脚本，我可能可以为你运行它。但目前我们还没有。所以我们将使用ResNet。
- en: '[PRE104]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: So we are basically going to start with `get_base` [[1:50:37](https://youtu.be/nG3tT31nPmQ?t=1h50m37s)].
    Base is our base network and that was defined back up in the first section.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们基本上要从`get_base`开始[[1:50:37](https://youtu.be/nG3tT31nPmQ?t=1h50m37s)]。Base是我们的基础网络，这在第一部分中已经定义过了。
- en: So get_base is going to be something that calls whatever f is and `f` is `resnet34`.
    So we are going to grab our ResNet34 and cut_model is the first thing that our
    convnet builder does. It basically removes everything from the adaptive pooling
    onwards, so that gives us back the backbone of ResNet34\. So `get_base` is going
    to give us back the ResNet34 backbone.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 所以`get_base`将调用`f`是什么，`f`是`resnet34`。所以我们将获取我们的ResNet34并且`cut_model`是我们的卷积网络构建器做的第一件事。它基本上删除了自适应池化之后的所有内容，这样我们就得到了ResNet34的骨干。所以`get_base`将给我们返回ResNet34的骨干。
- en: '[PRE105]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Then we are going to take that ResNet34 backbone and turn it into a, I call
    it a, Unet34 [[1:51:17](https://youtu.be/nG3tT31nPmQ?t=1h51m17s)]. So what that’s
    going to do is it’s going to save that ResNet that we passed in and then we are
    going to use a forward hook just like before to save the results at the 2nd, 4th,
    5th, and 6th blocks which as before is the layers before each stride 2 convolution.
    Then we are going to create a bunch of these things we are calling `UnetBlock`.
    We need to tell `UnetBlock` how many things are coming from the previous layer
    we are upsampling, how many are coming across, and then how many do we want to
    come out. The amount coming across is entirely defined by whatever the base network
    was — whatever the downward path was, we need that many layers. So this is a little
    bit awkward. Actually one of our master’s students here, Kerem, has actually created
    something called DynamicUnet that you’ll find in [fastai.model.DynamicUnet](https://github.com/fastai/fastai/blob/d3ef60a96cddf5b503361ed4c95d68dda4a873fc/fastai/models/unet.py#L53)
    and it actually calculates this all for you and automatically creates the whole
    Unet from your base model. It’s got some minor quirks still that I want to fix.
    By the time the video is out, it’ll definitely be working and I will at least
    have a notebook showing how to use it and possibly an additional video. But for
    now you’ll just have to go through and do it yourself. You can easily see it just
    by, once you’ve got a ResNet, you can just type in its name and it’ll print out
    the layers. And you can see how many many activations there are in each block.
    Or you can have it printed out for you for each block automatically. Anyway, I
    just did this manually.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将把那个ResNet34主干转换成一个，我称之为Unet34。因此，它将保存我们传入的ResNet，然后我们将使用一个前向钩子，就像以前一样，在第2、4、5和6个块处保存结果，这些块是每个步幅2卷积之前的层。然后我们将创建一堆我们称之为`UnetBlock`的东西。我们需要告诉`UnetBlock`有多少东西来自我们要上采样的上一层，有多少来自交叉路径，然后我们想要输出多少。来自上一层的数量完全由基础网络定义——无论下行路径是什么，我们都需要那么多层。这有点尴尬。实际上我们这里的一个硕士学生，Kerem，实际上创建了一个叫做DynamicUnet的东西，你可以在[fastai.model.DynamicUnet](https://github.com/fastai/fastai/blob/d3ef60a96cddf5b503361ed4c95d68dda4a873fc/fastai/models/unet.py#L53)中找到，它实际上为你计算这一切，并自动从你的基础模型创建整个Unet。它仍然有一些小问题，我想要修复。视频发布时，它肯定会正常工作，我至少会有一个展示如何使用它的笔记本，可能还有一个额外的视频。但现在你只能自己去做。一旦你有了一个ResNet，你可以输入它的名称，它会打印出层。你可以看到每个块中有多少激活。或者你可以让它自动为每个块打印出来。无论如何，我只是手动做了这个。
- en: 'So the UnetBlock works like this [[1:53:29](https://youtu.be/nG3tT31nPmQ?t=1h53m29s)]:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 所以UnetBlock的工作原理是这样的：
- en: '`up_in` : This many are coming up from the previous layer'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`up_in`：从上一层传入的数量'
- en: '`x_in` : This many are coming across (hence `x`) from the downward path'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x_in`：从下行路径传入的数量（因此`x`）'
- en: '`n_out` : The amount we want coming out'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_out`：我们想要输出的数量'
- en: Now what I do is , I then say, okay we’re going to create a certain amount of
    convolutions from the upward path and a certain amount from the cross path, and
    so I’m going to be concatenating them together so let’s divide the number we want
    out by 2\. And so we are going to have our cross convolution take our cross path
    and create number out divided by 2 (`n_out//2`). And then the upward path is going
    to be a `ConvTranspose2d` because we want to increase/upsample. Again here, we’ve
    got the number out divided by 2 (`up_out`), then at the end, I just concatenate
    those together.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我要做的是，然后我说，好的，我们将从上行路径创建一定数量的卷积，从交叉路径创建一定数量的卷积，所以我将它们连接在一起，所以让我们将我们想要的数量除以2。因此，我们将让我们的交叉卷积从交叉路径中取出并除以2（`n_out//2`）。然后上行路径将是`ConvTranspose2d`，因为我们想要增加/上采样。同样在这里，我们将我们想要的数量除以2（`up_out`），然后最后，我只是将它们连接在一起。
- en: So I’ve got an upward sample, I’ve got a cross convolution, I can concatenate
    the two together. That’s all a UnetBlock is. So that’s actually a pretty easy
    module to create.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我有一个上升样本，我有一个交叉卷积，我可以将这两者连接在一起。这就是UnetBlock的全部内容。所以这实际上是一个相当容易创建的模块。
- en: Then in my forward path, I need to pass to the forward of the UnetBlock the
    upward path and the cross path [[1:54:40](https://youtu.be/nG3tT31nPmQ?t=1h54m40s)].
    The upward path is just whatever I am up to so far. But then the cross path is
    whatever the activations are that I stored on the way down. So as I come up, it’s
    the last set of saved features that I need first. And as I gradually keep going
    up farther and farther, eventually it’s the first set of features.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在我的前向路径中，我需要将上升路径和交叉路径传递给UnetBlock的前向方法。上升路径只是到目前为止的任何事情。但是交叉路径是在下降过程中存储的激活。因此，当我上升时，我首先需要的是最后一组保存的特征。随着我逐渐向上走得更远，最终是第一组特征。
- en: There are some more tricks we can do to make this a little bit better, but this
    is a good stuff. So the simple upsampling approach looked horrible and had a dice
    of .968\. A Unet with everything else identical except we’ve now got these UnetBlocks
    has a dice of …
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些更多的技巧可以让这个变得更好一点，但这已经是一个很好的东西了。所以简单的上采样方法看起来很糟糕，dice值为0.968。一个Unet，除了现在我们有了这些UnetBlocks之外，其他一切都相同，dice值为…
- en: '[PRE106]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: .985! That’s like we halved the error with everything else exactly the same
    [[1:55:42](https://youtu.be/nG3tT31nPmQ?t=1h55m42s)]. And more the point, you
    can look at it.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 0.985！这就像我们将错误减半，其他一切完全相同。而且更重要的是，你可以看一下。
- en: '[PRE108]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: This is actually looking somewhat car-like compared to our non-Unet equivalent
    which is just a blob. Because trying to do this through down and up paths — it’s
    just asking too much. Where else, when we actually provide the downward path pixels
    at every point, it can actually start to create something car-ish.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的非Unet等效物相比，这实际上看起来有点像汽车，后者只是一个斑点。因为试图通过下行和上行路径来做这个——这只是要求太多了。而当我们实际上在每个点提供下行路径像素时，它实际上可以开始创建一些类似汽车的东西。
- en: '[PRE109]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: At the end of that, we’ll do m.close to remove those `sfs.features` taking up
    GPU memory.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们将执行m.close以删除占用GPU内存的`sfs.features`。 '
- en: '[PRE111]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 512x512 [[1:56:26](https://youtu.be/nG3tT31nPmQ?t=1h56m26s)]
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 512x512 [[1:56:26](https://youtu.be/nG3tT31nPmQ?t=1h56m26s)]
- en: Go to a smaller batch size, higher size
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 转到较小的批量大小，更高的大小
- en: '[PRE112]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: You can see the dice coefficients really going up [[1:56:30](https://youtu.be/nG3tT31nPmQ?t=1h56m30s)].
    So notice above, I’m loading in the 128x128 version of the network. We are doing
    this progressive resizing trick again, so that gets us .993.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到Dice系数真的在上升[[1:56:30](https://youtu.be/nG3tT31nPmQ?t=1h56m30s)]。所以请注意，我正在加载网络的128x128版本。我们再次使用渐进式调整大小的技巧，这样我们得到了0.993。
- en: '[PRE113]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: Then unfreeze to get to .994.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 然后解冻以达到0.994。
- en: '[PRE114]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: And you can see, it’s now looking pretty good.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，现在看起来很不错。
- en: '[PRE115]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 1024x1024 [[1:56:53](https://youtu.be/nG3tT31nPmQ?t=1h56m53s)]
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1024x1024 [[1:56:53](https://youtu.be/nG3tT31nPmQ?t=1h56m53s)]
- en: Go down to a batch size of 4, size of 1024.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 将批量大小降至4，大小为1024。
- en: '[PRE118]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Load in what we just saved with the 512.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 加载我们刚刚保存的512。
- en: '[PRE119]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: That gets us to .995.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们达到了0.995。
- en: '[PRE120]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Unfreeze takes us to… we’ll call that .996.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 解冻将我们带到...我们将称之为0.996。
- en: '[PRE121]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: As you can see, that actually looks good [[1:57:17](https://youtu.be/nG3tT31nPmQ?t=1h57m17s)].
    In accuracy terms, 99.82%. You can see this is looking like something you could
    just about use to cut out. I think, at this point, there’s a couple of minor tweaks
    we can do to get up to .997 but really the key thing then, I think, is just maybe
    to do a few bit of smoothing maybe or a little bit of post-processing. You can
    go and have a look at the Carvana winners’ blogs and see some of these tricks,
    but as I say, the difference between where we are at .996 and what the winners
    got of .997, it’s not heaps. So really that just the Unet on its own pretty much
    solves that problem.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，实际上看起来很不错[[1:57:17](https://youtu.be/nG3tT31nPmQ?t=1h57m17s)]。在准确性方面，99.82%。你可以看到这看起来像是你可以用来裁剪的东西。我认为，在这一点上，我们可以做一些微小的调整来达到0.997，但真正的关键是，我认为，也许只需要做一些平滑处理或一点后处理。你可以去看看Carvana获奖者的博客，看看其中的一些技巧，但正如我所说，我们目前的0.996和获奖者得到的0.997之间的差距并不大。所以实际上，U-Net基本上解决了这个问题。
- en: '[PRE123]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Back to Bounding Box [[1:58:15](https://youtu.be/nG3tT31nPmQ?t=1h58m15s)]
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到边界框[[1:58:15](https://youtu.be/nG3tT31nPmQ?t=1h58m15s)]
- en: Okay, so that’s it. The last thing I wanted to mention is now to come all the
    way back to bounding boxes because you might remember, I said our bounding box
    model was still not doing very well on small objects. So hopefully you might be
    able to guess where I’m going to go with this which is that for the bounding box
    model, remember how we had at different grid cells we spat out outputs of the
    model. And it was those earlier ones with the small grid sizes that weren’t very
    good. How do we fix it? U-Net it! Let’s have an upward path with cross connections.
    So then we are just going to do a U-Net and then spit them out of that. Because
    now those finer grid cells have all of the information of that path, and that
    path, and that path, and that path for leverage. Now of course, this is deep learning
    so that means you can’t write a paper saying we just used U-Net for bounding boxes.
    You have to invent a new word so this is called feature pyramid networks or FPNs.
    And this was used in RetinaNet paper, it was created in an earlier paper specifically
    about FPNs. And if memory serves correctly, they did briefly cite the U-Net paper
    but they kind of made it sound like it was this vaguely slightly connected thing
    that maybe some people could consider slightly useful. But really, FPNs are U-Nets.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，就是这样。我想要提到的最后一件事是现在回到边界框，因为你可能还记得，我说我们的边界框模型在小物体上仍然表现不佳。所以希望你能猜到我接下来要做什么，那就是对于边界框模型，记得我们在不同的网格单元中输出了模型的输出。那些较早的具有较小网格大小的输出并不好。我们该如何修复呢？用U-Net！让我们有一个带有交叉连接的向上路径。然后我们将使用U-Net，然后从中输出。因为现在那些更精细的网格单元具有该路径的所有信息，以及该路径、该路径和该路径的信息。当然，这是深度学习，这意味着你不能写一篇论文说我们只是用U-Net来处理边界框。你必须发明一个新词，所以这被称为特征金字塔网络或FPNs。这在RetinaNet论文中使用过，它是在早期关于FPNs的论文中创建的。如果我记得正确的话，他们确实简要引用了U-Net论文，但他们似乎让它听起来像是这个模糊地稍微相关的东西，也许有些人可能认为稍微有用。但实际上，FPNs就是U-Nets。
- en: I don’t have an implementation of it to show you but it will be a fun thing,
    maybe for some of us to try and I know some of the students have been trying to
    get it working well on the forums. So yeah, interesting thing to try. So I think
    a couple of things to look at after this class as well as the other things I mentioned
    would be playing around with FPNs and also maybe trying Kerem’s DynamicUnet. They
    would both be interesting things to look at.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有实现它来展示给你，但这将是一件有趣的事情，也许对于我们中的一些人来尝试，我知道一些学生一直在尝试在论坛上使其良好运行。所以是的，尝试一下是有趣的事情。所以我认为在这堂课之后要看的一些事情，以及我提到的其他事情，可能是玩玩FPNs，也可能尝试一下Kerem的DynamicUnet。它们都是值得一看的有趣的东西。
- en: So you guys have all been through 14 lessons of me talking at you now. So I’m
    sorry about that. Thanks for putting up with me. I think you’re going to find
    it hard to find people who actually know them as much about training neural networks
    and practice as you do. It’ll be really easy for you to overestimate how capable
    all these other people are and underestimate how capable you are. So the main
    thing I’d say is, please practice, please. Just because you don’t have this constant
    thing getting you to come back here every Monday night now. It’s very easy to
    kind of lose that momentum. So find ways to keep it. Organize a study group, a
    book reading group, or get together with some friends and work on a project, or
    do something more than just deciding I want to keep working on X. Unless you are
    kind of person who’s super motivated and whenever you decide to do something,
    it happens. That’s not me. It’s like I know, for something to happen, I have to
    say “yes, David. In October, I will absolutely teach that course” and then it’s
    like okay I better actually write some material. That’s the only way I can get
    stuff to happen. So we’ve got a great community there on the forums. If people
    have ideas for ways to make it better, please tell me. If you think you can help
    with, if you want to create some new forum or moderated in some different way
    or whatever, just let me know. You can always PM me and there’s a lot of projects
    going on through GitHub as well — lots of stuff. So I hope to see you all back
    here at something else and thanks so much for joining me on this journey.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你们现在已经经历了我对你们讲解的14堂课。对此我感到抱歉。谢谢你们忍受我。我认为你们会发现很难找到其他人对神经网络训练和实践了解得像你们这样多。你们很容易高估其他人的能力，低估自己的能力。所以我想说的是，请继续练习。因为现在没有每个星期一晚上都有我在这里让你们回来了。很容易失去动力。所以找到方法保持下去。组织一个学习小组，一个读书小组，或者和朋友们一起做项目，或者做一些不仅仅是决定我要继续做X的事情。除非你是那种超级有动力的人，每当你决定做某事，它就会发生。那不是我。我知道，要让事情发生，我必须说“是的，大卫。十月份，我绝对会教那门课程”，然后我就得开始写一些材料。这是我让事情发生的唯一方法。所以我们在论坛上有一个很棒的社区。如果有人有想法让它变得更好，请告诉我。如果你认为你可以帮忙，如果你想创建一些新的论坛或以某种不同的方式进行管理，或者其他什么的，只要告诉我。你可以随时私信我，GitHub上也有很多项目正在进行中——很多东西。所以我希望能在其他地方再见到你们，非常感谢你们加入我的旅程。
