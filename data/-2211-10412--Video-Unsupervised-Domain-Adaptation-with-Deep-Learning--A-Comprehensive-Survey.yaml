- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:43:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:43:04
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2211.10412] Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2211.10412] 视频无监督领域适应与深度学习：全面综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.10412](https://ar5iv.labs.arxiv.org/html/2211.10412)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2211.10412](https://ar5iv.labs.arxiv.org/html/2211.10412)
- en: Video Unsupervised Domain Adaptation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频无监督领域适应
- en: 'with Deep Learning: A Comprehensive Survey'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度学习：全面综述
- en: Yuecong Xu, Haozhi Cao, Zhenghua Chen, Xiaoli Li, Lihua Xie, , and Jianfei Yang
    Y. Xu, Z. Chen and X. Li are with the Institute for Infocomm Research, A*STAR,
    Singapore, 138632.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Yuecong Xu, Haozhi Cao, Zhenghua Chen, Xiaoli Li, Lihua Xie，和 Jianfei Yang Y.
    Xu, Z. Chen 和 X. Li 与新加坡A*STAR信息通信研究所合作，地址：138632。
- en: 'E-mail: {xuyu0014, chen0832}@e.ntu.edu.sg; xlli@i2r.a-star.edu.sg J. Yang,
    H. Cao, and L. Xie are with the School of Electrical and Electronics Engineering,
    Nanyang Technological University, Singapore, 639798.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：{xuyu0014, chen0832}@e.ntu.edu.sg; xlli@i2r.a-star.edu.sg J. Yang, H. Cao
    和 L. Xie 与新加坡南洋理工大学电气与电子工程学院合作，地址：639798。
- en: 'E-mail: {yang0478; haozhi001}@e.ntu.edu.sg; elhxie@ntu.edu.sg Z. Chen is also
    with the Centre for Frontier AI Research (CFAR), A*STAR, Singapore, 138632.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：{yang0478; haozhi001}@e.ntu.edu.sg; elhxie@ntu.edu.sg Z. Chen 还与新加坡A*STAR前沿人工智能研究中心（CFAR）合作，地址：138632。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Video analysis tasks such as action recognition have received increasing research
    interest with growing applications in fields such as smart healthcare, thanks
    to the introduction of large-scale datasets and deep learning-based representations.
    However, video models trained on existing datasets suffer from significant performance
    degradation when deployed directly to real-world applications due to domain shifts
    between the training public video datasets (source video domains) and real-world
    videos (target video domains). Further, with the high cost of video annotation,
    it is more practical to use unlabeled videos for training. To tackle performance
    degradation and address concerns in high video annotation cost uniformly, the
    video unsupervised domain adaptation (VUDA) is introduced to adapt video models
    from the labeled source domain to the unlabeled target domain by alleviating video
    domain shift, improving the generalizability and portability of video models.
    This paper surveys recent progress in VUDA with deep learning. We begin with the
    motivation of VUDA, followed by its definition, and recent progress of methods
    for both closed-set VUDA and VUDA under different scenarios, and current benchmark
    datasets for VUDA research. Eventually, future directions are provided to promote
    further VUDA research.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大规模数据集和基于深度学习的表示方法的引入，动作识别等视频分析任务在智能医疗等领域的应用不断增长，研究兴趣也日益增加。然而，由于训练公共视频数据集（源视频领域）与实际应用中的视频（目标视频领域）之间的领域偏移，基于现有数据集训练的视频模型在直接部署到实际应用时会遭遇显著的性能下降。此外，由于视频注释的高成本，使用未标记的视频进行训练更为实际。为了应对性能下降和统一解决高视频注释成本的问题，引入了视频无监督领域适应（VUDA）方法，通过缓解视频领域偏移，将视频模型从标记的源领域适应到未标记的目标领域，从而提高视频模型的泛化能力和可移植性。本文综述了VUDA与深度学习的最新进展。我们首先介绍VUDA的动机，然后定义其概念，接着回顾了闭集VUDA和不同场景下的VUDA方法的最新进展，以及当前VUDA研究的基准数据集。最后，提供了未来研究方向，以推动VUDA的进一步研究。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Video unsupervised domain adaptation, deep learning, action recognition, closed-set,
    benchmark datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 视频无监督领域适应，深度学习，动作识别，闭集，基准数据集。
- en: I Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: With the rapid growth of video data at an extraordinary rate, automatic video
    analysis tasks, e.g., action recognition (AR) and video segmentation, have received
    increasing research interest with growing applications. Over the past decade,
    there have been great developments in various video analysis tasks. This is largely
    enabled by the emergence of diverse large-scale video datasets and the continuous
    advancement in video representation learning, particularly with deep neural networks
    and deep learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着视频数据以惊人的速度快速增长，自动视频分析任务，如动作识别（AR）和视频分割，受到了越来越多的研究关注，应用也在不断增长。在过去的十年中，各种视频分析任务取得了巨大的进展。这主要得益于各种大型视频数据集的出现以及视频表示学习的持续进步，特别是深度神经网络和深度学习的应用。
- en: Despite the progress made in video analysis tasks (e.g., AR), most existing
    methods assume that the training and testing data are drawn from the same distribution,
    which yet may not hold in real-world applications. In practice, it is very common
    that the distribution of the training data from public datasets and testing data
    collected in real-world scenarios differ, and therefore a domain shift between
    the training (source) and testing (target) domains exists. In these scenarios,
    we observe significantly degraded performances of trained video models in the
    testing (target) domain despite the strong capacity of deep neural networks. For
    instance, deep video models trained for autonomous driving with current datasets
    (e.g., KITTI, nuScenes) would not be applicable for nighttime autonomous driving;
    while deep video models trained with regular action recognition datasets (e.g.,
    UCF101, HMDB51) may not be able to recognize actions of patients in hospitals.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在视频分析任务（例如，增强现实）方面取得了进展，大多数现有方法假设训练数据和测试数据来自相同的分布，但这在实际应用中可能并不成立。实际上，来自公共数据集的训练数据和在现实世界场景中收集的测试数据的分布差异是非常普遍的，因此训练（源）领域和测试（目标）领域之间存在领域转移。在这些情况下，尽管深度神经网络具有很强的能力，我们观察到训练的视频模型在测试（目标）领域的性能显著下降。例如，使用当前数据集（如KITTI、nuScenes）训练的深度视频模型可能不适用于夜间自动驾驶；而使用常规动作识别数据集（如UCF101、HMDB51）训练的深度视频模型可能无法识别医院中患者的动作。
- en: To tackle the performance degradation under domain shift, various domain adaptation
    (DA) methods have been proposed to utilize labeled data in the source domain to
    execute tasks in the target domain. Domain adaptation methods generally aim to
    learn a model from the source domain that can be generalized to the target domain
    by minimizing the difference between domain distributions. Meanwhile, due to the
    high cost of annotating large-scale real-world data for deep learning, it is more
    feasible to obtain unlabeled data for models to be adapted to target domains.
    The unsupervised domain adaptation (UDA) task is therefore introduced where models
    are adapted from the labeled source domain towards the unlabeled target domain
    by alleviating the negative effect of domain shift while avoiding costly data
    annotation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决领域转移下的性能退化，已经提出了各种领域适应（DA）方法，这些方法利用源领域的标记数据在目标领域执行任务。领域适应方法通常旨在通过最小化领域分布之间的差异，从源领域学习一个可以推广到目标领域的模型。同时，由于为深度学习注释大规模真实世界数据的成本较高，获取未标记数据以使模型适应目标领域更为可行。因此，引入了无监督领域适应（UDA）任务，在这种任务中，模型从标记的源领域适应到未标记的目标领域，通过减轻领域转移的负面影响，同时避免昂贵的数据注释。
- en: While UDA with deep learning greatly improves the generalizability and portability
    of models by tackling domain shift, prior research for visual applications generally
    focused on image data. An intuitive method for video unsupervised domain adaptation
    (VUDA) is to extend UDA methods for images to videos by directly substituting
    the image feature extractor with a video feature extractor (e.g., substituting
    2D CNNs with 3D CNNs). Meanwhile, video representations obtained through conventional
    video feature extractors are mainly from spatial features. However, videos contain
    not only spatial features but also temporal features as well as features of other
    modalities, e.g., optical flow and audio features. Domain shift would occur for
    all of these features. Therefore, such a vanilla substitution strategy that ignores
    domain shift across the multiple modalities of features produces inferior results
    for VUDA.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习的UDA显著提高了模型的普适性和可移植性，通过解决领域转移问题，但以前的研究主要集中在图像数据上。视频无监督领域适应（VUDA）的一个直观方法是将UDA图像方法扩展到视频，通过直接用视频特征提取器（例如，将2D
    CNN替换为3D CNN）替代图像特征提取器。同时，通过传统视频特征提取器获得的视频表示主要来自空间特征。然而，视频不仅包含空间特征，还包含时间特征以及其他模态的特征，例如光流和音频特征。这些特征都会发生领域转移。因此，这种忽略多模态特征领域转移的简单替代策略会导致VUDA效果不佳。
- en: 'Subsequently, various VUDA methods have been proposed to explicitly deal with
    the issue of domain shift for video tasks. Generally, they can be categorized
    into five categories: a) adversarial-based method, where feature generators are
    trained jointly with additional domain discriminators in an adversarial manner,
    with domain-invariant features obtained if the domain discriminators failed to
    discriminate whether they originate from the source or target domains; b) discrepancy-based
    (or metric-based) methods, where the discrepancy between the source and target
    domains are explicitly computed, while the target domain is aligned with the source
    domain by applying metric learning approaches, optimized with metric-based objectives
    such as MDD [[1](#bib.bib1)], CORAL [[2](#bib.bib2)], and MMD [[3](#bib.bib3)];
    c) semantic-based methods, where domain-invariant features are obtained subject
    to certain semantic constraints by leveraging approaches such as mutual information
    maximization [[4](#bib.bib4)], clustering [[5](#bib.bib5)], contrastive learning [[6](#bib.bib6),
    [7](#bib.bib7)] and pseudo-labeling [[8](#bib.bib8), [9](#bib.bib9)]; d) composite
    methods, where domain-invariant features are extracted by optimizing a combination
    of different objectives (i.e., domain discrepancy objectives, adversarial objectives,
    and semantic-based objectives); and e) reconstruction-based methods, where domain-invariant
    features are extracted by encoders trained with data-reconstruction objectives
    with the network commonly structured as an encoder-decoder.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，提出了各种 VUDA 方法，以明确处理视频任务中的领域偏移问题。一般来说，它们可以分为五类：a) 基于对抗的方法，其中特征生成器与额外的领域鉴别器一起进行对抗性训练，如果领域鉴别器未能区分特征是否来自源领域或目标领域，则获得领域不变的特征；b)
    基于差异的方法（或度量方法），通过显式计算源领域和目标领域之间的差异，同时通过应用度量学习方法使目标领域与源领域对齐，优化度量目标，例如 MDD [[1](#bib.bib1)]、CORAL [[2](#bib.bib2)]
    和 MMD [[3](#bib.bib3)]；c) 基于语义的方法，通过利用诸如互信息最大化 [[4](#bib.bib4)]、聚类 [[5](#bib.bib5)]、对比学习 [[6](#bib.bib6),
    [7](#bib.bib7)] 和伪标签 [[8](#bib.bib8), [9](#bib.bib9)] 等方法，获得受特定语义约束的领域不变特征；d)
    复合方法，通过优化不同目标（即领域差异目标、对抗目标和基于语义的目标）的组合来提取领域不变特征；e) 基于重建的方法，通过使用数据重建目标训练的编码器提取领域不变特征，网络通常结构为编码器-解码器。
- en: '![Refer to caption](img/6b3b61460382aea9800008205ff1fb97.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6b3b61460382aea9800008205ff1fb97.png)'
- en: 'Figure 1: Overview of the categorization of the different VUDA methods. Closed-set
    VUDA methods are constrained by the constraint of an identical label space shared
    by the single pair of video source/target domains and assume that both the source
    and target data are accessible, with action recognition as the cross-domain task.
    Any VUDA methods that does not satisfy the four constraints/assumptions are considered
    as non-closed-set VUDA. Closed-set VUDA methods can be categorized into five categories
    based on how source and target domains are aligned. Non-closed-set VUDA methods
    are categorized into four categories by how their related scenarios differ from
    the closed-set VUDA.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同 VUDA 方法的分类概述。封闭集 VUDA 方法受到单对视频源/目标领域共享的相同标签空间的约束，并假设源数据和目标数据均可访问，以动作识别作为跨领域任务。任何不符合四个约束/假设的
    VUDA 方法被认为是非封闭集 VUDA。封闭集 VUDA 方法可以根据源领域和目标领域的对齐方式分为五类。非封闭集 VUDA 方法根据其相关场景与封闭集
    VUDA 的差异分为四类。
- en: 'Meanwhile, though the aforementioned VUDA methods enable the learning of transferable
    knowledge across video domains, they are built upon several constraints and assumptions.
    These include the constraint of an identical label space shared by the single
    pair of the video source and target domains and the assumption that both the labeled
    source and unlabeled target domain data are accessible during training, with action
    recognition as the cross-domain task. VUDA under these constraints and assumptions
    are denoted as closed-set VUDA, and may not hold in real-world scenarios, prompting
    concerns in model portability due to issues such as data security. Over the past
    few years, there have been various research that looks to differ the constraints
    and assumptions such that VUDA methods could be more applicable in real-world
    scenarios, which could be broadly categorized as: a) methods with differed label
    space constraint; b) methods with differed source data assumption; c) methods
    with differed target data assumption; and d) methods with differed cross-domain
    tasks. Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Video Unsupervised Domain
    Adaptation with Deep Learning: A Comprehensive Survey") presents an overview of
    the categorization of the different VUDA methods.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，尽管上述VUDA方法使得跨视频领域的可迁移知识学习成为可能，但它们建立在一些约束和假设之上。这些包括视频源域和目标域共享相同标签空间的约束，以及在训练期间可以访问标记源数据和未标记目标数据的假设，并以动作识别作为跨域任务。在这些约束和假设下的VUDA被称为封闭集VUDA，可能在实际场景中不成立，因此在模型可移植性方面引发了数据安全等问题。近年来，已经有各种研究试图改变这些约束和假设，以使VUDA方法在实际场景中更具适用性，这些研究可以大致分为：a)
    不同标签空间约束的方法；b) 不同源数据假设的方法；c) 不同目标数据假设的方法；d) 不同跨域任务的方法。图[1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive
    Survey") 展示了不同VUDA方法的分类概述。'
- en: There had been prior surveys focusing on shallow and deep DA and UDA approaches
    with their applications in various image and natural language processing tasks.
    For example, [[10](#bib.bib10), [11](#bib.bib11)] surveyed shallow DA approaches
    for image tasks while [[12](#bib.bib12)] also briefly recapped some deep DA approaches.
    Subsequently, [[13](#bib.bib13)] further summarized other deep DA approaches for
    image tasks. Later, [[14](#bib.bib14)] outlined various UDA methods while [[15](#bib.bib15)]
    focused on UDA methods with deep learning. Several other works [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18)] discussed DA and UDA for various natural language
    processing tasks, such as machine translation and sentiment analysis. Meanwhile,
    there were also works surveyed on the broader transfer learning (TL) topic [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)], where domain adaptation
    can be viewed as a special case. Despite the effort made in surveying comprehensively
    DA, UDA, and TL, there has been no specific survey that investigates UDA for video
    tasks (i.e., VUDA). To the best of our knowledge, this is the first article that
    investigates and summarizes the recent progress of video unsupervised domain adaptation,
    where current works are generally deep learning-based. By summarizing existing
    literature, we propose the prospect of VUDA and the direction of future VUDA research.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的调查关注了浅层和深层的DA（领域适应）和UDA（无监督领域适应）方法及其在各种图像和自然语言处理任务中的应用。例如，[[10](#bib.bib10),
    [11](#bib.bib11)] 调查了图像任务的浅层DA方法，而 [[12](#bib.bib12)] 也简要回顾了一些深层DA方法。随后，[[13](#bib.bib13)]
    进一步总结了其他深层DA方法在图像任务中的应用。后来，[[14](#bib.bib14)] 概述了各种UDA方法，而 [[15](#bib.bib15)]
    专注于基于深度学习的UDA方法。还有一些其他研究 [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]
    讨论了各种自然语言处理任务的DA和UDA，例如机器翻译和情感分析。同时，也有关于更广泛的迁移学习（TL）主题的调查 [[19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]，其中领域适应可以被视为一个特例。尽管在全面调查DA、UDA和TL方面做出了努力，但尚未有专门调查视频任务（即VUDA）的UDA的文献。根据我们所知，这篇文章是首个调查和总结视频无监督领域适应近期进展的文章，当前的工作通常基于深度学习。通过总结现有文献，我们提出了VUDA的前景和未来VUDA研究的方向。
- en: 'The rest of the paper is organized as follows. Section [II](#S2 "II Definitions
    and Notations ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive
    Survey") defines the closed-set VUDA specifically and introduces the relevant
    notations. In Section [III](#S3 "III Methods for Closed-Set Video Unsupervised
    Domain Adaptation ‣ Video Unsupervised Domain Adaptation with Deep Learning: A
    Comprehensive Survey"), we review the deep learning-based closed-set VUDA methods,
    while methods for VUDA under different constraints, assumptions, and tasks are
    reviewed in Section [IV](#S4 "IV Methods for Video Unsupervised Domain Adaptation
    under Different Constraints, Assumptions, and Tasks ‣ Video Unsupervised Domain
    Adaptation with Deep Learning: A Comprehensive Survey"). We further summarize
    existing cross-domain video datasets for benchmarking VUDA in Section [V](#S5
    "V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣ Video Unsupervised
    Domain Adaptation with Deep Learning: A Comprehensive Survey"). Insights and future
    directions of VUDA research are discussed in Section [VI](#S6 "VI Discussion:
    Recent Progress and Future Directions ‣ Video Unsupervised Domain Adaptation with
    Deep Learning: A Comprehensive Survey") and the paper is concluded in Section [VII](#S7
    "VII Conclusion ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive
    Survey").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '论文的其余部分组织如下。第[II](#S2 "II Definitions and Notations ‣ Video Unsupervised Domain
    Adaptation with Deep Learning: A Comprehensive Survey")节具体定义了闭集VUDA并介绍了相关符号。在第[III](#S3
    "III Methods for Closed-Set Video Unsupervised Domain Adaptation ‣ Video Unsupervised
    Domain Adaptation with Deep Learning: A Comprehensive Survey")节，我们回顾了基于深度学习的闭集VUDA方法，而在第[IV](#S4
    "IV Methods for Video Unsupervised Domain Adaptation under Different Constraints,
    Assumptions, and Tasks ‣ Video Unsupervised Domain Adaptation with Deep Learning:
    A Comprehensive Survey")节中，方法在不同的约束、假设和任务下的VUDA进行了回顾。我们进一步总结了用于基准测试VUDA的现有跨域视频数据集，在第[V](#S5
    "V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣ Video Unsupervised
    Domain Adaptation with Deep Learning: A Comprehensive Survey")节中。第[VI](#S6 "VI
    Discussion: Recent Progress and Future Directions ‣ Video Unsupervised Domain
    Adaptation with Deep Learning: A Comprehensive Survey")节讨论了VUDA研究的见解和未来方向，而第[VII](#S7
    "VII Conclusion ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive
    Survey")节对论文进行了总结。'
- en: II Definitions and Notations
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 定义和符号
- en: 'In this section, we define the video unsupervised domain adaptation (VUDA)
    and introduce the relevant notations used in this survey. To maintain consistency
    with current VUDA works, the definitions and notations are defined with reference
    to [[23](#bib.bib23), [24](#bib.bib24)]. In VUDA, we are given a collection of
    $M_{S}$ video source domains and $\{\mathcal{D}_{S}^{1},\mathcal{D}_{S}^{2},...,\mathcal{D}_{S}^{M_{S}}\}$
    (which may or may not be accessible) and a collection of $M_{T}$ video target
    domains $\{\mathcal{D}_{T}^{1},\mathcal{D}_{T}^{2},...,\mathcal{D}_{T}^{M_{T}}\}$
    that are related to the collection of source domains. Each source domain contains
    $N_{S}^{k}$ labeled videos $\mathcal{D}_{S}^{k}=\{(V_{S}^{k,i},y_{S}^{k,i})\}^{N_{S}^{k}}_{i=1}$,
    characterized by the underlying probability distribution $p_{S}^{k}$ associated
    with the label space $\mathcal{Y}_{S}^{k}$ that contains $|\mathcal{C}_{S}^{k}|$
    classes. Meanwhile each target domain contains $N_{T}^{r}$ unlabeled videos $\mathcal{D}_{T}^{r}=\{V_{T}^{r,i}\}^{N_{T}^{r}}_{i=1}$
    characterized by the underlying probability distribution $p_{T}^{r}$ associated
    with the label space $\mathcal{Y}_{T}^{r}$ that contains $|\mathcal{C}_{T}^{r}|$
    classes. The goal of VUDA is to design a target model $G_{T}(.;\theta_{T})$ for
    the target domain that originates or is initialized from the source model $G_{S}(.;\theta_{S})$,
    which is capable of learning transferable features from the labeled source domains
    and minimize the empirical target risk $\epsilon_{T}$ across all target domains
    performed on certain video-based tasks (e.g., human action recognition, video
    segmentation or video quality assessment). The domain adaptation theory [[25](#bib.bib25)]
    proves that the empirical target risk $\epsilon_{T}$ is upper-bounded by three
    terms: a) the combined error of the ideal joint hypothesis on the source and target
    domains, which is assumed to be small so that domain adaptation can be achieved;
    b) the empirical source-domain error, and c) the divergence measured between source
    and target domains. All VUDA methods attempt to minimize $\epsilon_{T}$ by minimizing
    the third term and/or the second term which would be discussed in detail in subsequent
    Sections.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们定义了视频无监督领域适应（VUDA），并介绍了本调查中使用的相关符号。为了与当前VUDA工作保持一致，定义和符号参考了[[23](#bib.bib23),
    [24](#bib.bib24)]。在VUDA中，我们给定了一个包含$M_{S}$个视频源领域和$\{\mathcal{D}_{S}^{1},\mathcal{D}_{S}^{2},...,\mathcal{D}_{S}^{M_{S}}\}$（这些领域可能是可访问的，也可能不可访问）以及一个包含$M_{T}$个视频目标领域$\{\mathcal{D}_{T}^{1},\mathcal{D}_{T}^{2},...,\mathcal{D}_{T}^{M_{T}}\}$的集合，这些目标领域与源领域集合相关。每个源领域包含$N_{S}^{k}$个标记视频$\mathcal{D}_{S}^{k}=\{(V_{S}^{k,i},y_{S}^{k,i})\}^{N_{S}^{k}}_{i=1}$，由与标记空间$\mathcal{Y}_{S}^{k}$相关的基础概率分布$p_{S}^{k}$特征化，该空间包含$|\mathcal{C}_{S}^{k}|$个类别。同时，每个目标领域包含$N_{T}^{r}$个未标记视频$\mathcal{D}_{T}^{r}=\{V_{T}^{r,i}\}^{N_{T}^{r}}_{i=1}$，由与标记空间$\mathcal{Y}_{T}^{r}$相关的基础概率分布$p_{T}^{r}$特征化，该空间包含$|\mathcal{C}_{T}^{r}|$个类别。VUDA的目标是设计一个针对目标领域的模型$G_{T}(.;\theta_{T})$，该模型源自或初始化于源模型$G_{S}(.;\theta_{S})$，能够从标记源领域中学习可迁移特征，并最小化所有目标领域上某些基于视频的任务（例如，人类动作识别、视频分割或视频质量评估）的经验目标风险$\epsilon_{T}$。领域适应理论[[25](#bib.bib25)]证明，经验目标风险$\epsilon_{T}$由三个项上界：a)
    在源领域和目标领域上的理想联合假设的综合误差，假设该误差较小，从而实现领域适应；b) 经验源领域误差，以及c) 测量源领域与目标领域之间的分歧。所有VUDA方法都试图通过最小化第三项和/或第二项来最小化$\epsilon_{T}$，这些将在后续章节中详细讨论。
- en: 'While the above definition can be viewed as a general scenario of VUDA, it
    could be too challenging to tackle. Therefore, existing works would normally apply
    certain constraints or assumptions towards the general scenario to form scenarios
    that could be better tackled. The most common scenario is the closed-set VUDA
    scenario, which sets the below constraints and assumptions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述定义可以视为VUDA的一个一般场景，但它可能过于具有挑战性。因此，现有的工作通常会对这一一般场景施加某些约束或假设，以形成更容易处理的场景。最常见的场景是闭集VUDA场景，它设置了以下约束和假设：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: There would be only $M_{S}=1$ video source domain and only $M_{T}=1$ target
    domain (superscripts $k$ and $r$ are therefore omitted for simplicity).
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅有一个视频源领域$M_{S}=1$和一个目标领域$M_{T}=1$（因此为了简化省略了上标$k$和$r$）。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Both the source domain videos $\mathcal{D}_{S}$ and the source model $G_{S}(.;\theta_{S})$
    are accessible.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 源领域视频$\mathcal{D}_{S}$和源模型$G_{S}(.;\theta_{S})$是可访问的。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The source and target domain videos share the same label space (i.e., $\mathcal{Y}_{S}=\mathcal{Y}_{T}$
    and $|\mathcal{C}_{S}|=|\mathcal{C}_{T}|$).
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 源域和目标域的视频共享相同的标签空间（即 $\mathcal{Y}_{S}=\mathcal{Y}_{T}$ 和 $|\mathcal{C}_{S}|=|\mathcal{C}_{T}|$）。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The source and target domain videos share the same model (i.e., $G_{S}(.;\theta_{S})=G_{T}(.;\theta_{T})$.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 源域和目标域的视频共享相同的模型（即 $G_{S}(.;\theta_{S})=G_{T}(.;\theta_{T})$）。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The video-based task to be performed is assumed to be the human action recognition
    task.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行的视频任务被假定为人体动作识别任务。
- en: Based on the constraints and assumptions for the closed-set VUDA scenario, notations
    would be simplified while the superscripts $k,r$ are omitted and the joint label
    space $\mathcal{Y}$ contains $\mathcal{C}$ classes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据闭集 VUDA 场景的约束和假设，符号将简化，同时省略上标 $k,r$，并且联合标签空间 $\mathcal{Y}$ 包含 $\mathcal{C}$
    类。
- en: III Methods for Closed-Set Video Unsupervised Domain Adaptation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 闭集视频无监督领域适应的方法
- en: 'In this section, we review the various deep learning-based closed-set VUDA
    methods whose training and testing follow the constraints and assumptions as presented
    in Section [II](#S2 "II Definitions and Notations ‣ Video Unsupervised Domain
    Adaptation with Deep Learning: A Comprehensive Survey"). As briefly mentioned
    in Section [I](#S1 "I Introduction ‣ Video Unsupervised Domain Adaptation with
    Deep Learning: A Comprehensive Survey"), deep learning-based closed-set VUDA methods
    could be generally categorized into five categories. We discuss each category
    in the following sections. The list of all reviewed closed-set VUDA methods is
    collated and compared in Tab. [I](#S3.T1 "Table I ‣ III Methods for Closed-Set
    Video Unsupervised Domain Adaptation ‣ Video Unsupervised Domain Adaptation with
    Deep Learning: A Comprehensive Survey").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们回顾了各种基于深度学习的闭集 VUDA 方法，其训练和测试遵循第 [II](#S2 "II Definitions and Notations
    ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")
    节中介绍的约束和假设。正如第 [I](#S1 "I Introduction ‣ Video Unsupervised Domain Adaptation
    with Deep Learning: A Comprehensive Survey") 节中简要提到的，基于深度学习的闭集 VUDA 方法可以大致分为五类。我们在以下章节中讨论每一类别。所有回顾的闭集
    VUDA 方法的列表在表 [I](#S3.T1 "Table I ‣ III Methods for Closed-Set Video Unsupervised
    Domain Adaptation ‣ Video Unsupervised Domain Adaptation with Deep Learning: A
    Comprehensive Survey") 中汇总和比较。'
- en: 'TABLE I: Different categories of methods for closed-set VUDA. Methods are listed
    in chronological order.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 闭集 VUDA 的不同方法类别。方法按时间顺序列出。'
- en: '| Categories | Brief Descriptions | Methods |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 简要描述 | 方法 |'
- en: '| --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Adversarial-based | Domain discriminators to encourage domain confusion through
    adversarial objectives across video domain. | DAAA [[26](#bib.bib26)], TA³N [[27](#bib.bib27)],
    TCoN [[28](#bib.bib28)], MM-SADA [[29](#bib.bib29)], MA²L-TD [[30](#bib.bib30)],
    CIA [[31](#bib.bib31)] |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 基于对抗的 | 通过对抗目标鼓励领域混淆的领域判别器。 | DAAA [[26](#bib.bib26)], TA³N [[27](#bib.bib27)],
    TCoN [[28](#bib.bib28)], MM-SADA [[29](#bib.bib29)], MA²L-TD [[30](#bib.bib30)],
    CIA [[31](#bib.bib31)] |'
- en: '| Discrepancy-based | Discrepancy between domains are explicitly computed,
    align domains by applying metric learning approaches. | AMLS [[26](#bib.bib26)],
    PTC [[32](#bib.bib32)] |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 基于差异的 | 领域之间的差异被显式计算，通过应用度量学习方法来对齐领域。 | AMLS [[26](#bib.bib26)], PTC [[32](#bib.bib32)]
    |'
- en: '| Semantic-based | Domain-invariant features are obtained by exploiting the
    shared semantics across domains. | STCDA [[33](#bib.bib33)], CMCo [[34](#bib.bib34)],
    CoMix [[35](#bib.bib35)], CO²A [[36](#bib.bib36)], A³R [[37](#bib.bib37)], DVM [[38](#bib.bib38)]
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 基于语义的 | 通过利用跨领域的共享语义来获得领域不变特征。 | STCDA [[33](#bib.bib33)], CMCo [[34](#bib.bib34)],
    CoMix [[35](#bib.bib35)], CO²A [[36](#bib.bib36)], A³R [[37](#bib.bib37)], DVM [[38](#bib.bib38)]
    |'
- en: '| Composite | Exploit a composite of approaches to capitalize on the strength
    of each approach. | NEC-Drone [[39](#bib.bib39)], SAVA [[40](#bib.bib40)], PASTN [[41](#bib.bib41)],
    MAN [[42](#bib.bib42)], ACAN [[23](#bib.bib23)] |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 组合的 | 利用方法的组合来发挥每种方法的优势。 | NEC-Drone [[39](#bib.bib39)], SAVA [[40](#bib.bib40)],
    PASTN [[41](#bib.bib41)], MAN [[42](#bib.bib42)], ACAN [[23](#bib.bib23)] |'
- en: '| Reconstruction-based | Domain-invariant features from encoder-decoder networks
    with data-reconstruction objectives. | TranSVAE [[43](#bib.bib43)] |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 基于重建的 | 从编码器-解码器网络中获得的领域不变特征，具有数据重建目标。 | TranSVAE [[43](#bib.bib43)] |'
- en: III-A Adversarial-based VUDA Methods
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于对抗的 VUDA 方法
- en: With the assumption that the source empirical risk would be small with supervised
    learning on source videos, methods have been proposed to achieve effective domain
    adaptation by minimizing the discrepancy between the source and target domains.
    Intuitively, if the source and target domains share the same data distribution,
    the domain discriminators would be unable to identify whether a video originates
    from the source or target domain. Such intuition, along with the success of Generative
    Adversarial Networks (GANs) [[44](#bib.bib44)], motivates the proposal of adversarial-based
    VUDA methods, where additional domain discriminators are leveraged to encourage
    domain confusion through adversarial objectives across the source and target video
    domains. The discrepancy between the source and target domains is therefore minimized
    implicitly. Adversarial-based domain adaptation methods have previously found
    their success in image-based tasks (e.g., image recognition [[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)], object detection [[49](#bib.bib49),
    [50](#bib.bib50)] and person re-identification [[51](#bib.bib51)]), therefore
    it is intuitive to extend adversarial-based methods to VUDA.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设通过对源视频进行监督学习，源经验风险较小的情况下，已经提出了一些方法来通过最小化源域和目标域之间的差异来实现有效的领域适应。直观地说，如果源域和目标域共享相同的数据分布，领域鉴别器将无法识别视频是来自源域还是目标域。这种直观认识，加上生成对抗网络（GANs）[[44](#bib.bib44)]的成功，激发了基于对抗的VUDA方法的提出，其中利用额外的领域鉴别器，通过对源域和目标域视频之间的对抗目标来鼓励领域混淆。因此，源域和目标域之间的差异被隐式地最小化。基于对抗的领域适应方法以前在图像任务（例如，图像识别[[45](#bib.bib45)，[46](#bib.bib46)，[47](#bib.bib47)，[48](#bib.bib48)]，目标检测[[49](#bib.bib49)，[50](#bib.bib50)]和人脸重识别[[51](#bib.bib51)]）中取得了成功，因此将基于对抗的方法扩展到VUDA是直观的。
- en: One primitive work in this category is the Deep Adversarial Action Adaptation
    (DAAA) [[26](#bib.bib26)], where the original imaged-based DANN [[45](#bib.bib45),
    [52](#bib.bib52)] is extended to videos by substituting the image extractor (2D-CNN [[53](#bib.bib53)])
    with the clip/video extractor (3D-CNN [[54](#bib.bib54)]) while the input is changed
    from images to clips, which is formed by sampling frames from the videos. Though
    achieving much better performances compared to shallow domain adaptation methods,
    DAAA ignores the difference between the source-target discrepancies of spatial
    and temporal features, adapting spatial and temporal features uniformly and indiscriminately.
    Subsequently, Temporal Attentive Adversarial Adaptation Network (TA³N) [[27](#bib.bib27)]
    leverages on Temporal Relation Network (TRN) [[55](#bib.bib55)] to obtain more
    explicit temporal features, and align videos with both spatial and temporal features.
    TA³N is designed to focus dynamically on aligning the temporal dynamics which
    have higher domain discrepancy to effectively align the temporal features of videos.
    To achieve this, TA³N adopts an adaptive non-parametric attention mechanism based
    on domain prediction entropy. The strategy of aligning and adapting spatial and
    temporal features separately is further extended to the alignment of multiple
    levels of temporal features separately in MA²L-TD [[30](#bib.bib30)], with each
    level of temporal feature corresponding to the different length of video segment/clip
    generated by a temporally-dilated feature aggregation module. MA²L-TD assigns
    attention weights to the alignment of different levels by the degree of domain
    confusion in each level, where larger weights are assigned to levels where the
    corresponding domain discriminator cannot classify domains correctly.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类别中的一个原始工作是深度对抗动作适应（DAAA）[[26](#bib.bib26)]，其中原始的基于图像的DANN[[45](#bib.bib45),
    [52](#bib.bib52)]通过用剪辑/视频提取器（3D-CNN [[54](#bib.bib54)]）替代图像提取器（2D-CNN [[53](#bib.bib53)]），并将输入从图像改为由视频中采样的帧形成的剪辑来扩展到视频。尽管与浅层领域适应方法相比，DAAA取得了更好的性能，但它忽略了空间和时间特征之间源-目标差异的不同，对空间和时间特征进行了统一和不加区分的适应。随后，时间注意对抗适应网络（TA³N）[[27](#bib.bib27)]利用时间关系网络（TRN）[[55](#bib.bib55)]来获得更明确的时间特征，并使视频在空间和时间特征上对齐。TA³N的设计重点是动态对齐具有更高领域差异的时间动态，以有效对齐视频的时间特征。为此，TA³N采用了一种基于领域预测熵的自适应非参数注意机制。空间和时间特征分别对齐和适应的策略在MA²L-TD[[30](#bib.bib30)]中进一步扩展到多个时间特征级别的分别对齐，其中每个时间特征级别对应于由时间膨胀特征聚合模块生成的不同长度的视频片段/剪辑。MA²L-TD根据每个级别的领域混淆程度为不同级别的对齐分配注意权重，其中较大的权重分配给领域判别器无法正确分类领域的级别。
- en: Meanwhile, videos contain a series of non-key frames whose noisy background
    information is unrelated to the action and could affect adaptation negatively.
    Temporal Co-attention Network (TCoN) [[28](#bib.bib28)] copes with non-key frames
    by selecting the key segments that are critical for cross-domain action recognition.
    TCoN selects key segments by computing attention scores of each segment based
    on action informativeness and cross-domain similarity, obtained by a self-attention-inspired
    cross-domain co-attention matrix. Instead of direct adaptation across the source
    and target video features, TCoN adapts target video features to the source ones
    by constructing target-aligned source video features via transforming the original
    source video features through the cross-domain co-attention matrix.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，视频包含一系列与动作无关且可能对适应产生负面影响的非关键帧。时间共同注意网络（TCoN）[[28](#bib.bib28)]通过选择对跨领域动作识别至关重要的关键片段来处理非关键帧。TCoN通过计算每个片段的注意得分来选择关键片段，这些得分基于动作信息量和跨领域相似性，通过受自注意力启发的跨领域共同注意矩阵获得。TCoN不是直接对源和目标视频特征进行适应，而是通过构建目标对齐的源视频特征，通过跨领域共同注意矩阵转换原始源视频特征来对目标视频特征进行适应。
- en: Besides spatial and temporal features which are generally obtained from the
    RGB modality, videos also contain information about other modalities, such as
    optical flow and audio modalities. The multi-modality nature of videos poses more
    challenges towards VUDA as domain shift would be incurred for each modality. Methods
    have therefore been proposed to align source and target videos leveraging on the
    multi-modality information. Among these, MM-SADA [[29](#bib.bib29)] leverages
    the RGB and optical flow modalities, where adversarial alignment is applied to
    each modality separately. MM-SADA further adopts self-supervision learning across
    different modalities to learn the temporal correspondence between the different
    modalities. More recently, Cross-modal Interactive Alignment (CIA) [[31](#bib.bib31)]
    aligns video features with RGB, optical flow, and audio modalities. CIA further
    observes that cross-modal alignment could conflict with cross-domain alignment
    in VUDA, therefore it enhances the transferability of each modality by cross-modal
    interaction through a Mutual Complementarity (MC) module. The different modalities
    are therefore refined by absorbing the transferable knowledge from other modalities
    before they are aligned across source and target domains.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通常从 RGB 模态获得的空间和时间特征外，视频还包含其他模态的信息，例如光流和音频模态。视频的多模态特性对 VUDA 提出了更多挑战，因为每个模态都会发生领域迁移。因此，提出了利用多模态信息对源视频和目标视频进行对齐的方法。在这些方法中，MM-SADA [[29](#bib.bib29)]
    利用 RGB 和光流模态，其中对每个模态分别应用对抗对齐。MM-SADA 进一步采用跨模态自监督学习来学习不同模态之间的时间对应关系。最近，Cross-modal
    Interactive Alignment (CIA) [[31](#bib.bib31)] 将视频特征与 RGB、光流和音频模态对齐。CIA 进一步观察到跨模态对齐可能与
    VUDA 中的跨领域对齐发生冲突，因此通过 Mutual Complementarity (MC) 模块通过跨模态交互增强每个模态的可迁移性。因此，在源领域和目标领域之间对齐之前，不同模态会通过吸收其他模态的可迁移知识进行优化。
- en: III-B Discrepancy-based VUDA Methods
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于差异的 VUDA 方法
- en: While adversarial-based VUDA methods achieve decent performances in various
    VUDA benchmarks, the above methods do not compute the discrepancy between source
    and target domains or measure such discrepancy implicitly. Moreover, previous
    studies [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58)] have shown that
    adversarial training is unstable and may lead to model collapse and non-convergence.
    Discrepancy-based VUDA methods are therefore proposed as the more intuitive and
    stable approach towards tackling VUDA by computing and minimizing the video domain
    discrepancy explicitly. An early method is as proposed in AMLS [[26](#bib.bib26)]
    where the target videos are modeled as a sequence of points on the Grassmann manifold [[59](#bib.bib59)]
    with each point corresponding to a collection of clips aligned temporally, and
    the source videos are modeled as a single point on the manifold. VUDA is tackled
    by minimizing the Frobenius norm [[60](#bib.bib60)] between the source point and
    the series of target points on the Grassmann manifold. Meanwhile, later method
    has also leveraged multi-modal information with discrepancy computation and minimization.
    The Pairwise Two-stream ConvNets (PTC) [[32](#bib.bib32)] minimizes the MMD [[3](#bib.bib3)]
    loss across both RGB and optical flow modalities achieving better performances
    on more complex domain shift scenarios. PTC further improves its generalizability
    by fusing the RGB and optical flow features through a self-attention weight mechanism
    and selection of training videos at the boundary of action classes through a sphere
    boundary sample-selecting scheme.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于对抗的 VUDA 方法在各种 VUDA 基准测试中表现不错，但上述方法并没有计算源领域和目标领域之间的差异，也没有隐式地衡量这种差异。此外，之前的研究 [[56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58)] 已经表明，对抗训练是不稳定的，可能会导致模型崩溃和不收敛。因此，提出了基于差异的方法作为解决
    VUDA 的更直观和稳定的方法，通过显式地计算和最小化视频领域的差异。早期的方法如在 AMLS [[26](#bib.bib26)] 中提出，其中目标视频被建模为
    Grassmann 流形上的一系列点 [[59](#bib.bib59)]，每个点对应于一组时间上对齐的片段，而源视频被建模为流形上的一个单一点。通过最小化源点与
    Grassmann 流形上目标点序列之间的 Frobenius 范数 [[60](#bib.bib60)] 来解决 VUDA。同时，后续方法还利用了多模态信息进行差异计算和最小化。Pairwise
    Two-stream ConvNets (PTC) [[32](#bib.bib32)] 在 RGB 和光流模态上最小化 MMD [[3](#bib.bib3)]
    损失，在更复杂的领域迁移场景中表现更好。PTC 通过自注意力权重机制融合 RGB 和光流特征，并通过球面边界样本选择方案选择位于动作类别边界的训练视频，从而进一步提高了其泛化能力。
- en: III-C Semantic-based VUDA methods
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 基于语义的 VUDA 方法
- en: 'Besides minimizing discrepancies explicitly by discrepancy-based methods and
    implicitly by adversarial-based methods, aligning source and target video domains
    can also be accomplished by semantic-based VUDA methods [[61](#bib.bib61)] which
    exploit the shared semantics across the source and target domains such that domain-invariant
    features are obtained. Intuitively, if the target video domain aligns well with
    the source video domain through a certain model, the model extracts source-like
    representations for target videos, semantics embedded within the source video
    domain should therefore be shared with the target domain. Typical implications
    of shared semantics across domains include: a) spatio-temporal association: frames
    and clips (under different modalities) of videos possess strong spatio-temporal
    association and are placed in the correct time order and pose; b) feature clustering:
    features related to the videos of the same action classes are clustered and closed
    to each other, whereby features related to videos of different action classes
    are placed further away; and c) modality correspondence: features extracted from
    the different modalities of the same video are close together.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过基于差异的方法显式最小化差异，通过对抗性方法隐式最小化差异之外，还可以通过基于语义的 VUDA 方法 [[61](#bib.bib61)] 来对齐源视频域和目标视频域，这些方法利用源域和目标域之间的共享语义，以便获得领域不变特征。直观上，如果目标视频域通过某种模型与源视频域对齐良好，那么模型会为目标视频提取类似源的表示，因此嵌入在源视频域中的语义应该与目标域共享。跨域共享语义的典型含义包括：a)
    时空关联：视频的帧和片段（在不同的模态下）具有强烈的时空关联，并且被置于正确的时间顺序和姿态中；b) 特征聚类：与相同行为类别的视频相关的特征被聚集在一起并且彼此接近，而与不同动作类别的视频相关的特征则被置于较远的位置；c)
    模态对应：从同一视频的不同模态中提取的特征彼此接近。
- en: One typical semantic-based method proposed is the Spatio-Temporal Contrastive
    Domain Adaptation (STCDA) [[33](#bib.bib33)], which mines video representations
    of both RGB and optical flow modalities by applying a contrastive loss on both
    the clip and video level such that frames and clips are spatially and temporally
    associated. STCDA further bridges the domain shift of source and target videos
    by a video-based contrastive alignment (VCA) which minimizes the distance of the
    intra-class source and target features and maximizes the distance of the inter-class
    source and target features on the Reproducing Kernel Hilbert Space (RKHS) [[62](#bib.bib62)].
    The labels of target videos are obtained by pseudo-labeling through clustering
    with the features of the labeled source videos. Contrastive learning has also
    been applied in CMCo [[34](#bib.bib34)] which aims to extract video features with
    modality correspondence across RGB and optical flow modalities. Similarly, CO²A [[36](#bib.bib36)]
    trains video feature extractors with the objective of feature clustering with
    contrastive learning, employed on both the clip and video levels. CO²A also introduced
    supervised contrastive learning [[6](#bib.bib6)] for source video feature learning.
    Furthermore, CO²A encourages coherent correspondence predictions between source/target
    video pairs. The correspondence predictions of the source/target video pair predict
    whether the source/target videos are of the same label, and are obtained from
    either the label/pseudo-label or the features of source/target videos trained
    with contrastive learning. CoMix [[35](#bib.bib35)] is another method leveraging
    contrastive learning for VUDA, where it enforces temporal speed invariance in
    videos which encourages features extracted from the same video yet sampled with
    different temporal speeds to be similar. CoMix further employs the supervised
    contrastive loss [[6](#bib.bib6)] to target data by computing pseudo-labels and
    selecting target data whose pseudo-labels are of high confidence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的基于语义的方法是时空对比领域适应（STCDA）[[33](#bib.bib33)]，它通过在剪辑和视频层面上应用对比损失来挖掘RGB和光流模态的视频表示，从而使得帧和剪辑在空间和时间上相关。STCDA
    通过视频基础的对比对齐（VCA）进一步弥合源视频和目标视频的领域偏移，最小化同类源特征和目标特征之间的距离，同时最大化异类源特征和目标特征之间的距离，操作于再生核希尔伯特空间（RKHS）[[62](#bib.bib62)]。目标视频的标签通过对标记源视频特征的聚类伪标注获得。对比学习也被应用于CMCo
    [[34](#bib.bib34)]，旨在提取跨RGB和光流模态的视频特征。类似地，CO²A [[36](#bib.bib36)] 通过对比学习训练视频特征提取器，目标是进行特征聚类，这在剪辑和视频层面上都被使用。CO²A
    还引入了监督对比学习[[6](#bib.bib6)] 用于源视频特征学习。此外，CO²A 鼓励源/目标视频对之间的一致性预测。源/目标视频对的对应预测预测源/目标视频是否具有相同的标签，这些预测来自于标签/伪标签或通过对比学习训练的源/目标视频的特征。CoMix
    [[35](#bib.bib35)] 是另一种利用对比学习进行 VUDA 的方法，它强制视频中的时间速度不变性，鼓励从同一视频中提取的特征，即使在不同时间速度下采样，也要相似。CoMix
    进一步使用监督对比损失[[6](#bib.bib6)] 针对目标数据，通过计算伪标签并选择伪标签置信度高的目标数据。
- en: Besides contrastive learning, mixing source and target domain samples (or equivalently
    leveraging MixUp [[63](#bib.bib63)] across the source and target domains) have
    proven to benefit unsupervised domain adaptation for image-based tasks [[64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66)] and improve model robustness. To further exploit
    shared action semantics, CoMix [[35](#bib.bib35)] adopts such a strategy that
    incorporates synthetic videos into its contrastive objective. The synthetic videos
    are obtained by mixing the background of the video from one domain with the video
    from another domain. More recently, DVM [[38](#bib.bib38)] leverages MixUp to
    address the domain-wise gap directly at the input level, where the target videos
    are fused with the source videos progressively on the pixel-level. The corresponding
    target videos of the source videos are selected by obtaining the pseudo-labels
    of the target videos and matching them with the given labels of the source videos.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对比学习，将源领域和目标领域样本混合（或等效地在源领域和目标领域之间利用MixUp [[63](#bib.bib63)]）已被证明有利于图像任务的无监督领域适应 [[64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66)]，并提升模型鲁棒性。为了进一步利用共享的动作语义，CoMix [[35](#bib.bib35)]采用了这种策略，将合成视频融入其对比目标。合成视频通过将一个领域的视频背景与另一个领域的视频混合获得。最近，DVM [[38](#bib.bib38)]利用MixUp在输入层面直接解决领域间差距，其中目标视频在像素级别上与源视频逐步融合。源视频的相应目标视频通过获得目标视频的伪标签并与源视频的给定标签匹配来选择。
- en: The above methods mostly deal with the domain gap between source and target
    videos with RGB and/or optical flow modalities. The A³R [[37](#bib.bib37)] observe
    that sounds of actions can act as natural domain-invariant cues. Unlike previous
    methods where pseudo-labels are obtained directly by applying a classifier to
    the RGB and/or optical flow input, A³R introduces an absent activity learning
    where audio predictions are leveraged to indicate which actions cannot be heard
    in the video, while visual predictions are further encouraged to have low probabilities
    for these ‘pseudo-absent’ actions. A³R further proposes audio-balanced learning
    which exploits audio in the source domain to cluster samples. Finally, A³R applies
    an audio-balanced loss where the rare actions are weighted higher to handle the
    semantic shift between domains.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法大多处理源视频和目标视频之间的领域差距，使用RGB和/或光流模态。A³R [[37](#bib.bib37)]观察到动作的声音可以作为自然的领域不变提示。与以往直接通过对RGB和/或光流输入应用分类器获得伪标签的方法不同，A³R引入了一种缺失活动学习，其中利用音频预测指示视频中听不到的动作，而视觉预测则进一步鼓励对这些‘伪缺失’动作的低概率。A³R进一步提出了音频平衡学习，利用源领域中的音频对样本进行聚类。最后，A³R应用了音频平衡损失，对稀有动作给予更高权重，以处理领域之间的语义转移。
- en: III-D Composite VUDA Methods
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 复合VUDA方法
- en: The above categories attempt to tackle VUDA from different perspectives, and
    all have their own strength and shortcomings. The adversarial-based approach is
    the more common approach thanks to its high performance and ease of implementation,
    yet it relies on unstable adversarial learning which may result in fragile convergence
    and requires additional domain discriminators during training. The divergence-based
    approach computes domain discrepancies explicitly without additional network components,
    and its optimization is more stable. However, it generally produces inferior performances
    compared to either the adversarial-based approach or the semantic-based approach.
    The semantic-based approach also results in high adaptation performances and can
    be extended to other scenarios of adaptation (e.g., source-free VUDA), but they
    are susceptible to noise and are optimized with higher computation cost.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述类别尝试从不同角度解决VUDA问题，各有优缺点。基于对抗的方法因其高性能和易于实现而更为常见，但它依赖于不稳定的对抗学习，可能导致脆弱的收敛，并且在训练过程中需要额外的领域判别器。基于发散的方法明确计算领域差异而无需额外的网络组件，优化更为稳定。然而，它通常产生的性能不如基于对抗的方法或基于语义的方法。基于语义的方法也能产生高适应性能，并且可以扩展到其他适应场景（例如，无源VUDA），但它们容易受到噪声影响，且优化需要更高的计算成本。
- en: To capitalize on the strength of each approach for a more effective VUDA, various
    VUDA methods exploit a composite of approaches. For example, the NEC-Drone [[39](#bib.bib39)]
    proposed to combine the adversarial-based approach with the semantic-based approach
    by applying a triplet loss on the source data to learn embeddings of the videos
    which are agnostic of the specific classes but are aware of being similar. Similarly,
    the Pairwise Attentive adversarial Spatio-Temporal Network (PASTN) [[41](#bib.bib41)]
    also exploits both the adversarial-based and semantic-based approaches. PASTN
    is designed as a pairwise network with dual domain discriminators, one of which
    is structured without backpropagation and outputs the transferability weights
    for attentive adversarial learning. A margin-based discrimination loss [[67](#bib.bib67)]
    is employed across the source and target video features instead of the contrastive
    loss to compress intra-class samples within a margin and push inter-class samples
    away. This could extract shared semantics across source and target video domains
    by promoting feature clustering while taking the intra-class data distribution
    into consideration. Another method that adopts the same combination of approaches
    is SAVA [[40](#bib.bib40)]. SAVA aligns source and target video domains adversarially
    while attending to more discriminative clips through an attention mechanism. Further,
    SAVA focuses on encouraging temporal association in videos by applying an auxiliary
    clip order prediction task, which is more efficient and computationally less intensive
    than applying a contrastive loss.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用每种方法的优点以实现更有效的VUDA，各种VUDA方法结合了不同的方法。例如，NEC-Drone [[39](#bib.bib39)] 提出了将基于对抗的方法与基于语义的方法结合，通过对源数据应用三元组损失来学习视频的嵌入，这些嵌入对特定类别无感知但对相似性有意识。类似地，Pairwise
    Attentive adversarial Spatio-Temporal Network (PASTN) [[41](#bib.bib41)] 也利用了基于对抗的方法和基于语义的方法。PASTN
    设计为一个成对网络，具有双域判别器，其中一个不使用反向传播结构，并输出用于注意对抗学习的可转移权重。通过在源视频和目标视频特征之间使用基于边距的判别损失 [[67](#bib.bib67)]，而不是对比损失，来在边距内压缩同类样本并推动不同类样本远离。这可以通过促进特征聚类来提取源和目标视频领域的共享语义，同时考虑到同类数据的分布。另一种采用相同方法组合的方法是SAVA [[40](#bib.bib40)]。SAVA通过注意机制对源视频和目标视频领域进行对抗性对齐，同时关注更具判别性的剪辑。此外，SAVA通过应用辅助剪辑顺序预测任务来鼓励视频中的时间关联，这比应用对比损失更高效、计算量更小。
- en: Besides combining the adversarial-based approach with the semantic-based approach,
    there have been other works that combine the adversarial-based approach with the
    discrepancy-based approach. For example, the Multiple-view Adversarial learning
    Network (MAN) [[42](#bib.bib42)] performs adversarial learning to obtain domain-invariant
    video features from both RGB and optical flow modalities, fused by a Self-Attention
    Fusion Network (SAFN). MAN further improves domain invariance by applying the
    MK-MMD [[3](#bib.bib3)] loss over the fused video features. The Adversarial Correlation
    Alignment Network (ACAN) [[23](#bib.bib23)] is the other VUDA method that tackles
    VUDA with the composition of the adversarial-based and the discrepancy-based approach.
    Besides aligning spatial and temporal features, ACAN proposes to align correlation
    features extracted as long-range dependencies of pixels across spatiotemporal
    dimensions [[68](#bib.bib68)] by applying the adversarial domain loss to both
    the spatiotemporal video features and the correlation features. ACAN further aligns
    the correlation features by aligning the joint distribution of correlation information,
    which is computed as the covariance of correlation information. This is achieved
    by minimizing the Pixel Correlation Discrepancy across the source and target video
    domains implemented as the distance of correlation information distribution on
    the RKHS.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将对抗性方法与语义方法结合外，还有其他工作将对抗性方法与差异性方法结合。例如，多视角对抗学习网络 (MAN) [[42](#bib.bib42)]
    通过对抗学习从 RGB 和光流模态中获取领域不变的视频特征，这些特征由自注意力融合网络 (SAFN) 融合。MAN 通过对融合的视频特征应用 MK-MMD
    [[3](#bib.bib3)] 损失进一步改善领域不变性。对抗相关对齐网络 (ACAN) [[23](#bib.bib23)] 是另一种 VUDA 方法，它结合了对抗性方法和差异性方法来处理
    VUDA。除了对齐空间和时间特征外，ACAN 还提出通过对空间时间维度上像素的长程依赖提取的相关特征来对齐相关特征 [[68](#bib.bib68)]，通过将对抗领域损失应用于空间时间视频特征和相关特征来实现。ACAN
    通过对齐相关信息的联合分布（作为相关信息的协方差计算）进一步对齐相关特征。这是通过最小化源域和目标域之间的像素相关差异实现的，具体实现为 RKHS 上的相关信息分布距离。
- en: III-E Reconstruction-based VUDA Methods
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 基于重建的 VUDA 方法
- en: Reconstruction-based VUDA methods deal with VUDA by obtaining domain-invariant
    features by an encoder-decoder network trained with data-reconstruction objectives.
    There had been some image-based domain adaptation works leveraging the reconstruction-based
    approach [[69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71)] thanks to its
    robustness to noise. However, there are few attempts in extending the reconstruction-based
    approach to VUDA due to the complexity of video reconstruction. TranSVAE [[43](#bib.bib43)]
    is a recent attempt in leveraging data-reconstruction objectives for VUDA. It
    aims to disentangle domain information from other information during adaptation
    by disentangling the cross-domain videos into domain-specific static variables
    and domain-invariant dynamic variables. With domain information obtained, the
    effect of domain discrepancy on the prediction task could be largely eliminated.
    The disentanglement is achieved through a Variational AutoEncoder (VAE)-structured [[72](#bib.bib72)]
    network which models the cross-domain video generation process. TranSVAE further
    ensures the disentanglement serves the adaptation purpose by applying objectives
    to constraint the latent factors during the disentanglement, such as minimizing
    mutual dependence across static and dynamic variables and applying task-specific
    supervision on the dynamic variable from the source domain.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于重建的 VUDA 方法通过使用具有数据重建目标的编码器-解码器网络来处理 VUDA，从而获得领域不变特征。由于其对噪声的鲁棒性，一些基于图像的领域适配工作利用了基于重建的方法
    [[69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71)]。然而，由于视频重建的复杂性，扩展基于重建的方法到
    VUDA 的尝试较少。TranSVAE [[43](#bib.bib43)] 是最近在 VUDA 中利用数据重建目标的尝试。它旨在通过将跨域视频分解为领域特定的静态变量和领域不变的动态变量，在适配过程中解开领域信息。获得领域信息后，可以大大消除领域差异对预测任务的影响。这一分解是通过一个变分自编码器
    (VAE) 结构的网络 [[72](#bib.bib72)] 来实现的，该网络建模了跨域视频生成过程。TranSVAE 通过应用目标约束潜在因素的分解过程，进一步确保分解服务于适配目的，例如最小化静态变量和动态变量之间的相互依赖，并对源领域的动态变量施加任务特定的监督。
- en: 'TABLE II: Different categories of methods for non-closed-set VUDA.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 非闭集 VUDA 的不同方法类别。'
- en: '| Differences from closed-set | Scenarios | Assumptions/Constraints | Methods
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 与闭集的差异 | 场景 | 假设/约束 | 方法 |'
- en: '| Label space constraint | partial-set VUDA (PVDA) | $\mathcal{Y}_{T}\subset\mathcal{Y}_{S}$
    and $&#124;\mathcal{C}_{T}&#124;<&#124;\mathcal{C}_{S}&#124;$ | PATAN [[24](#bib.bib24)],
    MCAN [[73](#bib.bib73)] |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 标签空间约束 | 部分集合VUDA (PVDA) | $\mathcal{Y}_{T}\subset\mathcal{Y}_{S}$ 和 $|\mathcal{C}_{T}|<|\mathcal{C}_{S}|$
    | PATAN [[24](#bib.bib24)], MCAN [[73](#bib.bib73)] |'
- en: '| open-set VUDA (OSVDA) | $\mathcal{Y}_{S}\subset\mathcal{Y}_{T}$ and $&#124;\mathcal{C}_{S}&#124;<&#124;\mathcal{C}_{T}&#124;$
    | DMDA [[74](#bib.bib74)] |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 开放集VUDA (OSVDA) | $\mathcal{Y}_{S}\subset\mathcal{Y}_{T}$ 和 $|\mathcal{C}_{S}|<|\mathcal{C}_{T}|$
    | DMDA [[74](#bib.bib74)] |'
- en: '| Source data assumption | multi-source VUDA (MSVDA) | $M_{S}>1,M_{T}=1$ |
    TAMAN [[75](#bib.bib75)] |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 源数据假设 | 多源VUDA (MSVDA) | $M_{S}>1, M_{T}=1$ | TAMAN [[75](#bib.bib75)] |'
- en: '| source-free VUDA (SFVDA) | $\mathcal{D}_{S}$ not accessible | ATCoN [[76](#bib.bib76)]
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 无源VUDA (SFVDA) | $\mathcal{D}_{S}$ 不可访问 | ATCoN [[76](#bib.bib76)] |'
- en: '| black-box VUDA (BVDA) | $\mathcal{D}_{S}$ and $\theta_{S}$ not accessible
    | EXTERN [[77](#bib.bib77)] |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 黑箱VUDA (BVDA) | $\mathcal{D}_{S}$ 和 $\theta_{S}$ 不可访问 | EXTERN [[77](#bib.bib77)]
    |'
- en: '| Target data assumption | zero-shot VUDA (VDG) | $\mathcal{D}_{T}$ not accessible
    | VideoDG [[78](#bib.bib78)], RNA-Net [[79](#bib.bib79)] |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 目标数据假设 | 零样本VUDA (VDG) | $\mathcal{D}_{T}$ 不可访问 | VideoDG [[78](#bib.bib78)],
    RNA-Net [[79](#bib.bib79)] |'
- en: '| Cross-domain tasks | Temporal action segmentation | - | MTDA [[80](#bib.bib80)],
    SSTDA [[81](#bib.bib81)] |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 跨域任务 | 时间动作分割 | - | MTDA [[80](#bib.bib80)], SSTDA [[81](#bib.bib81)] |'
- en: '| Video semantic segmentation | - | DA-VSN [[82](#bib.bib82)], TPS [[83](#bib.bib83)]
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 视频语义分割 | - | DA-VSN [[82](#bib.bib82)], TPS [[83](#bib.bib83)] |'
- en: '| Video quality assessment | - | UCDA [[84](#bib.bib84)] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 视频质量评估 | - | UCDA [[84](#bib.bib84)] |'
- en: '| Video Sign language recognition | - | Li et. al [[85](#bib.bib85)] |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 视频手语识别 | - | Li 等人 [[85](#bib.bib85)] |'
- en: IV Methods for Video Unsupervised Domain Adaptation under Different Constraints,
    Assumptions, and Tasks
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 不同约束、假设和任务下的视频无监督领域自适应方法
- en: 'The methods presented in Section [III](#S3 "III Methods for Closed-Set Video
    Unsupervised Domain Adaptation ‣ Video Unsupervised Domain Adaptation with Deep
    Learning: A Comprehensive Survey") improve video model generalizability and enables
    knowledge to be transferred from a labeled source domain to an unlabeled target
    domain in the closed-set scenario. However, the constraints and assumptions of
    the closed-set VUDA may not hold in real-world scenarios, which could prompt concerns
    about model portability. In this section, we review deep learning-based VUDA methods
    in VUDA scenarios under different constraints and assumptions, categorized into
    four categories as introduced in Section [I](#S1 "I Introduction ‣ Video Unsupervised
    Domain Adaptation with Deep Learning: A Comprehensive Survey"). We summarize and
    compare all reviewed non-closed-set VUDA methods as presented in Tab. [II](#S3.T2
    "Table II ‣ III-E Reconstruction-based VUDA Methods ‣ III Methods for Closed-Set
    Video Unsupervised Domain Adaptation ‣ Video Unsupervised Domain Adaptation with
    Deep Learning: A Comprehensive Survey").'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '第[III](#S3 "III Methods for Closed-Set Video Unsupervised Domain Adaptation
    ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")节中提出的方法提高了视频模型的泛化能力，并使知识能够在封闭集场景中从标注的源域转移到未标注的目标域。然而，封闭集VUDA的约束和假设在现实世界场景中可能不成立，这可能引发对模型可移植性的担忧。在本节中，我们回顾了基于深度学习的VUDA方法，在不同约束和假设下的VUDA场景中进行分类，共分为四类，如第[I](#S1
    "I Introduction ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive
    Survey")节所述。我们总结并比较了所有评审的非封闭集VUDA方法，如表[II](#S3.T2 "Table II ‣ III-E Reconstruction-based
    VUDA Methods ‣ III Methods for Closed-Set Video Unsupervised Domain Adaptation
    ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")所示。'
- en: IV-A Methods with Differed Label Space Constraint
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 不同标签空间约束的方法
- en: In the closed-set VUDA scenario, we assume that the source and target domain
    videos share the same label space (i.e., $\mathcal{Y}_{S}=\mathcal{Y}_{T}$ and
    $|\mathcal{C}_{S}|=|\mathcal{C}_{T}|$). With the presence of large-scale labeled
    public video datasets (e.g., Kinetics [[86](#bib.bib86)] and Something-Something [[87](#bib.bib87)]),
    it is more feasible in real-world scenarios to transfer representations learned
    in these large-scale video datasets to unlabeled small-scale video datasets. It
    is reasonable to assume that large-scale public video datasets can subsume categories
    of small-scale target datasets. Such a scenario is defined as the partial-set
    VUDA, or the Partial Video Domain Adaptation (PVDA) [[24](#bib.bib24)]. It relaxes
    the constraint of identical source and target label spaces by assuming that the
    target label space is a subspace of the source one (i.e.,  $\mathcal{Y}_{T}\subset\mathcal{Y}_{S}$
    and $|\mathcal{C}_{T}|<|\mathcal{C}_{S}|$). Compared to the closed-set VUDA, tackling
    PVDA poses more challenges due to the existence of outlier label space in the
    source domain denoted as $\mathcal{Y}_{out}=\mathcal{Y}_{S}\backslash\mathcal{Y}_{T}$,
    which causes negative transfer effect to the network’s performance on the target
    domain. Meanwhile, during the training of the network, only labels of the target
    domain data are unknown, hence the part of which $\mathcal{Y}_{S}$ shares with
    $\mathcal{Y}_{T}$ is unknown.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在闭集VUDA场景中，我们假设源域和目标域视频共享相同的标签空间（即，$\mathcal{Y}_{S}=\mathcal{Y}_{T}$ 和 $|\mathcal{C}_{S}|=|\mathcal{C}_{T}|$）。由于存在大规模标注的公共视频数据集（例如，Kinetics [[86](#bib.bib86)]
    和 Something-Something [[87](#bib.bib87)]），在实际场景中，将这些大规模视频数据集中学习到的表示迁移到未标注的小规模视频数据集是更可行的。合理的假设是，大规模公共视频数据集可以包含小规模目标数据集的类别。这种情况被定义为部分集VUDA，或部分视频领域适应（PVDA）[[24](#bib.bib24)]。它通过假设目标标签空间是源标签空间的子空间（即，$\mathcal{Y}_{T}\subset\mathcal{Y}_{S}$
    和 $|\mathcal{C}_{T}|<|\mathcal{C}_{S}|$），放宽了源和目标标签空间相同的限制。与闭集VUDA相比，处理PVDA由于源域中存在的异常标签空间$\mathcal{Y}_{out}=\mathcal{Y}_{S}\backslash\mathcal{Y}_{T}$而面临更多挑战，这会对网络在目标域上的表现产生负面迁移效应。同时，在网络训练过程中，仅目标域数据的标签是未知的，因此$\mathcal{Y}_{S}$与$\mathcal{Y}_{T}$共享的部分是未知的。
- en: With the inclusion of temporal features and multi-modal information (e.g., optical
    flow or audio), PVDA is also more challenging than its image-based counterpart
    (PDA [[88](#bib.bib88)]) as negative transfer could be additionally triggered
    by the alignment of either temporal features or the multi-modal information. The
    key to tackling PVDA lies in mitigating negative effects brought by the unknown
    outlier label space $\mathcal{Y}_{S}\backslash\mathcal{Y}_{T}$ leveraging on the
    additional temporal or multi-modality features effectively. The pioneering work,
    Partial Adversarial Temporal Attentive Network (PATAN) [[24](#bib.bib24)] proposes
    to tackle PVDA by the filtering of source-only outlier classes to mitigate negative
    transfer. To achieve this, PATAN leverages temporal features from two perspectives.
    Firstly, PATAN constructs temporal features such that those in outlier source-only
    classes discriminate those in the target classes by an attentive combination of
    local temporal features, where the attention builds upon the contribution of the
    local temporal features towards the class filtration process where source-only
    classes are filtered. Secondly, PATAN exploits temporal features toward the filtration
    of source-only classes to alleviate the effects of the misclassification of spatial
    features. Later, the Multi-modality Cluster-calibrated partial Adversarial Network
    (MCAN) [[73](#bib.bib73)] also tackles PVDA by filtering source-only outlier classes,
    exploiting the multi-modal features (optical flow feature) in addition to the
    spatial and temporal features leveraged in PATAN. MCAN further improves PVDA performance
    by dealing with label distribution shift [[89](#bib.bib89)] across the source
    and target domain by clustering video features and weighing them accordingly such
    that MCAN promotes positive transfers of relevant source data and suppresses negative
    transfers of irrelevant source data jointly.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间特征和多模态信息（例如，光流或音频）的包含，PVDA比基于图像的方法（PDA [[88](#bib.bib88)]) 更具挑战性，因为负迁移可能会因时间特征或多模态信息的对齐而额外触发。解决PVDA的关键在于有效利用额外的时间或多模态特征来减轻未知异常标签空间
    $\mathcal{Y}_{S}\backslash\mathcal{Y}_{T}$ 带来的负面影响。开创性工作，部分对抗时间注意网络（PATAN）[[24](#bib.bib24)]，提出通过过滤仅源异常类别来减轻负迁移。为此，PATAN从两个方面利用时间特征。首先，PATAN构建时间特征，使得那些在异常源类别中的特征可以通过局部时间特征的注意性组合来区分目标类别，其中注意力建立在局部时间特征对类别过滤过程的贡献上，源类别仅被过滤。其次，PATAN利用时间特征对源异常类别进行过滤，以减轻空间特征误分类的影响。之后，多模态簇校准部分对抗网络（MCAN）[[73](#bib.bib73)]
    也通过过滤源异常类别来处理PVDA，除了PATAN中利用的空间和时间特征外，还利用了多模态特征（光流特征）。MCAN进一步通过对源和目标领域的标签分布偏移[[89](#bib.bib89)]
    进行处理，通过对视频特征进行聚类并相应加权，从而促进相关源数据的正向迁移，抑制无关源数据的负向迁移。
- en: Another practical VUDA scenario with differed label space assumption takes agnostic
    classes into consideration, assuming that there are unknown action classes in
    the target video domain, denoted as the open-set VUDA, or the Open-Set Video Domain
    Adaptation (OSVDA). Under such scenario, the source label space is a subspace
    of the target one (i.e.,, $\mathcal{Y}_{S}\subset\mathcal{Y}_{T}$ and $|\mathcal{C}_{S}|<|\mathcal{C}_{T}|$).
    To tackle OSVDA, the Dual Metric Domain Adaptation framework (DMDA) [[74](#bib.bib74)]
    is proposed which involves spatial and temporal features. DMDA deals with OSVDA
    by a Dual Metric Discriminator (DMD) which measures similarities between source
    and target video samples with a pre-trained classifier combined with prototypical
    optimal transport, applied to the frame, clip, and video levels. The DMD is further
    exploited as an initial separation and trains a binary discriminator to further
    distinguish whether target samples belong to the source action classes or the
    agnostic action classes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种实际的VUDA场景，考虑了不同的标签空间假设，假设目标视频领域中存在未知的动作类别，称为开放集VUDA，或开放集视频领域适应（OSVDA）。在这种场景下，源标签空间是目标标签空间的一个子空间（即，$\mathcal{Y}_{S}\subset\mathcal{Y}_{T}$
    和 $|\mathcal{C}_{S}|<|\mathcal{C}_{T}|$）。为了解决OSVDA问题，提出了双重度量领域适应框架（DMDA）[[74](#bib.bib74)]，该框架涉及空间和时间特征。DMDA通过一个双重度量判别器（DMD）来处理OSVDA，该判别器使用预训练的分类器结合原型最优传输来测量源视频样本和目标视频样本之间的相似性，并应用于帧、片段和视频级别。DMD进一步被用作初步分离，并训练一个二元判别器，以进一步区分目标样本是否属于源动作类别或未知动作类别。
- en: IV-B Methods with Differed Source Data Assumption
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 方法与不同源数据假设
- en: In addition to the constraints of the same label space across the source and
    target video domains, closed-set VUDA also makes several assumptions about the
    source data. Specifically, closed-set VUDA first assumes that there would be only
    $M_{S}=1$ video source domain with videos matching a uniform data distribution
    whose knowledge would be transferred to the target domain. In practice, source
    data are more likely to be collected from multiple datasets (e.g., the action
    “Diving” can be found in datasets UCF101 [[90](#bib.bib90)], Kinetics [[86](#bib.bib86)]
    and Sports-1M [[91](#bib.bib91)]). This VUDA scenario is defined as the multi-source
    VUDA, or the Multi-Source Video Domain Adaptation (MSVDA) [[75](#bib.bib75)] which
    relaxes the constraint of identical source video data distribution by assuming
    that source video data are sampled from $M_{S}>1$ video domains corresponding
    to different video data distributions. The challenges of MSVDA lie in the negative
    transfer that would be triggered if domain shifts between multiple domain pairs
    are reduced directly regardless of their inconsistencies caused by distinct spatial
    and temporal feature distributions. The Temporal Attentive Moment Alignment Network
    (TAMAN) [[75](#bib.bib75)] is a discrepancy-based method designed for MSVDA. It
    deals with MSVDA by constructing global temporal features via attentively combining
    local temporal features, where the attention strategies depend on both the local
    temporal feature classification confidence, as well as the disparity between the
    global and local feature discrepancies. Furthermore, TAMAN aligns spatial-temporal
    features jointly by aligning the moments of both spatial and temporal features
    across all domain pairs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了源视频领域和目标视频领域之间具有相同标签空间的约束之外，闭集VUDA还对源数据提出了若干假设。具体而言，闭集VUDA首先假设只有一个视频源领域，即$M_{S}=1$，其视频匹配一个统一的数据分布，其知识将被转移到目标领域。实际上，源数据更可能来自多个数据集（例如，动作“跳水”可以在数据集UCF101 [[90](#bib.bib90)]、Kinetics [[86](#bib.bib86)]和Sports-1M [[91](#bib.bib91)]中找到）。这种VUDA场景被定义为多源VUDA，或称为多源视频领域适应（MSVDA） [[75](#bib.bib75)]，它通过假设源视频数据来自$M_{S}>1$个视频领域对应不同的视频数据分布，从而放宽了相同源视频数据分布的约束。MSVDA的挑战在于如果直接减少多个领域对之间的领域偏移而不考虑由于空间和时间特征分布不同而产生的不一致性，则可能会触发负迁移。时序注意力时刻对齐网络（TAMAN） [[75](#bib.bib75)]是一种基于差异的方法，旨在解决MSVDA。它通过专注地结合局部时序特征来构建全局时序特征，其中注意力策略依赖于局部时序特征分类的信心以及全局和局部特征差异之间的差异。此外，TAMAN通过对齐所有领域对中的空间和时序特征的时刻来联合对齐空间-时序特征。
- en: Closed-set VUDA also assumes that the source domain videos $V_{S}\in\mathcal{D}_{S}$
    are always accessible during adaptation. However, action information in the source
    video domain usually contains the private and sensitive information of the actors,
    including their actions and the relevant scenes which is usually irrelevant to
    those in the target domain in real-world applications and should be protected
    from the target domain. For example, in hospitals, the anomaly action recognition
    of patients is usually required but videos that contain patients’ information
    cannot be shared across different hospitals. Current closed-set VUDA methods would
    therefore raise serious privacy and model portability issues, which are more severe
    than that raised by image-based domain adaptation. To address the video data privacy
    and model portability issue, a more practical VUDA scenario is formulated as the
    source-free VUDA, or Source-Free Video Domain Adaptation (SFVDA). In this VUDA
    scenario, only the well-trained source video models denoted as $G_{S}(:,\theta_{S})$,
    would be provided along with the unlabeled target video domain data for adaptation.
    Here $\theta_{S}$ is the parameter of $G_{S}$. With the absence of source data,
    the VUDA methods as reviewed that require data from both target and source domains
    for implicit or explicit alignment cannot be applied. Recently, the Attentive
    Temporal Consistent Network (ATCoN) [[76](#bib.bib76)] is proposed to deal with
    SFVDA. ATCoN aims to tackle SFVDA by obtaining temporal features that satisfy
    the cross-temporal hypothesis which hypothesizes that local temporal features
    (clip features) are not only discriminative but also consistent across each other
    and possess similar feature distribution patterns. This hypothesis is satisfied
    by ATCoN through learning temporal consistency composed of both feature and source
    prediction consistency. ATCoN further aligns target data to the source data distribution
    without source data access by attending to local temporal features with high source
    prediction confidence. While SFVDA attempts to address privacy concerns in VUDA,
    it still relies on the well-trained source model parameters which allow generative
    models [[44](#bib.bib44)] to recover source videos. Inspired by black-box unsupervised
    domain adaptation [[92](#bib.bib92)] in image-based domain adaptation, the black-box
    VUDA or Black-box Video Domain Adaptation (BVDA) is formulated more recently where
    the source video model is provided for adaptation only as a black-box predictor
    (e.g., API service). In other words, both the source domain $\mathcal{D}_{S}$
    and $\theta_{S}$ are not accessible. To tackle the more challenging BVDA, EXTERN [[77](#bib.bib77)]
    is proposed which is designed to adapt target models to the embedded semantic
    information of the source data resorting to the hard or soft predictions of the
    target domain from the black-box source predictor. EXTERN aims to extract effective
    temporal features in a self-supervised manner with high discriminability and complies
    with the cluster assumption [[93](#bib.bib93)] where regularizations are applied
    over clip features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 闭集 VUDA 还假设在适应过程中源领域视频 $V_{S}\in\mathcal{D}_{S}$ 始终可用。然而，源视频领域中的动作信息通常包含演员的私人和敏感信息，包括他们的动作和相关场景，这些在实际应用中通常与目标领域的情况无关，且应当从目标领域中保护。例如，在医院中，通常需要对患者的异常动作进行识别，但包含患者信息的视频不能在不同医院之间共享。因此，当前的闭集
    VUDA 方法会引发严重的隐私和模型可移植性问题，这些问题比图像基础领域适应方法所引发的要严重。为了应对视频数据隐私和模型可移植性问题，提出了更实际的 VUDA
    场景，即无源 VUDA，或称源无视频领域适应（SFVDA）。在这个 VUDA 场景中，只提供经过良好训练的源视频模型 $G_{S}(:,\theta_{S})$，以及未标记的目标视频领域数据用于适应。这里的
    $\theta_{S}$ 是 $G_{S}$ 的参数。在没有源数据的情况下，回顾的 VUDA 方法要求同时有来自目标和源领域的数据进行隐式或显式对齐，因此不能应用。最近，提出了注意力时间一致网络（ATCoN）[[76](#bib.bib76)]
    来处理 SFVDA。ATCoN 旨在通过获取满足跨时间假设的时间特征来解决 SFVDA，这一假设认为局部时间特征（片段特征）不仅具有区分性，而且在彼此之间保持一致，并具有类似的特征分布模式。ATCoN
    通过学习包括特征和源预测一致性的时间一致性来满足这一假设。ATCoN 通过关注具有高源预测信心的局部时间特征，进一步将目标数据对齐到源数据分布而无需源数据访问。虽然
    SFVDA 尝试解决 VUDA 中的隐私问题，但仍然依赖于经过良好训练的源模型参数，这使得生成模型[[44](#bib.bib44)]能够恢复源视频。受到图像基础领域适应中黑箱无监督领域适应[[92](#bib.bib92)]
    的启发，最近提出了黑箱 VUDA 或称黑箱视频领域适应（BVDA），在这种方法中，源视频模型仅作为黑箱预测器（例如 API 服务）提供用于适应。换句话说，源领域
    $\mathcal{D}_{S}$ 和 $\theta_{S}$ 都不可访问。为了解决更具挑战性的 BVDA，提出了 EXTERN [[77](#bib.bib77)]，该方法旨在通过利用黑箱源预测器的硬预测或软预测将目标模型适应到源数据的嵌入语义信息中。EXTERN
    旨在以自监督的方式提取具有高区分度的有效时间特征，并遵守聚类假设[[93](#bib.bib93)]，其中对片段特征应用了正则化。
- en: IV-C Methods with Differed Target Data Assumption
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 针对不同目标数据假设的方法
- en: Besides assumptions made on the source data, closed-set VUDA also supposes that
    the target domain data is readily available. This assumption may also not hold
    when applying to actual applications, since prior knowledge of target data distribution
    is not guaranteed. It is more practical to assume that the target domain is unseen
    (i.e., data of the target domain is unavailable) during adaptation, defined as
    the zero-shot VUDA, or Video Domain Generalization (VDG). Closed-set VUDA methods
    are also inadequate towards VDG, since similar to SFVDA, the domain discrepancy
    between source and target domains cannot be computed or estimated without knowledge
    of target data distribution. VideoDG [[78](#bib.bib78)] identifies the key towards
    VDG is to strike a balance between generalizability and discriminability, achieved
    by expanding the frame relations of the source domain such that they are diverse
    enough to be generalized to potential target domains while remaining discriminative.
    Inspired by the Transformer [[94](#bib.bib94)] and the Adversarial Domain Augmentation
    (ADA) [[95](#bib.bib95)], VideoDG reaches such balance with the introduction of
    the Adversarial Pyramid Network (APN) trained with the Robust Adversarial Domain
    Augmentation (RADA). Meanwhile, the RNA-Net [[79](#bib.bib79)] makes use of the
    multi-modal nature of videos by leveraging both audio and RGB features for tackling
    VDG. RNA-Net suggests that fusing multi-modal information naively may not bring
    improvements to the generalizability of models [[96](#bib.bib96)] due to certain
    modalities being privileged over the others. Therefore, it proposes a cross-modal
    audio-visual Relative Norm Alignment (RNA) loss which aims to progressively align
    the relative feature norms of the two modalities from the source domains such
    that domain-invariant audio-visual features are obtained for VDG.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对源数据所做的假设，封闭集VUDA还假设目标领域数据是 readily available。这一假设在实际应用中也可能不成立，因为目标数据分布的先验知识并不保证。在适应过程中，更实际的假设是目标领域未见过（即目标领域数据不可用），这被定义为零-shot
    VUDA，或视频领域泛化（VDG）。封闭集VUDA方法对于VDG也不够充分，因为类似于SFVDA，源领域和目标领域之间的领域差异无法在没有目标数据分布知识的情况下进行计算或估计。VideoDG
    [[78](#bib.bib78)] 识别了VDG的关键是平衡泛化能力和可区分性，通过扩展源领域的帧关系，使其多样化，以便能够泛化到潜在的目标领域，同时保持区分性。受到Transformer
    [[94](#bib.bib94)] 和对抗性领域增强（ADA） [[95](#bib.bib95)] 的启发，VideoDG通过引入与鲁棒对抗性领域增强（RADA）训练的对抗金字塔网络（APN）达到了这种平衡。同时，RNA-Net
    [[79](#bib.bib79)] 通过利用视频的多模态特性，结合音频和RGB特征来解决VDG问题。RNA-Net建议，单纯融合多模态信息可能不会提高模型的泛化能力
    [[96](#bib.bib96)]，因为某些模态可能比其他模态更具优势。因此，它提出了一种跨模态音频-视觉相对范数对齐（RNA）损失，旨在逐步对齐源领域两种模态的相对特征范数，从而获得领域不变的音频-视觉特征用于VDG。
- en: IV-D Methods with Differed Cross-Domain Tasks
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 针对不同跨领域任务的方法
- en: For all the aforementioned works, the VUDA methods are designed for the cross-domain
    action recognition task, which is one of the most fundamental video-based task [[97](#bib.bib97),
    [98](#bib.bib98)]. Besides action recognition, there have been various studies
    on other cross-domain video tasks, such as cross-domain temporal action segmentation.
    For the cross-domain temporal action segmentation task, the Mixed Temporal Domain
    Adaptation (MTDA) [[80](#bib.bib80)] is proposed as an adversarial-based method.
    MTDA deals with cross-domain temporal action segmentation by jointly aligning
    local and global embedded feature spaces while integrating a domain attention
    mechanism based on domain predictions to aggregate domain-specific frames for
    constructing global video representations. Subsequently, the Self-Supervised Temporal
    Domain Adaptation (SSTDA) [[81](#bib.bib81)] is proposed for the same cross-domain
    temporal action segmentation task. SSTDA leverages the adversarial-based approach
    for aligning source and target videos by integrating two self-supervised auxiliary
    tasks (i.e., the binary and sequential domain prediction tasks), performed on
    the frame and clip levels respectively.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述所有工作，VUDA 方法是为跨领域动作识别任务设计的，这是最基本的视频任务之一[[97](#bib.bib97), [98](#bib.bib98)]。除了动作识别，还有许多研究涉及其他跨领域视频任务，如跨领域时间动作分割。对于跨领域时间动作分割任务，提出了混合时间领域自适应（MTDA）[[80](#bib.bib80)]，这是一种基于对抗的方法。MTDA
    通过联合对齐局部和全局嵌入特征空间，同时整合基于领域预测的领域注意机制，以聚合领域特定的帧来构建全局视频表示，从而处理跨领域时间动作分割。随后，提出了自监督时间领域自适应（SSTDA）[[81](#bib.bib81)]，用于相同的跨领域时间动作分割任务。SSTDA
    利用对抗性方法，通过整合两个自监督辅助任务（即二元领域预测任务和顺序领域预测任务），在帧和片段级别上对源视频和目标视频进行对齐。
- en: Cross-domain video semantic segmentation is another task studied for VUDA, which
    is relevant towards robust and efficient deployment of vision-based autonomous
    driving systems [[99](#bib.bib99), [100](#bib.bib100)]. DA-VSN [[82](#bib.bib82)]
    is one pioneer work that tackles cross-domain video semantic segmentation by introducing
    temporal consistency regularization (TCR) to bridge the domain gap. The TCR consists
    of two components, the cross-domain TCR which minimizes the discrepancy between
    source and target video domains by guiding target predictions to have the same
    temporal consistency as the source ones, and the intra-domain TCR which guides
    unconfident target predictions to have the same temporal consistency as the confident
    ones. Lately, the Temporal Pseudo-Supervision (TPS) [[83](#bib.bib83)] is proposed
    inspired by the success of consistency training [[101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103)] on image-based domain adaptation. TPS explores consistency
    training in the spatiotemporal feature space by enforcing model predictions to
    be invariant to cross-frame augmentations which are applied to the unlabeled target
    video frames.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域视频语义分割是 VUDA 研究的另一个任务，它与视觉自主驾驶系统的强健和高效部署相关[[99](#bib.bib99), [100](#bib.bib100)]。DA-VSN[[82](#bib.bib82)]
    是一个开创性的工作，通过引入时间一致性正则化（TCR）来解决跨领域视频语义分割问题。TCR 包含两个组件：跨领域 TCR 通过引导目标预测具有与源预测相同的时间一致性来最小化源和目标视频领域之间的差异，以及领域内
    TCR，它引导不确定的目标预测具有与确定的目标预测相同的时间一致性。最近，提出了时间伪监督（TPS）[[83](#bib.bib83)]，灵感来自于在基于图像的领域自适应中一致性训练的成功[[101](#bib.bib101),
    [102](#bib.bib102), [103](#bib.bib103)]。TPS 通过强制模型预测对应用于未标记目标视频帧的跨帧增强保持不变，探索了在时空特征空间中的一致性训练。
- en: Besides the aforementioned segmentation tasks, an Unsupervised Curriculum Domain
    Adaptation (UCDA) [[84](#bib.bib84)] is proposed for the task of cross-modal video
    quality assessment (VQA) with VUDA which aims to output the quality score of the
    target videos given a set of source videos. UCDA deals with the task of cross-domain
    VQA through a two-stage adversarial adaptation with an uncertainty-based ranking
    function to sort the samples from the target domain into a different subdomain.
    Cross-domain video sign language recognition [[85](#bib.bib85)] is another novel
    task investigated by VUDA research that aims to recognize isolated sign words
    from the sign words available in web news. Li et. al [[85](#bib.bib85)] propose
    a coarse domain alignment approach for this task by jointly training a classifier
    on news signs and isolated signs to reduce the domain gap. In addition, they develop
    a prototypical memory to learn a domain-invariant descriptor for each isolated
    sign.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述分割任务之外，还提出了一种无监督课程领域适应（UCDA）[[84](#bib.bib84)]，用于跨模态视频质量评估（VQA）任务，结合VUDA的目标是根据一组源视频输出目标视频的质量分数。UCDA通过两阶段对抗性适应和基于不确定性的排序函数来处理跨领域VQA任务，以将目标领域的样本排序到不同的子领域中。跨领域视频手语识别[[85](#bib.bib85)]是VUDA研究调查的另一个新任务，旨在从网络新闻中的手语词汇中识别孤立的手语词。Li等[[85](#bib.bib85)]为此任务提出了一种粗略领域对齐的方法，通过在新闻手语和孤立手语上联合训练分类器来缩小领域差距。此外，他们还开发了一种原型记忆，以学习每个孤立手语的领域不变描述符。
- en: 'TABLE III: Comparison of current cross-domain video benchmark datasets.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：当前跨领域视频基准数据集的比较。
- en: '| Dataset | # Classes | # Train/Test Videos | Source of Data | VUDA Scenarios
    | Tasks | Year | Website |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类别数 | 训练/测试视频数量 | 数据来源 | VUDA场景 | 任务 | 年份 | 网站 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| UCF-Olympic [[104](#bib.bib104)] | 6 | 851/294 | UCF50, Olympic Sports |
    Closed-Set | Action recognition | 2014 | [Website](https://github.com/cmhungsteve/TA3N)
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| UCF-Olympic [[104](#bib.bib104)] | 6 | 851/294 | UCF50, 奥林匹克体育 | 闭集 | 动作识别
    | 2014 | [网站](https://github.com/cmhungsteve/TA3N) |'
- en: '| UCF-HMDB[small] [[104](#bib.bib104)] | 5 | 832/339 | UCF50, HMDB51 | Closed-Set
    | Action recognition | 2014 | [Website](https://github.com/cmhungsteve/TA3N) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| UCF-HMDB[小型] [[104](#bib.bib104)] | 5 | 832/339 | UCF50, HMDB51 | 闭集 | 动作识别
    | 2014 | [网站](https://github.com/cmhungsteve/TA3N) |'
- en: '| UCF-HMDB[full] [[27](#bib.bib27)] | 12 | 2278/931 | UCF50, HMDB51 | Closed-Set
    | Action recognition | 2019 | [Website](https://github.com/cmhungsteve/TA3N) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| UCF-HMDB[完整] [[27](#bib.bib27)] | 12 | 2278/931 | UCF50, HMDB51 | 闭集 | 动作识别
    | 2019 | [网站](https://github.com/cmhungsteve/TA3N) |'
- en: '| Kinetics-Gameplay [[27](#bib.bib27)] | 12 | 46003/3995 | Kinetics-600, Gameplay
    | Closed-Set | Action recognition | 2019 | [Website](https://github.com/cmhungsteve/TA3N)
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Kinetics-Gameplay [[27](#bib.bib27)] | 12 | 46003/3995 | Kinetics-600, Gameplay
    | 闭集 | 动作识别 | 2019 | [网站](https://github.com/cmhungsteve/TA3N) |'
- en: '| HMDB-ARID [[105](#bib.bib105)] | 11 | 3058/1153 | HMDB51, ARID | Closed-Set
    | Action recognition | 2021 | [Website](https://xuyu0010.github.io/vuda.html)
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| HMDB-ARID [[105](#bib.bib105)] | 11 | 3058/1153 | HMDB51, ARID | 闭集 | 动作识别
    | 2021 | [网站](https://xuyu0010.github.io/vuda.html) |'
- en: '| Kinetics$\to$NEC-Drone [[106](#bib.bib106)] | 7 | Total 5250 | Kinetics-400,
    NEC-Drone | Closed-Set | Action recognition | 2020 | [Website](https://www.nec-labs.com/%C2%A0mas/NEC-Drone/)
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Kinetics$\to$NEC-Drone [[106](#bib.bib106)] | 7 | 总计 5250 | Kinetics-400,
    NEC-Drone | 闭集 | 动作识别 | 2020 | [网站](https://www.nec-labs.com/%C2%A0mas/NEC-Drone/)
    |'
- en: '| Mixamo$\to$Kinetics [[36](#bib.bib36)] | 14 | 24533/11662 | Mixamo, Kinetics-700
    | Closed-Set | Action recognition | 2022 | [Website](https://github.com/vturrisi/CO2A)
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Mixamo$\to$Kinetics [[36](#bib.bib36)] | 14 | 24533/11662 | Mixamo, Kinetics-700
    | 闭集 | 动作识别 | 2022 | [网站](https://github.com/vturrisi/CO2A) |'
- en: '| ActorShift [[37](#bib.bib37)] | 7 | 1305/200 | Kinetics-700, YouTube | Closed-Set
    | Action recognition | 2022 | [Website](https://xiaobai1217.github.io/DomainAdaptation/)
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ActorShift [[37](#bib.bib37)] | 7 | 1305/200 | Kinetics-700, YouTube | 闭集
    | 动作识别 | 2022 | [网站](https://xiaobai1217.github.io/DomainAdaptation/) |'
- en: '| UCF-HMDB[partial] [[24](#bib.bib24)] | 14 | 2304/476 | UCF101, HMDB51 | Partial
    | Action recognition | 2021 | [Website](https://xuyu0010.github.io/pvda.html)
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| UCF-HMDB[部分] [[24](#bib.bib24)] | 14 | 2304/476 | UCF101, HMDB51 | 部分 | 动作识别
    | 2021 | [网站](https://xuyu0010.github.io/pvda.html) |'
- en: '| MiniKinetics-UCF [[24](#bib.bib24)] | 45 | 20996/1106 | MiniKinetics, UCF101
    | Partial | Action recognition | 2021 | [Website](https://xuyu0010.github.io/pvda.html)
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MiniKinetics-UCF [[24](#bib.bib24)] | 45 | 20996/1106 | MiniKinetics, UCF101
    | 部分 | 动作识别 | 2021 | [网站](https://xuyu0010.github.io/pvda.html) |'
- en: '| HMDB-ARID[partial] [[24](#bib.bib24)] | 10 | 2712/540 | HMDB51, ARID | Partial
    | Action recognition | 2021 | [Website](https://xuyu0010.github.io/pvda.html)
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| HMDB-ARID[部分] [[24](#bib.bib24)] | 10 | 2712/540 | HMDB51, ARID | 部分 | 行为识别
    | 2021 | [网站](https://xuyu0010.github.io/pvda.html) |'
- en: '| EPIC Kitchens [[29](#bib.bib29)] | 8 | 7935/2159 | EPIC Kitchens | Closed-Set
    | Action recognition | 2020 | [Website](https://EPIC%20Kitchens.github.io/2021)
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| EPIC Kitchens [[29](#bib.bib29)] | 8 | 7935/2159 | EPIC Kitchens | 封闭集 |
    行为识别 | 2020 | [网站](https://EPIC%20Kitchens.github.io/2021) |'
- en: '| Daily-DA [[75](#bib.bib75)] | 8 | 16295/2654 | ARID, HMDB51, Moments in Time,
    Kinetics-600 | Multi-Source/ Closed-Set | Action recognition | 2021 | [Website](https://xuyu0010.github.io/msvda.html)
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Daily-DA [[75](#bib.bib75)] | 8 | 16295/2654 | ARID, HMDB51, Moments in Time,
    Kinetics-600 | 多源/封闭集 | 行为识别 | 2021 | [网站](https://xuyu0010.github.io/msvda.html)
    |'
- en: '| Sports-DA [[75](#bib.bib75)] | 23 | 36003/4712 | UCF101, Sports-1M, Kinetics-600
    | Multi-Source/ Closed-Set | Action recognition | 2021 | [Website](https://xuyu0010.github.io/msvda.html)
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Sports-DA [[75](#bib.bib75)] | 23 | 36003/4712 | UCF101, Sports-1M, Kinetics-600
    | 多源/封闭集 | 行为识别 | 2021 | [网站](https://xuyu0010.github.io/msvda.html) |'
- en: '| VIPER$\to$Cityscapes-Seq [[82](#bib.bib82)] | 30 | 136645/500 | VIPER, Cityscapes-Seq
    | Closed-Set | Semantic segmentation | 2021 | [Website](https://github.com/Dayan-Guan/DA-VSN)
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| VIPER$\to$Cityscapes-Seq [[82](#bib.bib82)] | 30 | 136645/500 | VIPER, Cityscapes-Seq
    | 封闭集 | 语义分割 | 2021 | [网站](https://github.com/Dayan-Guan/DA-VSN) |'
- en: '| SYNTHIA-Seq$\to$Cityscapes-Seq [[82](#bib.bib82)] | 30 | 10975/500 | SYNTHIA-Seq,
    Cityscapes-Seq | Closed-Set | Semantic segmentation | 2021 | [Website](https://github.com/Dayan-Guan/DA-VSN)
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SYNTHIA-Seq$\to$Cityscapes-Seq [[82](#bib.bib82)] | 30 | 10975/500 | SYNTHIA-Seq,
    Cityscapes-Seq | 封闭集 | 语义分割 | 2021 | [网站](https://github.com/Dayan-Guan/DA-VSN)
    |'
- en: 'TABLE IV: Average accuracy ($\%$) on Primary VUDA Datasets. Methods are arranged
    in chronological order.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 主体 VUDA 数据集上的平均准确率（$\%$）。方法按时间顺序排列。'
- en: '| Methods | Backbones | Categories | UCF-Olympic | UCF-HMDB[small] | UCF-HMDB[full]
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | 类别 | UCF-Olympic | UCF-HMDB[小] | UCF-HMDB[全] |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| AMLS [[26](#bib.bib26)] | C3D [[107](#bib.bib107)] | Discrepancy | 85.24
    | 92.45 | - |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| AMLS [[26](#bib.bib26)] | C3D [[107](#bib.bib107)] | 差异 | 85.24 | 92.45 |
    - |'
- en: '| DAAA [[26](#bib.bib26)] | C3D [[107](#bib.bib107)] | Adversarial | 90.78
    | - | - |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DAAA [[26](#bib.bib26)] | C3D [[107](#bib.bib107)] | 对抗 | 90.78 | - | - |'
- en: '| TA³N [[27](#bib.bib27)] | TRN [[55](#bib.bib55)] | Adversarial | 95.54 |
    99.40 | 80.06 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| TA³N [[27](#bib.bib27)] | TRN [[55](#bib.bib55)] | 对抗 | 95.54 | 99.40 | 80.06
    |'
- en: '| TCoN [[28](#bib.bib28)] | TRN [[55](#bib.bib55)] | Adversarial | 94.95 |
    96.78 | 88.15 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| TCoN [[28](#bib.bib28)] | TRN [[55](#bib.bib55)] | 对抗 | 94.95 | 96.78 | 88.15
    |'
- en: '| SAVA [[40](#bib.bib40)] | I3D [[108](#bib.bib108)] | Composite | - | - |
    86.70 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| SAVA [[40](#bib.bib40)] | I3D [[108](#bib.bib108)] | 复合 | - | - | 86.70 |'
- en: '| MM-SADA [[29](#bib.bib29)] | I3D [[108](#bib.bib108)] | Adversarial | - |
    - | 87.65 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| MM-SADA [[29](#bib.bib29)] | I3D [[108](#bib.bib108)] | 对抗 | - | - | 87.65
    |'
- en: '| PASTN [[41](#bib.bib41)] | TR3D [[41](#bib.bib41)] | Composite | 99.05 |
    - | - |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| PASTN [[41](#bib.bib41)] | TR3D [[41](#bib.bib41)] | 复合 | 99.05 | - | - |'
- en: '| STCDA [[33](#bib.bib33)] | BNIncep [[109](#bib.bib109)]/I3D [[108](#bib.bib108)]
    | Semantic | 99.35 | 97.20 | 87.60 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| STCDA [[33](#bib.bib33)] | BNIncep [[109](#bib.bib109)]/I3D [[108](#bib.bib108)]
    | 语义 | 99.35 | 97.20 | 87.60 |'
- en: '| CMCo [[34](#bib.bib34)] | I3D [[108](#bib.bib108)] | Semantic | - | - | 88.75
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| CMCo [[34](#bib.bib34)] | I3D [[108](#bib.bib108)] | 语义 | - | - | 88.75 |'
- en: '| CoMix [[35](#bib.bib35)] | I3D [[108](#bib.bib108)] | Semantic | - | - |
    90.30 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| CoMix [[35](#bib.bib35)] | I3D [[108](#bib.bib108)] | 语义 | - | - | 90.30
    |'
- en: '| MAN [[42](#bib.bib42)] | ResNet-152 [[110](#bib.bib110)] | Composite | 94.80
    | - | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| MAN [[42](#bib.bib42)] | ResNet-152 [[110](#bib.bib110)] | 复合 | 94.80 | -
    | - |'
- en: '| ACAN [[23](#bib.bib23)] | MFNet [[111](#bib.bib111)] | Composite | - | -
    | 89.50 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| ACAN [[23](#bib.bib23)] | MFNet [[111](#bib.bib111)] | 复合 | - | - | 89.50
    |'
- en: '| CO²A [[36](#bib.bib36)] | I3D [[108](#bib.bib108)] | Semantic | 98.75 | -
    | 91.80 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| CO²A [[36](#bib.bib36)] | I3D [[108](#bib.bib108)] | 语义 | 98.75 | - | 91.80
    |'
- en: '| MA²l-TD [[30](#bib.bib30)] | ResNet-101 [[110](#bib.bib110)] | Adversarial
    | 97.36 | 99.40 | 85.80 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| MA²l-TD [[30](#bib.bib30)] | ResNet-101 [[110](#bib.bib110)] | 对抗 | 97.36
    | 99.40 | 85.80 |'
- en: '| CIA [[31](#bib.bib31)] | I3D [[108](#bib.bib108)] | Adversarial | - | - |
    93.26 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| CIA [[31](#bib.bib31)] | I3D [[108](#bib.bib108)] | 对抗 | - | - | 93.26 |'
- en: '| DVM [[38](#bib.bib38)] | TSM [[112](#bib.bib112)] | Semantic | 96.37 | -
    | 92.77 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DVM [[38](#bib.bib38)] | TSM [[112](#bib.bib112)] | 语义 | 96.37 | - | 92.77
    |'
- en: '| TranSVAE [[43](#bib.bib43)] | I3D [[108](#bib.bib108)] | Reconstruction |
    - | - | 93.37 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| TranSVAE [[43](#bib.bib43)] | I3D [[108](#bib.bib108)] | 重建 | - | - | 93.37
    |'
- en: '| ATCoN (SFVDA) [[76](#bib.bib76)] | TRN [[55](#bib.bib55)] | - | - | - | 82.51
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| ATCoN (SFVDA) [[76](#bib.bib76)] | TRN [[55](#bib.bib55)] | - | - | - | 82.51
    |'
- en: '| EXTERN (BVDA) [[77](#bib.bib77)] | TRN [[55](#bib.bib55)] | - | - | - | 90.42
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| EXTERN (BVDA) [[77](#bib.bib77)] | TRN [[55](#bib.bib55)] | - | - | - | 90.42
    |'
- en: V Benchmark Datasets for Video Unsupervised Domain Adaptation
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 基准数据集用于视频无监督领域自适应
- en: 'An important factor in the development of deep learning methods is the availability
    of relevant datasets for the training and evaluation of the proposed methods.
    This also applies to the development of research in VUDA methods. Over the past
    decade, there has been a significant increase in cross-domain video datasets,
    which greatly facilitates and promotes research in the various VUDA scenarios.
    In this section, we review and summarize existing cross-domain video datasets
    for VUDA. An overall comparison of existing datasets over their major attributes
    (number of action classes, number of training/testing videos, source of data,
    etc.) is presented in Tab. [III](#S4.T3 "Table III ‣ IV-D Methods with Differed
    Cross-Domain Tasks ‣ IV Methods for Video Unsupervised Domain Adaptation under
    Different Constraints, Assumptions, and Tasks ‣ Video Unsupervised Domain Adaptation
    with Deep Learning: A Comprehensive Survey"). Furthermore, we show the average
    performance of the methods on their respective benchmarked datasets in Tabs. [IV](#S4.T4
    "Table IV ‣ IV-D Methods with Differed Cross-Domain Tasks ‣ IV Methods for Video
    Unsupervised Domain Adaptation under Different Constraints, Assumptions, and Tasks
    ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey"), [V](#S5.T5
    "Table V ‣ V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣ Video
    Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey"), [VI](#S5.T6
    "Table VI ‣ V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣ Video
    Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey") [VII](#S5.T7
    "Table VII ‣ V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣ Video
    Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey"), and [VIII](#S5.T8
    "Table VIII ‣ V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣
    Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey").
    Note that due to the different backbones and training techniques applied by the
    different methods, direct comparison of their performance may not be fair and
    only serves as an intuitive reference towards the comparison of each method. All
    average performances are reported based on the original paper in which the respective
    methods are proposed when applicable.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习方法发展的一个重要因素是相关数据集的可用性，以便对提出的方法进行训练和评估。这同样适用于VUDA方法的研究。在过去十年中，跨领域视频数据集显著增加，这大大促进了各种VUDA场景的研究。在这一部分，我们回顾和总结了现有的跨领域视频数据集。现有数据集的主要属性（动作类别数量、训练/测试视频数量、数据来源等）的总体比较见表[III](#S4.T3
    "Table III ‣ IV-D Methods with Differed Cross-Domain Tasks ‣ IV Methods for Video
    Unsupervised Domain Adaptation under Different Constraints, Assumptions, and Tasks
    ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")。此外，我们展示了方法在其各自基准数据集上的平均性能，详见表[IV](#S4.T4
    "Table IV ‣ IV-D Methods with Differed Cross-Domain Tasks ‣ IV Methods for Video
    Unsupervised Domain Adaptation under Different Constraints, Assumptions, and Tasks
    ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")、[V](#S5.T5
    "Table V ‣ V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣ Video
    Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")、[VI](#S5.T6
    "Table VI ‣ V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣ Video
    Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")、[VII](#S5.T7
    "Table VII ‣ V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣ Video
    Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")和[VIII](#S5.T8
    "Table VIII ‣ V Benchmark Datasets for Video Unsupervised Domain Adaptation ‣
    Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey")。请注意，由于不同方法应用了不同的骨干网络和训练技术，直接比较其性能可能不公平，仅作为各方法比较的直观参考。所有平均性能均基于提出相应方法的原始论文进行报告（如适用）。'
- en: Primary VUDA Datasets. Earlier VUDA works typically rely on two sets of primary
    cross-domain action recognition datasets, namely the UCF-Olympic dataset [[104](#bib.bib104)]
    and the UCF-HMDB dataset [[104](#bib.bib104)]. The UCF-HMDB dataset [[104](#bib.bib104)]
    is subsequently denoted as the UCF-HMDB[small] dataset to differentiate with a
    later dataset. Specifically, the UCF-Olympic dataset is built across the UCF50 [[113](#bib.bib113)]
    and the Olympic Sports [[114](#bib.bib114)] datasets, while the UCF-HMDB[small]
    dataset is built across the UCF50 and the HMDB51 [[115](#bib.bib115)] datasets.
    Both cross-domain datasets are of a very small scale with limited action classes
    and training/testing videos. Subsequently, a larger UCF-HMDB[full] [[27](#bib.bib27)]
    dataset is introduced to facilitate further research on VUDA. The UCF-HMDB[full]
    dataset is also built across the UCF50 and the HMDB51 datasets, but with more
    than doubled number of classes compared to the UCF-HMDB[small] dataset and contains
    much more videos. The UCF-HMDB[full] dataset has become one of the most commonly
    used benchmark datasets for VUDA research.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的 VUDA 数据集。早期的 VUDA 工作通常依赖于两个主要的跨域动作识别数据集，即 UCF-Olympic 数据集 [[104](#bib.bib104)]
    和 UCF-HMDB 数据集 [[104](#bib.bib104)]。其中，UCF-HMDB 数据集 [[104](#bib.bib104)] 后来被称为
    UCF-HMDB[small] 数据集，以便与后来的数据集区分。具体而言，UCF-Olympic 数据集是基于 UCF50 [[113](#bib.bib113)]
    和 Olympic Sports [[114](#bib.bib114)] 数据集构建的，而 UCF-HMDB[small] 数据集是基于 UCF50 和
    HMDB51 [[115](#bib.bib115)] 数据集构建的。这两个跨域数据集规模非常小，动作类别和训练/测试视频数量有限。随后，引入了更大的 UCF-HMDB[full] [[27](#bib.bib27)]
    数据集，以促进 VUDA 的进一步研究。UCF-HMDB[full] 数据集也是基于 UCF50 和 HMDB51 数据集构建的，但类别数量比 UCF-HMDB[small]
    数据集多了两倍以上，视频数量也大大增加。UCF-HMDB[full] 数据集已经成为 VUDA 研究中最常用的基准数据集之一。
- en: VUDA Datasets with Larger Domain Shifts. The aforementioned datasets are all
    built on datasets whose videos are collected mostly on web platforms (e.g., YouTube)
    with videos shot in normal conditions (e.g., normal illumination and contrast
    with clear pictures). Therefore the domain shifts across the different domains
    may not be significant. Consequently, the generalizability of VUDA approaches
    well-performed on the aforementioned datasets would be low in real-world applications
    where the domain shifts may be much larger. To cope with such limitations, cross-domain
    VUDA datasets with larger domain shifts are introduced. One example is the Kinetics-Gameplay [[27](#bib.bib27)]
    dataset which bridges real-world videos with virtual-world videos. Kinetics-Gameplay
    is built with the Kinetic [[86](#bib.bib86)] and the Gameplay [[27](#bib.bib27)]
    datasets collected from current video games. Another cross-domain dataset that
    bridges real-world and synthetic videos is the Mixamo$\to$Kinetics dataset [[36](#bib.bib36)],
    built as a uni-directional dataset to transfer knowledge from synthetic videos
    built from the Mixamo system to the real-world videos of the Kinetics dataset.
    Another scenario where large domain shifts may encounter during adaptation is
    between regular human-captured videos and drone-captured videos, where drone-captured
    videos possess unique characteristics thanks to their distinct motions and perspectives.
    The Kinetics$\to$NEC-Drone [[106](#bib.bib106)] is introduced to leverage the
    existing large-scale Kinetics to aid video models to perform action recognition
    on the challenging drone-captured videos in the NEC-Drone [[106](#bib.bib106)]
    dataset. Meanwhile, large domain shifts could also occur due to significant differences
    in video statistics, such as between videos shot under normal illumination and
    videos shot under low illumination (or more generally, between videos shot under
    normal environment and adverse environment). To explore how to leverage current
    datasets to boost performance on videos shot in adverse environments, the HMDB-ARID
    dataset [[23](#bib.bib23)] is introduced. This dataset comprises videos from the
    HMDB51 and the ARID [[105](#bib.bib105)], whose videos are shot under adverse
    illumination conditions and with low contrast. Lately, the ActorShift [[37](#bib.bib37)]
    dataset is proposed to research the domain shift between human and animal actions,
    which is the first dataset to consider non-human actions. The source domain of
    human actions is collected from Kinetics-700 [[116](#bib.bib116)] dataset while
    the target domain of animal actions is collected directly from YouTube with the
    relevant action classes.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: VUDA 数据集具有更大的领域变化。前述数据集都是基于主要从网络平台（例如 YouTube）收集的视频，这些视频拍摄于正常条件下（例如正常照明和对比度，画面清晰）。因此，不同领域之间的领域变化可能不显著。因此，VUDA
    方法在上述数据集上的表现较好，但在实际应用中，领域变化可能会大得多，泛化能力较低。为应对这些限制，引入了具有更大领域变化的跨领域 VUDA 数据集。例如，Kinetics-Gameplay [[27](#bib.bib27)]
    数据集桥接了现实世界视频和虚拟世界视频。Kinetics-Gameplay 数据集基于当前视频游戏中的 Kinetic [[86](#bib.bib86)]
    和 Gameplay [[27](#bib.bib27)] 数据集。另一个桥接现实世界和合成视频的跨领域数据集是 Mixamo$\to$Kinetics 数据集 [[36](#bib.bib36)]，这是一个单向数据集，用于将
    Mixamo 系统中的合成视频知识转移到 Kinetics 数据集中的真实世界视频。另一个大领域变化的场景可能出现在常规人类拍摄视频和无人机拍摄视频之间，无人机拍摄的视频由于其独特的运动和视角具有独特的特点。Kinetics$\to$NEC-Drone [[106](#bib.bib106)]
    数据集被引入，以利用现有的大规模 Kinetics 数据集，帮助视频模型在 NEC-Drone [[106](#bib.bib106)] 数据集中的无人机拍摄视频上进行动作识别。同时，由于视频统计数据的显著差异（例如正常照明下拍摄的视频与低照明下拍摄的视频，或更普遍地说，正常环境下拍摄的视频与恶劣环境下拍摄的视频），也可能发生大领域变化。为探索如何利用当前数据集提升在恶劣环境下拍摄的视频性能，引入了
    HMDB-ARID 数据集 [[23](#bib.bib23)]。该数据集包括来自 HMDB51 和 ARID [[105](#bib.bib105)] 的视频，这些视频是在恶劣照明条件下拍摄，且对比度低。最近，提出了
    ActorShift [[37](#bib.bib37)] 数据集，用于研究人类和动物动作之间的领域变化，这是第一个考虑非人类动作的数据集。人类动作的源领域数据来自
    Kinetics-700 [[116](#bib.bib116)] 数据集，而动物动作的目标领域数据直接从 YouTube 中相关的动作类别收集。
- en: 'Partial-set VUDA (PVDA) Datasets. The datasets above are all constructed for
    the closed-set VUDA scenario where there exist only two source-target video domain
    pairs (i.e., Domain A$\to$Domain B and Domain B$\to$Domain A) whose label spaces
    are shared. However, as mentioned in Section [IV](#S4 "IV Methods for Video Unsupervised
    Domain Adaptation under Different Constraints, Assumptions, and Tasks ‣ Video
    Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey"), constraints
    and assumptions for closed-set VUDA may not hold in real-world scenarios. Therefore,
    other cross-domain video datasets are introduced to support and facilitate research
    on VUDA with different constraints and assumptions. For partial-set VUDA (PVDA),
    a collection of three cross-domain partial-set video datasets is introduced in [[24](#bib.bib24)],
    namely UCF-HMDB[partial], MiniKinetics-UCF, and HMDB-ARID[partial]. Among which
    the UCF-HMDB[partial] is constructed inspired by UCF-HMDB[full] [[27](#bib.bib27)],
    built across the UCF101 and HMDB51 datasets. The MiniKinetics-UCF dataset is of
    much larger scale (8$\times$ that of UCF-HMDB[partial]) and is designed to validate
    the effectiveness of PVDA approaches on large-scale datasets. It is built from
    the Mini-Kinetics [[117](#bib.bib117)] and UCF101 datasets. Meanwhile, the HMDB-ARID[partial]
    is built inspired by the HMDB-ARID [[23](#bib.bib23)] dataset and aims to boost
    the performance of video models on low-illumination model leveraging normal videos
    under the partial-set VUDA with larger domain shift.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '部分集VUDA（PVDA）数据集。上述数据集都构建于封闭集VUDA场景，其中只有两个源目标视频领域对（即Domain A$\to$Domain B和Domain
    B$\to$Domain A）共享标签空间。然而，如第[IV](#S4 "IV Methods for Video Unsupervised Domain
    Adaptation under Different Constraints, Assumptions, and Tasks ‣ Video Unsupervised
    Domain Adaptation with Deep Learning: A Comprehensive Survey")节所提及，封闭集VUDA的约束和假设在现实世界中可能不成立。因此，引入了其他跨域视频数据集，以支持和促进在不同约束和假设下的VUDA研究。对于部分集VUDA（PVDA），介绍了三种跨域部分集视频数据集，分别是UCF-HMDB[partial]、MiniKinetics-UCF和HMDB-ARID[partial]，参见[[24](#bib.bib24)]。其中，UCF-HMDB[partial]的构建灵感来源于UCF-HMDB[full]
    [[27](#bib.bib27)]，该数据集跨越了UCF101和HMDB51数据集。MiniKinetics-UCF数据集规模更大（是UCF-HMDB[partial]的8$\times$），旨在验证PVDA方法在大规模数据集上的有效性。它由Mini-Kinetics
    [[117](#bib.bib117)] 和UCF101数据集构建。同时，HMDB-ARID[partial]的构建灵感来源于HMDB-ARID [[23](#bib.bib23)]
    数据集，旨在提高视频模型在低光照条件下的性能，利用部分集VUDA下较大的领域偏移。'
- en: Multi-Domain VUDA Datasets. There are also several more recent datasets that
    are more comprehensive that include multiple domains within the cross-domain dataset
    such that there are more than 2 possible source/target video domain pairs. For
    instance, the Epic-Kitchens [[29](#bib.bib29)] cross-domain dataset contains 3
    domains from videos of three different kitchens in the original Epic-Kitchens
    action recognition dataset [[118](#bib.bib118)] and therefore includes 6 different
    combinations of source/target video domain pairs. Note that we follow the literatures
    in [[29](#bib.bib29), [40](#bib.bib40), [33](#bib.bib33), [35](#bib.bib35)] and
    still refer the Epic-Kitchens cross-domain dataset as “Epic-Kitchens”. Epic-Kitchens
    is generally built from a single large-scale action recognition dataset and contains
    videos collected from a controlled environment. Subsequently, other multi-domain
    VUDA datasets are introduced that contain videos from a wider range of different
    scenes collected from various public datasets. Inspired by the success of DomainNet [[119](#bib.bib119)]
    as a unified and comprehensive benchmark for evaluating image-based domain adaptation
    under both closed-set and multi-domain scenarios, the Sports-DA and Daily-DA cross-domain
    action recognition datasets [[75](#bib.bib75), [76](#bib.bib76)] are introduced.
    The Daily-DA dataset contains 4 different domains constructed from HMDB51 [[115](#bib.bib115)],
    ARID [[105](#bib.bib105)], Moment in Time [[120](#bib.bib120)], and Kinetics-600 [[86](#bib.bib86)],
    resulting in 12 different combinations of source/target video domain pairs, which
    is the largest number of source/target domain pairs to date. The Sports-DA dataset
    contains 3 different domains with sports videos from UCF101 [[90](#bib.bib90)],
    Kinetics-600 [[86](#bib.bib86)], and Sports-1M [[91](#bib.bib91)], resulting in
    6 different combinations of source/target video domain pairs, and contains more
    action classes than that in both Epic-Kitchens and Daily-DA.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 多领域VUDA数据集。还有几个更近期的数据集更加全面，它们包含跨领域数据集中的多个领域，因此存在超过2对可能的源/目标视频领域组合。例如，Epic-Kitchens [[29](#bib.bib29)]跨领域数据集包含来自原始Epic-Kitchens动作识别数据集 [[118](#bib.bib118)]中的三个不同厨房的视频，因而包括了6种不同的源/目标视频领域组合。请注意，我们遵循文献 [[29](#bib.bib29),
    [40](#bib.bib40), [33](#bib.bib33), [35](#bib.bib35)]，仍然将Epic-Kitchens跨领域数据集称为“Epic-Kitchens”。Epic-Kitchens通常来源于一个大型动作识别数据集，并包含从受控环境中收集的视频。随后，介绍了其他多领域VUDA数据集，这些数据集包含来自各种公共数据集的更广泛场景的视频。受到DomainNet [[119](#bib.bib119)]作为统一且全面的图像领域适配评估基准的成功启发，引入了Sports-DA和Daily-DA跨领域动作识别数据集 [[75](#bib.bib75),
    [76](#bib.bib76)]。Daily-DA数据集包含来自HMDB51 [[115](#bib.bib115)]、ARID [[105](#bib.bib105)]、Moment
    in Time [[120](#bib.bib120)]和Kinetics-600 [[86](#bib.bib86)]的4个不同领域，结果是12种不同的源/目标视频领域组合，这是迄今为止源/目标领域组合的最大数量。Sports-DA数据集包含来自UCF101 [[90](#bib.bib90)]、Kinetics-600 [[86](#bib.bib86)]和Sports-1M [[91](#bib.bib91)]的3个不同领域的体育视频，结果是6种不同的源/目标视频领域组合，并且包含的动作类别比Epic-Kitchens和Daily-DA都多。
- en: 'VUDA Dataset for Cross-Domain Video Semantic Segmentation. While all aforementioned
    datasets are meant for the cross-domain action recognition task, research on VUDA
    is not limited to such a task as mentioned in Section [IV-D](#S4.SS4 "IV-D Methods
    with Differed Cross-Domain Tasks ‣ IV Methods for Video Unsupervised Domain Adaptation
    under Different Constraints, Assumptions, and Tasks ‣ Video Unsupervised Domain
    Adaptation with Deep Learning: A Comprehensive Survey"). With increasing interest
    in other cross-domain video tasks, there have been some relevant datasets proposed.
    This is especially for cross-domain video semantic segmentation, where two cross-domain
    datasets are proposed: the VIPER$\to$Cityscapes-Seq and the SYNTHIA-Seq$\to$Cityscapes-Seq [[82](#bib.bib82)]
    datasets, built from current semantic segmentation datasets. The prior is built
    from Cityscapes-Seq [[121](#bib.bib121)] and VIPER [[122](#bib.bib122)], while
    the later from SYNTHIA-Seq [[123](#bib.bib123)] and Cityscapes-Seq. Both VIPER
    and SYNTHIA-Seq are synthetic videos generated from games or the Unity Development
    platform [[124](#bib.bib124)] while Cityscapes-Seq is built with videos captured
    in real-world scenes.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 'VUDA 数据集用于跨领域视频语义分割。虽然所有上述数据集都用于跨领域动作识别任务，但 VUDA 的研究不仅限于此任务，如第 [IV-D](#S4.SS4
    "IV-D 不同跨领域任务的方法 ‣ IV 不同约束、假设和任务下的视频无监督领域适应方法 ‣ 深度学习的视频无监督领域适应: 综合调查") 节所述。随着对其他跨领域视频任务的兴趣增加，提出了一些相关数据集。这特别针对跨领域视频语义分割，提出了两个跨领域数据集：VIPER$\to$Cityscapes-Seq
    和 SYNTHIA-Seq$\to$Cityscapes-Seq [[82](#bib.bib82)]，这些数据集建立在当前的语义分割数据集上。前者建立于
    Cityscapes-Seq [[121](#bib.bib121)] 和 VIPER [[122](#bib.bib122)]，后者建立于 SYNTHIA-Seq
    [[123](#bib.bib123)] 和 Cityscapes-Seq。VIPER 和 SYNTHIA-Seq 都是从游戏或 Unity 开发平台 [[124](#bib.bib124)]
    生成的合成视频，而 Cityscapes-Seq 是通过捕捉现实世界场景中的视频构建的。'
- en: 'TABLE V: Average accuracy ($\%$) on VUDA datasets with larger domain shifts.
    Methods are arranged in chronological order.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 在具有更大领域转移的 VUDA 数据集上的平均准确率 ($\%$)。方法按时间顺序排列。'
- en: '| Methods | Backbones | Categories | Kinetics-Gameplay | HMDB-ARID | Kinetics$\to$NEC-Drone
    | Mixamo$\to$Kinetics | ActorShift |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 骨干网络 | 类别 | Kinetics-Gameplay | HMDB-ARID | Kinetics$\to$NEC-Drone |
    Mixamo$\to$Kinetics | ActorShift |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| TA³N [[27](#bib.bib27)] | TRN [[55](#bib.bib55)] | Adversarial | 27.50 |
    21.10 | 28.10 | 10.00 | - |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| TA³N [[27](#bib.bib27)] | TRN [[55](#bib.bib55)] | 对抗性 | 27.50 | 21.10 |
    28.10 | 10.00 | - |'
- en: '| NEC-Drone [[106](#bib.bib106)] | I3D [[108](#bib.bib108)] | Composite | -
    | - | 15.10 | - | - |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| NEC-Drone [[106](#bib.bib106)] | I3D [[108](#bib.bib108)] | 组合型 | - | - |
    15.10 | - | - |'
- en: '| SAVA [[40](#bib.bib40)] | I3D [[108](#bib.bib108)] | Composite | - | - |
    31.60 | - | - |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| SAVA [[40](#bib.bib40)] | I3D [[108](#bib.bib108)] | 组合型 | - | - | 31.60
    | - | - |'
- en: '| MM-SADA [[29](#bib.bib29)] | SlowFast [[125](#bib.bib125)] | Adversarial
    | - | - | - | - | 62.60 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| MM-SADA [[29](#bib.bib29)] | SlowFast [[125](#bib.bib125)] | 对抗性 | - | -
    | - | - | 62.60 |'
- en: '| ACAN [[23](#bib.bib23)] | MFNet [[111](#bib.bib111)] | Composite | - | 52.20
    | - | - | - |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ACAN [[23](#bib.bib23)] | MFNet [[111](#bib.bib111)] | 组合型 | - | 52.20 |
    - | - | - |'
- en: '| CO²A [[36](#bib.bib36)] | I3D [[108](#bib.bib108)] | Semantic | - | - | 33.20
    | 16.40 | - |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CO²A [[36](#bib.bib36)] | I3D [[108](#bib.bib108)] | 语义性 | - | - | 33.20
    | 16.40 | - |'
- en: '| MA²l-TD [[30](#bib.bib30)] | ResNet-101 [[110](#bib.bib110)] | Adversarial
    | 31.45 | - | - | - | - |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| MA²l-TD [[30](#bib.bib30)] | ResNet-101 [[110](#bib.bib110)] | 对抗性 | 31.45
    | - | - | - | - |'
- en: '| A³R [[37](#bib.bib37)] | SlowFast [[125](#bib.bib125)] | Semantic | - | -
    | - | - | 67.30 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| A³R [[37](#bib.bib37)] | SlowFast [[125](#bib.bib125)] | 语义性 | - | - | -
    | - | 67.30 |'
- en: 'TABLE VI: Average accuracy ($\%$) on partial-set VUDA (PVDA) datasets. Methods
    are arranged in chronological order.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 部分集 VUDA (PVDA) 数据集的平均准确率 ($\%$)。方法按时间顺序排列。'
- en: '| Methods | Backbones | Categories | UCF-HMDB[partial] | MiniKinetics-UCF |
    HMDB-ARID[partial] |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 骨干网络 | 类别 | UCF-HMDB[部分] | MiniKinetics-UCF | HMDB-ARID[部分] |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| TA³N [[27](#bib.bib27)] | TRN [[55](#bib.bib55)] | Adversarial | 60.59 |
    61.97 | 21.25 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| TA³N [[27](#bib.bib27)] | TRN [[55](#bib.bib55)] | 对抗性 | 60.59 | 61.97 |
    21.25 |'
- en: '| SAVA [[40](#bib.bib40)] | TRN [[55](#bib.bib55)] | Composite | 65.93 | 66.58
    | 23.72 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| SAVA [[40](#bib.bib40)] | TRN [[55](#bib.bib55)] | 组合型 | 65.93 | 66.58 |
    23.72 |'
- en: '| PATAN (PVDA) [[24](#bib.bib24)] | TRN [[55](#bib.bib55)] | - | 81.83 | 76.04
    | 30.54 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| PATAN (PVDA) [[24](#bib.bib24)] | TRN [[55](#bib.bib55)] | - | 81.83 | 76.04
    | 30.54 |'
- en: '| MCAN (PVDA) [[73](#bib.bib73)] | TSN [[126](#bib.bib126)] | - | 83.94 | 81.34
    | 44.37 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| MCAN (PVDA) [[73](#bib.bib73)] | TSN [[126](#bib.bib126)] | - | 83.94 | 81.34
    | 44.37 |'
- en: 'TABLE VII: Average accuracy ($\%$) on multi-domain VUDA datasets. Methods are
    arranged in chronological order.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '表VII: 多领域VUDA数据集上的平均准确率（$\%$）。方法按时间顺序排列。'
- en: '| Methods | Backbones | Categories | Epic-Kitchens | Daily-DA | Sports-DA |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | 类别 | Epic-Kitchens | Daily-DA | Sports-DA |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| TA³N [[27](#bib.bib27)] | I3D [[108](#bib.bib108)]/TRN [[55](#bib.bib55)]
    | Adversarial | 43.20 | 28.49 | 70.26 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| TA³N [[27](#bib.bib27)] | I3D [[108](#bib.bib108)]/TRN [[55](#bib.bib55)]
    | 对抗性 | 43.20 | 28.49 | 70.26 |'
- en: '| MM-SADA [[29](#bib.bib29)] | I3D [[108](#bib.bib108)] | Adversarial | 50.30
    | - | - |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| MM-SADA [[29](#bib.bib29)] | I3D [[108](#bib.bib108)] | 对抗性 | 50.30 | - |
    - |'
- en: '| STCDA [[33](#bib.bib33)] | I3D [[108](#bib.bib108)] | Semantic | 51.20 |
    - | - |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| STCDA [[33](#bib.bib33)] | I3D [[108](#bib.bib108)] | 语义 | 51.20 | - | -
    |'
- en: '| CMCo [[34](#bib.bib34)] | I3D [[108](#bib.bib108)] | Semantic | 51.00 | -
    | - |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| CMCo [[34](#bib.bib34)] | I3D [[108](#bib.bib108)] | 语义 | 51.00 | - | - |'
- en: '| CoMix [[35](#bib.bib35)] | I3D [[108](#bib.bib108)] | Semantic | 43.20 |
    - | - |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| CoMix [[35](#bib.bib35)] | I3D [[108](#bib.bib108)] | 语义 | 43.20 | - | -
    |'
- en: '| TAMAN (MSVDA) [[75](#bib.bib75)] | TRN [[55](#bib.bib55)] | - | - | 44.85
    | 77.84 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| TAMAN (MSVDA) [[75](#bib.bib75)] | TRN [[55](#bib.bib55)] | - | - | 44.85
    | 77.84 |'
- en: '| CIA [[31](#bib.bib31)] | I3D [[108](#bib.bib108)] | Adversarial | 52.20 |
    - | - |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| CIA [[31](#bib.bib31)] | I3D [[108](#bib.bib108)] | 对抗性 | 52.20 | - | - |'
- en: '| A³R [[37](#bib.bib37)] | SlowFast [[125](#bib.bib125)] | Semantic | 61.00
    | - | - |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| A³R [[37](#bib.bib37)] | SlowFast [[125](#bib.bib125)] | 语义 | 61.00 | - |
    - |'
- en: '| TranSVAE [[43](#bib.bib43)] | I3D [[108](#bib.bib108)] | Reconstruction |
    52.60 | - | - |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| TranSVAE [[43](#bib.bib43)] | I3D [[108](#bib.bib108)] | 重建 | 52.60 | - |
    - |'
- en: '| ATCoN (SFVDA) [[76](#bib.bib76)] | TRN [[55](#bib.bib55)] | - | - | 33.53
    | 73.85 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ATCoN (SFVDA) [[76](#bib.bib76)] | TRN [[55](#bib.bib55)] | - | - | 33.53
    | 73.85 |'
- en: '| EXTERN (BVDA) [[77](#bib.bib77)] | TRN [[55](#bib.bib55)] | - | - | 39.64
    | 83.18 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| EXTERN (BVDA) [[77](#bib.bib77)] | TRN [[55](#bib.bib55)] | - | - | 39.64
    | 83.18 |'
- en: 'TABLE VIII: Average IOU on VUDA datasets for cross-domain video semantic segmentation.
    Methods are arranged in chronological order.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '表VIII: 跨领域视频语义分割的VUDA数据集上的平均IOU。方法按时间顺序排列。'
- en: '| Methods | Backbones | VIPER$\to$Cityscapes-Seq | SYNTHIA-Seq$\to$Cityscapes-Seq
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | VIPER$\to$Cityscapes-Seq | SYNTHIA-Seq$\to$Cityscapes-Seq |'
- en: '| --- | --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| DA-VSN [[82](#bib.bib82)] | ACCEL [[127](#bib.bib127)] | 47.80 | 49.50 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| DA-VSN [[82](#bib.bib82)] | ACCEL [[127](#bib.bib127)] | 47.80 | 49.50 |'
- en: '| TPS [[83](#bib.bib83)] | ACCEL [[127](#bib.bib127)] | 48.90 | 53.80 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| TPS [[83](#bib.bib83)] | ACCEL [[127](#bib.bib127)] | 48.90 | 53.80 |'
- en: 'VI Discussion: Recent Progress and Future Directions'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 讨论：近期进展与未来方向
- en: In this section, we summarize the recent progress in VUDA research with observations.
    We further analyze and provide our insights on possible future directions of development
    for VUDA research.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们总结了VUDA研究的最新进展，并进行了观察。我们进一步分析并提供了对VUDA研究未来发展方向的见解。
- en: VI-A Recent Progress in VUDA Research
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A VUDA研究的最新进展
- en: 'Compared to earlier works, recent VUDA research has made significant progress
    from three perspectives: a) tackling VUDA under different scenarios; b) leveraging
    the multi-modality nature of videos; and c) exploiting shared semantics across
    domains with semantic-based methods.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期工作相比，近期的VUDA研究从三个方面取得了显著进展：a) 处理不同场景下的VUDA；b) 利用视频的多模态特性；c) 采用基于语义的方法来挖掘跨领域的共享语义。
- en: 'a) Tackling VUDA Under Different Scenarios. The closed-set scenario has been
    the focus of VUDA research thanks to its simplicity that results from the assumptions
    of a single pair of the labeled video source and unlabeled video target domains
    with the source videos and source models accessible, and the constraints of a
    shared label space across the source/target domain pair. However, as mentioned
    in Section [IV](#S4 "IV Methods for Video Unsupervised Domain Adaptation under
    Different Constraints, Assumptions, and Tasks ‣ Video Unsupervised Domain Adaptation
    with Deep Learning: A Comprehensive Survey") and in [[24](#bib.bib24), [75](#bib.bib75),
    [73](#bib.bib73), [76](#bib.bib76)], closed-set VUDA may not be applicable in
    real-world scenarios. To cope with model portability and other (e.g., data privacy)
    issues caused by the constraints and assumptions of closed-set VUDA, several other
    scenarios of VUDA have been recently studied. These include the partial-set PVDA [[24](#bib.bib24),
    [73](#bib.bib73)], the open-set OSVDA [[74](#bib.bib74)], the multi-domain MSVDA [[75](#bib.bib75)],
    the SFVDA [[76](#bib.bib76)] and BVDA [[77](#bib.bib77)] with source-free/black-box
    source model settings, as well as the VDG [[78](#bib.bib78)] with target-free
    settings. The introduction of relevant datasets further promotes the research
    on various non-closed-set VUDA scenarios and further improves the capability of
    VUDA methods in real-world scenarios.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) 在不同场景下处理 VUDA。由于其简洁性，闭集场景一直是 VUDA 研究的重点，这种简洁性来源于假设只有一个标记的视频源和未标记的视频目标域，源视频和源模型是可访问的，并且源/目标域对有共享标签空间的约束。然而，如第
    [IV](#S4 "IV Methods for Video Unsupervised Domain Adaptation under Different
    Constraints, Assumptions, and Tasks ‣ Video Unsupervised Domain Adaptation with
    Deep Learning: A Comprehensive Survey") 节和 [[24](#bib.bib24), [75](#bib.bib75),
    [73](#bib.bib73), [76](#bib.bib76)] 所述，闭集 VUDA 可能不适用于实际场景。为了应对闭集 VUDA 的约束和假设带来的模型可移植性和其他（例如数据隐私）问题，最近研究了几种其他
    VUDA 场景。这些包括部分集 PVDA [[24](#bib.bib24), [73](#bib.bib73)]、开放集 OSVDA [[74](#bib.bib74)]、多域
    MSVDA [[75](#bib.bib75)]、源自由 SFVDA [[76](#bib.bib76)] 和黑箱 BVDA [[77](#bib.bib77)]
    设置，以及目标自由 VDG [[78](#bib.bib78)] 设置。相关数据集的引入进一步促进了对各种非闭集 VUDA 场景的研究，并进一步提高了 VUDA
    方法在实际场景中的能力。'
- en: b) Leveraging the Multi-Modality Nature of Videos. Tackling VUDA is more challenging
    than tackling image-based UDA largely thanks to the inclusion of the additional
    temporal features and features of other modalities (e.g., optical flow and audio)
    in videos. These additional features would all incur extra domain shifts across
    source and target domains. Earlier methods such as the adversarial-based TA³N [[27](#bib.bib27)],
    the discrepancy-based AMLS [[26](#bib.bib26)] or the composite VUDA method PASTN [[32](#bib.bib32)]
    focus primarily on tackling domain shift caused by the additional temporal features.
    Subsequently, more recent methods such as MM-SADA [[29](#bib.bib29)], CIA [[31](#bib.bib31)]
    and A³R [[37](#bib.bib37)] have realized the importance of tackling domain shift
    caused by the features of different modalities, with tackling domain shifts from
    optical flow and audio features being the focus. Later methods have achieved notable
    improvements over the same benchmark against prior methods without multi-modal
    feature alignment, which proves the efficacy of aligning multi-modal features
    toward achieving effective VUDA. However, it should be noted that audio features
    may not be readily available in benchmark datasets or in real-world scenarios
    (e.g., surveillance footage or autonomous driving footage where the audio captured
    is mostly ambient noise). Therefore there are certain limitations in applying
    VUDA methods that exploit audio features.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: b) 利用视频的多模态特性。处理 VUDA 比处理基于图像的 UDA 更具挑战性，这主要得益于视频中包含的额外时间特征和其他模态特征（例如光流和音频）。这些额外特征都会在源域和目标域之间引起额外的领域转移。早期的方法如对抗性
    TA³N [[27](#bib.bib27)]、差异性 AMLS [[26](#bib.bib26)] 或复合 VUDA 方法 PASTN [[32](#bib.bib32)]
    主要集中在处理由额外时间特征引起的领域转移。随后，较新的方法如 MM-SADA [[29](#bib.bib29)]、CIA [[31](#bib.bib31)]
    和 A³R [[37](#bib.bib37)] 意识到处理由不同模态特征引起的领域转移的重要性，尤其是光流和音频特征。后续的方法在同一基准测试中取得了显著的改进，超越了没有多模态特征对齐的先前方法，这证明了对齐多模态特征以实现有效
    VUDA 的有效性。然而，需要注意的是，音频特征在基准数据集或实际场景中可能不易获得（例如监控录像或自动驾驶录像，其中录制的音频大多是环境噪音）。因此，利用音频特征的
    VUDA 方法在应用上存在一定的限制。
- en: c) Exploiting Shared Semantics with Semantic-based VUDA Methods. Compared to
    adversarial-based and discrepancy-based VUDA methods, semantic-based VUDA methods
    have not been considered until more recently. This owes to the fact that aligning
    video domains by exploiting shared semantics is not as intuitive as aligning video
    domains by minimizing video domain discrepancies whether explicitly or implicitly.
    However, the performances of different semantic-based VUDA methods (e.g., CMCo [[34](#bib.bib34)]
    and CoMix [[35](#bib.bib35)]) proves that exploiting shared semantics with spatio-temporal
    association, feature clustering and modality correspondence is beneficial towards
    obtaining domain-invariant video features. Comparatively, semantic-based methods
    are more stable in terms of optimization compared to adversarial-based methods,
    while obtaining superior performance than discrepancy-based methods. Furthermore,
    semantic-based VUDA methods can be combined with both adversarial and discrepancy-based
    VUDA methods to form composite methods such as SAVA [[40](#bib.bib40)] and PASTN [[32](#bib.bib32)].
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: c) 利用共享语义的基于语义的VUDA方法。与基于对抗和差异的方法相比，基于语义的VUDA方法直到最近才被考虑。这是因为通过利用共享语义来对齐视频域并不像通过最小化视频域差异（无论是显式还是隐式）那样直观。然而，不同基于语义的VUDA方法（例如，CMCo [[34](#bib.bib34)]和CoMix [[35](#bib.bib35)]）的表现证明，通过时空关联、特征聚类和模态对应来利用共享语义对获得域不变的视频特征是有益的。相比之下，基于语义的方法在优化方面比基于对抗的方法更稳定，同时性能优于基于差异的方法。此外，基于语义的VUDA方法可以与基于对抗和差异的方法结合，形成复合方法，例如SAVA [[40](#bib.bib40)]和PASTN [[32](#bib.bib32)]。
- en: VI-B Challenges of Current VUDA Research and its Future Directions
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 当前VUDA研究的挑战及其未来方向
- en: 'Despite the notable progress made in VUDA research, there are still various
    challenges hampering the effectiveness of existing VUDA research. The challenges
    could generally be categorized into three categories: b) challenges in the multi-modal
    information leveraged; a) challenges in explored VUDA scenarios; and c) challenges
    in more effective VUDA methods with self-supervision. Dealing with these challenges
    would greatly benefit future VUDA methods and are considered as potential future
    directions for VUDA research.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管VUDA研究取得了显著进展，但现有的VUDA研究仍面临各种挑战。这些挑战通常可以分为三类：b) 多模态信息利用方面的挑战；a) 探索的VUDA场景方面的挑战；以及c)
    自监督的更有效VUDA方法方面的挑战。解决这些挑战将极大地有利于未来的VUDA方法，并被视为VUDA研究的潜在未来方向。
- en: 'a) Challenges in Explored VUDA Scenarios. There have been various non-closed-set
    VUDA scenarios researched as mentioned in Section [IV](#S4 "IV Methods for Video
    Unsupervised Domain Adaptation under Different Constraints, Assumptions, and Tasks
    ‣ Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey").
    However, compared to domain adaptation in NLP and image tasks which has been researched
    more comprehensively [[128](#bib.bib128), [129](#bib.bib129), [130](#bib.bib130)],
    we observe that there are still a number of scenarios that have not been touched
    upon for VUDA. For instance, while a method has been proposed for multi-source
    VUDA (MSVDA) where the constraint of $M_{S}=1$ video source domain is relaxed,
    the multi-target VUDA (MTVDA) where the constraint of $M_{T}=1$ video target domain
    is relaxed has not been touched upon in research. Combining active learning [[131](#bib.bib131)]
    and VUDA that formulates active VUDA which aims to adapt the source video model
    to the target video domain by acquiring labels for a selected maximally-informative
    subset of target videos via an oracle is another scenario that has not been researched
    in VUDA. To further protect source data privacy, black-box VUDA is another feasible
    VUDA scenario where besides source video data, the source video model is also
    made inaccessible to the target video domain, which prevents source videos to
    be recovered by generative models [[44](#bib.bib44)]. Meanwhile, the data privacy
    concern is also applicable to the current MSVDA scenario where video data are
    accessible between different source domains. Combining federated learning [[132](#bib.bib132)]
    with VUDA is a possible solution such that video data are not shareable between
    different video domains. The aforementioned VUDA scenarios are more realistic
    and future research on these scenarios could further boost the capability of VUDA
    methods in applying to real-world applications.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) 探索性 VUDA 场景中的挑战。正如第 [IV](#S4 "IV Methods for Video Unsupervised Domain Adaptation
    under Different Constraints, Assumptions, and Tasks ‣ Video Unsupervised Domain
    Adaptation with Deep Learning: A Comprehensive Survey") 节中提到的，已经研究了多种非封闭集的 VUDA
    场景。然而，与在 NLP 和图像任务中更为全面的领域适应研究相比 [[128](#bib.bib128), [129](#bib.bib129), [130](#bib.bib130)]，我们观察到
    VUDA 仍有许多场景未被触及。例如，虽然已有方法提出了多源 VUDA (MSVDA) 以放宽 $M_{S}=1$ 视频源领域的约束，但放宽 $M_{T}=1$
    视频目标领域约束的多目标 VUDA (MTVDA) 尚未被研究。结合主动学习 [[131](#bib.bib131)] 和 VUDA 的主动 VUDA 场景也尚未在
    VUDA 中研究，该场景旨在通过通过一个 oracle 为选择的最大信息子集的目标视频获取标签，从而将源视频模型适应到目标视频领域。为了进一步保护源数据隐私，黑箱
    VUDA 是另一种可行的 VUDA 场景，其中除了源视频数据外，源视频模型也对目标视频领域不可访问，这防止了生成模型恢复源视频 [[44](#bib.bib44)]。同时，数据隐私问题也适用于当前的
    MSVDA 场景，其中视频数据在不同源领域之间是可访问的。将联邦学习 [[132](#bib.bib132)] 与 VUDA 结合是一个可能的解决方案，使得视频数据在不同视频领域之间不可共享。上述
    VUDA 场景更具现实意义，未来在这些场景中的研究可以进一步提升 VUDA 方法在实际应用中的能力。'
- en: b) Challenges in Multi-Modal Information Leveraged. The multi-modal information
    leveraged for existing VUDA methods is largely limited to RGB, optical flow, and
    audio information. However, videos also contain more modalities of information,
    which have already been exploited for supervised video tasks but not exploited
    for VUDA. A typical example involves the human skeleton data [[133](#bib.bib133),
    [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136)] which is a compact
    and effective action descriptor that focuses on the temporal change of the pose
    of the actor and is known for its immune to contextual variation, such as background
    and illumination variation. Meanwhile, the input of existing VUDA methods rely
    only on RGB cameras, while videos could also be obtained by other sensors including
    depth cameras [[137](#bib.bib137), [138](#bib.bib138)], infrared cameras [[139](#bib.bib139),
    [140](#bib.bib140)], or even lidars [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143)].
    Future VUDA methods should also take videos taken from these sensors into consideration.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: b) 在多模态信息利用中的挑战。现有的VUDA方法中，多模态信息的利用主要限于RGB、光流和音频信息。然而，视频还包含更多的模态信息，这些信息已被用于监督视频任务，但未被用于VUDA。一个典型的例子是人体骨架数据[[133](#bib.bib133),
    [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136)]，它是一种紧凑而有效的动作描述符，关注演员姿势的时间变化，并且对背景和光照变化等上下文变化具有免疫力。同时，现有VUDA方法的输入仅依赖于RGB相机，而视频还可以通过其他传感器获取，包括深度相机[[137](#bib.bib137),
    [138](#bib.bib138)]、红外相机[[139](#bib.bib139), [140](#bib.bib140)]，甚至激光雷达[[141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143)]。未来的VUDA方法也应考虑这些传感器拍摄的视频。
- en: Besides the challenges faced in leveraging the different categories of modalities,
    the performance of current VUDA methods may also be hampered by the effectiveness
    of features extracted from the currently leveraged modalities. Existing works
    still tend to leverage ResNet-based [[110](#bib.bib110)] CNN networks to obtain
    features from RGB, optical flow, and audio. More recent research [[144](#bib.bib144),
    [145](#bib.bib145), [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148),
    [149](#bib.bib149)] have shown the efficacy of both Transformers [[94](#bib.bib94)]
    and Graph Neural Networks (GNN) [[150](#bib.bib150), [151](#bib.bib151)] in obtaining
    effective features for downstream video tasks. It is intuitive that aligning video
    features effectively should be built on the assumption that the video features
    to align with are effective themselves. Therefore, it is expected that future
    VUDA methods would further improve VUDA performance by utilizing Transformers
    and GNN for feature extraction.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用不同类别模态面临的挑战外，当前VUDA方法的性能还可能受到当前利用模态中提取的特征效果的影响。现有工作仍倾向于利用基于ResNet的[[110](#bib.bib110)]
    CNN网络从RGB、光流和音频中提取特征。更近期的研究[[144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149)]显示了Transformers[[94](#bib.bib94)]和图神经网络（GNN）[[150](#bib.bib150),
    [151](#bib.bib151)]在获取下游视频任务有效特征方面的有效性。显然，对视频特征的有效对齐应建立在这些视频特征本身有效的假设之上。因此，预计未来的VUDA方法将通过利用Transformers和GNN进行特征提取，进一步提高VUDA性能。
- en: c) Challenges in VUDA Methods with Self-Supervision. In recent years, there
    has been a significant increase in VUDA methods that leverage semantic-based or
    reconstruction-based approaches in full or in part, thanks to their high performance
    and extensibility towards combining with other approaches and towards different,
    more practical VUDA scenarios. Since target labels are unavailable for adaptation,
    both semantic-based and reconstruction-based approaches rely on self-supervision
    for obtaining shared cross-domain semantics or achieving data reconstruction.
    Among the various self-supervision techniques, contrastive learning has been widely
    utilized given the ease of formulation and their high performance. More recently,
    contrastive learning in visual tasks has been achieved not only by applying across
    visual features but also across visual features and their corresponding text labels
    or text descriptions [[152](#bib.bib152)]. The strategy of learning visual concepts
    from natural language supervision in a contrastive manner results in more a generalizable
    network that could easily adapt to new visual tasks with only natural language
    cues from the pre-trained task. This is in line with the goal of VUDA which attempts
    to adapt networks to new video domains given information from the source domain,
    which includes text-based information such as video labels. Therefore, the exploration
    of self-supervised VUDA methods leveraging on natural language cues within the
    labeled source domain (e.g., source video labels) could be an interesting yet
    effective way towards further improvement of VUDA performances.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: c) 自监督下的 VUDA 方法中的挑战。近年来，由于 VUDA 方法在完全或部分依赖基于语义或重建的方法的表现优异，且具有较高的扩展性（能够与其他方法结合并适应不同、更实际的
    VUDA 场景），因此这类方法显著增加。由于目标标签不可用于适应，语义基础和重建基础的方法均依赖自监督以获取共享的跨域语义或实现数据重建。在各种自监督技术中，对比学习因其易于制定和高性能而被广泛应用。最近，在视觉任务中的对比学习不仅通过应用于视觉特征之间，还通过视觉特征及其对应的文本标签或文本描述[[152](#bib.bib152)]。以对比方式从自然语言监督中学习视觉概念的策略使得网络更具泛化能力，能够仅通过预训练任务中的自然语言提示轻松适应新的视觉任务。这与
    VUDA 的目标相一致，VUDA 旨在根据源域信息（包括基于文本的信息，如视频标签）将网络适应新的视频领域。因此，探索利用标注源域（例如源视频标签）中的自然语言提示的自监督
    VUDA 方法可能是进一步提高 VUDA 性能的一个有趣而有效的途径。
- en: VII Conclusion
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: Video unsupervised domain adaptation (VUDA) plays a crucial role in improving
    video model portability and generalizability while avoiding costly data annotation
    by tackling the performance degradation problem under domain shift. This paper
    reviews the recent progress of VUDA with deep learning. We first investigate and
    summarize the methods for both closed-set VUDA, and non-closed-set VUDA scenarios
    with different constraints and assumptions of source and target domain. We observe
    that non-closed-set VUDA methods are more feasible in real-world applications.
    We further review available benchmark datasets for the various VUDA scenarios.
    We summarize the recent progress in VUDA research while providing insights into
    future VUDA research from the perspectives of leveraging multi-modal information,
    investigating reconstruction-based methods, and exploring other VUDA scenarios.
    We hope that these insights could help facilitate and promote future VUDA research,
    which enables robust and portable video models to be applied effectively and efficiently
    for real-world applications.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 视频无监督领域适应（VUDA）在提高视频模型的可移植性和泛化能力方面发挥了关键作用，同时通过解决领域迁移下的性能下降问题，避免了成本高昂的数据注释。本文回顾了基于深度学习的
    VUDA 的近期进展。我们首先调查并总结了针对不同约束和假设的闭集 VUDA 和非闭集 VUDA 场景的方法。我们观察到，非闭集 VUDA 方法在实际应用中更具可行性。我们进一步回顾了各种
    VUDA 场景下的基准数据集。我们总结了 VUDA 研究的近期进展，并从利用多模态信息、研究基于重建的方法和探索其他 VUDA 场景的角度提供了未来 VUDA
    研究的见解。我们希望这些见解能够促进和推动未来 VUDA 研究，使得鲁棒和可移植的视频模型能够在实际应用中有效和高效地应用。
- en: References
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. Zhang, T. Liu, M. Long, and M. Jordan, “Bridging theory and algorithm
    for domain adaptation,” in *International Conference on Machine Learning*.   PMLR,
    2019, pp. 7404–7413.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. Zhang, T. Liu, M. Long, 和 M. Jordan, “领域适应的理论与算法桥接，” 见于 *国际机器学习会议*。PMLR,
    2019, 页 7404–7413。'
- en: '[2] B. Sun, J. Feng, and K. Saenko, “Return of frustratingly easy domain adaptation,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 30, 2016.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] B. Sun, J. Feng 和 K. Saenko，“令人沮丧的简单领域适应的回归”，发表于*AAAI 人工智能会议论文集*，第30卷，2016年。'
- en: '[3] M. Long, Y. Cao, J. Wang, and M. Jordan, “Learning transferable features
    with deep adaptation networks,” in *International conference on machine learning*.   PMLR,
    2015, pp. 97–105.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Long, Y. Cao, J. Wang 和 M. Jordan，“通过深度适应网络学习可迁移特征”，发表于*国际机器学习会议*。PMLR，2015年，第97–105页。'
- en: '[4] K. Torkkola, “Feature extraction by non-parametric mutual information maximization,”
    *Journal of machine learning research*, vol. 3, no. Mar, pp. 1415–1438, 2003.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] K. Torkkola，“通过非参数互信息最大化进行特征提取”，*机器学习研究杂志*，第3卷，第3月，第1415–1438页，2003年。'
- en: '[5] R. Xu and D. Wunsch, “Survey of clustering algorithms,” *IEEE Transactions
    on neural networks*, vol. 16, no. 3, pp. 645–678, 2005.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] R. Xu 和 D. Wunsch，“聚类算法综述”，*IEEE 神经网络学报*，第16卷，第3期，第645–678页，2005年。'
- en: '[6] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot,
    C. Liu, and D. Krishnan, “Supervised contrastive learning,” *Advances in Neural
    Information Processing Systems*, vol. 33, pp. 18 661–18 673, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot,
    C. Liu 和 D. Krishnan，“监督对比学习”，*神经信息处理系统进展*，第33卷，第18 661–18 673页，2020年。'
- en: '[7] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for
    contrastive learning of visual representations,” in *International conference
    on machine learning*.   PMLR, 2020, pp. 1597–1607.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. Chen, S. Kornblith, M. Norouzi 和 G. Hinton，“视觉表征对比学习的简单框架”，发表于*国际机器学习会议*。PMLR，2020年，第1597–1607页。'
- en: '[8] D.-H. Lee *et al.*, “Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks,” in *Workshop on challenges in representation
    learning, ICML*, vol. 3, no. 2, 2013, p. 896.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] D.-H. Lee *等*，“伪标签：一种简单高效的半监督学习方法”，发表于*ICML 表示学习挑战研讨会*，第3卷，第2期，2013年，第896页。'
- en: '[9] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, and K. McGuinness, “Pseudo-labeling
    and confirmation bias in deep semi-supervised learning,” in *2020 International
    Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–8.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor 和 K. McGuinness，“伪标签和深度半监督学习中的确认偏差”，发表于*2020
    国际神经网络联合会议（IJCNN）*。IEEE，2020年，第1–8页。'
- en: '[10] O. Beijbom, “Domain adaptations for computer vision applications,” *arXiv
    preprint arXiv:1211.4860*, 2012.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] O. Beijbom，“计算机视觉应用的领域适应”，*arXiv 预印本 arXiv:1211.4860*，2012年。'
- en: '[11] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa, “Visual domain adaptation:
    A survey of recent advances,” *IEEE signal processing magazine*, vol. 32, no. 3,
    pp. 53–69, 2015.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] V. M. Patel, R. Gopalan, R. Li 和 R. Chellappa，“视觉领域适应：近期进展综述”，*IEEE 信号处理杂志*，第32卷，第3期，第53–69页，2015年。'
- en: '[12] G. Csurka, “Domain adaptation for visual applications: A comprehensive
    survey,” *arXiv preprint arXiv:1702.05374*, 2017.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] G. Csurka，“视觉应用的领域适应：全面综述”，*arXiv 预印本 arXiv:1702.05374*，2017年。'
- en: '[13] M. Wang and W. Deng, “Deep visual domain adaptation: A survey,” *Neurocomputing*,
    vol. 312, pp. 135–153, 2018.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Wang 和 W. Deng，“深度视觉领域适应：综述”，*神经计算*，第312卷，第135–153页，2018年。'
- en: '[14] W. M. Kouw and M. Loog, “A review of domain adaptation without target
    labels,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 43,
    no. 3, pp. 766–785, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] W. M. Kouw 和 M. Loog，“没有目标标签的领域适应综述”，*IEEE 模式分析与机器智能学报*，第43卷，第3期，第766–785页，2019年。'
- en: '[15] G. Wilson and D. J. Cook, “A survey of unsupervised deep domain adaptation,”
    *ACM Transactions on Intelligent Systems and Technology (TIST)*, vol. 11, no. 5,
    pp. 1–46, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] G. Wilson 和 D. J. Cook，“无监督深度领域适应综述”，*ACM 智能系统与技术交易*，第11卷，第5期，第1–46页，2020年。'
- en: '[16] L. Bungum and B. Gambäck, “A survey of domain adaptation in machine translation:
    Towards a refinement of domain space,” in *Proceedings of the India-Norway Workshop
    on Web Concepts and Technologies*, vol. 112, 2011.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] L. Bungum 和 B. Gambäck，“机器翻译中的领域适应调研：走向领域空间的精细化”，发表于*印度-挪威网络概念与技术研讨会论文集*，第112卷，2011年。'
- en: '[17] C. Chu and R. Wang, “A survey of domain adaptation for machine translation,”
    *Journal of information processing*, vol. 28, pp. 413–426, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] C. Chu 和 R. Wang，“机器翻译领域适应综述”，*信息处理学报*，第28卷，第413–426页，2020年。'
- en: '[18] A. Ramponi and B. Plank, “Neural unsupervised domain adaptation in nlp—a
    survey,” *arXiv preprint arXiv:2006.00632*, 2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Ramponi 和 B. Plank，“自然语言处理中的神经无监督领域适应——综述”，*arXiv 预印本 arXiv:2006.00632*，2020年。'
- en: '[19] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Transactions
    on knowledge and data engineering*, vol. 22, no. 10, pp. 1345–1359, 2009.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. J. Pan 和 Q. Yang, “迁移学习综述，” *IEEE知识与数据工程汇刊*，第22卷，第10期，页码1345–1359，2009年。'
- en: '[20] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,”
    *Journal of Big data*, vol. 3, no. 1, pp. 1–40, 2016.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Weiss, T. M. Khoshgoftaar, 和 D. Wang, “迁移学习综述，” *大数据期刊*，第3卷，第1期，页码1–40，2016年。'
- en: '[21] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on deep
    transfer learning,” in *International conference on artificial neural networks*.   Springer,
    2018, pp. 270–279.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, 和 C. Liu, “深度迁移学习综述，” 见 *国际人工神经网络会议*。
    Springer，2018年，页码270–279。'
- en: '[22] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He,
    “A comprehensive survey on transfer learning,” *Proceedings of the IEEE*, vol.
    109, no. 1, pp. 43–76, 2020.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, 和 Q. He, “迁移学习的全面综述，”
    *IEEE汇刊*，第109卷，第1期，页码43–76，2020年。'
- en: '[23] Y. Xu, J. Yang, H. Cao, K. Mao, J. Yin, and S. See, “Aligning correlation
    information for domain adaptation in action recognition,” 2021.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Xu, J. Yang, H. Cao, K. Mao, J. Yin, 和 S. See, “在动作识别中的领域适应对齐相关信息，”
    2021年。'
- en: '[24] Y. Xu, J. Yang, H. Cao, Z. Chen, Q. Li, and K. Mao, “Partial video domain
    adaptation with partial adversarial temporal attentive network,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 9332–9341.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Y. Xu, J. Yang, H. Cao, Z. Chen, Q. Li, 和 K. Mao, “带部分对抗时间注意网络的部分视频领域适应，”
    见 *IEEE/CVF国际计算机视觉会议论文集*，2021年，页码9332–9341。'
- en: '[25] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W.
    Vaughan, “A theory of learning from different domains,” *Machine learning*, vol. 79,
    no. 1, pp. 151–175, 2010.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, 和 J. W.
    Vaughan, “不同领域学习理论，” *机器学习*，第79卷，第1期，页码151–175，2010年。'
- en: '[26] A. Jamal, V. P. Namboodiri, D. Deodhare, and K. Venkatesh, “Deep domain
    adaptation in action space.” in *BMVC*, vol. 2, no. 3, 2018, p. 5.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Jamal, V. P. Namboodiri, D. Deodhare, 和 K. Venkatesh, “在动作空间中的深度领域适应。”
    见 *BMVC*，第2卷，第3期，2018年，页码5。'
- en: '[27] M.-H. Chen, Z. Kira, G. AlRegib, J. Yoo, R. Chen, and J. Zheng, “Temporal
    attentive alignment for large-scale video domain adaptation,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2019, pp. 6321–6330.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M.-H. Chen, Z. Kira, G. AlRegib, J. Yoo, R. Chen, 和 J. Zheng, “大规模视频领域适应的时间注意力对齐，”
    见 *IEEE国际计算机视觉会议论文集*，2019年，页码6321–6330。'
- en: '[28] B. Pan, Z. Cao, E. Adeli, and J. C. Niebles, “Adversarial cross-domain
    action recognition with co-attention.” in *AAAI*, 2020, pp. 11 815–11 822.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] B. Pan, Z. Cao, E. Adeli, 和 J. C. Niebles, “带共注意的对抗性跨领域动作识别。” 见 *AAAI*，2020年，页码11,815–11,822。'
- en: '[29] J. Munro and D. Damen, “Multi-modal domain adaptation for fine-grained
    action recognition,” in *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*, 2020, pp. 122–132.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Munro 和 D. Damen, “细粒度动作识别的多模态领域适应，” 见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页码122–132。'
- en: '[30] P. Chen, Y. Gao, and A. J. Ma, “Multi-level attentive adversarial learning
    with temporal dilation for unsupervised video domain adaptation,” in *Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision*, 2022, pp.
    1259–1268.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] P. Chen, Y. Gao, 和 A. J. Ma, “具有时间扩张的多级注意对抗学习用于无监督视频领域适应，” 见 *IEEE/CVF冬季计算机视觉应用会议论文集*，2022年，页码1259–1268。'
- en: '[31] L. Yang, Y. Huang, Y. Sugano, and Y. Sato, “Interact before align: Leveraging
    cross-modal knowledge for domain adaptive action recognition,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 14 722–14 732.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] L. Yang, Y. Huang, Y. Sugano, 和 Y. Sato, “对齐之前互动：利用跨模态知识进行领域自适应动作识别，”
    见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页码14,722–14,732。'
- en: '[32] Z. Gao, L. Guo, T. Ren, A.-A. Liu, Z.-Y. Cheng, and S. Chen, “Pairwise
    two-stream convnets for cross-domain action recognition with small data,” *IEEE
    Transactions on Neural Networks and Learning Systems*, 2020.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Z. Gao, L. Guo, T. Ren, A.-A. Liu, Z.-Y. Cheng, 和 S. Chen, “用于小数据跨领域动作识别的成对双流卷积网络，”
    *IEEE神经网络与学习系统汇刊*，2020年。'
- en: '[33] X. Song, S. Zhao, J. Yang, H. Yue, P. Xu, R. Hu, and H. Chai, “Spatio-temporal
    contrastive domain adaptation for action recognition,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 9787–9795.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] X. Song, S. Zhao, J. Yang, H. Yue, P. Xu, R. Hu, 和 H. Chai, “用于动作识别的时空对比领域适应，”
    见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，页码9787–9795。'
- en: '[34] D. Kim, Y.-H. Tsai, B. Zhuang, X. Yu, S. Sclaroff, K. Saenko, and M. Chandraker,
    “Learning cross-modal contrastive features for video domain adaptation,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 13 618–13 627.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D. Kim, Y.-H. Tsai, B. Zhuang, X. Yu, S. Sclaroff, K. Saenko, 和 M. Chandraker,
    “用于视频领域适应的跨模态对比特征学习，” 见于 *IEEE/CVF 国际计算机视觉会议论文集*，2021年，第13,618–13,627页。'
- en: '[35] A. Sahoo, R. Shah, R. Panda, K. Saenko, and A. Das, “Contrast and mix:
    Temporal contrastive video domain adaptation with background mixing,” *Advances
    in Neural Information Processing Systems*, vol. 34, pp. 23 386–23 400, 2021.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] A. Sahoo, R. Shah, R. Panda, K. Saenko, 和 A. Das, “对比与混合：背景混合的时序对比视频领域适应，”
    *神经信息处理系统进展*，第34卷，第23,386–23,400页，2021年。'
- en: '[36] V. G. T. da Costa, G. Zara, P. Rota, T. Oliveira-Santos, N. Sebe, V. Murino,
    and E. Ricci, “Dual-head contrastive domain adaptation for video action recognition,”
    in *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision*, 2022, pp. 1181–1190.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] V. G. T. da Costa, G. Zara, P. Rota, T. Oliveira-Santos, N. Sebe, V. Murino,
    和 E. Ricci, “用于视频动作识别的双头对比领域适应，” 见于 *IEEE/CVF 计算机视觉应用冬季会议论文集*，2022年，第1181–1190页。'
- en: '[37] Y. Zhang, H. Doughty, L. Shao, and C. G. Snoek, “Audio-adaptive activity
    recognition across video domains,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 13 791–13 800.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Zhang, H. Doughty, L. Shao, 和 C. G. Snoek, “跨视频领域的音频适应活动识别，” 见于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2022年，第13,791–13,800页。'
- en: '[38] H. Wu, C. Song, S. Yue, Z. Wang, J. Xiao, and Y. Liu, “Dynamic video mix-up
    for cross-domain action recognition,” *Neurocomputing*, vol. 471, pp. 358–368,
    2022.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] H. Wu, C. Song, S. Yue, Z. Wang, J. Xiao, 和 Y. Liu, “用于跨域动作识别的动态视频混合，”
    *神经计算*，第471卷，第358–368页，2022年。'
- en: '[39] J. Choi, G. Sharma, M. Chandraker, and J.-B. Huang, “Unsupervised and
    semi-supervised domain adaptation for action recognition from drones,” in *Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision*, 2020, pp.
    1717–1726.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Choi, G. Sharma, M. Chandraker, 和 J.-B. Huang, “用于无人机动作识别的无监督和半监督领域适应，”
    见于 *IEEE/CVF 计算机视觉应用冬季会议论文集*，2020年，第1717–1726页。'
- en: '[40] J. Choi, G. Sharma, S. Schulter, and J.-B. Huang, “Shuffle and attend:
    Video domain adaptation,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*.   Springer, 2020, pp. 678–695.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Choi, G. Sharma, S. Schulter, 和 J.-B. Huang, “洗牌与注意：视频领域适应，” 见于 *欧洲计算机视觉会议
    (ECCV) 论文集*。  施普林格，2020年，第678–695页。'
- en: '[41] Z. Gao, L. Guo, W. Guan, A.-A. Liu, T. Ren, and S. Chen, “A pairwise attentive
    adversarial spatiotemporal network for cross-domain few-shot action recognition-r2,”
    *IEEE Transactions on Image Processing*, vol. 30, pp. 767–782, 2020.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Z. Gao, L. Guo, W. Guan, A.-A. Liu, T. Ren, 和 S. Chen, “用于跨域少样本动作识别的成对注意对抗时空网络-r2，”
    *IEEE 图像处理汇刊*，第30卷，第767–782页，2020年。'
- en: '[42] Z. Gao, Y. Zhao, H. Zhang, D. Chen, A.-A. Liu, and S. Chen, “A novel multiple-view
    adversarial learning network for unsupervised domain adaptation action recognition,”
    *IEEE Transactions on Cybernetics*, 2021.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Gao, Y. Zhao, H. Zhang, D. Chen, A.-A. Liu, 和 S. Chen, “一种新型的多视角对抗学习网络用于无监督领域适应动作识别，”
    *IEEE 控制论汇刊*，2021年。'
- en: '[43] P. Wei, L. Kong, X. Qu, X. Yin, Z. Xu, J. Jiang, and Z. Ma, “Unsupervised
    video domain adaptation: A disentanglement perspective,” *arXiv preprint arXiv:2208.07365*,
    2022.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] P. Wei, L. Kong, X. Qu, X. Yin, Z. Xu, J. Jiang, 和 Z. Ma, “无监督视频领域适应：一个解缠结的视角，”
    *arXiv 预印本 arXiv:2208.07365*，2022年。'
- en: '[44] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *Advances in neural
    information processing systems*, vol. 27, 2014.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio, “生成对抗网络，” *神经信息处理系统进展*，第27卷，2014年。'
- en: '[45] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by backpropagation,”
    in *International conference on machine learning*.   PMLR, 2015, pp. 1180–1189.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Y. Ganin 和 V. Lempitsky, “通过反向传播进行无监督领域适应，” 见于 *国际机器学习会议*。  PMLR，2015年，第1180–1189页。'
- en: '[46] Q. Kang, S. Yao, M. Zhou, K. Zhang, and A. Abusorrah, “Effective visual
    domain adaptation via generative adversarial distribution matching,” *IEEE Transactions
    on Neural Networks and Learning Systems*, 2020.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Q. Kang, S. Yao, M. Zhou, K. Zhang, 和 A. Abusorrah, “通过生成对抗分布匹配进行有效的视觉领域适应，”
    *IEEE 神经网络与学习系统汇刊*，2020年。'
- en: '[47] J. Yang, H. Zou, Y. Zhou, Z. Zeng, and L. Xie, “Mind the discriminability:
    Asymmetric adversarial domain adaptation,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 589–606.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Yang, H. Zou, Y. Zhou, Z. Zeng 和 L. Xie, “注意判别能力：不对称对抗领域适配，” 收录于 *欧洲计算机视觉会议*。   Springer,
    2020年，页码 589–606。'
- en: '[48] J. Yang, H. Zou, Y. Zhou, and L. Xie, “Robust adversarial discriminative
    domain adaptation for real-world cross-domain visual recognition,” *Neurocomputing*,
    vol. 433, pp. 28–36, 2021.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Yang, H. Zou, Y. Zhou 和 L. Xie, “针对真实世界跨域视觉识别的鲁棒对抗判别领域适配，” *神经计算*,
    第433卷，页码 28–36, 2021年。'
- en: '[49] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool, “Domain adaptive
    faster r-cnn for object detection in the wild,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 3339–3348.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Chen, W. Li, C. Sakaridis, D. Dai 和 L. Van Gool, “领域自适应 Faster R-CNN
    用于野外目标检测，” 收录于 *IEEE 计算机视觉与模式识别会议论文集*，2018年，页码 3339–3348。'
- en: '[50] Q. Cai, Y. Pan, C.-W. Ngo, X. Tian, L. Duan, and T. Yao, “Exploring object
    relation in mean teacher for cross-domain detection,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2019, pp. 11 457–11 466.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Q. Cai, Y. Pan, C.-W. Ngo, X. Tian, L. Duan 和 T. Yao, “在 Mean Teacher
    中探索对象关系用于跨域检测，” 收录于 *IEEE 计算机视觉与模式识别会议论文集*，2019年，页码 11 457–11 466。'
- en: '[51] F. Yang, K. Yan, S. Lu, H. Jia, D. Xie, Z. Yu, X. Guo, F. Huang, and W. Gao,
    “Part-aware progressive unsupervised domain adaptation for person re-identification,”
    *IEEE Transactions on Multimedia*, 2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] F. Yang, K. Yan, S. Lu, H. Jia, D. Xie, Z. Yu, X. Guo, F. Huang 和 W. Gao,
    “面部感知渐进式无监督领域适配用于行人再识别，” *IEEE 多媒体学报*, 2020年。'
- en: '[52] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, and V. Lempitsky, “Domain-adversarial training of neural networks,”
    *The journal of machine learning research*, vol. 17, no. 1, pp. 2096–2030, 2016.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand 和 V. Lempitsky, “领域对抗神经网络训练，” *机器学习研究期刊*, 第17卷，第1期，页码 2096–2030, 2016年。'
- en: '[53] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern recognition*, vol. 77, pp. 354–377, 2018.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *等*, “卷积神经网络的最新进展，” *模式识别*, 第77卷，页码 354–377, 2018年。'
- en: '[54] S. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural networks for
    human action recognition,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 35, no. 1, pp. 221–231, 2012.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Ji, W. Xu, M. Yang 和 K. Yu, “用于人体动作识别的 3D 卷积神经网络，” *IEEE 模式分析与机器智能学报*,
    第35卷，第1期，页码 221–231, 2012年。'
- en: '[55] B. Zhou, A. Andonian, A. Oliva, and A. Torralba, “Temporal relational
    reasoning in videos,” in *Proceedings of the European Conference on Computer Vision
    (ECCV)*, 2018, pp. 803–818.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] B. Zhou, A. Andonian, A. Oliva 和 A. Torralba, “视频中的时间关系推理，” 收录于 *欧洲计算机视觉会议（ECCV）论文集*，2018年，页码
    803–818。'
- en: '[56] D. Saxena and J. Cao, “Generative adversarial networks (gans) challenges,
    solutions, and future directions,” *ACM Computing Surveys (CSUR)*, vol. 54, no. 3,
    pp. 1–42, 2021.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] D. Saxena 和 J. Cao, “生成对抗网络（GANs）的挑战、解决方案和未来方向，” *ACM 计算调查 (CSUR)*, 第54卷，第3期，页码
    1–42, 2021年。'
- en: '[57] L. Gonog and Y. Zhou, “A review: generative adversarial networks,” in
    *2019 14th IEEE conference on industrial electronics and applications (ICIEA)*.   IEEE,
    2019, pp. 505–510.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] L. Gonog 和 Y. Zhou, “综述：生成对抗网络，” 收录于 *2019年第14届IEEE工业电子与应用会议（ICIEA）*。   IEEE,
    2019年，页码 505–510。'
- en: '[58] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, and F.-Y. Wang, “Generative
    adversarial networks: introduction and outlook,” *IEEE/CAA Journal of Automatica
    Sinica*, vol. 4, no. 4, pp. 588–598, 2017.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng 和 F.-Y. Wang, “生成对抗网络：介绍与展望，”
    *IEEE/CAA 自动化学报*, 第4卷，第4期，页码 588–598, 2017年。'
- en: '[59] P. Turaga, A. Veeraraghavan, and R. Chellappa, “Statistical analysis on
    stiefel and grassmann manifolds with applications in computer vision,” in *2008
    IEEE conference on computer vision and pattern recognition*.   IEEE, 2008, pp.
    1–8.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] P. Turaga, A. Veeraraghavan 和 R. Chellappa, “Stiefel 和 Grassmann 流形的统计分析及其在计算机视觉中的应用，”
    收录于 *2008 IEEE 计算机视觉与模式识别会议*。   IEEE, 2008年，页码 1–8。'
- en: '[60] T. Huckle and A. Kallischko, “Frobenius norm minimization and probing
    for preconditioning,” *International Journal of Computer Mathematics*, vol. 84,
    no. 8, pp. 1225–1248, 2007.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. Huckle 和 A. Kallischko, “Frobenius范数最小化与预处理探测，” *国际计算数学杂志*, 第84卷，第8期，页码
    1225–1248, 2007年。'
- en: '[61] J. Yang, J. Yang, S. Wang, S. Cao, H. Zou, and L. Xie, “Advancing imbalanced
    domain adaptation: Cluster-level discrepancy minimization with a comprehensive
    benchmark,” *IEEE Transactions on Cybernetics*, 2021.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Yang, J. Yang, S. Wang, S. Cao, H. Zou, 和 L. Xie，“推进不平衡领域适应：通过全面基准进行集群级差异最小化，”
    *IEEE网络学报*，2021年。'
- en: '[62] A. Berlinet and C. Thomas-Agnan, *Reproducing kernel Hilbert spaces in
    probability and statistics*.   Springer Science & Business Media, 2011.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. Berlinet 和 C. Thomas-Agnan，*概率与统计中的再现核希尔伯特空间*。 Springer Science & Business
    Media，2011年。'
- en: '[63] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical
    risk minimization,” *International Conference on Learning Representations*, 2018\.
    [Online]. Available: [https://openreview.net/forum?id=r1Ddp1-Rb](https://openreview.net/forum?id=r1Ddp1-Rb)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] H. Zhang, M. Cisse, Y. N. Dauphin, 和 D. Lopez-Paz，“mixup: 超越经验风险最小化，”
    *国际学习表示会议*，2018年。 [在线]. 可用链接: [https://openreview.net/forum?id=r1Ddp1-Rb](https://openreview.net/forum?id=r1Ddp1-Rb)'
- en: '[64] M. Xu, J. Zhang, B. Ni, T. Li, C. Wang, Q. Tian, and W. Zhang, “Adversarial
    domain adaptation with domain mixup,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 34, no. 04, 2020, pp. 6502–6509.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] M. Xu, J. Zhang, B. Ni, T. Li, C. Wang, Q. Tian, 和 W. Zhang，“通过领域mixup进行对抗性领域适应，”
    见于 *AAAI人工智能会议论文集*，第34卷，第04期，2020年，第6502–6509页。'
- en: '[65] S. Yan, H. Song, N. Li, L. Zou, and L. Ren, “Improve unsupervised domain
    adaptation with mixup training,” *arXiv preprint arXiv:2001.00677*, 2020.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Yan, H. Song, N. Li, L. Zou, 和 L. Ren，“通过mixup训练提高无监督领域适应，” *arXiv预印本
    arXiv:2001.00677*，2020年。'
- en: '[66] Y. Wu, D. Inkpen, and A. El-Roby, “Dual mixup regularized learning for
    adversarial domain adaptation,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 540–555.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Y. Wu, D. Inkpen, 和 A. El-Roby，“用于对抗性领域适应的双重mixup正则化学习，” 见于 *欧洲计算机视觉会议*。
    Springer，2020年，第540–555页。'
- en: '[67] C.-Y. Wu, R. Manmatha, A. J. Smola, and P. Krahenbuhl, “Sampling matters
    in deep embedding learning,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2840–2848.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] C.-Y. Wu, R. Manmatha, A. J. Smola, 和 P. Krahenbuhl，“深度嵌入学习中采样的重要性，” 见于
    *IEEE国际计算机视觉会议论文集*，2017年，第2840–2848页。'
- en: '[68] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2018, pp. 7794–7803.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] X. Wang, R. Girshick, A. Gupta, 和 K. He，“非局部神经网络，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2018年，第7794–7803页。'
- en: '[69] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li, “Deep reconstruction-classification
    networks for unsupervised domain adaptation,” in *European conference on computer
    vision*.   Springer, 2016, pp. 597–613.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, 和 W. Li，“用于无监督领域适应的深度重建-分类网络，”
    见于 *欧洲计算机视觉会议*。 Springer，2016年，第597–613页。'
- en: '[70] J. Yang, W. An, S. Wang, X. Zhu, C. Yan, and J. Huang, “Label-driven reconstruction
    for domain adaptation in semantic segmentation,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 480–498.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Yang, W. An, S. Wang, X. Zhu, C. Yan, 和 J. Huang，“用于语义分割领域适应的标签驱动重建，”
    见于 *欧洲计算机视觉会议*。 Springer，2020年，第480–498页。'
- en: '[71] W. Deng, Z. Su, Q. Qiu, L. Zhao, G. Kuang, M. Pietikäinen, H. Xiao, and
    L. Liu, “Deep ladder reconstruction-classification network for unsupervised domain
    adaptation,” *Pattern Recognition Letters*, vol. 152, pp. 398–405, 2021.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] W. Deng, Z. Su, Q. Qiu, L. Zhao, G. Kuang, M. Pietikäinen, H. Xiao, 和
    L. Liu，“用于无监督领域适应的深度梯形重建-分类网络，” *模式识别快报*，第152卷，第398–405页，2021年。'
- en: '[72] D. P. Kingma, M. Welling *et al.*, “An introduction to variational autoencoders,”
    *Foundations and Trends® in Machine Learning*, vol. 12, no. 4, pp. 307–392, 2019.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] D. P. Kingma, M. Welling *等*，“变分自编码器简介，” *机器学习基础与趋势®*，第12卷，第4期，第307–392页，2019年。'
- en: '[73] X. Wang, Y. Xu, K. Mao, and J. Yang, “Calibrating class weights with multi-modal
    information for partial video domain adaptation,” *arXiv preprint arXiv:2204.06187*,
    2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] X. Wang, Y. Xu, K. Mao, 和 J. Yang，“使用多模态信息校准类权重以进行部分视频领域适应，” *arXiv预印本
    arXiv:2204.06187*，2022年。'
- en: '[74] Y. Wang, X. Song, Y. Wang, P. Xu, R. Hu, and H. Chai, “Dual metric discriminator
    for open set video domain adaptation,” in *ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE, 2021,
    pp. 8198–8202.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y. Wang, X. Song, Y. Wang, P. Xu, R. Hu, 和 H. Chai，“用于开放集视频领域适应的双重度量判别器，”
    见于 *ICASSP 2021-2021 IEEE国际声学、语音与信号处理会议（ICASSP）*。 IEEE，2021年，第8198–8202页。'
- en: '[75] Y. Xu, J. Yang, H. Cao, K. Wu, M. Wu, R. Zhao, and Z. Chen, “Multi-source
    video domain adaptation with temporal attentive moment alignment,” *arXiv preprint
    arXiv:2109.09964*, 2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Xu, J. Yang, H. Cao, K. Wu, M. Wu, R. Zhao, 和 Z. Chen，“具有时间注意瞬时对齐的多源视频领域适应，”
    *arXiv预印本 arXiv:2109.09964*，2021年。'
- en: '[76] Y. Xu, J. Yang, H. Cao, K. Wu, W. Min, and Z. Chen, “Learning temporal
    consistency for source-free video domain adaptation,” *arXiv preprint arXiv:2203.04559*,
    2022.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Xu, J. Yang, H. Cao, K. Wu, W. Min, 和 Z. Chen，“学习时间一致性以进行无源视频领域适应，”
    *arXiv预印本 arXiv:2203.04559*，2022年。'
- en: '[77] Y. Xu, J. Yang, M. Wu, X. Li, L. Xie, and Z. Chen, “Extern: Leveraging
    endo-temporal regularization for black-box video domain adaptation,” *arXiv preprint
    arXiv:2208.05187*, 2022.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. Xu, J. Yang, M. Wu, X. Li, L. Xie, 和 Z. Chen，“Extern：利用内时间正则化进行黑箱视频领域适应，”
    *arXiv预印本 arXiv:2208.05187*，2022年。'
- en: '[78] Z. Yao, Y. Wang, J. Wang, P. Yu, and M. Long, “Videodg: generalizing temporal
    relations in videos to novel domains,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Z. Yao, Y. Wang, J. Wang, P. Yu, 和 M. Long，“Videodg：将视频中的时间关系推广到新领域，”
    *IEEE模式分析与机器智能汇刊*，2021年。'
- en: '[79] M. Planamente, C. Plizzari, E. Alberti, and B. Caputo, “Domain generalization
    through audio-visual relative norm alignment in first person action recognition,”
    in *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision*, 2022, pp. 1807–1818.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Planamente, C. Plizzari, E. Alberti, 和 B. Caputo，“通过音频-视觉相对范数对齐进行领域泛化用于第一人称动作识别，”发表于
    *IEEE/CVF冬季计算机视觉应用会议论文集*，2022年，第1807–1818页。'
- en: '[80] M.-H. Chen, B. Li, Y. Bao, and G. AlRegib, “Action segmentation with mixed
    temporal domain adaptation,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2020, pp. 605–614.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M.-H. Chen, B. Li, Y. Bao, 和 G. AlRegib，“具有混合时间领域适应的动作分割，”发表于 *IEEE/CVF冬季计算机视觉应用会议论文集*，2020年，第605–614页。'
- en: '[81] M.-H. Chen, B. Li, Y. Bao, G. AlRegib, and Z. Kira, “Action segmentation
    with joint self-supervised temporal domain adaptation,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp. 9454–9463.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M.-H. Chen, B. Li, Y. Bao, G. AlRegib, 和 Z. Kira，“具有联合自监督时间领域适应的动作分割，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第9454–9463页。'
- en: '[82] D. Guan, J. Huang, A. Xiao, and S. Lu, “Domain adaptive video segmentation
    via temporal consistency regularization,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 8053–8064.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] D. Guan, J. Huang, A. Xiao, 和 S. Lu，“通过时间一致性正则化进行领域自适应视频分割，”发表于 *IEEE/CVF国际计算机视觉会议论文集*，2021年，第8053–8064页。'
- en: '[83] Y. Xing, D. Guan, J. Huang, and S. Lu, “Domain adaptive video segmentation
    via temporal pseudo supervision,” *arXiv preprint arXiv:2207.02372*, 2022.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Xing, D. Guan, J. Huang, 和 S. Lu，“通过时间伪监督进行领域自适应视频分割，” *arXiv预印本 arXiv:2207.02372*，2022年。'
- en: '[84] P. Chen, L. Li, J. Wu, W. Dong, and G. Shi, “Unsupervised curriculum domain
    adaptation for no-reference video quality assessment,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 5178–5187.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] P. Chen, L. Li, J. Wu, W. Dong, 和 G. Shi，“用于无参考视频质量评估的无监督课程领域适应，”发表于 *IEEE/CVF国际计算机视觉会议论文集*，2021年，第5178–5187页。'
- en: '[85] D. Li, X. Yu, C. Xu, L. Petersson, and H. Li, “Transferring cross-domain
    knowledge for video sign language recognition,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 6205–6214.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] D. Li, X. Yu, C. Xu, L. Petersson, 和 H. Li，“跨领域知识迁移用于视频手语识别，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第6205–6214页。'
- en: '[86] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman, “The kinetics
    human action video dataset,” 2017.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, 和 A. Zisserman，“Kinetics人类动作视频数据集，”2017年。'
- en: '[87] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal,
    H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag *et al.*, “The”
    something something” video database for learning and evaluating visual common
    sense,” in *Proceedings of the IEEE international conference on computer vision*,
    2017, pp. 5842–5850.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal,
    H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag *等*，“‘The’ something
    something” 视频数据库用于学习和评估视觉常识，”发表于 *IEEE国际计算机视觉会议论文集*，2017年，第5842–5850页。'
- en: '[88] Z. Cao, L. Ma, M. Long, and J. Wang, “Partial adversarial domain adaptation,”
    in *Proceedings of the European Conference on Computer Vision (ECCV)*, 2018, pp.
    135–150.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Z. Cao, L. Ma, M. Long, 和 J. Wang, “部分对抗领域适应，” 见 *欧洲计算机视觉会议论文集 (ECCV)*，2018年，第135–150页。'
- en: '[89] S. Garg, Y. Wu, S. Balakrishnan, and Z. Lipton, “A unified view of label
    shift estimation,” *Advances in Neural Information Processing Systems*, vol. 33,
    pp. 3290–3300, 2020.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] S. Garg, Y. Wu, S. Balakrishnan, 和 Z. Lipton, “标签偏移估计的统一视图，” *神经信息处理系统进展*，第33卷，第3290–3300页，2020年。'
- en: '[90] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human actions
    classes from videos in the wild,” *arXiv preprint arXiv:1212.0402*, 2012.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] K. Soomro, A. R. Zamir, 和 M. Shah, “UCF101：来自自然视频的101个人类动作类别数据集，” *arXiv预印本
    arXiv:1212.0402*，2012年。'
- en: '[91] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei,
    “Large-scale video classification with convolutional neural networks,” in *Proceedings
    of the IEEE conference on Computer Vision and Pattern Recognition*, 2014, pp.
    1725–1732.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, 和 L. Fei-Fei,
    “使用卷积神经网络的大规模视频分类，” 见 *IEEE计算机视觉与模式识别会议论文集*，2014年，第1725–1732页。'
- en: '[92] J. Yang, X. Peng, K. Wang, Z. Zhu, J. Feng, L. Xie, and Y. You, “Divide
    to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors,”
    *arXiv preprint arXiv:2205.14467*, 2022.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Yang, X. Peng, K. Wang, Z. Zhu, J. Feng, L. Xie, 和 Y. You, “分而适应：减轻黑箱预测器领域适应的确认偏差，”
    *arXiv预印本 arXiv:2205.14467*，2022年。'
- en: '[93] P. Rigollet, “Generalization error bounds in semi-supervised classification
    under the cluster assumption.” *Journal of Machine Learning Research*, vol. 8,
    no. 7, 2007.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] P. Rigollet, “在聚类假设下的半监督分类中的泛化误差界限。” *机器学习研究杂志*，第8卷，第7期，2007年。'
- en: '[94] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, “注意力机制就是你所需要的，” *神经信息处理系统进展*，第30卷，2017年。'
- en: '[95] R. Volpi, H. Namkoong, O. Sener, J. C. Duchi, V. Murino, and S. Savarese,
    “Generalizing to unseen domains via adversarial data augmentation,” *Advances
    in neural information processing systems*, vol. 31, 2018.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] R. Volpi, H. Namkoong, O. Sener, J. C. Duchi, V. Murino, 和 S. Savarese,
    “通过对抗数据增强推广到未见领域，” *神经信息处理系统进展*，第31卷，2018年。'
- en: '[96] W. Wang, D. Tran, and M. Feiszli, “What makes training multi-modal classification
    networks hard?” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 12 695–12 705.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] W. Wang, D. Tran, 和 M. Feiszli, “是什么让训练多模态分类网络变得困难？” 见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第12,695–12,705页。'
- en: '[97] S. Herath, M. Harandi, and F. Porikli, “Going deeper into action recognition:
    A survey,” *Image and vision computing*, vol. 60, pp. 4–21, 2017.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] S. Herath, M. Harandi, 和 F. Porikli, “深入探讨动作识别：综述，” *图像与视觉计算*，第60卷，第4–21页，2017年。'
- en: '[98] R. Poppe, “A survey on vision-based human action recognition,” *Image
    and vision computing*, vol. 28, no. 6, pp. 976–990, 2010.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] R. Poppe, “基于视觉的人类动作识别综述，” *图像与视觉计算*，第28卷，第6期，第976–990页，2010年。'
- en: '[99] X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, “The apolloscape
    open dataset for autonomous driving and its application,” *IEEE transactions on
    pattern analysis and machine intelligence*, vol. 42, no. 10, pp. 2702–2719, 2019.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng, 和 R. Yang, “用于自主驾驶的Apolloscape开放数据集及其应用，”
    *IEEE模式分析与机器智能学报*，第42卷，第10期，第2702–2719页，2019年。'
- en: '[100] M. Siam, A. Kendall, and M. Jagersand, “Video class agnostic segmentation
    benchmark for autonomous driving,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 2825–2834.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] M. Siam, A. Kendall, 和 M. Jagersand, “面向自主驾驶的视频无关分割基准，” 见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第2825–2834页。'
- en: '[101] Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le, “Unsupervised data augmentation
    for consistency training,” *Advances in Neural Information Processing Systems*,
    vol. 33, pp. 6256–6268, 2020.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Q. Xie, Z. Dai, E. Hovy, T. Luong, 和 Q. Le, “用于一致性训练的无监督数据增强，” *神经信息处理系统进展*，第33卷，第6256–6268页，2020年。'
- en: '[102] Y. Ouali, C. Hudelot, and M. Tami, “Semi-supervised semantic segmentation
    with cross-consistency training,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2020, pp. 12 674–12 684.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Y. Ouali, C. Hudelot, 和 M. Tami, “带有交叉一致性训练的半监督语义分割，” 见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第12,674–12,684页。'
- en: '[103] L. Melas-Kyriazi and A. K. Manrai, “Pixmatch: Unsupervised domain adaptation
    via pixelwise consistency training,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 12 435–12 445.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] L. Melas-Kyriazi 和 A. K. Manrai, “Pixmatch: 通过像素一致性训练实现无监督领域适应，” *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第12 435–12 445页。'
- en: '[104] W. Sultani and I. Saleemi, “Human action recognition across datasets
    by foreground-weighted histogram decomposition,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2014, pp. 764–771.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] W. Sultani 和 I. Saleemi, “通过前景加权直方图分解实现跨数据集的人类动作识别，” *IEEE计算机视觉与模式识别会议论文集*，2014年，第764–771页。'
- en: '[105] Y. Xu, J. Yang, H. Cao, K. Mao, J. Yin, and S. See, “Arid: A new dataset
    for recognizing action in the dark,” in *International Workshop on Deep Learning
    for Human Activity Recognition*.   Springer, 2021, pp. 70–84.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Xu, J. Yang, H. Cao, K. Mao, J. Yin, 和 S. See, “Arid: 用于黑暗环境下动作识别的新数据集，”
    *国际人类活动识别深度学习研讨会*。Springer，2021年，第70–84页。'
- en: '[106] J. Choi, G. Sharma, M. Chandraker, and J.-B. Huang, “Unsupervised and
    semi-supervised domain adaptation for action recognition from drones,” in *The
    IEEE Winter Conference on Applications of Computer Vision (WACV)*, March 2020.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. Choi, G. Sharma, M. Chandraker, 和 J.-B. Huang, “无人机动作识别的无监督和半监督领域适应，”
    *IEEE计算机视觉应用冬季会议（WACV）*，2020年3月。'
- en: '[107] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *Proceedings of the
    IEEE international conference on computer vision*, 2015, pp. 4489–4497.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] D. Tran, L. Bourdev, R. Fergus, L. Torresani, 和 M. Paluri, “使用3D卷积网络学习时空特征，”
    *IEEE国际计算机视觉会议论文集*，2015年，第4489–4497页。'
- en: '[108] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in *proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017, pp. 6299–6308.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Carreira 和 A. Zisserman, “行动识别的前景如何？新模型和kinetics数据集，” *IEEE计算机视觉与模式识别会议论文集*，2017年，第6299–6308页。'
- en: '[109] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” in *International conference on
    machine learning*.   PMLR, 2015, pp. 448–456.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] S. Ioffe 和 C. Szegedy, “批量归一化：通过减少内部协变量偏移加速深度网络训练，” *国际机器学习会议*。PMLR，2015年，第448–456页。'
- en: '[110] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] K. He, X. Zhang, S. Ren, 和 J. Sun, “深度残差学习用于图像识别，” *IEEE计算机视觉与模式识别会议论文集*，2016年，第770–778页。'
- en: '[111] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng, “Multi-fiber networks
    for video recognition,” in *Proceedings of the european conference on computer
    vision (ECCV)*, 2018, pp. 352–367.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Y. Chen, Y. Kalantidis, J. Li, S. Yan, 和 J. Feng, “视频识别的多纤维网络，” *欧洲计算机视觉会议论文集（ECCV）*，2018年，第352–367页。'
- en: '[112] J. Lin, C. Gan, K. Wang, and S. Han, “Tsm: Temporal shift module for
    efficient and scalable video understanding on edge devices,” *IEEE transactions
    on pattern analysis and machine intelligence*, 2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Lin, C. Gan, K. Wang, 和 S. Han, “TSM: 用于边缘设备上高效可扩展视频理解的时间迁移模块，” *IEEE模式分析与机器智能学报*，2020年。'
- en: '[113] K. K. Reddy and M. Shah, “Recognizing 50 human action categories of web
    videos,” *Machine vision and applications*, vol. 24, no. 5, pp. 971–981, 2013.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] K. K. Reddy 和 M. Shah, “识别网络视频中的50个人类动作类别，” *机器视觉与应用*，第24卷，第5期，第971–981页，2013年。'
- en: '[114] J. C. Niebles, C.-W. Chen, and L. Fei-Fei, “Modeling temporal structure
    of decomposable motion segments for activity classification,” in *European conference
    on computer vision*.   Springer, 2010, pp. 392–405.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. C. Niebles, C.-W. Chen, 和 L. Fei-Fei, “建模可分解运动片段的时间结构以进行活动分类，” *欧洲计算机视觉会议*。Springer，2010年，第392–405页。'
- en: '[115] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “Hmdb: a large
    video database for human motion recognition,” in *2011 International Conference
    on Computer Vision*.   IEEE, 2011, pp. 2556–2563.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, 和 T. Serre, “HMDB: 大规模视频数据库用于人类动作识别，”
    *2011年国际计算机视觉会议*。IEEE，2011年，第2556–2563页。'
- en: '[116] L. Smaira, J. Carreira, E. Noland, E. Clancy, A. Wu, and A. Zisserman,
    “A short note on the kinetics-700-2020 human action dataset,” *arXiv preprint
    arXiv:2010.10864*, 2020.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] L. Smaira, J. Carreira, E. Noland, E. Clancy, A. Wu, 和 A. Zisserman,
    “关于kinetics-700-2020人类动作数据集的简短说明，” *arXiv预印本 arXiv:2010.10864*，2020年。'
- en: '[117] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy, “Rethinking spatiotemporal
    feature learning for video understanding,” *arXiv preprint arXiv:1712.04851*,
    vol. 1, no. 2, p. 5, 2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Xie, C. Sun, J. Huang, Z. Tu, 和 K. Murphy, “重新思考视频理解中的时空特征学习，” *arXiv预印本
    arXiv:1712.04851*，第1卷，第2期，第5页，2017年。'
- en: '[118] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *et al.*, “Scaling egocentric vision:
    The epic-kitchens dataset,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 720–736.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *等人*，“缩放自我中心视觉：Epic-Kitchens 数据集，”发表于
    *欧洲计算机视觉会议（ECCV）论文集*，2018年，页码 720–736。'
- en: '[119] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang, “Moment matching
    for multi-source domain adaptation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 1406–1415.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, 和 B. Wang, “用于多源领域适应的时刻匹配，”发表于
    *IEEE/CVF国际计算机视觉会议论文集*，2019年，页码 1406–1415。'
- en: '[120] M. Monfort, A. Andonian, B. Zhou, K. Ramakrishnan, S. A. Bargal, T. Yan,
    L. Brown, Q. Fan, D. Gutfreund, C. Vondrick *et al.*, “Moments in time dataset:
    one million videos for event understanding,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 42, no. 2, pp. 502–508, 2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] M. Monfort, A. Andonian, B. Zhou, K. Ramakrishnan, S. A. Bargal, T. Yan,
    L. Brown, Q. Fan, D. Gutfreund, C. Vondrick *等人*，“时刻数据集：一百万个视频用于事件理解，” *IEEE模式分析与机器智能汇刊*，第42卷，第2期，页码
    502–508，2019年。'
- en: '[121] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2016, pp. 3213–3223.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, 和 B. Schiele, “Cityscapes 数据集用于语义城市场景理解，”发表于 *IEEE计算机视觉与模式识别会议论文集*，2016年，页码
    3213–3223。'
- en: '[122] S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,” in
    *Proceedings of the IEEE International Conference on Computer Vision*, 2017, pp.
    2213–2222.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] S. R. Richter, Z. Hayder, 和 V. Koltun, “为基准测试而玩，”发表于 *IEEE国际计算机视觉会议论文集*，2017年，页码
    2213–2222。'
- en: '[123] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
    synthia dataset: A large collection of synthetic images for semantic segmentation
    of urban scenes,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2016, pp. 3234–3243.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, 和 A. M. Lopez, “Synthia
    数据集：用于城市场景语义分割的大量合成图像集，”发表于 *IEEE计算机视觉与模式识别会议论文集*，2016年，页码 3234–3243。'
- en: '[124] J. K. Haas, “A history of the unity game engine,” *Diss. WORCESTER POLYTECHNIC
    INSTITUTE*, vol. 483, p. 484, 2014.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. K. Haas, “Unity游戏引擎的历史，” *WORCESTER POLYTECHNIC INSTITUTE博士论文*，第483卷，第484页，2014年。'
- en: '[125] C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast networks for
    video recognition,” in *Proceedings of the IEEE/CVF international conference on
    computer vision*, 2019, pp. 6202–6211.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] C. Feichtenhofer, H. Fan, J. Malik, 和 K. He, “Slowfast网络用于视频识别，”发表于 *IEEE/CVF国际计算机视觉会议论文集*，2019年，页码
    6202–6211。'
- en: '[126] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. V. Gool,
    “Temporal segment networks: Towards good practices for deep action recognition,”
    in *European conference on computer vision*.   Springer, 2016, pp. 20–36.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, 和 L. V. Gool, “时间段网络：深度动作识别的良好实践，”发表于
    *欧洲计算机视觉会议*。Springer, 2016年，页码 20–36。'
- en: '[127] S. Jain, X. Wang, and J. E. Gonzalez, “Accel: A corrective fusion network
    for efficient semantic segmentation on video,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2019, pp. 8866–8875.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] S. Jain, X. Wang, 和 J. E. Gonzalez, “Accel：用于高效语义分割的视频纠正融合网络，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，页码
    8866–8875。'
- en: '[128] J.-C. Su, Y.-H. Tsai, K. Sohn, B. Liu, S. Maji, and M. Chandraker, “Active
    adversarial domain adaptation,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2020, pp. 739–748.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] J.-C. Su, Y.-H. Tsai, K. Sohn, B. Liu, S. Maji, 和 M. Chandraker, “主动对抗领域适应，”发表于
    *IEEE/CVF冬季计算机视觉应用会议论文集*，2020年，页码 739–748。'
- en: '[129] V. Prabhu, A. Chandrasekaran, K. Saenko, and J. Hoffman, “Active domain
    adaptation via clustering uncertainty-weighted embeddings,” in *Proceedings of
    the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 8505–8514.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] V. Prabhu, A. Chandrasekaran, K. Saenko, 和 J. Hoffman, “通过聚类不确定性加权嵌入进行主动领域适应，”发表于
    *IEEE/CVF国际计算机视觉会议论文集*，2021年，页码 8505–8514。'
- en: '[130] X. Peng, Z. Huang, Y. Zhu, and K. Saenko, “Federated adversarial domain
    adaptation,” *arXiv preprint arXiv:1911.02054*, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] X. Peng, Z. Huang, Y. Zhu, 和 K. Saenko, “联邦对抗性领域适应，” *arXiv预印本arXiv:1911.02054*，2019年。'
- en: '[131] P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B. B. Gupta, X. Chen,
    and X. Wang, “A survey of deep active learning,” *ACM computing surveys (CSUR)*,
    vol. 54, no. 9, pp. 1–40, 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B. B. Gupta, X. Chen,
    和 X. Wang, “深度主动学习的综述，” *ACM计算调查（CSUR）*，第54卷，第9期，页码 1–40，2021年。'
- en: '[132] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen, and H. Yu, “Federated learning,”
    *Synthesis Lectures on Artificial Intelligence and Machine Learning*, vol. 13,
    no. 3, pp. 1–207, 2019.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen, 和 H. Yu, “联邦学习，” *人工智能与机器学习讲义*，第13卷，第3期，页码
    1–207，2019年。'
- en: '[133] Q. Ke, M. Bennamoun, S. An, F. Sohel, and F. Boussaid, “A new representation
    of skeleton sequences for 3d action recognition,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 3288–3297.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Q. Ke, M. Bennamoun, S. An, F. Sohel, 和 F. Boussaid, “用于3D动作识别的骨架序列的新表示，”
    见于 *IEEE计算机视觉与模式识别会议论文集*，2017年，页码 3288–3297。'
- en: '[134] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, “An end-to-end spatio-temporal
    attention model for human action recognition from skeleton data,” in *Proceedings
    of the AAAI conference on artificial intelligence*, vol. 31, no. 1, 2017.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] S. Song, C. Lan, J. Xing, W. Zeng, 和 J. Liu, “一种端到端时空注意力模型用于从骨架数据中识别人类动作，”
    见于 *AAAI人工智能会议论文集*，第31卷，第1期，2017年。'
- en: '[135] Y. Du, W. Wang, and L. Wang, “Hierarchical recurrent neural network for
    skeleton based action recognition,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2015, pp. 1110–1118.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Du, W. Wang, 和 L. Wang, “基于骨架的动作识别的层次递归神经网络，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2015年，页码
    1110–1118。'
- en: '[136] H. Duan, Y. Zhao, K. Chen, D. Lin, and B. Dai, “Revisiting skeleton-based
    action recognition,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 2969–2978.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. Duan, Y. Zhao, K. Chen, D. Lin, 和 B. Dai, “重新审视基于骨架的动作识别，” 见于 *IEEE/CVF计算机视觉与模式识别大会论文集*，2022年，页码
    2969–2978。'
- en: '[137] M. F. Bulbul and H. Ali, “Gradient local auto-correlation features for
    depth human action recognition,” *SN Applied Sciences*, vol. 3, no. 5, pp. 1–13,
    2021.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] M. F. Bulbul 和 H. Ali, “用于深度人体动作识别的梯度局部自相关特征，” *SN应用科学*，第3卷，第5期，页码 1–13，2021年。'
- en: '[138] Y. Xiao, J. Chen, Y. Wang, Z. Cao, J. T. Zhou, and X. Bai, “Action recognition
    for depth video using multi-view dynamic images,” *Information Sciences*, vol.
    480, pp. 287–304, 2019.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Xiao, J. Chen, Y. Wang, Z. Cao, J. T. Zhou, 和 X. Bai, “使用多视角动态图像的深度视频动作识别，”
    *信息科学*，第480卷，页码 287–304，2019年。'
- en: '[139] C. Gao, Y. Du, J. Liu, J. Lv, L. Yang, D. Meng, and A. G. Hauptmann,
    “Infar dataset: Infrared action recognition at different times,” *Neurocomputing*,
    vol. 212, pp. 36–47, 2016.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] C. Gao, Y. Du, J. Liu, J. Lv, L. Yang, D. Meng, 和 A. G. Hauptmann, “Infar数据集：不同时间的红外动作识别，”
    *神经计算*，第212卷，页码 36–47，2016年。'
- en: '[140] Y. Liu, Z. Lu, J. Li, T. Yang, and C. Yao, “Global temporal representation
    based cnns for infrared action recognition,” *IEEE Signal Processing Letters*,
    vol. 25, no. 6, pp. 848–852, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Y. Liu, Z. Lu, J. Li, T. Yang, 和 C. Yao, “基于全球时间表示的卷积神经网络用于红外动作识别，” *IEEE信号处理快报*，第25卷，第6期，页码
    848–852，2018年。'
- en: '[141] T. Zhong, W. Kim, M. Tanaka, and M. Okutomi, “Human segmentation with
    dynamic lidar data,” in *2020 25th International Conference on Pattern Recognition
    (ICPR)*.   IEEE, 2021, pp. 1166–1172.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] T. Zhong, W. Kim, M. Tanaka, 和 M. Okutomi, “利用动态激光雷达数据进行人体分割，” 见于 *2020年第25届国际模式识别大会（ICPR）*，IEEE，2021年，页码
    1166–1172。'
- en: '[142] M. You, C. Luo, H. Zhou, and S. Zhu, “Dynamic dense crf inference for
    video segmentation and semantic slam,” *Pattern Recognition*, p. 109023, 2022.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] M. You, C. Luo, H. Zhou, 和 S. Zhu, “用于视频分割和语义SLAM的动态密集CRF推断，” *模式识别*，页码
    109023，2022年。'
- en: '[143] C. Benedek, B. Gálai, B. Nagy, and Z. Jankó, “Lidar-based gait analysis
    and activity recognition in a 4d surveillance system,” *IEEE Transactions on Circuits
    and Systems for Video Technology*, vol. 28, no. 1, pp. 101–113, 2016.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] C. Benedek, B. Gálai, B. Nagy, 和 Z. Jankó, “基于激光雷达的步态分析与活动识别在4D监控系统中的应用，”
    *IEEE视频技术电路与系统汇刊*，第28卷，第1期，页码 101–113，2016年。'
- en: '[144] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, and C. Schmid, “Vivit:
    A video vision transformer,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 6836–6846.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, 和 C. Schmid, “Vivit：视频视觉变换器，”
    见于 *IEEE/CVF国际计算机视觉大会论文集*，2021年，页码 6836–6846。'
- en: '[145] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
    “Swin transformer: Hierarchical vision transformer using shifted windows,” in
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2021,
    pp. 10 012–10 022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin 和 B. Guo，"Swin
    变换器: 使用偏移窗口的分层视觉变换器"，在*IEEE/CVF 国际计算机视觉会议论文集*，2021年，第10 012–10 022页。'
- en: '[146] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian, “Visformer: The
    vision-friendly transformer,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 589–598.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei 和 Q. Tian，"Visformer: 视觉友好的变换器"，在*IEEE/CVF
    国际计算机视觉会议论文集*，2021年，第589–598页。'
- en: '[147] X. Sui, S. Li, X. Geng, Y. Wu, X. Xu, Y. Liu, R. Goh, and H. Zhu, “Craft:
    Cross-attentional flow transformer for robust optical flow,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp.
    17 602–17 611.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] X. Sui, S. Li, X. Geng, Y. Wu, X. Xu, Y. Liu, R. Goh 和 H. Zhu，"Craft:
    跨注意力流变换器用于鲁棒光流"，在*IEEE/CVF 计算机视觉与模式识别会议论文集*，2022年，第17 602–17 611页。'
- en: '[148] J. Gao, T. Zhang, and C. Xu, “Learning to model relationships for zero-shot
    video classification,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 43, no. 10, pp. 3476–3491, 2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Gao, T. Zhang 和 C. Xu，"学习建模关系以实现零样本视频分类"，*IEEE 计算机视觉与模式识别学报*，第43卷，第10期，第3476–3491页，2020年。'
- en: '[149] Y. Liu, K. Wang, L. Liu, H. Lan, and L. Lin, “Tcgl: Temporal contrastive
    graph for self-supervised video representation learning,” *IEEE Transactions on
    Image Processing*, vol. 31, pp. 1978–1993, 2022.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Liu, K. Wang, L. Liu, H. Lan 和 L. Lin，"Tcgl: 时序对比图用于自监督视频表示学习"，*IEEE
    图像处理学报*，第31卷，第1978–1993页，2022年。'
- en: '[150] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE transactions on neural networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner 和 G. Monfardini，"图神经网络模型"，*IEEE
    神经网络学报*，第20卷，第1期，第61–80页，2008年。'
- en: '[151] C. Zhang, D. Song, C. Huang, A. Swami, and N. V. Chawla, “Heterogeneous
    graph neural network,” in *Proceedings of the 25th ACM SIGKDD international conference
    on knowledge discovery & data mining*, 2019, pp. 793–803.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] C. Zhang, D. Song, C. Huang, A. Swami 和 N. V. Chawla，"异质图神经网络"，在*第25届ACM
    SIGKDD 国际知识发现与数据挖掘会议论文集*，2019年，第793–803页。'
- en: '[152] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *International Conference on Machine Learning*.   PMLR,
    2021, pp. 8748–8763.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *等*，"从自然语言监督中学习可迁移的视觉模型"，在*国际机器学习会议*上。PMLR, 2021年，第8748–8763页。'
