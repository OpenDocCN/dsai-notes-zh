- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:05:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:05:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1908.09381] Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1908.09381] 深度强化学习中的**概率图模型**和**变分推断**教程与综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1908.09381](https://ar5iv.labs.arxiv.org/html/1908.09381)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1908.09381](https://ar5iv.labs.arxiv.org/html/1908.09381)
- en: Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习中的**概率图模型**和**变分推断**教程与综述
- en: Xudong Sun Department of Statistics
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**徐东** 统计学系'
- en: Ludwig Maximillian University of Munich
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 路德维希·马克西米利安大学
- en: Munich, Germany
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 慕尼黑，德国
- en: 'Email: xudong.sun@stat.uni-muenchen.de    Bernd Bischl Department of Statistics'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 邮箱：xudong.sun@stat.uni-muenchen.de    **伯恩德·比施尔** 统计学系
- en: Ludwig Maximillian University of Munich
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 路德维希·马克西米利安大学
- en: Munich, Germany
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 慕尼黑，德国
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Aiming at a comprehensive and concise tutorial survey, recap of variational
    inference and reinforcement learning with Probabilistic Graphical Models are given
    with detailed derivations. Reviews and comparisons on recent advances in deep
    reinforcement learning are made from various aspects. We offer detailed derivations
    to a taxonomy of Probabilistic Graphical Model and Variational Inference methods
    in deep reinforcement learning, which serves as a complementary material on top
    of the original contributions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在提供一个全面且简洁的教程综述，详细推导了变分推断和使用**概率图模型**的强化学习。对深度强化学习的最新进展进行了各个方面的综述和比较。我们提供了深度强化学习中概率图模型和变分推断方法的详细推导，这些内容作为原始贡献的补充材料。
- en: 'Keywords:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Probabilistic Graphical Models; Variational Inference; Deep Reinforcement Learning
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率图模型**；**变分推断**；深度强化学习'
- en: I Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: 'Despite the recent successes of Reinforcement Learning, powered by Deep Neural
    Networks, in complicated tasks like games [[1](#bib.bib1)] and robot locomotion
    [[2](#bib.bib2)], as well as optimization tasks like Automatic Machine Learning
    [[3](#bib.bib3)]. The field still faces many challenges including expressing high
    dimensional state and policy, exploration in sparse reward, etc. Probabilistic
    Graphical Model and Variational Inference offers a great tool to express a wide
    spectrum of trajectory distributions as well as conducting inference which can
    serve as a control method. Due to the emerging popularity, we present a comprehensive
    and concise tutorial survey paper with the following contributions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近通过深度神经网络在复杂任务如游戏[[1](#bib.bib1)]和机器人运动[[2](#bib.bib2)]以及自动机器学习[[3](#bib.bib3)]等优化任务中取得了成功，该领域仍面临许多挑战，包括高维状态和策略的表达、稀疏奖励中的探索等。**概率图模型**和**变分推断**提供了表达广泛轨迹分布的强大工具，并可以进行推断，作为控制方法。由于其新兴的流行性，我们呈现了一篇全面且简洁的教程综述论文，包含以下贡献：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide Probabilistic Graphical Models for many basic concepts of Reinforcement
    Learning, which is rarely covered in literature. We also provide Probabilistic
    Graphical Models to some recent works on Deep Reinforcement Learning [[4](#bib.bib4),
    [5](#bib.bib5)] which does not exist in the original contributions.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们为强化学习中的许多基本概念提供了**概率图模型**，这些内容在文献中很少涉及。我们还为一些最近的深度强化学习工作提供了**概率图模型**[[4](#bib.bib4),
    [5](#bib.bib5)]，这些内容在原始贡献中并不存在。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We cover a taxonomy of Probabilistic Graphical Model and Variational Inference
    [[6](#bib.bib6)] methods used in Deep Reinforcement Learning and give detailed
    derivations to many of the critical equations, which is not given in the original
    contributions. Together with the recap of variational inference and deep reinforcement
    learning, the paper serves as a self-inclusive tutorial to both beginner and advanced
    readers.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们涵盖了深度强化学习中使用的**概率图模型**和**变分推断**[[6](#bib.bib6)]方法的分类，并对许多关键方程进行了详细推导，这些内容在原始贡献中没有提供。结合变分推断和深度强化学习的回顾，本文作为一个自包含的教程，适用于初学者和高级读者。
- en: I-A Organization of the paper
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 论文组织
- en: In section [I-B](#S1.SS2 "I-B Prerequisite on Probabilistic Graphical Models
    and Variational Inference, Terminologies and Conventions ‣ I Introduction ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning"), we first introduce the fundamentals of Probabilistic
    Graphical Models and Variational Inference, then we review the basics about reinforcement
    learning by connecting probabilistic graphical models (PGM) in section [II-A](#S2.SS1
    "II-A Basics about Reinforcement Learning with graphical model ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"), [II-B](#S2.SS2
    "II-B Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), [II-C](#S2.SS3
    "II-C Policy Gradient and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning"), as well as an overview about deep
    reinforcement learning, accompanied with a comparison of different methods in
    section [II-D](#S2.SS4 "II-D Basics of Deep Reinforcement Learning ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"). In
    section [III-A](#S3.SS1 "III-A Policy and value function with undirected graphs
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), we discuss how undirected graph could be used in modeling both the
    value function and the policy, which works well on high dimensional discrete state
    and action spaces. In section [III-B](#S3.SS2 "III-B Variational Inference on
    ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning"), we introduce the directed acyclic graph framework
    on how to treat the policy as posterior on actions, while adding many proofs that
    does not exist in the original contributions. In section [III-C](#S3.SS3 "III-C
    Variational Inference on the Environment ‣ III Taxonomy of PGM and VI in deep
    reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning"), we introduce works
    on how to use variational inference to approximate the environment model, while
    adding graphical models and proofs which does not exist in the original contributions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在章节 [I-B](#S1.SS2 "I-B 概率图模型和变分推断的先决条件、术语和约定 ‣ I 引言 ‣ 深度强化学习中的概率图模型和变分推断教程及综述")
    中，我们首先介绍概率图模型和变分推断的基础知识，然后通过在章节 [II-A](#S2.SS1 "II-A 图模型的强化学习基础 ‣ II 强化学习与深度强化学习
    ‣ 深度强化学习中的概率图模型和变分推断教程及综述")、[II-B](#S2.SS2 "II-B 值函数、贝尔曼方程、策略迭代 ‣ II 强化学习与深度强化学习
    ‣ 深度强化学习中的概率图模型和变分推断教程及综述")、[II-C](#S2.SS3 "II-C 策略梯度和演员评论家 ‣ II 强化学习与深度强化学习 ‣
    深度强化学习中的概率图模型和变分推断教程及综述") 中回顾强化学习的基础知识，并概述深度强化学习的不同方法，在章节 [II-D](#S2.SS4 "II-D
    深度强化学习基础 ‣ II 强化学习与深度强化学习 ‣ 深度强化学习中的概率图模型和变分推断教程及综述") 中进行比较。在章节 [III-A](#S3.SS1
    "III-A 使用无向图的策略和值函数 ‣ III 深度强化学习中概率图模型和变分推断的分类 ‣ 深度强化学习中的概率图模型和变分推断教程及综述") 中，我们讨论了如何使用无向图来建模值函数和策略，这在高维离散状态和动作空间中效果良好。在章节
    [III-B](#S3.SS2 "III-B 关于”最优”策略的变分推断 ‣ III 深度强化学习中概率图模型和变分推断的分类 ‣ 深度强化学习中的概率图模型和变分推断教程及综述")
    中，我们介绍了如何将策略视为动作的后验的有向无环图框架，同时添加了许多原始贡献中不存在的证明。在章节 [III-C](#S3.SS3 "III-C 环境的变分推断
    ‣ III 深度强化学习中概率图模型和变分推断的分类 ‣ 深度强化学习中的概率图模型和变分推断教程及综述") 中，我们介绍了如何使用变分推断来近似环境模型，并添加了原始贡献中不存在的图模型和证明。
- en: I-B Prerequisite on Probabilistic Graphical Models and Variational Inference,
    Terminologies and Conventions
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 概率图模型和变分推断的先决条件、术语和约定
- en: We use capital letter to denote a Random Variable (RV), while using the lower
    case letter to represent the realization. To avoid symbol collision of using $A$
    to represent advantage in many RL literature, we use $A^{act}$ explicitly to represent
    action. We use $(B\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}C)\mid
    A$ to represent $B$ is conditionally independent from $C$, given $A$, or equivalently
    $p(B|A,C)=p(B|A)$ or $p(BC|A)=P(B|A)P(C|A)$. Directed Acyclic Graphs (DAG) [[7](#bib.bib7)]
    as a PGM offers an instinctive way of defining factorized joint distributions
    of RV by assuming the conditional independence [[7](#bib.bib7)] through d-separation
    [[7](#bib.bib7)]. Undirected Graph including Markov Random Fields also specifies
    the conditional independence with local Markov property and global Markov property
    [[8](#bib.bib8)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用大写字母来表示随机变量（RV），而使用小写字母来表示其实现。为了避免在许多强化学习文献中使用 $A$ 来表示优势时符号冲突，我们明确使用 $A^{act}$
    来表示动作。我们用 $(B\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}C)\mid
    A$ 表示 $B$ 在给定 $A$ 的条件下与 $C$ 条件独立，或等价地 $p(B|A,C)=p(B|A)$ 或 $p(BC|A)=P(B|A)P(C|A)$。有向无环图（DAG）[[7](#bib.bib7)]
    作为概率图模型提供了一种直观的方法，通过 d-分离 [[7](#bib.bib7)] 假设条件独立来定义随机变量的分解联合分布 [[7](#bib.bib7)]。无向图，包括马尔可夫随机场，也通过局部马尔可夫性质和全局马尔可夫性质来指定条件独立
    [[8](#bib.bib8)]。
- en: Variational Inference (VI) approximates intractable posterior distribution $p(z\mid
    x)=\frac{1}{\int_{z^{{}^{\prime}}}p(z^{{}^{\prime}})p(x|z^{{}^{\prime}})dz^{{}^{\prime}}}p(z)p(x\mid
    z)$ with latent variable $z$ specified in a probabilistic graphical model, by
    a variational proposal posterior distribution $q_{\phi}(z\mid x)$, characterized
    by variational parameter $\phi$. By optimizing the Evidence Lower Bound (ELBO)
    [[6](#bib.bib6)], VI assigns the values to observed and latent variables at the
    same time. VI is widely used in Deep Learning Community like variational resampling
    [[9](#bib.bib9)]. VI is also used in approximating the posterior on the weights
    distribution of neural networks for Thompson Sampling to tackle the exploration-exploitation
    trade off in bandit problems [[10](#bib.bib10)], as well as approximating on the
    activations distribution like Variational AutoEncoder [[11](#bib.bib11)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推断（VI）通过变分提议后验分布 $q_{\phi}(z\mid x)$（由变分参数 $\phi$ 表征）来逼近在概率图模型中指定的不可处理的后验分布
    $p(z\mid x)=\frac{1}{\int_{z^{{}^{\prime}}}p(z^{{}^{\prime}})p(x|z^{{}^{\prime}})dz^{{}^{\prime}}}p(z)p(x\mid
    z)$。通过优化证据下界（ELBO）[[6](#bib.bib6)]，变分推断同时为观察到的变量和潜在变量赋值。变分推断在深度学习社区中被广泛使用，如变分重采样
    [[9](#bib.bib9)]。变分推断还用于逼近神经网络权重分布的后验，以解决在多臂老虎机问题中的探索-利用权衡 [[10](#bib.bib10)]，以及逼近激活分布，如变分自编码器
    [[11](#bib.bib11)]。
- en: As a contribution of this paper, we summarize the relationship of evidence $\log
    p(x)$, KL divergence $D_{KL}$, cross entropy $H_{q}(p)$, entropy $H(q)$, free
    energy $F(\phi,\theta)$ and $ELBO(\phi,\theta)$ in Equation ([1](#S1.E1 "In I-B
    Prerequisite on Probabilistic Graphical Models and Variational Inference, Terminologies
    and Conventions ‣ I Introduction ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本文的贡献，我们总结了证据 $\log p(x)$、KL 散度 $D_{KL}$、交叉熵 $H_{q}(p)$、熵 $H(q)$、自由能 $F(\phi,\theta)$
    和 $ELBO(\phi,\theta)$ 在方程 ([1](#S1.E1 "在 I-B 概率图模型和变分推断的前提条件，术语和约定 ‣ I 引言 ‣ 深度强化学习中的概率图模型和变分推断教程与调查"))
    中的关系。
- en: '|  |  | $\displaystyle\log p(x)$ |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\log p(x)$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{q}\left[\log p(x\mid z)\right]-E_{q}\left[\log\frac{q_{\phi}{(z\mid
    x)}}{p(z)}\right]+E_{q}\left[\log\frac{q_{\phi}{(z\mid x)}}{p(z\mid x)}\right]$
    |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{q}\left[\log p(x\mid z)\right]-E_{q}\left[\log\frac{q_{\phi}{(z\mid
    x)}}{p(z)}\right]+E_{q}\left[\log\frac{q_{\phi}{(z\mid x)}}{p(z\mid x)}\right]$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(q_{\phi}(z&#124;x)&#124;&#124;p(x,z))-E_{q}\left[\log
    p(z&#124;x)\right]+$ |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(q_{\phi}(z&#124;x)&#124;&#124;p(x,z))-E_{q}\left[\log
    p(z&#124;x)\right]+$ |  |'
- en: '|  |  | $\displaystyle E_{q}\log q_{\phi}(z&#124;x)$ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle E_{q}\log q_{\phi}(z&#124;x)$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle-F(\phi,\theta)+H_{q}(p)-H(q)$ |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-F(\phi,\theta)+H_{q}(p)-H(q)$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle ELBO(\phi,\theta)+D_{KL}(q_{\phi}(z\mid
    x)&#124;&#124;p(z\mid x))$ |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle ELBO(\phi,\theta)+D_{KL}(q_{\phi}(z\mid
    x)&#124;&#124;p(z\mid x))$ |  | (1) |'
- en: II Reinforcement Learning and Deep Reinforcement Learning
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 强化学习与深度强化学习
- en: II-A Basics about Reinforcement Learning with graphical model
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 关于图模型的强化学习基础
- en: \includegraphics
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.6]tikz_rl_def.pdf'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.6]tikz_rl_def.pdf'
- en: 'Figure 1: Concept of Reinforcement Learning'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：强化学习的概念
- en: II-A1 RL Concepts, Terminology and Convention
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 RL 概念、术语和约定
- en: As shown in Figure [1](#S2.F1 "Figure 1 ‣ II-A Basics about Reinforcement Learning
    with graphical model ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning"), Reinforcement Learning (RL) involves optimizing
    the behavior of an agent via interaction with the environment. At time $t$, the
    agent lives on state $S_{t}$, By executing an action $a_{t}$ according to a policy
    [[12](#bib.bib12)] $\pi(a|S_{t})$, the agent jumps to another state $S_{t+1}$,
    while receiving a reward $R_{t}$. Let discount factor $\gamma$ decides how much
    the immediate reward is favored compared to longer term return, with which one
    could also allow tractability in infinite horizon reinforcement learning [[12](#bib.bib12)],
    as well as reducing variance in Monte Carlo setting [[13](#bib.bib13)]. The goal
    is to maximize the accumulated rewards, $G=\sum_{t=0}^{T}\gamma^{t}R_{t}$ which
    is usually termed return in RL literature.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](#S2.F1 "Figure 1 ‣ II-A Basics about Reinforcement Learning with graphical
    model ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and
    Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning") 所示，强化学习（RL）涉及通过与环境的交互来优化智能体的行为。在时间 $t$，智能体处于状态 $S_{t}$，通过按照策略 [[12](#bib.bib12)]
    $\pi(a|S_{t})$ 执行动作 $a_{t}$，智能体跳到另一个状态 $S_{t+1}$，同时获得奖励 $R_{t}$。折扣因子 $\gamma$
    决定了即时奖励相对于长期回报的优先级，同时也可以在无限视野强化学习中允许可处理性 [[12](#bib.bib12)]，以及在蒙特卡罗设置中减少方差 [[13](#bib.bib13)]。目标是最大化累计奖励
    $G=\sum_{t=0}^{T}\gamma^{t}R_{t}$，这通常在 RL 文献中称为回报。
- en: 'For simplicity, we interchangeably use two conventions whenever convenient:
    Suppose an episode last from $t=0:T$, with $T\rightarrow\infty$ correspond to
    continuous non-episodic reinforcement learning. We use another convention of $t\in\{0,\cdots,\infty\}$
    by assuming when episode ends, the agent stays at a self absorbing state with
    a null action, while receiving null reward.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们在方便时交替使用两种约定：假设一个回合持续从 $t=0:T$，其中 $T\rightarrow\infty$ 对应于连续非回合制强化学习。我们使用另一种约定
    $t\in\{0,\cdots,\infty\}$，假设当回合结束时，智能体保持在一个自我吸收状态中，采取无效动作，同时获得零奖励。
- en: By unrolling Figure [1](#S2.F1 "Figure 1 ‣ II-A Basics about Reinforcement Learning
    with graphical model ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning"), we get a sequence of state, action and reward
    tuples $\{(S_{t},A^{act}_{t},R_{t})\}$ in an episode, which is coined trajectory
    $\tau$ [[14](#bib.bib14)]. Figure [2](#S2.F2 "Figure 2 ‣ II-A1 RL Concepts, Terminology
    and Convention ‣ II-A Basics about Reinforcement Learning with graphical model
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning") illustrates part of a trajectory in one rollout. The state space $\mathcal{S}$
    and action space $\mathcal{A}$, which can be either discrete or continuous and
    multi-dimensional, are each represented with one continuous dimension in Figure
    [2](#S2.F2 "Figure 2 ‣ II-A1 RL Concepts, Terminology and Convention ‣ II-A Basics
    about Reinforcement Learning with graphical model ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning") and plotted in
    an orthogonal way with different colors, while we use the thickness of the plate
    to represent the reward space $\mathcal{R}$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过展开图[1](#S2.F1 "Figure 1 ‣ II-A Basics about Reinforcement Learning with graphical
    model ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and
    Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")，我们得到一个由状态、动作和奖励元组 $\{(S_{t},A^{act}_{t},R_{t})\}$ 组成的序列，这个序列被称为轨迹 $\tau$
    [[14](#bib.bib14)]。图[2](#S2.F2 "Figure 2 ‣ II-A1 RL Concepts, Terminology and
    Convention ‣ II-A Basics about Reinforcement Learning with graphical model ‣ II
    Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on
    Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning") 说明了一个回合中轨迹的部分。状态空间 $\mathcal{S}$ 和动作空间 $\mathcal{A}$ 可以是离散的、连续的或多维的，在图[2](#S2.F2
    "Figure 2 ‣ II-A1 RL Concepts, Terminology and Convention ‣ II-A Basics about
    Reinforcement Learning with graphical model ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning") 中各自用一个连续维度表示，并用不同的颜色正交绘制，同时我们使用板的厚度来表示奖励空间
    $\mathcal{R}$。
- en: \includegraphics
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.6]tikz_drl_illustrator.pdf'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.6]tikz_drl_illustrator.pdf'
- en: 'Figure 2: Illustration of State, Action and Reward Trajectory'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 状态、动作和奖励轨迹的插图'
- en: II-A2 DAGs for (Partially Observed ) Markov Decision Process
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 (部分观察的) 马尔可夫决策过程的 DAG
- en: Reinforcement Learning is a stochastic decision process, which usually comes
    with three folds of uncertainty. That is, under a particular stochastic policy
    characterized by $\pi(a|s)=p(a|s)$, within a particular environment characterized
    by state transition probability $p(s_{t+1}|s_{t},a)$ and reward distribution function
    $p(r_{t}|s_{t},a_{t})$, a learning agent could observe different trajectories
    with different unrolling realizations. This is usually modeled as a Markov Decision
    Process [[12](#bib.bib12)], with its graphical model shown in Figure [3](#S2.F3
    "Figure 3 ‣ II-A2 DAGs for (Partially Observed ) Markov Decision Process ‣ II-A
    Basics about Reinforcement Learning with graphical model ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), where we could
    define a joint probability distribution over the trajectory of state , action
    and reward RVs. In Figure [3](#S2.F3 "Figure 3 ‣ II-A2 DAGs for (Partially Observed
    ) Markov Decision Process ‣ II-A Basics about Reinforcement Learning with graphical
    model ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and
    Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), we use dashed arrows connecting state and action to represent the
    policy, upon fixed policy $\pi$, we have the trajectory likelihood in Equation
    ([2](#S2.E2 "In II-A2 DAGs for (Partially Observed ) Markov Decision Process ‣
    II-A Basics about Reinforcement Learning with graphical model ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个随机决策过程，通常伴随着三重不确定性。也就是说，在一个特定的随机策略 $\pi(a|s)=p(a|s)$ 下，在一个特定环境中，其状态转移概率
    $p(s_{t+1}|s_{t},a)$ 和奖励分布函数 $p(r_{t}|s_{t},a_{t})$，学习代理可以观察到具有不同展开实现的不同轨迹。这通常被建模为马尔可夫决策过程
    [[12](#bib.bib12)]，其图形模型如图 [3](#S2.F3 "图 3 ‣ II-A2 (部分观察的) 马尔可夫决策过程的 DAG ‣ II-A
    图形模型中的强化学习基础 ‣ II 强化学习与深度强化学习 ‣ 深度强化学习中的概率图模型与变分推断教程与调查") 所示，我们可以定义一个状态、动作和奖励随机变量的轨迹上的联合概率分布。在图
    [3](#S2.F3 "图 3 ‣ II-A2 (部分观察的) 马尔可夫决策过程的 DAG ‣ II-A 图形模型中的强化学习基础 ‣ II 强化学习与深度强化学习
    ‣ 深度强化学习中的概率图模型与变分推断教程与调查") 中，我们使用虚线箭头连接状态和动作来表示策略，在固定策略 $\pi$ 下，我们在方程 ([2](#S2.E2
    "在 II-A2 (部分观察的) 马尔可夫决策过程的 DAG ‣ II-A 图形模型中的强化学习基础 ‣ II 强化学习与深度强化学习 ‣ 深度强化学习中的概率图模型与变分推断教程与调查"))
    中有轨迹的似然。
- en: '|  | $\displaystyle p(\tau)=p(s_{0})\prod_{t=0}^{T}p(s_{t+1}&#124;s_{t},a_{t})p(r_{t}&#124;s_{t},a_{t})\pi(a_{t}&#124;s_{t})$
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(\tau)=p(s_{0})\prod_{t=0}^{T}p(s_{t+1}|s_{t},a_{t})p(r_{t}|s_{t},a_{t})\pi(a_{t}|s_{t})$
    |  | (2) |'
- en: Upon observation of a state $s_{t}$ in Figure [3](#S2.F3 "Figure 3 ‣ II-A2 DAGs
    for (Partially Observed ) Markov Decision Process ‣ II-A Basics about Reinforcement
    Learning with graphical model ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning"), the action at the time step in question
    is conditionally independent with the state and action history $\mathcal{E}_{t}=\{S_{0},A^{act}_{0},\cdots,S_{t-1}\}$,
    which could be denoted as $(A^{act}_{t}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}\mathcal{E}_{t})\mid
    S_{t}$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [3](#S2.F3 "图 3 ‣ II-A2 (部分观察的) 马尔可夫决策过程的 DAG ‣ II-A 图形模型中的强化学习基础 ‣ II 强化学习与深度强化学习
    ‣ 深度强化学习中的概率图模型与变分推断教程与调查") 中观察到状态 $s_{t}$ 时，在该时间步的动作与状态和动作历史 $\mathcal{E}_{t}=\{S_{0},A^{act}_{0},\cdots,S_{t-1}\}$
    条件独立，这可以表示为 $(A^{act}_{t}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}\mathcal{E}_{t})\mid
    S_{t}$。
- en: \includegraphics
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.7]tikz_pgm_mdp.pdf'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.7]tikz_pgm_mdp.pdf'
- en: 'Figure 3: Directed Acyclic Graph For Markov Decision Process'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 马尔可夫决策过程的有向无环图'
- en: A more realistic model, however, is the Partially Observable Markov Decision
    process [[15](#bib.bib15)], with its DAG representation shown in Figure [4](#S2.F4
    "Figure 4 ‣ II-A2 DAGs for (Partially Observed ) Markov Decision Process ‣ II-A
    Basics about Reinforcement Learning with graphical model ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), where the agent
    could only observe the state partially by observing $O_{t}$ through a non invertible
    function of the next state $S_{t+1}$ and the action $a_{t}$, as indicated the
    Figure by $p(o_{t}|s_{t+1},a_{t})$, while the distributions on other edges are
    omitted since they are the same as in Figure [3](#S2.F3 "Figure 3 ‣ II-A2 DAGs
    for (Partially Observed ) Markov Decision Process ‣ II-A Basics about Reinforcement
    Learning with graphical model ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning"). Under the graph specification of Figure
    [4](#S2.F4 "Figure 4 ‣ II-A2 DAGs for (Partially Observed ) Markov Decision Process
    ‣ II-A Basics about Reinforcement Learning with graphical model ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"), the
    observable $O_{t}$ is no longer Markov, but depends on the whole history, however,
    the latent state $S_{t}$ is still Markov. For POMDP, belief state $b_{t}$ is defined
    at time $t$, which is associated with a probability distribution $b_{t}(s_{t})$
    over the hidden state $S_{t}$, with $\sum_{\mathcal{S}}b(S_{t})=1$, where state
    $S$ takes value in latent state space $\mathcal{S}$ [[15](#bib.bib15)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更现实的模型是部分可观察的马尔可夫决策过程 [[15](#bib.bib15)]，其有向无环图（DAG）表示如图[4](#S2.F4 "图 4 ‣ II-A2
    DAGs for (Partially Observed ) Markov Decision Process ‣ II-A Basics about Reinforcement
    Learning with graphical model ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")所示，其中代理只能通过不可逆函数观察到状态的部分信息，即通过观测到的 $O_{t}$
    和下一个状态 $S_{t+1}$ 以及动作 $a_{t}$，如图中 $p(o_{t}|s_{t+1},a_{t})$ 所示，而其他边的分布被省略，因为它们与图[3](#S2.F3
    "图 3 ‣ II-A2 DAGs for (Partially Observed ) Markov Decision Process ‣ II-A Basics
    about Reinforcement Learning with graphical model ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")中的相同。在图[4](#S2.F4
    "图 4 ‣ II-A2 DAGs for (Partially Observed ) Markov Decision Process ‣ II-A Basics
    about Reinforcement Learning with graphical model ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")的图形规格下，可观察到的 $O_{t}$
    不再是马尔可夫的，而是依赖于整个历史，但潜在状态 $S_{t}$ 仍然是马尔可夫的。对于 POMDP，信念状态 $b_{t}$ 在时间 $t$ 被定义，它与潜在状态
    $S_{t}$ 上的概率分布 $b_{t}(s_{t})$ 相关，其中 $\sum_{\mathcal{S}}b(S_{t})=1$，其中状态 $S$ 取值于潜在状态空间
    $\mathcal{S}$ [[15](#bib.bib15)]。
- en: \includegraphics
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.8]tikz_pgm_pomdp.pdf'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.8]tikz_pgm_pomdp.pdf'
- en: 'Figure 4: Probabilistic Graphical Model for POMDP'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: POMDP 的概率图模型'
- en: The latent state distribution associated with belief state can be updated in
    a Bayesian way in Equation ([6](#S2.E6 "In II-A2 DAGs for (Partially Observed
    ) Markov Decision Process ‣ II-A Basics about Reinforcement Learning with graphical
    model ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and
    Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与信念状态相关的潜在状态分布可以在方程 ([6](#S2.E6 "在 II-A2 DAGs for (Partially Observed ) Markov
    Decision Process ‣ II-A Basics about Reinforcement Learning with graphical model
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) 中以贝叶斯方式更新。
- en: '|  |  | $\displaystyle b_{t+1}(s_{t+1})$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle b_{t+1}(s_{t+1})$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle p(s_{t+1}\mid o_{t},a_{t},b_{t})$ |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle p(s_{t+1}\mid o_{t},a_{t},b_{t})$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1},o_{t},a_{t},b_{t})}{p(o_{t},a_{t},b_{t})}\frac{p(s_{t+1},a_{t},b_{t})}{p(s_{t+1},a_{t},b_{t})}$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1},o_{t},a_{t},b_{t})}{p(o_{t},a_{t},b_{t})}\frac{p(s_{t+1},a_{t},b_{t})}{p(s_{t+1},a_{t},b_{t})}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid a_{t},s_{t+1},b_{t})\frac{p(s_{t+1}\mid
    a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (3) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid a_{t},s_{t+1},b_{t})\frac{p(s_{t+1}\mid
    a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (3) |'
- en: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t},s_{t+1}\mid
    a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (4) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t},s_{t+1}\mid
    a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (4) |'
- en: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t+1}\mid
    s_{t},a_{t},b_{t})p(s_{t}\mid a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (5)
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t+1}\mid
    s_{t},a_{t},b_{t})p(s_{t}\mid a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (5)
    |'
- en: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t+1}\mid
    s_{t},a_{t})p(s_{t}\mid a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (6) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t+1}\mid
    s_{t},a_{t})p(s_{t}\mid a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (6) |'
- en: II-B Value Function, Bellman Equation, Policy Iteration
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 值函数、贝尔曼方程、策略迭代
- en: Define state value function of state $s\in\mathcal{S}$ in Equation ([7](#S2.E7
    "In II-B Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), where
    the corresponding Bellman Equation is derived in Equation ([8](#S2.E8 "In II-B
    Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 ([7](#S2.E7 "在 II-B 值函数、贝尔曼方程、策略迭代 ‣ II 强化学习与深度强化学习 ‣ 基于概率图模型和变分推断的深度强化学习教程与调查"))
    中定义状态 $s\in\mathcal{S}$ 的状态值函数，其中对应的贝尔曼方程在方程 ([8](#S2.E8 "在 II-B 值函数、贝尔曼方程、策略迭代
    ‣ II 强化学习与深度强化学习 ‣ 基于概率图模型和变分推断的深度强化学习教程与调查")) 中推导。
- en: '|  |  | $\displaystyle V^{\pi}(s)=E_{\pi,\varepsilon}[\sum_{i=0}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})\mid\forall
    S_{t}=s]$ |  | (7) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle V^{\pi}(s)=E_{\pi,\varepsilon}[\sum_{i=0}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})\mid\forall
    S_{t}=s]$ |  | (7) |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma\sum_{i=1}^{\infty}\gamma^{(i-1)}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma\sum_{i=1}^{\infty}\gamma^{(i-1)}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma\sum_{i^{{}^{\prime}}=0}^{\infty}\gamma^{i^{{}^{\prime}}}R_{t+1+i^{{}^{\prime}}}(S_{t+1+i^{{}^{\prime}}},A^{act}_{t+1+i^{{}^{\prime}}})]$
    |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma\sum_{i^{{}^{\prime}}=0}^{\infty}\gamma^{i^{{}^{\prime}}}R_{t+1+i^{{}^{\prime}}}(S_{t+1+i^{{}^{\prime}}},A^{act}_{t+1+i^{{}^{\prime}}})]$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma
    V^{\pi}(S_{t+1})]$ |  | (8) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma
    V^{\pi}(S_{t+1})]$ |  | (8) |'
- en: where $S_{t+i}\sim p(s_{t+i+1}|s_{t+i},a_{t+i})$ takes value from $\mathcal{S}$,
    $A^{act}_{t+i}\sim\pi(a|S_{t+i+1})$ taking value from $\mathcal{A}$, and we have
    used the $\pi$ and $\varepsilon$ in the subscript of the expectation $E$ operation
    to represent the probability distribution of the policy and the environment (including
    transition probability and reward probability) respectively. State action value
    function [[12](#bib.bib12)] is defined in Equation ([9](#S2.E9 "In II-B Value
    Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")), where in Equation
    ([10](#S2.E10 "In II-B Value Function, Bellman Equation, Policy Iteration ‣ II
    Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on
    Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), its relationship to the state value function is stated.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $S_{t+i}\sim p(s_{t+i+1}|s_{t+i},a_{t+i})$ 取自 $\mathcal{S}$，$A^{act}_{t+i}\sim\pi(a|S_{t+i+1})$
    取自 $\mathcal{A}$ 的情况下，我们在期望值 $E$ 操作的下标中使用了 $\pi$ 和 $\varepsilon$ 来表示策略和环境（包括转移概率和奖励概率）的概率分布。状态动作值函数
    [[12](#bib.bib12)] 在方程 ([9](#S2.E9 "在 II-B 值函数、贝尔曼方程、策略迭代 ‣ II 强化学习与深度强化学习 ‣ 基于概率图模型和变分推断的深度强化学习教程与调查"))
    中定义，方程 ([10](#S2.E10 "在 II-B 值函数、贝尔曼方程、策略迭代 ‣ II 强化学习与深度强化学习 ‣ 基于概率图模型和变分推断的深度强化学习教程与调查"))
    中陈述了它与状态值函数的关系。
- en: '|  |  | $\displaystyle Q^{\pi}(s,a)\,(\forall S_{t}=s,A^{act}_{t}=a)$ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle Q^{\pi}(s,a)\,(\forall S_{t}=s,A^{act}_{t}=a)$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t}=s,A^{act}_{t}=a)+\sum_{i=1}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  | (9) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t}=s,A^{act}_{t}=a)+\sum_{i=1}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  | (9) |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t}=s,A^{act}_{t}=a)+\gamma
    V^{\pi}(S_{t+1})]$ |  | (10) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t}=s,A^{act}_{t}=a)+\gamma
    V^{\pi}(S_{t+1})]$ |  | (10) |'
- en: Combining Equation ([8](#S2.E8 "In II-B Value Function, Bellman Equation, Policy
    Iteration ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")) and Equation ([9](#S2.E9 "In II-B Value Function, Bellman
    Equation, Policy Iteration ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")), we have
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 结合方程 ([8](#S2.E8 "在 II-B 价值函数，贝尔曼方程，策略迭代 ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中概率图模型与变分推断的教程与综述"))
    和方程 ([9](#S2.E9 "在 II-B 价值函数，贝尔曼方程，策略迭代 ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中概率图模型与变分推断的教程与综述"))，我们得到
- en: '|  | $V(s)=\sum_{a}\pi(a&#124;s)Q(s,a)$ |  | (11) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(s)=\sum_{a}\pi(a\mid s)Q(s,a)$ |  | (11) |'
- en: Define optimal policy [[12](#bib.bib12)] to be
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 定义最优策略 [[12](#bib.bib12)] 为
- en: '|  | $\displaystyle\pi^{*}$ | $\displaystyle=\underset{\pi}{arg\,max}\,V^{\pi}(s),\forall
    s\in S$ |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi^{*}$ | $\displaystyle=\underset{\pi}{arg\,max}\,V^{\pi}(s),\forall
    s\in S$ |  |'
- en: '|  |  | $\displaystyle=\underset{\pi}{arg\,max}\,E_{\pi}[R_{t}+\gamma V^{\pi}(S_{t+1})]$
    |  | (12) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\underset{\pi}{arg\,max}\,E_{\pi}[R_{t}+\gamma V^{\pi}(S_{t+1})]$
    |  | (12) |'
- en: Taking the optimal policy $\pi^{*}$ into the Bellman Equation in Equation ([8](#S2.E8
    "In II-B Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), we
    have
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将最优策略 $\pi^{*}$ 代入方程 ([8](#S2.E8 "在 II-B 价值函数，贝尔曼方程，策略迭代 ‣ II 强化学习与深度强化学习 ‣
    关于深度强化学习中概率图模型与变分推断的教程与综述")) 中，我们得到
- en: '|  | $\displaystyle V^{\pi^{*}}(s)=E_{\pi^{*},\varepsilon}\left[R_{t}(s,A^{act}_{t})+\gamma
    V^{\pi^{*}}(S_{t+1})\right]$ |  | (13) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{\pi^{*}}(s)=E_{\pi^{*},\varepsilon}\left[R_{t}(s,A^{act}_{t})+\gamma
    V^{\pi^{*}}(S_{t+1})\right]$ |  | (13) |'
- en: Taking the optimal policy $\pi^{*}$ into Equation ([9](#S2.E9 "In II-B Value
    Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")), we have
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将最优策略 $\pi^{*}$ 代入方程 ([9](#S2.E9 "在 II-B 价值函数，贝尔曼方程，策略迭代 ‣ II 强化学习与深度强化学习 ‣
    关于深度强化学习中概率图模型与变分推断的教程与综述")) 中，我们得到
- en: '|  | $\displaystyle Q^{\pi^{*}}(s,a)=E_{\pi^{*},\varepsilon}[R_{t}(s,a)+\sum_{i=1}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  | (14) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi^{*}}(s,a)=E_{\pi^{*},\varepsilon}[R_{t}(s,a)+\sum_{i=1}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  | (14) |'
- en: Based on Equation ([14](#S2.E14 "In II-B Value Function, Bellman Equation, Policy
    Iteration ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")) and Equation ([13](#S2.E13 "In II-B Value Function,
    Bellman Equation, Policy Iteration ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")), we get
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基于方程 ([14](#S2.E14 "在 II-B 价值函数，贝尔曼方程，策略迭代 ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中概率图模型与变分推断的教程与综述"))
    和方程 ([13](#S2.E13 "在 II-B 价值函数，贝尔曼方程，策略迭代 ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中概率图模型与变分推断的教程与综述"))，我们得到
- en: '|  | $\displaystyle V^{\pi^{*}}(s)=\underset{a}{max}\,Q^{\pi^{*}}(s,a)$ |  |
    (15) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{\pi^{*}}(s)=\underset{a}{max}\,Q^{\pi^{*}}(s,a)$ |  |
    (15) |'
- en: and
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\displaystyle Q^{\pi^{*}}(s,a)=E_{\varepsilon,\pi^{*}}\left[R_{t}(s,a)+\gamma\underset{\bar{a}}{max}\,Q^{\pi^{*}}(S_{t+1},\bar{a})\right]$
    |  | (16) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi^{*}}(s,a)=E_{\varepsilon,\pi^{*}}\left[R_{t}(s,a)+\gamma\underset{\bar{a}}{max}\,Q^{\pi^{*}}(S_{t+1},\bar{a})\right]$
    |  | (16) |'
- en: For learning the optimal policy and value function, General Policy Iteration
    [[12](#bib.bib12)] can be conducted, as shown in Figure [5](#S2.F5 "Figure 5 ‣
    II-B Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), where a contracting
    process [[12](#bib.bib12)] is drawn. Starting from initial policy $\pi_{0}$, the
    corresponding value function $V^{\pi_{0}}$ could be estimated, which could result
    in improved policy $\pi_{1}$ by greedy maximization over actions. The contracting
    process is supposed to converge to the optimal policy $\pi^{*}$.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习最优策略和值函数，可以进行一般策略迭代 [[12](#bib.bib12)]，如图 [5](#S2.F5 "图 5 ‣ II-B 值函数，贝尔曼方程，策略迭代
    ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程与调研") 所示，其中绘制了一个收敛过程 [[12](#bib.bib12)]。从初始策略
    $\pi_{0}$ 开始，可以估计相应的值函数 $V^{\pi_{0}}$，这可以通过对动作的贪婪最大化得到改进后的策略 $\pi_{1}$。收敛过程应当收敛到最优策略
    $\pi^{*}$。
- en: As theoretically fundamentals of learning algorithms, Dynamic programming and
    Monte Carlo learning serve as two extremities of complete knowledge of environment
    and complete model free [[12](#bib.bib12)], while time difference learning [[12](#bib.bib12)]
    is more ubiquitously used, like a bridge connecting the two extremities. Time
    difference learning is based on the Bellman update error $\delta_{t}=Q(s_{t},a_{t})-\left(R_{t}(s,a)+\gamma\underset{a}{max}\,Q(s_{t+1},a)\right)$.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 作为学习算法的理论基础，动态规划和蒙特卡罗学习作为环境完全知识和完全无模型的两个极端 [[12](#bib.bib12)]，而时间差分学习 [[12](#bib.bib12)]
    则更为普遍使用，像一个连接这两个极端的桥梁。时间差分学习基于贝尔曼更新误差 $\delta_{t}=Q(s_{t},a_{t})-\left(R_{t}(s,a)+\gamma\underset{a}{max}\,Q(s_{t+1},a)\right)$。
- en: \includegraphics
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.6]tikz_gpi.pdf'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.6]tikz_gpi.pdf'
- en: 'Figure 5: General Policy Iteration'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：一般策略迭代
- en: II-C Policy Gradient and Actor Critic
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 策略梯度与 Actor Critic
- en: Reinforcement Learning could be viewed as a functional optimization process.
    We could define an objective function over a policy $\pi_{\theta}(a|s)$, as a
    functional, characterized by parameter $\theta$, which could correspond to the
    neural network weights, for example.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以被视为一个函数优化过程。我们可以定义一个以策略 $\pi_{\theta}(a|s)$ 为基础的目标函数，这个函数由参数 $\theta$
    表征，$\theta$ 可以对应于神经网络权重，例如。
- en: Suppose all episodes start from an auxiliary initial state $s_{0}$, which with
    probability $h(s)$, jumps to different state $s\in\mathcal{S}$ without reward.
    $h(s)$ characterizes the initial state distribution which only depends on the
    environment. Let $\eta(s)$ represent the expected number of steps spent on state
    $s$, which can be calculated by summing up the $\gamma$ discounted probability
    $P^{\pi}(s_{0}\rightarrow s,k+1)$ of entering state $s$ with $k+1$ steps from
    auxiliary state $s_{0}$, as stated in Equation ([17](#S2.E17 "In II-C Policy Gradient
    and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣
    Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")), which can be thought of as the expectation
    of $\gamma^{k}$ conditional on state $s$.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有的 episodes 从一个辅助初始状态 $s_{0}$ 开始，状态 $s_{0}$ 以概率 $h(s)$ 跳转到不同的状态 $s\in\mathcal{S}$，且没有奖励。$h(s)$
    表征了初始状态分布，这仅依赖于环境。令 $\eta(s)$ 表示在状态 $s$ 上花费的预期步数，这可以通过将从辅助状态 $s_{0}$ 出发的进入状态 $s$
    的 $\gamma$ 折现概率 $P^{\pi}(s_{0}\rightarrow s,k+1)$ 相加来计算，如方程 ([17](#S2.E17 "在 II-C
    策略梯度与 Actor Critic ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程与调研")) 中所述，这可以被视为条件在状态
    $s$ 下的 $\gamma^{k}$ 的期望。
- en: '|  | $\displaystyle\eta(s)$ | $\displaystyle=\sum_{k=0}\gamma^{k}P^{\pi}(s_{0}\rightarrow
    s,k+1)$ |  | (17) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\eta(s)$ | $\displaystyle=\sum_{k=0}\gamma^{k}P^{\pi}(s_{0}\rightarrow
    s,k+1)$ |  | (17) |'
- en: '|  |  | $\displaystyle=h(s)+\sum_{\bar{s},a}\gamma\eta(\bar{s})\pi_{\theta}(a&#124;\bar{s})P(s&#124;\bar{s},a)$
    |  | (18) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=h(s)+\sum_{\bar{s},a}\gamma\eta(\bar{s})\pi_{\theta}(a&#124;\bar{s})P(s&#124;\bar{s},a)$
    |  | (18) |'
- en: In Equation ([18](#S2.E18 "In II-C Policy Gradient and Actor Critic ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), the
    quantity is calculated by either directly starting from state $s$, which correspond
    to $k=0$ in Equation ([17](#S2.E17 "In II-C Policy Gradient and Actor Critic ‣
    II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), or entering state $s$ from state $\bar{s}$ with one step, corresponding
    to $k+1\geq 2$ in Equation ([17](#S2.E17 "In II-C Policy Gradient and Actor Critic
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 ([18](#S2.E18 "在 II-C 策略梯度和演员评论家 ‣ II 强化学习和深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程和调查"))
    中，该量可以通过直接从状态 $s$ 开始计算，这对应于方程 ([17](#S2.E17 "在 II-C 策略梯度和演员评论家 ‣ II 强化学习和深度强化学习
    ‣ 关于深度强化学习中的概率图模型和变分推断的教程和调查")) 中的 $k=0$，或者通过一步从状态 $\bar{s}$ 进入状态 $s$，对应于方程 ([17](#S2.E17
    "在 II-C 策略梯度和演员评论家 ‣ II 强化学习和深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程和调查")) 中的 $k+1\geq
    2$。
- en: For an arbitrary state $s\in\mathcal{S}$, using $s^{{}^{\prime}}$ and $s^{{}^{\prime\prime}}$
    to represent subsequent states as dummy index,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意状态 $s\in\mathcal{S}$，使用 $s^{{}^{\prime}}$ 和 $s^{{}^{\prime\prime}}$ 表示后续状态作为虚拟索引，
- en: '|  |  | $\displaystyle\nabla_{\theta}V^{\pi(\theta)}(s)=\nabla_{\theta}\left[\sum_{a}Q^{\pi(\theta)}(s,a)\pi_{\theta}(a&#124;s)\right]$
    |  | (19) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\nabla_{\theta}V^{\pi(\theta)}(s)=\nabla_{\theta}\left[\sum_{a}Q^{\pi(\theta)}(s,a)\pi_{\theta}(a&#124;s)\right]$
    |  | (19) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\left[\nabla_{\theta}Q^{\pi(\theta)}(s,a)\pi_{\theta}(a&#124;s)+\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)\right]$
    |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\left[\nabla_{\theta}Q^{\pi(\theta)}(s,a)\pi_{\theta}(a&#124;s)+\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)\right]$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\nabla_{\theta}\left[\sum_{s^{{}^{\prime}},R}P(s^{{}^{\prime}},R&#124;s,a)\left(R+\gamma
    V^{\pi(\theta)}(s^{{}^{\prime}})\right)\right]\pi_{\theta}(a&#124;s)$ |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\nabla_{\theta}\left[\sum_{s^{{}^{\prime}},R}P(s^{{}^{\prime}},R&#124;s,a)\left(R+\gamma
    V^{\pi(\theta)}(s^{{}^{\prime}})\right)\right]\pi_{\theta}(a&#124;s)$ |  |'
- en: '|  |  | $\displaystyle+\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (20) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (20) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\sum_{s^{{}^{\prime}}}\gamma
    P(s^{{}^{\prime}}&#124;s,a)\nabla_{\theta}V^{\pi(\theta)}(s^{{}^{\prime}})\pi_{\theta}(a&#124;s)+$
    |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\sum_{s^{{}^{\prime}}}\gamma
    P(s^{{}^{\prime}}&#124;s,a)\nabla_{\theta}V^{\pi(\theta)}(s^{{}^{\prime}})\pi_{\theta}(a&#124;s)+$
    |  |'
- en: '|  |  | $\displaystyle\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (21) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (21) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)+\sum_{a}\sum_{s^{{}^{\prime}}}\gamma
    P(s^{{}^{\prime}}&#124;s,a)\pi_{\theta}(a&#124;s)$ |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)+\sum_{a}\sum_{s^{{}^{\prime}}}\gamma
    P(s^{{}^{\prime}}&#124;s,a)\pi_{\theta}(a&#124;s)$ |  |'
- en: '|  |  | $\displaystyle\left[\sum_{a^{{}^{\prime}}}\sum_{s^{{}^{\prime\prime}}}\gamma
    P(s^{{}^{\prime\prime}}&#124;s^{{}^{\prime}},a^{{}^{\prime}})\nabla_{\theta}V^{\pi(\theta)}(s^{{}^{\prime\prime}})\pi_{\theta}(a^{{}^{\prime}}&#124;s^{{}^{\prime}})+\right.$
    |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\left[\sum_{a^{{}^{\prime}}}\sum_{s^{{}^{\prime\prime}}}\gamma
    P(s^{{}^{\prime\prime}}&#124;s^{{}^{\prime}},a^{{}^{\prime}})\nabla_{\theta}V^{\pi(\theta)}(s^{{}^{\prime\prime}})\pi_{\theta}(a^{{}^{\prime}}&#124;s^{{}^{\prime}})+\right.$
    |  |'
- en: '|  |  | $\displaystyle\left.\sum_{a^{{}^{\prime}}}\nabla_{\theta}\pi_{\theta}(a^{{}^{\prime}}&#124;s^{{}^{\prime}})Q^{\pi(\theta)}(s^{{}^{\prime}},a^{{}^{\prime}})\right]$
    |  | (22) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\left.\sum_{a^{{}^{\prime}}}\nabla_{\theta}\pi_{\theta}(a^{{}^{\prime}}&#124;s^{{}^{\prime}})Q^{\pi(\theta)}(s^{{}^{\prime}},a^{{}^{\prime}})\right]$
    |  | (22) |'
- en: the terms in square brackets in Equation ([22](#S2.E22 "In II-C Policy Gradient
    and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣
    Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")) are simply Equation ([21](#S2.E21 "In II-C Policy
    Gradient and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")) with $a$ and $s^{{}^{\prime}}$ replaced by $a^{{}^{\prime}}$
    and $s^{{}^{\prime\prime}}$. Since $\nabla_{\theta}V^{\pi(\theta)}(s^{\infty})=0$,
    Equation ([22](#S2.E22 "In II-C Policy Gradient and Actor Critic ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")) could
    be written as Equation ([23](#S2.E23 "In II-C Policy Gradient and Actor Critic
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), where $s_{k}$ represent the state of $k$ steps after $s$ and $P^{\pi}(s\rightarrow
    s_{k},k)$ already includes integration of intermediate state $s_{k-1},\ldots s_{1}$
    before reaching state $s_{k}$.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 ([22](#S2.E22 "在 II-C 策略梯度和演员评论 ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程与调查"))
    中的方括号中的项实际上是方程 ([21](#S2.E21 "在 II-C 策略梯度和演员评论 ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程与调查"))
    将$a$和$s^{{}^{\prime}}$替换为$a^{{}^{\prime}}$和$s^{{}^{\prime\prime}}$。由于 $\nabla_{\theta}V^{\pi(\theta)}(s^{\infty})=0$，方程
    ([22](#S2.E22 "在 II-C 策略梯度和演员评论 ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程与调查"))
    可以写作方程 ([23](#S2.E23 "在 II-C 策略梯度和演员评论 ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程与调查"))，其中$s_{k}$
    表示在$s$之后 $k$ 步的状态，而$P^{\pi}(s\rightarrow s_{k},k)$ 已经包括了在到达状态$s_{k}$之前中间状态 $s_{k-1},\ldots
    s_{1}$ 的积分。
- en: '|  | $\displaystyle\nabla_{\theta}V^{\pi(\theta)}(s)=\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)+$
    |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}V^{\pi(\theta)}(s)=\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)+$
    |  |'
- en: '|  | $\displaystyle\sum_{k=1}\sum_{s_{k}}\sum_{a_{k}}\gamma^{k}P^{\pi}(s\rightarrow
    s_{k},k)\nabla_{\theta}\pi_{\theta}(a_{k}&#124;s_{k})Q^{\pi(\theta)}(s_{k},a_{k})$
    |  | (23) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{k=1}\sum_{s_{k}}\sum_{a_{k}}\gamma^{k}P^{\pi}(s\rightarrow
    s_{k},k)\nabla_{\theta}\pi_{\theta}(a_{k}&#124;s_{k})Q^{\pi(\theta)}(s_{k},a_{k})$
    |  | (23) |'
- en: Let objective function with respect to policy be defined to be the value function
    starting from auxiliary state $s_{0}$ as in Equation ([24](#S2.E24 "In II-C Policy
    Gradient and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数关于策略的定义为从辅助状态$s_{0}$开始的值函数，如方程 ([24](#S2.E24 "在 II-C 策略梯度和演员评论 ‣ II 强化学习与深度强化学习
    ‣ 关于深度强化学习中的概率图模型和变分推断的教程与调查")) 所示。
- en: '|  | $\displaystyle J(\pi_{\theta})=V^{\pi}(s_{0})=E_{\pi,\varepsilon}\sum_{t=0}^{\infty}\gamma^{t}R_{t}(S_{0}=s)$
    |  | (24) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J(\pi_{\theta})=V^{\pi}(s_{0})=E_{\pi,\varepsilon}\sum_{t=0}^{\infty}\gamma^{t}R_{t}(S_{0}=s)$
    |  | (24) |'
- en: The optimal policy could be obtained by gradient accent optimization, leading
    to the policy gradient algorithm [[12](#bib.bib12)], as in Equation ([29](#S2.E29
    "In II-C Policy Gradient and Actor Critic ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略可以通过梯度上升优化获得，从而得到策略梯度算法 [[12](#bib.bib12)]，如方程 ([29](#S2.E29 "在 II-C 策略梯度和演员评论
    ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中的概率图模型和变分推断的教程与调查")) 所示。
- en: '|  |  | $\displaystyle\nabla_{\theta}J(\pi_{\theta})=\nabla_{\theta}V^{\pi}(s_{0})$
    |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\nabla_{\theta}J(\pi_{\theta})=\nabla_{\theta}V^{\pi}(s_{0})$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{k=0}\sum_{s_{k}}\sum_{a_{k}}\gamma^{k}P^{\pi}(s_{0}\rightarrow
    s_{k},k)\nabla_{\theta}\pi_{\theta}(a_{k}&#124;s_{k})Q^{\pi(\theta)}(s_{k},a_{k})$
    |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{k=0}\sum_{s_{k}}\sum_{a_{k}}\gamma^{k}P^{\pi}(s_{0}\rightarrow
    s_{k},k)\nabla_{\theta}\pi_{\theta}(a_{k}&#124;s_{k})Q^{\pi(\theta)}(s_{k},a_{k})$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{s}\sum_{a}\eta(s)\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (25) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{s}\sum_{a}\eta(s)\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (25) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{\sum_{s}\eta(s)}{\sum_{s}\eta(s)}\sum_{s}\sum_{a}\eta(s)\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (26) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{\sum_{s}\eta(s)}{\sum_{s}\eta(s)}\sum_{s}\sum_{a}\eta(s)\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi(\theta)}(s,a)$
    |  | (26) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{\bar{s}}\eta(\bar{s})\sum_{s}\sum_{a}\mu(s)\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (27) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{\bar{s}}\eta(\bar{s})\sum_{s}\sum_{a}\mu(s)\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi(\theta)}(s,a)$
    |  | (27) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{\bar{s}}\eta(\bar{s})\sum_{s}\sum_{a}\mu(s)\frac{\pi_{\theta}(a&#124;s)}{\pi_{\theta}(a&#124;s)}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (28) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{\bar{s}}\eta(\bar{s})\sum_{s}\sum_{a}\mu(s)\frac{\pi_{\theta}(a|s)}{\pi_{\theta}(a|s)}\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi(\theta)}(s,a)$
    |  | (28) |'
- en: '|  | $\displaystyle\propto$ | $\displaystyle\,E_{\pi}\left[\frac{\nabla_{\theta}\pi_{\theta}(A&#124;S)}{\pi_{\theta}(A&#124;S)}\hat{Q}^{\pi(\theta)}(S,A)\right]$
    |  | (29) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\propto$ | $\displaystyle\,E_{\pi}\left[\frac{\nabla_{\theta}\pi_{\theta}(A|S)}{\pi_{\theta}(A|S)}\hat{Q}^{\pi(\theta)}(S,A)\right]$
    |  | (29) |'
- en: The policy gradient could be augmented to include zero gradient baseline $b(s)$,
    with respect to objective function $J(\pi_{\theta})$ in Equation ([28](#S2.E28
    "In II-C Policy Gradient and Actor Critic ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")), as a function of
    state $s$, which does not include parameters for policy $\theta$, since $\sum_{a}\nabla_{\theta}\pi_{\theta}(a|s)=0$.
    To reduce variance of the gradient, the baseline is usually chosen to be the state
    value function estimator $\hat{V}_{w}(s)$ to smooth out the variation of $Q(s,a)$
    at each state, while $\hat{V}_{w}(s)$ is updated in a Monte Carlo way by comparing
    with $\hat{Q}^{\pi_{\theta}}(S,A)=G_{t}$.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度可以增强为包含零梯度基准 $b(s)$，关于目标函数 $J(\pi_{\theta})$ 的方程 ([28](#S2.E28 "在 II-C 策略梯度与演员评论员
    ‣ II 强化学习与深度强化学习 ‣ 关于深度强化学习中概率图模型和变分推断的教程与调查"))，作为状态 $s$ 的函数，不包含策略 $\theta$ 的参数，因为
    $\sum_{a}\nabla_{\theta}\pi_{\theta}(a|s)=0$。为了减少梯度的方差，基准通常选择为状态值函数估计器 $\hat{V}_{w}(s)$，以平滑每个状态下
    $Q(s,a)$ 的变化，同时 $\hat{V}_{w}(s)$ 通过与 $\hat{Q}^{\pi_{\theta}}(S,A)=G_{t}$ 比较的蒙特卡洛方式进行更新。
- en: The actor-critic algorithm [[12](#bib.bib12)] decomposes $G_{t}-V_{w}(s_{t})$
    to be $R_{t}+\gamma V_{w}(s_{t+1})-V_{w}(s_{t})$, so bootstrap is used instead
    of Monte Carlo.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论员算法 [[12](#bib.bib12)] 将 $G_{t}-V_{w}(s_{t})$ 分解为 $R_{t}+\gamma V_{w}(s_{t+1})-V_{w}(s_{t})$，因此使用引导方法代替了蒙特卡洛方法。
- en: II-D Basics of Deep Reinforcement Learning
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 深度强化学习基础
- en: 'Deep Q learning [[1](#bib.bib1)] makes a breakthrough in using neural network
    as the functional approximator for value function on complicated tasks. It solves
    the transition correlation problem by random sampling from a replay memory. Specifically,
    the reinforcement learning is transformed in a supervised learning task by fitting
    on the target $R_{t}+\gamma\underset{a}{max}\,Q(s_{t+1},a)$ from the replay memory
    with state $s_{t}$ as input. However, the target can get drifted easily which
    leads to unstable learning. In [[1](#bib.bib1)], a target network is used to provide
    a stable target for the updating network to be learned before getting updated
    occasionally. Double Deep Q learning [[16](#bib.bib16)], however, solves the problem
    by having two Q network and update the parameters in a alternating way. We review
    some state of art deep reinforcement learning algorithms from different aspects:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习 [[1](#bib.bib1)] 在将神经网络用作复杂任务中值函数的功能逼近器方面取得了突破。它通过从重放记忆中随机抽样来解决转移相关问题。具体而言，强化学习被转化为一个监督学习任务，通过在目标
    $R_{t}+\gamma\underset{a}{max}\,Q(s_{t+1},a)$ 上进行拟合，输入为状态 $s_{t}$ 的重放记忆。然而，目标可能会很容易发生漂移，导致学习不稳定。在
    [[1](#bib.bib1)] 中，使用目标网络为更新网络提供稳定的目标，以便在偶尔更新之前进行学习。然而，双深度 Q 学习 [[16](#bib.bib16)]
    通过拥有两个 Q 网络并以交替方式更新参数来解决这个问题。我们从不同方面回顾一些最先进的深度强化学习算法：
- en: II-D1 Off Policy methods
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D1 离策略方法
- en: Except for Deep Q Learning [[1](#bib.bib1)] mentioned above, DDPG [[17](#bib.bib17)]
    extends Deterministic Policy Gradient (DPG) [[18](#bib.bib18)] with deep neural
    network functional approximator, which is an actor-critic algorithm and works
    well in continuous action spaces.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述提到的深度 Q 学习 [[1](#bib.bib1)]，DDPG [[17](#bib.bib17)] 使用深度神经网络功能逼近器扩展了确定性策略梯度（DPG）
    [[18](#bib.bib18)]，它是一种演员-评论员算法，在连续动作空间中表现良好。
- en: II-D2 On Policy methods
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D2 在策略方法
- en: A3C [[19](#bib.bib19)] stands out in the asynchronous methods in deep learning
    [[19](#bib.bib19)] which can be run in parallel on a single multi-core CPU. Trust
    Region Policy Optimization [[2](#bib.bib2)] and Proximal Policy Optimization [[20](#bib.bib20)]
    assimilates the natural policy gradient, which use a local approximation to the
    expected return. The local approximation could serve as a lower bound for the
    expected return, which can be optimized safely subject to the KL divergence constraint
    between two subsequent policies, while in practice, the constraint is relaxed
    to be a regularization.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: A3C [[19](#bib.bib19)] 在深度学习中的异步方法[[19](#bib.bib19)]中脱颖而出，可以在单个多核 CPU 上并行运行。信任区域策略优化[[2](#bib.bib2)]和近端策略优化[[20](#bib.bib20)]
    采纳了自然策略梯度，它们使用对预期回报的局部近似。局部近似可以作为预期回报的下界，可以在 KL 散度约束下安全地优化两个后续策略之间的差异，而在实际操作中，约束被放宽为正则化。
- en: II-D3 Goal based Reinforcement Learning
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D3 基于目标的强化学习
- en: In robot manipulation tasks, the goal could be represented with state in some
    cases [[14](#bib.bib14)]. Universal Value Function Approximator (UVFA) [[21](#bib.bib21)]
    incorporate the goal into the deep neural network, which let the neural network
    functional approximator also generalize to goal changes in tasks, similar to Recommendation
    System [[22](#bib.bib22)]. Work of this direction include [[23](#bib.bib23), [14](#bib.bib14)],
    for example.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人操控任务中，目标在某些情况下可以通过状态来表示[[14](#bib.bib14)]。通用价值函数逼近器（UVFA）[[21](#bib.bib21)]
    将目标纳入深度神经网络，使神经网络功能逼近器也能对任务中的目标变化进行泛化，类似于推荐系统[[22](#bib.bib22)]。这一方向的工作包括[[23](#bib.bib23),
    [14](#bib.bib14)]，例如。
- en: II-D4 Replay Memory Manipulation based Method
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D4 回放记忆操控方法
- en: Replay memory is a critical component in Deep Reinforcement Learning, which
    solves the problem of correlated transition in one episode. Beyond the uniform
    sampling of replay memory in Deep Q Network [[1](#bib.bib1)], Prioritized Experience
    Replay [[24](#bib.bib24)] improves the performance by giving priority to those
    transitions with bigger TD error, while Hindsight Experience Replay (HER) [[23](#bib.bib23)]
    manipulate the replay memory with changing goals to transition so as to change
    reward to promote exploration. Maximum entropy regularized multi goal reinforcement
    learning [[14](#bib.bib14)] gives priority to those rarely occurred trajectory
    in sampling, which has been shown to improve over HER [[14](#bib.bib14)].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 回放记忆是深度强化学习中的关键组件，它解决了单次实验中的相关转移问题。除了深度 Q 网络中的均匀采样[[1](#bib.bib1)]，优先经验回放[[24](#bib.bib24)]通过对那些具有更大
    TD 错误的转移给予优先级来提升性能，而事后经验回放（HER）[[23](#bib.bib23)]则通过改变目标来操控回放记忆，从而改变奖励以促进探索。最大熵正则化多目标强化学习[[14](#bib.bib14)]在采样时优先考虑那些很少发生的轨迹，已被证明优于
    HER [[14](#bib.bib14)]。
- en: II-D5 Surrogate policy optimization
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D5 替代策略优化
- en: Like surrogate model used in Bayesian Optimization [[25](#bib.bib25)], lower
    bound surrogate is also used in Reinforcement Learning. Trust Region Policy Optimization
    (TRPO) [[2](#bib.bib2)] is built on the identity from [[26](#bib.bib26)] in Equation
    ([30](#S2.E30 "In II-D5 Surrogate policy optimization ‣ II-D Basics of Deep Reinforcement
    Learning ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")), where $\eta_{\pi^{new}}(s)$ means the state visitation
    frequency under policy $\pi^{new}$ and advantage $A^{\pi^{old}}(a_{t},s_{t})=Q^{\pi^{old}}(a_{t},s_{t})-V^{\pi^{old}}(s_{t})$.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于贝叶斯优化中使用的替代模型[[25](#bib.bib25)]，在强化学习中也使用了下界替代模型。信任区域策略优化（TRPO）[[2](#bib.bib2)]
    基于[[26](#bib.bib26)]中的恒等式，见于公式 ([30](#S2.E30 "在 II-D5 替代策略优化 ‣ II-D 深度强化学习基础 ‣
    II 强化学习与深度强化学习 ‣ 关于深度强化学习中概率图模型和变分推断的教程和综述"))，其中 $\eta_{\pi^{new}}(s)$ 表示在策略 $\pi^{new}$
    下的状态访问频率，优势 $A^{\pi^{old}}(a_{t},s_{t})=Q^{\pi^{old}}(a_{t},s_{t})-V^{\pi^{old}}(s_{t})$。
- en: '|  | $\displaystyle J(\pi^{new})$ | $\displaystyle=J(\pi^{old})+\sum_{s}\eta_{\pi^{new}}(s)\sum_{a}\pi^{new}(a&#124;s)A^{\pi^{old}}(a,s)$
    |  | (30) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J(\pi^{new})$ | $\displaystyle=J(\pi^{old})+\sum_{s}\eta_{\pi^{new}}(s)\sum_{a}\pi^{new}(a&#124;s)A^{\pi^{old}}(a,s)$
    |  | (30) |'
- en: Based on Policy Advantage [[26](#bib.bib26)] $A_{\pi^{old},\eta_{old}}(\pi^{new})=\sum_{s}\eta_{\pi^{old}}(s)\sum_{a}\pi^{new}(a|s)A^{\pi^{old}}(a,s)$,
    a local approximation $L_{\pi^{old}}(\pi^{new})$ to Equation ([30](#S2.E30 "In
    II-D5 Surrogate policy optimization ‣ II-D Basics of Deep Reinforcement Learning
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) can be defined in Equation ([31](#S2.E31 "In II-D5 Surrogate policy
    optimization ‣ II-D Basics of Deep Reinforcement Learning ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), based on which,
    a surrogate function $M(\pi^{new},\pi^{old})$ is defined in Equation ([32](#S2.E32
    "In II-D5 Surrogate policy optimization ‣ II-D Basics of Deep Reinforcement Learning
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) that minorizes $J(\pi^{new})$ at $\pi^{old}$, where $D_{KL}^{max}(\pi^{old},\pi^{new})=\underset{s}{\max}D_{KL}(\pi^{old}(a|s),\pi^{new}(a|s))$
    is the maximum KL divergence, so MM [[2](#bib.bib2)] algorithm could be used to
    improve the policy, leading to the trust region method [[2](#bib.bib2)].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略优势 [[26](#bib.bib26)] $A_{\pi^{old},\eta_{old}}(\pi^{new})=\sum_{s}\eta_{\pi^{old}}(s)\sum_{a}\pi^{new}(a|s)A^{\pi^{old}}(a,s)$，可以在方程
    ([30](#S2.E30 "In II-D5 Surrogate policy optimization ‣ II-D Basics of Deep Reinforcement
    Learning ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")) 中定义一个局部近似 $L_{\pi^{old}}(\pi^{new})$，基于此，一个替代函数 $M(\pi^{new},\pi^{old})$
    在方程 ([32](#S2.E32 "In II-D5 Surrogate policy optimization ‣ II-D Basics of Deep
    Reinforcement Learning ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")) 中被定义，它在 $\pi^{old}$ 处次优化 $J(\pi^{new})$，其中 $D_{KL}^{max}(\pi^{old},\pi^{new})=\underset{s}{\max}D_{KL}(\pi^{old}(a|s),\pi^{new}(a|s))$
    是最大 KL 散度，因此 MM [[2](#bib.bib2)] 算法可以用于改进策略，从而导致信任区域方法 [[2](#bib.bib2)]。
- en: '|  |  | $\displaystyle L_{\pi^{old}}(\pi^{new})$ |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle L_{\pi^{old}}(\pi^{new})$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle J(\pi^{old})+\sum_{s}\eta_{\pi^{old}}(s)\sum_{a}\pi^{new}(a&#124;s)A^{\pi^{old}}(a_{t},s_{t})$
    |  | (31) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle J(\pi^{old})+\sum_{s}\eta_{\pi^{old}}(s)\sum_{a}\pi^{new}(a&#124;s)A^{\pi^{old}}(a_{t},s_{t})$
    |  | (31) |'
- en: '|  |  | $\displaystyle M(\pi^{new},\pi^{old})$ |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle M(\pi^{new},\pi^{old})$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle L_{\pi^{old}}(\pi^{new})-\frac{4\underset{a,s}{max}&#124;A^{\pi^{old}}(s,a)&#124;\gamma}{1-\gamma^{2}}D_{KL}^{max}(\pi^{old},\pi^{new})$
    |  | (32) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle L_{\pi^{old}}(\pi^{new})-\frac{4\underset{a,s}{max}&#124;A^{\pi^{old}}(s,a)&#124;\gamma}{1-\gamma^{2}}D_{KL}^{max}(\pi^{old},\pi^{new})$
    |  | (32) |'
- en: 'TABLE I: Comparison of deep reinforcement learning methods: ”S” means state
    and ”A” means action, where ”c” means continuous, ”d” means discrete. ”standalone”
    means whether the algorithm work independently or needs to be combined with another
    learning algorithm. ”var” means which probability the variational inference is
    approximating, ”p” means whether the method is on policy or off policy. ”na” means
    not applicable'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：深度强化学习方法比较：“S”表示状态，“A”表示动作，其中“c”表示连续，“d”表示离散。“standalone”表示算法是否独立工作或需要与其他学习算法结合。“var”表示变分推断所近似的概率，“p”表示方法是否在策略上或脱策略。“na”表示不适用
- en: '| Algorithm | S | A | standalone | var | p |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Algorithm | S | A | standalone | var | p |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Deep Q | c | d | y | na | off |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Deep Q | c | d | y | na | off |'
- en: '| A3C | c | c/d | y | na | on |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| A3C | c | c/d | y | na | on |'
- en: '| TRPO/PPO | c | c/d | y | na | on |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| TRPO/PPO | c | c/d | y | na | on |'
- en: '| DDPG | c | c | y | na | off |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| DDPG | c | c | y | na | off |'
- en: '| Boltzmann | d | d | y | na | on |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Boltzmann | d | d | y | na | on |'
- en: '| VIME | c | c | n | $p_{\theta}(s_{t+1}&#124;s_{t},a_{t})$ | na |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| VIME | c | c | n | $p_{\theta}(s_{t+1}&#124;s_{t},a_{t})$ | na |'
- en: '| VAST | c | d | n | $p(s_{t}&#124;o_{t-k})$ | na |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| VAST | c | d | n | $p(s_{t}&#124;o_{t-k})$ | na |'
- en: '| SoftQ | c | c/d | y | $p(a_{t}&#124;s_{t})$ | on |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| SoftQ | c | c/d | y | $p(a_{t}&#124;s_{t})$ | on |'
- en: III Taxonomy of PGM and VI in deep reinforcement learning
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度强化学习中的概率图模型和变分推断分类
- en: Despite the success of deep reinforcement learning in many talks, the field
    still faces some critical challenges. One problem is exploration with sparse reward.
    In complicated real environment, an agent has to explore for a long trajectory
    before it can get any reward as feedback. Due to lack of enough rewards, traditional
    Reinforcement Learning methods performs poorly, which lead to a lot of recent
    contributions in the exploration methods. Another challenge is how to represent
    policy in extremely large state and action spaces. Furthermore, sometimes it is
    beneficial to have multimodal behavior for a agent when some trajectory might
    be equivalent to other trajectories and we want to learn all of them.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度强化学习在许多讨论中取得了成功，但该领域仍面临一些关键挑战。其中一个问题是稀疏奖励的探索。在复杂的真实环境中，智能体需要长时间探索才能获得任何奖励反馈。由于奖励不足，传统的强化学习方法表现不佳，这导致了许多近期对探索方法的贡献。另一个挑战是如何在极大状态和动作空间中表示策略。此外，当某些轨迹可能等同于其他轨迹时，具有多模态行为对智能体有时是有益的，我们希望学习所有这些轨迹。
- en: In this section, we give detailed explanation on how graphical model and variational
    inference could be used to model and optimize the reinforcement learning process
    under these challenges and form a taxonomy of these methods.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细解释了图形模型和变分推断如何用于建模和优化在这些挑战下的强化学习过程，并形成这些方法的分类。
- en: Together with the deep reinforcement learning methods mentioned in section [II-D](#S2.SS4
    "II-D Basics of Deep Reinforcement Learning ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning"), we make a comparison
    of them in Table [I](#S2.T1 "TABLE I ‣ II-D5 Surrogate policy optimization ‣ II-D
    Basics of Deep Reinforcement Learning ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning").
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结合在[II-D](#S2.SS4 "II-D Basics of Deep Reinforcement Learning ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")节中提到的深度强化学习方法，我们在表格[I](#S2.T1
    "TABLE I ‣ II-D5 Surrogate policy optimization ‣ II-D Basics of Deep Reinforcement
    Learning ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")中对它们进行了比较。
- en: III-A Policy and value function with undirected graphs
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 无向图中的策略和价值函数
- en: We first discuss the application of undirected graphs in deep reinforcement
    learning, which models joint distribution of variables with cliques [[7](#bib.bib7)].
    In [[27](#bib.bib27)], the authors use Restricted Boltzmann Machine (RBM) [[8](#bib.bib8)],
    which has nice property of tractable factorized posterior distribution over the
    latent variables conditioned on observed variables. To deal with MDPs of large
    state and action spaces, they model the state-action value function with the negative
    free energy of a Restricted Boltzmann Machine. Specifically, the visible states
    of the Restricted Boltzmann Machine [[27](#bib.bib27)] consists of both state
    $s$ and action $a$ binary variables, as shown in Figure [6](#S3.F6 "Figure 6 ‣
    III-A Policy and value function with undirected graphs ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), where the hidden
    nodes consist of $L$ binary variables, while state variables $s_{i}$ are dark
    colored to represent it can be observed and actions $a_{j}$ are light colored
    to represent it need to be sampled. Together with the auxiliary hidden variables,
    the undirected graph defines a joint probability distribution over state and action
    pairs, which defines a stochastic policy network that could sample actions out
    for on policy learning. Since it is pretty easy to calculate the derivative of
    the free energy $F(s,a)$ with respect to the coefficient $w_{k,j}$ of the network,
    one could use temporal difference learning to update the coefficients in the network.
    Thanks to properties of Boltzmann Machine, the conditional distribution of action
    over state $p(a|s)$, which could be used as a policy, is still Boltzmann distributed
    as in Equation ([33](#S3.E33 "In III-A Policy and value function with undirected
    graphs ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")), governed by the free energy $F(a,s)$, where $Z(s)$
    is the partition function [[7](#bib.bib7)] and the negative free energy to approximate
    the state action value function $Q(s,a)$. By adjusting the temperature $T$, one
    could also change between different exploration strength.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论无向图在深度强化学习中的应用，它通过团体建模变量的联合分布[[7](#bib.bib7)]。在[[27](#bib.bib27)]中，作者使用限制玻尔兹曼机（RBM）[[8](#bib.bib8)]，它具有在观察变量条件下，对潜在变量的可处理的分解后验分布的优良特性。为了处理大状态和动作空间的马尔可夫决策过程（MDP），他们用限制玻尔兹曼机的负自由能来建模状态-动作值函数。具体来说，限制玻尔兹曼机的可见状态[[27](#bib.bib27)]包括状态
    $s$ 和动作 $a$ 的二进制变量，如图 [6](#S3.F6 "图 6 ‣ III-A 无向图中的策略和价值函数 ‣ III 深度强化学习中概率图模型和变分推断的分类
    ‣ 深度强化学习中概率图模型和变分推断的教程与调查") 所示，其中隐藏节点由 $L$ 个二进制变量组成，而状态变量 $s_{i}$ 为深色以表示它可以被观察到，动作
    $a_{j}$ 为浅色以表示它需要被采样。与辅助隐藏变量一起，无向图定义了状态和动作对的联合概率分布，这定义了一个随机策略网络，该网络可以在策略学习中采样动作。由于计算网络中自由能
    $F(s,a)$ 对系数 $w_{k,j}$ 的导数非常简单，可以使用时间差分学习来更新网络中的系数。得益于玻尔兹曼机的特性，给定状态下的动作条件分布 $p(a|s)$，它可以作为策略使用，仍然是玻尔兹曼分布的，如方程
    ([33](#S3.E33 "在 III-A 无向图中的策略和价值函数 ‣ III 深度强化学习中概率图模型和变分推断的分类 ‣ 深度强化学习中概率图模型和变分推断的教程与调查"))
    中所示，由自由能 $F(a,s)$ 控制，其中 $Z(s)$ 是分区函数[[7](#bib.bib7)]，负自由能用于逼近状态动作值函数 $Q(s,a)$。通过调整温度
    $T$，还可以在不同的探索强度之间进行变化。
- en: \includegraphics
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.6]tikz_pgm_boltzmanmachine.pdf'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.6]tikz_pgm_boltzmanmachine.pdf'
- en: 'Figure 6: Restricted Boltzmann Machine Value and Policy'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：限制玻尔兹曼机的值与策略
- en: '|  | $\displaystyle p(a&#124;s)={1/Z(s)}e^{-F(s,a)/T}={1/Z(s)}e^{Q(s,a)/T}$
    |  | (33) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(a|s)={1/Z(s)}e^{-F(s,a)/T}={1/Z(s)}e^{Q(s,a)/T}$ |  |
    (33) |'
- en: A few steps of MCMC sampling [[7](#bib.bib7)] could be used to sample actions,
    as an approximation of the policy, which can be fed into a time difference learning
    method like SARSA [[12](#bib.bib12)], to update the state value function $Q(s,a)$’s
    estimation. Such an on-policy process has been shown to be empirically effective
    in the large state actions spaces [[27](#bib.bib27)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 几步 MCMC 采样 [[7](#bib.bib7)] 可以用来采样动作，作为策略的近似，这可以输入到如 SARSA [[12](#bib.bib12)]
    这样的时间差分学习方法中，以更新状态值函数 $Q(s,a)$ 的估计。这种策略学习过程在大状态动作空间中已被实证证明是有效的[[27](#bib.bib27)]。
- en: III-B Variational Inference on ”optimal” Policies
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 对“最优”策略的变分推断
- en: III-B1 policy as ”optimal” posterior
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 政策作为“最优”后验
- en: The Boltzmann Machine defined Product of Expert Model in [[27](#bib.bib27)]
    works well for large state and action spaces, but are limited to discrete specifically
    binary state and action variables. For continuous state and action spaces, in
    [[28](#bib.bib28)], the author proposed deep energy based models with Directed
    Acyclic Graphs (DAG) [[7](#bib.bib7)], which we re-organize in a different form
    in Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal” posterior ‣ III-B
    Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep
    reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning") with annotations added.
    The difference with respect to Figure [3](#S2.F3 "Figure 3 ‣ II-A2 DAGs for (Partially
    Observed ) Markov Decision Process ‣ II-A Basics about Reinforcement Learning
    with graphical model ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning") is that, in Figure [7](#S3.F7 "Figure 7 ‣ III-B1
    policy as ”optimal” posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), the reward is not explicit expressed in the directed graphical model.
    Instead, an auxilliary binary Observable $O$ is used to define whether the corresponding
    action at the current step is optimal or not. The conditional probability of the
    action being optimal is $p(O_{t}=1|s_{t},a_{t})=\exp(r(s_{t},a_{t}))$, which connects
    conditional optimality with the amount of award received by encouraging the agent
    to take highly rewarded actions in an exponential manner. Note that the reward
    here must be negative to ensure the validity of probability, which does not hurt
    generality since reward range can be translated [[13](#bib.bib13)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 机器定义的专家模型产品在 [[27](#bib.bib27)] 对于大状态和动作空间效果良好，但仅限于离散的特别是二进制状态和动作变量。对于连续状态和动作空间，在
    [[28](#bib.bib28)] 中，作者提出了基于深度能量的模型与有向无环图（DAG） [[7](#bib.bib7)]，我们在图[7](#S3.F7
    "Figure 7 ‣ III-B1 policy as ”optimal” posterior ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning") 中以不同形式重新组织，并添加了注释。与图[3](#S2.F3 "Figure 3 ‣ II-A2
    DAGs for (Partially Observed ) Markov Decision Process ‣ II-A Basics about Reinforcement
    Learning with graphical model ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning") 的区别在于，在图[7](#S3.F7 "Figure 7 ‣ III-B1
    policy as ”optimal” posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning") 中，奖励在有向图模型中没有明确表示。相反，使用辅助的二进制可观察量 $O$ 来定义当前步骤的相应动作是否是最优的。动作最优的条件概率为
    $p(O_{t}=1|s_{t},a_{t})=\exp(r(s_{t},a_{t}))$，这将条件最优性与奖励的数量联系起来，通过鼓励代理采取高度奖励的动作来以指数方式进行。这些奖励必须是负的，以确保概率的有效性，但这不会影响一般性，因为奖励范围可以转换
    [[13](#bib.bib13)]。
- en: 'The Graphical Model in Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal”
    posterior ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy of
    PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning") in
    total defines the trajectory likelihood or the evidence in Equation ([34](#S3.E34
    "In III-B1 policy as ”optimal” posterior ‣ III-B Variational Inference on ”optimal”
    Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的图形模型[7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal” posterior ‣ III-B Variational
    Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning") 总体上定义了轨迹似然或方程中的证据 ([34](#S3.E34 "In
    III-B1 policy as ”optimal” posterior ‣ III-B Variational Inference on ”optimal”
    Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning"))：
- en: '|  | $\displaystyle p(\tau)$ | $\displaystyle=\left[p(s_{1})\prod_{t}p(s_{t+1}&#124;s_{t},a_{t})\right]\exp\left(\sum_{t}r(s_{t},a_{t})\right)$
    |  | (34) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(\tau)$ | $\displaystyle=\left[p(s_{1})\prod_{t}p(s_{t+1}&#124;s_{t},a_{t})\right]\exp\left(\sum_{t}r(s_{t},a_{t})\right)$
    |  | (34) |'
- en: .
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: By doing so, the author is forcing a form of functional expression on top of
    the conditional independence structure of the graph by assigning a likelihood.
    In this way, calculating the optimal policy of actions distributions becomes an
    inference problem of calculating the posterior $p(a_{t}|s_{t},O_{t:T}=1)$, which
    reads as, conditional on optimality from current time step until end of episode,
    and the current current state to be $s_{t}$, the distribution of action $a_{t}$,
    and this posterior corresponds to the optimal policy. Observing the d-separation
    from Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal” posterior ‣ III-B
    Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep
    reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning"), $O_{1:{t-1}}$ is conditionally
    independent of $a_{t}$ given $s_{t}$, $(O_{1:{t-1}}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}A^{act}_{t})\mid{S_{t}}$,
    so $p(a_{t}|s_{t},O_{1:t-1},O_{t:T})=p(a_{t}|s_{t},O_{t:T})$
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，作者强迫将一种功能表达形式应用于图的条件独立结构，通过分配一个似然函数。这样，计算动作分布的最优策略就变成了一个计算后验 $p(a_{t}|s_{t},O_{t:T}=1)$
    的推断问题，该后验表明，在从当前时间步到剧集结束的最优条件下，当前状态为 $s_{t}$，动作 $a_{t}$ 的分布，这个后验对应于最优策略。从图 [7](#S3.F7
    "图 7 ‣ III-B1 策略作为”最优”后验 ‣ III-B 对”最优”策略的变分推断 ‣ III 深度强化学习中的概率图模型与变分推断分类 ‣ 关于深度强化学习中概率图模型与变分推断的教程和综述")
    中观察到的 d-分离，$O_{1:{t-1}}$ 在给定 $s_{t}$ 的情况下与 $a_{t}$ 条件独立，即 $(O_{1:{t-1}}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}A^{act}_{t})\mid{S_{t}}$，因此
    $p(a_{t}|s_{t},O_{1:t-1},O_{t:T})=p(a_{t}|s_{t},O_{t:T})$
- en: \includegraphics
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.7]tikz_pgm_soft_q.pdf'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.7]tikz_pgm_soft_q.pdf'
- en: 'Figure 7: Optimal Policy as posterior on actions: $p(a_{t}|s_{t},O_{t:T}=1)$'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：动作的最优策略作为后验概率：$p(a_{t}|s_{t},O_{t:T}=1)$
- en: III-B2 Message passing for exact inference on the posterior
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 对后验的精确推断中的消息传递
- en: In this section, we give detailed derivation on conducting exact inference on
    the policy posterior which is not given in [[13](#bib.bib13)]. Similar to the
    forward-backward message passing algorithm [[7](#bib.bib7)] in Hidden Markov Models
    [[7](#bib.bib7)], the posterior $p(a_{t}|s_{t},O_{t:T}=1)$ could also be calculated
    by passing messages. We offer a detailed derivation of the decomposition of the
    posterior $p(a_{t}|s_{t},O_{t:T}=1)$ in Equation ([35](#S3.E35 "In III-B2 Message
    passing for exact inference on the posterior ‣ III-B Variational Inference on
    ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")), which is not available in [[13](#bib.bib13)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细推导了关于策略后验的精确推断，这在 [[13](#bib.bib13)] 中没有给出。类似于隐马尔可夫模型中的前向-后向消息传递算法
    [[7](#bib.bib7)]，后验 $p(a_{t}|s_{t},O_{t:T}=1)$ 也可以通过传递消息来计算。我们提供了关于方程 ([35](#S3.E35
    "在 III-B2 中对后验的精确推断的消息传递 ‣ III-B 对”最优”策略的变分推断 ‣ III 深度强化学习中的概率图模型与变分推断分类 ‣ 关于深度强化学习中概率图模型与变分推断的教程和综述"))
    中后验 $p(a_{t}|s_{t},O_{t:T}=1)$ 的分解的详细推导，这在 [[13](#bib.bib13)] 中不可用。
- en: '|  |  | $\displaystyle p(a_{t}&#124;s_{t},O_{t:T}=1)$ |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle p(a_{t}\mid s_{t},O_{t:T}=1)$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(a_{t},s_{t},O_{t:T}=1)}{p(s_{t},O_{t:T}=1)}$
    |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(a_{t},s_{t},O_{t:T}=1)}{p(s_{t},O_{t:T}=1)}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t},s_{t})}{p(s_{t},O_{t:T}=1)}$
    |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1\mid a_{t},s_{t})p(a_{t},s_{t})}{p(s_{t},O_{t:T}=1)}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t}&#124;s_{t})p(s_{t})}{\int_{a_{t}{{}^{\prime}}}p(s_{t},a_{t}^{{}^{\prime}},O_{t:T}=1)d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1\mid a_{t},s_{t})p(a_{t}\mid
    s_{t})p(s_{t})}{\int_{a_{t}{{}^{\prime}}}p(s_{t},a_{t}^{{}^{\prime}},O_{t:T}=1)d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t}&#124;s_{t})p(s_{t})}{\int_{a_{t}^{{}^{\prime}}}p(O_{t:T}=1&#124;a_{t}{{}^{\prime}},s_{t})p(a_{t}{{}^{\prime}}&#124;s_{t})p(s_{t})d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1\mid a_{t},s_{t})p(a_{t}\mid
    s_{t})p(s_{t})}{\int_{a_{t}^{{}^{\prime}}}p(O_{t:T}=1\mid a_{t}{{}^{\prime}},s_{t})p(a_{t}{{}^{\prime}}\mid
    s_{t})p(s_{t})d\{a_{t}^{{}^{\prime}}\}}$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t}&#124;s_{t})}{\int_{a_{t}^{{}^{\prime}}}p(O_{t:T}=1&#124;a_{t}{{}^{\prime}},s_{t})p(a_{t}{{}^{\prime}}&#124;s_{t})d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t}&#124;s_{t})}{\int_{a_{t}^{{}^{\prime}}}p(O_{t:T}=1&#124;a_{t}{{}^{\prime}},s_{t})p(a_{t}{{}^{\prime}}&#124;s_{t})d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{\beta(a_{t},s_{t})}{\int_{a_{t}^{{}^{\prime}}}\beta(a_{t}^{{}^{\prime}},s_{t})d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{\beta(a_{t},s_{t})}{\int_{a_{t}^{{}^{\prime}}}\beta(a_{t}^{{}^{\prime}},s_{t})d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{\beta(a_{t},s_{t})}{\beta(s_{t})}$
    |  | (35) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{\beta(a_{t},s_{t})}{\beta(s_{t})}$
    |  | (35) |'
- en: In Equation ([35](#S3.E35 "In III-B2 Message passing for exact inference on
    the posterior ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy
    of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), we
    define message $\beta(a_{t},s_{t})=p(O_{t:T}=1|a_{t},s_{t})p(a_{t}|s_{t})$ and
    message $\beta(s_{t})=\int_{a_{t}^{{}^{\prime}}}\beta(a_{t}^{{}^{\prime}},s_{t})d\{a_{t}^{{}^{\prime}}\}$.
    If we consider $p(a_{t}|s_{t})$ as a prior with a trivial form of uniform distribution
    [[13](#bib.bib13)], the only policy related term becomes $p(O_{t:T}=1|a_{t},s_{t})$.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 ([35](#S3.E35 "在 III-B2 后验精确推断中的消息传递 ‣ III-B 变分推断在“最优”策略上的应用 ‣ III 深度强化学习中的PGM和VI分类
    ‣ 关于深度强化学习中的概率图模型和变分推断的教程和综述")) 中，我们定义了消息 $\beta(a_{t},s_{t})=p(O_{t:T}=1|a_{t},s_{t})p(a_{t}|s_{t})$
    和消息 $\beta(s_{t})=\int_{a_{t}^{{}^{\prime}}}\beta(a_{t}^{{}^{\prime}},s_{t})d\{a_{t}^{{}^{\prime}}\}$。如果我们将
    $p(a_{t}|s_{t})$ 视为一个具有均匀分布的先验 [[13](#bib.bib13)]，那么唯一与策略相关的项就是 $p(O_{t:T}=1|a_{t},s_{t})$。
- en: In contrast to HMM, here, only the backward messages are relevant. Additionally,
    the backward message $\beta(a_{t},s_{t})$ here is not a probability distribution
    as in HMM, instead, is just a probability. In Figure [7](#S3.F7 "Figure 7 ‣ III-B1
    policy as ”optimal” posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), the backward message $\beta(a_{t},s_{t})$ could be decomposed recursively.
    Since in [[13](#bib.bib13)] the author only give the conclusion without derivation,
    we give a detailed derivaion of this recursion in Equation ([36](#S3.E36 "In III-B2
    Message passing for exact inference on the posterior ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与 HMM 相比，这里只有向后信息是相关的。此外，这里的向后信息 $\beta(a_{t},s_{t})$ 不是 HMM 中的概率分布，而仅仅是一个概率。在图
    [7](#S3.F7 "图 7 ‣ III-B1 策略作为“最优”后验 ‣ III-B 变分推断在“最优”策略上的应用 ‣ III 深度强化学习中的PGM和VI分类
    ‣ 关于深度强化学习中的概率图模型和变分推断的教程和综述") 中，向后信息 $\beta(a_{t},s_{t})$ 可以递归地分解。由于在 [[13](#bib.bib13)]
    中作者仅给出了结论而没有推导，我们在方程 ([36](#S3.E36 "在 III-B2 后验精确推断中的消息传递 ‣ III-B 变分推断在“最优”策略上的应用
    ‣ III 深度强化学习中的PGM和VI分类 ‣ 关于深度强化学习中的概率图模型和变分推断的教程和综述")) 中提供了这一递归的详细推导。
- en: '|  |  | $\displaystyle\beta(s_{t},a_{t})$ |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\beta(s_{t},a_{t})$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle p(O_{t}=1,O_{t+1:T}=1&#124;s_{t},a_{t})$
    |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle p(O_{t}=1,O_{t+1:T}=1&#124;s_{t},a_{t})$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{\int p(O_{t}=1,O_{t+1:T}=1,s_{t},a_{t},s_{t+1},a_{t+1})d\{s_{t+1},a_{t+1}\}}{p(s_{t},a_{t})}$
    |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{\int p(O_{t}=1,O_{t+1:T}=1,s_{t},a_{t},s_{t+1},a_{t+1})d\{s_{t+1},a_{t+1}\}}{p(s_{t},a_{t})}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1,s_{t+1},a_{t+1},O_{t}=1&#124;s_{t},a_{t})d\{s_{t+1},a_{t+1}\}$
    |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1,s_{t+1},a_{t+1},O_{t}=1&#124;s_{t},a_{t})d\{s_{t+1},a_{t+1}\}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1,s_{t+1},a_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})$
    |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1,s_{t+1},a_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})$
    |  |'
- en: '|  |  | $\displaystyle d\{s_{t+1},a_{t+1}\}$ |  | ($(O_{t+1:T},S_{t+1},A_{t+1}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}O_{t})\mid
    S_{t},A_{t}$) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle d\{s_{t+1},a_{t+1}\}$ |  | ($(O_{t+1:T},S_{t+1},A_{t+1}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}O_{t})\mid
    S_{t},A_{t}$) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\int\frac{p(O_{t+1:T}=1,s_{t+1},a_{t+1})}{p(s_{t+1},a_{t+1})}\frac{p(s_{t+1},s_{t},a_{t})}{p(s_{t},a_{t})}$
    |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\int\frac{p(O_{t+1:T}=1,s_{t+1},a_{t+1})}{p(s_{t+1},a_{t+1})}\frac{p(s_{t+1},s_{t},a_{t})}{p(s_{t},a_{t})}$
    |  |'
- en: '|  |  | $\displaystyle p(O_{t}=1&#124;s_{t},a_{t})d\{s_{t+1},a_{t+1}\}$ |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle p(O_{t}=1&#124;s_{t},a_{t})d\{s_{t+1},a_{t+1}\}$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1&#124;s_{t+1},a_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})$
    |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1&#124;s_{t+1},a_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})$
    |  |'
- en: '|  |  | $\displaystyle d\{s_{t+1},a_{t+1}\}$ |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle d\{s_{t+1},a_{t+1}\}$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\int\beta(s_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})ds_{t+1}$
    |  | (36) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\int\beta(s_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})ds_{t+1}$
    |  | (36) |'
- en: The recursion in Equation ([36](#S3.E36 "In III-B2 Message passing for exact
    inference on the posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) start from the last time point $T$ of an episode.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 ([36](#S3.E36 "在 III-B2 对后验进行精确推断的消息传递 ‣ III-B 关于“最优”政策的变分推断 ‣ III 深度强化学习中
    PGM 和 VI 的分类 ‣ 关于概率图模型和深度强化学习中变分推断的教程与综述")) 中的递归从一个情节的最后时间点 $T$ 开始。
- en: III-B3 Connection between Message Passing and Bellman equation
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 消息传递与贝尔曼方程之间的联系
- en: If we define Q function in Equation ([37](#S3.E37 "In III-B3 Connection between
    Message Passing and Bellman equation ‣ III-B Variational Inference on ”optimal”
    Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")) and V function in Equation ([38](#S3.E38 "In III-B3
    Connection between Message Passing and Bellman equation ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning"))
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在方程式 ([37](#S3.E37 "在 III-B3 消息传递与贝尔曼方程之间的联系 ‣ III-B 关于“最优”政策的变分推断 ‣ III
    深度强化学习中 PGM 和 VI 的分类 ‣ 关于概率图模型和深度强化学习中变分推断的教程与综述")) 中定义 Q 函数，并在方程式 ([38](#S3.E38
    "在 III-B3 消息传递与贝尔曼方程之间的联系 ‣ III-B 关于“最优”政策的变分推断 ‣ III 深度强化学习中 PGM 和 VI 的分类 ‣ 关于概率图模型和深度强化学习中变分推断的教程与综述"))
    中定义 V 函数
- en: '|  | $Q(s_{t},a_{t})=\log(\beta(a_{t},s_{t}))$ |  | (37) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})=\log(\beta(a_{t},s_{t}))$ |  | (37) |'
- en: '|  | $\displaystyle V(s_{t})$ | $\displaystyle=\log\beta(s_{t})=\log\int\beta(s_{t},a_{t})da_{t}$
    |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V(s_{t})$ | $\displaystyle=\log\beta(s_{t})=\log\int\beta(s_{t},a_{t})da_{t}$
    |  |'
- en: '|  |  | $\displaystyle=\log\int exp(Q(s_{t},a_{t}))da_{t}\approx\underset{a_{t}}{max}Q(s_{t},a_{t})$
    |  | (38) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\log\int \exp(Q(s_{t},a_{t}))da_{t}\approx\underset{a_{t}}{max}Q(s_{t},a_{t})$
    |  | (38) |'
- en: then the corresponding policy could be written as Equation ([39](#S3.E39 "In
    III-B3 Connection between Message Passing and Bellman equation ‣ III-B Variational
    Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 那么相应的政策可以写成方程式 ([39](#S3.E39 "在 III-B3 消息传递与贝尔曼方程之间的联系 ‣ III-B 关于“最优”政策的变分推断
    ‣ III 深度强化学习中 PGM 和 VI 的分类 ‣ 关于概率图模型和深度强化学习中变分推断的教程与综述"))。
- en: '|  | $\pi(a_{t}&#124;s_{t})=p(a_{t}&#124;s_{t},O_{t:T}=1)=\exp(Q(s_{t},a_{t})-V(s_{t}))$
    |  | (39) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi(a_{t}&#124;s_{t})=p(a_{t}&#124;s_{t},O_{t:T}=1)=\exp(Q(s_{t},a_{t})-V(s_{t}))$
    |  | (39) |'
- en: Taking the logrithm of Equation ([36](#S3.E36 "In III-B2 Message passing for
    exact inference on the posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), we get Equation ([40](#S3.E40 "In III-B3 Connection between Message
    Passing and Bellman equation ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"))
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对方程([36](#S3.E36 "在 III-B2 消息传递以进行精确推断关于后验 ‣ III-B 变分推断关于 ”最佳” 策略 ‣ III 深度强化学习中的PGM和VI的分类
    ‣ 深度强化学习中的概率图模型和变分推断教程和调查"))取对数，我们得到方程([40](#S3.E40 "在 III-B3 消息传递与贝尔曼方程的连接 ‣
    III-B 变分推断关于 ”最佳” 策略 ‣ III 深度强化学习中的PGM和VI的分类 ‣ 深度强化学习中的概率图模型和变分推断教程和调查"))
- en: '|  | $\displaystyle\log(\beta(s_{t},a_{t}))$ |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\log(\beta(s_{t},a_{t}))$ |  |'
- en: '|  | $\displaystyle=\log\int\beta(s_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})ds_{t+1}$
    |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\log\int\beta(s_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})ds_{t+1}$
    |  |'
- en: '|  | $\displaystyle=\log\int\exp[r(s_{t},a_{t})+V(s_{t+1})]p(s_{t+1}&#124;s_{t},a_{t})ds_{t+1}$
    |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\log\int\exp[r(s_{t},a_{t})+V(s_{t+1})]p(s_{t+1}&#124;s_{t},a_{t})ds_{t+1}$
    |  |'
- en: '|  | $\displaystyle=r(s_{t},a_{t})+\log\int\exp(V(s_{t+1}))p(s_{t+1}&#124;s_{t},a_{t})ds_{t+1}$
    |  | (40) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=r(s_{t},a_{t})+\log\int\exp(V(s_{t+1}))p(s_{t+1}&#124;s_{t},a_{t})ds_{t+1}$
    |  | (40) |'
- en: 'which reduces to the risk seeking backup in Equation ([41](#S3.E41 "In III-B3
    Connection between Message Passing and Bellman equation ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")) as mentioned in [[13](#bib.bib13)]:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这简化为方程([41](#S3.E41 "在 III-B3 消息传递与贝尔曼方程的连接 ‣ III-B 变分推断关于 ”最佳” 策略 ‣ III 深度强化学习中的PGM和VI的分类
    ‣ 深度强化学习中的概率图模型和变分推断教程和调查"))中提到的风险寻求备份 [[13](#bib.bib13)]：
- en: '|  | $Q(s_{t},a_{t})=r(s_{t},a_{t})+\log E_{s_{t+1}\sim p(s_{t+1}&#124;s_{t},a_{t})}[\exp(V(s_{t+1}))]$
    |  | (41) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})=r(s_{t},a_{t})+\log E_{s_{t+1}\sim p(s_{t+1}&#124;s_{t},a_{t})}[\exp(V(s_{t+1}))]$
    |  | (41) |'
- en: 'The mathematical insight here is that if we define the messages passed on the
    Directed Acyclic Graph in Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal”
    posterior ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy of
    PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"), then
    message passing correspond to a peculiar version Bellman Equation like backup,
    which lead to an unwanted risk seeking behavior [[13](#bib.bib13)]: when compared
    to Equation ([10](#S2.E10 "In II-B Value Function, Bellman Equation, Policy Iteration
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), the Q function here is taking a softmax instead of expectation over
    the next state.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的数学见解是，如果我们定义在图 [7](#S3.F7 "图 7 ‣ III-B1 策略作为 ”最佳” 后验 ‣ III-B 变分推断关于 ”最佳”
    策略 ‣ III 深度强化学习中的PGM和VI的分类 ‣ 深度强化学习中的概率图模型和变分推断教程和调查") 中传递的消息，那么消息传递对应于一种特殊的贝尔曼方程备份，这会导致一种不希望出现的风险寻求行为
    [[13](#bib.bib13)]：与方程 ([10](#S2.E10 "在 II-B 价值函数、贝尔曼方程、策略迭代 ‣ II 强化学习和深度强化学习
    ‣ 深度强化学习中的概率图模型和变分推断教程和调查")) 相比，这里的 Q 函数采用了软最大化而不是对下一个状态的期望值。
- en: III-B4 Variational approximation to ”optimal” policy
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 变分近似至 ”最佳” 策略
- en: 'Since the exact inference lead to unexpected behavior, approximate inference
    could be used. The optimization of the policy could be considered as a variational
    inference problem, and we use the variational policy of the action posterior distribution
    $q(a_{t}|s_{t})$, which could be represented by a neural network, to compose the
    proposal variational likelihood of the trajectory as in Equation ([42](#S3.E42
    "In III-B4 Variational approximation to ”optimal” policy ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于精确推断会导致意外行为，因此可以使用近似推断。策略的优化可以被视为一个变分推断问题，我们使用动作后验分布$q(a_{t}|s_{t})$的变分策略，该分布可以通过神经网络表示，以构建如方程（[42](#S3.E42
    "在 III-B4 对 ”最优” 策略的变分近似 ‣ III-B 对 ”最优” 策略的变分推断 ‣ III 深度强化学习中的PGM与VI分类 ‣ 深度强化学习中概率图模型和变分推断的教程与调查")）所示的提议变分似然函数：
- en: '|  | $\displaystyle q(\tau)$ | $\displaystyle=p(s_{1})\prod_{t}[p(s_{t+1}&#124;s_{t},a_{t})q(a_{t}&#124;s_{t})]$
    |  | (42) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q(\tau)$ | $\displaystyle=p(s_{1})\prod_{t}[p(s_{t+1}&#124;s_{t},a_{t})q(a_{t}&#124;s_{t})]$
    |  | (42) |'
- en: where the initial state distribution $p(s_{1})$ and the environmental dynamics
    of state transmission is kept intact. Using the proposal trajectory as a pivot,
    we could derive the Evidence Lower Bound (ELBO) of the optimal trajectory as in
    Equation ([43](#S3.E43 "In III-B4 Variational approximation to ”optimal” policy
    ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), which correspond
    to an interesting objective function of reward plus entropy return, as in Equation
    ([45](#S3.E45 "In III-B4 Variational approximation to ”optimal” policy ‣ III-B
    Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep
    reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，初始状态分布$p(s_{1})$和状态转移的环境动态保持不变。以建议轨迹作为枢轴，我们可以推导出最优轨迹的证据下界（ELBO），如方程（[43](#S3.E43
    "在 III-B4 对 ”最优” 策略的变分近似 ‣ III-B 对 ”最优” 策略的变分推断 ‣ III 深度强化学习中的PGM与VI分类 ‣ 深度强化学习中概率图模型和变分推断的教程与调查")）所示，这对应于奖励加熵回报的有趣目标函数，如方程（[45](#S3.E45
    "在 III-B4 对 ”最优” 策略的变分近似 ‣ III-B 对 ”最优” 策略的变分推断 ‣ III 深度强化学习中的PGM与VI分类 ‣ 深度强化学习中概率图模型和变分推断的教程与调查")）所示。
- en: '|  |  | $\displaystyle\log(p(O_{1:T}))$ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\log(p(O_{1:T}))$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\log\int p(O_{1:T}=1,s_{1:T},a_{1:T})\frac{q(s_{1:T},a_{1:T})}{q(s_{1:T},a_{1:T})}ds_{1:T}da_{1:T}$
    |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\log\int p(O_{1:T}=1,s_{1:T},a_{1:T})\frac{q(s_{1:T},a_{1:T})}{q(s_{1:T},a_{1:T})}ds_{1:T}da_{1:T}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\log E_{q(s_{1:T},a_{1:T})}\frac{p(O_{1:T}=1,s_{1:T},a_{1:T})}{q(s_{1:T},a_{1:T})}$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\log E_{q(s_{1:T},a_{1:T})}\frac{p(O_{1:T}=1,s_{1:T},a_{1:T})}{q(s_{1:T},a_{1:T})}$
    |  |'
- en: '|  | $\displaystyle\geq$ | $\displaystyle E_{q(s_{1:T},a_{1:T})}[\log p(O_{1:T}=1,s_{1:T},a_{1:T})-\log
    q(s_{1:T},a_{1:T})]$ |  | (43) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ | $\displaystyle E_{q(s_{1:T},a_{1:T})}[\log p(O_{1:T}=1,s_{1:T},a_{1:T})-\log
    q(s_{1:T},a_{1:T})]$ |  | (43) |'
- en: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(q(\tau)&#124;p(\tau))$ |  | (44)
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(q(\tau)&#124;p(\tau))$ |  | (44)
    |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{q(s_{1:T},a_{1:T})}[\sum_{t=1:T}[r(s_{t},a_{t})-\log
    q(a_{t}&#124;s_{t})]]$ |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{q(s_{1:T},a_{1:T})}[\sum_{t=1:T}[r(s_{t},a_{t})-\log
    q(a_{t}&#124;s_{t})]]$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{t=1:T}E_{s_{t},a_{t}}[r(s_{t},a_{t})+H(\pi(a_{t}&#124;s_{t}))]$
    |  | (45) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{t=1:T}E_{s_{t},a_{t}}[r(s_{t},a_{t})+H(\pi(a_{t}&#124;s_{t}))]$
    |  | (45) |'
- en: III-B5 Examples
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B5 示例
- en: In [[28](#bib.bib28)], the state action value function is defined in Equation
    ([46](#S3.E46 "In III-B5 Examples ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[28](#bib.bib28)]中，状态动作值函数在方程（[46](#S3.E46 "在 III-B5 示例 ‣ III-B 对 ”最优” 策略的变分推断
    ‣ III 深度强化学习中的PGM与VI分类 ‣ 深度强化学习中概率图模型和变分推断的教程与调查")）中定义。
- en: '|  | $Q^{\pi}_{soft}(s,a)=r_{0}+E_{r\sim\pi,s_{0}=s,a_{0}=a}[\sum_{t=1}^{\infty}\gamma^{t}(r_{t}+\alpha
    H(\pi(.&#124;s_{t})))]$ |  | (46) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{\pi}_{soft}(s,a)=r_{0}+E_{r\sim\pi,s_{0}=s,a_{0}=a}[\sum_{t=1}^{\infty}\gamma^{t}(r_{t}+\alpha
    H(\pi(. \mid s_{t})))]$ |  | (46) |'
- en: and a soft version of Bellman update similar to Q Learning [[12](#bib.bib12)]
    is carried out, which lead to policy improvement with respect to the corresponding
    functional objective in Equation ([47](#S3.E47 "In III-B5 Examples ‣ III-B Variational
    Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 和一个类似于 Q 学习的贝尔曼更新的软版本 [[12](#bib.bib12)] 被执行，这导致相对于方程 ([47](#S3.E47 "在 III-B5
    示例 ‣ III-B 关于“最优”策略的变分推断 ‣ III 深度强化学习中的PGM与VI分类 ‣ 概率图模型与深度强化学习中变分推断的教程与综述")) 中相应的目标函数的策略改进。
- en: '|  |  | $\displaystyle J(\pi)$ |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle J(\pi)$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{t}E_{(s_{t},a_{t})\sim\rho_{\pi}}\sum_{l=t}^{\infty}\gamma^{l-t}E_{(s_{l},a_{l})}[r(s_{l},a_{l})+$
    |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{t}E_{(s_{t},a_{t})\sim\rho_{\pi}}\sum_{l=t}^{\infty}\gamma^{l-t}E_{(s_{l},a_{l})}[r(s_{l},a_{l})+$
    |  |'
- en: '|  |  | $\displaystyle\alpha H(\pi(.&#124;s_{l}))&#124;s_{t},a_{t}]]$ |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\alpha H(\pi(. \mid s_{l})) \mid s_{t},a_{t}]]$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{t}E_{(s_{t},a_{t})\sim\rho_{\pi}}[Q_{soft}^{\pi}(s_{t},a_{t})+\alpha
    H(\pi(.&#124;s_{t}))]$ |  | (47) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sum_{t}E_{(s_{t},a_{t})\sim\rho_{\pi}}[Q_{soft}^{\pi}(s_{t},a_{t})+\alpha
    H(\pi(. \mid s_{t}))]$ |  | (47) |'
- en: Setting policy as Equation ([39](#S3.E39 "In III-B3 Connection between Message
    Passing and Bellman equation ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) lead to policy improvement. We offer a detailed proof for a key formula
    in Equation ([48](#S3.E48 "In III-B5 Examples ‣ III-B Variational Inference on
    ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")), which is stated in Equation (19) of [[28](#bib.bib28)]
    without proof. In Equation ([48](#S3.E48 "In III-B5 Examples ‣ III-B Variational
    Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")), we use $\pi(\cdot|s)$ to implicitly
    represent $\pi(a|s)$ to avoid symbol aliasing whenever necessary.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 设定策略为方程 ([39](#S3.E39 "在 III-B3 消息传递与贝尔曼方程的联系 ‣ III-B 关于“最优”策略的变分推断 ‣ III 深度强化学习中的PGM与VI分类
    ‣ 概率图模型与深度强化学习中变分推断的教程与综述")) 可导致策略改进。我们提供了方程 ([48](#S3.E48 "在 III-B5 示例 ‣ III-B
    关于“最优”策略的变分推断 ‣ III 深度强化学习中的PGM与VI分类 ‣ 概率图模型与深度强化学习中变分推断的教程与综述")) 中关键公式的详细证明，该公式在
    [[28](#bib.bib28)] 的方程 (19) 中没有证明。在方程 ([48](#S3.E48 "在 III-B5 示例 ‣ III-B 关于“最优”策略的变分推断
    ‣ III 深度强化学习中的PGM与VI分类 ‣ 概率图模型与深度强化学习中变分推断的教程与综述")) 中，我们使用 $\pi(\cdot|s)$ 来隐式表示
    $\pi(a|s)$，以避免符号别名。
- en: '|  |  | $\displaystyle H(\pi(\cdot&#124;s))+E_{a\sim\pi}[Q_{soft}^{\pi}(s,a)]$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle H(\pi(\cdot \mid s))+E_{a\sim\pi}[Q_{soft}^{\pi}(s,a)]$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a&#124;s)[\log\pi(a&#124;s)-Q_{soft}^{\pi}(s,a)]da$
    |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a\mid s)[\log\pi(a\mid s)-Q_{soft}^{\pi}(s,a)]da$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a&#124;s)[\log\pi(a&#124;s)-\log[\exp(Q_{soft}^{\pi}(s,a))]]da$
    |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a\mid s)[\log\pi(a\mid s)-\log[\exp(Q_{soft}^{\pi}(s,a))]]da$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a&#124;s)[\log\pi(a&#124;s)-\log[\frac{\exp(Q_{soft}^{\pi}(s,a))}{\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}}$
    |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a\mid s)[\log\pi(a\mid s)-\log[\frac{\exp(Q_{soft}^{\pi}(s,a))}{\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}}$
    |  |'
- en: '|  |  | $\displaystyle\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}]]da$
    |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}]]da$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a&#124;s)[\log\pi(a&#124;s)-\log[\tilde{\pi}(a&#124;s)]-$
    |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a\mid s)[\log\pi(a\mid s)-\log[\tilde{\pi}(a\mid
    s)]-$ |  |'
- en: '|  |  | $\displaystyle log\int exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))]da^{{}^{\prime}}$
    |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle \log\int \exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))]da^{{}^{\prime}}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(\pi(\cdot&#124;s)&#124;&#124;\tilde{\pi}(\cdot&#124;s))+\log\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}$
    |  | (48) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(\pi(\cdot|s)||\tilde{\pi}(\cdot|s))+\log\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}$
    |  | (48) |'
- en: For the rest of the proof, we invite the reader to read the appendix of [[28](#bib.bib28)].
    Algorithms of the this kind of maximum entropy family also include Soft Actor
    Critic [[29](#bib.bib29)].
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其余的证明，我们邀请读者查阅 [[28](#bib.bib28)] 的附录。这类最大熵家族的算法还包括软演员评论家 [[29](#bib.bib29)]。
- en: III-C Variational Inference on the Environment
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 环境中的变分推断
- en: Another direction of using Variational Inference in Reinforcement Learning is
    to learn an environmental model, either on the dynamics or the latent state space
    posterior.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中使用变分推断的另一个方向是学习环境模型，既可以是动态模型，也可以是潜在状态空间后验。
- en: III-C1 Variational inference on transition model
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 转移模型的变分推断
- en: In Variational Information Maximizing Exploration (VIME) [[4](#bib.bib4)], where
    dynamic model $p_{\theta}(s_{t+1}|s_{t},a_{t})$ for the agent’s interaction with
    the environment is modeled using Bayesian Neural Network [[10](#bib.bib10)]. The
    R.V. for $\theta$ is denoted by $\Theta$, and is treated in a Bayesian way by
    modeling the weight $\theta$ uncertainty of a neural network. We represent this
    model with the graphical model in Figure [8](#S3.F8 "Figure 8 ‣ III-C1 Variational
    inference on transition model ‣ III-C Variational Inference on the Environment
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), which is not given in [[4](#bib.bib4)]. The belief uncertainty about
    the environment is modeled as entropy of the posterior distribution of the neural
    network weights $H(\Theta|\xi_{t})$ based on trajectory observations $\xi_{t}=\{s_{1:t},a_{1:t-1}\}$.
    The method encourages taking exploratory actions by alleviating the average information
    gain of the agent’s belief about the environment after observing a new state $s_{t+1}$,
    which is $E_{p(s_{t+1}|\xi_{t},a_{t})}D_{KL}(p(\theta|\xi_{t+1})||p(\theta|\xi_{t}))$,
    and this is equivalent to the entropy minus conditional entropy $H(\Theta|\xi_{t},a_{t})-H(\Theta|\xi_{t},a_{t},s_{t+1})=H(\Theta|\xi_{t},a_{t})-H(\Theta|\xi_{t+1})$.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在变分信息最大化探索（VIME）[[4](#bib.bib4)]中，代理与环境的动态模型 $p_{\theta}(s_{t+1}|s_{t},a_{t})$
    是通过贝叶斯神经网络 [[10](#bib.bib10)] 来建模的。$\theta$ 的随机变量用 $\Theta$ 表示，并通过建模神经网络权重 $\theta$
    的不确定性以贝叶斯方式对待。我们用图 [8](#S3.F8 "图 8 ‣ III-C1 转移模型的变分推断 ‣ III-C 环境中的变分推断 ‣ III 深度强化学习中的概率图模型与变分推断分类
    ‣ 概率图模型与深度强化学习中的变分推断教程与综述") 中的图示表示此模型，该图在 [[4](#bib.bib4)] 中未给出。关于环境的信念不确定性被建模为基于轨迹观察
    $\xi_{t}=\{s_{1:t},a_{1:t-1}\}$ 的神经网络权重后验分布的熵 $H(\Theta|\xi_{t})$。该方法通过缓解观察到新状态
    $s_{t+1}$ 后代理对环境的信念的平均信息增益来鼓励采取探索性行动，其为 $E_{p(s_{t+1}|\xi_{t},a_{t})}D_{KL}(p(\theta|\xi_{t+1})||p(\theta|\xi_{t}))$，这等同于熵减去条件熵
    $H(\Theta|\xi_{t},a_{t})-H(\Theta|\xi_{t},a_{t},s_{t+1})=H(\Theta|\xi_{t},a_{t})-H(\Theta|\xi_{t+1})$。
- en: \includegraphics
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.6]tikz_pgm_mdp_vime.pdf'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.6]tikz_pgm_mdp_vime.pdf'
- en: 'Figure 8: Probabilistic Graphical Model For VIME'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：VIME 的概率图模型
- en: With the help of Equation ([49](#S3.E49 "In III-C1 Variational inference on
    transition model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy
    of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), as
    derived following the definition of conditional mutual information, we derive
    in Equation ([50](#S3.E50 "In III-C1 Variational inference on transition model
    ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and VI
    in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")) that the conditional
    entropy difference is actually the average information gain, which is equal to
    the conditional mutual information $I(\Theta,S_{t+1}|\xi_{t},a_{t})$ between environmental
    parameter $\Theta$ and the new state $S_{t+1}$. Such a derivation is not given
    in [[4](#bib.bib4)].
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式（[49](#S3.E49 "在 III-C1 转移模型上的变分推断 ‣ III-C 环境上的变分推断 ‣ III 深度强化学习中的概率图模型与变分推断的分类
    ‣ 深度强化学习中的概率图模型和变分推断的教程与调研"）的帮助下，根据条件互信息的定义推导，我们在公式（[50](#S3.E50 "在 III-C1 转移模型上的变分推断
    ‣ III-C 环境上的变分推断 ‣ III 深度强化学习中的概率图模型与变分推断的分类 ‣ 深度强化学习中的概率图模型和变分推断的教程与调研"）中推导出条件熵差实际上是平均信息增益，它等于环境参数$\Theta$和新状态$S_{t+1}$之间的条件互信息$I(\Theta,S_{t+1}\mid\xi_{t},a_{t})$。这样的推导在[[4](#bib.bib4)]中没有给出。
- en: '|  | $\displaystyle I(X;Y\mid Z)$ | $\displaystyle=\int_{x,y,z}p(z)p(x,y&#124;z)\log\frac{p(x,y&#124;z)}{p(x&#124;z)p(y&#124;z)}dxdydz$
    |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I(X;Y\mid Z)$ | $\displaystyle=\int_{x,y,z}p(z)p(x,y\mid
    z)\log\frac{p(x,y\mid z)}{p(x\mid z)p(y\mid z)}dxdydz$ |  |'
- en: '|  |  | $\displaystyle=-\int_{x,y,z}p(z)p(x,y&#124;z)\log p(x&#124;z)dxdzdy+$
    |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\int_{x,y,z}p(z)p(x,y\mid z)\log p(x\mid z)dxdzdy+$
    |  |'
- en: '|  |  | $\displaystyle+\int_{x,y,z}p(x,y,z)\log p(x&#124;y,z)dxdzdy$ |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\int_{x,y,z}p(x,y,z)\log p(x\mid y,z)dxdzdy$ |  |'
- en: '|  |  | $\displaystyle=H(X\mid Z)-H(X\mid Y,Z)$ |  | (49) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=H(X\mid Z)-H(X\mid Y,Z)$ |  | (49) |'
- en: '|  |  | $\displaystyle H(\Theta&#124;\xi_{t},a_{t})-H(\Theta&#124;\xi_{t},a_{t},s_{t+1})=I(\Theta,S_{t+1}&#124;\xi_{t},a_{t})$
    |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle H(\Theta\mid\xi_{t},a_{t})-H(\Theta\mid\xi_{t},a_{t},s_{t+1})=I(\Theta,S_{t+1}\mid\xi_{t},a_{t})$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}\int_{\Theta,\mathcal{S}}p(s_{t+1},\theta&#124;\xi_{t},a_{t})\log[\frac{p(s_{t+1},\theta&#124;\xi_{t},a_{t})}{p(\theta&#124;\xi_{t})p(s_{t+1}&#124;\xi_{t},a_{t})}]d\theta$
    |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}\int_{\Theta,\mathcal{S}}p(s_{t+1},\theta\mid\xi_{t},a_{t})\log[\frac{p(s_{t+1},\theta\mid\xi_{t},a_{t})}{p(\theta\mid\xi_{t})p(s_{t+1}\mid\xi_{t},a_{t})}]d\theta$
    |  |'
- en: '|  |  | $\displaystyle ds_{t+1}$ |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle ds_{t+1}$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}\int_{\Theta,\mathcal{S}}p(s_{t+1}&#124;\xi_{t},a_{t})p(\theta&#124;\xi_{t+1})\log[\frac{p(\theta&#124;\xi_{t+1})}{p(\theta&#124;\xi_{t})}]d\theta
    ds_{t+1}$ |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}\int_{\Theta,\mathcal{S}}p(s_{t+1}\mid\xi_{t},a_{t})p(\theta\mid\xi_{t+1})\log[\frac{p(\theta\mid\xi_{t+1})}{p(\theta\mid\xi_{t})}]d\theta
    ds_{t+1}$ |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}E_{p(s_{t+1}&#124;\xi_{t},a_{t})}D_{KL}(p(\theta&#124;\xi_{t+1})&#124;&#124;p(\theta&#124;\xi_{t}))$
    |  | (50) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}E_{p(s_{t+1}\mid\xi_{t},a_{t})}D_{KL}(p(\theta\mid\xi_{t+1})\|\
    p(\theta\mid\xi_{t}))$ |  | (50) |'
- en: Based on Equation ([50](#S3.E50 "In III-C1 Variational inference on transition
    model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), an intrinsic
    reward can be augmented from the environmental reward function, thus the method
    could be incorporated with any existing reinforcement learning algorithms for
    exploration, TRPO [[2](#bib.bib2)], for example. Upon additional observation of
    action $a_{t}$ and state $s_{t+1}$ pair on top of trajectory history $\xi_{t}$,
    the posterior on the distribution of the environmental parameter $\theta$, $p(\theta|\xi_{t})$,
    could be updated to be $p(\theta|\xi_{t+1})$ in a Bayesian way as derived in Equation
    ([51](#S3.E51 "In III-C1 Variational inference on transition model ‣ III-C Variational
    Inference on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")), which is first proposed in [[30](#bib.bib30)].
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 基于方程（[50](#S3.E50 "在 III-C1 转移模型的变分推断 ‣ III-C 环境上的变分推断 ‣ III 深度强化学习中的 PGMs 和
    VI 分类 ‣ 关于深度强化学习中概率图模型与变分推断的教程和调查")），可以从环境奖励函数中增强内在奖励，因此该方法可以与任何现有的强化学习算法（例如 TRPO
    [[2](#bib.bib2)]）结合使用。在对动作 $a_{t}$ 和状态 $s_{t+1}$ 对于轨迹历史 $\xi_{t}$ 的附加观察后，环境参数
    $\theta$ 的后验分布 $p(\theta|\xi_{t})$ 可以以贝叶斯方式更新为 $p(\theta|\xi_{t+1})$，如方程（[51](#S3.E51
    "在 III-C1 转移模型的变分推断 ‣ III-C 环境上的变分推断 ‣ III 深度强化学习中的 PGMs 和 VI 分类 ‣ 关于深度强化学习中概率图模型与变分推断的教程和调查")）中推导的，这一方法最早在
    [[30](#bib.bib30)] 中提出。
- en: '|  | $\displaystyle p(\theta&#124;\xi_{t+1})=$ | $\displaystyle\frac{p(\theta,\xi_{t},a_{t},s_{t+1})}{p(\xi_{t},a_{t},s_{t+1})}$
    |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(\theta&#124;\xi_{t+1})=$ | $\displaystyle\frac{p(\theta,\xi_{t},a_{t},s_{t+1})}{p(\xi_{t},a_{t},s_{t+1})}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta,\xi_{t},a_{t})}{p(\xi_{t},a_{t},s_{t+1})}$
    |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta,\xi_{t},a_{t})}{p(\xi_{t},a_{t},s_{t+1})}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta,\xi_{t},a_{t})}{p(a_{t},\xi_{t})p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta,\xi_{t},a_{t})}{p(a_{t},\xi_{t})p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta&#124;\xi_{t},a_{t})}{p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta&#124;\xi_{t},a_{t})}{p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta&#124;\xi_{t})}{p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  | (51) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta&#124;\xi_{t})}{p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  | (51) |'
- en: In Equation ([51](#S3.E51 "In III-C1 Variational inference on transition model
    ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and VI
    in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), the denominator
    can be written as Equation ([52](#S3.E52 "In III-C1 Variational inference on transition
    model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), so that the
    dynamics of the environment modeled by neural network weights $\theta$, $p(s_{t+1}|\theta,a_{t},\xi_{t})$,
    could be used.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程（[51](#S3.E51 "在 III-C1 转移模型的变分推断 ‣ III-C 环境上的变分推断 ‣ III 深度强化学习中的 PGMs 和
    VI 分类 ‣ 关于深度强化学习中概率图模型与变分推断的教程和调查")）中，分母可以写成方程（[52](#S3.E52 "在 III-C1 转移模型的变分推断
    ‣ III-C 环境上的变分推断 ‣ III 深度强化学习中的 PGMs 和 VI 分类 ‣ 关于深度强化学习中概率图模型与变分推断的教程和调查")），从而可以利用神经网络权重
    $\theta$ 模拟的环境动态 $p(s_{t+1}|\theta,a_{t},\xi_{t})$。
- en: '|  | $\displaystyle p(s_{t+1}&#124;a_{t},\xi_{t})$ |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(s_{t+1}&#124;a_{t},\xi_{t})$ |  |'
- en: '|  | $\displaystyle=\int_{\Theta}p(s_{t+1},\theta&#124;a_{t},\xi_{t})d\theta$
    |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\int_{\Theta}p(s_{t+1},\theta&#124;a_{t},\xi_{t})d\theta$
    |  |'
- en: '|  | $\displaystyle=\int_{\Theta}\frac{p(s_{t+1},\theta,a_{t},\xi_{t})}{p(a_{t},\xi_{t})}d\theta$
    |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\int_{\Theta}\frac{p(s_{t+1},\theta,a_{t},\xi_{t})}{p(a_{t},\xi_{t})}d\theta$
    |  |'
- en: '|  | $\displaystyle=\int_{\Theta}\frac{p(s_{t+1}&#124;\theta,a_{t},\xi_{t})p(\theta,a_{t},\xi_{t})}{p(a_{t},\xi_{t})}d\theta$
    |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\int_{\Theta}\frac{p(s_{t+1}&#124;\theta,a_{t},\xi_{t})p(\theta,a_{t},\xi_{t})}{p(a_{t},\xi_{t})}d\theta$
    |  |'
- en: '|  | $\displaystyle=\int_{\Theta}p(s_{t+1}&#124;\theta,a_{t},\xi_{t})p(\theta&#124;\xi_{t})d\theta$
    |  | (52) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\int_{\Theta}p(s_{t+1}&#124;\theta,a_{t},\xi_{t})p(\theta&#124;\xi_{t})d\theta$
    |  | (52) |'
- en: The last step of Equation ([52](#S3.E52 "In III-C1 Variational inference on
    transition model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy
    of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")) makes
    use of $p(\theta|\xi_{t},a_{t})=p(\theta|\xi_{t})$.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 ([52](#S3.E52 "在 III-C1 状态转移模型的变分推断中 ‣ III-C 环境的变分推断 ‣ III 深度强化学习中的概率图模型和变分推断的分类
    ‣ 深度强化学习中概率图模型和变分推断的教程和调查")) 的最后一步利用了 $p(\theta|\xi_{t},a_{t})=p(\theta|\xi_{t})$
    。
- en: Since the integral in Equation ([52](#S3.E52 "In III-C1 Variational inference
    on transition model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy
    of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")) is
    not tractable, variational treatment over the neural network weights posterior
    distribution $p(\theta|\xi_{t})$ is used, characterized by variational parameter
    $\phi$, as shown in the dotted line in Figure [8](#S3.F8 "Figure 8 ‣ III-C1 Variational
    inference on transition model ‣ III-C Variational Inference on the Environment
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"). The variational posterior about the model parameter $\theta$, updated
    at each step, could than be used to calculate the intrinsic reward in Equation
    ([50](#S3.E50 "In III-C1 Variational inference on transition model ‣ III-C Variational
    Inference on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 由于方程式 ([52](#S3.E52 "在 III-C1 状态转移模型的变分推断中 ‣ III-C 环境的变分推断 ‣ III 深度强化学习中的概率图模型和变分推断的分类
    ‣ 深度强化学习中概率图模型和变分推断的教程和调查")) 中的积分是不可计算的，因此对神经网络权重后验分布 $p(\theta|\xi_{t})$ 进行变分处理，由变分参数
    $\phi$ 表征，如图 [8](#S3.F8 "图 8 ‣ III-C1 状态转移模型的变分推断 ‣ III-C 环境的变分推断 ‣ III 深度强化学习中的概率图模型和变分推断的分类
    ‣ 深度强化学习中概率图模型和变分推断的教程和调查") 中虚线所示。在每一步更新的模型参数 $\theta$ 的变分后验，可用于计算方程式 ([50](#S3.E50
    "在 III-C1 状态转移模型的变分推断中 ‣ III-C 环境的变分推断 ‣ III 深度强化学习中的概率图模型和变分推断的分类 ‣ 深度强化学习中概率图模型和变分推断的教程和调查"))
    中的内在奖励。
- en: III-C2 Variational Inference on hidden state posterior
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 隐藏状态后验的变分推断
- en: In Variational State Tabulation (VaST) [[5](#bib.bib5)], the author assume the
    high dimensional observed state to be represented by Observable $O$, while the
    transition happens at the latent state space represented by $S$, which is finite
    and discrete. The author assume a factorized form of observation and latent space
    joint probability, which we explicitly state in Equation ([53](#S3.E53 "In III-C2
    Variational Inference on hidden state posterior ‣ III-C Variational Inference
    on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在变分状态表格（VaST）[[5](#bib.bib5)] 中，作者假设高维观察状态由 Observable $O$ 表示，而在潜在状态空间 $S$ 发生转换，该空间是有限离散的。
    作者假设观察和潜在空间联合概率具有分解形式，我们在方程式 ([53](#S3.E53 "在 III-C2 隐藏状态后验的变分推断 ‣ III-C 环境的变分推断
    ‣ III 深度强化学习中的概率图模型和变分推断的分类 ‣ 深度强化学习中概率图模型和变分推断的教程和调查")) 中明确陈述。
- en: '|  | $\displaystyle p(O,S)=\pi_{\theta_{0}}(s_{0})\prod_{t=0}^{T}p_{\theta^{R}}(o_{t}&#124;s_{t})\prod_{t=1}^{T}p_{\theta^{T}}(s_{t}&#124;s_{t-1},a_{t-1})$
    |  | (53) |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(O,S)=\pi_{\theta_{0}}(s_{0})\prod_{t=0}^{T}p_{\theta^{R}}(o_{t}&#124;s_{t})\prod_{t=1}^{T}p_{\theta^{T}}(s_{t}&#124;s_{t-1},a_{t-1})$
    |  | (53) |'
- en: Additionally, we characterize Equation ([53](#S3.E53 "In III-C2 Variational
    Inference on hidden state posterior ‣ III-C Variational Inference on the Environment
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) with the probabilistic graphical model in Figure [9](#S3.F9 "Figure
    9 ‣ III-C2 Variational Inference on hidden state posterior ‣ III-C Variational
    Inference on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning") which does not exist in [[5](#bib.bib5)].
    Compared to Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal” posterior
    ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), here the latent
    state $S$ is in discrete space instead of high dimension, and the observation
    is a high dimensional image instead of binary variable to indicate optimal action.
    By assuming a factorized form of the variational posterior in Equation ([54](#S3.E54
    "In III-C2 Variational Inference on hidden state posterior ‣ III-C Variational
    Inference on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")),
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们用图中的概率图模型来刻画方程 ([53](#S3.E53 "在 III-C2 隐状态后验的变分推断 ‣ III-C 环境中的变分推断 ‣ III
    深度强化学习中的概率图模型和变分推断分类 ‣ 深度强化学习中的概率图模型和变分推断教程与综述"))，[9](#S3.F9 "图 9 ‣ III-C2 隐状态后验的变分推断
    ‣ III-C 环境中的变分推断 ‣ III 深度强化学习中的概率图模型和变分推断分类 ‣ 深度强化学习中的概率图模型和变分推断教程与综述")中显示的该模型在
    [[5](#bib.bib5)] 中并不存在。与图 [7](#S3.F7 "图 7 ‣ III-B1 策略作为 ”最优” 后验 ‣ III-B 对 ”最优”
    策略的变分推断 ‣ III 深度强化学习中的概率图模型和变分推断分类 ‣ 深度强化学习中的概率图模型和变分推断教程与综述") 相比，这里的潜在状态 $S$
    是离散空间中的，而观察值是高维图像，而不是表示最优动作的二进制变量。通过假设方程 ([54](#S3.E54 "在 III-C2 隐状态后验的变分推断 ‣
    III-C 环境中的变分推断 ‣ III 深度强化学习中的概率图模型和变分推断分类 ‣ 深度强化学习中的概率图模型和变分推断教程与综述")) 中变分后验的因式分解形式，
- en: '|  | $\displaystyle q(S_{0:T}&#124;O_{0:T})=\prod_{t=0}^{T}q_{\phi}(S_{t}&#124;O_{t-k:t})$
    |  | (54) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q(S_{0:T}&#124;O_{0:T})=\prod_{t=0}^{T}q_{\phi}(S_{t}&#124;O_{t-k:t})$
    |  | (54) |'
- en: The author assume the episode length to be $T$, and default frame prior observation
    to be blank frames. The Evidence Lower Bound (ELBO) of the observed trajectory
    of Equation ([53](#S3.E53 "In III-C2 Variational Inference on hidden state posterior
    ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and VI
    in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")) could be easily
    represented by a Varitional AutoEncoder [[31](#bib.bib31)] like architecture,
    where the encoder $q_{\phi}$, together with the reparametrization trick [[31](#bib.bib31)],
    maps the observed state $O$ into parameters for the Con-crete distribution [[32](#bib.bib32)],
    so backprobagation could be used on deterministic variables to update the weight
    of the network based on the ELBO, which is decomposed into different parts of
    the reconstruction losses of the variational autoencoder like architecture. Like
    VIME [[4](#bib.bib4)], VaSt could be combined with other reinforcement learning
    algorithms. Here prioritized sweeping [[12](#bib.bib12)] is carried out on the
    Heviside activation of the encoder output directly, by counting the transition
    frequency, instead of waiting for the slowly learned environmental transition
    model $p_{\theta^{T}}(s_{t}|s_{t-1},a_{t-1})$ in Equation ([53](#S3.E53 "In III-C2
    Variational Inference on hidden state posterior ‣ III-C Variational Inference
    on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")). A potential problem of doing so is aliasing
    between latent state $s$ and observed state $o$. To alleviate this problem, in
    [[5](#bib.bib5)], the author actively relabel the transition history in the replay
    memory once found the observable has been assigned a different latent discrete
    state.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 作者假设回合长度为$T$，并将默认帧先验观测设为空帧。观察到的轨迹的证据下界（ELBO）（方程 ([53](#S3.E53 "在 III-C2 隐藏状态后验的变分推断
    ‣ III-C 环境的变分推断 ‣ III 深度强化学习中的 PGM 和 VI 分类 ‣ 深度强化学习中概率图模型和变分推断的教程与调查")) 可以通过类似于变分自编码器的架构[[31](#bib.bib31)]轻松表示，其中编码器$q_{\phi}$，结合重参数化技巧[[31](#bib.bib31)]，将观察状态$O$映射到Con-crete分布[[32](#bib.bib32)]的参数，因此可以在确定性变量上使用反向传播来更新网络的权重，基于ELBO，它被分解为变分自编码器架构的不同重建损失部分。像VIME
    [[4](#bib.bib4)]一样，VaSt可以与其他强化学习算法结合。这里在编码器输出的Heviside激活上直接进行优先级清扫[[12](#bib.bib12)]，通过计数过渡频率，而不是等待方程
    ([53](#S3.E53 "在 III-C2 隐藏状态后验的变分推断 ‣ III-C 环境的变分推断 ‣ III 深度强化学习中的 PGM 和 VI 分类
    ‣ 深度强化学习中概率图模型和变分推断的教程与调查")) 中缓慢学习的环境过渡模型 $p_{\theta^{T}}(s_{t}|s_{t-1},a_{t-1})$。这样做的一个潜在问题是潜在状态$s$和观察状态$o$之间的混叠。为缓解这个问题，在[[5](#bib.bib5)]中，作者一旦发现可观察变量被分配了不同的潜在离散状态，便主动重新标记回放记忆中的过渡历史。
- en: \includegraphics
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: \includegraphics
- en: '[scale=0.7]tikz_pgm_vast.pdf'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[scale=0.7]tikz_pgm_vast.pdf'
- en: 'Figure 9: Graphical Model for Variation State Tabulation'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：变化状态制表的图形模型
- en: IV Conclusion
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 结论
- en: As a tutorial survey, we recap Reinforcement Learning with Probabilistic Graphical
    Models, summarizes recent advances of Deep Reinforcement Learning and offer a
    taxonomy of Probabilistic Graphical Model and Variational Inference in DRL with
    detailed derivations which are not included in the original contributions.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项教程调查，我们回顾了结合概率图模型的强化学习，总结了深度强化学习的最新进展，并提供了深度强化学习中概率图模型和变分推断的分类，包含了原始贡献中未包括的详细推导。
- en: References
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p.
    529, 2015.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*，“通过深度强化学习实现人类水平控制，”
    *自然*，第518卷，第7540期，第529页，2015。'
- en: '[2] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International conference on machine learning*, 2015,
    pp. 1889–1897.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Schulman, S. Levine, P. Abbeel, M. Jordan 和 P. Moritz，“信任区域策略优化”，发表于
    *国际机器学习会议*，2015，第1889–1897页。'
- en: '[3] X. Sun, J. Lin, and B. Bischl, “Reinbo: Machine learning pipeline search
    and configuration with bayesian optimization embedded reinforcement learning,”
    2019.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. Sun, J. Lin 和 B. Bischl，“Reinbo: 机器学习管道搜索与配置，内嵌贝叶斯优化强化学习”，2019。'
- en: '[4] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel,
    “Vime: Variational information maximizing exploration,” in *Advances in Neural
    Information Processing Systems*, 2016, pp. 1109–1117.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, 和 P. Abbeel,
    “VIME：变分信息最大化探索，” 在 *神经信息处理系统进展*，2016年，第1109–1117页。'
- en: '[5] D. Corneil, W. Gerstner, and J. Brea, “Efficient model-based deep reinforcement
    learning with variational state tabulation,” *arXiv preprint arXiv:1802.04325*,
    2018.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. Corneil, W. Gerstner, 和 J. Brea, “基于模型的深度强化学习的高效变分状态表征，” *arXiv 预印本
    arXiv:1802.04325*，2018年。'
- en: '[6] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational inference:
    A review for statisticians,” *Journal of the American Statistical Association*,
    vol. 112, no. 518, pp. 859–877, 2017.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. M. Blei, A. Kucukelbir, 和 J. D. McAuliffe, “变分推断：统计学家的回顾，” *美国统计协会期刊*，第112卷，第518期，第859–877页，2017年。'
- en: '[7] C. M. Bishop, *Pattern recognition and machine learning*.   springer, 2006.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. M. Bishop, *模式识别与机器学习*。Springer，2006年。'
- en: '[8] A. Fischer and C. Igel, “Training restricted boltzmann machines: An introduction,”
    *Pattern Recognition*, vol. 47, pp. 25–39, 01 2014.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Fischer 和 C. Igel, “受限玻尔兹曼机的训练：简介，” *模式识别*，第47卷，第25–39页，2014年1月。'
- en: '[9] X. Sun, A. Gossmann, Y. Wang, and B. Bischl, “Variational resampling based
    assessment of deep neural networks under distribution shift,” 2019.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] X. Sun, A. Gossmann, Y. Wang, 和 B. Bischl, “在分布变化下基于变分重采样的深度神经网络评估，” 2019年。'
- en: '[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight uncertainty
    in neural networks,” *arXiv preprint arXiv:1505.05424*, 2015.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, 和 D. Wierstra, “神经网络中的权重不确定性，”
    *arXiv 预印本 arXiv:1505.05424*，2015年。'
- en: '[11] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] D. P. Kingma 和 M. Welling, “自编码变分贝叶斯，” *arXiv 预印本 arXiv:1312.6114*，2013年。'
- en: '[12] R. S. Sutton, A. G. Barto *et al.*, *Introduction to reinforcement learning*.   MIT
    press Cambridge, 1998, vol. 2, no. 4.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] R. S. Sutton, A. G. Barto *等*，*强化学习导论*。MIT Press Cambridge，1998年，第2卷，第4期。'
- en: '[13] S. Levine, “Reinforcement learning and control as probabilistic inference:
    Tutorial and review,” *arXiv preprint arXiv:1805.00909*, 2018.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Levine, “将强化学习和控制视为概率推断：教程与回顾，” *arXiv 预印本 arXiv:1805.00909*，2018年。'
- en: '[14] R. Zhao, X. Sun, and V. Tresp, “Maximum entropy-regularized multi-goal
    reinforcement learning,” *arXiv preprint arXiv:1905.08786*, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] R. Zhao, X. Sun, 和 V. Tresp, “最大熵正则化的多目标强化学习，” *arXiv 预印本 arXiv:1905.08786*，2019年。'
- en: '[15] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and acting
    in partially observable stochastic domains,” *Artificial i ntelligence*, vol.
    101, no. 1-2, pp. 99–134, 1998.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] L. P. Kaelbling, M. L. Littman, 和 A. R. Cassandra, “在部分可观察的随机领域中进行规划与行动，”
    *人工智能*，第101卷，第1-2期，第99–134页，1998年。'
- en: '[16] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning,” in *Thirtieth AAAI conference on artificial intelligence*,
    2016.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. Van Hasselt, A. Guez, 和 D. Silver, “使用双重 Q 学习的深度强化学习，” 在 *第三十届 AAAI
    人工智能会议*，2016年。'
- en: '[17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, 和 D. Wierstra, “使用深度强化学习的连续控制，” *arXiv 预印本 arXiv:1509.02971*，2015年。'
- en: '[18] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” 2014.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, 和 M. Riedmiller,
    “确定性策略梯度算法，” 2014年。'
- en: '[19] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International conference on machine learning*, 2016, pp. 1928–1937.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver, 和 K. Kavukcuoglu, “深度强化学习的异步方法，” 在 *国际机器学习大会*，2016年，第1928–1937页。'
- en: '[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, 和 O. Klimov, “近端策略优化算法，”
    *arXiv 预印本 arXiv:1707.06347*，2017年。'
- en: '[21] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Universal value function
    approximators,” in *International Conference on Machine Learning*, 2015, pp. 1312–1320.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] T. Schaul, D. Horgan, K. Gregor, 和 D. Silver, “通用价值函数逼近器，” 在 *国际机器学习大会*，2015年，第1312–1320页。'
- en: '[22] N. Kushwaha, X. Sun, B. Singh, and O. Vyas, “A lesson learned from pmf
    based approach for semantic recommender system,” *Journal of Intelligent Information
    Systems*, vol. 50, no. 3, pp. 441–453, 2018.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] N. Kushwaha, X. Sun, B. Singh, 和 O. Vyas, “从基于PMF的方法中获得的教训，用于语义推荐系统，”
    *智能信息系统杂志*，第50卷，第3期，第441–453页，2018年。'
- en: '[23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,
    B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba, “Hindsight experience replay,”
    in *Advances in Neural Information Processing Systems*, 2017, pp. 5048–5058.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,
    B. McGrew, J. Tobin, O. P. Abbeel, 和 W. Zaremba, “事后经验重放，” 发表在*神经信息处理系统进展*，2017年，第5048–5058页。'
- en: '[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *arXiv preprint arXiv:1511.05952*, 2015.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Schaul, J. Quan, I. Antonoglou, 和 D. Silver, “优先经验重放，” *arXiv 预印本 arXiv:1511.05952*，2015年。'
- en: '[25] X. Sun, A. Bommert, F. Pfisterer, J. Rahnenführer, M. Lang, and B. Bischl,
    “High dimensional restrictive federated model selection with multi-objective bayesian
    optimization over shifted distributions,” *arXiv preprint arXiv:1902.08999*, 2019.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] X. Sun, A. Bommert, F. Pfisterer, J. Rahnenführer, M. Lang, 和 B. Bischl,
    “高维限制性联邦模型选择与基于移位分布的多目标贝叶斯优化，” *arXiv 预印本 arXiv:1902.08999*，2019年。'
- en: '[26] S. Kakade and J. Langford, “Approximately optimal approximate reinforcement
    learning,” in *ICML*, vol. 2, 2002, pp. 267–274.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Kakade 和 J. Langford, “近似最优近似强化学习，” 发表在*ICML*，第2卷，2002年，第267–274页。'
- en: '[27] B. Sallans and G. E. Hinton, “Reinforcement learning with factored states
    and actions,” *Journal of Machine Learning Research*, vol. 5, no. Aug, pp. 1063–1088,
    2004.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] B. Sallans 和 G. E. Hinton, “具有分解状态和动作的强化学习，” *机器学习研究杂志*，第5卷，第8月，第1063–1088页，2004年。'
- en: '[28] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement learning
    with deep energy-based policies,” in *Proceedings of the 34th International Conference
    on Machine Learning-Volume 70*.   JMLR. org, 2017, pp. 1352–1361.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] T. Haarnoja, H. Tang, P. Abbeel, 和 S. Levine, “使用深度能量基策略的强化学习，” 发表在*第34届国际机器学习会议-第70卷*。JMLR.
    org，2017年，第1352–1361页。'
- en: '[29] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor,” *arXiv preprint
    arXiv:1801.01290*, 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] T. Haarnoja, A. Zhou, P. Abbeel, 和 S. Levine, “软演员-评论家：具有随机演员的离策略最大熵深度强化学习，”
    *arXiv 预印本 arXiv:1801.01290*，2018年。'
- en: '[30] Y. Sun, F. Gomez, and J. Schmidhuber, “Planning to be surprised: Optimal
    bayesian exploration in dynamic environments,” in *International Conference on
    Artificial General Intelligence*.   Springer, 2011, pp. 41–51.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Sun, F. Gomez, 和 J. Schmidhuber, “计划受到惊讶：动态环境中的最佳贝叶斯探索，” 发表在*人工智能通用智能国际会议*。Springer，2011年，第41–51页。'
- en: '[31] C. Doersch, “Tutorial on variational autoencoders,” *arXiv preprint arXiv:1606.05908*,
    2016.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Doersch, “变分自编码器教程，” *arXiv 预印本 arXiv:1606.05908*，2016年。'
- en: '[32] C. J. Maddison, A. Mnih, and Y. W. Teh, “The concrete distribution: A
    continuous relaxation of discrete random variables,” *arXiv preprint arXiv:1611.00712*,
    2016.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] C. J. Maddison, A. Mnih, 和 Y. W. Teh, “具体分布：离散随机变量的连续松弛，” *arXiv 预印本 arXiv:1611.00712*，2016年。'
