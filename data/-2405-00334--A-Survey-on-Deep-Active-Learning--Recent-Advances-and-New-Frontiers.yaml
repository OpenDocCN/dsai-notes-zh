- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:32:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:32:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2405.00334] A Survey on Deep Active Learning: Recent Advances and New Frontiers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2405.00334] 关于深度主动学习的综述：近期进展与新前沿'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.00334](https://ar5iv.labs.arxiv.org/html/2405.00334)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.00334](https://ar5iv.labs.arxiv.org/html/2405.00334)
- en: 'A Survey on Deep Active Learning: Recent Advances and New Frontiers'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于深度主动学习的综述：近期进展与新前沿
- en: 'Dongyuan Li, Zhen Wang, Yankai Chen, Renhe Jiang, Weiping Ding, , Manabu Okumura
    Dongyuan Li, Zhen Wang, Manabu Okumura are with the Institute of Innovative Research,
    School of Information and Communication Engineering, Tokyo Institute of Technology,
    Tokyo 152-8550, Japan. ({lidy,wzh}@lr.pi.titech.ac.jp, oku@pi.titech.ac.jp). (D.
    Li and Z. Wang contributed equally to this work). Yankai Chen is with the School
    of Computer Science and Engineering, The Chinese University of Hong Kong, (email:
    ykchen@cse.cuhk.edu.hk). Renhe Jiang is with the Center for Spatial Information
    Science, The University of Tokyo. Tokyo, Japan. (email: jiangrh@csis.u-tokyo.ac.jp).
    Weiping Ding is with the School of Information Science and Technology, Nantong
    University, Nantong 226019, China (e-mail: dwp9988@163.com) Corresponding authors:
    Weiping Ding and Manabu Okumura.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Dongyuan Li, Zhen Wang, Yankai Chen, Renhe Jiang, Weiping Ding, Manabu Okumura
    Dongyuan Li, Zhen Wang, Manabu Okumura 现任职于东京工业大学信息通信工程学院创新研究所，地址：日本东京 152-8550。（{lidy,wzh}@lr.pi.titech.ac.jp,
    oku@pi.titech.ac.jp）。(D. Li 和 Z. Wang 对这项工作贡献相同)。Yankai Chen 现任职于香港中文大学计算机科学与工程学院（电子邮件：ykchen@cse.cuhk.edu.hk）。Renhe
    Jiang 现任职于东京大学空间信息科学中心，地址：日本东京。（电子邮件：jiangrh@csis.u-tokyo.ac.jp）。Weiping Ding
    现任职于南通大学信息科学与技术学院，地址：中国南通 226019（电子邮件：dwp9988@163.com）。通讯作者：Weiping Ding 和 Manabu
    Okumura。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Active learning seeks to achieve strong performance with fewer training samples.
    It does this by iteratively asking an oracle to label new selected samples in
    a human-in-the-loop manner. This technique has gained increasing popularity due
    to its broad applicability, yet its survey papers, especially for deep learning-based
    active learning (DAL), remain scarce. Therefore, we conduct an advanced and comprehensive
    survey on DAL. We first introduce reviewed paper collection and filtering. Second,
    we formally define the DAL task and summarize the most influential baselines and
    widely used datasets. Third, we systematically provide a taxonomy of DAL methods
    from five perspectives, including annotation types, query strategies, deep model
    architectures, learning paradigms, and training processes, and objectively analyze
    their strengths and weaknesses. Then, we comprehensively summarize main applications
    of DAL in Natural Language Processing (NLP), Computer Vision (CV), and Data Mining
    (DM), etc. Finally, we discuss challenges and perspectives after a detailed analysis
    of current studies. This work aims to serve as a useful and quick guide for researchers
    in overcoming difficulties in DAL. We hope that this survey will spur further
    progress in this burgeoning field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习旨在通过更少的训练样本实现强大的性能。它通过迭代地请求一个“oracle”以人机协作的方式标记新的选择样本来实现这一目标。由于其广泛的适用性，这项技术越来越受欢迎，但关于基于深度学习的主动学习（DAL）的综述文献仍然稀缺。因此，我们进行了一项先进且全面的DAL综述。我们首先介绍了被审查论文的收集和筛选。其次，我们正式定义了DAL任务，并总结了最具影响力的基准方法和广泛使用的数据集。第三，我们从注释类型、查询策略、深度模型架构、学习范式和训练过程五个角度系统地提供了DAL方法的分类，并客观分析了它们的优缺点。然后，我们全面总结了DAL在自然语言处理（NLP）、计算机视觉（CV）和数据挖掘（DM）等领域的主要应用。最后，在详细分析当前研究之后，我们讨论了挑战和前景。这项工作旨在为研究人员在克服DAL中的困难提供一个有用且快捷的指南。我们希望这项综述能促进这一新兴领域的进一步发展。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Active learning, Deep learning, Natural language processing, Computer vision,
    Uncertainty quantification, Sequential optimal design, Adaptive sampling.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习，深度学习，自然语言处理，计算机视觉，不确定性量化，序列最优设计，自适应采样。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: 'The remarkable success of deep learning relies heavily on large-scale datasets
    with human-annotated labels [[1](#bib.bib1)]. However, continually labeling large-scale
    datasets is an extremely time-consuming, expensive, and laborious task, which
    tends to become a bottleneck for deep learning with limited labeled data. To tackle
    this issue, Deep Active Learning (DAL) recently exhibits great potential. As Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers") shows, DAL models are first trained on an initial training
    dataset. Then, query strategies can be iteratively applied to select the most
    informative and representative samples from a large pool of unlabeled data. Finally,
    an oracle labels the selected samples and adds them to the training dataset for
    retraining or fine-tuning of the DAL models. DAL aims to achieve competitive performance
    while reducing annotation costs within a reasonable time [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]. Benefiting from the strong representation capabilities of various
    neural networks, such as Graph Neural Networks (GNNs) [[5](#bib.bib5)], Convolutional
    Neural Networks (CNNs) [[6](#bib.bib6)], and Transformers [[7](#bib.bib7)], as
    well as leveraging prior knowledge from pre-trained models like Contrastive Language-Image
    Pre-Training (CLIP) [[8](#bib.bib8)] and Generative Pre-trained Transformer (GPT) [[9](#bib.bib9)],
    DAL has made significant advances.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习的显著成功在很大程度上依赖于带有人类标注标签的大规模数据集 [[1](#bib.bib1)]。然而，不断对大规模数据集进行标注是一个极其耗时、昂贵且繁琐的任务，这往往成为深度学习中的瓶颈，特别是在标注数据有限的情况下。为了解决这个问题，深度主动学习（DAL）最近展现了巨大的潜力。如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers")所示，DAL 模型首先在初始训练数据集上进行训练。然后，可以迭代地应用查询策略，从大量未标记的数据中选择最具信息量和代表性的样本。最后，由专家对选定的样本进行标注，并将其添加到训练数据集中，以便对
    DAL 模型进行再训练或微调。DAL 旨在在合理的时间内实现具有竞争力的性能，同时降低标注成本 [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]。凭借各种神经网络的强大表示能力，例如图神经网络（GNNs） [[5](#bib.bib5)]、卷积神经网络（CNNs） [[6](#bib.bib6)]和变换器 [[7](#bib.bib7)]，以及利用来自预训练模型的先验知识，如对比语言-图像预训练（CLIP） [[8](#bib.bib8)]和生成预训练变换器（GPT） [[9](#bib.bib9)]，DAL
    取得了显著进展。'
- en: '![Refer to caption](img/cfda53036970606f7f5c0be7de8fed75.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cfda53036970606f7f5c0be7de8fed75.png)'
- en: 'Figure 1: The general pipeline in deep active learning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：深度主动学习中的一般流程。
- en: As a methodology for selecting or generating a subset of training data in data-centric
    AI, DAL is closely related to learning settings and practical techniques, including
    curriculum learning [[10](#bib.bib10)], transfer learning [[11](#bib.bib11)],
    data augmentation or pruning [[12](#bib.bib12), [13](#bib.bib13)], and dataset
    distillation [[14](#bib.bib14)]. The commonality of these methods is to train
    or fine-tune a model using a small number of samples, aiming to remove noise and
    redundancy while improving training efficiency without decreasing models’ performance
    on downstream tasks. However, one primary difference from DAL is that these approaches
    have full access to all labels when selecting, distilling, or generating training
    subsets. DAL defaults to that all data should be unlabeled during the training
    subset selection process, making it better suited for real-world scenarios where
    labels are initially unavailable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据中心 AI 中选择或生成训练数据子集的方法论，DAL 与学习设置和实际技术密切相关，包括课程学习 [[10](#bib.bib10)]、迁移学习 [[11](#bib.bib11)]、数据增强或剪枝 [[12](#bib.bib12),
    [13](#bib.bib13)]和数据集蒸馏 [[14](#bib.bib14)]。这些方法的共同点是使用少量样本训练或微调模型，旨在去除噪声和冗余，同时提高训练效率，而不降低模型在下游任务上的性能。然而，与
    DAL 的主要区别在于，这些方法在选择、蒸馏或生成训练子集时可以完全访问所有标签。DAL 默认所有数据在训练子集选择过程中应为未标记，这使其更适合于标签最初不可用的现实场景。
- en: To summarize DAL methodologies, recent efforts have focused on specific tasks
    such as text classification [[15](#bib.bib15)] and image analysis [[16](#bib.bib16),
    [17](#bib.bib17)], specific domains like NLP [[18](#bib.bib18)] and CV [[19](#bib.bib19),
    [20](#bib.bib20)], or reproducing mainstream baselines [[21](#bib.bib21), [22](#bib.bib22)].
    As for most early survey work, one common inadequacy is that they may not have
    enough discussion of recent advances [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)],
    or lack summarization of emerging learning paradigms (contrastive learning etc.)
    and challenges [[26](#bib.bib26), [27](#bib.bib27)], especially in light of rapidly
    developing deep learning techniques (e.g., Fine-tune on pre-trained models). To
    assist researchers in reviewing, summarizing, and planning for future exploration,
    we provide a comprehensive review encompassing the latest advancements and insights
    in the field. While some survey papers focus on stream-based DAL [[28](#bib.bib28)],
    this paper concentrates on pool-based DAL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结DAL方法论，最近的工作集中于具体任务，如文本分类[[15](#bib.bib15)]和图像分析[[16](#bib.bib16), [17](#bib.bib17)]，特定领域如NLP[[18](#bib.bib18)]和CV[[19](#bib.bib19),
    [20](#bib.bib20)]，或重现主流基准[[21](#bib.bib21), [22](#bib.bib22)]。与大多数早期调查工作类似，一个常见的不足是它们可能对近期进展的讨论不够充分[[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)]，或者缺乏对新兴学习范式（对比学习等）和挑战[[26](#bib.bib26), [27](#bib.bib27)]的总结，特别是在深度学习技术（例如，基于预训练模型的微调）快速发展的背景下。为了帮助研究人员回顾、总结和规划未来的探索，我们提供了一份涵盖该领域最新进展和见解的综合性评述。虽然一些调查论文关注流式DAL[[28](#bib.bib28)]，但本文重点讨论了基于池的DAL。
- en: 'Specifically, we first introduce our strategy for collecting reviewed papers
    and explain our criteria for selecting them in Section [II](#S2 "II Paper Collection
    and Filtering ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers").
    Then, we give a specific formal definition for DAL in Section [III-A](#S3.SS1
    "III-A Notations & Definition ‣ III Deep Active Learning ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers"), and chronologically summarize the
    most influential DAL baselines and the widely used datasets in Section [III-C](#S3.SS3
    "III-C Important DAL Baselines and Datasets ‣ III Deep Active Learning ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers"). As Fig. [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers") shows, in Section [IV](#S4 "IV Taxonomy of DAL ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers"), we develop a high-level
    taxonomy to provide a broad overview of this field, categorizing previous studies
    from five perspectives. In Section [IV-A](#S4.SS1 "IV-A Annotation Type ‣ IV Taxonomy
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"),
    we classify the annotation types into hard, soft, hybrid, explanatory, and random/multi-agent
    annotations, and give a detailed introduction to each annotation type. In Section [IV-B](#S4.SS2
    "IV-B Query Strategy ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), we summarize query strategies into five distinct
    categories, including uncertainty-based, representative-based, influence-based,
    Bayesian-based and their hybrid methods, and analyze the strengths and weaknesses
    of each query type. As for deep model architectures, in Section [IV-C](#S4.SS3
    "IV-C Model Architecture ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), they are mainly categorized into Recurrent
    Neural Networks (RNNs), CNNs, GNNs, and Pre-trained methods. We discuss the benefits
    and drawbacks of each type of architecture. In Section [IV-D](#S4.SS4 "IV-D Learning
    Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers"), we are pleased to discover that various emerging learning
    paradigms, such as Curriculum Learning and Continual Learning, have shown promising
    results when combined with DAL. For each learning paradigm, we provide a detailed
    description of its definition and how to integrate it with DAL. Finally, in Section [IV-E](#S4.SS5
    "IV-E Training Process ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), three different training processes, including
    traditional training, curriculum learning-based training, and pre-training & fine-tuning
    will be introduced with typical examples.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，我们首先介绍我们收集审阅论文的策略，并解释我们选择论文的标准，详见第[II](#S2 "II Paper Collection and Filtering
    ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")节。然后，我们在第[III-A](#S3.SS1
    "III-A Notations & Definition ‣ III Deep Active Learning ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers")节中给出DAL的具体正式定义，并在第[III-C](#S3.SS3
    "III-C Important DAL Baselines and Datasets ‣ III Deep Active Learning ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers")节中按时间顺序总结了最有影响力的DAL基线和广泛使用的数据集。如图[2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers")所示，在第[IV](#S4 "IV Taxonomy of DAL ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers")节中，我们开发了一个高层次的分类法，以提供该领域的广泛概述，从五个角度对之前的研究进行分类。在第[IV-A](#S4.SS1
    "IV-A Annotation Type ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers")节中，我们将注释类型分为硬性、软性、混合、解释性和随机/多代理注释，并对每种注释类型进行了详细介绍。在第[IV-B](#S4.SS2
    "IV-B Query Strategy ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers")节中，我们将查询策略总结为五个不同的类别，包括基于不确定性、基于代表性、基于影响、基于贝叶斯及其混合方法，并分析每种查询类型的优缺点。至于深度模型架构，在第[IV-C](#S4.SS3
    "IV-C Model Architecture ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers")节中，它们主要分为递归神经网络（RNNs）、卷积神经网络（CNNs）、图神经网络（GNNs）和预训练方法。我们讨论了每种架构的优缺点。在第[IV-D](#S4.SS4
    "IV-D Learning Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers")节中，我们很高兴地发现，诸如课程学习和持续学习等各种新兴学习范式在与DAL结合时表现出良好的前景。对于每种学习范式，我们提供了其定义的详细描述以及如何将其与DAL结合。最后，在第[IV-E](#S4.SS5
    "IV-E Training Process ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers")节中，将介绍三种不同的训练过程，包括传统训练、基于课程学习的训练以及预训练与微调，并提供典型示例。'
- en: '![Refer to caption](img/269d839502ba34ad8965f623d603e311.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/269d839502ba34ad8965f623d603e311.png)'
- en: 'Figure 2: Taxonomy for deep active learning methods.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：深度主动学习方法的分类法。
- en: '![Refer to caption](img/bb6194866d5be41b040418ca169db074.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/bb6194866d5be41b040418ca169db074.png)'
- en: 'Figure 3: Emerging challenges in deep active learning.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：深度主动学习中的新兴挑战。
- en: 'In Section [V](#S5 "V Applications of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), we comprehensively show some domains in which
    DAL methods have been successfully applied, including NLP, CV, DM, etc. As depicted
    in Fig. [3](#S1.F3 "Figure 3 ‣ I Introduction ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), despite the remarkable progress in DAL, this
    rapidly developing field is still fraught with several crucial emerging challenges.
    In Section [VI](#S6 "VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers"), we analyze the causes and opportunities
    of each challenge, which can be summarized as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[V](#S5 "V DAL 的应用 ‣ 深度主动学习调查：近期进展与新前沿")节中，我们全面展示了DAL方法成功应用的一些领域，包括NLP、CV、DM等。正如图[3](#S1.F3
    "图 3 ‣ I 引言 ‣ 深度主动学习调查：近期进展与新前沿")所示，尽管DAL取得了显著进展，但这一快速发展的领域仍面临着几个关键的新兴挑战。在第[VI](#S6
    "VI DAL 的挑战与机遇 ‣ 深度主动学习调查：近期进展与新前沿")节中，我们分析了每个挑战的原因和机遇，具体总结如下：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pipeline-related: inefficient & costly human annotation, insufficient research
    on stopping strategies, and cold-start;'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与流程相关：低效且成本高的人类标注、对停止策略的研究不足以及冷启动问题；
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task-related: difficulty in cross-domain transfer, unstable performance, and
    lack of scalability and generalizability;'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与任务相关：跨领域迁移的困难、不稳定的性能以及缺乏可扩展性和泛化能力；
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset-related: outlier data & oracles, data scarcity & imbalance, and class
    distribution mismatch.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与数据集相关：异常数据与oracle、数据稀缺与不平衡，以及类别分布不匹配。
- en: 'Finally, after organizing and summarizing the current DAL-related research,
    we have four intriguing findings that we would like to share with the readers:
    (1) As shown in Section [IV-E](#S4.SS5 "IV-E Training Process ‣ IV Taxonomy of
    DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"), DAL
    has great potential as a sample selection strategy to apply few-shot or one-shot
    setting for large-scale pre-trained models with billions of parameters [[29](#bib.bib29),
    [30](#bib.bib30)]. Furthermore, as discussed in Section [III-C](#S3.SS3 "III-C
    Important DAL Baselines and Datasets ‣ III Deep Active Learning ‣ A Survey on
    Deep Active Learning: Recent Advances and New Frontiers"), many studies have shown
    that using only 10$\sim$20% labeled samples for fine-tuning the pre-trained language
    models with billions of parameters can yield even better performance and be 5$\sim$10
    times more efficient than training with a full labeled dataset [[31](#bib.bib31),
    [32](#bib.bib32)]. (2) Intuitively, having more high-quality samples can promote
    model performance for some tasks. Thus, as shown in Section [IV-D](#S4.SS4 "IV-D
    Learning Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers"), many works integrate DAL with semi-supervised strategies,
    allowing to obtain more high-quality labeled samples without increasing the need
    for human labor. However, as discussed in Section [VI-C](#S6.SS3 "VI-C Dataset-related
    Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers"), semi-supervised methods are highly sensitive
    to outliers and error labels, easily fueling a vicious cycle, i.e., models continue
    to label samples with wrong pseudo-labels. How to effectively integrate DAL with
    semi-supervised strategies, using human-labeled true signals to guide semi-supervised
    annotation and avoid the mislabel circular, remains an open and challenging issue
    waiting to be solved. (3) From the detailed analysis of Scalability & Generalizability
    in Section [VI-B](#S6.SS2 "VI-B Task-related Issues ‣ VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"),
    although DAL has achieved great success in classification tasks, comparing various
    DAL methods to choose the optimal one for a given task remains time-intensive
    and unrealistic in practice. Thus, there is an urgent need for a universal framework
    that is friendly to various downstream tasks. (4) By summarizing DAL applications
    for NLP in Section [V-A](#S5.SS1 "V-A Applications in Natural Language Processing
    ‣ V Applications of DAL ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers"), we find only a few DAL studies focused on generative tasks. Generative
    tasks, such as summarization and question answering, urgently require more attention
    and research compared to classification tasks. This is because generating informative
    objects, such as annotations, is more difficult and time-consuming. Defining the
    most meaningful samples for generation tasks and explaining why those samples
    play an important role are two core problems that need to be solved. We hope that
    future research can promote the development of DAL for generation tasks.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，在整理和总结当前与DAL相关的研究后，我们有四个有趣的发现愿与读者分享：(1) 如第[IV-E](#S4.SS5 "IV-E Training
    Process ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers")节所示，DAL作为样本选择策略在大规模预训练模型（参数量达数十亿）的少样本或单样本设置中具有巨大的潜力[[29](#bib.bib29),
    [30](#bib.bib30)]。此外，如第[III-C](#S3.SS3 "III-C Important DAL Baselines and Datasets
    ‣ III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers")节所讨论，许多研究表明，使用仅10$\sim$20%的标记样本对具有数十亿参数的预训练语言模型进行微调，能够获得更好的性能，并且效率比用全标记数据集训练高出5$\sim$10倍[[31](#bib.bib31),
    [32](#bib.bib32)]。(2) 直观上，更多高质量样本可以提升模型在某些任务中的性能。因此，如第[IV-D](#S4.SS4 "IV-D Learning
    Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers")节所示，许多工作将DAL与半监督策略结合，获得更多高质量的标记样本，而无需增加人力劳动。然而，如第[VI-C](#S6.SS3
    "VI-C Dataset-related Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers")节所讨论，半监督方法对异常值和错误标签非常敏感，容易陷入恶性循环，即模型持续用错误的伪标签标记样本。如何有效地将DAL与半监督策略结合，利用人工标记的真实信号指导半监督标注，并避免误标循环，仍然是一个亟待解决的开放性挑战。(3)
    从第[VI-B](#S6.SS2 "VI-B Task-related Issues ‣ VI Challenges & Opportunities of
    DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")节对可扩展性和泛化性的详细分析来看，尽管DAL在分类任务中取得了巨大成功，但比较各种DAL方法以选择适合特定任务的最佳方法仍然耗时且在实际中不切实际。因此，迫切需要一个对各种下游任务友好的通用框架。(4)
    通过总结第[V-A](#S5.SS1 "V-A Applications in Natural Language Processing ‣ V Applications
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")节中DAL在NLP中的应用，我们发现仅有少数DAL研究集中于生成任务。与分类任务相比，生成任务（如摘要生成和问答）急需更多关注和研究。这是因为生成信息性对象（如注释）更具挑战性且耗时。定义生成任务中最有意义的样本并解释这些样本为何扮演重要角色是两个核心问题。我们希望未来的研究能够推动DAL在生成任务中的发展。'
- en: 'Overall, the main contributions of this paper are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，本文的主要贡献如下：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This is the latest comprehensive and systematic survey paper on DAL to help
    researchers review, summarize, and look forward to the future about DAL.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是最新的全面且系统的DAL调查论文，旨在帮助研究人员回顾、总结并展望DAL的未来。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Based on the novel DAL texonomy, we detail the explanations and discussions
    of the methodology, ranging from annotation types, query strategies, deep model
    architectures, learning paradigms, and training processes.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于最新的DAL分类法，我们详细解释和讨论了该方法的各个方面，包括标注类型、查询策略、深度模型架构、学习范式和训练过程。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The difficult challenges in DAL are presented from multiple perspectives. By
    a detailed analysis of challenges and current studies, we discuss possible advanced
    solutions for them.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从多个角度展示了DAL面临的困难挑战。通过对挑战和当前研究的详细分析，我们讨论了可能的先进解决方案。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A GitHub repository¹¹1[https://github.com/Clearloveyuan/Awesome-Active-Learning](https://github.com/Clearloveyuan/Awesome-Active-Learning)
    is available with the most up-to-date DAL techniques, including papers, code,
    and datasets.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个GitHub仓库¹¹1[https://github.com/Clearloveyuan/Awesome-Active-Learning](https://github.com/Clearloveyuan/Awesome-Active-Learning)提供了最新的DAL技术，包括论文、代码和数据集。
- en: 'Remaining part of this survey is organized as follows. Section [II](#S2 "II
    Paper Collection and Filtering ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers") shows the collection of DAL papers. Section [III](#S3 "III
    Deep Active Learning ‣ A Survey on Deep Active Learning: Recent Advances and New
    Frontiers") introduces important DAL baselines and datasets. Section [IV](#S4
    "IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New
    Frontiers") details the taxonomy of DAL methods. Section [V](#S5 "V Applications
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")
    reviews DAL-related applications. Section [VI](#S6 "VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")
    introduces DAL challenges and opportunities. Section [VII](#S7 "VII Conclusion
    ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers") ends this
    article with the conclusions.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的剩余部分组织如下。第[II](#S2 "II Paper Collection and Filtering ‣ A Survey on Deep
    Active Learning: Recent Advances and New Frontiers")节展示了DAL论文的收集情况。第[III](#S3
    "III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers")节介绍了重要的DAL基线和数据集。第[IV](#S4 "IV Taxonomy of DAL ‣ A Survey on
    Deep Active Learning: Recent Advances and New Frontiers")节详细说明了DAL方法的分类。第[V](#S5
    "V Applications of DAL ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers")节回顾了与DAL相关的应用。第[VI](#S6 "VI Challenges & Opportunities of DAL ‣
    A Survey on Deep Active Learning: Recent Advances and New Frontiers")节介绍了DAL的挑战和机会。第[VII](#S7
    "VII Conclusion ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")节总结了本文的结论。'
- en: II Paper Collection and Filtering
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 论文收集与筛选
- en: 'We first determine relevant keywords used to search articles and create an
    initial keyword list, as shown in Fig. [4](#S2.F4 "Figure 4 ‣ II Paper Collection
    and Filtering ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers").
    We perform searches across multiple databases using all possible 3-keyword combinations
    from defined keyword groups, such as “Active Learning”, “Machine Learning”, and
    “Open-set”. The databases searched include Google Scholar, Scopus, Semantic Scholar,
    and Web of Science. We limit the number of papers collected per query to 200,
    and the publication date ranges from January 2013 to March 2023.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先确定了用于搜索文章的相关关键词，并创建了初始关键词列表，如图[4](#S2.F4 "Figure 4 ‣ II Paper Collection
    and Filtering ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")所示。我们使用定义的关键词组中的所有可能的3关键词组合在多个数据库中进行搜索，如“主动学习”、“机器学习”和“开放集”。搜索的数据库包括Google
    Scholar、Scopus、Semantic Scholar和Web of Science。我们将每次查询收集的论文数量限制为200篇，出版日期范围为2013年1月到2023年3月。'
- en: '![Refer to caption](img/1ec6b438efc3f31738b9b42ed42770d9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ec6b438efc3f31738b9b42ed42770d9.png)'
- en: 'Figure 4: Keywords and publication trend on DAL.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图4: DAL的关键词及出版趋势。'
- en: 'We collect a total of 10,000 research papers from various sources and obtain
    3,967 unique papers after removing any duplicates. Fig. [4](#S2.F4 "Figure 4 ‣
    II Paper Collection and Filtering ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers") shows the trend of these articles over time, revealing a growing
    interest in the topic we are investigating. To ensure the relevance of the collected
    articles to DAL, we conduct a detailed manual inspection of their abstracts. As
    a result, we identify 1,273 articles that are considered interesting and pertinent
    for our study. Based on the collected materials, we employ these keywords to perform
    a final filtering process and also consider the reputation of conferences or journals
    in which the papers were published, as well as their impact. This approach further
    refines our dataset, resulting in 405 articles that are selected for systematic
    analysis, and 220 articles are finally summarized and discussed, focusing on their
    key findings and contributions. This rigorous analysis ensures that the articles
    are relevant and provide valuable insight into the field of DAL.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从各种来源收集了总计10,000篇研究论文，并在去除重复项后获得了3,967篇唯一论文。图[4](#S2.F4 "Figure 4 ‣ II Paper
    Collection and Filtering ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers")展示了这些文章随时间的趋势，揭示了我们研究主题的兴趣不断增长。为了确保收集到的文章与DAL的相关性，我们对其摘要进行了详细的人工检查。结果，我们识别出1,273篇对我们的研究有趣且相关的文章。基于收集的材料，我们使用这些关键词进行最终筛选，同时考虑论文发表的会议或期刊的声誉及其影响力。这种方法进一步精炼了我们的数据集，最终选择了405篇文章进行系统分析，并最终总结和讨论了220篇文章，重点关注它们的关键发现和贡献。这一严格的分析确保了文章的相关性，并为DAL领域提供了宝贵的见解。'
- en: III Deep Active Learning
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度主动学习
- en: In this section, we first introduce the basic notation and definition of DAL
    and then discuss the most important DAL baselines based on their relevance and
    chronological order.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先介绍DAL的基本符号和定义，然后根据其相关性和时间顺序讨论最重要的DAL基准。
- en: Algorithm 1 DAL procedure.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 DAL过程。
- en: 'Input: Unlabeled Data $\mathcal{D}_{\textbf{pool}}$ Parameter: Batch Size $b$,
    Iteration Times $T$, Query Function $\alpha$ Output: The final trained model $\mathcal{M}$'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：未标记数据$\mathcal{D}_{\textbf{pool}}$ 参数：批量大小$b$，迭代次数$T$，查询函数$\alpha$ 输出：最终训练的模型$\mathcal{M}$
- en: 1:  $\mathcal{Q}_{0}\leftarrow$ Initialization sampling from $\mathcal{D}_{\textbf{pool}}$
    where $|\mathcal{Q}_{0}|=b$;2:  $\mathcal{D}_{\textbf{train}}^{0}\leftarrow\mathcal{Q}_{0}$
    [Initialization of training dataset];3:  $\mathcal{M}_{0}\leftarrow$ Train $\mathcal{M}_{0}$
    on $\mathcal{D}_{\textbf{train}}^{0}$;4:  while not stop-criterion( ) $\&amp;$
    i $\leq$  $T$ do5:     $\mathcal{Q}_{i}$  $\leftarrow$  $\alpha(\mathcal{M}_{i-1},\mathcal{D}_{\textbf{pool}}^{i-1},b$)
      [Annotating $b$ samples];6:     $\mathcal{D}_{\textbf{train}}^{i}$  =  $\mathcal{D}_{\textbf{train}}^{i-1}$  $\cup$  $\mathcal{Q}_{i}$;
    $\mathcal{D}_{\textbf{pool}}^{i}\leftarrow\mathcal{D}_{\textbf{pool}}^{i-1}\backslash\mathcal{Q}_{i}$;7:     $\mathcal{M}_{i}\leftarrow$
    Train $\mathcal{M}_{i-1}$ on $\mathcal{D}_{\textbf{train}}^{i}$;8:  end while
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  $\mathcal{Q}_{0}\leftarrow$ 从$\mathcal{D}_{\textbf{pool}}$进行初始化采样，其中$|\mathcal{Q}_{0}|=b$；2:  $\mathcal{D}_{\textbf{train}}^{0}\leftarrow\mathcal{Q}_{0}$
    [训练数据集的初始化]；3:  $\mathcal{M}_{0}\leftarrow$ 在$\mathcal{D}_{\textbf{train}}^{0}$上训练$\mathcal{M}_{0}$；4:  当not
    stop-criterion() $\&amp;$ i $\leq$  $T$时执行5:     $\mathcal{Q}_{i}$  $\leftarrow$  $\alpha(\mathcal{M}_{i-1},\mathcal{D}_{\textbf{pool}}^{i-1},b$)
    [标注$b$个样本]；6:     $\mathcal{D}_{\textbf{train}}^{i}$ = $\mathcal{D}_{\textbf{train}}^{i-1}$  $\cup$  $\mathcal{Q}_{i}$;
    $\mathcal{D}_{\textbf{pool}}^{i}\leftarrow\mathcal{D}_{\textbf{pool}}^{i-1}\backslash\mathcal{Q}_{i}$；7:     $\mathcal{M}_{i}\leftarrow$
    在$\mathcal{D}_{\textbf{train}}^{i}$上训练$\mathcal{M}_{i-1}$；8:  结束 while'
- en: III-A Notations & Definition
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 符号与定义
- en: 'We focus on pool-based DAL methods since most DAL methods belong to this category.
    Pool-based DAL methods iteratively select the most informative samples from a
    large pool of unlabeled datasets until either the base model reaches a certain
    level of performance or a pre-defined budget is exhausted. As shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers"), we use a classification task as an example for illustration,
    while other tasks follow the typical definition of their task domains. Given an
    initial labeled training dataset $\mathcal{D}_{\textbf{train}}=\{\bm{x}_{i},y_{i}\}_{i=1}^{m}$
    and a large-scale pool of unlabeled data $\mathcal{D}_{\textbf{pool}}=\{\bm{x}_{i}\}_{i=1}^{n}$,
    where m$\ll$n, $\bm{x}_{i}$ represents the feature vector of the $i$-th sample,
    and $y_{i}\in\{0,1\}$ is the class label for binary classification (or $y_{i}\in\{1,\dots,k\}$
    for multi-label classification), the DAL procedure is carried out in $T$ iterations.
    In the $i$-th iteration, a batch of samples $\mathcal{Q}^{i}$ with batch size
    $b$ is selected from $\mathcal{D}_{\textbf{pool}}^{i-1}$ on the basis of the base
    model $\mathcal{M}$ and an acquisition function $\alpha(\,)$. These samples $\mathcal{Q}^{i}$
    are then labeled by an oracle and added to the $i$-th training dataset $\mathcal{D}^{i}_{\textbf{train}}$,
    with which the model $\mathcal{M}$ is then re-trained. DAL terminates when the
    labeled budget $Q$ is exhausted or the desired performance of the model is reached.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '我们关注于基于池的深度主动学习方法，因为大多数深度主动学习方法属于这一类别。基于池的深度主动学习方法会从一个大的未标记数据池中迭代地选择最具信息量的样本，直到基础模型达到一定的性能水平或预定义的预算用尽。如算法[1](#alg1
    "Algorithm 1 ‣ III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers")所示，我们以分类任务作为示例进行说明，而其他任务遵循其任务领域的典型定义。给定一个初始的标记训练数据集$\mathcal{D}_{\textbf{train}}=\{\bm{x}_{i},y_{i}\}_{i=1}^{m}$和一个大规模的未标记数据池$\mathcal{D}_{\textbf{pool}}=\{\bm{x}_{i}\}_{i=1}^{n}$，其中m$\ll$n，$\bm{x}_{i}$代表第$i$个样本的特征向量，$y_{i}\in\{0,1\}$是二分类的类别标签（或$y_{i}\in\{1,\dots,k\}$为多标签分类），深度主动学习过程在$T$次迭代中进行。在第$i$次迭代中，根据基础模型$\mathcal{M}$和获取函数$\alpha(\,)$从$\mathcal{D}_{\textbf{pool}}^{i-1}$中选择一批大小为$b$的样本$\mathcal{Q}^{i}$。这些样本$\mathcal{Q}^{i}$然后由oracle标记，并添加到第$i$个训练数据集$\mathcal{D}^{i}_{\textbf{train}}$中，模型$\mathcal{M}$随后进行重新训练。当标记预算$Q$用尽或模型达到预期的性能时，深度主动学习终止。'
- en: III-B Comparisons between Traditional and Deep AL
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 传统主动学习与深度主动学习的比较
- en: 'The differences between traditional and Deep AL mainly lie in the following
    two aspects: (1) most traditional AL methods use fixed pre-processed features
    to calculate uncertainty/representativeness. In deep learning tasks, feature representations
    are jointly learned with Deep Neural Networks (DNNs). Therefore, feature representations
    dynamically change during DAL processes, and thus pairwise distances/similarities
    used by representativeness-based measures need to be re-computed in every stage.
    In contrast, for traditional AL with classical ML tasks, these pairwise terms
    should be pre-computed [[22](#bib.bib22)]. (2) DAL can leverage advanced large-scale
    pre-trained language models to achieve comparable performance in few-shot or one-shot
    settings. In contrast, traditional AL methods with few-shot or one-shot settings
    may not meet the minimum requirements for the number of training samples needed
    to achieve comparable performance [[30](#bib.bib30), [33](#bib.bib33)]. On the
    other hand, the most similar aspect between traditional and deep AL methods is
    their utilization of a small number of the most informative samples to train models,
    thereby improving efficiency and reducing reliance on labeled samples.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 传统主动学习和深度主动学习之间的差异主要体现在以下两个方面：（1）大多数传统的主动学习方法使用固定的预处理特征来计算不确定性/代表性。在深度学习任务中，特征表示是与深度神经网络（DNNs）共同学习的。因此，特征表示在深度主动学习过程中动态变化，因此基于代表性的测量中使用的成对距离/相似度需要在每个阶段重新计算。相比之下，对于传统的主动学习与经典机器学习任务，这些成对的项应该是预先计算的[[22](#bib.bib22)]。（2）深度主动学习可以利用先进的大规模预训练语言模型在少样本或一-shot
    设置下实现可比的性能。相比之下，传统的主动学习方法在少样本或一-shot 设置下可能无法满足实现可比性能所需的最小训练样本数量[[30](#bib.bib30),
    [33](#bib.bib33)]。另一方面，传统主动学习和深度主动学习方法之间最相似的方面是它们利用少量最具信息量的样本来训练模型，从而提高效率并减少对标记样本的依赖。
- en: III-C Important DAL Baselines and Datasets
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 重要的深度主动学习基准和数据集
- en: 'The most important baselines for DAL are carefully categorized in Table [I](#S3.T1
    "TABLE I ‣ III-C Important DAL Baselines and Datasets ‣ III Deep Active Learning
    ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers") from six
    perspectives to provide readers with a complete understanding of the development
    of DAL and the identification of the most relevant works. These influential studies
    have achieved breakthroughs in designing new DAL methods, tackling novel tasks,
    or integrating with emerging learning paradigms. They have been published in influential
    international conferences or high-quality journals in machine learning, CV, NLP,
    etc., and have been highly cited with more than 100 total citations or more than
    10 citations per year.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '表[I](#S3.T1 "TABLE I ‣ III-C Important DAL Baselines and Datasets ‣ III Deep
    Active Learning ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")中对DAL最重要的基准进行了仔细分类，从六个角度为读者提供了对DAL发展的全面理解，并识别出最相关的工作。这些有影响力的研究在设计新的DAL方法、解决新任务或与新兴学习范式集成方面取得了突破。它们已在机器学习、计算机视觉、自然语言处理等领域的国际会议或高质量期刊上发表，并且被高度引用，总引用次数超过100次或每年引用次数超过10次。'
- en: 'TABLE I: Detailed taxonomy of important Deep Active Learning baselines. Refer
    to Section [IV](#S4 "IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers") for a detailed explanation of each category. Any
    Types in Query Strategy means the proposed frameworks can be combined with any
    types of DAL query strategies.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 重要深度主动学习基准的详细分类。有关每个类别的详细解释，请参见第[IV](#S4 "IV Taxonomy of DAL ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers)节。查询策略中的任何类型意味着提出的框架可以与任何类型的DAL查询策略结合使用。'
- en: '| Method | Query Strategy | Architecture | Learning Paradigm | Annotation |
    Training | Tasks |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 查询策略 | 架构 | 学习范式 | 注释 | 训练 | 任务 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| BCBA [2016] [[34](#bib.bib34)] | Bayesian | CNNs | Traditional | Hard | Traditional
    | Image Classification |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| BCBA [2016] [[34](#bib.bib34)] | 贝叶斯 | CNNs | 传统 | 难 | 传统 | 图像分类 |'
- en: '| DBAL [2017] [[35](#bib.bib35)] | Bayesian | CNNs | Semi-supervised Learning
    | Hard | Traditional | Image Classification |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| DBAL [2017] [[35](#bib.bib35)] | 贝叶斯 | CNNs | 半监督学习 | 难 | 传统 | 图像分类 |'
- en: '| CEAL [2017] [[36](#bib.bib36)] | Uncertainty | CNNs | Curriculum Learning
    | Hybrid | Curriculum | Image Classification |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| CEAL [2017] [[36](#bib.bib36)] | 不确定性 | CNNs | 课程学习 | 混合 | 课程 | 图像分类 |'
- en: '| ESNN [2017] [[37](#bib.bib37)] | Uncertainty | BNNs | Adversarial Learning
    | Hard | Traditional | Image Classification |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ESNN [2017] [[37](#bib.bib37)] | 不确定性 | BNNs | 对抗学习 | 难 | 传统 | 图像分类 |'
- en: '| PAL [2017] [[38](#bib.bib38)] | Uncertainty | BNNs | Reinforcement Learning
    | Hard | Traditional | Named Entity Recognition |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| PAL [2017] [[38](#bib.bib38)] | 不确定性 | BNNs | 强化学习 | 难 | 传统 | 命名实体识别 |'
- en: '| LAL [2017] [[39](#bib.bib39)] | Influence | Random Forest | Traditional |
    Hard | Traditional | Regression Tasks |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| LAL [2017] [[39](#bib.bib39)] | 影响 | 随机森林 | 传统 | 难 | 传统 | 回归任务 |'
- en: '| GAAL [2017] [[40](#bib.bib40)] | Uncertainty | GNNs | Adversarial Learning
    | Hard | Traditional | Image Classification |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| GAAL [2017] [[40](#bib.bib40)] | 不确定性 | GNNs | 对抗学习 | 难 | 传统 | 图像分类 |'
- en: '| CoreSet [2018] [[41](#bib.bib41)] | Representative | CNNs | Semi-supervised
    Learning | Hard | Traditional | Image Classification |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| CoreSet [2018] [[41](#bib.bib41)] | 代表性 | CNNs | 半监督学习 | 难 | 传统 | 图像分类 |'
- en: '| DFAL [2018] [[42](#bib.bib42)] | Uncertainty | CNNs | Adversarial Training
    | Hard | Traditional | Image Classification |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| DFAL [2018] [[42](#bib.bib42)] | 不确定性 | CNNs | 对抗训练 | 难 | 传统 | 图像分类 |'
- en: '| ASM [2019] [[43](#bib.bib43)] | Uncertainty | CNNs | Curriculum Learning
    | Hybrid | Curriculum | Objective Detection |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ASM [2019] [[43](#bib.bib43)] | 不确定性 | CNNs | 课程学习 | 混合 | 课程 | 目标检测 |'
- en: '| MIAL [2019] [[44](#bib.bib44)] | Representative | SVM | Traditional | Hard
    | Traditional | Image Classification |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| MIAL [2019] [[44](#bib.bib44)] | 代表性 | SVM | 传统 | 难 | 传统 | 图像分类 |'
- en: '| BatchBALD [2019] [[45](#bib.bib45)] | Uncertainty | BNNs | Traditional |
    Hard | Traditional | Image Classification |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| BatchBALD [2019] [[45](#bib.bib45)] | 不确定性 | BNNs | 传统 | 难 | 传统 | 图像分类 |'
- en: '| DRAL [2019] [[46](#bib.bib46)] | Uncertainty | CNNs | Reinforcement Learning
    | Hard | Pre+FT | Person Re-Identification |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| DRAL [2019] [[46](#bib.bib46)] | 不确定性 | CNNs | 强化学习 | 难 | 预训练+微调 | 人脸再识别
    |'
- en: '| DLER [2019] [[47](#bib.bib47)] | Uncertainty | PLMs | Transfer Learning |
    Hard | Pre+FT | Entity Resolution |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| DLER [2019] [[47](#bib.bib47)] | 不确定性 | PLMs | 迁移学习 | 难 | 预训练+微调 | 实体解析 |'
- en: '| BGADL [2019] [[48](#bib.bib48)] | Hybrid | BNNs | Semi-supervised Learning
    | Hard | Traditional | Image Classification |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| BGADL [2019] [[48](#bib.bib48)] | 混合型 | BNNs | 半监督学习 | 难 | 传统 | 图像分类 |'
- en: '| VAAL [2019] [[49](#bib.bib49)] | Representative | VAE | Adversarial Learning
    | Hard | Traditional | Image Classification |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| VAAL [2019] [[49](#bib.bib49)] | 代表性 | VAE | 对抗学习 | 难 | 传统 | 图像分类 |'
- en: '| AADA [2020] [[50](#bib.bib50)] | Hybrid | CNNs | Transfer Learning | Hard
    | Pre+FT | Object Detection |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| AADA [2020] [[50](#bib.bib50)] | 混合型 | CNNs | 迁移学习 | 难 | 预训练+微调 | 目标检测 |'
- en: '| CSAL [2020] [[51](#bib.bib51)] | Hybrid | CNNs | Traditional | Hard | Pre+FT
    | Image Classification |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| CSAL [2020] [[51](#bib.bib51)] | 混合型 | CNNs | 传统 | 难 | 预训练+微调 | 图像分类 |'
- en: '| SRAAL [2020] [[52](#bib.bib52)] | Uncertainty | CNNs | Adversarial Learning
    | Hard | Pre+FT | Image Classification |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| SRAAL [2020] [[52](#bib.bib52)] | 不确定性 | CNNs | 对抗学习 | 难 | 预训练+微调 | 图像分类
    |'
- en: '| ALPS [2020] [[31](#bib.bib31)] | Uncertainty | PLMs | Traditional | Hard
    | Pre+FT | Cold-start Issue |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ALPS [2020] [[31](#bib.bib31)] | 不确定性 | PLMs | 传统 | 难 | 预训练+微调 | 冷启动问题 |'
- en: '| Ein-Dor et al. [2020] [[53](#bib.bib53)] | Any Types | PLMs | Traditional
    | Hard | Pre+FT | Text Classification |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Ein-Dor et al. [2020] [[53](#bib.bib53)] | 任何类型 | PLMs | 传统 | 难 | 预训练+微调
    | 文本分类 |'
- en: '| TOD [2021] [[54](#bib.bib54)] | Uncertainty | CNNs | Traditional | Hard |
    Pre+FT | Image Classification |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| TOD [2021] [[54](#bib.bib54)] | 不确定性 | CNNs | 传统 | 难 | 预训练+微调 | 图像分类 |'
- en: '| Cluster-Margin [2021] [[55](#bib.bib55)] | Representative | CNNs | Traditional
    | Hard | Pre+FT | Image Classification |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Cluster-Margin [2021] [[55](#bib.bib55)] | 代表性 | CNNs | 传统 | 难 | 预训练+微调 |
    图像分类 |'
- en: '| LADA [2021] [[56](#bib.bib56)] | Uncertainty | CNNs | Semi-supervised Learning
    | Hard | Traditional | Image Classification |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LADA [2021] [[56](#bib.bib56)] | 不确定性 | CNNs | 半监督学习 | 难 | 传统 | 图像分类 |'
- en: '| TA-VAAL [2021] [[57](#bib.bib57)] | Influence | VAE | Adversarial Learning
    | Hard | Pre+FT | Image Classification |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| TA-VAAL [2021] [[57](#bib.bib57)] | 影响力 | VAE | 对抗学习 | 难 | 预训练+微调 | 图像分类
    |'
- en: '| Karamcheti et al. [2021] [[58](#bib.bib58)] | Hybrid | PLMs | Traditional
    | Hard | Pre+FT | Visual Question Answering |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Karamcheti et al. [2021] [[58](#bib.bib58)] | 混合型 | PLMs | 传统 | 难 | 预训练+微调
    | 视觉问答 |'
- en: '| MAML [2022] [[59](#bib.bib59)] | Any Types | PLMs | Meta Learning | Hard
    | Pre+FT | Text Classification |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| MAML [2022] [[59](#bib.bib59)] | 任何类型 | PLMs | 元学习 | 难 | 预训练+微调 | 文本分类 |'
- en: '| BATL [2022] [[32](#bib.bib32)] | Hybrid | PLMs | Traditional | Hard | Pre+FT
    | Text Classification |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| BATL [2022] [[32](#bib.bib32)] | 混合型 | PLMs | 传统 | 难 | 预训练+微调 | 文本分类 |'
- en: '| TYROGUE [2022] [[60](#bib.bib60)] | Hybrid | PLMs | Traditional | Hard |
    Pre+FT | Text Classification |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| TYROGUE [2022] [[60](#bib.bib60)] | 混合型 | PLMs | 传统 | 难 | 预训练+微调 | 文本分类 |'
- en: '| Schroder et al. [2022] [[61](#bib.bib61)] | Uncertainty | PLMs | Traditional
    | Hard | Pre+FT | Text Classification |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Schroder et al. [2022] [[61](#bib.bib61)] | 不确定性 | PLMs | 传统 | 难 | 预训练+微调
    | 文本分类 |'
- en: BCBA [[34](#bib.bib34)] pioneers the combination of AL with Bayesian neural
    networks (BNNs), using Monte Carlo dropout for a variational Bayesian approximation
    to apply for image classification. Based on this, DBAL [[35](#bib.bib35)] proposes
    an uncertainty-based query strategy for high-dimensional image classification.
    To expand number of labeled samples without increasing human labors, CEAL [[36](#bib.bib36)]
    combines DAL with semi-supervised strategies by assigning pseudo-labels to high-confidence
    samples while requesting annotations for the most uncertain samples. Relying on
    a single query strategy may lead to errors. Thus, ESNN [[37](#bib.bib37)] uses
    a deep ensemble of DNNs to measure sample uncertainty from multiple aspects and
    achieves good robustness for unbalanced datasets. However, the aforementioned
    methods are criticized for being less effective for batch DAL [[45](#bib.bib45)].
    To address this issue, CoreSet [[41](#bib.bib41)] selects informative batches
    that cover the whole data distribution and BatchBALD [[45](#bib.bib45)] uses mutual
    information to identify the most informative batches. And Cluster-Margin [[55](#bib.bib55)]
    aims to select informative and diverse mini batches to improve accuracy and efficiency.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: BCBA [[34](#bib.bib34)] 开创性地将自适应学习（AL）与贝叶斯神经网络（BNNs）结合，使用蒙特卡洛 dropout 进行变分贝叶斯近似以应用于图像分类。在此基础上，DBAL [[35](#bib.bib35)]
    提出了基于不确定性的查询策略用于高维图像分类。为了在不增加人工劳动力的情况下扩展标注样本的数量，CEAL [[36](#bib.bib36)] 通过为高置信度样本分配伪标签，同时请求对最不确定样本进行标注，将DAL与半监督策略结合起来。依赖单一查询策略可能会导致错误。因此，ESNN [[37](#bib.bib37)]
    使用深度神经网络（DNN）的深度集成，从多个方面测量样本的不确定性，并在不平衡数据集上实现了良好的鲁棒性。然而，上述方法被批评在批量DAL [[45](#bib.bib45)]
    上效果不佳。为了解决这个问题，CoreSet [[41](#bib.bib41)] 选择覆盖整个数据分布的信息丰富的批次，而BatchBALD [[45](#bib.bib45)]
    则使用互信息来识别最具信息量的批次。Cluster-Margin [[55](#bib.bib55)] 旨在选择信息丰富且多样的迷你批次，以提高准确性和效率。
- en: To better help DAL adjust to different tasks, reinforcement learning provides
    detailed rewards for dynamically controlling query strategies. For example, PAL [[38](#bib.bib38)]
    learns a deep reinforcement learning-based Q-network as an adaptive policy to
    select data samples for labeling. Similarly, DRAL [[46](#bib.bib46)] uses a reinforcement
    learning framework to dynamically adjust the acquisition function via rewards
    to obtain high-quality queries. UCBVI [[62](#bib.bib62)] provides a new modification
    to the Q-network formulation for reward-free exploration, significantly reducing
    query complexity. However, reinforcement learning requires a large amount of training
    data and human-designed rewards, which is difficult for many real-world applications.
    To address this issue, meta learning and transfer learning have become main solutions.
    LAL [[39](#bib.bib39)] trains a regressor to learn optimal query strategies for
    downstream tasks. MAML [[59](#bib.bib59)] combines meta learning and DAL by initializing
    an active learner with meta-learned parameters obtained through meta-training
    on tasks similar to the target task during DAL. DLER [[47](#bib.bib47)] designs
    an architecture to learn a transferable model from a high-resource setting to
    a low-resource one, allowing DAL to select a few informative samples based on
    the knowledge of the source domain. AADA [[50](#bib.bib50)] jointly considers
    domain alignment, uncertainty, and diversity for sample selection.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地帮助DAL适应不同的任务，强化学习提供了详细的奖励用于动态控制查询策略。例如，PAL [[38](#bib.bib38)] 学习了一个基于深度强化学习的Q网络，作为一种自适应策略来选择数据样本进行标注。类似地，DRAL [[46](#bib.bib46)]
    使用强化学习框架通过奖励动态调整获取函数，以获得高质量的查询。UCBVI [[62](#bib.bib62)] 为Q网络公式提供了一种新的修改，用于无奖励探索，显著降低了查询复杂度。然而，强化学习需要大量的训练数据和人工设计的奖励，这对于许多实际应用来说很困难。为了解决这个问题，元学习和迁移学习成为主要解决方案。LAL [[39](#bib.bib39)]
    训练一个回归器以学习下游任务的最佳查询策略。MAML [[59](#bib.bib59)] 通过在DAL过程中对与目标任务类似的任务进行元训练，结合元学习和DAL，初始化一个活跃学习者并使用通过元学习获得的参数。DLER [[47](#bib.bib47)]
    设计了一种架构，以从高资源环境中学习一个可迁移的模型到低资源环境中，从而使DAL能够基于源领域的知识选择少量信息丰富的样本。AADA [[50](#bib.bib50)]
    联合考虑领域对齐、不确定性和多样性来进行样本选择。
- en: To enlarge the labeled training dataset for DNNs without incurring additional
    human labor costs, semi-supervised, semi-supervised, and self-supervised DAL methods
    have been proposed. MIAL [[44](#bib.bib44)] pioneers semi-supervised DAL using
    cluster-based strategies to measure sample informativeness. ASM [[43](#bib.bib43)]
    collaborates with self-learning and DAL, designing a selector function to selectively
    and seamlessly determine the confidence of the samples, where high-confidence
    samples are labeled by a pseudo-labeling module, and low-confidence samples are
    labeled by humans. CSAL [[51](#bib.bib51)] first uses semi-supervised learning
    to distill information from unlabeled data during the training stage and then
    uses consistency-based sample selection for DAL. TOD [[54](#bib.bib54)] leverages
    a novel unlabeled data sampling strategy for data annotation in conjunction with
    a semi-supervised training scheme to improve the performance of the task model
    with unlabeled data. Recently, data augmentation has expanded to become a deep
    neural model that generates virtual instances to help expand training datasets.
    GAAL [[40](#bib.bib40)] introduces a generative adversarial network to the DAL
    query method to generate informative samples to train the model. BGADL [[48](#bib.bib48)]
    expands GAAL and combines generative adversarial DAL with Bayesian data augmentation
    to generate diverse and informative samples. DFAL [[42](#bib.bib42)] uses adversarial
    DAL to select samples close to the decision boundary as the most informative samples
    for DAL. VAAL [[49](#bib.bib49)] learns a latent space using a variational autoencoder
    (VAE) to generate new informative samples and trains an adversarial network to
    discriminate labeled and unlabeled data. Inspired by these works, TA-VAAL [[57](#bib.bib57)]
    incorporates a learning loss prediction module and a task ranker to enable task-aware
    sample selection. SRAAL [[52](#bib.bib52)] proposes a relabel adversarial model
    that aims to obtain the most informative unlabeled samples. LADA [[56](#bib.bib56)]
    anticipates data augmentation impact by scoring both real and virtually augmented
    instances, allowing training in informative labeled and augmented data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩大深度神经网络（DNN）的标注训练数据集而不增加额外的人力成本，提出了半监督、半监督和自监督的DAL方法。MIAL [[44](#bib.bib44)]
    开创了使用基于聚类的策略来衡量样本信息量的半监督DAL方法。ASM [[43](#bib.bib43)] 与自学习和DAL合作，设计了一个选择函数来有选择地确定样本的置信度，其中高置信度样本由伪标注模块标记，而低置信度样本则由人工标记。CSAL [[51](#bib.bib51)]
    首次使用半监督学习在训练阶段从未标记数据中提取信息，然后使用基于一致性的样本选择进行DAL。TOD [[54](#bib.bib54)] 利用新颖的未标记数据采样策略进行数据标注，并结合半监督训练方案，以提高任务模型在未标记数据上的性能。最近，数据增强已扩展到生成深度神经模型，以帮助扩展训练数据集。GAAL [[40](#bib.bib40)]
    引入了生成对抗网络到DAL查询方法中，以生成信息量丰富的样本来训练模型。BGADL [[48](#bib.bib48)] 扩展了GAAL，并结合了生成对抗DAL与贝叶斯数据增强，以生成多样化和信息丰富的样本。DFAL [[42](#bib.bib42)]
    使用对抗DAL选择接近决策边界的样本作为DAL的最有信息量的样本。VAAL [[49](#bib.bib49)] 使用变分自编码器（VAE）学习潜在空间，以生成新的信息丰富的样本，并训练对抗网络来区分标记和未标记的数据。受到这些工作的启发，TA-VAAL [[57](#bib.bib57)]
    结合了学习损失预测模块和任务排序器，以实现任务感知样本选择。SRAAL [[52](#bib.bib52)] 提出了一个重新标注对抗模型，旨在获取最有信息量的未标记样本。LADA [[56](#bib.bib56)]
    通过对真实和虚拟增强实例进行评分，预测数据增强的影响，从而允许在信息丰富的标记和增强数据上进行训练。
- en: Large-scale pre-trained language models (PLMs) achieve great success and become
    a milestone in artificial intelligence. Due to sophisticated pre-training objectives
    and huge model parameters, large-scale PLMs effectively captures knowledge from
    massive labeled and unlabeled data. DAL also ushers in a new paradigm by leveraging
    the prior knowledge in PLMs to enable few-shot or zero-shot learning for many
    downstream tasks. ALPS [[31](#bib.bib31)] extracts knowledge from PLMs to select
    the first batch of data using masked language modeling loss, which successfully
    solves the cold-start problem of DAL. Ein-Dor et al. [[53](#bib.bib53)] use multiple
    DAL methods to select samples for fine-tuning in BERT-based text classification.
    It achieves comparable or higher performance than fine-tuning on full datasets
    only with 10%$\sim$20% labeled samples. Karamcheti et al. [[58](#bib.bib58)] use
    DAL to identify and remove noisy data, select balanced samples to fine-tune PLMs,
    and achieve better performance in visual question-answering. BATL [[32](#bib.bib32)]
    is a task-independent batch acquisition method on a PLMs with triplet loss to
    determine hard samples, which have similar features but difficult to identify
    labels in an unlabeled data pool. TYROGUE [[60](#bib.bib60)] designs an interactive
    DAL framework to flexibly select samples to fine-tune PLMs for multiple low-resource
    tasks. Schroder et al. [[61](#bib.bib61)] extend the PLMs using available unlabeled
    data for greater adaptability and introduce effective fine-tuning for the robustness
    of DAL in low-resource and high-resource settings.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模预训练语言模型（PLMs）取得了巨大成功，成为人工智能领域的一个里程碑。由于复杂的预训练目标和庞大的模型参数，大规模PLMs能够有效地从海量的标注和未标注数据中捕捉知识。DAL也通过利用PLMs中的先验知识，为许多下游任务实现了少样本或零样本学习，开启了新的范式。ALPS [[31](#bib.bib31)]利用掩码语言建模损失从PLMs中提取知识，选择第一批数据，成功解决了DAL的冷启动问题。Ein-Dor等人 [[53](#bib.bib53)]使用多种DAL方法选择样本以微调基于BERT的文本分类模型，仅用10%~20%的标注样本便能获得与在全数据集上微调相当或更高的性能。Karamcheti等人 [[58](#bib.bib58)]使用DAL识别和去除噪声数据，选择平衡样本以微调PLMs，并在视觉问答中取得了更好的表现。BATL [[32](#bib.bib32)]是一种任务无关的批量采集方法，基于PLMs和三元组损失来确定硬样本，这些样本具有相似的特征但难以在未标注的数据池中识别标签。TYROGUE [[60](#bib.bib60)]设计了一种交互式DAL框架，灵活地选择样本以微调PLMs用于多个低资源任务。Schroder等人 [[61](#bib.bib61)]利用可用的未标注数据扩展PLMs，以提高适应性，并引入有效的微调以增强DAL在低资源和高资源环境下的鲁棒性。
- en: 'As shown in Table [II](#S3.T2 "TABLE II ‣ III-C Important DAL Baselines and
    Datasets ‣ III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers"), we also conclude the most widely used datasets in
    DAL including images, text, and audio.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[II](#S3.T2 "TABLE II ‣ III-C Important DAL Baselines and Datasets ‣ III
    Deep Active Learning ‣ A Survey on Deep Active Learning: Recent Advances and New
    Frontiers")所示，我们还总结了在DAL中最广泛使用的数据集，包括图像、文本和音频。'
- en: 'TABLE II: Widely used DAL dataset information.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 广泛使用的DAL数据集信息。'
- en: '| Dataset | Size | Domain | Tasks |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 大小 | 领域 | 任务 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MNIST [[6](#bib.bib6)] | 70,000 | Images | Classification |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MNIST [[6](#bib.bib6)] | 70,000 | 图像 | 分类 |'
- en: '| CIFAR-10 [[63](#bib.bib63)] | 60,000 | Images | Classification |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-10 [[63](#bib.bib63)] | 60,000 | 图像 | 分类 |'
- en: '| SVHN [[64](#bib.bib64)] | 600,000 | Images | Classification, Localization
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SVHN [[64](#bib.bib64)] | 600,000 | 图像 | 分类，定位 |'
- en: '| ImageNet [[65](#bib.bib65)] | 1.2M | Images | Classification, Detection |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet [[65](#bib.bib65)] | 1.2M | 图像 | 分类，检测 |'
- en: '| MSCOCO [[66](#bib.bib66)] | 123,287 | Images | Object detection |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| MSCOCO [[66](#bib.bib66)] | 123,287 | 图像 | 目标检测 |'
- en: '| Cityscapes [[67](#bib.bib67)] | 5,000 | Images | Semantic segmentation |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Cityscapes [[67](#bib.bib67)] | 5,000 | 图像 | 语义分割 |'
- en: '| Caltech-101 [[68](#bib.bib68)] | 9,000 | Images | Classification |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Caltech-101 [[68](#bib.bib68)] | 9,000 | 图像 | 分类 |'
- en: '| SST [[69](#bib.bib69)] | 11,855 | Text | Sentiment analysis |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SST [[69](#bib.bib69)] | 11,855 | 文本 | 情感分析 |'
- en: '| TREC [[70](#bib.bib70)] | 5,952 | Text | Question answering |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| TREC [[70](#bib.bib70)] | 5,952 | 文本 | 问答 |'
- en: '| SNLI [[71](#bib.bib71)] | 570,000 | Text | Natural language inference |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SNLI [[71](#bib.bib71)] | 570,000 | 文本 | 自然语言推理 |'
- en: '| IMDB [[72](#bib.bib72)] | 50,000 | Text | Sentiment analysis |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| IMDB [[72](#bib.bib72)] | 50,000 | 文本 | 情感分析 |'
- en: '| AGNews [[73](#bib.bib73)] | 31,900 | Text | Classification |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| AGNews [[73](#bib.bib73)] | 31,900 | 文本 | 分类 |'
- en: '| PubMed [[74](#bib.bib74)] | 19,717 | Text | Document classification |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| PubMed [[74](#bib.bib74)] | 19,717 | 文本 | 文档分类 |'
- en: '| YouTube-8M [[75](#bib.bib75)] | 237,000 | Audio | Classification |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| YouTube-8M [[75](#bib.bib75)] | 237,000 | 音频 | 分类 |'
- en: '| MIMIC-III [[76](#bib.bib76)] | 112,000 | Medical | Healthcare analytic |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| MIMIC-III [[76](#bib.bib76)] | 112,000 | 医疗 | 医疗保健分析 |'
- en: IV Taxonomy of DAL
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV DAL 分类
- en: IV-A Annotation Type
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 注释类型
- en: \adfhalfrightarrowhead
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Hard annotations provide one or multiple discrete categorical labels independently
    for each sample. For example, Citovsky et al. [[55](#bib.bib55)] annotate each
    image with a specific label such as “balloon” or “strawberry” for an image classification
    task. Wiechman et al. [[77](#bib.bib77)] design an online annotation system to
    assign multiple labels to long documents based on their sentiments, topics, and
    spam/non-spam status.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 硬注释为每个样本提供一个或多个离散的类别标签。例如，Citovsky 等人 [[55](#bib.bib55)] 为图像分类任务注释每张图片，标注特定的标签，如“气球”或“草莓”。Wiechman
    等人 [[77](#bib.bib77)] 设计了一个在线注释系统，根据文档的情感、主题和垃圾邮件/非垃圾邮件状态，为长文档分配多个标签。
- en: \adfhalfrightarrowhead
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Soft annotations allow continuous and subjective labels for samples. For instance,
    ReDAL [[78](#bib.bib78)] annotate continuous 2D region labels for 3D point clouds
    in semantic segmentation. Kothawade et al. [[79](#bib.bib79)] use mutual information
    as an auxiliary metric to select annotation regions in images for autonomous vehicles.
    Xie et al. [[80](#bib.bib80)] propose a region-based approach to automatically
    query a small subset of image regions to label while maximizing segmentation performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 软注释允许对样本进行连续和主观的标签。例如，ReDAL [[78](#bib.bib78)] 为 3D 点云的语义分割注释连续的 2D 区域标签。Kothawade
    等人 [[79](#bib.bib79)] 使用互信息作为辅助指标，以选择图像中的注释区域用于自动驾驶车辆。Xie 等人 [[80](#bib.bib80)]
    提出了基于区域的方法，自动查询图像区域的一个小子集进行标注，同时最大化分割性能。
- en: \adfhalfrightarrowhead
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Hybrid annotations combine automatic pseudo-labels of high-confidence predictions
    with human labeling of low-confidence samples in an iterative self-paced manner [[43](#bib.bib43)].
    For example, Wang et al. [[36](#bib.bib36)] propose a complementary sample selection
    strategy to progressively choose the most informative samples, pseudo-labeling
    high-confidence predictions for training. Yu et al. [[81](#bib.bib81)] jointly
    use the expertise of different annotation groups, inter-relations between workers,
    and label correlations within groups. By weighting groups, they reduce the impact
    of low-quality workers and calculate reliable consensus labels.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 混合注释将高置信度预测的自动伪标签与低置信度样本的人为标注结合在一个迭代自适应的方式中 [[43](#bib.bib43)]。例如，Wang 等人 [[36](#bib.bib36)]
    提出了一个补充样本选择策略，逐步选择最具信息量的样本，伪标签高置信度预测用于训练。Yu 等人 [[81](#bib.bib81)] 共同利用不同注释组的专业知识、工作者之间的相互关系以及组内标签的相关性。通过加权组，他们减少低质量工人的影响，并计算可靠的共识标签。
- en: \adfhalfrightarrowhead
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Explanatory annotations provide a hard or soft label along with an explanation
    for each annotation. For example, Schroder et al. [[82](#bib.bib82)] use topic-related
    annotations for environmental texts. Similarly, Yan et al.[[83](#bib.bib83)] annotate
    the text and list keywords as evidence of the accuracy of the label. Unlike the
    above methods, Zhou et al. [[84](#bib.bib84)] annotate samples by minimizing correlations
    between tasks and provide explainable medical knowledge to distinguish selected
    samples.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性注释为每个注释提供硬标签或软标签以及解释。例如，Schroder 等人 [[82](#bib.bib82)] 为环境文本使用与主题相关的注释。同样，Yan
    等人 [[83](#bib.bib83)] 注释文本并列出关键字作为标签准确性的证据。与上述方法不同，Zhou 等人 [[84](#bib.bib84)]
    通过最小化任务之间的相关性来注释样本，并提供可解释的医学知识以区分所选样本。
- en: \adfhalfrightarrowhead
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Random/multi-agent annotations use multiple independent pseudo-annotators to
    randomly label new unlabeled samples without human input [[85](#bib.bib85)]. For
    example, Gong et al. [[86](#bib.bib86)] use an agent team to collaboratively select
    informative images for annotation based on the decisions from the other agents.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 随机/多代理注释使用多个独立的伪注释者随机标记新的未标记样本，无需人工输入 [[85](#bib.bib85)]。例如，Gong 等人 [[86](#bib.bib86)]
    使用代理团队根据其他代理的决策协作选择信息量大的图像进行注释。
- en: IV-B Query Strategy
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 查询策略
- en: \adfhalfrightarrowhead
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Uncertainty-based methods aim to select the most ambiguous samples according
    to model predictions. Given an input $\bm{x}_{i}$:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不确定性的方法旨在根据模型预测选择最模糊的样本。给定输入 $\bm{x}_{i}$：
- en: '|  | $\text{Entropy}(\bm{x}_{i})=\mathop{\arg\max}\limits_{\bm{x}_{i}}(\sum_{j}P(\hat{y}_{j}&#124;\bm{x}_{i})\log
    P(\hat{y}_{j}&#124;\bm{x}_{i})),$ |  | (1) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Entropy}(\bm{x}_{i})=\mathop{\arg\max}\limits_{\bm{x}_{i}}(\sum_{j}P(\hat{y}_{j}&#124;\bm{x}_{i})\log
    P(\hat{y}_{j}&#124;\bm{x}_{i})),$ |  | (1) |'
- en: where $P(\hat{y}_{i}|\bm{x}_{i})$ represents the likelihood that $\bm{x}_{i}$
    is classified into the $i$-th class [[87](#bib.bib87)]. Uncertainty-based methods
    focus on designing various score functions to measure sample uncertainty and informativeness,
    including predictive entropy [[87](#bib.bib87)], least confidence [[88](#bib.bib88)],
    highest estimated dual variables [[89](#bib.bib89)], mutual information between
    model posterior and predictions [[79](#bib.bib79)]. Some strategies check samples
    near the decision boundary as the most uncertain ones [[90](#bib.bib90)], such
    as instances close to the hyperplane [[44](#bib.bib44)] or close to the margin [[91](#bib.bib91)].
    Others combine multiple query strategies, forming a query-by-committee [[92](#bib.bib92)]
    or disagreement-based [[93](#bib.bib93)] DAL strategy to decrease errors made
    by a single query strategy. With the development of adversarial learning, instead
    of selecting samples from unlabeled datasets, models tend to generate the most
    informative and uncertain synthetic samples to expand the training dataset [[48](#bib.bib48)].
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P(\hat{y}_{i}|\bm{x}_{i})$表示$\bm{x}_{i}$被分类为第$i$类的可能性[[87](#bib.bib87)]。基于不确定性的方法专注于设计各种评分函数来衡量样本的不确定性和信息量，包括预测熵[[87](#bib.bib87)]、最小置信度[[88](#bib.bib88)]、最高估计的双变量[[89](#bib.bib89)]、模型后验和预测之间的互信息[[79](#bib.bib79)]。一些策略检查决策边界附近的样本作为最不确定的样本[[90](#bib.bib90)]，例如接近超平面[[44](#bib.bib44)]或接近边界[[91](#bib.bib91)]的实例。其他方法结合多种查询策略，形成基于委员会[[92](#bib.bib92)]或基于争议[[93](#bib.bib93)]的DAL策略，以减少单一查询策略产生的错误。随着对抗学习的发展，模型趋向于生成最具信息量和不确定性的合成样本，以扩展训练数据集[[48](#bib.bib48)]。
- en: 'However, they have some common drawbacks: (1) redundant samples, as uncertain
    points, are continually selected yet in short of coverage; (2) simply focusing
    on a single sample lacks robustness to outliers; (3) these task-specific designs
    exhibit limited generalizability.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，它们有一些共同的缺点：（1）冗余样本作为不确定点被不断选择，但覆盖率不足；（2）仅关注单一样本对异常值的鲁棒性不足；（3）这些任务特定的设计表现出有限的泛化能力。
- en: \adfhalfrightarrowhead
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Representative-based methods aim to sample the most prototypical data points
    that effectively cover the distribution of the entire feature space. Existing
    methods can be categorized into density-based and diversity-based approaches.
    Density-based methods prefer to select samples that can represent all unlabeled
    samples. They use clustering methods to select cluster centers [[94](#bib.bib94)]
    as the most representative samples or select samples that can maximize probability
    coverage of the whole feature space of unlabeled datasets [[41](#bib.bib41)].
    For example, Kim et al. [[95](#bib.bib95)] design the density awareness coreset
    approach to estimate sample densities and preferentially select diverse points
    from sparse regions. Given the input $\bm{x}_{i}$:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 基于代表性的算法旨在采样最具代表性的数据显示整个特征空间的分布。现有方法可以分为基于密度和基于多样性的方法。基于密度的方法倾向于选择可以代表所有未标记样本的样本。它们使用聚类方法选择聚类中心[[94](#bib.bib94)]作为最具代表性的样本，或选择可以最大化未标记数据集的整个特征空间概率覆盖的样本[[41](#bib.bib41)]。例如，Kim
    等人[[95](#bib.bib95)]设计了密度感知核心集方法来估计样本密度，并优先选择来自稀疏区域的多样点。给定输入$\bm{x}_{i}$：
- en: '|  | $\text{Density}(\bm{x}_{i})=\frac{1}{k}\sum_{j\in\mathcal{N}(\bm{x}_{i},k)}\&#124;\bm{x}_{i}-\bm{x}_{j}\&#124;_{2}^{2},$
    |  | (2) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{密度}(\bm{x}_{i})=\frac{1}{k}\sum_{j\in\mathcal{N}(\bm{x}_{i},k)}\&#124;\bm{x}_{i}-\bm{x}_{j}\&#124;_{2}^{2},$
    |  | (2) |'
- en: 'where $\mathcal{N}(\bm{x}_{i},k)$ represents the $k$-nearest neighbors of $\bm{x}_{i}$ [[95](#bib.bib95)].
    Coleman et al. [[96](#bib.bib96)] and Gudovskiy et al. [[97](#bib.bib97)] achieve
    efficiency by only considering nearest neighbors rather than all data or matching
    feature densities with self-supervised methods. Diversity-based methods prefer
    to select samples that are different from the labeled samples. They use context-sensitive
    methods [[98](#bib.bib98)] that take into account the distance between a sample
    and its surrounding labeled samples to enrich the diversity of the labeled dataset.
    BMAL [[99](#bib.bib99)] performs DAL for the image labeling problem, where diversity
    is measured by the KL-divergence of the class probabilities distribution of similar
    neighboring instances, formulated as:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{N}(\bm{x}_{i},k)$ 表示 $\bm{x}_{i}$ 的 $k$-最近邻 [[95](#bib.bib95)]。Coleman
    等人 [[96](#bib.bib96)] 和 Gudovskiy 等人 [[97](#bib.bib97)] 通过只考虑最近邻而非所有数据或使用自监督方法匹配特征密度来实现高效。基于多样性的方法倾向于选择与标记样本不同的样本。它们使用上下文敏感的方法
    [[98](#bib.bib98)]，考虑样本与其周围标记样本之间的距离，以丰富标记数据集的多样性。BMAL [[99](#bib.bib99)] 针对图像标记问题执行DAL，其中多样性通过类似邻近实例的类别概率分布的KL散度来度量，其公式为：
- en: '|  | $\small\text{Divergence}(\bm{x}_{i},\bm{x}_{j})=\sum_{j}P(\hat{y}_{j}&#124;\bm{x}_{i})-P(\hat{y}_{j}&#124;\bm{x}_{j})\log\frac{P(\hat{y}_{j}&#124;\bm{x}_{i})}{P(\hat{y}_{j}&#124;\bm{x}_{j})}.$
    |  | (3) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\text{Divergence}(\bm{x}_{i},\bm{x}_{j})=\sum_{j}P(\hat{y}_{j}\mid\bm{x}_{i})-P(\hat{y}_{j}\mid\bm{x}_{j})\log\frac{P(\hat{y}_{j}\mid\bm{x}_{i})}{P(\hat{y}_{j}\mid\bm{x}_{j})}.$
    |  | (3) |'
- en: Other diversity-based methods tend to train a model, such as adversarial networks [[57](#bib.bib57)],
    contrastive networks [[100](#bib.bib100)], hierarchical clustering [[44](#bib.bib44)],
    and pre-trained models [[53](#bib.bib53)], to help discriminate labeled and unlabeled
    sets and select the most different unlabeled samples. For example, Li et al. [[101](#bib.bib101)]
    explicitly learn a non-linear embedding to select representative samples. Parvaneh
    et al. [[102](#bib.bib102)] explore neighborhoods around unlabeled data by interpolating
    features with labeled points. Li et al. [[103](#bib.bib103)] propose an acquisition
    function that measures mutual information between a batch of queries to encourage
    diversity. To further increase label efficiency, Citovsky et al. [[55](#bib.bib55)]
    use hierarchical clustering to diversify batches, requiring only 40% of the labels
    to achieve the same target performance. However, since they use ResNet-101 as
    their backbone, which contains only 170 MB parameters, more than 20% labeled samples
    are required for fine-tuning the model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其他基于多样性的方法倾向于训练模型，例如对抗网络 [[57](#bib.bib57)]、对比网络 [[100](#bib.bib100)]、层次聚类 [[44](#bib.bib44)]
    和预训练模型 [[53](#bib.bib53)]，以帮助区分标记集和未标记集，并选择最不同的未标记样本。例如，Li 等人 [[101](#bib.bib101)]
    显式地学习非线性嵌入以选择代表性样本。Parvaneh 等人 [[102](#bib.bib102)] 通过与标记点插值特征来探索未标记数据周围的邻域。Li
    等人 [[103](#bib.bib103)] 提出了一种度量查询批次之间互信息的获取函数，以促进多样性。为了进一步提高标签效率，Citovsky 等人 [[55](#bib.bib55)]
    使用层次聚类来多样化批次，只需 40% 的标签即可达到相同的目标性能。然而，由于它们使用的 ResNet-101 作为主干网络仅包含 170 MB 的参数，因此需要超过
    20% 的标记样本来微调模型。
- en: However, the aforementioned representative-based methods, which solely focus
    on sampling diverse samples, are always insensitive to samples that are close
    to the decision boundary (excluding hybrid methods that jointly consider representative
    and uncertainty), despite the fact that such samples are probably more important
    to the prediction model, as suggested by Zhao et al. [[104](#bib.bib104)]. In
    addition, representative-based methods work well for a small sample of data and
    classifiers with a small number of classes since their computational complexity
    is almost quadratic with respect to data size [[55](#bib.bib55)].
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述仅关注样本多样性的代表性方法对于靠近决策边界的样本总是较为迟钝（排除那些同时考虑代表性和不确定性的混合方法），尽管 Zhao 等人 [[104](#bib.bib104)]
    提出这些样本可能对预测模型更为重要。此外，代表性方法在处理小样本数据和类别较少的分类器时效果较好，因为它们的计算复杂度几乎是数据大小的二次方 [[55](#bib.bib55)]。
- en: \adfhalfrightarrowhead
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Influence-based methods aim to select samples that will have the greatest impact
    on the performance of the target model. These techniques can be categorized into
    three main groups. (1) The first group is directly measuring the expected impact
    on the modal through metrics such as gradient norm [[105](#bib.bib105)], query
    complexity [[106](#bib.bib106)], kernel approximation [[107](#bib.bib107)], KL
    divergence [[97](#bib.bib97)], change of loss function [[108](#bib.bib108)], or
    model parameters [[54](#bib.bib54)], and expected error reduction (EER) [[109](#bib.bib109)].
    Specifically, EER can be formulated as
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 基于影响的方法旨在选择对目标模型性能影响最大的样本。这些技术可以分为三大类。(1) 第一类是通过梯度范数 [[105](#bib.bib105)], 查询复杂性 [[106](#bib.bib106)],
    核近似 [[107](#bib.bib107)], KL散度 [[97](#bib.bib97)], 损失函数变化 [[108](#bib.bib108)],
    或模型参数 [[54](#bib.bib54)], 以及预期误差减少 (EER) [[109](#bib.bib109)]等指标直接测量对模型的预期影响。具体而言，EER可以表述为
- en: '|  | $\small\text{EER}(\bm{x}_{i})=\mathbb{E}_{\bm{x}_{s}}\{\mathbb{E}_{y_{i}&#124;\bm{x}_{i}}[\max_{y_{s}}p(y_{s}&#124;\bm{x}_{s},\bm{x}_{i},y_{i})]-\max_{y_{s}}p(y_{s}&#124;\bm{x}_{s})\},$
    |  | (4) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\text{EER}(\bm{x}_{i})=\mathbb{E}_{\bm{x}_{s}}\{\mathbb{E}_{y_{i}&#124;\bm{x}_{i}}[\max_{y_{s}}p(y_{s}&#124;\bm{x}_{s},\bm{x}_{i},y_{i})]-\max_{y_{s}}p(y_{s}&#124;\bm{x}_{s})\},$
    |  | (4) |'
- en: where $\bm{x}_{s}$ refers to the labeled sample. (2) The second group is incorporating
    different learning policies, such as reinforcement learning and imitation learning,
    to select samples based on reward signals or demonstrated actions. Despite the
    promising advantages, this requires significant additional training [[110](#bib.bib110)].
    For example, Wertz et al. [[111](#bib.bib111)] propose reinforced DAL, a reinforcement
    learning policy that uses multiple elements of the data and the task to dynamically
    pick the most useful unlabeled subset during the DAL process; (3) The last group
    is training a separate model to estimate the impact on the target model [[89](#bib.bib89)].
    For example, Peng et al. [[14](#bib.bib14)] propose a knowledge distillation framework
    to evaluate the impact of samples based on the knowledge learned by the student
    model. Elenter et al. [[89](#bib.bib89)] use the dual variables of the original
    model to measure the impact on the target model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{x}_{s}$ 指标记样本。(2) 第二类是结合不同的学习策略，如强化学习和模仿学习，根据奖励信号或示范动作选择样本。尽管这些方法具有很大的潜力，但需要额外的训练 [[110](#bib.bib110)]。例如，Wertz等人 [[111](#bib.bib111)]
    提出了强化DAL，这是一种强化学习策略，利用数据和任务的多个元素在DAL过程中动态选择最有用的未标记子集；(3) 最后一类是训练一个单独的模型来估计对目标模型的影响 [[89](#bib.bib89)]。例如，Peng等人 [[14](#bib.bib14)]
    提出了一个知识蒸馏框架，以根据学生模型学到的知识评估样本的影响。Elenter等人 [[89](#bib.bib89)] 使用原始模型的对偶变量来衡量对目标模型的影响。
- en: However, despite recent advances, influence-based DAL remains challenging. Directly
    measuring model changes or incorporating new learning policies always requires
    huge time and space costs, and training a new model will over-rely on its accuracy
    and often lead to unstable results.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管近期有所进展，基于影响的DAL仍然具有挑战性。直接测量模型变化或结合新的学习策略总是需要巨大的时间和空间成本，训练新模型则过度依赖其准确性，常常导致结果不稳定。
- en: \adfhalfrightarrowhead
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Bayesian methods aim to minimize classification errors and improve model beliefs
    by leveraging Bayes’ rule. Most studies have treated Bayesian models (e.g., Gaussian
    process [[109](#bib.bib109)], BNNs [[35](#bib.bib35)], Bayesian probabilistic
    ensemble [[112](#bib.bib112)]) as uncertainty-based methods, using them to estimate
    the informativeness of the sample. However, Bayesian DAL is better viewed as its
    own distinct system, with methods that select batches by directly measuring impact
    on the target model, such as BatchBALD [[45](#bib.bib45)] and Causal-BALD [[113](#bib.bib113)].
    For example, we define a Bayesian model with model parameters $\bm{w}\sim p(\bm{w}|\mathcal{D}_{\text{train}})$,
    and BALD can be defined to estimate the mutual information between the model predictions
    and the model parameters, formulated as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法旨在通过利用贝叶斯规则来最小化分类错误并改善模型信念。大多数研究将贝叶斯模型（例如，Gaussian process [[109](#bib.bib109)],
    BNNs [[35](#bib.bib35)], Bayesian probabilistic ensemble [[112](#bib.bib112)]）视为基于不确定性的方法，用于估计样本的信息量。然而，贝叶斯DAL更应被视为独立的系统，其方法通过直接测量对目标模型的影响来选择批次，例如BatchBALD [[45](#bib.bib45)]和Causal-BALD [[113](#bib.bib113)]。例如，我们定义一个贝叶斯模型，模型参数为
    $\bm{w}\sim p(\bm{w}|\mathcal{D}_{\text{train}})$，而BALD可以定义为估计模型预测与模型参数之间的互信息，公式为：
- en: '|  | $\small\mathbb{I}(y;\bm{w}&#124;\bm{x},\mathcal{D}_{\text{train}})=\mathbb{H}(y&#124;\bm{x},\mathcal{D}_{\text{train}})-\mathbb{E}_{p(\bm{w}&#124;\mathcal{D}_{\text{train}})}[\mathbb{H}(y&#124;\bm{x},\bm{w},\mathcal{D}_{\text{train}})],$
    |  | (5) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathbb{I}(y;\bm{w}&#124;\bm{x},\mathcal{D}_{\text{train}})=\mathbb{H}(y&#124;\bm{x},\mathcal{D}_{\text{train}})-\mathbb{E}_{p(\bm{w}&#124;\mathcal{D}_{\text{train}})}[\mathbb{H}(y&#124;\bm{x},\bm{w},\mathcal{D}_{\text{train}})],$
    |  | (5) |'
- en: where $\mathbb{H}$ represents the entropy and $\mathbb{E}$ is the expectation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbb{H}$表示熵，$\mathbb{E}$是期望。
- en: Compared to standard DNNs, the aforementioned Bayesian DAL methods, which leverage
    the advantages of probabilistic graphical theory [[35](#bib.bib35)], can often
    provide reasonable explanations for why these samples should be selected [[45](#bib.bib45)].
    However, they often require extensive accurate prior knowledge and tend to underperform
    deep learning models in representation learning and fitting capacity.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准深度神经网络（DNNs）相比，上述贝叶斯DAL方法，利用了概率图理论的优势[[35](#bib.bib35)]，通常可以提供合理的解释，说明为什么这些样本应该被选择[[45](#bib.bib45)]。然而，它们通常需要大量准确的先验知识，并且在表征学习和拟合能力方面往往不如深度学习模型。
- en: \adfhalfrightarrowhead
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Hybrid methods aim to take advantage of the above multiple query strategies
    and to achieve a trade-off among them. Hybrid methods can be further categorized
    according to interaction patterns. Serial-form hybrids apply criteria sequentially
    within an DAL cycle, filtering out non-informative samples until the batch is
    filled [[55](#bib.bib55)]. Criteria-selection hybrids use only one query strategy
    in one DAL iteration, in which they select the best query strategy or network
    architecture with the highest criterion. For example, DUAL [[114](#bib.bib114)]
    switches between density-based and uncertainty-based selectors to choose the best
    criterion for each DAL cycle. Unlike DUAL, iNAS [[115](#bib.bib115)] searches
    a restricted candidate set to find the optimal model architecture incrementally
    in each DAL iteration. Parallel-form hybrids use multi-objective optimization
    methods or a weighted sum to merge multiple query criteria into one for sample
    selection. For example, Gu et al. [[2](#bib.bib2)] efficiently acquire batches
    with discriminative and representative samples by proposing procedures to update
    labeled and unlabeled sets, based on path-following optimization techniques. Citovsky
    et al. [[55](#bib.bib55)] jointly optimize the uncertainty and diversity criteria
    in batch mode using multi-objective acquisition functions. TOD [[54](#bib.bib54)]
    selects samples with high model uncertainty and outputs discrepancy through a
    weighted combination of both metrics.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法旨在利用上述多种查询策略，并在它们之间实现权衡。混合方法可以根据交互模式进一步分类。串行形式的混合方法在一个DAL周期内按顺序应用标准，过滤掉非信息性样本，直到批次填满[[55](#bib.bib55)]。标准选择混合方法在一个DAL迭代中只使用一种查询策略，其中选择最佳的查询策略或具有最高标准的网络架构。例如，DUAL[[114](#bib.bib114)]在基于密度和基于不确定性的选择器之间切换，以选择每个DAL周期的最佳标准。与DUAL不同，iNAS[[115](#bib.bib115)]在每个DAL迭代中逐步搜索受限的候选集，以找到最佳的模型架构。并行形式的混合方法使用多目标优化方法或加权和将多个查询标准合并为一个进行样本选择。例如，Gu等[[2](#bib.bib2)]通过提出基于路径跟踪优化技术的程序，来高效地获取具有判别性和代表性的样本批次。Citovsky等[[55](#bib.bib55)]在批量模式下联合优化不确定性和多样性标准，使用多目标获取函数。TOD[[54](#bib.bib54)]通过加权组合这两种度量来选择具有高模型不确定性的样本并输出差异。
- en: Hybrid methods combine the advantages of different query strategies. However,
    determining the most effective combinations and trade-offs between criteria is
    time consuming and still remains open for further investigation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法结合了不同查询策略的优势。然而，确定最有效的组合和标准之间的权衡是耗时的，并且仍然需要进一步的研究。
- en: IV-C Model Architecture
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 模型架构
- en: \adfhalfrightarrowhead
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Traditional Machine Learning architectures, such as Forest [[39](#bib.bib39)]
    and Support Vector Machine (SVM) [[44](#bib.bib44)], are statistical-based models
    that do not use neural networks. And they attract great attention in the early
    stage of the DAL development.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习架构，如森林[[39](#bib.bib39)]和支持向量机（SVM）[[44](#bib.bib44)]，是基于统计的模型，不使用神经网络。它们在DAL发展的早期阶段受到广泛关注。
- en: \adfhalfrightarrowhead
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Bayesian Neural Networks (BNNs) combine neural networks with Bayesian inference,
    quantifying the uncertainty introduced by the models in terms of outputs and weights
    to explain the trustworthiness of the prediction [[116](#bib.bib116)]. Many studies
    propose DAL strategies based on BNNs, aiming to improve efficiency and explainability
    in samples selection [[38](#bib.bib38), [45](#bib.bib45)].
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络（BNNs）将神经网络与贝叶斯推断相结合，通过输出和权重来量化模型引入的不确定性，以解释预测的可信度[[116](#bib.bib116)]。许多研究提出了基于BNNs的DAL策略，旨在提高样本选择的效率和可解释性[[38](#bib.bib38),
    [45](#bib.bib45)]。
- en: \adfhalfrightarrowhead
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Recurrent Neural Networks (RNNs) [[117](#bib.bib117)] use their reasoning from
    previous experiences to predict upcoming events and are able to learn features
    with long-term dependencies. They have been widely used for sequential data such
    as text and audio. DAL is seldom combined with RNNs since they require large-scale
    labeled datasets for training. Some special tasks that easily recognizable patterns,
    such as malicious word detection on social networks [[118](#bib.bib118)], can
    be solved with DAL.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）[[117](#bib.bib117)] 利用之前经验中的推理来预测即将发生的事件，并能够学习具有长期依赖关系的特征。它们广泛应用于文本和音频等序列数据。由于RNNs需要大规模标注数据集进行训练，DAL与RNNs结合的机会较少。一些具有易于识别模式的特殊任务，例如社交网络上的恶意词检测[[118](#bib.bib118)]，可以通过DAL解决。
- en: \adfhalfrightarrowhead
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Convolutional Neural Networks (CNNs) [[6](#bib.bib6)] are feedforward neural
    networks that can extract features from data with convolution structures and have
    been widely used for image processing with three advantages: local connections,
    weight sharing, and down-sampling dimensionality reduction. DAL can be effectively
    combined with CNNs since Sener et al. [[41](#bib.bib41)] proved that a subset
    of samples (coreset) can geometrically characterize all features of the entire
    image set and can be selected by minimizing a rigorous bound. Following their
    study, more studies have been conducted [[49](#bib.bib49), [55](#bib.bib55)].'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）[[6](#bib.bib6)] 是一种前馈神经网络，能够通过卷积结构从数据中提取特征，并且广泛应用于图像处理，具有三个优点：局部连接、权重共享和下采样降维。由于Sener等人[[41](#bib.bib41)]证明了样本的子集（核心集）可以几何地表征整个图像集的所有特征，并且可以通过最小化严格的界限进行选择，因此DAL可以与CNNs有效结合。根据他们的研究，之后还进行了更多研究[[49](#bib.bib49),
    [55](#bib.bib55)]。
- en: \adfhalfrightarrowhead
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Graph Neural Networks (GNNs) [[5](#bib.bib5)] learn node representations by
    aggregating neighborhood information and achieve great success in various tasks,
    such as node classification. However, effectively handling graph data with dense
    interconnections between samples using limited labeled data remains an open challenge [[119](#bib.bib119)].
    DAL can help address this by selectively querying labels for the most informative
    samples and executing only one training epoch to reduce the annotation cost for
    various types of graphs, such as homogeneous graphs [[120](#bib.bib120)], heterogeneous
    graphs [[121](#bib.bib121)] and attribute graphs [[122](#bib.bib122)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）[[5](#bib.bib5)] 通过聚合邻域信息来学习节点表示，并在各种任务中取得了巨大的成功，例如节点分类。然而，使用有限的标注数据有效处理样本之间密集连接的图数据仍然是一个未解的挑战[[119](#bib.bib119)]。DAL可以通过选择性地查询最有信息量的样本标签，并仅执行一个训练周期来减少各种类型图的注释成本，如同质图[[120](#bib.bib120)]、异质图[[121](#bib.bib121)]和属性图[[122](#bib.bib122)]。
- en: \adfhalfrightarrowhead
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Variational Autoencoders (VAEs) is a class of neural network architecture designed
    with an encoder-decoder framework [[123](#bib.bib123)]. It aims to capture the
    underlying data distribution and learn to generate samples that closely resemble
    the input data. VAEs-based DAL methods usually generate samples to fool discriminators
    in an adversarial training manner, thus improving discriminators’ ability to select
    the most challenging-to-distinguish samples for training DAL models [[49](#bib.bib49),
    [57](#bib.bib57)].
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAEs）是一类具有编码器-解码器框架的神经网络架构[[123](#bib.bib123)]。它旨在捕捉潜在的数据分布，并学习生成与输入数据非常相似的样本。基于VAEs的DAL方法通常通过对抗训练方式生成样本以欺骗鉴别器，从而提高鉴别器选择最具挑战性的样本用于训练DAL模型的能力[[49](#bib.bib49),
    [57](#bib.bib57)]。
- en: \adfhalfrightarrowhead
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Pre-Trained Language Models (PLMs), based on Transformers, utilize multi-head
    self-attention to capture long-term dependencies. By pre-training on large unlabeled
    corpora, PLMs embed substantial general knowledge and transfer to downstream tasks,
    enabling state-of-the-art (SOTA) performance [[30](#bib.bib30)]. For example,
    Seo et al. [[32](#bib.bib32)] identify the most informative samples for a given
    task, focusing on PLMs fine-tuning, to learn salient patterns with minimal annotation
    cost. The combination of pre-training rich knowledge foundation and DAL’s sample-efficient
    tuning unlocks PLMs ’s further potential for many applications.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformers 的预训练语言模型（PLMs）利用多头自注意力机制来捕捉长期依赖关系。通过在大规模未标记语料上进行预训练，PLMs 蕴含了大量的通用知识并转移到下游任务中，实现了最先进的（SOTA）表现[[30](#bib.bib30)]。例如，Seo
    等人[[32](#bib.bib32)] 识别出给定任务中最有信息量的样本，专注于 PLMs 微调，以最小的标注成本学习显著的模式。预训练的丰富知识基础与
    DAL 的样本高效调优的结合，释放了 PLMs 在许多应用中的更大潜力。
- en: IV-D Learning Paradigm
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 学习范式
- en: \adfhalfrightarrowhead
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Traditional Learning Paradigm, as illustrated in Algorithm [1](#alg1 "Algorithm
    1 ‣ III Deep Active Learning ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers"), iteratively queries and labels samples to train the models
    in a vanilla supervised learning manner, without incorporating any advanced learning
    paradigms [[34](#bib.bib34), [32](#bib.bib32)].'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '传统学习范式，如算法[1](#alg1 "Algorithm 1 ‣ III Deep Active Learning ‣ A Survey on Deep
    Active Learning: Recent Advances and New Frontiers")所示，通过迭代查询和标记样本以以原始监督学习方式训练模型，而未结合任何先进的学习范式[[34](#bib.bib34),
    [32](#bib.bib32)]。'
- en: \adfhalfrightarrowhead
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Semi-supervised Learning, also known as weakly-supervised learning, aims to
    jointly use real-labeled samples and pseudo-labeled samples to train the models.
    Current DAL methods are designed with various efficient strategies to obtain pseudo-labels
    for unlabeled samples. For instance, DBAL [[35](#bib.bib35)] and CoreSet [[41](#bib.bib41)]
    first predict pseudo-labels using their models and then calculate samples’ confidence
    scores to judge whether these pseudo-labels should be trusted or not. On the other
    hand, LADA [[56](#bib.bib56)] and BGADL [[48](#bib.bib48)] propose new data augmentation
    methods to create more samples based on original labeled samples, using their
    original real-labeled samples as pseudo-labels. These studies effectively reduce
    human-labors and achieve comparable performance compared with traditional supervised
    learning using larger labeled samples.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习，也称为弱监督学习，旨在联合使用真实标记样本和伪标记样本来训练模型。当前的 DAL 方法设计了各种高效策略来为未标记样本获取伪标签。例如，DBAL[[35](#bib.bib35)]
    和 CoreSet[[41](#bib.bib41)] 首先使用其模型预测伪标签，然后计算样本的置信度分数，以判断这些伪标签是否值得信赖。另一方面，LADA[[56](#bib.bib56)]
    和 BGADL[[48](#bib.bib48)] 提出了新的数据增强方法，根据原始标记样本创建更多样本，使用其原始真实标记样本作为伪标签。这些研究有效减少了人工劳动，并在性能上与使用更大标记样本的传统监督学习相当。
- en: \adfhalfrightarrowhead
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Contrastive Learning improves feature representation by pulling similar instances
    closer together while pushing dissimilar instances apart [[124](#bib.bib124)].
    Contrastive methods extract discriminative features, such as semantics [[100](#bib.bib100)]
    and distinctiveness [[57](#bib.bib57)], to estimate the sample uncertainty during
    acquisition. For example, as shown in Fig. [5](#S4.F5 "Figure 5 ‣ IV-D Learning
    Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active Learning: Recent Advances
    and New Frontiers"), Du et al. [[125](#bib.bib125)] extract both semantic and
    distinctive features with contrastive learning and then combine them in a query
    strategy to choose the most informative unlabeled samples with matched categories.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '对比学习通过将相似实例拉近，同时将不相似实例推开来改进特征表示[[124](#bib.bib124)]。对比方法提取具有区分性的特征，如语义[[100](#bib.bib100)]和独特性[[57](#bib.bib57)]，以在获取过程中估计样本的不确定性。例如，如图[5](#S4.F5
    "Figure 5 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers")所示，Du 等人[[125](#bib.bib125)] 使用对比学习提取语义和独特性特征，然后将它们结合在查询策略中，以选择最具信息量的未标记样本，并匹配类别。'
- en: '![Refer to caption](img/fee57d12174b6a89cec10254ad6984c0.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fee57d12174b6a89cec10254ad6984c0.png)'
- en: 'Figure 5: An example for contrastive learning based query strategies.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于对比学习的查询策略示例。
- en: \adfhalfrightarrowhead
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Adversarial Learning enables a model to train fully differentiable by solving
    minimax optimization problems [[49](#bib.bib49)]. This approach can be used as
    a generative query technique for DAL. For example, DAL can be combined with generative
    adversarial network, which consist of a generator and a discriminator, where the
    DAL model acts as the discriminator and the generator explores the distribution
    of unlabeled data to generate the most informative and uncertain synthetic samples
    for training [[57](#bib.bib57)]. Li et al. [[122](#bib.bib122)] propose SEAL,
    as shown in Fig. [6](#S4.F6 "Figure 6 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of
    DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers") which
    consists of two adversarial components. The graph embedding network encodes all
    nodes into a shared space, with the intention of making the discriminator treat
    all nodes as labeled. Additionally, a semi-supervised discriminator is used to
    differentiate unlabeled nodes from labeled ones. The divergence score of the discriminator
    is used as an informativeness measure to actively select the most informative
    node for labeling. The two components form a loop to mutually improve DAL.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '对抗学习使得模型能够通过解决最小最大优化问题进行完全可微训练[[49](#bib.bib49)]。这种方法可以作为DAL的生成查询技术。例如，DAL可以与生成对抗网络结合，生成对抗网络由生成器和判别器组成，其中DAL模型充当判别器，生成器则探索未标记数据的分布，生成最具信息量和不确定性的合成样本用于训练[[57](#bib.bib57)]。Li等人[[122](#bib.bib122)]提出了SEAL，如图[6](#S4.F6
    "Figure 6 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers")所示，它由两个对抗组件组成。图嵌入网络将所有节点编码到共享空间中，目的是让判别器将所有节点视为标记节点。此外，还使用半监督判别器来区分未标记节点和标记节点。判别器的发散分数作为信息量度量，主动选择最具信息量的节点进行标记。两个组件形成一个循环，互相提高DAL的性能。'
- en: '![Refer to caption](img/e7aa6b8b48493cfb226739f7a7d9804d.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e7aa6b8b48493cfb226739f7a7d9804d.png)'
- en: 'Figure 6: An example for contrastive learning based query strategies.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：对比学习基于查询策略的示例。
- en: \adfhalfrightarrowhead
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Meta Learning enables DNNs to leverage the knowledge acquired from multiple
    tasks, represented in the network with their weights, to adapt faster to new tasks.
    Meta learning can provide an acquisition function for DAL [[39](#bib.bib39), [126](#bib.bib126)]
    or favorable model initialization during DAL by controlling the transfer of knowledge
    from multiple source tasks. For example, Shao et al. [[127](#bib.bib127)] propose
    Learning-to-Sample, where a boosting model and sampling model dynamically learn
    from each other and iteratively improve performance. Zhu et al. [[59](#bib.bib59)]
    combine both paradigms by initializing an active learner with meta-learned parameters
    via meta-training on tasks similar to the target task.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习使得深度神经网络能够利用从多个任务中获得的知识，这些知识在网络中通过权重表示，以更快地适应新任务。元学习可以为DAL提供获取函数[[39](#bib.bib39),
    [126](#bib.bib126)]，或者通过控制来自多个源任务的知识转移，在DAL过程中提供有利的模型初始化。例如，Shao等人[[127](#bib.bib127)]提出了学习采样，其中提升模型和采样模型相互动态学习并迭代提高性能。Zhu等人[[59](#bib.bib59)]通过在与目标任务类似的任务上进行元训练，用元学习参数初始化主动学习者，从而结合了这两种范式。
- en: \adfhalfrightarrowhead
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Reinforcement Learning involves an agent that can interact with its environment
    and learn to alter its behavior in response to received rewards [[119](#bib.bib119)].
    Given that almost all DAL methods use heuristic acquisition functions with limited
    effectiveness, Reinforcement learning frames DAL as a reinforcement learning problem
    to explicitly optimize an acquisition policy. In the DAL with reinforcement learning
    setup, an autonomous agent (acquisition selector) controlled by a deep learning
    algorithm that observes a state $s_{t}$ from its environment (predictor) at time
    $t$. It takes an action $a_{t}$ to maximize the reward $r_{t}$ (prediction accuracy),
    where $a_{t}$ decides whether to query unlabeled samples [[62](#bib.bib62)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习涉及一个能够与环境互动的代理，并通过对收到的奖励做出反应来学习改变其行为[[119](#bib.bib119)]。鉴于几乎所有DAL方法都使用效果有限的启发式获取函数，强化学习将DAL视为强化学习问题，以明确优化获取策略。在强化学习设置的DAL中，一个由深度学习算法控制的自主代理（获取选择器）在时间$t$观察环境（预测器）中的状态$s_{t}$。它采取一个动作$a_{t}$以最大化奖励$r_{t}$（预测准确性），其中$a_{t}$决定是否查询未标记样本[[62](#bib.bib62)]。
- en: \adfhalfrightarrowhead
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Curriculum Learning mimic human and animal learning processes, where the training
    progresses gradually from simple to complex samples. This provides a natural way
    to exploit labeled data for robust learning [[10](#bib.bib10), [128](#bib.bib128)].
    Specifically, curriculum learning uses a predefined learning constraint to incrementally
    incorporate additional labeled samples during training. Curriculum Learning introduces
    a weighted loss on all labeled samples, acting as a general regularizer over the
    sample weights. For example, Wang et al. [[129](#bib.bib129)] use a pseudo-labels
    strategy which iteratively assigns pseudo-labels to unlabeled samples with high
    prediction confidence.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习模仿人类和动物的学习过程，其中训练逐渐从简单样本过渡到复杂样本。这提供了一种利用带标签数据进行鲁棒学习的自然方式 [[10](#bib.bib10),
    [128](#bib.bib128)]。具体而言，课程学习使用预定义的学习约束，在训练过程中逐步纳入额外的带标签样本。课程学习对所有带标签的样本引入加权损失，作为样本权重上的一般正则化器。例如，Wang
    等人 [[129](#bib.bib129)] 使用伪标签策略，迭代地将伪标签分配给具有高预测信心的未标记样本。
- en: \adfhalfrightarrowhead
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Continual Learning is developed for constraints on task-based settings, where
    the model continuously learns a sequence of tasks one at a time, where all data
    for the current task are labeled and available in increments. However, real-world
    systems do not have the luxury of large labeled datasets for each new task. To
    address this issue, Mundt et al. [[130](#bib.bib130)] present a detailed analysis
    of continual learning-based DAL and out-of-distribution detection works. They
    suggest a unified perspective with open-set recognition as a natural interface
    between continual learning and DAL. Ayub et al. [[30](#bib.bib30)] develop a method
    that allows a continue learning agent to continually learn new object classes
    from a few labeled examples.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 继续学习是为任务驱动的设置开发的，其中模型一次连续学习一系列任务，当前任务的所有数据都有标签并逐步提供。然而，现实世界的系统并没有为每个新任务提供大量带标签的数据的奢侈条件。为了解决这个问题，Mundt
    等人 [[130](#bib.bib130)] 提出了一个关于基于继续学习的 DAL 和分布外检测工作的详细分析。他们建议用开放集识别作为继续学习和 DAL
    之间自然接口的统一视角。Ayub 等人 [[30](#bib.bib30)] 开发了一种方法，允许继续学习的代理从少量带标签的示例中不断学习新的对象类别。
- en: '![Refer to caption](img/7282bff6a6c4f9dba102afbb6b725718.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7282bff6a6c4f9dba102afbb6b725718.png)'
- en: 'Figure 7: An example for transfer learning based query strategies.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于查询策略的迁移学习示例。
- en: \adfhalfrightarrowhead
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Transfer Learning extracts knowledge from one or more source tasks and applies
    it to a target task. It has two broad categories: transductive and inductive.
    While transductive methods adapt models learned from a labeled source domain to
    a different unlabeled target domain with the same task, inductive methods ensure
    that the domains of source and target are the same but tasks are different. DAL
    with transfer learning can better enhance each other’s performance by selecting
    the best target samples with a distribution similar to the source domain [[50](#bib.bib50)].
    In addition, transfer learning can minimize the number of annotation labels needed
    and provide auxiliary information for DAL acquisition functions. For example,
    as shown in Fig. [7](#S4.F7 "Figure 7 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of
    DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"), Xie
    et al. [[87](#bib.bib87)] propose an energy-based active domain adaptation that
    balances domain representation and uncertainty when selecting target data.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习从一个或多个源任务中提取知识，并将其应用于目标任务。它有两个广泛的类别：归纳和转导。转导方法将从带标签的源领域中学习到的模型适应到具有相同任务的不同未标记目标领域，而归纳方法确保源领域和目标领域相同但任务不同。迁移学习与
    DAL 可以通过选择分布类似于源领域的最佳目标样本来更好地提升彼此的性能 [[50](#bib.bib50)]。此外，迁移学习可以减少所需的注释标签数量，并为
    DAL 获取函数提供辅助信息。例如，如图 [7](#S4.F7 "图 7 ‣ IV-D 学习范式 ‣ IV DAL 分类 ‣ 深度主动学习：近期进展与新前沿")
    所示，Xie 等人 [[87](#bib.bib87)] 提出了基于能量的主动领域适应方法，在选择目标数据时平衡领域表示和不确定性。
- en: '![Refer to caption](img/d3ea36d6f9539651b6f7ca0e2e1ca2f3.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d3ea36d6f9539651b6f7ca0e2e1ca2f3.png)'
- en: 'Figure 8: An example for imitation learning [[131](#bib.bib131)].'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：模仿学习的一个示例 [[131](#bib.bib131)]。
- en: \adfhalfrightarrowhead
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Imitation Learning provides SOTA results in many structured prediction tasks
    by learning near-optimal search policies [[92](#bib.bib92)]. Such methods assume
    access to an expert during training that can provide the optimal action in any
    queried state, essentially asking “what would you do here?” and learning to mimic
    that choice. For example, Bullard et al. [[132](#bib.bib132)] use imitation learning
    to allow an agent in a constrained environment to concurrently reason about both
    its internal learning goals and externally impose environmental constraints within
    its objective function. Löffler et al. [[131](#bib.bib131)] propose an imitation
    learning scheme (IALE) that mimics the selection of the best-performing expert
    heuristic at each stage of the learning cycle in a batch-mode setting. As shown
    in Fig. [8](#S4.F8 "Figure 8 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of DAL ‣ A
    Survey on Deep Active Learning: Recent Advances and New Frontiers"), IALE can
    well imitate the Entropy-based and CoreSet-based methods and thus obtain better
    performance.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '模仿学习通过学习接近最优的搜索策略，在许多结构化预测任务中提供了最先进的结果[[92](#bib.bib92)]。这种方法假设在训练期间可以接触到一个专家，该专家可以在任何查询的状态下提供最佳行动，实际上是在问“你会在这里怎么做？”并学习模仿该选择。例如，Bullard
    等人[[132](#bib.bib132)] 使用模仿学习使得一个在受限环境中的代理能够同时考虑其内部学习目标和外部施加的环境约束在其目标函数中。Löffler
    等人[[131](#bib.bib131)] 提出了一个模仿学习方案（IALE），在批量模式设置中模仿学习周期每个阶段选择最佳表现专家启发式的方法。如图[8](#S4.F8
    "Figure 8 ‣ IV-D Learning Paradigm ‣ IV Taxonomy of DAL ‣ A Survey on Deep Active
    Learning: Recent Advances and New Frontiers")所示，IALE 可以很好地模仿基于熵的方法和基于核心集的方法，从而获得更好的性能。'
- en: \adfhalfrightarrowhead
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Multi-task Learning (MTL) focuses on formulating methods to maintain performance
    across multiple tasks rather than a single task. Multi-task DAL (MTAL) methods
    combine multiple individual task-related query strategies into a single unified
    approach and jointly optimize the unified one. In contrast to single-task query
    settings, where the uncertainty of a single selected task classifier is used to
    query unlabeled samples, in MTAL the uncertainty of an instance is determined
    by the uncertainties from classifiers across all tasks. For example, Ikhwantri
    et al. [[133](#bib.bib133)] propose an MTAL framework for semantic role labeling
    with entity recognition as an auxiliary task. This alleviated data needs and leverages
    entity information to aid role labeling. Their experiments show that MTAL can
    outperform single-task DAL and standard MTL, using 12% less training data than
    passive learning. Zhou et al. [[84](#bib.bib84)] propose a Multi-Task Adversarial
    DAL framework, where adversarial learning maintains the effectiveness of the MTL
    and DAL modules. A task discriminator eliminates irregular task-specific features,
    while a diversity discriminator exploits heterogeneity between samples to satisfy
    diversity constraints.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习（MTL）关注于制定在多个任务中保持性能的方法，而不是单一任务。多任务深度主动学习（MTAL）方法将多个单独的任务相关查询策略结合成一个统一的方法，并联合优化这个统一的方法。与单任务查询设置不同，在单任务查询设置中，使用单个选定任务分类器的不确定性来查询未标记样本，而在MTAL中，实例的不确定性由所有任务分类器的不确定性决定。例如，Ikhwantri
    等人[[133](#bib.bib133)] 提出了一个用于语义角色标注的MTAL框架，将实体识别作为辅助任务。这缓解了数据需求，并利用实体信息来帮助角色标注。他们的实验表明，MTAL
    可以优于单任务深度主动学习和标准的多任务学习，使用比被动学习少12%的训练数据。Zhou 等人[[84](#bib.bib84)] 提出了一个多任务对抗深度主动学习框架，其中对抗学习保持了MTL和DAL模块的有效性。任务判别器消除不规则的任务特定特征，而多样性判别器利用样本之间的异质性来满足多样性约束。
- en: IV-E Training Process
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 训练过程
- en: \adfhalfrightarrowhead
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Traditional Training first trains a model on an initialized training dataset
    and then selects unlabeled samples to annotate based on the predictions of the
    current model. The newly annotated samples are added to the training set for re-training
    the model in the next iteration [[134](#bib.bib134)]. This iterative process continues,
    with the model parameters randomly re-initialized before each epoch of re-training [[36](#bib.bib36)],
    until either the sample budget or number of DAL iterations is reached.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 传统训练首先在初始化的训练数据集上训练一个模型，然后根据当前模型的预测选择未标记样本进行标注。新标注的样本被添加到训练集中，以便在下一次迭代中重新训练模型[[134](#bib.bib134)]。这一迭代过程继续进行，在每个训练周期之前，模型参数会被随机重新初始化[[36](#bib.bib36)]，直到达到样本预算或深度主动学习迭代次数。
- en: \adfhalfrightarrowhead
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Curriculum Learning Training gradually progresses from easy to complex samples,
    mimicking human and animal learning processes. This provides a natural and iterative
    way to exploit labeled data for robust learning. For example, Tang et al. [[135](#bib.bib135)]
    propose a self-paced DAL approach that jointly considers the value and difficulty
    of a sample. It queries samples from easy to hard to minimize annotation cost.
    Wang et al. [[43](#bib.bib43)] show that curriculum learning alone improves the
    accuracy of the object detection by 3.6%, while the combination of curriculum
    learning and DAL improve the accuracy by 4.3%.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习训练逐渐从简单样本过渡到复杂样本，模拟人类和动物的学习过程。这提供了一种自然且迭代的方式来利用标注数据进行稳健学习。例如，Tang 等人 [[135](#bib.bib135)]
    提出了一个自适应的 DAL 方法，该方法共同考虑样本的价值和难度。从简单到困难查询样本，以最小化标注成本。Wang 等人 [[43](#bib.bib43)]
    显示，仅使用课程学习可以将目标检测的准确性提高 3.6%，而课程学习与 DAL 的组合将准确性提高了 4.3%。
- en: \adfhalfrightarrowhead
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Pre-training & Fine-tuning (Pre+FT) have become a primary training process with
    the development of large-scale PLMs [[58](#bib.bib58)]. It leverages the rich
    prior knowledge in PLMs to solve different downstream tasks. DAL attracts attention
    as a sample selection strategy for fine-tuning with only 10%$\sim$20% of labeled
    data achieving competitive performance compared to full data fine-tuning [[32](#bib.bib32)].
    DAL iteratively selects and annotates batches of informative samples to fine-tune
    the PLMs for the downstream task. This satisfies task-specific needs, while also
    enabling a few-shot learning [[30](#bib.bib30)].
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练与微调（Pre+FT）已成为大规模 PLMs 发展的主要训练过程 [[58](#bib.bib58)]。它利用 PLMs 中丰富的先验知识来解决不同的下游任务。DAL
    作为微调的样本选择策略引起关注，使用仅 10%$\sim$20% 的标注数据即可实现与全数据微调相媲美的性能 [[32](#bib.bib32)]。DAL
    迭代选择和标注信息丰富的样本批次，以微调 PLMs 以适应下游任务。这满足了任务特定需求，同时也支持少样本学习 [[30](#bib.bib30)]。
- en: 'TABLE III: Illustration of DAL-related applications in main fields, including
    classic methods with their advantages and disadvantages.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：展示 DAL 相关应用于主要领域，包括经典方法及其优缺点。
- en: '| Areas | Applications | Classic Methods | Advantages | Disadvantages |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 应用 | 经典方法 | 优势 | 缺点 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| NLP | Text Classification | generate samples for training [[83](#bib.bib83),
    [136](#bib.bib136)]. | make the selection process efficient. | high time consumption,
    unstable performance. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| NLP | 文本分类 | 生成用于训练的样本 [[83](#bib.bib83), [136](#bib.bib136)]。 | 提高选择过程的效率。
    | 高时间消耗，性能不稳定。 |'
- en: '| uncertainty sampling [[61](#bib.bib61)]. | high efficiency and performance.
    | vulnerable to outliers, unstable performance. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 不确定性采样 [[61](#bib.bib61)]。 | 高效性和性能。 | 对异常值敏感，性能不稳定。 |'
- en: '| use pre-trained language models [[137](#bib.bib137)]. | easily adapt to new
    datasets. | vulnerable to outliers and imbalanced datasets. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 使用预训练语言模型 [[137](#bib.bib137)]。 | 轻松适应新数据集。 | 对异常值和不平衡数据集敏感。 |'
- en: '| Text Summarization | PLMs with Monte Carlo dropout [[138](#bib.bib138)].
    | efficient and effectiveness. | vulnerable to outliers, unstable performance.
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 文本总结 | 带有 Monte Carlo dropout 的 PLMs [[138](#bib.bib138)]。 | 高效性和效果。 | 对异常值敏感，性能不稳定。
    |'
- en: '| diverse sampling [[139](#bib.bib139)]. | remove outliers and diverse sampling.
    | vulnerable to document embeddings. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 多样本采样 [[139](#bib.bib139)]。 | 去除异常值和多样本采样。 | 对文档嵌入敏感。 |'
- en: '| Question Answering | DataMap [[58](#bib.bib58)]. | eliminate outliers and
    improve accuracy. | high time consumption, lack of generalizability. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | DataMap [[58](#bib.bib58)]。 | 消除异常值并提高准确性。 | 高时间消耗，缺乏泛化能力。 |'
- en: '| interactive query strategy [[140](#bib.bib140)]. | efficiently minimize costly
    data annotations. | wait for human reaction, need expert knowledge. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 交互查询策略 [[140](#bib.bib140)]。 | 高效地最小化高成本数据标注。 | 等待人工反应，需要专家知识。 |'
- en: '| Information Extraction | label identical subsequences [[141](#bib.bib141)]
    | high efficiency and effectiveness. | lack of generalizability, cold-start. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 信息提取 | 标签相同的子序列 [[141](#bib.bib141)] | 高效率和效果。 | 缺乏泛化能力，冷启动。 |'
- en: '| label most novel words [[142](#bib.bib142)]. | high efficiency and effectiveness.
    | unstable performance, cold-start. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 标签最具新颖性的词汇 [[142](#bib.bib142)]。 | 高效率和效果。 | 性能不稳定，冷启动。 |'
- en: '| Semantic Parsing | hyperparameter selection [[143](#bib.bib143)]. | reduce
    data annotation. | high time consumption, lack of generalizability. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 语义解析 | 超参数选择 [[143](#bib.bib143)]。 | 减少数据标注。 | 高时间消耗，缺乏泛化能力。 |'
- en: '| hybrid query strategies [[144](#bib.bib144)]. | select the most semantically
    varied samples. | vulnerable to outliers, lack of scalability. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 混合查询策略 [[144](#bib.bib144)]。 | 选择语义上最具多样性的样本。 | 易受异常值影响，缺乏可扩展性。 |'
- en: '| CV | Image Captioning | semantic adversarial DAL [[145](#bib.bib145)] | overcome
    scarcity of labeled data. | difficulty in cross-domain transfer, cold-start. |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 计算机视觉 | 图像描述 | 语义对抗DAL [[145](#bib.bib145)] | 克服标注数据稀缺。 | 跨领域迁移困难，冷启动。 |'
- en: '| domain transfer learning [[146](#bib.bib146)]. | transfer knowledge from
    high-resource. | vulnerable to outliers, data scarcity. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 领域迁移学习 [[146](#bib.bib146)]。 | 从高资源领域迁移知识。 | 易受异常值和数据稀缺的影响。 |'
- en: '| Semantic Segmentation | uncertainty-based DAL [[147](#bib.bib147)]. | high
    efficiency and effectiveness. | unstable performance, easily select outliers.
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 语义分割 | 基于不确定性的DAL [[147](#bib.bib147)]。 | 高效且有效。 | 性能不稳定，易选择异常值。 |'
- en: '| region-based selection [[148](#bib.bib148), [80](#bib.bib80)]. | balance
    between label efforts and effect. | vulnerable to outliers, imbalance datasets.
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 基于区域的选择 [[148](#bib.bib148), [80](#bib.bib80)]。 | 在标签工作量和效果之间取得平衡。 | 易受异常值和不平衡数据集的影响。
    |'
- en: '| Object Detection | hybrid selection [[149](#bib.bib149), [43](#bib.bib43)].
    | avoid noisy samples and outliers. | data scaricity, unstable performance. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 目标检测 | 混合选择 [[149](#bib.bib149), [43](#bib.bib43)]。 | 避免噪声样本和异常值。 | 数据稀缺，性能不稳定。
    |'
- en: '| instance uncertainty learning [[150](#bib.bib150)]. | suppress noisy instances.
    | unstable performance, lack of scalability. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 实例不确定性学习 [[150](#bib.bib150)]。 | 抑制噪声实例。 | 性能不稳定，缺乏可扩展性。 |'
- en: '| Pose Estimation | traditional DAL strategy [[151](#bib.bib151), [152](#bib.bib152)].
    | effectiveness, easy to apply. | vulnerable to outliers, cold-start. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 姿态估计 | 传统DAL策略 [[151](#bib.bib151), [152](#bib.bib152)]。 | 有效，易于应用。 | 易受异常值和冷启动的影响。
    |'
- en: '| meta learning [[86](#bib.bib86)]. | can learn an optimal sampling policy.
    | vulnerable to outliers and imbalance datasets. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 元学习 [[86](#bib.bib86)]。 | 可以学习最佳采样策略。 | 易受异常值和不平衡数据集的影响。 |'
- en: '| Target Tracking | multi-frame collaboration [[153](#bib.bib153)] | eliminate
    background noise, ensure diversity. | unstable performance, lack of scalability.
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 目标跟踪 | 多帧协作 [[153](#bib.bib153)] | 消除背景噪声，确保多样性。 | 性能不稳定，缺乏可扩展性。 |'
- en: '| multi-target object tracking [[154](#bib.bib154)]. | high efficient and effectiveness.
    | high time consumption, cold-start |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 多目标跟踪 [[154](#bib.bib154)]。 | 高效且有效。 | 高时间消耗，冷启动 |'
- en: '| Person Re-identification | human-in-the-loop [[46](#bib.bib46)]. | improve
    model performance. | high time consumption, lack of generalizability. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 人物重识别 | 人工干预 [[46](#bib.bib46)]。 | 提高模型性能。 | 高时间消耗，缺乏泛化能力。 |'
- en: '| incremental annotation [[155](#bib.bib155)]. | select diverse samples without
    redundancy. | vulnerable to outliers, cold-start. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 增量标注 [[155](#bib.bib155)]。 | 选择多样化的样本而无冗余。 | 易受异常值和冷启动的影响。 |'
- en: '| DM | Node Classification | semi-supervised adversatial DAL [[122](#bib.bib122)].
    | better performance gains. | unstable performance, cold-start. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| DM | 节点分类 | 半监督对抗DAL [[122](#bib.bib122)]。 | 更好的性能提升。 | 性能不稳定，冷启动。 |'
- en: '| graph policy network [[120](#bib.bib120)]. | stable performance. | single
    sample selection costs much time. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 图谱策略网络 [[120](#bib.bib120)]。 | 稳定的性能。 | 单样本选择耗时较长。 |'
- en: '| Link Prediction | multi-view DAL [[156](#bib.bib156)]. | query informative
    samples from multi-view. | lack of scalability and generalizability. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 链接预测 | 多视角DAL [[156](#bib.bib156)]。 | 从多视角中查询信息样本。 | 缺乏可扩展性和泛化能力。 |'
- en: '| transfer learning DAL [[157](#bib.bib157)]. | easily apply to new datasets.
    | unstable performance, cold-start. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 迁移学习DAL [[157](#bib.bib157)]。 | 容易应用于新数据集。 | 性能不稳定，冷启动。 |'
- en: '| Community Detection | topic-based [[158](#bib.bib158)]. | reducing the unreliable
    dataset. | high time consumption, unstable performance. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 社区检测 | 基于主题 [[158](#bib.bib158)]。 | 减少不可靠的数据集。 | 高时间消耗，性能不稳定。 |'
- en: '| geometric block model [[159](#bib.bib159)]. | efficient and effectiveness.
    | unstable performance, cold-start. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 几何块模型 [[159](#bib.bib159)]。 | 高效且有效。 | 性能不稳定，冷启动。 |'
- en: V Applications of DAL
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V DAL应用
- en: 'As shown in Table [III](#S4.T3 "TABLE III ‣ IV-E Training Process ‣ IV Taxonomy
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"),
    the integration of DL and AL is leading to an increasing application of AL methods
    in various domains of life, ranging from agricultural development [[82](#bib.bib82)]
    to industrial revitalization [[82](#bib.bib82)], and from artificial intelligence [[137](#bib.bib137)]
    to biomedical fields [[160](#bib.bib160)]. In this section, we aim to provide
    a systematic and detailed overview of existing DAL-related work from a broad application
    perspective.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [III](#S4.T3 "TABLE III ‣ IV-E Training Process ‣ IV Taxonomy of DAL ‣ A
    Survey on Deep Active Learning: Recent Advances and New Frontiers") 所示，深度学习（DL）与主动学习（AL）的结合正导致
    AL 方法在各种生活领域的应用增加，从农业发展 [[82](#bib.bib82)] 到工业振兴 [[82](#bib.bib82)]，以及从人工智能 [[137](#bib.bib137)]
    到生物医学领域 [[160](#bib.bib160)]。在这一部分，我们旨在从广泛应用的角度提供系统而详细的 DAL 相关工作概述。'
- en: V-A Applications in Natural Language Processing
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 自然语言处理中的应用
- en: With the emergence of large-scale language models, NLP has achieved great success
    using computers to help understand intricate languages. However, fine-tuning these
    language models requires a substantial amount of data, computation resources,
    and time. DAL provides a strategy for searching high-quality small and high-quality
    samples to help fine-tune the model and save resources. In the following, we introduce
    some of the most influential DAL methods in NLP.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大规模语言模型的出现，自然语言处理（NLP）通过计算机帮助理解复杂语言取得了巨大成功。然而，对这些语言模型进行微调需要大量的数据、计算资源和时间。DAL
    提供了一种寻找高质量小样本的策略，以帮助微调模型并节省资源。接下来，我们将介绍一些在 NLP 中最具影响力的 DAL 方法。
- en: \adfhalfrightarrowhead
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Text Classification aims to classify large-scale text with particular labels
    such as topic or sentiment. Researchers propose several methods to efficiently
    select informative samples for training. For example, Yan et al. [[83](#bib.bib83)]
    generate the most informative examples for training, efficiently skipping the
    sample selection process. They approximate the generated example with a few summary
    words, which significantly reduces the labeling cost for annotators, as they only
    need to read a few words instead of a long document. Tan et al. [[136](#bib.bib136)]
    develop the Bayesian estimate of mean proper scores (BEMPS) framework for DAL,
    which allows the calculation of scores such as logarithmic probability to better
    help select informative and uncertainty samples. Experiments demonstrate that
    BEMPS is more effective than baselines in various text classification datasets.
    On the other hand, Schroder et al. [[61](#bib.bib61)] use transformers for uncertainty-based
    sample selection. Interestingly, they achieve comparable performance in widely
    used text classification datasets while training in less than 20% of the labeled
    data, which demonstrates their ability to utilize limited labeled data. In another
    study, Jelenic et al. [[137](#bib.bib137)] conduct an initial empirical study
    to investigate the transferability of the DAL by using PLMs . They find DAL can
    effectively adapt to new datasets with pre-trained models.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '文本分类旨在对大规模文本进行特定标签分类，如主题或情感。研究人员提出了几种方法来高效选择信息丰富的样本进行训练。例如，Yan 等 [[83](#bib.bib83)]
    生成最具信息性的训练样本，高效跳过样本选择过程。他们用少量总结性文字近似生成的示例，这大大减少了标注者的标注成本，因为标注者只需阅读几个单词而不是长文档。Tan
    等 [[136](#bib.bib136)] 为 DAL 开发了贝叶斯均值得分估计（BEMPS）框架，该框架允许计算诸如对数概率等得分，以更好地帮助选择信息丰富和不确定性样本。实验表明，BEMPS
    在各种文本分类数据集中比基线更有效。另一方面，Schroder 等 [[61](#bib.bib61)] 使用变换器进行基于不确定性的样本选择。有趣的是，他们在广泛使用的文本分类数据集中取得了与训练超过
    20% 标注数据相当的性能，这展示了他们利用有限标注数据的能力。在另一项研究中，Jelenic 等 [[137](#bib.bib137)] 通过使用预训练语言模型（PLMs）进行初步实证研究，以探讨
    DAL 的迁移性。他们发现，DAL 能够有效地适应新的数据集。 '
- en: \adfhalfrightarrowhead
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Abstractive Text Summarization (ATS) aims to compress a document into a brief,
    informative and readable summary that retains the key information of the original
    document. However, constructing human-annotated datasets is a time-consuming and
    costly endeavor. DAL are explored to reduce the amount of annotation needed while
    achieving a certain level of ATS performance. For example, Gidiotis et al. [[138](#bib.bib138)]
    address the issue from a Bayesian view and study uncertainty estimation for SOTA
    text summarization models. They augment the pre-trained summarization models with
    Monte Carlo dropout, forming the corresponding variational Bayesian PLMs models.
    By generating multiple summaries from these models, they approximate Bayesian
    inference and estimate the summarization uncertainty. Experiments on multiple
    benchmark datasets consistently demonstrate their improved summarization performance
    with higher Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores.
    Unlike the above method, as Fig. [9](#S5.F9 "Figure 9 ‣ V-A Applications in Natural
    Language Processing ‣ V Applications of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") (a) shows, Tsvigun et al. [[139](#bib.bib139)]
    propose an alternative query strategy for ATS based on diversity principles. This
    strategy, known as in-domain diversity sampling, involves selecting instances
    that are dissimilar from annotated documents, but similar to the core documents
    of the domain. Given limited annotation budget, they can improve model performance
    and consistency scores.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '抽象文本摘要 (ATS) 旨在将文档压缩为简短、信息丰富且可读的摘要，以保留原始文档的关键信息。然而，构建人工注释的数据集是一项耗时且成本高昂的工作。DAL
    被探索用于减少所需的标注量，同时达到一定水平的 ATS 性能。例如，Gidiotis 等人 [[138](#bib.bib138)] 从贝叶斯视角解决这个问题，并研究
    SOTA 文本摘要模型的置信度估计。他们通过蒙特卡洛丢弃增强预训练的摘要模型，形成相应的变分贝叶斯 PLMs 模型。通过从这些模型生成多个摘要，他们近似贝叶斯推断并估计摘要的不确定性。在多个基准数据集上的实验一致地展示了他们在召回导向摘要评价
    (ROUGE) 分数上改进的摘要性能。与上述方法不同，如图 [9](#S5.F9 "Figure 9 ‣ V-A Applications in Natural
    Language Processing ‣ V Applications of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") (a) 所示，Tsvigun 等人 [[139](#bib.bib139)] 提出了基于多样性原则的
    ATS 替代查询策略。这种策略称为领域内多样性采样，涉及选择与标注文档不同但与领域核心文档相似的实例。在有限的标注预算下，它们可以提高模型性能和一致性评分。'
- en: '![Refer to caption](img/a0b1867758bd079636d8088a56907083.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/a0b1867758bd079636d8088a56907083.png)'
- en: 'Figure 9: An example for samples selection of ATS and Datamap.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：ATS 和 Datamap 样本选择的示例。
- en: \adfhalfrightarrowhead
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Question Answering involves answering questions about images or passages of
    text [[161](#bib.bib161)]. However, current models require large-scale training
    data to achieve high performance. DAL methods, such as Datamap [[58](#bib.bib58)]
    and hierarchical dialog policies [[140](#bib.bib140)], are designed to maximize
    performance with minimal labeling effort. Specifically, in Fig. [9](#S5.F9 "Figure
    9 ‣ V-A Applications in Natural Language Processing ‣ V Applications of DAL ‣
    A Survey on Deep Active Learning: Recent Advances and New Frontiers") (b), DataMap [[58](#bib.bib58)]
    is able to detect and eliminate outlier examples from the unlabeled set, resulting
    in a significant increase in model accuracy with fewer labeled examples. Padmakumar
    et al. [[140](#bib.bib140)] develop a joint policy for clarification and DAL in
    an interactive image retrieval task. Asking users for clarification while querying
    new examples improves the model performance.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '问答系统涉及回答有关图像或文本段落的问题 [[161](#bib.bib161)]。然而，当前模型需要大规模的训练数据才能实现高性能。DAL 方法，如
    Datamap [[58](#bib.bib58)] 和分层对话策略 [[140](#bib.bib140)]，旨在以最小的标注工作量最大化性能。具体来说，在图 [9](#S5.F9
    "Figure 9 ‣ V-A Applications in Natural Language Processing ‣ V Applications of
    DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers") (b)
    中，DataMap [[58](#bib.bib58)] 能够检测并消除未标记集中的异常样本，从而在使用较少标记样本的情况下显著提高模型准确性。Padmakumar
    等人 [[140](#bib.bib140)] 开发了一种用于澄清和 DAL 的联合策略，用于交互式图像检索任务。在查询新样本的同时请求用户澄清，可以改善模型性能。'
- en: \adfhalfrightarrowhead
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Information Extraction refers to many NLP tasks, including named entity recognition,
    keyword extraction, word segmentation, etc. Manual annotation of large-scale sequences
    is time consuming, expensive, and thus difficult to realize. To address this,
    Brantley et al. [[92](#bib.bib92)] design a new DAL annotation manner. They use
    a noisy heuristic labeling function to provide initial low-quality labels, train
    a classifier to decide whether to trust these labels, and annotate the most uncertain
    samples with trustable labels. Their model achieves high efficiency and effectiveness
    on many information extraction tasks. Similarly, Radmard et al. [[141](#bib.bib141)]
    focus on improving the efficiency of DAL for naming entity recognition by querying
    subsequences within each sentence and propagating labels to unseen identical subsequences
    in the dataset. They demonstrate that the DAL strategy requires only 20% of the
    dataset to achieve the same results as training on the full dataset. Hua et al. [[142](#bib.bib142)]
    propose two model-independent acquisition strategies for identifying and understanding
    the structure of argumentative discourse, achieving competitive results with fewer
    computations The former selects samples with the most novel words for labeling,
    while the latter seeks to identify more relation links by matching any of the
    18 prominent discourse markers from a manual.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 信息提取指的是许多自然语言处理任务，包括命名实体识别、关键词提取、词语分割等。对大规模序列进行手动标注既耗时又昂贵，因此难以实现。为了解决这个问题，Brantley
    等人[[92](#bib.bib92)] 设计了一种新的DAL标注方式。他们使用噪声启发式标注函数提供初始低质量标签，训练分类器以决定是否信任这些标签，并用可靠的标签标注最不确定的样本。他们的模型在许多信息提取任务中实现了高效率和高效果。类似地，Radmard
    等人[[141](#bib.bib141)] 通过查询每个句子中的子序列并将标签传播到数据集中未见的相同子序列，专注于提高DAL在命名实体识别中的效率。他们展示了DAL策略只需20%的数据集即可达到与在整个数据集上训练相同的结果。Hua
    等人[[142](#bib.bib142)] 提出了两种与模型无关的获取策略，用于识别和理解论证话语的结构，取得了计算量较少的竞争性结果。前者选择具有最多新词的样本进行标注，而后者通过匹配手动记录的18个显著话语标记中的任何一个，寻求识别更多的关系链接。
- en: \adfhalfrightarrowhead
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Semantic Parsing aims to convert a natural language utterance to a logical
    form: a machine-understandable representation of its meaning [[162](#bib.bib162)].
    DAL can help reduce data requirements and improve efficiency for semantic parsing.
    For example, Duong et al. [[143](#bib.bib143)] design a simple hyperparameter
    selection technique for DAL to accelerate data annotation. Experiments show that
    their method significantly reduces the need for data annotation and improves the
    model’s performance on semantic parsing. Li et al. [[163](#bib.bib163)] also design
    a hyperparameter tuning module to reduce the additional annotation cost. In addition,
    they design a novel query strategy that prioritizes examples with various logical
    form structures and more lexical choices, which further improve the performance
    for semantic parsing. Cohen et al. [[144](#bib.bib144)] propose a novel DAL method
    with two new annotation manners, called HAT. Experiments show that HAT can pick
    out the most semantically varied and illustrative utterances, leading to the highest
    possible gains in parser performance.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 语义解析旨在将自然语言话语转换为逻辑形式：机器可理解的意义表示[[162](#bib.bib162)]。DAL可以帮助减少数据需求并提高语义解析的效率。例如，Duong
    等人[[143](#bib.bib143)] 设计了一种简单的DAL超参数选择技术，以加快数据标注。实验表明，他们的方法显著减少了数据标注的需求，并改善了模型在语义解析中的表现。Li
    等人[[163](#bib.bib163)] 也设计了一个超参数调整模块，以降低额外的标注成本。此外，他们设计了一种新的查询策略，优先考虑具有各种逻辑形式结构和更多词汇选择的示例，这进一步提升了语义解析的表现。Cohen
    等人[[144](#bib.bib144)] 提出了一个新的DAL方法，具有两种新的标注方式，称为HAT。实验表明，HAT能够挑选出最具语义多样性和说明性的发言，从而在解析器性能上获得最大的提升。
- en: V-B Applications in Computer Vision
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 计算机视觉中的应用
- en: With the remarkable success of CNNs and Vision Transformers, a valuable insight
    has been gained that more labeled image datasets can promote to obtain better
    performance of the task. However, as the amount of data increases, training DNNs
    becomes time and resource consuming. Additionally, even if the number of data
    increases, the presence of noise often leads to limited performance improvement.
    DAL can effectively reduce noise and time consumption in many CV tasks. Hereafter,
    we provide detailed information on specific tasks and their improvements achieved
    with DAL in CV.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 随着CNN和视觉变换器的显著成功，我们获得了一个宝贵的见解：更多标注的图像数据集可以促进任务性能的提升。然而，随着数据量的增加，训练深度神经网络变得耗时且资源消耗大。此外，即使数据量增加，噪声的存在往往会导致性能提升有限。DAL可以有效减少噪声和时间消耗。在此之后，我们提供了有关具体任务及其通过DAL在计算机视觉领域取得的改进的详细信息。
- en: \adfhalfrightarrowhead
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Image Classification aims to accurately classify images based on the provided
    labels for many specific fields such as remote sensing [[16](#bib.bib16)], medical
    imaging [[164](#bib.bib164)] and face recognition [[129](#bib.bib129)]. We list
    the most successful DAL methods for image classification in Section [III-C](#S3.SS3
    "III-C Important DAL Baselines and Datasets ‣ III Deep Active Learning ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers"), such as BCBA, DBAL
    and CEAL, which can be referred to for more detailed information.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '图像分类旨在根据提供的标签准确分类图像，适用于许多特定领域，如遥感[[16](#bib.bib16)]、医学成像[[164](#bib.bib164)]和人脸识别[[129](#bib.bib129)]。我们在[III-C](#S3.SS3
    "III-C Important DAL Baselines and Datasets ‣ III Deep Active Learning ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers")节中列出了图像分类中最成功的DAL方法，如BCBA、DBAL和CEAL，更多详细信息可以参考这些方法。'
- en: \adfhalfrightarrowhead
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Image Captioning aims to automatically generate descriptive text about the content
    of an image. Achieving high-quality captioning requires large-scale datasets with
    diverse images. Unfortunately, creating such a dataset is time-consuming and costly.
    To tackle this issue, Zhang et al. [[145](#bib.bib145)] devise a novel adversarial
    DAL model, which uses visual and textual information to select the most representative
    samples to optimize the performance of image captioning. Experiments show that
    they overcome the limitations of labeled data scarcity and improve the practicality
    and effectiveness of image captioning. In a similar vein, Cheikh et al. [[146](#bib.bib146)]
    introduce a knowledge-transferable DAL framework for low-resorce datasets. They
    take advantage of existing datasets, translating their captions into Arabic, and
    train the model with translated caption datasets as prior knowledge for low-resource
    ArabicFlickr1K datasets (which contain only 1,095 images). Their model achieves
    the Bilingual Evaluation Understudy (BLEU) score of 47%, serving as compelling
    evidence for the effectiveness of their approach.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图像字幕生成旨在自动生成关于图像内容的描述性文本。要实现高质量的字幕生成，需要大规模且多样化的图像数据集。不幸的是，创建这样的数据集既耗时又昂贵。为了解决这个问题，张等人[[145](#bib.bib145)]提出了一种新型的对抗性DAL模型，该模型利用视觉和文本信息来选择最具代表性的样本，从而优化图像字幕生成的性能。实验表明，他们克服了标注数据稀缺的限制，提高了图像字幕生成的实用性和有效性。类似地，Cheikh等人[[146](#bib.bib146)]引入了一种知识转移DAL框架，用于低资源数据集。他们利用现有的数据集，将其字幕翻译成阿拉伯语，并用翻译后的字幕数据集训练模型，以便为低资源的ArabicFlickr1K数据集（仅包含1,095张图像）提供先验知识。他们的模型达到了47%的双语评估下的（BLEU）评分，成为其方法有效性的有力证据。
- en: \adfhalfrightarrowhead
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Semantic Segmentation aims to understand images at the pixel level, serving
    as the basis for various applications, including autonomous driving [[80](#bib.bib80)]
    and robot manipulation [[30](#bib.bib30)]. However, training segmentation models
    requires an extensive amount of data with pixel-wise annotations, a process that
    is burdensome and prohibitively expensive [[78](#bib.bib78)]. To solve this challenge,
    Konyushkova et al. [[147](#bib.bib147)] propose an uncertainty-based DAL method
    with geometric priors to expedite and simplify the annotation process for image
    segmentation. Experiments show that their method can be applied to both background-foreground
    and multi-class segmentation tasks. Qiao et al. [[148](#bib.bib148)] introduce
    a collaborative panoptic regional DAL framework for partial annotated semantic
    segmentation. By incorporating semantic-agnostic panoptic matching and region-based
    selection and extension, their model strikes a balance between labeling efforts
    and performance. Similarly, Xie et al. [[80](#bib.bib80)] propose an automated
    region-based DAL approach for semantic segmentation considering the spatial adjacency
    of image regions and the confidence in prediction. Experiments show that they
    can use a small number of labeled image regions while maximizing segmentation
    performance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割旨在从像素级别理解图像，为各种应用奠定基础，包括自动驾驶 [[80](#bib.bib80)] 和机器人操作 [[30](#bib.bib30)]。然而，训练分割模型需要大量带有像素级标注的数据，这一过程繁琐且成本高昂
    [[78](#bib.bib78)]。为解决这一挑战，Konyushkova 等人 [[147](#bib.bib147)] 提出了一个基于不确定性的DAL方法，并结合几何先验来加速和简化图像分割的标注过程。实验表明，他们的方法适用于背景-前景和多类别分割任务。Qiao
    等人 [[148](#bib.bib148)] 介绍了一个协作全景区域DAL框架，用于部分标注的语义分割。通过结合语义无关的全景匹配和基于区域的选择与扩展，他们的模型在标注工作和性能之间达到了平衡。同样，Xie
    等人 [[80](#bib.bib80)] 提出了一个自动化的基于区域的DAL方法，考虑了图像区域的空间邻接性和预测的置信度。实验表明，他们能够在最大化分割性能的同时，使用少量标注的图像区域。
- en: \adfhalfrightarrowhead
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Object Detection is transformed into a region classification task by generating
    candidate regions of objects from the input image. Features are typically extracted
    from candidate object regions using CNNs and classifiers are subsequently employed
    for the final detection. DAL can reduce labeled data to better fit numerous parameters
    of CNN. Wu et al. [[149](#bib.bib149)] propose a novel hybrid query strategy that
    jointly considers uncertainty and diversity. Extensive experiments are conducted
    on two object detection datasets that effectively demonstrate the superiority
    and effectiveness of their model. Wang et al. [[43](#bib.bib43)] introduce active
    sample mining with switchable selection criteria to incrementally train robust
    object detectors using unlabeled or partially labeled samples, avoiding the influence
    of noisy samples and outliers. The effectiveness of the model is demonstrated
    through extensive experiments on publicly available object detection benchmarks.
    Yuan et al. [[150](#bib.bib150)] define an instance uncertainty learning module
    that takes advantage of the discrepancy of two adversarial instance classifiers
    trained in the labeled set to predict the instance uncertainty of the unlabeled
    set. With iterative instance uncertainty learning and re-weighting, they suppress
    noisy instances, bridging the gap between instance and image-level uncertainty.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测通过从输入图像中生成候选物体区域，将其转化为区域分类任务。特征通常通过CNN从候选物体区域中提取，然后使用分类器进行最终检测。DAL可以减少标注数据，以更好地适应CNN的众多参数。Wu
    等人 [[149](#bib.bib149)] 提出了一个新颖的混合查询策略，联合考虑不确定性和多样性。在两个物体检测数据集上进行了广泛的实验，有效展示了他们模型的优越性和有效性。Wang
    等人 [[43](#bib.bib43)] 引入了具有可切换选择标准的主动样本挖掘方法，以使用未标注或部分标注的样本逐步训练强健的物体检测器，避免了噪声样本和离群点的影响。通过在公开的物体检测基准上进行广泛实验，证明了模型的有效性。Yuan
    等人 [[150](#bib.bib150)] 定义了一个实例不确定性学习模块，该模块利用在标注集上训练的两个对抗性实例分类器的差异来预测未标注集的实例不确定性。通过迭代实例不确定性学习和重标定，他们抑制了噪声实例，弥合了实例级和图像级不确定性之间的差距。
- en: \adfhalfrightarrowhead
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Pose Estimation aims to localize the positions of specific key points in images,
    which has a wide range of applications, such as augmented reality, translation
    of sign language, and human-robot interaction. Obtaining pose annotations can
    be extremely expensive and laborious. To address this issue, Caramalau et al.[[151](#bib.bib151)]
    propose distribution-based methods for the selection of diverse and representative
    samples. Experiments demonstrate their high efficiency and effectiveness for pose
    estimation. Similarly, Shukla et al.[[152](#bib.bib152)] use an uncertainty-based
    query strategy and annotate samples with the lowest confidence scores and further
    improve the performance with fewer labeled samples. Gong et al. [[86](#bib.bib86)]
    design a novel meta agent teaming DAL (MATAL) framework to actively select and
    label informative images for effective learning. MATAL formulates the sample selection
    procedure as a Markov Decision Process and learns an optimal sampling policy that
    effectively maximizes the performance of the pose estimator.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态估计旨在定位图像中特定关键点的位置，这有广泛的应用，如增强现实、手语翻译和人机交互。获取姿态注释可能非常昂贵和费力。为了解决这个问题，Caramalau等人[[151](#bib.bib151)]提出了基于分布的方法，用于选择多样且具有代表性的样本。实验表明，他们的方法在姿态估计中具有很高的效率和有效性。同样，Shukla等人[[152](#bib.bib152)]使用基于不确定性的查询策略，并对置信度最低的样本进行注释，进一步提高了在标注样本较少的情况下的性能。Gong等人[[86](#bib.bib86)]设计了一种新颖的元代理团队DAL（MATAL）框架，以主动选择和标记有用的图像以进行有效学习。MATAL将样本选择过程制定为马尔可夫决策过程，并学习一种优化的采样策略，以有效地最大化姿态估计器的性能。
- en: \adfhalfrightarrowhead
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Target Tracking aims to accurately track targets in images, which can be applied
    for numerous applications, including video surveillance, autonomous vehicles,
    etc. Using DAL can better help train neural networks with limited labeled samples
    for target tracking. Yuan et al. [[153](#bib.bib153)] present a new DAL sequence
    selection method in a multi-frame collaboration way for target tracking. To ensure
    the diversity of selected sequences, they measure samples’ similarity by their
    temporal relation between multiple frames in each video, and they use a nearest
    neighbor discriminator to select the representative samples. Experiments show
    that their method can eliminate background noise and improve efficiency.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 目标跟踪旨在准确跟踪图像中的目标，这可以应用于许多领域，包括视频监控、自动驾驶等。使用DAL可以更好地帮助在有限标注样本下训练神经网络进行目标跟踪。Yuan等人[[153](#bib.bib153)]提出了一种新的DAL序列选择方法，通过多帧协作的方式进行目标跟踪。为了确保所选序列的多样性，他们通过每个视频中多帧之间的时间关系来测量样本的相似性，并使用最近邻鉴别器来选择具有代表性的样本。实验表明，他们的方法能够消除背景噪声并提高效率。
- en: \adfhalfrightarrowhead
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Person Re-identification (Re-ID) aims to match a specific pedestrian using different
    cameras, which is an essential task for public security. Previous efforts mainly
    concentrate on enhancing the performance of Re-ID models, relying on large labeled
    datasets. However, these efforts often overlook data redundancy issues that can
    arise in constructing Re-ID datasets. To address data redundancy in Re-ID datasets,
    Liu et al. [[46](#bib.bib46)] propose an alternative human-in-the-loop model based
    on reinforce learning. In their method, a human annotator provides binary feedback
    to fine-tune a pre-trained CNNs Re-ID model. Extensive experiments prove the superiority
    of their method compared to existing unsupervised, transfer learning, and DAL
    models. On the other hand, Xu et al. [[155](#bib.bib155)] focus on learning from
    scratch with incremental labeling through human annotators and model feedback.
    They combine DAL with an incremental annotation process to select informative
    and diverse samples without redundancy from an unlabeled set in each iteration.
    These samples are then labeled by human annotators to further improve the performance
    of the model.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 人物重识别（Re-ID）旨在通过不同的摄像头匹配特定的行人，这是公共安全的重要任务。以往的努力主要集中在提高Re-ID模型的性能，依赖于大量标注数据集。然而，这些努力往往忽略了在构建Re-ID数据集时可能出现的数据冗余问题。为了解决Re-ID数据集中的数据冗余问题，Liu等人[[46](#bib.bib46)]提出了一种基于强化学习的人机协作模型。在他们的方法中，人工标注员提供二进制反馈以微调预训练的CNN
    Re-ID模型。大量实验证明了他们的方法相较于现有的无监督、迁移学习和DAL模型的优越性。另一方面，Xu等人[[155](#bib.bib155)]专注于通过人工标注员和模型反馈从头开始学习与增量标注。他们将DAL与增量注释过程结合起来，从每次迭代中的未标记集合中选择有用且多样的样本，而不产生冗余。这些样本随后由人工标注员进行标注，以进一步提高模型的性能。
- en: '![Refer to caption](img/e0075b3d81eed3c098921a691985b262.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e0075b3d81eed3c098921a691985b262.png)'
- en: 'Figure 10: The framework of Graph Policy Network [[120](#bib.bib120)].'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：图策略网络的框架 [[120](#bib.bib120)]。
- en: V-C Applications in Graph Data Mining and Learning
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图数据挖掘和学习中的 V-C 应用
- en: There are substantial increase in content-rich network from various domains,
    such as social networks, citation networks, and financial networks. Graphs have
    emerged as a powerful tool for representing and discovering knowledge, with nodes
    representing instances characterized by rich content features and edges denoting
    relationships or interactions between nodes.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 各个领域的内容丰富的网络显著增加，如社交网络、引文网络和金融网络。图已经成为表示和发现知识的强大工具，节点表示由丰富内容特征表征的实例，边缘表示节点之间的关系或互动。
- en: \adfhalfrightarrowhead
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Node Classification is to predict the labels of unlabeled nodes in a partially
    labeled network. GNNs rely heavily on a sufficient number of labeled nodes, which
    is costly and time-consuming. To address this problem, many graph-based DAL methods
    are proposed. For example, ICA-based methods [[165](#bib.bib165)] leverage label
    dependence among neighboring nodes to select diverse samples for node classification,
    while AGE [[166](#bib.bib166)] and ANRMAB [[167](#bib.bib167)] integrate GCNs
    with three traditional DAL query strategies and achieve good performance on many
    node classification datasets. As Fig. [10](#S5.F10 "Figure 10 ‣ V-B Applications
    in Computer Vision ‣ V Applications of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") shows, Hu et al. [[120](#bib.bib120)] present
    a graph policy network for transferable DAL on graphs, which formalizes DAL on
    graphs as a Markov decision process and learns the optimal query strategy with
    reinforce learning. The state is defined based on the current graph status, and
    the action is to select a node for annotation at each query step. The reward is
    defined as the performance gain of the GNNs trained with the selected nodes.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '节点分类是预测部分标记网络中未标记节点的标签。GNNs 非常依赖足够数量的标记节点，这既昂贵又耗时。为了解决这个问题，提出了许多基于图的 DAL 方法。例如，基于
    ICA 的方法 [[165](#bib.bib165)] 利用邻近节点之间的标签依赖性来选择多样的样本进行节点分类，而 AGE [[166](#bib.bib166)]
    和 ANRMAB [[167](#bib.bib167)] 将 GCNs 与三种传统的 DAL 查询策略结合， 在许多节点分类数据集上表现良好。如图 [10](#S5.F10
    "Figure 10 ‣ V-B Applications in Computer Vision ‣ V Applications of DAL ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers") 所示，Hu 等 [[120](#bib.bib120)]
    提出了一个用于图上可转移 DAL 的图策略网络，该网络将图上的 DAL 形式化为一个马尔可夫决策过程，并通过强化学习学习最优查询策略。状态是基于当前图的状态定义的，动作是每次查询步骤选择一个节点进行标注。奖励定义为使用选定节点训练的
    GNNs 的性能提升。'
- en: \adfhalfrightarrowhead
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Link Prediction aims to predict missing or potential links between nodes in
    a given network. It involves using existing connections or relationships to infer
    the likelihood of forming new connections. In the context of link prediction,
    the challenge arises from the limited availability of existing link information
    between nodes in a network. DAL can help alleviate this issue, for example, DALAUP [[168](#bib.bib168)]
    uses neural networks to obtain vector representations of user pairs and utilizes
    multiple query strategies to select informative user pairs for labeling and model
    training, achieving superior performance compared to existing methods. Cai et
    al.[[156](#bib.bib156)] design a multi-view DAL method that reduces the annotation
    cost by selectively querying metadata for the most informative examples, using
    a mapping function from the visual view to the text view. They demonstrate that
    multi-view DAL can use richer information to help improve performance than using
    single view. Zhao et al.[[157](#bib.bib157)] propose a DAL-based transfer learning
    framework for link prediction in recommender systems, which iteratively selects
    entities from source systems for target systems using uncertainty-based criteria.
    Experiments show that their method successfully improves efficiency and effectiveness.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 链接预测旨在预测给定网络中节点之间缺失或潜在的链接。这涉及使用现有的连接或关系来推断形成新连接的可能性。在链接预测的背景下，挑战在于网络中节点之间现有链接信息的有限性。DAL
    可以帮助缓解这个问题，例如，DALAUP[[168](#bib.bib168)] 使用神经网络获得用户对的向量表示，并利用多种查询策略选择有用的用户对进行标记和模型训练，取得了优于现有方法的表现。Cai
    等人[[156](#bib.bib156)] 设计了一种多视角 DAL 方法，通过选择性地查询元数据来减少标注成本，使用从视觉视角到文本视角的映射函数。他们证明，多视角
    DAL 能够利用更丰富的信息来提高性能，而不仅仅是单视角。Zhao 等人[[157](#bib.bib157)] 提出了一个基于 DAL 的迁移学习框架，用于推荐系统中的链接预测，该框架使用基于不确定性的标准迭代选择源系统中的实体以用于目标系统。实验表明，他们的方法成功提高了效率和效果。
- en: \adfhalfrightarrowhead
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Community Detection aims to accurately partition nodes into distinct classes
    based on the topological structure of the networks. However, in many practical
    scenarios, unsupervised methods struggle to achieve the exact community. To solve
    this issue, Gupta et al. [[158](#bib.bib158)] propose community trolling, a DAL-based
    method for topic-based community detection. Their method selects relevant samples
    from polluted big data, reducing the unreliable dataset to a reliable one for
    studying communities. Chien et al. [[159](#bib.bib159)] propose a novel DAL method
    for geometric community detection. They first remove many cross-cluster edges
    while preserving intra-cluster connectivity to avoid noise. Then, they interactively
    query the label of one node for each disjoint component to recover the underlying
    clusters. Experiments show that they can achieve SOTA performance in community
    detection.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 社区检测旨在根据网络的拓扑结构将节点准确地划分为不同的类别。然而，在许多实际场景中，无监督方法难以实现精确的社区划分。为了解决这个问题，Gupta 等人[[158](#bib.bib158)]
    提出了社区钓鱼，这是一种基于 DAL 的主题社区检测方法。他们的方法从污染的大数据中选择相关样本，将不可靠的数据集减少到一个可靠的集合中，以研究社区。Chien
    等人[[159](#bib.bib159)] 提出了一个新颖的 DAL 方法用于几何社区检测。他们首先去除许多跨簇边，同时保留簇内的连通性以避免噪声。然后，他们交互式地查询每个离散组件的一个节点的标签，以恢复潜在的簇。实验表明，他们在社区检测中可以达到
    SOTA 性能。
- en: V-D Other Selected Interesting Applications
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 其他精选有趣的应用
- en: \adfhalfrightarrowhead
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: 'Engineering Systems. DAL methods exhibit remarkable performance in computationally
    demanding engineering systems by significantly reducing running time and computational
    costs. For example, Yue et al. [[169](#bib.bib169)] introduce two novel DAL algorithms:
    the variance-based weighted AL and the D-optimal weighted AL, designed specifically
    for Gaussian processes with uncertainties. Numerical studies demonstrate the effectiveness
    of their approach, notably improving predictive modeling for automatic shape control
    of composite fuselage structures. In another vein, Lee et al. [[170](#bib.bib170)]
    optimize their DAL acquisition function by jointly considering safe variance reduction
    and safe region expansion tasks, aiming to minimize failures without explicit
    knowledge of failure regions. This approach is tailored for real systems with
    uncertain failure conditions, as demonstrated in the predictive modeling of composite
    fuselage deformation, achieving zero failures by considering the composite failure
    criterion. Furthermore, Lee et al. [[171](#bib.bib171)] introduce a partitioned
    DAL method, comprising two systematic steps: global searching for uncertain design
    spaces and local searching using local Gaussian processes. They apply their method
    to aerospace manufacturing and materials science, achieving superior performance
    in prediction accuracy and computational efficiency compared to benchmarks.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 工程系统。DAL方法在计算要求高的工程系统中表现出显著的性能，通过大幅减少运行时间和计算成本。例如，岳等人 [[169](#bib.bib169)] 介绍了两种新颖的DAL算法：基于方差的加权AL和D-最优加权AL，专门针对具有不确定性的高斯过程。数值研究展示了他们方法的有效性，显著改善了复合机身结构的自动形状控制的预测建模。另一方面，李等人 [[170](#bib.bib170)]
    通过联合考虑安全方差减少和安全区域扩展任务来优化其DAL获取函数，旨在在没有明确失败区域知识的情况下最小化失败。这种方法针对具有不确定失败条件的实际系统，如在复合机身变形的预测建模中，通过考虑复合失败标准实现了零失败。此外，李等人 [[171](#bib.bib171)]
    引入了一种分区DAL方法，包括两个系统步骤：全局搜索不确定设计空间和使用局部高斯过程进行局部搜索。他们将其方法应用于航空航天制造和材料科学，与基准相比，在预测准确性和计算效率方面取得了优越的表现。
- en: \adfhalfrightarrowhead
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Personalized Medical Treatment explores how patient health is affected by taking
    a drug and how user questions are answered by search recommendation [[172](#bib.bib172)].
    Although modern methods can achieve impressive performance, they need a significant
    amount of labeled data. To solve this issue, Deng et al. [[160](#bib.bib160)]
    propose the use of DAL to recruit patients and assign treatments that reduce the
    uncertainty of an Individual Treatment Effect model. Sundin et al. [[173](#bib.bib173)]
    propose to use a Gaussian process to model the individual treatment effect and
    use the expected information gain over the S-type error rate, defined as the error
    in predicting the sign of the conditional average treatment effect, as their acquisition
    function. Jesson et al. [[113](#bib.bib113)] develop epistemic uncertainty-aware
    methods for DAL of personalized treatment effects from high-dimensional observational
    data. In contrast to previous work that only uses information gain as the acquisition
    objective, they propose Causal-BALD because they consider both information gain
    and overlap between the treatment and control groups. Li et al. [[174](#bib.bib174)]
    used DAL to help people by recognizing their emotion.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化医疗治疗探讨了患者健康如何受药物影响以及用户问题如何通过搜索推荐得到回答 [[172](#bib.bib172)]。虽然现代方法能实现令人印象深刻的性能，但需要大量标注数据。为解决这一问题，邓等人 [[160](#bib.bib160)]
    提出了使用DAL来招募患者并分配治疗，以减少个体治疗效果模型的不确定性。桑丁等人 [[173](#bib.bib173)] 提出了使用高斯过程来建模个体治疗效果，并使用预期的信息增益来作为获取函数，其定义为预测条件平均治疗效果符号的误差。杰森等人 [[113](#bib.bib113)]
    开发了关注认知不确定性的DAL方法，用于从高维观察数据中获取个性化治疗效果。与之前仅使用信息增益作为获取目标的工作不同，他们提出了Causal-BALD，因为他们同时考虑了信息增益和治疗与对照组之间的重叠。李等人 [[174](#bib.bib174)]
    使用DAL帮助人们识别他们的情绪。
- en: 'TABLE IV: Summary of various challenges and opportunities.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：各种挑战和机会的总结。
- en: '| Challenge Types | Challenges | Opportunities |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 挑战类型 | 挑战 | 机会 |'
- en: '| Pipeline-related Issues | Inefficient & Costly human annotation | servers,
    workers and annotators share information [[134](#bib.bib134)]. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 管道相关问题 | 低效且成本高的人为标注 | 服务器、工人和标注员共享信息 [[134](#bib.bib134)]。 |'
- en: '| self-supervised pseudo-labels to reduce human efforts [[36](#bib.bib36),
    [85](#bib.bib85)]. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 使用自监督伪标签减少人工工作量 [[36](#bib.bib36), [85](#bib.bib85)]。 |'
- en: '| incorporate additional knowledge to reduce expert knowledge [[94](#bib.bib94),
    [148](#bib.bib148)]. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 引入额外的知识以减少专家知识 [[94](#bib.bib94), [148](#bib.bib148)]。 |'
- en: '| Insufficient research on stopping strategies | the confidence among the selected
    samples does not increase [[175](#bib.bib175)]. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 停止策略的研究不足 | 选定样本之间的置信度没有增加 [[175](#bib.bib175)]。 |'
- en: '| stop when all instances lie between two contour lines [[176](#bib.bib176)].
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 当所有实例都位于两个轮廓线之间时停止 [[176](#bib.bib176)]。 |'
- en: '| upper bound in expected generalization errors as stopping criterion [[177](#bib.bib177)].
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 作为停止标准的期望泛化误差的上界 [[177](#bib.bib177)]。 |'
- en: '| Cold-start | use pre-trained embeddings [[31](#bib.bib31), [178](#bib.bib178)].
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 冷启动 | 使用预训练的嵌入 [[31](#bib.bib31), [178](#bib.bib178)]。 |'
- en: '| design initial queries [[179](#bib.bib179), [180](#bib.bib180)]. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 设计初始查询 [[179](#bib.bib179), [180](#bib.bib180)]。 |'
- en: '| use diverse sampling [[181](#bib.bib181), [182](#bib.bib182)]. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 使用多样化采样 [[181](#bib.bib181), [182](#bib.bib182)]。 |'
- en: '| Tasks-related Issues | Difficulty in cross-domain transfer | select samples
    in regions of joint disagreement between models [[84](#bib.bib84), [183](#bib.bib183),
    [154](#bib.bib154)]. |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 任务相关问题 | 跨领域转移的困难 | 在模型的共同分歧区域选择样本 [[84](#bib.bib84), [183](#bib.bib183),
    [154](#bib.bib154)]。 |'
- en: '| source and target domain distribution matching [[110](#bib.bib110), [184](#bib.bib184)].
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 源领域和目标领域分布匹配 [[110](#bib.bib110), [184](#bib.bib184)]。 |'
- en: '| transferable DAL policies between the source and target graphs [[120](#bib.bib120)].
    |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 源图和目标图之间可转移的DAL策略 [[120](#bib.bib120)]。 |'
- en: '| Unstable performance | avoid DAL’s sensitivity to the initial labeled set [[176](#bib.bib176),
    [185](#bib.bib185), [94](#bib.bib94), [31](#bib.bib31), [53](#bib.bib53)]. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 不稳定的表现 | 避免DAL对初始标记集的敏感性 [[176](#bib.bib176), [185](#bib.bib185), [94](#bib.bib94),
    [31](#bib.bib31), [53](#bib.bib53)]。 |'
- en: '| use distribution information to improve model’s robustness [[186](#bib.bib186),
    [187](#bib.bib187), [187](#bib.bib187)]. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 使用分布信息来提高模型的鲁棒性 [[186](#bib.bib186), [187](#bib.bib187), [187](#bib.bib187)]。
    |'
- en: '| use pre-trained language model [[61](#bib.bib61), [188](#bib.bib188)]. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 使用预训练语言模型 [[61](#bib.bib61), [188](#bib.bib188)]。 |'
- en: '| Lack of scalability & generalizability | hybrid strategies for sample selection [[60](#bib.bib60),
    [104](#bib.bib104)]. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 缺乏可扩展性和泛化能力 | 样本选择的混合策略 [[60](#bib.bib60), [104](#bib.bib104)]。 |'
- en: '| nearest-neighbor classifiers [[189](#bib.bib189)]. |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 最近邻分类器 [[189](#bib.bib189)]。 |'
- en: '| combining annotation and counterfactual sample construction [[190](#bib.bib190),
    [191](#bib.bib191)]. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 结合标注和反事实样本构建 [[190](#bib.bib190), [191](#bib.bib191)]。'
- en: '| Datasets-related Issues | Outlier Data & Noisy Oracles | find the best balance
    between purity and informativeness [[126](#bib.bib126), [89](#bib.bib89)]. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 数据集相关问题 | 异常数据与噪声oracle | 寻找纯度与信息量之间的最佳平衡 [[126](#bib.bib126), [89](#bib.bib89)]。
    |'
- en: '| knowledge distillation [[14](#bib.bib14)]. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 知识蒸馏 [[14](#bib.bib14)]。 |'
- en: '| relabeling frameworks for correst oracle labels [[81](#bib.bib81), [192](#bib.bib192),
    [193](#bib.bib193)]. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 用于纠正oracle标签的重新标记框架 [[81](#bib.bib81), [192](#bib.bib192), [193](#bib.bib193)]。
    |'
- en: '| Data Scarcity & Imbalance | data augmentation and large PLMs  [[12](#bib.bib12),
    [32](#bib.bib32), [97](#bib.bib97)]. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 数据稀缺与不平衡 | 数据增强和大型PLMs [[12](#bib.bib12), [32](#bib.bib32), [97](#bib.bib97)]。
    |'
- en: '| cost-sensitive learning [[176](#bib.bib176), [194](#bib.bib194)]. |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 成本敏感学习 [[176](#bib.bib176), [194](#bib.bib194)]。 |'
- en: '| design new query strategies for imbalanced datasets [[195](#bib.bib195),
    [196](#bib.bib196), [197](#bib.bib197)]. |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 为不平衡数据集设计新的查询策略 [[195](#bib.bib195), [196](#bib.bib196), [197](#bib.bib197)]。
    |'
- en: '| Class distribution mismatch | new DAL query strategy [[125](#bib.bib125),
    [198](#bib.bib198)]. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 类别分布不匹配 | 新的DAL查询策略 [[125](#bib.bib125), [198](#bib.bib198)]。 |'
- en: '| new DAL framework [[199](#bib.bib199)]. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 新的DAL框架 [[199](#bib.bib199)]。 |'
- en: '| incoporate additional detector [[200](#bib.bib200)]. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 引入额外的检测器 [[200](#bib.bib200)]。 |'
- en: VI Challenges & Opportunities of DAL
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI DAL的挑战与机遇
- en: 'As Table [IV](#S5.T4 "TABLE IV ‣ V-D Other Selected Interesting Applications
    ‣ V Applications of DAL ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers") shows, hereafter, we summarize the challenges and the corresponding
    potential solutions and opportunities.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[IV](#S5.T4 "TABLE IV ‣ V-D Other Selected Interesting Applications ‣ V Applications
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")所示，我们总结了挑战及相应的潜在解决方案和机遇。'
- en: VI-A Pipeline-related Issues
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 流程相关问题
- en: \adfhalfrightarrowhead
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Inefficient & Costly Human Annotation. DAL assumes that human annotators are
    readily available to label new samples once they are required. However, this assumption
    may not hold in some real-world applications. Human annotators can get tired or
    need breaks, forcing the DAL process to be suspended until they reappear. Moreover,
    human annotation is time-consuming and needs expert knowledge, resulting in long
    waits before models can be re-trained with new labeled data.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 低效且昂贵的人力标注。DAL假设人力标注员在需要时可以随时标注新样本。然而，这一假设在某些实际应用中可能不成立。人力标注员可能会感到疲倦或需要休息，导致DAL过程被暂停，直到他们重新出现。此外，人力标注费时且需要专业知识，因此在模型可以用新标记数据重新训练之前需要长时间等待。
- en: '![Refer to caption](img/eaf8035ee5f74fa126428fb051a2ceb0.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eaf8035ee5f74fa126428fb051a2ceb0.png)'
- en: 'Figure 11: The framework for efficiently annotation.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：高效标注的框架。
- en: 'To improve efficiency, DAL methods incorporate additional techniques to reduce
    human annotation. Wang et al. [[36](#bib.bib36)] use self-supervised learning
    by adding pseudo-labels with high confidence to help reduce human effort and improve
    the performance of the model. Go one step further, Yang et al. [[85](#bib.bib85)]
    introduce multiple pseudo-annotators that provide labels for unlabeled samples,
    achieving good performance without requiring human expert knowledge. On the other
    hand, as shown in Fig. [11](#S6.F11 "Figure 11 ‣ VI-A Pipeline-related Issues
    ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers"), Huang et al. [[134](#bib.bib134)] propose a new
    annotation strategy to allow servers, workers, and annotators to cooperate efficiently
    for sharing candidate queries and annotations. Experiments show that their model
    can avoid annotation noise and save much time for re-checking annotations. To
    further reduce expert knowledge, others tend to reduce the search scope in each
    iteration to improve efficiency. For example, Yang et al. [[94](#bib.bib94)] restrict
    candidate samples to their nearest neighbors of the labeled set rather than scanning
    all data.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提高效率，DAL方法结合了额外的技术以减少人力标注。Wang等人[[36](#bib.bib36)]通过添加高置信度的伪标签来进行自监督学习，以帮助减少人工工作量并提高模型的性能。更进一步，Yang等人[[85](#bib.bib85)]引入了多个伪标注者来为未标记样本提供标签，实现了良好的性能而无需人类专家知识。另一方面，如图[11](#S6.F11
    "Figure 11 ‣ VI-A Pipeline-related Issues ‣ VI Challenges & Opportunities of DAL
    ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")所示，Huang等人[[134](#bib.bib134)]提出了一种新的标注策略，以使服务器、工作者和标注员能够高效合作，共享候选查询和标注。实验表明，他们的模型可以避免标注噪声并节省大量重新检查标注的时间。为了进一步减少专家知识，其他研究倾向于在每次迭代中减少搜索范围以提高效率。例如，Yang等人[[94](#bib.bib94)]将候选样本限制为标记集的最近邻，而不是扫描所有数据。'
- en: \adfhalfrightarrowhead
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Insufficient Research on Stopping Strategies. Few studies are designed for stopping
    strategies of DAL methods [[196](#bib.bib196)]. However, stopping strategies are
    essential for DAL because they reduce the amount of human labor by limiting the
    number of samples that need to be labeled and prevent the inclusion of noisy and
    redundant samples, which can negatively affect the performance of DAL models.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对停止策略的研究不足。关于DAL方法的停止策略的研究很少[[196](#bib.bib196)]。然而，停止策略对DAL至关重要，因为它们通过限制需要标注的样本数量来减少人工劳动，并防止噪声和冗余样本的包含，这可能会对DAL模型的性能产生负面影响。
- en: McDonald et al. [[175](#bib.bib175)] design two novel stopping strategies for
    DAL methods in the document classification task. The first strategy measures the
    overall confidence of the classifiers in correctly classifying the remaining unlabeled
    documents. It assumes that when the classifier’s mean confidence level for the
    remaining documents stabilizes, the model stops the DAL process, since its effectiveness
    would no longer improve. The second strategy measures the confidence of the classifiers
    among the selected documents to be reviewed. It assumes that when the classifier’s
    confidence stops increasing for these documents, it has reached its maximal confidence
    and stops the DAL process. Benefiting from the idea of the margin exhaustion criterion,
    Yu et al. [[176](#bib.bib176)] identify two corresponding contour lines in the
    instance space and assume that the DAL process can only be stopped when all instances
    lying between these two contour lines have been labeled. They achieve good performance
    in many classification tasks. Based on the Bayesian theory, Ishibashi et al. [[177](#bib.bib177)]
    derive a novel upper bound for the difference in expected generalization errors
    before and after obtaining new training data. They then combine this upper bound
    with a statistical test to derive a stopping criterion for DAL and significantly
    improve efficiency.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: McDonald 等人 [[175](#bib.bib175)] 为文档分类任务设计了两种新颖的停止策略。第一种策略测量分类器对剩余未标记文档的整体信心。它假设当分类器对剩余文档的平均信心水平稳定时，模型停止
    DAL 过程，因为其效果不再改善。第二种策略测量在选择的待审文档中分类器的信心。它假设当分类器对这些文档的信心停止增长时，分类器已达到最大信心并停止 DAL
    过程。受边际耗尽准则的启发，Yu 等人 [[176](#bib.bib176)] 在实例空间中识别出两个对应的轮廓线，并假设当这些轮廓线之间的所有实例都已被标记时，才能停止
    DAL 过程。他们在许多分类任务中取得了良好的表现。基于贝叶斯理论，Ishibashi 等人 [[177](#bib.bib177)] 推导出了新训练数据获取前后期望泛化误差差异的一个新上界。他们将这个上界与统计测试相结合，推导出了一个
    DAL 停止准则，并显著提高了效率。
- en: \adfhalfrightarrowhead
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Cold-start. Most DAL methods fail to improve over random selection when the
    annotation budget is very small, a phenomenon sometimes term as “cold-start” [[179](#bib.bib179)].
    Uncertainty sampling has been shown to be inherently unsuitable for low budgets,
    possibly explaining the cold-start phenomenon [[201](#bib.bib201)]. Low budgets
    can be seen in many applications, especially those that require an expert tagger
    whose time is expensive. If we want to expand deep learning to new domains, overcoming
    the cold-start problem is an ever-important task.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 冷启动。大多数 DAL 方法在注释预算非常小的情况下无法优于随机选择，这种现象有时被称为“冷启动”[[179](#bib.bib179)]。不确定性采样被证明本质上不适用于低预算，这可能解释了冷启动现象
    [[201](#bib.bib201)]。低预算在许多应用中都可以看到，特别是那些需要花费昂贵时间的专家标注员的应用。如果我们想将深度学习扩展到新的领域，克服冷启动问题是一个极其重要的任务。
- en: '![Refer to caption](img/e7396d4ecc1e6860c507f61e8bc7cb2b.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e7396d4ecc1e6860c507f61e8bc7cb2b.png)'
- en: 'Figure 12: An example for cold-start data selection.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：冷启动数据选择的示例。
- en: 'To relieve the cold-start issue, Yuan et al. [[31](#bib.bib31)] use pre-trained
    embeddings on unsupervised tasks, decreasing budget dependency while remaining
    faithful to uncertainty sampling. Similarly, Yu et al. [[178](#bib.bib178)] try
    to use pre-trained knowledge from PLMs to avoid cold-start. They select few shot
    samples to fine-tune large-scale PLM, achieve SOTA performance in six datasets,
    and improve the efficiency of labeling over existing baselines by 3.2%–6.9% on
    average. On the other hand, in Fig. [12](#S6.F12 "Figure 12 ‣ VI-A Pipeline-related
    Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") (a-b), Yehuda et al. [[180](#bib.bib180)]
    develop a new DAL initialization strategy to solve the cold-start issue for low-budget
    image classification, which significantly outperforms CoreSet initialization in
    the low-budget regime. They also theoretically analyze different DAL strategies
    in embedding spaces and improve performance on both low- and high-budget scenes.
    In Fig. [12](#S6.F12 "Figure 12 ‣ VI-A Pipeline-related Issues ‣ VI Challenges
    & Opportunities of DAL ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers") (c), Cao et al. [[181](#bib.bib181)] apply the informative sampling
    policy on the $\gamma$ tube to solve the cold-start sampling problem. Mahmood
    et al. [[182](#bib.bib182)] query a diverse set of examples with minimal Wasserstein
    distance from unlabeled data. They report a significant performance boost in the
    low-budget regime.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解冷启动问题，Yuan 等人 [[31](#bib.bib31)] 使用在无监督任务上预训练的嵌入，减少了预算依赖，同时保持了对不确定性采样的忠实。类似地，Yu
    等人 [[178](#bib.bib178)] 尝试使用来自 PLMs 的预训练知识来避免冷启动。他们选择少量样本来微调大规模 PLM，在六个数据集上实现了
    SOTA 性能，并在现有基准上平均提高了 3.2%–6.9% 的标注效率。另一方面，在图 [12](#S6.F12 "图 12 ‣ VI-A 流程相关问题
    ‣ VI DAL 的挑战与机遇 ‣ 深度主动学习调查：近期进展与新前沿") (a-b) 中，Yehuda 等人 [[180](#bib.bib180)] 开发了一种新的
    DAL 初始化策略，以解决低预算图像分类的冷启动问题，该策略在低预算情况下显著优于 CoreSet 初始化。他们还在嵌入空间中理论分析了不同的 DAL 策略，并在低预算和高预算场景中提高了性能。在图 [12](#S6.F12
    "图 12 ‣ VI-A 流程相关问题 ‣ VI DAL 的挑战与机遇 ‣ 深度主动学习调查：近期进展与新前沿") (c) 中，Cao 等人 [[181](#bib.bib181)]
    在 $\gamma$ 立管上应用了信息采样策略以解决冷启动采样问题。Mahmood 等人 [[182](#bib.bib182)] 查询了与未标记数据距离最小的多样本。他们报告了在低预算情况下显著的性能提升。
- en: VI-B Task-related Issues
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 任务相关问题
- en: \adfhalfrightarrowhead
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Difficulty in Cross-domain Transfer. We discuss two difficulties of cross-domain
    transfer in DAL. First, machine learning systems are always deployed on various
    devices with the same labeled dataset. However, DAL is often model-dependent and
    not directly transferable, i.e., data queried for one model may be less effective
    for another [[183](#bib.bib183)]; Second, transfer learning biases DAL to select
    samples that match the distribution of the source domain to the target domain,
    leading to sampling bias and the high cost of transfer learning.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域转移的困难。我们讨论了 DAL 中跨领域转移的两种困难。首先，机器学习系统总是在相同的标记数据集上部署在各种设备上。然而，DAL 通常依赖于模型，且不可直接转移，即为一个模型查询的数据可能对另一个模型效果较差 [[183](#bib.bib183)]；其次，迁移学习使
    DAL 倾向于选择与源领域分布匹配的样本到目标领域，导致采样偏差和迁移学习的高成本。
- en: '![Refer to caption](img/257444e710c80f632886dcfef31526ac.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/257444e710c80f632886dcfef31526ac.png)'
- en: 'Figure 13: Multi-task learning transfer knowledge from sources [[84](#bib.bib84)].'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：多任务学习从来源中转移知识 [[84](#bib.bib84)]。
- en: 'To benefit multiple target models, some methods aim to select samples in joint
    disagreement regions across models [[183](#bib.bib183)], adopt multi-agent reinforcement
    learning for optimal selection [[154](#bib.bib154)], or leverage multi-task learning
    to transfer common knowledge from the source domain as shown in Fig. [13](#S6.F13
    "Figure 13 ‣ VI-B Task-related Issues ‣ VI Challenges & Opportunities of DAL ‣
    A Survey on Deep Active Learning: Recent Advances and New Frontiers"). To avoid
    sampling bias, Farquhar et al. [[184](#bib.bib184)] apply corrective weighting
    using an unbiased risk estimator to maintain the target distribution during pool-based
    sampling. Trang et al. [[110](#bib.bib110)] introduce a heuristic query strategy
    that matches the distribution of the source domain while retrieving valuable target
    samples. Hu et al. [[120](#bib.bib120)] learn transferable DAL policies on labeled
    source graphs that generalize selection to unlabeled target graphs. Experiments
    show that the above methods can achieve excellent performance and transferability.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使多个目标模型受益，一些方法旨在选择模型间共同的不一致区域的样本[[183](#bib.bib183)]，采用多智能体强化学习进行最佳选择[[154](#bib.bib154)]，或利用多任务学习从源领域转移共同知识，如图[13](#S6.F13
    "Figure 13 ‣ VI-B Task-related Issues ‣ VI Challenges & Opportunities of DAL ‣
    A Survey on Deep Active Learning: Recent Advances and New Frontiers")所示。为避免采样偏差，Farquhar
    等人[[184](#bib.bib184)] 使用无偏风险估计器进行校正加权，以在池式采样过程中保持目标分布。Trang 等人[[110](#bib.bib110)]
    引入了一种启发式查询策略，以匹配源领域的分布，同时检索有价值的目标样本。Hu 等人[[120](#bib.bib120)] 在标记源图上学习可转移的 DAL
    策略，以将选择推广到未标记的目标图。实验表明，以上方法能够实现出色的性能和可转移性。'
- en: \adfhalfrightarrowhead
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Unstable Performance. DAL methods always have unstable performance, i.e., results
    for the same method vary significantly with different initialized seeds [[108](#bib.bib108)].
    Two primary reasons can explain this instability. First, the DAL methods are sensitive
    to the initial labeled dataset. The initial selected samples have a great influence
    on the eventual outcome of the current approaches. With insufficient initial labeling,
    subsequent DAL cycles become highly biased, resulting in poor selection. Second,
    current DAL methods always separate active learning and deep learning methods
    into two separate processes, easily leading to sub-optimal and unstable performance [[202](#bib.bib202)].
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 不稳定的性能。DAL 方法通常具有不稳定的性能，即相同方法的结果在不同初始化种子下变化显著[[108](#bib.bib108)]。这种不稳定性可以用两个主要原因来解释。首先，DAL
    方法对初始标注数据集非常敏感。初始选择的样本对当前方法的最终结果有很大的影响。标注不足时，后续的 DAL 循环会变得高度偏向，从而导致选择效果较差。其次，当前的
    DAL 方法通常将主动学习和深度学习方法分为两个独立的过程，这容易导致次优和不稳定的性能[[202](#bib.bib202)]。
- en: To solve DAL’s sensitivity to the initialization, current methods always use
    diverse sampling and pre-trained models. Yu et al. [[176](#bib.bib176)] adopt
    hierarchical clustering to select 10% samples near each clustering center as representative
    samples. Their new initialization greatly helps stabilize the performance. Zlabinger
    et al. [[185](#bib.bib185)] take into account both diversity and polarization
    to effectively select initial samples for DAL methods that further stabilize the
    performance of the DAL process. Yang et al. [[94](#bib.bib94)] select initial
    samples by evaluating the total distance between the unlabeled samples and the
    initial samples, showing that the same distance between them can result in better
    and stable performance. On the other hand, Yuan et al. [[31](#bib.bib31)] incorporate
    language information as prior knowledge to help learn node representations and
    use clustering methods to select the initial data. Similarly, Ein-Dor [[53](#bib.bib53)]
    uses BERT to learn the representations of the input sentences and uses a hybrid
    query strategy to select the most uncertain and diverse samples as the initialized
    training data.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 DAL 对初始化的敏感性，当前方法通常使用多样本采样和预训练模型。Yu 等人[[176](#bib.bib176)] 采用层次聚类选择每个聚类中心附近的
    10% 样本作为代表样本。他们的新初始化方法大大有助于稳定性能。Zlabinger 等人[[185](#bib.bib185)] 考虑了多样性和极化因素，以有效选择初始样本，从而进一步稳定
    DAL 过程的性能。Yang 等人[[94](#bib.bib94)] 通过评估未标记样本与初始样本之间的总距离来选择初始样本，表明相同的距离可以带来更好且稳定的性能。另一方面，Yuan
    等人[[31](#bib.bib31)] 将语言信息作为先验知识来帮助学习节点表示，并使用聚类方法选择初始数据。类似地，Ein-Dor[[53](#bib.bib53)]
    使用 BERT 学习输入句子的表示，并采用混合查询策略选择最不确定和多样的样本作为初始化训练数据。
- en: '![Refer to caption](img/a593789a38e0ff64fc47d18cdb0d5d90.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a593789a38e0ff64fc47d18cdb0d5d90.png)'
- en: 'Figure 14: Stable performance of TrustAL [[186](#bib.bib186)].'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：TrustAL的稳定性能 [[186](#bib.bib186)]。
- en: 'To bridge the gap between AL and deep learning models, Kwak et al. [[186](#bib.bib186)]
    introduce Trustworthy AL (TrustAL), a label-efficient DAL framework by transferring
    distilled knowledge from deep learning models to the data selection process. As
    Fig. [14](#S6.F14 "Figure 14 ‣ VI-B Task-related Issues ‣ VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")
    shows, they jointly optimize knowledge distillation and DAL to obtain a more consistent
    and reliable performance compared to the two best performing baselines on three
    benchmarks. Similarly, Ma et al. [[187](#bib.bib187)] learn nonlinear embeddings
    to map inputs into a latent space and introduce a selection block to choose representative
    samples in the learned latent space to achieve stable performance. Margatina et
    al. [[61](#bib.bib61)] extend the PLMs to continually pre-train on available unlabeled
    data to tailor it to the task-specific domain, where they can benefit from both
    labeled and unlabeled data at each DAL iteration. Their experiments show considerable
    enhancements in data efficiency and stability compared to the standard fine-tuning
    approach, emphasizing the importance of a suitable training strategy in DAL. Mamooler
    et al. [[188](#bib.bib188)] try to combine DAL with PLMs in the legal domain,
    where they use unlabeled data in three stages: training the model to adjust it
    to the downstream task, using knowledge distillation to direct the embeddings
    to a semantically meaningful space, and identifying the initial set.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥合AL与深度学习模型之间的差距，Kwak等人 [[186](#bib.bib186)] 介绍了值得信赖的AL（TrustAL），这是一个通过将深度学习模型的提炼知识转移到数据选择过程中的标记高效的DAL框架。如图[14](#S6.F14
    "图 14 ‣ VI-B 任务相关问题 ‣ VI DAL的挑战与机遇 ‣ 深度主动学习综述：最新进展与新前沿")所示，他们将知识蒸馏与DAL联合优化，以获得比在三个基准测试中表现最好的两个基线更一致、更可靠的性能。同样，Ma等人 [[187](#bib.bib187)]
    学习非线性嵌入，将输入映射到潜在空间，并引入选择块以选择在学习的潜在空间中的代表性样本，以实现稳定性能。Margatina等人 [[61](#bib.bib61)]
    扩展了PLMs，持续在可用的未标记数据上进行预训练，以使其适应任务特定的领域，在每次DAL迭代中，他们可以从标记和未标记数据中受益。他们的实验显示，相较于标准的微调方法，在数据效率和稳定性方面有显著提升，强调了在DAL中合适的训练策略的重要性。Mamooler等人 [[188](#bib.bib188)]
    尝试将DAL与法律领域中的PLMs结合起来，他们在三个阶段使用未标记数据：训练模型以调整其适应下游任务，使用知识蒸馏将嵌入引导到语义上有意义的空间，以及识别初始集合。
- en: \adfhalfrightarrowhead
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Lack of Scalability & Generalizability. Current DAL methods lack scalability,
    as they always require significant modifications to neural network architectures
    for adapting to different query strategies. Another issue with current methods
    is their heavy reliance on DAL’s weight parameters, while the parameters may not
    be generalizable to different datasets. Users are required to prepare additional
    labeled samples as a validation set to tune parameters by cross-validation, which
    contradicts the goal of minimizing the need for labeled data.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏可扩展性和泛化能力。当前的DAL方法缺乏可扩展性，因为它们总是需要对神经网络架构进行重大修改，以适应不同的查询策略。当前方法的另一个问题是它们过于依赖DAL的权重参数，而这些参数可能无法泛化到不同的数据集。用户需要准备额外的标记样本作为验证集，通过交叉验证来调整参数，这与最小化对标记数据需求的目标相矛盾。
- en: 'In response to the above issues, Maekawa et al. [[60](#bib.bib60)] introduce
    a novel DAL method, called TYROGUE, that uses a hybrid query strategy to improve
    model generalization and reduce labeling costs. As Figure [15](#S6.F15 "Figure
    15 ‣ VI-B Task-related Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey
    on Deep Active Learning: Recent Advances and New Frontiers") shows, uncertainty-based
    methods tend to acquire similar data points from a specific area within an iteration,
    diversity-based methods tend to acquire data points similar to the samples acquired
    in previous iterations, and TYROGUE balances diversity and uncertainty by acquiring
    samples that are diverse and also closer to the model decision boundary. RMQCAL [[104](#bib.bib104)]
    is a novel scalable DAL method, which allows for any number and type of query
    criteria, eliminates the need for empirical parameters, and makes the trade-offs
    between the query criteria self-adaptive. On the other hand, Wan et al. [[189](#bib.bib189)]
    propose an embedded network of nearest-neighbor classifiers to enhance the generalization
    ability of models trained in labeled and unlabeled sub-spaces in a simple but
    effective manner. Deng et al. [[190](#bib.bib190)] focus on combining sample annotation
    and counterfactual sample construction in the DAL procedure to enhance the model’s
    out-of-distribution generalization. Wang et al. [[191](#bib.bib191)] introduce
    a new training manner to improve model’s generalizability and show a strong positive
    correlation between convergence speed and generalization performance under ultra-wide
    conditions.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '针对上述问题，Maekawa 等人 [[60](#bib.bib60)] 引入了一种新颖的 DAL 方法，称为 TYROGUE，该方法使用混合查询策略来提高模型的泛化能力并降低标注成本。正如图
    [15](#S6.F15 "Figure 15 ‣ VI-B Task-related Issues ‣ VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")
    所示，不确定性基础的方法倾向于在一次迭代中从特定区域获取相似的数据点，而多样性基础的方法倾向于获取与前几次迭代中获得的样本相似的数据点，而 TYROGUE
    通过获取多样且接近模型决策边界的样本来平衡多样性和不确定性。RMQCAL [[104](#bib.bib104)] 是一种新颖的可扩展 DAL 方法，允许任意数量和类型的查询标准，消除了对经验参数的需求，并使查询标准之间的权衡自适应。另一方面，Wan
    等人 [[189](#bib.bib189)] 提出了一个嵌入式最近邻分类器网络，以简单但有效的方式增强在标记和未标记子空间中训练的模型的泛化能力。Deng
    等人 [[190](#bib.bib190)] 关注于在 DAL 过程中结合样本标注和反事实样本构造，以提高模型在分布外的泛化能力。Wang 等人 [[191](#bib.bib191)]
    引入了一种新的训练方式，以提高模型的泛化能力，并展示了在超宽条件下，收敛速度与泛化性能之间的强正相关关系。'
- en: '![Refer to caption](img/c50f0e8e43a75dd106c16fab143624d9.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c50f0e8e43a75dd106c16fab143624d9.png)'
- en: 'Figure 15: TYROGUE can select better samples than baselines.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：TYROGUE 能够选择比基准方法更好的样本。
- en: VI-C Dataset-related Issues
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 数据集相关问题
- en: \adfhalfrightarrowhead
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Outlier Data & Noisy Oracles. DAL methods tend to acquire outliers since models
    always assign high uncertainty scores to outliers. Outliers can damage a model’s
    learning ability and fuel a vicious cycle in which DAL methods continue to select
    them [[43](#bib.bib43)]. Identifying and removing outliers has become an important
    direction in improving DAL performance and robustness. On the other hand, classic
    DAL methods assume that annotators have high labeling accuracy. However, in real-world
    settings, sample difficulty and annotator expertise can significantly affect the
    quality and accuracy of annotation, which may further degrade model performance.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 异常数据与噪声预言者。DAL 方法往往倾向于获取异常值，因为模型总是对异常值分配高的不确定性分数。异常值可能会损害模型的学习能力，并加剧一个恶性循环，使
    DAL 方法不断选择这些异常值 [[43](#bib.bib43)]。识别和去除异常值已成为提高 DAL 性能和鲁棒性的一个重要方向。另一方面，经典的 DAL
    方法假设标注者具有高标注准确性。然而，在实际环境中，样本难度和标注者的专业水平可能会显著影响标注的质量和准确性，这可能进一步降低模型性能。
- en: To remove outliers, Park et al. [[126](#bib.bib126)] propose MQ-Net to adaptively
    find the best balance between purity and informativeness of samples, filtering
    out noisy open-set data. Elenter et al. [[89](#bib.bib89)] introduce a new query
    strategy based on Lagrangian duality to select diverse samples, efficiently removing
    redundant data. Other studies [[14](#bib.bib14)] use knowledge distillation to
    compress useful knowledge into a small model, effectively identifying and removing
    outliers. To make high-quality annotations, AMCC [[81](#bib.bib81)] measures worker
    annotations considering both their commonality and individuality to reduce the
    impact of unreliable workers and improve effectiveness. Zhao et al. [[192](#bib.bib192)]
    actively select samples that are relabeled multiple times through crowd-sourcing
    majority voting. EMMA [[193](#bib.bib193)] relabels samples to remove noisy annotations
    by analyzing the stimulus based on model memory retention and greedy heuristics.
    BALT [[203](#bib.bib203)] improves human expertise during labeling to improve
    relabel quality and significantly improve model performance. Zlabinger [[185](#bib.bib185)]
    trains human annotators on a set of pre-labeled samples to improve the quality
    of annotations. Huang et al. [[134](#bib.bib134)] propose a multi-server, multi-worker
    framework for DAL, where servers and workers cooperate to select diverse samples
    and improve model performance.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 为了去除异常值，Park 等人 [[126](#bib.bib126)] 提出了 MQ-Net 方法，旨在自适应地找到样本纯度和信息量之间的最佳平衡，从而过滤掉噪声开放集数据。Elenter
    等人 [[89](#bib.bib89)] 引入了一种基于拉格朗日对偶的新查询策略，以选择多样化的样本，效率地去除冗余数据。其他研究 [[14](#bib.bib14)]
    使用知识蒸馏将有用知识压缩到一个小模型中，从而有效地识别和去除异常值。为了制作高质量的标注，AMCC [[81](#bib.bib81)] 考虑工人的共性和个性来评估工人标注，从而减少不可靠工人的影响，提高标注效果。Zhao
    等人 [[192](#bib.bib192)] 通过众包多数投票主动选择被多次重新标注的样本。EMMA [[193](#bib.bib193)] 通过分析基于模型记忆保留和贪婪启发式的方法重新标注样本，以去除噪声标注。BALT
    [[203](#bib.bib203)] 在标注过程中提高人工专业性，以提高重新标注质量，并显著提升模型性能。Zlabinger [[185](#bib.bib185)]
    在一组预标注样本上训练人工标注员，以提高标注质量。Huang 等人 [[134](#bib.bib134)] 提出了一个多服务器、多工人的 DAL 框架，其中服务器和工人合作选择多样化的样本，从而提升模型性能。
- en: \adfhalfrightarrowhead
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Data Scarcity & Imbalance. Data scarcity poses two critical challenges. First,
    datasets are difficult to collect and annotate [[204](#bib.bib204)]; Second, DAL
    methods have the common underlying assumption that all classes are equal, while
    some classes have more samples than others (skewed class distribution [[176](#bib.bib176)])
    or some classes may be more difficult to learn than others, leading to sampling
    bias in the acquisition process [[205](#bib.bib205)].
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 数据稀缺与不平衡。数据稀缺带来了两个关键挑战。首先，数据集难以收集和标注 [[204](#bib.bib204)]; 其次，DAL 方法通常假设所有类别是平等的，而实际上某些类别的样本比其他类别更多（类别分布偏斜
    [[176](#bib.bib176)]），或者某些类别可能比其他类别更难学习，导致在获取过程中的采样偏差 [[205](#bib.bib205)]。
- en: '![Refer to caption](img/816d4aff56093cbcce0599b564af2deb.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/816d4aff56093cbcce0599b564af2deb.png)'
- en: 'Figure 16: An example of imbalanced sampling [[195](#bib.bib195)].'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: 不平衡采样的示例 [[195](#bib.bib195)]。'
- en: 'For scarce datasets, Chen et al. [[12](#bib.bib12)] used data augmentation
    to generate diverse samples to expand training data. Other studies used PLMs as
    prior knowledge and fine-tuned them to reduce the required labeled samples [[32](#bib.bib32)].
    For difficult annotations, Gudovskiy et al. [[97](#bib.bib97)] introduce several
    novel self-supervised pseudo-labels estimators to correct acquisition bias by
    minimizing the distribution shift between unlabeled data and weakly labeled validation
    data. To mitigate the classes imbalance, Yu et al. [[176](#bib.bib176)] are the
    first to use cost-sensitive learning. They choose the extreme weighted learning
    machine as the base learner to select samples based on the class imbalance ratio,
    class overlap, and small disjunction. They investigate why DAL can be impacted
    by a skewed instance distribution and improve DAL performance on imbalanced datasets.
    Choi et al. [[194](#bib.bib194)] solve the issue of data imbalance by considering
    the probability of mislabeling a class, the probability of the data given a predicted
    class, and the prior probability of the abundance of a predicted class, during
    querying samples of DAL. Experiments show that they can significantly enhance
    the ability of existing DAL methods to handle unbalanced datasets. As shown in
    Fig. [16](#S6.F16 "Figure 16 ‣ VI-C Dataset-related Issues ‣ VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers"),
    Zhao et al. [[195](#bib.bib195)] propose an alternate query strategy by using
    the medial distribution to find a compromise between importance weighting and
    class-balanced sampling. Experiments show that their model can be easily combined
    with various DAL methods and successfully select balanced samples in imbalanced
    datasets. Hartford et al. [[196](#bib.bib196)] present an exemplar guided DAL
    method that shows strong empirical performance under extremely skewed label distributions
    by using exemplar embedding. Zhang et al. [[197](#bib.bib197)] propose a graph-based
    DAL method that applies a more sophisticated version of uncertainty sampling.
    Their strategy can select more evenly distributed examples for labeling than standard
    uncertainty sampling.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '对于稀缺的数据集，Chen等人[[12](#bib.bib12)] 使用数据增强技术生成多样的样本以扩展训练数据。其他研究则利用PLM作为先验知识，并对其进行微调以减少所需标注样本[[32](#bib.bib32)]。对于困难的标注，Gudovskiy等人[[97](#bib.bib97)]
    引入了几种新颖的自监督伪标签估计器，通过最小化未标记数据与弱标记验证数据之间的分布偏移来修正获取偏差。为了缓解类别不平衡问题，Yu等人[[176](#bib.bib176)]
    首次使用了成本敏感学习。他们选择了极端加权学习机作为基本学习器，根据类别不平衡比率、类别重叠和小的离散性来选择样本。他们研究了为什么DAL可能受到偏斜实例分布的影响，并提高了DAL在不平衡数据集上的性能。Choi等人[[194](#bib.bib194)]
    通过考虑类别误标记的概率、给定预测类别的数据的概率以及预测类别的先验概率来解决数据不平衡问题，从而在DAL样本查询过程中改进。实验表明，他们可以显著增强现有DAL方法处理不平衡数据集的能力。如图[16](#S6.F16
    "Figure 16 ‣ VI-C Dataset-related Issues ‣ VI Challenges & Opportunities of DAL
    ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")所示，Zhao等人[[195](#bib.bib195)]
    提出了一种替代查询策略，通过使用中位分布在重要性加权和类别平衡采样之间找到折中点。实验表明，他们的模型可以轻松与各种DAL方法结合，并成功地在不平衡数据集中选择平衡样本。Hartford等人[[196](#bib.bib196)]
    提出了一种示例指导的DAL方法，该方法通过使用示例嵌入在极度偏斜的标签分布下显示出强大的实证性能。Zhang等人[[197](#bib.bib197)] 提出了一种基于图的DAL方法，该方法应用了更复杂的不确定性采样版本。他们的策略可以比标准不确定性采样选择更均匀分布的标记示例。'
- en: \adfhalfrightarrowhead
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: \adfhalfrightarrowhead
- en: Class Distribution Mismatch. DAL methods assume that the labeled and unlabeled
    data are drawn from the same class distribution, which means that the categories
    of both datasets are identical [[200](#bib.bib200)]. However, in real-world scenarios,
    unlabeled data often come from uncontrolled sources, and a large portion of the
    examples may belong to unknown classes. For example, when crawling images for
    binary image classification using keywords like “dog” and “cat,” over 50% of the
    images in the unlabeled dataset are irrelevant to the task (e.g., “deer,” “horse”).
    Annotating these irrelevant images will lead to a waste of annotation budget as
    they are unnecessary for training the desired classifier. Despite this challenge,
    existing DAL systems tend to select these irrelevant images for annotation, as
    they contain more uncertain knowledge.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 类别分布不匹配。DAL 方法假设标记数据和未标记数据来自相同的类别分布，这意味着两个数据集的类别是相同的 [[200](#bib.bib200)]。然而，在现实世界中，未标记数据通常来自于不受控制的来源，并且大量示例可能属于未知类别。例如，当使用诸如“狗”和“猫”等关键词爬取图像以进行二分类时，未标记数据集中超过
    50% 的图像与任务无关（例如，“鹿”，“马”）。对这些无关图像进行标注将导致标注预算的浪费，因为它们对训练所需的分类器没有帮助。尽管面临这一挑战，现有的
    DAL 系统往往会选择这些无关图像进行标注，因为它们包含更多的不确定知识。
- en: 'To address this issue, As shown in Fig. [17](#S6.F17 "Figure 17 ‣ VI-C Dataset-related
    Issues ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning:
    Recent Advances and New Frontiers") (a), He et al. [[198](#bib.bib198)] propose
    the energy discrepancy to measure the density distribution between the seen and
    unseen classes. Then, they propose an iterative optimization strategy to facilitate
    the teacher-student distillation network to avoid selecting samples from unseen
    classes. Furthermore, Tang et al. [[199](#bib.bib199)] propose a dual DAL framework
    that simultaneously performs model search and data selection. Their framework
    effectively addressed the issue of distribution mismatch and significantly improves
    model performance. In Fig. [17](#S6.F17 "Figure 17 ‣ VI-C Dataset-related Issues
    ‣ VI Challenges & Opportunities of DAL ‣ A Survey on Deep Active Learning: Recent
    Advances and New Frontiers") (b), Ning et al. [[200](#bib.bib200)] introduce a
    detector-classifier DAL framework, where the detector filters unknown classes
    using Gaussian Mixture Models and the classifier selects uncertain in-distribution
    samples for retraining. By actively acquiring purer in-distribution query sets,
    this framework improves the model generalization on class distribution mismatch.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这个问题，如图 [17](#S6.F17 "Figure 17 ‣ VI-C Dataset-related Issues ‣ VI Challenges
    & Opportunities of DAL ‣ A Survey on Deep Active Learning: Recent Advances and
    New Frontiers") (a) 所示，He 等人 [[198](#bib.bib198)] 提出了能量差异来测量已见类和未见类之间的密度分布。然后，他们提出了一种迭代优化策略，以促进教师-学生蒸馏网络避免从未见类中选择样本。此外，Tang
    等人 [[199](#bib.bib199)] 提出了一个双重 DAL 框架，同时进行模型搜索和数据选择。他们的框架有效地解决了分布不匹配的问题，并显著提高了模型性能。在图
    [17](#S6.F17 "Figure 17 ‣ VI-C Dataset-related Issues ‣ VI Challenges & Opportunities
    of DAL ‣ A Survey on Deep Active Learning: Recent Advances and New Frontiers")
    (b) 中，Ning 等人 [[200](#bib.bib200)] 介绍了一个检测器-分类器 DAL 框架，其中检测器使用高斯混合模型过滤未知类别，而分类器选择不确定的分布内样本进行重新训练。通过积极获取更纯粹的分布内查询集，该框架提高了模型在类别分布不匹配上的泛化能力。'
- en: '![Refer to caption](img/c7d3b4df84d29b8c0ce1741cc691290e.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c7d3b4df84d29b8c0ce1741cc691290e.png)'
- en: 'Figure 17: Methods for solving class distribution mismatch.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：解决类别分布不匹配的方法。
- en: VII Conclusion
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: Due to the advantages of DAL, such as high efficiency, good effectiveness, and
    strong robustness, DAL has been deployed in both research and industry projects.
    This article provides a comprehensive survey on DAL, including its collection,
    definition, influential baselines and datasets, taxonomy, applications, challenges,
    and some inspiring prospects. First, we discuss the collection and filtering of
    DAL papers to ensure their high-quality. Second, we give the definition of DAL
    tasks, and present its basic pipeline, influential baselines, and widely used
    datasets. Third, we present our taxonomy for DAL methods from several perspectives
    and discuss their strengths and weaknesses. From them, we obtain some guidelines
    for selecting different query strategies, deep model architectures, and learning
    paradigms to apply for different tasks. In addition, different annotation strategies
    can significantly reduce manual labor while also bringing certain drawbacks. In
    terms of training process, curriculum learning training and Pre+FT can better
    adapt to the current era of large language models. Fourth, we discuss some typical
    applications of DAL. Other than the commonly used and popular DAL methods used
    for CV tasks, we also introduce the carefully designed DAL method for NLP, DM,
    etc. Finally, even though DAL has many benefits, we reckon that they can be refined
    further in terms of pipeline, tasks, and datasets. Specifically, there are many
    problems that DAL is hard to handle, such as inefficient human annotation, difficulty
    in cross-domain transfer, unstable performance, lack of scalability, data imbalance,
    and class distribution mismatch. We share DAL-related resources on Github. We
    hope that this work will be a quick guide for researchers and motivate them to
    solve important problems in the DAL domain.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DAL具有高效率、良好的效果和强韧性等优势，已经在研究和工业项目中得到了广泛应用。本文全面概述了DAL，包括收集、定义、有影响力的基线和数据集、分类方法、应用、挑战和一些具有启发性的前景。首先，我们讨论DAL文章的收集和过滤，以确保其高质量。其次，我们给出DAL任务的定义，并介绍其基本流程、有影响力的基线和广泛使用的数据集。第三，我们从几个角度提出了DAL方法的分类法，并讨论其优缺点。从中，我们得出了一些在不同任务中选择不同查询策略、深度模型架构和学习范式的指导方针。此外，不同的注释策略在显著减少人工劳动的同时也带来了一定的缺点。在训练过程中，课程学习训练和Pre+FT可以更好地适应当前大规模语言模型的时代。第四，我们讨论了DAL的一些典型应用。除了CV任务中常用和热门的DAL方法之外，我们还介绍了特别设计的用于NLP和DM等领域的DAL方法。最后，尽管DAL有很多好处，但我们认为它们在流程、任务和数据集方面还可以进一步完善。具体而言，DAL存在许多难以处理的问题，如低效的人工注释、跨域传递的困难、性能不稳定、缺乏可扩展性、数据不平衡和类分布不匹配。我们在Github上分享了与DAL相关的资源。希望这项工作能成为研究人员的快速指南，并激励他们解决DAL领域的重要问题。
- en: References
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A comprehensive
    survey on graph neural networks,” *IEEE Trans. Neural Networks Learn. Syst.*,
    vol. 32, no. 1, pp. 4–24, 2021.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “图神经网络的综合调查,”
    *IEEE Trans. Neural Networks Learn. Syst.*, vol. 32, no. 1, pp. 4–24, 2021.'
- en: '[2] B. Gu, Z. Zhai, C. Deng, and H. Huang, “Efficient active learning by querying
    discriminative and representative samples and fully exploiting unlabeled data,”
    *IEEE Trans. Neural Networks Learn. Syst.*, vol. 32, no. 9, pp. 4111–4122, 2021.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] B. Gu, Z. Zhai, C. Deng, and H. Huang, “通过查询有辨别性和代表性样本以及充分利用无标记数据来实现高效的活跃学习,”
    *IEEE Trans. Neural Networks Learn. Syst.*, vol. 32, no. 9, pp. 4111–4122, 2021.'
- en: '[3] X. Cao and I. Tsang, “Shattering distribution for active learning,” *IEEE
    Trans. Neural Networks Learn. Syst.*, vol. 33, no. 1, pp. 215–228, 2022.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. Cao and I. Tsang, “基于破碎分布的活跃学习,” *IEEE Trans. Neural Networks Learn.
    Syst.*, vol. 33, no. 1, pp. 215–228, 2022.'
- en: '[4] S. Liu, S. Xue, J. Wu, C. Zhou, J. Yang, Z. Li, and J. Cao, “Online active
    learning for drifting data streams,” *IEEE Trans. Neural Networks Learn. Syst.*,
    vol. 34, no. 1, pp. 186–200, 2023.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Liu, S. Xue, J. Wu, C. Zhou, J. Yang, Z. Li, and J. Cao, “在线学习漂移数据流的活跃学习,”
    *IEEE Trans. Neural Networks Learn. Syst.*, vol. 34, no. 1, pp. 186–200, 2023.'
- en: '[5] T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional
    networks,” in *Proc. of of ICLR*, 2017\. [Online]. Available: [https://openreview.net/forum?id=SJU4ayYgl](https://openreview.net/forum?id=SJU4ayYgl)'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. N. Kipf and M. Welling, “具有图卷积网络的半监督分类,” in *Proc. of of ICLR*, 2017\.
    [在线]. Available: [https://openreview.net/forum?id=SJU4ayYgl](https://openreview.net/forum?id=SJU4ayYgl)'
- en: '[6] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, 1998.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Y. LeCun, L. Bottou, Y. Bengio 和 P. Haffner，“基于梯度的学习在文档识别中的应用”，*IEEE 会议录*，第86卷，第11期，第2278–2324页，1998年。'
- en: '[7] A. Vaswani, N. Shazeer *et al.*, “Attention is all you need,” in *Proc.
    of NeurIPS*, 2017, pp. 1–11.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Vaswani, N. Shazeer *等*，“注意力即全部所需”，见 *NeurIPS 会议录*，2017年，第1–11页。'
- en: '[8] A. Radford, J. W. Kim *et al.*, “Learning transferable visual models from
    natural language supervision,” in *Proc. of ICML*, 2021, pp. 8748–8763.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Radford, J. W. Kim *等*，“从自然语言监督中学习可转移的视觉模型”，见 *ICML 会议录*，2021年，第8748–8763页。'
- en: '[9] OpenAI, “GPT-4 technical report,” *CoRR*, vol. abs/2303.08774, 2023. [Online].
    Available: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] OpenAI，“GPT-4 技术报告”，*CoRR*，第abs/2303.08774卷，2023年。[在线]。可用链接：[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
- en: '[10] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proc. of ICML*, 2009, pp. 41–48.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Bengio, J. Louradour, R. Collobert 和 J. Weston，“课程学习”，见 *ICML 会议录*，2009年，第41–48页。'
- en: '[11] L. Shao, F. Zhu, and X. Li, “Transfer learning for visual categorization:
    A survey,” *IEEE Trans. Neural Networks Learn. Syst.*, vol. 26, no. 5, pp. 1019–1034,
    2015.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] L. Shao, F. Zhu 和 X. Li，“视觉分类的迁移学习：调查”，*IEEE 神经网络学习系统汇刊*，第26卷，第5期，第1019–1034页，2015年。'
- en: '[12] Z. Chen, J. Zhang *et al.*, “When active learning meets implicit semantic
    data augmentation,” in *Proc. of ECCV*, 2022, pp. 56–72.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Z. Chen, J. Zhang *等*，“当主动学习遇到隐式语义数据增强”，见 *ECCV 会议录*，2022年，第56–72页。'
- en: '[13] S. Yang, Z. Xie *et al.*, “Dataset pruning: Reducing training data by
    examining generalization influence,” in *Proc. of ICLR*, 2023. [Online]. Available:
    [https://openreview.net/forum?id=4wZiAXD29TQ](https://openreview.net/forum?id=4wZiAXD29TQ)'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Yang, Z. Xie *等*，“数据集剪枝：通过检查泛化影响来减少训练数据”，见 *ICLR 会议录*，2023。[在线]。可用链接：[https://openreview.net/forum?id=4wZiAXD29TQ](https://openreview.net/forum?id=4wZiAXD29TQ)'
- en: '[14] F. Peng, C. Wang *et al.*, “Active learning for lane detection: A knowledge
    distillation approach,” in *Proc. of ICCV*, 2021, pp. 15 152–15 161.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] F. Peng, C. Wang *等*，“车道检测的主动学习：一种知识蒸馏方法”，见 *ICCV 会议录*，2021年，第15 152–15 161页。'
- en: '[15] Z. Zhang, E. Strubell, and E. H. Hovy, “A survey of active learning for
    natural language processing,” in *Proc. of EMNLP*, 2022, pp. 6166–6190.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Z. Zhang, E. Strubell 和 E. H. Hovy，“自然语言处理中的主动学习调查”，见 *EMNLP 会议录*，2022年，第6166–6190页。'
- en: '[16] D. Tuia, M. Volpi, L. Copa, M. F. Kanevski, and J. Muñoz-Marí, “A survey
    of active learning algorithms for supervised remote sensing image classification,”
    *CoRR*, 2021\. [Online]. Available: [https://arxiv.org/abs/2104.07784](https://arxiv.org/abs/2104.07784)'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] D. Tuia, M. Volpi, L. Copa, M. F. Kanevski 和 J. Muñoz-Marí，“监督遥感图像分类的主动学习算法调查”，*CoRR*，2021年。[在线]。可用链接：[https://arxiv.org/abs/2104.07784](https://arxiv.org/abs/2104.07784)'
- en: '[17] H. Wang, Q. Jin *et al.*, “A comprehensive survey on deep active learning
    and its applications in medical image analysis,” *CoRR*, 2023. [Online]. Available:
    [https://doi.org/10.48550/arXiv.2310.14230](https://doi.org/10.48550/arXiv.2310.14230)'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] H. Wang, Q. Jin *等*，“深度主动学习及其在医学图像分析中的应用的综合调查”，*CoRR*，2023年。[在线]。可用链接：[https://doi.org/10.48550/arXiv.2310.14230](https://doi.org/10.48550/arXiv.2310.14230)'
- en: '[18] H. Hadian and H. Sameti, “Active learning in noisy conditions for spoken
    language understanding,” in *Proc. of ACL*, 2014, pp. 1081–1090.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] H. Hadian 和 H. Sameti，“噪声条件下的主动学习用于口语理解”，见 *ACL 会议录*，2014年，第1081–1090页。'
- en: '[19] S. Budd, E. Robinson *et al.*, “A survey on active learning and human-in-the-loop
    deep learning for medical image analysis,” *Medical Image Anal.*, vol. 71, pp.
    102 062–102 082, 2021.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Budd, E. Robinson *等*，“主动学习和人类参与的深度学习在医学图像分析中的调查”，*医学图像分析*，第71卷，第102 062–102 082页，2021年。'
- en: '[20] R. Takezoe, X. Liu *et al.*, “Deep active learning for computer vision
    past and future,” *CoRR*, 2022\. [Online]. Available: [https://arxiv.org/abs/2211.14819](https://arxiv.org/abs/2211.14819)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] R. Takezoe, X. Liu *等*，“计算机视觉中的深度主动学习：过去与未来”，*CoRR*，2022年。[在线]。可用链接：[https://arxiv.org/abs/2211.14819](https://arxiv.org/abs/2211.14819)'
- en: '[21] X. Zhan, H. Liu *et al.*, “A comparative survey: Benchmarking for pool-based
    active learning,” in *Proc. of IJCAI*, 2021, pp. 4679–4686.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] X. Zhan, H. Liu *等*，“比较调查：池基主动学习的基准”，见 *IJCAI 会议录*，2021年，第4679–4686页。'
- en: '[22] X. Zhan, Q. Wang *et al.*, “A comparative survey of deep active learning,”
    *CoRR*, 2022\. [Online]. Available: [https://arxiv.org/abs/2203.13450](https://arxiv.org/abs/2203.13450)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] X. Zhan, Q. Wang *等*，“深度主动学习的比较调查”，*CoRR*，2022年。[在线]。可用链接：[https://arxiv.org/abs/2203.13450](https://arxiv.org/abs/2203.13450)'
- en: '[23] Y. Fu, X. Zhu, and B. Li, “A survey on instance selection for active learning,”
    *Knowl. Inf. Syst.*, vol. 35, no. 2, pp. 249–283, 2013.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Fu, X. Zhu, 和 B. Li, “关于主动学习中的实例选择的综述,” *Knowl. Inf. Syst.*, 卷 35,
    期 2, 页 249–283, 2013。'
- en: '[24] C. Aggarwal, X. Kong *et al.*, “Active learning: A survey,” in *Data Classification*,
    2014\. [Online]. Available: [https://charuaggarwal.net/active-survey.pdf](https://charuaggarwal.net/active-survey.pdf)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] C. Aggarwal, X. Kong *等*, “主动学习: 综述,” 载于 *Data Classification*, 2014\.
    [在线]. 可用: [https://charuaggarwal.net/active-survey.pdf](https://charuaggarwal.net/active-survey.pdf)'
- en: '[25] A. Tharwat and W. Schenck, “A survey on active learning: State-of-the-art,
    practical challenges and research directions,” *Mathematics*, vol. 11, no. 4,
    2023\. [Online]. Available: [www.mdpi.com/2227-7390/11/4/820](www.mdpi.com/2227-7390/11/4/820)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Tharwat 和 W. Schenck, “关于主动学习的综述: 最新进展、实际挑战与研究方向,” *Mathematics*, 卷
    11, 期 4, 2023\. [在线]. 可用: [www.mdpi.com/2227-7390/11/4/820](www.mdpi.com/2227-7390/11/4/820)'
- en: '[26] P. Ren, Y. Xiao *et al.*, “A survey of deep active learning,” *ACM Comput.
    Surv.*, vol. 54, no. 9, pp. 1–40, 2022.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] P. Ren, Y. Xiao *等*, “深度主动学习的综述,” *ACM Comput. Surv.*, 卷 54, 期 9, 页 1–40,
    2022。'
- en: '[27] P. Liu, L. Wang *et al.*, “A survey on active deep learning: From model
    driven to data driven,” *ACM Comput. Surv.*, vol. 54(10), pp. 1–34, 2022.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] P. Liu, L. Wang *等*, “关于主动深度学习的综述: 从模型驱动到数据驱动,” *ACM Comput. Surv.*, 卷
    54(10), 页 1–34, 2022。'
- en: '[28] D. Cacciarelli and M. Kulahci, “Active learning for data streams: a survey,”
    *Mach. Learn.*, vol. 113, no. 1, pp. 185–239, 2024.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] D. Cacciarelli 和 M. Kulahci, “数据流中的主动学习: 综述,” *Mach. Learn.*, 卷 113, 期
    1, 页 185–239, 2024。'
- en: '[29] K. Margatina, L. Barrault, and N. Aletras, “On the importance of effectively
    adapting pretrained language models for active learning,” in *Proceedings of ACL*,
    2022, pp. 825–836.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K. Margatina, L. Barrault, 和 N. Aletras, “有效适应预训练语言模型以进行主动学习的重要性,” 载于
    *Proceedings of ACL*, 2022, 页 825–836。'
- en: '[30] A. Ayub and C. Fendley, “Few-shot continual active learning by a robot,”
    in *Prof. NeurIPS*, vol. 35, 2022, pp. 30 612–30 624.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] A. Ayub 和 C. Fendley, “机器人进行少量样本持续主动学习,” 载于 *Prof. NeurIPS*, 卷 35, 2022,
    页 30 612–30 624。'
- en: '[31] M. Yuan, H.-T. Lin, and J. Boyd-Graber, “Cold-start active learning through
    self-supervised language modeling,” in *Proc. of EMNLP*, 2020, p. 7935–7948.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. Yuan, H.-T. Lin, 和 J. Boyd-Graber, “通过自监督语言建模进行冷启动主动学习,” 载于 *Proc.
    of EMNLP*, 2020, 页 7935–7948。'
- en: '[32] S. Seo, D. Kim, Y. Ahn, and K. Lee, “Active learning on pre-trained language
    model with task-independent triplet loss,” in *Proc. of AAAI*, 2022, pp. 11 276–11 284.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Seo, D. Kim, Y. Ahn, 和 K. Lee, “在预训练语言模型上进行任务无关三元组损失的主动学习,” 载于 *Proc.
    of AAAI*, 2022, 页 11 276–11 284。'
- en: '[33] K. Margatina, T. Schick, N. Aletras, and J. Dwivedi-Yu, “Active learning
    principles for in-context learning with large language models,” in *Findings of
    EMNLP*, 2023, pp. 5011–5034.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] K. Margatina, T. Schick, N. Aletras, 和 J. Dwivedi-Yu, “大型语言模型中的上下文学习的主动学习原则,”
    载于 *Findings of EMNLP*, 2023, 页 5011–5034。'
- en: '[34] Y. Gal and Z. Ghahramani, “Bayesian convolutional neural networks with
    bernoulli approximate variational inference,” *Proc. of ICLR*, 2016. [Online].
    Available: [https://arxiv.org/abs/1506.02158](https://arxiv.org/abs/1506.02158)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Y. Gal 和 Z. Ghahramani, “具有伯努利近似变分推断的贝叶斯卷积神经网络,” *Proc. of ICLR*, 2016.
    [在线]. 可用: [https://arxiv.org/abs/1506.02158](https://arxiv.org/abs/1506.02158)'
- en: '[35] Y. Gal, R. Islam *et al.*, “Deep bayesian active learning with image data,”
    in *Proc. of ICML*, 2017, pp. 1183–1192.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Gal, R. Islam *等*, “图像数据的深度贝叶斯主动学习,” 载于 *Proc. of ICML*, 2017, 页 1183–1192。'
- en: '[36] K. Wang, D. Zhang *et al.*, “Cost-effective active learning for deep image
    classification,” *IEEE Trans. Circuits Syst. Video Technol.*, vol. 27, no. 12,
    pp. 2591–2600, 2017.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] K. Wang, D. Zhang *等*, “深度图像分类的成本效益主动学习,” *IEEE Trans. Circuits Syst.
    Video Technol.*, 卷 27, 期 12, 页 2591–2600, 2017。'
- en: '[37] L. Balaji, A. Pritzel *et al.*, “Simple and scalable predictive uncertainty
    estimation using deep ensembles,” in *Proc. of NeurIPS*, 2017\. [Online]. Available:
    [https://arxiv.org/abs/1612.01474](https://arxiv.org/abs/1612.01474)'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] L. Balaji, A. Pritzel *等*, “使用深度集成的简单且可扩展的预测不确定性估计,” 载于 *Proc. of NeurIPS*,
    2017\. [在线]. 可用: [https://arxiv.org/abs/1612.01474](https://arxiv.org/abs/1612.01474)'
- en: '[38] M. Fang, Y. Li, and T. Cohn, “Learning how to active learn: A deep reinforcement
    learning approach,” in *Proc. of EMNLP*, 2017, pp. 595–605.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Fang, Y. Li, 和 T. Cohn, “学习如何主动学习: 一种深度强化学习方法,” 载于 *Proc. of EMNLP*,
    2017, 页 595–605。'
- en: '[39] K. Konyushkova, R. Sznitman, and P. Fua, “Learning active learning from
    data,” in *Proc. of NeurIPS*, 2017\. [Online]. Available: [https://arxiv.org/abs/1703.03365](https://arxiv.org/abs/1703.03365)'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. Konyushkova, R. Sznitman, 和 P. Fua, “从数据中学习主动学习,” 载于 *Proc. of NeurIPS*,
    2017\. [在线]. 可用: [https://arxiv.org/abs/1703.03365](https://arxiv.org/abs/1703.03365)'
- en: '[40] J. Zhu and J. Bento, “Generative adversarial active learning,” *CoRR*,
    2017\. [Online]. Available: [https://arxiv.org/abs/1702.07956](https://arxiv.org/abs/1702.07956)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Zhu 和 J. Bento, “生成对抗性主动学习，” *CoRR*，2017年。[在线]. 访问链接: [https://arxiv.org/abs/1702.07956](https://arxiv.org/abs/1702.07956)'
- en: '[41] O. Sener and S. Savarese, “Active learning for convolutional neural networks:
    A core-set approach,” in *Proc. of ICLR*, 2018\. [Online]. Available: [https://openreview.net/forum?id=H1aIuk-RW](https://openreview.net/forum?id=H1aIuk-RW)'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] O. Sener 和 S. Savarese, “卷积神经网络的主动学习：核心集方法，”发表于 *ICLR 会议论文集*，2018年。[在线].
    访问链接: [https://openreview.net/forum?id=H1aIuk-RW](https://openreview.net/forum?id=H1aIuk-RW)'
- en: '[42] M. Ducoffe and F. Precioso, “Adversarial active learning for deep networks:
    a margin based approach,” *CoRR*, 2018\. [Online]. Available: [https://arxiv.org/abs/1802.09841](https://arxiv.org/abs/1802.09841)'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Ducoffe 和 F. Precioso, “深度网络的对抗性主动学习：基于边界的方法，” *CoRR*，2018年。[在线]. 访问链接:
    [https://arxiv.org/abs/1802.09841](https://arxiv.org/abs/1802.09841)'
- en: '[43] K. Wang, L. Lin, X. Yan, Z. Chen, D. Zhang, and L. Zhang, “Cost-effective
    object detection: Active sample mining with switchable selection criteria,” *IEEE
    Trans. Neural Networks Learn. Syst.*, vol. 30, no. 3, pp. 834–850, 2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] K. Wang, L. Lin, X. Yan, Z. Chen, D. Zhang, 和 L. Zhang, “具有可切换选择标准的成本效益对象检测：主动样本挖掘，”
    *IEEE 神经网络与学习系统汇刊*，第30卷，第3期，页834–850，2019年。'
- en: '[44] M. Carbonneau, E. Granger, and G. Gagnon, “Bag-level aggregation for multi-instance
    active learning in instance classification problems,” *IEEE Trans. Neural Networks
    Learn. Syst.*, vol. 30, no. 5, pp. 1441–1451, 2019.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. Carbonneau, E. Granger, 和 G. Gagnon, “多实例分类问题中的包级聚合主动学习，” *IEEE 神经网络与学习系统汇刊*，第30卷，第5期，页1441–1451，2019年。'
- en: '[45] A. Kirsch, J. Amersfoort, and Y. Gal, “Batchbald: Efficient and diverse
    batch acquisition for deep bayesian active learning,” in *Proc. of NeurIPS*, 2019\.
    [Online]. Available: [https://arxiv.org/abs/1906.08158](https://arxiv.org/abs/1906.08158)'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Kirsch, J. Amersfoort, 和 Y. Gal, “Batchbald：高效多样的批量获取用于深度贝叶斯主动学习，”发表于
    *NeurIPS 会议论文集*，2019年。[在线]. 访问链接: [https://arxiv.org/abs/1906.08158](https://arxiv.org/abs/1906.08158)'
- en: '[46] Z. Liu, J. Wang, S. Gong, H. Lu, and D. Tao, “Deep reinforcement active
    learning for human-in-the-loop person re-identification,” in *Proc. of ICCV*,
    2019, pp. 6122–6131.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Z. Liu, J. Wang, S. Gong, H. Lu, 和 D. Tao, “用于人机协同的深度强化主动学习，”发表于 *ICCV
    会议论文集*，2019年，页6122–6131。'
- en: '[47] J. Kasai, K. Qian *et al.*, “Low-resource deep entity resolution with
    transfer and active learning,” in *Proc. of ACL*, 2019, pp. 5851–5861.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Kasai, K. Qian *等人*, “低资源深度实体解析的迁移和主动学习，”发表于 *ACL 会议论文集*，2019年，页5851–5861。'
- en: '[48] T. Tran, T. Do *et al.*, “Bayesian generative active deep learning,” in
    *Proc. of ICML*, 2019, pp. 6295–6304.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] T. Tran, T. Do *等人*, “贝叶斯生成主动深度学习，”发表于 *ICML 会议论文集*，2019年，页6295–6304。'
- en: '[49] S. Sinha, S. Ebrahimi, and T. Darrell, “Variational adversarial active
    learning,” in *Proc. of ICCV*, 2019, pp. 5972–5981.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] S. Sinha, S. Ebrahimi, 和 T. Darrell, “变分对抗性主动学习，”发表于 *ICCV 会议论文集*，2019年，页5972–5981。'
- en: '[50] J.-C. Su, Y.-H. Tsai, K. Sohn, B. Liu *et al.*, “Active adversarial domain
    adaptation,” in *Proc. of WACV*, 2020, pp. 739–748.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J.-C. Su, Y.-H. Tsai, K. Sohn, B. Liu *等人*, “主动对抗性领域适应，”发表于 *WACV 会议论文集*，2020年，页739–748。'
- en: '[51] M. Gao, Z. Zhang *et al.*, “Consistency-based semi-supervised active learning:
    Towards minimizing labeling cost,” in *Proc. of ECCV*, 2020, pp. 510–526.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] M. Gao, Z. Zhang *等人*, “基于一致性的半监督主动学习：旨在最小化标注成本，”发表于 *ECCV 会议论文集*，2020年，页510–526。'
- en: '[52] B. Zhang, L. Li *et al.*, “State-relabeling adversarial active learning,”
    in *Proc. of CVPR*, 2020, pp. 8756–8765.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] B. Zhang, L. Li *等人*, “状态重标记对抗性主动学习，”发表于 *CVPR 会议论文集*，2020年，页8756–8765。'
- en: '[53] L. Dor, A. Halfon *et al.*, “Active learning for bert an empirical study,”
    in *Proc. of EMNLP*, 2020, pp. 7949–7962.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] L. Dor, A. Halfon *等人*, “针对 BERT 的主动学习：一项实证研究，”发表于 *EMNLP 会议论文集*，2020年，页7949–7962。'
- en: '[54] S. Huang, T. Wang *et al.*, “Semi-supervised active learning with temporal
    output discrepancy,” in *Proc. of ICCV*, 2021, pp. 3447–3456.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Huang, T. Wang *等人*, “具有时间输出差异的半监督主动学习，”发表于 *ICCV 会议论文集*，2021年，页3447–3456。'
- en: '[55] G. Citovsky, G. DeSalvo, C. Gentile, L. Karydas, A. Rajagopalan, A. Rostamizadeh,
    and S. Kumar, “Batch active learning at scale,” in *Proc. of NeurIPS*, 2021, pp.
    11 933–11 944.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] G. Citovsky, G. DeSalvo, C. Gentile, L. Karydas, A. Rajagopalan, A. Rostamizadeh,
    和 S. Kumar, “大规模批量主动学习，”发表于 *NeurIPS 会议论文集*，2021年，页11 933–11 944。'
- en: '[56] Y. Kim, K. Song, J. Jang, and I. Moon, “LADA: look-ahead data acquisition
    via augmentation for deep active learning,” in *Proc. of NeurIPS*, 2021, pp. 22 919–22 930.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Kim, K. Song, J. Jang, 和 I. Moon, “LADA：通过增强实现深度主动学习的前瞻数据获取，”发表于 *NeurIPS
    会议论文集*，2021年，页22 919–22 930。'
- en: '[57] K. Kim, D. Park *et al.*, “Task-aware variational adversarial active learning,”
    in *Proc. of CVPR*, 2021, pp. 8166–8175.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Kim, D. Park *等人*, “任务感知的变分对抗主动学习，”发表于 *CVPR会议录*，2021年，页码8166–8175。'
- en: '[58] K. Siddharth, K. Ranjay *et al.*, “Mind your outliers! investigating the
    negative impact of outliers on active learning for visual question answering,”
    in *Proc. of ACL*, 2021, pp. 7265–7281.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] K. Siddharth, K. Ranjay *等人*, “关注离群点！调查离群点对视觉问答主动学习的负面影响，”发表于 *ACL会议录*，2021年，页码7265–7281。'
- en: '[59] Z. Zhue, V. Yadav *et al.*, “Few-shot initializing of active learner via
    meta-learning,” in *Proc. of EMNLP*, 2022, pp. 1117–1133.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Z. Zhue, V. Yadav *等人*, “通过元学习对主动学习者进行少样本初始化，”发表于 *EMNLP会议录*，2022年，页码1117–1133。'
- en: '[60] S. Maekawa, D. Zhang *et al.*, “Low-resource interactive active labeling
    for fine-tuning language models,” in *Proc. of EMNLP*, 2022, pp. 3230–3242.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Maekawa, D. Zhang *等人*, “低资源交互式主动标注用于微调语言模型，”发表于 *EMNLP会议录*，2022年，页码3230–3242。'
- en: '[61] C. Schröder, A. Niekler, and M. Potthast, “Revisiting uncertainty-based
    query strategies for active learning with transformers,” in *Findings of ACL*,
    2022, pp. 2194–2203.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] C. Schröder, A. Niekler 和 M. Potthast, “重新审视基于不确定性的查询策略用于具有变换器的主动学习，”发表于
    *ACL发现*，2022年，页码2194–2203。'
- en: '[62] P. Menard, O. Domingues *et al.*, “Fast active learning for pure exploration
    in reinforcement learning,” in *Proc. of ICML*, 2021, pp. 7599–7608.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] P. Menard, O. Domingues *等人*, “强化学习中的快速主动学习用于纯探索，”发表于 *ICML会议录*，2021年，页码7599–7608。'
- en: '[63] A. Krizhevsky, G. Hinton *et al.*, “Learning multiple layers of features
    from tiny images,” 2009\. [Online]. Available: [https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf](https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] A. Krizhevsky, G. Hinton *等人*, “从小图像中学习多个特征层，”2009年。[在线]。可用： [https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf](https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf)'
- en: '[64] Y. Netzer, T. Wang, A. Coates *et al.*, “Reading digits in natural images
    with unsupervised feature learning,” 2011\. [Online]. Available: [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Netzer, T. Wang, A. Coates *等人*, “通过无监督特征学习阅读自然图像中的数字，”2011年。[在线]。可用：
    [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)'
- en: '[65] J. Deng, W. Dong *et al.*, “Imagenet: A large-scale hierarchical image
    database,” in *Proc. of CVPR*, 2009, pp. 248–255.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] J. Deng, W. Dong *等人*, “Imagenet：大规模层次图像数据库，”发表于 *CVPR会议录*，2009年，页码248–255。'
- en: '[66] T.-Y. Lin, M. Maire *et al.*, “Microsoft coco: Common objects in context,”
    in *Proc. of ECCV*, 2014, pp. 740–755.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] T.-Y. Lin, M. Maire *等人*, “微软COCO：上下文中的常见对象，”发表于 *ECCV会议录*，2014年，页码740–755。'
- en: '[67] M. Cordts, M. Omran *et al.*, “The cityscapes dataset for semantic urban
    scene understanding,” in *Proc. of CVPR*, 2016, pp. 3213–3223.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] M. Cordts, M. Omran *等人*, “城市景观数据集用于语义城市场景理解，”发表于 *CVPR会议录*，2016年，页码3213–3223。'
- en: '[68] L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual models
    from few training examples: An incremental bayesian approach tested on 101 object
    categories,” in *Proc. of CVPR*, 2004, pp. 178–178.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] L. Fei-Fei, R. Fergus 和 P. Perona, “从少量训练样本中学习生成视觉模型：一种在101个对象类别上测试的增量贝叶斯方法，”发表于
    *CVPR会议录*，2004年，页码178–178。'
- en: '[69] R. Socher, A. Perelygin *et al.*, “Recursive deep models for semantic
    compositionality over a sentiment treebank,” in *Proc. of EMNLP*, 2013, pp. 1631–1642.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] R. Socher, A. Perelygin *等人*, “用于情感树库的递归深度模型，”发表于 *EMNLP会议录*，2013年，页码1631–1642。'
- en: '[70] X. Li and D. Roth, “Learning question classifiers,” in *Proc. of COLING*,
    2002, pp. 1–7.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] X. Li 和 D. Roth, “学习问题分类器，”发表于 *COLING会议录*，2002年，页码1–7。'
- en: '[71] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large annotated
    corpus for learning natural language inference,” in *Proc. of EMNLP*, 2015, pp.
    632–642.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] S. R. Bowman, G. Angeli, C. Potts 和 C. D. Manning, “用于学习自然语言推理的大型注释语料库，”发表于
    *EMNLP会议录*，2015年，页码632–642。'
- en: '[72] A. Maas, R. E. Daly *et al.*, “Learning word vectors for sentiment analysis,”
    in *Proc. of ACL*, 2011, pp. 142–150.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Maas, R. E. Daly *等人*, “用于情感分析的词向量学习，”发表于 *ACL会议录*，2011年，页码142–150。'
- en: '[73] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks
    for text classification,” *Proc. of NeurIPS*, 2015\. [Online]. Available: [https://arxiv.org/abs/1509.01626](https://arxiv.org/abs/1509.01626)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] X. Zhang, J. Zhao 和 Y. LeCun, “用于文本分类的字符级卷积网络，” *NeurIPS会议录*，2015年。[在线]。可用：
    [https://arxiv.org/abs/1509.01626](https://arxiv.org/abs/1509.01626)'
- en: '[74] P. Sen, G. Namata *et al.*, “Collective classification in network data,”
    *AI magazine*, vol. 29, no. 3, pp. 93–93, 2008.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] P. Sen, G. Namata *等人*, “网络数据中的集体分类，” *AI杂志*，第29卷，第3期，页码93–93，2008年。'
- en: '[75] S. Abu-El-Haija, N. Kothari *et al.*, “Youtube-8m: A large-scale video
    classification benchmark,” *arXiv*, 2016\. [Online]. Available: [https://arxiv.org/abs/1609.08675](https://arxiv.org/abs/1609.08675)'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. Abu-El-Haija, N. Kothari *等*，“Youtube-8m：大规模视频分类基准，” *arXiv*，2016 年。[在线].
    可用： [https://arxiv.org/abs/1609.08675](https://arxiv.org/abs/1609.08675)'
- en: '[76] A. E. Johnson, T. J. Pollard *et al.*, “Mimic-iii, a freely accessible
    critical care database,” *Scientific data*, vol. 3, no. 1, pp. 1–9, 2016.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] A. E. Johnson, T. J. Pollard *等*，“Mimic-iii，一个免费访问的重症监护数据库，” *Scientific
    data*，第 3 卷，第 1 期，第 1–9 页，2016 年。'
- en: '[77] M. Wiechmann, S. Yimam *et al.*, “Activeanno: General-purpose document-level
    annotation tool with active learning integration,” in *Proc. of NAACL*, 2021,
    pp. 99–105.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] M. Wiechmann, S. Yimam *等*，“Activeanno：带有主动学习集成的通用文档级注释工具，”在 *Proc. of
    NAACL*，2021 年，第 99–105 页。'
- en: '[78] T. Wu, Y. Liu *et al.*, “Redal: Region-based and diversity-aware active
    learning for point cloud semantic segmentation,” in *Proc. of ICCV*, 2021, pp.
    15 510–15 519.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. Wu, Y. Liu *等*，“Redal：基于区域和多样性意识的点云语义分割主动学习，”在 *Proc. of ICCV*，2021
    年，第 15 510–15 519 页。'
- en: '[79] S. Kothawade, S. Ghosh *et al.*, “Targeted active learning for object
    detection with rare classes and slices using submodular mutual information,” in
    *Proc. of ECCV*, 2022, pp. 1–16.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. Kothawade, S. Ghosh *等*，“针对具有稀有类和切片的目标检测的有针对性的主动学习，使用子模信息互信息，”在 *Proc.
    of ECCV*，2022 年，第 1–16 页。'
- en: '[80] B. Xie, L. Yuan *et al.*, “Towards fewer annotations: Active learning
    via region impurity and prediction uncertainty for domain adaptive semantic segmentation,”
    in *Proc. of CVPR*, 2022, pp. 8068–8078.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] B. Xie, L. Yuan *等*，“减少注释量：通过区域杂质和预测不确定性进行的领域自适应语义分割主动学习，”在 *Proc. of
    CVPR*，2022 年，第 8068–8078 页。'
- en: '[81] G. Yu, J. Tu, J. Wang, C. Domeniconi, and X. Zhang, “Active multilabel
    crowd consensus,” *IEEE Trans. Neural Networks Learn. Syst.*, vol. 32, no. 4,
    pp. 1448–1459, 2021.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] G. Yu, J. Tu, J. Wang, C. Domeniconi 和 X. Zhang，“主动多标签人群共识，” *IEEE Trans.
    Neural Networks Learn. Syst.*，第 32 卷，第 4 期，第 1448–1459 页，2021 年。'
- en: '[82] C. Schröder, K. Bürgl *et al.*, “Supporting land reuse of former open
    pit mining sites using text classification and active learning,” in *Proc. of
    ACL*, 2021, pp. 4141–4152.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C. Schröder, K. Bürgl *等*，“使用文本分类和主动学习支持前开放坑采矿场的土地再利用，”在 *Proc. of ACL*，2021
    年，第 4141–4152 页。'
- en: '[83] Y. Yan, S. Huang *et al.*, “Active learning with query generation for
    cost-effective text classification,” in *Proc. of AAAI*, 2020, pp. 6583–6590.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Yan, S. Huang *等*，“具有查询生成的主动学习用于高效文本分类，”在 *Proc. of AAAI*，2020 年，第
    6583–6590 页。'
- en: '[84] B. Zhou, X. Cai *et al.*, “MTAAL: multi-task adversarial active learning
    for medical named entity recognition and normalization,” in *Proc. of AAAI*, 2021,
    pp. 14 586–14 593.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] B. Zhou, X. Cai *等*，“MTAAL：用于医学命名实体识别和规范化的多任务对抗主动学习，”在 *Proc. of AAAI*，2021
    年，第 14 586–14 593 页。'
- en: '[85] Y. Yang and M. Loog, “Single shot active learning using pseudo annotators,”
    *Pattern Recognit.*, vol. 89, pp. 22–31, 2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Yang 和 M. Loog，“使用伪标注器的单次主动学习，” *Pattern Recognit.*，第 89 卷，第 22–31
    页，2019 年。'
- en: '[86] J. Gong, Z. Fan *et al.*, “Meta agent teaming active learning for pose
    estimation,” in *Proc. of CVPR*, 2022, pp. 11 069–11 079.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Gong, Z. Fan *等*，“用于姿势估计的元代理团队主动学习，”在 *Proc. of CVPR*，2022 年，第 11 069–11 079
    页。'
- en: '[87] B. Xie, L. Yuan *et al.*, “Active learning for domain adaptation: An energy-based
    approach,” in *Proc. of AAAI*, 2022, pp. 8708–8716.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. Xie, L. Yuan *等*，“领域适应的主动学习：一种基于能量的方法，”在 *Proc. of AAAI*，2022 年，第 8708–8716
    页。'
- en: '[88] D. Wang and Y. Shang, “A new active labeling method for deep learning,”
    in *Proc. of IJCNN*, 2014, pp. 112–119.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] D. Wang 和 Y. Shang，“深度学习的新主动标记方法，”在 *Proc. of IJCNN*，2014 年，第 112–119
    页。'
- en: '[89] J. Elenter, N. Naderializadeh, and A. Ribeiro, “A lagrangian duality approach
    to active learning,” in *Proc. of NeurIPS*, 2022, pp. 37 575–37 589.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Elenter, N. Naderializadeh 和 A. Ribeiro，“一种拉格朗日对偶方法用于主动学习，”在 *Proc.
    of NeurIPS*，2022 年，第 37 575–37 589 页。'
- en: '[90] W. Li, G. Dasarathy, K. N. Ramamurthy, and V. Berisha, “Finding the homology
    of decision boundaries with active learning,” in *Proc. of NeurIPS*, 2020, pp.
    8355–8365.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] W. Li, G. Dasarathy, K. N. Ramamurthy 和 V. Berisha，“使用主动学习找到决策边界的同源性，”在
    *Proc. of NeurIPS*，2020 年，第 8355–8365 页。'
- en: '[91] D. Roth, K. Small *et al.*, “Margin-based active learning for structured
    output spaces,” in *Proc. of ECML*, 2006, pp. 413–424.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] D. Roth, K. Small *等*，“针对结构化输出空间的基于边际的主动学习，”在 *Proc. of ECML*，2006 年，第
    413–424 页。'
- en: '[92] K. Brantley, H. D. III, and A. Sharaf, “Active imitation learning with
    noisy guidance,” in *Proc. of ACL*, 2020, pp. 2093–2105.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] K. Brantley, H. D. III 和 A. Sharaf，“带噪声指导的主动模仿学习，”在 *Proc. of ACL*，2020
    年，第 2093–2105 页。'
- en: '[93] S. Yan, K. Chaudhuri, and T. Javidi, “The label complexity of active learning
    from observational data,” in *Proc. of NeurIPS*, 2019, pp. 1808–1817.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Yan, K. Chaudhuri, 和 T. Javidi, “从观察数据中主动学习的标签复杂度，” 收录于 *Proc. of NeurIPS*,
    2019年，页码 1808–1817。'
- en: '[94] Y. Yang and M. Loog, “To actively initialize active learning,” *Pattern
    Recognit.*, vol. 131, p. 108836, 2022.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Yang 和 M. Loog, “主动初始化主动学习，” *Pattern Recognit.*, 第131卷，页码 108836,
    2022年。'
- en: '[95] Y. Kim and B. Shin, “In defense of core-set: A density-aware core-set
    selection for active learning,” in *Proc. of SIGKDD*, 2022, p. 804–812.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Kim 和 B. Shin, “为核心集辩护：一种面向主动学习的密度感知核心集选择，” 收录于 *Proc. of SIGKDD*,
    2022年，页码 804–812。'
- en: '[96] C. Coleman, E. Chou *et al.*, “Similarity search for efficient active
    learning and search of rare concepts,” in *Proc. of AAAI*, 2022, pp. 6402–6410.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Coleman, E. Chou *等*, “高效主动学习和稀有概念搜索的相似性搜索，” 收录于 *Proc. of AAAI*, 2022年，页码
    6402–6410。'
- en: '[97] D. A. Gudovskiy, A. Hodgkinson, T. Yamaguchi, and S. Tsukizawa, “Deep
    active learning for biased datasets via fisher kernel self-supervision,” in *Proc.
    of CVPR*, 2020, pp. 9041–9049.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] D. A. Gudovskiy, A. Hodgkinson, T. Yamaguchi, 和 S. Tsukizawa, “通过Fisher核自监督实现针对偏差数据集的深度主动学习，”
    收录于 *Proc. of CVPR*, 2020年，页码 9041–9049。'
- en: '[98] M. Hasan, S. Paul, A. I. Mourikis, and A. K. Roy-Chowdhury, “Context-aware
    query selection for active learning in event recognition,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 42, no. 3, pp. 554–567, 2020.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Hasan, S. Paul, A. I. Mourikis, 和 A. K. Roy-Chowdhury, “面向事件识别的上下文感知查询选择，”
    *IEEE Trans. Pattern Anal. Mach. Intell.*, 第42卷，第3期，页码 554–567, 2020年。'
- en: '[99] S. Chakraborty, V. N. Balasubramanian, Q. Sun, S. Panchanathan, and J. Ye,
    “Active batch selection via convex relaxations with guaranteed solution bounds,”
    *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 37, no. 10, pp. 1945–1958, 2015.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] S. Chakraborty, V. N. Balasubramanian, Q. Sun, S. Panchanathan, 和 J. Ye,
    “通过凸放松的主动批量选择与保证解的界限，” *IEEE Trans. Pattern Anal. Mach. Intell.*, 第37卷，第10期，页码
    1945–1958, 2015年。'
- en: '[100] Q. Jin, M. Yuan *et al.*, “One-shot active learning for image segmentation
    via contrastive learning and diversity-based sampling,” *Knowl. Based Syst.*,
    vol. 241, p. 108278, 2022.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Q. Jin, M. Yuan *等*, “通过对比学习和基于多样性的采样实现图像分割的一次性主动学习，” *Knowl. Based Syst.*,
    第241卷，页码 108278, 2022年。'
- en: '[101] C. Li, H. Ma, Z. Kang, Y. Yuan, X. Zhang, and G. Wang, “On deep unsupervised
    active learning,” in *Proc. of IJCAI*, 2020, pp. 2626–2632.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C. Li, H. Ma, Z. Kang, Y. Yuan, X. Zhang, 和 G. Wang, “关于深度无监督主动学习，” 收录于
    *Proc. of IJCAI*, 2020年，页码 2626–2632。'
- en: '[102] A. Parvaneh, E. Abbasnejad, D. Teney, R. Haffari, A. van den Hengel,
    and J. Q. Shi, “Active learning by feature mixing,” in *Proc. of CVPR*, 2022,
    pp. 12 227–12 236.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. Parvaneh, E. Abbasnejad, D. Teney, R. Haffari, A. van den Hengel,
    和 J. Q. Shi, “通过特征混合实现主动学习，” 收录于 *Proc. of CVPR*, 2022年，页码 12 227–12 236。'
- en: '[103] S. Li, J. M. Phillips, X. Yu, R. M. Kirby, and S. Zhe, “Batch multi-fidelity
    active learning with budget constraints,” in *Proc. of NeurIPS*, 2022, pp. 995–1007.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] S. Li, J. M. Phillips, X. Yu, R. M. Kirby, 和 S. Zhe, “在预算约束下的批量多保真度主动学习，”
    收录于 *Proc. of NeurIPS*, 2022年，页码 995–1007。'
- en: '[104] Y. Zhao, Z. Shi, J. Zhang, D. Chen, and L. Gu, “A novel active learning
    framework for classification: Using weighted rank aggregation to achieve multiple
    query criteria,” *Pattern Recognit.*, vol. 93, pp. 581–602, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Zhao, Z. Shi, J. Zhang, D. Chen, 和 L. Gu, “一种用于分类的新型主动学习框架：使用加权排名聚合实现多重查询标准，”
    *Pattern Recognit.*, 第93卷，页码 581–602, 2019年。'
- en: '[105] T. Wang, X. Li *et al.*, “Boosting active learning via improving test
    performance,” in *Proc. of AAAI*, 2022, pp. 8566–8574.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] T. Wang, X. Li *等*, “通过提高测试性能来提升主动学习，” 收录于 *Proc. of AAAI*, 2022年，页码
    8566–8574。'
- en: '[106] M. Thiessen and T. Gärtner, “Active learning of convex halfspaces on
    graphs,” in *Proc. of NeurIPS*, 2021, pp. 23 413–23 425.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] M. Thiessen 和 T. Gärtner, “图上的凸半空间的主动学习，” 收录于 *Proc. of NeurIPS*, 2021年，页码
    23 413–23 425。'
- en: '[107] M. A. Mohamadi, W. Bae, and D. J. Sutherland, “Making look-ahead active
    learning strategies feasible with neural tangent kernels,” in *Prof. NeurIPS*,
    vol. 35, 2022, pp. 12 542–12 553.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M. A. Mohamadi, W. Bae, 和 D. J. Sutherland, “利用神经切线核实现前瞻性主动学习策略的可行性，”
    收录于 *Prof. NeurIPS*, 第35卷，2022年，页码 12 542–12 553。'
- en: '[108] D. Yoo, I. Kweon *et al.*, “Learning loss for active learning,” in *Proc.
    of CVPR*, 2019, pp. 93–102.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] D. Yoo, I. Kweon *等*, “主动学习的损失学习，” 收录于 *Proc. of CVPR*, 2019年，页码 93–102。'
- en: '[109] G. Zhao, E. R. Dougherty *et al.*, “Efficient active learning for gaussian
    process classification by error reduction,” in *Proc. of NeurIPS*, 2021, pp. 9734–9746.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] G. Zhao, E. R. Dougherty *等*, “通过误差减少实现高效的高斯过程分类主动学习，” 收录于 *Proc. of
    NeurIPS*, 2021年，页码 9734–9746。'
- en: '[110] T. Vu, M. Liu *et al.*, “Learning how to active learn by dreaming,” in
    *Proc. of ACL*, 2019, p. 4091–4101.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] T. Vu, M. Liu *等*, “通过梦想学习如何主动学习，” 收录于 *Proc. of ACL*, 2019年，页码 4091–4101。'
- en: '[111] L. Wertz, J. Bogojeska, K. Mirylenka, and J. Kuhn, “Reinforced active
    learning for low-resource, domain-specific, multi-label text classification,”
    in *Findings of ACL*, 2023, pp. 10 959–10 977.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] L. Wertz, J. Bogojeska, K. Mirylenka 和 J. Kuhn，“针对低资源、特定领域、多标签文本分类的强化主动学习，”见于
    *ACL 会议记录*，2023年，第10 959–10 977页。'
- en: '[112] W. Beluch, T. Genewein *et al.*, “The power of ensembles for active learning
    in image classification,” in *Proc. of CVPR*, 2018, pp. 9368–9377.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] W. Beluch, T. Genewein *等*，“集成的力量在图像分类中的主动学习，”见于 *CVPR 会议录*，2018年，第9368–9377页。'
- en: '[113] A. Jesson, P. Tigas *et al.*, “Causal-bald: Deep bayesian active learning
    of outcomes to infer treatment-effects from observational data,” in *Proc. of
    NeurIPS*, 2021, pp. 30 465–30 478.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Jesson, P. Tigas *等*，“Causal-bald: 从观察数据推断处理效果的深度贝叶斯主动学习，”见于 *NeurIPS
    会议录*，2021年，第30 465–30 478页。'
- en: '[114] P. Donmez, J. Carbonell *et al.*, “Dual strategy active learning,” in
    *Proc. of ECML*, 2007, pp. 116–127.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] P. Donmez, J. Carbonell *等*，“双策略主动学习，”见于 *ECML 会议录*，2007年，第116–127页。'
- en: '[115] Y. Geifman and R. El-Yaniv, “Deep active learning with a neural architecture
    search,” in *Proc. of NeurIPS*, 2019, pp. 5974–5984.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Y. Geifman 和 R. El-Yaniv，“具有神经架构搜索的深度主动学习，”见于 *NeurIPS 会议录*，2019年，第5974–5984页。'
- en: '[116] P. Izmailov, S. Vikram, M. D. Hoffman, and A. G. Wilson, “What are bayesian
    neural network posteriors really like?” in *Proc. of ICML*, 2021, pp. 4629–4640.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] P. Izmailov, S. Vikram, M. D. Hoffman 和 A. G. Wilson，“贝叶斯神经网络后验究竟是什么样的？”见于
    *ICML 会议录*，2021年，第4629–4640页。'
- en: '[117] D. E. Rumelhart *et al.*, “Learning representations by back-propagating
    errors,” *Nature*, vol. 323, no. 6088, pp. 533–536, 1986.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] D. E. Rumelhart *等*，“通过反向传播误差学习表征，” *自然*，第323卷，第6088期，第533–536页，1986年。'
- en: '[118] X. Zeng, S. Garg *et al.*, “Empirical evaluation of active learning techniques
    for neural MT,” in *Proc. of EMNLP*, 2019, pp. 84–93.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] X. Zeng, S. Garg *等*，“对神经机器翻译主动学习技术的实证评估，”见于 *EMNLP 会议录*，2019年，第84–93页。'
- en: '[119] A. Amirinezhad, S. Salehkaleybar, and M. Hashemi, “Active learning of
    causal structures with deep reinforcement learning,” *Neural Networks*, vol. 154,
    pp. 22–30, 2022.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. Amirinezhad, S. Salehkaleybar 和 M. Hashemi，“通过深度强化学习的因果结构主动学习，” *神经网络*，第154卷，第22–30页，2022年。'
- en: '[120] S. Hu, Z. Xiong, M. Qu, X. Yuan, M. Côté, Z. Liu, and J. Tang, “Graph
    policy network for transferable active learning on graphs,” in *Proc. of NeurIPS*,
    2020.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] S. Hu, Z. Xiong, M. Qu, X. Yuan, M. Côté, Z. Liu 和 J. Tang，“用于图的可迁移主动学习的图策略网络，”见于
    *NeurIPS 会议录*，2020年。'
- en: '[121] Y. Ren, B. Wang, J. Zhang, and Y. Chang, “Adversarial active learning
    based heterogeneous graph neural network for fake news detection,” in *Proc. of
    ICDM*.   IEEE, 2020, pp. 452–461.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y. Ren, B. Wang, J. Zhang 和 Y. Chang，“基于对抗性主动学习的异质图神经网络用于虚假新闻检测，”见于 *ICDM
    会议录*，IEEE，2020年，第452–461页。'
- en: '[122] Y. Li, J. Yin, and L. Chen, “SEAL: semisupervised adversarial active
    learning on attributed graphs,” *IEEE Trans. Neural Networks Learn. Syst.*, vol. 32,
    no. 7, pp. 3136–3147, 2021.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Y. Li, J. Yin 和 L. Chen，“SEAL: 属性图上的半监督对抗性主动学习，” *IEEE 神经网络与学习系统期刊*，第32卷，第7期，第3136–3147页，2021年。'
- en: '[123] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in *Proc.
    of ICLR*, 2014\. [Online]. Available: [https://openreview.net/forum?id=33X9fd2-9FyZd](https://openreview.net/forum?id=33X9fd2-9FyZd)'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] D. P. Kingma 和 M. Welling，“自动编码变分贝叶斯，”见于 *ICLR 会议录*，2014年。[在线] 可用: [https://openreview.net/forum?id=33X9fd2-9FyZd](https://openreview.net/forum?id=33X9fd2-9FyZd)'
- en: '[124] S. Albelwi, “Survey on self-supervised learning: Auxiliary pretext tasks
    and contrastive learning methods in imaging,” *Entropy*, vol. 24, no. 4, p. 551,
    2022.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] S. Albelwi，“关于自监督学习的调查: 成像中的辅助前置任务和对比学习方法，” *熵*，第24卷，第4期，第551页，2022年。'
- en: '[125] P. Du, H. Chen, S. Zhao, S. Chai, H. Chen, and C. Li, “Contrastive active
    learning under class distribution mismatch,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 45, no. 4, pp. 4260–4273, 2023.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] P. Du, H. Chen, S. Zhao, S. Chai, H. Chen 和 C. Li，“在类别分布不匹配下的对比主动学习，”
    *IEEE 计算机学会模式分析与机器智能期刊*，第45卷，第4期，第4260–4273页，2023年。'
- en: '[126] D. Park, Y. Shin, J. Bang, Y. Lee, H. Song, and J. Lee, “Meta-query-net:
    Resolving purity-informativeness dilemma in open-set active learning,” in *Proc.
    of NeurIPS*, 2022, pp. 31 416–31 429.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] D. Park, Y. Shin, J. Bang, Y. Lee, H. Song 和 J. Lee，“Meta-query-net:
    解决开放集主动学习中的纯度-信息量困境，”见于 *NeurIPS 会议录*，2022年，第31 416–31 429页。'
- en: '[127] J. Shao, Q. Wang, and F. Liu, “Learning to sample: An active learning
    framework,” in *Proc. of ICDM*, 2019, pp. 538–547.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Shao, Q. Wang 和 F. Liu，“学习采样: 一个主动学习框架，”见于 *ICDM 会议录*，2019年，第538–547页。'
- en: '[128] L. Jiang, D. Meng, Q. Zhao, S. Shan, and A. G. Hauptmann, “Self-paced
    curriculum learning,” in *Proc. of AAAI*, 2015, pp. 2694–2700.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] L. Jiang, D. Meng, Q. Zhao, S. Shan 和 A. G. Hauptmann，“自定步伐课程学习，”见于 *AAAI
    会议录*，2015年，第2694–2700页。'
- en: '[129] L. Lin, K. Wang, D. Meng, W. Zuo, and L. Zhang, “Active self-paced learning
    for cost-effective and progressive face identification,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 40, no. 1, pp. 7–19, 2018.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] L. Lin, K. Wang, D. Meng, W. Zuo, 和 L. Zhang，“用于成本效益和渐进式面部识别的主动自适应学习，”
    *IEEE Trans. Pattern Anal. Mach. Intell.*, 第40卷，第1期，第7–19页，2018年。'
- en: '[130] M. Mundt, Y. Hong, I. Pliushch, and V. Ramesh, “Forgotten lessons and
    the bridge to active and open world learning,” *Neural Networks*, vol. 160, pp.
    306–336, 2023.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] M. Mundt, Y. Hong, I. Pliushch, 和 V. Ramesh，“被遗忘的教训和通向主动学习及开放世界学习的桥梁，”
    *Neural Networks*, 第160卷，第306–336页，2023年。'
- en: '[131] C. Löffler and C. Mutschler, “IALE: imitating active learner ensembles,”
    *J. Mach. Learn. Res.*, vol. 23, pp. 107:1–107:29, 2022.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] C. Löffler 和 C. Mutschler，“IALE：模仿主动学习者集成，” *J. Mach. Learn. Res.*, 第23卷，第107:1–107:29页，2022年。'
- en: '[132] K. Bullard, Y. Schroecker, and S. Chernova, “Active learning within constrained
    environments through imitation of an expert questioner,” in *Proc. of IJCAI*,
    2019, pp. 2045–2052.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] K. Bullard, Y. Schroecker, 和 S. Chernova，“通过模仿专家提问者在受限环境下进行主动学习，” 见 *IJCAI
    会议录*，2019年，第2045–2052页。'
- en: '[133] F. Ikhwantri, S. Louvan *et al.*, “Multi-task active learning for neural
    semantic role labeling on low resource conversational corpus,” in *Proc. of the
    Workshop on DLA for Low-Resource*, 2018, pp. 43–50.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] F. Ikhwantri, S. Louvan *等*，“用于低资源对话语料库的神经语义角色标注的多任务主动学习，” 见 *低资源 DLA
    研讨会*，2018年，第43–50页。'
- en: '[134] S. Huang, C. Zong *et al.*, “Asynchronous active learning with distributed
    label querying,” in *Proc. of IJCAI*, 2021, pp. 2570–2576.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] S. Huang, C. Zong *等*，“带有分布式标签查询的异步主动学习，” 见 *IJCAI 会议录*，2021年，第2570–2576页。'
- en: '[135] Y. Tang and S. Huang, “Self-paced active learning: Query the right thing
    at the right time,” in *Proc. of AAAI*, 2019, pp. 5117–5124.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Tang 和 S. Huang，“自适应主动学习：在正确的时间询问正确的事情，” 见 *AAAI 会议录*，2019年，第5117–5124页。'
- en: '[136] W. Tan, L. Du, and W. L. Buntine, “Diversity enhanced active learning
    with strictly proper scoring rules,” in *Proc. of NeurIPS*, 2021, pp. 10 906–10 918.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] W. Tan, L. Du, 和 W. L. Buntine，“使用严格的评分规则增强多样性的主动学习，” 见 *NeurIPS 会议录*，2021年，第10 906–10 918页。'
- en: '[137] F. Jelenic, J. Jukic, N. Drobac, and J. Snajder, “On dataset transferability
    in active learning for transformers,” in *Findings of ACL*, 2023, pp. 2282–2295.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] F. Jelenic, J. Jukic, N. Drobac, 和 J. Snajder，“关于变换器的主动学习中的数据集可转移性，”
    见 *ACL 发现*，2023年，第2282–2295页。'
- en: '[138] A. Gidiotis and G. Tsoumakas, “Should we trust this summary? bayesian
    abstractive summarization to the rescue,” in *Proc. of ACL*, 2022, pp. 4119–4131.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Gidiotis 和 G. Tsoumakas，“我们应该信任这个摘要吗？贝叶斯抽象摘要化来拯救，” 见 *ACL 会议录*，2022年，第4119–4131页。'
- en: '[139] A. Tsvigun, I. Lysenko *et al.*, “Active learning for abstractive text
    summarization,” in *Proc. of EMNLP*, 2022, pp. 5128–5152.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] A. Tsvigun, I. Lysenko *等*，“用于抽象文本摘要的主动学习，” 见 *EMNLP 会议录*，2022年，第5128–5152页。'
- en: '[140] A. Padmakumar and R. J. Mooney, “Dialog policy learning for joint clarification
    and active learning queries,” in *Proc. of AAAI*, 2021, pp. 13 604–13 612.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] A. Padmakumar 和 R. J. Mooney，“用于联合澄清和主动学习查询的对话策略学习，” 见 *AAAI 会议录*，2021年，第13 604–13 612页。'
- en: '[141] P. Radmard, Y. Fathullah, and A. Lipani, “Subsequence based deep active
    learning for named entity recognition,” in *Proc. of ACL*, 2021, pp. 4310–4321.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] P. Radmard, Y. Fathullah, 和 A. Lipani，“基于子序列的深度主动学习用于命名实体识别，” 见 *ACL
    会议录*，2021年，第4310–4321页。'
- en: '[142] X. Hua and L. Wang, “Efficient argument structure extraction with transfer
    learning and active learning,” in *Findings of ACL*, 2022, pp. 423–437.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] X. Hua 和 L. Wang，“通过迁移学习和主动学习高效提取论证结构，” 见 *ACL 发现*，2022年，第423–437页。'
- en: '[143] L. Duong, H. Afshar, D. Estival, G. Pink, P. R. Cohen, and M. Johnson,
    “Active learning for deep semantic parsing,” in *Proc. of ACL*, 2018, pp. 43–48.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] L. Duong, H. Afshar, D. Estival, G. Pink, P. R. Cohen, 和 M. Johnson，“深度语义解析的主动学习，”
    见 *ACL 会议录*，2018年，第43–48页。'
- en: '[144] Z. Li, L. Qu, P. R. Cohen, R. Tumuluri, and G. Haffari, “The best of
    both worlds: Combining human and machine translations for multilingual semantic
    parsing with active learning,” in *Proc. of ACL*, 2023, pp. 9511–9528.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Z. Li, L. Qu, P. R. Cohen, R. Tumuluri, 和 G. Haffari，“两全其美：结合人类和机器翻译进行多语言语义解析的主动学习，”
    见 *ACL 会议录*，2023年，第9511–9528页。'
- en: '[145] B. Zhang, L. Li, L. Su, S. Wang, J. Deng, Z. Zha, and Q. Huang, “Structural
    semantic adversarial active learning for image captioning,” in *Proc. of MM*.   ACM,
    2020, pp. 1112–1121.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] B. Zhang, L. Li, L. Su, S. Wang, J. Deng, Z. Zha, 和 Q. Huang，“用于图像描述的结构语义对抗主动学习，”
    见 *MM 会议录*。ACM, 2020年，第1112–1121页。'
- en: '[146] M. Cheikh and M. Zrigui, “Active learning based framework for image captioning
    corpus creation,” in *Proc. of LION*, vol. 12096, 2020, pp. 128–142.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] M. Cheikh 和 M. Zrigui, “用于图像描述语料库创建的主动学习框架,” 在 *Proc. of LION*, 卷 12096,
    2020, 页 128–142.'
- en: '[147] K. Konyushkova, R. Sznitman, and P. Fua, “Geometry in active learning
    for binary and multi-class image segmentation,” *Comput. Vis. Image Underst.*,
    vol. 182, pp. 1–16, 2019.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] K. Konyushkova, R. Sznitman, 和 P. Fua, “用于二类和多类图像分割的主动学习中的几何,” *Comput.
    Vis. Image Underst.*, 卷 182, 页 1–16, 2019.'
- en: '[148] Y. Qiao, J. Zhu *et al.*, “CPRAL: collaborative panoptic-regional active
    learning for semantic segmentation,” in *Proc. of AAAI*, 2022, pp. 2108–2116.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Y. Qiao, J. Zhu *等*, “CPRAL: 协作全景区域主动学习用于语义分割,” 在 *Proc. of AAAI*, 2022,
    页 2108–2116.'
- en: '[149] J. Wu, J. Chen, and D. Huang, “Entropy-based active learning for object
    detection with progressive diversity constraint,” in *Proc. of CVPR*, 2022, pp.
    9387–9396.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] J. Wu, J. Chen, 和 D. Huang, “基于熵的目标检测主动学习与渐进多样性约束,” 在 *Proc. of CVPR*,
    2022, 页 9387–9396.'
- en: '[150] T. Yuan, F. Wan, M. Fu, J. Liu, S. Xu, X. Ji, and Q. Ye, “Multiple instance
    active learning for object detection,” in *Proc. of CVPR*, 2021, pp. 5330–5339.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] T. Yuan, F. Wan, M. Fu, J. Liu, S. Xu, X. Ji, 和 Q. Ye, “用于目标检测的多实例主动学习,”
    在 *Proc. of CVPR*, 2021, 页 5330–5339.'
- en: '[151] R. Caramalau, B. Bhattarai, and T. Kim, “Active learning for bayesian
    3d hand pose estimation,” in *Proc. of WACV*, 2021, pp. 3418–3427.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] R. Caramalau, B. Bhattarai, 和 T. Kim, “用于贝叶斯三维手势估计的主动学习,” 在 *Proc. of
    WACV*, 2021, 页 3418–3427.'
- en: '[152] M. Shukla and S. Ahmed, “A mathematical analysis of learning loss for
    active learning in regression,” in *Proc. of CVPR*, 2021, pp. 1–9.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] M. Shukla 和 S. Ahmed, “回归中主动学习损失的数学分析,” 在 *Proc. of CVPR*, 2021, 页 1–9.'
- en: '[153] D. Yuan, X. Chang, Q. Liu, D. Wang, and Z. He, “Active learning for deep
    visual tracking,” *CoRR*, 2021\. [Online]. Available: [https://arxiv.org/abs/2110.13259](https://arxiv.org/abs/2110.13259)'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] D. Yuan, X. Chang, Q. Liu, D. Wang, 和 Z. He, “深度视觉跟踪中的主动学习,” *CoRR*,
    2021. [在线]. 可用: [https://arxiv.org/abs/2110.13259](https://arxiv.org/abs/2110.13259)'
- en: '[154] Z. Chen, J. Zhao *et al.*, “Multi-target active object tracking with
    monte carlo tree search and target motion modeling,” *CoRR*, 2022. [Online]. Available:
    [https://arxiv.org/abs/2205.03555?context=cs.LG](https://arxiv.org/abs/2205.03555?context=cs.LG)'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Z. Chen, J. Zhao *等*, “基于蒙特卡罗树搜索和目标运动建模的多目标主动对象跟踪,” *CoRR*, 2022. [在线].
    可用: [https://arxiv.org/abs/2205.03555?context=cs.LG](https://arxiv.org/abs/2205.03555?context=cs.LG)'
- en: '[155] X. Xu, L. Liu, X. Zhang, W. Guan, and R. Hu, “Rethinking data collection
    for person re-identification: active redundancy reduction,” *Pattern Recognit.*,
    vol. 113, p. 107827, 2021.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] X. Xu, L. Liu, X. Zhang, W. Guan, 和 R. Hu, “重新思考用于人脸重识别的数据收集：主动冗余减少,”
    *Pattern Recognit.*, 卷 113, 页 107827, 2021.'
- en: '[156] J. Cai, J. Tang, Q. Chen, Y. Hu, X. Wang, and S. Huang, “Multi-view active
    learning for video recommendation,” in *Proc. of IJCAI*, 2019, pp. 2053–2059.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J. Cai, J. Tang, Q. Chen, Y. Hu, X. Wang, 和 S. Huang, “用于视频推荐的多视角主动学习,”
    在 *Proc. of IJCAI*, 2019, 页 2053–2059.'
- en: '[157] L. Zhao, S. J. Pan, and Q. Yang, “A unified framework of active transfer
    learning for cross-system recommendation,” *Artif. Intell.*, vol. 245, pp. 38–55,
    2017.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] L. Zhao, S. J. Pan, 和 Q. Yang, “跨系统推荐的主动迁移学习统一框架,” *Artif. Intell.*,
    卷 245, 页 38–55, 2017.'
- en: '[158] P. Gupta, R. Jindal, and A. Sharma, “Community trolling: An active learning
    approach for topic based community detection in big data,” *J. Grid Comput.*,
    vol. 16, no. 4, pp. 553–567, 2018.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] P. Gupta, R. Jindal, 和 A. Sharma, “社区恶搞：一种针对大数据中的主题社区检测的主动学习方法,” *J.
    Grid Comput.*, 卷 16, 第 4 期, 页 553–567, 2018.'
- en: '[159] E. Chien, A. M. Tulino, and J. Llorca, “Active learning in the geometric
    block model,” in *Proc. of AAAI*, 2020, pp. 3641–3648.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] E. Chien, A. M. Tulino, 和 J. Llorca, “几何块模型中的主动学习,” 在 *Proc. of AAAI*,
    2020, 页 3641–3648.'
- en: '[160] K. Deng, J. Pineau, and S. A. Murphy, “Active learning for personalizing
    treatment,” in *Proc. of ADPRL*.   IEEE, 2011, pp. 32–39.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] K. Deng, J. Pineau, 和 S. A. Murphy, “用于个性化治疗的主动学习,” 在 *Proc. of ADPRL*.
    IEEE, 2011, 页 32–39.'
- en: '[161] K. Jedoui, R. Krishna, M. S. Bernstein, and L. Fei-Fei, “Deep bayesian
    active learning for multiple correct outputs,” *CoRR*, 2019\. [Online]. Available:
    [https://arxiv.org/abs/1912.01119](https://arxiv.org/abs/1912.01119)'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] K. Jedoui, R. Krishna, M. S. Bernstein, 和 L. Fei-Fei, “多正确输出的深度贝叶斯主动学习,”
    *CoRR*, 2019. [在线]. 可用: [https://arxiv.org/abs/1912.01119](https://arxiv.org/abs/1912.01119)'
- en: '[162] M. Moradshahi, V. Tsai, G. Campagna, and M. Lam, “Contextual semantic
    parsing for multilingual task-oriented dialogues,” in *Proc. of EACL*, 2023, pp.
    902–915.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] M. Moradshahi, V. Tsai, G. Campagna, 和 M. Lam, “多语言任务导向对话的上下文语义解析,” 在
    *Proc. of EACL*, 2023, 页 902–915.'
- en: '[163] Z. Li and G. Haffari, “Active learning for multilingual semantic parser,”
    in *Findings of EACL*, 2023, pp. 621–627.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Z. Li 和 G. Haffari，“多语言语义解析器的主动学习，”发表于 *EACL 发现*，2023年，页码 621–627。'
- en: '[164] Z. Zhou, J. Shin *et al.*, “Fine-tuning convolutional neural networks
    for biomedical image analysis: Actively and incrementally,” in *Proc. of CVPR*,
    2017, pp. 7340–7351.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Z. Zhou, J. Shin *等*，“卷积神经网络在生物医学图像分析中的微调：主动和增量地，”发表于 *CVPR 会议论文集*，2017年，页码
    7340–7351。'
- en: '[165] M. Bilgic, L. Mihalkova, and L. Getoor, “Active learning for networked
    data,” in *Proc. of ICML*, 2010, pp. 79–86.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] M. Bilgic, L. Mihalkova 和 L. Getoor，“网络数据的主动学习，”发表于 *ICML 会议论文集*，2010年，页码
    79–86。'
- en: '[166] H. Cai, V. W. Zheng, and K. C. Chang, “Active learning for graph embedding,”
    *CoRR*, 2017\. [Online]. Available: [https://arxiv.org/abs/1705.05085](https://arxiv.org/abs/1705.05085)'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] H. Cai, V. W. Zheng 和 K. C. Chang，“图嵌入的主动学习，” *计算机科学回顾*，2017年\. [在线].
    可用: [https://arxiv.org/abs/1705.05085](https://arxiv.org/abs/1705.05085)'
- en: '[167] L. Gao, H. Yang *et al.*, “Active discriminative network representation
    learning,” in *Proc. of IJCAI*, 2018, pp. 2142–2148.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] L. Gao, H. Yang *等*，“主动区分网络表示学习，”发表于 *IJCAI 会议论文集*，2018年，页码 2142–2148。'
- en: '[168] A. Cheng, C. Zhou *et al.*, “Deep active learning for anchor user prediction,”
    in *Proc. of IJCAI*, 2019, pp. 2151–2157.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] A. Cheng, C. Zhou *等*，“用于锚用户预测的深度主动学习，”发表于 *IJCAI 会议论文集*，2019年，页码 2151–2157。'
- en: '[169] X. Yue, Y. Wen, J. H. Hunt, and J. Shi, “Active learning for gaussian
    process considering uncertainties with application to shape control of composite
    fuselage,” *IEEE Trans Autom. Sci. Eng.*, vol. 18, no. 1, pp. 36–46, 2021.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] X. Yue, Y. Wen, J. H. Hunt 和 J. Shi，“考虑不确定性的高斯过程主动学习及其在复合材料机身形状控制中的应用，”
    *IEEE 自动化科学与工程汇刊*，第18卷，第1期，页码 36–46，2021年。'
- en: '[170] C. Lee, X. Wang, J. Wu, and X. Yue, “Failure-averse active learning for
    physics-constrained systems,” *IEEE Trans Autom. Sci. Eng.*, vol. 20, no. 4, pp.
    2215–2226, 2023\. [Online]. Available: [https://doi.org/10.1109/TASE.2022.3213827](https://doi.org/10.1109/TASE.2022.3213827)'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] C. Lee, X. Wang, J. Wu 和 X. Yue，“面向物理约束系统的避免失败主动学习，” *IEEE 自动化科学与工程汇刊*，第20卷，第4期，页码
    2215–2226，2023年\. [在线]. 可用: [https://doi.org/10.1109/TASE.2022.3213827](https://doi.org/10.1109/TASE.2022.3213827)'
- en: '[171] C. Lee, K. Wang, J. Wu, W. Cai, and X. Yue, “Partitioned active learning
    for heterogeneous systems,” *J. Comput. Inf. Sci. Eng.*, vol. 23, no. 4, 2023\.
    [Online]. Available: [https://doi.org/10.1115/1.4056567](https://doi.org/10.1115/1.4056567)'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] C. Lee, K. Wang, J. Wu, W. Cai 和 X. Yue，“异构系统的分区主动学习，” *计算与信息科学工程杂志*，第23卷，第4期，2023年\.
    [在线]. 可用: [https://doi.org/10.1115/1.4056567](https://doi.org/10.1115/1.4056567)'
- en: '[172] A. Rahman, “Algorithms of oppression: How search engines reinforce racism,”
    *New Media Soc.*, vol. 22, no. 3, pp. 308–310, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] A. Rahman，“压迫算法：搜索引擎如何加剧种族主义，” *新媒体与社会*，第22卷，第3期，页码 308–310，2020年。'
- en: '[173] I. Sundin, P. Schulam *et al.*, “Active learning for decision-making
    from imbalanced observational data,” in *Proc. of ICML*, 2019, pp. 6046–6055.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] I. Sundin, P. Schulam *等*，“从不平衡观测数据中进行决策的主动学习，”发表于 *ICML 会议论文集*，2019年，页码
    6046–6055。'
- en: '[174] D. Li, Y. Wang, K. Funakoshi, and M. Okumura, “After: Active learning
    based fine-tuning framework for speech emotion recognition,” in *Proc. of IEEE-ASRU*,
    2023, pp. 1–8.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] D. Li, Y. Wang, K. Funakoshi 和 M. Okumura，“After: 基于主动学习的语音情感识别微调框架，”发表于
    *IEEE-ASRU 会议论文集*，2023年，页码 1–8。'
- en: '[175] G. McDonald, C. Macdonald, and I. Ounis, “Active learning stopping strategies
    for technology-assisted sensitivity review,” in *Proc. of SIGIR*, 2020, pp. 2053–2056.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] G. McDonald, C. Macdonald 和 I. Ounis，“用于技术辅助敏感性审查的主动学习停止策略，”发表于 *SIGIR
    会议论文集*，2020年，页码 2053–2056。'
- en: '[176] H. Yu, X. Yang *et al.*, “Active learning from imbalanced data: A solution
    of online weighted extreme learning machine,” *IEEE Trans. Neural Networks Learn.
    Syst.*, vol. 30, no. 4, pp. 1088–1103, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] H. Yu, X. Yang *等*，“从不平衡数据中进行主动学习：在线加权极限学习机的解决方案，” *IEEE 神经网络与学习系统汇刊*，第30卷，第4期，页码
    1088–1103，2019年。'
- en: '[177] H. Ishibashi and H. Hino, “Stopping criterion for active learning based
    on deterministic generalization bounds,” in *Proc. of AISTATS*, 2020, pp. 386–397.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] H. Ishibashi 和 H. Hino，“基于确定性泛化界的主动学习停止准则，”发表于 *AISTATS 会议论文集*，2020年，页码
    386–397。'
- en: '[178] Y. Yu, R. Zhang *et al.*, “Cold-start data selection for better few-shot
    language model fine-tuning: A prompt-based uncertainty propagation approach,”
    in *Proc. of ACL*, 2023, pp. 2499–2521.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Y. Yu, R. Zhang *等*，“冷启动数据选择以优化少样本语言模型微调：一种基于提示的不确定性传播方法，”发表于 *ACL 会议论文集*，2023年，页码
    2499–2521。'
- en: '[179] L. Chen, Y. Bai *et al.*, “Making your first choice: To address cold
    start problem in vision active learning,” *CoRR*, 2022\. [Online]. Available:
    [https://arxiv.org/abs/2210.02442](https://arxiv.org/abs/2210.02442)'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] L. Chen, Y. Bai *等*，“做出你的第一次选择：解决视觉主动学习中的冷启动问题，”*CoRR*，2022年。 [在线]. 可用：
    [https://arxiv.org/abs/2210.02442](https://arxiv.org/abs/2210.02442)'
- en: '[180] O. Yehuda and A. Dekel, “Active learning through a covering lens,” in
    *Proc. of NeurIPS*, vol. 35, 2022, pp. 22 354–22 367.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] O. Yehuda 和 A. Dekel，“通过覆盖镜头的主动学习，”在*NeurIPS会议录*，第35卷，2022年，页码22 354–22 367。'
- en: '[181] X. Cao, I. W. Tsang, and J. Xu, “Cold-start active sampling via $\gamma$-tube,”
    *IEEE Trans. Cybern.*, vol. 52, no. 7, pp. 6034–6045, 2022.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] X. Cao, I. W. Tsang, 和 J. Xu，“通过$\gamma$-tube进行冷启动主动采样，”*IEEE Trans.
    Cybern.*，第52卷，第7期，页码6034–6045，2022年。'
- en: '[182] R. Mahmood, S. Fidler *et al.*, “Low-budget active learning via wasserstein
    distance: An integer programming approach,” in *Proc. of ICLR*, 2022\. [Online].
    Available: [https://arxiv.org/abs/2106.02968](https://arxiv.org/abs/2106.02968)'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] R. Mahmood, S. Fidler *等*，“通过wasserstein距离进行低预算主动学习：一种整数规划方法，”在*ICLR会议录*，2022年。
    [在线]. 可用： [https://arxiv.org/abs/2106.02968](https://arxiv.org/abs/2106.02968)'
- en: '[183] Y. Tang and S. Huang, “Active learning for multiple target models,” in
    *Proc. of NeurIPS*, 2022, pp. 38 424–38 435.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Y. Tang 和 S. Huang，“用于多目标模型的主动学习，”在*NeurIPS会议录*，2022年，页码38 424–38 435。'
- en: '[184] S. Farquhar, Y. Gal *et al.*, “On statistical bias in active learning:
    How and when to fix it,” in *Proc. of ICLR*, 2021\. [Online]. Available: [https://openreview.net/forum?id=JiYq3eqTKY](https://openreview.net/forum?id=JiYq3eqTKY)'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] S. Farquhar, Y. Gal *等*，“关于主动学习中的统计偏差：如何以及何时修复它，”在*ICLR会议录*，2021年。 [在线].
    可用： [https://openreview.net/forum?id=JiYq3eqTKY](https://openreview.net/forum?id=JiYq3eqTKY)'
- en: '[185] M. Zlabinger, “Efficient and effective text-annotation through active
    learning,” in *Proc. of SIGIR*, 2019, p. 1456.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] M. Zlabinger，“通过主动学习实现高效且有效的文本标注，”在*SIGIR会议录*，2019年，页码1456。'
- en: '[186] B. Kwak, Y. Kim *et al.*, “Trustal: Trustworthy active learning using
    knowledge distillation,” in *Proc. of AAAI*, 2022, pp. 7263–7271.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] B. Kwak, Y. Kim *等*，“Trustal: 使用知识蒸馏的可信主动学习，”在*AAAI会议录*，2022年，页码7263–7271。'
- en: '[187] H. Ma, C. Li, X. Shi, Y. Yuan, and G. Wang, “Deep unsupervised active
    learning on learnable graphs,” *CoRR*, 2021\. [Online]. Available: [https://arxiv.org/abs/2111.04286](https://arxiv.org/abs/2111.04286)'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] H. Ma, C. Li, X. Shi, Y. Yuan, 和 G. Wang，“在可学习图上进行深度无监督主动学习，”*CoRR*，2021年。
    [在线]. 可用： [https://arxiv.org/abs/2111.04286](https://arxiv.org/abs/2111.04286)'
- en: '[188] S. Mamooler, R. Lebret, S. Massonnet, and K. Aberer, “An efficient active
    learning pipeline for legal text classification,” in *Proc. of NLLP*, 2022, pp.
    345–358.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] S. Mamooler, R. Lebret, S. Massonnet, 和 K. Aberer，“用于法律文本分类的高效主动学习管道，”在*NLLP会议录*，2022年，页码345–358。'
- en: '[189] F. Wan, T. Yuan *et al.*, “Nearest neighbor classifier embedded network
    for active learning,” in *Proc. of AAAI*, 2021, pp. 10 041–10 048.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] F. Wan, T. Yuan *等*，“嵌入网络的最近邻分类器用于主动学习，”在*AAAI会议录*，2021年，页码10 041–10 048。'
- en: '[190] X. Deng, W. Wang, F. Feng, H. Zhang, X. He, and Y. Liao, “Counterfactual
    active learning for out-of-distribution generalization,” in *Proc. of ACL*, 2023,
    pp. 11 362–11 377.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] X. Deng, W. Wang, F. Feng, H. Zhang, X. He, 和 Y. Liao，“反事实主动学习以实现分布外泛化，”在*ACL会议录*，2023年，页码11 362–11 377。'
- en: '[191] H. Wang, W. Huang, Z. Wu, H. Tong, A. Margenot, and J. He, “Deep active
    learning by leveraging training dynamics,” in *Proc. of NeurIPS*, 2022, pp. 25 171–25 184.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] H. Wang, W. Huang, Z. Wu, H. Tong, A. Margenot, 和 J. He，“通过利用训练动态的深度主动学习，”在*NeurIPS会议录*，2022年，页码25 171–25 184。'
- en: '[192] L. Zhao, G. Sukthankar *et al.*, “Incremental relabeling for active learning
    with noisy crowdsourced annotations,” in *Proc. of PASSAT*, 2011, pp. 728–733.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] L. Zhao, G. Sukthankar *等*，“带有噪声众包注释的主动学习的增量重新标注，”在*PASSAT会议录*，2011年，页码728–733。'
- en: '[193] Z. Ashari, H. Ghasemzadeh *et al.*, “Mindful active learning,” in *Proc.
    of IJCAI*, 2019, pp. 2265–2271.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Z. Ashari, H. Ghasemzadeh *等*，“用心的主动学习，”在*IJCAI会议录*，2019年，页码2265–2271。'
- en: '[194] J. Choi, K. Yi *et al.*, “Vab-al: Incorporating class imbalance and difficulty
    with variational bayes for active learning,” in *Proc. of CVPR*, 2021, pp. 6749–6758.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] J. Choi, K. Yi *等*，“Vab-al: 结合类别不平衡和难度使用变分贝叶斯进行主动学习，”在*CVPR会议录*，2021年，页码6749–6758。'
- en: '[195] E. Zhao, A. Liu *et al.*, “Active learning under label shift,” in *Proc.
    of AISTATS*, 2021, pp. 3412–3420.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] E. Zhao, A. Liu *等*，“标签偏移下的主动学习，”在*AISTATS会议录*，2021年，页码3412–3420。'
- en: '[196] J. S. Hartford, K. Leyton-Brown, H. Raviv, D. Padnos, S. Lev, and B. Lenz,
    “Exemplar guided active learning,” in *Proc. of NeurIPS*, 2020. [Online]. Available:
    [https://arxiv.org/abs/2011.01285](https://arxiv.org/abs/2011.01285)'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] J. S. Hartford, K. Leyton-Brown, H. Raviv, D. Padnos, S. Lev, 和 B. Lenz,
    “示例引导的主动学习，” 在 *NeurIPS 会议录*，2020年。[在线]。可用： [https://arxiv.org/abs/2011.01285](https://arxiv.org/abs/2011.01285)'
- en: '[197] J. Zhang, J. Katz-Samuels, and R. D. Nowak, “GALAXY: graph-based active
    learning at the extreme,” in *Proc. of ICML*, 2022, pp. 26 223–26 238.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] J. Zhang, J. Katz-Samuels, 和 R. D. Nowak, “GALAXY：极端情况下的基于图的主动学习，” 在
    *ICML 会议录*，2022年，第26,223–26,238页。'
- en: '[198] R. He, Z. Han, X. Lu, and Y. Yin, “Safe-student for safe deep semi-supervised
    learning with unseen-class unlabeled data,” in *Proc. of CVPR*, 2022, pp. 14 565–14 574.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] R. He, Z. Han, X. Lu, 和 Y. Yin, “Safe-student for safe deep semi-supervised
    learning with unseen-class unlabeled data,” 在 *CVPR 会议录*，2022年，第14,565–14,574页。'
- en: '[199] Y. Tang and S. Huang, “Dual active learning for both model and data selection,”
    in *Proc. of IJCAI*, 2021, pp. 3052–3058.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Y. Tang 和 S. Huang, “双重主动学习用于模型和数据选择，” 在 *IJCAI 会议录*，2021年，第3052–3058页。'
- en: '[200] K. Ning, X. Zhao *et al.*, “Active learning for open-set annotation,”
    in *Proc. of CVPR*, 2022, pp. 41–49.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] K. Ning, X. Zhao *等*，“开放集标注的主动学习，” 在 *CVPR 会议录*，2022年，第41–49页。'
- en: '[201] G. Hacohen, A. Dekel, and D. Weinshall, “Active learning on a budget:
    Opposite strategies suit high and low budgets,” in *Proc. of ICML*, vol. 162,
    2022, pp. 8175–8195.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] G. Hacohen, A. Dekel, 和 D. Weinshall, “预算下的主动学习：高预算和低预算适用的相反策略，” 在 *ICML
    会议录*，第162卷，2022年，第8175–8195页。'
- en: '[202] M. Mosbach, M. Andriushchenko, and D. Klakow, “On the stability of fine-tuning
    bert: Misconceptions, explanations, and strong baselines,” in *Proc. of ICLR*,
    2021.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] M. Mosbach, M. Andriushchenko, 和 D. Klakow, “关于微调BERT的稳定性：误解、解释和强基线，”
    在 *ICLR 会议录*，2021年。'
- en: '[203] F. Tang, “Bidirectional active learning with gold-instance-based human
    training,” in *Proc. of IJCAI*, 2019, pp. 5989–5996.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] F. Tang, “基于金实例的人类训练的双向主动学习，” 在 *IJCAI 会议录*，2019年，第5989–5996页。'
- en: '[204] S. Kothawade, N. Beck *et al.*, “Submodular information measures based
    active learning in realistic scenarios,” in *Proc. of NeurIPS*, 2021, pp. 18 685–18 697.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] S. Kothawade, N. Beck *等*，“基于子模信息量的现实场景中的主动学习，” 在 *NeurIPS 会议录*，2021年，第18,685–18,697页。'
- en: '[205] S. Ertekin, J. Huang, and C. L. Giles, “Active learning for class imbalance
    problem,” in *Proc. of SIGIR*.   ACM, 2007, pp. 823–824.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] S. Ertekin, J. Huang, 和 C. L. Giles, “解决类别不平衡问题的主动学习，” 在 *SIGIR 会议录*。ACM，2007年，第823–824页。'
- en: '| ![[Uncaptioned image]](img/8fcae6e99057f47c2f8ddd7af8f90ca7.png) | Dongyuan
    Li received his bachelor’s degree in computer science from Dalian University of
    Technology in 2018\. He received his Master’s degree from the School of Computer
    Science and Technology, Xidian University in 2021. He currently is a third-year
    Ph.D. candidate in the School of Information and Communication Engineering, Tokyo
    Institute of Technology. His research interests include natural language processing,
    machine learning, data mining, social network analyses, and bio-informatics. |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/8fcae6e99057f47c2f8ddd7af8f90ca7.png) | 李东元于2018年获得了大连理工大学计算机科学学士学位。他从西电大学计算机科学与技术学院获得了硕士学位。目前，他是东京工业大学信息与通信工程学院的三年级博士生。他的研究兴趣包括自然语言处理、机器学习、数据挖掘、社交网络分析和生物信息学。
    |'
- en: '| ![[Uncaptioned image]](img/e1ceb8896b305b8f2f0571cd3cf3b34c.png) | Zhen Wang
    received his bachelor’s degree in computer science from Beihang University in
    2018\. He received his Master’s degree from the Faculty of Electrical Engineering,
    Mathematics, and Computer Science, Delft University of Technology. He is currently
    pursuing his Ph.D. degree in the School of Information and Communication Engineering,
    Tokyo Institute of Technology. His research interests include natural language
    processing, computer vision, and multimodal learning. |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/e1ceb8896b305b8f2f0571cd3cf3b34c.png) | 王臻于2018年获得了北航大学计算机科学学士学位。他从代尔夫特理工大学电气工程、数学和计算机科学学院获得了硕士学位。目前，他正在东京工业大学信息与通信工程学院攻读博士学位。他的研究兴趣包括自然语言处理、计算机视觉和多模态学习。
    |'
- en: '| ![[Uncaptioned image]](img/2024b71a13f4e6434d7bfcf81be0e0e4.png) | Yankai
    Chen currently is a fourth year Ph.D. candidate in Department of Computer Science
    and Engineering, The Chinese University of Hong Kong. He received the B.S. degree
    from Nanjing University in 2016 and the M.S. degree from the University of Hong
    Kong in 2018. His research interests include data mining and applied machine learning
    and for database management and information retrieval. |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/2024b71a13f4e6434d7bfcf81be0e0e4.png) | 陈彦凯目前是香港中文大学计算机科学与工程系的四年级博士生。他于2016年获得南京大学学士学位，并于2018年获得香港大学硕士学位。他的研究兴趣包括数据挖掘和应用机器学习，特别是在数据库管理和信息检索方面。
    |'
- en: '| ![[Uncaptioned image]](img/bb6d074cfe6594e103b3ab394b1c28ef.png) | Renhe
    Jiang is a lecturer at Center for Spatial Information Science, The University
    of Tokyo. He received his B.E. degree in Software Engineering from Dalian University
    of Technology in 2012, M.S. degree in Information Science from Nagoya University
    in 2015, and Ph.D. degree in Civil Engineering from The University of Tokyo in
    2019\. From 2019 to 2022, he was an assistant professor at Information Technology
    Center, The University of Tokyo. His research interests include spatiotemporal
    data mining, multivariate time series, graph neural networks. |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/bb6d074cfe6594e103b3ab394b1c28ef.png) | 任赫江是东京大学空间信息科学中心的讲师。他于2012年获得大连理工大学软件工程的学士学位，2015年获得名古屋大学信息科学的硕士学位，并于2019年获得东京大学土木工程的博士学位。从2019年到2022年，他曾在东京大学信息技术中心担任助理教授。他的研究兴趣包括时空数据挖掘、多变量时间序列和图神经网络。
    |'
- en: '| ![[Uncaptioned image]](img/2884a746a5e60108c6d49dcb4818def4.png) | Weiping
    Ding (M’16-SM’19) received the Ph.D. degree in Computer Science, Nanjing University
    of Aeronautics and Astronautics, Nanjing, China, in 2013\. In 2016, He was a Visiting
    Scholar at National University of Singapore, Singapore. From 2017 to 2018, he
    was a Visiting Professor at University of Technology Sydney, Australia. He is
    a Full Professor with the School of Information Science and Technology, Nantong
    University, Nantong, China, and also the supervisor of Ph.D postgraduate by the
    Faculty of Data Science at City University of Macau, China. His main research
    directions involve DNNs, multimodal machine learning, and medical images analysis.
    He has published over 250 articles, including over 110 IEEE Transactions papers.
    His nineteen authored/co-authored papers have been selected as ESI Highly Cited
    Papers. He has co-authored four books. He has holds 28 approved invention patents,
    including two U.S. patents and one Australian patent. He serves as an Associate
    Editor/Editorial Board member of IEEE Transactions on Neural Networks and Learning
    Systems, IEEE Transactions on Fuzzy Systems, IEEE Transactions on Intelligent
    Transportation Systems, IEEE Transactions on Intelligent Vehicles, IEEE Transactions
    on Artificial Intelligence, Information Fusion, Information Sciences, Neurocomputing,
    Applied Soft Computing. He is the Leading Guest Editor of Special Issues in several
    prestigious journals, including IEEE Transactions on Evolutionary Computation,
    IEEE Transactions on Fuzzy Systems, and Information Fusion. He is the Co-Editor-in-Chief
    of both Journal of Artificial Intelligence and Systems and Journal of Artificial
    Intelligence Advances. |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/2884a746a5e60108c6d49dcb4818def4.png) | 丁伟平（M’16-SM’19）于2013年获得南京航空航天大学计算机科学博士学位。2016年，他曾在新加坡国立大学担任访问学者。从2017年到2018年，他在悉尼科技大学担任访问教授。他是南通大学信息科学与技术学院的教授，同时也是澳门城市大学数据科学学院博士研究生的导师。他的主要研究方向包括深度神经网络、多模态机器学习和医学图像分析。他已发表超过250篇文章，其中包括110篇IEEE
    Transactions论文。他的十九篇论文被选为ESI高被引论文。他合著了四本书，持有28项批准的发明专利，其中包括两项美国专利和一项澳大利亚专利。他担任IEEE
    Transactions on Neural Networks and Learning Systems、IEEE Transactions on Fuzzy
    Systems、IEEE Transactions on Intelligent Transportation Systems、IEEE Transactions
    on Intelligent Vehicles、IEEE Transactions on Artificial Intelligence、Information
    Fusion、Information Sciences、Neurocomputing和Applied Soft Computing等期刊的副编辑/编辑委员会成员。他是多个知名期刊的特刊主编，包括IEEE
    Transactions on Evolutionary Computation、IEEE Transactions on Fuzzy Systems和Information
    Fusion。他是《人工智能与系统学报》和《人工智能进展》两本期刊的共同主编。 |'
- en: '| ![[Uncaptioned image]](img/df18044ab7bcc0a33398f3cfad282142.png) | Manabu
    Okumura was born in 1962\. He received B.E., M.E. and Dr. Eng. from Tokyo Institute
    of Technology in 1984, 1986 and 1989, respectively. He was an assistant professor
    at the Department of Computer Science, Tokyo Institute of Technology from 1989
    to 1992, and an associate professor at the School of Information Science, Japan
    Advanced Institute of Science and Technology from 1992 to 2000\. He was also a
    visiting associate professor at the Department of Computer Science, University
    of Toronto from 1997 to 1998\. From 2000, he was an associate professor at Precision
    and Intelligence Laboratory, Tokyo Institute of Technology, and he is currently
    a professor at Institute of Innovative Research, Tokyo Institute of Technology.
    His current research interests include natural language processing, especially
    text summarization, computer assisted language learning, and text data mining.
    |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/df18044ab7bcc0a33398f3cfad282142.png) | 大村学（Manabu Okumura）于1962年出生。他分别在1984年、1986年和1989年获得东京工业大学的工学学士、工学硕士和工学博士学位。他曾在1989年至1992年担任东京工业大学计算机科学系的助理教授，1992年至2000年担任日本先进工业技术学院信息科学学院的副教授。他还曾于1997年至1998年担任多伦多大学计算机科学系的访问副教授。从2000年起，他在东京工业大学精密与智能实验室担任副教授，目前是东京工业大学创新研究院的教授。他当前的研究兴趣包括自然语言处理，特别是文本摘要、计算机辅助语言学习和文本数据挖掘。'
