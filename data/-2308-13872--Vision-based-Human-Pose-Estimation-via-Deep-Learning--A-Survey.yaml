- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:37:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2308.13872] Vision-based Human Pose Estimation via Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2308.13872] 基于视觉的人体姿态估计：深度学习的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.13872](https://ar5iv.labs.arxiv.org/html/2308.13872)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.13872](https://ar5iv.labs.arxiv.org/html/2308.13872)
- en: 'Vision-based Human Pose Estimation via Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于视觉的人体姿态估计：深度学习的综述
- en: 'Gongjin Lan^∗ [<svg   height="13.95" overflow="visible" version="1.1" width="17.01"><g
    transform="translate(0,13.95) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,6.98)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.89 -2.36)" fill="#FFFFFF" stroke="#FFFFFF" color="#A6CE39"><foreignobject
    width="7.78" height="4.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ID</foreignobject></g></g></svg>](https://orcid.org/0000-0003-2020-8186)
    , , Yu Wu, Fei Hu, , Qi Hao^∗ [<svg   height="13.95" overflow="visible" version="1.1"
    width="17.01"><g transform="translate(0,13.95) matrix(1 0 0 -1 0 0) translate(8.5,0)
    translate(0,6.98)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.89 -2.36)" fill="#FFFFFF" stroke="#FFFFFF" color="#A6CE39"><foreignobject
    width="7.78" height="4.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ID</foreignobject></g></g></svg>](https://orcid.org/0000-0002-2792-5965)
    This work is partially supported by the National Natural Science Foundation of
    China (No: 61773197), the Shenzhen Fundamental Research Program (No: JCYJ20200109141622964),
    the Intel ICRI-IACV Research Fund ($CG\#52514373$). (Corresponding authors: Gongjin
    Lan; Qi Hao.)Gongjin Lan, Yu Wu, Qi Hao are with the Department of Computer Science
    and Engineering, Southern University of Science and Technology, Shenzhen, 518055,
    China (e-mail: langj@sustech.edu.cn, wuy@mail.sustech.edu.cn, hao.q@sustech.edu.cn)Fei
    Hu is with the Department of Electrical and Computer Engineering, University of
    Alabama, Tuscaloosa, AL (email: fei@eng.ua.edu)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Gongjin Lan^∗ [<svg height="13.95" overflow="visible" version="1.1" width="17.01"><g
    transform="translate(0,13.95) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,6.98)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.89 -2.36)" fill="#FFFFFF" stroke="#FFFFFF" color="#A6CE39"><foreignobject
    width="7.78" height="4.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ID</foreignobject></g></g></svg>](https://orcid.org/0000-0003-2020-8186)
    , , Yu Wu, Fei Hu, , Qi Hao^∗ [<svg height="13.95" overflow="visible" version="1.1"
    width="17.01"><g transform="translate(0,13.95) matrix(1 0 0 -1 0 0) translate(8.5,0)
    translate(0,6.98)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.89 -2.36)" fill="#FFFFFF" stroke="#FFFFFF" color="#A6CE39"><foreignobject
    width="7.78" height="4.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ID</foreignobject></g></g></svg>](https://orcid.org/0000-0002-2792-5965)
    本研究部分由中国国家自然科学基金（编号：61773197）、深圳市基础研究计划（编号：JCYJ20200109141622964）、英特尔 ICRI-IACV
    研究基金（$CG\#52514373$）资助。（通讯作者：Gongjin Lan；Qi Hao。）Gongjin Lan、Yu Wu 和 Qi Hao 隶属于南方科技大学计算机科学与工程系，深圳
    518055，中国（电子邮件：langj@sustech.edu.cn, wuy@mail.sustech.edu.cn, hao.q@sustech.edu.cn）Fei
    Hu 隶属于阿拉巴马大学电气与计算机工程系，塔斯卡卢萨，AL（电子邮件：fei@eng.ua.edu）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Human Pose Estimation (HPE) has attracted a significant amount of attention
    from the computer vision community in the past decades. Moreover, HPE has been
    applied to various domains such as human-computer interaction, sports analysis,
    and human tracking via images and videos. Recently, deep learning-based approaches
    have shown state-of-the-art performance in HPE-based applications. Although deep
    learning-based approaches have achieved remarkable performance in HPE, a comprehensive
    review of deep learning-based HPE methods remains lacking in the literature. In
    this paper, we provide an up-to-date and in-depth overview of the deep learning
    approaches in vision-based HPE. We summarize these methods of 2D and 3D HPE, and
    their applications, discuss the challenges and the research trends through bibliometrics
    and provide insightful recommendations for future research. This article provides
    a meaningful overview as introductory material for beginners to deep learning-based
    HPE, as well as supplementary material for advanced researchers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计（HPE）在过去几十年里引起了计算机视觉领域的广泛关注。此外，HPE 已被应用于多个领域，如人机交互、体育分析以及通过图像和视频进行人体跟踪。近年来，基于深度学习的方法在
    HPE 应用中表现出了最先进的性能。尽管基于深度学习的方法在 HPE 中取得了显著成绩，但文献中仍缺乏对这些方法的全面综述。本文提供了对基于视觉的 HPE
    中深度学习方法的最新、深入的概述。我们总结了这些 2D 和 3D HPE 方法及其应用，讨论了挑战和研究趋势，通过文献计量学提供了对未来研究的有益建议。本文为初学者提供了对深度学习
    HPE 的有意义概述，同时也为高级研究人员提供了补充材料。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'Index Terms:'
- en: Human pose estimation, Human performance assessment, Deep learning, Action recognition,
    Bibliometric.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计、人类表现评估、深度学习、动作识别、文献计量学。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Human Pose Estimation (HPE) refers to estimating the positions of human joints
    and their associations in images or videos, which is a popular research topic
    in computer vision. It has been widely applied to various applications, such as
    action analysis [[1](#bib.bib1)], HCI [[2](#bib.bib2)], gaming [[3](#bib.bib3)],
    sport analysis [[4](#bib.bib4), [5](#bib.bib5)], motion capture [[6](#bib.bib6)],
    computer-generated imagery [[7](#bib.bib7), [8](#bib.bib8)]. Although HPE has
    been studied for decades, it is still an open and challenging task since the two
    main aspects of the wide diversity of the human body (such as various human poses,
    various clothing, environment or illumination conditions) and reconstruction ambiguity
    caused by occlusions (particularly the crowd) [[9](#bib.bib9), [10](#bib.bib10)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计（HPE）指的是估计图像或视频中人体关节的位置及其关联，这是计算机视觉中的一个热门研究主题。它已广泛应用于各种应用中，如动作分析[[1](#bib.bib1)]、人机交互[[2](#bib.bib2)]、游戏[[3](#bib.bib3)]、运动分析[[4](#bib.bib4),
    [5](#bib.bib5)]、动作捕捉[[6](#bib.bib6)]、计算机生成图像[[7](#bib.bib7), [8](#bib.bib8)]。尽管HPE已经研究了几十年，但它仍然是一个开放且具有挑战性的任务，因为主要涉及人体的广泛多样性（如各种人体姿态、各种服装、环境或光照条件）和由于遮挡（尤其是人群）造成的重建模糊性[[9](#bib.bib9),
    [10](#bib.bib10)]。
- en: 'The early HPE approaches often use predefined models and statistical learning
    to describe the human poses [[11](#bib.bib11), [12](#bib.bib12)]. However, those
    methods are incapable of learning from a large amount of data and suffer from
    limited model representation capability. In recent years, deep learning-based
    approaches yield great improvements in many computer vision tasks such as classification
    [[13](#bib.bib13)], object detection [[14](#bib.bib14), [15](#bib.bib15)], and
    HPE [[9](#bib.bib9)]. The success of deep learning in HPE is mainly due to the
    following three facts: the availability of big data, superior representation capability
    of deep neural networks, and high-performance hardware (e.g., GPU platform). The
    deep learning-based methods dramatically outperform the traditional approaches.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的HPE方法通常使用预定义模型和统计学习来描述人体姿态[[11](#bib.bib11), [12](#bib.bib12)]。然而，这些方法无法从大量数据中学习，且模型表示能力有限。近年来，基于深度学习的方法在许多计算机视觉任务中取得了显著改进，如分类[[13](#bib.bib13)]、目标检测[[14](#bib.bib14),
    [15](#bib.bib15)]和HPE[[9](#bib.bib9)]。深度学习在HPE中的成功主要归因于以下三点：大数据的可用性、深度神经网络的优越表示能力以及高性能硬件（如GPU平台）。基于深度学习的方法显著优于传统方法。
- en: Although there are many promising methods in the vision-based HPE via deep learning
    models [[9](#bib.bib9), [16](#bib.bib16), [17](#bib.bib17)], a lack of articles
    with an up-to-date and in-depth review of this domain. We emphasize that a comprehensive
    overview of HPE should cover both 2D and 3D HPE studies. In this paper, we aim
    to provide a complete and solid survey, analyze the research challenges, and point
    out the research trends in HPE. In particular, we apply bibliometrics to retrieve
    scientific publications for analyzing the research trends in HPE. This paper comprehensively
    reviews HPE topics that cover both 2D and 3D HPE studies, discusses the challenges,
    observes the trends, and provides detailed bibliometrics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在基于视觉的深度学习模型的HPE（人体姿态估计）中有许多有前景的方法[[9](#bib.bib9), [16](#bib.bib16), [17](#bib.bib17)]，但仍缺乏关于该领域的最新和深入的综述文章。我们强调，对HPE的全面概述应涵盖2D和3D
    HPE的研究。本文旨在提供一个完整且扎实的调查，分析研究挑战，并指出HPE中的研究趋势。特别地，我们应用文献计量学来检索科学出版物，以分析HPE中的研究趋势。本文全面回顾了涵盖2D和3D
    HPE研究的HPE主题，讨论了挑战，观察了趋势，并提供了详细的文献计量分析。
- en: I-A Related Work
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 相关工作
- en: To date, several survey papers have discussed the related studies in HPE. Dang
    et al. [[18](#bib.bib18)] provided a survey on deep learning-based 2D HPE, including
    single- and multi-person pipelines. Poppe et al. [[19](#bib.bib19)] presented
    an overview of the literature on vision-based human action recognition. Currently,
    there are many studies that investigate monocular HPE, and several survey papers
    that discuss the studies in monocular HPE. In [[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)], the studies of monocular HPE are reviewed comprehensively,
    particularly the deep learning-based methods. Dang et al. [[23](#bib.bib23)] provided
    a comprehensive survey on sensor- or vision-based human activity recognition.
    Gadhiya et al. [[24](#bib.bib24)] analyzed and compared several prevalent HPE
    methods. The latest work [[25](#bib.bib25)] reviewed the studies of deep learning-based
    3D HPE. Although these survey papers covered HPE-related topics, they focus on
    one of the specific topics in HPE. In this paper, we aim to provide a comprehensive
    survey on the vision-based HPE based on deep learning models in terms of 2D and
    3D HPE.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，已有几篇调查论文讨论了HPE相关的研究。Dang 等人 [[18](#bib.bib18)] 提供了关于基于深度学习的2D HPE的调查，包括单人和多人管道。Poppe
    等人 [[19](#bib.bib19)] 介绍了基于视觉的人类动作识别文献的概述。目前，有许多研究探讨了单目HPE，并且有几篇调查论文讨论了单目HPE的研究。在
    [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)] 中，单目HPE的研究得到了全面回顾，特别是基于深度学习的方法。Dang
    等人 [[23](#bib.bib23)] 提供了关于传感器或视觉基础的人类活动识别的综合调查。Gadhiya 等人 [[24](#bib.bib24)]
    分析并比较了几种流行的HPE方法。最新的工作 [[25](#bib.bib25)] 回顾了基于深度学习的3D HPE的研究。尽管这些调查论文涵盖了HPE相关的主题，但它们主要关注HPE中的某一个具体主题。本文旨在提供基于深度学习模型的视觉基础HPE的全面调查，涵盖2D和3D
    HPE。
- en: I-B Contributions
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 贡献
- en: 'In this paper, we aim to provide a detailed overview of the existing studies
    on deep learning-based human pose estimation. This review has three objectives:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在提供对现有基于深度学习的人体姿态估计研究的详细概述。此综述有三个目标：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Delineate the picture of this field from a ‘helicopter view’.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从“直升机视角”描绘该领域的全貌。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Clarify main research streams and provide a complete overview of vision- and
    deep learning-based HPE.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 明确主要研究方向，并提供关于基于视觉和深度学习的HPE的完整概述。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discuss the challenges and the research trends through bibliometrics, and provide
    insightful recommendations for future research.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过文献计量学讨论挑战和研究趋势，并为未来的研究提供有见地的建议。
- en: 'This survey covers both 2D- and 3D-based approaches. [Figure 1](#S1.F1 "Figure
    1 ‣ I-B Contributions ‣ I Introduction ‣ Vision-based Human Pose Estimation via
    Deep Learning: A Survey") shows the taxonomy of the approaches (e.g., image-based
    or video-based, 2D HPE or 3D HPE, monocular or multi-view), applications, trends,
    and challenges in this survey. These contributions provide our survey with a more
    solid, up-to-date, and in-depth insight than the existing survey papers.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查涵盖了基于2D和3D的方法。[图 1](#S1.F1 "Figure 1 ‣ I-B Contributions ‣ I Introduction
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey") 显示了这些方法的分类（例如，基于图像或视频、2D
    HPE或3D HPE、单目或多视角）、应用、趋势和挑战。这些贡献使我们的调查比现有调查论文具有更为坚实、最新和深入的见解。'
- en: '![Refer to caption](img/b32e8dd998adc469dd6c59912e995991.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b32e8dd998adc469dd6c59912e995991.png)'
- en: 'Figure 1: Taxonomy of this survey.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：本调查的分类。
- en: 'The rest of this paper is organized as follows. In [section II](#S2 "II Preliminary
    Knowledge ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"),
    we present the preliminary knowledge, common datasets, and the metrics for HPE.
    Image-based 2D HPE is summarized in [section III](#S3 "III 2D Human Pose Estimation
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"). We address
    video-based 2D HPE in [subsection III-C](#S3.SS3 "III-C Video-based 2D HPE ‣ III
    2D Human Pose Estimation ‣ Vision-based Human Pose Estimation via Deep Learning:
    A Survey"). The studies of 3D HPE is addressed in [section IV](#S4 "IV 3D Human
    Pose Estimation ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").
    The applications of vision-based HPE using deep learning are presented in [section V](#S5
    "V Applications ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").
    Finally, the research trends and challenges are discussed in [section VI](#S6
    "VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey"), followed by the conclusions in [section VII](#S7 "VII Conclusions
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '本文其余部分组织如下。在[第II节](#S2 "II Preliminary Knowledge ‣ Vision-based Human Pose
    Estimation via Deep Learning: A Survey")中，我们介绍了基础知识、常见数据集以及HPE的指标。基于图像的2D HPE总结在[第III节](#S3
    "III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation via Deep Learning:
    A Survey")。视频基于的2D HPE在[III-C小节](#S3.SS3 "III-C Video-based 2D HPE ‣ III 2D Human
    Pose Estimation ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey")中讨论。3D
    HPE的研究在[第IV节](#S4 "IV 3D Human Pose Estimation ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey")中进行。使用深度学习的基于视觉的HPE的应用在[第V节](#S5 "V Applications
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey")中介绍。最后，研究趋势和挑战在[第VI节](#S6
    "VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey")中讨论，随后在[第VII节](#S7 "VII Conclusions ‣ Vision-based Human Pose
    Estimation via Deep Learning: A Survey")中给出结论。'
- en: II Preliminary Knowledge
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 基础知识
- en: In this section, we introduce the preliminary knowledge, including essential
    concepts to guide the readers on the big picture of HPE and describe the solution
    representation and the well-known datasets with the performance metrics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了基础知识，包括指导读者了解HPE总体概况的基本概念，并描述了解决方案的表示以及具有性能指标的知名数据集。
- en: '| 2D/3D | Type | Year | Dataset | URL (Open dataset) | Data Scale | #Joints
    | Metrics |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2D/3D | 类型 | 年份 | 数据集 | URL（开放数据集） | 数据规模 | #关节 | 指标 |'
- en: '| 2D | Single- Person | 2010 | LSP [[26](#bib.bib26)] | [http://sam.johnson.io/research/lsp.html](http://sam.johnson.io/research/lsp.html)
    | 2K images | 14 | PCK&PCP |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 2D | 单人 | 2010 | LSP [[26](#bib.bib26)] | [http://sam.johnson.io/research/lsp.html](http://sam.johnson.io/research/lsp.html)
    | 2K图像 | 14 | PCK&PCP |'
- en: '| 2013 | FLIC [[27](#bib.bib27)] | [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html)
    | 5K images | 10 | PCK&PCP |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | FLIC [[27](#bib.bib27)] | [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html)
    | 5K图像 | 10 | PCK&PCP |'
- en: '| 2013 | J-HMDB [[28](#bib.bib28)] | [http://jhmdb.is.tue.mpg.de/](http://jhmdb.is.tue.mpg.de/)
    | 928 video clips | 15 | PCK&PCP |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | J-HMDB [[28](#bib.bib28)] | [http://jhmdb.is.tue.mpg.de/](http://jhmdb.is.tue.mpg.de/)
    | 928个视频剪辑 | 15 | PCK&PCP |'
- en: '| 2013 | PennAction [[29](#bib.bib29)] | [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)
    | 2326 video clips | 13 | PCK&PCP |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | PennAction [[29](#bib.bib29)] | [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)
    | 2326个视频剪辑 | 13 | PCK&PCP |'
- en: '| Multi- Person | 2014 | MPII [[30](#bib.bib30)] | [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)
    | 25K images & 40K persons | 16 | mAP |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 多人 | 2014 | MPII [[30](#bib.bib30)] | [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)
    | 25K图像 & 40K人 | 16 | mAP |'
- en: '| 2016 | COCO [[31](#bib.bib31)] | [https://cocodataset.org/#home](https://cocodataset.org/#home)
    | 330K images & 250K persons | 17 | AP & AR |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | COCO [[31](#bib.bib31)] | [https://cocodataset.org/#home](https://cocodataset.org/#home)
    | 330K图像 & 250K人 | 17 | AP & AR |'
- en: '| 2018 | PoseTrack [[16](#bib.bib16)] | [https://github.com/umariqb/PoseTrack-CVPR2017](https://github.com/umariqb/PoseTrack-CVPR2017)
    | 46K frames & 276K persons | 15 | mAP |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | PoseTrack [[16](#bib.bib16)] | [https://github.com/umariqb/PoseTrack-CVPR2017](https://github.com/umariqb/PoseTrack-CVPR2017)
    | 46K帧 & 276K人 | 15 | mAP |'
- en: '| 2019 | CrowdPose [[32](#bib.bib32)] | [https://github.com/Jeff-sjtu/CrowdPose](https://github.com/Jeff-sjtu/CrowdPose)
    | 20K images & 80K persons | 14 | mAP |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | CrowdPose [[32](#bib.bib32)] | [https://github.com/Jeff-sjtu/CrowdPose](https://github.com/Jeff-sjtu/CrowdPose)
    | 20K图像 & 80K人 | 14 | mAP |'
- en: '| 3D | Single- Person | 2014 | Human3.6M [[33](#bib.bib33)] | [http://vision.imar.ro/human3.6m/](http://vision.imar.ro/human3.6m/)
    | 3.6M frames & 4 camera views | 17 | MPJPE |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 3D | 单人 | 2014 | Human3.6M [[33](#bib.bib33)] | [http://vision.imar.ro/human3.6m/](http://vision.imar.ro/human3.6m/)
    | 3.6M帧 & 4个摄像头视角 | 17 | MPJPE |'
- en: '| Multi- Person | 2017 | CMU Panoptic [[34](#bib.bib34)] | [http://domedb.perception.cs.cmu.edu/](http://domedb.perception.cs.cmu.edu/)
    | 1.5M frames & 512 camera views | 15 | MPJPE |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 多人 | 2017 | CMU Panoptic [[34](#bib.bib34)] | [http://domedb.perception.cs.cmu.edu/](http://domedb.perception.cs.cmu.edu/)
    | 150 万帧 & 512 个摄像头视角 | 15 | MPJPE |'
- en: '| 2018 | 3DPW [[35](#bib.bib35)] | [https://virtualhumans.mpi-inf.mpg.de/3DPW/](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    | 51K frames & 1 camera view | 18 | MPJPE |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 3DPW [[35](#bib.bib35)] | [https://virtualhumans.mpi-inf.mpg.de/3DPW/](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    | 51K 帧 & 1 个摄像头视角 | 18 | MPJPE |'
- en: '|  | 2017 | MPI-INF-3DHP [[36](#bib.bib36)] | [https://vcai.mpi-inf.mpg.de/3dhp-dataset/](https://vcai.mpi-inf.mpg.de/3dhp-dataset/)
    | 1.3M frames & 16 camera views | 15 | MPJPE |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | MPI-INF-3DHP [[36](#bib.bib36)] | [https://vcai.mpi-inf.mpg.de/3dhp-dataset/](https://vcai.mpi-inf.mpg.de/3dhp-dataset/)
    | 130 万帧 & 16 个摄像头视角 | 15 | MPJPE |'
- en: '|  | 2018 | JTA [[37](#bib.bib37)] | [https://github.com/fabbrimatteo/JTA-Dataset](https://github.com/fabbrimatteo/JTA-Dataset)
    | 460K frames & 1 camera view | 14 | MPJPE |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | JTA [[37](#bib.bib37)] | [https://github.com/fabbrimatteo/JTA-Dataset](https://github.com/fabbrimatteo/JTA-Dataset)
    | 460K 帧 & 1 个摄像头视角 | 14 | MPJPE |'
- en: 'TABLE I: Illustration of the well-known datasets in 2D and 3D HPE.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 2D 和 3D HPE 中常见数据集的示例。'
- en: II-A Deep Learning in HPE
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 深度学习在 HPE 中的应用
- en: 'Currently, deep learning-based approaches have become state-of-the-art methods
    in HPE. The availability of big datasets, advanced hardware like GPU, and the
    surpassing performance of deep neural networks lead to the increasing interest
    in deep learning-based HPE. In this subsection, we discuss three topical types
    of neural networks in HPE: Convolutional Neural Networks (CNNs), Recurrent Neural
    Networks (RNNs), and Graph Convolutional Networks (GCNs).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，基于深度学习的方法已经成为 HPE 的最先进技术。大型数据集的可用性、先进的硬件如 GPU 和深度神经网络的超越性能导致了对深度学习 HPE 的日益关注。在本小节中，我们探讨了
    HPE 中三种热门的神经网络类型：卷积神经网络 (CNNs)、循环神经网络 (RNNs) 和图卷积网络 (GCNs)。
- en: II-A1 Convolutional Neural Networks
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 卷积神经网络
- en: In general, a CNN for HPE tasks consists of two parts. The first part commonly
    uses off-the-shelf generic pre-trained networks such as ResNet [[38](#bib.bib38)]
    to extract features, the so-called backbone network. The second part, called the
    prediction head, predicts human poses with the extracted features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，用于 HPE 任务的 CNN 包含两部分。第一部分通常使用现成的通用预训练网络，如 ResNet [[38](#bib.bib38)]，来提取特征，即所谓的骨干网络。第二部分，称为预测头，利用提取的特征预测人体姿势。
- en: The well-known networks like AlexNet [[13](#bib.bib13)] and ResNet [[38](#bib.bib38)]
    show remarkable classification performance on the dataset of ImageNet, and advanced
    performance in HPE [[39](#bib.bib39), [40](#bib.bib40)] as well. However, there
    is a gap between classification and HPE tasks since their targeted features and
    prediction differ from each other. Instead of directly using the backbones from
    classification tasks, HPE-specified backbones need to be improved for HPE tasks.
    For example, Hourglass [[41](#bib.bib41)], Cascaded Pyramid Network (CPN) [[42](#bib.bib42)],
    and HRNet [[10](#bib.bib10)] are proposed to be the backbone for the deep learning-based
    approaches in HPE.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 像 AlexNet [[13](#bib.bib13)] 和 ResNet [[38](#bib.bib38)] 这样的著名网络在 ImageNet 数据集上展示了出色的分类性能，并且在
    HPE [[39](#bib.bib39), [40](#bib.bib40)] 中也表现优异。然而，分类任务和 HPE 任务之间存在差距，因为它们的目标特征和预测有所不同。与其直接使用分类任务的骨干网络，不如针对
    HPE 任务对骨干网络进行改进。例如，Hourglass [[41](#bib.bib41)]、级联金字塔网络 (CPN) [[42](#bib.bib42)]
    和 HRNet [[10](#bib.bib10)] 被提出作为 HPE 中深度学习方法的骨干网络。
- en: For prediction heads, there are mainly two representative solutions in HPE.
    The one directly predicts joint coordinates, which is regarded as the regression
    paradigm. The other one generates an intermediate heatmap representation before
    computing joint coordinates. For the regression paradigm, fully connected layers
    are often adopted to regress concrete keypoint coordinates. For the heatmap prediction
    paradigm, the operation of upsampling [[42](#bib.bib42), [41](#bib.bib41), [43](#bib.bib43)]
    is generally used to generate higher resolution heatmaps.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测头，在 HPE 中主要有两种代表性解决方案。一种是直接预测关节坐标，被视为回归范式。另一种是在计算关节坐标之前生成中间热图表示。对于回归范式，通常采用全连接层来回归具体的关键点坐标。对于热图预测范式，通常使用上采样
    [[42](#bib.bib42), [41](#bib.bib41), [43](#bib.bib43)] 来生成更高分辨率的热图。
- en: II-A2 Recurrent Neural Networks
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 循环神经网络
- en: 'Recurrent neural networks stake temporal information among sequential inputs
    into consideration. They are widely used in video-based HPE by considering videos
    as sequential RGB images. We present the general pipeline of this type of networks
    in [Figure 3](#S3.F3 "Figure 3 ‣ III-C Video-based 2D HPE ‣ III 2D Human Pose
    Estimation ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").
    RNN-based methods perform robust and the state-of-the-art accuracy [[44](#bib.bib44),
    [45](#bib.bib45)].'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '循环神经网络（RNNs）将序列输入中的时间信息纳入考虑。它们在视频基础的 HPE 中广泛应用，将视频视为序列 RGB 图像。我们在[图 3](#S3.F3
    "Figure 3 ‣ III-C Video-based 2D HPE ‣ III 2D Human Pose Estimation ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey")中展示了这种类型网络的一般流程。基于 RNN 的方法表现出强大的鲁棒性和最先进的精度
    [[44](#bib.bib44), [45](#bib.bib45)]。'
- en: II-A3 Graph Convolutional Networks
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 图卷积网络
- en: Instead of taking images as input, graph convolutional networks (GCNs) take
    graphs as input. As a human skeleton can be naturally represented as a graph,
    GCNs-based methods are prevalent in many skeleton-based tasks. In HPE, GCNs are
    generally expected to better exploit the relationship among keypoints and used
    for the pose refinement [[46](#bib.bib46)], joint association [[47](#bib.bib47)],
    2D-to-3D pose lifting [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积网络（GCNs）与其不同的是，GCNs 以图作为输入，而不是图像。由于人体骨架可以自然地表示为图，基于 GCNs 的方法在许多骨架基础的任务中很流行。在
    HPE 中，GCNs 通常被期望更好地利用关键点之间的关系，并用于姿态优化 [[46](#bib.bib46)]、关节关联 [[47](#bib.bib47)]、2D
    到 3D 姿态提升 [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)]。
- en: II-B Pose Representation
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 姿态表示
- en: Although the natural representation of human poses (keypoint positions) uses
    coordinates in the form of ordered tuples, the existing studies have shown a significant
    improvement by representing solutions with heatmaps that can be regarded as a
    confidence map. Currently, the heatmap representation has become a prevalent solution
    representation in HPE. In this subsection, we describe how heatmap representation
    works in the single-person and multi-person HPE.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人体姿态（关键点位置）的自然表示使用有序元组形式的坐标，但现有研究表明，通过将解决方案表示为可以视为置信度图的热力图取得了显著改进。目前，热力图表示已经成为
    HPE 中一种流行的解决方案表示方式。在本小节中，我们描述了热力图表示在单人和多人 HPE 中的工作原理。
- en: II-B1 Heatmaps in Single-person HPE
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 单人 HPE 中的热力图
- en: To enable neural networks for the heatmap prediction of human joints, the ground
    truth of a heatmap is essential. An intuitive way of producing the ground truth
    is to design a probability heatmap for each keypoint. For example, for single-person
    pose estimation tasks, the heatmaps of $\mathcal{K}$ keypoints can be defined
    as $\mathcal{K}$ matrices with the identical size to the input image $x$. The
    value of position $p$ (noted as $\mathcal{H}_{k}(p)$) can be generated by a 2D
    Gaussian distribution centered at the position of joint $k$ in $x$.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使神经网络能够预测人体关节的热力图，热力图的真实值是必不可少的。产生真实值的一种直观方法是为每个关键点设计一个概率热力图。例如，对于单人姿态估计任务，$\mathcal{K}$
    个关键点的热力图可以定义为与输入图像 $x$ 大小相同的 $\mathcal{K}$ 个矩阵。位置 $p$ 的值（记作 $\mathcal{H}_{k}(p)$）可以通过以关节
    $k$ 在 $x$ 中的位置为中心的 2D 高斯分布生成。
- en: '|  | $\small\mathcal{H}_{k}(p)=e^{\frac{\&#124;p-p_{k}^{*}\&#124;_{2}^{2}}{\sigma^{2}}},\forall
    k=1,2,\ldots,\mathcal{K}$ |  | (1) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathcal{H}_{k}(p)=e^{\frac{\&#124;p-p_{k}^{*}\&#124;_{2}^{2}}{\sigma^{2}}},\forall
    k=1,2,\ldots,\mathcal{K}$ |  | (1) |'
- en: 'where $p_{k}^{*}$ is the position of joint $k$ in $x$. In a heatmap, the predicted
    position (noted as $p_{pred}^{*}$) can be calculated by regression models as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{k}^{*}$ 是关节 $k$ 在 $x$ 中的位置。在热力图中，预测的位置（记作 $p_{pred}^{*}$）可以通过回归模型计算得出：
- en: '|  | $\small p_{pred}^{*}=\sum_{p\in\mathcal{P}}\mathcal{H}_{k}(p)*p,$ |  |
    (2) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small p_{pred}^{*}=\sum_{p\in\mathcal{P}}\mathcal{H}_{k}(p)*p,$ |  |
    (2) |'
- en: where $\mathcal{P}$ is the set of the possible positions of joint $k$.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{P}$ 是关节 $k$ 的可能位置集合。
- en: II-B2 Heatmaps in Multi-person HPE
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 多人 HPE 中的热力图
- en: 'There are two primary patterns of heatmaps in multi-person scenarios. One is
    to generate $\mathcal{N}\times\mathcal{K}$ heatmaps for $\mathcal{N}$ persons
    and their $\mathcal{K}$ joints separately. The other is to generate $\mathcal{K}$
    heatmaps for $\mathcal{K}$ joint of all persons. For the former pattern ($\mathcal{N}\times\mathcal{K}$
    heatmaps), each value at the position $p$ in a heatmap (noted as $\mathcal{H}_{k,n}(p)$)
    can be calculated as:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 多人场景中的热图有两种主要模式。一种是为$\mathcal{N}$个人和它们的$\mathcal{K}$个关节生成$\mathcal{N}\times\mathcal{K}$个热图。另一种是为所有人的$\mathcal{K}$个关节生成$\mathcal{K}$个热图。对于前一种模式（$\mathcal{N}\times\mathcal{K}$个热图），热图中位置$p$处的每个值（表示为$\mathcal{H}_{k,n}(p)$）可以计算如下：
- en: '|  | $\small\mathcal{H}_{k,n}(p)=e^{\frac{\&#124;p-p_{k,n}^{*}\&#124;_{2}^{2}}{\sigma^{2}}},~{}\forall
    k=1,2,\ldots,\mathcal{K},~{}\forall n=1,2,\ldots,\mathcal{N}$ |  | (3) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathcal{H}_{k,n}(p)=e^{\frac{\&#124;p-p_{k,n}^{*}\&#124;_{2}^{2}}{\sigma^{2}}},~{}\forall
    k=1,2,\ldots,\mathcal{K},~{}\forall n=1,2,\ldots,\mathcal{N}$ |  | (3) |'
- en: 'where $\mathcal{K}$ and $\mathcal{N}$ are the numbers of joints and persons,
    respectively. For the latter pattern, a general method to produce the ground truth
    heatmaps is aggregating $\mathcal{N}$ single-person heatmaps into one heatmap
    by using a $\max$ operator:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{K}$和$\mathcal{N}$分别是关节和人数的数量。对于后一种情况，生成地面真值热图的一般方法是通过使用$\max$运算符将$\mathcal{N}$个单人热图聚合成一个热图：
- en: '|  | $\small\mathcal{H}_{k}(p)=\max_{n}\mathcal{H}_{k,n}(p),\forall n=1,2,\ldots,\mathcal{N}$
    |  | (4) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathcal{H}_{k}(p)=\max_{n}\mathcal{H}_{k,n}(p),\forall n=1,2,\ldots,\mathcal{N}$
    |  | (4) |'
- en: 'In summary, [Equation 3](#S2.E3 "3 ‣ II-B2 Heatmaps in Multi-person HPE ‣ II-B
    Pose Representation ‣ II Preliminary Knowledge ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey") and [Equation 4](#S2.E4 "4 ‣ II-B2 Heatmaps in Multi-person
    HPE ‣ II-B Pose Representation ‣ II Preliminary Knowledge ‣ Vision-based Human
    Pose Estimation via Deep Learning: A Survey") are the two main methods to calculate
    the heatmaps in multi-person HPE.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '总结一下，[方程3](#S2.E3 "3 ‣ II-B2 Multi-person HPE 中的热图 ‣ II-B 姿势表示 ‣ II 初步知识 ‣
    基于深度学习的基于视觉的人体姿态估计: 一项调查")和[方程4](#S2.E4 "4 ‣ II-B2 Multi-person HPE 中的热图 ‣ II-B
    姿势表示 ‣ II 初步知识 ‣ 基于深度学习的基于视觉的人体姿态估计: 一项调查")是计算多人 HPE 中热图的两种主要方法。'
- en: II-C Datasets and Metrics
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 数据集和评估指标
- en: Datasets are critical for training and evaluating neural networks for deep learning-based
    HPE. In this subsection, we introduce the popular datasets and their applicable
    tasks, then review the evaluation metrics for image-based 2D HPE, video-based
    2D HPE, and 3D HPE.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集对于基于深度学习的 HPE 的神经网络的训练和评估至关重要。在本小节中，我们介绍流行的数据集及其适用任务，然后回顾针对基于图像的 2D HPE、基于视频的
    2D HPE 和 3D HPE 的评估指标。
- en: II-C1 Datasets in 2D HPE
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 2D HPE 中的数据集
- en: 'Many datasets have been proposed to evaluate the performance of 2D HPE approaches.
    Here, we introduce the prevalent datasets used in 2D HPE. Early datasets like
    LSP [[26](#bib.bib26)], FLIC [[27](#bib.bib27)], Penn Action [[29](#bib.bib29)],
    and J-HMDB [[28](#bib.bib28)] mainly focus on single-person scenes with relatively
    small scales. The recent datasets such as COCO [[31](#bib.bib31)], MPII [[30](#bib.bib30)],
    CrowdPose [[32](#bib.bib32)], and PoseTrack [[16](#bib.bib16)] are used for multi-person
    HPE with larger-scale data. We summarize and provide the links of the prevalent
    datasets as well as their scales and the evaluation metrics in [Table I](#S2.T1
    "TABLE I ‣ II Preliminary Knowledge ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '许多数据集已经被提出来评估 2D HPE 方法的性能。在这里，我们介绍了在 2D HPE 中使用的流行数据集。早期的数据集，如 LSP [[26](#bib.bib26)]，FLIC
    [[27](#bib.bib27)]，Penn Action [[29](#bib.bib29)] 和 J-HMDB [[28](#bib.bib28)]
    主要关注相对较小规模的单人场景。最近的数据集，如 COCO [[31](#bib.bib31)]，MPII [[30](#bib.bib30)]，CrowdPose
    [[32](#bib.bib32)] 和 PoseTrack [[16](#bib.bib16)] 用于具有较大规模数据的多人 HPE。我们总结并提供了流行数据集的链接以及它们的规模和评估指标，如[表I](#S2.T1
    "TABLE I ‣ II 初步知识 ‣ 基于深度学习的基于视觉的人体姿态估计: 一项调查")所示。'
- en: II-C2 Datasets in 3D HPE
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 3D HPE 中的数据集
- en: 'Unlike the datasets in 2D HPE, acquiring accurate 3D annotations for human
    joints in 3D HPE often requires a motion capture system that is generally hard
    to be installed in the outdoor environment. Most 3D HPE datasets are created in
    the indoor environments or simulation, such as CMU Panoptic [[34](#bib.bib34)],
    3DPW [[35](#bib.bib35)], MPI-INF-3DHP [[36](#bib.bib36)], JTA [[37](#bib.bib37)].
    Here we introduce the well-known datasets in 3D HPE and summarize their characteristics
    in [Table I](#S2.T1 "TABLE I ‣ II Preliminary Knowledge ‣ Vision-based Human Pose
    Estimation via Deep Learning: A Survey").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '与 2D HPE 数据集不同，获取 3D HPE 中人类关节的准确 3D 注释通常需要一个运动捕捉系统，这在户外环境中一般很难安装。大多数 3D HPE
    数据集都是在室内环境或仿真中创建的，例如 CMU Panoptic [[34](#bib.bib34)]、3DPW [[35](#bib.bib35)]、MPI-INF-3DHP
    [[36](#bib.bib36)] 和 JTA [[37](#bib.bib37)]。在这里，我们介绍了 3D HPE 中的一些著名数据集，并在[表 I](#S2.T1
    "TABLE I ‣ II Preliminary Knowledge ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey")中总结了它们的特点。'
- en: II-C3 Metrics in 2D HPE
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 2D HPE 中的指标
- en: 'In scientific research, we usually need metrics to evaluate how well a method
    performs. Here we introduce two common metrics used in HPE. *Percentage of Correct
    Keypoints (PCK)* literally indicates the percentage of correct detected keypoints,
    which can be noted as $\mathcal{PCK}=k/\mathcal{N}$, where k is the number of
    correct predicted keypoints, $\mathcal{N}$ is the total number of keypoints. This
    metric is generally applied to the studies with the LSP dataset, MPII dataset,
    and FLIC dataset in the early 2D HPE methods (see [Table II](#S2.T2 "TABLE II
    ‣ II-C3 Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey")).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '在科学研究中，我们通常需要一些指标来评估方法的表现。这里我们介绍两种常见的 HPE 指标。*关键点正确率 (PCK)* 字面上表示正确检测到的关键点的百分比，可以记作
    $\mathcal{PCK}=k/\mathcal{N}$，其中 k 是正确预测的关键点数量，$\mathcal{N}$ 是关键点的总数。这个指标通常应用于使用
    LSP 数据集、MPII 数据集和 FLIC 数据集的早期 2D HPE 方法（见[表 II](#S2.T2 "TABLE II ‣ II-C3 Metrics
    in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey")）。'
- en: '| Studies | Years | Backbone | Input Size | Highlights | PCKh@0.5 | #Params
    | GFlops |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 研究 | 年份 | 主干网络 | 输入尺寸 | 亮点 | PCKh@0.5 | 参数数量 | GFlops |'
- en: '| Toshev & Szegedy [[9](#bib.bib9)] | 2014 | AlexNet | $256\times 256$ | Original
    deep learning-based method for HPE | - | - | - |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Toshev & Szegedy [[9](#bib.bib9)] | 2014 | AlexNet | $256\times 256$ | 原始基于深度学习的
    HPE 方法 | - | - | - |'
- en: '| Tompson et al. [[51](#bib.bib51)] | 2014 | AlexNet | $320\times 240$ | Utilization
    of heatmaps for solution representation | 82.0 | - | - |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Tompson 等人 [[51](#bib.bib51)] | 2014 | AlexNet | $320\times 240$ | 热力图在解决方案表示中的应用
    | 82.0 | - | - |'
- en: '| Wei et al. [[52](#bib.bib52)] | 2016 | CPM | $368\times 368$ | A convolutional
    pose machine | 88.5 | 31.23M | 85.0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Wei 等人 [[52](#bib.bib52)] | 2016 | CPM | $368\times 368$ | 卷积姿态机器 | 88.5
    | 31.23M | 85.0 |'
- en: '| Newell et al. [[41](#bib.bib41)] | 2016 | Hourglass | $256\times 256$ | Stacked
    Hourglass Modules | 90.9 | 23.7M | 41.2 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Newell 等人 [[41](#bib.bib41)] | 2016 | Hourglass | $256\times 256$ | 堆叠 Hourglass
    模块 | 90.9 | 23.7M | 41.2 |'
- en: '| Xiao et al. [[43](#bib.bib43)] | 2018 | ResNet | $256\times 256$ | A simple
    yet effective architecture for HPE | 90.2 | 68.64 M | 17.02 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Xiao 等人 [[43](#bib.bib43)] | 2018 | ResNet | $256\times 256$ | 一种简单而有效的 HPE
    架构 | 90.2 | 68.64 M | 17.02 |'
- en: '| Chu et al. [[53](#bib.bib53)] | 2017 | Hourglass | $256\times 256$ | Attention
    mechanism in contextual representations | 91.5 | 58.1M | - |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Chu 等人 [[53](#bib.bib53)] | 2017 | Hourglass | $256\times 256$ | 上下文表示中的注意力机制
    | 91.5 | 58.1M | - |'
- en: '| Yang et al. [[54](#bib.bib54)] | 2017 | Hourglass | $256\times 256$ | Pyramid
    Residual Module | 92.0 | 26.9M | 45.9 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等人 [[54](#bib.bib54)] | 2017 | Hourglass | $256\times 256$ | 金字塔残差模块
    | 92.0 | 26.9M | 45.9 |'
- en: '| Sun et al. [[10](#bib.bib10)] | 2019 | HRNet | $256\times 256$ | HRnet for
    high-resolution representations | 92.3 | 28.54M | 10.27 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Sun 等人 [[10](#bib.bib10)] | 2019 | HRNet | $256\times 256$ | HRNet 用于高分辨率表示
    | 92.3 | 28.54M | 10.27 |'
- en: '| Bulat et al. [[55](#bib.bib55)] | 2020 | Hourglass+UNet | $256\times 256$
    | A hybrid structure by combining [[41](#bib.bib41)] and U-Net [[56](#bib.bib56)]
    | 94.1 | 8.5M | 9.9 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Bulat 等人 [[55](#bib.bib55)] | 2020 | Hourglass+UNet | $256\times 256$ | 结合
    [[41](#bib.bib41)] 和 U-Net [[56](#bib.bib56)] 的混合结构 | 94.1 | 8.5M | 9.9 |'
- en: 'TABLE II: The state-of-the-art approaches for 2D single-person pose estimation.
    The PCKh@0.5 scores are shown in the last column and defined in [subsubsection II-C3](#S2.SS3.SSS3
    "II-C3 Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"). They are obtained
    by testing the methods on the MPII dataset.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II：2D单人姿态估计的最新方法。PCKh@0.5 分数显示在最后一列，并定义在 [子小节 II-C3](#S2.SS3.SSS3 "II-C3
    Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey")。这些分数是通过在MPII数据集上测试方法得到的。'
- en: '*Object Keypoint Similarity (OKS)* was proposed originally in the COCO competition
    [[31](#bib.bib31)] as a variable to compute mean Average Precision (mAP). It can
    be calculated as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*对象关键点相似性（OKS）* 最初在COCO竞赛中提出 [[31](#bib.bib31)] 作为计算平均精度（mAP）的变量。它的计算公式如下：'
- en: '|  | $\small\mathcal{OKS}=\frac{\sum_{i}(-d_{i}^{2}/2s^{2}k_{i}^{2})\delta_{(v_{i}>0)}}{\sum_{i}\delta_{(v_{i}>0)}},\quad
    s.t.~{}v_{i}\in\{0,1,2\}$ |  | (5) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathcal{OKS}=\frac{\sum_{i}(-d_{i}^{2}/2s^{2}k_{i}^{2})\delta_{(v_{i}>0)}}{\sum_{i}\delta_{(v_{i}>0)}},\quad
    s.t.~{}v_{i}\in\{0,1,2\}$ |  | (5) |'
- en: 'where $i$ is a joint index, $d_{i}$ is the distance between the predicted joint
    and ground truth, $s$ (object scale) and $k_{i}$ are the keypoint constants given
    by the COCO dataset, $\delta=1$ when $v_{i}>0$, otherwise $\delta=1$, and $v_{i}$
    is the visibility flag ($v_{i}=0$: not labelled, $v_{i}=1$: labelled but not visible,
    and $v_{i}=2$: labelled and visible) of the ground truth. The average precision
    (AP) can be calculated with the $\mathcal{OKS}$ value by:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $i$ 是一个关节索引，$d_{i}$ 是预测关节与真实关节之间的距离，$s$（对象尺度）和 $k_{i}$ 是由COCO数据集给出的关键点常量，$\delta=1$
    当 $v_{i}>0$，否则 $\delta=1$，$v_{i}$ 是真实值的可见性标志（$v_{i}=0$：未标记，$v_{i}=1$：标记但不可见，$v_{i}=2$：标记且可见）。平均精度（AP）可以通过
    $\mathcal{OKS}$ 值计算：
- en: '|  | $\small AP=\frac{\mathcal{TP}_{(\mathcal{OKS}>td)}}{\mathcal{TP}_{(\mathcal{OKS}>td)}+\mathcal{FP}_{(\mathcal{OKS}\leq
    td)}}$ |  | (6) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small AP=\frac{\mathcal{TP}_{(\mathcal{OKS}>td)}}{\mathcal{TP}_{(\mathcal{OKS}>td)}+\mathcal{FP}_{(\mathcal{OKS}\leq
    td)}}$ |  | (6) |'
- en: 'where $\mathcal{TP}$ and $\mathcal{FP}$ are the numbers of true positive and
    false positive respectively, $td$ is a $\mathcal{OKS}$ threshold. The mean AP
    (mAP) is the mean of AP over ten $\mathcal{OKS}$ thresholds at (0.50, 0.55, .
    . . , 0.90, 0.95) [[31](#bib.bib31)], which is a common metric to evaluate 2D
    multi-person pose estimation (see [Table III](#S3.T3 "TABLE III ‣ III-B Image-based
    2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey")).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathcal{TP}$ 和 $\mathcal{FP}$ 分别是真正例和假正例的数量，$td$ 是 $\mathcal{OKS}$ 阈值。平均AP（mAP）是十个
    $\mathcal{OKS}$ 阈值（0.50, 0.55, . . . , 0.90, 0.95） [[31](#bib.bib31)] 的平均值，这是评估2D多人姿态估计的常用度量（参见
    [表 III](#S3.T3 "TABLE III ‣ III-B Image-based 2D Multi-Person HPE ‣ III 2D Human
    Pose Estimation ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey")）。'
- en: II-C4 Metrics in 3D HPE
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C4 3D HPE的度量
- en: 'Generally, the metrics of PCK can be extended to evaluate 3D HPE. However,
    *Mean Per Joint Position Error (MPJPE)* is currently a popular metric in 3D HPE
    (see [Table IV](#S4.T4 "TABLE IV ‣ IV 3D Human Pose Estimation ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey")). MPJPE calculates the euclidean
    distance between the predicted joint coordinates and ground truth joint coordinates,
    which can be formulated as:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '一般来说，PCK的度量可以扩展到评估3D HPE。然而，*平均关节位置误差（MPJPE）* 目前在3D HPE中是一个流行的度量（参见 [表 IV](#S4.T4
    "TABLE IV ‣ IV 3D Human Pose Estimation ‣ Vision-based Human Pose Estimation via
    Deep Learning: A Survey")）。MPJPE计算预测关节坐标与真实关节坐标之间的欧几里得距离，可以表述为：'
- en: '|  | $\small\text{MPJPE}=\frac{1}{\mathcal{T}\cdot\mathcal{N}}\sum\limits_{t=1}^{\mathcal{T}}\sum\limits_{i=1}^{\mathcal{N}}\left\lVert
    J_{i}^{t}-J_{root}^{t}-(\hat{J_{i}}^{t}-\hat{J}_{root}^{t})\right\rVert^{2}$ |  |
    (7) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\text{MPJPE}=\frac{1}{\mathcal{T}\cdot\mathcal{N}}\sum\limits_{t=1}^{\mathcal{T}}\sum\limits_{i=1}^{\mathcal{N}}\left\lVert
    J_{i}^{t}-J_{root}^{t}-(\hat{J_{i}}^{t}-\hat{J}_{root}^{t})\right\rVert^{2}$ |  |
    (7) |'
- en: Where $i$ and $t$ are the indexes of a joint and a sample respectively; $J_{i}$
    and $\hat{J_{i}}$ refer to the predicted coordinate and ground-truth coordinate
    of $i-th$ joint. $J_{root}$ refers to the coordinate of the root joint, which
    is usually predefined as the human pelvis.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $i$ 和 $t$ 分别是关节和样本的索引；$J_{i}$ 和 $\hat{J_{i}}$ 指的是 $i$-th 关节的预测坐标和真实坐标。$J_{root}$
    指的是根关节的坐标，通常预定义为人的骨盆。
- en: III 2D Human Pose Estimation
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 2D 人体姿态估计
- en: 'The image-based 2D HPE is to estimate human joint positions in images. Early
    approaches mainly use model-based methods. Currently, deep learning-based methods
    have shown superior performance in 2D HPE [[9](#bib.bib9)]. In this section, we
    introduce the deep learning-based 2D HPE approaches from three aspects: image-based
    single-person pose estimation (SPPE), image-based multi-person pose estimation
    (MPPE), and video-based 2D HPE. Finally, we summarize the open-source codes of
    the state-of-the-art 2D HPE in [Table VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets
    ‣ VI-B1 Challenges in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research Trends
    and Challenges ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '基于图像的 2D HPE 旨在估计图像中人体关节点的位置。早期的方法主要使用基于模型的方法。目前，基于深度学习的方法在 2D HPE 中表现出优越的性能[[9](#bib.bib9)]。在本节中，我们从三个方面介绍基于深度学习的
    2D HPE 方法：基于图像的单人姿态估计（SPPE）、基于图像的多人姿态估计（MPPE）和基于视频的 2D HPE。最后，我们在[Table VI](#S6.T6
    "TABLE VI ‣ Incomprehensive Datasets ‣ VI-B1 Challenges in Accurate HPE ‣ VI-B
    Research Challenges ‣ VI Research Trends and Challenges ‣ Vision-based Human Pose
    Estimation via Deep Learning: A Survey")中总结了最先进的 2D HPE 的开源代码。'
- en: III-A Image-based 2D Single-Person HPE
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于图像的 2D 单人姿态估计
- en: The SPPE task is to estimate the pose of a single person in an image. It is
    a fundamental task in HPE and is often used as a basic component of other HPE
    tasks. For example, the well-known study [[9](#bib.bib9)] proposed a cascaded
    multi-stage neural network to predict and refine human poses, in which images
    are cropped to ensure a single person in each image. To our knowledge, it is the
    original work that applies deep neural networks (DNN) for the HPE.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SPPE 任务是估计图像中单个人的姿态。这是 HPE 中的一个基础任务，通常作为其他 HPE 任务的基本组成部分。例如，著名的研究[[9](#bib.bib9)]
    提出了一个级联多阶段神经网络来预测和优化人体姿态，其中图像被裁剪以确保每张图像中只有一个人。据我们所知，这是将深度神经网络（DNN）应用于 HPE 的原始工作。
- en: 'Recent DNN-based approaches dramatically improve the accuracy of SPPE. Many
    generic neural networks with efficient architecture show remarkable performance
    in various applications [[13](#bib.bib13), [38](#bib.bib38)]. There are mainly
    two ways to improve SPPE performance: improving the solution representation and
    designing advanced neural networks. In [[51](#bib.bib51), [57](#bib.bib57), [58](#bib.bib58)],
    the methods with solution representation significantly improved the SPPE performance.
    In addition, well-designed neural architectures [[10](#bib.bib10), [52](#bib.bib52)]
    also performed remarkable performance in HPE. We summarize the state-of-the-art
    approaches for 2D single-person pose estimation in [Table II](#S2.T2 "TABLE II
    ‣ II-C3 Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '最近基于 DNN 的方法显著提高了 SPPE 的准确性。许多具有高效架构的通用神经网络在各种应用中表现出色[[13](#bib.bib13), [38](#bib.bib38)]。提高
    SPPE 性能主要有两种方法：改进解决方案表示和设计先进的神经网络。在[[51](#bib.bib51), [57](#bib.bib57), [58](#bib.bib58)]中，具有解决方案表示的方法显著提高了
    SPPE 性能。此外，设计良好的神经网络架构[[10](#bib.bib10), [52](#bib.bib52)] 在 HPE 中也表现出色。我们在[Table II](#S2.T2
    "TABLE II ‣ II-C3 Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary
    Knowledge ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey")中总结了
    2D 单人姿态估计的最先进方法。'
- en: III-A1 Solution Representation
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 解决方案表示
- en: 'In general, there are mainly two types of solution representations. One is
    to represent joint positions by using coordinates. The other one is to represent
    joint positions in the form of probability distributions, i.e., heatmaps as represented
    in [Equation 1](#S2.E1 "1 ‣ II-B1 Heatmaps in Single-person HPE ‣ II-B Pose Representation
    ‣ II Preliminary Knowledge ‣ Vision-based Human Pose Estimation via Deep Learning:
    A Survey"). Early studies mainly used a DNN-based regression from the input images
    to the estimated coordinates directly. DeepPose [[9](#bib.bib9)] is an early classic
    study that applied DNNs to represent coordinate regression from RGB images and
    multi-stage refinement for the SPPE tasks.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '一般而言，主要有两种类型的解决方案表示方式。一种是使用坐标表示关节点的位置。另一种是以概率分布的形式表示关节点的位置，即[Equation 1](#S2.E1
    "1 ‣ II-B1 Heatmaps in Single-person HPE ‣ II-B Pose Representation ‣ II Preliminary
    Knowledge ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey")中表示的热图。早期的研究主要使用基于
    DNN 的回归方法从输入图像直接估计坐标。DeepPose[[9](#bib.bib9)] 是一个早期的经典研究，应用 DNN 进行坐标回归表示，从 RGB
    图像中进行多阶段的 SPPE 任务优化。'
- en: Solution representation of heatmaps significantly improves the performance of
    DNN-based methods in HPE [[56](#bib.bib56)]. The methods of heatmap generation
    have been widely used in HPE tasks since heatmaps were proposed by Tompson et
    al. [[51](#bib.bib51)] and Jain et al. [[57](#bib.bib57)]. Moreover, Zhang et
    al. [[58](#bib.bib58)] improved the heatmap representation by the distribution-aware
    coordinate representation of keypoints (DARK).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 热图的解决方案表示显著提高了基于 DNN 的 HPE 方法的性能 [[56](#bib.bib56)]。自从 Tompson 等人 [[51](#bib.bib51)]
    和 Jain 等人 [[57](#bib.bib57)] 提出热图以来，热图生成的方法在 HPE 任务中被广泛使用。此外，Zhang 等人 [[58](#bib.bib58)]
    通过关键点的分布感知坐标表示（DARK）改进了热图表示。
- en: III-A2 Neural Network Design
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 神经网络设计
- en: How to design the architecture of neural networks in HPE is a crucial topic.
    A generic way is to use the existing well-known neural networks (e.g., AlexNet
    [[13](#bib.bib13)], ResNet [[38](#bib.bib38)]) that have already shown superior
    performance in other computer vision tasks. For example, Toshev et al. [[9](#bib.bib9)]
    used an AlexNet-like to regress the coordinates of human joints from images. Xiao
    et al. [[43](#bib.bib43)] used ResNet as the backbone of neural networks for HPE.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如何设计 HPE 中的神经网络架构是一个关键话题。一种通用的方法是使用现有的知名神经网络（例如，AlexNet [[13](#bib.bib13)]，ResNet
    [[38](#bib.bib38)]），这些网络在其他计算机视觉任务中已经表现出优越的性能。例如，Toshev 等人 [[9](#bib.bib9)] 使用类似于
    AlexNet 的网络从图像中回归人体关节的坐标。Xiao 等人 [[43](#bib.bib43)] 使用 ResNet 作为 HPE 神经网络的骨干网络。
- en: The latest neural network, the transformer that adopts the attention mechanism,
    has been applied in HPE with superior performance. In general, transformers process
    the features extracted by CNNs and leverage the attention mechanism to automatically
    capture long-range relationships among features. In 2021, Mao et al.[[59](#bib.bib59)]
    used ResNet to extract features from images and a Transformer model to predict
    keypoint positions. Li et al.[[60](#bib.bib60)] utilized vision transformers to
    implement regression-based HPE, where two cascaded transformers are applied to
    predict the bounding boxes of the person as well as their keypoints positions.
    Yang et al. [[61](#bib.bib61)] proposed a new network model, TransPose which introduces
    the transformer for human pose estimation. The attention layers built-in transformers
    enable TransPose to capture long-range relationships efficiently. In addition,
    Xu et al. proposed a scalable HPE backbone, VitPose [[62](#bib.bib62)] that employs
    plain and non-hierarchical vision transformers as backbones to extract features
    for a given person instance and a lightweight decoder for pose estimation. To
    date, the existing survey papers have not discussed the studies with transformers
    in HPE.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的神经网络——采用注意力机制的 transformer，已在 HPE 中表现出卓越的性能。一般来说，transformer 处理 CNN 提取的特征，并利用注意力机制自动捕捉特征之间的长距离关系。在
    2021 年，Mao 等人 [[59](#bib.bib59)] 使用 ResNet 从图像中提取特征，并利用 Transformer 模型预测关键点位置。Li
    等人 [[60](#bib.bib60)] 利用视觉 transformer 实现基于回归的 HPE，其中两个级联的 transformer 被应用于预测人的边界框及其关键点位置。Yang
    等人 [[61](#bib.bib61)] 提出了一个新的网络模型 TransPose，该模型引入 transformer 进行人体姿态估计。内置的 transformer
    注意力层使 TransPose 能够有效捕捉长距离关系。此外，Xu 等人提出了一种可扩展的 HPE 骨干网络 VitPose [[62](#bib.bib62)]，该网络使用简单的非层次化视觉
    transformer 作为骨干网络来提取给定人物实例的特征，并使用轻量级解码器进行姿态估计。迄今为止，现有的综述论文尚未讨论 transformer 在
    HPE 中的研究。
- en: Another generic way is to design specific networks for HPE tasks. For example,
    Newell et al. [[41](#bib.bib41)] designed a stacked hourglass network as the backbone
    for exacting multi-scale features automatically. Hourglass Networks are used in
    many HPE studies as backbone networks. Newell et al. [[63](#bib.bib63)] used the
    Hourglass Network backbone to estimate multi-person poses. BlazePose [[64](#bib.bib64)]
    utilized an Hourglass-based encoder network to accomplish real-time single-person
    pose estimation driven by the MediaPipe framework [[65](#bib.bib65)], which achieved
    a speed of 30 frames per second on the Google Pixel 2 mobile phone. Bulat et al.
    [[55](#bib.bib55)] designed a hybrid network structure with a combination of the
    hourglass network and U-Net [[56](#bib.bib56)]. Chen et al. [[42](#bib.bib42)]
    presented another remarkable study that a cascaded pyramid network is proposed
    to concatenate features among different scales. In addition, many studies enhanced
    the model performance by slightly modifying the units of a backbone. Specifically,
    Chu et al. [[53](#bib.bib53)] incorporated the attention mechanism into the hourglass
    network. Yang et al. [[54](#bib.bib54)] proposed a pyramid residual module to
    replace the residual module in the hourglass network.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种通用的方法是为姿态估计任务设计特定的网络。例如，Newell等人[[41](#bib.bib41)]设计了一个堆叠的沙漏网络作为提取多尺度特征的骨干网络。沙漏网络在许多姿态估计研究中被用作骨干网络。Newell等人[[63](#bib.bib63)]利用沙漏网络骨干来估计多人的姿态。BlazePose[[64](#bib.bib64)]利用基于沙漏的编码器网络，通过MediaPipe框架[[65](#bib.bib65)]实现了实时单人姿态估计，该框架在Google
    Pixel 2手机上达到了每秒30帧的速度。Bulat等人[[55](#bib.bib55)]设计了一种混合网络结构，将沙漏网络与U-Net[[56](#bib.bib56)]结合。Chen等人[[42](#bib.bib42)]提出了另一项重要研究，即提出了一个级联金字塔网络，用于连接不同尺度的特征。此外，许多研究通过稍微修改骨干网络的单元来增强模型性能。具体来说，Chu等人[[53](#bib.bib53)]将注意力机制融入沙漏网络。Yang等人[[54](#bib.bib54)]提出了一个金字塔残差模块，以替代沙漏网络中的残差模块。
- en: III-B Image-based 2D Multi-Person HPE
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于图像的2D多人姿态估计
- en: In general, MPPE is a more challenging task than SPPE because of the high complexity
    in terms of solution space and mutual occlusions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，多人姿态估计（MPPE）比单人姿态估计（SPPE）更具挑战性，因为它在解决空间和相互遮挡方面的复杂性更高。
- en: '![Refer to caption](img/f2394166eae3e349a0eea7b9e4797ae3.png)  ![Refer to caption](img/96b54aac34ebb8fffa34bedb1c856963.png)  ![Refer
    to caption](img/eaf6b5dc55850fdea393a22a147167b9.png)  Input Image Detected Bounding
    Boxes Human Poses  ![Refer to caption](img/f2394166eae3e349a0eea7b9e4797ae3.png)  ![Refer
    to caption](img/e242ccbf4d6ca37e6920fe58d0b3f22e.png)  ![Refer to caption](img/f34b11becff0039b6b3cf8139e84e563.png)  Input
    Image Detected Keypoints Human Poses'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/f2394166eae3e349a0eea7b9e4797ae3.png)  ![参见说明](img/96b54aac34ebb8fffa34bedb1c856963.png)  ![参见说明](img/eaf6b5dc55850fdea393a22a147167b9.png)  输入图像
    检测到的边界框 人体姿态  ![参见说明](img/f2394166eae3e349a0eea7b9e4797ae3.png)  ![参见说明](img/e242ccbf4d6ca37e6920fe58d0b3f22e.png)  ![参见说明](img/f34b11becff0039b6b3cf8139e84e563.png)  输入图像
    检测到的关键点 人体姿态'
- en: 'Figure 2: Illustration of the top-down and bottom-up framework. The first row
    shows a representative top-down framework with two stages: proposing a bounding
    box for each person and estimating human poses. The second row shows a representative
    bottom-up framework with two stages: detecting the joints of persons and grouping
    the joints into an associated person. The original images are from COCO dataset
    [[31](#bib.bib31)].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：自上而下和自下而下框架的示意图。第一行展示了一个具有两个阶段的代表性自上而下框架：为每个人提出一个边界框并估计人体姿态。第二行展示了一个具有两个阶段的代表性自下而下框架：检测个体的关节并将关节分组到一个关联的个体中。原始图像来自COCO数据集[[31](#bib.bib31)]。
- en: 'The current solutions of MPPE generally fall into either top-down approaches
    or bottom-up approaches:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的多人姿态估计（MPPE）解决方案通常分为自上而下的方法或自下而上的方法：
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Top-down approaches apply a person detector to detect all single-persons, followed
    by estimating the joints of each single-person and calculating each single-person
    pose separately.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自上而下的方法应用一个人检测器来检测所有单个人，随后估计每个单人的关节，并分别计算每个单人的姿态。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Bottom-up approaches detect all joints in an image, followed by associating/grouping
    the joints into an associated person.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自下而上的方法首先检测图像中的所有关节，然后将这些关节关联/分组到一个关联的个体中。
- en: 'In this subsection, we address the top-down and bottom-up approaches and discuss
    their merits and flaws. We illustrate the frameworks of both approaches in [Figure 2](#S3.F2
    "Figure 2 ‣ III-B Image-based 2D Multi-Person HPE ‣ III 2D Human Pose Estimation
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey") and summarize
    the recent well-known MPPE approaches in [Table III](#S3.T3 "TABLE III ‣ III-B
    Image-based 2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节中，我们探讨了自上而下和自下而上的方法，并讨论了它们的优缺点。我们在[图2](#S3.F2 "Figure 2 ‣ III-B Image-based
    2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey")中展示了这两种方法的框架，并在[表III](#S3.T3 "TABLE III ‣ III-B Image-based
    2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey")中总结了最近知名的MPPE方法。'
- en: '| Methods | Studies/Years | Input Size | Backbone | Detector/ Grouping | Highlights
    | mAP | #Params | GFlops |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 研究/年份 | 输入尺寸 | 骨干 | 检测器/分组 | 亮点 | mAP | 参数数量 | GFlops |'
- en: '| Top-down | [[66](#bib.bib66)] / 2017 | - | ResNet | Mask R-CNN | Detecting
    persons and keypoints by mask R-CNN | 63.1 | - | - |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下 | [[66](#bib.bib66)] / 2017 | - | ResNet | Mask R-CNN | 通过Mask R-CNN检测人员和关键点
    | 63.1 | - | - |'
- en: '| [[39](#bib.bib39)] / 2017 | $320\times 256$ | PyraNet | Faster R-CNN | Addressing
    inaccurate bounding box of persons | 72.3 | 28.1M | 26.7 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| [[39](#bib.bib39)] / 2017 | $320\times 256$ | PyraNet | Faster R-CNN | 处理不准确的边界框
    | 72.3 | 28.1M | 26.7 |'
- en: '| [[42](#bib.bib42)] / 2018 | $384\times 288$ | CPN | FPN | A two-stage cascaded
    network to refine keypoints | 72.1 | 27.0M | 6.2 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| [[42](#bib.bib42)] / 2018 | $384\times 288$ | CPN | FPN | 两阶段级联网络以优化关键点 |
    72.1 | 27.0M | 6.2 |'
- en: '| [[43](#bib.bib43)] / 2018 | $384\times 288$ | ResNet | Faster R-CNN | A remarkable
    simple baseline | 72.2 | - | - |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bib43)] / 2018 | $384\times 288$ | ResNet | Faster R-CNN | 一个非常简单的基线
    | 72.2 | - | - |'
- en: '| [[10](#bib.bib10)] / 2019 | $384\times 288$ | HRNet | Faster R-CNN | HRnet
    for high-resolution representations | 75.5 | 63.6M | 32.9 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| [[10](#bib.bib10)] / 2019 | $384\times 288$ | HRNet | Faster R-CNN | 高分辨率表示的HRnet
    | 75.5 | 63.6M | 32.9 |'
- en: '| [[67](#bib.bib67)] / 2019 | $384\times 288$ | MSPN | MegDet | A multi-stage
    network | 76.1 | 120M | 19.9 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| [[67](#bib.bib67)] / 2019 | $384\times 288$ | MSPN | MegDet | 一个多阶段网络 | 76.1
    | 120M | 19.9 |'
- en: '| [[17](#bib.bib17)] / 2020 | $512\times 384$ | EvoPose2D | Faster R-CNN |
    Neural Architecture Search | 76.8 | 14.7M | 17.7 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [[17](#bib.bib17)] / 2020 | $512\times 384$ | EvoPose2D | Faster R-CNN |
    神经架构搜索 | 76.8 | 14.7M | 17.7 |'
- en: '| [[46](#bib.bib46)] / 2020 | $384\times 384$ | ResNet | FPN | GCN-based keypoint
    refinement | 72.9 | 25.2M | 12.9 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| [[46](#bib.bib46)] / 2020 | $384\times 384$ | ResNet | FPN | 基于GCN的关键点优化
    | 72.9 | 25.2M | 12.9 |'
- en: '| [[59](#bib.bib59)] / 2021 | $384\times 288$ | ResNet+ Transformer | Faster
    R-CNN | Direct Coordinate Regression via Transformer | 72.2 | - | 20.4G |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] / 2021 | $384\times 288$ | ResNet+ Transformer | Faster
    R-CNN | 通过变换器进行直接坐标回归 | 72.2 | - | 20.4G |'
- en: '| [[61](#bib.bib61)] / 2021 | $256\times 192$ | TransPose | Faster R-CNN |
    Transformer decoders with CNNs-based extractor | 75.8 | 17.5M | 21.8G |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bib61)] / 2021 | $256\times 192$ | TransPose | Faster R-CNN |
    基于CNN的变换器解码器 | 75.8 | 17.5M | 21.8G |'
- en: '| [[62](#bib.bib62)] / 2022 | $256\times 192$ | VitPose | Faster R-CNN | Vision
    Transformer Backbone Baseline | 79.8 | 632M | - |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| [[62](#bib.bib62)] / 2022 | $256\times 192$ | VitPose | Faster R-CNN | 视觉变换器骨干基线
    | 79.8 | 632M | - |'
- en: '| Bottom-up | [[68](#bib.bib68)] / 2016 | $256\times 256$ | ResNet | Integer
    Programming | ResNet-based detectors and image-conditioned pairwise terms | -
    | - | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上 | [[68](#bib.bib68)] / 2016 | $256\times 256$ | ResNet | 整数规划 | 基于ResNet的检测器和图像条件配对项
    | - | - | - |'
- en: '| [[40](#bib.bib40)] / 2017 | $256\times 256$ | VGG+CPM | Part Affinity Fields
    | Grouping by body association and Hungarian algo. | 61.8 | 25.94M | 160 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| [[40](#bib.bib40)] / 2017 | $256\times 256$ | VGG+CPM | Part Affinity Fields
    | 基于身体关联和匈牙利算法的分组 | 61.8 | 25.94M | 160 |'
- en: '| [[63](#bib.bib63)] / 2017 | $512\times 512$ | Hourglass | Associative embedding
    | pixel-wise joint embedding for grouping | 66.3 | 277.8M | 206.9 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [[63](#bib.bib63)] / 2017 | $512\times 512$ | Hourglass | 关联嵌入 | 像素级关节嵌入以进行分组
    | 66.3 | 277.8M | 206.9 |'
- en: '| [[69](#bib.bib69)] / 2019 | $384\times 384$ | Hourglass | $\emptyset$ | Predicting
    root and joints position | 66.9 | - | - |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)] / 2019 | $384\times 384$ | Hourglass | $\emptyset$ | 预测根部和关节位置
    | 66.9 | - | - |'
- en: '| [[47](#bib.bib47)] / 2020 | $641\times 641$ | ResNet | DGCN | Graph convolutional
    network for grouping | 68.8 | 234M | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[47](#bib.bib47)] / 2020 | $641\times 641$ | ResNet | DGCN | 用于分组的图卷积网络
    | 68.8 | 234M | - |'
- en: '| [[70](#bib.bib70)] / 2020 | $512\times 512$ | Hourglass | Associative embedding
    | Graph Clustering for grouping | 68.3 | - | - |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[70](#bib.bib70)] / 2020 | $512\times 512$ | Hourglass | 关联嵌入 | 用于分组的图聚类
    | 68.3 | - | - |'
- en: '| [[71](#bib.bib71)] / 2018 | $480\times 480$ | ResNet+FPN | Person Detection
    | Pose Residual Network assigns keypoints to instances | 69.6 | - | - |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[71](#bib.bib71)] / 2018 | $480\times 480$ | ResNet+FPN | 人体检测 | 姿态残差网络分配关键点到实例
    | 69.6 | - | - |'
- en: '| [[72](#bib.bib72)] / 2020 | $640\times 640$ | HigherHRNet | Associative embedding
    | HigherHRNet for the scale variation challenge | 70.5 | 63.8M | 154.3 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#bib.bib72)] / 2020 | $640\times 640$ | HigherHRNet | 关联嵌入 | HigherHRNet
    应对尺度变化挑战 | 70.5 | 63.8M | 154.3 |'
- en: 'TABLE III: The state-of-the-art studies for image-based 2D multi-person HPE.
    The mAP scores are defined in [subsubsection II-C3](#S2.SS3.SSS3 "II-C3 Metrics
    in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey"), are obtained by testing on
    the COCO dataset.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III：基于图像的二维多人 HPE 的最新研究。mAP 分数在 [subsubsection II-C3](#S2.SS3.SSS3 "II-C3
    Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey") 中定义，通过在 COCO 数据集上测试获得。'
- en: III-B1 Top-down approaches
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 自上而下的方法
- en: The top-down method is an effective, popular method in 2D multi-person pose
    estimation tasks. By using a pretrained human detector to crop images in top-down
    approaches, the multi-person tasks can be converted into single-person tasks.
    The performance of top-down methods can be enhanced with the improvements of both
    human detector and single-person pose estimator. He et al. [[66](#bib.bib66)]
    showed that multi-person pose estimation can be implemented by extending HPE tasks
    to the detection tasks. Fang et al. [[39](#bib.bib39)] proposed a regional multi-person
    pose estimation framework to improve the performance of the human detector. Furthermore,
    the improvement of single-person pose estimators shows benefits for multi-person
    pose estimation [[58](#bib.bib58), [73](#bib.bib73), [42](#bib.bib42)].
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 自上而下的方法是一种有效且受欢迎的二维多人姿态估计任务方法。通过使用预训练的人体检测器来裁剪图像，自上而下的方法可以将多人任务转换为单人任务。通过改进人体检测器和单人姿态估计器，自上而下的方法的性能可以得到提升。He
    等人 [[66](#bib.bib66)] 表明，扩展 HPE 任务到检测任务可以实现多人姿态估计。Fang 等人 [[39](#bib.bib39)] 提出了一个区域性多人姿态估计框架，以提高人体检测器的性能。此外，单人姿态估计器的改进对多人姿态估计也有益处
    [[58](#bib.bib58), [73](#bib.bib73), [42](#bib.bib42)]。
- en: In general, the top-down approaches show advanced performance on datasets and
    can be easily implemented by combining the existing detectors and SPPE models.
    However, the computing of top-down approaches is significantly increasing over
    the number of detected persons, which limits its real-time performance for multiple-person
    scenarios. Therefore, the top-down approaches are hardly applied to the real-time
    HPE tasks in complex scenarios, particularly the crowd scenes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，自上而下的方法在数据集上表现出先进的性能，并且可以通过结合现有的检测器和 SPPE 模型来轻松实现。然而，自上而下方法的计算随着检测到的人员数量显著增加，这限制了其在多人人员场景中的实时性能。因此，自上而下的方法很难应用于复杂场景中的实时
    HPE 任务，特别是在人群场景中。
- en: III-B2 Bottom-up approaches
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 自下而上的方法
- en: 'Instead of performing keypoints detection in the proposed bounding boxes, the
    bottom-up methods often consist of two parts: keypoints detection and keypoints
    grouping. For keypoints detection, body keypoints of all persons in an image are
    detected directly by the heatmaps described in [Equation 3](#S2.E3 "3 ‣ II-B2
    Heatmaps in Multi-person HPE ‣ II-B Pose Representation ‣ II Preliminary Knowledge
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"). for keypoints
    grouping, the detected keypoints need to be grouped into the single-person. The
    deep neural networks are generally applied to assign keypoints to the proposed
    bounding boxes of the detected person. Newell et al. [[63](#bib.bib63)] introduced
    associative embedding to train neural networks for assigning keypoints to each
    person. Jin et al. [[70](#bib.bib70)] applied graph neural networks to group the
    detected joints.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '与在提议的边界框中执行关键点检测不同，自下而上的方法通常包括两个部分：关键点检测和关键点分组。对于关键点检测，通过 [Equation 3](#S2.E3
    "3 ‣ II-B2 Heatmaps in Multi-person HPE ‣ II-B Pose Representation ‣ II Preliminary
    Knowledge ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey") 中描述的热图直接检测图像中所有人的身体关键点。对于关键点分组，需要将检测到的关键点分组到单个人身上。深度神经网络通常用于将关键点分配到检测到的人的提议边界框中。Newell
    等人 [[63](#bib.bib63)] 引入了关联嵌入来训练神经网络，为每个人分配关键点。Jin 等人 [[70](#bib.bib70)] 应用了图神经网络来对检测到的关节进行分组。'
- en: Moreover, it is shown in [[72](#bib.bib72)] that the bottom-up approaches are
    more robust than the top-down approaches when applied to crowded scenes, which
    is crucial for practical applications. However, the top-down methods achieve better
    performance in terms of accuracy, while the computing time inevitably increases
    as the number of detected persons increases. By contrast, the bottom-up approaches
    take relatively constant computing time for multi-person HPE [[40](#bib.bib40),
    [69](#bib.bib69)], which is much less sensitive to the number of targeted persons.
    In conclusion, the bottom-up approaches are conducive to real-time multi-person
    pose estimation on the low-performance hardware platform.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[[72](#bib.bib72)] 显示，与顶层方法相比，底层方法在处理拥挤场景时更具鲁棒性，这对实际应用至关重要。然而，顶层方法在准确性方面表现更好，但随着检测人数的增加，计算时间不可避免地增加。相比之下，底层方法对多人
    HPE 的计算时间相对恒定 [[40](#bib.bib40), [69](#bib.bib69)]，对目标人数的变化不那么敏感。总之，底层方法有利于在低性能硬件平台上进行实时多人人体姿态估计。
- en: III-C Video-based 2D HPE
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 视频基础的 2D HPE
- en: 'The video-based 2D HPE is generally a more complicated task than the image-based
    2D HPE. A pipeline of the generic video-based 2D HPE is shown in [Figure 3](#S3.F3
    "Figure 3 ‣ III-C Video-based 2D HPE ‣ III 2D Human Pose Estimation ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey"). Unlike static images, video
    frames are likely to involve the problem of image degeneration such as motion
    blur and video defocus. Although the video-based 2D HPE performance may degenerate
    because of motion blur, video defocus, and temporary occlusions, these video-based
    approaches generally surpass image-based approaches by capturing temporal information.
    The correlations among video frames can be used to further improve the self-supervised
    approaches in HPE.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '视频基础的 2D HPE 通常比基于图像的 2D HPE 更复杂。通用视频基础的 2D HPE 流程图见[图 3](#S3.F3 "Figure 3
    ‣ III-C Video-based 2D HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human
    Pose Estimation via Deep Learning: A Survey")。与静态图像不同，视频帧可能涉及图像退化问题，如运动模糊和视频失焦。尽管由于运动模糊、视频失焦和暂时遮挡，视频基础的
    2D HPE 性能可能会退化，但这些视频基础的方法通常通过捕捉时间信息超过了基于图像的方法。视频帧之间的相关性可用于进一步改善 HPE 中的自监督方法。'
- en: '![Refer to caption](img/91be6775104e9bf3a6263d19764e7d49.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/91be6775104e9bf3a6263d19764e7d49.png)'
- en: 'Figure 3: Illustration of a video-based 2D HPE pipeline where a Neural Networks-based
    (NNs-based) module is utilized to extract temporal information. The original images
    are from the Penn Action dataset [[29](#bib.bib29)].'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：视频基础的 2D HPE 流程图，其中利用了基于神经网络（NNs-based）的模块来提取时间信息。原始图像来自 Penn Action 数据集
    [[29](#bib.bib29)]。
- en: In video-based 2D HPE, it is costly to manually annotate human joints in each
    frame of video, which restricts the obtaining of large-scale datasets for the
    video-based HPE. To solve this issue, the temporal correlation and consistency
    among the sequences need to be fully investigated. Jain et al. [[57](#bib.bib57)]
    originally proposed a CNN-based approach to combine RGB images and motion features
    for improving both the accuracy and speed of HPE. Specially, the recurrent neural
    networks, such as the well-known long short-term memory (LSTM), perform remarkably
    to capture temporal consistency among frames [[44](#bib.bib44), [45](#bib.bib45)].
    Nie et al. [[74](#bib.bib74)] proposed a dynamic kernel distillation model to
    distil previous temporal cues among frames. Although these methods perform state-of-the-art
    accuracy, they are supervised learning-based by using large-scale and densely
    labelled data in video-based HPE. By contrast, semi-supervised learning is useful
    for video-based 2D HPE with sparsely labelled data since labelling large-scale
    and densely data is costly and labour-intensive. To this end, Bertasius et al.
    [[75](#bib.bib75)] proposed PoseWarper propagate pose information in sparsely
    labelled (each k frames) videos.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于视频的2D人体姿态估计中，手动标注视频中每一帧的人体关节成本高昂，这限制了大规模数据集的获取。为了解决这个问题，需要充分研究序列之间的时间相关性和一致性。Jain等人[[57](#bib.bib57)]
    最初提出了一种基于CNN的方法，将RGB图像和运动特征结合起来，以提高人体姿态估计的准确性和速度。特别地，递归神经网络，如著名的长短期记忆网络（LSTM），在捕捉帧之间的时间一致性方面表现出色[[44](#bib.bib44),
    [45](#bib.bib45)]。Nie等人[[74](#bib.bib74)] 提出了一个动态内核蒸馏模型，以提取帧之间的时间线索。尽管这些方法在准确性上表现处于最先进水平，但它们在视频-based
    HPE中基于大规模和密集标注的数据进行监督学习。相比之下，半监督学习对基于视频的2D HPE尤其有用，因为标注大规模和密集数据既昂贵又劳动密集。为此，Bertasius等人[[75](#bib.bib75)]
    提出了PoseWarper，用于在稀疏标注（每k帧）的视频中传播姿态信息。
- en: 'Finally, we collect the open-source implementations of well-known state-of-the-art
    2D HPE works, as shown in [Table VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets
    ‣ VI-B1 Challenges in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research Trends
    and Challenges ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").
    We summarize the prevalent real-time open-source implementations for practical
    applications (e.g., OpenPose, AlphaPose, a lightweight version of OpenPose, and
    BlazePose [[64](#bib.bib64)] driven by the framework of MediaPipe [[65](#bib.bib65)]
    ), which generally offer open-source codes to users.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们收集了著名的最先进2D HPE工作的开源实现，如[表 VI](#S6.T6 "TABLE VI ‣ 不全面的数据集 ‣ VI-B1 精确HPE的挑战
    ‣ VI-B 研究挑战 ‣ VI 研究趋势和挑战 ‣ 基于视觉的深度学习人体姿态估计：综述")所示。我们总结了适用于实际应用的流行实时开源实现（例如，OpenPose、AlphaPose、OpenPose的轻量版本和BlazePose
    [[64](#bib.bib64)]，由MediaPipe [[65](#bib.bib65)] 框架驱动），这些实现通常为用户提供开源代码。
- en: III-D Robustness Analysis
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 鲁棒性分析
- en: Robustness is a crucial property that should be considered in deep learning-based
    methods. We review the studies of robustness analysis for HPE in this subsection.
    Currently, robustness analysis is usually applied to 2D HPE but rarely considered
    in 3D HPE. Deep learning-based methods are generally sensitive and vulnerable
    to the attack of adversarial samples.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性是深度学习方法中一个重要的属性。在本小节中，我们回顾了人体姿态估计的鲁棒性分析研究。目前，鲁棒性分析通常应用于2D HPE，但在3D HPE中很少考虑。深度学习方法通常对对抗样本的攻击敏感且易受攻击。
- en: Although the robustness of HPE is rarely studied, it is crucial to be considered
    in the design and evaluation of HPE methods. In [[41](#bib.bib41)], it is demonstrated
    that the PCK of the Hourglass network dramatically decrease from $89.4$ to $0.57$
    in the testing with adversarial samples. Wang et al. [[76](#bib.bib76)] proposed
    the new datasets COCO-C, MPIIC, and OCHuman-C, which were reconstructed on the
    basis of COCO [[31](#bib.bib31)], MPII [[30](#bib.bib30)], and OCHuman [[77](#bib.bib77)]
    for evaluating the robustness of HPE methods. Shah et al. [[78](#bib.bib78)] comprehensively
    investigated the adversarial robustness of HPE methods. The experimental results
    show that 1) heatmap-based methods perform more robust than regression-based methods,
    and 2) different body joints generally perform different robustness to the attacks
    of adversarial samples. For example, the head and neck exhibited prominent robustness,
    while the joints of the hips, knees, and ankles are sensitive to disturbances.
    These works revealed the robustness of the existing deep learning-based HPE methods.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管HPE的鲁棒性很少被研究，但在HPE方法的设计和评估中至关重要。在[[41](#bib.bib41)]中，演示了Hourglass网络的PCK在对抗样本测试中从$89.4$剧降至$0.57$。Wang等人[[76](#bib.bib76)]提出了新数据集COCO-C、MPIIC和OCHuman-C，这些数据集是在COCO[[31](#bib.bib31)]、MPII[[30](#bib.bib30)]和OCHuman[[77](#bib.bib77)]的基础上重建的，用于评估HPE方法的鲁棒性。Shah等人[[78](#bib.bib78)]全面调查了HPE方法的对抗鲁棒性。实验结果表明，1)
    基于热图的方法比基于回归的方法更鲁棒，2) 不同的身体关节对对抗样本的攻击一般表现出不同的鲁棒性。例如，头部和颈部表现出显著的鲁棒性，而臀部、膝盖和踝关节对干扰敏感。这些工作揭示了现有基于深度学习的HPE方法的鲁棒性。
- en: IV 3D Human Pose Estimation
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 3D 人体姿态估计
- en: The 3D HPE is to predict the 3D positions of human joints. It is a challenging
    task due to the large solution space and inherent ambiguity. Moreover, the lack
    of outdoor large-scale 3D HPE datasets challenges the practical performance of
    3D HPE approaches since the existing datasets 3D HPE is mostly collected by indoor
    motion capture systems [[33](#bib.bib33), [34](#bib.bib34)]. Early studies either
    implemented 3D HPE with model-based approaches [[79](#bib.bib79), [80](#bib.bib80)]
    or regarded the task as a regression problem which can be solved by optimization
    algorithms. Since the DNN-based methods [[81](#bib.bib81), [82](#bib.bib82)] outperform
    the previous works by automatically learning representations from large-scale
    data, deep learning-based methods have become the most popular methods in 3D HPE
    as well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 3D HPE旨在预测人体关节的3D位置。这是一个具有挑战性的任务，因为解决空间大且固有模糊。此外，缺乏户外的大规模3D HPE数据集挑战了3D HPE方法的实际表现，因为现有的数据集3D
    HPE大多是通过室内运动捕捉系统收集的[[33](#bib.bib33), [34](#bib.bib34)]。早期的研究要么实现了基于模型的方法[[79](#bib.bib79),
    [80](#bib.bib80)]，要么将任务视为可以通过优化算法解决的回归问题。由于基于DNN的方法[[81](#bib.bib81), [82](#bib.bib82)]通过自动从大规模数据中学习表示而优于以前的工作，基于深度学习的方法也已成为3D
    HPE中最受欢迎的方法。
- en: 'In this section, we review deep learning-based 3D HPE studies as shown in [Table IV](#S4.T4
    "TABLE IV ‣ IV 3D Human Pose Estimation ‣ Vision-based Human Pose Estimation via
    Deep Learning: A Survey"). According to the characteristics of inputs, we categorize
    the 3D HPE into three types: monocular 3D HPE, multi-view 3D HPE, and multimodal
    3D HPE. Finally, we present a collection of the open-source code of the state-of-the-art
    3D HPE approaches in [Table VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets ‣
    VI-B1 Challenges in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research Trends
    and Challenges ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们回顾了基于深度学习的3D HPE研究，如[表 IV](#S4.T4 "TABLE IV ‣ IV 3D Human Pose Estimation
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey")所示。根据输入的特点，我们将3D
    HPE分为三种类型：单目3D HPE、多视角3D HPE和多模态3D HPE。最后，我们在[表 VI](#S6.T6 "TABLE VI ‣ Incomprehensive
    Datasets ‣ VI-B1 Challenges in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research
    Trends and Challenges ‣ Vision-based Human Pose Estimation via Deep Learning:
    A Survey")中展示了最先进的3D HPE方法的开源代码集合。'
- en: '| Views | Modality | Studies | Methods | Highlights | MPJPE | Dataset |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 视角 | 模态 | 研究 | 方法 | 亮点 | MPJPE | 数据集 |'
- en: '| Monocular | Vision | Ching-Hang et al. [[82](#bib.bib82)] | CNN | A matching
    method to lift poses | 82.72 | Human3.6m |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 单目 | 视觉 | Ching-Hang等人[[82](#bib.bib82)] | CNN | 提升姿态的匹配方法 | 82.72 | Human3.6m
    |'
- en: '| Julieta et al. [[83](#bib.bib83)] | CNN | Off-the-shelf detectors & lifting
    networks | 87.3 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Julieta等人[[83](#bib.bib83)] | CNN | 现成检测器和提升网络 | 87.3 |'
- en: '| Dushyant et al. [[84](#bib.bib84)] | CNN | Occlusion-robust pose-maps | 69.9
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Dushyant等人[[84](#bib.bib84)] | CNN | 抗遮挡姿态图 | 69.9 |'
- en: '| Zhao et al. [[48](#bib.bib48)] | CNN+GCN | An novel SemGCN for 2D-3D lifting.
    | 60.8 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. [[48](#bib.bib48)] | CNN+GCN | 一种新型SemGCN用于2D-3D提升 | 60.8 |'
- en: '| Shichao et al. [[85](#bib.bib85)] | CNN | Evolutionary 2D-3D data augmentation
    & Cascaded 2D-3D lifting networks | 50.9 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Shichao et al. [[85](#bib.bib85)] | CNN | 演化的2D-3D数据增强与级联的2D-3D提升网络 | 50.9
    |'
- en: '| Kehong et al. [[86](#bib.bib86)] | CNN | Differentiable pose augmentor for
    2D-3D lifting | 50.2 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Kehong et al. [[86](#bib.bib86)] | CNN | 可微分的姿态增强器用于2D-3D提升 | 50.2 |'
- en: '| Yujun et al. $\cite[cite]{[\@@bibref{}{cai2019exploiting}{}{}]}^{\dagger}$
    | CNN+GCN | GCN-based 2D-3D sequence lifting. | 48.8 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Yujun et al. $\cite[cite]{[\@@bibref{}{cai2019exploiting}{}{}]}^{\dagger}$
    | CNN+GCN | 基于GCN的2D-3D序列提升 | 48.8 |'
- en: '|  | Wenbo et al. $\cite[cite]{[\@@bibref{}{hu2021conditional}{}{}]}^{\dagger}$
    | CNN+GCN | Graph for skeleton representation & GCN for 2D-3D sequence lifting
    | 41.1 |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | Wenbo et al. $\cite[cite]{[\@@bibref{}{hu2021conditional}{}{}]}^{\dagger}$
    | CNN+GCN | 用于骨架表示的图与用于2D-3D序列提升的GCN | 41.1 |  |'
- en: '|  | Vision & IMUs | Timo et al. [[35](#bib.bib35)] | CNN | Video Inertial
    Poser to fusing images and IMUs | 26 | 3DPW |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 视觉与IMUs | Timo et al. [[35](#bib.bib35)] | CNN | 视频惯性姿态融合图像和IMUs | 26
    | 3DPW |'
- en: '| Multi-view | Vision | Karim et al. [[87](#bib.bib87)] | CNN | An end-to-end
    DNN triangulation method | 17.7 | Human3.6m |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 多视角 | 视觉 | Karim et al. [[87](#bib.bib87)] | CNN | 一种端到端的DNN三角测量方法 | 17.7
    | Human3.6m |'
- en: '| Zhang et al. [[88](#bib.bib88)] | CNN | An adaptive multi-view fusion method
    | 19.5 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. [[88](#bib.bib88)] | CNN | 一种自适应的多视角融合方法 | 19.5 |'
- en: '| Yihui et al. [[89](#bib.bib89)] | CNN | An epipolar transformer | 26.9 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Yihui et al. [[89](#bib.bib89)] | CNN | 一种极线变换器 | 26.9 |'
- en: '| Haibo et al. [[90](#bib.bib90)] | CNN | A cross-view fusion network | 31.17
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Haibo et al. [[90](#bib.bib90)] | CNN | 一种交叉视角融合网络 | 31.17 |'
- en: '| Size et al. [[91](#bib.bib91)] | CNN+GCN | Learnable association matching
    & graph-based 3D pose refinement | 15.84 | CMU Panoptic |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Size et al. [[91](#bib.bib91)] | CNN+GCN | 可学习的关联匹配与基于图的3D姿态细化 | 15.84 |
    CMU Panoptic |'
- en: '| Vision & IMUs | Trumble et al. [[92](#bib.bib92)] | CNN | A two-stream network
    fuses video and IMUs | 87.3 | Human3.6m |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 视觉与IMUs | Trumble et al. [[92](#bib.bib92)] | CNN | 一种两流网络融合视频和IMUs | 87.3
    | Human3.6m |'
- en: '| Gilbert et al. [[93](#bib.bib93)] | CNN | Incorporating [[92](#bib.bib92)]
    with enhanced 3D HPE | 71.9 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Gilbert et al. [[93](#bib.bib93)] | CNN | 将[[92](#bib.bib92)]与增强的3D HPE结合
    | 71.9 |'
- en: '| Zhang et al. [[94](#bib.bib94)] | CNN | An orientation regularized pictorial
    model | 24.6 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. [[94](#bib.bib94)] | CNN | 一种方向正则化的图示模型 | 24.6 |'
- en: '| Malleson et al. [[95](#bib.bib95)] | CNN | Optimization based on IMUs and
    poses | 62 | Total Capture |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Malleson et al. [[95](#bib.bib95)] | CNN | 基于IMUs和姿态的优化 | 62 | Total Capture
    |'
- en: '|  | Huang et al. [[96](#bib.bib96)] | CNN | DeepFuse for vision-IMU data fusion
    | 28.9 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | Huang et al. [[96](#bib.bib96)] | CNN | DeepFuse用于视觉-IMU数据融合 | 28.9 |'
- en: 'TABLE IV: The state-of-the-art approaches for image-based 3D HPE. The MPJPE
    scores that are defined in [subsubsection II-C4](#S2.SS3.SSS4 "II-C4 Metrics in
    3D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge ‣ Vision-based Human
    Pose Estimation via Deep Learning: A Survey"), can be obtained by testing the
    methods on the Human3.6M dataset. The symbol ${\dagger}$ denotes the methods of
    using image sequence.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '表IV：基于图像的3D HPE的最先进方法。MPJPE评分定义在 [subsubsection II-C4](#S2.SS3.SSS4 "II-C4
    Metrics in 3D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey")，可以通过在Human3.6M数据集上测试这些方法获得。符号${\dagger}$表示使用图像序列的方法。'
- en: IV-A Monocular 3D HPE
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 单目3D HPE
- en: 'The monocular 3D HPE task is to predict the 3D positions of human joints for
    a monocular image, which is known to be an ill-posed problem. DNNs have shown
    remarkable performance on predicting the depth of monocular images [[97](#bib.bib97)]
    and the monocular 3D HPE tasks [[83](#bib.bib83), [98](#bib.bib98)]. The monocular
    3D HPE methods can be generally categorized into two types: the single-stage and
    the two-stage methods. The basic difference between these two types is that two-stage
    methods entail using off-the-shelf 2D predictors with the 2D HPE datasets. The
    single-stage methods predict 3D poses from images directly. By contrast, two-stage
    methods estimate 2D poses and then lift the 2D poses to 3D poses. In this subsection,
    we summarize the state-of-the-art works of both methods for monocular 3D HPE.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 单目3D HPE任务是预测单目图像中人体关节的3D位置，这被认为是一个病态问题。DNN在预测单目图像的深度[[97](#bib.bib97)]和单目3D
    HPE任务[[83](#bib.bib83), [98](#bib.bib98)]上表现出了显著的性能。单目3D HPE方法通常可以分为两类：单阶段方法和两阶段方法。这两种方法的基本区别在于，两阶段方法涉及使用现成的2D预测器与2D
    HPE数据集。单阶段方法直接从图像中预测3D姿势。相反，两阶段方法首先估计2D姿势，然后将2D姿势提升为3D姿势。在本小节中，我们总结了两种方法在单目3D
    HPE中的最新研究成果。
- en: IV-A1 Single-stage Approaches
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 单阶段方法
- en: As DNN-based methods can automatically build mappings from 2D images to 3D poses,
    the single-stage 3D HPE can be generally viewed as an extension of 2D HPE. Early
    works applied the regression paradigm to estimate 3D human pose directly. Li et
    al. [[81](#bib.bib81)] originally applied an end-to-end approach with the deep
    neural network for 3D HPE by the combination of a joint detector and a joint regressor.
    Luvizon et al. [[99](#bib.bib99)] proposed a multi-task framework to perform multi-task
    learning, which predicts 3D poses directly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于DNN的方法可以自动构建从2D图像到3D姿势的映射，单阶段3D HPE通常可以视为2D HPE的扩展。早期的工作应用回归范式直接估计3D人体姿势。Li等人[[81](#bib.bib81)]最初通过将关节点检测器和关节点回归器相结合，使用深度神经网络应用端到端方法进行3D
    HPE。Luvizon等人[[99](#bib.bib99)]提出了一个多任务框架，进行多任务学习，直接预测3D姿势。
- en: Compared to the early studies, there are many current single-stage approaches
    that apply heatmap representation to 3D HPE. For example, Pavlakos et al. [[100](#bib.bib100)]
    proposed a single-stage method that predicted 3D heatmaps in voxel space and proposed
    a coarse-to-fine prediction scheme to reduce the large 3D heatmap cost. To reduce
    the computational cost of direct predicting 3D voxel heatmaps, Nibali et al. [[101](#bib.bib101)]
    predicted the marginal 2D heatmaps to produce 3D coordinates.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期研究相比，目前有许多单阶段方法应用热图表示进行3D HPE。例如，Pavlakos等人[[100](#bib.bib100)]提出了一种单阶段方法，该方法在体素空间中预测3D热图，并提出了一种由粗到细的预测方案，以减少大型3D热图的成本。为了降低直接预测3D体素热图的计算成本，Nibali等人[[101](#bib.bib101)]预测了边际2D热图以生成3D坐标。
- en: 'For the single-stage methods in multi-person 3D HPE, they can be similarly
    categorized into the top-down and bottom-up methods, recalling the [Figure 2](#S3.F2
    "Figure 2 ‣ III-B Image-based 2D Multi-Person HPE ‣ III 2D Human Pose Estimation
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"). The top-down
    methods directly predict 3D poses of the detected person in each proposed bounding
    box. Bottom-up methods estimate the 3D pose by detecting all 3D joints and grouping
    all joints into person-specific sets. Typically, Fabbri et.al [[102](#bib.bib102)]
    proposed a volumetric heatmap autoencoder to estimate 3D joint locations and employed
    a distance-based heuristic strategy to associate the body joints of a person.
    In summary, the framework of the top-down and bottom-up methods in monocular multi-person
    3D HPE is similar to the frameworks in [Figure 2](#S3.F2 "Figure 2 ‣ III-B Image-based
    2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey"), while the joint points are distributed in the 3D
    space.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '对于多人物3D HPE中的单阶段方法，它们可以类似地分为自上而下和自下而上的方法，回忆[图2](#S3.F2 "Figure 2 ‣ III-B Image-based
    2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey")。自上而下的方法直接预测每个提议的边界框中检测到的人的3D姿势。自下而上的方法通过检测所有3D关节并将所有关节分组到特定于人的集合中来估计3D姿势。通常，Fabbri等人[[102](#bib.bib102)]提出了一种体积热图自动编码器来估计3D关节位置，并采用基于距离的启发式策略来关联一个人的身体关节。总的来说，单目多人物3D
    HPE中的自上而下和自下而上的方法框架类似于[图2](#S3.F2 "Figure 2 ‣ III-B Image-based 2D Multi-Person
    HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey")中的框架，而关节点分布在3D空间中。'
- en: IV-A2 Two-stage Approaches
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 两阶段方法
- en: Although the single-stage methods are efficient, the two-stage methods generally
    yield better performance as they could further benefit from 2D information and
    large-scale 2D HPE datasets. Many studies utilize off-the-shelf 2D estimators
    to produce 2D poses and followed by a 2D-3D lifting method. For example, Martinez
    et al. [[83](#bib.bib83)] used an off-the-shelf 2D estimator to produce 2D poses
    and then employed a simple lifting neural network to predict 3D poses on the basis
    of 2D poses. Zheng et al. [[103](#bib.bib103)] utilized vision transformer to
    implement the 3D lifting from 2D pose sequence. For two-stage methods, weak-supervised
    approaches are often used to leverage the 2D-to-3D constraint [[73](#bib.bib73)].
    Pavllo et al. [[104](#bib.bib104)] fuse 2D poses sequence from videos to predict
    accurate 3D poses that exploit unlabeled videos in a semi-supervised manner.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管单阶段方法效率较高，两阶段方法通常能获得更好的性能，因为它们可以进一步利用2D信息和大规模的2D HPE数据集。许多研究利用现成的2D估计器来生成2D姿态，然后采用2D-3D提升方法。例如，Martinez等人
    [[83](#bib.bib83)] 使用了现成的2D估计器来生成2D姿态，然后基于2D姿态使用简单的提升神经网络预测3D姿态。Zheng等人 [[103](#bib.bib103)]
    利用视觉变换器实现了从2D姿态序列的3D提升。对于两阶段方法，弱监督方法通常用于利用2D到3D的约束 [[73](#bib.bib73)]。Pavllo等人
    [[104](#bib.bib104)] 融合了来自视频的2D姿态序列，以预测准确的3D姿态，这种方法在半监督的方式下利用未标记的视频。
- en: In multi-person 3D HPE, the two-stage methods also employ the top-down and bottom-up
    paradigms yet incorporate the two-stage process. For the top-down methods, Rogez
    et al. [[105](#bib.bib105)] employed a region proposal network to propose bounding
    boxes for targeted persons and developed a pose proposal network to estimate the
    human poses in these bounding boxes. Zanfir et al. [[106](#bib.bib106)] performed
    bottom-up 2D MPPE and then recovered 3D poses from 2D poses by a 3D pose decoding
    module.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在多人3D人体姿态估计中，两阶段方法也采用了自上而下和自下而上的范式，但结合了两阶段过程。对于自上而下的方法，Rogez等人 [[105](#bib.bib105)]
    使用了一个区域提议网络来为目标人物提议边界框，并开发了一个姿态提议网络来估计这些边界框中的人体姿态。Zanfir等人 [[106](#bib.bib106)]
    进行了自下而上的2D MPPE，然后通过3D姿态解码模块从2D姿态恢复3D姿态。
- en: Many approaches conducted the two-stage approaches of graph neural networks
    to the lifting in 3D HPE. For two-stage approaches, a GCN-based lifting network
    generally takes the graph from 2D poses as the input to produce 3D poses. Zhao
    et al. [[48](#bib.bib48)] proposed a semantic graph convolutional network to regress
    3D joint coordinates from the 2D joint coordinates. Hu et al. [[49](#bib.bib49)]
    proposed a directed graph-based skeleton representation and applied a graph convolutional
    network to exploit both spatial and temporal information of image sequences. The
    two-stage GCN-based HPE approaches show competitive performance by considering
    the graph of the prior knowledge of the human skeleton. However, GCNs-based approaches
    generally integrate other DNNs-based models to extract 2D poses in the first stage
    as the input of GCNs (the second stage), which limits the usage of GCNs in the
    HPE field.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方法在3D人体姿态估计中采用了图神经网络的两阶段方法。对于两阶段方法，基于GCN的提升网络通常将来自2D姿态的图作为输入，以生成3D姿态。赵等人 [[48](#bib.bib48)]
    提出了一个语义图卷积网络，从2D关节坐标回归到3D关节坐标。胡等人 [[49](#bib.bib49)] 提出了一个基于有向图的骨架表示，并应用图卷积网络来利用图像序列的空间和时间信息。基于GCN的两阶段HPE方法通过考虑人体骨架的先验知识图，表现出具有竞争力的性能。然而，基于GCN的方法通常在第一阶段整合其他基于DNN的模型来提取2D姿态，作为GCN（第二阶段）的输入，这限制了GCN在HPE领域的应用。
- en: IV-B Multi-view 3D HPE
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 多视角3D人体姿态估计
- en: The multi-view 3D HPE tasks aim to predict 3D joints position with synchronous
    multi-view cameras. Although the classic triangulation method (the details refer
    to [[107](#bib.bib107)]) can calculate accurate 3D object locations via multi-view
    camera systems, the triangulation method performs a drawback of noticeable sensitivity
    to inaccurate 2D prediction. Alleviating the error of 2D prediction is a critical
    problem of multi-view 3D HPE. Iskakov et al. [[87](#bib.bib87)] proposed an end-to-end
    DNN-based learnable triangulation method to produce confidence weights for each
    view. Qiu et al. [[90](#bib.bib90)] proposed a multi-view fusion layer to improve
    2D pose estimation and used a recursive pictorial structure model to predict 3D
    pose. He et al. [[89](#bib.bib89)] introduced epipolar geometry to multi-view
    fusion, which significantly decreased the number of parameters in the fusion module.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角3D HPE任务旨在利用同步的多视角摄像机预测3D关节位置。尽管经典的三角测量方法（详情参见[[107](#bib.bib107)]）可以通过多视角摄像机系统计算出准确的3D物体位置，但三角测量方法对不准确的2D预测具有明显的敏感性。减轻2D预测的误差是多视角3D
    HPE的一个关键问题。Iskakov等人[[87](#bib.bib87)]提出了一种基于DNN的端到端可学习三角测量方法，以为每个视角生成置信度权重。Qiu等人[[90](#bib.bib90)]提出了一种多视角融合层来改善2D姿态估计，并使用递归图像结构模型来预测3D姿态。He等人[[89](#bib.bib89)]将极几何引入多视角融合中，这显著减少了融合模块中的参数数量。
- en: 'Similar to the multi-person HPE, the existing multi-view 3D HPE can be categorized
    into two types: the top-down and bottom-up methods. The top-down methods predict
    the 2D poses of each view and match the 2D poses to perform the 3D reconstruction.
    Dong et al. [[108](#bib.bib108)] proposed a multi-way matching algorithm that
    combined both geometric and appearance cues to match the detected 2D poses across
    views. Chen et al. [[109](#bib.bib109)] utilized the temporal consistency to match
    multi-view 2D poses to 3D poses, and retained and updated the 3D poses by the
    cross-view multi-person tracking. Huang et al. [[110](#bib.bib110)] proposed a
    dynamic matching algorithm to match corresponding multi-view 2D poses from different
    views for each person and then used point triangulation to recover 3D poses. Currently,
    there are a few studies that apply bottom-up approaches to multi-view 3D HPE.
    Elmi et al. [[111](#bib.bib111)] originally applied a bottom-up approach to the
    multi-view 3D HPE. The 2D features of each image were processed by a backbone
    network and then aggregated by an un-projection layer into a 3D input representation.
    Finally, a sub-voxel joint detection module and a skeleton decoder module were
    employed to produce a set of 3D poses.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于多人物HPE，现有的多视角3D HPE可以分为两类：自上而下方法和自下而上方法。自上而下的方法预测每个视角的2D姿态，然后将这些2D姿态匹配以进行3D重建。Dong等人[[108](#bib.bib108)]提出了一种多通道匹配算法，该算法结合了几何和外观线索，以匹配不同视角下检测到的2D姿态。Chen等人[[109](#bib.bib109)]利用时间一致性将多视角2D姿态匹配到3D姿态，并通过交视角多人物跟踪来保留和更新3D姿态。Huang等人[[110](#bib.bib110)]提出了一种动态匹配算法，用于匹配来自不同视角的相应多视角2D姿态，然后使用点三角测量恢复3D姿态。目前，应用自下而上的方法于多视角3D
    HPE的研究较少。Elmi等人[[111](#bib.bib111)]首次将自下而上的方法应用于多视角3D HPE。每张图像的2D特征由骨干网络处理，然后通过一个非投影层汇总为3D输入表示。最后，采用子体素关节检测模块和骨架解码模块生成一组3D姿态。
- en: IV-C Multimodal Learning in 3D HPE
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 多模态学习在3D HPE中的应用
- en: 'Multimodal learning is a deep learning-based approach that builds models with
    multiple modalities. A modality often refers to a sensory modality such as vision,
    touch signal, or radio signal. With the fusion of multiple sensors, multimodal
    learning approaches are more robust and capable of overcoming the challenges like
    occlusions than vision methods [[112](#bib.bib112)]. Although researchers have
    shown increasing interest in the field of multimodal learning approaches for HPE,
    a few studies apply multimodal learning to implement 3D HPE. We retrieve the related
    studies of multimodel learning in the Scopus database, as shown in [4(d)](#S6.F4.sf4
    "4(d) ‣ Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣
    Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '多模态学习是一种基于深度学习的方法，它通过多种模态建立模型。模态通常指的是感官模态，如视觉、触觉信号或无线电信号。通过融合多种传感器，多模态学习方法比视觉方法更具鲁棒性，并能够克服诸如遮挡等挑战[[112](#bib.bib112)]。尽管研究人员对HPE领域的多模态学习方法表现出越来越大的兴趣，但应用多模态学习实现3D
    HPE的研究较少。我们在Scopus数据库中检索了相关的多模态学习研究，如[4(d)](#S6.F4.sf4 "4(d) ‣ Figure 4 ‣ VI-A
    Research Trends ‣ VI Research Trends and Challenges ‣ Vision-based Human Pose
    Estimation via Deep Learning: A Survey")所示。'
- en: In this subsection, we review the related deep learning-based approaches that
    utilize 2D vision data, depth information, and multimodal information (e.g, IMUs
    signal). Marin et al. [[113](#bib.bib113)] proposed a deep depth pose model to
    combine RGB-D information and a set of predefined 3D poses to predict the 3D joint
    positions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们回顾了利用 2D 视觉数据、深度信息和多模态信息（例如 IMUs 信号）的相关深度学习方法。Marin 等人 [[113](#bib.bib113)]
    提出了一个深度深度姿态模型，将 RGB-D 信息与一组预定义的 3D 姿态结合，以预测 3D 关节位置。
- en: 'Furthermore, the deep learning-based methods with the data of IMUs and images
    have shown remarkable results in 3D HPE (as shown in [Table IV](#S4.T4 "TABLE
    IV ‣ IV 3D Human Pose Estimation ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey")). Marcard et al. [[35](#bib.bib35)] adopted IMUs and a mobile
    camera to estimate 3D human poses, and proposed the skinned multi-person linear
    model to produce initial 3D poses and then associated the 3D poses with 2D detected
    persons. Huang et al. [[96](#bib.bib96)] used the orientation of body parts, that
    is captured by IMUs, to refine the image-based pose estimation. In summary, multimodal
    HPE generally shows superior performance compared to vision-only HPE. However,
    it’s worth noting that the lack of labeled datasets is a critical problem in multimodal
    HPE.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，基于深度学习的方法结合 IMUs 和图像数据在 3D HPE 中表现出显著的结果（如 [表 IV](#S4.T4 "TABLE IV ‣ IV
    3D Human Pose Estimation ‣ Vision-based Human Pose Estimation via Deep Learning:
    A Survey") 中所示）。Marcard 等人 [[35](#bib.bib35)] 采用 IMUs 和移动摄像机来估计 3D 人体姿态，并提出了皮肤多人体线性模型来生成初始的
    3D 姿态，然后将 3D 姿态与 2D 检测到的人体相关联。Huang 等人 [[96](#bib.bib96)] 利用 IMUs 捕获的身体部位方向来细化基于图像的姿态估计。总之，与仅基于视觉的
    HPE 相比，多模态 HPE 通常表现出更优越的性能。然而，需要注意的是，缺乏标注数据集是多模态 HPE 的一个关键问题。'
- en: V Applications
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 应用
- en: 'HPE has been used in a variety of applications such as action analysis [[1](#bib.bib1)],
    HCI [[2](#bib.bib2)], gaming [[3](#bib.bib3)], sport analysis [[4](#bib.bib4),
    [5](#bib.bib5)], motion capture [[6](#bib.bib6)], computer-generated imagery [[7](#bib.bib7),
    [8](#bib.bib8)]. Accurate joint points are semantically informative and can be
    utilized in computer vision tasks like action recognition, and human tracking
    [[99](#bib.bib99)]. In this section, we present the main applications of HPE (see
    the summary in [Table V](#S5.T5 "TABLE V ‣ V Applications ‣ Vision-based Human
    Pose Estimation via Deep Learning: A Survey")).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 'HPE 已被应用于多种领域，如动作分析 [[1](#bib.bib1)]、人机交互 [[2](#bib.bib2)]、游戏 [[3](#bib.bib3)]、体育分析
    [[4](#bib.bib4), [5](#bib.bib5)]、动作捕捉 [[6](#bib.bib6)]、计算机生成图像 [[7](#bib.bib7),
    [8](#bib.bib8)]。准确的关节点在语义上具有信息量，可以用于计算机视觉任务，如动作识别和人体追踪 [[99](#bib.bib99)]。在本节中，我们介绍了
    HPE 的主要应用（详见 [表 V](#S5.T5 "TABLE V ‣ V Applications ‣ Vision-based Human Pose
    Estimation via Deep Learning: A Survey)")。'
- en: '| 2D/3D HPE | Applications | Methods | Remarks |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 2D/3D HPE | 应用 | 方法 | 备注 |'
- en: '| 2D HPE | Action Analysis | $\bullet$ Bottom-up MPPE (OpenPose [[40](#bib.bib40)])
    + GCN [[114](#bib.bib114)] $\bullet$ Top-down MPPE (AlphaPose [[39](#bib.bib39)])
    + GCN | HPE provides spatial joint data and implicit temporal correlationship
    for action analysis. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 2D HPE | 动作分析 | $\bullet$ 自下而上的 MPPE（OpenPose [[40](#bib.bib40)]）+ GCN [[114](#bib.bib114)]
    $\bullet$ 自上而下的 MPPE（AlphaPose [[39](#bib.bib39)]）+ GCN | HPE 提供空间关节数据和隐含的时间相关性，用于动作分析。
    |'
- en: '| Character Animation | $\bullet$ Top-down MPPE [[52](#bib.bib52)] + Motion
    Transfer [[115](#bib.bib115)] $\bullet$ Bottom-up MPPE [[40](#bib.bib40)] + Motion
    Transfer [[8](#bib.bib8)] | Animation by HPE-based motion transferring from human
    performer to animated character |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 角色动画 | $\bullet$ 自上而下的 MPPE [[52](#bib.bib52)] + 动作转移 [[115](#bib.bib115)]
    $\bullet$ 自下而上的 MPPE [[40](#bib.bib40)] + 动作转移 [[8](#bib.bib8)] | 通过 HPE 基于动作转移从人类表演者到动画角色的动画
    |'
- en: '| Sports Analysis | $\bullet$ Bottom-up MPPE [[40](#bib.bib40)] + Coaching
    method [[116](#bib.bib116)] $\bullet$ Video-based HPE + Coaching method [[4](#bib.bib4)]
    | HPE-based sports analysis by comparisons between players and exemplars |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 体育分析 | $\bullet$ 自下而上的 MPPE [[40](#bib.bib40)] + 教练方法 [[116](#bib.bib116)]
    $\bullet$ 基于视频的 HPE + 教练方法 [[4](#bib.bib4)] | 基于 HPE 的体育分析，通过对比球员与样本进行 |'
- en: '| Medical & Clinical | $\bullet$ Top-down MPPE + Exercise Supervision [[117](#bib.bib117)]
    $\bullet$ Bottom-up MPPE + Exercise Supervision [[118](#bib.bib118)] | Joint positions
    provide rich information for clinical applications like in-bed/sleep monitoring
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 医疗与临床 | $\bullet$ 自上而下的 MPPE + 锻炼监督 [[117](#bib.bib117)] $\bullet$ 自下而上的
    MPPE + 锻炼监督 [[118](#bib.bib118)] | 关节位置为临床应用提供了丰富的信息，如床上/睡眠监测 |'
- en: '| 3D HPE | Action Analysis | $\bullet$ Multimodal 3D HPE (Kinect-based) + Random
    Forest [[119](#bib.bib119)] $\bullet$ Multimodal 3D HPE (Kinect-based) + CNN [[120](#bib.bib120)]
    | 3D action analysis can exploit both 2D joint position and extra depth information
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 3D HPE | 动作分析 | $\bullet$ 多模态 3D HPE (基于 Kinect) + 随机森林 [[119](#bib.bib119)]
    $\bullet$ 多模态 3D HPE (基于 Kinect) + CNN [[120](#bib.bib120)] | 3D 动作分析可以利用 2D 关节位置和额外的深度信息
    |'
- en: '| HCI | $\bullet$ Multimodal 3D HPE (Kinect-based) + HCI [[2](#bib.bib2)] $\bullet$
    Monocular 3D HPE + HCI [[3](#bib.bib3)] | HPE-based HCI is a natural and contactless
    way |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 人机交互 | $\bullet$ 多模态 3D HPE (基于 Kinect) + 人机交互 [[2](#bib.bib2)] $\bullet$
    单目 3D HPE + 人机交互 [[3](#bib.bib3)] | 基于 HPE 的人机交互是一种自然且无接触的方式 |'
- en: '| Character Animation | $\bullet$ Two-stage Monocular 3D HPE + Motion Transfer
    [[121](#bib.bib121), [6](#bib.bib6)] | 3D character animations benefit from 3D
    HPE and motion transfer |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 角色动画 | $\bullet$ 两阶段单目 3D HPE + 动作传递 [[121](#bib.bib121), [6](#bib.bib6)]
    | 3D 角色动画受益于 3D HPE 和动作传递 |'
- en: '| Sports Analysis | $\bullet$ Multimodal 3D HPE (Kinect-based) + Coaching Method
    [[122](#bib.bib122), [5](#bib.bib5)] $\bullet$ Monocular 3D HPE [[81](#bib.bib81)]
    + Coaching Method [[123](#bib.bib123)] | 3D HPE provides more dimensionality (3D
    angles between joints) for a coaching system |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 体育分析 | $\bullet$ 多模态 3D HPE (基于 Kinect) + 教练方法 [[122](#bib.bib122), [5](#bib.bib5)]
    $\bullet$ 单目 3D HPE [[81](#bib.bib81)] + 教练方法 [[123](#bib.bib123)] | 3D HPE 为教练系统提供更多维度（关节间的
    3D 角度） |'
- en: '| Medical & Clinical | $\bullet$ Multimodal 3D HPE (Kinect-based) + Neural
    Recording [[124](#bib.bib124)] $\bullet$ Multimodal 3D HPE (Kinect-based) + Exercise
    Supervision [[125](#bib.bib125)] | 3D HPE-based clinical monitoring and rehabilitation
    system use human joint and depth information |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 医疗与临床 | $\bullet$ 多模态 3D HPE (基于 Kinect) + 神经记录 [[124](#bib.bib124)] $\bullet$
    多模态 3D HPE (基于 Kinect) + 锻炼监督 [[125](#bib.bib125)] | 基于 3D HPE 的临床监测和康复系统使用人体关节和深度信息
    |'
- en: 'TABLE V: Summary of the approaches in HPE-based applications.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 基于 HPE 的应用方法总结。'
- en: V-A Action Analysis
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 动作分析
- en: Action recognition/prediction is mainly a temporal task based on image sequences.
    The traditional methods might demand extensive computation and appear unstable
    to environmental variations in terms of illuminations, objects in the background
    or foreground, body scale, and motion blur [[126](#bib.bib126)]. The human skeleton
    is naturally a high-level representation which has shown benefits for action analysis
    tasks such as action recognition [[127](#bib.bib127)] and action detection [[128](#bib.bib128)].
    For example, Duan et al. [[129](#bib.bib129)] stacked 2D heatmaps from human pose
    sequences to 3D heatmap volumes and utilized ResNet layers to predict human actions
    from these volumes. Liu et al. [[1](#bib.bib1)] used graph convolutional network
    to utilize 3D pose sequence for action analysis.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 动作识别/预测主要是基于图像序列的时间任务。传统方法可能需要大量计算，并且对环境变化（如光照、背景或前景中的物体、身体尺度和运动模糊）表现不稳定 [[126](#bib.bib126)]。人体骨架自然是一种高级表示，已显示出对动作分析任务（如动作识别
    [[127](#bib.bib127)] 和动作检测 [[128](#bib.bib128)]）的好处。例如，Duan 等 [[129](#bib.bib129)]
    将人体姿态序列中的 2D 热图堆叠为 3D 热图体，并利用 ResNet 层从这些体积中预测人体动作。Liu 等 [[1](#bib.bib1)] 使用图卷积网络利用
    3D 姿态序列进行动作分析。
- en: Currently, with the development of sensors (e.g., Kinect [[125](#bib.bib125)])
    and HPE algorithms, large-scale and accurate skeleton data for action recognition
    [[127](#bib.bib127)] becomes accessible. The space correlations and temporality
    of skeleton sequences provide informative prior knowledge to yield a robust motion
    pattern. On large-scale action recognition dataset NTURGB-D [[127](#bib.bib127)],
    the state-of-the-art skeleton-based methods [[129](#bib.bib129), [130](#bib.bib130)]
    achieved more than 95% accuracy, while image-only methods [[99](#bib.bib99)] achieved
    less than 90% accuracy.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，随着传感器（如 Kinect [[125](#bib.bib125)]）和 HPE 算法的发展，大规模且准确的动作识别骨架数据 [[127](#bib.bib127)]
    已经可以获取。骨架序列的空间相关性和时间性提供了有用的先验知识，从而生成稳健的动作模式。在大规模动作识别数据集 NTURGB-D [[127](#bib.bib127)]
    上，最先进的基于骨架的方法 [[129](#bib.bib129), [130](#bib.bib130)] 实现了超过 95% 的准确率，而仅使用图像的方法
    [[99](#bib.bib99)] 的准确率低于 90%。
- en: In addition, the skeleton-based action analysis can be used to build smart surveillance
    systems. Hbali et al. [[119](#bib.bib119)] used HPE and action analysis to build
    up an elderly monitoring system for alerting dangerous activities. Guo et al.
    [[131](#bib.bib131)] used skeleton-based action recognition to identify unsafe
    behaviours of construction workers. In conclusion, HPE provides significant spatial
    joint information and implicit temporal correlations for skeleton-based action
    analysis.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于骨架的动作分析可以用来构建智能监控系统。Hbali等人[[119](#bib.bib119)] 使用HPE和动作分析构建了一个用于警示危险活动的老年人监测系统。Guo等人[[131](#bib.bib131)]
    使用基于骨架的动作识别来识别建筑工人的不安全行为。总之，HPE提供了显著的空间关节信息和隐含的时间相关性，用于骨架-based 动作分析。
- en: V-B Human-Computer Interaction
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 人机交互
- en: Human-computer interaction (HCI) has been studied for several decades and plays
    an important role in our daily lives. Traditional HCI techniques allow humans
    to interact with computers via tangible devices or interfaces such as mice, keyboards,
    or touch screens. Compared with traditional interactions, HPE-based HCI provides
    a natural and contactless way that is highly suitable for the difficult pandemic
    situation of COVID-19, particularly when using public devices.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 人机交互（HCI）已经研究了几十年，并在我们的日常生活中发挥着重要作用。传统的人机交互技术允许人类通过有形设备或接口（如鼠标、键盘或触摸屏）与计算机进行交互。与传统交互相比，基于HPE的人机交互提供了一种自然且无接触的方式，这种方式特别适合
    COVID-19 疫情的困难情况下，尤其是在使用公共设备时。
- en: HPE-based HCI techniques are widely applied in many applications such as arts,
    gaming, and virtual reality. Most vision-only applications consider the estimation
    of 2D human poses since currently, 2D HPE approaches can offer more accurate and
    prompt predictions than 3D HPE. While 3D HPE often employs depth-aware sensors
    (like RGB-D cameras) to produce reliable and informative 3D poses. The well-known
    depth-aware distributed Microsoft Kinects [[125](#bib.bib125)] is designed to
    capture human body movements for gaming and virtual reality teleconference systems
    [[132](#bib.bib132)]. Kamel et al. [[123](#bib.bib123)], Thar et al. [[133](#bib.bib133)],
    and Park [[5](#bib.bib5)] et al. used 3D monocular HPE and a single camera to
    capture human poses for producing action evaluation and feedback for Tai Chi,
    Yoga, and golf, respectively. In summary, HPE-based HCI is a type of natural and
    contactless interface that differs from the traditional HCI.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 基于HPE的人机交互技术广泛应用于艺术、游戏和虚拟现实等许多应用中。大多数仅基于视觉的应用考虑2D人体姿态的估计，因为目前2D HPE方法可以提供比3D
    HPE更准确和及时的预测。而3D HPE通常使用深度感知传感器（如RGB-D相机）来生成可靠和信息丰富的3D姿态。著名的深度感知分布式Microsoft Kinect[[125](#bib.bib125)]
    旨在捕捉人体运动，用于游戏和虚拟现实视频会议系统[[132](#bib.bib132)]。Kamel等人[[123](#bib.bib123)]、Thar等人[[133](#bib.bib133)]
    和 Park [[5](#bib.bib5)]等人使用3D单目HPE和单个摄像头来捕捉人体姿态，为太极、瑜伽和高尔夫分别生成动作评估和反馈。总之，基于HPE的人机交互是一种自然且无接触的界面，与传统的人机交互有所不同。
- en: V-C Character Animation
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 角色动画
- en: Creating high-quality character animation is important in animation films and
    computer games. Traditional methods rely on creating costly and time-consuming
    frame-wise animations. HPE-based performance-driven animations accomplish character
    motions by transferring motions from a human performer to an animated character,
    which is prevalent in the film and game industries conveniently and economically.
    For 2D animation applications, Willett et al. [[8](#bib.bib8)] proposed a novel
    method to jointly yield 2D animation and 2D character creation by leveraging human
    pose. In addition, the HPE technique is helpful in augmented reality. For instance,
    Weng et al. [[115](#bib.bib115)] utilized human skeleton information via HPE to
    generate 3D character animation from an image. To generate 3D animations, a commonly
    used method is to control all parts of a character via motion capture [[6](#bib.bib6)].
    In summary, DL-based HPE approaches provide an alternative to traditional motion
    capture for creating both 2D and 3D animations.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 创建高质量的角色动画在动画电影和计算机游戏中非常重要。传统方法依赖于创建成本高且耗时的逐帧动画。基于HPE的性能驱动动画通过将动作从人类表演者转移到动画角色上来实现角色动作，这在电影和游戏行业中便利且经济。对于2D动画应用，Willett
    等人 [[8](#bib.bib8)] 提出了一个新方法，通过利用人体姿态来共同生成2D动画和2D角色创建。此外，HPE技术在增强现实中也很有帮助。例如，Weng
    等人 [[115](#bib.bib115)] 通过HPE利用人体骨骼信息从图像生成3D角色动画。为了生成3D动画，常用的方法是通过动作捕捉 [[6](#bib.bib6)]
    控制角色的所有部位。总之，基于DL的HPE方法为创建2D和3D动画提供了一种替代传统动作捕捉的方法。
- en: V-D Sports Analysis
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 体育分析
- en: Sports performance analysis provides statistics/recording for coaches and players
    to improve their performance. The automatic sports analysis demands precise postures
    and motion of players, which requires accurate markerless motion capture techniques.
    The HPE approaches perform effectively in the analysis of many sporting activities.
    HPE-based methods can be used to compare the differences between learners and
    instructors in various sports. For example, Park et al. [[5](#bib.bib5)] used
    the HPE approach to analyze activities in golf. By comparing a user’s swing with
    the reference swing, the user can examine his/her evaluation result in head movement,
    knee alignment, swing rhythm, and balance of golf swing in their approach. Kamel
    et al. [[123](#bib.bib123)], and Thar et al. [[133](#bib.bib133)] employed HPE
    to evaluate the difference between a learner’s posture and an instructor’s posture
    in Tai Chi and Yoga, respectively. Wang et al. [[4](#bib.bib4)] built up an HPE-based
    athletic training assistance system to detect bad poses from a sequence of 2D
    users’ poses. In addition, HPE-based approach is applied to capture players’ motion
    in sports like badminton [[116](#bib.bib116)], soccer [[134](#bib.bib134), [135](#bib.bib135)],
    and tennis [[136](#bib.bib136), [137](#bib.bib137)]. In conclusion, HPE-based
    sports analysis mainly relies on a comparison between a player and an exemplar.
    2D HPE is more commonly adopted than 3D HPE as 3D HPE currently requires either
    multi-view setups or depth-aware sensors.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 体育表现分析为教练和运动员提供统计数据/记录，以改进他们的表现。自动体育分析要求运动员的姿势和动作精准，这需要准确的无标记动作捕捉技术。HPE（人体姿态估计）方法在许多运动活动的分析中表现有效。基于HPE的方法可以用来比较学习者和教练在各种运动中的差异。例如，Park
    等人 [[5](#bib.bib5)] 使用HPE方法分析高尔夫活动。通过将用户的挥杆动作与参考挥杆动作进行比较，用户可以检查其头部运动、膝部对齐、挥杆节奏以及高尔夫挥杆的平衡等评估结果。Kamel
    等人 [[123](#bib.bib123)] 和 Thar 等人 [[133](#bib.bib133)] 分别在太极和瑜伽中使用HPE来评估学习者姿势与教练姿势之间的差异。Wang
    等人 [[4](#bib.bib4)] 建立了一个基于HPE的运动训练辅助系统，以检测2D用户姿势序列中的不良姿势。此外，HPE方法还被应用于捕捉羽毛球 [[116](#bib.bib116)]、足球
    [[134](#bib.bib134), [135](#bib.bib135)] 和网球 [[136](#bib.bib136), [137](#bib.bib137)]
    等运动中的运动。总之，基于HPE的体育分析主要依赖于运动员与示范者之间的比较。由于目前3D HPE需要多视角设置或深度传感器，2D HPE比3D HPE更为常用。
- en: V-E Medical and Clinical Applications
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 医学和临床应用
- en: Accurate joint positions provide rich information for clinical applications
    like in-bed monitoring, sleep laboratories, epilepsy monitoring, and intensive
    care units. Gabeiel et al. [[124](#bib.bib124)] combined synchronized Kinect v2
    and standard clinical electrocorticography monitors, which record neural activity
    from the cortex, to investigate the relationship between human movement behaviours
    and neural activity. In rehabilitation medicine, HPE is successfully applied to
    improve the rehabilitation of patients. Obdržálek et al. [[125](#bib.bib125)]
    employed HPE for observation and online feedback when coaching elderly patients
    with the daily physical exercise routine. Li et al. [[117](#bib.bib117)] introduced
    an in-home lower-limb rehabilitation system with an HPE-based model to help patients
    perform rehabilitation activities even without the presence of physical therapists.
    In [[118](#bib.bib118)], Rabbito used OpenPose [[40](#bib.bib40)] and motion capture
    system Vicon ¹¹1[https://www.vicon.com/](https://www.vicon.com/) to analyze patient
    gait for rehabilitation medicine.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的关节位置为临床应用提供了丰富的信息，如床上监测、睡眠实验室、癫痫监测和重症监护病房。Gabeiel 等人 [[124](#bib.bib124)]
    将同步的 Kinect v2 和标准临床脑电图监测器结合起来，后者记录来自大脑皮层的神经活动，以研究人类运动行为与神经活动之间的关系。在康复医学中，**HPE**
    被成功应用于改善患者的康复。Obdržálek 等人 [[125](#bib.bib125)] 使用 HPE 进行观察和在线反馈，以指导老年患者进行日常身体锻炼。Li
    等人 [[117](#bib.bib117)] 引入了一种基于 HPE 的居家下肢康复系统，帮助患者即使在没有物理治疗师的情况下也能进行康复活动。在 [[118](#bib.bib118)]
    中，Rabbito 使用了 OpenPose [[40](#bib.bib40)] 和运动捕捉系统 Vicon ¹¹1[https://www.vicon.com/](https://www.vicon.com/)
    来分析患者的步态，以用于康复医学。
- en: In summary, HPE-based clinical applications generally fall into two main types,
    in-bed monitoring and rehabilitation training. Clinical environments are suitable
    for installing multi-view cameras or depth-aware sensors, which encourages 3D
    HPE. HPE-based Rehabilitation training currently is similar to HPE-based sports
    coaching, which is mostly based on the comparison between a user and an exemplar.
    While traditional motion capture systems are available in limited environments,
    HPE-based methods have great potential for providing more extensive human motion
    data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，基于 HPE 的临床应用一般分为两大类：床上监测和康复训练。临床环境适合安装多视角相机或深度感知传感器，这促进了 3D HPE 的发展。基于 HPE
    的康复训练目前与基于 HPE 的运动训练类似，主要基于用户与示例之间的比较。虽然传统的运动捕捉系统在环境方面受到限制，但基于 HPE 的方法在提供更广泛的人体运动数据方面具有巨大潜力。
- en: VI Research Trends and Challenges
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 研究趋势与挑战
- en: In this section, we aim to point out the observed research trends via bibliometrics,
    raise the challenging issues, and provide insightful recommendations for future
    research.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在通过文献计量学指出观察到的研究趋势，提出挑战性问题，并为未来的研究提供有益的建议。
- en: VI-A Research Trends
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 研究趋势
- en: In this subsection, we identify the research trends via bibliometrics which
    refers to the use of statistical methods to analyze books, articles and other
    publications. It is an effective way to measure publications in the scientific
    community. However, current survey papers rarely use bibliometrics to analyze
    the data of publications, particularly in HPE. This paper applies bibliometrics
    to retrieve the publications for discovering and demonstrating the research trends
    in HPE.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们通过文献计量学确定研究趋势，文献计量学指的是使用统计方法来分析书籍、文章和其他出版物。这是一种有效的衡量科学界出版物的方式。然而，目前的调查论文很少使用文献计量学来分析出版物的数据，特别是在
    HPE 领域。本文应用文献计量学来检索出版物，以发现和展示 HPE 领域的研究趋势。
- en: 'In general, bibliometric data can be obtained from various databases. In this
    paper, we choose the Scopus database ²²2[https://en.wikipedia.org/wiki/Scopus](https://en.wikipedia.org/wiki/Scopus)
    for literature retrieval. Scopus is the largest abstract/citation database with
    peer-reviewed literature, which is released by Elsevier. The resources of Scopus
    are more accurate and comprehensive than other alternatives such as PubMed, Web
    of Science, and Google Scholar [[138](#bib.bib138)]. Importantly, Elsevier provides
    a Python library ³³3[https://github.com/pybliometrics-dev/pybliometrics](https://github.com/pybliometrics-dev/pybliometrics)
    to retrieve the data for the expected topics from Scopus database [[139](#bib.bib139)].
    In this work, we review the articles in vision-based HPE using deep learning by
    retrieving "title, abstract, and keywords" in the Scopus database. We present
    the trends that are observed from the literature in the four aspects: multi-person
    pose estimation, 3D HPE, efficient deep learning-based HPE, and multi-modal learning.
    To observe the trends clearly, we retrieve the data for each year over ten years
    (2012 - 2022). We retrieve the number of publications in Scopus data from the
    four aspects, as shown in [Figure 4](#S6.F4 "Figure 4 ‣ VI-A Research Trends ‣
    VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey"), where the red dash lines are the fit of the number of publications
    from 2012 to 2021 (NOT 2022).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '一般来说，书目计量数据可以从各种数据库中获取。在本文中，我们选择了Scopus数据库²²2[https://en.wikipedia.org/wiki/Scopus](https://en.wikipedia.org/wiki/Scopus)进行文献检索。Scopus是由Elsevier发布的最大同行评审文献的摘要/引用数据库。与PubMed、Web
    of Science和Google Scholar等其他替代数据库相比，Scopus的资源更准确、更全面[[138](#bib.bib138)]。重要的是，Elsevier提供了一个Python库³³3[https://github.com/pybliometrics-dev/pybliometrics](https://github.com/pybliometrics-dev/pybliometrics)来从Scopus数据库中检索所需主题的数据[[139](#bib.bib139)]。在这项工作中，我们通过检索Scopus数据库中的“标题、摘要和关键词”来回顾基于视觉的HPE的深度学习文章。我们呈现了从文献中观察到的四个方面的趋势：多人姿态估计、3D
    HPE、高效的基于深度学习的HPE和多模态学习。为了清晰观察趋势，我们检索了2012年至2022年的每年数据。我们从四个方面检索了Scopus数据中的出版物数量，如[图4](#S6.F4
    "Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey")所示，其中红色虚线为2012年至2021年的出版物数量的拟合（不包括2022年）。'
- en: '![Refer to caption](img/663d286169155eb6cfdf39eaa1b24ee8.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/663d286169155eb6cfdf39eaa1b24ee8.png)'
- en: (a)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/48c6dc26b282602fcf3396c3547e9284.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/48c6dc26b282602fcf3396c3547e9284.png)'
- en: (b)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/bf95aad6eb2b300b60700a3bf95ec62a.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bf95aad6eb2b300b60700a3bf95ec62a.png)'
- en: (c)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: '![Refer to caption](img/9329ae4cfe06ce2ccbf0051083bae55b.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9329ae4cfe06ce2ccbf0051083bae55b.png)'
- en: (d)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: (d)
- en: 'Figure 4: The number of publications on the HPE topics of a) multi-person pose
    estimation, b) 3D HPE, c) efficient deep learning-based HPE, d) multimodal learning
    HPE from Scopus. Scopus database returned 274, 769, 185, 111 results for a), b),
    c), and d) respectively until 08/09/2022\. To observe the trends clearly, we retrieve
    the data of each year over the ten years from 2012 to 2022\. The red dashed lines
    are the fit of the number of publications from 2012 to 2021 (NOT 2022). The source
    code to retrieve the data are available at [%\textcolor{red}{https://github.com/wuyuuu/elsevier-search](%%5Ctextcolor%7Bred%7D%7Bhttps://github.com/wuyuuu/elsevier-search).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：关于HPE主题的出版物数量：a) 多人姿态估计，b) 3D HPE，c) 基于深度学习的高效HPE，d) 多模态学习HPE，数据来源于Scopus。Scopus数据库在2022年8月9日之前返回了a)、b)、c)和d)的结果分别为274、769、185和111个。为了清晰观察趋势，我们检索了2012年至2022年十年的数据。红色虚线为2012年至2021年的出版物数量的拟合（不包括2022年）。检索数据的源代码可在[%\textcolor{red}{https://github.com/wuyuuu/elsevier-search}](https://github.com/wuyuuu/elsevier-search)获取。
- en: VI-A1 Multi-Person HPE
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 多人HPE
- en: 'Multi-Person HPE has become an important research topic. We retrieve the literature
    on this topic by using the search code TITLE-ABS-KEY(("multi-person" or "crowd")
    and ("multi-person pose estimation" or "human pose estimation")). The number of
    publications over the years (from 2012 to 2022) is shown in [4(a)](#S6.F4.sf1
    "4(a) ‣ Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣
    Vision-based Human Pose Estimation via Deep Learning: A Survey"). It shows a very
    significant increasing trend in the number of Multi-Person HPE publications since
    2015, while an insignificant drop in 2021.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '多人姿态估计已成为一个重要的研究主题。我们通过使用搜索代码 TITLE-ABS-KEY(("multi-person" 或 "crowd") 和 ("multi-person
    pose estimation" 或 "human pose estimation")) 检索相关文献。2012年至2022年间的出版物数量如[4(a)](#S6.F4.sf1
    "4(a) ‣ Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣
    Vision-based Human Pose Estimation via Deep Learning: A Survey")所示。自2015年以来，多人人体姿态估计的出版物数量显示出显著的增长趋势，而2021年则略有下降。'
- en: On one hand, the small number of publications shows that the multi-person pose
    estimation in crowd scenes is still rarely investigated. On the other hand, the
    increasing number of publications shows the increasing trend in this topic. This
    technique has been applied in various applications such as multi-person gaming
    and sports analysis. However, as reported in [[32](#bib.bib32)], the performance
    of the existing methods degrades dramatically as the crowd level increases. In
    multi-person HPE, there are still many open issues and challenges that need to
    be further studied in the future.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，少量的出版物表明多人人体姿态估计在拥挤场景中仍然很少被研究。另一方面，出版物数量的增加显示了该主题的上升趋势。这项技术已被应用于各种应用中，如多人游戏和体育分析。然而，如[[32](#bib.bib32)]所述，现有方法的性能随着人群密度的增加而显著下降。在多人人体姿态估计中，仍有许多未解的问题和挑战需要在未来进一步研究。
- en: VI-A2 3D HPE
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A2 3D 人体姿态估计
- en: 'Accurately estimating 3D positions for human joints is an important topic in
    computer vision. Compared to 2D HPE, 3D HPE could provide extra depth information
    which brings broader applications such as markerless motion capture, video games,
    and sports analysis [[5](#bib.bib5), [92](#bib.bib92)]. We observe that 3D HPE
    attracts more and more research interest recently. We design the code of TITLE-ABS-KEY(("3D")
    and ("human pose estimation")) to retrieve the literature on the 3D HPE-related
    topics in the Scopus database. The number of publications over the years (from
    2012 to 2022) is shown in [4(b)](#S6.F4.sf2 "4(b) ‣ Figure 4 ‣ VI-A Research Trends
    ‣ VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey").'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '准确估计人体关节的3D位置是计算机视觉中的一个重要话题。与2D人体姿态估计相比，3D人体姿态估计提供了额外的深度信息，这带来了更广泛的应用，如无标记运动捕捉、视频游戏和体育分析[[5](#bib.bib5),
    [92](#bib.bib92)]。我们观察到3D人体姿态估计最近吸引了越来越多的研究兴趣。我们设计了 TITLE-ABS-KEY(("3D") 和 ("human
    pose estimation")) 的代码来检索Scopus数据库中与3D人体姿态估计相关的文献。2012年至2022年间的出版物数量如[4(b)](#S6.F4.sf2
    "4(b) ‣ Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣
    Vision-based Human Pose Estimation via Deep Learning: A Survey")所示。'
- en: We observe an increasing number of 3D HPE publications from 2014 to 2021. However,
    the existing 3D HPE methods still need to be investigated for desired performance
    because of unsolved problems such as the ill-posed property, the lack of outdoor
    environment annotations, and the incapability of performing real-time inference
    on edge devices. Therefore, the 3D HPE will be a crucial research trend in the
    future.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到从2014年到2021年，3D人体姿态估计的出版物数量不断增加。然而，现有的3D人体姿态估计方法仍需进一步研究以达到理想的性能，因为存在一些未解决的问题，如病态特性、缺乏户外环境标注和无法在边缘设备上进行实时推理。因此，3D人体姿态估计将在未来成为一个重要的研究趋势。
- en: VI-A3 Efficient Deep Learning-based HPE
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A3 高效的基于深度学习的人体姿态估计
- en: 'To apply HPE methods to real applications, the efficiency of HPE approaches
    is a crucial research topic. In particular, although deep learning-based methods
    have achieved state-of-the-art performance in HPE, the approaches of deep neural
    networks may suffer from high computational costs and inference delay. We design
    the code of TITLE-ABS-KEY(("efficient" or "real-time") and ("human pose estimation"))
    to retrieve literature in efficient HPE. The number of publications over year
    (from 2012 to 2022) is shown in [4(c)](#S6.F4.sf3 "4(c) ‣ Figure 4 ‣ VI-A Research
    Trends ‣ VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey"). It shows a significant increase since 2014 with
    respect to the number of publications in efficient HPE. We note that the HPE efficiency
    problem was generally tackled by designing efficient networks manually in previous
    approaches [[56](#bib.bib56), [40](#bib.bib40)], while current studies [[17](#bib.bib17),
    [140](#bib.bib140)] introduce neural architecture search (NAS) into HPE, which
    might be a typical method in solving the efficiency problem of neural network
    in the future.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将HPE方法应用于实际应用，HPE方法的效率是一个关键的研究主题。特别是，尽管基于深度学习的方法在HPE中已达到最先进的性能，但深度神经网络的方法可能会面临高计算成本和推理延迟的问题。我们设计了TITLE-ABS-KEY(("efficient"或"real-time")和("human
    pose estimation"))的代码来检索高效HPE的文献。从2012年到2022年的出版物数量如[4(c)](#S6.F4.sf3 "4(c) ‣
    图 4 ‣ VI-A 研究趋势 ‣ VI 研究趋势和挑战 ‣ 基于视觉的人体姿态估计：综述")所示。自2014年以来，高效HPE的出版物数量显著增加。我们注意到，以前的方法[[56](#bib.bib56),
    [40](#bib.bib40)]通常通过手动设计高效网络来解决HPE效率问题，而当前的研究[[17](#bib.bib17), [140](#bib.bib140)]则将神经架构搜索（NAS）引入HPE，这可能是未来解决神经网络效率问题的一种典型方法。
- en: VI-A4 Multimodal Learning for HPE
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A4 多模态学习在HPE中的应用
- en: 'Multimodal learning extracts features from multiple sensing modalities, which
    alleviates the complexity of unimodal methods. Recently, the multimodal learning-based
    approaches [[94](#bib.bib94), [35](#bib.bib35)] have been validated for robustness
    and accuracy in HPE tasks. We retrieve the literature by using the code of TITLE-ABS-KEY(("multi-modal"
    or"multimodal" or "IMUs" or "radio signal") and ("human pose estimation") and
    not ("distribution")) to observe and demonstrate the research trend. The number
    of publications over years (from 2012 to 2022) is shown in [4(d)](#S6.F4.sf4 "4(d)
    ‣ Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey"). Although the total number
    of publications implies a lack of attention in multimodal learning HPE, the steady
    growth of publications implies an increasing interest in this topic. The multimodal
    learning methods offer solutions and robustness to the vision-based model in addressing
    occlusions, which are promising solutions towards the in-the-wild challenge [[35](#bib.bib35)].
    The multimodal learning methods for HPE still need to be further investigated
    in the future.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习从多个传感模态中提取特征，从而减轻了单模态方法的复杂性。最近，基于多模态学习的方法[[94](#bib.bib94), [35](#bib.bib35)]在HPE任务中已被验证为具有鲁棒性和准确性。我们通过使用TITLE-ABS-KEY(("multi-modal"或"multimodal"或"IMUs"或"radio
    signal")和("human pose estimation")并且不包括("distribution"))的代码来检索文献，以观察和展示研究趋势。从2012年到2022年的出版物数量如[4(d)](#S6.F4.sf4
    "4(d) ‣ 图 4 ‣ VI-A 研究趋势 ‣ VI 研究趋势和挑战 ‣ 基于视觉的人体姿态估计：综述")所示。尽管总出版物数量表明多模态学习HPE关注较少，但出版物的稳步增长表明对这一主题的兴趣在增加。多模态学习方法为解决视觉模型中的遮挡问题提供了解决方案和鲁棒性，这些方法在应对现实世界挑战方面具有前景[[35](#bib.bib35)]。多模态学习方法在HPE中的应用仍需进一步研究。
- en: VI-B Research Challenges
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 研究挑战
- en: 'A remarkable HPE approach should consider both high accuracy and high efficiency.
    Although many studies have investigated the HPE with prominent performance, there
    are many considerable challenges to achieve both goals. An accurate HPE approach
    is generally demanded to deal with the challenges of various occlusions and personal
    appearances, estimate the depth of monocular image, and remain robust to image
    degeneration. In addition, the current biased datasets (e.g., incomprehensive
    outdoor 3D annotations, relatively rare uncommon poses) is a prevalent challenge
    for the practical accuracy of HPE in real-world scenarios. Moreover, implementing
    efficient deep learning-based HPE on resource-limited devices is a notorious challenge.
    The current deep learning-based HPE generally take a lot of computing time because
    of the large-scale networks. In particular, the computing time significantly creases
    with the creasing number of people for the crowd scenes. Therefore, a number-robust
    HPE algorithm is critical for implementing an efficient HPE. In this subsection,
    we summarize and discuss the challenges in HPE from two aspects: accuracy and
    efficiency.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一种显著的HPE方法应考虑高精度和高效率。尽管许多研究已调查具有突出的HPE性能，但实现这两个目标仍面临许多显著挑战。准确的HPE方法通常要求应对各种遮挡和个人外观的挑战，估计单目图像的深度，并保持对图像退化的鲁棒性。此外，当前存在的数据集偏差（例如，不全面的户外3D注释、相对稀有的不常见姿势）是现实场景中HPE实际准确性的普遍挑战。此外，在资源有限的设备上实现高效的基于深度学习的HPE是一个臭名昭著的挑战。由于大规模网络，当前的基于深度学习的HPE通常需要大量计算时间。特别是在拥挤场景中，随着人数的增加，计算时间显著增加。因此，一个数量鲁棒的HPE算法对于实现高效的HPE至关重要。在这一小节中，我们总结并讨论了从准确性和效率两个方面的HPE挑战。
- en: VI-B1 Challenges in Accurate HPE
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B1 准确HPE中的挑战
- en: Diverse Human Poses and Appearances
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多样化的人体姿势和外观
- en: A fundamental challenge in developing accurate HPE comes from the diversity
    of human poses [[12](#bib.bib12)]. Despite the vast range of human appearances,
    the human body entails high degree of freedom, demanding an advanced presentation
    ability for data-driven approaches in HPE. Additionally, image degeneration like
    motion blur and image defocus exists in video-based data [[44](#bib.bib44)], which
    also hinders HPE approaches from achieving remarkable performance.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 开发准确HPE的一个基本挑战来自于人体姿势的多样性[[12](#bib.bib12)]。尽管人体外观范围广泛，但人体具有高度自由度，需要在数据驱动的方法中具备先进的呈现能力。此外，视频数据中存在运动模糊和图像失焦等图像退化问题[[44](#bib.bib44)]，这也阻碍了HPE方法取得显著性能。
- en: Occlusions
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 遮挡
- en: Although current HPE methods perform outstanding performance on many public
    datasets, a well-known issue is noticeable performance degeneration caused by
    occlusions and highly deformable human body [[30](#bib.bib30), [39](#bib.bib39),
    [141](#bib.bib141)]. Self-occlusions and mutual occlusions could prompt the occlusions
    and environmental truncation, while mutual occlusions can occur extensively in
    crowd scenarios [[32](#bib.bib32)] to cause the performance to decline dramatically.
    The highly deformable human body can cause ambiguity in small-scale human instances
    [[141](#bib.bib141)] or the specific human poses as well. Thus, designing the
    powerful HPE method for occlusion scenarios with the capability of utilizing global
    information and prior knowledge would be a challenging issue.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前的HPE方法在许多公共数据集上表现出色，但一个众所周知的问题是由于遮挡和高度可变形的人体导致的性能退化[[30](#bib.bib30), [39](#bib.bib39),
    [141](#bib.bib141)]。自遮挡和相互遮挡可能引发遮挡和环境截断，而在拥挤场景中，相互遮挡可能广泛发生[[32](#bib.bib32)]，导致性能显著下降。高度可变形的人体可能导致小规模人体实例[[141](#bib.bib141)]或特定人体姿势的模糊。因此，为遮挡场景设计一个能够利用全局信息和先验知识的强大HPE方法将是一个具有挑战性的任务。
- en: Incomprehensive Datasets
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不全面的数据集
- en: To apply HPE approaches to practical applications, challenges could come from
    the gap between current incomprehensive datasets and real-world applications.
    For example, uncommon poses like falling down are less likely to appear in datasets,
    and the outdoors 3D HPE dataset is relatively rare. This gap leads to an imbalanced
    learning problem, which could hinder the applications of 3D HPE in the real world.
    Although current approaches can leverage semi-supervised learning [[104](#bib.bib104)]
    or synthetic dataset [[37](#bib.bib37)] to enrich the datasets, the semi-supervised
    methods still need a lot of quality training data [[98](#bib.bib98)]. Current
    datasets lack realistic simulation of lighting effects, clothing meshes, and environment
    interactions [[142](#bib.bib142)]. Therefore, it is difficult to transfer the
    trained neural networks from the simulation to real-world applications. Developing
    a remarkable HPE on current incomprehensive datasets is still challenging, particularly
    for deployment in complex environments.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 应用HPE方法到实际应用中，可能会面临当前不完善的数据集与真实世界应用之间的差距。例如，像跌倒这样的罕见姿态在数据集中出现的可能性较小，室外3D HPE数据集也相对稀缺。这种差距导致了不平衡的学习问题，这可能会阻碍3D
    HPE在实际世界中的应用。尽管当前的方法可以利用半监督学习[[104](#bib.bib104)]或合成数据集[[37](#bib.bib37)]来丰富数据集，但半监督方法仍需大量高质量的训练数据[[98](#bib.bib98)]。当前的数据集缺乏对照明效果、服装网格和环境交互的现实模拟[[142](#bib.bib142)]。因此，将训练好的神经网络从模拟转移到实际应用中是困难的。在当前不完善的数据集上开发出卓越的HPE仍然具有挑战性，特别是在复杂环境中的部署。
- en: '| Types | Studies | URL (Open source code) | Remarks |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 研究 | URL（开源代码） | 备注 |'
- en: '| 2D/3D HPE | MMPose | [https://github.com/open-mmlab/mmpose](https://github.com/open-mmlab/mmpose)
    | Well-known platform |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 2D/3D HPE | MMPose | [https://github.com/open-mmlab/mmpose](https://github.com/open-mmlab/mmpose)
    | 知名平台 |'
- en: '| 2D HPE | Associative embedding [[63](#bib.bib63)] | [https://github.com/princeton-vl/pose-ae-train](https://github.com/princeton-vl/pose-ae-train)
    | A SOTA grouping for bottom-up approaches |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 2D HPE | 关联嵌入 [[63](#bib.bib63)] | [https://github.com/princeton-vl/pose-ae-train](https://github.com/princeton-vl/pose-ae-train)
    | 一种SOTA的自下而上分组方法 |'
- en: '| Hourglass [[41](#bib.bib41)] | [https://github.com/princeton-vl/pose-hg-demo](https://github.com/princeton-vl/pose-hg-demo)
    | Effective yet simple backbone |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 沙漏 [[41](#bib.bib41)] | [https://github.com/princeton-vl/pose-hg-demo](https://github.com/princeton-vl/pose-hg-demo)
    | 有效且简单的骨干网络 |'
- en: '| OpenPose [[40](#bib.bib40)] | [https://github.com/CMU-Perceptual-Computing-Lab/openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
    | Real-time & bottom-up |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| OpenPose [[40](#bib.bib40)] | [https://github.com/CMU-Perceptual-Computing-Lab/openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
    | 实时&自下而上 |'
- en: '| AlphaPose [[39](#bib.bib39)] | [https://github.com/MVIG-SJTU/AlphaPose](https://github.com/MVIG-SJTU/AlphaPose)
    | Real-time & top-down |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| AlphaPose [[39](#bib.bib39)] | [https://github.com/MVIG-SJTU/AlphaPose](https://github.com/MVIG-SJTU/AlphaPose)
    | 实时&自上而下 |'
- en: '| Higher-HRNet [[72](#bib.bib72)] | [https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation](https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation)
    | A SOTA bottom-up approach |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Higher-HRNet [[72](#bib.bib72)] | [https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation](https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation)
    | 一种SOTA的自下而上方法 |'
- en: '| HRNet [[10](#bib.bib10)] | [https://github.com/HRNet/HRNet-Human-Pose-Estimation](https://github.com/HRNet/HRNet-Human-Pose-Estimation)
    | A SOTA top-down approach |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| HRNet [[10](#bib.bib10)] | [https://github.com/HRNet/HRNet-Human-Pose-Estimation](https://github.com/HRNet/HRNet-Human-Pose-Estimation)
    | 一种SOTA的自上而下方法 |'
- en: '| RLE [[143](#bib.bib143)] | [https://github.com/Jeff-sjtu/res-loglikelihood-regression](https://github.com/Jeff-sjtu/res-loglikelihood-regression)
    | A SOTA regression-based HPE |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| RLE [[143](#bib.bib143)] | [https://github.com/Jeff-sjtu/res-loglikelihood-regression](https://github.com/Jeff-sjtu/res-loglikelihood-regression)
    | 一种基于回归的SOTA HPE |'
- en: '| UDP-POSE [[144](#bib.bib144)] | [https://github.com/HuangJunJie2017/UDP-Pose](https://github.com/HuangJunJie2017/UDP-Pose)
    | 1st in ICCV 2019 COCO keypoint challenge |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| UDP-POSE [[144](#bib.bib144)] | [https://github.com/HuangJunJie2017/UDP-Pose](https://github.com/HuangJunJie2017/UDP-Pose)
    | ICCV 2019 COCO关键点挑战赛第一名 |'
- en: '| DARK [[58](#bib.bib58)] | [https://ilovepose.github.io/coco/](https://ilovepose.github.io/coco/)
    | 2nd in ICCV 2019 COCO keypoint challenge |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| DARK [[58](#bib.bib58)] | [https://ilovepose.github.io/coco/](https://ilovepose.github.io/coco/)
    | ICCV 2019 COCO关键点挑战赛第二名 |'
- en: '| Lite-HRNet [[145](#bib.bib145)] | [https://github.com/HRNet/Lite-HRNet](https://github.com/HRNet/Lite-HRNet)
    | Lightweight HRNet-based model |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Lite-HRNet [[145](#bib.bib145)] | [https://github.com/HRNet/Lite-HRNet](https://github.com/HRNet/Lite-HRNet)
    | 轻量级HRNet基础模型 |'
- en: '| Lightweight OpenPose [[146](#bib.bib146)] | [https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch)
    | Real-time on CPU |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Lightweight OpenPose [[146](#bib.bib146)] | [https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch)
    | 在 CPU 上实时运行 |'
- en: '| BlazePose [[64](#bib.bib64)] | [https://google.github.io/mediapipe/solutions/pose.html](https://google.github.io/mediapipe/solutions/pose.html)
    | Real-time 2D single-person HPE driven by MediaPipe [[65](#bib.bib65)] |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| BlazePose [[64](#bib.bib64)] | [https://google.github.io/mediapipe/solutions/pose.html](https://google.github.io/mediapipe/solutions/pose.html)
    | 由 MediaPipe [[65](#bib.bib65)] 驱动的实时 2D 单人体 HPE |'
- en: '| PRTR [[60](#bib.bib60)] | [https://github.com/mlpc-ucsd/PRTR](https://github.com/mlpc-ucsd/PRTR)
    | 2D Pose Regression transformer |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| PRTR [[60](#bib.bib60)] | [https://github.com/mlpc-ucsd/PRTR](https://github.com/mlpc-ucsd/PRTR)
    | 2D 姿态回归变换器 |'
- en: '|  | DCPose [[147](#bib.bib147)] | [https://github.com/Pose-Group/DCPose](https://github.com/Pose-Group/DCPose)
    | 1st in PoseTrack2017 & PoseTrack2018 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | DCPose [[147](#bib.bib147)] | [https://github.com/Pose-Group/DCPose](https://github.com/Pose-Group/DCPose)
    | PoseTrack2017 & PoseTrack2018 第一名 |'
- en: '| 3D HPE | Epipolar transformer [[89](#bib.bib89)] | [https://github.com/yihui-he/epipolar-transformers](https://github.com/yihui-he/epipolar-transformers)
    | A SOTA multi-view approach |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 3D HPE | Epipolar transformer [[89](#bib.bib89)] | [https://github.com/yihui-he/epipolar-transformers](https://github.com/yihui-he/epipolar-transformers)
    | 一种 SOTA 多视角方法 |'
- en: '| Learnable triangulation [[87](#bib.bib87)] | [https://saic-violet.github.io/learnable-triangulation/](https://saic-violet.github.io/learnable-triangulation/)
    | A SOTA multi-view approach |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Learnable triangulation [[87](#bib.bib87)] | [https://saic-violet.github.io/learnable-triangulation/](https://saic-violet.github.io/learnable-triangulation/)
    | 一种 SOTA 多视角方法 |'
- en: '| SMAP [[148](#bib.bib148)] | [https://github.com/zju3dv/SMAP](https://github.com/zju3dv/SMAP)
    | SOTA single-view multi-person |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| SMAP [[148](#bib.bib148)] | [https://github.com/zju3dv/SMAP](https://github.com/zju3dv/SMAP)
    | SOTA 单视角多人体 |'
- en: '| DOPE [[149](#bib.bib149)] | [https://github.com/naver/dope](https://github.com/naver/dope)
    | Real-time whole-body 3D HPE |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| DOPE [[149](#bib.bib149)] | [https://github.com/naver/dope](https://github.com/naver/dope)
    | 实时全身 3D HPE |'
- en: '| VNect [[150](#bib.bib150)] | [http://gvv.mpi-inf.mpg.de/projects/VNect/](http://gvv.mpi-inf.mpg.de/projects/VNect/)
    | Real-time single-view approach |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| VNect [[150](#bib.bib150)] | [http://gvv.mpi-inf.mpg.de/projects/VNect/](http://gvv.mpi-inf.mpg.de/projects/VNect/)
    | 实时单视角方法 |'
- en: '| Synthetic occlusion [[151](#bib.bib151)] | [https://github.com/isarandi/synthetic-occlusion](https://github.com/isarandi/synthetic-occlusion)
    | 1st place in ECCV2018 3D HPE Challenge |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Synthetic occlusion [[151](#bib.bib151)] | [https://github.com/isarandi/synthetic-occlusion](https://github.com/isarandi/synthetic-occlusion)
    | ECCV2018 3D HPE 挑战赛第一名 |'
- en: '| Integral regression [[152](#bib.bib152)] | [https://github.com/JimmySuen/integral-human-pose](https://github.com/JimmySuen/integral-human-pose)
    | 2nd place in ECCV2018 3D HPE Challenge |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Integral regression [[152](#bib.bib152)] | [https://github.com/JimmySuen/integral-human-pose](https://github.com/JimmySuen/integral-human-pose)
    | ECCV2018 3D HPE 挑战赛第二名 |'
- en: '| PoseFormer [[103](#bib.bib103)] | [https://github.com/zczcwh/PoseFormer](https://github.com/zczcwh/PoseFormer)
    | 3D pose transformer |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| PoseFormer [[103](#bib.bib103)] | [https://github.com/zczcwh/PoseFormer](https://github.com/zczcwh/PoseFormer)
    | 3D 姿态变换器 |'
- en: '| Top-down & bottom-up Integration [[153](#bib.bib153)] | [https://github.com/3dpose/3D-Multi-Person-Pose](https://github.com/3dpose/3D-Multi-Person-Pose)
    | A SOTA monocular multi-person 3D HPE |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Top-down & bottom-up Integration [[153](#bib.bib153)] | [https://github.com/3dpose/3D-Multi-Person-Pose](https://github.com/3dpose/3D-Multi-Person-Pose)
    | 一种 SOTA 单目多人体 3D HPE |'
- en: '| Normalizing flows [[154](#bib.bib154)] | [https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows](https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows)
    | A SOTA monocular 3D HPE |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Normalizing flows [[154](#bib.bib154)] | [https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows](https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows)
    | 一种 SOTA 单目 3D HPE |'
- en: '| PoseAug [[86](#bib.bib86)] | [https://github.com/jfzhang95/PoseAug](https://github.com/jfzhang95/PoseAug)
    | A data augmentation framework for 3D HPE |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| PoseAug [[86](#bib.bib86)] | [https://github.com/jfzhang95/PoseAug](https://github.com/jfzhang95/PoseAug)
    | 一种用于 3D HPE 的数据增强框架 |'
- en: 'TABLE VI: The open-source code of the state-of-the-art HPE methods.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：最先进的 HPE 方法的开源代码。
- en: VI-B2 Challenges in Efficient HPE
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B2 高效 HPE 面临的挑战
- en: Computation-intensive Neural Networks
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算密集型神经网络
- en: Eventually, the HPE approach needs to be implemented for applications in the
    real world. However, the state-of-the-art neural networks [[10](#bib.bib10), [148](#bib.bib148)]
    are generally hard to be implemented on mobile devices or embedded devices as
    their enormous computational cost. Thus, it is crucial to design lightweight neural
    networks for efficient HPE. The existing methods of designing lightweight neural
    networks are mainly manual design and heuristic design (e.g., NAS [[140](#bib.bib140)]).
    However, the method of manual design is hard to balance the accuracy and network
    size [[145](#bib.bib145)]. NAS-based methods [[140](#bib.bib140), [17](#bib.bib17)]
    generally need various computational cost even weeks of CPU time. Therefore, developing
    lightweight neural networks for HPE is still a challenging task.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，HPE 方法需要在实际应用中实现。然而，最先进的神经网络[[10](#bib.bib10), [148](#bib.bib148)]通常难以在移动设备或嵌入式设备上实现，因为它们巨大的计算成本。因此，为高效
    HPE 设计轻量级神经网络至关重要。现有的轻量级神经网络设计方法主要有人工设计和启发式设计（例如，NAS [[140](#bib.bib140)]）。然而，人工设计方法很难平衡准确性和网络规模[[145](#bib.bib145)]。基于
    NAS 的方法[[140](#bib.bib140), [17](#bib.bib17)]通常需要各种计算成本，甚至需要几周的 CPU 时间。因此，为 HPE
    开发轻量级神经网络仍然是一项具有挑战性的任务。
- en: Time-Consuming MPPE
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 耗时的 MPPE
- en: Currently, MPPE algorithms consume increasing computation time over the increasing
    number of targeted persons. Top-down approaches estimate the pose of each detected
    person after the person detection stage. Bottom-up approaches [[63](#bib.bib63),
    [40](#bib.bib40)] predict similarity values between keypoints and employ a matching
    algorithm (e.g., Hungarian algorithm) for grouping keypoints. Note that top-down
    and bottom-up approaches are two-stage methods due to top-down approaches require
    an extra detection stage and bottom-up approaches require an extra grouping stage,
    besides estimating keypoint locations.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，MPPE 算法随着目标人数的增加而消耗越来越多的计算时间。自上而下的方法在人员检测阶段之后估计每个检测到的人的姿态。自下而上方法[[63](#bib.bib63),
    [40](#bib.bib40)]预测关键点之间的相似度值，并使用匹配算法（例如，匈牙利算法）对关键点进行分组。请注意，自上而下和自下而上方法都是两阶段方法，因为自上而下方法需要额外的检测阶段，自下而上方法需要额外的分组阶段，除了估计关键点位置。
- en: Compared to two-stage methods, single-stage methods generally perform superior
    in terms of computational cost. A promising single-stage method [[69](#bib.bib69)]
    predicts the locations of persons and keypoints’ offsets to each location. However,
    the single-stage methods are generally not as competitive as the state-of-the-art
    two-stage methods [[72](#bib.bib72), [10](#bib.bib10)] in terms of accuracy. How
    to develop a desired efficient MPPE is still an interesting challenge, particularly
    for applications in the real world.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 与两阶段方法相比，单阶段方法通常在计算成本方面表现更优。一种有前景的单阶段方法[[69](#bib.bib69)]预测了每个人的位置和关键点相对于每个位置的偏移量。然而，单阶段方法在准确性方面通常不如最先进的两阶段方法[[72](#bib.bib72),
    [10](#bib.bib10)]具有竞争力。如何开发出一种理想的高效 MPPE 仍然是一个有趣的挑战，特别是对于实际应用。
- en: VII Conclusions
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: 'In this paper, we presented an up-to-date and in-depth overview of the deep
    learning approaches in vision-based HPE. We systematically introduced the preliminary
    knowledge in HPE and reviewed the HPE approaches in two categories: 2D-based approaches
    and 3D-based approaches. We discussed a number of interesting applications of
    deep learning-based HPE. Finally, we pointed out the research trends via bibliometrics,
    raised the challenging issues, and provided insightful recommendations for future
    research. To help readers to reproduce the state-of-the-art methods, we summarized
    the open-source codes of the well-known studies for 2D and 3D deep learning-based
    HPE in [Table VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets ‣ VI-B1 Challenges
    in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research Trends and Challenges
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"), which could
    help readers easily implement their HPE tasks. This article provides a meaningful
    overview as introductory material for beginners to deep learning-based HPE, as
    well as supplementary material for advanced researchers.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们提供了基于视觉的人体姿态估计（HPE）中深度学习方法的最新和深入的概述。我们系统地介绍了HPE的基础知识，并回顾了HPE方法的两大类：基于2D的方法和基于3D的方法。我们讨论了许多基于深度学习的HPE的有趣应用。最后，我们通过文献计量学指出了研究趋势，提出了挑战性问题，并为未来的研究提供了有见地的建议。为了帮助读者重现最先进的方法，我们在[表
    VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets ‣ VI-B1 Challenges in Accurate
    HPE ‣ VI-B Research Challenges ‣ VI Research Trends and Challenges ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey")中总结了知名研究的2D和3D深度学习HPE的开源代码，这可以帮助读者轻松实现他们的HPE任务。本文为初学者提供了基于深度学习的HPE的有意义的概述，同时也作为高级研究人员的补充材料。'
- en: References
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Z. Liu, H. Zhang, Z. Chen, Z. Wang, and W. Ouyang, “Disentangling and unifying
    graph convolutions for skeleton-based action recognition,” in *CVPR 2020*, pp.
    143–152.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Z. Liu, H. Zhang, Z. Chen, Z. Wang, and W. Ouyang, “解构和统一骨架基础的动作识别图卷积”，发表于*CVPR
    2020*，第143–152页。'
- en: '[2] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman,
    and A. Blake, “Real-time human pose recognition in parts from single depth images,”
    in *CVPR 2011*, pp. 1297–1304.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A.
    Kipman, and A. Blake, “实时人体姿态识别：从单幅深度图像中识别部分”，发表于*CVPR 2011*，第1297–1304页。'
- en: '[3] S.-R. Ke, L. Zhu, J.-N. Hwang, H.-I. Pai, K.-M. Lan, and C.-P. Liao, “Real-time
    3D human pose estimation from monocular view with applications to event detection
    and video gaming,” in *2010 7th IEEE International Conference on Advanced Video
    and Signal Based Surveillance*.   IEEE, 2010, pp. 489–496.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S.-R. Ke, L. Zhu, J.-N. Hwang, H.-I. Pai, K.-M. Lan, and C.-P. Liao, “从单目视图中实时3D人体姿态估计及其在事件检测和视频游戏中的应用”，发表于*2010年第七届IEEE国际先进视频和信号监控会议*，IEEE，2010年，第489–496页。'
- en: '[4] J. Wang, K. Qiu, H. Peng, J. Fu, and J. Zhu, “AI coach: Deep human pose
    estimation and analysis for personalized athletic training assistance,” in *Proceedings
    of the 27th ACM International Conference on Multimedia*, 2019, pp. 374–382.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Wang, K. Qiu, H. Peng, J. Fu, and J. Zhu, “AI教练：用于个性化运动训练辅助的深度人体姿态估计与分析”，发表于*Proceedings
    of the 27th ACM International Conference on Multimedia*，2019年，第374–382页。'
- en: '[5] S. Park, J. Yong Chang, H. Jeong, J.-H. Lee, and J.-Y. Park, “Accurate
    and efficient 3D human pose estimation algorithm using single depth images for
    pose analysis in golf,” in *CVPR Workshops 2017*, pp. 49–57.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Park, J. Yong Chang, H. Jeong, J.-H. Lee, and J.-Y. Park, “使用单幅深度图像进行高尔夫姿态分析的准确高效3D人体姿态估计算法”，发表于*CVPR
    Workshops 2017*，第49–57页。'
- en: '[6] Z. Yang, W. Zhu, W. Wu, C. Qian, Q. Zhou, B. Zhou, and C. C. Loy, “TransMoMo:
    Invariance-driven unsupervised video motion retargeting,” in *CVPR 2020*, pp.
    5306–5315.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Z. Yang, W. Zhu, W. Wu, C. Qian, Q. Zhou, B. Zhou, and C. C. Loy, “TransMoMo：基于不变性的无监督视频运动重定向”，发表于*CVPR
    2020*，第5306–5315页。'
- en: '[7] A. Hornung, E. Dekkers, and L. Kobbelt, “Character animation from 2D pictures
    and 3D motion data,” *ACM Transactions on Graphics*, vol. 26, no. 1, pp. 1–es,
    2007.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Hornung, E. Dekkers, and L. Kobbelt, “基于2D图片和3D运动数据的角色动画”，*ACM Transactions
    on Graphics*，第26卷，第1期，第1–es页，2007年。'
- en: '[8] N. S. Willett, H. V. Shin, Z. Jin, W. Li, and A. Finkelstein, “Pose2Pose:
    pose selection and transfer for 2D character animation,” in *Proceedings of the
    25th International Conference on Intelligent User Interfaces*, 2020, pp. 88–99.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] N. S. Willett, H. V. Shin, Z. Jin, W. Li, and A. Finkelstein, “Pose2Pose：2D角色动画中的姿态选择和转移”，发表于*Proceedings
    of the 25th International Conference on Intelligent User Interfaces*，2020年，第88–99页。'
- en: '[9] A. Toshev and C. Szegedy, “DeepPose: Human pose estimation via deep neural
    networks,” in *CVPR 2014*, pp. 1653–1660.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Toshev and C. Szegedy, “DeepPose：通过深度神经网络进行人体姿态估计”，发表于*CVPR 2014*，第1653–1660页。'
- en: '[10] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation
    learning for human pose estimation,” in *CVPR 2019*, pp. 5693–5703.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] K. Sun, B. Xiao, D. Liu, 和 J. Wang, “用于人体姿态估计的深度高分辨率表示学习，” 见 *CVPR 2019*，第
    5693–5703 页。'
- en: '[11] D. Hogg, “Model-based vision: a program to see a walking person,” *Image
    and Vision computing*, vol. 1, no. 1, pp. 5–20, 1983.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] D. Hogg, “基于模型的视觉：一个看见行走人的程序，” *图像与视觉计算*，第 1 卷，第 1 期，第 5–20 页，1983年。'
- en: '[12] T. B. Moeslund and E. Granum, “A survey of computer vision-based human
    motion capture,” *Computer vision and image understanding*, vol. 81, no. 3, pp.
    231–268, 2001.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] T. B. Moeslund 和 E. Granum, “基于计算机视觉的人体运动捕捉综述，” *计算机视觉与图像理解*，第 81 卷，第
    3 期，第 231–268 页，2001年。'
- en: '[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Advances in neural information processing
    systems*, vol. 25, pp. 1097–1105, 2012.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络的 Imagenet 分类，”
    *神经信息处理系统进展*，第 25 卷，第 1097–1105 页，2012年。'
- en: '[14] G. Lan, J. Benito-Picazo, D. M. Roijers, E. Domínguez, and A. Eiben, “Real-time
    robot vision on low-performance computing hardware,” in *2018 15th International
    Conference on Control, Automation, Robotics and Vision (ICARCV)*.   IEEE, 2018,
    pp. 1959–1965.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] G. Lan, J. Benito-Picazo, D. M. Roijers, E. Domínguez, 和 A. Eiben, “在低性能计算硬件上的实时机器人视觉，”
    见 *2018年第15届国际控制、自动化、机器人与视觉会议 (ICARCV)*。 IEEE，2018年，第 1959–1965 页。'
- en: '[15] G. Lan, L. De Vries, and S. Wang, “Evolving efficient deep neural networks
    for real-time object recognition,” in *2019 IEEE Symposium Series on Computational
    Intelligence (SSCI)*.   IEEE, 2019, pp. 2571–2578.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] G. Lan, L. De Vries, 和 S. Wang, “为实时物体识别演化高效的深度神经网络，” 见 *2019 IEEE 计算智能研讨会系列
    (SSCI)*。 IEEE，2019年，第 2571–2578 页。'
- en: '[16] M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall,
    and B. Schiele, “PoseTrack: A benchmark for human pose estimation and tracking,”
    in *CVPR 2018*, pp. 5167–5176.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall,
    和 B. Schiele, “PoseTrack：一个用于人体姿态估计和跟踪的基准，” 见 *CVPR 2018*，第 5167–5176 页。'
- en: '[17] W. McNally, K. Vats, A. Wong, and J. McPhee, “EvoPose2D: Pushing the boundaries
    of 2d human pose estimation using accelerated neuroevolution with weight transfer,”
    *IEEE Access*, vol. 9, pp. 139 403–139 414, 2021.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] W. McNally, K. Vats, A. Wong, 和 J. McPhee, “EvoPose2D：利用加速神经进化和权重转移推动2D人体姿态估计的极限，”
    *IEEE Access*，第 9 卷，第 139403–139414 页，2021年。'
- en: '[18] Q. Dang, J. Yin, B. Wang, and W. Zheng, “Deep learning based 2D human
    pose estimation: A survey,” *Tsinghua Science and Technology*, vol. 24, no. 6,
    pp. 663–676, 2019.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Q. Dang, J. Yin, B. Wang, 和 W. Zheng, “基于深度学习的2D人体姿态估计：综述，” *清华科学技术*，第
    24 卷，第 6 期，第 663–676 页，2019年。'
- en: '[19] R. Poppe, “A survey on vision-based human action recognition,” *Image
    and vision computing*, vol. 28, no. 6, pp. 976–990, 2010.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] R. Poppe, “基于视觉的人类动作识别综述，” *图像与视觉计算*，第 28 卷，第 6 期，第 976–990 页，2010年。'
- en: '[20] Y. Chen, Y. Tian, and M. He, “Monocular human pose estimation: A survey
    of deep learning-based methods,” *Computer Vision and Image Understanding*, vol.
    192, p. 102897, 2020.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Chen, Y. Tian, 和 M. He, “单目人体姿态估计：基于深度学习的方法综述，” *计算机视觉与图像理解*，第 192
    卷，第 102897 页，2020年。'
- en: '[21] W. Liu and T. Mei, “Recent advances of monocular 2D and 3D human pose
    estimation: A deep learning perspective,” *ACM Computing Surveys (CSUR)*, 2022.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] W. Liu 和 T. Mei, “单目2D和3D人体姿态估计的最新进展：深度学习视角，” *ACM计算调查 (CSUR)*，2022年。'
- en: '[22] W. Gong, X. Zhang, J. Gonzàlez, A. Sobral, T. Bouwmans, C. Tu, and E.-h.
    Zahzah, “Human pose estimation from monocular images: A comprehensive survey,”
    *Sensors*, vol. 16, no. 12, p. 1966, 2016.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] W. Gong, X. Zhang, J. Gonzàlez, A. Sobral, T. Bouwmans, C. Tu, 和 E.-h.
    Zahzah, “从单目图像中进行人体姿态估计：综合综述，” *传感器*，第 16 卷，第 12 期，第 1966 页，2016年。'
- en: '[23] L. M. Dang, K. Min, H. Wang, M. J. Piran, C. H. Lee, and H. Moon, “Sensor-based
    and vision-based human activity recognition: A comprehensive survey,” *Pattern
    Recognition*, vol. 108, p. 107561, 2020.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] L. M. Dang, K. Min, H. Wang, M. J. Piran, C. H. Lee, 和 H. Moon, “基于传感器和视觉的人体活动识别：综合综述，”
    *模式识别*，第 108 卷，第 107561 页，2020年。'
- en: '[24] R. Gadhiya and N. Kalani, “Analysis of deep learning based pose estimation
    techniques for locating landmarks on human body parts,” in *2021 International
    Conference on Circuits, Controls and Communications (CCUBE)*.   IEEE, 2021, pp.
    1–4.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] R. Gadhiya 和 N. Kalani, “基于深度学习的姿态估计技术在定位人体部位标记中的分析，” 见 *2021 国际电路、控制与通信会议
    (CCUBE)*。 IEEE，2021年，第 1–4 页。'
- en: '[25] J. Wang, S. Tan, X. Zhen, S. Xu, F. Zheng, Z. He, and L. Shao, “Deep 3D
    human pose estimation: A review,” *Computer Vision and Image Understanding*, p.
    103225, 2021.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. Wang, S. Tan, X. Zhen, S. Xu, F. Zheng, Z. He, 和 L. Shao, “深度3D人体姿态估计：综述，”
    *计算机视觉与图像理解*，第 103225 页，2021年。'
- en: '[26] S. Johnson and M. Everingham, “Clustered pose and nonlinear appearance
    models for human pose estimation,” in *Proceedings of the British Machine Vision
    Conference*, 2010.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Johnson 和 M. Everingham，“集群姿态和非线性外观模型用于人体姿态估计”，在 *英国机器视觉会议论文集*，2010
    年。'
- en: '[27] B. Sapp and B. Taskar, “MODEC: Multimodal decomposable models for human
    pose estimation,” in *CVPR 2013*.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] B. Sapp 和 B. Taskar，“MODEC: 多模态可分解模型用于人体姿态估计”，在 *CVPR 2013*。'
- en: '[28] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards understanding
    action recognition,” in *ICCV 2013*, pp. 3192–3199.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] H. Jhuang, J. Gall, S. Zuffi, C. Schmid 和 M. J. Black，“朝向理解动作识别”，在 *ICCV
    2013*，第 3192–3199 页。'
- en: '[29] W. Zhang, M. Zhu, and K. G. Derpanis, “From actemes to action: A strongly-supervised
    representation for detailed action understanding,” in *ICCV 2013*, pp. 2248–2255.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] W. Zhang, M. Zhu 和 K. G. Derpanis，“从动作体到动作：一种强监督表示以详细理解动作”，在 *ICCV 2013*，第
    2248–2255 页。'
- en: '[30] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, “2D human pose
    estimation: New benchmark and state of the art analysis,” in *CVPR 2014*, pp.
    3686–3693.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] M. Andriluka, L. Pishchulin, P. Gehler 和 B. Schiele，“2D 人体姿态估计：新的基准和现状分析”，在
    *CVPR 2014*，第 3686–3693 页。'
- en: '[31] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft COCO: Common objects in context,” in *ECCV 2014*,
    pp. 740–755.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár
    和 C. L. Zitnick，“Microsoft COCO: 上下文中的常见物体”，在 *ECCV 2014*，第 740–755 页。'
- en: '[32] J. Li, C. Wang, H. Zhu, Y. Mao, H.-S. Fang, and C. Lu, “CrowdPose: Efficient
    crowded scenes pose estimation and a new benchmark,” in *CVPR 2019*, pp. 10 863–10 872.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Li, C. Wang, H. Zhu, Y. Mao, H.-S. Fang 和 C. Lu，“CrowdPose: 高效的拥挤场景姿态估计及新基准”，在
    *CVPR 2019*，第 10,863–10,872 页。'
- en: '[33] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6M: Large
    scale datasets and predictive methods for 3D human sensing in natural environments,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 36, no. 7,
    pp. 1325–1339, jul 2014.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C. Ionescu, D. Papava, V. Olaru 和 C. Sminchisescu，“Human3.6M: 大规模数据集和自然环境中
    3D 人体感测的预测方法”，*IEEE 计算机视觉与模式分析汇刊*，第 36 卷，第 7 期，第 1325–1339 页，2014 年 7 月。'
- en: '[34] H. Joo, T. Simon, X. Li, H. Liu, L. Tan, L. Gui, S. Banerjee, T. Godisart,
    B. Nabbe, I. Matthews *et al.*, “Panoptic studio: A massively multiview system
    for social interaction capture,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 41, no. 1, pp. 190–204, 2017.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] H. Joo, T. Simon, X. Li, H. Liu, L. Tan, L. Gui, S. Banerjee, T. Godisart,
    B. Nabbe, I. Matthews *等*，“全景工作室：一个用于社会互动捕捉的大规模多视角系统”，*IEEE 计算机视觉与模式分析汇刊*，第 41
    卷，第 1 期，第 190–204 页，2017 年。'
- en: '[35] T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll,
    “Recovering accurate 3D human pose in the wild using IMUs and a moving camera,”
    in *ECCV 2018*, pp. 601–617.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn 和 G. Pons-Moll，“使用
    IMU 和移动相机在野外恢复准确的 3D 人体姿态”，在 *ECCV 2018*，第 601–617 页。'
- en: '[36] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, and C. Theobalt,
    “Monocular 3D human pose estimation in the wild using improved cnn supervision,”
    in *2017 international conference on 3D vision*.   IEEE, 2017, pp. 506–516.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu 和 C. Theobalt，“使用改进的
    cnn 监督在野外进行单目 3D 人体姿态估计”，在 *2017 国际 3D 视觉会议*，IEEE，2017，第 506–516 页。'
- en: '[37] M. Fabbri, F. Lanzi, S. Calderara, A. Palazzi, R. Vezzani, and R. Cucchiara,
    “Learning to detect and track visible and occluded body joints in a virtual world,”
    in *ECCV 2018*, pp. 430–446.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. Fabbri, F. Lanzi, S. Calderara, A. Palazzi, R. Vezzani 和 R. Cucchiara，“学习在虚拟世界中检测和跟踪可见和遮挡的身体关节”，在
    *ECCV 2018*，第 430–446 页。'
- en: '[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR 2016*, June.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] K. He, X. Zhang, S. Ren 和 J. Sun，“用于图像识别的深度残差学习”，在 *CVPR 2016*，6 月。'
- en: '[39] H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu, “RMPE: Regional multi-person
    pose estimation,” in *ICCV 2017*, pp. 2334–2343.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] H.-S. Fang, S. Xie, Y.-W. Tai 和 C. Lu，“RMPE: 区域多人体姿态估计”，在 *ICCV 2017*，第
    2334–2343 页。'
- en: '[40] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2D
    pose estimation using part affinity fields,” in *CVPR 2017*, pp. 7291–7299.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Z. Cao, T. Simon, S.-E. Wei 和 Y. Sheikh，“实时多人人体 2D 姿态估计使用部件关联场”，在 *CVPR
    2017*，第 7291–7299 页。'
- en: '[41] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for human
    pose estimation,” in *European conference on computer vision*.   Springer, 2016,
    pp. 483–499.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Newell, K. Yang 和 J. Deng，“用于人体姿态估计的堆叠沙漏网络”，在 *欧洲计算机视觉会议*，Springer，2016，第
    483–499 页。'
- en: '[42] Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun, “Cascaded pyramid
    network for multi-person pose estimation,” in *CVPR 2018*, pp. 7103–7112.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, 和 J. Sun，“用于多人姿态估计的级联金字塔网络”，在
    *CVPR 2018*，第7103–7112页。'
- en: '[43] B. Xiao, H. Wu, and Y. Wei, “Simple baselines for human pose estimation
    and tracking,” in *Proceedings of the European conference on computer vision (ECCV)*,
    2018, pp. 466–481.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] B. Xiao, H. Wu, 和 Y. Wei，“用于人体姿态估计和跟踪的简单基线”，在 *欧洲计算机视觉会议论文集（ECCV）*，2018年，第466–481页。'
- en: '[44] Y. Luo, J. Ren, Z. Wang, W. Sun, J. Pan, J. Liu, J. Pang, and L. Lin,
    “LSTM pose machines,” in *CVPR 2018*, pp. 5207–5215.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Luo, J. Ren, Z. Wang, W. Sun, J. Pan, J. Liu, J. Pang, 和 L. Lin，“LSTM姿态机器”，在
    *CVPR 2018*，第5207–5215页。'
- en: '[45] B. Artacho and A. Savakis, “UniPose: Unified human pose estimation in
    single images and videos,” in *CVPR 2020*, pp. 7035–7044.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] B. Artacho 和 A. Savakis，“UniPose：单幅图像和视频中的统一人体姿态估计”，在 *CVPR 2020*，第7035–7044页。'
- en: '[46] Y. Bin, Z.-M. Chen, X.-S. Wei, X. Chen, C. Gao, and N. Sang, “Structure-aware
    human pose estimation with graph convolutional networks,” *Pattern Recognition*,
    vol. 106, p. 107410, 2020.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Bin, Z.-M. Chen, X.-S. Wei, X. Chen, C. Gao, 和 N. Sang，“带有图卷积网络的结构感知人体姿态估计”，*模式识别*，第106卷，第107410页，2020年。'
- en: '[47] Z. Qiu, K. Qiu, J. Fu, and D. Fu, “DGCN: Dynamic graph convolutional network
    for efficient multi-person pose estimation,” in *AAAI 2020*, vol. 34, no. 07,
    pp. 11 924–11 931.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Z. Qiu, K. Qiu, J. Fu, 和 D. Fu，“DGCN：用于高效多人人体姿态估计的动态图卷积网络”，在 *AAAI 2020*，第34卷，第07期，第11,924–11,931页。'
- en: '[48] L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. N. Metaxas, “Semantic graph
    convolutional networks for 3D human pose regression,” in *CVPR 2019*, pp. 3425–3435.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] L. Zhao, X. Peng, Y. Tian, M. Kapadia, 和 D. N. Metaxas，“用于3D人体姿态回归的语义图卷积网络”，在
    *CVPR 2019*，第3425–3435页。'
- en: '[49] W. Hu, C. Zhang, F. Zhan, L. Zhang, and T.-T. Wong, “Conditional directed
    graph convolution for 3D human pose estimation,” in *Proceedings of the 29th ACM
    International Conference on Multimedia*, 2021, pp. 602–611.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] W. Hu, C. Zhang, F. Zhan, L. Zhang, 和 T.-T. Wong，“用于3D人体姿态估计的条件定向图卷积”，在
    *第29届ACM国际多媒体会议论文集*，2021年，第602–611页。'
- en: '[50] Y. Cai, L. Ge, J. Liu, J. Cai, T.-J. Cham, J. Yuan, and N. M. Thalmann,
    “Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional
    networks,” in *ICCV 2019*, pp. 2272–2281.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. Cai, L. Ge, J. Liu, J. Cai, T.-J. Cham, J. Yuan, 和 N. M. Thalmann，“通过图卷积网络利用时空关系进行3D姿态估计”，在
    *ICCV 2019*，第2272–2281页。'
- en: '[51] J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler, “Joint training of a
    convolutional network and a graphical model for human pose estimation,” in *Advances
    in neural information processing systems*, 2014, pp. 1799–1807.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] J. J. Tompson, A. Jain, Y. LeCun, 和 C. Bregler，“卷积网络和图形模型的联合训练用于人体姿态估计”，在
    *神经信息处理系统进展*，2014年，第1799–1807页。'
- en: '[52] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, “Convolutional pose
    machines,” in *CVPR 2016*, pp. 4724–4732.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S.-E. Wei, V. Ramakrishna, T. Kanade, 和 Y. Sheikh，“卷积姿态机器”，在 *CVPR 2016*，第4724–4732页。'
- en: '[53] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang, “Multi-context
    attention for human pose estimation,” in *CVPR 2017*, pp. 1831–1840.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, 和 X. Wang，“用于人体姿态估计的多上下文注意力”，在
    *CVPR 2017*，第1831–1840页。'
- en: '[54] W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang, “Learning feature pyramids
    for human pose estimation,” in *ICCV 2017*, pp. 1281–1290.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] W. Yang, S. Li, W. Ouyang, H. Li, 和 X. Wang，“用于人体姿态估计的特征金字塔学习”，在 *ICCV
    2017*，第1281–1290页。'
- en: '[55] A. Bulat, J. Kossaifi, G. Tzimiropoulos, and M. Pantic, “Toward fast and
    accurate human pose estimation via soft-gated skip connections,” in *2020 15th
    IEEE International Conference on Automatic Face and Gesture Recognition*, 2020,
    pp. 101–108.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] A. Bulat, J. Kossaifi, G. Tzimiropoulos, 和 M. Pantic，“通过软门控跳跃连接实现快速准确的人体姿态估计”，在
    *2020年第15届IEEE国际自动面部和姿态识别会议*，2020年，第101–108页。'
- en: '[56] U. Rafi, B. Leibe, J. Gall, and I. Kostrikov, “An efficient convolutional
    network for human pose estimation.” in *BMVC*, vol. 1, 2016, p. 2.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] U. Rafi, B. Leibe, J. Gall, 和 I. Kostrikov，“用于人体姿态估计的高效卷积网络”，在 *BMVC*，第1卷，2016年，第2页。'
- en: '[57] A. Jain, J. Tompson, Y. LeCun, and C. Bregler, “Modeep: A deep learning
    framework using motion features for human pose estimation,” in *Asian conference
    on computer vision*.   Springer, 2014, pp. 302–315.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Jain, J. Tompson, Y. LeCun, 和 C. Bregler，“Modeep：使用运动特征进行人体姿态估计的深度学习框架”，在
    *亚洲计算机视觉会议*。   Springer，2014年，第302–315页。'
- en: '[58] F. Zhang, X. Zhu, H. Dai, M. Ye, and C. Zhu, “Distribution-aware coordinate
    representation for human pose estimation,” in *CVPR 2020*, pp. 7093–7102.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] F. Zhang, X. Zhu, H. Dai, M. Ye, 和 C. Zhu，“用于人体姿态估计的分布感知坐标表示”，在 *CVPR
    2020*，第7093–7102页。'
- en: '[59] W. Mao, Y. Ge, C. Shen, Z. Tian, X. Wang, and Z. Wang, “Tfpose: Direct
    human pose estimation with transformers,” *arXiv preprint arXiv:2103.15320*, 2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] W. Mao, Y. Ge, C. Shen, Z. Tian, X. Wang, 和 Z. Wang, “Tfpose: 使用变压器的直接人体姿态估计”，*arXiv
    预印本 arXiv:2103.15320*，2021年。'
- en: '[60] K. Li, S. Wang, X. Zhang, Y. Xu, W. Xu, and Z. Tu, “Pose recognition with
    cascade transformers,” in *CVPR 2021*, pp. 1944–1953.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] K. Li, S. Wang, X. Zhang, Y. Xu, W. Xu, 和 Z. Tu, “使用级联变压器的姿态识别”，在 *CVPR
    2021*，第1944–1953页。'
- en: '[61] S. Yang, Z. Quan, M. Nie, and W. Yang, “Transpose: Keypoint localization
    via transformer,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2021, pp. 11 802–11 812.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] S. Yang, Z. Quan, M. Nie, 和 W. Yang, “Transpose: 通过变压器的关键点定位”，在 *IEEE/CVF
    国际计算机视觉会议论文集*，2021年，第11,802–11,812页。'
- en: '[62] Y. Xu, J. Zhang, Q. Zhang, and D. Tao, “ViTPose: Simple vision transformer
    baselines for human pose estimation,” *arXiv preprint arXiv:2204.12484*, 2022.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Y. Xu, J. Zhang, Q. Zhang, 和 D. Tao, “ViTPose: 用于人体姿态估计的简单视觉变压器基线”，*arXiv
    预印本 arXiv:2204.12484*，2022年。'
- en: '[63] A. Newell, Z. Huang, and J. Deng, “Associative embedding: End-to-end learning
    for joint detection and grouping,” in *Advances in neural information processing
    systems*, 2017, pp. 2277–2287.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] A. Newell, Z. Huang, 和 J. Deng, “Associative embedding: 端到端的联合检测和分组学习”，在
    *神经信息处理系统进展*，2017年，第2277–2287页。'
- en: '[64] V. Bazarevsky, I. Grishchenko, K. Raveendran, T. Zhu, F. Zhang, and M. Grundmann,
    “Blazepose: On-device real-time body pose tracking,” *arXiv preprint arXiv:2006.10204*,
    2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] V. Bazarevsky, I. Grishchenko, K. Raveendran, T. Zhu, F. Zhang, 和 M. Grundmann,
    “Blazepose: 设备上的实时身体姿态追踪”，*arXiv 预印本 arXiv:2006.10204*，2020年。'
- en: '[65] C. Lugaresi, J. Tang, H. Nash, C. McClanahan, E. Uboweja, M. Hays, F. Zhang,
    C.-L. Chang, M. G. Yong, J. Lee *et al.*, “Mediapipe: A framework for building
    perception pipelines,” *arXiv preprint arXiv:1906.08172*, 2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] C. Lugaresi, J. Tang, H. Nash, C. McClanahan, E. Uboweja, M. Hays, F.
    Zhang, C.-L. Chang, M. G. Yong, J. Lee *等*，“Mediapipe: 用于构建感知管道的框架”，*arXiv 预印本
    arXiv:1906.08172*，2019年。'
- en: '[66] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *ICCV
    2017*, pp. 2961–2969.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] K. He, G. Gkioxari, P. Dollár, 和 R. Girshick, “Mask r-cnn”，在 *ICCV 2017*，第2961–2969页。'
- en: '[67] W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei,
    and J. Sun, “Rethinking on multi-stage networks for human pose estimation,” *arXiv
    preprint arXiv:1901.00148*, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei,
    和 J. Sun, “重新思考多阶段网络在人类姿态估计中的应用”，*arXiv 预印本 arXiv:1901.00148*，2019年。'
- en: '[68] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele,
    “Deepercut: A deeper, stronger, and faster multi-person pose estimation model,”
    in *European Conference on Computer Vision*.   Springer, 2016, pp. 34–50.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, 和 B. Schiele,
    “Deepercut: 一个更深、更强、更快的多人体姿态估计模型”，在 *欧洲计算机视觉会议*。Springer，2016，第34–50页。'
- en: '[69] X. Nie, J. Feng, J. Zhang, and S. Yan, “Single-stage multi-person pose
    machines,” in *ICCV 2019*, pp. 6951–6960.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] X. Nie, J. Feng, J. Zhang, 和 S. Yan, “单阶段多人体姿态机器”，在 *ICCV 2019*，第6951–6960页。'
- en: '[70] S. Jin, W. Liu, E. Xie, W. Wang, C. Qian, W. Ouyang, and P. Luo, “Differentiable
    hierarchical graph grouping for multi-person pose estimation,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 718–734.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Jin, W. Liu, E. Xie, W. Wang, C. Qian, W. Ouyang, 和 P. Luo, “用于多人体姿态估计的可微分分层图分组”，在
    *欧洲计算机视觉会议*。Springer，2020年，第718–734页。'
- en: '[71] M. Kocabas, S. Karagoz, and E. Akbas, “Multiposenet: Fast multi-person
    pose estimation using pose residual network,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 417–433.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Kocabas, S. Karagoz, 和 E. Akbas, “Multiposenet: 使用姿态残差网络的快速多人体姿态估计”，在
    *欧洲计算机视觉会议 (ECCV) 论文集*，2018年，第417–433页。'
- en: '[72] B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang, “HigherHRNet:
    Scale-aware representation learning for bottom-up human pose estimation,” in *CVPR
    2020*, pp. 5386–5395.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, 和 L. Zhang, “HigherHRNet:
    针对自下而上的人体姿态估计的尺度感知表示学习”，在 *CVPR 2020*，第5386–5395页。'
- en: '[73] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler,
    and K. Murphy, “Towards accurate multi-person pose estimation in the wild,” in
    *CVPR 2017*, pp. 4903–4911.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler,
    和 K. Murphy, “迈向野外准确的多人体姿态估计”，在 *CVPR 2017*，第4903–4911页。'
- en: '[74] X. Nie, Y. Li, L. Luo, N. Zhang, and J. Feng, “Dynamic kernel distillation
    for efficient pose estimation in videos,” in *ICCV 2019*, pp. 6942–6950.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] X. Nie, Y. Li, L. Luo, N. Zhang, 和 J. Feng, “动态内核蒸馏用于视频中的高效姿态估计”，在 *ICCV
    2019*，第6942–6950页。'
- en: '[75] G. Bertasius, C. Feichtenhofer, D. Tran, J. Shi, and L. Torresani, “Learning
    temporal pose estimation from sparsely-labeled videos,” in *Advances in neural
    information processing systems*, 2019, pp. 3027–3038.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] G. Bertasius、C. Feichtenhofer、D. Tran、J. Shi 和 L. Torresani，“从稀疏标注视频中学习时间姿态估计”，发表于
    *神经信息处理系统进展*，2019，页3027–3038。'
- en: '[76] J. Wang, S. Jin, W. Liu, W. Liu, C. Qian, and P. Luo, “When human pose
    estimation meets robustness: Adversarial algorithms and benchmarks,” in *CVPR
    2021*, pp. 11 855–11 864.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. Wang、S. Jin、W. Liu、W. Liu、C. Qian 和 P. Luo，“当人体姿态估计遇上鲁棒性：对抗算法与基准”，发表于
    *CVPR 2021*，页11 855–11 864。'
- en: '[77] S.-H. Zhang, R. Li, X. Dong, P. Rosin, Z. Cai, X. Han, D. Yang, H. Huang,
    and S.-M. Hu, “Pose2Seg: Detection free human instance segmentation,” in *CVPR
    2019*, pp. 889–898.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S.-H. Zhang、R. Li、X. Dong、P. Rosin、Z. Cai、X. Han、D. Yang、H. Huang 和 S.-M.
    Hu，“Pose2Seg：无检测的人体实例分割”，发表于 *CVPR 2019*，页889–898。'
- en: '[78] N. Jain, S. Shah, A. Kumar, and A. Jain, “On the robustness of human pose
    estimation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition Workshops*, 2019, pp. 29–38.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] N. Jain、S. Shah、A. Kumar 和 A. Jain，“人体姿态估计的鲁棒性”，发表于 *IEEE/CVF计算机视觉与模式识别会议研讨会*，2019，页29–38。'
- en: '[79] M. Andriluka, S. Roth, and B. Schiele, “Discriminative appearance models
    for pictorial structures,” *International journal of computer vision*, vol. 99,
    no. 3, pp. 259–280, 2012.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Andriluka、S. Roth 和 B. Schiele，“图像结构的辨别外观模型”，*计算机视觉国际期刊*，第99卷，第3期，页259–280，2012。'
- en: '[80] M. Bergtholdt, J. Kappes, S. Schmidt, and C. Schnörr, “A study of parts-based
    object class detection using complete graphs,” *International journal of computer
    vision*, vol. 87, no. 1-2, p. 93, 2010.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M. Bergtholdt、J. Kappes、S. Schmidt 和 C. Schnörr，“基于部件的目标类别检测研究”，*计算机视觉国际期刊*，第87卷，第1-2期，页93，2010。'
- en: '[81] S. Li and A. B. Chan, “3D human pose estimation from monocular images
    with deep convolutional neural network,” in *Asian Conference on Computer Vision*.   Springer,
    2014, pp. 332–347.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] S. Li 和 A. B. Chan，“基于单目图像的深度卷积神经网络三维人体姿态估计”，发表于 *亚洲计算机视觉会议*。Springer，2014，页332–347。'
- en: '[82] C. Chen and D. Ramanan, “3D human pose estimation = 2D pose estimation
    + matching,” in *CVPR 2017*, pp. 5759–5767.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C. Chen 和 D. Ramanan，“三维人体姿态估计 = 二维姿态估计 + 匹配”，发表于 *CVPR 2017*，页5759–5767。'
- en: '[83] J. Martinez, R. Hossain, J. Romero, and J. J. Little, “A simple yet effective
    baseline for 3D human pose estimation,” in *ICCV 2017*, pp. 2659–2668.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. Martinez、R. Hossain、J. Romero 和 J. J. Little，“三维人体姿态估计的简单而有效的基线”，发表于
    *ICCV 2017*，页2659–2668。'
- en: '[84] D. Mehta, O. Sotnychenko, F. Mueller, W. Xu, S. Sridhar, G. Pons-Moll,
    and C. Theobalt, “Single-shot multi-person 3D pose estimation from monocular RGB,”
    in *2018 International Conference on 3D Vision*.   IEEE Computer Society, 2018,
    pp. 120–130.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] D. Mehta、O. Sotnychenko、F. Mueller、W. Xu、S. Sridhar、G. Pons-Moll 和 C.
    Theobalt，“基于单目RGB的单次多人体三维姿态估计”，发表于 *2018国际三维视觉会议*。IEEE计算机学会，2018，页120–130。'
- en: '[85] S. Li, L. Ke, K. Pratama, Y.-W. Tai, C.-K. Tang, and K.-T. Cheng, “Cascaded
    deep monocular 3D human pose estimation with evolutionary training data,” in *CVPR
    2020*, pp. 6173–6183.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Li、L. Ke、K. Pratama、Y.-W. Tai、C.-K. Tang 和 K.-T. Cheng，“级联深度单目三维人体姿态估计与进化训练数据”，发表于
    *CVPR 2020*，页6173–6183。'
- en: '[86] K. Gong, J. Zhang, and J. Feng, “PoseAug: A differentiable pose augmentation
    framework for 3D human pose estimation,” in *CVPR 2021*, pp. 8575–8584.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] K. Gong、J. Zhang 和 J. Feng，“PoseAug：用于三维人体姿态估计的可微分姿态增强框架”，发表于 *CVPR 2021*，页8575–8584。'
- en: '[87] K. Iskakov, E. Burkov, V. S. Lempitsky, and Y. Malkov, “Learnable triangulation
    of human pose,” in *ICCV 2019*, pp. 7717–7726.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] K. Iskakov、E. Burkov、V. S. Lempitsky 和 Y. Malkov，“可学习的人体姿态三角剖分”，发表于 *ICCV
    2019*，页7717–7726。'
- en: '[88] Z. Zhang, C. Wang, W. Qiu, W. Qin, and W. Zeng, “AdaFuse: Adaptive multiview
    fusion for accurate human pose estimation in the wild,” *International Journal
    of Computer Vision*, pp. 1–16, 2020.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Z. Zhang、C. Wang、W. Qiu、W. Qin 和 W. Zeng，“AdaFuse：适应性多视角融合以实现野外准确的人体姿态估计”，*计算机视觉国际期刊*，页1–16，2020。'
- en: '[89] Y. He, R. Yan, K. Fragkiadaki, and S. Yu, “Epipolar transformers,” in
    *CVPR 2020*, pp. 7776–7785.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. He、R. Yan、K. Fragkiadaki 和 S. Yu，“视差变换器”，发表于 *CVPR 2020*，页7776–7785。'
- en: '[90] H. Qiu, C. Wang, J. Wang, N. Wang, and W. Zeng, “Cross view fusion for
    3D human pose estimation,” in *ICCV 2019*.   IEEE, pp. 4341–4350.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] H. Qiu、C. Wang、J. Wang、N. Wang 和 W. Zeng，“用于三维人体姿态估计的视角融合”，发表于 *ICCV 2019*。IEEE，页4341–4350。'
- en: '[91] S. Wu, S. Jin, W. Liu, L. Bai, C. Qian, D. Liu, and W. Ouyang, “Graph-based
    3D multi-person pose estimation using multi-view images,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 11 148–11 157.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] S. Wu, S. Jin, W. Liu, L. Bai, C. Qian, D. Liu, 和 W. Ouyang, “基于图的3D多人的姿态估计使用多视角图像，”
    见 *IEEE/CVF国际计算机视觉会议论文集*，2021年，第11,148–11,157页。'
- en: '[92] M. Trumble, A. Gilbert, C. Malleson, A. Hilton, and J. P. Collomosse,
    “Total capture: 3D human pose estimation fusing video and inertial sensors.” in
    *BMVC*, vol. 2, no. 5, 2017, pp. 1–13.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] M. Trumble, A. Gilbert, C. Malleson, A. Hilton, 和 J. P. Collomosse, “全捕捉：融合视频和惯性传感器的3D人体姿态估计。”
    见 *BMVC*，第2卷，第5期，2017年，第1–13页。'
- en: '[93] A. Gilbert, M. Trumble, C. Malleson, A. Hilton, and J. Collomosse, “Fusing
    visual and inertial sensors with semantics for 3D human pose estimation,” *International
    Journal of Computer Vision*, vol. 127, no. 4, pp. 381–397, 2019.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. Gilbert, M. Trumble, C. Malleson, A. Hilton, 和 J. Collomosse, “结合视觉和惯性传感器以及语义进行3D人体姿态估计，”
    *国际计算机视觉杂志*，第127卷，第4期，第381–397页，2019年。'
- en: '[94] Z. Zhang, C. Wang, W. Qin, and W. Zeng, “Fusing wearable IMUs with multi-view
    images for human pose estimation: A geometric approach,” in *CVPR 2020*, pp. 2200–2209.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Z. Zhang, C. Wang, W. Qin, 和 W. Zeng, “融合可穿戴IMU与多视角图像进行人体姿态估计：一种几何方法，”
    见 *CVPR 2020*，第2200–2209页。'
- en: '[95] C. Malleson, A. Gilbert, M. Trumble, J. Collomosse, A. Hilton, and M. Volino,
    “Real-time full-body motion capture from video and IMUs,” in *2017 International
    Conference on 3D Vision (3DV)*.   IEEE, 2017, pp. 449–457.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] C. Malleson, A. Gilbert, M. Trumble, J. Collomosse, A. Hilton, 和 M. Volino,
    “从视频和惯性测量单元（IMU）进行实时全身动作捕捉，” 见 *2017年国际3D视觉会议（3DV）*。IEEE，2017年，第449–457页。'
- en: '[96] F. Huang, A. Zeng, M. Liu, Q. Lai, and Q. Xu, “DeepFuse: An imu-aware
    network for real-time 3D human pose estimation from multi-view image,” in *The
    IEEE Winter Conference on Applications of Computer Vision*, 2020, pp. 429–438.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] F. Huang, A. Zeng, M. Liu, Q. Lai, 和 Q. Xu, “DeepFuse：一种用于实时3D人体姿态估计的IMU感知网络，”
    见 *IEEE冬季计算机视觉应用会议*，2020年，第429–438页。'
- en: '[97] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, “Deep ordinal regression
    network for monocular depth estimation,” in *CVPR 2018*, pp. 2002–2011.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. Fu, M. Gong, C. Wang, K. Batmanghelich, 和 D. Tao, “用于单目深度估计的深度序数回归网络，”
    见 *CVPR 2018*，第2002–2011页。'
- en: '[98] H. Rhodin, M. Salzmann, and P. Fua, “Unsupervised geometry-aware representation
    for 3D human pose estimation,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 750–767.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] H. Rhodin, M. Salzmann, 和 P. Fua, “无监督几何感知表示用于3D人体姿态估计，” 见 *欧洲计算机视觉会议（ECCV）*，2018年，第750–767页。'
- en: '[99] D. C. Luvizon, D. Picard, and H. Tabia, “2D/3D pose estimation and action
    recognition using multitask deep learning,” in *CVPR 2018*, pp. 5137–5146.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] D. C. Luvizon, D. Picard, 和 H. Tabia, “使用多任务深度学习进行2D/3D姿态估计和动作识别，” 见 *CVPR
    2018*，第5137–5146页。'
- en: '[100] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis, “Coarse-to-fine
    volumetric prediction for single-image 3D human pose,” in *CVPR 2017*, pp. 7025–7034.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] G. Pavlakos, X. Zhou, K. G. Derpanis, 和 K. Daniilidis, “从粗到细的体积预测用于单图像3D人体姿态，”
    见 *CVPR 2017*，第7025–7034页。'
- en: '[101] A. Nibali, Z. He, S. Morgan, and L. Prendergast, “3D human pose estimation
    with 2D marginal heatmaps,” in *2019 IEEE Winter Conference on Applications of
    Computer Vision (WACV)*.   IEEE, 2019, pp. 1477–1485.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] A. Nibali, Z. He, S. Morgan, 和 L. Prendergast, “使用2D边际热图进行3D人体姿态估计，”
    见 *2019年IEEE冬季计算机视觉应用会议（WACV）*。IEEE，2019年，第1477–1485页。'
- en: '[102] M. Fabbri, F. Lanzi, S. Calderara, S. Alletto, and R. Cucchiara, “Compressed
    volumetric heatmaps for multi-person 3D pose estimation,” in *CVPR 2020*, pp.
    7204–7213.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. Fabbri, F. Lanzi, S. Calderara, S. Alletto, 和 R. Cucchiara, “压缩体积热图用于多人3D姿态估计，”
    见 *CVPR 2020*，第7204–7213页。'
- en: '[103] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding, “3D human
    pose estimation with spatial and temporal transformers,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 11 656–11 665.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, 和 Z. Ding, “结合空间和时间变换器的3D人体姿态估计，”
    见 *IEEE/CVF国际计算机视觉会议论文集*，2021年，第11,656–11,665页。'
- en: '[104] D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli, “3D human pose
    estimation in video with temporal convolutions and semi-supervised training,”
    in *CVPR 2019*, pp. 7753–7762.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] D. Pavllo, C. Feichtenhofer, D. Grangier, 和 M. Auli, “视频中的3D人体姿态估计：利用时间卷积和半监督训练，”
    见 *CVPR 2019*，第7753–7762页。'
- en: '[105] G. Rogez, P. Weinzaepfel, and C. Schmid, “LCR-NET++: Multi-person 2D
    and 3D pose detection in natural images,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 42, no. 5, pp. 1146–1161, 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] G. Rogez, P. Weinzaepfel 和 C. Schmid, “LCR-NET++：自然图像中的多人体 2D 和 3D 姿态检测”，*IEEE
    模式分析与机器智能交易*，第 42 卷，第 5 期，第 1146–1161 页，2019。'
- en: '[106] A. Zanfir, E. Marinoiu, M. Zanfir, A.-I. Popa, and C. Sminchisescu, “Deep
    network for the integrated 3D sensing of multiple people in natural images,” *Advances
    in Neural Information Processing Systems*, vol. 31, pp. 8410–8419, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Zanfir, E. Marinoiu, M. Zanfir, A.-I. Popa 和 C. Sminchisescu, “用于自然图像中多人的集成
    3D 感知的深度网络”，*神经信息处理系统进展*，第 31 卷，第 8410–8419 页，2018。'
- en: '[107] A. Harltey and A. Zisserman, *Multiple view geometry in computer vision
    (2\. ed.)*.   Cambridge University Press, 2006.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Harltey 和 A. Zisserman, *计算机视觉中的多视角几何（第2版）*。剑桥大学出版社，2006。'
- en: '[108] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, “Fast and robust multi-person
    3D pose estimation from multiple views,” in *CVPR 2019*, pp. 7792–7801.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Dong, W. Jiang, Q. Huang, H. Bao 和 X. Zhou, “快速且鲁棒的多人体 3D 姿态估计”，发表于
    *CVPR 2019*，第 7792–7801 页。'
- en: '[109] L. Chen, H. Ai, R. Chen, Z. Zhuang, and S. Liu, “Cross-view tracking
    for multi-human 3D pose estimation at over 100 FPS,” in *CVPR 2020*, pp. 3279–3288.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] L. Chen, H. Ai, R. Chen, Z. Zhuang 和 S. Liu, “跨视角跟踪实现超过 100 FPS 的多人体
    3D 姿态估计”，发表于 *CVPR 2020*，第 3279–3288 页。'
- en: '[110] C. Huang, S. Jiang, Y. Li, Z. Zhang, J. Traish, C. Deng, S. Ferguson,
    and R. Y. Da Xu, “End-to-end dynamic matching network for multi-view multi-person
    3D pose estimation,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 477–493.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] C. Huang, S. Jiang, Y. Li, Z. Zhang, J. Traish, C. Deng, S. Ferguson
    和 R. Y. Da Xu, “端到端动态匹配网络用于多视角多人体 3D 姿态估计”，发表于 *欧洲计算机视觉会议*。Springer，2020，第 477–493
    页。'
- en: '[111] A. Elmi, D. Mazzini, and P. Tortella, “Light3DPose: Real-time multi-person
    3D pose estimation from multiple views,” in *2020 25th International Conference
    on Pattern Recognition (ICPR)*.   IEEE, 2021, pp. 2755–2762.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. Elmi, D. Mazzini 和 P. Tortella, “Light3DPose：来自多个视角的实时多人体 3D 姿态估计”，发表于
    *2020年第25届国际模式识别会议（ICPR）*。IEEE，2021，第 2755–2762 页。'
- en: '[112] M. Zhao, T. Li, M. Abu Alsheikh, Y. Tian, H. Zhao, A. Torralba, and D. Katabi,
    “Through-wall human pose estimation using radio signals,” in *CVPR 2018*, pp.
    7356–7365.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] M. Zhao, T. Li, M. Abu Alsheikh, Y. Tian, H. Zhao, A. Torralba 和 D. Katabi,
    “利用无线电信号进行墙壁透视的人体姿态估计”，发表于 *CVPR 2018*，第 7356–7365 页。'
- en: '[113] M. J. Marin-Jimenez, F. J. Romero-Ramirez, R. Muñoz-Salinas, and R. Medina-Carnicer,
    “3D human pose estimation from depth maps using a deep combination of poses,”
    *Journal of Visual Communication and Image Representation*, vol. 55, pp. 627–639,
    2018.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. J. Marin-Jimenez, F. J. Romero-Ramirez, R. Muñoz-Salinas 和 R. Medina-Carnicer,
    “利用深度姿态组合从深度图中估计 3D 人体姿态”，*视觉通信与图像表示期刊*，第 55 卷，第 627–639 页，2018。'
- en: '[114] S. Kim, K. Yun, J. Park, and J. Y. Choi, “Skeleton-based action recognition
    of people handling objects,” in *2019 IEEE Winter Conference on Applications of
    Computer Vision (WACV)*.   IEEE, 2019, pp. 61–70.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] S. Kim, K. Yun, J. Park 和 J. Y. Choi, “基于骨架的物体操作人动作识别”，发表于 *2019 IEEE
    冬季计算机视觉应用会议（WACV）*。IEEE，2019，第 61–70 页。'
- en: '[115] C.-Y. Weng, B. Curless, and I. Kemelmacher-Shlizerman, “Photo wake-up:
    3D character animation from a single photo,” in *CVPR 2019*, pp. 5908–5917.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] C.-Y. Weng, B. Curless 和 I. Kemelmacher-Shlizerman, “照片唤醒：从单张照片生成 3D
    角色动画”，发表于 *CVPR 2019*，第 5908–5917 页。'
- en: '[116] Y. Su and Z. Liu, “Position detection for badminton tactical analysis
    based on multi-person pose estimation,” in *2018 14th International Conference
    on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)*.   IEEE,
    2018, pp. 379–383.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Y. Su 和 Z. Liu, “基于多人体姿态估计的羽毛球战术分析中的位置检测”，发表于 *2018年第14届国际自然计算、模糊系统和知识发现会议（ICNC-FSKD）*。IEEE，2018，第
    379–383 页。'
- en: '[117] Y. Li, C. Wang, Y. Cao, B. Liu, J. Tan, and Y. Luo, “Human pose estimation
    based in-home lower body rehabilitation system,” in *2020 International Joint
    Conference on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–8.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Y. Li, C. Wang, Y. Cao, B. Liu, J. Tan 和 Y. Luo, “基于家庭内下肢康复系统的人体姿态估计”，发表于
    *2020 国际联合神经网络会议（IJCNN）*。IEEE，2020，第 1–8 页。'
- en: '[118] R. Rabbito, “Using deep learning-based pose estimation algorithms for
    markerless gait analysis in rehabilitation medicine,” Master’s thesis, Politecnico
    di Torino, 2021.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] R. Rabbito, “使用基于深度学习的姿态估计算法进行无标记步态分析在康复医学中的应用”，硕士论文，都灵理工大学，2021。'
- en: '[119] Y. Hbali, S. Hbali, L. Ballihi, and M. Sadgal, “Skeleton-based human
    activity recognition for elderly monitoring systems,” *IET Computer Vision*, vol. 12,
    no. 1, pp. 16–26, 2017.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Y. Hbali, S. Hbali, L. Ballihi, 和 M. Sadgal, “基于骨架的老年人活动识别系统，” *IET 计算机视觉*，第
    12 卷，第 1 期，页码 16–26，2017 年。'
- en: '[120] T.-H. Tsai and C.-W. Hsu, “Implementation of fall detection system based
    on 3D skeleton for deep learning technique,” *IEEE Access*, vol. 7, pp. 153 049–153 059,
    2019.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] T.-H. Tsai 和 C.-W. Hsu, “基于 3D 骨架的深度学习技术跌倒检测系统的实现，” *IEEE Access*，第 7
    卷，页码 153 049–153 059，2019 年。'
- en: '[121] L. Kumarapu and P. Mukherjee, “Animepose: Multi-person 3d pose estimation
    and animation,” *Pattern Recognition Letters*, vol. 147, pp. 16–24, 2021.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] L. Kumarapu 和 P. Mukherjee, “Animepose：多人 3D 姿态估计与动画，” *模式识别快报*，第 147
    卷，页码 16–24，2021 年。'
- en: '[122] A. Elaoud, W. Barhoumi, E. Zagrouba, and B. Agrebi, “Skeleton-based comparison
    of throwing motion for handball players,” *Journal of Ambient Intelligence and
    Humanized Computing*, vol. 11, no. 1, pp. 419–431, 2020.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] A. Elaoud, W. Barhoumi, E. Zagrouba, 和 B. Agrebi, “手球运动员投掷动作的骨架比较，” *环境智能与人性化计算期刊*，第
    11 卷，第 1 期，页码 419–431，2020 年。'
- en: '[123] A. Kamel, B. Liu, P. Li, and B. Sheng, “An investigation of 3D human
    pose estimation for learning tai chi: A human factor perspective,” *International
    Journal of Human–Computer Interaction*, vol. 35, no. 4-5, pp. 427–439, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] A. Kamel, B. Liu, P. Li, 和 B. Sheng, “从人因学视角探讨学习太极的 3D 人体姿态估计，” *国际人机交互期刊*，第
    35 卷，第 4-5 期，页码 427–439，2019 年。'
- en: '[124] P. Gabriel, W. K. Doyle, O. Devinsky, D. Friedman, T. Thesen, and V. Gilja,
    “Neural correlates to automatic behavior estimations from RGB-D video in epilepsy
    unit,” in *2016 38th Annual International Conference of the IEEE Engineering in
    Medicine and Biology Society*.   IEEE, 2016, pp. 3402–3405.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] P. Gabriel, W. K. Doyle, O. Devinsky, D. Friedman, T. Thesen, 和 V. Gilja,
    “RGB-D 视频自动行为估计的神经相关性，” 见 *2016 年 IEEE 医学与生物学工程年会第 38 届年会*。 IEEE，2016 年，页码 3402–3405。'
- en: '[125] Š. Obdržálek, G. Kurillo, F. Ofli, R. Bajcsy, E. Seto, H. Jimison, and
    M. Pavel, “Accuracy and robustness of kinect pose estimation in the context of
    coaching of elderly population,” in *2012 Annual International Conference of the
    IEEE Engineering in Medicine and Biology Society*.   IEEE, 2012, pp. 1188–1193.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Š. Obdržálek, G. Kurillo, F. Ofli, R. Bajcsy, E. Seto, H. Jimison, 和
    M. Pavel, “Kinect 姿态估计在老年人训练中的准确性与稳健性，” 见 *2012 年 IEEE 医学与生物学工程年会*。 IEEE，2012
    年，页码 1188–1193。'
- en: '[126] B. Ren, M. Liu, R. Ding, and H. Liu, “A survey on 3D skeleton-based action
    recognition using learning method,” *arXiv preprint arXiv:2002.05907*, 2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] B. Ren, M. Liu, R. Ding, 和 H. Liu, “基于 3D 骨架的动作识别方法综述，” *arXiv 预印本 arXiv:2002.05907*，2020
    年。'
- en: '[127] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C. Kot, “NTU
    RGB+ D 120: A large-scale benchmark for 3D human activity understanding,” *IEEE
    transactions on pattern analysis and machine intelligence*, vol. 42, no. 10, pp.
    2684–2701, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, 和 A. C. Kot, “NTU
    RGB+ D 120：大规模 3D 人类活动理解基准，” *IEEE 模式分析与机器智能学报*，第 42 卷，第 10 期，页码 2684–2701，2019
    年。'
- en: '[128] B. Li, H. Chen, Y. Chen, Y. Dai, and M. He, “Skeleton boxes: Solving
    skeleton based action detection with a single deep convolutional neural network,”
    in *2017 IEEE International Conference on Multimedia & Expo Workshops*.   IEEE,
    2017, pp. 613–616.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] B. Li, H. Chen, Y. Chen, Y. Dai, 和 M. He, “骨架框：使用单一深度卷积神经网络解决基于骨架的动作检测，”
    见 *2017 IEEE 国际多媒体与展览研讨会*。 IEEE，2017 年，页码 613–616。'
- en: '[129] H. Duan, Y. Zhao, K. Chen, D. Lin, and B. Dai, “Revisiting skeleton-based
    action recognition,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 2969–2978.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] H. Duan, Y. Zhao, K. Chen, D. Lin, 和 B. Dai, “重访基于骨架的动作识别，” 见 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2022 年，页码 2969–2978。'
- en: '[130] S. Das, S. Sharma, R. Dai, F. Bremond, and M. Thonnat, “VPN: Learning
    video-pose embedding for activities of daily living,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 72–90.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] S. Das, S. Sharma, R. Dai, F. Bremond, 和 M. Thonnat, “VPN：学习视频-姿态嵌入用于日常生活活动，”
    见 *欧洲计算机视觉大会*。 Springer，2020 年，页码 72–90。'
- en: '[131] H. Guo, Y. Yu, Q. Ding, and M. Skitmore, “Image-and-skeleton-based parameterized
    approach to real-time identification of construction workers’ unsafe behaviors,”
    *Journal of Construction Engineering and Management*, vol. 144, no. 6, p. 04018042,
    2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] H. Guo, Y. Yu, Q. Ding, 和 M. Skitmore, “基于图像和骨架的参数化方法实时识别施工工人的不安全行为，”
    *建筑工程与管理期刊*，第 144 卷，第 6 期，页码 04018042，2018 年。'
- en: '[132] G. Lan, Z. Luo, and Q. Hao, “Development of a virtual reality teleconference
    system using distributed depth sensors,” in *2016 2nd IEEE International Conference
    on Computer and Communications (ICCC)*.   IEEE, 2016, pp. 975–978.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] G. Lan、Z. Luo 和 Q. Hao，“基于分布式深度传感器的虚拟现实电话会议系统的开发，”在 *2016 年第二届 IEEE 国际计算机与通信会议
    (ICCC)* 中。IEEE，2016 年，页码 975–978。'
- en: '[133] M. C. Thar, K. Z. N. Winn, and N. Funabiki, “A proposal of yoga pose
    assessment method using pose detection for self-learning,” in *2019 International
    Conference on Advanced Information Technologies (ICAIT)*.   IEEE, 2019, pp. 137–142.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] M. C. Thar、K. Z. N. Winn 和 N. Funabiki，“利用姿态检测进行自学的瑜伽姿态评估方法提案，”在 *2019
    年国际先进信息技术会议 (ICAIT)* 中。IEEE，2019 年，页码 137–142。'
- en: '[134] R. Afrouzian, H. Seyedarabi, and S. Kasaei, “Pose estimation of soccer
    players using multiple uncalibrated cameras,” *Multimedia Tools and Applications*,
    vol. 75, no. 12, pp. 6809–6827, 2016.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] R. Afrouzian、H. Seyedarabi 和 S. Kasaei，“使用多个未标定摄像头的足球运动员姿态估计，” *Multimedia
    Tools and Applications*，第 75 卷，第 12 期，页码 6809–6827，2016 年。'
- en: '[135] L. Bridgeman, M. Volino, J.-Y. Guillemaut, and A. Hilton, “Multi-person
    3D pose estimation and tracking in sports,” in *CVPR Workshops 2019*, pp. 2487–2496.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] L. Bridgeman、M. Volino、J.-Y. Guillemaut 和 A. Hilton，“体育运动中的多人 3D 姿态估计与跟踪，”在
    *CVPR 研讨会 2019* 中，页码 2487–2496。'
- en: '[136] R. Kurose, M. Hayashi, T. Ishii, and Y. Aoki, “Player pose analysis in
    tennis video based on pose estimation,” in *2018 International Workshop on Advanced
    Image Technology (IWAIT)*.   IEEE, 2018, pp. 1–4.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] R. Kurose、M. Hayashi、T. Ishii 和 Y. Aoki，“基于姿态估计的网球视频中的运动员姿态分析，”在 *2018
    年国际先进图像技术研讨会 (IWAIT)* 中。IEEE，2018 年，页码 1–4。'
- en: '[137] E. Wu and H. Koike, “FuturePong: Real-time table tennis trajectory forecasting
    using pose prediction network,” in *Extended Abstracts of the 2020 CHI Conference
    on Human Factors in Computing Systems*, 2020, pp. 1–8.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] E. Wu 和 H. Koike，“FuturePong：使用姿态预测网络进行实时乒乓球轨迹预测，”在 *2020 年 CHI 计算机系统人因会议扩展摘要*
    中，2020 年，页码 1–8。'
- en: '[138] A. Martín-Martín, E. Orduna-Malea, M. Thelwall, and E. D. López-Cózar,
    “Google scholar, web of science, and scopus: A systematic comparison of citations
    in 252 subject categories,” *Journal of Informetrics*, vol. 12, no. 4, pp. 1160–1177,
    2018.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Martín-Martín、E. Orduna-Malea、M. Thelwall 和 E. D. López-Cózar，“Google
    Scholar、Web of Science 和 Scopus：对 252 个学科类别中引文的系统比较，” *信息计量学杂志*，第 12 卷，第 4 期，页码
    1160–1177，2018 年。'
- en: '[139] M. E. Rose and J. R. Kitchin, “Pybliometrics: Scriptable bibliometrics
    using a python interface to scopus,” *SoftwareX*, vol. 10, p. 100263, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] M. E. Rose 和 J. R. Kitchin，“Pybliometrics: 使用 Python 接口进行可脚本化的文献计量学，”
    *SoftwareX*，第 10 卷，页码 100263，2019 年。'
- en: '[140] L. Xu, Y. Guan, S. Jin, W. Liu, C. Qian, P. Luo, W. Ouyang, and X. Wang,
    “ViPNAS: Efficient video pose estimation via neural architecture search,” in *CVPR
    2021*, pp. 16 072–16 081.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] L. Xu、Y. Guan、S. Jin、W. Liu、C. Qian、P. Luo、W. Ouyang 和 X. Wang，“ViPNAS：通过神经架构搜索进行高效的视频姿态估计，”在
    *CVPR 2021* 中，页码 16 072–16 081。'
- en: '[141] B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang, “HigherHRNet:
    Scale-aware representation learning for bottom-up human pose estimation,” in *CVPR
    2020*.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] B. Cheng、B. Xiao、J. Wang、H. Shi、T. S. Huang 和 L. Zhang，“HigherHRNet：用于自下而上人体姿态估计的尺度感知表示学习，”在
    *CVPR 2020* 中。'
- en: '[142] C. Doersch and A. Zisserman, “Sim2real transfer learning for 3D human
    pose estimation: motion to the rescue,” in *Advances in Neural Information Processing
    Systems*, 2019, pp. 12 949–12 961.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Doersch 和 A. Zisserman，“用于 3D 人体姿态估计的 Sim2real 迁移学习：运动来救援，”在 *神经信息处理系统进展*
    中，2019 年，页码 12 949–12 961。'
- en: '[143] J. Li, S. Bian, A. Zeng, C. Wang, B. Pang, W. Liu, and C. Lu, “Human
    pose regression with residual log-likelihood estimation,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 11 025–11 034.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Li、S. Bian、A. Zeng、C. Wang、B. Pang、W. Liu 和 C. Lu，“带有残差对数似然估计的人体姿态回归，”在
    *IEEE/CVF 国际计算机视觉会议论文集* 中，2021 年，页码 11 025–11 034。'
- en: '[144] J. Huang, Z. Zhu, F. Guo, and G. Huang, “The devil is in the details:
    Delving into unbiased data processing for human pose estimation,” in *CVPR 2020*,
    pp. 5700–5709.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. Huang、Z. Zhu、F. Guo 和 G. Huang，“细节决定成败：*深入研究*无偏数据处理在人体姿态估计中的应用，”在
    *CVPR 2020* 中，页码 5700–5709。'
- en: '[145] C. Yu, B. Xiao, C. Gao, L. Yuan, L. Zhang, N. Sang, and J. Wang, “Lite-HRNet:
    A lightweight high-resolution network,” in *CVPR 2021*, pp. 10 440–10 450.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] C. Yu、B. Xiao、C. Gao、L. Yuan、L. Zhang、N. Sang 和 J. Wang，“Lite-HRNet：一种轻量级高分辨率网络，”在
    *CVPR 2021* 中，页码 10 440–10 450。'
- en: '[146] D. Osokin, “Real-time 2D multi-person pose estimation on CPU: Lightweight
    OpenPose,” *arXiv preprint arXiv:1811.12004*, 2018.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] D. Osokin，“CPU 上的实时 2D 多人姿态估计：轻量级 OpenPose，” *arXiv 预印本 arXiv:1811.12004*，2018
    年。'
- en: '[147] Z. Liu, H. Chen, R. Feng, S. Wu, S. Ji, B. Yang, and X. Wang, “Deep dual
    consecutive network for human pose estimation,” in *CVPR 2021*, pp. 525–534.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Z. Liu, H. Chen, R. Feng, S. Wu, S. Ji, B. Yang, 和 X. Wang，“用于人体姿态估计的深度双重连续网络，”
    *CVPR 2021*，页码 525–534。'
- en: '[148] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, and X. Zhou, “Smap:
    Single-shot multi-person absolute 3D pose estimation,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 550–566.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, 和 X. Zhou，“Smap：单次拍摄的多人人体绝对
    3D 姿态估计，” *欧洲计算机视觉大会*。 Springer，2020年，页码 550–566。'
- en: '[149] P. Weinzaepfel, R. Brégier, H. Combaluzier, V. Leroy, and G. Rogez, “Dope:
    Distillation of part experts for whole-body 3D pose estimation in the wild,” in
    *European Conference on Computer Vision*.   Springer, 2020, pp. 380–397.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] P. Weinzaepfel, R. Brégier, H. Combaluzier, V. Leroy, 和 G. Rogez，“Dope：野外全身
    3D 姿态估计的部分专家蒸馏，” *欧洲计算机视觉大会*。 Springer，2020年，页码 380–397。'
- en: '[150] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-P. Seidel,
    W. Xu, D. Casas, and C. Theobalt, “VNect: Real-time 3D human pose estimation with
    a single RGB camera,” *ACM Transactions on Graphics (TOG)*, vol. 36, no. 4, pp.
    1–14, 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-P. Seidel,
    W. Xu, D. Casas, 和 C. Theobalt，“VNect：使用单个 RGB 摄像头的实时 3D 人体姿态估计，” *ACM图形学学报（TOG）*，第
    36 卷，第 4 期，页码 1–14，2017年。'
- en: '[151] I. Sárándi, T. Linder, K. O. Arras, and B. Leibe, “Synthetic occlusion
    augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on
    3D human pose estimation,” *arXiv preprint arXiv:1809.04987*, 2018.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] I. Sárándi, T. Linder, K. O. Arras, 和 B. Leibe，“通过体积热图的合成遮挡增强用于 2018
    ECCV PoseTrack 挑战中的 3D 人体姿态估计，” *arXiv 预印本 arXiv:1809.04987*，2018年。'
- en: '[152] X. Sun, B. Xiao, F. Wei, S. Liang, and Y. Wei, “Integral human pose regression,”
    in *ECCV 2018*, pp. 529–545.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] X. Sun, B. Xiao, F. Wei, S. Liang, 和 Y. Wei，“整体人体姿态回归，” *ECCV 2018*，页码
    529–545。'
- en: '[153] Y. Cheng, B. Wang, B. Yang, and R. T. Tan, “Monocular 3d multi-person
    pose estimation by integrating top-down and bottom-up networks,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 7649–7659.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Y. Cheng, B. Wang, B. Yang, 和 R. T. Tan，“通过整合自上而下和自下而上的网络进行单目 3D 多人姿态估计，”
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，页码 7649–7659。'
- en: '[154] T. Wehrbein, M. Rudolph, B. Rosenhahn, and B. Wandt, “Probabilistic monocular
    3D human pose estimation with normalizing flows,” *arXiv preprint arXiv:2107.13788*,
    2021.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] T. Wehrbein, M. Rudolph, B. Rosenhahn, 和 B. Wandt，“基于正规化流的概率单目 3D 人体姿态估计，”
    *arXiv 预印本 arXiv:2107.13788*，2021年。'
