- en: 'Machine Learning 1: Lesson 8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ 1ï¼šç¬¬8è¯¾
- en: åŸæ–‡ï¼š[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-8-fa1a87064a53](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-8-fa1a87064a53)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-8-fa1a87064a53](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-8-fa1a87064a53)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to â€œreallyâ€ understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¥è‡ª*[*æœºå™¨å­¦ä¹ è¯¾ç¨‹*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*çš„ä¸ªäººç¬”è®°ã€‚éšç€æˆ‘ç»§ç»­å¤ä¹ è¯¾ç¨‹ä»¥â€œçœŸæ­£â€ç†è§£å®ƒï¼Œè¿™äº›ç¬”è®°å°†ç»§ç»­æ›´æ–°å’Œæ”¹è¿›ã€‚éå¸¸æ„Ÿè°¢*[*Jeremy*](https://twitter.com/jeremyphoward)*å’Œ*[*Rachel*](https://twitter.com/math_rachel)*ç»™äº†æˆ‘è¿™ä¸ªå­¦ä¹ çš„æœºä¼šã€‚*'
- en: Neural nets broadly defined
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰å®šä¹‰çš„ç¥ç»ç½‘ç»œ
- en: '[Video](https://youtu.be/DzE0eSdy5Hk) / [Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[è§†é¢‘](https://youtu.be/DzE0eSdy5Hk) / [ç¬”è®°æœ¬](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
- en: As we discussed at the end of last lesson, weâ€™re moving from decision tree ensembles
    to neural nets broadly defined. As you know, random forests and decision trees
    are limited by the fact in the end that they are basically doing nearest neighbors.
    All they can do is to return the average of a bunch of other points. So they canâ€™t
    extrapolate out to, if you are thinking what happens if Iâ€™ll increase my price
    by 20% and youâ€™ve never priced at that level before, or whatâ€™s going to happen
    to sales next year and obviously weâ€™ve never seen next year before and itâ€™s very
    hard to extrapolate. Itâ€™s also hard as it can only do around log base 2 N decisions
    so if there is a time series it needs to fit to, that takes 4 steps to get to
    the right time area then suddenly thereâ€™s not many decisions left for it to make
    so itâ€™s kind of this limited amount of computation that it can do. So there is
    a limited complexity of relationship that it can model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šä¸€è¯¾ç»“æŸæ—¶è®¨è®ºçš„é‚£æ ·ï¼Œæˆ‘ä»¬æ­£åœ¨ä»å†³ç­–æ ‘é›†æˆè½¬å‘å¹¿ä¹‰å®šä¹‰çš„ç¥ç»ç½‘ç»œã€‚å¦‚ä½ æ‰€çŸ¥ï¼Œéšæœºæ£®æ—å’Œå†³ç­–æ ‘å—åˆ°ä¸€ä¸ªé™åˆ¶ï¼Œå³å®ƒä»¬åŸºæœ¬ä¸Šåªæ˜¯åœ¨åšæœ€è¿‘é‚»ã€‚å®ƒä»¬æ‰€èƒ½åšçš„å°±æ˜¯è¿”å›ä¸€å †å…¶ä»–ç‚¹çš„å¹³å‡å€¼ã€‚å› æ­¤ï¼Œå®ƒä»¬æ— æ³•å¤–æ¨ï¼Œå¦‚æœä½ åœ¨è€ƒè™‘å¦‚æœæˆ‘å°†ä»·æ ¼æé«˜20%ï¼Œè€Œä½ ä»¥å‰ä»æœªå®šä»·åˆ°é‚£ä¸ªæ°´å¹³ï¼Œæˆ–è€…æ˜å¹´çš„é”€å”®æƒ…å†µä¼šå‘ç”Ÿä»€ä¹ˆï¼Œæ˜¾ç„¶æˆ‘ä»¬ä»¥å‰ä»æœªè§è¿‡æ˜å¹´ï¼Œå¤–æ¨æ˜¯éå¸¸å›°éš¾çš„ã€‚å®ƒä¹Ÿå¾ˆéš¾ï¼Œå› ä¸ºå®ƒåªèƒ½åšå¤§çº¦å¯¹æ•°2çš„Næ¬¡å†³ç­–ï¼Œæ‰€ä»¥å¦‚æœæœ‰ä¸€ä¸ªæ—¶é—´åºåˆ—éœ€è¦æ‹Ÿåˆï¼Œéœ€è¦4æ­¥æ‰èƒ½åˆ°è¾¾æ­£ç¡®çš„æ—¶é—´åŒºåŸŸï¼Œç„¶åçªç„¶å®ƒæ²¡æœ‰å¤šå°‘å†³ç­–å¯ä»¥åšäº†ï¼Œæ‰€ä»¥å®ƒå¯ä»¥åšçš„è®¡ç®—é‡æœ‰é™ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥å»ºæ¨¡çš„å…³ç³»å¤æ‚åº¦æœ‰é™ã€‚
- en: '**Question**: Can I ask about one more drawback of random forests? If we have
    a data as categorical variable which are not in sequential order, for random forests,
    we encode them and treat them as numbers, letâ€™s say we have 20 cardinality so
    the split random forest gives is like less than 5 or less than 6\. But if the
    categories are not sequential (i.e. not in any order), what does that mean [[2:00](https://youtu.be/DzE0eSdy5Hk?t=2m)]?
    So if youâ€™ve got like, letâ€™s go back to bulldozers, EROPS, EROPS w A/C, OROPS,
    N/A, etc, and we arbitrarily label them from 0 to 3\. Actually we know that all
    that really mattered was if it had air conditioning. So whatâ€™s going to happen?
    Itâ€™s basically going to say, if I group it into EROPS w A/C and OROPS together,
    and N/A and EROPS together, thatâ€™s an interesting break just because it so happens
    that the air conditioning ones all are going to end up in the right hand side.
    Having done that, itâ€™s then going to say within the group with the EROPS w A/C
    and OROPS, itâ€™s going to notice that itâ€™s furthermore going to have to split it
    into two more groups. So eventually itâ€™s going to get there. Itâ€™s going to pull
    out the category with AC. Itâ€™s just itâ€™s going to take more splits than we would
    ideally like. So itâ€™s kind of similar to the fact that for it to model a line,
    it can only do it with lots of splits and only approximately.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šæˆ‘å¯ä»¥é—®ä¸€ä¸ªå…³äºéšæœºæ£®æ—çš„å¦ä¸€ä¸ªç¼ºç‚¹å—ï¼Ÿå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®ä½œä¸ºåˆ†ç±»å˜é‡ï¼Œè¿™äº›å˜é‡ä¸æ˜¯æŒ‰é¡ºåºæ’åˆ—çš„ï¼Œå¯¹äºéšæœºæ£®æ—ï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œç¼–ç å¹¶å°†å®ƒä»¬è§†ä¸ºæ•°å­—ï¼Œå‡è®¾æˆ‘ä»¬æœ‰20ä¸ªåŸºæ•°ï¼Œé‚£ä¹ˆéšæœºæ£®æ—ç»™å‡ºçš„åˆ†å‰²ç»“æœå¯èƒ½æ˜¯å°äº5æˆ–å°äº6ã€‚ä½†å¦‚æœç±»åˆ«ä¸æ˜¯æŒ‰é¡ºåºæ’åˆ—ï¼ˆå³æ²¡æœ‰ä»»ä½•é¡ºåºï¼‰ï¼Œé‚£æ„å‘³ç€ä»€ä¹ˆï¼Ÿæ‰€ä»¥å¦‚æœä½ æœ‰ï¼Œæ¯”å¦‚è¯´ï¼Œè®©æˆ‘ä»¬å›åˆ°æ¨åœŸæœºï¼ŒEROPSï¼Œå¸¦ç©ºè°ƒçš„EROPSï¼ŒOROPSï¼ŒN/Aç­‰ï¼Œæˆ‘ä»¬ä»»æ„åœ°å°†å®ƒä»¬æ ‡è®°ä¸º0åˆ°3ã€‚å®é™…ä¸Šæˆ‘ä»¬çŸ¥é“çœŸæ­£é‡è¦çš„æ˜¯æ˜¯å¦æœ‰ç©ºè°ƒã€‚é‚£ä¼šå‘ç”Ÿä»€ä¹ˆï¼ŸåŸºæœ¬ä¸Šå®ƒä¼šè¯´ï¼Œå¦‚æœæˆ‘å°†EROPS
    w A/Cå’ŒOROPSç»„åˆåœ¨ä¸€èµ·ï¼Œå°†N/Aå’ŒEROPSç»„åˆåœ¨ä¸€èµ·ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„åˆ†å‰²ï¼Œå› ä¸ºç¢°å·§æ‰€æœ‰å¸¦ç©ºè°ƒçš„éƒ½ä¼šæœ€ç»ˆå‡ºç°åœ¨å³ä¾§ã€‚åšå®Œè¿™ä¸€æ­¥åï¼Œå®ƒä¼šè¿›ä¸€æ­¥æ³¨æ„åˆ°åœ¨EROPS
    w A/Cå’ŒOROPSç»„ä¸­ï¼Œå®ƒè¿˜éœ€è¦å°†å…¶è¿›ä¸€æ­¥åˆ†æˆä¸¤ç»„ã€‚æœ€ç»ˆå®ƒä¼šåˆ°è¾¾é‚£é‡Œã€‚å®ƒä¼šæå–å¸¦æœ‰ç©ºè°ƒçš„ç±»åˆ«ã€‚åªæ˜¯å®ƒéœ€è¦æ›´å¤šçš„åˆ†å‰²ï¼Œæ¯”æˆ‘ä»¬ç†æƒ³ä¸­å¸Œæœ›çš„è¦å¤šã€‚æ‰€ä»¥è¿™æœ‰ç‚¹ç±»ä¼¼äºå®ƒè¦å»ºæ¨¡ä¸€æ¡çº¿ï¼Œåªèƒ½é€šè¿‡å¤§é‡åˆ†å‰²å¹¶ä¸”åªæ˜¯è¿‘ä¼¼åœ°å®Œæˆã€‚'
- en: '**Follow up question**: So random forest is fine with categories that are not
    sequential also [[3:58](https://youtu.be/DzE0eSdy5Hk?t=3m58s)]? Yes, it can do
    it. Itâ€™s just in ways itâ€™s sub-optimal because we just need to do more breakpoints
    than we would have liked, but it gets there. It does a pretty good job. So even
    although random forests do have some deficiencies, they are incredibly powerful,
    particularly because they have so few assumptions that they are really hard to
    screw up. Itâ€™s kind of hard to actually hard to win Kaggle competition with a
    random forest, but itâ€™s very easy to get like top 10%. So in real life where often
    that third decimal place doesnâ€™t really matter, random forests are often what
    you end up doing. But for some things like this Ecuadorian groceries competition,
    itâ€™s very very hard to get a good result with a random forest because thereâ€™s
    a huge time series component and nearly everything is these two massively high
    cardinality categorical variables which is the store and the item. So thereâ€™s
    very little layer to even throw at a random forest and the difference between
    every pair of stores is kind of different in different ways so there are some
    things that are just hard to get even relatively good results for the random forest.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**åç»­é—®é¢˜**ï¼šé‚£ä¹ˆéšæœºæ£®æ—å¯¹äºä¸æ˜¯è¿ç»­çš„ç±»åˆ«ä¹Ÿå¯ä»¥å—ï¼Ÿæ˜¯çš„ï¼Œå®ƒå¯ä»¥ã€‚åªæ˜¯åœ¨æŸäº›æ–¹é¢å®ƒä¸å¤Ÿç†æƒ³ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åšæ¯”æˆ‘ä»¬æƒ³è¦çš„æ›´å¤šçš„åˆ†å‰²ç‚¹ï¼Œä½†å®ƒå¯ä»¥åšåˆ°ã€‚å®ƒåšå¾—ç›¸å½“ä¸é”™ã€‚å› æ­¤ï¼Œå°½ç®¡éšæœºæ£®æ—ç¡®å®å­˜åœ¨ä¸€äº›ç¼ºé™·ï¼Œä½†å®ƒä»¬éå¸¸å¼ºå¤§ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºå®ƒä»¬å‡ ä¹æ²¡æœ‰å‡è®¾ï¼Œæ‰€ä»¥å¾ˆéš¾å‡ºé”™ã€‚ç”¨éšæœºæ£®æ—èµ¢å¾—Kaggleæ¯”èµ›æœ‰ç‚¹å›°éš¾ï¼Œä½†å¾ˆå®¹æ˜“è¿›å…¥å‰10%ã€‚å› æ­¤ï¼Œåœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œé€šå¸¸ç¬¬ä¸‰ä½å°æ•°å¹¶ä¸æ˜¯å¾ˆé‡è¦ï¼Œéšæœºæ£®æ—é€šå¸¸æ˜¯ä½ æœ€ç»ˆä¼šåšçš„äº‹æƒ…ã€‚ä½†å¯¹äºåƒå„ç“œå¤šå°”æ‚è´§æ¯”èµ›è¿™æ ·çš„äº‹æƒ…ï¼Œç”¨éšæœºæ£®æ—å¾ˆéš¾å¾—åˆ°å¥½çš„ç»“æœï¼Œå› ä¸ºæœ‰ä¸€ä¸ªå·¨å¤§çš„æ—¶é—´åºåˆ—ç»„ä»¶ï¼Œå‡ ä¹æ‰€æœ‰çš„ä¸œè¥¿éƒ½æ˜¯è¿™ä¸¤ä¸ªå¤§è§„æ¨¡é«˜åŸºæ•°çš„åˆ†ç±»å˜é‡ï¼Œå³åº—é“ºå’Œå•†å“ã€‚å› æ­¤ï¼Œç”šè‡³æ²¡æœ‰å¤ªå¤šçš„å±‚å¯ä»¥ç”¨éšæœºæ£®æ—ï¼Œæ¯å¯¹åº—é“ºä¹‹é—´çš„å·®å¼‚åœ¨ä¸åŒæ–¹é¢éƒ½æ˜¯ä¸åŒçš„ï¼Œå› æ­¤æœ‰ä¸€äº›äº‹æƒ…å³ä½¿å¯¹äºéšæœºæ£®æ—æ¥è¯´ä¹Ÿå¾ˆéš¾å¾—åˆ°ç›¸å¯¹å¥½çš„ç»“æœã€‚'
- en: Another example is recognizing numbers. You can get okay results with a random
    forest, but in the end, they are kind of the relationship between the spacial
    structure turns out to be important. And you kind of want to be able to do computations
    like finding edges or whatever that carry forward through the computation. So
    just doing a clever nearest neighbors like a random forest turns out not to be
    ideal. So for stuff like this, neural networks turn out that they are ideal. Neural
    networks turn out to be something that works particularly well for both things
    like Ecuadorian groceries competition (i.e. forecasting sales over time by store
    and by item) and for things like recognizing digits, and for things like turning
    voice into speech. So itâ€™s nice between these two things, neural nets and random
    forests, we cover the territory. I havenâ€™t needed to use anything other than these
    two things for a very long time. And at some point, we will learn how to combine
    the two because you can combine the two in really cool ways.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªä¾‹å­æ˜¯è¯†åˆ«æ•°å­—ã€‚ä½ å¯ä»¥ç”¨éšæœºæ£®æ—å¾—åˆ°å¯ä»¥æ¥å—çš„ç»“æœï¼Œä½†æœ€ç»ˆï¼Œç©ºé—´ç»“æ„ä¹‹é—´çš„å…³ç³»å˜å¾—é‡è¦ã€‚ä½ å¯èƒ½æƒ³è¦èƒ½å¤Ÿè¿›è¡ŒåƒæŸ¥æ‰¾è¾¹ç¼˜æˆ–å…¶ä»–è®¡ç®—ä¸€æ ·çš„è®¡ç®—ï¼Œè¿™äº›è®¡ç®—ä¼šåœ¨è®¡ç®—ä¸­ç»§ç»­è¿›è¡Œã€‚å› æ­¤ï¼Œä»…ä»…åšä¸€ä¸ªèªæ˜çš„æœ€è¿‘é‚»ç±»ä¼¼äºéšæœºæ£®æ—çš„æ–¹æ³•å¹¶ä¸ç†æƒ³ã€‚æ‰€ä»¥å¯¹äºè¿™æ ·çš„äº‹æƒ…ï¼Œç¥ç»ç½‘ç»œæ˜¯ç†æƒ³çš„ã€‚ç¥ç»ç½‘ç»œè¢«è¯æ˜å¯¹äºåƒå„ç“œå¤šå°”æ‚è´§æ¯”èµ›ï¼ˆå³é€šè¿‡åº—é“ºå’Œå•†å“é¢„æµ‹é”€å”®é¢ï¼‰å’Œè¯†åˆ«æ•°å­—è¿™æ ·çš„äº‹æƒ…éå¸¸æœ‰æ•ˆã€‚æ‰€ä»¥åœ¨è¿™ä¸¤ä¸ªäº‹æƒ…ä¹‹é—´ï¼Œç¥ç»ç½‘ç»œå’Œéšæœºæ£®æ—ï¼Œæˆ‘ä»¬è¦†ç›–äº†é¢†åŸŸã€‚æˆ‘å¾ˆé•¿ä¸€æ®µæ—¶é—´ä»¥æ¥ä¸€ç›´æ²¡æœ‰ä½¿ç”¨é™¤äº†è¿™ä¸¤ä¸ªæ–¹æ³•ä¹‹å¤–çš„ä»»ä½•å…¶ä»–æ–¹æ³•ã€‚åœ¨æŸä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å°†è¿™ä¸¤ç§æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œå› ä¸ºä½ å¯ä»¥ä»¥éå¸¸é…·çš„æ–¹å¼å°†å®ƒä»¬ç»“åˆèµ·æ¥ã€‚
- en: MNIST [[6:37](https://youtu.be/DzE0eSdy5Hk?t=6m37s)]
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST [[6:37](https://youtu.be/DzE0eSdy5Hk?t=6m37s)]
- en: '![](../Images/6d0d56b41815a97b5971ddf6c4241d62.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d0d56b41815a97b5971ddf6c4241d62.png)'
- en: Here is a picture from [Adam Geitgey](/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)
    of an image. An image is just a bunch of numbers and each of those numbers is
    naught to 255 and the dark ones are close to 255, light ones are close to zero.
    Here is an example of digit from this MNIST dataset. MNIST is a really old, like
    a hello world of neural networks. So here is an example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯[Adam Geitgey](/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)çš„ä¸€å¼ å›¾ç‰‡ã€‚ä¸€å¼ å›¾ç‰‡åªæ˜¯ä¸€å †æ•°å­—ï¼Œæ¯ä¸ªæ•°å­—éƒ½æ˜¯ä»0åˆ°255ï¼Œæš—çš„æ¥è¿‘255ï¼Œäº®çš„æ¥è¿‘0ã€‚è¿™æ˜¯æ¥è‡ªMNISTæ•°æ®é›†çš„ä¸€ä¸ªæ•°å­—çš„ä¾‹å­ã€‚MNISTæ˜¯ä¸€ä¸ªéå¸¸å¤è€çš„ï¼Œå°±åƒç¥ç»ç½‘ç»œçš„hello
    worldä¸€æ ·ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚
- en: '![](../Images/337b7d6e6d7c98914ba89ff2e1f32983.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/337b7d6e6d7c98914ba89ff2e1f32983.png)'
- en: There are 28 by 28 pixels. If it was color, there would be three of these â€”
    one for red, one for green, one for blue. Our job is to look at the array of numbers
    and figure out that this is the number 8 which is tricky. How do we do that?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰28x28ä¸ªåƒç´ ã€‚å¦‚æœæ˜¯å½©è‰²çš„è¯ï¼Œä¼šæœ‰ä¸‰ä¸ª â€”â€” ä¸€ä¸ªçº¢è‰²çš„ï¼Œä¸€ä¸ªç»¿è‰²çš„ï¼Œä¸€ä¸ªè“è‰²çš„ã€‚æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æŸ¥çœ‹æ•°å­—æ•°ç»„å¹¶å¼„æ¸…æ¥šè¿™æ˜¯ä¸€ä¸ªæ£˜æ‰‹çš„æ•°å­—8ã€‚æˆ‘ä»¬å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Ÿ
- en: We are going to use a small number of FastAI pieces and we are gradually going
    to remove more and more until by the end, weâ€™ll have implemented our own neural
    network from scratch, our own training loop from scratch, and our own matrix multiplication
    from scratch. So we are gradually going to dig in further and further.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ä¸€å°éƒ¨åˆ†FastAIçš„å†…å®¹ï¼Œå¹¶é€æ¸å»é™¤æ›´å¤šï¼Œç›´åˆ°æœ€åï¼Œæˆ‘ä»¬å°†ä»å¤´å¼€å§‹å®ç°è‡ªå·±çš„ç¥ç»ç½‘ç»œï¼Œè‡ªå·±çš„è®­ç»ƒå¾ªç¯ï¼Œä»¥åŠè‡ªå·±çš„çŸ©é˜µä¹˜æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†é€æ¸æ·±å…¥æŒ–æ˜æ›´å¤šã€‚
- en: Data [[7:54](https://youtu.be/DzE0eSdy5Hk?t=7m54s)]
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ® [[7:54](https://youtu.be/DzE0eSdy5Hk?t=7m54s)]
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data for MNIST, which is the name of this very famous dataset is available
    from here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MNISTçš„æ•°æ®ï¼Œè¿™ä¸ªéå¸¸è‘—åçš„æ•°æ®é›†çš„åç§°ï¼Œå¯ä»¥ä»è¿™é‡Œè·å–ï¼š
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And we have a thing in `fastai.io` called `get_data` which will grab it from
    URL and store it on your computer unless itâ€™s already there in which case itâ€™ll
    just go ahead and use it. And weâ€™ve got a little function here called `load_mnist`
    which simply loads it up. Youâ€™ll see that itâ€™s zipped so we could just use Pythonâ€™s
    gzip to open it up. And then itâ€™s also pickled, so if you have any kind of Python
    object at all, you can use this build-in Python library called `pickle` to dump
    it out onto your disk, share it around, load it up later, and you get back the
    same Python object you started with. Youâ€™ve already seen something like this with
    Pandasâ€™ feather format. Pickle is not just for Pandas, itâ€™s not just for anything,
    it works for basically nearly every Python object. So which might lead to the
    question why didnâ€™t we use pickle for a Pandasâ€™ DataFrame. The answer is pickle
    works for nearly every Python object but itâ€™s probably not optimal for nearly
    any Python object. So because we were looking at Pandas DataFrames with over a
    hundred million rows, we really want to save that quickly so feather is a format
    thatâ€™s specifically designed for that purpose and so itâ€™s going to do that really
    fast. If we tried to pickle it, it would have taken a lot longer. Also note that
    pickle files are only for Python so you canâ€™t give them to somebody else where
    else a feather file, you can hand around. So itâ€™s worth knowing that pickle exists
    because if youâ€™ve got some dictionary or some kind of object floating around that
    you want to save for later or send to somebody else, you can always just pickle
    it. So in this particular case, the folks at deeplearning.net was kind enough
    to provide a pickled version.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ `fastai.io` ä¸­æœ‰ä¸€ä¸ªå«åš `get_data` çš„ä¸œè¥¿ï¼Œå®ƒä¼šä» URL ä¸­è·å–æ•°æ®å¹¶å°†å…¶å­˜å‚¨åœ¨ä½ çš„è®¡ç®—æœºä¸Šï¼Œé™¤éå®ƒå·²ç»å­˜åœ¨ï¼Œå¦åˆ™å®ƒå°†ç»§ç»­ä½¿ç”¨å®ƒã€‚æˆ‘ä»¬è¿™é‡Œæœ‰ä¸€ä¸ªå«åš
    `load_mnist` çš„å°å‡½æ•°ï¼Œå®ƒç®€å•åœ°åŠ è½½æ•°æ®ã€‚ä½ ä¼šçœ‹åˆ°å®ƒæ˜¯å‹ç¼©çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python çš„ gzip æ¥æ‰“å¼€å®ƒã€‚ç„¶åå®ƒä¹Ÿè¢« pickledï¼Œæ‰€ä»¥å¦‚æœä½ æœ‰ä»»ä½•ç±»å‹çš„
    Python å¯¹è±¡ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªå†…ç½®çš„ Python åº“å«åš `pickle` æ¥å°†å…¶è½¬å‚¨åˆ°ä½ çš„ç£ç›˜ä¸Šï¼Œåˆ†äº«å®ƒï¼Œç¨ååŠ è½½å®ƒï¼Œä½ ä¼šå¾—åˆ°ä¸å¼€å§‹æ—¶ç›¸åŒçš„ Python
    å¯¹è±¡ã€‚ä½ å·²ç»çœ‹åˆ°äº†ç±»ä¼¼äº Pandas çš„ feather æ ¼å¼çš„ä¸œè¥¿ã€‚Pickle ä¸ä»…ä»…é€‚ç”¨äº Pandasï¼Œä¹Ÿä¸ä»…ä»…é€‚ç”¨äºä»»ä½•ä¸œè¥¿ï¼Œå®ƒåŸºæœ¬ä¸Šé€‚ç”¨äºå‡ ä¹æ¯ä¸ª
    Python å¯¹è±¡ã€‚è¿™å¯èƒ½ä¼šå¼•å‘ä¸€ä¸ªé—®é¢˜ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ä¸º Pandas çš„ DataFrame ä½¿ç”¨ pickleã€‚ç­”æ¡ˆæ˜¯ pickle é€‚ç”¨äºå‡ ä¹æ¯ä¸ª Python
    å¯¹è±¡ï¼Œä½†å¯¹äºå‡ ä¹ä»»ä½• Python å¯¹è±¡æ¥è¯´ï¼Œå®ƒå¯èƒ½ä¸æ˜¯æœ€ä½³é€‰æ‹©ã€‚å› æ­¤ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨æŸ¥çœ‹å…·æœ‰è¶…è¿‡ä¸€äº¿è¡Œçš„ Pandas DataFramesï¼Œæˆ‘ä»¬çœŸçš„å¸Œæœ›å¿«é€Ÿä¿å­˜ï¼Œæ‰€ä»¥
    feather æ˜¯ä¸“é—¨ä¸ºæ­¤ç›®çš„è®¾è®¡çš„æ ¼å¼ï¼Œå› æ­¤å®ƒä¼šéå¸¸å¿«é€Ÿåœ°å®Œæˆã€‚å¦‚æœæˆ‘ä»¬å°è¯• pickle å®ƒï¼Œé‚£å°†éœ€è¦æ›´é•¿çš„æ—¶é—´ã€‚å¦å¤–è¯·æ³¨æ„ï¼Œpickle æ–‡ä»¶ä»…é€‚ç”¨äº
    Pythonï¼Œå› æ­¤ä½ ä¸èƒ½å°†å®ƒä»¬äº¤ç»™å…¶ä»–äººï¼Œè€Œ feather æ–‡ä»¶å¯ä»¥ä¼ é€’ã€‚æ‰€ä»¥å€¼å¾—çŸ¥é“ pickle çš„å­˜åœ¨ï¼Œå› ä¸ºå¦‚æœä½ æœ‰ä¸€äº›å­—å…¸æˆ–æŸç§å¯¹è±¡æ¼‚æµ®åœ¨å‘¨å›´ï¼Œä½ æƒ³è¦ç¨åä¿å­˜æˆ–å‘é€ç»™å…¶ä»–äººï¼Œä½ æ€»æ˜¯å¯ä»¥å°†å…¶
    pickle åŒ–ã€‚æ‰€ä»¥åœ¨è¿™ç§ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œdeeplearning.net çš„äººä»¬å¾ˆå‹å¥½åœ°æä¾›äº†ä¸€ä¸ª pickled ç‰ˆæœ¬ã€‚
- en: Pickle has changed slightly over time so old pickle files like this one (this
    was Python 2 one), you actually have to tell it that it was encoded using this
    particular Python 2 character set [[10:10](https://youtu.be/DzE0eSdy5Hk?t=610)].
    But other than that, Python 2 and 3, you can normally open each otherâ€™s pickle
    files.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pickle éšç€æ—¶é—´çš„æ¨ç§»æœ‰äº›å˜åŒ–ï¼Œæ‰€ä»¥åƒè¿™æ ·çš„æ—§ pickle æ–‡ä»¶ï¼ˆè¿™æ˜¯ Python 2 çš„ä¸€ä¸ªï¼‰ï¼Œä½ å®é™…ä¸Šå¿…é¡»å‘Šè¯‰å®ƒæ˜¯ä½¿ç”¨è¿™ä¸ªç‰¹å®šçš„ Python
    2 å­—ç¬¦é›†ç¼–ç çš„ã€‚ä½†é™¤æ­¤ä¹‹å¤–ï¼ŒPython 2 å’Œ 3ï¼Œä½ é€šå¸¸å¯ä»¥æ‰“å¼€å½¼æ­¤çš„ pickle æ–‡ä»¶ã€‚
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once we loaded that in, we loaded in like so `((x, y), (x_valid, y_valid), _)`.
    And so this thing which we are doing here is called destructuring. Destructuring
    means that `load_mnist` is giving us back a tuple of tuples. If we have on the
    left hand side of the equal sign a tuple of tuples, we can fill all these things
    in. So we are given back a tuple of training data, a tuple of validation data,
    and a tuple of test data. In this case, I donâ€™t care about the test data so I
    just put it into a variable called `_` which Python people tend to think of as
    being a special variable which we put things weâ€™re going to throw away into. Itâ€™s
    actually not special but itâ€™s really common. If you see something assigned to
    underscore, it probably means youâ€™re just throwing it away.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬åŠ è½½äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å°±åƒè¿™æ ·åŠ è½½ `((x, y), (x_valid, y_valid), _)`ã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œæ­£åœ¨åšçš„äº‹æƒ…å«åšè§£æ„ã€‚è§£æ„æ„å‘³ç€
    `load_mnist` ç»™æˆ‘ä»¬è¿”å›äº†ä¸€ä¸ªå…ƒç»„çš„å…ƒç»„ã€‚å¦‚æœåœ¨ç­‰å·çš„å·¦è¾¹æœ‰ä¸€ä¸ªå…ƒç»„çš„å…ƒç»„ï¼Œæˆ‘ä»¬å¯ä»¥å¡«å……æ‰€æœ‰è¿™äº›å†…å®¹ã€‚æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªè®­ç»ƒæ•°æ®çš„å…ƒç»„ï¼Œä¸€ä¸ªéªŒè¯æ•°æ®çš„å…ƒç»„ï¼Œä»¥åŠä¸€ä¸ªæµ‹è¯•æ•°æ®çš„å…ƒç»„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä¸å…³å¿ƒæµ‹è¯•æ•°æ®ï¼Œæ‰€ä»¥æˆ‘æŠŠå®ƒæ”¾åˆ°ä¸€ä¸ªåä¸º
    `_` çš„å˜é‡ä¸­ï¼ŒPython çš„äººä»¬å€¾å‘äºè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„å˜é‡ï¼Œæˆ‘ä»¬æŠŠè¦ä¸¢å¼ƒçš„ä¸œè¥¿æ”¾è¿›å»ã€‚å®ƒå®é™…ä¸Šå¹¶ä¸ç‰¹æ®Šï¼Œä½†éå¸¸å¸¸è§ã€‚å¦‚æœä½ çœ‹åˆ°æœ‰ä¸œè¥¿è¢«èµ‹å€¼ç»™ä¸‹åˆ’çº¿ï¼Œé‚£å¯èƒ½æ„å‘³ç€ä½ åªæ˜¯è¦ä¸¢å¼ƒå®ƒã€‚
- en: By the way, in a Jupyter notebook it does have a special meaning which is the
    last cell that you calculate is always available in underscore [[11:24](https://youtu.be/DzE0eSdy5Hk?t=684)].
    But thatâ€™s kind of a separate issue.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­å®ƒç¡®å®æœ‰ä¸€ä¸ªç‰¹æ®Šçš„å«ä¹‰ï¼Œå³ä½ è®¡ç®—çš„æœ€åä¸€ä¸ªå•å…ƒæ ¼å§‹ç»ˆåœ¨ä¸‹åˆ’çº¿ä¸­å¯ç”¨ã€‚ä½†è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ã€‚
- en: Then the first thing in that tuple is itself a tuple and so weâ€™re going to stick
    that into x and y for our training data, and then the second one goes into x and
    y for our validation data. So thatâ€™s called destructuring and itâ€™s pretty common
    in lots of languages. Some languages donâ€™t support it but those that do, life
    becomes a lot easier. As soon as I look at some new dataset, I just check out
    what have I got. Whatâ€™s its type? Numpy array. Whatâ€™s its shape? 50,000 by 784\.
    Then what about the dependent variables? Thatâ€™s an array, its shape is 50,000.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå…ƒç»„ä¸­çš„ç¬¬ä¸€ä»¶äº‹æœ¬èº«å°±æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æŠŠå®ƒæ”¾å…¥ x å’Œ y ä¸­ä½œä¸ºæˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ï¼Œç„¶åç¬¬äºŒä¸ªå…ƒç»„æ”¾å…¥ x å’Œ y ä¸­ä½œä¸ºæˆ‘ä»¬çš„éªŒè¯æ•°æ®ã€‚æ‰€ä»¥è¿™å°±æ˜¯æ‰€è°“çš„è§£æ„ï¼Œå®ƒåœ¨è®¸å¤šè¯­è¨€ä¸­éƒ½å¾ˆå¸¸è§ã€‚æœ‰äº›è¯­è¨€ä¸æ”¯æŒå®ƒï¼Œä½†é‚£äº›æ”¯æŒçš„è¯­è¨€ï¼Œç”Ÿæ´»ä¼šå˜å¾—æ›´å®¹æ˜“ã€‚ä¸€æ—¦æˆ‘çœ‹åˆ°ä¸€äº›æ–°çš„æ•°æ®é›†ï¼Œæˆ‘å°±ä¼šæŸ¥çœ‹æˆ‘å¾—åˆ°äº†ä»€ä¹ˆã€‚å®ƒæ˜¯ä»€ä¹ˆç±»å‹ï¼ŸNumpy
    æ•°ç»„ã€‚å®ƒçš„å½¢çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿ50,000 x 784ã€‚é‚£ä¹ˆå› å˜é‡å‘¢ï¼Ÿé‚£æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå®ƒçš„å½¢çŠ¶æ˜¯ 50,000ã€‚
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The image of 8 we saw earlier is not of length 784, itâ€™s of size 28 by 28 [[12:18](https://youtu.be/DzE0eSdy5Hk?t=740)].
    So what happened here? It turns out that all they did was they took the second
    row and concatenate it to the first row, and the third row and concatenate it
    to that, and the fourth row and concatenated that. So in other words, they took
    the whole 28 by 28 and flattened it out into a single 1D array. Does that makes
    sense? So itâ€™s going to be of size 28Â². This is not normal by any means, so donâ€™t
    think everything you see is going to be like this. Most of the time when people
    share images, they share them as JPEGs or PNGs, you load them up, you get back
    a nice 2D array. But in this particular case for whatever reason, the thing that
    they pickled was flattened out to be 784\. And this word â€œflattenâ€ is very common
    with working with tensors so when you flatten a tensor, it just means that youâ€™re
    turning it into a lower rank tensor than you started with. In this case, we started
    with a rank 2 tensor (i.e. a matrix) for each image and we turned each one into
    a rank 1 tensor (i.e. a vector). So overall the whole thing is a rank 2 tensor
    rather than a rank 3 . tensor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„8çš„å›¾åƒä¸æ˜¯é•¿åº¦ä¸º784ï¼Œè€Œæ˜¯å¤§å°ä¸º28ä¹˜ä»¥28ã€‚æ‰€ä»¥è¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿäº‹å®è¯æ˜ï¼Œä»–ä»¬åªæ˜¯å°†ç¬¬äºŒè¡Œè¿æ¥åˆ°ç¬¬ä¸€è¡Œï¼Œå°†ç¬¬ä¸‰è¡Œè¿æ¥åˆ°ç¬¬äºŒè¡Œï¼Œå°†ç¬¬å››è¡Œè¿æ¥åˆ°ç¬¬ä¸‰è¡Œã€‚æ¢å¥è¯è¯´ï¼Œä»–ä»¬å°†æ•´ä¸ª28ä¹˜ä»¥28å±•å¹³æˆä¸€ä¸ªå•ä¸€çš„ä¸€ç»´æ•°ç»„ã€‚è¿™æœ‰æ„ä¹‰å—ï¼Ÿæ‰€ä»¥å®ƒçš„å¤§å°å°†æ˜¯28Â²ã€‚è¿™ç»å¯¹ä¸æ˜¯æ­£å¸¸çš„ï¼Œæ‰€ä»¥ä¸è¦è®¤ä¸ºä½ çœ‹åˆ°çš„ä¸€åˆ‡éƒ½ä¼šæ˜¯è¿™æ ·ã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œå½“äººä»¬åˆ†äº«å›¾åƒæ—¶ï¼Œä»–ä»¬ä¼šå°†å®ƒä»¬åˆ†äº«ä¸ºJPEGæˆ–PNGæ ¼å¼ï¼Œä½ åŠ è½½å®ƒä»¬ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ¼‚äº®çš„äºŒç»´æ•°ç»„ã€‚ä½†åœ¨è¿™ç§ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œå‡ºäºæŸç§åŸå› ï¼Œä»–ä»¬æ‹¿å‡ºæ¥çš„ä¸œè¥¿è¢«å±•å¹³æˆäº†784ã€‚è¿™ä¸ªâ€œå±•å¹³â€è¿™ä¸ªè¯åœ¨å¤„ç†å¼ é‡æ—¶éå¸¸å¸¸è§ï¼Œæ‰€ä»¥å½“ä½ å±•å¹³ä¸€ä¸ªå¼ é‡æ—¶ï¼Œè¿™æ„å‘³ç€ä½ å°†å®ƒè½¬æ¢ä¸ºæ¯”ä½ å¼€å§‹çš„æ›´ä½ç§©çš„å¼ é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå›¾åƒå¼€å§‹æ—¶æ˜¯ä¸€ä¸ªç§©ä¸º2çš„å¼ é‡ï¼ˆå³çŸ©é˜µï¼‰ï¼Œç„¶åæˆ‘ä»¬å°†æ¯ä¸ªå›¾åƒè½¬æ¢ä¸ºä¸€ä¸ªç§©ä¸º1çš„å¼ é‡ï¼ˆå³å‘é‡ï¼‰ã€‚æ‰€ä»¥æ•´ä½“æ¥è¯´ï¼Œæ•´ä¸ªä¸œè¥¿æ˜¯ä¸€ä¸ªç§©ä¸º2çš„å¼ é‡ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç§©ä¸º3çš„å¼ é‡ã€‚
- en: So just to remind us of the jargon here [[13:50](https://youtu.be/DzE0eSdy5Hk?t=830)],
    this in math, we would call a vector. In computer science, we would call it a
    1D array, but because deep learning have people who have to come across as smarter
    than everybody else, we have to call this a rank 1 tensor. They all mean the same
    thing more or less unless youâ€™re a physicist â€” in which case, this means something
    else and you get very angry at the deep learning people because you say â€œitâ€™s
    not a tensorâ€. So there you go. Donâ€™t blame me. This is just what people say.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åªæ˜¯ä¸ºäº†æé†’æˆ‘ä»¬è¿™é‡Œçš„è¡Œè¯ï¼Œè¿™åœ¨æ•°å­¦ä¸­æˆ‘ä»¬ä¼šç§°ä¹‹ä¸ºå‘é‡ã€‚åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œæˆ‘ä»¬ä¼šç§°ä¹‹ä¸ºä¸€ç»´æ•°ç»„ï¼Œä½†æ˜¯å› ä¸ºæ·±åº¦å­¦ä¹ çš„äººä»¬å¿…é¡»è¡¨ç°å¾—æ¯”å…¶ä»–äººæ›´èªæ˜ï¼Œæˆ‘ä»¬ä¸å¾—ä¸ç§°ä¹‹ä¸ºç§©ä¸º1çš„å¼ é‡ã€‚å®ƒä»¬åŸºæœ¬ä¸Šæ„æ€ç›¸åŒï¼Œé™¤éä½ æ˜¯ç‰©ç†å­¦å®¶â€”â€”åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™æ„å‘³ç€å…¶ä»–äº‹æƒ…ï¼Œä½ ä¼šå¯¹æ·±åº¦å­¦ä¹ çš„äººä»¬æ„Ÿåˆ°éå¸¸ç”Ÿæ°”ï¼Œå› ä¸ºä½ ä¼šè¯´â€œè¿™ä¸æ˜¯å¼ é‡â€ã€‚æ‰€ä»¥å°±æ˜¯è¿™æ ·ã€‚ä¸è¦è´£æ€ªæˆ‘ã€‚è¿™åªæ˜¯äººä»¬è¯´çš„è¯ã€‚
- en: '![](../Images/6f52769b04a3f34bd5622692e7b815b6.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f52769b04a3f34bd5622692e7b815b6.png)'
- en: So this is either a matrix or a 2D array or a rank 2 tensor.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™è¦ä¹ˆæ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œè¦ä¹ˆæ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œè¦ä¹ˆæ˜¯ä¸€ä¸ªç§©ä¸º2çš„å¼ é‡ã€‚
- en: '![](../Images/7e7bcdcd094e2066d9d6ee0928ec0433.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e7bcdcd094e2066d9d6ee0928ec0433.png)'
- en: Once we start to get into three dimensions, we start to run out of mathematical
    names which is why we start to be nice and just say rank three tensor. So thereâ€™s
    actually nothing special about vectors and matrices that makes them in any way
    more important than rank 3 tensors or rank 4 tensors. So I try not to use the
    terms vector and matrix where possible because I donâ€™t really think theyâ€™re any
    more special than any other rank of tensor. So itâ€™s good to get used to thinking
    of this `numpy.ndarray (50,000, 784)` as a rank 2 tensor.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬å¼€å§‹è¿›å…¥ä¸‰ç»´ï¼Œæˆ‘ä»¬å¼€å§‹ç”¨å®Œæ•°å­¦åå­—ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å¼€å§‹å‹å¥½åœ°è¯´ç§©ä¸º3çš„å¼ é‡ã€‚æ‰€ä»¥å®é™…ä¸Šï¼Œæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„å…³äºå‘é‡å’ŒçŸ©é˜µä½¿å®ƒä»¬æ¯”ç§©ä¸º3æˆ–ç§©ä¸º4çš„å¼ é‡æ›´é‡è¦ã€‚æ‰€ä»¥æˆ‘å°½é‡ä¸ä½¿ç”¨å‘é‡å’ŒçŸ©é˜µè¿™äº›æœ¯è¯­ï¼Œå› ä¸ºæˆ‘çœŸçš„ä¸è®¤ä¸ºå®ƒä»¬æ¯”å…¶ä»–ç§©çš„å¼ é‡æ›´ç‰¹åˆ«ã€‚æ‰€ä»¥ä¹ æƒ¯å°†`numpy.ndarray
    (50,000, 784)`çœ‹ä½œç§©ä¸º2çš„å¼ é‡æ˜¯å¾ˆå¥½çš„ã€‚
- en: And then the rows and columns [[15:25](https://youtu.be/DzE0eSdy5Hk?t=925)].
    If we were computer science people, we would call this dimension zero and dimension
    one. But if we were deep learning people, we would call this axis zero and axis
    one. Then just to be really confusing, if you were an image person, columns are
    the first axis and rows are the second axis.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ˜¯è¡Œå’Œåˆ—ã€‚å¦‚æœæˆ‘ä»¬æ˜¯è®¡ç®—æœºç§‘å­¦äººå‘˜ï¼Œæˆ‘ä»¬ä¼šç§°ä¹‹ä¸ºé›¶ç»´å’Œä¸€ç»´ã€‚ä½†å¦‚æœæˆ‘ä»¬æ˜¯æ·±åº¦å­¦ä¹ äººå‘˜ï¼Œæˆ‘ä»¬ä¼šç§°ä¹‹ä¸ºè½´é›¶å’Œè½´ä¸€ã€‚ç„¶åä¸ºäº†æ›´åŠ æ··æ·†ï¼Œå¦‚æœä½ æ˜¯ä¸€ä¸ªå›¾åƒäººå‘˜ï¼Œåˆ—æ˜¯ç¬¬ä¸€ä¸ªè½´ï¼Œè¡Œæ˜¯ç¬¬äºŒä¸ªè½´ã€‚
- en: '![](../Images/c79efecfed2414479477e970e4608da8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c79efecfed2414479477e970e4608da8.png)'
- en: So if you think about TVs, 1920 by 1080 â€” columns by rows. Everybody else including
    deep learning and mathematicians, rows by columns. So this is pretty confusing
    if you use Python imaging library, you get columns by rows; pretty much everything
    else, rows by columns. So be careful. [A student asks â€œwhy do they do that?â€]
    Because they hate us, because theyâ€™re bad people, I guess ğŸ˜†
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ æƒ³åˆ°ç”µè§†ï¼Œ1920ä¹˜ä»¥1080â€”â€”åˆ—ä¹˜ä»¥è¡Œã€‚å…¶ä»–äººåŒ…æ‹¬æ·±åº¦å­¦ä¹ å’Œæ•°å­¦å®¶ï¼Œè¡Œä¹˜ä»¥åˆ—ã€‚æ‰€ä»¥å¦‚æœä½ ä½¿ç”¨Pythonå›¾åƒåº“ï¼Œä½ ä¼šå¾—åˆ°åˆ—ä¹˜ä»¥è¡Œï¼›å‡ ä¹å…¶ä»–æ‰€æœ‰æƒ…å†µï¼Œè¡Œä¹˜ä»¥åˆ—ã€‚æ‰€ä»¥è¦å°å¿ƒã€‚[ä¸€ä¸ªå­¦ç”Ÿé—®â€œä¸ºä»€ä¹ˆä»–ä»¬è¿™æ ·åšï¼Ÿâ€]å› ä¸ºä»–ä»¬è®¨åŒæˆ‘ä»¬ï¼Œå› ä¸ºä»–ä»¬æ˜¯åäººï¼Œæˆ‘çŒœğŸ˜†
- en: Thereâ€™s a lot of, particularly in deep learning, a whole lot of different areas
    have come together like information theory, computer vision, statistics, signal
    processing and youâ€™ve ended up with this hodgepodge of nomenclature in deep learning
    [[16:39](https://youtu.be/DzE0eSdy5Hk?t=999)]. Often like every version of things
    will be used, so today, we are going to hear about something thatâ€™s called either
    negative log likelihood or binomial or categorical cross entropy, depending on
    where you come from. Weâ€™ve already seen something thatâ€™s called either one hot
    encoding or dummy variables depending on where you come from. And really itâ€™s
    just like the same concept gets kind of somewhat independently invented in different
    fields and eventually they find their way to machine learning and then we donâ€™t
    know what to call them so we call them all of the above â€” something like that.
    So I think thatâ€™s what happened with computer vision rows and columns.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ‰å¾ˆå¤šï¼Œè®¸å¤šä¸åŒé¢†åŸŸæ±‡é›†åœ¨ä¸€èµ·ï¼Œå¦‚ä¿¡æ¯è®ºã€è®¡ç®—æœºè§†è§‰ã€ç»Ÿè®¡å­¦ã€ä¿¡å·å¤„ç†ï¼Œæœ€ç»ˆå½¢æˆäº†æ·±åº¦å­¦ä¹ ä¸­çš„è¿™ç§æ··æ‚çš„å‘½åæ³•ã€‚é€šå¸¸ï¼Œæ¯ä¸ªç‰ˆæœ¬çš„äº‹ç‰©éƒ½ä¼šè¢«ä½¿ç”¨ï¼Œæ‰€ä»¥ä»Šå¤©ï¼Œæˆ‘ä»¬å°†å¬åˆ°ä¸€äº›è¢«ç§°ä¸ºè´Ÿå¯¹æ•°ä¼¼ç„¶æˆ–äºŒé¡¹å¼æˆ–åˆ†ç±»äº¤å‰ç†µçš„ä¸œè¥¿ï¼Œè¿™å–å†³äºä½ æ¥è‡ªå“ªé‡Œã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ä¸€äº›è¢«ç§°ä¸ºç‹¬çƒ­ç¼–ç æˆ–è™šæ‹Ÿå˜é‡çš„ä¸œè¥¿ï¼Œè¿™å–å†³äºä½ æ¥è‡ªå“ªé‡Œã€‚å®é™…ä¸Šï¼Œè¿™åªæ˜¯ç›¸åŒçš„æ¦‚å¿µåœ¨ä¸åŒé¢†åŸŸä¸­æœ‰ç‚¹ç‹¬ç«‹åœ°è¢«å‘æ˜ï¼Œæœ€ç»ˆå®ƒä»¬æ‰¾åˆ°äº†é€šå¾€æœºå™¨å­¦ä¹ çš„é“è·¯ï¼Œç„¶åæˆ‘ä»¬ä¸çŸ¥é“è¯¥å¦‚ä½•ç§°å‘¼å®ƒä»¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§°å®ƒä»¬ä¸ºä»¥ä¸Šæ‰€æœ‰çš„ä¸œè¥¿â€”â€”å°±åƒè¿™æ ·ã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºè¿™å°±æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„è¡Œå’Œåˆ—å‘ç”Ÿçš„äº‹æƒ…ã€‚
- en: Normalize [[17:38](https://youtu.be/DzE0eSdy5Hk?t=1058)]
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å½’ä¸€åŒ–
- en: Thereâ€™s this idea of normalizing data which is subtracting out the mean and
    dividing by the standard deviation. A question for you. Often itâ€™s important to
    normalize the data so that we can more easily train a model. Do you think it would
    be important to normalize the independent variables for a random forest (if we
    are training a random forest)?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¿™æ ·ä¸€ä¸ªæ¦‚å¿µï¼Œå³å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œå³å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®ã€‚ä¸€ä¸ªé—®é¢˜ç»™ä½ ã€‚é€šå¸¸ï¼Œå½’ä¸€åŒ–æ•°æ®å¾ˆé‡è¦ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ›´å®¹æ˜“åœ°è®­ç»ƒæ¨¡å‹ã€‚ä½ è®¤ä¸ºåœ¨è®­ç»ƒéšæœºæ£®æ—æ—¶ï¼Œå½’ä¸€åŒ–ç‹¬ç«‹å˜é‡æ˜¯å¦é‡è¦å‘¢ï¼Ÿ
- en: 'Student: To be honest, I donâ€™t know why we donâ€™t need to normalize, I just
    know that we donâ€™t.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ç”Ÿï¼šè€å®è¯´ï¼Œæˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸éœ€è¦å½’ä¸€åŒ–ï¼Œæˆ‘åªçŸ¥é“æˆ‘ä»¬ä¸éœ€è¦ã€‚
- en: Okay, does anybody want to think about why? So really, the key is that when
    we are deciding where to split, all that matters is the order. Like all that matters
    is how they are sorted, so if we subtract the mean divide by the standard deviation,
    they are still sorted in the same order. So remember when we implemented the random
    forest, we said sort them and then we completely ignored the values. We just said
    now add on one thing from the dependent at a time. So random forests only care
    about the sort order of the independent variables. They donâ€™t care at all about
    their size. So thatâ€™s why theyâ€™re wonderfully immune to outliers because they
    totally ignore the fact that itâ€™s an outlier, they only care about which one is
    higher than what other thing. So this is an important concept. It doesnâ€™t just
    appear in random forests. It occurs in some metrics as well. For example, area
    under the ROC curve, you come across a lot, area under the ROC curve completely
    ignores scale and only cares about sort. We saw something else when we did the
    dendrogram. Spearmanâ€™s correlation is a rank correlation â€” only cares about order,
    not about scale. So random forests, one of the many wonderful things about them
    are that we can completely ignore a lot of these statistical distribution issues.
    But we canâ€™t for deep learning because deep learning, we are trying to train a
    parameterized model. So we do need to normalize our data. If we donâ€™t then itâ€™s
    going to be much harder to create a network that trains effectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæœ‰äººæƒ³æƒ³ä¸ºä»€ä¹ˆå—ï¼ŸçœŸæ­£çš„å…³é”®æ˜¯ï¼Œå½“æˆ‘ä»¬å†³å®šåœ¨å“ªé‡Œåˆ†å‰²æ—¶ï¼Œå”¯ä¸€é‡è¦çš„æ˜¯é¡ºåºã€‚å°±åƒå”¯ä¸€é‡è¦çš„æ˜¯å®ƒä»¬æ˜¯å¦‚ä½•æ’åºçš„ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬å‡å»å‡å€¼é™¤ä»¥æ ‡å‡†å·®ï¼Œå®ƒä»¬ä»ç„¶æŒ‰ç›¸åŒçš„é¡ºåºæ’åºã€‚æ‰€ä»¥è®°ä½å½“æˆ‘ä»¬å®ç°éšæœºæ£®æ—æ—¶ï¼Œæˆ‘ä»¬è¯´å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œç„¶åå®Œå…¨å¿½ç•¥å€¼ã€‚æˆ‘ä»¬åªæ˜¯è¯´ç°åœ¨ä¸€æ¬¡æ·»åŠ ä¸€ä¸ªæ¥è‡ªä¾èµ–å˜é‡çš„ä¸œè¥¿ã€‚æ‰€ä»¥éšæœºæ£®æ—åªå…³å¿ƒç‹¬ç«‹å˜é‡çš„æ’åºé¡ºåºã€‚å®ƒä»¬æ ¹æœ¬ä¸å…³å¿ƒå®ƒä»¬çš„å¤§å°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒä»¬å¯¹å¼‚å¸¸å€¼éå¸¸å…ç–«çš„åŸå› ï¼Œå› ä¸ºå®ƒä»¬å®Œå…¨å¿½ç•¥äº†å®ƒæ˜¯å¼‚å¸¸å€¼ï¼Œå®ƒä»¬åªå…³å¿ƒå“ªä¸ªæ¯”å…¶ä»–ä¸œè¥¿æ›´é«˜ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µã€‚å®ƒä¸ä»…å‡ºç°åœ¨éšæœºæ£®æ—ä¸­ã€‚å®ƒä¹Ÿå‡ºç°åœ¨ä¸€äº›æŒ‡æ ‡ä¸­ã€‚ä¾‹å¦‚ï¼ŒROCæ›²çº¿ä¸‹é¢ç§¯ï¼Œä½ ä¼šç»å¸¸é‡åˆ°ï¼ŒROCæ›²çº¿ä¸‹é¢ç§¯å®Œå…¨å¿½ç•¥äº†æ¯”ä¾‹ï¼Œåªå…³å¿ƒæ’åºã€‚å½“æˆ‘ä»¬åšæ ‘çŠ¶å›¾æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦ä¸€ç§æƒ…å†µã€‚æ–¯çš®å°”æ›¼ç›¸å…³æ˜¯ä¸€ç§ç§©ç›¸å…³â€”â€”åªå…³å¿ƒé¡ºåºï¼Œä¸å…³å¿ƒæ¯”ä¾‹ã€‚æ‰€ä»¥éšæœºæ£®æ—çš„è®¸å¤šç¾å¥½ä¹‹å¤„ä¹‹ä¸€æ˜¯æˆ‘ä»¬å¯ä»¥å®Œå…¨å¿½ç•¥è®¸å¤šè¿™äº›ç»Ÿè®¡åˆ†å¸ƒé—®é¢˜ã€‚ä½†æ˜¯å¯¹äºæ·±åº¦å­¦ä¹ æ¥è¯´ä¸è¡Œï¼Œå› ä¸ºåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬è¯•å›¾è®­ç»ƒä¸€ä¸ªå‚æ•°åŒ–æ¨¡å‹ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ã€‚å¦‚æœä¸è¿™æ ·åšï¼Œé‚£ä¹ˆåˆ›å»ºä¸€ä¸ªæœ‰æ•ˆè®­ç»ƒçš„ç½‘ç»œå°†ä¼šæ›´åŠ å›°éš¾ã€‚
- en: So we grab the mean and the standard deviation of our training data and subtract
    out the mean, divide by the standard deviation, and that gives us a mean of zero
    and standard deviation of one [[20:53](https://youtu.be/DzE0eSdy5Hk?t=1253)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æŠ“å–æˆ‘ä»¬è®­ç»ƒæ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå‡å»å‡å€¼ï¼Œé™¤ä»¥æ ‡å‡†å·®ï¼Œè¿™ç»™æˆ‘ä»¬ä¸€ä¸ªå‡å€¼ä¸ºé›¶ï¼Œæ ‡å‡†å·®ä¸ºä¸€çš„ç»“æœã€‚
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now for our validation data, we need to use the standard deviation and mean
    from the training data. We have to normalize it the same way. Just like categorical
    variables, we had to make sure they had the same indexes mapped to the same levels
    for a random forest. Or missing values, we had to make sure we have the same median
    used when we were replacing the missing values. You need to make sure anything
    you do in the training set, you do exactly the same thing in the test and validation
    set. So here, Iâ€™m subtracting out the training set mean and dividing by the training
    set standard deviation, so this is not exactly zero and one, but itâ€™s pretty close.
    So in general, if you find you try something on a validation set or a test set
    and itâ€™s much much much worse, than your training set, thatâ€™s probably because
    you normalized in an inconsistent way or encoded categories in an inconsistent
    way or something like that.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å¯¹äºæˆ‘ä»¬çš„éªŒè¯æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æ ‡å‡†å·®å’Œå‡å€¼ã€‚æˆ‘ä»¬å¿…é¡»ä»¥ç›¸åŒçš„æ–¹å¼å¯¹å…¶è¿›è¡Œæ ‡å‡†åŒ–ã€‚å°±åƒåˆ†ç±»å˜é‡ä¸€æ ·ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿å®ƒä»¬çš„ç›¸åŒç´¢å¼•æ˜ å°„åˆ°éšæœºæ£®æ—ä¸­çš„ç›¸åŒçº§åˆ«ã€‚æˆ–è€…ç¼ºå¤±å€¼ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿åœ¨æ›¿æ¢ç¼ºå¤±å€¼æ—¶ä½¿ç”¨ç›¸åŒçš„ä¸­ä½æ•°ã€‚ä½ éœ€è¦ç¡®ä¿ä½ åœ¨è®­ç»ƒé›†ä¸­åšçš„ä»»ä½•äº‹æƒ…ï¼Œåœ¨æµ‹è¯•å’ŒéªŒè¯é›†ä¸­éƒ½è¦å®Œå…¨ç›¸åŒã€‚æ‰€ä»¥åœ¨è¿™é‡Œï¼Œæˆ‘å‡å»äº†è®­ç»ƒé›†çš„å‡å€¼å¹¶é™¤ä»¥è®­ç»ƒé›†çš„æ ‡å‡†å·®ï¼Œæ‰€ä»¥è¿™ä¸æ˜¯å®Œå…¨æ˜¯é›¶å’Œä¸€ï¼Œä½†å®ƒéå¸¸æ¥è¿‘ã€‚æ€»çš„æ¥è¯´ï¼Œå¦‚æœä½ å‘ç°ä½ åœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šå°è¯•æŸäº›ä¸œè¥¿ï¼Œè€Œå®ƒæ¯”ä½ çš„è®­ç»ƒé›†å·®å¾—å¤šå¾—å¤šï¼Œé‚£å¯èƒ½æ˜¯å› ä¸ºä½ ä»¥ä¸ä¸€è‡´çš„æ–¹å¼è¿›è¡Œäº†æ ‡å‡†åŒ–æˆ–ç¼–ç ç±»åˆ«æˆ–å…¶ä»–ä¸€äº›ä¸ä¸€è‡´çš„æ–¹å¼ã€‚
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Looking at the data [[22:03](https://youtu.be/DzE0eSdy5Hk?t=1323)]
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æ•°æ®[[22:03](https://youtu.be/DzE0eSdy5Hk?t=1323)]
- en: Letâ€™s take a look at some of this data. So weâ€™ve got 10,000 images in the validation
    set and each one is a rank one tensor of length 784.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™äº›æ•°æ®ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨éªŒè¯é›†ä¸­æœ‰10,000å¼ å›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½æ˜¯é•¿åº¦ä¸º784çš„ç§©ä¸º1çš„å¼ é‡ã€‚
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In order to display it, I want to turn it into a rank 2 tensor of 28 by 28\.
    Numpy has a reshape function that takes a tensor in and reshapes it to whatever
    size tensor you request. Now if you think about it, you only need to tell it about
    if there are *D* axes, you only need to tell it about *D-1* of the axes you want
    because the last one, it can figure out for itself. So in total, there are 10,000
    by 784 numbers here altogether. so if you way I want my last axes to be 28 by
    28, then you can figure out that (the first axis) this must be 10,000 otherwise
    itâ€™s not going to fit. So if you put -1, it says make it as big or as small as
    you have to to make it fit. So you can see here, it figured out that it has to
    be 10,000\. Youâ€™ll see this used in neural net software pre-processing and stuff
    like that all the time. I could have written 10,000 here, but I try to get into
    a habit of like anytime Iâ€™m referring to how many items in my input, I tend to
    use -1 because it means later on I could use a subsample, this code wouldnâ€™t break.
    I could do some kind of stratified sampling if it was unbalanced, this code wouldnâ€™t
    break. So by using this kind of approach of saying -1 here for the size, it just
    makes it more resilient to changes later. Itâ€™s a good habit to get into.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ˜¾ç¤ºå®ƒï¼Œæˆ‘æƒ³å°†å®ƒè½¬æ¢ä¸ºä¸€ä¸ª28x28çš„ç§©ä¸º2çš„å¼ é‡ã€‚Numpyæœ‰ä¸€ä¸ªreshapeå‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ªå¼ é‡å¹¶å°†å…¶é‡å¡‘ä¸ºä½ è¯·æ±‚çš„ä»»ä½•å¤§å°çš„å¼ é‡ã€‚ç°åœ¨å¦‚æœä½ è€ƒè™‘ä¸€ä¸‹ï¼Œä½ åªéœ€è¦å‘Šè¯‰å®ƒæœ‰*D*ä¸ªè½´ï¼Œä½ åªéœ€è¦å‘Šè¯‰å®ƒä½ æƒ³è¦çš„*D-1*ä¸ªè½´ï¼Œå› ä¸ºæœ€åä¸€ä¸ªï¼Œå®ƒå¯ä»¥è‡ªå·±ç®—å‡ºæ¥ã€‚æ‰€ä»¥æ€»å…±ï¼Œè¿™é‡Œä¸€å…±æœ‰10,000ä¹˜ä»¥784ä¸ªæ•°å­—ã€‚æ‰€ä»¥å¦‚æœä½ è¯´æˆ‘å¸Œæœ›æˆ‘çš„æœ€åä¸€ä¸ªè½´æ˜¯28x28ï¼Œé‚£ä¹ˆä½ å¯ä»¥ç®—å‡ºï¼ˆç¬¬ä¸€ä¸ªè½´ï¼‰è¿™å¿…é¡»æ˜¯10,000ï¼Œå¦åˆ™å®ƒå°±ä¸ä¼šé€‚åˆã€‚æ‰€ä»¥å¦‚æœä½ æ”¾-1ï¼Œå®ƒä¼šè¯´è®©å®ƒå°½å¯èƒ½å¤§æˆ–å°½å¯èƒ½å°ä»¥ä½¿å…¶é€‚åˆã€‚æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°ï¼Œå®ƒç®—å‡ºæ¥å¿…é¡»æ˜¯10,000ã€‚ä½ ä¼šçœ‹åˆ°è¿™ç§æ–¹æ³•åœ¨ç¥ç»ç½‘ç»œè½¯ä»¶çš„é¢„å¤„ç†ä¸­ç»å¸¸ä½¿ç”¨ã€‚æˆ‘å¯ä»¥åœ¨è¿™é‡Œå†™10,000ï¼Œä½†æˆ‘è¯•å›¾å…»æˆä¸€ç§ä¹ æƒ¯ï¼Œå°±æ˜¯æ¯å½“æˆ‘æåˆ°è¾“å…¥ä¸­æœ‰å¤šå°‘é¡¹æ—¶ï¼Œæˆ‘å€¾å‘äºä½¿ç”¨-1ï¼Œå› ä¸ºè¿™æ„å‘³ç€ä»¥åæˆ‘å¯ä»¥ä½¿ç”¨å­æ ·æœ¬ï¼Œè¿™æ®µä»£ç ä¸ä¼šå‡ºé”™ã€‚å¦‚æœå®ƒæ˜¯ä¸å¹³è¡¡çš„ï¼Œæˆ‘å¯ä»¥è¿›è¡Œä¸€äº›åˆ†å±‚æŠ½æ ·ï¼Œè¿™æ®µä»£ç ä¸ä¼šå‡ºé”™ã€‚æ‰€ä»¥é€šè¿‡åœ¨è¿™é‡Œä½¿ç”¨-1è¿™ç§å¤§å°ï¼Œå®ƒä½¿å¾—ä»¥åçš„æ›´æ”¹æ›´å…·å¼¹æ€§ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¹ æƒ¯ã€‚
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This idea of being able to take tensors and reshape them and change axes around
    and stuff like that is something you need to be able to totally do without thinking
    [[23:56](https://youtu.be/DzE0eSdy5Hk?t=1436)]. Because itâ€™s going to happen all
    the time. So for example, here is one. I tried to read in some images, they were
    flattened, I need to unflatten them into a bunch of matrices â€” okay, reshape.
    bang. I read some images in with OpenCV and it turns out OpenCV orders the channels
    blue green red, everything else expects them to be red green blue. I need to reverse
    the last axes. How do you do that? I read in some images with Python imaging library.
    It reads them as rows by columns by channels, PyTorch expects channels by rows
    by columns. How do I transform that. So these are all things you need to be able
    to do without thinking, like straightaway. Because it happens all the time and
    you never want to be sitting there thinking about it for ages. So make sure you
    spend a lot of time over the week just practicing with things like all the stuff
    you are going to see today: reshaping, slicing, reordering dimensions, stuff like
    that. So the best way is to create some small tensors yourself and start thinking
    like okay what shall I experiment with.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿå–å¼ é‡å¹¶é‡æ–°å¡‘å½¢ã€æ”¹å˜è½´çº¿ç­‰ç­‰çš„æƒ³æ³•æ˜¯ä½ éœ€è¦èƒ½å¤Ÿå®Œå…¨ä¸å‡æ€ç´¢åœ°åšåˆ°çš„[[23:56](https://youtu.be/DzE0eSdy5Hk?t=1436)]ã€‚å› ä¸ºè¿™ç§æƒ…å†µä¼šç»å¸¸å‘ç”Ÿã€‚æ¯”å¦‚ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªä¾‹å­ã€‚æˆ‘å°è¯•è¯»å–ä¸€äº›å›¾åƒï¼Œå®ƒä»¬æ˜¯æ‰å¹³åŒ–çš„ï¼Œæˆ‘éœ€è¦å°†å®ƒä»¬é‡æ–°å¡‘å½¢æˆä¸€å †çŸ©é˜µâ€”â€”å¥½çš„ï¼Œé‡æ–°å¡‘å½¢ã€‚æˆ‘ç”¨OpenCVè¯»å–äº†ä¸€äº›å›¾åƒï¼Œç»“æœå‘ç°OpenCVæŒ‰ç…§è“ç»¿çº¢çš„é¡ºåºæ’åˆ—é€šé“ï¼Œå…¶ä»–æ‰€æœ‰çš„éƒ½å¸Œæœ›å®ƒä»¬æ˜¯çº¢ç»¿è“çš„ã€‚æˆ‘éœ€è¦é¢ å€’æœ€åä¸€ä¸ªè½´ã€‚å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Ÿæˆ‘ç”¨Pythonå›¾åƒåº“è¯»å–äº†ä¸€äº›å›¾åƒã€‚å®ƒå°†å®ƒä»¬è¯»å–ä¸ºè¡Œã€åˆ—ã€é€šé“ï¼ŒPyTorchå¸Œæœ›é€šé“ã€è¡Œã€åˆ—ã€‚æˆ‘è¯¥å¦‚ä½•è½¬æ¢ã€‚æ‰€ä»¥è¿™äº›éƒ½æ˜¯ä½ éœ€è¦èƒ½å¤Ÿä¸å‡æ€ç´¢åœ°åšåˆ°çš„äº‹æƒ…ï¼Œå°±åƒç«‹åˆ»å°±èƒ½åšåˆ°ã€‚å› ä¸ºè¿™ç§æƒ…å†µç»å¸¸å‘ç”Ÿï¼Œä½ ç»ä¸æƒ³ååœ¨é‚£é‡Œæƒ³äº†å¾ˆä¹…ã€‚æ‰€ä»¥ç¡®ä¿ä½ åœ¨è¿™ä¸€å‘¨èŠ±å¾ˆå¤šæ—¶é—´ç»ƒä¹ ä»Šå¤©ä½ å°†çœ‹åˆ°çš„æ‰€æœ‰ä¸œè¥¿ï¼šé‡æ–°å¡‘å½¢ã€åˆ‡ç‰‡ã€é‡æ–°æ’åºç»´åº¦ç­‰ç­‰ã€‚æ‰€ä»¥æœ€å¥½çš„æ–¹æ³•æ˜¯è‡ªå·±åˆ›å»ºä¸€äº›å°å¼ é‡ï¼Œå¼€å§‹æ€è€ƒï¼Œæ¯”å¦‚æˆ‘åº”è¯¥å°è¯•ä»€ä¹ˆã€‚
- en: '**Question**: Back in normalize, you said many machine learning algorithms
    behave better when the data is normalized, but you also just said scales donâ€™t
    really matter [[25:26](https://youtu.be/DzE0eSdy5Hk?t=1526)]? I said it doesnâ€™t
    matter for random forests. So random forests are just going to spit things based
    on order and so we love them. We love random forests for the way they are so immune
    to worrying about distributional assumptions. But we are not doing random forests.
    We are doing deep learning. And deep learning does care.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šåœ¨å½’ä¸€åŒ–æ—¶ï¼Œæ‚¨è¯´è®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•åœ¨æ•°æ®å½’ä¸€åŒ–æ—¶è¡¨ç°æ›´å¥½ï¼Œä½†æ‚¨ä¹Ÿåˆšåˆšè¯´è¿‡å°ºåº¦å¹¶ä¸é‡è¦ï¼Ÿæˆ‘è¯´å¯¹äºéšæœºæ£®æ—æ¥è¯´å¹¶ä¸é‡è¦ã€‚å› æ­¤ï¼Œéšæœºæ£®æ—åªä¼šæ ¹æ®é¡ºåºè¾“å‡ºç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬å–œæ¬¢å®ƒä»¬ã€‚æˆ‘ä»¬å–œæ¬¢éšæœºæ£®æ—æ˜¯å› ä¸ºå®ƒä»¬å¯¹åˆ†å¸ƒå‡è®¾ä¸å¤ªæ‹…å¿ƒã€‚ä½†æˆ‘ä»¬ç°åœ¨ä¸æ˜¯åœ¨åšéšæœºæ£®æ—ï¼Œæˆ‘ä»¬åœ¨åšæ·±åº¦å­¦ä¹ ã€‚è€Œæ·±åº¦å­¦ä¹ ç¡®å®ä¼šåœ¨ä¹å°ºåº¦ã€‚
- en: '**Question**: If we have parametric, then we should scale. If we have non-parametric,
    we shouldnâ€™t have to scale [[26:06](https://youtu.be/DzE0eSdy5Hk?t=1566)]? No
    not quite. Because like k-nearest neighbors is nonparametric and scale matters
    heck of a lot, so I would say things involving trees generally it just going to
    split at a point and so probably you donâ€™t care about scale but you probably just
    need to think like is this an algorithm that uses order or does it use specific
    numbers.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šå¦‚æœæˆ‘ä»¬æœ‰å‚æ•°åŒ–ï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥è¿›è¡Œå°ºåº¦è°ƒæ•´ã€‚å¦‚æœæˆ‘ä»¬æœ‰éå‚æ•°åŒ–ï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦è¿›è¡Œå°ºåº¦è°ƒæ•´ï¼Ÿä¸å®Œå…¨æ˜¯è¿™æ ·ã€‚å› ä¸ºåƒkæœ€è¿‘é‚»æ˜¯éå‚æ•°åŒ–çš„ï¼Œå°ºåº¦å¾ˆé‡è¦ï¼Œæ‰€ä»¥æˆ‘ä¼šè¯´æ¶‰åŠæ ‘çš„äº‹æƒ…é€šå¸¸åªä¼šåœ¨æŸä¸ªç‚¹è¿›è¡Œåˆ†å‰²ï¼Œæ‰€ä»¥ä½ å¯èƒ½ä¸åœ¨ä¹å°ºåº¦ï¼Œä½†ä½ å¯èƒ½éœ€è¦è€ƒè™‘è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨é¡ºåºè¿˜æ˜¯ä½¿ç”¨å…·ä½“æ•°å­—çš„ç®—æ³•ã€‚
- en: '**Question**: Can you give us an intuition of why it needs scale just because
    that may clarify some of the issues [[26:38](https://youtu.be/DzE0eSdy5Hk?t=1598)]?
    Not until we get to doing SGD, so we are going to get to that. So for now, weâ€™re
    just going to say take my word for it.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šæ‚¨èƒ½ç›´è§‚è§£é‡Šä¸€ä¸‹ä¸ºä»€ä¹ˆå®ƒéœ€è¦å°ºåº¦å—ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šæ¾„æ¸…ä¸€äº›é—®é¢˜ï¼Ÿç›´åˆ°æˆ‘ä»¬å¼€å§‹è¿›è¡Œéšæœºæ¢¯åº¦ä¸‹é™æ—¶æ‰éœ€è¦ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šè®²åˆ°é‚£ä¸€ç‚¹ã€‚æ‰€ä»¥ç°åœ¨ï¼Œæˆ‘ä»¬åªèƒ½è¯´ç›¸ä¿¡æˆ‘çš„è¯ã€‚
- en: '**Question**: Can you explain a little bit more what you mean by scale? Because
    when I think of scale, I think all the numbers should be generally the same size.
    Is that the case with the cats and dogs that we over with the deep learning like
    you could have a small cat and a larger cat but it would still know what those
    are both cats [[26:54](https://youtu.be/DzE0eSdy5Hk?t=1614)]? I guess this is
    one of those problems where language gets overloaded. So in computer vision, when
    we scale an image, we are actually increasing the size of the cat. In this case,
    we are scaling the actual pixel values. So in both case, scaling means to make
    something bigger and smaller. In this case, we are taking the numbers from naught
    to 255 and making them so that they have an average of zero and a standard deviation
    of one.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šæ‚¨èƒ½è§£é‡Šä¸€ä¸‹å°ºåº¦æ˜¯ä»€ä¹ˆæ„æ€å—ï¼Ÿå› ä¸ºå½“æˆ‘æƒ³åˆ°å°ºåº¦æ—¶ï¼Œæˆ‘è®¤ä¸ºæ‰€æœ‰æ•°å­—åº”è¯¥å¤§è‡´ç›¸åŒå¤§å°ã€‚åœ¨æˆ‘ä»¬è¿›è¡Œæ·±åº¦å­¦ä¹ æ—¶ï¼ŒçŒ«å’Œç‹—çš„æƒ…å†µæ˜¯è¿™æ ·çš„ï¼Œä½ å¯èƒ½æœ‰ä¸€åªå°çŒ«å’Œä¸€åªå¤§çŒ«ï¼Œä½†å®ƒä»ç„¶çŸ¥é“å®ƒä»¬éƒ½æ˜¯çŒ«ï¼Ÿæˆ‘æƒ³è¿™æ˜¯è¯­è¨€è¢«é‡è½½çš„é—®é¢˜ä¹‹ä¸€ã€‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œå½“æˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œç¼©æ”¾æ—¶ï¼Œå®é™…ä¸Šæ˜¯å¢åŠ äº†çŒ«çš„å¤§å°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ­£åœ¨ç¼©æ”¾å®é™…çš„åƒç´ å€¼ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œç¼©æ”¾æ„å‘³ç€ä½¿æŸç‰©å˜å¤§å’Œå˜å°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æ•°å­—ä»é›¶åˆ°255ï¼Œå¹¶ä½¿å®ƒä»¬çš„å¹³å‡å€¼ä¸ºé›¶ï¼Œæ ‡å‡†å·®ä¸ºä¸€ã€‚
- en: '**Question**: Could you explain us is it by column? by row? In general when
    you are scaling, just not thinking about a picture but kind of input to machine
    learning [[27:43](https://youtu.be/DzE0eSdy5Hk?t=1663)]. Okay, sure. I mean itâ€™s
    a little bit subtle, but in this case, Iâ€™ve just got a single mean and a single
    standard deviation. So itâ€™s basically on average, how much black is there. So
    on average, we have a mean and a standard deviation across all the pixels. In
    computer vision, we would normally do it by channel, so we would normally have
    one number for red, one number for green, one number for blue. In general, you
    need a different set of normalization coefficients for each thing you would expect
    to behave differently. So if we were doing like a structured dataset where weâ€™ve
    got like income, distance in kilometers, and a number of children, you need three
    separate normalization coefficients for those as they are very different kinds
    of things. So itâ€™s a bit domain-specific here. In this case, all of the pixels
    are levels of gray so we just got a single scaling number. Where else you could
    imagine if they were red vs. green vs. blue, you would need to scale those channels
    in different ways.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šæ‚¨èƒ½è§£é‡Šä¸€ä¸‹æ˜¯æŒ‰åˆ—è¿˜æ˜¯æŒ‰è¡Œï¼Ÿä¸€èˆ¬æ¥è¯´ï¼Œå½“æ‚¨è¿›è¡Œç¼©æ”¾æ—¶ï¼Œä¸ä»…ä»…è€ƒè™‘å›¾åƒï¼Œè€Œæ˜¯è¾“å…¥åˆ°æœºå™¨å­¦ä¹ çš„å†…å®¹ã€‚å¥½çš„ï¼Œå½“ç„¶ã€‚è¿™æœ‰ç‚¹å¾®å¦™ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘åªæœ‰ä¸€ä¸ªå¹³å‡å€¼å’Œä¸€ä¸ªæ ‡å‡†å·®ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šï¼Œå¹³å‡æœ‰å¤šå°‘é»‘è‰²ã€‚å› æ­¤ï¼Œå¹³å‡è€Œè¨€ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå¹³å‡å€¼å’Œä¸€ä¸ªæ ‡å‡†å·®è·¨è¶Šæ‰€æœ‰åƒç´ ã€‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæŒ‰é€šé“è¿›è¡Œæ“ä½œï¼Œæ‰€ä»¥é€šå¸¸ä¼šæœ‰ä¸€ä¸ªæ•°å­—ä»£è¡¨çº¢è‰²ï¼Œä¸€ä¸ªæ•°å­—ä»£è¡¨ç»¿è‰²ï¼Œä¸€ä¸ªæ•°å­—ä»£è¡¨è“è‰²ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ‚¨éœ€è¦ä¸ºæ¯ä¸ªæ‚¨å¸Œæœ›è¡¨ç°ä¸åŒçš„äº‹ç‰©å‡†å¤‡ä¸åŒçš„å½’ä¸€åŒ–ç³»æ•°ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬åƒå¤„ç†ä¸€ä¸ªç»“æ„åŒ–æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬æ”¶å…¥ã€å…¬é‡Œæ•°å’Œå­©å­æ•°é‡ï¼Œæ‚¨éœ€è¦ä¸ºè¿™äº›äº‹ç‰©å‡†å¤‡ä¸‰ä¸ªå•ç‹¬çš„å½’ä¸€åŒ–ç³»æ•°ï¼Œå› ä¸ºå®ƒä»¬æ˜¯éå¸¸ä¸åŒçš„äº‹ç‰©ã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œæœ‰ç‚¹é¢†åŸŸç‰¹å®šã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰€æœ‰åƒç´ éƒ½æ˜¯ç°åº¦çº§åˆ«ï¼Œå› æ­¤æˆ‘ä»¬åªæœ‰ä¸€ä¸ªç¼©æ”¾æ•°å­—ã€‚è€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œå¦‚æœå®ƒä»¬æ˜¯çº¢è‰²ä¸ç»¿è‰²ä¸è“è‰²ï¼Œæ‚¨éœ€è¦ä»¥ä¸åŒæ–¹å¼ç¼©æ”¾è¿™äº›é€šé“ã€‚
- en: '**Question**: So Iâ€™m having a little bit of trouble imagining what would happen
    if you donâ€™t normalize in this case [[29:19](https://youtu.be/DzE0eSdy5Hk?t=1759)].
    Weâ€™ll get there. So this is kind of what Yannet was saying like why do we normalize
    and for now, we are normalizing because I say we have to. When we get to looking
    at stochastic gradient descent, weâ€™ll basically discover that if youâ€¦ Basically
    to skip ahead a little bit, we are going to be doing a matrix multiply by a bunch
    of weights. We are going to pick those weights in such a way that when we do the
    matrix multiply, we are going to try to keep the number at the same scale that
    they started out as. And thatâ€™s going to basically require the initial numbers
    we are going to have to know what their scale is. So basically itâ€™s much easier
    to create a single neural network architecture that works for lots of different
    kinds of inputs if we know that they are consistently going to be mean zero standard
    deviation one. That would be the short answer. But weâ€™ll learn a lot more about
    it and if in a couple of lessons you are still not quite sure why, letâ€™s come
    back to it because itâ€™s a really interesting thing to talk about.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šæ‰€ä»¥æˆ‘æœ‰ç‚¹éš¾ä»¥æƒ³è±¡å¦‚æœåœ¨è¿™ç§æƒ…å†µä¸‹ä¸è¿›è¡Œå½’ä¸€åŒ–ä¼šå‘ç”Ÿä»€ä¹ˆã€‚æˆ‘ä»¬ä¼šè®¨è®ºåˆ°é‚£é‡Œã€‚è¿™å°±æ˜¯Yannetæ‰€è¯´çš„ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦å½’ä¸€åŒ–çš„åŸå› ï¼Œç›®å‰æˆ‘ä»¬æ­£åœ¨å½’ä¸€åŒ–æ˜¯å› ä¸ºæˆ‘è¯´æˆ‘ä»¬å¿…é¡»è¿™æ ·åšã€‚å½“æˆ‘ä»¬å¼€å§‹ç ”ç©¶éšæœºæ¢¯åº¦ä¸‹é™æ—¶ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šä¼šå‘ç°ï¼Œå¦‚æœä½ ...åŸºæœ¬ä¸Šä¸ºäº†ç¨å¾®æå‰ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†ä¼šé€šè¿‡ä¸€å †æƒé‡è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚æˆ‘ä»¬å°†ä»¥è¿™æ ·ä¸€ç§æ–¹å¼é€‰æ‹©è¿™äº›æƒé‡ï¼Œä»¥ä¾¿å½“æˆ‘ä»¬è¿›è¡ŒçŸ©é˜µä¹˜æ³•æ—¶ï¼Œæˆ‘ä»¬å°†å°è¯•ä¿æŒæ•°å­—ä¸å®ƒä»¬æœ€åˆçš„è§„æ¨¡ç›¸åŒã€‚è¿™åŸºæœ¬ä¸Šéœ€è¦æˆ‘ä»¬çŸ¥é“åˆå§‹æ•°å­—çš„è§„æ¨¡ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“å®ƒä»¬ä¸€ç›´æ˜¯å‡å€¼ä¸ºé›¶ï¼Œæ ‡å‡†å·®ä¸ºä¸€ï¼Œé‚£ä¹ˆå°±æ›´å®¹æ˜“ä¸ºè®¸å¤šä¸åŒç±»å‹çš„è¾“å…¥åˆ›å»ºä¸€ä¸ªå•ä¸€çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¿™å°†æ˜¯ç®€çŸ­çš„ç­”æ¡ˆã€‚ä½†æˆ‘ä»¬å°†å­¦åˆ°æ›´å¤šå…³äºå®ƒçš„çŸ¥è¯†ï¼Œå¦‚æœåœ¨å‡ èŠ‚è¯¾åä½ ä»ç„¶ä¸å¤ªæ˜ç™½ä¸ºä»€ä¹ˆï¼Œè®©æˆ‘ä»¬å›è¿‡å¤´æ¥è®¨è®ºï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„è¯é¢˜ã€‚
- en: '**Question**: Iâ€™m trying to visualize the axes weâ€™re working with here. So
    under plots, when you write `x _valid.shape`, we get 10,000 by 784\. Does that
    mean that we brought in 10,000 pictures of that dimension [[30:27](https://youtu.be/DzE0eSdy5Hk?t=1827)]?
    Yes, exactly. **Question continued**: In the next line, when you choose to reshape
    it, is there a reason why you put 28, 28 as Y or Z coordinates? Or is there a
    reason why theyâ€™re in that order?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šæˆ‘è¯•å›¾å¯è§†åŒ–æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„åæ ‡è½´ã€‚æ‰€ä»¥åœ¨ç»˜å›¾ä¸­ï¼Œå½“ä½ å†™`x_valid.shape`æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°äº†10,000ä¹˜ä»¥784ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¸¦å…¥äº†é‚£ä¸ªç»´åº¦çš„10,000å¼ å›¾ç‰‡å—ï¼Ÿæ˜¯çš„ï¼Œç¡®åˆ‡åœ°è¯´ã€‚é—®é¢˜ç»§ç»­ï¼šåœ¨ä¸‹ä¸€è¡Œï¼Œå½“ä½ é€‰æ‹©é‡å¡‘æ—¶ï¼Œä¸ºä»€ä¹ˆå°†28ã€28ä½œä¸ºYæˆ–Zåæ ‡ï¼Ÿæˆ–è€…å®ƒä»¬æŒ‰ç…§é‚£ä¸ªé¡ºåºæœ‰ä»€ä¹ˆåŸå› å—ï¼Ÿ
- en: '![](../Images/eea87430f7a5c1a002310787f239edea.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eea87430f7a5c1a002310787f239edea.png)'
- en: Yes, there is. Pretty much all neural network libraries assume that the first
    axis is kind of equivalent of a row. Itâ€™s like a separate thing, itâ€™s a sentence
    or an image or example of sales or whatever. So I want each image to be as separate
    item of the first axis. Then so that leaves two more axes for the rows and columns
    of the images. And thatâ€™s totally standard. I donâ€™t think Iâ€™ve ever seen a library
    that doesnâ€™t work that way.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæœ‰çš„ã€‚å‡ ä¹æ‰€æœ‰çš„ç¥ç»ç½‘ç»œåº“éƒ½å‡è®¾ç¬¬ä¸€ä¸ªè½´ç›¸å½“äºä¸€è¡Œã€‚å°±åƒä¸€ä¸ªå•ç‹¬çš„ä¸œè¥¿ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€å¹…å›¾åƒæˆ–é”€å”®ç¤ºä¾‹ã€‚æ‰€ä»¥æˆ‘å¸Œæœ›æ¯ä¸ªå›¾åƒéƒ½æ˜¯ç¬¬ä¸€ä¸ªè½´çš„å•ç‹¬é¡¹ç›®ã€‚ç„¶åï¼Œè¿™æ ·å°±ä¸ºå›¾åƒçš„è¡Œå’Œåˆ—ç•™ä¸‹äº†å¦å¤–ä¸¤ä¸ªè½´ã€‚è¿™æ˜¯å®Œå…¨æ ‡å‡†çš„ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»æ¥æ²¡æœ‰è§è¿‡ä¸€ä¸ªä¸æ˜¯è¿™æ ·å·¥ä½œçš„åº“ã€‚
- en: '**Question**: While normalizing the validation data, I saw you have used mean
    of x and standard deviation of x data (i.e. training data). Shouldnâ€™t we use mean
    and standard deviation of validation data [[31:37](https://youtu.be/DzE0eSdy5Hk?t=1897)]?
    No, because you see, then you would be normalizing the validation set using different
    numbers and so now the meaning of this pixel has a value of 3 in the validation
    set has a different meaning to the meaning of 3 in the training set. It would
    be like if we had days of the week encoded such that Monday was a 1 in the training
    set and was a 0 in the validation set. Weâ€™ve got now two different sets where
    the same number has a different meaning.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šåœ¨å½’ä¸€åŒ–éªŒè¯æ•°æ®æ—¶ï¼Œæˆ‘çœ‹åˆ°ä½ ä½¿ç”¨äº†xæ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆå³è®­ç»ƒæ•°æ®ï¼‰ã€‚æˆ‘ä»¬ä¸åº”è¯¥ä½¿ç”¨éªŒè¯æ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®å—ï¼Ÿä¸ï¼Œå› ä¸ºä½ çœ‹ï¼Œé‚£æ ·çš„è¯ï¼Œä½ å°†ä½¿ç”¨ä¸åŒçš„æ•°å­—å¯¹éªŒè¯é›†è¿›è¡Œå½’ä¸€åŒ–ï¼Œå› æ­¤ç°åœ¨è¿™ä¸ªåƒç´ çš„å€¼åœ¨éªŒè¯é›†ä¸­çš„å«ä¹‰ä¸åœ¨è®­ç»ƒé›†ä¸­çš„å«ä¹‰ä¸åŒã€‚è¿™å°±å¥½åƒå¦‚æœæˆ‘ä»¬å°†ä¸€å‘¨çš„å¤©æ•°ç¼–ç ï¼Œä½¿å¾—æ˜ŸæœŸä¸€åœ¨è®­ç»ƒé›†ä¸­æ˜¯1ï¼Œåœ¨éªŒè¯é›†ä¸­æ˜¯0ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸¤ç»„ä¸åŒçš„æ•°æ®ï¼Œå…¶ä¸­ç›¸åŒçš„æ•°å­—å…·æœ‰ä¸åŒçš„å«ä¹‰ã€‚
- en: Let me give an example. Letâ€™s say we were doing full color image and our training
    set contained like green frogs, green snakes and gray elephants. Weâ€™re training
    to figure out which was which. Now we normalized using the each channel mean.
    Then we have a validation set and a test set which are just green frogs and green
    snakes. If we would have normalized by the validation sets statistics, we would
    end up saying things on average are green. So we would remove all the greenness
    out and so we would now fail to recognize the green frogs and the green snakes
    effectively. So we actually want to use the same normalization coefficients that
    we were training on. For those of you doing the deep learning class, we actually
    go further than that. When we use a pre-trained network, we have to use the same
    normalization coefficients that the original authors trained on. So the idea is
    that a number needs to have this consistent meaning across every dataset where
    you use it. This means when you are looking at the test set, you normalize the
    test set based on the training set mean and standard deviation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä¸¾ä¸ªä¾‹å­ã€‚å‡è®¾æˆ‘ä»¬æ­£åœ¨å¤„ç†å…¨å½©è‰²å›¾åƒï¼Œæˆ‘ä»¬çš„è®­ç»ƒé›†åŒ…å«ç»¿è‰²é’è›™ã€ç»¿è‰²è›‡å’Œç°è‰²å¤§è±¡ã€‚æˆ‘ä»¬æ­£åœ¨è®­ç»ƒä»¥å¼„æ¸…æ¥šå®ƒä»¬å„è‡ªæ˜¯ä»€ä¹ˆã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨æ¯ä¸ªé€šé“çš„å‡å€¼è¿›è¡Œäº†å½’ä¸€åŒ–ã€‚ç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ªéªŒè¯é›†å’Œä¸€ä¸ªæµ‹è¯•é›†ï¼Œé‡Œé¢åªæœ‰ç»¿è‰²é’è›™å’Œç»¿è‰²è›‡ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨éªŒè¯é›†çš„ç»Ÿè®¡æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šè¯´å¹³å‡è€Œè¨€éƒ½æ˜¯ç»¿è‰²ã€‚æ‰€ä»¥æˆ‘ä»¬ä¼šå»é™¤æ‰€æœ‰çš„ç»¿è‰²ï¼Œå› æ­¤æˆ‘ä»¬ç°åœ¨å°†æ— æ³•æœ‰æ•ˆåœ°è¯†åˆ«ç»¿è‰²é’è›™å’Œç»¿è‰²è›‡ã€‚æ‰€ä»¥æˆ‘ä»¬å®é™…ä¸Šå¸Œæœ›ä½¿ç”¨æˆ‘ä»¬è®­ç»ƒæ—¶ä½¿ç”¨çš„ç›¸åŒçš„å½’ä¸€åŒ–ç³»æ•°ã€‚å¯¹äºé‚£äº›æ­£åœ¨å­¦ä¹ æ·±åº¦å­¦ä¹ è¯¾ç¨‹çš„äººï¼Œæˆ‘ä»¬å®é™…ä¸Šåšå¾—æ›´å¤šã€‚å½“æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨åŸå§‹ä½œè€…è®­ç»ƒæ—¶ä½¿ç”¨çš„ç›¸åŒçš„å½’ä¸€åŒ–ç³»æ•°ã€‚å› æ­¤ï¼Œè¿™ä¸ªæ•°å­—åœ¨ä½ ä½¿ç”¨å®ƒçš„æ¯ä¸ªæ•°æ®é›†ä¸­éƒ½éœ€è¦æœ‰ä¸€è‡´çš„å«ä¹‰ã€‚è¿™æ„å‘³ç€å½“ä½ æŸ¥çœ‹æµ‹è¯•é›†æ—¶ï¼Œä½ éœ€è¦æ ¹æ®è®­ç»ƒé›†çš„å‡å€¼å’Œæ ‡å‡†å·®å¯¹æµ‹è¯•é›†è¿›è¡Œå½’ä¸€åŒ–ã€‚
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/28111eb58a2b5f03eab5e2878d4ea4e9.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28111eb58a2b5f03eab5e2878d4ea4e9.png)'
- en: So validation y values are just rank one tensor of 10,000 [[34:03](https://youtu.be/DzE0eSdy5Hk?t=2043)].
    Remember this is kind of weird Python thing where a tuple with this one thing
    in it needs a trailing comma. So this is a rank 1 tensor of length 10,000.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥éªŒè¯yå€¼åªæ˜¯ä¸€ä¸ªé•¿åº¦ä¸º10,000çš„ç§©ä¸º1çš„å¼ é‡ã€‚è®°ä½è¿™æ˜¯ä¸€ç§å¥‡æ€ªçš„Pythonäº‹æƒ…ï¼Œä¸€ä¸ªåŒ…å«è¿™ä¸€ä¸ªä¸œè¥¿çš„å…ƒç»„éœ€è¦ä¸€ä¸ªå°¾éšé€—å·ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º10,000çš„ç§©ä¸º1çš„å¼ é‡ã€‚
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So here is an example of something from that. Itâ€™s just a number 3\. So thatâ€™s
    our labels.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªä¾‹å­ã€‚åªæ˜¯ä¸€ä¸ªæ•°å­—3ã€‚è¿™å°±æ˜¯æˆ‘ä»¬çš„æ ‡ç­¾ã€‚
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Slicing [[34:28](https://youtu.be/DzE0eSdy5Hk?t=2068)]
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ‡ç‰‡
- en: So here is another thing you need to be able to do in your sleep. Slicing into
    a tensor. In this case, weâ€™re slicing into the first axis with 0, so that means
    weâ€™re grabbing the first slice. Because this is a single number, this is going
    to reduce the rank of the tensor by one. Itâ€™s going to turn it from a 3 dimensional
    tensor into a 2 dimensional tensor. So you can see here, this is now just a matrix.
    And then we are going to grab 10 through 14 inclusive rows, 10 through 14 inclusive
    columns, and here it is. So this is the kind of thing you need to be super comfortable
    â€” grabbing pieces out, looking at the numbers, and looking at the picture.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å¦ä¸€ä»¶ä½ éœ€è¦èƒ½å¤Ÿåšåˆ°ç†Ÿç»ƒçš„äº‹æƒ…ã€‚åˆ‡ç‰‡æˆä¸€ä¸ªå¼ é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç”¨0åˆ‡ç‰‡åˆ°ç¬¬ä¸€ä¸ªè½´ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ­£åœ¨è·å–ç¬¬ä¸€ä¸ªåˆ‡ç‰‡ã€‚å› ä¸ºè¿™æ˜¯ä¸€ä¸ªå•ä¸€æ•°å­—ï¼Œè¿™å°†å‡å°‘å¼ é‡çš„ç§©ä¸€æ¬¡ã€‚å®ƒå°†æŠŠä¸€ä¸ª3ç»´å¼ é‡å˜æˆä¸€ä¸ª2ç»´å¼ é‡ã€‚æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°ï¼Œè¿™ç°åœ¨åªæ˜¯ä¸€ä¸ªçŸ©é˜µã€‚ç„¶åæˆ‘ä»¬å°†æŠ“å–10åˆ°14è¡Œï¼Œ10åˆ°14åˆ—ï¼Œè¿™å°±æ˜¯å®ƒã€‚æ‰€ä»¥è¿™æ˜¯ä½ éœ€è¦éå¸¸ç†Ÿç»ƒçš„äº‹æƒ…â€”â€”æŠ“å–ç‰‡æ®µï¼ŒæŸ¥çœ‹æ•°å­—ï¼ŒæŸ¥çœ‹å›¾ç‰‡ã€‚
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So here is an example of a little piece of that first image. So you kind of
    want to get used to this idea that if you are working with something like pictures
    or audio, this is something your brain is really good at interpreting. So keep
    showing pictures of what youâ€™re doing whenever you can. But also remember behind
    the scenes they are numbers, so if something is going weird, print out a few of
    the actual numbers. You might find somehow some of them have become infinity or
    they are all zero or whatever. So use this interactive environment to explore
    data as you go.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ç¬¬ä¸€å¼ å›¾åƒçš„ä¸€å°éƒ¨åˆ†çš„ä¾‹å­ã€‚æ‰€ä»¥ä½ åº”è¯¥ä¹ æƒ¯è¿™æ ·çš„æƒ³æ³•ï¼Œå¦‚æœä½ åœ¨å¤„ç†åƒå›¾ç‰‡æˆ–éŸ³é¢‘è¿™æ ·çš„ä¸œè¥¿ï¼Œè¿™æ˜¯ä½ çš„å¤§è„‘çœŸçš„å¾ˆæ“…é•¿è§£é‡Šçš„ä¸œè¥¿ã€‚æ‰€ä»¥å°½å¯èƒ½ç»å¸¸å±•ç¤ºä½ æ­£åœ¨åšçš„äº‹æƒ…çš„å›¾ç‰‡ã€‚ä½†ä¹Ÿè®°ä½åœ¨å¹•åå®ƒä»¬æ˜¯æ•°å­—ï¼Œæ‰€ä»¥å¦‚æœå‡ºç°å¥‡æ€ªçš„æƒ…å†µï¼Œæ‰“å°å‡ºä¸€äº›å®é™…çš„æ•°å­—ã€‚ä½ å¯èƒ½ä¼šå‘ç°å…¶ä¸­ä¸€äº›å˜æˆäº†æ— ç©·å¤§ï¼Œæˆ–è€…å®ƒä»¬å…¨éƒ½æ˜¯é›¶ï¼Œæˆ–è€…å…¶ä»–ä»€ä¹ˆã€‚æ‰€ä»¥åœ¨æ¢ç´¢æ•°æ®æ—¶åˆ©ç”¨è¿™ä¸ªäº¤äº’å¼ç¯å¢ƒã€‚
- en: '**Question**: Just a quick semantic question. Why when itâ€™s a tensor of rank
    3, is it stored as like XYZ instead of like to me, it would make more sense to
    store it as a list of 2D tensors [[35:56](https://youtu.be/DzE0eSdy5Hk?t=2156)]?
    Itâ€™s not stored as either. So letâ€™s look at this as a 3D. So here is a 3D. So
    a 3D tensor is formatted as showing a list of 2D tensors basically.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šåªæ˜¯ä¸€ä¸ªå¿«é€Ÿçš„è¯­ä¹‰é—®é¢˜ã€‚ä¸ºä»€ä¹ˆå½“å®ƒæ˜¯ä¸€ä¸ªç§©ä¸º3çš„å¼ é‡æ—¶ï¼Œå®ƒè¢«å­˜å‚¨ä¸ºXYZè€Œä¸æ˜¯å¯¹æˆ‘æ¥è¯´ï¼Œå°†å®ƒå­˜å‚¨ä¸º2Då¼ é‡çš„åˆ—è¡¨ä¼šæ›´æœ‰æ„ä¹‰ï¼Ÿå®ƒä¸æ˜¯å­˜å‚¨ä¸ºä»»ä½•ä¸€ç§ã€‚æ‰€ä»¥è®©æˆ‘ä»¬æŠŠè¿™çœ‹ä½œæ˜¯ä¸€ä¸ª3Dã€‚è¿™é‡Œæ˜¯ä¸€ä¸ª3Dã€‚æ‰€ä»¥ä¸€ä¸ª3Då¼ é‡è¢«æ ¼å¼åŒ–ä¸ºåŸºæœ¬ä¸Šæ˜¾ç¤ºä¸€ç³»åˆ—2Då¼ é‡ã€‚
- en: '![](../Images/2f820b0c17fd39d9aeb1c5da76833725.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f820b0c17fd39d9aeb1c5da76833725.png)'
- en: '**Question**: But why isnâ€™t it like `x_imgs[0][10:15][10:15]` ? Oh, because
    that has a different meaning. Itâ€™s kind of the difference between tensors and
    jagged arrays. So basically if you do something like `a[2][3]` , that says take
    the second list item and from it, grab the third list item. So we tend to use
    that when we have something called jagged array which is where each sub-array
    may be of a different length. Where else, we have a single object of three dimensions.
    So we are trying to say which little piece of it do we want. So the idea is that
    is a single slice object to go in and grab that piece out.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šä½†ä¸ºä»€ä¹ˆä¸åƒ`x_imgs[0][10:15][10:15]`é‚£æ ·ï¼Ÿå“¦ï¼Œå› ä¸ºé‚£æœ‰ä¸åŒçš„å«ä¹‰ã€‚è¿™å°±æ˜¯å¼ é‡å’Œä¸è§„åˆ™æ•°ç»„ä¹‹é—´çš„åŒºåˆ«ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šå¦‚æœä½ åšç±»ä¼¼`a[2][3]`è¿™æ ·çš„äº‹æƒ…ï¼Œé‚£å°±æ˜¯è¯´å–ç¬¬äºŒä¸ªåˆ—è¡¨é¡¹ï¼Œç„¶åä»ä¸­è·å–ç¬¬ä¸‰ä¸ªåˆ—è¡¨é¡¹ã€‚æ‰€ä»¥å½“æˆ‘ä»¬æœ‰ä¸€ä¸ªå«åšä¸è§„åˆ™æ•°ç»„çš„ä¸œè¥¿æ—¶ï¼Œæˆ‘ä»¬å€¾å‘äºä½¿ç”¨è¿™ç§æ–¹å¼ï¼Œå…¶ä¸­æ¯ä¸ªå­æ•°ç»„çš„é•¿åº¦å¯èƒ½ä¸åŒã€‚è€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä¸‰ç»´çš„å•ä¸€å¯¹è±¡ã€‚æ‰€ä»¥æˆ‘ä»¬è¯•å›¾è¯´æˆ‘ä»¬æƒ³è¦å®ƒçš„å“ªä¸€å°éƒ¨åˆ†ã€‚æ‰€ä»¥è¿™ä¸ªæƒ³æ³•æ˜¯ä¸€ä¸ªå•ä¸€çš„åˆ‡ç‰‡å¯¹è±¡å»æŠ“å–é‚£ä¸€éƒ¨åˆ†å‡ºæ¥ã€‚
- en: '![](../Images/187aa61a9c4393575a69ce4f3e94be1d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/187aa61a9c4393575a69ce4f3e94be1d.png)'
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/01a922ceb1cf6362c9218cb68b51c486.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01a922ceb1cf6362c9218cb68b51c486.png)'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/63eb52d74552a92f3556b6f7cf6aca00.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63eb52d74552a92f3556b6f7cf6aca00.png)'
- en: So here is an example of a few of those images along with their labels [[37:33](https://youtu.be/DzE0eSdy5Hk?t=2230)].
    This kind of stuff, you want to be able to do pretty quickly with matplotlib.
    Itâ€™s going to help you a lot in life so you can have a look at what Rachel wrote
    here when she wrote `plots`. We can use add_subplot to basically create those
    little separate plots. And you need to know that `imshow` is how we basically
    take a numpy array and draw it as a picture. Then weâ€™ve also added the title on
    top. So there it is.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›å›¾åƒåŠå…¶æ ‡ç­¾çš„ä¾‹å­ã€‚è¿™ç§ä¸œè¥¿ï¼Œä½ å¸Œæœ›èƒ½å¤Ÿç”¨matplotlibå¾ˆå¿«åœ°å®Œæˆã€‚è¿™å°†å¸®åŠ©ä½ å¾ˆå¤šï¼Œè¿™æ ·ä½ å°±å¯ä»¥çœ‹çœ‹Rachelåœ¨å†™`plots`æ—¶å†™çš„ä¸œè¥¿ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨add_subplotæ¥åˆ›å»ºè¿™äº›å°çš„ç‹¬ç«‹å›¾ã€‚ä½ éœ€è¦çŸ¥é“`imshow`æ˜¯æˆ‘ä»¬å¦‚ä½•å°†ä¸€ä¸ªnumpyæ•°ç»„ç»˜åˆ¶æˆå›¾ç‰‡çš„ã€‚ç„¶åæˆ‘ä»¬è¿˜æ·»åŠ äº†é¡¶éƒ¨çš„æ ‡é¢˜ã€‚æ‰€ä»¥å°±æ˜¯è¿™æ ·ã€‚
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Neural Networks [[38:19](https://youtu.be/DzE0eSdy5Hk?t=2299)]
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œ
- en: Letâ€™s take that data and try to build a neural network with it. Sorry, this
    is going to be a lot of review for those of you already doing deep learning. A
    neural network is just a particular mathematical function or a class of mathematical
    functions but itâ€™s a really important class because it has the property, it supports
    whatâ€™s called the universal approximation theorem. It means that a neural network
    can approximate any other function arbitrarily closely. So in other words, it
    can do, in theory, anything as long as we make it big enough. So this is very
    different to a function like 3*x* + 5 which can only do one thing â€” itâ€™s a specific
    function. Or the class of functions *ax* + *b* which can only represent lines
    of different slopes moving it up and down different amounts. Or even the function
    *axÂ² + bx + c + sin d* again only can represent a very specific subset of relationships.
    The neural network, however, is a function that can represent any other function
    to arbitrarily close accuracy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‹¿è¿™äº›æ•°æ®æ¥å°è¯•æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œã€‚å¯¹äºé‚£äº›å·²ç»åœ¨è¿›è¡Œæ·±åº¦å­¦ä¹ çš„äººæ¥è¯´ï¼Œè¿™å°†æ˜¯å¾ˆå¤šå¤ä¹ ã€‚ç¥ç»ç½‘ç»œå®é™…ä¸Šåªæ˜¯ä¸€ä¸ªç‰¹å®šçš„æ•°å­¦å‡½æ•°æˆ–æ•°å­¦å‡½æ•°ç±»ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ç±»ï¼Œå› ä¸ºå®ƒå…·æœ‰æ”¯æŒæ‰€è°“çš„é€šç”¨é€¼è¿‘å®šç†çš„å±æ€§ã€‚è¿™æ„å‘³ç€ç¥ç»ç½‘ç»œå¯ä»¥ä»»æ„æ¥è¿‘åœ°é€¼è¿‘ä»»ä½•å…¶ä»–å‡½æ•°ã€‚æ¢å¥è¯è¯´ï¼Œç†è®ºä¸Šï¼Œåªè¦æˆ‘ä»¬ä½¿å®ƒè¶³å¤Ÿå¤§ï¼Œå®ƒå°±å¯ä»¥åšä»»ä½•äº‹æƒ…ã€‚è¿™ä¸åªèƒ½æ‰§è¡Œä¸€ç§ç‰¹å®šåŠŸèƒ½çš„å‡½æ•°å¦‚3*x*
    + 5éå¸¸ä¸åŒã€‚æˆ–è€…åªèƒ½è¡¨ç¤ºä¸åŒæ–œç‡çš„çº¿æ¡ä¸Šä¸‹ç§»åŠ¨ä¸åŒé‡çš„å‡½æ•°ç±»*ax* + *b*ã€‚ç”šè‡³å‡½æ•°*axÂ² + bx + c + sin d*ä¹Ÿåªèƒ½è¡¨ç¤ºä¸€ä¸ªéå¸¸å…·ä½“çš„å…³ç³»å­é›†ã€‚ç„¶è€Œï¼Œç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªå¯ä»¥ä»»æ„æ¥è¿‘åœ°è¡¨ç¤ºä»»ä½•å…¶ä»–å‡½æ•°çš„å‡½æ•°ã€‚
- en: So what we are going to do is we are going to learn how to take a function,
    letâ€™s take *ax + b*, and we are going to learn how to find its parameters (in
    this case *a* and *b*) which allows it to fit as closely as possible to a set
    of data. So this here is showing example from a notebook that we will be looking
    at in deep learning course which basically show what happens when we use something
    called stochastic gradient descent to try and set *a* and *b*. Basically what
    happens is we are going to pick a random *a* to start with, a random *b* to start
    with, then we are going to basically figure out do I need to increase or decrease
    *a* to make the line close to the dots? Do I need to increase or decrease *b*
    to make the line close to the dots? And then just keep increasing and decreasing
    *a* and *b* lots and lots of times. So thatâ€™s what we are going to do and to answer
    the question do I need to increase or decrease *a* and *b*, we are going to take
    the derivative. So the derivative of the function with respect *a* and *b* tells
    us how will that function change as we change *a* and *b*. So thatâ€™s basically
    what weâ€™re going to do. But we are not going to start with just a line, the idea
    is we are to build up to actually having a neural net and so itâ€™s going to be
    exactly the same idea but because itâ€™s an infinitely flexible function, we are
    going to be able to use this exact same technique to fit to arbitrarily complex
    relationships. Thatâ€™s basically the idea.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯å­¦ä¹ å¦‚ä½•å–ä¸€ä¸ªå‡½æ•°ï¼Œæ¯”å¦‚*ax + b*ï¼Œå¹¶å­¦ä¹ å¦‚ä½•æ‰¾åˆ°å…¶å‚æ•°ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ä¸º*a*å’Œ*b*ï¼‰ï¼Œä½¿å…¶å°½å¯èƒ½æ¥è¿‘ä¸€ç»„æ•°æ®ã€‚è¿™é‡Œå±•ç¤ºäº†æˆ‘ä»¬å°†åœ¨æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­æŸ¥çœ‹çš„ç¬”è®°æœ¬ä¸­çš„ç¤ºä¾‹ï¼ŒåŸºæœ¬ä¸Šå±•ç¤ºäº†å½“æˆ‘ä»¬ä½¿ç”¨ç§°ä¸ºéšæœºæ¢¯åº¦ä¸‹é™æ¥å°è¯•è®¾ç½®*a*å’Œ*b*æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å°†ä»ä¸€ä¸ªéšæœºçš„*a*å’Œä¸€ä¸ªéšæœºçš„*b*å¼€å§‹ï¼Œç„¶åæˆ‘ä»¬åŸºæœ¬ä¸Šè¦å¼„æ¸…æ¥šæˆ‘æ˜¯å¦éœ€è¦å¢åŠ æˆ–å‡å°‘*a*æ¥ä½¿çº¿æ¡æ¥è¿‘ç‚¹ï¼Ÿæˆ‘æ˜¯å¦éœ€è¦å¢åŠ æˆ–å‡å°‘*b*æ¥ä½¿çº¿æ¡æ¥è¿‘ç‚¹ï¼Ÿç„¶ååªéœ€å¤šæ¬¡å¢åŠ å’Œå‡å°‘*a*å’Œ*b*ã€‚è¿™å°±æ˜¯æˆ‘ä»¬è¦åšçš„ï¼Œä¸ºäº†å›ç­”æ˜¯å¦éœ€è¦å¢åŠ æˆ–å‡å°‘*a*å’Œ*b*çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†å–å¯¼æ•°ã€‚å› æ­¤ï¼Œå‡½æ•°å…³äº*a*å’Œ*b*çš„å¯¼æ•°å‘Šè¯‰æˆ‘ä»¬å½“æˆ‘ä»¬æ”¹å˜*a*å’Œ*b*æ—¶è¯¥å‡½æ•°å°†å¦‚ä½•å˜åŒ–ã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯æˆ‘ä»¬è¦åšçš„ã€‚ä½†æˆ‘ä»¬ä¸ä¼šä»…ä»…ä»ä¸€æ¡çº¿å¼€å§‹ï¼Œæƒ³æ³•æ˜¯æˆ‘ä»¬è¦é€æ­¥å»ºç«‹ä¸€ä¸ªå®é™…ä¸Šå…·æœ‰ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼Œå› æ­¤è¿™å°†æ˜¯å®Œå…¨ç›¸åŒçš„æƒ³æ³•ï¼Œä½†ç”±äºå®ƒæ˜¯ä¸€ä¸ªæ— é™çµæ´»çš„å‡½æ•°ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿä½¿ç”¨è¿™ä¸ªå®Œå…¨ç›¸åŒçš„æŠ€æœ¯æ¥é€‚åº”ä»»æ„å¤æ‚çš„å…³ç³»ã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯è¿™ä¸ªæƒ³æ³•ã€‚
- en: '![](../Images/6f22748597af5433773d5ecfed422476.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f22748597af5433773d5ecfed422476.png)'
- en: Then what you need to know is that neural net is actually a very simple thing
    [[41:12](https://youtu.be/DzE0eSdy5Hk?t=2472)]. A neural net actually is something
    which takes as input, letâ€™s say a vector, does a matrix product by that vector.
    So if the vector is size *r*, and the matrix is *r* by *c*, the matrix product
    will spit out something of size *c*. Then we do something called non-linearity
    which is basically we are going to throw away all the negative values (i.e. `max(0,
    x)`). And we are going to put that through another matrix multiply and then put
    that through another `max(0, x)`, and put that through another matrix multiply
    and so on until eventually we end up the single vector that we want. In other
    words, each stage of our neural network, the key thing going on is a matrix multiply,
    in other words, a linear function. So basically deep learning, most of their calculation
    is lots and lots of linear functions, but between each one weâ€™re going to replace
    the negative numbers with zeros.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆä½ éœ€è¦çŸ¥é“çš„æ˜¯ï¼Œç¥ç»ç½‘ç»œå®é™…ä¸Šæ˜¯ä¸€ä»¶éå¸¸ç®€å•çš„äº‹æƒ…ã€‚ç¥ç»ç½‘ç»œå®é™…ä¸Šæ˜¯ä¸€ä¸ªä»¥è¾“å…¥ä¸ºå‘é‡çš„ä¸œè¥¿ï¼Œé€šè¿‡è¯¥å‘é‡è¿›è¡ŒçŸ©é˜µä¹˜ç§¯ã€‚å› æ­¤ï¼Œå¦‚æœå‘é‡çš„å¤§å°ä¸º*r*ï¼ŒçŸ©é˜µä¸º*r*ä¹˜ä»¥*c*ï¼Œåˆ™çŸ©é˜µä¹˜ç§¯å°†è¾“å‡ºå¤§å°ä¸º*c*çš„ç»“æœã€‚ç„¶åæˆ‘ä»¬è¿›è¡Œä¸€ç§ç§°ä¸ºéçº¿æ€§çš„æ“ä½œï¼ŒåŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬è¦ä¸¢å¼ƒæ‰€æœ‰è´Ÿå€¼ï¼ˆå³`max(0,
    x)`ï¼‰ã€‚ç„¶åæˆ‘ä»¬å°†é€šè¿‡å¦ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œå†é€šè¿‡å¦ä¸€ä¸ª`max(0, x)`ï¼Œå†é€šè¿‡å¦ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œç›´åˆ°æœ€ç»ˆå¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„å•ä¸ªå‘é‡ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ç¥ç»ç½‘ç»œçš„æ¯ä¸ªé˜¶æ®µï¼Œå…³é”®çš„äº‹æƒ…æ˜¯è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œæ¢å¥è¯è¯´ï¼Œæ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šï¼Œæ·±åº¦å­¦ä¹ ä¸­å¤§éƒ¨åˆ†çš„è®¡ç®—æ˜¯å¤§é‡çš„çº¿æ€§å‡½æ•°ï¼Œä½†åœ¨æ¯ä¸ªçº¿æ€§å‡½æ•°ä¹‹é—´ï¼Œæˆ‘ä»¬å°†ç”¨é›¶æ›¿æ¢è´Ÿæ•°ã€‚
- en: '![](../Images/1dd457d493aafbdb7d4695f3b068049f.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dd457d493aafbdb7d4695f3b068049f.png)'
- en: '**Question**: Why are we throwing away the negative numbers [[42:53](https://youtu.be/DzE0eSdy5Hk?t=2573)]?
    We will see. The short answer is if you apply a linear function to a linear function
    to a linear function, itâ€™s still just a linear function. So itâ€™s totally useless.
    But if you throw away the negatives, thatâ€™s actually a nonlinear transformation.
    So it turns out that if you apply a linear function to the thing we threw away
    the negatives, then apply that to a linear function that creates a neural network
    and it turns out thatâ€™s the thing that can approximate any other function arbitrarily
    closely. So this tiny little difference actually makes all the difference. And
    if you are interested in it, check out the deep learning video where we cover
    this because I actually show a nice visually intuitive proof, not something that
    I created, but something Michael Nielsen created. Or if you want to skip straight
    to his website, you could go to [Michael Nielsen universal approximation theorem](http://neuralnetworksanddeeplearning.com/chap4.html),
    heâ€™s got a really nice walkthrough with lots of animations where you can see why
    this works.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ä¸¢å¼ƒè´Ÿæ•°ï¼Ÿæˆ‘ä»¬å°†çœ‹åˆ°ã€‚ç®€çŸ­çš„ç­”æ¡ˆæ˜¯ï¼Œå¦‚æœä½ å¯¹ä¸€ä¸ªçº¿æ€§å‡½æ•°åº”ç”¨å¦ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œå®ƒä»ç„¶åªæ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚æ‰€ä»¥è¿™å®Œå…¨æ²¡æœ‰ç”¨ã€‚ä½†æ˜¯å¦‚æœä½ ä¸¢å¼ƒè´Ÿæ•°ï¼Œé‚£å®é™…ä¸Šæ˜¯ä¸€ä¸ªéçº¿æ€§è½¬æ¢ã€‚ç»“æœè¡¨æ˜ï¼Œå¦‚æœä½ å¯¹æˆ‘ä»¬ä¸¢å¼ƒçš„è´Ÿæ•°åº”ç”¨ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œç„¶åå°†å…¶åº”ç”¨äºåˆ›å»ºç¥ç»ç½‘ç»œçš„çº¿æ€§å‡½æ•°ï¼Œç»“æœå°±æ˜¯è¿™ä¸ªä¸œè¥¿å¯ä»¥ä»»æ„æ¥è¿‘ä»»ä½•å…¶ä»–å‡½æ•°ã€‚æ‰€ä»¥è¿™ä¸ªå¾®å°çš„å·®å¼‚å®é™…ä¸Šäº§ç”Ÿäº†å¾ˆå¤§çš„ä¸åŒã€‚å¦‚æœä½ å¯¹æ­¤æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬æ¶µç›–è¿™ä¸€å†…å®¹çš„æ·±åº¦å­¦ä¹ è§†é¢‘ï¼Œå› ä¸ºæˆ‘å®é™…ä¸Šå±•ç¤ºäº†ä¸€ä¸ªç›´è§‚çš„è¯æ˜ï¼Œä¸æ˜¯æˆ‘åˆ›é€ çš„ï¼Œè€Œæ˜¯Michael
    Nielsenåˆ›é€ çš„ã€‚æˆ–è€…ï¼Œå¦‚æœä½ æƒ³ç›´æ¥è·³è½¬åˆ°ä»–çš„ç½‘ç«™ï¼Œä½ å¯ä»¥è®¿é—®Michael Nielsençš„é€šç”¨é€¼è¿‘å®šç†ï¼Œä»–æœ‰ä¸€ä¸ªéå¸¸å¥½çš„æ­¥éª¤æŒ‡å—ï¼Œå…¶ä¸­åŒ…å«è®¸å¤šåŠ¨ç”»ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸ºä»€ä¹ˆè¿™æ ·è¿ä½œã€‚
- en: Why you (yes, you) should blog [[44:17](https://youtu.be/DzE0eSdy5Hk?t=2657)]
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä½ ï¼ˆæ˜¯çš„ï¼Œä½ ï¼‰åº”è¯¥å†™åšå®¢
- en: I feel like the hardest thing with getting started with technical writing on
    the internet is just like posting your first thing. In [this blog](/@racheltho/why-you-yes-you-should-blog-7d2544ac1045),
    Rachel actually says the top advice she would give to her younger self would be
    to start blogging sooner. And she has both reasons why you should do it, some
    examples of places sheâ€™s blogged has turned out to be great for her and her career,
    and some tips about how to get started.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§‰å¾—åœ¨äº’è”ç½‘ä¸Šå¼€å§‹æŠ€æœ¯å†™ä½œæœ€å›°éš¾çš„äº‹æƒ…å°±æ˜¯å‘å¸ƒä½ çš„ç¬¬ä¸€ç¯‡æ–‡ç« ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼ŒRachelå®é™…ä¸Šè¯´å¥¹ç»™å¹´è½»è‡ªå·±çš„æœ€å¥½å»ºè®®æ˜¯å°½æ—©å¼€å§‹å†™åšå®¢ã€‚å¥¹åˆ—ä¸¾äº†ä¸ºä»€ä¹ˆä½ åº”è¯¥è¿™æ ·åšçš„åŸå› ï¼Œå¥¹å†™åšå®¢çš„ä¸€äº›åœ°æ–¹å¯¹å¥¹å’Œå¥¹çš„èŒä¸šéƒ½å¾ˆæœ‰å¸®åŠ©ï¼Œä»¥åŠä¸€äº›å¦‚ä½•å¼€å§‹çš„å»ºè®®ã€‚
- en: I remember when I first suggested to Rachel she might think about blogging because
    she had so much interesting to say and at first she was kind of surprised at the
    idea that she could blog. Now people come up to us at conferences and theyâ€™re
    like â€œyouâ€™re Rachel Thomas! I love your writing!!â€ So Iâ€™ve seen that transition
    from â€œwow could I blog?â€ to being known as a strong technical author. So check
    out this article if you still need convincing or if you are wondering how to get
    started. Since the first one is the first one is the hardest, maybe your first
    one should be something really easy for you to write. So it could be like here
    is a summary of the first 15 minutes of lesson 3 of our machine learning course
    â€” here is whatâ€™s interesting, here is what we learned. Or it could be like here
    is a summary of how I used a random forest to solve a particular problem in my
    practicum.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®°å¾—å½“æˆ‘ç¬¬ä¸€æ¬¡å»ºè®®Rachelè€ƒè™‘å†™åšå®¢æ—¶ï¼Œå› ä¸ºå¥¹æœ‰å¾ˆå¤šæœ‰è¶£çš„äº‹æƒ…è¦è¯´ï¼Œèµ·åˆå¥¹å¯¹è‡ªå·±èƒ½å†™åšå®¢è¿™ä¸ªæƒ³æ³•æ„Ÿåˆ°æœ‰äº›æƒŠè®¶ã€‚ç°åœ¨äººä»¬åœ¨ä¼šè®®ä¸Šèµ°è¿‡æ¥å¯¹æˆ‘ä»¬è¯´ï¼šâ€œä½ æ˜¯Rachel
    Thomasï¼æˆ‘å–œæ¬¢ä½ çš„æ–‡ç« ï¼â€æ‰€ä»¥æˆ‘çœ‹åˆ°äº†ä»â€œå“‡ï¼Œæˆ‘èƒ½å†™åšå®¢å—ï¼Ÿâ€åˆ°è¢«è®¤ä¸ºæ˜¯ä¸€ä½ä¼˜ç§€çš„æŠ€æœ¯ä½œè€…çš„è¿‡æ¸¡ã€‚æ‰€ä»¥å¦‚æœä½ ä»ç„¶éœ€è¦è¯´æœï¼Œæˆ–è€…æƒ³çŸ¥é“å¦‚ä½•å¼€å§‹ï¼Œè¯·æŸ¥çœ‹è¿™ç¯‡æ–‡ç« ã€‚å› ä¸ºç¬¬ä¸€ç¯‡æ˜¯æœ€éš¾çš„ï¼Œä¹Ÿè®¸ä½ çš„ç¬¬ä¸€ç¯‡åº”è¯¥æ˜¯å¯¹ä½ æ¥è¯´éå¸¸å®¹æ˜“å†™çš„ä¸œè¥¿ã€‚æ‰€ä»¥å¯ä»¥æ˜¯è¿™æ ·çš„ï¼Œè¿™æ˜¯æˆ‘ä»¬æœºå™¨å­¦ä¹ è¯¾ç¨‹ç¬¬3è¯¾çš„å‰15åˆ†é’Ÿçš„æ‘˜è¦
    - è¿™æ˜¯æœ‰è¶£çš„åœ°æ–¹ï¼Œè¿™æ˜¯æˆ‘ä»¬å­¦åˆ°çš„ä¸œè¥¿ã€‚æˆ–è€…å¯ä»¥æ˜¯è¿™æ ·çš„ï¼Œè¿™æ˜¯æˆ‘å¦‚ä½•ä½¿ç”¨éšæœºæ£®æ—è§£å†³å®ä¹ ä¸­çš„ç‰¹å®šé—®é¢˜çš„æ‘˜è¦ã€‚
- en: I often get questions like â€œoh my practicum, my organization, weâ€™ve got sensitive
    commercial dataâ€ â€” thatâ€™s fine. Just find another dataset and do it on that instead
    to show the example, or anonymize all of the values and change the names of the
    variables or whatever. You can talk to your employer or your practicum partner
    to make sure that they are comfortable with whatever it is youâ€™re writing. In
    general though, people love it when their interns blog about what they are working
    on because it makes them look super cool. Itâ€™s like â€œhey Iâ€™m an intern working
    at this company and I wrote this post about this cool analysis I didâ€ and then
    other people would be like wow that looks like great company to work for. So generally
    speaking, you should find people are pretty supportive. Besides thereâ€™s lots and
    lots of datasets out there available so even if you canâ€™t base it on the work
    you are doing, you can find something similar for sure.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç»å¸¸è¢«é—®åˆ°â€œå“¦ï¼Œæˆ‘çš„å®ä¹ ï¼Œæˆ‘çš„ç»„ç»‡ï¼Œæˆ‘ä»¬æœ‰æ•æ„Ÿçš„å•†ä¸šæ•°æ®â€ - æ²¡å…³ç³»ã€‚åªéœ€æ‰¾åˆ°å¦ä¸€ä¸ªæ•°æ®é›†ï¼Œç„¶ååœ¨é‚£ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œæ“ä½œä»¥å±•ç¤ºç¤ºä¾‹ï¼Œæˆ–è€…å¯¹æ‰€æœ‰å€¼è¿›è¡ŒåŒ¿ååŒ–å¹¶æ›´æ”¹å˜é‡çš„åç§°ç­‰ã€‚æ‚¨å¯ä»¥ä¸é›‡ä¸»æˆ–å®ä¹ åˆä½œä¼™ä¼´äº¤è°ˆï¼Œä»¥ç¡®ä¿ä»–ä»¬å¯¹æ‚¨å†™çš„ä»»ä½•å†…å®¹æ„Ÿåˆ°èˆ’é€‚ã€‚æ€»çš„æ¥è¯´ï¼Œäººä»¬å–œæ¬¢ä»–ä»¬çš„å®ä¹ ç”Ÿå†™åšå®¢ï¼Œè®²è¿°ä»–ä»¬æ­£åœ¨åšçš„äº‹æƒ…ï¼Œå› ä¸ºè¿™è®©ä»–ä»¬çœ‹èµ·æ¥å¾ˆé…·ã€‚å°±åƒâ€œå˜¿ï¼Œæˆ‘æ˜¯åœ¨è¿™å®¶å…¬å¸å®ä¹ çš„ï¼Œæˆ‘å†™äº†è¿™ç¯‡å…³äºæˆ‘æ‰€åšçš„é…·åˆ†æçš„æ–‡ç« â€ï¼Œç„¶åå…¶ä»–äººä¼šè¯´å“‡ï¼Œè¿™çœ‹èµ·æ¥æ˜¯ä¸€å®¶å¾ˆæ£’çš„å…¬å¸ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä½ ä¼šå‘ç°äººä»¬éå¸¸æ”¯æŒã€‚æ­¤å¤–ï¼Œæœ‰å¾ˆå¤šæ•°æ®é›†å¯ç”¨ï¼Œæ‰€ä»¥å³ä½¿æ‚¨ä¸èƒ½ä»¥æ‚¨æ­£åœ¨è¿›è¡Œçš„å·¥ä½œä¸ºåŸºç¡€ï¼Œæ‚¨ä¹Ÿè‚¯å®šå¯ä»¥æ‰¾åˆ°ç±»ä¼¼çš„ä¸œè¥¿ã€‚
- en: PyTorch [[47:15](https://youtu.be/DzE0eSdy5Hk?t=2835)]
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch
- en: We are going to start building our neural network. We are going to build it
    using something called a PyTorch. PyTorch is a library that basically looks a
    lot like numpy. But when you create some code with PyTorch, you can run it on
    the GPU rather than the CPU. So the GPU is something which is basically going
    to be probably at least an order of magnitude, possibly hundreds of times, faster
    than the code that you might write for the CPU for particularly stuff involving
    lots of linear algebra. So with deep learning, neural nets, if you donâ€™t have
    a GPU you can do it on the CPU but itâ€™s going to be frustratingly slow. Mac does
    not have a GPU that we can use for this because we need NVIDIA GPU. I would actually
    much prefer that we could use your Macâ€™s because competition is great. But NVIDIA
    was really the first one to create a GPU which did a good job of supporting General
    Purpose Graphics Programming Units (GPGPU) â€” in other words that means using a
    GPU for things other than playing computer games. They created a framework called
    CUDA. Itâ€™s a very good framework and pretty much universally used in deep learning.
    If you donâ€™t have a NVIDIA GPU, you canâ€™t use it and no current Mac has a NVIDIA
    GPU. Most laptops of any kind donâ€™t have a NVIDIA GPU. If you are interested in
    doing deep learning on your laptop, the good news is that you need to buy one
    which is really good for playing computer games on. There is a place called [XOTIC
    PC Gaming Laptops](https://www.xoticpc.com/) where you can go and buy yourself
    a great laptop for doing deep learning. You can tell your parents that you need
    the money to do deep learning. Youâ€™ll generally find a whole bunch of laptops
    with names like predator and viper with pictures of robots and stuff. Anyway,
    having said that, I donâ€™t know that many people that do much deep learning on
    their laptop. Most people will log into a cloud environment. By far the easiest
    I know of to use is called [Crestle](https://www.crestle.com/). With Crestle,
    you can basically sign up and straight away, the first thing you get is you get
    thrown straight into a jupyter notebook. Itâ€™s backed by a GPU, costs 60 cents
    an hour with all of the Fast AI libraries and data already available. So that
    makes life really easy. Itâ€™s less flexible and in some ways less fast than using
    AWS which is the Amazon Web Services option. It costs a little bit more, 90 cents
    an hour rather than 60 cents. But itâ€™s very likely that your employer is already
    using that and itâ€™s good to get to know anyway. Theyâ€™ve got more different choices
    around GPUs and itâ€™s a good choice. If you google for github student pack if you
    are a student, you can get $150 of credits straight away pretty much. So itâ€™s
    a really good way to get started.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¼€å§‹æ„å»ºæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå«åšPyTorchçš„ä¸œè¥¿æ¥æ„å»ºå®ƒã€‚PyTorchæ˜¯ä¸€ä¸ªåŸºæœ¬ä¸Šçœ‹èµ·æ¥å¾ˆåƒnumpyçš„åº“ã€‚ä½†æ˜¯å½“ä½ ç”¨PyTorchåˆ›å»ºä¸€äº›ä»£ç æ—¶ï¼Œä½ å¯ä»¥åœ¨GPUä¸Šè¿è¡Œå®ƒè€Œä¸æ˜¯CPUã€‚æ‰€ä»¥GPUåŸºæœ¬ä¸Šå¯èƒ½ä¼šæ¯”ä½ ä¸ºCPUç¼–å†™çš„ä»£ç å¿«è‡³å°‘ä¸€ä¸ªæ•°é‡çº§ï¼Œå¯èƒ½æ˜¯æ•°ç™¾å€ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠå¤§é‡çº¿æ€§ä»£æ•°çš„ä¸œè¥¿ã€‚æ‰€ä»¥åœ¨æ·±åº¦å­¦ä¹ ã€ç¥ç»ç½‘ç»œä¸­ï¼Œå¦‚æœä½ æ²¡æœ‰GPUï¼Œä½ å¯ä»¥åœ¨CPUä¸Šåšï¼Œä½†ä¼šéå¸¸æ…¢ã€‚Macæ²¡æœ‰æˆ‘ä»¬å¯ä»¥ç”¨äºè¿™ä¸ªçš„GPUï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦NVIDIA
    GPUã€‚æˆ‘å®é™…ä¸Šæ›´å¸Œæœ›æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä½ çš„Macï¼Œå› ä¸ºç«äº‰æ˜¯å¾ˆå¥½çš„ã€‚ä½†NVIDIAç¡®å®æ˜¯ç¬¬ä¸€ä¸ªåˆ›å»ºæ”¯æŒé€šç”¨å›¾å½¢ç¼–ç¨‹å•å…ƒï¼ˆGPGPUï¼‰çš„GPUçš„å…¬å¸ï¼Œæ¢å¥è¯è¯´ï¼Œè¿™æ„å‘³ç€ä½¿ç”¨GPUè¿›è¡Œé™¤äº†ç©ç”µè„‘æ¸¸æˆä»¥å¤–çš„äº‹æƒ…ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªå«åšCUDAçš„æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ¡†æ¶ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­å‡ ä¹è¢«æ™®éä½¿ç”¨ã€‚å¦‚æœä½ æ²¡æœ‰NVIDIA
    GPUï¼Œä½ å°±ä¸èƒ½ä½¿ç”¨å®ƒï¼Œç›®å‰æ²¡æœ‰ä»»ä½•Macæœ‰NVIDIA GPUã€‚ä»»ä½•ç±»å‹çš„å¤§å¤šæ•°ç¬”è®°æœ¬ç”µè„‘éƒ½æ²¡æœ‰NVIDIA GPUã€‚å¦‚æœä½ æœ‰å…´è¶£åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿›è¡Œæ·±åº¦å­¦ä¹ ï¼Œå¥½æ¶ˆæ¯æ˜¯ä½ éœ€è¦è´­ä¹°ä¸€å°éå¸¸é€‚åˆç©ç”µè„‘æ¸¸æˆçš„ç¬”è®°æœ¬ç”µè„‘ã€‚æœ‰ä¸€ä¸ªåœ°æ–¹å«åš[XOTIC
    PC Gaming Laptops](https://www.xoticpc.com/)ï¼Œä½ å¯ä»¥å»é‚£é‡Œè´­ä¹°ä¸€å°é€‚åˆè¿›è¡Œæ·±åº¦å­¦ä¹ çš„ä¼˜ç§€ç¬”è®°æœ¬ç”µè„‘ã€‚ä½ å¯ä»¥å‘Šè¯‰ä½ çš„çˆ¶æ¯ï¼Œä½ éœ€è¦è¿™ç¬”é’±æ¥è¿›è¡Œæ·±åº¦å­¦ä¹ ã€‚ä½ é€šå¸¸ä¼šå‘ç°ä¸€å¤§å †å¸¦æœ‰predatorå’Œviperç­‰åç§°çš„ç¬”è®°æœ¬ç”µè„‘ï¼Œä¸Šé¢æœ‰æœºå™¨äººå’Œå…¶ä»–ä¸œè¥¿çš„å›¾ç‰‡ã€‚æ— è®ºå¦‚ä½•ï¼Œè¯è™½å¦‚æ­¤ï¼Œæˆ‘ä¸è®¤è¯†å¾ˆå¤šäººåœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šåšå¾ˆå¤šæ·±åº¦å­¦ä¹ çš„ã€‚å¤§å¤šæ•°äººä¼šç™»å½•åˆ°äº‘ç¯å¢ƒä¸­ã€‚æˆ‘çŸ¥é“çš„æœ€å®¹æ˜“ä½¿ç”¨çš„æ˜¯[Crestle](https://www.crestle.com/)ã€‚ä½¿ç”¨Crestleï¼Œä½ åŸºæœ¬ä¸Šå¯ä»¥æ³¨å†Œï¼Œç„¶åç«‹å³å¾—åˆ°çš„ç¬¬ä¸€ä»¶äº‹å°±æ˜¯ä½ è¢«ç›´æ¥æŠ•å…¥åˆ°ä¸€ä¸ªjupyterç¬”è®°æœ¬ä¸­ã€‚å®ƒæ”¯æŒGPUï¼Œæ¯å°æ—¶60ç¾åˆ†ï¼Œæ‰€æœ‰Fast
    AIåº“å’Œæ•°æ®éƒ½å·²ç»å¯ç”¨ã€‚è¿™ä½¿å¾—ç”Ÿæ´»å˜å¾—éå¸¸å®¹æ˜“ã€‚å®ƒæ¯”ä½¿ç”¨äºšé©¬é€Šç½‘ç»œæœåŠ¡é€‰é¡¹çš„AWS lessçµæ´»ï¼ŒæŸäº›æ–¹é¢ä¹Ÿä¸é‚£ä¹ˆå¿«ã€‚å®ƒçš„æˆæœ¬ç¨å¾®é«˜ä¸€ç‚¹ï¼Œæ¯å°æ—¶90ç¾åˆ†è€Œä¸æ˜¯60ç¾åˆ†ã€‚ä½†å¾ˆå¯èƒ½ä½ çš„é›‡ä¸»å·²ç»åœ¨ä½¿ç”¨å®ƒï¼Œäº†è§£ä¸€ä¸‹ä¹Ÿæ˜¯å¥½çš„ã€‚ä»–ä»¬åœ¨GPUå‘¨å›´æœ‰æ›´å¤šä¸åŒçš„é€‰æ‹©ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚å¦‚æœä½ æ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢githubå­¦ç”ŸåŒ…ï¼Œä½ å¯ä»¥ç«‹å³è·å¾—150ç¾å…ƒçš„ä¿¡ç”¨é¢åº¦ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå¼€å§‹çš„å¥½æ–¹æ³•ã€‚
- en: '**Question**: I wanted to know your opinion on Intel recently published an
    open source way of boosting regular packages that they claim as equivalent to
    if you use the bottom tier GPU. On your CPU, if you use their boost packages,
    you can get the same performance [[51:13](https://youtu.be/DzE0eSdy5Hk?t=3073)].
    Actually Intel makes some great numerical programming libraries particularly this
    one called MKL, Matrix Kernel Library. They definitely make things faster than
    not using those libraries, but if you look at a graph of performance over time,
    GPUs have consistently throughout the last 10 years including now are about 10
    times more floating-point operations per second than equivalent CPU, and they
    are generally about 1/5 of the price for that performance. Because of that, everybody
    doing anything with deep learning basically does it on NVIDIA GPUs and therefore
    using anything other than NVIDIA GPU is currently very annoying â€” so slower, more
    expensive, more annoying. I really hope there will be more activity around AMD
    GPUs in particular in this area, but AMDâ€™s got literally years of catching up
    to do, so it might take a while.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šæˆ‘æƒ³çŸ¥é“ä½ å¯¹è‹±ç‰¹å°”æœ€è¿‘å‘å¸ƒçš„ä¸€ç§æå‡å¸¸è§„è½¯ä»¶åŒ…çš„å¼€æºæ–¹å¼çš„çœ‹æ³•ï¼Œä»–ä»¬å£°ç§°è¿™ç›¸å½“äºä½¿ç”¨åº•å±‚GPUã€‚åœ¨ä½ çš„CPUä¸Šï¼Œå¦‚æœä½ ä½¿ç”¨ä»–ä»¬çš„æå‡è½¯ä»¶åŒ…ï¼Œä½ å¯ä»¥è·å¾—ç›¸åŒçš„æ€§èƒ½ã€‚å®é™…ä¸Šï¼Œè‹±ç‰¹å°”åˆ¶ä½œäº†ä¸€äº›å¾ˆæ£’çš„æ•°å€¼ç¼–ç¨‹åº“ï¼Œç‰¹åˆ«æ˜¯è¿™ä¸ªå«åšMKLçš„åº“ï¼ŒçŸ©é˜µæ ¸å¿ƒåº“ã€‚å®ƒä»¬ç¡®å®æ¯”ä¸ä½¿ç”¨è¿™äº›åº“æ›´å¿«ï¼Œä½†å¦‚æœä½ çœ‹ä¸€ä¸‹æ€§èƒ½éšæ—¶é—´å˜åŒ–çš„å›¾è¡¨ï¼ŒGPUåœ¨è¿‡å»10å¹´ä¸­ä¸€ç›´ä¿æŒç€å¤§çº¦æ¯ç§’10æ¬¡æµ®ç‚¹è¿ç®—ï¼Œç°åœ¨ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè€Œä¸”é€šå¸¸ä»·æ ¼åªæœ‰ç›¸åŒæ€§èƒ½çš„CPUçš„1/5ã€‚å› æ­¤ï¼Œå‡ ä¹æ‰€æœ‰è¿›è¡Œæ·±åº¦å­¦ä¹ çš„äººåŸºæœ¬ä¸Šéƒ½æ˜¯åœ¨NVIDIA
    GPUä¸Šè¿›è¡Œçš„ï¼Œå› æ­¤ä½¿ç”¨é™¤äº†NVIDIA GPUä¹‹å¤–çš„ä»»ä½•ä¸œè¥¿ç›®å‰éƒ½éå¸¸çƒ¦äººâ€”â€”æ›´æ…¢ã€æ›´æ˜‚è´µã€æ›´çƒ¦äººã€‚æˆ‘çœŸçš„å¸Œæœ›åœ¨è¿™ä¸ªé¢†åŸŸå°¤å…¶æ˜¯åœ¨AMD GPUå‘¨å›´ä¼šæœ‰æ›´å¤šçš„æ´»åŠ¨ï¼Œä½†AMDç¡®å®éœ€è¦å¤šå¹´çš„è¿½èµ¶ï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ã€‚'
- en: '**Comment**: I just wanted to point out that you can also buy a thing such
    as a GPU extender to a laptop that may be a first step solution before new laptop
    or AWS [[52:46](https://youtu.be/DzE0eSdy5Hk?t=3166)]. Yes, I think for like $300
    or so, you can buy something that plugs into your Thunderbolt port if you have
    a Mac and then for another $500 or $600, you can buy a GPU to plug into that.
    Having said that, for about $1000, you can actually create a pretty good GPU based
    desktop and so if you are considering that, Fast AI forums have lots of threads
    where people help each other spec out something at a particular price point.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯„è®º**ï¼šæˆ‘åªæ˜¯æƒ³æŒ‡å‡ºï¼Œä½ ä¹Ÿå¯ä»¥è´­ä¹°ä¸€ä¸ªGPUæ‰©å±•å™¨è¿æ¥åˆ°ç¬”è®°æœ¬ç”µè„‘ï¼Œè¿™å¯èƒ½æ˜¯åœ¨è´­ä¹°æ–°ç¬”è®°æœ¬ç”µè„‘æˆ–AWSä¹‹å‰çš„ç¬¬ä¸€æ­¥è§£å†³æ–¹æ¡ˆ[[52:46](https://youtu.be/DzE0eSdy5Hk?t=3166)]ã€‚æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºå¤§çº¦$300å·¦å³ï¼Œä½ å¯ä»¥è´­ä¹°ä¸€ä¸ªæ’å…¥åˆ°ä½ çš„Thunderboltç«¯å£çš„ä¸œè¥¿ï¼Œå¦‚æœä½ æœ‰ä¸€å°Macï¼Œç„¶åå†èŠ±$500æˆ–$600ï¼Œä½ å¯ä»¥è´­ä¹°ä¸€ä¸ªGPUæ’å…¥å…¶ä¸­ã€‚è¯è™½å¦‚æ­¤ï¼Œå¤§çº¦$1000ï¼Œä½ å°±å¯ä»¥åˆ›å»ºä¸€ä¸ªç›¸å½“ä¸é”™çš„åŸºäºGPUçš„å°å¼æœºï¼Œæ‰€ä»¥å¦‚æœä½ åœ¨è€ƒè™‘è¿™ä¸ªï¼ŒFast
    AIè®ºå›æœ‰å¾ˆå¤šå¸–å­ï¼Œäººä»¬åœ¨ç‰¹å®šä»·æ ¼ç‚¹ä¸Šäº’ç›¸å¸®åŠ©ã€‚'
- en: Anyway, to start with, Iâ€™d say use Crestle and then when you are ready to invest
    a few extra minutes getting going, use AWS. To use AWS, when you get there, go
    to EC2 [[53:52](https://youtu.be/DzE0eSdy5Hk?t=3232)]. Thereâ€™s lots of stuff on
    AWS, and EC2 is the bit where we get to rent computers by the hour.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œæˆ‘å»ºè®®ä¸€å¼€å§‹ä½¿ç”¨Crestleï¼Œç„¶åå½“ä½ å‡†å¤‡å¥½æŠ•å…¥ä¸€äº›é¢å¤–çš„æ—¶é—´æ—¶ï¼Œä½¿ç”¨AWSã€‚è¦ä½¿ç”¨AWSï¼Œå½“ä½ åˆ°è¾¾é‚£é‡Œæ—¶ï¼Œå»EC2ã€‚AWSä¸Šæœ‰å¾ˆå¤šä¸œè¥¿ï¼ŒEC2æ˜¯æˆ‘ä»¬å¯ä»¥æŒ‰å°æ—¶ç§Ÿç”¨è®¡ç®—æœºçš„éƒ¨åˆ†ã€‚
- en: '![](../Images/85c7e933f065d84beb55e789dcc3db87.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85c7e933f065d84beb55e789dcc3db87.png)'
- en: Now, we are gonna need a GPU based instance. Unfortunately when you first sign
    up for AWS, they donâ€™t give you access to them. So go to Limits (up in the top
    left).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåŸºäºGPUçš„å®ä¾‹ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå½“ä½ ç¬¬ä¸€æ¬¡æ³¨å†ŒAWSæ—¶ï¼Œä»–ä»¬ä¸ä¼šç»™ä½ è®¿é—®æƒé™ã€‚æ‰€ä»¥å»åˆ°Limitsï¼ˆå·¦ä¸Šè§’ï¼‰ã€‚
- en: '![](../Images/608a596e781dd0e4034342c986a78974.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/608a596e781dd0e4034342c986a78974.png)'
- en: And the main GPU instance weâ€™ll be using is called the p2\. So scroll down to
    p2.xlarge, you need to make sure that number is not zero. If youâ€™ve just got a
    new account, it probably is zero which means you wonâ€™t be allowed to create one.
    So you have to go â€œRequest limit increaseâ€ and the trick there is when it asks
    you why you want the limit increase, type â€œfast.aiâ€ because AWS knows to look
    out and they know that fast.ai people are good people so theyâ€™ll do it quite quickly.
    That takes a day or two generally speaking to go through.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨çš„ä¸»è¦GPUå®ä¾‹ç§°ä¸ºp2ã€‚æ‰€ä»¥æ»šåŠ¨åˆ°p2.xlargeï¼Œä½ éœ€è¦ç¡®ä¿æ•°å­—ä¸æ˜¯é›¶ã€‚å¦‚æœä½ åˆšåˆšæ³¨å†Œäº†ä¸€ä¸ªæ–°è´¦æˆ·ï¼Œå®ƒå¯èƒ½æ˜¯é›¶ï¼Œè¿™æ„å‘³ç€ä½ å°†æ— æ³•åˆ›å»ºä¸€ä¸ªã€‚æ‰€ä»¥ä½ å¿…é¡»å»â€œè¯·æ±‚é™åˆ¶å¢åŠ â€ï¼Œå…¶ä¸­çš„è¯€çªæ˜¯å½“å®ƒé—®ä½ ä¸ºä»€ä¹ˆè¦å¢åŠ é™åˆ¶æ—¶ï¼Œè¾“å…¥â€œfast.aiâ€ï¼Œå› ä¸ºAWSçŸ¥é“è¦ç•™æ„ï¼Œä»–ä»¬çŸ¥é“fast.aiçš„äººæ˜¯å¥½äººï¼Œæ‰€ä»¥ä»–ä»¬ä¼šå¾ˆå¿«å¤„ç†ã€‚é€šå¸¸éœ€è¦ä¸€ä¸¤å¤©çš„æ—¶é—´ã€‚
- en: '![](../Images/2fb948c88cc1415fd43666d3da22c491.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fb948c88cc1415fd43666d3da22c491.png)'
- en: 'So once you get the email saying youâ€™ve been approved for p2 instances, you
    can then go back here and say Launch Instance:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¸€æ—¦ä½ æ”¶åˆ°æ‰¹å‡†ä½¿ç”¨p2å®ä¾‹çš„é‚®ä»¶ï¼Œä½ å°±å¯ä»¥å›åˆ°è¿™é‡Œå¹¶ç‚¹å‡»Launch Instanceï¼š
- en: '![](../Images/7a2a1cecc7a4dea502a3cf9c65707620.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a2a1cecc7a4dea502a3cf9c65707620.png)'
- en: Weâ€™ve basically set up one that has everything you need. So if you click on
    Community AMIs and AMI is an Amazon Machine Image â€” itâ€™s basically a completely
    set up computer. So if you type fastai (all one word), youâ€™ll find here fastai
    DL part 1 v2 for p2\. So thatâ€™s all set up ready to go.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åŸºæœ¬ä¸Šè®¾ç½®äº†ä¸€ä¸ªæ‹¥æœ‰ä¸€åˆ‡ä½ éœ€è¦çš„ä¸œè¥¿ã€‚æ‰€ä»¥å¦‚æœä½ ç‚¹å‡»Community AMIsï¼ŒAMIæ˜¯Amazon Machine Imageçš„ç¼©å†™â€”â€”å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªå®Œå…¨è®¾ç½®å¥½çš„è®¡ç®—æœºã€‚æ‰€ä»¥å¦‚æœä½ è¾“å…¥fastaiï¼ˆè¿åœ¨ä¸€èµ·ï¼‰ï¼Œä½ ä¼šåœ¨è¿™é‡Œæ‰¾åˆ°fastai
    DL part 1 v2 for p2ã€‚æ‰€ä»¥ä¸€åˆ‡éƒ½å‡†å¤‡å°±ç»ªã€‚
- en: '![](../Images/713991f594162edc2375980526a36dd0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/713991f594162edc2375980526a36dd0.png)'
- en: So if you click on Select [[55:34](https://youtu.be/DzE0eSdy5Hk?t=3334)], itâ€™ll
    say what kind of computer do you want. So we have to say I want a â€œGPU computeâ€
    type and specifically I want p2.xlarge. And you can say â€œReview and Launchâ€.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ ç‚¹å‡»Selectï¼Œå®ƒä¼šé—®ä½ æƒ³è¦ä»€ä¹ˆæ ·çš„è®¡ç®—æœºã€‚æ‰€ä»¥æˆ‘ä»¬å¿…é¡»è¯´æˆ‘æƒ³è¦ä¸€ä¸ªâ€œGPUè®¡ç®—â€ç±»å‹ï¼Œå…·ä½“æ¥è¯´æˆ‘æƒ³è¦p2.xlargeã€‚ç„¶åä½ å¯ä»¥ç‚¹å‡»â€œReview
    and Launchâ€ã€‚
- en: '![](../Images/ff36f38789f7c7b31e4487a3e27bfe7d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff36f38789f7c7b31e4487a3e27bfe7d.png)'
- en: Iâ€™m assuming you already know how to deal with SSH keys and all that kind of
    stuff. If you donâ€™t, check out the introductory tutorials and work shop videos
    that we have online, or google around for SSH keys. Very important skill to know
    anyway. So hopefully you get through all that, you have something running on a
    GPU with the Fast AI repo. If you use Crestle, just `cd fastai2` the repo is already
    there, `git pull`. AWS, `cd fastai`, the repo is already there, `git pull`. If
    itâ€™s your own computer, youâ€™ll just have to `git clone` and then away you go.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‡è®¾ä½ å·²ç»çŸ¥é“å¦‚ä½•å¤„ç†SSHå¯†é’¥å’Œæ‰€æœ‰è¿™äº›ä¸œè¥¿ã€‚å¦‚æœä½ ä¸çŸ¥é“ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘ä»¬åœ¨çº¿çš„å…¥é—¨æ•™ç¨‹å’Œå·¥ä½œåŠè§†é¢‘ï¼Œæˆ–è€…åœ¨ç½‘ä¸Šæœç´¢SSHå¯†é’¥ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æŠ€èƒ½ã€‚æ‰€ä»¥å¸Œæœ›ä½ é€šè¿‡æ‰€æœ‰è¿™äº›ï¼Œä½ å¯ä»¥åœ¨GPUä¸Šè¿è¡ŒFast
    AI repoã€‚å¦‚æœä½ ä½¿ç”¨Crestleï¼Œåªéœ€`cd fastai2`ï¼Œrepoå·²ç»åœ¨é‚£é‡Œï¼Œ`git pull`ã€‚AWSï¼Œ`cd fastai`ï¼Œrepoå·²ç»åœ¨é‚£é‡Œï¼Œ`git
    pull`ã€‚å¦‚æœæ˜¯ä½ è‡ªå·±çš„ç”µè„‘ï¼Œä½ åªéœ€è¦`git clone`ï¼Œç„¶åå°±å¯ä»¥å¼€å§‹äº†ã€‚
- en: PyTorch is pre-installed, so PyTorch basically means we can write code that
    looks a lot like numpy but itâ€™s going to run really quickly on the GPU. Secondly,
    since we need to know like which direction and how much to move our parameters
    to improve our loss, we need to know the derivative of functions. PyTorch has
    this amazing thing where any code you write using PyTorch library, it can automatically
    take the derivative of that for you. So we are not going to look at any calculus
    in this course. And I donâ€™t look at any calculus in any of my courses or at any
    of my work basically ever in terms of actually calculating derivatives myself
    because Iâ€™ve never had to. Itâ€™s done for me by the library. So as long as you
    write the Python code, the derivative is done. So the only calculus you really
    need to know to be an effective practitioner is what is it mean to be a derivative.
    And you also need to know the chain rule which we will come to.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchæ˜¯é¢„å®‰è£…çš„ï¼Œæ‰€ä»¥PyTorchåŸºæœ¬ä¸Šæ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç¼–å†™çœ‹èµ·æ¥å¾ˆåƒnumpyçš„ä»£ç ï¼Œä½†åœ¨GPUä¸Šè¿è¡Œé€Ÿåº¦éå¸¸å¿«ã€‚å…¶æ¬¡ï¼Œç”±äºæˆ‘ä»¬éœ€è¦çŸ¥é“å‚æ•°å¦‚ä½•ç§»åŠ¨ä»¥æ”¹å–„æˆ‘ä»¬çš„æŸå¤±ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“å‡½æ•°çš„å¯¼æ•°ã€‚PyTorchæœ‰è¿™ä¸ªç¥å¥‡çš„åŠŸèƒ½ï¼Œä»»ä½•ä½ ä½¿ç”¨PyTorchåº“ç¼–å†™çš„ä»£ç ï¼Œå®ƒéƒ½å¯ä»¥è‡ªåŠ¨ä¸ºä½ è®¡ç®—å¯¼æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é—¨è¯¾ç¨‹ä¸­ä¸ä¼šæ¶‰åŠä»»ä½•å¾®ç§¯åˆ†ã€‚æˆ‘åœ¨æˆ‘çš„è¯¾ç¨‹ä¸­ä»æ¥æ²¡æœ‰çœ‹è¿‡å¾®ç§¯åˆ†ï¼Œä¹Ÿä»æ¥æ²¡æœ‰åœ¨æˆ‘çš„å·¥ä½œä¸­è®¡ç®—è¿‡å¯¼æ•°ï¼Œå› ä¸ºè¿™äº›éƒ½æ˜¯ç”±åº“æ¥å®Œæˆçš„ã€‚åªè¦ä½ å†™å¥½Pythonä»£ç ï¼Œå¯¼æ•°å°±ä¼šè¢«è®¡ç®—å‡ºæ¥ã€‚æ‰€ä»¥æˆä¸ºä¸€ä¸ªæœ‰æ•ˆçš„ä»ä¸šè€…ï¼Œä½ çœŸæ­£éœ€è¦äº†è§£çš„å¾®ç§¯åˆ†åªæ˜¯å¯¼æ•°æ˜¯ä»€ä¹ˆæ„æ€ã€‚ä½ è¿˜éœ€è¦çŸ¥é“é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬ä¼šè®²åˆ°ã€‚
- en: Neural Net for Logistic Regression in PyTorch [[57:45](https://youtu.be/DzE0eSdy5Hk?t=3465)]
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorchä¸­çš„é€»è¾‘å›å½’ç¥ç»ç½‘ç»œ[[57:45](https://youtu.be/DzE0eSdy5Hk?t=3465)]
- en: Alright, so we are going to start out kind of top-down, create a neural net,
    and weâ€™re going to assume a whole bunch of stuff. And gradually we are going to
    dig into each piece. So to create neural nets, we need to import the PyTorch neural
    net library. PyTorch, funnily enough, is not called PyTorch â€” itâ€™s called torch.
    So `torch.nn` is the PyTorch subsection thatâ€™s responsible for neural nets. Weâ€™ll
    call that nn. And we are going to import a few bits out of Fast AI just to make
    life a bit easier for us.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ä»ä¸Šåˆ°ä¸‹å¼€å§‹ï¼Œåˆ›å»ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå¹¶å‡è®¾å¾ˆå¤šä¸œè¥¿ã€‚ç„¶åé€æ¸æˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶æ¯ä¸ªéƒ¨åˆ†ã€‚æ‰€ä»¥è¦åˆ›å»ºç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥PyTorchç¥ç»ç½‘ç»œåº“ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒPyTorchå¹¶ä¸å«PyTorchâ€”â€”å®ƒå«torchã€‚æ‰€ä»¥`torch.nn`æ˜¯è´Ÿè´£ç¥ç»ç½‘ç»œçš„PyTorchå­éƒ¨åˆ†ã€‚æˆ‘ä»¬å°†ç§°ä¹‹ä¸ºnnã€‚æˆ‘ä»¬å°†ä»Fast
    AIä¸­å¯¼å…¥ä¸€äº›éƒ¨åˆ†ï¼Œä»¥ä½¿æˆ‘ä»¬çš„ç”Ÿæ´»å˜å¾—æ›´å®¹æ˜“ã€‚
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So here is how you create a neural network in PyTorch. The simplest possible
    neural network, you say Sequential. And Sequential means I am now going to give
    you a list of the layers that I want in my neural network. So in this case, my
    list has two things in it. The first thing says I want a linear layer. Now a linear
    layer is something thatâ€™s basically going to do *y = ax + b* but matrix matrix
    multiply, not univariate obviously. So itâ€™s going to do a matrix product basically.
    The input of the matrix product is going to be a vector of length 28 times 28
    because thatâ€™s how many pixels we have and the output needs to be of size 10 (we
    will talk about why in a moment). For now this is how we define a linear layer.
    Then again, weâ€™re going to dig into this in detail but every linear layer just
    about in neural nets has to have a non-linearity after it. Then we are going to
    learn about this particular non-linearity in a moment, itâ€™s called the softmax
    and if youâ€™ve done the DL course, youâ€™ve already seen this. So thatâ€™s how we define
    a neural net. This is a two layer neural net.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å¦‚ä½•åœ¨PyTorchä¸­åˆ›å»ºç¥ç»ç½‘ç»œçš„ã€‚æœ€ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œä½ è¯´Sequentialã€‚Sequentialæ„å‘³ç€æˆ‘ç°åœ¨è¦ç»™ä½ ä¸€ä¸ªæˆ‘æƒ³è¦åœ¨æˆ‘çš„ç¥ç»ç½‘ç»œä¸­çš„å±‚çš„åˆ—è¡¨ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘çš„åˆ—è¡¨ä¸­æœ‰ä¸¤ä¸ªä¸œè¥¿ã€‚ç¬¬ä¸€ä»¶äº‹è¯´æˆ‘æƒ³è¦ä¸€ä¸ªçº¿æ€§å±‚ã€‚ç°åœ¨çº¿æ€§å±‚åŸºæœ¬ä¸Šä¼šæ‰§è¡Œ*y
    = ax + b*ï¼Œä½†æ˜¯çŸ©é˜µçŸ©é˜µç›¸ä¹˜ï¼Œè€Œä¸æ˜¯å•å˜é‡ã€‚æ‰€ä»¥å®ƒåŸºæœ¬ä¸Šä¼šæ‰§è¡Œä¸€ä¸ªçŸ©é˜µä¹˜ç§¯ã€‚çŸ©é˜µä¹˜ç§¯çš„è¾“å…¥å°†æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º28ä¹˜ä»¥28çš„å‘é‡ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬æœ‰å¤šå°‘åƒç´ ï¼Œè¾“å‡ºéœ€è¦æ˜¯å¤§å°ä¸º10ï¼ˆæˆ‘ä»¬ç¨åä¼šè®¨è®ºåŸå› ï¼‰ã€‚ç›®å‰è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•å®šä¹‰ä¸€ä¸ªçº¿æ€§å±‚ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºè¿™ä¸€ç‚¹ï¼Œä½†æ˜¯å‡ ä¹æ¯ä¸ªç¥ç»ç½‘ç»œä¸­çš„çº¿æ€§å±‚ä¹‹åéƒ½å¿…é¡»æœ‰ä¸€ä¸ªéçº¿æ€§ã€‚ç„¶åæˆ‘ä»¬å°†åœ¨ä¸€ä¼šå„¿å­¦ä¹ è¿™ä¸ªç‰¹å®šçš„éçº¿æ€§ï¼Œå®ƒè¢«ç§°ä¸ºsoftmaxï¼Œå¦‚æœä½ å·²ç»å­¦è¿‡æ·±åº¦å­¦ä¹ è¯¾ç¨‹ï¼Œä½ å·²ç»è§è¿‡è¿™ä¸ªã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•å®šä¹‰ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚è¿™æ˜¯ä¸€ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œã€‚
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: There is also kind of an implicit additional first layer which is the input,
    but with PyTorch, you donâ€™t have to explicitly mention the input. But normally
    we think conceptually like the input image is kind of also a layer. Because we
    are doing things pretty manually, with PyTorch we are not taking advantage of
    any of the conveniences in Fast AI for building your stuff, we have to then write
    `.cuda()` which tells PyTorch to copy this neural network across to the GPU. So
    from now on, that network is going to be actually running on the GPU. If we didnâ€™t
    say that, it would run on the CPU. So that gives us back a neural net â€” a very
    simple neural net.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ç§éšå«çš„é¢å¤–ç¬¬ä¸€å±‚ï¼Œå³è¾“å…¥å±‚ï¼Œä½†æ˜¯ä½¿ç”¨PyTorchï¼Œä½ ä¸å¿…æ˜¾å¼æåŠè¾“å…¥ã€‚ä½†é€šå¸¸æˆ‘ä»¬åœ¨æ¦‚å¿µä¸Šè®¤ä¸ºè¾“å…¥å›¾åƒä¹Ÿæ˜¯ä¸€ç§å±‚ã€‚å› ä¸ºæˆ‘ä»¬æ­£åœ¨ç›¸å½“æ‰‹åŠ¨åœ°è¿›è¡Œæ“ä½œï¼Œä½¿ç”¨PyTorchæˆ‘ä»¬æ²¡æœ‰åˆ©ç”¨Fast
    AIä¸­æ„å»ºä½ çš„ä¸œè¥¿çš„ä»»ä½•ä¾¿åˆ©æ€§ï¼Œæˆ‘ä»¬å¿…é¡»ç„¶åå†™`.cuda()`ï¼Œè¿™å‘Šè¯‰PyTorchå°†è¿™ä¸ªç¥ç»ç½‘ç»œå¤åˆ¶åˆ°GPUä¸Šã€‚ä»ç°åœ¨å¼€å§‹ï¼Œè¯¥ç½‘ç»œå®é™…ä¸Šå°†åœ¨GPUä¸Šè¿è¡Œã€‚å¦‚æœæˆ‘ä»¬æ²¡æœ‰è¯´ï¼Œå®ƒå°†åœ¨CPUä¸Šè¿è¡Œã€‚è¿™ç»™æˆ‘ä»¬è¿”å›äº†ä¸€ä¸ªç¥ç»ç½‘ç»œâ€”â€”ä¸€ä¸ªéå¸¸ç®€å•çš„ç¥ç»ç½‘ç»œã€‚
- en: Data [[1:00:22](https://youtu.be/DzE0eSdy5Hk?t=3622)]
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®[[1:00:22](https://youtu.be/DzE0eSdy5Hk?t=3622)]
- en: 'We are then going to try and fit the neural net to some data. So we need some
    data. Fast AI has this concept of a ModelData object which is basically something
    that wraps up training data, validation data, and optionally test data. So to
    create a ModelData object, you can just say:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†å°è¯•å°†ç¥ç»ç½‘ç»œæ‹Ÿåˆåˆ°ä¸€äº›æ•°æ®ä¸Šã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€äº›æ•°æ®ã€‚Fast AIæœ‰ä¸€ä¸ªModelDataå¯¹è±¡çš„æ¦‚å¿µï¼ŒåŸºæœ¬ä¸Šæ˜¯å°†è®­ç»ƒæ•°æ®ã€éªŒè¯æ•°æ®å’Œå¯é€‰çš„æµ‹è¯•æ•°æ®åŒ…è£…åœ¨ä¸€èµ·çš„ä¸œè¥¿ã€‚æ‰€ä»¥è¦åˆ›å»ºä¸€ä¸ªModelDataå¯¹è±¡ï¼Œä½ å¯ä»¥è¿™æ ·è¯´ï¼š
- en: I want to create some image classifier data (`ImageClassifierData`)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³åˆ›å»ºä¸€äº›å›¾åƒåˆ†ç±»å™¨æ•°æ®ï¼ˆ`ImageClassifierData`ï¼‰
- en: Iâ€™m going to grab it from some arrays (`from_arrays`)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä»ä¸€äº›æ•°ç»„ä¸­è·å–å®ƒï¼ˆ`from_arrays`ï¼‰
- en: This is the path that Iâ€™m going to save any temporary files (`path`)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘å°†ä¿å­˜ä»»ä½•ä¸´æ—¶æ–‡ä»¶çš„è·¯å¾„ï¼ˆ`path`ï¼‰
- en: This is my training data arrays (`(x, y)`)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘çš„è®­ç»ƒæ•°æ®æ•°ç»„ï¼ˆ`ï¼ˆxï¼Œyï¼‰`ï¼‰
- en: This is my validation data arrays (`(x_valid, y_valid)`)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘çš„éªŒè¯æ•°æ®æ•°ç»„ï¼ˆ`ï¼ˆx_validï¼Œy_validï¼‰`ï¼‰
- en: So that just returns an object thatâ€™s going to wrap that all up. So we are going
    to able to fit to that data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯è¿”å›ä¸€ä¸ªå°†æ‰€æœ‰è¿™äº›åŒ…è£…èµ·æ¥çš„å¯¹è±¡ã€‚æ‰€ä»¥æˆ‘ä»¬å°†èƒ½å¤Ÿæ‹Ÿåˆåˆ°è¿™äº›æ•°æ®ä¸Šã€‚
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that we have a neural net and some data, we are going to come back to this
    in a moment but we basically say what loss function do we want to use, what optimizer
    do we want to use, and then we say fit [[1:01:07](https://youtu.be/DzE0eSdy5Hk?t=3667)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªç¥ç»ç½‘ç»œå’Œä¸€äº›æ•°æ®ï¼Œæˆ‘ä»¬å°†åœ¨ä¸€ä¼šå„¿å›åˆ°è¿™é‡Œï¼Œä½†åŸºæœ¬ä¸Šæˆ‘ä»¬è¯´æˆ‘ä»¬æƒ³ä½¿ç”¨ä»€ä¹ˆæŸå¤±å‡½æ•°ï¼Œæƒ³ä½¿ç”¨ä»€ä¹ˆä¼˜åŒ–å™¨ï¼Œç„¶åè¯´æ‹Ÿåˆ[[1:01:07](https://youtu.be/DzE0eSdy5Hk?t=3667)]ã€‚
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We say fit this network `net` to this data `md` going over every image once
    (`n_epochs`) using this loss function `loss`, this optimizer `opt`, and print
    out these metrics `metrics`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¯´å°†è¿™ä¸ªç½‘ç»œ`net`æ‹Ÿåˆåˆ°è¿™ä¸ªæ•°æ®`md`ä¸Šï¼Œæ¯æ¬¡éå†æ¯ä¸ªå›¾åƒä¸€æ¬¡ï¼ˆ`n_epochs`ï¼‰ï¼Œä½¿ç”¨è¿™ä¸ªæŸå¤±å‡½æ•°`loss`ï¼Œè¿™ä¸ªä¼˜åŒ–å™¨`opt`ï¼Œå¹¶æ‰“å°å‡ºè¿™äº›æŒ‡æ ‡`metrics`ã€‚
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/683068e628e31f95304d12949a5acd1f.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/683068e628e31f95304d12949a5acd1f.png)'
- en: This says here this is 91.8% accurate. So thatâ€™s like the simplest possible
    neural net. What thatâ€™s doing is itâ€™s creating a matrix multiplication, followed
    by a non-linearity, and itâ€™s trying to find the values for this matrix (`nn.Linear(28*28,
    10)`) which basically that fit the data *as well as possible* that end up predicting
    this is a 1, this is a 9, this is a 3.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œè¯´è¿™æ˜¯91.8%çš„å‡†ç¡®ç‡ã€‚æ‰€ä»¥è¿™å°±æ˜¯æœ€ç®€å•çš„ç¥ç»ç½‘ç»œã€‚å®ƒæ­£åœ¨åˆ›å»ºä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œç„¶åæ˜¯ä¸€ä¸ªéçº¿æ€§ï¼Œå®ƒè¯•å›¾æ‰¾åˆ°è¿™ä¸ªçŸ©é˜µçš„å€¼ï¼ˆ`nn.Linear(28*28,
    10)`ï¼‰ï¼ŒåŸºæœ¬ä¸Šæ˜¯å°½å¯èƒ½å¥½åœ°æ‹Ÿåˆæ•°æ®ï¼Œæœ€ç»ˆé¢„æµ‹è¿™æ˜¯1ï¼Œè¿™æ˜¯9ï¼Œè¿™æ˜¯3ã€‚
- en: Loss Function [[1:02:08](https://youtu.be/DzE0eSdy5Hk?t=3728)]
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°[[1:02:08](https://youtu.be/DzE0eSdy5Hk?t=3728)]
- en: So we need some definition for â€œas well as possibleâ€. So the general term for
    that thing is called the loss function. So the loss function is the function thatâ€™s
    going to be lower if this is better. Just like with random forests, we had this
    concept of information gain, and we got to pick what function you want to use
    to define information gain and we were mainly looking at root mean square error.
    Most machine learning algorithms we call something very similar to that â€œlossâ€.
    So the loss is how do we score how good we are. So in the end, we are going to
    calculate the derivative of the loss with respect to the weight matrix that we
    are multiplying by to figure out how to update it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€äº›å…³äºâ€œå°½å¯èƒ½å¥½â€çš„å®šä¹‰ã€‚é‚£ä¸ªä¸œè¥¿çš„ä¸€èˆ¬æœ¯è¯­å«åšæŸå¤±å‡½æ•°ã€‚æ‰€ä»¥æŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå¦‚æœè¿™ä¸ªå‡½æ•°æ›´ä½ï¼Œé‚£ä¹ˆå°±æ›´å¥½ã€‚å°±åƒéšæœºæ£®æ—ä¸€æ ·ï¼Œæˆ‘ä»¬æœ‰ä¿¡æ¯å¢ç›Šçš„æ¦‚å¿µï¼Œæˆ‘ä»¬å¾—é€‰æ‹©ç”¨ä»€ä¹ˆå‡½æ•°æ¥å®šä¹‰ä¿¡æ¯å¢ç›Šï¼Œæˆ‘ä»¬ä¸»è¦çœ‹çš„æ˜¯å‡æ–¹æ ¹è¯¯å·®ã€‚å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•æˆ‘ä»¬ç§°ä¹‹ä¸ºç±»ä¼¼äºâ€œæŸå¤±â€çš„ä¸œè¥¿ã€‚æ‰€ä»¥æŸå¤±æ˜¯æˆ‘ä»¬å¦‚ä½•è¯„åˆ†æˆ‘ä»¬æœ‰å¤šå¥½çš„ä¸œè¥¿ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†è®¡ç®—æŸå¤±å¯¹æˆ‘ä»¬æ­£åœ¨ä¹˜ä»¥çš„æƒé‡çŸ©é˜µçš„å¯¼æ•°ï¼Œä»¥æ‰¾å‡ºå¦‚ä½•æ›´æ–°å®ƒã€‚
- en: 'We are going to use something called Negative Log Likelihood Loss (`NLLLoss`).
    Negative log likelihood loss is also known as cross entropy â€” they are literally
    the same thing. Thereâ€™s two versions, one called binary cross entropy or binary
    negative log likelihood, and another called categorical cross entropy. They are
    the same thing, one is for when youâ€™ve only got a zero or one dependent, the other
    is if youâ€™ve got like cat, dog, airplane, or horse, or 0, 1, through 9 and so
    forth. So what we got here is the binary version of cross entropy:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§ç§°ä¸ºè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆ`NLLLoss`ï¼‰çš„ä¸œè¥¿ã€‚è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ä¹Ÿè¢«ç§°ä¸ºäº¤å‰ç†µï¼Œå®ƒä»¬å®é™…ä¸Šæ˜¯ä¸€æ ·çš„ã€‚æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œä¸€ä¸ªç§°ä¸ºäºŒå…ƒäº¤å‰ç†µæˆ–äºŒå…ƒè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œå¦ä¸€ä¸ªç§°ä¸ºåˆ†ç±»äº¤å‰ç†µã€‚å®ƒä»¬æ˜¯ä¸€æ ·çš„ï¼Œä¸€ä¸ªæ˜¯å½“ä½ åªæœ‰ä¸€ä¸ªé›¶æˆ–ä¸€ä¸ªä¾èµ–æ—¶ï¼Œå¦ä¸€ä¸ªæ˜¯å¦‚æœä½ æœ‰çŒ«ã€ç‹—ã€é£æœºæˆ–é©¬ï¼Œæˆ–è€…0ã€1ã€åˆ°9ç­‰ç­‰ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬æœ‰äº¤å‰ç†µçš„äºŒå…ƒç‰ˆæœ¬ï¼š
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: So here `-(y * np.log(p) + (1-y)*np.log(1-p))` is the definition. I think maybe
    the easiest way to understand this definition is to look at an example [[1:03:35](https://youtu.be/DzE0eSdy5Hk?t=3815)].
    Letâ€™s say we are trying to predict cat vs. dog. One is cat, zero is dog. So here,
    weâ€™ve got cat, dog, dog, cat (`[1, 0, 0, 1]`). And here are our predictions (`[0.9,
    0.1, 0.2, 0.8]`). We said 90% sure itâ€™s a cat, 90% sure itâ€™s a dog, 80% sure itâ€™s
    a dog, 80% sure itâ€™s a cat. So we can then calculate the binary cross entropy
    by calling our function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é‡Œçš„å®šä¹‰æ˜¯`-(y * np.log(p) + (1-y)*np.log(1-p))`ã€‚æˆ‘è®¤ä¸ºç†è§£è¿™ä¸ªå®šä¹‰çš„æœ€ç®€å•æ–¹æ³•å¯èƒ½æ˜¯çœ‹ä¸€ä¸ªä¾‹å­ã€‚å‡è®¾æˆ‘ä»¬è¯•å›¾é¢„æµ‹çŒ«å’Œç‹—ã€‚1ä»£è¡¨çŒ«ï¼Œ0ä»£è¡¨ç‹—ã€‚æ‰€ä»¥è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰çŒ«ã€ç‹—ã€ç‹—ã€çŒ«ï¼ˆ`[1,
    0, 0, 1]`ï¼‰ã€‚è¿™æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ï¼ˆ`[0.9, 0.1, 0.2, 0.8]`ï¼‰ã€‚æˆ‘ä»¬è¯´90%ç¡®å®šæ˜¯çŒ«ï¼Œ90%ç¡®å®šæ˜¯ç‹—ï¼Œ80%ç¡®å®šæ˜¯ç‹—ï¼Œ80%ç¡®å®šæ˜¯çŒ«ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨æˆ‘ä»¬çš„å‡½æ•°æ¥è®¡ç®—äºŒå…ƒäº¤å‰ç†µã€‚
- en: For the first one, we have *y=1*, *p=0.9* (i.e. `(1 * np.log(0.9)` since the
    second term is skipped). For the second one, the first part is skipped (multiply
    by 0) and the second part will be `(1-0)*np.log(0.9)`. In other words, the first
    piece and the second piece of this are going to give exactly the same number which
    make sense because the first one we said we were 90% confident it was a cat and
    it was, and the second we said we were 90% confident it was a dog and it was.
    So in each case, the loss is coming from the fact that we could have been more
    confident. So if we said we were 100% confident, the loss would have been zero.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç¬¬ä¸€ä¸ªï¼Œæˆ‘ä»¬æœ‰*y=1*ï¼Œ*p=0.9*ï¼ˆå³`(1 * np.log(0.9)`ï¼Œå› ä¸ºç¬¬äºŒé¡¹è¢«è·³è¿‡äº†ï¼‰ã€‚å¯¹äºç¬¬äºŒä¸ªï¼Œç¬¬ä¸€éƒ¨åˆ†è¢«è·³è¿‡ï¼ˆä¹˜ä»¥0ï¼‰ï¼Œç¬¬äºŒéƒ¨åˆ†å°†æ˜¯`(1-0)*np.log(0.9)`ã€‚æ¢å¥è¯è¯´ï¼Œè¿™ä¸ªçš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†å°†ç»™å‡ºå®Œå…¨ç›¸åŒçš„æ•°å­—ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªæˆ‘ä»¬è¯´æˆ‘ä»¬å¯¹æ˜¯çŒ«90%æœ‰ä¿¡å¿ƒï¼Œè€Œå®é™…ä¸Šæ˜¯ï¼Œç¬¬äºŒä¸ªæˆ‘ä»¬è¯´æˆ‘ä»¬å¯¹æ˜¯ç‹—90%æœ‰ä¿¡å¿ƒï¼Œè€Œå®é™…ä¸Šæ˜¯ã€‚æ‰€ä»¥åœ¨æ¯ç§æƒ…å†µä¸‹ï¼ŒæŸå¤±éƒ½æ¥è‡ªäºæˆ‘ä»¬æœ¬å¯ä»¥æ›´æœ‰ä¿¡å¿ƒã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬è¯´æˆ‘ä»¬100%æœ‰ä¿¡å¿ƒï¼ŒæŸå¤±å°†ä¸ºé›¶ã€‚
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So letâ€™s look at that in Excel [[1:05:17](https://youtu.be/DzE0eSdy5Hk?t=3917)].
    From the top row:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬åœ¨Excelä¸­çœ‹ä¸€ä¸‹ã€‚ä»é¡¶éƒ¨è¡Œå¼€å§‹ï¼š
- en: our predictions
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„é¢„æµ‹
- en: actual/target values
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®é™…/ç›®æ ‡å€¼
- en: 1 minus actual/target values
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1å‡å»å®é™…/ç›®æ ‡å€¼
- en: log of our predictions
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„é¢„æµ‹çš„å¯¹æ•°
- en: log of 1 minus our predictions
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„é¢„æµ‹çš„å¯¹æ•°çš„1å‡
- en: sum
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ€»å’Œ
- en: '![](../Images/4257aee51f75d1ee43b881468c925dde.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4257aee51f75d1ee43b881468c925dde.png)'
- en: If you think about it, and I want you to think about this during the week, you
    could replace this (`np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))`) with an if
    statement rather than y, because y is always 1 or 0 then itâ€™s only ever going
    to use either this `np.log(p)` or this `(np.log(1-p)`. So you could replace this
    with an if statement. So Iâ€™d like you, during the week, to try to rewrite this
    with an if statement.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»”ç»†æƒ³ä¸€æƒ³ï¼Œæˆ‘å¸Œæœ›ä½ åœ¨è¿™ä¸€å‘¨å†…è€ƒè™‘ä¸€ä¸‹ï¼Œä½ å¯ä»¥ç”¨ä¸€ä¸ªifè¯­å¥æ¥æ›¿æ¢è¿™ä¸ª(`np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))`)ï¼Œè€Œä¸æ˜¯yï¼Œå› ä¸ºyæ€»æ˜¯1æˆ–0ï¼Œæ‰€ä»¥å®ƒåªä¼šä½¿ç”¨`np.log(p)`æˆ–`(np.log(1-p)`ä¸­çš„ä¸€ä¸ªã€‚æ‰€ä»¥ä½ å¯ä»¥ç”¨ä¸€ä¸ªifè¯­å¥æ¥æ›¿æ¢è¿™ä¸ªã€‚æ‰€ä»¥æˆ‘å¸Œæœ›ä½ åœ¨è¿™ä¸€å‘¨å†…å°è¯•ç”¨ä¸€ä¸ªifè¯­å¥æ¥é‡å†™è¿™ä¸ªã€‚
- en: And then see if you can then scale it out to be a categorical cross entropy
    [[1:06:17](https://youtu.be/DzE0eSdy5Hk?t=3977)]. So categorical cross entropy
    works this way. Letâ€™s say we were trying to predict 3, 6, 7, 2.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åçœ‹çœ‹ä½ æ˜¯å¦èƒ½å°†å…¶æ‰©å±•ä¸ºåˆ†ç±»äº¤å‰ç†µã€‚æ‰€ä»¥åˆ†ç±»äº¤å‰ç†µçš„å·¥ä½œæ–¹å¼æ˜¯è¿™æ ·çš„ã€‚å‡è®¾æˆ‘ä»¬è¯•å›¾é¢„æµ‹3ã€6ã€7ã€2ã€‚
- en: '![](../Images/918e173521c2e7a4679d82ca2e48c9bd.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/918e173521c2e7a4679d82ca2e48c9bd.png)'
- en: 'So if we were trying to predict 3 and we actually predicted 5, or try to predict
    3 and we accidentally predicted 9\. Being 5 instead of 3 is no better than being
    9 instead of 3\. So we are not actually going to say how far away is the actual
    number. We are going to express it differently. Or to put it another way, what
    if we were trying to predict cats, dogs, horses, and airplanes. How far away is
    cat from horse? So we are going to express these a little bit differently. Rather
    than thinking of it as a 3, letâ€™s think of it as a vector with a 1 in the third
    location:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœæˆ‘ä»¬è¯•å›¾é¢„æµ‹3ï¼Œè€Œå®é™…ä¸Šé¢„æµ‹äº†5ï¼Œæˆ–è€…è¯•å›¾é¢„æµ‹3ï¼Œå´æ„å¤–åœ°é¢„æµ‹äº†9ã€‚5è€Œä¸æ˜¯3å¹¶ä¸æ¯”9è€Œä¸æ˜¯3æ›´å¥½ã€‚æ‰€ä»¥æˆ‘ä»¬å®é™…ä¸Šä¸ä¼šè¯´å®é™…æ•°å­—æœ‰å¤šè¿œã€‚æˆ‘ä»¬ä¼šç”¨ä¸åŒçš„æ–¹å¼è¡¨è¾¾å®ƒã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬è¯•å›¾é¢„æµ‹çŒ«ã€ç‹—ã€é©¬å’Œé£æœºã€‚çŒ«å’Œé©¬ä¹‹é—´æœ‰å¤šè¿œï¼Ÿæ‰€ä»¥æˆ‘ä»¬ä¼šç¨å¾®ä¸åŒåœ°è¡¨è¾¾è¿™äº›ã€‚ä¸å…¶æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ª3ï¼Œä¸å¦‚æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ªåœ¨ç¬¬ä¸‰ä¸ªä½ç½®ä¸Šæœ‰ä¸€ä¸ª1çš„å‘é‡ï¼š
- en: '![](../Images/e953942ce47f597d2a6e18eadb744c5c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e953942ce47f597d2a6e18eadb744c5c.png)'
- en: Rather than thinking it as a 6, letâ€™s think of it as a vector of zeros for the
    one in the 6th location. So in other words, one-hot-encoding. So letâ€™s one hot
    encode our dependent variable. So that way now, rather than trying to predict
    a single number, letâ€™s predict ten numbers. Letâ€™s predict whatâ€™s the probability
    that itâ€™s a 0, whatâ€™s the probability itâ€™s a 1, and so forth.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ª6ï¼Œè®©æˆ‘ä»¬æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ªé›¶å‘é‡ï¼Œç¬¬6ä¸ªä½ç½®æ˜¯1ã€‚æ¢å¥è¯è¯´ï¼Œç‹¬çƒ­ç¼–ç ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¯¹æˆ‘ä»¬çš„å› å˜é‡è¿›è¡Œç‹¬çƒ­ç¼–ç ã€‚è¿™æ ·ç°åœ¨ï¼Œæˆ‘ä»¬ä¸å†è¯•å›¾é¢„æµ‹ä¸€ä¸ªæ•°å­—ï¼Œè€Œæ˜¯é¢„æµ‹åä¸ªæ•°å­—ã€‚è®©æˆ‘ä»¬é¢„æµ‹å®ƒæ˜¯0çš„æ¦‚ç‡ï¼Œå®ƒæ˜¯1çš„æ¦‚ç‡ï¼Œä¾æ­¤ç±»æ¨ã€‚
- en: '![](../Images/4dce003a5190e7c35eaf848bbfeccf81.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dce003a5190e7c35eaf848bbfeccf81.png)'
- en: So letâ€™s say we are trying to predict the 2, then here is our categorical cross
    entropy [[1:07:50](https://youtu.be/DzE0eSdy5Hk?t=4070)]. So itâ€™s just saying
    okay did this one predict correctly or not, how far off was it, and so forth for
    each one, and add them all up. So categorical cross entropy is identical to binary
    cross entropy. We just have to add it up across all of the categories.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬æ­£åœ¨é¢„æµ‹2ï¼Œè¿™é‡Œæ˜¯æˆ‘ä»¬çš„åˆ†ç±»äº¤å‰ç†µ[[1:07:50](https://youtu.be/DzE0eSdy5Hk?t=4070)]ã€‚æ‰€ä»¥å®ƒåªæ˜¯åœ¨è¯´ï¼Œè¿™ä¸ªé¢„æµ‹æ˜¯å¦æ­£ç¡®ï¼Œæœ‰å¤šå¤§åå·®ï¼Œä¾æ­¤ç±»æ¨ï¼Œå¯¹æ¯ä¸€ä¸ªè¿›è¡Œè®¡ç®—ï¼Œç„¶åå°†å®ƒä»¬å…¨éƒ¨åŠ èµ·æ¥ã€‚åˆ†ç±»äº¤å‰ç†µä¸äºŒå…ƒäº¤å‰ç†µæ˜¯ç›¸åŒçš„ã€‚æˆ‘ä»¬åªéœ€è¦å°†å®ƒä»¬åŠ èµ·æ¥è·¨è¶Šæ‰€æœ‰çš„ç±»åˆ«ã€‚
- en: '![](../Images/c66951412eb40ab3d6245168c32364ec.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c66951412eb40ab3d6245168c32364ec.png)'
- en: So try and turn the binary cross entropy function in Python into a categorical
    cross entropy in Python. Maybe create both the version with the if statement and
    the version with the sum and the product.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å°è¯•å°†Pythonä¸­çš„äºŒå…ƒäº¤å‰ç†µå‡½æ•°è½¬æ¢ä¸ºPythonä¸­çš„åˆ†ç±»äº¤å‰ç†µã€‚ä¹Ÿè®¸åˆ›å»ºå¸¦æœ‰ifè¯­å¥çš„ç‰ˆæœ¬å’Œå¸¦æœ‰æ±‚å’Œå’Œä¹˜ç§¯çš„ç‰ˆæœ¬ã€‚
- en: So thatâ€™s why in our PyTorch, we had 10 as the output dimensionality for this
    matrix because when we multiply a matrix with 10 columns, we are going to end
    up with something of length 10 which is what we want [[1:08:35](https://youtu.be/DzE0eSdy5Hk?t=4115)].
    We want to have 10 predictions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨æˆ‘ä»¬çš„PyTorchä¸­ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªçŸ©é˜µçš„è¾“å‡ºç»´åº¦è®¾ç½®ä¸º10ï¼Œå› ä¸ºå½“æˆ‘ä»¬å°†ä¸€ä¸ªæœ‰10åˆ—çš„çŸ©é˜µç›¸ä¹˜æ—¶ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªé•¿åº¦ä¸º10çš„ç»“æœï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„[[1:08:35](https://youtu.be/DzE0eSdy5Hk?t=4115)]ã€‚æˆ‘ä»¬æƒ³è¦æœ‰10ä¸ªé¢„æµ‹ã€‚
- en: '![](../Images/248a380a51674fd3951fe989cd53e82a.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/248a380a51674fd3951fe989cd53e82a.png)'
- en: So thatâ€™s the loss function that we are using. Then we can fit the model and
    what it does is it goes through every image, this many times (`epochs`). So in
    this case itâ€™s just looking at every image once, and going to slightly update
    the values in that weight matrix based on those gradients.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥æ‹Ÿåˆæ¨¡å‹ï¼Œå®ƒä¼šéå†æ¯ä¸ªå›¾åƒï¼Œè¿™ä¹ˆå¤šæ¬¡ï¼ˆ`epochs`ï¼‰ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒåªæ˜¯æŸ¥çœ‹æ¯ä¸ªå›¾åƒä¸€æ¬¡ï¼Œå¹¶ä¸”ä¼šæ ¹æ®è¿™äº›æ¢¯åº¦ç¨å¾®æ›´æ–°é‚£ä¸ªæƒé‡çŸ©é˜µä¸­çš„å€¼ã€‚
- en: '![](../Images/abdffccd094662b0136d77886cf8668e.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abdffccd094662b0136d77886cf8668e.png)'
- en: So once weâ€™ve trained it, we can then say `predict` using this model (`net`)on
    the validation set (`md.val_dl`).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¸€æ—¦æˆ‘ä»¬è®­ç»ƒå¥½äº†ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å‹(`net`)åœ¨éªŒè¯é›†(`md.val_dl`)ä¸Šè¿›è¡Œ`predict`ã€‚
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that spits out something of 10,000 by 10\. We have 10,000 images we are
    validating on, and we actually make 10 predictions per image. In other words,
    each one of these row is the probabilities that itâ€™s a 0, itâ€™s a 1, itâ€™s a 2,
    and so forth.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™ä¼šè¾“å‡ºä¸€ä¸ª10,000ä¹˜ä»¥10çš„ä¸œè¥¿ã€‚æˆ‘ä»¬æœ‰10,000å¼ å›¾åƒè¿›è¡ŒéªŒè¯ï¼Œå®é™…ä¸Šæ¯å¼ å›¾åƒè¿›è¡Œ10æ¬¡é¢„æµ‹ã€‚æ¢å¥è¯è¯´ï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯å®ƒæ˜¯0çš„æ¦‚ç‡ï¼Œå®ƒæ˜¯1çš„æ¦‚ç‡ï¼Œå®ƒæ˜¯2çš„æ¦‚ç‡ï¼Œä¾æ­¤ç±»æ¨ã€‚
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Argmax [[1:10:22](https://youtu.be/DzE0eSdy5Hk?t=4222)]
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Argmax [[1:10:22](https://youtu.be/DzE0eSdy5Hk?t=4222)]
- en: 'In math, thereâ€™s a really common operation we do called `argmax`. When I say
    itâ€™s common, itâ€™s funny like at high school, I never saw argmax. First year undergrad,
    I never saw argmax. But somehow after university, everythingâ€™s about argmax. So
    one of those things thatâ€™s for some reason not really taught at school but it
    actually turns out to be super critical. So argmax is both something that youâ€™ll
    see in math (itâ€™s just written out in full â€œargmaxâ€), itâ€™s in numpy, itâ€™s in PyTorch,
    itâ€™s super important. What it does is it says letâ€™s take this array of predictions,
    and letâ€™s figure out on a given axis (`axis=1` â€” remember, axis 1 is columns),
    so as Chis said for 10 predictions for each row, letâ€™s find which prediction has
    the highest value and return not that (if it just said max, it would return the
    value) argmax returns the index of the value. So by saying `argmax(axis=1)`, itâ€™s
    going to return the index which is actually the number itself. So letâ€™s grab the
    first 5:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°å­¦ä¸­ï¼Œæœ‰ä¸€ä¸ªæˆ‘ä»¬ç»å¸¸åšçš„æ“ä½œå«åš`argmax`ã€‚å½“æˆ‘è¯´å®ƒå¾ˆå¸¸è§æ—¶ï¼Œå¾ˆæœ‰è¶£çš„æ˜¯åœ¨é«˜ä¸­ï¼Œæˆ‘ä»æ¥æ²¡æœ‰è§è¿‡argmaxã€‚å¤§ä¸€ï¼Œæˆ‘ä¹Ÿä»æ¥æ²¡æœ‰è§è¿‡argmaxã€‚ä½†ä¸çŸ¥ä½•æ•…ï¼Œå¤§å­¦æ¯•ä¸šåï¼Œä¸€åˆ‡éƒ½ä¸argmaxæœ‰å…³ã€‚æ‰€ä»¥æœ‰äº›äº‹æƒ…åœ¨å­¦æ ¡é‡Œä¼¼ä¹å¹¶æ²¡æœ‰çœŸæ­£æ•™ï¼Œä½†å®é™…ä¸Šå®ƒéå¸¸å…³é”®ã€‚argmaxæ—¢æ˜¯æ•°å­¦ä¸­çš„ä¸€ä¸ªä¸œè¥¿ï¼ˆå®ƒåªæ˜¯å®Œæ•´åœ°å†™å‡ºâ€œargmaxâ€ï¼‰ï¼Œå®ƒåœ¨numpyä¸­ï¼Œåœ¨PyTorchä¸­ï¼Œéå¸¸é‡è¦ã€‚å®ƒçš„ä½œç”¨æ˜¯è®©æˆ‘ä»¬æ‹¿è¿™äº›é¢„æµ‹æ•°ç»„ï¼Œç„¶ååœ¨ç»™å®šçš„è½´ä¸Šï¼ˆ`axis=1`
    - è®°ä½ï¼Œè½´1æ˜¯åˆ—ï¼‰ï¼Œå°±åƒChisæ‰€è¯´çš„ï¼Œå¯¹äºæ¯ä¸€è¡Œçš„10ä¸ªé¢„æµ‹ï¼Œè®©æˆ‘ä»¬æ‰¾å‡ºå“ªä¸ªé¢„æµ‹å€¼æœ€é«˜ï¼Œç„¶åè¿”å›ä¸æ˜¯é‚£ä¸ªå€¼ï¼ˆå¦‚æœåªæ˜¯è¯´maxï¼Œå®ƒä¼šè¿”å›å€¼ï¼‰ï¼Œargmaxè¿”å›å€¼çš„ç´¢å¼•ã€‚æ‰€ä»¥é€šè¿‡è¯´`argmax(axis=1)`ï¼Œå®ƒå°†è¿”å›å®é™…ä¸Šæ˜¯æ•°å­—æœ¬èº«çš„ç´¢å¼•ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å–å‰5ä¸ªï¼š
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: So thatâ€™s how we can convert our probabilities back into predictions. We save
    that away and call it `preds`. We can then say when does `preds` equal the ground
    truth. Thatâ€™s going to return an array of booleans which we can treat as ones
    and zeros and the mean of a bunch of ones and zeros is just the average. So that
    gives us the accuracy of 91.8%.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•å°†æˆ‘ä»¬çš„æ¦‚ç‡è½¬æ¢å›é¢„æµ‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä¿å­˜ä¸‹æ¥å¹¶ç§°ä¹‹ä¸º`preds`ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´`preds`ä½•æ—¶ç­‰äºçœŸå®å€¼ã€‚è¿™å°†è¿”å›ä¸€ä¸ªå¸ƒå°”æ•°ç»„ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸º1å’Œ0ï¼Œä¸€å †1å’Œ0çš„å¹³å‡å€¼å°±æ˜¯å¹³å‡å€¼ã€‚è¿™ç»™äº†æˆ‘ä»¬91.8%çš„å‡†ç¡®ç‡ã€‚
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So you want to be able to replicate the numbers you see and here it is. Here
    is our 91.8%.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ æƒ³è¦èƒ½å¤Ÿå¤åˆ¶ä½ çœ‹åˆ°çš„æ•°å­—ï¼Œè¿™é‡Œå°±æ˜¯ã€‚è¿™é‡Œæ˜¯æˆ‘ä»¬çš„91.8%ã€‚
- en: '![](../Images/fe5ab63da5923878c6b70e7decad39d8.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe5ab63da5923878c6b70e7decad39d8.png)'
- en: So when we train this, the last thing tells us is whatever metric we asked for,
    and we asked for accuracy. Then before that we get the training set loss. The
    loss is again whatever loss we asked for (`nn.NLLLoss()`), and the second thing
    is the validation set loss. PyTorch doesnâ€™t use the word loss, they use the word
    criterion. So youâ€™ll see here `crit` so thatâ€™s criterion equal loss. So this is
    what loss function we want to use, they call that the criterion. Same thing. So
    `np.mean(preds == y_valid)` is how we can recreate that accuracy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å½“æˆ‘ä»¬è®­ç»ƒè¿™ä¸ªæ¨¡å‹æ—¶ï¼Œæœ€åä¸€ä»¶äº‹å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯æˆ‘ä»¬è¦æ±‚çš„ä»»ä½•æŒ‡æ ‡ï¼Œæˆ‘ä»¬è¦æ±‚çš„æ˜¯å‡†ç¡®ç‡ã€‚ç„¶ååœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å¾—åˆ°äº†è®­ç»ƒé›†çš„æŸå¤±ã€‚æŸå¤±åˆæ˜¯æˆ‘ä»¬è¦æ±‚çš„ä»»ä½•æŸå¤±(`nn.NLLLoss()`)ï¼Œç¬¬äºŒä»¶äº‹æ˜¯éªŒè¯é›†çš„æŸå¤±ã€‚PyTorchä¸ä½¿ç”¨æŸå¤±è¿™ä¸ªè¯ï¼Œä»–ä»¬ä½¿ç”¨å‡†åˆ™è¿™ä¸ªè¯ã€‚æ‰€ä»¥ä½ ä¼šåœ¨è¿™é‡Œçœ‹åˆ°`crit`ï¼Œè¿™å°±æ˜¯å‡†åˆ™ç­‰äºæŸå¤±ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æŸå¤±å‡½æ•°ï¼Œä»–ä»¬ç§°ä¹‹ä¸ºå‡†åˆ™ã€‚åŒæ ·çš„äº‹æƒ…ã€‚æ‰€ä»¥`np.mean(preds
    == y_valid)`å°±æ˜¯æˆ‘ä»¬å¦‚ä½•é‡æ–°åˆ›å»ºå‡†ç¡®ç‡çš„æ–¹æ³•ã€‚
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/72058ea150fd0e6d88efb9914ee67e12.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72058ea150fd0e6d88efb9914ee67e12.png)'
- en: So now we can go ahead and plot eight of the images along with their predictions.
    For the ones we got wrong, you can see why they are wrong. The image of 4 is pretty
    close to 9\. Itâ€™s just missing a little cross at the top. The 3 is pretty close
    to 5\. Itâ€™s got a little bit of the extra on top. So weâ€™ve made a start. And all
    weâ€™ve done so far is, we havenâ€™t actually created a deep neural net. Weâ€™ve actually
    got only one layer. So what weâ€™ve actually done is weâ€™ve created a logistic regression.
    Logistic regression is literally what we just built and you could try and replicate
    this with sklearnâ€™s logistic regression package. When I did it, I got similar
    accuracy, but this version ran much faster because this is running on the GPU
    where else sklearn runs on the CPU. So even for something like logistic regression,
    we can implement it very quickly qith PyTorch.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­ç»˜åˆ¶å…«å¹…å›¾åƒä»¥åŠå®ƒä»¬çš„é¢„æµ‹ã€‚å¯¹äºæˆ‘ä»¬é¢„æµ‹é”™è¯¯çš„é‚£äº›ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°å®ƒä»¬ä¸ºä»€ä¹ˆé”™è¯¯ã€‚æ•°å­—4çš„å›¾åƒéå¸¸æ¥è¿‘æ•°å­—9ã€‚å®ƒåªæ˜¯åœ¨é¡¶éƒ¨å°‘äº†ä¸€ä¸ªå°äº¤å‰ã€‚æ•°å­—3éå¸¸æ¥è¿‘æ•°å­—5ã€‚å®ƒåœ¨é¡¶éƒ¨æœ‰ä¸€ç‚¹é¢å¤–çš„éƒ¨åˆ†ã€‚æ‰€ä»¥æˆ‘ä»¬å·²ç»å¼€å§‹äº†ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å®é™…ä¸Šè¿˜æ²¡æœ‰åˆ›å»ºä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å®é™…ä¸Šåªæœ‰ä¸€ä¸ªå±‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ‰€åšçš„æ˜¯åˆ›å»ºäº†ä¸€ä¸ªé€»è¾‘å›å½’ã€‚é€»è¾‘å›å½’å°±æ˜¯æˆ‘ä»¬åˆšåˆšæ„å»ºçš„å†…å®¹ï¼Œæ‚¨å¯ä»¥å°è¯•ä½¿ç”¨sklearnçš„é€»è¾‘å›å½’åŒ…æ¥å¤åˆ¶è¿™ä¸ªè¿‡ç¨‹ã€‚å½“æˆ‘è¿™æ ·åšæ—¶ï¼Œæˆ‘å¾—åˆ°äº†ç±»ä¼¼çš„å‡†ç¡®æ€§ï¼Œä½†è¿™ä¸ªç‰ˆæœ¬è¿è¡Œå¾—æ›´å¿«ï¼Œå› ä¸ºå®ƒåœ¨GPUä¸Šè¿è¡Œï¼Œè€Œsklearnåœ¨CPUä¸Šè¿è¡Œã€‚å› æ­¤ï¼Œå³ä½¿å¯¹äºåƒé€»è¾‘å›å½’è¿™æ ·çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨PyTorchéå¸¸å¿«é€Ÿåœ°å®ç°å®ƒã€‚
- en: '**Question**: When we are creating our net, we ahve to do `.cuda()`. What would
    be the consequence of not doing that? Would it just not run [[1:14:16](https://youtu.be/DzE0eSdy5Hk?t=4456)]?
    It wouldnâ€™t run quickly. Itâ€™ll run on the CPU.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šå½“æˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬å¿…é¡»æ‰§è¡Œ`.cuda()`ã€‚å¦‚æœä¸è¿™æ ·åšä¼šæœ‰ä»€ä¹ˆåæœï¼Ÿå®ƒåªæ˜¯ä¸ä¼šå¿«é€Ÿè¿è¡Œã€‚å®ƒå°†åœ¨CPUä¸Šè¿è¡Œã€‚
- en: '**Question**: Why do we have to do linear and followed by nonlinear [[1:14:34](https://youtu.be/DzE0eSdy5Hk?t=4474)]?
    The short answer is because thatâ€™s what the universal approximation theorem says
    is the structure which can give you arbitrarily accurate functions for any functional
    form. The long answer is the details of why the universal approximation theorem
    works. Another version of the short answer is, thatâ€™s the definition of a neural
    network. So the definition of a neural network is a linear layer followed by an
    activation function followed by a linear layer followed by an activation function,
    etc. We go into a lot more detail of this in the deep learning course but for
    this purpose itâ€™s enough to know that it works. So far, of course, we havenâ€™t
    actually built a deep neural net at all. Weâ€™ve just built a logistic regression.
    So at this point, if you think about it, all weâ€™re doing is we are taking every
    input pixel and multiplying it by a weight for each possible outcome. So we are
    basically saying on average the number 1 has these pixels turned on. The number
    two has these pixels turned on. Thatâ€™s why itâ€™s not terribly accurate. Thatâ€™s
    not how digit recognition works in real life. But thatâ€™s all we build so far.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬å¿…é¡»å…ˆè¿›è¡Œçº¿æ€§æ“ä½œï¼Œç„¶åå†è¿›è¡Œéçº¿æ€§æ“ä½œï¼Ÿç®€çŸ­çš„ç­”æ¡ˆæ˜¯å› ä¸ºè¿™æ˜¯é€šç”¨é€¼è¿‘å®šç†æ‰€è¯´çš„ç»“æ„ï¼Œå¯ä»¥ä¸ºä»»ä½•å‡½æ•°å½¢å¼æä¾›ä»»æ„ç²¾ç¡®çš„å‡½æ•°ã€‚é•¿ç­”æ¡ˆæ˜¯é€šç”¨é€¼è¿‘å®šç†ä¸ºä½•æœ‰æ•ˆçš„ç»†èŠ‚ã€‚å¦ä¸€ä¸ªç®€çŸ­ç­”æ¡ˆæ˜¯ï¼Œè¿™å°±æ˜¯ç¥ç»ç½‘ç»œçš„å®šä¹‰ã€‚å› æ­¤ï¼Œç¥ç»ç½‘ç»œçš„å®šä¹‰æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œåè·Ÿä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œå†åè·Ÿä¸€ä¸ªçº¿æ€§å±‚ï¼Œå†åè·Ÿä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œä¾æ­¤ç±»æ¨ã€‚æˆ‘ä»¬åœ¨æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­ä¼šæ›´è¯¦ç»†åœ°è®¨è®ºè¿™ä¸€ç‚¹ï¼Œä½†å°±æ­¤ç›®çš„è€Œè¨€ï¼ŒçŸ¥é“å®ƒæœ‰æ•ˆå°±è¶³å¤Ÿäº†ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šè¿˜æ²¡æœ‰æ„å»ºä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬åªæ˜¯æ„å»ºäº†ä¸€ä¸ªé€»è¾‘å›å½’ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œå¦‚æœä½ è€ƒè™‘ä¸€ä¸‹ï¼Œæˆ‘ä»¬æ‰€åšçš„å°±æ˜¯å°†æ¯ä¸ªè¾“å…¥åƒç´ ä¹˜ä»¥æ¯ä¸ªå¯èƒ½ç»“æœçš„æƒé‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯åœ¨è¯´ï¼Œå¹³å‡è€Œè¨€ï¼Œæ•°å­—1å…·æœ‰è¿™äº›åƒç´ ç‚¹äº®ã€‚æ•°å­—2å…·æœ‰è¿™äº›åƒç´ ç‚¹äº®ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒä¸æ˜¯éå¸¸å‡†ç¡®çš„åŸå› ã€‚è¿™ä¸æ˜¯ç°å®ç”Ÿæ´»ä¸­æ•°å­—è¯†åˆ«çš„å·¥ä½œæ–¹å¼ã€‚ä½†åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ„å»ºçš„å…¨éƒ¨å†…å®¹ã€‚
- en: '**Question**: So you keep saying this Universal approximation theorem. Did
    you define that [[1:16:07](https://youtu.be/DzE0eSdy5Hk?t=4567)]? Yeah, but letâ€™s
    cover it again because itâ€™s worth talking about. So Michael Nielsen has this great
    website called neural networks and deep learning. And his [chapter 4](http://neuralnetworksanddeeplearning.com/chap4.html)
    is actually famous now and in it, he does this walkthrough of basically showing
    that a neural network can approximate any other function to arbitrarily close
    accuracy as long as itâ€™s big enough. And we walk through this in a lot of detail
    in the deep learning course but the basic trick is that he shows that with a few
    different numbers, you can basically cause these things to create little boxes,
    you can move the boxes up and down, you can move them around, you can join them
    together to eventually basically create like connections of towers which you can
    use to approximate any kind of surface.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šæ‰€ä»¥ä½ ä¸€ç›´åœ¨è¯´è¿™ä¸ªé€šç”¨é€¼è¿‘å®šç†ã€‚ä½ æœ‰å®šä¹‰è¿‡å—ï¼Ÿæ˜¯çš„ï¼Œä½†è®©æˆ‘ä»¬å†æ¬¡è®¨è®ºä¸€ä¸‹ï¼Œå› ä¸ºè¿™å€¼å¾—è°ˆè®ºã€‚å› æ­¤ï¼ŒMichael Nielsenæœ‰ä¸€ä¸ªåä¸ºç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ çš„ä¼˜ç§€ç½‘ç«™ã€‚ä»–çš„ç¬¬4ç« ç°åœ¨å®é™…ä¸Šå¾ˆæœ‰åï¼Œå…¶ä¸­ä»–é€šè¿‡æ¼”ç¤ºç¥ç»ç½‘ç»œå¯ä»¥ä»¥è¶³å¤Ÿå¤§çš„è§„æ¨¡é€¼è¿‘ä»»ä½•å…¶ä»–å‡½æ•°ï¼Œåªè¦å®ƒè¶³å¤Ÿå¤§ï¼Œæ¥è¯¦ç»†ä»‹ç»è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬åœ¨æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­è¯¦ç»†è®¨è®ºäº†è¿™ä¸€ç‚¹ï¼Œä½†åŸºæœ¬çš„è¯€çªæ˜¯ï¼Œä»–å±•ç¤ºäº†é€šè¿‡å‡ ä¸ªä¸åŒçš„æ•°å­—ï¼Œæ‚¨åŸºæœ¬ä¸Šå¯ä»¥ä½¿è¿™äº›äº‹ç‰©åˆ›å»ºå°ç›’å­ï¼Œæ‚¨å¯ä»¥å°†ç›’å­ä¸Šä¸‹ç§»åŠ¨ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬ç§»åŠ¨ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ï¼Œæœ€ç»ˆåŸºæœ¬ä¸Šå¯ä»¥åˆ›å»ºåƒå¡”ä¸€æ ·çš„è¿æ¥ï¼Œæ‚¨å¯ä»¥ç”¨æ¥é€¼è¿‘ä»»ä½•ç±»å‹çš„è¡¨é¢ã€‚
- en: '![](../Images/36cc00e3507966e1b4b8a06349cac3a3.png)![](../Images/0184c296d1637cb289ecd868956402a8.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36cc00e3507966e1b4b8a06349cac3a3.png)![](../Images/0184c296d1637cb289ecd868956402a8.png)'
- en: So thatâ€™s basically the trick. So all we need to do, given that, is to kind
    of find the parameters for each of the linear functions in that neural network.
    So to find the weights in each of the matrices. So far, weâ€™ve got just one matrix
    and weâ€™ve just built a simple logistic regression.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šå°±æ˜¯è¯€çªã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ‰€éœ€è¦åšçš„å°±æ˜¯ï¼Œé‰´äºæ­¤ï¼Œæ‰¾åˆ°ç¥ç»ç½‘ç»œä¸­æ¯ä¸ªçº¿æ€§å‡½æ•°çš„å‚æ•°ã€‚å› æ­¤ï¼Œæ‰¾åˆ°æ¯ä¸ªçŸ©é˜µä¸­çš„æƒé‡ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªçŸ©é˜µï¼Œæˆ‘ä»¬åªæ˜¯æ„å»ºäº†ä¸€ä¸ªç®€å•çš„é€»è¾‘å›å½’ã€‚
- en: '**Question**: I just wanted to confirm that when you showed the examples of
    images which were misclassified, they look rectangular so itâ€™s just that while
    rendering, pixels are being scaled differently [[1:17:50](https://youtu.be/DzE0eSdy5Hk?t=4670)]?
    They are 28 by 28\. I think they just look rectangular because theyâ€™ve got titles
    on the top. Matplotlib does often fiddle around with what it considers black versus
    while and having different size axes and stuff. So you do have to be little bit
    careful there sometimes.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šæˆ‘åªæ˜¯æƒ³ç¡®è®¤ä¸€ä¸‹ï¼Œå½“ä½ å±•ç¤ºè¢«é”™è¯¯åˆ†ç±»çš„å›¾åƒçš„ä¾‹å­æ—¶ï¼Œå®ƒä»¬çœ‹èµ·æ¥æ˜¯çŸ©å½¢çš„ï¼Œæ‰€ä»¥åªæ˜¯åœ¨æ¸²æŸ“æ—¶ï¼Œåƒç´ è¢«ä¸åŒåœ°ç¼©æ”¾äº†å—ï¼Ÿå®ƒä»¬æ˜¯28ä¹˜28çš„ã€‚æˆ‘è®¤ä¸ºå®ƒä»¬çœ‹èµ·æ¥æ˜¯çŸ©å½¢çš„ï¼Œå› ä¸ºå®ƒä»¬é¡¶éƒ¨æœ‰æ ‡é¢˜ã€‚Matplotlibç»å¸¸ä¼šè°ƒæ•´å®ƒè®¤ä¸ºçš„é»‘è‰²ä¸ç™½è‰²ä»¥åŠå…·æœ‰ä¸åŒå¤§å°è½´ç­‰çš„ä¸œè¥¿ã€‚å› æ­¤ï¼Œæœ‰æ—¶ä½ å¿…é¡»å°å¿ƒä¸€ç‚¹ã€‚'
- en: Defining Logistic Regression Ourselves [[1:18:31](https://youtu.be/DzE0eSdy5Hk?t=4711)]
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªå·±å®šä¹‰é€»è¾‘å›å½’
- en: Hopefully this will now make more sense because what weâ€™re going to do is dig
    in a layer deeper and define logistic regression without using `nn.Sequential`,
    `nn.Linear`, or `nn.LogSoftmax`. So we are going to do nearly all of the layer
    definition from scratch. So to do that, weâ€™re going to have to define a PyTorch
    module. PyTorch module is basically either a neural net or a layer in a neural
    net which is actually a powerful concept of itself. Basically anything that can
    behave like a neural net can itself be part of another neural net. So this is
    how we can construct particularly powerful architectures combining lots of other
    pieces.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ç°åœ¨è¿™ä¼šæ›´æœ‰æ„ä¹‰ï¼Œå› ä¸ºæˆ‘ä»¬è¦æ·±å…¥ä¸€å±‚ï¼Œå®šä¹‰é€»è¾‘å›å½’ï¼Œè€Œä¸ä½¿ç”¨`nn.Sequential`ã€`nn.Linear`æˆ–`nn.LogSoftmax`ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å‡ ä¹æ‰€æœ‰çš„å±‚å®šä¹‰éƒ½ä»å¤´å¼€å§‹åšã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸å®šä¹‰ä¸€ä¸ªPyTorchæ¨¡å—ã€‚PyTorchæ¨¡å—åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œæˆ–ç¥ç»ç½‘ç»œä¸­çš„ä¸€å±‚ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¦‚å¿µã€‚åŸºæœ¬ä¸Šï¼Œä»»ä½•å¯ä»¥åƒç¥ç»ç½‘ç»œä¸€æ ·è¡Œä¸ºçš„ä¸œè¥¿æœ¬èº«å¯ä»¥æˆä¸ºå¦ä¸€ä¸ªç¥ç»ç½‘ç»œçš„ä¸€éƒ¨åˆ†ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•æ„å»ºç‰¹åˆ«å¼ºå¤§çš„æ¶æ„ï¼Œç»“åˆäº†è®¸å¤šå…¶ä»–éƒ¨åˆ†ã€‚
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So to create a PyTorch module, just create a Python class but it has to inherit
    from `nn.Module`. So we havenâ€™t done inheritance before, other than that, this
    is all the same concepts weâ€™ve seen in OO already. Basically if you put something
    in parentheses here (after a class name), what it means is that our class gets
    all of the functionality of this class for free. Itâ€™s called sub-classing it.
    So we are going to get all of the capabilities of a neural network module that
    the PyTorch authors have provided and then we are going to add additional functionality
    to it. When you create a sub class, there is one key thing you need to remember
    to do which is when you initialize your class, you have to first of all initialize
    the superclass. So superclass is the `nn.Module`. So `nn.Module` has to be built
    before you can start adding your pieces to it. So this is just like something
    you can copy and paste into every one of your modules. You just say `super().__init__()`
    . It just means construct the superclass first.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¦åˆ›å»ºä¸€ä¸ªPyTorchæ¨¡å—ï¼Œåªéœ€åˆ›å»ºä¸€ä¸ªPythonç±»ï¼Œä½†å®ƒå¿…é¡»ç»§æ‰¿è‡ª`nn.Module`ã€‚å› æ­¤ï¼Œé™¤äº†ç»§æ‰¿ä¹‹å¤–ï¼Œè¿™æ˜¯æˆ‘ä»¬å·²ç»åœ¨é¢å‘å¯¹è±¡ä¸­çœ‹åˆ°çš„æ‰€æœ‰æ¦‚å¿µã€‚åŸºæœ¬ä¸Šï¼Œå¦‚æœä½ åœ¨è¿™é‡Œï¼ˆåœ¨ç±»ååé¢ï¼‰æ”¾å…¥æ‹¬å·ä¸­çš„å†…å®¹ï¼Œæ„å‘³ç€æˆ‘ä»¬çš„ç±»ä¼šå…è´¹è·å¾—è¿™ä¸ªç±»çš„æ‰€æœ‰åŠŸèƒ½ã€‚è¿™è¢«ç§°ä¸ºå­ç±»åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è·å¾—PyTorchä½œè€…æä¾›çš„ç¥ç»ç½‘ç»œæ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½ï¼Œç„¶åæˆ‘ä»¬å°†æ·»åŠ é¢å¤–çš„åŠŸèƒ½ã€‚å½“ä½ åˆ›å»ºä¸€ä¸ªå­ç±»æ—¶ï¼Œæœ‰ä¸€ä»¶é‡è¦çš„äº‹æƒ…ä½ éœ€è¦è®°ä½ï¼Œé‚£å°±æ˜¯å½“ä½ åˆå§‹åŒ–ä½ çš„ç±»æ—¶ï¼Œä½ é¦–å…ˆå¿…é¡»åˆå§‹åŒ–è¶…ç±»ã€‚å› æ­¤ï¼Œè¶…ç±»æ˜¯`nn.Module`ã€‚å› æ­¤ï¼Œåœ¨ä½ å¼€å§‹æ·»åŠ ä½ çš„éƒ¨åˆ†ä¹‹å‰ï¼Œå¿…é¡»å…ˆæ„å»º`nn.Module`ã€‚è¿™å°±åƒä½ å¯ä»¥å¤åˆ¶å¹¶ç²˜è´´åˆ°ä½ çš„æ¯ä¸€ä¸ªæ¨¡å—ä¸­çš„ä¸œè¥¿ã€‚ä½ åªéœ€è¯´`super().__init__()`ã€‚è¿™æ„å‘³ç€é¦–å…ˆæ„é€ è¶…ç±»ã€‚
- en: So having done that, we can now define our weights and our bias [[1:20:29](https://youtu.be/DzE0eSdy5Hk?t=4829)].
    Our weights is the weight matrix. Itâ€™s the actual matrix that weâ€™re going to multiply
    our data by. And as we discussed, itâ€™s going to have 28 times 28 rows and 10 columns.
    Thatâ€™s because if we take an image which is weâ€™ve flattened out into a 28 by 28
    length vector, then we can multiply it by this weight matrix to get back out a
    length 10 vector which we can then use to consider it as a set of predictions.
    So thatâ€™s our weight matrix. Now the problem is that we donâ€™t just want *y = ax*.
    We want *y = ax + b*. So *+ b* in neural nets is called bias. So as well as defining
    weights, we are also going to define bias. Since this thing `get_weights(28*28,
    10)` is going to spit out for every image something of length 10\. That means
    that we need to create a vector of length 10 to be our biases. In other words,
    for everything naught, 1, 2, 3 up to 9, we are going to have a different plus
    *b* that would be adding. So weâ€™ve got our data matrix which is of length 10,000
    by 28 â¨‰ 28\. Then weâ€™ve got our weight matrix which is 28 â¨‰ 28 by 10\. So if we
    multiply those together, we get something of size 10,000 by 10.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„æƒé‡å’Œåå·®ã€‚æˆ‘ä»¬çš„æƒé‡æ˜¯æƒé‡çŸ©é˜µã€‚è¿™æ˜¯æˆ‘ä»¬å°†è¦ç”¨æ¥ä¹˜ä»¥æˆ‘ä»¬çš„æ•°æ®çš„å®é™…çŸ©é˜µã€‚æ­£å¦‚æˆ‘ä»¬è®¨è®ºè¿‡çš„ï¼Œå®ƒå°†æœ‰28ä¹˜28è¡Œå’Œ10åˆ—ã€‚è¿™æ˜¯å› ä¸ºå¦‚æœæˆ‘ä»¬å–ä¸€ä¸ªæˆ‘ä»¬å·²ç»å±•å¹³æˆä¸€ä¸ª28ä¹˜28é•¿åº¦å‘é‡çš„å›¾åƒï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å°†å®ƒä¹˜ä»¥è¿™ä¸ªæƒé‡çŸ©é˜µï¼Œå¾—åˆ°ä¸€ä¸ªé•¿åº¦ä¸º10çš„å‘é‡ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºä¸€ç»„é¢„æµ‹ã€‚è¿™å°±æ˜¯æˆ‘ä»¬çš„æƒé‡çŸ©é˜µã€‚ç°åœ¨é—®é¢˜æ˜¯æˆ‘ä»¬ä¸åªæ˜¯æƒ³è¦*y
    = ax*ã€‚æˆ‘ä»¬æƒ³è¦*y = ax + b*ã€‚å› æ­¤ï¼Œåœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œ*+ b*è¢«ç§°ä¸ºåå·®ã€‚å› æ­¤ï¼Œé™¤äº†å®šä¹‰æƒé‡ï¼Œæˆ‘ä»¬è¿˜å°†å®šä¹‰åå·®ã€‚ç”±äºè¿™ä¸ª`get_weights(28*28,
    10)`å°†ä¸ºæ¯ä¸ªå›¾åƒè¾“å‡ºé•¿åº¦ä¸º10çš„ä¸œè¥¿ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º10çš„å‘é‡ä½œä¸ºæˆ‘ä»¬çš„åå·®ã€‚æ¢å¥è¯è¯´ï¼Œå¯¹äºæ¯ä¸ª0ã€1ã€2ã€3ç›´åˆ°9ï¼Œæˆ‘ä»¬å°†æœ‰ä¸€ä¸ªä¸åŒçš„åŠ *b*ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„æ•°æ®çŸ©é˜µï¼Œå®ƒçš„é•¿åº¦æ˜¯10,000ä¹˜ä»¥28ä¹˜ä»¥28ã€‚ç„¶åæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„æƒé‡çŸ©é˜µï¼Œå®ƒæ˜¯28ä¹˜ä»¥28ä¹˜ä»¥10ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å°†å®ƒä»¬ç›¸ä¹˜ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªå¤§å°ä¸º10,000ä¹˜ä»¥10çš„ä¸œè¥¿ã€‚
- en: '![](../Images/c12901781ee389bcc3d8d3f8a8f59878.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c12901781ee389bcc3d8d3f8a8f59878.png)'
- en: 'Then we want to add on our bias like so:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æƒ³è¦æ·»åŠ æˆ‘ä»¬çš„åå·®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/dfdd967bfab9651c9b258575e5d070e7.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfdd967bfab9651c9b258575e5d070e7.png)'
- en: We are going to learn a lot more about this later, but when we add on a vector
    like this, it basically going to get added to every row. So that bias is going
    to get added to every rows. So we first of all define those. To define them, weâ€™ve
    created a tiny little function called `get_weights` which basically just creates
    some normally distributed random numbers. `torch.randn` returns a tensor filled
    with random numbers from a normal distribution.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»¥åä¼šå­¦åˆ°æ›´å¤šå…³äºè¿™ä¸ªçš„çŸ¥è¯†ï¼Œä½†æ˜¯å½“æˆ‘ä»¬åƒè¿™æ ·æ·»åŠ ä¸€ä¸ªå‘é‡æ—¶ï¼ŒåŸºæœ¬ä¸Šå®ƒä¼šè¢«æ·»åŠ åˆ°æ¯ä¸€è¡Œã€‚å› æ­¤ï¼Œé‚£ä¸ªåå·®å°†è¢«æ·»åŠ åˆ°æ¯ä¸€è¡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰è¿™äº›ã€‚ä¸ºäº†å®šä¹‰å®ƒä»¬ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸º`get_weights`çš„å°å‡½æ•°ï¼Œå®ƒåŸºæœ¬ä¸Šåªæ˜¯åˆ›å»ºä¸€äº›æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ã€‚`torch.randn`è¿”å›ä¸€ä¸ªå¡«å……æœ‰æ­£æ€åˆ†å¸ƒéšæœºæ•°çš„å¼ é‡ã€‚
- en: '![](../Images/415721b17b1cebc5d4e0965e3e69595d.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/415721b17b1cebc5d4e0965e3e69595d.png)'
- en: We have to be a bit careful though. When we do deep learning, like when we add
    more linear layers later. Imagine if we have a matrix which on average tends to
    increase the size of the inputs we give to it. If we then multiply it by lots
    of matrices of that size, itâ€™s going to make the numbers bigger and bigger and
    bigger, like exponentially bigger. Or what if made them a bit smaller? Itâ€™s going
    to make them smaller and smaller and smaller exponentially smaller. Because a
    deep network applies lots of linear layers, if on average they result in things
    a bit bigger than they started with or a bit smaller than they started with, itâ€™s
    going to exponentially multiply that difference. So we need to make sure that
    the weight matrix is of an appropriate size that the inputs to it (more specifically,
    the mean of the inputs) is not going to change.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬å¿…é¡»è¦å°å¿ƒã€‚å½“æˆ‘ä»¬è¿›è¡Œæ·±åº¦å­¦ä¹ æ—¶ï¼Œæ¯”å¦‚ä»¥åæ·»åŠ æ›´å¤šçš„çº¿æ€§å±‚ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªçŸ©é˜µï¼Œå¹³å‡å€¾å‘äºå¢åŠ æˆ‘ä»¬è¾“å…¥çš„å¤§å°ã€‚å¦‚æœæˆ‘ä»¬å°†å…¶ä¹˜ä»¥è®¸å¤šç›¸åŒå¤§å°çš„çŸ©é˜µï¼Œå®ƒä¼šä½¿æ•°å­—å˜å¾—è¶Šæ¥è¶Šå¤§ï¼ŒæŒ‡æ•°çº§å¢é•¿ã€‚æˆ–è€…å¦‚æœæˆ‘ä»¬è®©å®ƒä»¬å˜å°ä¸€ç‚¹å‘¢ï¼Ÿå®ƒä¼šä½¿å®ƒä»¬å˜å¾—è¶Šæ¥è¶Šå°ï¼ŒæŒ‡æ•°çº§å‡å°ã€‚å› ä¸ºæ·±åº¦ç½‘ç»œåº”ç”¨äº†è®¸å¤šçº¿æ€§å±‚ï¼Œå¦‚æœå¹³å‡è€Œè¨€å®ƒä»¬å¯¼è‡´çš„ç»“æœæ¯”èµ·å§‹å€¼ç¨å¾®å¤§ä¸€ç‚¹æˆ–ç¨å¾®å°ä¸€ç‚¹ï¼Œé‚£ä¹ˆå®ƒå°†æŒ‡æ•°çº§åœ°æ”¾å¤§è¿™ç§å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æƒé‡çŸ©é˜µçš„å¤§å°é€‚å½“ï¼Œä½¿å¾—è¾“å…¥åˆ°å®ƒçš„ï¼ˆæ›´å…·ä½“åœ°è¯´ï¼Œè¾“å…¥çš„å‡å€¼ï¼‰ä¸ä¼šæ”¹å˜ã€‚
- en: So it turns out that if you use normally distributed random numbers and divided
    by the number of rows in the weight matrix, this particular random initialization
    keeps your numbers at about the right scale. So this idea that, if youâ€™ve done
    linear algebra, basically if the first eigenvalue is bigger than one or smaller
    than one, itâ€™s going to cause the gradients to get bigger and bigger or smaller
    and smaller. Thatâ€™s called gradient explosion. So weâ€™ll talk more about this in
    the deep learning course, but if you are interested, you can look at [Kaiming
    He initialization](https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)
    and read all about this concept, but for now, itâ€™s probably just enough to know
    that if you use this type of random number generation (i.e. `torch.randn(dims)/dims[0]`),
    youâ€™re going to get random numbers that are nicely behaved. You are going to start
    out with an input which is mean 0 standard deviation 1\. Once you put it through
    this set of random numbers, youâ€™ll still have something thatâ€™s about mean 0 standard
    deviation 1\. Thatâ€™s basically the goal.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®è¯æ˜ï¼Œå¦‚æœä½ ä½¿ç”¨æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°å¹¶é™¤ä»¥æƒé‡çŸ©é˜µä¸­çš„è¡Œæ•°ï¼Œè¿™ç§éšæœºåˆå§‹åŒ–å¯ä»¥ä¿æŒä½ çš„æ•°å­—åœ¨å¤§çº¦æ­£ç¡®çš„èŒƒå›´å†…ã€‚å› æ­¤ï¼Œè¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œå¦‚æœä½ åšè¿‡çº¿æ€§ä»£æ•°ï¼ŒåŸºæœ¬ä¸Šå¦‚æœç¬¬ä¸€ä¸ªç‰¹å¾å€¼å¤§äº1æˆ–å°äº1ï¼Œå®ƒä¼šå¯¼è‡´æ¢¯åº¦å˜å¾—è¶Šæ¥è¶Šå¤§æˆ–è¶Šæ¥è¶Šå°ã€‚è¿™å°±æ˜¯æ¢¯åº¦çˆ†ç‚¸ã€‚æˆ‘ä»¬å°†åœ¨æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­æ›´å¤šåœ°è®¨è®ºè¿™ä¸ªé—®é¢˜ï¼Œä½†å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œä½ å¯ä»¥æŸ¥çœ‹[Kaiming
    Heåˆå§‹åŒ–](https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)ï¼Œå¹¶é˜…è¯»æœ‰å…³è¿™ä¸ªæ¦‚å¿µçš„æ‰€æœ‰å†…å®¹ï¼Œä½†ç°åœ¨ï¼ŒçŸ¥é“å¦‚æœä½ ä½¿ç”¨è¿™ç§ç±»å‹çš„éšæœºæ•°ç”Ÿæˆï¼ˆå³`torch.randn(dims)/dims[0]`ï¼‰ï¼Œä½ å°†å¾—åˆ°è¡Œä¸ºè‰¯å¥½çš„éšæœºæ•°ã€‚ä½ å°†ä»å‡å€¼ä¸º0æ ‡å‡†å·®ä¸º1çš„è¾“å…¥å¼€å§‹ã€‚ä¸€æ—¦ä½ é€šè¿‡è¿™ç»„éšæœºæ•°ï¼Œä½ ä»ç„¶ä¼šå¾—åˆ°å¤§çº¦å‡å€¼ä¸º0æ ‡å‡†å·®ä¸º1çš„ä¸œè¥¿ã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯ç›®æ ‡ã€‚
- en: One nice thing about PyTorch is that you can play with this stuff [[1:25:44](https://youtu.be/DzE0eSdy5Hk?t=5144)].
    So try it out. Every time you see a function being used, run it and take a look.
    So youâ€™ll see, it looks a lot like numpy but it doesnâ€™t return a numpy array.
    It returns a tensor.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchçš„ä¸€ä¸ªå¥½å¤„æ˜¯ä½ å¯ä»¥ç©å¼„è¿™äº›ä¸œè¥¿[[1:25:44](https://youtu.be/DzE0eSdy5Hk?t=5144)]ã€‚æ‰€ä»¥è¯•ä¸€è¯•ã€‚æ¯å½“ä½ çœ‹åˆ°ä¸€ä¸ªå‡½æ•°è¢«ä½¿ç”¨æ—¶ï¼Œè¿è¡Œå®ƒå¹¶æŸ¥çœ‹ä¸€ä¸‹ã€‚æ‰€ä»¥ä½ ä¼šå‘ç°ï¼Œå®ƒçœ‹èµ·æ¥å¾ˆåƒnumpyï¼Œä½†å®ƒä¸è¿”å›ä¸€ä¸ªnumpyæ•°ç»„ã€‚å®ƒè¿”å›ä¸€ä¸ªå¼ é‡ã€‚
- en: '![](../Images/a2bb48e179731174df8512c4dccd6422.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2bb48e179731174df8512c4dccd6422.png)'
- en: And in fact, now Iâ€™m GPU programming.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼Œç°åœ¨æˆ‘åœ¨è¿›è¡ŒGPUç¼–ç¨‹ã€‚
- en: '![](../Images/329d688f00141a1aaa4035ef72dfbf08.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/329d688f00141a1aaa4035ef72dfbf08.png)'
- en: Put `.cuda()` and now itâ€™s doing it on the GPU.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨`.cuda()`ï¼Œç°åœ¨å®ƒåœ¨GPUä¸Šè¿è¡Œã€‚
- en: '![](../Images/7da66ebc0fff94c8da8b6ff8c721ea4a.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7da66ebc0fff94c8da8b6ff8c721ea4a.png)'
- en: I just multiplied that matrix by 3 very quickly on GPU! So thatâ€™s how we do
    GPU programming with PyTorch.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨GPUä¸Šéå¸¸å¿«åœ°å°†é‚£ä¸ªçŸ©é˜µä¹˜ä»¥3ï¼è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨PyTorchè¿›è¡ŒGPUç¼–ç¨‹ã€‚
- en: As we said, we create one 28*28 by 10 weight matrix, and the other is just rank
    1 of 10 for biases [[1:26:29](https://youtu.be/DzE0eSdy5Hk?t=5189)]. We have to
    make them a parameter. This is basically telling PyTorch which things to update
    when it does SGD. Thatâ€™s very minor technical detail.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è¯´ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª28*28ä¹˜ä»¥10çš„æƒé‡çŸ©é˜µï¼Œå¦ä¸€ä¸ªåªæ˜¯10çš„ç§©1åå·®[[1:26:29](https://youtu.be/DzE0eSdy5Hk?t=5189)]ã€‚æˆ‘ä»¬å¿…é¡»å°†å®ƒä»¬è®¾ä¸ºå‚æ•°ã€‚è¿™åŸºæœ¬ä¸Šå‘Šè¯‰PyTorchåœ¨æ‰§è¡ŒSGDæ—¶è¦æ›´æ–°å“ªäº›å†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å¾®å°çš„æŠ€æœ¯ç»†èŠ‚ã€‚
- en: '![](../Images/1d7edd5f587825661a3a62a910167cfd.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d7edd5f587825661a3a62a910167cfd.png)'
- en: So having created the weight matrices, we then define a special method with
    the name `forward`. This is a special method and the name forward has a special
    meaning in PyTorch. A method called forward in PyTorch is the name of the method
    that will get called when your layer is calculated. So if you create a neural
    net or a layer, you have to define forward and itâ€™s going to get passed the data
    from the previous layer. Our definition is to do a matrix multiplication of our
    input data times our weights and add on the biases. Thatâ€™s it. Thatâ€™s what happened
    earlier on when we said `nn.Linear`. It created this thing for us.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºäº†æƒé‡çŸ©é˜µåï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸º`forward`çš„ç‰¹æ®Šæ–¹æ³•ã€‚è¿™æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•ï¼Œè€Œåœ¨PyTorchä¸­ï¼Œåç§°forwardå…·æœ‰ç‰¹æ®Šå«ä¹‰ã€‚åœ¨PyTorchä¸­ï¼Œç§°ä¸ºforwardçš„æ–¹æ³•æ˜¯åœ¨è®¡ç®—å±‚æ—¶å°†è¢«è°ƒç”¨çš„æ–¹æ³•åç§°ã€‚å› æ­¤ï¼Œå¦‚æœä½ åˆ›å»ºäº†ä¸€ä¸ªç¥ç»ç½‘ç»œæˆ–ä¸€ä¸ªå±‚ï¼Œä½ å¿…é¡»å®šä¹‰forwardï¼Œå®ƒå°†ä¼ é€’å‰ä¸€å±‚çš„æ•°æ®ã€‚æˆ‘ä»¬çš„å®šä¹‰æ˜¯å¯¹è¾“å…¥æ•°æ®å’Œæƒé‡è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå¹¶åŠ ä¸Šåå·®ã€‚å°±æ˜¯è¿™æ ·ã€‚è¿™å°±æ˜¯æˆ‘ä»¬ä¹‹å‰è¯´çš„`nn.Linear`æ—¶å‘ç”Ÿçš„äº‹æƒ…ã€‚å®ƒä¸ºæˆ‘ä»¬åˆ›å»ºäº†è¿™ä¸ªä¸œè¥¿ã€‚
- en: Now unfortunately though, we are not getting a 28 by 28 long vector. We are
    getting a 28 row by 28 column matrix, so we have to flatten it. Unfortunately,
    in PyTorch, they tend to rename things. They spell â€œresizeâ€ â€œviewâ€. So `view`
    means reshape. So you can see here `x.view(x.size(0), -1)`, we end up with something
    where the number of images (`x.size(0)`), we are going to leave the same. Then
    we are going to replace row by column with a single axis. Again, `-1` meaning
    as long as required. So this is how we flatten something using PyTorch.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰å¾—åˆ°ä¸€ä¸ª28ä¹˜ä»¥28çš„é•¿å‘é‡ã€‚æˆ‘ä»¬å¾—åˆ°çš„æ˜¯ä¸€ä¸ª28è¡Œä¹˜ä»¥28åˆ—çš„çŸ©é˜µï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å°†å…¶å±•å¹³ã€‚ä¸å¹¸çš„æ˜¯ï¼Œåœ¨PyTorchä¸­ï¼Œå®ƒä»¬å€¾å‘äºé‡æ–°å‘½åäº‹ç‰©ã€‚ä»–ä»¬å°†â€œresizeâ€æ‹¼å†™ä¸ºâ€œviewâ€ã€‚æ‰€ä»¥`view`æ„å‘³ç€é‡å¡‘ã€‚å› æ­¤ï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œ`x.view(x.size(0),
    -1)`ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°çš„æ˜¯ä¸€ä¸ªå›¾åƒæ•°é‡ï¼ˆ`x.size(0)`ï¼‰ä¸å˜ã€‚ç„¶åæˆ‘ä»¬å°†è¡Œæ›¿æ¢ä¸ºåˆ—ï¼Œå½¢æˆä¸€ä¸ªå•ä¸€è½´ã€‚å†æ¬¡ï¼Œ`-1`çš„æ„æ€æ˜¯å°½å¯èƒ½é•¿ã€‚è¿™å°±æ˜¯æˆ‘ä»¬ä½¿ç”¨PyTorchå±•å¹³çš„æ–¹æ³•ã€‚
- en: So we flatten it, do a matrix multiply, and then finally we do our softmax [[1:28:23](https://youtu.be/DzE0eSdy5Hk?t=5303)].
    So softmax is the activation function we use. If you look in the deep learning
    repo, youâ€™ll find something called [entropy example](https://github.com/fastai/fastai/blob/master/courses/dl1/excel/entropy_example.xlsx)
    where you will see an example of softmax. Softmax simply takes the outputs from
    our final layer, so we get our outputs from our linear layer. And what we do is
    we go *e* to the power of (*e*^) for each output.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†å…¶å±•å¹³ï¼Œè¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œæœ€åè¿›è¡Œsoftmaxã€‚æ‰€ä»¥softmaxæ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚å¦‚æœæ‚¨æŸ¥çœ‹æ·±åº¦å­¦ä¹ å­˜å‚¨åº“ï¼Œæ‚¨ä¼šå‘ç°ä¸€ä¸ªåä¸ºç†µç¤ºä¾‹çš„å†…å®¹ï¼Œæ‚¨å°†åœ¨å…¶ä¸­çœ‹åˆ°softmaxçš„ç¤ºä¾‹ã€‚Softmaxç®€å•åœ°è·å–æˆ‘ä»¬æœ€ç»ˆå±‚çš„è¾“å‡ºï¼Œå› æ­¤æˆ‘ä»¬ä»çº¿æ€§å±‚è·å–è¾“å‡ºã€‚æˆ‘ä»¬æ‰€åšçš„æ˜¯å¯¹æ¯ä¸ªè¾“å‡ºè¿›è¡Œ*e*çš„(*e*^)è¿ç®—ã€‚
- en: '![](../Images/95e41c192f16b15b0842c2ff3dfc6cc6.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95e41c192f16b15b0842c2ff3dfc6cc6.png)'
- en: Then we take that number and divide by the sum of the *e* to the poser ofâ€™s.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å–è¿™ä¸ªæ•°å­—ï¼Œé™¤ä»¥*e*çš„å¹‚çš„æ€»å’Œã€‚
- en: '![](../Images/7cd292ddd615acc65ceb6ab7de594fe4.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cd292ddd615acc65ceb6ab7de594fe4.png)'
- en: Thatâ€™s called softmax. Why do we do that? Well, because we are dividing this
    (exp) with the sum, that means the sum of those itself must add to one. And thatâ€™s
    what we want. We want the probabilities of all the possible outcomes add to one.
    Furthermore, because we are using *e*^ , that means we know that every one of
    these (softmax) is between zero and one. And probabilities we know would be between
    zero and one. Then finally because we are using *e* to the power of, it tends
    to mean that slightly bigger values in the input turn into much bigger values
    in the output. So youâ€™ll see, generally speaking, my softmax there are going to
    be one big number and lots of small numbers. And thatâ€™s what we want because we
    know that the output is one hot encoded. So in other words a softmax activation
    function, the softmax non-linearity, is something that returns things that behave
    like probabilities where one of those probabilities is more likely to be kind
    of high and the other ones are more likely to be low. And we know thatâ€™s what
    we want to map to our one hot encoding so a softmax is a great activation function
    to use to help the neural net, make it easier for the neural net to map to the
    output you wanted. And this is what we generally want. When we are designing neural
    networks, we try to come up with little architectural tweaks that make it as easy
    for it as possible to match the output that we know we want.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æ‰€è°“çš„softmaxã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿™æ ·åšï¼Ÿå› ä¸ºæˆ‘ä»¬æ­£åœ¨å°†è¿™ä¸ªï¼ˆexpï¼‰é™¤ä»¥æ€»å’Œï¼Œè¿™æ„å‘³ç€è¿™äº›æœ¬èº«çš„æ€»å’Œå¿…é¡»åŠ èµ·æ¥ä¸ºä¸€ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰å¯èƒ½ç»“æœçš„æ¦‚ç‡æ€»å’Œä¸ºä¸€ã€‚æ­¤å¤–ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨*e*^ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çŸ¥é“è¿™äº›ï¼ˆsoftmaxï¼‰ä¸­çš„æ¯ä¸€ä¸ªéƒ½åœ¨é›¶å’Œä¸€ä¹‹é—´ã€‚æˆ‘ä»¬çŸ¥é“æ¦‚ç‡å°†åœ¨é›¶å’Œä¸€ä¹‹é—´ã€‚æœ€åï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨*e*çš„å¹‚ï¼Œè¿™æ„å‘³ç€è¾“å…¥ä¸­ç¨å¤§çš„å€¼ä¼šå˜æˆè¾“å‡ºä¸­çš„æ›´å¤§å€¼ã€‚å› æ­¤ï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨ä¼šçœ‹åˆ°æˆ‘çš„softmaxä¸­æœ‰ä¸€ä¸ªå¤§æ•°å’Œè®¸å¤šå°æ•°ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“è¾“å‡ºæ˜¯ä¸€çƒ­ç¼–ç çš„ã€‚æ¢å¥è¯è¯´ï¼Œsoftmaxæ¿€æ´»å‡½æ•°ï¼Œsoftmaxéçº¿æ€§ï¼Œæ˜¯ä¸€ç§è¿”å›ç±»ä¼¼æ¦‚ç‡çš„ä¸œè¥¿çš„ä¸œè¥¿ï¼Œå…¶ä¸­å…¶ä¸­ä¸€ä¸ªæ¦‚ç‡æ›´æœ‰å¯èƒ½æ˜¯é«˜çš„ï¼Œå…¶ä»–æ¦‚ç‡æ›´æœ‰å¯èƒ½æ˜¯ä½çš„ã€‚æˆ‘ä»¬çŸ¥é“è¿™å°±æ˜¯æˆ‘ä»¬æƒ³è¦æ˜ å°„åˆ°æˆ‘ä»¬çš„ä¸€çƒ­ç¼–ç çš„å†…å®¹ï¼Œå› æ­¤softmaxæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥å¸®åŠ©ç¥ç»ç½‘ç»œæ›´å®¹æ˜“åœ°æ˜ å°„åˆ°æ‚¨æƒ³è¦çš„è¾“å‡ºã€‚è¿™é€šå¸¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚å½“æˆ‘ä»¬è®¾è®¡ç¥ç»ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬å°è¯•æå‡ºä¸€äº›å°çš„æ¶æ„è°ƒæ•´ï¼Œä½¿å…¶å°½å¯èƒ½å®¹æ˜“åœ°åŒ¹é…æˆ‘ä»¬æƒ³è¦çš„è¾“å‡ºã€‚
- en: So thatâ€™s basically it [[1:30:45](https://youtu.be/DzE0eSdy5Hk?t=5445)]. Rather
    than doing Sequential and using `nn.Linear` and `nn.LogSoftmax`, weâ€™ve defined
    it from scratch. We can now say, just like before, our `net2` is equal to `LogReg().cuda()`
    and we can say `fit` and we get to, within a slight random deviation, exactly
    the same output.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šå°±æ˜¯è¿™æ ·ã€‚ä¸å…¶ä½¿ç”¨Sequentialå’Œ`nn.Linear`ä»¥åŠ`nn.LogSoftmax`ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹å®šä¹‰äº†å®ƒã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯´ï¼Œå°±åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬çš„`net2`ç­‰äº`LogReg().cuda()`ï¼Œæˆ‘ä»¬å¯ä»¥è¯´`fit`ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å‡ ä¹å®Œå…¨ç›¸åŒçš„è¾“å‡ºï¼Œåªæ˜¯æœ‰è½»å¾®çš„éšæœºåå·®ã€‚
- en: '[PRE28]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So what I like you to do during the week is to play around with `torch.randn`
    to generate some random tensors, `torch.matmul` to start multiplying them together,
    adding them up, try to make sure that you can rewrite softmax yourself from scratch.
    Try to fiddle around a bit with reshaping, view, all that kind of stuff so by
    the time you come back next week you feel pretty comfortable with PyTorch.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¸Œæœ›ä½ åœ¨è¿™ä¸€å‘¨é‡Œå°è¯•ä½¿ç”¨`torch.randn`ç”Ÿæˆä¸€äº›éšæœºå¼ é‡ï¼Œä½¿ç”¨`torch.matmul`å¼€å§‹å°†å®ƒä»¬ç›¸ä¹˜ï¼Œç›¸åŠ ï¼Œå°è¯•ç¡®ä¿ä½ å¯ä»¥è‡ªå·±ä»å¤´å¼€å§‹é‡å†™softmaxã€‚å°è¯•ç©å¼„ä¸€ä¸‹é‡å¡‘ã€viewç­‰ç­‰ï¼Œè¿™æ ·åˆ°ä¸‹å‘¨ä½ å›æ¥æ—¶å°±ä¼šæ„Ÿè§‰å¯¹PyTorchç›¸å½“èˆ’é€‚ã€‚
- en: And if you google for PyTorch tutorial, youâ€™ll see thereâ€™s a lot of great material
    actually on the [PyTorch website](https://pytorch.org/tutorials/) to help you
    along â€” showing you how to create tensors, modify them, and do operations on them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœç´¢PyTorchæ•™ç¨‹ï¼Œæ‚¨ä¼šçœ‹åˆ°[PyTorchç½‘ç«™](https://pytorch.org/tutorials/)ä¸Šæœ‰å¾ˆå¤šå¾ˆå¥½çš„ææ–™å¯ä»¥å¸®åŠ©æ‚¨ï¼Œå‘æ‚¨å±•ç¤ºå¦‚ä½•åˆ›å»ºå¼ é‡ï¼Œä¿®æ”¹å®ƒä»¬ä»¥åŠå¯¹å®ƒä»¬è¿›è¡Œæ“ä½œã€‚
- en: '**Question**: So I see that the forward is the layer that gets applied after
    each of the linear layers[[1:31:57](https://youtu.be/DzE0eSdy5Hk?t=5517)].'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šæˆ‘çœ‹åˆ°å‰å‘æ˜¯åœ¨æ¯ä¸ªçº¿æ€§å±‚ä¹‹ååº”ç”¨çš„å±‚ã€‚
- en: '**Jeremy**: Not quite. The forward is just the definition of the module, so
    this is how we are implementing Linear.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremyï¼šä¸å®Œå…¨æ˜¯ã€‚å‰å‘åªæ˜¯æ¨¡å—çš„å®šä¹‰ï¼Œè¿™æ˜¯æˆ‘ä»¬å®ç°Linearçš„æ–¹å¼ã€‚
- en: '**Continued**: Does that mean after each linear layer, you have to apply the
    same function? Letâ€™s say we canâ€™t do a LogSoftmax after layer 1 and then apply
    some other function after layer two if we have a multi-layer neural network?'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§ç»­ï¼šè¿™æ˜¯å¦æ„å‘³ç€åœ¨æ¯ä¸ªçº¿æ€§å±‚ä¹‹åï¼Œæ‚¨å¿…é¡»åº”ç”¨ç›¸åŒçš„å‡½æ•°ï¼Ÿå‡è®¾æˆ‘ä»¬ä¸èƒ½åœ¨ç¬¬ä¸€å±‚ä¹‹ååº”ç”¨LogSoftmaxï¼Œç„¶ååœ¨ç¬¬äºŒå±‚ä¹‹ååº”ç”¨å…¶ä»–å‡½æ•°ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œï¼Ÿ
- en: '**Jeremy**: So normally we define neural networks like so:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremyï¼šæ‰€ä»¥é€šå¸¸æˆ‘ä»¬è¿™æ ·å®šä¹‰ç¥ç»ç½‘ç»œï¼š
- en: '![](../Images/361ca59ba0adecf8fe540de419fe837f.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/361ca59ba0adecf8fe540de419fe837f.png)'
- en: 'We just say here is a list of the layers we want. You donâ€™t have to write your
    own forward. All we did just now is to say instead of doing this, letâ€™s not use
    any of this at all, but write it all by hand ourselves. So you can write as many
    layers as you like in any order you like here. The point was that here, we are
    not using any of that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªæ˜¯è¯´è¿™é‡Œæ˜¯æˆ‘ä»¬æƒ³è¦çš„å±‚çš„åˆ—è¡¨ã€‚æ‚¨ä¸å¿…ç¼–å†™è‡ªå·±çš„å‰å‘ã€‚æˆ‘ä»¬åˆšåˆšåšçš„æ˜¯è¯´ï¼Œä¸å…¶è¿™æ ·åšï¼Œä¸å¦‚å®Œå…¨ä¸ä½¿ç”¨è¿™äº›ï¼Œè€Œæ˜¯è‡ªå·±æ‰‹å†™æ‰€æœ‰å†…å®¹ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥æŒ‰ä»»ä½•é¡ºåºç¼–å†™ä»»æ„æ•°é‡çš„å±‚ã€‚é‡ç‚¹æ˜¯åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ä»»ä½•è¿™äº›ï¼š
- en: '![](../Images/49a74247bc6ace55da99560b4810f673.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49a74247bc6ace55da99560b4810f673.png)'
- en: Weâ€™ve written our own `matmul` plus bias, our own softmax, so this is just Python
    code. You can write whatever Python code inside forward that you like to define
    your own neural net. You wonâ€™t normally do this yourself. Normally youâ€™ll just
    use the layers that PyTorch provides and youâ€™ll use `.Sequential` to put them
    together. Or even more likely, youâ€™ll download a predefined architecture and use
    that. Weâ€™re just doing this to learn how it works behind the scenes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»ç¼–å†™äº†è‡ªå·±çš„`matmul`åŠ åç½®é¡¹ï¼Œè‡ªå·±çš„softmaxï¼Œæ‰€ä»¥è¿™åªæ˜¯Pythonä»£ç ã€‚æ‚¨å¯ä»¥åœ¨forwardå‡½æ•°å†…ç¼–å†™ä»»ä½•æ‚¨å–œæ¬¢çš„Pythonä»£ç æ¥å®šä¹‰è‡ªå·±çš„ç¥ç»ç½‘ç»œã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨ä¸ä¼šè‡ªå·±è¿™æ ·åšã€‚é€šå¸¸æ‚¨åªä¼šä½¿ç”¨PyTorchæä¾›çš„å±‚ï¼Œå¹¶ä½¿ç”¨`.Sequential`å°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ã€‚æˆ–è€…æ›´æœ‰å¯èƒ½çš„æ˜¯ï¼Œæ‚¨ä¼šä¸‹è½½ä¸€ä¸ªé¢„å®šä¹‰çš„æ¶æ„å¹¶ä½¿ç”¨å®ƒã€‚æˆ‘ä»¬åªæ˜¯ä¸ºäº†å­¦ä¹ å®ƒåœ¨å¹•åæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: Alright, great. Thanks everybody!
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œå¤ªæ£’äº†ã€‚è°¢è°¢å¤§å®¶ï¼
