- en: 'Machine Learning 1: Lesson 8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第8课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-8-fa1a87064a53](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-8-fa1a87064a53)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-8-fa1a87064a53](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-8-fa1a87064a53)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自*[*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*[*Jeremy*](https://twitter.com/jeremyphoward)*和*[*Rachel*](https://twitter.com/math_rachel)*给了我这个学习的机会。*'
- en: Neural nets broadly defined
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广义定义的神经网络
- en: '[Video](https://youtu.be/DzE0eSdy5Hk) / [Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[视频](https://youtu.be/DzE0eSdy5Hk) / [笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
- en: As we discussed at the end of last lesson, we’re moving from decision tree ensembles
    to neural nets broadly defined. As you know, random forests and decision trees
    are limited by the fact in the end that they are basically doing nearest neighbors.
    All they can do is to return the average of a bunch of other points. So they can’t
    extrapolate out to, if you are thinking what happens if I’ll increase my price
    by 20% and you’ve never priced at that level before, or what’s going to happen
    to sales next year and obviously we’ve never seen next year before and it’s very
    hard to extrapolate. It’s also hard as it can only do around log base 2 N decisions
    so if there is a time series it needs to fit to, that takes 4 steps to get to
    the right time area then suddenly there’s not many decisions left for it to make
    so it’s kind of this limited amount of computation that it can do. So there is
    a limited complexity of relationship that it can model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一课结束时讨论的那样，我们正在从决策树集成转向广义定义的神经网络。如你所知，随机森林和决策树受到一个限制，即它们基本上只是在做最近邻。它们所能做的就是返回一堆其他点的平均值。因此，它们无法外推，如果你在考虑如果我将价格提高20%，而你以前从未定价到那个水平，或者明年的销售情况会发生什么，显然我们以前从未见过明年，外推是非常困难的。它也很难，因为它只能做大约对数2的N次决策，所以如果有一个时间序列需要拟合，需要4步才能到达正确的时间区域，然后突然它没有多少决策可以做了，所以它可以做的计算量有限。因此，它可以建模的关系复杂度有限。
- en: '**Question**: Can I ask about one more drawback of random forests? If we have
    a data as categorical variable which are not in sequential order, for random forests,
    we encode them and treat them as numbers, let’s say we have 20 cardinality so
    the split random forest gives is like less than 5 or less than 6\. But if the
    categories are not sequential (i.e. not in any order), what does that mean [[2:00](https://youtu.be/DzE0eSdy5Hk?t=2m)]?
    So if you’ve got like, let’s go back to bulldozers, EROPS, EROPS w A/C, OROPS,
    N/A, etc, and we arbitrarily label them from 0 to 3\. Actually we know that all
    that really mattered was if it had air conditioning. So what’s going to happen?
    It’s basically going to say, if I group it into EROPS w A/C and OROPS together,
    and N/A and EROPS together, that’s an interesting break just because it so happens
    that the air conditioning ones all are going to end up in the right hand side.
    Having done that, it’s then going to say within the group with the EROPS w A/C
    and OROPS, it’s going to notice that it’s furthermore going to have to split it
    into two more groups. So eventually it’s going to get there. It’s going to pull
    out the category with AC. It’s just it’s going to take more splits than we would
    ideally like. So it’s kind of similar to the fact that for it to model a line,
    it can only do it with lots of splits and only approximately.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我可以问一个关于随机森林的另一个缺点吗？如果我们有一个数据作为分类变量，这些变量不是按顺序排列的，对于随机森林，我们对它们进行编码并将它们视为数字，假设我们有20个基数，那么随机森林给出的分割结果可能是小于5或小于6。但如果类别不是按顺序排列（即没有任何顺序），那意味着什么？所以如果你有，比如说，让我们回到推土机，EROPS，带空调的EROPS，OROPS，N/A等，我们任意地将它们标记为0到3。实际上我们知道真正重要的是是否有空调。那会发生什么？基本上它会说，如果我将EROPS
    w A/C和OROPS组合在一起，将N/A和EROPS组合在一起，这是一个有趣的分割，因为碰巧所有带空调的都会最终出现在右侧。做完这一步后，它会进一步注意到在EROPS
    w A/C和OROPS组中，它还需要将其进一步分成两组。最终它会到达那里。它会提取带有空调的类别。只是它需要更多的分割，比我们理想中希望的要多。所以这有点类似于它要建模一条线，只能通过大量分割并且只是近似地完成。'
- en: '**Follow up question**: So random forest is fine with categories that are not
    sequential also [[3:58](https://youtu.be/DzE0eSdy5Hk?t=3m58s)]? Yes, it can do
    it. It’s just in ways it’s sub-optimal because we just need to do more breakpoints
    than we would have liked, but it gets there. It does a pretty good job. So even
    although random forests do have some deficiencies, they are incredibly powerful,
    particularly because they have so few assumptions that they are really hard to
    screw up. It’s kind of hard to actually hard to win Kaggle competition with a
    random forest, but it’s very easy to get like top 10%. So in real life where often
    that third decimal place doesn’t really matter, random forests are often what
    you end up doing. But for some things like this Ecuadorian groceries competition,
    it’s very very hard to get a good result with a random forest because there’s
    a huge time series component and nearly everything is these two massively high
    cardinality categorical variables which is the store and the item. So there’s
    very little layer to even throw at a random forest and the difference between
    every pair of stores is kind of different in different ways so there are some
    things that are just hard to get even relatively good results for the random forest.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**后续问题**：那么随机森林对于不是连续的类别也可以吗？是的，它可以。只是在某些方面它不够理想，因为我们需要做比我们想要的更多的分割点，但它可以做到。它做得相当不错。因此，尽管随机森林确实存在一些缺陷，但它们非常强大，特别是因为它们几乎没有假设，所以很难出错。用随机森林赢得Kaggle比赛有点困难，但很容易进入前10%。因此，在现实生活中，通常第三位小数并不是很重要，随机森林通常是你最终会做的事情。但对于像厄瓜多尔杂货比赛这样的事情，用随机森林很难得到好的结果，因为有一个巨大的时间序列组件，几乎所有的东西都是这两个大规模高基数的分类变量，即店铺和商品。因此，甚至没有太多的层可以用随机森林，每对店铺之间的差异在不同方面都是不同的，因此有一些事情即使对于随机森林来说也很难得到相对好的结果。'
- en: Another example is recognizing numbers. You can get okay results with a random
    forest, but in the end, they are kind of the relationship between the spacial
    structure turns out to be important. And you kind of want to be able to do computations
    like finding edges or whatever that carry forward through the computation. So
    just doing a clever nearest neighbors like a random forest turns out not to be
    ideal. So for stuff like this, neural networks turn out that they are ideal. Neural
    networks turn out to be something that works particularly well for both things
    like Ecuadorian groceries competition (i.e. forecasting sales over time by store
    and by item) and for things like recognizing digits, and for things like turning
    voice into speech. So it’s nice between these two things, neural nets and random
    forests, we cover the territory. I haven’t needed to use anything other than these
    two things for a very long time. And at some point, we will learn how to combine
    the two because you can combine the two in really cool ways.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是识别数字。你可以用随机森林得到可以接受的结果，但最终，空间结构之间的关系变得重要。你可能想要能够进行像查找边缘或其他计算一样的计算，这些计算会在计算中继续进行。因此，仅仅做一个聪明的最近邻类似于随机森林的方法并不理想。所以对于这样的事情，神经网络是理想的。神经网络被证明对于像厄瓜多尔杂货比赛（即通过店铺和商品预测销售额）和识别数字这样的事情非常有效。所以在这两个事情之间，神经网络和随机森林，我们覆盖了领域。我很长一段时间以来一直没有使用除了这两个方法之外的任何其他方法。在某个时候，我们将学习如何将这两种方法结合起来，因为你可以以非常酷的方式将它们结合起来。
- en: MNIST [[6:37](https://youtu.be/DzE0eSdy5Hk?t=6m37s)]
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST [[6:37](https://youtu.be/DzE0eSdy5Hk?t=6m37s)]
- en: '![](../Images/6d0d56b41815a97b5971ddf6c4241d62.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d0d56b41815a97b5971ddf6c4241d62.png)'
- en: Here is a picture from [Adam Geitgey](/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)
    of an image. An image is just a bunch of numbers and each of those numbers is
    naught to 255 and the dark ones are close to 255, light ones are close to zero.
    Here is an example of digit from this MNIST dataset. MNIST is a really old, like
    a hello world of neural networks. So here is an example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[Adam Geitgey](/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)的一张图片。一张图片只是一堆数字，每个数字都是从0到255，暗的接近255，亮的接近0。这是来自MNIST数据集的一个数字的例子。MNIST是一个非常古老的，就像神经网络的hello
    world一样。所以这是一个例子。
- en: '![](../Images/337b7d6e6d7c98914ba89ff2e1f32983.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/337b7d6e6d7c98914ba89ff2e1f32983.png)'
- en: There are 28 by 28 pixels. If it was color, there would be three of these —
    one for red, one for green, one for blue. Our job is to look at the array of numbers
    and figure out that this is the number 8 which is tricky. How do we do that?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有28x28个像素。如果是彩色的话，会有三个 —— 一个红色的，一个绿色的，一个蓝色的。我们的任务是查看数字数组并弄清楚这是一个棘手的数字8。我们如何做到这一点？
- en: We are going to use a small number of FastAI pieces and we are gradually going
    to remove more and more until by the end, we’ll have implemented our own neural
    network from scratch, our own training loop from scratch, and our own matrix multiplication
    from scratch. So we are gradually going to dig in further and further.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一小部分FastAI的内容，并逐渐去除更多，直到最后，我们将从头开始实现自己的神经网络，自己的训练循环，以及自己的矩阵乘法。因此，我们将逐渐深入挖掘更多。
- en: Data [[7:54](https://youtu.be/DzE0eSdy5Hk?t=7m54s)]
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据 [[7:54](https://youtu.be/DzE0eSdy5Hk?t=7m54s)]
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data for MNIST, which is the name of this very famous dataset is available
    from here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST的数据，这个非常著名的数据集的名称，可以从这里获取：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And we have a thing in `fastai.io` called `get_data` which will grab it from
    URL and store it on your computer unless it’s already there in which case it’ll
    just go ahead and use it. And we’ve got a little function here called `load_mnist`
    which simply loads it up. You’ll see that it’s zipped so we could just use Python’s
    gzip to open it up. And then it’s also pickled, so if you have any kind of Python
    object at all, you can use this build-in Python library called `pickle` to dump
    it out onto your disk, share it around, load it up later, and you get back the
    same Python object you started with. You’ve already seen something like this with
    Pandas’ feather format. Pickle is not just for Pandas, it’s not just for anything,
    it works for basically nearly every Python object. So which might lead to the
    question why didn’t we use pickle for a Pandas’ DataFrame. The answer is pickle
    works for nearly every Python object but it’s probably not optimal for nearly
    any Python object. So because we were looking at Pandas DataFrames with over a
    hundred million rows, we really want to save that quickly so feather is a format
    that’s specifically designed for that purpose and so it’s going to do that really
    fast. If we tried to pickle it, it would have taken a lot longer. Also note that
    pickle files are only for Python so you can’t give them to somebody else where
    else a feather file, you can hand around. So it’s worth knowing that pickle exists
    because if you’ve got some dictionary or some kind of object floating around that
    you want to save for later or send to somebody else, you can always just pickle
    it. So in this particular case, the folks at deeplearning.net was kind enough
    to provide a pickled version.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `fastai.io` 中有一个叫做 `get_data` 的东西，它会从 URL 中获取数据并将其存储在你的计算机上，除非它已经存在，否则它将继续使用它。我们这里有一个叫做
    `load_mnist` 的小函数，它简单地加载数据。你会看到它是压缩的，所以我们可以使用 Python 的 gzip 来打开它。然后它也被 pickled，所以如果你有任何类型的
    Python 对象，你可以使用这个内置的 Python 库叫做 `pickle` 来将其转储到你的磁盘上，分享它，稍后加载它，你会得到与开始时相同的 Python
    对象。你已经看到了类似于 Pandas 的 feather 格式的东西。Pickle 不仅仅适用于 Pandas，也不仅仅适用于任何东西，它基本上适用于几乎每个
    Python 对象。这可能会引发一个问题，为什么我们不为 Pandas 的 DataFrame 使用 pickle。答案是 pickle 适用于几乎每个 Python
    对象，但对于几乎任何 Python 对象来说，它可能不是最佳选择。因此，因为我们正在查看具有超过一亿行的 Pandas DataFrames，我们真的希望快速保存，所以
    feather 是专门为此目的设计的格式，因此它会非常快速地完成。如果我们尝试 pickle 它，那将需要更长的时间。另外请注意，pickle 文件仅适用于
    Python，因此你不能将它们交给其他人，而 feather 文件可以传递。所以值得知道 pickle 的存在，因为如果你有一些字典或某种对象漂浮在周围，你想要稍后保存或发送给其他人，你总是可以将其
    pickle 化。所以在这种特殊情况下，deeplearning.net 的人们很友好地提供了一个 pickled 版本。
- en: Pickle has changed slightly over time so old pickle files like this one (this
    was Python 2 one), you actually have to tell it that it was encoded using this
    particular Python 2 character set [[10:10](https://youtu.be/DzE0eSdy5Hk?t=610)].
    But other than that, Python 2 and 3, you can normally open each other’s pickle
    files.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pickle 随着时间的推移有些变化，所以像这样的旧 pickle 文件（这是 Python 2 的一个），你实际上必须告诉它是使用这个特定的 Python
    2 字符集编码的。但除此之外，Python 2 和 3，你通常可以打开彼此的 pickle 文件。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once we loaded that in, we loaded in like so `((x, y), (x_valid, y_valid), _)`.
    And so this thing which we are doing here is called destructuring. Destructuring
    means that `load_mnist` is giving us back a tuple of tuples. If we have on the
    left hand side of the equal sign a tuple of tuples, we can fill all these things
    in. So we are given back a tuple of training data, a tuple of validation data,
    and a tuple of test data. In this case, I don’t care about the test data so I
    just put it into a variable called `_` which Python people tend to think of as
    being a special variable which we put things we’re going to throw away into. It’s
    actually not special but it’s really common. If you see something assigned to
    underscore, it probably means you’re just throwing it away.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们加载了这个，我们就像这样加载 `((x, y), (x_valid, y_valid), _)`。所以我们这里正在做的事情叫做解构。解构意味着
    `load_mnist` 给我们返回了一个元组的元组。如果在等号的左边有一个元组的元组，我们可以填充所有这些内容。所以我们得到了一个训练数据的元组，一个验证数据的元组，以及一个测试数据的元组。在这种情况下，我不关心测试数据，所以我把它放到一个名为
    `_` 的变量中，Python 的人们倾向于认为这是一个特殊的变量，我们把要丢弃的东西放进去。它实际上并不特殊，但非常常见。如果你看到有东西被赋值给下划线，那可能意味着你只是要丢弃它。
- en: By the way, in a Jupyter notebook it does have a special meaning which is the
    last cell that you calculate is always available in underscore [[11:24](https://youtu.be/DzE0eSdy5Hk?t=684)].
    But that’s kind of a separate issue.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，在 Jupyter 笔记本中它确实有一个特殊的含义，即你计算的最后一个单元格始终在下划线中可用。但这是一个独立的问题。
- en: Then the first thing in that tuple is itself a tuple and so we’re going to stick
    that into x and y for our training data, and then the second one goes into x and
    y for our validation data. So that’s called destructuring and it’s pretty common
    in lots of languages. Some languages don’t support it but those that do, life
    becomes a lot easier. As soon as I look at some new dataset, I just check out
    what have I got. What’s its type? Numpy array. What’s its shape? 50,000 by 784\.
    Then what about the dependent variables? That’s an array, its shape is 50,000.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后元组中的第一件事本身就是一个元组，所以我们将把它放入 x 和 y 中作为我们的训练数据，然后第二个元组放入 x 和 y 中作为我们的验证数据。所以这就是所谓的解构，它在许多语言中都很常见。有些语言不支持它，但那些支持的语言，生活会变得更容易。一旦我看到一些新的数据集，我就会查看我得到了什么。它是什么类型？Numpy
    数组。它的形状是什么？50,000 x 784。那么因变量呢？那是一个数组，它的形状是 50,000。
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The image of 8 we saw earlier is not of length 784, it’s of size 28 by 28 [[12:18](https://youtu.be/DzE0eSdy5Hk?t=740)].
    So what happened here? It turns out that all they did was they took the second
    row and concatenate it to the first row, and the third row and concatenate it
    to that, and the fourth row and concatenated that. So in other words, they took
    the whole 28 by 28 and flattened it out into a single 1D array. Does that makes
    sense? So it’s going to be of size 28². This is not normal by any means, so don’t
    think everything you see is going to be like this. Most of the time when people
    share images, they share them as JPEGs or PNGs, you load them up, you get back
    a nice 2D array. But in this particular case for whatever reason, the thing that
    they pickled was flattened out to be 784\. And this word “flatten” is very common
    with working with tensors so when you flatten a tensor, it just means that you’re
    turning it into a lower rank tensor than you started with. In this case, we started
    with a rank 2 tensor (i.e. a matrix) for each image and we turned each one into
    a rank 1 tensor (i.e. a vector). So overall the whole thing is a rank 2 tensor
    rather than a rank 3 . tensor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的8的图像不是长度为784，而是大小为28乘以28。所以这里发生了什么？事实证明，他们只是将第二行连接到第一行，将第三行连接到第二行，将第四行连接到第三行。换句话说，他们将整个28乘以28展平成一个单一的一维数组。这有意义吗？所以它的大小将是28²。这绝对不是正常的，所以不要认为你看到的一切都会是这样。大多数时候，当人们分享图像时，他们会将它们分享为JPEG或PNG格式，你加载它们，你会得到一个漂亮的二维数组。但在这种特殊情况下，出于某种原因，他们拿出来的东西被展平成了784。这个“展平”这个词在处理张量时非常常见，所以当你展平一个张量时，这意味着你将它转换为比你开始的更低秩的张量。在这种情况下，我们为每个图像开始时是一个秩为2的张量（即矩阵），然后我们将每个图像转换为一个秩为1的张量（即向量）。所以整体来说，整个东西是一个秩为2的张量，而不是一个秩为3的张量。
- en: So just to remind us of the jargon here [[13:50](https://youtu.be/DzE0eSdy5Hk?t=830)],
    this in math, we would call a vector. In computer science, we would call it a
    1D array, but because deep learning have people who have to come across as smarter
    than everybody else, we have to call this a rank 1 tensor. They all mean the same
    thing more or less unless you’re a physicist — in which case, this means something
    else and you get very angry at the deep learning people because you say “it’s
    not a tensor”. So there you go. Don’t blame me. This is just what people say.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以只是为了提醒我们这里的行话，这在数学中我们会称之为向量。在计算机科学中，我们会称之为一维数组，但是因为深度学习的人们必须表现得比其他人更聪明，我们不得不称之为秩为1的张量。它们基本上意思相同，除非你是物理学家——在这种情况下，这意味着其他事情，你会对深度学习的人们感到非常生气，因为你会说“这不是张量”。所以就是这样。不要责怪我。这只是人们说的话。
- en: '![](../Images/6f52769b04a3f34bd5622692e7b815b6.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f52769b04a3f34bd5622692e7b815b6.png)'
- en: So this is either a matrix or a 2D array or a rank 2 tensor.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这要么是一个矩阵，要么是一个二维数组，要么是一个秩为2的张量。
- en: '![](../Images/7e7bcdcd094e2066d9d6ee0928ec0433.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e7bcdcd094e2066d9d6ee0928ec0433.png)'
- en: Once we start to get into three dimensions, we start to run out of mathematical
    names which is why we start to be nice and just say rank three tensor. So there’s
    actually nothing special about vectors and matrices that makes them in any way
    more important than rank 3 tensors or rank 4 tensors. So I try not to use the
    terms vector and matrix where possible because I don’t really think they’re any
    more special than any other rank of tensor. So it’s good to get used to thinking
    of this `numpy.ndarray (50,000, 784)` as a rank 2 tensor.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们开始进入三维，我们开始用完数学名字，这就是为什么我们开始友好地说秩为3的张量。所以实际上，没有什么特别的关于向量和矩阵使它们比秩为3或秩为4的张量更重要。所以我尽量不使用向量和矩阵这些术语，因为我真的不认为它们比其他秩的张量更特别。所以习惯将`numpy.ndarray
    (50,000, 784)`看作秩为2的张量是很好的。
- en: And then the rows and columns [[15:25](https://youtu.be/DzE0eSdy5Hk?t=925)].
    If we were computer science people, we would call this dimension zero and dimension
    one. But if we were deep learning people, we would call this axis zero and axis
    one. Then just to be really confusing, if you were an image person, columns are
    the first axis and rows are the second axis.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是行和列。如果我们是计算机科学人员，我们会称之为零维和一维。但如果我们是深度学习人员，我们会称之为轴零和轴一。然后为了更加混淆，如果你是一个图像人员，列是第一个轴，行是第二个轴。
- en: '![](../Images/c79efecfed2414479477e970e4608da8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c79efecfed2414479477e970e4608da8.png)'
- en: So if you think about TVs, 1920 by 1080 — columns by rows. Everybody else including
    deep learning and mathematicians, rows by columns. So this is pretty confusing
    if you use Python imaging library, you get columns by rows; pretty much everything
    else, rows by columns. So be careful. [A student asks “why do they do that?”]
    Because they hate us, because they’re bad people, I guess 😆
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你想到电视，1920乘以1080——列乘以行。其他人包括深度学习和数学家，行乘以列。所以如果你使用Python图像库，你会得到列乘以行；几乎其他所有情况，行乘以列。所以要小心。[一个学生问“为什么他们这样做？”]因为他们讨厌我们，因为他们是坏人，我猜😆
- en: There’s a lot of, particularly in deep learning, a whole lot of different areas
    have come together like information theory, computer vision, statistics, signal
    processing and you’ve ended up with this hodgepodge of nomenclature in deep learning
    [[16:39](https://youtu.be/DzE0eSdy5Hk?t=999)]. Often like every version of things
    will be used, so today, we are going to hear about something that’s called either
    negative log likelihood or binomial or categorical cross entropy, depending on
    where you come from. We’ve already seen something that’s called either one hot
    encoding or dummy variables depending on where you come from. And really it’s
    just like the same concept gets kind of somewhat independently invented in different
    fields and eventually they find their way to machine learning and then we don’t
    know what to call them so we call them all of the above — something like that.
    So I think that’s what happened with computer vision rows and columns.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中有很多，许多不同领域汇集在一起，如信息论、计算机视觉、统计学、信号处理，最终形成了深度学习中的这种混杂的命名法。通常，每个版本的事物都会被使用，所以今天，我们将听到一些被称为负对数似然或二项式或分类交叉熵的东西，这取决于你来自哪里。我们已经看到了一些被称为独热编码或虚拟变量的东西，这取决于你来自哪里。实际上，这只是相同的概念在不同领域中有点独立地被发明，最终它们找到了通往机器学习的道路，然后我们不知道该如何称呼它们，所以我们称它们为以上所有的东西——就像这样。所以我认为这就是计算机视觉中的行和列发生的事情。
- en: Normalize [[17:38](https://youtu.be/DzE0eSdy5Hk?t=1058)]
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化
- en: There’s this idea of normalizing data which is subtracting out the mean and
    dividing by the standard deviation. A question for you. Often it’s important to
    normalize the data so that we can more easily train a model. Do you think it would
    be important to normalize the independent variables for a random forest (if we
    are training a random forest)?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有这样一个概念，即对数据进行归一化，即减去均值并除以标准差。一个问题给你。通常，归一化数据很重要，这样我们就可以更容易地训练模型。你认为在训练随机森林时，归一化独立变量是否重要呢？
- en: 'Student: To be honest, I don’t know why we don’t need to normalize, I just
    know that we don’t.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 学生：老实说，我不知道为什么我们不需要归一化，我只知道我们不需要。
- en: Okay, does anybody want to think about why? So really, the key is that when
    we are deciding where to split, all that matters is the order. Like all that matters
    is how they are sorted, so if we subtract the mean divide by the standard deviation,
    they are still sorted in the same order. So remember when we implemented the random
    forest, we said sort them and then we completely ignored the values. We just said
    now add on one thing from the dependent at a time. So random forests only care
    about the sort order of the independent variables. They don’t care at all about
    their size. So that’s why they’re wonderfully immune to outliers because they
    totally ignore the fact that it’s an outlier, they only care about which one is
    higher than what other thing. So this is an important concept. It doesn’t just
    appear in random forests. It occurs in some metrics as well. For example, area
    under the ROC curve, you come across a lot, area under the ROC curve completely
    ignores scale and only cares about sort. We saw something else when we did the
    dendrogram. Spearman’s correlation is a rank correlation — only cares about order,
    not about scale. So random forests, one of the many wonderful things about them
    are that we can completely ignore a lot of these statistical distribution issues.
    But we can’t for deep learning because deep learning, we are trying to train a
    parameterized model. So we do need to normalize our data. If we don’t then it’s
    going to be much harder to create a network that trains effectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，有人想想为什么吗？真正的关键是，当我们决定在哪里分割时，唯一重要的是顺序。就像唯一重要的是它们是如何排序的，所以如果我们减去均值除以标准差，它们仍然按相同的顺序排序。所以记住当我们实现随机森林时，我们说对它们进行排序，然后完全忽略值。我们只是说现在一次添加一个来自依赖变量的东西。所以随机森林只关心独立变量的排序顺序。它们根本不关心它们的大小。这就是为什么它们对异常值非常免疫的原因，因为它们完全忽略了它是异常值，它们只关心哪个比其他东西更高。所以这是一个重要的概念。它不仅出现在随机森林中。它也出现在一些指标中。例如，ROC曲线下面积，你会经常遇到，ROC曲线下面积完全忽略了比例，只关心排序。当我们做树状图时，我们看到了另一种情况。斯皮尔曼相关是一种秩相关——只关心顺序，不关心比例。所以随机森林的许多美好之处之一是我们可以完全忽略许多这些统计分布问题。但是对于深度学习来说不行，因为在深度学习中，我们试图训练一个参数化模型。所以我们需要对数据进行归一化。如果不这样做，那么创建一个有效训练的网络将会更加困难。
- en: So we grab the mean and the standard deviation of our training data and subtract
    out the mean, divide by the standard deviation, and that gives us a mean of zero
    and standard deviation of one [[20:53](https://youtu.be/DzE0eSdy5Hk?t=1253)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们抓取我们训练数据的均值和标准差，减去均值，除以标准差，这给我们一个均值为零，标准差为一的结果。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now for our validation data, we need to use the standard deviation and mean
    from the training data. We have to normalize it the same way. Just like categorical
    variables, we had to make sure they had the same indexes mapped to the same levels
    for a random forest. Or missing values, we had to make sure we have the same median
    used when we were replacing the missing values. You need to make sure anything
    you do in the training set, you do exactly the same thing in the test and validation
    set. So here, I’m subtracting out the training set mean and dividing by the training
    set standard deviation, so this is not exactly zero and one, but it’s pretty close.
    So in general, if you find you try something on a validation set or a test set
    and it’s much much much worse, than your training set, that’s probably because
    you normalized in an inconsistent way or encoded categories in an inconsistent
    way or something like that.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于我们的验证数据，我们需要使用训练数据的标准差和均值。我们必须以相同的方式对其进行标准化。就像分类变量一样，我们必须确保它们的相同索引映射到随机森林中的相同级别。或者缺失值，我们必须确保在替换缺失值时使用相同的中位数。你需要确保你在训练集中做的任何事情，在测试和验证集中都要完全相同。所以在这里，我减去了训练集的均值并除以训练集的标准差，所以这不是完全是零和一，但它非常接近。总的来说，如果你发现你在验证集或测试集上尝试某些东西，而它比你的训练集差得多得多，那可能是因为你以不一致的方式进行了标准化或编码类别或其他一些不一致的方式。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Looking at the data [[22:03](https://youtu.be/DzE0eSdy5Hk?t=1323)]
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看数据[[22:03](https://youtu.be/DzE0eSdy5Hk?t=1323)]
- en: Let’s take a look at some of this data. So we’ve got 10,000 images in the validation
    set and each one is a rank one tensor of length 784.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这些数据。所以我们在验证集中有10,000张图像，每张图像都是长度为784的秩为1的张量。
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In order to display it, I want to turn it into a rank 2 tensor of 28 by 28\.
    Numpy has a reshape function that takes a tensor in and reshapes it to whatever
    size tensor you request. Now if you think about it, you only need to tell it about
    if there are *D* axes, you only need to tell it about *D-1* of the axes you want
    because the last one, it can figure out for itself. So in total, there are 10,000
    by 784 numbers here altogether. so if you way I want my last axes to be 28 by
    28, then you can figure out that (the first axis) this must be 10,000 otherwise
    it’s not going to fit. So if you put -1, it says make it as big or as small as
    you have to to make it fit. So you can see here, it figured out that it has to
    be 10,000\. You’ll see this used in neural net software pre-processing and stuff
    like that all the time. I could have written 10,000 here, but I try to get into
    a habit of like anytime I’m referring to how many items in my input, I tend to
    use -1 because it means later on I could use a subsample, this code wouldn’t break.
    I could do some kind of stratified sampling if it was unbalanced, this code wouldn’t
    break. So by using this kind of approach of saying -1 here for the size, it just
    makes it more resilient to changes later. It’s a good habit to get into.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了显示它，我想将它转换为一个28x28的秩为2的张量。Numpy有一个reshape函数，它接受一个张量并将其重塑为你请求的任何大小的张量。现在如果你考虑一下，你只需要告诉它有*D*个轴，你只需要告诉它你想要的*D-1*个轴，因为最后一个，它可以自己算出来。所以总共，这里一共有10,000乘以784个数字。所以如果你说我希望我的最后一个轴是28x28，那么你可以算出（第一个轴）这必须是10,000，否则它就不会适合。所以如果你放-1，它会说让它尽可能大或尽可能小以使其适合。所以你可以看到，它算出来必须是10,000。你会看到这种方法在神经网络软件的预处理中经常使用。我可以在这里写10,000，但我试图养成一种习惯，就是每当我提到输入中有多少项时，我倾向于使用-1，因为这意味着以后我可以使用子样本，这段代码不会出错。如果它是不平衡的，我可以进行一些分层抽样，这段代码不会出错。所以通过在这里使用-1这种大小，它使得以后的更改更具弹性。这是一个很好的习惯。
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This idea of being able to take tensors and reshape them and change axes around
    and stuff like that is something you need to be able to totally do without thinking
    [[23:56](https://youtu.be/DzE0eSdy5Hk?t=1436)]. Because it’s going to happen all
    the time. So for example, here is one. I tried to read in some images, they were
    flattened, I need to unflatten them into a bunch of matrices — okay, reshape.
    bang. I read some images in with OpenCV and it turns out OpenCV orders the channels
    blue green red, everything else expects them to be red green blue. I need to reverse
    the last axes. How do you do that? I read in some images with Python imaging library.
    It reads them as rows by columns by channels, PyTorch expects channels by rows
    by columns. How do I transform that. So these are all things you need to be able
    to do without thinking, like straightaway. Because it happens all the time and
    you never want to be sitting there thinking about it for ages. So make sure you
    spend a lot of time over the week just practicing with things like all the stuff
    you are going to see today: reshaping, slicing, reordering dimensions, stuff like
    that. So the best way is to create some small tensors yourself and start thinking
    like okay what shall I experiment with.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 能够取张量并重新塑形、改变轴线等等的想法是你需要能够完全不假思索地做到的[[23:56](https://youtu.be/DzE0eSdy5Hk?t=1436)]。因为这种情况会经常发生。比如，这里有一个例子。我尝试读取一些图像，它们是扁平化的，我需要将它们重新塑形成一堆矩阵——好的，重新塑形。我用OpenCV读取了一些图像，结果发现OpenCV按照蓝绿红的顺序排列通道，其他所有的都希望它们是红绿蓝的。我需要颠倒最后一个轴。如何做到这一点？我用Python图像库读取了一些图像。它将它们读取为行、列、通道，PyTorch希望通道、行、列。我该如何转换。所以这些都是你需要能够不假思索地做到的事情，就像立刻就能做到。因为这种情况经常发生，你绝不想坐在那里想了很久。所以确保你在这一周花很多时间练习今天你将看到的所有东西：重新塑形、切片、重新排序维度等等。所以最好的方法是自己创建一些小张量，开始思考，比如我应该尝试什么。
- en: '**Question**: Back in normalize, you said many machine learning algorithms
    behave better when the data is normalized, but you also just said scales don’t
    really matter [[25:26](https://youtu.be/DzE0eSdy5Hk?t=1526)]? I said it doesn’t
    matter for random forests. So random forests are just going to spit things based
    on order and so we love them. We love random forests for the way they are so immune
    to worrying about distributional assumptions. But we are not doing random forests.
    We are doing deep learning. And deep learning does care.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在归一化时，您说许多机器学习算法在数据归一化时表现更好，但您也刚刚说过尺度并不重要？我说对于随机森林来说并不重要。因此，随机森林只会根据顺序输出结果，所以我们喜欢它们。我们喜欢随机森林是因为它们对分布假设不太担心。但我们现在不是在做随机森林，我们在做深度学习。而深度学习确实会在乎尺度。
- en: '**Question**: If we have parametric, then we should scale. If we have non-parametric,
    we shouldn’t have to scale [[26:06](https://youtu.be/DzE0eSdy5Hk?t=1566)]? No
    not quite. Because like k-nearest neighbors is nonparametric and scale matters
    heck of a lot, so I would say things involving trees generally it just going to
    split at a point and so probably you don’t care about scale but you probably just
    need to think like is this an algorithm that uses order or does it use specific
    numbers.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：如果我们有参数化，那么我们应该进行尺度调整。如果我们有非参数化，我们就不需要进行尺度调整？不完全是这样。因为像k最近邻是非参数化的，尺度很重要，所以我会说涉及树的事情通常只会在某个点进行分割，所以你可能不在乎尺度，但你可能需要考虑这是一个使用顺序还是使用具体数字的算法。
- en: '**Question**: Can you give us an intuition of why it needs scale just because
    that may clarify some of the issues [[26:38](https://youtu.be/DzE0eSdy5Hk?t=1598)]?
    Not until we get to doing SGD, so we are going to get to that. So for now, we’re
    just going to say take my word for it.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：您能直观解释一下为什么它需要尺度吗，因为这可能会澄清一些问题？直到我们开始进行随机梯度下降时才需要，所以我们会讲到那一点。所以现在，我们只能说相信我的话。
- en: '**Question**: Can you explain a little bit more what you mean by scale? Because
    when I think of scale, I think all the numbers should be generally the same size.
    Is that the case with the cats and dogs that we over with the deep learning like
    you could have a small cat and a larger cat but it would still know what those
    are both cats [[26:54](https://youtu.be/DzE0eSdy5Hk?t=1614)]? I guess this is
    one of those problems where language gets overloaded. So in computer vision, when
    we scale an image, we are actually increasing the size of the cat. In this case,
    we are scaling the actual pixel values. So in both case, scaling means to make
    something bigger and smaller. In this case, we are taking the numbers from naught
    to 255 and making them so that they have an average of zero and a standard deviation
    of one.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：您能解释一下尺度是什么意思吗？因为当我想到尺度时，我认为所有数字应该大致相同大小。在我们进行深度学习时，猫和狗的情况是这样的，你可能有一只小猫和一只大猫，但它仍然知道它们都是猫？我想这是语言被重载的问题之一。在计算机视觉中，当我们对图像进行缩放时，实际上是增加了猫的大小。在这种情况下，我们正在缩放实际的像素值。因此，在这两种情况下，缩放意味着使某物变大和变小。在这种情况下，我们将数字从零到255，并使它们的平均值为零，标准差为一。
- en: '**Question**: Could you explain us is it by column? by row? In general when
    you are scaling, just not thinking about a picture but kind of input to machine
    learning [[27:43](https://youtu.be/DzE0eSdy5Hk?t=1663)]. Okay, sure. I mean it’s
    a little bit subtle, but in this case, I’ve just got a single mean and a single
    standard deviation. So it’s basically on average, how much black is there. So
    on average, we have a mean and a standard deviation across all the pixels. In
    computer vision, we would normally do it by channel, so we would normally have
    one number for red, one number for green, one number for blue. In general, you
    need a different set of normalization coefficients for each thing you would expect
    to behave differently. So if we were doing like a structured dataset where we’ve
    got like income, distance in kilometers, and a number of children, you need three
    separate normalization coefficients for those as they are very different kinds
    of things. So it’s a bit domain-specific here. In this case, all of the pixels
    are levels of gray so we just got a single scaling number. Where else you could
    imagine if they were red vs. green vs. blue, you would need to scale those channels
    in different ways.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：您能解释一下是按列还是按行？一般来说，当您进行缩放时，不仅仅考虑图像，而是输入到机器学习的内容。好的，当然。这有点微妙，但在这种情况下，我只有一个平均值和一个标准差。所以基本上，平均有多少黑色。因此，平均而言，我们有一个平均值和一个标准差跨越所有像素。在计算机视觉中，我们通常会按通道进行操作，所以通常会有一个数字代表红色，一个数字代表绿色，一个数字代表蓝色。一般来说，您需要为每个您希望表现不同的事物准备不同的归一化系数。因此，如果我们像处理一个结构化数据集，其中包括收入、公里数和孩子数量，您需要为这些事物准备三个单独的归一化系数，因为它们是非常不同的事物。因此，在这里有点领域特定。在这种情况下，所有像素都是灰度级别，因此我们只有一个缩放数字。而在其他情况下，如果它们是红色与绿色与蓝色，您需要以不同方式缩放这些通道。
- en: '**Question**: So I’m having a little bit of trouble imagining what would happen
    if you don’t normalize in this case [[29:19](https://youtu.be/DzE0eSdy5Hk?t=1759)].
    We’ll get there. So this is kind of what Yannet was saying like why do we normalize
    and for now, we are normalizing because I say we have to. When we get to looking
    at stochastic gradient descent, we’ll basically discover that if you… Basically
    to skip ahead a little bit, we are going to be doing a matrix multiply by a bunch
    of weights. We are going to pick those weights in such a way that when we do the
    matrix multiply, we are going to try to keep the number at the same scale that
    they started out as. And that’s going to basically require the initial numbers
    we are going to have to know what their scale is. So basically it’s much easier
    to create a single neural network architecture that works for lots of different
    kinds of inputs if we know that they are consistently going to be mean zero standard
    deviation one. That would be the short answer. But we’ll learn a lot more about
    it and if in a couple of lessons you are still not quite sure why, let’s come
    back to it because it’s a really interesting thing to talk about.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所以我有点难以想象如果在这种情况下不进行归一化会发生什么。我们会讨论到那里。这就是Yannet所说的为什么我们要归一化的原因，目前我们正在归一化是因为我说我们必须这样做。当我们开始研究随机梯度下降时，我们基本上会发现，如果你...基本上为了稍微提前一点，我们将会通过一堆权重进行矩阵乘法。我们将以这样一种方式选择这些权重，以便当我们进行矩阵乘法时，我们将尝试保持数字与它们最初的规模相同。这基本上需要我们知道初始数字的规模。因此，如果我们知道它们一直是均值为零，标准差为一，那么就更容易为许多不同类型的输入创建一个单一的神经网络架构。这将是简短的答案。但我们将学到更多关于它的知识，如果在几节课后你仍然不太明白为什么，让我们回过头来讨论，因为这是一个非常有趣的话题。
- en: '**Question**: I’m trying to visualize the axes we’re working with here. So
    under plots, when you write `x _valid.shape`, we get 10,000 by 784\. Does that
    mean that we brought in 10,000 pictures of that dimension [[30:27](https://youtu.be/DzE0eSdy5Hk?t=1827)]?
    Yes, exactly. **Question continued**: In the next line, when you choose to reshape
    it, is there a reason why you put 28, 28 as Y or Z coordinates? Or is there a
    reason why they’re in that order?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我试图可视化我们正在使用的坐标轴。所以在绘图中，当你写`x_valid.shape`时，我们得到了10,000乘以784。这意味着我们带入了那个维度的10,000张图片吗？是的，确切地说。问题继续：在下一行，当你选择重塑时，为什么将28、28作为Y或Z坐标？或者它们按照那个顺序有什么原因吗？
- en: '![](../Images/eea87430f7a5c1a002310787f239edea.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eea87430f7a5c1a002310787f239edea.png)'
- en: Yes, there is. Pretty much all neural network libraries assume that the first
    axis is kind of equivalent of a row. It’s like a separate thing, it’s a sentence
    or an image or example of sales or whatever. So I want each image to be as separate
    item of the first axis. Then so that leaves two more axes for the rows and columns
    of the images. And that’s totally standard. I don’t think I’ve ever seen a library
    that doesn’t work that way.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，有的。几乎所有的神经网络库都假设第一个轴相当于一行。就像一个单独的东西，它是一个句子或一幅图像或销售示例。所以我希望每个图像都是第一个轴的单独项目。然后，这样就为图像的行和列留下了另外两个轴。这是完全标准的。我认为我从来没有见过一个不是这样工作的库。
- en: '**Question**: While normalizing the validation data, I saw you have used mean
    of x and standard deviation of x data (i.e. training data). Shouldn’t we use mean
    and standard deviation of validation data [[31:37](https://youtu.be/DzE0eSdy5Hk?t=1897)]?
    No, because you see, then you would be normalizing the validation set using different
    numbers and so now the meaning of this pixel has a value of 3 in the validation
    set has a different meaning to the meaning of 3 in the training set. It would
    be like if we had days of the week encoded such that Monday was a 1 in the training
    set and was a 0 in the validation set. We’ve got now two different sets where
    the same number has a different meaning.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在归一化验证数据时，我看到你使用了x数据的均值和标准差（即训练数据）。我们不应该使用验证数据的均值和标准差吗？不，因为你看，那样的话，你将使用不同的数字对验证集进行归一化，因此现在这个像素的值在验证集中的含义与在训练集中的含义不同。这就好像如果我们将一周的天数编码，使得星期一在训练集中是1，在验证集中是0。现在我们有了两组不同的数据，其中相同的数字具有不同的含义。
- en: Let me give an example. Let’s say we were doing full color image and our training
    set contained like green frogs, green snakes and gray elephants. We’re training
    to figure out which was which. Now we normalized using the each channel mean.
    Then we have a validation set and a test set which are just green frogs and green
    snakes. If we would have normalized by the validation sets statistics, we would
    end up saying things on average are green. So we would remove all the greenness
    out and so we would now fail to recognize the green frogs and the green snakes
    effectively. So we actually want to use the same normalization coefficients that
    we were training on. For those of you doing the deep learning class, we actually
    go further than that. When we use a pre-trained network, we have to use the same
    normalization coefficients that the original authors trained on. So the idea is
    that a number needs to have this consistent meaning across every dataset where
    you use it. This means when you are looking at the test set, you normalize the
    test set based on the training set mean and standard deviation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我举个例子。假设我们正在处理全彩色图像，我们的训练集包含绿色青蛙、绿色蛇和灰色大象。我们正在训练以弄清楚它们各自是什么。然后我们使用每个通道的均值进行了归一化。然后我们有一个验证集和一个测试集，里面只有绿色青蛙和绿色蛇。如果我们使用验证集的统计数据进行归一化，我们最终会说平均而言都是绿色。所以我们会去除所有的绿色，因此我们现在将无法有效地识别绿色青蛙和绿色蛇。所以我们实际上希望使用我们训练时使用的相同的归一化系数。对于那些正在学习深度学习课程的人，我们实际上做得更多。当我们使用预训练的网络时，我们必须使用原始作者训练时使用的相同的归一化系数。因此，这个数字在你使用它的每个数据集中都需要有一致的含义。这意味着当你查看测试集时，你需要根据训练集的均值和标准差对测试集进行归一化。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/28111eb58a2b5f03eab5e2878d4ea4e9.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28111eb58a2b5f03eab5e2878d4ea4e9.png)'
- en: So validation y values are just rank one tensor of 10,000 [[34:03](https://youtu.be/DzE0eSdy5Hk?t=2043)].
    Remember this is kind of weird Python thing where a tuple with this one thing
    in it needs a trailing comma. So this is a rank 1 tensor of length 10,000.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以验证y值只是一个长度为10,000的秩为1的张量。记住这是一种奇怪的Python事情，一个包含这一个东西的元组需要一个尾随逗号。所以这是一个长度为10,000的秩为1的张量。
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So here is an example of something from that. It’s just a number 3\. So that’s
    our labels.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子。只是一个数字3。这就是我们的标签。
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Slicing [[34:28](https://youtu.be/DzE0eSdy5Hk?t=2068)]
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切片
- en: So here is another thing you need to be able to do in your sleep. Slicing into
    a tensor. In this case, we’re slicing into the first axis with 0, so that means
    we’re grabbing the first slice. Because this is a single number, this is going
    to reduce the rank of the tensor by one. It’s going to turn it from a 3 dimensional
    tensor into a 2 dimensional tensor. So you can see here, this is now just a matrix.
    And then we are going to grab 10 through 14 inclusive rows, 10 through 14 inclusive
    columns, and here it is. So this is the kind of thing you need to be super comfortable
    — grabbing pieces out, looking at the numbers, and looking at the picture.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一件你需要能够做到熟练的事情。切片成一个张量。在这种情况下，我们用0切片到第一个轴，这意味着我们正在获取第一个切片。因为这是一个单一数字，这将减少张量的秩一次。它将把一个3维张量变成一个2维张量。所以你可以看到，这现在只是一个矩阵。然后我们将抓取10到14行，10到14列，这就是它。所以这是你需要非常熟练的事情——抓取片段，查看数字，查看图片。
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So here is an example of a little piece of that first image. So you kind of
    want to get used to this idea that if you are working with something like pictures
    or audio, this is something your brain is really good at interpreting. So keep
    showing pictures of what you’re doing whenever you can. But also remember behind
    the scenes they are numbers, so if something is going weird, print out a few of
    the actual numbers. You might find somehow some of them have become infinity or
    they are all zero or whatever. So use this interactive environment to explore
    data as you go.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是第一张图像的一小部分的例子。所以你应该习惯这样的想法，如果你在处理像图片或音频这样的东西，这是你的大脑真的很擅长解释的东西。所以尽可能经常展示你正在做的事情的图片。但也记住在幕后它们是数字，所以如果出现奇怪的情况，打印出一些实际的数字。你可能会发现其中一些变成了无穷大，或者它们全都是零，或者其他什么。所以在探索数据时利用这个交互式环境。
- en: '**Question**: Just a quick semantic question. Why when it’s a tensor of rank
    3, is it stored as like XYZ instead of like to me, it would make more sense to
    store it as a list of 2D tensors [[35:56](https://youtu.be/DzE0eSdy5Hk?t=2156)]?
    It’s not stored as either. So let’s look at this as a 3D. So here is a 3D. So
    a 3D tensor is formatted as showing a list of 2D tensors basically.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：只是一个快速的语义问题。为什么当它是一个秩为3的张量时，它被存储为XYZ而不是对我来说，将它存储为2D张量的列表会更有意义？它不是存储为任何一种。所以让我们把这看作是一个3D。这里是一个3D。所以一个3D张量被格式化为基本上显示一系列2D张量。
- en: '![](../Images/2f820b0c17fd39d9aeb1c5da76833725.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f820b0c17fd39d9aeb1c5da76833725.png)'
- en: '**Question**: But why isn’t it like `x_imgs[0][10:15][10:15]` ? Oh, because
    that has a different meaning. It’s kind of the difference between tensors and
    jagged arrays. So basically if you do something like `a[2][3]` , that says take
    the second list item and from it, grab the third list item. So we tend to use
    that when we have something called jagged array which is where each sub-array
    may be of a different length. Where else, we have a single object of three dimensions.
    So we are trying to say which little piece of it do we want. So the idea is that
    is a single slice object to go in and grab that piece out.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：但为什么不像`x_imgs[0][10:15][10:15]`那样？哦，因为那有不同的含义。这就是张量和不规则数组之间的区别。所以基本上如果你做类似`a[2][3]`这样的事情，那就是说取第二个列表项，然后从中获取第三个列表项。所以当我们有一个叫做不规则数组的东西时，我们倾向于使用这种方式，其中每个子数组的长度可能不同。而在其他情况下，我们有一个三维的单一对象。所以我们试图说我们想要它的哪一小部分。所以这个想法是一个单一的切片对象去抓取那一部分出来。
- en: '![](../Images/187aa61a9c4393575a69ce4f3e94be1d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/187aa61a9c4393575a69ce4f3e94be1d.png)'
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/01a922ceb1cf6362c9218cb68b51c486.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01a922ceb1cf6362c9218cb68b51c486.png)'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/63eb52d74552a92f3556b6f7cf6aca00.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63eb52d74552a92f3556b6f7cf6aca00.png)'
- en: So here is an example of a few of those images along with their labels [[37:33](https://youtu.be/DzE0eSdy5Hk?t=2230)].
    This kind of stuff, you want to be able to do pretty quickly with matplotlib.
    It’s going to help you a lot in life so you can have a look at what Rachel wrote
    here when she wrote `plots`. We can use add_subplot to basically create those
    little separate plots. And you need to know that `imshow` is how we basically
    take a numpy array and draw it as a picture. Then we’ve also added the title on
    top. So there it is.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些图像及其标签的例子。这种东西，你希望能够用matplotlib很快地完成。这将帮助你很多，这样你就可以看看Rachel在写`plots`时写的东西。我们可以使用add_subplot来创建这些小的独立图。你需要知道`imshow`是我们如何将一个numpy数组绘制成图片的。然后我们还添加了顶部的标题。所以就是这样。
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Neural Networks [[38:19](https://youtu.be/DzE0eSdy5Hk?t=2299)]
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: Let’s take that data and try to build a neural network with it. Sorry, this
    is going to be a lot of review for those of you already doing deep learning. A
    neural network is just a particular mathematical function or a class of mathematical
    functions but it’s a really important class because it has the property, it supports
    what’s called the universal approximation theorem. It means that a neural network
    can approximate any other function arbitrarily closely. So in other words, it
    can do, in theory, anything as long as we make it big enough. So this is very
    different to a function like 3*x* + 5 which can only do one thing — it’s a specific
    function. Or the class of functions *ax* + *b* which can only represent lines
    of different slopes moving it up and down different amounts. Or even the function
    *ax² + bx + c + sin d* again only can represent a very specific subset of relationships.
    The neural network, however, is a function that can represent any other function
    to arbitrarily close accuracy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拿这些数据来尝试构建一个神经网络。对于那些已经在进行深度学习的人来说，这将是很多复习。神经网络实际上只是一个特定的数学函数或数学函数类，但它是一个非常重要的类，因为它具有支持所谓的通用逼近定理的属性。这意味着神经网络可以任意接近地逼近任何其他函数。换句话说，理论上，只要我们使它足够大，它就可以做任何事情。这与只能执行一种特定功能的函数如3*x*
    + 5非常不同。或者只能表示不同斜率的线条上下移动不同量的函数类*ax* + *b*。甚至函数*ax² + bx + c + sin d*也只能表示一个非常具体的关系子集。然而，神经网络是一个可以任意接近地表示任何其他函数的函数。
- en: So what we are going to do is we are going to learn how to take a function,
    let’s take *ax + b*, and we are going to learn how to find its parameters (in
    this case *a* and *b*) which allows it to fit as closely as possible to a set
    of data. So this here is showing example from a notebook that we will be looking
    at in deep learning course which basically show what happens when we use something
    called stochastic gradient descent to try and set *a* and *b*. Basically what
    happens is we are going to pick a random *a* to start with, a random *b* to start
    with, then we are going to basically figure out do I need to increase or decrease
    *a* to make the line close to the dots? Do I need to increase or decrease *b*
    to make the line close to the dots? And then just keep increasing and decreasing
    *a* and *b* lots and lots of times. So that’s what we are going to do and to answer
    the question do I need to increase or decrease *a* and *b*, we are going to take
    the derivative. So the derivative of the function with respect *a* and *b* tells
    us how will that function change as we change *a* and *b*. So that’s basically
    what we’re going to do. But we are not going to start with just a line, the idea
    is we are to build up to actually having a neural net and so it’s going to be
    exactly the same idea but because it’s an infinitely flexible function, we are
    going to be able to use this exact same technique to fit to arbitrarily complex
    relationships. That’s basically the idea.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们要做的是学习如何取一个函数，比如*ax + b*，并学习如何找到其参数（在这种情况下为*a*和*b*），使其尽可能接近一组数据。这里展示了我们将在深度学习课程中查看的笔记本中的示例，基本上展示了当我们使用称为随机梯度下降来尝试设置*a*和*b*时会发生什么。基本上，我们将从一个随机的*a*和一个随机的*b*开始，然后我们基本上要弄清楚我是否需要增加或减少*a*来使线条接近点？我是否需要增加或减少*b*来使线条接近点？然后只需多次增加和减少*a*和*b*。这就是我们要做的，为了回答是否需要增加或减少*a*和*b*的问题，我们将取导数。因此，函数关于*a*和*b*的导数告诉我们当我们改变*a*和*b*时该函数将如何变化。这基本上就是我们要做的。但我们不会仅仅从一条线开始，想法是我们要逐步建立一个实际上具有神经网络的模型，因此这将是完全相同的想法，但由于它是一个无限灵活的函数，我们将能够使用这个完全相同的技术来适应任意复杂的关系。这基本上就是这个想法。
- en: '![](../Images/6f22748597af5433773d5ecfed422476.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f22748597af5433773d5ecfed422476.png)'
- en: Then what you need to know is that neural net is actually a very simple thing
    [[41:12](https://youtu.be/DzE0eSdy5Hk?t=2472)]. A neural net actually is something
    which takes as input, let’s say a vector, does a matrix product by that vector.
    So if the vector is size *r*, and the matrix is *r* by *c*, the matrix product
    will spit out something of size *c*. Then we do something called non-linearity
    which is basically we are going to throw away all the negative values (i.e. `max(0,
    x)`). And we are going to put that through another matrix multiply and then put
    that through another `max(0, x)`, and put that through another matrix multiply
    and so on until eventually we end up the single vector that we want. In other
    words, each stage of our neural network, the key thing going on is a matrix multiply,
    in other words, a linear function. So basically deep learning, most of their calculation
    is lots and lots of linear functions, but between each one we’re going to replace
    the negative numbers with zeros.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你需要知道的是，神经网络实际上是一件非常简单的事情。神经网络实际上是一个以输入为向量的东西，通过该向量进行矩阵乘积。因此，如果向量的大小为*r*，矩阵为*r*乘以*c*，则矩阵乘积将输出大小为*c*的结果。然后我们进行一种称为非线性的操作，基本上是我们要丢弃所有负值（即`max(0,
    x)`）。然后我们将通过另一个矩阵乘法，再通过另一个`max(0, x)`，再通过另一个矩阵乘法，直到最终得到我们想要的单个向量。换句话说，我们神经网络的每个阶段，关键的事情是进行矩阵乘法，换句话说，是一个线性函数。因此，基本上，深度学习中大部分的计算是大量的线性函数，但在每个线性函数之间，我们将用零替换负数。
- en: '![](../Images/1dd457d493aafbdb7d4695f3b068049f.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dd457d493aafbdb7d4695f3b068049f.png)'
- en: '**Question**: Why are we throwing away the negative numbers [[42:53](https://youtu.be/DzE0eSdy5Hk?t=2573)]?
    We will see. The short answer is if you apply a linear function to a linear function
    to a linear function, it’s still just a linear function. So it’s totally useless.
    But if you throw away the negatives, that’s actually a nonlinear transformation.
    So it turns out that if you apply a linear function to the thing we threw away
    the negatives, then apply that to a linear function that creates a neural network
    and it turns out that’s the thing that can approximate any other function arbitrarily
    closely. So this tiny little difference actually makes all the difference. And
    if you are interested in it, check out the deep learning video where we cover
    this because I actually show a nice visually intuitive proof, not something that
    I created, but something Michael Nielsen created. Or if you want to skip straight
    to his website, you could go to [Michael Nielsen universal approximation theorem](http://neuralnetworksanddeeplearning.com/chap4.html),
    he’s got a really nice walkthrough with lots of animations where you can see why
    this works.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么我们要丢弃负数？我们将看到。简短的答案是，如果你对一个线性函数应用另一个线性函数，它仍然只是一个线性函数。所以这完全没有用。但是如果你丢弃负数，那实际上是一个非线性转换。结果表明，如果你对我们丢弃的负数应用一个线性函数，然后将其应用于创建神经网络的线性函数，结果就是这个东西可以任意接近任何其他函数。所以这个微小的差异实际上产生了很大的不同。如果你对此感兴趣，请查看我们涵盖这一内容的深度学习视频，因为我实际上展示了一个直观的证明，不是我创造的，而是Michael
    Nielsen创造的。或者，如果你想直接跳转到他的网站，你可以访问Michael Nielsen的通用逼近定理，他有一个非常好的步骤指南，其中包含许多动画，您可以看到为什么这样运作。
- en: Why you (yes, you) should blog [[44:17](https://youtu.be/DzE0eSdy5Hk?t=2657)]
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么你（是的，你）应该写博客
- en: I feel like the hardest thing with getting started with technical writing on
    the internet is just like posting your first thing. In [this blog](/@racheltho/why-you-yes-you-should-blog-7d2544ac1045),
    Rachel actually says the top advice she would give to her younger self would be
    to start blogging sooner. And she has both reasons why you should do it, some
    examples of places she’s blogged has turned out to be great for her and her career,
    and some tips about how to get started.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得在互联网上开始技术写作最困难的事情就是发布你的第一篇文章。在这篇博客中，Rachel实际上说她给年轻自己的最好建议是尽早开始写博客。她列举了为什么你应该这样做的原因，她写博客的一些地方对她和她的职业都很有帮助，以及一些如何开始的建议。
- en: I remember when I first suggested to Rachel she might think about blogging because
    she had so much interesting to say and at first she was kind of surprised at the
    idea that she could blog. Now people come up to us at conferences and they’re
    like “you’re Rachel Thomas! I love your writing!!” So I’ve seen that transition
    from “wow could I blog?” to being known as a strong technical author. So check
    out this article if you still need convincing or if you are wondering how to get
    started. Since the first one is the first one is the hardest, maybe your first
    one should be something really easy for you to write. So it could be like here
    is a summary of the first 15 minutes of lesson 3 of our machine learning course
    — here is what’s interesting, here is what we learned. Or it could be like here
    is a summary of how I used a random forest to solve a particular problem in my
    practicum.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得当我第一次建议Rachel考虑写博客时，因为她有很多有趣的事情要说，起初她对自己能写博客这个想法感到有些惊讶。现在人们在会议上走过来对我们说：“你是Rachel
    Thomas！我喜欢你的文章！”所以我看到了从“哇，我能写博客吗？”到被认为是一位优秀的技术作者的过渡。所以如果你仍然需要说服，或者想知道如何开始，请查看这篇文章。因为第一篇是最难的，也许你的第一篇应该是对你来说非常容易写的东西。所以可以是这样的，这是我们机器学习课程第3课的前15分钟的摘要
    - 这是有趣的地方，这是我们学到的东西。或者可以是这样的，这是我如何使用随机森林解决实习中的特定问题的摘要。
- en: I often get questions like “oh my practicum, my organization, we’ve got sensitive
    commercial data” — that’s fine. Just find another dataset and do it on that instead
    to show the example, or anonymize all of the values and change the names of the
    variables or whatever. You can talk to your employer or your practicum partner
    to make sure that they are comfortable with whatever it is you’re writing. In
    general though, people love it when their interns blog about what they are working
    on because it makes them look super cool. It’s like “hey I’m an intern working
    at this company and I wrote this post about this cool analysis I did” and then
    other people would be like wow that looks like great company to work for. So generally
    speaking, you should find people are pretty supportive. Besides there’s lots and
    lots of datasets out there available so even if you can’t base it on the work
    you are doing, you can find something similar for sure.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常被问到“哦，我的实习，我的组织，我们有敏感的商业数据” - 没关系。只需找到另一个数据集，然后在那个数据集上进行操作以展示示例，或者对所有值进行匿名化并更改变量的名称等。您可以与雇主或实习合作伙伴交谈，以确保他们对您写的任何内容感到舒适。总的来说，人们喜欢他们的实习生写博客，讲述他们正在做的事情，因为这让他们看起来很酷。就像“嘿，我是在这家公司实习的，我写了这篇关于我所做的酷分析的文章”，然后其他人会说哇，这看起来是一家很棒的公司。一般来说，你会发现人们非常支持。此外，有很多数据集可用，所以即使您不能以您正在进行的工作为基础，您也肯定可以找到类似的东西。
- en: PyTorch [[47:15](https://youtu.be/DzE0eSdy5Hk?t=2835)]
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch
- en: We are going to start building our neural network. We are going to build it
    using something called a PyTorch. PyTorch is a library that basically looks a
    lot like numpy. But when you create some code with PyTorch, you can run it on
    the GPU rather than the CPU. So the GPU is something which is basically going
    to be probably at least an order of magnitude, possibly hundreds of times, faster
    than the code that you might write for the CPU for particularly stuff involving
    lots of linear algebra. So with deep learning, neural nets, if you don’t have
    a GPU you can do it on the CPU but it’s going to be frustratingly slow. Mac does
    not have a GPU that we can use for this because we need NVIDIA GPU. I would actually
    much prefer that we could use your Mac’s because competition is great. But NVIDIA
    was really the first one to create a GPU which did a good job of supporting General
    Purpose Graphics Programming Units (GPGPU) — in other words that means using a
    GPU for things other than playing computer games. They created a framework called
    CUDA. It’s a very good framework and pretty much universally used in deep learning.
    If you don’t have a NVIDIA GPU, you can’t use it and no current Mac has a NVIDIA
    GPU. Most laptops of any kind don’t have a NVIDIA GPU. If you are interested in
    doing deep learning on your laptop, the good news is that you need to buy one
    which is really good for playing computer games on. There is a place called [XOTIC
    PC Gaming Laptops](https://www.xoticpc.com/) where you can go and buy yourself
    a great laptop for doing deep learning. You can tell your parents that you need
    the money to do deep learning. You’ll generally find a whole bunch of laptops
    with names like predator and viper with pictures of robots and stuff. Anyway,
    having said that, I don’t know that many people that do much deep learning on
    their laptop. Most people will log into a cloud environment. By far the easiest
    I know of to use is called [Crestle](https://www.crestle.com/). With Crestle,
    you can basically sign up and straight away, the first thing you get is you get
    thrown straight into a jupyter notebook. It’s backed by a GPU, costs 60 cents
    an hour with all of the Fast AI libraries and data already available. So that
    makes life really easy. It’s less flexible and in some ways less fast than using
    AWS which is the Amazon Web Services option. It costs a little bit more, 90 cents
    an hour rather than 60 cents. But it’s very likely that your employer is already
    using that and it’s good to get to know anyway. They’ve got more different choices
    around GPUs and it’s a good choice. If you google for github student pack if you
    are a student, you can get $150 of credits straight away pretty much. So it’s
    a really good way to get started.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始构建我们的神经网络。我们将使用一个叫做PyTorch的东西来构建它。PyTorch是一个基本上看起来很像numpy的库。但是当你用PyTorch创建一些代码时，你可以在GPU上运行它而不是CPU。所以GPU基本上可能会比你为CPU编写的代码快至少一个数量级，可能是数百倍，特别是涉及大量线性代数的东西。所以在深度学习、神经网络中，如果你没有GPU，你可以在CPU上做，但会非常慢。Mac没有我们可以用于这个的GPU，因为我们需要NVIDIA
    GPU。我实际上更希望我们可以使用你的Mac，因为竞争是很好的。但NVIDIA确实是第一个创建支持通用图形编程单元（GPGPU）的GPU的公司，换句话说，这意味着使用GPU进行除了玩电脑游戏以外的事情。他们创建了一个叫做CUDA的框架。这是一个非常好的框架，在深度学习中几乎被普遍使用。如果你没有NVIDIA
    GPU，你就不能使用它，目前没有任何Mac有NVIDIA GPU。任何类型的大多数笔记本电脑都没有NVIDIA GPU。如果你有兴趣在笔记本电脑上进行深度学习，好消息是你需要购买一台非常适合玩电脑游戏的笔记本电脑。有一个地方叫做[XOTIC
    PC Gaming Laptops](https://www.xoticpc.com/)，你可以去那里购买一台适合进行深度学习的优秀笔记本电脑。你可以告诉你的父母，你需要这笔钱来进行深度学习。你通常会发现一大堆带有predator和viper等名称的笔记本电脑，上面有机器人和其他东西的图片。无论如何，话虽如此，我不认识很多人在笔记本电脑上做很多深度学习的。大多数人会登录到云环境中。我知道的最容易使用的是[Crestle](https://www.crestle.com/)。使用Crestle，你基本上可以注册，然后立即得到的第一件事就是你被直接投入到一个jupyter笔记本中。它支持GPU，每小时60美分，所有Fast
    AI库和数据都已经可用。这使得生活变得非常容易。它比使用亚马逊网络服务选项的AWS less灵活，某些方面也不那么快。它的成本稍微高一点，每小时90美分而不是60美分。但很可能你的雇主已经在使用它，了解一下也是好的。他们在GPU周围有更多不同的选择，这是一个不错的选择。如果你是学生，可以搜索github学生包，你可以立即获得150美元的信用额度。所以这是一个开始的好方法。
- en: '**Question**: I wanted to know your opinion on Intel recently published an
    open source way of boosting regular packages that they claim as equivalent to
    if you use the bottom tier GPU. On your CPU, if you use their boost packages,
    you can get the same performance [[51:13](https://youtu.be/DzE0eSdy5Hk?t=3073)].
    Actually Intel makes some great numerical programming libraries particularly this
    one called MKL, Matrix Kernel Library. They definitely make things faster than
    not using those libraries, but if you look at a graph of performance over time,
    GPUs have consistently throughout the last 10 years including now are about 10
    times more floating-point operations per second than equivalent CPU, and they
    are generally about 1/5 of the price for that performance. Because of that, everybody
    doing anything with deep learning basically does it on NVIDIA GPUs and therefore
    using anything other than NVIDIA GPU is currently very annoying — so slower, more
    expensive, more annoying. I really hope there will be more activity around AMD
    GPUs in particular in this area, but AMD’s got literally years of catching up
    to do, so it might take a while.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我想知道你对英特尔最近发布的一种提升常规软件包的开源方式的看法，他们声称这相当于使用底层GPU。在你的CPU上，如果你使用他们的提升软件包，你可以获得相同的性能。实际上，英特尔制作了一些很棒的数值编程库，特别是这个叫做MKL的库，矩阵核心库。它们确实比不使用这些库更快，但如果你看一下性能随时间变化的图表，GPU在过去10年中一直保持着大约每秒10次浮点运算，现在也是如此，而且通常价格只有相同性能的CPU的1/5。因此，几乎所有进行深度学习的人基本上都是在NVIDIA
    GPU上进行的，因此使用除了NVIDIA GPU之外的任何东西目前都非常烦人——更慢、更昂贵、更烦人。我真的希望在这个领域尤其是在AMD GPU周围会有更多的活动，但AMD确实需要多年的追赶，所以可能需要一段时间。'
- en: '**Comment**: I just wanted to point out that you can also buy a thing such
    as a GPU extender to a laptop that may be a first step solution before new laptop
    or AWS [[52:46](https://youtu.be/DzE0eSdy5Hk?t=3166)]. Yes, I think for like $300
    or so, you can buy something that plugs into your Thunderbolt port if you have
    a Mac and then for another $500 or $600, you can buy a GPU to plug into that.
    Having said that, for about $1000, you can actually create a pretty good GPU based
    desktop and so if you are considering that, Fast AI forums have lots of threads
    where people help each other spec out something at a particular price point.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**：我只是想指出，你也可以购买一个GPU扩展器连接到笔记本电脑，这可能是在购买新笔记本电脑或AWS之前的第一步解决方案[[52:46](https://youtu.be/DzE0eSdy5Hk?t=3166)]。是的，我认为大约$300左右，你可以购买一个插入到你的Thunderbolt端口的东西，如果你有一台Mac，然后再花$500或$600，你可以购买一个GPU插入其中。话虽如此，大约$1000，你就可以创建一个相当不错的基于GPU的台式机，所以如果你在考虑这个，Fast
    AI论坛有很多帖子，人们在特定价格点上互相帮助。'
- en: Anyway, to start with, I’d say use Crestle and then when you are ready to invest
    a few extra minutes getting going, use AWS. To use AWS, when you get there, go
    to EC2 [[53:52](https://youtu.be/DzE0eSdy5Hk?t=3232)]. There’s lots of stuff on
    AWS, and EC2 is the bit where we get to rent computers by the hour.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我建议一开始使用Crestle，然后当你准备好投入一些额外的时间时，使用AWS。要使用AWS，当你到达那里时，去EC2。AWS上有很多东西，EC2是我们可以按小时租用计算机的部分。
- en: '![](../Images/85c7e933f065d84beb55e789dcc3db87.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85c7e933f065d84beb55e789dcc3db87.png)'
- en: Now, we are gonna need a GPU based instance. Unfortunately when you first sign
    up for AWS, they don’t give you access to them. So go to Limits (up in the top
    left).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要一个基于GPU的实例。不幸的是，当你第一次注册AWS时，他们不会给你访问权限。所以去到Limits（左上角）。
- en: '![](../Images/608a596e781dd0e4034342c986a78974.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/608a596e781dd0e4034342c986a78974.png)'
- en: And the main GPU instance we’ll be using is called the p2\. So scroll down to
    p2.xlarge, you need to make sure that number is not zero. If you’ve just got a
    new account, it probably is zero which means you won’t be allowed to create one.
    So you have to go “Request limit increase” and the trick there is when it asks
    you why you want the limit increase, type “fast.ai” because AWS knows to look
    out and they know that fast.ai people are good people so they’ll do it quite quickly.
    That takes a day or two generally speaking to go through.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的主要GPU实例称为p2。所以滚动到p2.xlarge，你需要确保数字不是零。如果你刚刚注册了一个新账户，它可能是零，这意味着你将无法创建一个。所以你必须去“请求限制增加”，其中的诀窍是当它问你为什么要增加限制时，输入“fast.ai”，因为AWS知道要留意，他们知道fast.ai的人是好人，所以他们会很快处理。通常需要一两天的时间。
- en: '![](../Images/2fb948c88cc1415fd43666d3da22c491.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fb948c88cc1415fd43666d3da22c491.png)'
- en: 'So once you get the email saying you’ve been approved for p2 instances, you
    can then go back here and say Launch Instance:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一旦你收到批准使用p2实例的邮件，你就可以回到这里并点击Launch Instance：
- en: '![](../Images/7a2a1cecc7a4dea502a3cf9c65707620.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a2a1cecc7a4dea502a3cf9c65707620.png)'
- en: We’ve basically set up one that has everything you need. So if you click on
    Community AMIs and AMI is an Amazon Machine Image — it’s basically a completely
    set up computer. So if you type fastai (all one word), you’ll find here fastai
    DL part 1 v2 for p2\. So that’s all set up ready to go.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上设置了一个拥有一切你需要的东西。所以如果你点击Community AMIs，AMI是Amazon Machine Image的缩写——它基本上是一个完全设置好的计算机。所以如果你输入fastai（连在一起），你会在这里找到fastai
    DL part 1 v2 for p2。所以一切都准备就绪。
- en: '![](../Images/713991f594162edc2375980526a36dd0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/713991f594162edc2375980526a36dd0.png)'
- en: So if you click on Select [[55:34](https://youtu.be/DzE0eSdy5Hk?t=3334)], it’ll
    say what kind of computer do you want. So we have to say I want a “GPU compute”
    type and specifically I want p2.xlarge. And you can say “Review and Launch”.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你点击Select，它会问你想要什么样的计算机。所以我们必须说我想要一个“GPU计算”类型，具体来说我想要p2.xlarge。然后你可以点击“Review
    and Launch”。
- en: '![](../Images/ff36f38789f7c7b31e4487a3e27bfe7d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff36f38789f7c7b31e4487a3e27bfe7d.png)'
- en: I’m assuming you already know how to deal with SSH keys and all that kind of
    stuff. If you don’t, check out the introductory tutorials and work shop videos
    that we have online, or google around for SSH keys. Very important skill to know
    anyway. So hopefully you get through all that, you have something running on a
    GPU with the Fast AI repo. If you use Crestle, just `cd fastai2` the repo is already
    there, `git pull`. AWS, `cd fastai`, the repo is already there, `git pull`. If
    it’s your own computer, you’ll just have to `git clone` and then away you go.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你已经知道如何处理SSH密钥和所有这些东西。如果你不知道，可以查看我们在线的入门教程和工作坊视频，或者在网上搜索SSH密钥。这是一个非常重要的技能。所以希望你通过所有这些，你可以在GPU上运行Fast
    AI repo。如果你使用Crestle，只需`cd fastai2`，repo已经在那里，`git pull`。AWS，`cd fastai`，repo已经在那里，`git
    pull`。如果是你自己的电脑，你只需要`git clone`，然后就可以开始了。
- en: PyTorch is pre-installed, so PyTorch basically means we can write code that
    looks a lot like numpy but it’s going to run really quickly on the GPU. Secondly,
    since we need to know like which direction and how much to move our parameters
    to improve our loss, we need to know the derivative of functions. PyTorch has
    this amazing thing where any code you write using PyTorch library, it can automatically
    take the derivative of that for you. So we are not going to look at any calculus
    in this course. And I don’t look at any calculus in any of my courses or at any
    of my work basically ever in terms of actually calculating derivatives myself
    because I’ve never had to. It’s done for me by the library. So as long as you
    write the Python code, the derivative is done. So the only calculus you really
    need to know to be an effective practitioner is what is it mean to be a derivative.
    And you also need to know the chain rule which we will come to.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是预安装的，所以PyTorch基本上意味着我们可以编写看起来很像numpy的代码，但在GPU上运行速度非常快。其次，由于我们需要知道参数如何移动以改善我们的损失，我们需要知道函数的导数。PyTorch有这个神奇的功能，任何你使用PyTorch库编写的代码，它都可以自动为你计算导数。所以我们在这门课程中不会涉及任何微积分。我在我的课程中从来没有看过微积分，也从来没有在我的工作中计算过导数，因为这些都是由库来完成的。只要你写好Python代码，导数就会被计算出来。所以成为一个有效的从业者，你真正需要了解的微积分只是导数是什么意思。你还需要知道链式法则，我们会讲到。
- en: Neural Net for Logistic Regression in PyTorch [[57:45](https://youtu.be/DzE0eSdy5Hk?t=3465)]
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的逻辑回归神经网络[[57:45](https://youtu.be/DzE0eSdy5Hk?t=3465)]
- en: Alright, so we are going to start out kind of top-down, create a neural net,
    and we’re going to assume a whole bunch of stuff. And gradually we are going to
    dig into each piece. So to create neural nets, we need to import the PyTorch neural
    net library. PyTorch, funnily enough, is not called PyTorch — it’s called torch.
    So `torch.nn` is the PyTorch subsection that’s responsible for neural nets. We’ll
    call that nn. And we are going to import a few bits out of Fast AI just to make
    life a bit easier for us.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们将从上到下开始，创建一个神经网络，并假设很多东西。然后逐渐我们将深入研究每个部分。所以要创建神经网络，我们需要导入PyTorch神经网络库。有趣的是，PyTorch并不叫PyTorch——它叫torch。所以`torch.nn`是负责神经网络的PyTorch子部分。我们将称之为nn。我们将从Fast
    AI中导入一些部分，以使我们的生活变得更容易。
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So here is how you create a neural network in PyTorch. The simplest possible
    neural network, you say Sequential. And Sequential means I am now going to give
    you a list of the layers that I want in my neural network. So in this case, my
    list has two things in it. The first thing says I want a linear layer. Now a linear
    layer is something that’s basically going to do *y = ax + b* but matrix matrix
    multiply, not univariate obviously. So it’s going to do a matrix product basically.
    The input of the matrix product is going to be a vector of length 28 times 28
    because that’s how many pixels we have and the output needs to be of size 10 (we
    will talk about why in a moment). For now this is how we define a linear layer.
    Then again, we’re going to dig into this in detail but every linear layer just
    about in neural nets has to have a non-linearity after it. Then we are going to
    learn about this particular non-linearity in a moment, it’s called the softmax
    and if you’ve done the DL course, you’ve already seen this. So that’s how we define
    a neural net. This is a two layer neural net.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何在PyTorch中创建神经网络的。最简单的神经网络，你说Sequential。Sequential意味着我现在要给你一个我想要在我的神经网络中的层的列表。所以在这种情况下，我的列表中有两个东西。第一件事说我想要一个线性层。现在线性层基本上会执行*y
    = ax + b*，但是矩阵矩阵相乘，而不是单变量。所以它基本上会执行一个矩阵乘积。矩阵乘积的输入将是一个长度为28乘以28的向量，因为这是我们有多少像素，输出需要是大小为10（我们稍后会讨论原因）。目前这就是我们如何定义一个线性层。然后，我们将详细讨论这一点，但是几乎每个神经网络中的线性层之后都必须有一个非线性。然后我们将在一会儿学习这个特定的非线性，它被称为softmax，如果你已经学过深度学习课程，你已经见过这个。这就是我们如何定义一个神经网络。这是一个两层神经网络。
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: There is also kind of an implicit additional first layer which is the input,
    but with PyTorch, you don’t have to explicitly mention the input. But normally
    we think conceptually like the input image is kind of also a layer. Because we
    are doing things pretty manually, with PyTorch we are not taking advantage of
    any of the conveniences in Fast AI for building your stuff, we have to then write
    `.cuda()` which tells PyTorch to copy this neural network across to the GPU. So
    from now on, that network is going to be actually running on the GPU. If we didn’t
    say that, it would run on the CPU. So that gives us back a neural net — a very
    simple neural net.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种隐含的额外第一层，即输入层，但是使用PyTorch，你不必显式提及输入。但通常我们在概念上认为输入图像也是一种层。因为我们正在相当手动地进行操作，使用PyTorch我们没有利用Fast
    AI中构建你的东西的任何便利性，我们必须然后写`.cuda()`，这告诉PyTorch将这个神经网络复制到GPU上。从现在开始，该网络实际上将在GPU上运行。如果我们没有说，它将在CPU上运行。这给我们返回了一个神经网络——一个非常简单的神经网络。
- en: Data [[1:00:22](https://youtu.be/DzE0eSdy5Hk?t=3622)]
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据[[1:00:22](https://youtu.be/DzE0eSdy5Hk?t=3622)]
- en: 'We are then going to try and fit the neural net to some data. So we need some
    data. Fast AI has this concept of a ModelData object which is basically something
    that wraps up training data, validation data, and optionally test data. So to
    create a ModelData object, you can just say:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将尝试将神经网络拟合到一些数据上。所以我们需要一些数据。Fast AI有一个ModelData对象的概念，基本上是将训练数据、验证数据和可选的测试数据包装在一起的东西。所以要创建一个ModelData对象，你可以这样说：
- en: I want to create some image classifier data (`ImageClassifierData`)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我想创建一些图像分类器数据（`ImageClassifierData`）
- en: I’m going to grab it from some arrays (`from_arrays`)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将从一些数组中获取它（`from_arrays`）
- en: This is the path that I’m going to save any temporary files (`path`)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是我将保存任何临时文件的路径（`path`）
- en: This is my training data arrays (`(x, y)`)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是我的训练数据数组（`（x，y）`）
- en: This is my validation data arrays (`(x_valid, y_valid)`)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是我的验证数据数组（`（x_valid，y_valid）`）
- en: So that just returns an object that’s going to wrap that all up. So we are going
    to able to fit to that data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是返回一个将所有这些包装起来的对象。所以我们将能够拟合到这些数据上。
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that we have a neural net and some data, we are going to come back to this
    in a moment but we basically say what loss function do we want to use, what optimizer
    do we want to use, and then we say fit [[1:01:07](https://youtu.be/DzE0eSdy5Hk?t=3667)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个神经网络和一些数据，我们将在一会儿回到这里，但基本上我们说我们想使用什么损失函数，想使用什么优化器，然后说拟合[[1:01:07](https://youtu.be/DzE0eSdy5Hk?t=3667)]。
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We say fit this network `net` to this data `md` going over every image once
    (`n_epochs`) using this loss function `loss`, this optimizer `opt`, and print
    out these metrics `metrics`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说将这个网络`net`拟合到这个数据`md`上，每次遍历每个图像一次（`n_epochs`），使用这个损失函数`loss`，这个优化器`opt`，并打印出这些指标`metrics`。
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/683068e628e31f95304d12949a5acd1f.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/683068e628e31f95304d12949a5acd1f.png)'
- en: This says here this is 91.8% accurate. So that’s like the simplest possible
    neural net. What that’s doing is it’s creating a matrix multiplication, followed
    by a non-linearity, and it’s trying to find the values for this matrix (`nn.Linear(28*28,
    10)`) which basically that fit the data *as well as possible* that end up predicting
    this is a 1, this is a 9, this is a 3.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里说这是91.8%的准确率。所以这就是最简单的神经网络。它正在创建一个矩阵乘法，然后是一个非线性，它试图找到这个矩阵的值（`nn.Linear(28*28,
    10)`），基本上是尽可能好地拟合数据，最终预测这是1，这是9，这是3。
- en: Loss Function [[1:02:08](https://youtu.be/DzE0eSdy5Hk?t=3728)]
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数[[1:02:08](https://youtu.be/DzE0eSdy5Hk?t=3728)]
- en: So we need some definition for “as well as possible”. So the general term for
    that thing is called the loss function. So the loss function is the function that’s
    going to be lower if this is better. Just like with random forests, we had this
    concept of information gain, and we got to pick what function you want to use
    to define information gain and we were mainly looking at root mean square error.
    Most machine learning algorithms we call something very similar to that “loss”.
    So the loss is how do we score how good we are. So in the end, we are going to
    calculate the derivative of the loss with respect to the weight matrix that we
    are multiplying by to figure out how to update it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要一些关于“尽可能好”的定义。那个东西的一般术语叫做损失函数。所以损失函数是一个函数，如果这个函数更低，那么就更好。就像随机森林一样，我们有信息增益的概念，我们得选择用什么函数来定义信息增益，我们主要看的是均方根误差。大多数机器学习算法我们称之为类似于“损失”的东西。所以损失是我们如何评分我们有多好的东西。最终，我们将计算损失对我们正在乘以的权重矩阵的导数，以找出如何更新它。
- en: 'We are going to use something called Negative Log Likelihood Loss (`NLLLoss`).
    Negative log likelihood loss is also known as cross entropy — they are literally
    the same thing. There’s two versions, one called binary cross entropy or binary
    negative log likelihood, and another called categorical cross entropy. They are
    the same thing, one is for when you’ve only got a zero or one dependent, the other
    is if you’ve got like cat, dog, airplane, or horse, or 0, 1, through 9 and so
    forth. So what we got here is the binary version of cross entropy:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一种称为负对数似然损失（`NLLLoss`）的东西。负对数似然损失也被称为交叉熵，它们实际上是一样的。有两个版本，一个称为二元交叉熵或二元负对数似然，另一个称为分类交叉熵。它们是一样的，一个是当你只有一个零或一个依赖时，另一个是如果你有猫、狗、飞机或马，或者0、1、到9等等。所以这里我们有交叉熵的二元版本：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: So here `-(y * np.log(p) + (1-y)*np.log(1-p))` is the definition. I think maybe
    the easiest way to understand this definition is to look at an example [[1:03:35](https://youtu.be/DzE0eSdy5Hk?t=3815)].
    Let’s say we are trying to predict cat vs. dog. One is cat, zero is dog. So here,
    we’ve got cat, dog, dog, cat (`[1, 0, 0, 1]`). And here are our predictions (`[0.9,
    0.1, 0.2, 0.8]`). We said 90% sure it’s a cat, 90% sure it’s a dog, 80% sure it’s
    a dog, 80% sure it’s a cat. So we can then calculate the binary cross entropy
    by calling our function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的定义是`-(y * np.log(p) + (1-y)*np.log(1-p))`。我认为理解这个定义的最简单方法可能是看一个例子。假设我们试图预测猫和狗。1代表猫，0代表狗。所以这里，我们有猫、狗、狗、猫（`[1,
    0, 0, 1]`）。这是我们的预测（`[0.9, 0.1, 0.2, 0.8]`）。我们说90%确定是猫，90%确定是狗，80%确定是狗，80%确定是猫。所以我们可以通过调用我们的函数来计算二元交叉熵。
- en: For the first one, we have *y=1*, *p=0.9* (i.e. `(1 * np.log(0.9)` since the
    second term is skipped). For the second one, the first part is skipped (multiply
    by 0) and the second part will be `(1-0)*np.log(0.9)`. In other words, the first
    piece and the second piece of this are going to give exactly the same number which
    make sense because the first one we said we were 90% confident it was a cat and
    it was, and the second we said we were 90% confident it was a dog and it was.
    So in each case, the loss is coming from the fact that we could have been more
    confident. So if we said we were 100% confident, the loss would have been zero.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个，我们有*y=1*，*p=0.9*（即`(1 * np.log(0.9)`，因为第二项被跳过了）。对于第二个，第一部分被跳过（乘以0），第二部分将是`(1-0)*np.log(0.9)`。换句话说，这个的第一部分和第二部分将给出完全相同的数字，这是有道理的，因为第一个我们说我们对是猫90%有信心，而实际上是，第二个我们说我们对是狗90%有信心，而实际上是。所以在每种情况下，损失都来自于我们本可以更有信心。所以如果我们说我们100%有信心，损失将为零。
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So let’s look at that in Excel [[1:05:17](https://youtu.be/DzE0eSdy5Hk?t=3917)].
    From the top row:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们在Excel中看一下。从顶部行开始：
- en: our predictions
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的预测
- en: actual/target values
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际/目标值
- en: 1 minus actual/target values
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1减去实际/目标值
- en: log of our predictions
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的预测的对数
- en: log of 1 minus our predictions
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的预测的对数的1减
- en: sum
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总和
- en: '![](../Images/4257aee51f75d1ee43b881468c925dde.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4257aee51f75d1ee43b881468c925dde.png)'
- en: If you think about it, and I want you to think about this during the week, you
    could replace this (`np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))`) with an if
    statement rather than y, because y is always 1 or 0 then it’s only ever going
    to use either this `np.log(p)` or this `(np.log(1-p)`. So you could replace this
    with an if statement. So I’d like you, during the week, to try to rewrite this
    with an if statement.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细想一想，我希望你在这一周内考虑一下，你可以用一个if语句来替换这个(`np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))`)，而不是y，因为y总是1或0，所以它只会使用`np.log(p)`或`(np.log(1-p)`中的一个。所以你可以用一个if语句来替换这个。所以我希望你在这一周内尝试用一个if语句来重写这个。
- en: And then see if you can then scale it out to be a categorical cross entropy
    [[1:06:17](https://youtu.be/DzE0eSdy5Hk?t=3977)]. So categorical cross entropy
    works this way. Let’s say we were trying to predict 3, 6, 7, 2.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后看看你是否能将其扩展为分类交叉熵。所以分类交叉熵的工作方式是这样的。假设我们试图预测3、6、7、2。
- en: '![](../Images/918e173521c2e7a4679d82ca2e48c9bd.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/918e173521c2e7a4679d82ca2e48c9bd.png)'
- en: 'So if we were trying to predict 3 and we actually predicted 5, or try to predict
    3 and we accidentally predicted 9\. Being 5 instead of 3 is no better than being
    9 instead of 3\. So we are not actually going to say how far away is the actual
    number. We are going to express it differently. Or to put it another way, what
    if we were trying to predict cats, dogs, horses, and airplanes. How far away is
    cat from horse? So we are going to express these a little bit differently. Rather
    than thinking of it as a 3, let’s think of it as a vector with a 1 in the third
    location:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们试图预测3，而实际上预测了5，或者试图预测3，却意外地预测了9。5而不是3并不比9而不是3更好。所以我们实际上不会说实际数字有多远。我们会用不同的方式表达它。换句话说，如果我们试图预测猫、狗、马和飞机。猫和马之间有多远？所以我们会稍微不同地表达这些。与其把它看作是一个3，不如把它看作是一个在第三个位置上有一个1的向量：
- en: '![](../Images/e953942ce47f597d2a6e18eadb744c5c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e953942ce47f597d2a6e18eadb744c5c.png)'
- en: Rather than thinking it as a 6, let’s think of it as a vector of zeros for the
    one in the 6th location. So in other words, one-hot-encoding. So let’s one hot
    encode our dependent variable. So that way now, rather than trying to predict
    a single number, let’s predict ten numbers. Let’s predict what’s the probability
    that it’s a 0, what’s the probability it’s a 1, and so forth.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 不要把它看作是一个6，让我们把它看作是一个零向量，第6个位置是1。换句话说，独热编码。所以让我们对我们的因变量进行独热编码。这样现在，我们不再试图预测一个数字，而是预测十个数字。让我们预测它是0的概率，它是1的概率，依此类推。
- en: '![](../Images/4dce003a5190e7c35eaf848bbfeccf81.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dce003a5190e7c35eaf848bbfeccf81.png)'
- en: So let’s say we are trying to predict the 2, then here is our categorical cross
    entropy [[1:07:50](https://youtu.be/DzE0eSdy5Hk?t=4070)]. So it’s just saying
    okay did this one predict correctly or not, how far off was it, and so forth for
    each one, and add them all up. So categorical cross entropy is identical to binary
    cross entropy. We just have to add it up across all of the categories.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们假设我们正在预测2，这里是我们的分类交叉熵[[1:07:50](https://youtu.be/DzE0eSdy5Hk?t=4070)]。所以它只是在说，这个预测是否正确，有多大偏差，依此类推，对每一个进行计算，然后将它们全部加起来。分类交叉熵与二元交叉熵是相同的。我们只需要将它们加起来跨越所有的类别。
- en: '![](../Images/c66951412eb40ab3d6245168c32364ec.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c66951412eb40ab3d6245168c32364ec.png)'
- en: So try and turn the binary cross entropy function in Python into a categorical
    cross entropy in Python. Maybe create both the version with the if statement and
    the version with the sum and the product.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 所以尝试将Python中的二元交叉熵函数转换为Python中的分类交叉熵。也许创建带有if语句的版本和带有求和和乘积的版本。
- en: So that’s why in our PyTorch, we had 10 as the output dimensionality for this
    matrix because when we multiply a matrix with 10 columns, we are going to end
    up with something of length 10 which is what we want [[1:08:35](https://youtu.be/DzE0eSdy5Hk?t=4115)].
    We want to have 10 predictions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在我们的PyTorch中，我们将这个矩阵的输出维度设置为10，因为当我们将一个有10列的矩阵相乘时，我们将得到一个长度为10的结果，这正是我们想要的[[1:08:35](https://youtu.be/DzE0eSdy5Hk?t=4115)]。我们想要有10个预测。
- en: '![](../Images/248a380a51674fd3951fe989cd53e82a.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/248a380a51674fd3951fe989cd53e82a.png)'
- en: So that’s the loss function that we are using. Then we can fit the model and
    what it does is it goes through every image, this many times (`epochs`). So in
    this case it’s just looking at every image once, and going to slightly update
    the values in that weight matrix based on those gradients.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们正在使用的损失函数。然后我们可以拟合模型，它会遍历每个图像，这么多次（`epochs`）。所以在这种情况下，它只是查看每个图像一次，并且会根据这些梯度稍微更新那个权重矩阵中的值。
- en: '![](../Images/abdffccd094662b0136d77886cf8668e.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abdffccd094662b0136d77886cf8668e.png)'
- en: So once we’ve trained it, we can then say `predict` using this model (`net`)on
    the validation set (`md.val_dl`).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一旦我们训练好了，我们就可以用这个模型(`net`)在验证集(`md.val_dl`)上进行`predict`。
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that spits out something of 10,000 by 10\. We have 10,000 images we are
    validating on, and we actually make 10 predictions per image. In other words,
    each one of these row is the probabilities that it’s a 0, it’s a 1, it’s a 2,
    and so forth.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这会输出一个10,000乘以10的东西。我们有10,000张图像进行验证，实际上每张图像进行10次预测。换句话说，每一行都是它是0的概率，它是1的概率，它是2的概率，依此类推。
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Argmax [[1:10:22](https://youtu.be/DzE0eSdy5Hk?t=4222)]
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Argmax [[1:10:22](https://youtu.be/DzE0eSdy5Hk?t=4222)]
- en: 'In math, there’s a really common operation we do called `argmax`. When I say
    it’s common, it’s funny like at high school, I never saw argmax. First year undergrad,
    I never saw argmax. But somehow after university, everything’s about argmax. So
    one of those things that’s for some reason not really taught at school but it
    actually turns out to be super critical. So argmax is both something that you’ll
    see in math (it’s just written out in full “argmax”), it’s in numpy, it’s in PyTorch,
    it’s super important. What it does is it says let’s take this array of predictions,
    and let’s figure out on a given axis (`axis=1` — remember, axis 1 is columns),
    so as Chis said for 10 predictions for each row, let’s find which prediction has
    the highest value and return not that (if it just said max, it would return the
    value) argmax returns the index of the value. So by saying `argmax(axis=1)`, it’s
    going to return the index which is actually the number itself. So let’s grab the
    first 5:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，有一个我们经常做的操作叫做`argmax`。当我说它很常见时，很有趣的是在高中，我从来没有见过argmax。大一，我也从来没有见过argmax。但不知何故，大学毕业后，一切都与argmax有关。所以有些事情在学校里似乎并没有真正教，但实际上它非常关键。argmax既是数学中的一个东西（它只是完整地写出“argmax”），它在numpy中，在PyTorch中，非常重要。它的作用是让我们拿这些预测数组，然后在给定的轴上（`axis=1`
    - 记住，轴1是列），就像Chis所说的，对于每一行的10个预测，让我们找出哪个预测值最高，然后返回不是那个值（如果只是说max，它会返回值），argmax返回值的索引。所以通过说`argmax(axis=1)`，它将返回实际上是数字本身的索引。所以让我们取前5个：
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: So that’s how we can convert our probabilities back into predictions. We save
    that away and call it `preds`. We can then say when does `preds` equal the ground
    truth. That’s going to return an array of booleans which we can treat as ones
    and zeros and the mean of a bunch of ones and zeros is just the average. So that
    gives us the accuracy of 91.8%.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们如何将我们的概率转换回预测的方法。我们保存下来并称之为`preds`。然后我们可以说`preds`何时等于真实值。这将返回一个布尔数组，我们可以将其视为1和0，一堆1和0的平均值就是平均值。这给了我们91.8%的准确率。
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So you want to be able to replicate the numbers you see and here it is. Here
    is our 91.8%.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你想要能够复制你看到的数字，这里就是。这里是我们的91.8%。
- en: '![](../Images/fe5ab63da5923878c6b70e7decad39d8.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe5ab63da5923878c6b70e7decad39d8.png)'
- en: So when we train this, the last thing tells us is whatever metric we asked for,
    and we asked for accuracy. Then before that we get the training set loss. The
    loss is again whatever loss we asked for (`nn.NLLLoss()`), and the second thing
    is the validation set loss. PyTorch doesn’t use the word loss, they use the word
    criterion. So you’ll see here `crit` so that’s criterion equal loss. So this is
    what loss function we want to use, they call that the criterion. Same thing. So
    `np.mean(preds == y_valid)` is how we can recreate that accuracy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我们训练这个模型时，最后一件事告诉我们的是我们要求的任何指标，我们要求的是准确率。然后在此之前，我们得到了训练集的损失。损失又是我们要求的任何损失(`nn.NLLLoss()`)，第二件事是验证集的损失。PyTorch不使用损失这个词，他们使用准则这个词。所以你会在这里看到`crit`，这就是准则等于损失。这就是我们想要使用的损失函数，他们称之为准则。同样的事情。所以`np.mean(preds
    == y_valid)`就是我们如何重新创建准确率的方法。
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/72058ea150fd0e6d88efb9914ee67e12.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72058ea150fd0e6d88efb9914ee67e12.png)'
- en: So now we can go ahead and plot eight of the images along with their predictions.
    For the ones we got wrong, you can see why they are wrong. The image of 4 is pretty
    close to 9\. It’s just missing a little cross at the top. The 3 is pretty close
    to 5\. It’s got a little bit of the extra on top. So we’ve made a start. And all
    we’ve done so far is, we haven’t actually created a deep neural net. We’ve actually
    got only one layer. So what we’ve actually done is we’ve created a logistic regression.
    Logistic regression is literally what we just built and you could try and replicate
    this with sklearn’s logistic regression package. When I did it, I got similar
    accuracy, but this version ran much faster because this is running on the GPU
    where else sklearn runs on the CPU. So even for something like logistic regression,
    we can implement it very quickly qith PyTorch.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们可以继续绘制八幅图像以及它们的预测。对于我们预测错误的那些，您可以看到它们为什么错误。数字4的图像非常接近数字9。它只是在顶部少了一个小交叉。数字3非常接近数字5。它在顶部有一点额外的部分。所以我们已经开始了。到目前为止，我们实际上还没有创建一个深度神经网络。我们实际上只有一个层。因此，我们实际上所做的是创建了一个逻辑回归。逻辑回归就是我们刚刚构建的内容，您可以尝试使用sklearn的逻辑回归包来复制这个过程。当我这样做时，我得到了类似的准确性，但这个版本运行得更快，因为它在GPU上运行，而sklearn在CPU上运行。因此，即使对于像逻辑回归这样的东西，我们也可以使用PyTorch非常快速地实现它。
- en: '**Question**: When we are creating our net, we ahve to do `.cuda()`. What would
    be the consequence of not doing that? Would it just not run [[1:14:16](https://youtu.be/DzE0eSdy5Hk?t=4456)]?
    It wouldn’t run quickly. It’ll run on the CPU.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：当我们创建我们的网络时，我们必须执行`.cuda()`。如果不这样做会有什么后果？它只是不会快速运行。它将在CPU上运行。
- en: '**Question**: Why do we have to do linear and followed by nonlinear [[1:14:34](https://youtu.be/DzE0eSdy5Hk?t=4474)]?
    The short answer is because that’s what the universal approximation theorem says
    is the structure which can give you arbitrarily accurate functions for any functional
    form. The long answer is the details of why the universal approximation theorem
    works. Another version of the short answer is, that’s the definition of a neural
    network. So the definition of a neural network is a linear layer followed by an
    activation function followed by a linear layer followed by an activation function,
    etc. We go into a lot more detail of this in the deep learning course but for
    this purpose it’s enough to know that it works. So far, of course, we haven’t
    actually built a deep neural net at all. We’ve just built a logistic regression.
    So at this point, if you think about it, all we’re doing is we are taking every
    input pixel and multiplying it by a weight for each possible outcome. So we are
    basically saying on average the number 1 has these pixels turned on. The number
    two has these pixels turned on. That’s why it’s not terribly accurate. That’s
    not how digit recognition works in real life. But that’s all we build so far.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么我们必须先进行线性操作，然后再进行非线性操作？简短的答案是因为这是通用逼近定理所说的结构，可以为任何函数形式提供任意精确的函数。长答案是通用逼近定理为何有效的细节。另一个简短答案是，这就是神经网络的定义。因此，神经网络的定义是一个线性层，后跟一个激活函数，再后跟一个线性层，再后跟一个激活函数，依此类推。我们在深度学习课程中会更详细地讨论这一点，但就此目的而言，知道它有效就足够了。到目前为止，当然，我们实际上还没有构建一个深度神经网络。我们只是构建了一个逻辑回归。因此，在这一点上，如果你考虑一下，我们所做的就是将每个输入像素乘以每个可能结果的权重。因此，我们基本上是在说，平均而言，数字1具有这些像素点亮。数字2具有这些像素点亮。这就是为什么它不是非常准确的原因。这不是现实生活中数字识别的工作方式。但到目前为止，这就是我们构建的全部内容。
- en: '**Question**: So you keep saying this Universal approximation theorem. Did
    you define that [[1:16:07](https://youtu.be/DzE0eSdy5Hk?t=4567)]? Yeah, but let’s
    cover it again because it’s worth talking about. So Michael Nielsen has this great
    website called neural networks and deep learning. And his [chapter 4](http://neuralnetworksanddeeplearning.com/chap4.html)
    is actually famous now and in it, he does this walkthrough of basically showing
    that a neural network can approximate any other function to arbitrarily close
    accuracy as long as it’s big enough. And we walk through this in a lot of detail
    in the deep learning course but the basic trick is that he shows that with a few
    different numbers, you can basically cause these things to create little boxes,
    you can move the boxes up and down, you can move them around, you can join them
    together to eventually basically create like connections of towers which you can
    use to approximate any kind of surface.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所以你一直在说这个通用逼近定理。你有定义过吗？是的，但让我们再次讨论一下，因为这值得谈论。因此，Michael Nielsen有一个名为神经网络与深度学习的优秀网站。他的第4章现在实际上很有名，其中他通过演示神经网络可以以足够大的规模逼近任何其他函数，只要它足够大，来详细介绍这一点。我们在深度学习课程中详细讨论了这一点，但基本的诀窍是，他展示了通过几个不同的数字，您基本上可以使这些事物创建小盒子，您可以将盒子上下移动，您可以将它们移动，您可以将它们连接在一起，最终基本上可以创建像塔一样的连接，您可以用来逼近任何类型的表面。
- en: '![](../Images/36cc00e3507966e1b4b8a06349cac3a3.png)![](../Images/0184c296d1637cb289ecd868956402a8.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36cc00e3507966e1b4b8a06349cac3a3.png)![](../Images/0184c296d1637cb289ecd868956402a8.png)'
- en: So that’s basically the trick. So all we need to do, given that, is to kind
    of find the parameters for each of the linear functions in that neural network.
    So to find the weights in each of the matrices. So far, we’ve got just one matrix
    and we’ve just built a simple logistic regression.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这基本上就是诀窍。因此，我们所需要做的就是，鉴于此，找到神经网络中每个线性函数的参数。因此，找到每个矩阵中的权重。到目前为止，我们只有一个矩阵，我们只是构建了一个简单的逻辑回归。
- en: '**Question**: I just wanted to confirm that when you showed the examples of
    images which were misclassified, they look rectangular so it’s just that while
    rendering, pixels are being scaled differently [[1:17:50](https://youtu.be/DzE0eSdy5Hk?t=4670)]?
    They are 28 by 28\. I think they just look rectangular because they’ve got titles
    on the top. Matplotlib does often fiddle around with what it considers black versus
    while and having different size axes and stuff. So you do have to be little bit
    careful there sometimes.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我只是想确认一下，当你展示被错误分类的图像的例子时，它们看起来是矩形的，所以只是在渲染时，像素被不同地缩放了吗？它们是28乘28的。我认为它们看起来是矩形的，因为它们顶部有标题。Matplotlib经常会调整它认为的黑色与白色以及具有不同大小轴等的东西。因此，有时你必须小心一点。'
- en: Defining Logistic Regression Ourselves [[1:18:31](https://youtu.be/DzE0eSdy5Hk?t=4711)]
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自己定义逻辑回归
- en: Hopefully this will now make more sense because what we’re going to do is dig
    in a layer deeper and define logistic regression without using `nn.Sequential`,
    `nn.Linear`, or `nn.LogSoftmax`. So we are going to do nearly all of the layer
    definition from scratch. So to do that, we’re going to have to define a PyTorch
    module. PyTorch module is basically either a neural net or a layer in a neural
    net which is actually a powerful concept of itself. Basically anything that can
    behave like a neural net can itself be part of another neural net. So this is
    how we can construct particularly powerful architectures combining lots of other
    pieces.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在这会更有意义，因为我们要深入一层，定义逻辑回归，而不使用`nn.Sequential`、`nn.Linear`或`nn.LogSoftmax`。因此，我们将几乎所有的层定义都从头开始做。为了做到这一点，我们将不得不定义一个PyTorch模块。PyTorch模块基本上是一个神经网络或神经网络中的一层，这实际上是一个强大的概念。基本上，任何可以像神经网络一样行为的东西本身可以成为另一个神经网络的一部分。这就是我们如何构建特别强大的架构，结合了许多其他部分。
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So to create a PyTorch module, just create a Python class but it has to inherit
    from `nn.Module`. So we haven’t done inheritance before, other than that, this
    is all the same concepts we’ve seen in OO already. Basically if you put something
    in parentheses here (after a class name), what it means is that our class gets
    all of the functionality of this class for free. It’s called sub-classing it.
    So we are going to get all of the capabilities of a neural network module that
    the PyTorch authors have provided and then we are going to add additional functionality
    to it. When you create a sub class, there is one key thing you need to remember
    to do which is when you initialize your class, you have to first of all initialize
    the superclass. So superclass is the `nn.Module`. So `nn.Module` has to be built
    before you can start adding your pieces to it. So this is just like something
    you can copy and paste into every one of your modules. You just say `super().__init__()`
    . It just means construct the superclass first.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要创建一个PyTorch模块，只需创建一个Python类，但它必须继承自`nn.Module`。因此，除了继承之外，这是我们已经在面向对象中看到的所有概念。基本上，如果你在这里（在类名后面）放入括号中的内容，意味着我们的类会免费获得这个类的所有功能。这被称为子类化。因此，我们将获得PyTorch作者提供的神经网络模块的所有功能，然后我们将添加额外的功能。当你创建一个子类时，有一件重要的事情你需要记住，那就是当你初始化你的类时，你首先必须初始化超类。因此，超类是`nn.Module`。因此，在你开始添加你的部分之前，必须先构建`nn.Module`。这就像你可以复制并粘贴到你的每一个模块中的东西。你只需说`super().__init__()`。这意味着首先构造超类。
- en: So having done that, we can now define our weights and our bias [[1:20:29](https://youtu.be/DzE0eSdy5Hk?t=4829)].
    Our weights is the weight matrix. It’s the actual matrix that we’re going to multiply
    our data by. And as we discussed, it’s going to have 28 times 28 rows and 10 columns.
    That’s because if we take an image which is we’ve flattened out into a 28 by 28
    length vector, then we can multiply it by this weight matrix to get back out a
    length 10 vector which we can then use to consider it as a set of predictions.
    So that’s our weight matrix. Now the problem is that we don’t just want *y = ax*.
    We want *y = ax + b*. So *+ b* in neural nets is called bias. So as well as defining
    weights, we are also going to define bias. Since this thing `get_weights(28*28,
    10)` is going to spit out for every image something of length 10\. That means
    that we need to create a vector of length 10 to be our biases. In other words,
    for everything naught, 1, 2, 3 up to 9, we are going to have a different plus
    *b* that would be adding. So we’ve got our data matrix which is of length 10,000
    by 28 ⨉ 28\. Then we’ve got our weight matrix which is 28 ⨉ 28 by 10\. So if we
    multiply those together, we get something of size 10,000 by 10.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做之后，我们现在可以定义我们的权重和偏差。我们的权重是权重矩阵。这是我们将要用来乘以我们的数据的实际矩阵。正如我们讨论过的，它将有28乘28行和10列。这是因为如果我们取一个我们已经展平成一个28乘28长度向量的图像，然后我们可以将它乘以这个权重矩阵，得到一个长度为10的向量，然后我们可以将其视为一组预测。这就是我们的权重矩阵。现在问题是我们不只是想要*y
    = ax*。我们想要*y = ax + b*。因此，在神经网络中，*+ b*被称为偏差。因此，除了定义权重，我们还将定义偏差。由于这个`get_weights(28*28,
    10)`将为每个图像输出长度为10的东西。这意味着我们需要创建一个长度为10的向量作为我们的偏差。换句话说，对于每个0、1、2、3直到9，我们将有一个不同的加*b*。因此，我们有我们的数据矩阵，它的长度是10,000乘以28乘以28。然后我们有我们的权重矩阵，它是28乘以28乘以10。因此，如果我们将它们相乘，我们将得到一个大小为10,000乘以10的东西。
- en: '![](../Images/c12901781ee389bcc3d8d3f8a8f59878.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c12901781ee389bcc3d8d3f8a8f59878.png)'
- en: 'Then we want to add on our bias like so:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们想要添加我们的偏差，如下所示：
- en: '![](../Images/dfdd967bfab9651c9b258575e5d070e7.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfdd967bfab9651c9b258575e5d070e7.png)'
- en: We are going to learn a lot more about this later, but when we add on a vector
    like this, it basically going to get added to every row. So that bias is going
    to get added to every rows. So we first of all define those. To define them, we’ve
    created a tiny little function called `get_weights` which basically just creates
    some normally distributed random numbers. `torch.randn` returns a tensor filled
    with random numbers from a normal distribution.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以后会学到更多关于这个的知识，但是当我们像这样添加一个向量时，基本上它会被添加到每一行。因此，那个偏差将被添加到每一行。因此，我们首先定义这些。为了定义它们，我们创建了一个名为`get_weights`的小函数，它基本上只是创建一些正态分布的随机数。`torch.randn`返回一个填充有正态分布随机数的张量。
- en: '![](../Images/415721b17b1cebc5d4e0965e3e69595d.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/415721b17b1cebc5d4e0965e3e69595d.png)'
- en: We have to be a bit careful though. When we do deep learning, like when we add
    more linear layers later. Imagine if we have a matrix which on average tends to
    increase the size of the inputs we give to it. If we then multiply it by lots
    of matrices of that size, it’s going to make the numbers bigger and bigger and
    bigger, like exponentially bigger. Or what if made them a bit smaller? It’s going
    to make them smaller and smaller and smaller exponentially smaller. Because a
    deep network applies lots of linear layers, if on average they result in things
    a bit bigger than they started with or a bit smaller than they started with, it’s
    going to exponentially multiply that difference. So we need to make sure that
    the weight matrix is of an appropriate size that the inputs to it (more specifically,
    the mean of the inputs) is not going to change.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须要小心。当我们进行深度学习时，比如以后添加更多的线性层。想象一下，如果我们有一个矩阵，平均倾向于增加我们输入的大小。如果我们将其乘以许多相同大小的矩阵，它会使数字变得越来越大，指数级增长。或者如果我们让它们变小一点呢？它会使它们变得越来越小，指数级减小。因为深度网络应用了许多线性层，如果平均而言它们导致的结果比起始值稍微大一点或稍微小一点，那么它将指数级地放大这种差异。因此，我们需要确保权重矩阵的大小适当，使得输入到它的（更具体地说，输入的均值）不会改变。
- en: So it turns out that if you use normally distributed random numbers and divided
    by the number of rows in the weight matrix, this particular random initialization
    keeps your numbers at about the right scale. So this idea that, if you’ve done
    linear algebra, basically if the first eigenvalue is bigger than one or smaller
    than one, it’s going to cause the gradients to get bigger and bigger or smaller
    and smaller. That’s called gradient explosion. So we’ll talk more about this in
    the deep learning course, but if you are interested, you can look at [Kaiming
    He initialization](https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)
    and read all about this concept, but for now, it’s probably just enough to know
    that if you use this type of random number generation (i.e. `torch.randn(dims)/dims[0]`),
    you’re going to get random numbers that are nicely behaved. You are going to start
    out with an input which is mean 0 standard deviation 1\. Once you put it through
    this set of random numbers, you’ll still have something that’s about mean 0 standard
    deviation 1\. That’s basically the goal.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，如果你使用正态分布的随机数并除以权重矩阵中的行数，这种随机初始化可以保持你的数字在大约正确的范围内。因此，这个想法是，如果你做过线性代数，基本上如果第一个特征值大于1或小于1，它会导致梯度变得越来越大或越来越小。这就是梯度爆炸。我们将在深度学习课程中更多地讨论这个问题，但如果你感兴趣，你可以查看[Kaiming
    He初始化](https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)，并阅读有关这个概念的所有内容，但现在，知道如果你使用这种类型的随机数生成（即`torch.randn(dims)/dims[0]`），你将得到行为良好的随机数。你将从均值为0标准差为1的输入开始。一旦你通过这组随机数，你仍然会得到大约均值为0标准差为1的东西。这基本上就是目标。
- en: One nice thing about PyTorch is that you can play with this stuff [[1:25:44](https://youtu.be/DzE0eSdy5Hk?t=5144)].
    So try it out. Every time you see a function being used, run it and take a look.
    So you’ll see, it looks a lot like numpy but it doesn’t return a numpy array.
    It returns a tensor.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的一个好处是你可以玩弄这些东西[[1:25:44](https://youtu.be/DzE0eSdy5Hk?t=5144)]。所以试一试。每当你看到一个函数被使用时，运行它并查看一下。所以你会发现，它看起来很像numpy，但它不返回一个numpy数组。它返回一个张量。
- en: '![](../Images/a2bb48e179731174df8512c4dccd6422.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2bb48e179731174df8512c4dccd6422.png)'
- en: And in fact, now I’m GPU programming.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，现在我在进行GPU编程。
- en: '![](../Images/329d688f00141a1aaa4035ef72dfbf08.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/329d688f00141a1aaa4035ef72dfbf08.png)'
- en: Put `.cuda()` and now it’s doing it on the GPU.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`.cuda()`，现在它在GPU上运行。
- en: '![](../Images/7da66ebc0fff94c8da8b6ff8c721ea4a.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7da66ebc0fff94c8da8b6ff8c721ea4a.png)'
- en: I just multiplied that matrix by 3 very quickly on GPU! So that’s how we do
    GPU programming with PyTorch.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我在GPU上非常快地将那个矩阵乘以3！这就是我们如何使用PyTorch进行GPU编程。
- en: As we said, we create one 28*28 by 10 weight matrix, and the other is just rank
    1 of 10 for biases [[1:26:29](https://youtu.be/DzE0eSdy5Hk?t=5189)]. We have to
    make them a parameter. This is basically telling PyTorch which things to update
    when it does SGD. That’s very minor technical detail.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，我们创建了一个28*28乘以10的权重矩阵，另一个只是10的秩1偏差[[1:26:29](https://youtu.be/DzE0eSdy5Hk?t=5189)]。我们必须将它们设为参数。这基本上告诉PyTorch在执行SGD时要更新哪些内容。这是一个非常微小的技术细节。
- en: '![](../Images/1d7edd5f587825661a3a62a910167cfd.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d7edd5f587825661a3a62a910167cfd.png)'
- en: So having created the weight matrices, we then define a special method with
    the name `forward`. This is a special method and the name forward has a special
    meaning in PyTorch. A method called forward in PyTorch is the name of the method
    that will get called when your layer is calculated. So if you create a neural
    net or a layer, you have to define forward and it’s going to get passed the data
    from the previous layer. Our definition is to do a matrix multiplication of our
    input data times our weights and add on the biases. That’s it. That’s what happened
    earlier on when we said `nn.Linear`. It created this thing for us.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了权重矩阵后，我们定义了一个名为`forward`的特殊方法。这是一个特殊的方法，而在PyTorch中，名称forward具有特殊含义。在PyTorch中，称为forward的方法是在计算层时将被调用的方法名称。因此，如果你创建了一个神经网络或一个层，你必须定义forward，它将传递前一层的数据。我们的定义是对输入数据和权重进行矩阵乘法，并加上偏差。就是这样。这就是我们之前说的`nn.Linear`时发生的事情。它为我们创建了这个东西。
- en: Now unfortunately though, we are not getting a 28 by 28 long vector. We are
    getting a 28 row by 28 column matrix, so we have to flatten it. Unfortunately,
    in PyTorch, they tend to rename things. They spell “resize” “view”. So `view`
    means reshape. So you can see here `x.view(x.size(0), -1)`, we end up with something
    where the number of images (`x.size(0)`), we are going to leave the same. Then
    we are going to replace row by column with a single axis. Again, `-1` meaning
    as long as required. So this is how we flatten something using PyTorch.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们并没有得到一个28乘以28的长向量。我们得到的是一个28行乘以28列的矩阵，所以我们必须将其展平。不幸的是，在PyTorch中，它们倾向于重新命名事物。他们将“resize”拼写为“view”。所以`view`意味着重塑。因此，你可以看到这里`x.view(x.size(0),
    -1)`，我们最终得到的是一个图像数量（`x.size(0)`）不变。然后我们将行替换为列，形成一个单一轴。再次，`-1`的意思是尽可能长。这就是我们使用PyTorch展平的方法。
- en: So we flatten it, do a matrix multiply, and then finally we do our softmax [[1:28:23](https://youtu.be/DzE0eSdy5Hk?t=5303)].
    So softmax is the activation function we use. If you look in the deep learning
    repo, you’ll find something called [entropy example](https://github.com/fastai/fastai/blob/master/courses/dl1/excel/entropy_example.xlsx)
    where you will see an example of softmax. Softmax simply takes the outputs from
    our final layer, so we get our outputs from our linear layer. And what we do is
    we go *e* to the power of (*e*^) for each output.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将其展平，进行矩阵乘法，最后进行softmax。所以softmax是我们使用的激活函数。如果您查看深度学习存储库，您会发现一个名为熵示例的内容，您将在其中看到softmax的示例。Softmax简单地获取我们最终层的输出，因此我们从线性层获取输出。我们所做的是对每个输出进行*e*的(*e*^)运算。
- en: '![](../Images/95e41c192f16b15b0842c2ff3dfc6cc6.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95e41c192f16b15b0842c2ff3dfc6cc6.png)'
- en: Then we take that number and divide by the sum of the *e* to the poser of’s.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们取这个数字，除以*e*的幂的总和。
- en: '![](../Images/7cd292ddd615acc65ceb6ab7de594fe4.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cd292ddd615acc65ceb6ab7de594fe4.png)'
- en: That’s called softmax. Why do we do that? Well, because we are dividing this
    (exp) with the sum, that means the sum of those itself must add to one. And that’s
    what we want. We want the probabilities of all the possible outcomes add to one.
    Furthermore, because we are using *e*^ , that means we know that every one of
    these (softmax) is between zero and one. And probabilities we know would be between
    zero and one. Then finally because we are using *e* to the power of, it tends
    to mean that slightly bigger values in the input turn into much bigger values
    in the output. So you’ll see, generally speaking, my softmax there are going to
    be one big number and lots of small numbers. And that’s what we want because we
    know that the output is one hot encoded. So in other words a softmax activation
    function, the softmax non-linearity, is something that returns things that behave
    like probabilities where one of those probabilities is more likely to be kind
    of high and the other ones are more likely to be low. And we know that’s what
    we want to map to our one hot encoding so a softmax is a great activation function
    to use to help the neural net, make it easier for the neural net to map to the
    output you wanted. And this is what we generally want. When we are designing neural
    networks, we try to come up with little architectural tweaks that make it as easy
    for it as possible to match the output that we know we want.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的softmax。为什么我们这样做？因为我们正在将这个（exp）除以总和，这意味着这些本身的总和必须加起来为一。这就是我们想要的。我们希望所有可能结果的概率总和为一。此外，因为我们使用*e*^，这意味着我们知道这些（softmax）中的每一个都在零和一之间。我们知道概率将在零和一之间。最后，因为我们使用*e*的幂，这意味着输入中稍大的值会变成输出中的更大值。因此，通常情况下，您会看到我的softmax中有一个大数和许多小数。这就是我们想要的，因为我们知道输出是一热编码的。换句话说，softmax激活函数，softmax非线性，是一种返回类似概率的东西的东西，其中其中一个概率更有可能是高的，其他概率更有可能是低的。我们知道这就是我们想要映射到我们的一热编码的内容，因此softmax是一个很好的激活函数，可以帮助神经网络更容易地映射到您想要的输出。这通常是我们想要的。当我们设计神经网络时，我们尝试提出一些小的架构调整，使其尽可能容易地匹配我们想要的输出。
- en: So that’s basically it [[1:30:45](https://youtu.be/DzE0eSdy5Hk?t=5445)]. Rather
    than doing Sequential and using `nn.Linear` and `nn.LogSoftmax`, we’ve defined
    it from scratch. We can now say, just like before, our `net2` is equal to `LogReg().cuda()`
    and we can say `fit` and we get to, within a slight random deviation, exactly
    the same output.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是这样。与其使用Sequential和`nn.Linear`以及`nn.LogSoftmax`，我们从头开始定义了它。现在我们可以说，就像以前一样，我们的`net2`等于`LogReg().cuda()`，我们可以说`fit`，我们得到了几乎完全相同的输出，只是有轻微的随机偏差。
- en: '[PRE28]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So what I like you to do during the week is to play around with `torch.randn`
    to generate some random tensors, `torch.matmul` to start multiplying them together,
    adding them up, try to make sure that you can rewrite softmax yourself from scratch.
    Try to fiddle around a bit with reshaping, view, all that kind of stuff so by
    the time you come back next week you feel pretty comfortable with PyTorch.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我希望你在这一周里尝试使用`torch.randn`生成一些随机张量，使用`torch.matmul`开始将它们相乘，相加，尝试确保你可以自己从头开始重写softmax。尝试玩弄一下重塑、view等等，这样到下周你回来时就会感觉对PyTorch相当舒适。
- en: And if you google for PyTorch tutorial, you’ll see there’s a lot of great material
    actually on the [PyTorch website](https://pytorch.org/tutorials/) to help you
    along — showing you how to create tensors, modify them, and do operations on them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您搜索PyTorch教程，您会看到[PyTorch网站](https://pytorch.org/tutorials/)上有很多很好的材料可以帮助您，向您展示如何创建张量，修改它们以及对它们进行操作。
- en: '**Question**: So I see that the forward is the layer that gets applied after
    each of the linear layers[[1:31:57](https://youtu.be/DzE0eSdy5Hk?t=5517)].'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我看到前向是在每个线性层之后应用的层。
- en: '**Jeremy**: Not quite. The forward is just the definition of the module, so
    this is how we are implementing Linear.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy：不完全是。前向只是模块的定义，这是我们实现Linear的方式。
- en: '**Continued**: Does that mean after each linear layer, you have to apply the
    same function? Let’s say we can’t do a LogSoftmax after layer 1 and then apply
    some other function after layer two if we have a multi-layer neural network?'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 继续：这是否意味着在每个线性层之后，您必须应用相同的函数？假设我们不能在第一层之后应用LogSoftmax，然后在第二层之后应用其他函数，如果我们有一个多层神经网络？
- en: '**Jeremy**: So normally we define neural networks like so:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy：所以通常我们这样定义神经网络：
- en: '![](../Images/361ca59ba0adecf8fe540de419fe837f.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/361ca59ba0adecf8fe540de419fe837f.png)'
- en: 'We just say here is a list of the layers we want. You don’t have to write your
    own forward. All we did just now is to say instead of doing this, let’s not use
    any of this at all, but write it all by hand ourselves. So you can write as many
    layers as you like in any order you like here. The point was that here, we are
    not using any of that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是说这里是我们想要的层的列表。您不必编写自己的前向。我们刚刚做的是说，与其这样做，不如完全不使用这些，而是自己手写所有内容。因此，您可以按任何顺序编写任意数量的层。重点是在这里，我们没有使用任何这些：
- en: '![](../Images/49a74247bc6ace55da99560b4810f673.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49a74247bc6ace55da99560b4810f673.png)'
- en: We’ve written our own `matmul` plus bias, our own softmax, so this is just Python
    code. You can write whatever Python code inside forward that you like to define
    your own neural net. You won’t normally do this yourself. Normally you’ll just
    use the layers that PyTorch provides and you’ll use `.Sequential` to put them
    together. Or even more likely, you’ll download a predefined architecture and use
    that. We’re just doing this to learn how it works behind the scenes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经编写了自己的`matmul`加偏置项，自己的softmax，所以这只是Python代码。您可以在forward函数内编写任何您喜欢的Python代码来定义自己的神经网络。通常情况下，您不会自己这样做。通常您只会使用PyTorch提供的层，并使用`.Sequential`将它们组合在一起。或者更有可能的是，您会下载一个预定义的架构并使用它。我们只是为了学习它在幕后是如何工作的。
- en: Alright, great. Thanks everybody!
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，太棒了。谢谢大家！
