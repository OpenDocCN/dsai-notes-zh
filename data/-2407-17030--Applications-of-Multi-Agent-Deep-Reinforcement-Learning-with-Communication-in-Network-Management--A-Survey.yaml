- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:31:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:31:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2407.17030] Applications of Multi-Agent Deep Reinforcement Learning with Communication
    in Network Management: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2407.17030] 多智能体深度强化学习与通信在网络管理中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17030](https://ar5iv.labs.arxiv.org/html/2407.17030)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17030](https://ar5iv.labs.arxiv.org/html/2407.17030)
- en: Applications of Multi-Agent Deep
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多智能体深度强化学习的应用
- en: Reinforcement Learning with Communication
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与通信
- en: 'in Network Management: A Survey'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络管理中的应用：综述
- en: Yue Pi23, Wang Zhang2, Yong Zhang4, Hairong Huang4, Baoquan Rao4,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 岳皮23，王璋2，钟永4，黄海容4，饶宝全4，
- en: '¹¹1Corresponding author.¹Yulong Ding2, ²²footnotemark: 2¹Shuanghua Yang25 2Shenzhen
    Key Laboratory of Safety and Security for Next Generation of Industrial Internet
    and Department of Computer Science and Engineering, Southern University of Science
    and Technology, Shenzhen, China 3Peng Cheng Laboratory, Shenzhen, China 4Huawei
    Technologies Co., Ltd. 5Department of Computer Science, University of Reading,
    UK'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹1通讯作者。¹丁玉龙2，²²脚注标记：2¹杨双华25 2深圳科技大学工业互联网下一代安全与保障深圳市重点实验室及计算机科学与工程系，中国深圳 3鹏城实验室，中国深圳
    4华为技术有限公司 5计算机科学系，英国雷丁大学
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the advancement of artificial intelligence technology, the automation of
    network management, also known as Autonomous Driving Networks (ADN), is gaining
    widespread attention. The network management has shifted from traditional homogeneity
    and centralization to heterogeneity and decentralization. Multi-agent deep reinforcement
    learning (MADRL) allows agents to make decisions based on local observations independently.
    This approach is in line with the needs of automation and has garnered significant
    attention from academia and industry. In a distributed environment, information
    interaction between agents can effectively address the non-stationarity problem
    of multiple agents and promote cooperation. Therefore, in this survey, we first
    examined the application of MADRL in network management, including specific application
    fields such as traffic engineering, wireless network access, power control, and
    network security. Then, we conducted a detailed analysis of communication behavior
    between agents, including communication schemes, communication content construction,
    communication object selection, message processing, and communication constraints.
    Finally, we discussed the open issues and future research directions of agent
    communication in MADRL for future network management and ADN applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能技术的进步，网络管理的自动化，也被称为自动驾驶网络（ADN），正受到广泛关注。网络管理已经从传统的同质化和集中化转变为异质化和分散化。多智能体深度强化学习（MADRL）允许智能体根据本地观察独立做出决策。这种方法符合自动化的需求，并且获得了学术界和工业界的广泛关注。在分布式环境中，智能体之间的信息交互可以有效解决多智能体的非平稳性问题，并促进合作。因此，在这项综述中，我们首先审视了MADRL在网络管理中的应用，包括交通工程、无线网络接入、电源控制和网络安全等具体应用领域。然后，我们对智能体之间的通信行为进行了详细分析，包括通信方案、通信内容构建、通信对象选择、消息处理和通信约束。最后，我们讨论了MADRL中智能体通信在未来网络管理和ADN应用中的开放问题和未来研究方向。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Multi-agent deep reinforcement learning, Agent communication, Emergent communication,
    Autonomous Driving Networks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体深度强化学习，智能体通信，突现通信，自动驾驶网络。
- en: I Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: The improvement of general computing capabilities has led to the continuous
    expansion of various production networks. The traditional centralized network
    management mode has gradually become unable to meet the demand. Therefore, deploying
    distributed and decentralized network engineering technologies to improve network
    performance under different network resource and service demand conditions is
    an essential trend in future network management. Network management involves monitoring
    and controlling network resources to ensure effective network operation. This
    comprises several technologies catering to different scenarios and requirements.
    Traditional network management technologies usually rely on heuristic methods,
    but with the increase of new devices and services, the network is becoming more
    extensive and more complex, and it is difficult to model and predict network devices
    accurately. At the same time, traditional methods are hard to meet the demand
    for automation and intelligent management for future network management.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一般计算能力的提升导致各种生产网络的不断扩展。传统的集中式网络管理模式逐渐无法满足需求。因此，部署分布式和去中心化的网络工程技术，以在不同网络资源和服务需求条件下提升网络性能，已成为未来网络管理的重要趋势。网络管理涉及监控和控制网络资源，以确保网络的有效运行。这包括几种适用于不同场景和需求的技术。传统网络管理技术通常依赖于启发式方法，但随着新设备和服务的增加，网络变得越来越大且复杂，准确建模和预测网络设备变得困难。同时，传统方法难以满足未来网络管理对自动化和智能管理的需求。
- en: In recent years, due to the development of artificial intelligence technology,
    technologies such as Machine Learning, Deep Learning (DL), and Reinforcement Learning
    (RL) have started to be implemented in network management. Multi-Agent Deep Reinforcement
    Learning (MADRL) based on DL and RL is considered an effective technique to provide
    AI network solutions for critical problems in the future Internet[[94](#biba.bibx24)].
    In MADRL, each network entity is regarded as an agent, gathers dynamic and uncertain
    environmental information, and independently takes action. However, in some partially
    observable distributed multi-agent systems, due to the mutual influence of adaptation
    strategies, the system is vulnerable to non-stationary problems [[106](#biba.bibx36)].
    Sharing observations, intentions, or experiences helps agents understand the environment
    and achieve stable learning. Meanwhile, effective communication between agents
    can be crucial in promoting cooperation among them [[130](#biba.bibx60)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，由于人工智能技术的发展，机器学习、深度学习（DL）和强化学习（RL）等技术已经开始在网络管理中应用。基于DL和RL的多智能体深度强化学习（MADRL）被认为是一种有效的技术，用于为未来互联网中的关键问题提供AI网络解决方案[[94](#biba.bibx24)]。在MADRL中，每个网络实体被视为一个代理，收集动态且不确定的环境信息，并独立采取行动。然而，在一些部分可观察的分布式多智能体系统中，由于适应策略的相互影响，该系统容易受到非平稳问题的影响[[106](#biba.bibx36)]。共享观察、意图或经验有助于代理理解环境并实现稳定学习。同时，代理之间的有效通信在促进其合作中也可能至关重要[[130](#biba.bibx60)]。
- en: The current communication protocols between agents are usually predefined, resulting
    in a considerable increase in the total communication overhead as the number of
    agents increases. An alternative method called emergent communication enables
    agents to autonomously learn communication protocols and select the communication
    objects through interaction. However, it also faces several challenges, such as
    unreadable emergent messages by humans and requiring massive training before deployment.
    Moreover, its current application in network management is relatively limited;
    therefore, the reliability and feasibility of emergent communication need further
    validation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当前代理之间的通信协议通常是预定义的，随着代理数量的增加，总通信开销显著增加。另一种方法称为自发通信，它允许代理通过交互自主学习通信协议并选择通信对象。然而，它也面临一些挑战，例如人类无法读取的自发消息和需要大量训练才能部署。此外，它在网络管理中的当前应用相对有限，因此自发通信的可靠性和可行性需要进一步验证。
- en: This survey summarizes the communication behavior of multi-agent systems in
    network management. We first introduce and summarize the MARL system with agent
    communication currently applied in the four research directions of network management.
    Then we conduct a detailed analysis of the communication schemes of the system,
    the type of communication messages, the selection of communication objects, the
    message processing method of communication, and the constraints in agents’ communication.
    Finally, we propose existing issues and suggest future research directions for
    the communication of agents in network management.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查总结了网络管理中多智能体系统的通信行为。首先，我们介绍和总结了目前在网络管理四个研究方向中应用的MARL系统和代理通信。然后，我们对系统的通信方案，通信消息的类型，通信对象的选择，通信的消息处理方法以及代理通信中的约束进行了详细分析。最后，我们提出了现有问题，并为网络管理中代理通信的未来研究方向提供了建议。
- en: TABLE I
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表格I
- en: Existing Surveys on MADRL Communication or Network Management
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有关MADRL通信或网络管理的现有调查
- en: '| Work | MADRL | Scope |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | MADRL | 范围 |'
- en: '| Network Management | Communication |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 网络管理 | 通信 |'
- en: '| [[94](#biba.bibx24)] | ✓ |  | Future Internet |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#biba.bibx24)] | ✓ |  | 未来互联网 |'
- en: '| [[78](#biba.bibx8)] | ✓ |  | B5G/6G wireless network |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [[78](#biba.bibx8)] | ✓ |  | B5G/6G无线网络 |'
- en: '| [[123](#biba.bibx53)] | ✓ |  | Traffic Engineering |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| [[123](#biba.bibx53)] | ✓ |  | 流量工程 |'
- en: '| [[72](#biba.bibx2)] | ✓ |  | Vehicular Networks |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#biba.bibx2)] | ✓ |  | 车载网络 |'
- en: '| [[140](#biba.bibx70)] |  | ✓ | Agent Communication |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [[140](#biba.bibx70)] |  | ✓ | 代理通信 |'
- en: '| [[73](#biba.bibx3)] |  | ✓ | Emergent Communication |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [[73](#biba.bibx3)] |  | ✓ | 紧急通信 |'
- en: '| Our Survey | ✓ | ✓ | Network Management + Agent Communication |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 我们的调查 | ✓ | ✓ | 网络管理 + 代理通信 |'
- en: '![Refer to caption](img/bde68aa84a83278a8dd3bbe4f9018d87.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![见标题](img/bde68aa84a83278a8dd3bbe4f9018d87.png)'
- en: 'Figure 1: A classification of the applications of MADRL in network management
    with communication.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：MADRL在具有通信功能的网络管理中应用的分类。
- en: I-A Related Existing Survey and Our Scope
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 相关的现有调查和我们的研究范围
- en: 'Since MADRL has been widely used to solve critical problems in network management,
    many surveys have summarized and classified relevant literature from different
    aspects, as shown in Table I. For example, The authors in [[94](#biba.bibx24)]
    conducted a detailed analysis of the application of MARL in five fields in the
    future network: network access, edge computing, routing, unmanned aerial vehicle
    (UAV), and network security. In [[78](#biba.bibx8)], the authors focus on the
    MADRL application in the future 6G network management. Furthermore, some works
    focus on specific scenarios, such as the authors in [[123](#biba.bibx53)] detailing
    DRL and MADRL’s application in Traffic Engineering. In [[72](#biba.bibx2)], the
    authors analyzed some applications of MARL in vehicular networks. In addition
    to network management, we also pay attention to the reviews of communication between
    multiple agents. For example, in [[140](#biba.bibx70)], the authors analyzed the
    characteristics of communication between agents in MADRL from nine dimensions
    in detail. The authors in [[73](#biba.bibx3)] discuss the potential application
    of Emergent Communication in future wireless networks.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MADRL广泛应用于解决网络管理中的关键问题，许多调查从不同的角度总结和分类相关文献，如表I所示。例如，[[94](#biba.bibx24)]中的作者对MARL在未来网络的五个领域：网络接入，边缘计算，路由，无人机（UAV）和网络安全中的应用进行了详细分析。在[[78](#biba.bibx8)]中，作者们重点关注MADRL在未来6G网络管理中的应用。此外，一些作品关注具体场景，例如[[123](#biba.bibx53)]中的作者详细介绍了DRL和MADRL在流量工程中的应用。在[[72](#biba.bibx2)]中，作者分析了MARL在车载网络中的一些应用。除了网络管理，我们还关注多个代理之间的通信综述。例如，在[[140](#biba.bibx70)]中，作者详细分析了MADRL中代理之间通信的九个维度的特征。[[73](#biba.bibx3)]的作者讨论了紧急通信在未来无线网络中的潜在应用。
- en: However, to the best of our knowledge, there is currently no comprehensive review
    of communication in MADRL systems applied to network management. Therefore, this
    survey focuses on reviewing, analyzing, and comparing existing MADRL works with
    communication among agents applied to network management, as shown in Figure 1\.
    We classify and summarize six key characteristics of agent communication and conduct
    a separate analysis of each of them. Meanwhile, we introduce several MADRL emergent
    communication algorithms that have the potential for application to network management
    in the future.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，据我们所知，目前没有关于应用于网络管理的MADRL系统通信的全面综述。因此，本调查重点回顾、分析和比较现有的应用于网络管理的MADRL带通信的工作，如图1所示。我们对代理通信的六个关键特征进行了分类和总结，并对每个特征进行了单独分析。同时，我们介绍了几种具有未来网络管理应用潜力的MADRL突发通信算法。
- en: I-B Contributions and Survey Organization
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 贡献和调查组织
- en: 'Our contributions are as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献如下：
- en: $\bullet$
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We introduce the application of MADRL in network management and classify the
    properties of the agent communication in the MADRL system. We summarize the settings
    for agent communication that could be selected in different application scenarios
    and show the potential of using these algorithms to solve networking issues in
    future network management.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了MADRL在网络管理中的应用，并对MADRL系统中代理通信的属性进行了分类。我们总结了不同应用场景下可选择的代理通信设置，并展示了使用这些算法解决未来网络管理中的网络问题的潜力。
- en: $\bullet$
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We summarize several uncommon characteristics in MADRL related to communication
    in network management and discuss the unresolved issues and future research directions
    of using MADRL systems with agent communication in future network management.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结了MADRL在网络管理中的通信相关的几个不常见特征，并讨论了未来网络管理中使用MADRL系统与代理通信的未解决问题和研究方向。
- en: 'The rest of this paper is organized as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：
- en: Section II summarizes the recent works for the MADRL with communication in the
    four research directions of network management, including traffic engineering,
    network spectrum access, transmit power control, and network security. Section
    III classifies these works by their communication schemes. In Section IV, we introduce
    the classification of the message content of agent communication. Sections V and
    VI introduce the selection scheme of communication objects and message processing
    methods of MADRL’s work with communication. Section VII provides a summary of
    the communication constraints of agent communication. In Section VIII, we discuss
    the open issues and future research directions of agent communication of the applications
    of MARL for network management. The conclusions are given in Section IX.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第二节总结了在网络管理的四个研究方向（包括流量工程、网络频谱接入、发射功率控制和网络安全）中，带通信的MADRL的最新工作。第三节按其通信方案对这些工作进行了分类。在第四节，我们介绍了代理通信的消息内容分类。第五节和第六节介绍了MADRL带通信的工作中通信对象的选择方案和消息处理方法。第七节总结了代理通信的通信约束。第八节讨论了MARL在网络管理中的应用的代理通信的开放问题和未来研究方向。第九节给出了结论。
- en: The summary of the classification of MADRL communication in network management
    is shown in Table II.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 网络管理中MADRL通信的分类总结如表II所示。
- en: TABLE II
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表II
- en: Summary of MADRL Communication in Network Management
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 网络管理中的MADRL通信总结
- en: '| Work | Application Background | Communication Schemes | Communication Content
    | Communication object | Message Processing | Communication Constraints |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 应用背景 | 通信方案 | 通信内容 | 通信对象 | 消息处理 | 通信约束 |'
- en: '| [31] | Traffic Engineering | Fully Distributed | State | Neighbor | Concatenation
    | - |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| [31] | 流量工程 | 完全分布式 | 状态 | 邻居 | 连接 | - |'
- en: '| [34] | CTDE | State | Neighbor | Concatenation | Limited Bandwidth |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| [34] | CTDE | 状态 | 邻居 | 连接 | 有限带宽 |'
- en: '| [55] | Fully Distributed | Reward | All | - | - |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| [55] | 完全分布式 | 奖励 | 全部 | - | - |'
- en: '| [56] | Fully Distributed | State | All | - | - |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| [56] | 完全分布式 | 状态 | 全部 | - | - |'
- en: '| [57] | Fully Distributed | State | All | - | Noise |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| [57] | 完全分布式 | 状态 | 全部 | - | 噪声 |'
- en: '| [35] | Fully Distributed | Reward | Neighbor | - | Noise |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| [35] | 完全分布式 | 奖励 | 邻居 | - | 噪声 |'
- en: '| [32] | Fully Distributed | M&OP | Neighbor | Neural Network | - |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| [32] | 完全分布式 | M&OP | 邻居 | 神经网络 | - |'
- en: '| [36] | Fully Distributed | Reward | All | - | - |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| [36] | 完全分布式 | 奖励 | 全部 | - | - |'
- en: '| [58] | Fully Distributed | Reward | All | - | - |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| [58] | 完全分布式 | 奖励 | 全部 | - | - |'
- en: '| [37] | CTDE | State | Neighbor | Concatenation | - |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [37] | CTDE | 状态 | 邻居 | 级联 | - |'
- en: '| [33] | CTDE | State | Neighbor | Neural Network | - |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| [33] | CTDE | 状态 | 邻居 | 神经网络 | - |'
- en: '| [40] | Network Spectrum Access | CTDE | State | All | Concatenation | Noise
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| [40] | 网络频谱访问 | CTDE | 状态 | 全部 | 级联 | 噪声 |'
- en: '| [39] | Fully Centralized | M&OP | Central | Neural Network | Noise |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [39] | 完全集中式 | M&OP | 中央 | 神经网络 | 噪声 |'
- en: '| [54] | Fully Centralized | M&OP | Central | - | - |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [54] | 完全集中式 | M&OP | 中央 | - | - |'
- en: '| [42] | Fully Distributed | State | Neighbor | Concatenation | - |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| [42] | 完全分布式 | 状态 | 邻居 | 级联 | - |'
- en: '| [38] | Fully Distributed | M&OP | All | Concatenation | - |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| [38] | 完全分布式 | M&OP | 全部 | 级联 | - |'
- en: '| [43] | Fully Distributed | M&OP | All | Average | - |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [43] | 完全分布式 | M&OP | 全部 | 平均 | - |'
- en: '| [41] | CTDE | State | Neighbor | Neural Network | Noise |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [41] | CTDE | 状态 | 邻居 | 神经网络 | 噪声 |'
- en: '| [44] | Transmit Power Control | CTDE | State | All | Concatenation | - |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [44] | 发射功率控制 | CTDE | 状态 | 全部 | 级联 | - |'
- en: '| [49] | Fully Distributed | State | Neighbor | - | - |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [49] | 完全分布式 | 状态 | 邻居 | - | - |'
- en: '| [45] | CTDE | State | Neighbor | - | - |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| [45] | CTDE | 状态 | 邻居 | - | - |'
- en: '| [46] | Fully Distributed | State | Neighbor | Concatenation | - |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| [46] | 完全分布式 | 状态 | 邻居 | 级联 | - |'
- en: '| [50] | Fully Distributed | State | Neighbor | Concatenation | - |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| [50] | 完全分布式 | 状态 | 邻居 | 级联 | - |'
- en: '| [47] | Fully Distributed | State | All | Concatenation | - |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| [47] | 完全分布式 | 状态 | 全部 | 级联 | - |'
- en: '| [48] | Fully Distributed | Reward | All | Concatenation | - |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| [48] | 完全分布式 | 奖励 | 全部 | 级联 | - |'
- en: '| [51] | Network Security | CTDE | Reward | All | Concatenation | - |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| [51] | 网络安全 | CTDE | 奖励 | 全部 | 级联 | - |'
- en: '| [52] | CTDE | Reward | All | Concatenation | - |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| [52] | CTDE | 奖励 | 全部 | 级联 | - |'
- en: '| [53] | Fully Centralized | M&OP | Central | - | Limited Bandwidth |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| [53] | 完全集中式 | M&OP | 中央 | - | 带宽有限 |'
- en: II MADRL Communication with Application
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II MADRL 应用中的通信
- en: The information interaction between agents can help each agent in the system
    better understand the variation of the global environment and the impact of other
    agents on the environment, thereby improving the system’s overall performance.
    Moreover, agents can effectively achieve better cooperation by sharing information
    such as observations, actions, or reward values, thereby achieving better team
    performance [[78](#biba.bibx8)]. In wireless communication environments, where
    communication could be unreliable or costly, some works assume that agents don’t
    communicate with each other. For example, wireless heterogeneous networks [[96](#biba.bibx26),
    [137](#biba.bibx67), [133](#biba.bibx63), [100](#biba.bibx30), [82](#biba.bibx12)],
    cognitive radio (CR) networks [[88](#biba.bibx18), [126](#biba.bibx56)], wireless
    cellular networks [[131](#biba.bibx61), [77](#biba.bibx7), [97](#biba.bibx27),
    [127](#biba.bibx57), [93](#biba.bibx23)], UAV networks[[134](#biba.bibx64), [135](#biba.bibx65),
    [108](#biba.bibx38), [114](#biba.bibx44), [75](#biba.bibx5)], vehicular networks
    [[98](#biba.bibx28), [104](#biba.bibx34)], and IoT [[115](#biba.bibx45), [111](#biba.bibx41)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 代理之间的信息交互可以帮助系统中的每个代理更好地理解全球环境的变化以及其他代理对环境的影响，从而提高系统的整体性能。此外，代理通过共享信息如观察、行动或奖励值，可以有效实现更好的合作，从而实现更好的团队表现
    [[78](#biba.bibx8)]。在无线通信环境中，由于通信可能不可靠或成本高，一些研究假设代理之间不进行通信。例如，无线异构网络 [[96](#biba.bibx26),
    [137](#biba.bibx67), [133](#biba.bibx63), [100](#biba.bibx30), [82](#biba.bibx12)]、认知无线电（CR）网络
    [[88](#biba.bibx18), [126](#biba.bibx56)]、无线蜂窝网络 [[131](#biba.bibx61), [77](#biba.bibx7),
    [97](#biba.bibx27), [127](#biba.bibx57), [93](#biba.bibx23)]、无人机网络 [[134](#biba.bibx64),
    [135](#biba.bibx65), [108](#biba.bibx38), [114](#biba.bibx44), [75](#biba.bibx5)]、车载网络
    [[98](#biba.bibx28), [104](#biba.bibx34)] 和物联网 [[115](#biba.bibx45), [111](#biba.bibx41)]。
- en: 'In this survey, we only focus on the existing applications of MADRL in network
    management with agent communication. This section categorizes works with agent
    communication by their application: traffic engineering, network spectrum access,
    transmit power control, and network security.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们仅关注具有代理通信的MADRL在网络管理中的现有应用。本节根据代理通信的应用对相关工作进行分类：交通工程、网络频谱访问、发射功率控制和网络安全。
- en: II-A Traffic Engineering
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 交通工程
- en: Traffic engineering refers to optimizing routing paths for traffic flows based
    on their characteristics and balancing the load between different switches, routers,
    and links in the network. This requires the system to achieve dynamic and real-time
    monitoring, analysis, control, and prediction of traffic status on the network
    [[123](#biba.bibx53)]. Traditional traffic engineering methods or routing protocols
    are defined by fixed rules [[112](#biba.bibx42)], which make it challenging to
    achieve the autonomous monitoring and dynamic network management requirements
    in the network. Nevertheless, MADRL methods enable each network entity, as an
    agent, to independently learn an adaptive routing strategy and dynamically change
    its routing rules. Furthermore, the MADRL system with agent communication enables
    agents to make dynamic decisions that consider the current network status and
    the impact of other agent routing policies. Therefore, MADRL, especially with
    agents’ communication, might be an effective solution for traffic engineering
    in future network management.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 流量工程指的是根据流量特性优化路由路径，并在网络中的不同交换机、路由器和链路之间平衡负载。这需要系统实现对网络流量状态的动态和实时监控、分析、控制和预测
    [[123](#biba.bibx53)]。传统的流量工程方法或路由协议由固定规则定义 [[112](#biba.bibx42)]，这使得在网络中实现自主监控和动态网络管理要求变得具有挑战性。然而，MADRL方法使每个网络实体作为代理独立学习适应性路由策略，并动态地更改其路由规则。此外，具有代理通信的MADRL系统使得代理可以做出考虑当前网络状态和其他代理路由策略影响的动态决策。因此，MADRL，特别是有代理通信的情况，可能是未来网络管理中流量工程的有效解决方案。
- en: The authors of [[138](#biba.bibx68)] analyze and verify the improvement of agents’
    communication on Autonomous Systems (ASs) throughput. They propose a MADRL routing
    method for ASs, where each AS on the Internet acts as an agent. ASs select the
    best next-hop AS for different flows to maximize system throughput, according
    to the observation of oneself and adjacent agents. The authors test the system
    performance when the agent communicates with neighbor agents within different
    ranges. The simulation results show that the system with agent communication performs
    better than the system where agents only use their local observations to make
    discussions. Moreover, improving the communication ranges results in an increase
    in the system’s average throughput. The author points out that interacting with
    more agents can help agents better understand the network state and reduce the
    impact of non-stationary issues in the distributed MADRL system.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[[138](#biba.bibx68)] 的作者分析和验证了自主系统（ASs）中代理通信对吞吐量的改进。他们提出了一种用于ASs的MADRL路由方法，其中互联网中的每个AS充当一个代理。ASs根据对自身和邻近代理的观察，选择不同流的最佳下一跳AS以最大化系统吞吐量。作者测试了代理在不同范围内与邻近代理通信时的系统性能。模拟结果表明，代理通信的系统比仅依赖本地观察进行讨论的系统表现更好。此外，改进通信范围会导致系统的平均吞吐量增加。作者指出，与更多代理互动可以帮助代理更好地理解网络状态，并减少分布式MADRL系统中非平稳问题的影响。'
- en: 'In [[71](#biba.bibx1)], the authors design two communication mechanisms for
    the packet routing problem: value sharing and model sharing. Agents establish
    communication connections through signaling and sharing their Deep Neural Networks
    (DNN) models (model sharing) or exchange estimates of end-to-end delay (value
    sharing) with neighboring agents. Regarding packet loss ratio and average end-to-end
    packet delay, model sharing is always better than value sharing when replay-memory
    sizes are large enough.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[71](#biba.bibx1)] 中，作者为数据包路由问题设计了两种通信机制：值共享和模型共享。代理通过信号传递建立通信连接，并共享他们的深度神经网络（DNN）模型（模型共享）或与邻近代理交换端到端延迟的估计（值共享）。在数据包丢失率和平均端到端数据包延迟方面，当回放记忆的大小足够大时，模型共享总是优于值共享。
- en: In [[87](#biba.bibx17)], graph convolution reinforcement learning is used to
    adapt to the dynamics of the underlying network graph in multi-agent environments.
    Unlike the traditional MADRL approach, it regards each data packet in the network
    rather than the router as an agent. The packet as the agent is a node in the graph,
    and the local observation encoding of the agent is a node feature. Agents formulate
    cooperation policies by using convolutional neural networks (CNNs) to learn the
    feature information of nodes from the communication between agents. The experiment
    shows that compared to the routing algorithm using the Deep Q network, the routing
    algorithm that learns to encode and communicate messages through Graph Neural
    Networks (GNN) performs better in system throughput and latency.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[87](#biba.bibx17)]中，图卷积强化学习被用来适应多代理环境中底层网络图的动态。与传统的MADRL方法不同，它将网络中的每个数据包而非路由器视为代理。数据包作为代理是图中的一个节点，代理的局部观察编码是节点特征。代理通过使用卷积神经网络（CNN）从代理之间的通信中学习节点的特征信息来制定合作策略。实验表明，与使用深度Q网络的路由算法相比，通过图神经网络（GNN）学习编码和通信消息的路由算法在系统吞吐量和延迟方面表现更好。
- en: In addition, several works allow information exchange among agents but do not
    discuss the impact of communication between agents on the system. For example,
    In [[129](#biba.bibx59)], the authors propose Deep Q-routing, based on the Deep
    Q network, to reduce the average transmission time of packets in Autonomous Systems.
    The network model is represented as a directed graph. The routers, which are the
    nodes on the network graph, act as agents. Routers choose one of the neighboring
    agents as the next hop node for the data packet based on their local observations
    and neighbors’ information.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究允许代理之间的信息交换，但没有讨论代理之间的通信对系统的影响。例如，在[[129](#biba.bibx59)]中，作者提出了基于深度 Q
    网络的 Deep Q-routing，以减少自主系统中数据包的平均传输时间。网络模型表示为一个有向图。路由器，即网络图上的节点，充当代理。路由器根据其本地观察和邻居的信息选择一个邻近的代理作为数据包的下一跳节点。
- en: The study presented in [[74](#biba.bibx4)] has introduced a traffic allocation
    plan for the Crowd-sourced Licast services (CLS) system. The authors use the Augmented
    Graph Model to model a large-scale CLS system as a multi-hop routing problem.
    As an agent, the CLS system’s network node determines the flow path in the network
    depending on observation and the other agents’ local rewards.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[[74](#biba.bibx4)]中介绍了一种针对众包Licast服务（CLS）系统的流量分配方案。作者使用增强图模型将大规模CLS系统建模为一个多跳路由问题。作为代理，CLS系统的网络节点根据观察和其他代理的局部奖励来确定网络中的流量路径。'
- en: The authors of [[101](#biba.bibx31)] found that designing a single reward function
    can lead to agents becoming lazy or selfish. Thus, the authors designed a delay-tolerant
    global and local reward function. By capturing information from neighboring agents
    during training and implicitly sharing packet TTL during execution, a higher level
    of cooperation between agents is achieved in the SDN environment.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[[101](#biba.bibx31)]的作者发现，设计单一的奖励函数可能导致代理变得懒惰或自私。因此，作者设计了一个延迟容忍的全局和局部奖励函数。通过在训练过程中捕获来自邻近代理的信息，并在执行过程中隐式地共享数据包TTL，在SDN环境中实现了更高水平的代理合作。'
- en: Work [[132](#biba.bibx62)] proposes a MADRL routing algorithm in a mixed-distributed
    IoT system. Agents collect and exchange self-observation information containing
    queue length and remaining energy with other agents to maximize the total amount
    of data transmission for long-term goals while reducing device energy consumption.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[[132](#biba.bibx62)]提出了一种在混合分布式物联网系统中的MADRL路由算法。代理收集并与其他代理交换包含队列长度和剩余能量的自我观察信息，以最大化长期目标的数据传输总量，同时减少设备能耗。'
- en: II-B Network Spectrum Access
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 网络频谱访问
- en: The network spectrum access method aims to improve spectrum efficiency by allocating
    spectrum resources reasonably and avoiding quality of service (QoS) reduction
    caused by competition among network entities for spectrum resources. The algorithm
    based on MARL can enable network entities to adaptively select spectrum resources
    based on environmental changes, and is therefore widely used. Agents in the MARL
    system typically make decisions to improve system spectrum utilization and throughput
    by collecting and sharing information such as channel state information (CSI),
    QoS, etc. The information exchange between agents can help them perceive the network
    status and make better channel allocation policies.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 网络频谱访问方法旨在通过合理分配频谱资源并避免网络实体之间为频谱资源竞争而导致的服务质量（QoS）降低来提高频谱效率。基于MARL的算法可以使网络实体能够根据环境变化自适应地选择频谱资源，因此被广泛应用。MARL系统中的代理通常通过收集和共享信道状态信息（CSI）、QoS等信息来做出决策，以改进系统的频谱利用率和吞吐量。代理之间的信息交换可以帮助它们感知网络状态并制定更好的信道分配策略。
- en: 'The authors of [[76](#biba.bibx6)] argue that the different types of agent
    information exchange can impact the MADRL dynamic spectrum access system. They
    explore a MADRL algorithm that maximizes the information rate of secondary users
    while ensuring that the primary user’s spectrum usage is not affected. Moreover,
    the performance of three agent communication models is tested: (1) agents only
    exchanging reward information, (2) agents exchanging reward and state, and (3)
    agents exchanging reward, state, and action. The result of the experiment shows
    that as the types of message parameters exchanged between agents increase, the
    system can adapt more quickly in the spectrum environment and avoid more channel
    conflicts. Moreover, more information exchange can also improve the sum information
    rate of the secondary users.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[[76](#biba.bibx6)]的作者认为不同类型的代理信息交换会影响MADRL动态频谱访问系统。他们探索了一种MADRL算法，该算法在确保不影响主用户频谱使用的同时，最大化了次级用户的信息速率。此外，他们还测试了三种代理通信模型的性能：(1)
    仅交换奖励信息的代理，(2) 交换奖励和状态的代理，以及(3) 交换奖励、状态和行为的代理。实验结果显示，随着代理之间交换的消息参数类型的增加，系统可以更快地适应频谱环境并避免更多的信道冲突。此外，更多的信息交换还可以提高次级用户的总信息速率。'
- en: The authors in [[119](#biba.bibx49)] propose a communication information pre-processing
    method for agent communication. In [[119](#biba.bibx49)], each vehicle in the
    vehicle networking system acts as an agent, transmitting its local CSI to the
    base station, which is the central controller. The central controller integrates
    the messages and sends local agents the vehicle-to-vehicle (V2V) link channel
    allocation policies to maximize the system’s throughput. To avoid the significant
    signaling overhead that may arise from the instantaneous global CSI collected
    [[94](#biba.bibx24)], each local agent learns to compress the CSI with an independent
    DNN.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[[119](#biba.bibx49)]的作者提出了一种用于代理通信的通信信息预处理方法。在[[119](#biba.bibx49)]中，车辆网络系统中的每辆车都充当了一个代理，将其本地CSI传输到作为中央控制器的基站。中央控制器集成消息并将车辆之间的链路（V2V）频道分配策略发送给本地代理，以最大化系统的吞吐量。为了避免由瞬态全局CSI收集所导致的显著信令开销，每个本地代理通过独立的DNN学习压缩CSI。'
- en: The authors in [[81](#biba.bibx11)] design a MADRL-based method to maximize
    vehicle networks’ packet reception rate (PRR) in cellular-free scenarios. Each
    vehicle agent independently selects spectrum resources based on its state and
    messages exchanged with other agents through V2V links.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[[81](#biba.bibx11)]的作者设计了一种基于MADRL的方法，以在无蜂窝网络场景中最大化车辆网络的数据包接收率（PRR）。每个车辆代理根据自身状态和通过V2V链路与其他代理交换的消息独立选择频谱资源。'
- en: Different for [[81](#biba.bibx11)], [[122](#biba.bibx52)] proposes a decentralized
    spectrum access algorithm for cellular vehicle-to-everything (C-V2X) networks
    to maximize the total throughput of vehicle-to-infrastructure (V2I) users. Agents
    in [[122](#biba.bibx52)] learn communication through dedicated channels using
    a DNN-based message generator module (MGM) independent of action selection.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[81](#biba.bibx11)]不同，[[122](#biba.bibx52)]提出了一种用于车联网基础设施（C-V2X）网络的分布式频谱访问算法，以最大化车辆对基础设施（V2I）用户的总吞吐量。[[122](#biba.bibx52)]中的代理使用基于DNN的消息生成器模块(MGM)通过专用信道进行通信，与动作选择无关。
- en: '[[139](#biba.bibx69)] proposed a joint optimization scheme for mode selection
    and channel allocation in Device-to-Device (D2D) Heterogeneous Cellular Networks
    based on MADRL for millimeter wave and cellular frequency bands. The authors studied
    the interference problem caused by spectrum sharing between users of different
    cellular networks. To ensure the quality of service (QoS) of each D2D user, the
    agents exchange their satisfaction state information on QoS constraints to maximize
    user satisfaction.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[[139](#biba.bibx69)] 提出了一个基于 MADRL 的设备到设备（D2D）异构蜂窝网络中模式选择和频道分配的联合优化方案，适用于毫米波和蜂窝频段。作者研究了不同蜂窝网络用户之间频谱共享引起的干扰问题。为了确保每个
    D2D 用户的服务质量（QoS），智能体交换其在 QoS 约束下的满意度状态信息，以最大化用户满意度。'
- en: The MADRL system in [[107](#biba.bibx37)] implemented a two-level unlicensed
    spectrum access framework consisting of a feedback cycle and an execution cycle.
    The base stations, as agents, transmit data to unlicensed spectrums to alleviate
    the pressure on cellular networks. The MADRL system is designed as a multi-agent
    game model, where agents share their state and reward information through broadcast
    to achieve the system’s Nash equilibrium and maximize the network’s total throughput.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[[107](#biba.bibx37)] 中的 MADRL 系统实现了一个由反馈周期和执行周期组成的两级无许可证频谱访问框架。基站作为智能体，将数据传输到无许可证频谱，以缓解蜂窝网络的压力。MADRL
    系统被设计为一个多智能体博弈模型，智能体通过广播共享其状态和奖励信息，以实现系统的纳什均衡，并最大化网络的总吞吐量。'
- en: II-C Transmit Power Control
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 传输功率控制
- en: In wireless networks, network entities typically need to control transmission
    power to reduce interference with other network entities. In the MADRL system,
    agents can adaptively select transmit power based on the observation of the environmental
    changes, which is considered an effective tool for solving power control problems.
    Moreover, through communication, agents can obtain the interference they cause
    to other agents or the power allocation of other agents to find a balance between
    improving transmit power and reducing interference.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在无线网络中，网络实体通常需要控制传输功率，以减少与其他网络实体的干扰。在 MADRL 系统中，智能体可以根据环境变化的观察自适应地选择传输功率，这被认为是解决功率控制问题的有效工具。此外，通过通信，智能体可以获得它们对其他智能体造成的干扰或其他智能体的功率分配，以在提高传输功率和减少干扰之间找到平衡。
- en: '[[90](#biba.bibx20)] proposed a power adaptive allocation algorithm based on
    reinforcement learning in multi-user cellular networks. The agent is the base
    station (BS), and the state of each BS is the local CSI and the power allocation
    of the previous time step. BSs exchange their power allocation information to
    improve the network throughput.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[[90](#biba.bibx20)] 提出了一个基于强化学习的多用户蜂窝网络中的功率自适应分配算法。智能体是基站（BS），每个 BS 的状态是本地
    CSI 和上一个时间步的功率分配。BS 交换其功率分配信息，以提高网络吞吐量。'
- en: '[[136](#biba.bibx66)] designs a power control algorithm for a cellular vehicle
    network based on MADRL. The system consists of a BS and multiple vehicle user
    equipment (VUEs) covered by that BS, where the agent is active V2V link. Agents
    reduce channel interference by sharing their channel selection from the previous
    time step with neighbors, maximizing the total capacity of the V2I link.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[[136](#biba.bibx66)] 设计了一种基于 MADRL 的蜂窝车载网络功率控制算法。该系统由一个基站和多个由该基站覆盖的车辆用户设备（VUEs）组成，其中智能体是活动的
    V2V 链路。智能体通过与邻居共享之前时间步的频道选择来减少频道干扰，从而最大化 V2I 链路的总容量。'
- en: The authors of [[102](#biba.bibx32)] proposed a radio resource management algorithm
    based on MADRL, where each Access Point (AP) is an agent, and each AP connects
    multiple user equipment devices (UEs). The data is adaptively transmitted to the
    associated UEs based on SNR and weight. APs exchange observations with neighboring
    agents, which are the weights and SNR of UEs. In addition to direct communication
    between APs, agents also receive feedback reports from UEs to understand the information
    of other APs, which has an unavoidable delay.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[[102](#biba.bibx32)] 的作者提出了一种基于 MADRL 的无线电资源管理算法，其中每个接入点（AP）都是一个智能体，每个 AP
    连接多个用户设备（UEs）。数据根据 SNR 和权重自适应地传输到相关的 UEs。AP 与邻近智能体交换观察信息，即 UEs 的权重和 SNR。除了 AP
    之间的直接通信外，智能体还接收来自 UEs 的反馈报告，以了解其他 AP 的信息，这不可避免地存在延迟。'
- en: In multi-user downlink small cell networks, traditional cooperative resource
    allocation (RA) requires collecting global CSI to calculate SNR, which is difficult
    to achieve in network environments. To address the difficulty of collecting global
    CSI in practical networks with limited direct link capacity, [[85](#biba.bibx15)]
    proposes a power control algorithm based on MADRL for small cell clusters. Small
    cell BSs as agents only use local CSI at the transmitters and exchange in-cell
    sum rate through direct links to maximize the system sum rate.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在多用户下行小型蜂窝网络中，传统的协作资源分配（RA）需要收集全局 CSI 以计算 SNR，这在网络环境中很难实现。为了应对在具有有限直接链路容量的实际网络中收集全局
    CSI 的困难，[[85](#biba.bibx15)] 提出了基于 MADRL 的小型蜂窝集群功率控制算法。小型蜂窝基站作为代理仅使用发送机的本地 CSI，并通过直接链路交换单元内的总速率，以最大化系统的总速率。
- en: To overcome the cross-layer signal interference problem between small and macro
    cells, the authors of [[124](#biba.bibx54)] designed a penalty-based Q-learning
    algorithm. By introducing regularization terms into the loss function, agents
    are encouraged to choose experiential actions with high global rewards to promote
    cooperation between agents. Agents share local observations through wired and
    wireless backhaul links to achieve balanced power allocation policies.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决小型和宏蜂窝之间的跨层信号干扰问题，[[124](#biba.bibx54)] 的作者设计了一种基于惩罚的 Q 学习算法。通过在损失函数中引入正则化项，鼓励代理选择具有高全局奖励的经验性行动，以促进代理之间的合作。代理通过有线和无线回传链路共享本地观察，以实现平衡的功率分配策略。
- en: In [[103](#biba.bibx33)], the authors introduce an interference sorting technique
    to handle interference information in dynamic power allocation in wireless networks.
    In HetNet with multiple APs and users, the interference sources of the transmitter
    are classified based on the received power of the corresponding interference sources
    at the receiver. Each transmitter (agent) not only collects CSI and QoS information
    from neighboring agents but also exchanges status information, such as transmit
    power interference through communication, and adjusts its transmit power accordingly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[103](#biba.bibx33)] 中，作者介绍了一种干扰排序技术，以处理无线网络中动态功率分配的干扰信息。在拥有多个 AP 和用户的 HetNet
    中，发送机的干扰源是根据接收机接收到的相应干扰源的功率进行分类的。每个发送机（代理）不仅收集来自邻近代理的 CSI 和 QoS 信息，还通过通信交换状态信息，如发射功率干扰，并据此调整其发射功率。
- en: '[[80](#biba.bibx10)] implementing collaborative power control and resource
    management in a multi-user downlink small cell network using MADRL. The optimal
    power allocation strategy for joint sub-carriers in IoT systems is learned through
    a dual deep Q network algorithm without complete instantaneous CSI. The adjacent
    agents share information regarding spectral efficiency, channel gain, and received
    power.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[[80](#biba.bibx10)] 在多用户下行小型蜂窝网络中实现了使用 MADRL 的协作功率控制和资源管理。通过双重深度 Q 网络算法学习
    IoT 系统中联合子载波的最佳功率分配策略，而无需完整的瞬时 CSI。相邻的代理共享关于频谱效率、信道增益和接收功率的信息。'
- en: II-D Network Security
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 网络安全
- en: Network security is a technology that protects network entities and information
    from malicious attacks. Common challenges include jamming attacks, distributed
    denial of service (DDoS) attacks, etc. The jamming attack refers to attackers
    interfering with legitimate communication channels by sending interference signals.
    DDoS is a distributed cyber attack aimed at depleting the network resources of
    the target system. Recently, the joint network attack defense method based on
    MADRL for multiple network entities has been extensively studied.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全是一种保护网络实体和信息免受恶意攻击的技术。常见的挑战包括干扰攻击、分布式拒绝服务（DDoS）攻击等。干扰攻击是指攻击者通过发送干扰信号来干扰合法的通信通道。DDoS
    是一种分布式网络攻击，旨在耗尽目标系统的网络资源。最近，基于 MADRL 的多网络实体联合网络攻击防御方法已得到广泛研究。
- en: '[[128](#biba.bibx58)] uses a multi-agent Q-learning algorithm to learn distributed
    anti-jamming strategies for each agent. A jammer in the system initiates jamming
    attacks on one of the channels each time. Meanwhile, agents choose the same channel,
    causing co-channel interference. The agent is the legitimate user, each agent
    exchanges Q-values and sums them up to select the joint action that can maximize
    the sum of Q-values.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[[128](#biba.bibx58)] 使用多智能体 Q 学习算法来学习每个智能体的分布式抗干扰策略。系统中的干扰源每次对其中一个频道发起干扰攻击。同时，智能体选择相同的频道，导致共频道干扰。作为合法用户的智能体，每个智能体交换
    Q 值并将其相加，以选择能够最大化 Q 值总和的联合行动。'
- en: The authors in [[125](#biba.bibx55)] propose a collaborative anti-jamming algorithm
    based on multi-agent Q-learning in UAV communication networks. As agents, the
    UAV group users are usually in a competitive relationship without communication.
    When the agent perceives that the energy of the co-channel interference signal
    exceeds the threshold, the agent determines that they have been affected by the
    co-channel jamming, and the agents in the system switch to cooperative mode. In
    the cooperative mode, agents share their Q table to output joint action to counteract
    jamming attacks and maximize the user utility.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[[125](#biba.bibx55)] 的作者提出了一种基于多智能体 Q 学习的协作抗干扰算法，用于无人机通信网络。作为智能体的无人机组用户通常处于没有通信的竞争关系中。当智能体感知到共频道干扰信号的能量超过阈值时，智能体判断自己已经受到共频道干扰，系统中的智能体将切换到协作模式。在协作模式下，智能体共享它们的
    Q 表以输出联合行动，以对抗干扰攻击并最大化用户效用。'
- en: The authors in [[121](#biba.bibx51)] use a centralized MADRL system based on
    a hierarchical communication mechanism to defend against DDoS attacks. Each router,
    as a local agent, sends its traffic reading to the central router, which then
    decides the throttling rate for each router. To reduce the huge communication
    costs caused by the frequent exchange of information with the central agent, each
    local agent adds a deep deterministic policy gradient network to determine whether
    to send local information to the central agent.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[[121](#biba.bibx51)] 的作者使用基于分层通信机制的集中式 MADRL 系统来防御 DDoS 攻击。每个路由器作为局部智能体，将其流量读数发送到中央路由器，中央路由器随后决定每个路由器的节流率。为了减少因与中央智能体频繁交换信息而造成的巨大通信成本，每个局部智能体添加了深度确定性策略梯度网络，以确定是否将本地信息发送给中央智能体。'
- en: III Communication Schemes
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 通信方案
- en: The communication schemes refer to the learning and execution schemes of the
    MADRL system, which can be classified into fully centralized learning and execution,
    centralized training and distributed execution, and fully distributed execution.
    Different learning and execution schemes will be selected according to the needs
    of different application scenarios.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通信方案指的是 MADRL 系统的学习和执行方案，可以分为完全集中式学习和执行、集中式训练与分布式执行以及完全分布式执行。不同的学习和执行方案将根据不同应用场景的需求进行选择。
- en: The summary is shown in Table III.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要见表 III。
- en: TABLE III
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III
- en: The Category of Communication Schemes
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通信方案类别
- en: '| Types | Traffic Engineering | Network Access | Power Control | Network Security
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 流量工程 | 网络接入 | 功率控制 | 网络安全 |'
- en: '| Fully Centralized | - | [[119](#biba.bibx49)]  [[113](#biba.bibx43)] | -
    | [[121](#biba.bibx51)] |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 完全集中式 | - | [[119](#biba.bibx49)]  [[113](#biba.bibx43)] | - | [[121](#biba.bibx51)]
    |'
- en: '| CTDE | [[129](#biba.bibx59)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[81](#biba.bibx11)]  [[122](#biba.bibx52)] | [[90](#biba.bibx20)]  [[136](#biba.bibx66)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CTDE | [[129](#biba.bibx59)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[81](#biba.bibx11)]  [[122](#biba.bibx52)] | [[90](#biba.bibx20)]  [[136](#biba.bibx66)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
- en: '| Fully Distributed | [[138](#biba.bibx68)]  [[89](#biba.bibx19)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[74](#biba.bibx4)]  [[71](#biba.bibx1)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | [[139](#biba.bibx69)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)] | [[103](#biba.bibx33)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]  [[124](#biba.bibx54)]
    | - |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 完全分布式 | [[138](#biba.bibx68)]  [[89](#biba.bibx19)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[74](#biba.bibx4)]  [[71](#biba.bibx1)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | [[139](#biba.bibx69)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)] | [[103](#biba.bibx33)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]  [[124](#biba.bibx54)]
    | - |'
- en: III-A Fully Centralized
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 完全集中式
- en: In the fully centralized scheme, local agents report their observations to a
    central agent, which decides what local agents should execute. The central agent
    can effectively output the globally optimal policy by utilizing instantaneous
    global state information.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全集中方案中，地方代理将他们的观察结果报告给中央代理，中央代理决定地方代理应该执行什么。中央代理可以通过利用瞬时全球状态信息，有效地输出全球最优策略。
- en: For example, the traditional dynamic spectrum access algorithm usually relies
    on global CSI [[120](#biba.bibx50), [91](#biba.bibx21)], while the MADRL dynamic
    spectrum access algorithm, such as [[119](#biba.bibx49)], evolved from traditional
    methods, adopts a fully centralized structure to enable the central agent to achieve
    adaptive channel adjustment based on global CSI received from the local agents.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，传统的动态频谱接入算法通常依赖于全球CSI [[120](#biba.bibx50), [91](#biba.bibx21)]，而MADRL动态频谱接入算法，如[[119](#biba.bibx49)]，从传统方法演变而来，采用了完全集中结构，以使中央代理能够根据来自地方代理的全球CSI实现自适应频道调整。
- en: By collecting global instantaneous state information, a fully centralized model
    can effectively overcome non-stationary problems. However, frequent interaction
    between all local agents and the central agent may lead to high communication
    costs. In addition, communication delays may result in the central control being
    unable to collect complete instantaneous global states, thereby affecting agent
    decision-making.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过收集全球瞬时状态信息，完全集中模型可以有效克服非平稳问题。然而，所有地方代理与中央代理之间的频繁交互可能导致高通信成本。此外，通信延迟可能导致中央控制无法收集完整的瞬时全球状态，从而影响代理的决策。
- en: III-B Centralized Training and Distributed Execution
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 集中训练和分布式执行
- en: Centralized Training and Distributed Execution (CTDE) allows agents to use non-immediate
    global information for centralized learning, and make their own decisions independently
    during the execution phase. Compared with the fully centralized system, the MADRL
    system with the CTDE framework usually has less communication cost during execution
    and has been widely used in network management. Additionally, compared with distributed
    learning, agents learning a shared policy can reduce training parameters and accelerate
    convergence speed [[83](#biba.bibx13)]. Therefore, the CTDE framework has been
    widely used in network management. However, centralized training may lead to poor
    scalability. Changes in network topology may require all agents to be retrained
    in a centralized training system.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 集中训练和分布式执行（CTDE）允许代理使用非即时全球信息进行集中学习，并在执行阶段独立做出决策。与完全集中系统相比，具有CTDE框架的MADRL系统在执行阶段通常具有较低的通信成本，并且在网络管理中得到了广泛应用。此外，与分布式学习相比，学习共享策略的代理可以减少训练参数并加速收敛速度[[83](#biba.bibx13)]。因此，CTDE框架在网络管理中得到了广泛应用。然而，集中训练可能导致可扩展性差。网络拓扑的变化可能需要在集中训练系统中重新训练所有代理。
- en: III-C Fully Distributed
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 完全分布式
- en: In the fully distributed scheme, each agent is trained with an independent network.
    Similar to CTDE, agents can overcome non-stationarity during the execution phase
    by exchanging messages with other agents. Compared to centrally trained models,
    distributed training models have better flexibility and scalability, thereby have
    been used in networks built on mobile network entities, such as [[89](#biba.bibx19)]
    [[95](#biba.bibx25)] [[92](#biba.bibx22)] [[101](#biba.bibx31)] [[109](#biba.bibx39)]
    [[103](#biba.bibx33)], or heterogeneous networks composed of different network
    entities, such as [[139](#biba.bibx69)] [[124](#biba.bibx54)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全分布式方案中，每个代理使用独立的网络进行训练。类似于CTDE，代理可以通过与其他代理交换消息来克服执行阶段的非平稳性。与集中训练模型相比，分布式训练模型具有更好的灵活性和可扩展性，因此被广泛应用于基于移动网络实体构建的网络，如[[89](#biba.bibx19)]
    [[95](#biba.bibx25)] [[92](#biba.bibx22)] [[101](#biba.bibx31)] [[109](#biba.bibx39)]
    [[103](#biba.bibx33)]，或由不同网络实体组成的异构网络，如[[139](#biba.bibx69)] [[124](#biba.bibx54)]。
- en: The fully distributed learning and execution schemes align with the trend of
    future network management toward distribution and decentralization. However, each
    agent must be trained through an independent network, which can result in higher
    training costs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 完全分布式的学习和执行方案符合未来网络管理向分布式和去中心化方向的发展趋势。然而，每个代理必须通过独立的网络进行训练，这可能会导致更高的训练成本。
- en: IV Communication Content
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 通信内容
- en: The communication content refers to what information is encoded in the communication
    messages. The communication messages may generally be the agent’s state, action,
    reward, or strategy. This information may be used to assist agents in completing
    their perception of environmental changes, learning policies from other agents,
    or promoting collaboration between agents. Agents in different systems interact
    with various types of information based on diverse application scenarios. The
    summary is shown in Table IV.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通信内容指的是通信消息中编码的信息。通信消息通常可能包括代理的状态、行动、奖励或策略。这些信息可以帮助代理完成对环境变化的感知、从其他代理处学习策略，或促进代理之间的合作。不同系统中的代理根据不同的应用场景互动各种类型的信息。总结见表
    IV。
- en: TABLE IV
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV
- en: The Category of Communication Content
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通信内容类别
- en: '| Types | Traffic Engineering | Network Access | Power Control | Network Security
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 交通工程 | 网络接入 | 功率控制 | 网络安全 |'
- en: '| State | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[81](#biba.bibx11)]  [[139](#biba.bibx69)]  [[122](#biba.bibx52)] | [[90](#biba.bibx20)]  [[103](#biba.bibx33)]  [[136](#biba.bibx66)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]
    | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[81](#biba.bibx11)]  [[139](#biba.bibx69)]  [[122](#biba.bibx52)] | [[90](#biba.bibx20)]  [[103](#biba.bibx33)]  [[136](#biba.bibx66)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]
    | - |'
- en: '| Reward | [[74](#biba.bibx4)]  [[89](#biba.bibx19)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | - | [[124](#biba.bibx54)] | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | [[74](#biba.bibx4)]  [[89](#biba.bibx19)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | - | [[124](#biba.bibx54)] | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
- en: '| Mixed & Other | [[71](#biba.bibx1)] | [[119](#biba.bibx49)]  [[113](#biba.bibx43)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)]
    | - | [[121](#biba.bibx51)] |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 混合与其他 | [[71](#biba.bibx1)] | [[119](#biba.bibx49)]  [[113](#biba.bibx43)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)]
    | - | [[121](#biba.bibx51)] |'
- en: IV-A State or observation
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 状态或观察
- en: Agents can compensate for their partial knowledge of the environment by exchanging
    observation (partial state) or complete state information, and its specific parameters
    are usually closely related to the application scenario and optimization goal
    of the MADRL system.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以通过交换观察（部分状态）或完整状态信息来弥补对环境的部分了解，其具体参数通常与MADRL系统的应用场景和优化目标密切相关。
- en: For example, in research on traffic engineering, agents learn to select the
    next hop for data packets or flows, in order to improve system throughput, reduce
    transmission delay, and avoid congestion. Therefore, to minimize the average transmission
    time of each data packet, in [[129](#biba.bibx59)] and [[132](#biba.bibx62)],
    agents exchange queue length with neighbors.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在交通工程研究中，代理学习选择数据包或流的下一个跳点，以提高系统吞吐量，减少传输延迟，并避免拥塞。因此，为了最小化每个数据包的平均传输时间，在[[129](#biba.bibx59)]和[[132](#biba.bibx62)]中，代理与邻居交换队列长度。
- en: In [[138](#biba.bibx68)], the objective of the research is to enhance the average
    throughput performance of the system that comprises ASs acting as agents. The
    agents collaborate by sharing their observations including the current flow in
    the agent, the maximum number of flows, the number of neighboring agents, and
    the throughput of the flow.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[138](#biba.bibx68)]中，研究的目标是提升包含作为代理的AS的系统的平均吞吐性能。代理通过共享他们的观察结果进行合作，包括代理中的当前流、最大流数、邻居代理的数量和流的吞吐量。
- en: Additionally, traffic engineering in wireless communication would consider the
    link stability or data validation. For example, the research on unmanned space
    self-organizing network [[92](#biba.bibx22)] shares the effective data payload
    with the next hop agent. In research on underwater wireless sensor networks [[95](#biba.bibx25)],
    communication messages are designed to reflect agent energy and link stability
    to reduce sensor energy use and extend the network life cycle.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，无线通信中的交通工程还需考虑链路稳定性或数据验证。例如，无人空间自组织网络的研究[[92](#biba.bibx22)]与下一个跳点代理共享有效的数据负载。在水下无线传感器网络的研究[[95](#biba.bibx25)]中，通信消息设计用来反映代理的能量和链路稳定性，以减少传感器的能量使用并延长网络生命周期。
- en: '[[87](#biba.bibx17)] propose Graph Convolutional Reinforcement Learning in
    a graph representation multi-agent system to achieve adaptive routing. The agent
    is a data packet, and the characteristics of the nodes on the graph serve as the
    state information of the agent, including the packet’s current location, destination,
    data size, link load, and the number of adjacent data packets. Agents use GNN
    to learn the encoding of state messages and share them with neighboring agents.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[[87](#biba.bibx17)] 提出了图卷积强化学习（Graph Convolutional Reinforcement Learning），用于图表示的多代理系统中实现自适应路由。代理是数据包，图中节点的特征作为代理的状态信息，包括数据包的当前位置、目的地、数据大小、链路负载和相邻数据包的数量。代理使用GNN学习状态消息的编码，并与邻近代理共享。'
- en: Research on transmit power control in wireless communication networks usually
    aims to improve the transmission rate while reducing the impact of interference
    between network entities on the system. At the same time, adjusting the power
    allocation ratio improves the overall network throughput or reduces energy consumption.
    Therefore, the state information interacted by agents in related research includes
    power allocation ratio [[90](#biba.bibx20)], interference from other agents [[103](#biba.bibx33)],
    SNR [[102](#biba.bibx32)], channel gain [[103](#biba.bibx33), [80](#biba.bibx10)],
    received power [[80](#biba.bibx10)], and CSI [[124](#biba.bibx54)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 无线通信网络中的发射功率控制研究通常旨在提高传输速率，同时减少网络实体间干扰对系统的影响。与此同时，调整功率分配比例可以提高整体网络吞吐量或减少能耗。因此，相关研究中代理交互的状态信息包括功率分配比例[[90](#biba.bibx20)]、其他代理的干扰[[103](#biba.bibx33)]、信噪比（SNR）[[102](#biba.bibx32)]、信道增益[[103](#biba.bibx33),
    [80](#biba.bibx10)]、接收功率[[80](#biba.bibx10)]和CSI[[124](#biba.bibx54)]。
- en: The network spectrum access method needs to avoid excessive competition for
    spectrum resources among network entities and choose a balanced spectrum resource
    allocation policy. Therefore, agents inform other agents of channel occupancy
    information [[139](#biba.bibx69)] or interference [[122](#biba.bibx52)] through
    communication. Moreover, the MADRL system in [[139](#biba.bibx69)] aims to maximize
    user satisfaction, thereby, the agents also exchange their satisfaction with QoS
    constraints with each other.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 网络频谱接入方法需要避免网络实体之间过度竞争频谱资源，并选择一种平衡的频谱资源分配策略。因此，代理通过通信向其他代理通报信道占用信息[[139](#biba.bibx69)]或干扰[[122](#biba.bibx52)]。此外，[[139](#biba.bibx69)]中的MADRL系统旨在最大化用户满意度，因此，代理之间也会互相交换其对QoS约束的满意度。
- en: Besides, in [[122](#biba.bibx52)], the agent does not directly transmit observation
    information but encodes the observation through a message communication network
    that is opposed to the action decision network. Message communication network
    of agents in [[122](#biba.bibx52)] applying a Discretization/Regularization Unit
    (DRU) to regularize the output during the training phase and discretize it during
    the execution phase so that the parameters of the communication network can be
    updated using gradient backpropagation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[[122](#biba.bibx52)]中，代理并不直接传输观测信息，而是通过与动作决策网络相对立的消息通信网络对观测进行编码。在[[122](#biba.bibx52)]中，代理的消息通信网络应用了离散化/正则化单元（DRU），在训练阶段对输出进行正则化，在执行阶段进行离散化，从而使得通信网络的参数可以通过梯度反向传播进行更新。
- en: In addition, the location information of mobile devices as agents is also considered
    as shared status information in the wireless communication MADRL system, such
    as [[81](#biba.bibx11)].
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，移动设备作为代理的位置信息也被视为无线通信MADRL系统中的共享状态信息，例如[[81](#biba.bibx11)]。
- en: IV-B Reward
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 奖励
- en: The reward function and its parameters intuitively reflect the impact of the
    agent’s decisions on the environment. Agents can better perceive their impact
    on the environment by exchanging rewards. For example, [[74](#biba.bibx4)] models
    the CLS system as a flow routing problem to achieve the minor consumption of system
    joint resources. The reward function used for sharing considers the number of
    flows in the current agent, the number of agents that each flow needs to pass
    through, and the number of virtual links connected to the agent. Similar work
    include [[101](#biba.bibx31)] and [[109](#biba.bibx39)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数及其参数直观地反映了代理决策对环境的影响。代理通过交换奖励可以更好地感知其对环境的影响。例如，[[74](#biba.bibx4)]将CLS系统建模为流量路由问题，以实现系统联合资源的最小消耗。用于共享的奖励函数考虑了当前代理中的流量数量、每个流量需要经过的代理数量以及连接到代理的虚拟链路数量。类似的工作包括[[101](#biba.bibx31)]和[[109](#biba.bibx39)]。
- en: '[[124](#biba.bibx54)] investigates maximizing the network overall rate by using
    power MADRL power control, with each cell’s in-cell sum rate (ICSR) as the agent’s
    reward. The simulation results indicate that sharing ICSR between agents can enhance
    cooperation and improve the overall rate. In addition, in wireless communication,
    agents can confirm the arrival of packets through feedback ACK messages, which
    is part of the reward function, such as in [[89](#biba.bibx19)].'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[[124](#biba.bibx54)] 研究了通过使用 MADRL 功率控制来最大化网络整体速率，其中每个小区的入内总速率 (ICSR) 作为智能体的奖励。仿真结果表明，共享智能体之间的
    ICSR 可以增强合作并提高整体速率。此外，在无线通信中，智能体可以通过反馈 ACK 消息确认数据包的到达，这也是奖励函数的一部分，例如在 [[89](#biba.bibx19)]
    中。'
- en: In Q-learning, the Q-value is the maximum reward that an agent can obtain in
    a particular state and specific action [[110](#biba.bibx40)]. Agents can achieve
    the global optimal joint action by sharing Q-values. In scenarios with jamming
    attacks, in [[128](#biba.bibx58)], each legitimate user, regarded as an agent,
    learns the best joint co-channel anti-interference strategy by sharing Q-values.
    Similarly, in the research anti-jamming attacks, the agents in [[125](#biba.bibx55)]
    share Q-tables instead of Q-values. Simulation results depict that by sharing
    Q-tables, agents can avoid co-channel jamming effectively, but the Q-table is
    about 1024 bytes, while each Q-value is only 1 bytes. Thus, agents only communicate
    with each other when the system is subjected to external co-channel jamming in
    [[125](#biba.bibx55)].
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Q 学习中，Q 值是智能体在特定状态和特定动作下可以获得的最大奖励 [[110](#biba.bibx40)]。智能体可以通过共享 Q 值来实现全局最优联合动作。在具有干扰攻击的场景中，在
    [[128](#biba.bibx58)] 中，每个被视为智能体的合法用户通过共享 Q 值学习最佳的联合共频道抗干扰策略。类似地，在研究抗干扰攻击中，[[125](#biba.bibx55)]
    中的智能体共享 Q 表而不是 Q 值。仿真结果表明，通过共享 Q 表，智能体可以有效避免共频道干扰，但 Q 表约为 1024 字节，而每个 Q 值仅为 1
    字节。因此，在 [[125](#biba.bibx55)] 中，智能体仅在系统受到外部共频道干扰时进行通信。
- en: IV-C Mixed & Other Parameters
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 混合与其他参数
- en: The communication messages between agents may contain various parameters, not
    just sharing one of the states, actions, or rewards.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体之间的通信消息可能包含各种参数，而不仅仅是共享一个状态、动作或奖励。
- en: In the fully centralized MADRL model, such as [[119](#biba.bibx49)], [[113](#biba.bibx43)],
    and [[121](#biba.bibx51)], local agents submit observations to the central agent
    and receive action decisions information. Furthermore, [[119](#biba.bibx49)] uses
    DNN to compress the observations of each local agent in the vehicular network
    system and further enhances it through a quantization layer to reduce network
    signaling costs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全集中式的 MADRL 模型中，例如 [[119](#biba.bibx49)]、[[113](#biba.bibx43)] 和 [[121](#biba.bibx51)]，本地智能体将观察结果提交给中央智能体，并接收行动决策信息。此外，[[119](#biba.bibx49)]
    使用 DNN 压缩每个本地智能体在车载网络系统中的观察，并通过量化层进一步增强，以降低网络信号成本。
- en: The authors of [[71](#biba.bibx1)] design two communication modes for agents,
    model sharing and value sharing. In model sharing, agents would share a copy of
    their machine learning model parameters, and agents would share their observations
    during value sharing mode. The system performance using model sharing is superior
    to value sharing in all aspects, but the size of each value sharing information
    is 8 Bytes, while the size of each target model update packet is 512 Bytes.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[[71](#biba.bibx1)] 的作者为智能体设计了两种通信模式：模型共享和价值共享。在模型共享中，智能体将共享其机器学习模型参数的副本，而在价值共享模式中，智能体将共享其观察结果。使用模型共享的系统性能在各个方面优于价值共享，但每个价值共享信息的大小为
    8 字节，而每个目标模型更新包的大小为 512 字节。'
- en: The authors in [[76](#biba.bibx6)] compared the effect of agents transmitting
    states, actions, and rewards concurrently in communication, as well as only exchanging
    partial information, on the overall spectrum utilization of the system. According
    to the simulation results, the system exhibited better performance with an increase
    in the number of types of communication parameters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[[76](#biba.bibx6)] 的作者比较了智能体在通信中同时传输状态、动作和奖励与仅交换部分信息对系统整体频谱利用率的影响。根据仿真结果，系统在通信参数类型增加时表现出更好的性能。'
- en: V Communication Object
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 通信对象
- en: The communication object refers to whom the agents in the system determine to
    send messages to. Depending on the range of communication objects that can be
    reached, they are categorized as neighbor agents, all other agents in the system,
    or a central agent.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通信对象是指系统中的代理决定发送消息的对象。根据可以达到的通信对象范围，它们被分类为邻近代理、系统中的所有其他代理或中央代理。
- en: The summary is shown in Table V.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要见表 V。
- en: TABLE V
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V
- en: The Category of Communication Object
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通信对象的类别
- en: '| Types | Traffic Engineering | Network Access | Power Control | Network Security
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 流量工程 | 网络访问 | 电源控制 | 网络安全 |'
- en: '| Neighbor | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[74](#biba.bibx4)]  [[71](#biba.bibx1)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[139](#biba.bibx69)]  [[122](#biba.bibx52)] | [[103](#biba.bibx33)]  [[136](#biba.bibx66)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[124](#biba.bibx54)]
    | - |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 邻近 | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[74](#biba.bibx4)]  [[71](#biba.bibx1)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[139](#biba.bibx69)]  [[122](#biba.bibx52)] | [[103](#biba.bibx33)]  [[136](#biba.bibx66)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[124](#biba.bibx54)]
    | - |'
- en: '| All | [[89](#biba.bibx19)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | [[81](#biba.bibx11)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)] | [[90](#biba.bibx20)]  [[85](#biba.bibx15)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 所有 | [[89](#biba.bibx19)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | [[81](#biba.bibx11)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)] | [[90](#biba.bibx20)]  [[85](#biba.bibx15)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
- en: '| Central | - | [[119](#biba.bibx49)]  [[113](#biba.bibx43)] | - | [[121](#biba.bibx51)]
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 中央 | - | [[119](#biba.bibx49)]  [[113](#biba.bibx43)] | - | [[121](#biba.bibx51)]
    |'
- en: V-A Neighbor Agents
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 邻近代理
- en: In a wired network, network entities such as routers or AS typically have established
    upstream and downstream relationships in the network topology. In this environment
    with relatively fixed network topology, agents usually choose to share information
    with neighboring entities, considering their greater impact than other agents
    in the network, such as [[138](#biba.bibx68), [129](#biba.bibx59), [74](#biba.bibx4),
    [71](#biba.bibx1), [132](#biba.bibx62), [87](#biba.bibx17)].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在有线网络中，网络实体如路由器或自治系统通常在网络拓扑中已经建立了上下游关系。在这种网络拓扑相对固定的环境中，代理通常选择与邻近实体共享信息，因为它们对网络的影响大于其他代理，例如[[138](#biba.bibx68)、[129](#biba.bibx59)、[74](#biba.bibx4)、[71](#biba.bibx1)、[132](#biba.bibx62)、[87](#biba.bibx17)]。
- en: In addition, in the wireless network, to ensure communication quality, agents
    are set to communicate with other agents that can establish stable communication
    channels within a specific range, such as [[139](#biba.bibx69), [103](#biba.bibx33),
    [136](#biba.bibx66), [102](#biba.bibx32), [80](#biba.bibx10), [124](#biba.bibx54),
    [122](#biba.bibx52)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在无线网络中，为了确保通信质量，代理被设置为与能够在特定范围内建立稳定通信通道的其他代理进行通信，例如[[139](#biba.bibx69)、[103](#biba.bibx33)、[136](#biba.bibx66)、[102](#biba.bibx32)、[80](#biba.bibx10)、[124](#biba.bibx54)、[122](#biba.bibx52)]。
- en: V-B All Agents
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 所有代理
- en: Network entities can naturally communicate with each other on the Internet,
    and agents applied on the Internet can communicate with all other agents directly,
    such as in [[101](#biba.bibx31)], [[128](#biba.bibx58)] and [[125](#biba.bibx55)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 网络实体可以在互联网上自然地进行通信，应用于互联网的代理可以直接与所有其他代理通信，例如在[[101](#biba.bibx31)]、[[128](#biba.bibx58)]和[[125](#biba.bibx55)]。
- en: In addition, in wireless communication environments, agents, such as UAVs or
    vehicles, broadcast their information to all other agents in the system due to
    device mobility or to simplify the model. For example, [[89](#biba.bibx19), [95](#biba.bibx25),
    [92](#biba.bibx22), [109](#biba.bibx39), [81](#biba.bibx11), [76](#biba.bibx6),
    [107](#biba.bibx37), [90](#biba.bibx20), [85](#biba.bibx15)].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在无线通信环境中，代理，如无人机或车辆，由于设备的移动性或为了简化模型，会向系统中的所有其他代理广播其信息。例如[[89](#biba.bibx19)、[95](#biba.bibx25)、[92](#biba.bibx22)、[109](#biba.bibx39)、[81](#biba.bibx11)、[76](#biba.bibx6)、[107](#biba.bibx37)、[90](#biba.bibx20)、[85](#biba.bibx15)]。
- en: Besides, through deep learning or gating mechanisms, agents can selectively
    transmit messages to all other agents within the system, such as [[117](#biba.bibx47),
    [84](#biba.bibx14), [118](#biba.bibx48)], to reduce communication costs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过深度学习或门控机制，代理可以选择性地将消息传递给系统中的所有其他代理，例如[[117](#biba.bibx47)、[84](#biba.bibx14)、[118](#biba.bibx48)]，以减少通信成本。
- en: V-C Central Agent
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 中央代理
- en: In the fully centralized training and execution system, all agents only communicate
    with a central agent. The agents transmit their local information to the central
    agent and receive decision information from the center without communication with
    other local agents [[119](#biba.bibx49), [113](#biba.bibx43), [121](#biba.bibx51)].
    The central agent maintains real-time communication with all local agents, inevitably
    resulting in high communication costs. Therefore, in order to reduce communication
    costs, each agent in [[121](#biba.bibx51)] adds a Deep Deterministic Policy Gradient
    network to decide whether to send messages to the central agent. The central agent
    aggregates traffic information and makes joint action decisions that are allocated
    to several specific agents for execution.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全集中式的训练和执行系统中，所有代理仅与中央代理通信。代理将其本地信息传递给中央代理，并从中心接收决策信息，而无需与其他本地代理通信[[119](#biba.bibx49),
    [113](#biba.bibx43), [121](#biba.bibx51)]。中央代理与所有本地代理保持实时通信，必然导致高通信成本。因此，为了降低通信成本，[[121](#biba.bibx51)]中的每个代理添加了一个深度确定性策略梯度网络，以决定是否将消息发送给中央代理。中央代理聚合流量信息并做出联合行动决策，这些决策被分配给几个特定的代理执行。
- en: Additionally, in order to enable each agent to observe the global environment
    in a distributed execution system, the agent communication algorithm using a central
    communication agent has been proposed [[105](#biba.bibx35), [86](#biba.bibx16),
    [99](#biba.bibx29), [116](#biba.bibx46)]. The central communication agent aggregates
    and encodes other agents’ messages as feedback, without outputting any action
    decisions. However, to our knowledge, this algorithm has not yet been applied
    to network management instances.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使每个代理能够在分布式执行系统中观察全局环境，已经提出了使用中央通信代理的代理通信算法[[105](#biba.bibx35), [86](#biba.bibx16),
    [99](#biba.bibx29), [116](#biba.bibx46)]。中央通信代理将其他代理的消息聚合并编码为反馈，而不输出任何行动决策。然而，值得注意的是，这种算法尚未应用于网络管理实例。
- en: VI Communication Message Processing
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 通信消息处理
- en: Communication message processing refers to the process of an agent integrating
    messages that are received from other agents. Deep Neural Networks typically require
    a fixed-dimension state representation as input for iteration. Therefore, incoming
    messages from other agents must be integrated to match input dimensions. Several
    methods for aggregating messages include concatenation, averaging, attention mechanisms,
    and neural networks.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通信消息处理指的是代理集成来自其他代理的消息的过程。深度神经网络通常需要固定维度的状态表示作为迭代输入。因此，来自其他代理的消息必须被集成以匹配输入维度。几种消息聚合方法包括连接、平均、注意力机制和神经网络。
- en: In addition, messages may be utilized as parameters for the reward function
    rather than inputs for neural networks. For example, in the wireless network,
    exchanging messages is to confirm whether the action is completed, such as in
    [[89](#biba.bibx19), [95](#biba.bibx25), [92](#biba.bibx22)]. Moreover, the message
    may be used as the parameters for calculating the reward function [[101](#biba.bibx31),
    [109](#biba.bibx39), [103](#biba.bibx33)]. Additionally, in the centralized system,
    the local agents receive action decisions from the central agent, which information
    would not need to be integrated [[119](#biba.bibx49), [113](#biba.bibx43), [121](#biba.bibx51)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，消息也可以用作奖励函数的参数，而不是神经网络的输入。例如，在无线网络中，交换消息是为了确认操作是否完成，如[[89](#biba.bibx19),
    [95](#biba.bibx25), [92](#biba.bibx22)]所示。此外，消息还可以用作计算奖励函数的参数[[101](#biba.bibx31),
    [109](#biba.bibx39), [103](#biba.bibx33)]。另外，在集中式系统中，局部代理从中央代理接收行动决策，而这些信息不需要被集成[[119](#biba.bibx49),
    [113](#biba.bibx43), [121](#biba.bibx51)]。
- en: In this survey, we only focus on the message integration methods in agent communication
    and classify recent works into three categories, as shown in Table VI.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们只关注代理通信中的消息集成方法，并将近期的工作分类为三类，如表VI所示。
- en: TABLE VI
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表VI
- en: The Category of Communication Message Processing
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通信消息处理的类别
- en: '| Types | Traffic Engineering | Network Access | Power Control | Network Security
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 流量工程 | 网络接入 | 能源控制 | 网络安全 |'
- en: '| Concatenation | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[132](#biba.bibx62)]
    | [[139](#biba.bibx69)]  [[76](#biba.bibx6)] | [[90](#biba.bibx20)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]  [[124](#biba.bibx54)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 连接 | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[132](#biba.bibx62)]
    | [[139](#biba.bibx69)]  [[76](#biba.bibx6)] | [[90](#biba.bibx20)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]  [[124](#biba.bibx54)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
- en: '| Average | - | [[107](#biba.bibx37)] | - | - |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | - | [[107](#biba.bibx37)] | - | - |'
- en: '| Neural Network | [[71](#biba.bibx1)]  [[87](#biba.bibx17)] | [[119](#biba.bibx49)]  [[122](#biba.bibx52)]
    | - | - |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 神经网络 | [[71](#biba.bibx1)]  [[87](#biba.bibx17)] | [[119](#biba.bibx49)]  [[122](#biba.bibx52)]
    | - | - |'
- en: VI-A Concatenation
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 连接
- en: Concatenation is a common message integration method, ensuring that messages
    are not lost during the integration process. However, the concatenation method
    does not consider the message weights of different agents, and it reduces the
    system’s scalability. The recent work that has used the concatenation method is
    presented in Table IV.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 连接是一种常见的消息整合方法，确保在整合过程中消息不会丢失。然而，连接方法未考虑不同代理的消息权重，并且降低了系统的可扩展性。最近使用连接方法的工作展示在表IV中。
- en: VI-B Average
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 平均
- en: The average calculation is a simple information integration method for agents’
    communication, such as in [[107](#biba.bibx37)]. It reduces system complexity,
    but it is obvious that this approach overlooks the impact of the number of agents
    and their interdependence. Thus, it has not been widely used.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 平均计算是一种简单的信息整合方法，用于代理之间的通信，例如在[[107](#biba.bibx37)]中。它降低了系统复杂性，但显然这种方法忽视了代理数量及其相互依赖性的影响。因此，尚未得到广泛使用。
- en: VI-C Neural Network
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 神经网络
- en: Simple neural networks or deep neural networks are used to learn to integrate
    messages from other agents, such as [[71](#biba.bibx1), [119](#biba.bibx49), [122](#biba.bibx52)].
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 简单神经网络或深度神经网络用于学习整合来自其他代理的信息，例如[[71](#biba.bibx1), [119](#biba.bibx49), [122](#biba.bibx52)]。
- en: Moreover, the agents in [[87](#biba.bibx17)] measure the weight of information
    with attention mechanism and use GCN to integrate the feature vectors in neighbor
    messages. Then, generate latent feature vectors as the interactive messages.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[[87](#biba.bibx17)]中的代理使用注意力机制来测量信息的权重，并利用GCN来整合邻居消息中的特征向量。然后，生成潜在特征向量作为交互消息。
- en: VII Communication Constraints
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 通信约束
- en: In a natural environment, various constraints, such as noise or limited bandwidth,
    can affect the communication performance between agents. However, currently, there
    is still relatively little research on how to address communication constraints
    between agents.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然环境中，各种约束，如噪声或有限带宽，可能会影响代理之间的通信性能。然而，目前关于如何解决代理之间的通信约束的研究仍然相对较少。
- en: VII-A Limited Bandwidth
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 有限带宽
- en: Bandwidth is frequently restricted in the natural surroundings. When the available
    bandwidth is being fully utilized, the messages sent by the agents may not be
    transmitted on time, leading to a delay in the agents’ decision-making process.
    The network management literature studies the effect of limited system bandwidth
    on communication between multiple agents, including [[129](#biba.bibx59)] and
    [[121](#biba.bibx51)]. The experiment proves that the two works have better system
    performance in a limited Bandwidth environment compared to their baseline.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽在自然环境中经常受到限制。当可用带宽被充分利用时，代理发送的消息可能无法及时传输，导致代理决策过程的延迟。网络管理文献研究了有限系统带宽对多个代理之间通信的影响，包括[[129](#biba.bibx59)]和[[121](#biba.bibx51)]。实验证明，在有限带宽环境下，这两项工作在系统性能上优于其基准。
- en: VII-B Noise
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 噪声
- en: In communication, the influence of noise is inevitable. The network management
    literature which has considered the effects of noise on communication between
    agents includes [[92](#biba.bibx22), [74](#biba.bibx4), [81](#biba.bibx11), [119](#biba.bibx49),
    [122](#biba.bibx52)]. These works have been experimentally proven to have good
    robustness in systems under the influence of noise, but no denoising method has
    been proposed or applied.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在通信中，噪声的影响是不可避免的。考虑噪声对代理之间通信影响的网络管理文献包括[[92](#biba.bibx22), [74](#biba.bibx4),
    [81](#biba.bibx11), [119](#biba.bibx49), [122](#biba.bibx52)]。这些工作在噪声影响下的系统中被实验证明具有良好的鲁棒性，但尚未提出或应用去噪方法。
- en: VIII Open Issues and Future Research Direction
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 未解决的问题和未来的研究方向
- en: This section discusses the issues and challenges of multi-agent communication
    in current network management. And briefly review future research directions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了当前网络管理中多智能体通信的问题和挑战，并简要回顾了未来的研究方向。
- en: VIII-A Denoise
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 去噪
- en: Noise can corrupt data and reduce the reliability of communication between agents
    [[79](#biba.bibx9)]. However, existing research on multi-agent systems in network
    management has not given much attention to the impact of communication noise between
    agents. Moreover, there is a lack of research on denoising in multi-agent communication
    processes. Hence, the problem of noise and interference in communication among
    agents is a pressing issue that requires immediate attention and resolution.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声可能会破坏数据并降低智能体之间通信的可靠性 [[79](#biba.bibx9)]。然而，现有的网络管理多智能体系统研究并没有给予智能体间通信噪声的影响足够的关注。此外，多智能体通信过程中去噪的研究也很缺乏。因此，智能体之间通信中的噪声和干扰问题是一个紧迫的问题，需要立即关注和解决。
- en: Recent research has continuously explored the application of denoising with
    deep learning. Deep learning, especially CNN-based image-denoising methods, has
    been widely studied. Since many MARL environments can be characterized as graphs,
    the impact of noise on agent communication might also be optimized using image-denoising
    algorithms in the future.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究持续探讨了深度学习去噪的应用。深度学习，特别是基于 CNN 的图像去噪方法，已经被广泛研究。由于许多 MARL 环境可以被表征为图形，因此未来也可能通过图像去噪算法优化噪声对智能体通信的影响。
- en: VIII-B Scalability
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 可扩展性
- en: In scenarios where the number of agents varies over time, the dimension of the
    system state representation changes dynamically. However, Neural networks typically
    require fixed-dimension state representation, posing scalability issues for MADRL
    systems. Many existing studies use mean calculation, attention, or deep neural
    networks to aggregate messages and reduce them to a fixed size. However, this
    approach may not be applicable in all scenarios. When new agents are added to
    a system, it can increase the number of messages and require changes to the action
    space of other agents, which can pose a significant challenge to the system’s
    scalability.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能体数量随时间变化的场景中，系统状态表示的维度会动态变化。然而，神经网络通常需要固定维度的状态表示，这对 MADRL 系统提出了可扩展性问题。许多现有研究使用均值计算、注意力机制或深度神经网络来聚合消息并将其减少到固定大小。然而，这种方法可能并不适用于所有场景。当新智能体被添加到系统中时，可能会增加消息数量，并需要改变其他智能体的动作空间，这可能对系统的可扩展性构成重大挑战。
- en: VIII-C Synchronization and Information Delay
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 同步和信息延迟
- en: In distributed or decentralized multi-agent networks, ensuring synchronization
    of agent states during training is a common challenge. Agents in a system rely
    on communication with each other to make decisions and cooperate effectively.
    However, if messages are delayed or agents are out of sync, it can lead to decision
    delays and decreased overall system performance. This is because agents may use
    outdated information for training or make decisions. Therefore, it is important
    to ensure that agents can communicate effectively and in a timely manner to optimize
    system performance.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式或去中心化的多智能体网络中，确保训练期间智能体状态的同步是一个常见的挑战。系统中的智能体依赖相互之间的通信来做出决策和有效合作。然而，如果消息延迟或智能体不同步，可能会导致决策延迟并降低整体系统性能。这是因为智能体可能会使用过时的信息进行训练或做出决策。因此，确保智能体能够有效且及时地进行通信以优化系统性能是非常重要的。
- en: VIII-D Limited Bandwidth
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-D 带宽限制
- en: In the natural environment, bandwidth cannot be unlimited. Excessive messages
    from agents can result in delays in transmitting information, which further reduces
    the system’s performance. It is important to ensure that messages are transmitted
    efficiently in limited bandwidth. Emergent communication research seems to be
    an effective solution by allowing agents to self-learn to determine communication
    objects, content, or timing. However, the use of emergent communication in managing
    multi-agent networks is still relatively limited.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然环境中，带宽不可能是无限的。智能体过多的消息可能会导致信息传输的延迟，从而进一步降低系统性能。确保在有限带宽下高效传输消息是非常重要的。新兴通信研究通过允许智能体自我学习以确定通信对象、内容或时机，似乎是一个有效的解决方案。然而，新兴通信在管理多智能体网络中的应用仍然相对有限。
- en: VIII-E Readability of Emergent Communication
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-E 新兴通信的可读性
- en: The messages of communication can emerge from agents through deep learning,
    such as in [[87](#biba.bibx17), [119](#biba.bibx49), [122](#biba.bibx52)], but
    the emergent messages may be unreadable to humans. This makes it difficult to
    evaluate communication effectiveness and formulate relevant specifications for
    industrial applications. Therefore, emergent message interpretation is one of
    the future research directions of agent communication.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深度学习，通信中的消息可以从代理中产生，例如在 [[87](#biba.bibx17), [119](#biba.bibx49), [122](#biba.bibx52)]
    中，但这些生成的消息可能对人类不可读。这使得评估通信效果和制定相关的工业应用规范变得困难。因此，生成消息的解释是代理通信未来研究的一个方向。
- en: VIII-F Communication Security
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-F 通信安全
- en: In the application scenario of network management, agents may contain privacy-sensitive
    information, such as user location, traffic, device power, etc., through sharing
    state or observation. If the agent containing privacy-sensitive information is
    left unprocessed, hackers may gain access to the entire system by attacking and
    controlling some agents. In this case, the information security issues in agent
    communication are worth studying.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络管理的应用场景中，代理可能包含隐私敏感信息，如用户位置、流量、设备功耗等，这些信息通过共享状态或观察进行传递。如果包含隐私敏感信息的代理未被处理，黑客可能通过攻击和控制某些代理来访问整个系统。在这种情况下，代理通信中的信息安全问题值得研究。
- en: IX Conclusion
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 结论
- en: In conclusion, we conducted a comprehensive survey of the application of MADRL
    in network management, especially a detailed analysis of research involving communication
    between multiple agents. First, we introduced the research contributions in current
    network management that involve communication between numerous agents. Then, we
    compared and classified agent communication work in detail from the aspects of
    learning and training schemes, communication objects, communication content, and
    communication constraints. Finally, we analyzed the challenges of Multi-agent
    communication applied in current network management and the future research directions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们对 MADRL 在网络管理中的应用进行了全面调查，特别是对涉及多个代理之间通信的研究进行了详细分析。首先，我们介绍了当前网络管理中涉及多个代理之间通信的研究贡献。接着，我们从学习和训练方案、通信对象、通信内容和通信约束等方面对代理通信工作进行了详细比较和分类。最后，我们分析了当前网络管理中应用多代理通信的挑战以及未来研究方向。
- en: Acknowledgment
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research is supported in part by the Huawei Technologies Co., Ltd., in
    part by the National Natural Science Foundation of China (Grant No. 92067109,
    61873119, 62211530106), and in part by Shenzhen Science and Technology Program
    (Grant No. ZDSYS20210623092007023, GJHZ20210705141808024).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分由华为技术有限公司资助，部分由中国国家自然科学基金（资助号：92067109, 61873119, 62211530106）资助，部分由深圳市科技计划（资助号：ZDSYS20210623092007023,
    GJHZ20210705141808024）资助。
- en: '{refcontext}'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '{refcontext}'
- en: '[sorting = none]'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[sorting = none]'
- en: References
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Tianxu Li et al. “Applications of multi-agent reinforcement learning in
    future internet: A comprehensive survey” In *IEEE Communications Surveys & Tutorials*
    24.2 IEEE, 2022, pp. 1240–1279'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Tianxu Li 等 “未来互联网中的多代理强化学习应用：综述” 见 *IEEE Communications Surveys & Tutorials*
    24.2 IEEE, 2022, 页 1240–1279'
- en: '[2] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman and Stefano V
    Albrecht “Dealing with non-stationarity in multi-agent deep reinforcement learning”
    In *arXiv preprint arXiv:1906.04737*, 2019'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman 和 Stefano V Albrecht
    “处理多代理深度强化学习中的非平稳性” 见 *arXiv preprint arXiv:1906.04737*, 2019'
- en: '[3] Mohamed Salah Zaiem and Etienne Bennequin “Learning to communicate in multi-agent
    reinforcement learning: A review” In *arXiv preprint arXiv:1911.05438*, 2019'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Mohamed Salah Zaiem 和 Etienne Bennequin “在多代理强化学习中学习通信：综述” 见 *arXiv preprint
    arXiv:1911.05438*, 2019'
- en: '[4] Amal Feriani and Ekram Hossain “Single and multi-agent deep reinforcement
    learning for AI-enabled wireless networks: A tutorial” In *IEEE Communications
    Surveys & Tutorials* 23.2 IEEE, 2021, pp. 1226–1252'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Amal Feriani 和 Ekram Hossain “用于 AI 驱动无线网络的单代理和多代理深度强化学习：教程” 见 *IEEE Communications
    Surveys & Tutorials* 23.2 IEEE, 2021, 页 1226–1252'
- en: '[5] Yang Xiao, Jun Liu, Jiawei Wu and Nirwan Ansari “Leveraging deep reinforcement
    learning for traffic engineering: A survey” In *IEEE Communications Surveys &
    Tutorials* 23.4 IEEE, 2021, pp. 2064–2097'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Yang Xiao, Jun Liu, Jiawei Wu 和 Nirwan Ansari “利用深度强化学习进行流量工程：综述” 见 *IEEE
    Communications Surveys & Tutorials* 23.4 IEEE, 2021, 页 2064–2097'
- en: '[6] Ibrahim Althamary, Chih-Wei Huang and Phone Lin “A survey on multi-agent
    reinforcement learning methods for vehicular networks” In *2019 15th International
    Wireless Communications & Mobile Computing Conference (IWCMC)*, 2019, pp. 1154–1159
    IEEE'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 易卜拉欣·阿尔塔玛里、黄志伟和林锋 “关于车辆网络的多智能体强化学习方法的综述” 发表在*2019年第15届国际无线通信与移动计算会议 (IWCMC)*,
    2019, pp. 1154–1159 IEEE'
- en: '[7] Changxi Zhu, Mehdi Dastani and Shihan Wang “A survey of multi-agent reinforcement
    learning with communication” In *arXiv preprint arXiv:2203.08975*, 2022'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 朱长西、梅赫迪·达斯塔尼和王士汉 “带通信的多智能体强化学习综述” 发表在*arXiv预印本 arXiv:2203.08975*, 2022'
- en: '[8] Marwa Chafii et al. “Emergent Communication in Multi-Agent Reinforcement
    Learning for Future Wireless Networks” In *IEEE Internet of Things Magazine* 6.4
    IEEE, 2023, pp. 18–24'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 马尔瓦·查菲等 “未来无线网络中的多智能体强化学习中的新兴通信” 发表在*IEEE物联网杂志* 6.4 IEEE, 2023, pp. 18–24'
- en: '[9] Zheng Li and Caili Guo “Multi-agent deep reinforcement learning based spectrum
    allocation for D2D underlay communications” In *IEEE Transactions on Vehicular
    Technology* 69.2 IEEE, 2019, pp. 1828–1840'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 郑丽和郭彩丽 “基于多智能体深度强化学习的D2D底层通信的频谱分配” 发表在*IEEE交通技术学报* 69.2 IEEE, 2019, pp.
    1828–1840'
- en: '[10] Nan Zhao et al. “Deep reinforcement learning for user association and
    resource allocation in heterogeneous cellular networks” In *IEEE Transactions
    on Wireless Communications* 18.11 IEEE, 2019, pp. 5141–5152'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 南赵等 “异构蜂窝网络中用户关联与资源分配的深度强化学习” 发表在*IEEE无线通信学报* 18.11 IEEE, 2019, pp. 5141–5152'
- en: '[11] Xu Zhang et al. “Deep multi-agent reinforcement learning for resource
    allocation in D2D communication underlaying cellular networks” In *2020 21st Asia-Pacific
    Network Operations and Management Symposium (APNOMS)*, 2020, pp. 55–60 IEEE'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 许璋等 “在D2D通信底层蜂窝网络中的资源分配的深度多智能体强化学习” 发表在*2020年第21届亚太网络运营与管理研讨会 (APNOMS)*,
    2020, pp. 55–60 IEEE'
- en: '[12] Fan Meng, Peng Chen, Lenan Wu and Julian Cheng “Power allocation in multi-user
    cellular networks: Deep reinforcement learning approaches” In *IEEE Transactions
    on Wireless Communications* 19.10 IEEE, 2020, pp. 6255–6267'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 樊萌、彭晨、伍乐南和朱利安·程 “多用户蜂窝网络中的功率分配：深度强化学习方法” 发表在*IEEE无线通信学报* 19.10 IEEE, 2020,
    pp. 6255–6267'
- en: '[13] Delin Guo, Lan Tang, Xinggan Zhang and Ying-Chang Liang “Joint optimization
    of handover control and power allocation based on multi-agent deep reinforcement
    learning” In *IEEE Transactions on Vehicular Technology* 69.11 IEEE, 2020, pp.
    13124–13138'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 郭德林、唐兰、张兴感和梁英昌 “基于多智能体深度强化学习的切换控制和功率分配的联合优化” 发表在*IEEE交通技术学报* 69.11 IEEE,
    2020, pp. 13124–13138'
- en: '[14] Amandeep Kaur and Krishan Kumar “Energy-efficient resource allocation
    in cognitive radio networks under cooperative multi-agent model-free reinforcement
    learning schemes” In *IEEE Transactions on Network and Service Management* 17.3
    IEEE, 2020, pp. 1337–1348'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 阿曼迪普·考尔和克里尚·库马尔 “在合作多智能体模型无关强化学习方案下的认知无线电网络中的节能资源分配” 发表在*IEEE网络与服务管理学报*
    17.3 IEEE, 2020, pp. 1337–1348'
- en: '[15] Ning Yang, Haijun Zhang and Randall Berry “Partially observable multi-agent
    deep reinforcement learning for cognitive resource management” In *GLOBECOM 2020-2020
    IEEE Global Communications Conference*, 2020, pp. 1–6 IEEE'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 宁杨、张海军和兰德尔·贝瑞 “用于认知资源管理的部分可观察多智能体深度强化学习” 发表在*GLOBECOM 2020-2020 IEEE全球通信大会*,
    2020, pp. 1–6 IEEE'
- en: '[16] Lin Zhang and Ying-Chang Liang “Deep reinforcement learning for multi-agent
    power control in heterogeneous networks” In *IEEE Transactions on Wireless Communications*
    20.4 IEEE, 2020, pp. 2551–2564'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 林章和梁英昌 “异构网络中多智能体功率控制的深度强化学习” 发表在*IEEE无线通信学报* 20.4 IEEE, 2020, pp. 2551–2564'
- en: '[17] Akash Doshi et al. “A deep reinforcement learning framework for contention-based
    spectrum sharing” In *IEEE Journal on Selected Areas in Communications* 39.8 IEEE,
    2021, pp. 2526–2540'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 阿卡什·多希等 “基于深度强化学习的争用型频谱共享框架” 发表在*IEEE选择领域通信学报* 39.8 IEEE, 2021, pp. 2526–2540'
- en: '[18] Zheng Li, Caili Guo and Yidi Xuan “A multi-agent deep reinforcement learning
    based spectrum allocation framework for D2D communications” In *2019 IEEE Global
    Communications Conference (GLOBECOM)*, 2019, pp. 1–6 IEEE'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 郑丽、郭彩丽和宣一迪 “基于多智能体深度强化学习的D2D通信频谱分配框架” 发表在*2019 IEEE全球通信大会 (GLOBECOM)*,
    2019, pp. 1–6 IEEE'
- en: '[19] Yi Yang et al. “Dynamic power allocation in cellular network based on
    multi-agent double deep reinforcement learning” In *Computer Networks* 217 Elsevier,
    2022, pp. 109342'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Yi Yang 等人。“基于多智能体双深度强化学习的蜂窝网络动态功率分配” 发表在 *计算机网络* 217 Elsevier，2022年，第109342页'
- en: '[20] Fenglei Li, Zhixin Liu, Xinzhe Zhang and Yi Yang “Dynamic power allocation
    in IIoT based on multi-agent deep reinforcement learning” In *Neurocomputing*
    505 Elsevier, 2022, pp. 10–18'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Fenglei Li, Zhixin Liu, Xinzhe Zhang 和 Yi Yang “基于多智能体深度强化学习的工业物联网动态功率分配”
    发表在 *Neurocomputing* 505 Elsevier，2022年，第10–18页'
- en: '[21] Yu Zhang et al. “Multi-agent deep reinforcement learning for secure UAV
    communications” In *2020 IEEE Wireless Communications and Networking Conference
    (WCNC)*, 2020, pp. 1–5 IEEE'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Yu Zhang 等人。“用于安全无人机通信的多智能体深度强化学习” 发表在 *2020年IEEE无线通信与网络会议 (WCNC)*，2020年，第1–5页
    IEEE'
- en: '[22] Yu Zhang et al. “UAV-enabled secure communications by multi-agent deep
    reinforcement learning” In *IEEE Transactions on Vehicular Technology* 69.10 IEEE,
    2020, pp. 11599–11611'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Yu Zhang 等人。“通过多智能体深度强化学习实现的无人机支持的安全通信” 发表在 *IEEE车辆技术汇刊* 69.10 IEEE，2020年，第11599–11611页'
- en: '[23] Xiulin Qiu et al. “A data-driven packet routing algorithm for an unmanned
    aerial vehicle swarm: A multi-agent reinforcement learning approach” In *IEEE
    Wireless Communications Letters* 11.10 IEEE, 2022, pp. 2160–2164'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Xiulin Qiu 等人。“针对无人机群的数据驱动数据包路由算法：一种多智能体强化学习方法” 发表在 *IEEE无线通信快报* 11.10
    IEEE，2022年，第2160–2164页'
- en: '[24] Alireza Shamsoshoara et al. “Distributed cooperative spectrum sharing
    in uav networks using multi-agent reinforcement learning” In *2019 16th IEEE Annual
    Consumer Communications & Networking Conference (CCNC)*, 2019, pp. 1–6 IEEE'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Alireza Shamsoshoara 等人。“基于多智能体强化学习的无人机网络中的分布式协作频谱共享” 发表在 *2019年第16届IEEE年度消费通信与网络会议
    (CCNC)*，2019年，第1–6页 IEEE'
- en: '[25] Jingjing Cui, Yuanwei Liu and Arumugam Nallanathan “Multi-agent reinforcement
    learning-based resource allocation for UAV networks” In *IEEE Transactions on
    Wireless Communications* 19.2 IEEE, 2019, pp. 729–743'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Jingjing Cui, Yuanwei Liu 和 Arumugam Nallanathan “基于多智能体强化学习的无人机网络资源分配”
    发表在 *IEEE无线通信汇刊* 19.2 IEEE，2019年，第729–743页'
- en: '[26] Le Liang, Hao Ye and Geoffrey Ye Li “Spectrum sharing in vehicular networks
    based on multi-agent reinforcement learning” In *IEEE Journal on Selected Areas
    in Communications* 37.10 IEEE, 2019, pp. 2282–2292'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Le Liang, Hao Ye 和 Geoffrey Ye Li “基于多智能体强化学习的车载网络频谱共享” 发表在 *IEEE精选通信领域期刊*
    37.10 IEEE，2019年，第2282–2292页'
- en: '[27] Khoi Khac Nguyen et al. “Distributed deep deterministic policy gradient
    for power allocation control in D2D-based V2V communications” In *IEEE Access*
    7 IEEE, 2019, pp. 164533–164543'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Khoi Khac Nguyen 等人。“基于深度确定性策略梯度的D2D基础的V2V通信功率分配控制” 发表在 *IEEE Access*
    7 IEEE，2019年，第164533–164543页'
- en: '[28] Mohit K Sharma, Alessio Zappone, Mérouane Debbah and Mohamad Assaad “Multi-agent
    deep reinforcement learning based power control for large energy harvesting networks”
    In *2019 International Symposium on Modeling and Optimization in Mobile, Ad Hoc,
    and Wireless Networks (WiOPT)*, 2019, pp. 1–7 IEEE'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Mohit K Sharma, Alessio Zappone, Mérouane Debbah 和 Mohamad Assaad “基于多智能体深度强化学习的大型能源收集网络功率控制”
    发表在 *2019年移动、临时和无线网络建模与优化国际研讨会 (WiOPT)*，2019年，第1–7页 IEEE'
- en: '[29] Adeb Salh et al. “Intelligent Resource Management Using Multiagent Double
    Deep Q-Networks to Guarantee Strict Reliability and Low Latency in IoT Network”
    In *IEEE Open Journal of the Communications Society* 3 IEEE, 2022, pp. 2245–2257'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Adeb Salh 等人。“利用多智能体双深度Q网络进行智能资源管理，以确保物联网网络中的严格可靠性和低延迟” 发表在 *IEEE开放通信学会期刊*
    3 IEEE，2022年，第2245–2257页'
- en: '[30] Zawar Shah et al. “Routing protocols for mobile Internet of things (IoT):
    A survey on challenges and solutions” In *Electronics* 10.19 MDPI, 2021, pp. 2320'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Zawar Shah 等人。“移动物联网 (IoT) 的路由协议：挑战与解决方案的调查” 发表在 *Electronics* 10.19 MDPI，2021年，第2320页'
- en: '[31] Xiaoyang Zhao, Chuan Wu and Franck Le “Improving inter-domain routing
    through multi-agent reinforcement learning” In *IEEE INFOCOM 2020-IEEE Conference
    on Computer Communications Workshops (INFOCOM WKSHPS)*, 2020, pp. 1129–1134 IEEE'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Xiaoyang Zhao, Chuan Wu 和 Franck Le “通过多智能体强化学习改进跨域路由” 发表在 *IEEE INFOCOM
    2020-IEEE计算机通信工作坊 (INFOCOM WKSHPS)*，2020年，第1129–1134页 IEEE'
- en: '[32] Redha A Alliche, Tiago Silva Barros, Ramon Aparicio-Pardo and Lucile Sassatelli
    “Impact evaluation of control signalling onto distributed learning-based packet
    routing” In *34th Intl. Teletraffic Congress, ITC 2022*, 2022'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Redha A Alliche, Tiago Silva Barros, Ramon Aparicio-Pardo 和 Lucile Sassatelli
    “控制信令对基于分布式学习的数据包路由的影响评估” 发表在 *第34届国际交通大会，ITC 2022*，2022年'
- en: '[33] Jiechuan Jiang, Chen Dun, Tiejun Huang and Zongqing Lu “Graph convolutional
    reinforcement learning” In *arXiv preprint arXiv:1810.09202*, 2018'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Jiechuan Jiang、Chen Dun、Tiejun Huang和Zongqing Lu“图卷积强化学习” 发表在*arXiv预印本
    arXiv:1810.09202*，2018年'
- en: '[34] Xinyu You et al. “Toward packet routing with fully distributed multiagent
    deep reinforcement learning” In *IEEE Transactions on Systems, Man, and Cybernetics:
    Systems* 52.2 IEEE, 2020, pp. 855–868'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Xinyu You等人。“朝着完全分布式多智能体深度强化学习的分组路由” 发表在*IEEE系统、人类和控制论学报：系统* 52.2 IEEE，2020年，第855–868页'
- en: '[35] Xingyan Chen et al. “A universal transcoding and transmission method for
    livecast with networked multi-agent reinforcement learning” In *IEEE INFOCOM 2021-IEEE
    Conference on Computer Communications*, 2021, pp. 1–10 IEEE'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Xingyan Chen等人。“一种用于网络化多智能体强化学习的实时转码和传输方法” 发表在*IEEE INFOCOM 2021-IEEE计算机通信会议*，2021年，第1–10页
    IEEE'
- en: '[36] Aniket Modi et al. “Multi-Agent Packet Routing (MAPR): Co-Operative Packet
    Routing Algorithm with Multi-Agent Reinforcement Learning” In *2023 15th International
    Conference on COMmunication Systems & NETworkS (COMSNETS)*, 2023, pp. 722–730
    IEEE'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Aniket Modi等人。“多智能体分组路由（MAPR）：一种具有多智能体强化学习的合作分组路由算法” 发表在*2023年第15届国际通信系统与网络大会（COMSNETS）*，2023年，第722–730页
    IEEE'
- en: '[37] Wen Zhang et al. “Sac: A novel multi-hop routing policy in hybrid distributed
    iot system based on multi-agent reinforcement learning” In *2021 22nd International
    Symposium on Quality Electronic Design (ISQED)*, 2021, pp. 129–134 IEEE'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Wen Zhang等人。“Sac：一种基于多智能体强化学习的混合分布式物联网系统中的新型多跳路由策略” 发表在*2021年第22届国际电子设计质量研讨会（ISQED）*，2021年，第129–134页
    IEEE'
- en: '[38] Liang Dong, Yuchen Qian and Yuan Xing “Dynamic spectrum access and sharing
    through actor-critic deep reinforcement learning” In *EURASIP Journal on Wireless
    Communications and Networking* 2022.1 Springer, 2022, pp. 48'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Liang Dong、Yuchen Qian和Yuan Xing“通过行为-评论家深度强化学习实现动态频谱访问和共享” 发表在*EURASIP无线通信与网络杂志*
    2022.1 Springer，2022年，第48页'
- en: '[39] Liang Wang, Hao Ye, Le Liang and Geoffrey Ye Li “Learn to compress CSI
    and allocate resources in vehicular networks” In *IEEE Transactions on Communications*
    68.6 IEEE, 2020, pp. 3640–3653'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Liang Wang、Hao Ye、Le Liang和Geoffrey Ye Li“学习压缩CSI和分配车载网络中的资源” 发表在*IEEE通信学报*
    68.6 IEEE，2020年，第3640–3653页'
- en: '[40] Alperen Gündoğan, H Murat Gürsu, Volker Pauli and Wolfgang Kellerer “Distributed
    resource allocation with multi-agent deep reinforcement learning for 5G-V2V communication”
    In *Proceedings of the Twenty-First International Symposium on Theory, Algorithmic
    Foundations, and Protocol Design for Mobile Networks and Mobile Computing*, 2020,
    pp. 357–362'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Alperen Gündoğan、H Murat Gürsu、Volker Pauli和Wolfgang Kellerer“用于5G-V2V通信的分布式资源分配与多智能体深度强化学习”
    发表在*第二十一届国际移动网络与移动计算理论、算法基础和协议设计研讨会论文集*，2020年，第357–362页'
- en: '[41] Ping Xiang et al. “Multi-Agent Reinforcement Learning-Based Decentralized
    Spectrum Access in Vehicular Networks With Emergent Communication” In *IEEE Communications
    Letters* 27.1 IEEE, 2022, pp. 195–199'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Ping Xiang等人。“基于多智能体强化学习的车载网络去中心化频谱接入与新兴通信” 发表在*IEEE通讯快报* 27.1 IEEE，2022年，第195–199页'
- en: '[42] Yuan Zhi et al. “Deep reinforcement learning-based resource allocation
    for D2D communications in heterogeneous cellular networks” In *Digital Communications
    and Networks* 8.5 Elsevier, 2022, pp. 834–842'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Yuan Zhi等人。“基于深度强化学习的异构蜂窝网络中D2D通信的资源分配” 发表在*数字通信与网络* 8.5 Elsevier，2022年，第834–842页'
- en: '[43] Errong Pei et al. “Intelligent Access to Unlicensed Spectrum: A Mean Field
    Based Deep Reinforcement Learning Approach” In *IEEE Transactions on Wireless
    Communications* 22.4 IEEE, 2022, pp. 2325–2337'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Errong Pei等人。“智能接入非许可频谱：一种基于均值场的深度强化学习方法” 发表在*IEEE无线通信学报* 22.4 IEEE，2022年，第2325–2337页'
- en: '[44] Ahmad Ali Khan and Raviraj S Adve “Centralized and distributed deep reinforcement
    learning methods for downlink sum-rate optimization” In *IEEE Transactions on
    Wireless Communications* 19.12 IEEE, 2020, pp. 8410–8426'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Ahmad Ali Khan和Raviraj S Adve“集中式和分布式深度强化学习方法用于下行链路总速率优化” 发表在*IEEE无线通信学报*
    19.12 IEEE，2020年，第8410–8426页'
- en: '[45] Di Zhao et al. “A reinforcement learning method for joint mode selection
    and power adaptation in the V2V communication network in 5G” In *IEEE Transactions
    on Cognitive Communications and Networking* 6.2 IEEE, 2020, pp. 452–463'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Di Zhao等人。“一种用于5G中V2V通信网络的联合模式选择和功率适配的强化学习方法” 发表在*IEEE认知通信与网络学报* 6.2 IEEE，2020年，第452–463页'
- en: '[46] Navid Naderializadeh, Jaroslaw J Sydir, Meryem Simsek and Hosein Nikopour
    “Resource management in wireless networks via multi-agent deep reinforcement learning”
    In *IEEE Transactions on Wireless Communications* 20.6 IEEE, 2021, pp. 3507–3523'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Navid Naderializadeh, Jaroslaw J Sydir, Meryem Simsek 和 Hosein Nikopour。“通过多智能体深度强化学习进行无线网络中的资源管理”
    见 *IEEE无线通信汇刊* 20.6 IEEE，2021年，第3507–3523页'
- en: '[47] Jonggyu Jang and Hyun Jong Yang “Deep reinforcement learning-based resource
    allocation and power control in small cells with limited information exchange”
    In *IEEE transactions on vehicular technology* 69.11 IEEE, 2020, pp. 13768–13783'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Jonggyu Jang 和 Hyun Jong Yang。“基于深度强化学习的资源分配和小区电力控制，信息交换有限” 见 *IEEE车辆技术汇刊*
    69.11 IEEE，2020年，第13768–13783页'
- en: '[48] Kaidi Xu, Nguyen Van Huynh and Geoffrey Ye Li “Distributed-Training-and-Execution
    Multi-Agent Reinforcement Learning for Power Control in HetNet” In *IEEE Transactions
    on Communications* IEEE, 2023'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Kaidi Xu, Nguyen Van Huynh 和 Geoffrey Ye Li。“用于HetNet的分布式训练与执行多智能体强化学习的电力控制”
    见 *IEEE通信汇刊* IEEE，2023年'
- en: '[49] Yasar Sinan Nasir and Dongning Guo “Multi-agent deep reinforcement learning
    for dynamic power allocation in wireless networks” In *IEEE Journal on Selected
    Areas in Communications* 37.10 IEEE, 2019, pp. 2239–2250'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Yasar Sinan Nasir 和 Dongning Guo。“用于无线网络的多智能体深度强化学习动态功率分配” 见 *IEEE选定领域通信期刊*
    37.10 IEEE，2019年，第2239–2250页'
- en: '[50] Bo Gu, Xu Zhang, Ziqi Lin and Mamoun Alazab “Deep multiagent reinforcement-learning-based
    resource allocation for internet of controllable things” In *IEEE Internet of
    Things Journal* 8.5 IEEE, 2020, pp. 3066–3074'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Bo Gu, Xu Zhang, Ziqi Lin 和 Mamoun Alazab。“基于深度多智能体强化学习的可控物联网资源分配” 见 *IEEE物联网期刊*
    8.5 IEEE，2020年，第3066–3074页'
- en: '[51] Fuqiang Yao and Luliang Jia “A collaborative multi-agent reinforcement
    learning anti-jamming algorithm in wireless networks” In *IEEE wireless communications
    letters* 8.4 IEEE, 2019, pp. 1024–1027'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Fuqiang Yao 和 Luliang Jia。“一种协作的多智能体强化学习抗干扰算法在无线网络中的应用” 见 *IEEE无线通信通讯*
    8.4 IEEE，2019年，第1024–1027页'
- en: '[52] Yifan Xu et al. “Interference-aware cooperative anti-jamming distributed
    channel selection in UAV communication networks” In *Applied Sciences* 8.10 MDPI,
    2018, pp. 1911'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Yifan Xu 等人。“UAV通信网络中的干扰感知协作抗干扰分布式信道选择” 见 *应用科学* 8.10 MDPI，2018年，第1911页'
- en: '[53] Shi-Ming Xia et al. “A new smart router-throttling method to mitigate
    DDoS attacks” In *IEEE Access* 7 IEEE, 2019, pp. 107952–107963'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Shi-Ming Xia 等人。“一种新的智能路由器限流方法以减轻DDoS攻击” 见 *IEEE Access* 7 IEEE，2019年，第107952–107963页'
- en: '[54] Alireza Shamsoshoara et al. “An autonomous spectrum management scheme
    for unmanned aerial vehicle networks in disaster relief operations” In *IEEE Access*
    8 IEEE, 2020, pp. 58064–58079'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Alireza Shamsoshoara 等人。“灾难救援操作中无人机网络的自主频谱管理方案” 见 *IEEE Access* 8 IEEE，2020年，第58064–58079页'
- en: '[55] Saeed Kaviani et al. “Robust and scalable routing with multi-agent deep
    reinforcement learning for MANETs” In *arXiv preprint arXiv:2101.03273*, 2021'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Saeed Kaviani 等人。“基于多智能体深度强化学习的MANETs鲁棒且可扩展的路由” 见 *arXiv预印本 arXiv:2101.03273*，2021年'
- en: '[56] Xinge Li, Xiaoya Hu, Rongqing Zhang and Liuqing Yang “Routing protocol
    design for underwater optical wireless sensor networks: A multiagent reinforcement
    learning approach” In *IEEE Internet of Things Journal* 7.10 IEEE, 2020, pp. 9805–9818'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Xinge Li, Xiaoya Hu, Rongqing Zhang 和 Liuqing Yang。“针对水下光无线传感网络的路由协议设计：一种多智能体强化学习方法”
    见 *IEEE物联网期刊* 7.10 IEEE，2020年，第9805–9818页'
- en: '[57] Chao Li and Jing Liu “A modified multi-agent reinforcement learning protocol
    based on prediction for UAANETs” In *2020 IEEE 92nd Vehicular Technology Conference
    (VTC2020-Fall)*, 2020, pp. 1–5 IEEE'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Chao Li 和 Jing Liu。“基于预测的UAANETs修改版多智能体强化学习协议” 见 *2020 IEEE第92届车辆技术大会（VTC2020-Fall）*，2020年，第1–5页
    IEEE'
- en: '[58] Xiulin Qiu et al. “QLGR: A Q-learning-based Geographic FANET Routing Algorithm
    Based on Multiagent Reinforcement Learning.” In *KSII Transactions on Internet
    & Information Systems* 15.11, 2021'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Xiulin Qiu 等人。“QLGR：一种基于Q学习的地理FANET路由算法，基于多智能体强化学习。” 见 *KSII互联网与信息系统汇刊*
    15.11，2021年'
- en: '[59] Di Wu and Nirwan Ansari “High capacity spectrum allocation for multiple
    D2D users reusing downlink spectrum in LTE” In *2018 IEEE International Conference
    on Communications (ICC)*, 2018, pp. 1–6 IEEE'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Di Wu 和 Nirwan Ansari。“针对多D2D用户在LTE下行频谱中重用的高容量频谱分配” 见 *2018 IEEE国际通信会议（ICC）*，2018年，第1–6页
    IEEE'
- en: '[60] Zhufang Kuang, Gang Liu, Gongqiang Li and Xiaoheng Deng “Energy efficient
    resource allocation algorithm in energy harvesting-based D2D heterogeneous networks”
    In *IEEE Internet of Things Journal* 6.1 IEEE, 2018, pp. 557–567'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 庄芳·匡、刘刚、李功强和邓晓恒 “在能量收集型D2D异构网络中的节能资源分配算法” 发表在 *IEEE物联网期刊* 6.1 IEEE，2018年，第557–567页'
- en: '[61] Jayesh K Gupta, Maxim Egorov and Mykel Kochenderfer “Cooperative multi-agent
    control using deep reinforcement learning” In *Autonomous Agents and Multiagent
    Systems: AAMAS 2017 Workshops, Best Papers, São Paulo, Brazil, May 8-12, 2017,
    Revised Selected Papers 16*, 2017, pp. 66–83 Springer'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 贾耶什·K·古普塔、马克西姆·埃戈罗夫和迈克尔·科亨德费尔 “使用深度强化学习的合作多智能体控制” 发表在 *自主智能体与多智能体系统：AAMAS
    2017工作坊，最佳论文，巴西圣保罗，2017年5月8-12日，修订精选论文集16*，2017年，第66–83页 Springer'
- en: '[62] Martin Riedmiller “Neural fitted Q iteration–first experiences with a
    data efficient neural reinforcement learning method” In *Machine Learning: ECML
    2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7,
    2005\. Proceedings 16*, 2005, pp. 317–328 Springer'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 马丁·里德米勒 “神经拟合Q迭代——对数据高效神经强化学习方法的初步体验” 发表在 *机器学习：ECML 2005：第16届欧洲机器学习会议，葡萄牙波尔图，2005年10月3-7日，会议录16*，2005年，第317–328页
    Springer'
- en: '[63] Amanpreet Singh, Tushar Jain and Sainbayar Sukhbaatar “Learning when to
    communicate at scale in multiagent cooperative and competitive tasks” In *arXiv
    preprint arXiv:1812.09755*, 2018'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 阿曼普里特·辛格、图沙尔·贾因和赛恩巴亚尔·苏赫巴图尔 “在多智能体合作与竞争任务中大规模学习何时进行通信” 发表在 *arXiv预印本 arXiv:1812.09755*，2018年'
- en: '[64] Guangzheng Hu et al. “Event-triggered multi-agent reinforcement learning
    with communication under limited-bandwidth constraint” In *arXiv preprint arXiv:2010.04978*,
    2020'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 光征·胡等 “在有限带宽约束下进行事件触发的多智能体强化学习” 发表在 *arXiv预印本 arXiv:2010.04978*，2020年'
- en: '[65] Qingshuang Sun et al. “Learning controlled and targeted communication
    with the centralized critic for the multi-agent system” In *Applied Intelligence*
    53.12 Springer, 2023, pp. 14819–14837'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 孙青爽等 “通过集中式评论学习控制和针对性的通信，用于多智能体系统” 发表在 *应用智能* 53.12 Springer，2023年，第14819–14837页'
- en: '[66] Yaru Niu, Rohan R Paleja and Matthew C Gombolay “Multi-Agent Graph-Attention
    Communication and Teaming.” In *AAMAS*, 2021, pp. 964–973'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 尼乌·雅如、罗汉·R·帕雷贾和马修·C·贡博雷 “多智能体图注意力通信与团队合作。” 发表在 *AAMAS*，2021年，第964–973页'
- en: '[67] Jiechuan Jiang and Zongqing Lu “Learning attentional communication for
    multi-agent cooperation” In *Advances in neural information processing systems*
    31, 2018'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] 姜杰川和陆宗青 “为多智能体合作学习注意力通信” 发表在 *神经信息处理系统进展* 31，2018年'
- en: '[68] Hangyu Mao et al. “Learning agent communication under limited bandwidth
    by message pruning” In *Proceedings of the AAAI Conference on Artificial Intelligence*
    34.04, 2020, pp. 5142–5149'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] 毛航宇等 “通过消息修剪在有限带宽下学习智能体通信” 发表在 *人工智能领域会议论文集* 34.04，2020年，第5142–5149页'
- en: '[69] Junjie Sheng et al. “Learning structured communication for multi-agent
    reinforcement learning” In *Autonomous Agents and Multi-Agent Systems* 36.2 Springer,
    2022, pp. 50'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] 盛俊杰等 “为多智能体强化学习学习结构化通信” 发表在 *自主智能体与多智能体系统* 36.2 Springer，2022年，第50页'
- en: '[70] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas and Shimon
    Whiteson “Learning to communicate with deep multi-agent reinforcement learning”
    In *Advances in neural information processing systems* 29, 2016'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] 雅各布·福斯特、伊奥尼斯·亚历山德罗斯·阿萨埃尔、南多·德·弗雷塔斯和希蒙·怀特森 “通过深度多智能体强化学习学习通信” 发表在 *神经信息处理系统进展*
    29，2016年'
- en: References
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[71] Redha A Alliche, Tiago Silva Barros, Ramon Aparicio-Pardo and Lucile Sassatelli
    “Impact evaluation of control signalling onto distributed learning-based packet
    routing” In *34th Intl. Teletraffic Congress, ITC 2022*, 2022'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] 雷德哈·A·阿利切、蒂亚戈·席尔瓦·巴罗斯、拉蒙·阿帕里西奥-帕尔多和露西尔·萨萨泰利 “控制信号对分布式学习型数据包路由的影响评估” 发表在
    *第34届国际交通会议，ITC 2022*，2022年'
- en: '[72] Ibrahim Althamary, Chih-Wei Huang and Phone Lin “A survey on multi-agent
    reinforcement learning methods for vehicular networks” In *2019 15th International
    Wireless Communications & Mobile Computing Conference (IWCMC)*, 2019, pp. 1154–1159
    IEEE'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] 易卜拉欣·阿尔塔马里、黄志伟和林锋 “关于车载网络的多智能体强化学习方法的综述” 发表在 *2019年第15届国际无线通信与移动计算会议 (IWCMC)*，2019年，第1154–1159页
    IEEE'
- en: '[73] Marwa Chafii et al. “Emergent Communication in Multi-Agent Reinforcement
    Learning for Future Wireless Networks” In *IEEE Internet of Things Magazine* 6.4
    IEEE, 2023, pp. 18–24'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] 玛尔瓦·查菲等 “面向未来无线网络的多智能体强化学习中的突现通信” 发表在 *IEEE物联网杂志* 6.4 IEEE，2023年，第18–24页'
- en: '[74] Xingyan Chen et al. “A universal transcoding and transmission method for
    livecast with networked multi-agent reinforcement learning” In *IEEE INFOCOM 2021-IEEE
    Conference on Computer Communications*, 2021, pp. 1–10 IEEE'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Xingyan Chen 等 “基于网络多代理强化学习的实时转码与传输方法” 见 *IEEE INFOCOM 2021-IEEE Conference
    on Computer Communications*, 2021, 页 1–10 IEEE'
- en: '[75] Jingjing Cui, Yuanwei Liu and Arumugam Nallanathan “Multi-agent reinforcement
    learning-based resource allocation for UAV networks” In *IEEE Transactions on
    Wireless Communications* 19.2 IEEE, 2019, pp. 729–743'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Jingjing Cui, Yuanwei Liu 和 Arumugam Nallanathan “基于多代理强化学习的 UAV 网络资源分配”
    见 *IEEE Transactions on Wireless Communications* 19.2 IEEE, 2019, 页 729–743'
- en: '[76] Liang Dong, Yuchen Qian and Yuan Xing “Dynamic spectrum access and sharing
    through actor-critic deep reinforcement learning” In *EURASIP Journal on Wireless
    Communications and Networking* 2022.1 Springer, 2022, pp. 48'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Liang Dong, Yuchen Qian 和 Yuan Xing “通过演员-评论家深度强化学习进行动态频谱访问与共享” 见 *EURASIP
    Journal on Wireless Communications and Networking* 2022.1 Springer, 2022, 页 48'
- en: '[77] Akash Doshi et al. “A deep reinforcement learning framework for contention-based
    spectrum sharing” In *IEEE Journal on Selected Areas in Communications* 39.8 IEEE,
    2021, pp. 2526–2540'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Akash Doshi 等 “基于争用的频谱共享的深度强化学习框架” 见 *IEEE Journal on Selected Areas in
    Communications* 39.8 IEEE, 2021, 页 2526–2540'
- en: '[78] Amal Feriani and Ekram Hossain “Single and multi-agent deep reinforcement
    learning for AI-enabled wireless networks: A tutorial” In *IEEE Communications
    Surveys & Tutorials* 23.2 IEEE, 2021, pp. 1226–1252'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Amal Feriani 和 Ekram Hossain “单代理与多代理深度强化学习用于 AI 驱动的无线网络：教程” 见 *IEEE Communications
    Surveys & Tutorials* 23.2 IEEE, 2021, 页 1226–1252'
- en: '[79] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas and Shimon
    Whiteson “Learning to communicate with deep multi-agent reinforcement learning”
    In *Advances in neural information processing systems* 29, 2016'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas 和 Shimon Whiteson
    “使用深度多代理强化学习进行通信学习” 见 *Advances in neural information processing systems* 29,
    2016'
- en: '[80] Bo Gu, Xu Zhang, Ziqi Lin and Mamoun Alazab “Deep multiagent reinforcement-learning-based
    resource allocation for internet of controllable things” In *IEEE Internet of
    Things Journal* 8.5 IEEE, 2020, pp. 3066–3074'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Bo Gu, Xu Zhang, Ziqi Lin 和 Mamoun Alazab “基于深度多代理强化学习的可控物联网资源分配” 见 *IEEE
    Internet of Things Journal* 8.5 IEEE, 2020, 页 3066–3074'
- en: '[81] Alperen Gündoğan, H Murat Gürsu, Volker Pauli and Wolfgang Kellerer “Distributed
    resource allocation with multi-agent deep reinforcement learning for 5G-V2V communication”
    In *Proceedings of the Twenty-First International Symposium on Theory, Algorithmic
    Foundations, and Protocol Design for Mobile Networks and Mobile Computing*, 2020,
    pp. 357–362'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Alperen Gündoğan, H Murat Gürsu, Volker Pauli 和 Wolfgang Kellerer “基于多代理深度强化学习的分布式资源分配用于
    5G-V2V 通信” 见 *Proceedings of the Twenty-First International Symposium on Theory,
    Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing*,
    2020, 页 357–362'
- en: '[82] Delin Guo, Lan Tang, Xinggan Zhang and Ying-Chang Liang “Joint optimization
    of handover control and power allocation based on multi-agent deep reinforcement
    learning” In *IEEE Transactions on Vehicular Technology* 69.11 IEEE, 2020, pp.
    13124–13138'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Delin Guo, Lan Tang, Xinggan Zhang 和 Ying-Chang Liang “基于多代理深度强化学习的切换控制与功率分配的联合优化”
    见 *IEEE Transactions on Vehicular Technology* 69.11 IEEE, 2020, 页 13124–13138'
- en: '[83] Jayesh K Gupta, Maxim Egorov and Mykel Kochenderfer “Cooperative multi-agent
    control using deep reinforcement learning” In *Autonomous Agents and Multiagent
    Systems: AAMAS 2017 Workshops, Best Papers, São Paulo, Brazil, May 8-12, 2017,
    Revised Selected Papers 16*, 2017, pp. 66–83 Springer'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Jayesh K Gupta, Maxim Egorov 和 Mykel Kochenderfer “使用深度强化学习的协作多代理控制” 见
    *Autonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Best Papers,
    São Paulo, Brazil, May 8-12, 2017, Revised Selected Papers 16*, 2017, 页 66–83
    Springer'
- en: '[84] Guangzheng Hu et al. “Event-triggered multi-agent reinforcement learning
    with communication under limited-bandwidth constraint” In *arXiv preprint arXiv:2010.04978*,
    2020'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Guangzheng Hu 等 “事件触发的多代理强化学习与有限带宽约束下的通信” 见 *arXiv 预印本 arXiv:2010.04978*,
    2020'
- en: '[85] Jonggyu Jang and Hyun Jong Yang “Deep reinforcement learning-based resource
    allocation and power control in small cells with limited information exchange”
    In *IEEE transactions on vehicular technology* 69.11 IEEE, 2020, pp. 13768–13783'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Jonggyu Jang 和 Hyun Jong Yang “基于深度强化学习的资源分配与小区功率控制的有限信息交换” 见 *IEEE Transactions
    on Vehicular Technology* 69.11 IEEE, 2020, 页 13768–13783'
- en: '[86] Jiechuan Jiang and Zongqing Lu “Learning attentional communication for
    multi-agent cooperation” In *Advances in neural information processing systems*
    31, 2018'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Jiechuan Jiang 和 Zongqing Lu “学习注意力通信以实现多代理合作” 见 *Advances in neural information
    processing systems* 31, 2018'
- en: '[87] Jiechuan Jiang, Chen Dun, Tiejun Huang and Zongqing Lu “Graph convolutional
    reinforcement learning” In *arXiv preprint arXiv:1810.09202*, 2018'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Jiechuan Jiang, Chen Dun, Tiejun Huang 和 Zongqing Lu “图卷积强化学习” 见 *arXiv预印本
    arXiv:1810.09202*，2018年'
- en: '[88] Amandeep Kaur and Krishan Kumar “Energy-efficient resource allocation
    in cognitive radio networks under cooperative multi-agent model-free reinforcement
    learning schemes” In *IEEE Transactions on Network and Service Management* 17.3
    IEEE, 2020, pp. 1337–1348'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Amandeep Kaur 和 Krishan Kumar “认知无线电网络中基于合作多智能体无模型强化学习方案的节能资源分配” 见 *IEEE网络与服务管理汇刊*
    17.3 IEEE，2020年，第1337–1348页'
- en: '[89] Saeed Kaviani et al. “Robust and scalable routing with multi-agent deep
    reinforcement learning for MANETs” In *arXiv preprint arXiv:2101.03273*, 2021'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Saeed Kaviani 等 “使用多智能体深度强化学习的鲁棒且可扩展的MANET路由” 见 *arXiv预印本 arXiv:2101.03273*，2021年'
- en: '[90] Ahmad Ali Khan and Raviraj S Adve “Centralized and distributed deep reinforcement
    learning methods for downlink sum-rate optimization” In *IEEE Transactions on
    Wireless Communications* 19.12 IEEE, 2020, pp. 8410–8426'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Ahmad Ali Khan 和 Raviraj S Adve “用于下行总速率优化的集中式和分布式深度强化学习方法” 见 *IEEE无线通信汇刊*
    19.12 IEEE，2020年，第8410–8426页'
- en: '[91] Zhufang Kuang, Gang Liu, Gongqiang Li and Xiaoheng Deng “Energy efficient
    resource allocation algorithm in energy harvesting-based D2D heterogeneous networks”
    In *IEEE Internet of Things Journal* 6.1 IEEE, 2018, pp. 557–567'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Zhufang Kuang, Gang Liu, Gongqiang Li 和 Xiaoheng Deng “基于能量采集的D2D异构网络中的节能资源分配算法”
    见 *IEEE物联网杂志* 6.1 IEEE，2018年，第557–567页'
- en: '[92] Chao Li and Jing Liu “A modified multi-agent reinforcement learning protocol
    based on prediction for UAANETs” In *2020 IEEE 92nd Vehicular Technology Conference
    (VTC2020-Fall)*, 2020, pp. 1–5 IEEE'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Chao Li 和 Jing Liu “基于预测的改进型多智能体强化学习协议用于UAANETs” 见 *2020 IEEE 第92届车辆技术大会
    (VTC2020-Fall)*，2020年，第1–5页 IEEE'
- en: '[93] Fenglei Li, Zhixin Liu, Xinzhe Zhang and Yi Yang “Dynamic power allocation
    in IIoT based on multi-agent deep reinforcement learning” In *Neurocomputing*
    505 Elsevier, 2022, pp. 10–18'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Fenglei Li, Zhixin Liu, Xinzhe Zhang 和 Yi Yang “基于多智能体深度强化学习的IIoT动态功率分配”
    见 *Neurocomputing* 505 Elsevier，2022年，第10–18页'
- en: '[94] Tianxu Li et al. “Applications of multi-agent reinforcement learning in
    future internet: A comprehensive survey” In *IEEE Communications Surveys & Tutorials*
    24.2 IEEE, 2022, pp. 1240–1279'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Tianxu Li 等 “多智能体强化学习在未来互联网中的应用：综合调查” 见 *IEEE通信调查与教程* 24.2 IEEE，2022年，第1240–1279页'
- en: '[95] Xinge Li, Xiaoya Hu, Rongqing Zhang and Liuqing Yang “Routing protocol
    design for underwater optical wireless sensor networks: A multiagent reinforcement
    learning approach” In *IEEE Internet of Things Journal* 7.10 IEEE, 2020, pp. 9805–9818'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Xinge Li, Xiaoya Hu, Rongqing Zhang 和 Liuqing Yang “水下光无线传感器网络的路由协议设计：一种多智能体强化学习方法”
    见 *IEEE物联网杂志* 7.10 IEEE，2020年，第9805–9818页'
- en: '[96] Zheng Li and Caili Guo “Multi-agent deep reinforcement learning based
    spectrum allocation for D2D underlay communications” In *IEEE Transactions on
    Vehicular Technology* 69.2 IEEE, 2019, pp. 1828–1840'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Zheng Li 和 Caili Guo “基于多智能体深度强化学习的D2D覆盖通信频谱分配” 见 *IEEE车辆技术汇刊* 69.2 IEEE，2019年，第1828–1840页'
- en: '[97] Zheng Li, Caili Guo and Yidi Xuan “A multi-agent deep reinforcement learning
    based spectrum allocation framework for D2D communications” In *2019 IEEE Global
    Communications Conference (GLOBECOM)*, 2019, pp. 1–6 IEEE'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Zheng Li, Caili Guo 和 Yidi Xuan “基于多智能体深度强化学习的D2D通信频谱分配框架” 见 *2019 IEEE全球通信大会
    (GLOBECOM)*，2019年，第1–6页 IEEE'
- en: '[98] Le Liang, Hao Ye and Geoffrey Ye Li “Spectrum sharing in vehicular networks
    based on multi-agent reinforcement learning” In *IEEE Journal on Selected Areas
    in Communications* 37.10 IEEE, 2019, pp. 2282–2292'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Le Liang, Hao Ye 和 Geoffrey Ye Li “基于多智能体强化学习的车载网络频谱共享” 见 *IEEE选定领域通信杂志*
    37.10 IEEE，2019年，第2282–2292页'
- en: '[99] Hangyu Mao et al. “Learning agent communication under limited bandwidth
    by message pruning” In *Proceedings of the AAAI Conference on Artificial Intelligence*
    34.04, 2020, pp. 5142–5149'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Hangyu Mao 等 “通过消息剪枝在有限带宽下学习智能体通信” 见 *AAAI人工智能会议论文集* 34.04，2020年，第5142–5149页'
- en: '[100] Fan Meng, Peng Chen, Lenan Wu and Julian Cheng “Power allocation in multi-user
    cellular networks: Deep reinforcement learning approaches” In *IEEE Transactions
    on Wireless Communications* 19.10 IEEE, 2020, pp. 6255–6267'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Fan Meng, Peng Chen, Lenan Wu 和 Julian Cheng “多用户蜂窝网络中的功率分配：深度强化学习方法”
    见 *IEEE无线通信汇刊* 19.10 IEEE，2020年，第6255–6267页'
- en: '[101] Aniket Modi et al. “Multi-Agent Packet Routing (MAPR): Co-Operative Packet
    Routing Algorithm with Multi-Agent Reinforcement Learning” In *2023 15th International
    Conference on COMmunication Systems & NETworkS (COMSNETS)*, 2023, pp. 722–730
    IEEE'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Aniket Modi 等人. “多智能体数据包路由（MAPR）：一种具有多智能体强化学习的合作数据包路由算法” 发表在 *2023年第15届国际通信系统与网络会议（COMSNETS）*，2023年，第722–730页
    IEEE'
- en: '[102] Navid Naderializadeh, Jaroslaw J Sydir, Meryem Simsek and Hosein Nikopour
    “Resource management in wireless networks via multi-agent deep reinforcement learning”
    In *IEEE Transactions on Wireless Communications* 20.6 IEEE, 2021, pp. 3507–3523'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Navid Naderializadeh, Jaroslaw J Sydir, Meryem Simsek 和 Hosein Nikopour
    “通过多智能体深度强化学习进行无线网络资源管理” 发表在 *IEEE无线通讯学报* 20.6 IEEE，2021年，第3507–3523页'
- en: '[103] Yasar Sinan Nasir and Dongning Guo “Multi-agent deep reinforcement learning
    for dynamic power allocation in wireless networks” In *IEEE Journal on Selected
    Areas in Communications* 37.10 IEEE, 2019, pp. 2239–2250'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Yasar Sinan Nasir 和 Dongning Guo “无线网络中的多智能体深度强化学习动态功率分配” 发表在 *IEEE选定领域通讯学报*
    37.10 IEEE，2019年，第2239–2250页'
- en: '[104] Khoi Khac Nguyen et al. “Distributed deep deterministic policy gradient
    for power allocation control in D2D-based V2V communications” In *IEEE Access*
    7 IEEE, 2019, pp. 164533–164543'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Khoi Khac Nguyen 等人. “基于分布式深度确定性策略梯度的 D2D 基于 V2V 通信中的功率分配控制” 发表在 *IEEE
    Access* 7 IEEE，2019年，第164533–164543页'
- en: '[105] Yaru Niu, Rohan R Paleja and Matthew C Gombolay “Multi-Agent Graph-Attention
    Communication and Teaming.” In *AAMAS*, 2021, pp. 964–973'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Yaru Niu, Rohan R Paleja 和 Matthew C Gombolay “多智能体图注意力通信与团队合作。” 发表在
    *AAMAS*，2021年，第964–973页'
- en: '[106] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman and Stefano
    V Albrecht “Dealing with non-stationarity in multi-agent deep reinforcement learning”
    In *arXiv preprint arXiv:1906.04737*, 2019'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman 和 Stefano V
    Albrecht “处理多智能体深度强化学习中的非平稳性” 发表在 *arXiv 预印本 arXiv:1906.04737*，2019年'
- en: '[107] Errong Pei et al. “Intelligent Access to Unlicensed Spectrum: A Mean
    Field Based Deep Reinforcement Learning Approach” In *IEEE Transactions on Wireless
    Communications* 22.4 IEEE, 2022, pp. 2325–2337'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Errong Pei 等人. “智能访问非授权频谱：基于均场的深度强化学习方法” 发表在 *IEEE无线通讯学报* 22.4 IEEE，2022年，第2325–2337页'
- en: '[108] Xiulin Qiu et al. “A data-driven packet routing algorithm for an unmanned
    aerial vehicle swarm: A multi-agent reinforcement learning approach” In *IEEE
    Wireless Communications Letters* 11.10 IEEE, 2022, pp. 2160–2164'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Xiulin Qiu 等人. “用于无人机集群的数据驱动数据包路由算法：一种多智能体强化学习方法” 发表在 *IEEE无线通讯快报* 11.10
    IEEE，2022年，第2160–2164页'
- en: '[109] Xiulin Qiu et al. “QLGR: A Q-learning-based Geographic FANET Routing
    Algorithm Based on Multiagent Reinforcement Learning.” In *KSII Transactions on
    Internet & Information Systems* 15.11, 2021'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Xiulin Qiu 等人. “QLGR：一种基于 Q 学习的多智能体强化学习的地理 FANET 路由算法” 发表在 *KSII互联网与信息系统交易*
    15.11，2021年'
- en: '[110] Martin Riedmiller “Neural fitted Q iteration–first experiences with a
    data efficient neural reinforcement learning method” In *Machine Learning: ECML
    2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7,
    2005\. Proceedings 16*, 2005, pp. 317–328 Springer'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Martin Riedmiller “神经拟合 Q 迭代——对数据高效神经强化学习方法的初步经验” 发表在 *机器学习：ECML 2005：第16届欧洲机器学习会议，葡萄牙波尔图，2005年10月3-7日。论文集
    16*，2005年，第317–328页 Springer'
- en: '[111] Adeb Salh et al. “Intelligent Resource Management Using Multiagent Double
    Deep Q-Networks to Guarantee Strict Reliability and Low Latency in IoT Network”
    In *IEEE Open Journal of the Communications Society* 3 IEEE, 2022, pp. 2245–2257'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Adeb Salh 等人. “使用多智能体双深度 Q 网络进行智能资源管理，以保证 IoT 网络中的严格可靠性和低延迟” 发表在 *IEEE通讯学会开放期刊*
    3 IEEE，2022年，第2245–2257页'
- en: '[112] Zawar Shah et al. “Routing protocols for mobile Internet of things (IoT):
    A survey on challenges and solutions” In *Electronics* 10.19 MDPI, 2021, pp. 2320'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Zawar Shah 等人. “移动物联网（IoT）的路由协议：挑战与解决方案调查” 发表在 *电子学* 10.19 MDPI，2021年，第2320页'
- en: '[113] Alireza Shamsoshoara et al. “An autonomous spectrum management scheme
    for unmanned aerial vehicle networks in disaster relief operations” In *IEEE Access*
    8 IEEE, 2020, pp. 58064–58079'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Alireza Shamsoshoara 等人. “一种针对无人机网络在灾后救援操作中的自主频谱管理方案” 发表在 *IEEE Access*
    8 IEEE，2020年，第58064–58079页'
- en: '[114] Alireza Shamsoshoara et al. “Distributed cooperative spectrum sharing
    in uav networks using multi-agent reinforcement learning” In *2019 16th IEEE Annual
    Consumer Communications & Networking Conference (CCNC)*, 2019, pp. 1–6 IEEE'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Alireza Shamsoshoara 等人. “基于多智能体强化学习的无人机网络分布式协作频谱共享” 发表在 *2019年第16届IEEE年会消费电子通讯与网络会议（CCNC）*，2019年，第1–6页
    IEEE'
- en: '[115] Mohit K Sharma, Alessio Zappone, Mérouane Debbah and Mohamad Assaad “Multi-agent
    deep reinforcement learning based power control for large energy harvesting networks”
    In *2019 International Symposium on Modeling and Optimization in Mobile, Ad Hoc,
    and Wireless Networks (WiOPT)*, 2019, pp. 1–7 IEEE'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Mohit K Sharma, Alessio Zappone, Mérouane Debbah 和 Mohamad Assaad。“基于多智能体深度强化学习的大型能源采集网络的功率控制”
    发表在 *2019 International Symposium on Modeling and Optimization in Mobile, Ad Hoc,
    and Wireless Networks (WiOPT)*, 2019, 页码 1–7 IEEE'
- en: '[116] Junjie Sheng et al. “Learning structured communication for multi-agent
    reinforcement learning” In *Autonomous Agents and Multi-Agent Systems* 36.2 Springer,
    2022, pp. 50'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Junjie Sheng 等人。“为多智能体强化学习学习结构化通信” 发表在 *Autonomous Agents and Multi-Agent
    Systems* 36.2 Springer, 2022, 页码 50'
- en: '[117] Amanpreet Singh, Tushar Jain and Sainbayar Sukhbaatar “Learning when
    to communicate at scale in multiagent cooperative and competitive tasks” In *arXiv
    preprint arXiv:1812.09755*, 2018'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Amanpreet Singh, Tushar Jain 和 Sainbayar Sukhbaatar。“在多智能体合作和竞争任务中学习何时进行大规模通信”
    发表在 *arXiv preprint arXiv:1812.09755*, 2018'
- en: '[118] Qingshuang Sun et al. “Learning controlled and targeted communication
    with the centralized critic for the multi-agent system” In *Applied Intelligence*
    53.12 Springer, 2023, pp. 14819–14837'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Qingshuang Sun 等人。“基于集中式评论者的多智能体系统中的受控和定向通信学习” 发表在 *Applied Intelligence*
    53.12 Springer, 2023, 页码 14819–14837'
- en: '[119] Liang Wang, Hao Ye, Le Liang and Geoffrey Ye Li “Learn to compress CSI
    and allocate resources in vehicular networks” In *IEEE Transactions on Communications*
    68.6 IEEE, 2020, pp. 3640–3653'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Liang Wang, Hao Ye, Le Liang 和 Geoffrey Ye Li。“在车载网络中学习压缩 CSI 和分配资源”
    发表在 *IEEE Transactions on Communications* 68.6 IEEE, 2020, 页码 3640–3653'
- en: '[120] Di Wu and Nirwan Ansari “High capacity spectrum allocation for multiple
    D2D users reusing downlink spectrum in LTE” In *2018 IEEE International Conference
    on Communications (ICC)*, 2018, pp. 1–6 IEEE'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Di Wu 和 Nirwan Ansari。“LTE 中多个 D2D 用户重用下行频谱的高容量频谱分配” 发表在 *2018 IEEE International
    Conference on Communications (ICC)*, 2018, 页码 1–6 IEEE'
- en: '[121] Shi-Ming Xia et al. “A new smart router-throttling method to mitigate
    DDoS attacks” In *IEEE Access* 7 IEEE, 2019, pp. 107952–107963'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Shi-Ming Xia 等人。“一种新的智能路由器限流方法以缓解 DDoS 攻击” 发表在 *IEEE Access* 7 IEEE,
    2019, 页码 107952–107963'
- en: '[122] Ping Xiang et al. “Multi-Agent Reinforcement Learning-Based Decentralized
    Spectrum Access in Vehicular Networks With Emergent Communication” In *IEEE Communications
    Letters* 27.1 IEEE, 2022, pp. 195–199'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Ping Xiang 等人。“基于多智能体强化学习的车载网络去中心化频谱接入与自发通信” 发表在 *IEEE Communications
    Letters* 27.1 IEEE, 2022, 页码 195–199'
- en: '[123] Yang Xiao, Jun Liu, Jiawei Wu and Nirwan Ansari “Leveraging deep reinforcement
    learning for traffic engineering: A survey” In *IEEE Communications Surveys &
    Tutorials* 23.4 IEEE, 2021, pp. 2064–2097'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Yang Xiao, Jun Liu, Jiawei Wu 和 Nirwan Ansari。“利用深度强化学习进行交通工程：综述” 发表在
    *IEEE Communications Surveys & Tutorials* 23.4 IEEE, 2021, 页码 2064–2097'
- en: '[124] Kaidi Xu, Nguyen Van Huynh and Geoffrey Ye Li “Distributed-Training-and-Execution
    Multi-Agent Reinforcement Learning for Power Control in HetNet” In *IEEE Transactions
    on Communications* IEEE, 2023'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Kaidi Xu, Nguyen Van Huynh 和 Geoffrey Ye Li。“用于 HetNet 的分布式训练和执行多智能体强化学习的功率控制”
    发表在 *IEEE Transactions on Communications* IEEE, 2023'
- en: '[125] Yifan Xu et al. “Interference-aware cooperative anti-jamming distributed
    channel selection in UAV communication networks” In *Applied Sciences* 8.10 MDPI,
    2018, pp. 1911'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Yifan Xu 等人。“干扰感知的无人机通信网络中的协作抗干扰分布式信道选择” 发表在 *Applied Sciences* 8.10
    MDPI, 2018, 页码 1911'
- en: '[126] Ning Yang, Haijun Zhang and Randall Berry “Partially observable multi-agent
    deep reinforcement learning for cognitive resource management” In *GLOBECOM 2020-2020
    IEEE Global Communications Conference*, 2020, pp. 1–6 IEEE'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Ning Yang, Haijun Zhang 和 Randall Berry。“用于认知资源管理的部分可观测多智能体深度强化学习” 发表在
    *GLOBECOM 2020-2020 IEEE Global Communications Conference*, 2020, 页码 1–6 IEEE'
- en: '[127] Yi Yang et al. “Dynamic power allocation in cellular network based on
    multi-agent double deep reinforcement learning” In *Computer Networks* 217 Elsevier,
    2022, pp. 109342'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Yi Yang 等人。“基于多智能体双重深度强化学习的蜂窝网络动态功率分配” 发表在 *Computer Networks* 217 Elsevier,
    2022, 页码 109342'
- en: '[128] Fuqiang Yao and Luliang Jia “A collaborative multi-agent reinforcement
    learning anti-jamming algorithm in wireless networks” In *IEEE wireless communications
    letters* 8.4 IEEE, 2019, pp. 1024–1027'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Fuqiang Yao 和 Luliang Jia。“无线网络中的协作多智能体强化学习抗干扰算法” 发表在 *IEEE wireless
    communications letters* 8.4 IEEE, 2019, 页码 1024–1027'
- en: '[129] Xinyu You et al. “Toward packet routing with fully distributed multiagent
    deep reinforcement learning” In *IEEE Transactions on Systems, Man, and Cybernetics:
    Systems* 52.2 IEEE, 2020, pp. 855–868'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] 辛宇优等“基于完全分布式多智能体深度强化学习的分组路由” 在*IEEE系统、人物与控制论：系统* 52.2 IEEE，2020年，第855–868页'
- en: '[130] Mohamed Salah Zaiem and Etienne Bennequin “Learning to communicate in
    multi-agent reinforcement learning: A review” In *arXiv preprint arXiv:1911.05438*,
    2019'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] 穆罕默德·萨拉赫·扎伊姆和艾蒂安·贝内坤“多智能体强化学习中的沟通学习：综述” 在*arXiv预印本 arXiv:1911.05438*，2019年'
- en: '[131] Lin Zhang and Ying-Chang Liang “Deep reinforcement learning for multi-agent
    power control in heterogeneous networks” In *IEEE Transactions on Wireless Communications*
    20.4 IEEE, 2020, pp. 2551–2564'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] 林张和颖昌梁“异构网络中的多智能体深度强化学习电源控制” 在*IEEE无线通信杂志* 20.4 IEEE，2020年，第2551–2564页'
- en: '[132] Wen Zhang et al. “Sac: A novel multi-hop routing policy in hybrid distributed
    iot system based on multi-agent reinforcement learning” In *2021 22nd International
    Symposium on Quality Electronic Design (ISQED)*, 2021, pp. 129–134 IEEE'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] 文张等“基于多智能体强化学习的混合分布式物联网系统中的新型多跳路由策略” 在*2021年第22届国际电子设计质量研讨会（ISQED）*，2021年，第129–134页
    IEEE'
- en: '[133] Xu Zhang et al. “Deep multi-agent reinforcement learning for resource
    allocation in D2D communication underlaying cellular networks” In *2020 21st Asia-Pacific
    Network Operations and Management Symposium (APNOMS)*, 2020, pp. 55–60 IEEE'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] 许张等“在D2D通信覆盖的蜂窝网络中用于资源分配的深度多智能体强化学习” 在*2020年第21届亚太网络运营和管理研讨会（APNOMS）*，2020年，第55–60页
    IEEE'
- en: '[134] Yu Zhang et al. “Multi-agent deep reinforcement learning for secure UAV
    communications” In *2020 IEEE Wireless Communications and Networking Conference
    (WCNC)*, 2020, pp. 1–5 IEEE'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] 余张等“用于安全无人机通信的多智能体深度强化学习” 在*2020年IEEE无线通信和网络会议（WCNC）*，2020年，第1–5页 IEEE'
- en: '[135] Yu Zhang et al. “UAV-enabled secure communications by multi-agent deep
    reinforcement learning” In *IEEE Transactions on Vehicular Technology* 69.10 IEEE,
    2020, pp. 11599–11611'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] 余张等“通过多智能体深度强化学习实现无人机安全通信” 在*IEEE车辆技术杂志* 69.10 IEEE，2020年，第11599–11611页'
- en: '[136] Di Zhao et al. “A reinforcement learning method for joint mode selection
    and power adaptation in the V2V communication network in 5G” In *IEEE Transactions
    on Cognitive Communications and Networking* 6.2 IEEE, 2020, pp. 452–463'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] 迪赵等“5G V2V通信网络中联合模式选择和功率适应的强化学习方法” 在*IEEE认知通信与网络杂志* 6.2 IEEE，2020年，第452–463页'
- en: '[137] Nan Zhao et al. “Deep reinforcement learning for user association and
    resource allocation in heterogeneous cellular networks” In *IEEE Transactions
    on Wireless Communications* 18.11 IEEE, 2019, pp. 5141–5152'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] 南赵等“异构蜂窝网络中的用户关联和资源分配的深度强化学习” 在*IEEE无线通信杂志* 18.11 IEEE，2019年，第5141–5152页'
- en: '[138] Xiaoyang Zhao, Chuan Wu and Franck Le “Improving inter-domain routing
    through multi-agent reinforcement learning” In *IEEE INFOCOM 2020-IEEE Conference
    on Computer Communications Workshops (INFOCOM WKSHPS)*, 2020, pp. 1129–1134 IEEE'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] 小阳赵、川吴和弗朗克·乐“通过多智能体强化学习改进跨域路由” 在*IEEE INFOCOM 2020-IEEE计算机通信研讨会（INFOCOM
    WKSHPS）*，2020年，第1129–1134页 IEEE'
- en: '[139] Yuan Zhi et al. “Deep reinforcement learning-based resource allocation
    for D2D communications in heterogeneous cellular networks” In *Digital Communications
    and Networks* 8.5 Elsevier, 2022, pp. 834–842'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] 袁智等“基于深度强化学习的异构蜂窝网络中的D2D通信资源分配” 在*数字通信与网络* 8.5 Elsevier，2022年，第834–842页'
- en: '[140] Changxi Zhu, Mehdi Dastani and Shihan Wang “A survey of multi-agent reinforcement
    learning with communication” In *arXiv preprint arXiv:2203.08975*, 2022'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] 常熙朱、梅赫迪·达斯塔尼和石汉·王“多智能体强化学习与通信的综述” 在*arXiv预印本 arXiv:2203.08975*，2022年'
