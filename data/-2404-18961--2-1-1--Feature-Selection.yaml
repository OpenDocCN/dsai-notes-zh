- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:33:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:33:02'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.18961] 2.1.1\. Feature Selection'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.18961] 2.1.1\. 特征选择'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18961](https://ar5iv.labs.arxiv.org/html/2404.18961)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18961](https://ar5iv.labs.arxiv.org/html/2404.18961)
- en: '| Model Name | Origin | Year | Type | Matrix Regularizer | Vector Formalization
    |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 来源 | 年份 | 类型 | 矩阵正则化 | 向量形式 |'
- en: '| Regularized MTL | KDD | evgeniou2004regularized | Group regularization |
    Frobenius norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}-\frac{1}{T}\sum_{t=1}^{T}\boldsymbol{w}^{t}\&#124;}^{2}_{2}+\lambda_{2}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}\&#124;}^{2}_{2}$
    |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 正则化多任务学习 | KDD | evgeniou2004regularized | 组正则化 | Frobenius 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}-\frac{1}{T}\sum_{t=1}^{T}\boldsymbol{w}^{t}\&#124;}^{2}_{2}+\lambda_{2}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}\&#124;}^{2}_{2}$
    |'
- en: '| Learning Multiple Tasks with Kernel Methods | JMLR | \citeyearevgeniou2005learning
    | Priori Sharing | Adaptive penalty | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{t=1}^{T}{\boldsymbol{w}^{t}}^{\top}\boldsymbol{V}^{+}\boldsymbol{w}^{t},$  s.t. $\boldsymbol{V}\in\boldsymbol{S}_{+}^{D},$
    $\boldsymbol{V}\in\boldsymbol{S}^{D}$ |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 使用核方法学习多任务 | JMLR | \citeyearevgeniou2005learning | 先验共享 | 自适应惩罚 | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{t=1}^{T}{\boldsymbol{w}^{t}}^{\top}\boldsymbol{V}^{+}\boldsymbol{w}^{t},$  s.t. $\boldsymbol{V}\in\boldsymbol{S}_{+}^{D},$
    $\boldsymbol{V}\in\boldsymbol{S}^{D}$ |'
- en: '| Alternating structure optimization | JMLR | \citeyearando2005framework |
    Decomposition | Frobenius norm | $\min\limits_{\{\boldsymbol{W},\boldsymbol{V}\},\Theta}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{w}^{t}+\Theta^{\top}\boldsymbol{v}^{t})-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}$,  s.t. $\Theta\Theta^{\top}=\boldsymbol{I}_{h\times
    h}$ |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 交替结构优化 | JMLR | \citeyearando2005framework | 分解 | Frobenius 范数 | $\min\limits_{\{\boldsymbol{W},\boldsymbol{V}\},\Theta}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{w}^{t}+\Theta^{\top}\boldsymbol{v}^{t})-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}$,  s.t. $\Theta\Theta^{\top}=\boldsymbol{I}_{h\times
    h}$ |'
- en: '| Multi-task feature selection | Tech. Rep.¹ | \citeyearobozinski2006multi
    | Group-sparse learning | $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}$
    |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 多任务特征选择 | Tech. Rep.¹ | \citeyearobozinski2006multi | 组稀疏学习 | $\ell_{2,1}$
    范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}$
    |'
- en: '| Multi-task Lasso | Thesis² | \citeyearzhang2006a | Group-sparse learning
    | $\ell_{\infty,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{\infty}$
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 多任务 Lasso | Thesis² | \citeyearzhang2006a | 组稀疏学习 | $\ell_{\infty,1}$ 范数
    | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{\infty}$
    |'
- en: '| Multi-task feature learning | NeurIPS | \citeyearargyriou2006multi | Group-sparse
    learning, feature learning | $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{U},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;({\boldsymbol{X}^{(t)}}\boldsymbol{U})\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda(\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2})^{2}$,  s.t. $\boldsymbol{U}\in\boldsymbol{O}^{D}$
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 多任务特征学习 | NeurIPS | \citeyearargyriou2006multi | 组稀疏学习，特征学习 | $\ell_{2,1}$
    范数 | $\min\limits_{\boldsymbol{U},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;({\boldsymbol{X}^{(t)}}\boldsymbol{U})\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda(\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2})^{2}$,  s.t. $\boldsymbol{U}\in\boldsymbol{O}^{D}$
    |'
- en: '| Convex multi-task feature learning | Mach. Lea. | \citeyearargyriou2008convex
    | Feature learning | Adaptive penalty | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{t=1}^{T}{\boldsymbol{w}^{t}}^{\top}\boldsymbol{V}^{+}\boldsymbol{w}^{t},$  s.t. $\boldsymbol{V}\in\boldsymbol{S}_{+}^{D},$
    tr$(\boldsymbol{V})\leq 1$, col$(\boldsymbol{W})\subseteq$col$(\boldsymbol{V})$
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 凸多任务特征学习 | Mach. Lea. | \citeyearargyriou2008convex | 特征学习 | 自适应惩罚 | $\min\limits_{\boldsymbol{V},
    \boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda\sum_{t=1}^{T}{\boldsymbol{w}^{t}}^{\top}\boldsymbol{V}^{+}\boldsymbol{w}^{t},$  s.t. $\boldsymbol{V}\in\boldsymbol{S}_{+}^{D},$
    tr$(\boldsymbol{V})\leq 1$, col$(\boldsymbol{W})\subseteq$col$(\boldsymbol{V})$
    |'
- en: '| Low rank MTL | ICML | \citeyearji2009accelerated | Low-rank learning | Trace
    norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\&#124;\boldsymbol{W}\&#124;_{*}$
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 低秩 MTL | ICML | \citeyearji2009accelerated | 低秩学习 | 跟踪范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda\lvert\boldsymbol{W}\rvert_{*}$
    |'
- en: '| Convex ASO | ICML | \citeyearchen2009convex | — | — | $\min\limits_{\boldsymbol{U},\Theta}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{u}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda\eta(1-\eta)\text{tr}(\boldsymbol{U}^{\top}(\eta\boldsymbol{I}+\Theta^{\top}\Theta)^{-1}\boldsymbol{U}),~{}~{}s.t.~{}\Theta\Theta^{\top}=\boldsymbol{I}_{h\times
    h}$ |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 凸 ASO | ICML | \citeyearchen2009convex | — | — | $\min\limits_{\boldsymbol{U},
    \Theta}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{u}^{t}-\boldsymbol{y}^{t}\rvert_{2}^{2}+\lambda\eta(1-\eta)\text{tr}(\boldsymbol{U}^{\top}(\eta\boldsymbol{I}+\Theta^{\top}\Theta)^{-1}\boldsymbol{U}),~{}~{}s.t.~{}\Theta\Theta^{\top}=\boldsymbol{I}_{h\times
    h}$ |'
- en: '| Dirty block-sparse model | NeurIPS | \citeyearjalali2010dirty | Group-sparse
    learning, decomposition | $\ell_{\infty,1}$ norm $+$ $\ell_{1,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{s}^{t}+\boldsymbol{b}^{t})-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}{\&#124;\boldsymbol{s}_{d}\&#124;}_{1}+\lambda_{2}\sum_{d=1}^{D}{\&#124;\boldsymbol{b}_{d}\&#124;}_{\infty}$,  s.t. $\boldsymbol{W}=\boldsymbol{S}+\boldsymbol{B}$
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 脏块稀疏模型 | NeurIPS | \citeyearjalali2010dirty | 组稀疏学习，分解 | $\ell_{\infty,1}$
    范数 $+$ $\ell_{1,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}(\boldsymbol{s}^{t}+\boldsymbol{b}^{t})-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\lvert\boldsymbol{s}_{d}\rvert_{1}+\lambda_{2}\sum_{d=1}^{D}\lvert\boldsymbol{b}_{d}\rvert_{\infty}$,  s.t. $\boldsymbol{W}=\boldsymbol{S}+\boldsymbol{B}$
    |'
- en: '| Sparse multi-task Lasso | NeurIPS | \citeyearlee2010adaptive | Group-sparse
    learning | Weighted $\ell_{2,1}$ norm $+$ weighted $\ell_{1,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\rho_{d}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}+\lambda_{2}\sum_{d=1}^{D}\theta_{d}{\&#124;\boldsymbol{w}_{d}\&#124;}_{1}$
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏多任务 Lasso | NeurIPS | \citeyearlee2010adaptive | 组稀疏学习 | 加权 $\ell_{2,1}$
    范数 $+$ 加权 $\ell_{1,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\rho_{d}\lvert\boldsymbol{w}_{d}\rvert_{2}+\lambda_{2}\sum_{d=1}^{D}\theta_{d}\lvert\boldsymbol{w}_{d}\rvert_{1}$
    |'
- en: '| \cdashline1-6 |  |  |  | Weighted $\ell_{2,1}$ norm $+$ weighted $\ell_{1,1}$
    norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\rho_{d}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}+\lambda_{2}\sum_{d=1}^{D}\theta_{d}{\&#124;\boldsymbol{w}_{d}\&#124;}_{1}+\log
    Z(\boldsymbol{\rho},\boldsymbol{\theta})$, |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-6 |  |  |  | 加权 $\ell_{2,1}$ 范数 $+$ 加权 $\ell_{1,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\rho_{d}\lvert\boldsymbol{w}_{d}\rvert_{2}+\lambda_{2}\sum_{d=1}^{D}\theta_{d}\lvert\boldsymbol{w}_{d}\rvert_{1}+\log
    Z(\boldsymbol{\rho}, \boldsymbol{\theta})$, |'
- en: '| Adaptive multi-task Lasso | NeurIPS | \citeyearlee2010adaptive | Group-sparse
    learning | $+$ adaptive penalty | $P(\boldsymbol{W}&#124;\boldsymbol{\rho},\boldsymbol{\theta})=\frac{1}{Z(\boldsymbol{\rho},\boldsymbol{\theta})}\prod_{d=1}^{D}\prod_{t=1}^{T}\exp(-\theta_{d}\lvert
    w_{n,t}\rvert)\times\prod_{d=1}^{D}\exp(-\rho_{d}\&#124;\mathbf{w}_{d}\&#124;_{2})$
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 自适应多任务 Lasso | NeurIPS | \citeyearlee2010adaptive | 组稀疏学习 | $+$ 自适应惩罚 | $P(\boldsymbol{W}
    \mid \boldsymbol{\rho}, \boldsymbol{\theta})=\frac{1}{Z(\boldsymbol{\rho}, \boldsymbol{\theta})}\prod_{d=1}^{D}\prod_{t=1}^{T}\exp(-\theta_{d}\lvert
    w_{n,t}\rvert)\times\prod_{d=1}^{D}\exp(-\rho_{d}\lvert \mathbf{w}_{d}\rvert_{2})$
    |'
- en: '|  |  |  |  |  | $\min\limits_{\mathbf{M}_{0},\ldots,\mathbf{M}_{T}}\gamma_{0}\&#124;\mathbf{M}_{0}-\mathbf{I}\&#124;_{F}^{2}+\sum\nolimits_{t=1}^{T}\left[\gamma_{t}\&#124;\mathbf{M}_{t}\&#124;_{F}^{2}+\sum\nolimits_{(i,j)\in
    J_{t},j\neq i}d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{j})+\sum\nolimits_{(i,j,k)\in
    S_{t}}\xi_{ijk}\right]$ |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | $\min\limits_{\mathbf{M}_{0},\ldots,\mathbf{M}_{T}}\gamma_{0}\&#124;\mathbf{M}_{0}-\mathbf{I}\&#124;_{F}^{2}+\sum\nolimits_{t=1}^{T}\left[\gamma_{t}\&#124;\mathbf{M}_{t}\&#124;_{F}^{2}+\sum\nolimits_{(i,j)\in
    J_{t},j\neq i}d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{j})+\sum\nolimits_{(i,j,k)\in
    S_{t}}\xi_{ijk}\right]$ |'
- en: '| Large margin multi-task metric learning | NeurIPS | \citeyearparameswaran2010large
    | Priori Sharing | Frobenius norm | s.t. $\forall t,\forall(i,j,k)\in S_{t}\colon\quad
    d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{k})-d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{j})\geq
    1-\xi_{ijk};\xi_{ijk}\geq 0;\mathbf{M}_{0},\mathbf{M}_{1},\ldots,\mathbf{M}_{T}\geq
    0$ |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 大边际多任务度量学习 | NeurIPS | \citeyearparameswaran2010large | 先验共享 | Frobenius
    范数 | s.t. $\forall t,\forall(i,j,k)\in S_{t}\colon\quad d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{k})-d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{j})\geq
    1-\xi_{ijk};\xi_{ijk}\geq 0;\mathbf{M}_{0},\mathbf{M}_{1},\ldots,\mathbf{M}_{T}\geq
    0$ |'
- en: '| Hierarchical multitask structured output learning | NeurIPS | \citeyeargornitz2011hierarchical
    | Priori Sharing | Frobenius norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{1}{2}\sum_{t=1}^{T}&#124;&#124;\boldsymbol{w}&#124;&#124;_{2}^{2}-\lambda\boldsymbol{w}^{T}\boldsymbol{w}_{p}$,
    where $p$ is the parent node. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 层次多任务结构化输出学习 | NeurIPS | \citeyeargornitz2011hierarchical | 先验共享 | Frobenius
    范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{1}{2}\sum_{t=1}^{T}&#124;&#124;\boldsymbol{w}&#124;&#124;_{2}^{2}-\lambda\boldsymbol{w}^{T}\boldsymbol{w}_{p}$，其中
    $p$ 是父节点。 |'
- en: '|  |  |  | low-rank learning |  |  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 低秩学习 |  |  |'
- en: '| Robust MTL | KDD | \citeyearchen2011integrating | Decomposition, group-sparse
    learning, | Trace norm + $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{l}^{t}+\boldsymbol{s}^{t})-\boldsymbol{y}^{(t)}\&#124;_{2}^{2}+\lambda_{1}\&#124;\boldsymbol{L}\&#124;_{*}+\lambda_{2}\sum_{t=1}^{T}\&#124;\boldsymbol{s}_{t}\&#124;_{2}$,  s.t. $\boldsymbol{W}=\boldsymbol{L}+\boldsymbol{S}$
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 稳健的 MTL | KDD | \citeyearchen2011integrating | 分解，组稀疏学习 | 跟踪范数 + $\ell_{2,1}$
    范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{l}^{t}+\boldsymbol{s}^{t})-\boldsymbol{y}^{(t)}\&#124;_{2}^{2}+\lambda_{1}\&#124;\boldsymbol{L}\&#124;_{*}+\lambda_{2}\sum_{t=1}^{T}\&#124;\boldsymbol{s}_{t}\&#124;_{2}$,  s.t. $\boldsymbol{W}=\boldsymbol{L}+\boldsymbol{S}$
    |'
- en: '| Temporal group Lasso | KDD | \citeyearzhou2011multi | Group-sparse learning
    | Frobenius norm + $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}+\lambda_{2}\sum_{t=1}^{T-1}\&#124;\boldsymbol{w}^{t}-\boldsymbol{w}^{t+1}\&#124;_{2}^{2}+\lambda_{3}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}$
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 时间组 Lasso | KDD | \citeyearzhou2011multi | 组稀疏学习 | Frobenius 范数 + $\ell_{2,1}$
    范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}+\lambda_{2}\sum_{t=1}^{T-1}\&#124;\boldsymbol{w}^{t}-\boldsymbol{w}^{t+1}\&#124;_{2}^{2}+\lambda_{3}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}$
    |'
- en: '| Clustered MTL | NeurIPS | \citeyearzhou2011clustered | task clustering |
    Clustering penalty + $\ell_{2,2}$ norm | $\min\limits_{\boldsymbol{W},\boldsymbol{F}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda_{1}(\text{tr}(\boldsymbol{W}^{\top}\boldsymbol{W})-\text{tr}(\boldsymbol{F}^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{F}))+\lambda_{2}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}\&#124;}^{2}_{2},$
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 MTL | NeurIPS | \citeyearzhou2011clustered | 任务聚类 | 聚类惩罚 + $\ell_{2,2}$
    范数 | $\min\limits_{\boldsymbol{W},\boldsymbol{F}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda_{1}(\text{tr}(\boldsymbol{W}^{\top}\boldsymbol{W})-\text{tr}(\boldsymbol{F}^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{F}))+\lambda_{2}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}\&#124;}^{2}_{2},$
    |'
- en: '| $~{}~{}\text{s.t.}~{}\boldsymbol{F}_{t,j}=1/\sqrt{n_{j}}~{}\text{if}~{}t\in\mathcal{C}_{j}~{}\text{otherwise}~{}0,$
    $t=1,\cdots,T$, where $n_{j}$ is the #task in the $j$-th cluster $\mathbf{\mathcal{C}}_{j}$.
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| $~{}~{}\text{s.t.}~{}\boldsymbol{F}_{t,j}=1/\sqrt{n_{j}}~{}\text{if}~{}t\in\mathcal{C}_{j}~{}\text{otherwise}~{}0,$
    $t=1,\cdots,T$, 其中 $n_{j}$ 是第 $j$ 个聚类 $\mathbf{\mathcal{C}}_{j}$ 中的任务数。 |'
- en: '|  |  |  | Decomposition, sparse learning, |  |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 分解，稀疏学习 |  |  |'
- en: '| Sparse and low rank MTL | TKDD | \citeyearchen2012learning | low-rank learning
    | $\ell_{1,1}$ norm + trace norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}\&#124;\boldsymbol{p}_{d}\&#124;_{1}$,  s.t. $\boldsymbol{W}=\boldsymbol{P}+\boldsymbol{Q},\&#124;\boldsymbol{Q}\&#124;_{*}\leq\tau$
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏和低秩 MTL | TKDD | \citeyearchen2012learning | 低秩学习 | $\ell_{1,1}$ 范数 + 迹范数
    | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}\&#124;\boldsymbol{p}_{d}\&#124;_{1}$，满足 $\boldsymbol{W}=\boldsymbol{P}+\boldsymbol{Q},\&#124;\boldsymbol{Q}\&#124;_{*}\leq\tau$
    |'
- en: '| Convex fused sparse group Lasso | KDD | \citeyearzhou2012modeling | Group-sparse
    learning | $\ell_{1,1}$ norm $+$ $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{1}+\lambda_{2}\sum_{t=1}^{T-1}\&#124;\boldsymbol{w}^{t}-\boldsymbol{w}^{t+1}\&#124;_{1}+\lambda_{3}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}$
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 凸融合稀疏组套索 | KDD | \citeyearzhou2012modeling | 组稀疏学习 | $\ell_{1,1}$ 范数 $+$
    $\ell_{2,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{1}+\lambda_{2}\sum_{t=1}^{T-1}\&#124;\boldsymbol{w}^{t}-\boldsymbol{w}^{t+1}\&#124;_{1}+\lambda_{3}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}$
    |'
- en: '| Adaptive multi-task elastic-net | SDM | \citeyearchen2012adaptive | Group-sparse
    learning | $\ell_{2,1}$ norm $+$ Frobenius norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}+\lambda_{2}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}$
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 自适应多任务弹性网络 | SDM | \citeyearchen2012adaptive | 组稀疏学习 | $\ell_{2,1}$ 范数 $+$
    Frobenius 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}+\lambda_{2}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}$
    |'
- en: '| Multi-level Lasso | ICML | \citeyearlozano2012multi | Decomposition, sparse
    learning | $\ell_{1,1}$ norm + adaptive penalty | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\theta_{d}+\lambda_{2}\sum_{d=1}^{D}\&#124;\boldsymbol{\boldsymbol{\gamma}}_{d}\&#124;_{1}$,  s.t. $\boldsymbol{W}=\vec{\boldsymbol{\theta}}\boldsymbol{\Lambda}\boldsymbol{\Gamma},\vec{\boldsymbol{\theta}}\geq\boldsymbol{0}$
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 多层次套索 | ICML | \citeyearlozano2012multi | 分解，稀疏学习 | $\ell_{1,1}$ 范数 + 自适应惩罚
    | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\theta_{d}+\lambda_{2}\sum_{d=1}^{D}\&#124;\boldsymbol{\boldsymbol{\gamma}}_{d}\&#124;_{1}$，满足 $\boldsymbol{W}=\vec{\boldsymbol{\theta}}\boldsymbol{\Lambda}\boldsymbol{\Gamma},\vec{\boldsymbol{\theta}}\geq\boldsymbol{0}$
    |'
- en: '| Robust multi-task feature learning | KDD | \citeyeargong2012robust | Decomposition,
    group-sparse learning | $\ell_{2,1}$ norm + $\ell_{1,2}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{p}_{d}\&#124;_{2}+\lambda_{2}\sqrt{\sum_{d=1}^{D}\&#124;\boldsymbol{q}_{d}\&#124;_{1}^{2}}$,  s.t. $\boldsymbol{W}=\boldsymbol{P}+\boldsymbol{Q}$
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 稳健的多任务特征学习 | KDD | \citeyeargong2012robust | 分解，组稀疏学习 | $\ell_{2,1}$ 范数 +
    $\ell_{1,2}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{p}_{d}\&#124;_{2}+\lambda_{2}\sqrt{\sum_{d=1}^{D}\&#124;\boldsymbol{q}_{d}\&#124;_{1}^{2}}$，满足 $\boldsymbol{W}=\boldsymbol{P}+\boldsymbol{Q}$
    |'
- en: '| Multi-stage multi-task feature learning | NeurIPS | \citeyeargong2012multi
    | Sparse learning | Capped $\ell_{1}$ norm \citepzhang2010analysis | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{R}\min\{\&#124;\boldsymbol{w}_{d}\&#124;_{1},\tau\}$
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 多阶段多任务特征学习 | NeurIPS | \citeyeargong2012multi | 稀疏学习 | 限制的 $\ell_{1}$ 范数
    \citepzhang2010analysis | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{R}\min\{\&#124;\boldsymbol{w}_{d}\&#124;_{1},\tau\}$
    |'
- en: '| Convex formulation for MTL | IJCAI | \citeyearzhang2012convex | Priori sharing
    | Clustering penalty | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{\lambda_{1}}{2}$tr$(\boldsymbol{W}\boldsymbol{W}^{T})+\frac{\lambda_{2}}{2}$tr$(\boldsymbol{W}\boldsymbol{\Omega}^{-1}\boldsymbol{W}^{T})$  s.t. $\boldsymbol{\Omega}\in\boldsymbol{S}_{+}^{D}$,
    tr$\boldsymbol{\Omega}=1$ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| MTL的凸优化形式 | IJCAI | \citeyearzhang2012convex | 先验共享 | 聚类惩罚 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{\lambda_{1}}{2}$tr$(\boldsymbol{W}\boldsymbol{W}^{T})+\frac{\lambda_{2}}{2}$tr$(\boldsymbol{W}\boldsymbol{\Omega}^{-1}\boldsymbol{W}^{T})$
    约束条件 $\boldsymbol{\Omega}\in\boldsymbol{S}_{+}^{D}$, tr$\boldsymbol{\Omega}=1$
    |'
- en: '| Multi-linear multi-task learning | ICML | \citeyearromera2013multilinear
    | Low-rank learning | Overlapped tensor trace norm | $\min\limits_{\boldsymbol{\mathcal{W}}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{k=1}^{N}\&#124;\boldsymbol{W}_{(k)}\&#124;_{*}$
    where $\boldsymbol{W}_{(k)}$ is the mode-$k$ unfolding of tensor $\boldsymbol{\mathcal{W}}\in\mathbb{R}^{D\times
    I_{2}\times\cdots\times I_{N}}$. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 多线性多任务学习 | ICML | \citeyearromera2013multilinear | 低秩学习 | 重叠张量迹范数 | $\min\limits_{\boldsymbol{\mathcal{W}}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{k=1}^{N}\&#124;\boldsymbol{W}_{(k)}\&#124;_{*}$
    其中 $\boldsymbol{W}_{(k)}$ 是张量 $\boldsymbol{\mathcal{W}}\in\mathbb{R}^{D\times
    I_{2}\times\cdots\times I_{N}}$ 的模式-$k$ 展开。 |'
- en: '| Regularization approach to learn MTL | TKDD | \citeyearzhang2014regularization
    | Priori sharing | Clustering penalty + $\ell_{2,2}$ norm | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{\lambda}{2}\sum_{t=1}^{T}&#124;&#124;\boldsymbol{w}^{t}&#124;&#124;_{2}^{2}+$tr$(\boldsymbol{W}\boldsymbol{\Omega}^{-1}\boldsymbol{W}^{T})+d$ln$\boldsymbol{\Omega}$  s.t. $\boldsymbol{\Omega}\in\boldsymbol{S}_{+}^{D}$
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 学习MTL的正则化方法 | TKDD | \citeyearzhang2014regularization | 先验共享 | 聚类惩罚 + $\ell_{2,2}$
    范数 | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{\lambda}{2}\sum_{t=1}^{T}&#124;&#124;\boldsymbol{w}^{t}&#124;&#124;_{2}^{2}+$tr$(\boldsymbol{W}\boldsymbol{\Omega}^{-1}\boldsymbol{W}^{T})+d$ln$\boldsymbol{\Omega}$
    约束条件 $\boldsymbol{\Omega}\in\boldsymbol{S}_{+}^{D}$ |'
- en: '| Multi-linear multi-task learning | NeurIPS | \citeyearwimalawarne2014multitask
    | Low-rank learning | Scaled latent tensor trace norm | $\min\limits_{\boldsymbol{\mathcal{W}}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\inf_{\boldsymbol{\mathcal{W}}^{(1)}+\cdots+\boldsymbol{\mathcal{W}}^{(N)}=\boldsymbol{\mathcal{W}}}\lambda\sum_{k=1}^{N}I_{k}^{-1/2}\&#124;\boldsymbol{W}_{(k)}^{(k)}\&#124;_{*}$
    where $\boldsymbol{\mathcal{W}}\in\mathbb{R}^{D\times I_{2}\times\cdots\times
    I_{N}}$ is a tensor. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 多线性多任务学习 | NeurIPS | \citeyearwimalawarne2014multitask | 低秩学习 | 缩放的潜在张量迹范数
    | $\min\limits_{\boldsymbol{\mathcal{W}}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\inf_{\boldsymbol{\mathcal{W}}^{(1)}+\cdots+\boldsymbol{\mathcal{W}}^{(N)}=\boldsymbol{\mathcal{W}}}\lambda\sum_{k=1}^{N}I_{k}^{-1/2}\&#124;\boldsymbol{W}_{(k)}^{(k)}\&#124;_{*}$
    其中 $\boldsymbol{\mathcal{W}}\in\mathbb{R}^{D\times I_{2}\times\cdots\times I_{N}}$
    是一个张量。 |'
- en: '| Task Tree model | KDD | \citeyearhan2015learning | task clustering | $\ell_{2,2}$
    norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\sum_{h=1}^{H}\boldsymbol{w}_{h}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\sum_{h=1}^{H}\lambda_{h}\sum_{i<j}^{T}\&#124;\boldsymbol{w}_{h}^{i}-\boldsymbol{w}_{h}^{j}\&#124;^{2}_{2},\text{s.t.}&#124;\boldsymbol{w}_{h-1}^{i}-\boldsymbol{w}_{h-1}^{j}&#124;\succeq&#124;\boldsymbol{w}_{h}^{i}-\boldsymbol{w}_{h}^{j}&#124;,\forall
    h\geq 2,\forall i<j$ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 任务树模型 | KDD | \citeyearhan2015learning | 任务聚类 | $\ell_{2,2}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\sum_{h=1}^{H}\boldsymbol{w}_{h}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\sum_{h=1}^{H}\lambda_{h}\sum_{i<j}^{T}\&#124;\boldsymbol{w}_{h}^{i}-\boldsymbol{w}_{h}^{j}\&#124;^{2}_{2},
    \text{s.t.} \&#124;\boldsymbol{w}_{h-1}^{i}-\boldsymbol{w}_{h-1}^{j}\&#124;\succeq
    \&#124;\boldsymbol{w}_{h}^{i}-\boldsymbol{w}_{h}^{j}\&#124;,\forall h\geq 2,\forall
    i<j$ |'
- en: '| Reduced rank multi-stage MTL | AAAI | \citeyearhan2016multi | Low-rank learning
    | Capped trace norm \citepsun2013robust | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{r=1}^{R}\min\{\sigma_{r}(\boldsymbol{W}),\tau\}$
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 降秩多阶段MTL | AAAI | \citeyearhan2016multi | 低秩学习 | 截断迹范数 \citepsun2013robust
    | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{r=1}^{R}\min\{\sigma_{r}(\boldsymbol{W}),\tau\}$
    |'
- en: '1'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: This work is published in Technical Report, the Department of Statistics, UC
    Berkeley.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这项工作发表于《技术报告》，统计学系，加州大学伯克利分校。
- en: '2'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: This work is published in Jian Zhang’s Ph.D. Thesis, CMU Technical Report CMU-LTI-06-006,
    2006.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这项工作发表于简·张的博士论文，CMU技术报告CMU-LTI-06-006，2006年。
- en: 2.1.1\. Feature Selection
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1.1\. 特征选择
- en: The high-dimensional scaling \citepnegahban2008joint where the number of model
    weights is much larger than that of the observations/features, i.e., $D\gg N$,
    arises in many real-world problems, leading it costly and arduous to seek effective
    predictor variables. Sparse learning with an $\ell_{1}$ regularizer that aims
    to identify a structure characterized by a reduced number of non-zero elements.
    This parsimonious solution ensures the retention and selection of the most effective
    and efficient subset of features tailored to the target task\citeptibshirani1996regression.
    In MTL, Assumption LABEL:assump:parameter underpins the development of all sparse
    learning models. Under the settings of sparse learning, this assumption posits
    that similar sparsity patterns in model parameters suggest the relatedness between
    tasks. As a result, sparsity patterns subtly represent task relatedness, underscoring
    a subset of common features derived from these limited samples. More benefits
    and efficacy of employing sparsity in MTL have been thoroughly assessed and discussed
    in
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 高维缩放\citepnegahban2008joint，其中模型权重的数量远大于观测值/特征的数量，即$D\gg N$，在许多现实世界的问题中出现，使得寻求有效预测变量变得昂贵且艰难。稀疏学习使用$\ell_{1}$正则化器，旨在识别由减少的非零元素数量特征的结构。这种简约的解决方案确保了保留和选择最有效和高效的特征子集，以适应目标任务\citeptibshirani1996regression。在MTL中，假设LABEL:assump:parameter支撑了所有稀疏学习模型的发展。在稀疏学习的设置下，该假设认为模型参数中的相似稀疏模式暗示了任务之间的相关性。因此，稀疏模式微妙地表示任务相关性，突显了从这些有限样本中得出的共同特征子集。采用稀疏性在MTL中的更多好处和效果已被充分评估和讨论
- en: Conversion to HTML had a Fatal error and exited abruptly. This document may
    be truncated or damaged.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为HTML时发生致命错误并突然退出。此文档可能被截断或损坏。
