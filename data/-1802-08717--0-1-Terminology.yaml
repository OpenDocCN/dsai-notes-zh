- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:08:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:08:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1802.08717] 0.1 Terminology'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1802.08717] 0.1 术语'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1802.08717](https://ar5iv.labs.arxiv.org/html/1802.08717)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1802.08717](https://ar5iv.labs.arxiv.org/html/1802.08717)
- en: 0.1 Terminology
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0.1 术语
- en: 'To understand deep learning, it is helpful to first understand the related
    concepts of artificial intelligence and machine learning. Artificial intelligence
    is a set of computer algorithms that are able to perform complicated tasks or
    tasks that require intelligence when conducted by humans. Machine learning is
    a subset of artificial intelligence algorithms which, to perform these complicated
    tasks, are able to learn from provided data and do not require pre-defined rules
    of reasoning. The field of machine learning is very diverse and has already had
    notable applications in medical imaging [Erickson2017]. Deep learning is a sub-discipline
    of machine learning that relies on networks of simple interconnected units. In
    deep learning models, these units are connected to form multiple layers that are
    capable of generating increasingly high level representations of the provided
    input (e.g., images). Below, in order to explain the architecture of deep learning
    models, we introduce the artificial neural network in general and one specific
    type: the convolutional neural network. Then, we detail the ”learning” process
    of these networks, which is the process of incorporating the patterns extracted
    from data into deep neural networks.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解深度学习，首先了解人工智能和机器学习的相关概念是有帮助的。人工智能是一组能够执行复杂任务或需要人类智慧的任务的计算机算法。机器学习是人工智能算法的一个子集，它能够从提供的数据中学习，不需要预定义的推理规则，以执行这些复杂的任务。机器学习领域非常广泛，已经在医学影像[Erickson2017]中有了显著应用。深度学习是机器学习的一个子学科，它依赖于简单互联单元的网络。在深度学习模型中，这些单元连接形成多个层，能够生成对提供输入（如图像）的越来越高级的表示。下面，为了说明深度学习模型的架构，我们介绍了人工神经网络的一般概念以及一种具体类型：卷积神经网络。然后，我们详细描述了这些网络的“学习”过程，即将从数据中提取的模式融入深度神经网络的过程。
- en: 0.2 Artificial Neural Networks
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0.2 人工神经网络
- en: Artificial neural networks (ANNs) are machine learning models based on basic
    concepts dating as far as 1940s, significant development in 1970s and 1980s and
    a period notable popularity in 1990s and 2000s followed by a period of being overshadowed
    by other machine learning algorithms. ANNs consist of a multitude of interconnected
    processing units, called neurons, usually organized in layers. A traditional ANN
    typically used in the practice of machine learning contains 2 to 3 layers of neurons.
    Each neuron performs a very simple operation. While many neuron models were proposed,
    a typical neuron simply multiplies each input by a certain weight, then adds all
    the products for all the inputs and applies a simple nondecreasing function at
    the end. Even though each neuron performs a very rudimentary calculation, the
    interconnected nature of the network allows for the performance of very sophisticated
    calculations and implementation of very complicated functions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）是基于1940年代的基本概念的机器学习模型，1970年代和1980年代有显著发展，1990年代和2000年代有较高的流行度，随后被其他机器学习算法所掩盖。ANNs由大量相互连接的处理单元（称为神经元）组成，通常组织成层。一个传统的ANN通常包含2到3层神经元。每个神经元执行非常简单的操作。虽然提出了许多神经元模型，但一个典型的神经元只是将每个输入乘以某个权重，然后将所有输入的乘积相加，最后应用一个简单的非递减函数。尽管每个神经元执行的是非常基础的计算，但网络的互联特性使得可以执行非常复杂的计算并实现非常复杂的功能。
- en: 0.3 Convolutional Neural Networks
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0.3 卷积神经网络
- en: Deep neural networks are a special type of an ANN. The most common type of a
    deep neural network is a deep convolutional neural network (CNN). A deep convolutional
    neural network, while inheriting the properties of a generic ANN, has also its
    own specific features. First, it is deep. A typical number of layers is 10\nobreakdash-30
    but in extreme cases it could exceed 1 000. Second, the neurons are connected
    such that multiple neurons share weights. This effectively allows the network
    to perform convolutions (or template matching) of the input image with the filters
    (defined by the weights) within the CNN. Other special feature of CNNs is that
    between some layers, they perform pooling which makes the network invariant to
    small shifts of the images. Finally, CNNs typically use a different activation
    function of the neurons as compared to traditional ANNs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络是人工神经网络的一种特殊类型。最常见的深度神经网络类型是深度卷积神经网络 (CNN)。深度卷积神经网络虽然继承了通用人工神经网络的特性，但也有其特定的特点。首先，它是深度的。典型的层数为
    10\nobreakdash-30 层，但在极端情况下可能超过 1 000 层。其次，神经元的连接方式使得多个神经元共享权重。这使得网络能够对输入图像进行卷积（或模板匹配）操作，使用
    CNN 内部的滤波器（由权重定义）。CNN 的另一个特殊特征是它们在一些层之间执行池化操作，使网络对图像的小幅移动不变。最后，与传统的人工神经网络相比，CNN
    通常使用不同的激活函数。
- en: '![Refer to caption](img/6a4d096f5f867b5efb5e12217a69813f.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6a4d096f5f867b5efb5e12217a69813f.png)'
- en: 'Figure 1: A diagram illustrating a typical architecture of a convolutional
    neural network.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 说明卷积神经网络典型架构的示意图。'
- en: Figure [1](#S0.F1 "Figure 1 ‣ 0.3 Convolutional Neural Networks") shows an example
    of a small architecture for a typical CNN. One can see that the first layers are
    the convolutional ones which serve the role of generating useful features for
    classification. Those layers can be thought of as implementing image filters,
    ranging from simple filters that match edges to those that eventually match much
    more complicated shapes such as eyes, or tumors. Further from the network input
    are so called fully connected layers (similar to traditional ANNs) which utilize
    the features extracted by the convolutional layers to generate a decision (e.g.,
    assign a label). A variety of deep learning architectures have been proposed,
    often driven by characteristics of the task at hand (e.g., fully convolutional
    neural networks for image segmentation). Some of these are described in more detail
    in the section of this paper that reviews the current state of the art.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S0.F1 "Figure 1 ‣ 0.3 Convolutional Neural Networks") 显示了典型 CNN 的一个小型架构示例。可以看到，前几层是卷积层，它们的作用是生成用于分类的有用特征。这些层可以看作是实现图像滤波的，从简单的边缘匹配滤波器到最终匹配更复杂形状（如眼睛或肿瘤）的滤波器。网络输入处更远的层被称为全连接层（类似于传统的人工神经网络），它们利用卷积层提取的特征生成决策（例如，分配标签）。已经提出了各种深度学习架构，通常由手头任务的特征驱动（例如，用于图像分割的全卷积神经网络）。这些在本文中回顾当前最先进技术的部分有更详细的描述。
- en: 0.4 The learning process in convolutional neural networks
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0.4 卷积神经网络中的学习过程
- en: 'Above, we described general characteristics of traditional neural networks
    and deep learning’s flagship: the convolutional neural network. Next, we will
    explore how to make those networks perform useful tasks. This is accomplished
    in the process referred to as learning or training. The learning process of a
    convolutional neural network simply consists of changing the weights of the individual
    neurons in response to the provided data. In the most popular type of learning
    process, called supervised learning, a training example contains an object of
    interest (e.g., an image of a tumor) and a label (e.g., the tumor’s pathology:
    benign or malignant). In our example, the image is presented to the network’s
    input, and the calculation is carried out within the network to produce a prediction
    based on the current weights of the network. Then, the network’s prediction is
    compared to the actual label of the object and an error is calculated. This error
    is then propagated through the network to change the values of the network’s weights
    such that the next time the network analyzes this example, the error decreases.
    In practice, the adaptation of the weights is performed after a group of examples
    (a batch) are presented to the network. This process is called error backpropagation
    or stochastic gradient descent. Various modifications of stochastic gradient descent
    algorithm have been developed [ruder2016overview]. In principle, this iterative
    process consists of calculations of error between the output of the model and
    the desired output and adjusting the weights in the direction where the error
    decreases.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容描述了传统神经网络和深度学习的旗舰：卷积神经网络的一般特征。接下来，我们将深入探讨如何使这些网络执行有用的任务。这是在所谓的学习或训练过程中实现的。卷积神经网络的学习过程简单地包括根据提供的数据改变各个神经元的权重。在最流行的学习过程中，称为监督学习，训练示例包含一个感兴趣的对象（例如肿瘤的图像）和一个标签（例如肿瘤的病理：良性或恶性）。在我们的示例中，图像被呈现给网络的输入，计算在网络内进行，以基于当前的网络权重生成预测。然后，将网络的预测与对象的实际标签进行比较，并计算错误。这个错误随后在网络中传播，以改变网络权重的值，使得下一次网络分析这个示例时，错误减少。实际上，权重的调整是在一组示例（一个批次）呈现给网络后进行的。这个过程称为误差反向传播或随机梯度下降。已经开发了各种随机梯度下降算法的修改版 [ruder2016overview]。原则上，这个迭代过程包括计算模型输出与期望输出之间的误差，并调整权重以减少误差的方向。
- en: '![Refer to caption](img/802784e2c6018e169eea4a91088c0b84.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/802784e2c6018e169eea4a91088c0b84.png)'
- en: 'Figure 2: An illustration of different ways of training in deep neural networks.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：深度神经网络中不同训练方法的示意图。
- en: 'The most straightforward way of training is to start with a random set of weights
    and train using available data specific to the problem being solved (training
    from scratch). However, given the large number of parameters (weights) in a network,
    often above 10 million, and a limited amount of training data (common in medical
    imaging), a network may overfit to the available data, resulting in poor performance
    on test data. Two training methods have been developed to address this issue:
    transfer learning [yosinski2014transferable] and off-the-shelf features (a.k.a.
    deep features) [sharif2014cnn]. A diagram comparing training from scratch with
    transfer learning and off-the-shelf deep features is shown in Figure [2](#S0.F2
    "Figure 2 ‣ 0.4 The learning process in convolutional neural networks").'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练最直接的方法是从随机的权重集开始，并使用特定于问题的数据进行训练（从头开始训练）。然而，由于网络中参数（权重）的数量往往超过1000万，并且训练数据有限（在医学影像中常见），网络可能会过拟合这些数据，导致在测试数据上的表现不佳。为解决此问题，已经开发了两种训练方法：迁移学习 [yosinski2014transferable]和现成特征（即深度特征） [sharif2014cnn]。图 [2](#S0.F2
    "Figure 2 ‣ 0.4 The learning process in convolutional neural networks")展示了从头开始训练与迁移学习和现成深度特征的比较图。
- en: In the transfer learning approach, the network is first trained using a different
    dataset, for example an ImageNet collection. Then, the network is ”fine-tuned”
    through additional training with data specific to the problem to be addressed.
    The idea behind this approach is that solving different visual tasks shares a
    certain level of processing such as recognition of edges or simple shapes. This
    approach has been shown successful in, for example, prediction of survival time
    from brain MRI in patients with glioblastoma tumor [ahmed2017fine] or in skin
    lesion classification [Esteva]. Another approach that addresses the issue of limited
    training data is the deep ”off-the-shelf” features approach which uses convolutional
    neural networks which have been trained on a different dataset to extract features
    from the images. This is done by extracting outputs of layers prior to the network’s
    final layer. Those layers typically have hundreds or thousands of outputs. Then,
    these outputs are used as inputs to ”traditional” classifiers such as linear discriminant
    analysis, support vector machines, or decision trees. This is similar to transfer
    learning (and is sometimes considered a part of transfer learning) with the difference
    being that the last layers of a CNN are replaced by a traditional classifier and
    the early layers are not additionally trained.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习方法中，网络首先使用不同的数据集进行训练，例如 ImageNet 集合。然后，通过使用特定于待解决问题的数据进行额外训练，将网络“微调”。这种方法的想法是解决不同的视觉任务共享某些级别的处理，例如边缘或简单形状的识别。这种方法已在预测脑部
    MRI 中的生存时间（例如，胶质母细胞瘤患者 [ahmed2017fine]）或皮肤病变分类 [Esteva] 中证明了成功。另一个解决有限训练数据问题的方法是深度“现成”特征方法，它使用在不同数据集上训练过的卷积神经网络来提取图像特征。这是通过提取网络最终层之前的层的输出完成的。这些层通常具有数百或数千个输出。然后，这些输出被用作“传统”分类器的输入，例如线性判别分析、支持向量机或决策树。这类似于迁移学习（有时被认为是迁移学习的一部分），区别在于
    CNN 的最后几层被传统分类器替代，早期层则不再进行额外训练。
- en: 0.5 Deep learning vs ”traditional” machine learning
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0.5 深度学习与“传统”机器学习
- en: Increasingly often we hear a distinction between deep learning and ”traditional”
    machine learning (see Figure [3](#S0.F3 "Figure 3 ‣ 0.5 Deep learning vs ”traditional”
    machine learning")). The difference is very important, particularly in the context
    of medical imaging. In traditional machine learning, the typical first step is
    feature extraction. This means that to classify an object, one must decide which
    characteristics of an object will be important and implement algorithms that are
    able to capture these characteristics. A number of sophisticated algorithms in
    the field of computer vision have been proposed for this purpose and a variety
    of size, shape, texture, and other features were extracted. This process is to
    a large extent arbitrary since the machine learning researcher or practitioner
    often must guess which features will be of use for a particular task and runs
    the risk of including useless and redundant features and, more importantly, not
    including truly useful features. In deep learning, the process of feature extraction
    and decision making are merged and trainable, and therefore no choices need to
    be made regarding which features should be extracted; this is decided by the network
    in the training process. However, the cost of allowing the neural network to select
    its own features is a requirement for much larger training data sets.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们越来越频繁地听到深度学习与“传统”机器学习之间的区别（见图[3](#S0.F3 "Figure 3 ‣ 0.5 Deep learning vs ”traditional”
    machine learning")）。这种差异非常重要，特别是在医学成像的背景下。在传统机器学习中，典型的第一步是特征提取。这意味着为了对一个对象进行分类，必须决定对象的哪些特征是重要的，并实现能够捕捉这些特征的算法。在计算机视觉领域，已经提出了一些复杂的算法用于此目的，并提取了各种大小、形状、纹理及其他特征。这一过程在很大程度上是随意的，因为机器学习研究人员或从业者通常必须猜测哪些特征对特定任务有用，并且有可能包含无用和冗余的特征，更重要的是，可能遗漏真正有用的特征。在深度学习中，特征提取和决策过程被合并并且可训练，因此无需决定提取哪些特征；这一决定由网络在训练过程中做出。然而，允许神经网络选择自己的特征的代价是需要更大的训练数据集。
- en: '![Refer to caption](img/34fb41137c5ad57f729ee9f95527908b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34fb41137c5ad57f729ee9f95527908b.png)'
- en: 'Figure 3: An illustration of difference between “traditional” machine learning
    and deep learning.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: “传统”机器学习与深度学习之间差异的示意图。'
