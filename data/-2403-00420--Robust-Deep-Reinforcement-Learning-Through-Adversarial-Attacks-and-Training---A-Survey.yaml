- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:34:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:34:11'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2403.00420] Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2403.00420] 通过对抗攻击和训练提高深度强化学习的鲁棒性：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00420](https://ar5iv.labs.arxiv.org/html/2403.00420)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00420](https://ar5iv.labs.arxiv.org/html/2403.00420)
- en: \equalcont
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \equalcont
- en: These authors contributed equally to this work.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些作者对本工作做出了同等贡献。
- en: \equalcont
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \equalcont
- en: These authors contributed equally to this work.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些作者对本工作做出了同等贡献。
- en: 1]\orgnameIRT SystemX, \orgaddress\cityPalaiseau, \postcode91120, \countryFrance
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 1]\orgnameIRT SystemX, \orgaddress\cityPalaiseau, \postcode91120, \countryFrance
- en: 2]\orgdivMLIA, ISIR, \orgnameSorbonne Université, \orgaddress\cityParis, \postcode75005,
    \countryFrance
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 2]\orgdivMLIA, ISIR, \orgnameSorbonne Université, \orgaddress\cityParis, \postcode75005,
    \countryFrance
- en: 3]\orgnamePolytechnique Montréal, \orgaddress\cityMontréal, \postcode6079, \stateQuébec,
    \countryCanada
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 3]\orgnamePolytechnique Montréal, \orgaddress\cityMontréal, \postcode6079, \stateQuébec,
    \countryCanada
- en: 4]\orgnameSafran Tech, \orgaddress\cityChâteaufort, \postcode78117, \countryFrance
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 4]\orgnameSafran Tech, \orgaddress\cityChâteaufort, \postcode78117, \countryFrance
- en: 5]\orgdivLERIA, \orgnameUniversité d’Angers, \orgaddress\cityAngers, \postcode49000,
    \countryFrance
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 5]\orgdivLERIA, \orgnameUniversité d’Angers, \orgaddress\cityAngers, \postcode49000,
    \countryFrance
- en: 'Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过对抗攻击和训练提高深度强化学习的鲁棒性：综述
- en: \fnmLucas \surSchott    \fnmJoséphine \surDelas    \fnmHatem \surHajri    \fnmElies
    \surGherbi    \fnmReda \surYaich    \fnmNora \surBoulahia-Cuppens    \fnmFrederic
    \surCuppens    \fnmSylvain \surLamprier [ [ [ [ [
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \fnmLucas \surSchott    \fnmJoséphine \surDelas    \fnmHatem \surHajri    \fnmElies
    \surGherbi    \fnmReda \surYaich    \fnmNora \surBoulahia-Cuppens    \fnmFrederic
    \surCuppens    \fnmSylvain \surLamprier [ [ [ [ [
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep Reinforcement Learning (DRL) is an approach for training autonomous agents
    across various complex environments. Despite its significant performance in well
    known environments, it remains susceptible to minor conditions variations, raising
    concerns about its reliability in real-world applications. To improve usability,
    DRL must demonstrate trustworthiness and robustness. A way to improve robustness
    of DRL to unknown changes in the conditions is through Adversarial Training, by
    training the agent against well suited adversarial attacks on the dynamics of
    the environment. Addressing this critical issue, our work presents an in-depth
    analysis of contemporary adversarial attack methodologies, systematically categorizing
    them and comparing their objectives and operational mechanisms. This classification
    offers a detailed insight into how adversarial attacks effectively act for evaluating
    the resilience of DRL agents, thereby paving the way for enhancing their robustness.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）是一种用于训练在各种复杂环境中自主代理的方法。尽管在已知环境中表现显著，但它对条件的微小变化仍然敏感，这引发了对其在实际应用中可靠性的担忧。为了提高可用性，DRL
    必须展现出可信性和鲁棒性。提高 DRL 对未知条件变化的鲁棒性的一种方法是通过对抗训练，通过对环境动态进行适当的对抗攻击来训练代理。针对这一关键问题，我们的工作对现代对抗攻击方法进行了深入分析，系统地对其进行分类，并比较了它们的目标和操作机制。这种分类为了解对抗攻击如何有效评估
    DRL 代理的韧性提供了详细的见解，从而为增强其鲁棒性铺平了道路。
- en: 'keywords:'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'keywords:'
- en: Deep Reinforcement Learning, Robustness, Adversarial Attacks, Adversarial Training
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习、鲁棒性、对抗攻击、对抗训练
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 Introduction
- en: The advent of Deep Reinforcement Learning (DRL) has marked a significant shift
    in various fields, including games [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)],
    autonomous robotics [[4](#bib.bib4)], autonomous driving [[5](#bib.bib5)], and
    energy management [[6](#bib.bib6)]. By integrating Reinforcement Learning (RL)
    with Deep Neural Networks (DNN), DRL can leverages high dimensional continuous
    observations and rewards to train neural policies, without the need for supervised
    example trajectories.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）的出现标志着在各个领域的重大转变，包括游戏 [[1](#bib.bib1)、[2](#bib.bib2)、[3](#bib.bib3)]、自主机器人
    [[4](#bib.bib4)]、自动驾驶 [[5](#bib.bib5)] 和能源管理 [[6](#bib.bib6)]。通过将强化学习（RL）与深度神经网络（DNN）结合，DRL
    能够利用高维连续观察和奖励来训练神经策略，而无需监督的示例轨迹。
- en: While DRL achieves remarkable performances in well known controlled environments,
    it also encounter challenges in ensuring robust performance amid diverse condition
    changes and real-world perturbations. It particularly struggle to bridge the reality
    gap [[7](#bib.bib7), [8](#bib.bib8)], often DRL agents are trained in simulation
    that remains an imitation of the real-world, resulting in a gap between the performance
    of a trained agent in the simulation and its performance once transferred to the
    real-world application. Even without trying to bridge the reality gap, agents
    can be trained in the first place in some conditions, and be deployed later and
    the conditions may have changed since. This pose the problem of robustness, which
    refers to the agent’s ability to maintain performance in deployment despite slight
    conditions changes in the environment or minor perturbations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度强化学习（DRL）在知名的受控环境中取得了显著的成果，但它在确保在各种条件变化和现实世界扰动中保持稳健性能时也面临挑战。它特别难以弥合现实差距
    [[7](#bib.bib7), [8](#bib.bib8)]，通常DRL智能体是在模拟环境中训练的，而这些模拟环境仍然是对现实世界的模仿，导致训练智能体在模拟环境中的性能与转移到现实世界应用后的性能之间存在差距。即使不尝试弥合现实差距，智能体在某些条件下也可以先进行训练，之后部署时条件可能已发生变化。这就提出了稳健性的问题，即智能体在部署过程中，尽管环境条件发生轻微变化或出现小的扰动，仍能保持性能的能力。
- en: Moreover the emergence of adversarial attacks that generate perturbation in
    the inputs and disturbances in the dynamics of the environment, which are deliberately
    designed to mislead neural network decisions, poses unique challenges in RL [[9](#bib.bib9),
    [10](#bib.bib10)] and can be a key to resolve the robustness problem in RL, necessitating
    further exploration and understanding.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，产生输入扰动和环境动态扰动的对抗攻击的出现，这些攻击故意设计以误导神经网络决策，在强化学习中带来了独特的挑战 [[9](#bib.bib9), [10](#bib.bib10)]，并可能是解决RL中稳健性问题的关键，需进一步探索和理解。
- en: This survey aims to address these critical areas of concern. It focuses on key
    issues by presenting a comprehensive framework for understanding the concept of
    robustness of DRL agent. It covers both robustness to perturbed inputs as well
    as robustness to perturbed dynamics of the environment. Additionally, it introduces
    a new classification system that organizes every type of perturbation affecting
    robustness into a unified model. It also offers a review of the existing literature
    on adversarial methods for robust DRL agents and classify the existing methods
    in the proposed taxonomy. The goal is to provide a deeper understanding of various
    adversarial techniques, including their strengths, limitations, and the impact
    they have on the performance, robustness and generalization capabilities of DRL
    agents.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查旨在解决这些关键问题。它通过呈现一个全面的框架来理解DRL智能体稳健性的概念，重点关注关键问题。它涵盖了对扰动输入和环境动态扰动的稳健性。此外，它引入了一个新的分类系统，将影响稳健性的每种扰动组织到一个统一的模型中。它还提供了关于稳健DRL智能体对抗方法的现有文献综述，并在提出的分类体系中对现有方法进行分类。目标是提供对各种对抗技术的深入理解，包括它们的优缺点，以及它们对DRL智能体的性能、稳健性和泛化能力的影响。
- en: Historically, the primary focus on adversarial examples has been in the realm
    of supervised learning [[11](#bib.bib11)]. Attempts to extend this scope to RL
    have been made, but these have primarily concentrated on adversarial evasion methods
    and robustness-oriented classification [[12](#bib.bib12), [9](#bib.bib9)]. To
    bridge this gap, our work introduces a robustness-centric study of adversarial
    methods in DRL.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，对抗样本的主要关注点一直是监督学习领域 [[11](#bib.bib11)]。虽然尝试将这一范围扩展到强化学习（RL），但这些尝试主要集中在对抗规避方法和稳健性导向的分类上
    [[12](#bib.bib12), [9](#bib.bib9)]。为了弥合这一差距，我们的工作引入了一个以稳健性为中心的DRL对抗方法研究。
- en: 'The key contributions of this work include:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的关键贡献包括：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Formalizing the concept of Robustness in DRL.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 形式化DRL中稳健性的概念。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Developing a taxonomy and classification for adversarial attack in DRL.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为DRL中的对抗攻击开发分类法和分类体系。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reviewing existing adversarial attack, characterized using our proposed taxonomy.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评审现有的对抗攻击，并使用我们提出的分类体系进行特征化。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reviewing how adversarial attacks can be used to improve robustness of DRL agents.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评审对抗攻击如何用于提高DRL智能体的稳健性。
- en: 'The structure of the survey is organized as follows: Section [2](#S2 "2 Background
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") provides an introduction to RL and the security implications of DNNs,
    as well as the mathematical prerequisites for analyzing RL Robustness. Section
    [3](#S3 "3 Formalization and Scope ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey") introduces a formalization of the
    notion of Robustness in DRL. Section [4](#S4 "4 Taxonomy of Adversarial Attacks
    of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") presents a taxonomy for categorizing adversarial attack methods as
    shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey"). Sections [5.1](#S5.SS1
    "5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") and [5.2](#S5.SS2
    "5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") explore observation and
    dynamic alterations attacks, respectively. Finally section [6](#S6 "6 Strategies
    of Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and
    Training : A Survey") focuses on strategies for applying adversarial attacks and
    adversarial training.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '调查的结构组织如下：第[2](#S2 "2 Background ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey")节介绍了RL和DNN的安全影响，以及分析RL鲁棒性的数学先决条件。第[3](#S3
    "3 Formalization and Scope ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey")节介绍了DRL鲁棒性概念的形式化。第[4](#S4 "4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey")节提供了一个分类攻击方法的分类法，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")所示。[5.1](#S5.SS1 "5.1 Observation Alteration Attacks ‣ 5 Adversarial
    Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")节和[5.2](#S5.SS2 "5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣
    Robust Deep Reinforcement Learning Through Adversarial Attacks and Training :
    A Survey")节分别探讨了观察和动态修改攻击。最后，第[6](#S6 "6 Strategies of Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey")节专注于应用对抗性攻击和对抗性训练的策略。'
- en: '![Refer to caption](img/fdddcac16768e24135975a9ef3bf9d1e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/fdddcac16768e24135975a9ef3bf9d1e.png)'
- en: 'Figure 1: Categorization of the adversarial attacks of the literature as described
    in Section[5](#S5 "5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") with the taxonomy introduced
    Section [4](#S4 "4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") of this survey.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：文献中描述的对抗攻击的分类，详见第[5](#S5 "5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey")节，以及本调查的[4](#S4
    "4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey")节中介绍的分类法。'
- en: 2 Background
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Reinforcement Learning
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 强化学习
- en: 'RL focuses on decision-making in dynamic environments [[13](#bib.bib13)]. RL
    agents learn by interacting with an environment: they take actions and receive
    feedback in terms of numerical rewards. The objective of a RL agent is to learn
    a policy, a mapping from states to actions, which maximizes the expected cumulative
    reward over time.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）专注于动态环境中的决策制定[[13](#bib.bib13)]。RL代理通过与环境进行交互来学习：他们采取行动，并以数字奖励的形式获得反馈。RL代理的目标是学习策略，即从状态到动作的映射，以最大化随时间累积的预期奖励。
- en: 2.1.1 Partially Observable Markov Decision Process
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 部分可观察马尔可夫决策过程
- en: 'A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making
    problems where an agent interacts with an environment over discrete time steps.
    In most real-world applications, the agent may not have access to the environment’s
    complete states and instead receives partial observations. This scenario is known
    as a Partially Observable Markov Decision Process (POMDP), which is a generalization
    of the MDP framework, represented by the tuple $\Omega=(S,A,T,R,X,O)$, where:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）是用于建模代理与环境在离散时间步长上交互的数学框架。在大多数实际应用中，代理可能无法访问完整的环境状态，而是接收部分观察。这种情况被称为部分可观察马尔可夫决策过程（POMDP），它是MDP框架的一种泛化，由元组$\Omega=(S,A,T,R,X,O)$来表示，其中：
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $S$ is the set of states in the environment,
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S$ 是环境中的状态集，
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $A$ is the set of actions available to the agent,
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A$ 是代理可执行的动作集，
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $T:S\times S\times A\rightarrow[0,1]$ is the stochastic transition function,
    with $T(s_{+}|s,a)$ denoting the probability of transitioning to state $s_{+}$
    given state $s$ and action $a$,
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $T:S\times S\times A\rightarrow[0,1]$ 是随机转移函数，其中 $T(s_{+}\mid s,a)$ 表示在状态 $s$
    和动作 $a$ 下转移到状态 $s_{+}$ 的概率，
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $R:S\times A\times S\rightarrow\mathbb{R}$ is the reward function. $R(s,a,s_{+})$
    is received by the agent for taking action $a$ in state $s$ and moving to state
    $s_{+}$,
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $R:S\times A\times S\rightarrow\mathbb{R}$ 是奖励函数。$R(s,a,s_{+})$ 是代理在状态 $s$ 执行动作
    $a$ 并转移到状态 $s_{+}$ 时获得的奖励，
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $X$ is the set of observations as perceived by the agent,
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $X$ 是代理感知的观察集合，
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $O:S\times X\rightarrow[0,1]$ is the observation function, with $O(x|s)$ denoting
    the probability of observing $x$ given state $s$.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $O:S\times X\rightarrow[0,1]$ 是观察函数，其中 $O(x\mid s)$ 表示在状态 $s$ 下观察到 $x$ 的概率。
- en: A step in the environment represented by the POMDP $\Omega$ is represented by
    the transition $(s_{t},x_{t},a_{t},s_{t+1})$, where $s_{t}$ stands for the sate,
    $x_{t}$ the observation of this state, $a_{t}$ the action applied by the agent,
    $s_{t+1}$ the next state after transition. In this paper, we will use the POMDP
    framework as a general model, even though some environments could be described
    as MDPs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中的一步由 POMDP $\Omega$ 表示，表示为转移 $(s_{t},x_{t},a_{t},s_{t+1})$，其中 $s_{t}$ 表示状态，$x_{t}$
    表示该状态的观察，$a_{t}$ 表示代理执行的动作，$s_{t+1}$ 表示转移后的下一个状态。在本文中，我们将使用 POMDP 框架作为通用模型，尽管某些环境可以描述为
    MDPs。
- en: 2.1.2 Fundamentals of Reinforcement Learning
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 强化学习基础
- en: 'In RL, the goal is to learn a policy $\pi:A\times S\rightarrow[0,1]$, $\pi(a|s)$
    denoting the probability of selecting the action $a$ given state $s$. The optimal
    policy, denoted as $\pi^{*}$, therefore maximizes the expected cumulative discounted
    reward :'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，目标是学习一个策略 $\pi:A\times S\rightarrow[0,1]$，其中 $\pi(a\mid s)$ 表示在状态 $s$
    下选择动作 $a$ 的概率。因此，最优策略 $\pi^{*}$ 最大化期望的累计折扣奖励：
- en: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)]$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)]$ |  |'
- en: with
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: with
- en: '|  | $R(\tau)=\sum_{t=0}^{&#124;\tau&#124;}\gamma^{t}R(s_{t},a_{t},s_{t+1})$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\tau)=\sum_{t=0}^{\lvert\tau\rvert}\gamma^{t}R(s_{t},a_{t},s_{t+1})$
    |  |'
- en: where $\tau=(s_{0},a_{0},s_{1},...,s_{|\tau|})$ is sampled from the distribution
    $\pi^{\Omega}$ of trajectories obtained by executing policy $\pi$ in environment
    $\Omega$. The discount factor $\gamma$, ranging from 0 to 1, weights the importance
    of future rewards.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau=(s_{0},a_{0},s_{1},...,s_{|\tau|})$ 是从通过执行策略 $\pi$ 在环境 $\Omega$ 中获得的轨迹分布
    $\pi^{\Omega}$ 中采样得到的。折扣因子 $\gamma$ 范围从 0 到 1，权衡未来奖励的重要性。
- en: 'An important criterion for defining optimality is the state value function,
    denoted as $V^{\pi}:S\rightarrow\mathbb{R}$. For a state $s$, the value $V^{\pi}(s)$
    represents the expected cumulative discounted reward starting from $s$ and following
    the policy $\pi$ thereafter. This can be formally expressed as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 定义最优性的一个重要标准是状态价值函数，记作 $V^{\pi}:S\rightarrow\mathbb{R}$。对于状态 $s$，值 $V^{\pi}(s)$
    表示从 $s$ 开始并随后遵循策略 $\pi$ 的期望累计折扣奖励。这可以正式表示为：
- en: '|  | $V^{\pi}(s)=\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)&#124;s_{0}=s]$ |  |
    (1) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $V^{\pi}(s)=\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)\mid s_{0}=s]$ |  |
    (1) |'
- en: 'It can be expressed recursively with the Bellman equation :'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用贝尔曼方程递归表示：
- en: '|  | $\begin{gathered}V^{\pi}(s)=\\ \scalebox{0.95}{$\displaystyle\sum_{a}\pi(a&#124;s)\sum_{s_{+}}T(s_{+}&#124;s,a)\Big{(}R(s,a,s_{+})+\gamma
    V^{\pi}(s_{+})\Big{)}$}\end{gathered}$ |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}V^{\pi}(s)=\\ \scalebox{0.95}{$\displaystyle\sum_{a}\pi(a\mid
    s)\sum_{s_{+}}T(s_{+}\mid s,a)\Big{(}R(s,a,s_{+})+\gamma V^{\pi}(s_{+})\Big{)}$}\end{gathered}$
    |  |'
- en: 'Finally, the state-action value function $Q^{\pi}:S\times A\rightarrow R$ is
    used in many algorithms as an alternative to $V^{\pi}$. The Q-value function of
    a state $s$ and action $a$ is the expected cumulative discounted reward, starting
    from $s$, taking $a$, and following $\pi$:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，状态-动作价值函数 $Q^{\pi}:S\times A\rightarrow R$ 在许多算法中作为 $V^{\pi}$ 的替代。状态 $s$
    和动作 $a$ 的 Q 值函数是从 $s$ 开始，执行 $a$ 并随后按照 $\pi$ 行动的期望累计折扣奖励：
- en: '|  | $Q^{\pi}(s,a)=\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)&#124;s_{0}=s,a_{0}=a]$
    |  | (2) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{\pi}(s,a)=\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)\mid s_{0}=s,a_{0}=a]$
    |  | (2) |'
- en: 'It can be expressed recursively with the equation :'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用方程递归表示：
- en: '|  | $\begin{gathered}Q^{\pi}(s,a)=\\ \scalebox{0.95}{$\displaystyle\sum_{s_{+}}T(s_{+}&#124;s,a)\Big{(}R(s,a,s_{+})+\gamma\sum_{a_{+}}\pi(a_{+}&#124;s_{+})\big{[}Q^{\pi}(s_{+},a_{+})\big{]}\Big{)}$}\end{gathered}$
    |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}Q^{\pi}(s,a)=\\ \scalebox{0.95}{$\displaystyle\sum_{s_{+}}T(s_{+}\mid
    s,a)\Big{(}R(s,a,s_{+})+\gamma\sum_{a_{+}}\pi(a_{+}\mid s_{+})\big{[}Q^{\pi}(s_{+},a_{+})\big{]}\Big{)}$}\end{gathered}$
    |  |'
- en: 'In the POMDP setting, since states are not directly observable by agents, the
    practice is to base policies and value functions on the history of observations
    (i.e., $x_{0:t}$ at step $t$) in place of the true state of the system (i.e.,
    $s_{t}$). For the ease of notations, we consider in the following policies and
    value functions defined with only the last observation as input (i.e., $x_{t}$),
    while every approach presented below can be extended to methods leveraging full
    histories of observations. More specifically, we consider in the following policies
    defined as $\pi:A\times X\rightarrow[0;1]$ and action-value functions as $Q:A\times
    X\rightarrow\mathbb{R}$. Figure [2](#S2.F2 "Figure 2 ‣ 2.1.2 Fundamentals of Reinforcement
    Learning ‣ 2.1 Reinforcement Learning ‣ 2 Background ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") shows the flowchart
    of an agent with a policy function $\pi$ interacting with a POMDP environment.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在部分可观测马尔可夫决策过程（POMDP）环境中，由于状态无法被代理直接观测，通常基于观察历史（即，$x_{0:t}$ 在步骤 $t$）来制定策略和价值函数，而不是系统的真实状态（即，$s_{t}$）。为了简化符号，我们在以下内容中考虑仅用最后一次观察作为输入（即，$x_{t}$）定义的策略和价值函数，尽管下面展示的每种方法都可以扩展到利用完整观察历史的方法。更具体地，我们在以下内容中考虑定义为
    $\pi:A\times X\rightarrow[0;1]$ 的策略和定义为 $Q:A\times X\rightarrow\mathbb{R}$ 的行动价值函数。图
    [2](#S2.F2 "图 2 ‣ 2.1.2 强化学习基础 ‣ 2.1 强化学习 ‣ 2 背景 ‣ 通过对抗攻击和训练的稳健深度强化学习：综述") 展示了带有策略函数
    $\pi$ 的代理与 POMDP 环境交互的流程图。
- en: '![Refer to caption](img/29f2cd9575252cbc21ef8ad7edc3ecda.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/29f2cd9575252cbc21ef8ad7edc3ecda.png)'
- en: 'Figure 2: Flowchart of an agent with a policy function $\pi$ interacting with
    a POMDP environment'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：带有策略函数 $\pi$ 的代理与 POMDP 环境交互的流程图
- en: 2.2 Neural Networks and Deep Reinforcement Learning
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 神经网络与深度强化学习
- en: To solve the complex task of RL problems in a large input space and enable generalization,
    RL methods are combined with DNNs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决大输入空间中的复杂强化学习（RL）问题并实现泛化，RL 方法与 DNNs 结合使用。
- en: 2.2.1 Deep Neural Networks (DNNs)
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 深度神经网络（DNNs）
- en: 'A neural network is a system of interconnected nodes (neurons) that process
    and transmit signals. DNNs are models utilizing multiple layers of neurons, featuring
    varying degrees of architecture complexity, to analyze intricate data patterns.
    Training involves adjusting inter-neuron weights parameters to reduce errors (called
    loss function) between the network’s predictions and actual outcomes, often employing
    Stochastic Gradient Descent (SGD) inspired algorithms. This training refines the
    network’s ability to recognize and respond to input data accurately. The update
    rule of the parameters $\theta$ of the model $f_{\theta}$ in this context, given
    inputs $x$, labels $y$, learning rate $\alpha$ and loss function $\mathcal{L}$,
    is expressed as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个由互连的节点（神经元）组成的系统，用于处理和传输信号。深度神经网络（DNNs）是利用多个神经元层的模型，具有不同程度的架构复杂性，用于分析复杂的数据模式。训练过程涉及调整神经元之间的权重参数，以减少网络预测与实际结果之间的误差（称为损失函数），通常采用受随机梯度下降（SGD）启发的算法。这种训练优化了网络准确识别和响应输入数据的能力。在这种背景下，给定输入
    $x$、标签 $y$、学习率 $\alpha$ 和损失函数 $\mathcal{L}$，模型 $f_{\theta}$ 参数 $\theta$ 的更新规则表示为：
- en: '|  | $\theta=\theta-\alpha\cdot\nabla_{\theta}\mathcal{L}(f_{\theta}(x),y)$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta=\theta-\alpha\cdot\nabla_{\theta}\mathcal{L}(f_{\theta}(x),y)$
    |  |'
- en: 2.2.2 Deep Reinforcement Learning (DRL)
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 深度强化学习（DRL）
- en: 'DRL combines the principles of RL with the capabilities of DNNs. The central
    concept in DRL is to construct a policy $\pi$ using a DNN. This can be achieved
    either by approximating the Q-function (as in Equation ([2](#S2.E2 "In 2.1.2 Fundamentals
    of Reinforcement Learning ‣ 2.1 Reinforcement Learning ‣ 2 Background ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey"))),
    the V-function (as in Equation ([1](#S2.E1 "In 2.1.2 Fundamentals of Reinforcement
    Learning ‣ 2.1 Reinforcement Learning ‣ 2 Background ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey"))), or by directly
    inferring the policy from experiences. There are several popular DRL algorithms,
    each with their specific strengths and weaknesses, some are better suited for
    specific context like discrete or continuous action space, or depending on possibility
    to train the DNNs on- or off-policy. The fundamental DRL algorithms are PG [[14](#bib.bib14)],
    DQN [[15](#bib.bib15)] and DDPG [[16](#bib.bib16)], but the most effective contemporary
    algorithms are Rainbow [[17](#bib.bib17)], PPO [[18](#bib.bib18)], SAC [[19](#bib.bib19)]
    or TQC [[20](#bib.bib20)] depending on the context.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: DRL将RL的原则与DNN的能力结合起来。DRL的核心概念是使用DNN构建一个策略$\pi$。这可以通过近似Q函数（如方程 ([2](#S2.E2 "在2.1.2
    强化学习基础 ‣ 2.1 强化学习 ‣ 2 背景 ‣ 通过对抗性攻击和训练实现稳健深度强化学习：调查")）），V函数（如方程 ([1](#S2.E1 "在2.1.2
    强化学习基础 ‣ 2.1 强化学习 ‣ 2 背景 ‣ 通过对抗性攻击和训练实现稳健深度强化学习：调查")）），或通过直接从经验中推断策略来实现。有几个流行的DRL算法，每个算法都有其特定的优缺点，有些更适合特定的上下文，如离散或连续动作空间，或取决于是否可以在策略内或策略外训练DNN。基本的DRL算法是PG
    [[14](#bib.bib14)]、DQN [[15](#bib.bib15)] 和DDPG [[16](#bib.bib16)]，但最有效的现代算法是Rainbow
    [[17](#bib.bib17)]、PPO [[18](#bib.bib18)]、SAC [[19](#bib.bib19)] 或TQC [[20](#bib.bib20)]，具体取决于上下文。
- en: 2.3 Security challenges in DNNs
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 DNN中的安全挑战
- en: DNNs are a powerful tool now used in numerous real-world applications. However,
    their complex and highly non-linear structure make them hard to control, raising
    growing concerns about their reliabilities. Adv ML recently emerged to exhibit
    vulnerabilities of DNNs by processing attacks on their inputs that modify outcomes.
    NIST’s National Cybersecurity Center of Excellence (NCCE) [[21](#bib.bib21)] and
    its European counterpart, ETSI Standards [[22](#bib.bib22)], provide terminologies
    and ontologies to frame the study of these adversarial methods.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DNN是现在用于众多实际应用的强大工具。然而，它们复杂且高度非线性的结构使得它们难以控制，逐渐引发了对其可靠性的担忧。对抗性机器学习最近出现，通过对其输入进行攻击以修改结果，从而展示DNN的脆弱性。NIST的国家网络安全卓越中心（NCCE）
    [[21](#bib.bib21)] 和其欧洲对口机构ETSI标准 [[22](#bib.bib22)]，提供了框架以研究这些对抗性方法的术语和本体。
- en: 2.3.1 Adversarial Machine Learning
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 对抗性机器学习
- en: Adversarial attacks are initially designed to exploit vulnerabilities in DNNs,
    threatening their privacy, availability, or integrity [[23](#bib.bib23)]. If the
    term adversarial attack suggests a malicious intention, it is to be noted that
    these methods may also be used by a model’s owner to improve its performances
    and assess its vulnerability. Indeed, adversarial ML aims to analyze the capabilities
    of potential attackers, comprehend the impact of their attacks and develop ML
    algorithms able to withstand these security threats. An adversary may act during
    the learning phase by poisoning the training data, or the inference phase modifying
    the inputs to evade decision. In this paper we consider robustness of already
    trained models, as well as leveraging adversarial examples as a defense method
    during the training phase in order to improve robustness at the inference phase,
    therefore we focuses on discussing model robustness to evasion methods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击最初是为了利用DNN中的漏洞，威胁其隐私、可用性或完整性 [[23](#bib.bib23)]。如果“对抗性攻击”这个术语暗示了恶意意图，需要注意的是，这些方法也可以被模型的拥有者用来提高其性能并评估其脆弱性。实际上，对抗性机器学习旨在分析潜在攻击者的能力，理解其攻击的影响，并开发能够抵御这些安全威胁的机器学习算法。对手可能在学习阶段通过毒化训练数据进行攻击，或在推理阶段通过修改输入来规避决策。在本文中，我们考虑已训练模型的鲁棒性，以及利用对抗性样本作为训练阶段的防御方法，以提高推理阶段的鲁棒性，因此我们重点讨论模型对规避方法的鲁棒性。
- en: 2.3.2 Adversarial Examples [[21](#bib.bib21)]
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 对抗性示例 [[21](#bib.bib21)]
- en: 'The large number of dimensions of a DNNs input space inevitably leads to blind
    spots and high sensitivity to small perturbations. In the restrained domain of
    classification, Adversarial examples are slightly altered data instances, carefully
    crafted to trick the model into misclassification while staying undetected. Computation
    techniques range from costly hand-made modifications [[24](#bib.bib24)] to perturbations
    generated by complex algorithms, yet the fundamental objective of adversarial
    example generation remains simple and can be summarized in Equation ([3](#S2.E3
    "In 2.3.2 Adversarial Examples [21] ‣ 2.3 Security challenges in DNNs ‣ 2 Background
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")): given the original instance $x$, find the closest example $x^{\prime}$
    relative to the chosen metric $||.||$ that leads a model’s function $f_{\theta}$
    to change its output.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: DNN输入空间的高维度不可避免地导致了盲点和对小扰动的高度敏感。在受限的分类领域中，对抗样本是稍微修改的数据实例，经过精心设计以欺骗模型使其错误分类，同时保持未被检测。计算技术包括从昂贵的手工修改[[24](#bib.bib24)]到由复杂算法生成的扰动，但对抗样本生成的基本目标仍然很简单，可以总结为公式
    ([3](#S2.E3 "在 2.3.2 对抗样本 [21] ‣ 2.3 DRL中的安全挑战 ‣ 2 背景 ‣ 通过对抗攻击和训练进行稳健深度强化学习：综述"))：给定原始实例
    $x$，找到相对于选择的度量 $||.||$ 的最接近示例 $x^{\prime}$，使得模型的函数 $f_{\theta}$ 改变其输出。
- en: '|  | $\min_{x^{\prime}}&#124;&#124;x-x^{\prime}&#124;&#124;\quad s.t.\quad
    f_{\theta}(x)\neq f_{\theta}(x^{\prime})$ |  | (3) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{x^{\prime}}&#124;&#124;x-x^{\prime}&#124;&#124;\quad 满足\quad f_{\theta}(x)\neq
    f_{\theta}(x^{\prime})$ |  | (3) |'
- en: A variety of perturbation methods exist for the supervised classification problem,
    depending on the adversary’s objective and the model’s constraints. A extensive
    overview of these methods can be found together with defense strategies in [[11](#bib.bib11)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 针对监督分类问题存在多种扰动方法，具体取决于对抗者的目标和模型的限制。这些方法及其防御策略的全面概述可以在[[11](#bib.bib11)]中找到。
- en: 2.4 Security challenges in DRL
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 DRL中的安全挑战
- en: DRL enables agents to learn complex behaviors by interacting with their environment.
    However, this interaction introduces unique security challenges not fully encountered
    in traditional deep learning contexts. The dynamic nature of DRL, combined with
    the necessity for long-term strategic decision-making, exposes DRL systems to
    a range of security threats that can compromise their learning process, decision-making
    integrity, and overall effectiveness. These challenges are further exacerbated
    by the adversarial landscape, where attackers can manipulate the environment or
    the agent’s perception to induce faulty learning or decision-making. Addressing
    these challenges is crucial for deploying DRL in security-sensitive applications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: DRL使得代理能够通过与环境互动来学习复杂行为。然而，这种互动引入了独特的安全挑战，这些挑战在传统深度学习环境中并未完全遇到。DRL的动态特性，加上长期战略决策的必要性，使得DRL系统面临一系列安全威胁，这些威胁可能会破坏其学习过程、决策完整性和整体有效性。这些挑战在对抗性环境中更为严重，在这种环境中，攻击者可以操纵环境或代理的感知，以引发错误的学习或决策。解决这些挑战对于在安全敏感应用中部署DRL至关重要。
- en: 2.4.1 Safe RL Control
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 安全RL控制
- en: 'Formulating the challenge of safe control in RL [[25](#bib.bib25)] merges insights
    from the realms of control theory and reinforcement learning, aiming to optimize
    a solution that balances task achievement with stringent safety standards. At
    the heart of this approach lies three critical elements: the dynamic system behavior
    encapsulated within a model of the agent, the objectives or targets of the control
    task expressed through a cost function, and a set of safety constraints that the
    solution must adhere to. The goal is to develop a policy or controller that is
    capable of producing the necessary actions to navigate the system towards its
    objectives, all while strictly complying with the predefined safety protocols.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL中制定安全控制挑战[[25](#bib.bib25)]结合了控制理论和强化学习领域的见解，旨在优化一个解决方案，该方案在实现任务的同时平衡严格的安全标准。该方法的核心包含三个关键要素：在代理模型中封装的动态系统行为、通过成本函数表达的控制任务的目标或目标，以及解决方案必须遵守的一组安全约束。目标是制定一个能够产生必要动作的策略或控制器，以将系统引导到其目标，同时严格遵守预定义的安全协议。
- en: 2.4.2 Reality Gap, Real World Perturbations and Generalization
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 现实差距、现实世界扰动和泛化
- en: The Reality Gap [[7](#bib.bib7), [8](#bib.bib8)] refers to the divergence between
    the simulated training environments of DRL agents and the complex, unpredictable
    conditions they encounter in real-world applications. This discrepancy challenges
    not only the agent’s ability to generalize across different contexts but also
    presents a profound security vulnerability. Real-world perturbations—unexpected
    changes in the environment—can lead to degraded performance or entirely erroneous
    actions by the DRL agent, particularly when these agents are confronted with scenarios
    slightly different from their training conditions. Such perturbations may arise
    naturally, from unmodeled aspects of the environment, or be adversarially crafted,
    with the intent to exploit these generalization weaknesses and induce failures.
    Addressing the Reality Gap, thereby enhancing the agents’ ability to generalize
    effectively and securing them against both natural and adversarial perturbations,
    is crucial for the safe and reliable deployment of DRL systems in environments
    demanding high levels of security and robust decision-making.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现实差距 [[7](#bib.bib7), [8](#bib.bib8)] 指的是 DRL 代理的模拟训练环境与它们在真实应用中遇到的复杂、不可预测条件之间的差异。这种差异不仅挑战了代理在不同背景下的泛化能力，还构成了严重的安全漏洞。现实世界的扰动——环境中的意外变化——可能导致
    DRL 代理性能下降或完全错误的行为，特别是当这些代理面对与其训练条件略有不同的情境时。这些扰动可能自然产生，也可能是敌对地设计的，旨在利用这些泛化弱点并引发失败。解决现实差距，从而提高代理的有效泛化能力，并保护它们免受自然和对抗性扰动，是在需要高安全性和强大决策能力的环境中安全可靠地部署
    DRL 系统的关键。
- en: 2.4.3 Robust RL Control
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3 鲁棒 RL 控制
- en: Robust RL control introduces an advanced framework for RL by incorporating elements
    of uncertainty, such as parametric variations and external disturbances, into
    the system dynamics [[26](#bib.bib26)]. This approach shifts the optimization
    focus towards minimizing the maximum possible loss, essentially preparing the
    system to handle the worst-case scenario efficiently. It does so through a min-max
    optimization strategy, where the goal is to find a control policy that minimizes
    the maximum expected cost.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒 RL 控制通过将不确定性因素（如参数变化和外部干扰）融入系统动态，推出了一个先进的 RL 框架 [[26](#bib.bib26)]。这种方法将优化重点转向最小化最大可能损失，实质上是准备系统有效处理最坏情况。它通过一种最小-最大优化策略来实现，目标是找到一个最小化最大预期成本的控制策略。
- en: '|  | $\min_{\pi}\max_{\delta\in\Delta}J(\pi,\delta)$ |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\pi}\max_{\delta\in\Delta}J(\pi,\delta)$ |  |'
- en: Where $J(\pi,\delta)$ represents the expected cost (or loss) of policy $\pi$
    when subjected to perturbations $\delta$ introduced by the adversary. The set
    $\Delta$ defines the allowable perturbations or disturbances.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: $J(\pi,\delta)$ 代表当策略 $\pi$ 遭遇对手引入的扰动 $\delta$ 时的预期成本（或损失）。集合 $\Delta$ 定义了允许的扰动或干扰。
- en: This methodology ensures that the control system remains effective and reliable
    even when faced with unpredictable changes or adverse conditions, thereby enhancing
    its robustness and resilience in uncertain environments. This framework for enhancing
    robust control in RL, can participate for generalization of the policies across
    conditions changes thus helping to bridge the reality gap and overcome real world
    perturbations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法确保了控制系统在面对不可预测的变化或不利条件时仍然保持有效和可靠，从而提高了其在不确定环境中的鲁棒性和弹性。这个增强鲁棒控制的框架可以参与政策在条件变化下的泛化，从而帮助弥合现实差距并克服真实世界的扰动。
- en: 2.4.4 Adversarial Attacks of DRL
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.4 DRL 的对抗攻击
- en: If adversarial attacks were historically developed for supervised image classification
    models, they were proven equally effective for DRL agents. [[27](#bib.bib27)]
    first established the vulnerability of DQNs to adversarial perturbations, their
    statement soon supported by further studies [[23](#bib.bib23)]. Moreover, the
    RL framework offers more adversarial possibilities than the simple adaptation
    of supervised methods. Indeed, various components of the POMDP can be found vulnerable
    (like observation or transition function) through various elements which could
    be critic entry points (observations, states, actions), while the long-term dependencies
    inherent to DRL raise complex security challenges. On the other hand, this higher
    level of adversarial latitude enables new defense strategies for improving the
    agents robustness. This survey explores how adversarial attacks in RL can be used
    to generate perturbations that result in the worst-case scenarios crucial for
    Robust RL Control.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对抗攻击最初是为了监督图像分类模型而发展起来的，它们同样被证明对 DRL 代理也有效。[[27](#bib.bib27)] 首次揭示了 DQN 对对抗扰动的脆弱性，这一说法很快得到了进一步研究的支持
    [[23](#bib.bib23)]。此外，RL 框架提供了比简单适应监督方法更多的对抗可能性。实际上，POMDP 的各种组件（如观察或过渡函数）可以通过可能成为评论入口点的各种元素（观察、状态、动作）来发现脆弱性，而
    DRL 的长期依赖性则带来了复杂的安全挑战。另一方面，这种更高的对抗余地使得新防御策略成为可能，从而提升代理的鲁棒性。本调查探讨了如何利用 RL 中的对抗攻击生成扰动，以产生对鲁棒
    RL 控制至关重要的最坏情况场景。
- en: 3 Formalization and Scope
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 正式化与范围
- en: The existence of adversarial examples poses a significant threat for DRL agents,
    particularly in applications where incorrect predictions can have serious consequences,
    such as autonomous driving or medical diagnosis. Developing robust DRL algorithms
    that can defend against adversarial attacks and bridge the reality gap is an important
    and active research area in the field.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗示例的存在对 DRL 代理构成了重大威胁，特别是在错误预测可能产生严重后果的应用中，如自动驾驶或医疗诊断。开发能够防御对抗攻击并弥合现实差距的鲁棒
    DRL 算法是该领域一个重要且活跃的研究领域。
- en: This survey aims to identify and assess how using adversarial examples during
    policy training can improve agent robustness. More specifically, we discuss the
    ability of various types of adversarial generation strategies to help anticipate
    the reality gap, which refers to a discrepancy between the training environment
    (e.g., a simulator) and the deployment one (which can include perturbations, whether
    they can be adversarially generated or not).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查旨在识别和评估在策略训练过程中使用对抗示例如何提高代理的鲁棒性。更具体地，我们讨论了各种类型的对抗生成策略在帮助预测现实差距方面的能力，现实差距指的是训练环境（例如模拟器）与部署环境（可能包括对抗生成的扰动或其他扰动）之间的差异。
- en: 3.1 The problem of Robustness in RL
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 RL 中的鲁棒性问题
- en: 'Generally speaking, we are interested in the following optimization problem:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们感兴趣的是以下优化问题：
- en: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\Omega\sim\Phi(\Omega&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\Omega}(\tau)}[R(\tau)]$
    |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\Omega\sim\Phi(\Omega&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\Omega}(\tau)}[R(\tau)]$
    |  |'
- en: where $\Phi$ corresponds to the distribution of environments to which the agent
    is likely to be confronted when deployed (whether it adversarially considers $\pi$
    or not at test time), $\pi^{\Omega}(\tau)$ is the distribution of trajectories
    using the policy $\pi$ and the dynamics from $\Omega$, and $R(\tau)$ is the cumulative
    reward collected in $\tau$. While this formulation suggests meta-reinforcement
    learning, in our setting $\Phi(\Omega|\pi)$ is unknown at train time. The training
    setup is composed of a unique MDP on which the policy can be learned, which is
    usually the case for many applications.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Phi$ 对应于在部署时代理可能遇到的环境分布（无论它是否在测试时对 $\pi$ 进行对抗考虑），$\pi^{\Omega}(\tau)$ 是使用策略
    $\pi$ 和来自 $\Omega$ 的动态的轨迹分布，$R(\tau)$ 是在 $\tau$ 中收集的累积奖励。虽然这一表述暗示了元强化学习，但在我们的设置中，$\Phi(\Omega|\pi)$
    在训练时是未知的。训练设置由一个唯一的 MDP 组成，在该 MDP 上可以学习策略，这在许多应用中通常是情况。
- en: 'Given a unique training POMDP $\Omega$, the problem of robustness we are interested
    in can be reformulated by means of an alteration distribution $\Phi(\phi|\pi)$:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个独特的训练 POMDP $\Omega$，我们感兴趣的鲁棒性问题可以通过修改分布 $\Phi(\phi|\pi)$ 来重新表述：
- en: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\phi\sim\Phi(\phi&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\phi,{\Omega}}(\tau)}[R(\tau)]$
    |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\phi\sim\Phi(\phi&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\phi,{\Omega}}(\tau)}[R(\tau)]$
    |  |'
- en: 'where $\pi^{\phi,\Omega}$ is the distribution of trajectories using policy
    $\pi$ on $\phi(\Omega)$, standing as the MDP $\Omega$ altered by $\phi$. Generally
    speaking, we can set $\phi$ as a function that can alter any component of $\Omega$
    as $\phi(\Omega)=(\phi_{S}(S^{\Omega}),\phi_{A}(A^{\Omega}),\phi_{T}(T^{\Omega}),\phi_{R}(R^{\Omega}),\phi_{X}(X^{\Omega}),\phi_{O}(O^{\Omega}))$.
    As discussed below and also in [[28](#bib.bib28)], while $\phi$ can simultaneously
    affect any of these components, we particularly focus on two crucial components
    for robustness:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\pi^{\phi,\Omega}$是使用策略$\pi$在$\phi(\Omega)$上的轨迹分布，代表着被$\phi$改变的MDP $\Omega$。一般来说，我们可以设定$\phi$为一个可以改变$\Omega$的任何组件的函数，如$\phi(\Omega)=(\phi_{S}(S^{\Omega}),\phi_{A}(A^{\Omega}),\phi_{T}(T^{\Omega}),\phi_{R}(R^{\Omega}),\phi_{X}(X^{\Omega}),\phi_{O}(O^{\Omega}))$。在下文中并且也在[[28](#bib.bib28)]中讨论过，虽然$\phi$可以同时影响任何这些组件，我们特别关注两个关键的鲁棒性组件：
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Observation alterations: $\phi_{O}$ denotes alterations of the observation
    function of $\Omega$. In the corresponding altered environment $\widetilde{\Omega}=(S^{\Omega},A^{\Omega},T^{\Omega},R^{\Omega},X^{\Omega},\phi_{O}(O^{\Omega}))$,
    the observation obtained from a state $s\in S^{\Omega}$ could differ from that
    in $\Omega$. This can result from an adversarial attacker, that perturb signals
    from sensors to induce failures, observation abilities from real world that might
    be different than in simulation, or even unexpected failures of some sensors.
    These perturbations only induce perception alterations for $\pi$, without any
    effect on the true internal state of the system in the environment. Occurring
    at a specific step $t$ of a trajectory $\tau$, such alteration thus only impacts
    the future of $\tau$ if it induces changes in the policy decision at $t$.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察值改变：$\phi_{O}$表示$\Omega$的观察函数的改变。在相应的改变环境$\widetilde{\Omega}=(S^{\Omega},A^{\Omega},T^{\Omega},R^{\Omega},X^{\Omega},\phi_{O}(O^{\Omega}))$中，从状态$s\in
    S^{\Omega}$获得的观察能与在$\Omega$中不同。这可能是由于敌对攻击者扰乱传感器信号以诱发故障，真实世界的观察能力可能与模拟中的不同，或者甚至是一些传感器的意外故障。这些扰动只会影响$\pi$的感知，对环境中真实的内部状态没有任何影响。在轨迹$\tau$的特定步骤$t$发生这样的改变，只会影响$\tau$的未来，如果它在$t$处对策略决策产生改变。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dynamics alterations: $\phi_{T}$ denotes alterations of the transition function
    of $\Omega$. In the corresponding altered environment $\widetilde{\Omega}=(S^{\Omega},A^{\Omega},\phi_{T}(T^{\Omega}),R^{\Omega},X^{\Omega},O^{\Omega})$,
    dynamics are modified, such that actions have not the exactly same effect as in
    $\Omega$. This can result from an adversarial attacker, that modifies components
    of the environment to induce failures, from real world physics, that might be
    different than those from the training simulator, or from external events, that
    can incur unexpected situations. Dynamics alterations act on trajectories by modifying
    the resulting state $s_{t+1}$ emitted by the transition function $T$ at any step
    $t$. Even when localized at a single specific step $t$ of a trajectory, they thus
    impact its whole future.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动力学改变：$\phi_{T}$表示$\Omega$的转移函数的改变。在相应的改变环境$\widetilde{\Omega}=(S^{\Omega},A^{\Omega},\phi_{T}(T^{\Omega}),R^{\Omega},X^{\Omega},O^{\Omega})$中，动力学被修改，使得行动的效果不同于在$\Omega$中的完全相同。这可能是由于敌对攻击者修改环境的组件以诱发故障，来自真实世界物理的情况可能与训练模拟器中的不同，或者来自外部事件，可能引发意外情况。动力学改变通过修改转移函数$T$在任何步骤$t$产生的结果状态$s_{t+1}$来影响轨迹。即使是在轨迹的单个特定步骤$t$上定位，它们也会影响其整个未来。
- en: In this work, we do not explicitly address variation of other components ($S$,
    $A$, $R$ and $X$), as they usually pertain to different problem areas. $\phi_{S}$
    (resp. $\phi_{A}$) denotes alterations of the state (resp. action) set, where
    states (resp. actions) can be removed or introduced in $S^{\Omega}$ (resp. $A^{\Omega}$).
    $\phi_{X}$ denotes alterations of the observation support $X^{\Omega}$. While
    some perturbations of dynamics $\phi_{T}$ or observations $\phi_{O}$ can lead
    the agent to reach new states or observations never considered during training
    (which corresponds to implicit $\phi_{S}$ or $\phi_{X}$ perturbations), $\phi_{S}$,
    $\phi_{A}$, and $\phi_{X}$ all correspond to support shifts, related to static
    Out-Of-Domain issues, which we do not specifically focus on in this work. $\phi_{R}$
    denotes alterations of the reward function, which does not pose problem of robustness
    in usage, since the reward function is only used during training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们并未明确讨论其他组件（$S$、$A$、$R$ 和 $X$）的变化，因为这些通常涉及不同的问题领域。$\phi_{S}$（或 $\phi_{A}$）表示状态（或行动）集合的改变，其中状态（或行动）可以在
    $S^{\Omega}$（或 $A^{\Omega}$）中被移除或引入。$\phi_{X}$ 表示对观察支持 $X^{\Omega}$ 的改变。虽然某些动态
    $\phi_{T}$ 或观察 $\phi_{O}$ 的扰动可能导致智能体达到训练中未曾考虑的新状态或观察（这对应于隐式的 $\phi_{S}$ 或 $\phi_{X}$
    扰动），但 $\phi_{S}$、$\phi_{A}$ 和 $\phi_{X}$ 都对应于支持的转变，涉及静态的领域外问题，而这些问题在本工作中我们并未特别关注。$\phi_{R}$
    表示奖励函数的改变，这在使用中不会导致稳健性问题，因为奖励函数仅在训练过程中使用。
- en: 3.2 Adversarial Attacks for Robust RL
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 针对鲁棒强化学习的对抗攻击
- en: 'Following distributionally robust optimization (DRO) principles [[29](#bib.bib29)],
    unknown distribution shifts can be anticipated by considering worst-case settings
    in some uncertainty sets ${\cal R}$. In our robust RL setting, this comes down
    to the following max-min optimization problem:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据分布鲁棒优化（DRO）原则[[29](#bib.bib29)]，可以通过考虑一些不确定性集合 ${\cal R}$ 中的最坏情况来预期未知的分布变化。在我们的鲁棒强化学习设置中，这归结为以下的最大最小优化问题：
- en: '|  | $\pi^{*}=\arg\max_{\pi}\min_{\tilde{\Phi}\in{\cal R}}\mathbb{E}_{\phi\sim\tilde{\Phi}(\phi&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\phi,{\Omega}}(\tau)}[R(\tau)]$
    |  | (4) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=\arg\max_{\pi}\min_{\tilde{\Phi}\in{\cal R}}\mathbb{E}_{\phi\sim\tilde{\Phi}(\phi&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\phi,{\Omega}}(\tau)}[R(\tau)]$
    |  | (4) |'
- en: where ${\cal R}$ is a set of perturbation distributions. As well-known in DRO
    literature for supervised regression problems, the shape of ${\cal R}$ has a strong
    impact on the corresponding optimal decision system. In our RL setting, increasing
    the level of disparities allowed by the set ${\cal R}$ constrains the resulting
    policy $\pi$ to have to perform simultaneously over a broader spectrum of environmental
    conditions. While this enables better generalization for environmental shifts,
    it also implies to deal with various highly unrealistic scenarios if the set $\cal
    R$ is not restricted on reasonable levels of perturbations. With extremely large
    sets ${\cal R}$, the policy $\pi$ is expected to be equally effective for any
    possible environment, eventually converging to a trivial uniform policy, that
    allocates equal probability to every action for any state from $S^{\Omega}$. The
    shape of $\cal R$ has thus to be controlled to find an accurate trade-off between
    generalization and effectiveness. This is done in the following by setting restricted
    supports of perturbation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，${\cal R}$ 是一个扰动分布的集合。如同在监督回归问题的DRO文献中所知，${\cal R}$ 的形状对相应的最优决策系统有着强烈的影响。在我们的强化学习设置中，增加集合
    ${\cal R}$ 允许的差异水平，迫使得到的策略 $\pi$ 在更广泛的环境条件下同时进行表现。虽然这有助于对环境变化进行更好的泛化，但也意味着如果集合
    $\cal R$ 的扰动水平不受合理限制，则需要处理各种高度不现实的场景。对于极其大的集合 ${\cal R}$，策略 $\pi$ 预计对任何可能的环境都同样有效，最终收敛到一个平凡的均匀策略，该策略为
    $S^{\Omega}$ 中的任何状态分配相等的行动概率。因此，$\cal R$ 的形状需要得到控制，以找到泛化与有效性之间的准确权衡。这是通过设置受限的扰动支持来完成的。
- en: 'In our setting, dealing with worst-case distributions of perturbations defined
    over full supports of $\Omega$ is highly intractable in most realistic applications.
    In this survey, we rather focus on adversarial training that leverage the simultaneous
    optimization of an attacker agent $\xi$, that produces perturbations for situations
    reached by the protagonist $\pi$, by acting on adversarial actions $A_{\xi}^{\Omega}$
    that the environment $\Omega$ permits:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设定中，处理定义在 $\Omega$ 完全支持上的最坏情况扰动在大多数实际应用中是高度不可解的。在这项调查中，我们更关注对抗性训练，该训练利用攻击者智能体
    $\xi$ 的同步优化，该智能体对主角 $\pi$ 达到的情境产生扰动，通过对环境 $\Omega$ 允许的对抗性动作 $A_{\xi}^{\Omega}$
    进行操作。
- en: '|  | $\displaystyle\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\tau\sim\pi^{\xi^{*},\Omega}(\tau)}[R(\tau)]$
    |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\tau\sim\pi^{\xi^{*},\Omega}(\tau)}[R(\tau)]$
    |  |'
- en: '|  | $\displaystyle s.t.\qquad\xi^{*}=\arg\min_{\xi}\Delta^{\pi,\Omega}(\xi)$
    |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s.t.\qquad\xi^{*}=\arg\min_{\xi}\Delta^{\pi,\Omega}(\xi)$
    |  |'
- en: 'where $\Delta^{\pi,\Omega}(\xi)$ stands as the optimization objective of the
    adversarial agent given $\pi$ and the training environment $\Omega$, which ranges
    from adverse reward functions to divergence metrics (c.f., section [4.3](#S4.SS3
    "4.3 Adversarial Objective ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey")),
    and $\pi^{\xi,\Omega}(\tau)$ corresponds to the probability of a trajectory following
    policy $\pi$ in a POMDP dynamically modified by an adversarial agent $\xi$, given
    a set of actions $A_{\xi}^{\Omega}=(A_{\xi,X}^{\Omega},A_{\xi,A}^{\Omega},A_{\xi,S}^{\Omega},A_{\xi,T}^{\Omega},A_{\xi,S+}^{\Omega})$.
    The action $a^{\xi}_{t}=(a^{\xi,X}_{t},a^{\xi,A}_{t},a^{\xi,S}_{t},a^{\xi,T}_{t},a^{\xi,S+}_{t})$
    of adversary $\xi$ can target any element of any transition $\tau_{t}=(s_{t},x_{t},a_{t},s_{t+1})$
    of trajectories in $\Omega$. While any perturbation of $x_{t}$ induce an alteration
    of the observation function $O^{\Omega}$, any perturbation of $s_{t}$, $a_{t}$
    or $s_{t+1}$ induce an alteration of the transition function $T^{\Omega}$ (either
    directly, through its internal dynamics, or indirectly via the modification of
    its inputs or outputs).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\Delta^{\pi,\Omega}(\xi)$ 代表对抗性智能体在给定策略 $\pi$ 和训练环境 $\Omega$ 下的优化目标，其范围从不利的奖励函数到发散度度量（参见[4.3](#S4.SS3
    "4.3 Adversarial Objective ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey")），而
    $\pi^{\xi,\Omega}(\tau)$ 对应于在对抗性智能体 $\xi$ 动态修改的部分可观测马尔可夫决策过程（POMDP）中，策略 $\pi$
    下轨迹的概率，给定一组动作 $A_{\xi}^{\Omega}=(A_{\xi,X}^{\Omega},A_{\xi,A}^{\Omega},A_{\xi,S}^{\Omega},A_{\xi,T}^{\Omega},A_{\xi,S+}^{\Omega})$。对抗者
    $\xi$ 的动作 $a^{\xi}_{t}=(a^{\xi,X}_{t},a^{\xi,A}_{t},a^{\xi,S}_{t},a^{\xi,T}_{t},a^{\xi,S+}_{t})$
    可以针对 $\Omega$ 中轨迹的任何转移 $\tau_{t}=(s_{t},x_{t},a_{t},s_{t+1})$ 的任何元素。虽然对 $x_{t}$
    的任何扰动会引起观测函数 $O^{\Omega}$ 的变化，但对 $s_{t}$、$a_{t}$ 或 $s_{t+1}$ 的任何扰动都会引起转移函数 $T^{\Omega}$
    的变化（无论是直接的，通过其内部动态，还是间接的，通过修改其输入或输出）。'
- en: 'In this setting, any trajectory is composed as a sequence of adversary-augmented
    transitions $\widetilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$,
    where the elements $x^{\prime}_{t}$ (resp. $a^{\prime}_{t}$) stands for the perturbed
    observation (resp. action) produced by the application of the adversary action
    $a^{\xi,X}_{t}$ (resp. $a^{\xi,A}_{t}$) at step $t$. $\widetilde{s}_{t}$ (resp.
    $\widetilde{s}_{t+1}$) stands for the intermediary state produced by the application
    of the adversary action $a^{\xi,S}_{t}$ (resp. $a^{\xi,T}_{t}$) at step $t$ before
    (resp. during) the transition function, and $\widetilde{x}_{t}$ (resp. $\widetilde{x}_{t+1}$)
    is the observation of this state. Finally $s_{t+1}$ stands for the final next
    state produced by the application of the adversary action $a^{\xi,S+}_{t}$ after
    the transition function, its observation is $x_{t+1}$. The support and scope of
    adversarial actions define the level of perturbations allowed in the corresponding
    uncertainty set ${\cal R}$ from ([4](#S3.E4 "In 3.2 Adversarial Attacks for Robust
    RL ‣ 3 Formalization and Scope ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey")), with impacts on the generalization/accuracy
    trade-off of the resulting policy $\pi$. While the protagonist agent $\pi$ acts
    from $x_{t}$ with $a_{t}\sim\pi(\cdot|x_{t})$, in the following, we consider the
    general case of adversaries $\xi$ that act from $s_{t}$, $x_{t}$ and $a_{t}$,
    that is $\xi:S^{\Omega}\times X^{\Omega}\times A^{\Omega}\times A_{\xi}^{\Omega}\rightarrow[0;1]$
    where $a^{\xi}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$. By doing this we consider
    adversaries $\xi$ that have full knowledge of the environment, observation and
    action, while this could be easily limited to adversarial policies $\xi$ that
    act only from partial information.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，任何轨迹都被组成一个对抗性增强的转移序列 $\widetilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$，其中元素
    $x^{\prime}_{t}$（即 $a^{\prime}_{t}$）代表对抗动作 $a^{\xi,X}_{t}$（即 $a^{\xi,A}_{t}$）在步骤
    $t$ 上应用后产生的扰动观察（即行动）。$\widetilde{s}_{t}$（即 $\widetilde{s}_{t+1}$）代表在步骤 $t$ 应用对抗动作
    $a^{\xi,S}_{t}$（即 $a^{\xi,T}_{t}$）前（即在过渡函数中）产生的中间状态，而 $\widetilde{x}_{t}$（即 $\widetilde{x}_{t+1}$）是该状态的观察。最后，$s_{t+1}$
    代表在过渡函数应用后的最终下一个状态，其观察为 $x_{t+1}$。对抗性动作的支持和范围定义了在相应不确定性集合 ${\cal R}$ 中允许的扰动水平，从
    ([4](#S3.E4 "在 3.2 对抗性攻击的正式化和范围 ‣ 3 形式化和范围 ‣ 通过对抗性攻击和训练实现的鲁棒深度强化学习：综述"))，对结果策略
    $\pi$ 的泛化/准确性权衡产生影响。当主角代理 $\pi$ 从 $x_{t}$ 行动，并且 $a_{t}\sim\pi(\cdot|x_{t})$ 时，以下我们考虑一般情况下对抗者
    $\xi$ 从 $s_{t}$、$x_{t}$ 和 $a_{t}$ 行动，即 $\xi:S^{\Omega}\times X^{\Omega}\times
    A^{\Omega}\times A_{\xi}^{\Omega}\rightarrow[0;1]$ 其中 $a^{\xi}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$。通过这样做，我们考虑对抗者
    $\xi$ 具有环境、观察和行动的完整知识，尽管这可以很容易地限制为仅从部分信息中行动的对抗性策略 $\xi$。
- en: 4 Taxonomy of Adversarial Attacks of DRL
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度强化学习的对抗性攻击分类
- en: 'We conduct a systematic analysis of adversarial attacks for RL agents, with
    a focus on their purposes and applications. To better grasp the variety of methods
    available and their specificities, we propose a taxonomy of adversarial attacks
    for DRL. This taxonomy is used to categorize the adversarial attack as previously
    shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") and later described
    in Table [1](#S5.T1 "Table 1 ‣ Classical Adversarial Policy Learning Based Methods
    ‣ 5.2.3 Other Objective ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").
    This section discusses the different components of adversarial approaches for
    robust RL, before developing main approaches in the next section.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 RL 代理的对抗性攻击进行了系统分析，重点关注其目的和应用。为了更好地理解可用的方法及其特性，我们提出了 DRL 的对抗性攻击分类。这一分类用于对之前在图
    [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 通过对抗性攻击和训练实现的鲁棒深度强化学习：综述") 和后面描述于表 [1](#S5.T1 "表 1 ‣
    经典对抗性策略学习方法 ‣ 5.2.3 其他目标 ‣ 5.2 动态变化 ‣ 5 对抗性攻击 ‣ 通过对抗性攻击和训练实现的鲁棒深度强化学习：综述") 中展示的对抗性攻击进行分类。本节讨论了鲁棒
    RL 的对抗性方法的不同组成部分，然后在下一节中展开主要方法。
- en: 4.1 Perturbed Element
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 受扰动的元素
- en: 'An adversarial attacks is a method that use an adversarial action $a^{\xi}_{t}\in
    A_{\xi}^{\Omega}$ emitted by the adversary agent $\xi$ at step $t$, to produce
    a perturbation in the simulation during the trajectory of an agent. Given the
    type of attack, an action $a^{\xi}_{t}$ can directly perturb different elements
    :'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击是一种方法，使用对抗性行动 $a^{\xi}_{t}\in A_{\xi}^{\Omega}$ 由对抗代理 $\xi$ 在步骤 $t$ 发出，以在代理的轨迹中产生模拟扰动。根据攻击类型，行动
    $a^{\xi}_{t}$ 可以直接扰动不同的元素：
- en: The observations $x_{t}$
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 观测 $x_{t}$
- en: Via a perturbation function $O^{\xi}:X^{\Omega}\times X^{\Omega}\times A_{\xi,X}^{\Omega}\rightarrow[0;1]$,
    where $x^{\prime}_{t}\sim O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个扰动函数 $O^{\xi}:X^{\Omega}\times X^{\Omega}\times A_{\xi,X}^{\Omega}\rightarrow[0;1]$，其中
    $x^{\prime}_{t}\sim O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$。
- en: The actions $a_{t}$
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 行动 $a_{t}$
- en: Via a perturbation function $A^{\xi}:A^{\Omega}\times A^{\Omega}\times A_{\xi,A}^{\Omega}\rightarrow[0;1]$
    where $a^{\prime}_{t}\sim A^{\xi}(\cdot|a_{t},a^{\xi,A}_{t})$.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个扰动函数 $A^{\xi}:A^{\Omega}\times A^{\Omega}\times A_{\xi,A}^{\Omega}\rightarrow[0;1]$，其中
    $a^{\prime}_{t}\sim A^{\xi}(\cdot|a_{t},a^{\xi,A}_{t})$。
- en: The current state $s_{t}$ (before transition)
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 当前状态 $s_{t}$（过渡前）
- en: Via an additional transition functions $T_{\xi-}^{\Omega}:S^{\Omega}\times S^{\Omega}\times
    A_{\xi,S}^{\Omega}\rightarrow[0;1]$ where $\widetilde{s}_{t}\sim T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$
    is applied after the decision $a_{t}$ of the agent is taken and before the main
    transition function of the environment is applied.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个额外的过渡函数 $T_{\xi-}^{\Omega}:S^{\Omega}\times S^{\Omega}\times A_{\xi,S}^{\Omega}\rightarrow[0;1]$，其中
    $\widetilde{s}_{t}\sim T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$ 在代理的决策 $a_{t}$
    做出之后以及环境的主要过渡函数应用之前被应用。
- en: The transition function $T^{\Omega}$
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 过渡函数 $T^{\Omega}$
- en: Via an adversarially augmented transition functions $T_{\xi}^{\Omega}:S^{\Omega}\times
    S^{\Omega}\times A^{\Omega}\times A_{\xi,T}^{\Omega}\rightarrow[0;1]$ where $\widetilde{s}_{t+1}\sim
    T_{\xi}^{\Omega}(\cdot|s_{t},a_{t},a^{\xi,T}_{t})$ is applied as substitute of
    the main transition function of the environment $T^{\Omega}$.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个对抗性增强的过渡函数 $T_{\xi}^{\Omega}:S^{\Omega}\times S^{\Omega}\times A^{\Omega}\times
    A_{\xi,T}^{\Omega}\rightarrow[0;1]$，其中 $\widetilde{s}_{t+1}\sim T_{\xi}^{\Omega}(\cdot|s_{t},a_{t},a^{\xi,T}_{t})$
    被用作环境的主要过渡函数 $T^{\Omega}$ 的替代。
- en: The next state $s_{t+1}$ (after transition)
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 下一个状态 $s_{t+1}$（过渡后）
- en: Via an additional transition functions $T_{\xi+}^{\Omega}:S^{\Omega}\times S^{\Omega}\times
    A_{\xi,S+}^{\Omega}\rightarrow[0;1]$ where $s_{t+1}\sim T_{\xi+}^{\Omega}(\cdot|s_{t+1},a^{\xi,S+}_{t})$
    is applied after the main transition function of the environment and before the
    next decision $a_{t+1}$ of the agent is taken.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个额外的过渡函数 $T_{\xi+}^{\Omega}:S^{\Omega}\times S^{\Omega}\times A_{\xi,S+}^{\Omega}\rightarrow[0;1]$，其中
    $s_{t+1}\sim T_{\xi+}^{\Omega}(\cdot|s_{t+1},a^{\xi,S+}_{t})$ 在环境的主要过渡函数之后以及代理的下一个决策
    $a_{t+1}$ 做出之前被应用。
- en: The perturbations on the two first types of elements (observation and action)
    require just to modify a vector which will be fed as input of another function,
    so they are easy to implement in any environment. The perturbations on the three
    last types of elements (state, transition function and next state) are more complex
    and require to modify the environment itself, either by being able to modify the
    state with an additional transition function, or being able to modify the main
    transition function itself by incorporating the effect of the adversary action.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对前两类元素（观测和行动）的扰动只需修改一个向量，该向量将作为另一个函数的输入，因此在任何环境中实现起来都很容易。对后三类元素（状态、过渡函数和下一个状态）的扰动则更为复杂，需要修改环境本身，要么通过能够用额外的过渡函数修改状态，要么通过将对抗行动的效果纳入主要过渡函数本身。
- en: Here and in the following, we use the term perturb to denote direct modification
    of an element by an adversarial action $a^{\xi}_{t}$. For example, direct perturbation
    of an observation $x_{t}$, perturbation of a state $s_{t}$, perturbations of an
    action $a_{t}$. We do not use the term perturbations for indirect modifications,
    for example by directly perturbing an observation $x_{t}$, the action $a_{t}$
    chosen by the agent could be modifies, this new action cannot be seen as a perturbed
    action but results from the application of the policy $\pi$ of the agent given
    a perturbed observation $x^{\prime}_{t}$.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里及之后，我们使用术语“扰动”来表示通过对抗性行动 $a^{\xi}_{t}$ 直接修改某个元素。例如，直接扰动观测 $x_{t}$，扰动状态 $s_{t}$，扰动行动
    $a_{t}$。我们不使用“扰动”一词来指代间接修改，例如，通过直接扰动观测 $x_{t}$，代理选择的行动 $a_{t}$ 可能会被修改，这种新行动不能被视为扰动的行动，而是由代理的策略
    $\pi$ 应用在扰动观测 $x^{\prime}_{t}$ 上所产生的结果。
- en: 'In the following we use the term disturb to denote any perturbation of one
    of the following elements : action, state, transition function or next state.
    More generally the term disturb is used to denote perturbations that modifies
    the dynamics of the environment.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下文中，我们使用“扰动”一词来表示以下元素之一的任何扰动：动作、状态、转移函数或下一个状态。更一般地，"扰动"一词用于表示那些改变环境动态的扰动。
- en: 4.2 Altered POMDP Component
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 改变的 POMDP 组件
- en: Following the two main types of alterations $\phi$ that are discussed in previous
    section, the main axis of the taxonomy of approaches concerns the impact on the
    POMDP of actions that are emitted by adversary agents during training of $\pi$.
    Given adversarial elements defined in the previous section, we specify each possible
    perturbation independently to discuss each specific adversarial impact on the
    POMDP.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前一节讨论的两种主要类型的扰动 $\phi$，方法分类的主要方向涉及对 POMDP 的影响，这些影响来自对手代理在训练 $\pi$ 过程中发出的动作。根据前一节定义的对抗元素，我们独立指定每种可能的扰动，以讨论每种具体的对抗影响对
    POMDP 的影响。
- en: Here and in the following, we use the term alter to denote the modification
    of a component of the POMDP from the POV of the agent. For example, adding an
    adversarial attack that perturb the observations is an alteration of the observation
    function $O^{\Omega}$ of the POMDP from the POV of the agent. Alternatively, adding
    an adversarial attack that perturb the action, the state, the transition function
    or the next state, is an alteration of the transition function $T^{\Omega}$ of
    the POMDP (dynamics of the environment) from the POV of the agent.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里以及以下内容中，我们使用“改变”一词来表示从代理的角度修改 POMDP 的一个组件。例如，添加一个扰动观察的对抗攻击，是从代理的角度改变 POMDP
    的观察函数 $O^{\Omega}$。或者，添加一个扰动动作、状态、转移函数或下一个状态的对抗攻击，是从代理的角度改变 POMDP 的转移函数 $T^{\Omega}$（环境动态）。
- en: 4.2.1 Alteration of the Observations Function $O^{\Omega}$
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 观察函数 $O^{\Omega}$ 的改变
- en: The first type of component alteration is the alteration of the observation
    function $O^{\Omega}$ of the POMDP. Directly inspired from adversarial attacks
    in supervised machine learning, many methods are designed to modify the inputs
    that are perceived by the protagonist agent $\pi$. The principle is to modify
    the input vector of an agent, which can correspond for instance to the outputs
    of a sensor of a physical agent, like an autonomous vehicle. The observation is
    perturbed before the agent take any decision, so the agent get the perturbed observation
    and can be fooled.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种组件改变类型是 POMDP 的观察函数 $O^{\Omega}$ 的改变。直接受到监督机器学习中对抗攻击的启发，许多方法旨在修改主角代理 $\pi$
    感知到的输入。原理是修改代理的输入向量，这可以例如对应于物理代理的传感器输出，如自动驾驶车辆。观察在代理做出任何决策之前被扰动，因此代理接收到的是扰动后的观察，可能被欺骗。
- en: 'More formally, in the setting of an observation attack, the adversary $\xi$
    acts to produce a perturbed observation $x^{\prime}_{t}$ before it is fed as input
    to $\pi$, by perturbing the observation $x_{t}$ via the specific perturbation
    function $O^{\xi}(x^{\prime}_{t}|x_{t},a^{\xi,X}_{t})$ introduced in Section [4.1](#S4.SS1
    "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '更正式地，在观察攻击的设置中，对手 $\xi$ 通过特定的扰动函数 $O^{\xi}(x^{\prime}_{t}|x_{t},a^{\xi,X}_{t})$
    对观察 $x_{t}$ 进行扰动，生成扰动后的观察 $x^{\prime}_{t}$，然后将其作为输入提供给 $\pi$，这一过程在第[4.1](#S4.SS1
    "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey")节中介绍。'
- en: 'In that case, $\xi$ can be regarded as an adversary agent that acts by emitting
    adversarial actions $a^{\xi,X}_{t}\sim\xi(\cdot|s_{t},x_{t})$ with $a^{\xi,X}_{t}\in
    A^{\Omega}_{\xi,X}$, given $\pi$ in a POMDP defined as $\Omega^{\pi}=(S^{\Omega},A_{\xi,X}^{\Omega},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$,
    where sampling $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,X}_{t})$ is performed
    in four steps, as shown in Algorithm [1](#alg1 "Algorithm 1 ‣ 4.2.1 Alteration
    of the Observations Function 𝑂^Ω ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of
    Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，$\xi$ 可以被视为一个对手代理，它通过发出对抗性动作 $a^{\xi,X}_{t}\sim\xi(\cdot|s_{t},x_{t})$
    并且 $a^{\xi,X}_{t}\in A^{\Omega}_{\xi,X}$，给定一个 POMDP 定义为 $\Omega^{\pi}=(S^{\Omega},A_{\xi,X}^{\Omega},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$，其中采样
    $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,X}_{t})$ 是在四个步骤中执行的，如算法 [1](#alg1
    "Algorithm 1 ‣ 4.2.1 Alteration of the Observations Function 𝑂^Ω ‣ 4.2 Altered
    POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") 所示。'
- en: Algorithm 1 $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,X}_{t})$
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,X}_{t})$
- en: 1:Input $s_{t},a^{\xi,X}_{t}$ $\triangleright$ state and adversary action2:sample
    $x_{t}\sim O^{\Omega}(\cdot|s_{t})$ $\triangleright$ observation3:sample $x^{\prime}_{t}\sim
    O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$ $\triangleright$ perturbed observation4:sample
    $a_{t}\sim\pi(\cdot|x^{\prime}_{t})$ $\triangleright$ agent action5:sample $s_{t+1}\sim
    T^{\Omega}(\cdot|s_{t},a_{t})$ $\triangleright$ next state after transition6:return
    $s_{t+1}$
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入 $s_{t},a^{\xi,X}_{t}$ $\triangleright$ 状态和对抗性动作 2: 采样 $x_{t}\sim O^{\Omega}(\cdot|s_{t})$
    $\triangleright$ 观察 3: 采样 $x^{\prime}_{t}\sim O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$
    $\triangleright$ 扰动后的观察 4: 采样 $a_{t}\sim\pi(\cdot|x^{\prime}_{t})$ $\triangleright$
    代理动作 5: 采样 $s_{t+1}\sim T^{\Omega}(\cdot|s_{t},a_{t})$ $\triangleright$ 转移后的下一个状态
    6: 返回 $s_{t+1}$'
- en: 'Reversely, agent $\pi$ acts on an altered POMDP $\Omega^{\xi}=(S^{\Omega},A^{\Omega},T^{\Omega},R^{\Omega},X^{\Omega},O^{\xi,\Omega})$
    where the input observation $x^{\prime}_{t}\sim O^{\xi,\Omega}(\cdot|s_{t})$ is
    performed in three steps, as shown in Algorithm [2](#alg2 "Algorithm 2 ‣ 4.2.1
    Alteration of the Observations Function 𝑂^Ω ‣ 4.2 Altered POMDP Component ‣ 4
    Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey").'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '相反，代理 $\pi$ 在一个修改后的 POMDP $\Omega^{\xi}=(S^{\Omega},A^{\Omega},T^{\Omega},R^{\Omega},X^{\Omega},O^{\xi,\Omega})$
    上行动，其中输入观察 $x^{\prime}_{t}\sim O^{\xi,\Omega}(\cdot|s_{t})$ 是在三个步骤中执行的，如算法 [2](#alg2
    "Algorithm 2 ‣ 4.2.1 Alteration of the Observations Function 𝑂^Ω ‣ 4.2 Altered
    POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") 所示。'
- en: Algorithm 2 $x^{\prime}_{t}\sim O^{\xi,\Omega}(\cdot|s_{t})$
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 $x^{\prime}_{t}\sim O^{\xi,\Omega}(\cdot|s_{t})$
- en: 1:Input $s_{t}$ $\triangleright$ state2:sample $x_{t}\sim O^{\Omega}(\cdot|s_{t})$
    $\triangleright$ observation3:sample $a^{\xi,X}_{t}\sim\xi(\cdot|s_{t},x_{t})$
    $\triangleright$ adversary action4:sample $x^{\prime}_{t}\sim O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$
    $\triangleright$ perturbed observation5:return $x^{\prime}_{t}$
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入 $s_{t}$ $\triangleright$ 状态 2: 采样 $x_{t}\sim O^{\Omega}(\cdot|s_{t})$
    $\triangleright$ 观察 3: 采样 $a^{\xi,X}_{t}\sim\xi(\cdot|s_{t},x_{t})$ $\triangleright$
    对抗性动作 4: 采样 $x^{\prime}_{t}\sim O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$ $\triangleright$
    扰动后的观察 5: 返回 $x^{\prime}_{t}$'
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4.2.1 Alteration of the Observations Function
    𝑂^Ω ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣
    Robust Deep Reinforcement Learning Through Adversarial Attacks and Training :
    A Survey") presents a flowchart illustrating how the observation perturbation
    integrates into the POMDP.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S4.F3 "Figure 3 ‣ 4.2.1 Alteration of the Observations Function 𝑂^Ω
    ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey")
    展示了一个流程图，说明了观察扰动如何融入 POMDP。'
- en: '![Refer to caption](img/c0399df22e568e31d7598a77da8d98ef.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c0399df22e568e31d7598a77da8d98ef.png)'
- en: 'Figure 3: Flowchart of the perturbation of the observation'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 观察扰动的流程图'
- en: 'Following this, the probability of an adversary-augmented transition $\tilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$
    given past $\tilde{\tau}_{0:t-1}$, is given by:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，给定过去的 $\tilde{\tau}_{0:t-1}$，对抗性增强转移 $\tilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$
    的概率为：
- en: '|  | $\pi^{\xi,\Omega}(\tau_{t}&#124;s_{t})=O^{\Omega}(x_{t}&#124;s_{t})\xi(a^{\xi,X}_{t}&#124;s_{t},x_{t})O^{\xi}(x^{\prime}_{t}&#124;x_{t},a^{\xi,X}_{t})\pi(a_{t}&#124;x^{\prime}_{t})\delta_{a_{t}}(a^{\prime}_{t})\\
    \delta_{s_{t}}(\widetilde{s}_{t})O^{\Omega}(\widetilde{x}_{t}&#124;\widetilde{s}_{t})T^{\Omega}(\widetilde{s}_{t+1}&#124;\widetilde{s}_{t},a^{\prime}_{t})O^{\Omega}(\widetilde{x}_{t+1}&#124;s_{t+1})\delta_{\widetilde{s}_{t+1}}(s_{t+1})$
    |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{\xi,\Omega}(\tau_{t}&#124;s_{t})=O^{\Omega}(x_{t}&#124;s_{t})\xi(a^{\xi,X}_{t}&#124;s_{t},x_{t})O^{\xi}(x^{\prime}_{t}&#124;x_{t},a^{\xi,X}_{t})\pi(a_{t}&#124;x^{\prime}_{t})\delta_{a_{t}}(a^{\prime}_{t})\\
    \delta_{s_{t}}(\widetilde{s}_{t})O^{\Omega}(\widetilde{x}_{t}&#124;\widetilde{s}_{t})T^{\Omega}(\widetilde{s}_{t+1}&#124;\widetilde{s}_{t},a^{\prime}_{t})O^{\Omega}(\widetilde{x}_{t+1}&#124;s_{t+1})\delta_{\widetilde{s}_{t+1}}(s_{t+1})$
    |  |'
- en: where $\delta_{x}$ stands for a Dirac distribution centered on $x$.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\delta_{x}$表示一个以$x$为中心的Dirac分布。
- en: 4.2.2 Alteration of the Transition Function $T^{\Omega}$ (Environment Dynamics)
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 过渡函数$T^{\Omega}$的修改（环境动态）
- en: The other type of component alteration is the alteration of the transition function
    $T^{\Omega}$ of the POMDP (altering the dynamics of the environment). The principle
    is to modify effects of actions of the protagonist in the environment. For example,
    this can include moving or modifying the behavior of some physical objects in
    the environment, like modifying the positions or speed of some vehicles in autonomous
    driving simulator, or modifying the way the protagonist’s actions are affecting
    the environment (e.g. by amplifying or reversing actions).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种组件修改类型是修改POMDP的过渡函数$T^{\Omega}$（改变环境的动态）。原则是修改主角在环境中的动作效果。例如，这可以包括移动或修改环境中一些物理对象的行为，如在自动驾驶模拟器中修改某些车辆的位置或速度，或修改主角的动作对环境的影响方式（例如，通过放大或反转动作）。
- en: 'This is done by emitting adversarial actions $A^{\Omega}_{\xi}$, that are allowed
    by the environment $\Omega$ through a specific adversary function $A^{\xi}$, $T_{\xi-}^{\Omega}$,
    $T_{\xi}^{\Omega}$ or $T_{\xi+}^{\Omega}$, creating an altered transition function
    $T^{\xi,\Omega}$ for the protagonist actions. In that setting, four types of adversaries
    can be considered:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过发出对环境$\Omega$允许的对抗性动作$A^{\Omega}_{\xi}$，使用特定的对抗性函数$A^{\xi}$、$T_{\xi-}^{\Omega}$、$T_{\xi}^{\Omega}$或$T_{\xi+}^{\Omega}$，为主角动作创建一个修改后的过渡函数$T^{\xi,\Omega}$。在这种设置中，可以考虑四种类型的对抗者：
- en: Transition Perturbation
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 过渡扰动
- en: In this setting, the process begins with the agent in an initial state. The
    agent then chooses an action, which is applied to the environment. This should
    leads to a transition to a new state, according to the environment’s transition
    function. However, this transition function is perturbed, effectively altering
    the dynamics of the environment. Resulting in a different new subsequent state
    than if the transition had not been perturbed.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，过程从代理处于初始状态开始。代理随后选择一个动作，并将其应用于环境。这应导致根据环境的过渡函数过渡到一个新状态。然而，这个过渡函数被扰动，有效地改变了环境的动态。结果是与如果过渡函数没有被扰动时所得到的不同的新状态。
- en: For instance, in the context of an autonomous vehicle, the vehicle might decide
    to change lanes (action) based on the existing traffic setup (state). As this
    action leads to a transition, the behavior of surrounding vehicles is unpredictably
    modified (perturbed transition). Consequently, the vehicle emerges in a new traffic
    configuration (next state) that is different from what would typically result
    from the chosen action.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在自动驾驶车辆的背景下，车辆可能会根据现有的交通情况（状态）决定换道（动作）。由于这个动作导致了过渡，周围车辆的行为被不可预测地修改（扰动过渡）。因此，车辆会出现在一个与所选动作通常结果不同的新交通配置（下一个状态）中。
- en: This process introduces variability into the environment’s dynamics by directly
    changing the environment’s inherent transition function.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程通过直接改变环境固有的过渡函数，引入环境动态的变异性。
- en: 'More formally, the adversary $\xi$ acts to induce an altered next state $s^{\prime}_{t+1}$
    by modifying the transition function itself, replacing it with the perturbed transition
    function $T_{\xi}^{\Omega}(s_{t+1}|(s_{t},a_{t}),a^{\xi,T}_{t})$ introduced in
    Section [4.1](#S4.SS1 "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks
    of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey"). In that case, $\xi$ can be regarded as an agent that acts by emitting
    adversarial actions $a^{\xi,T}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$, given $\pi$
    in a POMDP defined as: $\Omega^{\pi}=((S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,T},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$,
    where sampling $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,T}_{t})$
    is performed in three steps, as shown in Algorithm [3](#alg3 "Algorithm 3 ‣ Transition
    Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics)
    ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，敌对者 $\xi$ 通过修改过渡函数本身来诱导一个改变后的下一个状态 $s^{\prime}_{t+1}$，将其替换为第 [4.1](#S4.SS1
    "4.1 扰动元素 ‣ 4 强敌攻击的分类 ‣ 通过强敌攻击和训练实现稳健的深度强化学习：综述") 节中介绍的扰动过渡函数 $T_{\xi}^{\Omega}(s_{t+1}|(s_{t},a_{t}),a^{\xi,T}_{t})$。在这种情况下，$\xi$
    可以视为一个通过发出敌对动作 $a^{\xi,T}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$ 的代理，给定 POMDP 中的
    $\pi$，定义为：$\Omega^{\pi}=((S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,T},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$，其中采样
    $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,T}_{t})$ 分三步进行，如算法
    [3](#alg3 "算法 3 ‣ 过渡扰动 ‣ 4.2.2 过渡函数 𝑇^Ω (环境动态) 的改变 ‣ 4.2 改变的 POMDP 组件 ‣ 4 强敌攻击的分类
    ‣ 通过强敌攻击和训练实现稳健的深度强化学习：综述") 所示。
- en: Algorithm 3 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,T}_{t})$
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,T}_{t})$
- en: 1:Input $s_{t},a_{t},a^{\xi,T}_{t}$ $\triangleright$ state, agent action and
    adversary action2:sample $s_{t+1}\sim T_{\xi}^{\Omega}(\cdot|s_{t},a_{t},a^{\xi,T}_{t})$
    $\triangleright$ next state after perturbed transition3:sample $x_{t+1}\sim O^{\Omega}(\cdot|s_{t+1})$
    $\triangleright$ next observation4:sample $a_{t+1}\sim\pi(\cdot|x_{t+1})$ $\triangleright$
    next agent action5:return $s_{t+1}$, $a_{t+1}$
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入 $s_{t},a_{t},a^{\xi,T}_{t}$ $\triangleright$ 状态、代理动作和敌对动作2: 采样 $s_{t+1}\sim
    T_{\xi}^{\Omega}(\cdot|s_{t},a_{t},a^{\xi,T}_{t})$ $\triangleright$ 扰动后的下一个状态3:
    采样 $x_{t+1}\sim O^{\Omega}(\cdot|s_{t+1})$ $\triangleright$ 下一个观测4: 采样 $a_{t+1}\sim\pi(\cdot|x_{t+1})$
    $\triangleright$ 下一个代理动作5: 返回 $s_{t+1}$，$a_{t+1}$'
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ Transition Perturbation ‣ 4.2.2 Alteration of
    the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") presents a flowchart illustrating
    how the transition perturbation integrates into the POMDP.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S4.F4 "图 4 ‣ 过渡扰动 ‣ 4.2.2 过渡函数 𝑇^Ω (环境动态) 的改变 ‣ 4.2 改变的 POMDP 组件 ‣ 4
    强敌攻击的分类 ‣ 通过强敌攻击和训练实现稳健的深度强化学习：综述") 展示了一个流程图，说明了过渡扰动如何融入 POMDP。
- en: '![Refer to caption](img/3891f8756f6314bc9bf6551b3524f64c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3891f8756f6314bc9bf6551b3524f64c.png)'
- en: 'Figure 4: Flowchart of the perturbation of the transition function'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：过渡函数扰动的流程图
- en: Current State Perturbation
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 当前状态扰动
- en: In this setting, the process begins with the agent in an initial state. The
    agent then chooses an action to be applied within the environment. However, before
    this action is applied, the current state is subjected to a perturbation. This
    perturbation alters the initial state, leading to a modified state in which the
    chosen action is applied. The application of the action in this perturbed state
    results in a transition, resulting in a new subsequent state according to the
    environment’s transition function.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，过程从代理在初始状态开始。然后，代理选择一个在环境中应用的动作。然而，在应用该动作之前，当前状态会受到扰动。这种扰动改变了初始状态，导致在修改后的状态中应用所选动作。在这个扰动状态中应用动作会导致一个过渡，生成一个根据环境的过渡函数的新后续状态。
- en: For example, consider an autonomous vehicle deciding to change lanes (action)
    based on the prevailing traffic configuration (state). Prior to executing this
    maneuver, the traffic configuration is altered (perturbed state), such as by adjusting
    the positions of nearby vehicles. Consequently, when the vehicle executes its
    lane change, it does so in this adjusted traffic scenario, leading to a different
    traffic configuration (next state) than if the original state had not been modified.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个自主车辆根据当前交通状况（状态）决定变道（动作）。在执行此操作之前，交通状况会被改变（扰动状态），例如通过调整附近车辆的位置。因此，当车辆执行变道时，它是在调整后的交通情境中进行的，导致不同于原始状态的交通配置（下一状态）。
- en: This process introduces variability into the environment’s dynamics without
    necessitating a direct modification of the environment’s transition function.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在不直接修改环境转移函数的情况下，引入了环境动态的变异性。
- en: 'More formally, the adversary $\xi$ acts to induce an altered next state $s^{\prime}_{t+1}$
    by perturbing the state before the transition function via the prior transition
    function $T_{\xi-}^{\Omega}(\widetilde{s}_{t}|s_{t},a^{\xi,S}_{t})$ introduced
    in Section [4.1](#S4.SS1 "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks
    of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey"). In that case, $\xi$ can be regarded as an agent that acts by emitting
    adversarial actions $a^{\xi,S}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$, given $\pi$
    in a POMDP defined as: $\Omega^{\pi}=((S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,S},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$,
    where sampling $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,S}_{t})$
    is performed in four steps, as shown in Algorithm [4](#alg4 "Algorithm 4 ‣ Current
    State Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment
    Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of
    DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey").'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地，敌对者 $\xi$ 通过在转移函数之前扰动状态，诱导一个变化后的下一状态 $s^{\prime}_{t+1}$，这一过程使用了在第 [4.1](#S4.SS1
    "4.1 扰动元素 ‣ 4 强化学习对抗攻击的分类 ‣ 通过对抗攻击和训练实现的稳健深度强化学习：综述") 节中引入的先前转移函数 $T_{\xi-}^{\Omega}(\widetilde{s}_{t}|s_{t},a^{\xi,S}_{t})$。在这种情况下，$\xi$
    可以视为一个代理，按照 POMDP 的定义发出对抗动作 $a^{\xi,S}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$，其中
    $\pi$ 的 POMDP 定义为：$\Omega^{\pi}=((S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,S},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$，其中采样
    $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,S}_{t})$ 分四步进行，如算法
    [4](#alg4 "算法 4 ‣ 当前状态扰动 ‣ 4.2.2 转移函数 𝑇^Ω （环境动态）的变化 ‣ 4.2 变化的 POMDP 组件 ‣ 4 强化学习对抗攻击的分类
    ‣ 通过对抗攻击和训练实现的稳健深度强化学习：综述") 所示。
- en: Algorithm 4 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,S}_{t})$
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,S}_{t})$
- en: 1:Input $s_{t},a_{t},a^{\xi,S}_{t}$ $\triangleright$ state, agent action and
    adversary action2:sample $\widetilde{s}_{t}\sim T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$
    $\triangleright$ perturbed state3:sample $s_{t+1}\sim T^{\Omega}(\cdot|\widetilde{s}_{t},a_{t})$
    $\triangleright$ next state after transition4:sample $x_{t+1}\sim O^{\Omega}(\cdot|s_{t+1})$
    $\triangleright$ next observation5:sample $a_{t+1}\sim\pi(\cdot|x_{t+1})$ $\triangleright$
    next agent action6:return $s_{t+1}$, $a_{t+1}$
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入 $s_{t},a_{t},a^{\xi,S}_{t}$ $\triangleright$ 状态、代理动作和对抗者动作 2: 采样 $\widetilde{s}_{t}\sim
    T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$ $\triangleright$ 扰动状态 3: 采样 $s_{t+1}\sim
    T^{\Omega}(\cdot|\widetilde{s}_{t},a_{t})$ $\triangleright$ 转移后的下一状态 4: 采样 $x_{t+1}\sim
    O^{\Omega}(\cdot|s_{t+1})$ $\triangleright$ 下一观察 5: 采样 $a_{t+1}\sim\pi(\cdot|x_{t+1})$
    $\triangleright$ 下一代理动作 6: 返回 $s_{t+1}$，$a_{t+1}$'
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ Current State Perturbation ‣ 4.2.2 Alteration
    of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") presents a flowchart illustrating
    how the current state perturbation integrates into the POMDP.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S4.F5 "图 5 ‣ 当前状态扰动 ‣ 4.2.2 转移函数 𝑇^Ω （环境动态）的变化 ‣ 4.2 变化的 POMDP 组件 ‣ 4
    强化学习对抗攻击的分类 ‣ 通过对抗攻击和训练实现的稳健深度强化学习：综述") 展示了一个流程图，说明了当前状态扰动如何融入到 POMDP 中。
- en: '![Refer to caption](img/8f3a3e2a17a360fe350d608e70031d62.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f3a3e2a17a360fe350d608e70031d62.png)'
- en: 'Figure 5: Flowchart of the perturbation of the current state'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：当前状态扰动的流程图
- en: Next State Perturbation
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 下一状态扰动
- en: In this setting, the process begins with the agent in an initial state. The
    agent chooses an action which is then applied in the environment. This leads to
    transition to a new subsequent state, according to the environment’s transition
    function. However, before the agent can choose its next action, this new state
    is perturbed.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，过程从代理处于初始状态开始。代理选择一个动作，然后在环境中应用该动作。这导致根据环境的过渡函数转移到一个新的后续状态。然而，在代理选择下一个动作之前，这个新状态会被扰动。
- en: For instance, in the case of an autonomous vehicle, the vehicle might choose
    to change lanes (action) based on the current traffic configuration (state). After
    the action is executed, the vehicle finds itself in a new traffic configuration
    (next state). Before choosing the next action, this new state is perturbed, for
    example, by altering the positions of surrounding vehicles. This means the vehicle
    now faces a modified traffic configuration (perturbed next state) from which it
    must decide its next move.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在自动驾驶汽车的情况下，汽车可能会根据当前的交通配置（状态）选择变道（动作）。动作执行后，汽车会发现自己处于新的交通配置（下一个状态）。在选择下一个动作之前，这个新状态会被扰动，例如，通过改变周围车辆的位置。这意味着汽车现在面临的是一个经过修改的交通配置（扰动后的下一个状态），它必须从中决定下一步行动。
- en: This process introduces variability into the environment’s dynamics without
    necessitating a direct modification of the environment’s transition function.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程为环境的动态引入了变异性，而无需直接修改环境的过渡函数。
- en: The key difference between perturbing the current state and perturbing the next
    state lies in the agent’s awareness of its situation. In current state perturbation,
    the agent lacks true knowledge of its precise state when choosing the action because
    this state is modified just before the action is applied. However, in next state
    perturbation, the agent has full awareness of its current state when choosing
    the action.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 扰动当前状态和扰动下一个状态的关键区别在于代理对其情况的意识。在当前状态扰动中，代理在选择动作时对其精确状态缺乏真实知识，因为该状态在动作应用前刚刚被修改。然而，在下一个状态扰动中，代理在选择动作时对其当前状态具有完全的意识。
- en: 'More formally, the adversary $\xi$ acts to produce an altered next state $s^{\prime}_{t+1}$
    by perturbing the state after the transition function via the posterior transition
    function $T_{\xi+}^{\Omega}(\widetilde{s}_{t+1}|s_{t+1},a^{\xi,S+}_{t})$ introduced
    in Section [4.1](#S4.SS1 "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks
    of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey"). In that case, $\xi$ can be regarded as an agent that acts by emitting
    adversarial actions $a^{\xi,S+}_{t}\sim\xi(\cdot|s_{t},x_{t})$, given $\pi$ in
    a POMDP defined as: $\Omega^{\pi}=(S^{\Omega},A^{\Omega}_{\xi,S+},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$,
    where sampling $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$ is performed
    in three steps, as shown in Algorithm [5](#alg5 "Algorithm 5 ‣ Next State Perturbation
    ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2
    Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '更正式地说，对手 $\xi$ 通过在过渡函数之后扰动状态，从而产生一个被改变的下一个状态 $s^{\prime}_{t+1}$，这一过程由在第 [4.1](#S4.SS1
    "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") 节中引入的后验过渡函数
    $T_{\xi+}^{\Omega}(\widetilde{s}_{t+1}|s_{t+1},a^{\xi,S+}_{t})$ 实现。在这种情况下，$\xi$
    可以视为一个通过发出对抗性动作 $a^{\xi,S+}_{t}\sim\xi(\cdot|s_{t},x_{t})$ 进行操作的代理，给定在 POMDP 中定义的
    $\pi$：$\Omega^{\pi}=(S^{\Omega},A^{\Omega}_{\xi,S+},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$，其中对
    $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$ 的采样是通过三个步骤完成的，如算法 [5](#alg5
    "Algorithm 5 ‣ Next State Perturbation ‣ 4.2.2 Alteration of the Transition Function
    𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey") 所示。'
- en: Algorithm 5 $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 5 $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$
- en: 1:Input $s_{t},a^{\xi,S+}_{t}$ $\triangleright$ state, adversary action2:sample
    $\widetilde{s}_{t}\sim T_{\xi+}^{\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$ $\triangleright$
    perturbed state3:sample $\widetilde{x}_{t}\sim O^{\Omega}(\cdot|\widetilde{s}_{t})$
    $\triangleright$ observation4:sample $a_{t}\sim\pi(\cdot|x_{t})$ $\triangleright$
    agent action5:sample $s_{t+1}\sim T^{\Omega}(\cdot|\widetilde{s}_{t},a_{t})$ $\triangleright$
    next state after transition6:return $s_{t+1}$
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入 $s_{t},a^{\xi,S+}_{t}$ $\triangleright$ 状态，敌对动作 2: 采样 $\widetilde{s}_{t}\sim
    T_{\xi+}^{\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$ $\triangleright$ 扰动状态 3: 采样 $\widetilde{x}_{t}\sim
    O^{\Omega}(\cdot|\widetilde{s}_{t})$ $\triangleright$ 观察 4: 采样 $a_{t}\sim\pi(\cdot|x_{t})$
    $\triangleright$ 代理动作 5: 采样 $s_{t+1}\sim T^{\Omega}(\cdot|\widetilde{s}_{t},a_{t})$
    $\triangleright$ 转移后的下一个状态 6: 返回 $s_{t+1}$'
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ Next State Perturbation ‣ 4.2.2 Alteration of
    the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") presents a flowchart illustrating
    how the next state perturbation integrates into the POMDP.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '图[6](#S4.F6 "Figure 6 ‣ Next State Perturbation ‣ 4.2.2 Alteration of the Transition
    Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy
    of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey")展示了一个流程图，说明了下一个状态扰动如何融入POMDP中。'
- en: '![Refer to caption](img/56e72c58ab14589a358011dd14f38c12.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/56e72c58ab14589a358011dd14f38c12.png)'
- en: 'Figure 6: Flowchart of the perturbation of the next state'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: 下一个状态的扰动流程图'
- en: Action Perturbation
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动作扰动
- en: In this setting, the process starts with the agent in an initial state. The
    agent chooses an action, which is intended to be applied in the environment. However,
    before this action can be applied, it undergoes a perturbation, resulting in a
    perturbed action. This perturbed action is then applied, leading to transition
    to a new state according to the environment’s transition function.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，过程从代理处于初始状态开始。代理选择一个动作，该动作旨在应用于环境中。然而，在此动作可以应用之前，它会经历一次扰动，导致产生一个扰动后的动作。然后应用这个扰动后的动作，按照环境的过渡函数，导致状态转移到一个新的状态。
- en: For instance, consider an autonomous vehicle that decides to steer at an angle
    of $\alpha$ (action) based on the current traffic configuration (state). Before
    the steering action is executed, it is perturbed, so the actual steering angle
    applied to the vehicle becomes $\alpha+\epsilon$ (perturbed action). As a result,
    the vehicle transitions into a new traffic configuration (next state) that reflects
    the outcome of the perturbed steering action.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个自主车辆，它决定根据当前交通配置以角度 $\alpha$（动作）进行转向。在转向动作执行之前，它会被扰动，因此实际施加到车辆上的转向角度变为
    $\alpha+\epsilon$（扰动动作）。结果，车辆转移到一个新的交通配置（下一个状态），反映了扰动转向动作的结果。
- en: This process introduces variability into the environment’s dynamics without
    necessitating a direct modification of the environment’s transition function or
    modification of the state of the environment. However, this approach to modifying
    dynamics, while introducing variability, is confined to the scope of action perturbation,
    limiting the diversity of potential dynamics alterations.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在环境的动态中引入了变异，而无需直接修改环境的过渡函数或环境状态。然而，这种修改动态的方法虽然引入了变异，但仅限于动作扰动的范围，限制了潜在动态变化的多样性。
- en: 'More formally, the adversary $\xi$ acts to induce an altered next state $s^{\prime}_{t+1}$
    by perturbing the action decided by the agent $a_{t}\sim\pi(\cdot|x_{t})$ via
    the specific perturbation function $A^{\xi}(a^{\prime}_{t}|a_{t},a^{\xi,A}_{t})$
    introduced in Section [4.1](#S4.SS1 "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey").'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '更正式地，敌对者 $\xi$ 通过扰动代理决定的动作 $a_{t}\sim\pi(\cdot|x_{t})$ 来引起一个改变后的下一个状态 $s^{\prime}_{t+1}$，使用在[4.1](#S4.SS1
    "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey")节中介绍的特定扰动函数
    $A^{\xi}(a^{\prime}_{t}|a_{t},a^{\xi,A}_{t})$。'
- en: 'In that case $\xi$ can be regarded as an agent that acts by emitting adversarial
    actions $a^{\xi,A}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$, given $\pi$ in a POMDP
    defined as: $\Omega^{\pi}=\big{(}(S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,A},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega}\big{)}$,
    where sampling $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,A}_{t})$
    is performed in four steps, as shown in Algorithm [6](#alg6 "Algorithm 6 ‣ Action
    Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics)
    ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，$\xi$ 可以视为一个通过发出对抗性动作 $a^{\xi,A}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$
    行动的代理，给定 $\pi$ 在定义为：$\Omega^{\pi}=\big{(}(S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,A},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega}\big{)}$
    的 POMDP 中，其中采样 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,A}_{t})$
    执行了四步，如算法 [6](#alg6 "Algorithm 6 ‣ Action Perturbation ‣ 4.2.2 Alteration of the
    Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣
    4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") 所示。'
- en: Algorithm 6 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,A}_{t})$
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 6 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,A}_{t})$
- en: 1:Input $s_{t},a_{t},a^{\xi,A}_{t}$ $\triangleright$ state, agent action and
    adversary action2:sample $a^{\prime}_{t}\sim A^{\xi}(\cdot|a_{t},a^{\xi,A}_{t})$
    $\triangleright$ perturbed agent action3:sample $s_{t+1}\sim T^{\Omega}(\cdot|s_{t},a^{\prime}_{t})$
    $\triangleright$ next state after transition4:sample $x_{t+1}\sim O^{\Omega}(\cdot|s_{t+1})$
    $\triangleright$ next observation5:sample $a_{t+1}\sim\pi(\cdot|x_{t+1})$ $\triangleright$
    next agent action6:return $s_{t+1}$, $a_{t+1}$
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入 $s_{t},a_{t},a^{\xi,A}_{t}$ $\triangleright$ 状态、代理动作和对抗动作'
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the
    Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣
    4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") presents a flowchart illustrating
    how the action perturbation integrates into the POMDP.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#S4.F7 "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition
    Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy
    of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") 展示了一个流程图，说明了动作扰动如何融入 POMDP。'
- en: '![Refer to caption](img/8baa29bcab52d8570993df69d7d30163.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8baa29bcab52d8570993df69d7d30163.png)'
- en: 'Figure 7: Flowchart of the perturbation of the action'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 动作扰动的流程图'
- en: 'Reversely, we can gather the point of view of the protagonist agent $\pi$ in
    a single example combining all four possible attacks just described by denoting
    the adversaries $\xi_{A}$, $\xi_{S-}$, $\xi_{T}$ and $\xi_{S+}$. The agent $\pi$
    acts on an altered POMDP $\Omega^{\xi}=(S^{\Omega},A^{\Omega},T^{\xi,\Omega},R^{\Omega},X^{\Omega},O^{\Omega})$,
    where $s_{t+1}\sim T^{\xi,\Omega}(\cdot|s_{t},a_{t})$ is performed in eleven steps,
    as shown in Algorithm [7](#alg7 "Algorithm 7 ‣ Action Perturbation ‣ 4.2.2 Alteration
    of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey").'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '反向地，我们可以通过将四种可能的攻击类型结合在一个示例中，来汇集主角代理 $\pi$ 的观点，记为对手 $\xi_{A}$、$\xi_{S-}$、$\xi_{T}$
    和 $\xi_{S+}$。代理 $\pi$ 在一个修改过的 POMDP $\Omega^{\xi}=(S^{\Omega},A^{\Omega},T^{\xi,\Omega},R^{\Omega},X^{\Omega},O^{\Omega})$
    上操作，其中 $s_{t+1}\sim T^{\xi,\Omega}(\cdot|s_{t},a_{t})$ 执行了十一步，如算法 [7](#alg7 "Algorithm
    7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment
    Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of
    DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") 所示。'
- en: Algorithm 7 $s_{t+1}\sim T^{\xi,\Omega}(\cdot|s_{t},a_{t})$
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 7 $s_{t+1}\sim T^{\xi,\Omega}(\cdot|s_{t},a_{t})$
- en: 1:Input $s_{t},a_{t}$ $\triangleright$ state and agent action2:sample $x_{t}\sim
    O^{\Omega}(\cdot|s_{t})$ $\triangleright$ observation3:sample $a^{\xi,A}_{t}\sim\xi_{A}(\cdot|s_{t},x_{t},a_{t})$
    $\triangleright$ adversary action A4:sample $a^{\xi,S}_{t}\sim\xi_{T-}(\cdot|s_{t},x_{t},a_{t})$
    $\triangleright$ adversary action S5:sample $a^{\prime}_{t}\sim A^{\xi}(\cdot|a_{t},a^{\xi,A}_{t})$
    $\triangleright$ perturbed action6:sample $\widetilde{s}_{t}\sim T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$
    $\triangleright$ perturbed state7:sample $\widetilde{x}_{t}\sim O^{\Omega}(\cdot|\widetilde{s}_{t})$
    $\triangleright$ observation of the perturbed state8:sample $a^{\xi,T}_{t}\sim\xi_{T}(\cdot|\widetilde{s}_{t},\widetilde{x}_{t},a^{\prime}_{t})$
    $\triangleright$ adversary action T9:sample $\widetilde{s}_{t+1}\sim T_{\xi}^{\Omega}(\cdot|\widetilde{s}_{t},a^{\prime}_{t},a^{\xi,T}_{t})$
    $\triangleright$ next state after perturbed transition10:sample $\widetilde{x}_{t+1}\sim
    O^{\Omega}(\cdot|\widetilde{s}_{t+1})$ $\triangleright$ observation of the next
    state11:sample $a^{\xi,S+}_{t}\sim\xi_{T+}(\cdot|\widetilde{s}_{t+1},\widetilde{x}_{t+1})$
    $\triangleright$ adversary action S+12:sample $s_{t+1}\sim T_{\xi+}^{\Omega}(\cdot|\widetilde{s}_{t+1},a^{\xi,S+}_{t})$
    $\triangleright$ perturbed next state13:return $s_{t+1}$
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入 $s_{t},a_{t}$ $\triangleright$ 状态和代理动作2: 采样 $x_{t}\sim O^{\Omega}(\cdot|s_{t})$
    $\triangleright$ 观测3: 采样 $a^{\xi,A}_{t}\sim\xi_{A}(\cdot|s_{t},x_{t},a_{t})$ $\triangleright$
    敌对动作 A4: 采样 $a^{\xi,S}_{t}\sim\xi_{T-}(\cdot|s_{t},x_{t},a_{t})$ $\triangleright$
    敌对动作 S5: 采样 $a^{\prime}_{t}\sim A^{\xi}(\cdot|a_{t},a^{\xi,A}_{t})$ $\triangleright$
    扰动动作6: 采样 $\widetilde{s}_{t}\sim T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$
    $\triangleright$ 扰动状态7: 采样 $\widetilde{x}_{t}\sim O^{\Omega}(\cdot|\widetilde{s}_{t})$
    $\triangleright$ 扰动状态的观测8: 采样 $a^{\xi,T}_{t}\sim\xi_{T}(\cdot|\widetilde{s}_{t},\widetilde{x}_{t},a^{\prime}_{t})$
    $\triangleright$ 敌对动作 T9: 采样 $\widetilde{s}_{t+1}\sim T_{\xi}^{\Omega}(\cdot|\widetilde{s}_{t},a^{\prime}_{t},a^{\xi,T}_{t})$
    $\triangleright$ 扰动转移后的下一个状态10: 采样 $\widetilde{x}_{t+1}\sim O^{\Omega}(\cdot|\widetilde{s}_{t+1})$
    $\triangleright$ 下一个状态的观测11: 采样 $a^{\xi,S+}_{t}\sim\xi_{T+}(\cdot|\widetilde{s}_{t+1},\widetilde{x}_{t+1})$
    $\triangleright$ 敌对动作 S+12: 采样 $s_{t+1}\sim T_{\xi+}^{\Omega}(\cdot|\widetilde{s}_{t+1},a^{\xi,S+}_{t})$
    $\triangleright$ 扰动下一个状态13: 返回 $s_{t+1}$'
- en: 'Following this, the probability of an adversary-augmented transition $\tilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$
    given past $\tilde{\tau}_{0:t-1}$, is given by:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，给定过去的$\tilde{\tau}_{0:t-1}$，对敌对增强转移$\tilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$的概率由下式给出：
- en: '|  | <math   alttext="\pi^{\xi,\Omega}(\tilde{\tau}_{t}&#124;s_{t})=O^{\Omega}(x_{t}&#124;s_{t})\delta_{x_{t}}(x^{\prime}_{t})\pi(a_{t}&#124;x^{\prime}_{t})\xi_{A}(a^{\xi,A}_{t}&#124;s_{t},x_{t},a_{t})A^{\xi}(a^{\prime}_{t}&#124;a_{t},a^{\xi,A}_{t})\\
    \xi_{S-}(a^{\xi,S}_{t}&#124;s_{t},x_{t},a_{t})T_{\xi-}^{\Omega}(\widetilde{s}_{t}&#124;s_{t},a^{\xi,S}_{t})O^{\Omega}(\widetilde{x}_{t}&#124;\widetilde{s}_{t})\xi_{T}(a^{\xi,T}_{t}&#124;\widetilde{s}_{t},\widetilde{x}_{t},a^{\prime}_{t})T_{\xi}^{\Omega}(\widetilde{s}_{t+1}&#124;\widetilde{s}_{t},a^{\prime}_{t},a^{\xi,T}_{t})\\'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\pi^{\xi,\Omega}(\tilde{\tau}_{t}&#124;s_{t})=O^{\Omega}(x_{t}&#124;s_{t})\delta_{x_{t}}(x^{\prime}_{t})\pi(a_{t}&#124;x^{\prime}_{t})\xi_{A}(a^{\xi,A}_{t}&#124;s_{t},x_{t},a_{t})A^{\xi}(a^{\prime}_{t}&#124;a_{t},a^{\xi,A}_{t})\\
    \xi_{S-}(a^{\xi,S}_{t}&#124;s_{t},x_{t},a_{t})T_{\xi-}^{\Omega}(\widetilde{s}_{t}&#124;s_{t},a^{\xi,S}_{t})O^{\Omega}(\widetilde{x}_{t}&#124;\widetilde{s}_{t})\xi_{T}(a^{\xi,T}_{t}&#124;\widetilde{s}_{t},\widetilde{x}_{t},a^{\prime}_{t})T_{\xi}^{\Omega}(\widetilde{s}_{t+1}&#124;\widetilde{s}_{t},a^{\prime}_{t},a^{\xi,T}_{t})\\'
- en: O^{\Omega}(\widetilde{x}_{t+1}&#124;\widetilde{s}_{t+1})\xi_{S+}(a^{\xi,S+}_{t}&#124;\widetilde{s}_{t+1},\widetilde{x}_{t+1})T_{\xi+}^{\Omega}(s_{t+1}&#124;\widetilde{s}_{t+1},a^{\xi,S+}_{t})"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left" ><mrow ><mrow ><msup ><mi >π</mi><mrow ><mi >ξ</mi><mo
    >,</mo><mi mathvariant="normal" >Ω</mi></mrow></msup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mover accent="true" ><mi
    >τ</mi><mo >~</mo></mover><mi >t</mi></msub><mo fence="false" >&#124;</mo><msub
    ><mi >s</mi><mi >t</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><msup ><mi >O</mi><mi mathvariant="normal"  >Ω</mi></msup><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi >x</mi><mi
    >t</mi></msub><mo fence="false" >&#124;</mo><msub ><mi >s</mi><mi >t</mi></msub></mrow><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0em" rspace="0em" >​</mo><msub ><mi
    >δ</mi><msub ><mi >x</mi><mi >t</mi></msub></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >x</mi><mi >t</mi><mo
    >′</mo></msubsup><mo stretchy="false" >)</mo></mrow><mo lspace="0em" rspace="0em"
    >​</mo><mi >π</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><msub ><mi >a</mi><mi >t</mi></msub><mo fence="false"  >&#124;</mo><msubsup ><mi
    >x</mi><mi >t</mi><mo >′</mo></msubsup></mrow><mo stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mi >ξ</mi><mi >A</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><msubsup ><mi
    >a</mi><mi >t</mi><mrow ><mi >ξ</mi><mo >,</mo><mi >A</mi></mrow></msubsup><mo
    fence="false"  >&#124;</mo><mrow ><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi >x</mi><mi >t</mi></msub><mo >,</mo><msub ><mi >a</mi><mi >t</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0em" rspace="0em" >​</mo><msup ><mi
    >A</mi><mi >ξ</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><msubsup ><mi >a</mi><mi >t</mi><mo >′</mo></msubsup><mo fence="false"  >&#124;</mo><mrow
    ><msub ><mi >a</mi><mi >t</mi></msub><mo >,</mo><msubsup ><mi >a</mi><mi >t</mi><mrow
    ><mi >ξ</mi><mo >,</mo><mi >A</mi></mrow></msubsup></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right" ><mrow ><msub ><mi >ξ</mi><mrow ><mi >S</mi><mo >−</mo></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup
    ><mi >a</mi><mi >t</mi><mrow ><mi >ξ</mi><mo >,</mo><mi >S</mi></mrow></msubsup><mo
    fence="false"  >&#124;</mo><mrow ><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi >x</mi><mi >t</mi></msub><mo >,</mo><msub ><mi >a</mi><mi >t</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0em" rspace="0em" >​</mo><msubsup
    ><mi >T</mi><mrow ><mi >ξ</mi><mo >−</mo></mrow><mi mathvariant="normal" >Ω</mi></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub
    ><mover accent="true" ><mi >s</mi><mo >~</mo></mover><mi >t</mi></msub><mo fence="false"
    >&#124;</mo><mrow ><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msubsup ><mi
    >a</mi><mi >t</mi><mrow ><mi >ξ</mi><mo >,</mo><mi >S</mi></mrow></msubsup></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0em" rspace="0em" >​</mo><msup ><mi
    >O</mi><mi mathvariant="normal" >Ω</mi></msup><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><msub ><mover accent="true" ><mi >x</mi><mo
    >~</mo></mover><mi >t</mi></msub><mo fence="false" >&#124;</mo><msub ><mover accent="true"  ><mi
    >s</mi><mo >~</mo></mover><mi >t</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mi >ξ</mi><mi >T</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><msubsup ><mi
    >a</mi><mi >t</mi><mrow ><mi >ξ</mi><mo >,</mo><mi >T</mi></mrow></msubsup><mo
    fence="false"  >&#124;</mo><mrow ><msub ><mover accent="true"  ><mi >s</mi><mo
    >~</mo></mover><mi >t</mi></msub><mo >,</mo><msub ><mover accent="true"  ><mi
    >x</mi><mo >~</mo></mover><mi >t</mi></msub><mo >,</mo><msubsup ><mi >a</mi><mi
    >t</mi><mo >′</mo></msubsup></mrow></mrow><mo stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msubsup ><mi >T</mi><mi >ξ</mi><mi mathvariant="normal"
    >Ω</mi></msubsup><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><msub ><mover accent="true" ><mi >s</mi><mo >~</mo></mover><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo fence="false"  >&#124;</mo><mrow
    ><msub ><mover accent="true"  ><mi >s</mi><mo >~</mo></mover><mi >t</mi></msub><mo
    >,</mo><msubsup ><mi >a</mi><mi >t</mi><mo >′</mo></msubsup><mo >,</mo><msubsup
    ><mi >a</mi><mi >t</mi><mrow ><mi >ξ</mi><mo >,</mo><mi >T</mi></mrow></msubsup></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"
    ><mrow ><msup ><mi >O</mi><mi mathvariant="normal" >Ω</mi></msup><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mover accent="true"
    ><mi >x</mi><mo >~</mo></mover><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo
    fence="false"  >&#124;</mo><msub ><mover accent="true"  ><mi >s</mi><mo >~</mo></mover><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></mrow><mo stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mi >ξ</mi><mrow ><mi >S</mi><mo >+</mo></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup
    ><mi >a</mi><mi >t</mi><mrow ><mi >ξ</mi><mo >,</mo><mrow ><mi >S</mi><mo >+</mo></mrow></mrow></msubsup><mo
    fence="false"  >&#124;</mo><mrow ><msub ><mover accent="true"  ><mi >s</mi><mo
    >~</mo></mover><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mover accent="true"  ><mi >x</mi><mo >~</mo></mover><mrow ><mi >t</mi><mo >+</mo><mn
    >1</mn></mrow></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow><mo lspace="0em"
    rspace="0em" >​</mo><msubsup ><mi >T</mi><mrow ><mi >ξ</mi><mo >+</mo></mrow><mi
    mathvariant="normal"  >Ω</mi></msubsup><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><msub ><mi >s</mi><mrow ><mi >t</mi><mo >+</mo><mn
    >1</mn></mrow></msub><mo fence="false"  >&#124;</mo><mrow ><msub ><mover accent="true"  ><mi
    >s</mi><mo >~</mo></mover><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo
    >,</mo><msubsup ><mi >a</mi><mi >t</mi><mrow ><mi >ξ</mi><mo >,</mo><mrow ><mi
    >S</mi><mo >+</mo></mrow></mrow></msubsup></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝜋</ci><list ><ci >𝜉</ci><ci >Ω</ci></list></apply><apply ><csymbol cd="latexml"
    >conditional</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >~</ci><ci >𝜏</ci></apply><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply></apply></apply><apply ><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑂</ci><ci >Ω</ci></apply><apply
    ><csymbol cd="latexml"  >conditional</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑡</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝛿</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑥</ci><ci >′</ci></apply><ci >𝑡</ci></apply><ci >𝜋</ci><apply ><csymbol cd="latexml"  >conditional</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑥</ci><ci >′</ci></apply><ci >𝑡</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜉</ci><ci >𝐴</ci></apply><apply ><csymbol cd="latexml"  >conditional</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑎</ci><list ><ci >𝜉</ci><ci >𝐴</ci></list></apply><ci >𝑡</ci></apply><list ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply></list></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝐴</ci><ci >𝜉</ci></apply><apply
    ><csymbol cd="latexml"  >conditional</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑎</ci><ci >′</ci></apply><ci
    >𝑡</ci></apply><list ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑎</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑎</ci><list ><ci >𝜉</ci><ci
    >𝐴</ci></list></apply><ci >𝑡</ci></apply></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜉</ci><apply ><csymbol cd="latexml"  >limit-from</csymbol><ci >𝑆</ci></apply></apply><apply
    ><csymbol cd="latexml"  >conditional</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑎</ci><list ><ci >𝜉</ci><ci
    >𝑆</ci></list></apply><ci >𝑡</ci></apply><list ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑎</ci><ci >𝑡</ci></apply></list></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑇</ci><apply ><csymbol cd="latexml"  >limit-from</csymbol><ci
    >𝜉</ci></apply></apply><ci >Ω</ci></apply><apply ><csymbol cd="latexml"  >conditional</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >~</ci><ci >𝑠</ci></apply><ci
    >𝑡</ci></apply><list ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑎</ci><list ><ci >𝜉</ci><ci
    >𝑆</ci></list></apply><ci >𝑡</ci></apply></list></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑂</ci><ci >Ω</ci></apply><apply ><csymbol cd="latexml"  >conditional</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >~</ci><ci >𝑥</ci></apply><ci
    >𝑡</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci
    >~</ci><ci >𝑠</ci></apply><ci >𝑡</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜉</ci><ci >𝑇</ci></apply><apply ><csymbol cd="latexml"  >conditional</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑎</ci><list ><ci >𝜉</ci><ci >𝑇</ci></list></apply><ci >𝑡</ci></apply><list ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >~</ci><ci >𝑠</ci></apply><ci
    >𝑡</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci
    >~</ci><ci >𝑥</ci></apply><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑎</ci><ci >′</ci></apply><ci
    >𝑡</ci></apply></list></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑇</ci><ci >𝜉</ci></apply><ci
    >Ω</ci></apply><apply ><csymbol cd="latexml"  >conditional</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><ci >~</ci><ci >𝑠</ci></apply><apply
    ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply><list ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply ><ci >~</ci><ci >𝑠</ci></apply><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑎</ci><ci >′</ci></apply><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑎</ci><list ><ci >𝜉</ci><ci
    >𝑇</ci></list></apply><ci >𝑡</ci></apply></list></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑂</ci><ci >Ω</ci></apply><apply ><csymbol cd="latexml"  >conditional</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >~</ci><ci >𝑥</ci></apply><apply
    ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><ci >~</ci><ci >𝑠</ci></apply><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜉</ci><apply ><csymbol cd="latexml"  >limit-from</csymbol><ci
    >𝑆</ci></apply></apply><apply ><csymbol cd="latexml"  >conditional</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑎</ci><list ><ci >𝜉</ci><apply ><csymbol cd="latexml"  >limit-from</csymbol><ci
    >𝑆</ci></apply></list></apply><ci >𝑡</ci></apply><list ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><ci >~</ci><ci >𝑠</ci></apply><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >~</ci><ci >𝑥</ci></apply><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></list></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑇</ci><apply ><csymbol cd="latexml"  >limit-from</csymbol><ci >𝜉</ci></apply></apply><ci
    >Ω</ci></apply><apply ><csymbol cd="latexml"  >conditional</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply><list
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><ci >~</ci><ci >𝑠</ci></apply><apply
    ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑎</ci><list ><ci >𝜉</ci><apply ><csymbol cd="latexml" >limit-from</csymbol><ci
    >𝑆</ci></apply></list></apply><ci >𝑡</ci></apply></list></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\pi^{\xi,\Omega}(\tilde{\tau}_{t}&#124;s_{t})=O^{\Omega}(x_{t}&#124;s_{t})\delta_{x_{t}}(x^{\prime}_{t})\pi(a_{t}&#124;x^{\prime}_{t})\xi_{A}(a^{\xi,A}_{t}&#124;s_{t},x_{t},a_{t})A^{\xi}(a^{\prime}_{t}&#124;a_{t},a^{\xi,A}_{t})\\
    \xi_{S-}(a^{\xi,S}_{t}&#124;s_{t},x_{t},a_{t})T_{\xi-}^{\Omega}(\widetilde{s}_{t}&#124;s_{t},a^{\xi,S}_{t})O^{\Omega}(\widetilde{x}_{t}&#124;\widetilde{s}_{t})\xi_{T}(a^{\xi,T}_{t}&#124;\widetilde{s}_{t},\widetilde{x}_{t},a^{\prime}_{t})T_{\xi}^{\Omega}(\widetilde{s}_{t+1}&#124;\widetilde{s}_{t},a^{\prime}_{t},a^{\xi,T}_{t})\\
    O^{\Omega}(\widetilde{x}_{t+1}&#124;\widetilde{s}_{t+1})\xi_{S+}(a^{\xi,S+}_{t}&#124;\widetilde{s}_{t+1},\widetilde{x}_{t+1})T_{\xi+}^{\Omega}(s_{t+1}&#124;\widetilde{s}_{t+1},a^{\xi,S+}_{t})</annotation></semantics></math>
    |  |
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: \(O^{\Omega}(\widetilde{x}_{t+1} \mid \widetilde{s}_{t+1}) \xi_{S+}(a^{\xi,S+}_{t}
    \mid \widetilde{s}_{t+1}, \widetilde{x}_{t+1}) T_{\xi+}^{\Omega}(s_{t+1} \mid
    \widetilde{s}_{t+1}, a^{\xi,S+}_{t})\)
- en: In all settings, the reward functions $R^{\Omega}$ and $R_{\xi}^{\Omega}$ are
    defined on environment transitions $(s_{t},a_{t},s_{t+1})$.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有设置中，奖励函数 $R^{\Omega}$ 和 $R_{\xi}^{\Omega}$ 定义在环境转移 $(s_{t},a_{t},s_{t+1})$
    上。
- en: 4.3 Adversarial Objective
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 对抗目标
- en: Adversarial attacks in RL are strategically designed to compromise specific
    aspects of agent behavior or environment dynamics. In general, they aim to prevent
    the agent from acting optimally, but the attacks vary in their objectives and
    methodologies. Even if the general goal of any adversarial attack is to reduce
    the performances of the agent, methods to achieve this can primarily have different
    objective function, for specific performance reductions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的对抗攻击是策略性地设计用来破坏代理行为或环境动态的特定方面。一般来说，它们旨在阻止代理采取最优行动，但攻击的目标和方法各不相同。即使任何对抗攻击的一般目标是降低代理的表现，实现这一目标的方法可以主要有不同的目标函数，以实现特定的表现降低。
- en: 4.3.1 Deviate Policy
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 偏离策略
- en: 'The primary goal is to deviate the agent from its initial, typically optimal,
    policy. We can deviate the policy to make it diverge from the original policy
    : in that case the adversary $\xi$ is designed to maximize the agent’s expected
    loss over the pairs of policy given perturbed observation $x^{\prime}$ and original
    observation $x$, this is often referred as untargeted attacks. Or, we can also
    deviate the policy to make it converge to a target policy: in that case the adversary
    $\xi$ is designed to minimize the agent’s expected loss over the pairs of policy
    given perturbed observation $x^{\prime}$ and another policy $g$ given original
    observation $x$, this is often referred as targeted attacks.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 主要目标是使代理偏离其初始的、通常是最优的策略。我们可以使策略偏离，使其与原始策略分歧：在这种情况下，对抗者 $\xi$ 的设计旨在最大化代理在给定扰动观测
    $x^{\prime}$ 和原始观测 $x$ 的策略对中的期望损失，这通常称为非定向攻击。或者，我们也可以使策略偏离，使其趋向于一个目标策略：在这种情况下，对抗者
    $\xi$ 的设计旨在最小化代理在给定扰动观测 $x^{\prime}$ 和另一策略 $g$ 在原始观测 $x$ 下的期望损失，这通常称为定向攻击。
- en: 'For adversarial attacks that alter the observation function, assuming the following
    shorthand notations: $\>\mathbb{E}_{x}=\mathbb{E}_{x\sim X}$; $\>\mathbb{E}_{a}=\mathbb{E}_{a^{\xi}\sim\xi(\cdot|x)}$;
    $\>\mathbb{E}_{x^{\prime}}=\mathbb{E}_{x^{\prime}\sim O^{\xi}(\cdot|x,a^{\xi}),||x^{\prime}-x||<\varepsilon}$,
    the objective function for untargeted attacks (policy divergence) is :'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于改变观测函数的对抗攻击，假设以下简写符号：$\>\mathbb{E}_{x}=\mathbb{E}_{x\sim X}$；$\>\mathbb{E}_{a}=\mathbb{E}_{a^{\xi}\sim\xi(\cdot|x)}$；$\>\mathbb{E}_{x^{\prime}}=\mathbb{E}_{x^{\prime}\sim
    O^{\xi}(\cdot|x,a^{\xi}),||x^{\prime}-x||<\varepsilon}$，非定向攻击（策略分歧）的目标函数是：
- en: '|  | $\xi^{*}=\arg\max_{\xi}\mathbb{E}_{x}\mathbb{E}_{a}\mathbb{E}_{x^{\prime}}\Big{[}\>\mathcal{L}\big{(}\,\pi(x^{\prime}),\pi(x)\,\big{)}\>\Big{]}$
    |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $\xi^{*}=\arg\max_{\xi}\mathbb{E}_{x}\mathbb{E}_{a}\mathbb{E}_{x^{\prime}}\Big{[}\>\mathcal{L}\big{(}\,\pi(x^{\prime}),\pi(x)\,\big{)}\>\Big{]}$
    |  |'
- en: 'The objective function for targeted attacks (policy convergence) is :'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 定向攻击（策略收敛）的目标函数是：
- en: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{x}\mathbb{E}_{a}E_{x^{\prime}}\Big{[}\>\mathcal{L}\big{(}\,\pi(x^{\prime}),g(x)\,\big{)}\>\Big{]}$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{x}\mathbb{E}_{a}\mathbb{E}_{x^{\prime}}\Big{[}\>\mathcal{L}\big{(}\,\pi(x^{\prime}),g(x)\,\big{)}\>\Big{]}$
    |  |'
- en: These formulations seek for the optimal adversarial strategy $\xi^{*}$ that
    maximizes (resp. minimizes) the expected loss $\mathcal{L}$, computed over a distribution
    of original observations $x$, actions $a^{\xi}$ according to the adversary policy,
    and perturbed observations $x^{\prime}$ resulting from the altered observation
    function $O^{\xi}$, constrained by the condition that the perturbation in observation
    is less than $\varepsilon$. The loss $\mathcal{L}$ measures the difference between
    the policy’s output over the perturbed observations $\pi(x^{\prime})$ and the
    policy’s output over the original observations $\pi(x)$ (resp. the target policy’s
    output over the original observations $g(x)$).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这些公式寻求最优对抗策略 $\xi^{*}$，该策略最大化（或最小化）期望损失 $\mathcal{L}$，该损失是对原始观测 $x$ 的分布、根据对抗者策略的动作
    $a^{\xi}$ 和由改变后的观测函数 $O^{\xi}$ 产生的扰动观测 $x^{\prime}$ 计算的，且受限于观测的扰动小于 $\varepsilon$
    的条件。损失 $\mathcal{L}$ 测量了在扰动观测 $\pi(x^{\prime})$ 上的策略输出与在原始观测 $\pi(x)$ 上的策略输出之间的差异（或目标策略在原始观测
    $g(x)$ 上的输出）。
- en: 'For adversarial attacks that alter the dynamics of the environment, assuming
    the following shorthand notations: $\mathbb{E}_{s,a}=\mathbb{E}_{(s,a)\sim(S,A)}$;
    $\>\mathbb{E}_{x}=\mathbb{E}_{x\sim O^{\Omega}(\cdot|s)}$; $\>\mathbb{E}_{a^{\xi}}=\mathbb{E}_{a^{\xi}\sim\xi(\cdot|x)}$;
    $\>\mathbb{E}_{s_{t+1}}=\mathbb{E}_{s_{t+1}\sim T^{\Omega}(\cdot|s,a)}$; $\>\mathbb{E}_{x_{t+1}}=\mathbb{E}_{x_{t+1}\sim
    O(\cdot|s_{t+1})}$; $\>\mathbb{E}_{\widetilde{s}_{t+1}}=\mathbb{E}_{\widetilde{s}_{t+1}\sim
    T^{\xi,\Omega}(\cdot|s,a),||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon}$; $\>\mathbb{E}_{\widetilde{x}_{t+1}}=\mathbb{E}_{\widetilde{x}_{t+1}\sim
    O(\cdot|\widetilde{s}_{t+1})}$, the objective function for untargeted attacks
    (policy divergence) is :'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些改变环境动态的对抗性攻击，假设以下简写符号：$\mathbb{E}_{s,a}=\mathbb{E}_{(s,a)\sim(S,A)}$；$\>\mathbb{E}_{x}=\mathbb{E}_{x\sim
    O^{\Omega}(\cdot|s)}$；$\>\mathbb{E}_{a^{\xi}}=\mathbb{E}_{a^{\xi}\sim\xi(\cdot|x)}$；$\>\mathbb{E}_{s_{t+1}}=\mathbb{E}_{s_{t+1}\sim
    T^{\Omega}(\cdot|s,a)}$；$\>\mathbb{E}_{x_{t+1}}=\mathbb{E}_{x_{t+1}\sim O(\cdot|s_{t+1})}$；$\>\mathbb{E}_{\widetilde{s}_{t+1}}=\mathbb{E}_{\widetilde{s}_{t+1}\sim
    T^{\xi,\Omega}(\cdot|s,a),||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon}$；$\>\mathbb{E}_{\widetilde{x}_{t+1}}=\mathbb{E}_{\widetilde{x}_{t+1}\sim
    O(\cdot|\widetilde{s}_{t+1})}$，对于未定向攻击（策略分歧）的目标函数为：
- en: '|  | $\xi^{*}=\arg\max_{\xi}\mathbb{E}_{s,a}\mathbb{E}_{x}\mathbb{E}_{a^{\xi}}\mathbb{E}_{s_{t+1}}\mathbb{E}_{x_{t+1}}\mathbb{E}_{\widetilde{s}_{t+1}}\mathbb{E}_{\widetilde{x}_{t+1}}\Big{[}\>\mathcal{L}\big{(}\pi(\widetilde{x}_{t+1}),\pi(x_{t+1})\big{)}\>\Big{]}$
    |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\xi^{*}=\arg\max_{\xi}\mathbb{E}_{s,a}\mathbb{E}_{x}\mathbb{E}_{a^{\xi}}\mathbb{E}_{s_{t+1}}\mathbb{E}_{x_{t+1}}\mathbb{E}_{\widetilde{s}_{t+1}}\mathbb{E}_{\widetilde{x}_{t+1}}\Big{[}\>\mathcal{L}\big{(}\pi(\widetilde{x}_{t+1}),\pi(x_{t+1})\big{)}\>\Big{]}$
    |  |'
- en: 'The objective function for targeted attacks (policy convergence) is :'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 针对有目标攻击（策略收敛）的目标函数为：
- en: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{s,a}\mathbb{E}_{x}\mathbb{E}_{a^{\xi}}\mathbb{E}_{s_{t+1}}\mathbb{E}_{x_{t+1}}\mathbb{E}_{\widetilde{s}_{t+1}}\mathbb{E}_{\widetilde{x}_{t+1}}\Big{[}\>\mathcal{L}\big{(}\pi(\widetilde{x}_{t+1}),g(x_{t+1})\big{)}\>\Big{]}$
    |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{s,a}\mathbb{E}_{x}\mathbb{E}_{a^{\xi}}\mathbb{E}_{s_{t+1}}\mathbb{E}_{x_{t+1}}\mathbb{E}_{\widetilde{s}_{t+1}}\mathbb{E}_{\widetilde{x}_{t+1}}\Big{[}\>\mathcal{L}\big{(}\pi(\widetilde{x}_{t+1}),g(x_{t+1})\big{)}\>\Big{]}$
    |  |'
- en: Here, $\xi^{*}$ is the optimal adversarial strategy that maximizes (resp. minimizes)
    the expected loss, considering the altered state dynamics and resulting observations,
    with the constraint that the perturbation in the state is less than $\varepsilon$.
    The expectations are over the distribution of original states $s$ and their observations
    $x$, adversarial actions $a^{\xi}$, perturbed states $\widetilde{s}_{t+1}$ from
    the modified transition function $T^{\xi,\Omega}$, and observations $\widetilde{x}_{t+1}$
    from the perturbed states. The loss $\mathcal{L}$ measures the difference between
    the policy’s output over the observations of perturbed next states $\pi(\widetilde{x}_{t+1})$
    and the policy’s output over the observations of original non perturbed next states
    $\pi(x_{t+1})$ (resp. the target policy’s output over the observations of original
    non perturbed next states $g(x_{t+1})$).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\xi^{*}$ 是最优对抗策略，旨在最大化（或最小化）期望损失，考虑到改变后的状态动态和由此产生的观察结果，并且满足状态扰动小于 $\varepsilon$
    的约束。期望值是对原始状态 $s$ 及其观察 $x$、对抗性动作 $a^{\xi}$、由修改过的转移函数 $T^{\xi,\Omega}$ 得到的扰动状态
    $\widetilde{s}_{t+1}$ 和扰动状态 $\widetilde{x}_{t+1}$ 的观察结果的分布进行的。损失 $\mathcal{L}$
    测量策略对扰动后状态观察的输出 $\pi(\widetilde{x}_{t+1})$ 和策略对原始未扰动状态观察的输出 $\pi(x_{t+1})$（或目标策略对原始未扰动状态观察的输出
    $g(x_{t+1})$）之间的差异。
- en: Even if the goal of applying these attacks can be to reduce the performances
    of the agent, the attacks themselves are designed to maximize the divergence (resp.
    convergence) of the policy, effectively causing the policy to produce significantly
    different actions or decisions based on the manipulated environment dynamics and
    observations.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些攻击的目标可能是降低智能体的表现，这些攻击本身也是设计用来最大化策略的分歧（或收敛），有效地导致策略在操控环境动态和观察的基础上产生显著不同的行动或决策。
- en: 4.3.2 Reward Minimization
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 奖励最小化
- en: In contrast, some adversarial attacks focus on leading the agent to less favorable
    states or decisions, thereby minimizing the total expected reward the agent accrues.
    These attacks are often targeted and seek for reduction of the efficacy or efficiency
    of the agent’s behavior by altering its reward acquisition.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，一些对抗性攻击则专注于将智能体引导到不利的状态或决策，从而最小化智能体获得的总期望奖励。这些攻击通常是有针对性的，通过改变奖励获取来寻求减少智能体行为的有效性或效率。
- en: 'The objective function optimized in such attacks, subject to the relevant perturbation
    constraints, is:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种攻击中，优化的目标函数，在相关的扰动约束条件下为：
- en: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{\widetilde{\tau}\sim\pi^{\xi,\Omega}(\widetilde{\tau})}\Big{[}R(\tau)\Big{]}\\
    \footnotesize{\widetilde{\tau}\text{ subject to }&#124;&#124;x^{\prime}-x&#124;&#124;<\varepsilon\text{
    or }&#124;&#124;\widetilde{s}_{t+1}-s_{t+1}&#124;&#124;<\varepsilon}$ |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{\widetilde{\tau}\sim\pi^{\xi,\Omega}(\widetilde{\tau})}\Big{[}R(\tau)\Big{]}\\
    \footnotesize{\widetilde{\tau}\text{ subject to }&#124;&#124;x^{\prime}-x&#124;&#124;<\varepsilon\text{
    or }&#124;&#124;\widetilde{s}_{t+1}-s_{t+1}&#124;&#124;<\varepsilon}$ |  |'
- en: Here, $\xi^{*}$ represents the adversarial strategy aimed at minimizing the
    agent’s total expected reward. The expectation is taken over the trajectories
    $\tau$ sampled according to a policy $\pi$ perturbed by the adversary in the environment
    $\Omega$. The function $R(\tau)$ calculates the discounted sum of rewards for
    each trajectory, with the goal of the adversary being to minimize this quantity
    through interventions, while adhering to the specified constraints on the perturbations.
    For attacks altering the observation, the difference between the perturbed observation
    $x^{\prime}$ and the original observation $x$ is constrained such that $||x^{\prime}-x||<\varepsilon$.
    Similarly, for attacks that alter the dynamics of the environment, the perturbation
    in the state is constrained with $||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon$.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\xi^{*}$ 代表旨在最小化代理的总期望奖励的对抗策略。期望是对根据对手在环境 $\Omega$ 中扰动的策略 $\pi$ 采样的轨迹 $\tau$
    进行的。函数 $R(\tau)$ 计算每个轨迹的折扣奖励总和，对手的目标是通过干预来最小化这一数量，同时遵守对扰动的指定约束。对于改变观察的攻击，扰动后的观察
    $x^{\prime}$ 和原始观察 $x$ 之间的差异被约束为 $||x^{\prime}-x||<\varepsilon$。类似地，对于改变环境动态的攻击，状态的扰动被约束为
    $||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon$。
- en: In practice, it is hard to verify $||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon$
    since once the transition function applied an the next state get, in most of the
    simulation environment available, it is not possible to redo the transition with
    some other inputs to get the alternative next state, to be able to compare them.
    Therefore often, the actually verified constraint is $||a^{\xi}_{t}||<\varepsilon$,
    this makes it hard to compare different Dynamic Attacks that do not alter the
    same element $s_{t}$, $a_{t}$, $T^{\Omega}$, or $s_{t+1}$.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，验证 $||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon$ 是困难的，因为一旦转移函数应用并得到下一个状态，在大多数可用的仿真环境中，无法用其他输入重新进行转移以获取备用下一个状态，进而进行比较。因此，实际验证的约束往往是
    $||a^{\xi}_{t}||<\varepsilon$，这使得很难比较不同的动态攻击，这些攻击没有改变相同的元素 $s_{t}$、$a_{t}$、$T^{\Omega}$
    或 $s_{t+1}$。
- en: 4.3.3 Others
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 其他
- en: Some methods have other objectives, for example to lead the agent to a specific
    target state. The objective is then to minimize the distance between the current
    state and the target state. Some other specific objectives can exist.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法有其他目标，例如将代理引导到特定的目标状态。目标是最小化当前状态与目标状态之间的距离。还可能存在其他特定的目标。
- en: 4.4 Knowledge Requirement
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 知识要求
- en: In the realm of adversarial attacks against DRL agents, the extent and nature
    of the attacker’s knowledge about the agent significantly influence the strategy
    and effectiveness of the attack. Broadly, these can be categorized into White
    Box and Black Box approaches, each with its own set of strategies, challenges,
    and considerations.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在针对DRL代理的对抗攻击领域中，对手对代理的知识程度和性质显著影响攻击的策略和有效性。广义上，这些可以分为白盒攻击和黑盒攻击，每种方法都有其自身的策略、挑战和考虑因素。
- en: 4.4.1 White Box
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 白盒攻击
- en: In this scenario, the adversary has complete knowledge of the agent’s architecture,
    parameters, and training data. This scenario represents the most informed type
    of attacks, where the adversary has access to all the inner workings of the agent,
    including its policy, value function, and possibly even the environment model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，对手对代理的架构、参数和训练数据有完全的了解。这种情况代表了最知情的攻击类型，对手可以访问代理的所有内部工作，包括其策略、价值函数，甚至可能包括环境模型。
- en: '$-$ Policy and Model Access: The adversary knows the exact policy and decision-making
    process of the agent. This includes access to the policy’s parameters, algorithm
    type, and architecture. In model-based RL, the attacker might also know the transition
    dynamics and reward function.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 策略和模型访问：对手知道智能体的确切策略和决策过程。这包括访问策略的参数、算法类型和架构。在基于模型的 RL 中，攻击者可能还知道转移动态和奖励函数。
- en: '$-$ Optimization and Perturbation: With complete knowledge, the attacker can
    craft precise and potent perturbations to the agent’s inputs or environment to
    maximize the deviation from desired behaviors or minimize rewards. They can calculate
    the exact gradients or other relevant information needed to optimize their attack
    strategy.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 优化和扰动：拥有完全知识的攻击者可以制定精确而强有力的扰动，以最大化行为偏离或最小化奖励。他们可以计算出优化攻击策略所需的精确梯度或其他相关信息。
- en: '$-$ Challenges and Implications: While white box attacks represent an idealized
    scenario with maximal knowledge, they provide a comprehensive framework for testing
    the agent’s robustness. By simulating the most extreme conditions an agent could
    face, developers can identify and reinforce potential vulnerabilities, leading
    to policies that are not only effective but also resilient to a wide range of
    scenarios, including unexpected environmental changes. This approach is particularly
    valuable in safety-critical applications where ensuring reliability against all
    possible disturbances is crucial.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 挑战和影响：虽然白盒攻击代表了一个理想化的具有最大知识的场景，但它们提供了一个全面的框架来测试智能体的鲁棒性。通过模拟智能体可能面临的最极端条件，开发者可以识别并强化潜在的漏洞，从而形成不仅有效而且能够抵御各种场景的策略，包括意外的环境变化。这种方法在安全关键应用中尤其有价值，因为确保系统在所有可能干扰下的可靠性至关重要。
- en: 4.4.2 Black Box
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 黑盒
- en: In this scenario, the adversary has limited or no knowledge of the internal
    workings of the agent. They may not know the specific policy, parameters, or architecture
    of the RL agent. Instead, they must rely on observable behaviors or outputs to
    infer information and craft their attacks.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，对手对智能体的内部运作知识有限或没有了解。他们可能不知道具体的策略、参数或 RL 智能体的架构。相反，他们必须依赖可观察到的行为或输出信息来推断并制定攻击策略。
- en: '$-$ Observational Inference: The attacker observes the agent’s actions and
    possibly some aspects of the state transitions to infer patterns, weaknesses,
    or predict future actions. This process often involves probing the agent with
    different inputs and analyzing the outputs.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 观察推断：攻击者观察智能体的动作，并可能观察到一些状态转移方面的信息，以推断模式、弱点或预测未来动作。这个过程通常涉及使用不同的输入探测智能体并分析输出。
- en: '$-$ Surrogate Models and Transferability: Attackers might train a surrogate
    model to approximate the agent’s behavior or policy. If an attack is successful
    on the surrogate, it might also be effective on the target agent due to transferability,
    especially if both are trained in similar environments or tasks.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 替代模型和可转移性：攻击者可能训练一个替代模型来近似智能体的行为或策略。如果攻击在替代模型上成功，那么它也可能对目标智能体有效，尤其是当两者在类似的环境或任务中训练时。
- en: '$-$ Challenges and Implications: The use of black box methods in enhancing
    robustness is not directly about realism of adversarial intent but rather about
    preparing for a variety of uncertain conditions and environmental changes. These
    methods encourage the development of general defense mechanisms that improve the
    agent’s adaptability and resilience. While the adversarial mindset might not reflect
    the typical operational challenges, the diversity and unpredictability of black
    box approaches help ensure that RL systems are robust not only against potential
    adversaries but also against a wide array of non-adversarial issues that could
    arise in dynamic and uncertain environments.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 挑战和影响：使用黑盒方法来增强鲁棒性并不直接涉及对手意图的现实性，而是关于为各种不确定条件和环境变化做好准备。这些方法鼓励开发通用防御机制，提高智能体的适应性和鲁棒性。尽管对手的心态可能不反映典型的操作挑战，但黑盒方法的多样性和不可预测性有助于确保
    RL 系统不仅能抵御潜在的对手，还能应对动态和不确定环境中可能出现的各种非对手问题。
- en: Both white and black box attacks paradigms play crucial roles in the study and
    development of adversarial strategies in RL. They help researchers and practitioners
    understand the spectrum of threats and devise more robust algorithms and defenses.
    By considering these different knowledge scenarios, one can better prepare RL
    agents to withstand or recover from adversarial attacks in various real-world
    applications.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是白盒还是黑盒攻击范式，在对抗策略的研究和开发中都扮演着关键角色。它们帮助研究人员和从业者理解威胁的范围，并制定更强大的算法和防御措施。通过考虑这些不同的知识场景，可以更好地准备RL代理应对或恢复各种现实应用中的对抗攻击。
- en: 4.5 Category of Approach
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 方法分类
- en: This section delineates the various methodologies utilized in crafting adversarial
    attacks, each with distinct strategies and theoretical underpinnings. It primarily
    divides into direct optimization-based and adversarial policy learning approaches.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了在制定对抗攻击时使用的各种方法，每种方法都有其独特的策略和理论基础。主要分为基于直接优化的方法和对抗策略学习方法。
- en: 4.5.1 Direct Optimization Based Approaches
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1 基于直接优化的方法
- en: They focus on directly manipulating the input or parameters of a model to induce
    misbehavior. These methods are subdivided into first-order and zeroth-order techniques,
    depending on the availability and usage of gradient information.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 它们专注于直接操控模型的输入或参数以引发不良行为。这些方法根据梯度信息的可用性和使用情况分为一阶和零阶技术。
- en: 'First Order Optimization approaches (White Box) : Gradient Attacks'
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一阶优化方法（白盒）：梯度攻击
- en: They utilize the gradient information of the model to craft adversarial examples,
    efficiently targeting the model’s weaknesses. Common in white-box scenarios, gradient
    attacks are powerful when model internals are accessible.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 它们利用模型的梯度信息来生成对抗样本，有效地针对模型的弱点。梯度攻击在模型内部信息可访问的白盒场景中很常见，并且非常强大。
- en: Zeroth Order Optimization approaches (Black Box)
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 零阶优化方法（黑盒）
- en: Or derivative-free methods, optimize the adversarial objective without requiring
    gradient information, making them suitable for black-box scenarios. Techniques
    include simulated annealing, genetic algorithms, and random search.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 或无导数的方法，优化对抗目标而无需梯度信息，使其适用于黑盒场景。技术包括模拟退火、遗传算法和随机搜索。
- en: 4.5.2 Adversarial Policy Learning Based Approaches
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2 基于对抗策略学习的方法
- en: 'These approaches involve training a separate model or policy to generate adversarial
    attacks. The adversarial model learns an optimal attack strategy through interaction
    with the target system, often using RL techniques. To train an Adversarial Policy
    (AP), optimization methods are used and could also be classified as First and
    Zeroth Order methods, but on the contrary to direct optimization methods, the
    optimization is used to train the adversary, not to directly craft the perturbation.
    Adversarial Policy Learning Based Approaches can be divided into two categories
    :'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法涉及训练一个独立的模型或策略来生成对抗攻击。对抗模型通过与目标系统的交互学习最优攻击策略，通常使用RL技术。训练对抗策略（AP）时使用优化方法，也可以分为一阶和零阶方法，但与直接优化方法不同，优化用于训练对手，而不是直接生成扰动。基于对抗策略学习的方法可以分为两类：
- en: Classical Adversarial Policies
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 经典对抗策略
- en: Learned via RL or any other method, only need a black box access to the model
    of the agent since they are policies sufficient in themselves.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是通过RL还是其他方法学习的，只需对代理的模型进行黑盒访问，因为它们自身的策略已经足够。
- en: Augmented Adversarial Policies
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增强对抗策略
- en: Learned via RL or any other approach, are augmented either with a white Box
    access to the agent’s model during training or inference phase, or with some Direct
    Optimization method are added besides the Adversarial Policy to improve its performances.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 通过RL或其他方法学习的，训练或推理阶段要么增强了对代理模型的白盒访问，要么在对抗策略之外添加了一些直接优化方法以提高性能。
- en: 'In the following sections, we will use this taxonomy as a framework to examine
    recent research on adversarial examples for DRL. Section [5.1](#S5.SS1 "5.1 Observation
    Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") focuses on input-space perturbations,
    and Section [5.2](#S5.SS2 "5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey")
    on environment-space perturbations.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用这一分类法作为框架来审视最近关于DRL的对抗样本研究。章节 [5.1](#S5.SS1 "5.1 观察变化攻击 ‣ 5 对抗攻击
    ‣ 通过对抗攻击和训练实现稳健的深度强化学习：一项调查") 关注输入空间扰动，章节 [5.2](#S5.SS2 "5.2 动态变化 ‣ 5 对抗攻击 ‣ 通过对抗攻击和训练实现稳健的深度强化学习：一项调查")
    关注环境空间扰动。
- en: 5 Adversarial Attacks
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 对抗攻击
- en: 'In this section, we conduct a comprehensive review of contemporary adversarial
    attacks as documented in current literature, presented in a hierarchical, tree-like
    structure (refer to Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey")).
    The review categorizes these attacks first based on the type of alteration induced
    in the POMDP: either Observation Alteration or Dynamic Alteration. Next, the categorization
    considers the underlying objective driving these attacks, which could be either
    to Deviate Policy or Minimize Reward. Lastly, the classification focuses on the
    computational approach employed: Direct Optimization (First or Zeroth Order) or
    Adversarial Policy Learning. For each method in this classification tree, we will
    provide a detailed description, ensuring to consistently include the following
    critical information: the nature of the perturbation support (whether it’s an
    observation, state, action, or transition function), the level of knowledge about
    the model required to execute the attack (white-box or black-box), and any specific
    constraints or potential limitations associated with the method.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对当前文献中记录的现代对抗攻击进行了全面的回顾，采用层次化的树状结构（参见图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 通过对抗攻击和训练实现稳健的深度强化学习：一项调查")）。回顾首先根据在POMDP中引发的变化类型对这些攻击进行分类：观察变化或动态变化。接着，分类考虑了驱动这些攻击的基本目标，这些目标可以是偏离策略或最小化奖励。最后，分类关注所采用的计算方法：直接优化（第一阶或零阶）或对抗策略学习。对于分类树中的每种方法，我们将提供详细描述，并确保始终包括以下关键信息：扰动支持的性质（是否为观察、状态、动作或转移函数），执行攻击所需的模型知识水平（白盒或黑盒），以及与方法相关的任何特定约束或潜在局限性。
- en: 5.1 Observation Alteration Attacks
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 观察变化攻击
- en: This section delves into the analysis of Observation Alteration Attacks targeting
    RL agents. These attacks specifically modify the observation function in the POMDP
    framework. Such methods are instrumental in simulating sensor errors in an agent,
    creating discrepancies between the agent’s perceived observations and the actual
    underlying state. These techniques can be particularly beneficial during an agent’s
    training phase, enhancing its resilience to potential observation discrepancies
    that might be encountered in real-world deployment scenarios. Observation Alteration
    Attacks generate a perturbation $a^{\xi,X}_{\epsilon}$ for a given observation
    $x$, resulting in a perturbed observation $x^{\prime}=x+a^{\xi,X}_{\epsilon}$.
    The perturbation $a^{\xi,X}_{\epsilon}$ is constrained within an $\epsilon$-ball
    of a specified norm $p\in{L_{1},L_{2},\ldots,L_{\infty}}$. This constraint can
    be achieved from any arbitrary perturbation $a^{\xi,X}$ by computing $a^{\xi,X}_{\epsilon}=\dfrac{\epsilon}{||a^{\xi,X}||_{p}}\cdot
    a^{\xi,X}$. Alternatively, in the context of a $L_{0}$ $\epsilon$-ball, $a^{\xi,X}_{\epsilon}$
    is defined by assigning the top-$\epsilon$ values of $a^{\xi,X}$ as 1 and setting
    all other values to 0.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 本节深入分析了针对RL代理的观察篡改攻击。这些攻击特别是在POMDP框架中修改观察函数。这些方法在模拟代理的传感器错误方面至关重要，产生代理感知观察与实际底层状态之间的差异。这些技术在代理的训练阶段尤其有益，提高了其对在实际部署场景中可能遇到的观察差异的抗干扰能力。观察篡改攻击为给定观察$x$生成一个扰动$a^{\xi,X}_{\epsilon}$，导致一个扰动观察$x^{\prime}=x+a^{\xi,X}_{\epsilon}$。扰动$a^{\xi,X}_{\epsilon}$被限制在指定范数$p\in{L_{1},L_{2},\ldots,L_{\infty}}$的$\epsilon$-球内。这个约束可以通过计算$a^{\xi,X}_{\epsilon}=\dfrac{\epsilon}{||a^{\xi,X}||_{p}}\cdot
    a^{\xi,X}$从任意扰动$a^{\xi,X}$中实现。或者，在$L_{0}$ $\epsilon$-球的背景下，$a^{\xi,X}_{\epsilon}$通过将$a^{\xi,X}$的前$\epsilon$大值设置为1，并将所有其他值设置为0来定义。
- en: 5.1.1 Deviate Policy
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 偏离策略
- en: In the field of adversarial attacks on observations, most methods developed
    primarily focus on optimizing the deviation of the policy. These methods function
    by crafting a perturbed observation $x^{\prime}$ that replaces the original observation
    $x$. Consequently, the policy $\pi$ generates a different output $\pi(x^{\prime})\neq\pi(x)$.
    In the untargeted scenario, the goal is to maximize divergence from the original
    policy; this is achieved by maximizing a specific loss function between the policy
    output on the altered observation and the original observation, formulated as
    $\arg\max_{x^{\prime}}L(\pi(x^{\prime}),\pi(x))$. Conversely, in targeted attacks,
    the objective is to guide the policy towards a particular behavior. This is done
    by minimizing a defined loss between the policy output on the altered observation
    and a target policy $g$ on the original observation, expressed as $\arg\min_{x^{\prime}}L(\pi(x^{\prime}),g(x))$.
    While the primary focus of these optimization functions is the deviation of the
    policy, this often results in a corresponding reduction in the reward garnered
    by the agent. Although Adversarial Policy Learning Based Methods could theoretically
    be employed to create observation attacks intended to deviate policy, the prevailing
    methods in practice are predominantly Direct Optimization Methods.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察的对抗攻击领域，大多数方法主要集中在优化策略的偏离上。这些方法通过制作一个扰动观察$x^{\prime}$来替代原始观察$x$来运行。因此，策略$\pi$生成了不同的输出$\pi(x^{\prime})\neq\pi(x)$。在无目标场景中，目标是最大化与原始策略的偏差；这通过最大化对扰动观察和原始观察上策略输出之间的特定损失函数来实现，形式为$\arg\max_{x^{\prime}}L(\pi(x^{\prime}),\pi(x))$。相反，在目标攻击中，目标是引导策略朝向特定行为。这通过最小化扰动观察上策略输出与原始观察上的目标策略$g$之间的定义损失来实现，表达为$\arg\min_{x^{\prime}}L(\pi(x^{\prime}),g(x))$。虽然这些优化函数的主要关注点是策略的偏离，但这通常导致代理获得的奖励相应减少。尽管理论上可以使用对抗策略学习方法创建旨在偏离策略的观察攻击，但实际中主要使用的仍是直接优化方法。
- en: Direct Optimization Methods refer to techniques that directly compute an adversarial
    perturbation directly with optimization approaches. These approaches include gradient
    descent, evolutionary methods, stochastic optimization, among others. They are
    particularly effective for generating perturbations in observations with the aim
    of altering the policy’s behavior. A key advantage of these methods is their ability
    to be applied directly to a given agent model, without the necessity for extensive
    prior knowledge or preliminary computations. However, it is important to note
    that some of these methods might entail significant computational resources for
    each perturbation calculation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 直接优化方法指的是直接通过优化方法计算对抗扰动的技术。这些方法包括梯度下降、进化方法、随机优化等。它们特别有效于生成扰动，旨在改变策略的行为。这些方法的一个关键优势是可以直接应用于给定的代理模型，无需大量的先验知识或初步计算。然而，需要注意的是，这些方法中的一些可能需要大量的计算资源来进行每次扰动计算。
- en: 'First Order Optimization Methods: Gradient Attacks (White Box)'
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一阶优化方法：梯度攻击（白盒）
- en: 'They are methods initially introduced in the context of supervised classification.
    They utilize the gradient of the attacked model to compute a perturbation $a^{\xi,X}$
    for a given input $x$, thereby crafting a perturbed input $x^{\prime}$. Consequently,
    these methods require white-box access to the model being attacked. In the realm
    of supervised classification, they are typically defined using the general formula:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法最初在监督分类的背景下引入。它们利用被攻击模型的梯度来计算给定输入 $x$ 的扰动 $a^{\xi,X}$，从而构造一个扰动后的输入 $x^{\prime}$。因此，这些方法需要对被攻击的模型进行白盒访问。在监督分类领域，它们通常使用通用公式来定义：
- en: '|  | $x^{\prime}=x+a^{\xi,X}\quad\text{with}\quad a^{\xi,X}=\varepsilon\times\ldots\nabla_{x}L\big{(}f(x),y\big{)}\ldots$
    |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $x^{\prime}=x+a^{\xi,X}\quad\text{with}\quad a^{\xi,X}=\varepsilon\times\ldots\nabla_{x}L\big{(}f(x),y\big{)}\ldots$
    |  |'
- en: Here, $\varepsilon$ represents the magnitude of the perturbation, $f(x)$ is
    the model output, $y$ denotes the ground truth label for untargeted attacks or
    the target class for targeted attacks, and $L$ is a loss function (often the same
    one used in training, but not exclusively). The term $\nabla_{x}L$ signifies the
    gradient of the loss function $L$ with respect to the input $x$, and the \say…
    indicates that additional operations can be applied to this core equation to tailor
    the update function to the specific optimization problem at hand. When adapted
    to RL, the formula essentially remains unchanged, except $f(x)$ is replaced by
    $\pi(x)$, the output of the agent’s policy function. In this context, $y$ no longer
    represents the ground truth but rather the current action $a$ for untargeted attacks,
    or a targeted action for targeted attacks. Numerous gradient attack methods exist,
    with the most notable being FGSM and its extensions (BIM, PGD, C&W, DeepFool,
    …), as well as JSMA and its extensions (XSMA, VFGA, …). All these methods, initially
    designed for supervised classification, are applicable in RL. Their primary objective
    is to deviate the agent’s policy by creating adversarial observations. As they
    are designed to generate minimal perturbations around a given observation, they
    are generally suited for agents and environments with continuous observation spaces,
    such as images, feature vectors, and signals. These attacks employ first-order
    optimization methods as they utilize the direction directly provided by the model’s
    gradient, thus categorizing them as white-box methods, which necessitate complete
    knowledge of the agent model’s architecture to obtain the gradient. These methods
    can be employed either in an untargeted manner, by maximizing the loss between
    the chosen action and itself, or in a targeted manner, by minimizing the loss
    between the chosen action and a specific target action.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\varepsilon$ 代表扰动的幅度，$f(x)$ 是模型输出，$y$ 表示未指定目标攻击的真实标签或指定目标攻击的目标类别，而 $L$
    是损失函数（通常与训练中使用的相同，但并不一定）。术语 $\nabla_{x}L$ 表示损失函数 $L$ 对输入 $x$ 的梯度，\say… 表示可以对这个核心方程应用额外的操作，以将更新函数量身定制为特定的优化问题。当适应于
    RL 时，公式本质上保持不变，只是 $f(x)$ 被替换为 $\pi(x)$，即智能体的策略函数的输出。在这个背景下，$y$ 不再表示真实标签，而是未指定目标攻击的当前动作
    $a$，或指定目标攻击的目标动作。存在许多梯度攻击方法，其中最著名的是 FGSM 及其扩展（BIM、PGD、C&W、DeepFool 等），以及 JSMA
    及其扩展（XSMA、VFGA 等）。所有这些方法最初是为监督分类设计的，但也适用于 RL。它们的主要目标是通过生成对抗性观测来偏离智能体的策略。由于它们旨在围绕给定观测生成最小扰动，因此通常适用于具有连续观测空间的智能体和环境，如图像、特征向量和信号。这些攻击采用一阶优化方法，因为它们直接利用模型梯度提供的方向，从而将其归类为白盒方法，这需要对智能体模型的架构有完整的了解以获得梯度。这些方法可以以未指定目标的方式应用，通过最大化所选动作与自身之间的损失，或者以指定目标的方式应用，通过最小化所选动作与特定目标动作之间的损失。
- en: $-$ FGSM [[31](#bib.bib31)] is a fast computing method for crafting effective
    perturbed observations with $x^{\prime}=x+\varepsilon\cdot a^{\xi,X}$ with $a^{\xi,X}=\text{sign}\big{(}\nabla_{x}L(f(x),y)\big{)}$.
    In some case a variant is preferred with $a^{\xi,X}=\nabla_{x}L\big{(}f(x),y\big{)}$.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ FGSM [[31](#bib.bib31)] 是一种快速计算方法，用于生成有效的扰动观测，其形式为 $x^{\prime}=x+\varepsilon\cdot
    a^{\xi,X}$，其中 $a^{\xi,X}=\text{sign}\big{(}\nabla_{x}L(f(x),y)\big{)}$。在某些情况下，可能更偏好一种变体，其形式为
    $a^{\xi,X}=\nabla_{x}L\big{(}f(x),y\big{)}$。
- en: $-$ BIM [[31](#bib.bib31)] and PGD [[32](#bib.bib32)] are iterative versions
    of FGSM, BIM simply applies FGSM multiple times with small steps, while PGD is
    more refined and projects the adversarial example back into a feasible set after
    each iteration. They are more computation needing, since they are iterative methods
    that computes the gradient several times to craft more precise adversarial observations.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ BIM [[31](#bib.bib31)] 和 PGD [[32](#bib.bib32)] 是 FGSM 的迭代版本，BIM 只是多次应用
    FGSM 并采取小步，而 PGD 则更为精细，每次迭代后将对抗样本投影回可行集合。它们需要更多的计算，因为它们是迭代方法，计算梯度多次以生成更精确的对抗观测。
- en: $-$ DeepFool [[33](#bib.bib33)] is an iterative method. In each iteration, it
    linearizes the classifier’s decision boundary around the current input and then
    computes the perturbation to cross this linearized boundary.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ DeepFool [[33](#bib.bib33)] 是一种迭代方法。在每次迭代中，它线性化分类器在当前输入周围的决策边界，然后计算扰动以穿越这个线性化的边界。
- en: $-$ C&W [[34](#bib.bib34)] is a method that seeks to minimize the perturbation
    while ensuring that the perturbed input is classified as a specific target class.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ C&W [[34](#bib.bib34)] 是一种试图在确保扰动输入被分类为特定目标类别的同时最小化扰动的方法。
- en: These methods can be used on any agent having whether their type of action space
    (discrete or continuous).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法可以应用于任何代理，无论其动作空间的类型（离散或连续）。
- en: $-$ JSMA [[35](#bib.bib35)] is another gradient attack. It is more computationally
    expensive than FGSM, since it is an iterative method that craft perturbation with
    several iteration of computation of a Jacobian matrix for each output. It applies
    perturbation pixel by pixel, this make it particularly suitable for $L_{0}$ bounded
    perturbations.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ JSMA [[35](#bib.bib35)] 是另一种梯度攻击。它比FGSM计算上更昂贵，因为它是一个迭代方法，需要对每个输出计算雅可比矩阵的多个迭代来制作扰动。它逐像素应用扰动，使其特别适合$L_{0}$有界扰动。
- en: $-$ XSMA [[36](#bib.bib36)], VFGA [[37](#bib.bib37)], are methods based on JSMA,
    improving its effectiveness.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ XSMA [[36](#bib.bib36)]、VFGA [[37](#bib.bib37)] 是基于JSMA的方法，改进了其有效性。
- en: Theses methods have been applied to RL in untargeted way [[27](#bib.bib27),
    [23](#bib.bib23)] in various type of environments, and in targeted way [[38](#bib.bib38)].
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法已在未针对性地应用于强化学习中 [[27](#bib.bib27), [23](#bib.bib23)]，适用于各种类型的环境，也在有针对性地应用于
    [[38](#bib.bib38)]。
- en: 'A representation of the integration and application of Gradient Attacks in
    crafting observation perturbations within an RL framework is shown in Figure [8](#S5.F8
    "Figure 8 ‣ First Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.1.1
    Deviate Policy ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣
    Robust Deep Reinforcement Learning Through Adversarial Attacks and Training :
    A Survey").'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '图[8](#S5.F8 "Figure 8 ‣ First Order Optimization Methods: Gradient Attacks
    (White Box) ‣ 5.1.1 Deviate Policy ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial
    Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")展示了在强化学习框架中利用梯度攻击进行观察扰动的整合和应用。'
- en: '![Refer to caption](img/f5e4440ef3b51db8c44bf06a3f43d881.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f5e4440ef3b51db8c44bf06a3f43d881.png)'
- en: 'Figure 8: Gradient Observation Attacks : The adversarial attack intercept the
    observation $x_{t}$, computes a perturbation $a^{\xi,X}_{t}$ by back-propagating
    the gradient of a loss in the neural network of a copy of the agent $\pi$, this
    perturbation is used to craft a perturbed observation $x^{\prime}_{t}$, which
    is sent to the agent'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：梯度观察攻击：对抗攻击拦截观察$x_{t}$，通过反向传播神经网络中损失的梯度计算扰动$a^{\xi,X}_{t}$，此扰动用于制作扰动观察$x^{\prime}_{t}$，并将其发送给代理。
- en: Zeroth Order Optimization Methods (Black Box)
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 零阶优化方法（黑箱）
- en: They represent an alternative category of direct adversarial attacks on observations.
    Unlike methods that rely on gradient computation, these attacks use optimization
    techniques that do not depend on gradient information to generate perturbations.
    Their primary objective is to alter the agent’s policy by producing a perturbed
    observation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 它们代表了一类直接对观察进行的对抗攻击。与依赖梯度计算的方法不同，这些攻击使用不依赖梯度信息的优化技术来生成扰动。其主要目标是通过生成扰动观察来改变代理的策略。
- en: These methods employ various search techniques, such as random search, meta-heuristic
    optimization, or methods for estimating the gradient without direct computation.
    They operate in a black box setting, where the attacker has access only to the
    inputs and outputs of the model, without any internal knowledge of the agent.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法采用各种搜索技术，如随机搜索、元启发式优化或无直接计算的梯度估计方法。它们在黑箱设置中操作，攻击者仅访问模型的输入和输出，而没有代理的任何内部知识。
- en: Square Attack [[39](#bib.bib39)] is one such method that performs a random search
    within an $\epsilon$-ball to discover adversarial examples. Although computationally
    demanding due to the number of iterations required for effective perturbation
    discovery, this method is applicable to any continuous observation space, including
    feature vectors, images, and signals, and to both discrete and continuous action
    spaces.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Square Attack [[39](#bib.bib39)] 是一种在$\epsilon$-球内进行随机搜索以发现对抗样本的方法。尽管由于需要进行有效扰动发现的迭代次数而计算上较为昂贵，但这种方法适用于任何连续观察空间，包括特征向量、图像和信号，以及离散和连续动作空间。
- en: Finite Difference [[40](#bib.bib40), [28](#bib.bib28)] offers a technique for
    Gradient Estimation through querying the agent’s model, bypassing the need for
    white-box access. This estimated gradient is then utilized to craft a perturbed
    observation. This approach necessitates querying the neural network $2\times N$
    times for an input of size N.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 有限差分 [[40](#bib.bib40), [28](#bib.bib28)] 提供了一种通过查询智能体模型进行梯度估计的技术，无需白盒访问。然后利用这个估计的梯度来制造扰动观察。此方法需要对大小为
    N 的输入查询神经网络 $2\times N$ 次。
- en: 'A representation of the integration and application of Zeroth Order Optimization
    Methods in crafting observation perturbations within an RL framework is shown
    in Figure [9](#S5.F9 "Figure 9 ‣ Zeroth Order Optimization Methods (Black Box)
    ‣ 5.1.1 Deviate Policy ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey").'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [9](#S5.F9 "图 9 ‣ 零阶优化方法（黑盒） ‣ 5.1.1 偏离策略 ‣ 5.1 观察变化攻击 ‣ 5 对抗攻击 ‣ 通过对抗攻击和训练的鲁棒深度强化学习：综述")
    展示了在 RL 框架中整合和应用零阶优化方法以制造观察扰动的表示。
- en: '![Refer to caption](img/575c97be67134e374b751210601ddd0d.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/575c97be67134e374b751210601ddd0d.png)'
- en: 'Figure 9: Zeroth Order Optimization Observation Attacks : The adversarial attack
    intercept the observation $x_{t}$, computes a perturbation $a^{\xi,X}_{t}$ by
    querying the neural network of the agent $\pi$ through an interface and applying
    a zeroth order optimization algorithm to maximize a loss, this perturbation is
    used to craft a perturbed observation $x^{\prime}_{t}$, which is sent to the agent.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：零阶优化观察攻击：对抗攻击拦截观察 $x_{t}$，通过接口查询智能体 $\pi$ 的神经网络，计算扰动 $a^{\xi,X}_{t}$ 并应用零阶优化算法以最大化损失，这个扰动用于制造扰动观察
    $x^{\prime}_{t}$，然后发送给智能体。
- en: 5.1.2 Minimize Reward
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 最小化奖励
- en: Several methods have been developed with the specific aim of producing perturbations
    in observations that directly minimize the reward obtained by an agent. These
    methods generate an adversarial action $a^{\xi}$ that induces perturbations $a^{\xi,X}$
    on the observation $x^{\prime}$, thereby replacing the original observation $x$.
    As a result, the policy $\pi$ yields a different output $\pi(x^{\prime})\neq\pi(x)$,
    leading to undesirable situations characterized by lower rewards.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 已开发出几种方法，专门旨在对观察结果施加扰动，从而直接最小化智能体获得的奖励。这些方法生成对观察 $x^{\prime}$ 施加扰动 $a^{\xi,X}$
    的对抗性动作 $a^{\xi}$，从而替代原始观察 $x$。结果，策略 $\pi$ 会产生不同的输出 $\pi(x^{\prime})\neq\pi(x)$，导致奖励降低的恶劣情况。
- en: 'Most of these methods fall under the category of Adversarial Policy Learning
    Based Methods. As pointed out by [[41](#bib.bib41)], learning an optimal adversary
    to perturb observations is equivalent to learning a optimal policy in a new POMDP
    from its point of view as described in [4.2.1](#S4.SS2.SSS1 "4.2.1 Alteration
    of the Observations Function 𝑂^Ω ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of
    Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey"). The effectiveness of these approaches is largely
    due to their ability to leverage the sequential nature of the environment and
    the anticipation of future rewards, which aids in the development of effective
    Adversarial Policies. In contrast, Direct Optimization Methods are generally not
    used for this purpose, as they often struggle to capture the sequential dynamics
    of the environment and the implications for future rewards.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些方法属于对抗策略学习基础的方法。正如 [[41](#bib.bib41)] 指出，学习一个最优的对手以扰动观察相当于从其视角学习一个新 POMDP
    中的最优策略，如 [4.2.1](#S4.SS2.SSS1 "4.2.1 观察函数 𝑂^Ω 的改变 ‣ 4.2 修改 POMDP 组件 ‣ 4 对抗攻击的分类
    ‣ 通过对抗攻击和训练的鲁棒深度强化学习：综述") 中所描述。这些方法的有效性主要归功于它们能够利用环境的序列特性和对未来奖励的预期，从而帮助开发有效的对抗策略。相比之下，直接优化方法通常不用于此目的，因为它们往往难以捕捉环境的序列动态及其对未来奖励的影响。
- en: Adversarial Policy Learning Based Methods involve training an adversarial agent
    that initially learns to generate perturbations and subsequently uses this knowledge
    to produce perturbations during inference. Typically, these methods employ an
    adversarial policy to generate perturbations $a^{\xi,X}$ from observations $x$,
    creating a perturbed observation $x^{\prime}$. These methods require a training
    phase prior to being deployed as an attack, making them computationally intensive
    initially. However, once trained, these policies can be directly applied to generate
    perturbations with a significantly reduced computational cost when used in an
    attack scenario. In scenarios where the agent and the adversary are trained concurrently,
    these methods resemble techniques used in Generative Adversarial Networks (GAN).
    In this setup, the agent learns to perform its task while also becoming robust
    to the perturbations generated by the adversary. Simultaneously, the adversary
    refines its ability to create more effective perturbations to hinder the agent’s
    task performance. This co-learning approach enhances the adaptability and effectiveness
    of both the agent and the adversarial policy.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗策略学习方法涉及训练一个对抗性代理，该代理最初学习生成扰动，然后利用这些知识在推理过程中产生扰动。这些方法通常使用对抗策略从观察值$x$生成扰动$a^{\xi,X}$，从而创建一个扰动后的观察值$x^{\prime}$。这些方法在作为攻击部署之前需要一个训练阶段，使得初期计算开销较大。然而，一旦训练完成，这些策略可以直接应用于生成扰动，计算成本在攻击场景中显著降低。在代理和对手同时训练的情况下，这些方法类似于生成对抗网络（GAN）中使用的技术。在这种设置下，代理不仅学习执行其任务，还变得对对手生成的扰动具有鲁棒性。同时，对手也在不断提升其生成更有效扰动的能力，以阻碍代理的任务表现。这种共同学习的方法提升了代理和对抗策略的适应性和有效性。
- en: Classical Adversarial Policies (Black Box)
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 经典对抗策略（黑箱）
- en: 'They are methods that employ an adversarial RL agent trained to create perturbations
    in observations. These methods function as black-box attacks, particularly during
    inference, and do not require comprehensive knowledge of the agent’s model to
    produce perturbations. Instead, their only requirement during the training phase
    is the ability to query the model for outputs based on various inputs. In the
    subsequent attack phase, the already-trained adversarial agent no longer needs
    additional information except for its own policy model parameters. To launch an
    attack, the agent simply performs a forward pass through its policy, generating
    a perturbation that results in a perturbed observation. The methods that follow
    this principle are Optimal Attack on Reinforcement Learning Policies OARLP [[42](#bib.bib42),
    [43](#bib.bib43)] and State Adversarial SA-MDP [[44](#bib.bib44)]. ATLA [[45](#bib.bib45)]
    also use this principle, but it is more focused on how to use it effectively in
    adversarial training, this is discussed further in Section [7.1](#S7.SS1 "7.1
    Robustness Strategies with Adversarial Training ‣ 7 Adversarial and Robust Training
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey"). The adversary can use the same observation as the agent or augment
    its input with additional data, such as the agent’s action based on the original
    observation. This approach is highly flexible, allowing application across various
    observation spaces, including tabular data, feature vectors, images, and signals,
    and suitable for both discrete and continuous action spaces. Being black-box in
    nature, these methods only require the output of the agent model for a given input
    and do not need further information from the agent model.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法使用经过训练的对抗性强化学习智能体来创建观察扰动。这些方法作为黑盒攻击，尤其在推理过程中，不需要全面了解智能体模型就可以产生扰动。相反，它们在训练阶段的唯一要求是能够根据各种输入查询模型的输出。在随后的攻击阶段，已经训练好的对抗性智能体不再需要额外的信息，除了其自身的策略模型参数。要发起攻击，智能体只需通过其策略进行前向传播，生成一个导致扰动观察的扰动。遵循这一原则的方法包括强化学习策略的最优攻击
    OARLP [[42](#bib.bib42), [43](#bib.bib43)] 和状态对抗 SA-MDP [[44](#bib.bib44)]。ATLA
    [[45](#bib.bib45)] 也使用这一原则，但更侧重于如何在对抗训练中有效使用它，这在第 [7.1](#S7.SS1 "7.1 通过对抗训练的鲁棒性策略
    ‣ 7 对抗性和鲁棒性训练 ‣ 通过对抗性攻击和训练实现鲁棒深度强化学习：一项调查") 节中进一步讨论。对抗者可以使用与智能体相同的观察，或通过附加数据来增强其输入，例如基于原始观察的智能体动作。这种方法具有很高的灵活性，适用于各种观察空间，包括表格数据、特征向量、图像和信号，适合离散和连续动作空间。由于本质上是黑盒的，这些方法仅需智能体模型在给定输入下的输出，而不需要来自智能体模型的进一步信息。
- en: 'A representation of the integration and application of Adversarial Policies
    in crafting observation perturbations within an RL framework is shown in Figure
    [10](#S5.F10 "Figure 10 ‣ Classical Adversarial Policies (Black Box) ‣ 5.1.2 Minimize
    Reward ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10](#S5.F10 "图 10 ‣ 经典对抗性策略（黑盒） ‣ 5.1.2 最小化奖励 ‣ 5.1 观察干扰攻击 ‣ 5 对抗性攻击 ‣ 通过对抗性攻击和训练实现鲁棒深度强化学习：一项调查")
    展示了在强化学习框架中，如何将对抗性策略应用于构建观察扰动的整合和应用。
- en: '![Refer to caption](img/bb2a964ae4fe648bbd174b8e11fdd502.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bb2a964ae4fe648bbd174b8e11fdd502.png)'
- en: 'Figure 10: Classical Adversarial Policy Observation Attacks : The adversarial
    policy intercept the observation $x_{t}$, computes a perturbation $a^{\xi,X}_{t}$
    by a forward pass in its neural network, this perturbation is used to craft a
    perturbed observation $x^{\prime}_{t}$, which is sent to the agent. The opposite
    reward as the agent is send to the adversarial policy to be trained.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：经典对抗性策略观察攻击：对抗性策略截取观察 $x_{t}$，通过其神经网络的前向传播计算扰动 $a^{\xi,X}_{t}$，该扰动用于构造扰动观察
    $x^{\prime}_{t}$，然后将其发送给智能体。与智能体相反的奖励被发送到对抗性策略进行训练。
- en: Augmented Adversarial Policies (White Box)
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增强的对抗性策略（白盒）
- en: Adversarial Policies can also be augmented with specific white-box techniques
    that can improve their performances to be more effective.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性策略也可以通过特定的白盒技术来增强，这些技术可以提高其性能，使其更有效。
- en: The Adversarial Transformer Network (ATN) method, as used and studied by [[46](#bib.bib46),
    [47](#bib.bib47)] shows that training an adversarial policy can also involve utilizing
    the gradients of the agent model. In this approach, the adversary is trained by
    back-propagating the agent’s loss through its input, which corresponds to the
    adversary’s output, and subsequently updating the adversary’s parameters. This
    technique effectively trains the adversary to generate perturbations that counteract
    the agent’s tendencies. During training, this method is considered white-box as
    it relies on the agent model’s gradients. However, at inference time, it functions
    as a black-box method since the trained policy alone is sufficient for operation.
    This methods are highly adaptable, applicable to any type of observation space,
    including tables, feature vectors, images, and signals, as well as to both discrete
    and continuous action spaces.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性变换网络（ATN）方法，如[[46](#bib.bib46), [47](#bib.bib47)]所示，表明训练对抗策略也可以利用智能体模型的梯度。在这种方法中，通过将智能体的损失通过其输入（即对抗者的输出）进行反向传播来训练对抗者，然后更新对抗者的参数。这种技术有效地训练对抗者生成对抗智能体倾向的扰动。在训练过程中，该方法被认为是白盒方法，因为它依赖于智能体模型的梯度。然而，在推理时，它作为黑盒方法运行，因为仅训练后的策略即可用于操作。这些方法高度适应性，适用于任何类型的观察空间，包括表格、特征向量、图像和信号，以及离散和连续动作空间。
- en: 'A representation of the integration and application of the Augmented Adversarial
    Policy ATN in crafting observation perturbations within an RL framework is shown
    in Figure [11](#S5.F11 "Figure 11 ‣ Augmented Adversarial Policies (White Box)
    ‣ 5.1.2 Minimize Reward ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey").'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '在RL框架中，增强对抗性策略ATN在制造观察扰动中的集成和应用的表示如图[11](#S5.F11 "Figure 11 ‣ Augmented Adversarial
    Policies (White Box) ‣ 5.1.2 Minimize Reward ‣ 5.1 Observation Alteration Attacks
    ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey")所示。'
- en: '![Refer to caption](img/ece355c7a48f140aff22ca4b8ec4b190.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ece355c7a48f140aff22ca4b8ec4b190.png)'
- en: 'Figure 11: Adversarial Transformer Network ATN Observation Attacks : The adversarial
    policy intercept the observation $x_{t}$, computes a perturbation $a^{\xi,X}_{t}$
    by a forward pass in its neural network, this perturbation is used to craft a
    perturbed observation $x^{\prime}_{t}$, which is sent to the agent. The opposite
    reward is sent to the agent, which back-propagate the loss to its input, then
    this loss is back-propagated in the adversarial policy network to be trained.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：对抗性变换网络ATN观察攻击：对抗策略拦截观察 $x_{t}$，通过在其神经网络中进行前向传播计算扰动 $a^{\xi,X}_{t}$，该扰动用于制造扰动观察
    $x^{\prime}_{t}$，并发送给智能体。相反的奖励被发送给智能体，智能体将损失反向传播到其输入，然后该损失在对抗策略网络中反向传播进行训练。
- en: 'Another augmented adversarial policy approach is PA-AD [[48](#bib.bib48)],
    this method craft an attack in two steps: First an RL based adversary, the Director
    adversary, is gives the direction of the perturbation wanted in the policy space,
    then this direction is given as target to the Actor adversary which is a direct
    Optimization method that compute the perturbation in the observation space to
    produce in order to make the agent choose the action wanted by the adversary.
    The director adversary in trained by RL by getting the opposite reward as the
    agent, thus improving its direction given to the actor adversary. The actor adversary
    cannot be learn, it only apply a direct optimization algorithm by optimizing the
    direction given by the actor. A representation of the integration and application
    of the Augmented Adversarial Policy PA-AD in crafting observation perturbations
    within an RL framework is shown in Figure [12](#S5.F12 "Figure 12 ‣ Augmented
    Adversarial Policies (White Box) ‣ 5.1.2 Minimize Reward ‣ 5.1 Observation Alteration
    Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey").'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种增强型对抗策略方法是 PA-AD [[48](#bib.bib48)]，该方法分两步进行攻击：首先，一个基于 RL 的对抗者，即导演对抗者，给出策略空间中所需的扰动方向，然后这个方向作为目标提供给演员对抗者，演员对抗者是一个直接优化方法，用于计算观察空间中的扰动，以使代理选择对抗者希望的动作。导演对抗者通过获取与代理相反的奖励进行
    RL 训练，从而改善其给演员对抗者的方向。演员对抗者无法学习，它只是通过优化演员给出的方向来应用直接优化算法。图 [12](#S5.F12 "Figure
    12 ‣ Augmented Adversarial Policies (White Box) ‣ 5.1.2 Minimize Reward ‣ 5.1
    Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") 展示了在 RL 框架中增强型对抗策略
    PA-AD 在制造观察扰动的集成和应用。'
- en: '![Refer to caption](img/4c4d21b91ead774994c0cfbe2226a92b.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4c4d21b91ead774994c0cfbe2226a92b.png)'
- en: 'Figure 12: PA-AD Observation Attacks : The adversarial policy intercept the
    observation $x_{t}$, the director adversary computes a direction in the policy
    space by forward pass in its neural network, the actor adversary computes a perturbation
    $a^{\xi,X}_{t}$ by direct optimization, this perturbation is used to craft a perturbed
    observation $x^{\prime}_{t}$, which is sent to the agent. The opposite reward
    as the agent is send to the director adversary to be trained.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：PA-AD 观察攻击：对抗策略拦截观察 $x_{t}$，导演对抗者通过在其神经网络中的前向传播计算策略空间中的一个方向，演员对抗者通过直接优化计算扰动
    $a^{\xi,X}_{t}$，该扰动用于制造一个扰动观察 $x^{\prime}_{t}$，然后将其发送给代理。与代理相反的奖励被发送给导演对抗者进行训练。
- en: 5.2 Dynamic Alteration
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 动态改变
- en: 'This section presents an analysis of Dynamics Alteration Attacks for RL agents,
    which are methods that alter the transition function of the POMDP, they are useful
    to simulate mismatch between the dynamics of a deployment environment compared
    to the dynamics of the training environment, and they can be used to apply during
    the training of the agent to improve its robustness to unpredictable changes in
    the dynamics of the environment. Their goal is to produce an alteration of the
    transition function by producing a perturbation $a^{\xi}$ at a certain state $t$
    with the current state being $s_{t}$. This perturbation $a^{\xi}$ applied to any
    element of the transition function will have the consequence to lead to an alternative
    next state $\widetilde{s}_{t+1}$, which is different than the original next $s_{t+1}$
    that would have been produced without alteration. To achieve this goal the attack
    can either :'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了针对 RL 代理的动态改变攻击的分析，这些方法会改变 POMDP 的转移函数，它们对于模拟部署环境与训练环境的动态不匹配非常有用，并且可以在代理的训练过程中应用，以提高其对环境动态不可预测变化的鲁棒性。它们的目标是通过在某个状态
    $t$ 产生扰动 $a^{\xi}$ 来改变转移函数，其中当前状态为 $s_{t}$。这种扰动 $a^{\xi}$ 应用于转移函数的任何元素，将导致一个不同于原始下一个状态
    $s_{t+1}$ 的替代下一个状态 $\widetilde{s}_{t+1}$。为了实现这个目标，攻击可以：
- en: $-$ compute a perturbation $a^{\xi,S}$ to craft a perturbed state $\widetilde{s}_{t}=s_{t}+a^{\xi,S}$.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 计算一个扰动 $a^{\xi,S}$ 以制造一个扰动状态 $\widetilde{s}_{t}=s_{t}+a^{\xi,S}$。
- en: $-$ compute a perturbation $a^{\xi,A}$ to craft a perturbed action $a^{\prime}_{t}=a+a^{\xi,A}$.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 计算一个扰动 $a^{\xi,A}$ 以制造一个扰动动作 $a^{\prime}_{t}=a+a^{\xi,A}$。
- en: $-$ compute a perturbation $a^{\xi,T}$ to directly alter the transition function
    $T^{\Omega}$ to induce alternative next state $T^{\Omega}_{\xi}(\widetilde{s}_{t+1}|...,a^{\xi,T}$).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 计算一个扰动 $a^{\xi,T}$ 以直接改变转移函数 $T^{\Omega}$，从而引发替代的下一状态 $T^{\Omega}_{\xi}(\widetilde{s}_{t+1}|...,a^{\xi,T}$)。
- en: $-$ compute a perturbation $a^{\xi,S+}$ to craft a perturbed next state $\widetilde{s}_{t+1}=s_{t+1}+a^{\xi,S+}$.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: $-$ 计算一个扰动 $a^{\xi,S+}$ 以构造一个扰动后的下一状态 $\widetilde{s}_{t+1}=s_{t+1}+a^{\xi,S+}$。
- en: 'As previously shown in Figures [5](#S4.F5 "Figure 5 ‣ Current State Perturbation
    ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2
    Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") [7](#S4.F7
    "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition Function
    𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey") [4](#S4.F4 "Figure 4 ‣ Transition Perturbation ‣ 4.2.2
    Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered
    POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") [6](#S4.F6 "Figure
    6 ‣ Next State Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω
    (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey") in Section [4.2.2](#S4.SS2.SSS2 "4.2.2 Alteration of
    the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") All these methods have the
    consequence to lead to an alternative next state $\widetilde{s}_{t+1}$. Theoretically,
    the amount of perturbation produced by such attacks should be measured as a distance
    between the original next state that would have been produced without alteration
    and the alternative next state given a norm $L_{p}$, $||s_{t+1}-\widetilde{s}_{t+1}||_{p}$.
    But since it is often hard to redo several times the transition function of an
    environment, in practice often only the difference to the directly perturbed element
    is measured : either $||s_{t}-\widetilde{s}_{t}||_{p}$, $||a_{t}-a^{\prime}_{t}||_{p}$,
    $||a^{\xi,T}||_{p}$, or $||\widetilde{s}_{t+1}-s_{t+1}||_{p}$ given the type of
    attack, thus making hard to compare disturbance magnitude between two different
    type of attacks that does not perturb the same element.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如在图 [5](#S4.F5 "Figure 5 ‣ 当前状态扰动 ‣ 4.2.2 转移函数 𝑇^Ω (环境动态) 的改变 ‣ 4.2 改变的 POMDP
    组件 ‣ 4 强敌对攻击的分类 ‣ 通过敌对攻击和训练实现鲁棒深度强化学习：综述") [7](#S4.F7 "Figure 7 ‣ 行动扰动 ‣ 4.2.2
    转移函数 𝑇^Ω (环境动态) 的改变 ‣ 4.2 改变的 POMDP 组件 ‣ 4 强敌对攻击的分类 ‣ 通过敌对攻击和训练实现鲁棒深度强化学习：综述")
    [4](#S4.F4 "Figure 4 ‣ 转移扰动 ‣ 4.2.2 转移函数 𝑇^Ω (环境动态) 的改变 ‣ 4.2 改变的 POMDP 组件 ‣ 4
    强敌对攻击的分类 ‣ 通过敌对攻击和训练实现鲁棒深度强化学习：综述") [6](#S4.F6 "Figure 6 ‣ 下一状态扰动 ‣ 4.2.2 转移函数
    𝑇^Ω (环境动态) 的改变 ‣ 4.2 改变的 POMDP 组件 ‣ 4 强敌对攻击的分类 ‣ 通过敌对攻击和训练实现鲁棒深度强化学习：综述") 在第 [4.2.2](#S4.SS2.SSS2
    "4.2.2 转移函数 𝑇^Ω (环境动态) 的改变 ‣ 4.2 改变的 POMDP 组件 ‣ 4 强敌对攻击的分类 ‣ 通过敌对攻击和训练实现鲁棒深度强化学习：综述")
    节中已展示。所有这些方法都会导致替代的下一状态 $\widetilde{s}_{t+1}$。理论上，这些攻击所产生的扰动量应当以原始下一状态与未改变状态的距离来度量，给定一个范数
    $L_{p}$，即 $||s_{t+1}-\widetilde{s}_{t+1}||_{p}$。但由于重新计算环境的转移函数通常很困难，实际上往往只测量直接扰动元素的差异：无论是
    $||s_{t}-\widetilde{s}_{t}||_{p}$、$||a_{t}-a^{\prime}_{t}||_{p}$、$||a^{\xi,T}||_{p}$，还是
    $||\widetilde{s}_{t+1}-s_{t+1}||_{p}$，具体取决于攻击类型，从而使得在不同类型攻击之间比较扰动幅度变得困难，因为这些攻击扰动的元素可能不同。
- en: 'Tampering with the transition is a completely different approach than with
    the observation. The methods developed in this section assume that the environments
    simulate physical, real-life settings : the perturbations are more restricted,
    ruling out gradient-based methods, and their effects on the agent is now indirect.
    In this section we first discuss methods that are designed to minimize the rewards
    obtained by the agent by altering the transition, then we discuss methods that
    are designed to deviate the policy of the agent.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 修改过渡与修改观察是完全不同的方法。本节中开发的方法假设环境模拟物理、现实生活中的设置：扰动更加受限，排除了基于梯度的方法，其对代理的影响现在是间接的。在本节中，我们首先讨论通过改变过渡来最小化代理获得的奖励的方法，然后讨论旨在偏离代理策略的方法。
- en: 5.2.1 Minimize Reward
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 最小化奖励
- en: Certain methods are specifically designed to modify the dynamics of an environment,
    aiming to reduce the rewards an agent receives. These methods achieve this by
    generating an adversarial action $a^{\xi}$, which leads to an altered subsequent
    state $\widetilde{s}_{t+1}$, diverging from the original next state $s_{t+1}$.
    As a result, the agent finds itself in an unfavorable situation $\widetilde{s}_{t+1}$
    at the next step, potentially leading to reduced immediate or future rewards.
    Most of these methods fall under the category of Adversarial Policy Learning Based
    Methods. This is because they are well-suited to exploit the sequential nature
    of the environment and the anticipation of future rewards, which aids in developing
    effective Adversarial Policies.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 某些方法专门设计用于修改环境的动态，旨在减少代理所获得的奖励。这些方法通过生成对抗性动作 $a^{\xi}$ 来实现这一点，该动作导致一个被改变的后续状态
    $\widetilde{s}_{t+1}$，与原始的下一个状态 $s_{t+1}$ 偏离。因此，代理在下一个步骤中发现自己处于一个不利的情况 $\widetilde{s}_{t+1}$，可能会导致即时或未来奖励的减少。这些方法大多数属于对抗性策略学习方法，因为它们非常适合利用环境的序列性质和对未来奖励的预期，从而有助于开发有效的对抗性策略。
- en: Adversarial Policies involve methods where an adversarial agent is trained to
    create perturbations. Initially, this agent learns how to generate these perturbations,
    and once trained, it can efficiently produce them during inference. Typically,
    these methods utilize an adversarial policy to create an adversarial action $a^{\xi}$,
    aimed at altering the environment’s transitions. The training of these methods
    must be completed before they are deployed for attacks, which makes the initial
    phase computationally demanding. However, once the training phase is over, the
    adversarial policy can be used directly to generate perturbations at a significantly
    reduced computational cost in attack scenarios. In such a setup, the primary agent
    learns to perform its task while also becoming resilient to the adversary’s perturbations.
    Concurrently, the adversarial agent refines its skills in creating more effective
    perturbations to hinder the primary agent’s task performance.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性策略涉及一种方法，其中一个对抗性代理被训练以生成扰动。最初，这个代理学习如何生成这些扰动，一旦训练完成，它可以在推理过程中有效地生成这些扰动。通常，这些方法利用对抗性策略来创建对抗性动作
    $a^{\xi}$，旨在改变环境的过渡。这些方法的训练必须在部署攻击之前完成，这使得初期阶段计算上要求很高。然而，一旦训练阶段结束，对抗性策略可以直接用于在攻击场景中以显著降低的计算成本生成扰动。在这种设置下，主要代理学习执行其任务，同时也变得对对抗者的扰动具有一定的抗性。同时，对抗性代理则不断提高其生成更有效扰动的技能，以妨碍主要代理的任务执行。
- en: Classical Adversarial Policies (Black Box)
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 经典对抗性策略（黑箱）
- en: 'They utilize an adversarial RL agent trained to create adversarial actions
    that modify transitions within the environment. Functioning primarily as black-box
    attacks, especially during the inference phase, these methods do not necessitate
    comprehensive understanding of the agent’s model to produce perturbations. Instead,
    their main requirement during the training phase is the ability to query the model
    and obtain its outputs for various inputs. Once the adversarial agent completes
    its training, no additional information is needed beyond the parameters of its
    own policy model. During the attack phase, generating a perturbation involves
    simply executing a forward pass through the adversarial policy to create a perturbed
    observation. Examples of methods adhering to this approach include Robust Adversarial
    Reinforcement Learning (RARL) [[49](#bib.bib49)], along with its advancements
    such as Risk Averse (RA-RARL) [[50](#bib.bib50)], and Semi-Competitive (SC-RARL)
    [[51](#bib.bib51)]. These methods exemplify the application of Adversarial Policies
    in crafting effective black-box attacks in RL contexts. FSP [[52](#bib.bib52),
    [53](#bib.bib53)] also use this approach, but its points is more focused on how
    to use it effectively in adversarial training, this is discussed further in Section
    [7.1](#S7.SS1 "7.1 Robustness Strategies with Adversarial Training ‣ 7 Adversarial
    and Robust Training ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey"). These methods have been introduced as adding an adversarial
    action to an augmented version of the transition function $T_{\xi}^{\Omega}$,
    a representation of the integration and usage of Adversarial Policies Attacks
    to add transition perturbations in an RL context is shown in Figure [13](#S5.F13
    "Figure 13 ‣ Classical Adversarial Policies (Black Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey"). The perturbation is added
    in the environment as previously shown in Figure [4](#S4.F4 "Figure 4 ‣ Transition
    Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics)
    ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").
    These methods can also be used to generate perturbations on the state $s_{t}$,
    the next state $s_{t+1}$, and also the action $a_{t}$ as previously shown in Figures
    [5](#S4.F5 "Figure 5 ‣ Current State Perturbation ‣ 4.2.2 Alteration of the Transition
    Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy
    of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") [6](#S4.F6 "Figure 6 ‣ Next State Perturbation
    ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2
    Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") [7](#S4.F7
    "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition Function
    𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey") in Section [4.2.2](#S4.SS2.SSS2 "4.2.2 Alteration of
    the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey"). The Probabilistic Action
    Robust PR-MDP method [[54](#bib.bib54)], follows the same principle as the other
    adversarial policy methods, the goal is to train an adversarial policy to generate
    perturbation, but specifically on the action space as previously shown in Figure
    [7](#S4.F7 "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition
    Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy
    of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey"). These methods are very versatile, they can
    be applied to any observation space (table, features vector, images, signals)
    and action space (discrete or continuous). When adding disturbances in the transition
    function directly or in the states there is the constraint of having activatable
    lever in the environment to be used by the adversarial policy add disturbances,
    this constraint is not prevent when disturbing the action since the action itself
    is already the lever, but the attacks perturbing the action may have less means
    for disturbing the dynamics than methods that perturb state or transition. A representation
    of the integration and usage of Adversarial Policies Attacks to add transition
    perturbations in an RL context is shown in Figure [13](#S5.F13 "Figure 13 ‣ Classical
    Adversarial Policies (Black Box) ‣ 5.2.1 Minimize Reward ‣ 5.2 Dynamic Alteration
    ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey").'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '他们利用一种对抗性强化学习（RL）代理，该代理经过训练以生成对抗性动作，从而修改环境中的过渡。主要作为黑箱攻击运作，尤其是在推理阶段，这些方法不需要对代理的模型进行全面理解就可以产生扰动。相反，在训练阶段，它们的主要需求是能够查询模型并获取各种输入的输出。一旦对抗性代理完成训练，除了自身策略模型的参数外，不需要额外的信息。在攻击阶段，生成扰动只需通过对抗策略执行一次前向传递，以创建一个扰动的观察。遵循这种方法的示例包括鲁棒对抗强化学习（RARL）[[49](#bib.bib49)]，以及其进展，如风险厌恶（RA-RARL）[[50](#bib.bib50)]，和半竞争（SC-RARL）[[51](#bib.bib51)]。这些方法展示了在强化学习上下文中应用对抗策略进行有效黑箱攻击的实例。FSP
    [[52](#bib.bib52), [53](#bib.bib53)] 也使用这种方法，但其重点更在于如何在对抗训练中有效使用，详细讨论见第[7.1](#S7.SS1
    "7.1 Robustness Strategies with Adversarial Training ‣ 7 Adversarial and Robust
    Training ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and
    Training : A Survey")节。这些方法被引入为向扩展版的过渡函数 $T_{\xi}^{\Omega}$ 添加对抗性动作，在强化学习上下文中添加过渡扰动的表示见图[13](#S5.F13
    "Figure 13 ‣ Classical Adversarial Policies (Black Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey")。扰动如图[4](#S4.F4 "Figure 4
    ‣ Transition Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment
    Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of
    DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")所示，已添加到环境中。这些方法还可以用来生成对状态 $s_{t}$、下一个状态 $s_{t+1}$ 和动作 $a_{t}$ 的扰动，如图[5](#S4.F5
    "Figure 5 ‣ Current State Perturbation ‣ 4.2.2 Alteration of the Transition Function
    𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey") [6](#S4.F6 "Figure 6 ‣ Next State Perturbation ‣ 4.2.2
    Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered
    POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") [7](#S4.F7 "Figure
    7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment
    Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of
    DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")第4.2.2节所示。这些方法遵循与其他对抗策略方法相同的原则，目标是训练一种对抗性策略以生成扰动，但特别针对动作空间，如图[7](#S4.F7
    "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition Function
    𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey")所示。这些方法非常灵活，可以应用于任何观察空间（表格、特征向量、图像、信号）和动作空间（离散或连续）。直接在过渡函数或状态中添加扰动时，需要环境中有可激活的杠杆供对抗性策略添加扰动，这种约束在扰动动作时不会存在，因为动作本身已经是杠杆，但扰动动作的攻击可能在扰动动态方面的手段较少，较之于扰动状态或过渡的方法。图[13](#S5.F13
    "Figure 13 ‣ Classical Adversarial Policies (Black Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey")展示了在强化学习上下文中整合和使用对抗策略攻击以添加过渡扰动的表示。'
- en: '![Refer to caption](img/841d32f5dd14ed7e486382e7f53d7e3f.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/841d32f5dd14ed7e486382e7f53d7e3f.png)'
- en: 'Figure 13: Classical Adversarial Policy Dynamic Attack : The agent and the
    adversary get an observation $x_{t}$ of the environment. The agent chooses the
    action $a_{t}$ to apply and the adversary chooses the perturbation $a^{\xi}_{t}$
    to apply to alter the dynamics. The step function of the environment is run incorporating
    both agent action $a_{t}$ and adversarial action $a^{\xi}_{t}$. The reward opposite
    to that of the agent is sent to the adversarial policy to be trained.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：经典对抗策略动态攻击：代理和对手获得环境的观察值$x_{t}$。代理选择应用的动作$a_{t}$，对手选择施加的扰动$a^{\xi}_{t}$来改变动态。环境的步进函数运行时考虑了代理动作$a_{t}$和对抗动作$a^{\xi}_{t}$。与代理相反的奖励被发送到对抗策略进行训练。
- en: Other works like APDRL [[55](#bib.bib55)], A-MCTS [[56](#bib.bib56)], APT [[57](#bib.bib57)]
    and ICMCTS-BR [[58](#bib.bib58)] have used the concept of adversarial policy or
    adversarial agent, but more in the context of a real two players game, where an
    agent learns to do a task and an adversarial agent learns to make the agent fail.
    The key difference with methods previously discussed like RARL, is that here the
    environment itself is a two players game. The agent and the adversary are always
    here, contrary to methods previously discussed like RARL where the original setup
    is an agent alone learning to do a task and the adversary comes to challenge the
    agent and make it improve its performances for itself. So the methods presented
    in these works are very similar to RARL with some more specificities, and even
    if there were not presented in the context of the robustness for a single agent
    they can also be adapted and used in this setup.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究如APDRL [[55](#bib.bib55)]、A-MCTS [[56](#bib.bib56)]、APT [[57](#bib.bib57)]和ICMCTS-BR
    [[58](#bib.bib58)]已经使用了对抗策略或对抗代理的概念，但更多地是在真实的双人游戏的背景下，其中一个代理学习执行任务，另一个对抗代理学习使代理失败。与之前讨论的方法如RARL的关键区别在于这里的环境本身是一个双人游戏。代理和对手总是存在，这与之前讨论的方法如RARL中原始设置为一个代理单独学习执行任务，对手来挑战代理并使其提升性能有所不同。因此，这些研究中呈现的方法与RARL非常相似，具有一些更具体的特点，即使它们没有在单个代理的鲁棒性背景下呈现，它们也可以在此设置中适应和使用。
- en: Augmented Adversarial Policies (White Box)
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增强对抗策略（白盒）
- en: 'Another possibility is to augment the information observed by the adversary
    with internal state of the agent like latent spaces or others. This is done by
    White-Box Adversarial Policy WB-AP [[59](#bib.bib59)] which is very similar to
    RARL except that the adversary has white box access to the agent internal data.
    This enables to improve the attack effectiveness of the perturbations since the
    adversary can learn to adapt the perturbations to the internal states of the agent
    that is attacked. This method is white box since it requires access to the internal
    state of the agent. It can be applied on any observation, and action spaces. A
    representation of the integration and usage of Adversarial Policies Attacks to
    add transition perturbations in an RL context is shown in Figure [14](#S5.F14
    "Figure 14 ‣ Augmented Adversarial Policies (White Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey")'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种可能性是通过代理的内部状态（如潜在空间等）来增强对手观察到的信息。这由White-Box Adversarial Policy WB-AP [[59](#bib.bib59)]完成，该方法与RARL非常相似，只是对手可以白盒访问代理的内部数据。这提高了扰动的攻击效果，因为对手可以学习将扰动适应于被攻击代理的内部状态。该方法是白盒的，因为它需要访问代理的内部状态。它可以应用于任何观察和动作空间。图[14](#S5.F14
    "Figure 14 ‣ Augmented Adversarial Policies (White Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey")展示了在RL背景下添加转移扰动的对抗策略攻击的集成和使用。'
- en: '![Refer to caption](img/60fc18b0783aeb4245d3d634a0cb5837.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/60fc18b0783aeb4245d3d634a0cb5837.png)'
- en: 'Figure 14: White Box Adversarial Policy Dynamic Attack : The agent get an observation
    $x_{t}$ of the environment and chooses the action $a_{t}$ to apply. The adversary
    get the observation and some white box internal state of the agent and chooses
    the perturbation $a^{\xi}_{t}$ to apply to alter the dynamics. The step function
    of the environment is run incorporating both agent action $a_{t}$ and adversarial
    action $a^{\xi}_{t}$. The reward opposite to that of the agent is sent to the
    adversarial policy to be trained.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：白盒对抗策略动态攻击：智能体获得环境观察 $x_{t}$ 并选择要应用的动作 $a_{t}$。对手获取观察和智能体的一些白盒内部状态，并选择扰动
    $a^{\xi}_{t}$ 以改变动态。环境的步进函数运行时结合了智能体的动作 $a_{t}$ 和对抗动作 $a^{\xi}_{t}$。与智能体相对的奖励被发送到对抗策略进行训练。
- en: Direct Optimization Methods
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 直接优化方法
- en: They use direct optimization to generate adversarial action $a^{\xi}$ to alter
    the transition of the environment. These methods on the contrary to Adversarial
    Policies does not require a prior training before usage in attacks. Although,
    they can be more computation needing when used in attacks since they have to solve
    an optimization problem at each perturbation rather than doing a forward pass
    in a neural network of an adversary policy.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法使用直接优化生成对环境转换进行干扰的对抗性动作 $a^{\xi}$。与对抗策略不同，这些方法在攻击中不需要事先训练。尽管如此，它们在攻击中可能更需要计算，因为每次扰动时都需要解决一个优化问题，而不是在对抗策略的神经网络中进行前向传递。
- en: 'First Order Optimization Methods: Gradient Attacks (White Box)'
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一阶优化方法：梯度攻击（白盒）
- en: 'MAS, LAS [[60](#bib.bib60), [61](#bib.bib61)] are methods that alter the action
    $a$ of the agent with a perturbation $a^{\xi,A}$ to make a perturbed action $a^{\prime}=a+a^{\xi,A}$
    that is less effective that the original action $a$. This method work only on
    RL agent that works with an actor network that produce the action and a Q-critic
    network that estimate the Q-value of the observation-action tuple. The method
    is to apply a gradient attack on the Q-Critic network by computing the gradient
    of the Q-value with respect to the action $a^{\xi,A}=\epsilon\nabla_{a}Q(x_{t},a_{t})$
    to minimize the Q-value estimated by perturbing the action. LAS is an extension
    of the MAS method which computes perturbation to apply over a sequence of future
    states to improve the long term impact of the attacks. This requires specific
    conditions to be applied such as an environment with repayable sequences, because
    the agent must be able to be reset at the specific state after the next pre-computed
    steps used to craft the perturbation. The scheme of the integration and usage
    of MAS attack in an RL context is shown in Figure [15](#S5.F15 "Figure 15 ‣ First
    Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey").'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 'MAS、LAS [[60](#bib.bib60), [61](#bib.bib61)] 是通过施加扰动 $a^{\xi,A}$ 修改智能体的动作 $a$，使扰动后的动作
    $a^{\prime}=a+a^{\xi,A}$ 不如原始动作 $a$ 有效。这种方法仅适用于具有执行动作的 actor 网络和评估观察-动作元组的 Q-critic
    网络的 RL 智能体。该方法通过计算相对于动作 $a^{\xi,A}=\epsilon\nabla_{a}Q(x_{t},a_{t})$ 的 Q-值梯度，对
    Q-Critic 网络进行梯度攻击，以通过扰动动作来最小化 Q-值。LAS 是 MAS 方法的扩展，通过计算扰动在未来状态序列中应用，以改善攻击的长期影响。这需要特定的条件，例如有可回报序列的环境，因为智能体必须能够在使用预计算步骤来设计扰动后的特定状态下重置。MAS
    攻击在 RL 上下文中的集成和使用方案见图 [15](#S5.F15 "Figure 15 ‣ First Order Optimization Methods:
    Gradient Attacks (White Box) ‣ 5.2.1 Minimize Reward ‣ 5.2 Dynamic Alteration
    ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey")。'
- en: '![Refer to caption](img/1d859325ddf400eb7e59527156e53fff.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1d859325ddf400eb7e59527156e53fff.png)'
- en: 'Figure 15: MAS Dynamic Attacks on the Action: The agent and the adversarial
    attack get the observation $x_{t}$ of the environment. The agent chooses the action
    $a_{t}$ to apply, the adversarial attack computes the gradient of a copy of the
    Q-Critic Network of the agent with respect to the action $a_{t}$, this gradient
    is used as disturbance $a^{\xi,A}_{t}$ to add in the action $a^{\prime}_{t}=a_{t}+a^{\xi,A}_{t}$.
    The step function of the environment is ran starting from the perturbed state
    $\widetilde{s}_{t}$, with the agent action $a_{t}$.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：MAS 动态攻击于动作：代理和对抗攻击获取环境的观察 $x_{t}$。代理选择要应用的动作 $a_{t}$，对抗攻击计算代理 Q-Critic
    网络副本相对于动作 $a_{t}$ 的梯度，这个梯度作为扰动 $a^{\xi,A}_{t}$ 添加到动作 $a^{\prime}_{t}=a_{t}+a^{\xi,A}_{t}$
    中。环境的步进函数从扰动状态 $\widetilde{s}_{t}$ 开始运行，使用代理的动作 $a_{t}$。
- en: Environment Attack based on the value-Critic Network EACN [[62](#bib.bib62)]
    is a method that apply a gradient attack to compute a perturbed observation $x^{\prime}$
    and then use it to craft a perturbed state $\widetilde{s}$. The general idea is
    to use the knowledge of the critic network’s gradient to progressively increase
    the complexity of the task during training. At step $t$ with state $s$ and observation
    $x=O(s)$, EACN computes the gradient of the input of the Value-Critic Network
    $V$ to minimize the output value. The value-Critic Network evaluates the Value
    function of the environment given the policy of the agent. So the method generates
    a perturbed input $x^{\prime}$ with $V(x^{\prime})<V(x)$, this perturbed input
    is then used to create a perturbed state $\widetilde{s}$ with the property $x^{\prime}=O(\widetilde{s})$.
    Then the value estimated by the agent of the state is less than what it would
    have been without perturbation $V(O(\widetilde{s}))<V(O(s))$. This attack can
    either be applied before or after the transition function. The method requires
    a Value-Critic Network as in the PPO algorithm, but any other RL approaches can
    be considered by just adding the training of a Value-Critic Network on the resulting
    policy.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 基于值-批评网络 EACN 的环境攻击 [[62](#bib.bib62)] 是一种应用梯度攻击来计算扰动观察 $x^{\prime}$，然后用其构造扰动状态
    $\widetilde{s}$ 的方法。一般思路是利用批评网络的梯度知识来逐步增加训练过程中的任务复杂性。在步骤 $t$，状态 $s$ 和观察 $x=O(s)$
    下，EACN 计算值-批评网络 $V$ 输入的梯度，以最小化输出值。值-批评网络评估给定代理策略的环境的值函数。因此，该方法生成一个扰动输入 $x^{\prime}$，使得
    $V(x^{\prime})<V(x)$，然后用这个扰动输入创建一个扰动状态 $\widetilde{s}$，使得 $x^{\prime}=O(\widetilde{s})$。然后代理对状态的估计值低于没有扰动时的值
    $V(O(\widetilde{s}))<V(O(s))$。该攻击可以在过渡函数之前或之后应用。该方法要求使用 PPO 算法中的值-批评网络，但可以通过在结果策略上添加值-批评网络的训练来考虑任何其他
    RL 方法。
- en: 'The main advantage of EACN is that is avoid training an adversarial policy.
    But it is less versatile than an adversarial policy since it needs one-to-one
    correspondence between the observations and the disturbances available, since
    the disturbance are computed from gradient on the observations. So most of the
    time EACN is applicable only on environments where the observation space is a
    feature space, with modifiable features. The method works with an agent with any
    action space (discrete or continuous). A scheme of the integration and usage of
    EACN Gradient method on dynamic attacks in an RL context is shown in Figure [16](#S5.F16
    "Figure 16 ‣ First Order Optimization Methods: Gradient Attacks (White Box) ‣
    5.2.1 Minimize Reward ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 'EACN 的主要优点在于避免训练对抗策略。但它的适用性不如对抗策略广泛，因为它需要观察和可用扰动之间的一一对应关系，扰动是通过观察的梯度计算得到的。因此，EACN
    大多数情况下只适用于观察空间是特征空间且特征可修改的环境。该方法适用于具有任何动作空间（离散或连续）的代理。图 [16](#S5.F16 "Figure 16
    ‣ First Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.2.1 Minimize
    Reward ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") 显示了 EACN 梯度方法在
    RL 环境中的动态攻击的集成和使用方案。'
- en: '![Refer to caption](img/da4c9ef3af3d84703b79cb075c043351.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da4c9ef3af3d84703b79cb075c043351.png)'
- en: 'Figure 16: Environment Attack based on the Critic Network EACN Dynamic Attacks
    : The agent and the adversarial attack get the observation $x_{t}$ of the environment.
    The agent chooses the action $a_{t}$ to apply and while the adversarial attack
    computes the gradient of a loss in a copy of the Value-Critic Network of the agent,
    this gradient is used as disturbance $a^{\xi,S}_{t}$ to add in the state $\widetilde{s}_{t}=s_{t}+a^{\xi,S}_{t}$.
    The step function of the environment is ran starting from the perturbed state
    $\widetilde{s}_{t}$, with the agent action $a_{t}$.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：基于评论网络EACN的环境攻击动态攻击：代理和对抗攻击获取环境的观察$x_{t}$。代理选择要应用的动作$a_{t}$，同时对抗攻击在代理的价值-评论网络的副本中计算损失的梯度，这个梯度作为扰动$a^{\xi,S}_{t}$被添加到状态$\widetilde{s}_{t}=s_{t}+a^{\xi,S}_{t}$中。环境的步进函数从扰动状态$\widetilde{s}_{t}$开始运行，使用代理动作$a_{t}$。
- en: Some other approaches have been proposed like to generate perturbations in gridworld
    environments. These methods are WBA [[63](#bib.bib63)] which works by analyzing
    the Q table, and CDG [[64](#bib.bib64)] which works by analyzing the gradient
    of the Q network on the grid. But these methods add new obstacles in the grid,
    then changing the states-set. But our definition of perturbation of environments
    relies on dynamic alterations, by perturbing the transition function which is
    not the case for these methods.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他方法被提出，例如在网格世界环境中生成扰动。这些方法包括WBA [[63](#bib.bib63)]，它通过分析Q表来工作，和CDG [[64](#bib.bib64)]，它通过分析网格上Q网络的梯度来工作。但这些方法在网格中增加了新的障碍，从而改变了状态集。而我们对环境扰动的定义依赖于动态变化，通过扰动转移函数，而这些方法并非如此。
- en: 5.2.2 Deviate Policy
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 偏离策略
- en: Certain methods are designed to alter the transitions and to produce perturbation
    optimized to deviate the policy of the agent. By generating an adversarial action
    $a^{\xi}$ that will induce an alternative next state $\widetilde{s}_{t+1}$ to
    replace the original next state $s_{t+1}$. The agent will be placed at the next
    step in a situation $\widetilde{s}_{t+1}$ where it should change the action that
    it would initially have chosen.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 某些方法旨在改变转移，并产生优化的扰动以偏离代理的策略。通过生成对抗性动作$a^{\xi}$，这将诱导一个替代的下一状态$\widetilde{s}_{t+1}$来替代原始的下一状态$s_{t+1}$。代理将在下一步被置于一个应该改变它最初选择的动作的状态$\widetilde{s}_{t+1}$中。
- en: 'First Order Optimization Methods: Gradient Attacks (White Box)'
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一阶优化方法：梯度攻击（白盒）
- en: 'Environment Attack based on the Actor Network EAAN [[62](#bib.bib62)]. At step
    $t$ with state $s$ and observation $x=O(s)$, EAAN is used to compute the gradient
    of a loss on the input of the model of the agent to generate a perturbed input
    $x^{\prime}$ with $\pi(x^{\prime})\neq\pi(x)$, this perturbed input is then used
    to create a perturbed state $\widetilde{s}$ with the property $x^{\prime}=O(\widetilde{s})$.
    Then, the action chosen by the agent at this step is different than what it would
    have been without perturbation $\pi(O(\widetilde{s}))\neq\pi(O(s))$. This attack
    can either be applied before or after the transition function. EACN, this method
    is a bit less versatile than with an adversarial policy, since in addition to
    have activatable lever in the environment to be used to add disturbances, these
    lever need to have one-to-one correspondence with the observations used as inputs
    of the agent. So most of the time EAAN is applicable only on environments where
    the observation space is a feature space, with modifiable features. The method
    works with any action space (discrete or continuous). The scheme of the integration
    and usage of EAAN Gradient method on dynamic attacks in an RL context is shown
    exactly the same as for EACN which is shown in Figure [16](#S5.F16 "Figure 16
    ‣ First Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.2.1 Minimize
    Reward ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey"), the only difference
    is that for EAAN the gradient is computed on the actor network $\pi$, rather than
    on the value-critic network $V$.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Actor 网络的环境攻击 EAAN [[62](#bib.bib62)]。在步骤 $t$ 中，状态 $s$ 和观察 $x=O(s)$，EAAN
    用于计算模型输入的损失梯度，以生成扰动输入 $x^{\prime}$，使得 $\pi(x^{\prime})\neq\pi(x)$，然后使用这个扰动输入创建具有属性
    $x^{\prime}=O(\widetilde{s})$ 的扰动状态 $\widetilde{s}$。然后，代理在这个步骤中选择的动作与没有扰动时的选择不同，即
    $\pi(O(\widetilde{s}))\neq\pi(O(s))$。这种攻击可以在过渡函数之前或之后应用。EACN，这种方法的灵活性略逊于对抗策略，因为除了在环境中有可激活的杠杆以添加扰动外，这些杠杆还需要与作为代理输入的观察之间具有一对一的对应关系。因此，EAAN
    大多数情况下仅适用于观察空间是特征空间且具有可修改特征的环境。该方法适用于任何动作空间（离散或连续）。EAAN 梯度方法在 RL 上下文中动态攻击的整合和使用方案与
    EACN 完全相同，如图 [16](#S5.F16 "图 16 ‣ 一阶优化方法：梯度攻击 (白盒) ‣ 5.2.1 最小化奖励 ‣ 5.2 动态改变 ‣
    5 对抗攻击 ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述") 所示，唯一的区别在于 EAAN 的梯度计算在 Actor 网络 $\pi$ 上，而不是在值-评论员网络
    $V$ 上。
- en: 5.2.3 Other Objective
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 其他目标
- en: Classical Adversarial Policy Learning Based Methods
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于经典对抗策略学习的方法
- en: 'Environment-Search Attack ESA : [[28](#bib.bib28)] train an adversary to perturb
    the environment transition model $M$ with an adversarial reward based on the distance
    between the disturbed state and the original state: the goal is to make small
    changes to $M$ but to induce a completely different disturbed state. The scheme
    of the integration and usage of ESA on dynamic attacks in an RL context is shown
    exactly the same as for Adversarial Policy which is shown in Figure [13](#S5.F13
    "Figure 13 ‣ Classical Adversarial Policies (Black Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey"), the only difference is
    that in ESA reward get by the adversary is not the opposite of the reward of the
    agent, but is a reward based on the state distance.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '环境搜索攻击 ESA : [[28](#bib.bib28)] 训练对手来扰动环境转移模型 $M$，通过基于扰动状态与原始状态之间的距离的对抗奖励：目标是对
    $M$ 进行小幅度的更改，但引发完全不同的扰动状态。ESA 在 RL 上下文中动态攻击的整合和使用方案与对抗策略的方案完全相同，如图 [13](#S5.F13
    "图 13 ‣ 经典对抗策略 (黑盒) ‣ 5.2.1 最小化奖励 ‣ 5.2 动态改变 ‣ 5 对抗攻击 ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述")
    所示，唯一的区别在于 ESA 中对手获得的奖励不是代理的奖励的相反，而是基于状态距离的奖励。'
- en: '|'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Component &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 组件 &#124;'
- en: '&#124; Alteration &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 改变 &#124;'
- en: '| Objective | Category |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 分类 |'
- en: '&#124; Perturbed &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 扰动 &#124;'
- en: '&#124; Element &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 元素 &#124;'
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Model &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '&#124; Knowledge &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识 &#124;'
- en: '|  | Method |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 |'
- en: '| Observations Alteration see [5.1](#S5.SS1 "5.1 Observation Alteration Attacks
    ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") | Deviate Policy | Gradient Attack see [5.1.1](#S5.SS1.SSS1.Px1
    "First Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.1.1 Deviate
    Policy ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") |
    observation $x$ | white-box |  | FGSM [[65](#bib.bib65)] |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 观察改变见 [5.1](#S5.SS1 "5.1 观察改变攻击 ‣ 5 对抗攻击 ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述") | 偏离策略
    | 梯度攻击见 [5.1.1](#S5.SS1.SSS1.Px1 "一阶优化方法：梯度攻击（白盒） ‣ 5.1.1 偏离策略 ‣ 5.1 观察改变攻击 ‣
    5 对抗攻击 ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述") | 观察 $x$ | 白盒 |  | FGSM [[65](#bib.bib65)]
    |'
- en: '|  | BIM [[31](#bib.bib31)] |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | BIM [[31](#bib.bib31)] |'
- en: '|  | PGD [[32](#bib.bib32)] |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | PGD [[32](#bib.bib32)] |'
- en: '|  | DeepFool [[33](#bib.bib33)] |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepFool [[33](#bib.bib33)] |'
- en: '|  | C&W [[34](#bib.bib34)] |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | C&W [[34](#bib.bib34)] |'
- en: '|  | JSMA [[35](#bib.bib35)] |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | JSMA [[35](#bib.bib35)] |'
- en: '|  | XSMA [[36](#bib.bib36)] |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | XSMA [[36](#bib.bib36)] |'
- en: '|  | VFGA [[37](#bib.bib37)] |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | VFGA [[37](#bib.bib37)] |'
- en: '| Zeroth Order Optimization Attack see [5.1.1](#S5.SS1.SSS1.Px2 "Zeroth Order
    Optimization Methods (Black Box) ‣ 5.1.1 Deviate Policy ‣ 5.1 Observation Alteration
    Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") | observation $x$ | black-box |  | FD [[40](#bib.bib40),
    [28](#bib.bib28)] SA [[39](#bib.bib39)] |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 零阶优化攻击见 [5.1.1](#S5.SS1.SSS1.Px2 "零阶优化方法（黑盒） ‣ 5.1.1 偏离策略 ‣ 5.1 观察改变攻击 ‣
    5 对抗攻击 ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述") | 观察 $x$ | 黑盒 |  | FD [[40](#bib.bib40), [28](#bib.bib28)]
    SA [[39](#bib.bib39)] |'
- en: '| Minimize Reward | Adversarial Policy see [5.1.2](#S5.SS1.SSS2 "5.1.2 Minimize
    Reward ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") |
    observation $x$ | black-box |  | OARLP [[42](#bib.bib42), [43](#bib.bib43)] |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 最小化奖励 | 对抗策略见 [5.1.2](#S5.SS1.SSS2 "5.1.2 最小化奖励 ‣ 5.1 观察改变攻击 ‣ 5 对抗攻击 ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述")
    | 观察 $x$ | 黑盒 |  | OARLP [[42](#bib.bib42), [43](#bib.bib43)] |'
- en: '|  | SA-MDP [[44](#bib.bib44)] |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | SA-MDP [[44](#bib.bib44)] |'
- en: '| white-box |  | ATN [[46](#bib.bib46), [47](#bib.bib47)] |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 |  | ATN [[46](#bib.bib46), [47](#bib.bib47)] |'
- en: '|  | PA-AD [[48](#bib.bib48)] |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | PA-AD [[48](#bib.bib48)] |'
- en: '| Dynamics Alteration see [5.2](#S5.SS2 "5.2 Dynamic Alteration ‣ 5 Adversarial
    Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") | Minimize Reward | Adversarial Policy see [5.2.1](#S5.SS2.SSS1.Px2
    "Augmented Adversarial Policies (White Box) ‣ 5.2.1 Minimize Reward ‣ 5.2 Dynamic
    Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey") | transition $T^{\Omega}$ state
    $s$ or action $a$ | black-box |  | RARL [[49](#bib.bib49)] |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 动态改变见 [5.2](#S5.SS2 "5.2 动态改变 ‣ 5 对抗攻击 ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述") | 最小化奖励
    | 对抗策略见 [5.2.1](#S5.SS2.SSS1.Px2 "增强对抗策略（白盒） ‣ 5.2.1 最小化奖励 ‣ 5.2 动态改变 ‣ 5 对抗攻击
    ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述") | 转移 $T^{\Omega}$ 状态 $s$ 或 动作 $a$ | 黑盒 |  | RARL [[49](#bib.bib49)]
    |'
- en: '|  | RA-RARL [[50](#bib.bib50)] |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | RA-RARL [[50](#bib.bib50)] |'
- en: '|  | SC-RARL [[51](#bib.bib51)] |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | SC-RARL [[51](#bib.bib51)] |'
- en: '|  | A-MCTS [[56](#bib.bib56)] |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | A-MCTS [[56](#bib.bib56)] |'
- en: '|  | APT [[57](#bib.bib57)] |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | APT [[57](#bib.bib57)] |'
- en: '|  | ICMCTS-BR [[58](#bib.bib58)] |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | ICMCTS-BR [[58](#bib.bib58)] |'
- en: '| white-box |  | WB-AP [[59](#bib.bib59)] |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 |  | WB-AP [[59](#bib.bib59)] |'
- en: '| action $a$ | black-box |  | PR-MDP [[54](#bib.bib54)] |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 动作 $a$ | 黑盒 |  | PR-MDP [[54](#bib.bib54)] |'
- en: '| Gradient Attack see [5.2.1](#S5.SS2.SSS1.Px3 "Direct Optimization Methods
    ‣ 5.2.1 Minimize Reward ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey")
    | action $a$ | white-box |  | MAS, LAS [[60](#bib.bib60), [61](#bib.bib61)] |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 梯度攻击见 [5.2.1](#S5.SS2.SSS1.Px3 "直接优化方法 ‣ 5.2.1 最小化奖励 ‣ 5.2 动态改变 ‣ 5 对抗攻击
    ‣ 通过对抗攻击和训练进行稳健的深度强化学习：综述") | 动作 $a$ | 白盒 |  | MAS, LAS [[60](#bib.bib60), [61](#bib.bib61)]
    |'
- en: '| state $s$ | white-box |  | EACN [[62](#bib.bib62)] |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 状态 $s$ | 白盒 |  | EACN [[62](#bib.bib62)] |'
- en: '| Deviate Policy | Gradient Attack see [5.2.2](#S5.SS2.SSS2 "5.2.2 Deviate
    Policy ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") | state $s$ | white-box
    |  | EAAN [[62](#bib.bib62)] |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 偏差策略 | 梯度攻击见 [5.2.2](#S5.SS2.SSS2 "5.2.2 Deviate Policy ‣ 5.2 Dynamic Alteration
    ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") | 状态 $s$ | 白箱 |  | EAAN [[62](#bib.bib62)] |'
- en: '| Other Objective | Adversarial Policy see [5.2.3](#S5.SS2.SSS3 "5.2.3 Other
    Objective ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") | state $s$ | black-box
    |  | ESA [[28](#bib.bib28)] |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 其他目标 | 对抗策略见 [5.2.3](#S5.SS2.SSS3 "5.2.3 Other Objective ‣ 5.2 Dynamic Alteration
    ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") | 状态 $s$ | 黑箱 |  | ESA [[28](#bib.bib28)] |'
- en: 'Table 1: Characterization and Constraints of Adversarial Attack Methods for
    Reinforcement Learning : Summary of the Content of Section [5](#S5 "5 Adversarial
    Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 对强化学习对抗攻击方法的特征与约束：第 [5](#S5 "5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") 节内容的总结'
- en: 6 Strategies of Attacks
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 攻击策略
- en: Adversarial attack can be used in many different ways and there can be various
    approaches to use them. Often adversarial attack are introduced with a specific
    application strategy. In this section we develop about how various strategies
    are applied to use adversarial attacks.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击可以以多种方式使用，并且可以有各种方法来应用它们。对抗攻击通常伴随着特定的应用策略。在这一节中，我们将探讨各种策略如何应用于对抗攻击。
- en: 6.1 Strategies for using White Box Attacks in Black Box Scenarios
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 在黑箱场景中使用白箱攻击的策略
- en: Some strategies leverage white box attack methods in scenarios where the adversary
    has limited knowledge about the target agent (black box scenarios). Techniques
    in this category, like imitation learning, often involve understanding or approximating
    the internal mechanisms of the target system without complete access to its architecture
    or training data. This approach is particularly challenging as it requires the
    adversary to make accurate guesses or approximations about the target’s behavior
    based on limited observable outputs or effects. The AEPI strategy [[66](#bib.bib66)]
    targets black-box environments where the attacker has limited knowledge about
    the victim’s system. It uses Deep Q-Learning from Demonstration (DQfD) to imitate
    the victim’s Q-function, to the apply white box adversarial attacks. The RS strategy
    [[44](#bib.bib44)] is designed to not relying on the victim’s Q-function’s accuracy.
    RS focuses on learning the Q-function of the victim’s policy and them applying
    any white box adversarial attacks.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 一些策略在对目标代理了解有限的场景中利用白箱攻击方法（即黑箱场景）。这类技术，如模仿学习，通常涉及在没有完全访问目标系统架构或训练数据的情况下理解或近似目标系统的内部机制。这种方法特别具有挑战性，因为它要求对手基于有限的可观察输出或效果对目标的行为做出准确的猜测或近似。AEPI
    策略 [[66](#bib.bib66)] 旨在针对对目标系统了解有限的黑箱环境。它使用来自演示的深度 Q 学习（DQfD）来模仿受害者的 Q 函数，从而应用白箱对抗攻击。RS
    策略 [[44](#bib.bib44)] 旨在不依赖于受害者 Q 函数的准确性。RS 侧重于学习受害者策略的 Q 函数，然后应用任何白箱对抗攻击。
- en: 6.2 Strategies for Timing and Stealthiness
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 时机和隐秘性策略
- en: Some Strategies are characterized by their focus on the timing of attacks and
    maintaining stealthiness. These strategies are designed to minimize detection
    while maximizing impact, often by carefully choosing when to launch an attack
    or by subtly altering agent behavior. This approach is particularly relevant in
    scenarios where avoiding detection is crucial, either for the success of the attack
    or to study the system’s vulnerabilities without triggering alarms.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 一些策略以攻击的时机和保持隐秘性为特点。这些策略旨在最小化被检测的风险，同时最大化影响，通常通过仔细选择攻击的时机或微妙地改变代理行为来实现。这种方法在避免被检测的场景中尤其重要，无论是为了攻击的成功还是为了在不触发警报的情况下研究系统的脆弱性。
- en: K&S [[67](#bib.bib67)], and Strategically-Timed Attack STA [[68](#bib.bib68)],
    both concentrate on the timing of perturbations. [[67](#bib.bib67)] found that
    altering the frequency of FGSM perturbation injections can maintain effectiveness
    while reducing computational costs. Similarly, STA employs a preference metric
    to determine the optimal moments for launching perturbations, targeting only 25%
    of the states.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: K&S [[67](#bib.bib67)] 和战略时机攻击 STA [[68](#bib.bib68)] 都集中于扰动的时机。[[67](#bib.bib67)]
    发现，通过改变 FGSM 扰动注入的频率可以在降低计算成本的同时保持效果。类似地，STA 使用偏好度量来确定发起扰动的最佳时机，仅针对 25% 的状态。
- en: Weighted Majority Algorithm WMA [[69](#bib.bib69)] and Critical Point and Antagonist
    Attack CPA and AA [[70](#bib.bib70)]. Take the concept further by introducing
    more sophisticated timing strategies. WMA uses real-time calculations to select
    the most sensitive timeframes for attacks, while CPA and AA focus on identifying
    critical moments for injections, predicting the next environment state or using
    an adversarial agent to decide the timing.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 加权多数算法 WMA [[69](#bib.bib69)] 和关键点及对抗攻击 CPA 和 AA [[70](#bib.bib70)] 进一步引入了更复杂的时机策略。WMA
    使用实时计算来选择攻击的最敏感时间框架，而 CPA 和 AA 专注于识别注入的关键时刻，预测下一个环境状态或使用对抗代理来决定时机。
- en: Static Reward Impact Maps SRIMA [[71](#bib.bib71)] and Minimalistic Attack MA
    [[72](#bib.bib72)] emphasize efficiency and stealth. SRIMA method selects only
    the most influential features for perturbation, reducing the gradient computation
    cost, and is suitable for time-constrained environments. MA pushes stealth to
    the limits by altering only a small fraction of state features and timeframes,
    making it hard for the victim to detect the attack.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 静态奖励影响图 SRIMA [[71](#bib.bib71)] 和简约攻击 MA [[72](#bib.bib72)] 强调效率和隐蔽性。SRIMA
    方法仅选择最具影响力的特征进行扰动，从而减少梯度计算成本，适合于时间受限的环境。MA 通过仅改变少量状态特征和时间框架，将隐蔽性推至极限，使受害者难以检测到攻击。
- en: The Enchanting Attack EA [[68](#bib.bib68)] is a strategy that use a model that
    predict the future state depending on the action applied, and then based on the
    possible future states, apply an adversarial attack targeting the action that
    lead to the target state.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 迷人攻击 EA [[68](#bib.bib68)] 是一种使用模型预测未来状态的策略，该模型基于应用的动作预测未来状态，然后根据可能的未来状态，应用对抗攻击以针对导致目标状态的动作。
- en: 6.3 Strategies for Real Time and Resource Constraint Scenarios
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 实时和资源受限场景的策略
- en: Some strategies are optimized for efficiency and speed, making them suitable
    for real-time applications or scenarios with significant resource constraints.
    Universal attacks, which create perturbations that are broadly effective across
    different inputs or states, are a key part of this category. These techniques
    are valuable in environments where the computational power is limited or where
    decisions need to be made swiftly, such as in certain gaming or real-time decision-making
    applications.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 一些策略在效率和速度上得到了优化，使其适合于实时应用或资源受限的场景。通用攻击通过创建对不同输入或状态广泛有效的扰动，成为这一类别的关键部分。这些技术在计算能力有限或需要迅速做出决策的环境中非常有价值，例如某些游戏或实时决策应用中。
- en: 'Among the input-space adversarial examples, universal methods are particularly
    interesting when dealing with time-constrained environments: state-independent
    perturbations are computed offline, thus allowing for online injections with very
    small delay. These perturbations are short-term by nature, but may lead to controlling
    the victim’s policy and are therefore usually combined with adversarial policies.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入空间对抗样本中，通用方法在处理时间受限的环境时尤其引人注目：状态无关的扰动是离线计算的，因此可以实现非常小延迟的在线注入。这些扰动本质上是短期的，但可能会控制受害者的策略，因此通常与对抗策略相结合。
- en: CopyCAT [[73](#bib.bib73)] use an additive mask $\delta_{a}$ is computed offline
    for each action $a$, maximizing the expected $\pi(a,o^{k}_{t}+\delta_{a})$ over
    a set of pre-collected observations $(o^{k}_{t})_{k,t}$. Once each mask is computed,
    the chosen perturbation can be applied to the last observation $o_{t}$ to make
    the agent follow any target policy. The authors do not focus on how to compute
    such policy, but there existing techniques were described earlier in the survey
    [[74](#bib.bib74)]. Experiments are conducted on Atari games, and CopyCAT provides
    better result than the targeted version of FGSM.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: CopyCAT [[73](#bib.bib73)] 使用的附加掩码 $\delta_{a}$ 是为每个动作 $a$ 离线计算的，最大化在一组预收集的观察
    $(o^{k}_{t})_{k,t}$ 上的期望 $\pi(a,o^{k}_{t}+\delta_{a})$。一旦计算出每个掩码，选择的扰动可以应用于最后的观察
    $o_{t}$，使代理遵循任何目标策略。作者没有专注于如何计算这种策略，但之前在综述中已经描述了现有的技术 [[74](#bib.bib74)]。实验在 Atari
    游戏上进行，CopyCAT 比 FGSM 的有针对性版本提供了更好的结果。
- en: Universal Adversarial Perturbations UAP-S, UAP-O [[75](#bib.bib75)] inspired
    from a known DL-based universal method, UAP [[76](#bib.bib76)]. The adversary
    first gathers a set of observed states $D_{train}$ and sanitizes it, keeping only
    the ones having a critical influence on the episode. Then, the unique additive
    mask is computed using the UAP algorithm [[76](#bib.bib76)]. In the case of Atari
    games, where a state $s_{t}$ consists in several consecutive observations ${o_{t-N+1},...,o_{t}}$,
    the perturbation can be state-agnostic (UAP-S), i.e. unique for all states but
    different for each $o_{i}$ within a state, or observation-agnostic (UAP-O), i.e.
    unique for all observations. Experiments show promising results for both methods
    (with an advantage for UAP-S) on DQN, PPO and A2C agents in three different Atari
    games. Authors show that the SA-MDP method [[44](#bib.bib44)] is not sufficient
    to mitigate these attacks, and introduce an effective detection technique called
    $AD^{3}$.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 通用对抗扰动 UAP-S，UAP-O [[75](#bib.bib75)] 灵感来源于已知的基于深度学习的通用方法 UAP [[76](#bib.bib76)]。对抗者首先收集一组观察到的状态
    $D_{train}$ 并对其进行清理，仅保留对回合具有关键影响的状态。然后，使用 UAP 算法 [[76](#bib.bib76)] 计算唯一的附加掩码。在
    Atari 游戏的情况下，其中一个状态 $s_{t}$ 由多个连续观察 ${o_{t-N+1},...,o_{t}}$ 组成，扰动可以是状态无关的（UAP-S），即对所有状态唯一但对状态内每个
    $o_{i}$ 不同，或者观察无关的（UAP-O），即对所有观察唯一。实验显示这两种方法（UAP-S 更具优势）在三种不同的 Atari 游戏中的 DQN、PPO
    和 A2C 代理上都取得了有希望的结果。作者表明 SA-MDP 方法 [[44](#bib.bib44)] 对这些攻击的缓解不够充分，并引入了一种有效的检测技术称为
    $AD^{3}$。
- en: DAP [[74](#bib.bib74)] decoupled adversarial policy attack is also universal,
    though the paper is more focused on the adversarial policy aspect. Here each additive
    mask $\delta_{a,a^{\prime}}$ is specific to the victim’s initial action $a$ as
    well as the targeted action $a^{\prime}$. States are then classified into the
    right $(a,a^{\prime})$ category using both the victim’s policy and the decoupled
    policy, and the designated mask is added to the observation if the switch policy
    allows it.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: DAP [[74](#bib.bib74)] 解耦对抗策略攻击也是通用的，尽管该论文更侧重于对抗策略方面。在这里，每个附加掩码 $\delta_{a,a^{\prime}}$
    特定于受害者的初始动作 $a$ 以及目标动作 $a^{\prime}$。然后，状态被使用受害者的策略和解耦策略分类到正确的 $(a,a^{\prime})$
    类别中，如果切换策略允许，将指定的掩码添加到观察中。
- en: '| Category | Strategies |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 策略 |'
- en: '| White Box Attacks in Black Box Scenarios see [6.1](#S6.SS1 "6.1 Strategies
    for using White Box Attacks in Black Box Scenarios ‣ 6 Strategies of Attacks ‣
    Robust Deep Reinforcement Learning Through Adversarial Attacks and Training :
    A Survey") | AEPI [[66](#bib.bib66)] RS [[44](#bib.bib44)] |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 在黑箱场景中的白箱攻击见 [6.1](#S6.SS1 "6.1 使用白箱攻击在黑箱场景中的策略 ‣ 攻击策略 ‣ 通过对抗攻击和训练的鲁棒深度强化学习：综述")
    | AEPI [[66](#bib.bib66)] RS [[44](#bib.bib44)] |'
- en: '| Timing and Stealthiness see [6.2](#S6.SS2 "6.2 Strategies for Timing and
    Stealthiness ‣ 6 Strategies of Attacks ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey") | K&S [[67](#bib.bib67)] |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 时机与隐蔽性见 [6.2](#S6.SS2 "6.2 时机与隐蔽性策略 ‣ 攻击策略 ‣ 通过对抗攻击和训练的鲁棒深度强化学习：综述") | K&S
    [[67](#bib.bib67)] |'
- en: '| STA [[68](#bib.bib68)] |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| STA [[68](#bib.bib68)] |'
- en: '| WMA [[69](#bib.bib69)] |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| WMA [[69](#bib.bib69)] |'
- en: '| CPA, AA [[70](#bib.bib70)] |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| CPA, AA [[70](#bib.bib70)] |'
- en: '| SRIMA [[71](#bib.bib71)] |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| SRIMA [[71](#bib.bib71)] |'
- en: '| MA [[72](#bib.bib72)] |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| MA [[72](#bib.bib72)] |'
- en: '| EA [[68](#bib.bib68)] |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| EA [[68](#bib.bib68)] |'
- en: '| Real Time and Resource Constraint Scenarios Methods see [6.3](#S6.SS3 "6.3
    Strategies for Real Time and Resource Constraint Scenarios ‣ 6 Strategies of Attacks
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") | CopyCAT [[73](#bib.bib73)] |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 实时和资源约束场景方法见[6.3](#S6.SS3 "6.3 Strategies for Real Time and Resource Constraint
    Scenarios ‣ 6 Strategies of Attacks ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey") | CopyCAT [[73](#bib.bib73)] |'
- en: '| UAP-S / O [[75](#bib.bib75), [76](#bib.bib76)] |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| UAP-S / O [[75](#bib.bib75), [76](#bib.bib76)] |'
- en: '| DAP [[74](#bib.bib74)] |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| DAP [[74](#bib.bib74)] |'
- en: '| \botrule |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| \botrule |  |'
- en: 'Table 2: Strategies for Adversarial Attack : Summary of the Content of Section
    [6](#S6 "6 Strategies of Attacks ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey")'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：对抗攻击策略：第[6](#S6 "6 Strategies of Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey")节内容总结'
- en: 7 Adversarial and Robust Training
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 对抗性与鲁棒性训练
- en: 7.1 Robustness Strategies with Adversarial Training
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 对抗训练的鲁棒性策略
- en: 'Adversarial training in RL is a technique aimed at improving the robustness
    of RL agents against adversarial attacks. The general principle of adversarial
    training involves repeatedly exposing the agent to adversarial examples or perturbations
    during its training phase. This process is akin to inoculating the agent against
    potential attacks it might encounter in the real world or more complex environments.
    This method is conceptually akin to the principles of robust control [[77](#bib.bib77)],
    where the focus is on ensuring that control systems maintain stability and performance
    despite uncertainties and external disturbances. In RL, adversarial training can
    take various forms, but the core idea is consistent: the RL agent is trained not
    only with normal experiences drawn from its interaction with the environment but
    also with experiences that have been modified by adversarial perturbations [[10](#bib.bib10)].
    These perturbations are typically generated using methods akin to those used in
    adversarial attacks – for example, by altering the agent’s observations or by
    modifying the dynamics of the environment. By training in such an adversarially
    challenging environment, the RL agent learns to perform its task effectively even
    when faced with manipulated inputs or altered state transitions. This makes the
    agent more robust and less susceptible to potential adversarial manipulations
    post-deployment. As detailed in [3.2](#S3.SS2 "3.2 Adversarial Attacks for Robust
    RL ‣ 3 Formalization and Scope ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") the training process often involves a sort of
    min-max game, where one tries to minimize the maximum possible loss that an adversary
    can induce. This approach mirrors robust control’s emphasis on preparing systems
    to handle worst-case scenarios and uncertainties, such as variations in system
    dynamics or external noise. Adversarial training in RL can also be related to
    GANs in supervised learning, where models are trained to be robust against an
    adversary that tries to generate examples to fool the model. In the RL context,
    this approach helps the agent to not only optimize its policy for the given task
    but also to harden it against unexpected changes or adversarial strategies that
    might be encountered, thereby enhancing its overall performance and reliability.
    Any adversarial attack method can be used in adversarial training, certain are
    better than others to improve robustness of the agent. And specific strategies
    fro applying attacks can be used to better improve robustness.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练在强化学习（RL）中是一种旨在提高RL智能体对对抗性攻击的鲁棒性的技术。对抗训练的一般原理包括在训练阶段反复将智能体暴露于对抗性示例或扰动。这一过程类似于对智能体进行免疫，以应对它可能在现实世界或更复杂环境中遇到的潜在攻击。这种方法在概念上类似于鲁棒控制的原则[[77](#bib.bib77)]，其重点是确保控制系统在面对不确定性和外部干扰时保持稳定性和性能。在RL中，对抗训练可以采取各种形式，但核心思想是一致的：RL智能体不仅接受来自环境交互的正常经验，还接受经过对抗性扰动修改的经验[[10](#bib.bib10)]。这些扰动通常使用类似于对抗攻击的方法生成——例如，通过改变智能体的观察或修改环境的动态。通过在这种对抗性挑战的环境中进行训练，RL智能体学会即使面对操控的输入或改变的状态转移时，也能有效地执行任务。这使得智能体更具鲁棒性，并且在部署后对潜在的对抗性操控不那么敏感。正如[3.2](#S3.SS2
    "3.2 对抗攻击在鲁棒强化学习中的应用 ‣ 3 正式化与范围 ‣ 通过对抗攻击和训练的鲁棒深度强化学习：综述")中详细介绍的，训练过程通常涉及一种类似于最小-最大游戏的方式，即试图最小化对手可能造成的最大损失。这种方法反映了鲁棒控制在准备系统应对最坏情况和不确定性（如系统动态的变化或外部噪声）方面的重点。RL中的对抗训练也可以与监督学习中的生成对抗网络（GANs）相关联，在监督学习中，模型被训练以抵御试图生成样本来欺骗模型的对手。在RL背景下，这种方法不仅帮助智能体优化其任务策略，还增强了其对意外变化或对抗性策略的防御能力，从而提高了其整体性能和可靠性。任何对抗攻击方法都可以用于对抗训练，但某些方法比其他方法更能提高智能体的鲁棒性。特定的攻击应用策略可以用于更好地提高鲁棒性。
- en: 'In the following the describe the different adversarial training strategies
    that can be applied :'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来描述了可以应用的不同对抗训练策略：
- en: 7.1.1 Initial Adversarial Training
  id: totrans-420
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1 初始对抗训练
- en: It Consists on training an adversary against an agent until convergence, and
    then adversarially train the agent against this agent for it to become robust.
    This technique works with adversarial attacks based on adversarial policies as
    done with the method RARL [[49](#bib.bib49)].
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括训练一个对手与一个代理直到收敛，然后对抗性地训练代理使其变得鲁棒。这项技术对基于对抗策略的对抗攻击效果很好，如使用RARL方法所做的那样[[49](#bib.bib49)]。
- en: 7.1.2 Continuous Adversarial Training
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2 连续对抗训练
- en: It consists on training the adversary to converge. And then co-train the adversary
    and the agent simultaneously.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括训练对手达到收敛，然后同时共同训练对手和代理。
- en: –
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: This approach of adversarial training is the one used for attack techniques
    based Direct Optimization such agent gradient or zeroth order. Since the attacks
    are based on the policy of the agent, the attack automatically re-adapt to the
    new policy, for example gradient attacks automatically compute gradient on the
    new parameters of the agents policy.
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种对抗训练方法用于基于直接优化的攻击技术，如代理梯度或零阶方法。由于攻击基于代理的策略，因此攻击会自动重新适应新的策略，例如，梯度攻击会自动计算代理策略的新参数上的梯度。
- en: –
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: However, for attack techniques based on adversarial policy this techniques is
    less usable since it is hard to maintain the effectiveness of the adversarial
    policy since both agent and adversarial policies are learned simultaneously, one
    agent could learn faster than the other and the the whole processes can be less
    effective. So for adversarial policies the previous approaches of alternate training
    is more preferable.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，对于基于对抗策略的攻击技术，这项技术的实用性较低，因为很难保持对抗策略的有效性，因为代理和对抗策略是同时学习的，一个代理可能比另一个学习得更快，从而使整个过程的有效性降低。因此，对于对抗策略，之前的交替训练方法更为可取。
- en: 7.1.3 Alternate Adversarial Training
  id: totrans-428
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3 交替对抗训练
- en: It consists on alternately training an adversary against an agent until convergence,
    and then train the agent against this adversary until convergence. And repeat
    the loop convergence of both agent and adversary. This technique works well with
    adversarial attacks based on adversarial policies as done with the method ATLA
    [[45](#bib.bib45)].
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括交替训练一个对手和一个代理直到收敛，然后再训练代理对抗这个对手直到收敛。并重复这个代理和对手的收敛循环。这项技术对基于对抗策略的对抗攻击效果很好，如使用ATLA方法所做的那样[[45](#bib.bib45)]。
- en: 7.1.4 Fictitious Self Play
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.4 虚拟自我对弈
- en: FSP [[52](#bib.bib52), [53](#bib.bib53)] is a strategy for applying adversarial
    policies attacks during the adversarial training to maximize the gain in robustness
    of the agent. The adversarial training is done alternatively between the agent
    and the adversary each until convergence. The idea is to not always apply fully
    effective attacks during the training phases of the agent, but sometimes applying
    a random or average disturbance. This strategy enhance RARL [[49](#bib.bib49)]
    to improve its effectiveness for improving generalization of the policy adversarially
    trained.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: FSP [[52](#bib.bib52), [53](#bib.bib53)]是一种在对抗训练期间应用对抗策略攻击以最大化代理鲁棒性的策略。对抗训练在代理和对手之间交替进行，直到收敛。其理念是，在代理训练阶段，并不是总是应用完全有效的攻击，而是有时应用随机或平均的干扰。这一策略增强了RARL
    [[49](#bib.bib49)]，以提高其对对抗性训练策略的泛化效果。
- en: 7.2 Other Robustness Strategies
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 其他鲁棒性策略
- en: 'Leveraging adversarial training for more robust and reliable DRL algorithms
    is the most used defense against adversarial attacks, the variety of methods available
    fitting each specific use case. However, the issue raised by [[78](#bib.bib78)]
    remain: the high amount of possible adversarial inputs makes the design of a unique
    and adaptive defense method unlikely. Combining design-specific adversarial training
    methods with more classic defensive measures is therefore an interesting lead
    to protect DRL models from malicious behaviour, as countermeasures can be implemented
    at each stage of the models’ construction process.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 利用对抗训练以获得更鲁棒和可靠的DRL算法是对抗对抗攻击的最常用防御措施，现有的各种方法适用于每个特定的用例。然而，[[78](#bib.bib78)]提出的问题仍然存在：大量可能的对抗输入使得设计一个唯一且自适应的防御方法不太可能。因此，将设计特定的对抗训练方法与更经典的防御措施结合起来是一个有趣的思路，可以在模型构建过程的每个阶段实施对策，以保护DRL模型免受恶意行为。
- en: First, a defendant may focus on the design stage to choose a robust network
    architecture for the agent’s policy. [[79](#bib.bib79)] works on the impact of
    network architecture to a model’s robustness shows that within a same parameters
    budget, some architectural configurations are more robust than others. In addition,
    the authors show that reducing the deeper layers capacity also improves robustness.
    Though their findings focus on supervised models, they could be extend to DRL.
    Robust architecture may also be achieved through defensive distillation [[80](#bib.bib80)],
    i.e. training a smaller student network to imitate a larger teacher network with
    class probability outputs, resulting in a lighter network with more regularization.
    Distillation was shown to be effective for DRL [[81](#bib.bib81)], and [[82](#bib.bib82)]
    studied the relevance of distillation approaches according to different contexts.
    Observation alterations automatically transform any MDP into a POMDP, since the
    observation is not anymore deterministic, it has been shown that for POMDP, recurrent
    architecture improve performances of the policies [[83](#bib.bib83)], so do defend
    against observation alteration recurrent policies can be useful [[45](#bib.bib45)].
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，防御方可以在设计阶段专注于选择代理策略的鲁棒网络架构。[[79](#bib.bib79)] 研究了网络架构对模型鲁棒性的影响，表明在相同参数预算下，一些架构配置比其他配置更具鲁棒性。此外，作者还表明，减少深层的容量也能提高鲁棒性。虽然他们的研究集中在监督模型上，但这些发现可以扩展到深度强化学习。鲁棒架构还可以通过防御蒸馏
    [[80](#bib.bib80)] 来实现，即训练一个较小的学生网络来模仿一个较大的教师网络的类别概率输出，从而得到一个更轻的网络并具有更多的正则化。蒸馏已被证明对深度强化学习
    [[81](#bib.bib81)] 有效，[[82](#bib.bib82)] 研究了不同情境下蒸馏方法的相关性。观测的变化自动将任何MDP转换为POMDP，因为观测不再是确定性的，已显示对于POMDP，递归架构可以提高策略的表现
    [[83](#bib.bib83)]，因此防御观测变化的递归策略也可能有用 [[45](#bib.bib45)]。
- en: For improving robustness, the defendant may also improve regularization through
    noisy training rewards [[84](#bib.bib84), [85](#bib.bib85)]. Or increase exploration
    with noisy actions [[30](#bib.bib30)].
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高鲁棒性，防御方还可以通过噪声训练奖励[[84](#bib.bib84), [85](#bib.bib85)]来改善正则化。或者通过噪声动作[[30](#bib.bib30)]来增加探索。
- en: Worst-Case-Aware Robust Reinforcement Learning WocaR-RL [[86](#bib.bib86)] is
    a method that improve robustness to observations perturbation without attacking.
    It only works for adversarial attacks on the observations, it consist of constructing
    a $\epsilon$-ball around the observation and computing the upper and lower bound
    of the action $\hat{A}$ of the agent by convex relaxation of neural network. The
    the worst-attack action value network $\underline{Q}(x_{t},a_{t})$ is then learned
    based on $Q(x_{t},\hat{a}_{t})$, and then the agent is trained to maximize the
    worst-attack action value $\underline{Q}(x_{t},a_{t})$
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 最坏情况感知鲁棒强化学习 WocaR-RL [[86](#bib.bib86)] 是一种在不进行攻击的情况下提高对观测扰动鲁棒性的方法。它仅适用于对观测的对抗攻击，通过在观测周围构建一个$\epsilon$-球，并通过神经网络的凸松弛计算代理的动作$\hat{A}$的上下界来实现。然后基于$Q(x_{t},\hat{a}_{t})$学习最坏攻击动作值网络$\underline{Q}(x_{t},a_{t})$，并训练代理以最大化最坏攻击动作值$\underline{Q}(x_{t},a_{t})$。
- en: Some other methods improve robustness defining a distribution of transition
    function for training the agent, the method RAMU [[87](#bib.bib87)] design a architecture
    enabling to learn policies across a distribution of transition function.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他方法通过定义过渡函数的分布来提高鲁棒性，方法RAMU [[87](#bib.bib87)] 设计了一种架构，使得能够在过渡函数分布上学习策略。
- en: 'Finally, a classic method to add a layer of security during the testing stage
    is adversarial detection: identifying modified observations for denoising or removal.
    Detection may be done through successor representation [[88](#bib.bib88)], or
    using a separately trained model [[89](#bib.bib89)]. Adversarial detection is
    a widespread technique in supervised learning [[90](#bib.bib90), [91](#bib.bib91)],
    but it is limited when it comes to the RL context: indeed, environment-space perturbations
    produce legitimate (though unlikely) observations and are therefore very difficult
    to detect. In addition [[92](#bib.bib92)] showed that detection methods are usually
    not very versatile, and easily subject to small changes in the attack methods.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在测试阶段增加安全层的经典方法是对抗性检测：识别修改过的观察结果以进行去噪或删除。检测可以通过继任者表示[[88](#bib.bib88)]，或使用单独训练的模型[[89](#bib.bib89)]。对抗性检测在监督学习中是一种广泛应用的技术[[90](#bib.bib90)，[91](#bib.bib91)]，但在
    RL 背景下却受到限制：环境空间的扰动会产生合法（尽管不太可能）的观察结果，因此非常难以检测。此外[[92](#bib.bib92)] 还表明，检测方法通常不太通用，且容易受到攻击方法微小变化的影响。
- en: 'There is a wide spectrum of defense methods for DRL algorithms, each with their
    benefits and limitations. Combining several methods allows to cover different
    aspects of the adversarial threat, but the defendant must keep in mind that simply
    stacking defense layers does not necessarily improve robustness [[93](#bib.bib93)]:
    each method must be analyzed and selected with care according to the given context.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 算法的防御方法范围广泛，每种方法都有其优缺点。结合多种方法可以涵盖对抗威胁的不同方面，但被告必须记住，单纯堆叠防御层并不一定能提高鲁棒性[[93](#bib.bib93)]：每种方法都必须根据具体情况进行仔细分析和选择。
- en: 8 Discussion
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论
- en: 8.1 Current issues
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 当前问题
- en: Adversarial methods for reinforcement learning is a recent yet booming research
    field. We encountered a few shortcomings in the tools and literature, the resolving
    of which would substantially help future usage and research.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的对抗性方法是一个近期但迅速发展的研究领域。我们在工具和文献中遇到了一些不足，解决这些问题将大大有助于未来的使用和研究。
- en: 8.1.1 Consistency
  id: totrans-443
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.1 一致性
- en: New methods to compute RL-based adversarial examples are published regularly,
    and the number and variety of existing malicious or defensive techniques is constantly
    increasing. Each paper addresses a specific aspect of adversarial RL, however
    the positioning of new methods amongst the pre-existing techniques is often unclear,
    and a more global vision is usually missing. We aim to address this need of a
    generic framework for classification, by introducing a precise and clear taxonomy
    and mapping existing methods onto it. Following a similar structure for presenting
    future work would highly improve the field’s coherence and clarity. This also
    applies to the metrics used in the experiments and to evaluate.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 计算基于 RL 的对抗性示例的新方法定期发布，现有的恶意或防御技术的数量和种类不断增加。每篇论文都涉及对抗 RL 的特定方面，但新方法在已有技术中的定位往往不明确，通常缺乏更全面的视角。我们旨在通过引入一个准确清晰的分类法并将现有方法映射到其中，来解决对通用框架分类的需求。采用类似的结构来展示未来的工作将大大提高该领域的一致性和清晰度。这同样适用于实验中使用的指标和评估。
- en: 8.1.2 Code availability
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.2 代码可用性
- en: A key element for the ongoing research is reproducibility. Indeed, a method’s
    validity relies heavily on its comparison with state-of-the-art techniques. Yet
    as some articles do not provide the corresponding code to replicate their experiments,
    reproducing existing methods becomes a tedious process for researchers, with sometimes
    unreliable results. We therefore wish to highlight the importance of publicly
    available code to make the best use of published works and progress further into
    the field.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 持续研究的关键要素是可重复性。确实，一个方法的有效性在很大程度上依赖于其与最先进技术的比较。然而，由于一些文章未提供对应的代码来重复其实验，复现现有方法成为了研究人员一项繁琐的工作，有时结果不可靠。因此，我们希望强调公开代码的重要性，以便充分利用已发布的工作并进一步推进该领域。
- en: 8.1.3 Common tools and methodology
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.3 常见工具和方法论
- en: 'Comparing the performance of the various existing methods is challenging. The
    metrics used in the articles may differ depending on the authors’ objective, threat
    model, and chosen RL environments. Even if the metrics are similar, the results
    are also dependent on the chosen RL algorithm and its hyperparameters. To properly
    assess a methods efficiency, a standard methodology is needed: [[94](#bib.bib94)]
    propose such approach for RL robustness evaluation, and a few other usage-specific
    methodologies can be found [[95](#bib.bib95), [96](#bib.bib96)]. From a broader
    perspective, authors can follow a simple rule of thumb: using open source and
    peer-reviewed toolkits that provide reliable implementations of RL algorithms
    (Stable-baselines3 [[97](#bib.bib97)], Tianshou [[98](#bib.bib98)]), attacks methods
    (DRL-oriented [[99](#bib.bib99)] or not [[100](#bib.bib100)]), and robust training
    algorithms [[101](#bib.bib101)].'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 比较各种现有方法的性能是具有挑战性的。文章中使用的度量标准可能会因作者的目标、威胁模型和选择的RL环境而异。即使度量标准相似，结果也取决于所选择的RL算法及其超参数。为了正确评估方法的效率，需要一个标准化的方法：[[94](#bib.bib94)]提出了用于RL鲁棒性评估的方法，还有一些其他特定用途的方法可以参考[[95](#bib.bib95),
    [96](#bib.bib96)]。从更广泛的角度来看，作者可以遵循一个简单的经验法则：使用开源且经过同行评审的工具包，这些工具包提供了RL算法（Stable-baselines3
    [[97](#bib.bib97)]、Tianshou [[98](#bib.bib98)]）、攻击方法（面向DRL的[[99](#bib.bib99)]或非DRL的[[100](#bib.bib100)]）以及鲁棒训练算法[[101](#bib.bib101)]的可靠实现。
- en: 8.2 Open challenges
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 未解决的挑战
- en: This survey is also an opportunity to highlight the major research directions
    to be explored in the future.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查也是一个机会，突显未来需要探索的主要研究方向。
- en: 8.2.1 Explainability
  id: totrans-451
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1 可解释性
- en: The question of robustness and vulnerability to adversarial behaviour in RL
    boils down to the issue of trustworthy artificial intelligence. Explainable reinforcement
    learning (XLR) techniques are developed in order to deeply understand the RL decision-making
    process and improve its transparency [[102](#bib.bib102), [103](#bib.bib103)].
    Recent works focus on the correlation between explainability and robustness for
    machine learning in general [[104](#bib.bib104)], and extending these concept
    to DRL may lead to significant progress in both domains.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL中，对抗行为的鲁棒性和脆弱性的问题归结为可信赖人工智能的问题。为深入理解RL决策过程并提高其透明度，开发了可解释的强化学习（XLR）技术[[102](#bib.bib102),
    [103](#bib.bib103)]。最近的工作集中于可解释性与机器学习的鲁棒性之间的相关性[[104](#bib.bib104)]，将这些概念扩展到DRL可能会在两个领域取得显著进展。
- en: 8.2.2 Attack feasability
  id: totrans-453
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2 攻击可行性
- en: 'Another interesting lead for the robustness of DL algorithms, and particularly
    for DRL, is the study of the practicality of adversarial examples. It is now acknowledge
    that an all-mighty, omniscient adversary can fool about any model: however, RL
    environments often model real-life situations with physical constraints. Recent
    works on the compatibility of adversarial examples with such constraint show promising
    results in the defense of DL models [[105](#bib.bib105)]. A thorough study on
    RL-based adversarial methods’ feasability could help improve the challenge of
    universally robust models.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的关于深度学习（DL）算法，尤其是深度强化学习（DRL）鲁棒性的线索是对对抗样本实际应用性的研究。现在已经认可，一个全能、全知的对手可以欺骗几乎任何模型：然而，强化学习（RL）环境通常模拟具有物理约束的现实情况。最近关于对抗样本与这些约束兼容性的研究显示出在防御DL模型方面的有希望的结果[[105](#bib.bib105)]。对基于RL的对抗方法的可行性进行深入研究可能有助于提高通用鲁棒模型的挑战。
- en: 9 Conclusion
  id: totrans-455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: This survey has provided an extensive examination of the robustness challenges
    in RL and the various adversarial training methods aimed at enhancing this robustness.
    Our work highlighted the susceptibility of RL agents to dynamic and observation
    alterations with adversarial attacks, underscoring a significant gap in their
    application in real-world scenarios where reliability and safety are paramount.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查提供了对RL中鲁棒性挑战及各种对抗训练方法的广泛审查。我们的工作突出了RL智能体在对抗攻击下对动态和观察变化的易感性，强调了在可靠性和安全性至关重要的现实世界场景中的应用存在的显著差距。
- en: We have presented a novel taxonomy of adversarial attacks, categorizing them
    based on their impact on the dynamics and observations within the RL environment.
    This classification system not only aids in understanding the nature of these
    attacks but also serves as a guide for researchers and practitioners in identifying
    appropriate adversarial training strategies tailored to specific types of vulnerabilities.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的对抗性攻击分类法，根据攻击对RL环境中的动态和观察的影响进行分类。这一分类系统不仅有助于理解这些攻击的性质，还为研究人员和从业人员在识别适当的对抗性训练策略时提供了指导，以应对特定类型的脆弱性。
- en: Our formalization of the robustness problem in RL, drawing from the principles
    of distributionally robust optimization for bot observation and dynamic alterations,
    provides a foundational framework for future research. By considering the worst-case
    scenarios within a controlled uncertainty set, we can develop RL agents that are
    not only robust to known adversarial attacks but also equipped to handle unexpected
    variations in real-world environments.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对RL中鲁棒性问题的形式化，从分布鲁棒优化原理中汲取灵感，提供了未来研究的基础框架。通过在受控不确定性集内考虑最坏情况，我们可以开发出不仅对已知对抗攻击鲁棒，而且能够处理现实环境中意外变化的RL智能体。
- en: The exploration of adversarial training strategies in this survey emphasizes
    the importance of simulating realistic adversarial conditions during the training
    phase. By doing so, RL agents can be better prepared for the complexities and
    uncertainties of real-world operations, leading to more reliable and effective
    performance.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述中对对抗性训练策略的探讨强调了在训练阶段模拟现实对抗条件的重要性。通过这样做，RL智能体可以更好地为现实操作中的复杂性和不确定性做好准备，从而实现更可靠和有效的性能。
- en: In conclusion, while our work sheds light on the current state of adversarial
    methods in DRL and their role in enhancing agent robustness, it also opens the
    door for further exploration. Future research should focus on refining adversarial
    training techniques, exploring new forms of attacks, and expanding the taxonomy
    as the field evolves. Additionally, there is a need for developing more sophisticated
    models that can balance the trade-off between robustness and performance efficiency.
    As DRL continues to evolve, the pursuit of robust, reliable, and safe autonomous
    agents remains a critical objective, ensuring their applicability and trustworthiness
    in a wide range of real-world applications.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，虽然我们的工作揭示了DRL中对抗性方法的现状及其在增强智能体鲁棒性方面的作用，但也为进一步探索打开了大门。未来的研究应关注于改进对抗性训练技术、探索新的攻击形式以及随着领域的发展扩展分类法。此外，还需要开发更复杂的模型，以平衡鲁棒性和性能效率之间的权衡。随着DRL的不断发展，追求鲁棒、可靠和安全的自主智能体仍然是一个关键目标，确保其在各种现实应用中的适用性和可信度。
- en: \bmhead
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: \bmhead
- en: Acknowledgements This work has been supported by the French government under
    the \sayFrance 2030 program, as part of the SystemX Technological Research Institute
    within the Confiance.ai program.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢 本研究得到了法国政府在\sayFrance 2030计划下的支持，作为SystemX技术研究所Confiance.ai计划的一部分。
- en: References
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: \bibcommenthead
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \bibcommenthead
- en: 'Mnih et al. [2015] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness,
    J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G.,
    Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
    D., Legg, S., Hassabis, D.: Human-level control through deep reinforcement learning.
    Nature (2015). Accessed 2023-02-13'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mnih等人[2015] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J.,
    Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen,
    S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D.,
    Legg, S., Hassabis, D.: 通过深度强化学习实现人类水平的控制。自然 (2015)。访问日期 2023-02-13'
- en: 'Silver et al. [2016] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre,
    L., Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot,
    M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,
    T., Leach, M., Kavukcuoglu, K., Graepel, T., Hassabis, D.: Mastering the game
    of go with deep neural networks and tree search. Nature (2016). Accessed 2023-02-13'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Silver等人[2016] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L.,
    Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot,
    M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,
    T., Leach, M., Kavukcuoglu, K., Graepel, T., Hassabis, D.: 利用深度神经网络和树搜索掌握围棋。自然
    (2016)。访问日期 2023-02-13'
- en: 'Vinyals et al. [2019] Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu,
    M., Dudzik, A., Chung, J., Choi, D.H., Powell, R., Ewalds, T., Georgiev, P., Oh,
    J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou,
    J.P., Jaderberg, M., Vezhnevets, A.S., Leblond, R., Pohlen, T., Dalibard, V.,
    Budden, D., Sulsky, Y., Molloy, J., Paine, T.L., Gulcehre, C., Wang, Z., Pfaff,
    T., Wu, Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul,
    T., Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., Silver, D.: Grandmaster
    level in starcraft ii using multi-agent reinforcement learning. Nature (2019).
    Accessed 2023-02-13'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vinyals等人[2019] Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu, M.,
    Dudzik, A., Chung, J., Choi, D.H., Powell, R., Ewalds, T., Georgiev, P., Oh, J.,
    Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou,
    J.P., Jaderberg, M., Vezhnevets, A.S., Leblond, R., Pohlen, T., Dalibard, V.,
    Budden, D., Sulsky, Y., Molloy, J., Paine, T.L., Gulcehre, C., Wang, Z., Pfaff,
    T., Wu, Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul,
    T., Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., Silver, D.: 使用多智能体强化学习在《星际争霸II》中达到大师级别。自然（2019）。访问日期2023-02-13'
- en: 'Levine et al. [2016] Levine, S., Finn, C., Darrell, T., Abbeel, P.: End-to-End
    Training of Deep Visuomotor Policies. arXiv (2016). [http://arxiv.org/abs/1504.00702](http://arxiv.org/abs/1504.00702)
    Accessed 2023-02-13'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Levine等人[2016] Levine, S., Finn, C., Darrell, T., Abbeel, P.: 深度视觉运动策略的端到端训练。arXiv（2016）。[http://arxiv.org/abs/1504.00702](http://arxiv.org/abs/1504.00702)
    访问日期2023-02-13'
- en: 'Kiran et al. [2021] Kiran, B.R., Sobh, I., Talpaert, V., Mannion, P., Sallab,
    A.A.A., Yogamani, S., Pérez, P.: Deep Reinforcement Learning for Autonomous Driving:
    A Survey. arXiv (2021). [http://arxiv.org/abs/2002.00444](http://arxiv.org/abs/2002.00444)
    Accessed 2023-02-13'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kiran等人[2021] Kiran, B.R., Sobh, I., Talpaert, V., Mannion, P., Sallab, A.A.A.,
    Yogamani, S., Pérez, P.: 用于自动驾驶的深度强化学习：综述。arXiv（2021）。[http://arxiv.org/abs/2002.00444](http://arxiv.org/abs/2002.00444)
    访问日期2023-02-13'
- en: 'Zhang et al. [2018] Zhang, D., Han, X., Deng, C.: Review on the research and
    practice of deep learning and reinforcement learning in smart grids. CSEE Journal
    of Power and Energy Systems (2018)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人[2018] Zhang, D., Han, X., Deng, C.: 深度学习与强化学习在智能电网中的研究与实践综述。CSEE电力与能源系统学报（2018）'
- en: 'Höfer et al. [2020] Höfer, S., Bekris, K., Handa, A., Gamboa, J.C., Golemo,
    F., Mozifian, M., Atkeson, C., Fox, D., Goldberg, K., Leonard, J., et al.: Perspectives
    on sim2real transfer for robotics: A summary of the r: Ss 2020 workshop. arXiv
    preprint arXiv:2012.03806 (2020)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Höfer等人[2020] Höfer, S., Bekris, K., Handa, A., Gamboa, J.C., Golemo, F., Mozifian,
    M., Atkeson, C., Fox, D., Goldberg, K., Leonard, J., 等人：关于机器人领域的sim2real转移的观点：r:
    Ss 2020研讨会总结。arXiv预印本arXiv:2012.03806（2020）'
- en: 'Collins et al. [2019] Collins, J., Howard, D., Leitner, J.: Quantifying the
    reality gap in robotic manipulation tasks. In: 2019 International Conference on
    Robotics and Automation (ICRA), pp. 6706–6712 (2019). IEEE'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Collins等人[2019] Collins, J., Howard, D., Leitner, J.: 量化机器人操作任务中的现实差距。在：2019年国际机器人与自动化会议（ICRA），第6706–6712页（2019）。IEEE'
- en: 'Ilahi et al. [2022] Ilahi, I., Usama, M., Qadir, J., Janjua, M.U., Al-Fuqaha,
    A., Hoang, D.T., Niyato, D.: Challenges and countermeasures for adversarial attacks
    on deep reinforcement learning. IEEE Transactions on Artificial Intelligence (2022)'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ilahi等人[2022] Ilahi, I., Usama, M., Qadir, J., Janjua, M.U., Al-Fuqaha, A.,
    Hoang, D.T., Niyato, D.: 对深度强化学习的对抗攻击的挑战与对策。IEEE人工智能学报（2022）'
- en: 'Moos et al. [2022] Moos, J., Hansel, K., Abdulsamad, H., Stark, S., Clever,
    D., Peters, J.: Robust reinforcement learning: A review of foundations and recent
    advances. Machine Learning and Knowledge Extraction 4(1), 276–315 (2022)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moos等人[2022] Moos, J., Hansel, K., Abdulsamad, H., Stark, S., Clever, D., Peters,
    J.: 稳健的强化学习：基础与近期进展的综述。机器学习与知识提取 4(1)，276–315（2022）'
- en: 'Yuan et al. [2019] Yuan, X., He, P., Zhu, Q., Li, X.: Adversarial examples:
    Attacks and defenses for deep learning. IEEE Transactions on Neural Networks and
    Learning Systems (2019). Accessed 2022-10-12'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan等人[2019] Yuan, X., He, P., Zhu, Q., Li, X.: 对抗样本：深度学习中的攻击与防御。IEEE神经网络与学习系统学报（2019）。访问日期2022-10-12'
- en: 'Chen et al. [2019] Chen, T., Liu, J., Xiang, Y., Niu, W., Tong, E., Han, Z.:
    Adversarial attack and defense in reinforcement learning-from ai security view.
    Cybersecurity (2019). Accessed 2022-10-12'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen等人[2019] Chen, T., Liu, J., Xiang, Y., Niu, W., Tong, E., Han, Z.: 强化学习中的对抗攻击与防御——来自人工智能安全视角。网络安全（2019）。访问日期2022-10-12'
- en: 'Sutton and Barto [1998] Sutton, R.S., Barto, A.G.: Reinforcement Learning,
    Second Edition: An Introduction. MIT Press, ??? (1998)'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sutton和Barto[1998] Sutton, R.S., Barto, A.G.: 强化学习，第二版：导论。MIT出版社，？？？（1998）'
- en: 'Williams [1992] Williams, R.J.: Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. Machine Learning (1992). Accessed 2023-02-22'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Williams [1992] Williams, R.J.: 用于连接主动式强化学习的简单统计梯度跟随算法。机器学习（1992）。2023-02-22
    访问'
- en: 'Mnih et al. [2013] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,
    I., Wierstra, D., Riedmiller, M.: Playing Atari with Deep Reinforcement Learning.
    arXiv (2013). [http://arxiv.org/abs/1312.5602](http://arxiv.org/abs/1312.5602)
    Accessed 2022-11-10'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mnih等 [2013] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,
    I., Wierstra, D., Riedmiller, M.: 用深度强化学习玩Atari。arXiv（2013）。[http://arxiv.org/abs/1312.5602](http://arxiv.org/abs/1312.5602)
    访问 2022-11-10'
- en: 'Lillicrap et al. [2015] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.,
    Erez, T., Tassa, Y., Silver, D., Wierstra, D.: Continuous control with deep reinforcement
    learning. arXiv preprint arXiv:1509.02971 (2015)'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lillicrap等 [2015] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez,
    T., Tassa, Y., Silver, D., Wierstra, D.: 用深度强化学习进行连续控制。arXiv预印本arXiv:1509.02971
    （2015）'
- en: 'Hessel et al. [2018] Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T.,
    Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., Silver, D.: Rainbow:
    Combining improvements in deep reinforcement learning. In: Proceedings of the
    AAAI Conference on Artificial Intelligence (2018)'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hessel等 [2018] Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski,
    G., Dabney, W., Horgan, D., Piot, B., Azar, M., Silver, D.: 彩虹：结合深度强化学习的改进。在：AAAI人工智能大会论文集（2018）'
- en: 'Schulman et al. [2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    Klimov, O.: Proximal Policy Optimization Algorithms. arXiv (2017). [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
    Accessed 2022-11-10'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schulman等 [2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov,
    O.: 近端策略优化算法。arXiv（2017）。[http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
    访问 2022-11-10'
- en: 'Haarnoja et al. [2018] Haarnoja, T., Zhou, A., Abbeel, P., Levine, S.: Soft
    actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic
    actor. In: International Conference on Machine Learning (2018). PMLR'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Haarnoja等 [2018] Haarnoja, T., Zhou, A., Abbeel, P., Levine, S.: 软演员-评论者：以随机剧作者最大熵深度强化学习为基础的离线学习。在：国际机器学习会议（2018）。PMLR'
- en: 'Kuznetsov et al. [2020] Kuznetsov, A., Shvechikov, P., Grishin, A., Vetrov,
    D.: Controlling overestimation bias with truncated mixture of continuous distributional
    quantile critics. In: International Conference on Machine Learning (2020). PMLR'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuznetsov等 [2020] Kuznetsov, A., Shvechikov, P., Grishin, A., Vetrov, D.: 利用截断的连续分布分位数评论控制过高的估计偏差。在：国际机器学习会议（2020）。PMLR'
- en: 'Vassilev et al. [2024] Vassilev, A., Oprea, A., Fordyce, A., Andersen, H.:
    Adversarial machine learning: A taxonomy and terminology of attacks and mitigations
    (2024)'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vassilev等 [2024] Vassilev, A., Oprea, A., Fordyce, A., Andersen, H.: 对抗机器学习：攻击和减轻的分类与术语（2024）'
- en: '[22] Dahmen-Lhuissier, S.: ETSI - Best Security Standards | ETSI Security Standards.
    [https://www.etsi.org/technologies/securing-artificial-intelligence](https://www.etsi.org/technologies/securing-artificial-intelligence)
    Accessed 2022-12-02'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22]  Dahmen-Lhuissier, S.: ETSI - 最佳安全标准 | ETSI安全标准。[https://www.etsi.org/technologies/securing-artificial-intelligence](https://www.etsi.org/technologies/securing-artificial-intelligence)
    访问 2022-12-02'
- en: 'Huang et al. [2017] Huang, S., Papernot, N., Goodfellow, I., Duan, Y., Abbeel,
    P.: Adversarial Attacks on Neural Network Policies. arXiv (2017). [http://arxiv.org/abs/1702.02284](http://arxiv.org/abs/1702.02284)
    Accessed 2022-11-22'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang等 [2017] Huang, S., Papernot, N., Goodfellow, I., Duan, Y., Abbeel, P.:
    对神经网络策略的敌对攻击。arXiv（2017）。[http://arxiv.org/abs/1702.02284](http://arxiv.org/abs/1702.02284)
    访问 2022-11-22'
- en: 'Barreno et al. [2010] Barreno, M., Nelson, B., Joseph, A.D., Tygar, J.D.: The
    security of machine learning. Machine Learning (2010). Accessed 2022-11-10'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Barreno等 [2010] Barreno, M., Nelson, B., Joseph, A.D., Tygar, J.D.: 机器学习的安全性。机器学习（2010）。2022-11-10访问'
- en: 'Brunke et al. [2022] Brunke, L., Greeff, M., Hall, A.W., Yuan, Z., Zhou, S.,
    Panerati, J., Schoellig, A.P.: Safe learning in robotics: From learning-based
    control to safe reinforcement learning. Annual Review of Control, Robotics, and
    Autonomous Systems 5, 411–444 (2022)'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brunke等 [2022] Brunke, L., Greeff, M., Hall, A.W., Yuan, Z., Zhou, S., Panerati,
    J., Schoellig, A.P.: 机器人领域的安全学习：从基于学习的控制到安全强化学习。控制、机器人和自主系统年度综述 5, 411-444（2022）'
- en: 'Morimoto and Doya [2005] Morimoto, J., Doya, K.: Robust reinforcement learning.
    Neural computation 17(2), 335–359 (2005)'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Morimoto和Doya [2005] Morimoto, J., Doya, K.: 强化学习的稳健性。神经计算17（2），335-359（2005）'
- en: 'Behzadan and Munir [2017] Behzadan, V., Munir, A.: Vulnerability of deep reinforcement
    learning to policy induction attacks. In: Perner, P. (ed.) Machine Learning and
    Data Mining in Pattern Recognition. Springer, ??? (2017)'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Behzadan 和 Munir [2017] Behzadan, V., Munir, A.: 深度强化学习对策略诱导攻击的脆弱性。载于：Perner,
    P.（编）模式识别中的机器学习与数据挖掘。Springer，???（2017）'
- en: 'Pan et al. [2022] Pan, X., Xiao, C., He, W., Yang, S., Peng, J., Sun, M., Yi,
    J., Yang, Z., Liu, M., Li, B., Song, D.: Characterizing Attacks on Deep Reinforcement
    Learning. arXiv (2022). [http://arxiv.org/abs/1907.09470](http://arxiv.org/abs/1907.09470)
    Accessed 2022-11-23'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pan 等 [2022] Pan, X., Xiao, C., He, W., Yang, S., Peng, J., Sun, M., Yi, J.,
    Yang, Z., Liu, M., Li, B., Song, D.: 深度强化学习中的攻击特征。arXiv（2022）。[http://arxiv.org/abs/1907.09470](http://arxiv.org/abs/1907.09470)
    访问时间：2022-11-23'
- en: 'Rahimian and Mehrotra [2019] Rahimian, H., Mehrotra, S.: Distributionally robust
    optimization: A review. arXiv preprint arXiv:1908.05659 (2019)'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rahimian 和 Mehrotra [2019] Rahimian, H., Mehrotra, S.: 分布鲁棒优化：综述。arXiv 预印本
    arXiv:1908.05659（2019）'
- en: 'Hollenstein et al. [2022] Hollenstein, J., Auddy, S., Saveriano, M., Renaudo,
    E., Piater, J.: Action Noise in Off-Policy Deep Reinforcement Learning: Impact
    on Exploration and Performance. arXiv (2022). [http://arxiv.org/abs/2206.03787](http://arxiv.org/abs/2206.03787)
    Accessed 2023-01-23'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hollenstein 等 [2022] Hollenstein, J., Auddy, S., Saveriano, M., Renaudo, E.,
    Piater, J.: 离策略深度强化学习中的动作噪声：对探索和性能的影响。arXiv（2022）。[http://arxiv.org/abs/2206.03787](http://arxiv.org/abs/2206.03787)
    访问时间：2023-01-23'
- en: 'Kurakin et al. [2017] Kurakin, A., Goodfellow, I., Bengio, S.: Adversarial
    examples in the physical world. arXiv (2017). [http://arxiv.org/abs/1607.02533](http://arxiv.org/abs/1607.02533)
    Accessed 2022-12-05'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kurakin 等 [2017] Kurakin, A., Goodfellow, I., Bengio, S.: 物理世界中的对抗样本。arXiv（2017）。[http://arxiv.org/abs/1607.02533](http://arxiv.org/abs/1607.02533)
    访问时间：2022-12-05'
- en: 'Madry et al. [2017] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu,
    A.: Towards deep learning models resistant to adversarial attacks. arXiv preprint
    arXiv:1706.06083 (2017)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madry 等 [2017] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.:
    朝着对抗攻击鲁棒的深度学习模型发展。arXiv 预印本 arXiv:1706.06083（2017）'
- en: 'Moosavi-Dezfooli et al. [2016] Moosavi-Dezfooli, S.-M., Fawzi, A., Frossard,
    P.: Deepfool: a simple and accurate method to fool deep neural networks. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574–2582
    (2016)'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moosavi-Dezfooli 等 [2016] Moosavi-Dezfooli, S.-M., Fawzi, A., Frossard, P.:
    Deepfool: 一种简单而准确的欺骗深度神经网络的方法。载于：IEEE 计算机视觉与模式识别会议论文集，第2574–2582页（2016）'
- en: 'Carlini and Wagner [2017] Carlini, N., Wagner, D.: Towards evaluating the robustness
    of neural networks. In: 2017 IEEE Symposium on Security and Privacy (SP) (2017)'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carlini 和 Wagner [2017] Carlini, N., Wagner, D.: 评估神经网络鲁棒性的方法。载于：2017 IEEE
    安全与隐私研讨会（SP）（2017）'
- en: 'Papernot et al. [2016] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M.,
    Celik, Z.B., Swami, A.: The limitations of deep learning in adversarial settings.
    In: 2016 IEEE European Symposium on Security and Privacy (EuroS&P) (2016)'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papernot 等 [2016] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik,
    Z.B., Swami, A.: 深度学习在对抗设置中的局限性。载于：2016 IEEE 欧洲安全与隐私研讨会（EuroS&P）（2016）'
- en: 'Césaire et al. [2021] Césaire, M., Schott, L., Hajri, H., Lamprier, S., Gallinari,
    P.: Stochastic sparse adversarial attacks. In: 2021 IEEE 33rd International Conference
    on Tools with Artificial Intelligence (ICTAI), pp. 1247–1254 (2021). IEEE'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Césaire 等 [2021] Césaire, M., Schott, L., Hajri, H., Lamprier, S., Gallinari,
    P.: 随机稀疏对抗攻击。载于：2021 IEEE 第33届国际人工智能工具会议（ICTAI），第1247–1254页（2021）。IEEE'
- en: 'Hajri et al. [2022] Hajri, H., Cesaire, M., Schott, L., Lamprier, S., Gallinari,
    P.: Neural adversarial attacks with random noises. International Journal on Artificial
    Intelligence Tools (2022)'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hajri 等 [2022] Hajri, H., Cesaire, M., Schott, L., Lamprier, S., Gallinari,
    P.: 带有随机噪声的神经对抗攻击。国际人工智能工具期刊（2022）'
- en: 'Pattanaik et al. [2017] Pattanaik, A., Tang, Z., Liu, S., Bommannan, G., Chowdhary,
    G.: Robust Deep Reinforcement Learning with Adversarial Attacks. arXiv (2017).
    [http://arxiv.org/abs/1712.03632](http://arxiv.org/abs/1712.03632) Accessed 2022-11-15'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pattanaik 等 [2017] Pattanaik, A., Tang, Z., Liu, S., Bommannan, G., Chowdhary,
    G.: 具有对抗攻击的鲁棒深度强化学习。arXiv（2017）。[http://arxiv.org/abs/1712.03632](http://arxiv.org/abs/1712.03632)
    访问时间：2022-11-15'
- en: 'Andriushchenko et al. [2020] Andriushchenko, M., Croce, F., Flammarion, N.,
    Hein, M.: Square attack: a query-efficient black-box adversarial attack via random
    search. In: European Conference on Computer Vision, pp. 484–501 (2020). Springer'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Andriushchenko 等 [2020] Andriushchenko, M., Croce, F., Flammarion, N., Hein,
    M.: Square attack: 一种通过随机搜索的查询高效黑箱对抗攻击。载于：欧洲计算机视觉大会，第484–501页（2020）。Springer'
- en: 'Bhagoji et al. [2017] Bhagoji, A.N., He, W., Li, B., Song, D.: Exploring the
    space of black-box attacks on deep neural networks. arXiv preprint arXiv:1712.09491
    (2017)'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bhagoji 等 [2017] Bhagoji, A.N., He, W., Li, B., Song, D.: 探索黑箱攻击深度神经网络的空间。arXiv
    预印本 arXiv:1712.09491 (2017)'
- en: 'Zhang et al. [2020] Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning,
    D., Hsieh, C.-J.: Robust deep reinforcement learning against adversarial perturbations
    on state observations. Advances in Neural Information Processing Systems 33, 21024–21037
    (2020)'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2020] Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning, D.,
    Hsieh, C.-J.: 针对状态观察的对抗性扰动的鲁棒深度强化学习。神经信息处理系统进展 33, 21024–21037 (2020)'
- en: 'Russo and Proutiere [2019] Russo, A., Proutiere, A.: Optimal Attacks on Reinforcement
    Learning Policies. arXiv (2019). [http://arxiv.org/abs/1907.13548](http://arxiv.org/abs/1907.13548)
    Accessed 2022-10-17'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Russo 和 Proutiere [2019] Russo, A., Proutiere, A.: 对强化学习策略的*最优*攻击。arXiv (2019)。[http://arxiv.org/abs/1907.13548](http://arxiv.org/abs/1907.13548)
    访问时间 2022-10-17'
- en: 'Russo and Proutiere [2021] Russo, A., Proutiere, A.: Towards optimal attacks
    on reinforcement learning policies. In: 2021 American Control Conference (ACC)
    (2021)'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Russo 和 Proutiere [2021] Russo, A., Proutiere, A.: 朝着对强化学习策略的*最优*攻击前进。在：2021
    年美国控制会议 (ACC) (2021)'
- en: 'Zhang et al. [2020] Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning,
    D., Hsieh, C.-J.: Robust deep reinforcement learning against adversarial perturbations
    on state observations. In: Advances in Neural Information Processing Systems.
    Curran Associates, Inc., ??? (2020). [https://proceedings.neurips.cc/paper/2020/hash/f0eb6568ea114ba6e293f903c34d7488-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/f0eb6568ea114ba6e293f903c34d7488-Abstract.html)
    Accessed 2022-11-15'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2020] Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning, D.,
    Hsieh, C.-J.: 针对状态观察的对抗性扰动的鲁棒深度强化学习。在：神经信息处理系统进展。Curran Associates, Inc., ???
    (2020)。[https://proceedings.neurips.cc/paper/2020/hash/f0eb6568ea114ba6e293f903c34d7488-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/f0eb6568ea114ba6e293f903c34d7488-Abstract.html)
    访问时间 2022-11-15'
- en: 'Zhang et al. [2021] Zhang, H., Chen, H., Boning, D., Hsieh, C.-J.: Robust reinforcement
    learning on state observations with learned optimal adversary. arXiv preprint
    arXiv:2101.08452 (2021)'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2021] Zhang, H., Chen, H., Boning, D., Hsieh, C.-J.: 基于学习的*最优*对手的状态观察鲁棒强化学习。arXiv
    预印本 arXiv:2101.08452 (2021)'
- en: 'Tretschk et al. [2018] Tretschk, E., Oh, S.J., Fritz, M.: Sequential Attacks
    on Agents for Long-Term Adversarial Goals. arXiv (2018). [http://arxiv.org/abs/1805.12487](http://arxiv.org/abs/1805.12487)
    Accessed 2022-10-17'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tretschk 等 [2018] Tretschk, E., Oh, S.J., Fritz, M.: 对代理的顺序攻击以实现长期对抗性目标。arXiv
    (2018)。[http://arxiv.org/abs/1805.12487](http://arxiv.org/abs/1805.12487) 访问时间
    2022-10-17'
- en: 'Baluja and Fischer [2018] Baluja, S., Fischer, I.: Learning to attack: Adversarial
    transformation networks. Proceedings of the AAAI Conference on Artificial Intelligence
    (2018). Accessed 2022-10-17'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baluja 和 Fischer [2018] Baluja, S., Fischer, I.: 学习攻击：对抗性变换网络。人工智能协会会议论文集 (2018)。访问时间
    2022-10-17'
- en: 'Sun et al. [2022] Sun, Y., Zheng, R., Liang, Y., Huang, F.: Who is the strongest
    enemy? towards optimal and efficient evasion attacks in deep rl. International
    Conference on Learning Representations (ICLR) (2022)'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 [2022] Sun, Y., Zheng, R., Liang, Y., Huang, F.: 谁是最强的敌人？朝着深度强化学习中的*最优*和*高效*规避攻击前进。国际学习表征会议
    (ICLR) (2022)'
- en: 'Pinto et al. [2017] Pinto, L., Davidson, J., Sukthankar, R., Gupta, A.: Robust
    adversarial reinforcement learning. In: Proceedings of the 34th International
    Conference on Machine Learning. PMLR, ??? (2017). [https://proceedings.mlr.press/v70/pinto17a.html](https://proceedings.mlr.press/v70/pinto17a.html)
    Accessed 2022-10-18'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pinto 等 [2017] Pinto, L., Davidson, J., Sukthankar, R., Gupta, A.: 鲁棒对抗性强化学习。在：第34届国际机器学习会议论文集。PMLR,
    ??? (2017)。[https://proceedings.mlr.press/v70/pinto17a.html](https://proceedings.mlr.press/v70/pinto17a.html)
    访问时间 2022-10-18'
- en: 'Pan et al. [2019] Pan, X., Seita, D., Gao, Y., Canny, J.: Risk averse robust
    adversarial reinforcement learning. In: 2019 International Conference on Robotics
    and Automation (ICRA) (2019)'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pan 等 [2019] Pan, X., Seita, D., Gao, Y., Canny, J.: 风险厌恶的鲁棒对抗性强化学习。在：2019
    年国际机器人与自动化会议 (ICRA) (2019)'
- en: 'Ma et al. [2018] Ma, X., Driggs-Campbell, K., Kochenderfer, M.J.: Improved
    robustness and safety for autonomous vehicle control with adversarial reinforcement
    learning. In: 2018 IEEE Intelligent Vehicles Symposium (IV) (2018). IEEE'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等 [2018] Ma, X., Driggs-Campbell, K., Kochenderfer, M.J.: 通过对抗性强化学习提高自动驾驶控制的鲁棒性和安全性。在：2018
    年 IEEE 智能车辆研讨会 (IV) (2018)。IEEE'
- en: 'Heinrich et al. [2015] Heinrich, J., Lanctot, M., Silver, D.: Fictitious self-play
    in extensive-form games. In: International Conference on Machine Learning (2015).
    PMLR'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Heinrich 等 [2015] Heinrich, J., Lanctot, M., Silver, D.: 广泛形式游戏中的虚构自我对弈。发表于：国际机器学习会议
    (2015)。PMLR'
- en: 'Heinrich and Silver [2016] Heinrich, J., Silver, D.: Deep reinforcement learning
    from self-play in imperfect-information games. arXiv preprint arXiv:1603.01121
    (2016)'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Heinrich 和 Silver [2016] Heinrich, J., Silver, D.: 从自我对弈中学习深度强化学习。arXiv 预印本
    arXiv:1603.01121 (2016)'
- en: 'Tessler et al. [2019] Tessler, C., Efroni, Y., Mannor, S.: Action robust reinforcement
    learning and applications in continuous control. In: Proceedings of the 36th International
    Conference on Machine Learning. PMLR, ??? (2019). [https://proceedings.mlr.press/v97/tessler19a.html](https://proceedings.mlr.press/v97/tessler19a.html)
    Accessed 2022-10-24'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tessler 等 [2019] Tessler, C., Efroni, Y., Mannor, S.: 动作鲁棒强化学习及其在连续控制中的应用。发表于：第36届国际机器学习会议。PMLR,
    ??? (2019)。[https://proceedings.mlr.press/v97/tessler19a.html](https://proceedings.mlr.press/v97/tessler19a.html)
    访问时间 2022-10-24'
- en: 'Gleave et al. [2021] Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S.,
    Russell, S.: Adversarial Policies: Attacking Deep Reinforcement Learning. arXiv
    (2021). [http://arxiv.org/abs/1905.10615](http://arxiv.org/abs/1905.10615) Accessed
    2022-11-09'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gleave 等 [2021] Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S., Russell,
    S.: 对抗策略：攻击深度强化学习。arXiv (2021)。[http://arxiv.org/abs/1905.10615](http://arxiv.org/abs/1905.10615)
    访问时间 2022-11-09'
- en: 'Wang et al. [2022] Wang, T.T., Gleave, A., Belrose, N., Tseng, T., Miller,
    J., Dennis, M.D., Duan, Y., Pogrebniak, V., Levine, S., Russell, S.: Adversarial
    Policies Beat Professional-Level Go AIs. arXiv (2022). [http://arxiv.org/abs/2211.00241](http://arxiv.org/abs/2211.00241)
    Accessed 2022-11-23'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2022] Wang, T.T., Gleave, A., Belrose, N., Tseng, T., Miller, J., Dennis,
    M.D., Duan, Y., Pogrebniak, V., Levine, S., Russell, S.: 对抗策略击败专业级围棋 AI。arXiv
    (2022)。[http://arxiv.org/abs/2211.00241](http://arxiv.org/abs/2211.00241) 访问时间
    2022-11-23'
- en: 'Wu et al. [2021] Wu, X., Guo, W., Wei, H., Xing, X.: Adversarial policy training
    against deep reinforcement learning. In: 30th USENIX Security Symposium (USENIX
    Security 21) (2021)'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等 [2021] Wu, X., Guo, W., Wei, H., Xing, X.: 针对深度强化学习的对抗策略训练。发表于：第30届 USENIX
    安全研讨会 (USENIX Security 21) (2021)'
- en: 'Timbers et al. [2022] Timbers, F., Bard, N., Lockhart, E., Lanctot, M., Schmid,
    M., Burch, N., Schrittwieser, J., Hubert, T., Bowling, M.: Approximate exploitability:
    learning a best response. In: Proceedings of the International Joint Conference
    on Artificial Intelligence (IJCAI) (2022)'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Timbers 等 [2022] Timbers, F., Bard, N., Lockhart, E., Lanctot, M., Schmid,
    M., Burch, N., Schrittwieser, J., Hubert, T., Bowling, M.: 近似可利用性：学习最佳回应。发表于：国际联合人工智能会议
    (IJCAI) (2022)'
- en: 'Casper et al. [2022] Casper, S., Killian, T., Kreiman, G., Hadfield-Menell,
    D.: Red teaming with mind reading: White-box adversarial policies against rl agents.
    arXiv preprint arXiv:2209.02167 (2022)'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Casper 等 [2022] Casper, S., Killian, T., Kreiman, G., Hadfield-Menell, D.:
    带有心灵读取的红队攻击：针对强化学习代理的白盒对抗策略。arXiv 预印本 arXiv:2209.02167 (2022)'
- en: 'Lee et al. [2020] Lee, X.Y., Ghadai, S., Tan, K.L., Hegde, C., Sarkar, S.:
    Spatiotemporally constrained action space attacks on deep reinforcement learning
    agents. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol.
    34, pp. 4577–4584 (2020)'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等 [2020] Lee, X.Y., Ghadai, S., Tan, K.L., Hegde, C., Sarkar, S.: 对深度强化学习代理的时空约束动作空间攻击。发表于：AAAI
    人工智能会议论文集，第34卷，第4577–4584页 (2020)'
- en: 'Tan et al. [2020] Tan, K.L., Esfandiari, Y., Lee, X.Y., Sarkar, S., et al.:
    Robustifying reinforcement learning agents via action space adversarial training.
    In: 2020 American Control Conference (ACC), pp. 3959–3964 (2020). IEEE'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan 等 [2020] Tan, K.L., Esfandiari, Y., Lee, X.Y., Sarkar, S., 等.: 通过动作空间对抗训练来增强强化学习代理的鲁棒性。发表于：2020
    美国控制会议 (ACC)，第3959–3964页 (2020)。IEEE'
- en: 'Schott et al. [2022] Schott, L., Hajri, H., Lamprier, S.: Improving robustness
    of deep reinforcement learning agents: Environment attack based on the critic
    network. In: 2022 International Joint Conference on Neural Networks (IJCNN) (2022).
    [http://arxiv.org/abs/2104.03154](http://arxiv.org/abs/2104.03154) Accessed 2022-11-08'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schott 等 [2022] Schott, L., Hajri, H., Lamprier, S.: 提高深度强化学习代理的鲁棒性：基于评估网络的环境攻击。发表于：2022
    国际联合神经网络会议 (IJCNN) (2022)。[http://arxiv.org/abs/2104.03154](http://arxiv.org/abs/2104.03154)
    访问时间 2022-11-08'
- en: 'Bai et al. [2018] Bai, X., Niu, W., Liu, J., Gao, X., Xiang, Y., Liu, J.: Adversarial
    examples construction towards white-box q table variation in dqn pathfinding training.
    In: 2018 IEEE Third International Conference on Data Science in Cyberspace (DSC)
    (2018)'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等 [2018] Bai, X., Niu, W., Liu, J., Gao, X., Xiang, Y., Liu, J.: 针对 DQN
    路径规划训练中的白盒 Q 表变化的对抗样本构造。发表于：2018 IEEE 第三届网络空间数据科学国际会议 (DSC) (2018)'
- en: 'Chen et al. [2018] Chen, T., Niu, W., Xiang, Y., Bai, X., Liu, J., Han, Z.,
    Li, G.: Gradient Band-based Adversarial Training for Generalized Attack Immunity
    of A3C Path Finding. arXiv (2018). [http://arxiv.org/abs/1807.06752](http://arxiv.org/abs/1807.06752)
    Accessed 2022-10-20'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2018] Chen, T., Niu, W., Xiang, Y., Bai, X., Liu, J., Han, Z., Li,
    G.: 基于梯度带的对抗训练用于A3C路径寻找的通用攻击免疫。arXiv (2018). [http://arxiv.org/abs/1807.06752](http://arxiv.org/abs/1807.06752)
    访问时间 2022-10-20'
- en: 'Goodfellow et al. [2015] Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining
    and Harnessing Adversarial Examples. arXiv (2015). [http://arxiv.org/abs/1412.6572](http://arxiv.org/abs/1412.6572)
    Accessed 2022-12-05'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goodfellow 等 [2015] Goodfellow, I.J., Shlens, J., Szegedy, C.: 解释和利用对抗样本。arXiv
    (2015). [http://arxiv.org/abs/1412.6572](http://arxiv.org/abs/1412.6572) 访问时间
    2022-12-05'
- en: 'Behzadan and Hsu [2019] Behzadan, V., Hsu, W.: Adversarial Exploitation of
    Policy Imitation. arXiv (2019). [http://arxiv.org/abs/1906.01121](http://arxiv.org/abs/1906.01121)
    Accessed 2022-10-17'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Behzadan 和 Hsu [2019] Behzadan, V., Hsu, W.: 策略模仿的对抗性利用。arXiv (2019). [http://arxiv.org/abs/1906.01121](http://arxiv.org/abs/1906.01121)
    访问时间 2022-10-17'
- en: 'Kos and Song [2017] Kos, J., Song, D.: Delving into adversarial attacks on
    deep policies. arXiv (2017). [http://arxiv.org/abs/1705.06452](http://arxiv.org/abs/1705.06452)
    Accessed 2022-11-22'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kos 和 Song [2017] Kos, J., Song, D.: 深入研究对深度策略的对抗攻击。arXiv (2017). [http://arxiv.org/abs/1705.06452](http://arxiv.org/abs/1705.06452)
    访问时间 2022-11-22'
- en: 'Lin et al. [2019] Lin, Y.-C., Hong, Z.-W., Liao, Y.-H., Shih, M.-L., Liu, M.-Y.,
    Sun, M.: Tactics of Adversarial Attack on Deep Reinforcement Learning Agents.
    arXiv (2019). [http://arxiv.org/abs/1703.06748](http://arxiv.org/abs/1703.06748)
    Accessed 2022-10-14'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 [2019] Lin, Y.-C., Hong, Z.-W., Liao, Y.-H., Shih, M.-L., Liu, M.-Y.,
    Sun, M.: 针对深度强化学习代理的对抗攻击策略。arXiv (2019). [http://arxiv.org/abs/1703.06748](http://arxiv.org/abs/1703.06748)
    访问时间 2022-10-14'
- en: 'Yang et al. [2020] Yang, C.-H.H., Qi, J., Chen, P.-Y., Ouyang, Y., Hung, I.-T.D.,
    Lee, C.-H., Ma, X.: Enhanced adversarial strategically-timed attacks against deep
    reinforcement learning. In: ICASSP 2020 - 2020 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP) (2020). [http://arxiv.org/abs/2002.09027](http://arxiv.org/abs/2002.09027)
    Accessed 2022-10-17'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等 [2020] Yang, C.-H.H., Qi, J., Chen, P.-Y., Ouyang, Y., Hung, I.-T.D.,
    Lee, C.-H., Ma, X.: 增强的对抗性战略时机攻击针对深度强化学习。在: ICASSP 2020 - 2020 IEEE国际声学、语音与信号处理会议
    (ICASSP) (2020). [http://arxiv.org/abs/2002.09027](http://arxiv.org/abs/2002.09027)
    访问时间 2022-10-17'
- en: 'Sun et al. [2020] Sun, J., Zhang, T., Xie, X., Ma, L., Zheng, Y., Chen, K.,
    Liu, Y.: Stealthy and Efficient Adversarial Attacks against Deep Reinforcement
    Learning. arXiv (2020). [http://arxiv.org/abs/2005.07099](http://arxiv.org/abs/2005.07099)
    Accessed 2022-11-22'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 [2020] Sun, J., Zhang, T., Xie, X., Ma, L., Zheng, Y., Chen, K., Liu,
    Y.: 隐蔽且高效的针对深度强化学习的对抗攻击。arXiv (2020). [http://arxiv.org/abs/2005.07099](http://arxiv.org/abs/2005.07099)
    访问时间 2022-11-22'
- en: 'Chan et al. [2020] Chan, P.P.K., Wang, Y., Yeung, D.S.: Adversarial attack
    against deep reinforcement learning with static reward impact map. In: Proceedings
    of the 15th ACM Asia Conference on Computer and Communications Security. Association
    for Computing Machinery, ??? (2020). [https://doi.org/10.1145/3320269.3384715](https://doi.org/10.1145/3320269.3384715)
    Accessed 2022-11-22'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chan 等 [2020] Chan, P.P.K., Wang, Y., Yeung, D.S.: 针对深度强化学习的静态奖励影响图的对抗攻击。在:
    第15届ACM亚洲计算机与通信安全会议论文集。计算机协会，??? (2020). [https://doi.org/10.1145/3320269.3384715](https://doi.org/10.1145/3320269.3384715)
    访问时间 2022-11-22'
- en: 'Qu et al. [2021] Qu, X., Sun, Z., Ong, Y.-S., Gupta, A., Wei, P.: Minimalistic
    attacks: How little it takes to fool deep reinforcement learning policies. IEEE
    Transactions on Cognitive and Developmental Systems (2021)'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qu 等 [2021] Qu, X., Sun, Z., Ong, Y.-S., Gupta, A., Wei, P.: 最小化攻击：欺骗深度强化学习策略所需的最小成本。IEEE
    Transactions on Cognitive and Developmental Systems (2021)'
- en: 'Hussenot et al. [2020] Hussenot, L., Geist, M., Pietquin, O.: CopyCAT: Taking
    Control of Neural Policies with Constant Attacks. arXiv (2020). [http://arxiv.org/abs/1905.12282](http://arxiv.org/abs/1905.12282)
    Accessed 2022-10-20'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hussenot 等 [2020] Hussenot, L., Geist, M., Pietquin, O.: CopyCAT：通过恒定攻击控制神经策略。arXiv
    (2020). [http://arxiv.org/abs/1905.12282](http://arxiv.org/abs/1905.12282) 访问时间
    2022-10-20'
- en: 'Mo et al. [2022] Mo, K., Tang, W., Li, J., Yuan, X.: Attacking deep reinforcement
    learning with decoupled adversarial policy. IEEE Transactions on Dependable and
    Secure Computing (2022)'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mo 等 [2022] Mo, K., Tang, W., Li, J., Yuan, X.: 通过解耦对抗策略攻击深度强化学习。IEEE Transactions
    on Dependable and Secure Computing (2022)'
- en: 'Tekgul et al. [2022] Tekgul, B.G.A., Wang, S., Marchal, S., Asokan, N.: Real-time
    Adversarial Perturbations against Deep Reinforcement Learning Policies: Attacks
    and Defenses. arXiv (2022). [http://arxiv.org/abs/2106.08746](http://arxiv.org/abs/2106.08746)
    Accessed 2022-10-24'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tekgul 等 [2022] Tekgul, B.G.A., Wang, S., Marchal, S., Asokan, N.: 针对深度强化学习策略的实时对抗扰动：攻击与防御。arXiv（2022年）。[http://arxiv.org/abs/2106.08746](http://arxiv.org/abs/2106.08746)
    访问日期 2022-10-24'
- en: 'Moosavi-Dezfooli et al. [2017] Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O.,
    Frossard, P.: Universal adversarial perturbations. arXiv (2017). [http://arxiv.org/abs/1610.08401](http://arxiv.org/abs/1610.08401)
    Accessed 2023-01-12'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moosavi-Dezfooli 等 [2017] Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., Frossard,
    P.: 通用对抗扰动。arXiv（2017年）。[http://arxiv.org/abs/1610.08401](http://arxiv.org/abs/1610.08401)
    访问日期 2023-01-12'
- en: 'Dorato [1987] Dorato, P.: A historical review of robust control. IEEE Control
    Systems Magazine 7(2), 44–47 (1987)'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dorato [1987] Dorato, P.: 鲁棒控制的历史回顾。IEEE 控制系统杂志 7(2)，44-47（1987年）'
- en: 'Wu [2017] Wu, D.J.: Is attacking machine learning easier than defending it?
    (2017). [http://cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html](http://cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html)
    Accessed 2023-01-23'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu [2017] Wu, D.J.: 攻击机器学习比防御更容易吗？（2017年）。[http://cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html](http://cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html)
    访问日期 2023-01-23'
- en: 'Huang et al. [2021] Huang, H., Wang, Y., Erfani, S., Gu, Q., Bailey, J., Ma,
    X.: Exploring architectural ingredients of adversarially robust deep neural networks.
    In: Advances in Neural Information Processing Systems. Curran Associates, Inc.,
    ??? (2021). [https://proceedings.neurips.cc/paper/2021/hash/2bd7f907b7f5b6bbd91822c0c7b835f6-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/2bd7f907b7f5b6bbd91822c0c7b835f6-Abstract.html)
    Accessed 2023-01-23'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等 [2021] Huang, H., Wang, Y., Erfani, S., Gu, Q., Bailey, J., Ma, X.:
    探索对抗性鲁棒深度神经网络的结构成分。载于：神经信息处理系统进展。Curran Associates, Inc., ???（2021年）。[https://proceedings.neurips.cc/paper/2021/hash/2bd7f907b7f5b6bbd91822c0c7b835f6-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/2bd7f907b7f5b6bbd91822c0c7b835f6-Abstract.html)
    访问日期 2023-01-23'
- en: 'Papernot et al. [2016] Papernot, N., McDaniel, P., Wu, X., Jha, S., Swami,
    A.: Distillation as a Defense to Adversarial Perturbations against Deep Neural
    Networks. arXiv (2016). [http://arxiv.org/abs/1511.04508](http://arxiv.org/abs/1511.04508)
    Accessed 2023-02-14'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papernot 等 [2016] Papernot, N., McDaniel, P., Wu, X., Jha, S., Swami, A.: 作为对抗性扰动防御的蒸馏方法。arXiv（2016年）。[http://arxiv.org/abs/1511.04508](http://arxiv.org/abs/1511.04508)
    访问日期 2023-02-14'
- en: 'Rusu et al. [2016] Rusu, A.A., Colmenarejo, S.G., Gulcehre, C., Desjardins,
    G., Kirkpatrick, J., Pascanu, R., Mnih, V., Kavukcuoglu, K., Hadsell, R.: Policy
    Distillation. arXiv (2016). [http://arxiv.org/abs/1511.06295](http://arxiv.org/abs/1511.06295)
    Accessed 2023-01-23'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rusu 等 [2016] Rusu, A.A., Colmenarejo, S.G., Gulcehre, C., Desjardins, G.,
    Kirkpatrick, J., Pascanu, R., Mnih, V., Kavukcuoglu, K., Hadsell, R.: 策略蒸馏。arXiv（2016年）。[http://arxiv.org/abs/1511.06295](http://arxiv.org/abs/1511.06295)
    访问日期 2023-01-23'
- en: 'Czarnecki et al. [2019] Czarnecki, W.M., Pascanu, R., Osindero, S., Jayakumar,
    S., Swirszcz, G., Jaderberg, M.: Distilling policy distillation. In: Proceedings
    of the Twenty-Second International Conference on Artificial Intelligence And Statistics.
    PMLR, ??? (2019). [https://proceedings.mlr.press/v89/czarnecki19a.html](https://proceedings.mlr.press/v89/czarnecki19a.html)
    Accessed 2023-01-23'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Czarnecki 等 [2019] Czarnecki, W.M., Pascanu, R., Osindero, S., Jayakumar, S.,
    Swirszcz, G., Jaderberg, M.: 策略蒸馏。载于：第二十二届国际人工智能与统计会议论文集。PMLR，???（2019年）。[https://proceedings.mlr.press/v89/czarnecki19a.html](https://proceedings.mlr.press/v89/czarnecki19a.html)
    访问日期 2023-01-23'
- en: 'Wierstra et al. [2007] Wierstra, D., Foerster, A., Peters, J., Schmidhuber,
    J.: Solving deep memory pomdps with recurrent policy gradients. In: Artificial
    Neural Networks–ICANN 2007: 17th International Conference, Porto, Portugal, September
    9-13, 2007, Proceedings, Part I 17, pp. 697–706 (2007). Springer'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wierstra 等 [2007] Wierstra, D., Foerster, A., Peters, J., Schmidhuber, J.:
    使用递归策略梯度解决深度记忆pomdp。载于：人工神经网络–ICANN 2007：第17届国际会议，葡萄牙波尔图，2007年9月9-13日，论文集，第17部分，第697-706页（2007年）。施普林格'
- en: 'Kumar [2019] Kumar, A.: Enhancing performance of reinforcement learning models
    in the presence of noisy rewards. Thesis (April 2019). [https://repositories.lib.utexas.edu/handle/2152/75758](https://repositories.lib.utexas.edu/handle/2152/75758)
    Accessed 2023-01-23'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumar [2019] Kumar, A.: 在有噪声奖励的情况下提升强化学习模型性能。论文（2019年4月）。[https://repositories.lib.utexas.edu/handle/2152/75758](https://repositories.lib.utexas.edu/handle/2152/75758)
    访问日期 2023-01-23'
- en: 'Wang et al. [2020] Wang, J., Liu, Y., Li, B.: Reinforcement Learning with Perturbed
    Rewards. arXiv (2020). [http://arxiv.org/abs/1810.01032](http://arxiv.org/abs/1810.01032)
    Accessed 2023-01-23'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2020] Wang, J., Liu, Y., Li, B.: 具有扰动奖励的强化学习。arXiv (2020)。 [http://arxiv.org/abs/1810.01032](http://arxiv.org/abs/1810.01032)
    访问日期 2023-01-23'
- en: 'Liang et al. [2022] Liang, Y., Sun, Y., Zheng, R., Huang, F.: Efficient adversarial
    training without attacking: Worst-case-aware robust reinforcement learning. Advances
    in Neural Information Processing Systems 35, 22547–22561 (2022)'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等 [2022] Liang, Y., Sun, Y., Zheng, R., Huang, F.: 高效的对抗训练无需攻击：最坏情况感知的鲁棒强化学习。神经信息处理系统进展
    35, 22547–22561 (2022)'
- en: 'Queeney and Benosman [2023] Queeney, J., Benosman, M.: Risk-averse model uncertainty
    for distributionally robust safe reinforcement learning. arXiv preprint arXiv:2301.12593
    (2023)'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Queeney 和 Benosman [2023] Queeney, J., Benosman, M.: 面向分布鲁棒安全强化学习的风险厌恶模型不确定性。arXiv
    预印本 arXiv:2301.12593 (2023)'
- en: 'Lin et al. [2017] Lin, Y.-C., Liu, M.-Y., Sun, M., Huang, J.-B.: Detecting
    Adversarial Attacks on Neural Network Policies with Visual Foresight. arXiv (2017).
    [http://arxiv.org/abs/1710.00814](http://arxiv.org/abs/1710.00814) Accessed 2023-01-23'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 [2017] Lin, Y.-C., Liu, M.-Y., Sun, M., Huang, J.-B.: 通过视觉预见检测神经网络策略上的对抗攻击。arXiv
    (2017)。 [http://arxiv.org/abs/1710.00814](http://arxiv.org/abs/1710.00814) 访问日期
    2023-01-23'
- en: 'Hickling et al. [2022] Hickling, T., Aouf, N., Spencer, P.: Robust Adversarial
    Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance
    and Planning. arXiv (2022). [http://arxiv.org/abs/2206.02670](http://arxiv.org/abs/2206.02670)
    Accessed 2023-01-23'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hickling 等 [2022] Hickling, T., Aouf, N., Spencer, P.: 基于可解释的深度强化学习的鲁棒对抗攻击检测用于无人机导航与规划。arXiv
    (2022)。 [http://arxiv.org/abs/2206.02670](http://arxiv.org/abs/2206.02670) 访问日期
    2023-01-23'
- en: 'Metzen et al. [2017] Metzen, J.H., Genewein, T., Fischer, V., Bischoff, B.:
    On Detecting Adversarial Perturbations. arXiv (2017). [http://arxiv.org/abs/1702.04267](http://arxiv.org/abs/1702.04267)
    Accessed 2023-01-23'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Metzen 等 [2017] Metzen, J.H., Genewein, T., Fischer, V., Bischoff, B.: 关于检测对抗性扰动。arXiv
    (2017)。 [http://arxiv.org/abs/1702.04267](http://arxiv.org/abs/1702.04267) 访问日期
    2023-01-23'
- en: 'Pang et al. [2018] Pang, T., Du, C., Dong, Y., Zhu, J.: Towards robust detection
    of adversarial examples. In: Advances in Neural Information Processing Systems.
    Curran Associates, Inc., ??? (2018). [https://proceedings.neurips.cc/paper/2018/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html)
    Accessed 2023-01-23'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pang 等 [2018] Pang, T., Du, C., Dong, Y., Zhu, J.: 朝着对抗样本的鲁棒检测迈进。在：神经信息处理系统进展。Curran
    Associates, Inc., ??? (2018)。 [https://proceedings.neurips.cc/paper/2018/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html)
    访问日期 2023-01-23'
- en: 'Carlini and Wagner [2017] Carlini, N., Wagner, D.: Adversarial Examples Are
    Not Easily Detected: Bypassing Ten Detection Methods. arXiv (2017). [http://arxiv.org/abs/1705.07263](http://arxiv.org/abs/1705.07263)
    Accessed 2023-02-15'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carlini 和 Wagner [2017] Carlini, N., Wagner, D.: 对抗样本并不容易被检测到：绕过十种检测方法。arXiv
    (2017)。 [http://arxiv.org/abs/1705.07263](http://arxiv.org/abs/1705.07263) 访问日期
    2023-02-15'
- en: 'He et al. [2017] He, W., Wei, J., Chen, X., Carlini, N., Song, D.: Adversarial
    example defense: Ensembles of weak defenses are not strong. In: 11th USENIX Workshop
    on Offensive Technologies (WOOT 17) (2017)'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等 [2017] He, W., Wei, J., Chen, X., Carlini, N., Song, D.: 对抗样本防御：弱防御的集成并不强。
    In: 第11届USENIX进攻技术研讨会 (WOOT 17) (2017)'
- en: 'Behzadan and Hsu [2019] Behzadan, V., Hsu, W.: RL-Based Method for Benchmarking
    the Adversarial Resilience and Robustness of Deep Reinforcement Learning Policies.
    arXiv (2019). [http://arxiv.org/abs/1906.01110](http://arxiv.org/abs/1906.01110)
    Accessed 2023-01-26'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Behzadan 和 Hsu [2019] Behzadan, V., Hsu, W.: 基于强化学习的深度强化学习策略的对抗弹性和鲁棒性基准测试方法。arXiv
    (2019)。 [http://arxiv.org/abs/1906.01110](http://arxiv.org/abs/1906.01110) 访问日期
    2023-01-26'
- en: 'Behzadan and Munir [2018] Behzadan, V., Munir, A.: Adversarial Reinforcement
    Learning Framework for Benchmarking Collision Avoidance Mechanisms in Autonomous
    Vehicles. arXiv (2018). [http://arxiv.org/abs/1806.01368](http://arxiv.org/abs/1806.01368)
    Accessed 2023-01-26'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Behzadan 和 Munir [2018] Behzadan, V., Munir, A.: 用于基准测试自主车辆碰撞规避机制的对抗性强化学习框架。arXiv
    (2018)。 [http://arxiv.org/abs/1806.01368](http://arxiv.org/abs/1806.01368) 访问日期
    2023-01-26'
- en: 'Behzadan and Hsu [2019] Behzadan, V., Hsu, W.: Sequential Triggers for Watermarking
    of Deep Reinforcement Learning Policies. arXiv (2019). [http://arxiv.org/abs/1906.01126](http://arxiv.org/abs/1906.01126)
    Accessed 2023-01-26'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Behzadan 和 Hsu [2019] Behzadan, V., Hsu, W.: 深度强化学习策略的水印序列触发器。arXiv (2019)。
    [http://arxiv.org/abs/1906.01126](http://arxiv.org/abs/1906.01126) 访问日期 2023-01-26'
- en: 'Raffin et al. [2021] Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus,
    M., Dormann, N.: Stable-baselines3: Reliable reinforcement learning implementations.
    Journal of Machine Learning Research (2021). Accessed 2023-01-26'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raffin 等人 [2021] Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus,
    M., Dormann, N.: Stable-baselines3: 可靠的强化学习实现。机器学习研究期刊 (2021)。访问日期 2023-01-26'
- en: 'Weng et al. [2022] Weng, J., Chen, H., Yan, D., You, K., Duburcq, A., Zhang,
    M., Su, Y., Su, H., Zhu, J.: Tianshou: a Highly Modularized Deep Reinforcement
    Learning Library. arXiv (2022). [http://arxiv.org/abs/2107.14171](http://arxiv.org/abs/2107.14171)
    Accessed 2023-01-26'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Weng 等人 [2022] Weng, J., Chen, H., Yan, D., You, K., Duburcq, A., Zhang, M.,
    Su, Y., Su, H., Zhu, J.: Tianshou：一个高度模块化的深度强化学习库。arXiv (2022)。[http://arxiv.org/abs/2107.14171](http://arxiv.org/abs/2107.14171)
    访问日期 2023-01-26'
- en: 'Behzadan and Munir [2017] Behzadan, V., Munir, A.: Whatever Does Not Kill Deep
    Reinforcement Learning, Makes It Stronger. arXiv (2017). [http://arxiv.org/abs/1712.09344](http://arxiv.org/abs/1712.09344)
    Accessed 2023-01-26'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Behzadan 和 Munir [2017] Behzadan, V., Munir, A.: 什么没有击垮深度强化学习，就让它变得更强。arXiv
    (2017)。[http://arxiv.org/abs/1712.09344](http://arxiv.org/abs/1712.09344) 访问日期
    2023-01-26'
- en: 'Papernot et al. [2018] Papernot, N., Faghri, F., Carlini, N., Goodfellow, I.,
    Feinman, R., Kurakin, A., Xie, C., Sharma, Y., Brown, T., Roy, A., Matyasko, A.,
    Behzadan, V., Hambardzumyan, K., Zhang, Z., Juang, Y.-L., Li, Z., Sheatsley, R.,
    Garg, A., Uesato, J., Gierke, W., Dong, Y., Berthelot, D., Hendricks, P., Rauber,
    J., Long, R., McDaniel, P.: Technical Report on the CleverHans v2.1.0 Adversarial
    Examples Library. arXiv (2018). [http://arxiv.org/abs/1610.00768](http://arxiv.org/abs/1610.00768)
    Accessed 2023-01-26'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papernot 等人 [2018] Papernot, N., Faghri, F., Carlini, N., Goodfellow, I., Feinman,
    R., Kurakin, A., Xie, C., Sharma, Y., Brown, T., Roy, A., Matyasko, A., Behzadan,
    V., Hambardzumyan, K., Zhang, Z., Juang, Y.-L., Li, Z., Sheatsley, R., Garg, A.,
    Uesato, J., Gierke, W., Dong, Y., Berthelot, D., Hendricks, P., Rauber, J., Long,
    R., McDaniel, P.: CleverHans v2.1.0 对抗性示例库技术报告。arXiv (2018)。[http://arxiv.org/abs/1610.00768](http://arxiv.org/abs/1610.00768)
    访问日期 2023-01-26'
- en: 'Yuan et al. [2022] Yuan, Z., Hall, A.W., Zhou, S., Brunke, L., Greeff, M.,
    Panerati, J., Schoellig, A.P.: safe-control-gym: a Unified Benchmark Suite for
    Safe Learning-based Control and Reinforcement Learning in Robotics. arXiv (2022).
    [http://arxiv.org/abs/2109.06325](http://arxiv.org/abs/2109.06325) Accessed 2023-01-26'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等人 [2022] Yuan, Z., Hall, A.W., Zhou, S., Brunke, L., Greeff, M., Panerati,
    J., Schoellig, A.P.: safe-control-gym：一个统一的安全学习控制和机器人强化学习基准套件。arXiv (2022)。[http://arxiv.org/abs/2109.06325](http://arxiv.org/abs/2109.06325)
    访问日期 2023-01-26'
- en: 'Heuillet et al. [2021] Heuillet, A., Couthouis, F., Díaz-Rodríguez, N.: Explainability
    in deep reinforcement learning. Knowledge-Based Systems (2021). Accessed 2023-01-27'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Heuillet 等人 [2021] Heuillet, A., Couthouis, F., Díaz-Rodríguez, N.: 深度强化学习中的解释性。知识基础系统
    (2021)。访问日期 2023-01-27'
- en: 'Vouros [2022] Vouros, G.A.: Explainable deep reinforcement learning: State
    of the art and challenges. ACM Computing Surveys (2022). Accessed 2023-01-27'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vouros [2022] Vouros, G.A.: 可解释的深度强化学习：现状与挑战。ACM计算机调查 (2022)。访问日期 2023-01-27'
- en: 'Datta et al. [2021] Datta, A., Fredrikson, M., Leino, K., Lu, K., Sen, S.,
    Wang, Z.: Machine learning explainability and robustness: Connected at the hip.
    In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
    Mining. Association for Computing Machinery, ??? (2021). [https://doi.org/10.1145/3447548.3470806](https://doi.org/10.1145/3447548.3470806)
    Accessed 2023-01-27'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Datta 等人 [2021] Datta, A., Fredrikson, M., Leino, K., Lu, K., Sen, S., Wang,
    Z.: 机器学习的可解释性和稳健性：紧密相连。收录于第27届ACM SIGKDD知识发现与数据挖掘会议论文集。计算机协会，??? (2021)。[https://doi.org/10.1145/3447548.3470806](https://doi.org/10.1145/3447548.3470806)
    访问日期 2023-01-27'
- en: 'Sun et al. [2018] Sun, L., Tan, M., Zhou, Z.: A survey of practical adversarial
    example attacks. Cybersecurity (2018). Accessed 2023-01-27'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人 [2018] Sun, L., Tan, M., Zhou, Z.: 实用对抗性示例攻击的综述。网络安全 (2018)。访问日期 2023-01-27'
