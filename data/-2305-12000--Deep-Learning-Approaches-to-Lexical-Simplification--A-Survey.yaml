- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:39:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:39:30'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.12000] Deep Learning Approaches to Lexical Simplification: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.12000] 深度学习方法在词汇简化中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.12000](https://ar5iv.labs.arxiv.org/html/2305.12000)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.12000](https://ar5iv.labs.arxiv.org/html/2305.12000)
- en: 'Deep Learning Approaches to Lexical Simplification: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习方法在词汇简化中的应用：综述
- en: Kai North¹, Tharindu Ranasinghe², Matthew Shardlow³, Marcos Zampieri¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kai North¹, Tharindu Ranasinghe², Matthew Shardlow³, Marcos Zampieri¹
- en: ¹George Mason University, USA, ²Aston University, UK
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹乔治·梅森大学，美国，²阿斯顿大学，英国
- en: ³Manchester Metropolitan University, UK
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³曼彻斯特城市大学，英国
- en: knorth8@gmu.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: knorth8@gmu.edu
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Lexical Simplification (LS) is the task of replacing complex for simpler words
    in a sentence whilst preserving the sentence’s original meaning. LS is the lexical
    component of Text Simplification (TS) with the aim of making texts more accessible
    to various target populations. A past survey Paetzold and Specia ([2017b](#bib.bib38))
    has provided a detailed overview of LS. Since this survey, however, the AI/NLP
    community has been taken by storm by recent advances in deep learning, particularly
    with the introduction of large language models (LLM) and prompt learning. The
    high performance of these models sparked renewed interest in LS. To reflect these
    recent advances, we present a comprehensive survey of papers published between
    2017 and 2023 on LS and its sub-tasks with a special focus on deep learning. We
    also present benchmark datasets for the future development of LS systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**词汇简化（LS）**的任务是将句子中的复杂词汇替换为更简单的词汇，同时保持句子的原始意义。**词汇简化（LS）**是**文本简化（TS）**的词汇组件，旨在使文本对各种目标人群更加可访问。一项过去的调查
    Paetzold 和 Specia ([2017b](#bib.bib38)) 提供了**词汇简化（LS）**的详细概述。然而，自此调查以来，AI/NLP
    社区因深度学习的最新进展，特别是大型语言模型（LLM）和提示学习的引入而发生了巨大变化。这些模型的高性能激发了对**词汇简化（LS）**的新兴趣。为了反映这些最新进展，我们提供了对2017年至2023年间关于**词汇简化（LS）**及其子任务的论文的全面综述，特别关注深度学习。我们还提供了**词汇简化（LS）**系统未来发展的基准数据集。'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: LS improves the readability of any given text with the aim of helping vocabulary
    and literacy development. LS achieves this by replacing complex words in a sentence
    with simpler alternatives. LS returns a simplified sentence which can be passed
    to a TS system for further syntactic and grammatical simplification. The replaced
    complex words are those words which a general or targeted population found to
    be hard to read, interpret, or understand. Previous LS systems have been designed
    to simplify complex words for children, second language learners, individuals
    with reading disabilities or low-literacy Paetzold and Specia ([2017b](#bib.bib38)).
    LS therefore provides both developers and users with a degree of personalization
    that is unattainable through seq2seq or generative TS systems Yeung and Lee ([2018](#bib.bib59));
    Lee and Yeung ([2018a](#bib.bib24)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**词汇简化（LS）**旨在提高任何给定文本的可读性，以帮助词汇和读写能力的发展。**词汇简化（LS）**通过用更简单的替代词替换句子中的复杂词汇来实现这一目标。**词汇简化（LS）**返回一个简化的句子，可以传递给**文本简化（TS）**系统进行进一步的句法和语法简化。被替换的复杂词汇是指一般或特定人群认为难以阅读、解释或理解的词汇。以前的**词汇简化（LS）**系统旨在为儿童、第二语言学习者、阅读障碍或低读写能力的个人简化复杂词汇
    Paetzold 和 Specia ([2017b](#bib.bib38))。因此，**词汇简化（LS）**为开发者和用户提供了一种通过seq2seq或生成式**文本简化（TS）**系统无法实现的个性化程度
    Yeung 和 Lee ([2018](#bib.bib59)); Lee 和 Yeung ([2018a](#bib.bib24))。'
- en: 'Deep learning, and latterly, LLM and prompt learning, have revolutionized the
    way we approach many NLP tasks, including LS. Previous LS systems have relied
    upon lexicons, rule-based, statistical, n-gram, and word embedding models to identify
    and then simplify complex words Paetzold and Specia ([2017b](#bib.bib38)). These
    approaches would identify a complex word, for example, “bombardment” as being
    in need of simplification and would suggest “attack” as a suitable alternative
    (Figure [1](#S2.F1 "Figure 1 ‣ 2 Pipeline ‣ Deep Learning Approaches to Lexical
    Simplification: A Survey")), hereby referred to as a candidate substitution.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，尤其是大型语言模型（LLM）和提示学习，已经彻底改变了我们处理许多自然语言处理（NLP）任务的方法，包括**词汇简化（LS）**。以前的**词汇简化（LS）**系统依赖于词汇表、基于规则的、统计学、n-gram
    和词嵌入模型来识别和简化复杂词汇 Paetzold 和 Specia ([2017b](#bib.bib38))。这些方法会识别出复杂词汇，例如“bombardment”，认为其需要简化，并建议“attack”作为合适的替代词（图
    [1](#S2.F1 "图 1 ‣ 2 流程图 ‣ 深度学习方法在词汇简化中的应用：综述")），此处称为候选替换。
- en: State-of-the-art deep learning models, such as BERT Devlin et al. ([2019](#bib.bib15)),
    RoBERTa Liu et al. ([2019](#bib.bib27)), GPT-3 Brown et al. ([2020](#bib.bib10)),
    and others, automatically generate, select, and rank candidate substitutions with
    performances superior to traditional approaches. These include relying on pre-existing
    lexicons, simplification rules, or engineered features Saggion et al. ([2022](#bib.bib46)).
    There have been no surveys published on deep learning approaches for LS. The paper
    by Paetzold and Specia ([2017b](#bib.bib38)) is the most recent survey on LS but
    it precedes studies that demonstrate the headway made by state-of-the-art deep
    learning approaches. A broad comprehensive survey on TS was published in 2021Al-Thanyyan
    and Azmi ([2021](#bib.bib1)). However, this survey likewise does not cover recent
    advances in the field nor does it focus specifically on LS. This paper therefore
    continues pre-existing literature by providing an updated survey of the latest
    deep learning approaches for LS and its sub-tasks of substitute generation (SG),
    selection (SS), and ranking (SR).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的深度学习模型，如BERT Devlin et al. ([2019](#bib.bib15))、RoBERTa Liu et al. ([2019](#bib.bib27))、GPT-3
    Brown et al. ([2020](#bib.bib10))等，能够自动生成、选择和排序候选替代词，其表现优于传统方法。这些传统方法包括依赖于现有词典、简化规则或工程特征
    Saggion et al. ([2022](#bib.bib46))。目前尚无关于深度学习方法用于词汇简化（LS）的综述文章。Paetzold 和 Specia
    ([2017b](#bib.bib38)) 的论文是最近期的词汇简化综述，但它早于展示最先进深度学习方法所取得进展的研究。Al-Thanyyan 和 Azmi
    ([2021](#bib.bib1)) 于2021年发表了一项广泛的综合性文本简化综述。然而，该综述同样未涵盖领域内的最新进展，也未专门聚焦于词汇简化。因此，本论文通过提供最新深度学习方法在词汇简化及其子任务（包括替代词生成（SG）、选择（SS）和排序（SR））的更新综述，继续扩展现有文献。
- en: 2 Pipeline
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 流程
- en: 'We structure this survey around the main components of the LS pipeline: SG,
    SS, and SR (Section [3](#S3 "3 Deep Learning Approaches ‣ Deep Learning Approaches
    to Lexical Simplification: A Survey")). We also provide an overview of recent
    datasets (Section [4](#S4 "4 Resources ‣ Deep Learning Approaches to Lexical Simplification:
    A Survey")), and discuss open challenges in LS (Section [5.1](#S5.SS1 "5.1 Open
    Challenges in LS ‣ 5 Discussion and Conclusion ‣ Deep Learning Approaches to Lexical
    Simplification: A Survey")). Normally, an LS pipeline starts with complex word
    identification (CWI). However, since it is often considered as a standalone precursor,
    we refer the reader to North et al. ([2022b](#bib.bib35)), for a detailed survey
    on CWI methods.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '我们围绕词汇简化流程的主要组成部分：SG、SS和SR（第[3](#S3 "3 Deep Learning Approaches ‣ Deep Learning
    Approaches to Lexical Simplification: A Survey")节）来构建本综述。我们还提供了最近数据集的概述（第[4](#S4
    "4 Resources ‣ Deep Learning Approaches to Lexical Simplification: A Survey")节），并讨论了词汇简化中的开放挑战（第[5.1](#S5.SS1
    "5.1 Open Challenges in LS ‣ 5 Discussion and Conclusion ‣ Deep Learning Approaches
    to Lexical Simplification: A Survey")节）。通常，词汇简化流程从复杂词识别（CWI）开始。然而，由于它通常被视为一个独立的前置步骤，我们建议读者参考
    North et al. ([2022b](#bib.bib35))，以获取关于CWI方法的详细综述。'
- en: '<svg   height="381.9" overflow="visible" version="1.1" width="453.86"><g transform="translate(0,381.9)
    matrix(1 0 0 -1 0 0) translate(108.54,0) translate(0,281.69)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -94.36 -95.32)" fill="#000000"
    stroke="#000000"><g  transform="matrix(1 0 0 -1 0 184.49)"><g transform="matrix(1
    0 0 1 0 178.34)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 39.01 0)"><g transform="matrix(1 0 0 -1 0 178.34)"><g  transform="matrix(1
    0 0 1 0 178.34)"><g  transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1
    0 0 -1 0 172.19)"><g transform="matrix(1 0 0 1 0 175.65)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="110.7"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Complex Sentence</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 187.95)"><g class="ltx_tikzmatrix_col ltx_nopad_l
    ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1
    0 0)">Bombardment by regime forces</text></g></g></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 -88.04 -186.06)" fill="#000000" stroke="#000000"><g transform="matrix(1
    0 0 -1 0 46.635)"><g  transform="matrix(1 0 0 1 0 41.83)"><g  transform="matrix(1
    0 0 -1 0 0)"><g  transform="matrix(1 0 0 -1 0 41.83)"><g transform="matrix(1 0
    0 1 0 41.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g transform="matrix(1 0 0 -1 0 35.68)"><g  transform="matrix(1 0
    0 1 0 39.14)"><g  transform="matrix(1 0 0 -1 0 0)"><foreignobject width="177.61"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Complex Word
    Identification</foreignobject></g></g></g></g></g></g></g></g><g transform="matrix(1
    0 0 1 0 51.44)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 26.23 0)"><foreignobject width="123.61" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CWI: Bombardment</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -73.56 -276.8)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 45.29)"><g transform="matrix(1 0 0 1 0 39.14)"><g
    transform="matrix(1 0 0 -1 6.57 0)"><g  transform="matrix(1 0 0 -1 0 39.14)"><g  transform="matrix(1
    0 0 1 0 39.14)"><g transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1 0
    0 -1 0 34.335)"><g transform="matrix(1 0 0 1 0 39.14)"><g transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="133.97" height="9.61" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">Substitute Generation</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 48.75)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="147.11" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SG: assault, raid, attack</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 165.01 -22.14)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 38.13)"><g  transform="matrix(1 0 0 1 0 31.98)"><g transform="matrix(1
    0 0 -1 12.95 0)"><g  transform="matrix(1 0 0 -1 0 31.98)"><g transform="matrix(1
    0 0 1 0 31.98)"><g transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1 0
    0 -1 0 25.83)"><g  transform="matrix(1 0 0 1 0 29.3)"><g transform="matrix(1 0
    0 -1 0 0)"><foreignobject width="117.61" height="12.3" transform="matrix(1 0 0
    -1 0 16.6)" overflow="visible">Simplified Sentence</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 41.59)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Attack
    by regime forces</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 152.81
    -118.35)" fill="#000000" stroke="#000000"><g transform="matrix(1 0 0 -1 0 47.98)"><g  transform="matrix(1
    0 0 1 0 41.83)"><g  transform="matrix(1 0 0 -1 25.21 0)"><g  transform="matrix(1
    0 0 -1 0 41.83)"><g transform="matrix(1 0 0 1 0 41.83)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><g transform="matrix(1
    0 0 -1 0 35.68)"><g  transform="matrix(1 0 0 1 0 39.14)"><g  transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="117.5" height="12.3" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">Substitute Ranking</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 51.44)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="167.93" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SR: #1\. attack, #2\. assault</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 164.8 -209.09)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 45.29)"><g  transform="matrix(1 0 0 1 0 39.14)"><g  transform="matrix(1
    0 0 -1 11.59 0)"><g  transform="matrix(1 0 0 -1 0 39.14)"><g transform="matrix(1
    0 0 1 0 39.14)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g transform="matrix(1 0 0 -1 0 34.335)"><g  transform="matrix(1
    0 0 1 0 39.14)"><g  transform="matrix(1 0 0 -1 0 0)"><foreignobject width="120.77"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Substitute
    Selection</foreignobject></g></g></g></g></g></g></g></g><g transform="matrix(1
    0 0 1 0 48.75)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="143.94" height="12.3" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">SS: assault, raid, attack</foreignobject></g></g></g></g></g></g></svg>'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg height="381.9" overflow="visible" version="1.1" width="453.86"><g transform="translate(0,381.9)
    matrix(1 0 0 -1 0 0) translate(108.54,0) translate(0,281.69)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -94.36 -95.32)" fill="#000000"
    stroke="#000000"><g transform="matrix(1 0 0 -1 0 184.49)"><g transform="matrix(1
    0 0 1 0 178.34)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 39.01 0)"><g transform="matrix(1 0 0 -1 0 178.34)"><g transform="matrix(1
    0 0 1 0 178.34)"><g transform="matrix(1 0 0 -1 0 0)"><g transform="matrix(1 0
    0 -1 0 172.19)"><g transform="matrix(1 0 0 1 0 175.65)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="110.7"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">复杂句</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 187.95)"><g class="ltx_tikzmatrix_col ltx_nopad_l
    ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1
    0 0)">政权军队的轰炸</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 -88.04
    -186.06)" fill="#000000" stroke="#000000"><g transform="matrix(1 0 0 -1 0 46.635)"><g
    transform="matrix(1 0 0 1 0 41.83)"><g transform="matrix(1 0 0 -1 0 0)"><g transform="matrix(1
    0 0 -1 0 41.83)"><g transform="matrix(1 0 0 1 0 41.83)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><g transform="matrix(1
    0 0 -1 0 35.68)"><g transform="matrix(1 0 0 1 0 39.14)"><g transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="177.61" height="12.3" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">复杂词汇识别</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 51.44)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 26.23 0)"><foreignobject width="123.61" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CWI: 轰炸</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -73.56 -276.8)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 45.29)"><g transform="matrix(1 0 0 1 0 39.14)"><g
    transform="matrix(1 0 0 -1 6.57 0)"><g transform="matrix(1 0 0 -1 0 39.14)"><g
    transform="matrix(1 0 0 1 0 39.14)"><g transform="matrix(1 0 0 -1 0 0)"><g transform="matrix(1
    0 0 -1 0 34.335)"><g transform="matrix(1 0 0 1 0 39.14)"><g transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="133.97" height="9.61" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">替代生成</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 48.75)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="147.11" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SG: 攻击, 袭击, 打击</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 165.01 -22.14)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 38.13)"><g transform="matrix(1 0 0 1 0 31.98)"><g
    transform="matrix(1 0 0 -1 12.95 0)"><g transform="matrix(1 0 0 -1 0 31.98)"><g
    transform="matrix(1 0 0 1 0 31.98)"><g transform="matrix(1 0 0 -1 0 0)"><g transform="matrix(1
    0 0 -1 0 25.83)"><g transform="matrix(1 0 0 1 0 29.3)"><g transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="117.61" height="12.3" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">简化句</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 41.59)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">政权军队的攻击</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 152.81 -118.35)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 47.98)"><g transform="matrix(1 0 0 1 0 41.83)"><g
    transform="matrix(1 0 0 -1 25.21 0)"><g transform="matrix(1 0 0 -1 0 41.83)"><g
    transform="matrix(1 0 0 1 0 41.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><g transform="matrix(1 0 0 -1 0 35.68)"><g transform="matrix(1
    0 0 1 0 39.14)"><g transform="matrix(1 0 0 -1 0 0)"><foreignobject width="117.5"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">替代排名</foreignobject></g></g></g></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 51.44)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="167.93" height="12.3" transform="matrix'
- en: 'Figure 1: LS Pipeline. SG, SS, and SR are the main components of LS.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LS 流程图。SG、SS 和 SR 是 LS 的主要组件。
- en: Substitute Generation
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 替代生成
- en: 'SG returns a number: k, of candidates substitutions that are suitable replacements
    for a previously identified complex word. Usually, an LS system will generate
    candidate substitution in the range of k = [1, 3, 5, or 10] with top-k referring
    to the most appropriate candidates. These candidate substitutions need to be more
    simple, hence easier to read, interpret, or understand than the original complex
    word. The candidate substitutions also need to preserve the original complex word’s
    meaning, especially in its provided context.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: SG 返回一个数字：k，表示适合替换先前识别的复杂词的候选替代。通常，LS 系统会生成 k = [1、3、5 或 10] 范围内的候选替代，其中前 k
    名是最合适的候选。这些候选替代需要比原始复杂词更简单，因此更易于阅读、解释或理解。候选替代还需要保留原始复杂词的含义，特别是在其提供的上下文中。
- en: Substitute Selection
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 替代选择
- en: SS filters the generated top-k candidate substitutions and removes those which
    are not suitable. For instance, candidate substitutions which are not synonymous
    to the original complex word or that are more complex are often removed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SS 过滤生成的前 k 名候选替代，移除不合适的那些。例如，不同义于原始复杂词或更复杂的候选替代通常会被移除。
- en: Substitute Ranking
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 替代排名
- en: SR orders the remaining top-k candidate substitutions from the most to the least
    appropriate simplification. The original complex word is then replaced with the
    most suitable candidate substitution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SR 将剩余的前 k 个候选替代按从最适合到最不适合的顺序排列。然后，用最合适的候选替代替换原始复杂词。
- en: 2.1 Evaluation Metrics
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 评估指标
- en: 'All sub-tasks of the LS pipeline are evaluated using precision, accuracy, recall,
    and F1-score. Several additional metrics have also been used: potential, mean
    average precision (MAP), and accuracy at top-k. Potential is the ratio of predicted
    candidate substitutions for which at least one of the top-k candidate substitutions
    generated was among the gold labels Saggion et al. ([2022](#bib.bib46)). MAP evaluates
    whether the returned top-k candidate substitutions match the gold labels as well
    as whether they have the same positional rank. Accuracy at top-k = [1, 2, or 3]
    is the ratio of instances where at least one of the candidate substitutions at
    k is among the gold labels.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LS 流程图的所有子任务都使用精度、准确率、召回率和 F1 分数进行评估。还使用了一些额外的度量：潜力、均值平均精度（MAP）和前 k 名准确率。潜力是预测候选替代中至少一个前
    k 名候选替代是否在金标准标签中的比例（Saggion 等 ([2022](#bib.bib46))）。MAP 评估返回的前 k 名候选替代是否匹配金标准标签，以及它们是否具有相同的位置排名。前
    k 名准确率 = [1、2 或 3] 是至少一个候选替代在 k 名中是否在金标准标签中的比例。
- en: 3 Deep Learning Approaches
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种深度学习方法
- en: '| Deep Learning Approaches | ACC | ACC@1 | ACC@3 | MAP@3 | Potential@3 | Paper
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习方法 | ACC | ACC@1 | ACC@3 | MAP@3 | Potential@3 | 论文 |'
- en: '| SG | SS & SR | TSAR-2022 (EN) |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| SG | SS & SR | TSAR-2022 (EN) |  |'
- en: '| GPT-3+Prompts | GPT-3 | 0.8096 | 0.4289 | 0.6863 | 0.5834 | 0.9624 | Aumiller
    and Gertz ([2022](#bib.bib7)) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3+Prompts | GPT-3 | 0.8096 | 0.4289 | 0.6863 | 0.5834 | 0.9624 | Aumiller
    和 Gertz ([2022](#bib.bib7)) |'
- en: '| MLM | LLM+Embeddings+Freq | 0.6568 | 0.3190 | 0.5388 | 0.4730 | 0.8766 |
    Li et al. ([2022](#bib.bib26)) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| MLM | LLM+嵌入+频率 | 0.6568 | 0.3190 | 0.5388 | 0.4730 | 0.8766 | Li 等 ([2022](#bib.bib26))
    |'
- en: '| LLM+Prompt | MLM Prediction Score | 0.6353 | 0.2895 | 0.5308 | 0.4244 | 0.8739
    | Vásquez-Rodríguez et al. ([2022](#bib.bib55)) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| LLM+Prompt | MLM 预测分数 | 0.6353 | 0.2895 | 0.5308 | 0.4244 | 0.8739 | Vásquez-Rodríguez
    等 ([2022](#bib.bib55)) |'
- en: '| SG | SS & SR | TSAR-2022 (ES) |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| SG | SS & SR | TSAR-2022 (ES) |  |'
- en: '| GPT-3+Prompts | GPT-3 | 0.6521 | 0.3505 | 0.5788 | 0.4281 | 0.8206 | Aumiller
    and Gertz ([2022](#bib.bib7)) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3+Prompts | GPT-3 | 0.6521 | 0.3505 | 0.5788 | 0.4281 | 0.8206 | Aumiller
    和 Gertz ([2022](#bib.bib7)) |'
- en: '| MLM | Embeddings+POS | 0.3695 | 0.2038 | 0.3288 | 0.2145 | 0.5842 | Whistely
    et al. ([2022](#bib.bib56)) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| MLM | 嵌入+POS | 0.3695 | 0.2038 | 0.3288 | 0.2145 | 0.5842 | Whistely 等 ([2022](#bib.bib56))
    |'
- en: '| LLM+Prompt | MLM Prediction Score | 0.3668 | 0.160 | 0.2690 | 0.2128 | 0.5326
    | Vásquez-Rodríguez et al. ([2022](#bib.bib55)) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| LLM+Prompt | MLM 预测分数 | 0.3668 | 0.160 | 0.2690 | 0.2128 | 0.5326 | Vásquez-Rodríguez
    等 ([2022](#bib.bib55)) |'
- en: '| SG | SS & SR | TSAR-2022 (PT) |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| SG | SS & SR | TSAR-2022 (PT) |  |'
- en: '| GPT-3+Prompts | GPT-3 | 0.7700 | 0.4358 | 0.6299 | 0.5014 | 0.9171 | Aumiller
    and Gertz ([2022](#bib.bib7)) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3+Prompts | GPT-3 | 0.7700 | 0.4358 | 0.6299 | 0.5014 | 0.9171 | Aumiller
    和 Gertz ([2022](#bib.bib7)) |'
- en: '| MLM | MLM Prediction Score | 0.4812 | 0.2540 | 0.3957 | 0.2816 | 0.6871 |
    North et al. ([2022a](#bib.bib34)) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| MLM | MLM 预测分数 | 0.4812 | 0.2540 | 0.3957 | 0.2816 | 0.6871 | North 等 ([2022a](#bib.bib34))
    |'
- en: '| MLM | Freq+BinaryClassifier | 0.3689 | 0.1737 | 0.2673 | 0.1983 | 0.5240
    | Wilkens et al. ([2022](#bib.bib57)) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| MLM | Freq+BinaryClassifier | 0.3689 | 0.1737 | 0.2673 | 0.1983 | 0.5240
    | Wilkens 等人（[2022](#bib.bib57)） |'
- en: 'Table 1: The top 3 deep learning approaches across the TSAR-2022 datasets.
    Best performances in bold.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：TSAR-2022数据集上的前3名深度学习方法。最佳表现以**粗体**显示。
- en: Prior to deep learning approaches, lexicon, rule-based, statistical, n-gram,
    and word embedding models were state-of-the-art for SG, SS, and SR. As previously
    mentioned, Paetzold and Specia ([2017b](#bib.bib38)) have provided a comprehensive
    survey detailing these approaches, their performances, as well as their impact
    on LS literature. The following sections provide an extension of the work carried
    out by Paetzold and Specia ([2017b](#bib.bib38)). We introduce new deep learning
    approaches for LS and begin our survey of the LS pipeline at the SG phase. The
    recent developments in the CWI step of the pipeline have been extensively surveyed
    by North et al. ([2022b](#bib.bib35)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习方法出现之前，词典、基于规则的、统计、n-gram和词嵌入模型是SG、同义词消歧（SS）和同义词替换（SR）领域的最先进技术。如前所述，Paetzold
    和 Specia（[2017b](#bib.bib38)）提供了一项全面的调查，详细介绍了这些方法及其性能，以及它们对语言学文献的影响。以下部分扩展了Paetzold
    和 Specia（[2017b](#bib.bib38)）所做的工作。我们引入了新的深度学习方法，并开始对语言学流水线的SG阶段进行调查。North 等人（[2022b](#bib.bib35)）对流水线中的CWI步骤的最新进展进行了广泛调查。
- en: 3.1 Substitute Generation
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 替代词生成
- en: In 2017, word embedding models were state-of-the-art for SG. Word embedding
    models, such as Word2Vec Mikolov et al. ([2013](#bib.bib32)), were used alongside
    more traditional approaches, such as querying a lexicon, or generating candidate
    substitutions based on certain rules Paetzold and Specia ([2017b](#bib.bib38)).
    Word embedding models conducted SG by converting potential candidate substitutions
    into vectors, hence word embeddings, and then calculating which of these vectors
    had the highest cosine similarity, or lowest cosine distance, with the vector
    of the target complex word. These vectors were then converted back into their
    word forms and were considered the top-k candidate substitutions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，词嵌入模型是同义词生成（SG）领域的最先进技术。词嵌入模型，如Word2Vec Mikolov等（[2013](#bib.bib32)），与传统方法（如查询词典或基于某些规则生成候选替代词
    Paetzold 和 Specia（[2017b](#bib.bib38)））一起使用。词嵌入模型通过将潜在的候选替代词转换为向量，即词嵌入，然后计算这些向量与目标复杂词向量的余弦相似度或余弦距离，从而进行SG。这些向量随后被转换回词形，并被认为是前k个候选替代词。
- en: Word Embeddings + LLMs
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词嵌入 + 大型语言模型（LLMs）
- en: Post 2017, word embedding models continued to be implemented for SG. However,
    they were now combined with the word embeddings produced by LLMs or by a LLM’s
    prediction scores. Alarcón et al. ([2021a](#bib.bib2)) experimented with various
    word embeddings models for generating Spanish candidate substitutions. They used
    word embeddings models, such as Word2Vec, Sense2Vec Trask et al. ([2015](#bib.bib53)),
    and FastText Bojanowski et al. ([2016](#bib.bib9)), along with the pre-trained
    LLM BERT, to generate these word embeddings. It was discovered that a more traditional
    approach that produced candidate substitutions by querying a pre-existing lexicon
    outperformed these word embedding models in terms of both potential and recall
    yet slightly under-performed these word embedding models in regards to precision.
    The traditional approach achieved a potential of 0.898, a recall of 0.597, and
    a precision of 0.043 on the EASIER dataset Alarcón et al. ([2021b](#bib.bib3)).
    The highest performing word embedding model (Sense2Vec), on the other hand, attained
    a potential, recall, and precision score of 0.506, 0.282, and 0.056, respectively.
    Surprisingly, this went against the assumption that word embedding models would
    have achieved a superior performance given their state-of-the-art reputation demonstrated
    by Paetzold and Specia ([2017a](#bib.bib37)). During error analysis, it was found
    that these word embeddings models often produced antonyms of the target complex
    word as potential candidate substitutions. This is due to how word embedding models
    calculate word similarity between vectors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年后，词嵌入模型继续在SG中得到应用。然而，它们现在与LLMs生成的词嵌入或LLM的预测得分结合在一起。Alarcón等人 ([2021a](#bib.bib2))
    对各种词嵌入模型进行了实验，以生成西班牙语的候选替代词。他们使用了诸如Word2Vec、Sense2Vec Trask等 ([2015](#bib.bib53))
    和FastText Bojanowski等 ([2016](#bib.bib9)) 等词嵌入模型，以及预训练的LLM BERT，来生成这些词嵌入。研究发现，一种更传统的方法，通过查询现有词典生成候选替代词，在潜在性和召回率方面都优于这些词嵌入模型，但在精确度方面稍逊于这些词嵌入模型。传统方法在EASIER数据集上达到了0.898的潜在性、0.597的召回率和0.043的精确度。另一方面，表现最好的词嵌入模型（Sense2Vec）分别达到了0.506、0.282和0.056的潜在性、召回率和精确度。令人惊讶的是，这与词嵌入模型由于其先进的声誉而表现更优的假设相悖。Paetzold和Specia
    ([2017a](#bib.bib37)) 的研究也表明了这一点。在错误分析过程中发现，这些词嵌入模型经常将目标复杂词的反义词作为候选替代词。这是由于词嵌入模型计算词向量之间的相似度的方式所致。
- en: 'Seneviratne et al. ([2022](#bib.bib47)) used a word embedding model and a pre-trained
    LLM: XLNet Yang et al. ([2019](#bib.bib58)), to produce an embedding similarity
    score and a prediction score for SG. They followed a similar approach conducted
    by Arefyev et al. ([2020](#bib.bib6)). Arefyev et al. ([2020](#bib.bib6)) utilized
    context2vec Melamud et al. ([2016](#bib.bib30)) and ELMo Peters et al. ([2018](#bib.bib41))
    to encode the context of the target complex word to gain a probability distribution
    of each word belonging to that particular context. They then used this probability
    distribution to estimate the likelihood, or appropriateness, of a potential candidate
    substitution replacing the target complex word. This score was used alongside
    a LLM prediction score from either BERT, RoBERTa, or XLNet, to produce a final
    list of top-k candidate substitutions. Both Seneviratne et al. ([2022](#bib.bib47))
    and Arefyev et al. ([2020](#bib.bib6)) discovered that their combined approach
    of using a word embedding model alongside a pre-trained LLM prediction score failed
    to surpass the performance of using a single pre-trained LLM. For instance, Seneviratne
    et al. ([2022](#bib.bib47)) was outperformed by North et al. ([2022a](#bib.bib34))
    on the TSAR-2022 dataset.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Seneviratne等人 ([2022](#bib.bib47)) 使用了一个词嵌入模型和一个预训练的LLM：XLNet Yang等 ([2019](#bib.bib58))，以产生嵌入相似度得分和SG的预测得分。他们采用了与Arefyev等人
    ([2020](#bib.bib6)) 相似的方法。Arefyev等人 ([2020](#bib.bib6)) 使用了context2vec Melamud等
    ([2016](#bib.bib30)) 和ELMo Peters等 ([2018](#bib.bib41)) 对目标复杂词的上下文进行编码，以获取每个词属于特定上下文的概率分布。然后，他们使用这个概率分布来估算一个潜在候选替代词替换目标复杂词的可能性或适当性。这个得分与来自BERT、RoBERTa或XLNet的LLM预测得分一起使用，以生成最终的top-k候选替代词列表。Seneviratne等人
    ([2022](#bib.bib47)) 和Arefyev等人 ([2020](#bib.bib6)) 发现，他们将词嵌入模型与预训练LLM预测得分结合使用的方法未能超越单一预训练LLM的表现。例如，Seneviratne等人
    ([2022](#bib.bib47)) 在TSAR-2022数据集上被North等人 ([2022a](#bib.bib34)) 超越。
- en: Masked Language Modeling
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 掩码语言建模
- en: The introduction of pre-trained LLMs, also saw the arrival of Masked Language
    Modeling (MLM) for SG. Przybyła and Shardlow ([2020](#bib.bib42)) used LLMs trained
    on a MLM objective for multi-word LS, whereas Qiang et al. ([2020](#bib.bib43))
    were the first to use MLM for Spanish SG. MLM has subsequently become a popular
    approach to SG. 7 out of the 11 system reports submitted to TSAR-2022 Saggion
    et al. ([2022](#bib.bib46)), described their approach as consisting of a MLM objective.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLM）的引入，同时也带来了掩码语言建模（MLM）用于同义词生成（SG）。Przybyła和Shardlow（[2020](#bib.bib42)）使用了以MLM目标训练的LLM进行多词同义词生成，而Qiang等人（[2020](#bib.bib43)）是首次将MLM应用于西班牙语SG。MLM随后成为SG的一种流行方法。在提交给TSAR-2022的11份系统报告中，有7份Saggion等人（[2022](#bib.bib46)）描述了他们的方法包含MLM目标。
- en: Known as LSBert, the model introduced by Qiang et al. ([2020](#bib.bib43)),
    used the pre-trained LLM BERT. Sentences were taken from the LS datasets LexMTurkHorn
    et al. ([2014](#bib.bib20)), BenchLS Paetzold and Specia ([2016b](#bib.bib39)),
    and NNSeval Paetzold and Specia ([2016c](#bib.bib40)). Two versions of each sentence
    were then concatenated, being separated by the [SEP] special token. They were
    then fed into the LLM. The first sentence was identical to that extracted from
    the datasets, whereas the second sentence had its complex word replaced with the
    [MASK] special token. The LLM then attempted to predict the word replaced by the
    [MASK] special token by taking into consideration its left and right context as
    well as the prior original sentence. In this way, LLMs provide candidate substitutions
    with the highest probability (highest prediction score) of fitting into the surrounding
    context and that are also similar to the target complex word in the original sentence.
    For the top-k=1 candidate substitution, LSBert achieved F1-scores for SG of 0.259,
    0.272, and 0.218 on the three datasets LexMTurk Horn et al. ([2014](#bib.bib20)),
    BenchLS Paetzold and Specia ([2016b](#bib.bib39)), and NNSeval Paetzold and Specia
    ([2016c](#bib.bib40)) respectively. These performances surpassed that of all prior
    approaches Paetzold and Specia ([2017b](#bib.bib38)). The previous highest F1-score
    was achieved by a word-embedding model Paetzold and Specia ([2017a](#bib.bib37)),
    which produced F1-scores of 0.195, 0.236, and 0.218 for each dataset, respectively.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为LSBert的模型，由Qiang等人（[2020](#bib.bib43)）提出，使用了预训练的LLM BERT。句子取自LS数据集LexMTurkHorn等人（[2014](#bib.bib20)）、BenchLS
    Paetzold和Specia（[2016b](#bib.bib39)）和NNSeval Paetzold和Specia（[2016c](#bib.bib40)）。每个句子有两个版本被连接在一起，中间用[SEP]特殊标记分隔。它们被输入到LLM中。第一个句子与从数据集中提取的句子相同，而第二个句子的复杂词被替换为[MASK]特殊标记。LLM然后试图预测被[MASK]特殊标记替换的词，考虑到其左右上下文以及之前的原始句子。通过这种方式，LLM提供了与周围上下文最匹配且与原始句子中的目标复杂词相似的候选替换词。对于top-k=1的候选替换，LSBert在三个数据集LexMTurk
    Horn等人（[2014](#bib.bib20)）、BenchLS Paetzold和Specia（[2016b](#bib.bib39)）和NNSeval
    Paetzold和Specia（[2016c](#bib.bib40)）上分别达到了0.259、0.272和0.218的F1分数。这些表现超越了所有先前的方法Paetzold和Specia（[2017b](#bib.bib38)）。之前的最高F1分数由一个词嵌入模型Paetzold和Specia（[2017a](#bib.bib37)）取得，分别为0.195、0.236和0.218。
- en: 'Before the release of the TSAR-2022 shared-task Saggion et al. ([2022](#bib.bib46)),
    Ferres and Saggion ([2022](#bib.bib18)) introduced a new dataset: ALEXSIS (TSAR-2022
    ES), that would later make up (along with an additional English and Portuguese
    dataset) the TSAR-2022 dataset Saggion et al. ([2022](#bib.bib46)). Using their
    Spanish dataset, they experimented with a number of monolingual LLMs pre-trained
    on either Spanish data as well as several multilingual LLMs, such as mBERT and
    RoBERTa. Ferres and Saggion ([2022](#bib.bib18)) adopted the MLM approach used
    by LSBert. They experimented with the Spanish LLMs: BETO Cañete et al. ([2020](#bib.bib11)),
    BERTIN De la Rosa and Fernández ([2022](#bib.bib14)), RoBERTa-base-BNE, and RoBERTA-large-BNE
    Fandiño et al. ([2022](#bib.bib17)) for SG. They discovered that their largest
    pre-trained Spanish LLM: RoBERTA-large-BNE, achieved the greatest SG performance
    after having also removed candidate substitutions equal to the complex word, regardless
    of capitalization or accentuation and being less than 2 characters long.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TSAR-2022 共享任务发布之前，Saggion et al. ([2022](#bib.bib46))，Ferres 和 Saggion ([2022](#bib.bib18))
    引入了一个新数据集：ALEXSIS（TSAR-2022 ES），该数据集后来（以及额外的英语和葡萄牙语数据集）组成了 TSAR-2022 数据集 Saggion
    et al. ([2022](#bib.bib46))。使用他们的西班牙语数据集，他们实验了多种在西班牙语数据上预训练的单语 LLM 以及几种多语言 LLM，例如
    mBERT 和 RoBERTa。Ferres 和 Saggion ([2022](#bib.bib18)) 采用了 LSBert 使用的 MLM 方法。他们实验了西班牙语
    LLM：BETO Cañete et al. ([2020](#bib.bib11))、BERTIN De la Rosa 和 Fernández ([2022](#bib.bib14))、RoBERTa-base-BNE
    和 RoBERTA-large-BNE Fandiño et al. ([2022](#bib.bib17)) 用于 SG。他们发现，最大的预训练西班牙语
    LLM：RoBERTA-large-BNE，达到了最佳的 SG 性能，此前他们还去除了与复杂词相等的候选替代项，不论大小写或重音，并且字符数少于 2。
- en: 'North et al. ([2022a](#bib.bib34)) was inspired by the success of the monolingual
    LLMs shown by Ferres and Saggion ([2022](#bib.bib18)). They likewise tested a
    range of LLMs for SG with a MLM objective, including multilingual LLLMs: mBERT,
    and XLM-R Conneau et al. ([2020](#bib.bib13)), and several monolingual LLMs, including
    Electra for English Clark et al. ([2020](#bib.bib12)), RoBERTA-large-BNE for Spanish,
    and BERTimbau Souza et al. ([2020](#bib.bib51)) for Portuguese. Their monolingual
    LLMs scored an acc@1 score of 0.517, 0.353, and 0.481 on the English, Spanish,
    and Portuguese TSAR-2022 datasets respectively. Whistely et al. ([2022](#bib.bib56))
    also experimented with similar monolingual LLMs for SG. They used BERT for English,
    BETO for Spanish, and BERTimbau for Portuguese. Interestingly, their models’ performances
    were lower compared to that of North et al. ([2022a](#bib.bib34)), despite their
    Portuguese LS system consisting of the same language model. Whistely et al. ([2022](#bib.bib56))
    achieved acc@1 scores of 0.378, 0.250, and 0.3074 for English, Spanish, and Portuguese,
    respectively. This is likely due to the additional SS and SR steps implemented
    by Whistely et al. ([2022](#bib.bib56)) and the lack thereof shown within the
    LS system provided by North et al. ([2022a](#bib.bib34)) (Section [3.2](#S3.SS2
    "3.2 Substitute Selection and Ranking ‣ 3 Deep Learning Approaches ‣ Deep Learning
    Approaches to Lexical Simplification: A Survey")).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'North et al. ([2022a](#bib.bib34)) 受到 Ferres 和 Saggion ([2022](#bib.bib18))
    展示的单语 LLM 成功的启发。他们同样测试了多种具有 MLM 目标的 LLM，包括多语言 LLLMs：mBERT 和 XLM-R Conneau et al.
    ([2020](#bib.bib13))，以及几种单语 LLM，包括英语的 Electra Clark et al. ([2020](#bib.bib12))、西班牙语的
    RoBERTA-large-BNE 和葡萄牙语的 BERTimbau Souza et al. ([2020](#bib.bib51))。他们的单语 LLM
    在英语、西班牙语和葡萄牙语 TSAR-2022 数据集上的 acc@1 分别为 0.517、0.353 和 0.481。Whistely et al. ([2022](#bib.bib56))
    也对类似的单语 LLM 进行了 SG 实验。他们使用了英语的 BERT、西班牙语的 BETO 和葡萄牙语的 BERTimbau。有趣的是，他们的模型表现低于
    North et al. ([2022a](#bib.bib34))，尽管他们的葡萄牙语 LS 系统使用了相同的语言模型。Whistely et al. ([2022](#bib.bib56))
    分别在英语、西班牙语和葡萄牙语上取得了 0.378、0.250 和 0.3074 的 acc@1 分数。这可能是由于 Whistely et al. ([2022](#bib.bib56))
    实施了额外的 SS 和 SR 步骤，而 North et al. ([2022a](#bib.bib34)) 的 LS 系统中则缺少这些步骤（见[3.2](#S3.SS2
    "3.2 Substitute Selection and Ranking ‣ 3 Deep Learning Approaches ‣ Deep Learning
    Approaches to Lexical Simplification: A Survey")）。'
- en: 'Wilkens et al. ([2022](#bib.bib57)) also used a range of monolingual LLMs for
    SG. However, they used an ensemble of BERT-like models with three different masking
    strategies: 1). copy, 2). query expansion, and 3). paraphrase. The copy strategy
    replicated that of LSBert Qiang et al. ([2020](#bib.bib43)), whereby two sentences
    were inputted into a LLM concatenated with the [SEP] special token. The first
    sentence being an unaltered version of the original sentence, and the second sentence
    having its complex word masked. The query expansion strategy used FastText to
    generate five related words with the highest cosine similarity to the target complex
    word. For iteration 2a). of the query expansion strategy, the first sentence was
    the original unaltered sentence, the second sentence replaced the complex word
    with one of the suggested similar words produced by FastText, and sentence 3 was
    the masked sentence. Iteration 2b). of this strategy was the same as iteration
    2a)., however, sentence 2 now consisted of all five suggested words. Lastly, the
    paraphrase strategy generated 10 new contexts for each complex word composed of
    paraphrases of the original sentence. These new contexts were limited to 512 tokens.
    The ensembles used for these three masking strategies consisted of BERT and RoBERTa
    LLMs for English, several BETO LLMs for Spanish, and several BERTimbau LLMs for
    Portuguese. The paraphrase strategy showed the worst performance with a joint
    MAP/Potential@1 score of 0.217, whereas the query expansion strategy obtained
    a MAP/Potential@1 score of 0.528, 0.477, and 0.476 for English, Spanish, and Portuguese,
    respectively. This surpassed the performance of the paraphrase strategy and the
    original copy strategy used by LSBert, regardless of the LLMs used.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Wilkens 等人 ([2022](#bib.bib57)) 也使用了一系列单语 LLMs 用于 SG。然而，他们使用了一组 BERT 类模型，采用了三种不同的掩码策略：1).
    复制策略，2). 查询扩展策略，3). 释义策略。复制策略复用了 LSBert Qiang 等人 ([2020](#bib.bib43)) 的方法，其中两句话被输入到
    LLM 中，并用 [SEP] 特殊标记连接。第一句话是原始句子的未修改版本，第二句话则将复杂词汇进行了掩码。查询扩展策略使用 FastText 生成与目标复杂词具有最高余弦相似度的五个相关词。在查询扩展策略的第
    2a) 轮中，第一句话是原始未修改句子，第二句话用 FastText 生成的一个相似词替换了复杂词，而第三句话是掩码句子。该策略的第 2b) 轮与第 2a)
    轮相同，但第二句话现在包括所有五个建议词。最后，释义策略为每个复杂词生成了 10 个新语境，这些语境由原始句子的释义组成。这些新语境被限制在 512 个标记内。这三种掩码策略使用的模型包括英语的
    BERT 和 RoBERTa LLMs、西班牙语的多个 BETO LLMs，以及葡萄牙语的多个 BERTimbau LLMs。释义策略的表现最差，MAP/Potential@1
    分数为 0.217，而查询扩展策略在英语、西班牙语和葡萄牙语中的 MAP/Potential@1 分数分别为 0.528、0.477 和 0.476。这超越了释义策略和
    LSBert 使用的原始复制策略的表现，无论使用何种 LLMs。
- en: Prompt Learning
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示学习
- en: 'Prompt learning has also been used for SG and is currently state-of-the-art
    (Table [3](#S3 "3 Deep Learning Approaches ‣ Deep Learning Approaches to Lexical
    Simplification: A Survey")). Prompt learning involves feeding into a LLM input
    that is presented in such a way as to provide a description of the task as well
    as to return a desired output. PromptLS is an example of prompt learning applied
    to SG. Created by Vásquez-Rodríguez et al. ([2022](#bib.bib55)), PromptLS consisted
    of a variety of pre-trained LLMs fine-tuned on several LS datasets. These fined-tuned
    LLMs were then presented with four combinations of prompts: a). “a easier word
    for bombardment is”, b). “a simple word for bombardment is”, c). “a easier synonym
    for bombardment is”, and lastly, d). “a simple synonym for bombardment is”. These
    prompt combinations were supplied to a RoBERTa LLM on all of the English data
    extracted from the LexMTurk Horn et al. ([2014](#bib.bib20)), BenchLS Paetzold
    and Specia ([2016b](#bib.bib39)), NNSeval Paetzold and Specia ([2016c](#bib.bib40)),
    and CERF-LS Uchida et al. ([2018](#bib.bib54)) LS datasets. They were also translated
    and fed into BERTIN fine-tuned on the Spanish data obtained from EASIER, along
    with BR-BERTo fine-tuned on all of the Portuguese data taken from SIMPLEX-PB Hartmann
    and Aluísio ([2020](#bib.bib19)). Vásquez-Rodríguez et al. ([2022](#bib.bib55))
    also used these prompts on a zero-shot condition. It was discovered that the fine-tuned
    LLMs outperformed the zero-shot models on all conditions by an average increase
    in performance between 0.3 to 0.4 across all metrics: acc@1, acc@3, MAP@3, and
    Precision@3\. The prompt combinations that produced the best candidate substitutions
    were “easier word” for English, “palabra simple” and “palabra fácil” for Spanish,
    and “palavra simples” and “sinônimo simples” for Portuguese.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 提示学习也被用于简化生成（SG），并且目前是最先进的技术（表格 [3](#S3 "3 深度学习方法 ‣ 深度学习方法在词汇简化中的应用：一项调查")）。提示学习涉及向大型语言模型（LLM）输入以描述任务并返回期望输出的内容。PromptLS
    是应用于 SG 的提示学习的一个例子。由 Vásquez-Rodríguez 等人 ([2022](#bib.bib55)) 创建的 PromptLS 包含了多种在多个词汇简化（LS）数据集上微调的预训练
    LLM。这些微调过的 LLM 然后接收了四种提示组合：a). “bombardment 的一个更简单的词是”，b). “bombardment 的一个简单词是”，c).
    “bombardment 的一个更简单的同义词是”，以及 d). “bombardment 的一个简单同义词是”。这些提示组合被提供给 RoBERTa LLM，应用于从
    LexMTurk Horn 等人 ([2014](#bib.bib20))、BenchLS Paetzold 和 Specia ([2016b](#bib.bib39))、NNSeval
    Paetzold 和 Specia ([2016c](#bib.bib40))、和 CERF-LS Uchida 等人 ([2018](#bib.bib54))
    收集的所有英语数据上。它们还被翻译并输入到在 EASIER 上微调的 BERTIN，以及在 SIMPLEX-PB Hartmann 和 Aluísio ([2020](#bib.bib19))
    提供的所有葡萄牙语数据上微调的 BR-BERTo 中。Vásquez-Rodríguez 等人 ([2022](#bib.bib55)) 还在零样本条件下使用了这些提示。研究发现，微调后的
    LLM 在所有条件下的表现优于零样本模型，性能在所有指标（acc@1, acc@3, MAP@3 和 Precision@3）上平均提高了 0.3 到 0.4。产生最佳候选替代词的提示组合是英语中的“easier
    word”，西班牙语中的“palabra simple”和“palabra fácil”，以及葡萄牙语中的“palavra simples”和“sinônimo
    simples”。
- en: 'Prompt learning has likewise been applied to causal language models for SG,
    such as GPT-3\. Aumiller and Gertz ([2022](#bib.bib7)) experimented with a variety
    of different prompts, which they fed into a GPT-3\. These prompts were of four
    types: 1). zero-shot with context, 2). single-shot with context, two-shot with
    context, 3). zero-shot without context, and 4). single-shot without context. The
    size of each shot: n, refers to how many times a prompt is inputted into GPT-3\.
    For instance, those shots with context would input a given sentence and then ask
    the question, “Given the above context, list ten alternative words for $<$complex
    word$>$ that are easier to understand.”, n number of times. Those without context,
    however, would input n times the following:“Give me ten simplified synonyms for
    the following word: $<$complex word$>$”. Aumiller and Gertz ([2022](#bib.bib7))
    also combined all types of prompts in an ensemble, generating candidate substitutions
    from each prompt type and then deciding upon final candidate substations through
    plurality voting and additional SS and SR steps (Section [3.2](#S3.SS2 "3.2 Substitute
    Selection and Ranking ‣ 3 Deep Learning Approaches ‣ Deep Learning Approaches
    to Lexical Simplification: A Survey")). Their ensemble approach outperformed all
    other prompt types and SG models submitted to TSAR-2022 Saggion et al. ([2022](#bib.bib46))
    (Table [3](#S3 "3 Deep Learning Approaches ‣ Deep Learning Approaches to Lexical
    Simplification: A Survey")).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '提示学习同样被应用于因果语言模型，如 GPT-3。Aumiller 和 Gertz ([2022](#bib.bib7)) 试验了各种不同的提示，并将它们输入到
    GPT-3 中。这些提示有四种类型：1). 带上下文的零-shot，2). 带上下文的单-shot，双-shot 带上下文，3). 不带上下文的零-shot，以及
    4). 不带上下文的单-shot。每次输入的大小：n，指的是提示被输入到 GPT-3 中的次数。例如，带上下文的提示会输入一个给定的句子，然后问“根据上述上下文，列出
    $<$复杂词汇$>$ 的十个更易理解的替代词。”，重复 n 次。而不带上下文的提示则会输入 n 次以下内容：“给我 $<$复杂词汇$>$ 的十个简化同义词。”Aumiller
    和 Gertz ([2022](#bib.bib7)) 还将所有类型的提示结合在一个集成体中，从每种提示类型生成候选替代词，然后通过多数投票和额外的 SS
    和 SR 步骤（第 [3.2](#S3.SS2 "3.2 Substitute Selection and Ranking ‣ 3 Deep Learning
    Approaches ‣ Deep Learning Approaches to Lexical Simplification: A Survey") 节）来决定最终的候选替代词。他们的集成方法优于所有其他提示类型和提交给
    TSAR-2022 的 SG 模型 Saggion 等 ([2022](#bib.bib46))（表 [3](#S3 "3 Deep Learning Approaches
    ‣ Deep Learning Approaches to Lexical Simplification: A Survey")）。'
- en: 3.2 Substitute Selection and Ranking
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 替代词选择和排序
- en: Traditional approaches to SS are still implemented post SG. Methods such as
    POS-tag and antonym filtering, semantic or sentence thresholds have been used
    to remove inappropriate candidate substitutions after having been generating from
    the above deep learning approaches Saggion et al. ([2022](#bib.bib46)). Nevertheless,
    the majority of modern deep learning approaches have minimal SS, with SS often
    being simultaneously conducted during SG or SR. For instance, the metric used
    to generate the top-k candidate substitutions, by it either similarity between
    word embeddings, or a pre-train LLM’s prediction score, tends not to suggest candidate
    substitutions that are deemed as being inappropriate by other SS methods. Likewise,
    SR techniques that rank candidate substitutions in order of their appropriateness
    will in turn move inappropriate simplifications further down the list of top-k
    candidate substitutions to the point that they are no longer considered.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 SS 方法仍然在 SG 后实现。像 POS 标记和反义词过滤、语义或句子阈值这样的技术已被用于去除不适当的候选替代词，这些词是在上述深度学习方法生成后产生的
    Saggion 等 ([2022](#bib.bib46))。然而，大多数现代深度学习方法的 SS 最小，SS 通常在 SG 或 SR 过程中同时进行。例如，用于生成
    top-k 候选替代词的指标，通常是词嵌入之间的相似性，或预训练 LLM 的预测分数，往往不会建议被其他 SS 方法认为不适当的候选替代词。同样，SR 技术将候选替代词按其适当性排序，将不适当的简化进一步移动到
    top-k 候选替代词列表的末尾，直到不再被考虑。
- en: Word Embeddings
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Word embedding models continued to be used for SS without LLMs, regardless of
    the arrival of pre-trained LLMs, such as BERT. For instance, Song et al. ([2020](#bib.bib50))
    created a unique LS system that filtered candidate substitutions by applying a
    semantic similarity threshold, matching only those candidate substitutions with
    the same POS tag as the target complex word, calculating contextual relevance,
    being a measure of how reasonable and fluent a sentence is after the complex word
    had been replaced, and by using cosine similarity between word embeddings to rank
    candidate substitutions. They generated word embeddings by Word2Vec and evaluated
    their model’s performance on the LS-2007 dataset McCarthy and Navigli ([2007](#bib.bib29)).
    It was found that the use of Word2Vec improved their model’s performance having
    achieved an acc@1 of 0.269\. Their second highest performing model, without the
    use of Word2Vec embeddings, produced an acc@1 of 0.218.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在预训练的 LLMs（如 BERT）出现后，词嵌入模型仍继续用于 SS。例如，Song 等人 ([2020](#bib.bib50)) 创建了一个独特的
    LS 系统，该系统通过应用语义相似性阈值来筛选候选替换，仅匹配与目标复杂词具有相同 POS 标签的候选替换，计算上下文相关性，这是替换复杂词后句子的合理性和流畅性的衡量标准，并使用词嵌入之间的余弦相似度对候选替换进行排名。他们通过
    Word2Vec 生成词嵌入，并在 LS-2007 数据集 McCarthy 和 Navigli ([2007](#bib.bib29)) 上评估了模型的表现。结果发现，使用
    Word2Vec 提升了模型的表现，取得了 0.269 的 acc@1。没有使用 Word2Vec 嵌入的第二高表现模型的 acc@1 为 0.218。
- en: Neural Regression
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经回归
- en: Maddela and Xu ([2018](#bib.bib28)) created the neural readability ranker (NNR)
    for SR. Consisting of a feature extraction, a Gaussian-based feature vectorization
    layer, and a task specific output node, NNR is a deep learning algorithm capable
    of ranking candidate substitutions based on their perceived complexity. It performances
    regression, whereby having been trained on the Word Complexity Lexicon (WCL),
    as well as several features and character n-grams converted into Gaussian vectors,
    it is able to provide a value between 0 and 1 corresponding to the complexity
    of any given word. It achieves this by conducting pairwise aggregation. For each
    pair of potential candidate substitutions, the model predicts a value that defines
    which candidate substitution is more or less complex than the other. A return
    positive value indicates that the first candidate substitution is more complex
    than the second, whereas a negative value dictates that the second candidate substitution
    is more complex than the first. This is applied to all combinations of candidate
    substitutions given a complex word. Each candidate substitution is then ranked
    in accordance to its comparative complexity with all other potential candidate
    substitutions. Maddela and Xu ([2018](#bib.bib28)) applied their NNR model to
    the LS-2012 dataset and outperformed prior word embedding techniques for SR. They
    achieved an Prec@1 of 0.673, whereas the previous state-of-the-art model provided
    by Paetzold and Specia ([2017a](#bib.bib37)) achieved an Prec@1 of 0.656.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Maddela 和 Xu ([2018](#bib.bib28)) 为 SR 创建了神经可读性排序器（NNR）。NNR 包含一个特征提取模块、一个基于高斯的特征向量化层和一个特定任务的输出节点，是一种能够根据感知复杂性对候选替换进行排名的深度学习算法。它执行回归，通过在词汇复杂性词典（WCL）上进行训练，以及将若干特征和字符
    n-grams 转换为高斯向量，它能够提供一个介于 0 和 1 之间的值，表示任何给定词汇的复杂性。它通过进行成对聚合来实现这一点。对于每对潜在的候选替换，模型预测一个值，以定义哪个候选替换比另一个更复杂或更简单。返回的正值表示第一个候选替换比第二个更复杂，而负值则表示第二个候选替换比第一个更复杂。这一方法应用于复杂词汇的所有候选替换组合。每个候选替换随后根据其与所有其他潜在候选替换的比较复杂性进行排名。Maddela
    和 Xu ([2018](#bib.bib28)) 将他们的 NNR 模型应用于 LS-2012 数据集，并超越了之前的 SR 词嵌入技术。他们获得了 0.673
    的 Prec@1，而 Paetzold 和 Specia ([2017a](#bib.bib37)) 提供的之前的最先进模型则获得了 0.656 的 Prec@1。
- en: Word Embeddings + LLMs
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词嵌入 + LLMs
- en: One of the most common approaches to SS and SR involves the use of word embeddings
    and LLMs. Seneviratne et al. ([2022](#bib.bib47)) filtered and ranked top-k=20
    candidate substitutions based on the same combined score that they used for SG.
    It consisted of their MLM model’s prediction score of the generated candidate
    together with the inner product of the target word’s embedding and the embedding
    of the potential candidate substitution. These top-k=20 candidate substitutions
    were then subject to one of three additional ranking metrics. The first ranking
    metric (CILex_1) ranked candidate substitutions on their cosine similarity between
    the original sentence and a copy of the original sentence with the candidate substitution
    in place of its complex word. The second and third ranking metrics made use of
    dictionary definitions of the target complex word and its candidate substitutions.
    They calculated the cosine similarity between each embedding of each definition
    and the embedding of the sentence of the target complex word. Those with the highest
    cosine similarities between a). the definition of the target complex word and
    the definition of the candidate substitution (CILex_2), or b). the definition
    of the target complex word and the word embedding of the original sentence with
    the candidate substitution in place of its complex word (CILex_3), were used to
    determine the rank of each candidate substitution. They discovered that all three
    metrics produced similar performances on the TSAR-2022 dataset with CILex 1, 2,
    and 3 achieving acc@1 scores of 0.375, 0.380, and 0.386, respectively.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SS和SR最常用的方法之一是使用词嵌入和LLMs。Seneviratne等人（[2022](#bib.bib47)）基于与SG相同的综合评分对前k=20个候选替换进行了筛选和排名。评分包括其MLM模型对生成候选词的预测分数以及目标词的嵌入和潜在候选替换的嵌入之间的内积。这些前k=20个候选替换随后经过了三种额外排名指标中的一种。第一个排名指标（CILex_1）根据候选替换与原句的余弦相似度对候选替换进行排名，候选替换替换了其复杂词。第二和第三个排名指标利用了目标复杂词及其候选替换的词典定义。他们计算了每个定义的嵌入与目标复杂词句子的嵌入之间的余弦相似度。那些具有最高余弦相似度的定义a).
    目标复杂词的定义和候选替换的定义（CILex_2），或b). 目标复杂词的定义和原句中复杂词被候选替换词替换后的词嵌入（CILex_3），用于确定每个候选替换的排名。他们发现这三种指标在TSAR-2022数据集上表现相似，CILex
    1、2和3分别达到了0.375、0.380和0.386的acc@1得分。
- en: Li et al. ([2022](#bib.bib26)) used a set of features taken from LSBert combined
    with what they referred to as an equivalence score. Equivalence score was created
    to gauge semantic similarity between candidate substitution and complex word to
    an extent that was more expressive than the cosine similarity between word embeddings.
    To obtain this equivalence score, they used a pre-trained RoBERTa LLM trained
    for natural language inference (NLI) which predicts the likelihood of one sentence
    entailing another. The model was trained on a multi-genre corpus with a MLM objective.
    The product of the returned likelihood of the original sentence with the candidate
    substitution preceding the original sentence and vice-versa equated to the equivalence
    score. Since Li et al. ([2022](#bib.bib26)) used the same method of SG as LSBert,
    having only changed their LLM to RoBERTa, they concluded that their system’s superior
    performance was a consequence of its unique SR. They achieved an acc@1 of 0.659,
    whereas LSBert attained an acc@1 of 0.598 on the English TSAR-2022 dataset Saggion
    et al. ([2022](#bib.bib46)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人（[2022](#bib.bib26)）使用了一组来自LSBert的特征，并结合了他们所称的等效评分。等效评分是为了衡量候选替换和复杂词之间的语义相似度，其表达能力超过了词嵌入之间的余弦相似度。为了获得这个等效评分，他们使用了一个经过预训练的RoBERTa
    LLM，该模型经过自然语言推理（NLI）的训练，预测一个句子是否包含另一个句子的可能性。该模型在一个多类型语料库上进行训练，目标是MLM。返回的原句与候选替换前置的原句和反向的原句的可能性相乘，等于等效评分。由于Li等人（[2022](#bib.bib26)）使用了与LSBert相同的SG方法，只是将LLM更改为RoBERTa，他们得出结论，他们系统的优越性能是其独特SR的结果。他们在英语TSAR-2022数据集上达到了0.659的acc@1，而LSBert在Saggion等人（[2022](#bib.bib46)）的数据集上达到了0.598的acc@1。
- en: 'Aleksandrova and Brochu Dufour ([2022](#bib.bib4)) ranked candidate substitutions
    on three metrics: a). grammaticality, b). meaning preservation, and c). simplicity.
    Grammaticality was calculated by firstly determining whether the candidate substitution
    had the same POS tag in terms of person, number, mood, tense, and so forth. Those
    that matched on all POS-tag categories were assigned the value of 1 or 0 if at
    least one category did not match. Preservation was determined by using BERTScore
    to generate cosine similarities between the embeddings of the original sentence
    and the embeddings of the original sentence, having replaced the target complex
    word with the candidate substitution. Lastly, preservation was obtained by using
    a CEFR vocabulary classifier trained on data from the English Vocabulary Profile
    (EVP). The data used to train the CEFR classifier was first masked and fed into
    a pre-trained LLM: BERT. The outputted encodings were then used to train an SVM
    model resulting in their CEFR classifier. Their model failed to surpass the baseline
    LSBert models at TSAR-2022 in terms of acc@1, having achieved a score of 0.544.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Aleksandrova 和 Brochu Dufour ([2022](#bib.bib4)) 根据三个指标对候选替代项进行排名：a). 语法性，b).
    意义保留，和 c). 简单性。语法性通过首先确定候选替代项在语法类别（如人称、数、情态、时态等）是否匹配来计算。如果在所有 POS 标签类别上匹配，则赋值为
    1；如果至少一个类别不匹配，则赋值为 0。意义保留通过使用 BERTScore 生成原句与替代复杂词后的句子嵌入之间的余弦相似度来确定。最后，通过使用在英语词汇配置文件
    (EVP) 数据上训练的 CEFR 词汇分类器来获得保留。用于训练 CEFR 分类器的数据首先被掩码，并输入到预训练 LLM：BERT 中。输出的编码然后用于训练
    SVM 模型，从而形成他们的 CEFR 分类器。他们的模型在 TSAR-2022 的 acc@1 性能上未能超越基线 LSBert 模型，得分为 0.544。
- en: MLM Prediction Scores
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLM 预测分数
- en: 'LS systems have also relied entirely on MLM prediction scores for SS and SR.
    North et al. ([2022a](#bib.bib34)) and Vásquez-Rodríguez et al. ([2022](#bib.bib55))
    adopt this approach. They have no additional SR steps and rank their candidate
    substitutions per their generated MLM prediction scores. They do, however, apply
    some basic filtering with both studies removing duplicates as well as candidate
    substitutions equal to the complex word. Surprisingly, minimal SR has been shown
    to surpass other more technical approaches (Table [3](#S3 "3 Deep Learning Approaches
    ‣ Deep Learning Approaches to Lexical Simplification: A Survey")). North et al.
    ([2022a](#bib.bib34)) has achieved state-of-the-art performance on the TSAR-2022
    Portuguese dataset, whereas Vásquez-Rodríguez et al. ([2022](#bib.bib55)) has
    consistently produced high performances across the English and Spanish TSAR-2022
    datasets. Only GPT-3 based-models have surpassed these performances Aumiller and
    Gertz ([2022](#bib.bib7)) (Table [3](#S3 "3 Deep Learning Approaches ‣ Deep Learning
    Approaches to Lexical Simplification: A Survey")).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'LS 系统也完全依赖于 MLM 预测分数来进行 SS 和 SR。North 等人 ([2022a](#bib.bib34)) 和 Vásquez-Rodríguez
    等人 ([2022](#bib.bib55)) 采用了这种方法。他们没有额外的 SR 步骤，而是根据生成的 MLM 预测分数对候选替代项进行排序。然而，他们确实进行了基本的过滤，两项研究都去除了重复项以及与复杂词相同的候选替代项。令人惊讶的是，最小
    SR 已被证明优于其他更技术性的 approaches (Table [3](#S3 "3 Deep Learning Approaches ‣ Deep
    Learning Approaches to Lexical Simplification: A Survey"))。North 等人 ([2022a](#bib.bib34))
    在 TSAR-2022 葡萄牙语数据集上达到了最先进的性能，而 Vásquez-Rodríguez 等人 ([2022](#bib.bib55)) 在英语和西班牙语
    TSAR-2022 数据集上 consistently 提供了高性能。只有基于 GPT-3 的模型超越了这些表现 Aumiller 和 Gertz ([2022](#bib.bib7))
    (Table [3](#S3 "3 Deep Learning Approaches ‣ Deep Learning Approaches to Lexical
    Simplification: A Survey"))。'
- en: 4 Resources
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: 'Post 2017 LS datasets have been created for either all sub-tasks within the
    LS pipeline or for a specific purpose (Appendix, Table [2](#A1.T2 "Table 2 ‣ Appendix
    A Appendix ‣ Deep Learning Approaches to Lexical Simplification: A Survey")).
    Recent international competitions (shared-tasks) have also provided their own
    LS datasets (*). LS resources are available for multiple languages, predominately
    English (EN), Spanish (ES), Portuguese (PT), French (FR), Japanese (JP), and Chinese
    (ZH).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '2017 年后创建的 LS 数据集已经覆盖了 LS 流程中的所有子任务或用于特定目的 (Appendix, Table [2](#A1.T2 "Table
    2 ‣ Appendix A Appendix ‣ Deep Learning Approaches to Lexical Simplification:
    A Survey"))。最近的国际竞赛（共享任务）也提供了他们自己的 LS 数据集 (*)。LS 资源可用于多种语言，主要包括英语（EN）、西班牙语（ES）、葡萄牙语（PT）、法语（FR）、日语（JP）和中文（ZH）。'
- en: 4.1 English
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 英语
- en: Personalized-LS
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 个性化 LS
- en: Lee and Yeung ([2018b](#bib.bib25)) constructed a dataset of 12,000 English
    words for personalized LS. These words were ranked on a five-point Likert scale.
    15 native Japanese speakers were tasked with rating the complexity of each word.
    These complexity rating were then applied to BenchLS, in turn personalizing the
    dataset for Japanese speakers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Lee 和 Yeung ([2018b](#bib.bib25)) 为个性化语言简化（LS）构建了一个包含12,000个英语单词的数据集。这些单词根据五级李克特量表进行了排名。15位以日语为母语的评估者负责对每个单词的复杂度进行评分。这些复杂度评分随后被应用于
    BenchLS，从而为日语使用者个性化了数据集。
- en: WCL
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: WCL
- en: Maddela and Xu ([2018](#bib.bib28)) introduced the Word Complexity Lexicon (WCL).
    The WCL is a dataset made up of 15,000 English words annotated with complexity
    ratings. Annotators were 11 non-native English speakers using a six-point Likert
    scale.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Maddela 和 Xu ([2018](#bib.bib28)) 介绍了单词复杂度词典（WCL）。WCL 是一个由15,000个带有复杂度评分的英语单词组成的数据集。标注者是11位非母语英语使用者，使用六级李克特量表进行标注。
- en: LCP-2021*
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LCP-2021*
- en: The dataset provided at the LCP-2021 shared-task (CompLex) Shardlow et al. ([2020](#bib.bib49)),
    was developed using crowd sourcing. 10,800 complex words in context were selected
    from three corpora covering the Bible, biomedical articles, and European Parliamentary
    proceedings. Their lexical complexities were annotated using a 5-point Likert
    scale.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LCP-2021 共享任务（CompLex）中提供的数据集（Shardlow et al. ([2020](#bib.bib49)）是通过众包方式开发的。10,800
    个复杂单词的语境从涵盖《圣经》、生物医学文章和欧洲议会记录的三个语料库中选择。它们的词汇复杂度使用了5级李克特量表进行标注。
- en: SimpleText-2021*
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SimpleText-2021*
- en: 'The SimpleText-2021 shared-task Ermakova et al. ([2021](#bib.bib16)) introduced
    three pilot tasks: 1). to select passages to be simplified, 2). to identify complex
    concepts within these passages, and 3). to simplify these complex concepts to
    generate an easier to understand passage. They provided their participants with
    two sources of data, these being the Citation Network Dataset, DBLP+Citation,
    ACM Citation network, together with titles extracted from The Guardian newspaper
    with manually annotated keywords.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: SimpleText-2021 共享任务中，Ermakova 等人 ([2021](#bib.bib16)) 引入了三个试点任务：1) 选择需要简化的段落，2)
    识别这些段落中的复杂概念，3) 简化这些复杂概念以生成更易理解的段落。他们为参与者提供了两个数据来源，这些数据分别是引用网络数据集、DBLP+Citation、ACM
    引用网络，以及从《卫报》报纸中提取的带有手动标注关键词的标题。
- en: TSAR-2022*
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TSAR-2022*
- en: TSAR-2022 Saggion et al. ([2022](#bib.bib46)) supplied datasets in English,
    Spanish, and Portuguese. These datasets contained target words in contexts taken
    from journalistic texts and Wikipedia articles, along with 10 candidate substitutions
    (approx. 20 in raw data) provided by crowd-sourced annotators located in the UK,
    Spain, and Brazil. The candidate substitutions were ranked per their suggestion
    frequency. The English, Spanish, and Portuguese datasets contained 386, 381, and
    386 instances, respectively.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: TSAR-2022 Saggion 等人 ([2022](#bib.bib46)) 提供了英文、西班牙文和葡萄牙文的数据集。这些数据集中包含了来自新闻文本和维基百科文章的目标单词及其上下文，以及由位于英国、西班牙和巴西的众包标注者提供的10个候选替换词（原始数据中约为20个）。候选替换词根据建议频率进行了排名。这些数据集的英文、西班牙文和葡萄牙文实例分别为386、381和386个。
- en: 4.2 Datasets in Other Languages
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 其他语言的数据集
- en: Spanish
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 西班牙语
- en: The ALexS-2020 shared-task Zambrano and Ráez ([2020](#bib.bib61)) included a
    Spanish dataset consisting of 723 complex words from recorded transcripts. Merejildo
    ([2021](#bib.bib31)) provided the Spanish CWI corpus (ES-CWI). A group of 40 native-speaking
    Spanish annotators identified complex words within 3,887 academic texts. The EASIER
    corpus Alarcón et al. ([2021b](#bib.bib3)) contains 5,310 Spanish complex words
    in contexts taken from newspapers with 7,892 candidate substitutions. A small
    version of the corpus is also provided with 500 instances (EASIER-500).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ALexS-2020 共享任务中，Zambrano 和 Ráez ([2020](#bib.bib61)) 包含了一个西班牙语数据集，其中包含来自记录转录本的723个复杂单词。Merejildo
    ([2021](#bib.bib31)) 提供了西班牙语CWI语料库（ES-CWI）。40名西班牙语母语标注者在3,887篇学术文本中识别了复杂单词。EASIER语料库（Alarcón
    et al. ([2021b](#bib.bib3)））包含了5,310个西班牙语复杂单词的上下文，并提供了7,892个候选替换词。还有一个小版本的语料库，包含500个实例（EASIER-500）。
- en: Portuguese
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 葡萄牙语
- en: The PorSimples dataset Aluísio and Gasperin ([2010](#bib.bib5)) consists of
    extracts taken from Brazilian newspapers. The dataset is divided into nine sub-corpora
    separated by degree of simplification and source text. The PorSimplesSent dataset
    Leal et al. ([2018](#bib.bib23)) was adapted from the previous PorSimples dataset.
    It contains strong and natural simplifications of PorSimples’s original sentences.
    SIMPLEX-PB Hartmann and Aluísio ([2020](#bib.bib19)) provides a selection of features
    for each of its candidate substitutions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: PorSimples数据集Aluísio和Gasperin（[2010](#bib.bib5)）包含从巴西报纸中提取的片段。数据集被分为九个子语料库，按简化程度和来源文本进行区分。PorSimplesSent数据集Leal等（[2018](#bib.bib23)）是从之前的PorSimples数据集中改编的。它包含对PorSimples原始句子的强简化和自然简化。SIMPLEX-PB
    Hartmann和Aluísio（[2020](#bib.bib19)）提供了每个候选替换的特征选择。
- en: French
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 法语
- en: 'ReSyf contains French synonyms that have been ranked in regards to their reading
    difficulty using a SVM Billami et al. ([2018](#bib.bib8)). It consists of 57,589
    instances with a total of 148,648 candidate substitutions. FrenchLys is a LS tool
    designed by Rolin et al. ([2021](#bib.bib45)). It provides its own dataset that
    contains sentences sampled from a French TS dataset: ALECTOR, and french schoolbooks.
    Substitute candidates were provided by 20 French speaking annotators.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ReSyf包含了按阅读难度排名的法语同义词，使用了SVM Billami等（[2018](#bib.bib8)）。它由57,589个实例组成，总共有148,648个候选替换。FrenchLys是Rolin等（[2021](#bib.bib45)）设计的一个LS工具。它提供了自己的数据集，包括从法语TS数据集ALECTOR和法国教科书中抽取的句子。候选替换由20名讲法语的标注者提供。
- en: Japanese
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 日语
- en: 'The Japanese Lexical Substitution (JLS) dataset Kajiwara and Yamamoto ([2015](#bib.bib21))
    contains 243 target words, each with 10 contexts (2,430 instances in total). Crowd-sourced
    annotators provided and ranked candidate substitutions. The JLS Balanced Dataset
    Kodaira et al. ([2016](#bib.bib22)) expanded the previous JLS dataset to make
    it more representative of different genres and contains 2,010 generalized instances.
    Nishihara and Kajiwara ([2020](#bib.bib33)) created a new dataset (JWCL & JSSL)
    that increased the Japanese Education Vocabulary List (JEV). It houses 18,000
    Japanese words divided into three levels of difficulty: easy, medium, or difficult.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 日语词汇替换（JLS）数据集Kajiwara和Yamamoto（[2015](#bib.bib21)）包含243个目标词，每个目标词有10个上下文（总共2,430个实例）。众包标注者提供并排序了候选替换。JLS平衡数据集Kodaira等（[2016](#bib.bib22)）扩展了之前的JLS数据集，使其更具代表性，并包含2,010个泛化实例。Nishihara和Kajiwara（[2020](#bib.bib33)）创建了一个新的数据集（JWCL
    & JSSL），增加了日本教育词汇表（JEV）。它包含18,000个日语词汇，分为三个难度等级：简单、中等或困难。
- en: Chinese
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 汉语
- en: Personalized-ZH Lee and Yeung ([2018a](#bib.bib24)) consists of 600 Chinese
    words. Each word’s complexity was ranked by eight learners of Chinese on a 5-point
    lickert-scale. HanLS was constructed by Qiang et al. ([2021](#bib.bib44)). It
    contains 534 Chinese complex words. 5 native-speaking annotators gave and ranked
    candidate substitutions. Each complex word has on average 8 candidate substitutions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Personalized-ZH Lee和Yeung（[2018a](#bib.bib24)）包含600个中文词汇。每个词汇的复杂性由八名中文学习者在5点评分量表上进行排名。HanLS由Qiang等（[2021](#bib.bib44)）构建。它包含534个中文复杂词汇。5名以中文为母语的标注者提供并排名了候选替换。每个复杂词汇平均有8个候选替换。
- en: 5 Discussion and Conclusion
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与结论
- en: 'Since the 2017 survey on LS Paetzold and Specia ([2017b](#bib.bib38)), deep
    learning approaches have provided new headway within the field. MLM is now the
    go to method for SG, with the majority of recent LS studies having employed a
    MLM objective. The casual language model: GPT-3, surpasses the performance of
    all other approaches when subjected to prompt learning, especially when an ensemble
    of prompts are taken into consideration (Table [3](#S3 "3 Deep Learning Approaches
    ‣ Deep Learning Approaches to Lexical Simplification: A Survey")). The prediction
    scores of MLM or casual language modeling have replaced various SS and SR techniques.
    LS systems that employ minimal SS and no SR apart from ranking their LLM’s prediction
    scores, have outperformed more technical, feature-oriented, and unsupervised ranking
    methods (Table [3](#S3 "3 Deep Learning Approaches ‣ Deep Learning Approaches
    to Lexical Simplification: A Survey")). However, an exception is made with regards
    to equivalence score Li et al. ([2022](#bib.bib26)), which has been shown to be
    effective at SR.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '自从2017年Paetzold和Specia的LS调查（[2017b](#bib.bib38)）以来，深度学习方法在这一领域取得了新的进展。MLM现在是SG的首选方法，最近的大多数LS研究都采用了MLM目标。对话语言模型GPT-3，在进行提示学习时超越了所有其他方法的表现，尤其是当考虑到提示的集合时（表[3](#S3
    "3 Deep Learning Approaches ‣ Deep Learning Approaches to Lexical Simplification:
    A Survey")）。MLM或对话语言建模的预测分数已取代了各种SS和SR技术。那些除了对LLM的预测分数进行排名之外，几乎不使用SS和不使用SR的LS系统，已超越了更多技术性、特征导向和无监督的排名方法（表[3](#S3
    "3 Deep Learning Approaches ‣ Deep Learning Approaches to Lexical Simplification:
    A Survey")）。然而，关于等效分数Li等（[2022](#bib.bib26)）的研究表明，它在SR方面仍然有效。'
- en: Future LS systems will make use of new advances in deep learning. We believe
    prompt learning and models, such as GPT-3, will become increasingly popular, given
    their state-of-the-art performance at SG. Using an ensemble of various prompts
    for SS and SR may advance LS performance. In addition, the creation of new metrics
    similar to equivalence score will likewise be beneficial.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的LS系统将利用深度学习的新进展。我们相信，鉴于其在SG领域的最先进表现，**prompt learning**和类似GPT-3的模型将变得越来越受欢迎。使用各种提示的集合进行SS和SR可能会提升LS的表现。此外，创建类似于等效分数的新指标也将是有益的。
- en: 5.1 Open Challenges in LS
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 LS中的开放挑战
- en: LS has a number of open research areas that are either unaddressed, or the current
    body of work is inconclusive. In this brief section, we conclude this survey by
    outlining a few key areas for future development of LS research.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: LS有许多未解决或现有工作尚无结论的开放研究领域。在这一简要部分，我们通过概述一些关键领域来总结这项调查，为LS研究的未来发展提供方向。
- en: 'Evaluation:'
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估：
- en: 'The metrics we use to evaluate LS are not perfect (Section [2.1](#S2.SS1 "2.1
    Evaluation Metrics ‣ 2 Pipeline ‣ Deep Learning Approaches to Lexical Simplification:
    A Survey")). Automated metrics that condense a wide problem into a single numerical
    score can harm outcomes with human participants. Development of more faithful
    resources, as well as direct evaluation with intended user groups of simplification
    systems is a fruitful avenue for future work. This can be done by taking into
    consideration variation in data annotation instead of labels produced by aggregating
    unique annotations as in most datasets currently available.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们用于评估LS的指标并不完美（第[2.1节](#S2.SS1 "2.1 Evaluation Metrics ‣ 2 Pipeline ‣ Deep
    Learning Approaches to Lexical Simplification: A Survey")）。将广泛问题压缩成单一数值分数的自动化指标可能会对人类参与者的结果产生负面影响。开发更可靠的资源以及与简化系统的预期用户群体进行直接评估是未来工作的一个有益方向。这可以通过考虑数据注释的变异，而不是像当前大多数可用数据集中那样汇总的唯一注释标签来完成。'
- en: 'Explainability:'
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可解释性：
- en: Lexical simplifications are inherently more explainable than sentence simplification
    as the operations are directly applied at the lexeme level. However, the decision
    process on whether to simplify and which word to choose is increasingly hidden
    behind the black-box of a model. Work to explain and interpret these decisions
    will allow researchers to better understand the opportunities and threats of applying
    modern NLP techniques to LS research.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇简化本质上比句子简化更具可解释性，因为操作直接在词汇层面上进行。然而，是否简化以及选择哪个词的决策过程越来越隐藏在模型的黑箱中。解释和解读这些决策的工作将帮助研究人员更好地理解将现代NLP技术应用于LS研究的机遇和威胁。
- en: 'Personalization:'
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 个性化：
- en: One model does not fit all. The simplification needs of a language learner compared
    to a stroke victim, compared to a child are each very different. Modeling these
    needs and using them to personalize LS systems will allow for personalized simplification
    output more adequate the needs of particular user groups.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一种模型并不适用于所有人。语言学习者、脑卒中患者和儿童的简化需求各不相同。建模这些需求并利用它们个性化LS系统，将允许提供更符合特定用户群体需求的个性化简化输出。
- en: 'Perspectivism:'
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Perspectivism:'
- en: Even within a population of common characteristics, each individual will bring
    a unique perspective on what and how to simplify. Systems which can alter their
    outputs to each user’s needs will provide adaptive simplifications that go beyond
    our current technology. This will, in turn, improve the evaluation of LS models
    as previously discussed in this section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在具有共同特征的群体中，每个个体也会对简化的内容和方式带来独特的视角。能够根据每个用户的需求调整输出的系统，将提供超越我们当前技术的适应性简化。这反过来将改善对LS模型的评估，正如本节之前讨论的那样。
- en: 'Integration:'
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Integration:'
- en: LS is only one part of the wider simplification puzzle. Integrating LS systems
    with explanation generation, redundancy removal, and sentence splitting will further
    accelerate the adoption of automated simplification practices beyond the halls
    of research allowing such technology to reach a wider audience.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: LS只是更广泛简化拼图的一部分。将LS系统与解释生成、冗余去除和句子拆分整合，将进一步加速自动化简化实践的推广，让这些技术超越研究领域，触及更广泛的受众。
- en: References
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: 'Al-Thanyyan and Azmi (2021) Suha S. Al-Thanyyan and Aqil M. Azmi. 2021. Automated
    Text Simplification: A Survey. *ACM Comput. Surv.*, 54(2).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Thanyyan和Azmi（2021）Suha S. Al-Thanyyan和Aqil M. Azmi. 2021. 自动化文本简化：综述。*ACM
    Comput. Surv.*，54(2)。
- en: Alarcón et al. (2021a) Rodrigo Alarcón, Lourdes Moreno, and Paloma Martínez.
    2021a. Exploration of Spanish Word Embeddings for Lexical Simplification. In *Proceedings
    of CTTS*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alarcón等（2021a）Rodrigo Alarcón、Lourdes Moreno和Paloma Martínez. 2021a. 探索西班牙语词嵌入用于词汇简化。在*CTTS会议录*中。
- en: Alarcón et al. (2021b) Rodrigo Alarcón, Lourdes Moreno, and Paloma Martínez.
    2021b. Lexical Simplification System to Improve Web Accessibility. *IEEE Access*,
    9:58755–58767.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alarcón等（2021b）Rodrigo Alarcón、Lourdes Moreno和Paloma Martínez. 2021b. 改进网页可访问性的词汇简化系统。*IEEE
    Access*，9:58755–58767。
- en: 'Aleksandrova and Brochu Dufour (2022) Desislava Aleksandrova and Olivier Brochu Dufour.
    2022. RCML at TSAR-2022 Shared Task: Lexical Simplification With Modular Substitution
    Candidate Ranking. In *Proceedings of TSAR*.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aleksandrova和Brochu Dufour（2022）Desislava Aleksandrova和Olivier Brochu Dufour.
    2022. RCML在TSAR-2022共享任务中的应用：带有模块化替代候选排名的词汇简化。在*TSAR会议录*中。
- en: 'Aluísio and Gasperin (2010) Sandra Maria Aluísio and Caroline Gasperin. 2010.
    Fostering digital inclusion and accessibility: The porsimples project for simplification
    of portuguese texts. In *Proceedings of YIWCALA*.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aluísio和Gasperin（2010）Sandra Maria Aluísio和Caroline Gasperin. 2010. 促进数字包容性和可访问性：porsimples项目用于简化葡萄牙语文本。在*YIWCALA会议录*中。
- en: 'Arefyev et al. (2020) Nikolay Arefyev, Boris Sheludko, Alexander Podolskiy,
    and Alexander Panchenko. 2020. Always Keep your Target in Mind: Studying Semantics
    and Improving Performance of Neural Lexical Substitution. In *Proceedings of COLING*.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arefyev等（2020）Nikolay Arefyev、Boris Sheludko、Alexander Podolskiy和Alexander Panchenko.
    2020. 始终牢记目标：研究语义并提高神经词汇替代的表现。在*COLING会议录*中。
- en: 'Aumiller and Gertz (2022) Dennis Aumiller and Michael Gertz. 2022. UniHD at
    TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification? In *Proceedings
    of TSAR*.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aumiller和Gertz（2022）Dennis Aumiller和Michael Gertz. 2022. UniHD在TSAR-2022共享任务中的应用：计算是否是词汇简化所需的一切？在*TSAR会议录*中。
- en: 'Billami et al. (2018) Mokhtar B. Billami, Thomas François, and Núria Gala.
    2018. ReSyf: a French lexicon with ranked synonyms. In *Proceedings of ACL*.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Billami等（2018）Mokhtar B. Billami、Thomas François和Núria Gala. 2018. ReSyf：一个带有排名同义词的法语词典。在*ACL会议录*中。
- en: Bojanowski et al. (2016) Piotr Bojanowski, Edouard Grave, Armand Joulin, and
    Tomas Mikolov. 2016. Enriching Word Vectors with Subword Information. *arXiv preprint
    arXiv:1607.04606*.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bojanowski等（2016）Piotr Bojanowski、Edouard Grave、Armand Joulin和Tomas Mikolov.
    2016. 通过子词信息丰富词向量。*arXiv预印本arXiv:1607.04606*。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, and Others. 2020. Language Models Are Few-Shot
    Learners. In *Proceedings of NeurIPS*.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared Kaplan、Prafulla
    Dhariwal和其他人. 2020. 语言模型是少样本学习者。在*NeurIPS会议录*中。
- en: Cañete et al. (2020) José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui
    Ho, Hojin Kang, and Jorge Pérez. 2020. Spanish pre-trained bert model and evaluation
    data. In *Proceedings of PML4DC at ICLR*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cañete et al. (2020) José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui
    Ho, Hojin Kang 和 Jorge Pérez. 2020. 西班牙语预训练 BERT 模型及评估数据。见于 *PML4DC at ICLR 会议论文集*。
- en: 'Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc Le, and Christopher
    Manning. 2020. ELECTRA: Pre-training text encoders as discriminators rather than
    generators. In *Proceedings of ICLR*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc Le 和 Christopher Manning.
    2020. ELECTRA: 预训练文本编码器作为判别器而非生成器。见于 *ICLR 会议论文集*。'
- en: Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, and Others. 2020. Unsupervised cross-lingual representation learning
    at scale. In *Proceedings of ACL*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary 和其他人。2020. 大规模无监督跨语言表示学习。见于 *ACL 会议论文集*。
- en: De la Rosa and Fernández (2022) Javier De la Rosa and Andres Fernández. 2022.
    Zero-shot reading comprehension and reasoning for spanish with BERTIN GPT-J-6B.
    In *Proceedings of SEPLN*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De la Rosa and Fernández (2022) Javier De la Rosa 和 Andres Fernández. 2022.
    基于 BERTIN GPT-J-6B 的西班牙语零样本阅读理解与推理。见于 *SEPLN 会议论文集*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of NAACL*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    2019. Bert: 深度双向变换器的预训练用于语言理解。见于 *NAACL 会议论文集*。'
- en: Ermakova et al. (2021) Liana Ermakova, Patrice Bellot, Pavel Braslavski, Jaap
    Kamps, Josiane Mothe, and Others. 2021. Overview of SimpleText CLEF 2021 Workshop
    and Pilot Tasks. In *Proceedings of LREC*.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ermakova et al. (2021) Liana Ermakova, Patrice Bellot, Pavel Braslavski, Jaap
    Kamps, Josiane Mothe 和其他人。2021. SimpleText CLEF 2021 研讨会及试点任务概述。见于 *LREC 会议论文集*。
- en: 'Fandiño et al. (2022) Asier Gutiérrez Fandiño, Jordi Armengol Estapé, Marc
    Pàmies, Joan Llop Palao, Joaquin Silveira Ocampo, and Others. 2022. Maria: Spanish
    language models. *Procesamiento del Lenguaje Natural*, 68.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fandiño et al. (2022) Asier Gutiérrez Fandiño, Jordi Armengol Estapé, Marc
    Pàmies, Joan Llop Palao, Joaquin Silveira Ocampo 和其他人。2022. Maria: 西班牙语语言模型。*Procesamiento
    del Lenguaje Natural*, 68。'
- en: 'Ferres and Saggion (2022) Daniel Ferres and Horacio Saggion. 2022. ALEXSIS:
    A dataset for lexical simplification in Spanish. In *Proceedings of LREC*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ferres and Saggion (2022) Daniel Ferres 和 Horacio Saggion. 2022. ALEXSIS: 用于西班牙语词汇简化的数据集。见于
    *LREC 会议论文集*。'
- en: Hartmann and Aluísio (2020) Nathan Siegle Hartmann and Sandra Maria Aluísio.
    2020. Adaptação lexical automática em textos informativos do português brasileiro
    para o ensino fundamental. *Linguamática*, 12(2):3–27.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hartmann and Aluísio (2020) Nathan Siegle Hartmann 和 Sandra Maria Aluísio. 2020.
    巴西葡萄牙语信息文本的自动词汇适应。*Linguamática*, 12(2):3–27。
- en: Horn et al. (2014) Colby Horn, Cathryn Manduca, and David Kauchak. 2014. Learning
    a lexical simplifier using Wikipedia. In *Proceedings of ACL*.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horn et al. (2014) Colby Horn, Cathryn Manduca 和 David Kauchak. 2014. 使用维基百科学习词汇简化器。见于
    *ACL 会议论文集*。
- en: Kajiwara and Yamamoto (2015) Tomoyuki Kajiwara and Kazuhide Yamamoto. 2015.
    Evaluation Dataset and System for Japanese Lexical Simplification. In *Proceedings
    of ACL*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kajiwara and Yamamoto (2015) Tomoyuki Kajiwara 和 Kazuhide Yamamoto. 2015. 用于日语词汇简化的评估数据集和系统。见于
    *ACL 会议论文集*。
- en: Kodaira et al. (2016) Tomonori Kodaira, Tomoyuki Kajiwara, and Mamoru Komachi.
    2016. Controlled and Balanced Dataset for Japanese Lexical Simplification. In
    *Proceedings of ACL*.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kodaira et al. (2016) Tomonori Kodaira, Tomoyuki Kajiwara 和 Mamoru Komachi.
    2016. 用于日语词汇简化的受控和平衡数据集。见于 *ACL 会议论文集*。
- en: Leal et al. (2018) Sidney Evaldo Leal, Magali Sanches Duran, and Sandra Maria
    Aluísio. 2018. A nontrivial sentence corpus for the task of sentence readability
    assessment in Portuguese. In *Proceedings of COLING*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leal et al. (2018) Sidney Evaldo Leal, Magali Sanches Duran 和 Sandra Maria Aluísio.
    2018. 用于评估葡萄牙语句子可读性的非平凡句子语料库。见于 *COLING 会议论文集*。
- en: Lee and Yeung (2018a) John Lee and Chak Yan Yeung. 2018a. Automatic prediction
    of vocabulary knowledge for learners of chinese as a foreign language. In *Proceedings
    of ICNLSP*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee and Yeung (2018a) John Lee 和 Chak Yan Yeung. 2018a. 为汉语作为外语学习者自动预测词汇知识。见于
    *ICNLSP 会议论文集*。
- en: Lee and Yeung (2018b) John Lee and Chak Yan Yeung. 2018b. Personalizing lexical
    simplification. In *Proceedings of COLING*.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee and Yeung (2018b) John Lee 和 Chak Yan Yeung. 2018b. 个性化词汇简化。见于 *COLING 会议论文集*。
- en: 'Li et al. (2022) Xiaofei Li, Daniel Wiechmann, Yu Qiao, and Elma Kerz. 2022.
    MANTIS at TSAR-2022 Shared Task: Improved Unsupervised Lexical Simplification
    with Pretrained Encoders. In *Proceedings of TSAR*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Xiaofei Li, Daniel Wiechmann, Yu Qiao 和 Elma Kerz. 2022. MANTIS
    在 TSAR-2022 共享任务中的应用：基于预训练编码器的改进无监督词汇简化。见于 *TSAR 会议论文集*。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, and Others.
    2019. Roberta: A robustly optimized bert pretraining approach. *arXiv preprint
    arXiv:1907.11692*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du 和其他人。2019. Roberta：一种稳健优化的
    BERT 预训练方法。*arXiv 预印本 arXiv:1907.11692*。
- en: Maddela and Xu (2018) Mounica Maddela and Wei Xu. 2018. A word-complexity lexicon
    and a neural readability ranking model for lexical simplification. In *Proceedings
    of EMNLP*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maddela 和 Xu (2018) Mounica Maddela 和 Wei Xu. 2018. 一种词汇复杂性词典和用于词汇简化的神经可读性排名模型。在
    *EMNLP 会议论文集* 中。
- en: 'McCarthy and Navigli (2007) Diana McCarthy and Roberto Navigli. 2007. SemEval-2007
    Task 10: English Lexical Substitution Task. In *Proceedings of SemEval*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCarthy 和 Navigli (2007) Diana McCarthy 和 Roberto Navigli. 2007. SemEval-2007
    任务 10：英语词汇替代任务。在 *SemEval 会议论文集* 中。
- en: 'Melamud et al. (2016) Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016.
    context2vec: Learning Generic Context Embedding with Bidirectional LSTM. In *Proceedings
    of SIGNLL*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Melamud 等 (2016) Oren Melamud, Jacob Goldberger 和 Ido Dagan. 2016. context2vec：通过双向
    LSTM 学习通用上下文嵌入。在 *SIGNLL 会议论文集* 中。
- en: Merejildo (2021) Borbor Merejildo. 2021. Creación de un corpus de textos universitarios
    en español para la identificación de palabras complejas en el área de la simplificación
    léxica. Master’s thesis, Universidad de Guayaquil.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merejildo (2021) Borbor Merejildo. 2021. 创建西班牙语大学文本语料库以识别复杂词汇在词汇简化领域的应用。硕士论文，瓜亚基尔大学。
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. Efficient Estimation of word Representations in Vector Space. In *Proceedings
    of ICLR*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等 (2013) Tomas Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean. 2013.
    在向量空间中有效估计词表示。在 *ICLR 会议论文集* 中。
- en: Nishihara and Kajiwara (2020) Daiki Nishihara and Tomoyuki Kajiwara. 2020. Word
    Complexity Estimation for Japanese Lexical Simplification. In *Proceedings of
    LREC*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nishihara 和 Kajiwara (2020) Daiki Nishihara 和 Tomoyuki Kajiwara. 2020. 用于日语词汇简化的词汇复杂性估计。在
    *LREC 会议论文集* 中。
- en: 'North et al. (2022a) Kai North, Alphaeus Dmonte, Tharindu Ranasinghe, and Marcos
    Zampieri. 2022a. GMU-WLV at TSAR-2022 Shared Task: Evaluating Lexical Simplification
    Models. In *Proceedings of TSAR*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: North 等 (2022a) Kai North, Alphaeus Dmonte, Tharindu Ranasinghe 和 Marcos Zampieri.
    2022a. GMU-WLV 在 TSAR-2022 共享任务中的表现：评估词汇简化模型。在 *TSAR 会议论文集* 中。
- en: 'North et al. (2022b) Kai North, Marcos Zampieri, and Matthew Shardlow. 2022b.
    Lexical Complexity Prediction: An Overview. *ACM Computing Surveys*, 55(9).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: North 等 (2022b) Kai North, Marcos Zampieri 和 Matthew Shardlow. 2022b. 词汇复杂性预测：概述。*ACM
    计算机调查*，55(9)。
- en: 'Paetzold and Specia (2016a) Gustavo Paetzold and Lucia Specia. 2016a. SemEval
    2016 Task 11: Complex Word Identification. In *Proceedings of SemEval*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paetzold 和 Specia (2016a) Gustavo Paetzold 和 Lucia Specia. 2016a. SemEval 2016
    任务 11：复杂词汇识别。在 *SemEval 会议论文集* 中。
- en: Paetzold and Specia (2017a) Gustavo Paetzold and Lucia Specia. 2017a. Lexical
    simplification with neural ranking. In *Proceedings of EACL*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paetzold 和 Specia (2017a) Gustavo Paetzold 和 Lucia Specia. 2017a. 使用神经排名的词汇简化。在
    *EACL 会议论文集* 中。
- en: Paetzold and Specia (2017b) Gustavo H. Paetzold and Lucia Specia. 2017b. A Survey
    on Lexical Simplification. *J. Artif. Int. Res.*, 60(1):549–593.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paetzold 和 Specia (2017b) Gustavo H. Paetzold 和 Lucia Specia. 2017b. 词汇简化调查。*J.
    Artif. Int. Res.*，60(1):549–593。
- en: Paetzold and Specia (2016b) Gustavo Henrique Paetzold and Lucia Specia. 2016b.
    Benchmarking Lexical Simplification Systems. In *Proceedings of LREC*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paetzold 和 Specia (2016b) Gustavo Henrique Paetzold 和 Lucia Specia. 2016b. 词汇简化系统的基准测试。在
    *LREC 会议论文集* 中。
- en: Paetzold and Specia (2016c) Gustavo Henrique Paetzold and Lucia Specia. 2016c.
    Unsupervised lexical simplification for non-native speakers. In *Proceedings of
    AAAI*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paetzold 和 Specia (2016c) Gustavo Henrique Paetzold 和 Lucia Specia. 2016c. 针对非母语者的无监督词汇简化。在
    *AAAI 会议论文集* 中。
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized
    Word Representations. In *Proceedings of NAACL*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 等 (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee 和 Luke Zettlemoyer. 2018. 深度上下文化词表示。在 *NAACL 会议论文集*
    中。
- en: Przybyła and Shardlow (2020) Piotr Przybyła and Matthew Shardlow. 2020. Multi-Word
    Lexical Simplification. In *Proceedings of COLING*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Przybyła 和 Shardlow (2020) Piotr Przybyła 和 Matthew Shardlow. 2020. 多词汇词汇简化。在
    *COLING 会议论文集* 中。
- en: Qiang et al. (2020) Jipeng Qiang, Yun Li, Zhu Yi, Yunhao Yuan, and Xindong Wu.
    2020. Lexical simplification with pretrained encoders. In *Proceedings of AAAI*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiang 等 (2020) Jipeng Qiang, Yun Li, Zhu Yi, Yunhao Yuan 和 Xindong Wu. 2020.
    使用预训练编码器进行词汇简化。在 *AAAI 会议论文集* 中。
- en: Qiang et al. (2021) Jipeng Qiang, Xinyu Lu, Yun Li, Yunhao Yuan, and Xindong
    Wu. 2021. Chinese Lexical Simplification. *IEEE/ACM Transactions on Audio, Speech,
    and Language Processing*, 29:1819–1828.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiang 等 (2021) Jipeng Qiang、Xinyu Lu、Yun Li、Yunhao Yuan 和 Xindong Wu。2021。中文词汇简化。*IEEE/ACM
    音频、语音和语言处理期刊*, 29:1819–1828。
- en: 'Rolin et al. (2021) Eva Rolin, Quentin Langlois, Patrick Watrin, and Thomas
    François. 2021. FrenLyS: A Tool for the Automatic Simplification of French General
    Language Texts. In *Proceedings of RANLP*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rolin 等 (2021) Eva Rolin、Quentin Langlois、Patrick Watrin 和 Thomas François。2021。FrenLyS：一种自动简化法语普通语言文本的工具。在
    *RANLP 会议录* 中。
- en: Saggion et al. (2022) Horacio Saggion, Sanja Štajner, Daniel Ferrés, Kim Cheng
    Sheang, Matthew Shardlow, Kai North, and Marcos Zampieri. 2022. Findings of the
    TSAR-2022 Shared Task on Multilingual Lexical Simplification. In *Proceedings
    of TSAR*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saggion 等 (2022) Horacio Saggion、Sanja Štajner、Daniel Ferrés、Kim Cheng Sheang、Matthew
    Shardlow、Kai North 和 Marcos Zampieri。2022。TSAR-2022 共享任务中多语言词汇简化的发现。在 *TSAR 会议录*
    中。
- en: 'Seneviratne et al. (2022) Sandaru Seneviratne, Elena Daskalaki, and Hanna Suominen.
    2022. CILS at TSAR-2022 Shared Task: Investigating the Applicability of Lexical
    Substitution Methods for Lexical Simplification. In *Proceedings of TSAR*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seneviratne 等 (2022) Sandaru Seneviratne、Elena Daskalaki 和 Hanna Suominen。2022。CILS
    在 TSAR-2022 共享任务中的表现：探讨词汇替换方法在词汇简化中的适用性。在 *TSAR 会议录* 中。
- en: 'Shardlow (2013) Matthew Shardlow. 2013. The CW Corpus: A New Resource for Evaluating
    the Identification of Complex Words. In *Proceedings of ACL*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shardlow (2013) Matthew Shardlow。2013。CW 语料库：评估复杂词识别的新资源。在 *ACL 会议录* 中。
- en: Shardlow et al. (2020) Matthew Shardlow, Michael Cooper, and Marcos Zampieri.
    2020. CompLex — a new corpus for lexical complexity prediction from Likert Scale
    data. In *Proceedings of READI*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shardlow 等 (2020) Matthew Shardlow、Michael Cooper 和 Marcos Zampieri。2020。CompLex
    - 一个新的 Likert 量表数据词汇复杂性预测语料库。在 *READI 会议录* 中。
- en: Song et al. (2020) Jiayin Song, Jingyue Hu, Leung-Pun Wong, Lap-Kei Lee, and
    Tianyong Hao. 2020. A New Context-Aware Method Based on Hybrid Ranking for Community-Oriented
    Lexical Simplification. In *Proceedings of DASFAA*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 (2020) Jiayin Song、Jingyue Hu、Leung-Pun Wong、Lap-Kei Lee 和 Tianyong Hao。2020。基于混合排名的社区导向词汇简化的新方法。在
    *DASFAA 会议录* 中。
- en: 'Souza et al. (2020) Fábio Souza, Rodrigo Nogueira, and Roberto Lotufo. 2020.
    BERTimbau: pretrained BERT models for Brazilian Portuguese. In *Proceedings of
    BRACIS*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Souza 等 (2020) Fábio Souza、Rodrigo Nogueira 和 Roberto Lotufo。2020。BERTimbau：针对巴西葡萄牙语的预训练
    BERT 模型。在 *BRACIS 会议录* 中。
- en: 'Specia et al. (2012) Lucia Specia, Kumar Jauhar, Sujay, and Rada Mihalcea.
    2012. Semeval - 2012 task 1: English lexical simplification. In *Proceedings of
    SemEval*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Specia 等 (2012) Lucia Specia、Kumar Jauhar、Sujay 和 Rada Mihalcea。2012。Semeval
    - 2012 任务 1：英语词汇简化。在 *SemEval 会议录* 中。
- en: Trask et al. (2015) Andrew Trask, Phil Michalak, and John Liu. 2015. sense2vec
    - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings.
    *ArXiv*, abs/1511.06388.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trask 等 (2015) Andrew Trask、Phil Michalak 和 John Liu。2015。sense2vec - 一种快速准确的词义消歧方法，基于神经词嵌入。*ArXiv*,
    abs/1511.06388。
- en: Uchida et al. (2018) Satoru Uchida, Shohei Takada, and Yuki Arase. 2018. CEFR-based
    Lexical Simplification Dataset. In *Proceedings of LREC*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uchida 等 (2018) Satoru Uchida、Shohei Takada 和 Yuki Arase。2018。基于 CEFR 的词汇简化数据集。在
    *LREC 会议录* 中。
- en: 'Vásquez-Rodríguez et al. (2022) Laura Vásquez-Rodríguez, Nhung Nguyen, Sophia
    Ananiadou, and Matthew Shardlow. 2022. UoM&MMU at TSAR-2022 Shared Task: Prompt
    Learning for Lexical Simplification. In *Proceedings of TSAR*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vásquez-Rodríguez 等 (2022) Laura Vásquez-Rodríguez、Nhung Nguyen、Sophia Ananiadou
    和 Matthew Shardlow。2022。UoM&MMU 在 TSAR-2022 共享任务中的表现：用于词汇简化的提示学习。在 *TSAR 会议录*
    中。
- en: 'Whistely et al. (2022) Peniel John Whistely, Sandeep Mathias, and Galiveeti
    Poornima. 2022. PresiUniv at TSAR-2022 Shared Task: Generation and Ranking of
    Simplification Substitutes of Complex Words in Multiple Languages. In *Proceedings
    of TSAR*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whistely 等 (2022) Peniel John Whistely、Sandeep Mathias 和 Galiveeti Poornima。2022。PresiUniv
    在 TSAR-2022 共享任务中的表现：多语言复杂词的简化替代词生成与排序。在 *TSAR 会议录* 中。
- en: 'Wilkens et al. (2022) Rodrigo Wilkens, David Alfter, Rémi Cardon, Isabelle
    Gribomont, and Others. 2022. CENTAL at TSAR-2022 Shared Task: How Does Context
    Impact BERT-Generated Substitutions for Lexical Simplification? In *Proceedings
    of TSAR*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilkens 等 (2022) Rodrigo Wilkens、David Alfter、Rémi Cardon、Isabelle Gribomont
    和其他人。2022。CENTAL 在 TSAR-2022 共享任务中的表现：上下文如何影响 BERT 生成的词汇简化替代词？在 *TSAR 会议录* 中。
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan
    Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining
    for Language Understanding. In *Proceedings of NeurIPS*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan
    Salakhutdinov 和 Quoc V. Le. 2019. XLNet：语言理解的广义自回归预训练。在 *NeurIPS 论文集* 中。
- en: Yeung and Lee (2018) Chak Yan Yeung and John Lee. 2018. Personalized text retrieval
    for learners of Chinese as a foreign language. In *Proceedings of COLING*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeung 和 Lee (2018) Chak Yan Yeung 和 John Lee. 2018. 针对中文学习者的个性化文本检索。在 *COLING
    论文集* 中。
- en: Yimam et al. (2018) Seid Muhie Yimam, Chris Biemann, Shervin Malmasi, Gustavo
    Paetzold, Luci Specia, Sanja Štajner, Anaïs Tack, and Marcos Zampieri. 2018. A
    Report on the Complex Word Identification Shared Task 2018. In *Proceedings of
    BEA*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yimam 等人 (2018) Seid Muhie Yimam, Chris Biemann, Shervin Malmasi, Gustavo Paetzold,
    Luci Specia, Sanja Štajner, Anaïs Tack 和 Marcos Zampieri. 2018. 关于复杂词识别共享任务 2018
    的报告。在 *BEA 论文集* 中。
- en: 'Zambrano and Ráez (2020) Jenny Alexandra Ortiz Zambrano and Arturo Montejo
    Ráez. 2020. Overview of ALexS 2020: First Workshop on Lexical Analysis at SEPLN.
    In *Proceedings of ALexS*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zambrano 和 Ráez (2020) Jenny Alexandra Ortiz Zambrano 和 Arturo Montejo Ráez.
    2020. ALexS 2020 概述：SEPLN 首次词汇分析研讨会。在 *ALexS 论文集* 中。
- en: Appendix A Appendix
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: '|  | Dataset | LS Pipeline | Languages | # CWs | Avg. # Subs | Domain | Annotators
    | Paper |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据集 | LS 流程 | 语言 | # CWs | 平均 # Subs | 领域 | 注释员 | 论文 |'
- en: '| Pre-2017 | LS–2007* | SG, SS | EN | 201 | 1 | Mix | 5 UK-based. | McCarthy
    and Navigli ([2007](#bib.bib29)) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Pre-2017 | LS–2007* | SG, SS | EN | 201 | 1 | 混合 | 5 英国基础。 | McCarthy 和 Navigli
    ([2007](#bib.bib29)) |'
- en: '| PorSimples | SG, SS | PT | 3066 | 1 | News | 1 Linguist. | Aluísio and Gasperin
    ([2010](#bib.bib5)) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| PorSimples | SG, SS | PT | 3066 | 1 | 新闻 | 1 语言学家。 | Aluísio 和 Gasperin ([2010](#bib.bib5))
    |'
- en: '| LS–2012* | SG, SS, SR | EN | 201 | 5 | Mix | L1 English Speakers. | Specia
    et al. ([2012](#bib.bib52)) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| LS–2012* | SG, SS, SR | EN | 201 | 5 | 混合 | L1 英语说话者。 | Specia 等人 ([2012](#bib.bib52))
    |'
- en: '| CW Corpus | SS | EN | 731 | 0 | Wikipedia | Wikipedia Edits. | Shardlow ([2013](#bib.bib48))
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| CW Corpus | SS | EN | 731 | 0 | Wikipedia | Wikipedia 编辑。 | Shardlow ([2013](#bib.bib48))
    |'
- en: '| LexMTurk | SG, SS, SR | EN | 500 | 50 | Wikipedia | 50 US-based. | Horn et al.
    ([2014](#bib.bib20)) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| LexMTurk | SG, SS, SR | EN | 500 | 50 | Wikipedia | 50 美国基础。 | Horn 等人 ([2014](#bib.bib20))
    |'
- en: '| JLS | SG, SS, SR | JP | 243 | 5 | Mix | 5 L1 JP Speakers. | Kajiwara and
    Yamamoto ([2015](#bib.bib21)) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| JLS | SG, SS, SR | JP | 243 | 5 | 混合 | 5 L1 JP 说话者。 | Kajiwara 和 Yamamoto
    ([2015](#bib.bib21)) |'
- en: '| JLS Balanced | SG, SS, SR | JP | 2,010 | 5 | Mix | L1 JP Speakers | Kodaira
    et al. ([2016](#bib.bib22)) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| JLS Balanced | SG, SS, SR | JP | 2,010 | 5 | 混合 | L1 JP 说话者 | Kodaira 等人
    ([2016](#bib.bib22)) |'
- en: '| CWI–2016* | SS | EN | 90,458 | 0 | News | 400 L2 EN Speakers. | Paetzold
    and Specia ([2016a](#bib.bib36)) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| CWI–2016* | SS | EN | 90,458 | 0 | 新闻 | 400 L2 EN 说话者。 | Paetzold 和 Specia
    ([2016a](#bib.bib36)) |'
- en: '| BenchLS | SG, SS, SR | EN | 929 | 7 | Mix | US-Based. | Paetzold and Specia
    ([2016b](#bib.bib39)) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| BenchLS | SG, SS, SR | EN | 929 | 7 | 混合 | 美国基础。 | Paetzold 和 Specia ([2016b](#bib.bib39))
    |'
- en: '| NNSeval | SG, SS, SR | EN | 239 | 7 | Mix | 400 L2 EN Speakers. | Paetzold
    and Specia ([2016c](#bib.bib40)) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| NNSeval | SG, SS, SR | EN | 239 | 7 | 混合 | 400 L2 EN 说话者。 | Paetzold 和 Specia
    ([2016c](#bib.bib40)) |'
- en: '| Post-2017 | CERF-LS | SG, SS, SR | EN | 406 | 12 | Academic | 1 L1 EN Speaker.
    | Uchida et al. ([2018](#bib.bib54)) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Post-2017 | CERF-LS | SG, SS, SR | EN | 406 | 12 | 学术 | 1 L1 EN 说话者。 | Uchida
    等人 ([2018](#bib.bib54)) |'
- en: '| Personalized-ZH | SG, SS, SR | ZH | 600 | 7 | Mix | 8 L1 ZH Speakers | Lee
    and Yeung ([2018a](#bib.bib24)) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Personalized-ZH | SG, SS, SR | ZH | 600 | 7 | 混合 | 8 L1 ZH 说话者 | Lee 和 Yeung
    ([2018a](#bib.bib24)) |'
- en: '| WCL | SS, SR | EN | 15,000 | 0 | Mix | 11 L2 EN Speakers. | Maddela and Xu
    ([2018](#bib.bib28)) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| WCL | SS, SR | EN | 15,000 | 0 | 混合 | 11 L2 EN 说话者。 | Maddela 和 Xu ([2018](#bib.bib28))
    |'
- en: '| ReSyf | SG, SS | FR | 57,589 | 3 | Mix | L1 FR Speakers. | Billami et al.
    ([2018](#bib.bib8)) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ReSyf | SG, SS | FR | 57,589 | 3 | 混合 | L1 FR 说话者。 | Billami 等人 ([2018](#bib.bib8))
    |'
- en: '| Personalized-LS | SG, SS, SR | EN | 929 | 7 | Mix | 15 L2 EN Speakers. |
    Lee and Yeung ([2018b](#bib.bib25)) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Personalized-LS | SG, SS, SR | EN | 929 | 7 | 混合 | 15 L2 EN 说话者。 | Lee 和
    Yeung ([2018b](#bib.bib25)) |'
- en: '| CWI–2018* | SS, SR | EN, FR, GR, ES | 62,550 | 0 | News | L1&L2 EN Speakers.
    | Yimam et al. ([2018](#bib.bib60)) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| CWI–2018* | SS, SR | EN, FR, GR, ES | 62,550 | 0 | 新闻 | L1&L2 EN 说话者。 | Yimam
    等人 ([2018](#bib.bib60)) |'
- en: '| PorSimplesSent | SG, SS | PT | 6109 | 1 | News | 3 Linguists. | Leal et al.
    ([2018](#bib.bib23)) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| PorSimplesSent | SG, SS | PT | 6109 | 1 | 新闻 | 3 语言学家。 | Leal 等人 ([2018](#bib.bib23))
    |'
- en: '| LCP-2021* | SS, SR | EN | 10,800 | 0 | Mix | 7 US/UK/AUS-based. | Shardlow
    et al. ([2020](#bib.bib49)) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| LCP-2021* | SS, SR | EN | 10,800 | 0 | 混合 | 7 美国/英国/澳大利亚基础。 | Shardlow 等人
    ([2020](#bib.bib49)) |'
- en: '| SIMPLEX-PB | SG, SS, SR | PT | 730 | 5 | Academic | pt-BR Speakers. | Hartmann
    and Aluísio ([2020](#bib.bib19)) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| SIMPLEX-PB | SG, SS, SR | PT | 730 | 5 | 学术 | pt-BR 说话者。 | Hartmann 和 Aluísio
    ([2020](#bib.bib19)) |'
- en: '| JWCL-JSSL | SG | JP | 18,000 | 0 | Mix | 5 L1 JP Speakers. | Nishihara and
    Kajiwara ([2020](#bib.bib33)) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| JWCL-JSSL | SG | JP | 18,000 | 0 | 综合 | 5名母语日语使用者。 | Nishihara 和 Kajiwara
    ([2020](#bib.bib33)) |'
- en: '| ALexS-2020* | SG | ES | 723 | 0 | Academic | 430 ES Speakers. | Zambrano
    and Ráez ([2020](#bib.bib61)) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| ALexS-2020* | SG | ES | 723 | 0 | 学术 | 430名西班牙语使用者。 | Zambrano 和 Ráez ([2020](#bib.bib61))
    |'
- en: '| SimpleText-2021* | SG, SS, SR | EN | 1000 | 10 | Academic | Participating
    Teams. | Ermakova et al. ([2021](#bib.bib16)) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| SimpleText-2021* | SG, SS, SR | EN | 1000 | 10 | 学术 | 参与团队。 | Ermakova 等
    ([2021](#bib.bib16)) |'
- en: '| ES-CWI | SG | ES | 3,887 | 0 | Academic | 40 L1 ES speakers. | Merejildo
    ([2021](#bib.bib31)) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| ES-CWI | SG | ES | 3,887 | 0 | 学术 | 40名母语西班牙语使用者。 | Merejildo ([2021](#bib.bib31))
    |'
- en: '| EASIER | SG, SS | ES | 5,310 | 3 | News | L1 ES speakers. | Alarcón et al.
    ([2021b](#bib.bib3)) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| EASIER | SG, SS | ES | 5,310 | 3 | 新闻 | 母语西班牙语使用者。 | Alarcón 等 ([2021b](#bib.bib3))
    |'
- en: '| FrenLys | SG, SS, SR | FR | 57,589 | 3 | Mix | 20 L1 FR Speakers. | Rolin
    et al. ([2021](#bib.bib45)) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| FrenLys | SG, SS, SR | FR | 57,589 | 3 | 综合 | 20名母语法语使用者。 | Rolin 等 ([2021](#bib.bib45))
    |'
- en: '| HanLS | SG, SS, SR | ZH | 534 | 8 | Mix | 5 L1 ZH Speakers. | Qiang et al.
    ([2021](#bib.bib44)) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| HanLS | SG, SS, SR | ZH | 534 | 8 | 综合 | 5名母语中文使用者。 | Qiang 等 ([2021](#bib.bib44))
    |'
- en: '| TSAR-2022* | SG, SS, SR | EN, ES, PT | 1153 | 20 | News | 21 UK/ES/BR-based.
    | Saggion et al. ([2022](#bib.bib46)) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| TSAR-2022* | SG, SS, SR | EN, ES, PT | 1153 | 20 | 新闻 | 21名来自英国/西班牙/巴西的使用者。
    | Saggion 等 ([2022](#bib.bib46)) |'
- en: 'Table 2: Datasets that can be used for LS arranged in chronological order.
    Marked datasets (*) were used in benchmark competitions. L1 and L2 refers to first
    and second language speakers.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：按时间顺序排列的可用于语言服务的 数据集。标记数据集 (*) 在基准测试中使用。L1 和 L2 指第一语言和第二语言的使用者。
