- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:48:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:48:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2201.09267] Spectral, Probabilistic, and Deep Metric Learning: Tutorial and
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2201.09267] 《光谱、概率与深度度量学习：教程与调研》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2201.09267](https://ar5iv.labs.arxiv.org/html/2201.09267)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2201.09267](https://ar5iv.labs.arxiv.org/html/2201.09267)
- en: 'Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《光谱、概率与深度度量学习：教程与调研》
- en: Benyamin Ghojogh Department of Electrical and Computer Engineering,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本雅明·戈霍吉 电气与计算机工程系，
- en: Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada   
    Ali Ghodsi Department of Statistics and Actuarial Science & David R. Cheriton
    School of Computer Science,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 滑铁卢大学机器学习实验室，加拿大安大略省滑铁卢市    阿里·戈赫西 统计与精算科学系 & 大卫·R·切里顿计算机科学学院，
- en: Data Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada    Fakhri
    Karray Department of Electrical and Computer Engineering,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 滑铁卢大学数据分析实验室，加拿大安大略省滑铁卢市    法赫里·卡雷 电子与计算机工程系，
- en: Centre for Pattern Analysis and Machine Intelligence, University of Waterloo,
    Waterloo, ON, Canada    Mark Crowley Department of Electrical and Computer Engineering,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 滑铁卢大学模式分析与机器智能中心，加拿大安大略省滑铁卢市    马克·克劳利 电气与计算机工程系，
- en: Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 滑铁卢大学机器学习实验室，加拿大安大略省滑铁卢市
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This is a tutorial and survey paper on metric learning. Algorithms are divided
    into spectral, probabilistic, and deep metric learning. We first start with the
    definition of distance metric, Mahalanobis distance, and generalized Mahalanobis
    distance. In spectral methods, we start with methods using scatters of data, including
    the first spectral metric learning, relevant methods to Fisher discriminant analysis,
    Relevant Component Analysis (RCA), Discriminant Component Analysis (DCA), and
    the Fisher-HSIC method. Then, large-margin metric learning, imbalanced metric
    learning, locally linear metric adaptation, and adversarial metric learning are
    covered. We also explain several kernel spectral methods for metric learning in
    the feature space. We also introduce geometric metric learning methods on the
    Riemannian manifolds. In probabilistic methods, we start with collapsing classes
    in both input and feature spaces and then explain the neighborhood component analysis
    methods, Bayesian metric learning, information theoretic methods, and empirical
    risk minimization in metric learning. In deep learning methods, we first introduce
    reconstruction autoencoders and supervised loss functions for metric learning.
    Then, Siamese networks and its various loss functions, triplet mining, and triplet
    sampling are explained. Deep discriminant analysis methods, based on Fisher discriminant
    analysis, are also reviewed. Finally, we introduce multi-modal deep metric learning,
    geometric metric learning by neural networks, and few-shot metric learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇关于度量学习的教程和调研论文。算法分为光谱、概率和深度度量学习。我们首先介绍距离度量、马哈拉诺比斯距离以及广义马哈拉诺比斯距离。在光谱方法中，我们从使用数据散布的方法开始，包括第一个光谱度量学习、与Fisher判别分析相关的方法、相关成分分析（RCA）、判别成分分析（DCA）和Fisher-HSIC方法。然后，我们讨论大间隔度量学习、不平衡度量学习、局部线性度量适应和对抗性度量学习。我们还解释了特征空间中的几种核光谱方法。我们还介绍了在黎曼流形上的几何度量学习方法。在概率方法中，我们从输入和特征空间中的类别收缩开始，然后解释邻域成分分析方法、贝叶斯度量学习、信息论方法以及度量学习中的经验风险最小化。在深度学习方法中，我们首先介绍了重建自编码器和用于度量学习的监督损失函数。然后，解释了Siamese网络及其各种损失函数、三元组挖掘和三元组采样。基于Fisher判别分析的深度判别分析方法也被回顾。最后，我们介绍了多模态深度度量学习、通过神经网络进行的几何度量学习以及少样本度量学习。
- en: Tutorial\AddToShipoutPictureBG
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Tutorial\AddToShipoutPictureBG
- en: '*\AtPageUpperLeft                                                                                             
    <svg version="1.1" width="595.68" height="12.3" overflow="visible"><g transform="translate(0,12.3)
    scale(1,-1)"><g  transform="translate(-297.84,-43.65)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black">To appear as a part of an upcoming textbook on dimensionality
    reduction and manifold learning.</text></g></g></svg>'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*\AtPageUpperLeft                                                                                             
    <svg version="1.1" width="595.68" height="12.3" overflow="visible"><g transform="translate(0,12.3)
    scale(1,-1)"><g  transform="translate(-297.84,-43.65)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black">将作为即将出版的关于降维和流形学习的教科书的一部分出现。</text></g></g></svg>'
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Dimensionality reduction and manifold learning are used for feature extraction
    from raw data. A family of dimensionality reduction methods is metric learning
    which learns a distance metric or an embedding space for separation of dissimilar
    points and closeness of similar points. In supervised metric learning, we aim
    to discriminate classes by learning an appropriate metric. Dimensionality reduction
    methods can be divided into spectral, probabilistic, and deep methods (Ghojogh,
    [2021](#bib.bib36)). Spectral methods have a geometrical approach and usually
    are reduced to generalized eigenvalue problems (Ghojogh et al., [2019a](#bib.bib38)).
    Probabilistic methods are based on probability distributions. Deep methods use
    neural network for learning. In each of these categories, there exist several
    metric learning methods. In this paper, we review and introduce the most important
    metric learning algorithms in these categories. Note that there exist some other
    surveys on metric learning such as (Yang & Jin, [2006](#bib.bib132); Yang, [2007](#bib.bib131);
    Kulis, [2013](#bib.bib76); Bellet et al., [2013](#bib.bib10); Wang & Sun, [2015](#bib.bib116);
    Suárez et al., [2021](#bib.bib107)). A survey specific to deep metric learning
    is (Kaya & Bilge, [2019](#bib.bib72)). A book on metric learning is (Bellet et al.,
    [2015](#bib.bib11)). Finally, some Python toolboxes for metric learning are (Suárez
    et al., [2020](#bib.bib106); De Vazelhes et al., [2020](#bib.bib24); Musgrave
    et al., [2020](#bib.bib90)). The remainder of this paper is organized as follows.
    Section [2](#S2 "2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey") defines distance metric and the
    generalized Mahalanobis distance. Sections [3](#S3 "3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"), [4](#S4
    "4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"), and [5](#S5 "5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey") introduce and discuss spectral,
    probabilistic, and deep metric learning methods, respectively. Finally, section
    [6](#S6 "6 Conclusion ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") concludes the paper. The table of contents can be found at the end
    of paper.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '降维和流形学习用于从原始数据中提取特征。降维方法的一种是度量学习，它学习一个距离度量或嵌入空间，以区分不相似的点和接近相似的点。在监督式度量学习中，我们的目标是通过学习适当的度量来区分类别。降维方法可以分为谱方法、概率方法和深度方法（Ghojogh，[2021](#bib.bib36)）。谱方法具有几何方法，通常简化为广义特征值问题（Ghojogh
    等，[2019a](#bib.bib38)）。概率方法基于概率分布。深度方法使用神经网络进行学习。在这些类别中存在多种度量学习方法。本文回顾并介绍了这些类别中最重要的度量学习算法。需要注意的是，关于度量学习还有一些其他的调查，如（Yang
    & Jin，[2006](#bib.bib132)；Yang，[2007](#bib.bib131)；Kulis，[2013](#bib.bib76)；Bellet
    等，[2013](#bib.bib10)；Wang & Sun，[2015](#bib.bib116)；Suárez 等，[2021](#bib.bib107)）。特定于深度度量学习的调查是（Kaya
    & Bilge，[2019](#bib.bib72)）。有关度量学习的书籍是（Bellet 等，[2015](#bib.bib11)）。最后，一些度量学习的
    Python 工具箱有（Suárez 等，[2020](#bib.bib106)；De Vazelhes 等，[2020](#bib.bib24)；Musgrave
    等，[2020](#bib.bib90)）。本文其余部分组织如下。第 [2](#S2 "2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")
    节定义了距离度量和广义马哈拉诺比斯距离。第 [3](#S3 "3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")、[4](#S4 "4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")
    和 [5](#S5 "5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey") 节分别介绍和讨论谱方法、概率方法和深度方法的度量学习方法。最后，第 [6](#S6 "6 Conclusion
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey") 节总结了本文。目录可在论文末尾找到。'
- en: Required Background for the Reader
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读者所需背景
- en: This paper assumes that the reader has general knowledge of calculus, probability,
    linear algebra, and basics of optimization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文假设读者具备微积分、概率论、线性代数和优化基础知识。
- en: 2 Generalized Mahalanobis Distance Metric
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 广义马哈拉诺比斯距离度量
- en: 2.1 Distance Metric
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 距离度量
- en: Definition 1  (Distance metric).
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1  （距离度量）。
- en: 'Consider a metric space $\mathcal{X}$. A distance metric is a mapping $d:\mathcal{X}\times\mathcal{X}\rightarrow[0,\infty)$
    which satisfies the following properties:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 设考虑一个度量空间 $\mathcal{X}$。距离度量是一个映射 $d:\mathcal{X}\times\mathcal{X}\rightarrow[0,\infty)$，它满足以下属性：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'non-negativity: $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\geq 0$'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非负性：$d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\geq 0$
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'identity: $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=0\iff\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 恒等性：$d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=0\iff\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'symmetry: $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=d(\boldsymbol{x}_{j},\boldsymbol{x}_{i})$'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对称性：$d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=d(\boldsymbol{x}_{j},\boldsymbol{x}_{i})$
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'triangle inequality: $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\leq d(\boldsymbol{x}_{i},\boldsymbol{x}_{k})+d(\boldsymbol{x}_{k},\boldsymbol{x}_{j})$'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 三角不等式：$d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\leq d(\boldsymbol{x}_{i},\boldsymbol{x}_{k})+d(\boldsymbol{x}_{k},\boldsymbol{x}_{j})$
- en: where $\boldsymbol{x}_{i},\boldsymbol{x}_{j},\boldsymbol{x}_{k}\in\mathcal{X}$.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{x}_{i},\boldsymbol{x}_{j},\boldsymbol{x}_{k}\in\mathcal{X}$。
- en: 'An example of distance metric is the Euclidean distance:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个距离度量的例子是欧几里得距离：
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{2}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}.$
    |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{2}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}.$
    |  | (1) |'
- en: 2.2 Mahalanobis Distance
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 马氏距离
- en: The Mahalanobis distance is another distance metric which was originally proposed
    in (Mahalanobis, [1930](#bib.bib82)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 马氏距离是另一种距离度量，最初在（Mahalanobis, [1930](#bib.bib82)）中提出。
- en: Definition 2  (Mahalanobis distance (Mahalanobis, [1930](#bib.bib82))).
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2  （马氏距离（Mahalanobis, [1930](#bib.bib82)））。
- en: 'Consider a $d$-dimensional metric space $\mathcal{X}$. Let two clouds or sets
    of points $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$ be in the data, i.e., $\mathcal{X}_{1},\mathcal{X}_{2}\in\mathcal{X}$.
    A point is considered in each set, i.e., $\boldsymbol{x}_{i}\in\mathcal{X}_{1}$
    and $\boldsymbol{x}_{j}\in\mathcal{X}_{2}$. The Mahalanobis distance between the
    two points is:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 $d$-维度的度量空间 $\mathcal{X}$。假设数据中有两个点云或集合 $\mathcal{X}_{1}$ 和 $\mathcal{X}_{2}$，即
    $\mathcal{X}_{1},\mathcal{X}_{2}\in\mathcal{X}$。每个集合中有一个点，即 $\boldsymbol{x}_{i}\in\mathcal{X}_{1}$
    和 $\boldsymbol{x}_{j}\in\mathcal{X}_{2}$。这两点之间的马氏距离为：
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})},$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})},$
    |  | (2) |'
- en: where $\boldsymbol{\Sigma}\in\mathbb{R}^{d\times d}$ is the covariance matrix
    of data in the two sets $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\Sigma}\in\mathbb{R}^{d\times d}$ 是数据集合 $\mathcal{X}_{1}$ 和
    $\mathcal{X}_{2}$ 的协方差矩阵。
- en: 'If the points $\boldsymbol{x}_{i}$ and $\boldsymbol{x}_{j}$ are the means of
    the sets $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$, respectively, as the representatives
    of the sets, this Mahalanobis distance is a good measure of distance of the sets
    (McLachlan, [1999](#bib.bib84)):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果点 $\boldsymbol{x}_{i}$ 和 $\boldsymbol{x}_{j}$ 分别是集合 $\mathcal{X}_{1}$ 和 $\mathcal{X}_{2}$
    的均值，作为这些集合的代表，那么这个马氏距离是度量这些集合距离的一个很好的方法（McLachlan, [1999](#bib.bib84)）：
- en: '|  | $\displaystyle\&#124;\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2})},$
    |  | (3) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2})},$
    |  | (3) |'
- en: where $\boldsymbol{\mu}_{1}$ and $\boldsymbol{\mu}_{2}$ are the means of the
    sets $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$, respectively.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\mu}_{1}$ 和 $\boldsymbol{\mu}_{2}$ 分别是集合 $\mathcal{X}_{1}$ 和
    $\mathcal{X}_{2}$ 的均值。
- en: 'Let $\mathcal{X}_{1}:=\{\boldsymbol{x}_{1,i}\}_{i=1}^{n_{1}}$ and $\mathcal{X}_{2}:=\{\boldsymbol{x}_{2,i}\}_{i=1}^{n_{2}}$.
    The unbiased sample covariance matrices of these two sets are:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $\mathcal{X}_{1}:=\{\boldsymbol{x}_{1,i}\}_{i=1}^{n_{1}}$ 和 $\mathcal{X}_{2}:=\{\boldsymbol{x}_{2,i}\}_{i=1}^{n_{2}}$。这两个集合的无偏样本协方差矩阵为：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{1}:=\frac{1}{n_{1}-1}\sum_{i=1}^{n_{1}}(\boldsymbol{x}_{1,i}-\boldsymbol{\mu}_{1})(\boldsymbol{x}_{1,i}-\boldsymbol{\mu}_{1})^{\top},$
    |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}_{1}:=\frac{1}{n_{1}-1}\sum_{i=1}^{n_{1}}(\boldsymbol{x}_{1,i}-\boldsymbol{\mu}_{1})(\boldsymbol{x}_{1,i}-\boldsymbol{\mu}_{1})^{\top},$
    |  |'
- en: 'and $\boldsymbol{\Sigma}_{2}$ similarly. The covariance matrix $\boldsymbol{\Sigma}$
    can be an unbiased sample covariance matrix (McLachlan, [1999](#bib.bib84)):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以及 $\boldsymbol{\Sigma}_{2}$ 类似地。协方差矩阵 $\boldsymbol{\Sigma}$ 可以是一个无偏样本协方差矩阵（McLachlan,
    [1999](#bib.bib84)）：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}:=\frac{1}{n_{1}+n_{2}-2}\Big{(}(n_{1}-1)\boldsymbol{\Sigma}_{1}+(n_{2}-1)\boldsymbol{\Sigma}_{2}\Big{)}.$
    |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}:=\frac{1}{n_{1}+n_{2}-2}\Big{(}(n_{1}-1)\boldsymbol{\Sigma}_{1}+(n_{2}-1)\boldsymbol{\Sigma}_{2}\Big{)}.$
    |  |'
- en: 'The Mahalanobis distance can also be defined between a point $\boldsymbol{x}$
    and a cloud or set of points $\mathcal{X}$ (De Maesschalck et al., [2000](#bib.bib23)).
    Let $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ be the mean and the (sample)
    covariance matrix of the set $\mathcal{X}$. The Mahalanobis distance of $\boldsymbol{x}$
    and $\mathcal{X}$ is:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 马哈拉诺比斯距离也可以定义为点 $\boldsymbol{x}$ 与一组点 $\mathcal{X}$ 之间的距离（De Maesschalck 等，[2000](#bib.bib23)）。设
    $\boldsymbol{\mu}$ 和 $\boldsymbol{\Sigma}$ 为集合 $\mathcal{X}$ 的均值和（样本）协方差矩阵。点 $\boldsymbol{x}$
    与 $\mathcal{X}$ 的马哈拉诺比斯距离为：
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}-\boldsymbol{\mu}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}.$
    |  | (4) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}-\boldsymbol{\mu}\&#124;_{\boldsymbol{\Sigma}}:=\sqrt{(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}.$
    |  | (4) |'
- en: '![Refer to caption](img/250b5f41ffd51a70b54877640e6751d0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/250b5f41ffd51a70b54877640e6751d0.png)'
- en: 'Figure 1: An example for comparison of the Euclidean and Mahalanobis distances.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：欧几里得距离和马哈拉诺比斯距离的比较示例。
- en: Remark 1  (Justification of the Mahalanobis distance (De Maesschalck et al.,
    [2000](#bib.bib23))).
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 1（马哈拉诺比斯距离的合理性（De Maesschalck 等，[2000](#bib.bib23)））。
- en: 'Consider two clouds of data, $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$, depicted
    in Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"). We want to compute the distance of a point $\boldsymbol{x}$ from
    these two data clouds to see which cloud this point is closer to. The Euclidean
    distance ignores the scatter/variance of clouds and only measures the distances
    of the point from the means of clouds. Hence, in this example, it says that $\boldsymbol{x}$
    belongs to $\mathcal{X}_{1}$ because it is closer to the mean of $\mathcal{X}_{1}$
    compared to $\mathcal{X}_{2}$. However, the Mahalanobis distance takes the variance
    of clouds into account and says that $\boldsymbol{x}$ belongs to $\mathcal{X}_{2}$
    because it is closer to its scatter compared to $\mathcal{X}_{1}$. Visually, human
    also says $\boldsymbol{x}$ belongs to $\mathcal{X}_{2}$; hence, the Mahalanobis
    distance has performed better than the Euclidean distance by considering the variances
    of data.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图 [1](#S2.F1 "图1 ‣ 2.2 马哈拉诺比斯距离 ‣ 2 广义马哈拉诺比斯距离度量 ‣ 光谱、概率和深度度量学习：教程和调查") 中描绘的两组数据
    $\mathcal{X}_{1}$ 和 $\mathcal{X}_{2}$。我们想计算点 $\boldsymbol{x}$ 与这两组数据的距离，以查看这个点更接近哪一组数据。欧几里得距离忽略了数据云的散布/方差，仅测量点与数据云均值之间的距离。因此，在这个示例中，它表示
    $\boldsymbol{x}$ 属于 $\mathcal{X}_{1}$，因为它比 $\mathcal{X}_{2}$ 更接近 $\mathcal{X}_{1}$
    的均值。然而，马哈拉诺比斯距离考虑了数据云的方差，并表示 $\boldsymbol{x}$ 属于 $\mathcal{X}_{2}$，因为它比 $\mathcal{X}_{1}$
    更接近其散布。从视觉上看，人们也会认为 $\boldsymbol{x}$ 属于 $\mathcal{X}_{2}$；因此，马哈拉诺比斯距离通过考虑数据的方差，比欧几里得距离表现得更好。
- en: 2.3 Generalized Mahalanobis Distance
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 广义马哈拉诺比斯距离
- en: Definition 3  (Generalized Mahalanobis distance).
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3（广义马哈拉诺比斯距离）。
- en: 'In Mahalanobis distance, i.e. Eq. ([2](#S2.E2 "In Definition 2 (Mahalanobis
    distance (Mahalanobis, 1930)). ‣ 2.2 Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), the covariance matrix $\boldsymbol{\Sigma}$ and its inverse $\boldsymbol{\Sigma}^{-1}$
    are positive semi-definite. We can replace $\boldsymbol{\Sigma}^{-1}$ with a positive
    semi-definite weight matrix $\boldsymbol{W}\succeq\boldsymbol{0}$ in the squared
    Mahalanobis distance. We name this distance a generalized Mahalanobis distance:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在马哈拉诺比斯距离中，即公式 ([2](#S2.E2 "在定义2（马哈拉诺比斯距离（马哈拉诺比斯，1930年））。 ‣ 2.2 马哈拉诺比斯距离 ‣ 2
    广义马哈拉诺比斯距离度量 ‣ 光谱、概率和深度度量学习：教程和调查"))，协方差矩阵 $\boldsymbol{\Sigma}$ 及其逆矩阵 $\boldsymbol{\Sigma}^{-1}$
    是正半定的。我们可以在平方马哈拉诺比斯距离中用一个正半定的权重矩阵 $\boldsymbol{W}\succeq\boldsymbol{0}$ 替代 $\boldsymbol{\Sigma}^{-1}$。我们将这种距离称为广义马哈拉诺比斯距离：
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}.$
    |  | (5) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}:=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}.$
    |  | (5) |'
- en: '|  |  | $\displaystyle\therefore\,\,\,\,\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}:=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}).$
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\therefore\,\,\,\,\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}:=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}).$
    |  |'
- en: 'We define the generalized Mahalanobis norm as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将广义马氏范数定义为：
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}\&#124;_{\boldsymbol{W}}:=\sqrt{\boldsymbol{x}^{\top}\boldsymbol{W}\boldsymbol{x}}.$
    |  | (6) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}\&#124;_{\boldsymbol{W}}:=\sqrt{\boldsymbol{x}^{\top}\boldsymbol{W}\boldsymbol{x}}.$
    |  | (6) |'
- en: Lemma 1  (Triangle inequality of norm).
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1 （范数的三角不等式）。
- en: 'Let $\|.\|$ be a norm. Using the Cauchy-Schwarz inequality, it satisfies the
    triangle inequality:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\|.\|$ 为一个范数。利用柯西-施瓦茨不等式，它满足三角不等式：
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}+\boldsymbol{x}_{j}\&#124;\leq\&#124;\boldsymbol{x}_{i}\&#124;+\&#124;\boldsymbol{x}_{j}\&#124;.$
    |  | (7) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}+\boldsymbol{x}_{j}\&#124;\leq\&#124;\boldsymbol{x}_{i}\&#124;+\&#124;\boldsymbol{x}_{j}\&#124;.$
    |  | (7) |'
- en: Proof.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}+\boldsymbol{x}_{j}\&#124;^{2}$
    | $\displaystyle=(\boldsymbol{x}_{i}+\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{i}+\boldsymbol{x}_{j})$
    |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}+\boldsymbol{x}_{j}\&#124;^{2}$
    | $\displaystyle=(\boldsymbol{x}_{i}+\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{i}+\boldsymbol{x}_{j})$
    |  |'
- en: '|  |  | $\displaystyle=\&#124;\boldsymbol{x}_{i}\&#124;^{2}+\&#124;\boldsymbol{x}_{j}\&#124;^{2}+2\boldsymbol{x}_{i}^{\top}\boldsymbol{x}_{j}$
    |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\&#124;\boldsymbol{x}_{i}\&#124;^{2}+\&#124;\boldsymbol{x}_{j}\&#124;^{2}+2\boldsymbol{x}_{i}^{\top}\boldsymbol{x}_{j}$
    |  |'
- en: '|  |  | $\displaystyle\overset{(a)}{\leq}\&#124;\boldsymbol{x}_{i}\&#124;^{2}+\&#124;\boldsymbol{x}_{j}\&#124;^{2}+2\&#124;\boldsymbol{x}_{i}\&#124;\&#124;\boldsymbol{x}_{j}\&#124;$
    |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(a)}{\leq}\&#124;\boldsymbol{x}_{i}\&#124;^{2}+\&#124;\boldsymbol{x}_{j}\&#124;^{2}+2\&#124;\boldsymbol{x}_{i}\&#124;\&#124;\boldsymbol{x}_{j}\&#124;$
    |  |'
- en: '|  |  | $\displaystyle=(\&#124;\boldsymbol{x}_{i}\&#124;+\&#124;\boldsymbol{x}_{j}\&#124;)^{2},$
    |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(\&#124;\boldsymbol{x}_{i}\&#124;+\&#124;\boldsymbol{x}_{j}\&#124;)^{2},$
    |  |'
- en: 'where $(a)$ is because of the Cauchy-Schwarz inequality, i.e., $\boldsymbol{x}_{i}^{\top}\boldsymbol{x}_{j}\leq\|\boldsymbol{x}_{i}\|\|\boldsymbol{x}_{j}\|$.
    Taking second root from the sides gives Eq. ([7](#S2.E7 "In Lemma 1 (Triangle
    inequality of norm). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). Q.E.D. ∎'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于柯西-施瓦茨不等式，即 $\boldsymbol{x}_{i}^{\top}\boldsymbol{x}_{j}\leq\|\boldsymbol{x}_{i}\|\|\boldsymbol{x}_{j}\|$。对两边取平方根得
    Eq. ([7](#S2.E7 "在引理 1（范数的三角不等式）中。 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 光谱、概率和深度度量学习：教程和调查"))。证毕。∎
- en: Proposition 1.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 1。
- en: The generalized Mahalanobis distance is a valid distance metric.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 广义马氏距离是一个有效的距离度量。
- en: Proof.
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We show that the characteristics in Definition [1](#Thmdefinition1 "Definition
    1 (Distance metric). ‣ 2.1 Distance Metric ‣ 2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")
    are satisfied:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了定义 [1](#Thmdefinition1 "定义 1（距离度量）。 ‣ 2.1 距离度量 ‣ 2 广义马氏距离度量 ‣ 光谱、概率和深度度量学习：教程和调查")
    中的特性被满足：
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'As $\boldsymbol{W}\succeq\boldsymbol{0}$, Eq. ([5](#S2.E5 "In Definition 3
    (Generalized Mahalanobis distance). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) is non-negative.'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于 $\boldsymbol{W}\succeq\boldsymbol{0}$，Eq. ([5](#S2.E5 "在定义 3（广义马氏距离）中。 ‣
    2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 光谱、概率和深度度量学习：教程和调查")) 是非负的。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'identity: if $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=0$,
    according to Eq. ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis distance).
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we
    have $\boldsymbol{x}_{i}-\boldsymbol{x}_{j}=0\implies\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$.
    If $\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$, we have $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=0$
    according to Eq. ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis distance).
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 恒等性：如果 $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=0$，根据 Eq.
    ([5](#S2.E5 "在定义 3（广义马氏距离）中。 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 光谱、概率和深度度量学习：教程和调查")),
    我们有 $\boldsymbol{x}_{i}-\boldsymbol{x}_{j}=0\implies\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$。如果
    $\boldsymbol{x}_{i}=\boldsymbol{x}_{j}$，我们根据 Eq. ([5](#S2.E5 "在定义 3（广义马氏距离）中。
    ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 光谱、概率和深度度量学习：教程和调查")) 也有 $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=0$。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'symmetry:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对称性：
- en: $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}=\sqrt{(\boldsymbol{x}_{j}-\boldsymbol{x}_{i})^{\top}\boldsymbol{W}(\boldsymbol{x}_{j}-\boldsymbol{x}_{i})}=\|\boldsymbol{x}_{j}-\boldsymbol{x}_{i}\|_{\boldsymbol{W}}$.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=\sqrt{(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})}=\sqrt{(\boldsymbol{x}_{j}-\boldsymbol{x}_{i})^{\top}\boldsymbol{W}(\boldsymbol{x}_{j}-\boldsymbol{x}_{i})}=\|\boldsymbol{x}_{j}-\boldsymbol{x}_{i}\|_{\boldsymbol{W}}$。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'triangle inequality: $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}+\boldsymbol{x}_{k}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}\overset{(\ref{equation_xi_xj_triangle_inequality})}{\leq}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\|_{\boldsymbol{W}}+\|\boldsymbol{x}_{k}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}$.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 三角不等式：$\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}=\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}+\boldsymbol{x}_{k}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}\overset{(\ref{equation_xi_xj_triangle_inequality})}{\leq}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\|_{\boldsymbol{W}}+\|\boldsymbol{x}_{k}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}$。
- en: ∎
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Remark 2.
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 2。
- en: It is noteworthy that $\boldsymbol{W}\succeq\boldsymbol{0}$ is required so that
    the generalized Mahalanobis distance is convex and satisfies the triangle inequality.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，要求 $\boldsymbol{W}\succeq\boldsymbol{0}$ 以使广义马氏距离是凸的并满足三角不等式。
- en: Remark 3.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 3。
- en: 'The weight matrix $\boldsymbol{W}$ in Eq. ([5](#S2.E5 "In Definition 3 (Generalized
    Mahalanobis distance). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) weights the dimensions and determines some correlation
    between dimensions of data points. In other words, it changes the space in a way
    that the scatters of clouds are considered.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '公式 ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis distance). ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) 中的权重矩阵 $\boldsymbol{W}$ 权衡了维度，并确定了数据点维度之间的某些相关性。换句话说，它以一种考虑云散布的方式改变空间。'
- en: Remark 4.
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 4。
- en: 'The Euclidean distance is a special case of the Mahalanobis distance where
    the weight matrix is the identity matrix, i.e., $\boldsymbol{W}=\boldsymbol{I}$
    (cf. Eqs. ([1](#S2.E1 "In 2.1 Distance Metric ‣ 2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis distance). ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))). In other words, the Euclidean
    distance does not change the space for computing the distance.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '欧几里得距离是马氏距离的一个特例，其中权重矩阵是单位矩阵，即 $\boldsymbol{W}=\boldsymbol{I}$（参见公式 ([1](#S2.E1
    "In 2.1 Distance Metric ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 和 ([5](#S2.E5
    "In Definition 3 (Generalized Mahalanobis distance). ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))）。换句话说，欧几里得距离不会改变计算距离的空间。'
- en: Proposition 2  (Projection in metric learning).
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 2（度量学习中的投影）。
- en: 'Consider the eigenvalue decomposition of the weight matrix $\boldsymbol{W}$
    in the generalized Mahalanobis distance with $\boldsymbol{V}$ and $\boldsymbol{\Lambda}$
    as the matrix of eigenvectors and the diagonal matrix of eigenvalues of the weight,
    respectively. Let $\boldsymbol{U}:=\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}$.
    The generalized Mahalanobis distance can be seen as the Euclidean distance after
    applying a linear projection onto the column space of $\boldsymbol{U}$:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑广义马氏距离中权重矩阵 $\boldsymbol{W}$ 的特征值分解，其中 $\boldsymbol{V}$ 和 $\boldsymbol{\Lambda}$
    分别是特征向量矩阵和权重的对角特征值矩阵。令 $\boldsymbol{U}:=\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}$。广义马氏距离可以看作是应用线性投影到
    $\boldsymbol{U}$ 的列空间后的欧几里得距离：
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    | $\displaystyle=(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})$
    |  | (8) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$
    | $\displaystyle=(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})$
    |  | (8) |'
- en: '|  |  | $\displaystyle=\&#124;\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\&#124;_{2}^{2}.$
    |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\|\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\|_{2}^{2}.$
    |  |'
- en: If $\boldsymbol{U}\in\mathbb{R}^{d\times p}$ with $p\leq d$, the column space
    of the projection matrix $\boldsymbol{U}$ is a $p$-dimensional subspace.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $\boldsymbol{U}\in\mathbb{R}^{d\times p}$ 且 $p\leq d$，则投影矩阵 $\boldsymbol{U}$
    的列空间是一个 $p$ 维子空间。
- en: Proof.
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'By the eigenvalue decomposition of $\boldsymbol{W}$, we have:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对 $\boldsymbol{W}$ 进行特征值分解，我们得到：
- en: '|  | $\displaystyle\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\overset{(a)}{=}\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}\boldsymbol{\Lambda}^{(1/2)}\boldsymbol{V}^{\top}\overset{(b)}{=}\boldsymbol{U}\boldsymbol{U}^{\top},$
    |  | (9) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\overset{(a)}{=}\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}\boldsymbol{\Lambda}^{(1/2)}\boldsymbol{V}^{\top}\overset{(b)}{=}\boldsymbol{U}\boldsymbol{U}^{\top},$
    |  | (9) |'
- en: 'where $(a)$ is because $\boldsymbol{W}$ is positive semi-definite so all its
    eigenvalues are non-negative and can be written as multiplication of its second
    roots. Also, $(b)$ is because we define $\boldsymbol{U}:=\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}$.
    Substituting Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) in Eq. ([5](#S2.E5 "In Definition 3 (Generalized
    Mahalanobis distance). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) gives:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是因为 $\boldsymbol{W}$ 是半正定的，因此所有特征值都是非负的，并且可以写成其平方根的乘积。$(b)$ 是因为我们定义
    $\boldsymbol{U}:=\boldsymbol{V}\boldsymbol{\Lambda}^{(1/2)}$。将方程 ([9](#S2.E9 "在证明中。
    ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 光谱、概率及深度度量学习：教程与调查")) 代入方程 ([5](#S2.E5 "在定义 3（广义马氏距离）。
    ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 光谱、概率及深度度量学习：教程与调查")) 得到：
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    | $\displaystyle=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    | $\displaystyle=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
- en: '|  |  | $\displaystyle=(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})$
    |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j})$
    |  |'
- en: '|  |  | $\displaystyle=\&#124;\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\&#124;_{2}^{2}.$
    |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\&#124;\boldsymbol{U}^{\top}\boldsymbol{x}_{i}-\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\&#124;_{2}^{2}.$
    |  |'
- en: 'Q.E.D. It is noteworthy that Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) can also be obtained using singular
    value decomposition rather than eigenvalue decomposition. In that case, the matrices
    of right and left singular vectors are equal because of symmetry of $\boldsymbol{W}$.
    ∎'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 证毕。值得注意的是，方程 ([9](#S2.E9 "在证明中。 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 光谱、概率及深度度量学习：教程与调查"))
    也可以通过奇异值分解而不是特征值分解得到。在这种情况下，由于 $\boldsymbol{W}$ 的对称性，右奇异向量和左奇异向量的矩阵是相等的。∎
- en: 2.4 The Main Idea of Metric Learning
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 度量学习的主要思想
- en: 'Consider a $d$-dimensional dataset $\{\boldsymbol{x}_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d}$
    of size $n$. Assume some data points are similar in some sense. For example, they
    have similar pattern or the same characteristics. Hence, we have a set of similar
    pair points, denotes by $\mathcal{S}$. In contrast, we can have dissimilar points
    which are different in pattern or characteristics. Let the set of dissimilar pair
    points be denoted by $\mathcal{D}$. In summary:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 $d$ 维的数据集 $\{\boldsymbol{x}_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d}$，大小为 $n$。假设某些数据点在某种意义上是相似的。例如，它们具有相似的模式或相同的特征。因此，我们有一组相似的点对，记作
    $\mathcal{S}$。相对地，我们也可以有不同的点对，这些点对在模式或特征上有所不同。让不同点对的集合记作 $\mathcal{D}$。总结如下：
- en: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\text{
    if }\boldsymbol{x}_{i}\text{ and }\boldsymbol{x}_{j}\text{ are similar},$ |  |
    (10) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\text{
    如果 }\boldsymbol{x}_{i}\text{ 和 }\boldsymbol{x}_{j}\text{ 是相似的},$ |  | (10) |'
- en: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}\text{
    if }\boldsymbol{x}_{i}\text{ and }\boldsymbol{x}_{j}\text{ are dissimilar}.$ |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}\text{
    如果 }\boldsymbol{x}_{i}\text{ 和 }\boldsymbol{x}_{j}\text{ 不相似}.$ |  |'
- en: 'The measure of similarity and dissimilarity can be belonging to the same or
    different classes, if class labels are available for dataset. In this case, we
    have:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性和不相似性的度量可以属于相同或不同的类别，如果数据集有类别标签的话。在这种情况下，我们有：
- en: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\text{
    if }\boldsymbol{x}_{i}\text{ and }\boldsymbol{x}_{j}\text{ are in the same class},$
    |  | (11) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\text{
    如果 }\boldsymbol{x}_{i}\text{ 和 }\boldsymbol{x}_{j}\text{ 属于同一类别},$ |  | (11) |'
- en: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}\text{
    if }\boldsymbol{x}_{i}\text{ and }\boldsymbol{x}_{j}\text{ are in different classes}.$
    |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}\text{
    如果 }\boldsymbol{x}_{i}\text{ 和 }\boldsymbol{x}_{j}\text{ 属于不同类别}.$ |  |'
- en: 'In metric learning, we learn the weight matrix so that the distances of similar
    points become smaller and the distances of dissimilar points become larger. In
    this way, the variance of similar and dissimilar points get smaller and larger,
    respectively. A 2D visualization of metric learning is depicted in Fig. [2](#S2.F2
    "Figure 2 ‣ 2.4 The Main Idea of Metric Learning ‣ 2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey").
    If the class labels are available, metric learning tries to make the intra-class
    and inter-class variances smaller and larger, respectively. This is the same idea
    as the idea of Fisher Discriminant Analysis (FDA) (Fisher, [1936](#bib.bib33);
    Ghojogh et al., [2019b](#bib.bib39)).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在度量学习中，我们学习权重矩阵，使得相似点之间的距离变小，而不相似点之间的距离变大。这样，相似点和不相似点的方差分别变小和变大。度量学习的二维可视化如图[2](#S2.F2
    "图 2 ‣ 2.4 度量学习的主要思想 ‣ 2 广义马氏距离度量 ‣ 谱、概率和深度度量学习：教程和综述")所示。如果有类别标签，度量学习试图使得类内和类间的方差分别变小和变大。这与Fisher判别分析（FDA）的思想相同（Fisher，[1936](#bib.bib33)；Ghojogh等，[2019b](#bib.bib39)）。
- en: '![Refer to caption](img/508b5d11ef0b47e4fe0b78b0344c5a49.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/508b5d11ef0b47e4fe0b78b0344c5a49.png)'
- en: 'Figure 2: Visualizing metric learning in 2D: (a) the contour of Euclidean distance
    which does not properly discriminate classes, and (b) the contour of Euclidean
    distance which is better in discrimination of classes.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在二维中可视化度量学习：(a) 不能很好区分类别的欧几里得距离轮廓，以及 (b) 更好区分类别的欧几里得距离轮廓。
- en: 3 Spectral Metric Learning
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 谱度量学习
- en: 3.1 Spectral Methods Using Scatters
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 使用散点的谱方法
- en: 3.1.1 The First Spectral Method
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 第一个谱方法
- en: 'The first metric learning method was proposed in (Xing et al., [2002](#bib.bib127)).
    In this method, we minimize the distances of the similar points by the weight
    matrix $\boldsymbol{W}$ where this matrix is positive semi-definite:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个度量学习方法是在(Xing等，[2002](#bib.bib127))中提出的。在此方法中，我们通过权重矩阵$\boldsymbol{W}$来最小化相似点的距离，其中该矩阵是半正定的：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受制于 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: 'However, the solution of this optimization problem is trivial, i.e., $\boldsymbol{W}=\boldsymbol{0}$.
    Hence, we add a constraint on the dissimilar points to have distances larger than
    some positive amount:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个优化问题的解是平凡的，即$\boldsymbol{W}=\boldsymbol{0}$。因此，我们对不相似点添加了一个约束，使得距离大于某个正的量：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (12) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (12) |'
- en: '|  |  | subject to |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}\geq\alpha,$
    |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受制于 |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}\geq\alpha,$
    |  |'
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: where $\alpha>0$ is some positive number such as $\alpha=1$.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha>0$ 是某个正数，例如 $\alpha=1$。
- en: Lemma 2  ((Xing et al., [2002](#bib.bib127))).
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 2  ((Xing 等人，[2002](#bib.bib127))).
- en: 'If the constraint in Eq. ([12](#S3.E12 "In 3.1.1 The First Spectral Method
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is squared, i.e.,
    $\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}\geq\alpha$,
    the solution of optimization will have rank $1$. Hence, we are using a non-squared
    constraint in the optimization problem.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '如果方程 ([12](#S3.E12 "In 3.1.1 The First Spectral Method ‣ 3.1 Spectral Methods
    Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) 中的约束被平方化，即 $\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}\geq\alpha$，则优化问题的解将具有秩
    $1$。因此，我们在优化问题中使用了非平方约束。'
- en: Proof.
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'If the constraint in Eq. ([12](#S3.E12 "In 3.1.1 The First Spectral Method
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is squared, the
    problem is equivalent to (see (Ghojogh et al., [2019b](#bib.bib39), Appendix B)
    for proof):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '如果方程 ([12](#S3.E12 "In 3.1.1 The First Spectral Method ‣ 3.1 Spectral Methods
    Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) 中的约束被平方化，则问题等价于（参见 (Ghojogh 等人，[2019b](#bib.bib39)，附录
    B) 的证明）：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}},$
    |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}},$
    |  |'
- en: 'which is a Rayleigh-Ritz quotient (Ghojogh et al., [2019a](#bib.bib38)). We
    can restate $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$ as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 Rayleigh-Ritz 商 (Ghojogh 等人，[2019a](#bib.bib38))。我们可以重新陈述 $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$
    为：
- en: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}),$
    |  | (13) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}),$
    |  | (13) |'
- en: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{D}}),$
    |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{D}}),$
    |  |'
- en: 'where $\textbf{tr}(.)$ denotes the trace of matrix and:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{tr}(.)$ 表示矩阵的迹，且：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{S}}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  | (14) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{S}}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  | (14) |'
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  |'
- en: 'Hence, we have:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有：
- en: '|  |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}=\frac{\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{D}})}{\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})}\overset{(\ref{equation_W_U_UT})}{=}\frac{\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}})}{\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}})}$
    |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}}=\frac{\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{D}})}{\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})}\overset{(\ref{equation_W_U_UT})}{=}\frac{\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}})}{\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}})}$
    |  |'
- en: '|  |  | $\displaystyle\overset{(a)}{=}\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{U})}=\frac{\sum_{i=1}^{d}\boldsymbol{u}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{u}}{\sum_{i=1}^{d}\boldsymbol{u}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{u}},$
    |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(a)}{=}\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{U})}=\frac{\sum_{i=1}^{d}\boldsymbol{u}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{u}}{\sum_{i=1}^{d}\boldsymbol{u}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{u}},$
    |  |'
- en: 'where $(a)$ is because of the cyclic property of trace and $(b)$ is because
    $\boldsymbol{U}=[\boldsymbol{u}_{1},\dots,\boldsymbol{u}_{d}]$. Maximizing this
    Rayleigh-Ritz quotient results in the following generalized eigenvalue problem
    (Ghojogh et al., [2019a](#bib.bib38)):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于迹的循环特性，$(b)$ 是因为 $\boldsymbol{U}=[\boldsymbol{u}_{1},\dots,\boldsymbol{u}_{d}]$。最大化这个
    Rayleigh-Ritz 商会得到以下广义特征值问题（Ghojogh 等，[2019a](#bib.bib38)）：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{u}_{1}=\lambda\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{u}_{1},$
    |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{u}_{1}=\lambda\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{u}_{1},$
    |  |'
- en: where $\boldsymbol{u}_{1}$ is the eigenvector with largest eigenvalue and the
    other eigenvectors $\boldsymbol{u}_{2},\dots,\boldsymbol{u}_{d}$ are zero vectors.
    Q.E.D. ∎
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{u}_{1}$ 是具有最大特征值的特征向量，其他特征向量 $\boldsymbol{u}_{2},\dots,\boldsymbol{u}_{d}$
    是零向量。证毕。 ∎
- en: 'The Eq. ([12](#S3.E12 "In 3.1.1 The First Spectral Method ‣ 3.1 Spectral Methods
    Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) can be restated as a maximization problem:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '方程 ([12](#S3.E12 "In 3.1.1 The First Spectral Method ‣ 3.1 Spectral Methods
    Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) 可以重新表述为一个最大化问题：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}$
    |  | (15) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最大化}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}$
    |  | (15) |'
- en: '|  |  | subject to |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq\alpha,$
    |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 满足 |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq\alpha,$
    |  |'
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: 'We can solve this problem using projected gradient method (Ghojogh et al.,
    [2021c](#bib.bib48)) where a step of gradient ascent is followed by projection
    onto the two constraint sets:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用投影梯度方法（Ghojogh 等，[2021c](#bib.bib48)）来解决这个问题，其中梯度上升步骤后跟随投影到两个约束集合上：
- en: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{W}+\eta\frac{\partial}{\partial\boldsymbol{W}}\Big{(}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}\Big{)},$
    |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{W}+\eta\frac{\partial}{\partial\boldsymbol{W}}\Big{(}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}\Big{)},$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{W}:=\arg\min_{\boldsymbol{Q}}\Big{(}\&#124;\boldsymbol{Q}-\boldsymbol{W}\&#124;_{F}^{2}\,\text{
    s.t.}$ |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}:=\arg\min_{\boldsymbol{Q}}\Big{(}\&#124;\boldsymbol{Q}-\boldsymbol{W}\&#124;_{F}^{2}\,\text{
    满足 }$ |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{Q}}^{2}\leq\alpha\Big{)},$
    |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}|_{\boldsymbol{Q}}^{2}\leq\alpha\Big{)},$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
- en: 'where $\eta>0$ is the learning rate and $\boldsymbol{V}$ and $\boldsymbol{\Lambda}=\textbf{diag}(\lambda_{1},\dots,\lambda_{d})$
    are the eigenvectors and eigenvalues of $\boldsymbol{W}$, respectively (see Eq.
    ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\eta>0$是学习率，$\boldsymbol{V}$和$\boldsymbol{\Lambda}=\textbf{diag}(\lambda_{1},\dots,\lambda_{d})$分别是$\boldsymbol{W}$的特征向量和特征值（见方程
    ([9](#S2.E9 "在证明中。‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱、概率和深度度量学习：教程和调查"))）。
- en: 3.1.2 Formulating as Semidefinite Programming
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 表述为半正定规划
- en: 'Another metric learning method is (Ghodsi et al., [2007](#bib.bib35)) which
    minimizes the distances of similar points and maximizes the distances of dissimilar
    points. For this, we minimize the distances of similar points and the negation
    of distances of dissimilar points. The weight matrix should be positive semi-definite
    to satisfy the triangle inequality and convexity. The trace of weight matrix is
    also set to a constant to eliminate the trivial solution $\boldsymbol{W}=\boldsymbol{0}$.
    The optimization problem is:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种度量学习方法是（Ghodsi et al., [2007](#bib.bib35)），它最小化相似点之间的距离并最大化不相似点之间的距离。为此，我们最小化相似点之间的距离和不相似点之间距离的负值。权重矩阵应为正半定，以满足三角不等式和凸性。权重矩阵的迹也设置为常数，以消除平凡解$\boldsymbol{W}=\boldsymbol{0}$。优化问题是：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (16) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\frac{1}{|\mathcal{S}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}|_{\boldsymbol{W}}^{2}$
    |  | (16) |'
- en: '|  |  | $\displaystyle-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{1}{|\mathcal{D}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}|_{\boldsymbol{W}}^{2}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受制于 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: '|  |  | $\displaystyle\textbf{tr}(\boldsymbol{W})=1,$ |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\textbf{tr}(\boldsymbol{W})=1,$ |  |'
- en: where $|.|$ denotes the cardinality of set.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$|.|$表示集合的基数。
- en: Lemma 3  ((Ghodsi et al., [2007](#bib.bib35))).
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 3  （Ghodsi et al., [2007](#bib.bib35)）。
- en: 'The objective function can be simplified as:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数可以简化为：
- en: '|  |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}$
    |  | (17) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\frac{1}{|\mathcal{S}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}|_{\boldsymbol{W}}^{2}-\frac{1}{|\mathcal{D}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}|_{\boldsymbol{W}}$
    |  | (17) |'
- en: '|  |  | $\displaystyle~{}~{}=\textbf{vec}(\boldsymbol{W})^{\top}\Big{(}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{vec}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\big{)}$
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}=\textbf{vec}(\boldsymbol{W})^{\top}\Big{(}\frac{1}{|\mathcal{S}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{vec}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\big{)}$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{vec}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\big{)}\Big{)},$
    |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{vec}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\big{)}\Big{)},$
    |  |'
- en: where $\textbf{vec}(.)$ vectorizes the matrix to a vector (Ghojogh et al., [2021c](#bib.bib48)).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{vec}(.)$ 将矩阵向量化为一个向量 (Ghojogh 等， [2021c](#bib.bib48))。
- en: Proof.
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: See (Ghodsi et al., [2007](#bib.bib35), Section 2.1) for proof. ∎
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 见 (Ghodsi 等， [2007](#bib.bib35), 第 2.1 节) 以获得证明。 ∎
- en: 'According to Lemma [3](#Thmlemma3 "Lemma 3 ((Ghodsi et al., 2007)). ‣ 3.1.2
    Formulating as Semidefinite Programming ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"), Eq. ([16](#S3.E16 "In 3.1.2 Formulating as Semidefinite
    Programming ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is
    a Semidefinite Programming (SDP) problem. It can be solved iteratively using the
    interior-point method (Ghojogh et al., [2021c](#bib.bib48)).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 根据引理 [3](#Thmlemma3 "引理 3 ((Ghodsi 等，2007)). ‣ 3.1.2 形式化为半正定规划 ‣ 3.1 使用散点的谱方法
    ‣ 3 谱度量学习 ‣ 谱方法、概率方法和深度度量学习：教程与调查")，公式 ([16](#S3.E16 "在 3.1.2 形式化为半正定规划 ‣ 3.1
    使用散点的谱方法 ‣ 3 谱度量学习 ‣ 谱方法、概率方法和深度度量学习：教程与调查")) 是一个半正定规划 (SDP) 问题。可以使用内点法 (Ghojogh
    等， [2021c](#bib.bib48)) 迭代求解。
- en: 3.1.3 Relevant to Fisher Discriminant Analysis
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 与 Fisher 判别分析相关
- en: Another metric learning method is (Alipanahi et al., [2008](#bib.bib2)) which
    has two approaches, introduced in the following. The relation of metric learning
    with Fisher discriminant analysis (Fisher, [1936](#bib.bib33); Ghojogh et al.,
    [2019b](#bib.bib39)) was discussed in this paper (Alipanahi et al., [2008](#bib.bib2)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种度量学习方法是 (Alipanahi 等， [2008](#bib.bib2))，该方法有两种方法，下面将介绍。本文讨论了度量学习与 Fisher
    判别分析 (Fisher, [1936](#bib.bib33); Ghojogh 等， [2019b](#bib.bib39)) 的关系 (Alipanahi
    等， [2008](#bib.bib2))。
- en: '– Approach 1: As $\boldsymbol{W}\succeq\boldsymbol{0}$, the weight matrix can
    be decomposed as in Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), i.e., $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$.
    Hence, we have:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '– 方法 1: 由于 $\boldsymbol{W}\succeq\boldsymbol{0}$，权重矩阵可以如公式 ([9](#S2.E9 "在证明中。
    ‣ 2.3 广义马哈拉诺比斯距离 ‣ 2 广义马哈拉诺比斯距离度量 ‣ 谱方法、概率方法和深度度量学习：教程与调查")) 中的分解，即 $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$。因此，我们得到：'
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    | $\displaystyle\overset{(\ref{equation_generalized_Mahalanobis_distance})}{=}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    | $\displaystyle\overset{(\ref{equation_generalized_Mahalanobis_distance})}{=}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
- en: '|  |  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}$
    |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}$
    |  |'
- en: '|  |  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}\textbf{tr}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}$
    |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}\textbf{tr}\big{(}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}$
    |  |'
- en: '|  |  | $\displaystyle\overset{(b)}{=}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)},$
    |  | (18) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(b)}{=}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)},$
    |  | (18) |'
- en: 'where $(a)$ is because a scalar is equal to its trace and $(b)$ is because
    of the cyclic property of trace. We can substitute Eq. ([18](#S3.E18 "In 3.1.3
    Relevant to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) in Eq. ([16](#S3.E16 "In 3.1.2 Formulating as Semidefinite
    Programming ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) to
    obtain an optimization problem:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是因为一个标量等于其迹，$(b)$ 是因为迹的循环性质。我们可以将方程 ([18](#S3.E18 "在 3.1.3 节 相关于费舍尔判别分析
    ‣ 3.1 使用散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和综述")) 代入方程 ([16](#S3.E16 "在 3.1.2
    节 公式化为半正定规划 ‣ 3.1 使用散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和综述")) 以得到一个优化问题：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)}$
    |  | (19) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{最小化}}$ |  | $\displaystyle\frac{1}{\|\mathcal{S}\|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)}$
    |  | (19) |'
- en: '|  |  | $\displaystyle\!\!\!\!\!\!\!-\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)}$
    |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\!\!\!\!\!\!\!-\frac{1}{\|\mathcal{D}\|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\big{)}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})=1,$
    |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})=1,$
    |  |'
- en: 'whose objective variable is $\boldsymbol{U}$. Note that the constraint $\boldsymbol{W}\succeq\boldsymbol{0}$
    is implicitly satisfied because of the decomposition $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$.
    We define:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其目标变量是 $\boldsymbol{U}$。注意，由于分解 $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$，约束条件
    $\boldsymbol{W}\succeq\boldsymbol{0}$ 被隐式满足。我们定义：
- en: '|  |  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\overset{(\ref{equation_spectral_ML_first_method_trace_W_Sigma_S})}{=}\frac{1}{&#124;\mathcal{S}&#124;}\boldsymbol{\Sigma}_{\mathcal{S}},$
    |  | (20) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\frac{1}{\|\mathcal{S}\|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\overset{(\ref{equation_spectral_ML_first_method_trace_W_Sigma_S})}{=}\frac{1}{\|\mathcal{S}\|}\boldsymbol{\Sigma}_{\mathcal{S}},$
    |  | (20) |'
- en: '|  |  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\overset{(\ref{equation_spectral_ML_first_method_trace_W_Sigma_S})}{=}\frac{1}{&#124;\mathcal{D}&#124;}\boldsymbol{\Sigma}_{\mathcal{D}}.$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\frac{1}{\|\mathcal{D}\|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\overset{(\ref{equation_spectral_ML_first_method_trace_W_Sigma_S})}{=}\frac{1}{\|\mathcal{D}\|}\boldsymbol{\Sigma}_{\mathcal{D}}.$
    |  |'
- en: 'Hence, Eq. ([19](#S3.E19 "In 3.1.3 Relevant to Fisher Discriminant Analysis
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can be restated
    as:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，方程 ([19](#S3.E19 "在 3.1.3 节 相关于费舍尔判别分析 ‣ 3.1 使用散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和综述"))
    可以重新表述为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})$
    |  | (21) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{最小化}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})$
    |  | (21) |'
- en: '|  |  | subject to |  | $\displaystyle\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})=1,$
    |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})=1,$
    |  |'
- en: 'whose Lagrangian is (Ghojogh et al., [2021c](#bib.bib48)):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 其拉格朗日函数是 (Ghojogh 等人，[2021c](#bib.bib48))：
- en: '|  | $\displaystyle\mathcal{L}=\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})-\lambda(\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})-1).$
    |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})-\lambda(\textbf{tr}(\boldsymbol{U}\boldsymbol{U}^{\top})-1).$
    |  |'
- en: 'Taking derivative of the Lagrangian and setting it to zero gives:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对拉格朗日函数取导数并令其为零得到：
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{U}}=2(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}-2\lambda\boldsymbol{U}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{U}}=2(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}-2\lambda\boldsymbol{U}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
- en: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}=\lambda\boldsymbol{U},$
    |  | (22) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}=\lambda\boldsymbol{U},$
    |  | (22) |'
- en: 'which is the eigenvalue problem for $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    (Ghojogh et al., [2019a](#bib.bib38)). Hence, $\boldsymbol{U}$ is the eigenvector
    of $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    with the smallest eigenvalue because Eq. ([19](#S3.E19 "In 3.1.3 Relevant to Fisher
    Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    is a minimization problem.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '这是 $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    的特征值问题 (Ghojogh et al., [2019a](#bib.bib38))。因此，$\boldsymbol{U}$ 是 $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    的特征向量，具有最小特征值，因为公式 ([19](#S3.E19 "In 3.1.3 Relevant to Fisher Discriminant Analysis
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 是一个最小化问题。'
- en: '– Approach 2: We can change the constraint in Eq. ([21](#S3.E21 "In 3.1.3 Relevant
    to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) to have orthogonal projection matrix, i.e., $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$.
    Rather, we can make the rotation of the projection matrix by the matrix $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$
    be orthogonal, i.e., $\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\boldsymbol{U}=\boldsymbol{I}$.
    Hence, the optimization problem becomes:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '– 方法 2：我们可以将公式 ([21](#S3.E21 "In 3.1.3 Relevant to Fisher Discriminant Analysis
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 中的约束更改为具有正交投影矩阵，即
    $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$。而是，我们可以使投影矩阵通过矩阵 $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$
    的旋转是正交的，即 $\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\boldsymbol{U}=\boldsymbol{I}$。因此，优化问题变为：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})$
    |  | (23) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})$
    |  | (23) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}=\boldsymbol{I},$
    |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}=\boldsymbol{I},$
    |  |'
- en: 'whose Lagrangian is (Ghojogh et al., [2021c](#bib.bib48)):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 其拉格朗日函数为 (Ghojogh et al., [2021c](#bib.bib48))：
- en: '|  | $\displaystyle\mathcal{L}=\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})-\textbf{tr}(\boldsymbol{\Lambda}^{\top}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}-\boldsymbol{I})).$
    |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U})-\textbf{tr}(\boldsymbol{\Lambda}^{\top}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}-\boldsymbol{I})).$
    |  |'
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{U}}=2(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}-2\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}\boldsymbol{\Lambda}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{U}}=2(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}-2\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}\boldsymbol{\Lambda}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
- en: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}=\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}\boldsymbol{\Lambda},$
    |  | (24) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})\boldsymbol{U}=\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}\,\boldsymbol{U}\boldsymbol{\Lambda},$
    |  | (24) |'
- en: which is the generalized eigenvalue problem for $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}},\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$
    (Ghojogh et al., [2019a](#bib.bib38)). Hence, $\boldsymbol{U}$ is a matrix whose
    columns are the eigenvectors sorted from the smallest to largest eigenvalues.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对 $(\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}},\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$
    的广义特征值问题（Ghojogh 等，[2019a](#bib.bib38)）。因此，$\boldsymbol{U}$ 是一个矩阵，其列是按特征值从小到大的特征向量。
- en: The optimization problem is similar to the optimization of Fisher discriminant
    analysis (FDA) (Fisher, [1936](#bib.bib33); Ghojogh et al., [2019b](#bib.bib39))
    where $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ and $\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$
    are replaced with the intra-class and inter-class covariance matrices of data,
    respectively. This shows the relation of this method with FDA. It makes sense
    because both metric learning and FDA have the same goal and that is decreasing
    and increasing the variances of similar and dissimilar points, respectively.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题类似于 Fisher 判别分析（FDA）的优化（Fisher，[1936](#bib.bib33)；Ghojogh 等，[2019b](#bib.bib39)），其中
    $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ 和 $\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$
    被数据的类内和类间协方差矩阵替代。这显示了该方法与 FDA 的关系。这是有意义的，因为度量学习和 FDA 的最终目标相同，即分别减少和增加相似和不相似点的方差。
- en: 3.1.4 Relevant Component Analysis (RCA)
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 相关组件分析（RCA）
- en: 'Suppose the $n$ data points can be divided into $c$ clusters, or so-called
    chunklets. If class labels are available, classes are the chunklets. If $\mathcal{X}_{l}$
    denotes the data of the $l$-th cluster and $\boldsymbol{\mu}_{l}$ is the mean
    of $\mathcal{X}_{l}$, the summation of intra-cluster scatters is:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 $n$ 个数据点可以被分成 $c$ 个簇，或称为块。如果类标签可用，那么类就是这些块。如果 $\mathcal{X}_{l}$ 表示第 $l$ 个簇的数据，$\boldsymbol{\mu}_{l}$
    是 $\mathcal{X}_{l}$ 的均值，那么簇内散布的总和为：
- en: '|  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{w}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{\boldsymbol{x}_{i}\in\mathcal{X}_{l}}(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{l})(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{l})^{\top}.$
    |  | (25) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{w}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{\boldsymbol{x}_{i}\in\mathcal{X}_{l}}(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{l})(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{l})^{\top}.$
    |  | (25) |'
- en: 'Relevant Component Analysis (RCA) (Shental et al., [2002](#bib.bib101)) is
    a metric learning method. In this method, we first apply Principal Component Analysis
    (PCA) (Ghojogh & Crowley, [2019](#bib.bib37)) on data using the total scatter
    of data. Let the projection matrix of PCA be denoted by $\boldsymbol{U}$. After
    projection onto the PCA subspace, the summation of intra-cluster scatters is $\widehat{\boldsymbol{S}}_{w}:=\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U}$
    because of the quadratic characteristic of covariance. RCA uses $\widehat{\boldsymbol{S}}_{w}$
    as the covariance matrix in the Mahalanobis distance, i.e., Eq. ([2](#S2.E2 "In
    Definition 2 (Mahalanobis distance (Mahalanobis, 1930)). ‣ 2.2 Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")). According to Eq. ([8](#S2.E8 "In Proposition
    2 (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")), the subspace of RDA is obtained by the eigenvalue
    (or singular value) decomposition of $\widehat{\boldsymbol{S}}_{w}^{-1}$ (see
    Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '相关成分分析 (RCA) (Shental 等人，[2002](#bib.bib101)) 是一种度量学习方法。在此方法中，我们首先对数据应用主成分分析
    (PCA) (Ghojogh & Crowley，[2019](#bib.bib37))，利用数据的总散布。设 PCA 的投影矩阵为 $\boldsymbol{U}$。在
    PCA 子空间上的投影后，类内散布的总和为 $\widehat{\boldsymbol{S}}_{w}:=\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U}$，这是由于协方差的二次特性。RCA
    使用 $\widehat{\boldsymbol{S}}_{w}$ 作为 Mahalanobis 距离中的协方差矩阵，即 Eq. ([2](#S2.E2 "In
    Definition 2 (Mahalanobis distance (Mahalanobis, 1930)). ‣ 2.2 Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) 所示。根据 Eq. ([8](#S2.E8 "In Proposition
    2 (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey"))，RDA 的子空间通过 $\widehat{\boldsymbol{S}}_{w}^{-1}$
    的特征值（或奇异值）分解获得（见 Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"))）。'
- en: 3.1.5 Discriminative Component Analysis (DCA)
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 判别成分分析 (DCA)
- en: 'Discriminative Component Analysis (DCA) (Hoi et al., [2006](#bib.bib69)) is
    another spectral metric learning method based on scatters of clusters/classes.
    Consider the $c$ clusters, chunklets, or classes of data. The intra-class scatter
    is as in Eq. ([25](#S3.E25 "In 3.1.4 Relevant Component Analysis (RCA) ‣ 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). The inter-class scatter is:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '判别成分分析 (DCA) (Hoi 等人，[2006](#bib.bib69)) 是另一种基于类散布的光谱度量学习方法。考虑 $c$ 个聚类、块集或数据类。类内散布如
    Eq. ([25](#S3.E25 "In 3.1.4 Relevant Component Analysis (RCA) ‣ 3.1 Spectral Methods
    Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) 所示。类间散布为：'
- en: '|  |  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{b}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{j=1}^{c}(\boldsymbol{\mu}_{l}-\boldsymbol{\mu}_{j})(\boldsymbol{\mu}_{l}-\boldsymbol{\mu}_{j})^{\top},\text{
    or }$ |  | (26) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{b}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{j=1}^{c}(\boldsymbol{\mu}_{l}-\boldsymbol{\mu}_{j})(\boldsymbol{\mu}_{l}-\boldsymbol{\mu}_{j})^{\top},\text{
    or }$ |  | (26) |'
- en: '|  |  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{b}:=\frac{1}{n}\sum_{l=1}^{c}(\boldsymbol{\mu}_{l}-\boldsymbol{\mu})(\boldsymbol{\mu}_{l}-\boldsymbol{\mu})^{\top},$
    |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{S}_{b}:=\frac{1}{n}\sum_{l=1}^{c}(\boldsymbol{\mu}_{l}-\boldsymbol{\mu})(\boldsymbol{\mu}_{l}-\boldsymbol{\mu})^{\top},$
    |  |'
- en: 'where $\boldsymbol{\mu}_{l}$ is the mean of the $l$-th class and $\boldsymbol{\mu}$
    is the total mean of data. According to Proposition [2](#Thmproposition2 "Proposition
    2 (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey"), metric learning can be seen as Euclidean distance
    after projection onto the column space of a projection matrix $\boldsymbol{U}$
    where $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$. Similar to Fisher
    discriminant analysis (Fisher, [1936](#bib.bib33); Ghojogh et al., [2019b](#bib.bib39)),
    DCA maximizes the inter-class variance and minimizes the intra-class variance
    after projection. Hence, its optimization is:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\mu}_{l}$ 是第 $l$ 类的均值，$\boldsymbol{\mu}$ 是数据的总均值。根据命题 [2](#Thmproposition2
    "命题 2 (度量学习中的投影)。 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱、概率及深度度量学习：教程与调查")，度量学习可以被视为在投影矩阵
    $\boldsymbol{U}$ 的列空间上投影后的欧氏距离，其中 $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$。类似于
    Fisher 判别分析（Fisher，[1936](#bib.bib33)；Ghojogh 等，[2019b](#bib.bib39)），DCA 在投影后最大化类间方差并最小化类内方差。因此，其优化为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U})},$
    |  | (27) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{最大化}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U})},$
    |  | (27) |'
- en: 'which is a generalized Rayleigh-Ritz quotient. The solution $\boldsymbol{U}$
    to this optimization problem is the generalized eigenvalue problem $(\boldsymbol{S}_{b},\boldsymbol{S}_{w})$
    (Ghojogh et al., [2019a](#bib.bib38)). According to Eq. ([9](#S2.E9 "In Proof.
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we
    can set the weight matrix of the generalized Mahalanobis distance as $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$
    where $\boldsymbol{U}$ is the matrix of eigenvectors.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种广义的 Rayleigh-Ritz 商。该优化问题的解 $\boldsymbol{U}$ 是广义特征值问题 $(\boldsymbol{S}_{b},\boldsymbol{S}_{w})$（Ghojogh
    等，[2019a](#bib.bib38)）。根据方程 ([9](#S2.E9 "在证明中 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱、概率及深度度量学习：教程与调查")），我们可以将广义马氏距离的权重矩阵设置为
    $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$，其中 $\boldsymbol{U}$ 是特征向量矩阵。
- en: 3.1.6 High Dimensional Discriminative Component Analysis
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.6 高维判别成分分析
- en: 'Another spectral method for metric learning is (Xiang et al., [2008](#bib.bib126))
    which minimizes and maximizes the intra-class and inter-class variances, respectively,
    by the the same optimization problem as Eq. ([27](#S3.E27 "In 3.1.5 Discriminative
    Component Analysis (DCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    with an additional constraint on the orthogonality of the projection matrix, i.e.,
    $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$. This problem can be restated
    by posing penalty on the denominator:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于度量学习的谱方法是（Xiang 等，[2008](#bib.bib126)），该方法通过与方程 ([27](#S3.E27 "在 3.1.5
    判别成分分析 (DCA) ‣ 3.1 谱方法使用散布 ‣ 3 谱度量学习 ‣ 谱、概率及深度度量学习：教程与调查")) 相同的优化问题来最小化和最大化类内和类间方差，同时对投影矩阵的正交性施加额外约束，即
    $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$。这个问题可以通过对分母施加惩罚来重新表述：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w})\boldsymbol{U})$
    |  | (28) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{最大化}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w})\boldsymbol{U})$
    |  | (28) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
- en: 'where $\lambda>0$ is the regularization parameter. The solution to this problem
    is the eigenvalue problem for $\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w}$.
    The eigenvectors are the columns of $\boldsymbol{U}$ and the weight matrix of
    the generalized Mahalanobis is obtained using Eq. ([9](#S2.E9 "In Proof. ‣ 2.3
    Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\lambda>0$ 是正则化参数。这个问题的解是 $\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w}$
    的特征值问题。特征向量是 $\boldsymbol{U}$ 的列，广义马哈拉诺比斯的权重矩阵是通过公式 ([9](#S2.E9 "In Proof. ‣ 2.3
    Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 获得的。'
- en: 'If the dimensionality of data is large, computing the eigenvectors of $(\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w})\in\mathbb{R}^{d\times
    d}$ is very time-consuming. According to (Xiang et al., [2008](#bib.bib126), Theorem
    3), the optimization problem ([28](#S3.E28 "In 3.1.6 High Dimensional Discriminative
    Component Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can
    be solved in the orthogonal complement space of the null space of $\boldsymbol{S}_{b}+\boldsymbol{S}_{w}$
    without loss of any information (see (Xiang et al., [2008](#bib.bib126), Appendix
    A) for proof). Hence, if $d\gg 1$, we find $\boldsymbol{U}$ as follows. Let $\boldsymbol{X}:=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]\in\mathbb{R}^{d\times
    n}$ be the matrix of data. Let $\boldsymbol{A}_{w}$ and $\boldsymbol{A}_{b}$ be
    the adjacency matrices for the sets $\mathcal{S}$ and $\mathcal{D}$, respectively.
    For example, if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$, then
    $\boldsymbol{A}_{w}(i,j)=1$; otherwise, $\boldsymbol{A}_{w}(i,j)=0$. If $\boldsymbol{L}_{w}$
    and $\boldsymbol{L}_{b}$ are the Laplacian matrices of $\boldsymbol{A}_{w}$ and
    $\boldsymbol{A}_{b}$, respectively, we have $\boldsymbol{S}_{w}=0.5\boldsymbol{X}\boldsymbol{L}_{w}\boldsymbol{X}^{\top}$
    and $\boldsymbol{S}_{b}=0.5\boldsymbol{X}\boldsymbol{L}_{b}\boldsymbol{X}^{\top}$
    (see (Belkin & Niyogi, [2002](#bib.bib9); Ghojogh et al., [2021d](#bib.bib49))
    for proof). We have $\textbf{tr}(\boldsymbol{S}_{w}+\boldsymbol{S}_{b})=\textbf{tr}(\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b})\boldsymbol{X}^{\top})=\textbf{tr}(\boldsymbol{X}^{\top}\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b}))$
    because of the cyclic property of trace. If the rank of $\boldsymbol{L}:=\boldsymbol{X}^{\top}\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b})\in\mathbb{R}^{n\times
    n}$ is $r\leq n$, it has $r$ non-zero eigenvalues which we compute its corresponding
    eigenvectors. We stack these eigenvectors to have $\boldsymbol{V}\in\mathbb{R}^{d\times
    r}$. The projected intra-class and inter-class variances after projection onto
    the column space of $\boldsymbol{V}$ are $\boldsymbol{S}^{\prime}_{w}:=\boldsymbol{V}^{\top}\boldsymbol{S}_{w}\boldsymbol{V}$
    and $\boldsymbol{S}^{\prime}_{b}:=\boldsymbol{V}^{\top}\boldsymbol{S}_{b}\boldsymbol{V}$,
    respectively. Then, we use $\boldsymbol{S}^{\prime}_{w}$ and $\boldsymbol{S}^{\prime}_{b}$
    in Eq. ([28](#S3.E28 "In 3.1.6 High Dimensional Discriminative Component Analysis
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) and the weight
    matrix of the generalized Mahalanobis is obtained using Eq. ([9](#S2.E9 "In Proof.
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据的维度很大，那么计算$(\boldsymbol{S}_{b}-\lambda\boldsymbol{S}_{w})\in\mathbb{R}^{d\times
    d}$的特征向量是非常耗时的。根据（Xiang 等，[2008](#bib.bib126)，定理 3），优化问题（[28](#S3.E28 "在 3.1.6
    高维判别成分分析 ‣ 3.1 基于散度的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程与调查")）可以在$\boldsymbol{S}_{b}+\boldsymbol{S}_{w}$的零空间的正交补空间中解决，而不会丢失任何信息（见（Xiang
    等，[2008](#bib.bib126)，附录 A）以获取证明）。因此，如果$d\gg 1$，我们如下找到$\boldsymbol{U}$。令$\boldsymbol{X}:=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]\in\mathbb{R}^{d\times
    n}$为数据矩阵。令$\boldsymbol{A}_{w}$和$\boldsymbol{A}_{b}$分别为集合$\mathcal{S}$和$\mathcal{D}$的邻接矩阵。例如，如果$(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$，则$\boldsymbol{A}_{w}(i,j)=1$；否则，$\boldsymbol{A}_{w}(i,j)=0$。如果$\boldsymbol{L}_{w}$和$\boldsymbol{L}_{b}$分别是$\boldsymbol{A}_{w}$和$\boldsymbol{A}_{b}$的拉普拉斯矩阵，我们有$\boldsymbol{S}_{w}=0.5\boldsymbol{X}\boldsymbol{L}_{w}\boldsymbol{X}^{\top}$和$\boldsymbol{S}_{b}=0.5\boldsymbol{X}\boldsymbol{L}_{b}\boldsymbol{X}^{\top}$（见（Belkin
    & Niyogi，[2002](#bib.bib9)；Ghojogh 等，[2021d](#bib.bib49)）以获取证明）。我们有$\textbf{tr}(\boldsymbol{S}_{w}+\boldsymbol{S}_{b})=\textbf{tr}(\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b})\boldsymbol{X}^{\top})=\textbf{tr}(\boldsymbol{X}^{\top}\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b}))$，这是由于迹的循环性质。如果$\boldsymbol{L}:=\boldsymbol{X}^{\top}\boldsymbol{X}(0.5\boldsymbol{L}_{w}+0.5\boldsymbol{L}_{b})\in\mathbb{R}^{n\times
    n}$的秩为$r\leq n$，它有$r$个非零特征值，我们计算其对应的特征向量。我们将这些特征向量堆叠起来得到$\boldsymbol{V}\in\mathbb{R}^{d\times
    r}$。投影到$\boldsymbol{V}$的列空间后的类内方差和类间方差分别为$\boldsymbol{S}^{\prime}_{w}:=\boldsymbol{V}^{\top}\boldsymbol{S}_{w}\boldsymbol{V}$和$\boldsymbol{S}^{\prime}_{b}:=\boldsymbol{V}^{\top}\boldsymbol{S}_{b}\boldsymbol{V}$。然后，我们在方程（[28](#S3.E28
    "在 3.1.6 高维判别成分分析 ‣ 3.1 基于散度的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程与调查")）中使用$\boldsymbol{S}^{\prime}_{w}$和$\boldsymbol{S}^{\prime}_{b}$，并使用方程（[9](#S2.E9
    "在证明 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱的、概率的和深度的度量学习：教程与调查")）得到广义马氏距离的权重矩阵。
- en: 3.1.7 Regularization by Locally Linear Embedding
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.7 通过局部线性嵌入进行正则化
- en: 'The spectral metric learning methods using scatters can be modeled as maximization
    of the following Rayleigh–Ritz quotient (Baghshah & Shouraki, [2009](#bib.bib4)):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用散度的谱度量学习方法可以被建模为最大化以下 Rayleigh–Ritz 商（Baghshah & Shouraki，[2009](#bib.bib4)）：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}+\lambda\Omega(\boldsymbol{U})},$
    |  | (29) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{最大化}}$ |  | $\displaystyle\frac{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}}{\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}+\lambda\Omega(\boldsymbol{U})},$
    |  | (29) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受制于 |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
- en: 'where $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$ (see Eq. ([9](#S2.E9
    "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"))), $\lambda>0$ is the regularization parameter, and $\Omega(\boldsymbol{U})$
    is a penalty or regularization term on the projection matrix $\boldsymbol{U}$.
    This optimization maximizes and minimizes the distances of the similar and dissimilar
    points, respectively. According to Section [3.1.3](#S3.SS1.SSS3 "3.1.3 Relevant
    to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"), Eq. ([29](#S3.E29 "In 3.1.7 Regularization by Locally Linear Embedding
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can be restated
    as:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$（见方程（[9](#S2.E9 "证明中
    ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱、概率和深度度量学习：教程和调查")）），$\lambda>0$ 是正则化参数，$\Omega(\boldsymbol{U})$
    是对投影矩阵 $\boldsymbol{U}$ 的惩罚或正则化项。该优化最大化和最小化相似点和不相似点的距离。根据第 [3.1.3](#S3.SS1.SSS3
    "3.1.3 相关于 Fisher 判别分析 ‣ 3.1 基于散度的谱方法 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程和调查") 节，方程（[29](#S3.E29
    "在 3.1.7 通过局部线性嵌入进行正则化 ‣ 3.1 基于散度的谱方法 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程和调查")）可以重述为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U})+\lambda\Omega(\boldsymbol{U})},$
    |  | (30) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{最大化}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{w}\boldsymbol{U})+\lambda\Omega(\boldsymbol{U})},$
    |  | (30) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受制于 |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
- en: 'As was discussed in Proposition [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"), metric learning can be seen as projection onto a subspace. The regularization
    term can be linear reconstruction of every projected point by its $k$ Nearest
    Neighbors ($k$NN) using the same reconstruction weights as before projection (Baghshah
    & Shouraki, [2009](#bib.bib4)). The weights for linear reconstruction in the input
    space can be found as in locally linear embedding (Roweis & Saul, [2000](#bib.bib97);
    Ghojogh et al., [2020a](#bib.bib40)). If $s_{ij}$ denotes the weight of $\boldsymbol{x}_{j}$
    in reconstruction of $\boldsymbol{x}_{i}$ and $\mathcal{N}(\boldsymbol{x}_{i})$
    is the set of $k$NN for $\boldsymbol{x}_{i}$, we have:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如在命题 [2](#Thmproposition2 "命题 2（度量学习中的投影）。 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱、概率和深度度量学习：教程和调查")
    中讨论的，度量学习可以被视为对一个子空间的投影。正则化项可以是通过其 $k$ 个最近邻（$k$NN）对每个投影点的线性重建，使用与投影前相同的重建权重（Baghshah
    & Shouraki，[2009](#bib.bib4)）。在输入空间中的线性重建权重可以像局部线性嵌入一样找到（Roweis & Saul，[2000](#bib.bib97)；Ghojogh
    et al.，[2020a](#bib.bib40)）。如果 $s_{ij}$ 表示在重建 $\boldsymbol{x}_{i}$ 时 $\boldsymbol{x}_{j}$
    的权重，并且 $\mathcal{N}(\boldsymbol{x}_{i})$ 是 $\boldsymbol{x}_{i}$ 的 $k$NN 集合，我们有：
- en: '|  | $\displaystyle\underset{s_{ij}}{\text{minimize}}$ | $\displaystyle\sum_{i=1}^{n}\Big{\&#124;}\boldsymbol{x}_{i}-\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s_{ij}\boldsymbol{x}_{j}\Big{\&#124;}_{2}^{2},$
    |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{s_{ij}}{\text{最小化}}$ | $\displaystyle\sum_{i=1}^{n}\Big{\&#124;}\boldsymbol{x}_{i}-\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s_{ij}\boldsymbol{x}_{j}\Big{\&#124;}_{2}^{2},$
    |  |'
- en: '|  | subject to | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s_{ij}=1.$
    |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | 约束条件 | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s_{ij}=1.$
    |  |'
- en: 'The solution of this optimization is (Ghojogh et al., [2020a](#bib.bib40)):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 该优化的解是 (Ghojogh et al., [2020a](#bib.bib40))：
- en: '|  | $\displaystyle s_{ij}^{*}=\frac{\boldsymbol{G}_{i}^{-1}\boldsymbol{1}}{\boldsymbol{1}^{\top}\boldsymbol{G}_{i}^{-1}\boldsymbol{1}},$
    |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s_{ij}^{*}=\frac{\boldsymbol{G}_{i}^{-1}\boldsymbol{1}}{\boldsymbol{1}^{\top}\boldsymbol{G}_{i}^{-1}\boldsymbol{1}},$
    |  |'
- en: 'where $\boldsymbol{G}_{i}:=(\boldsymbol{x}_{i}\boldsymbol{1}^{\top}-\boldsymbol{X}_{i})^{\top}(\boldsymbol{x}_{i}\boldsymbol{1}^{\top}-\boldsymbol{X}_{i})$
    in which $\boldsymbol{X}_{i}\in\mathbb{R}^{d\times k}$ denotes the stack of $k$NN
    for $\boldsymbol{x}_{i}$. We define $\boldsymbol{S}^{*}:=[s^{*}_{ij}]\in\mathbb{R}^{n\times
    n}$. The regularization term can be reconstruction in the subspace using the same
    reconstruction weights as in the input space (Baghshah & Shouraki, [2009](#bib.bib4)):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{G}_{i}:=(\boldsymbol{x}_{i}\boldsymbol{1}^{\top}-\boldsymbol{X}_{i})^{\top}(\boldsymbol{x}_{i}\boldsymbol{1}^{\top}-\boldsymbol{X}_{i})$，其中
    $\boldsymbol{X}_{i}\in\mathbb{R}^{d\times k}$ 表示 $\boldsymbol{x}_{i}$ 的 $k$ 最近邻的堆叠。我们定义
    $\boldsymbol{S}^{*}:=[s^{*}_{ij}]\in\mathbb{R}^{n\times n}$。正则化项可以使用与输入空间相同的重建权重在子空间中重建（Baghshah
    & Shouraki, [2009](#bib.bib4)）：
- en: '|  | $\displaystyle\Omega(\boldsymbol{U})$ | $\displaystyle:=\sum_{i=1}^{n}\Big{\&#124;}\boldsymbol{U}^{\top}\boldsymbol{x}-\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s^{*}_{ij}\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\Big{\&#124;}_{2}^{2}$
    |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Omega(\boldsymbol{U})$ | $\displaystyle:=\sum_{i=1}^{n}\Big{\&#124;}\boldsymbol{U}^{\top}\boldsymbol{x}-\sum_{\boldsymbol{x}_{j}\in\mathcal{N}(\boldsymbol{x}_{i})}s^{*}_{ij}\boldsymbol{U}^{\top}\boldsymbol{x}_{j}\Big{\&#124;}_{2}^{2}$
    |  |'
- en: '|  |  | $\displaystyle=\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top}\boldsymbol{U}),$
    |  | (31) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top}\boldsymbol{U}),$
    |  | (31) |'
- en: 'where $\boldsymbol{X}=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]\in\mathbb{R}^{d\times
    n}$ and $\mathbb{R}^{n\times n}\ni\boldsymbol{E}:=(\boldsymbol{I}-\boldsymbol{S}^{*})^{\top}(\boldsymbol{I}-\boldsymbol{S}^{*})$.
    Putting Eq. ([31](#S3.E31 "In 3.1.7 Regularization by Locally Linear Embedding
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) in Eq. ([30](#S3.E30
    "In 3.1.7 Regularization by Locally Linear Embedding ‣ 3.1 Spectral Methods Using
    Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) gives:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{X}=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]\in\mathbb{R}^{d\times
    n}$ 和 $\mathbb{R}^{n\times n}\ni\boldsymbol{E}:=(\boldsymbol{I}-\boldsymbol{S}^{*})^{\top}(\boldsymbol{I}-\boldsymbol{S}^{*})$。将
    Eq. ([31](#S3.E31 "在 3.1.7 局部线性嵌入的正则化 ‣ 3.1 使用散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查"))
    代入 Eq. ([30](#S3.E30 "在 3.1.7 局部线性嵌入的正则化 ‣ 3.1 使用散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查"))
    得到：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})\boldsymbol{U}\big{)}},$
    |  | (32) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{最大化}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}\boldsymbol{U})}{\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})\boldsymbol{U}\big{)}},$
    |  | (32) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
- en: 'The solution to this optimization problem is the generalized eigenvalue problem
    $(\boldsymbol{S}_{b},\boldsymbol{S}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})$
    where $\boldsymbol{U}$ has the eigenvectors as its columns (Ghojogh et al., [2019a](#bib.bib38)).
    According to Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), the weight matrix of metric is $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 该优化问题的解是广义特征值问题 $(\boldsymbol{S}_{b},\boldsymbol{S}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})$，其中
    $\boldsymbol{U}$ 的列是特征向量（Ghojogh et al., [2019a](#bib.bib38)）。根据公式 ([9](#S2.E9
    "在证明中。 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱、概率和深度度量学习：教程和调查")），度量的权重矩阵是 $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$。
- en: 3.1.8 Fisher-HSIC Multi-view Metric Learning (FISH-MML)
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.8 Fisher-HSIC 多视图度量学习（FISH-MML）
- en: 'Fisher-HSIC Multi-view Metric Learning (FISH-MML) (Zhang et al., [2018](#bib.bib142))
    is a metric learning method for multi-view data. In multi-view data, we have different
    types of features for every data point. For example, an image dataset, which has
    a descriptive caption for every image, is multi-view. Let $\boldsymbol{X}^{(r)}:=\{\boldsymbol{x}_{i}^{(r)}\}_{i=1}^{n}$
    be the features of data points in the $r$-th view, $c$ be the number of classes/clusters,
    and $v$ be the number of views. According to Proposition [2](#Thmproposition2
    "Proposition 2 (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"), metric learning is the Euclidean
    distance after projection with $\boldsymbol{U}$. The inter-class scatter of data,
    in the $r$-th view, is denoted by $\boldsymbol{S}_{b}^{(r)}$ and calculated using
    Eqs. ([26](#S3.E26 "In 3.1.5 Discriminative Component Analysis (DCA) ‣ 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). The total scatter of data, in
    the $r$-th view, is denoted by $\boldsymbol{S}_{t}^{(r)}$ and is the covariance
    of data in that view.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Fisher-HSIC 多视图度量学习（FISH-MML）（Zhang et al., [2018](#bib.bib142)）是一种针对多视图数据的度量学习方法。在多视图数据中，每个数据点有不同类型的特征。例如，一个图像数据集，每张图像都有一个描述性标题，就是多视图的。设
    $\boldsymbol{X}^{(r)}:=\{\boldsymbol{x}_{i}^{(r)}\}_{i=1}^{n}$ 为第 $r$ 种视图中的数据点特征，$c$
    为类别/簇的数量，$v$ 为视图的数量。根据命题 [2](#Thmproposition2 "命题 2（度量学习中的投影）。 ‣ 2.3 广义马氏距离 ‣
    2 广义马氏距离度量 ‣ 谱、概率和深度度量学习：教程和调查")，度量学习是通过 $\boldsymbol{U}$ 投影后的欧几里得距离。第 $r$ 种视图中的类间散布由
    $\boldsymbol{S}_{b}^{(r)}$ 表示，并通过公式 ([26](#S3.E26 "在 3.1.5 判别分量分析（DCA） ‣ 3.1 使用散布的谱方法
    ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程和调查")) 计算。第 $r$ 种视图中的数据总散布由 $\boldsymbol{S}_{t}^{(r)}$
    表示，是该视图中数据的协方差。
- en: 'Inspired by Fisher discriminant analysis (Fisher, [1936](#bib.bib33); Ghojogh
    et al., [2019b](#bib.bib39)), we maximize the inter-class variances of projected
    data, $\sum_{r=1}^{v}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}^{(r)}\boldsymbol{U})$,
    to discriminate the classes after projection. Also, inspired by principal component
    analysis (Ghojogh & Crowley, [2019](#bib.bib37)), we maximize the total scatter
    of projected data, $\sum_{r=1}^{v}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{t}^{(r)}\boldsymbol{U})$,
    for expressiveness. Moreover, we maximize the dependence of the projected data
    in all views because various views of a point should be related. A measure of
    dependence between two random variables $X$ and $Y$ is the Hilbert-Schmidt Independence
    Criterion (HSIC) (Gretton et al., [2005](#bib.bib56)) whose empirical estimation
    is:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 受 Fisher 判别分析的启发（Fisher, [1936](#bib.bib33); Ghojogh et al., [2019b](#bib.bib39)），我们最大化投影数据的类间方差，$\sum_{r=1}^{v}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}^{(r)}\boldsymbol{U})$，以在投影后区分类别。同时，受主成分分析的启发（Ghojogh
    & Crowley, [2019](#bib.bib37)），我们最大化投影数据的总散布，$\sum_{r=1}^{v}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{t}^{(r)}\boldsymbol{U})$，以增加表达能力。此外，我们最大化所有视图中投影数据的依赖性，因为一个点的不同视图应该是相关的。两个随机变量
    $X$ 和 $Y$ 之间的依赖度量是 Hilbert-Schmidt 独立性标准（HSIC）（Gretton et al., [2005](#bib.bib56)），其经验估计为：
- en: '|  | $\displaystyle\text{HSIC}(X,Y)=\frac{1}{(n-1)^{2}}\textbf{tr}(\boldsymbol{K}_{x}\boldsymbol{H}\boldsymbol{K}_{y}\boldsymbol{H}),$
    |  | (33) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{HSIC}(X,Y)=\frac{1}{(n-1)^{2}}\textbf{tr}(\boldsymbol{K}_{x}\boldsymbol{H}\boldsymbol{K}_{y}\boldsymbol{H}),$
    |  | (33) |'
- en: 'where $\boldsymbol{K}_{x}$ and $\boldsymbol{K}_{y}$ are kernel matrices over
    $X$ and $Y$ variables, respectively, and $\boldsymbol{H}:=\boldsymbol{I}-(1/n)\boldsymbol{1}\boldsymbol{1}^{\top}$
    is the centering matrix. The HSIC between projection of two views $\boldsymbol{X}^{(r)}$
    and $\boldsymbol{X}^{(w)}$ is:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\boldsymbol{K}_{x}$和$\boldsymbol{K}_{y}$分别是$X$和$Y$变量的核矩阵，而$\boldsymbol{H}:=\boldsymbol{I}-(1/n)\boldsymbol{1}\boldsymbol{1}^{\top}$是中心化矩阵。两个视角$\boldsymbol{X}^{(r)}$和$\boldsymbol{X}^{(w)}$的投影之间的HSIC为：
- en: '|  | $\displaystyle\text{HSIC}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)},\boldsymbol{U}^{\top}\boldsymbol{X}^{(w)})\overset{(\ref{equation_HSIC})}{\propto}\textbf{tr}(\boldsymbol{K}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H})$
    |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{HSIC}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)},\boldsymbol{U}^{\top}\boldsymbol{X}^{(w)})\overset{(\ref{equation_HSIC})}{\propto}\textbf{tr}(\boldsymbol{K}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H})$
    |  |'
- en: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{X}^{(r)\top}\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H})$
    |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{X}^{(r)\top}\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H})$
    |  |'
- en: '|  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}\boldsymbol{U})$
    |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}\boldsymbol{U})$
    |  |'
- en: where $(a)$ is because we use the linear kernel for $\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}$,
    i.e., $\boldsymbol{K}^{(r)}:=(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)})^{\top}\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}$
    and $(b)$ is because of the cyclic property of trace.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$(a)$是因为我们对$\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}$使用线性核，即$\boldsymbol{K}^{(r)}:=(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)})^{\top}\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}$，而$(b)$是由于迹的循环性质。
- en: 'In summary, we maximize the summation of inter-class scatter, total scatter,
    and the dependence of views, which is:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们最大化类间散布、总散布和视角依赖的总和，其为：
- en: '|  | $\displaystyle\sum_{r=1}^{v}\big{(}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}^{(r)}\boldsymbol{U})+\lambda_{1}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{t}^{(r)}\boldsymbol{U})$
    |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{r=1}^{v}\big{(}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{b}^{(r)}\boldsymbol{U})+\lambda_{1}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{t}^{(r)}\boldsymbol{U})$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}\boldsymbol{U})\big{)}$
    |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}\boldsymbol{U})\big{)}$
    |  |'
- en: '|  | $\displaystyle=\sum_{r=1}^{v}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\sum_{r=1}^{v}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top})\boldsymbol{U}\big{)},$
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top})\boldsymbol{U}\big{)},$
    |  |'
- en: 'where $\lambda_{1},\lambda_{2}>0$ are the regularization parameters. The optimization
    problem is:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\lambda_{1},\lambda_{2}>0$是正则化参数。优化问题为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{r=1}^{v}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}$
    |  | (34) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{r=1}^{v}\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}$
    |  | (34) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top})\boldsymbol{U}\big{)}$
    |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top})\boldsymbol{U}\big{)}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},$
    |  |'
- en: whose solution is the eigenvalue problem for $\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}$
    where $\boldsymbol{U}$ has the eigenvectors as its columns (Ghojogh et al., [2019a](#bib.bib38)).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其解为特征值问题，涉及$\boldsymbol{S}_{b}^{(r)}+\lambda_{1}\boldsymbol{S}_{t}^{(r)}+\lambda_{2}\boldsymbol{X}^{(r)}\boldsymbol{H}\boldsymbol{K}^{(w)}\boldsymbol{H}\boldsymbol{X}^{(r)\top}$，其中$\boldsymbol{U}$的列为特征向量（Ghojogh
    et al., [2019a](#bib.bib38)）。
- en: 3.2 Spectral Methods Using Hinge Loss
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用铰链损失的谱方法
- en: 3.2.1 Large-Margin Metric Learning
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 大间隔度量学习
- en: '$k$-Nearest Neighbors ($k$NN) classification is highly impacted by the metric
    used for measuring distances between points. Hence, we can use metric learning
    for improving the performance of $k$NN classification (Weinberger et al., [2006](#bib.bib124);
    Weinberger & Saul, [2009](#bib.bib123)). Let $y_{ij}=1$ if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$
    and $y_{ij}=0$ if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$. Moreover,
    we consider $k$NN for similar points where we find the nearest neighbors of every
    point among the similar points to that point. Let $\eta_{ij}=1$ if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$
    and $\boldsymbol{x}_{j}$ is among $k$NN of $\boldsymbol{x}_{i}$. Otherwise, $\eta_{ij}=0$.
    The optimization problem for finding the best weigh matrix in the metric can be
    (Weinberger et al., [2006](#bib.bib124); Weinberger & Saul, [2009](#bib.bib123)):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: $k$-最近邻 ($k$NN) 分类受用于测量点间距离的度量的影响很大。因此，我们可以使用度量学习来提高 $k$NN 分类的性能（Weinberger
    et al., [2006](#bib.bib124); Weinberger & Saul, [2009](#bib.bib123)）。设 $y_{ij}=1$
    如果 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$，且 $y_{ij}=0$ 如果 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$。此外，我们考虑对类似点的
    $k$NN，其中我们找到每个点在类似点中的最近邻。设 $\eta_{ij}=1$ 如果 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$
    且 $\boldsymbol{x}_{j}$ 在 $\boldsymbol{x}_{i}$ 的 $k$NN 中。否则，$\eta_{ij}=0$。在度量中寻找最佳权重矩阵的优化问题可以是（Weinberger
    et al., [2006](#bib.bib124); Weinberger & Saul, [2009](#bib.bib123)）：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}\eta_{ij}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (35) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}\eta_{ij}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (35) |'
- en: '|  |  | $\displaystyle+\lambda\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{l=1}^{n}\eta_{ij}(1-y_{il})\Big{[}1$
    |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{l=1}^{n}\eta_{ij}(1-y_{il})\Big{[}1$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\Big{]}_{+},$
    |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\Big{]}_{+},$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: where $\lambda>0$ is the regularization parameter, and $[.]_{+}:=\max(.,0)$
    is the standard Hinge loss.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 是正则化参数，$[.]_{+}:=\max(.,0)$ 是标准的 Hinge 损失。
- en: 'The first term in Eq. ([35](#S3.E35 "In 3.2.1 Large-Margin Metric Learning
    ‣ 3.2 Spectral Methods Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) pushes the similar
    neighbors close to each other. The second term in this equation is the triplet
    loss (Schroff et al., [2015](#bib.bib100)) which pushes the similar neighbors
    to each other and pulls the dissimilar points away from one another. This is because
    minimizing $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$ for
    $\eta_{ij}=1$ decreases the distances of similar neighbors. Moreover, minimizing
    $-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}$ for $1-y_{il}=1$
    (i.e., $y_{il}=0$) is equivalent to maximizing $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}$
    which maximizes the distances of dissimilar points. Minimizing the whole second
    term forces the distances of dissimilar points to be at least greater that the
    distances of similar points up to a threshold (or margin) of one. We can change
    the margin by changing $1$ in this term with some other positive number. In this
    sense, this loss is closely related to the triplet loss for neural networks (Schroff
    et al., [2015](#bib.bib100)) (see Section [5.3.5](#S5.SS3.SSS5 "5.3.5 Triplet
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 ([35](#S3.E35 "在 3.2.1 大间隔度量学习 ‣ 3.2 使用铰链损失的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程与调查"))
    中的第一项将相似的邻居彼此拉近。此方程中的第二项是三元组损失（Schroff 等，[2015](#bib.bib100)），它将相似的邻居拉近，同时将不相似的点拉远。这是因为最小化
    $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$ 对于 $\eta_{ij}=1$
    会减少相似邻居的距离。此外，最小化 $-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}$
    对于 $1-y_{il}=1$（即 $y_{il}=0$）等同于最大化 $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}$，这会最大化不相似点的距离。最小化整个第二项会迫使不相似点的距离至少大于相似点的距离，直至一个阈值（或边距）为
    1。我们可以通过将该项中的 1 更改为其他正数来改变边距。在这个意义上，这种损失与神经网络的三元组损失（Schroff 等，[2015](#bib.bib100)）（参见
    [5.3.5](#S5.SS3.SSS5 "5.3.5 三元组损失 ‣ 5.3 利用孪生网络的度量学习 ‣ 5 深度度量学习 ‣ 谱的、概率的和深度的度量学习：教程与调查")）密切相关。
- en: 'Eq. ([35](#S3.E35 "In 3.2.1 Large-Margin Metric Learning ‣ 3.2 Spectral Methods
    Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) can be restated using slack variables
    $\xi_{ijl},\forall i,j,l\in\{1,\dots,n\}$. The Hinge loss in term $[1+\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}]_{+}$
    requires to have:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 ([35](#S3.E35 "在 3.2.1 大间隔度量学习 ‣ 3.2 使用铰链损失的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程与调查"))
    可以使用松弛变量 $\xi_{ijl},\forall i,j,l\in\{1,\dots,n\}$ 进行重述。铰链损失项 $[1+\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}]_{+}$
    需要满足：
- en: '|  | $\displaystyle 1+\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\geq
    0$ |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 1+\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\geq
    0$ |  |'
- en: '|  | $\displaystyle\implies\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1.$ |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1.$ |  |'
- en: 'If $\xi_{ijl}\geq 0$, we can have sandwich the term $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$
    in order to minimize it:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $\xi_{ijl}\geq 0$，我们可以将 $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\|_{\boldsymbol{W}}^{2}-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$
    夹在中间以进行最小化：
- en: '|  | $\displaystyle 1-\xi_{ijl}\leq\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1.$ |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 1-\xi_{ijl}\leq\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1.$ |  |'
- en: 'Hence, we can replace the term of Hinge loss with the slack variable. Therefore,
    Eq. ([35](#S3.E35 "In 3.2.1 Large-Margin Metric Learning ‣ 3.2 Spectral Methods
    Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) can be restated as (Weinberger et al.,
    [2006](#bib.bib124); Weinberger & Saul, [2009](#bib.bib123)):'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们可以用松弛变量替换 Hinge 损失项。因此，方程 ([35](#S3.E35 "In 3.2.1 Large-Margin Metric
    Learning ‣ 3.2 Spectral Methods Using Hinge Loss ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 可以重述为（Weinberger
    等人，[2006](#bib.bib124)；Weinberger & Saul，[2009](#bib.bib123)）：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W},\,\{\xi_{ijl}\}}{\text{minimize}}$
    |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}\eta_{ij}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (36) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W},\,\{\xi_{ijl}\}}{\text{minimize}}$
    |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}\eta_{ij}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (36) |'
- en: '|  |  | $\displaystyle+\lambda\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{l=1}^{n}\eta_{ij}(1-y_{il})\,\xi_{ijl}$
    |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{l=1}^{n}\eta_{ij}(1-y_{il})\,\xi_{ijl}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\geq
    1-\xi_{ijl},$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\geq
    1-\xi_{ijl},$ |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},\eta_{ij}=1,(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},\eta_{ij}=1,(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
- en: '|  |  | $\displaystyle\xi_{ijl}\geq 0,$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\xi_{ijl}\geq 0,$ |  |'
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: This optimization problem is a semidefinite programming which can be solved
    iteratively using interior-point method (Ghojogh et al., [2021c](#bib.bib48)).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这个优化问题是一个半正定规划问题，可以使用内点法（Ghojogh 等人，[2021c](#bib.bib48)）迭代求解。
- en: This problem uses triplets of similar and dissimilar points, i.e., $\{\boldsymbol{x}_{i},\boldsymbol{x}_{j},\boldsymbol{x}_{l}\}$
    where $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$, $\eta_{ij}=1$,
    $(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D}$. Hence, triplets should
    be extracted randomly from the dataset for this metric learning. Solving semidefinite
    programming is usually slow and time-consuming especially for large datasets.
    Triplet minimizing can be used for finding the best triplets for learning (Poorheravi
    et al., [2020](#bib.bib92)). For example, the similar and dissimilar points with
    smallest and/or largest distances can be used to limit the number of triplets
    (Sikaroudi et al., [2020a](#bib.bib102)). The reader can also refer to for Lipschitz
    analysis in large margin metric learning (Dong, [2019](#bib.bib29)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题使用了相似和不相似点的三元组，即 $\{\boldsymbol{x}_{i},\boldsymbol{x}_{j},\boldsymbol{x}_{l}\}$，其中
    $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$，$\eta_{ij}=1$，$(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D}$。因此，三元组应从数据集中随机提取以进行此度量学习。解决半正定规划通常较慢且耗时，特别是对于大型数据集。可以使用三元组最小化来寻找最佳三元组进行学习（Poorheravi
    等人，[2020](#bib.bib92)）。例如，可以使用具有最小和/或最大距离的相似和不相似点来限制三元组的数量（Sikaroudi 等人，[2020a](#bib.bib102)）。读者还可以参考大边距度量学习中的
    Lipschitz 分析（Dong，[2019](#bib.bib29)）。
- en: 3.2.2 Imbalanced Metric Learning (IML)
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 不平衡度量学习（IML）
- en: 'Imbalanced Metric Learning (IML) (Gautheron et al., [2019](#bib.bib34)) is
    a spectral metric learning method which handles imbalanced classes by further
    decomposition of the similar set $\mathcal{S}$ and dissimilar set $\mathcal{D}$.
    Suppose the dataset is composed of two classes $c_{0}$ and $c_{1}$. Let $\mathcal{S}_{0}$
    and $\mathcal{S}_{1}$ denote the similarity sets for classes $c_{0}$ and $c_{1}$,
    respectively. We define pairs of points taken randomly from these sets to have
    similarity and dissimilarity sets (Gautheron et al., [2019](#bib.bib34)):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡度量学习（IML）（Gautheron 等人，[2019](#bib.bib34)）是一种光谱度量学习方法，它通过进一步分解相似集 $\mathcal{S}$
    和不相似集 $\mathcal{D}$ 来处理不平衡类。假设数据集由两个类别 $c_{0}$ 和 $c_{1}$ 组成。令 $\mathcal{S}_{0}$
    和 $\mathcal{S}_{1}$ 分别表示类别 $c_{0}$ 和 $c_{1}$ 的相似性集合。我们定义从这些集合中随机选取的点对具有相似性和不相似性集合（Gautheron
    等人，[2019](#bib.bib34)）：
- en: '|  | $\displaystyle\text{Sim}_{0}\subseteq\mathcal{S}_{0}\times\mathcal{S}_{0},\quad\text{Sim}_{1}\subseteq\mathcal{S}_{1}\times\mathcal{S}_{1},$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Sim}_{0}\subseteq\mathcal{S}_{0}\times\mathcal{S}_{0},\quad\text{Sim}_{1}\subseteq\mathcal{S}_{1}\times\mathcal{S}_{1},$
    |  |'
- en: '|  | $\displaystyle\text{Dis}_{0}\subseteq\mathcal{S}_{0}\times\mathcal{S}_{1},\quad\text{Dis}_{1}\subseteq\mathcal{S}_{1}\times\mathcal{S}_{0}.$
    |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Dis}_{0}\subseteq\mathcal{S}_{0}\times\mathcal{S}_{1},\quad\text{Dis}_{1}\subseteq\mathcal{S}_{1}\times\mathcal{S}_{0}.$
    |  |'
- en: 'The optimization problem of IML is:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: IML 的优化问题是：
- en: '|  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}~{}~{}~{}~{}~{}~{}\frac{\lambda}{4&#124;\text{Sim}_{0}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Sim}_{0}}\big{[}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-1\big{]}_{+}$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}~{}~{}~{}~{}~{}~{}\frac{\lambda}{4\|\text{Sim}_{0}\|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Sim}_{0}}\big{[}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}-1\big{]}_{+}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{\lambda}{4&#124;\text{Sim}_{1}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Sim}_{1}}\big{[}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-1\big{]}_{+}$
    |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{\lambda}{4\|\text{Sim}_{1}\|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Sim}_{1}}\big{[}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}-1\big{]}_{+}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{1-\lambda}{4&#124;\text{Dis}_{0}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Dis}_{0}}\big{[}\!-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}+1+m\big{]}_{+}$
    |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{1-\lambda}{4\|\text{Dis}_{0}\|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Dis}_{0}}\big{[}\!-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}+1+m\big{]}_{+}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{1-\lambda}{4&#124;\text{Dis}_{1}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Dis}_{1}}\big{[}\!-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}+1+m\big{]}_{+}$
    |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\frac{1-\lambda}{4\|\text{Dis}_{1}\|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\text{Dis}_{1}}\big{[}\!-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}+1+m\big{]}_{+}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\gamma\&#124;\boldsymbol{W}-\boldsymbol{I}\&#124;_{F}^{2}$
    |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\gamma\|\boldsymbol{W}-\boldsymbol{I}\|_{F}^{2}$
    |  |'
- en: '|  | $\displaystyle\text{subject to}~{}~{}~{}~{}\boldsymbol{W}\succeq\boldsymbol{0},$
    |  | (37) |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{subject to}~{}~{}~{}~{}\boldsymbol{W}\succeq\boldsymbol{0},$
    |  | (37) |'
- en: where $|.|$ denotes the cardinality of set, $[.]_{+}:=\max(.,0)$ is the standard
    Hinge loss, $m>0$ is the desired margin between classes, and $\lambda\in[0,1]$
    and $\gamma>0$ are the regularization parameters. This optimization pulls the
    similar points to have distance less than $1$ and pushes the dissimilar points
    away to have distance more than $m+1$. Also, the regularization term $\|\boldsymbol{W}-\boldsymbol{I}\|_{F}^{2}$
    tries to make the weight matrix is the generalized Mahalanobis distance close
    to identity for simplicity of metric. In this way, the metric becomes close to
    the Euclidean distance, preventing overfitting, while satisfying the desired margins
    in distances.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|.|$ 表示集合的基数，$[.]_{+}:=\max(.,0)$ 是标准的 Hinge 损失，$m>0$ 是期望的类别间边界，$\lambda\in[0,1]$
    和 $\gamma>0$ 是正则化参数。该优化目标将相似的点拉近到小于 $1$ 的距离，并将不相似的点推远到大于 $m+1$ 的距离。此外，正则化项 $\|\boldsymbol{W}-\boldsymbol{I}\|_{F}^{2}$
    试图使权重矩阵使广义马氏距离接近单位矩阵，以简化度量。通过这种方式，度量变得接近欧几里得距离，从而防止过拟合，同时满足期望的距离边界。
- en: 3.3 Locally Linear Metric Adaptation (LLMA)
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 局部线性度量适应（LLMA）
- en: 'Another method for metric learning is Locally Linear Metric Adaptation (LLMA)
    (Chang & Yeung, [2004](#bib.bib16)). LLMA performs nonlinear and linear transformations
    globally and locally, respectively. For every point $\boldsymbol{x}_{l}$, we consider
    its $k$ nearest (similar) neighbors. The local linear transformation for every
    point $\boldsymbol{x}_{l}$ is:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种度量学习方法是局部线性度量适应（LLMA）（Chang & Yeung, [2004](#bib.bib16)）。LLMA 分别在全局和局部执行非线性和线性变换。对于每个点
    $\boldsymbol{x}_{l}$，我们考虑其 $k$ 个最近的（相似的）邻居。每个点 $\boldsymbol{x}_{l}$ 的局部线性变换为：
- en: '|  | $\displaystyle\mathbb{R}^{d}\ni\boldsymbol{y}_{l}:=\boldsymbol{x}_{l}+\boldsymbol{B}\boldsymbol{\pi}_{i},$
    |  | (38) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{R}^{d}\ni\boldsymbol{y}_{l}:=\boldsymbol{x}_{l}+\boldsymbol{B}\boldsymbol{\pi}_{i},$
    |  | (38) |'
- en: where $\boldsymbol{B}\in\mathbb{R}^{d\times k}$ is the matrix of biases, $\mathbb{R}^{k}\ni\boldsymbol{\pi}_{i}=[\pi_{i1},\dots,\pi_{ik}]^{\top}$,
    and $\pi_{ij}:=\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}^{2}/2w^{2})$
    is a Gaussian measure of similarity between $\boldsymbol{x}_{i}$ and $\boldsymbol{x}_{j}$.
    The variables $\boldsymbol{B}$ and $w$ are found by optimization.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{B}\in\mathbb{R}^{d\times k}$ 是偏置矩阵，$\mathbb{R}^{k}\ni\boldsymbol{\pi}_{i}=[\pi_{i1},\dots,\pi_{ik}]^{\top}$，而
    $\pi_{ij}:=\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}^{2}/2w^{2})$ 是
    $\boldsymbol{x}_{i}$ 和 $\boldsymbol{x}_{j}$ 之间相似性的高斯度量。变量 $\boldsymbol{B}$ 和 $w$
    通过优化得到。
- en: 'In this method, we minimize the distances between the linearly transformed
    similar points while the distances of similar points are tried to be preserved
    after the transformation:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方法中，我们最小化线性变换后的相似点之间的距离，同时尽量保持相似点在变换后的距离：
- en: '|  |  | $\displaystyle\underset{\{\boldsymbol{y}_{i}\}_{i=1}^{n},\boldsymbol{B},w,\sigma}{\text{minimize}}$
    |  | $\displaystyle\sum_{(\boldsymbol{y}_{i},\boldsymbol{y}_{j})\in\mathcal{S}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{y}_{j}\&#124;_{2}^{2}$
    |  | (39) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\{\boldsymbol{y}_{i}\}_{i=1}^{n},\boldsymbol{B},w,\sigma}{\text{minimize}}$
    |  | $\displaystyle\sum_{(\boldsymbol{y}_{i},\boldsymbol{y}_{j})\in\mathcal{S}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{y}_{j}\&#124;_{2}^{2}$
    |  | (39) |'
- en: '|  |  | $\displaystyle+\lambda\,\sum_{i=1}^{n}\sum_{j=1}^{n}(q_{ij}-d_{ij})^{2}\exp(\frac{-d_{ij}^{2}}{\sigma^{2}}),$
    |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda\,\sum_{i=1}^{n}\sum_{j=1}^{n}(q_{ij}-d_{ij})^{2}\exp(\frac{-d_{ij}^{2}}{\sigma^{2}}),$
    |  |'
- en: where $\lambda>0$ is the regularization parameter, $\sigma_{2}^{2}$ is the variance
    to be optimized, and $d_{ij}:=\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}$ and
    $q_{ij}:=\|\boldsymbol{y}_{i}-\boldsymbol{y}_{j}\|_{2}$. This objective function
    is optimized iteratively until convergence.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 是正则化参数，$\sigma_{2}^{2}$ 是待优化的方差，$d_{ij}:=\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}$
    和 $q_{ij}:=\|\boldsymbol{y}_{i}-\boldsymbol{y}_{j}\|_{2}$。这个目标函数通过迭代优化，直到收敛。
- en: 3.4 Relevant to Support Vector Machine
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 与支持向量机相关
- en: 'Inspired by $\nu$-Support Vector Machine ($\nu$-SVM) (Schölkopf et al., [2000](#bib.bib99)),
    the weight matrix in the generalized Mahalanobis distance can be obtained as (Tsang
    et al., [2003](#bib.bib112)):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 $\nu$-支持向量机 ($\nu$-SVM) (Schölkopf et al., [2000](#bib.bib99)) 的启发，广义马氏距离中的权重矩阵可以通过
    (Tsang et al., [2003](#bib.bib112)) 获得：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W},\gamma,\{\xi_{il}\}}{\text{minimize}}$
    |  | $\displaystyle\frac{1}{2}\&#124;\boldsymbol{W}\&#124;_{2}^{2}+\frac{\lambda_{1}}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (40) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W},\gamma,\{\xi_{il}\}}{\text{minimize}}$
    |  | $\displaystyle\frac{1}{2}\&#124;\boldsymbol{W}\&#124;_{2}^{2}+\frac{\lambda_{1}}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (40) |'
- en: '|  |  | $\displaystyle+\lambda_{2}\Big{(}\nu\gamma+\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D}}\xi_{il}\Big{)}$
    |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda_{2}\Big{(}\nu\gamma+\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D}}\xi_{il}\Big{)}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: '|  |  | $\displaystyle\gamma\geq 0,$ |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\gamma\geq 0,$ |  |'
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\geq\gamma-\xi_{il},$
    |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\&#124;_{\boldsymbol{W}}^{2}\geq\gamma-\xi_{il},$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
- en: '|  |  | $\displaystyle\xi_{il}\geq 0,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\xi_{il}\geq 0,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{l})\in\mathcal{D},$
    |  |'
- en: 'where $\lambda_{1},\lambda_{2}>0$ are regularization parameters. Using KKT
    conditions and Lagrange multipliers (Ghojogh et al., [2021c](#bib.bib48)), the
    dual optimization problem is (see (Tsang et al., [2003](#bib.bib112)) for derivation):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1},\lambda_{2}>0$ 是正则化参数。使用 KKT 条件和拉格朗日乘数 (Ghojogh et al., [2021c](#bib.bib48))，对偶优化问题为
    (详见 (Tsang et al., [2003](#bib.bib112)) 的推导)：
- en: '|  |  | $\displaystyle\underset{\{\alpha_{ij}\}}{\text{maximize}}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  | (41) |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\{\alpha_{ij}\}}{\text{最大化}}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  | (41) |'
- en: '|  |  | $\displaystyle-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{D}}\alpha_{ij}\alpha_{kl}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{k}-\boldsymbol{x}_{l}))^{2}$
    |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{D}}\alpha_{ij}\alpha_{kl}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{k}-\boldsymbol{x}_{l}))^{2}$
    |  |'
- en: '|  |  | $\displaystyle+\frac{\lambda_{1}}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{S}}\alpha_{ij}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{k}-\boldsymbol{x}_{l}))^{2}$
    |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\frac{\lambda_{1}}{| \mathcal{S} |}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{S}}\alpha_{ij}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}(\boldsymbol{x}_{k}-\boldsymbol{x}_{l}))^{2}$
    |  |'
- en: '|  |  | $\displaystyle\text{subject to}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\frac{1}{\lambda_{2}}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\geq\nu,$
    |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{满足}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\frac{1}{\lambda_{2}}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\geq\nu,$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\alpha_{ij}\in[0,\frac{\lambda_{2}}{&#124;\mathcal{D}&#124;}],$
    |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\alpha_{ij}\in[0,\frac{\lambda_{2}}{|
    \mathcal{D} |}],$ |  |'
- en: where $\{\alpha_{ij}\}$ are the dual variables. This problem is a quadratic
    programming problem and can be solved using optimization solvers.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\{\alpha_{ij}\}$是对偶变量。这个问题是一个二次规划问题，可以通过优化求解器解决。
- en: 3.5 Relevant to Multidimensional Scaling
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 与多维尺度分析相关
- en: 'Multidimensional Scaling (MDS) tries to preserve the distance after projection
    onto its subspace (Cox & Cox, [2008](#bib.bib21); Ghojogh et al., [2020b](#bib.bib41)).
    We saw in Proposition [2](#Thmproposition2 "Proposition 2 (Projection in metric
    learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") that metric learning can be seen as projection onto the column space
    of $\boldsymbol{U}$ where $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$.
    Inspired by MDS, we can learn a metric which preserves the distances between points
    after projection onto the subspace of metric (Zhang et al., [2003](#bib.bib144)):'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '多维尺度分析（MDS）试图在投影到其子空间后保持距离（Cox & Cox, [2008](#bib.bib21)；Ghojogh 等人, [2020b](#bib.bib41)）。我们在命题
    [2](#Thmproposition2 "Proposition 2 (Projection in metric learning). ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey") 中看到，度量学习可以看作是投影到$\boldsymbol{U}$的列空间，其中$\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$。受MDS启发，我们可以学习一种度量，该度量在投影到度量的子空间后保持点之间的距离（Zhang
    等人, [2003](#bib.bib144)）：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{2}^{2}-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})^{2}$
    |  | (42) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}(|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}|_{2}^{2}-|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}|_{\boldsymbol{W}}^{2})^{2}$
    |  | (42) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 满足 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: It can be solved using any optimization method (Ghojogh et al., [2021c](#bib.bib48)).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用任何优化方法解决（Ghojogh 等人, [2021c](#bib.bib48)）。
- en: 3.6 Kernel Spectral Metric Learning
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 核谱度量学习
- en: Let $k(\boldsymbol{x}_{i},\boldsymbol{x}_{j}):=\boldsymbol{\phi}(\boldsymbol{x}_{i})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})$
    be the kernel function over data points $\boldsymbol{x}_{i}$ and $\boldsymbol{x}_{j}$,
    where $\boldsymbol{\phi}(.)$ is the pulling function to the Reproducing Kernel
    Hilbert Space (RKHS) (Ghojogh et al., [2021e](#bib.bib50)). Let $\mathbb{R}^{n\times
    n}\ni\boldsymbol{K}:=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})$
    be the kernel matrix of data. In the following, we introduce some of the kernel
    spectral metric learning methods.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $k(\boldsymbol{x}_{i},\boldsymbol{x}_{j}):=\boldsymbol{\phi}(\boldsymbol{x}_{i})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})$
    为数据点 $\boldsymbol{x}_{i}$ 和 $\boldsymbol{x}_{j}$ 上的核函数，其中 $\boldsymbol{\phi}(.)$
    是映射到再生核希尔伯特空间（RKHS）的拉伸函数（Ghojogh 等，[2021e](#bib.bib50)）。设 $\mathbb{R}^{n\times
    n}\ni\boldsymbol{K}:=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})$
    为数据的核矩阵。接下来，我们介绍一些核谱度量学习方法。
- en: 3.6.1 Using Eigenvalue Decomposition of Kernel
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1 使用核的特征值分解
- en: 'One of the kernel methods for spectral metric learning is (Yeung & Chang, [2007](#bib.bib140)).
    It has two approaches; we explain one of its approaches here. The eigenvalue decomposition
    of the kernel matrix is:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于谱度量学习的核方法是（Yeung & Chang，[2007](#bib.bib140)）。它有两种方法；我们在这里解释其中一种方法。核矩阵的特征值分解是：
- en: '|  | $\displaystyle\boldsymbol{K}=\sum_{r=1}^{p}\beta_{r}^{2}\boldsymbol{\alpha}_{r}\boldsymbol{\alpha}_{r}^{\top}\overset{(a)}{=}\sum_{r=1}^{p}\beta_{r}^{2}\boldsymbol{K}_{r}$
    |  | (43) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{K}=\sum_{r=1}^{p}\beta_{r}^{2}\boldsymbol{\alpha}_{r}\boldsymbol{\alpha}_{r}^{\top}\overset{(a)}{=}\sum_{r=1}^{p}\beta_{r}^{2}\boldsymbol{K}_{r}$
    |  | (43) |'
- en: 'where $p$ is the rank of kernel matrix, $\beta_{r}^{2}$ is the non-negative
    $r$-th eigenvalue (because $\boldsymbol{K}\succeq\boldsymbol{0}$), $\boldsymbol{\alpha}_{r}\in\mathbb{R}^{n}$
    is the $r$-th eigenvector, and $(a)$ is because we define $\boldsymbol{K}_{r}:=\boldsymbol{\alpha}_{r}\boldsymbol{\alpha}_{r}^{\top}$.
    We can consider $\{\beta_{r}^{2}\}_{r=1}^{p}$ as learnable parameters and not
    the eigenvalues. Hence, we learn $\{\beta_{r}^{2}\}_{r=1}^{p}$ for the sake of
    metric learning. The distance metric of pulled data points to RKHS is (Schölkopf,
    [2001](#bib.bib98); Ghojogh et al., [2021e](#bib.bib50)):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p$ 是核矩阵的秩，$\beta_{r}^{2}$ 是非负的 $r$-th 特征值（因为 $\boldsymbol{K}\succeq\boldsymbol{0}$），$\boldsymbol{\alpha}_{r}\in\mathbb{R}^{n}$
    是第 $r$ 个特征向量，$(a)$ 是因为我们定义了 $\boldsymbol{K}_{r}:=\boldsymbol{\alpha}_{r}\boldsymbol{\alpha}_{r}^{\top}$。我们可以将
    $\{\beta_{r}^{2}\}_{r=1}^{p}$ 视为可学习的参数，而不是特征值。因此，我们学习 $\{\beta_{r}^{2}\}_{r=1}^{p}$
    以进行度量学习。映射到 RKHS 的拉伸数据点的距离度量是（Schölkopf，[2001](#bib.bib98)；Ghojogh 等，[2021e](#bib.bib50)）：
- en: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-$ | $\displaystyle\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  | (44) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-$ | $\displaystyle\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  | (44) |'
- en: '|  |  | $\displaystyle=k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j}).$
    |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j}).$
    |  |'
- en: 'In metric learning, we want to make the distances of similar points small;
    hence the objective to be minimized is: Hence, we have:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在度量学习中，我们希望使相似点的距离变小；因此，需要最小化的目标是：因此，我们得到：
- en: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
- en: '|  | $\displaystyle=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
- en: '|  | $\displaystyle\overset{(\ref{equation_kernel_eigenvalue_decomposition})}{=}\sum_{r=1}^{p}\beta_{r}^{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}k_{r}(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k_{r}(\boldsymbol{x}_{j},\boldsymbol{x}_{j})$
    |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(\ref{equation_kernel_eigenvalue_decomposition})}{=}\sum_{r=1}^{p}\beta_{r}^{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}k_{r}(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k_{r}(\boldsymbol{x}_{j},\boldsymbol{x}_{j})$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-2k_{r}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-2k_{r}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
- en: '|  | $\displaystyle\overset{(a)}{=}\sum_{r=1}^{p}\beta_{r}^{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{r}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})$
    |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(a)}{=}\sum_{r=1}^{p}\beta_{r}^{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{r}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})$
    |  |'
- en: '|  | $\displaystyle\overset{(b)}{=}\sum_{r=1}^{p}\beta_{r}^{2}f_{r}\overset{(c)}{=}\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta},$
    |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(b)}{=}\sum_{r=1}^{p}\beta_{r}^{2}f_{r}\overset{(c)}{=}\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta},$
    |  |'
- en: 'where $(a)$ is because $\boldsymbol{e}_{i}$ is the vector whose $i$-th element
    is one and other elements are zero, $(b)$ is because we define $f_{r}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{r}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})$,
    and $(c)$ is because we define $\boldsymbol{D}_{\mathcal{S}}:=\textbf{diag}([f_{1},\dots,f_{p}]^{\top})$
    and $\boldsymbol{\beta}:=[\beta_{1},\dots,\beta_{p}]^{\top}$. By adding a constraint
    on the summation of $\{\beta_{r}^{2}\}_{r=1}^{p}$, the optimization problem for
    metric learning is:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是因为 $\boldsymbol{e}_{i}$ 是一个第 $i$ 个元素为1，其它元素为0的向量，$(b)$ 是因为我们定义 $f_{r}:=\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{r}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})$，$(c)$
    是因为我们定义 $\boldsymbol{D}_{\mathcal{S}}:=\textbf{diag}([f_{1},\dots,f_{p}]^{\top})$
    和 $\boldsymbol{\beta}:=[\beta_{1},\dots,\beta_{p}]^{\top}$。通过对 $\{\beta_{r}^{2}\}_{r=1}^{p}$
    的和添加约束，度量学习的优化问题是：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{\beta}}{\text{minimize}}$ |  |
    $\displaystyle\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}$
    |  | (45) |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{\beta}}{\text{最小化}}$ |  | $\displaystyle\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}$
    |  | (45) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{1}^{\top}\boldsymbol{\beta}=1.$
    |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 满足 |  | $\displaystyle\boldsymbol{1}^{\top}\boldsymbol{\beta}=1.$ |  |'
- en: 'This optimization is similar to the form of one of the optimization problems
    in locally linear embedding (Roweis & Saul, [2000](#bib.bib97); Ghojogh et al.,
    [2020a](#bib.bib40)). The Lagrangian for this problem is (Ghojogh et al., [2021c](#bib.bib48)):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 该优化问题类似于局部线性嵌入中的一个优化问题（Roweis & Saul, [2000](#bib.bib97); Ghojogh et al., [2020a](#bib.bib40)）。此问题的拉格朗日函数是（Ghojogh
    et al., [2021c](#bib.bib48)）：
- en: '|  | $\displaystyle\mathcal{L}=\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}-\lambda(\boldsymbol{1}^{\top}\boldsymbol{\beta}-1),$
    |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=\boldsymbol{\beta}^{\top}\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}-\lambda(\boldsymbol{1}^{\top}\boldsymbol{\beta}-1),$
    |  |'
- en: 'where $\lambda$ is the dual variable. Taking derivative of the Lagrangian w.r.t.
    the variables and setting to zero gives:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是对偶变量。对拉格朗日函数关于变量求导并设为零得到：
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{\beta}}=2\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}-\lambda\boldsymbol{1}\overset{\text{set}}{=}0\implies\boldsymbol{\beta}=\frac{\lambda}{2}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1},$
    |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\boldsymbol{\beta}}=2\boldsymbol{D}_{\mathcal{S}}\boldsymbol{\beta}-\lambda\boldsymbol{1}\overset{\text{设定}}{=}0\implies\boldsymbol{\beta}=\frac{\lambda}{2}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1},$
    |  |'
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\lambda}=\boldsymbol{1}^{\top}\boldsymbol{\beta}-1\overset{\text{set}}{=}0\implies\boldsymbol{1}^{\top}\boldsymbol{\beta}=1,$
    |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial\lambda}=\boldsymbol{1}^{\top}\boldsymbol{\beta}-1\overset{\text{设定}}{=}0\implies\boldsymbol{1}^{\top}\boldsymbol{\beta}=1,$
    |  |'
- en: '|  | $\displaystyle\implies\frac{\lambda}{2}\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}=1\implies\lambda=\frac{2}{\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}$
    |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\frac{\lambda}{2}\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}=1\implies\lambda=\frac{2}{\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}$
    |  |'
- en: '|  | $\displaystyle\implies\boldsymbol{\beta}=\frac{\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}{\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}.$
    |  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\boldsymbol{\beta}=\frac{\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}{\boldsymbol{1}^{\top}\boldsymbol{D}_{\mathcal{S}}^{-1}\boldsymbol{1}}.$
    |  |'
- en: Hence, the optimal $\boldsymbol{\beta}$ is obtained for metric learning in the
    RKHS where the distances of similar points is smaller than in the input Euclidean
    space.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最佳的$\boldsymbol{\beta}$是在RKHS中的度量学习得到的，其中相似点的距离比输入的欧几里得空间中的距离要小。
- en: 3.6.2 Regularization by Locally Linear Embedding
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2 通过局部线性嵌入的正则化
- en: 'The method (Baghshah & Shouraki, [2009](#bib.bib4)), which was introduced in
    Section [3.1.7](#S3.SS1.SSS7 "3.1.7 Regularization by Locally Linear Embedding
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey"), can be kernelized.
    Recall that this method used locally linear embedding for regularization. According
    to the representation theory (Ghojogh et al., [2021e](#bib.bib50)), the solution
    in the RKHS can be represented as a linear combination of all pulled data points
    to RKHS:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 方法（Baghshah & Shouraki，[2009](#bib.bib4)），如第[3.1.7节](#S3.SS1.SSS7 "3.1.7 通过局部线性嵌入的正则化
    ‣ 3.1 基于散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查")中介绍的，可以进行核化。回顾一下，这种方法使用了局部线性嵌入进行正则化。根据表示理论（Ghojogh
    et al.，[2021e](#bib.bib50)），RKHS中的解可以表示为所有拉入RKHS的数据点的线性组合：
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T},$
    |  | (46) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T},$
    |  | (46) |'
- en: where $\boldsymbol{X}=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]$ and $\boldsymbol{T}\in\mathbb{R}^{n\times
    p}$ ($p$ is the dimensionality of subspace) is the coefficients.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{X}=[\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}]$ 和 $\boldsymbol{T}\in\mathbb{R}^{n\times
    p}$（$p$ 是子空间的维度）是系数。
- en: 'We define the similarity and dissimilarity adjacency matrices as:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将相似性和不相似性邻接矩阵定义为：
- en: '|  |  | $\displaystyle\boldsymbol{A}_{S}(i,j):=\left\{\begin{array}[]{ll}1&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},\\ 0&amp;\mbox{otherwise.}\end{array}\right.$
    |  | (47) |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{A}_{S}(i,j):=\left\{\begin{array}[]{ll}1&\mbox{如果
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},\\ 0&\mbox{否则。}\end{array}\right.$
    |  | (47) |'
- en: '|  |  | $\displaystyle\boldsymbol{A}_{D}(i,j):=\left\{\begin{array}[]{ll}1&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},\\ 0&amp;\mbox{otherwise.}\end{array}\right.$
    |  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{A}_{D}(i,j):=\left\{\begin{array}[]{ll}1&\mbox{如果
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},\\ 0&\mbox{否则。}\end{array}\right.$
    |  |'
- en: 'Let $\boldsymbol{L}_{w}$ and $\boldsymbol{L}_{b}$ denote the Laplacian matrices
    (Ghojogh et al., [2021d](#bib.bib49)) of these adjacency matrices:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $\boldsymbol{L}_{w}$ 和 $\boldsymbol{L}_{b}$ 表示这些邻接矩阵的拉普拉斯矩阵（Ghojogh et al.，[2021d](#bib.bib49)）：
- en: '|  | $\displaystyle\boldsymbol{L}_{w}:=\boldsymbol{D}_{S}-\boldsymbol{A}_{S}(i,j),\quad\boldsymbol{L}_{b}:=\boldsymbol{D}_{D}-\boldsymbol{A}_{D}(i,j),$
    |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{L}_{w}:=\boldsymbol{D}_{S}-\boldsymbol{A}_{S}(i,j),\quad\boldsymbol{L}_{b}:=\boldsymbol{D}_{D}-\boldsymbol{A}_{D}(i,j),$
    |  |'
- en: 'where $\boldsymbol{D}_{S}(i,i):=\sum_{j=1}^{n}\boldsymbol{A}_{S}(i,j)$ and
    $\boldsymbol{D}_{D}(i,i):=\sum_{j=1}^{n}\boldsymbol{A}_{D}(i,j)$ are diagonal
    matrices. The terms in the objective of Eq. ([32](#S3.E32 "In 3.1.7 Regularization
    by Locally Linear Embedding ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) can be restated using Laplacian of adjacency matrices rather than
    the scatters:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{D}_{S}(i,i):=\sum_{j=1}^{n}\boldsymbol{A}_{S}(i,j)$ 和 $\boldsymbol{D}_{D}(i,i):=\sum_{j=1}^{n}\boldsymbol{A}_{D}(i,j)$
    是对角矩阵。目标函数中的项（第[32](#S3.E32 "在3.1.7节 通过局部线性嵌入的正则化 ‣ 3.1 基于散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查")节）可以使用邻接矩阵的拉普拉斯算子而不是散点来重新表述：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{L}_{b}\boldsymbol{U})}{\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{L}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})\boldsymbol{U}\big{)}},$
    |  | (48) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{L}_{b}\boldsymbol{U})}{\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{L}_{w}+\lambda\boldsymbol{X}\boldsymbol{E}\boldsymbol{X}^{\top})\boldsymbol{U}\big{)}},$
    |  | (48) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 满足 |  | $\displaystyle\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}.$
    |  |'
- en: 'According to the representation theory, the pulled Laplacian matrices to RKHS
    are $\boldsymbol{\Phi}(\boldsymbol{L}_{b})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$
    and $\boldsymbol{\Phi}(\boldsymbol{L}_{w})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$.
    Hence, the numerator of Eq. ([32](#S3.E32 "In 3.1.7 Regularization by Locally
    Linear Embedding ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) in
    RKHS becomes:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表示理论，拉普拉斯矩阵在 RKHS 中的映射为 $\boldsymbol{\Phi}(\boldsymbol{L}_{b})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$
    和 $\boldsymbol{\Phi}(\boldsymbol{L}_{w})=\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$。因此，方程
    ([32](#S3.E32 "在 3.1.7 小范围线性嵌入正则化 ‣ 3.1 基于散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度度量学习：教程与调研"))
    中 RKHS 的分子变为：
- en: '|  | $\displaystyle\textbf{tr}(\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{U}))$
    |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{tr}(\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{U}))$
    |  |'
- en: '|  | $\displaystyle=\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\big{)}$
    |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{b}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\big{)}$
    |  |'
- en: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{L}_{b}\boldsymbol{K}_{x}\boldsymbol{T}\big{)},$
    |  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{L}_{b}\boldsymbol{K}_{x}\boldsymbol{T}\big{)},$
    |  |'
- en: where $(a)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)),
    i.e.,
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于核技巧（Ghojogh 等，[2021e](#bib.bib50)），即，
- en: '|  | $\displaystyle\boldsymbol{K}_{x}:=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}).$
    |  | (49) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{K}_{x}:=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}).$
    |  | (49) |'
- en: 'similarly, the denominator of Eq. ([32](#S3.E32 "In 3.1.7 Regularization by
    Locally Linear Embedding ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    in RKHS becomes:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，方程 ([32](#S3.E32 "在 3.1.7 小范围线性嵌入正则化 ‣ 3.1 基于散点的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度度量学习：教程与调研"))
    中 RKHS 的分母变为：
- en: '|  | $\displaystyle\textbf{tr}\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}+\lambda\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{E}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})\boldsymbol{\Phi}(\boldsymbol{U})\big{)}$
    |  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{tr}\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}+\lambda\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{E}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})\boldsymbol{\Phi}(\boldsymbol{U})\big{)}$
    |  |'
- en: '|  | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$
    |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}_{w}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{E}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\big{)}$
    |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{E}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\big{)}$
    |  |'
- en: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}(\boldsymbol{L}_{w}+\lambda\boldsymbol{E})\boldsymbol{K}_{x}\boldsymbol{T}\big{)},$
    |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}(\boldsymbol{L}_{w}+\lambda\boldsymbol{E})\boldsymbol{K}_{x}\boldsymbol{T}\big{)},$
    |  |'
- en: 'where $(a)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)).
    The constrain in RKHS becomes:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于核技巧（Ghojogh 等，[2021e](#bib.bib50)）。RKHS 中的约束变为：
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{U})\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\overset{(a)}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T},$
    |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{U})\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}\overset{(a)}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T},$
    |  |'
- en: 'where $(a)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)).
    The Eq. ([32](#S3.E32 "In 3.1.7 Regularization by Locally Linear Embedding ‣ 3.1
    Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) in RKHS is:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于核技巧 (Ghojogh et al., [2021e](#bib.bib50))。RKHS 中的方程 ([32](#S3.E32
    "在 3.1.7 拉普拉斯正则化 ‣ 3.1 谱方法使用散布 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程和调查")) 为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{L}_{b}\boldsymbol{K}_{x}\boldsymbol{T}\big{)}}{\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}(\boldsymbol{L}_{w}+\lambda\boldsymbol{E})\boldsymbol{K}_{x}\boldsymbol{T}\big{)}},$
    |  | (50) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{最大化}}$ |  | $\displaystyle\frac{\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{L}_{b}\boldsymbol{K}_{x}\boldsymbol{T}\big{)}}{\textbf{tr}\big{(}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}(\boldsymbol{L}_{w}+\lambda\boldsymbol{E})\boldsymbol{K}_{x}\boldsymbol{T}\big{)}},$
    |  | (50) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}=\boldsymbol{I}.$
    |  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}=\boldsymbol{I}.$
    |  |'
- en: 'It can be solved using projected gradient method (Ghojogh et al., [2021c](#bib.bib48))
    to find the optimal $\boldsymbol{T}$. Then, the projected data onto the subspace
    of metric is found as:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用投影梯度法 (Ghojogh et al., [2021c](#bib.bib48)) 来找到最优的 $\boldsymbol{T}$。然后，投影到度量子空间的数据被表示为：
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\overset{(a)}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x},$
    |  | (51) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\overset{(a)}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x},$
    |  | (51) |'
- en: where $(a)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于核技巧 (Ghojogh et al., [2021e](#bib.bib50))。
- en: 3.6.3 Regularization by Laplacian
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.3 拉普拉斯正则化
- en: 'Another kernel spectral metric learning method is (Baghshah & Shouraki, [2010](#bib.bib5))
    whose optimization is in the form:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种核谱度量学习方法是 (Baghshah & Shouraki, [2010](#bib.bib5))，其优化形式为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{\Phi}(\boldsymbol{X})}{\text{minimize}}$
    |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}+\lambda\Omega(\boldsymbol{\Phi}(\boldsymbol{X})),$
    |  | (52) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{\Phi}(\boldsymbol{X})}{\text{最小化}}$
    |  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}+\lambda\Omega(\boldsymbol{\Phi}(\boldsymbol{X})),$
    |  | (52) |'
- en: '|  |  | subject to |  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}\geq
    c,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},$ |  |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}\geq
    c,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},$ |  |'
- en: where $c>0$ is a hyperparameter and $\lambda>0$ is the regularization parameter.
    Consider the $k$NN graph of data with an adjacency matrix $\boldsymbol{A}\in\mathbb{R}^{n\times
    n}$ whose $(i,j)$-th element is one if $\boldsymbol{x}_{i}$ and $\boldsymbol{x}_{j}$
    are neighbors and is zero otherwise. Let the Laplacian matrix of this adjacency
    matrix be denoted by $\boldsymbol{L}$.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c>0$ 是一个超参数，$\lambda>0$ 是正则化参数。考虑数据的 $k$NN 图，其邻接矩阵为 $\boldsymbol{A}\in\mathbb{R}^{n\times
    n}$，其中 $(i,j)$ 元素为1如果 $\boldsymbol{x}_{i}$ 和 $\boldsymbol{x}_{j}$ 是邻居，否则为0。设该邻接矩阵的拉普拉斯矩阵记为
    $\boldsymbol{L}$。
- en: 'In this method, the regularization term $\Omega(\boldsymbol{\Phi}(\boldsymbol{X}))$
    can be the objective of Laplacian eigenmap (Ghojogh et al., [2021d](#bib.bib49)):'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，正则化项 $\Omega(\boldsymbol{\Phi}(\boldsymbol{X}))$ 可以是拉普拉斯特征映射的目标 (Ghojogh
    et al., [2021d](#bib.bib49))：
- en: '|  | $\displaystyle\Omega(\boldsymbol{\Phi}(\boldsymbol{X})):=$ | $\displaystyle\frac{1}{2n}\sum_{i=1}^{n}\sum_{j=1}^{n}\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}\boldsymbol{A}(i,j)$
    |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Omega(\boldsymbol{\Phi}(\boldsymbol{X})):=$ | $\displaystyle\frac{1}{2n}\sum_{i=1}^{n}\sum_{j=1}^{n}\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}\boldsymbol{A}(i,j)$
    |  |'
- en: '|  |  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})$
    |  |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{L}\boldsymbol{\Phi}(\boldsymbol{X})^{\top})$
    |  |'
- en: '|  |  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{L}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}))\overset{(c)}{=}\textbf{tr}(\boldsymbol{L}\boldsymbol{K}_{x}),$
    |  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{L}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}))\overset{(c)}{=}\textbf{tr}(\boldsymbol{L}\boldsymbol{K}_{x}),$
    |  |'
- en: 'where $(a)$ is according to (Belkin & Niyogi, [2001](#bib.bib8)) (see (Ghojogh
    et al., [2021d](#bib.bib49)) for proof), $(b)$ is because of the cyclic property
    of trace, and $(c)$ is because of the kernel trick (Ghojogh et al., [2021e](#bib.bib50)).
    Moreover, according to Eq. ([44](#S3.E44 "In 3.6.1 Using Eigenvalue Decomposition
    of Kernel ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), the
    distance in RKHS is $\|\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\|_{2}^{2}=k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$.
    We can simplify the term in Eq. ([52](#S3.E52 "In 3.6.3 Regularization by Laplacian
    ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) as:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$(a)$ 根据 (Belkin & Niyogi, [2001](#bib.bib8)) (见 (Ghojogh et al., [2021d](#bib.bib49))
    以获取证明)，$(b)$ 由于迹的循环性质，$(c)$ 由于核技巧 (Ghojogh et al., [2021e](#bib.bib50))。此外，根据公式
    ([44](#S3.E44 "In 3.6.1 Using Eigenvalue Decomposition of Kernel ‣ 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))，RKHS 中的距离是 $\|\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\|_{2}^{2}=k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$。我们可以将公式
    ([52](#S3.E52 "In 3.6.3 Regularization by Laplacian ‣ 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) 中的项简化为：'
- en: '|  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
- en: '|  | $\displaystyle\overset{(\ref{equation_distance_in_RKHS})}{=}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(\ref{equation_distance_in_RKHS})}{=}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    |  |'
- en: '|  | $\displaystyle=\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})\overset{(a)}{=}\textbf{tr}(\boldsymbol{E}_{\mathcal{S}}\boldsymbol{K}_{x}),$
    |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\!\!\!\!\!(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})\overset{(a)}{=}\textbf{tr}(\boldsymbol{E}_{\mathcal{S}}\boldsymbol{K}_{x}),$
    |  |'
- en: where $(a)$ is because the scalar is equal to its trace and we use the cyclic
    property of trace, i.e., $(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})=\textbf{tr}((\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j}))=\textbf{tr}((\boldsymbol{e}_{i}-\boldsymbol{e}_{j})(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x})$,
    and then we define $\boldsymbol{E}_{\mathcal{S}}:=(1/|\mathcal{S}|)\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}$.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是因为标量等于其迹，我们使用迹的循环性质，即 $(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})=\textbf{tr}((\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j}))=\textbf{tr}((\boldsymbol{e}_{i}-\boldsymbol{e}_{j})(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}\boldsymbol{K}_{x})$，然后我们定义
    $\boldsymbol{E}_{\mathcal{S}}:=(1/|\mathcal{S}|)\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})(\boldsymbol{e}_{i}-\boldsymbol{e}_{j})^{\top}$。
- en: 'Hence, Eq. ([52](#S3.E52 "In 3.6.3 Regularization by Laplacian ‣ 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) can be restated as:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，方程 ([52](#S3.E52 "In 3.6.3 Regularization by Laplacian ‣ 3.6 Kernel Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) 可以重新表述为：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{K}_{x}}{\text{minimize}}$ |  |
    $\displaystyle\textbf{tr}(\boldsymbol{E}_{\mathcal{S}}\boldsymbol{K}_{x})+\lambda\,\textbf{tr}(\boldsymbol{L}\boldsymbol{K}_{x}),$
    |  | (53) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{K}_{x}}{\text{minimize}}$ |  |
    $\displaystyle\textbf{tr}(\boldsymbol{E}_{\mathcal{S}}\boldsymbol{K}_{x})+\lambda\,\textbf{tr}(\boldsymbol{L}\boldsymbol{K}_{x}),$
    |  | (53) |'
- en: '|  |  | subject to |  | $\displaystyle k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\geq
    c,$ |  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle k(\boldsymbol{x}_{i},\boldsymbol{x}_{i})+k(\boldsymbol{x}_{j},\boldsymbol{x}_{j})-2k(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\geq
    c,$ |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},$
    |  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},$
    |  |'
- en: '|  |  | $\displaystyle\boldsymbol{K}_{x}\succeq\boldsymbol{0},$ |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{K}_{x}\succeq\boldsymbol{0},$ |  |'
- en: 'noticing that the kernel matrix is positive semidefinite. This problem is a
    Semidefinite Programming (SDP) problem and can be solved using the interior point
    method (Ghojogh et al., [2021c](#bib.bib48)). The optimal kernel matrix can be
    decomposed using eigenvalue decomposition to find the embedding of data in RKHS,
    i.e., $\boldsymbol{\Phi}(\boldsymbol{X})$:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到核矩阵是半正定的。这个问题是一个半正定规划（SDP）问题，可以使用内点法解决（Ghojogh 等，[2021c](#bib.bib48)）。最优核矩阵可以通过特征值分解来求得数据在
    RKHS 中的嵌入，即 $\boldsymbol{\Phi}(\boldsymbol{X})$：
- en: '|  | $\displaystyle\boldsymbol{K}_{x}=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}\boldsymbol{V}=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}^{(1/2}\boldsymbol{\Sigma}^{(1/2)}\boldsymbol{V}\overset{(\ref{equation_Kernel_X})}{=}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}),$
    |  |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{K}_{x}=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}\boldsymbol{V}=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}^{(1/2)}\boldsymbol{\Sigma}^{(1/2)}\boldsymbol{V}\overset{(\ref{equation_Kernel_X})}{=}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X}),$
    |  |'
- en: where $\boldsymbol{V}$ and $\boldsymbol{\Sigma}$ are the eigenvectors and eigenvalues,
    $(a)$ is because $\boldsymbol{K}_{x}\succeq\boldsymbol{0}$ so its eigenvalues
    are non-negative can be taken second root of, and $(b)$ is because we get $\boldsymbol{\Phi}(\boldsymbol{X}):=\boldsymbol{\Sigma}^{(1/2)}\boldsymbol{V}$.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{V}$ 和 $\boldsymbol{\Sigma}$ 分别是特征向量和特征值，$(a)$ 是因为 $\boldsymbol{K}_{x}\succeq\boldsymbol{0}$
    所以其特征值是非负的，可以取平方根，$(b)$ 是因为我们得到 $\boldsymbol{\Phi}(\boldsymbol{X}):=\boldsymbol{\Sigma}^{(1/2)}\boldsymbol{V}$。
- en: 3.6.4 Kernel Discriminative Component Analysis
  id: totrans-397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.4 核判别成分分析
- en: 'Here, we explain the kernel version of DCA (Hoi et al., [2006](#bib.bib69))
    which was introduced in Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Discriminative Component
    Analysis (DCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey").'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们解释了 DCA 的核版本（Hoi 等，[2006](#bib.bib69)），它在第 [3.1.5](#S3.SS1.SSS5 "3.1.5
    Discriminative Component Analysis (DCA) ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey") 节中介绍。'
- en: Lemma 4.
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 4。
- en: 'The generalized Mahalanobis distance metric in RKHS, with the pulled weight
    matrix to RKHS denoted by $\boldsymbol{\Phi}(\boldsymbol{W})$, can be seen as
    measuring the Euclidean distance in RKHS after projection onto the column subspace
    of $\boldsymbol{T}$ where $\boldsymbol{T}$ is the coefficient matrix in Eq. ([46](#S3.E46
    "In 3.6.2 Regularization by Locally Linear Embedding ‣ 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")). In other words:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: RKHS 中的广义马氏距离度量，权重矩阵投影到 RKHS 的 $\boldsymbol{\Phi}(\boldsymbol{W})$ 表示为，在 $\boldsymbol{T}$
    的列子空间上测量欧几里得距离，其中 $\boldsymbol{T}$ 是公式 ([46](#S3.E46 "在 3.6.2 通过局部线性嵌入正则化 ‣ 3.6
    核谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程和调查")) 中的系数矩阵。换句话说：
- en: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-$ | $\displaystyle\,\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}=\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    |  | (54) |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-$ | $\displaystyle\,\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}=\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    |  | (54) |'
- en: '|  |  | $\displaystyle=(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j}),$
    |  |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j}),$
    |  |'
- en: where $\boldsymbol{k}_{i}:=\boldsymbol{k}(\boldsymbol{X},\boldsymbol{x}_{i})=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})=[k(\boldsymbol{x}_{1},\boldsymbol{x}_{i}),\dots,k(\boldsymbol{x}_{n},\boldsymbol{x}_{i})]^{\top}\in\mathbb{R}^{n}$
    is the kernel vector between $\boldsymbol{X}$ and $\boldsymbol{x}_{i}$.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{k}_{i}:=\boldsymbol{k}(\boldsymbol{X},\boldsymbol{x}_{i})=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})=[k(\boldsymbol{x}_{1},\boldsymbol{x}_{i}),\dots,k(\boldsymbol{x}_{n},\boldsymbol{x}_{i})]^{\top}\in\mathbb{R}^{n}$
    是 $\boldsymbol{X}$ 和 $\boldsymbol{x}_{i}$ 之间的核向量。
- en: Proof.
  id: totrans-404
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We can have the decomposition of the weight matrix, i.e. Eq. ([9](#S2.E9 "In
    Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance
    Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")),
    in RKHS which is:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对权重矩阵进行分解，即公式 ([9](#S2.E9 "证明 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱、概率和深度度量学习：教程和调查")),
    在 RKHS 中表示为：
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{W})=\boldsymbol{\Phi}(\boldsymbol{U})\boldsymbol{\Phi}(\boldsymbol{U})^{\top}.$
    |  | (55) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{W})=\boldsymbol{\Phi}(\boldsymbol{U})\boldsymbol{\Phi}(\boldsymbol{U})^{\top}.$
    |  | (55) |'
- en: 'The generalized Mahalanobis distance metric in RKHS is:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: RKHS 中的广义马氏距离度量为：
- en: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}$
    |  |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}$
    |  |'
- en: '|  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}(\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j}))^{\top}\boldsymbol{\Phi}(\boldsymbol{U})\boldsymbol{\Phi}(\boldsymbol{U})^{\top}(\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j}))$
    |  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}(\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j}))^{\top}\boldsymbol{\Phi}(\boldsymbol{U})\boldsymbol{\Phi}(\boldsymbol{U})^{\top}(\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j}))$
    |  |'
- en: '|  | $\displaystyle=\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}^{\top}$
    |  |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}^{\top}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}$
    |  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\big{(}\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}$
    |  |'
- en: '|  | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}^{\top}$
    |  |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}^{\top}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}$
    |  |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\big{(}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{j})\big{)}$
    |  |'
- en: '|  | $\displaystyle\overset{(a)}{=}\big{(}\boldsymbol{T}^{\top}\boldsymbol{k}_{i}-\boldsymbol{T}^{\top}\boldsymbol{k}_{j}\big{)}^{\top}\big{(}\boldsymbol{T}^{\top}\boldsymbol{k}_{i}-\boldsymbol{T}^{\top}\boldsymbol{k}_{j}\big{)}$
    |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(a)}{=}\big{(}\boldsymbol{T}^{\top}\boldsymbol{k}_{i}-\boldsymbol{T}^{\top}\boldsymbol{k}_{j}\big{)}^{\top}\big{(}\boldsymbol{T}^{\top}\boldsymbol{k}_{i}-\boldsymbol{T}^{\top}\boldsymbol{k}_{j}\big{)}$
    |  |'
- en: '|  | $\displaystyle=\big{(}\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\big{)}^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}\big{(}\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\big{)}=\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2},$
    |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\big{(}\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\big{)}^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}\big{(}\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\big{)}=\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2},$
    |  |'
- en: where $(a)$ is because of the kernel trick, i.e., $\boldsymbol{k}(\boldsymbol{X},\boldsymbol{x}_{i})=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})$.
    Q.E.D. ∎
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于核技巧，即 $\boldsymbol{k}(\boldsymbol{X},\boldsymbol{x}_{i})=\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{i})$。证毕。
    ∎
- en: 'Let $\boldsymbol{\nu}_{l}:=[\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}\boldsymbol{k}(\boldsymbol{x}_{1},\boldsymbol{x}_{i}),\dots,\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}\boldsymbol{k}(\boldsymbol{x}_{n},\boldsymbol{x}_{i})]^{\top}\in\mathbb{R}^{n}$
    where $n_{l}$ denotes the cardinality of the $l$-th class. Let $\boldsymbol{K}_{w}$
    and $\boldsymbol{K}_{b}$ be the kernelized versions of $\boldsymbol{S}_{w}$ and
    $\boldsymbol{S}_{b}$, respectively (see Eqs. ([25](#S3.E25 "In 3.1.4 Relevant
    Component Analysis (RCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and ([26](#S3.E26 "In 3.1.5 Discriminative Component Analysis (DCA) ‣ 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))). If $\mathcal{X}_{l}$ denotes
    the $l$-th class, we have:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\boldsymbol{\nu}_{l}:=[\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}\boldsymbol{k}(\boldsymbol{x}_{1},\boldsymbol{x}_{i}),\dots,\frac{1}{n_{l}}\sum_{i=1}^{n_{l}}\boldsymbol{k}(\boldsymbol{x}_{n},\boldsymbol{x}_{i})]^{\top}\in\mathbb{R}^{n}$
    其中 $n_{l}$ 表示第 $l$ 类的基数。设 $\boldsymbol{K}_{w}$ 和 $\boldsymbol{K}_{b}$ 分别为 $\boldsymbol{S}_{w}$
    和 $\boldsymbol{S}_{b}$ 的核化版本（参见方程（[25](#S3.E25 "在 3.1.4 相关成分分析（RCA） ‣ 3.1 使用散布的谱方法
    ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程与调查")）和（[26](#S3.E26 "在 3.1.5 判别成分分析（DCA） ‣ 3.1 使用散布的谱方法
    ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程与调查")））。如果 $\mathcal{X}_{l}$ 表示第 $l$ 类，则我们有：
- en: '|  | $\displaystyle\mathbb{R}^{n\times n}\ni\boldsymbol{K}_{w}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{\boldsymbol{x}_{i}\in\mathcal{X}_{l}}(\boldsymbol{k}_{i}-\boldsymbol{\nu}_{l})(\boldsymbol{k}_{i}-\boldsymbol{\nu}_{l})^{\top}$
    |  | (56) |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{R}^{n\times n}\ni\boldsymbol{K}_{w}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{\boldsymbol{x}_{i}\in\mathcal{X}_{l}}(\boldsymbol{k}_{i}-\boldsymbol{\nu}_{l})(\boldsymbol{k}_{i}-\boldsymbol{\nu}_{l})^{\top}$
    |  | (56) |'
- en: '|  | $\displaystyle\mathbb{R}^{n\times n}\ni\boldsymbol{K}_{b}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{j=1}^{c}(\boldsymbol{\nu}_{l}-\boldsymbol{\nu}_{j})(\boldsymbol{\nu}_{l}-\boldsymbol{\nu}_{j})^{\top}.$
    |  | (57) |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{R}^{n\times n}\ni\boldsymbol{K}_{b}:=\frac{1}{n}\sum_{l=1}^{c}\sum_{j=1}^{c}(\boldsymbol{\nu}_{l}-\boldsymbol{\nu}_{j})(\boldsymbol{\nu}_{l}-\boldsymbol{\nu}_{j})^{\top}.$
    |  | (57) |'
- en: 'We saw the metric in RKHS can be seen as projection onto a subspace with the
    projection matrix $\boldsymbol{T}$. Therefore, Eq. ([27](#S3.E27 "In 3.1.5 Discriminative
    Component Analysis (DCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    in RKHS becomes (Hoi et al., [2006](#bib.bib69)):'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 RKHS 中的度量可以被视为投影到一个子空间上，投影矩阵为$\boldsymbol{T}$。因此，RKHS 中的方程（[27](#S3.E27
    "在 3.1.5 判别成分分析（DCA） ‣ 3.1 使用散布的谱方法 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程与调查")）变为（Hoi 等人，[2006](#bib.bib69)）：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{K}_{b}\boldsymbol{T})}{\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{K}_{w}\boldsymbol{T})},$
    |  | (58) |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{maximize}}$ |  | $\displaystyle\frac{\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{K}_{b}\boldsymbol{T})}{\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{K}_{w}\boldsymbol{T})},$
    |  | (58) |'
- en: 'which is a generalized Rayleigh-Ritz quotient. The solution $\boldsymbol{T}$
    to this optimization problem is the generalized eigenvalue problem $(\boldsymbol{K}_{b},\boldsymbol{K}_{w})$
    (Ghojogh et al., [2019a](#bib.bib38)). The weight matrix of the generalized Mahalanobis
    distance is obtained by Eqs. ([46](#S3.E46 "In 3.6.2 Regularization by Locally
    Linear Embedding ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) and
    ([55](#S3.E55 "In Proof. ‣ 3.6.4 Kernel Discriminative Component Analysis ‣ 3.6
    Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")).'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种广义的 Rayleigh-Ritz 商。此优化问题的解 $\boldsymbol{T}$ 是广义特征值问题 $(\boldsymbol{K}_{b},\boldsymbol{K}_{w})$（Ghojogh
    等，[2019a](#bib.bib38)）。广义 Mahalanobis 距离的权重矩阵通过方程 ([46](#S3.E46 "在 3.6.2 通过局部线性嵌入的正则化
    ‣ 3.6 内核谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程与综述")) 和 ([55](#S3.E55 "在证明中 ‣ 3.6.4 内核判别分量分析
    ‣ 3.6 内核谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程与综述")) 得到。
- en: 3.6.5 Relevant to Kernel Fisher Discriminant Analysis
  id: totrans-423
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.5 与内核 Fisher 判别分析相关
- en: 'Here, we explain the kernel version of the metric learning method (Alipanahi
    et al., [2008](#bib.bib2)) which was introduced in Section [3.1.3](#S3.SS1.SSS3
    "3.1.3 Relevant to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey").'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们解释了度量学习方法的内核版本（Alipanahi 等，[2008](#bib.bib2)），该方法在第 [3.1.3](#S3.SS1.SSS3
    "3.1.3 与 Fisher 判别分析相关 ‣ 3.1 基于散度的谱方法 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程与综述")节中介绍。
- en: 'According to Eq. ([54](#S3.E54 "In Lemma 4\. ‣ 3.6.4 Kernel Discriminative
    Component Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we
    have:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程 ([54](#S3.E54 "在引理 4\. ‣ 3.6.4 内核判别分量分析 ‣ 3.6 内核谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程与综述"))，我们有：
- en: '|  | $\displaystyle\&#124;\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\&#124;_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}=(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})$
    |  |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathord{\mid}\boldsymbol{\phi}(\boldsymbol{x}_{i})-\boldsymbol{\phi}(\boldsymbol{x}_{j})\mathord{\mid}_{\boldsymbol{\Phi}(\boldsymbol{W})}^{2}=(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}\big{(}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})\big{)}$
    |  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}\big{(}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})\big{)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(b)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)},$
    |  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(b)}{=}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)},$
    |  |'
- en: 'where $(a)$ is because a scalar it equal to its trace and $(b)$ is because
    of the cyclic property of trace. Hence, Eq. ([20](#S3.E20 "In 3.1.3 Relevant to
    Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) in RKHS becomes:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$(a)$ 是因为标量等于其迹，$(b)$ 是由于迹的循环性质。因此，第 ([20](#S3.E20 "在 3.1.3 与 Fisher 判别分析相关
    ‣ 3.1 基于散度的谱方法 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程与综述")) 节中的方程在 RKHS 中变为：
- en: '|  | $\displaystyle\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}$
    |  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{\mathord{\mid}\mathcal{S}\mathord{\mid}}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}$
    |  |'
- en: '|  | $\displaystyle=\textbf{tr}\Big{(}\boldsymbol{T}^{\top}\big{(}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}\Big{)}$
    |  |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\textbf{tr}\Big{(}\boldsymbol{T}^{\top}\big{(}\frac{1}{|\mathcal{S}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}\Big{)}$
    |  |'
- en: '|  | $\displaystyle=\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}\boldsymbol{T}),$
    |  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}\boldsymbol{T}),$
    |  |'
- en: 'and likewise:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地：
- en: '|  | $\displaystyle\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}$
    |  |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{|\mathcal{D}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{T}^{\top}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}\big{)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}}\boldsymbol{T}),$
    |  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=\textbf{tr}(\boldsymbol{T}^{\top}\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}}\boldsymbol{T}),$
    |  |'
- en: 'where:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}:=\frac{1}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top},$
    |  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}:=\frac{1}{|\mathcal{S}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top},$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}}:=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}.$
    |  |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}}:=\frac{1}{|\mathcal{D}|}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}.$
    |  |'
- en: 'Hence, in RKHS, the objective of the optimization problem ([23](#S3.E23 "In
    3.1.3 Relevant to Fisher Discriminant Analysis ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) becomes $\textbf{tr}(\boldsymbol{T}^{\top}(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}})\boldsymbol{T}^{\top})$.
    We change the constraint in Eq. ([23](#S3.E23 "In 3.1.3 Relevant to Fisher Discriminant
    Analysis ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) to $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$.
    In RKHS, this constraint becomes:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 RKHS 中，优化问题的目标 ([23](#S3.E23 "在 3.1.3 节相关于费舍尔判别分析 ‣ 3.1 使用散射的谱方法 ‣ 3 谱度量学习
    ‣ 谱的、概率的和深度的度量学习：教程和调查")) 变为 $\textbf{tr}(\boldsymbol{T}^{\top}(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}})\boldsymbol{T}^{\top})$。我们将
    Eq. ([23](#S3.E23 "在 3.1.3 节相关于费舍尔判别分析 ‣ 3.1 使用散射的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查"))
    中的约束条件更改为 $\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I}$。在 RKHS 中，这个约束变为：
- en: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{U})$
    | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}$
    |  |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Phi}(\boldsymbol{U})^{\top}\boldsymbol{\Phi}(\boldsymbol{U})$
    | $\displaystyle\overset{(\ref{equation_kernelization_representation_theory})}{=}\boldsymbol{T}^{\top}\boldsymbol{\Phi}(\boldsymbol{X})^{\top}\boldsymbol{\Phi}(\boldsymbol{X})\boldsymbol{T}$
    |  |'
- en: '|  |  | $\displaystyle\overset{(\ref{equation_Kernel_X})}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}\overset{\text{set}}{=}\boldsymbol{I},$
    |  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(\ref{equation_Kernel_X})}{=}\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}\overset{\text{set}}{=}\boldsymbol{I},$
    |  |'
- en: 'Finally, ([23](#S3.E23 "In 3.1.3 Relevant to Fisher Discriminant Analysis ‣
    3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) in RKHS becomes:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，([23](#S3.E23 "在 3.1.3 节相关于费舍尔判别分析 ‣ 3.1 使用散射的谱方法 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查"))
    在 RKHS 中变为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{T}^{\top}(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}})\boldsymbol{T})$
    |  | (59) |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{T}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{T}^{\top}(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}})\boldsymbol{T})$
    |  | (59) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}=\boldsymbol{I},$
    |  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\boldsymbol{T}^{\top}\boldsymbol{K}_{x}\boldsymbol{T}=\boldsymbol{I},$
    |  |'
- en: 'whose solution is a generalized eigenvalue problem $(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}},\boldsymbol{K}_{x})$
    where $\boldsymbol{T}$ is the matrix of eigenvectors. The weight matrix of the
    generalized Mahalanobis distance is obtained by Eqs. ([46](#S3.E46 "In 3.6.2 Regularization
    by Locally Linear Embedding ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) and ([55](#S3.E55 "In Proof. ‣ 3.6.4 Kernel Discriminative Component
    Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")). This
    is relevant to kernel Fisher discriminant analysis (Mika et al., [1999](#bib.bib86);
    Ghojogh et al., [2019b](#bib.bib39)) which minimizes and maximizes the intra-class
    and inter-class variances in RKHS.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '其解是一个广义特征值问题 $(\boldsymbol{\Sigma}^{\phi}_{\mathcal{S}}-\boldsymbol{\Sigma}^{\phi}_{\mathcal{D}},\boldsymbol{K}_{x})$，其中
    $\boldsymbol{T}$ 是特征向量矩阵。广义马哈拉诺比斯距离的权重矩阵由公式 ([46](#S3.E46 "In 3.6.2 Regularization
    by Locally Linear Embedding ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) 和 ([55](#S3.E55 "In Proof. ‣ 3.6.4 Kernel Discriminative Component
    Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 获得。这与核
    Fisher 判别分析（Mika 等，[1999](#bib.bib86)；Ghojogh 等，[2019b](#bib.bib39)）相关，该分析在 RKHS
    中最小化和最大化类内和类间方差。'
- en: 3.6.6 Relevant to Kernel Support Vector Machine
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.6 与核支持向量机相关
- en: 'Here, we explain the kernel version of the metric learning method (Tsang et al.,
    [2003](#bib.bib112)) which was introduced in Section [3.4](#S3.SS4 "3.4 Relevant
    to Support Vector Machine ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"). It is relevant to kernel SVM.
    Using kernel trick (Ghojogh et al., [2021e](#bib.bib50)) and Eq. ([54](#S3.E54
    "In Lemma 4\. ‣ 3.6.4 Kernel Discriminative Component Analysis ‣ 3.6 Kernel Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), the Eq. ([41](#S3.E41 "In 3.4 Relevant
    to Support Vector Machine ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) can be kernelized as (Tsang et al.,
    [2003](#bib.bib112)):'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们解释了度量学习方法的核版本（Tsang 等，[2003](#bib.bib112)），该方法在第 [3.4](#S3.SS4 "3.4 Relevant
    to Support Vector Machine ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey") 节中介绍。它与核 SVM 相关。通过使用核技巧（Ghojogh
    等，[2021e](#bib.bib50)）和公式 ([54](#S3.E54 "In Lemma 4\. ‣ 3.6.4 Kernel Discriminative
    Component Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")），公式
    ([41](#S3.E41 "In 3.4 Relevant to Support Vector Machine ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 可以被核化为（Tsang
    等，[2003](#bib.bib112)）：'
- en: '|  |  | $\displaystyle\underset{\{\alpha_{ij}\}}{\text{maximize}}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\boldsymbol{T}^{\top}(k_{ii}+k_{jj}-2k_{ij})$
    |  | (60) |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\{\alpha_{ij}\}}{\text{maximize}}~{}~{}~{}~{}~{}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\boldsymbol{T}^{\top}(k_{ii}+k_{jj}-2k_{ij})$
    |  | (60) |'
- en: '|  |  | $\displaystyle-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{D}}\alpha_{ij}\alpha_{kl}(k_{ik}-k_{il}-k_{jk}+k_{jl})^{2}$
    |  |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{D}}\alpha_{ij}\alpha_{kl}(k_{ik}-k_{il}-k_{jk}+k_{jl})^{2}$
    |  |'
- en: '|  |  | $\displaystyle+\frac{\lambda_{1}}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{S}}\alpha_{ij}(k_{ik}-k_{il}-k_{jk}+k_{jl})^{2}$
    |  |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\frac{\lambda_{1}}{&#124;\mathcal{S}&#124;}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\sum_{(\boldsymbol{x}_{k},\boldsymbol{x}_{l})\in\mathcal{S}}\alpha_{ij}(k_{ik}-k_{il}-k_{jk}+k_{jl})^{2}$
    |  |'
- en: '|  |  | $\displaystyle\text{subject to}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\frac{1}{\lambda_{2}}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\geq\nu,$
    |  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{subject to}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\frac{1}{\lambda_{2}}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\alpha_{ij}\geq\nu,$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\alpha_{ij}\in[0,\frac{\lambda_{2}}{&#124;\mathcal{D}&#124;}],$
    |  |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\alpha_{ij}\in[0,\frac{\lambda_{2}}{&#124;\mathcal{D}&#124;}],$
    |  |'
- en: which is a quadratic programming problem and can be solved by optimization solvers.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个二次规划问题，可以通过优化求解器来解决。
- en: 3.7 Geometric Spectral Metric Learning
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 几何谱度量学习
- en: Some spectral metric learning methods are geometric methods which use Riemannian
    manifolds. In the following, we introduce the mist well-known geometric methods.
    There are some other geometric methods, such as (Hauberg et al., [2012](#bib.bib62)),
    which are not covered for brevity.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 一些谱度量学习方法是几何方法，它们使用黎曼流形。接下来，我们将介绍一些最著名的几何方法。为了简洁起见，本文不涵盖其他几何方法，例如 (Hauberg et
    al., [2012](#bib.bib62))。
- en: 3.7.1 Geometric Mean Metric Learning
  id: totrans-456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.1 几何均值度量学习
- en: One of the geometric spectral metric learning is Geometric Mean Metric Learning
    (GMML) (Zadeh et al., [2016](#bib.bib141)). Let $\boldsymbol{W}$ be the weight
    matrix in the generalized Mahalanobis distance for similar points.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 几何谱度量学习的一种方法是几何均值度量学习（GMML）（Zadeh et al., [2016](#bib.bib141)）。设 $\boldsymbol{W}$
    为相似点的广义马氏距离中的权重矩阵。
- en: '– Regular GMML: In GMML, we use the inverse of weight matrix, i.e. $\boldsymbol{W}^{-1}$
    , for the dissimilar points. The optimization problem of GMML is (Zadeh et al.,
    [2016](#bib.bib141)):'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: – 常规 GMML：在 GMML 中，我们对不相似点使用权重矩阵的逆，即 $\boldsymbol{W}^{-1}$。GMML 的优化问题是 (Zadeh
    et al., [2016](#bib.bib141))：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (61) |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}$
    |  | (61) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}^{-1}}^{2}$
    |  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}^{-1}}^{2}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: 'According to Eq. ([13](#S3.E13 "In Proof. ‣ 3.1.1 The First Spectral Method
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), this problem
    can be restated as:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '根据公式 ([13](#S3.E13 "In Proof. ‣ 3.1.1 The First Spectral Method ‣ 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))，该问题可以重新表述为：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (62) |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (62) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: 'where $\boldsymbol{\Sigma}_{\mathcal{S}}$ and $\boldsymbol{\Sigma}_{\mathcal{D}}$
    are defined in Eq. ([14](#S3.E14 "In Proof. ‣ 3.1.1 The First Spectral Method
    ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")). Taking derivative
    of the objective function w.r.t. $\boldsymbol{W}$ and setting it to zero gives:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\boldsymbol{\Sigma}_{\mathcal{S}}$ 和 $\boldsymbol{\Sigma}_{\mathcal{D}}$
    在公式 ([14](#S3.E14 "In Proof. ‣ 3.1.1 The First Spectral Method ‣ 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) 中定义。对目标函数关于 $\boldsymbol{W}$
    取导数并将其设为零得到：'
- en: '|  | $\displaystyle\frac{\partial}{\partial\boldsymbol{W}}\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})\big{)}$
    |  |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial\boldsymbol{W}}\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})\big{)}$
    |  |'
- en: '|  | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}-\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{W}^{-1}\overset{\text{set}}{=}\boldsymbol{0}\implies\boldsymbol{\Sigma}_{\mathcal{D}}=\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}.$
    |  | (63) |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}-\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{W}^{-1}\overset{\text{set}}{=}\boldsymbol{0}\implies\boldsymbol{\Sigma}_{\mathcal{D}}=\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}.$
    |  | (63) |'
- en: This equation is the Riccati equation (Riccati, [1724](#bib.bib94)) and its
    solution is the midpoint of the geodesic connecting $\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}$
    and $\boldsymbol{\Sigma}_{\mathcal{D}}$ (Bhatia, [2007](#bib.bib12), Section 1.2.13).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是 Riccati 方程 (Riccati, [1724](#bib.bib94))，它的解是连接 $\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}$
    和 $\boldsymbol{\Sigma}_{\mathcal{D}}$ 的测地线的中点 (Bhatia, [2007](#bib.bib12), 第1.2.13节)。
- en: Lemma 5  ((Bhatia, [2007](#bib.bib12), Chapter 6)).
  id: totrans-469
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 5  ((Bhatia, [2007](#bib.bib12), 第6章))。
- en: 'The geodesic curve connecting two points $\boldsymbol{\Sigma}_{1}$ and $\boldsymbol{\Sigma}_{2}$
    on the Symmetric Positive Definite (SPD) Riemannian manifold is denoted by $\boldsymbol{\Sigma}_{1}\sharp_{t}\boldsymbol{\Sigma}_{2}$
    and is computed as:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 连接两个点 $\boldsymbol{\Sigma}_{1}$ 和 $\boldsymbol{\Sigma}_{2}$ 在对称正定（SPD）黎曼流形上的测地线曲线表示为
    $\boldsymbol{\Sigma}_{1}\sharp_{t}\boldsymbol{\Sigma}_{2}$，计算公式为：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{1}\sharp_{t}\boldsymbol{\Sigma}_{2}:=\boldsymbol{\Sigma}_{1}^{(1/2)}\big{(}\boldsymbol{\Sigma}_{1}^{(-1/2)}\boldsymbol{\Sigma}_{2}\boldsymbol{\Sigma}_{1}^{(-1/2)}\big{)}^{t}\boldsymbol{\Sigma}_{1}^{(1/2)},$
    |  | (64) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}_{1}\sharp_{t}\boldsymbol{\Sigma}_{2}:=\boldsymbol{\Sigma}_{1}^{(1/2)}\big{(}\boldsymbol{\Sigma}_{1}^{(-1/2)}\boldsymbol{\Sigma}_{2}\boldsymbol{\Sigma}_{1}^{(-1/2)}\big{)}^{t}\boldsymbol{\Sigma}_{1}^{(1/2)},$
    |  | (64) |'
- en: where $t\in[0,1]$.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t\in[0,1]$。
- en: 'Hence, the solution of Eq. ([63](#S3.E63 "In 3.7.1 Geometric Mean Metric Learning
    ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，方程 ([63](#S3.E63 "在 3.7.1 几何平均度量学习 ‣ 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率与深度度量学习：教程与调研"))
    的解为：
- en: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}$
    |  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}$
    |  |'
- en: '|  |  | $\displaystyle\overset{(\ref{equation_SPD_geodesic})}{=}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}\big{(}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\big{)}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}.$
    |  | (65) |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(\ref{equation_SPD_geodesic})}{=}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}\big{(}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\big{)}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}.$
    |  | (65) |'
- en: 'The proof of Eq. ([65](#S3.E65 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7
    Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) is as follows (Hajiabadi et al.,
    [2019](#bib.bib59)):'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 方程的证明 ([65](#S3.E65 "在 3.7.1 几何平均度量学习 ‣ 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率与深度度量学习：教程与调研"))
    如下 (Hajiabadi 等， [2019](#bib.bib59))：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}\overset{(\ref{equation_GMML_solution_1})}{=}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}$
    |  |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}_{\mathcal{D}}\overset{(\ref{equation_GMML_solution_1})}{=}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}$
    |  |'
- en: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}$
    |  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}$
    |  |'
- en: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
- en: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}((\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}))^{(1/2)}$
    |  |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}((\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}))^{(1/2)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})$
    |  |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})$
    |  |'
- en: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}$
    |  |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}=\boldsymbol{W},$
    |  |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}(\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)})\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}=\boldsymbol{W},$
    |  |'
- en: where $(a)$ is because $\boldsymbol{\Sigma}_{\mathcal{S}}\succeq\boldsymbol{0}$
    so its eigenvalues are non-negative and the matrix of eigenvalues can be decomposed
    by the second root in its eigenvalue decomposition to have $\boldsymbol{\Sigma}_{\mathcal{S}}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}$.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是因为 $\boldsymbol{\Sigma}_{\mathcal{S}}\succeq\boldsymbol{0}$，因此其特征值是非负的，并且可以通过其特征值分解中的二次根来分解矩阵，以得到
    $\boldsymbol{\Sigma}_{\mathcal{S}}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}$。
- en: '– Regularized GMML: The matrix $\boldsymbol{\Sigma}_{\mathcal{S}}$ might be
    singular or near singular and hence non-invertible. Therefore, we regularize Eq.
    ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) to make the weight matrix close to a prior
    known positive definite matrix $\boldsymbol{W}_{0}$.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化 GMML：矩阵 $\boldsymbol{\Sigma}_{\mathcal{S}}$ 可能是奇异的或接近奇异，因此不可逆。因此，我们对方程 ([62](#S3.E62
    "在 3.7.1 几何均值度量学习 ‣ 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程和调查")) 进行正则化，以使权重矩阵接近一个已知的正定矩阵
    $\boldsymbol{W}_{0}$。
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (66) |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (66) |'
- en: '|  |  | $\displaystyle+\lambda\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{W}_{0})-2d\big{)},$
    |  |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{W}_{0})-2d\big{)},$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: 'where $\lambda>0$ is the regularization parameter. The regularization term
    is the symmetrized log-determinant divergence between $\boldsymbol{W}$ and $\boldsymbol{W}_{0}$.
    Taking derivative of the objective function w.r.t. $\boldsymbol{W}$ and setting
    it to zero gives:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 是正则化参数。正则化项是 $\boldsymbol{W}$ 和 $\boldsymbol{W}_{0}$ 之间的对称化对数行列式散度。对目标函数关于
    $\boldsymbol{W}$ 进行求导并将其设置为零，得到：
- en: '|  | $\displaystyle\frac{\partial}{\partial\boldsymbol{W}}\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})+\lambda\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})$
    |  |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial\boldsymbol{W}}\big{(}\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})+\lambda\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{W}_{0})-2\lambda
    d\big{)}$ |  |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{W}_{0})-2\lambda
    d\big{)}$ |  |'
- en: '|  | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}-\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{W}^{-1}+\lambda\boldsymbol{W}_{0}^{-1}$
    |  |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\boldsymbol{\Sigma}_{\mathcal{S}}-\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{W}^{-1}+\lambda\boldsymbol{W}_{0}^{-1}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\boldsymbol{W}^{-1}\boldsymbol{W}_{0}\boldsymbol{W}^{-1}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\boldsymbol{W}^{-1}\boldsymbol{W}_{0}\boldsymbol{W}^{-1}\overset{\text{设}}{=}\boldsymbol{0}$
    |  |'
- en: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}=\boldsymbol{W}(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})\boldsymbol{W},$
    |  |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}=\boldsymbol{W}(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})\boldsymbol{W},$
    |  |'
- en: 'which is again a Riccati equation (Riccati, [1724](#bib.bib94)) whose solution
    is the midpoint of the geodesic connecting $(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}$
    and $(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0})$:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 这再次是一个 Riccati 方程（Riccati，[1724](#bib.bib94)），其解是连接 $(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}$
    和 $(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0})$ 的测地线的中点：
- en: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}\sharp_{(1/2)}(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}).$
    |  | (67) |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}\sharp_{(1/2)}(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}).$
    |  | (67) |'
- en: '– Weighted GMML: Eq. ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning
    ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can be restated
    as:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '– 加权 GMML: 方程 ([62](#S3.E62 "在 3.7.1 几何均值度量学习 ‣ 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查"))
    可以重述为：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{S}}^{-1})+\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (68) |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{S}}^{-1})+\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (68) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: 'where $\delta(.,.)$ is the Riemannian distance (or Fréchet mean) on the SPD
    manifold (Arsigny et al., [2007](#bib.bib3), Eq 1.1):'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\delta(.,.)$ 是 SPD 流形上的黎曼距离（或 Fréchet 均值）（Arsigny 等人，[2007](#bib.bib3)，方程
    1.1）：
- en: '|  | $\displaystyle\delta(\boldsymbol{\Sigma}_{1},\boldsymbol{\Sigma}_{2}):=\&#124;\log(\boldsymbol{\Sigma}_{2}^{(-1/2)}\boldsymbol{\Sigma}_{1}\boldsymbol{\Sigma}_{2}^{(-1/2)})\&#124;_{F},$
    |  |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\delta(\boldsymbol{\Sigma}_{1},\boldsymbol{\Sigma}_{2}):=\&#124;\log(\boldsymbol{\Sigma}_{2}^{(-1/2)}\boldsymbol{\Sigma}_{1}\boldsymbol{\Sigma}_{2}^{(-1/2)})\&#124;_{F},$
    |  |'
- en: 'where $\|.\|_{F}$ is the Frobenius norm. We can weight the objective in Eq.
    ([68](#S3.E68 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")):'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\|.\|_{F}$ 是 Frobenius 范数。我们可以在方程 ([68](#S3.E68 "在 3.7.1 几何均值度量学习 ‣ 3.7
    几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查")) 中加权目标：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle(1-t)\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{S}}^{-1})+t\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (69) |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle(1-t)\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{S}}^{-1})+t\delta^{2}(\boldsymbol{W},\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (69) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: 'where $t\in[0,1]$ is a hyperparameter. The solution of this problem is the
    weighted version of Eq. ([67](#S3.E67 "In 3.7.1 Geometric Mean Metric Learning
    ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")):'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t\in[0,1]$ 是一个超参数。该问题的解是方程 ([67](#S3.E67 "在 3.7.1 几何均值度量学习 ‣ 3.7 几何谱度量学习
    ‣ 3 谱度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查")) 的加权版本：
- en: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}\sharp_{t}(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}).$
    |  | (70) |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}$ | $\displaystyle=(\boldsymbol{\Sigma}_{\mathcal{S}}+\lambda\boldsymbol{W}_{0}^{-1})^{-1}\sharp_{t}(\boldsymbol{\Sigma}_{\mathcal{D}}+\lambda\boldsymbol{W}_{0}).$
    |  | (70) |'
- en: 3.7.2 Low-rank Geometric Mean Metric Learning
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.2 低秩几何均值度量学习
- en: 'We can learn a low-rank weight matrix in GMML (Bhutani et al., [2018](#bib.bib13)),
    where the rank of wight matrix is set to be $p\ll d$:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 GMML (Bhutani et al., [2018](#bib.bib13)) 中学习一个低秩权重矩阵，其中权重矩阵的秩设置为 $p\ll
    d$：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (71) |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  | (71) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
- en: '|  |  | $\displaystyle\textbf{rank}(\boldsymbol{W})=p.$ |  |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\textbf{rank}(\boldsymbol{W})=p.$ |  |'
- en: 'We can decompose it using eigenvalue decomposition as done in Eq. ([9](#S2.E9
    "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), i.e., $\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}=\boldsymbol{U}\boldsymbol{U}^{\top}$,
    where we only have $p$ eigenvectors and $p$ eigenvalues. Therefore, the sizes
    of matrices are $\boldsymbol{V}\in\mathbb{R}^{d\times p}$, $\boldsymbol{\Lambda}\in\mathbb{R}^{p\times
    p}$, and $\boldsymbol{U}\in\mathbb{R}^{d\times p}$. By this decomposition, the
    objective function in Eq. ([71](#S3.E71 "In 3.7.2 Low-rank Geometric Mean Metric
    Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey") can
    be restated as:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以使用特征值分解来进行分解，如方程 ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) 中所做的，即 $\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}=\boldsymbol{U}\boldsymbol{U}^{\top}$，其中我们只有
    $p$ 个特征向量和 $p$ 个特征值。因此，矩阵的大小为 $\boldsymbol{V}\in\mathbb{R}^{d\times p}$，$\boldsymbol{\Lambda}\in\mathbb{R}^{p\times
    p}$ 和 $\boldsymbol{U}\in\mathbb{R}^{d\times p}$。通过这种分解，方程 ([71](#S3.E71 "In 3.7.2
    Low-rank Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey") 的目标函数可以重新表述为：'
- en: '|  | $\displaystyle\textbf{tr}(\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{V}\boldsymbol{\Lambda}^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{tr}(\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{V}\boldsymbol{\Lambda}^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}})$
    |  |'
- en: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V})$
    |  |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(a)}{=}\textbf{tr}(\boldsymbol{\Lambda}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V})$
    |  |'
- en: '|  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{\Lambda}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}),$
    |  |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(b)}{=}\textbf{tr}(\boldsymbol{\Lambda}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}),$
    |  |'
- en: 'where $(\boldsymbol{V}^{\top})^{-1}=\boldsymbol{V}$ because it is orthogonal,
    $(a)$ is because of the cyclic property of trace, and $(b)$ is because we define
    $\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}}:=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V}$
    and $\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}:=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V}$.
    Noticing that the matrix of eigenvectors $\boldsymbol{V}$ is orthogonal, the Eq.
    ([71](#S3.E71 "In 3.7.2 Low-rank Geometric Mean Metric Learning ‣ 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) is restated to:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $(\boldsymbol{V}^{\top})^{-1}=\boldsymbol{V}$ 因为它是正交的，$(a)$ 是由于迹的循环性质，$(b)$
    是因为我们定义了 $\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}}:=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V}$
    和 $\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}:=\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V}$。注意到特征向量矩阵
    $\boldsymbol{V}$ 是正交的，Eq. ([71](#S3.E71 "In 3.7.2 Low-rank Geometric Mean Metric
    Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 被重述为：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{\Lambda},\boldsymbol{V}}{\text{minimize}}$
    |  | $\displaystyle\textbf{tr}(\boldsymbol{\Lambda}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}})$
    |  | (72) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{\Lambda},\boldsymbol{V}}{\text{minimize}}$
    |  | $\displaystyle\textbf{tr}(\boldsymbol{\Lambda}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{\Lambda}^{-1}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}})$
    |  | (72) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{\Lambda}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受制于 |  | $\displaystyle\boldsymbol{\Lambda}\succeq\boldsymbol{0},$
    |  |'
- en: '|  |  | $\displaystyle\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I},$
    |  |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I},$
    |  |'
- en: 'where $\textbf{rank}(\boldsymbol{W})=p$ is automatically satisfied by taking
    $\boldsymbol{V}\in\mathbb{R}^{d\times p}$ and $\boldsymbol{\Lambda}\in\mathbb{R}^{p\times
    p}$ in the decomposition. This problem can be solved by the alternative optimization
    (Ghojogh et al., [2021c](#bib.bib48)). If the variable $\boldsymbol{V}$ is fixed,
    minimization w.r.t. $\boldsymbol{\Lambda}$ is similar to the problem ([62](#S3.E62
    "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")); hence, its solution is similar to Eq. ([65](#S3.E65 "In
    3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), i.e., $\boldsymbol{\Lambda}={\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}}}^{-1}\sharp_{(1/2)}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}$
    (see Eq. ([64](#S3.E64 "In Lemma 5 ((Bhatia, 2007, Chapter 6)). ‣ 3.7.1 Geometric
    Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    for the definition of $\sharp_{t}$). If $\boldsymbol{\Lambda}$ is fixed, the orthogonality
    constraint $\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}$ can be modeled
    by $\boldsymbol{V}$ belonging to the Grassmannian manifold $G(p,d)$ which is the
    set of $p$-dimensional subspaces of $\mathbb{R}^{d}$. To sum up, the alternative
    optimization is:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\textbf{rank}(\boldsymbol{W})=p$ 通过取 $\boldsymbol{V}\in\mathbb{R}^{d\times
    p}$ 和 $\boldsymbol{\Lambda}\in\mathbb{R}^{p\times p}$ 在分解中自动满足。这个问题可以通过交替优化解决（Ghojogh
    et al., [2021c](#bib.bib48)）。如果变量 $\boldsymbol{V}$ 固定，则相对于 $\boldsymbol{\Lambda}$
    的最小化类似于问题 ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))；因此，其解决方案类似于 Eq. ([65](#S3.E65
    "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))，即 $\boldsymbol{\Lambda}={\widetilde{\boldsymbol{\Sigma}}_{\mathcal{S}}}^{-1}\sharp_{(1/2)}\widetilde{\boldsymbol{\Sigma}}_{\mathcal{D}}$（见
    Eq. ([64](#S3.E64 "In Lemma 5 ((Bhatia, 2007, Chapter 6)). ‣ 3.7.1 Geometric Mean
    Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 中的
    $\sharp_{t}$ 定义）。如果 $\boldsymbol{\Lambda}$ 固定，正交约束 $\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}$
    可以通过 $\boldsymbol{V}$ 属于 Grassmann 流形 $G(p,d)$ 来建模，该流形是 $\mathbb{R}^{d}$ 的 $p$
    维子空间的集合。总之，交替优化为：'
- en: '|  | $\displaystyle\boldsymbol{\Lambda}^{(\tau+1)}=(\boldsymbol{V}^{(\tau)\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V}^{(\tau)})^{-1}\sharp_{(1/2)}(\boldsymbol{V}^{(\tau)\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V}^{(\tau)}),$
    |  |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Lambda}^{(\tau+1)}=(\boldsymbol{V}^{(\tau)\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V}^{(\tau)})^{-1}\sharp_{(1/2)}(\boldsymbol{V}^{(\tau)\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V}^{(\tau)}),$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{V}^{(\tau+1)}:=\arg\min_{\boldsymbol{V}\in G(p,d)}\Big{(}\textbf{tr}(\boldsymbol{\Lambda}^{(\tau+1)}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V})$
    |  |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{V}^{(\tau+1)}:=\arg\min_{\boldsymbol{V}\in G(p,d)}\Big{(}\textbf{tr}(\boldsymbol{\Lambda}^{(\tau+1)}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{S}}\boldsymbol{V})$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\textbf{tr}((\boldsymbol{\Lambda}^{(\tau+1)})^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V})\Big{)},$
    |  |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\textbf{tr}((\boldsymbol{\Lambda}^{(\tau+1)})^{-1}\boldsymbol{V}^{\top}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{V})\Big{)},$
    |  |'
- en: where $\tau$ is the iteration index. Optimization of $\boldsymbol{V}$ can be
    solved by Riemannian optimization (Absil et al., [2009](#bib.bib1)).
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau$ 是迭代索引。$\boldsymbol{V}$ 的优化可以通过黎曼优化（Absil 等人，[2009](#bib.bib1)）解决。
- en: 3.7.3 Geometric Mean Metric Learning for Partial Labels
  id: totrans-527
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.3 部分标签的几何均值度量学习
- en: Partial label learning (Cour et al., [2011](#bib.bib20)) refers to when a set
    of candidate labels is available for every data point. GMML can be modified to
    be used for partial label learning (Zhou & Gu, [2018](#bib.bib145)). Let $\mathcal{Y}_{i}$
    denote the set of candidate labels for $\boldsymbol{x}_{i}$. If there are $q$
    candidate labels in total, we denote $\boldsymbol{y}_{i}=[y_{i1},\dots,y_{iq}]^{\top}\in\{0,1\}^{q}$
    where $y_{ij}$ is one if the $j$-th label is a candidate label for $\boldsymbol{x}_{i}$
    and is zero otherwise. We define $\boldsymbol{X}_{i}^{+}:=\{\boldsymbol{x}_{j}|j=1,\dots,n,j\neq
    i,\mathcal{Y}_{i}\cap\mathcal{Y}_{j}\neq\varnothing\}$ and $\boldsymbol{X}_{i}^{-}:=\{\boldsymbol{x}_{j}|j=1,\dots,n,\mathcal{Y}_{i}\cap\mathcal{Y}_{j}=\varnothing\}$.
    In other words, $\boldsymbol{X}_{i}^{+}$ and $\boldsymbol{X}_{i}^{-}$ are the
    data points which share and do not share some candidate labels with $\boldsymbol{x}_{i}$,
    respectively. Let $\mathcal{N}_{i}^{+}$ be the indices of the $k$ nearest neighbors
    of $\boldsymbol{x}_{i}$ among $\boldsymbol{X}_{i}^{+}$. Also, let $\mathcal{N}_{i}^{-}$
    be the indices of points in $\boldsymbol{X}_{i}^{-}$ whose distance from $\boldsymbol{x}_{i}$
    are smaller than the distance of the furthest point in $\mathcal{N}_{i}^{+}$ from
    $\boldsymbol{x}_{i}$. In other words, $\mathcal{N}_{i}^{-}:=\{j|j=1,\dots,n,\boldsymbol{x}_{j}\in\boldsymbol{X}_{i}^{-},\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}\leq\max_{t\in\mathcal{N}_{i}^{+}}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{t}\|_{2}\}$.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 部分标签学习（Cour 等人，[2011](#bib.bib20)）指的是每个数据点都有一组候选标签。GMML 可以被修改为用于部分标签学习（Zhou
    & Gu，[2018](#bib.bib145)）。设 $\mathcal{Y}_{i}$ 表示 $\boldsymbol{x}_{i}$ 的候选标签集合。如果总共有
    $q$ 个候选标签，我们用 $\boldsymbol{y}_{i}=[y_{i1},\dots,y_{iq}]^{\top}\in\{0,1\}^{q}$
    表示，其中 $y_{ij}$ 是1如果第 $j$ 个标签是 $\boldsymbol{x}_{i}$ 的候选标签，否则为0。我们定义 $\boldsymbol{X}_{i}^{+}:=\{\boldsymbol{x}_{j}|j=1,\dots,n,j\neq
    i,\mathcal{Y}_{i}\cap\mathcal{Y}_{j}\neq\varnothing\}$ 和 $\boldsymbol{X}_{i}^{-}:=\{\boldsymbol{x}_{j}|j=1,\dots,n,\mathcal{Y}_{i}\cap\mathcal{Y}_{j}=\varnothing\}$。换句话说，$\boldsymbol{X}_{i}^{+}$
    和 $\boldsymbol{X}_{i}^{-}$ 分别是与 $\boldsymbol{x}_{i}$ 共享和不共享某些候选标签的数据点。设 $\mathcal{N}_{i}^{+}$
    为 $\boldsymbol{x}_{i}$ 在 $\boldsymbol{X}_{i}^{+}$ 中的 $k$ 个最近邻的索引。同时，设 $\mathcal{N}_{i}^{-}$
    为 $\boldsymbol{X}_{i}^{-}$ 中距离 $\boldsymbol{x}_{i}$ 小于 $\mathcal{N}_{i}^{+}$ 中最远点与
    $\boldsymbol{x}_{i}$ 的距离的点的索引。换句话说，$\mathcal{N}_{i}^{-}:=\{j|j=1,\dots,n,\boldsymbol{x}_{j}\in\boldsymbol{X}_{i}^{-},\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{2}\leq\max_{t\in\mathcal{N}_{i}^{+}}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{t}\|_{2}\}$。
- en: 'Let $\boldsymbol{w}_{i}^{(1)}=[w_{i,t}^{(1)},\forall t\in\mathcal{N}_{i}^{+}]^{\top}\in\mathbb{R}^{k}$
    contain the probabilities that each of the $k$ neighbors of $\boldsymbol{x}_{i}$
    share the same label with $\boldsymbol{x}_{i}$. It can be estimated by linear
    reconstruction of $\boldsymbol{y}_{i}$ by the neighbor $\boldsymbol{y}_{t}$’s:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\boldsymbol{w}_{i}^{(1)}=[w_{i,t}^{(1)},\forall t\in\mathcal{N}_{i}^{+}]^{\top}\in\mathbb{R}^{k}$
    包含了每个 $\boldsymbol{x}_{i}$ 的 $k$ 个邻居与 $\boldsymbol{x}_{i}$ 共享相同标签的概率。它可以通过邻居 $\boldsymbol{y}_{t}$
    的线性重建来估计。
- en: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{i}^{(1)}}{\text{minimize}}$
    |  | $\displaystyle\frac{1}{q}\big{\&#124;}\boldsymbol{y}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}\boldsymbol{y}_{t}\big{\&#124;}_{2}^{2}+\frac{\lambda_{1}}{k}\sum_{t\in\mathcal{N}_{i}^{+}}(w_{i,t}^{(1)})^{2}$
    |  |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{i}^{(1)}}{\text{minimize}}$
    |  | $\displaystyle\frac{1}{q}\big{\&#124;}\boldsymbol{y}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}\boldsymbol{y}_{t}\big{\&#124;}_{2}^{2}+\frac{\lambda_{1}}{k}\sum_{t\in\mathcal{N}_{i}^{+}}(w_{i,t}^{(1)})^{2}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle w_{i,t}^{(1)}\geq 0,\quad t\in\mathcal{N}_{i}^{+},$
    |  |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle w_{i,t}^{(1)}\geq 0,\quad t\in\mathcal{N}_{i}^{+},$
    |  |'
- en: 'where $\lambda_{1}>0$ is the regularization parameter. Let $\boldsymbol{w}_{i}^{(2)}=[w_{i,t}^{(2)},\forall
    t\in\mathcal{N}_{i}^{+}]^{\top}\in\mathbb{R}^{k}$ denote the coefficients for
    linear reconstruction of $\boldsymbol{x}_{i}$ by its $k$ nearest neighbors. It
    is obtained as:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1}>0$ 是正则化参数。令 $\boldsymbol{w}_{i}^{(2)}=[w_{i,t}^{(2)},\forall
    t\in\mathcal{N}_{i}^{+}]^{\top}\in\mathbb{R}^{k}$ 表示通过 $k$ 个最近邻对 $\boldsymbol{x}_{i}$
    进行线性重建的系数。它的获得方式如下：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{i}^{(2)}}{\text{minimize}}$
    |  | $\displaystyle\big{\&#124;}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\big{\&#124;}_{2}^{2}$
    |  |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{i}^{(2)}}{\text{minimize}}$
    |  | $\displaystyle\big{\&#124;}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\big{\&#124;}_{2}^{2}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle w_{i,t}^{(2)}\geq 0,\quad t\in\mathcal{N}_{i}^{+}.$
    |  |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 满足 |  | $\displaystyle w_{i,t}^{(2)}\geq 0,\quad t\in\mathcal{N}_{i}^{+}.$
    |  |'
- en: These two optimization problems are quadratic programming and can be solved
    using the interior point method (Ghojogh et al., [2021c](#bib.bib48)).
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个优化问题是二次规划问题，可以使用内点法解决 (Ghojogh et al., [2021c](#bib.bib48))。
- en: 'The main optimization problem of GMML for partial labels is (Zhou & Gu, [2018](#bib.bib145)):'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: GMML 对部分标签的主要优化问题是 (Zhou & Gu, [2018](#bib.bib145))：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    |  | (73) |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})+\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    |  | (73) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 满足 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: 'where:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{i=1}^{n}\Bigg{(}\frac{\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})^{\top}}{\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}}$
    |  |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{i=1}^{n}\Bigg{(}\frac{\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})^{\top}}{\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(1)}}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\Big{(}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\Big{)}\Big{(}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\Big{)}^{\top}\Bigg{)},$
    |  |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\Big{(}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\Big{)}\Big{(}\boldsymbol{x}_{i}-\sum_{t\in\mathcal{N}_{i}^{+}}w_{i,t}^{(2)}\boldsymbol{x}_{t}\Big{)}^{\top}\Bigg{)},$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{i=1}^{n}\sum_{t\in\mathcal{N}_{i}^{-}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})^{\top}.$
    |  |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{i=1}^{n}\sum_{t\in\mathcal{N}_{i}^{-}}(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})(\boldsymbol{x}_{i}-\boldsymbol{x}_{t})^{\top}.$
    |  |'
- en: 'Minimizing the first term of $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ in
    $\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$ decreases
    the distances of similar points which share some candidate labels. Minimizing
    the second term of $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ in $\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$
    tries to preserve linear reconstruction of $\boldsymbol{x}_{i}$ by its neighbors
    after projection onto the subspace of metric. Minimizing $\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    increases the the distances of dissimilar points which do not share any candidate
    labels. The problem ([73](#S3.E73 "In 3.7.3 Geometric Mean Metric Learning for
    Partial Labels ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is
    similar to the problem ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning
    ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")); hence, its solution
    is similar to Eq. ([65](#S3.E65 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7
    Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), i.e., $\boldsymbol{W}={\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$
    (see Eq. ([64](#S3.E64 "In Lemma 5 ((Bhatia, 2007, Chapter 6)). ‣ 3.7.1 Geometric
    Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    for the definition of $\sharp_{t}$).'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '最小化 $\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$
    中的第一项 $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ 会减少共享某些候选标签的相似点的距离。最小化 $\textbf{tr}(\boldsymbol{W}\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}})$
    中的第二项 $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ 则试图在投影到度量子空间后保持 $\boldsymbol{x}_{i}$
    通过其邻居的线性重构。最小化 $\textbf{tr}(\boldsymbol{W}^{-1}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}})$
    会增加不共享任何候选标签的不同点之间的距离。问题（[73](#S3.E73 "In 3.7.3 Geometric Mean Metric Learning
    for Partial Labels ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")）类似于问题（[62](#S3.E62
    "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")），因此其解也类似于 Eq. ([65](#S3.E65 "In 3.7.1 Geometric Mean Metric
    Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))，即
    $\boldsymbol{W}={\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$（参见
    Eq. ([64](#S3.E64 "In Lemma 5 ((Bhatia, 2007, Chapter 6)). ‣ 3.7.1 Geometric Mean
    Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 对
    $\sharp_{t}$ 的定义）。'
- en: 3.7.4 Geometric Mean Metric Learning on SPD and Grassmannian Manifolds
  id: totrans-544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.4 在 SPD 和 Grassmannian 流形上的几何均值度量学习
- en: 'The GMML method (Zadeh et al., [2016](#bib.bib141)), introduced in Section
    [3.7.1](#S3.SS7.SSS1 "3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"), can be implemented on Symmetric Positive
    Definite (SPD) and Grassmannian manifolds (Zhu et al., [2018](#bib.bib146)). If
    $\boldsymbol{X}_{i},\boldsymbol{X}_{j}\in\mathcal{S}_{++}^{d}$ is a point on the
    SPD manifold, the distance metric on this manifold is (Zhu et al., [2018](#bib.bib146)):'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 'GMML 方法（Zadeh et al., [2016](#bib.bib141)），在[3.7.1节](#S3.SS7.SSS1 "3.7.1 Geometric
    Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")中介绍，可以在对称正定（SPD）流形和
    Grassmannian 流形（Zhu et al., [2018](#bib.bib146)）上实现。如果 $\boldsymbol{X}_{i},\boldsymbol{X}_{j}\in\mathcal{S}_{++}^{d}$
    是 SPD 流形上的点，那么该流形上的距离度量为（Zhu et al., [2018](#bib.bib146)）：'
- en: '|  | $\displaystyle d_{\boldsymbol{W}}(\boldsymbol{T}_{i},\boldsymbol{T}_{j}):=\textbf{tr}\big{(}\boldsymbol{W}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)},$
    |  | (74) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d_{\boldsymbol{W}}(\boldsymbol{T}_{i},\boldsymbol{T}_{j}):=\textbf{tr}\big{(}\boldsymbol{W}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)},$
    |  | (74) |'
- en: 'where $\boldsymbol{W}\in\mathbb{R}^{d\times d}$ is the weight matrix of metric
    and $\boldsymbol{T}_{i}:=\log(\boldsymbol{X}_{i})$ is the logarithm operation
    on the SPD manifold. The Grassmannian manifold $Gr(k,d)$ is the $k$-dimensional
    subspaces of the $d$-dimensional vector space. A point in $Gr(k,d)$ is a linear
    subspace spanned by a full-rank $\boldsymbol{X}_{i}\in\mathbb{R}^{d\times k}$
    which is orthogonal, i.e., $\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}=\boldsymbol{I}$.
    If $\boldsymbol{M}\in\mathbb{R}^{d\times r}$ is any matrix, We define $\boldsymbol{X}^{\prime}_{i}$
    in a way that $\boldsymbol{M}^{\top}\boldsymbol{X}^{\prime}_{i}$ is the orthogonal
    components of $\boldsymbol{M}^{\top}\boldsymbol{X}_{i}$. If $\mathbb{R}^{d\times
    d}\ni\boldsymbol{T}_{ij}:=\boldsymbol{X}^{\prime}_{i}\boldsymbol{X}^{{}^{\prime}\top}_{i}-\boldsymbol{X}^{\prime}_{j}\boldsymbol{X}^{{}^{\prime}\top}_{j}$,
    the distance on the Grassmannian manifold is (Zhu et al., [2018](#bib.bib146)):'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{W}\in\mathbb{R}^{d\times d}$ 是度量的权重矩阵，$\boldsymbol{T}_{i}:=\log(\boldsymbol{X}_{i})$
    是SPD流形上的对数运算。Grassmann流形 $Gr(k,d)$ 是 $d$ 维向量空间的 $k$ 维子空间。$Gr(k,d)$ 中的一点是由一个满秩的
    $\boldsymbol{X}_{i}\in\mathbb{R}^{d\times k}$ 张成的线性子空间，该子空间是正交的，即 $\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}=\boldsymbol{I}$。如果
    $\boldsymbol{M}\in\mathbb{R}^{d\times r}$ 是任意矩阵，我们定义 $\boldsymbol{X}^{\prime}_{i}$
    使得 $\boldsymbol{M}^{\top}\boldsymbol{X}^{\prime}_{i}$ 是 $\boldsymbol{M}^{\top}\boldsymbol{X}_{i}$
    的正交分量。如果 $\mathbb{R}^{d\times d}\ni\boldsymbol{T}_{ij}:=\boldsymbol{X}^{\prime}_{i}\boldsymbol{X}^{{}^{\prime}\top}_{i}-\boldsymbol{X}^{\prime}_{j}\boldsymbol{X}^{{}^{\prime}\top}_{j}$,
    Grassmann流形上的距离为 (Zhu 等, [2018](#bib.bib146))：
- en: '|  | $\displaystyle d_{\boldsymbol{W}}(\boldsymbol{T}_{ij}):=\textbf{tr}\big{(}\boldsymbol{W}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)},$
    |  | (75) |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d_{\boldsymbol{W}}(\boldsymbol{T}_{ij}):=\textbf{tr}\big{(}\boldsymbol{W}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)},$
    |  | (75) |'
- en: $\boldsymbol{W}\in\mathbb{R}^{d\times d}$ is the weight matrix of metric.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: $\boldsymbol{W}\in\mathbb{R}^{d\times d}$ 是度量的权重矩阵。
- en: 'Similar to the optimization problem of GMML, i.e. Eq. ([61](#S3.E61 "In 3.7.1
    Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), we solve the following problem for the SPD manifold:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GMML的优化问题，即 Eq. ([61](#S3.E61 "在 3.7.1 几何均值度量学习 ‣ 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣
    谱的、概率的和深度度量学习：教程与调查")), 我们为SPD流形解决以下问题：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{W}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)}$
    |  | (76) |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{W}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)}$
    |  | (76) |'
- en: '|  |  | $\displaystyle+\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{W}^{-1}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)}$
    |  |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{W}^{-1}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})\big{)}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
- en: 'Likewise, for the Grassmannian manifold, the optimization problem is:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于Grassmann流形，优化问题为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{W}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)}$
    |  | (77) |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\textbf{tr}\big{(}\boldsymbol{W}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)}$
    |  | (77) |'
- en: '|  |  | $\displaystyle+\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{W}^{-1}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)}$
    |  |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\textbf{tr}\big{(}\boldsymbol{W}^{-1}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}\big{)}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
- en: 'Suppose, for the SPD manifold, we define:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，对于SPD流形，我们定义：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j}),$
    |  |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j}),$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j}).$
    |  |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}(\boldsymbol{T}_{i}-\boldsymbol{T}_{j})(\boldsymbol{T}_{i}-\boldsymbol{T}_{j}).$
    |  |'
- en: 'and, for the Grassmannian manifold, we define:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Grassmannian 流形，我们定义：
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij},$
    |  |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{S}}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij},$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}.$
    |  |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}:=\sum_{(\boldsymbol{T}_{i},\boldsymbol{T}_{j})\in\mathcal{D}}\boldsymbol{T}_{ij}\boldsymbol{T}_{ij}.$
    |  |'
- en: 'Hence, for either SPD or Grassmannian manifold, the optimization problem becomes
    Eq. ([62](#S3.E62 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) in which $\boldsymbol{\Sigma}_{\mathcal{S}}$
    and $\boldsymbol{\Sigma}_{\mathcal{D}}$ are replaced with $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$
    and $\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$, respectively.'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于 SPD 或 Grassmannian 流形，优化问题变为方程 ([62](#S3.E62 "在 3.7.1 几何均值度量学习 ‣ 3.7 几何谱度量学习
    ‣ 3 谱度量学习 ‣ 谱、概率和深度度量学习：教程和综述"))，其中 $\boldsymbol{\Sigma}_{\mathcal{S}}$ 和 $\boldsymbol{\Sigma}_{\mathcal{D}}$
    被分别替换为 $\boldsymbol{\Sigma}^{\prime}_{\mathcal{S}}$ 和 $\boldsymbol{\Sigma}^{\prime}_{\mathcal{D}}$。
- en: 3.7.5 Metric Learning on Stiefel and SPD Manifolds
  id: totrans-565
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.5 在 Stiefel 和 SPD 流形上的度量学习
- en: 'According to Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), the weight matrix in the metric can be
    decomposed as $\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}$.
    If we do not restrict $\boldsymbol{V}$ and $\boldsymbol{\Lambda}$ to be the matrices
    of eigenvectors and eigenvalues as in Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), we can learn both $\boldsymbol{V}\in\mathbb{R}^{d\times
    p}$ and $\boldsymbol{\Lambda}\in\mathbb{R}^{p\times p}$ by optimization (Harandi
    et al., [2017](#bib.bib60)). The optimization problem in this method is:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程 ([9](#S2.E9 "在证明 ‣ 2.3 广义 Mahalanobis 距离 ‣ 2 广义 Mahalanobis 距离度量 ‣ 谱、概率和深度度量学习：教程和综述"))，度量中的权重矩阵可以分解为
    $\boldsymbol{W}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}$。如果我们不限制
    $\boldsymbol{V}$ 和 $\boldsymbol{\Lambda}$ 为像方程 ([9](#S2.E9 "在证明 ‣ 2.3 广义 Mahalanobis
    距离 ‣ 2 广义 Mahalanobis 距离度量 ‣ 谱、概率和深度度量学习：教程和综述")) 中的特征向量和特征值矩阵，我们可以通过优化学习 $\boldsymbol{V}\in\mathbb{R}^{d\times
    p}$ 和 $\boldsymbol{\Lambda}\in\mathbb{R}^{p\times p}$ (Harandi et al., [2017](#bib.bib60))。该方法中的优化问题是：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{V},\boldsymbol{\Lambda}}{\text{minimize}}$
    |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\log(1+q_{ij})$
    |  | (78) |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{V},\boldsymbol{\Lambda}}{\text{最小化}}$
    |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\log(1+q_{ij})$
    |  | (78) |'
- en: '|  |  | $\displaystyle+\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\log(1+q_{ij}^{-1})$
    |  |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\log(1+q_{ij}^{-1})$
    |  |'
- en: '|  |  | $\displaystyle+\lambda\Big{(}\textbf{tr}(\boldsymbol{\Lambda}\boldsymbol{\Lambda}_{0}^{-1})-\log\big{(}\textbf{det}(\boldsymbol{\Lambda}\boldsymbol{\Lambda}_{0}^{-1})\big{)}-p\Big{)}$
    |  |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda\Big{(}\textbf{tr}(\boldsymbol{\Lambda}\boldsymbol{\Lambda}_{0}^{-1})-\log\big{(}\textbf{det}(\boldsymbol{\Lambda}\boldsymbol{\Lambda}_{0}^{-1})\big{)}-p\Big{)}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I},$
    |  |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I},$
    |  |'
- en: '|  |  | $\displaystyle\boldsymbol{\Lambda}\succeq\boldsymbol{0},$ |  |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{\Lambda}\succeq\boldsymbol{0},$ |  |'
- en: 'where $\lambda>0$ is the regularization parameter, $\textbf{det}(.)$ denotes
    the determinant of matrix, and $q_{ij}$ models Gaussian distribution with the
    generalized Mahalanobis distance metric:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 是正则化参数，$\textbf{det}(.)$ 表示矩阵的行列式，$q_{ij}$ 以广义 Mahalanobis 距离度量建模高斯分布：
- en: '|  | $\displaystyle q_{ij}:=\exp(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}}).$
    |  |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q_{ij}:=\exp(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}}).$
    |  |'
- en: The constraint $\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}$ means that
    the matrix $\boldsymbol{V}$ belongs to the Stiefel manifold $\text{St}(p,d):=\{\boldsymbol{V}\in\mathbb{R}^{d\times
    p}|\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}\}$ and the constraint $\boldsymbol{\Lambda}\succeq\boldsymbol{0}$
    means $\boldsymbol{\Lambda}$ belongs to the SPD manifold $\mathcal{S}^{p}_{++}$.
    Hence, these two variables belong to the product manifold $\text{St}(p,d)\times\mathcal{S}^{p}_{++}$.
    Hence, we can solve this optimization problem using Riemannian optimization methods
    (Absil et al., [2009](#bib.bib1)). This method can also be kernelized; the reader
    can refer to (Harandi et al., [2017](#bib.bib60), Section 4) for its kernel version.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 约束$\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}$意味着矩阵$\boldsymbol{V}$属于Stiefel流形$\text{St}(p,d):=\{\boldsymbol{V}\in\mathbb{R}^{d\times
    p}|\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}\}$，而约束$\boldsymbol{\Lambda}\succeq\boldsymbol{0}$意味着$\boldsymbol{\Lambda}$属于SPD流形$\mathcal{S}^{p}_{++}$。因此，这两个变量属于乘积流形$\text{St}(p,d)\times\mathcal{S}^{p}_{++}$。因此，我们可以使用黎曼优化方法（Absil
    et al., [2009](#bib.bib1)）来解决这个优化问题。这种方法也可以进行核化，读者可以参考(Harandi et al., [2017](#bib.bib60)，Section
    4)了解其核版本。
- en: 3.7.6 Curvilinear Distance Metric Learning (CDML)
  id: totrans-575
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.6 曲线距离度量学习（CDML）
- en: Lemma 6  ((Chen et al., [2019](#bib.bib18))).
  id: totrans-576
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理6 ((Chen et al., [2019](#bib.bib18)))。
- en: 'The generalized Mahalanobis distance can be restated as:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 广义马哈拉诺比斯距离可以重新表述为：
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\Big{(}\int_{T_{l}(\boldsymbol{x}_{i})}^{T_{l}(\boldsymbol{x}_{j})}\&#124;\boldsymbol{u}_{l}\&#124;_{2}\,dt\Big{)}^{2},$
    |  | (79) |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\Big{(}\int_{T_{l}(\boldsymbol{x}_{i})}^{T_{l}(\boldsymbol{x}_{j})}\&#124;\boldsymbol{u}_{l}\&#124;_{2}\,dt\Big{)}^{2},$
    |  | (79) |'
- en: 'where $\boldsymbol{u}_{l}\in\mathbb{R}^{d}$ is the $l$-th column of $\boldsymbol{U}$
    in Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), $t\in\mathbb{R}$, and $T_{l}(\boldsymbol{x})\in\mathbb{R}$
    is the projection of $\boldsymbol{x}$ satisfying $(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$.'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$\boldsymbol{u}_{l}\in\mathbb{R}^{d}$是方程（[9](#S2.E9 "In Proof. ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")）中$\boldsymbol{U}$的第$l$列，$t\in\mathbb{R}$，$T_{l}(\boldsymbol{x})\in\mathbb{R}$是$\boldsymbol{x}$的投影，满足$(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$。'
- en: Proof.
  id: totrans-580
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
- en: '|  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})=\&#124;\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(\ref{equation_W_U_UT})}{=}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})=\&#124;\boldsymbol{U}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\&#124;_{2}^{2}$
    |  |'
- en: '|  | $\displaystyle=\&#124;[\boldsymbol{u}_{1}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}),\dots,\boldsymbol{u}_{p}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})]^{\top}\&#124;_{2}^{2}$
    |  |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\&#124;[\boldsymbol{u}_{1}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}),\dots,\boldsymbol{u}_{p}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})]^{\top}\&#124;_{2}^{2}$
    |  |'
- en: '|  | $\displaystyle=\sum_{l=1}^{p}\big{(}\boldsymbol{u}_{l}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}^{2}$
    |  |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\sum_{l=1}^{p}\big{(}\boldsymbol{u}_{l}^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}^{2}$
    |  |'
- en: '|  | $\displaystyle\overset{(a)}{=}\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{2}^{2}\cos^{2}(\boldsymbol{u}_{l},\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(a)}{=}\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{2}^{2}\cos^{2}(\boldsymbol{u}_{l},\boldsymbol{x}_{i}-\boldsymbol{x}_{j})$
    |  |'
- en: '|  | $\displaystyle\overset{(b)}{=}\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\&#124;\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{i})-\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{j})\&#124;_{2}^{2},$
    |  |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(b)}{=}\sum_{l=1}^{p}\&#124;\boldsymbol{u}_{l}\&#124;_{2}^{2}\&#124;\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{i})-\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{j})\&#124;_{2}^{2},$
    |  |'
- en: 'where $(a)$ is because of the law of cosines and $(b)$ is because of $(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$.
    The distance $\|\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{i})-\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{j})\|_{2}$
    can be replaced by the length of the arc between $T_{l}(\boldsymbol{x}_{i})$ and
    $T_{l}(\boldsymbol{x}_{j})$ on the straight line $\boldsymbol{u}_{l}t$ for $t\in\mathbb{R}$.
    This gives the Eq. ([79](#S3.E79 "In Lemma 6 ((Chen et al., 2019)). ‣ 3.7.6 Curvilinear
    Distance Metric Learning (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). Q.E.D. ∎'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于余弦定理，而 $(b)$ 是因为 $(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$。距离
    $\|\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{i})-\boldsymbol{u}_{l}T_{l}(\boldsymbol{x}_{j})\|_{2}$
    可以用直线 $\boldsymbol{u}_{l}t$ 上 $T_{l}(\boldsymbol{x}_{i})$ 和 $T_{l}(\boldsymbol{x}_{j})$
    之间的弧长替代，$t\in\mathbb{R}$。这给出了等式 ([79](#S3.E79 "在引理6 ((Chen et al., 2019)). ‣ 3.7.6
    曲线距离度量学习 (CDML) ‣ 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率及深度度量学习：教程与调查"))。证明完毕 ∎
- en: 'The condition $(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$
    is equivalent to finding the nearest neighbor to the line $\boldsymbol{u}_{l}t,\forall
    t\in\mathbb{R}$, i.e., $T_{l}(\boldsymbol{x}):=\arg\min_{t\in\mathbb{R}}\|\boldsymbol{u}_{l}t-\boldsymbol{x}\|_{2}^{2}$
    (Chen et al., [2019](#bib.bib18)). This equation can be generalized to find the
    nearest neighbor to the geodesic curve $\boldsymbol{\theta}_{l}(t)$ rather than
    the line $\boldsymbol{u}_{l}t$:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 条件 $(\boldsymbol{u}_{l}T_{l}(\boldsymbol{x})-\boldsymbol{x})^{\top}\boldsymbol{u}_{l}=0$
    等价于找到直线 $\boldsymbol{u}_{l}t,\forall t\in\mathbb{R}$ 的最近邻，即 $T_{l}(\boldsymbol{x}):=\arg\min_{t\in\mathbb{R}}\|\boldsymbol{u}_{l}t-\boldsymbol{x}\|_{2}^{2}$
    (Chen et al., [2019](#bib.bib18))。这个方程可以推广为找到测地线曲线 $\boldsymbol{\theta}_{l}(t)$
    的最近邻，而不是直线 $\boldsymbol{u}_{l}t$：
- en: '|  | $\displaystyle T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}):=\arg\min_{t\in\mathbb{R}}\&#124;\boldsymbol{\theta}_{l}(t)-\boldsymbol{x}\&#124;_{2}^{2}.$
    |  | (80) |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}):=\arg\min_{t\in\mathbb{R}}\&#124;\boldsymbol{\theta}_{l}(t)-\boldsymbol{x}\&#124;_{2}^{2}.$
    |  | (80) |'
- en: 'Hence, we can replace the arc length of the straight line in Eq. ([79](#S3.E79
    "In Lemma 6 ((Chen et al., 2019)). ‣ 3.7.6 Curvilinear Distance Metric Learning
    (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) with
    the arc length of the curve:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以用曲线的弧长替换等式中的直线弧长 ([79](#S3.E79 "在引理6 ((Chen et al., 2019)). ‣ 3.7.6 曲线距离度量学习
    (CDML) ‣ 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率及深度度量学习：教程与调查"))：
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\sum_{l=1}^{p}\alpha_{l}\Big{(}\int_{T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}_{i})}^{T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}_{j})}\&#124;\boldsymbol{\theta}^{\prime}_{l}(t)\&#124;_{2}\,dt\Big{)}^{2},$
    |  | (81) |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}=\sum_{l=1}^{p}\alpha_{l}\Big{(}\int_{T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}_{i})}^{T_{\boldsymbol{\theta}_{l}}(\boldsymbol{x}_{j})}\&#124;\boldsymbol{\theta}^{\prime}_{l}(t)\&#124;_{2}\,dt\Big{)}^{2},$
    |  | (81) |'
- en: 'where $\boldsymbol{\theta}^{\prime}_{l}(t)$ is derivative of $\boldsymbol{\theta}_{l}(t)$
    w.r.t. $t$ and $\alpha_{l}:=(\int_{0}^{1}\|\boldsymbol{\theta}^{\prime}_{l}(t)\|_{2}\,dt)^{2}$
    is the scale factor. The Curvilinear Distance Metric Learning (CDML) (Chen et al.,
    [2019](#bib.bib18)) uses this approximation of distance metric by the above curvy
    geodesic on manifold, i.e., Eq. ([81](#S3.E81 "In 3.7.6 Curvilinear Distance Metric
    Learning (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")). The
    optimization problem in CDML is:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\theta}^{\prime}_{l}(t)$ 是 $\boldsymbol{\theta}_{l}(t)$ 关于 $t$
    的导数，$\alpha_{l}:=(\int_{0}^{1}\|\boldsymbol{\theta}^{\prime}_{l}(t)\|_{2}\,dt)^{2}$
    是尺度因子。曲线距离度量学习 (CDML) (Chen et al., [2019](#bib.bib18)) 使用了上面曲线测地线的距离度量近似，即等式
    ([81](#S3.E81 "在 3.7.6 曲线距离度量学习 (CDML) ‣ 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱、概率及深度度量学习：教程与调查"))。CDML
    中的优化问题为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{\Theta}}{\text{minimize}}$ |  |
    $\displaystyle\frac{1}{n}\sum_{i=1}^{n}\mathcal{L}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2};y_{ij})+\lambda\Omega(\boldsymbol{\Theta}),$
    |  | (82) |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{\Theta}}{\text{minimize}}$ |  |
    $\displaystyle\frac{1}{n}\sum_{i=1}^{n}\mathcal{L}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2};y_{ij})+\lambda\Omega(\boldsymbol{\Theta}),$
    |  | (82) |'
- en: 'where $n$ is the number of points, $\boldsymbol{\Theta}:=[\boldsymbol{\theta}_{1},\dots,\boldsymbol{\theta}_{p}]$,
    $y_{ij}=1$ if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$ and $y_{ij}=0$
    if $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$, $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$
    is defined in Eq. ([81](#S3.E81 "In 3.7.6 Curvilinear Distance Metric Learning
    (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), $\lambda>0$
    is the regularization parameter, $\mathcal{L}(.)$ is some loss function, and $\Omega(\boldsymbol{\Theta})$
    is some penalty term. The optimal $\boldsymbol{\Theta}$, obtained from Eq. ([82](#S3.E82
    "In 3.7.6 Curvilinear Distance Metric Learning (CDML) ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), can be used in Eq. ([81](#S3.E81 "In
    3.7.6 Curvilinear Distance Metric Learning (CDML) ‣ 3.7 Geometric Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) to have the optimal distance metric. A recent
    follow-up of CDML is (Zhang et al., [2021](#bib.bib143)).'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $n$ 是点的数量，$\boldsymbol{\Theta}:=[\boldsymbol{\theta}_{1},\dots,\boldsymbol{\theta}_{p}]$，如果
    $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$ 则 $y_{ij}=1$，如果 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$
    则 $y_{ij}=0$，$\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}$
    在 Eq. ([81](#S3.E81 "In 3.7.6 Curvilinear Distance Metric Learning (CDML) ‣ 3.7
    Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) 中定义，$\lambda>0$ 是正则化参数，$\mathcal{L}(.)$
    是某种损失函数，$\Omega(\boldsymbol{\Theta})$ 是某种惩罚项。从 Eq. ([82](#S3.E82 "In 3.7.6 Curvilinear
    Distance Metric Learning (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) 中获得的最优 $\boldsymbol{\Theta}$ 可以在 Eq. ([81](#S3.E81 "In 3.7.6 Curvilinear
    Distance Metric Learning (CDML) ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) 中使用，以获得最优距离度量。CDML 的最新进展是 (Zhang et al., [2021](#bib.bib143))。'
- en: 3.8 Adversarial Metric Learning (AML)
  id: totrans-595
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 对抗性度量学习 (AML)
- en: Adversarial Metric Learning (AML) (Chen et al., [2018](#bib.bib17)) uses adversarial
    learning (Goodfellow et al., [2014](#bib.bib54); Ghojogh et al., [2021b](#bib.bib47))
    for metric learning. On one hand, we have a distinguishment stage which tries
    to discriminate the dissimilar points and push similar points close to one another.
    On the other hand, we have an confusion or adversarial stage which tries to fool
    the metric learning method by pulling the dissimilar points close to each other
    and pushing the similar points away. The distinguishment and confusion stages
    are trained simultaneously and they make each other stronger gradually.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性度量学习 (AML) (Chen et al., [2018](#bib.bib17)) 使用对抗学习 (Goodfellow et al., [2014](#bib.bib54);
    Ghojogh et al., [2021b](#bib.bib47)) 来进行度量学习。一方面，我们有一个区分阶段，试图区分不相似的点并将相似的点拉近。另一方面，我们有一个混淆或对抗阶段，试图通过将不相似的点拉近并将相似的点推开，来欺骗度量学习方法。区分阶段和混淆阶段同时训练，它们逐渐增强彼此的效果。
- en: 'From the dataset, we form random pairs $\mathcal{X}:=\{(\boldsymbol{x}_{i},\boldsymbol{x}^{\prime}_{i})\}_{i=1}^{n/2}$.
    If $\boldsymbol{x}_{i}$ and $\boldsymbol{x}^{\prime}_{i}$ are similar points,
    we set $y_{i}=1$ and if they are dissimilar, we have $y_{i}=-1$. We also generate
    some random new points in pairs $\mathcal{X}^{g}:=\{(\boldsymbol{x}_{i}^{g},\boldsymbol{x}^{g^{\prime}}_{i})\}_{i=1}^{n/2}$.
    The generated points are updated iteratively by optimization of the confusion
    stage to fool the metric. The loss functions for both stages are Eq. ([61](#S3.E61
    "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) used in geometric mean metric learning (see Section [3.7.1](#S3.SS7.SSS1
    "3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")).'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '从数据集中，我们形成随机对 $\mathcal{X}:=\{(\boldsymbol{x}_{i},\boldsymbol{x}^{\prime}_{i})\}_{i=1}^{n/2}$。如果
    $\boldsymbol{x}_{i}$ 和 $\boldsymbol{x}^{\prime}_{i}$ 是相似的点，我们设置 $y_{i}=1$；如果它们不相似，我们设
    $y_{i}=-1$。我们还生成一些随机的新点对 $\mathcal{X}^{g}:=\{(\boldsymbol{x}_{i}^{g},\boldsymbol{x}^{g^{\prime}}_{i})\}_{i=1}^{n/2}$。生成的点通过混淆阶段的优化进行迭代更新，以欺骗度量。两个阶段的损失函数为几何均值度量学习中使用的方程
    ([61](#S3.E61 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"))（见 [3.7.1](#S3.SS7.SSS1 "3.7.1 Geometric
    Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")）。'
- en: 'The alternative optimization (Ghojogh et al., [2021c](#bib.bib48)) used in
    AML is:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 替代优化方法（Ghojogh 等，[2021c](#bib.bib48)）在 AML 中使用：
- en: '|  |  | $\displaystyle\boldsymbol{W}^{(t+1)}:=\arg\min_{\boldsymbol{W}}\Big{(}\sum_{y_{i}=1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{W}^{(t+1)}:=\arg\min_{\boldsymbol{W}}\Big{(}\sum_{y_{i}=1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
- en: '|  |  | $\displaystyle+\sum_{y_{i}=-1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}^{-1}}^{2}+\lambda_{1}\big{(}\sum_{y_{i}=1}\&#124;\boldsymbol{x}^{g(t)}_{i}-\boldsymbol{x}^{g^{\prime}(t)}_{i}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{y_{i}=-1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}^{-1}}^{2}+\lambda_{1}\big{(}\sum_{y_{i}=1}\&#124;\boldsymbol{x}^{g(t)}_{i}-\boldsymbol{x}^{g^{\prime}(t)}_{i}\&#124;_{\boldsymbol{W}}^{2}$
    |  |'
- en: '|  |  | $\displaystyle+\sum_{y_{i}=-1}\&#124;\boldsymbol{x}^{g(t)}_{i}-\boldsymbol{x}^{g^{\prime}(t)}_{i}\&#124;_{\boldsymbol{W}^{-1}}^{2}\big{)}\Big{)},$
    |  |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{y_{i}=-1}\&#124;\boldsymbol{x}^{g(t)}_{i}-\boldsymbol{x}^{g^{\prime}(t)}_{i}\&#124;_{\boldsymbol{W}^{-1}}^{2}\big{)}\Big{)},$
    |  |'
- en: '|  |  | $\displaystyle{\mathcal{X}}^{g(t+1)}:=\arg\min_{\mathcal{X}^{\prime}}\Big{(}\sum_{y_{i}=-1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}$
    |  | (83) |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\mathcal{X}}^{g(t+1)}:=\arg\min_{\mathcal{X}^{\prime}}\Big{(}\sum_{y_{i}=-1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}$
    |  | (83) |'
- en: '|  |  | $\displaystyle+\sum_{y_{i}=1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{(\boldsymbol{W}^{(t+1)})^{-1}}^{2}+\lambda_{2}\big{(}\sum_{i=1}^{n/2}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{g}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}$
    |  |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{y_{i}=1}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{\prime}_{i}\&#124;_{(\boldsymbol{W}^{(t+1)})^{-1}}^{2}+\lambda_{2}\big{(}\sum_{i=1}^{n/2}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}^{g}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}$
    |  |'
- en: '|  |  | $\displaystyle+\sum_{i=1}^{n/2}\&#124;\boldsymbol{x}^{\prime}_{i}-\boldsymbol{x}^{g^{\prime}}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}\big{)}\Big{)},$
    |  |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{i=1}^{n/2}\&#124;\boldsymbol{x}^{\prime}_{i}-\boldsymbol{x}^{g^{\prime}}_{i}\&#124;_{\boldsymbol{W}^{(t+1)}}^{2}\big{)}\Big{)},$
    |  |'
- en: until convergence, where $\lambda_{1},\lambda_{2}>0$ are the regularization
    parameters. Updating $\boldsymbol{W}$ and $\mathcal{X}^{g}$ are the distinguishment
    and confusion stages, respectively. In the distinguishment stage, we find a weight
    matrix $\boldsymbol{W}$ to minimize the distances of similar points in both $\mathcal{X}$
    and $\mathcal{X}^{g}$ and maximize the distances of dissimilar points in both
    $\mathcal{X}$ and $\mathcal{X}^{g}$. In the confusion stage, we generate new points
    $\mathcal{X}^{g}$ to adversarially maximize the distances of similar points in
    $\mathcal{X}$ and adversarially minimize the distances of dissimilar points in
    $\mathcal{X}$. In this stage, we also make the points $\boldsymbol{x}_{i}^{g}$
    and $\boldsymbol{x}_{i}^{g^{\prime}}$ similar to their corresponding points $\boldsymbol{x}_{i}$
    and $\boldsymbol{x}^{\prime}_{i}$, respectively.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 直到收敛，其中$\lambda_{1},\lambda_{2}>0$是正则化参数。更新$\boldsymbol{W}$和$\mathcal{X}^{g}$分别为区分阶段和混淆阶段。在区分阶段，我们寻找一个权重矩阵$\boldsymbol{W}$，以最小化$\mathcal{X}$和$\mathcal{X}^{g}$中相似点的距离，同时最大化$\mathcal{X}$和$\mathcal{X}^{g}$中不相似点的距离。在混淆阶段，我们生成新的点$\mathcal{X}^{g}$，以对抗性地最大化$\mathcal{X}$中相似点的距离，并对抗性地最小化$\mathcal{X}$中不相似点的距离。在这一阶段，我们还使点$\boldsymbol{x}_{i}^{g}$和$\boldsymbol{x}_{i}^{g^{\prime}}$分别与其对应的点$\boldsymbol{x}_{i}$和$\boldsymbol{x}^{\prime}_{i}$相似。
- en: 4 Probabilistic Metric Learning
  id: totrans-606
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 概率度量学习
- en: Probabilistic methods for metric learning learn the weight matrix in the generalized
    Mahalanobis distance using probability distributions. They define some probability
    distribution for each point accepting other points as its neighbors. Of course,
    the closer points have higher probability for being neighbors.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 概率方法用于度量学习，通过使用概率分布来学习广义马哈拉诺比斯距离中的权重矩阵。它们为每个点定义一些概率分布，接受其他点作为其邻居。当然，距离较近的点被认为是邻居的概率较高。
- en: 4.1 Collapsing Classes
  id: totrans-608
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 类别收缩
- en: 'One probabilistic method for metric learning is collapsing similar points to
    the same class while pushing the dissimilar points away from one another (Globerson
    & Roweis, [2005](#bib.bib52)). The probability distribution between points for
    being neighbors can be a Gaussian distribution which uses the generalized Mahalanobis
    distance as its metric. The distribution for $\boldsymbol{x}_{i}$ to take $\boldsymbol{x}_{j}$
    as its neighbor is (Goldberger et al., [2005](#bib.bib53)):'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于度量学习的概率方法是将相似点收缩到同一类别，同时将不相似点推离（Globerson & Roweis, [2005](#bib.bib52)）。点之间成为邻居的概率分布可以是高斯分布，它使用广义马哈拉诺比斯距离作为其度量。$\boldsymbol{x}_{i}$将$\boldsymbol{x}_{j}$作为其邻居的分布是（Goldberger
    et al., [2005](#bib.bib53)）：
- en: '|  | $\displaystyle p^{W}_{ij}:=\frac{\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})}{\sum_{k\neq
    i}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\&#124;_{\boldsymbol{W}}^{2})},\quad
    j\neq i,$ |  | (84) |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{W}_{ij}:=\frac{\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2})}{\sum_{k\neq
    i}\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\|_{\boldsymbol{W}}^{2})},\quad
    j\neq i,$ |  | (84) |'
- en: 'where we define the normalization factor, also called the partition function,
    as $Z_{i}:=\sum_{k\neq i}\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\|_{\boldsymbol{W}}^{2})$.
    This factor makes the summation of distribution one. Eq. ([84](#S4.E84 "In 4.1
    Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) is a Gaussian distribution whose
    covariance matrix is $\boldsymbol{W}^{-1}$ because it is equivalent to:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义归一化因子，也称为分区函数，为$Z_{i}:=\sum_{k\neq i}\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\|_{\boldsymbol{W}}^{2})$。该因子使得分布的总和为一。方程
    ([84](#S4.E84 "在4.1 类别收缩 ‣ 4 概率度量学习 ‣ 谱、概率和深度度量学习：教程和调查")) 是高斯分布，其协方差矩阵为$\boldsymbol{W}^{-1}$，因为它等同于：
- en: '|  | $\displaystyle p^{W}_{ij}:=\frac{1}{Z_{i}}\exp\big{(}-(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}.$
    |  |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{W}_{ij}:=\frac{1}{Z_{i}}\exp\big{(}-(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})\big{)}.$
    |  |'
- en: 'We want the similar points to collapse to the same point after projection onto
    the subspace of metric (see Proposition [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). Hence, we define the desired neighborhood distribution to be a
    bi-level distribution (Globerson & Roweis, [2005](#bib.bib52)):'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望相似点在投影到度量的子空间后能聚合到同一个点（见命题 [2](#Thmproposition2 "命题 2（度量学习中的投影）。 ‣ 2.3 广义马氏距离
    ‣ 2 广义马氏距离度量 ‣ 谱的、概率的和深度度量学习：教程与调查")）。因此，我们定义期望的邻域分布为双层分布（Globerson & Roweis,
    [2005](#bib.bib52)）：
- en: '|  | $\displaystyle p^{0}_{ij}:=\left\{\begin{array}[]{ll}1&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    0&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (87) |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{0}_{ij}:=\left\{\begin{array}[]{ll}1&\mbox{如果 }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    0&\mbox{如果 }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (87) |'
- en: This makes all similar points of a group/class a same point after projection.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得在投影后，一个组/类的所有相似点都变为同一个点。
- en: 4.1.1 Collapsing Classes in the Input Space
  id: totrans-616
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 输入空间中的类压缩
- en: 'For making $p^{W}_{ij}$ close to the desired distribution $p^{0}_{ij}$, we
    minimize the KL-divergence between them (Globerson & Roweis, [2005](#bib.bib52)):'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使$p^{W}_{ij}$接近期望分布$p^{0}_{ij}$，我们最小化它们之间的KL散度（Globerson & Roweis, [2005](#bib.bib52)）：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}\text{KL}(p^{0}_{ij}\,\&#124;\,p^{W}_{ij})$ |  | (88) |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}\text{KL}(p^{0}_{ij}\,\|\,p^{W}_{ij})$ |  | (88) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: Lemma 7  ((Globerson & Roweis, [2005](#bib.bib52))).
  id: totrans-620
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 7 ((Globerson & Roweis, [2005](#bib.bib52))).
- en: 'Let the the objective function in Eq. ([88](#S4.E88 "In 4.1.1 Collapsing Classes
    in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) be
    denoted by $c:=\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}\text{KL}(p^{0}_{ij}\,\|\,p^{W}_{ij})$.
    The gradient of this function w.r.t. $\boldsymbol{W}$ is:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 设公式 ([88](#S4.E88 "在 4.1.1 输入空间中的类压缩 ‣ 4.1 类压缩 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度度量学习：教程与调查"))
    中的目标函数记作$c:=\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}\text{KL}(p^{0}_{ij}\,\|\,p^{W}_{ij})$。该函数关于$\boldsymbol{W}$的梯度为：
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}(p^{0}_{ij}-p^{W}_{ij})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  | (89) |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}(p^{0}_{ij}-p^{W}_{ij})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  | (89) |'
- en: Proof.
  id: totrans-623
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'The derivation is similar to the derivation of gradient in Stochastic Neighbor
    Embedding (SNE) and t-SNE (Hinton & Roweis, [2003](#bib.bib66); van der Maaten
    & Hinton, [2008](#bib.bib113); Ghojogh et al., [2020c](#bib.bib42)). Let:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 该推导类似于随机邻居嵌入（SNE）和t-SNE中的梯度推导（Hinton & Roweis, [2003](#bib.bib66); van der Maaten
    & Hinton, [2008](#bib.bib113); Ghojogh et al., [2020c](#bib.bib42)）。设：
- en: '|  | $\displaystyle\mathbb{R}\ni r_{ij}:=d_{ij}^{2}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}.$
    |  | (90) |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{R}\ni r_{ij}:=d_{ij}^{2}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}.$
    |  | (90) |'
- en: 'By changing $\boldsymbol{x}_{i}$, we only have change impact in $d_{ij}$ and
    $d_{ji}$ (or $r_{ij}$ and $r_{ji}$) for all $j$’s. According to chain rule, we
    have:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变$\boldsymbol{x}_{i}$，我们仅对所有$j$的$d_{ij}$和$d_{ji}$（或$r_{ij}$和$r_{ji}$）产生变化影响。根据链式法则，我们有：
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\sum_{i,j}\big{(}\frac{\partial
    c}{\partial r_{ij}}\frac{\partial r_{ij}}{\partial\boldsymbol{W}}+\frac{\partial
    c}{\partial r_{ji}}\frac{\partial r_{ji}}{\partial\boldsymbol{W}}\big{)}.$ |  |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\sum_{i,j}\big{(}\frac{\partial
    c}{\partial r_{ij}}\frac{\partial r_{ij}}{\partial\boldsymbol{W}}+\frac{\partial
    c}{\partial r_{ji}}\frac{\partial r_{ji}}{\partial\boldsymbol{W}}\big{)}.$ |  |'
- en: 'According to Eq. ([90](#S4.E90 "In Proof. ‣ 4.1.1 Collapsing Classes in the
    Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we have:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公式 ([90](#S4.E90 "在证明中。 ‣ 4.1.1 输入空间中的类压缩 ‣ 4.1 类压缩 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度度量学习：教程与调查"))，我们有：
- en: '|  | $\displaystyle r_{ij}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))$
    |  |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{ij}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}=\textbf{tr}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W})$
    |  |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}((\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{W})$
    |  |'
- en: '|  | $\displaystyle\implies\frac{\partial r_{ij}}{\partial\boldsymbol{W}}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\frac{\partial r_{ij}}{\partial\boldsymbol{W}}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  |'
- en: '|  | $\displaystyle r_{ji}=&#124;&#124;\boldsymbol{x}_{j}-\boldsymbol{x}_{i}&#124;&#124;_{\boldsymbol{W}}^{2}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}=r_{ij}$
    |  |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{ji}=&#124;&#124;\boldsymbol{x}_{j}-\boldsymbol{x}_{i}&#124;&#124;_{\boldsymbol{W}}^{2}=&#124;&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}&#124;&#124;_{\boldsymbol{W}}^{2}=r_{ij}$
    |  |'
- en: '|  | $\displaystyle\implies\frac{\partial r_{ji}}{\partial\boldsymbol{W}}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\frac{\partial r_{ji}}{\partial\boldsymbol{W}}=(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top},$
    |  |'
- en: 'where $(a)$ is because of the cyclic property of trace. Therefore:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是因为迹的循环特性。因此：
- en: '|  | $\displaystyle\therefore~{}~{}~{}~{}\frac{\partial c}{\partial\boldsymbol{W}}=2\sum_{i,j}\big{(}\frac{\partial
    c}{\partial r_{ij}}\big{)}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  | (91) |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\therefore~{}~{}~{}~{}\frac{\partial c}{\partial\boldsymbol{W}}=2\sum_{i,j}\big{(}\frac{\partial
    c}{\partial r_{ij}}\big{)}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}.$
    |  | (91) |'
- en: 'The dummy variables in cost function can be re-written as:'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数中的虚拟变量可以重新写为：
- en: '|  | $\displaystyle c$ | $\displaystyle=\sum_{k}\sum_{l\neq k}p_{0}(l&#124;k)\log(\frac{p_{0}(l&#124;k)}{p_{W}(l&#124;k)})$
    |  |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c$ | $\displaystyle=\sum_{k}\sum_{l\neq k}p_{0}(l&#124;k)\log(\frac{p_{0}(l&#124;k)}{p_{W}(l&#124;k)})$
    |  |'
- en: '|  |  | $\displaystyle=\sum_{k\neq l}p_{0}(l&#124;k)\log(\frac{p_{0}(l&#124;k)}{p_{W}(l&#124;k)})$
    |  |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{k\neq l}p_{0}(l&#124;k)\log(\frac{p_{0}(l&#124;k)}{p_{W}(l&#124;k)})$
    |  |'
- en: '|  |  | $\displaystyle=\sum_{k\neq l}\big{(}p_{0}(l&#124;k)\log(p_{0}(l&#124;k))-p_{0}(l&#124;k)\log(p_{W}(l&#124;k))\big{)},$
    |  |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{k\neq l}\big{(}p_{0}(l&#124;k)\log(p_{0}(l&#124;k))-p_{0}(l&#124;k)\log(p_{W}(l&#124;k))\big{)},$
    |  |'
- en: 'whose first term is a constant with respect to $p_{W}(l|k)$ and thus to $\boldsymbol{W}$.
    We have:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 其第一项是关于 $p_{W}(l|k)$ 的常数，因此也对 $\boldsymbol{W}$ 是常数。我们有：
- en: '|  | $\displaystyle\mathbb{R}\ni\frac{\partial c}{\partial r_{ij}}=-\sum_{k\neq
    l}p_{0}(l&#124;k)\frac{\partial(\log(p_{W}(l&#124;k)))}{\partial r_{ij}}.$ |  |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{R}\ni\frac{\partial c}{\partial r_{ij}}=-\sum_{k\neq
    l}p_{0}(l&#124;k)\frac{\partial(\log(p_{W}(l&#124;k)))}{\partial r_{ij}}.$ |  |'
- en: 'According to Eqs. ([84](#S4.E84 "In 4.1 Collapsing Classes ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) and ([90](#S4.E90 "In Proof. ‣ 4.1.1 Collapsing Classes in the Input
    Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), the $p_{W}(l|k)$ is:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程 ([84](#S4.E84 "在 4.1 类别折叠 ‣ 4 概率度量学习 ‣ 光谱、概率和深度度量学习：教程和综述")) 和 ([90](#S4.E90
    "在证明 ‣ 4.1.1 输入空间中的类别折叠 ‣ 4.1 类别折叠 ‣ 4 概率度量学习 ‣ 光谱、概率和深度度量学习：教程和综述"))，$p_{W}(l|k)$
    为：
- en: '|  | $\displaystyle p_{W}(l&#124;k):=\frac{\exp(-d_{kl}^{2})}{\sum_{k\neq f}\exp(-d_{kf}^{2})}=\frac{\exp(-r_{kl})}{\sum_{k\neq
    f}\exp(-r_{kf})}.$ |  |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{W}(l&#124;k):=\frac{\exp(-d_{kl}^{2})}{\sum_{k\neq f}\exp(-d_{kf}^{2})}=\frac{\exp(-r_{kl})}{\sum_{k\neq
    f}\exp(-r_{kf})}.$ |  |'
- en: 'We take the denominator of $p_{W}(l|k)$ as:'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 $p_{W}(l|k)$ 的分母表示为：
- en: '|  | $\displaystyle\beta:=\sum_{k\neq f}\exp(-d_{kf}^{2})=\sum_{k\neq f}\exp(-r_{kf}).$
    |  | (92) |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\beta:=\sum_{k\neq f}\exp(-d_{kf}^{2})=\sum_{k\neq f}\exp(-r_{kf}).$
    |  | (92) |'
- en: 'We have $\log(p_{W}(l|k))=\log(p_{W}(l|k))+\log\beta-\log\beta=\log(p_{W}(l|k)\,\beta)-\log\beta$.
    Therefore:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 $\log(p_{W}(l|k))=\log(p_{W}(l|k))+\log\beta-\log\beta=\log(p_{W}(l|k)\,\beta)-\log\beta$。因此：
- en: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=-\sum_{k\neq
    l}p_{0}(l&#124;k)\frac{\partial\big{(}\log(p_{W}(l&#124;k)\beta)-\log\beta\big{)}}{\partial
    r_{ij}}$ |  |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=-\sum_{k\neq
    l}p_{0}(l&#124;k)\frac{\partial\big{(}\log(p_{W}(l&#124;k)\beta)-\log\beta\big{)}}{\partial
    r_{ij}}$ |  |'
- en: '|  | $\displaystyle=-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{\partial\big{(}\log(p_{W}(l&#124;k)\beta)\big{)}}{\partial
    r_{ij}}-\frac{\partial\big{(}\log\beta\big{)}}{\partial r_{ij}}\bigg{]}$ |  |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{\partial\big{(}\log(p_{W}(l&#124;k)\beta)\big{)}}{\partial
    r_{ij}}-\frac{\partial\big{(}\log\beta\big{)}}{\partial r_{ij}}\bigg{]}$ |  |'
- en: '|  | $\displaystyle=-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{1}{p_{W}(l&#124;k)\beta}\frac{\partial\big{(}p_{W}(l&#124;k)\beta\big{)}}{\partial
    r_{ij}}-\frac{1}{\beta}\frac{\partial\beta}{\partial r_{ij}}\bigg{]}.$ |  |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{1}{p_{W}(l&#124;k)\beta}\frac{\partial\big{(}p_{W}(l&#124;k)\beta\big{)}}{\partial
    r_{ij}}-\frac{1}{\beta}\frac{\partial\beta}{\partial r_{ij}}\bigg{]}.$ |  |'
- en: 'The $p_{W}(l|k)\beta$ is:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: $p_{W}(l|k)\beta$ 为：
- en: '|  | $\displaystyle p_{W}(l&#124;k)\beta$ | $\displaystyle=\frac{\exp(-r_{kl})}{\sum_{f\neq
    k}\exp(-r_{kf})}\times\sum_{k\neq f}\exp(-r_{kf})$ |  |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{W}(l&#124;k)\beta$ | $\displaystyle=\frac{\exp(-r_{kl})}{\sum_{f\neq
    k}\exp(-r_{kf})}\times\sum_{k\neq f}\exp(-r_{kf})$ |  |'
- en: '|  |  | $\displaystyle=\exp(-r_{kl}).$ |  |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\exp(-r_{kl}).$ |  |'
- en: 'Therefore, we have:'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有：
- en: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=$
    |  |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=$
    |  |'
- en: '|  | $\displaystyle-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{1}{p_{W}(l&#124;k)\beta}\frac{\partial\big{(}\exp(-r_{kl})\big{)}}{\partial
    r_{ij}}-\frac{1}{\beta}\frac{\partial\beta}{\partial r_{ij}}\bigg{]}.$ |  |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=-\sum_{k\neq l}p_{0}(l&#124;k)\bigg{[}\frac{1}{p_{W}(l&#124;k)\beta}\frac{\partial\big{(}\exp(-r_{kl})\big{)}}{\partial
    r_{ij}}-\frac{1}{\beta}\frac{\partial\beta}{\partial r_{ij}}\bigg{]}.$ |  |'
- en: 'The $\partial\big{(}\exp(-r_{kl})\big{)}/\partial r_{ij}$ is non-zero for only
    $k=i$ and $l=j$; therefore:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: $\partial\big{(}\exp(-r_{kl})\big{)}/\partial r_{ij}$ 只有在 $k=i$ 和 $l=j$ 时非零；因此：
- en: '|  | $\displaystyle\frac{\partial\big{(}\exp(-r_{ij})\big{)}}{\partial r_{ij}}$
    | $\displaystyle=-\exp(-r_{ij}),$ |  |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial\big{(}\exp(-r_{ij})\big{)}}{\partial r_{ij}}$
    | $\displaystyle=-\exp(-r_{ij}),$ |  |'
- en: '|  | $\displaystyle\frac{\partial\beta}{\partial r_{ij}}$ | $\displaystyle=\frac{\partial\sum_{k\neq
    f}\exp(-r_{kf})}{\partial r_{ij}}=\frac{\partial\exp(-r_{ij})}{\partial r_{ij}}$
    |  |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial\beta}{\partial r_{ij}}$ | $\displaystyle=\frac{\partial\sum_{k\neq
    f}\exp(-r_{kf})}{\partial r_{ij}}=\frac{\partial\exp(-r_{ij})}{\partial r_{ij}}$
    |  |'
- en: '|  |  | $\displaystyle=-\exp(-r_{ij}).$ |  |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\exp(-r_{ij}).$ |  |'
- en: 'Therefore:'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 因此：
- en: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=$
    |  |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\therefore~{}~{}~{}\frac{\partial c}{\partial r_{ij}}=$
    |  |'
- en: '|  | $\displaystyle-\bigg{(}p^{0}_{ij}\Big{[}\frac{-1}{p^{W}_{ij}\beta}\exp(-r_{ij})\Big{]}+0+\dots+0\bigg{)}$
    |  |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=-\bigg{(}p^{0}_{ij}\Big{[}\frac{-1}{p^{W}_{ij}\beta}\exp(-r_{ij})\Big{]}+0+\dots+0\bigg{)}$
    |  |'
- en: '|  | $\displaystyle-\sum_{k\neq l}p_{0}(l&#124;k)\Big{[}\frac{1}{\beta}\exp(-r_{ij})\Big{]}.$
    |  |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=-\sum_{k\neq l}p_{0}(l&#124;k)\Big{[}\frac{1}{\beta}\exp(-r_{ij})\Big{]}.$
    |  |'
- en: 'We have $\sum_{k\neq l}p_{0}(l|k)=1$ because summation of all possible probabilities
    is one. Thus:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 $\sum_{k\neq l}p_{0}(l|k)=1$，因为所有可能的概率之和为一。因此：
- en: '|  | $\displaystyle\frac{\partial c}{\partial r_{ij}}$ | $\displaystyle=-p^{0}_{ij}\Big{[}\frac{-1}{p^{W}_{ij}\beta}\exp(-r_{ij})\Big{]}-\Big{[}\frac{1}{\beta}\exp(-r_{ij})\Big{]}$
    |  |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial c}{\partial r_{ij}}$ | $\displaystyle=-p^{0}_{ij}\Big{[}\frac{-1}{p^{W}_{ij}\beta}\exp(-r_{ij})\Big{]}-\Big{[}\frac{1}{\beta}\exp(-r_{ij})\Big{]}$
    |  |'
- en: '|  |  | $\displaystyle=\underbrace{\frac{\exp(-r_{ij})}{\beta}}_{=p^{W}_{ij}}\Big{[}\frac{p^{0}_{ij}}{p^{W}_{ij}}-1\Big{]}=p^{0}_{ij}-p^{W}_{ij}.$
    |  | (93) |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\underbrace{\frac{\exp(-r_{ij})}{\beta}}_{=p^{W}_{ij}}\Big{[}\frac{p^{0}_{ij}}{p^{W}_{ij}}-1\Big{]}=p^{0}_{ij}-p^{W}_{ij}.$
    |  | (93) |'
- en: 'Substituting the obtained derivative in Eq. ([91](#S4.E91 "In Proof. ‣ 4.1.1
    Collapsing Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) gives Eq. ([89](#S4.E89 "In Lemma 7 ((Globerson & Roweis, 2005)).
    ‣ 4.1.1 Collapsing Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). Q.E.D. ∎'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '将所得导数代入公式 ([91](#S4.E91 "In Proof. ‣ 4.1.1 Collapsing Classes in the Input
    Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) 得到公式 ([89](#S4.E89 "In Lemma
    7 ((Globerson & Roweis, 2005)). ‣ 4.1.1 Collapsing Classes in the Input Space
    ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). 证毕。∎'
- en: 'The optimization problem ([88](#S4.E88 "In 4.1.1 Collapsing Classes in the
    Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) is convex; hence,
    it has a unique solution. We can solve it using any optimization method such as
    the projected gradient method, where after every gradient descent step, we project
    the solution onto the positive semi-definite cone (Ghojogh et al., [2021c](#bib.bib48)):'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题（[88](#S4.E88 "在 4.1.1 输入空间中的类的合并 ‣ 4.1 类的合并 ‣ 4 概率度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")）是凸的，因此具有唯一解。我们可以使用任何优化方法解决，例如投影梯度法，其中每次梯度下降步骤后，我们将解投影到正半定锥中（Ghojogh
    等， [2021c](#bib.bib48)）：
- en: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{W}-\eta\frac{\partial c}{\partial\boldsymbol{W}},$
    |  |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{W}-\eta\frac{\partial c}{\partial\boldsymbol{W}},$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
- en: 'where $\eta>0$ is the learning rate and $\boldsymbol{V}$ and $\boldsymbol{\Lambda}=\textbf{diag}(\lambda_{1},\dots,\lambda_{d})$
    are the eigenvectors and eigenvalues of $\boldsymbol{W}$, respectively (see Eq.
    ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))).'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\eta>0$ 是学习率，$\boldsymbol{V}$ 和 $\boldsymbol{\Lambda}=\textbf{diag}(\lambda_{1},\dots,\lambda_{d})$
    分别是 $\boldsymbol{W}$ 的特征向量和特征值（见方程（[9](#S2.E9 "在证明中。 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量
    ‣ 光谱、概率和深度度量学习：教程和调查")）。
- en: 4.1.2 Collapsing Classes in the Feature Space
  id: totrans-672
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 特征空间中的类合并
- en: 'According to Eq. ([54](#S3.E54 "In Lemma 4\. ‣ 3.6.4 Kernel Discriminative
    Component Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), the
    distance in the feature space can be stated using kernels as $\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    where $\boldsymbol{k}_{i}\in\mathbb{R}^{n}$ is the kernel vector between dataset
    $\boldsymbol{X}$ and the point $\boldsymbol{x}_{i}$. We define $\boldsymbol{R}:=\boldsymbol{T}\boldsymbol{T}^{\top}\in\mathbb{R}^{n\times
    n}$. Hence, in the feature space, Eq. ([84](#S4.E84 "In 4.1 Collapsing Classes
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) becomes:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程（[54](#S3.E54 "在引理 4。 ‣ 3.6.4 核 discriminative 组件分析 ‣ 3.6 核光谱度量学习 ‣ 3 光谱度量学习
    ‣ 光谱、概率和深度度量学习：教程和调查")），特征空间中的距离可以使用核表示为 $\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$，其中
    $\boldsymbol{k}_{i}\in\mathbb{R}^{n}$ 是数据集 $\boldsymbol{X}$ 和点 $\boldsymbol{x}_{i}$
    之间的核向量。我们定义 $\boldsymbol{R}:=\boldsymbol{T}\boldsymbol{T}^{\top}\in\mathbb{R}^{n\times
    n}$。因此，在特征空间中，方程（[84](#S4.E84 "在 4.1 类的合并 ‣ 4 概率度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")）变为：
- en: '|  | $\displaystyle p^{R}_{ij}:=\frac{\exp(-\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{R}}^{2})}{\sum_{k\neq
    i}\exp(-\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{k}\&#124;_{\boldsymbol{R}}^{2})},\quad
    j\neq i.$ |  | (94) |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{R}_{ij}:=\frac{\exp(-\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\&#124;_{\boldsymbol{R}}^{2})}{\sum_{k\neq
    i}\exp(-\&#124;\boldsymbol{k}_{i}-\boldsymbol{k}_{k}\&#124;_{\boldsymbol{R}}^{2})},\quad
    j\neq i.$ |  | (94) |'
- en: 'The gradient in Eq. ([89](#S4.E89 "In Lemma 7 ((Globerson & Roweis, 2005)).
    ‣ 4.1.1 Collapsing Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) becomes:'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中的梯度（[89](#S4.E89 "在引理 7（（Globerson & Roweis, 2005））。 ‣ 4.1.1 输入空间中的类的合并 ‣
    4.1 类的合并 ‣ 4 概率度量学习 ‣ 光谱、概率和深度度量学习：教程和调查）") 变为：
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{R}}=\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}(p^{0}_{ij}-p^{R}_{ij})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}.$
    |  | (95) |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{R}}=\sum_{i=1}^{n}\sum_{j=1,j\neq
    i}^{n}(p^{0}_{ij}-p^{R}_{ij})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}.$
    |  | (95) |'
- en: Again, we can find the optimal $\boldsymbol{R}$ using projected gradient method.
    This gives us the optimal metric for collapsing classes in the feature space (Globerson
    & Roweis, [2005](#bib.bib52)). Note that we can also regularize the objective
    function, using the trace operator or Frobenius norm, for avoiding overfitting.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用投影梯度法找到最优的 $\boldsymbol{R}$。这为我们在特征空间中合并类别提供了最优度量（Globerson & Roweis，[2005](#bib.bib52)）。注意，我们也可以通过使用迹算子或
    Frobenius 范数来正则化目标函数，以避免过拟合。
- en: 4.2 Neighborhood Component Analysis Methods
  id: totrans-678
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 邻域组件分析方法
- en: Neighborhood Component Analysis (NCA) is one of the most well-known probabilistic
    metric learning methods. In the following, we introduce different variants of
    NCA.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域组件分析 (NCA) 是最著名的概率度量学习方法之一。接下来，我们将介绍 NCA 的不同变体。
- en: 4.2.1 Neighborhood Component Analysis (NCA)
  id: totrans-680
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 邻域组件分析 (NCA)
- en: 'In the original NCA (Goldberger et al., [2005](#bib.bib53)), the probability
    that $\boldsymbol{x}_{j}$ takes $\boldsymbol{x}_{i}$ as its neighbor is as in
    Eq. ([84](#S4.E84 "In 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), where
    we assume $p^{W}_{ii}=0$ by convention:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 NCA (Goldberger 等，[2005](#bib.bib53)) 中，$\boldsymbol{x}_{j}$ 将 $\boldsymbol{x}_{i}$
    作为其邻居的概率如方程 ([84](#S4.E84 "在 4.1 合并类别 ‣ 4 概率度量学习 ‣ 谱方法、概率方法和深度度量学习：教程与调查")) 中所示，其中我们假设
    $p^{W}_{ii}=0$ 为惯例：
- en: '|  | $\displaystyle p^{W}_{ij}:=\left\{\begin{array}[]{ll}\frac{\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})}{\sum_{k\neq
    i}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\&#124;_{\boldsymbol{W}}^{2})}&amp;\mbox{if
    }j\neq i\\ 0&amp;\mbox{if }j=i.\end{array}\right.$ |  | (98) |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{W}_{ij}:=\left\{\begin{array}[]{ll}\frac{\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})}{\sum_{k\neq
    i}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\&#124;_{\boldsymbol{W}}^{2})}&amp;\mbox{如果
    }j\neq i\\ 0&amp;\mbox{如果 }j=i.\end{array}\right.$ |  | (98) |'
- en: 'Consider the decomposition of the weight matrix of metric as in Eq. ([9](#S2.E9
    "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), i.e., $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$. Let
    $\mathcal{S}_{i}$ denote the set of similar points to $\boldsymbol{x}_{i}$ where
    $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$. The optimization problem
    of NCA is to find a $\boldsymbol{U}$ to maximize this probability distribution
    for similar points (Goldberger et al., [2005](#bib.bib53)):'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑度量的权重矩阵的分解，如方程 ([9](#S2.E9 "在证明中 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱方法、概率方法和深度度量学习：教程与调查"))
    中所示，即 $\boldsymbol{W}=\boldsymbol{U}\boldsymbol{U}^{\top}$。令 $\mathcal{S}_{i}$
    表示与 $\boldsymbol{x}_{i}$ 相似的点集，其中 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$。NCA
    的优化问题是找到一个 $\boldsymbol{U}$ 来最大化这些相似点的概率分布（Goldberger 等，[2005](#bib.bib53)）：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}=\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}=\sum_{i=1}^{n}p^{W}_{i},$
    |  | (99) |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{最大化}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}=\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}=\sum_{i=1}^{n}p^{W}_{i},$
    |  | (99) |'
- en: 'where:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '|  | $\displaystyle p^{W}_{i}:=\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}.$
    |  | (100) |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{W}_{i}:=\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}.$
    |  | (100) |'
- en: 'Note that the required constraint $\boldsymbol{W}\succeq\boldsymbol{0}$ is
    already satisfied because of the decomposition in Eq. ([84](#S4.E84 "In 4.1 Collapsing
    Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")).'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所需的约束 $\boldsymbol{W}\succeq\boldsymbol{0}$ 已经因为方程 ([84](#S4.E84 "在 4.1 合并类别
    ‣ 4 概率度量学习 ‣ 谱方法、概率方法和深度度量学习：教程与调查")) 中的分解而得到满足。
- en: Lemma 8  ((Goldberger et al., [2005](#bib.bib53))).
  id: totrans-688
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8  ((Goldberger 等，[2005](#bib.bib53)))。
- en: 'Suppose the objective function of Eq. ([99](#S4.E99 "In 4.2.1 Neighborhood
    Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) is denoted by $c$. The gradient of this cost function w.r.t. $\boldsymbol{U}$
    is:'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 假设方程 ([99](#S4.E99 "在 4.2.1 邻域组件分析 (NCA) ‣ 4.2 邻域组件分析方法 ‣ 4 概率度量学习 ‣ 谱方法、概率方法和深度度量学习：教程与调查"))
    的目标函数用 $c$ 表示。该成本函数相对于 $\boldsymbol{U}$ 的梯度是：
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=$ | $\displaystyle\,2\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{k=1}^{n}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (101) |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=$ | $\displaystyle\,2\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{k=1}^{n}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (101) |'
- en: '|  |  | $\displaystyle-\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\Big{)}\boldsymbol{U}.$
    |  |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\Big{)}\boldsymbol{U}.$
    |  |'
- en: 'The derivation of this gradient is similar to the approach in the proof of
    Lemma [7](#Thmlemma7 "Lemma 7 ((Globerson & Roweis, 2005)). ‣ 4.1.1 Collapsing
    Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"). We
    can use gradient ascent for solving the optimization.'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '该梯度的推导类似于引理 [7](#Thmlemma7 "Lemma 7 ((Globerson & Roweis, 2005)). ‣ 4.1.1 Collapsing
    Classes in the Input Space ‣ 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey") 证明中的方法。我们可以使用梯度上升法来解决优化问题。'
- en: 'Another approach is to maximize the log-likelihood of neighborhood probability
    (Goldberger et al., [2005](#bib.bib53)):'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是最大化邻域概率的对数似然 (Goldberger et al., [2005](#bib.bib53))：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}\Big{)},$
    |  | (102) |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}\Big{)},$
    |  | (102) |'
- en: 'whose gradient is (Goldberger et al., [2005](#bib.bib53)):'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 其梯度为 (Goldberger et al., [2005](#bib.bib53))：
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=$ | $\displaystyle\,2\sum_{i=1}^{n}\Big{(}\sum_{k=1}^{n}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (103) |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=$ | $\displaystyle\,2\sum_{i=1}^{n}\Big{(}\sum_{k=1}^{n}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (103) |'
- en: '|  |  | $\displaystyle-\frac{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}}\Big{)}\boldsymbol{U}.$
    |  |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}p^{W}_{ij}}\Big{)}\boldsymbol{U}.$
    |  |'
- en: 'Again, gradient ascent can give us the optimal $\boldsymbol{U}$. As explained
    in Proposition [2](#Thmproposition2 "Proposition 2 (Projection in metric learning).
    ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"), the
    subspace is metric is the column space of $\boldsymbol{U}$ and projection of points
    onto this subspace reduces the dimensionality of data.'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '再次，梯度上升法可以给我们最优的 $\boldsymbol{U}$。如命题 [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") 中所述，子空间的度量是 $\boldsymbol{U}$ 的列空间，将点投影到该子空间可以降低数据的维度。'
- en: 4.2.2 Regularized Neighborhood Component Analysis
  id: totrans-699
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 正则化的邻域成分分析
- en: 'It is shown by some experiments that NCA can overfit to training data for high-dimensional
    data (Yang & Laaksonen, [2007](#bib.bib138)). Hence, we can regularize it to avoid
    overfitting. In regularized NCA (Yang & Laaksonen, [2007](#bib.bib138)), we use
    the log-posterior of the matrix $\boldsymbol{U}$ which is equal to:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 一些实验表明，NCA 对高维数据可能会过拟合训练数据 (Yang & Laaksonen, [2007](#bib.bib138))。因此，我们可以对其进行正则化以避免过拟合。在正则化的
    NCA (Yang & Laaksonen, [2007](#bib.bib138)) 中，我们使用矩阵 $\boldsymbol{U}$ 的对数后验，其等于：
- en: '|  | $\displaystyle\mathbb{P}(\boldsymbol{U}&#124;\boldsymbol{x}_{i},\mathcal{S}_{i})=\frac{\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}&#124;\boldsymbol{U})\,\mathbb{P}(\boldsymbol{U})}{\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i})},$
    |  | (104) |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}(\boldsymbol{U}|\boldsymbol{x}_{i},\mathcal{S}_{i})=\frac{\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}|\boldsymbol{U})\,\mathbb{P}(\boldsymbol{U})}{\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i})},$
    |  | (104) |'
- en: 'according to the Bayes’ rule. We can use Gaussian distribution for the prior:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 根据贝叶斯规则。我们可以使用高斯分布作为先验：
- en: '|  | $\displaystyle\mathbb{P}(\boldsymbol{U})=\prod_{k=1}^{d}\prod_{l=1}^{d}c\,\exp(-\lambda(\boldsymbol{U}(k,l))^{2}),$
    |  | (105) |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}(\boldsymbol{U})=\prod_{k=1}^{d}\prod_{l=1}^{d}c\,\exp(-\lambda(\boldsymbol{U}(k,l))^{2}),$
    |  | (105) |'
- en: 'where $c>0$ is a constant factor including the normalization factor, $\lambda>0$
    is the inverse of variance, and $\boldsymbol{U}(k,l)$ is the $(k,l)$-th element
    of $\boldsymbol{U}\in\mathbb{R}^{d\times d}$. Note that we can have $\boldsymbol{U}\in\mathbb{R}^{d\times
    p}$ if we truncate it to have $p$ leading eigenvectors of $\boldsymbol{W}$ (see
    Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))). The likelihood'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c>0$ 是包含归一化因子的常数因子，$\lambda>0$ 是方差的倒数，而 $\boldsymbol{U}(k,l)$ 是 $\boldsymbol{U}\in\mathbb{R}^{d\times
    d}$ 的 $(k,l)$-th 元素。注意，如果我们将其截断为 $\boldsymbol{W}$ 的 $p$ 个主特征向量，则 $\boldsymbol{U}\in\mathbb{R}^{d\times
    p}$（参见公式 ([9](#S2.E9 "在证明中 ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱方法、概率方法和深度度量学习：教程和调查"))）。似然函数
- en: '|  | $\displaystyle\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}&#124;\boldsymbol{U})\propto\exp\Big{(}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}\Big{)}.$
    |  | (106) |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}&#124;\boldsymbol{U})\propto\exp\Big{(}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}\Big{)}.$
    |  | (106) |'
- en: 'The regularized NCA maximizes the log-posterior (Yang & Laaksonen, [2007](#bib.bib138)):'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化 NCA 最大化对数后验 (Yang & Laaksonen, [2007](#bib.bib138))：
- en: '|  |  | $\displaystyle\log\mathbb{P}(\boldsymbol{U}&#124;\boldsymbol{x}_{i},\mathcal{S}_{i})\overset{(\ref{equation_regularized_NCA_posterior})}{=}\log\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}&#124;\boldsymbol{U})+\log\mathbb{P}(\boldsymbol{U})$
    |  |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\log\mathbb{P}(\boldsymbol{U}&#124;\boldsymbol{x}_{i},\mathcal{S}_{i})\overset{(\ref{equation_regularized_NCA_posterior})}{=}\log\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i}&#124;\boldsymbol{U})+\log\mathbb{P}(\boldsymbol{U})$
    |  |'
- en: '|  |  | $\displaystyle-\underbrace{\log\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i})}_{\text{constant
    w.r.t. }\boldsymbol{U}}\overset{(a)}{=}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}-\lambda\&#124;\boldsymbol{U}\&#124;_{F}^{2},$
    |  |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\underbrace{\log\mathbb{P}(\boldsymbol{x}_{i},\mathcal{S}_{i})}_{\text{对
    }\boldsymbol{U} \text{ 常数}}\overset{(a)}{=}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}-\lambda\&#124;\boldsymbol{U}\&#124;_{F}^{2},$
    |  |'
- en: 'where $(a)$ is because of Eqs. ([105](#S4.E105 "In 4.2.2 Regularized Neighborhood
    Component Analysis ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) and ([106](#S4.E106 "In 4.2.2 Regularized Neighborhood Component
    Analysis ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and $\|.\|_{F}$ denotes the Frobenius norm. Hence, the optimization problem of
    regularized NCA is (Yang & Laaksonen, [2007](#bib.bib138)):'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是由于公式 ([105](#S4.E105 "在 4.2.2 正则化邻域成分分析 ‣ 4.2 邻域成分分析方法 ‣ 4 概率度量学习
    ‣ 谱方法、概率方法和深度度量学习：教程和调查")) 和 ([106](#S4.E106 "在 4.2.2 正则化邻域成分分析 ‣ 4.2 邻域成分分析方法
    ‣ 4 概率度量学习 ‣ 谱方法、概率方法和深度度量学习：教程和调查")) 的结果，而 $\|.\|_{F}$ 表示 Frobenius 范数。因此，正则化
    NCA 的优化问题是 (Yang & Laaksonen, [2007](#bib.bib138))：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}-\lambda\&#124;\boldsymbol{U}\&#124;_{F}^{2},$
    |  | (107) |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}p^{W}_{ij}-\lambda\&#124;\boldsymbol{U}\&#124;_{F}^{2},$
    |  | (107) |'
- en: 'where $\lambda>0$ can be seen as the regularization parameter. The gradient
    is similar to Eq. ([101](#S4.E101 "In Lemma 8 ((Goldberger et al., 2005)). ‣ 4.2.1
    Neighborhood Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis Methods
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) but plus the derivative of the regularization term which
    is $-2\lambda\boldsymbol{U}$.'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 可视为正则化参数。梯度与公式 ([101](#S4.E101 "在引理 8 ((Goldberger et al., 2005)).
    ‣ 4.2.1 邻域成分分析 (NCA) ‣ 4.2 邻域成分分析方法 ‣ 4 概率度量学习 ‣ 谱方法、概率方法和深度度量学习：教程和调查")) 相似，但多了正则化项的导数，即
    $-2\lambda\boldsymbol{U}$。
- en: 4.2.3 Fast Neighborhood Component Analysis
  id: totrans-712
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 快速邻域成分分析
- en: '– Fast NCA: The fast NCA (Yang et al., [2012](#bib.bib136)) accelerates NCA
    by using $k$-Nearest Neighbors ($k$NN) rather than using all points for computing
    the neighborhood distribution of every point. Let $\mathcal{N}_{i}$ and $\mathcal{M}_{i}$
    denote the $k$NN of $\boldsymbol{x}_{i}$ among the similar points to $\boldsymbol{x}_{i}$
    (denoted by $\mathcal{S}_{i}$) and dissimilar points (denoted by $\mathcal{D}_{i}$),
    respectively. Fast NCA uses following probability distribution for $\boldsymbol{x}_{i}$
    to take $\boldsymbol{x}_{i}$ as its neighbor (Yang et al., [2012](#bib.bib136)):'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: – 快速 NCA：快速 NCA（Yang 等人，[2012](#bib.bib136)）通过使用 $k$-最近邻（$k$NN）而不是使用所有点来计算每个点的邻域分布，从而加速了
    NCA。令 $\mathcal{N}_{i}$ 和 $\mathcal{M}_{i}$ 分别表示 $\boldsymbol{x}_{i}$ 在与 $\boldsymbol{x}_{i}$
    相似的点（记作 $\mathcal{S}_{i}$）和不相似的点（记作 $\mathcal{D}_{i}$）中的 $k$NN。快速 NCA 使用以下概率分布使
    $\boldsymbol{x}_{i}$ 将 $\boldsymbol{x}_{i}$ 作为其邻居（Yang 等人，[2012](#bib.bib136)）：
- en: '|  |  | $\displaystyle p^{W}_{ij}:=\left\{\begin{array}[]{ll}\frac{\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}})}{\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}\cup\mathcal{M}_{i}}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\&#124;_{\boldsymbol{W}})}&amp;\mbox{if
    }\boldsymbol{x}_{k}\in\mathcal{N}_{i}\cup\mathcal{M}_{i}\\ 0&amp;\mbox{otherwise.}\end{array}\right.$
    |  | (110) |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle p^{W}_{ij}:=\left\{\begin{array}[]{ll}\frac{\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}})}{\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}\cup\mathcal{M}_{i}}\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\|_{\boldsymbol{W}})}&\text{如果
    } \boldsymbol{x}_{k}\in\mathcal{N}_{i}\cup\mathcal{M}_{i}\\ 0&\text{否则.}\end{array}\right.$
    |  | (110) |'
- en: 'The optimization problem of fast NCA is similar to Eq. ([107](#S4.E107 "In
    4.2.2 Regularized Neighborhood Component Analysis ‣ 4.2 Neighborhood Component
    Analysis Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")):'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 快速 NCA 的优化问题类似于方程 ([107](#S4.E107 "在 4.2.2 正则化邻域分量分析 ‣ 4.2 邻域分量分析方法 ‣ 4 概率度量学习
    ‣ 谱的、概率的和深度的度量学习：教程与调查"))：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}-\lambda\&#124;\boldsymbol{U}\&#124;_{F}^{2},$
    |  | (111) |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}-\lambda\|\boldsymbol{U}\|_{F}^{2},$
    |  | (111) |'
- en: 'where $p^{W}_{ij}$ is Eq. ([110](#S4.E110 "In 4.2.3 Fast Neighborhood Component
    Analysis ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and $\boldsymbol{U}$ is the matrix in the decomposition of $\boldsymbol{W}$ (see
    Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))).'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p^{W}_{ij}$ 是方程 ([110](#S4.E110 "在 4.2.3 快速邻域分量分析 ‣ 4.2 邻域分量分析方法 ‣ 4 概率度量学习
    ‣ 谱的、概率的和深度的度量学习：教程与调查")) 和 $\boldsymbol{U}$ 是 $\boldsymbol{W}$ 分解中的矩阵（见方程 ([9](#S2.E9
    "在证明 ‣ 2.3 广义马哈拉诺比斯距离 ‣ 2 广义马哈拉诺比斯距离度量 ‣ 谱的、概率的和深度的度量学习：教程与调查"))）。
- en: Lemma 9  ((Yang et al., [2012](#bib.bib136))).
  id: totrans-718
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 9（Yang 等人，[2012](#bib.bib136)）。
- en: 'Suppose the objective function of Eq. ([111](#S4.E111 "In 4.2.3 Fast Neighborhood
    Component Analysis ‣ 4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) is denoted by $c$. The gradient of this cost function w.r.t. $\boldsymbol{U}$
    is:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 设方程 ([111](#S4.E111 "在 4.2.3 快速邻域分量分析 ‣ 4.2 邻域分量分析方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度的度量学习：教程与调查"))
    的目标函数为 $c$。该成本函数相对于 $\boldsymbol{U}$ 的梯度为：
- en: '|  |  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (112) |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}=\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}}p^{W}_{ik}(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})(\boldsymbol{x}_{i}-\boldsymbol{x}_{k})^{\top}$
    |  | (112) |'
- en: '|  |  | $\displaystyle+(p^{W}_{i}-1)\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\Big{)}\boldsymbol{U}-2\lambda\boldsymbol{U}.$
    |  |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+(p^{W}_{i}-1)\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\Big{)}\boldsymbol{U}-2\lambda\boldsymbol{U}.$
    |  |'
- en: 'This is similar to Eq. ([101](#S4.E101 "In Lemma 8 ((Goldberger et al., 2005)).
    ‣ 4.2.1 Neighborhood Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")). See (Yang et al., [2012](#bib.bib136))
    for the derivation. We can use gradient ascent for solving the optimization.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 这与公式 ([101](#S4.E101 "在引理 8 ((Goldberger 等, 2005)). ‣ 4.2.1 邻域成分分析 (NCA) ‣ 4.2
    邻域成分分析方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查")) 相似。参见 (Yang 等，[2012](#bib.bib136))
    以获取推导。我们可以使用梯度上升法来解决优化问题。
- en: '– Kernel Fast NCA: According to Eq. ([54](#S3.E54 "In Lemma 4\. ‣ 3.6.4 Kernel
    Discriminative Component Analysis ‣ 3.6 Kernel Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), the distance in the feature space is $\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    where $\boldsymbol{k}_{i}\in\mathbb{R}^{n}$ is the kernel vector between dataset
    $\boldsymbol{X}$ and the point $\boldsymbol{x}_{i}$. We can use this distance
    metric in Eq. ([110](#S4.E110 "In 4.2.3 Fast Neighborhood Component Analysis ‣
    4.2 Neighborhood Component Analysis Methods ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) to
    have kernel fast NCA (Yang et al., [2012](#bib.bib136)). Hence, the gradient of
    kernel fast NCA is similar to Eq. ([112](#S4.E112 "In Lemma 9 ((Yang et al., 2012)).
    ‣ 4.2.3 Fast Neighborhood Component Analysis ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")):'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: – 核快速 NCA：根据公式 ([54](#S3.E54 "在引理 4\. ‣ 3.6.4 核判别成分分析 ‣ 3.6 核谱度量学习 ‣ 3 谱度量学习
    ‣ 谱的、概率的和深度的度量学习：教程和调查"))，特征空间中的距离为$\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$，其中
    $\boldsymbol{k}_{i}\in\mathbb{R}^{n}$ 是数据集 $\boldsymbol{X}$ 与点 $\boldsymbol{x}_{i}$
    之间的核向量。我们可以在公式 ([110](#S4.E110 "在 4.2.3 快速邻域成分分析 ‣ 4.2 邻域成分分析方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查"))
    中使用这个距离度量来得到核快速 NCA (Yang 等，[2012](#bib.bib136))。因此，核快速 NCA 的梯度与公式 ([112](#S4.E112
    "在引理 9 ((Yang 等, 2012)). ‣ 4.2.3 快速邻域成分分析 ‣ 4.2 邻域成分分析方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查"))
    相似：
- en: '|  |  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{T}}=\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}}p^{W}_{ik}(\boldsymbol{k}_{i}-\boldsymbol{k}_{k})(\boldsymbol{k}_{i}-\boldsymbol{k}_{k})^{\top}$
    |  | (113) |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{T}}=\sum_{i=1}^{n}\Big{(}p^{W}_{i}\sum_{\boldsymbol{x}_{k}\in\mathcal{N}_{i}}p^{W}_{ik}(\boldsymbol{k}_{i}-\boldsymbol{k}_{k})(\boldsymbol{k}_{i}-\boldsymbol{k}_{k})^{\top}$
    |  | (113) |'
- en: '|  |  | $\displaystyle+(p^{W}_{i}-1)\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\Big{)}\boldsymbol{T}-2\lambda\boldsymbol{T}.$
    |  |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+(p^{W}_{i}-1)\sum_{\boldsymbol{x}_{j}\in\mathcal{M}_{i}}p^{W}_{ij}(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\Big{)}\boldsymbol{T}-2\lambda\boldsymbol{T}.$
    |  |'
- en: Again, we can find the optimal $\boldsymbol{T}$ using gradient ascent. Note
    that the same technique can be used to kernelize the original NCA.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用梯度上升法找到最优的$\boldsymbol{T}$。请注意，相同的技术可以用来对原始 NCA 进行核化。
- en: 4.3 Bayesian Metric Learning Methods
  id: totrans-727
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 贝叶斯度量学习方法
- en: In this section, we introduce the Bayesian metric learning methods which use
    variational inference (Ghojogh et al., [2021a](#bib.bib46)) for metric learning.
    In Bayesian metric learning, we learn a distribution for the distance metric between
    every two points; we sample the pairwise distances from these learned distributions.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了使用变分推断（Ghojogh 等，[2021a](#bib.bib46)）进行度量学习的贝叶斯度量学习方法。在贝叶斯度量学习中，我们为每两个点之间的距离度量学习一个分布；我们从这些学习到的分布中采样成对距离。
- en: 'First, we provide some definition required in these methods. According to Eq.
    ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), we can decompose the weight matrix in the metric using
    the eigenvalue decomposition. Accordingly, we can approximate this matrix by:'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们提供这些方法所需的一些定义。根据公式 ([9](#S2.E9 "在证明中. ‣ 2.3 广义马氏距离 ‣ 2 广义马氏距离度量 ‣ 谱的、概率的和深度的度量学习：教程和调查"))，我们可以通过特征值分解来分解度量中的权重矩阵。因此，我们可以通过以下方式近似这个矩阵：
- en: '|  | $\displaystyle\boldsymbol{W}\approx\boldsymbol{V}_{x}\boldsymbol{\Lambda}\boldsymbol{V}_{x}^{\top},$
    |  | (114) |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{W}\approx\boldsymbol{V}_{x}\boldsymbol{\Lambda}\boldsymbol{V}_{x}^{\top},$
    |  | (114) |'
- en: where $\boldsymbol{V}_{x}$ contains the eigenvectors of $\boldsymbol{X}\boldsymbol{X}^{\top}$
    and $\boldsymbol{\Lambda}=\textbf{diag}([\lambda_{1},\dots,\lambda_{d}]^{\top})$
    is the diagonal matrix of eigenvalues which we learn in Bayesian metric learning.
    Let $X$ and $Y$ denote the random variables for data and labels, respectively,
    and let $\boldsymbol{\lambda}=[\lambda_{1},\dots,\lambda_{d}]^{\top}\in\mathbb{R}^{d}$
    denote the learnable eigenvalues. Let $\boldsymbol{v}_{x}^{l}\in\mathbb{R}^{d}$
    denote the $l$-th column of $\boldsymbol{V}_{x}$. We define $\boldsymbol{w}_{ij}=[w_{ij}^{1},\dots,w_{ij}^{d}]^{\top}:=[((\boldsymbol{v}_{x}^{1})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))^{2},\dots,((\boldsymbol{v}_{x}^{d})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))^{2}]^{\top}\in\mathbb{R}^{d}$.
    The reader should not confuse $\boldsymbol{w}_{ij}$ with $\boldsymbol{W}$ which
    is the weight matrix of metric in out notations.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{V}_{x}$ 包含 $\boldsymbol{X}\boldsymbol{X}^{\top}$ 的特征向量，$\boldsymbol{\Lambda}=\textbf{diag}([\lambda_{1},\dots,\lambda_{d}]^{\top})$
    是我们在贝叶斯度量学习中学习的特征值的对角矩阵。令 $X$ 和 $Y$ 分别表示数据和标签的随机变量，$\boldsymbol{\lambda}=[\lambda_{1},\dots,\lambda_{d}]^{\top}\in\mathbb{R}^{d}$
    表示可学习的特征值。令 $\boldsymbol{v}_{x}^{l}\in\mathbb{R}^{d}$ 表示 $\boldsymbol{V}_{x}$
    的第 $l$ 列。我们定义 $\boldsymbol{w}_{ij}=[w_{ij}^{1},\dots,w_{ij}^{d}]^{\top}:=[((\boldsymbol{v}_{x}^{1})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))^{2},\dots,((\boldsymbol{v}_{x}^{d})^{\top}(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}))^{2}]^{\top}\in\mathbb{R}^{d}$。读者不应将
    $\boldsymbol{w}_{ij}$ 与 $\boldsymbol{W}$ 混淆，后者是我们符号中的度量权重矩阵。
- en: 4.3.1 Bayesian Metric Learning Using Sigmoid Function
  id: totrans-732
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 使用 Sigmoid 函数的贝叶斯度量学习
- en: 'One of the Bayesian metric learning methods is (Yang et al., [2007](#bib.bib134)).
    We define:'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯度量学习方法之一是 (Yang et al., [2007](#bib.bib134))。我们定义：
- en: '|  | $\displaystyle y_{ij}:=\left\{\begin{array}[]{ll}1&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    -1&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (117) |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y_{ij}:=\left\{\begin{array}[]{ll}1&amp;\mbox{如果 }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    -1&amp;\mbox{如果 }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (117) |'
- en: 'We can consider a sigmoid function for the likelihood (Yang et al., [2007](#bib.bib134)):'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以考虑用于似然的 Sigmoid 函数 (Yang et al., [2007](#bib.bib134))：
- en: '|  | $\displaystyle\mathbb{P}(Y&#124;X,\boldsymbol{\Lambda})=\frac{1}{1+\exp(y_{ij}(\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}-\mu))},$
    |  | (118) |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}(Y\mid X,\boldsymbol{\Lambda})=\frac{1}{1+\exp(y_{ij}(\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}-\mu))},$
    |  | (118) |'
- en: 'where $\mu>0$ is a threshold. We can also derive an evidence lower bound for
    $\mathbb{P}(\mathcal{S},\mathcal{D})$; we do not provide the derivation for brevity
    (see (Yang et al., [2007](#bib.bib134)) for derivation of the lower bound). As
    in the variational inference, we maximize this lower bound for likelihood maximization
    (Ghojogh et al., [2021a](#bib.bib46)). We assume a Gaussian distribution with
    mean $\boldsymbol{m}_{\lambda}\in\mathbb{R}^{d}$ and covariance $\boldsymbol{V}_{\lambda}\in\mathbb{R}^{d\times
    d}$ for the distribution $\mathbb{P}(\boldsymbol{\lambda})$. By maximizing the
    lower bound, we can estimate these parameters as (Yang et al., [2007](#bib.bib134)):'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu>0$ 是一个阈值。我们还可以推导出 $\mathbb{P}(\mathcal{S},\mathcal{D})$ 的证据下界；为了简洁起见，我们不提供推导过程（有关下界的推导，请参见
    (Yang et al., [2007](#bib.bib134))）。与变分推断类似，我们通过最大化这个下界来实现似然最大化 (Ghojogh et al.,
    [2021a](#bib.bib46))。我们假设分布 $\mathbb{P}(\boldsymbol{\lambda})$ 服从均值为 $\boldsymbol{m}_{\lambda}\in\mathbb{R}^{d}$
    和协方差为 $\boldsymbol{V}_{\lambda}\in\mathbb{R}^{d\times d}$ 的高斯分布。通过最大化下界，我们可以估计这些参数，如
    (Yang et al., [2007](#bib.bib134)) 所示：
- en: '|  | $\displaystyle\boldsymbol{V}_{T}:=\Big{(}\delta\boldsymbol{I}+2\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\frac{\tanh(\xi_{ij}^{s})}{4\xi_{ij}^{s}}\boldsymbol{w}_{ij}\boldsymbol{w}_{ij}^{\top}$
    |  |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{V}_{T}:=\Big{(}\delta\boldsymbol{I}+2\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\frac{\tanh(\xi_{ij}^{s})}{4\xi_{ij}^{s}}\boldsymbol{w}_{ij}\boldsymbol{w}_{ij}^{\top}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+2\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\frac{\tanh(\xi_{ij}^{d})}{4\xi_{ij}^{d}}\boldsymbol{w}_{ij}\boldsymbol{w}_{ij}^{\top}\Big{)}^{-1},$
    |  | (119) |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+2\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\frac{\tanh(\xi_{ij}^{d})}{4\xi_{ij}^{d}}\boldsymbol{w}_{ij}\boldsymbol{w}_{ij}^{\top}\Big{)}^{-1},$
    |  | (119) |'
- en: '|  | $\displaystyle\boldsymbol{m}_{T}:=\boldsymbol{V}_{T}\Big{(}\delta\boldsymbol{\gamma}_{0}-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\boldsymbol{w}_{ij}+\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\boldsymbol{w}_{ij}\Big{)},$
    |  | (120) |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{m}_{T}:=\boldsymbol{V}_{T}\Big{(}\delta\boldsymbol{\gamma}_{0}-\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\boldsymbol{w}_{ij}+\frac{1}{2}\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\boldsymbol{w}_{ij}\Big{)},$
    |  | (120) |'
- en: 'where $\delta>0$ and $\boldsymbol{\gamma}_{0}$ are hyper-parameters related
    to the priors on the weight matrix of metric and the threshold. We define the
    following variational parameter (Yang et al., [2007](#bib.bib134)):'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\delta>0$ 和 $\boldsymbol{\gamma}_{0}$ 是与度量的权重矩阵的先验和阈值相关的超参数。我们定义了以下变分参数（Yang
    et al., [2007](#bib.bib134)）：
- en: '|  | $\displaystyle\xi_{ij}^{s}:=\sqrt{(\boldsymbol{m}_{T}^{\top}\boldsymbol{w}_{ij})^{2}+\boldsymbol{w}_{ij}^{\top}\boldsymbol{V}_{T}\boldsymbol{w}_{ij}},$
    |  | (121) |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\xi_{ij}^{s}:=\sqrt{(\boldsymbol{m}_{T}^{\top}\boldsymbol{w}_{ij})^{2}+\boldsymbol{w}_{ij}^{\top}\boldsymbol{V}_{T}\boldsymbol{w}_{ij}},$
    |  | (121) |'
- en: 'for $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$. We similarly define
    the variational parameter $\xi_{ij}^{d}$ for $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$.
    The variables $\boldsymbol{V}_{T}$, $\boldsymbol{m}_{T}$, $\xi_{ij}^{s}$, and
    $\xi_{ij}^{d}$ are updated iteratively by Eqs. ([123](#S4.E123 "In 4.3.2 Bayesian
    Neighborhood Component Analysis ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), ([124](#S4.E124 "In 4.3.2 Bayesian Neighborhood Component Analysis
    ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), and ([121](#S4.E121
    "In 4.3.1 Bayesian Metric Learning Using Sigmoid Function ‣ 4.3 Bayesian Metric
    Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), respectively, until convergence.
    After these parameters are learned, we can sample the eigenvalues from the posterior,
    $\boldsymbol{\lambda}\sim\mathcal{N}(\boldsymbol{m}_{T},\boldsymbol{V}_{T})$.
    These eigenvalues can be used in Eq. ([114](#S4.E114 "In 4.3 Bayesian Metric Learning
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) to obtain the weight matrix in the metric.
    Note that Bayesian metric learning can also be used for active learning (see (Yang
    et al., [2007](#bib.bib134)) for details).'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}$。我们类似地定义变分参数 $\xi_{ij}^{d}$
    用于 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}$。变量 $\boldsymbol{V}_{T}$、$\boldsymbol{m}_{T}$、$\xi_{ij}^{s}$
    和 $\xi_{ij}^{d}$ 会通过方程（[123](#S4.E123 "在4.3.2贝叶斯邻域成分分析 ‣ 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习
    ‣ 光谱、概率和深度度量学习：教程和调查")）、（[124](#S4.E124 "在4.3.2贝叶斯邻域成分分析 ‣ 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习
    ‣ 光谱、概率和深度度量学习：教程和调查")）和（[121](#S4.E121 "在4.3.1贝叶斯度量学习使用Sigmoid函数 ‣ 4.3 贝叶斯度量学习方法
    ‣ 4 概率度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")）中更新，直到收敛。学习这些参数后，我们可以从后验分布中采样特征值，$\boldsymbol{\lambda}\sim\mathcal{N}(\boldsymbol{m}_{T},\boldsymbol{V}_{T})$。这些特征值可以用于方程（[114](#S4.E114
    "在4.3贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")）中以获得度量中的权重矩阵。注意，贝叶斯度量学习也可以用于主动学习（详情见（Yang
    et al., [2007](#bib.bib134)））。
- en: 4.3.2 Bayesian Neighborhood Component Analysis
  id: totrans-744
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 贝叶斯邻域成分分析
- en: 'Bayesian NCA (Wang & Tan, [2017](#bib.bib115)) using variational inference
    (Ghojogh et al., [2021a](#bib.bib46)) in the NCA formulation. If $\mathcal{N}_{im}$
    denotes the dataset index of the $m$-th nearest neighbor of $\boldsymbol{x}_{i}$,
    we define $\boldsymbol{W}_{i}^{j}:=[w_{ij}-w_{i\mathcal{N}_{i1}},\dots,w_{ij}-w_{i\mathcal{N}_{ik}}]\in\mathbb{R}^{d\times
    k}$. As in the variational inference (Ghojogh et al., [2021a](#bib.bib46)), we
    consider an evidence lower-bound on the log-likelihood:'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯NCA（Wang & Tan, [2017](#bib.bib115)）在NCA公式中使用变分推断（Ghojogh et al., [2021a](#bib.bib46)）。如果
    $\mathcal{N}_{im}$ 表示 $\boldsymbol{x}_{i}$ 的第 $m$ 个最近邻的数据集索引，我们定义 $\boldsymbol{W}_{i}^{j}:=[w_{ij}-w_{i\mathcal{N}_{i1}},\dots,w_{ij}-w_{i\mathcal{N}_{ik}}]\in\mathbb{R}^{d\times
    k}$。与变分推断（Ghojogh et al., [2021a](#bib.bib46)）类似，我们考虑对对数似然函数的证据下界：
- en: '|  | $\displaystyle\log(\mathbb{P}(Y&#124;X,\boldsymbol{\Lambda}))>\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\Big{(}$
    | $\displaystyle-\frac{1}{2}\boldsymbol{\lambda}^{\top}\boldsymbol{W}_{i}^{j}\boldsymbol{H}(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{\lambda}$
    |  |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\log(\mathbb{P}(Y|X,\boldsymbol{\Lambda}))>\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\Big{(}$
    | $\displaystyle-\frac{1}{2}\boldsymbol{\lambda}^{\top}\boldsymbol{W}_{i}^{j}\boldsymbol{H}(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{\lambda}$
    |  |'
- en: '|  |  | $\displaystyle+\boldsymbol{b}_{ij}^{\top}(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{\lambda}-c_{ij}\Big{)},$
    |  |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\boldsymbol{b}_{ij}^{\top}(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{\lambda}-c_{ij}\Big{)},$
    |  |'
- en: 'where $\mathcal{N}_{i}$ was defined before in Section [4.2.3](#S4.SS2.SSS3
    "4.2.3 Fast Neighborhood Component Analysis ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"), $\boldsymbol{H}:=\frac{1}{2}(\boldsymbol{I}-\frac{1}{k+1}\boldsymbol{1}\boldsymbol{1}^{\top})\in\mathbb{R}^{k\times
    k}$ is the centering matrix, and:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{N}_{i}$在[4.2.3](#S4.SS2.SSS3 "4.2.3 快速邻域组件分析 ‣ 4.2 邻域组件分析方法 ‣ 4
    概率度量学习 ‣ 谱、概率和深度度量学习：教程和调查")节中定义，$\boldsymbol{H}:=\frac{1}{2}(\boldsymbol{I}-\frac{1}{k+1}\boldsymbol{1}\boldsymbol{1}^{\top})\in\mathbb{R}^{k\times
    k}$是中心化矩阵，且：
- en: '|  | $\displaystyle\mathbb{R}^{k}\ni\boldsymbol{b}_{ij}:=\boldsymbol{H}\boldsymbol{\psi}_{ij}$
    |  |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{R}^{k}\ni\boldsymbol{b}_{ij}:=\boldsymbol{H}\boldsymbol{\psi}_{ij}$
    |  |'
- en: '|  | $\displaystyle-\exp\Bigg{(}\boldsymbol{\psi}_{ij}-\log\Big{(}1+\sum_{\boldsymbol{x}_{t}\in\mathcal{N}_{i}}\exp\big{(}(\boldsymbol{w}_{ij}-\boldsymbol{w}_{it})^{\top}\boldsymbol{\lambda}\big{)}\Big{)}\Bigg{)},$
    |  | (122) |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle-\exp\Bigg{(}\boldsymbol{\psi}_{ij}-\log\Big{(}1+\sum_{\boldsymbol{x}_{t}\in\mathcal{N}_{i}}\exp\big{(}(\boldsymbol{w}_{ij}-\boldsymbol{w}_{it})^{\top}\boldsymbol{\lambda}\big{)}\Big{)}\Bigg{)},$
    |  | (122) |'
- en: 'in which $\boldsymbol{\psi}_{ij}\in\mathbb{R}^{k}$ is the learnable variational
    parameter. See (Wang & Tan, [2017](#bib.bib115)) for the derivation of this lower-bound.
    The sketch of this derivation is using Eq. ([84](#S4.E84 "In 4.1 Collapsing Classes
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) but for the $k$NN among the similar points, i.e., $\mathcal{N}_{i}$.
    Then, the lower-bound is obtained by a logarithm inequality as well as the Bohning’s
    quadratic bound (Murphy, [2012](#bib.bib89)).'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\boldsymbol{\psi}_{ij}\in\mathbb{R}^{k}$是可学习的变分参数。有关此下界的推导，请参见（Wang & Tan,
    [2017](#bib.bib115)）。该推导的概要是使用方程([84](#S4.E84 "在4.1 退化类 ‣ 4 概率度量学习 ‣ 谱、概率和深度度量学习：教程和调查"))，但针对相似点中的$k$NN，即$\mathcal{N}_{i}$。然后，通过对数不等式以及Bohning的二次界限（Murphy,
    [2012](#bib.bib89)）获得下界。
- en: 'We assume a Gaussian distribution for the prior of $\boldsymbol{\lambda}$ with
    mean $\boldsymbol{m}_{0}\in\mathbb{R}^{d}$ and covariance $\boldsymbol{V}_{0}\in\mathbb{R}^{d\times
    d}$. This prior is assumed to be known. Likewise, we assume a Gaussian distribution
    with mean $\boldsymbol{m}_{T}\in\mathbb{R}^{d}$ and covariance $\boldsymbol{V}_{T}\in\mathbb{R}^{d\times
    d}$ for the posterior $\mathbb{P}(X,\boldsymbol{\Lambda}|Y)$. Using Bayes’ rule
    and the above lower-bound on the likelihood, we can estimate these parameters
    as (Wang & Tan, [2017](#bib.bib115)):'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设$\boldsymbol{\lambda}$的先验是均值为$\boldsymbol{m}_{0}\in\mathbb{R}^{d}$、协方差为$\boldsymbol{V}_{0}\in\mathbb{R}^{d\times
    d}$的高斯分布。这个先验假设是已知的。同样，我们假设后验$\mathbb{P}(X,\boldsymbol{\Lambda}|Y)$是均值为$\boldsymbol{m}_{T}\in\mathbb{R}^{d}$、协方差为$\boldsymbol{V}_{T}\in\mathbb{R}^{d\times
    d}$的高斯分布。利用贝叶斯规则和上述对数似然的下界，我们可以估计这些参数（Wang & Tan, [2017](#bib.bib115)）：
- en: '|  | $\displaystyle\boldsymbol{V}_{T}:=\Big{(}\boldsymbol{V}_{0}^{-1}+\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\boldsymbol{W}_{i}^{j}\boldsymbol{H}(\boldsymbol{W}_{i}^{j})^{\top}\Big{)}^{-1},$
    |  | (123) |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{V}_{T}:=\Big{(}\boldsymbol{V}_{0}^{-1}+\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\boldsymbol{W}_{i}^{j}\boldsymbol{H}(\boldsymbol{W}_{i}^{j})^{\top}\Big{)}^{-1},$
    |  | (123) |'
- en: '|  | $\displaystyle\boldsymbol{m}_{T}:=\boldsymbol{V}_{T}\Big{(}\boldsymbol{V}_{0}^{-1}\boldsymbol{m}_{0}+\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\boldsymbol{W}_{i}^{j}\boldsymbol{b}_{ij}\Big{)}.$
    |  | (124) |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{m}_{T}:=\boldsymbol{V}_{T}\Big{(}\boldsymbol{V}_{0}^{-1}\boldsymbol{m}_{0}+\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{N}_{i}}\boldsymbol{W}_{i}^{j}\boldsymbol{b}_{ij}\Big{)}.$
    |  | (124) |'
- en: 'The variational parameter can also be obtained by (Wang & Tan, [2017](#bib.bib115)):'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 变分参数也可以通过（Wang & Tan, [2017](#bib.bib115)）获得。
- en: '|  | $\displaystyle\boldsymbol{\psi}_{ij}:=(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{m}_{T}.$
    |  | (125) |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\psi}_{ij}:=(\boldsymbol{W}_{i}^{j})^{\top}\boldsymbol{m}_{T}.$
    |  | (125) |'
- en: 'The variables $\boldsymbol{b}_{ij}$, $\boldsymbol{V}_{T}$, $\boldsymbol{m}_{T}$,
    and $\boldsymbol{\psi}_{ij}$ are updated iteratively by Eqs. ([122](#S4.E122 "In
    4.3.2 Bayesian Neighborhood Component Analysis ‣ 4.3 Bayesian Metric Learning
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), ([123](#S4.E123 "In 4.3.2 Bayesian Neighborhood
    Component Analysis ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")),
    ([124](#S4.E124 "In 4.3.2 Bayesian Neighborhood Component Analysis ‣ 4.3 Bayesian
    Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), and ([125](#S4.E125 "In 4.3.2
    Bayesian Neighborhood Component Analysis ‣ 4.3 Bayesian Metric Learning Methods
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), respectively, until convergence.'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 $\boldsymbol{b}_{ij}$、$\boldsymbol{V}_{T}$、$\boldsymbol{m}_{T}$ 和 $\boldsymbol{\psi}_{ij}$
    通过公式 ([122](#S4.E122 "在4.3.2 贝叶斯邻域成分分析 ‣ 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查"))、([123](#S4.E123
    "在4.3.2 贝叶斯邻域成分分析 ‣ 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查"))、([124](#S4.E124
    "在4.3.2 贝叶斯邻域成分分析 ‣ 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查")) 和 ([125](#S4.E125
    "在4.3.2 贝叶斯邻域成分分析 ‣ 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查")) 进行迭代更新，直到收敛。
- en: 'After these parameters are learned, we can sample the eigenvalues from the
    posterior, $\boldsymbol{\lambda}\sim\mathcal{N}(\boldsymbol{m}_{T},\boldsymbol{V}_{T})$.
    These eigenvalues can be used in Eq. ([114](#S4.E114 "In 4.3 Bayesian Metric Learning
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) to obtain the weight matrix in the metric.
    Alternatively, we can directly sample the distance metric from the following distribution:'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些参数被学习之后，我们可以从后验中抽样特征值，$\boldsymbol{\lambda}\sim\mathcal{N}(\boldsymbol{m}_{T},\boldsymbol{V}_{T})$。这些特征值可以用来在公式
    ([114](#S4.E114 "在4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查")) 中获得度量矩阵。或者，我们也可以直接从以下分布中抽样距离度量：
- en: '|  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\sim\mathcal{N}(\boldsymbol{w}_{ij}^{\top}\boldsymbol{m}_{T},\boldsymbol{w}_{ij}^{\top}\boldsymbol{V}_{T}\boldsymbol{w}_{ij}).$
    |  | (126) |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2}\sim\mathcal{N}(\boldsymbol{w}_{ij}^{\top}\boldsymbol{m}_{T},\boldsymbol{w}_{ij}^{\top}\boldsymbol{V}_{T}\boldsymbol{w}_{ij}).$
    |  | (126) |'
- en: 4.3.3 Local Distance Metric (LDM)
  id: totrans-760
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 本地距离度量（LDM）
- en: 'Let the set of similar and dissimilar points for the point $\boldsymbol{x}_{i}$
    be denoted by $\mathcal{S}_{i}$ and $\mathcal{D}_{i}$, respectively. In Local
    Distance Metric (LDM) (Yang et al., [2006](#bib.bib133)), we consider the following
    for the likelihood:'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 设点 $\boldsymbol{x}_{i}$ 的相似点和不相似点集合分别用 $\mathcal{S}_{i}$ 和 $\mathcal{D}_{i}$
    表示。在本地距离度量（LDM）（Yang 等，[2006](#bib.bib133)）中，我们考虑以下似然：
- en: '|  | $\displaystyle\mathbb{P}(y_{i}&#124;\boldsymbol{x}_{i})=$ | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})$
    |  |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}(y_{i}|\boldsymbol{x}_{i})=$ | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2})$
    |  |'
- en: '|  |  | $\displaystyle\times\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})$
    |  |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2})$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp(-\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2})\Big{)}^{-1}.$
    |  | (127) |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp(-\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{W}}^{2})\Big{)}^{-1}.$
    |  | (127) |'
- en: 'If we consider Eq. ([114](#S4.E114 "In 4.3 Bayesian Metric Learning Methods
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) for decomposition of the weight matrix, the log-likelihood
    becomes:'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑公式 ([114](#S4.E114 "在4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查"))
    进行权重矩阵的分解，则对数似然变为：
- en: '|  | $\displaystyle\sum_{i=1}^{n}\log(\mathbb{P}(y_{i}&#124;\boldsymbol{x}_{i},\boldsymbol{\Lambda}))=$
    |  |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{i=1}^{n}\log(\mathbb{P}(y_{i}|\boldsymbol{x}_{i},\boldsymbol{\Lambda}))=$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)}$
    |  |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}$
    |  |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)}.$
    |  |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)}.$
    |  |'
- en: 'We want to maximize this log-likelihood for learning the variables $\{\lambda_{1},\dots,\lambda_{d}\}$.
    An evidence lower bound on this log-likelihood can be (Yang et al., [2006](#bib.bib133)):'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望最大化这个对数似然以学习变量 $\{\lambda_{1},\dots,\lambda_{d}\}$。这个对数似然的证据下界可以参考 (Yang
    et al., [2006](#bib.bib133))：
- en: '|  | $\displaystyle\sum_{i=1}^{n}\log$ | $\displaystyle(\mathbb{P}(y_{i}&#124;\boldsymbol{x}_{i},\boldsymbol{\Lambda}))\geq$
    |  | (128) |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{i=1}^{n}\log$ | $\displaystyle(\mathbb{P}(y_{i}&#124;\boldsymbol{x}_{i},\boldsymbol{\Lambda}))\geq$
    |  | (128) |'
- en: '|  |  | $\displaystyle\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\phi_{ij}\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}$
    |  |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sum_{i=1}^{n}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\phi_{ij}\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}$
    |  |'
- en: '|  |  | $\displaystyle-\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}$
    |  |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\sum_{i=1}^{n}\log\Big{(}\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)},$
    |  |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{j}\in\mathcal{D}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}\Big{)},$
    |  |'
- en: 'where $\phi_{ij}$ is the variational parameter which is:'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi_{ij}$ 是变分参数，其定义为：
- en: '|  |  | $\displaystyle\phi_{ij}:=\frac{\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}\times$
    |  | (129) |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\phi_{ij}:=\frac{\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}\times$
    |  | (129) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\Big{(}1+\frac{\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}\Big{)}^{-1}.$
    |  |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\Big{(}1+\frac{\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}{\sum_{\boldsymbol{x}_{j}\in\mathcal{S}_{i}}\exp\big{(}\!-\sum_{l=1}^{d}\lambda_{l}w_{ij}^{l}\big{)}}\Big{)}^{-1}.$
    |  |'
- en: 'See (Yang et al., [2006](#bib.bib133)) for derivation of the lower bound. Iteratively,
    we maximize the lower bound, i.e. Eq. ([128](#S4.E128 "In 4.3.3 Local Distance
    Metric (LDM) ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), and
    update $\phi_{ij}$ by Eq. ([129](#S4.E129 "In 4.3.3 Local Distance Metric (LDM)
    ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")). The learned parameters
    $\{\lambda_{1},\dots,\lambda_{d}\}$ can be used in Eq. ([114](#S4.E114 "In 4.3
    Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) to obtain the
    weight matrix in the metric.'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 (Yang et al., [2006](#bib.bib133)) 以获取下界的推导。我们通过迭代来最大化下界，即公式 ([128](#S4.E128
    "在 4.3.3 局部距离度量 (LDM) ‣ 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱、概率和深度度量学习：教程和调查"))，并通过公式
    ([129](#S4.E129 "在 4.3.3 局部距离度量 (LDM) ‣ 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱、概率和深度度量学习：教程和调查"))
    更新 $\phi_{ij}$。学到的参数 $\{\lambda_{1},\dots,\lambda_{d}\}$ 可以在公式 ([114](#S4.E114
    "在 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 谱、概率和深度度量学习：教程和调查")) 中使用，以获取度量中的权重矩阵。
- en: 4.4 Information Theoretic Metric Learning
  id: totrans-779
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 信息论度量学习
- en: There exist information theoretic approaches for metric learning where KL-divergence
    (relative entropy) or mutual information is used.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 存在信息论方法用于度量学习，其中使用了 KL 散度（相对熵）或互信息。
- en: 4.4.1 Information Theoretic Metric Learning with a Prior Weight Matrix
  id: totrans-781
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 使用先验权重矩阵的信息论度量学习
- en: 'One of the information theoretic methods for metric learning is using a prior
    weight matrix (Davis et al., [2007](#bib.bib22)) where we consider a known weight
    matrix $\boldsymbol{W}_{0}$ as the regularizer and try to minimize the KL-divergence
    between the distributions with $\boldsymbol{W}$ and $\boldsymbol{W}_{0}$:'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 信息论度量学习的一种方法是使用先验权重矩阵（Davis 等，[2007](#bib.bib22)），我们将已知的权重矩阵$\boldsymbol{W}_{0}$作为正则化项，并尝试最小化分布间的KL散度，即$\boldsymbol{W}$和$\boldsymbol{W}_{0}$之间的KL散度：
- en: '|  | $\displaystyle\text{KL}(p_{ij}^{W_{0}}\&#124;p_{ij}^{W}):=\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij}^{W_{0}}\log\Big{(}\frac{p_{ij}^{W_{0}}}{p_{ij}^{W}}\Big{)}.$
    |  | (130) |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{KL}(p_{ij}^{W_{0}}\&#124;p_{ij}^{W}):=\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij}^{W_{0}}\log\Big{(}\frac{p_{ij}^{W_{0}}}{p_{ij}^{W}}\Big{)}.$
    |  | (130) |'
- en: There are both offline and online approaches for metric learning using batch
    and streaming data, respectively.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 对于度量学习，有离线和在线方法，分别使用批量和流数据。
- en: '– Offline Information Theoretic Metric Learning: We consider a Gaussian distribution,
    i.e. Eq. ([84](#S4.E84 "In 4.1 Collapsing Classes ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")), for
    the probability of $\boldsymbol{x}_{i}$ taking $\boldsymbol{x}_{j}$ as its neighbor,
    i.e. $p_{ij}^{W}$. While we make the weight matrix similar to the prior weight
    matrix through KL-divergence, we find a weight matrix which makes all the distances
    of similar points less than an upper bound $u>0$ and all the distances of dissimilar
    points larger than a lower bound $l$ (where $l>u$). Note that, for Gaussian distributions,
    the KL divergence is related to the LogDet $D_{ld}(.,.)$ between covariance matrices
    (Dhillon, [2007](#bib.bib25)); hence, we can say:'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: – 离线信息论度量学习：我们考虑高斯分布，即公式 ([84](#S4.E84 "在 4.1 类别收缩 ‣ 4 概率度量学习 ‣ 光谱、概率及深度度量学习：教程与综述"))，用于计算$\boldsymbol{x}_{i}$将$\boldsymbol{x}_{j}$作为邻居的概率，即$p_{ij}^{W}$。当我们通过KL散度使得权重矩阵与先验权重矩阵相似时，我们找到一个权重矩阵，使得所有相似点的距离小于上界$u>0$，而所有不相似点的距离大于下界$l$（其中$l>u$）。请注意，对于高斯分布，KL散度与协方差矩阵之间的LogDet
    $D_{ld}(.,.)$有关（Dhillon, [2007](#bib.bib25)）；因此，我们可以说：
- en: '|  | $\displaystyle\text{KL}(p_{ij}^{W_{0}}\&#124;p_{ij}^{W})=\frac{1}{2}D_{ld}(\boldsymbol{W}_{0}^{-1},\boldsymbol{W}^{-1})=\frac{1}{2}D_{ld}(\boldsymbol{W},\boldsymbol{W}_{0})$
    |  |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{KL}(p_{ij}^{W_{0}}\&#124;p_{ij}^{W})=\frac{1}{2}D_{ld}(\boldsymbol{W}_{0}^{-1},\boldsymbol{W}^{-1})=\frac{1}{2}D_{ld}(\boldsymbol{W},\boldsymbol{W}_{0})$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})-\log(\det(\boldsymbol{W}\boldsymbol{W}_{0}^{-1}))-n,$
    |  |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\overset{(a)}{=}\textbf{tr}(\boldsymbol{W}\boldsymbol{W}_{0}^{-1})-\log(\det(\boldsymbol{W}\boldsymbol{W}_{0}^{-1}))-n,$
    |  |'
- en: 'where $(a)$ is because of the definition of LogDet. Hence, the optimization
    problem can be (Davis et al., [2007](#bib.bib22)):'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(a)$ 是因为LogDet的定义。因此，优化问题可以是（Davis 等，[2007](#bib.bib22)）：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle
    D_{ld}(\boldsymbol{W},\boldsymbol{W}_{0})$ |  | (131) |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle
    D_{ld}(\boldsymbol{W},\boldsymbol{W}_{0})$ |  | (131) |'
- en: '|  |  | subject to |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    u,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},$ |  |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    u,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S},$ |  |'
- en: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\geq
    l,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.$ |  |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\geq
    l,\quad\forall(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.$ |  |'
- en: '– Online Information Theoretic Metric Learning: The online information theoretic
    metric learning (Davis et al., [2007](#bib.bib22)) is suitable for streaming data.
    For this, we use the offline approach where the known weight matrix $\boldsymbol{W}_{0}$
    is learned weight matrix by the data which have been received so far. Consider
    the time slot $t$ where we have been accumulated some data until then and some
    new data points are received at this time. The optimization problem is Eq. ([131](#S4.E131
    "In 4.4.1 Information Theoretic Metric Learning with a Prior Weight Matrix ‣ 4.4
    Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")) where $\boldsymbol{W}_{0}=\boldsymbol{W}_{t}$
    which is the learned weight matrix so far at time $t$. Note that if there is some
    label information available, we can incorporate it in the optimization problem
    as a regularizer.'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: '– 在线信息理论度量学习：在线信息理论度量学习（Davis等，[2007](#bib.bib22)）适用于流数据。为此，我们使用离线方法，其中已知的权重矩阵
    $\boldsymbol{W}_{0}$ 是通过目前已接收到的数据学习得到的权重矩阵。考虑时间段 $t$，在此期间我们已经积累了一些数据，并且此时接收到一些新的数据点。优化问题为公式
    ([131](#S4.E131 "In 4.4.1 Information Theoretic Metric Learning with a Prior Weight
    Matrix ‣ 4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))，其中
    $\boldsymbol{W}_{0}=\boldsymbol{W}_{t}$ 是在时间 $t$ 时至今学习到的权重矩阵。请注意，如果有可用的标签信息，我们可以将其作为正则项纳入优化问题中。'
- en: 4.4.2 Information Theoretic Metric Learning for Imbalanced Data
  id: totrans-793
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 信息理论度量学习用于失衡数据
- en: 'Distance Metric by Balancing KL-divergence (DMBK) (Feng et al., [2018](#bib.bib31))
    can be used for imbalanced data where the cardinality of classes are different.
    Assume the classes have Gaussian distributions where $\boldsymbol{\mu}_{i}\in\mathbb{R}^{d}$
    and $\boldsymbol{\Sigma}_{i}\in\mathbb{R}^{d\times d}$ denote the mean and covariance
    of the $i$-th class. Recall the projection matrix $\boldsymbol{U}$ in Eq. ([9](#S2.E9
    "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) and Proposition [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"). The KL-divergence between the probabilities of the $i$-th and $j$-th
    classes after projection onto the subspace of metric is (Feng et al., [2018](#bib.bib31)):'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: '基于平衡KL散度的距离度量（DMBK）（Feng等，[2018](#bib.bib31)）可以用于类别数量不同的失衡数据。假设类别具有高斯分布，其中
    $\boldsymbol{\mu}_{i}\in\mathbb{R}^{d}$ 和 $\boldsymbol{\Sigma}_{i}\in\mathbb{R}^{d\times
    d}$ 表示第 $i$ 类的均值和协方差。回顾公式中的投影矩阵 $\boldsymbol{U}$ ([9](#S2.E9 "In Proof. ‣ 2.3
    Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 和命题
    [2](#Thmproposition2 "Proposition 2 (Projection in metric learning). ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")。KL散度在度量的子空间上的第 $i$ 类和第 $j$ 类的概率之间为（Feng等，[2018](#bib.bib31)）：'
- en: '|  | $\displaystyle\text{KL}(p_{i}\&#124;p_{j})=$ | $\displaystyle\,\frac{1}{2}\Big{(}\log\big{(}\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{j}\boldsymbol{U})\big{)}$
    |  | (132) |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{KL}(p_{i}\&#124;p_{j})=$ | $\displaystyle\,\frac{1}{2}\Big{(}\log\big{(}\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{j}\boldsymbol{U})\big{)}$
    |  | (132) |'
- en: '|  |  | $\displaystyle-\log\big{(}\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{i}\boldsymbol{U})\big{)}$
    |  |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\log\big{(}\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{i}\boldsymbol{U})\big{)}$
    |  |'
- en: '|  |  | $\displaystyle+\textbf{tr}\big{(}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{j}\boldsymbol{U})^{-1}\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}_{i}+\boldsymbol{D}_{ij})\boldsymbol{U}\big{)}\Big{)},$
    |  |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\textbf{tr}\big{(}(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{j}\boldsymbol{U})^{-1}\boldsymbol{U}^{\top}(\boldsymbol{\Sigma}_{i}+\boldsymbol{D}_{ij})\boldsymbol{U}\big{)}\Big{)},$
    |  |'
- en: 'where $\boldsymbol{D}_{ij}:=(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{j})(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{j})^{\top}$.
    To cancel the effect of cardinality of classes in imbalanced data, we use the
    normalized divergence of classes:'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{D}_{ij}:=(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{j})(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{j})^{\top}$。为了消除失衡数据中类别数量的影响，我们使用类别的归一化散度：
- en: '|  | $\displaystyle e_{ij}:=\frac{n_{i}n_{j}\text{KL}(p_{i}\&#124;p_{j})}{\sum_{1\leq
    k<l\leq c}n_{k}n_{l}\text{KL}(p_{k}\&#124;p_{l})},$ |  | (133) |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle e_{ij}:=\frac{n_{i}n_{j}\text{KL}(p_{i}\&#124;p_{j})}{\sum_{1\leq
    k<l\leq c}n_{k}n_{l}\text{KL}(p_{k}\&#124;p_{l})},$ |  | (133) |'
- en: 'where $n_{i}$ and $c$ denote the number of the $i$-th class and the number
    of classes, respectively. We maximize the geometric mean of this divergence between
    pairs of classes to separate classes after projection onto the subspace of metric.
    A regularization term is used to increase the distances of dissimilar points and
    a constraint is used to decrease the similar points (Feng et al., [2018](#bib.bib31)):'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n_{i}$ 和 $c$ 分别表示第 $i$ 类的数量和类别总数。我们最大化类别对之间这种散度的几何均值，以便在投影到度量子空间后分离类别。使用正则化项来增加不同点之间的距离，并且使用约束来减少相似点之间的距离
    (Feng et al., [2018](#bib.bib31))：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\log\Big{(}\Big{(}\prod_{1\leq
    i<j\leq c}e_{ij}\Big{)}^{\frac{1}{c(c-1)}}\Big{)}$ |  | (134) |'
  id: totrans-801
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最大化}}$ |  | $\displaystyle\log\Big{(}\Big{(}\prod_{1\leq
    i<j\leq c}e_{ij}\Big{)}^{\frac{1}{c(c-1)}}\Big{)}$ |  | (134) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}$
    |  |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\lambda\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1,$ |  |'
  id: totrans-803
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}}\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}\leq
    1,$ |  |'
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: where $\lambda>0$ is the regularization parameter. This problem can be solved
    using projected gradient method (Ghojogh et al., [2021c](#bib.bib48)).
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 为正则化参数。该问题可以通过投影梯度法来解决（Ghojogh et al., [2021c](#bib.bib48)）。
- en: 4.4.3 Probabilistic Relevant Component Analysis Methods
  id: totrans-806
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3 概率相关组件分析方法
- en: 'Recall the Relevant Component Analysis (RCA method) (Shental et al., [2002](#bib.bib101))
    which was introduced in Section [3.1.4](#S3.SS1.SSS4 "3.1.4 Relevant Component
    Analysis (RCA) ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"). Here,
    we introduce probabilistic RCA (Bar-Hillel et al., [2003](#bib.bib6), [2005](#bib.bib7))
    which uses information theory. Suppose the $n$ data points can be divided into
    $c$ clusters, or so-called chunklets. Let $\mathcal{X}_{l}$ denote the data of
    the $l$-th chunklet and $\boldsymbol{\mu}_{l}$ be the mean of $\mathcal{X}_{l}$.
    Consider Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2
    Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) for decomposition of the weight matrix in the
    metric where the column-space of $\boldsymbol{U}$ is the subspace of metric. Let
    projection of data onto this subspace be denoted by $\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$,
    the projected data in the $l$-th chunklet be $\mathcal{Y}_{l}$, and $\boldsymbol{\mu}^{y}_{l}$
    be the mean of $\mathcal{Y}_{l}$.'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: '回忆相关组件分析（RCA 方法）（Shental et al., [2002](#bib.bib101)），该方法在第 [3.1.4](#S3.SS1.SSS4
    "3.1.4 Relevant Component Analysis (RCA) ‣ 3.1 Spectral Methods Using Scatters
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey") 节中介绍。在这里，我们引入使用信息理论的概率 RCA（Bar-Hillel et al., [2003](#bib.bib6),
    [2005](#bib.bib7)）。假设 $n$ 个数据点可以被划分为 $c$ 个簇，或称为块。设 $\mathcal{X}_{l}$ 表示第 $l$ 个块的数据，$\boldsymbol{\mu}_{l}$
    为 $\mathcal{X}_{l}$ 的均值。考虑公式 ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) 用于度量中权重矩阵的分解，其中 $\boldsymbol{U}$
    的列空间是度量的子空间。设数据投影到该子空间的表示为 $\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$，第
    $l$ 个块中的投影数据为 $\mathcal{Y}_{l}$，$\boldsymbol{\mu}^{y}_{l}$ 为 $\mathcal{Y}_{l}$
    的均值。'
- en: 'In probabilistic RCA, we maximize the mutual information between data and the
    projected data while we want the summation of distances of points in a chunklet
    from the mean of chunklet is less than a threshold or margin $m>0$. The mutual
    information is related to the entropy as $I(X,Y):=H(Y)-H(Y|X)$; hence, we can
    maximize the entropy of projected data $H(Y)$ rather than the mutual information.
    Because $\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$, we have $H(Y)\propto\det(\boldsymbol{U})$.
    According to Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), we have $\det(\boldsymbol{U})\propto\det(\boldsymbol{W})$.
    Hence, the optimization problem can be (Bar-Hillel et al., [2003](#bib.bib6),
    [2005](#bib.bib7)):'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: '在概率型 RCA 中，我们最大化数据与投影数据之间的互信息，同时我们希望一个小块中点到小块均值的距离之和小于阈值或边际 $m>0$。互信息与熵相关，如
    $I(X,Y):=H(Y)-H(Y|X)$；因此，我们可以最大化投影数据的熵 $H(Y)$ 而不是互信息。因为 $\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$，我们有
    $H(Y)\propto\det(\boldsymbol{U})$。根据方程 ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))，我们有 $\det(\boldsymbol{U})\propto\det(\boldsymbol{W})$。因此，优化问题可以表示为
    (Bar-Hillel et al., [2003](#bib.bib6), [2005](#bib.bib7))：'
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\det(\boldsymbol{W})$
    |  | (135) |'
  id: totrans-809
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\det(\boldsymbol{W})$
    |  | (135) |'
- en: '|  |  | subject to |  | $\displaystyle\sum_{l=1}^{c}\sum_{\boldsymbol{y}_{i}\in\mathcal{Y}_{l}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{\mu}^{y}_{l}\&#124;_{\boldsymbol{W}}^{2}\leq
    m,$ |  |'
  id: totrans-810
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\sum_{l=1}^{c}\sum_{\boldsymbol{y}_{i}\in\mathcal{Y}_{l}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{\mu}^{y}_{l}\&#124;_{\boldsymbol{W}}^{2}\leq
    m,$ |  |'
- en: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
  id: totrans-811
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: This preserves the information of data after projection while the inter-chunklet
    variances are upper-bounded by a margin.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 这在投影后保留了数据的信息，同时小块间的方差被边际上界。
- en: 'If we assume Gaussian distribution for each chunklet with the covariance matrix
    $\boldsymbol{\Sigma}_{l}$ for the $l$-th chunklet, we have $\det(\boldsymbol{W})\propto\log(\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{l}\boldsymbol{U}))$
    because of the quadratic characteristic of covariance. In this case, the optimization
    problem becomes:'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设每个小块的高斯分布，且 $l$-th 小块的协方差矩阵为 $\boldsymbol{\Sigma}_{l}$，我们有 $\det(\boldsymbol{W})\propto\log(\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{l}\boldsymbol{U}))$，这是由于协方差的二次特性。在这种情况下，优化问题变为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{l=1}^{c}\log(\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{l}\boldsymbol{U}))$
    |  | (136) |'
  id: totrans-814
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{maximize}}$ |  | $\displaystyle\sum_{l=1}^{c}\log(\det(\boldsymbol{U}^{\top}\boldsymbol{\Sigma}_{l}\boldsymbol{U}))$
    |  | (136) |'
- en: '|  |  | subject to |  | $\displaystyle\sum_{l=1}^{c}\sum_{\boldsymbol{y}_{i}\in\mathcal{Y}_{l}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{\mu}^{y}_{l}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}\leq
    m,$ |  |'
  id: totrans-815
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\sum_{l=1}^{c}\sum_{\boldsymbol{y}_{i}\in\mathcal{Y}_{l}}\&#124;\boldsymbol{y}_{i}-\boldsymbol{\mu}^{y}_{l}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}\leq
    m,$ |  |'
- en: 'where $\boldsymbol{W}\succeq\boldsymbol{0}$ is already satisfied because of
    Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")).'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '因为方程 ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) 的关系，$\boldsymbol{W}\succeq\boldsymbol{0}$ 已经得到满足。'
- en: 4.4.4 Metric Learning by Information Geometry
  id: totrans-817
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4 通过信息几何进行度量学习
- en: 'Another information theoretic methods for metric learning is using information
    geometry in which kernels on data and labels are used (Wang & Jin, [2009](#bib.bib118)).
    Let $\boldsymbol{L}\in\mathbb{R}^{c\times n}$ denote the one-hot encoded labels
    of $n$ data points with $c$ classes and let $\boldsymbol{X}\in\mathbb{R}^{d\times
    n}$ be the data points. The kernel matrix on the labels is $\boldsymbol{K}_{L}=\boldsymbol{Y}^{\top}\boldsymbol{Y}+\lambda\boldsymbol{I}$
    whose main diagonal is strengthened by a small positive number $\lambda$ to have
    a full rank. Recall Proposition [2](#Thmproposition2 "Proposition 2 (Projection
    in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") and Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) where $\boldsymbol{U}$ is the projection
    matrix onto the subspace of metric. The kernel matrix over the projected data,
    $\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$, is:'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种信息理论方法用于度量学习是使用信息几何，其中数据和标签上的核被使用（Wang & Jin，[2009](#bib.bib118)）。设$\boldsymbol{L}\in\mathbb{R}^{c\times
    n}$表示$n$个数据点的独热编码标签，其中有$c$个类别，$\boldsymbol{X}\in\mathbb{R}^{d\times n}$为数据点。标签上的核矩阵为$\boldsymbol{K}_{L}=\boldsymbol{Y}^{\top}\boldsymbol{Y}+\lambda\boldsymbol{I}$，其主对角线通过一个小的正数$\lambda$得到加强以保证满秩。回顾命题[2](#Thmproposition2
    "命题2（度量学习中的投影）。 ‣ 2.3 一般化马氏距离 ‣ 2 一般化马氏距离度量 ‣ 谱、概率和深度度量学习：教程与综述")和等式([9](#S2.E9
    "证明中。 ‣ 2.3 一般化马氏距离 ‣ 2 一般化马氏距离度量 ‣ 谱、概率和深度度量学习：教程与综述"))，其中$\boldsymbol{U}$是映射到度量子空间的投影矩阵。投影数据上的核矩阵$\boldsymbol{Y}=\boldsymbol{U}^{\top}\boldsymbol{X}$是：
- en: '|  | $\displaystyle\boldsymbol{K}_{Y}=$ | $\displaystyle\boldsymbol{Y}^{\top}\boldsymbol{Y}=(\boldsymbol{U}^{\top}\boldsymbol{X})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{X})$
    |  |'
  id: totrans-819
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{K}_{Y}=$ | $\displaystyle\boldsymbol{Y}^{\top}\boldsymbol{Y}=(\boldsymbol{U}^{\top}\boldsymbol{X})^{\top}(\boldsymbol{U}^{\top}\boldsymbol{X})$
    |  |'
- en: '|  |  | $\displaystyle=\boldsymbol{X}^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{X}\overset{(\ref{equation_W_U_UT})}{=}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X}.$
    |  | (137) |'
  id: totrans-820
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\boldsymbol{X}^{\top}\boldsymbol{U}\boldsymbol{U}^{\top}\boldsymbol{X}\overset{(\ref{equation_W_U_UT})}{=}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X}.$
    |  | (137) |'
- en: 'We can minimize the KL-divergence between the distributions of kernels $\boldsymbol{K}_{Y}$
    and $\boldsymbol{K}_{L}$ (Wang & Jin, [2009](#bib.bib118)):'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以最小化核函数$\boldsymbol{K}_{Y}$和$\boldsymbol{K}_{L}$的分布之间的KL散度（Wang & Jin，[2009](#bib.bib118)）：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\text{KL}(\boldsymbol{K}_{Y}\&#124;\boldsymbol{K}_{L})$
    |  | (138) |'
  id: totrans-822
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{最小化}}$ |  | $\displaystyle\text{KL}(\boldsymbol{K}_{Y}\&#124;\boldsymbol{K}_{L})$
    |  | (138) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-823
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受制于 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: 'For simplicity, we assume Gaussian distributions for the kernels. The KL divergence
    between the distributions of two matrices, $\boldsymbol{K}_{Y}\in\mathbb{R}^{n\times
    n}$ and $\boldsymbol{K}_{L}\in\mathbb{R}^{n\times n}$, with Gaussian distributions
    is simplified to (Wang & Jin, [2009](#bib.bib118), Theorem 1):'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 为简化起见，我们假设核函数服从高斯分布。两矩阵$\boldsymbol{K}_{Y}\in\mathbb{R}^{n\times n}$和$\boldsymbol{K}_{L}\in\mathbb{R}^{n\times
    n}$的高斯分布之间的KL散度被简化为（Wang & Jin，[2009](#bib.bib118)，定理1）：
- en: '|  | $\displaystyle\text{KL}(\boldsymbol{K}_{Y}\&#124;\boldsymbol{K}_{L})=\frac{1}{2}\Big{(}\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{K}_{Y})+\log(\det(\boldsymbol{K}_{L}))$
    |  |'
  id: totrans-825
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{KL}(\boldsymbol{K}_{Y}\&#124;\boldsymbol{K}_{L})=\frac{1}{2}\Big{(}\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{K}_{Y})+\log(\det(\boldsymbol{K}_{L}))$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\log(\det(\boldsymbol{K}_{Y}))-n\Big{)}$
    |  |'
  id: totrans-826
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\log(\det(\boldsymbol{K}_{Y}))-n\Big{)}$
    |  |'
- en: '|  | $\displaystyle\overset{(\ref{equation_ML_information_geometry_K_Y})}{\propto}\frac{1}{2}\Big{(}\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})+\log(\det(\boldsymbol{K}_{L}))$
    |  |'
  id: totrans-827
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(\ref{equation_ML_information_geometry_K_Y})}{\propto}\frac{1}{2}\Big{(}\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})+\log(\det(\boldsymbol{K}_{L}))$
    |  |'
- en: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\log(\det(\boldsymbol{W}))-n\Big{)}.$
    |  |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\log(\det(\boldsymbol{W}))-n\Big{)}.$
    |  |'
- en: 'After ignoring the constant terms w.r.t. $\boldsymbol{W}$, we can restate Eq.
    ([138](#S4.E138 "In 4.4.4 Metric Learning by Information Geometry ‣ 4.4 Information
    Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) to:'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略关于 $\boldsymbol{W}$ 的常数项后，我们可以将公式 ([138](#S4.E138 "在4.4.4 信息几何的量度学习 ‣ 4.4
    信息论量度学习 ‣ 4 概率量度学习 ‣ 谱、概率和深度量度学习：教程与调查")) 重新表述为：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})-\log(\det(\boldsymbol{W}))$
    |  | (139) |'
  id: totrans-830
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{minimize}}$ |  | $\displaystyle\textbf{tr}(\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}\boldsymbol{W}\boldsymbol{X})-\log(\det(\boldsymbol{W}))$
    |  | (139) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-831
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 满足 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0}.$ |  |'
- en: 'If we take the derivative of the objective function in Eq. ([139](#S4.E139
    "In 4.4.4 Metric Learning by Information Geometry ‣ 4.4 Information Theoretic
    Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and
    Deep Metric Learning: Tutorial and Survey")) and set it to zero, we have:'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对公式 ([139](#S4.E139 "在4.4.4 信息几何的量度学习 ‣ 4.4 信息论量度学习 ‣ 4 概率量度学习 ‣ 谱、概率和深度量度学习：教程与调查"))
    中的目标函数求导并设为零，我们得到：
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\boldsymbol{X}\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}-\boldsymbol{W}^{-1}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{W}}=\boldsymbol{X}\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top}-\boldsymbol{W}^{-1}\overset{\text{set}}{=}\boldsymbol{0}$
    |  |'
- en: '|  | $\displaystyle\implies\boldsymbol{W}=(\boldsymbol{X}\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top})^{-1}.$
    |  | (140) |'
  id: totrans-834
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\implies\boldsymbol{W}=(\boldsymbol{X}\boldsymbol{K}_{L}^{-1}\boldsymbol{X}^{\top})^{-1}.$
    |  | (140) |'
- en: 'Note that the constraint $\boldsymbol{W}\succeq\boldsymbol{0}$ is already satisfied
    by the solution, i.e., Eq. ([140](#S4.E140 "In 4.4.4 Metric Learning by Information
    Geometry ‣ 4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，约束条件 $\boldsymbol{W}\succeq\boldsymbol{0}$ 已由解满足，即公式 ([140](#S4.E140 "在4.4.4
    信息几何的量度学习 ‣ 4.4 信息论量度学习 ‣ 4 概率量度学习 ‣ 谱、概率和深度量度学习：教程与调查"))。
- en: 'Although this method has used kernels, it can be kernelized further. We can
    also have a kernel version of this method by using Eq. ([54](#S3.E54 "In Lemma
    4\. ‣ 3.6.4 Kernel Discriminative Component Analysis ‣ 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")) as the generalized Mahalanobis distance in the
    feature space, where $\boldsymbol{T}$ (defined in Eq. ([46](#S3.E46 "In 3.6.2
    Regularization by Locally Linear Embedding ‣ 3.6 Kernel Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))) is the projection matrix for the metric. Using this in
    Eqs. ([139](#S4.E139 "In 4.4.4 Metric Learning by Information Geometry ‣ 4.4 Information
    Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) and ([140](#S4.E140 "In 4.4.4
    Metric Learning by Information Geometry ‣ 4.4 Information Theoretic Metric Learning
    ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) can give us the kernel version of this method. See (Wang
    & Jin, [2009](#bib.bib118)) for more information about it.'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此方法已经使用了核，但它可以进一步进行核化。我们还可以通过使用公式 ([54](#S3.E54 "在引理4\. ‣ 3.6.4 核判别分量分析 ‣
    3.6 核谱量度学习 ‣ 3 谱量度学习 ‣ 谱、概率和深度量度学习：教程与调查")) 作为特征空间中的广义马氏距离来得到这个方法的核版本，其中 $\boldsymbol{T}$（定义在公式
    ([46](#S3.E46 "在3.6.2 局部线性嵌入的正则化 ‣ 3.6 核谱量度学习 ‣ 3 谱量度学习 ‣ 谱、概率和深度量度学习：教程与调查")))
    是度量的投影矩阵。将其应用于公式 ([139](#S4.E139 "在4.4.4 信息几何的量度学习 ‣ 4.4 信息论量度学习 ‣ 4 概率量度学习 ‣
    谱、概率和深度量度学习：教程与调查")) 和 ([140](#S4.E140 "在4.4.4 信息几何的量度学习 ‣ 4.4 信息论量度学习 ‣ 4 概率量度学习
    ‣ 谱、概率和深度量度学习：教程与调查")) 可以得到这个方法的核版本。有关更多信息，请参见（Wang & Jin, [2009](#bib.bib118)）。
- en: 4.5 Empirical Risk Minimization in Metric Learning
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 量化学习中的经验风险最小化
- en: We can learn the metric by minimizing some empirical risk. In the following,
    some metric learning metric learning methods by risk minimization are introduced.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过最小化某些经验风险来学习度量。接下来介绍了一些通过风险最小化的度量学习方法。
- en: 4.5.1 Metric Learning Using the Sigmoid Function
  id: totrans-839
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1 使用 sigmoid 函数的度量学习
- en: 'One of the metric learning methods by risk minimization is (Guillaumin et al.,
    [2009](#bib.bib57)). The distribution for $\boldsymbol{x}_{i}$ to take $\boldsymbol{x}_{j}$
    as its neighbor can be stated using a sigmoid function:'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 通过风险最小化的度量学习方法之一是（Guillaumin 等人，[2009](#bib.bib57)）。可以使用 sigmoid 函数来表述 $\boldsymbol{x}_{i}$
    将 $\boldsymbol{x}_{j}$ 作为其邻居的分布：
- en: '|  | $\displaystyle p^{W}_{ij}$ | $\displaystyle:=\frac{1}{1+\exp(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-b)},$
    |  | (141) |'
  id: totrans-841
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{W}_{ij}$ | $\displaystyle:=\frac{1}{1+\exp(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}}^{2}-b)},$
    |  | (141) |'
- en: 'where $b>0$ is a bias, because close-by points should have larger probability.
    We can maximize and minimize this probability for similar and dissimilar points,
    respectively:'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b>0$ 是一个偏差，因为邻近点应具有更大的概率。我们可以分别最大化和最小化这种概率，以适应相似点和不相似点：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}y_{ij}\log(p^{W}_{ij})+(1-y_{ij})\log(1-p^{W}_{ij})$
    |  | (142) |'
  id: totrans-843
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}}{\text{maximize}}$ |  | $\displaystyle\sum_{i=1}^{n}\sum_{j=1}^{n}y_{ij}\log(p^{W}_{ij})+(1-y_{ij})\log(1-p^{W}_{ij})$
    |  | (142) |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$
    |  |'
  id: totrans-844
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 约束条件 |  | $\displaystyle\boldsymbol{W}\succeq\boldsymbol{0},$ |  |'
- en: 'where $y_{ij}$ is defined in Eq. ([117](#S4.E117 "In 4.3.1 Bayesian Metric
    Learning Using Sigmoid Function ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). This can be solved using projected gradient method (Ghojogh et al.,
    [2021c](#bib.bib48)). This optimization can be seen as minimization of the empirical
    risk where close-by points are pushed toward each other and dissimilar points
    are pushed away to have less error.'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $y_{ij}$ 在公式 ([117](#S4.E117 "In 4.3.1 Bayesian Metric Learning Using Sigmoid
    Function ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 中定义。这个问题可以使用投影梯度方法（Ghojogh
    等人，[2021c](#bib.bib48)）解决。这个优化问题可以看作是最小化经验风险，其中邻近点被推向彼此，而不相似的点被推远，以减少错误。'
- en: 4.5.2 Pairwise Constrained Component Analysis (PCCA)
  id: totrans-846
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2 成对约束成分分析（PCCA）
- en: 'Pairwise Constrained Component Analysis (PCCA) (Mignon & Jurie, [2012](#bib.bib85))
    minimizes the following empirical risk to minimize and maximize the distances
    of similar points and dissimilar points, respectively:'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 成对约束成分分析（PCCA）（Mignon & Jurie，[2012](#bib.bib85)）最小化以下经验风险，以分别最小化和最大化相似点和不相似点的距离：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | (143)
    |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}$ |  | (143)
    |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\sum_{j=1}^{n}\log\Big{(}1+\exp\big{(}y_{ij}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}-b)\big{)}\Big{)},$
    |  |'
  id: totrans-849
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}\sum_{i=1}^{n}\sum_{j=1}^{n}\log\Big{(}1+\exp\big{(}y_{ij}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}-b)\big{)}\Big{)},$
    |  |'
- en: 'where $y_{ij}$ is defined in Eq. ([117](#S4.E117 "In 4.3.1 Bayesian Metric
    Learning Using Sigmoid Function ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), $b>0$ is a bias, $\boldsymbol{W}\succeq\boldsymbol{0}$ is already
    satisfied because of Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). This can be solved using projected
    gradient method (Ghojogh et al., [2021c](#bib.bib48)) with the gradient (Mignon
    & Jurie, [2012](#bib.bib85)):'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $y_{ij}$ 在公式 ([117](#S4.E117 "In 4.3.1 Bayesian Metric Learning Using Sigmoid
    Function ‣ 4.3 Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 中定义，$b>0$
    是一个偏差，$\boldsymbol{W}\succeq\boldsymbol{0}$ 因为公式 ([9](#S2.E9 "In Proof. ‣ 2.3
    Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) 已经满足。这个问题可以使用投影梯度方法（Ghojogh
    等人，[2021c](#bib.bib48)）解决，其梯度（Mignon & Jurie，[2012](#bib.bib85)）为：'
- en: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}$ | $\displaystyle=2\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{y_{ij}}{1+\exp\big{(}y_{ij}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}-b)\big{)}}$
    |  | (144) |'
  id: totrans-851
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial c}{\partial\boldsymbol{U}}$ | $\displaystyle=2\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{y_{ij}}{1+\exp\big{(}y_{ij}(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}-b)\big{)}}$
    |  | (144) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\times(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}.$
    |  |'
  id: totrans-852
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\times(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}.$
    |  |'
- en: 'Note that we can have kernel PCCA by using Eq. ([54](#S3.E54 "In Lemma 4\.
    ‣ 3.6.4 Kernel Discriminative Component Analysis ‣ 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")). In other words, we can replace $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}$
    and $(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}$
    with $\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    and $(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}$,
    respectively, to have PCCA in the feature space.'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以通过使用公式 ([54](#S3.E54 "在引理4\. ‣ 3.6.4 核判别成分分析 ‣ 3.6 核谱度量学习 ‣ 3 谱度量学习 ‣
    谱的、概率的和深度的度量学习：教程与调查")) 来获得核PCCA。换句话说，我们可以将 $\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\|_{\boldsymbol{U}\boldsymbol{U}^{\top}}^{2}$
    和 $(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})(\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^{\top}\boldsymbol{U}$
    分别替换为 $\|\boldsymbol{k}_{i}-\boldsymbol{k}_{j}\|_{\boldsymbol{T}\boldsymbol{T}^{\top}}^{2}$
    和 $(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})(\boldsymbol{k}_{i}-\boldsymbol{k}_{j})^{\top}\boldsymbol{T}$，以在特征空间中实现PCCA。
- en: 4.5.3 Metric Learning for Privileged Information
  id: totrans-854
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.3 特权信息的度量学习
- en: 'In some applications, we have a dataset with privileged information where for
    every point, we have two feature vector; one for the main feature (denoted by
    $\{\boldsymbol{x}_{i}\}_{i=1}^{n}$) and one for the privileged information (denoted
    by $\{\boldsymbol{z}_{i}\}_{i=1}^{n}$). A metric learning method for using privileged
    information is (Yang et al., [2016](#bib.bib137)) where we minimize and maximize
    the distances of similar and dissimilar points, respectively, for the main features.
    Simultaneously, we make the distances of privileged features close to the distances
    of main features. Having these two simultaneous goals, we minimize the following
    empirical risk (Yang et al., [2016](#bib.bib137)):'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些应用中，我们有一个包含特权信息的数据集，其中每个点都有两个特征向量；一个是主要特征（记作 $\{\boldsymbol{x}_{i}\}_{i=1}^{n}$），另一个是特权信息（记作
    $\{\boldsymbol{z}_{i}\}_{i=1}^{n}$）。一种使用特权信息的度量学习方法是 (Yang 等，[2016](#bib.bib137))，其中我们分别最小化和最大化相似点和不相似点的距离，同时使特权特征的距离接近主要特征的距离。在这两个目标同时存在的情况下，我们最小化以下经验风险
    (Yang 等，[2016](#bib.bib137))：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{W}_{1},\boldsymbol{W}_{2}}{\text{minimize}}$
    |  | $\displaystyle\sum_{i=1}^{n}\log\Big{(}1+$ |  | (145) |'
  id: totrans-856
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{W}_{1},\boldsymbol{W}_{2}}{\text{minimize}}$
    |  | $\displaystyle\sum_{i=1}^{n}\log\Big{(}1+$ |  | (145) |'
- en: '|  |  | $\displaystyle\exp\big{(}y_{ij}\,(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}_{1}}^{2}-\&#124;\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\&#124;_{\boldsymbol{W}_{2}}^{2})\big{)}\Big{)}$
    |  |'
  id: totrans-857
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\exp\big{(}y_{ij}\,(\&#124;\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\&#124;_{\boldsymbol{W}_{1}}^{2}-\&#124;\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\&#124;_{\boldsymbol{W}_{2}}^{2})\big{)}\Big{)}$
    |  |'
- en: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}_{1}\succeq\boldsymbol{0},\quad\boldsymbol{W}_{2}\succeq\boldsymbol{0}.$
    |  |'
  id: totrans-858
  prefs: []
  type: TYPE_TB
  zh: '|  |  | subject to |  | $\displaystyle\boldsymbol{W}_{1}\succeq\boldsymbol{0},\quad\boldsymbol{W}_{2}\succeq\boldsymbol{0}.$
    |  |'
- en: 5 Deep Metric Learning
  id: totrans-859
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 深度度量学习
- en: 'We saw in Sections [3](#S3 "3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey") and [4](#S4 "4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")
    that both spectral and probabilistic metric learning methods use the generalized
    Mahalanobis distance, i.e. Eq. ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis
    distance). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), and learn the weight matrix in the metric. Deep metric learning,
    however, has a different approach. The methods in deep metric learning usually
    do not use a generalized Mahalanobis distance but they earn an embedding space
    using a neural network. The network learns a $p$-dimensional embedding space for
    discriminating classes or the dissimilar points and making the similar points
    close to each other. The network embeds data in the embedding space (or subspace)
    of metric. Then, any distance metric $d(.,.):\mathbb{R}^{p}\times\mathbb{R}^{p}\rightarrow\mathbb{R}$
    can be used in this embedding space. In the loss functions of network, we can
    use the distance function $d(.,.)$ in the embedding space. For example, an option
    for the distance function is the squared $\ell_{2}$ norm or squared Euclidean
    distance:'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第 [3](#S3 "3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey") 节和第 [4](#S4 "4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey") 节中看到，谱度量学习和概率度量学习方法都使用广义马氏距离，即公式
    ([5](#S2.E5 "In Definition 3 (Generalized Mahalanobis distance). ‣ 2.3 Generalized
    Mahalanobis Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))，并学习度量中的权重矩阵。然而，深度度量学习有不同的方法。深度度量学习中的方法通常不使用广义马氏距离，而是通过神经网络学习一个嵌入空间。网络学习一个
    $p$-维嵌入空间，用于区分类别或不相似的点，并使相似的点彼此接近。网络将数据嵌入到度量的嵌入空间（或子空间）中。然后，可以在这个嵌入空间中使用任何距离度量
    $d(.,.):\mathbb{R}^{p}\times\mathbb{R}^{p}\rightarrow\mathbb{R}$。在网络的损失函数中，我们可以使用嵌入空间中的距离函数
    $d(.,.)$。例如，距离函数的一个选项是平方的 $\ell_{2}$ 范数或平方欧几里得距离：'
- en: '|  | $\displaystyle d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}:=\&#124;\textbf{f}(\boldsymbol{x}_{i}^{1})-\textbf{f}(\boldsymbol{x}_{i}^{2})\&#124;_{2}^{2},$
    |  | (146) |'
  id: totrans-861
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}:=\&#124;\textbf{f}(\boldsymbol{x}_{i}^{1})-\textbf{f}(\boldsymbol{x}_{i}^{2})\&#124;_{2}^{2},$
    |  | (146) |'
- en: where $\textbf{f}(\boldsymbol{x}_{i})\in\mathbb{R}^{p}$ denotes the output of
    network for the input $\boldsymbol{x}_{i}$ as its $p$-dimensional embedding. We
    train the network using mini-batch methods such as the mini-batch stochastic gradient
    descent and denote the mini-batch size by $b$. The shared weights of sub-networks
    are denoted by the learnable parameter $\theta$.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{f}(\boldsymbol{x}_{i})\in\mathbb{R}^{p}$ 表示网络对输入 $\boldsymbol{x}_{i}$
    的输出，作为其 $p$-维嵌入。我们使用小批量方法如小批量随机梯度下降来训练网络，并将小批量大小表示为 $b$。子网络的共享权重用可学习参数 $\theta$
    表示。
- en: 5.1 Reconstruction Autoencoders
  id: totrans-863
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 重建自编码器
- en: 5.1.1 Types of Autoencoders
  id: totrans-864
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 自编码器类型
- en: An autoencoder is a model consisting of an encoder $E(.)$ and a decoder $D(.)$.
    There are several types of autoencoders. All types of autoencoders learn a code
    layer in the middle of encoder and decoder. Inferential autoencoders learn a stochastic
    latent space in the code layer between the encoder and decoder. Variational autoencoder
    (Ghojogh et al., [2021a](#bib.bib46)) and adversarial autoencoder (Ghojogh et al.,
    [2021b](#bib.bib47)) are two important types of inferential autoencoders. Another
    type of autoencoder is the reconstruction autoencoder consisting of an encoder,
    transforming data to a code, and a decoder, transforming the code back to the
    data. Hence, the decoder reconstructs the input data to the encoder. The code
    is a representation for data. Each of the encoder and decoder can be multiple
    layers of neural network with activation functions.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一个包含编码器 $E(.)$ 和解码器 $D(.)$ 的模型。有几种类型的自编码器。所有类型的自编码器都在编码器和解码器之间学习一个代码层。推断自编码器在编码器和解码器之间的代码层中学习一个随机潜在空间。变分自编码器（Ghojogh
    等，[2021a](#bib.bib46)）和对抗自编码器（Ghojogh 等，[2021b](#bib.bib47)）是两个重要的推断自编码器类型。另一种类型的自编码器是重建自编码器，包括一个编码器，将数据转换为代码，以及一个解码器，将代码转换回数据。因此，解码器将输入数据重建到编码器。代码是数据的表示。编码器和解码器可以是多个层的神经网络，具有激活函数。
- en: 5.1.2 Reconstruction Loss
  id: totrans-866
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 重建损失
- en: 'We denote the input data point to the encoder by $\boldsymbol{x}\in\mathbb{R}^{d}$
    where $d$ is the dimensionality of data. The reconstructed data point is the output
    of decoder and is denoted by $\widehat{\boldsymbol{x}}\in\mathbb{R}^{d}$. The
    representation code, which is the output of encoder and the input of decoder,
    is denoted by $\textbf{f}(\boldsymbol{x}):=E(\boldsymbol{x})\in\mathbb{R}^{p}$.
    We have $\widehat{\boldsymbol{x}}=D(E(\boldsymbol{x}))=D(\textbf{f}(\boldsymbol{x}))$.
    If the dimensionality of code is greater than the dimensionality of input data,
    i.e. $p>d$, the autoencoder is called an over-complete autoencoder (Goodfellow
    et al., [2016](#bib.bib55)). Otherwise, if $p<d$, the autoencoder is an under-complete
    autoencoder (Goodfellow et al., [2016](#bib.bib55)). The loss function of reconstruction
    autoencoder tries to make the reconstructed data close to the input data:'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 $\boldsymbol{x}\in\mathbb{R}^{d}$ 表示输入数据点，其中 $d$ 是数据的维度。重建的数据点是解码器的输出，表示为
    $\widehat{\boldsymbol{x}}\in\mathbb{R}^{d}$。表示代码是编码器的输出和解码器的输入，表示为 $\textbf{f}(\boldsymbol{x}):=E(\boldsymbol{x})\in\mathbb{R}^{p}$。我们有
    $\widehat{\boldsymbol{x}}=D(E(\boldsymbol{x}))=D(\textbf{f}(\boldsymbol{x}))$。如果代码的维度大于输入数据的维度，即
    $p>d$，则该自编码器称为完备自编码器（Goodfellow et al., [2016](#bib.bib55)）。否则，如果 $p<d$，则自编码器为欠完备自编码器（Goodfellow
    et al., [2016](#bib.bib55)）。重建自编码器的损失函数试图使重建的数据接近输入数据：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}\Big{(}d\big{(}\boldsymbol{x}_{i},\widehat{\boldsymbol{x}}_{i}\big{)}+\lambda\Omega(\theta)\Big{)},$
    |  | (147) |'
  id: totrans-868
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}\Big{(}d\big{(}\boldsymbol{x}_{i},\widehat{\boldsymbol{x}}_{i}\big{)}+\lambda\Omega(\theta)\Big{)},$
    |  | (147) |'
- en: where $\lambda\geq 0$ is the regularization parameter and $\Omega(\theta)$ is
    some penalty or regularization on the weights. Here, the distance function $d(.,.)$
    is defined on $\mathbb{R}^{d}\times\mathbb{R}^{d}$. Note that the penalty term
    can be regularization on the code $\textbf{f}(\boldsymbol{x}_{i})$. If the used
    distance metric is the squared Euclidean distance, this loss is named the regularized
    Mean Squared Error (MSE) loss.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda\geq 0$ 是正则化参数，$\Omega(\theta)$ 是对权重的某种惩罚或正则化。这里，距离函数 $d(.,.)$ 定义在
    $\mathbb{R}^{d}\times\mathbb{R}^{d}$ 上。注意，惩罚项可以是对代码 $\textbf{f}(\boldsymbol{x}_{i})$
    的正则化。如果使用的距离度量是平方欧几里得距离，则此损失称为正则化均方误差（MSE）损失。
- en: 5.1.3 Denoising Autoencoder
  id: totrans-870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 去噪自编码器
- en: 'A problem with over-complete autoencoder is that its training only copies each
    feature of data input to one of the neurons in the code layer and then copies
    it back to the corresponding feature of output layer. This is because the number
    of neurons in the code layer is greater than the number of neurons in the input
    and output layers. In other words, the networks just memorizes or gets overfit.
    This coping happens by making some of the weights equal to one (or a scale of
    one depending on the activation functions) and the rest of weights equal to zero.
    To avoid this problem in over-complete autoencoders, one can add some noise to
    the input data and try to reconstruct the data without noise. For this, Eq. ([147](#S5.E147
    "In 5.1.2 Reconstruction Loss ‣ 5.1 Reconstruction Autoencoders ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    is used while the input to the network is the mini-batch plus some noise. This
    forces the over-complete autoencoder to not just copy data to the code layer.
    This autoencoder can be used for denoising as it reconstructs the data without
    noise for a noisy input. This network is called the Denoising Autoencoder (DAE)
    (Goodfellow et al., [2016](#bib.bib55)).'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 完备自编码器的一个问题是其训练仅将数据输入的每个特征复制到代码层的一个神经元中，然后再复制回输出层的对应特征。这是因为代码层中的神经元数量大于输入层和输出层中的神经元数量。换句话说，网络只是记忆或过拟合。这种复制发生是通过使一些权重等于一（或根据激活函数的不同而等于一的尺度），其余权重等于零。为了避免在完备自编码器中出现此问题，可以在输入数据中添加一些噪声，并尝试重建无噪声的数据。为此，使用公式
    ([147](#S5.E147 "在5.1.2 重建损失 ‣ 5.1 重建自编码器 ‣ 5 深度度量学习 ‣ 谱的、概率的和深度的度量学习：教程和调查"))，同时网络的输入是迷你批次加上一些噪声。这迫使完备自编码器不仅仅将数据复制到代码层。该自编码器可以用于去噪，因为它在有噪声的输入下重建无噪声的数据。这个网络被称为去噪自编码器（DAE）（Goodfellow
    et al., [2016](#bib.bib55)）。
- en: 5.1.4 Metric Learning by Reconstruction Autoencoder
  id: totrans-872
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 通过重建自编码器的度量学习
- en: 'The under-complete reconstruction autoencoder can be used for metric learning
    and dimensionality reduction, especially when $p\ll d$. The loss function for
    learning a low-dimensional representation code and reconstructing data by the
    autoencoder is Eq. ([147](#S5.E147 "In 5.1.2 Reconstruction Loss ‣ 5.1 Reconstruction
    Autoencoders ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")). The code layer between the encoder and decoder
    is the embedding space of metric.'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: '欠完备重建自编码器可用于度量学习和降维，特别是当$p\ll d$时。用于学习低维表示代码和通过自编码器重建数据的损失函数为公式 ([147](#S5.E147
    "In 5.1.2 Reconstruction Loss ‣ 5.1 Reconstruction Autoencoders ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))。编码器和解码器之间的代码层是度量的嵌入空间。'
- en: 'Note that if the activation functions of all layers are linear, the under-complete
    autoencoder is reduced to Principal Component Analysis (Ghojogh & Crowley, [2019](#bib.bib37)).
    Let $\boldsymbol{U}_{l}$ denote the weight matrix of the $l$-th layer of network,
    $\ell_{e}$ be the number of layers of encoder, and $\ell_{d}$ be the number of
    layers of decoder. With linear activation function, the encoder and decoder are:'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果所有层的激活函数都是线性的，则欠完备自编码器会简化为主成分分析（Ghojogh & Crowley，[2019](#bib.bib37)）。设$\boldsymbol{U}_{l}$表示第$l$层网络的权重矩阵，$\ell_{e}$是编码器的层数，$\ell_{d}$是解码器的层数。在使用线性激活函数的情况下，编码器和解码器为：
- en: '|  | $\displaystyle\text{encoder: }\quad\mathbb{R}^{p}\ni\textbf{f}(\boldsymbol{x}_{i})=\underbrace{\boldsymbol{U}_{\ell_{e}}^{\top}\boldsymbol{U}_{\ell_{e}-1}^{\top}\dots\boldsymbol{U}_{1}^{\top}}_{\boldsymbol{U}_{e}^{\top}}\boldsymbol{x}_{i},$
    |  |'
  id: totrans-875
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{编码器: }\quad\mathbb{R}^{p}\ni\textbf{f}(\boldsymbol{x}_{i})=\underbrace{\boldsymbol{U}_{\ell_{e}}^{\top}\boldsymbol{U}_{\ell_{e}-1}^{\top}\dots\boldsymbol{U}_{1}^{\top}}_{\boldsymbol{U}_{e}^{\top}}\boldsymbol{x}_{i},$
    |  |'
- en: '|  | $\displaystyle\text{decoder: }\quad\mathbb{R}^{d}\ni\widehat{\boldsymbol{x}}_{i}=\underbrace{\boldsymbol{U}_{1}\dots\boldsymbol{U}_{\ell_{d}-1}\boldsymbol{U}_{\ell_{d}}}_{\boldsymbol{U}_{d}}\textbf{f}(\boldsymbol{x}_{i}),$
    |  |'
  id: totrans-876
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{解码器: }\quad\mathbb{R}^{d}\ni\widehat{\boldsymbol{x}}_{i}=\underbrace{\boldsymbol{U}_{1}\dots\boldsymbol{U}_{\ell_{d}-1}\boldsymbol{U}_{\ell_{d}}}_{\boldsymbol{U}_{d}}\textbf{f}(\boldsymbol{x}_{i}),$
    |  |'
- en: where linear projection by $\ell$ projection matrices can be replaced by linear
    projection with one projection matrices $\boldsymbol{U}_{e}$ and $\boldsymbol{U}_{d}$.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 其中通过$\ell$个投影矩阵的线性投影可以被一个投影矩阵$\boldsymbol{U}_{e}$和$\boldsymbol{U}_{d}$的线性投影所替代。
- en: 'For learning complicated data patterns, we can use nonlinear activation functions
    between layers of the encoder and decoder to have nonlinear metric learning and
    dimensionality reduction. It is noteworthy that nonlinear neural network can be
    seen as an ensemble or concatenation of dimensionality reduction (or feature extraction)
    and kernel methods. The justification of this claim is as follows. Let the dimensionality
    for a layer of network be $\boldsymbol{U}\in\mathbb{R}^{d_{1}\times d_{2}}$ so
    it connects $d_{1}$ neurons to $d_{2}$ neurons. Two cases can happen:'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习复杂的数据模式，我们可以在编码器和解码器的层之间使用非线性激活函数，以实现非线性度量学习和降维。值得注意的是，非线性神经网络可以被视为降维（或特征提取）和核方法的集成或串联。这个说法的理由如下。设网络的一层的维度为$\boldsymbol{U}\in\mathbb{R}^{d_{1}\times
    d_{2}}$，它将$d_{1}$个神经元连接到$d_{2}$个神经元。可能会有两种情况：
- en: •
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If $d_{1}\geq d_{2}$, this layer acts as dimensionality reduction or feature
    extraction because it has reduced the dimensionality of its input data. If this
    layer has a nonlinear activation function, the dimensionality reduction is nonlinear;
    otherwise, it is linear.
  id: totrans-880
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果$d_{1}\geq d_{2}$，则该层作为降维或特征提取，因为它降低了输入数据的维度。如果该层有一个非线性激活函数，则降维是非线性的；否则，降维是线性的。
- en: •
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If $d_{1}<d_{2}$, this layer acts as a kernel method which maps its input data
    to the high-dimensional feature space in some Reproducing Kernel Hilbert Space
    (RKHS). This kernelization can help nonlinear separation of some classes which
    are not separable linearly (Ghojogh et al., [2021e](#bib.bib50)). An example use
    of kernelization in machine learning is kernel support vector machine (Vapnik,
    [1995](#bib.bib114)).
  id: totrans-882
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果$d_{1}<d_{2}$，则该层作为核方法，将其输入数据映射到某个再生核希尔伯特空间（RKHS）的高维特征空间。这种核化可以帮助非线性地分离一些不能线性分开的类别（Ghojogh
    et al.，[2021e](#bib.bib50)）。在机器学习中使用核化的一个例子是核支持向量机（Vapnik，[1995](#bib.bib114)）。
- en: Therefore, a neural network is a complicated feature extraction method as a
    concatenation of dimensionality reduction and kernel methods. Each layer of network
    learns its own features from data.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络是一种复杂的特征提取方法，结合了降维和核方法。网络的每一层从数据中学习自身的特征。
- en: 5.2 Supervised Metric Learning by Supervised Loss Functions
  id: totrans-884
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 通过监督损失函数进行监督度量学习
- en: 'Various loss functions exist for supervised metric learning by neural networks.
    Supervised loss functions can teach the network to separate classes in the embedding
    space (Sikaroudi et al., [2020b](#bib.bib103)). For this, we use a network whose
    last layer is for classification of data points. The features of the one-to-last
    layer can be used for feature embedding. The last layer after the embedding features
    is named the classification layer. The structure of this network is shown in Fig.
    [3](#S5.F3 "Figure 3 ‣ 5.2 Supervised Metric Learning by Supervised Loss Functions
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"). Let the $i$-th point in the mini-batch be denoted by $\boldsymbol{x}_{i}\in\mathbb{R}^{d}$
    and its label be denoted by $y_{i}\in\mathbb{R}$. Suppose the network has one
    output neuron and its output for the input $\boldsymbol{x}_{i}$ is denoted by
    $\textbf{f}_{o}(\boldsymbol{x}_{i})\in\mathbb{R}$. This output is the estimated
    class label by the network. We denote output of the the one-to-last layer by $\textbf{f}(\boldsymbol{x}_{i})\in\mathbb{R}^{p}$
    where $p$ is the number of neurons in that layer which is equivalent to the dimensionality
    of the embedding space. The last layer of network, connecting the $p$ neurons
    to the output neuron is a fully-connected layer. The network until the one-to-last
    layer can be any feed-forward or convolutional network depending on the type of
    data. If the network is convolutional, it should be flattened at the one-to-last
    layer. The network learns to classify the classes, by the supervised loss functions,
    so the features of the one-to-last layers will be discriminating features and
    suitable for embedding.'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通过神经网络进行的监督度量学习，存在各种损失函数。监督损失函数可以教会网络在嵌入空间中区分类别（Sikaroudi 等，[2020b](#bib.bib103)）。为此，我们使用一个其最后一层用于数据点分类的网络。倒数第二层的特征可以用于特征嵌入。嵌入特征后的最后一层被称为分类层。该网络的结构如图
    [3](#S5.F3 "图 3 ‣ 5.2 通过监督损失函数进行监督度量学习 ‣ 5 深度度量学习 ‣ 谱的、概率的和深度的度量学习：教程与调查") 所示。设小批量中的第
    $i$ 个点记为 $\boldsymbol{x}_{i}\in\mathbb{R}^{d}$，其标签记为 $y_{i}\in\mathbb{R}$。假设网络有一个输出神经元，输入
    $\boldsymbol{x}_{i}$ 的输出记为 $\textbf{f}_{o}(\boldsymbol{x}_{i})\in\mathbb{R}$。该输出是网络估计的类别标签。我们将倒数第二层的输出记为
    $\textbf{f}(\boldsymbol{x}_{i})\in\mathbb{R}^{p}$，其中 $p$ 是该层神经元的数量，相当于嵌入空间的维度。网络的最后一层将
    $p$ 个神经元连接到输出神经元，这是一个全连接层。直到倒数第二层的网络可以是任何前馈或卷积网络，具体取决于数据类型。如果网络是卷积的，它应在倒数第二层进行展平。网络通过监督损失函数学习分类，因此倒数第二层的特征将是具有区分性的特征，并适合用于嵌入。
- en: '![Refer to caption](img/ea9454e1a404f5c59dd66e70a29e326d.png)'
  id: totrans-886
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ea9454e1a404f5c59dd66e70a29e326d.png)'
- en: 'Figure 3: The structure of network for metric learning with supervised loss
    function.'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：带有监督损失函数的度量学习网络结构。
- en: 5.2.1 Mean Squared Error and Mean Absolute Value Losses
  id: totrans-888
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 均方误差和均绝对值损失
- en: 'One of the supervised losses is the Mean Squared Error (MSE) which makes the
    estimated labels close to the true labels using squared $\ell_{2}$ norm:'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 一种监督损失是均方误差（MSE），它使用平方的 $\ell_{2}$ 范数使估计标签接近真实标签：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}(\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i})^{2}.$
    |  | (148) |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}(\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i})^{2}.$
    |  | (148) |'
- en: 'One problem with this loss function is exaggerating outliers because of the
    square but its advantage is its differentiability. Another loss function is the
    Mean Absolute Error (MAE) which makes the estimated labels close to the true labels
    using $\ell_{1}$ norm or the absolute value:'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数的一个问题是由于平方的存在会夸大异常值，但它的优点是可导性。另一种损失函数是均绝对误差（MAE），它使用 $\ell_{1}$ 范数或绝对值，使得估计标签接近真实标签：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;.$
    |  | (149) |'
  id: totrans-892
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;.$
    |  | (149) |'
- en: The distance used in this loss is also named the Manhattan distance. This loss
    function does not have the problem of MSE and it can be used for imposing sparsity
    in the embedding. It is not differentiable at the point $\textbf{f}(\boldsymbol{x}_{i})=y_{i}$
    but as the derivatives are calculated numerically by the neural network, this
    is not a big issue nowadays.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失使用的距离也称为曼哈顿距离。此损失函数没有MSE的问题，可用于在嵌入中施加稀疏性。它在点$\textbf{f}(\boldsymbol{x}_{i})=y_{i}$处不可微分，但由于导数是由神经网络数值计算的，这在如今不再是大问题。
- en: 5.2.2 Huber and KL-Divergence Losss
  id: totrans-894
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 Huber和KL散度损失
- en: 'Another loss function is the Huber loss which is a combination of the MSE and
    MAE to have the advantages of both of them:'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种损失函数是Huber损失，它是MSE和MAE的组合，结合了两者的优点：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}$ |  | (150)
    |'
  id: totrans-896
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最小化}}~{}~{}~{}$ |  | (150) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}\sum_{i=1}^{b}\left\{\begin{array}[]{ll}0.5(\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i})^{2}&amp;\mbox{if
    }&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;\leq\delta\\ \delta(&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;-0.5\delta)&amp;\mbox{otherwise}.\end{array}\right.$
    |  |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}\sum_{i=1}^{b}\left\{\begin{array}[]{ll}0.5(\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i})^{2}&amp;\mbox{如果
    }&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;\leq\delta\\ \delta(&#124;\textbf{f}_{o}(\boldsymbol{x}_{i})-y_{i}&#124;-0.5\delta)&amp;\mbox{否则}.\end{array}\right.$
    |  |'
- en: 'KL-divergence loss function makes the distribution of the estimated labels
    close to the distribution of the true labels:'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度损失函数使估计标签的分布接近真实标签的分布：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\text{KL}(\mathbb{P}(\textbf{f}(\boldsymbol{x}))\&#124;\mathbb{P}(y))=\sum_{i=1}^{b}\textbf{f}(\boldsymbol{x}_{i})\log(\frac{\textbf{f}(\boldsymbol{x}_{i})}{y_{i}}).$
    |  | (151) |'
  id: totrans-899
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{最小化}}~{}~{}~{}\text{KL}(\mathbb{P}(\textbf{f}(\boldsymbol{x}))\&#124;\mathbb{P}(y))=\sum_{i=1}^{b}\textbf{f}(\boldsymbol{x}_{i})\log(\frac{\textbf{f}(\boldsymbol{x}_{i})}{y_{i}}).$
    |  | (151) |'
- en: 5.2.3 Hinge Loss
  id: totrans-900
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 铰链损失
- en: 'If there are two classes, i.e. $c=2$, we can have true labels as $y_{i}\in\{-1,1\}$.
    In this case, a possible loss function is the Hinge loss:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有两个类别，即$c=2$，我们可以将真实标签设为$y_{i}\in\{-1,1\}$。在这种情况下，一种可能的损失函数是铰链损失：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}\big{[}m-y_{i}\,\textbf{f}_{o}(\boldsymbol{x}_{i})\big{]}_{+},$
    |  | (152) |'
  id: totrans-902
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{最小化}}~{}~{}~{}\sum_{i=1}^{b}\big{[}m-y_{i}\,\textbf{f}_{o}(\boldsymbol{x}_{i})\big{]}_{+},$
    |  | (152) |'
- en: where $[\cdot]_{+}:=\max(\cdot,0)$ and $m>0$ is the margin. If the signs of
    the estimated and true labels are different, the loss is positive which should
    be minimized. If the signs are the same and $|\textbf{f}_{o}(\boldsymbol{x}_{i})|\geq
    m$, then the loss function is zero. If the signs are the same but $|\textbf{f}_{o}(\boldsymbol{x}_{i})|<m$,
    the loss is positive and should be minimized because the estimation is correct
    but not with enough margin from the incorrect estimation.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$[\cdot]_{+}:=\max(\cdot,0)$，$m>0$是边际。如果估计标签和真实标签的符号不同，损失是正的，需要最小化。如果符号相同且$|\textbf{f}_{o}(\boldsymbol{x}_{i})|\geq
    m$，则损失函数为零。如果符号相同但$|\textbf{f}_{o}(\boldsymbol{x}_{i})|<m$，损失是正的，需要最小化，因为估计是正确的，但与错误估计的边际不够。
- en: 5.2.4 Cross-entropy Loss
  id: totrans-904
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 交叉熵损失
- en: 'For any number of classes, denoted by $c$, we can have a cross-entropy loss.
    For this loss, we have $c$ neurons, rather than one neuron, at the last layer.
    In contrast to the MSE, MAE, Huber, and KL-divergence losses which use linear
    activation function at the last layer, cross-entropy requires softmax or sigmoid
    activation function at the last layer so the output values are between zero and
    one. For this loss, we have $c$ outputs, i.e. $\textbf{f}_{o}(\boldsymbol{x}_{i})\in\mathbb{R}^{c}$
    (continuous values between zero and one), and the true labels are one-hot encoded,
    i.e., $\boldsymbol{y}_{i}\in\{0,1\}^{c}$. This loss is defined as:'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何类别数，记作$c$，我们可以使用交叉熵损失。对于这种损失，我们在最后一层有$c$个神经元，而不是一个神经元。与使用线性激活函数的MSE、MAE、Huber和KL散度损失不同，交叉熵需要在最后一层使用softmax或sigmoid激活函数，以便输出值介于零和一之间。对于这种损失，我们有$c$个输出，即$\textbf{f}_{o}(\boldsymbol{x}_{i})\in\mathbb{R}^{c}$（介于零和一之间的连续值），真实标签为one-hot编码，即$\boldsymbol{y}_{i}\in\{0,1\}^{c}$。该损失定义为：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}-\sum_{i=1}^{b}\sum_{l=1}^{c}(\boldsymbol{y}_{i})_{l}\log\big{(}\textbf{f}_{o}(\boldsymbol{x}_{i})_{l}\big{)},$
    |  | (153) |'
  id: totrans-906
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{最小化}}~{}~{}~{}-\sum_{i=1}^{b}\sum_{l=1}^{c}(\boldsymbol{y}_{i})_{l}\log\big{(}\textbf{f}_{o}(\boldsymbol{x}_{i})_{l}\big{)},$
    |  | (153) |'
- en: where $(\boldsymbol{y}_{i})_{l}$ and $\textbf{f}_{o}(\boldsymbol{x}_{i})_{l}$
    denote the $l$-th element of $\boldsymbol{y}_{i}$ and $\textbf{f}_{o}(\boldsymbol{x}_{i})$,
    respectively. Minimizing this loss separates classes for classification; this
    separation of classes also gives us discriminating embedding in the one-to-last
    layer (Sikaroudi et al., [2020b](#bib.bib103); Boudiaf et al., [2020](#bib.bib14)).
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\boldsymbol{y}_{i})_{l}$ 和 $\textbf{f}_{o}(\boldsymbol{x}_{i})_{l}$ 分别表示
    $\boldsymbol{y}_{i}$ 和 $\textbf{f}_{o}(\boldsymbol{x}_{i})$ 的第 $l$ 个元素。最小化此损失可以区分类别用于分类；这种类别的分离也为我们提供了一个到倒数第二层的区分嵌入
    (Sikaroudi et al., [2020b](#bib.bib103); Boudiaf et al., [2020](#bib.bib14))。
- en: 'The reason for why cross-entropy can be suitable for metric learning is theoretically
    justified in (Boudiaf et al., [2020](#bib.bib14)), explained in the following.
    Consider the mutual information between the true labels $Y$ and the estimated
    labels $\textbf{f}_{o}(X)$:'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么交叉熵适合度量学习在理论上已在 (Boudiaf et al., [2020](#bib.bib14)) 中得到解释。考虑真实标签 $Y$ 和估计标签
    $\textbf{f}_{o}(X)$ 之间的互信息：
- en: '|  | $\displaystyle I(\textbf{f}_{o}(X);Y)$ | $\displaystyle=H(\textbf{f}_{o}(X))-H(\textbf{f}_{o}(X)&#124;Y)$
    |  | (154) |'
  id: totrans-909
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I(\textbf{f}_{o}(X);Y)$ | $\displaystyle=H(\textbf{f}_{o}(X))-H(\textbf{f}_{o}(X)&#124;Y)$
    |  | (154) |'
- en: '|  |  | $\displaystyle=H(Y)-H(Y&#124;\textbf{f}_{o}(X)),$ |  | (155) |'
  id: totrans-910
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=H(Y)-H(Y&#124;\textbf{f}_{o}(X)),$ |  | (155) |'
- en: 'where $H(.)$ denotes entropy. On the one hand, Eq. ([154](#S5.E154 "In 5.2.4
    Cross-entropy Loss ‣ 5.2 Supervised Metric Learning by Supervised Loss Functions
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) has a generative view which exists in the metric learning
    loss functions generating embedding features. Eq. ([155](#S5.E155 "In 5.2.4 Cross-entropy
    Loss ‣ 5.2 Supervised Metric Learning by Supervised Loss Functions ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")),
    one the other hand, has a discriminative view used in the cross-entropy loss function.
    Therefore, the metric learning losses and the cross-entropy loss are related.
    It is shown in (Boudiaf et al., [2020](#bib.bib14), Proposition 1) that the cross-entropy
    is an upper-bound on the metric learning losses so its minimization for classification
    also provides embedding features.'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H(.)$ 表示熵。一方面，方程 ([154](#S5.E154 "在 5.2.4 交叉熵损失 ‣ 5.2 通过监督损失函数的监督度量学习 ‣
    5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与综述")) 具有生成视角，该视角存在于生成嵌入特征的度量学习损失函数中。另一方面，方程 ([155](#S5.E155
    "在 5.2.4 交叉熵损失 ‣ 5.2 通过监督损失函数的监督度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与综述")) 具有在交叉熵损失函数中使用的判别视角。因此，度量学习损失和交叉熵损失是相关的。
    (Boudiaf et al., [2020](#bib.bib14), 命题 1) 表明，交叉熵是度量学习损失的上界，因此其最小化用于分类也提供嵌入特征。
- en: It is noteworthy that another supervised loss function is triplet loss, introduced
    in the next section. Triplet loss can be used for both hard labels (for classification)
    and soft labels (for similarity and dissimilarity of points). The triplet loss
    also does not need a last classification layer; therefore, the embedding layer
    can be the last layer for this loss.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，另一种监督损失函数是三元组损失，将在下一节介绍。三元组损失既可用于硬标签（用于分类），也可用于软标签（用于点的相似性和不相似性）。三元组损失也不需要最后的分类层；因此，嵌入层可以作为此损失的最后一层。
- en: 5.3 Metric Learning by Siamese Networks
  id: totrans-913
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 基于 Siamese 网络的度量学习
- en: '![Refer to caption](img/7eb80bae9536c4a86a7cca93f33527db.png)'
  id: totrans-914
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7eb80bae9536c4a86a7cca93f33527db.png)'
- en: 'Figure 4: The structure of Siamese network with (a) two and (b) three sub-networks.'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：带有 (a) 两个和 (b) 三个子网络的 Siamese 网络结构。
- en: 5.3.1 Siamese and Triplet Networks
  id: totrans-916
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 Siamese 和三元组网络
- en: One of the important deep metric learning methods is Siamese network which is
    widely used for feature extraction. Siamese network, originally proposed in (Bromley
    et al., [1993](#bib.bib15)), is a network consisting of several equivalent sub-networks
    sharing their weights. The number of sub-networks in a Siamese network can be
    any number but it usually is two or three. A Siamese network with three sub-networks
    is also called a triplet network (Hoffer & Ailon, [2015](#bib.bib68)). The weights
    of sub-networks in a Siamese network are trained in a way that the intra- and
    inter-class variances are decreased and increased, respectively. In other words,
    the similar points are pushed toward each other while the dissimilar points are
    pulled away from one another. Siamese networks have been used in various applications
    such as computer vision (Schroff et al., [2015](#bib.bib100)) and natural language
    processing (Yang et al., [2020](#bib.bib135)).
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个重要的深度度量学习方法是Siamese网络，它被广泛用于特征提取。Siamese网络最初在(Bromley et al., [1993](#bib.bib15))中提出，是一个由多个共享权重的等效子网络组成的网络。Siamese网络中的子网络数量可以是任意数量，但通常是两个或三个。具有三个子网络的Siamese网络也称为三元组网络
    (Hoffer & Ailon, [2015](#bib.bib68))。Siamese网络中子网络的权重以减少类内方差和增加类间方差的方式进行训练。换句话说，相似的点被推向彼此，而不相似的点则被拉开。Siamese网络已被应用于计算机视觉
    (Schroff et al., [2015](#bib.bib100)) 和自然语言处理 (Yang et al., [2020](#bib.bib135))
    等各种应用中。
- en: 5.3.2 Pairs and Triplets of Data Points
  id: totrans-918
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 数据点的对和三元组
- en: Depending on the number of sub-networks in the Siamese network, we have loss
    functions for training. The loss functions of Siamese networks usually require
    pairs or triplets of data points. Siamese networks do not use the data points
    one by one but we need to make pairs or triplets of points out of dataset for
    training a Siamese network. For making the pairs or triplets, we consider every
    data point as the anchor point, denoted by $\boldsymbol{x}_{i}^{a}$. Then, we
    take one of the similar points to the anchor point as the positive (or neighbor)
    point, denoted by $\boldsymbol{x}_{i}^{p}$. We also take one of the dissimilar
    points to the anchor point as the negative (or distant) point, denoted by $\boldsymbol{x}_{i}^{n}$.
    If class labels are available, we can use them to find the positive point as one
    of the points in the same class as the anchor point, and to find the the negative
    point as one of the points in a different class from the anchor point’s class.
    Another approach is to augment the anchor point, using one of the augmentation
    methods, to obtain a positive points for the anchor point (Khodadadeh et al.,
    [2019](#bib.bib73); Chen et al., [2020](#bib.bib19)).
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Siamese网络中的子网络数量，我们有不同的损失函数用于训练。Siamese网络的损失函数通常需要数据点的对或三元组。Siamese网络不是逐一使用数据点，而是需要从数据集中构造对或三元组用于训练。构造对或三元组时，我们将每个数据点视为锚点，记作
    $\boldsymbol{x}_{i}^{a}$。然后，我们选择一个与锚点相似的点作为正点（或邻近点），记作 $\boldsymbol{x}_{i}^{p}$。我们还选择一个与锚点不相似的点作为负点（或远离点），记作
    $\boldsymbol{x}_{i}^{n}$。如果有类别标签，我们可以使用它们找到一个与锚点在同一类别中的点作为正点，并找到一个与锚点类别不同的点作为负点。另一种方法是通过使用一种数据增强方法来增强锚点，从而获得一个正点（Khodadadeh
    et al., [2019](#bib.bib73); Chen et al., [2020](#bib.bib19)）。
- en: For Siamese networks with two sub-networks, we make pairs of anchor-positive
    points $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p})\}_{i=1}^{n_{t}}$ and
    anchor-negative points $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{n})\}_{i=1}^{n_{t}}$,
    where $n_{t}$ is the number of pairs. For Siamese networks with three sub-networks,
    we make triplets of anchor-positive-negative points $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{n_{t}}$,
    where $n_{t}$ is the number of triplets. If we consider every point of dataset
    as an anchor, the number of pairs/triplets is the same as the number of data points,
    i.e., $n_{t}=n$.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有两个子网络的Siamese网络，我们构造锚点-正点对 $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p})\}_{i=1}^{n_{t}}$
    和锚点-负点对 $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{n})\}_{i=1}^{n_{t}}$，其中
    $n_{t}$ 是对的数量。对于具有三个子网络的Siamese网络，我们构造锚点-正点-负点三元组 $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{n_{t}}$，其中
    $n_{t}$ 是三元组的数量。如果我们将数据集中的每个点视为锚点，则对/三元组的数量与数据点的数量相同，即 $n_{t}=n$。
- en: Various loss functions of Siamese networks use pairs or triplets of data points
    to push the positive point towards the anchor point and pull the negative point
    away from it. Doing this iteratively for all pairs or triplets will make the intra-class
    variances smaller and the inter-class variances larger for better discrimination
    of classes or clusters. Later in the following, we introduce some of the loss
    functions for training a Siamese network.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese网络的各种损失函数使用数据点的对或三元组，将正样本推向锚点，并将负样本远离锚点。对所有对或三元组进行迭代操作，可以使类内方差变小，类间方差变大，以实现更好的类别或聚类区分。在接下来的内容中，我们介绍一些用于训练Siamese网络的损失函数。
- en: 5.3.3 Implementation of Siamese Networks
  id: totrans-922
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 Siamese网络的实现
- en: 'A Siamese network with two and three sub-networks is depicted in Fig. [4](#S5.F4
    "Figure 4 ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"). We denote
    the output of Siamese network for input $\boldsymbol{x}\in\mathbb{R}^{d}$ by $\textbf{f}(\boldsymbol{x})\in\mathbb{R}^{p}$
    where $p$ is the dimensionality of embedding (or the number of neurons at the
    last layer of the network) which is usually much less than the dimensionality
    of data, i.e., $p\ll d$. Note that the sub-networks of a Siamese network can be
    any fully-connected or convolutional network depending on the type of data. The
    used network structure for the sub-networks is usually called the backbone network.'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#S5.F4 "Figure 4 ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")中描绘了具有两个和三个子网络的Siamese网络。我们用$\boldsymbol{x}\in\mathbb{R}^{d}$表示输入$\boldsymbol{x}$的Siamese网络的输出，记为$\textbf{f}(\boldsymbol{x})\in\mathbb{R}^{p}$，其中$p$是嵌入的维度（或网络的最后一层的神经元数量），通常要远小于数据的维度，即$p\ll
    d$。注意，Siamese网络的子网络可以是任何全连接或卷积网络，取决于数据的类型。通常称为骨干网的子网络所使用的网络结构。'
- en: '![Refer to caption](img/42359ac264e77a7e100ae4ec5ffd0ced.png)'
  id: totrans-924
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/42359ac264e77a7e100ae4ec5ffd0ced.png)'
- en: 'Figure 5: Visualization of what contrastive and triplet losses do: (a) a triplet
    of anchor (green circle), positive (blue circle), and negative (red diamond) points,
    (b) the effect of contrastive loss making a margin between the anchor and negative
    point, and (c) the effect of triplet loss making a margin between the positive
    and negative points.'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：对比损失和三元组损失的可视化：（a）锚点（绿色圆圈）、正样本（蓝色圆圈）和负样本（红色菱形）的三元组，（b）对比损失的效果，形成了锚点和负样本之间的边界，（c）三元组损失的效果，形成了正样本和负样本之间的边界。
- en: 'The weights of sub-networks are shared in the sense that the values of their
    weights are equal. Implementation of a Siamese network can be done in two ways:'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 子网络的权重是共享的，即它们的权重值是相等的。实现Siamese网络有两种方式：
- en: '1.'
  id: totrans-927
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We can implement several sub-networks in the memory. In the training phase,
    we feed every data point in the pairs or triplets to one of the sub-networks and
    take the outputs of sub-networks to have $\textbf{f}(\boldsymbol{x}_{i}^{a})$,
    $\textbf{f}(\boldsymbol{x}_{i}^{p})$, and $\textbf{f}(\boldsymbol{x}_{i}^{n})$.
    We use these in the loss function and update the weights of only one of the sub-networks
    by backpropagation (Ghojogh et al., [2021c](#bib.bib48)). Then, we copy the updated
    weights to the other sub-networks. We repeat this for all mini-batches and epochs
    until convergence. In the test phase, we feed the test point $\boldsymbol{x}$
    to only one of the sub-networks and get the output $\textbf{f}(\boldsymbol{x})$
    as its embedding.
  id: totrans-928
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在内存中，我们可以实现多个子网络。在训练阶段，我们将数据对或三元组中的每个数据点输入到一个子网络中，并取子网络的输出为$\textbf{f}(\boldsymbol{x}_{i}^{a})$、$\textbf{f}(\boldsymbol{x}_{i}^{p})$和$\textbf{f}(\boldsymbol{x}_{i}^{n})$。我们在损失函数中使用它们，并通过反向传播仅更新一个子网络的权重（Ghojogh等，[2021c](#bib.bib48)）。然后，我们将更新后的权重复制到其他子网络中。我们重复这个过程，直到收敛，对所有的小批量和迭代次数进行操作。在测试阶段，我们将测试点$\boldsymbol{x}$只输入到一个子网络中，并将输出$\textbf{f}(\boldsymbol{x})$作为其嵌入。
- en: '2.'
  id: totrans-929
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We can implement only one sub-network in the memory. In the training phase,
    we feed the data points in the pairs or triplets to the sub-network ont by one
    and take the outputs of sub-network to have $\textbf{f}(\boldsymbol{x}_{i}^{a})$,
    $\textbf{f}(\boldsymbol{x}_{i}^{p})$, and $\textbf{f}(\boldsymbol{x}_{i}^{n})$.
    We use these in the loss function and update the weights of the sub-network by
    backpropagation (Ghojogh et al., [2021c](#bib.bib48)). We repeat this for all
    mini-batches and epochs until convergence. In the test phase, we feed the test
    point $\boldsymbol{x}$ to the sub-network and get the output $\textbf{f}(\boldsymbol{x})$
    as its embedding.
  id: totrans-930
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们只能在内存中实现一个子网络。在训练阶段，我们将点对或三元组的数据逐个送入子网络，并获取子网络的输出，以得到$\textbf{f}(\boldsymbol{x}_{i}^{a})$、$\textbf{f}(\boldsymbol{x}_{i}^{p})$和$\textbf{f}(\boldsymbol{x}_{i}^{n})$。我们将这些用于损失函数，并通过反向传播
    (Ghojogh et al., [2021c](#bib.bib48)) 更新子网络的权重。我们对所有小批量和轮次重复这一过程，直到收敛。在测试阶段，我们将测试点$\boldsymbol{x}$送入子网络，得到输出$\textbf{f}(\boldsymbol{x})$作为其嵌入。
- en: The advantage of the first approach is to have all the sub-networks ready and
    we do not need to feed the points of pairs or triplets one by one. Its disadvantage
    is using more memory. As the number of points in the pairs or triplets is small
    (i.e., only two or three), the second approach is more recommended as it is memory-efficient.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法的优点是所有子网络都准备好了，我们不需要逐个输入点对或三元组。缺点是需要更多的内存。由于点对或三元组的数量较少（即仅为两到三个），第二种方法更为推荐，因为它在内存使用上更高效。
- en: 5.3.4 Contrastive Loss
  id: totrans-932
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4 对比损失
- en: One loss function for Siamese networks is the contrastive loss which uses the
    anchor-positive and anchor-negative pairs of points. Suppose, in each mini-batch,
    we have $b$ pairs of points $\{(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\}_{i=1}^{b}$
    some of which are anchor-positive and some are anchor-negative pairs. The points
    in an anchor-positive pair are similar, i.e. $(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{S}$,
    and the points in an anchor-negative pair are dissimilar, i.e. $(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{D}$,
    where $\mathcal{S}$ and $\mathcal{D}$ denote the similar and dissimilar sets.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 对于孪生网络，一种损失函数是对比损失，它使用锚点-正样本和锚点-负样本的点对。假设在每个小批量中，我们有$b$对点$\{(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\}_{i=1}^{b}$，其中一些是锚点-正样本对，另一些是锚点-负样本对。锚点-正样本对中的点是相似的，即$(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{S}$，锚点-负样本对中的点是不相似的，即$(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{D}$，其中$\mathcal{S}$和$\mathcal{D}$表示相似集和不相似集。
- en: '– Contrastive Loss: We define:'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: – 对比损失：我们定义为：
- en: '|  | $\displaystyle y_{i}:=\left\{\begin{array}[]{ll}0&amp;\mbox{if }(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{S}\\
    1&amp;\mbox{if }(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{D}.\end{array}\right.\quad\forall
    i\in\{1,\dots,n_{t}\}.$ |  | (158) |'
  id: totrans-935
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y_{i}:=\left\{\begin{array}[]{ll}0&amp;\mbox{如果 }(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{S}\\
    1&amp;\mbox{如果 }(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\in\mathcal{D}.\end{array}\right.\quad\forall
    i\in\{1,\dots,n_{t}\}.$ |  | (158) |'
- en: 'The main contrastive loss was proposed in (Hadsell et al., [2006](#bib.bib58))
    and is:'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的对比损失函数是在 (Hadsell et al., [2006](#bib.bib58)) 中提出的，其公式为：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}$
    | $\displaystyle\Big{(}(1-y_{i})d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}$
    |  | (159) |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}$
    | $\displaystyle\Big{(}(1-y_{i})d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}$
    |  | (159) |'
- en: '|  |  | $\displaystyle+y_{i}\big{[}\!-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}+m\big{]}_{+}\Big{)},$
    |  |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+y_{i}\big{[}\!-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}+m\big{]}_{+}\Big{)},$
    |  |'
- en: 'where $m>0$ is the margin and $[.]_{+}:=\max(.,0)$ is the standard Hinge loss.
    The first term of loss minimizes the embedding distances of similar points and
    the second term maximizes the embedding distances of dissimilar points. As shown
    in Fig. [5](#S5.F5 "Figure 5 ‣ 5.3.3 Implementation of Siamese Networks ‣ 5.3
    Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")-b, it tries to make the distances
    of similar points as small as possible and the distances of dissimilar points
    at least greater than a margin $m$ (because the term inside the Hinge loss should
    become close to zero).'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $m>0$ 是边际，而 $[.]_{+}:=\max(.,0)$ 是标准的 Hinge 损失。损失的第一个项最小化相似点的嵌入距离，而第二个项最大化不相似点的嵌入距离。如图
    [5](#S5.F5 "Figure 5 ‣ 5.3.3 Implementation of Siamese Networks ‣ 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")-b 所示，它试图使相似点的距离尽可能小，而不相似点的距离至少大于边际 $m$（因为
    Hinge 损失中的项应该接近零）。'
- en: '– Generalized Contrastive Loss: The $y_{i}$, defined in Eq. ([158](#S5.E158
    "In 5.3.4 Contrastive Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")), is used in the contrastive loss, i.e., Eq. ([159](#S5.E159 "In
    5.3.4 Contrastive Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")).
    This variable is binary and a hard measure of similarity and dissimilarity. Rather
    than this hard measure, we can have a soft measure of similarity and dissimilarity,
    denoted by $\psi_{i}$, which states how similar $\boldsymbol{x}_{i}^{1}$ and $\boldsymbol{x}_{i}^{2}$
    are. This measure is between zero (completely similar) and one (completely dissimilar).
    It can be either given by the dataset as a hand-set measure or can be computed
    using any similarity measure such as the cosine function:'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: '– 广义对比损失：在 Eq. ([158](#S5.E158 "In 5.3.4 Contrastive Loss ‣ 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) 中定义的 $y_{i}$ 被用于对比损失，即 Eq. ([159](#S5.E159
    "In 5.3.4 Contrastive Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"))。这个变量是二元的，是相似性和不相似性的硬度量。与这种硬度量不同，我们可以使用 $\psi_{i}$ 表示相似性和不相似性的软度量，表明
    $\boldsymbol{x}_{i}^{1}$ 和 $\boldsymbol{x}_{i}^{2}$ 之间的相似程度。这个度量在零（完全相似）和一（完全不相似）之间。它可以是数据集提供的手动设置度量，也可以通过任何相似性度量如余弦函数计算得到：'
- en: '|  | $\displaystyle[0,1]\ni\psi_{i}:=\frac{1}{2}\big{(}-\cos(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})+1\big{)}.$
    |  | (160) |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle[0,1]\ni\psi_{i}:=\frac{1}{2}\big{(}-\cos(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})+1\big{)}.$
    |  | (160) |'
- en: 'In this case, the pairs $\{(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\}_{i=1}^{b}$
    need not be completely similar or dissimilar points but they can be any two random
    points from the dataset with some level of similarity/dissimilarity. The generalized
    contrastive loss generalizes the contrastive loss using this soft measure of similarity
    (Leyva-Vallina et al., [2021](#bib.bib78)):'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，成对的 $\{(\boldsymbol{x}_{i}^{1},\boldsymbol{x}_{i}^{2})\}_{i=1}^{b}$ 不需要完全相似或不相似，但它们可以是数据集中任何两个随机点，这些点之间具有一定程度的相似性/不相似性。广义对比损失通过这种相似性的软度量来推广对比损失（Leyva-Vallina
    et al., [2021](#bib.bib78)）：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}$
    | $\displaystyle\Big{(}(1-\psi_{i})d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}$
    |  | (161) |'
  id: totrans-943
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}$
    | $\displaystyle\Big{(}(1-\psi_{i})d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}$
    |  | (161) |'
- en: '|  |  | $\displaystyle+\psi_{i}\big{[}\!-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}+m\big{]}_{+}\Big{)}.$
    |  |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\psi_{i}\big{[}\!-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{1}),\textbf{f}(\boldsymbol{x}_{i}^{2})\big{)}+m\big{]}_{+}\Big{)}.$
    |  |'
- en: 5.3.5 Triplet Loss
  id: totrans-945
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.5 三元组损失
- en: 'One of the losses for Siamese networks with three sub-networks is the triplet
    loss (Schroff et al., [2015](#bib.bib100)) which uses the triplets in mini-batches,
    denoted by $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{b}$.
    It is defined as:'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有三个子网络的孪生网络，其中一种损失是三元组损失（Schroff et al., [2015](#bib.bib100)），它使用迷你批次中的三元组，记作
    $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{b}$。它定义为：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}\,\sum_{i=1}^{b}$ | $\displaystyle\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}+m\Big{]}_{+},$
    |  | (162) |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}\,\sum_{i=1}^{b}$ | $\displaystyle\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}+m\Big{]}_{+},$
    |  | (162) |'
- en: 'where $m>0$ is the margin and $[.]_{+}:=\max(.,0)$ is the standard Hinge loss.
    As shown in Fig. [5](#S5.F5 "Figure 5 ‣ 5.3.3 Implementation of Siamese Networks
    ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")-c, because of the
    used Hinge loss, this loss makes the distances of dissimilar points greater than
    the distances of similar points by at least a margin $m$; in other words, there
    will be a distance of at least margin $m$ between the positive and negative points.
    This loss desires to eventually have:'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$m>0$是边距，$[.]_{+}:=\max(.,0)$是标准的Hinge损失。如图[5](#S5.F5 "Figure 5 ‣ 5.3.3 Implementation
    of Siamese Networks ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")-c所示，由于使用了Hinge损失，这种损失使得不相似点之间的距离大于相似点之间的距离至少边距$m$；换句话说，正样本和负样本之间将有至少边距$m$的距离。这种损失最终希望达到：'
- en: '|  | $\displaystyle d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}+m\leq
    d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)},$
    |  | (163) |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}+m\leq
    d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)},$
    |  | (163) |'
- en: 'for all triplets. The triplet loss is closely related to the cost function
    for spectral large margin metric learning (Weinberger et al., [2006](#bib.bib124);
    Weinberger & Saul, [2009](#bib.bib123)) (see Section [3.2.1](#S3.SS2.SSS1 "3.2.1
    Large-Margin Metric Learning ‣ 3.2 Spectral Methods Using Hinge Loss ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")). It is also noteworthy that using the triplet loss as regularization
    for cross-entropy loss has been shown to increase robustness of network to some
    adversarial attacks (Mao et al., [2019](#bib.bib83)).'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: '对所有三元组而言，三元组损失与谱大边距度量学习的成本函数紧密相关（Weinberger et al., [2006](#bib.bib124); Weinberger
    & Saul, [2009](#bib.bib123)）（参见第[3.2.1](#S3.SS2.SSS1 "3.2.1 Large-Margin Metric
    Learning ‣ 3.2 Spectral Methods Using Hinge Loss ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")节）。值得注意的是，使用三元组损失作为交叉熵损失的正则化已被证明可以增强网络对某些对抗攻击的鲁棒性（Mao
    et al., [2019](#bib.bib83)）。'
- en: 5.3.6 Tuplet Loss
  id: totrans-951
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.6 元组损失
- en: 'In triplet loss, i.e. Eq. ([162](#S5.E162 "In 5.3.5 Triplet Loss ‣ 5.3 Metric
    Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")), we use one positive and one
    negative point per anchor point. The tuplet loss (Sohn, [2016](#bib.bib105)) uses
    several negative points per anchor point. If $k$ denotes the number of negative
    points per anchor point and $\boldsymbol{x}_{i}^{n,j}$ denotes the $j$-th negative
    point for $\boldsymbol{x}_{i}$, the tuplet loss is (Sohn, [2016](#bib.bib105)):'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: '在三元组损失中，即式（[162](#S5.E162 "In 5.3.5 Triplet Loss ‣ 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")），我们使用每个锚点一个正样本和一个负样本。元组损失（Sohn, [2016](#bib.bib105)）则使用每个锚点多个负样本。如果$k$表示每个锚点的负样本数，$\boldsymbol{x}_{i}^{n,j}$表示$\boldsymbol{x}_{i}$的第$j$个负样本，则元组损失为（Sohn,
    [2016](#bib.bib105)）：'
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}\,\sum_{i=1}^{b}\sum_{j=1}^{k}$
    | $\displaystyle\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}$
    |  | (164) |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}\,\sum_{i=1}^{b}\sum_{j=1}^{k}$
    | $\displaystyle\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}$
    |  | (164) |'
- en: '|  |  | $\displaystyle-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n,j})\big{)}+m\Big{]}_{+}.$
    |  |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n,j})\big{)}+m\Big{]}_{+}.$
    |  |'
- en: This loss function pushes multiple negative points away from the anchor point
    simultaneously.
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失函数同时将多个负样本推离锚点。
- en: 5.3.7 Neighborhood Component Analysis Loss
  id: totrans-956
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.7 邻域组件分析损失
- en: 'Neighborhood Component Analysis (NCA) (Goldberger et al., [2005](#bib.bib53))
    was originally proposed as a spectral metric learning method (see Section [4.2.1](#S4.SS2.SSS1
    "4.2.1 Neighborhood Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")). After the success of deep learning, it
    was used as the loss function of Siamese networks where we minimize the negative
    log-likelihood using Gaussian distribution or the softmax form within the mini-batch.
    Assume we have $c$ classes in every mini-batch. We denote the class index of $\boldsymbol{x}_{i}$
    by $c(\boldsymbol{x}_{i})$ and the data points of the $j$-th class in the mini-batch
    by $\mathcal{X}_{j}$. The NCA loss is:'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: '邻域组件分析（NCA）（Goldberger等，[2005](#bib.bib53)）最初被提议作为一种谱度量学习方法（见第[4.2.1节](#S4.SS2.SSS1
    "4.2.1 Neighborhood Component Analysis (NCA) ‣ 4.2 Neighborhood Component Analysis
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")）。在深度学习成功之后，它被用作Siamese网络的损失函数，在这里我们通过高斯分布或小批量中的softmax形式来最小化负对数似然。假设每个小批量中有$c$类。我们用$c(\boldsymbol{x}_{i})$表示$\boldsymbol{x}_{i}$的类别索引，用$\mathcal{X}_{j}$表示小批量中第$j$类的数据点。NCA损失为：'
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}\big{)}$
    |  | (165) |'
  id: totrans-958
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}\big{)}$
    |  | (165) |'
- en: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\sum_{\boldsymbol{x}_{j}^{n}\in\mathcal{X}_{j}}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\textbf{f}(\boldsymbol{x}_{j}^{n})\big{)}\big{)}\Big{]}^{-1}\Big{)}.$
    |  |'
  id: totrans-959
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\sum_{\boldsymbol{x}_{j}^{n}\in\mathcal{X}_{j}}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\textbf{f}(\boldsymbol{x}_{j}^{n})\big{)}\big{)}\Big{]}^{-1}\Big{)}.$
    |  |'
- en: The numerator minimizes the distances of similar points and the denominator
    maximizes the distances of dissimilar points.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 分子最小化相似点的距离，而分母最大化不相似点的距离。
- en: 5.3.8 Proxy Neighborhood Component Analysis Loss
  id: totrans-961
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.8 Proxy邻域组件分析损失
- en: 'Computation of terms, especially the normalization factor in the denominator,
    is time- and memory-consuming in the NCA loss function (see Eq. ([165](#S5.E165
    "In 5.3.7 Neighborhood Component Analysis Loss ‣ 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))). Proxy-NCA loss functions define some proxy points in
    the embedding space of network and use them in the NCA loss to accelerate computation
    and make it memory-efficient (Movshovitz-Attias et al., [2017](#bib.bib87)). The
    proxies are representatives of classes in the embedding space and they can be
    defined in various ways. The simplest way is to define the proxy of every class
    as the mean of embedded points of that class. Of course, new mini-batches come
    during training. We can accumulate the embedded points of mini-batches and update
    the proxies after training the network by every mini-batch. Another approach for
    defining proxies is to cluster the embedded points into $c$ clusters (e.g., by
    K-means) and use the centroid of clusters.'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: '计算这些项，尤其是分母中的归一化因子，在NCA损失函数中是耗时且占用内存的（见方程([165](#S5.E165 "In 5.3.7 Neighborhood
    Component Analysis Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")））。Proxy-NCA损失函数在网络的嵌入空间中定义了一些代理点，并在NCA损失中使用它们以加速计算并提高内存效率（Movshovitz-Attias等，[2017](#bib.bib87)）。这些代理是嵌入空间中类别的代表，可以通过各种方式定义。最简单的方法是将每个类别的代理定义为该类别嵌入点的均值。当然，训练过程中会有新的小批量数据到来。我们可以积累小批量的嵌入点，并在每个小批量训练网络后更新代理。另一种定义代理的方法是将嵌入点聚类成$c$个簇（例如，通过K-means），并使用簇的质心。'
- en: 'Let the set of proxies be denotes by $\mathcal{P}$ whose cardinality is the
    number of classes, i.e., $c$. Every embedded point is assigned to one of the proxies
    by (Movshovitz-Attias et al., [2017](#bib.bib87)):'
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 设代理点集合为$\mathcal{P}$，其基数为类别的数量，即$c$。每个嵌入点通过（Movshovitz-Attias等，[2017](#bib.bib87)）被分配给其中一个代理点：
- en: '|  | $\displaystyle\Pi(\textbf{f}(\boldsymbol{x}_{i})):=\arg\min_{\boldsymbol{\pi}\in\mathcal{P}}\&#124;\textbf{f}(\boldsymbol{x}_{i})-\boldsymbol{\pi}\&#124;_{2}^{2},$
    |  | (166) |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Pi(\textbf{f}(\boldsymbol{x}_{i})):=\arg\min_{\boldsymbol{\pi}\in\mathcal{P}}\&#124;\textbf{f}(\boldsymbol{x}_{i})-\boldsymbol{\pi}\&#124;_{2}^{2},$
    |  | (166) |'
- en: 'or we can assign every point to the proxy of its own class. Let $\boldsymbol{pi}_{j}$
    denote the proxy associated with the $j$-th class. The Proxy-NCA loss is the NCA
    loss, i.e. Eq. ([165](#S5.E165 "In 5.3.7 Neighborhood Component Analysis Loss
    ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), but using proxies
    (Movshovitz-Attias et al., [2017](#bib.bib87)):'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: '或者我们可以将每个点分配给其自身类别的代理。令 $\boldsymbol{pi}_{j}$ 表示与第 $j$ 类相关联的代理。Proxy-NCA 损失是
    NCA 损失，即公式 ([165](#S5.E165 "在 5.3.7 Neighborhood Component Analysis Loss ‣ 5.3
    Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))，但使用代理 (Movshovitz-Attias et al.,
    [2017](#bib.bib87))：'
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\Pi(\textbf{f}(\boldsymbol{x}_{i}^{p}))\big{)}\big{)}$
    |  | (167) |'
  id: totrans-966
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\Pi(\textbf{f}(\boldsymbol{x}_{i}^{p}))\big{)}\big{)}$
    |  | (167) |'
- en: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\boldsymbol{\pi}_{j}\big{)}\big{)}\Big{]}^{-1}\Big{)}.$
    |  |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\boldsymbol{\pi}_{j}\big{)}\big{)}\Big{]}^{-1}\Big{)}.$
    |  |'
- en: 'It is shown in (Movshovitz-Attias et al., [2017](#bib.bib87)) that the Proxy-NCA
    loss, i.e. Eq. ([167](#S5.E167 "In 5.3.8 Proxy Neighborhood Component Analysis
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), is an upper-bound
    on the NCA loss, i.e. Eq. ([165](#S5.E165 "In 5.3.7 Neighborhood Component Analysis
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")); hence, its minimization
    also achieves the goal of NCA. Comparing Eqs. ([165](#S5.E165 "In 5.3.7 Neighborhood
    Component Analysis Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    and ([167](#S5.E167 "In 5.3.8 Proxy Neighborhood Component Analysis Loss ‣ 5.3
    Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) shows that Proxy-NCA is faster
    and more efficient than NCA because it uses only proxies of negative classes rather
    than using all negative points in the mini-batch. Proxy-NCA has also been used
    in feature extraction from medical images (Teh & Taylor, [2020](#bib.bib108)).
    It is noteworthy that we can incorporate temperature scaling (Hinton et al., [2014](#bib.bib65))
    in the Proxy-NCA loss. The obtained loss is named Proxy-NCA++ (Teh et al., [2020](#bib.bib109))
    and is defined as:'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '(Movshovitz-Attias et al., [2017](#bib.bib87)) 中显示，Proxy-NCA 损失，即公式 ([167](#S5.E167
    "在 5.3.8 Proxy Neighborhood Component Analysis Loss ‣ 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))，是 NCA 损失的上界，即公式 ([165](#S5.E165 "在 5.3.7 Neighborhood Component
    Analysis Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))；因此，其最小化也实现了
    NCA 的目标。比较公式 ([165](#S5.E165 "在 5.3.7 Neighborhood Component Analysis Loss ‣ 5.3
    Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) 和 ([167](#S5.E167 "在 5.3.8 Proxy
    Neighborhood Component Analysis Loss ‣ 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) 显示，Proxy-NCA 比 NCA 更快、更高效，因为它仅使用负类的代理点，而不是使用所有负点。Proxy-NCA
    还被用于从医学图像中提取特征 (Teh & Taylor, [2020](#bib.bib108))。值得注意的是，我们可以在 Proxy-NCA 损失中引入温度缩放
    (Hinton et al., [2014](#bib.bib65))。获得的损失被称为 Proxy-NCA++ (Teh et al., [2020](#bib.bib109))，定义如下：'
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\Pi(\textbf{f}(\boldsymbol{x}_{i}^{p}))\big{)}\times\frac{1}{\tau}\big{)}$
    |  | (168) |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\Pi(\textbf{f}(\boldsymbol{x}_{i}^{p}))\big{)}\times\frac{1}{\tau}\big{)}$
    |  | (168) |'
- en: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\boldsymbol{\pi}_{j}\big{)}\times\frac{1}{\tau}\big{)}\Big{]}^{-1}\Big{)},$
    |  |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times\Big{[}\sum_{j=1,j\neq c(\boldsymbol{x}_{i})}^{c}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})-\boldsymbol{\pi}_{j}\big{)}\times\frac{1}{\tau}\big{)}\Big{]}^{-1}\Big{)},$
    |  |'
- en: where $\tau>0$ is the temperature which is a hyper-parameter.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau>0$ 是温度参数，是一个超参数。
- en: 5.3.9 Softmax Triplet Loss
  id: totrans-972
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.9 Softmax Triplet Loss
- en: 'Consider a mini-batch containing points from $c$ classes where $c(\boldsymbol{x}_{i})$
    is the class index of $\boldsymbol{x}_{i}$ and $\mathcal{X}_{j}$ denotes the points
    of the $j$-th class in the mini-batch. We can use the softmax function or the
    Gaussian distribution for the probability that the point $\boldsymbol{x}_{i}$
    takes $\boldsymbol{x}_{j}$ as its neighbor. Similar to Eq. ([84](#S4.E84 "In 4.1
    Collapsing Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) or Eq. ([165](#S5.E165 "In 5.3.7
    Neighborhood Component Analysis Loss ‣ 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")), we can have the softmax function used in NCA (Goldberger
    et al., [2005](#bib.bib53)):'
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含来自 $c$ 类的点的迷你批次，其中 $c(\boldsymbol{x}_{i})$ 是 $\boldsymbol{x}_{i}$ 的类别索引，$\mathcal{X}_{j}$
    表示迷你批次中第 $j$ 类的点。我们可以使用软最大函数或高斯分布来计算点 $\boldsymbol{x}_{i}$ 选择 $\boldsymbol{x}_{j}$
    作为其邻居的概率。类似于 Eq. ([84](#S4.E84 "在 4.1 类别压缩 ‣ 4 概率度量学习 ‣ 谱、概率和深度度量学习：教程和调查")) 或
    Eq. ([165](#S5.E165 "在 5.3.7 邻域组件分析损失 ‣ 5.3 基于孪生网络的度量学习 ‣ 5 深度度量学习 ‣ 谱、概率和深度度量学习：教程和调查"))，我们可以使用
    NCA 中的软最大函数 (Goldberger 等，[2005](#bib.bib53))：
- en: '|  | $\displaystyle p_{ij}:=\frac{\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}\big{)}}{\sum_{k\neq
    i,k=1}^{b}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}\big{)}},\quad
    j\neq i.$ |  | (169) |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{ij}:=\frac{\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}\big{)}}{\sum_{k\neq
    i,k=1}^{b}\exp\big{(}\!-\!d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}\big{)}},\quad
    j\neq i.$ |  | (169) |'
- en: 'Another approach for the softmax form is to use inner product in the exponent
    (Ye et al., [2019](#bib.bib139)):'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种软最大形式的方法是使用指数中的内积 (Ye 等，[2019](#bib.bib139))：
- en: '|  | $\displaystyle p_{ij}:=\frac{\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}}{\sum_{k=1,k\neq
    i}^{b}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\big{)}},\quad
    j\neq i.$ |  | (170) |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{ij}:=\frac{\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}}{\sum_{k=1,k\neq
    i}^{b}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\big{)}},\quad
    j\neq i.$ |  | (170) |'
- en: 'The loss function for training the network can be the negative log-likelihood
    which can be called the softmax triplet loss (Ye et al., [2019](#bib.bib139)):'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练网络的损失函数可以是负对数似然，也称为软最大三元组损失 (Ye 等，[2019](#bib.bib139))：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}-\sum_{i=1}^{b}\Big{(}$
    | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\log(p_{ij})$
    |  | (171) |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}-\sum_{i=1}^{b}\Big{(}$
    | $\displaystyle\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\log(p_{ij})$
    |  | (171) |'
- en: '|  |  | $\displaystyle-\sum_{\boldsymbol{x}_{j}\not\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\log(1-p_{ij})\Big{)}.$
    |  |'
  id: totrans-979
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\sum_{\boldsymbol{x}_{j}\not\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\log(1-p_{ij})\Big{)}.$
    |  |'
- en: This decreases and increases the distances of similar points and dissimilar
    points, respectively.
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 这分别减少和增加了相似点和不同点的距离。
- en: 5.3.10 Triplet Global Loss
  id: totrans-981
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.10 三元组全局损失
- en: 'The triplet global loss (Kumar BG et al., [2016](#bib.bib77)) uses the mean
    and variance of the anchor-positive pairs and anchor-negative pairs. It is defined
    as:'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组全局损失 (Kumar BG 等，[2016](#bib.bib77)) 使用锚点-正样本对和锚点-负样本对的均值和方差。它定义为：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}(\sigma_{p}^{2}+\sigma_{n}^{2})+\lambda\,[\mu_{p}-\mu_{n}+m]_{+},$
    |  | (172) |'
  id: totrans-983
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}(\sigma_{p}^{2}+\sigma_{n}^{2})+\lambda\,[\mu_{p}-\mu_{n}+m]_{+},$
    |  | (172) |'
- en: 'where $\lambda>0$ is the regularization parameter, $m>0$ is the margin, the
    means of pairs are:'
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 是正则化参数，$m>0$ 是边际，样本对的均值为：
- en: '|  | $\displaystyle\mu_{p}:=\frac{1}{b}\sum_{i=1}^{b}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)},$
    |  |'
  id: totrans-985
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mu_{p}:=\frac{1}{b}\sum_{i=1}^{b}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)},$
    |  |'
- en: '|  | $\displaystyle\mu_{n}:=\frac{1}{b}\sum_{i=1}^{b}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)},$
    |  |'
  id: totrans-986
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mu_{n}:=\frac{1}{b}\sum_{i=1}^{b}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)},$
    |  |'
- en: 'and the variances of pairs are:'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 这对的方差为：
- en: '|  | $\displaystyle\sigma_{p}^{2}:=\frac{1}{b}\sum_{i=1}^{b}\Big{(}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-\mu_{p}\Big{)}^{2},$
    |  |'
  id: totrans-988
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sigma_{p}^{2}:=\frac{1}{b}\sum_{i=1}^{b}\Big{(}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-\mu_{p}\Big{)}^{2},$
    |  |'
- en: '|  | $\displaystyle\sigma_{n}^{2}:=\frac{1}{b}\sum_{i=1}^{b}\Big{(}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}-\mu_{n}\Big{)}^{2}.$
    |  |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sigma_{n}^{2}:=\frac{1}{b}\sum_{i=1}^{b}\Big{(}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}-\mu_{n}\Big{)}^{2}.$
    |  |'
- en: The first term of this loss minimizes the variances of anchor-positive and anchor-negative
    pairs. The second term, however, discriminates the anchor-positive pairs from
    the anchor-negative pairs. Hence, the negative points are separated from the positive
    points.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失的第一项最小化了锚点-正点对和锚点-负点对的方差。然而，第二项则区分了锚点-正点对与锚点-负点对。因此，负点从正点中被分离出来。
- en: 5.3.11 Angular Loss
  id: totrans-991
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.11 角度损失
- en: 'For a triplet $(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})$,
    consider a triangle whose vertices are the anchor, positive, and negative points.
    To satisfy Eq. ([163](#S5.E163 "In 5.3.5 Triplet Loss ‣ 5.3 Metric Learning by
    Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")) in the triplet loss, the angle at the
    vertex $\boldsymbol{x}_{i}^{n}$ should be small so the edge $d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}$
    becomes larger than the edge $d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}$.
    Hence, we need to have and upper bound $\alpha>0$ on the angle at the vertex $\boldsymbol{x}_{i}^{n}$.
    If $\boldsymbol{x}_{i}^{c}:=(\boldsymbol{x}_{i}^{a}+\boldsymbol{x}_{i}^{p})/2$,
    the angular loss is defined to be (Wang et al., [2017](#bib.bib117)):'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个三元组 $(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})$，考虑一个三角形，其顶点是锚点、正点和负点。为了在三元组损失中满足方程
    ([163](#S5.E163 "在 5.3.5 三元组损失 ‣ 5.3 通过孪生网络进行度量学习 ‣ 5 深度度量学习 ‣ 谱、概率和深度度量学习：教程和调查"))，顶点
    $\boldsymbol{x}_{i}^{n}$ 处的角度应该很小，以使边 $d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{n})\big{)}$
    大于边 $d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}$。因此，我们需要在顶点
    $\boldsymbol{x}_{i}^{n}$ 处设定一个上界 $\alpha>0$。如果 $\boldsymbol{x}_{i}^{c}:=(\boldsymbol{x}_{i}^{a}+\boldsymbol{x}_{i}^{p})/2$，角度损失定义为
    (Wang et al., [2017](#bib.bib117))：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,$ |  | (173) |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,$ |  | (173) |'
- en: '|  |  | $\displaystyle\sum_{i=1}^{b}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-4\tan^{2}\!\big{(}\alpha\,d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{c})\big{)}\big{)}\Big{]}_{+}.$
    |  |'
  id: totrans-994
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sum_{i=1}^{b}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p})\big{)}-4\tan^{2}\!\big{(}\alpha\,d\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{c})\big{)}\big{)}\Big{]}_{+}.$
    |  |'
- en: This loss reduces the distance of the anchor and positive points and increases
    the distance of anchor and $\boldsymbol{x}_{i}^{c}$ and the upper bound $\alpha$.
    This increases the distance of the anchor and negative points for discrimination
    of dissimilar points.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失减少了锚点和正点之间的距离，同时增加了锚点和 $\boldsymbol{x}_{i}^{c}$ 之间的距离以及上界 $\alpha$。这增加了锚点和负点之间的距离，以区分不同点。
- en: 5.3.12 SoftTriple Loss
  id: totrans-996
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.12 SoftTriple 损失
- en: 'If we normalize the points to have unit length, Eq. ([163](#S5.E163 "In 5.3.5
    Triplet Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")) can
    be restated by using inner products:'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将点归一化为单位长度，方程 ([163](#S5.E163 "在 5.3.5 三元组损失 ‣ 5.3 通过孪生网络进行度量学习 ‣ 5 深度度量学习
    ‣ 谱、概率和深度度量学习：教程和调查")) 可以通过使用内积重新表述：
- en: '|  | $\displaystyle\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}_{i}^{p})+m\leq\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}_{i}^{n}),$
    |  | (174) |'
  id: totrans-998
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}_{i}^{p})+m\leq\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}_{i}^{n}),$
    |  | (174) |'
- en: 'whose margin is not exactly equal to the margin in Eq. ([163](#S5.E163 "In
    5.3.5 Triplet Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")). Consider
    a Siamese network whose last layer’s weights are $\{\boldsymbol{w}_{l}\in\mathbb{R}^{p}\}_{l=1}^{c}$
    where $p$ is the dimensionality of the one-to-last layer and $c$ is the number
    of classes and the number of output neurons. We consider $k$ centers for the embedding
    of every class; hence, we define $\boldsymbol{w}_{l}^{j}\in\mathbb{R}^{p}$ as
    $\boldsymbol{w}_{l}$ for its $j$-th center. It is shown in (Qian et al., [2019](#bib.bib93))
    that softmax loss results in Eq. ([174](#S5.E174 "In 5.3.12 SoftTriple Loss ‣
    5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). Therefore, we can use the SoftTriple
    loss for training a Siamese network (Qian et al., [2019](#bib.bib93)):'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 其边距与公式 ([163](#S5.E163 "在 5.3.5 三元组损失 ‣ 5.3 基于相似网络的度量学习 ‣ 5 深度度量学习 ‣ 谱、概率和深度度量学习：教程和调查"))
    中的边距不完全相同。考虑一个其最后一层权重为 $\{\boldsymbol{w}_{l}\in\mathbb{R}^{p}\}_{l=1}^{c}$ 的相似网络，其中
    $p$ 是倒数第二层的维度，$c$ 是类别数量和输出神经元数量。我们考虑每个类别的 $k$ 个中心；因此，我们将 $\boldsymbol{w}_{l}^{j}\in\mathbb{R}^{p}$
    定义为其第 $j$ 个中心的 $\boldsymbol{w}_{l}$。在 (Qian et al., [2019](#bib.bib93)) 中显示，softmax
    损失导致公式 ([174](#S5.E174 "在 5.3.12 SoftTriple 损失 ‣ 5.3 基于相似网络的度量学习 ‣ 5 深度度量学习 ‣
    谱、概率和深度度量学习：教程和调查")) 中的结果。因此，我们可以使用 SoftTriple 损失来训练相似网络 (Qian et al., [2019](#bib.bib93))：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp(\lambda(s_{i,y_{i}}-\delta))$
    |  | (175) |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\log\Big{(}\exp(\lambda(s_{i,y_{i}}-\delta))$
    |  | (175) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\times\big{(}\exp(\lambda(s_{i,y_{i}}-\delta))+\sum_{l=1,l\neq
    y_{i}}^{c}\exp(\lambda s_{i,l})\big{)}^{-1}\Big{)},$ |  |'
  id: totrans-1001
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\times\big{(}\exp(\lambda(s_{i,y_{i}}-\delta))+\sum_{l=1,l\neq
    y_{i}}^{c}\exp(\lambda s_{i,l})\big{)}^{-1}\Big{)},$ |  |'
- en: 'where $\lambda,\delta>0$ are hyper-parameters, $y_{i}$ is the label of $\boldsymbol{x}_{i}$,
    and:'
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda,\delta>0$ 是超参数，$y_{i}$ 是 $\boldsymbol{x}_{i}$ 的标签，以及：
- en: '|  | $\displaystyle s_{i,l}:=\sum_{j=1}^{k}\frac{\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{j}\big{)}}{\sum_{t=1}^{k}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{t}\big{)}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{k}.$
    |  |'
  id: totrans-1003
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s_{i,l}:=\sum_{j=1}^{k}\frac{\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{j}\big{)}}{\sum_{t=1}^{k}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{t}\big{)}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\boldsymbol{w}_{l}^{k}.$
    |  |'
- en: This loss increases and decreases the intra-class and inter-class distances,
    respectively.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失函数分别增加和减少类内距离和类间距离。
- en: 5.3.13 Fisher Siamese Losses
  id: totrans-1005
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.13 费希尔相似损失
- en: Fisher Discriminant Analysis (FDA) (Fisher, [1936](#bib.bib33); Ghojogh et al.,
    [2019b](#bib.bib39)) decreases the intra-class variance and increases the inter-class
    variance by maximizing the Fisher criterion. This idea is very similar to the
    idea of loss functions for Siamese networks. Hence, we can combine the methods
    of FDA and Siamese loss functions.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 费希尔判别分析 (FDA) (Fisher, [1936](#bib.bib33); Ghojogh et al., [2019b](#bib.bib39))
    通过最大化费希尔准则来减少类内方差并增加类间方差。这一理念与相似网络的损失函数的理念非常相似。因此，我们可以将 FDA 和相似损失函数的方法结合起来。
- en: 'Consider a Siamese network whose last layer is denoted by the projection matrix
    $\boldsymbol{U}$. We consider the features of the one-to-last layer in the mini-batch.
    The covariance matrices of similar points and dissimilar points (one-to-last layer
    features) in the mini-batch are denoted by $\boldsymbol{S}_{W}$ and $\boldsymbol{S}_{B}$.
    These covariances become $\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U}$
    and $\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}$, respectively, after
    the later layer’s projection because of the quadratic characteristic of covariance.
    As in FDA, we can maximize the Fisher criterion or equivalently minimize the negative
    Fisher criterion:'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个最后一层用投影矩阵 $\boldsymbol{U}$ 表示的相似网络。我们考虑小批量中的倒数第二层特征。小批量中相似点和不相似点（倒数第二层特征）的协方差矩阵分别用
    $\boldsymbol{S}_{W}$ 和 $\boldsymbol{S}_{B}$ 表示。由于协方差的二次特性，这些协方差在后续层投影后变成了 $\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U}$
    和 $\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}$。与 FDA 一样，我们可以最大化费希尔准则或等效地最小化负的费希尔准则：
- en: '|  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}~{}~{}~{}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}).$
    |  |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}~{}~{}~{}\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}).$
    |  |'
- en: 'This problem is ill-posed because it increases the total covariance of embedded
    data to increase the term $\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})$.
    Hence, we add minimization of the total covariance as the regularization term:'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题不适定，因为它通过增加$\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})$来增加嵌入数据的总协方差。因此，我们添加了总协方差的最小化作为正则化项：
- en: '|  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}{}{}{}$ | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})$
    |  |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\boldsymbol{U}}{\text{minimize}}{}{}{}$ | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})$
    |  |'
- en: '|  |  | $\displaystyle+\epsilon\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{T}\boldsymbol{U}),$
    |  |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\epsilon\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{T}\boldsymbol{U}),$
    |  |'
- en: 'where $\epsilon\in(0,1)$ is the regularization parameter and $\boldsymbol{S}_{T}$
    is the covariance of all points of the mini-batch in the one-to-last layer. The
    total scatter can be written as the summation of $\boldsymbol{S}_{W}$ and $\boldsymbol{S}_{B}$;
    hence:'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\epsilon\in(0,1)$是正则化参数，$\boldsymbol{S}_{T}$是迷你批次中倒数第二层所有点的协方差。总散布可以写作$\boldsymbol{S}_{W}$和$\boldsymbol{S}_{B}$的和；因此：
- en: '|  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+\epsilon\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{T}\boldsymbol{U})$
    |  |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+\epsilon\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{T}\boldsymbol{U})$
    |  |'
- en: '|  | $\displaystyle=\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{W}-\boldsymbol{S}_{W}+\epsilon\boldsymbol{S}_{W}+\epsilon\boldsymbol{S}_{B})\boldsymbol{U}\big{)}$
    |  |'
  id: totrans-1014
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\textbf{tr}\big{(}\boldsymbol{U}^{\top}(\boldsymbol{S}_{W}-\boldsymbol{S}_{W}+\epsilon\boldsymbol{S}_{W}+\epsilon\boldsymbol{S}_{B})\boldsymbol{U}\big{)}$
    |  |'
- en: '|  | $\displaystyle=(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}),$
    |  |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U}),$
    |  |'
- en: 'where $\lambda:=1-\epsilon$. Inspired by Eq. ([162](#S5.E162 "In 5.3.5 Triplet
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we can have the
    following loss, named the Fisher discriminant triplet loss (Ghojogh et al., [2020f](#bib.bib45)):'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$\lambda:=1-\epsilon$。受公式 ([162](#S5.E162 "In 5.3.5 Triplet Loss ‣ 5.3 Metric
    Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) 启发，我们可以得到以下损失，称为费舍尔判别三元组损失 (Ghojogh
    et al., [2020f](#bib.bib45))：'
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}{}$ | $\displaystyle\Big{[}(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})$
    |  | (176) |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}{}$ | $\displaystyle\Big{[}(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})$
    |  | (176) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+m\Big{]}_{+},$
    |  |'
  id: totrans-1018
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+m\Big{]}_{+},$
    |  |'
- en: 'where $m>0$ is the margin. Backpropagating the error of this loss can update
    both $\boldsymbol{U}$ and other layers of network. Note that the summation over
    the mini-batch is integrated in the computation of covariance matrices $\boldsymbol{S}_{W}$
    and $\boldsymbol{S}_{B}$. Inspired by Eq. ([159](#S5.E159 "In 5.3.4 Contrastive
    Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")), we can also have
    the Fisher discriminant contrastive loss (Ghojogh et al., [2020f](#bib.bib45)):'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$m>0$是边距。通过反向传播这个损失的误差可以更新$\boldsymbol{U}$和网络的其他层。请注意，迷你批次的求和已集成在协方差矩阵$\boldsymbol{S}_{W}$和$\boldsymbol{S}_{B}$的计算中。受公式
    ([159](#S5.E159 "In 5.3.4 Contrastive Loss ‣ 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) 启发，我们也可以得到费舍尔判别对比损失 (Ghojogh et al., [2020f](#bib.bib45))：'
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}$ | $\displaystyle(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})$
    |  | (177) |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}$ | $\displaystyle(2-\lambda)\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})$
    |  | (177) |'
- en: '|  |  | $\displaystyle+\big{[}\!-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+m\big{]}_{+}.$
    |  |'
  id: totrans-1021
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\big{[}\!-\lambda\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})+m\big{]}_{+}.$
    |  |'
- en: 'Note that the variable $y_{i}$ used in the contrastive loss (see Eq. ([158](#S5.E158
    "In 5.3.4 Contrastive Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey"))) is already used in computation of the covariances $\boldsymbol{S}_{W}$
    and $\boldsymbol{S}_{B}$. There exist some other loss functions inspired by Fisher
    discriminant analysis but they are not used for Siamese networks. Those methods
    will be introduced in Section [5.4](#S5.SS4 "5.4 Deep Discriminant Analysis Metric
    Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey").'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在对比损失中使用的变量 $y_{i}$（参见公式 ([158](#S5.E158 "在 5.3.4 对比损失 ‣ 5.3 蕴含网络的度量学习 ‣
    5 深度度量学习 ‣ 光谱、概率和深度度量学习：教程和调查"))）已经在协方差 $\boldsymbol{S}_{W}$ 和 $\boldsymbol{S}_{B}$
    的计算中使用过了。存在一些受 Fisher 判别分析启发的其他损失函数，但它们并未用于孪生网络。这些方法将在第 [5.4](#S5.SS4 "5.4 深度判别分析度量学习
    ‣ 5 深度度量学习 ‣ 光谱、概率和深度度量学习：教程和调查") 节中介绍。
- en: 5.3.14 Deep Adversarial Metric Learning
  id: totrans-1023
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.14 深度对抗度量学习
- en: 'In deep adversarial metric learning (Duan et al., [2018](#bib.bib30)), negative
    points are generated in an adversarial learning (Goodfellow et al., [2014](#bib.bib54);
    Ghojogh et al., [2021b](#bib.bib47)). In this method, we have a generator $G(.)$
    which tries to generate negative points fooling the metric learning. Using triplet
    inputs $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{b}$,
    the loss function of generator is (Duan et al., [2018](#bib.bib30)):'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度对抗度量学习中（Duan et al., [2018](#bib.bib30)），负样本是通过对抗学习生成的（Goodfellow et al.,
    [2014](#bib.bib54); Ghojogh et al., [2021b](#bib.bib47)）。在这种方法中，我们有一个生成器 $G(.)$，它试图生成欺骗度量学习的负样本。使用三元组输入
    $\{(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})\}_{i=1}^{b}$，生成器的损失函数是
    (Duan et al., [2018](#bib.bib30))：
- en: '|  |  | $\displaystyle\mathcal{L}_{G}:=\sum_{i=1}^{b}\Big{(}\&#124;G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})-\boldsymbol{x}_{i}^{a}\&#124;_{2}^{2}$
    |  | (178) |'
  id: totrans-1025
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}_{G}:=\sum_{i=1}^{b}\Big{(}\&#124;G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})-\boldsymbol{x}_{i}^{a}\&#124;_{2}^{2}$
    |  | (178) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{1}\&#124;G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})-\boldsymbol{x}_{i}^{n}\&#124;_{2}^{2}$
    |  |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{1}\&#124;G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})-\boldsymbol{x}_{i}^{n}\&#124;_{2}^{2}$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\big{[}d(\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})))$
    |  |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}+\lambda_{2}\big{[}d(\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(G(\boldsymbol{x}_{i}^{a},\boldsymbol{x}_{i}^{p},\boldsymbol{x}_{i}^{n})))$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d(\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p}))+m\big{]}_{+}\Big{)},$
    |  |'
  id: totrans-1028
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d(\textbf{f}(\boldsymbol{x}_{i}^{a}),\textbf{f}(\boldsymbol{x}_{i}^{p}))+m\big{]}_{+}\Big{)},$
    |  |'
- en: where $\lambda_{1},\lambda_{2}>0$ are the regularization parameters. This loss
    makes the generated negative point close to the real negative point (to be negative)
    and the anchor point (for fooling metric learning adversarially). The Hinge loss
    makes the generated negative point different from the anchor and positive points
    so it also acts like a real negative. If $\mathcal{L}_{M}$ denotes any loss function
    for Siamese network, such as the triplet loss, the total loss function in deep
    adversarial metric learning is minimizing $\mathcal{L}_{G}+\lambda_{3}\mathcal{L}_{M}$
    where $\lambda_{3}>0$ is the regularization parameter (Duan et al., [2018](#bib.bib30)).
    It is noteworthy that there exists another adversarial metric learning which is
    not for Siamese networks but for cross-modal data (Xu et al., [2019a](#bib.bib128)).
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1},\lambda_{2}>0$ 是正则化参数。这个损失函数使生成的负样本接近真实的负样本（为了负样本）和锚点（为了对抗度量学习）。Hinge
    损失使生成的负样本与锚点和正样本不同，因此它也充当了一个真实的负样本。如果 $\mathcal{L}_{M}$ 表示任意的 Siamese 网络损失函数，例如三元组损失，那么深度对抗度量学习中的总损失函数是最小化
    $\mathcal{L}_{G}+\lambda_{3}\mathcal{L}_{M}$，其中 $\lambda_{3}>0$ 是正则化参数（Duan 等，
    [2018](#bib.bib30)）。值得注意的是，还存在另一种对抗度量学习，它不是针对 Siamese 网络而是针对跨模态数据（Xu 等， [2019a](#bib.bib128)）。
- en: 5.3.15 Triplet Mining
  id: totrans-1030
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.15 三元组挖掘
- en: 'In every mini-batch containing data points from $c$ classes, we can select
    and use triplets of data points in different ways. For example, we can use all
    similar and dissimilar points for every anchor point as positive and negative
    points, respectively. Another approach is to only use some of the similar and
    dissimilar points within the mini-batch. These approaches for selecting and using
    triplets are called triplet mining (Sikaroudi et al., [2020a](#bib.bib102)). In
    the following, we review some of the most important triplet mining methods. We
    use triplet mining methods for the triplet loss, i.e., Eq. ([162](#S5.E162 "In
    5.3.5 Triplet Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")). Suppose
    $b$ is the mini-batch size, $c(\boldsymbol{x}_{i})$ is the class index of $\boldsymbol{x}_{i}$,
    $\mathcal{X}_{j}$ denotes the points of the $j$-th class in the mini-batch, and
    $\mathcal{X}$ denotes the data points in the mini-batch.'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个包含 $c$ 类数据点的小批量中，我们可以以不同的方式选择和使用数据点的三元组。例如，我们可以将所有相似和不相似的点分别用作每个锚点的正样本和负样本。另一种方法是仅使用小批量中的一些相似和不相似的点。这些选择和使用三元组的方法称为三元组挖掘（Sikaroudi
    等， [2020a](#bib.bib102)）。接下来，我们回顾一些最重要的三元组挖掘方法。我们将三元组挖掘方法用于三元组损失，即公式 ([162](#S5.E162
    "在 5.3.5 三元组损失 ‣ 5.3 Siamese 网络的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程和调查"))。假设 $b$
    是小批量的大小，$c(\boldsymbol{x}_{i})$ 是 $\boldsymbol{x}_{i}$ 的类别索引，$\mathcal{X}_{j}$
    表示小批量中第 $j$ 类的点，$\mathcal{X}$ 表示小批量中的数据点。
- en: '– Batch-all: Batch-all triplet mining (Ding et al., [2015](#bib.bib28)) considers
    every point in the mini-batch as an anchor point. All points in the mini-batch
    which are in the same class the anchor point are used as positive points. All
    points in the mini-batch which are in a different class from the class of anchor
    point are used as negative points:'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: '– Batch-all: Batch-all 三元组挖掘（Ding 等， [2015](#bib.bib28)）将小批量中的每个点视为锚点。小批量中与锚点相同类别的所有点被用作正样本。小批量中与锚点类别不同的所有点被用作负样本。'
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (179) |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (179) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
- en: Batch-all mining makes use of all data points in the mini-batch to utilize all
    available information.
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: Batch-all 挖掘利用了小批量中的所有数据点，以便利用所有可用的信息。
- en: '– Batch-hard: Batch-hard triplet mining (Hermans et al., [2017](#bib.bib64))
    considers every point in the mini-batch as an anchor point. The hardest positive,
    which is the farthest point from the anchor point in the same class, is used as
    the positive point. The hardest negative, which is the closest point to the anchor
    point from another class, is used as the negative point:'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: '– Batch-hard: Batch-hard triplet mining (Hermans et al., [2017](#bib.bib64))
    将小批量中的每个点视为锚点。最难的正样本，即在同一类别中距离锚点最远的点，被用作正样本。最难的负样本，即与锚点来自其他类别且距离最近的点，被用作负样本：'
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\Big{[}\max_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (180) |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最小化}}\,\,\,\sum_{i=1}^{b}\Big{[}\max_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (180) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
  id: totrans-1038
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
- en: Bath-hard mining uses hardest points so that the network learns the hardest
    cases. By learning the hardest cases, other cases are expected to be learned properly.
    Learning the hardest cases can also be justified by the opposition-based learning
    (Tizhoosh, [2005](#bib.bib110)). Batch-hard mining has been used in many applications
    such as person re-identification (Wang et al., [2019](#bib.bib120)).
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: Batch-hard 挖掘使用最难的点，使得网络学习最难的案例。通过学习最难的案例，其他案例也会被适当学习。学习最难的案例也可以通过对抗学习（Tizhoosh,
    [2005](#bib.bib110)）来证明。Batch-hard 挖掘已被用于许多应用中，如人员再识别（Wang et al., [2019](#bib.bib120)）。
- en: '– Batch-semi-hard: Batch-semi-hard triplet mining (Schroff et al., [2015](#bib.bib100))
    considers every point in the mini-batch as an anchor point. All points in the
    mini-batch which are in the same class the anchor point are used as positive points.
    The hardest negative (closest to the anchor point from another class), which is
    farther than the positive point, is used as the negative point:'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: '– Batch-semi-hard: Batch-semi-hard triplet mining (Schroff et al., [2015](#bib.bib100))
    将小批量中的每个点视为锚点。所有在同一类别中的点被用作正样本。最难的负样本（与锚点来自其他类别且距离锚点较远）被用作负样本：'
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (181) |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最小化}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (181) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{\{}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}\,&#124;\,$
    |  |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{\{}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}\,&#124;\,$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}>d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}\big{\}}+m\Big{]}_{+}.$
    |  |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}>d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}\big{\}}+m\Big{]}_{+}.$
    |  |'
- en: '– Easy-positive: Easy-positive triplet mining (Xuan et al., [2020](#bib.bib130))
    considers every point in the mini-batch as an anchor point. The easiest positive
    (closest to the anchor point from the same class) is used as the positive point.
    All points in the mini-batch which are in a different class from the class of
    anchor point are used as negative points:'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: '– Easy-positive: Easy-positive triplet mining (Xuan et al., [2020](#bib.bib130))
    将小批量中的每个点视为锚点。最容易的正样本（与锚点在同一类别且距离最近）被用作正样本。所有来自不同类别的点被用作负样本：'
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (182) |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (182) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
- en: 'We can use this triplet mining approach in NCA loss function such as in Eq.
    ([170](#S5.E170 "In 5.3.9 Softmax Triplet Loss ‣ 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")). For example, we can have (Xuan et al., [2020](#bib.bib130)):'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以在NCA损失函数中使用这种三元组挖掘方法，例如在公式 ([170](#S5.E170 "在 5.3.9 Softmax 三元组损失 ‣ 5.3
    度量学习通过Siamese网络 ‣ 5 深度度量学习 ‣ 谱、概率和深度度量学习：教程和调查")) 中。举例来说，我们可以参考 (Xuan et al.,
    [2020](#bib.bib130)):'
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\bigg{(}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (183) |'
  id: totrans-1048
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\bigg{(}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (183) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}\times\Big{(}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  |'
  id: totrans-1049
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}\times\Big{(}\min_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\big{)}\Big{)}^{-1}\bigg{)},$
    |  |'
  id: totrans-1050
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\big{)}\Big{)}^{-1}\bigg{)},$
    |  |'
- en: where the embeddings for all points of the mini-batch are normalized to have
    length one.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 所有小批量点的嵌入都被归一化为长度为一。
- en: '– Lifted embedding loss: The lifted embedding loss (Oh Song et al., [2016](#bib.bib91))
    is related to the anchor-positive distance and the smallest (hardest) anchor-negative
    distance:'
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: – 提升的嵌入损失：提升的嵌入损失 (Oh Song et al., [2016](#bib.bib91)) 与锚点-正样本距离以及最小（最困难的）锚点-负样本距离有关：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}$ | $\displaystyle\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{(}\Big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))$
    |  | (184) |'
  id: totrans-1053
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}{}{}{}$ | $\displaystyle\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{(}\Big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))$
    |  | (184) |'
- en: '|  |  | $\displaystyle+\max\Big{(}\max_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{\{}m-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k}))\big{\}},$
    |  |'
  id: totrans-1054
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\max\Big{(}\max_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{\{}m-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k}))\big{\}},$
    |  |'
- en: '|  |  | $\displaystyle\max_{\boldsymbol{x}_{l}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{j})}}\big{\{}m-d(\textbf{f}(\boldsymbol{x}_{j}),\textbf{f}(\boldsymbol{x}_{l}))\big{\}}\Big{)}\Big{]}_{+}\Big{)}^{2},$
    |  |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\max_{\boldsymbol{x}_{l}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{j})}}\big{\{}m-d(\textbf{f}(\boldsymbol{x}_{j}),\textbf{f}(\boldsymbol{x}_{l}))\big{\}}\Big{)}\Big{]}_{+}\Big{)}^{2},$
    |  |'
- en: 'This loss is using triplet mining because of using extreme distances. Alternatively,
    another version of this loss function uses logarithm and exponential operators
    (Oh Song et al., [2016](#bib.bib91)):'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: '这种损失使用了三元组挖掘，因为它利用了极端距离。另一种版本的损失函数使用了对数和指数运算符 (Oh Song et al., [2016](#bib.bib91)):'
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}$ | $\displaystyle\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{(}\Big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))$
    |  | (185) |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}$ | $\displaystyle\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{(}\Big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))$
    |  | (185) |'
- en: '|  |  | $\displaystyle+\log\Big{(}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}m-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k}))\big{)},$
    |  |'
  id: totrans-1058
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\log\Big{(}\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\exp\big{(}m-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{k}))\big{)},$
    |  |'
- en: '|  |  | $\displaystyle\sum_{\boldsymbol{x}_{l}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{j})}}\exp\big{(}m-d(\textbf{f}(\boldsymbol{x}_{j}),\textbf{f}(\boldsymbol{x}_{l}))\big{)}\Big{)}\Big{]}_{+}\Big{)}^{2}.$
    |  |'
  id: totrans-1059
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sum_{\boldsymbol{x}_{l}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{j})}}\exp\big{(}m-d(\textbf{f}(\boldsymbol{x}_{j}),\textbf{f}(\boldsymbol{x}_{l}))\big{)}\Big{)}\Big{]}_{+}\Big{)}^{2}.$
    |  |'
- en: '– Hard mining center-triplet loss: Let the mini-batch contain data points from
    $c$ classes. Hard mining center–triplet loss (Lv et al., [2019](#bib.bib81)) considers
    the mean of every class as an anchor point. The hardest (farthest) positive point
    and the hardest (closest) negative point are used in this loss as (Lv et al.,
    [2019](#bib.bib81)):'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: – 硬挖掘中心三元组损失：设小批次包含来自 $c$ 类的数据点。硬挖掘中心三元组损失 (Lv et al., [2019](#bib.bib81)) 将每个类别的均值作为锚点。这个损失中使用了最困难（最远）的正点和最困难（最近）的负点
    (Lv et al., [2019](#bib.bib81))：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{l=1}^{c}\Big{[}\max_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\bar{\boldsymbol{x}}^{l})}}d\big{(}\textbf{f}(\bar{\boldsymbol{x}}^{l}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (186) |'
  id: totrans-1061
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最小化}}\,\,\,\sum_{l=1}^{c}\Big{[}\max_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\bar{\boldsymbol{x}}^{l})}}d\big{(}\textbf{f}(\bar{\boldsymbol{x}}^{l}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (186) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\bar{\boldsymbol{x}}^{l})}}d\big{(}\textbf{f}(\bar{\boldsymbol{x}}^{l}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-\min_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\bar{\boldsymbol{x}}^{l})}}d\big{(}\textbf{f}(\bar{\boldsymbol{x}}^{l}),\textbf{f}(\boldsymbol{x}_{k})\big{)}+m\Big{]}_{+}.$
    |  |'
- en: where $\bar{\boldsymbol{x}}^{l}$ denotes the mean of the $l$-th class.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{\boldsymbol{x}}^{l}$ 表示第 $l$ 类的均值。
- en: '– Triplet loss with cross-batch memory: A version of triplet loss can be (Wang
    et al., [2020a](#bib.bib121)):'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: – 三元组损失与跨批次记忆：一种三元组损失的版本可以参考 (Wang et al., [2020a](#bib.bib121))：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\bigg{(}-\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})$
    |  | (187) |'
  id: totrans-1065
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最小化}}\,\,\,\sum_{i=1}^{b}\bigg{(}-\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{j})$
    |  | (187) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\bigg{)}.$
    |  |'
  id: totrans-1066
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\boldsymbol{x}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\boldsymbol{x}_{k})\bigg{)}.$
    |  |'
- en: 'This triplet loss can use a cross-batch memory where we accumulate a few latest
    mini-batches. Every coming mini-batch updates the memory. Let the capacity of
    the memory be $w$ points and the mini-batch size be $b$. Let $\widetilde{\boldsymbol{x}}_{i}$
    denote the $i$-th data point in the memory. The triplet loss with cross-batch
    memory is defined as (Wang et al., [2020a](#bib.bib121)):'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: 这种三元组损失可以使用跨批次记忆，我们在其中累积最新的几个小批次。每个到来的小批次都会更新记忆。设记忆的容量为 $w$ 点，小批次的大小为 $b$。设
    $\widetilde{\boldsymbol{x}}_{i}$ 表示记忆中的第 $i$ 个数据点。具有跨批次记忆的三元组损失定义为 (Wang et al.,
    [2020a](#bib.bib121))：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\bigg{(}-\sum_{\widetilde{\boldsymbol{x}}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\widetilde{\boldsymbol{x}}_{j})$
    |  | (188) |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最小化}}\,\,\,\sum_{i=1}^{b}\bigg{(}-\sum_{\widetilde{\boldsymbol{x}}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\widetilde{\boldsymbol{x}}_{j})$
    |  | (188) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\widetilde{\boldsymbol{x}}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\widetilde{\boldsymbol{x}}_{k})\bigg{)},$
    |  |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\sum_{\widetilde{\boldsymbol{x}}_{k}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\textbf{f}(\boldsymbol{x}_{i})^{\top}\textbf{f}(\widetilde{\boldsymbol{x}}_{k})\bigg{)},$
    |  |'
- en: which takes the positive and negative points from the memory rather than from
    the coming mini-batch.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: 这从内存中选取正样本和负样本，而不是从即将到来的小批量中选取。
- en: 5.3.16 Triplet Sampling
  id: totrans-1071
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.16 三元组抽样
- en: 'Rather than using the extreme (hardest or easiest) positive and negative points
    (Sikaroudi et al., [2020a](#bib.bib102)), we can sample positive and negative
    points from the points in the mini-batch or from some distributions. There are
    several approaches for the positive and negative points to be sampled (Ghojogh,
    [2021](#bib.bib36)):'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用极端（最难或最容易）的正样本和负样本（Sikaroudi et al., [2020a](#bib.bib102)）不同，我们可以从小批量中的点或某些分布中抽取正样本和负样本。正样本和负样本的抽样方法有几种（Ghojogh,
    [2021](#bib.bib36)）：
- en: •
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sampled by extreme distances of points,
  id: totrans-1074
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过点的极端距离进行抽样，
- en: •
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sampled randomly from classes,
  id: totrans-1076
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从类别中随机抽样，
- en: •
  id: totrans-1077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sampled by distribution but from existing points,
  id: totrans-1078
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 按分布抽样但从现有点中抽样，
- en: •
  id: totrans-1079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sampled stochastically from distributions of classes.
  id: totrans-1080
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从类别分布中随机抽样。
- en: 'These approaches are used for triplet sampling. The first approach was introduced
    in Section [5.3.15](#S5.SS3.SSS15 "5.3.15 Triplet Mining ‣ 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"). The first, second, and third approaches
    sample the positive and negative points from the set of points in the mini-batch.
    This type of sampling is called survey sampling (Ghojogh et al., [2020e](#bib.bib44)).
    The third and fourth approaches sample points from distributions stochastically.
    In the following, we introduce some of the triplet sampling methods.'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: '这些方法用于三元组抽样。第一种方法在[5.3.15](#S5.SS3.SSS15 "5.3.15 Triplet Mining ‣ 5.3 Metric
    Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")节中介绍。第一、第二和第三种方法从小批量中的点集合中抽取正样本和负样本。这种抽样称为调查抽样（Ghojogh
    et al., [2020e](#bib.bib44)）。第三种和第四种方法从分布中随机抽样。以下，我们将介绍一些三元组抽样的方法。'
- en: '– Distance weighted sampling: Distance weighted sampling (Wu et al., [2017](#bib.bib125))
    is a method in the third approach, i.e., sampling by distribution but from existing
    points. The distribution of the pairwise distances is proportional to (Wu et al.,
    [2017](#bib.bib125)):'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: – 距离加权抽样：距离加权抽样（Wu et al., [2017](#bib.bib125)）是第三种方法，即按分布抽样，但从现有点中抽样。成对距离的分布与（Wu
    et al., [2017](#bib.bib125)）成正比：
- en: '|  | $\displaystyle\mathbb{P}\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}$
    | $\displaystyle\sim\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}^{p-2}\times$
    |  |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}$
    | $\displaystyle\sim\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}^{p-2}\times$
    |  |'
- en: '|  |  | $\displaystyle\Big{(}1-0.25\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}^{2}\Big{)}^{(b-3)/2},$
    |  |'
  id: totrans-1084
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\Big{(}1-0.25\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}^{2}\Big{)}^{(b-3)/2},$
    |  |'
- en: 'where $b$ is the number of points in the mini-batch and $p$ is the dimensionality
    of embedding space (i.e., the number of neurons in the last layer of the Siamese
    network). In every mini-batch, we consider every point once as an anchor point.
    For an anchor point, we consider all points of the mini-batch which are in a different
    class as candidates for the negative point. We sample a negative point, denoted
    by $\boldsymbol{x}_{*}^{n}$ from these candidates (Wu et al., [2017](#bib.bib125)):'
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ 是小批量中的点数，$p$ 是嵌入空间的维度（即 Siamese 网络最后一层的神经元数量）。在每个小批量中，我们将每个点视作一次锚点。对于一个锚点，我们将小批量中所有不同类别的点视作负样本候选点。从这些候选点中，我们随机抽取一个负样本，记作
    $\boldsymbol{x}_{*}^{n}$（Wu et al., [2017](#bib.bib125)）：
- en: '|  | $\displaystyle\boldsymbol{x}_{*}^{n}\sim\min\Big{(}\lambda,\mathbb{P}^{-1}\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}\Big{)},\quad\forall
    j\neq i,$ |  |'
  id: totrans-1086
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{x}_{*}^{n}\sim\min\Big{(}\lambda,\mathbb{P}^{-1}\big{(}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))\big{)}\Big{)},\quad\forall
    j\neq i,$ |  |'
- en: 'where $\lambda>0$ is a hyperparameter to ensure that all candidates have a
    chance to be chosen. This sampling is performed for every mini-batch. The loss
    function in distance weighted sampling is (Wu et al., [2017](#bib.bib125)):'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 是一个超参数，用于确保所有候选点都有被选择的机会。这种抽样在每个小批量中进行。距离加权抽样的损失函数是（Wu et al.,
    [2017](#bib.bib125)）：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (189) |'
  id: totrans-1088
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}\,\,\,\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\Big{[}d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j})\big{)}$
    |  | (189) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{*}^{n})\big{)}+m\Big{]}_{+}.$
    |  |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-d\big{(}\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{*}^{n})\big{)}+m\Big{]}_{+}.$
    |  |'
- en: '– Sampling by Bayesian updating theorem: We can sample triplets from distributions
    of classes which is the forth approach of sampling, mentioned above. One method
    for this sampling is using the Bayesian updating theorem (Sikaroudi et al., [2021](#bib.bib104))
    which is updating the posterior by the Bayes’ rule from some new data. In this
    method, we assume $p$-dimensional Gaussian distribution for every class in the
    embedding space where $p$ is the dimensionality of embedding space. We accumulate
    the embedded points for every class when the new mini-batches are introduced to
    the network. The distributions of classes are updated based on both the existing
    points available so far and the new-coming data points. It can be shown that the
    posterior of mean and covariance of a Gaussian distribution is a normal inverse
    Wishart distribution (Murphy, [2007](#bib.bib88)). The mean and covariance of
    a Gaussian distribution have a generalized Student-t distribution and inverse
    Wishart distribution, respectively (Murphy, [2007](#bib.bib88)). Let the so-far
    available data have sample size $n_{0}$, mean $\boldsymbol{\mu}^{0}$, and covariance
    $\boldsymbol{\Sigma}^{0}$. Also, let the newly coming data have sample size $n^{\prime}$,
    mean $\boldsymbol{\mu}^{\prime}$, and covariance $\boldsymbol{\Sigma}^{\prime}$.
    We update the mean and covariance by expectation of these distributions (Sikaroudi
    et al., [2021](#bib.bib104)):'
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: '-   通过贝叶斯更新定理进行采样：我们可以从类的分布中采样三元组，这也是上述的第四种采样方法。该采样的一种方法是使用贝叶斯更新定理（Sikaroudi
    等，[2021](#bib.bib104)），即通过贝叶斯规则从一些新数据中更新后验。在该方法中，我们假设在嵌入空间中每个类的分布为 $p$-维高斯分布，其中
    $p$ 是嵌入空间的维度。当新的小批量数据引入网络时，我们累积每个类的嵌入点。类的分布基于迄今为止存在的点和新来的数据点进行更新。可以证明，高斯分布的均值和协方差的后验分布是正态逆Wishart分布（Murphy，[2007](#bib.bib88)）。高斯分布的均值和协方差分别具有广义学生-t分布和逆Wishart分布（Murphy，[2007](#bib.bib88)）。假设现有数据的样本大小为
    $n_{0}$，均值为 $\boldsymbol{\mu}^{0}$，协方差为 $\boldsymbol{\Sigma}^{0}$。同时，假设新来的数据的样本大小为
    $n^{\prime}$，均值为 $\boldsymbol{\mu}^{\prime}$，协方差为 $\boldsymbol{\Sigma}^{\prime}$。我们通过这些分布的期望来更新均值和协方差（Sikaroudi
    等，[2021](#bib.bib104)）：'
- en: '|  | $\displaystyle\boldsymbol{\mu}^{0}\leftarrow\mathbb{E}(\boldsymbol{\mu}\,&#124;\,\boldsymbol{x}^{0})=\frac{n^{\prime}\boldsymbol{\mu}^{\prime}+n_{0}\boldsymbol{\mu}^{0}}{n^{\prime}+n_{0}},$
    |  |'
  id: totrans-1091
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\mu}^{0}\leftarrow\mathbb{E}(\boldsymbol{\mu}\,|\,\boldsymbol{x}^{0})=\frac{n^{\prime}\boldsymbol{\mu}^{\prime}+n_{0}\boldsymbol{\mu}^{0}}{n^{\prime}+n_{0}},$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{\Sigma}^{0}\leftarrow\mathbb{E}(\boldsymbol{\Sigma}\,&#124;\,\boldsymbol{x}^{0})=\frac{\boldsymbol{\Upsilon}^{-1}}{n^{\prime}\!+\!n_{0}\!-\!p\!-\!1},~{}~{}~{}\forall\,n^{\prime}\!+\!n_{0}\!>\!p\!+\!1,$
    |  |'
  id: totrans-1092
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\Sigma}^{0}\leftarrow\mathbb{E}(\boldsymbol{\Sigma}\,|\,\boldsymbol{x}^{0})=\frac{\boldsymbol{\Upsilon}^{-1}}{n^{\prime}\!+\!n_{0}\!-\!p\!-\!1},~{}~{}~{}\forall\,n^{\prime}\!+\!n_{0}\!>\!p\!+\!1,$
    |  |'
- en: 'where:'
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '|  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{\Upsilon}:=$ | $\displaystyle\,n^{\prime}\boldsymbol{\Sigma}^{\prime}+n_{0}\boldsymbol{\Sigma}^{0}$
    |  |'
  id: totrans-1094
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{R}^{d\times d}\ni\boldsymbol{\Upsilon}:=$ | $\displaystyle\,n^{\prime}\boldsymbol{\Sigma}^{\prime}+n_{0}\boldsymbol{\Sigma}^{0}$
    |  |'
- en: '|  |  | $\displaystyle+\frac{n^{\prime}_{1}n_{0}}{n^{\prime}_{1}+n_{0}}(\boldsymbol{\mu}^{0}-\boldsymbol{\mu}^{\prime})(\boldsymbol{\mu}^{0}-\boldsymbol{\mu}^{\prime})^{\top}.$
    |  |'
  id: totrans-1095
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\frac{n^{\prime}_{1}n_{0}}{n^{\prime}_{1}+n_{0}}(\boldsymbol{\mu}^{0}-\boldsymbol{\mu}^{\prime})(\boldsymbol{\mu}^{0}-\boldsymbol{\mu}^{\prime})^{\top}.$
    |  |'
- en: The updated mean and covariance are used for Gaussian distributions of the classes.
    Then, we sample triplets from the distributions of classes rather than from the
    points of mini-batch. We consider every point of the new mini-batch as an anchor
    point and sample a positive point from the distribution of the same class. We
    sample $c-1$ negative points from the distributions of $c-1$ other classes. If
    this triplet sampling procedure is used with triplet and contrastive loss functions,
    the approach is named Bayesian Updating with Triplet loss (BUT) and Bayesian Updating
    with NCA loss (BUNCA) (Sikaroudi et al., [2021](#bib.bib104)).
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的均值和协方差用于类别的高斯分布。然后，我们从类别的分布中采样三元组，而不是从小批量的数据点中采样。我们将新小批量的每个点视为锚点，并从同一类别的分布中采样一个正样本。我们从其他$c-1$个类别的分布中采样$c-1$个负样本。如果这种三元组采样过程与三元组和对比损失函数一起使用，该方法称为带有三元组损失的贝叶斯更新（BUT）和带有NCA损失的贝叶斯更新（BUNCA）（Sikaroudi等，[2021](#bib.bib104)）。
- en: '– Hard negative sampling: Let the anchor, positive, and negative points be
    denoted by $\boldsymbol{x}^{a}$, $\boldsymbol{x}^{p}$, and $\boldsymbol{x}^{n}$,
    respectively. Consider the following distributions for the negative and positive
    points (Robinson et al., [2021](#bib.bib95)):'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: – 硬负样本采样：令锚点、正样本和负样本分别表示为$\boldsymbol{x}^{a}$、$\boldsymbol{x}^{p}$和$\boldsymbol{x}^{n}$。考虑以下负样本和正样本的分布（Robinson等，[2021](#bib.bib95)）：
- en: '|  | $\displaystyle\mathbb{P}(\boldsymbol{x}^{n})\propto\alpha\mathbb{P}_{n}(\boldsymbol{x}^{n})+(1-\alpha)\mathbb{P}_{p}(\boldsymbol{x}^{n}),$
    |  |'
  id: totrans-1098
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}(\boldsymbol{x}^{n})\propto\alpha\mathbb{P}_{n}(\boldsymbol{x}^{n})+(1-\alpha)\mathbb{P}_{p}(\boldsymbol{x}^{n}),$
    |  |'
- en: '|  | $\displaystyle\mathbb{P}_{n}(\boldsymbol{x})\propto\exp\big{(}\beta\textbf{f}(\boldsymbol{x}^{a})^{\top}\textbf{f}(\boldsymbol{x})\big{)}\,\mathbb{P}(\boldsymbol{x}&#124;c(\boldsymbol{x})\neq
    c(\boldsymbol{x}^{a})),$ |  |'
  id: totrans-1099
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}_{n}(\boldsymbol{x})\propto\exp\big{(}\beta\textbf{f}(\boldsymbol{x}^{a})^{\top}\textbf{f}(\boldsymbol{x})\big{)}\,\mathbb{P}(\boldsymbol{x}&#124;c(\boldsymbol{x})\neq
    c(\boldsymbol{x}^{a})),$ |  |'
- en: '|  | $\displaystyle\mathbb{P}_{p}(\boldsymbol{x})\propto\exp\big{(}\beta\textbf{f}(\boldsymbol{x}^{a})^{\top}\textbf{f}(\boldsymbol{x})\big{)}\,\mathbb{P}(\boldsymbol{x}&#124;c(\boldsymbol{x})=c(\boldsymbol{x}^{a})),$
    |  |'
  id: totrans-1100
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}_{p}(\boldsymbol{x})\propto\exp\big{(}\beta\textbf{f}(\boldsymbol{x}^{a})^{\top}\textbf{f}(\boldsymbol{x})\big{)}\,\mathbb{P}(\boldsymbol{x}&#124;c(\boldsymbol{x})=c(\boldsymbol{x}^{a})),$
    |  |'
- en: 'where $\alpha\in(0,1)$ is a hyper-parameter. The loss function with hard negative
    sampling is (Robinson et al., [2021](#bib.bib95)):'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha\in(0,1)$是一个超参数。带有硬负样本采样的损失函数是（Robinson等，[2021](#bib.bib95)）：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\mathbb{E}_{\boldsymbol{x}^{p}\sim\mathbb{P}_{p}(\boldsymbol{x})}\log\bigg{(}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{p})\big{)}$
    |  | (190) |'
  id: totrans-1102
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}-\!\sum_{i=1}^{b}\mathbb{E}_{\boldsymbol{x}^{p}\sim\mathbb{P}_{p}(\boldsymbol{x})}\log\bigg{(}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{p})\big{)}$
    |  | (190) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\Big{(}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{p})\big{)}$
    |  |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}\Big{(}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{p})\big{)}$
    |  |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\mathbb{E}_{\boldsymbol{x}^{n}\sim\mathbb{P}(\boldsymbol{x}^{n})}\big{[}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{n})\big{)}\big{]}\Big{)}^{-1}\bigg{)},$
    |  |'
  id: totrans-1104
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}+\mathbb{E}_{\boldsymbol{x}^{n}\sim\mathbb{P}(\boldsymbol{x}^{n})}\big{[}\exp\big{(}\textbf{f}(\boldsymbol{x}_{i}^{a})^{\top}\textbf{f}(\boldsymbol{x}^{n})\big{)}\big{]}\Big{)}^{-1}\bigg{)},$
    |  |'
- en: where positive and negative points are sampled from positive and negative distributions
    defined above. The expectations can be estimated using the Monte Carlo approximation
    (Ghojogh et al., [2020e](#bib.bib44)). This time of triplet sampling is a method
    in the fourth type of triplet sampling, i.e., sampling stochastically from distributions
    of classes.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中正样本和负样本是从上述正负分布中采样的。期望值可以使用蒙特卡罗近似进行估计（Ghojogh等，[2020e](#bib.bib44)）。这种三元组采样方法属于第四类三元组采样，即从类别分布中随机采样。
- en: 5.4 Deep Discriminant Analysis Metric Learning
  id: totrans-1106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 深度判别分析度量学习
- en: 'Deep discriminant analysis metric learning methods use the idea of Fisher discriminant
    analysis (Fisher, [1936](#bib.bib33); Ghojogh et al., [2019b](#bib.bib39)) in
    deep learning, for learning an embedding space which separates classes. Some of
    these methods are deep probabilistic discriminant analysis (Li et al., [2019](#bib.bib79)),
    discriminant analysis with virtual samples (Kim & Song, [2021](#bib.bib74)), Fisher
    Siamese losses (Ghojogh et al., [2020f](#bib.bib45)), and deep Fisher discriminant
    analysis (Díaz-Vico et al., [2017](#bib.bib27); Díaz-Vico & Dorronsoro, [2019](#bib.bib26)).
    The Fisher Siamese losses were already introduced in Section [5.3.13](#S5.SS3.SSS13
    "5.3.13 Fisher Siamese Losses ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey").'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: '深度判别分析度量学习方法在深度学习中利用了Fisher判别分析（Fisher, [1936](#bib.bib33); Ghojogh et al.,
    [2019b](#bib.bib39)）的思想，用于学习一个能够区分类别的嵌入空间。这些方法包括深度概率判别分析（Li et al., [2019](#bib.bib79)）、虚拟样本判别分析（Kim
    & Song, [2021](#bib.bib74)）、Fisher Siamese 损失（Ghojogh et al., [2020f](#bib.bib45)）和深度
    Fisher 判别分析（Díaz-Vico et al., [2017](#bib.bib27); Díaz-Vico & Dorronsoro, [2019](#bib.bib26)）。Fisher
    Siamese 损失已在第[5.3.13](#S5.SS3.SSS13 "5.3.13 Fisher Siamese Losses ‣ 5.3 Metric
    Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")节中介绍。'
- en: 5.4.1 Deep Probabilistic Discriminant Analysis
  id: totrans-1108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 深度概率判别分析
- en: 'Deep probabilistic discriminant analysis (Li et al., [2019](#bib.bib79)) minimizes
    the inverse Fisher criterion:'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: 深度概率判别分析（Li et al., [2019](#bib.bib79)）最小化逆 Fisher 标准：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\frac{\mathbb{E}[\textbf{tr}(\text{cov}(\textbf{f}(\boldsymbol{x})&#124;y))]}{\textbf{tr}(\text{cov}(\mathbb{E}[\textbf{f}(\boldsymbol{x})&#124;y]))}=\frac{\sum_{i=1}^{b}\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]}{\sum_{i=1}^{b}\text{var}(\mathbb{E}[\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}])}$
    |  | (191) |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最小化}}~{}~{}~{}\frac{\mathbb{E}[\textbf{tr}(\text{cov}(\textbf{f}(\boldsymbol{x})&#124;y))]}{\textbf{tr}(\text{cov}(\mathbb{E}[\textbf{f}(\boldsymbol{x})&#124;y]))}=\frac{\sum_{i=1}^{b}\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]}{\sum_{i=1}^{b}\text{var}(\mathbb{E}[\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}])}$
    |  | (191) |'
- en: '|  |  | $\displaystyle\overset{(a)}{=}\frac{\sum_{i=1}^{b}\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]}{\sum_{i=1}^{b}\big{(}\text{var}(\textbf{f}(\boldsymbol{x}_{i}))-\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]\big{)}}$
    |  |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(a)}{=}\frac{\sum_{i=1}^{b}\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]}{\sum_{i=1}^{b}\big{(}\text{var}(\textbf{f}(\boldsymbol{x}_{i}))-\mathbb{E}[\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i})]\big{)}}$
    |  |'
- en: '|  |  | $\displaystyle\overset{(b)}{=}\frac{\sum_{i=1}^{b}\sum_{l=1}^{c}\mathbb{P}(y=l)\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}=l)}{\sum_{i=1}^{b}\big{(}\text{var}(\textbf{f}(\boldsymbol{x}_{i}))-\sum_{l=1}^{c}\mathbb{P}(y=l)\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}=l)\big{)}},$
    |  |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(b)}{=}\frac{\sum_{i=1}^{b}\sum_{l=1}^{c}\mathbb{P}(y=l)\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}=l)}{\sum_{i=1}^{b}\big{(}\text{var}(\textbf{f}(\boldsymbol{x}_{i}))-\sum_{l=1}^{c}\mathbb{P}(y=l)\text{var}(\textbf{f}(\boldsymbol{x}_{i})&#124;y_{i}=l)\big{)}},$
    |  |'
- en: where $b$ is the mini-batch size, $c$ is the number of classes, $y_{i}$ is the
    class label of $\boldsymbol{x}_{i}$, $\text{cov}(.)$ denotes covariance, $\text{var}(.)$
    denotes variance, $\mathbb{P}(y=l)$ is the prior of the $l$-th class (estimated
    by the ratio of class population to the total number of points in the mini-batch),
    $(a)$ is because of the law of total variance, and $(b)$ is because of the definition
    of expectation. The numerator and denominator represent the intra-class and inter-class
    variances, respectively.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ 是小批量大小，$c$ 是类别数，$y_{i}$ 是 $\boldsymbol{x}_{i}$ 的类别标签，$\text{cov}(.)$
    表示协方差，$\text{var}(.)$ 表示方差，$\mathbb{P}(y=l)$ 是第 $l$ 类的先验（通过类别人口与小批量中点的总数之比估计），$(a)$
    是由于总方差法则，$(b)$ 是由于期望定义。分子和分母分别表示类内和类间方差。
- en: 5.4.2 Discriminant Analysis with Virtual Samples
  id: totrans-1114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 虚拟样本判别分析
- en: In discriminant analysis metric learning with virtual samples (Kim & Song, [2021](#bib.bib74)),
    we consider any backbone network until the one-to-last layer of neural network
    and a last layer with linear activation function. Let the outputs of the one-to-last
    layer be denoted by $\{\textbf{f}^{\prime}(\boldsymbol{x}_{i})\}_{i=1}^{b}$ and
    the weights of the last layer be $\boldsymbol{U}$. We compute the intra-class
    scatter $\boldsymbol{S}_{W}$ and inter-class scatter $\boldsymbol{S}_{B}$ for
    the one-to-last layer’s features $\{\textbf{f}^{\prime}(\boldsymbol{x}_{i})\}_{i=1}^{b}$.
    If we see the last layer as a Fisher discriminant analysis model with projection
    matrix $\boldsymbol{U}$, the solution is the eigenvalue problem (Ghojogh et al.,
    [2019a](#bib.bib38)) for $\boldsymbol{S}_{W}^{-1}\boldsymbol{S}_{B}$. Let $\lambda_{j}$
    denote the $j$-th eigenvalue of this problem.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: 在带虚拟样本的判别分析度量学习中 (Kim & Song, [2021](#bib.bib74))，我们考虑任何主干网络直到神经网络的倒数第二层，以及一个具有线性激活函数的最后一层。设倒数第二层的输出为
    $\{\textbf{f}^{\prime}(\boldsymbol{x}_{i})\}_{i=1}^{b}$，最后一层的权重为 $\boldsymbol{U}$。我们计算倒数第二层特征
    $\{\textbf{f}^{\prime}(\boldsymbol{x}_{i})\}_{i=1}^{b}$ 的类内散布 $\boldsymbol{S}_{W}$
    和类间散布 $\boldsymbol{S}_{B}$。如果我们将最后一层视为具有投影矩阵 $\boldsymbol{U}$ 的 Fisher 判别分析模型，则解决方案是
    (Ghojogh et al., [2019a](#bib.bib38)) 对于 $\boldsymbol{S}_{W}^{-1}\boldsymbol{S}_{B}$
    的特征值问题。设 $\lambda_{j}$ 表示该问题的第 $j$ 个特征值。
- en: 'Assume $\mathcal{S}_{b}$ and $\mathcal{D}_{b}$ denote the similar and dissimilar
    points in the mini-batch where $|\mathcal{S}_{b}|=|\mathcal{D}_{b}|=q$. We define
    (Kim & Song, [2021](#bib.bib74)):'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: '假设 $\mathcal{S}_{b}$ 和 $\mathcal{D}_{b}$ 分别表示 mini-batch 中的相似点和不相似点，其中 $|\mathcal{S}_{b}|=|\mathcal{D}_{b}|=q$。我们定义
    (Kim & Song, [2021](#bib.bib74)):'
- en: '|  | $\displaystyle\boldsymbol{g}_{p}:=[\exp(-\textbf{f}^{\prime}(\boldsymbol{x}_{i})^{\top}\textbf{f}^{\prime}(\boldsymbol{x}_{j}))\,&#124;\,(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}_{b}]^{\top}\in\mathbb{R}^{q},$
    |  |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{g}_{p}:=[\exp(-\textbf{f}^{\prime}(\boldsymbol{x}_{i})^{\top}\textbf{f}^{\prime}(\boldsymbol{x}_{j}))\,\vert\,(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}_{b}]^{\top}\in\mathbb{R}^{q},$
    |  |'
- en: '|  | $\displaystyle\boldsymbol{g}_{n}:=[\exp(-\textbf{f}^{\prime}(\boldsymbol{x}_{i})^{\top}\textbf{f}^{\prime}(\boldsymbol{x}_{j}))\,&#124;\,(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}_{b}]^{\top}\in\mathbb{R}^{q},$
    |  |'
  id: totrans-1118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{g}_{n}:=[\exp(-\textbf{f}^{\prime}(\boldsymbol{x}_{i})^{\top}\textbf{f}^{\prime}(\boldsymbol{x}_{j}))\,\vert\,(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}_{b}]^{\top}\in\mathbb{R}^{q},$
    |  |'
- en: '|  | $\displaystyle s_{ctr}:=\frac{1}{2q}\sum_{i=1}^{q}\big{(}\boldsymbol{g}_{p}(i)+\boldsymbol{g}_{n}(i)\big{)},$
    |  |'
  id: totrans-1119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s_{ctr}:=\frac{1}{2q}\sum_{i=1}^{q}\big{(}\boldsymbol{g}_{p}(i)+\boldsymbol{g}_{n}(i)\big{)},$
    |  |'
- en: where $\boldsymbol{g}(i)$ is the $i$-th element of $\boldsymbol{g}$. We sample
    $q$ numbers, namely virtual samples, from the uniform distribution $U(s_{ctr}-\epsilon\bar{\lambda},s_{ctr}+\epsilon\bar{\lambda})$
    where $\epsilon$ is a small positive number and $\bar{\lambda}$ is the mean of
    eigenvalues $\lambda_{j}$’s. The $q$ virtual samples are put in a vector $\boldsymbol{r}\in\mathbb{R}^{q}$.
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{g}(i)$ 是 $\boldsymbol{g}$ 的第 $i$ 个元素。我们从均匀分布 $U(s_{ctr}-\epsilon\bar{\lambda},s_{ctr}+\epsilon\bar{\lambda})$
    中抽取 $q$ 个数，即虚拟样本，其中 $\epsilon$ 是一个小正数，$\bar{\lambda}$ 是特征值 $\lambda_{j}$ 的均值。这
    $q$ 个虚拟样本被放入向量 $\boldsymbol{r}\in\mathbb{R}^{q}$ 中。
- en: 'The loss function for discriminant analysis with virtual samples is (Kim &
    Song, [2021](#bib.bib74)):'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: '带虚拟样本的判别分析的损失函数为 (Kim & Song, [2021](#bib.bib74)):'
- en: '|  |  | $\displaystyle\underset{\theta,\boldsymbol{U}}{\text{minimize}}~{}~{}~{}\frac{1}{q}\sum_{i=1}^{q}\Big{[}\frac{1}{q}\,\boldsymbol{g}_{p}(i)\,\&#124;\boldsymbol{r}\&#124;_{1}-\frac{1}{q}\,\boldsymbol{g}_{n}(i)\,\&#124;\boldsymbol{r}\&#124;_{1}+m\Big{]}_{+}$
    |  | (192) |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta,\boldsymbol{U}}{\text{minimize}}~{}~{}~{}\frac{1}{q}\sum_{i=1}^{q}\Big{[}\frac{1}{q}\,\boldsymbol{g}_{p}(i)\,\vert\boldsymbol{r}\vert_{1}-\frac{1}{q}\,\boldsymbol{g}_{n}(i)\,\vert\boldsymbol{r}\vert_{1}+m\Big{]}_{+}$
    |  | (192) |'
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-10^{-5}\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})},$
    |  |'
  id: totrans-1123
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}~{}-10^{-5}\frac{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{B}\boldsymbol{U})}{\textbf{tr}(\boldsymbol{U}^{\top}\boldsymbol{S}_{W}\boldsymbol{U})},$
    |  |'
- en: where $\|.\|_{1}$ is the $\ell_{1}$ norm, $[.]_{+}:=\max(.,0)$, $m>0$ is the
    margin, and the second term is maximization of the Fisher criterion.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\|.\|_{1}$ 是 $\ell_{1}$ 范数，$[.]_{+}:=\max(.,0)$，$m>0$ 是边际，第二项是 Fisher 标准的最大化。
- en: 5.4.3 Deep Fisher Discriminant Analysis
  id: totrans-1125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3 深度 Fisher 判别分析
- en: 'It is shown in (Hart et al., [2000](#bib.bib61)) that the solution to the following
    least squares problem is equivalent to the solution of Fisher discriminant analysis:'
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: (Hart 等，[2000](#bib.bib61)) 显示，以下最小二乘问题的解等同于 Fisher 判别分析的解：
- en: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{0}\in\mathbb{R}^{c},\boldsymbol{W}\in\mathbb{R}^{d\times
    c}}{\text{minimize}}~{}~{}~{}\frac{1}{2}\&#124;\boldsymbol{Y}-\boldsymbol{1}_{n\times
    1}\boldsymbol{w}_{0}^{\top}-\boldsymbol{X}\boldsymbol{W}\&#124;_{F}^{2},$ |  |
    (193) |'
  id: totrans-1127
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\boldsymbol{w}_{0}\in\mathbb{R}^{c},\boldsymbol{W}\in\mathbb{R}^{d\times
    c}}{\text{最小化}}~{}~{}~{}\frac{1}{2}\&#124;\boldsymbol{Y}-\boldsymbol{1}_{n\times
    1}\boldsymbol{w}_{0}^{\top}-\boldsymbol{X}\boldsymbol{W}\&#124;_{F}^{2},$ |  |
    (193) |'
- en: where $\|.\|_{F}$ is the Frobenius norm, $\boldsymbol{X}\in\mathbb{R}^{n\times
    d}$ is the row-wise stack of data points, $\boldsymbol{Y}:=\boldsymbol{H}\boldsymbol{E}\boldsymbol{\Pi}^{-(1/2)}\in\mathbb{R}^{n\times
    c}$ where $\boldsymbol{H}:=\boldsymbol{I}-(1/n)\boldsymbol{1}\boldsymbol{1}^{\top}\in\mathbb{R}^{n\times
    n}$ is the centering matrix, $\boldsymbol{E}\in\{0,1\}^{n\times c}$ is the one-hot-encoded
    labels stacked row-wise, $\boldsymbol{\Pi}\in\mathbb{R}^{c\times c}$ is the diagonal
    matrix whose $(l,l)$-th element is the cardinality of the $l$-th class.
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\|.\|_{F}$ 是 Frobenius 范数，$\boldsymbol{X}\in\mathbb{R}^{n\times d}$ 是逐行堆叠的数据点，$\boldsymbol{Y}:=\boldsymbol{H}\boldsymbol{E}\boldsymbol{\Pi}^{-(1/2)}\in\mathbb{R}^{n\times
    c}$ 其中 $\boldsymbol{H}:=\boldsymbol{I}-(1/n)\boldsymbol{1}\boldsymbol{1}^{\top}\in\mathbb{R}^{n\times
    n}$ 是中心化矩阵，$\boldsymbol{E}\in\{0,1\}^{n\times c}$ 是逐行堆叠的独热编码标签，$\boldsymbol{\Pi}\in\mathbb{R}^{c\times
    c}$ 是对角矩阵，其 $(l,l)$-th 元素是第 $l$ 类的基数。
- en: 'Deep Fisher discriminant analysis (Díaz-Vico et al., [2017](#bib.bib27); Díaz-Vico
    & Dorronsoro, [2019](#bib.bib26)) implements Eq. ([193](#S5.E193 "In 5.4.3 Deep
    Fisher Discriminant Analysis ‣ 5.4 Deep Discriminant Analysis Metric Learning
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")) by a nonlinear neural network with loss function:'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Fisher 判别分析 (Díaz-Vico 等，[2017](#bib.bib27)；Díaz-Vico & Dorronsoro，[2019](#bib.bib26))
    通过一个具有损失函数的非线性神经网络实现公式 ([193](#S5.E193 "在 5.4.3 深度 Fisher 判别分析 ‣ 5.4 深度判别分析度量学习
    ‣ 5 深度度量学习 ‣ 光谱、概率和深度度量学习：教程和综述"))：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\frac{1}{2}\&#124;\boldsymbol{Y}-\textbf{f}(\boldsymbol{X};\theta)\&#124;_{F}^{2},$
    |  | (194) |'
  id: totrans-1130
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最小化}}~{}~{}~{}\frac{1}{2}\&#124;\boldsymbol{Y}-\textbf{f}(\boldsymbol{X};\theta)\&#124;_{F}^{2},$
    |  | (194) |'
- en: where $\theta$ is the weights of network, $\boldsymbol{X}\in\mathbb{R}^{n\times
    d}$ denotes the row-wise stack of points in the mini-batch of size $b$, $\boldsymbol{Y}:=\boldsymbol{H}\boldsymbol{E}\boldsymbol{\Pi}^{-(1/2)}\in\mathbb{R}^{b\times
    c}$ is computed in every mini-batch, and $\textbf{f}(.)\in\mathbb{R}^{b\times
    c}$ is the row-wise stack of output embeddings of the network. After training,
    the output $\textbf{f}(\boldsymbol{x})$ is the embedding for the input point $\boldsymbol{x}$.
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 是网络的权重，$\boldsymbol{X}\in\mathbb{R}^{n\times d}$ 表示大小为 $b$ 的迷你批次中的逐行堆叠点，$\boldsymbol{Y}:=\boldsymbol{H}\boldsymbol{E}\boldsymbol{\Pi}^{-(1/2)}\in\mathbb{R}^{b\times
    c}$ 在每个迷你批次中计算，$\textbf{f}(.)\in\mathbb{R}^{b\times c}$ 是网络的逐行堆叠输出嵌入。训练后，输出 $\textbf{f}(\boldsymbol{x})$
    是输入点 $\boldsymbol{x}$ 的嵌入。
- en: 5.5 Multi-Modal Deep Metric Learning
  id: totrans-1132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 多模态深度度量学习
- en: 'Data has several modals where a separate set of features is available for every
    modality of data. In other words, we can have several features for every data
    point. Note that the dimensionality of features may differ. Multi-modal deep metric
    learning (Roostaiyan et al., [2017](#bib.bib96)) addresses this problem in metric
    learning. Let $m$ denote the number of modalities. Consider $m$ stacked autoencoders
    each of which is for one of the modalities. The $l$-th autoencoder gets the $l$-th
    modality of the $i$-th data point, denoted by $\boldsymbol{x}_{i}^{l}$, and reconstructs
    it as output, denoted by $\widehat{\boldsymbol{x}}_{i}^{l}$. The embedding layer,
    or the layer between encoder and decoder, is shared between all $m$ autoencoders.
    We denote the output of this shared embedding layer by $\textbf{f}(\boldsymbol{x}_{i})$.
    The loss function for training the $m$ stacked autoencoders with the shared embedding
    layer can be (Roostaiyan et al., [2017](#bib.bib96)):'
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: 数据具有多种模态，每种数据模态都有一组单独的特征。换句话说，我们可以为每个数据点有多种特征。请注意，特征的维度可能不同。多模态深度度量学习（Roostaiyan等人，[2017](#bib.bib96)）解决了度量学习中的这一问题。设$m$表示模态数量。考虑到$m$个堆叠的自编码器，每个自编码器用于一个模态。第$l$个自编码器接收第$i$个数据点的第$l$个模态，表示为$\boldsymbol{x}_{i}^{l}$，并将其重构为输出$\widehat{\boldsymbol{x}}_{i}^{l}$。嵌入层，即编码器和解码器之间的层，被所有$m$个自编码器共享。我们用$\textbf{f}(\boldsymbol{x}_{i})$表示这个共享嵌入层的输出。训练具有共享嵌入层的$m$个堆叠自编码器的损失函数可以是（Roostaiyan等人，[2017](#bib.bib96)）：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}\sum_{l=1}^{m}\&#124;\boldsymbol{x}_{i}^{l}-\widehat{\boldsymbol{x}}_{i}^{l}\&#124;_{2}^{2}$
    |  | (195) |'
  id: totrans-1134
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{minimize}}~{}~{}~{}\sum_{i=1}^{b}\sum_{l=1}^{m}\&#124;\boldsymbol{x}_{i}^{l}-\widehat{\boldsymbol{x}}_{i}^{l}\&#124;_{2}^{2}$
    |  | （195）|'
- en: '|  |  | $\displaystyle+\lambda_{1}\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))-m_{1}\big{]}_{+}$
    |  |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda_{1}\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{[}d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))-m_{1}\big{]}_{+}$
    |  |'
- en: '|  |  | $\displaystyle+\lambda_{2}\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{[}-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))+m_{2}\big{]}_{+},$
    |  |'
  id: totrans-1136
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda_{2}\sum_{i=1}^{b}\sum_{\boldsymbol{x}_{j}\in\mathcal{X}\setminus\mathcal{X}_{c(\boldsymbol{x}_{i})}}\big{[}-d(\textbf{f}(\boldsymbol{x}_{i}),\textbf{f}(\boldsymbol{x}_{j}))+m_{2}\big{]}_{+},$
    |  |'
- en: where $\lambda_{1},\lambda_{2}>0$ are the regularization parameters and $m_{1},m_{2}>0$
    are the margins. The first term is the reconstruction loss and the second and
    third terms are for metric learning which collapses each class to a margin $m_{1}$
    and discriminates classes by a margin $m_{2}$. This loss function is optimized
    in a stacked autoencoder setup (Hinton & Salakhutdinov, [2006](#bib.bib67); Wang
    et al., [2014](#bib.bib119)). Then, it is fine-tuned by backpropagation (Ghojogh
    et al., [2021f](#bib.bib51)). After training, the embedding layer can be used
    for embedding data points. Note that another there exists another multi-modal
    deep metric learning, which is (Xu et al., [2019a](#bib.bib128)).
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\lambda_{1},\lambda_{2}>0$是正则化参数，$m_{1},m_{2}>0$是边界。第一项是重构损失，第二和第三项是度量学习，通过$m_{1}$将每个类别折叠起来，并通过$m_{2}$边界区分类别。这个损失函数在堆叠自编码器设置中优化（Hinton
    & Salakhutdinov，[2006](#bib.bib67); Wang等，[2014](#bib.bib119)）。然后，通过反向传播进行微调（Ghojogh等，[2021f](#bib.bib51)）。训练完成后，嵌入层可用于嵌入数据点。请注意，还存在另一种多模态深度度量学习，即（Xu等，[2019a](#bib.bib128)）。
- en: 5.6 Geometric Metric Learning by Neural Network
  id: totrans-1138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 几何度量学习的神经网络
- en: 'There exist some works, such as (Huang & Van Gool, [2017](#bib.bib70)), (Hauser,
    [2017](#bib.bib63)), and (Hajiabadi et al., [2019](#bib.bib59)), which have implemented
    neural networks on the Riemannian manifolds. Layered geometric learning (Hajiabadi
    et al., [2019](#bib.bib59)) implements Geometric Mean Metric Learning (GMML) (Zadeh
    et al., [2016](#bib.bib141)) (recall Section [3.7.1](#S3.SS7.SSS1 "3.7.1 Geometric
    Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    in a neural network framework. In this method, every layer of network is a metric
    layer which projects the output of its previous layer onto the subspace of its
    own metric (see Proposition [2](#Thmproposition2 "Proposition 2 (Projection in
    metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey") and Proposition [8](#S2.E8 "In Proposition 2 (Projection in metric
    learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")).'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: '一些研究，如 (Huang & Van Gool, [2017](#bib.bib70))、(Hauser, [2017](#bib.bib63))
    和 (Hajiabadi et al., [2019](#bib.bib59))，已经在黎曼流形上实现了神经网络。分层几何学习 (Hajiabadi et
    al., [2019](#bib.bib59)) 在神经网络框架中实现了几何均值度量学习 (GMML) (Zadeh et al., [2016](#bib.bib141))（见第[3.7.1节](#S3.SS7.SSS1
    "3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning
    ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")）。在这种方法中，网络的每一层都是一个度量层，将其前一层的输出投影到其自身度量的子空间上（见命题[2](#Thmproposition2
    "Proposition 2 (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis
    Distance ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")和命题[8](#S2.E8 "In Proposition 2
    (Projection in metric learning). ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")）。'
- en: For the $l$-th layer of network, we denote the weight matrix (i.e., the projection
    matrix of metric) and the output of layer for the $i$-th data point by $\boldsymbol{U}_{l}$
    and $\boldsymbol{x}_{i,l}$, respectively. Hence, the metric in the $l$-th layer
    models $\|\boldsymbol{x}_{i,l}-\boldsymbol{x}_{j,l}\|_{\boldsymbol{U}_{l}\boldsymbol{U}_{l}^{\top}}$.
    Consider the dataset of $n$ points $\boldsymbol{X}\in\mathbb{R}^{d\times n}$.
    We denote the output of the $l$-th layer by $\boldsymbol{X}_{l}\in\mathbb{R}^{d\times
    n}$. The projection of a layer onto its metric subspace is $\boldsymbol{X}_{l}=\boldsymbol{U}_{l}^{\top}\boldsymbol{X}_{l-1}$.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第 $l$ 层网络，我们用 $\boldsymbol{U}_{l}$ 表示权重矩阵（即度量的投影矩阵），用 $\boldsymbol{x}_{i,l}$
    表示第 $i$ 个数据点的层输出。因此，第 $l$ 层的度量模型为 $\|\boldsymbol{x}_{i,l}-\boldsymbol{x}_{j,l}\|_{\boldsymbol{U}_{l}\boldsymbol{U}_{l}^{\top}}$。考虑
    $n$ 个点的 dataset $\boldsymbol{X}\in\mathbb{R}^{d\times n}$。我们用 $\boldsymbol{X}_{l}\in\mathbb{R}^{d\times
    n}$ 表示第 $l$ 层的输出。将一层投影到其度量子空间上的结果是 $\boldsymbol{X}_{l}=\boldsymbol{U}_{l}^{\top}\boldsymbol{X}_{l-1}$。
- en: 'Every layer solves the optimization problem of GMML (Zadeh et al., [2016](#bib.bib141)),
    i.e., Eq. ([61](#S3.E61 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")). For this, we start from the
    first layer and proceed to the last layer by feed-propagation. The $l$-th layer
    computes $\boldsymbol{\Sigma}_{\mathcal{S}}$ and $\boldsymbol{\Sigma}_{\mathcal{D}}$
    for $\boldsymbol{X}_{l-1}$ by Eq. ([14](#S3.E14 "In Proof. ‣ 3.1.1 The First Spectral
    Method ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")). Then, the solution
    of optimization ([61](#S3.E61 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")) is computed which is the Eq.
    ([65](#S3.E65 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), i.e., $\boldsymbol{W}_{l}=\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}\big{(}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\big{)}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}$.
    Then, using Eq. ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance
    ‣ 2 Generalized Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")), we decompose the obtained $\boldsymbol{W}_{l}$
    to find $\boldsymbol{U}_{l}$. Then, data points are projected onto the metric
    subspace as $\boldsymbol{X}_{l}=\boldsymbol{U}_{l}^{\top}\boldsymbol{X}_{l-1}$.'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: '每一层都解决 GMML 的优化问题（Zadeh et al.，[2016](#bib.bib141)），即方程 ([61](#S3.E61 "In 3.7.1
    Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")）。为此，我们从第一层开始，按顺序进行前向传播直到最后一层。第 $l$ 层通过方程 ([14](#S3.E14 "In Proof.
    ‣ 3.1.1 The First Spectral Method ‣ 3.1 Spectral Methods Using Scatters ‣ 3 Spectral
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")) 计算 $\boldsymbol{\Sigma}_{\mathcal{S}}$ 和 $\boldsymbol{\Sigma}_{\mathcal{D}}$，用于
    $\boldsymbol{X}_{l-1}$。然后，计算优化问题的解 ([61](#S3.E61 "In 3.7.1 Geometric Mean Metric
    Learning ‣ 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))，即方程
    ([65](#S3.E65 "In 3.7.1 Geometric Mean Metric Learning ‣ 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey"))，即 $\boldsymbol{W}_{l}=\boldsymbol{\Sigma}_{\mathcal{S}}^{-1}\sharp_{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}=\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}\big{(}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{D}}\boldsymbol{\Sigma}_{\mathcal{S}}^{(1/2)}\big{)}^{(1/2)}\boldsymbol{\Sigma}_{\mathcal{S}}^{(-1/2)}$。然后，使用方程
    ([9](#S2.E9 "In Proof. ‣ 2.3 Generalized Mahalanobis Distance ‣ 2 Generalized
    Mahalanobis Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey"))，我们对获得的 $\boldsymbol{W}_{l}$ 进行分解，以找到 $\boldsymbol{U}_{l}$。然后，将数据点投影到度量子空间中，得到
    $\boldsymbol{X}_{l}=\boldsymbol{U}_{l}^{\top}\boldsymbol{X}_{l-1}$。'
- en: 'If we want the output of layers lie on the positive semi-definite manifold,
    the activation function of every layer can be projection onto the positive semi-definite
    cone (Ghojogh et al., [2021c](#bib.bib48)):'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望层的输出位于正半定流形上，则每层的激活函数可以投影到正半定锥上（Ghojogh et al.，[2021c](#bib.bib48)）：
- en: '|  | $\displaystyle\boldsymbol{X}_{l}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
  id: totrans-1143
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{X}_{l}:=\boldsymbol{V}\,\textbf{diag}(\max(\lambda_{1},0),\dots,\max(\lambda_{d},0))\,\boldsymbol{V}^{\top},$
    |  |'
- en: where $\boldsymbol{V}$ and $\{\lambda_{1},\dots,\lambda_{d}\}$ are the eigenvectors
    and eigenvalues of $\boldsymbol{X}_{l}$, respectively. This activation function
    is called the eigenvalue rectification layer in (Huang & Van Gool, [2017](#bib.bib70)).
    Finally, it is noteworthy that there is another work, named backprojection (Ghojogh
    et al., [2020d](#bib.bib43)), which has similar idea but in the Euclidean and
    Hilbert spaces and not in the Riemannian space.
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{V}$ 和 $\{\lambda_{1},\dots,\lambda_{d}\}$ 分别是 $\boldsymbol{X}_{l}$
    的特征向量和特征值。这个激活函数被称为特征值校正层（Huang & Van Gool，[2017](#bib.bib70)）。最后，值得注意的是，还有另一项工作，名为反向投影（Ghojogh
    et al.，[2020d](#bib.bib43)），其思想类似，但在欧几里得空间和希尔伯特空间中，而非黎曼空间中。
- en: 5.7 Few-shot Metric Learning
  id: totrans-1145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 少样本度量学习
- en: Few-shot learning refers to learning from a few data points rather than from
    a large enough dataset. Few-shot learning is used for domain generalization to
    be able to use for unseen data in the test phase (Wang et al., [2020b](#bib.bib122)).
    The training phase of few-shot learning is episodic where in every iteration or
    so-called episode of training, we have a support set and a query set. In other
    words, the training dataset is divided into mini-batches where every mini-batch
    contains a support set and a query set (Triantafillou et al., [2020](#bib.bib111)).
    Consider a training dataset with $c_{\text{tr}}$ classes and a test dataset with
    $c_{\text{te}}$ classes. As mentioned before, test and training datasets are usually
    disjoint in few-shot learning so it is useful for domain generalization. In every
    episode, also called the task or the mini-batch, we train using some (and not
    all) training classes by randomly sampling from classes.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习指的是从少量数据点中进行学习，而不是从足够大的数据集中进行学习。少样本学习用于领域泛化，以便在测试阶段能够处理未见过的数据（Wang et al.,
    [2020b](#bib.bib122)）。少样本学习的训练阶段是分回合的，在每次迭代或称为回合的训练中，我们都有一个支持集和一个查询集。换句话说，训练数据集被划分为小批量，每个小批量包含一个支持集和一个查询集（Triantafillou
    et al., [2020](#bib.bib111)）。考虑一个有 $c_{\text{tr}}$ 个类别的训练数据集和一个有 $c_{\text{te}}$
    个类别的测试数据集。如前所述，测试和训练数据集在少样本学习中通常是不同的，因此它对领域泛化很有用。在每个回合中，也称为任务或小批量，我们通过从类别中随机抽样来训练一些（而不是所有）训练类别。
- en: The support set is $\mathcal{S}_{s}:=\{(\boldsymbol{x}_{s,i},y_{s,i})\}_{i=1}^{|\mathcal{S}_{s}|}$
    where $\boldsymbol{x}$ and $y$ denote the data point and its label, respectively.
    The query set is $\mathcal{S}_{q}:=\{(\boldsymbol{x}_{q,i},y_{q,i})\}_{i=1}^{|\mathcal{S}_{q}|}$.
    The training data of every episode (mini-batch) is the union of the support and
    query sets. At every episode, we randomly sample $c_{s}$ classes out of the total
    $c_{\text{tr}}$ classes of training dataset, where we usually have $c_{s}\ll c_{\text{tr}}$.
    Then, we sample $k_{s}$ training data points from these $c_{s}$ selected classes.
    These $c_{s}\times k_{s}=|\mathcal{S}_{s}|$ form the support set. This few-shot
    setup is called $c_{s}$-way, $k_{s}$-shot in which the support set contains $c_{s}$
    classes and $k_{s}$ points in every class. The number of classes and every class’s
    points in the query set of every episode may or may not be the same as in the
    support set.
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 支持集是 $\mathcal{S}_{s}:=\{(\boldsymbol{x}_{s,i},y_{s,i})\}_{i=1}^{|\mathcal{S}_{s}|}$，其中
    $\boldsymbol{x}$ 和 $y$ 分别表示数据点及其标签。查询集是 $\mathcal{S}_{q}:=\{(\boldsymbol{x}_{q,i},y_{q,i})\}_{i=1}^{|\mathcal{S}_{q}|}$。每个训练回合（小批量）的训练数据是支持集和查询集的并集。在每个回合中，我们从总的
    $c_{\text{tr}}$ 个训练数据类别中随机选择 $c_{s}$ 个类别，通常有 $c_{s}\ll c_{\text{tr}}$。然后，我们从这些
    $c_{s}$ 个选定的类别中抽取 $k_{s}$ 个训练数据点。这些 $c_{s}\times k_{s}=|\mathcal{S}_{s}|$ 形成了支持集。这种少样本设置被称为
    $c_{s}$-way, $k_{s}$-shot，其中支持集包含 $c_{s}$ 个类别，每个类别有 $k_{s}$ 个点。每个回合的查询集的类别数和每个类别的点数可能与支持集不同。
- en: In every episode of the training phase of few-shot learning, we update the network
    weights by back-propagating error using the support set. These updated weights
    are not finalized yet. We feed the query set to the network with the updated weights
    and back-propagate error using the query set. This second back-propagation with
    the query set updates the weights of network finally at the end of episode. In
    other words, the query set is used to evaluate how good the update by support
    set are. This learning procedure for few-shot learning is called meta-learning
    (Finn et al., [2017](#bib.bib32)).
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 在少样本学习的每个训练回合中，我们通过使用支持集进行误差反向传播来更新网络权重。这些更新的权重尚未最终确定。我们将查询集输入到使用更新权重的网络中，并通过查询集进行误差反向传播。这种使用查询集的第二次反向传播最终在回合结束时更新网络的权重。换句话说，查询集用于评估支持集的更新效果。这种少样本学习的学习过程称为元学习（Finn
    et al., [2017](#bib.bib32)）。
- en: 'There are several family of methods for few-shot learning, one of which is
    some deep metric learning methods. Various metric learning methods have been proposed
    for learning from few-shot data. For example, Siamese network, introduced in Section
    [5.3](#S5.SS3 "5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"), has
    been used for few-shot learning (Koch et al., [2015](#bib.bib75); Li et al., [2020](#bib.bib80)).
    In the following, we introduce two metric learning methods for few-shot learning.'
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法家族用于少样本学习，其中之一是一些深度度量学习方法。已经提出了各种度量学习方法来从少样本数据中进行学习。例如，在第 [5.3](#S5.SS3
    "5.3 基于孪生网络的度量学习 ‣ 5 深度度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查") 节中介绍的孪生网络，已被用于少样本学习 (Koch
    et al., [2015](#bib.bib75); Li et al., [2020](#bib.bib80))。接下来，我们介绍两种用于少样本学习的度量学习方法。
- en: 5.7.1 Multi-scale Metric Learning
  id: totrans-1150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.1 多尺度度量学习
- en: 'Multi-scale metric learning (Jiang et al., [2020](#bib.bib71)) learns the embedding
    space by learning multiple scales of middle features in the training process.
    It has several steps. In the first step, we use a pre-trained network with multiple
    output layers which produce several different scales of features for both the
    support and query sets. In the second step, within every scale of support set,
    we take average of the $k_{s}$ features in every class. This gives us $c_{s}$
    features for every scale in the support set. This and the features of the query
    set are fed to the third step. In the third step, we feed every scale to a sub-network
    where larger scales are fed to sub-networks with more number of layers as they
    contain more information to process. These sub-networks are concatenated to give
    a scalar output for every data point with multiple scales of features. Hence,
    we obtain a scalar score for every data point in the support and query sets. Finally,
    a combination of a classification loss function, such as the cross-entropy loss
    (see Eq. ([153](#S5.E153 "In 5.2.4 Cross-entropy Loss ‣ 5.2 Supervised Metric
    Learning by Supervised Loss Functions ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey"))), and triplet loss (see Eq. [162](#S5.E162
    "In 5.3.5 Triplet Loss ‣ 5.3 Metric Learning by Siamese Networks ‣ 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey"))
    is used in the support-query setup explained before.'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度度量学习 (Jiang et al., [2020](#bib.bib71)) 通过在训练过程中学习多个尺度的中间特征来学习嵌入空间。它包含几个步骤。第一步，我们使用一个预训练网络，该网络具有多个输出层，为支持集和查询集生成几个不同尺度的特征。第二步，在每个尺度的支持集中，我们对每个类别中的
    $k_{s}$ 特征取平均。这为支持集中的每个尺度提供了 $c_{s}$ 特征。这些特征与查询集的特征一起输入到第三步。第三步，我们将每个尺度输入到一个子网络中，其中较大的尺度被输入到具有更多层的子网络，因为它们包含更多需要处理的信息。这些子网络被连接在一起，为每个数据点生成具有多个尺度特征的标量输出。因此，我们为支持集和查询集中的每个数据点获得一个标量分数。最后，使用组合的分类损失函数，如交叉熵损失（见
    Eq. ([153](#S5.E153 "在 5.2.4 交叉熵损失 ‣ 5.2 通过监督损失函数进行的监督度量学习 ‣ 5 深度度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查"))），以及三元组损失（见
    Eq. [162](#S5.E162 "在 5.3.5 三元组损失 ‣ 5.3 基于孪生网络的度量学习 ‣ 5 深度度量学习 ‣ 谱的、概率的和深度度量学习：教程和调查")）在前述的支持-查询设置中进行使用。
- en: 5.7.2 Metric Learning with Continuous Similarity Scores
  id: totrans-1152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.2 具有连续相似度分数的度量学习
- en: 'Another few-shot metric learning is (Xu et al., [2019b](#bib.bib129)) which
    takes pairs of data points as the input support and query sets. For the pair $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$,
    consider binary similarity score, $y_{ij}$, defined as:'
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个少样本度量学习方法是 (Xu et al., [2019b](#bib.bib129))，它将数据点对作为输入支持集和查询集。对于对 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$，考虑二进制相似度分数
    $y_{ij}$，定义如下：
- en: '|  | $\displaystyle y_{ij}:=\left\{\begin{array}[]{ll}1&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    0&amp;\mbox{if }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (198) |'
  id: totrans-1154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y_{ij}:=\left\{\begin{array}[]{ll}1&amp;\mbox{如果 }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\
    0&amp;\mbox{如果 }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$
    |  | (198) |'
- en: 'where $\mathcal{S}$ and $\mathcal{D}$ denote the sets of similar and dissimilar
    points, respectively. We can define continuous similarity score, $y^{\prime}_{ij}$,
    as (Xu et al., [2019b](#bib.bib129)):'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{S}$ 和 $\mathcal{D}$ 分别表示相似点和不相似点的集合。我们可以将连续相似度分数 $y^{\prime}_{ij}$
    定义为 (Xu et al., [2019b](#bib.bib129))：
- en: '|  | $\displaystyle y^{\prime}_{ij}:=\left\{\begin{array}[]{ll}(\beta-1)d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})+1&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\ -\alpha d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})+\alpha&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},\end{array}\right.$ |  |
    (201) |'
  id: totrans-1156
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{\prime}_{ij}:=\left\{\begin{array}[]{ll}(\beta-1)d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})+1&\text{如果
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\ -\alpha d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})+\alpha&\text{如果
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D},\end{array}\right.$ |  |
    (201) |'
- en: 'where $0<\alpha<\beta<1$ and $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$ is
    the normalized squared Euclidean distance (we normalize distances within every
    mini-batch). The ranges of these continuous similarities are:'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $0<\alpha<\beta<1$，且 $d(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$ 是归一化的平方欧几里得距离（我们在每个小批量内归一化距离）。这些连续相似度的范围为：
- en: '|  | $\displaystyle y^{\prime}_{ij}\in\left\{\begin{array}[]{ll}\,[\beta,1]&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\ \,[0,\alpha]&amp;\mbox{if
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$ |  |'
  id: totrans-1158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{\prime}_{ij}\in\left\{\begin{array}[]{ll}\,[\beta,1]&\text{如果
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{S}\\ \,[0,\alpha]&\text{如果
    }(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{D}.\end{array}\right.$ |  |'
- en: 'In every episode (mini-batch), the pairs are fed to a network with several
    feature vector outputs. For every pair $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$,
    these feature vectors are fed to another network which outputs a scalar similarity
    score $s_{ij}$. The loss function of metric learning in this method is (Xu et al.,
    [2019b](#bib.bib129)):'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阶段（小批量）中，将成对数据输入到具有多个特征向量输出的网络中。对于每对 $(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$，这些特征向量被输入到另一个网络，该网络输出一个标量相似度评分
    $s_{ij}$。该方法的度量学习损失函数为 (Xu et al., [2019b](#bib.bib129))：
- en: '|  |  | $\displaystyle\underset{\theta}{\text{maximize}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{X}}(1+\lambda)(s_{ij}-y^{\prime}_{ij})^{2},$
    |  | (202) |'
  id: totrans-1160
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\theta}{\text{最大化}}$ |  | $\displaystyle\sum_{(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\in\mathcal{X}}(1+\lambda)(s_{ij}-y^{\prime}_{ij})^{2},$
    |  | (202) |'
- en: '|  |  | subject to |  | $\displaystyle\beta\leq s_{ij},y^{\prime}_{ij}\leq
    1\quad\text{ if }\quad y_{ij}=1,$ |  |'
  id: totrans-1161
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 受限于 |  | $\displaystyle\beta\leq s_{ij},y^{\prime}_{ij}\leq 1\quad\text{
    如果 }\quad y_{ij}=1,$ |  |'
- en: '|  |  | $\displaystyle 0\leq s_{ij},y^{\prime}_{ij}\leq\alpha\quad\text{ if
    }\quad y_{ij}=0,$ |  |'
  id: totrans-1162
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle 0\leq s_{ij},y^{\prime}_{ij}\leq\alpha\quad\text{ 如果
    }\quad y_{ij}=0,$ |  |'
- en: where $\lambda>0$ is the regularization parameter and $\mathcal{X}$ is the mini-batch
    of the support or query set depending on whether it is the phase of support or
    query.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda>0$ 是正则化参数，而 $\mathcal{X}$ 是支持集或查询集的小批量，取决于当前是支持阶段还是查询阶段。
- en: 6 Conclusion
  id: totrans-1164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This was a tutorial and survey on spectral, probabilistic, and deep metric learning.
    We started with defining distance metric. In spectral methods, we covered methods
    using scatters of data, methods using Hinge loss, locally linear metric adaptation,
    kernel methods, geometric methods, and adversarial metric learning. In probabilistic
    category, we covered collapsing classes, neighborhood component analysis, Bayesian
    metric learning, information theoretic methods, and empirical risk minimization
    approaches. In deep learning methods, we explain reconstruction autoencoders,
    supervised loss functions, Siamese networks, deep discriminant analysis methods,
    multi-modal learning, geometric deep metric learning, and few-shot metric learning.
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于光谱、概率和深度度量学习的教程和综述。我们从定义距离度量开始。在光谱方法中，我们涵盖了使用数据散布的方法、使用 Hinge 损失的方法、局部线性度量适配、核方法、几何方法和对抗性度量学习。在概率类别中，我们涵盖了类别崩溃、邻域组件分析、贝叶斯度量学习、信息理论方法和经验风险最小化方法。在深度学习方法中，我们解释了重建自编码器、监督损失函数、Siamese
    网络、深度判别分析方法、多模态学习、几何深度度量学习和小样本度量学习。
- en: References
  id: totrans-1166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Absil et al. (2009) Absil, P-A, Mahony, Robert, and Sepulchre, Rodolphe. *Optimization
    algorithms on matrix manifolds*. Princeton University Press, 2009.
  id: totrans-1167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Absil 等（2009）Absil, P-A, Mahony, Robert, 和 Sepulchre, Rodolphe。*矩阵流形上的优化算法*。普林斯顿大学出版社，2009
    年。
- en: Alipanahi et al. (2008) Alipanahi, Babak, Biggs, Michael, and Ghodsi, Ali. Distance
    metric learning vs. Fisher discriminant analysis. In *Proceedings of the 23rd
    national conference on Artificial intelligence*, volume 2, pp.  598–603, 2008.
  id: totrans-1168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alipanahi 等（2008）Alipanahi, Babak, Biggs, Michael, 和 Ghodsi, Ali。距离度量学习与 Fisher
    判别分析。在 *第23届人工智能全国会议论文集*，第 2 卷，第 598–603 页，2008 年。
- en: Arsigny et al. (2007) Arsigny, Vincent, Fillard, Pierre, Pennec, Xavier, and
    Ayache, Nicholas. Geometric means in a novel vector space structure on symmetric
    positive-definite matrices. *SIAM journal on matrix analysis and applications*,
    29(1):328–347, 2007.
  id: totrans-1169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arsigny 等 (2007) Arsigny, Vincent, Fillard, Pierre, Pennec, Xavier, 和 Ayache,
    Nicholas. 在对称正定矩阵上的新型向量空间结构中的几何均值。*SIAM 矩阵分析与应用杂志*，29(1):328–347，2007年。
- en: Baghshah & Shouraki (2009) Baghshah, Mahdieh Soleymani and Shouraki, Saeed Bagheri.
    Semi-supervised metric learning using pairwise constraints. In *Twenty-First International
    Joint Conference on Artificial Intelligence*, 2009.
  id: totrans-1170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baghshah & Shouraki (2009) Baghshah, Mahdieh Soleymani 和 Shouraki, Saeed Bagheri.
    使用成对约束的半监督度量学习。见 *第21届国际人工智能联合会议*，2009年。
- en: Baghshah & Shouraki (2010) Baghshah, Mahdieh Soleymani and Shouraki, Saeed Bagheri.
    Kernel-based metric learning for semi-supervised clustering. *Neurocomputing*,
    73(7-9):1352–1361, 2010.
  id: totrans-1171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baghshah & Shouraki (2010) Baghshah, Mahdieh Soleymani 和 Shouraki, Saeed Bagheri.
    基于核的度量学习用于半监督聚类。*神经计算*，73(7-9):1352–1361，2010年。
- en: Bar-Hillel et al. (2003) Bar-Hillel, Aharon, Hertz, Tomer, Shental, Noam, and
    Weinshall, Daphna. Learning distance functions using equivalence relations. In
    *Proceedings of the 20th international conference on machine learning (ICML-03)*,
    pp.  11–18, 2003.
  id: totrans-1172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bar-Hillel 等 (2003) Bar-Hillel, Aharon, Hertz, Tomer, Shental, Noam, 和 Weinshall,
    Daphna. 使用等价关系学习距离函数。见 *第20届国际机器学习会议（ICML-03）论文集*，第11–18页，2003年。
- en: Bar-Hillel et al. (2005) Bar-Hillel, Aharon, Hertz, Tomer, Shental, Noam, Weinshall,
    Daphna, and Ridgeway, Greg. Learning a mahalanobis metric from equivalence constraints.
    *Journal of machine learning research*, 6(6), 2005.
  id: totrans-1173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bar-Hillel 等 (2005) Bar-Hillel, Aharon, Hertz, Tomer, Shental, Noam, Weinshall,
    Daphna, 和 Ridgeway, Greg. 从等价约束中学习马氏距离。*机器学习研究杂志*，6(6)，2005年。
- en: Belkin & Niyogi (2001) Belkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps
    and spectral techniques for embedding and clustering. In *Advances in neural information
    processing systems*, volume 14, pp.  585–591, 2001.
  id: totrans-1174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belkin & Niyogi (2001) Belkin, Mikhail 和 Niyogi, Partha. 拉普拉斯特征映射和用于嵌入与聚类的谱技术。见
    *神经信息处理系统进展*，第14卷，第585–591页，2001年。
- en: Belkin & Niyogi (2002) Belkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps
    and spectral techniques for embedding and clustering. In *Advances in neural information
    processing systems*, pp. 585–591, 2002.
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belkin & Niyogi (2002) Belkin, Mikhail 和 Niyogi, Partha. 拉普拉斯特征映射和用于嵌入与聚类的谱技术。见
    *神经信息处理系统进展*，第585–591页，2002年。
- en: Bellet et al. (2013) Bellet, Aurélien, Habrard, Amaury, and Sebban, Marc. A
    survey on metric learning for feature vectors and structured data. *arXiv preprint
    arXiv:1306.6709*, 2013.
  id: totrans-1176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellet 等 (2013) Bellet, Aurélien, Habrard, Amaury, 和 Sebban, Marc. 关于特征向量和结构化数据的度量学习综述。*arXiv
    预印本 arXiv:1306.6709*，2013年。
- en: Bellet et al. (2015) Bellet, Aurélien, Habrard, Amaury, and Sebban, Marc. Metric
    learning. *Synthesis Lectures on Artificial Intelligence and Machine Learning*,
    9(1):1–151, 2015.
  id: totrans-1177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellet 等 (2015) Bellet, Aurélien, Habrard, Amaury, 和 Sebban, Marc. 度量学习。*人工智能与机器学习综合讲座*，9(1):1–151，2015年。
- en: Bhatia (2007) Bhatia, Rajendra. *Positive definite matrices*. Princeton university
    press, 2007.
  id: totrans-1178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatia (2007) Bhatia, Rajendra. *正定矩阵*。普林斯顿大学出版社，2007年。
- en: Bhutani et al. (2018) Bhutani, Mukul, Jawanpuria, Pratik, Kasai, Hiroyuki, and
    Mishra, Bamdev. Low-rank geometric mean metric learning. *arXiv preprint arXiv:1806.05454*,
    2018.
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhutani 等 (2018) Bhutani, Mukul, Jawanpuria, Pratik, Kasai, Hiroyuki, 和 Mishra,
    Bamdev. 低秩几何均值度量学习。*arXiv 预印本 arXiv:1806.05454*，2018年。
- en: 'Boudiaf et al. (2020) Boudiaf, Malik, Rony, Jérôme, Ziko, Imtiaz Masud, Granger,
    Eric, Pedersoli, Marco, Piantanida, Pablo, and Ayed, Ismail Ben. A unifying mutual
    information view of metric learning: cross-entropy vs. pairwise losses. In *European
    Conference on Computer Vision*, pp.  548–564. Springer, 2020.'
  id: totrans-1180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boudiaf 等 (2020) Boudiaf, Malik, Rony, Jérôme, Ziko, Imtiaz Masud, Granger,
    Eric, Pedersoli, Marco, Piantanida, Pablo, 和 Ayed, Ismail Ben. 度量学习的统一互信息视角：交叉熵与成对损失。见
    *欧洲计算机视觉会议*，第548–564页。施普林格，2020年。
- en: Bromley et al. (1993) Bromley, Jane, Bentz, James W, Bottou, Léon, Guyon, Isabelle,
    LeCun, Yann, Moore, Cliff, Säckinger, Eduard, and Shah, Roopak. Signature verification
    using a “Siamese” time delay neural network. *International Journal of Pattern
    Recognition and Artificial Intelligence*, 7(04):669–688, 1993.
  id: totrans-1181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bromley 等 (1993) Bromley, Jane, Bentz, James W, Bottou, Léon, Guyon, Isabelle,
    LeCun, Yann, Moore, Cliff, Säckinger, Eduard, 和 Shah, Roopak. 使用“孪生”时延神经网络的签名验证。*模式识别与人工智能国际杂志*，7(04):669–688，1993年。
- en: Chang & Yeung (2004) Chang, Hong and Yeung, Dit-Yan. Locally linear metric adaptation
    for semi-supervised clustering. In *Proceedings of the twenty-first international
    conference on Machine learning*, pp.  20, 2004.
  id: totrans-1182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang & Yeung（2004）Chang, Hong 和 Yeung, Dit-Yan. 用于半监督聚类的局部线性度量适应。在 *第二十一届国际机器学习会议论文集*，第
    20 页，2004。
- en: Chen et al. (2018) Chen, Shuo, Gong, Chen, Yang, Jian, Li, Xiang, Wei, Yang,
    and Li, Jun. Adversarial metric learning. *arXiv preprint arXiv:1802.03170*, 2018.
  id: totrans-1183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2018）Chen, Shuo, Gong, Chen, Yang, Jian, Li, Xiang, Wei, Yang, 和 Li,
    Jun. 对抗性度量学习。*arXiv 预印本 arXiv:1802.03170*，2018。
- en: Chen et al. (2019) Chen, Shuo, Luo, Lei, Yang, Jian, Gong, Chen, Li, Jun, and
    Huang, Heng. Curvilinear distance metric learning. *Advances in Neural Information
    Processing Systems*, 32, 2019.
  id: totrans-1184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2019）Chen, Shuo, Luo, Lei, Yang, Jian, Gong, Chen, Li, Jun, 和 Huang,
    Heng. 曲线距离度量学习。*神经信息处理系统进展*，32, 2019。
- en: Chen et al. (2020) Chen, Ting, Kornblith, Simon, Norouzi, Mohammad, and Hinton,
    Geoffrey. A simple framework for contrastive learning of visual representations.
    In *International conference on machine learning*, pp. 1597–1607, 2020.
  id: totrans-1185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020）Chen, Ting, Kornblith, Simon, Norouzi, Mohammad, 和 Hinton, Geoffrey.
    一种简单的视觉表示对比学习框架。在 *国际机器学习会议*，第 1597–1607 页，2020。
- en: Cour et al. (2011) Cour, Timothee, Sapp, Ben, and Taskar, Ben. Learning from
    partial labels. *The Journal of Machine Learning Research*, 12:1501–1536, 2011.
  id: totrans-1186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cour 等（2011）Cour, Timothee, Sapp, Ben, 和 Taskar, Ben. 从部分标签中学习。*机器学习研究杂志*，12:1501–1536,
    2011。
- en: Cox & Cox (2008) Cox, Michael AA and Cox, Trevor F. Multidimensional scaling.
    In *Handbook of data visualization*, pp.  315–347\. Springer, 2008.
  id: totrans-1187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cox & Cox（2008）Cox, Michael AA 和 Cox, Trevor F. 多维尺度分析。在 *数据可视化手册*，第 315–347
    页。Springer，2008。
- en: Davis et al. (2007) Davis, Jason V, Kulis, Brian, Jain, Prateek, Sra, Suvrit,
    and Dhillon, Inderjit S. Information-theoretic metric learning. In *Proceedings
    of the 24th international conference on Machine learning*, pp.  209–216, 2007.
  id: totrans-1188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davis 等（2007）Davis, Jason V, Kulis, Brian, Jain, Prateek, Sra, Suvrit, 和 Dhillon,
    Inderjit S. 信息理论度量学习。在 *第24届国际机器学习会议论文集*，第 209–216 页，2007。
- en: De Maesschalck et al. (2000) De Maesschalck, Roy, Jouan-Rimbaud, Delphine, and
    Massart, Désiré L. The mahalanobis distance. *Chemometrics and intelligent laboratory
    systems*, 50(1):1–18, 2000.
  id: totrans-1189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Maesschalck 等（2000）De Maesschalck, Roy, Jouan-Rimbaud, Delphine, 和 Massart,
    Désiré L. 马氏距离。*化学计量学与智能实验室系统*，50(1):1–18, 2000。
- en: 'De Vazelhes et al. (2020) De Vazelhes, William, Carey, CJ, Tang, Yuan, Vauquier,
    Nathalie, and Bellet, Aurélien. metric-learn: Metric learning algorithms in Python.
    *Journal of Machine Learning Research*, 21:138–1, 2020.'
  id: totrans-1190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'De Vazelhes 等（2020）De Vazelhes, William, Carey, CJ, Tang, Yuan, Vauquier, Nathalie,
    和 Bellet, Aurélien. metric-learn: Python中的度量学习算法。*机器学习研究杂志*，21:138–1, 2020。'
- en: Dhillon (2007) Dhillon, JVDI. Differential entropic clustering of multivariate
    Gaussians. *Advances in Neural Information Processing Systems*, 19:337, 2007.
  id: totrans-1191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhillon（2007）Dhillon, JVDI. 多变量高斯分布的差分熵聚类。*神经信息处理系统进展*，19:337, 2007。
- en: Díaz-Vico & Dorronsoro (2019) Díaz-Vico, David and Dorronsoro, José R. Deep
    least squares Fisher discriminant analysis. *IEEE transactions on neural networks
    and learning systems*, 31(8):2752–2763, 2019.
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Díaz-Vico & Dorronsoro（2019）Díaz-Vico, David 和 Dorronsoro, José R. 深度最小二乘Fisher判别分析。*IEEE神经网络与学习系统学报*，31(8):2752–2763,
    2019。
- en: Díaz-Vico et al. (2017) Díaz-Vico, David, Omari, Adil, Torres-Barrán, Alberto,
    and Dorronsoro, José Ramón. Deep Fisher discriminant analysis. In *International
    Work-Conference on Artificial Neural Networks*, pp.  501–512\. Springer, 2017.
  id: totrans-1193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Díaz-Vico 等（2017）Díaz-Vico, David, Omari, Adil, Torres-Barrán, Alberto, 和 Dorronsoro,
    José Ramón. 深度Fisher判别分析。在 *国际人工神经网络工作会议*，第 501–512 页。Springer，2017。
- en: Ding et al. (2015) Ding, Shengyong, Lin, Liang, Wang, Guangrun, and Chao, Hongyang.
    Deep feature learning with relative distance comparison for person re-identification.
    *Pattern Recognition*, 48(10):2993–3003, 2015.
  id: totrans-1194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2015）Ding, Shengyong, Lin, Liang, Wang, Guangrun, 和 Chao, Hongyang. 用于行人再识别的深度特征学习与相对距离比较。*模式识别*，48(10):2993–3003,
    2015。
- en: Dong (2019) Dong, Minghzi. *Metric learning with Lipschitz continuous functions*.
    PhD thesis, UCL (University College London), 2019.
  id: totrans-1195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong（2019）Dong, Minghzi. *使用Lipschitz连续函数的度量学习*。博士论文，UCL（伦敦大学学院），2019。
- en: Duan et al. (2018) Duan, Yueqi, Zheng, Wenzhao, Lin, Xudong, Lu, Jiwen, and
    Zhou, Jie. Deep adversarial metric learning. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pp.  2780–2789, 2018.
  id: totrans-1196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等（2018）Duan, Yueqi, Zheng, Wenzhao, Lin, Xudong, Lu, Jiwen, 和 Zhou, Jie.
    深度对抗性度量学习。在 *IEEE计算机视觉与模式识别会议论文集*，第 2780–2789 页，2018。
- en: 'Feng et al. (2018) Feng, Lin, Wang, Huibing, Jin, Bo, Li, Haohao, Xue, Mingliang,
    and Wang, Le. Learning a distance metric by balancing KL-divergence for imbalanced
    datasets. *IEEE Transactions on Systems, Man, and Cybernetics: Systems*, 49(12):2384–2395,
    2018.'
  id: totrans-1197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng et al. (2018) Feng, Lin, Wang, Huibing, Jin, Bo, Li, Haohao, Xue, Mingliang,
    和 Wang, Le. 通过平衡KL散度来学习距离度量以应对不平衡数据集。*IEEE系统、人工智能与控制学报: 系统*, 49(12):2384–2395,
    2018。'
- en: Finn et al. (2017) Finn, Chelsea, Abbeel, Pieter, and Levine, Sergey. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *International Conference
    on Machine Learning*, pp. 1126–1135, 2017.
  id: totrans-1198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. (2017) Finn, Chelsea, Abbeel, Pieter, 和 Levine, Sergey. 模型无关的元学习用于快速适应深度网络。发表于*国际机器学习会议*,
    页码 1126–1135, 2017。
- en: Fisher (1936) Fisher, Ronald A. The use of multiple measurements in taxonomic
    problems. *Annals of eugenics*, 7(2):179–188, 1936.
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fisher (1936) Fisher, Ronald A. 多重测量在分类问题中的应用。*优生学年鉴*, 7(2):179–188, 1936。
- en: Gautheron et al. (2019) Gautheron, Léo, Habrard, Amaury, Morvant, Emilie, and
    Sebban, Marc. Metric learning from imbalanced data. In *2019 IEEE 31st International
    Conference on Tools with Artificial Intelligence (ICTAI)*, pp.  923–930\. IEEE,
    2019.
  id: totrans-1200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gautheron et al. (2019) Gautheron, Léo, Habrard, Amaury, Morvant, Emilie, 和
    Sebban, Marc. 从不平衡数据中学习度量。发表于*2019 IEEE第31届人工智能工具国际会议 (ICTAI)*, 页码 923–930。IEEE,
    2019。
- en: Ghodsi et al. (2007) Ghodsi, Ali, Wilkinson, Dana F, and Southey, Finnegan.
    Improving embeddings by flexible exploitation of side information. In *IJCAI*,
    pp.  810–816, 2007.
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghodsi et al. (2007) Ghodsi, Ali, Wilkinson, Dana F, 和 Southey, Finnegan. 通过灵活利用辅助信息来改善嵌入。发表于*IJCAI*,
    页码 810–816, 2007。
- en: Ghojogh (2021) Ghojogh, Benyamin. *Data Reduction Algorithms in Machine Learning
    and Data Science*. PhD thesis, University of Waterloo, 2021.
  id: totrans-1202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh (2021) Ghojogh, Benyamin. *机器学习和数据科学中的数据降维算法*。博士学位论文，滑铁卢大学，2021。
- en: 'Ghojogh & Crowley (2019) Ghojogh, Benyamin and Crowley, Mark. Unsupervised
    and supervised principal component analysis: Tutorial. *arXiv preprint arXiv:1906.03148*,
    2019.'
  id: totrans-1203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh & Crowley (2019) Ghojogh, Benyamin 和 Crowley, Mark. 无监督和监督主成分分析：教程。*arXiv预印本
    arXiv:1906.03148*, 2019。
- en: 'Ghojogh et al. (2019a) Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
    Eigenvalue and generalized eigenvalue problems: Tutorial. *arXiv preprint arXiv:1903.11240*,
    2019a.'
  id: totrans-1204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2019a) Ghojogh, Benyamin, Karray, Fakhri, 和 Crowley, Mark. 特征值和广义特征值问题：教程。*arXiv预印本
    arXiv:1903.11240*, 2019a。
- en: 'Ghojogh et al. (2019b) Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
    Fisher and kernel Fisher discriminant analysis: Tutorial. *arXiv preprint arXiv:1906.09436*,
    2019b.'
  id: totrans-1205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2019b) Ghojogh, Benyamin, Karray, Fakhri, 和 Crowley, Mark. Fisher和核Fisher判别分析：教程。*arXiv预印本
    arXiv:1906.09436*, 2019b。
- en: 'Ghojogh et al. (2020a) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Locally linear embedding and its variants: Tutorial and survey.
    *arXiv preprint arXiv:2011.10925*, 2020a.'
  id: totrans-1206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2020a) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. 局部线性嵌入及其变体：教程和调查。*arXiv预印本 arXiv:2011.10925*, 2020a。
- en: 'Ghojogh et al. (2020b) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Multidimensional scaling, Sammon mapping, and Isomap: Tutorial
    and survey. *arXiv preprint arXiv:2009.08136*, 2020b.'
  id: totrans-1207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2020b) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. 多维尺度分析、Sammon映射和Isomap：教程和调查。*arXiv预印本 arXiv:2009.08136*, 2020b。
- en: 'Ghojogh et al. (2020c) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Stochastic neighbor embedding with Gaussian and Student-t distributions:
    Tutorial and survey. *arXiv preprint arXiv:2009.10301*, 2020c.'
  id: totrans-1208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2020c) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. 使用高斯和Student-t分布的随机邻居嵌入：教程和调查。*arXiv预印本 arXiv:2009.10301*, 2020c。
- en: Ghojogh et al. (2020d) Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
    Backprojection for training feedforward neural networks in the input and feature
    spaces. In *International Conference on Image Analysis and Recognition*, pp. 
    16–24\. Springer, 2020d.
  id: totrans-1209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2020d) Ghojogh, Benyamin, Karray, Fakhri, 和 Crowley, Mark. 在输入和特征空间中训练前馈神经网络的反向投影。发表于*国际图像分析与识别会议*,
    页码 16–24。Springer, 2020d。
- en: 'Ghojogh et al. (2020e) Ghojogh, Benyamin, Nekoei, Hadi, Ghojogh, Aydin, Karray,
    Fakhri, and Crowley, Mark. Sampling algorithms, from survey sampling to Monte
    Carlo methods: Tutorial and literature review. *arXiv preprint arXiv:2011.00901*,
    2020e.'
  id: totrans-1210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2020e) Ghojogh, Benyamin, Nekoei, Hadi, Ghojogh, Aydin, Karray,
    Fakhri, 和 Crowley, Mark. 采样算法，从调查采样到Monte Carlo方法：教程和文献综述。*arXiv预印本 arXiv:2011.00901*,
    2020e。
- en: Ghojogh et al. (2020f) Ghojogh, Benyamin, Sikaroudi, Milad, Shafiei, Sobhan,
    Tizhoosh, Hamid R, Karray, Fakhri, and Crowley, Mark. Fisher discriminant triplet
    and contrastive losses for training Siamese networks. In *2020 international joint
    conference on neural networks (IJCNN)*, pp.  1–7\. IEEE, 2020f.
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2020f) Ghojogh, Benyamin, Sikaroudi, Milad, Shafiei, Sobhan,
    Tizhoosh, Hamid R, Karray, Fakhri, 和 Crowley, Mark. Fisher 判别三元组和对比损失用于训练孪生网络。收录于*2020
    国际联合神经网络会议 (IJCNN)*，第 1–7 页。IEEE，2020f。
- en: 'Ghojogh et al. (2021a) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Factor analysis, probabilistic principal component analysis, variational
    inference, and variational autoencoder: Tutorial and survey. *arXiv preprint arXiv:2101.00734*,
    2021a.'
  id: totrans-1212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2021a) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. 因子分析、概率主成分分析、变分推断和变分自编码器：教程和调查。*arXiv 预印本 arXiv:2101.00734*，2021a。
- en: 'Ghojogh et al. (2021b) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Generative adversarial networks and adversarial autoencoders: Tutorial
    and survey. *arXiv preprint arXiv:2111.13282*, 2021b.'
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2021b) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. 生成对抗网络和对抗自编码器：教程和调查。*arXiv 预印本 arXiv:2111.13282*，2021b。
- en: 'Ghojogh et al. (2021c) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. KKT conditions, first-order and second-order optimization, and
    distributed optimization: Tutorial and survey. *arXiv preprint arXiv:2110.01858*,
    2021c.'
  id: totrans-1214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2021c) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. KKT 条件、一级和二级优化以及分布式优化：教程和调查。*arXiv 预印本 arXiv:2110.01858*，2021c。
- en: 'Ghojogh et al. (2021d) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Laplacian-based dimensionality reduction including spectral clustering,
    Laplacian eigenmap, locality preserving projection, graph embedding, and diffusion
    map: Tutorial and survey. *arXiv preprint arXiv:2106.02154*, 2021d.'
  id: totrans-1215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2021d) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. 基于拉普拉斯的降维，包括谱聚类、拉普拉斯特征映射、局部保持投影、图嵌入和扩散映射：教程和调查。*arXiv 预印本 arXiv:2106.02154*，2021d。
- en: 'Ghojogh et al. (2021e) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Reproducing kernel Hilbert space, Mercer’s theorem, eigenfunctions,
    Nyström method, and use of kernels in machine learning: Tutorial and survey. *arXiv
    preprint arXiv:2106.08443*, 2021e.'
  id: totrans-1216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2021e) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. 再生核希尔伯特空间、Mercer 定理、特征函数、Nyström 方法及其在机器学习中的应用：教程和调查。*arXiv 预印本 arXiv:2106.08443*，2021e。
- en: 'Ghojogh et al. (2021f) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
    Crowley, Mark. Restricted Boltzmann machine and deep belief network: Tutorial
    and survey. *arXiv preprint arXiv:2107.12521*, 2021f.'
  id: totrans-1217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghojogh et al. (2021f) Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, 和 Crowley,
    Mark. 限制玻尔兹曼机和深度信念网络：教程和调查。*arXiv 预印本 arXiv:2107.12521*，2021f。
- en: Globerson & Roweis (2005) Globerson, Amir and Roweis, Sam. Metric learning by
    collapsing classes. *Advances in neural information processing systems*, 18:451–458,
    2005.
  id: totrans-1218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Globerson & Roweis (2005) Globerson, Amir 和 Roweis, Sam. 通过类折叠进行度量学习。*神经信息处理系统进展*，18:451–458，2005。
- en: Goldberger et al. (2005) Goldberger, Jacob, Hinton, Geoffrey E, Roweis, Sam T,
    and Salakhutdinov, Ruslan R. Neighbourhood components analysis. In *Advances in
    neural information processing systems*, pp. 513–520, 2005.
  id: totrans-1219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldberger et al. (2005) Goldberger, Jacob, Hinton, Geoffrey E, Roweis, Sam
    T, 和 Salakhutdinov, Ruslan R. 邻域组件分析。收录于*神经信息处理系统进展*，第 513–520 页，2005。
- en: Goodfellow et al. (2014) Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi,
    Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua.
    Generative adversarial nets. *Advances in neural information processing systems*,
    27, 2014.
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi,
    Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, 和 Bengio, Yoshua.
    生成对抗网络。*神经信息处理系统进展*，27，2014。
- en: Goodfellow et al. (2016) Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron.
    *Deep learning*. MIT press, 2016.
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2016) Goodfellow, Ian, Bengio, Yoshua, 和 Courville, Aaron.
    *深度学习*。MIT 出版社，2016。
- en: Gretton et al. (2005) Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and Schölkopf,
    Bernhard. Measuring statistical dependence with Hilbert-Schmidt norms. In *International
    conference on algorithmic learning theory*, pp.  63–77\. Springer, 2005.
  id: totrans-1222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gretton et al. (2005) Gretton, Arthur, Bousquet, Olivier, Smola, Alex, 和 Schölkopf,
    Bernhard. 使用希尔伯特-施密特范数测量统计依赖。收录于*国际算法学习理论会议*，第 63–77 页。Springer，2005。
- en: Guillaumin et al. (2009) Guillaumin, Matthieu, Verbeek, Jakob, and Schmid, Cordelia.
    Is that you? metric learning approaches for face identification. In *2009 IEEE
    12th international conference on computer vision*, pp.  498–505\. IEEE, 2009.
  id: totrans-1223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guillaumin et al. (2009) Guillaumin, Matthieu, Verbeek, Jakob, 和 Schmid, Cordelia.
    那是你吗？面部识别的度量学习方法。收录于*2009 IEEE 12th international conference on computer vision*，第498–505页。IEEE，2009年。
- en: Hadsell et al. (2006) Hadsell, Raia, Chopra, Sumit, and LeCun, Yann. Dimensionality
    reduction by learning an invariant mapping. In *2006 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition (CVPR’06)*, volume 2, pp.  1735–1742\.
    IEEE, 2006.
  id: totrans-1224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadsell et al. (2006) Hadsell, Raia, Chopra, Sumit, 和 LeCun, Yann. 通过学习不变映射进行降维。收录于*2006
    IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)*，第2卷，第1735–1742页。IEEE，2006年。
- en: Hajiabadi et al. (2019) Hajiabadi, Hamideh, Godaz, Reza, Ghasemi, Morteza, and
    Monsefi, Reza. Layered geometric learning. In *International Conference on Artificial
    Intelligence and Soft Computing*, pp.  571–582\. Springer, 2019.
  id: totrans-1225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hajiabadi et al. (2019) Hajiabadi, Hamideh, Godaz, Reza, Ghasemi, Morteza, 和
    Monsefi, Reza. 分层几何学习。收录于*International Conference on Artificial Intelligence
    and Soft Computing*，第571–582页。Springer，2019年。
- en: 'Harandi et al. (2017) Harandi, Mehrtash, Salzmann, Mathieu, and Hartley, Richard.
    Joint dimensionality reduction and metric learning: A geometric take. In *International
    Conference on Machine Learning*, pp. 1404–1413\. PMLR, 2017.'
  id: totrans-1226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harandi et al. (2017) Harandi, Mehrtash, Salzmann, Mathieu, 和 Hartley, Richard.
    共同降维和度量学习：几何视角。收录于*International Conference on Machine Learning*，第1404–1413页。PMLR，2017年。
- en: Hart et al. (2000) Hart, Peter E, Stork, David G, and Duda, Richard O. *Pattern
    classification*. Wiley Hoboken, 2000.
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hart et al. (2000) Hart, Peter E, Stork, David G, 和 Duda, Richard O. *Pattern
    classification*。Wiley Hoboken，2000年。
- en: Hauberg et al. (2012) Hauberg, Søren, Freifeld, Oren, and Black, Michael J.
    A geometric take on metric learning. In *Advances in neural information processing
    systems*, volume 25, pp.  2033–2041, 2012.
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hauberg et al. (2012) Hauberg, Søren, Freifeld, Oren, 和 Black, Michael J. 对度量学习的几何视角。收录于*Advances
    in neural information processing systems*，第25卷，第2033–2041页，2012年。
- en: Hauser (2017) Hauser, Michael B. Principles of Riemannian geometry in neural
    networks. In *Advances in neural information processing systems*, pp. 2807––2816,
    2017.
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hauser (2017) Hauser, Michael B. 神经网络中的黎曼几何原理。收录于*Advances in neural information
    processing systems*，第2807–2816页，2017年。
- en: Hermans et al. (2017) Hermans, Alexander, Beyer, Lucas, and Leibe, Bastian.
    In defense of the triplet loss for person re-identification. *arXiv preprint arXiv:1703.07737*,
    2017.
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hermans et al. (2017) Hermans, Alexander, Beyer, Lucas, 和 Leibe, Bastian. 为行人再识别辩护三元组损失。*arXiv
    preprint arXiv:1703.07737*，2017年。
- en: Hinton et al. (2014) Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling
    the knowledge in a neural network. In *NIPS 2014 Deep Learning Workshop*, 2014.
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2014) Hinton, Geoffrey, Vinyals, Oriol, 和 Dean, Jeff. 提炼神经网络中的知识。收录于*NIPS
    2014 Deep Learning Workshop*，2014年。
- en: Hinton & Roweis (2003) Hinton, Geoffrey E and Roweis, Sam T. Stochastic neighbor
    embedding. In *Advances in neural information processing systems*, pp. 857–864,
    2003.
  id: totrans-1232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton & Roweis (2003) Hinton, Geoffrey E 和 Roweis, Sam T. 随机邻域嵌入。收录于*Advances
    in neural information processing systems*，第857–864页，2003年。
- en: Hinton & Salakhutdinov (2006) Hinton, Geoffrey E and Salakhutdinov, Ruslan R.
    Reducing the dimensionality of data with neural networks. *Science*, 313(5786):504–507,
    2006.
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton & Salakhutdinov (2006) Hinton, Geoffrey E 和 Salakhutdinov, Ruslan R.
    使用神经网络降低数据的维度。*Science*，313(5786):504–507，2006年。
- en: Hoffer & Ailon (2015) Hoffer, Elad and Ailon, Nir. Deep metric learning using
    triplet network. In *International workshop on similarity-based pattern recognition*,
    pp.  84–92\. Springer, 2015.
  id: totrans-1234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffer & Ailon (2015) Hoffer, Elad 和 Ailon, Nir. 使用三元组网络的深度度量学习。收录于*International
    workshop on similarity-based pattern recognition*，第84–92页。Springer，2015年。
- en: Hoi et al. (2006) Hoi, Steven CH, Liu, Wei, Lyu, Michael R, and Ma, Wei-Ying.
    Learning distance metrics with contextual constraints for image retrieval. In
    *2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    (CVPR’06)*, volume 2, pp.  2072–2078\. IEEE, 2006.
  id: totrans-1235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoi et al. (2006) Hoi, Steven CH, Liu, Wei, Lyu, Michael R, 和 Ma, Wei-Ying.
    具有上下文约束的距离度量学习用于图像检索。收录于*2006 IEEE Computer Society Conference on Computer Vision
    and Pattern Recognition (CVPR’06)*，第2卷，第2072–2078页。IEEE，2006年。
- en: Huang & Van Gool (2017) Huang, Zhiwu and Van Gool, Luc. A Riemannian network
    for SPD matrix learning. In *Thirty-First AAAI Conference on Artificial Intelligence*,
    2017.
  id: totrans-1236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang & Van Gool (2017) Huang, Zhiwu 和 Van Gool, Luc. 用于 SPD 矩阵学习的黎曼网络。收录于*Thirty-First
    AAAI Conference on Artificial Intelligence*，2017年。
- en: Jiang et al. (2020) Jiang, Wen, Huang, Kai, Geng, Jie, and Deng, Xinyang. Multi-scale
    metric learning for few-shot learning. *IEEE Transactions on Circuits and Systems
    for Video Technology*, 31(3):1091–1102, 2020.
  id: totrans-1237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2020) Jiang, Wen, Huang, Kai, Geng, Jie, and Deng, Xinyang. 多尺度度量学习用于少样本学习。*IEEE
    Transactions on Circuits and Systems for Video Technology*, 31(3):1091–1102, 2020。
- en: 'Kaya & Bilge (2019) Kaya, Mahmut and Bilge, Hasan Şakir. Deep metric learning:
    A survey. *Symmetry*, 11(9):1066, 2019.'
  id: totrans-1238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaya & Bilge（2019）Kaya, Mahmut 和 Bilge, Hasan Şakir. 深度度量学习：综述。*对称性*，11(9):1066，2019年。
- en: Khodadadeh et al. (2019) Khodadadeh, Siavash, Bölöni, Ladislau, and Shah, Mubarak.
    Unsupervised meta-learning for few-shot image classification. In *Advances in
    neural information processing systems*, 2019.
  id: totrans-1239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khodadadeh 等（2019）Khodadadeh, Siavash, Bölöni, Ladislau, 和 Shah, Mubarak. 用于少样本图像分类的无监督元学习。在*神经信息处理系统进展*，2019年。
- en: Kim & Song (2021) Kim, Dae Ha and Song, Byung Cheol. Virtual sample-based deep
    metric learning using discriminant analysis. *Pattern Recognition*, 110:107643,
    2021.
  id: totrans-1240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim & Song（2021）Kim, Dae Ha 和 Song, Byung Cheol. 基于虚拟样本的深度度量学习，采用判别分析。*模式识别*，110:107643，2021年。
- en: Koch et al. (2015) Koch, Gregory, Zemel, Richard, Salakhutdinov, Ruslan, et al.
    Siamese neural networks for one-shot image recognition. In *ICML deep learning
    workshop*, volume 2\. Lille, 2015.
  id: totrans-1241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koch 等（2015）Koch, Gregory, Zemel, Richard, Salakhutdinov, Ruslan, 等. 用于单次图像识别的Siamese神经网络。在*ICML深度学习研讨会*，第2卷。里尔，2015年。
- en: 'Kulis (2013) Kulis, Brian. Metric learning: A survey. *Foundations and Trends®
    in Machine Learning*, 5(4):287–364, 2013.'
  id: totrans-1242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulis（2013）Kulis, Brian. 度量学习：综述。*机器学习基础与趋势®*，5(4):287–364，2013年。
- en: Kumar BG et al. (2016) Kumar BG, Vijay, Carneiro, Gustavo, and Reid, Ian. Learning
    local image descriptors with deep Siamese and triplet convolutional networks by
    minimising global loss functions. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, pp.  5385–5394, 2016.
  id: totrans-1243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar BG 等（2016）Kumar BG, Vijay, Carneiro, Gustavo, 和 Reid, Ian. 通过最小化全局损失函数学习局部图像描述符，利用深度
    Siamese 和三元组卷积网络。在*IEEE计算机视觉与模式识别会议论文集*，第5385–5394页，2016年。
- en: Leyva-Vallina et al. (2021) Leyva-Vallina, María, Strisciuglio, Nicola, and
    Petkov, Nicolai. Generalized contrastive optimization of Siamese networks for
    place recognition. *arXiv preprint arXiv:2103.06638*, 2021.
  id: totrans-1244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leyva-Vallina 等（2021）Leyva-Vallina, María, Strisciuglio, Nicola, 和 Petkov, Nicolai.
    用于位置识别的Siamese网络的广义对比优化。*arXiv预印本 arXiv:2103.06638*，2021年。
- en: Li et al. (2019) Li, Li, Doroslovački, Miloš, and Loew, Murray H. Discriminant
    analysis deep neural networks. In *2019 53rd annual conference on information
    sciences and systems (CISS)*, pp.  1–6\. IEEE, 2019.
  id: totrans-1245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019）Li, Li, Doroslovački, Miloš, 和 Loew, Murray H. 判别分析深度神经网络。在*2019年第53届信息科学与系统年会（CISS）*，第1–6页。IEEE，2019年。
- en: Li et al. (2020) Li, Xiaomeng, Yu, Lequan, Fu, Chi-Wing, Fang, Meng, and Heng,
    Pheng-Ann. Revisiting metric learning for few-shot image classification. *Neurocomputing*,
    406:49–58, 2020.
  id: totrans-1246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Li, Xiaomeng, Yu, Lequan, Fu, Chi-Wing, Fang, Meng, 和 Heng, Pheng-Ann.
    重访少样本图像分类中的度量学习。*神经计算*，406:49–58，2020年。
- en: Lv et al. (2019) Lv, Xinbi, Zhao, Cairong, and Chen, Wei. A novel hard mining
    center-triplet loss for person re-identification. In *Chinese Conference on Pattern
    Recognition and Computer Vision (PRCV)*, pp.  199–210\. Springer, 2019.
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv 等（2019）Lv, Xinbi, Zhao, Cairong, 和 Chen, Wei. 一种新颖的硬挖掘中心三元组损失用于行人重识别。在*中国模式识别与计算机视觉会议（PRCV）*，第199–210页。Springer，2019年。
- en: Mahalanobis (1930) Mahalanobis, Prasanta Chandra. On tests and measures of group
    divergence. *Journal of the Asiatic Society of Bengal*, 26:541–588, 1930.
  id: totrans-1248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahalanobis（1930）Mahalanobis, Prasanta Chandra. 关于群体差异的检验和测量。*孟加拉亚细亚学会杂志*，26:541–588，1930年。
- en: Mao et al. (2019) Mao, Chengzhi, Zhong, Ziyuan, Yang, Junfeng, Vondrick, Carl,
    and Ray, Baishakhi. Metric learning for adversarial robustness. *Advances in neural
    information processing systems*, 2019.
  id: totrans-1249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2019）Mao, Chengzhi, Zhong, Ziyuan, Yang, Junfeng, Vondrick, Carl, 和 Ray,
    Baishakhi. 对抗性鲁棒性的度量学习。*神经信息处理系统进展*，2019年。
- en: McLachlan (1999) McLachlan, Goeffrey J. Mahalanobis distance. *Resonance*, 4(6):20–26,
    1999.
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McLachlan（1999）McLachlan, Geoffrey J. Mahalanobis距离。*共振*，4(6):20–26，1999年。
- en: 'Mignon & Jurie (2012) Mignon, Alexis and Jurie, Frédéric. PCCA: A new approach
    for distance learning from sparse pairwise constraints. In *2012 IEEE conference
    on computer vision and pattern recognition*, pp.  2666–2672\. IEEE, 2012.'
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mignon & Jurie（2012）Mignon, Alexis 和 Jurie, Frédéric. PCCA：一种基于稀疏成对约束的距离学习新方法。在*2012
    IEEE计算机视觉与模式识别会议*，第2666–2672页。IEEE，2012年。
- en: 'Mika et al. (1999) Mika, Sebastian, Ratsch, Gunnar, Weston, Jason, Scholkopf,
    Bernhard, and Mullers, Klaus-Robert. Fisher discriminant analysis with kernels.
    In *Neural networks for signal processing IX: Proceedings of the 1999 IEEE signal
    processing society workshop (cat. no. 98th8468)*, pp. 41–48\. Ieee, 1999.'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mika 等（1999）Mika, Sebastian, Ratsch, Gunnar, Weston, Jason, Scholkopf, Bernhard,
    和 Mullers, Klaus-Robert. 带核的Fisher判别分析。在*信号处理神经网络IX：1999 IEEE信号处理学会研讨会论文集（Cat.
    No. 98th8468）*，第41–48页。IEEE，1999年。
- en: Movshovitz-Attias et al. (2017) Movshovitz-Attias, Yair, Toshev, Alexander,
    Leung, Thomas K, Ioffe, Sergey, and Singh, Saurabh. No fuss distance metric learning
    using proxies. In *Proceedings of the IEEE International Conference on Computer
    Vision*, pp.  360–368, 2017.
  id: totrans-1253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Movshovitz-Attias 等人 (2017) Movshovitz-Attias, Yair, Toshev, Alexander, Leung,
    Thomas K, Ioffe, Sergey, 和 Singh, Saurabh。使用代理的简单距离度量学习。发表于 *IEEE国际计算机视觉会议论文集*，第360–368页，2017年。
- en: Murphy (2007) Murphy, Kevin P. Conjugate Bayesian analysis of the Gaussian distribution.
    Technical report, University of British Colombia, 2007.
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murphy (2007) Murphy, Kevin P。高斯分布的共轭贝叶斯分析。技术报告，英属哥伦比亚大学，2007年。
- en: 'Murphy (2012) Murphy, Kevin P. *Machine learning: a probabilistic perspective*.
    MIT press, 2012.'
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murphy (2012) Murphy, Kevin P。*机器学习：概率视角*。MIT出版社，2012年。
- en: Musgrave et al. (2020) Musgrave, Kevin, Belongie, Serge, and Lim, Ser-Nam. Pytorch
    metric learning. *arXiv preprint arXiv:2008.09164*, 2020.
  id: totrans-1256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Musgrave 等人 (2020) Musgrave, Kevin, Belongie, Serge, 和 Lim, Ser-Nam。Pytorch度量学习。*arXiv预印本
    arXiv:2008.09164*，2020年。
- en: Oh Song et al. (2016) Oh Song, Hyun, Xiang, Yu, Jegelka, Stefanie, and Savarese,
    Silvio. Deep metric learning via lifted structured feature embedding. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pp.  4004–4012,
    2016.
  id: totrans-1257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh Song 等人 (2016) Oh Song, Hyun, Xiang, Yu, Jegelka, Stefanie, 和 Savarese, Silvio。通过提升结构化特征嵌入进行深度度量学习。发表于
    *IEEE计算机视觉与模式识别会议论文集*，第4004–4012页，2016年。
- en: Poorheravi et al. (2020) Poorheravi, Parisa Abdolrahim, Ghojogh, Benyamin, Gaudet,
    Vincent, Karray, Fakhri, and Crowley, Mark. Acceleration of large margin metric
    learning for nearest neighbor classification using triplet mining and stratified
    sampling. *Journal of Computational Vision and Imaging Systems*, 6(1), 2020.
  id: totrans-1258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poorheravi 等人 (2020) Poorheravi, Parisa Abdolrahim, Ghojogh, Benyamin, Gaudet,
    Vincent, Karray, Fakhri, 和 Crowley, Mark。使用三元组挖掘和分层采样加速大边距度量学习以进行最近邻分类。*计算机视觉与成像系统期刊*，6(1)，2020年。
- en: 'Qian et al. (2019) Qian, Qi, Shang, Lei, Sun, Baigui, Hu, Juhua, Li, Hao, and
    Jin, Rong. SoftTriple loss: Deep metric learning without triplet sampling. In
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 
    6450–6458, 2019.'
  id: totrans-1259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人 (2019) Qian, Qi, Shang, Lei, Sun, Baigui, Hu, Juhua, Li, Hao, 和 Jin,
    Rong。SoftTriple 损失：无需三元组采样的深度度量学习。发表于 *IEEE/CVF国际计算机视觉会议论文集*，第6450–6458页，2019年。
- en: Riccati (1724) Riccati, Jacobo. Animadversiones in aequationes differentiales
    secundi gradus. *Actorum Eruditorum Supplementa*, 8(1724):66–73, 1724.
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riccati (1724) Riccati, Jacobo。关于二阶微分方程的注释。*Actorum Eruditorum Supplementa*，8(1724):66–73，1724年。
- en: Robinson et al. (2021) Robinson, Joshua, Chuang, Ching-Yao, Sra, Suvrit, and
    Jegelka, Stefanie. Contrastive learning with hard negative samples. In *International
    Conference on Learning Representations*, 2021.
  id: totrans-1261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robinson 等人 (2021) Robinson, Joshua, Chuang, Ching-Yao, Sra, Suvrit, 和 Jegelka,
    Stefanie。带有困难负样本的对比学习。发表于 *国际学习表征会议*，2021年。
- en: Roostaiyan et al. (2017) Roostaiyan, Seyed Mahdi, Imani, Ehsan, and Baghshah,
    Mahdieh Soleymani. Multi-modal deep distance metric learning. *Intelligent Data
    Analysis*, 21(6):1351–1369, 2017.
  id: totrans-1262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roostaiyan 等人 (2017) Roostaiyan, Seyed Mahdi, Imani, Ehsan, 和 Baghshah, Mahdieh
    Soleymani。多模态深度距离度量学习。*智能数据分析*，21(6):1351–1369，2017年。
- en: Roweis & Saul (2000) Roweis, Sam T and Saul, Lawrence K. Nonlinear dimensionality
    reduction by locally linear embedding. *Science*, 290(5500):2323–2326, 2000.
  id: totrans-1263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roweis & Saul (2000) Roweis, Sam T 和 Saul, Lawrence K。通过局部线性嵌入进行非线性维度降维。*科学*，290(5500):2323–2326，2000年。
- en: Schölkopf (2001) Schölkopf, Bernhard. The kernel trick for distances. *Advances
    in neural information processing systems*, pp. 301–307, 2001.
  id: totrans-1264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schölkopf (2001) Schölkopf, Bernhard。距离的核技巧。*神经信息处理系统进展*，第301–307页，2001年。
- en: Schölkopf et al. (2000) Schölkopf, Bernhard, Smola, Alex J, Williamson, Robert C,
    and Bartlett, Peter L. New support vector algorithms. *Neural computation*, 12(5):1207–1245,
    2000.
  id: totrans-1265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schölkopf 等人 (2000) Schölkopf, Bernhard, Smola, Alex J, Williamson, Robert C,
    和 Bartlett, Peter L。新的支持向量算法。*神经计算*，12(5):1207–1245，2000年。
- en: 'Schroff et al. (2015) Schroff, Florian, Kalenichenko, Dmitry, and Philbin,
    James. FaceNet: A unified embedding for face recognition and clustering. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pp.  815–823,
    2015.'
  id: totrans-1266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schroff 等人 (2015) Schroff, Florian, Kalenichenko, Dmitry, 和 Philbin, James。FaceNet：用于面部识别和聚类的统一嵌入。发表于
    *IEEE计算机视觉与模式识别会议论文集*，第815–823页，2015年。
- en: Shental et al. (2002) Shental, Noam, Hertz, Tomer, Weinshall, Daphna, and Pavel,
    Misha. Adjustment learning and relevant component analysis. In *European conference
    on computer vision*, pp.  776–790. Springer, 2002.
  id: totrans-1267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shental 等人 (2002) Shental, Noam, Hertz, Tomer, Weinshall, Daphna, 和 Pavel, Misha。调整学习和相关组件分析。发表于
    *欧洲计算机视觉会议*，第776–790页。Springer，2002年。
- en: Sikaroudi et al. (2020a) Sikaroudi, Milad, Ghojogh, Benyamin, Safarpoor, Amir,
    Karray, Fakhri, Crowley, Mark, and Tizhoosh, Hamid R. Offline versus online triplet
    mining based on extreme distances of histopathology patches. In *International
    Symposium on Visual Computing*, pp. 333–345\. Springer, 2020a.
  id: totrans-1268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sikaroudi 等（2020a）Sikaroudi, Milad, Ghojogh, Benyamin, Safarpoor, Amir, Karray,
    Fakhri, Crowley, Mark, 和 Tizhoosh, Hamid R. 基于极端距离的离线与在线三元组挖掘。在 *国际视觉计算研讨会*，第333–345页。Springer，2020a。
- en: 'Sikaroudi et al. (2020b) Sikaroudi, Milad, Safarpoor, Amir, Ghojogh, Benyamin,
    Shafiei, Sobhan, Crowley, Mark, and Tizhoosh, Hamid R. Supervision and source
    domain impact on representation learning: A histopathology case study. In *2020
    42nd Annual International Conference of the IEEE Engineering in Medicine & Biology
    Society (EMBC)*, pp.  1400–1403\. IEEE, 2020b.'
  id: totrans-1269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sikaroudi 等（2020b）Sikaroudi, Milad, Safarpoor, Amir, Ghojogh, Benyamin, Shafiei,
    Sobhan, Crowley, Mark, 和 Tizhoosh, Hamid R. 监督和源领域对表示学习的影响：一个组织病理学的案例研究。在 *2020年IEEE医学与生物工程学会第42届年会（EMBC）*，第1400–1403页。IEEE，2020b。
- en: Sikaroudi et al. (2021) Sikaroudi, Milad, Ghojogh, Benyamin, Karray, Fakhri,
    Crowley, Mark, and Tizhoosh, Hamid R. Batch-incremental triplet sampling for training
    triplet networks using Bayesian updating theorem. In *2020 25th International
    Conference on Pattern Recognition (ICPR)*, pp.  7080–7086\. IEEE, 2021.
  id: totrans-1270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sikaroudi 等（2021）Sikaroudi, Milad, Ghojogh, Benyamin, Karray, Fakhri, Crowley,
    Mark, 和 Tizhoosh, Hamid R. 批量增量三元组采样用于通过贝叶斯更新定理训练三元组网络。在 *2020年第25届国际模式识别大会（ICPR）*，第7080–7086页。IEEE，2021。
- en: Sohn (2016) Sohn, Kihyuk. Improved deep metric learning with multi-class n-pair
    loss objective. In *Advances in neural information processing systems*, pp. 1857–1865,
    2016.
  id: totrans-1271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn（2016）Sohn, Kihyuk. 具有多类别 n-对损失目标的改进深度度量学习。在 *神经信息处理系统进展*，第1857–1865页，2016。
- en: 'Suárez et al. (2020) Suárez, Juan-Luis, García, Salvador, and Herrera, Francisco.
    pyDML: A Python library for distance metric learning. *Journal of Machine Learning
    Research*, 21:96–1, 2020.'
  id: totrans-1272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suárez 等（2020）Suárez, Juan-Luis, García, Salvador, 和 Herrera, Francisco. pyDML：一个用于距离度量学习的Python库。*机器学习研究期刊*，21:96–1，2020。
- en: 'Suárez et al. (2021) Suárez, Juan Luis, García, Salvador, and Herrera, Francisco.
    A tutorial on distance metric learning: Mathematical foundations, algorithms,
    experimental analysis, prospects and challenges. *Neurocomputing*, 425:300–322,
    2021.'
  id: totrans-1273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suárez 等（2021）Suárez, Juan Luis, García, Salvador, 和 Herrera, Francisco. 关于距离度量学习的教程：数学基础、算法、实验分析、前景与挑战。*神经计算*，425:300–322，2021。
- en: Teh & Taylor (2020) Teh, Eu Wern and Taylor, Graham W. Learning with less data
    via weakly labeled patch classification in digital pathology. In *2020 IEEE 17th
    International Symposium on Biomedical Imaging (ISBI)*, pp.  471–475\. IEEE, 2020.
  id: totrans-1274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teh & Taylor（2020）Teh, Eu Wern 和 Taylor, Graham W. 通过数字病理学中的弱标签补丁分类实现更少数据的学习。在
    *2020年IEEE第17届生物医学成像国际研讨会（ISBI）*，第471–475页。IEEE，2020。
- en: 'Teh et al. (2020) Teh, Eu Wern, DeVries, Terrance, and Taylor, Graham W. ProxyNCA++:
    Revisiting and revitalizing proxy neighborhood component analysis. In *European
    Conference on Computer Vision (ECCV)*, pp. 448–464\. Springer, 2020.'
  id: totrans-1275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teh 等（2020）Teh, Eu Wern, DeVries, Terrance, 和 Taylor, Graham W. ProxyNCA++：重新审视和振兴代理邻域组件分析。在
    *欧洲计算机视觉会议（ECCV）*，第448–464页。Springer，2020。
- en: 'Tizhoosh (2005) Tizhoosh, Hamid R. Opposition-based learning: a new scheme
    for machine intelligence. In *International conference on computational intelligence
    for modelling, control and automation and international conference on intelligent
    agents, web technologies and internet commerce (CIMCA-IAWTIC’06)*, volume 1, pp. 
    695–701\. IEEE, 2005.'
  id: totrans-1276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tizhoosh（2005）Tizhoosh, Hamid R. 基于对立的学习：一种新的机器智能方案。在 *计算智能建模、控制与自动化国际会议及智能代理、网络技术与互联网商务国际会议（CIMCA-IAWTIC’06）*，第1卷，第695–701页。IEEE，2005。
- en: 'Triantafillou et al. (2020) Triantafillou, Eleni, Zhu, Tyler, Dumoulin, Vincent,
    Lamblin, Pascal, Evci, Utku, Xu, Kelvin, Goroshin, Ross, Gelada, Carles, Swersky,
    Kevin, Manzagol, Pierre-Antoine, et al. Meta-dataset: A dataset of datasets for
    learning to learn from few examples. In *International Conference on Learning
    Representations*, 2020.'
  id: totrans-1277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Triantafillou 等（2020）Triantafillou, Eleni, Zhu, Tyler, Dumoulin, Vincent, Lamblin,
    Pascal, Evci, Utku, Xu, Kelvin, Goroshin, Ross, Gelada, Carles, Swersky, Kevin,
    Manzagol, Pierre-Antoine, 等. Meta-dataset：一个用于从少量示例中学习的多数据集。 在 *国际表示学习会议*，2020。
- en: Tsang et al. (2003) Tsang, Ivor W, Kwok, James T, Bay, C, and Kong, H. Distance
    metric learning with kernels. In *Proceedings of the International Conference
    on Artificial Neural Networks*, pp.  126–129, 2003.
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsang 等（2003）Tsang, Ivor W, Kwok, James T, Bay, C, 和 Kong, H. 具有核的距离度量学习。在 *国际人工神经网络会议论文集*，第126–129页，2003。
- en: van der Maaten & Hinton (2008) van der Maaten, Laurens and Hinton, Geoffrey.
    Visualizing data using t-SNE. *Journal of machine learning research*, 9(Nov):2579–2605,
    2008.
  id: totrans-1279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van der Maaten & Hinton (2008) van der Maaten，Laurens和Hinton，Geoffrey。 使用t-SNE可视化数据。
    *机器学习研究期刊*，9(Nov):2579–2605，2008。
- en: Vapnik (1995) Vapnik, Vladimir. *The nature of statistical learning theory*.
    Springer science & business media, 1995.
  id: totrans-1280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vapnik (1995) 瓦普尼克，弗拉迪米尔。 *统计学习理论的本质*。 Springer科学与商业媒体，1995。
- en: Wang & Tan (2017) Wang, Dong and Tan, Xiaoyang. Bayesian neighborhood component
    analysis. *IEEE transactions on neural networks and learning systems*, 29(7):3140–3151,
    2017.
  id: totrans-1281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang & Tan (2017) 王东和谭晓阳。 贝叶斯领域成分分析。 *IEEE神经网络与学习系统交易*，29(7):3140–3151，2017。
- en: Wang & Sun (2015) Wang, Fei and Sun, Jimeng. Survey on distance metric learning
    and dimensionality reduction in data mining. *Data mining and knowledge discovery*,
    29(2):534–564, 2015.
  id: totrans-1282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang & Sun (2015) 王飞和孙吉明。 数据挖掘中的距离度量学习和降维技术调查。 *数据挖掘与知识发现*，29(2):534–564，2015。
- en: Wang et al. (2017) Wang, Jian, Zhou, Feng, Wen, Shilei, Liu, Xiao, and Lin,
    Yuanqing. Deep metric learning with angular loss. In *Proceedings of the IEEE
    International Conference on Computer Vision*, pp.  2593–2601, 2017.
  id: totrans-1283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 (2017) 王健，周峰，温士磊，刘啸和林远清。 带有角度损失的深度度量学习。 在 *IEEE国际计算机视觉会议论文集* 中，第2593-2601页，2017。
- en: Wang & Jin (2009) Wang, Shijun and Jin, Rong. An information geometry approach
    for distance metric learning. In *Artificial intelligence and statistics*, pp. 
    591–598. PMLR, 2009.
  id: totrans-1284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang & Jin (2009) 王世军和金荣。 距离度量学习的信息几何方法。 在 *人工智能与统计学* 中，第591-598页。PMLR，2009。
- en: Wang et al. (2014) Wang, Wei, Ooi, Beng Chin, Yang, Xiaoyan, Zhang, Dongxiang,
    and Zhuang, Yueting. Effective multi-modal retrieval based on stacked auto-encoders.
    *Proceedings of the VLDB Endowment*, 7(8):649–660, 2014.
  id: totrans-1285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 (2014) 王伟，Ooi，Beng Chin，杨晓燕，张东祥和庄悦婷。 基于堆叠自动编码器的有效多模态检索。 *VLDB纪事*，7(8):649–660，2014。
- en: Wang et al. (2019) Wang, Xiao, Chen, Ziliang, Yang, Rui, Luo, Bin, and Tang,
    Jin. Improved hard example mining by discovering attribute-based hard person identity.
    *arXiv preprint arXiv:1905.02102*, 2019.
  id: totrans-1286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 (2019) 王霄，陈子良，杨睿，罗斌和唐劲。 通过发现基于属性的难题人物身份进行改进。 *arXiv预印本arXiv:1905.02102*，2019。
- en: Wang et al. (2020a) Wang, Xun, Zhang, Haozhi, Huang, Weilin, and Scott, Matthew R.
    Cross-batch memory for embedding learning. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  6388–6397, 2020a.
  id: totrans-1287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 (2020a) Wang，Xun，Zhang，Haozhi，Huang，Weilin和Scott，Matthew R. 交叉批次记忆用于嵌入学习。
    在 *计算机视觉与模式识别IEEE/CVF会议论文集* 中，第6388-6397页，2020a。
- en: 'Wang et al. (2020b) Wang, Yaqing, Yao, Quanming, Kwok, James T, and Ni, Lionel M.
    Generalizing from a few examples: A survey on few-shot learning. *ACM Computing
    Surveys (CSUR)*, 53(3):1–34, 2020b.'
  id: totrans-1288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 (2020b) 王雅 青，姚 全明，郭克明 james T 和倪立洪。 从少数样本中进行泛化：少样本学习调查。 *ACM计算调查(CSUR)*，53(3):1–34，2020b。
- en: Weinberger & Saul (2009) Weinberger, Kilian Q and Saul, Lawrence K. Distance
    metric learning for large margin nearest neighbor classification. *Journal of
    machine learning research*, 10(2), 2009.
  id: totrans-1289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weinberger & Saul (2009) Weinberger，Kilian Q和Saul，Lawrence K. 用于大边缘最近邻分类的距离度量学习。
    *机器学习研究期刊*，10(2), 2009。
- en: Weinberger et al. (2006) Weinberger, Kilian Q, Blitzer, John, and Saul, Lawrence K.
    Distance metric learning for large margin nearest neighbor classification. In
    *Advances in neural information processing systems*, pp. 1473–1480, 2006.
  id: totrans-1290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weinberger等人 (2006) Weinberger，Kilian Q，Blitzer，John和Saul，Lawrence K. 用于大边缘最近邻分类的距离度量学习。
    在 *神经信息处理系统进展* 中，第1473-1480页，2006。
- en: Wu et al. (2017) Wu, Chao-Yuan, Manmatha, R, Smola, Alexander J, and Krahenbuhl,
    Philipp. Sampling matters in deep embedding learning. In *Proceedings of the IEEE
    International Conference on Computer Vision*, pp.  2840–2848, 2017.
  id: totrans-1291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人 (2017) Wu，Chao-Yuan，Manmatha，R，Smola，Alexander J和Krahenbuhl，Philipp。 深度嵌入学习中的抽样问题。
    在 *IEEE国际计算机视觉会议论文集* 中，第2840-2848页，2017。
- en: Xiang et al. (2008) Xiang, Shiming, Nie, Feiping, and Zhang, Changshui. Learning
    a Mahalanobis distance metric for data clustering and classification. *Pattern
    recognition*, 41(12):3600–3612, 2008.
  id: totrans-1292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang等人 (2008) 项石明，聂飞平和张长水。 学习数据聚类和分类的马氏距离度量。 *模式识别*，41(12):3600–3612，2008。
- en: Xing et al. (2002) Xing, Eric, Jordan, Michael, Russell, Stuart J, and Ng, Andrew.
    Distance metric learning with application to clustering with side-information.
    *Advances in neural information processing systems*, 15:521–528, 2002.
  id: totrans-1293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing et al. (2002) 邢艾瑞克、乔丹迈克尔、拉塞尔斯图尔特J和吴安德鲁。带有附加信息的聚类应用的距离度量学习。*神经信息处理系统进展*，15:521–528，2002年。
- en: Xu et al. (2019a) Xu, Xing, He, Li, Lu, Huimin, Gao, Lianli, and Ji, Yanli.
    Deep adversarial metric learning for cross-modal retrieval. *World Wide Web*,
    22(2):657–672, 2019a.
  id: totrans-1294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2019a) 许兴、贺力、卢惠敏、高连力和季艳丽。用于跨模态检索的深度对抗度量学习。*国际网络*，22(2):657–672，2019年。
- en: Xu et al. (2019b) Xu, Xinyi, Cao, Huanhuan, Yang, Yanhua, Yang, Erkun, and Deng,
    Cheng. Zero-shot metric learning. In *International Joint Conference on Artificial
    Intelligence*, pp.  3996–4002, 2019b.
  id: totrans-1295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2019b) 许欣怡、曹欢欢、杨燕华、杨尔坤和邓程。零样本度量学习。见于*国际人工智能联合会议*，第3996–4002页，2019年。
- en: Xuan et al. (2020) Xuan, Hong, Stylianou, Abby, and Pless, Robert. Improved
    embeddings with easy positive triplet mining. In *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, pp.  2474–2482, 2020.
  id: totrans-1296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xuan et al. (2020) 宣宏、斯蒂利亚努、艾比和普莱斯、罗伯特。改进的嵌入与简单正三元组挖掘。见于*IEEE/CVF冬季计算机视觉应用会议论文集*，第2474–2482页，2020年。
- en: Yang (2007) Yang, Liu. An overview of distance metric learning. In *Proceedings
    of the computer vision and pattern recognition conference*, 2007.
  id: totrans-1297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang (2007) 杨刘。距离度量学习概述。见于*计算机视觉与模式识别会议论文集*，2007年。
- en: 'Yang & Jin (2006) Yang, Liu and Jin, Rong. Distance metric learning: A comprehensive
    survey. *Michigan State Universiy*, 2(2):4, 2006.'
  id: totrans-1298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang & Jin (2006) 杨刘和金荣。距离度量学习：全面综述。*密歇根州立大学*，2(2):4，2006年。
- en: Yang et al. (2006) Yang, Liu, Jin, Rong, Sukthankar, Rahul, and Liu, Yi. An
    efficient algorithm for local distance metric learning. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 2, pp.  543–548, 2006.
  id: totrans-1299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2006) 杨刘、金荣、苏克坦卡、拉胡尔和刘毅。局部距离度量学习的高效算法。见于*AAAI人工智能会议论文集*，第2卷，第543–548页，2006年。
- en: Yang et al. (2007) Yang, Liu, Jin, Rong, and Sukthankar, Rahul. Bayesian active
    distance metric learning. In *Conference on Uncertainty in Artificial Intelligence
    (UAI)*, 2007.
  id: totrans-1300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2007) 杨刘、金荣和苏克坦卡、拉胡尔。贝叶斯主动距离度量学习。见于*人工智能不确定性会议（UAI）*，2007年。
- en: 'Yang et al. (2020) Yang, Liu, Zhang, Mingyang, Li, Cheng, Bendersky, Michael,
    and Najork, Marc. Beyond 512 tokens: Siamese multi-depth transformer-based hierarchical
    encoder for long-form document matching. In *Proceedings of the 29th ACM International
    Conference on Information & Knowledge Management*, pp.  1725–1734, 2020.'
  id: totrans-1301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2020) 杨刘、张明阳、李程、本德斯基、迈克尔和纳约克、马克。超越512个标记：基于Siamese多深度变换器的层次编码器用于长文档匹配。见于*第29届ACM国际信息与知识管理会议论文集*，第1725–1734页，2020年。
- en: Yang et al. (2012) Yang, Wei, Wang, Kuanquan, and Zuo, Wangmeng. Fast neighborhood
    component analysis. *Neurocomputing*, 83:31–37, 2012.
  id: totrans-1302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2012) 杨伟、王宽权和左望萌。快速邻域成分分析。*神经计算*，83:31–37，2012年。
- en: Yang et al. (2016) Yang, Xun, Wang, Meng, Zhang, Luming, and Tao, Dacheng. Empirical
    risk minimization for metric learning using privileged information. In *IJCAI
    International Joint Conference on Artificial Intelligence*, 2016.
  id: totrans-1303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2016) 杨勋、王萌、张鲁明和陶大成。利用特权信息进行度量学习的经验风险最小化。见于*IJCAI国际人工智能联合会议*，2016年。
- en: Yang & Laaksonen (2007) Yang, Zhirong and Laaksonen, Jorma. Regularized neighborhood
    component analysis. In *Scandinavian Conference on Image Analysis*, pp.  253–262.
    Springer, 2007.
  id: totrans-1304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang & Laaksonen (2007) 杨志荣和劳克森，约尔玛。正则化邻域成分分析。见于*斯堪的纳维亚图像分析会议*，第253–262页，Springer，2007年。
- en: Ye et al. (2019) Ye, Mang, Zhang, Xu, Yuen, Pong C, and Chang, Shih-Fu. Unsupervised
    embedding learning via invariant and spreading instance feature. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  6210–6219,
    2019.
  id: totrans-1305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2019) 叶芒、张旭、袁鹏C和常世富。通过不变和扩散实例特征的无监督嵌入学习。见于*IEEE/CVF计算机视觉与模式识别会议论文集*，第6210–6219页，2019年。
- en: Yeung & Chang (2007) Yeung, Dit-Yan and Chang, Hong. A kernel approach for semisupervised
    metric learning. *IEEE Transactions on Neural Networks*, 18(1):141–149, 2007.
  id: totrans-1306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeung & Chang (2007) 杨迪恩和常洪。一种用于半监督度量学习的核方法。*IEEE神经网络学报*，18(1):141–149，2007年。
- en: Zadeh et al. (2016) Zadeh, Pourya, Hosseini, Reshad, and Sra, Suvrit. Geometric
    mean metric learning. In *International conference on machine learning*, pp. 2464–2471,
    2016.
  id: totrans-1307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zadeh等（2016）Zadeh, Pourya, Hosseini, Reshad, 和 Sra, Suvrit. 几何均值度量学习。发表在*国际机器学习会议*，第2464–2471页，2016年。
- en: 'Zhang et al. (2018) Zhang, Changqing, Liu, Yeqing, Liu, Yue, Hu, Qinghua, Liu,
    Xinwang, and Zhu, Pengfei. FISH-MML: Fisher-HSIC multi-view metric learning. In
    *IJCAI*, pp.  3054–3060, 2018.'
  id: totrans-1308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等（2018）Zhang, Changqing, Liu, Yeqing, Liu, Yue, Hu, Qinghua, Liu, Xinwang,
    和 Zhu, Pengfei. FISH-MML: Fisher-HSIC 多视角度量学习。发表在*IJCAI*，第3054–3060页，2018年。'
- en: Zhang et al. (2021) Zhang, Hangbin, Wong, Raymond K, and Chu, Victor W. Curvilinear
    collaborative metric learning with macro-micro attentions. In *2021 International
    Joint Conference on Neural Networks (IJCNN)*, pp.  1–8\. IEEE, 2021.
  id: totrans-1309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2021）Zhang, Hangbin, Wong, Raymond K, 和 Chu, Victor W. 带有宏观-微观注意机制的曲线协作度量学习。发表在*2021国际神经网络联合会议（IJCNN）*，第1–8页。IEEE，2021年。
- en: Zhang et al. (2003) Zhang, Zhihua, Kwok, James T, and Yeung, Dit-Yan. Parametric
    distance metric learning with label information. In *IJCAI*, volume 1450, 2003.
  id: totrans-1310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2003）Zhang, Zhihua, Kwok, James T, 和 Yeung, Dit-Yan. 带标签信息的参数化距离度量学习。发表在*IJCAI*，第1450卷，2003年。
- en: Zhou & Gu (2018) Zhou, Yu and Gu, Hong. Geometric mean metric learning for partial
    label data. *Neurocomputing*, 275:394–402, 2018.
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou & Gu（2018）Zhou, Yu 和 Gu, Hong. 用于部分标签数据的几何均值度量学习。*Neurocomputing*，275:394–402，2018年。
- en: Zhu et al. (2018) Zhu, Pengfei, Cheng, Hao, Hu, Qinghua, Wang, Qilong, and Zhang,
    Changqing. Towards generalized and efficient metric learning on riemannian manifold.
    In *IJCAI*, pp.  3235–3241, 2018.
  id: totrans-1312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等（2018）Zhu, Pengfei, Cheng, Hao, Hu, Qinghua, Wang, Qilong, 和 Zhang, Changqing.
    面向黎曼流形的广义和高效度量学习。发表在*IJCAI*，第3235–3241页，2018年。
- en: Contents
  id: totrans-1313
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#S1 "In Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 介绍](#S1 "在光谱、概率和深度度量学习：教程与调查")'
- en: '[2 Generalized Mahalanobis Distance Metric](#S2 "In Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 广义Mahalanobis距离度量](#S2 "在光谱、概率和深度度量学习：教程与调查")'
- en: '[2.1 Distance Metric](#S2.SS1 "In 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1316
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1 距离度量](#S2.SS1 "在2广义Mahalanobis距离度量 ‣ 光谱、概率和深度度量学习：教程与调查")'
- en: '[2.2 Mahalanobis Distance](#S2.SS2 "In 2 Generalized Mahalanobis Distance Metric
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2 Mahalanobis距离](#S2.SS2 "在2广义Mahalanobis距离度量 ‣ 光谱、概率和深度度量学习：教程与调查")'
- en: '[2.3 Generalized Mahalanobis Distance](#S2.SS3 "In 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  id: totrans-1318
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.3 广义Mahalanobis距离](#S2.SS3 "在2广义Mahalanobis距离度量 ‣ 光谱、概率和深度度量学习：教程与调查")'
- en: '[2.4 The Main Idea of Metric Learning](#S2.SS4 "In 2 Generalized Mahalanobis
    Distance Metric ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  id: totrans-1319
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.4 度量学习的主要思想](#S2.SS4 "在2广义Mahalanobis距离度量 ‣ 光谱、概率和深度度量学习：教程与调查")'
- en: '[3 Spectral Metric Learning](#S3 "In Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")'
  id: totrans-1320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 光谱度量学习](#S3 "在光谱、概率和深度度量学习：教程与调查")'
- en: '[3.1 Spectral Methods Using Scatters](#S3.SS1 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1321
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 光谱方法使用散点](#S3.SS1 "在3光谱度量学习 ‣ 光谱、概率和深度度量学习：教程与调查")'
- en: '[3.1.1 The First Spectral Method](#S3.SS1.SSS1 "In 3.1 Spectral Methods Using
    Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")'
  id: totrans-1322
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.1 第一个光谱方法](#S3.SS1.SSS1 "在3.1光谱方法使用散点 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程与调查")'
- en: '[3.1.2 Formulating as Semidefinite Programming](#S3.SS1.SSS2 "In 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1323
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.2 表述为半正定规划](#S3.SS1.SSS2 "在3.1光谱方法使用散点 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程与调查")'
- en: '[3.1.3 Relevant to Fisher Discriminant Analysis](#S3.SS1.SSS3 "In 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1324
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.3 与Fisher判别分析相关](#S3.SS1.SSS3 "在3.1光谱方法使用散点 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程与调查")'
- en: '[3.1.4 Relevant Component Analysis (RCA)](#S3.SS1.SSS4 "In 3.1 Spectral Methods
    Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1325
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.4 相关成分分析（RCA）](#S3.SS1.SSS4 "在 3.1 光谱方法使用散点 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.1.5 Discriminative Component Analysis (DCA)](#S3.SS1.SSS5 "In 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1326
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.5 判别成分分析（DCA）](#S3.SS1.SSS5 "在 3.1 光谱方法使用散点 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.1.6 High Dimensional Discriminative Component Analysis](#S3.SS1.SSS6 "In
    3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1327
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.6 高维判别成分分析](#S3.SS1.SSS6 "在 3.1 光谱方法使用散点 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.1.7 Regularization by Locally Linear Embedding](#S3.SS1.SSS7 "In 3.1 Spectral
    Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1328
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.7 通过局部线性嵌入进行正则化](#S3.SS1.SSS7 "在 3.1 光谱方法使用散点 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.1.8 Fisher-HSIC Multi-view Metric Learning (FISH-MML)](#S3.SS1.SSS8 "In
    3.1 Spectral Methods Using Scatters ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1329
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.8 Fisher-HSIC 多视角度量学习（FISH-MML）](#S3.SS1.SSS8 "在 3.1 光谱方法使用散点 ‣ 3 光谱度量学习
    ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.2 Spectral Methods Using Hinge Loss](#S3.SS2 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 使用铰链损失的光谱方法](#S3.SS2 "在 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.2.1 Large-Margin Metric Learning](#S3.SS2.SSS1 "In 3.2 Spectral Methods
    Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1331
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.1 大间隔度量学习](#S3.SS2.SSS1 "在 3.2 使用铰链损失的光谱方法 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.2.2 Imbalanced Metric Learning (IML)](#S3.SS2.SSS2 "In 3.2 Spectral Methods
    Using Hinge Loss ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1332
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.2 不平衡度量学习（IML）](#S3.SS2.SSS2 "在 3.2 使用铰链损失的光谱方法 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.3 Locally Linear Metric Adaptation (LLMA)](#S3.SS3 "In 3 Spectral Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 局部线性度量适应（LLMA）](#S3.SS3 "在 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.4 Relevant to Support Vector Machine](#S3.SS4 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.4 相关支持向量机](#S3.SS4 "在 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.5 Relevant to Multidimensional Scaling](#S3.SS5 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5 相关多维尺度分析](#S3.SS5 "在 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.6 Kernel Spectral Metric Learning](#S3.SS6 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.6 核光谱度量学习](#S3.SS6 "在 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.6.1 Using Eigenvalue Decomposition of Kernel](#S3.SS6.SSS1 "In 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1337
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.6.1 使用核的特征值分解](#S3.SS6.SSS1 "在 3.6 核光谱度量学习 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.6.2 Regularization by Locally Linear Embedding](#S3.SS6.SSS2 "In 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1338
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.6.2 通过局部线性嵌入进行正则化](#S3.SS6.SSS2 "在 3.6 核光谱度量学习 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.6.3 Regularization by Laplacian](#S3.SS6.SSS3 "In 3.6 Kernel Spectral Metric
    Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")'
  id: totrans-1339
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.6.3 通过拉普拉斯进行正则化](#S3.SS6.SSS3 "在 3.6 核光谱度量学习 ‣ 3 光谱度量学习 ‣ 光谱、概率和深度度量学习：教程和调查")'
- en: '[3.6.4 Kernel Discriminative Component Analysis](#S3.SS6.SSS4 "In 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1340
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.6.4 核判别组件分析](#S3.SS6.SSS4 "在 3.6 核谱度量学习 ‣ 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.6.5 Relevant to Kernel Fisher Discriminant Analysis](#S3.SS6.SSS5 "In 3.6
    Kernel Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1341
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.6.5 与核 Fisher 判别分析相关](#S3.SS6.SSS5 "在 3.6 核谱度量学习 ‣ 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.6.6 Relevant to Kernel Support Vector Machine](#S3.SS6.SSS6 "In 3.6 Kernel
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1342
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.6.6 与核支持向量机相关](#S3.SS6.SSS6 "在 3.6 核谱度量学习 ‣ 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.7 Geometric Spectral Metric Learning](#S3.SS7 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.7 几何谱度量学习](#S3.SS7 "在 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.7.1 Geometric Mean Metric Learning](#S3.SS7.SSS1 "In 3.7 Geometric Spectral
    Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1344
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.7.1 几何平均度量学习](#S3.SS7.SSS1 "在 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.7.2 Low-rank Geometric Mean Metric Learning](#S3.SS7.SSS2 "In 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1345
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.7.2 低秩几何平均度量学习](#S3.SS7.SSS2 "在 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.7.3 Geometric Mean Metric Learning for Partial Labels](#S3.SS7.SSS3 "In
    3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1346
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.7.3 部分标签的几何平均度量学习](#S3.SS7.SSS3 "在 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.7.4 Geometric Mean Metric Learning on SPD and Grassmannian Manifolds](#S3.SS7.SSS4
    "In 3.7 Geometric Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1347
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.7.4 SPD 和 Grassmannian 流形上的几何平均度量学习](#S3.SS7.SSS4 "在 3.7 几何谱度量学习 ‣ 3 谱度量学习
    ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.7.5 Metric Learning on Stiefel and SPD Manifolds](#S3.SS7.SSS5 "In 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1348
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.7.5 Stiefel 和 SPD 流形上的度量学习](#S3.SS7.SSS5 "在 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.7.6 Curvilinear Distance Metric Learning (CDML)](#S3.SS7.SSS6 "In 3.7 Geometric
    Spectral Metric Learning ‣ 3 Spectral Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1349
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.7.6 曲线距离度量学习（CDML）](#S3.SS7.SSS6 "在 3.7 几何谱度量学习 ‣ 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[3.8 Adversarial Metric Learning (AML)](#S3.SS8 "In 3 Spectral Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1350
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.8 对抗性度量学习（AML）](#S3.SS8 "在 3 谱度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[4 Probabilistic Metric Learning](#S4 "In Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 概率度量学习](#S4 "在 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[4.1 Collapsing Classes](#S4.SS1 "In 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1352
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 类别合并](#S4.SS1 "在 4 概率度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[4.1.1 Collapsing Classes in the Input Space](#S4.SS1.SSS1 "In 4.1 Collapsing
    Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1353
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.1 输入空间中的类别合并](#S4.SS1.SSS1 "在 4.1 类别合并 ‣ 4 概率度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[4.1.2 Collapsing Classes in the Feature Space](#S4.SS1.SSS2 "In 4.1 Collapsing
    Classes ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1354
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.2 特征空间中的类别合并](#S4.SS1.SSS2 "在 4.1 类别合并 ‣ 4 概率度量学习 ‣ 谱度量、概率度量和深度度量学习：教程和调查")'
- en: '[4.2 Neighborhood Component Analysis Methods](#S4.SS2 "In 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1355
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 邻域组件分析方法](#S4.SS2 "在 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.2.1 Neighborhood Component Analysis (NCA)](#S4.SS2.SSS1 "In 4.2 Neighborhood
    Component Analysis Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1356
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.1 邻域组件分析 (NCA)](#S4.SS2.SSS1 "在 4.2 邻域组件分析方法 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.2.2 Regularized Neighborhood Component Analysis](#S4.SS2.SSS2 "In 4.2 Neighborhood
    Component Analysis Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1357
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.2 正则化邻域组件分析](#S4.SS2.SSS2 "在 4.2 邻域组件分析方法 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.2.3 Fast Neighborhood Component Analysis](#S4.SS2.SSS3 "In 4.2 Neighborhood
    Component Analysis Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1358
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.3 快速邻域组件分析](#S4.SS2.SSS3 "在 4.2 邻域组件分析方法 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.3 Bayesian Metric Learning Methods](#S4.SS3 "In 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1359
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 贝叶斯度量学习方法](#S4.SS3 "在 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.3.1 Bayesian Metric Learning Using Sigmoid Function](#S4.SS3.SSS1 "In 4.3
    Bayesian Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1360
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3.1 使用 sigmoid 函数的贝叶斯度量学习](#S4.SS3.SSS1 "在 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.3.2 Bayesian Neighborhood Component Analysis](#S4.SS3.SSS2 "In 4.3 Bayesian
    Metric Learning Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1361
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3.2 贝叶斯邻域组件分析](#S4.SS3.SSS2 "在 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.3.3 Local Distance Metric (LDM)](#S4.SS3.SSS3 "In 4.3 Bayesian Metric Learning
    Methods ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1362
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3.3 局部距离度量 (LDM)](#S4.SS3.SSS3 "在 4.3 贝叶斯度量学习方法 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.4 Information Theoretic Metric Learning](#S4.SS4 "In 4 Probabilistic Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1363
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4 信息理论度量学习](#S4.SS4 "在 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.4.1 Information Theoretic Metric Learning with a Prior Weight Matrix](#S4.SS4.SSS1
    "In 4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1364
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4.1 带先验权重矩阵的信息理论度量学习](#S4.SS4.SSS1 "在 4.4 信息理论度量学习 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.4.2 Information Theoretic Metric Learning for Imbalanced Data](#S4.SS4.SSS2
    "In 4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1365
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4.2 针对不平衡数据的信息理论度量学习](#S4.SS4.SSS2 "在 4.4 信息理论度量学习 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.4.3 Probabilistic Relevant Component Analysis Methods](#S4.SS4.SSS3 "In
    4.4 Information Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1366
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4.3 概率相关组件分析方法](#S4.SS4.SSS3 "在 4.4 信息理论度量学习 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.4.4 Metric Learning by Information Geometry](#S4.SS4.SSS4 "In 4.4 Information
    Theoretic Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1367
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4.4 信息几何学度量学习](#S4.SS4.SSS4 "在 4.4 信息理论度量学习 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.5 Empirical Risk Minimization in Metric Learning](#S4.SS5 "In 4 Probabilistic
    Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  id: totrans-1368
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.5 度量学习中的经验风险最小化](#S4.SS5 "在 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程和调查")'
- en: '[4.5.1 Metric Learning Using the Sigmoid Function](#S4.SS5.SSS1 "In 4.5 Empirical
    Risk Minimization in Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1369
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.5.1 使用 sigmoid 函数的度量学习](#S4.SS5.SSS1 "在 4.5 度量学习中的经验风险最小化 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[4.5.2 Pairwise Constrained Component Analysis (PCCA)](#S4.SS5.SSS2 "In 4.5
    Empirical Risk Minimization in Metric Learning ‣ 4 Probabilistic Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1370
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.5.2 成对约束分量分析 (PCCA)](#S4.SS5.SSS2 "在 4.5 度量学习中的经验风险最小化 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[4.5.3 Metric Learning for Privileged Information](#S4.SS5.SSS3 "In 4.5 Empirical
    Risk Minimization in Metric Learning ‣ 4 Probabilistic Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1371
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.5.3 针对特权信息的度量学习](#S4.SS5.SSS3 "在 4.5 度量学习中的经验风险最小化 ‣ 4 概率度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5 Deep Metric Learning](#S5 "In Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 深度度量学习](#S5 "在 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.1 Reconstruction Autoencoders](#S5.SS1 "In 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1373
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 重建自编码器](#S5.SS1 "在 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.1.1 Types of Autoencoders](#S5.SS1.SSS1 "In 5.1 Reconstruction Autoencoders
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1374
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.1 自编码器的类型](#S5.SS1.SSS1 "在 5.1 重建自编码器 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.1.2 Reconstruction Loss](#S5.SS1.SSS2 "In 5.1 Reconstruction Autoencoders
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1375
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.2 重建损失](#S5.SS1.SSS2 "在 5.1 重建自编码器 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.1.3 Denoising Autoencoder](#S5.SS1.SSS3 "In 5.1 Reconstruction Autoencoders
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1376
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.3 去噪自编码器](#S5.SS1.SSS3 "在 5.1 重建自编码器 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.1.4 Metric Learning by Reconstruction Autoencoder](#S5.SS1.SSS4 "In 5.1
    Reconstruction Autoencoders ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1377
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.4 基于重建自编码器的度量学习](#S5.SS1.SSS4 "在 5.1 重建自编码器 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.2 Supervised Metric Learning by Supervised Loss Functions](#S5.SS2 "In 5
    Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  id: totrans-1378
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 通过监督损失函数的监督度量学习](#S5.SS2 "在 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.2.1 Mean Squared Error and Mean Absolute Value Losses](#S5.SS2.SSS1 "In
    5.2 Supervised Metric Learning by Supervised Loss Functions ‣ 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1379
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.1 均方误差和均值绝对值损失](#S5.SS2.SSS1 "在 5.2 通过监督损失函数的监督度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.2.2 Huber and KL-Divergence Losss](#S5.SS2.SSS2 "In 5.2 Supervised Metric
    Learning by Supervised Loss Functions ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1380
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.2 Huber 和 KL 散度损失](#S5.SS2.SSS2 "在 5.2 通过监督损失函数的监督度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.2.3 Hinge Loss](#S5.SS2.SSS3 "In 5.2 Supervised Metric Learning by Supervised
    Loss Functions ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric
    Learning: Tutorial and Survey")'
  id: totrans-1381
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.3 钳制损失](#S5.SS2.SSS3 "在 5.2 通过监督损失函数的监督度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.2.4 Cross-entropy Loss](#S5.SS2.SSS4 "In 5.2 Supervised Metric Learning
    by Supervised Loss Functions ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1382
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.4 交叉熵损失](#S5.SS2.SSS4 "在 5.2 通过监督损失函数的监督度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3 Metric Learning by Siamese Networks](#S5.SS3 "In 5 Deep Metric Learning
    ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1383
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3 基于孪生网络的度量学习](#S5.SS3 "在 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.1 Siamese and Triplet Networks](#S5.SS3.SSS1 "In 5.3 Metric Learning by
    Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1384
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.1 Siamese和三重网络](#S5.SS3.SSS1 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.2 Pairs and Triplets of Data Points](#S5.SS3.SSS2 "In 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1385
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.2 数据点对和三重](#S5.SS3.SSS2 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.3 Implementation of Siamese Networks](#S5.SS3.SSS3 "In 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1386
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.3 Siamese网络的实现](#S5.SS3.SSS3 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.4 Contrastive Loss](#S5.SS3.SSS4 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1387
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.4 对比损失](#S5.SS3.SSS4 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.5 Triplet Loss](#S5.SS3.SSS5 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1388
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.5 三重损失](#S5.SS3.SSS5 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.6 Tuplet Loss](#S5.SS3.SSS6 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1389
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.6 四元组损失](#S5.SS3.SSS6 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.7 Neighborhood Component Analysis Loss](#S5.SS3.SSS7 "In 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1390
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.7 邻域组件分析损失](#S5.SS3.SSS7 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.8 Proxy Neighborhood Component Analysis Loss](#S5.SS3.SSS8 "In 5.3 Metric
    Learning by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1391
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.8 代理邻域组件分析损失](#S5.SS3.SSS8 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.9 Softmax Triplet Loss](#S5.SS3.SSS9 "In 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1392
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.9 Softmax三重损失](#S5.SS3.SSS9 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.10 Triplet Global Loss](#S5.SS3.SSS10 "In 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1393
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.10 三重全局损失](#S5.SS3.SSS10 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.11 Angular Loss](#S5.SS3.SSS11 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1394
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.11 角度损失](#S5.SS3.SSS11 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.12 SoftTriple Loss](#S5.SS3.SSS12 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1395
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.12 SoftTriple损失](#S5.SS3.SSS12 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣
    频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.13 Fisher Siamese Losses](#S5.SS3.SSS13 "In 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1396
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.13 Fisher Siamese损失](#S5.SS3.SSS13 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习
    ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.14 Deep Adversarial Metric Learning](#S5.SS3.SSS14 "In 5.3 Metric Learning
    by Siamese Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep
    Metric Learning: Tutorial and Survey")'
  id: totrans-1397
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.14 深度对抗度量学习](#S5.SS3.SSS14 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.15 Triplet Mining](#S5.SS3.SSS15 "In 5.3 Metric Learning by Siamese Networks
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1398
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.15 三重挖掘](#S5.SS3.SSS15 "在5.3 节，通过Siamese网络进行的度量学习 ‣ 5 深度度量学习 ‣ 频谱、概率和深度度量学习：教程与调查")'
- en: '[5.3.16 Triplet Sampling](#S5.SS3.SSS16 "In 5.3 Metric Learning by Siamese
    Networks ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1399
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.16 三元组采样](#S5.SS3.SSS16 "在 5.3 由连体网络进行的度量学习 ‣ 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.4 Deep Discriminant Analysis Metric Learning](#S5.SS4 "In 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1400
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4 深度判别分析度量学习](#S5.SS4 "在 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.4.1 Deep Probabilistic Discriminant Analysis](#S5.SS4.SSS1 "In 5.4 Deep
    Discriminant Analysis Metric Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1401
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.1 深度概率判别分析](#S5.SS4.SSS1 "在 5.4 深度判别分析度量学习 ‣ 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.4.2 Discriminant Analysis with Virtual Samples](#S5.SS4.SSS2 "In 5.4 Deep
    Discriminant Analysis Metric Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1402
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.2 带有虚拟样本的判别分析](#S5.SS4.SSS2 "在 5.4 深度判别分析度量学习 ‣ 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.4.3 Deep Fisher Discriminant Analysis](#S5.SS4.SSS3 "In 5.4 Deep Discriminant
    Analysis Metric Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and
    Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1403
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.3 深度 Fisher 判别分析](#S5.SS4.SSS3 "在 5.4 深度判别分析度量学习 ‣ 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.5 Multi-Modal Deep Metric Learning](#S5.SS5 "In 5 Deep Metric Learning ‣
    Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1404
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.5 多模态深度度量学习](#S5.SS5 "在 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.6 Geometric Metric Learning by Neural Network](#S5.SS6 "In 5 Deep Metric
    Learning ‣ Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1405
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.6 几何度量学习的神经网络方法](#S5.SS6 "在 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.7 Few-shot Metric Learning](#S5.SS7 "In 5 Deep Metric Learning ‣ Spectral,
    Probabilistic, and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1406
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.7 少样本度量学习](#S5.SS7 "在 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.7.1 Multi-scale Metric Learning](#S5.SS7.SSS1 "In 5.7 Few-shot Metric Learning
    ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic, and Deep Metric Learning:
    Tutorial and Survey")'
  id: totrans-1407
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.7.1 多尺度度量学习](#S5.SS7.SSS1 "在 5.7 少样本度量学习 ‣ 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[5.7.2 Metric Learning with Continuous Similarity Scores](#S5.SS7.SSS2 "In
    5.7 Few-shot Metric Learning ‣ 5 Deep Metric Learning ‣ Spectral, Probabilistic,
    and Deep Metric Learning: Tutorial and Survey")'
  id: totrans-1408
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.7.2 连续相似度评分的度量学习](#S5.SS7.SSS2 "在 5.7 少样本度量学习 ‣ 5 深度度量学习 ‣ 谱学、概率学和深度度量学习：教程与调查")'
- en: '[6 Conclusion](#S6 "In Spectral, Probabilistic, and Deep Metric Learning: Tutorial
    and Survey")'
  id: totrans-1409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 结论](#S6 "在 谱学、概率学和深度度量学习：教程与调查")'
