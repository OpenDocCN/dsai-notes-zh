- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:36:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:36:20
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2310.13039] Human Pose-based Estimation, Tracking and Action Recognition with
    Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2310.13039] 基于人体姿态的估计、跟踪和动作识别：深度学习综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.13039](https://ar5iv.labs.arxiv.org/html/2310.13039)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.13039](https://ar5iv.labs.arxiv.org/html/2310.13039)
- en: \equalcont
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \equalcont
- en: These authors contributed equally to this work. \equalcontThese authors contributed
    equally to this work. \equalcontThese authors contributed equally to this work.
    [1]Zhimin Gao
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些作者对本工作贡献相等。 \equalcont这些作者对本工作贡献相等。 \equalcont这些作者对本工作贡献相等。 [1] Zhimin Gao
- en: 1]School of Computer and Artificial Intelligence, Zhengzhou University, China
    2]Amazon Prime Video, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 1]计算机与人工智能学院，郑州大学，中国 2]亚马逊Prime Video，美国
- en: 'Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于人体姿态的估计、跟踪和动作识别：深度学习综述
- en: Lijuan Zhou [ieljzhou@zzu.edu.cn](mailto:ieljzhou@zzu.edu.cn)    Xiang Meng
    [mengxiangzzu@163.com](mailto:mengxiangzzu@163.com)    Zhihuan Liu [liuzhihuanzzu@163.com](mailto:liuzhihuanzzu@163.com)
       Mengqi Wu [mengqiwuzzu@163.com](mailto:mengqiwuzzu@163.com)    [iegaozhimin@zzu.edu.cn](mailto:iegaozhimin@zzu.edu.cn)
       Pichao Wang [pichaowang@gmail.com](mailto:pichaowang@gmail.com) [ [
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Lijuan Zhou [ieljzhou@zzu.edu.cn](mailto:ieljzhou@zzu.edu.cn)    Xiang Meng
    [mengxiangzzu@163.com](mailto:mengxiangzzu@163.com)    Zhihuan Liu [liuzhihuanzzu@163.com](mailto:liuzhihuanzzu@163.com)
       Mengqi Wu [mengqiwuzzu@163.com](mailto:mengqiwuzzu@163.com)    [iegaozhimin@zzu.edu.cn](mailto:iegaozhimin@zzu.edu.cn)
       Pichao Wang [pichaowang@gmail.com](mailto:pichaowang@gmail.com) [ [
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Human pose analysis has garnered significant attention within both the research
    community and practical applications, owing to its expanding array of uses, including
    gaming, video surveillance, sports performance analysis, and human-computer interactions,
    among others. The advent of deep learning has significantly improved the accuracy
    of pose capture, making pose-based applications increasingly practical. This paper
    presents a comprehensive survey of pose-based applications utilizing deep learning,
    encompassing pose estimation, pose tracking, and action recognition.Pose estimation
    involves the determination of human joint positions from images or image sequences.
    Pose tracking is an emerging research direction aimed at generating consistent
    human pose trajectories over time. Action recognition, on the other hand, targets
    the identification of action types using pose estimation or tracking data. These
    three tasks are intricately interconnected, with the latter often reliant on the
    former. In this survey, we comprehensively review related works, spanning from
    single-person pose estimation to multi-person pose estimation, from 2D pose estimation
    to 3D pose estimation, from single image to video, from mining temporal context
    gradually to pose tracking, and lastly from tracking to pose-based action recognition.
    As a survey centered on the application of deep learning to pose analysis, we
    explicitly discuss both the strengths and limitations of existing techniques.
    Notably, we emphasize methodologies for integrating these three tasks into a unified
    framework within video sequences. Additionally, we explore the challenges involved
    and outline potential directions for future research.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态分析在研究界和实际应用中都引起了广泛关注，其应用范围不断扩展，包括游戏、视频监控、运动表现分析和人机交互等。深度学习的出现显著提高了姿态捕捉的准确性，使得基于姿态的应用变得越来越实用。本文对利用深度学习的基于姿态的应用进行了全面的综述，包括姿态估计、姿态跟踪和动作识别。姿态估计涉及从图像或图像序列中确定人体关节位置。姿态跟踪是一个新兴的研究方向，旨在生成一致的人体姿态轨迹。另一方面，动作识别则致力于使用姿态估计或跟踪数据来识别动作类型。这三项任务密切相关，后者通常依赖于前者。在本综述中，我们全面回顾了相关工作，涵盖了从单人姿态估计到多人姿态估计，从2D姿态估计到3D姿态估计，从单图像到视频，从逐步挖掘时间上下文到姿态跟踪，最后从跟踪到基于姿态的动作识别。作为一项以深度学习在姿态分析中的应用为中心的综述，我们明确讨论了现有技术的优缺点。特别是，我们强调了将这三项任务整合到视频序列中的统一框架的方法。此外，我们探讨了相关挑战，并概述了未来研究的潜在方向。
- en: 'keywords:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Pose Estimation, Pose Tracking, Action Recognition, Deep Learning, Survey
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态估计，姿态跟踪，动作识别，深度学习，综述
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Human pose estimation, tracking, and pose-based action recognition represent
    three fundamental research directions within the field of computer vision. These
    areas have a broad spectrum of applications, spanning from video surveillance,
    human-computer interactions, gaming, sports analysis, intelligent driving, and
    the emerging landscape of new retail stores. Articulated human pose estimation
    involves the task of estimating the configuration of the human body in a given
    image or video. Human pose tracking targets to generate consistent pose trajectories
    over time, which is usually used to analyze the motion proprieties of human. Human
    pose-based or skeleton-based action recognition is to recognize the types of actions
    based on the pose estimation or tracking data. Although these three tasks fall
    within the domain of human motion analysis, they are typically treated as distinct
    entities in the existing literature.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计、跟踪和基于姿态的动作识别是计算机视觉领域的三个基本研究方向。这些领域有着广泛的应用，包括视频监控、人机交互、游戏、运动分析、智能驾驶以及新零售的崭新领域。细致的人体姿态估计涉及在给定图像或视频中估计人体的配置。人体姿态跟踪的目标是生成一致的姿态轨迹，这通常用于分析人体的运动特性。基于姿态的动作识别或骨架动作识别是基于姿态估计或跟踪数据来识别动作类型。尽管这三项任务都属于人体运动分析领域，但在现有文献中，它们通常被视为不同的实体。
- en: Human motion analysis is a long-standing research topic, and there are a vast
    of works and several surveys on this task [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. In these surveys, human detection, tracking, pose estimation
    and motion recognition are usually reviewed together. Several survey papers have
    summarized the research on human pose estimation [[10](#bib.bib10), [11](#bib.bib11)],
    tracking [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)], and action recognition [[17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20)]. With the development of deep learning, the
    three tasks have achieved significant improvements compared to hand-crafted feature
    era [[21](#bib.bib21), [22](#bib.bib22)]. The previous surveys either reviewed
    the whole vision-based human motion domain [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)],
    or have focused on specific tasks [[10](#bib.bib10), [11](#bib.bib11), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)].
    However, there is no such survey paper which simultaneously reviews pose estimation,
    pose tracking, and pose recognition. Inspired by Lagrangian viewpoint of motion
    analysis [[28](#bib.bib28)], pose information and tracking are beneficial for
    action recognition. Therefore, these three tasks are closely related each other.
    It is significantly useful for reviewing the methods linking the three tasks together,
    and providing a deep understanding for the separate solution of each task and
    more exploration for a unified solution of joint tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 人体运动分析是一个长期研究的课题，关于此任务的研究和综述非常丰富 [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]。在这些综述中，人类检测、跟踪、姿态估计和动作识别通常一起进行评审。几篇综述文章总结了人体姿态估计 [[10](#bib.bib10),
    [11](#bib.bib11)]，跟踪 [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)]，以及动作识别 [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)]。随着深度学习的发展，这三项任务相比于手工特征时代取得了显著进展 [[21](#bib.bib21), [22](#bib.bib22)]。之前的综述要么回顾了整个基于视觉的人体运动领域 [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)]，要么集中于特定任务 [[10](#bib.bib10), [11](#bib.bib11),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27)]。然而，目前尚无综述论文同时回顾姿态估计、姿态跟踪和姿态识别。受到拉格朗日视角的启发 [[28](#bib.bib28)]，姿态信息和跟踪对动作识别具有重要意义。因此，这三项任务彼此密切相关。回顾将这三项任务联系起来的方法，并深入理解每项任务的独立解决方案以及探索联合任务的统一解决方案是非常有用的。
- en: 'In this paper, we will conduct a comprehensive review of previous works using
    deep learning approach on these three tasks individually, and discuss the strengths
    and weaknesses of previous research paper. Furthermore, we elucidate the inherent
    connections that bind these three tasks together, while championing the adoption
    of a deep learning-based framework that seamlessly integrates them. Specifically,
    we will review previous works with deep learning from 2D pose estimation to 3D
    pose estimation from single images to videos, from mining temporal contexts gradually
    to pose tracking, and lastly from tracking to pose-based action recognition. According
    to the number of persons for pose estimation, 2D/3D pose estimation can be divided
    into single-person and multi-person pose estimation. Depending on the input to
    the networks, each category can be further divided into image and video-based
    single-person/multi-person pose estimation. To link the poses across the frames,
    pose tracking can be divided into post-processing and integrated methods for single-person
    pose tracking, top-down and bottom-up approaches for multi-person pose tracking.
    After getting the trajectory of poses in the videos, pose-based action recognition
    could be naturally conducted which can be divided into estimated pose and skeleton-based
    action recognition. The former takes RGB videos as the input and jointly conducts
    pose estimation, tracking, and action recognition. The latter extracts skeleton
    sequences captured by sensors such as motion capture, time-of-flight, and structured
    light cameras for action recognition. For skeleton-based action recognition, four
    categories are identified including Convolutional Neural Networks (CNN), Recurrent
    Neural Networks (RNN), Graph Neural Networks (GCN) and Transformer-based approaches.
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") illustrates the taxonomy
    of this survey.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们将对使用深度学习方法处理这三项任务的前人工作进行全面回顾，并讨论先前研究论文的优缺点。此外，我们阐明了将这三项任务联系在一起的内在关系，同时倡导采用一个基于深度学习的框架，将它们无缝集成。具体而言，我们将回顾从2D姿态估计到3D姿态估计，从单图像到视频，从逐步挖掘时间上下文到姿态跟踪，最后从跟踪到基于姿态的动作识别的前人工作。根据姿态估计的人数，2D/3D姿态估计可以分为单人和多人姿态估计。根据网络的输入，每个类别可以进一步分为基于图像和基于视频的单人/多人姿态估计。为了在帧之间连接姿态，姿态跟踪可以分为单人姿态跟踪的后处理方法和集成方法，多人姿态跟踪的自上而下和自下而上的方法。在获得视频中的姿态轨迹后，可以自然地进行基于姿态的动作识别，这可以分为估计姿态和基于骨架的动作识别。前者以RGB视频作为输入，同时进行姿态估计、跟踪和动作识别。后者提取由传感器（如运动捕捉、飞行时间和结构光摄像机）捕获的骨架序列以进行动作识别。对于基于骨架的动作识别，识别出四种类别，包括卷积神经网络（CNN）、递归神经网络（RNN）、图神经网络（GCN）和基于变换器的方法。图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Human Pose-based Estimation, Tracking and Action
    Recognition with Deep Learning: A Survey") 说明了本调查的分类。'
- en: 'The key novelty of this survey is the focus on three closely related tasks
    that use deep learning approach, which has never been done in previous surveys.
    In reviewing the various methods, consideration has been given to the connections
    between the three tasks, hence, this survey tends to discuss the advantages and
    limitations of the reviewed methods from the viewpoint of assembling them to get
    more practical applications. This is the first survey to put them together to
    analysis their inner connections in deep learning era. Besides, this survey distinguishes
    itself from other surveys through the following contributions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的关键创新在于关注使用深度学习方法的三个密切相关的任务，而这一点在以往的调查中从未涉及。在回顾各种方法时，我们考虑了这三项任务之间的联系，因此，本调查倾向于从将这些方法组合以获得更多实际应用的角度讨论其优缺点。这是第一次将它们结合起来分析在深度学习时代的内在联系。此外，本调查通过以下贡献使其区别于其他调查：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A thorough and all-encompassing coverage of the most advanced deep learning-based
    methodologies developed since 2014\. This extensive coverage affords readers a
    comprehensive overview of the latest research methodologies and their outcomes.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对自2014年以来开发的最先进的深度学习方法进行了全面而详尽的覆盖。这一广泛的覆盖为读者提供了最新研究方法及其结果的全面概述。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An insightful categorization and analysis of methods on the three tasks, and
    highlights of the pros and cons, promoting potential exploration of better solutions.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对三项任务的方法进行了深刻的分类和分析，并突出了优缺点，推动了对更好解决方案的潜在探索。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An extensive review of the most commonly used benchmark datasets for these three
    tasks, and the state-of-the-art results on the benchmark datasets.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对这三项任务最常用基准数据集的全面回顾，以及在这些基准数据集上的最新研究成果。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An earnest discussion of the challenges of three tasks and potential research
    directions through limitation analysis of available methods.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过对现有方法的局限性分析，认真讨论三项任务的挑战及潜在的研究方向。
- en: 'Subsequent sections of this survey are organized as follows. Sections  [2](#S2
    "2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") through  [4](#S4 "4 Action Recognition ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey")
    delve into the methods of pose estimation, pose tracking, and action recognition,
    respectively. Commonly used benchmark datasets and the performance comparison
    for three tasks are described in Section [5](#S5 "5 Benchmark datasets ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey").
    Challenges of these three tasks and pointers to future directions are presented
    in Section [6](#S6 "6 Challenges and Future Directions ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey"). The survey provides
    concluding remarks in Section [7](#S7 "7 Conclusion ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的后续部分组织如下。第 [2](#S2 "2 姿态估计 ‣ 基于人体姿态的深度学习姿态估计、跟踪和动作识别：一项调查")节至第 [4](#S4 "4
    动作识别 ‣ 基于人体姿态的深度学习姿态估计、跟踪和动作识别：一项调查")节分别探讨了姿态估计、姿态跟踪和动作识别的方法。常用基准数据集及三项任务的性能比较在第 [5](#S5
    "5 基准数据集 ‣ 基于人体姿态的深度学习姿态估计、跟踪和动作识别：一项调查")节中描述。第 [6](#S6 "6 挑战与未来方向 ‣ 基于人体姿态的深度学习姿态估计、跟踪和动作识别：一项调查")节介绍了这三项任务的挑战及未来方向的指引。第 [7](#S7
    "7 结论 ‣ 基于人体姿态的深度学习姿态估计、跟踪和动作识别：一项调查")节提供了总结性评论。
- en: '![Refer to caption](img/dec4d00b003244e8dc72c4b5cff9d710.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dec4d00b003244e8dc72c4b5cff9d710.png)'
- en: 'Figure 1: The taxonomy of this survey.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本调查的分类法。
- en: 2 Pose estimation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 姿态估计
- en: 'Human representation can be approached through three distinct models: the kinematic
    model, the planar model, and the volumetric model. The kinematic model employs
    a combination of joint positions and limb orientations to faithfully depict the
    human body’s structure. In contrast, the planar model utilizes rectangles to represent
    both body shape and appearance, while the volumetric model leverages mesh data
    to capture the intricacies of the human body’s shape. It’s essential to underscore
    that this paper exclusively focuses on the kinematic model-based human representation.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 人体表示可以通过三种不同的模型进行：运动学模型、平面模型和体积模型。运动学模型结合关节位置和肢体方向来准确描绘人体结构。相对而言，平面模型利用矩形表示身体形状和外观，而体积模型则利用网格数据捕捉人体形状的复杂性。需要强调的是，本论文专注于基于运动学模型的人体表示。
- en: 'Pose estimation, pose tracking and action recognition are three intimately
    interrelated tasks. Fig. [2](#S2.F2 "Figure 2 ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") shows
    the relationship among the three tasks. Pose estimation aims to estimate joint
    coordinates from an image or a video. Pose tracking is an extension of pose estimation
    in the context of videos, which associates each estimated pose with its corresponding
    identity over time. It is interesting noting that a recent work [[29](#bib.bib29)]
    tends to estimate poses after tracking volumes of persons, which implies that
    the two-way relationship of pose estimation and tracking. Pose-based action recognition
    aims to give the tracked pose with an identity the corresponding action label.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '姿态估计、姿态跟踪和动作识别是三个紧密相关的任务。图[2](#S2.F2 "Figure 2 ‣ 2 Pose estimation ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey")展示了这三项任务之间的关系。姿态估计旨在从图像或视频中估计关节坐标。姿态跟踪是在视频上下文中对姿态估计的扩展，它将每个估计的姿态与其对应的身份在时间上关联起来。有趣的是，最近的一项工作[[29](#bib.bib29)]倾向于在跟踪多个个体后估计姿态，这暗示了姿态估计和跟踪之间的双向关系。基于姿态的动作识别旨在为跟踪的姿态分配一个身份和相应的动作标签。'
- en: For pose estimation, we generally classify the reviewed methods into two categories,
    2D pose estimation and 3D pose estimation. The 2D pose estimation is to estimate
    a 2D pose $(x,y)$ coordinates for each joint from a RGB image or video while 3D
    pose estimation is to estimate a 3D pose $(x,y,z)$ coordinates.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于姿态估计，我们通常将审查的方法分为两类：二维姿态估计和三维姿态估计。二维姿态估计是从RGB图像或视频中估计每个关节的二维姿态 $(x,y)$ 坐标，而三维姿态估计是估计三维姿态
    $(x,y,z)$ 坐标。
- en: '![Refer to caption](img/20b41563a2ad8bd1c0034b05be741169.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/20b41563a2ad8bd1c0034b05be741169.png)'
- en: 'Figure 2: The relationship among the three tasks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：三项任务之间的关系。
- en: 2.1 2D pose estimation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 二维姿态估计
- en: For 2D pose estimation, two sub-divisions are identified, single-person pose
    estimation and multi-person pose estimation. Depending on the input to the networks,
    single (multi) person pose estimation could be further divided into image-based
    single (multi) person pose estimation and video-based single (multi) person pose
    estimation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二维姿态估计，通常有两个子分类：单人姿态估计和多人姿态估计。根据网络输入的不同，单人（多人）姿态估计可以进一步分为基于图像的单人（多人）姿态估计和基于视频的单人（多人）姿态估计。
- en: 2.1.1 Image-based single-person pose estimation
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 基于图像的单人姿态估计
- en: 'For image-based Single-Person Pose Estimation (SPPE), the task involves providing
    the position and a rough scale of a person or their bounding box as a precursor
    to the estimation process. Early works adopt the pictorial structures framework
    that represents an object by a collection of parts arranged in a deformable configuration,
    and a part in the collection is an appearance template matched in an image. Different
    from early works, the deep learning-based methods target to locate keypoints of
    human parts. Two typical frameworks, namely, direct regression and heatmap-based
    approaches, are available for image-based single-person pose estimation. In the
    direct regression-based approach, keypoints are directly predicted from the image
    features, whereas the heatmap-based approach initially generates heatmaps and
    subsequently infers keypoint locations based on these heatmaps. Fig. [3](#S2.F3
    "Figure 3 ‣ 2.1.1 Image-based single-person pose estimation ‣ 2.1 2D pose estimation
    ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") provides an illustrative overview of the general
    framework for image-based 2D SPPE, showcasing the two predominant approaches.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '对于基于图像的单人姿态估计（SPPE），任务包括提供一个人的位置和大致尺度，作为估计过程的前置条件。早期工作采用了图示结构框架，该框架通过一组按变形配置排列的部件来表示一个对象，而这些部件中的每一个都是在图像中匹配的外观模板。不同于早期工作，基于深度学习的方法旨在定位人体部位的关键点。基于图像的单人姿态估计有两种典型框架，即直接回归方法和基于热图的方法。在直接回归方法中，关键点是直接从图像特征中预测的，而基于热图的方法则首先生成热图，然后基于这些热图推断关键点位置。图[3](#S2.F3
    "Figure 3 ‣ 2.1.1 Image-based single-person pose estimation ‣ 2.1 2D pose estimation
    ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey")提供了基于图像的二维单人姿态估计的总体框架示意图，展示了两种主要方法。'
- en: '![Refer to caption](img/d5477a7a235b611fe722d44f979780b7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/d5477a7a235b611fe722d44f979780b7.png)'
- en: 'Figure 3: The framework of two approaches for image-based 2D SPPE.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于图像的二维 SPPE 两种方法框架。
- en: (1) Regression-based approach
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 基于回归的方法
- en: 'The pioneer work [[30](#bib.bib30)], DeepPose, formulates pose estimation as
    a convolutional neural network(CNN)-based regression task towards body joints.
    A cascade of regressors are adopted to refine the pose estimates, as shown in
    Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.1 Image-based single-person pose estimation ‣
    2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey"). This work could reason
    about pose in a holistic fashion in occlusion situations. Carreira et al. [[31](#bib.bib31)]
    introduced the Iterative Error Feedback approach, wherein prediction errors were
    recursively fed back into the input space, resulting in progressively improved
    estimations. Sun et al. [[32](#bib.bib32)] presented a reparameterized pose representation
    using bones instead of joints. This method defines a compositional loss function
    that captures the long range interactions within the pose by exploiting the joint
    connection structure. In more recent developments,  [[33](#bib.bib33)] introduced
    a novel approach that employed softmax functions to convert heatmaps into coordinates
    in a fully differentiable manner. This innovative technique was coupled with a
    keypoint error distance-based loss function and context-based structures.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 先锋工作 [[30](#bib.bib30)]，DeepPose，将姿态估计定义为基于卷积神经网络（CNN）的回归任务，针对身体关节。采用了一系列回归器来细化姿态估计，如图 [4](#S2.F4
    "图 4 ‣ 2.1.1 基于图像的单人姿态估计 ‣ 2.1 2D 姿态估计 ‣ 2 姿态估计 ‣ 基于深度学习的人体姿态估计、跟踪和动作识别：综述") 所示。这项工作在遮挡情况下能够以整体方式推理姿态。Carreira
    等人 [[31](#bib.bib31)] 引入了迭代误差反馈方法，其中预测误差递归地反馈到输入空间，从而逐渐改进估计。Sun 等人 [[32](#bib.bib32)]
    提出了使用骨骼而非关节的重新参数化姿态表示方法。这种方法定义了一种组合损失函数，通过利用关节连接结构来捕捉姿态中的长程交互。在更近期的发展中，[[33](#bib.bib33)]
    引入了一种新方法，该方法采用 softmax 函数以完全可微分的方式将热图转换为坐标。这种创新技术结合了基于关键点误差距离的损失函数和基于上下文的结构。
- en: '![Refer to caption](img/f3b362baa8c613d36e1af8e7d6e960e5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/f3b362baa8c613d36e1af8e7d6e960e5.png)'
- en: 'Figure 4: The DeepPose architecture [[30](#bib.bib30)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：DeepPose 架构 [[30](#bib.bib30)]。
- en: Subsequently, researchers [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)] began exploring pose estimation methods based on transformer
    architectures. The attention modules in transformers offered the ability to capture
    long-range dependencies and global evidence crucial for accurate pose estimation.
    For example, TFPose [[34](#bib.bib34)] first introduced Transformer to the pose
    estimation framework in a regression-based manner. PRTR [[35](#bib.bib35)] introduced
    a two-stage, end-to-end regression-based framework that employed cascading Transformers,
    achieving state-of-the-art performance among regression-based methods. Mao et
    al. [[36](#bib.bib36)] framed pose estimation as a sequence prediction task, which
    they addressed with the Poseur model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，研究人员 [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]
    开始探索基于 transformer 架构的姿态估计方法。transformers 中的注意力模块提供了捕捉长程依赖和全球证据的能力，这对准确的姿态估计至关重要。例如，TFPose [[34](#bib.bib34)]
    首次将 Transformer 引入姿态估计框架，并采用回归方法。PRTR [[35](#bib.bib35)] 引入了一个两阶段、端到端的回归框架，采用级联
    Transformer，在回归方法中达到了最先进的性能。Mao 等人 [[36](#bib.bib36)] 将姿态估计框架设定为序列预测任务，并通过 Poseur
    模型解决了这一问题。
- en: However, it’s worth noting that these direct regression methods sometimes struggle
    in high-precision scenarios. This limitation may stem from the intricate mapping
    of RGB images to $(x,y)$ locations, adding unnecessary complexity to the learning
    process and hampering generalization. For instance, direct regression may encounter
    challenges when handling multi-modal outputs, where a valid joint appears in two
    distinct spatial locations. The constraint of producing a single output for a
    given regression input can limit the network’s ability to represent small errors,
    potentially leading to over-training.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，这些直接回归方法在高精度场景中有时会遇到困难。这一局限性可能源于将 RGB 图像映射到 $(x,y)$ 位置的复杂过程，给学习过程增加了不必要的复杂性，阻碍了泛化。例如，直接回归在处理多模态输出时可能遇到挑战，其中一个有效的关节点可能出现在两个不同的空间位置。为给定回归输入生成单一输出的限制可能会限制网络表示小误差的能力，进而导致过拟合。
- en: (2) Heatmap-based approach
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 基于热图的方法
- en: Heatmaps have gained substantial attention due to its ability to provide comprehensive
    spatial information, making itself invaluable for training Convolutional Neural
    Networks (CNNs). This has spurred a surge of interest in the development of CNN
    architectures for pose estimation. Jain et al. [[38](#bib.bib38)] pioneered an
    approach where multiple CNNs were trained for independent binary body-part classification,
    with each network dedicated to a specific feature. This strategy effectively constrained
    the network’s outputs to a much smaller class of valid configurations, enhancing
    overall performance. Recognizing the importance of structural domain constraints,
    such as the geometric relationships between body joint locations, Tompson et al. [[39](#bib.bib39)]
    pursued a joint training approach, simultaneously training CNNs and graphical
    models for human pose estimation. Similarly, Chen and Yuille [[40](#bib.bib40)]
    adopt Convnets to learn conditional probabilities for the presence of parts and
    their spatial relationships within image patches. To address the limitations of
    pooling techniques in  [[39](#bib.bib39)] for improving spatial locality precision,
    Tompson et al. [[41](#bib.bib41)] proposed a position refinement model (namely,
    a multi-resolution Convents) that is trained to predict the joint offset location
    within a localized region of the image. The works of  [[39](#bib.bib39)], [[40](#bib.bib40)]
    and [[41](#bib.bib41)] sought to merge the representational flexibility inherent
    in graphical models with the efficiency and statistical power offered by CNNs.
    To avoid using graphical models, Wei et al. [[42](#bib.bib42)] introduced the
    Convolutional Pose Machines to learn long-range spatial relationships without
    explicitly adopting graphical models. Hu and Ramanan [[43](#bib.bib43)] proposed
    an architecture that could be used for multiple stages of predictions, and ties
    weights in the bottom-up and top-down portions of computation as well as across
    iteration. Similarly, Newell et al. [[44](#bib.bib44)] proposed the Stacked Hourglass
    Network (SHN) for single-person pose estimation. The SHN leverages a series of
    successive pooling and upsampling steps to generate a final set of predictions,
    showcasing its efficacy. In addressing challenging scenarios characterized by
    severe part occlusions, Bulat and Tzimiropoulos [[45](#bib.bib45)] presented a
    detection-followed-by-regression CNN cascade. This robust approach adeptly infers
    poses, even in the presence of significant occlusions. Lifshitz et al. [[46](#bib.bib46)]
    introduced a novel voting scheme that harnesses information from the entire image,
    allowing for the aggregation of numerous votes to yield highly accurate keypoint
    detections. Chu et al. [[47](#bib.bib47)] incorporated CNNs into their approach,
    enhancing it with a multi-context attention mechanism for pose estimation. This
    dynamic mechanism autonomously learns and infers contextual representations, directing
    the model’s focus toward regions of interest. Furthermore, Yang et al. [[48](#bib.bib48)]
    devised a Pyramid Residual Module (PRMs) to bolster the scale invariance of CNNs.
    PRMs effectively learn feature pyramids, which prove instrumental in precise pose
    estimation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 热图因其提供全面空间信息的能力而受到广泛关注，这使其在训练卷积神经网络（CNNs）时不可或缺。这激发了对姿态估计的 CNN 架构开发的浓厚兴趣。Jain
    等人 [[38](#bib.bib38)] 首创了一种方法，即训练多个 CNN 用于独立的二进制身体部位分类，每个网络专注于特定特征。这种策略有效地将网络的输出约束到一个较小的有效配置类别，从而提升了整体性能。认识到结构领域约束的重要性，如身体关节位置之间的几何关系，Tompson
    等人 [[39](#bib.bib39)] 采取了联合训练的方法，同时训练 CNN 和图形模型用于人体姿态估计。同样，Chen 和 Yuille [[40](#bib.bib40)]
    采用 Convnets 来学习部件存在及其在图像补丁中空间关系的条件概率。为了应对 [[39](#bib.bib39)] 中池化技术在提高空间局部精度方面的局限性，Tompson
    等人 [[41](#bib.bib41)] 提出了一个位置精细化模型（即多分辨率 Convents），该模型经过训练以预测图像局部区域内的关节偏移位置。 [[39](#bib.bib39)]、 [[40](#bib.bib40)]
    和 [[41](#bib.bib41)] 的研究旨在将图形模型固有的表现灵活性与 CNN 提供的效率和统计能力结合起来。为了避免使用图形模型，Wei 等人 [[42](#bib.bib42)]
    引入了卷积姿态机，以在不显式采用图形模型的情况下学习长距离空间关系。Hu 和 Ramanan [[43](#bib.bib43)] 提出了一个可以用于多个预测阶段的架构，并在自下而上和自上而下的计算部分以及迭代过程中绑定权重。同样，Newell
    等人 [[44](#bib.bib44)] 提出了用于单人姿态估计的堆叠沙漏网络（SHN）。SHN 利用一系列连续的池化和上采样步骤生成最终的预测结果，展示了其有效性。在处理严重部件遮挡的挑战性场景时，Bulat
    和 Tzimiropoulos [[45](#bib.bib45)] 提出了一个检测跟随回归的 CNN 级联。该稳健的方法能够在显著遮挡的情况下精准推断姿态。Lifshitz
    等人 [[46](#bib.bib46)] 引入了一种新颖的投票机制，利用整个图像的信息，通过聚合大量投票来实现高精度的关键点检测。Chu 等人 [[47](#bib.bib47)]
    将 CNN 纳入他们的方法中，增强了一个多上下文注意机制用于姿态估计。该动态机制自动学习和推断上下文表示，将模型的焦点引导到感兴趣的区域。此外，Yang 等人 [[48](#bib.bib48)]
    设计了一个金字塔残差模块（PRMs）来增强 CNN 的尺度不变性。PRMs 有效地学习特征金字塔，这对于精确的姿态估计至关重要。
- en: With the development of Generative Adversarial Networks (GAN) [[49](#bib.bib49)],
    Chen et al. [[50](#bib.bib50)] designed discriminators to distinguish the real
    poses from the fake ones to incorporate priors about the structure of human bodies.
    Ning et al. [[51](#bib.bib51)] proposed to explore external knowledge to guide
    the network training process using learned projections that impose proper prior.
    Sun et al. [[52](#bib.bib52)] presented a two-stage normalization scheme, human
    body normalization and limb normalization, to make the distribution of the relative
    joint locations compact, resulting in easier learning of convolutional spatial
    models and more accurate pose estimation. Marras et al. [[53](#bib.bib53)] introduced
    a Markov Random Field (MRF)-based spatial model network between the coarse and
    the refinement model that introduces geometric constraints on the relative locations
    of the body joints. To deal with annotating pose problem, Liu and Ferrari [[54](#bib.bib54)]
    presented an active learning framework for pose estimation. Ke et al. [[55](#bib.bib55)]
    proposed a multi-scale structure-aware network for human pose estimation. Peng
    et al. [[56](#bib.bib56)] proposed adversarial data augmentation for jointly optimize
    data augmentation and network training. The main idea is to design an augmentation
    network (generator) that competes against a target network (discriminator) by
    generating ”hard” augmentation operations online. Tang et al. [[57](#bib.bib57)]
    introduced a Deeply Learned Compositional Model for pose estimation by exploiting
    deep neural networks to learn compositions of human body. Nie et al. [[58](#bib.bib58)]
    proposed the parsing induced learner including a parsing encoder and a pose model
    parameter adapter, which estimates dynamic parameters in the pose model through
    joint learning to extract complementary useful features for more accurate pose
    estimation. Nie et al. [[59](#bib.bib59)] proposed to jointly conduct human parsing
    and pose estimation in one framework by incorporating information from their counterparts,
    giving more robust and accurate results. Tang and Wu [[60](#bib.bib60)] proposed
    a data-driven approach to group-related parts based on how much information they
    share, and then a part-based branching network (PBN) is introduced to learn representations
    specific to each part group. To speed up the pose estimation, Zhang et al. [[61](#bib.bib61)]
    presented a Fast Pose Distillation (FPD) model that trains a lightweight pose
    neural network architecture capable of executing rapidly with low computational
    cost, by effectively transferring pose structure knowledge of a robust teacher
    network.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成对抗网络（GAN）的发展[[49](#bib.bib49)]，陈等人[[50](#bib.bib50)] 设计了判别器以区分真实姿势和虚假姿势，从而结合有关人体结构的先验知识。宁等人[[51](#bib.bib51)]
    提出了利用外部知识指导网络训练过程的方案，通过学习的投影施加适当的先验。孙等人[[52](#bib.bib52)] 提出了一个两阶段的归一化方案，包括人体归一化和肢体归一化，以使相对关节位置的分布更加紧凑，从而使卷积空间模型的学习更容易，姿势估计更准确。马拉斯等人[[53](#bib.bib53)]
    引入了一种基于马尔可夫随机场（MRF）的空间模型网络，用于粗糙模型和精细模型之间，引入了关于身体关节相对位置的几何约束。为了处理标注姿势的问题，刘和费拉里[[54](#bib.bib54)]
    提出了一个用于姿势估计的主动学习框架。柯等人[[55](#bib.bib55)] 提出了一个多尺度结构感知网络用于人体姿势估计。彭等人[[56](#bib.bib56)]
    提出了对抗数据增强方法，以联合优化数据增强和网络训练。主要思路是设计一个对抗网络（生成器），通过在线生成“困难”增强操作，与目标网络（判别器）进行竞争。唐等人[[57](#bib.bib57)]
    介绍了一种深度学习组合模型，通过利用深度神经网络学习人体的组合。聂等人[[58](#bib.bib58)] 提出了一个解析驱动学习器，包括解析编码器和姿势模型参数适配器，通过联合学习估计姿势模型中的动态参数，以提取有用的补充特征，来提高姿势估计的准确性。聂等人[[59](#bib.bib59)]
    提出了一个框架，将人体解析和姿势估计联合进行，通过结合来自对方的信息，提供更稳健和准确的结果。唐和吴[[60](#bib.bib60)] 提出了一个数据驱动的方法，根据它们共享的信息量将相关部件分组，然后引入了一个基于部件的分支网络（PBN），以学习每个部件组特定的表示。为了加快姿势估计，张等人[[61](#bib.bib61)]
    提出了一个快速姿势蒸馏（FPD）模型，训练一个轻量级的姿势神经网络架构，能够以低计算成本快速执行，通过有效地转移鲁棒教师网络的姿势结构知识。
- en: 'In summary, regression-based methods have advantages in speed but disadvantages
    in accuracy on pose estimation task. Heatmap-based methods can explicitly learn
    spatial information by estimating heatmap likelihood, resulting in high accuracy.
    However, heatmap-based methods suffer seriously a long-standing challenge known
    as the quantization error problem, which is caused by mapping the continuous coordinate
    values into discretized downscaled heatmaps. To address this problem, Li et al [[62](#bib.bib62)]
    proposed a Simple Coordinate Classification (SimCC) method which formulates pose
    estimation as two classification tasks for horizontal and vertical coordinates.
    Despite the improvement in quantization error, the estimation of heatmaps requires
    exceptionally high computational cost, resulting in slow preprocessing operations.
    Therefore, how to take advantage of both heatmap-based and regression-based methods
    remains a challenging problem. Some works [[63](#bib.bib63), [64](#bib.bib64)]
    tend to solve the above problem by transferring the knowledge from heatmap-based
    to regression-based models. However, due to the different output spaces of regression
    models and heatmap models, directly transferring knowledge between heatmaps and
    vectors may result in information loss. To the end, DistilPose [[64](#bib.bib64)]
    (as shown in Fig. [5](#S2.F5 "Figure 5 ‣ 2.1.1 Image-based single-person pose
    estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey")) is proposed to
    transfer heatmap-based knowledge from a teacher model to a regression-based student
    model through token-distilling encoder and simulated heatmaps.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，基于回归的方法在速度上具有优势，但在姿态估计任务上精度不足。基于热图的方法可以通过估计热图的可能性明确学习空间信息，从而实现高精度。然而，基于热图的方法严重受到长期存在的量化误差问题的困扰，该问题由将连续坐标值映射到离散的缩小热图中引起。为了解决这个问题，Li
    等人[[62](#bib.bib62)] 提出了一个简单坐标分类（SimCC）方法，该方法将姿态估计公式化为水平和垂直坐标的两个分类任务。尽管在量化误差方面有所改进，但热图的估计仍需极高的计算成本，导致预处理操作缓慢。因此，如何利用基于热图和基于回归的方法的优势仍然是一个挑战性的问题。一些工作[[63](#bib.bib63),
    [64](#bib.bib64)]倾向于通过将知识从基于热图的方法转移到基于回归的方法来解决上述问题。然而，由于回归模型和热图模型的输出空间不同，直接在热图和向量之间转移知识可能会导致信息丢失。为此，DistilPose[[64](#bib.bib64)]（如图[5](#S2.F5
    "Figure 5 ‣ 2.1.1 Image-based single-person pose estimation ‣ 2.1 2D pose estimation
    ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey")所示）被提出，通过令牌蒸馏编码器和模拟热图，将基于热图的知识从教师模型转移到基于回归的学生模型。'
- en: '![Refer to caption](img/ffc8e2370de50b81fc778ae91c7a1e20.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ffc8e2370de50b81fc778ae91c7a1e20.png)'
- en: 'Figure 5: The DistilPose framework [[64](#bib.bib64)].'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：DistilPose 框架[[64](#bib.bib64)]。
- en: 2.1.2 Image-based multi-person pose estimation
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 基于图像的多人体姿态估计
- en: 'Compared with single-person pose estimation (SPPE), multi-person pose estimation
    (MPPE) is more difficult. First, the number or the position of the person is not
    given, and the pose can occur at any position or scale; second, interactions between
    people induce complex spatial interference, due to contact, occlusion, and limb
    articulations, making association of parts difficult; third, runtime complexity
    tends to grow with the number of people in the image, making realtime performance
    a challenge. MPPE must address both global (human-level) and local (keypoint-level)
    dependencies (as depicted in Fig. [6](#S2.F6 "Figure 6 ‣ 2.1.2 Image-based multi-person
    pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey")), which
    involve different levels of semantic granularity. Mainstream solutions are normally
    two-stage approaches, which divide the problem into two separate subproblems including
    global human detection and local keypoint regression. Typically, two primary frameworks
    have been proposed to tackle these subproblems, known as the top-down and bottom-up
    approaches. Inspired by the success of end-to-end object detection, another viable
    solution is the one-stage approach. This approach aims to develop a fully end-to-end
    trainable method capable of unifying the two disassembled subproblems.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与单人人体姿态估计（SPPE）相比，多人人体姿态估计（MPPE）更具挑战性。首先，人数或位置未给出，姿态可能出现在任何位置或尺度；其次，人际间的互动会引起复杂的空间干扰，由于接触、遮挡和肢体关节，使得部件的关联变得困难；第三，运行时复杂度通常随着图像中人数的增加而增长，使得实时性能成为挑战。MPPE必须解决全球（人类级别）和局部（关键点级别）的依赖关系（如图[6](#S2.F6
    "图6 ‣ 2.1.2 基于图像的多人人体姿态估计 ‣ 2.1 2D 姿态估计 ‣ 2 姿态估计 ‣ 基于深度学习的人体姿态估计、跟踪和动作识别：综述")所示），这涉及不同级别的语义粒度。主流解决方案通常是两阶段的方法，将问题分为包括全局人体检测和局部关键点回归的两个独立子问题。通常，已提出两种主要框架来解决这些子问题，分别称为自上而下和自下而上方法。受到端到端目标检测成功的启发，另一种可行的解决方案是单阶段方法。该方法旨在开发一种完全端到端可训练的方法，能够统一这两个拆分的子问题。
- en: '![Refer to caption](img/7ab30e23ef8c250b552f845ba7e013a5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7ab30e23ef8c250b552f845ba7e013a5.png)'
- en: 'Figure 6: Perception of multi-person pose estimation task [[65](#bib.bib65)].'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：多人人体姿态估计任务的感知 [[65](#bib.bib65)]。
- en: '![Refer to caption](img/f42638a720496ebffaeddeb8e723d55c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f42638a720496ebffaeddeb8e723d55c.png)'
- en: 'Figure 7: The framework of two approaches for image-based 2D MPPE. Part of
    the figure is from [[66](#bib.bib66)].'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：基于图像的2D MPPE的两种方法框架。图的一部分来自 [[66](#bib.bib66)]。
- en: (1) Top-down approach
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 自上而下方法
- en: 'Top-down approaches in multi-person pose estimation begin by detecting all
    individuals within a given image, as shown in Fig. [7](#S2.F7 "Figure 7 ‣ 2.1.2
    Image-based multi-person pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation
    ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey"), and subsequently employ single-person pose estimation techniques within
    each detected bounding box.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 多人人体姿态估计中的自上而下方法首先检测给定图像中的所有个体，如图[7](#S2.F7 "图7 ‣ 2.1.2 基于图像的多人人体姿态估计 ‣ 2.1
    2D 姿态估计 ‣ 2 姿态估计 ‣ 基于深度学习的人体姿态估计、跟踪和动作识别：综述")所示，然后在每个检测到的边界框内使用单人人体姿态估计技术。
- en: A group of methods [[67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75),
    [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)] aim to designing and improving
    modules within pose estimation networks. Papandreou et al. [[67](#bib.bib67)]
    adopt Faster RCNN [[79](#bib.bib79)] for person detection and keypoints estimation
    within the bounding box. They introduce an aggregation procedure to obtain highly
    localized keypoint predictions, along with a keypoint-based Non-Maximum-Suppression
    (NMS) to prevent duplicate pose detection. Sun et al. [[71](#bib.bib71)] proposed
    a novel High-Resolution net(HRNet) to learn such representation. To address systematic
    errors in standard data transformation and encoding-decoding structures that degrade
    top-down pipeline performance, Huang et al. [[73](#bib.bib73)] proposed solutions
    to correct common biased data processing in human pose estimation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一组方法[[67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75),
    [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)]旨在设计和改进姿态估计网络中的模块。**Papandreou等人**[[67](#bib.bib67)]采用Faster
    RCNN[[79](#bib.bib79)]进行人检测和边界框内的关键点估计。他们引入了一种聚合程序，以获得高度本地化的关键点预测，并使用基于关键点的非极大值抑制（NMS）来防止重复姿态检测。**孙等人**[[71](#bib.bib71)]提出了一种新颖的高分辨率网络（HRNet），以学习这种表示。为了解决标准数据变换和编码解码结构中的系统误差，这些误差降低了自上而下管道的性能，**黄等人**[[73](#bib.bib73)]提出了解决方案，以纠正人体姿态估计中的常见偏差数据处理。
- en: Human detectors may fail in the first step of top-down pipeline due to occlusion
    affected by the overlapping of limbs. Another group of works [[80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)] aim to
    address this issue. Fang et al. [[81](#bib.bib81)] proposed a novel Regional Multi-person
    Pose Estimation (RMPE) to facilitate pose estimation even when inaccurate human
    bounding boxes exist. Chen et al. [[82](#bib.bib82)] designed a Cascaded Pyramid
    Network (CPN) that contains GlobalNet and RefineNet for localizing simple and
    hard keypoints with occlusion respectively. Su et al. [[83](#bib.bib83)] proposed
    two novel modules to perform the enhancement of the information for the multi-person
    pose estimation under occluded scenes, namely, Channel Shuffle Module (CSM) and
    Spatial, Channel-wise Attention Residual Bottleneck (SCARB), where CSM promoting
    cross-channel information communication among the pyramid feature maps and SCARB
    highlighting the information of feature maps both in the spatial and channel-wise
    context. An occluded pose estimation and correction module [[84](#bib.bib84)]
    is proposed to solve the occlusion problem in crowd pose estimation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 人体检测器在自上而下管道的第一步中可能会因肢体重叠而受遮挡影响而失败。另一组工作[[80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)]旨在解决这一问题。**方等人**[[81](#bib.bib81)]提出了一种新颖的区域多人人体姿态估计（RMPE）方法，以便在存在不准确的人体边界框时仍能促进姿态估计。**陈等人**[[82](#bib.bib82)]设计了一种级联金字塔网络（CPN），包含GlobalNet和RefineNet，用于分别定位简单和困难的关键点。**苏等人**[[83](#bib.bib83)]提出了两个新颖的模块，以在遮挡场景下执行多人人体姿态估计的信息增强，即通道洗牌模块（CSM）和空间、通道注意残差瓶颈（SCARB），其中CSM促进金字塔特征图之间的跨通道信息通信，而SCARB在空间和通道上下文中突显特征图的信息。提出了一种遮挡姿态估计和校正模块[[84](#bib.bib84)]，以解决人群姿态估计中的遮挡问题。
- en: Much like single-person pose estimation, multi-person pose estimation has also
    undergone rapid advancements, transitioning from CNNs to vision transformer networks.
    Some recent works tend to treat transformer as a better decoder. TransPose [[85](#bib.bib85)]
    processes the features extracted by CNNs to model the global relationship. Zhou
    et al. [[86](#bib.bib86)] proposed a Bottom-Up Conditioned Top-Down pose estimation
    (BUCTD) method which modifies TransPose to accept conditions as side-information
    generated by CTD. Different from other top-down methods, BUCTD applies a bottom-up
    model as a person detector. TokenPose [[87](#bib.bib87)] proposes a token-based
    representation to estimate the locations of occluded keypoints and model the relationship
    among different keypoints. HRFormer [[88](#bib.bib88)] proposes to fuse multi-resolution
    features by a transformer module. The above works either require CNNs for feature
    extraction or careful designs of transformer structures. In contrast, a simple
    yet effective baseline model, ViTPose [[89](#bib.bib89)], is proposed based on
    the plain vision transformers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与单人姿态估计类似，多人姿态估计也经历了快速的发展，从CNNs转向了视觉变换器网络。最近的一些研究倾向于将变换器视为更好的解码器。TransPose [[85](#bib.bib85)]
    处理由CNNs提取的特征，以建模全局关系。Zhou等人 [[86](#bib.bib86)] 提出了一个自下而上的条件化自上而下姿态估计（BUCTD）方法，该方法修改了TransPose，使其接受由CTD生成的条件作为附加信息。与其他自上而下的方法不同，BUCTD将自下而上的模型作为行人检测器。TokenPose [[87](#bib.bib87)]
    提出了基于标记的表示方法，以估计遮挡关键点的位置并建模不同关键点之间的关系。HRFormer [[88](#bib.bib88)] 提出了通过变换器模块融合多分辨率特征。上述工作要么需要CNNs进行特征提取，要么需要精心设计的变换器结构。相比之下，基于普通视觉变换器的简单而有效的基线模型ViTPose [[89](#bib.bib89)]
    被提出。
- en: (2) Bottom-up approach
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 自下而上的方法
- en: In contrast to the top-down approach, the bottom-up approach initially detects
    all individual body parts or keypoints and subsequently associates them with the
    corresponding subjects using part association strategies. The seminal work of
    Pishchulin et al. [[90](#bib.bib90)] proposed a bottom-up approach that jointly
    labels part detection candidates and associates them to individual people. However,
    solving the integer linear programming problem over a fully connected graph is
    an NP-hard problem and the average processing time is on the order of hours. In
    the work by Insafutdinov et al.  [[91](#bib.bib91)], a more robust part detector
    and innovative image-conditioned pairwise terms were proposed to enhance runtime
    efficiency. Nevertheless, this work encountered challenges in precisely regressing
    the pairwise representations and a separate logistic regression is required. Iqbal
    and Gall [[80](#bib.bib80)] considered multi-person pose estimation as a joint-to-person
    association problem. They construct a fully connected graph from a set of detected
    joint candidates in an image and resolve the joint-to-person association and outlier
    detection using integer linear programming. OpenPose [[92](#bib.bib92), [93](#bib.bib93)]
    proposes the first bottom-up representation of association scores via Part Affinity
    Fields (PAFs) which are a set of 2D vector fields that encode the location and
    orientation of limbs over the image domain. Kreiss et al. [[94](#bib.bib94)] proposed
    to use a Part Intensity Field (PIF) for body parts localization and a PAF for
    body part association with each other to form full human poses. To handle missed
    small-scale persons, Cheng et al. [[95](#bib.bib95)] proposed multi-scale training
    and dual anatomical canters to enhance the network. The above methods mainly apply
    heatmap prediction based on overall L2 loss to locate keypoints. However, minimizing
    L2 loss cannot always locate all keypoints since each heatmap often includes multiple
    body joints. To solve this problem, Qu et al. [[96](#bib.bib96)] proposed to optimize
    heatmap prediction based on minimizing the distance between the characteristic
    functions of the predicted and ground-truth heatmaps.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与自上而下的方法相比，自下而上的方法最初检测所有个体身体部位或关键点，然后使用部件关联策略将它们与相应的主体关联起来。Pishchulin 等人[[90](#bib.bib90)]
    的开创性工作提出了一种自下而上的方法，该方法联合标记部件检测候选并将其与个体关联。然而，在完全连接的图上解决整数线性规划问题是一个 NP-难题，平均处理时间通常为几个小时。在
    Insafutdinov 等人[[91](#bib.bib91)] 的工作中，提出了一种更为鲁棒的部件检测器和创新的图像条件成对项以提高运行效率。然而，该工作在精确回归成对表示方面遇到了挑战，需要额外的逻辑回归。Iqbal
    和 Gall[[80](#bib.bib80)] 将多人的姿态估计视为关节到个人的关联问题。他们从图像中检测到的一组关节候选中构建完全连接的图，并使用整数线性规划解决关节到个人的关联和离群值检测。OpenPose[[92](#bib.bib92),
    [93](#bib.bib93)] 提出了通过部件亲和场 (PAFs) 进行的首个自下而上的关联分数表示，PAFs 是一组 2D 向量场，编码了图像域中肢体的位置和方向。Kreiss
    等人[[94](#bib.bib94)] 提议使用部件强度场 (PIF) 进行身体部位定位，并使用 PAF 进行身体部位之间的关联以形成完整的人体姿态。为了解决遗漏的小规模人员问题，Cheng
    等人[[95](#bib.bib95)] 提出了多尺度训练和双重解剖中心以增强网络。上述方法主要应用基于整体 L2 损失的热图预测来定位关键点。然而，最小化
    L2 损失并不能总是定位所有关键点，因为每个热图通常包含多个身体关节。为了解决这个问题，Qu 等人[[96](#bib.bib96)] 提出了通过最小化预测热图与真实热图的特征函数之间的距离来优化热图预测。
- en: Different from the above two-stage bottom-up approach, some works focus on joint
    detection and grouping, which belong to single-stage bottom-up approach. Newell
    et al. [[97](#bib.bib97)] simultaneously produced score maps and pixel-wise embedding
    to group the candidate keypoints among different people to get final multi-person
    pose estimation. Kocabas et al. [[98](#bib.bib98)] designed a MultiPoseNet that
    jointly handle person detection, person segmentation and pose estimation problems,
    by the implementation of Pose Residual Network (PRN) which receives keypoint and
    person detections, and produces accurate poses by assigning keypoints to person
    instances. To deal with the crowded scene, Li et al. [[99](#bib.bib99)] built
    a new benchmark called CrowdPose and proposed two components, namely, joint-candidate
    single-person pose estimation and global maximum joints association, for crowded
    pose estimation. Jin et al. [[100](#bib.bib100)] proposed a new differentiable
    hierarchical graph grouping method to learn human part grouping. Cheng et al. [[101](#bib.bib101)]
    extended the HRNet and proposed a higher resolution network (HigherHRNet) by deconvolving
    the high-resolution hetamaps generated by HRNet to solve the variation challenge.
    Besides the above bottom-up methods, some methods directly regress a set of pose
    candidates from image pixels and the keypoints in each candidate might be from
    the same person. A post-processing step is required to generate the final poses
    which are more spatially accurate. For instance, single-stage multi-person Pose
    Machine (SPM) method [[102](#bib.bib102)] applies a hierarchical structured 2D/3D
    pose representation to assist the long-range regression. The keypoints are predicted
    based on person-agnostic heatmaps so that grouping post-processing is required
    to assemble keypoints to the full-body pose. Disentangled Keypoint Regression
    (DEKR) [[103](#bib.bib103)] regresses pose candidates by learning representations
    that focus on keypoint regions. The pose candidates were scored and ranked to
    generate the final poses based on keypoints and center heatmap estimation loss.
    PolarPose [[104](#bib.bib104)] aims to simplify 2D regression to a classification
    task by performing it in polar coordinate.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述两阶段自下而上的方法不同，一些研究专注于联合检测和分组，这些属于单阶段自下而上的方法。Newell 等人 [[97](#bib.bib97)] 同时生成得分图和逐像素嵌入，以在不同的人之间分组候选关键点，从而得到最终的多人的姿态估计。Kocabas
    等人 [[98](#bib.bib98)] 设计了一个 MultiPoseNet，联合处理人检测、人分割和姿态估计问题，通过 Pose Residual Network
    (PRN) 的实现，该网络接收关键点和人员检测，并通过将关键点分配给人实例来生成准确的姿态。为了处理拥挤的场景，Li 等人 [[99](#bib.bib99)]
    建立了一个新的基准叫做 CrowdPose，并提出了两个组件，即联合候选单人姿态估计和全局最大关节关联，用于拥挤的姿态估计。Jin 等人 [[100](#bib.bib100)]
    提出了一个新的可微分的分层图分组方法来学习人体部件分组。Cheng 等人 [[101](#bib.bib101)] 扩展了 HRNet 并提出了一个更高分辨率的网络
    (HigherHRNet)，通过对 HRNet 生成的高分辨率热图进行反卷积来解决变化挑战。除了上述自下而上的方法外，一些方法直接从图像像素回归一组姿态候选，并且每个候选中的关键点可能来自同一个人。需要后处理步骤来生成更空间准确的最终姿态。例如，单阶段多人的
    Pose Machine (SPM) 方法 [[102](#bib.bib102)] 应用层次化的 2D/3D 姿态表示来辅助长距离回归。关键点基于与人无关的热图进行预测，因此需要分组后处理来将关键点组装到全身姿态中。Disentangled
    Keypoint Regression (DEKR) [[103](#bib.bib103)] 通过学习关注关键点区域的表示来回归姿态候选。姿态候选被评分和排序，以生成基于关键点和中心热图估计损失的最终姿态。PolarPose
    [[104](#bib.bib104)] 旨在通过在极坐标中执行将 2D 回归简化为分类任务。
- en: (3) One-stage approach
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 单阶段方法
- en: The one-stage approach aims to learn an end-to-end network for MPPE without
    person detection and grouping post-processing. Tian et al. [[105](#bib.bib105)]
    first proposed a one-stage method based on DirectPose to directly predict instance
    aware keypoints for all persons from an image. To boost both accuracy and speed,
    Mao et al. [[106](#bib.bib106)] later presented a Fully Convolutional Pose (FCPose)
    estimation framework to build dynamic filters in compact keypoint heads. Meanwhile,
    Shi et al. [[107](#bib.bib107)] designed InsPose, which adaptively adjusts the
    network parameters for each instance. To reduce the effect of false positive poses
    in regression loss, the Single-stage Multi-person Pose Regression (SMPR) network [[108](#bib.bib108)]
    was presented by adapting three positive pose identification strategies for initial
    and final pose regression, and the Non-Maximum Suppression (NMS) step. These methods
    could avoid the need for heuristic grouping in bottom-up methods or bounding-box
    detection and region of interest (RoI) cropping in top-down ones. However, they
    still require hand-crafted operations, like NMS, to remove duplicates in the postprocessing
    stage. To further remove NMS, a multi-person Pose Estimation framework with TRansformers
    (PETR) [[109](#bib.bib109)] regards pose estimation as a set prediction, which
    is the first fully end-to-end framework without any postprocessing. The above
    one-stage methods adopts a pose decoder with randomly initialized pose queries,
    making keypoint matching across persons ambiguous and training convergence slow.
    To this end, Yang et al. [[65](#bib.bib65)] proposed an Explicit box Detection
    process for pose estimation (ED-pose) by realizing each box detection using a
    decoder and cascading them to form an end-to-end framework, making the model fast
    in convergence, precise and scalable.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶段方法旨在学习一个端到端的网络，用于MPPE，无需人物检测和分组后处理。Tian等人[[105](#bib.bib105)] 首次提出了一种基于DirectPose的单阶段方法，直接从图像中预测所有人物的实例感知关键点。为了提高准确性和速度，Mao等人[[106](#bib.bib106)]
    后来提出了一个完全卷积姿态（FCPose）估计框架，以在紧凑的关键点头部构建动态滤波器。与此同时，Shi等人[[107](#bib.bib107)] 设计了InsPose，它为每个实例自适应地调整网络参数。为了减少回归损失中的假阳性姿态的影响，Single-stage
    Multi-person Pose Regression (SMPR) 网络[[108](#bib.bib108)] 通过适应三种正姿态识别策略用于初始和最终姿态回归，以及非极大值抑制（NMS）步骤。这些方法可以避免底向上方法中启发式分组的需求，或顶向下方法中的边界框检测和兴趣区域（RoI）裁剪。然而，它们仍然需要手工操作，例如NMS，以在后处理阶段去除重复项。为了进一步去除NMS，一个具有TRansformers的多人物姿态估计框架（PETR）[[109](#bib.bib109)]
    将姿态估计视为集合预测，这是第一个完全端到端且没有任何后处理的框架。上述一阶段方法采用了具有随机初始化姿态查询的姿态解码器，使得跨人物的关键点匹配变得模糊且训练收敛缓慢。为此，Yang等人[[65](#bib.bib65)]
    提出了一个显式框检测过程用于姿态估计（ED-pose），通过实现每个框的检测并将其级联以形成一个端到端的框架，使模型在收敛上快速、精确且可扩展。
- en: Although the above end-to-end methods have achieved promising performance, they
    rely on complex decoders. For instance, ED-pose includes a human detection decoder
    and a human-to-keypoint detection decoder to detect human and keypoint boxes explicitly.PETR
    includes a pose decoder and a joint decoder. In contrast, Group Pose [[110](#bib.bib110)]
    only uses a simple transformer decoder for pursing efficiency.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述端到端的方法已取得了有希望的性能，但它们依赖于复杂的解码器。例如，ED-pose包括一个人体检测解码器和一个人体到关键点检测解码器，以显式地检测人体和关键点框。PETR包括一个姿态解码器和一个关节解码器。相比之下，Group
    Pose [[110](#bib.bib110)] 仅使用一个简单的变压器解码器以追求效率。
- en: 'In summary, top-down approaches directly leverage existing techniques for single-person
    pose estimation, but suffer from early commitment: if the person detector fails
    as it is prone to do when people are in close proximity, there is no recourse
    to recovery. Furthermore, the runtime of these top-down approaches is proportional
    to the number of people. For each detection, a single-person pose estimator is
    run, thus, the more people there are, the greater the computational cost. In contrast,
    bottom-up approaches are attractive due to their robustness to early commitment
    and the potential to decouple runtime complexity from the number of people in
    the image. Yet, bottom-up approaches do not directly leverage global contextual
    cues from other body parts and individuals. One-stage methods eliminate the intermediate
    operations like grouping, ROI, bounding-box detection, NMS and bypass the major
    shortcomings of both top-down and bottom-up methods.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，自上而下的方法直接利用现有的单人姿态估计技术，但遭受早期承诺的困扰：如果人物检测器失败（当人们在接近时容易出现这种情况），就没有恢复的办法。此外，这些自上而下的方法的运行时间与人数成正比。对于每个检测，都需要运行单人姿态估计器，因此人数越多，计算成本越高。相比之下，自下而上的方法由于对早期承诺的鲁棒性以及将运行时复杂性与图像中人数解耦的潜力而显得有吸引力。然而，自下而上的方法并不直接利用来自其他身体部位和个体的全局上下文信息。一阶段的方法消除了中间操作，如分组、ROI、边界框检测、NMS，绕过了自上而下和自下而上的方法的主要缺点。
- en: 2.1.3 Video-based single-person pose estimation
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 基于视频的单人姿态估计
- en: 'Video-based pose estimation aims to estimate single or multiple poses in each
    video frame. Compared with image-based pose estimation, it is more challenging
    due to high variation in human pose and foreground appearance such as clothing
    and self-occlusion. For video-based pose estimation, human tracking is not considered
    in the video. Similar to image-based SPPE, direct regression and heatmap-based
    approaches are also available for video-based SPPE. However, differently, video-based
    pose estimation has the advantage of temporal information, which can enhance the
    accuracy of pose estimation but can also introduce additional computational overhead
    due to temporal redundancy. Therefore, achieving a balance between accuracy and
    efficiency is paramount for video-based pose estimation. Based on handling the
    efficiency, video-based SPPE approaches are categorized into the frame-by-frame
    approach and sample frames-based ones. Fig. [8](#S2.F8 "Figure 8 ‣ 2.1.3 Video-based
    single-person pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey")
    illustrates the general framework of two approaches for video-based SPPE.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '基于视频的姿态估计旨在估计每个视频帧中的单个或多个姿态。与基于图像的姿态估计相比，它更具挑战性，因为人体姿态和前景外观（如服装和自遮挡）变化很大。对于基于视频的姿态估计，视频中的人体跟踪并未考虑。类似于基于图像的SPPE，直接回归和基于热图的方法也适用于基于视频的SPPE。然而，与图像相比，基于视频的姿态估计具有时间信息的优势，这可以提高姿态估计的准确性，但也可能因时间冗余引入额外的计算开销。因此，实现准确性与效率之间的平衡对于基于视频的姿态估计至关重要。根据处理效率，基于视频的SPPE方法分为帧-by-帧方法和基于采样帧的方法。图[8](#S2.F8
    "Figure 8 ‣ 2.1.3 Video-based single-person pose estimation ‣ 2.1 2D pose estimation
    ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey")展示了基于视频的SPPE的两种方法的总体框架。'
- en: (1) Frame-by-frame approach
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 帧-by-帧方法
- en: 'The frame-by-frame approach, illustrated in Fig. [8](#S2.F8 "Figure 8 ‣ 2.1.3
    Video-based single-person pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation
    ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey"), focuses on estimating poses individually for each frame in the video
    sequence. With the success of image-based pose estimation, this category of methods
    mainly apply image-based pose estimation methods on each video frame by incorporating
    temporal information to keep geometric consistency across frames. The temporal
    information is normally captured by fusion from concatenated consecutive frames,
    applying 3D temporal convolution, using dense optical flow and pose propagation.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '帧-by-帧方法，如图[8](#S2.F8 "Figure 8 ‣ 2.1.3 Video-based single-person pose estimation
    ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey")所示，专注于对视频序列中的每一帧单独估计姿态。随着基于图像的姿态估计的成功，这类方法主要在每个视频帧上应用基于图像的姿态估计方法，通过结合时间信息保持帧间几何一致性。时间信息通常通过融合连续帧、应用3D时间卷积、使用密集光流和姿态传播来捕捉。'
- en: '![Refer to caption](img/37b6da96412bd3902bee616144addac8.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/37b6da96412bd3902bee616144addac8.png)'
- en: 'Figure 8: The framework of two approaches for video-based 2D SPPE.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：基于视频的二维SPPE的两种方法框架。
- en: In the early stages of this approach, Pfister et al. [[111](#bib.bib111)] proposed
    to use deep ConvNets for estimating human pose in videos. They designed a regression
    layer to predict the location of upper-body joints while considering temporal
    information through the direct processing of concatenated consecutive frames along
    the channel axis. Grinciunaite et al. [[112](#bib.bib112)] extended 2D convolution
    into 3D convolution and temporal information can be efficiently represented in
    the third dimension of 3D convolutional for video-based human pose estimation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法的早期阶段，Pfister等人[[111](#bib.bib111)] 提出了使用深度卷积网络（ConvNets）来估计视频中的人体姿态。他们设计了一个回归层来预测上半身关节的位置，同时通过直接处理沿通道轴连接的连续帧来考虑时间信息。Grinciunaite等人[[112](#bib.bib112)]
    将二维卷积扩展到三维卷积，并且时间信息可以在三维卷积的第三维度中有效表示，用于基于视频的人体姿态估计。
- en: Some works tend to use optical flow to produce smooth movement. Pfister et al. [[113](#bib.bib113)]
    used dense optical flow to predict joint positions for all neighboring frames
    and design spatial fusion layers to learn dependencies between the human parts
    locations. Song et al. [[114](#bib.bib114)] also utilized optical flow warping
    to capture high temporal consistency and propose spatio-temporal message passing
    layer to incorporate domain-specific knowledge into deep networks. Jain et al. [[115](#bib.bib115)]
    use Local Contrast Normalization and Local Motion Normalization to process the
    RGB image and optical-flow features respectively and then combine them to feed
    into Part-Detector network. These methods have high complexity due to dense flowing
    computation, making them not applicable in real-time applications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究倾向于使用光流来生成平滑的运动。Pfister等人[[113](#bib.bib113)] 使用密集光流预测所有相邻帧的关节位置，并设计了空间融合层来学习人体部位位置之间的依赖关系。Song等人[[114](#bib.bib114)]
    也利用光流变形捕捉高时间一致性，并提出了时空消息传递层，将领域特定知识融入深度网络中。Jain等人[[115](#bib.bib115)] 分别使用局部对比度归一化和局部运动归一化处理RGB图像和光流特征，然后将它们结合起来输入到部件检测网络中。这些方法由于密集的流计算而具有较高的复杂性，因此不适用于实时应用。
- en: 'Subsequently, some works [[116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118),
    [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122),
    [123](#bib.bib123), [124](#bib.bib124)] apply pose propagation which transfer
    features from previous frames to the current frame in an online fashion. For example,
    Charles et al. [[117](#bib.bib117)] proposed a personalized ConvNet to estimate
    human pose including four stages: initial annotation, spatial matching, temporal
    propagation, and self evaluation. In the initial annotation stage, high-precision
    pose estimation is obtained by using flowing Convnets. Then Image patches from
    the new frames without annotations are matched to image patches of body joints
    in frames with annotations by spatial matching process. Dense optical flow is
    used for temporal propagation. Finally, the quality of the spatial-temporal propagated
    annotations is automatically evaluated to optimize the model. Luo et al. [[118](#bib.bib118)]
    proposed Long Short-Term Memory (LSTM) pose machines by combining Convolutional
    Pose Machine (CPM) [[42](#bib.bib42)] and LSTM network learning the temporal dependency
    among video frames to effectively capture the geometric relationships of joints
    in space and time. Nie et al. [[119](#bib.bib119)] designed a Dynamic Kernel Distillation
    (DKD) model. The DKD model introduces a pose kernel distillator and transmits
    pose knowledge in time. Xu et al. [[122](#bib.bib122)] proposed a novel neural
    architecture search to select the most effective temporal feature fusion for optimizing
    the accuracy and speed across video frames. Dang et al. [[123](#bib.bib123)] proposed
    a Relation-based Pose Semantics Transfer Network (RPSTN) by designing a joint
    relation-guided pose semantic propagator to learn the temporal semantic continuity
    of poses. Despite various strategies are applied to reduce computation cost, this
    category of methods still leads to sub-optimal efficiency improvement due to the
    estimation frame by frame.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，一些研究[[116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123),
    [124](#bib.bib124)] 应用了姿态传播技术，这种技术以在线方式将特征从之前的帧转移到当前帧。例如，Charles 等人[[117](#bib.bib117)]
    提出了一个个性化的 ConvNet 来估计人体姿态，包括四个阶段：初始标注、空间匹配、时间传播和自我评估。在初始标注阶段，通过使用流动的 Convnets
    获得高精度的姿态估计。然后，通过空间匹配过程将没有标注的新帧的图像块与有标注的帧中身体关节的图像块进行匹配。密集光流被用于时间传播。最后，空间-时间传播的标注质量被自动评估以优化模型。Luo
    等人[[118](#bib.bib118)] 提出了结合卷积姿态机器（CPM）[[42](#bib.bib42)] 和长短期记忆（LSTM）网络的姿态机器，通过学习视频帧之间的时间依赖关系来有效捕捉关节在空间和时间中的几何关系。Nie
    等人[[119](#bib.bib119)] 设计了一个动态内核蒸馏（DKD）模型。DKD 模型引入了一个姿态内核蒸馏器，并在时间上传递姿态知识。Xu 等人[[122](#bib.bib122)]
    提出了一个新颖的神经结构搜索方法，以选择最有效的时间特征融合，以优化视频帧的准确性和速度。Dang 等人[[123](#bib.bib123)] 通过设计一个联合关系引导的姿态语义传播器，提出了基于关系的姿态语义转移网络（RPSTN），以学习姿态的时间语义连续性。尽管采用了各种策略来减少计算成本，但由于逐帧估计，这类方法仍然导致效率提升不理想。
- en: (2) Sample frames-based approach
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 基于样本帧的方法
- en: 'This category of approach aims to recover all poses based on the estimated
    poses from selected frames. As shown in Fig. [8](#S2.F8 "Figure 8 ‣ 2.1.3 Video-based
    single-person pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey"),
    the general workflow includes sample pose estimation and all poses recovering.
    One line of works generates sample poses by selecting keyframes and estimating
    the poses of keyframes. For example, Zhang et al [[125](#bib.bib125)] introduced
    a Key-Frame Proposal Network (K-FPN) to select informative frames and a human
    pose interpolation module to generate all poses from the poses in keyframes based
    on human pose dynamics. Pose dynamic-based dictionary formulation may become challenging
    when the pose sequence to be interpolated becomes complex. Therefore, to effectively
    exploit the dynamic information, REinforced MOtion Transformation nEtwork (REMOTE) [[126](#bib.bib126)]
    includes a motion transformer to conduct cross frame reconstruction. Although
    the computational efficiency of the above works is improved due to keyframes,
    they still require to take cost on keyframe selection, making it hard to further
    reduce the complexity. To solve this problem, Zeng et al. [[127](#bib.bib127)]
    proposed a novel Sample-Denoise-Recover pipeline (namely DeciWatch) to uniformly
    sample less than 10% of video frames for estimation. The estimatied poses based
    on sample frames are denoised with a Transformer architecture and the rest poses
    are also recovered by another Transformer network. DeciWatch can be used in both
    2D/3D pose estimation from videos and it can maintain or even improve the pose
    estimation accuracy as the previous methods with small cost on computation. Although
    uniform sampling reduces the cost of selecting keyframes, a refinement module
    is added to clean noisy poses. In contrast, MixSynthFormer [[128](#bib.bib128)]
    deletes the refinement module by combining a transformer encoder with an MLP-based
    mixed synthetic attention, thus pursing highly efficient 2D/3D video-based pose
    estimation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '这一类别的方法旨在基于从选定帧估计的姿态来恢复所有姿态。如图 [8](#S2.F8 "Figure 8 ‣ 2.1.3 Video-based single-person
    pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") 所示，整体工作流程包括样本姿态估计和所有姿态恢复。一些工作通过选择关键帧并估计关键帧的姿态来生成样本姿态。例如，Zhang
    等人 [[125](#bib.bib125)] 引入了关键帧提议网络 (K-FPN) 来选择信息丰富的帧，并通过人体姿态插值模块基于人体姿态动态从关键帧的姿态生成所有姿态。当待插值的姿态序列变得复杂时，基于姿态动态的字典构造可能会变得具有挑战性。因此，为了有效利用动态信息，REinforced
    MOtion Transformation nEtwork (REMOTE) [[126](#bib.bib126)] 包含一个运动变换器来进行跨帧重建。尽管由于关键帧，以上工作的计算效率有所提高，但它们仍需要在关键帧选择上付出代价，这使得进一步降低复杂度变得困难。为了解决这个问题，Zeng
    等人 [[127](#bib.bib127)] 提出了一个新颖的样本去噪恢复管道（即 DeciWatch），通过均匀采样不到 10% 的视频帧进行估计。基于样本帧的估计姿态通过
    Transformer 架构进行去噪，其余姿态也通过另一个 Transformer 网络进行恢复。DeciWatch 可用于视频中的 2D/3D 姿态估计，并且可以在计算成本较小的情况下保持甚至提高姿态估计精度。尽管均匀采样减少了选择关键帧的成本，但添加了一个细化模块以清理噪声姿态。相比之下，MixSynthFormer
    [[128](#bib.bib128)] 通过将 Transformer 编码器与基于 MLP 的混合合成注意力相结合，删除了细化模块，从而追求高效的 2D/3D
    视频姿态估计。'
- en: Overall, frame-by-frame approaches could benefit from image-based pose estimation
    but suffer from the computation complexity. Sample frame-based approaches offer
    a solution to improve efficiency but raise questions about how to obtain sample
    frames and recover poses. The paper employs uniform sampling; however, considering
    the significant variations in joint movements under different actions, an adaptive
    sampling strategy might be more suitable for further enhancing efficiency. Additionally,
    the design of dynamic recovery methods should be explored to handle non-uniform
    sampling effectively.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，逐帧方法可以从基于图像的姿态估计中受益，但会受到计算复杂度的影响。样本帧方法提供了一种提高效率的解决方案，但引发了如何获取样本帧和恢复姿态的问题。本文采用了均匀采样；然而，考虑到在不同动作下关节运动的显著变化，适应性采样策略可能更适合进一步提高效率。此外，还应探索动态恢复方法的设计，以有效处理非均匀采样。
- en: 2.1.4 Video-based multi-person pose estimation
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 基于视频的多人体姿态估计
- en: Given the video-based SPPE just introduced, it is natural to extend them to
    handle multiple individuals. Following the taxonomy of video-based SPPE, most
    video-based MPPE approaches fall into frame-by-frame category. They can be achieved
    by employing image-based MPPE frame by frame. Therefore, the approaches of video-based
    MPPE can be categorized into Top-down and Bottom-up approaches.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于刚刚介绍的视频基础SPPE，将其扩展到处理多个人物是自然而然的。根据视频基础SPPE的分类，大多数视频基础MPPE方法都属于逐帧处理类别。通过逐帧使用基于图像的MPPE可以实现这些方法。因此，视频基础MPPE的方法可以分为自上而下和自下而上两种方法。
- en: (1) Top-down approach
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 自上而下的方法
- en: 'Top-down approaches mainly estimate poses by first detecting all persons for
    all frames and then conducting image-based single-person pose estimation frame
    by frame. Xiao et al. [[69](#bib.bib69)] proposed a simple baseline based on ResNet
    to estimate poses in each frame and the estimated poses were then tracked based
    on optical flow. Xiu et al. [[129](#bib.bib129)] estimated multiple poses for
    each frame based on RMPE method which can be replaced by other top-down methods
    for image-based MPPE. With the estimated poses in each frame, a Pose Flow Builder
    (PF-Builder) is proposed for building the association of cross-frame poses by
    maximizing overall confidence along the temporal sequence (as shown in Fig. [9](#S2.F9
    "Figure 9 ‣ 2.1.4 Video-based multi-person pose estimation ‣ 2.1 2D pose estimation
    ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey")), and a Pose Flow Non-Maximum Suppression (PF-NMS)
    is designed to robustly reduce redundant pose flows and re-link temporal disjoint
    ones. Girdhar et al. [[130](#bib.bib130)] estimated poses for each frame based
    on Mask R-CNN and then generated keypoint predictions linked over the video by
    lightweight tracking. Wang et al. [[131](#bib.bib131)] proposed a clip tracking
    network to perform pose estimation and tracking simultaneously. To construct the
    clip tracking network, the 3D HRNet is proposed for estimating poses which incorporating
    temporal dimension into the original HRNet. AlphaPose [[132](#bib.bib132)] is
    also proposed for joint pose estimation and tracking. In particular, all persons
    for each frame are firstly detected using off-the-shelf object detectors like
    YoloV3 or EfficientDet. To solve the quantization error, the symmetric integral
    keypoints regression method is then proposed to localize keypoints in different
    scales accurately. Pose-guided alignment module is applied on the predicted human
    re-id feature to obtain pose-aligned human re-id features after removing redundant
    poses based on NMS. At last, a pose-aware identity embedding is presented to produce
    tracking identity. Estimating poses frame by frame ignores motion dynamics which
    is fundamentally important for accurate pose estimation from videos. A recent
    method [[133](#bib.bib133)] presents Temporal Difference Learning based on Mutual
    Information (TDMI) for pose estimation. A multi-stage temporal difference encoder
    was designed for learning informative motion representations and a representation
    disentanglement module was introduced to distill task-relevant motion features
    to enhance frame representation for pose estimation. The temporal difference features
    can be applied in pose tracking by measuring the similarity of motions for data
    association. Gai et al. [[134](#bib.bib134)] proposed a Sptiotemporal Learning
    Transformer for video-based Pose estimation (SLT-Pose) to capture the shallow
    feature information. With the introduction of diffusion models in computer vision
    tasks (eg. image segmentation [[135](#bib.bib135)], object detection [[136](#bib.bib136)]),
    DiffPose [[137](#bib.bib137)] is the first diffusion model and formulates video-based
    pose estimation as a conditional heatmap generation problem.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '自顶向下的方法主要通过首先检测所有帧中的所有人，然后逐帧进行基于图像的单人姿态估计来估计姿态。Xiao 等人 [[69](#bib.bib69)] 提出了一个基于
    ResNet 的简单基线来估计每一帧的姿态，并且估计的姿态随后基于光流进行跟踪。Xiu 等人 [[129](#bib.bib129)] 基于 RMPE 方法估计每帧的多个姿态，这些方法可以替换为其他自顶向下的方法进行基于图像的
    MPPE。利用每帧的估计姿态，提出了一个 Pose Flow Builder (PF-Builder) 来通过最大化整个时间序列的总体置信度来建立跨帧姿态的关联（如图 [9](#S2.F9
    "Figure 9 ‣ 2.1.4 Video-based multi-person pose estimation ‣ 2.1 2D pose estimation
    ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey")所示），并且设计了一个 Pose Flow Non-Maximum Suppression (PF-NMS)
    来稳健地减少冗余的姿态流并重新连接时间上不连续的姿态。Girdhar 等人 [[130](#bib.bib130)] 基于 Mask R-CNN 估计每帧的姿态，然后通过轻量级跟踪生成视频中的关键点预测。Wang
    等人 [[131](#bib.bib131)] 提出了一个剪辑跟踪网络以同时执行姿态估计和跟踪。为了构建剪辑跟踪网络，提出了 3D HRNet 来估计姿态，将时间维度融入原始
    HRNet。AlphaPose [[132](#bib.bib132)] 也被提出用于联合姿态估计和跟踪。特别是，每一帧的所有人首先通过现成的目标检测器如
    YoloV3 或 EfficientDet 进行检测。为了解决量化误差，提出了对称积分关键点回归方法，以准确地在不同尺度下定位关键点。基于 NMS 移除冗余姿态后，在预测的人体
    re-id 特征上应用姿态引导对齐模块，以获得姿态对齐的人体 re-id 特征。最后，提出了姿态感知身份嵌入以生成跟踪身份。逐帧估计姿态忽略了运动动态，这对于从视频中准确估计姿态至关重要。最近的方法 [[133](#bib.bib133)]
    提出了基于互信息的时间差学习 (TDMI) 来进行姿态估计。设计了一个多阶段时间差编码器来学习信息丰富的运动表示，并引入了表示解耦模块以提取与任务相关的运动特征，以增强姿态估计的帧表示。时间差特征可以通过测量运动相似性来应用于姿态跟踪。Gai
    等人 [[134](#bib.bib134)] 提出了一个用于视频基础姿态估计的时空学习 Transformer (SLT-Pose) 以捕捉浅层特征信息。随着扩散模型在计算机视觉任务中的引入（例如图像分割 [[135](#bib.bib135)]，目标检测 [[136](#bib.bib136)]），DiffPose [[137](#bib.bib137)]
    是第一个扩散模型，并将视频基础姿态估计公式化为条件热图生成问题。'
- en: '![Refer to caption](img/267096815811dc42f1024c0ab497fe5a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/267096815811dc42f1024c0ab497fe5a.png)'
- en: 'Figure 9: The Pose Flow framework [[129](#bib.bib129)].'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: Pose Flow 框架 [[129](#bib.bib129)]。'
- en: (2) Bottom-up approach
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 自下而上的方法
- en: Bottom-up approaches estimate poses by applying body part detection and grouping
    frame by frame. For example, one of the commonly used image-based MPPE methods,
    OpenPose[[93](#bib.bib93)], can be also applied for MPPE from video by directly
    estimating poses frame by frame. Jin et al. [[138](#bib.bib138)] proposed a Pose-Guided
    Grouping (PGG) network for joint pose estimation and tracking. PGG consists of
    two components including SpatialNet and TemporalNet. SpatialNet tackles multi-person
    pose estimation by body part detection and part-level spatial grouping for each
    frame. TemporalNet extends SpatialNet to deal with online human-level temporal
    grouping.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自下而上的方法通过逐帧应用身体部位检测和分组来估计姿态。例如，常用的基于图像的 MPPE 方法之一 OpenPose[[93](#bib.bib93)]，也可以通过直接逐帧估计姿态来应用于视频中的
    MPPE。Jin 等人 [[138](#bib.bib138)] 提出了一个 Pose-Guided Grouping (PGG) 网络，用于联合姿态估计和跟踪。PGG
    由两个组件组成，包括 SpatialNet 和 TemporalNet。SpatialNet 通过身体部位检测和每帧的部位级空间分组来处理多人的姿态估计。TemporalNet
    扩展了 SpatialNet，以处理在线的人的时间分组。
- en: Overall, 2D HPE has been significantly improved with the development of deep
    learning techniques. For the image-based SPPE, heatmap-based approaches generally
    outperform regression-based ones in accuracy but may be of challenge in the quantization
    error problem. When extending SPPE to MPPE, both top-down and bottom-up approaches
    have their advantages and disadvantages. Moreover, both approaches have a challenge
    of reliable detection of individual persons under significant occlusion. Person
    detector in top-down approaches may fail in identifying the boundaries of overlapped
    human bodies. Body part association for occluded scenes may fail in bottom-up
    approaches. One-stage approaches bypass both the shortcomings of top-down and
    bottom-up ones, yet they are still less frequently used. With the advancement
    of image-based pose estimation, it is natural to extend it to videos by directly
    applying off-the-shelf image-based pose estimation methods frame by frame or incorporating
    a temporal network. Sample frames-based methods are preferred for the pose estimation
    from videos since they can largely improve efficiency without looking at all frames,
    while they have been used less in the video-based MPPE. Considering the benefits
    of one-stage approaches for image-based MPPE, more effort is required to explore
    one-stage approaches for video-based ones.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，随着深度学习技术的发展，2D HPE 已经取得了显著进展。对于基于图像的 SPPE，基于热图的方法在准确性上通常优于基于回归的方法，但在量化误差问题上可能面临挑战。将
    SPPE 扩展到 MPPE 时，顶端向下和自下而上的方法各有优缺点。此外，两种方法在严重遮挡下可靠检测个体也存在挑战。顶端向下方法中的人员检测器可能无法识别重叠人体的边界。自下而上的方法在遮挡场景下的身体部位关联可能失败。单阶段方法绕过了顶端向下和自下而上的方法的缺点，但仍然使用较少。随着基于图像的姿态估计的进展，自然可以通过直接逐帧应用现成的基于图像的姿态估计方法或结合时间网络，将其扩展到视频中。基于样本帧的方法在视频中的姿态估计中更受欢迎，因为它们可以大大提高效率而不需要查看所有帧，但在基于视频的
    MPPE 中使用较少。考虑到单阶段方法在基于图像的 MPPE 中的优势，需要更多的努力来探索单阶段方法在基于视频的 MPPE 中的应用。
- en: 2.2 3D pose estimation
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 3D 姿态估计
- en: Generally speaking, recovering 3D pose is considered more difficult than 2D
    pose estimation, due to the larger 3D pose space and more ambiguities. An algorithm
    has to be invariant to some factors, including background scenes, lighting, clothing
    shape and texture, skin color, and image imperfections, among others.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，恢复 3D 姿态被认为比 2D 姿态估计更困难，因为 3D 姿态空间更大且存在更多模糊性。算法必须对一些因素保持不变，包括背景场景、光照、衣物形状和纹理、肤色以及图像缺陷等。
- en: 2.2.1 Image-based single-person pose estimation
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 基于图像的单人姿态估计
- en: 'Imaged-based single-person 3D human pose estimation (HPE) can be classified
    into skeleton-based and mesh-based approaches. The former one estimates 3D human
    joints as the final output and the latter one is required to reconstruct 3D human
    mesh representation. Since this paper focuses only on the kinematic model-based
    human representation, we only review skeleton-based approaches which can be further
    categorized into one-step pose estimation and two-steps pose estimation (recover
    3D pose from 2D pose). Fig. [10](#S2.F10 "Figure 10 ‣ 2.2.1 Image-based single-person
    pose estimation ‣ 2.2 3D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") shows
    the general framework of the two approaches for image-based 3D SPPE.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像的单人3D人体姿势估计（HPE）可以分为基于骨架的方法和基于网格的方法。前者将3D人体关节作为最终输出，后者则需要重建3D人体网格表示。由于本文仅关注基于运动学模型的人体表示，因此我们只审视基于骨架的方法，这些方法可以进一步分为一步姿势估计和两步姿势估计（从2D姿势恢复3D姿势）。图 [10](#S2.F10
    "图 10 ‣ 2.2.1 基于图像的单人姿势估计 ‣ 2.2 3D姿势估计 ‣ 2 姿势估计 ‣ 基于深度学习的人体姿势估计、跟踪和动作识别综述") 显示了图像基于的3D单人姿势估计的两种方法的一般框架。
- en: '![Refer to caption](img/f46a04c720931320ef3cbd412de4d54a.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f46a04c720931320ef3cbd412de4d54a.png)'
- en: 'Figure 10: The framework of two approaches for image-based 3D SPPE.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 图像基于的3D单人姿势估计的两种方法的框架。'
- en: (1) One-stage approach
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 一阶段方法
- en: 'This category of approaches directly infer 3D pose from images without estimating
    2D pose representation. Li and Chan [[139](#bib.bib139)] first proposed to estimate
    3D poses from monocular images using ConvNets. The framework consists of two types
    of tasks: joint point regression and joint point detection. Both tasks take bounding
    box images containing human subjects as input. The regression task aims to estimate
    the positions of joint points relative to the root joint position, while each
    detection task classifies whether one specific joint is present in the local window
    or not.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法直接从图像中推断3D姿势，而不需要估计2D姿势表示。Li和Chan [[139](#bib.bib139)] 首次提出使用卷积网络从单目图像中估计3D姿势。该框架包括两种任务：关节点回归和关节点检测。两种任务都以包含人类对象的边界框图像作为输入。回归任务旨在估计相对于根关节位置的关节点位置，而每个检测任务则分类判断特定的关节是否存在于局部窗口中。
- en: The multi-task learning framework is the first to show that deep neural networks
    can be applied to 3D human pose estimation from single images. However, one drawback
    of these regression-based methods is their limitation in predicting only one pose
    for a given image This may cause difficulties in images where the pose is ambiguous
    due to partial self-occlusion, and hence several poses might be valid. In contrast,
    Li et al. [[140](#bib.bib140)] proposed a unified framework for maximum-margin
    structured learning with a deep neural network for 3D human pose estimation, where
    the unified framework can jointly learn the image and pose feature representations
    and the score function. Tekin et al. [[141](#bib.bib141)] introduced an architecture
    relying on an overcomplete auto-encoder to learn a high-dimensional latent pose
    representation for joint dependencies. Zhou et al. [[142](#bib.bib142)] proposed
    a novel method which directly embeds a kinematic object model into the deep neutral
    network learning, where the kinematic function is defined on the appropriately
    parameterized object motion variables. Mehta et al. [[143](#bib.bib143)] explored
    transfer learning to leverage the highly relevant middle and high-level features
    from 2D pose datasets in conjunction with the existing annotated 3D pose datasets.
    Similarly, Zhou et al. [[144](#bib.bib144)] introduced a Weakly-supervised Transfer
    Learning (WTL) method that employs mixed 2D and 3D labels in a unified deep neural
    network, which is end-to-end and fully exploits the correlation between the 2D
    pose and depth estimation sub-tasks. Since regressing directly from image space,
    one-step-based methods often require a high computation cost.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习框架首次展示了深度神经网络可以应用于从单张图像中进行3D人体姿态估计。然而，这些基于回归的方法的一个缺点是它们只能预测给定图像的一个姿态。这可能会导致在姿态因部分自遮挡而模糊的图像中遇到困难，因此多个姿态可能是有效的。相反，Li等人[[140](#bib.bib140)]提出了一个统一框架，用于最大边际结构学习，结合深度神经网络进行3D人体姿态估计，其中统一框架可以联合学习图像和姿态特征表示以及评分函数。Tekin等人[[141](#bib.bib141)]引入了一种依赖于超完备自编码器的架构，用于学习高维潜在姿态表示以获取关节依赖关系。Zhou等人[[142](#bib.bib142)]提出了一种新颖的方法，该方法将运动学对象模型直接嵌入到深度神经网络学习中，其中运动学函数定义在适当参数化的对象运动变量上。Mehta等人[[143](#bib.bib143)]探索了迁移学习，以利用2D姿态数据集中与现有标注的3D姿态数据集高度相关的中层和高层特征。类似地，Zhou等人[[144](#bib.bib144)]提出了一种弱监督迁移学习（WTL）方法，该方法在一个统一的深度神经网络中使用混合的2D和3D标签，该网络端到端地充分利用了2D姿态和深度估计子任务之间的相关性。由于直接从图像空间回归，一步法通常需要高计算成本。
- en: (2) Two-stage approach
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 两阶段方法
- en: 'This category of approaches infer 3D pose from the intermediately estimated
    2D pose. They are often conducted in two steps: 1) estimating 2D pose based on
    image-based single-person 2D pose estimation methods. 2) Lifting the 2D pose to
    3D pose through a simple regressor. For instance, Martinez et al. [[145](#bib.bib145)]
    proposed a simple baseline based on a fully connected residual network to regress
    3D poses from 2D poses. This baseline method achieves good results at that time,
    however, it could fail due to reconstruction ambiguity of over-reliance on 2D
    pose detector. To overcome this problem, several techniques are applied such as
    replacing 2D poses with heatmaps for estimating 3D poses [[146](#bib.bib146),
    [147](#bib.bib147)], regressing 3D poses from 2D poses and depth information [[148](#bib.bib148),
    [149](#bib.bib149)], selecting best 3D poses from 3D pose hypotheses using ranking
    networks [[150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152)].'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法从中间估计的2D姿态推断3D姿态。它们通常分为两步：1）基于图像的单人2D姿态估计方法估计2D姿态。2）通过简单的回归器将2D姿态提升为3D姿态。例如，Martinez等人[[145](#bib.bib145)]提出了一个基于全连接残差网络的简单基线，从2D姿态回归3D姿态。这个基线方法在当时取得了不错的结果，但由于过度依赖2D姿态检测器的重建模糊性，它可能会失败。为了克服这个问题，应用了几种技术，如用热图替代2D姿态以估计3D姿态[[146](#bib.bib146),
    [147](#bib.bib147)]，从2D姿态和深度信息回归3D姿态[[148](#bib.bib148), [149](#bib.bib149)]，使用排名网络从3D姿态假设中选择最佳3D姿态[[150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)]。
- en: With the introduction of Graph convolutional networks(GCN)-based representation
    for human joints, some methods [[153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159),
    [160](#bib.bib160), [161](#bib.bib161)] apply GCN for lifting 2D to 3D poses.
    To overcome the limitations of shared weights in GCN, a locally connected network
    (LCN) [[153](#bib.bib153)] was proposed which leverages a fully connected network
    and GCN to encode the relationship among joints. Similarly, Zhao et al. [[154](#bib.bib154)]
    proposed a semantic-GCN to learn channel-wise weights for edges. A Pose2Mesh [[155](#bib.bib155)]
    based on GCN was proposed to refine the intermediate 3D pose from its PoseNet.
    Xu and Takano [[159](#bib.bib159)] proposed a Graph Stacked Hourglass (GraphSH)
    networks which consists of repeated encoder-decoder for representing three different
    scales of human skeletons. To overcome the loss of joint interactions in current
    GCN methods, Zhai et al. [[162](#bib.bib162)] proposed Hop-wise GraphFormer with
    Intragroup Joint Refinement (HopFIR) for lifting 3D poses.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于图卷积网络（GCN）的骨骼关节表示方法的引入，一些方法[[153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159),
    [160](#bib.bib160), [161](#bib.bib161)]应用GCN将2D姿态提升到3D姿态。为了克服GCN中共享权重的局限性，提出了一种局部连接网络（LCN）[[153](#bib.bib153)]，它利用完全连接网络和GCN来编码关节之间的关系。类似地，赵等人[[154](#bib.bib154)]提出了一种语义GCN，以学习边缘的通道权重。一种基于GCN的Pose2Mesh[[155](#bib.bib155)]被提出用来优化其PoseNet中的中间3D姿态。徐和高野[[159](#bib.bib159)]提出了一种图堆叠Hourglass（GraphSH）网络，该网络由重复的编码器-解码器组成，用于表示三种不同尺度的人体骨架。为了解决当前GCN方法中关节交互的丧失，翟等人[[162](#bib.bib162)]提出了Hop-wise
    GraphFormer与组内关节优化（HopFIR）以提升3D姿态。
- en: Inspired by the recent success in the nature language field, there is a growing
    interest in exploring the use of Transformer architecture for vision tasks. Lin
    et al. [[163](#bib.bib163)] first applied Transformer for 3D pose estimation.
    A multi-layer Transformer with progressive dimensionality reduction was proposed
    to regress the 3D coordinates of joints. Here, the standard transformer ignores
    the interaction of adjacency nodes. To overcome this problem, Zhao et al. [[164](#bib.bib164)]
    proposed a graph-oriented Transformer which enlarges the receptive field through
    self-attention and models graph structure by GCN to improve the performance on
    3D pose estimation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 受最近自然语言领域成功的启发，越来越多的研究开始探索将Transformer架构应用于视觉任务。林等人[[163](#bib.bib163)]首次将Transformer应用于3D姿态估计。提出了一种具有逐步降维的多层Transformer来回归关节的3D坐标。在这里，标准Transformer忽略了邻接节点的交互。为了解决这个问题，赵等人[[164](#bib.bib164)]提出了一种面向图的Transformer，它通过自注意力扩展感受野，并通过GCN建模图结构，以提高3D姿态估计的性能。
- en: For in-the-wild data, it is difficult to obtain accurate 3D pose annotations.
    To deal with the lack of 3D pose annotation problem, some weakly supervised, self-supervised,
    or unsupervised methods [[144](#bib.bib144), [165](#bib.bib165), [166](#bib.bib166),
    [167](#bib.bib167), [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170),
    [171](#bib.bib171), [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174)]
    were proposed for estimating 3D poses from in-the-wild images without 3D pose
    annotations. A weakly supervised transfer learning method [[144](#bib.bib144)]
    was proposed to transfer the knowledge from 3D annotations of indoor images to
    in-the-wild images. 3D bone length constraint-induced loss was applied in the
    weakly supervised learning. Habibie et al. [[166](#bib.bib166)] applied a projection
    loss to refine 3D pose without annotation. A lifting network [[167](#bib.bib167)]
    was proposed to recover 3D poses in a self-supervised mode by introducing a geometrical
    consistency loss based on the closure and invariance lifting property. The previous
    self-supervised methods have largely relied on weak supervisions like consistency
    loss to guide the learning, which inevitably leads to inferior results in real-world
    scenarios with unseen poses. Comparatively, Gong et al. [[173](#bib.bib173)] propose
    a PoseTriplet method that allows explicit generating 2D-3D pose pairs for augmenting
    supervision, through a self-enhancing dual-loop learning framework. Benefiting
    from the reliable 2D pose detection, two-step-based approaches generally outperform
    one-step-based ones.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于现实世界中的数据，获取准确的三维姿态标注是困难的。为了解决三维姿态标注不足的问题，提出了一些弱监督、自监督或无监督的方法 [[144](#bib.bib144),
    [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168),
    [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172),
    [173](#bib.bib173), [174](#bib.bib174)]，用于从现实世界中的图像中估计三维姿态，而无需三维姿态标注。提出了一种弱监督迁移学习方法 [[144](#bib.bib144)]，用于将室内图像的三维标注知识迁移到现实世界的图像中。在弱监督学习中应用了三维骨长约束引发的损失。Habibie
    等人 [[166](#bib.bib166)] 采用了投影损失来细化无标注的三维姿态。提出了一种提升网络 [[167](#bib.bib167)]，通过引入基于闭合和不变提升特性的几何一致性损失，以自监督模式恢复三维姿态。以前的自监督方法在很大程度上依赖于一致性损失等弱监督来指导学习，这不可避免地导致了在真实场景中遇到未知姿态时的较差结果。相比之下，Gong
    等人 [[173](#bib.bib173)] 提出了PoseTriplet方法，该方法允许通过自增强双循环学习框架显式生成 2D-3D 姿态对以增强监督。得益于可靠的二维姿态检测，两步法通常优于一步法。
- en: 2.2.2 Image-based multi-person pose estimation
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 基于图像的多人姿态估计
- en: 'Similar to 2D multi-person pose estimation, 3D multi-person pose estimation
    for images can be also divided into: top-down approaches, bottom-up approaches
    and one-stage approaches. Top-down and bottom-up approaches involve two stages
    for pose estimation. Fig. [11](#S2.F11 "Figure 11 ‣ 2.2.2 Image-based multi-person
    pose estimation ‣ 2.2 3D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") illustrates
    the general framework of the two approaches for image-based 3D MPPE.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于二维多人姿态估计，图像中的三维多人姿态估计也可以分为：自上而下的方法、自下而上的方法和单阶段的方法。自上而下和自下而上的方法涉及两个阶段的姿态估计。图 [11](#S2.F11
    "图 11 ‣ 2.2.2 基于图像的多人姿态估计 ‣ 2.2 三维姿态估计 ‣ 2 姿态估计 ‣ 基于深度学习的人体姿态估计、跟踪和动作识别综述") 说明了图像中三维多人姿态估计的两种方法的总体框架。
- en: '![Refer to caption](img/a487777ed5dde5698d8f814f2812ec37.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a487777ed5dde5698d8f814f2812ec37.png)'
- en: 'Figure 11: The framework of two approaches for image-based 3D MPPE. Part of
    the figure is from [[175](#bib.bib175)].'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：图像中三维多人姿态估计的两种方法的框架。图中的部分来自 [[175](#bib.bib175)]。
- en: (1) Top-down approach
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 自上而下的方法
- en: Top-down approaches first detect each person based on human detection networks
    and then generate 3D poses based on single-person estimation approaches. Localization
    Classification-Regression Network (LCR-Net) [[176](#bib.bib176), [177](#bib.bib177)]
    proposes a pose proposal network to generate human bounding boxes and a series
    of human pose hypotheses. The pose hypotheses were refined based on the cropped
    ROI features for generating 3D poses. Moon et al. [[178](#bib.bib178)] proposed
    a camera distance-aware method for estimating the camera-centric human poses which
    consists of human detection, absolute 3D human root localization, and root-relative
    3D single-person pose estimation modules. Here, the root-relative poses ignore
    the absolute locations of each pose. Comparatively, Lin and Lee [[179](#bib.bib179)]
    proposed the Human Depth Estimation Network (HDNet) for absolute root joint localization
    in the camera coordinate space. HDNet could estimate the human depth with considerably
    high performance based on the prior knowledge of the typical size of the human
    pose and body joints. The top-down methods mostly estimate poses based on each
    bounding box, which results in the doubt that the top-down models are not able
    to understand multi-person relationships and handle complex scenes. To address
    this limitation, Wang et al. [[180](#bib.bib180)] proposed a hierarchical multi-person
    ordinal relations (HMOR) to leverage the relationship among multiple persons for
    pose estimation. HMOR could encode the interaction information as ordinal relations,
    supervising the networks to output 3D poses in the correct order. Cha et al. [[181](#bib.bib181)]
    designed a transformer-based relation-aware refinement to capture the intra- and
    inter-person relationships. Although the top-down approaches achieve high accuracy,
    they suffer high computation costs as person number increases. Meanwhile, these
    methods may neglect global information (inter-person relationship) in the scene
    since poses are individually estimated.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，自顶向下的方法基于人体检测网络检测每个人，然后基于单人估计方法生成3D姿势。Localization Classification-Regression
    Network (LCR-Net) [[176](#bib.bib176), [177](#bib.bib177)] 提出了一个姿势提议网络，用于生成人类边界框和一系列人体姿势假设。基于裁剪的
    ROI 特征对姿势假设进行了修正，用于生成3D姿势。Moon 等人 [[178](#bib.bib178)] 提出了一种相机距离感知方法，用于估计相机中心的人体姿势，该方法包括人体检测、绝对3D人体根部定位和根相对3D单人姿势估计模块。在这里，根相对姿势忽略了每个姿势的绝对位置。相比之下，Lin
    和 Lee [[179](#bib.bib179)] 提出了用于摄像机坐标空间中的绝对根关节定位的Human Depth Estimation Network
    (HDNet)。HDNet可以基于人体姿势和身体关节的典型大小的先验知识，高效地估计人体深度。自顶向下的方法主要根据每个边界框估计姿势，这导致人们怀疑自顶向下模型无法理解多人关系并处理复杂场景。为解决这一限制，Wang
    等人 [[180](#bib.bib180)] 提出了一种用于姿势估计的分层多人序关系（HMOR），以利用多人之间的关系。HMOR能够将交互信息编码为序关系，监督网络按照正确顺序输出3D姿势。Cha
    等人 [[181](#bib.bib181)] 设计了一种基于Transformer的关系感知细化方法，用于捕捉人与人之间的内部和相互关系。虽然自顶向下的方法可以实现高准确性，但随着人数的增加，计算成本也相应增加。与此同时，这些方法可能忽视了场景中的全局信息（人与人之间的关系），因为姿势是分别估计的。
- en: (2) Bottom-up approach
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 自底向上方法
- en: Bottom-up approaches first produce all body joint locations and then associate
    joints to each person according to root depth and part relative depth. Zanfir
    et al. [[182](#bib.bib182)] proposed MubyNet to group human joints according to
    body part scores based on integrated 2D and 3D information. One group of bottom-up
    approaches aim to group body joints belonging to each person. Learning on Compressed
    Output (LoCO) method [[183](#bib.bib183)] first applied volumetric heatmaps to
    produce joint locations with an encoder-decoder network for feature compression,
    and a distance-based heuristic was then applied to retrieve 3D pose for each person.
    A distance-based heuristic was applied for linking joints. The previous methods
    are trained in a fully-supervised fashion which requires 3D pose annotations,
    while Kundu et al. [[184](#bib.bib184)] proposed a unsupervised method for 3D
    pose estimation. Without paired 2D images and 3D pose annotations, a frozen network
    was applied to exploit the shared latent space between two different modalities
    based on cross-modal alignment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 自底向上的方法首先生成所有身体关节位置，然后根据根深度和部位相对深度将关节与每个人关联起来。Zanfir 等人[[182](#bib.bib182)]
    提出了 MubyNet，以基于集成的 2D 和 3D 信息按身体部位分数对人体关节进行分组。一组自底向上的方法旨在将属于每个人的身体关节分组。Compressed
    Output (LoCO) 方法[[183](#bib.bib183)] 首先应用体积热图，通过编码器-解码器网络进行特征压缩来生成关节位置，然后应用基于距离的启发式方法来检索每个人的
    3D 姿态。基于距离的启发式方法用于关节连接。之前的方法采用完全监督的方式训练，需要 3D 姿态标注，而 Kundu 等人[[184](#bib.bib184)]
    提出了一个无监督的 3D 姿态估计方法。在没有配对的 2D 图像和 3D 姿态标注的情况下，应用了一个冻结网络，通过跨模态对齐利用两种不同模态之间的共享潜在空间。
- en: Another group of bottom-up approaches focus on occlusion. Mehta et al.[[185](#bib.bib185)]
    combined the joint location maps and the occlusion-robust pose-maps to infer the
    3D poses. The joint location redundancy is applied to infer occluded joints. XNect [[186](#bib.bib186)]
    encodes the immediate local context of joints in the kinematic tree to address
    occlusion. Zhen et al. [[187](#bib.bib187)] developed 3D part affinity field for
    depth-aware part association by reasoning about inter-person occlusion, and utilized
    a refined network to refine the 3D pose given predicted 2D and 3D joint coordinates.
    All of these methods handle occlusion from the perspective of single-person and
    require initial grouping joints into individuals, which results in error-prone
    estimates in multi-person scenarios. Liu et al. [[188](#bib.bib188)] proposed
    an occluded keypoints reasoning module based on a deeply supervised encoder distillation
    network to reason about the invisible information from the visible ones. Chen
    et al. [[189](#bib.bib189)] presented Articulation-aware Knowledge Exploration
    (AKE) for keypoints associated with a progressive scheme in the occlusion situation.
    In comparison to top-down approaches, bottom-up approaches offer the advantage
    of not requiring repeated single-person pose estimation and they enjoy linear
    computation. However, the bottom-up approaches require a second association stage
    for joint grouping. Furthermore, since all persons are processed at the same scale,
    these methods are inevitably sensitive to human scale variations, which limits
    their applicability in wild videos.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组自底向上的方法专注于遮挡问题。Mehta 等人[[185](#bib.bib185)] 将关节位置图和抗遮挡的姿态图结合起来，以推断 3D 姿态。关节位置的冗余用于推断被遮挡的关节。XNect[[186](#bib.bib186)]
    在运动学树中编码关节的即时局部上下文以应对遮挡。Zhen 等人[[187](#bib.bib187)] 开发了 3D 部件亲和场，通过推理人际间遮挡来进行深度感知的部件关联，并利用精细化网络在给定预测的
    2D 和 3D 关节坐标的情况下进一步优化 3D 姿态。这些方法从单人视角处理遮挡，并需要最初将关节分组到个体中，这在多人的场景中导致易出错的估计。Liu
    等人[[188](#bib.bib188)] 提出了一个基于深度监督编码器蒸馏网络的遮挡关键点推理模块，用于推理来自可见信息的不可见信息。Chen 等人[[189](#bib.bib189)]
    提出了关节意识知识探索 (AKE)，用于在遮挡情况下与渐进方案关联的关键点。与自顶向下的方法相比，自底向上的方法不需要重复的单人姿态估计，且计算线性。然而，自底向上的方法需要第二阶段的关节分组。此外，由于所有人以相同的尺度处理，这些方法不可避免地对人体尺度变化敏感，限制了它们在实际视频中的适用性。
- en: (3) One-stage approach
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 一阶段方法
- en: One-stage approaches treat pose estimation as parallel human center localizing
    and center-to-joint regression problem. Instead of separating joints localizing
    and grouping in the two-stage approaches, these approaches predict each of the
    joint offsets from the detected center points, which is usually set as the root
    joint of human. Since the joint offsets are directly correlated to estimated center
    points, this strategy avoids the manually designed grouping post-processing and
    is end-to-end trainable. Zhou et al.[[190](#bib.bib190)] modeled an object as
    a single point and regressed joints from image features at the human center. Wei
    et al. [[191](#bib.bib191)] proposed to regress joints from point-set anchors
    which serve as prior of basic human poses. Wang et al. [[175](#bib.bib175)] reconstructed
    joints from 2.5D human centers and 3D center-relative joint offsets. Jin et al. [[192](#bib.bib192)]
    proposed a Decoupled Regression Model (DRM) by solving 2D pose regression and
    depth regression. Recently, Qiu et al. [[193](#bib.bib193)] estimated 3D poses
    directly by fine-tuning a Weakly-Supervised Pre-training (WSP) network on 3D pose
    datasets.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶段方法将姿态估计视为平行的人体中心定位和中心到关节回归问题。这些方法预测每个关节的偏移量，而不是在两阶段方法中分离关节定位和分组，这些偏移量通常设置为人体的根关节。由于关节偏移量与估计的中心点直接相关，这种策略避免了手动设计的分组后处理，并且是端到端可训练的。周等人[[190](#bib.bib190)]将对象建模为一个单一的点，并从人体中心的图像特征回归关节。魏等人[[191](#bib.bib191)]建议从作为基本人体姿态先验的点集锚点回归关节。王等人[[175](#bib.bib175)]从2.5D人体中心和3D中心相对关节偏移量中重建关节。金等人[[192](#bib.bib192)]通过解决2D姿态回归和深度回归提出了一种解耦回归模型（DRM）。最近，邱等人[[193](#bib.bib193)]通过在3D姿态数据集上微调弱监督预训练（WSP）网络直接估计三维姿态。
- en: 2.2.3 Video-based single-person pose estimation
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 基于视频的单人姿态估计
- en: Instead of estimating 3D poses from images, videos can provide temporal information
    to improve the accuracy and robustness of pose estimation. Similar to image-based
    3D HPE, video-based 3D HPE can also be categorized into one-stage and two-stage
    approaches.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与从图像中估计三维姿态不同，视频可以提供时间信息，从而提高姿态估计的准确性和鲁棒性。类似于基于图像的三维人类姿态估计（3D HPE），基于视频的三维HPE也可以分为一阶段和两阶段方法。
- en: (1) One-stage approach
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 一阶段方法
- en: 'There are few research belong to this category of approaches. Tekin et al. [[194](#bib.bib194)]
    proposed a regression function to directly predict the 3D pose in a given frame
    of a sequence from a spatio-temporal volume centered around it. This volume comprises
    bounding boxes surrounding the person in consecutive frames coming before and
    after the central one. Mehta et al. [[195](#bib.bib195)] proposed the VNect, which
    is capable of obtaining a temporally consistent, full 3D skeletal pose of a human
    from a monocular RGB camera by Convents regression and kinematic skeleton fitting.
    The VNect could regress 2D and 3D joint locations simultaneously. Dabral et al. [[196](#bib.bib196)]
    proposed two structure-aware loss functions: illegal angle loss and left-right
    symmetry loss to directly predict 3D body pose from the video sequence. The illegal
    angle loss is to distinguish the internal and external angle of a 3D joint and
    the symmetry loss is defined as the difference in lengths of left/right bone pairs.
    Qiu [[197](#bib.bib197)] proposed an end-to-end framework based on Instance-guided
    Video Transformer (IVT) to predict 3D single and multiple poses directly from
    videos. An unsupervised feature extraction method [[198](#bib.bib198)] based on
    Constrastive Self-Supervised (CSS) learning was presented to capture rich temporal
    features for pose estimation. Time-variant and time-invariant latent features
    are learned using CSS by reconstructing the input video frames and time-variant
    features are then applied to predicting 3D poses.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 属于这一类方法的研究较少。Tekin等人[[194](#bib.bib194)]提出了一种回归函数，通过围绕给定帧的时空体直接预测该帧的3D姿态。该体积包括围绕中央帧前后连续帧中的人的边界框。Mehta等人[[195](#bib.bib195)]提出了VNect，它能够通过卷积回归和运动学骨架拟合，从单目RGB摄像头中获得时间一致的完整3D骨架姿态。VNect可以同时回归2D和3D关节位置。Dabral等人[[196](#bib.bib196)]提出了两种结构感知损失函数：非法角度损失和左右对称性损失，以直接从视频序列中预测3D身体姿态。非法角度损失用于区分3D关节的内角和外角，对称性损失定义为左右骨骼对的长度差。Qiu[[197](#bib.bib197)]提出了一种基于实例引导的视频Transformer（IVT）的端到端框架，以直接从视频中预测3D单一和多个姿态。提出了一种基于对比自监督（CSS）学习的无监督特征提取方法[[198](#bib.bib198)]，用于捕捉丰富的时间特征以进行姿态估计。通过重建输入视频帧，CSS学习了时间变异和时间不变的潜在特征，然后将时间变异特征应用于预测3D姿态。
- en: (2) Two-stage approach
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 两阶段方法
- en: 'Similar to two-step 3D poses estimated from images, two-step 3D HPE involves
    two stages: estimating 2D poses and lifting 3D poses from 2D poses. However, the
    difference is that a sequence of 2D poses is applied for lifting a sequence of
    3D poses in video-based 3D HPE. Based on different lifting methods, this category
    of approaches can be summarized into Seq2frame and Seq2seq-based methods.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与从图像中估计的两步3D姿态类似，两步3D HPE涉及两个阶段：估计2D姿态并从2D姿态中提取3D姿态。然而，区别在于，在基于视频的3D HPE中，应用了一系列2D姿态来提取一系列3D姿态。根据不同的提取方法，这一类方法可以总结为Seq2frame和Seq2seq方法。
- en: Seq2frame-based methods pay attention to predicting the central frame of the
    input video to produce a robust prediction and less sensitivity to noise. Pavllo
    et al. [[199](#bib.bib199)] presented a Temporal Convolutional Network (TCN) on
    2D keypoint trajectories with semi-supervised training method. In the network,
    1D convolutions are used to capture temporal information with fewer parameters.
    In semi-supervised training, the 3D pose estimator is used as the encoder and
    the decoder maps the predicted pose back to the 2D space. Some following works
    improved the performance of TCN by solving the occlusion problem [[200](#bib.bib200)],
    utilizing the attention [[201](#bib.bib201)], or decomposing the pose estimation
    task into bone length and bone direction prediction [[202](#bib.bib202)]. Except
    TCN, Cai et al. [[203](#bib.bib203)] employs GCN for modeling temporal information
    in which learning multi-scale features for 3D human body estimation from a short
    sequence of 2D joint detection. Without convolution architecture involved, Zheng
    et al. [[204](#bib.bib204)] proposed a PoseFormer based on a spatial-temporal
    transformer for estimating the 3D pose of the center frame. To overcome the huge
    computational cost of PoseFormer when increasing the frame number for better performance,
    PoseFormerV2 [[205](#bib.bib205)] applies a frequency-domain representation of
    2D pose sequences for lifting 3D poses. Similarly, Li et al. [[206](#bib.bib206)]
    proposed a stridden transformer encoder to reconstruct 3D pose of the center frame
    by reducing the sequence redundancy and computation cost. Li et al. [[207](#bib.bib207)]
    further designed a Multi-Hypothesis transFormer (MHFormer) to exploit spatial-temporal
    representations of multiple pose hypotheses. Based on MHFormer, MHFormer++ [[208](#bib.bib208)]
    is proposed to further model local information of joints by incorporating graph
    Transformer encoder and effectively aggregate multi-hypothesis features by adding
    a fusion block. With the similar idea of pose hypothesis [[207](#bib.bib207),
    [208](#bib.bib208)], DiffPose [[209](#bib.bib209)] and Diffusion-based 3D Pose
    (D3DP) [[210](#bib.bib210)] aim to apply a diffusion model to predict multiple
    adjustable hypotheses for a given 2D pose due to its ability of high-field samples.
    The aforementioned Transformer-based methods [[204](#bib.bib204), [205](#bib.bib205),
    [206](#bib.bib206), [208](#bib.bib208)] mainly model spatial and temporal information
    sequentially by different stages of networks, thus resulting in insufficient learning
    of motion patterns. Therefore, Tang et al. [[211](#bib.bib211)] proposed Spatio-Temporal
    Criss-cross Transformer (STCFormer) by stacking multiple STC attention blocks
    to model spatial and temporal information in parallel with a two-pathway network.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Seq2frame的方法注重预测输入视频的中心帧，以产生稳健的预测并减少对噪声的敏感性。Pavllo等人[[199](#bib.bib199)]提出了一种在2D关键点轨迹上使用半监督训练方法的时间卷积网络（TCN）。在该网络中，1D卷积用于捕获时间信息，且参数较少。在半监督训练中，3D姿态估计器用作编码器，解码器将预测的姿态映射回2D空间。一些后续工作通过解决遮挡问题[[200](#bib.bib200)]、利用注意力机制[[201](#bib.bib201)]或将姿态估计任务分解为骨长和骨方向预测[[202](#bib.bib202)]来提高TCN的性能。除了TCN，Cai等人[[203](#bib.bib203)]使用GCN来建模时间信息，通过从短序列的2D关节检测中学习多尺度特征以进行3D人体估计。没有卷积架构的涉及，Zheng等人[[204](#bib.bib204)]提出了一种基于空间-时间变换器的PoseFormer，用于估计中心帧的3D姿态。为了克服PoseFormer在增加帧数以提高性能时的巨大计算成本，PoseFormerV2[[205](#bib.bib205)]应用了2D姿态序列的频域表示来提升3D姿态。类似地，Li等人[[206](#bib.bib206)]提出了一种步态变换器编码器，通过减少序列冗余和计算成本来重建中心帧的3D姿态。Li等人[[207](#bib.bib207)]进一步设计了一种多假设变换器（MHFormer），以利用多重姿态假设的空间-时间表示。基于MHFormer，MHFormer++[[208](#bib.bib208)]被提出，以通过结合图形变换器编码器进一步建模关节的局部信息，并通过添加融合块有效地聚合多假设特征。基于姿态假设的相似理念[[207](#bib.bib207),
    [208](#bib.bib208)]，DiffPose[[209](#bib.bib209)]和基于扩散的3D姿态（D3DP）[[210](#bib.bib210)]旨在应用扩散模型来预测给定2D姿态的多个可调假设，因其具有高场样本的能力。上述基于变换器的方法[[204](#bib.bib204),
    [205](#bib.bib205), [206](#bib.bib206), [208](#bib.bib208)]主要通过不同阶段的网络顺序建模空间和时间信息，从而导致对运动模式的学习不足。因此，Tang等人[[211](#bib.bib211)]提出了空间-时间交叉变换器（STCFormer），通过堆叠多个STC注意力块以并行建模空间和时间信息，使用双路径网络。
- en: 'Seq2seq-based methods reconstruct all frames of input sequence at once for
    improving coherence and efficiency of 3D pose estimation. The earlier methods
    apply recurrent neural network (RNN) or long short-term memory (LSTM) as the Seq2Seq
    network. Lin et al. [[212](#bib.bib212)] designed a Recurrent 3D Pose Sequence
    Machine(RPSM) for estimating 3D human poses from a sequence of images. The RPSM
    consists of three modules: a 2D pose module; a 3D pose recurrent module and a
    feature adaption module for transforming the pose representations from 2D to 3D
    domain. Hossain et al. [[213](#bib.bib213)] presented a sequence-to-sequence network
    by using LSTM units and residual connections on the decoder side. The sequence
    of 2D joint locations is as input to the sequence-to-sequence network to predict
    a temporally coherent sequence of 3D poses. Lee et al. [[214](#bib.bib214)] proposed
    propagating long short-term memory networks (p-LSTMs) to estimates depth information
    from 2D joint location through learning the intrinsic joint interdependency. Katircioglu
    et al. [[215](#bib.bib215)] proposed a deep learning regression architecture to
    learn a high-dimensional latent pose representation by using an autoencoder and
    a Long Short-Term Memory network is proposed to enforce temporal consistency on
    3D pose predictions. Raymond et al. [[216](#bib.bib216)] proposed Chirality Nets.
    In Chirality Nets, fully connected layers, convolutional layers, batch-normalization,
    and LSTM/GRU cells can be chiral. According to this kind of symmetry, it naturally
    estimates 3D pose by exploiting the left/right mirroring of the human body. Later,
    there are some methods [[217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219),
    [220](#bib.bib220), [221](#bib.bib221), [222](#bib.bib222)] apply GCN or transformer
    for Seq2seq learning. Wang et al. [[217](#bib.bib217)] exploited a GCN-based method
    combining a corresponding loss to model motion in both short temporal intervals
    and long temporal ranges. Zhang et al. [[219](#bib.bib219)] proposed a mixed spatio-temporal
    encoder(MixSTE) which includes a temporal transformer to model the temporal motion
    of each joint and a spatial transformer to learn inter-joint spatial correlations.
    The MixSTE directly reconstructs the entire frames to improve the coherence between
    input and output sequences. Chen et al. [[220](#bib.bib220)] proposed High-order
    Directed Transformer (HDFormer) to reconstruct 3D pose sequences from 2D pose
    sequences by incorporating self-attention and high-order attention to model joint-joint,
    bone-joint, and hyperbone-joint interactions.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Seq2seq的方法一次性重建输入序列的所有帧，以提高3D姿态估计的一致性和效率。早期的方法使用递归神经网络（RNN）或长短期记忆网络（LSTM）作为Seq2Seq网络。Lin等人[[212](#bib.bib212)]设计了一种递归3D姿态序列机器（RPSM），用于从一系列图像中估计3D人体姿态。RPSM包含三个模块：2D姿态模块；3D姿态递归模块和用于将姿态表示从2D领域转换到3D领域的特征适配模块。Hossain等人[[213](#bib.bib213)]提出了一种序列到序列网络，通过在解码器端使用LSTM单元和残差连接。2D关节位置序列作为输入传递给序列到序列网络，以预测一个时间上连贯的3D姿态序列。Lee等人[[214](#bib.bib214)]提出了传播长短期记忆网络（p-LSTMs），通过学习内在的关节依赖关系来从2D关节位置估计深度信息。Katircioglu等人[[215](#bib.bib215)]提出了一种深度学习回归架构，通过使用自编码器和长短期记忆网络来学习高维潜在姿态表示，并强制3D姿态预测的时间一致性。Raymond等人[[216](#bib.bib216)]提出了Chirality
    Nets。在Chirality Nets中，全连接层、卷积层、批量归一化层和LSTM/GRU单元可以是对称的。根据这种对称性，它通过利用人体的左右镜像自然估计3D姿态。随后，一些方法[[217](#bib.bib217),
    [218](#bib.bib218), [219](#bib.bib219), [220](#bib.bib220), [221](#bib.bib221),
    [222](#bib.bib222)]应用GCN或transformer进行Seq2seq学习。Wang等人[[217](#bib.bib217)]利用一种基于GCN的方法，结合相应的损失来建模短时间间隔和长时间范围的运动。Zhang等人[[219](#bib.bib219)]提出了一种混合时空编码器（MixSTE），包括一个时间transformer来建模每个关节的时间运动和一个空间transformer来学习关节间的空间相关性。MixSTE直接重建整个帧，以提高输入和输出序列之间的一致性。Chen等人[[220](#bib.bib220)]提出了高阶定向transformer（HDFormer），通过结合自注意力和高阶注意力来建模关节-关节、骨骼-关节和超骨骼-关节交互，从而从2D姿态序列重建3D姿态序列。
- en: 2.2.4 Video-based multi-person pose estimation
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 基于视频的多人人体姿态估计
- en: Different from the image-based multi-person pose estimation, video-based multi-person
    pose estimation often suffers from fast motion, large variability in appearance
    and clothing, and person-to-person occlusion. A successful approach in this context
    must be capable of accurately identifying the number of individuals present in
    each video frame, as well as determining the precise joint locations for each
    person and effectively associating these joints over time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于图像的多人姿态估计不同，基于视频的多人姿态估计常常受到快速运动、外观和服装的大变化以及人与人之间遮挡的影响。在这种情况下，一个成功的方法必须能够准确识别每个视频帧中出现的个体数量，以及确定每个人的精确关节位置，并有效地将这些关节在时间上关联起来。
- en: With the improvement of video-based single-person 3D HPE, one method of video-based
    multi-based 3D HPE is two-step-based method that first detects each person based
    on human detection networks and then generates 3D poses based on video-based single-person
    3D HPE methods. Cheng et al. [[223](#bib.bib223)] proposed a novel framework for
    integrating graph convolutional network (GCN) and time convolutional network (TCN)
    to estimate multi-person 3D pose. In particular, bounding boxes are firstly detected
    for representing humans and 2D poses are then estimated based on the bounding
    box. The 3D poses for each frame are estimated by feeding 2D poses into joint-
    and bone-GCNs. The 3D pose sequence is finally fed into temporal TCN to enforce
    the temporal and human-dynamic constraints. This category of methods applies top-down
    technique to estimate 3D poses, which rely on detecting each person independently.
    Therefore, it is likely to suffer from inter-person occlusion and close interactions.
    To overcome this problem, the same author[[224](#bib.bib224)] later proposed an
    Multi-person Pose Estimation Integration (MPEI) network by adding a bottom-up
    branch for capturing global-awareness poses on the same top-down branch as the
    paper [[223](#bib.bib223)]. The final 3D poses are estimated based on matching
    the estimated 3D poses from both bottom-up and top-down branches. An interaction-aware
    discriminator was applied to enforce the natural interaction of two persons. To
    overcome the occlusion problem, Park et al. [[225](#bib.bib225)] presented POTR-3D
    to lift 3D pose sequences by directly processing 2D pose sequences rather than
    a single frame at a time, and devise a data augmentation strategy to generate
    occlusion-aware data with devise views. Capturing long-range temporal information
    normally requires computing on more frames, which results in high computational
    cost. To cope with this problem, a recent work, TEMporal POse estimation method
    (TEMPO) [[29](#bib.bib29)], learns a spatio-temporal representation by a recurrent
    architecture to speed up the inference time while preserving estimation accuracy.
    To be specific, persons are firstly detected and represented by feature volumes.
    A spatio-temporal pose representation is then learned by recurrently combining
    features from current and previous timesteps. It is finally decoded into an estimation
    of the current pose and poses at future timestaps. Note that the poses are estimated
    based on the tracking results of feature volumes, which hints that pose estimation
    performance can be improved by pose tracking. Moreover, TEMPO also provides a
    solution for action prediction.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于视频的单人3D人体姿态估计的改进，一种视频基础的多人体3D姿态估计方法是两步法，首先基于人体检测网络检测每个人，然后基于视频基础的单人3D姿态估计方法生成3D姿态。Cheng等人[[223](#bib.bib223)]提出了一种将图卷积网络（GCN）和时间卷积网络（TCN）整合的新框架，用于估计多人体3D姿态。特别地，首先检测边界框以表示人类，然后基于边界框估计2D姿态。通过将2D姿态输入到关节和骨骼GCN中来估计每帧的3D姿态。最终，将3D姿态序列输入到时间TCN中，以加强时间和人体动态约束。这类方法应用自上而下的技术来估计3D姿态，依赖于独立检测每个人。因此，它容易受到人与人之间的遮挡和紧密互动的影响。为了解决这个问题，同一作者[[224](#bib.bib224)]后来提出了一种多人体姿态估计集成（MPEI）网络，通过在与论文[[223](#bib.bib223)]相同的自上而下分支上添加一个自下而上的分支来捕获全局意识姿态。最终的3D姿态基于从自下而上和自上而下分支中匹配估计的3D姿态进行估计。应用了一个互动感知判别器来加强两个人的自然互动。为了解决遮挡问题，Park等人[[225](#bib.bib225)]提出了POTR-3D，通过直接处理2D姿态序列而不是逐帧处理来提升3D姿态序列，并设计了一种数据增强策略来生成带有遮挡感知的数据。捕获长时间范围的时间信息通常需要在更多帧上进行计算，这导致了高计算成本。为应对这个问题，最近的研究工作TEMporal
    POse estimation method（TEMPO）[[29](#bib.bib29)]通过递归架构学习时空表示，以加快推断速度，同时保持估计准确性。具体来说，首先检测并通过特征体积表示人物。然后，通过递归地结合当前和之前时间步的特征来学习时空姿态表示。最后，将其解码为当前姿态和未来时间步的姿态估计。需要注意的是，姿态是基于特征体积的跟踪结果估计的，这提示姿态估计性能可以通过姿态跟踪来提高。此外，TEMPO还为动作预测提供了解决方案。
- en: In the above two-step-based methods, the result of the latter step depends on
    the ones of the former step. Therefore, one-step pose estimation is proposed recently
    based on end-to-end network. IVT [[197](#bib.bib197)] can be also used to predict
    multiple poses directly from videos. The instance-guided tokens include deep features
    and instance 2D offsets (from body center to keypoints) which are sent into a
    video transformer to capture the contextual depth information between multi-person
    joints in spatial and temporal dimensions. A cross-scale instance-guided attention
    mechanism is introduced to handle the variational scales among multiple persons.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述的两步方法中，后续步骤的结果依赖于前一步骤的结果。因此，最近提出了一种基于端到端网络的一步姿态估计方法。IVT [[197](#bib.bib197)]
    也可以直接从视频中预测多个姿态。实例引导的标记包括深度特征和实例2D偏移（从身体中心到关键点），这些信息被送入视频变换器以捕获空间和时间维度中多人的关联系统深度信息。引入了跨尺度实例引导的注意机制，以处理多人的尺度变化。
- en: In summary, 3D HPE has made significant advancements recent years. Due to the
    progress in 2D HPE, a large number of 3D image/video-based single-person HPE methods
    apply 2D to 3D lifting strategy. When extending single-person to multi-person
    in 3D image/video-based HPE, two step (top-down and bottom-up) and one-step methods
    are always applied. Although top-down methods could achieve promising results
    by the state-of-the-art person detection and single-person methods, they suffer
    from high computation cost as person number increases and the missing of inter-person
    relationship measurement. The bottom-up methods could enjoy linear computation,
    however, they are sensitive to human scale variations. Therefore, one-step based
    methods are preferable for 3D image/video-based multi-person HPE. When extending
    image-based 3D single/multi-person HPE to video-based ones, temporal information
    is measured for learning joint association across frames. Similar to images-methods,
    two-step-based methods are commonly used due to the success of 2D to 3D lifting
    strategy. Among them, Seq2seq-based methods are preferable, as they contribute
    to enhancing the coherence and efficiency of 3D pose estimation. To capture the
    temporal information, TCN (Temporal Convolutional Networks), RNN (Recurrent Neural
    Network)-related architectures, and Transformers are commonly used networks.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，3D HPE 最近几年取得了显著进展。由于 2D HPE 的进展，大量基于 2D 到 3D 提升策略的 3D 图像/视频单人 HPE 方法得以应用。在将单人扩展到多人
    3D 图像/视频 HPE 时，通常应用两步（自上而下和自下而上）和一步方法。虽然自上而下的方法通过最先进的人物检测和单人方法可以取得令人满意的结果，但随着人物数量的增加，它们会受到高计算成本和缺乏人际关系测量的困扰。自下而上的方法可以享受线性计算，但对人体尺度变化敏感。因此，一步基于方法更适用于
    3D 图像/视频多人 HPE。在将基于图像的 3D 单人/多人 HPE 扩展到基于视频的 HPE 时，时间信息被用来学习跨帧的关联系统。与图像方法类似，由于
    2D 到 3D 提升策略的成功，常用两步方法。在这些方法中，Seq2seq 基础的方法更受欢迎，因为它们有助于提高 3D 姿态估计的连贯性和效率。为了捕获时间信息，通常使用
    TCN（时间卷积网络）、RNN（递归神经网络）相关架构和 Transformers。
- en: 3 Pose tracking
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 姿态跟踪
- en: 'Pose tracking aims to estimate human poses from videos and link the poses across
    frames to obtain a number of trackers. It is related to video-based pose estimation,
    but it requires to capturing the association of estimated poses across frames
    which is different from video-based pose estimation. With the pose estimation
    methods reviewed in Section 2, the main task of pose tracking becomes pose linking.
    The fundamental problem of pose linking is to measure the similarity between pairs
    of poses in adjacent frames. The pose similarity is normally measured based on
    temporal information (eg. optical flow, temporal smoothness priors), and appearance
    information from images. Following the taxonomy of two kinds of estimated poses,
    we divide the pose tracking methods into two categories: 2D pose tracking and
    3D pose tracking.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态跟踪旨在从视频中估计人体姿态，并链接跨帧的姿态以获得多个跟踪器。它与基于视频的姿态估计相关，但需要捕获估计姿态在帧间的关联，这与基于视频的姿态估计不同。利用第2节中回顾的姿态估计方法，姿态跟踪的主要任务变为姿态链接。姿态链接的基本问题是测量相邻帧中姿态对的相似性。姿态相似性通常基于时间信息（如光流、时间平滑先验）以及图像中的外观信息来测量。根据估计姿态的两种分类，我们将姿态跟踪方法分为两类：2D
    姿态跟踪和 3D 姿态跟踪。
- en: 3.1 2D pose tracking
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 2D 姿态跟踪
- en: According to the number of persons for tracking, 2D pose tracking can be divided
    into single-person and multi-person pose tracking. Fewer methods solve the problem
    of single-person pose tracking since they actually aim to update the estimated
    poses for obtaining more accurate poses with temporal consistency. Therefore,
    pose tracking mainly solves the tracking problem of multiple persons. Nevertheless,
    we will give a review of two categories of methods including single-person and
    multi-person pose tracking.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据跟踪人数的不同，2D 姿态跟踪可以分为单人姿态跟踪和多人姿态跟踪。由于单人姿态跟踪实际上旨在更新估计姿态以获得具有时间一致性的更准确姿态，因此解决单人姿态跟踪问题的方法较少。因此，姿态跟踪主要解决多人的跟踪问题。不过，我们将对包括单人姿态跟踪和多人姿态跟踪在内的两类方法进行回顾。
- en: 3.1.1 Single-person pose tracking
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 单人姿态跟踪
- en: 'Based on the core idea of updating the estimated poses by tracking, this category
    of approaches can be usually divided into two types, post-processing and integration
    approaches. The post-processing approaches estimate the pose of each frame individually,
    and then correlation analysis is conducted on the estimated poses across different
    frames to reduce inconsistencies and generate a smooth result. The integrated
    approaches unite pose estimation and visual tracking within a single framework.
    Visual tracking ensures the temporal consistency of the poses, while pose estimation
    enhances the accuracy of the tracked body parts. By combining the strengths of
    both visual tracking and pose estimation, the integrated approaches achieve improved
    results in pose tracking. Fig. [12](#S3.F12 "Figure 12 ‣ 3.1.1 Single-person pose
    tracking ‣ 3.1 2D pose tracking ‣ 3 Pose tracking ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey") illustrates the
    general framework of the two approaches for single person pose tracking.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '基于通过跟踪更新估计姿态的核心思想，这类方法通常可以分为两种类型：后处理方法和集成方法。后处理方法单独估计每一帧的姿态，然后对不同帧之间的估计姿态进行相关性分析，以减少不一致性并生成平滑的结果。集成方法则在一个框架内结合了姿态估计和视觉跟踪。视觉跟踪确保姿态的时间一致性，而姿态估计则提高了跟踪身体部位的准确性。通过结合视觉跟踪和姿态估计的优势，集成方法在姿态跟踪中取得了更好的结果。图
    [12](#S3.F12 "Figure 12 ‣ 3.1.1 Single-person pose tracking ‣ 3.1 2D pose tracking
    ‣ 3 Pose tracking ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") 展示了单人姿态跟踪的两种方法的总体框架。'
- en: '![Refer to caption](img/9a2b98220d40b0d30f62f617a0f52b61.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9a2b98220d40b0d30f62f617a0f52b61.png)'
- en: 'Figure 12: The framework of two approaches for 2D Single person pose tracking.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 2D 单人姿态跟踪的两种方法的框架。'
- en: (1) Post-processing approach
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 后处理方法
- en: Zhao et al. [[226](#bib.bib226)] proposed to track human body pose by adopting
    the max-margin Markov model. They proposed a spatio-temporal model composed of
    two sub-models for spatial parsing and temporal parsing respectively. Spatial
    parsing is used to estimate candidate human poses in a frame, while temporal parsing
    determines the most probable pose part locations over time. An inference iteration
    of sub-models is conducted to obtain the final result. Samanta et al.  [[227](#bib.bib227)]
    proposed a data-driven method for human body pose tracking in video data. They
    initially estimated the pose in the first frame of the video, and employed local
    object tracking to maintain spatial relationships between body parts across different
    frames.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人 [[226](#bib.bib226)] 提出了通过采用最大边际马尔可夫模型来跟踪人体姿态。他们提出了一个由两个子模型组成的时空模型，分别用于空间解析和时间解析。空间解析用于估计帧中的候选人体姿态，而时间解析则确定随时间推移最可能的姿态部位位置。通过子模型的推理迭代来获得最终结果。Samanta
    等人 [[227](#bib.bib227)] 提出了一个数据驱动的方法用于视频数据中的人体姿态跟踪。他们最初在视频的第一帧中估计姿态，并采用局部物体跟踪来保持不同帧之间身体部位的空间关系。
- en: (2) Integrated approach
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 集成方法
- en: Zhao et al. [[228](#bib.bib228)] proposed a two-step iterative method that combines
    pose estimation and visual tracking into a unified framework to compensate for
    each other, the pose estimation improves the accuracy of visual tracking, and
    the result of visual tracking facilitates the pose estimation. The two steps are
    performed iteratively to get the final pose. In addition, they designed a reinitialization
    mechanism to prevent pose tracking failures. Previous methods required future
    frames or entire sequences to refine the current pose and were difficult to track
    online. Ma et al. [[229](#bib.bib229)] solved the problem of online tracking human
    pose of joint motion in dynamic environments. They proposed a coupled-layer framework
    composed of a global layer for pose tracking and a local layer for pose estimation.
    The core idea is to decompose the global pose candidate in any particular frame
    into several local part candidates and then recombine selected local parts to
    obtain an accurate pose for the frame.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人[[228](#bib.bib228)] 提出了一个两步迭代方法，将姿态估计和视觉跟踪结合成一个统一的框架，以相互补偿，姿态估计提高视觉跟踪的准确性，而视觉跟踪的结果又促进了姿态估计。这两个步骤是迭代进行的，以获得最终姿态。此外，他们设计了一个重新初始化机制以防止姿态跟踪失败。以前的方法需要未来的帧或整个序列来细化当前姿态，且难以在线跟踪。Ma
    等人[[229](#bib.bib229)] 解决了动态环境中关节运动的人体姿态在线跟踪问题。他们提出了一个耦合层框架，由一个用于姿态跟踪的全局层和一个用于姿态估计的局部层组成。核心思想是将任何特定帧中的全局姿态候选分解成几个局部部分候选，然后重新组合所选的局部部分以获得该帧的准确姿态。
- en: Post-processing approaches first obtain a set of plausible pose assumptions
    from the video and then stitch together compatible detections over time to form
    pose tracking. However, due to the multiplicative cost of using global information,
    models in this category can usually only include local spatio-temporal trajectories
    (evidence). These local spatio-temporal trajectories may be ambiguous, thus leading
    to the disadvantage of objective models. Furthermore, post-processing methods
    are difficult to track online, but integrated approaches allow for a more robust
    and accurate representation of the poses over time, ensuring that the tracked
    body retrains its appropriate configuration throughout the tracking process.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理方法首先从视频中获取一组可能的姿态假设，然后将时间上的兼容检测结果拼接在一起形成姿态跟踪。然而，由于使用全局信息的成本呈乘法增长，这一类别的模型通常只能包含局部时空轨迹（证据）。这些局部时空轨迹可能会模糊，从而导致客观模型的劣势。此外，后处理方法难以在线跟踪，但集成方法允许对姿态进行更稳健和准确的表示，确保跟踪中的身体保持其适当的配置。
- en: 3.1.2 Multi-person pose tracking
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 多人姿态跟踪
- en: Unlike single-person pose tracking, multi-person pose tracking involves measuring
    human interactions, which can introduce challenges to the tracking process. The
    number of the tracking people is unknown, and the human interaction may cause
    the occlusion and overlap. Similar to multi-person pose estimation, existing methods
    can be divided into two categories, top-down and bottom-up approaches.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与单人姿态跟踪不同，多人姿态跟踪涉及到测量人体交互，这可能给跟踪过程带来挑战。跟踪人数未知，人际互动可能会导致遮挡和重叠。类似于多人姿态估计，现有方法可以分为两类：自上而下的方法和自下而上的方法。
- en: (1) Top-down approach
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 自上而下的方法
- en: Top-down approaches [[131](#bib.bib131), [132](#bib.bib132)] start by detecting
    the overall location and bounding box of the human body in frames and then estimates
    the keypoints of each person. Finally, the estimated human poses are associated
    according to similarity between poses in different frames. Girdhar et al. [[130](#bib.bib130)]
    proposed a two-stage method for estimating and tracking human keypoints in complex
    multi-person videos. The method utilizes Mask R-CNN to perform frame-level pose
    estimation which detects person tubes and estimates keypoints in predicted tubes,
    then performs a person-level tracking module by using lightweight optimization
    to connect estimated keypoints over time. However, this method does not consider
    motion and pose information, which causes difficulty in tracking the occasional
    truncated human. To address the issue, Xiu et al. [[129](#bib.bib129)] employed
    pose flow as a unit and proposed a new pose flow generator which consists of Pose
    Flow Builder and Pose Flow NMS. They initially estimated multi-person poses by
    employing an improved RMPE, and then maximizing overall confidence to construct
    pose flows. Finally, pose flows were purified by applying Plow Flow NMS to obtain
    reasonable multi-pose trajectories. To ease the complexity of method, Xiao et
    al. [[69](#bib.bib69)] proposed a simple but effective method for pose estimation
    and tracking. They adopted the pose propagation and similarity measurement based
    on optical flow to improve the greedy matching method for pose tracking. Zhang
    et al. [[230](#bib.bib230)] solved the articulated multi-person pose estimation
    and real-time velocity tracking. An end-to-end multi-task network (MTN) was designed
    for simultaneously performing human detection, pose estimation, and person re-identification
    (Re-ID) tasks. Given the detection box, keypoints and Re-ID feature provided by
    MTN, an occlusion-aware strategy is applied for pose tracking. Ning et al. [[231](#bib.bib231)]
    proposed a top-down approach that combines single-person pose tracking (SPT) and
    visual object tracking (VOT) into a unified online functional entity that can
    be easily implemented with a replaceable single person pose estimator. They processed
    each human candidate separately and associated the lost tracked candidate to the
    targets from the previous frames through pose matching. The human pose matching
    can be achieved by applying the Siamese Graph Convolution Network as the Re-ID
    module. Umer et al. [[232](#bib.bib232)] proposed a method that relies on the
    correspondence relationship of keypoints to associate the figures in the video.
    It is trained on large image data sets to use self-monitoring for body pose estimation.
    In combination with the top-down human pose estimation framework, keypoint correspondence
    is used to recover lost pose detection based on the temporal context and associate
    detected and recovered poses for pose tracking.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 自上而下的方法 [[131](#bib.bib131), [132](#bib.bib132)] 首先检测图像帧中人体的总体位置和边界框，然后估计每个人的关键点。最后，根据不同帧中的姿势相似性关联估计出的人体姿势。Girdhar
    等人 [[130](#bib.bib130)] 提出了一个用于在复杂多人物视频中估计和跟踪人体关键点的两阶段方法。该方法利用 Mask R-CNN 进行帧级姿势估计，检测人员管道并在预测的管道中估计关键点，然后通过使用轻量级优化来连接随时间变化的估计关键点，执行人员级跟踪模块。然而，这种方法没有考虑运动和姿势信息，这导致跟踪偶尔被截断的人体时的困难。为了解决这个问题，Xiu
    等人 [[129](#bib.bib129)] 采用姿势流作为单位，提出了一种新的姿势流生成器，包括 Pose Flow Builder 和 Pose Flow
    NMS。他们最初通过改进的 RMPE 估计多人的姿势，然后最大化整体置信度以构建姿势流。最后，通过应用 Plow Flow NMS 来净化姿势流，以获得合理的多姿势轨迹。为简化方法的复杂性，Xiao
    等人 [[69](#bib.bib69)] 提出了一个简单但有效的姿势估计和跟踪方法。他们采用基于光流的姿势传播和相似性测量，以改进贪婪匹配方法来进行姿势跟踪。Zhang
    等人 [[230](#bib.bib230)] 解决了关节化的多人物姿势估计和实时速度跟踪问题。设计了一个端到端的多任务网络 (MTN) 用于同时执行人体检测、姿势估计和人员重识别
    (Re-ID) 任务。鉴于 MTN 提供的检测框、关键点和 Re-ID 特征，应用了一个感知遮挡的策略来进行姿势跟踪。Ning 等人 [[231](#bib.bib231)]
    提出了一个自上而下的方法，该方法将单人姿势跟踪 (SPT) 和视觉目标跟踪 (VOT) 结合成一个统一的在线功能实体，可以通过可替换的单人姿势估计器轻松实现。他们分别处理每个人体候选者，并通过姿势匹配将丢失的跟踪候选者与前一帧中的目标关联。通过将
    Siamese 图卷积网络作为 Re-ID 模块，可以实现人体姿势匹配。Umer 等人 [[232](#bib.bib232)] 提出了一个依赖关键点对应关系来关联视频中人物的方法。该方法在大型图像数据集上进行训练，利用自我监控进行身体姿势估计。结合自上而下的人体姿势估计框架，使用关键点对应关系来基于时间上下文恢复丢失的姿势检测，并将检测到的和恢复的姿势关联起来进行姿势跟踪。
- en: The methods discussed in this section typically begin by detecting the human
    body boundary, which can make them susceptible to challenges like occlusion and
    truncation. Moreover, most methods first estimate poses in each frame and then
    implement data association and refinement. This strategy essentially relies heavily
    on non-existent visual evidence in the case of occlusion, so detection is inevitably
    easy to miss. To this end, Yang et al. [[233](#bib.bib233)] derived dynamic predictions
    through GNN that explicitly takes into account spatio-temporal and visual information.
    It leverages historical pose tracklets as input and predicts corresponding poses
    in the following frames for each tracklet. The predicted poses will then be aggregated
    with the detected poses, so as to recover occluded joints that may have been missed
    by the estimator, significantly improving the robustness of the method.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的方法通常从检测人体边界开始，这使得它们容易受到遮挡和截断等挑战的影响。此外，大多数方法首先在每一帧中估计姿态，然后实现数据关联和优化。这种策略在遮挡情况下基本上严重依赖于不存在的视觉证据，因此检测难免容易遗漏。为此，Yang
    等人[[233](#bib.bib233)]通过 GNN 推导了动态预测，明确考虑了时空和视觉信息。它利用历史姿态轨迹作为输入，并预测每个轨迹在后续帧中的相应姿态。然后，将预测的姿态与检测到的姿态聚合，以恢复可能被估计器遗漏的遮挡关节，从而显著提高了方法的鲁棒性。
- en: The methods mentioned above primarily emphasize pose-based similarities for
    matching, which usually struggle to re-identify tracks that have been occluded
    for extended periods or significant pose deformations. In light of this, Doering
    et al. [[234](#bib.bib234)] proposed a novel gated attention approach which utilizes
    a duplicate-aware association, and automatically adapts the impact of pose-based
    similarities and appearance-based similarities according to the attention probabilities
    associated with each similarity metric.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法主要强调基于姿态的相似性进行匹配，这通常在重新识别长时间遮挡或显著姿态变形的轨迹时遇到困难。鉴于此，Doering 等人[[234](#bib.bib234)]提出了一种新颖的门控注意力方法，该方法利用了重复感知关联，并根据与每个相似性度量相关的注意力概率自动调整基于姿态的相似性和基于外观的相似性的影响。
- en: (2) Bottom-up approach
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 自下而上的方法
- en: In contrast, bottom-up approaches first detect keypoints of the human body and
    then group the keypoints into individuals. The grouped keypoints are then connected
    and associated across frames to generate the complete pose. Iqbal et al. [[235](#bib.bib235)]
    proposed a novel method which jointly models multi-person pose estimation and
    tracking in a single formula. They represented the detected body joints in the
    video by a spatio-temporal graph which can be divided into sub-graphs corresponding
    to the possible trajectories of each human body pose by solving an integer linear
    program. Raaj et al. [[236](#bib.bib236)] proposed Spatio-Temporal Affinity Fields
    (STAF) across a video sequence for online pose tracking. The connections across
    keypoints in each frame are represented by Part Affinity Fields (PAFs) and connections
    between keypoints across frames are represented by Temporal Affinity Fields. Jin
    et al. [[138](#bib.bib138)] viewed pose tracking as a hierarchical detection and
    grouping problem. They proposed a unified framework consisting of SpatialNet and
    TemporalNet. SpatialNet implements single-frame body part detection and part-level
    data association, and TemporalNet groups human instances in continuous frames
    into trajectories. The grouping process is modeled by a differentiable Pose-Guided
    Grouping (PGG) module to make the entire part detection and grouping pipeline
    fully end-to-end trainable.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，自下而上的方法首先检测人体的关键点，然后将这些关键点分组到个体中。将分组后的关键点连接并跨帧关联，生成完整的姿态。Iqbal 等人[[235](#bib.bib235)]提出了一种新颖的方法，通过一个公式联合建模多人的姿态估计和跟踪。他们通过一个时空图表示视频中检测到的身体关节，该图可以通过解决整数线性程序分解为对应于每个人体姿态的子图。Raaj
    等人[[236](#bib.bib236)]提出了视频序列中的时空亲和场（STAF）用于在线姿态跟踪。每帧中的关键点之间的连接由部件亲和场（PAFs）表示，帧间关键点之间的连接由时间亲和场表示。Jin
    等人[[138](#bib.bib138)]将姿态跟踪视为层次检测和分组问题。他们提出了一个由 SpatialNet 和 TemporalNet 组成的统一框架。SpatialNet
    实现了单帧身体部位检测和部件级数据关联，TemporalNet 将连续帧中的人体实例分组为轨迹。分组过程通过可微分的姿态引导分组（PGG）模块建模，使整个部件检测和分组流程完全可端到端训练。
- en: The bottom-up approach relates joints spatially and temporally without detecting
    bounding boxes. Therefore, the computational cost of the methods is almost unaffected
    by the change in the number of human candidates. However, they require significant
    computational resources and often suffers from the ambiguous keypoints assignment
    without the global pose view. The top-down approach enhances single-frame pose
    estimation by incorporating temporal context information to correlate estimated
    poses across different frames. It simplifies the complex task and improves the
    keypoints assignment accuracy, although it may increase calculation cost in case
    of a large number of human candidates. In summary, the top-down approach outperforms
    the bottom-up approach both in accuracy and tracking speed, so most of the state-of-the-art
    methods follow the top-down approach.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 自下而上的方法在不检测边界框的情况下空间和时间上关联关节。因此，这些方法的计算成本几乎不受候选人数变化的影响。然而，它们需要大量的计算资源，并且通常由于缺乏全局姿势视图而遭遇关键点分配模糊问题。自上而下的方法通过结合时间上下文信息来增强单帧姿势估计，以便在不同帧之间关联估计的姿势。它简化了复杂任务并提高了关键点分配的准确性，尽管在候选人数较多时可能增加计算成本。总之，自上而下的方法在准确性和跟踪速度方面优于自下而上的方法，因此大多数最先进的方法都遵循自上而下的方法。
- en: 3.2 3D pose tracking
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 3D姿势跟踪
- en: 'With the advancement of 3D pose estimation, pose tracking can be naturally
    extended into 3D space. Given that current methods primarily focus on multi-person
    scenarios, we categorize them into two groups without specifying single or multi-person
    tracking: multi-stage and one-stage approaches.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 随着3D姿势估计的发展，姿势跟踪可以自然地扩展到3D空间。考虑到当前方法主要关注多人场景，我们将它们分为两个组，而不具体区分单人或多人跟踪：多阶段方法和单阶段方法。
- en: (1) Multi-stage approach
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 多阶段方法
- en: The multi-stage approaches generally track poses involving several steps such
    as 2D/3D pose estimation, lifting 2D to 3D poses and 3D pose linking. These tasks
    are served as independent sub-tasks. For example, Bridgeman et al. [[237](#bib.bib237)]
    performed independent 2D pose detection per frame and associated 2D pose detection
    between different camera views through a fast greedy algorithm. Then the associated
    poses are used to generate and track 3D pose. Zanfir et al. [[238](#bib.bib238)]
    first conducted a single person feedforward-feedback model to compute 2D and 3D
    pose, and then performed joint multiple person optimization under constraints
    to reconstruct and track multiple person 3D pose. Metha et al. [[186](#bib.bib186)]
    estimated 2D and 3D pose features and employed a fully-connected neural network
    to decode features into complete 3D poses, followed by a space-time skeletal model
    fitting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 多阶段方法通常涉及几个步骤，如2D/3D姿势估计、将2D姿势提升为3D姿势和3D姿势链接。这些任务被视为独立的子任务。例如，Bridgeman 等人 [[237](#bib.bib237)]
    对每帧进行独立的2D姿势检测，并通过快速贪婪算法关联不同相机视角之间的2D姿势检测。然后，关联的姿势被用来生成和跟踪3D姿势。Zanfir 等人 [[238](#bib.bib238)]
    首先进行单人前馈-反馈模型以计算2D和3D姿势，然后在约束下执行多人联合优化，以重建和跟踪多人3D姿势。Metha 等人 [[186](#bib.bib186)]
    估计2D和3D姿势特征，并使用完全连接的神经网络将特征解码为完整的3D姿势，然后进行时空骨架模型拟合。
- en: The above works firstly estimate poses and then link poses across frames in
    which the concept of tracking is to associate joints of the same person together
    over time, using joints localized independently in each frame. By contrast, Sun
    et al. [[239](#bib.bib239)] improved joint localization based on the information
    from other frames. They proposed to first learn the spatio-temporal joint relationships
    and then formulated pose tracking as a simple linear optimization problem.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法首先估计姿势，然后在不同帧之间链接姿势，其中跟踪的概念是将同一个人的关节在时间上关联起来，使用在每帧中独立定位的关节。相比之下，Sun 等人 [[239](#bib.bib239)]
    基于其他帧的信息改进了关节定位。他们提出首先学习时空关节关系，然后将姿势跟踪公式化为一个简单的线性优化问题。
- en: (2) One-stage approach
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 单阶段方法
- en: 'One-stage approach [[240](#bib.bib240), [241](#bib.bib241), [29](#bib.bib29),
    [242](#bib.bib242)] aims to train a single end-to-end framework for jointly estimating
    and linking 3D poses, which can propagate the errors of the sub-tasks in the multi-stage
    approaches back to the input image pixels of videos. For instance, Reddy et al. [[240](#bib.bib240)]
    introduced Tessetrack to jointly infer about 3D pose reconstructions and associations
    in space and time in a single end-to-end learnable framework. Tessetrack consists
    of three key components: person detection, pose tracking and pose estimation.
    With the detected persons, a spatial-temporal person-specific representation is
    learned for measuring similarity to link poses by solving an assignment problem
    based on bipartite graph matching. All matched representations are then merged
    into a single representation which is deconvolved into a 3D pose and taken as
    the estimated pose. To handle the occlusions, VoxelTrack [[241](#bib.bib241)]
    introduces an occlusion-aware multi-view feature fusion strategy for linking poses.
    Specifically, it jointly estimates and tracks 3D poses from a 3D voxel-based representation
    constructed from multi-view images. Poses are linked over time by bipartite graph
    matching based on fused representation from different views without occlusion.
    PHALP [[243](#bib.bib243)] accumulates 3D representations over time for better
    tracking. It relies on a backbone for estimating 3D representations for each human
    detection, aggregating representations over time and forecasting future states,
    and eventually associating tracklets with detections using predicted representations
    in a probabilistic framework. Snipper [[242](#bib.bib242)] conducts a deformable
    attention mechanism to aggregate spatiotemporal information for multi-person 3D
    pose estimation, tracking, and motion forecasting simultaneously in a single shot.
    Similar to Snipper, TEMPO [[29](#bib.bib29)] performs a recurrent architecture
    to fuse both spatial and temporal information into a single representation, which
    enabling pose estimation, tracking, and forecasting from multi-view information
    without sacrificing efficiency.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶段方法[[240](#bib.bib240), [241](#bib.bib241), [29](#bib.bib29), [242](#bib.bib242)]
    旨在训练一个单一的端到端框架来联合估计和连接3D姿态，该方法可以将多阶段方法中子任务的误差传播回视频的输入图像像素。例如，Reddy等人[[240](#bib.bib240)]引入了Tessetrack，以在一个可学习的端到端框架中共同推断3D姿态重建以及空间和时间上的关联。Tessetrack由三个关键组件组成：人员检测、姿态跟踪和姿态估计。通过检测到的人，学习一个空间-时间上的人员特定表示，用于通过基于二分图匹配的分配问题来测量相似性以链接姿态。所有匹配的表示随后被合并为一个单一的表示，该表示被反卷积为3D姿态并作为估计姿态。为处理遮挡，VoxelTrack[[241](#bib.bib241)]引入了一种遮挡感知的多视角特征融合策略来链接姿态。具体来说，它从构建于多视角图像上的3D体素表示中联合估计和跟踪3D姿态。姿态通过基于不同视角融合表示的二分图匹配在时间上进行链接，没有遮挡。PHALP[[243](#bib.bib243)]通过时间累积3D表示以便更好地跟踪。它依赖于一个骨干网来估计每个人检测的3D表示，随时间聚合表示并预测未来状态，最终使用概率框架中的预测表示将跟踪段与检测结果关联。Snipper[[242](#bib.bib242)]进行了一种可变形注意机制，以同时在一个步骤中聚合时空信息用于多人3D姿态估计、跟踪和运动预测。类似于Snipper，TEMPO[[29](#bib.bib29)]执行了一个递归架构，将空间和时间信息融合为单一表示，从而在不牺牲效率的情况下实现从多视角信息中进行姿态估计、跟踪和预测。
- en: Although both approaches have achieved good performance on 3D multi-person pose
    tracking, for the first approach, solving each sub-problem independently leads
    to performance degradation. 1) 2D pose estimation easily suffers from noise, especially
    in the presence of occlusion. 2) The accuracy of 3D estimation depends on the
    2D estimates and associations across all views. 3) Occlusion-induced unreliable
    appearance features impact the accuracy of 3D pose tracking. As a result, the
    second approach has gained prominence in recent years in 3D multi-person pose
    tracking.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两种方法在3D多人姿态跟踪上都取得了良好的性能，但对于第一种方法，独立解决每个子问题会导致性能下降。1）2D姿态估计容易受到噪声影响，尤其是在存在遮挡的情况下。2）3D估计的准确性依赖于所有视角的2D估计和关联。3）遮挡引起的不可靠外观特征会影响3D姿态跟踪的准确性。因此，第二种方法在近年来在3D多人姿态跟踪中获得了显著关注。
- en: 4 Action Recognition
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 动作识别
- en: '![Refer to caption](img/020cecdfac65686e9173117896dc3027.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/020cecdfac65686e9173117896dc3027.png)'
- en: 'Figure 13: Two categories of approaches for action recognition.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：动作识别的两种方法类别。
- en: '![Refer to caption](img/7342f51f96cadecfcb8ec0ae80cd6961.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7342f51f96cadecfcb8ec0ae80cd6961.png)'
- en: 'Figure 14: Four approaches for skeleton-based action recognition. (1) RNN example [[244](#bib.bib244)].
    (2) CNN example [[245](#bib.bib245)]. (3) GCN example [[246](#bib.bib246)]. (4)
    Transformer example [[247](#bib.bib247)].'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：基于骨架的动作识别的四种方法。(1) RNN 示例 [[244](#bib.bib244)]。 (2) CNN 示例 [[245](#bib.bib245)]。
    (3) GCN 示例 [[246](#bib.bib246)]。 (4) Transformer 示例 [[247](#bib.bib247)]。
- en: 'Action recognition aims to identify the class labels of human actions in the
    input images or videos. For the connection with pose estimation and tracking,
    this paper only reviews the action recognition methods based on poses. Pose-based
    action recognition can be categorized into two approaches: estimated pose-based
    and skeleton-based. Estimated pose-based action recognition approaches apply RGB
    videos as the input and classify actions using poses estimated from RGB videos.
    On the other hand, skeleton-based action recognition methods utilize skeletons
    as their input which can be obtained through various sensors, including motion
    capture devices, time-of-flight cameras, and structured light cameras. Fig. [13](#S4.F13
    "Figure 13 ‣ 4 Action Recognition ‣ Human Pose-based Estimation, Tracking and
    Action Recognition with Deep Learning: A Survey") illustrate the prevailing frameworks
    of these two categories approaches of pose-based action recognition.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 动作识别旨在识别输入图像或视频中的人类动作类别标签。对于与姿态估计和跟踪的联系，本文仅回顾基于姿态的动作识别方法。姿态基础的动作识别可以分为两种方法：估计姿态基础和骨架基础。估计姿态基础的动作识别方法使用RGB视频作为输入，并利用从RGB视频中估计出的姿态来分类动作。另一方面，骨架基础的动作识别方法利用骨架作为输入，这些骨架可以通过各种传感器获得，包括运动捕捉设备、飞行时间相机和结构光相机。图[13](#S4.F13
    "图 13 ‣ 4 动作识别 ‣ 基于深度学习的人体姿态估计、跟踪与动作识别的调查")展示了这两类姿态基础动作识别方法的主要框架。
- en: 4.1 Estimated pose-based action recognition
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 估计姿态基础的动作识别
- en: Pose features have been shown in performing much better than low/mid features
    and acting as discriminative cues for action recognition [[248](#bib.bib248)].
    With the success of pose estimation, some methods follow a two-stage strategy
    which first applies existing pose estimation methods to generate poses from videos
    and then conduct action recognition using pose features. Cheron et al. [[249](#bib.bib249)]
    proposed P-CNN to extract appearance and flow features conditioned on estimated
    human poses for action recognition. Mohammadreza et al. [[250](#bib.bib250)] designed
    a body part segmentation network to generate poses and then applied it to a multi-stream
    3D-CNN to integrate poses, optical flow and RGB visual information for action
    recognition. After generating joint heatmaps by pose estimator, Choutas et al.
     [[251](#bib.bib251)] proposed a Pose moTion (PoTion) representation by temporally
    aggregating the heatmaps for action recognition. To avoid relying on the inaccurate
    poses from pose estimation maps, Liu et al. [[252](#bib.bib252)] aggregated pose
    estimation maps to form poses and heatmaps, and then evolved them for action recognition.
    Moon et al. [[253](#bib.bib253)] proposed an algorithm for a pose-driven approach
    to integrate appearance and pre-estimated pose information for action recognition.
    Shah et al. [[254](#bib.bib254)] designed a Joint-Motion Reasoning Network (JMRN)
    for better capturing inter-joint dependencies of poses generated followed by running
    a pose detector on each video frame. This line of methods considers pose estimation
    and action recognition as two separate tasks so that action recognition performance
    may be affected by inaccurate pose estimation. Duan et al. [[255](#bib.bib255)]
    proposed PoseConv3D to form 3D heatmap volume by estimating 2D poses by existing
    pose estimator and stacking 2D heatmaps along the temporal dimension, and to classify
    actions by 3D CNN on top of the volume. Sato et al. [[256](#bib.bib256)] presented
    a user prompt-guided zero-shot learning method based on target domain-independent
    joint features and the joints are pre-extracted by the existing multi-person pose
    estimation technique. Rajasegaran et al. [[28](#bib.bib28)] proposed a Lagrangian
    Action Recognition with Tracking (LART) method to apply the tracking results for
    predicting actions. Pose and appearance features are firstly obtained by the PHALP
    tracking algorithm [[243](#bib.bib243)], and then fused as the input of a transformer
    network to predict actions. Hachiuma et al. [[257](#bib.bib257)] introduced a
    unified framework based on structured keypoint pooling for enhancing the adaptability
    and scalability of skeleton-based action recognition. Human keypoints and object
    contour points are initially obtained through multi-person pose estimation and
    object detection. A structured keypoint pooling is then applied to aggregate keypoint
    features to overcome skeleton detection and tracking errors. Addtionally, non-human
    object keypoints are severed as additional input for eliminating the variety restrictions
    of targeted actions. Finally, A pooling-switch trick is proposed for weakly supervised
    spatio-temporal action localization to achieve action recognition for each person
    in each frame.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态特征已被证明在表现上远胜于低级/中级特征，并且作为动作识别的区分线索[[248](#bib.bib248)]。随着姿态估计的成功，一些方法采用了两阶段策略，首先应用现有的姿态估计方法从视频中生成姿态，然后使用姿态特征进行动作识别。Cheron
    等人[[249](#bib.bib249)] 提出了 P-CNN，通过条件估计的人体姿态来提取外观和光流特征以进行动作识别。Mohammadreza 等人[[250](#bib.bib250)]
    设计了一个身体部位分割网络以生成姿态，然后将其应用于多流 3D-CNN，将姿态、光流和 RGB 视觉信息整合以进行动作识别。在通过姿态估计器生成关节热图后，Choutas
    等人[[251](#bib.bib251)] 提出了一个 Pose moTion (PoTion) 表征，通过时间聚合热图以进行动作识别。为了避免依赖姿态估计图中的不准确姿态，Liu
    等人[[252](#bib.bib252)] 聚合了姿态估计图以形成姿态和热图，然后对其进行演变以进行动作识别。Moon 等人[[253](#bib.bib253)]
    提出了一个基于姿态驱动的方法，将外观和预估姿态信息整合以进行动作识别。Shah 等人[[254](#bib.bib254)] 设计了一个联合动作推理网络 (JMRN)，以更好地捕捉生成姿态的关节间依赖关系，然后在每个视频帧上运行姿态检测器。这类方法将姿态估计和动作识别视为两个独立的任务，因此动作识别的性能可能会受到不准确的姿态估计的影响。Duan
    等人[[255](#bib.bib255)] 提出了 PoseConv3D，通过现有的姿态估计器估计 2D 姿态并沿时间维度堆叠 2D 热图来形成 3D 热图体，并通过
    3D CNN 对热图体进行分类。Sato 等人[[256](#bib.bib256)] 提出了一个基于目标领域无关的联合特征的用户提示引导的零样本学习方法，这些联合特征是通过现有的多人体姿态估计技术预先提取的。Rajasegaran
    等人[[28](#bib.bib28)] 提出了一个拉格朗日动作识别与跟踪 (LART) 方法，利用跟踪结果来预测动作。首先通过 PHALP 跟踪算法[[243](#bib.bib243)]
    获得姿态和外观特征，然后将其融合为变换器网络的输入以预测动作。Hachiuma 等人[[257](#bib.bib257)] 引入了一个基于结构化关键点池化的统一框架，以增强基于骨架的动作识别的适应性和可扩展性。人体关键点和物体轮廓点最初通过多人体姿态估计和物体检测获得。然后应用结构化关键点池化来聚合关键点特征，以克服骨架检测和跟踪错误。此外，非人体物体关键点作为额外输入用于消除目标动作的多样性限制。最后，提出了一种池化切换技巧用于弱监督时空动作定位，以实现对每个人在每一帧中的动作识别。
- en: Another line of methods jointly solves pose estimation and action recognition
    tasks. Luvizon et al. [[258](#bib.bib258)] proposed a multi-task CNN for joint
    pose estimation from still images and action recognition from video sequences
    based on appearance and pose features. Due to the different output formats of
    the pose estimation and the action recognition tasks, Foo et al. [[259](#bib.bib259)]
    designed a Unified Pose Sequence (UPS) multi-task model, which constructs text-based
    action labels and coordinate-based poses into a heterogeneous output format, for
    simultaneously processing the two tasks.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类方法则共同解决姿势估计和动作识别任务。Luvizon等人[[258](#bib.bib258)] 提出了一个多任务CNN，用于从静态图像中进行联合姿势估计，并基于外观和姿势特征从视频序列中进行动作识别。由于姿势估计和动作识别任务的输出格式不同，Foo等人[[259](#bib.bib259)]
    设计了一个统一姿势序列（UPS）多任务模型，将基于文本的动作标签和基于坐标的姿势构建成异质输出格式，以同时处理这两个任务。
- en: 4.2 Skeleton-based Action Recognition
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 骨架动作识别
- en: 'Skeleton data is one form of 3D data commonly used for action recognition.
    It consists of a sequence of skeletons, representing a schematic model of the
    locations of trunk, head, and limbs of the human body. Compared with another two
    commonly used data including RGB and depth, skeleton data is robust to illumination
    change and invariant to camera location and subject appearance. With the development
    of deep learning techniques, skeleton-based action recognition has transitioned
    from hand-crafted features to deep learning-based features. This survey mainly
    reviews the recent methods based on different deep learning networks which can
    be categorized into CNN-based, RNN-based, GCN-based, and Transformer-based methods,
    as shown in Fig. [14](#S4.F14 "Figure 14 ‣ 4 Action Recognition ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey").'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 骨架数据是一种常用于动作识别的3D数据形式。它由一系列骨架组成，表示人体躯干、头部和四肢的位置的示意模型。与RGB和深度数据这两种常用数据相比，骨架数据对光照变化具有鲁棒性，并且对相机位置和主体外观不变。随着深度学习技术的发展，骨架动作识别已从手工特征转变为基于深度学习的特征。本调查主要回顾了基于不同深度学习网络的最新方法，这些方法可以分为基于CNN、RNN、GCN和Transformer的方法，如图[14](#S4.F14
    "图14 ‣ 4 动作识别 ‣ 基于深度学习的人体姿势估计、跟踪和动作识别：综述")所示。
- en: 4.2.1 CNN-based approach
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 基于CNN的方法
- en: Convolutional Neural Networks (CNN), widely employed in the realm of computer
    vision, possess a natural advantage in image feature extraction due to their exceptional
    local perception and weight-sharing capabilities. Due to the success of CNN in
    image processing, CNN can better capture spatial information in skeleton sequences.
    CNN-based methods for skeleton-based action recognition can be categorized into
    2D and 3D CNN-based approaches, depending on the type of neural network utilized.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN），广泛应用于计算机视觉领域，由于其卓越的局部感知和权重共享能力，在图像特征提取方面具有天然优势。由于CNN在图像处理中的成功，CNN能够更好地捕捉骨架序列中的空间信息。基于CNN的骨架动作识别方法可以根据所使用的神经网络类型分为2D和3D
    CNN方法。
- en: Most of the 2D CNN-based methods [[260](#bib.bib260), [261](#bib.bib261), [262](#bib.bib262),
    [263](#bib.bib263), [264](#bib.bib264), [265](#bib.bib265), [245](#bib.bib245),
    [266](#bib.bib266)] first convert the skeleton sequence into a pseudo-image, in
    which the spatial-temporal information of the skeleton sequence is embedded in
    the colors and textures. Du et al. [[260](#bib.bib260)] mapped the Cartesian coordinates
    of the joints to RGB coordinates and then quantized the skeleton sequences into
    an image for feature extraction and action recognition. To reduce the inter-articular
    occlusion due to perspective transformations, some works  [[261](#bib.bib261),
    [262](#bib.bib262)] proposed to encode the spatial-temporal information of skeleton
    sequences into three orthogonal color texture images. The pair-wise distances
    between joints on single or multiple skeleton sequences are represented by Joint
    Distance Map (JDM) [[263](#bib.bib263)] which is encoded as a color change in
    the texture image. To explore better spatial feature representations, Ding et
    al. [[267](#bib.bib267)] encoded the distance, direction and angle of the joints
    as spatial features into the texture color images. Ke et al. [[268](#bib.bib268)]
    proposed to represent segments of skeleton sequences by images and classified
    actions using a multi-task learning network based on CNN. Similarly, Liang et
    al. [[269](#bib.bib269)] applied a multi-tasking learning based on three-stream
    CNN to encode skeletal fragment features, position and motion information.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数2D CNN-based方法[[260](#bib.bib260), [261](#bib.bib261), [262](#bib.bib262),
    [263](#bib.bib263), [264](#bib.bib264), [265](#bib.bib265), [245](#bib.bib245),
    [266](#bib.bib266)]首先将骨架序列转换为伪图像，其中骨架序列的空间-时间信息嵌入在颜色和纹理中。Du等人[[260](#bib.bib260)]将关节的笛卡尔坐标映射到RGB坐标，然后将骨架序列量化为图像进行特征提取和动作识别。为了减少由于透视变换引起的关节间遮挡，一些工作[[261](#bib.bib261),
    [262](#bib.bib262)]提出将骨架序列的空间-时间信息编码为三种正交的颜色纹理图像。单个或多个骨架序列中关节间的距离通过关节距离图（JDM）[[263](#bib.bib263)]表示，JDM被编码为纹理图像中的颜色变化。为了探索更好的空间特征表示，Ding等人[[267](#bib.bib267)]将关节的距离、方向和角度编码为空间特征，嵌入纹理颜色图像中。Ke等人[[268](#bib.bib268)]提出用图像表示骨架序列的片段，并基于CNN使用多任务学习网络进行动作分类。同样，Liang等人[[269](#bib.bib269)]基于三流CNN应用了多任务学习，以编码骨架片段特征、位置和运动信息。
- en: When compressing skeleton sequences into images by 2D CNN, it is unavoidable
    to lose some temporal information. By contrast, 3D CNN-based methods [[270](#bib.bib270),
    [271](#bib.bib271)] are more excellent at learning spatio-temporal features. Hernandez
    et al. [[271](#bib.bib271)] encoded skeleton sequences as stacked Euclidean Distance
    Matrices (EDM) computed over joints and then performed convolution along time
    dimension for learning spatiao-temporal dynamics of the data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过2D CNN将骨架序列压缩为图像时，难免会丢失一些时间信息。相比之下，3D CNN-based方法[[270](#bib.bib270), [271](#bib.bib271)]在学习时空特征方面更为出色。Hernandez等人[[271](#bib.bib271)]将骨架序列编码为在关节上计算的堆叠欧几里得距离矩阵（EDM），然后沿时间维度进行卷积，以学习数据的时空动态。
- en: 4.2.2 RNN-based approach
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 基于RNN的方法
- en: RNN-related networks are often used for processing time-series data to effectively
    capture the temporal information within skeleton sequences. Except for temporal
    information, spatial information is another important cue for action recognition
    which may be ignored by RNN-related networks. Some methods focus on solving this
    problem by spatial division of the human body. For exmaple, Du et al. [[272](#bib.bib272),
    [273](#bib.bib273)] proposed a hierarchical RNN for processing skeleton sequences
    of five body parts for action recognition. Shahroudy et al. [[274](#bib.bib274)]
    proposed a Partially-aware LSTM (P-LSTM) for separately modeling skeleton sequences
    of body parts and classified actions based on the concatenation of memory cells.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: RNN相关网络通常用于处理时间序列数据，以有效捕捉骨架序列中的时间信息。除了时间信息，空间信息是动作识别中的另一个重要线索，但RNN相关网络可能会忽略这一点。一些方法通过对人体的空间划分来解决这个问题。例如，Du等人[[272](#bib.bib272),
    [273](#bib.bib273)]提出了一种层次化RNN，用于处理五个身体部位的骨架序列进行动作识别。Shahroudy等人[[274](#bib.bib274)]提出了一种部分感知LSTM（P-LSTM），用于分别建模身体部位的骨架序列，并基于记忆单元的连接进行动作分类。
- en: To better focus on the key spatial information in the skeleton data, some methods
    tend to incorporate attention mechanism. Song et al. [[275](#bib.bib275)] proposed
    a spatiotemporal attention model using LSTM which includes a spatial attention
    module to adaptively select key joints in each frame, and a temporal attention
    module to select keyframes in skeleton sequences. Similarly, Liu et al. [[276](#bib.bib276)]
    proposed a cyclic attention mechanism to iteratively enhance the performance of
    attention for focusing on key joints. The subsequent improvement work by Song
    et al. [[277](#bib.bib277)] used spatio-temporal regularization to encourage the
    exploration of relationships among all nodes rather than overemphasizing certain
    nodes and avoided an unbounded increase in temporal attention. Zhang et al. [[278](#bib.bib278)]
    proposed a simple, effective, and generalized Element Attention Gate (EleAttG)
    to enhance the attentional ability of RNN neurons. Si et al. [[279](#bib.bib279)]
    proposed an Attention enhanced Graph Convolutional LSTM (AGC-LSTM) to enhance
    the feature representations of key nodes.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地关注骨架数据中的关键空间信息，一些方法倾向于引入注意力机制。Song 等人 [[275](#bib.bib275)] 提出了一个使用 LSTM
    的时空注意力模型，该模型包括一个空间注意力模块，用于自适应选择每帧中的关键关节，以及一个时间注意力模块，用于选择骨架序列中的关键帧。同样，Liu 等人 [[276](#bib.bib276)]
    提出了一个循环注意力机制，用于迭代增强注意力的性能，以集中关注关键关节。Song 等人 [[277](#bib.bib277)] 后续的改进工作使用了时空正则化，以鼓励探索所有节点之间的关系，而不是过度强调某些节点，并避免了时间注意力的无限增加。Zhang
    等人 [[278](#bib.bib278)] 提出了一个简单、有效且通用的元素注意力门（EleAttG），以增强 RNN 神经元的注意力能力。Si 等人
    [[279](#bib.bib279)] 提出了一个注意力增强图卷积 LSTM（AGC-LSTM），以增强关键节点的特征表示。
- en: To simultaneously exploit the temporal and spatial features of skeleton sequences,
    some methods aim to design spatial and/or temporal networks. Wang et al. [[244](#bib.bib244)]
    proposed a two-stream RNN for simultaneously learning spatial and temporal relationships
    of skeleton sequences and enhancing the generalization ability of the model through
    a skeleton data enhancement technique with 3D transformations. Liu et al. [[280](#bib.bib280)]
    proposed a spatial-temporal LSTM network, extending the traditional LSTM-based
    learning into the temporal and spatial domains. Considering the importance of
    the relationships between non-neighboring joints in the skeleton data, Zhang et
    al. [[281](#bib.bib281)] designed eight geometric relational features to model
    the spatial information and evaluated them in a three-layer LSTM network. Si et
    al. [[282](#bib.bib282)] proposed a spatial-based Reasoning and Temporal Stack
    Learning (SR-TSL) novel model to capture high-level spatial structural information
    within each frame, and model the detailed dynamic information by combining multiple
    jump-segment LSTMs.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了同时利用骨架序列的时间和空间特征，一些方法旨在设计空间和/或时间网络。Wang 等人 [[244](#bib.bib244)] 提出了一个双流 RNN，用于同时学习骨架序列的空间和时间关系，并通过具有
    3D 变换的骨架数据增强技术提高模型的泛化能力。Liu 等人 [[280](#bib.bib280)] 提出了一个时空 LSTM 网络，将传统的 LSTM
    基础学习扩展到时间和空间域。考虑到骨架数据中非邻近关节之间关系的重要性，Zhang 等人 [[281](#bib.bib281)] 设计了八种几何关系特征来建模空间信息，并在三层
    LSTM 网络中进行了评估。Si 等人 [[282](#bib.bib282)] 提出了一个基于空间的推理和时间堆叠学习（SR-TSL）新模型，以捕捉每帧中的高级空间结构信息，并通过结合多个跳跃段
    LSTM 来建模详细的动态信息。
- en: 4.2.3 GCN-based approach
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 基于 GCN 的方法
- en: GCN is a recent popular network for skeleton-based action recognition due to
    the human skeleton is a natural graph structure. Compared with CNN and RNN-based
    methods, GCN-based methods could better capture the relationship between joints
    in the skeleton sequence. According to whether the topology (namely vertex connection
    relationship) is dynamically adjusted during inference, GCN-based methods can
    be classified into static methods [[246](#bib.bib246), [283](#bib.bib283), [284](#bib.bib284),
    [285](#bib.bib285)] and dynamic methods [[286](#bib.bib286), [287](#bib.bib287),
    [288](#bib.bib288), [289](#bib.bib289), [290](#bib.bib290), [291](#bib.bib291),
    [292](#bib.bib292), [293](#bib.bib293), [294](#bib.bib294), [295](#bib.bib295),
    [296](#bib.bib296), [297](#bib.bib297), [298](#bib.bib298), [299](#bib.bib299),
    [300](#bib.bib300)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人体骨架是一种自然的图结构，GCN 成为骨架动作识别的热门网络。与基于 CNN 和 RNN 的方法相比，基于 GCN 的方法能够更好地捕捉骨架序列中关节之间的关系。根据拓扑（即顶点连接关系）在推理过程中是否动态调整，基于
    GCN 的方法可以分为静态方法[[246](#bib.bib246), [283](#bib.bib283), [284](#bib.bib284), [285](#bib.bib285)]和动态方法[[286](#bib.bib286),
    [287](#bib.bib287), [288](#bib.bib288), [289](#bib.bib289), [290](#bib.bib290),
    [291](#bib.bib291), [292](#bib.bib292), [293](#bib.bib293), [294](#bib.bib294),
    [295](#bib.bib295), [296](#bib.bib296), [297](#bib.bib297), [298](#bib.bib298),
    [299](#bib.bib299), [300](#bib.bib300)]。
- en: For static methods, the topologies of GCNs remian fixed during inference. For
    instance, an early application of graph convolutions, spatial-temporal GCN (ST-GCN) [[246](#bib.bib246)],
    is proposed which applies a predefined and fixed topology based on the human body
    structure. Liu et al. [[284](#bib.bib284)] proposed a multi-scale graph topology
    to GCNs for modeling multi-range joint relationships.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于静态方法，GCN 的拓扑在推理过程中保持固定。例如，早期应用图卷积的空间-时间 GCN（ST-GCN）[[246](#bib.bib246)] 提出了基于人体结构的预定义且固定的拓扑。Liu
    等人[[284](#bib.bib284)] 提出了用于建模多范围关节关系的多尺度图拓扑。
- en: For dynamic methods, the topologies of GCNs are dynamically inferred during
    inference. Action structure graph convolution network (AS-GCN) [[286](#bib.bib286)]
    applies an A-link inference module to capture action-specific correlations. Two-stream
    adaptive GCN (2s-AGCN) [[287](#bib.bib287)] and semantics-guided network (SGN) [[301](#bib.bib301)]
    enhanced topology learning with self-attention mechanism for modeling correlations
    between two joints. Although topology dynamic modeling is beneficial for inferring
    intrinsic relations of joints, it may be difficult to encode the context of an
    action since the captured topologies are independent of a pose. Therefore, some
    methods focus on context-dependent intrinsic topology modeling. In Dynamic GCN [[302](#bib.bib302)],
    contextual features of all joints are incorporated to learn the relations of joints.
    Channel topology refinement GCN (CTR-GCN) [[290](#bib.bib290)] focuses on embedding
    joint topology in different channels, while InfoGCN [[291](#bib.bib291)] introduces
    attention-based graph convolution to capture the context-dependent topology based
    on the latent representation learned by information bottleneck. Multi-Level Spatial-Temporal
    excited Graph Network (ML-STGNet) [[298](#bib.bib298)] introduces a spatial data-driven
    excitation module based on Transformer to learn joint relations of different samples
    in a data-dependent way. Multi-View Interactional Graph Network (MV-IGNet) [[303](#bib.bib303)]
    designs a global context adaptation module for adaptive learning of topology structures
    on multi-level spatial skeleton contexts. Spatial Graph Diffusion Convolutional
    (S-GDC) network [[304](#bib.bib304)] aims to learn new graphs by graph diffusion
    for capturing the connections of distant joints on the same body and two interacting
    bodies. In the above dynamic methods, the topology modeling is based only on joint
    information. By contrast, a language model knowledge-assisted GCN (LA-GCN) [[305](#bib.bib305)]
    applies large-scale language model to incorporate action-related prior information
    to learn topology for action recognition.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动态方法，GCNs 的拓扑在推理过程中是动态推断的。动作结构图卷积网络（AS-GCN）[[286](#bib.bib286)] 应用了 A-link
    推断模块来捕捉动作特定的相关性。双流自适应 GCN（2s-AGCN）[[287](#bib.bib287)] 和语义引导网络（SGN）[[301](#bib.bib301)]
    通过自注意力机制增强了拓扑学习，以建模两个关节之间的相关性。尽管拓扑动态建模有助于推断关节的内在关系，但由于捕获的拓扑与姿势无关，因此可能很难编码动作的上下文。因此，一些方法专注于上下文依赖的内在拓扑建模。在动态
    GCN [[302](#bib.bib302)] 中，结合了所有关节的上下文特征以学习关节之间的关系。通道拓扑细化 GCN（CTR-GCN）[[290](#bib.bib290)]
    关注于在不同通道中嵌入关节拓扑，而 InfoGCN [[291](#bib.bib291)] 通过基于信息瓶颈学习的潜在表示引入了基于注意力的图卷积来捕捉上下文依赖的拓扑。多级空间-时间激发图网络（ML-STGNet）[[298](#bib.bib298)]
    引入了基于 Transformer 的空间数据驱动激发模块，以数据依赖的方式学习不同样本的关节关系。多视图交互图网络（MV-IGNet）[[303](#bib.bib303)]
    设计了一个全球上下文适应模块，用于在多级空间骨架上下文中自适应地学习拓扑结构。空间图扩散卷积（S-GDC）网络[[304](#bib.bib304)] 旨在通过图扩散学习新的图，以捕捉同一身体和两个相互作用身体上远距离关节的连接。在上述动态方法中，拓扑建模仅基于关节信息。相比之下，语言模型知识辅助
    GCN（LA-GCN）[[305](#bib.bib305)] 应用了大规模语言模型，将动作相关的先验信息融入其中，以学习动作识别的拓扑。
- en: No matter the static or dynamic methods, they aim to construct different GCNs
    for modeling spatial and temporal features of actions. In contrast, some papers
    work on strategies to assist the ability of different GCNs. For instance, Wang
    et al. [[306](#bib.bib306)] proposed neural Koopman pooling to replace the temporal
    average/max pooling for aggregating spatial-temporal features. The Koopman pooling
    learns class-wise dynamics for better classification. Zhou et al. [[307](#bib.bib307)]
    presented a Feature Refinement head (FR Head) based on contrastive learning to
    improve the discriminative power of ambiguous actions. With the FR Head, the performance
    of some existing methods (eg. 2s-AGCN [[287](#bib.bib287)], CTR-GCN [[290](#bib.bib290)])
    can be improved by about 1%.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是静态方法还是动态方法，它们的目标都是构建不同的 GCN 来建模动作的空间和时间特征。相比之下，一些论文致力于辅助不同 GCN 的能力。例如，Wang
    等人 [[306](#bib.bib306)] 提出了神经 Koopman 池化来替代时间平均/最大池化，用于聚合空间-时间特征。Koopman 池化学习类别特定的动态特性，以便更好地分类。Zhou
    等人 [[307](#bib.bib307)] 提出了基于对比学习的特征细化头（FR Head），以提高对模糊动作的区分能力。通过 FR Head，一些现有方法（例如
    2s-AGCN [[287](#bib.bib287)]、CTR-GCN [[290](#bib.bib290)]）的性能可以提高约 1%。
- en: In summary, GCN-based methods can effectively utilize and handle the joint relations
    by topological networks but are generally limited to local spatial-temporal neighborhoods.
    Compared with static methods, dynamic methods have stronger generalization capabilities
    due to the dynamic topologies.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，基于 GCN 的方法可以通过拓扑网络有效利用和处理关节关系，但通常局限于局部空间-时间邻域。与静态方法相比，动态方法由于动态拓扑具有更强的泛化能力。
- en: 4.2.4 Transformer-based approach
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 基于 Transformer 的方法
- en: 'Transformer was originally designed for machine translation tasks in natural
    language processing. Vision Transformer (ViT) [[308](#bib.bib308)] is the first
    work to use a Transformer encoder to extract image features in computer vision.
    When introducing Transformer to skeleton-based action recognition, the core is
    how to design a better encoder for modeling spatial and temporal information of
    skeleton sequences. Compared with GCN-methods, Transformer-based methods can quickly
    obtain global topology information and enhance the correlation of non-physical
    joints. There are mainly three categories of methods: pure Transformer, hybid
    Transformer and unsupervised Transformer.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 最初是为自然语言处理中的机器翻译任务而设计的。视觉 Transformer（ViT）[[308](#bib.bib308)] 是第一个将
    Transformer 编码器应用于计算机视觉中的图像特征提取的工作。当将 Transformer 引入骨架动作识别时，核心在于如何设计一个更好的编码器，以建模骨架序列的空间和时间信息。与
    GCN 方法相比，基于 Transformer 的方法可以快速获取全局拓扑信息，并增强非物理关节的相关性。主要有三种方法类别：纯 Transformer、混合
    Transformer 和无监督 Transformer。
- en: The first category of methods applies the standard Transformer for learning
    spatial and temporal features. A spatial Transformer and a temporal Transformer
    are often applied alternately or together based on one stream [[309](#bib.bib309),
    [310](#bib.bib310), [311](#bib.bib311)] or two-stream [[312](#bib.bib312), [313](#bib.bib313),
    [314](#bib.bib314)] network. Shi et al. [[309](#bib.bib309)] proposed to decouple
    the data into spatial and temporal dimensions, where the spatial and temporal
    streams respectively include motion-irrelevant and motion-relevant features. A
    Decoupled Spatial-Temporal Attention Network (DSTA-Net) was proposed to encode
    the two streams sequentially based on the attention module. It allows modeling
    spatial-temporal dependencies between joints without the information about their
    positions or mutual connections. Ijaz et al. [[311](#bib.bib311)] proposed a multi-modal
    Transformer-based network for nursing activity recognition which fuses the encoding
    results of the spatial-temporal skeleton model and acceleration model. The spatial-temporal
    skeleton model comprises of spatial and temporal Transformer encoder in a sequential
    processing, which computes spatial and temporal features from joints. The acceleration
    model has one Transformer block, which computes correlation across acceleration
    data points for a given action sample. Zhang et al. [[312](#bib.bib312)] proposed
    a Spatial-Temporal Special Transformer (STST) to capture skeleton sequences in
    the temporal and spatial dimensions separately. STST is a two-stream structure
    including a spatial transformer block and a directional temporal transformer block.
    Relation-mining Self-Attention Network (RSA-Net) [[314](#bib.bib314)] applies
    seven RSA bolcks in spatial and temporal domains for learning intra-frame and
    inter-frame action features. Such a two-stream structure leads to the extension
    of the feature dimension and makes the network capture richer information, but
    at the same time increases the computational cost. To reduce the computational
    cost, Shi et al. [[313](#bib.bib313)] proposed a Sparse Transformer-based Action
    Recognition (ST-AR) model. ST-AR consists of a sparse self-attention module performed
    on sparse matrix multiplications for capturing spatial correlations, and a segmented
    linear self-attention module processed on variable lengths of sequences for capturing
    temporal correlations to further reduce the computation and memory cost.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类方法使用标准的Transformer来学习空间和时间特征。空间Transformer和时间Transformer通常交替或一起应用于单流[[309](#bib.bib309)、[310](#bib.bib310)、[311](#bib.bib311)]或双流[[312](#bib.bib312)、[313](#bib.bib313)、[314](#bib.bib314)]网络。Shi等[[309](#bib.bib309)]提出将数据解耦为空间和时间维度，其中空间和时间流分别包括与运动无关和与运动相关的特征。提出了一种解耦空间-时间注意力网络（DSTA-Net），基于注意力模块对两个流进行顺序编码。它允许在没有关节位置或相互连接信息的情况下建模关节之间的空间-时间依赖关系。Ijaz等[[311](#bib.bib311)]提出了一种基于多模态Transformer的网络，用于护理活动识别，该网络融合了空间-时间骨架模型和加速度模型的编码结果。空间-时间骨架模型包括顺序处理的空间和时间Transformer编码器，用于从关节中计算空间和时间特征。加速度模型具有一个Transformer块，用于计算给定动作样本的加速度数据点之间的相关性。Zhang等[[312](#bib.bib312)]提出了一种空间-时间特定Transformer（STST），分别捕捉时间和空间维度中的骨架序列。STST是一个包括空间Transformer块和方向性时间Transformer块的双流结构。Relation-mining自注意力网络（RSA-Net）[[314](#bib.bib314)]在空间和时间域应用了七个RSA块，以学习帧内和帧间动作特征。这种双流结构扩展了特征维度，使网络捕捉到更丰富的信息，但同时增加了计算成本。为了降低计算成本，Shi等[[313](#bib.bib313)]提出了一种基于稀疏Transformer的动作识别（ST-AR）模型。ST-AR包括一个在稀疏矩阵乘法上执行的稀疏自注意力模块，用于捕捉空间相关性，以及一个在可变长度序列上处理的分段线性自注意力模块，用于捕捉时间相关性，以进一步减少计算和内存成本。
- en: Since Transformer is weak in extracting discriminative information from local
    features and short-term temporal information, the second category of methods [[247](#bib.bib247),
    [315](#bib.bib315), [316](#bib.bib316), [317](#bib.bib317), [318](#bib.bib318),
    [319](#bib.bib319), [320](#bib.bib320), [321](#bib.bib321), [306](#bib.bib306),
    [322](#bib.bib322)] integrate Transformer with GCN and CNN for better feature
    extraction, which is beneficial to utilize the advantages of different networks.
    Plizzari et al. [[247](#bib.bib247)] proposed a two-stream Spatial-Temporal TRansformer
    network (ST-TR) by integrating spatial and temporal Transformers with Temporal
    Convolution Network and GCN. Qiu et al. [[316](#bib.bib316)] proposed a Spatio-Temporal
    Tuples Transformer (STTFormer) which includes a spatio-temporal tuples self-attention
    module for capturing joint relationship in consecutive frames, and an Inter-Frame
    Feature Aggregation (IFFA) module for enhancing the ability to distinguish similar
    actions. Similar to ST-TR, the IFFA module applies TCN to aggregate features of
    sub-actions. Yang et al. [[318](#bib.bib318)] presented Zoom-Former for extending
    single-person action recognition to multi-person group activities. The Zoom-Former
    improves the traditional GCN by designing a Relation-aware Attention mechanism,
    which comprehensively leverages the prior knowledge of body structure and the
    global characteristic of human motion to exploit the multi-level features. With
    this improvement, Zoom-Former could hierarchically extract the low-level motion
    information of a single person and the high-level interaction information of multiple
    people. To effectively capture the relationship between key local joints and global
    contextual information in the spatial and temporal dimension, Gao et al. [[319](#bib.bib319)]
    proposed an end-to-end Focal and Global Spatial-Temporal transFormer (FG-STForm)
    by integrating temporal convolutions into a global self-attention mechanism. Liu
    et al. [[320](#bib.bib320)] proposed a Kernel Attention Adaptive Graph Transformer
    Network to use a graph transformer operator for modeling higher-order spatial
    dependencies between joints. Wang et al. [[306](#bib.bib306)] proposed a Multi-order
    Multi-mode Transformer (3Mformer) by applying a higher-order Transformer to process
    hypergraphs of skeleton data for better capturing higher-order motion patterns
    between body joints. SkeleTR [[322](#bib.bib322)] initially employs a GCN to capture
    intra-person dynamic information and then applies a stacked Transformer encoder
    to model the person interaction. It can handle different tasks including video-level
    action recognition, instance-level action detection and group activity recognition.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Transformer 在提取局部特征和短期时间信息方面较弱，第二类方法[[247](#bib.bib247), [315](#bib.bib315),
    [316](#bib.bib316), [317](#bib.bib317), [318](#bib.bib318), [319](#bib.bib319),
    [320](#bib.bib320), [321](#bib.bib321), [306](#bib.bib306), [322](#bib.bib322)]
    将 Transformer 与 GCN 和 CNN 结合，以更好地提取特征，从而利用不同网络的优势。Plizzari 等人[[247](#bib.bib247)]
    提出了一个双流空间-时间 Transformer 网络（ST-TR），通过将空间和时间 Transformer 与时间卷积网络和 GCN 结合在一起。Qiu
    等人[[316](#bib.bib316)] 提出了一个时空元组 Transformer（STTFormer），其中包含一个时空元组自注意力模块，用于捕捉连续帧中的联合关系，以及一个帧间特征聚合（IFFA）模块，用于增强区分相似动作的能力。类似于
    ST-TR，IFFA 模块应用 TCN 来聚合子动作的特征。Yang 等人[[318](#bib.bib318)] 提出了 Zoom-Former，将单人动作识别扩展到多人体群体活动。Zoom-Former
    通过设计一种关系感知注意力机制来改进传统的 GCN，这种机制综合利用了身体结构的先验知识和人体运动的全球特征，从而挖掘多层次特征。通过这一改进，Zoom-Former
    能够分层提取单人的低级运动信息和多人的高级互动信息。为了有效捕捉空间和时间维度中关键局部关节与全球背景信息之间的关系，Gao 等人[[319](#bib.bib319)]
    提出了一个端到端的焦点与全局空间-时间 Transformer（FG-STForm），通过将时间卷积集成到全局自注意力机制中。Liu 等人[[320](#bib.bib320)]
    提出了一个内核注意力自适应图 Transformer 网络，使用图 Transformer 操作符来建模关节之间的高阶空间依赖性。Wang 等人[[306](#bib.bib306)]
    提出了一个多阶多模态 Transformer（3Mformer），通过应用高阶 Transformer 来处理骨架数据的超图，以更好地捕捉关节之间的高阶运动模式。SkeleTR[[322](#bib.bib322)]
    首先使用 GCN 来捕捉个体动态信息，然后应用堆叠的 Transformer 编码器来建模人员互动。它可以处理包括视频级动作识别、实例级动作检测和群体活动识别在内的不同任务。
- en: To improve the generalization ability of features, the third category of methods [[323](#bib.bib323),
    [324](#bib.bib324), [325](#bib.bib325), [326](#bib.bib326), [327](#bib.bib327),
    [328](#bib.bib328)] focus on unsupervised or self-supervised action recognition
    based on Transformer which has demonstrated excellent performance in capturing
    global context and local joint dynamics. These methods normally apply contrastive
    learning or Encoder-Decoder architecture for learning a better representation
    of actions. Kim et al. [[323](#bib.bib323)] proposed GL-Transformer, which designs
    a global and local attention mechanism to learn the local joint motion changes
    and global contextual information of skeleton sequences. With the motion sequence
    representation, actions are classified based on their average pooling on the temporal
    axis. Anshul et al. [[325](#bib.bib325)] designed the HaLP module by generating
    hallucinating latent positive samples for self-supervised learning based on contrastive
    learning. This module can explore the potential space of human postures in the
    appropriate directions to generate new positive samples, and optimize the solution
    efficiency by a new approximation function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高特征的泛化能力，第三类方法[[323](#bib.bib323), [324](#bib.bib324), [325](#bib.bib325),
    [326](#bib.bib326), [327](#bib.bib327), [328](#bib.bib328)]专注于基于Transformer的无监督或自监督动作识别，这在捕捉全局上下文和局部关节动态方面表现出色。这些方法通常应用对比学习或Encoder-Decoder架构来学习更好的动作表示。Kim等人[[323](#bib.bib323)]提出了GL-Transformer，它设计了一个全局和局部注意力机制，以学习骨架序列的局部关节运动变化和全局上下文信息。通过运动序列表示，基于时间轴上的平均池化对动作进行分类。Anshul等人[[325](#bib.bib325)]设计了HaLP模块，通过生成幻觉潜在正样本进行基于对比学习的自监督学习。该模块可以在适当的方向上探索人体姿态的潜在空间，以生成新的正样本，并通过新的近似函数优化解决方案效率。
- en: In summary, the research on skeleton-based action recognition has made great
    progress in recent years. CNN-based methods mainly convert skeleton sequences
    into images, excelling at capturing spatial information of actions but potentially
    losing temporal information. With the help of RNN for representing temporal information,
    RNN-based methods focus on representing spatial information based on the spatial
    division of the human body combining attention mechanism. Compared with CNN and
    RNN-based methods, GCN and Transformer-based methods have greater advantages and
    become the mainstream methods. GCN-based methods are beneficial for representing
    joint relations by topological networks in which dynamic topology-based methods
    have stronger generalization ability than static ones. However, they are mostly
    confined to local spatial-temporal neighborhoods. Transformer-based methods can
    quickly obtain global topology information and enhance the correlation of non-physical
    joints. Combining Transformers with CNN and GCN represents a promising approach
    for extracting both local and global features, enhancing action recognition performance.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，基于骨架的动作识别在近年来取得了巨大进展。基于CNN的方法主要将骨架序列转换为图像，擅长捕捉动作的空间信息，但可能丧失时间信息。借助RNN表示时间信息，基于RNN的方法侧重于基于人体的空间分割来表示空间信息，并结合注意力机制。与CNN和RNN方法相比，基于GCN和Transformer的方法具有更大的优势，已成为主流方法。基于GCN的方法通过拓扑网络表示关节关系，其中基于动态拓扑的方法比静态的具有更强的泛化能力。然而，它们大多局限于局部的时空邻域。基于Transformer的方法可以快速获取全局拓扑信息，并增强非物理关节的相关性。将Transformers与CNN和GCN结合，代表了一种提取局部和全局特征的有前途的方法，从而增强动作识别性能。
- en: 5 Benchmark datasets
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基准数据集
- en: This section reviews the commonly used datasets for the three tasks and also
    compares the performance of different methods on some popular datasets.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了用于三个任务的常用数据集，并比较了不同方法在一些流行数据集上的表现。
- en: 'Table 1: Datasets for 2D HPE. PCP: Percentage of Correct Localized Parts, PCPm:
    Mean Percentage of Correctly Localized Parts, PCK: Percentage of Correct Keypoints,
    PCKh: Percentage of Correct Keypoints with a specified head size, AP: Average
    Precision, mAP: mean Average Precision. IB: Image-based, VB: Video-based. SP:
    single person, MP: multi-person. Train, Val and Test represent frame numbers except
    for Penn Action and PoseTrack, and they represent video numbers.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：2D HPE数据集。PCP：正确定位部件的百分比，PCPm：正确定位部件的平均百分比，PCK：正确关键点的百分比，PCKh：指定头部尺寸的正确关键点百分比，AP：平均精度，mAP：平均平均精度。IB：基于图像，VB：基于视频。SP：单人，MP：多人。Train、Val和Test表示帧数，除了Penn
    Action和PoseTrack，它们表示视频数量。
- en: '|  | Dataset | Year | Citation | #Poses | #Joints | Train | Val | Test | SP/MP
    | Actions | Metrics |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据集 | 年份 | 引用 | 姿态数量 | 关节数量 | 训练 | 验证 | 测试 | SP/MP | 动作 | 指标 |'
- en: '| IB | LSP [[329](#bib.bib329)] | 2010 | 971 | 2,000 | 14 | 1k | - | 1k | SP
    | $\times$ | PCP/PCK |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| IB | LSP [[329](#bib.bib329)] | 2010 | 971 | 2,000 | 14 | 1k | - | 1k | SP
    | $\times$ | PCP/PCK |'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LSPET [[330](#bib.bib330)] &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LSPET [[330](#bib.bib330)] &#124;'
- en: '| 2011 | 509 | 10,000 | 14 | 10k | - | - | SP | $\times$ | PCP |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 2011 | 509 | 10,000 | 14 | 10k | - | - | SP | $\times$ | PCP |'
- en: '|'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; FLIC [[331](#bib.bib331)] &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FLIC [[331](#bib.bib331)] &#124;'
- en: '| 2013 | 537 | 5,003 | 10 | 4k | - | 1k | SP | $\times$ | PCK/PCP |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 537 | 5,003 | 10 | 4k | - | 1k | SP | $\times$ | PCK/PCP |'
- en: '|'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MPII [[332](#bib.bib332)] &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPII [[332](#bib.bib332)] &#124;'
- en: '| 2014 | 2583 | 26,429 | 16 | 29k | - | 12k | SP | ✓ | PCPm/PCKh |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 2583 | 26,429 | 16 | 29k | - | 12k | SP | ✓ | PCPm/PCKh |'
- en: '|'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MPII multi-person [[332](#bib.bib332)] &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPII multi-person [[332](#bib.bib332)] &#124;'
- en: '| 2014 | 2583 | 14,993 | 16 | 3.8k | - | 1.7k | MP | ✓ | mAP |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 2583 | 14,993 | 16 | 3.8k | - | 1.7k | MP | ✓ | mAP |'
- en: '|'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MSCOCO16 [[333](#bib.bib333)] &#124;'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSCOCO16 [[333](#bib.bib333)] &#124;'
- en: '| 2014 | 37862 | 105,698 | 17 | 45k | 22k | 80k | MP | $\times$ | AP |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 37862 | 105,698 | 17 | 45k | 22k | 80k | MP | $\times$ | AP |'
- en: '|'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MSCOCO17 [[333](#bib.bib333)] &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSCOCO17 [[333](#bib.bib333)] &#124;'
- en: '| 2014 | 37862 | - | 17 | 64k | 2.7k | 40k | MP | $\times$ | AP |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 37862 | - | 17 | 64k | 2.7k | 40k | MP | $\times$ | AP |'
- en: '|'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LIP [[334](#bib.bib334)] &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LIP [[334](#bib.bib334)] &#124;'
- en: '| 2017 | 482 | 50462 | 16 | 30k | 10k | 10k | SP | $\times$ | PCK |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 482 | 50462 | 16 | 30k | 10k | 10k | SP | $\times$ | PCK |'
- en: '|'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CrowdPose [[335](#bib.bib335)] &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CrowdPose [[335](#bib.bib335)] &#124;'
- en: '| 2019 | 423 | 80000 | 14 | 10k | 2k | 8k | MP | $\times$ | mAP |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 423 | 80000 | 14 | 10k | 2k | 8k | MP | $\times$ | mAP |'
- en: '| VB |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| VB |'
- en: '&#124; J-HMDB [[248](#bib.bib248)] &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; J-HMDB [[248](#bib.bib248)] &#124;'
- en: '| 2013 | 849 | 31,838 | 15 | 2.4k | - | 0.8k | SP | ✓ | PCK |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 849 | 31,838 | 15 | 2.4k | - | 0.8k | SP | ✓ | PCK |'
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Penn Action [[336](#bib.bib336)] &#124;'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Penn Action [[336](#bib.bib336)] &#124;'
- en: '| 2013 | 367 | 159,633 | 13 | 1k | - | 1k | SP | ✓ | PCK |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 367 | 159,633 | 13 | 1k | - | 1k | SP | ✓ | PCK |'
- en: '|'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PoseTrack17 [[337](#bib.bib337)] &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PoseTrack17 [[337](#bib.bib337)] &#124;'
- en: '| 2017 | 420 | 153,615 | 15 | 292 | 50 | 208 | MP | ✓ | mAP |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 420 | 153,615 | 15 | 292 | 50 | 208 | MP | ✓ | mAP |'
- en: '|  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; PoseTrack18 [[337](#bib.bib337)] &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PoseTrack18 [[337](#bib.bib337)] &#124;'
- en: '| 2018 | 420 | - | 15 | 593 | 170 | 375 | MP | ✓ | mAP |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 420 | - | 15 | 593 | 170 | 375 | MP | ✓ | mAP |'
- en: '|  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; PoseTrack21 [[338](#bib.bib338)] &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PoseTrack21 [[338](#bib.bib338)] &#124;'
- en: '| 2022 | 15 | - | 15 | 593 | 170 | - | MP | ✓ | mAP |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | 15 | - | 15 | 593 | 170 | - | MP | ✓ | mAP |'
- en: 'Table 2: Datasets for 3D HPE. MPJPE:Mean Per Joint Position Error, PA-MPJPE:
    Procrustes Analysis Mean Per Joint Position Error, MPJAE: Mean Per Joint Angular
    Erro, 3DPCK: 3D Percentage of Correct Keypoints, MPJAE: Mean Per Joint Angular
    Error, AP: Average Precision.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 3D HPE 数据集。MPJPE: 平均每个关节位置误差，PA-MPJPE: Procrustes 分析平均每个关节位置误差，MPJAE:
    平均每个关节角度误差，3DPCK: 3D 正确关键点的百分比，AP: 平均精度。'
- en: '|  | Dataset | Year | Citation | #Joints | #Frames | SP/MP | Actions | Metrics
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据集 | 年份 | 引用 | 关节数量 | 帧数 | SP/MP | 动作 | 指标 |'
- en: '| VB |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| VB |'
- en: '&#124; HumanEva-I [[339](#bib.bib339)] &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HumanEva-I [[339](#bib.bib339)] &#124;'
- en: '| 2010 | 1678 | 15 | 37.6k | SP | ✓ | MPJPE/PA-MPJPE |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | 1678 | 15 | 37.6k | SP | ✓ | MPJPE/PA-MPJPE |'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Human3.6M [[340](#bib.bib340)] &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Human3.6M [[340](#bib.bib340)] &#124;'
- en: '| 2014 | 2677 | 17 | 3.6M | SP | ✓ | MPJPE |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 2677 | 17 | 3.6M | SP | ✓ | MPJPE |'
- en: '|'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MPI-INF-3DHP [[143](#bib.bib143)] &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPI-INF-3DHP [[143](#bib.bib143)] &#124;'
- en: '| 2017 | 851 | 15 | 1.3M | SP | ✓ | 3DPCK |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 851 | 15 | 1.3M | SP | ✓ | 3DPCK |'
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CMU Panoptic [[341](#bib.bib341)] &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CMU Panoptic [[341](#bib.bib341)] &#124;'
- en: '| 2017 | 680 | 15 | 1.5M | MP | ✓ | 3DPCK/MPJPE |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 680 | 15 | 1.5M | MP | ✓ | 3DPCK/MPJPE |'
- en: '|'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3DPW [[342](#bib.bib342)] &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3DPW [[342](#bib.bib342)] &#124;'
- en: '| 2018 | 674 | 18 | 51k | MP | $\times$ | MPJPE/MPJAE/PA-MPJPE |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 674 | 18 | 51k | MP | $\times$ | MPJPE/MPJAE/PA-MPJPE |'
- en: '|'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MuPoTs-3D [[185](#bib.bib185)] &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MuPoTs-3D [[185](#bib.bib185)] &#124;'
- en: '| 2018 | 346 | 15 | 8k | MP | $\times$ | 3DPCK |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 346 | 15 | 8k | MP | $\times$ | 3DPCK |'
- en: '|  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; MuCo-3DHP [[185](#bib.bib185)] &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MuCo-3DHP [[185](#bib.bib185)] &#124;'
- en: '| 2018 | 346 | - | - | MP | $\times$ | 3DPCK |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 346 | - | - | MP | $\times$ | 3DPCK |'
- en: 5.1 Pose estimation
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 姿态估计
- en: 'The datasets are reviewed based on 2D and 3D pose estimation tasks and the
    details are summarized in Table [1](#S5.T1 "Table 1 ‣ 5 Benchmark datasets ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey")
    and [2](#S5.T2 "Table 2 ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey"). Due to the page
    limit, we mainly review some popular and large-scale pose datasets in the following
    sections.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '这些数据集基于2D和3D姿态估计任务进行了审查，详细信息总结在表格[1](#S5.T1 "Table 1 ‣ 5 Benchmark datasets
    ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey")和[2](#S5.T2 "Table 2 ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey")中。由于页面限制，以下部分主要回顾了一些流行的大规模姿态数据集。'
- en: 5.1.1 Datasets for 2D pose estimation
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 2D姿态估计的数据集
- en: For the image-based 2D pose estimation, Microsoft Common Objects in Context
    (COCO) [[333](#bib.bib333)] and Max Planck Institute for Informatics (MPII) [[332](#bib.bib332)]
    are popular datasets. Joint-annotated HMDB (J-HMDB) dataset [[248](#bib.bib248)]
    and Penn Action [[336](#bib.bib336)] datasets are often used for the 2D video-based
    single-person pose estimation (SPPE), while PoseTrack [[337](#bib.bib337)] is
    often used for video-based multiple-person pose estimation (MPPE).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于图像的2D姿态估计，微软的常见物体上下文（COCO）[[333](#bib.bib333)]和马普计算机科学研究所（MPII）[[332](#bib.bib332)]是流行的数据集。联合标注的HMDB（J-HMDB）数据集[[248](#bib.bib248)]和Penn
    Action[[336](#bib.bib336)]数据集通常用于2D视频中的单人姿态估计（SPPE），而PoseTrack[[337](#bib.bib337)]则常用于视频中的多人姿态估计（MPPE）。
- en: 'The COCO dataset [[333](#bib.bib333)] is the most widely used large-scale dataset
    for pose estimation. It was created by extracting everyday scene images with common
    objects and labeling the objects using per-instance segmentation. This dataset
    consists of more than 330,000 images and 200,000 labeled persons, and each person
    is labeled with 17 keypoints. It has two versions for pose estimation including
    COCO2016 and COCO2017\. The two versions are different with the number of images
    for training, testing and validation as shown in Table [1](#S5.T1 "Table 1 ‣ 5
    Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey"). Except of pose estimation, this dataset can be
    also suitable for object detection, image segmentation and captioning.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'COCO数据集[[333](#bib.bib333)]是姿态估计中最广泛使用的大规模数据集。它通过提取带有常见物体的日常场景图像并使用实例分割对物体进行标注创建。该数据集包含超过330,000张图像和200,000个标注的人物，每个人物标注有17个关键点。它有两个姿态估计版本，包括COCO2016和COCO2017。两个版本在训练、测试和验证的图像数量上有所不同，如表格[1](#S5.T1
    "Table 1 ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action
    Recognition with Deep Learning: A Survey")所示。除了姿态估计之外，该数据集还适用于物体检测、图像分割和图像说明。'
- en: The MPII dataset [[332](#bib.bib332)] was collected from 3,913 YouTube videos
    by the Max Planck Institute for Informatics. It consists of 24,920 images including
    over 40,000 individuals with 16 annotated body joints. These images were collected
    by a two-level hierarchical method to capture everyday human activities. This
    dataset involves 491 activity samples in 21 classes and all the images are labeled.
    Except for joints, rich annotations including body occlusion, 3D torso and head
    orientations are also labeled on Amazon Mechanical Turk. The MPII dataset serves
    as a valuable resource for both 2D single-person and multi-person pose estimation.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: MPII数据集[[332](#bib.bib332)]由马普计算机科学研究所从3,913个YouTube视频中收集。它包含24,920张图像，涉及40,000多个人体，标注了16个关节。这些图像通过两级层次方法收集，以捕捉日常人类活动。该数据集涉及21个类别的491个活动样本，所有图像均已标注。除了关节，Amazon
    Mechanical Turk上还标注了丰富的注释，包括身体遮挡、3D躯干和头部方向。MPII数据集为2D单人和多人姿态估计提供了宝贵的资源。
- en: The J-HMDB dataset  [[248](#bib.bib248)] was created by annotating human joints
    of the HMDB51 action dataset. From HMDB51, 928 videos including 21 actions of
    a single person were extracted and the human joints of each were annotated using
    a 2D articulated human puppet model. Each video consists of 15-40 frames. In total,
    there are 31,838 annotated frames. This dataset can serve as a benchmark for human
    detection, pose estimation, pose tracking and action recognition. It also presents
    a new challenge for video-based pose estimation or tracking since it includes
    more variations in camera motions, motion blur and partial or full-body visibility.
    Sub-J-HMDB dataset [[248](#bib.bib248)] is a subset of the J-HMDB dataset and
    contains 316 videos with a total of 11,200 frames.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: J-HMDB 数据集 [[248](#bib.bib248)] 是通过对 HMDB51 动作数据集的人体关节进行标注而创建的。从 HMDB51 中提取了
    928 个视频，包括 21 种单人动作，每个视频的人体关节都使用 2D 人物模型进行了标注。每个视频包含 15-40 帧。总共有 31,838 帧已标注。该数据集可以作为人体检测、姿态估计、姿态跟踪和动作识别的基准。它还为基于视频的姿态估计或跟踪提出了新的挑战，因为它包括更多的摄像机运动、运动模糊和部分或全身可见性变化。Sub-J-HMDB
    数据集 [[248](#bib.bib248)] 是 J-HMDB 数据集的一个子集，包含 316 个视频，共 11,200 帧。
- en: The Penn Action dataset [[336](#bib.bib336)] is also an annotated sports action
    dataset collected by the University of Pennsylvania. It consists of 2,326 videos
    with 15 actions and each frame was annotated with 13 keypoints for each person.
    The dataset can be used for the tasks of pose estimation, action detection and
    recognition.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Penn Action 数据集 [[336](#bib.bib336)] 也是由宾夕法尼亚大学收集的标注运动动作数据集。它包含 2,326 个视频，涉及
    15 种动作，每帧标注了每个人的 13 个关键点。该数据集可用于姿态估计、动作检测和识别任务。
- en: 'The PoseTrack Dataset [[337](#bib.bib337)] was collected from raw videos of
    the MPII Pose Dataset. For each frame in MPII, 41-298 neighboring frames with
    crowded scenes and multiple individuals were selected for PoseTrack dataset. The
    selected videos were annotated with person locations, identities, body pose and
    ignore regions. According to different number of videos, this dataset currently
    exists in three versions: PoseTrack2017, PoseTrack2018, and PoseTrack2021\. In
    total, PoseTrack2017 contains 292 videos for training, and 50 videos for validation
    and 208 videos for testing. Among them, 23,000 frames are labeled with a very
    lager number (i.e. 153,615) of annotated poses. PoseTrack2018 increases the number
    of the video and contains 593 videos for training, 170 videos for validation,
    and 315 videos for testing, and consists of 46,933 labeled frames. PoseTrack2021
    is an extension of PoseTrack2018 with more annotations (eg. bounding box of small
    persons, joint occlusions). With the person identities, this dataset has been
    widely used as a benchmark to evaluate multi-person pose estimation and tracking
    algorithms.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: PoseTrack 数据集 [[337](#bib.bib337)] 是从 MPII Pose 数据集的原始视频中收集的。对于 MPII 中的每一帧，PoseTrack
    数据集选择了 41-298 帧包含拥挤场景和多个个体的相邻帧。所选视频进行了人物位置、身份、身体姿态和忽略区域的标注。根据视频数量的不同，该数据集目前存在三个版本：PoseTrack2017、PoseTrack2018
    和 PoseTrack2021。总的来说，PoseTrack2017 包含 292 个用于训练的视频，50 个用于验证的视频和 208 个用于测试的视频。其中，23,000
    帧被标注了非常大量（即 153,615）的标注姿态。PoseTrack2018 增加了视频数量，包括 593 个用于训练的视频，170 个用于验证的视频和
    315 个用于测试的视频，并包含 46,933 帧标注帧。PoseTrack2021 是 PoseTrack2018 的扩展，增加了更多的标注（例如，小人物的边界框、关节遮挡）。有了人物身份，这个数据集被广泛用作评估多人物姿态估计和跟踪算法的基准。
- en: 'Table 3: Performance comparison for 2D image-based pose estimation on COCO
    dataset.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于 2D 图像的 COCO 数据集姿态估计性能比较。
- en: '|  | Category | Year | Method | COCO |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | 类别 | 年份 | 方法 | COCO |'
- en: '|  | Backbone | Inputsize | AP | AP.5 | AP.75 | APM | APL |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | 主干 | 输入尺寸 | AP | AP.5 | AP.75 | APM | APL |'
- en: '| SP | Regression-based | 2021 | TFPose [[34](#bib.bib34)] | ResNet-50 | 384×288
    | 72.2 | 90.9 | 80.1 | 69.1 | 78.8 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| SP | 基于回归 | 2021 | TFPose [[34](#bib.bib34)] | ResNet-50 | 384×288 | 72.2
    | 90.9 | 80.1 | 69.1 | 78.8 |'
- en: '| 2021 | PRTR [[35](#bib.bib35)] | HRNet-W32 | 512×384 | 72.1 | 90.4 | 79.6
    | 68.1 | 79.4 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | PRTR [[35](#bib.bib35)] | HRNet-W32 | 512×384 | 72.1 | 90.4 | 79.6
    | 68.1 | 79.4 |'
- en: '| 2022 | Panteleris et al. [[37](#bib.bib37)] | - | 384×288 | 72.6 | - | -
    | - | - |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Panteleris 等 [[37](#bib.bib37)] | - | 384×288 | 72.6 | - | - | - |
    - |'
- en: '| Heatmap-based | 2021 | Li et al. [[63](#bib.bib63)] | HRNet-W48 | - | 75.7
    | 92.3 | 82.9 | 72.3 | 81.3 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 基于热图 | 2021 | Li 等 [[63](#bib.bib63)] | HRNet-W48 | - | 75.7 | 92.3 | 82.9
    | 72.3 | 81.3 |'
- en: '| 2022 | Li et al. [[62](#bib.bib62)] | HRNet-W48 | 384×288 | 76.0 | 92.4 |
    83.5 | 72.5 | 81.9 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Li 等 [[62](#bib.bib62)] | HRNet-W48 | 384×288 | 76.0 | 92.4 | 83.5
    | 72.5 | 81.9 |'
- en: '| 2023 | DistilPose [[64](#bib.bib64)] | HRNet-W48-stage3 | 256×192 | 73.7
    | 91.6 | 81.1 | 70.2 | 79.6 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | DistilPose [[64](#bib.bib64)] | HRNet-W48-stage3 | 256×192 | 73.7
    | 91.6 | 81.1 | 70.2 | 79.6 |'
- en: '| MP | Top-down | 2017 | Papandreou et al. [[67](#bib.bib67)] | ResNet-101
    | 353×257 | 68.5 | 87.1 | 75.5 | 65.8 | 73.3 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| MP | 自顶向下 | 2017 | Papandreou et al. [[67](#bib.bib67)] | ResNet-101 | 353×257
    | 68.5 | 87.1 | 75.5 | 65.8 | 73.3 |'
- en: '| 2017 | RMPE [[81](#bib.bib81)] | Hourglass | - | 61.8 | 83.7 | 69.8 | 58.6
    | 67.6 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | RMPE [[81](#bib.bib81)] | Hourglass | - | 61.8 | 83.7 | 69.8 | 58.6
    | 67.6 |'
- en: '| 2018 | Xiao et al. [[69](#bib.bib69)] | ResNet-152 | 384×288 | 73.7 | 91.9
    | 81.1 | 70.3 | 80.0 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Xiao et al. [[69](#bib.bib69)] | ResNet-152 | 384×288 | 73.7 | 91.9
    | 81.1 | 70.3 | 80.0 |'
- en: '| 2018 | CPN [[82](#bib.bib82)] | ResNet | 384×288 | 73.0 | 91.7 | 80.9 | 69.5
    | 78.1 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | CPN [[82](#bib.bib82)] | ResNet | 384×288 | 73.0 | 91.7 | 80.9 | 69.5
    | 78.1 |'
- en: '| 2019 | Posefix [[70](#bib.bib70)] | ResNet-152 | 384×288 | 73.6 | 90.8 |
    81.0 | 70.3 | 79.8 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Posefix [[70](#bib.bib70)] | ResNet-152 | 384×288 | 73.6 | 90.8 |
    81.0 | 70.3 | 79.8 |'
- en: '| 2019 | Sun et al. [[71](#bib.bib71)] | HRNet-W48 | 384×288 | 77 | 92.7 |
    84.5 | 73.4 | 83.1 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Sun et al. [[71](#bib.bib71)] | HRNet-W48 | 384×288 | 77 | 92.7 |
    84.5 | 73.4 | 83.1 |'
- en: '| 2019 | Su et al. [[83](#bib.bib83)] | ResNet-152 | 384×288 | 74.6 | 91.8
    | 82.1 | 70.9 | 80.6 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Su et al. [[83](#bib.bib83)] | ResNet-152 | 384×288 | 74.6 | 91.8
    | 82.1 | 70.9 | 80.6 |'
- en: '| 2020 | Cai et al. [[72](#bib.bib72)] | 4×RSN-50 | 384×288 | 78.6 | 94.3 |
    86.6 | 75.5 | 83.3 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Cai et al. [[72](#bib.bib72)] | 4×RSN-50 | 384×288 | 78.6 | 94.3 |
    86.6 | 75.5 | 83.3 |'
- en: '| 2020 | Huang et al. [[73](#bib.bib73)] | HRNet | 384×288 | 77.5 | 92.7 |
    84.0 | 73.0 | 82.4 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Huang et al. [[73](#bib.bib73)] | HRNet | 384×288 | 77.5 | 92.7 |
    84.0 | 73.0 | 82.4 |'
- en: '| 2020 | Zhang et al. [[74](#bib.bib74)] | HRNet-W48 | 384×288 | 77.4 | 92.6
    | 84.6 | 73.6 | 83.7 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Zhang et al. [[74](#bib.bib74)] | HRNet-W48 | 384×288 | 77.4 | 92.6
    | 84.6 | 73.6 | 83.7 |'
- en: '| 2020 | Graphpcnn [[75](#bib.bib75)] | HR48 | 384×288 | 76.8 | 92.6 | 84.3
    | 73.3 | 82.7 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Graphpcnn [[75](#bib.bib75)] | HR48 | 384×288 | 76.8 | 92.6 | 84.3
    | 73.3 | 82.7 |'
- en: '| 2020 | Qiu et al. [[84](#bib.bib84)] | - | 384×288 | 74.1 | 91.9 | 82.2 |
    - | - |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Qiu et al. [[84](#bib.bib84)] | - | 384×288 | 74.1 | 91.9 | 82.2 |
    - | - |'
- en: '| 2021 | TransPose [[85](#bib.bib85)] | HRNet-W48 | 256×192 | 75.0 | 92.2 |
    82.3 | 71.3 | 81.1 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | TransPose [[85](#bib.bib85)] | HRNet-W48 | 256×192 | 75.0 | 92.2 |
    82.3 | 71.3 | 81.1 |'
- en: '| 2021 | TokenPose [[87](#bib.bib87)] | - | 384×288 | 75.9 | 92.3 | 83.4 |
    72.2 | 82.1 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | TokenPose [[87](#bib.bib87)] | - | 384×288 | 75.9 | 92.3 | 83.4 |
    72.2 | 82.1 |'
- en: '| 2021 | HRFormer [[88](#bib.bib88)] | - | 384×288 | 76.2 | 92.7 | 83.8 | 72.5
    | 82.3 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | HRFormer [[88](#bib.bib88)] | - | 384×288 | 76.2 | 92.7 | 83.8 | 72.5
    | 82.3 |'
- en: '| 2022 | ViTPose [[89](#bib.bib89)] | ViTAE-G | 576×432 | 81.1 | 95.0 | 88.2
    | 77.8 | 86.0 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | ViTPose [[89](#bib.bib89)] | ViTAE-G | 576×432 | 81.1 | 95.0 | 88.2
    | 77.8 | 86.0 |'
- en: '| 2022 | Xu et al. [[76](#bib.bib76)] | HR48 | 384×288 | 76.6 | 92.4 | 84.3
    | 73.2 | 82.5 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Xu et al. [[76](#bib.bib76)] | HR48 | 384×288 | 76.6 | 92.4 | 84.3
    | 73.2 | 82.5 |'
- en: '| 2023 | PGA-Net [[77](#bib.bib77)] | HRNet-W48 | 384x288 | 76.0 | 92.5 | 83.5
    | 72.4 | 82.1 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | PGA-Net [[77](#bib.bib77)] | HRNet-W48 | 384x288 | 76.0 | 92.5 | 83.5
    | 72.4 | 82.1 |'
- en: '| 2023 | BCIR [[78](#bib.bib78)] | HRNet-W48 | 384x288 | 76.1 | - | - | - |
    - |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | BCIR [[78](#bib.bib78)] | HRNet-W48 | 384x288 | 76.1 | - | - | - |
    - |'
- en: '| Bottom-up | 2017 | Associative embedding [[97](#bib.bib97)] | Hourglass |
    512×512 | 65.5 | 86.8 | 72.3 | 60.6 | 72.6 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 自底向上 | 2017 | Associative embedding [[97](#bib.bib97)] | Hourglass | 512×512
    | 65.5 | 86.8 | 72.3 | 60.6 | 72.6 |'
- en: '| 2018 | Multiposenet [[98](#bib.bib98)] | ResNet50 | 480×480 | 69.6 | 86.3
    | 76.6 | 65.0 | 76.3 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Multiposenet [[98](#bib.bib98)] | ResNet50 | 480×480 | 69.6 | 86.3
    | 76.6 | 65.0 | 76.3 |'
- en: '| 2018 | OpenPose [[93](#bib.bib93)] | - | - | 61.8 | 84.9 | 67.5 | 57.1 |
    68.2 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | OpenPose [[93](#bib.bib93)] | - | - | 61.8 | 84.9 | 67.5 | 57.1 |
    68.2 |'
- en: '| 2019 | Pifpaf [[94](#bib.bib94)] | ResNet50 | - | 55.0 | 76.0 | 57.9 | 39.4
    | 76.4 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Pifpaf [[94](#bib.bib94)] | ResNet50 | - | 55.0 | 76.0 | 57.9 | 39.4
    | 76.4 |'
- en: '| 2020 | Jin et al. [[100](#bib.bib100)] | Hourglass | 512×512 | 67.6 | 85.1
    | 73.7 | 62.7 | 74.6 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Jin et al. [[100](#bib.bib100)] | Hourglass | 512×512 | 67.6 | 85.1
    | 73.7 | 62.7 | 74.6 |'
- en: '| 2020 | Higherhrnet [[101](#bib.bib101)] | HrHRNet-W48 | 640×640 | 72.3 |
    91.5 | 79.8 | 67.9 | 78.2 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Higherhrnet [[101](#bib.bib101)] | HrHRNet-W48 | 640×640 | 72.3 |
    91.5 | 79.8 | 67.9 | 78.2 |'
- en: '| 2021 | DEKR [[103](#bib.bib103)] | HRNet-W48 | 640x640 | 71.0 | 89.2 | 78.0
    | 67.1 | 76.9 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | DEKR [[103](#bib.bib103)] | HRNet-W48 | 640x640 | 71.0 | 89.2 | 78.0
    | 67.1 | 76.9 |'
- en: '| 2023 | HOP [[96](#bib.bib96)] | HRNet-W48 | 640×640 | 70.5 | 89.3 | 77.2
    | 66.6 | 75.8 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | HOP [[96](#bib.bib96)] | HRNet-W48 | 640×640 | 70.5 | 89.3 | 77.2
    | 66.6 | 75.8 |'
- en: '| 2023 | Cheng et al. [[95](#bib.bib95)] | HRNet-W48 | 640×640 | 71.5 | 89.1
    | 78.5 | 67.2 | 78.1 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | Cheng et al. [[95](#bib.bib95)] | HRNet-W48 | 640×640 | 71.5 | 89.1
    | 78.5 | 67.2 | 78.1 |'
- en: '| 2023 | PolarPose [[104](#bib.bib104)] | HRNet-W48 | 640x640 | 70.2 | 89.5
    | 77.5 | 66.1 | 76.4 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | PolarPose [[104](#bib.bib104)] | HRNet-W48 | 640x640 | 70.2 | 89.5
    | 77.5 | 66.1 | 76.4 |'
- en: '| One-stage | 2019 | Directpose [[105](#bib.bib105)] | ResNet-101 | 800×800
    | 64.8 | 87.8 | 71.1 | 60.4 | 71.5 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 一阶段 | 2019 | Directpose [[105](#bib.bib105)] | ResNet-101 | 800×800 | 64.8
    | 87.8 | 71.1 | 60.4 | 71.5 |'
- en: '| 2021 | FCPose [[106](#bib.bib106)] | DLA-60 | 736 × 512 | 65.9 | 89.1 | 72.6
    | 60.9 | 74.1 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | FCPose [[106](#bib.bib106)] | DLA-60 | 736 × 512 | 65.9 | 89.1 | 72.6
    | 60.9 | 74.1 |'
- en: '| 2021 | InsPose [[107](#bib.bib107)] | HRNet-w32 | - | 71.0 | 91.3 | 78.0
    | 67.5 | 76.5 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | InsPose [[107](#bib.bib107)] | HRNet-w32 | - | 71.0 | 91.3 | 78.0
    | 67.5 | 76.5 |'
- en: '| 2022 | PETR [[109](#bib.bib109)] | Swin-L | - | 71.2 | 91.4 | 79.6 | 66.9
    | 78.0 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | PETR [[109](#bib.bib109)] | Swin-L | - | 71.2 | 91.4 | 79.6 | 66.9
    | 78.0 |'
- en: '|  | 2023 | ED-pose [[65](#bib.bib65)] | Swin-L | - | 72.7 | 92.3 | 80.9 |
    67.6 | 80.0 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | ED-pose [[65](#bib.bib65)] | Swin-L | - | 72.7 | 92.3 | 80.9 |
    67.6 | 80.0 |'
- en: '|  | 2023 | GroupPose [[110](#bib.bib110)] | Swin-L | - | 72.8 | 92.5 | 81.0
    | 67.7 | 80.3 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | GroupPose [[110](#bib.bib110)] | Swin-L | - | 72.8 | 92.5 | 81.0
    | 67.7 | 80.3 |'
- en: '|  | 2023 | SMPR [[108](#bib.bib108)] | HRNet-w32 | 800x800 | 70.2 | 89.7 |
    77.5 | 65.9 | 77.2 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | SMPR [[108](#bib.bib108)] | HRNet-w32 | 800x800 | 70.2 | 89.7 |
    77.5 | 65.9 | 77.2 |'
- en: 'Table 4: Performance comparison for 2D video-based SPPE on Penn Action dataset
    and JHMDB dataset. FF: frame-by-frame; SF: sample frame-based.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: Penn Action 数据集和 JHMDB 数据集上 2D 视频基础 SPPE 的性能比较。FF: 帧对帧; SF: 采样帧基础。'
- en: '| Category | Year | Method | Penn | JHMDB |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 年份 | 方法 | Penn | JHMDB |'
- en: '| PCK | PCK |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| PCK | PCK |'
- en: '| FF | 2016 | Gkioxari et al. [[116](#bib.bib116)] | 91.8 | - |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| FF | 2016 | Gkioxari et al. [[116](#bib.bib116)] | 91.8 | - |'
- en: '| 2017 | Song et al. [[114](#bib.bib114)] | 96.4 | 92.1 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Song et al. [[114](#bib.bib114)] | 96.4 | 92.1 |'
- en: '| 2018 | LSTM [[118](#bib.bib118)] | 97.7 | 93.6 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | LSTM [[118](#bib.bib118)] | 97.7 | 93.6 |'
- en: '| 2019 | DKD [[119](#bib.bib119)] | 97.8 | 94 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | DKD [[119](#bib.bib119)] | 97.8 | 94 |'
- en: '| 2019 | Li et al. [[120](#bib.bib120)] | - | 94.8 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Li et al. [[120](#bib.bib120)] | - | 94.8 |'
- en: '| 2022 | RPSTN [[123](#bib.bib123)] | 98.7 | 97.7 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | RPSTN [[123](#bib.bib123)] | 98.7 | 97.7 |'
- en: '| 2023 | HANet [[124](#bib.bib124)] | - | 99.6 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | HANet [[124](#bib.bib124)] | - | 99.6 |'
- en: '| SF | 2020 | K-FPN [[125](#bib.bib125)] | 98 | 94.7 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| SF | 2020 | K-FPN [[125](#bib.bib125)] | 98 | 94.7 |'
- en: '| 2022 | REMOTE [[126](#bib.bib126)] | 98.6 | 95.9 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | REMOTE [[126](#bib.bib126)] | 98.6 | 95.9 |'
- en: '| 2022 | DeciWatch [[127](#bib.bib127)] | - | 98.9 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | DeciWatch [[127](#bib.bib127)] | - | 98.9 |'
- en: '| 2023 | MixSynthFormer [[128](#bib.bib128)] | - | 99.3 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | MixSynthFormer [[128](#bib.bib128)] | - | 99.3 |'
- en: 'Table 5: Performance comparison for 2D video-based MPPE on PoseTrack2017 dataset.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: PoseTrack2017 数据集上 2D 视频基础 MPPE 的性能比较。'
- en: '| Category | Year | Method | Val | Test |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 年份 | 方法 | 验证 | 测试 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| mAP | mAP |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| mAP | mAP |'
- en: '| --- | --- |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Top-down | 2018 | Xiao et al. [[69](#bib.bib69)] | 76.7 | 73.9 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下 | 2018 | Xiao et al. [[69](#bib.bib69)] | 76.7 | 73.9 |'
- en: '| 2018 | Pose Flow [[129](#bib.bib129)] | 66.5 | 63.0 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Pose Flow [[129](#bib.bib129)] | 66.5 | 63.0 |'
- en: '| 2018 | Detect-Track [[130](#bib.bib130)] | - | 64.1 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Detect-Track [[130](#bib.bib130)] | - | 64.1 |'
- en: '| 2020 | Wang et al. [[131](#bib.bib131)] | 81.5 | 73.5 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Wang et al. [[131](#bib.bib131)] | 81.5 | 73.5 |'
- en: '| 2022 | AlphaPose [[132](#bib.bib132)] | 74.7 | - |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | AlphaPose [[132](#bib.bib132)] | 74.7 | - |'
- en: '| 2023 | SLT-Pose [[134](#bib.bib134)] | 81.5 | - |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | SLT-Pose [[134](#bib.bib134)] | 81.5 | - |'
- en: '| 2023 | DiffPose [[137](#bib.bib137)] | 83.0 | - |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | DiffPose [[137](#bib.bib137)] | 83.0 | - |'
- en: '|  | 2023 | TDMI [[133](#bib.bib133)] | 83.6 | - |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | TDMI [[133](#bib.bib133)] | 83.6 | - |'
- en: '| Bottom-up | 2019 | PGG [[138](#bib.bib138)] | 77.0 | - |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上 | 2019 | PGG [[138](#bib.bib138)] | 77.0 | - |'
- en: 5.1.2 Datasets for 3D pose estimation
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 3D 姿态估计的数据集
- en: Compared with the 2D datasets, acquiring high-quality annotation for 3D poses
    is more challenging and requires motion caption systems (eg., Mocap, wearable
    IMUs). Therefore, 3D pose datasets are normally built in constrained environments.
    Currently, Human3.6M and MPI-INF-3DHP are widely used for the task of SPPE, and
    MuPoTs-3D is often used for MPPE task.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 与 2D 数据集相比，获取高质量的 3D 姿态标注更具挑战性，需要动作捕捉系统（例如 Mocap、可穿戴 IMU）。因此，3D 姿态数据集通常在受限环境中构建。目前，Human3.6M
    和 MPI-INF-3DHP 被广泛用于 SPPE 任务，而 MuPoTs-3D 常用于 MPPE 任务。
- en: The Human3.6M dataset [[340](#bib.bib340)] is the largest and most representation
    indoor dataset for 3D single-person pose estimation. It was collected by recording
    videos of 11 human subjects performing 17 activities from 4 camera views, and
    capturing poses by marker-based Mocap systems. In total, this dataset consists
    of 3.6 million poses with one pose in one frame. This dataset is suitable for
    the HPE task from images or videos. With video-based HPE, a sequence of frames
    in a suitable receptive field is considered as the input. Protocol 1 is the most
    common protocol which applies frames of 5 subjects (S1, S5, S6, S7, S8) for training
    and the frames of 2 subjects (S9, S11) for test.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Human3.6M 数据集 [[340](#bib.bib340)] 是用于 3D 单人姿态估计的最大且最具代表性的室内数据集。它通过录制 11 名被试执行
    17 种活动的 4 个摄像头视角的视频，并通过基于标记的 Mocap 系统捕捉姿态来收集数据。总计，该数据集包含 360 万个姿态，每个姿态位于一个帧中。该数据集适用于来自图像或视频的
    HPE 任务。对于基于视频的 HPE，适当感受场域中的一系列帧被认为是输入。协议 1 是最常用的协议，它应用 5 名被试（S1, S5, S6, S7, S8）的帧进行训练，并用
    2 名被试（S9, S11）的帧进行测试。
- en: The MPI-INF-3DHP dataset [[143](#bib.bib143)] is a large 3D single-person pose
    dataset in both indoor and outdoor environments. It was captured by a maker-less
    MoCap system in a multi-camera studio. There are 8 subjects performing 8 activities
    from 14 camera views. This dataset provides 1.3 million frames, but more diverse
    motions than Human3.6M. Same as Human3.6M, this dataset is also suitable for the
    HPE task from images or videos. The test set includes the frames of 6 subjects
    with different scenes.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: MPI-INF-3DHP 数据集 [[143](#bib.bib143)] 是一个大型的 3D 单人姿态数据集，涵盖室内和室外环境。它由多摄像头工作室中的无标记
    MoCap 系统捕获。数据集中包含 8 名被试执行 8 种活动的 14 个摄像头视角的帧。该数据集提供了 130 万帧，比 Human3.6M 具有更多的动作多样性。与
    Human3.6M 相同，该数据集也适用于来自图像或视频的 HPE 任务。测试集包括 6 名被试的帧，涵盖不同的场景。
- en: The MuPoTs-3D dataset [[185](#bib.bib185)] is a multi-person 3D pose dataset
    in both indoor and outdoor environments. Same as MPI-INF-3DHP, it was also captured
    by a multi-view marker-less MoCap system. Over 8,000 frames were collected in
    20 videos by 8 subjects. There are some challenging frames with occlusions, drastic
    illumination changes and lens flares in some outdoor scenes.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: MuPoTs-3D 数据集 [[185](#bib.bib185)] 是一个包含室内和室外环境的多人人体 3D 姿态数据集。与 MPI-INF-3DHP
    相同，它也是通过多视角无标记 MoCap 系统捕捉的。数据集中包含 8 名被试的 20 个视频中的 8,000 帧。数据集中有一些具有挑战性的帧，包括遮挡、剧烈光照变化以及某些室外场景中的镜头光晕。
- en: 'Table 6: Performance comparison for 3D SPPE on Human3.6M and MPI-INF-3DHP dataset.
    IB: Image-based, VB: Video-based.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：Human3.6M 和 MPI-INF-3DHP 数据集上 3D SPPE 的性能比较。IB：基于图像，VB：基于视频。
- en: '|  | Category | Year | Method |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  | 类别 | 年份 | 方法 |'
- en: '&#124; Human3.6M &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Human3.6M &#124;'
- en: '&#124; MPJPE$\downarrow$   PMPJPE$\downarrow$ &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPJPE$\downarrow$   PMPJPE$\downarrow$ &#124;'
- en: '|'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MPI-INF-3DHP &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPI-INF-3DHP &#124;'
- en: '&#124; PCK      AUC &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PCK      AUC &#124;'
- en: '|'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| IB | One-stage | 2015 | Li et al. [[140](#bib.bib140)] | 122.0 | - | - |
    - |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| IB | 单阶段 | 2015 | Li 等人 [[140](#bib.bib140)] | 122.0 | - | - | - |'
- en: '| 2016 | Zhou et al. [[142](#bib.bib142)] | 107.3 | - | - | - |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Zhou 等人 [[142](#bib.bib142)] | 107.3 | - | - | - |'
- en: '| 2017 | Mehta et al. [[143](#bib.bib143)] | 74.1 | - | 57.3 | 28.0 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Mehta 等人 [[143](#bib.bib143)] | 74.1 | - | 57.3 | 28.0 |'
- en: '| 2017 | WTL [[144](#bib.bib144)] | 64.9 | - | 69.2 | 32.5 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | WTL [[144](#bib.bib144)] | 64.9 | - | 69.2 | 32.5 |'
- en: '| Two-stage | 2017 | Martinez et al. [[145](#bib.bib145)] | 62.9 | 47.7 | -
    | - |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 双阶段 | 2017 | Martinez 等人 [[145](#bib.bib145)] | 62.9 | 47.7 | - | - |'
- en: '| 2017 | Tekin et al. [[146](#bib.bib146)] | 69.7 | - | - | - |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Tekin 等人 [[146](#bib.bib146)] | 69.7 | - | - | - |'
- en: '| 2017 | Jahangiri et al. [[150](#bib.bib150)] | - | 68.0 | - | - |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Jahangiri 等人 [[150](#bib.bib150)] | - | 68.0 | - | - |'
- en: '| 2018 | Drpose3d [[148](#bib.bib148)] | 57.8 | 42.9 | - | - |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Drpose3d [[148](#bib.bib148)] | 57.8 | 42.9 | - | - |'
- en: '| 2018 | Yang et al. [[165](#bib.bib165)] | 58.6 | 37.7 | 80.1 | 45.8 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Yang 等人 [[165](#bib.bib165)] | 58.6 | 37.7 | 80.1 | 45.8 |'
- en: '| 2019 | Habibie et al. [[166](#bib.bib166)] | 49.2 | - | 82.9 | 45.4 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Habibie 等人 [[166](#bib.bib166)] | 49.2 | - | 82.9 | 45.4 |'
- en: '| 2019 | Chen et al. [[167](#bib.bib167)] | - | 68.0 | 71.1 | 36.3 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Chen 等人 [[167](#bib.bib167)] | - | 68.0 | 71.1 | 36.3 |'
- en: '| 2019 | RepNet [[168](#bib.bib168)] | 80.9 | 65.1 | 82.5 | 58.5 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | RepNet [[168](#bib.bib168)] | 80.9 | 65.1 | 82.5 | 58.5 |'
- en: '| 2019 | Hemlets pose [[147](#bib.bib147)] | - | - | 75.3 | 38.0 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Hemlets pose [[147](#bib.bib147)] | - | - | 75.3 | 38.0 |'
- en: '| 2019 | Sharma et al. [[151](#bib.bib151)] | 58.0 | 40.9 | - | - |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Sharma 等人 [[151](#bib.bib151)] | 58.0 | 40.9 | - | - |'
- en: '| 2019 | Li and Lee [[152](#bib.bib152)] | 52.7 | 42.6 | 67.9 | - |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Li 和 Lee [[152](#bib.bib152)] | 52.7 | 42.6 | 67.9 | - |'
- en: '| 2019 | LCN [[153](#bib.bib153)] | 52.7 | 42.2 | 74.0 | 36.7 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | LCN [[153](#bib.bib153)] | 52.7 | 42.2 | 74.0 | 36.7 |'
- en: '| 2019 | semantic-GCN [[154](#bib.bib154)] | - | 57.6 | - | - |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | semantic-GCN [[154](#bib.bib154)] | - | 57.6 | - | - |'
- en: '| 2020 | Iqbal et al. [[169](#bib.bib169)] | 67.4 | 54.5 | 79.5 | - |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Iqbal et al. [[169](#bib.bib169)] | 67.4 | 54.5 | 79.5 | - |'
- en: '| 2020 | Pose2mesh [[155](#bib.bib155)] | 64.9 | 48.0 | - | - |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Pose2mesh [[155](#bib.bib155)] | 64.9 | 48.0 | - | - |'
- en: '| 2020 | Srnet [[156](#bib.bib156)] | 44.8 | - | 77.6 | 43.8 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Srnet [[156](#bib.bib156)] | 44.8 | - | 77.6 | 43.8 |'
- en: '| 2020 | Liu et al. [[157](#bib.bib157)] | 52.4 | 41.2 | - | - |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Liu et al. [[157](#bib.bib157)] | 52.4 | 41.2 | - | - |'
- en: '| 2021 | Zou et al. [[158](#bib.bib158)] | 49.4 | 39.1 | 86.1 | 53.7 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Zou et al. [[158](#bib.bib158)] | 49.4 | 39.1 | 86.1 | 53.7 |'
- en: '| 2021 | GraphSH [[159](#bib.bib159)] | 51.9 | - | 80.1 | 45.8 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GraphSH [[159](#bib.bib159)] | 51.9 | - | 80.1 | 45.8 |'
- en: '| 2021 | Lin et al. [[163](#bib.bib163)] | 54.0 | 36.7 | - | - |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Lin et al. [[163](#bib.bib163)] | 54.0 | 36.7 | - | - |'
- en: '| 2021 | Yu et al. [[172](#bib.bib172)] | 92.4 | 52.3 | 86.2 | 51.7 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Yu et al. [[172](#bib.bib172)] | 92.4 | 52.3 | 86.2 | 51.7 |'
- en: '| 2022 | Graformer [[164](#bib.bib164)] | 51.8 | - | - | - |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Graformer [[164](#bib.bib164)] | 51.8 | - | - | - |'
- en: '| 2022 | PoseTriplet [[173](#bib.bib173)] | 78 | 51.8 | 89.1 | 53.1 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | PoseTriplet [[173](#bib.bib173)] | 78 | 51.8 | 89.1 | 53.1 |'
- en: '| 2023 | HopFIR [[162](#bib.bib162)] | 48.5 | - | 87.2 | 57.0 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | HopFIR [[162](#bib.bib162)] | 48.5 | - | 87.2 | 57.0 |'
- en: '| 2023 | SSP-Net [[149](#bib.bib149)] | 51.6 | - | 83.2 | 44.3 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | SSP-Net [[149](#bib.bib149)] | 51.6 | - | 83.2 | 44.3 |'
- en: '|  |  | 2023 | PHGANet [[160](#bib.bib160)] | 49.1 | - | 86.9 | 55.0 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2023 | PHGANet [[160](#bib.bib160)] | 49.1 | - | 86.9 | 55.0 |'
- en: '|  |  | 2023 | RS-Net [[161](#bib.bib161)] | 47.0 | 38.6 | 85.6 | 53.2 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2023 | RS-Net [[161](#bib.bib161)] | 47.0 | 38.6 | 85.6 | 53.2 |'
- en: '| VB | One-stage | 2016 | Tekin et al. [[194](#bib.bib194)] | 125.0 | - | -
    | - |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| VB | 单阶段 | 2016 | Tekin et al. [[194](#bib.bib194)] | 125.0 | - | - | - |'
- en: '| 2017 | Vnect [[195](#bib.bib195)] | 80.5 | - | 79.4 | 41.6 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Vnect [[195](#bib.bib195)] | 80.5 | - | 79.4 | 41.6 |'
- en: '| 2018 | Dabral et al. [[196](#bib.bib196)] | 52.1 | 36.3 | 76.7 | 39.1 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Dabral et al. [[196](#bib.bib196)] | 52.1 | 36.3 | 76.7 | 39.1 |'
- en: '| 2022 | IVT [[197](#bib.bib197)] | 40.2 | 28.5 | - | - |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | IVT [[197](#bib.bib197)] | 40.2 | 28.5 | - | - |'
- en: '| 2023 | CSS [[198](#bib.bib198)] | 60.1 | 46.0 | - | - |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | CSS [[198](#bib.bib198)] | 60.1 | 46.0 | - | - |'
- en: '| Two-stage | 2017 | RPSM [[212](#bib.bib212)] | 73.1 | - | - | - |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 两阶段 | 2017 | RPSM [[212](#bib.bib212)] | 73.1 | - | - | - |'
- en: '| 2018 | Rayat et al. [[213](#bib.bib213)] | 51.9 | 42.0 | - | - |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Rayat et al. [[213](#bib.bib213)] | 51.9 | 42.0 | - | - |'
- en: '| 2018 | p-LSTMs [[214](#bib.bib214)] | 55.8 | 46.2 | - | - |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | p-LSTMs [[214](#bib.bib214)] | 55.8 | 46.2 | - | - |'
- en: '| 2018 | Katircioglu et al. [[215](#bib.bib215)] | 67.3 | - | - | - |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Katircioglu et al. [[215](#bib.bib215)] | 67.3 | - | - | - |'
- en: '| 2019 | Cheng et al. [[200](#bib.bib200)] | 42.9 | 32.8 | - | - |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Cheng et al. [[200](#bib.bib200)] | 42.9 | 32.8 | - | - |'
- en: '| 2019 | Cai et al. [[203](#bib.bib203)] | 48.8 | 39.0 | - | - |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Cai et al. [[203](#bib.bib203)] | 48.8 | 39.0 | - | - |'
- en: '| 2019 | TCN [[199](#bib.bib199)] | 46.8 | 36.5 | - | - |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | TCN [[199](#bib.bib199)] | 46.8 | 36.5 | - | - |'
- en: '| 2019 | Chirality Nets [[216](#bib.bib216)] | 46.7 | - | - | - |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Chirality Nets [[216](#bib.bib216)] | 46.7 | - | - | - |'
- en: '| 2020 | UGCN [[217](#bib.bib217)] | 42.6 | 32.7 | 86.9 | 62.1 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | UGCN [[217](#bib.bib217)] | 42.6 | 32.7 | 86.9 | 62.1 |'
- en: '| 2020 | GAST-Net [[201](#bib.bib201)] | 44.9 | 35.2 | - | - |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | GAST-Net [[201](#bib.bib201)] | 44.9 | 35.2 | - | - |'
- en: '| 2021 | Chen et al. [[202](#bib.bib202)] | 44.1 | 35.0 | 87.9 | 54.0 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Chen et al. [[202](#bib.bib202)] | 44.1 | 35.0 | 87.9 | 54.0 |'
- en: '| 2021 | PoseFormer [[204](#bib.bib204)] | 44.3 | 34.6 | 88.6 | 56.4 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | PoseFormer [[204](#bib.bib204)] | 44.3 | 34.6 | 88.6 | 56.4 |'
- en: '| 2022 | Strided [[206](#bib.bib206)] | 43.7 | 35.2 | - | - |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Strided [[206](#bib.bib206)] | 43.7 | 35.2 | - | - |'
- en: '| 2022 | Mhformer [[207](#bib.bib207)] | 43.0 | - | 93.8 | 63.3 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Mhformer [[207](#bib.bib207)] | 43.0 | - | 93.8 | 63.3 |'
- en: '| 2022 | MixSTE [[219](#bib.bib219)] | 39.8 | 30.6 | 94.4 | 66.5 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | MixSTE [[219](#bib.bib219)] | 39.8 | 30.6 | 94.4 | 66.5 |'
- en: '| 2022 | UPS [[259](#bib.bib259)] | 40.8 | 32.5 | - | - |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | UPS [[259](#bib.bib259)] | 40.8 | 32.5 | - | - |'
- en: '| 2023 | DSTFormer [[222](#bib.bib222)] | 37.5 | - | - | - |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | DSTFormer [[222](#bib.bib222)] | 37.5 | - | - | - |'
- en: '| 2023 | GLA-GCN [[218](#bib.bib218)] | 44.4 | 34.8 | 98.5 | 79.1 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | GLA-GCN [[218](#bib.bib218)] | 44.4 | 34.8 | 98.5 | 79.1 |'
- en: '| 2023 | D3DP [[210](#bib.bib210)] | 35.4 | - | 98.0 | 79.1 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | D3DP [[210](#bib.bib210)] | 35.4 | - | 98.0 | 79.1 |'
- en: '| 2023 | DiffPose [[209](#bib.bib209)] | 43.3 | 32.0 | 84.9 | - |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | DiffPose [[209](#bib.bib209)] | 43.3 | 32.0 | 84.9 | - |'
- en: '| 2023 | STCFormer [[211](#bib.bib211)] | 40.5 | 31.8 | 98.7 | 83.9 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | STCFormer [[211](#bib.bib211)] | 40.5 | 31.8 | 98.7 | 83.9 |'
- en: '| 2023 | PoseFormerV2 [[205](#bib.bib205)] | 45.2 | 35.6 | 97.9 | 78.8 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | PoseFormerV2 [[205](#bib.bib205)] | 45.2 | 35.6 | 97.9 | 78.8 |'
- en: '|  | 2023 | MTF-Transformer [[221](#bib.bib221)] | 26.2 | - | - | - |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | MTF-Transformer [[221](#bib.bib221)] | 26.2 | - | - | - |'
- en: 'Table 7: Performance comparison for 3D Image-based MPPE on MuPoTS-3D dataset.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：MuPoTS-3D 数据集上的 3D 基于图像的 MPPE 性能比较。
- en: '|  |  |  | MuPoTS-3D |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | MuPoTS-3D |'
- en: '| Category | Year | Method | All people | Matched people |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 年份 | 方法 | 所有人 | 匹配的人 |'
- en: '|  |  |  | PCKrel | PCKabs | PCKrel | PCKabs | PCKroot | AUCrel |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | PCKrel | PCKabs | PCKrel | PCKabs | PCKroot | AUCrel |'
- en: '| Top-down | 2019 | LCR-Net [[177](#bib.bib177)] | 70.6 | - | 74.0 | - | -
    | - |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下 | 2019 | LCR-Net [[177](#bib.bib177)] | 70.6 | - | 74.0 | - | - | -
    |'
- en: '| 2019 | Moon et al. [[178](#bib.bib178)] | 81.8 | 31.5 | 82.5 | 31.8 | 31.0
    | 40.9 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Moon 等人 [[178](#bib.bib178)] | 81.8 | 31.5 | 82.5 | 31.8 | 31.0 |
    40.9 |'
- en: '| 2020 | HDNet [[179](#bib.bib179)] | - | - | 83.7 | 35.2 | - | - |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | HDNet [[179](#bib.bib179)] | - | - | 83.7 | 35.2 | - | - |'
- en: '| 2020 | HMOR [[180](#bib.bib180)] | - | - | 82.0 | 43.8 | - | - |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | HMOR [[180](#bib.bib180)] | - | - | 82.0 | 43.8 | - | - |'
- en: '| 2022 | Cha et al. [[181](#bib.bib181)] | 89.9 | - | 91.7 | - | - | - |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Cha 等人 [[181](#bib.bib181)] | 89.9 | - | 91.7 | - | - | - |'
- en: '| Bottom-up | 2018 | Mehta et al. [[185](#bib.bib185)] | 65.0 | - | 69.8 |
    - | - | - |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上 | 2018 | Mehta 等人 [[185](#bib.bib185)] | 65.0 | - | 69.8 | - | - | -
    |'
- en: '| 2020 | Kundu et al. [[184](#bib.bib184)] | 74.0 | 28.1 | 75.8 | - | - | -
    |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Kundu 等人 [[184](#bib.bib184)] | 74.0 | 28.1 | 75.8 | - | - | - |'
- en: '| 2020 | XNect [[186](#bib.bib186)] | 70.4 | - | 75.8 | - | - | - |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | XNect [[186](#bib.bib186)] | 70.4 | - | 75.8 | - | - | - |'
- en: '| 2020 | Smap [[187](#bib.bib187)] | 73.5 | 35.4 | 80.5 | 38.7 | 45.5 | 42.7
    |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Smap [[187](#bib.bib187)] | 73.5 | 35.4 | 80.5 | 38.7 | 45.5 | 42.7
    |'
- en: '| 2022 | Liu et al. [[188](#bib.bib188)] | 79.4 | 36.5 | 86.5 | 39.3 | - |
    - |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | Liu 等人 [[188](#bib.bib188)] | 79.4 | 36.5 | 86.5 | 39.3 | - | - |'
- en: '| 2023 | AKE [[189](#bib.bib189)] | 74.7 | 37.2 | 81.1 | 40.1 | - | - |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | AKE [[189](#bib.bib189)] | 74.7 | 37.2 | 81.1 | 40.1 | - | - |'
- en: '| One-stage | 2022 | Wang et al. [[175](#bib.bib175)] | 82.7 | 39.2 | - | -
    | - | - |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 一阶段 | 2022 | Wang 等人 [[175](#bib.bib175)] | 82.7 | 39.2 | - | - | - | - |'
- en: '| 2022 | DRM [[192](#bib.bib192)] | 80.9 | 39.3 | 85.1 | 41.0 | 45.6 | 45.4
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | DRM [[192](#bib.bib192)] | 80.9 | 39.3 | 85.1 | 41.0 | 45.6 | 45.4
    |'
- en: '| 2023 | WSP [[193](#bib.bib193)] | 82.4 | - | 83.2 | - | - | - |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | WSP [[193](#bib.bib193)] | 82.4 | - | 83.2 | - | - | - |'
- en: 5.1.3 Performance comparison
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 性能比较
- en: 'In Table [3](#S5.T3 "Table 3 ‣ 5.1.1 Datasets for 2D pose estimation ‣ 5.1
    Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey"), we present a comparison
    of different methods for 2D image-based SPPE and MPPE on the COCO dataset. For
    the SPPE task, the performance of heatmap-based methods generally outperforms
    the regression-based methods. This superiority can be attributed to the richer
    spatial information provided by heatmaps, where the probabilistic prediction of
    each pixel enhances the accuracy of keypoint localization. However, heatmap-based
    methods [[64](#bib.bib64)] suffer seriously from the quantization error problem
    and high-computational cost using high resolution heatmaps. For the MPPE task,
    the top-down methods overall outperform the bottom-up methods by the success of
    existing SPPE techniques after detecting individuals. However, they suffer from
    early commitment and have greater computational costs than bottom-up methods.
    One-stage methods speed up the process by eliminating the intermediate operations
    (eg., grouping, ROI, NMS) introduced by top-down and bottom-up methods, while
    their performance [[110](#bib.bib110)] is still lower (about 9% of AP score in
    the best case) than top-down methods [[89](#bib.bib89)]. Moreover, It is also
    observed that the backbone and input image size are two factors for the results.
    The commonly used backbone includes ResNet, HRNet and Hourglass. The recent Transformer-based
    network (eg., ViTAE-G, Swin-L) can be also used as the backbone and the method [[89](#bib.bib89)]
    based on ViTAE-G network achieves the best performance. When using the same backbone [[74](#bib.bib74),
    [85](#bib.bib85)] for the same category of methods, the larger the image size,
    the better the performance.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[3](#S5.T3 "Table 3 ‣ 5.1.1 Datasets for 2D pose estimation ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey")中，我们展示了在COCO数据集上2D图像基础SPPE和MPPE的不同方法的比较。对于SPPE任务，基于热图的方法的表现通常优于基于回归的方法。这种优越性可以归因于热图提供的更丰富的空间信息，其中每个像素的概率预测提高了关键点定位的准确性。然而，基于热图的方法[[64](#bib.bib64)]在使用高分辨率热图时严重受制于量化误差问题和高计算成本。对于MPPE任务，整体上自上而下的方法通过检测个体后应用现有的SPPE技术，优于自下而上的方法。然而，它们受制于早期承诺，并且计算成本高于自下而上的方法。一阶段的方法通过消除自上而下和自下而上方法引入的中间操作（例如，分组、ROI、NMS）来加速过程，但其性能[[110](#bib.bib110)]仍低于自上而下的方法[[89](#bib.bib89)]（在最佳情况下约为9%的AP分数）。此外，还观察到骨干网络和输入图像大小是结果的两个因素。常用的骨干网络包括ResNet、HRNet和Hourglass。最近的基于Transformer的网络（例如ViTAE-G、Swin-L）也可以用作骨干网络，基于ViTAE-G网络的方法[[89](#bib.bib89)]表现最佳。当使用相同的骨干网络[[74](#bib.bib74),
    [85](#bib.bib85)]处理相同类别的方法时，图像尺寸越大，性能越好。'
- en: 'Table [4](#S5.T4 "Table 4 ‣ 5.1.1 Datasets for 2D pose estimation ‣ 5.1 Pose
    estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and
    Action Recognition with Deep Learning: A Survey") and Table [5](#S5.T5 "Table
    5 ‣ 5.1.1 Datasets for 2D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark
    datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep
    Learning: A Survey") compare the different methods for 2D video-based SPPE and
    MPPE. Overall, two categories of methods for video-based SPPE achieve comparable
    results on two datasets. Yet sample frames-based methods[[127](#bib.bib127)] are
    generally faster than frame-by-frame ones by ignoring looking at all frames. Similar
    to image-based MPPE, the top-down methods achieve better performance than the
    bottom-up methods for video-based MPPE.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '表[4](#S5.T4 "Table 4 ‣ 5.1.1 Datasets for 2D pose estimation ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey")和表[5](#S5.T5 "Table 5 ‣ 5.1.1 Datasets for 2D pose
    estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey")比较了2D视频基础SPPE和MPPE的不同方法。总体而言，视频基础SPPE的两类方法在两个数据集上的结果相当。然而，基于样本帧的方法[[127](#bib.bib127)]通常比逐帧方法更快，因为它们忽略了查看所有帧。与基于图像的MPPE类似，自上而下的方法在视频基础MPPE中表现优于自下而上的方法。'
- en: 'For 3D pose estimation, taken Human3.6M, MPI-INF-3DHP and MuPoTS-3D datasets
    as examples, Table [6](#S5.T6 "Table 6 ‣ 5.1.2 Datasets for 3D pose estimation
    ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") and Table [7](#S5.T7 "Table
    7 ‣ 5.1.2 Datasets for 3D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark
    datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep
    Learning: A Survey") respectively shows the comparisons for SPPE and MPPE from
    images or videos. The comparison for video-based MPPE was not conducted due to
    only fewer existing methods. For the SPPE task, two-stage methods normally lift
    3D poses from the estimated 2D poses, they generally outperform one-stage methods
    due to the success of the 2D pose estimation technique. It is also noted that
    the recent one-stage method based on Transformer network [[197](#bib.bib197)]
    also achieves pretty good results. Compared to the same category of methods between
    images and videos, the performance based on videos is better than the ones based
    on images. It demonstrates that the temporal information of videos is beneficial
    for estimating more accurate poses. From Table [7](#S5.T7 "Table 7 ‣ 5.1.2 Datasets
    for 3D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey"), good
    progress has been made in recent years for the MPPE task. Specifically, one-stage
    methods generally perform better than most top-down and bottom-up methods, which
    further implies that the end-to-end training could reduce intermediate errors
    such as human detection and joint grouping.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 3D 姿态估计，以 Human3.6M、MPI-INF-3DHP 和 MuPoTS-3D 数据集为例，表 [6](#S5.T6 "Table 6
    ‣ 5.1.2 Datasets for 3D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets
    ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey") 和表 [7](#S5.T7 "Table 7 ‣ 5.1.2 Datasets for 3D pose estimation ‣ 5.1
    Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") 分别显示了来自图像或视频的 SPPE 和 MPPE
    的比较。由于现有方法较少，视频基础的 MPPE 比较未进行。对于 SPPE 任务，二阶段方法通常从估计的 2D 姿态中提升 3D 姿态，由于 2D 姿态估计技术的成功，它们通常优于一阶段方法。还注意到，基于
    Transformer 网络的最近一阶段方法 [[197](#bib.bib197)] 也取得了相当好的结果。与图像和视频之间的同类方法相比，基于视频的性能优于基于图像的性能。这表明视频的时间信息有助于估计更准确的姿态。从表 [7](#S5.T7
    "Table 7 ‣ 5.1.2 Datasets for 3D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark
    datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep
    Learning: A Survey") 可以看出，近年来 MPPE 任务取得了良好的进展。具体而言，一阶段方法通常比大多数自上而下和自下而上的方法表现更好，这进一步表明端到端训练可以减少中间错误，例如人体检测和关节分组。'
- en: 'Table 8: Datasets for Pose tracking. MOTA: Multiple Object Tracking Accuracy,
    PCP: Percentage of Correct Parts, KLE: Keypoint Localization Error.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 姿态跟踪的数据集。MOTA: 多目标跟踪准确率，PCP: 正确部位的百分比，KLE: 关键点定位误差。'
- en: '| Dataset | Year | Citation | #Joints | Size | 2D/3D | Metrics |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 引用 | #关节 | 大小 | 2D/3D | 指标 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; VideoPose2.0 [[343](#bib.bib343)] &#124;'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VideoPose2.0 [[343](#bib.bib343)] &#124;'
- en: '| 2011 | 198 | - | 44 videos | 2D |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 2011 | 198 | - | 44 个视频 | 2D |'
- en: '&#124; AP &#124;'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AP &#124;'
- en: '|'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-Person PoseTrack [[235](#bib.bib235)] &#124;'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多人姿态跟踪 [[235](#bib.bib235)] &#124;'
- en: '| 2017 | 238 | 14 | 16 subjects, 60 videos | 2D | MOTA |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 238 | 14 | 16 个主题，60 个视频 | 2D | MOTA |'
- en: '|'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PoseTrack17 [[337](#bib.bib337)] &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PoseTrack17 [[337](#bib.bib337)] &#124;'
- en: '| 2018 | 420 | 15 | 40 subjects, 550 videos | 2D | MOTA |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 420 | 15 | 40 个主题，550 个视频 | 2D | MOTA |'
- en: '|'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PoseTrack18 [[337](#bib.bib337)] &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PoseTrack18 [[337](#bib.bib337)] &#124;'
- en: '| 2018 | 420 | 15 | 1138 videos | 2D | MOTA |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 420 | 15 | 1138 个视频 | 2D | MOTA |'
- en: '|'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ICDPose [[130](#bib.bib130)] &#124;'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICDPose [[130](#bib.bib130)] &#124;'
- en: '| 2018 | 250 | 14 | 60 videos | 2D | MOTA |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 250 | 14 | 60 个视频 | 2D | MOTA |'
- en: '|'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Campus dataset [[344](#bib.bib344)] &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 校园数据集 [[344](#bib.bib344)] &#124;'
- en: '| 2011 | 1253 | - | 3 subjects, 3 views, 6k frames | 3D | PCP |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 2011 | 1253 | - | 3 个主题，3 个视角，6k 帧 | 3D | PCP |'
- en: '|'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Outdoor Pose [[345](#bib.bib345)] &#124;'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室外姿态 [[345](#bib.bib345)] &#124;'
- en: '| 2013 | 61 | 14 | 4 subjects, 828 frames | 3D | PCP/KLE |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 61 | 14 | 4 个主题，828 帧 | 3D | PCP/KLE |'
- en: '|'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CMU Panoptic [[341](#bib.bib341)] &#124;'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CMU Panoptic [[341](#bib.bib341)] &#124;'
- en: '| 2017 | 680 | 15 | 8 subjects, 480 views, 65 videos | 3D | MOTA |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 680 | 15 | 8 个主题，480 个视角，65 个视频 | 3D | MOTA |'
- en: 'Table 9: Performance comparison for 2D single person pose tracking on Videopose2.0.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: Videopose2.0 上 2D 单人姿态跟踪的性能比较。'
- en: '| Method | Category | Year | AP |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类别 | 年份 | AP |'
- en: '| --- | --- | --- | --- |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Zhao et al. |  |  |  |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. |  |  |  |'
- en: '| [[226](#bib.bib226)] | Post-processing | 2015 | 85.0 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| [[226](#bib.bib226)] | 后处理 | 2015 | 85.0 |'
- en: '| Samanta et al. |  |  |  |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| Samanta et al. |  |  |  |'
- en: '| [[227](#bib.bib227)] | Post-processing | 2016 | 89.9 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| [[227](#bib.bib227)] | 后处理 | 2016 | 89.9 |'
- en: '| Zhao et al. |  |  |  |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. |  |  |  |'
- en: '| [[228](#bib.bib228)] | Integrated | 2015 | 80.0 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| [[228](#bib.bib228)] | 集成 | 2015 | 80.0 |'
- en: '| Ma et al. |  |  |  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| Ma et al. |  |  |  |'
- en: '| [[229](#bib.bib229)] | Integrated | 2016 | 95.0 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| [[229](#bib.bib229)] | 集成 | 2016 | 95.0 |'
- en: 'Table 10: Performance comparison for 2D multi-person pose tracking on PoseTrack2017
    and PoseTrack2018.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: PoseTrack2017 和 PoseTrack2018 上 2D 多人姿态跟踪的性能比较。'
- en: '| Method | Category | Year |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类别 | 年份 |'
- en: '&#124; 2017 Testing &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2017 测试 &#124;'
- en: '&#124; MOTA &#124;'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MOTA &#124;'
- en: '|'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2017 Validation &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2017 验证 &#124;'
- en: '&#124; MOTA &#124;'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MOTA &#124;'
- en: '|'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2018 Testing &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2018 测试 &#124;'
- en: '&#124; MOTA &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MOTA &#124;'
- en: '|'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2018 Validation &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2018 验证 &#124;'
- en: '&#124; MOTA &#124;'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MOTA &#124;'
- en: '|'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Detect-and-Track |  |  |  |  |  |  |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| Detect-and-Track |  |  |  |  |  |  |'
- en: '| [[130](#bib.bib130)] | Top-down | 2018 | 51.8 | 55.2 | - | - |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| [[130](#bib.bib130)] | 自上而下 | 2018 | 51.8 | 55.2 | - | - |'
- en: '| Pose Flow |  |  |  |  |  |  |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| Pose Flow |  |  |  |  |  |  |'
- en: '| [[129](#bib.bib129)] | Top-down | 2018 | 51.0 | 58.3 | - | - |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| [[129](#bib.bib129)] | 自上而下 | 2018 | 51.0 | 58.3 | - | - |'
- en: '| Flow Track |  |  |  |  |  |  |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| Flow Track |  |  |  |  |  |  |'
- en: '| [[69](#bib.bib69)] | Top-down | 2018 | 57.8 | 65.4 | - | - |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)] | 自上而下 | 2018 | 57.8 | 65.4 | - | - |'
- en: '| Fastpose |  |  |  |  |  |  |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| Fastpose |  |  |  |  |  |  |'
- en: '| [[230](#bib.bib230)] | Top-down | 2019 | 57.4 | 63.2 | - | - |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| [[230](#bib.bib230)] | 自上而下 | 2019 | 57.4 | 63.2 | - | - |'
- en: '| LightTrack |  |  |  |  |  |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| LightTrack |  |  |  |  |  |  |'
- en: '| [[231](#bib.bib231)] | Top-down | 2020 | 58.0 | - | - | 64.6 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| [[231](#bib.bib231)] | 自上而下 | 2020 | 58.0 | - | - | 64.6 |'
- en: '| Umer et al. |  |  |  |  |  |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| Umer et al. |  |  |  |  |  |  |'
- en: '| [[232](#bib.bib232)] | Top-down | 2020 | 60.0 | 68.3 | 60.7 | 69.1 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| [[232](#bib.bib232)] | 自上而下 | 2020 | 60.0 | 68.3 | 60.7 | 69.1 |'
- en: '| Clip Tracking |  |  |  |  |  |  |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| Clip Tracking |  |  |  |  |  |  |'
- en: '| [[131](#bib.bib131)] | Top-down | 2020 | 64.1 | 71.6 | 64.3 | 68.7 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| [[131](#bib.bib131)] | 自上而下 | 2020 | 64.1 | 71.6 | 64.3 | 68.7 |'
- en: '| Yang et al. |  |  |  |  |  |  |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. |  |  |  |  |  |  |'
- en: '| [[233](#bib.bib233)] | Top-down | 2021 | - | 73.4 | - | 69.2 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| [[233](#bib.bib233)] | 自上而下 | 2021 | - | 73.4 | - | 69.2 |'
- en: '| AlphaPose |  |  |  |  |  |  |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| AlphaPose |  |  |  |  |  |  |'
- en: '| [[132](#bib.bib132)] | Top-down | 2022 | - | 65.7 | - | 64.7 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | 自上而下 | 2022 | - | 65.7 | - | 64.7 |'
- en: '| GatedTrack |  |  |  |  |  |  |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| GatedTrack |  |  |  |  |  |  |'
- en: '| [[234](#bib.bib234)] | Top-down | 2023 | - | - | - | 64.5 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| [[234](#bib.bib234)] | 自上而下 | 2023 | - | - | - | 64.5 |'
- en: '| Posetrack |  |  |  |  |  |  |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| Posetrack |  |  |  |  |  |  |'
- en: '| [[235](#bib.bib235)] | Bottom-up | 2017 | 48.4 | - | - | - |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| [[235](#bib.bib235)] | 自下而上 | 2017 | 48.4 | - | - | - |'
- en: '| Raaj et al. |  |  |  |  |  |  |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| Raaj et al. |  |  |  |  |  |  |'
- en: '| [[236](#bib.bib236)] | Bottom-up | 2019 | 53.8 | 62.7 | - | 60.9 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| [[236](#bib.bib236)] | 自下而上 | 2019 | 53.8 | 62.7 | - | 60.9 |'
- en: '| Jin et al. |  |  |  |  |  |  |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| Jin et al. |  |  |  |  |  |  |'
- en: '| [[138](#bib.bib138)] | Bottom-up | 2019 | - | 71.8 | - | - |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | 自下而上 | 2019 | - | 71.8 | - | - |'
- en: 'Table 11: Performance comparison for 3D multi-person pose tracking on CMU Panoptic
    and Campus dataset.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: CMU Panoptic 和 Campus 数据集上 3D 多人姿态跟踪的性能比较。'
- en: '| Method | Category | Year |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类别 | 年份 |'
- en: '&#124; CMU &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CMU &#124;'
- en: '&#124; MOTA &#124;'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MOTA &#124;'
- en: '|'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Campus &#124;'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Campus &#124;'
- en: '&#124; PCP &#124;'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PCP &#124;'
- en: '|'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Bridgeman et al. |  |  |  |  |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| Bridgeman et al. |  |  |  |  |'
- en: '| [[237](#bib.bib237)] | Multi-stage | 2019 | - | 92.6 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| [[237](#bib.bib237)] | 多阶段 | 2019 | - | 92.6 |'
- en: '| Tessetrack |  |  |  |  |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| Tessetrack |  |  |  |  |'
- en: '| [[240](#bib.bib240)] | One-stage | 2021 | 94.1 | 97.4 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| [[240](#bib.bib240)] | 单阶段 | 2021 | 94.1 | 97.4 |'
- en: '| Voxeltrack |  |  |  |  |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| Voxeltrack |  |  |  |  |'
- en: '| [[241](#bib.bib241)] | One-stage | 2022 | 98.5 | 96.7 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| [[241](#bib.bib241)] | 单阶段 | 2022 | 98.5 | 96.7 |'
- en: '| Snipper |  |  |  |  |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| Snipper |  |  |  |  |'
- en: '| [[242](#bib.bib242)] | One-stage | 2023 | 93.4 | - |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| [[242](#bib.bib242)] | 单阶段 | 2023 | 93.4 | - |'
- en: '| TEMPO |  |  |  |  |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| TEMPO |  |  |  |  |'
- en: '| [[29](#bib.bib29)] | One-stage | 2023 | 98.4 | - |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] | 单阶段 | 2023 | 98.4 | - |'
- en: 5.2 Pose tracking
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 姿态跟踪
- en: This section reviews the datasets for pose tracking and also compares different
    methods on some datasets.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了姿态跟踪的数据集，并在一些数据集上比较了不同的方法。
- en: 5.2.1 Datasets
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 数据集
- en: 'Table  [8](#S5.T8 "Table 8 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") summarizes the datasets, with a focus on the Campus,
    CMP Panoptic, and PoseTrack datasets, which are highly cited and frequently used
    for evaluating multi-person pose tracking. These datasets are preferred because
    multi-person poses are more representative of real-world scenarios. In the earlier
    stage, VideoPose2.0 was often applied for single-person pose tracking. The PoseTrack
    dataset has been discussed in Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Datasets for
    2D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey"). In
    the following, we only review other three datasets.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [8](#S5.T8 "Table 8 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") 总结了数据集，重点关注 Campus、CMP Panoptic 和 PoseTrack 数据集，这些数据集被高度引用且常用于评估多人姿势跟踪。这些数据集更受欢迎，因为多人姿势更能代表真实世界场景。在早期阶段，VideoPose2.0
    通常用于单人姿势跟踪。PoseTrack 数据集在第 [5.1.1](#S5.SS1.SSS1 "5.1.1 Datasets for 2D pose estimation
    ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") 节中进行了讨论。接下来，我们只回顾其他三个数据集。'
- en: The VideoPose2.0 dataset [[343](#bib.bib343)] is a video dataset for tracking
    the poses of upper and lower arms. The videos were collected from TV shows ”Friends”
    and ”Lost” and are normally with a single actor and a variety of movements. This
    dataset includes 44 videos, each lasting 2-3 seconds, totaling 1,286 frames. Each
    frame is hand-annotated with joint locations. This dataset is an extension of
    the VideoPose dataset [[346](#bib.bib346)], but more challenging since about 30%
    of lower arms are significantly foreshortened.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: VideoPose2.0 数据集 [[343](#bib.bib343)] 是用于跟踪上臂和下臂姿势的视频数据集。这些视频来自于电视节目《Friends》和《Lost》，通常包含单个演员和多种动作。该数据集包括44个视频，每个视频持续2-3秒，总计1,286帧。每一帧都手动标注了关节位置。该数据集是
    VideoPose 数据集 [[346](#bib.bib346)] 的扩展，但更具挑战性，因为约30%的下臂显著被缩短。
- en: 'The CMU Panoptic Dataset [[341](#bib.bib341)] was created by capturing subjects
    engaged in social interactions using the camera system with 480 views. Subjects
    were engaged in different games: Ultimatum (with 3 subjects), Prisoner’s dilemma
    (with 8 subjects), Mafia (with 8 subjects), Haggling (with 3 subjects), and 007-bang
    game (with 5 subjects). The number of subjects in each game varies from three
    to eight. In total, this dataset consists of 65 videos and 1.5 million 3D poses
    estimated using Kinects. It is often used for evaluating multi-person 3D pose
    estimation and pose tracking methods.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: CMU Panoptic 数据集 [[341](#bib.bib341)] 是通过使用480视角的摄像系统捕捉受试者在社交互动中的表现而创建的。受试者参与了不同的游戏：Ultimatum（3名受试者）、Prisoner’s
    dilemma（8名受试者）、Mafia（8名受试者）、Haggling（3名受试者）和007-bang游戏（5名受试者）。每个游戏的受试者数量从三人到八人不等。总共有65个视频和150万3D姿势数据，使用Kinects进行估计。该数据集通常用于评估多人3D姿势估计和姿势跟踪方法。
- en: The Campus Dataset [[347](#bib.bib347)] was collected by capturing interactions
    among three individuals in an outdoor environment using 3 cameras. It contains
    6,000 frames including 3 views, and each view provides 2,000 frames. It is widely
    used for 3D multi-person pose estimation and tracking. Due to a small number of
    cameras and wide baseline views, it is challenging for pose tracking.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: Campus 数据集 [[347](#bib.bib347)] 是通过使用3台摄像机在户外环境中捕捉三个人之间的互动而收集的。它包含6,000帧，包括3个视角，每个视角提供2,000帧。该数据集广泛用于3D多人姿势估计和跟踪。由于摄像机数量少且基线视角宽，这对姿势跟踪提出了挑战。
- en: 5.2.2 Performance comparison
  id: totrans-543
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 性能比较
- en: 'Table [9](#S5.T9 "Table 9 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") and Table [10](#S5.T10 "Table 10 ‣ 5.1.3 Performance
    comparison ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey") respectively show
    the comparison of 2D pose tracking methods. For 2D single-person pose tracking,
    integrated methods jointly optimize pose estimation and pose tracking within a
    unified framework, leveraging the benefits of each to achieve better results.
    From Table [9](#S5.T9 "Table 9 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey"), it can be observed that one of the integrated
    methods [[229](#bib.bib229)] exhibits state-of-the-art performance. For 2D multi-person
    pose tracking, most methods follow the top-down strategy by well-estimated poses
    of single-person estimation technique. Undoubtedly, these methods outperform bottom-up
    ones about 2-15% of MOTA scores on the Posetrack2017 and 2018 datasets. Regarding
    3D multi-person pose tracking, there are currently fewer existing works. Among
    them, one-stage methods perform better than multi-stage methods shown in Table [11](#S5.T11
    "Table 11 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets
    ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey"), and Voxeltrack  [[241](#bib.bib241)] achieves the best results. This
    is because one-stage methods jointly estimate and link 3D poses, which can propagate
    the errors of sub-tasks in the multi-stage methods back to the input image pixels
    of videos.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [9](#S5.T9 "Table 9 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") 和表 [10](#S5.T10 "Table 10 ‣ 5.1.3 Performance comparison
    ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") 分别展示了2D姿势跟踪方法的比较。对于2D单人姿势跟踪，集成方法在一个统一的框架内共同优化姿势估计和姿势跟踪，利用每种方法的优势以获得更好的结果。从表
    [9](#S5.T9 "Table 9 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation ‣ 5 Benchmark
    datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep
    Learning: A Survey") 可以看出，某些集成方法 [[229](#bib.bib229)] 展现了最先进的性能。对于2D多人姿势跟踪，大多数方法采用自上而下的策略，通过良好的单人姿势估计技术进行估计。毫无疑问，这些方法在Posetrack2017和2018数据集上的MOTA分数比自下而上的方法高出约2-15%。关于3D多人姿势跟踪，目前现有的研究较少。在这些研究中，一阶段方法比多阶段方法表现更好，如表
    [11](#S5.T11 "Table 11 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation ‣
    5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") 所示，而Voxeltrack [[241](#bib.bib241)] 取得了最佳结果。这是因为一阶段方法共同估计和连接3D姿势，可以将多阶段方法中的子任务误差传播回视频的输入图像像素。'
- en: 5.3 Action recognition
  id: totrans-545
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 动作识别
- en: This section reviews the datasets that are more commonly used for pose-based
    action recognition and also compares different categories of the methods.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了用于基于姿势的动作识别的常用数据集，并比较了不同类别的方法。
- en: 'Table 12: A review of human action recognition datasets. C: Colour, D: Depth,
    S: Skeleton, I: Infrared frame; LOSubO: Leave One Subject Out, CS: Cross Subject,
    CV: Cross Validation; tr: training, va: validation, te: test'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 人类动作识别数据集综述。C: 颜色, D: 深度, S: 骨架, I: 红外帧; LOSubO: 留一主体法, CS: 跨主体, CV:
    交叉验证; tr: 训练, va: 验证, te: 测试'
- en: '| Dataset | Year | Citation | Modality | Sensors | #Actions | #Subjects | #Samples
    | Protocol |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 引用 | 模态 | 传感器 | 动作数 | 受试者数 | 样本数 | 协议 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HDM05 &#124;'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HDM05 &#124;'
- en: '&#124; [[348](#bib.bib348)] &#124;'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[348](#bib.bib348)] &#124;'
- en: '| 2007 | 503 | C,D,S | RRM | 130 | 5 | 2317 | 10-fold CV |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| 2007 | 503 | C,D,S | RRM | 130 | 5 | 2317 | 10-fold CV |'
- en: '|'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MSR-Action3D &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSR-Action3D &#124;'
- en: '&#124; [[349](#bib.bib349)] &#124;'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[349](#bib.bib349)] &#124;'
- en: '| 2010 | 1736 | D,S | Kinect | 20 | 10 | 557 | CS(1/3 tr; 2/3 tr; half tr,
    half te) |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | 1736 | D,S | Kinect | 20 | 10 | 557 | CS(1/3 tr; 2/3 tr; 半 tr, 半 te)
    |'
- en: '|'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MSRC-12 &#124;'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSRC-12 &#124;'
- en: '&#124; [[350](#bib.bib350)] &#124;'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[350](#bib.bib350)] &#124;'
- en: '| 2012 | 494 | S | Kinect | 12 | 30 | 6244 | LOSubO |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | 494 | S | Kinect | 12 | 30 | 6244 | LOSubO |'
- en: '|'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; G3D &#124;'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; G3D &#124;'
- en: '&#124; [[351](#bib.bib351)] &#124;'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[351](#bib.bib351)] &#124;'
- en: '| 2012 | 262 | C,D,S | Kinect | 20 | 10 | 659 | CS(4 tr, 1 va, 5 te) |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | 262 | C,D,S | Kinect | 20 | 10 | 659 | CS(4 tr, 1 va, 5 te) |'
- en: '|'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SBU Kinect &#124;'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SBU Kinect &#124;'
- en: '&#124; [[352](#bib.bib352)] &#124;'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[352](#bib.bib352)] &#124;'
- en: '| 2012 | 575 | C,D,S | Kinect | 8 | 7 | 300 | 5-fold CV |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | 575 | C,D,S | Kinect | 8 | 7 | 300 | 5折交叉验证 |'
- en: '|'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; UTKinect-Action3D &#124;'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UTKinect-Action3D &#124;'
- en: '&#124; [[353](#bib.bib353)] &#124;'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[353](#bib.bib353)] &#124;'
- en: '| 2012 | 1716 | C,D,S | Kinect | 10 | 10 | 200 | LOSubO |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | 1716 | C,D,S | Kinect | 10 | 10 | 200 | LOSubO |'
- en: '|'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Northwestern-UCLA &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Northwestern-UCLA &#124;'
- en: '&#124; [[354](#bib.bib354)] &#124;'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[354](#bib.bib354)] &#124;'
- en: '| 2014 | 497 | C,D,S | Kinect | 10 | 10 | 1494 | LOSubO; cross view(2 tr, 1
    te) |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 497 | C,D,S | Kinect | 10 | 10 | 1494 | LOSubO; 交叉视角(2 tr, 1 te) |'
- en: '|'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; UTD-MHAD &#124;'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UTD-MHAD &#124;'
- en: '&#124; [[355](#bib.bib355)] &#124;'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[355](#bib.bib355)] &#124;'
- en: '| 2015 | 706 | C,D,S,I | Kinect | 27 | 8 | 861 | CS(odd tr, even te) |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 706 | C,D,S,I | Kinect | 27 | 8 | 861 | CS(奇数 tr, 偶数 te) |'
- en: '|'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SYSU &#124;'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SYSU &#124;'
- en: '&#124; [[356](#bib.bib356)] &#124;'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[356](#bib.bib356)] &#124;'
- en: '| 2015 | 594 | C,D,S | Kinect | 12 | 40 | 480 | CS(half tr, half te) |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 594 | C,D,S | Kinect | 12 | 40 | 480 | CS(一半 tr, 一半 te) |'
- en: '|'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; NTU-RGB+D &#124;'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NTU-RGB+D &#124;'
- en: '&#124; [[274](#bib.bib274)] &#124;'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[274](#bib.bib274)] &#124;'
- en: '| 2016 | 2452 | C,D,S,I | Kinect | 60 | 40 | 56880 | CS(half tr, half te);
    cross view(half tr, half te) |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | 2452 | C,D,S,I | Kinect | 60 | 40 | 56880 | CS(一半 tr, 一半 te); 交叉视角(一半
    tr, 一半 te) |'
- en: '|'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PKU-MMD &#124;'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PKU-MMD &#124;'
- en: '&#124; [[357](#bib.bib357)] &#124;'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[357](#bib.bib357)] &#124;'
- en: '| 2017 | 195 | C,D,S,I | Kinect | 51 | 66 | 1076 | CS(57 tr, 9 te); cross view(2
    tr, 1 te) |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 195 | C,D,S,I | Kinect | 51 | 66 | 1076 | CS(57 tr, 9 te); 交叉视角(2
    tr, 1 te) |'
- en: '|'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Kinetics &#124;'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 动力学 &#124;'
- en: '&#124; [[358](#bib.bib358)] &#124;'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[358](#bib.bib358)] &#124;'
- en: '| 2017 | 3402 | C,S | YouTube | 400 | - | 306245 | CV(250-1000 tr, 50 va, 100
    te per action) |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 3402 | C,S | YouTube | 400 | - | 306245 | CV(250-1000 tr, 50 va, 100
    te per action) |'
- en: '|'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; NTU RGB+D 120 &#124;'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NTU RGB+D 120 &#124;'
- en: '&#124; [[359](#bib.bib359)] &#124;'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[359](#bib.bib359)] &#124;'
- en: '| 2019 | 907 | C,D,S,I | Kinect | 120 | 106 | 114480 | CS(half tr, half te);
    cross view(half tr, half te) |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 907 | C,D,S,I | Kinect | 120 | 106 | 114480 | CS(一半 tr, 一半 te); 交叉视角(一半
    tr, 一半 te) |'
- en: 5.3.1 Datasets
  id: totrans-602
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 数据集
- en: 'In Section [4](#S4 "4 Action Recognition ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey"), we have reviewed the pose-based
    action recognition methods which can be divided into estimated pose-based and
    skeleton-based action recognition. The former one applies RGB data and the latter
    one directly uses skeleton data as the input. Table [12](#S5.T12 "Table 12 ‣ 5.3
    Action recognition ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") summaries the large-scale
    datasets that are prevalent in deep learning-based action recognition.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[4](#S4 "4 Action Recognition ‣ Human Pose-based Estimation, Tracking and
    Action Recognition with Deep Learning: A Survey")节中，我们回顾了基于姿态的动作识别方法，这些方法可以分为估计姿态和骨架基础的动作识别。前者使用RGB数据，而后者直接使用骨架数据作为输入。表[12](#S5.T12
    "Table 12 ‣ 5.3 Action recognition ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey")总结了在深度学习基础的动作识别中流行的大规模数据集。'
- en: 'NTU RGB+D dataset [[274](#bib.bib274)] was constructed by Nanyang Technological
    University, Singapore. Four modalities were collected using Mincrosoft Kinect
    v2 sensor including RGB, depth maps, skeletons and infrared frames. The dataset
    consists of 60 actions performed by 40 subjects. The actions can be divided into
    three groups including: 40 daily actions, 9 health-related actions and 11 person-person
    interaction actions. The age range of the subjects is from 10 to 35 years and
    each subject performs an action for several times. In total, there are 56880 samples
    which are captured in 80 distinct camera views. The large amount of variation
    in subjects and views makes it possible to have more cross-subject and cross-view
    evaluations for action recognition methods.'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: NTU RGB+D 数据集[[274](#bib.bib274)]由新加坡南洋理工大学构建。使用微软Kinect v2传感器收集了包括RGB、深度图、骨架和红外帧在内的四种模态。数据集包括40名受试者执行的60个动作。这些动作可以分为三组：40个日常动作、9个健康相关动作和11个人物互动动作。受试者的年龄范围从10岁到35岁，每个受试者执行一个动作多次。总共有56880个样本，这些样本是在80个不同的摄像机视角下捕捉的。受试者和视角的巨大变化使得动作识别方法可以进行更多的跨受试者和跨视角评估。
- en: NTU RGB+D 120 dataset [[359](#bib.bib359)] is an extension of the NTU RGB+D
    dataset [[274](#bib.bib274)]. An additional 60 action categories performed by
    another 66 subjects including 57,600 samples were added to the NTU RGB+D dataset.
    This dataset also provides four modalities including RGB, depth maps, skeletons
    and infrared frames. More number of actions, subjects and samples enable it more
    challenging than NTU RGB+D dataset in action recognition.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: NTU RGB+D 120 数据集 [[359](#bib.bib359)] 是 NTU RGB+D 数据集 [[274](#bib.bib274)]
    的扩展。新增了 60 个动作类别，由另外 66 个被试执行，包括 57,600 个样本。该数据集还提供了四种模态，包括 RGB、深度图、骨架和红外帧。更多的动作、被试和样本使其在动作识别上比
    NTU RGB+D 数据集更具挑战性。
- en: 'Table 13: Performance of estimated pose-based action recognition methods on
    three datasets for showing the benefits of pose estimation or tracking for recognition.
    GT: ground-truth.'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：三种数据集上基于估计姿势的动作识别方法的性能，展示了姿势估计或跟踪对识别的好处。GT：真实值。
- en: '| Dataset | Method | Highlights | Accuracy |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | 亮点 | 准确率 |'
- en: '| JHMDB | PoTion | estimated poses | 58.5$\pm$1.5 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| JHMDB | PoTion | 估计姿势 | 58.5$\pm$1.5 |'
- en: '| GT poses | 62.1$\pm$1.1 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| GT 姿势 | 62.1$\pm$1.1 |'
- en: '| [[251](#bib.bib251)] | GT poses + crop | 67.9$\pm$2.4 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| [[251](#bib.bib251)] | GT 姿势 + 裁剪 | 67.9$\pm$2.4 |'
- en: '| AVA | LART | -poses-tracking | 40.2 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| AVA | LART | -姿势跟踪 | 40.2 |'
- en: '| -poses | 41.4 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| -姿势 | 41.4 |'
- en: '| [[28](#bib.bib28)] | full model | 42.3 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | 完整模型 | 42.3 |'
- en: '| NTU60 | UPS | separate training | 89.6 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| NTU60 | UPS | 分开训练 | 89.6 |'
- en: '| [[259](#bib.bib259)] | joint training | 92.6 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| [[259](#bib.bib259)] | 联合训练 | 92.6 |'
- en: PKU-MMD dataset [[357](#bib.bib357)] is a large-scale multi-modality dataset
    for action detection and recognition tasks. Four modalities including RGB, depth
    maps, skeletons and infrared frames were captured by Mincrosoft Kinect v2 sensor.
    This dataset consists of 1,076 videos composed of 51 actions which are performed
    by 66 subjects in 3 views. The action classes cover 41 daily actions and 10 person-person
    interaction actions. Each video contains more than twenty action samples. In total,
    this dataset includes 3,000 minutes and 5,400,000 frames. The large amount of
    actions in one untrimmed video makes the robustness of action detection methods.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: PKU-MMD 数据集 [[357](#bib.bib357)] 是一个大规模的多模态数据集，用于动作检测和识别任务。四种模态，包括 RGB、深度图、骨架和红外帧，是通过
    Microsoft Kinect v2 传感器捕捉的。该数据集包含 1,076 个视频，涵盖 51 种动作，由 66 个被试在 3 个视角下执行。这些动作类别包括
    41 种日常动作和 10 种人际互动动作。每个视频包含超过二十个动作样本。总的来说，该数据集包含 3,000 分钟和 5,400,000 帧。大量未裁剪视频中的动作提高了动作检测方法的鲁棒性。
- en: 'Kinetics-Skeleton dataset [[358](#bib.bib358)] is an extra large-scale action
    dataset captured by searching RGB videos from YouTube and generating skeletons
    by OpenPose. It has 400 actions, with 400-1150 clips for each action, each from
    a unique YouTube video. Each clip lasts around 10 seconds. The total number of
    video samples is 306,245\. The action classes include: person actions, person-person
    actions and person-object actions. Due to the source of YouTube, the videos are
    not as professional as the ones recorded in experimental background. Therefore,
    the dataset has considerable camera motion, illumination variations, shadows,
    background clutter and a large variety of subjects.'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: Kinetics-Skeleton 数据集 [[358](#bib.bib358)] 是一个超大规模的动作数据集，通过从 YouTube 搜索 RGB
    视频并通过 OpenPose 生成骨架来捕捉。它有 400 种动作，每种动作有 400-1150 个剪辑，每个剪辑来自一个独特的 YouTube 视频。每个剪辑大约持续
    10 秒。视频样本总数为 306,245 个。动作类别包括：人物动作、人际动作和人物-物体动作。由于来源于 YouTube，这些视频不像实验背景中录制的视频那样专业。因此，该数据集有显著的相机运动、光照变化、阴影、背景杂乱和大量的受试者。
- en: 'Table 14: Performance comparison of action recognition methods on NTU RGB+D
    and NTU RGB+D 120 datasets.'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：NTU RGB+D 和 NTU RGB+D 120 数据集上动作识别方法的性能比较。
- en: '| Method | Category | Sub-category | Year |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类别 | 子类别 | 年份 |'
- en: '&#124; NTU RGB + D 60 &#124;'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NTU RGB + D 60 &#124;'
- en: '&#124; C-Sub      C-Set &#124;'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C-Sub      C-Set &#124;'
- en: '|'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; NTU RGB + D 120 &#124;'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NTU RGB + D 120 &#124;'
- en: '&#124; C-Sub      C-Set &#124;'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C-Sub      C-Set &#124;'
- en: '|'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Zolfaghari et al. [[250](#bib.bib250)] | Estimated Pose-based | two-stage
    strategy | 2017 | 80.8 | - | - | - |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| Zolfaghari et al. [[250](#bib.bib250)] | 基于估计姿势 | 两阶段策略 | 2017 | 80.8 | -
    | - | - |'
- en: '| Liu et al. [[252](#bib.bib252)] | Estimated Pose-based | two-stage strategy
    | 2018 | 91.7 | 95.3 | - | - |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. [[252](#bib.bib252)] | 基于估计姿势 | 两阶段策略 | 2018 | 91.7 | 95.3 | -
    | - |'
- en: '| IntegralAction [[253](#bib.bib253)] | Estimated Pose-based | two-stage strategy
    | 2021 | 91.7 | - | - | - |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| IntegralAction [[253](#bib.bib253)] | 基于估计姿势 | 两阶段策略 | 2021 | 91.7 | - |
    - | - |'
- en: '| PoseConv3D [[255](#bib.bib255)] | Estimated Pose-based | two-stage strategy
    | 2021 | 94.1 | 97.1 | 86.9 | 90.3 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| PoseConv3D [[255](#bib.bib255)] | 基于估计姿态 | 两阶段策略 | 2021 | 94.1 | 97.1 | 86.9
    | 90.3 |'
- en: '| Luvizonet al. [[258](#bib.bib258)] | Estimated Pose-based | one-stage strategy
    | 2018 | 85.5 | - | - | - |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| Luvizon 等人 [[258](#bib.bib258)] | 基于估计姿态 | 一阶段策略 | 2018 | 85.5 | - | - |
    - |'
- en: '| UPS [[259](#bib.bib259)] | Estimated Pose-based | one-stage strategy | 2023
    | 92.6 | 97.0 | 89.3 | 91.1 |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| UPS [[259](#bib.bib259)] | 基于估计姿态 | 一阶段策略 | 2023 | 92.6 | 97.0 | 89.3 | 91.1
    |'
- en: '| 2 Layere P-LSTM [[274](#bib.bib274)] | RNN-based | spatial division of human
    body | 2016 | 62.9 | 70.3 | - | - |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| 2 Layere P-LSTM [[274](#bib.bib274)] | 基于 RNN | 人体空间划分 | 2016 | 62.9 | 70.3
    | - | - |'
- en: '| Trust Gate ST-LSTM [[280](#bib.bib280)] | RNN-based | spatial and/or temporal
    networks | 2016 | 69.2 | 77.7 | - | - |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| Trust Gate ST-LSTM [[280](#bib.bib280)] | 基于 RNN | 空间和/或时间网络 | 2016 | 69.2
    | 77.7 | - | - |'
- en: '| Two-stream RNN [[244](#bib.bib244)] | RNN-based | spatial and/or temporal
    networks | 2017 | 71.3 | 79.5 | - | - |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| Two-stream RNN [[244](#bib.bib244)] | 基于 RNN | 空间和/或时间网络 | 2017 | 71.3 |
    79.5 | - | - |'
- en: '| Zhang et al. [[281](#bib.bib281)] | RNN-based | spatial and/or temporal networks
    | 2017 | 70.3 | 82.4 | - | - |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 [[281](#bib.bib281)] | 基于 RNN | 空间和/或时间网络 | 2017 | 70.3 | 82.4 |
    - | - |'
- en: '| SR-TSL [[282](#bib.bib282)] | RNN-based | spatial and/or temporal networks
    | 2018 | 84.8 | 92.4 | - | - |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| SR-TSL [[282](#bib.bib282)] | 基于 RNN | 空间和/或时间网络 | 2018 | 84.8 | 92.4 | -
    | - |'
- en: '| GCA-LSTM [[276](#bib.bib276)] | RNN-based | attention mechanism | 2017 |
    74.4 | 82.8 | 58.3 | 59.2 |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| GCA-LSTM [[276](#bib.bib276)] | 基于 RNN | 注意力机制 | 2017 | 74.4 | 82.8 | 58.3
    | 59.2 |'
- en: '| STA-LSTM [[277](#bib.bib277)] | RNN-based | attention mechanism | 2018 |
    73.4 | 81.2 | - | - |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| STA-LSTM [[277](#bib.bib277)] | 基于 RNN | 注意力机制 | 2018 | 73.4 | 81.2 | - |
    - |'
- en: '| EleAtt-GRU [[278](#bib.bib278)] | RNN-based | attention mechanism | 2019
    | 80.7 | 88.4 | - | - |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| EleAtt-GRU [[278](#bib.bib278)] | 基于 RNN | 注意力机制 | 2019 | 80.7 | 88.4 | -
    | - |'
- en: '| 2s AGC-LSTM [[279](#bib.bib279)] | RNN-based | attention mechanism | 2019
    | 89.2 | 95.0 | - | - |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| 2s AGC-LSTM [[279](#bib.bib279)] | 基于 RNN | 注意力机制 | 2019 | 89.2 | 95.0 |
    - | - |'
- en: '| JTM [[261](#bib.bib261)] | CNN-based | 2D CNN | 2017 | 73.4 | 75.2 | - |
    - |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| JTM [[261](#bib.bib261)] | 基于 CNN | 2D CNN | 2017 | 73.4 | 75.2 | - | - |'
- en: '| JDM [[263](#bib.bib263)] | CNN-based | 2D CNN | 2017 | 76.2 | 82.3 | - |
    - |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| JDM [[263](#bib.bib263)] | 基于 CNN | 2D CNN | 2017 | 76.2 | 82.3 | - | - |'
- en: '| Liu et al. [[264](#bib.bib264)] | CNN-based | 2D CNN | 2017 | 80.0 | 87.2
    | 60.3 | 63.2 |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 [[264](#bib.bib264)] | 基于 CNN | 2D CNN | 2017 | 80.0 | 87.2 | 60.3
    | 63.2 |'
- en: '| SkeletonNet [[265](#bib.bib265)] | CNN-based | 2D CNN | 2017 | 75.9 | 81.2
    | - | - |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| SkeletonNet [[265](#bib.bib265)] | 基于 CNN | 2D CNN | 2017 | 75.9 | 81.2 |
    - | - |'
- en: '| Ke et al. [[268](#bib.bib268)] | CNN-based | 2D CNN | 2017 | 79.6 | 86.8
    | - | - |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| Ke 等人 [[268](#bib.bib268)] | 基于 CNN | 2D CNN | 2017 | 79.6 | 86.8 | - | -
    |'
- en: '| Li et al. [[360](#bib.bib360)] | CNN-based | 2D CNN | 2017 | 85.0 | 92.3
    | - | - |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 [[360](#bib.bib360)] | 基于 CNN | 2D CNN | 2017 | 85.0 | 92.3 | - | -
    |'
- en: '| Ding et al. [[267](#bib.bib267)] | CNN-based | 2D CNN | 2017 | - | 82.3 |
    - | - |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| Ding 等人 [[267](#bib.bib267)] | 基于 CNN | 2D CNN | 2017 | - | 82.3 | - | -
    |'
- en: '| Li et al. [[266](#bib.bib266)] | CNN-based | 2D CNN | 2017 | 82.8 | 90.1
    | - | - |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 [[266](#bib.bib266)] | 基于 CNN | 2D CNN | 2017 | 82.8 | 90.1 | - | -
    |'
- en: '| TSRJI [[245](#bib.bib245)] | CNN-based | 2D CNN | 2019 | 73.3 | 80.3 | 65.5
    | 59.7 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| TSRJI [[245](#bib.bib245)] | 基于 CNN | 2D CNN | 2019 | 73.3 | 80.3 | 65.5
    | 59.7 |'
- en: '| SkeletonMotion [[245](#bib.bib245)] | CNN-based | 2D CNN | 2019 | 76.5 |
    84.7 | 67.7 | 66.9 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| SkeletonMotion [[245](#bib.bib245)] | 基于 CNN | 2D CNN | 2019 | 76.5 | 84.7
    | 67.7 | 66.9 |'
- en: '| 3SCNN [[269](#bib.bib269)] | CNN-based | 2D CNN | 2019 | 88.6 | 93.7 | -
    | - |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| 3SCNN [[269](#bib.bib269)] | 基于 CNN | 2D CNN | 2019 | 88.6 | 93.7 | - | -
    |'
- en: '| DM-3DCNN [[271](#bib.bib271)] | CNN-based | 3D CNN | 2017 | 82.0 | 89.5 |
    - | - |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| DM-3DCNN [[271](#bib.bib271)] | 基于 CNN | 3D CNN | 2017 | 82.0 | 89.5 | -
    | - |'
- en: '| ST-GCN [[246](#bib.bib246)] | GCN-based | static method | 2018 | 81.5 | 88.3
    | - | - |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| ST-GCN [[246](#bib.bib246)] | 基于 GCN | 静态方法 | 2018 | 81.5 | 88.3 | - | -
    |'
- en: '| STIGCN [[283](#bib.bib283)] | GCN-based | static method | 2020 | 90.1 | 96.1
    | - | - |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| STIGCN [[283](#bib.bib283)] | 基于 GCN | 静态方法 | 2020 | 90.1 | 96.1 | - | -
    |'
- en: '| MS-G3D [[284](#bib.bib284)] | GCN-based | static method | 2020 | 91.5 | 96.2
    | 86.9 | 88.4 |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| MS-G3D [[284](#bib.bib284)] | 基于 GCN | 静态方法 | 2020 | 91.5 | 96.2 | 86.9 |
    88.4 |'
- en: '| CA-GCN [[285](#bib.bib285)] | GCN-based | static method | 2020 | 83.5 | 91.4
    | - | - |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| CA-GCN [[285](#bib.bib285)] | 基于 GCN | 静态方法 | 2020 | 83.5 | 91.4 | - | -
    |'
- en: '| AS-GCN [[286](#bib.bib286)] | GCN-based | dynamic method | 2018 | 86.8 |
    94.2 | - | - |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| AS-GCN [[286](#bib.bib286)] | 基于 GCN | 动态方法 | 2018 | 86.8 | 94.2 | - | -
    |'
- en: '| 2s-AGCN [[287](#bib.bib287)] | GCN-based | dynamic method | 2020 | 88.5 |
    95.1 | - | - |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| 2s-AGCN [[287](#bib.bib287)] | 基于 GCN | 动态方法 | 2020 | 88.5 | 95.1 | - | -
    |'
- en: '| SGN [[301](#bib.bib301)] | GCN-based | dynamic method | 2020 | 89.0 | 94.5
    | 79.2 | 81.5 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| SGN [[301](#bib.bib301)] | 基于 GCN | 动态方法 | 2020 | 89.0 | 94.5 | 79.2 | 81.5
    |'
- en: '| 4s Shift-GCN [[288](#bib.bib288)] | GCN-based | dynamic method | 2020 | 90.7
    | 96.5 | 85.9 | 87.6 |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| 4s Shift-GCN [[288](#bib.bib288)] | 基于GCN | 动态方法 | 2020 | 90.7 | 96.5 | 85.9
    | 87.6 |'
- en: '| DC-GCN+ADC [[361](#bib.bib361)] | GCN-based | dynamic method | 2020 | 90.8
    | 96.6 | 86.5 | 88.1 |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| DC-GCN+ADC [[361](#bib.bib361)] | 基于GCN | 动态方法 | 2020 | 90.8 | 96.6 | 86.5
    | 88.1 |'
- en: '| DDGCN [[289](#bib.bib289)] | GCN-based | dynamic method | 2020 | 91.1 | 97.1
    | - | - |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| DDGCN [[289](#bib.bib289)] | 基于GCN | 动态方法 | 2020 | 91.1 | 97.1 | - | - |'
- en: '| Dynamic GCN [[302](#bib.bib302)] | GCN-based | dynamic method | 2020 | 91.5
    | 96.0 | 87.3 | 88.6 |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| Dynamic GCN [[302](#bib.bib302)] | 基于GCN | 动态方法 | 2020 | 91.5 | 96.0 | 87.3
    | 88.6 |'
- en: '| CTR-GCN [[290](#bib.bib290)] | GCN-based | dynamic method | 2021 | 92.4 |
    96.8 | 88.9 | 90.6 |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| CTR-GCN [[290](#bib.bib290)] | 基于GCN | 动态方法 | 2021 | 92.4 | 96.8 | 88.9 |
    90.6 |'
- en: '| InfoGCN [[291](#bib.bib291)] | GCN-based | dynamic method | 2021 | 93.0 |
    97.1 | 89.8 | 91.2 |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| InfoGCN [[291](#bib.bib291)] | 基于GCN | 动态方法 | 2021 | 93.0 | 97.1 | 89.8 |
    91.2 |'
- en: '| DG-STGCN [[292](#bib.bib292)] | GCN-based | dynamic method | 2022 | 93.2
    | 97.5 | 89.6 | 91.3 |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| DG-STGCN [[292](#bib.bib292)] | 基于GCN | 动态方法 | 2022 | 93.2 | 97.5 | 89.6
    | 91.3 |'
- en: '| TCA-GCN [[293](#bib.bib293)] | GCN-based | dynamic method | 2022 | 92.8 |
    97.0 | 89.4 | 90.8 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| TCA-GCN [[293](#bib.bib293)] | 基于GCN | 动态方法 | 2022 | 92.8 | 97.0 | 89.4 |
    90.8 |'
- en: '| ML-STGNet [[298](#bib.bib298)] | GCN-based | dynamic method | 2023 | 91.9
    | 96.2 | 88.6 | 90.0 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| ML-STGNet [[298](#bib.bib298)] | 基于GCN | 动态方法 | 2023 | 91.9 | 96.2 | 88.6
    | 90.0 |'
- en: '| MV-IGNet [[303](#bib.bib303)] | GCN-based | dynamic method | 2023 | 89.2
    | 96.3 | 83.9 | 85.6 |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| MV-IGNet [[303](#bib.bib303)] | 基于GCN | 动态方法 | 2023 | 89.2 | 96.3 | 83.9
    | 85.6 |'
- en: '| S-GDC [[304](#bib.bib304)] | GCN-based | dynamic method | 2023 | 88.6 | 94.9
    | 85.2 | 86.1 |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| S-GDC [[304](#bib.bib304)] | 基于GCN | 动态方法 | 2023 | 88.6 | 94.9 | 85.2 | 86.1
    |'
- en: '| Motif-GCN+TBs [[294](#bib.bib294)] | GCN-based | dynamic method | 2023 |
    90.5 | 96.1 | 87.1 | 87.7 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| Motif-GCN+TBs [[294](#bib.bib294)] | 基于GCN | 动态方法 | 2023 | 90.5 | 96.1 |
    87.1 | 87.7 |'
- en: '| 3s-ActCLR [[295](#bib.bib295)] | GCN-based | dynamic method | 2023 | 84.3
    | 88.8 | 74.3 | 75.7 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| 3s-ActCLR [[295](#bib.bib295)] | 基于GCN | 动态方法 | 2023 | 84.3 | 88.8 | 74.3
    | 75.7 |'
- en: '| GSTLN [[297](#bib.bib297)] | GCN-based | dynamic method | 2023 | 91.9 | 96.6
    | 88.1 | 89.3 |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| GSTLN [[297](#bib.bib297)] | 基于GCN | 动态方法 | 2023 | 91.9 | 96.6 | 88.1 | 89.3
    |'
- en: '| 4s STF-Net [[300](#bib.bib300)] | GCN-based | dynamic method | 2023 | 91.1
    | 96.5 | 86.5 | 88.2 |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| 4s STF-Net [[300](#bib.bib300)] | 基于GCN | 动态方法 | 2023 | 91.1 | 96.5 | 86.5
    | 88.2 |'
- en: '| LA-GCN [[305](#bib.bib305)] | GCN-based | dynamic method | 2023 | 93.5 |
    97.2 | 90.7 | 91.8 |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| LA-GCN [[305](#bib.bib305)] | 基于GCN | 动态方法 | 2023 | 93.5 | 97.2 | 90.7 |
    91.8 |'
- en: '| DSTA-Net [[309](#bib.bib309)] | Transformer-based | pure Transformer | 2020
    | 91.5 | 96.4 | 86.6 | 89.0 |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| DSTA-Net [[309](#bib.bib309)] | 基于Transformer | 纯Transformer | 2020 | 91.5
    | 96.4 | 86.6 | 89.0 |'
- en: '| STAR [[313](#bib.bib313)] | Transformer-based | pure Transformer | 2021 |
    83.4 | 89.0 | 78.3 | 80.2 |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| STAR [[313](#bib.bib313)] | 基于Transformer | 纯Transformer | 2021 | 83.4 |
    89.0 | 78.3 | 80.2 |'
- en: '| STST [[312](#bib.bib312)] | Transformer-based | pure Transformer | 2021 |
    91.9 | 96.8 | - | - |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| STST [[312](#bib.bib312)] | 基于Transformer | 纯Transformer | 2021 | 91.9 |
    96.8 | - | - |'
- en: '| IIP-Former [[310](#bib.bib310)] | Transformer-based | pure Transformer |
    2022 | 92.3 | 96.4 | 88.4 | 89.7 |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| IIP-Former [[310](#bib.bib310)] | 基于Transformer | 纯Transformer | 2022 | 92.3
    | 96.4 | 88.4 | 89.7 |'
- en: '| RSA-Net [[314](#bib.bib314)] | Transformer-based | pure Transformer | 2023
    | 91.8 | 96.8 | 88.4 | 89.7 |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| RSA-Net [[314](#bib.bib314)] | 基于Transformer | 纯Transformer | 2023 | 91.8
    | 96.8 | 88.4 | 89.7 |'
- en: '| ST-TR [[247](#bib.bib247)] | Transformer-based | hybrid Transformer | 2021
    | 89.9 | 96.1 | 81.9 | 84.1 |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| ST-TR [[247](#bib.bib247)] | 基于Transformer | 混合Transformer | 2021 | 89.9
    | 96.1 | 81.9 | 84.1 |'
- en: '| Zoom Transformer [[318](#bib.bib318)] | Transformer-based | hybrid Transformer
    | 2022 | 90.1 | 95.3 | 84.8 | 86.5 |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| Zoom Transformer [[318](#bib.bib318)] | 基于Transformer | 混合Transformer | 2022
    | 90.1 | 95.3 | 84.8 | 86.5 |'
- en: '| KA-AGTN [[320](#bib.bib320)] | Transformer-based | hybrid Transformer | 2022
    | 90.4 | 96.1 | 86.1 | 88.0 |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| KA-AGTN [[320](#bib.bib320)] | 基于Transformer | 混合Transformer | 2022 | 90.4
    | 96.1 | 86.1 | 88.0 |'
- en: '| STTFormer [[316](#bib.bib316)] | Transformer-based | hybrid Transformer |
    2022 | 92.3 | 96.5 | 88.3 | 89.2 |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| STTFormer [[316](#bib.bib316)] | 基于Transformer | 混合Transformer | 2022 | 92.3
    | 96.5 | 88.3 | 89.2 |'
- en: '| FG-STFormer [[319](#bib.bib319)] | Transformer-based | hybrid Transformer
    | 2022 | 92.6 | 96.7 | 89.0 | 90.6 |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| FG-STFormer [[319](#bib.bib319)] | 基于Transformer | 混合Transformer | 2022 |
    92.6 | 96.7 | 89.0 | 90.6 |'
- en: '| GSTN [[362](#bib.bib362)] | Transformer-based | hybrid Transformer | 2022
    | 91.3 | 96.6 | 86.4 | 88.7 |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| GSTN [[362](#bib.bib362)] | 基于Transformer | 混合Transformer | 2022 | 91.3 |
    96.6 | 86.4 | 88.7 |'
- en: '| IGFormer [[321](#bib.bib321)] | Transformer-based | hybrid Transformer |
    2022 | 93.6 | 96.5 | 85.4 | 86.5 |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| IGFormer [[321](#bib.bib321)] | 基于Transformer | 混合Transformer | 2022 | 93.6
    | 96.5 | 85.4 | 86.5 |'
- en: '| 3Mformer [[306](#bib.bib306)] | Transformer-based | hybrid Transformer |
    2023 | 94.8 | 98.7 | 92.0 | 93.8 |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| 3Mformer [[306](#bib.bib306)] | 基于Transformer | 混合Transformer | 2023 | 94.8
    | 98.7 | 92.0 | 93.8 |'
- en: '| SkeleTR [[322](#bib.bib322)] | Transformer-based | hybrid Transformer | 2023
    | 94.8 | 97.7 | 87.8 | 88.3 |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| SkeleTR [[322](#bib.bib322)] | 基于 Transformer | 混合 Transformer | 2023 | 94.8
    | 97.7 | 87.8 | 88.3 |'
- en: '| GL-Transformer [[323](#bib.bib323)] | Transformer-based | unsupervised Transformer
    | 2022 | 76.3 | 83.8 | 66.0 | 68.7 |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| GL-Transformer [[323](#bib.bib323)] | 基于 Transformer | 无监督 Transformer |
    2022 | 76.3 | 83.8 | 66.0 | 68.7 |'
- en: '| HiCo-LSTM [[324](#bib.bib324)] | Transformer-based | unsupervised Transformer
    | 2023 | 81.4 | 88.8 | 73.7 | 74.5 |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| HiCo-LSTM [[324](#bib.bib324)] | 基于 Transformer | 无监督 Transformer | 2023
    | 81.4 | 88.8 | 73.7 | 74.5 |'
- en: '| HaLP+CMD [[325](#bib.bib325)] | Transformer-based | self-supervised Transformer
    | 2023 | 82.1 | 88.6 | 72.6 | 73.1 |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| HaLP+CMD [[325](#bib.bib325)] | 基于 Transformer | 自监督 Transformer | 2023 |
    82.1 | 88.6 | 72.6 | 73.1 |'
- en: '| SkeAttnCLR [[328](#bib.bib328)] | Transformer-based | self-supervised Transformer
    | 2023 | 82.0 | 86.5 | 77.1 | 80.0 |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| SkeAttnCLR [[328](#bib.bib328)] | 基于 Transformer | 自监督 Transformer | 2023
    | 82.0 | 86.5 | 77.1 | 80.0 |'
- en: '| SkeletonMAE [[327](#bib.bib327)] | Transformer-based | self-supervised Transformer
    | 2023 | 86.6 | 92.9 | 76.8 | 79.1 |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| SkeletonMAE [[327](#bib.bib327)] | 基于 Transformer | 自监督 Transformer | 2023
    | 86.6 | 92.9 | 76.8 | 79.1 |'
- en: 5.3.2 Performance comparison
  id: totrans-696
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 性能比较
- en: 'In Table [14](#S5.T14 "Table 14 ‣ 5.3.1 Datasets ‣ 5.3 Action recognition ‣
    5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey"), we compare the results of different action recognition
    methods on two prominent datasets. Estimated poses-based methods apply RGB data
    as the input, and the best performance [[255](#bib.bib255), [259](#bib.bib259)]
    is lower than the ones [[306](#bib.bib306)] used skeletons as the input on two
    datasets (especially the larger one). This is reasonable because some facts (eg.
    illumination, background) could affect the performance when using RGB. In particular,
    methods based on one-stage strategy jointly address pose estimation and action
    recognition, thus reducing the errors of intermediate steps and generally achieving
    better results than the methods based on a two-stage strategy. Moreover, Table
    [13](#S5.T13 "Table 13 ‣ 5.3.1 Datasets ‣ 5.3 Action recognition ‣ 5 Benchmark
    datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep
    Learning: A Survey") illustrates the effects of pose estimation (PE) and tracking
    on action recognition (AR). It can be easily seen that pose estimation and tracking
    results can improve the performance of action recognition, which further emphasizes
    the relationship of these three tasks.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [14](#S5.T14 "表 14 ‣ 5.3.1 数据集 ‣ 5.3 动作识别 ‣ 5 基准数据集 ‣ 基于深度学习的人体姿态估计、跟踪与动作识别：综述")
    中，我们比较了不同动作识别方法在两个重要数据集上的结果。基于姿态估计的方法使用 RGB 数据作为输入，而最佳性能 [[255](#bib.bib255),
    [259](#bib.bib259)] 低于那些 [[306](#bib.bib306)] 使用骨架作为输入的方法在两个数据集上的表现（尤其是较大的那个）。这是合理的，因为一些因素（例如光照、背景）可能会影响使用
    RGB 时的性能。特别是，基于单阶段策略的方法同时解决姿态估计和动作识别，从而减少了中间步骤的错误，通常比基于两阶段策略的方法取得更好的结果。此外，表 [13](#S5.T13
    "表 13 ‣ 5.3.1 数据集 ‣ 5.3 动作识别 ‣ 5 基准数据集 ‣ 基于深度学习的人体姿态估计、跟踪与动作识别：综述") 说明了姿态估计 (PE)
    和跟踪对动作识别 (AR) 的影响。可以清楚地看到，姿态估计和跟踪结果可以提高动作识别的性能，这进一步强调了这三个任务之间的关系。
- en: For the skeleton-based methods, the recent methods mainly apply GCN and Transformer,
    consistently outperforming CNN and RNN-based methods. This improvement demonstrate
    the benefit of local and global feature learning based on GCN and Transformer
    for action recognition. Specifically, dynamic GCN-based methods generally perform
    better than static GCN-based ones due to stronger generalization capabilities.
    Hybrid Transformer-based methods outperform pure Transformer-based ones on large
    datasets since integrating the Transformer with GCN or CNN can better learn both
    local and global features. Specifically, the method [[306](#bib.bib306)] of applying
    transformer encoder on hypergraph achieved the best performance on two datasets,
    which provides a hint of representing actions using hypergraph for classification.
    It is also worth noting that the method [[305](#bib.bib305)] based on the guidance
    of natural language respectively achieves pretty good performance on two datasets,
    which implies the advantage of incorporating linguistic context for action recognition.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于骨架的方法，近期的方法主要应用GCN和Transformer，这些方法一贯地优于基于CNN和RNN的方法。这一改进展示了基于GCN和Transformer的局部和全局特征学习对动作识别的好处。具体来说，由于更强的泛化能力，基于动态GCN的方法通常表现优于基于静态GCN的方法。基于混合Transformer的方法在大数据集上优于纯Transformer的方法，因为将Transformer与GCN或CNN结合可以更好地学习局部和全局特征。具体来说，方法[[306](#bib.bib306)]在超图上应用transformer编码器，在两个数据集上取得了最佳性能，这为使用超图表示动作以进行分类提供了线索。值得注意的是，基于自然语言指导的方法[[305](#bib.bib305)]在两个数据集上分别取得了相当好的性能，这暗示了将语言上下文融入动作识别的优势。
- en: 6 Challenges and Future Directions
  id: totrans-699
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 个挑战与未来方向
- en: This paper has reviewed recent deep learning-based approaches for pose estimation,
    tracking and action recognition. It also includes a discussion of commonly used
    datasets and a comparative analysis of various methods. Despite the the remarkable
    successes in these domains, there are still some challenges and corresponding
    research directions to promote advances for the three tasks.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 本文回顾了近期基于深度学习的姿态估计、跟踪和动作识别方法。它还包括了常用数据集的讨论以及各种方法的比较分析。尽管在这些领域取得了显著的成功，但仍存在一些挑战以及相应的研究方向，以推动这三项任务的进步。
- en: 6.1 Pose estimation
  id: totrans-701
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 姿态估计
- en: There are five main challenges for the pose estimation task as follows.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态估计任务面临五个主要挑战，如下所示。
- en: (1) Occlusion
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 遮挡
- en: Although the current methods have achieved outstanding performance on public
    datasets, they still suffer from the occlusion problem. Occlusion results in unreliable
    human detection and declined performance for pose estimation. Person detectors
    in top-down approaches may fail in identifying the boundaries of overlapped human
    bodies and body part association for occluded scenes may fail in bottom-up approaches.
    Mutual occlusion in crowd scenarios caused largely declined performance for current
    3D HPE methods.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前的方法在公共数据集上表现出色，但仍然存在遮挡问题。遮挡导致人类检测不可靠，并降低了姿态估计的性能。顶视图方法中的人检测器可能无法识别重叠人体的边界，且底视图方法中的身体部位关联在遮挡场景中可能失败。在拥挤场景中的相互遮挡显著降低了当前3D
    HPE方法的性能。
- en: To overcome this problem, some methods [[363](#bib.bib363), [364](#bib.bib364),
    [365](#bib.bib365)] have been proposed based on multi-view learning. This is because
    the occluded part in one view may become visible in other views. However, these
    methods often need large memory and expensive computation costs, especially for
    3D MPPE under multi-view. Moreover, some methods based on multi-modal learning
    have also been demonstrated for robustness to occlusion, which could extract enrich
    features from different sensing modalities such as depth [[366](#bib.bib366)]
    and wearable inertial measurement units [[367](#bib.bib367)]. When applying pose
    estimation from different modalities, it may face another problem of few available
    datasets with different modalities. With the development of vision-language models,
    texts could provide semantics for pose estimation and also be easily generated
    by GPT, thus a better direction for another modality. Based on pose semantics,
    the occluded part can be inferred. With regard the semantics, human-scene relationships
    can also provide some semantic cues such as a person cannot be simultaneously
    present in the locations of other objects in the scene.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，一些方法[[363](#bib.bib363), [364](#bib.bib364), [365](#bib.bib365)] 基于多视角学习被提出。这是因为在一个视角中被遮挡的部分可能在其他视角中变得可见。然而，这些方法通常需要大量的内存和高昂的计算成本，特别是在多视角下的3D
    MPPE。此外，一些基于多模态学习的方法也已被证明对遮挡具有鲁棒性，这些方法可以从不同的传感模态中提取丰富的特征，例如深度[[366](#bib.bib366)]和可穿戴惯性测量单元[[367](#bib.bib367)]。在应用来自不同模态的姿态估计时，可能会面临不同模态下可用数据集稀少的另一个问题。随着视觉-语言模型的发展，文本可以为姿态估计提供语义，并且可以通过
    GPT 轻松生成，因此这是另一个模态的更好方向。基于姿态语义，可以推断出被遮挡的部分。关于语义，人-场景关系也可以提供一些语义线索，例如一个人不能同时出现在场景中其他物体的位置。
- en: (2) Low resolution
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 低分辨率
- en: In the real-word application, low-resolution images or videos are often captured
    due to wide-view cameras, long-distance shooting capturing devices and so on.
    Obscured persons also exist due to environmental shadows. The current methods
    are usually trained on high-resolution input, which may cause low accuracy when
    applying them to low-resolution input. One solution for estimating poses from
    low-resolution input is to recover image resolution by applying super-resolution
    methods as image pre-processing. However, the optimization of super-resolution
    does not contribution to high-level human pose analysis. Wang et al. [[368](#bib.bib368)]
    observed that low-resolution would exaggerate the degree of quantization error,
    thus offset modeling may be helpful for pose estimation with low-resolution input.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，由于广角摄像机、长距离拍摄设备等原因，低分辨率的图像或视频经常被捕捉到。此外，由于环境阴影，人物也可能被遮挡。当前的方法通常是在高分辨率输入上进行训练的，因此在应用于低分辨率输入时可能会导致准确度降低。一个从低分辨率输入中估计姿态的解决方案是通过应用超分辨率方法来恢复图像分辨率，作为图像预处理。然而，超分辨率的优化对高级人类姿态分析没有贡献。Wang
    等人[[368](#bib.bib368)] 观察到低分辨率会夸大量化误差的程度，因此偏移建模可能对低分辨率输入的姿态估计有所帮助。
- en: (3) Computation complexity
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 计算复杂度
- en: 'As reviewed in Section [2](#S2 "2 Pose estimation ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey"), many methods have
    been proposed for solving computation complexity. For example, one-stage methods
    for image-based MPPE are proposed to save the increased time consumption caused
    by intermediate steps. Sample frames-based methods for video-based pose estimation
    are proposed to reduce the complexity of processing each frame. However, such
    one-stage methods may sacrifice accuracy when improving efficiency (eg. the recent
    ED-pose network [[65](#bib.bib65)] takes the shortest time and would sacrifice
    about %4 AP on CoCO val2017 dataset). Therefore, more effort into one-stage methods
    for MPPE is required to achieve computationally efficient pose estimation while
    maintaining high accuracy. Sample frames-based methods [[127](#bib.bib127)] estimate
    poses based on three steps, which still results in more time consumption. Hence,
    an end-to-end network is preferred to incorporate with sample frames-based methods
    for video-based pose estimation.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '正如在第 [2](#S2 "2 Pose estimation ‣ Human Pose-based Estimation, Tracking and
    Action Recognition with Deep Learning: A Survey") 节中回顾的，已经提出了许多解决计算复杂性的方法。例如，提出了一阶段图像
    MPPE 方法以节省由中间步骤引起的时间消耗。提出了基于样本帧的视频姿势估计方法，以降低处理每帧的复杂性。然而，这种一阶段方法可能在提高效率时牺牲准确性（例如，最近的
    ED-pose 网络 [[65](#bib.bib65)] 需要最短时间，但在 CoCO val2017 数据集上的 AP 可能减少约 4%）。因此，需要更多努力在一阶段
    MPPE 方法上，以实现计算高效的姿势估计，同时保持高准确性。基于样本帧的方法 [[127](#bib.bib127)] 基于三步骤进行姿势估计，这仍然会导致更多时间消耗。因此，端到端网络更适合与基于样本帧的方法结合用于视频姿势估计。'
- en: Transformer-based architectures for video-based 3D pose estimation inevitably
    incur high computational costs. This is because that they typically regard each
    video frame as a pose token and apply extremely long video frames to achieve advanced
    performance. For instance, Strided [[206](#bib.bib206)] and Mhformer [[207](#bib.bib207)]
    require 351 frames, and MixSTE [[207](#bib.bib207)] and DSTformer [[222](#bib.bib222)]
    require 243 frames. Self-attention complexity increases quadratically with the
    number of tokens. Although directly reducing the frame number can reduce the cost,
    it may result in lower performance due to a small temporal receptive field. Therefore,
    it is preferable to design an efficient architecture while maintaining a large
    temporal receptive field for accurate estimation. Considering that similar tokens
    may exist in deep transformer blocks [[369](#bib.bib369)], one potential solution
    is to prune pose tokens to improve the efficiency.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 的视频 3D 姿势估计架构不可避免地会产生高计算成本。这是因为它们通常将每个视频帧视为一个姿势 token，并应用极长的视频帧以实现先进的性能。例如，Strided [[206](#bib.bib206)]
    和 Mhformer [[207](#bib.bib207)] 需要 351 帧，而 MixSTE [[207](#bib.bib207)] 和 DSTformer [[222](#bib.bib222)]
    需要 243 帧。自注意力复杂性随着 token 数量的平方增加。虽然直接减少帧数可以降低成本，但由于时间感受野较小，这可能导致性能降低。因此，设计一个高效的架构，同时保持较大的时间感受野以进行准确估计是更为理想的。考虑到深层
    Transformer 块中可能存在相似的 token [[369](#bib.bib369)]，一种潜在的解决方案是修剪姿势 token 以提高效率。
- en: (4) Limited data for uncommon poses
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 不常见姿势的数据有限
- en: The current public datasets have limited training data for uncommon poses (eg.
    falling), which results in model bias and further low accuracy on such poses.
    Data augmentation [[370](#bib.bib370), [371](#bib.bib371)] for uncommon poses
    is a common method for generating new samples with more diversity. Optimization-based
    methods [[372](#bib.bib372)] can mitigate the impact of domain gaps, by estimating
    poses case-by-case rather than learning. Therefore, deep-learning-based method
    combining optimization techniques might be helpful for uncommon pose estimation.
    Moreover, open vocabulary learning can be also applied to estimating uncommon
    poses by the semantic relationship between these poses with other common poses.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 目前公开的数据集对于不常见姿势（如跌倒）的训练数据有限，这导致模型偏倚，并且在这些姿势上的准确性较低。数据增强 [[370](#bib.bib370),
    [371](#bib.bib371)] 是生成具有更多多样性的新样本的常用方法。基于优化的方法 [[372](#bib.bib372)] 可以通过逐个估计姿势而不是学习来减轻领域间隙的影响。因此，结合优化技术的深度学习方法可能对不常见姿势估计有所帮助。此外，通过这些姿势与其他常见姿势之间的语义关系，开放词汇学习也可以用于估计不常见姿势。
- en: (5) High uncertainty of 3D poses
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 3D 姿势的高不确定性
- en: Predicting 3D poses from 2D poses is required to handle uncertainty and indeterminacy
    due to depth ambiguity and potential occlusion. However, most of the existing
    methods [[210](#bib.bib210)] belong to deterministic methods which aim to construct
    single and definite 3D poses from images. Therefore, how to handle uncertainty
    and indeterminacy of poses remains an open question. Inspired by the strong capability
    of diffusion models to generate samples with high uncertainty, applying diffusion
    models is a promising direction for pose estimation. Few methods [[373](#bib.bib373),
    [209](#bib.bib209), [137](#bib.bib137)] have been recently proposed by formulating
    3D pose estimation as a reverse diffusion process.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 从2D姿态预测3D姿态需要处理由于深度模糊和潜在遮挡带来的不确定性和不确定性。然而，大多数现有方法[[210](#bib.bib210)]属于确定性方法，其目标是从图像中构建单一且明确的3D姿态。因此，如何处理姿态的不确定性和不确定性仍然是一个悬而未决的问题。受到扩散模型在生成高不确定性样本方面强大能力的启发，应用扩散模型是姿态估计的一个有前景的方向。最近提出了一些方法[[373](#bib.bib373),
    [209](#bib.bib209), [137](#bib.bib137)]，通过将3D姿态估计形式化为反向扩散过程。
- en: 6.2 Pose tracking
  id: totrans-715
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 姿态跟踪
- en: Most pose tracking methods follow pose estimation and linking strategy, pose
    tracking performance highly depends on the results of pose estimation. Therefore,
    some challenges of pose estimation also exist in pose tracking, such as occlusion.
    Multi-view features fusion [[241](#bib.bib241)] is one method of eliminating unreliable
    appearances by occlusion for improving the results of pose linking. Linking every
    detection box rather than only high score detection boxes [[374](#bib.bib374)]
    is another method to make up non-negligible true poses by occlusion. In the following,
    we will present some more challenges for pose tracking.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数姿态跟踪方法遵循姿态估计和链接策略，姿态跟踪的性能高度依赖于姿态估计的结果。因此，姿态估计的一些挑战在姿态跟踪中也存在，例如遮挡。多视角特征融合[[241](#bib.bib241)]是通过遮挡消除不可靠外观以改进姿态链接结果的一种方法。链接每个检测框而不仅仅是高分检测框[[374](#bib.bib374)]是另一种方法，通过遮挡补偿不可忽视的真实姿态。接下来，我们将介绍一些姿态跟踪的其他挑战。
- en: (1) Multi-person pose tracking under multiple cameras
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 多摄像机下的多人的姿态跟踪
- en: The main challenge is how to fuse the scenes of different views. Although Voxteltrack [[241](#bib.bib241)]
    tends to fuse multi-view features fusion, it would be researched more. If scenes
    from non-overlapping cameras are fused and projected in a virtual world, poses
    can be tracked in a long area continuously.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 主要挑战是如何融合不同视角的场景。尽管Voxteltrack[[241](#bib.bib241)]倾向于融合多视角特征，但仍需进一步研究。如果将来自非重叠摄像机的场景融合并投射到虚拟世界中，可以在较长的区域内连续跟踪姿态。
- en: (2) Similar appearance and diverse motion
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 相似外观和多样运动
- en: To link poses across frames, the general solution is to measure the similarity
    between every pair of poses in neighboring frames based on appearance and motion.
    Persons sometimes have uniform appearance and diverse motions at the same time,
    such as group dancers, and sports players. They are highly similar and almost
    undistinguished in appearance by uniform clothes, and in complicated motion and
    interaction patterns. In this case, measuring the similarity is challenging. However,
    such poses with similar appearance can be easily distinguished by textual semantics.
    One possible solution is to incorporate some multi-modality pre-training models,
    such as Contrastive Language-Image Pre-training (CLIP) [[375](#bib.bib375)], for
    measuring similarity based on their semantic representation.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跨帧链接姿态，通常的解决方案是基于外观和运动测量相邻帧中每对姿态之间的相似性。人们有时在相同时间具有统一的外观和多样的运动，例如集体舞者和运动员。他们在外观上由于统一的衣物而高度相似，且在复杂的运动和互动模式中几乎无法区分。在这种情况下，测量相似性是具有挑战性的。然而，这种外观相似的姿态可以通过文本语义轻松区分。一个可能的解决方案是结合一些多模态预训练模型，如对比语言-图像预训练（CLIP）[[375](#bib.bib375)]，基于它们的语义表示来测量相似性。
- en: (3) Fast camera motion
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 快速摄像机运动
- en: Existing methods mainly address pose tracking by assuming slow camera motion.
    However, fast camera motion with ego-camera capturing is very often in real-world
    application. How to address egocentric pose tracking with fast camera motion is
    a challenging problem. Khirodkar et al. [[376](#bib.bib376)] proposed a new benchmark
    (EgoHumans) for egocentric pose estimation and tracking, and designed a multi-stream
    transformer to track multiple persons. Experiments have shown that there is still
    a gap between the performance of static and dynamic capture systems due to camera
    synchronization and calibration. More effort can be made to bridge the gap.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的方法主要通过假设相机运动缓慢来处理姿态跟踪。然而，现实世界应用中，快速相机运动与自我相机捕捉非常常见。如何处理具有快速相机运动的自我中心姿态跟踪是一个具有挑战性的问题。Khirodkar
    等人 [[376](#bib.bib376)] 提出了一个新的基准（EgoHumans）用于自我中心姿态估计和跟踪，并设计了一个多流变换器来跟踪多个人。实验表明，由于相机同步和校准的原因，静态和动态捕捉系统的性能之间仍存在差距。可以进一步努力弥合这一差距。
- en: 6.3 Action recognition
  id: totrans-723
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 动作识别
- en: With the rapid advancement of deep learning techniques, promise results have
    been achieved on large-scale action datasets. There are still some open questions
    as follows.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习技术的迅速发展，在大规模动作数据集上取得了有希望的结果。仍然存在一些未解的问题如下。
- en: (1) Computation complexity
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 计算复杂性
- en: 'According to the performance comparison (Table [14](#S5.T14 "Table 14 ‣ 5.3.1
    Datasets ‣ 5.3 Action recognition ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey")) of different methods,
    the method of integrating transformer with GCNs achieves the best accuracy. However,
    as mentioned before the computation required for a transformer and the amount
    of memory required increases on a quadratic scale with the number of tokens [[377](#bib.bib377)].
    Therefore, how to select significant tokens from video frames or skeletons is
    an open question for efficient transformer-based action recognition. Similar to
    transformer-based pose estimation, pruning tokens or discarding input matches [[378](#bib.bib378)]
    tend to reduce the cost. Moreover, integrating lightweight GCNs [[379](#bib.bib379)]
    can be further beneficial for efficiency.'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: '根据不同方法的性能比较（表 [14](#S5.T14 "Table 14 ‣ 5.3.1 Datasets ‣ 5.3 Action recognition
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey)），将变换器与GCNs整合的方法达到了最佳精度。然而，如前所述，变换器所需的计算量和内存需求随着标记数量的平方增长
    [[377](#bib.bib377)]。因此，如何从视频帧或骨骼中选择重要的标记是一个开放的问题，以实现高效的变换器基础动作识别。类似于变换器基础的姿态估计，修剪标记或丢弃输入匹配
    [[378](#bib.bib378)] 倾向于减少成本。此外，整合轻量级GCNs [[379](#bib.bib379)] 也可能进一步有利于提高效率。'
- en: (2) Zero-shot learning on skeletons
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 骨骼的零样本学习
- en: Annotating and labeling large-amount data is expensive, and zero-shot learning
    is desirable in real-world applications. Existing zero-shot action recognition
    methods mainly apply RGB data as the input. However, skeleton data has become
    a promising alternative to RGB data due to its robustness to variations in appearance
    and background. Therefore, zero-shot skeleton-based action recognition is more
    desirable. Few methods [[380](#bib.bib380), [381](#bib.bib381)] were proposed
    to learn a mapping between skeletons and word embedding of class labels. Class
    labels may possess less semantics than textual descriptions which are natural
    languages for describing how an action is performed. In the future, new methods
    can be pursued based on textual descriptions for zero-shot skeleton-based action
    recognition.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 注释和标记大量数据是昂贵的，而在现实应用中，零样本学习是很有希望的。现有的零样本动作识别方法主要使用RGB数据作为输入。然而，由于骨骼数据对外观和背景变化的鲁棒性，骨骼数据已成为RGB数据的有前景的替代方案。因此，零样本基于骨骼的动作识别更具吸引力。一些方法
    [[380](#bib.bib380), [381](#bib.bib381)] 被提出用于学习骨骼与类别标签的词嵌入之间的映射。类别标签可能比文本描述具有更少的语义，因为文本描述是自然语言，用于描述动作的执行方式。在未来，可以基于文本描述寻求新的方法用于零样本基于骨骼的动作识别。
- en: (3) Multi-modality fusion
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 多模态融合
- en: Estimated pose-based methods take RGB data as the input and recognize actions
    based on RGB and estimated skeletons. Moreover, text data can guide improving
    the performance of visually similar actions and zero-shot learning, which is another
    modality for action recognition. Due to the heterogeneity of different modalities,
    how to fully utilize them deserves to be further explored by researchers. Although
    some methods [[255](#bib.bib255)] tend to propose a particular model for fusing
    different modalities, such model lacks of generalization. In the future, a universal
    fusing method regardless of models is a better option.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 基于估计姿态的方法以RGB数据作为输入，并根据RGB和估计的骨架识别动作。此外，文本数据可以指导提升视觉上相似动作和零样本学习的性能，这是动作识别的另一种模态。由于不同模态的异质性，如何充分利用它们值得研究人员进一步探索。尽管一些方法[[255](#bib.bib255)]倾向于提出特定的模型来融合不同的模态，但这种模型缺乏泛化性。未来，无论模型如何，一种通用的融合方法是更好的选择。
- en: 6.4 Unified models
  id: totrans-731
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 统一模型
- en: 'As reviewed in Section [4.1](#S4.SS1 "4.1 Estimated pose-based action recognition
    ‣ 4 Action Recognition ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey"), some methods tend to conduct action recognition
    based on results of pose estimation or tracking. Table [13](#S5.T13 "Table 13
    ‣ 5.3.1 Datasets ‣ 5.3 Action recognition ‣ 5 Benchmark datasets ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") further
    demonstrates pose estimation and tracking can improve action recognition performance.
    These observations emphasize these three tasks are closely related together, which
    provides a direction for designing unified models for solving three tasks. Recently,
    a unified model (UPS [[259](#bib.bib259)]) has been proposed for 3D video-based
    pose estimation and estimated poses-based action recognition, however, their performance
    is largely lower than the ones of separate models. Hence, more unified models
    are preferable for jointly solving these three tasks.'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[4.1节](#S4.SS1 "4.1 基于姿态的动作识别 ‣ 4 动作识别 ‣ 基于深度学习的人体姿态估计、跟踪与动作识别的调查")中回顾的，一些方法倾向于基于姿态估计或跟踪的结果进行动作识别。表[13](#S5.T13
    "表 13 ‣ 5.3.1 数据集 ‣ 5.3 动作识别 ‣ 5 基准数据集 ‣ 基于深度学习的人体姿态估计、跟踪与动作识别的调查")进一步展示了姿态估计和跟踪可以提升动作识别性能。这些观察结果强调了这三项任务之间的紧密关系，这为设计统一模型解决这三项任务提供了方向。最近，已经提出了一种统一模型
    (UPS [[259](#bib.bib259)]) 用于3D视频姿态估计和基于估计姿态的动作识别，但其性能远低于单独模型。因此，更加统一的模型对于共同解决这三项任务是更可取的。
- en: 7 Conclusion
  id: totrans-733
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This survey has presented a systematic overview of recent works about human
    pose-based estimation, tracking and action recognition with deep learning. We
    have reviewed pose estimation approaches from 2D to 3D, from single-person to
    multi-person, and from images to videos. After estimating poses, we summarized
    the methods of linking poses across frames for tracking poses. Pose-based action
    recognition approaches have been also reviewed which are taken as the application
    of pose estimation and tracking. For each task, we have reviewed different categories
    of methods and discussed their advantages and disadvantages. Meanwhile, end-to-end
    methods were highlighted for jointly conducting pose estimation, tracking and
    action recognition in the category of estimated pose-based action recognition.
    Commonly used datasets have been reviewed and performance comparisons of different
    methods have been covered to further demonstrate the benefits of some methods.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查提供了关于基于人体姿态的估计、跟踪和动作识别的深度学习的最新工作的系统概述。我们回顾了从2D到3D，从单人到多人，从图像到视频的姿态估计方法。在估计姿态之后，我们总结了跨帧链接姿态以进行姿态跟踪的方法。也回顾了基于姿态的动作识别方法，这些方法作为姿态估计和跟踪的应用。对于每项任务，我们回顾了不同类别的方法，并讨论了它们的优缺点。同时，在估计姿态基础上的动作识别类别中，我们突出了端到端的方法，以共同进行姿态估计、跟踪和动作识别。常用的数据集已被回顾，不同方法的性能比较也已涵盖，以进一步展示一些方法的优势。
- en: Based on the strengths and weaknesses of the existing works, we point out a
    few promising future directions. For pose estimation, more effort can be made
    on pose estimation with occlusion, low resolution, limited data with uncommon
    poses and balancing the performance with computation complexity. Multi-person
    pose tracking can be further resolved under multiple cameras, similar appearance,
    diverse motions and fast camera motion. Zero-shot learning on skeletons and multi-modality
    fusion can be also further explored for action recognition.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 基于现有工作的优缺点，我们指出了几个有前景的未来方向。对于姿态估计，可以更多地关注遮挡、低分辨率、数据有限且姿态不常见的情况，以及在性能与计算复杂性之间的平衡。多人的姿态跟踪可以在多摄像头、相似外观、多样动作和快速摄像机运动的情况下进一步解决。零样本学习骨架和多模态融合也可以在动作识别中进一步探索。
- en: \bmhead
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: \bmhead
- en: Acknowledgements This work is supported by the National Natural Science Foundation
    of China (Grant No. 62006211, 61502491) and China Postdoctoral Science Foundation
    (Grant No. 2019TQ0286, 2020M682349).
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: Acknowledgements 本工作得到中国国家自然科学基金（资助号：62006211, 61502491）和中国博士后科学基金（资助号：2019TQ0286,
    2020M682349）的支持。
- en: References
  id: totrans-738
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: \bibcommenthead
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \bibcommenthead
- en: 'Gavrila [1999] Gavrila, D.M.: The visual analysis of human movement: A survey.
    CVIU 73(1), 82–98 (1999)'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gavrila [1999] Gavrila, D.M.：《人类运动的视觉分析：综述》。CVIU 73(1), 82–98 (1999)
- en: 'Aggarwal and Cai [1999] Aggarwal, J.K., Cai, Q.: Human motion analysis: A review.
    CVIU 73(3), 428–440 (1999)'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aggarwal and Cai [1999] Aggarwal, J.K., Cai, Q.：《人类运动分析：综述》。CVIU 73(3), 428–440
    (1999)
- en: 'Moeslund and Granum [2001] Moeslund, T.B., Granum, E.: A survey of computer
    vision-based human motion capture. CVIU 81(3), 231–268 (2001)'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moeslund and Granum [2001] Moeslund, T.B., Granum, E.：《基于计算机视觉的人类运动捕捉综述》。CVIU
    81(3), 231–268 (2001)
- en: 'Wang et al. [2003] Wang, L., Hu, W., Tan, T.: Recent developments in human
    motion analysis. PR 36(3), 585–601 (2003)'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2003] Wang, L., Hu, W., Tan, T.：《人类运动分析的最新进展》。PR 36(3), 585–601
    (2003)
- en: 'Moeslund et al. [2006] Moeslund, T.B., Hilton, A., Krüger, V.: A survey of
    advances in vision-based human motion capture and analysis. CVIU 104(2-3), 90–126
    (2006)'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moeslund et al. [2006] Moeslund, T.B., Hilton, A., Krüger, V.：《基于视觉的人类运动捕捉和分析进展综述》。CVIU
    104(2-3), 90–126 (2006)
- en: 'Poppe [2007] Poppe, R.: Vision-based human motion analysis: An overview. CVIU
    108(1-2), 4–18 (2007)'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poppe [2007] Poppe, R.：《基于视觉的人类运动分析：概述》。CVIU 108(1-2), 4–18 (2007)
- en: 'Sminchisescu [2008] Sminchisescu, C.: 3d human motion analysis in monocular
    video: techniques and challenges. Human Motion: Understanding, Modelling, Capture,
    and Animation, 185–211 (2008)'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sminchisescu [2008] Sminchisescu, C.：《单目视频中的3D人类运动分析：技术与挑战》。人类运动：理解、建模、捕捉和动画，185–211
    (2008)
- en: 'Ji and Liu [2009] Ji, X., Liu, H.: Advances in view-invariant human motion
    analysis: a review. IEEE Transactions on Systems, Man, and Cybernetics 40(1),
    13–24 (2009)'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji and Liu [2009] Ji, X., Liu, H.：《视角不变的人类运动分析进展：综述》。IEEE Transactions on Systems,
    Man, and Cybernetics 40(1), 13–24 (2009)
- en: 'Moeslund et al. [2011] Moeslund, T.B., Hilton, A., Krüger, V., Sigal, L.: Visual
    Analysis of Humans. Springer, ??? (2011)'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moeslund et al. [2011] Moeslund, T.B., Hilton, A., Krüger, V., Sigal, L.：《人类的视觉分析》。Springer，???
    (2011)
- en: 'Liu et al. [2015] Liu, Z., Zhu, J., Bu, J., Chen, C.: A survey of human pose
    estimation: The body parts parsing based methods. JVCIR 32, 10–19 (2015)'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2015] Liu, Z., Zhu, J., Bu, J., Chen, C.：《人类姿态估计综述：基于身体部位解析的方法》。JVCIR
    32, 10–19 (2015)
- en: 'Sarafianos et al. [2016] Sarafianos, N., Boteanu, B., Ionescu, B., Kakadiaris,
    I.A.: 3d human pose estimation: A review of the literature and analysis of covariates.
    CVIU 152, 1–20 (2016)'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarafianos et al. [2016] Sarafianos, N., Boteanu, B., Ionescu, B., Kakadiaris,
    I.A.：《3D人类姿态估计：文献综述与协变量分析》。CVIU 152, 1–20 (2016)
- en: 'Yilmaz et al. [2006] Yilmaz, A., Javed, O., Shah, M.: Object tracking: A survey.
    ACM CSUR 38(4), 13 (2006)'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yilmaz et al. [2006] Yilmaz, A., Javed, O., Shah, M.：《物体跟踪：综述》。ACM CSUR 38(4),
    13 (2006)
- en: 'Watada et al. [2010] Watada, J., Musa, Z., Jain, L.C., Fulcher, J.: Human tracking:
    A state-of-art survey. In: International Conference on Knowledge-Based and Intelligent
    Information and Engineering Systems, pp. 454–463 (2010)'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watada et al. [2010] Watada, J., Musa, Z., Jain, L.C., Fulcher, J.：《人类跟踪：前沿综述》。在：国际知识型和智能信息与工程系统会议，页码
    454–463 (2010)
- en: 'Salti et al. [2012] Salti, S., Cavallaro, A., Di Stefano, L.: Adaptive appearance
    modeling for video tracking: Survey and evaluation. TIP 21(10), 4334–4348 (2012)'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salti et al. [2012] Salti, S., Cavallaro, A., Di Stefano, L.：《用于视频跟踪的自适应外观建模：综述与评估》。TIP
    21(10), 4334–4348 (2012)
- en: 'Smeulders et al. [2013] Smeulders, A.W., Chu, D.M., Cucchiara, R., Calderara,
    S., Dehghan, A., Shah, M.: Visual tracking: An experimental survey. TPAMI 36(7),
    1442–1468 (2013)'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smeulders et al. [2013] Smeulders, A.W., Chu, D.M., Cucchiara, R., Calderara,
    S., Dehghan, A., Shah, M.：《视觉跟踪：实验综述》。TPAMI 36(7), 1442–1468 (2013)
- en: 'Wu et al. [2015] Wu, Y., Lim, J., Yang, M.-H.: Object tracking benchmark. TPAMI
    37(9), 1834–1848 (2015)'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等 [2015] Wu, Y., Lim, J., Yang, M.-H.: 对象跟踪基准。TPAMI 37(9), 1834–1848 (2015)'
- en: 'Cedras and Shah [1995] Cedras, C., Shah, M.: Motion-based recognition a survey.
    IVT 13(2), 129–155 (1995)'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cedras 和 Shah [1995] Cedras, C., Shah, M.: 基于运动的识别综述。IVT 13(2), 129–155 (1995)'
- en: 'Turaga et al. [2008] Turaga, P., Chellappa, R., Subrahmanian, V.S., Udrea,
    O.: Machine recognition of human activities: A survey. TCSVT 18(11), 1473 (2008)'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Turaga 等 [2008] Turaga, P., Chellappa, R., Subrahmanian, V.S., Udrea, O.: 人体活动的机器识别：综述。TCSVT
    18(11), 1473 (2008)'
- en: 'Poppe [2010] Poppe, R.: A survey on vision-based human action recognition.
    IVT 28(6), 976–990 (2010)'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Poppe [2010] Poppe, R.: 基于视觉的人体动作识别综述。IVT 28(6), 976–990 (2010)'
- en: 'Guo and Lai [2014] Guo, G., Lai, A.: A survey on still image based human action
    recognition. PR 47(10), 3343–3361 (2014)'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 和 Lai [2014] Guo, G., Lai, A.: 基于静态图像的人体动作识别综述。PR 47(10), 3343–3361 (2014)'
- en: 'Zhu et al. [2016] Zhu, F., Shao, L., Xie, J., Fang, Y.: From handcrafted to
    learned representations for human action recognition: a survey. IVT 55, 42–52
    (2016)'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等 [2016] Zhu, F., Shao, L., Xie, J., Fang, Y.: 从手工特征到学习特征的人体动作识别：综述。IVT
    55, 42–52 (2016)'
- en: 'Wang et al. [2018] Wang, P., Li, W., Ogunbona, P., Wan, J., Escalera, S.: Rgb-d-based
    human motion recognition with deep learning: A survey. CVIU 171, 118–139 (2018)'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2018] Wang, P., Li, W., Ogunbona, P., Wan, J., Escalera, S.: 基于RGB-D的深度学习人体动作识别：综述。CVIU
    171, 118–139 (2018)'
- en: 'Chen et al. [2020] Chen, Y., Tian, Y., He, M.: Monocular human pose estimation:
    A survey of deep learning-based methods. Computer vision and image understanding
    192, 102897 (2020)'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2020] Chen, Y., Tian, Y., He, M.: 单目人体姿态估计：基于深度学习的方法综述。计算机视觉与图像理解 192,
    102897 (2020)'
- en: 'Liu et al. [2022] Liu, W., Bao, Q., Sun, Y., Mei, T.: Recent advances of monocular
    2d and 3d human pose estimation: A deep learning perspective. ACM Computing Surveys
    55(4), 1–41 (2022)'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2022] Liu, W., Bao, Q., Sun, Y., Mei, T.: 单目2D和3D人体姿态估计的近期进展：深度学习视角。ACM
    Computing Surveys 55(4), 1–41 (2022)'
- en: 'Sun et al. [2022] Sun, Z., Ke, Q., Rahmani, H., Bennamoun, M., Wang, G., Liu,
    J.: Human action recognition from various data modalities: A review. TPAMI (2022)'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 [2022] Sun, Z., Ke, Q., Rahmani, H., Bennamoun, M., Wang, G., Liu, J.:
    来自各种数据模态的人体动作识别：综述。TPAMI (2022)'
- en: 'Zheng et al. [2023] Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J.,
    Kehtarnavaz, N., Shah, M.: Deep learning-based human pose estimation: A survey.
    ACM Computing Surveys 56(1), 1–37 (2023)'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng 等 [2023] Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J., Kehtarnavaz,
    N., Shah, M.: 基于深度学习的人体姿态估计：综述。ACM Computing Surveys 56(1), 1–37 (2023)'
- en: 'Xin et al. [2023] Xin, W., Liu, R., Liu, Y., Chen, Y., Yu, W., Miao, Q.: Transformer
    for skeleton-based action recognition: A review of recent advances. Neurocomputing
    (2023)'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xin 等 [2023] Xin, W., Liu, R., Liu, Y., Chen, Y., Yu, W., Miao, Q.: 基于骨架的动作识别的变换器：近期进展综述。Neurocomputing
    (2023)'
- en: 'Rajasegaran et al. [2023] Rajasegaran, J., Pavlakos, G., Kanazawa, A., Feichtenhofer,
    C., Malik, J.: On the benefits of 3d pose and tracking for human action recognition.
    In: CVPR, pp. 640–649 (2023)'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajasegaran 等 [2023] Rajasegaran, J., Pavlakos, G., Kanazawa, A., Feichtenhofer,
    C., Malik, J.: 3D姿态和跟踪对人体动作识别的益处。发表于 CVPR, pp. 640–649 (2023)'
- en: 'Choudhury et al. [2023] Choudhury, R., Kitani, K., Jeni, L.A.: TEMPO: Efficient
    multi-view pose estimation, tracking, and forecasting. In: ICCV, pp. 14750–14760
    (2023)'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choudhury 等 [2023] Choudhury, R., Kitani, K., Jeni, L.A.: TEMPO: 高效的多视角姿态估计、跟踪和预测。发表于
    ICCV, pp. 14750–14760 (2023)'
- en: 'Toshev and Szegedy [2014] Toshev, A., Szegedy, C.: Deeppose: Human pose estimation
    via deep neural networks. In: CVPR, pp. 1653–1660 (2014)'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Toshev 和 Szegedy [2014] Toshev, A., Szegedy, C.: Deeppose: 通过深度神经网络进行人体姿态估计。发表于
    CVPR, pp. 1653–1660 (2014)'
- en: 'Carreira et al. [2016] Carreira, J., Agrawal, P., Fragkiadaki, K., Malik, J.:
    Human pose estimation with iterative error feedback. In: CVPR, pp. 4733–4742 (2016)'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carreira 等 [2016] Carreira, J., Agrawal, P., Fragkiadaki, K., Malik, J.: 使用迭代误差反馈进行人体姿态估计。发表于
    CVPR, pp. 4733–4742 (2016)'
- en: 'Sun et al. [2017] Sun, X., Shang, J., Liang, S., Wei, Y.: Compositional human
    pose regression. In: ICCV, pp. 2602–2611 (2017)'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 [2017] Sun, X., Shang, J., Liang, S., Wei, Y.: 组合式人体姿态回归。发表于 ICCV, pp.
    2602–2611 (2017)'
- en: 'Luvizon et al. [2019] Luvizon, D.C., Tabia, H., Picard, D.: Human pose regression
    by combining indirect part detection and contextual information. Computers & Graphics
    85, 15–22 (2019)'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luvizon 等 [2019] Luvizon, D.C., Tabia, H., Picard, D.: 通过结合间接部件检测和上下文信息进行人体姿态回归。Computers
    & Graphics 85, 15–22 (2019)'
- en: 'Mao et al. [2021] Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z.:
    Tfpose: Direct human pose estimation with transformers. arXiv preprint arXiv:2103.15320
    (2021)'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mao 等 [2021] Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z.: Tfpose:
    基于变换器的直接人体姿态估计。arXiv 预印本 arXiv:2103.15320 (2021)'
- en: 'Li et al. [2021] Li, K., Wang, S., Zhang, X., Xu, Y., Xu, W., Tu, Z.: Pose
    recognition with cascade transformers. In: CVPR, pp. 1944–1953 (2021)'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2021] Li, K., Wang, S., Zhang, X., Xu, Y., Xu, W., Tu, Z.: 使用级联变压器的姿态识别。见：CVPR，第
    1944–1953 页 (2021)'
- en: 'Mao et al. [2022] Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z.,
    Hengel, A.v.: Poseur: Direct human pose regression with transformers. In: ECCV,
    pp. 72–88 (2022)'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mao 等人 [2022] Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z., Hengel,
    A.v.: Poseur: 使用变压器的直接人体姿态回归。见：ECCV，第 72–88 页 (2022)'
- en: 'Panteleris and Argyros [2022] Panteleris, P., Argyros, A.: Pe-former: Pose
    estimation transformer. In: ICPRAI, pp. 3–14 (2022)'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Panteleris 和 Argyros [2022] Panteleris, P., Argyros, A.: Pe-former: 姿态估计变压器。见：ICPRAI，第
    3–14 页 (2022)'
- en: 'Jain et al. [2014] Jain, A., Tompson, J., Andriluka, M., Taylor, G.W., Bregler,
    C.: Learning human pose estimation features with convolutional networks. In: ICLR
    (2014)'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jain 等人 [2014] Jain, A., Tompson, J., Andriluka, M., Taylor, G.W., Bregler,
    C.: 使用卷积网络学习人体姿态估计特征。见：ICLR (2014)'
- en: 'Tompson et al. [2014] Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint
    training of a convolutional network and a graphical model for human pose estimation.
    In: NIPS, pp. 1799–1807 (2014)'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tompson 等人 [2014] Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: 卷积网络和图模型联合训练用于人体姿态估计。见：NIPS，第
    1799–1807 页 (2014)'
- en: 'Chen and Yuille [2014] Chen, X., Yuille, A.L.: Articulated pose estimation
    by a graphical model with image dependent pairwise relations. In: NIPS, pp. 1736–1744
    (2014)'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 和 Yuille [2014] Chen, X., Yuille, A.L.: 通过图形模型和图像依赖的配对关系进行关节姿态估计。见：NIPS，第
    1736–1744 页 (2014)'
- en: 'Tompson et al. [2015] Tompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler,
    C.: Efficient object localization using convolutional networks. In: CVPR, pp.
    648–656 (2015)'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tompson 等人 [2015] Tompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler,
    C.: 使用卷积网络的高效物体定位。见：CVPR，第 648–656 页 (2015)'
- en: 'Wei et al. [2016] Wei, S.-E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional
    pose machines. In: CVPR, pp. 4724–4732 (2016)'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等人 [2016] Wei, S.-E., Ramakrishna, V., Kanade, T., Sheikh, Y.: 卷积姿态网络。见：CVPR，第
    4724–4732 页 (2016)'
- en: 'Hu and Ramanan [2016] Hu, P., Ramanan, D.: Bottom-up and top-down reasoning
    with hierarchical rectified gaussians. In: CVPR, pp. 5600–5609 (2016)'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 和 Ramanan [2016] Hu, P., Ramanan, D.: 使用分层修正高斯的自下而上和自上而下推理。见：CVPR，第 5600–5609
    页 (2016)'
- en: 'Newell et al. [2016] Newell, A., Yang, K., Deng, J.: Stacked hourglass networks
    for human pose estimation. In: ECCV, pp. 483–499 (2016)'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Newell 等人 [2016] Newell, A., Yang, K., Deng, J.: 堆叠沙漏网络用于人体姿态估计。见：ECCV，第 483–499
    页 (2016)'
- en: 'Bulat and Tzimiropoulos [2016] Bulat, A., Tzimiropoulos, G.: Human pose estimation
    via convolutional part heatmap regression. In: ECCV, pp. 717–732 (2016)'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bulat 和 Tzimiropoulos [2016] Bulat, A., Tzimiropoulos, G.: 通过卷积部位热图回归进行人体姿态估计。见：ECCV，第
    717–732 页 (2016)'
- en: 'Lifshitz et al. [2016] Lifshitz, I., Fetaya, E., Ullman, S.: Human pose estimation
    using deep consensus voting. In: ECCV, pp. 246–260 (2016)'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lifshitz 等人 [2016] Lifshitz, I., Fetaya, E., Ullman, S.: 使用深度一致性投票进行人体姿态估计。见：ECCV，第
    246–260 页 (2016)'
- en: 'Chu et al. [2017] Chu, X., Yang, W., Ouyang, W., Ma, C., Yuille, A.L., Wang,
    X.: Multi-context attention for human pose estimation. In: CVPR, pp. 1831–1840
    (2017)'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chu 等人 [2017] Chu, X., Yang, W., Ouyang, W., Ma, C., Yuille, A.L., Wang, X.:
    多上下文注意力用于人体姿态估计。见：CVPR，第 1831–1840 页 (2017)'
- en: 'Yang et al. [2017] Yang, W., Li, S., Ouyang, W., Li, H., Wang, X.: Learning
    feature pyramids for human pose estimation. In: ICCV, pp. 1281–1290 (2017)'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 [2017] Yang, W., Li, S., Ouyang, W., Li, H., Wang, X.: 学习特征金字塔用于人体姿态估计。见：ICCV，第
    1281–1290 页 (2017)'
- en: 'Goodfellow et al. [2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial
    nets. In: NIPS, pp. 2672–2680 (2014)'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goodfellow 等人 [2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: 生成对抗网络。见：NIPS，第 2672–2680
    页 (2014)'
- en: 'Chen et al. [2017] Chen, Y., Shen, C., Wei, X.-S., Liu, L., Yang, J.: Adversarial
    posenet: A structure-aware convolutional network for human pose estimation. In:
    ICCV, pp. 1212–1221 (2017)'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2017] Chen, Y., Shen, C., Wei, X.-S., Liu, L., Yang, J.: 对抗性姿态网络：一种结构感知的卷积网络用于人体姿态估计。见：ICCV，第
    1212–1221 页 (2017)'
- en: 'Ning et al. [2017] Ning, G., Zhang, Z., He, Z.: Knowledge-guided deep fractal
    neural networks for human pose estimation. IEEE Transactions on Multimedia 20(5),
    1246–1259 (2017)'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ning 等人 [2017] Ning, G., Zhang, Z., He, Z.: 知识引导的深度分形神经网络用于人体姿态估计。IEEE Multimedia
    Transactions 20(5), 1246–1259 (2017)'
- en: 'Sun et al. [2017] Sun, K., Lan, C., Xing, J., Zeng, W., Liu, D., Wang, J.:
    Human pose estimation using global and local normalization. In: ICCV, pp. 5599–5607
    (2017)'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人 [2017] Sun, K., Lan, C., Xing, J., Zeng, W., Liu, D., Wang, J.: 使用全局和局部归一化进行人体姿态估计。见：ICCV，第
    5599–5607 页 (2017)'
- en: 'Marras et al. [2017] Marras, I., Palasek, P., Patras, I.: Deep globally constrained
    mrfs for human pose estimation. In: ICCV, pp. 3466–3475 (2017)'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marras 等 [2017] Marras, I., Palasek, P., Patras, I.: 深度全局约束的 MRFs 用于人体姿态估计。载于：ICCV，第3466–3475页（2017）'
- en: 'Liu and Ferrari [2017] Liu, B., Ferrari, V.: Active learning for human pose
    estimation. In: ICCV, pp. 4363–4372 (2017)'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 和 Ferrari [2017] Liu, B., Ferrari, V.: 用于人体姿态估计的主动学习。载于：ICCV，第4363–4372页（2017）'
- en: 'Ke et al. [2018] Ke, L., Chang, M.-C., Qi, H., Lyu, S.: Multi-scale structure-aware
    network for human pose estimation. In: ECCV, pp. 713–728 (2018)'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ke 等 [2018] Ke, L., Chang, M.-C., Qi, H., Lyu, S.: 用于人体姿态估计的多尺度结构感知网络。载于：ECCV，第713–728页（2018）'
- en: 'Peng et al. [2018] Peng, X., Tang, Z., Yang, F., Feris, R.S., Metaxas, D.:
    Jointly optimize data augmentation and network training: Adversarial data augmentation
    in human pose estimation. In: CVPR, pp. 2226–2234 (2018)'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等 [2018] Peng, X., Tang, Z., Yang, F., Feris, R.S., Metaxas, D.: 联合优化数据增强和网络训练：对抗性数据增强在人类姿态估计中的应用。载于：CVPR，第2226–2234页（2018）'
- en: 'Tang et al. [2018] Tang, W., Yu, P., Wu, Y.: Deeply learned compositional models
    for human pose estimation. In: ECCV, pp. 190–206 (2018)'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang 等 [2018] Tang, W., Yu, P., Wu, Y.: 深度学习的组合模型用于人体姿态估计。载于：ECCV，第190–206页（2018）'
- en: 'Nie et al. [2018a] Nie, X., Feng, J., Zuo, Y., Yan, S.: Human pose estimation
    with parsing induced learner. In: CVPR, pp. 2100–2108 (2018)'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nie 等 [2018a] Nie, X., Feng, J., Zuo, Y., Yan, S.: 使用解析引导学习器的人体姿态估计。载于：CVPR，第2100–2108页（2018）'
- en: 'Nie et al. [2018b] Nie, X., Feng, J., Yan, S.: Mutual learning to adapt for
    joint human parsing and pose estimation. In: ECCV, pp. 502–517 (2018)'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nie 等 [2018b] Nie, X., Feng, J., Yan, S.: 互助学习以适应联合人体解析和姿态估计。载于：ECCV，第502–517页（2018）'
- en: 'Tang and Wu [2019] Tang, W., Wu, Y.: Does learning specific features for related
    parts help human pose estimation? In: CVPR, pp. 1107–1116 (2019)'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang 和 Wu [2019] Tang, W., Wu, Y.: 学习特定特征以帮助人体姿态估计是否有效？载于：CVPR，第1107–1116页（2019）'
- en: 'Zhang et al. [2019] Zhang, F., Zhu, X., Ye, M.: Fast human pose estimation.
    In: CVPR, pp. 3517–3526 (2019)'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2019] Zhang, F., Zhu, X., Ye, M.: 快速人体姿态估计。载于：CVPR，第3517–3526页（2019）'
- en: 'Li et al. [2022] Li, Y., Yang, S., Liu, P., Zhang, S., Wang, Y., Wang, Z.,
    Yang, W., Xia, S.-T.: SimCC: A simple coordinate classification perspective for
    human pose estimation. In: ECCV, pp. 89–106 (2022)'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 [2022] Li, Y., Yang, S., Liu, P., Zhang, S., Wang, Y., Wang, Z., Yang,
    W., Xia, S.-T.: SimCC: 一种简单的坐标分类视角用于人体姿态估计。载于：ECCV，第89–106页（2022）'
- en: 'Li et al. [2021] Li, J., Bian, S., Zeng, A., Wang, C., Pang, B., Liu, W., Lu,
    C.: Human pose regression with residual log-likelihood estimation. In: ICCV, pp.
    11025–11034 (2021)'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 [2021] Li, J., Bian, S., Zeng, A., Wang, C., Pang, B., Liu, W., Lu, C.:
    带残差对数似然估计的人体姿态回归。载于：ICCV，第11025–11034页（2021）'
- en: 'Ye et al. [2023] Ye, S., Zhang, Y., Hu, J., Cao, L., Zhang, S., Shen, L., Wang,
    J., Ding, S., Ji, R.: Distilpose: Tokenized pose regression with heatmap distillation.
    In: CVPR, pp. 2163–2172 (2023)'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye 等 [2023] Ye, S., Zhang, Y., Hu, J., Cao, L., Zhang, S., Shen, L., Wang,
    J., Ding, S., Ji, R.: DistilPose: 使用热图蒸馏的令牌化姿态回归。载于：CVPR，第2163–2172页（2023）'
- en: 'Yang et al. [2023] Yang, J., Zeng, A., Liu, S., Li, F., Zhang, R., Zhang, L.:
    Explicit box detection unifies end-to-end multi-person pose estimation. In: ICLR
    (2023)'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等 [2023] Yang, J., Zeng, A., Liu, S., Li, F., Zhang, R., Zhang, L.: 显式框检测统一了端到端的多人体姿态估计。载于：ICLR（2023）'
- en: 'Zheng et al. [2020] Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J.,
    Kehtarnavaz, N., Shah, M.: Deep learning-based human pose estimation: A survey.
    ACM Computing Surveys (2020)'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng 等 [2020] Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J., Kehtarnavaz,
    N., Shah, M.: 基于深度学习的人体姿态估计：综述。ACM Computing Surveys（2020）'
- en: 'Papandreou et al. [2017] Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A.,
    Tompson, J., Bregler, C., Murphy, K.: Towards accurate multi-person pose estimation
    in the wild. In: CVPR, pp. 4903–4911 (2017)'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papandreou 等 [2017] Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson,
    J., Bregler, C., Murphy, K.: 朝向准确的野外多人体姿态估计。载于：CVPR，第4903–4911页（2017）'
- en: 'He et al. [2017] He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn.
    In: ICCV, pp. 2961–2969 (2017)'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等 [2017] He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN。载于：ICCV，第2961–2969页（2017）'
- en: 'Xiao et al. [2018] Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose
    estimation and tracking. In: ECCV, pp. 466–481 (2018)'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等 [2018] Xiao, B., Wu, H., Wei, Y.: 简单的基线用于人体姿态估计和跟踪。载于：ECCV，第466–481页（2018）'
- en: 'Moon et al. [2019] Moon, G., Chang, J.Y., Lee, K.M.: Posefix: Model-agnostic
    general human pose refinement network. In: CVPR, pp. 7773–7781 (2019)'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moon 等 [2019] Moon, G., Chang, J.Y., Lee, K.M.: PoseFix: 模型无关的通用人体姿态优化网络。载于：CVPR，第7773–7781页（2019）'
- en: 'Sun et al. [2019] Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution
    representation learning for human pose estimation. In: CVPR, pp. 5693–5703 (2019)'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 [2019] Sun, K., Xiao, B., Liu, D., Wang, J.: 深度高分辨率表示学习用于人体姿态估计。载于：CVPR，第5693–5703页（2019）'
- en: 'Cai et al. [2020] Cai, Y., Wang, Z., Luo, Z., Yin, B., Du, A., Wang, H., Zhang,
    X., Zhou, X., Zhou, E., Sun, J.: Learning delicate local representations for multi-person
    pose estimation. In: ECCV, pp. 455–472 (2020)'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等 [2020] Cai, Y., Wang, Z., Luo, Z., Yin, B., Du, A., Wang, H., Zhang, X.,
    Zhou, X., Zhou, E., Sun, J.：为多人人体姿态估计学习精细的局部表示。发表于：ECCV，第455–472页（2020年）
- en: 'Huang et al. [2020] Huang, J., Zhu, Z., Guo, F., Huang, G.: The devil is in
    the details: Delving into unbiased data processing for human pose estimation.
    In: CVPR, pp. 5700–5709 (2020)'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 [2020] Huang, J., Zhu, Z., Guo, F., Huang, G.：魔鬼在细节中：深入研究人体姿态估计中的无偏数据处理。发表于：CVPR，第5700–5709页（2020年）
- en: 'Zhang et al. [2020] Zhang, F., Zhu, X., Dai, H., Ye, M., Zhu, C.: Distribution-aware
    coordinate representation for human pose estimation. In: CVPR, pp. 7093–7102 (2020)'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2020] Zhang, F., Zhu, X., Dai, H., Ye, M., Zhu, C.：分布感知坐标表示用于人体姿态估计。发表于：CVPR，第7093–7102页（2020年）
- en: 'Wang et al. [2020] Wang, J., Long, X., Gao, Y., Ding, E., Wen, S.: Graph-pcnn:
    Two stage human pose estimation with graph pose refinement. In: ECCV, pp. 492–508
    (2020)'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2020] Wang, J., Long, X., Gao, Y., Ding, E., Wen, S.：Graph-pcnn：通过图形姿态细化的两阶段人体姿态估计。发表于：ECCV，第492–508页（2020年）
- en: 'Xu et al. [2022] Xu, X., Zou, Q., Lin, X.: Adaptive hypergraph neural network
    for multi-person pose estimation. In: AAAI, pp. 2955–2963 (2022)'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 [2022] Xu, X., Zou, Q., Lin, X.：适应性超图神经网络用于多人人体姿态估计。发表于：AAAI，第2955–2963页（2022年）
- en: 'Jiang et al. [2023] Jiang, C., Huang, K., Zhang, S., Wang, X., Xiao, J., Goulermas,
    Y.: Aggregated pyramid gating network for human pose estimation without pre-training.
    PR 138, 109429 (2023)'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 [2023] Jiang, C., Huang, K., Zhang, S., Wang, X., Xiao, J., Goulermas,
    Y.：无预训练的人体姿态估计的聚合金字塔门控网络。PR 138，第109429页（2023年）
- en: 'Gu et al. [2023] Gu, K., Yang, L., Mi, M.B., Yao, A.: Bias-compensated integral
    regression for human pose estimation. TPAMI 45(9), 10687–10702 (2023)'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 [2023] Gu, K., Yang, L., Mi, M.B., Yao, A.：偏差补偿积分回归用于人体姿态估计。TPAMI 45(9)，第10687–10702页（2023年）
- en: 'Ren et al. [2015] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards
    real-time object detection with region proposal networks. In: NIPS, pp. 91–99
    (2015)'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等 [2015] Ren, S., He, K., Girshick, R., Sun, J.：Faster r-cnn：面向实时目标检测的区域提议网络。发表于：NIPS，第91–99页（2015年）
- en: 'Iqbal and Gall [2016] Iqbal, U., Gall, J.: Multi-person pose estimation with
    local joint-to-person associations. In: ECCV, pp. 627–642 (2016)'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iqbal 和 Gall [2016] Iqbal, U., Gall, J.：具有局部关节到人体关联的多人人体姿态估计。发表于：ECCV，第627–642页（2016年）
- en: 'Fang et al. [2017] Fang, H.-S., Xie, S., Tai, Y.-W., Lu, C.: Rmpe: Regional
    multi-person pose estimation. In: ICCV, pp. 2334–2343 (2017)'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等 [2017] Fang, H.-S., Xie, S., Tai, Y.-W., Lu, C.：Rmpe：区域多人人体姿态估计。发表于：ICCV，第2334–2343页（2017年）
- en: 'Chen et al. [2018] Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.:
    Cascaded pyramid network for multi-person pose estimation. In: CVPR, pp. 7103–7112
    (2018)'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2018] Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.：级联金字塔网络用于多人人体姿态估计。发表于：CVPR，第7103–7112页（2018年）
- en: 'Su et al. [2019] Su, K., Yu, D., Xu, Z., Geng, X., Wang, C.: Multi-person pose
    estimation with enhanced channel-wise and spatial information. In: CVPR, pp. 5674–5682
    (2019)'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等 [2019] Su, K., Yu, D., Xu, Z., Geng, X., Wang, C.：具有增强的通道和空间信息的多人人体姿态估计。发表于：CVPR，第5674–5682页（2019年）
- en: 'Qiu et al. [2020] Qiu, L., Zhang, X., Li, Y., Li, G., Wu, X., Xiong, Z., Han,
    X., Cui, S.: Peeking into occluded joints: A novel framework for crowd pose estimation.
    In: ECCV, pp. 488–504 (2020)'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等 [2020] Qiu, L., Zhang, X., Li, Y., Li, G., Wu, X., Xiong, Z., Han, X.,
    Cui, S.：窥探遮挡关节：一种用于人群姿态估计的新颖框架。发表于：ECCV，第488–504页（2020年）
- en: 'Yang et al. [2021] Yang, S., Quan, Z., Nie, M., Yang, W.: Transpose: Keypoint
    localization via transformer. In: ICCV, pp. 11802–11812 (2021)'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 [2021] Yang, S., Quan, Z., Nie, M., Yang, W.：Transpose：通过变换器进行关键点定位。发表于：ICCV，第11802–11812页（2021年）
- en: 'Zhou et al. [2023] Zhou, M., Stoffl, L., Mathis, M., Mathis, A.: Rethinking
    pose estimation in crowds: overcoming the detection information-bottleneck and
    ambiguity. In: ICCV (2023)'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 [2023] Zhou, M., Stoffl, L., Mathis, M., Mathis, A.：重新思考人群中的姿态估计：克服检测信息瓶颈和模糊性。发表于：ICCV（2023年）
- en: 'Li et al. [2021] Li, Y., Zhang, S., Wang, Z., Yang, S., Yang, W., Xia, S.-T.,
    Zhou, E.: Tokenpose: Learning keypoint tokens for human pose estimation. In: ICCV,
    pp. 11313–11322 (2021)'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2021] Li, Y., Zhang, S., Wang, Z., Yang, S., Yang, W., Xia, S.-T., Zhou,
    E.：Tokenpose：用于人体姿态估计的关键点令牌学习。发表于：ICCV，第11313–11322页（2021年）
- en: 'Yuan et al. [2021] Yuan, Y., Fu, R., Huang, L., Lin, W., Zhang, C., Chen, X.,
    Wang, J.: Hrformer: High-resolution transformer for dense prediction. In: NIPS
    (2021)'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 [2021] Yuan, Y., Fu, R., Huang, L., Lin, W., Zhang, C., Chen, X., Wang,
    J.：Hrformer：用于密集预测的高分辨率变换器。发表于：NIPS（2021年）
- en: 'Xu et al. [2022] Xu, Y., Zhang, J., Zhang, Q., Tao, D.: Vitpose: Simple vision
    transformer baselines for human pose estimation. In: NIPS, vol. 35, pp. 38571–38584
    (2022)'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu等人[2022] Xu, Y., Zhang, J., Zhang, Q., Tao, D.: Vitpose: 简单的视觉变换器基线用于人体姿态估计。在：NIPS，第35卷，第38571–38584页（2022）'
- en: 'Pishchulin et al. [2016] Pishchulin, L., Insafutdinov, E., Tang, S., Andres,
    B., Andriluka, M., Gehler, P.V., Schiele, B.: Deepcut: Joint subset partition
    and labeling for multi person pose estimation. In: CVPR, pp. 4929–4937 (2016)'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pishchulin等人[2016] Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B.,
    Andriluka, M., Gehler, P.V., Schiele, B.: Deepcut: 多人体姿态估计的联合子集划分和标记。在：CVPR，第4929–4937页（2016）'
- en: 'Insafutdinov et al. [2016] Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
    M., Schiele, B.: Deepercut: A deeper, stronger, and faster multi-person pose estimation
    model. In: ECCV, pp. 34–50 (2016)'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Insafutdinov等人[2016] Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
    M., Schiele, B.: Deepercut: 一个更深、更强、更快的多人体姿态估计模型。在：ECCV，第34–50页（2016）'
- en: 'Cao et al. [2017a] Cao, Z., Simon, T., Wei, S.-E., Sheikh, Y.: Realtime multi-person
    2d pose estimation using part affinity fields. In: CVPR, pp. 7291–7299 (2017)'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao等人[2017a] Cao, Z., Simon, T., Wei, S.-E., Sheikh, Y.: 使用部件关联场的实时多人体2D姿态估计。在：CVPR，第7291–7299页（2017）'
- en: 'Cao et al. [2017b] Cao, Z., Simon, T., Wei, S.-E., Sheikh, Y.: OpenPose:realtime
    multi-person 2D pose estimation using part affinity fields. In: CVPR, pp. 7291–7299
    (2017)'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao等人[2017b] Cao, Z., Simon, T., Wei, S.-E., Sheikh, Y.: OpenPose: 使用部件关联场的实时多人体2D姿态估计。在：CVPR，第7291–7299页（2017）'
- en: 'Kreiss et al. [2019] Kreiss, S., Bertoni, L., Alahi, A.: Pifpaf: Composite
    fields for human pose estimation. In: CVPR, pp. 11977–11986 (2019)'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kreiss等人[2019] Kreiss, S., Bertoni, L., Alahi, A.: Pifpaf: 人体姿态估计的复合场。在：CVPR，第11977–11986页（2019）'
- en: 'Cheng et al. [2023] Cheng, Y., Ai, Y., Wang, B., Wang, X., Tan, R.T.: Bottom-up
    2d pose estimation via dual anatomical centers for small-scale persons. PR 139,
    109403 (2023)'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng等人[2023] Cheng, Y., Ai, Y., Wang, B., Wang, X., Tan, R.T.: 通过双解剖中心的自下而上的2D姿态估计用于小规模个体。PR
    139, 109403（2023）'
- en: 'Qu et al. [2023] Qu, H., Cai, Y., Foo, L.G., Kumar, A., Liu, J.: A characteristic
    function-based method for bottom-up human pose estimation. In: CVPR, pp. 13009–13018
    (2023)'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qu等人[2023] Qu, H., Cai, Y., Foo, L.G., Kumar, A., Liu, J.: 基于特征函数的方法用于自下而上的人体姿态估计。在：CVPR，第13009–13018页（2023）'
- en: 'Newell et al. [2017] Newell, A., Huang, Z., Deng, J.: Associative embedding:
    End-to-end learning for joint detection and grouping. In: NIPS, pp. 2277–2287
    (2017)'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Newell等人[2017] Newell, A., Huang, Z., Deng, J.: 关联嵌入：用于联合检测和分组的端到端学习。在：NIPS，第2277–2287页（2017）'
- en: 'Kocabas et al. [2018] Kocabas, M., Karagoz, S., Akbas, E.: Multiposenet: Fast
    multi-person pose estimation using pose residual network. In: ECCV, pp. 417–433
    (2018)'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kocabas等人[2018] Kocabas, M., Karagoz, S., Akbas, E.: Multiposenet: 使用姿态残差网络的快速多人体姿态估计。在：ECCV，第417–433页（2018）'
- en: 'Li et al. [2019] Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.-S., Lu, C.: Crowdpose:
    Efficient crowded scenes pose estimation and a new benchmark. In: CVPR, pp. 10863–10872
    (2019)'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人[2019] Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.-S., Lu, C.: Crowdpose:
    高效的拥挤场景姿态估计和一个新的基准。在：CVPR，第10863–10872页（2019）'
- en: 'Jin et al. [2020] Jin, S., Liu, W., Xie, E., Wang, W., Qian, C., Ouyang, W.,
    Luo, P.: Differentiable hierarchical graph grouping for multi-person pose estimation.
    In: ECCV, pp. 718–734 (2020)'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin等人[2020] Jin, S., Liu, W., Xie, E., Wang, W., Qian, C., Ouyang, W., Luo,
    P.: 多人体姿态估计的可微分层次图分组。在：ECCV，第718–734页（2020）'
- en: 'Cheng et al. [2020] Cheng, B., Xiao, B., Wang, J., Shi, H., Huang, T.S., Zhang,
    L.: Higherhrnet: Scale-aware representation learning for bottom-up human pose
    estimation. In: CVPR, pp. 5386–5395 (2020)'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng等人[2020] Cheng, B., Xiao, B., Wang, J., Shi, H., Huang, T.S., Zhang, L.:
    Higherhrnet: 面向自下而上的人体姿态估计的尺度感知表示学习。在：CVPR，第5386–5395页（2020）'
- en: 'Nie et al. [2019] Nie, X., Feng, J., Zhang, J., Yan, S.: Single-stage multi-person
    pose machines. In: ICCV, pp. 6951–6960 (2019)'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nie等人[2019] Nie, X., Feng, J., Zhang, J., Yan, S.: 单阶段多人体姿态机器。在：ICCV，第6951–6960页（2019）'
- en: 'Geng et al. [2021] Geng, Z., Sun, K., Xiao, B., Zhang, Z., Wang, J.: Bottom-up
    human pose estimation via disentangled keypoint regression. In: CVPR, pp. 14676–14686
    (2021)'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Geng等人[2021] Geng, Z., Sun, K., Xiao, B., Zhang, Z., Wang, J.: 通过解耦关键点回归的自下而上的人体姿态估计。在：CVPR，第14676–14686页（2021）'
- en: 'Li et al. [2023] Li, J., Wang, Y., Zhang, S.: PolarPose: Single-stage multi-person
    pose estimation in polar coordinates. IEEE Transactions on Image Processing 32,
    1108–1119 (2023)'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人[2023] Li, J., Wang, Y., Zhang, S.: PolarPose: 极坐标下的单阶段多人体姿态估计。IEEE Transactions
    on Image Processing 32, 1108–1119（2023）'
- en: 'Tian et al. [2019] Tian, Z., Chen, H., Shen, C.: Directpose: Direct end-to-end
    multi-person pose estimation. arXiv preprint arXiv:1911.07451 (2019)'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tian等人[2019] Tian, Z., Chen, H., Shen, C.: Directpose: 直接端到端的多人体姿态估计。arXiv预印本
    arXiv:1911.07451（2019）'
- en: 'Mao et al. [2021] Mao, W., Tian, Z., Wang, X., Shen, C.: Fcpose: Fully convolutional
    multi-person pose estimation with dynamic instance-aware convolutions. In: CVPR,
    pp. 9034–9043 (2021)'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mao et al. [2021] Mao, W., Tian, Z., Wang, X., Shen, C.: Fcpose: Fully convolutional
    multi-person pose estimation with dynamic instance-aware convolutions. In: CVPR,
    pp. 9034–9043 (2021)'
- en: 'Shi et al. [2021] Shi, D., Wei, X., Yu, X., Tan, W., Ren, Y., Pu, S.: Inspose:
    instance-aware networks for single-stage multi-person pose estimation. In: ACMMM,
    pp. 3079–3087 (2021)'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi et al. [2021] Shi, D., Wei, X., Yu, X., Tan, W., Ren, Y., Pu, S.: Inspose:
    instance-aware networks for single-stage multi-person pose estimation. In: ACMMM,
    pp. 3079–3087 (2021)'
- en: 'Miao et al. [2023] Miao, H., Lin, J., Cao, J., He, X., Su, Z., Liu, R.: Smpr:
    Single-stage multi-person pose regression. PR 143, 109743 (2023)'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miao et al. [2023] Miao, H., Lin, J., Cao, J., He, X., Su, Z., Liu, R.: Smpr:
    Single-stage multi-person pose regression. PR 143, 109743 (2023)'
- en: 'Shi et al. [2022] Shi, D., Wei, X., Li, L., Ren, Y., Tan, W.: End-to-end multi-person
    pose estimation with transformers. In: CVPR, pp. 11069–11078 (2022)'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi et al. [2022] Shi, D., Wei, X., Li, L., Ren, Y., Tan, W.: End-to-end multi-person
    pose estimation with transformers. In: CVPR, pp. 11069–11078 (2022)'
- en: 'Liu et al. [2023] Liu, H., Chen, Q., Tan, Z., Liu, J.-J., Wang, J., Su, X.,
    Li, X., Yao, K., Han, J., Ding, E., Zhao, Y., Wang, J.: Group pose: A simple baseline
    for end-to-end multi-person pose estimation. In: ICCV, pp. 15029–15038 (2023)'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023] Liu, H., Chen, Q., Tan, Z., Liu, J.-J., Wang, J., Su, X.,
    Li, X., Yao, K., Han, J., Ding, E., Zhao, Y., Wang, J.: Group pose: A simple baseline
    for end-to-end multi-person pose estimation. In: ICCV, pp. 15029–15038 (2023)'
- en: 'Pfister et al. [2014] Pfister, T., Simonyan, K., Charles, J., Zisserman, A.:
    Deep convolutional neural networks for efficient pose estimation in gesture videos.
    In: ACCV, pp. 538–552 (2014)'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfister et al. [2014] Pfister, T., Simonyan, K., Charles, J., Zisserman, A.:
    Deep convolutional neural networks for efficient pose estimation in gesture videos.
    In: ACCV, pp. 538–552 (2014)'
- en: 'Grinciunaite et al. [2016] Grinciunaite, A., Gudi, A., Tasli, E., Den Uyl,
    M.: Human pose estimation in space and time using 3d cnn. In: ECCV, pp. 32–39
    (2016)'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Grinciunaite et al. [2016] Grinciunaite, A., Gudi, A., Tasli, E., Den Uyl,
    M.: Human pose estimation in space and time using 3d cnn. In: ECCV, pp. 32–39
    (2016)'
- en: 'Pfister et al. [2015] Pfister, T., Charles, J., Zisserman, A.: Flowing convnets
    for human pose estimation in videos. In: ICCV, pp. 1913–1921 (2015)'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfister et al. [2015] Pfister, T., Charles, J., Zisserman, A.: Flowing convnets
    for human pose estimation in videos. In: ICCV, pp. 1913–1921 (2015)'
- en: 'Song et al. [2017] Song, J., Wang, L., Van Gool, L., Hilliges, O.: Thin-slicing
    network: A deep structured model for pose estimation in videos. In: CVPR, pp.
    4220–4229 (2017)'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. [2017] Song, J., Wang, L., Van Gool, L., Hilliges, O.: Thin-slicing
    network: A deep structured model for pose estimation in videos. In: CVPR, pp.
    4220–4229 (2017)'
- en: 'Jain et al. [2014] Jain, A., Tompson, J., LeCun, Y., Bregler, C.: Modeep: A
    deep learning framework using motion features for human pose estimation. In: ACCV,
    pp. 302–315 (2014)'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jain et al. [2014] Jain, A., Tompson, J., LeCun, Y., Bregler, C.: Modeep: A
    deep learning framework using motion features for human pose estimation. In: ACCV,
    pp. 302–315 (2014)'
- en: 'Gkioxari et al. [2016] Gkioxari, G., Toshev, A., Jaitly, N.: Chained predictions
    using convolutional neural networks. In: ECCV, pp. 728–743 (2016)'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gkioxari et al. [2016] Gkioxari, G., Toshev, A., Jaitly, N.: Chained predictions
    using convolutional neural networks. In: ECCV, pp. 728–743 (2016)'
- en: 'Charles et al. [2016] Charles, J., Pfister, T., Magee, D., Hogg, D., Zisserman,
    A.: Personalizing human video pose estimation. In: CVPR, pp. 3063–3072 (2016)'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Charles et al. [2016] Charles, J., Pfister, T., Magee, D., Hogg, D., Zisserman,
    A.: Personalizing human video pose estimation. In: CVPR, pp. 3063–3072 (2016)'
- en: 'Luo et al. [2018] Luo, Y., Ren, J., Wang, Z., Sun, W., Pan, J., Liu, J., Pang,
    J., Lin, L.: Lstm pose machines. In: CVPR, pp. 5207–5215 (2018)'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. [2018] Luo, Y., Ren, J., Wang, Z., Sun, W., Pan, J., Liu, J., Pang,
    J., Lin, L.: Lstm pose machines. In: CVPR, pp. 5207–5215 (2018)'
- en: 'Nie et al. [2019] Nie, X., Li, Y., Luo, L., Zhang, N., Feng, J.: Dynamic kernel
    distillation for efficient pose estimation in videos. In: ICCV, pp. 6942–6950
    (2019)'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nie et al. [2019] Nie, X., Li, Y., Luo, L., Zhang, N., Feng, J.: Dynamic kernel
    distillation for efficient pose estimation in videos. In: ICCV, pp. 6942–6950
    (2019)'
- en: 'Li et al. [2019a] Li, H., Yang, W., Liao, Q.: Temporal feature enhancing network
    for human pose estimation in videos. In: ICIP, pp. 579–583 (2019)'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2019a] Li, H., Yang, W., Liao, Q.: Temporal feature enhancing network
    for human pose estimation in videos. In: ICIP, pp. 579–583 (2019)'
- en: 'Li et al. [2019b] Li, W., Xu, X., Zhang, Y.-J.: Temporal feature correlation
    for human pose estimation in videos. In: ICIP, pp. 599–603 (2019)'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2019b] Li, W., Xu, X., Zhang, Y.-J.: Temporal feature correlation
    for human pose estimation in videos. In: ICIP, pp. 599–603 (2019)'
- en: 'Xu et al. [2021] Xu, L., Guan, Y., Jin, S., Liu, W., Qian, C., Luo, P., Ouyang,
    W., Wang, X.: Vipnas: Efficient video pose estimation via neural architecture
    search. In: CVPR, pp. 16072–16081 (2021)'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. [2021] Xu, L., Guan, Y., Jin, S., Liu, W., Qian, C., Luo, P., Ouyang,
    W., Wang, X.: Vipnas: Efficient video pose estimation via neural architecture
    search. In: CVPR, pp. 16072–16081 (2021)'
- en: 'Dang et al. [2022] Dang, Y., Yin, J., Zhang, S.: Relation-based associative
    joint location for human pose estimation in videos. IEEE Transactions on Image
    Processing 31, 3973–3986 (2022)'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dang等[2022] Dang, Y., Yin, J., Zhang, S.: 基于关系的关联关节位置用于视频中的人类姿态估计。在IEEE图像处理事务中，页码3973–3986
    (2022)'
- en: 'Jin et al. [2023] Jin, K.-M., Lim, B.-S., Lee, G.-H., Kang, T.-K., Lee, S.-W.:
    Kinematic-aware hierarchical attention network for human pose estimation in videos.
    In: WACV, pp. 5725–5734 (2023)'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin等[2023] Jin, K.-M., Lim, B.-S., Lee, G.-H., Kang, T.-K., Lee, S.-W.: 动态学感知层次注意力网络，用于视频中的人类姿态估计。在WACV会议中，页码5725–5734
    (2023)'
- en: 'Zhang et al. [2020] Zhang, Y., Wang, Y., Camps, O., Sznaier, M.: Key frame
    proposal network for efficient pose estimation in videos. In: ECCV, pp. 609–625
    (2020)'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等[2020] Zhang, Y., Wang, Y., Camps, O., Sznaier, M.: 关键帧提议网络，用于视频中高效的姿态估计。在ECCV会议中，页码609–625
    (2020)'
- en: 'Ma et al. [2022] Ma, X., Rahmani, H., Fan, Z., Yang, B., Chen, J., Liu, J.:
    Remote: Reinforced motion transformation network for semi-supervised 2d pose estimation
    in videos. In: AAAI, pp. 1944–1952 (2022)'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma等[2022] Ma, X., Rahmani, H., Fan, Z., Yang, B., Chen, J., Liu, J.: Remote:
    强化运动转换网络用于视频中的半监督2D姿态估计。在AAAI会议中，页码1944–1952 (2022)'
- en: 'Zeng et al. [2022] Zeng, A., Ju, X., Yang, L., Gao, R., Zhu, X., Dai, B., Xu,
    Q.: Deciwatch: A simple baseline for 10$\times$ efficient 2d and 3d pose estimation.
    In: ECCV, pp. 607–624 (2022)'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng等[2022] Zeng, A., Ju, X., Yang, L., Gao, R., Zhu, X., Dai, B., Xu, Q.:
    Deciwatch: 一种简单的基线用于10$\times$高效的2D和3D姿态估计。在ECCV会议中，页码607–624 (2022)'
- en: 'Sun et al. [2023] Sun, Y., Dougherty, A.W., Zhang, Z., Choi, Y.K., Wu, C.:
    Mixsynthformer: A transformer encoder-like structure with mixed synthetic self-attention
    for efficient human pose estimation. In: ICCV, pp. 14884–14893 (2023)'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun等[2023] Sun, Y., Dougherty, A.W., Zhang, Z., Choi, Y.K., Wu, C.: Mixsynthformer:
    一种类似于变换器编码器的结构，具有混合合成自注意力，用于高效的人类姿态估计。在ICCV会议中，页码14884–14893 (2023)'
- en: 'Xiu et al. [2018] Xiu, Y., Li, J., Wang, H., Fang, Y., Lu, C.: Pose flow: Efficient
    online pose tracking. In: ECCV (2018)'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiu等[2018] Xiu, Y., Li, J., Wang, H., Fang, Y., Lu, C.: 姿态流: 高效的在线姿态跟踪。在ECCV会议中
    (2018)'
- en: 'Girdhar et al. [2018] Girdhar, R., Gkioxari, G., Torresani, L., Paluri, M.,
    Tran, D.: Detect-and-track: Efficient pose estimation in videos. In: CVPR, pp.
    350–359 (2018)'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Girdhar等[2018] Girdhar, R., Gkioxari, G., Torresani, L., Paluri, M., Tran,
    D.: Detect-and-track: 视频中高效的姿态估计。在CVPR会议中，页码350–359 (2018)'
- en: 'Wang et al. [2020] Wang, M., Tighe, J., Modolo, D.: Combining detection and
    tracking for human pose estimation in videos. In: CVPR, pp. 11088–11096 (2020)'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等[2020] Wang, M., Tighe, J., Modolo, D.: 结合检测和跟踪用于视频中的人类姿态估计。在CVPR会议中，页码11088–11096
    (2020)'
- en: 'Fang et al. [2022] Fang, H.-S., Li, J., Tang, H., Xu, C., Zhu, H., Xiu, Y.,
    Li, Y.-L., Lu, C.: Alphapose: Whole-body regional multi-person pose estimation
    and tracking in real-time. TPAMI (2022)'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang等[2022] Fang, H.-S., Li, J., Tang, H., Xu, C., Zhu, H., Xiu, Y., Li, Y.-L.,
    Lu, C.: Alphapose: 实时全身区域多人人物姿态估计和跟踪。在TPAMI中 (2022)'
- en: 'Feng et al. [2023] Feng, R., Gao, Y., Ma, X., Tse, T.H.E., Chang, H.J.: Mutual
    information-based temporal difference learning for human pose estimation in video.
    In: CVPR, pp. 17131–17141 (2023)'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng等[2023] Feng, R., Gao, Y., Ma, X., Tse, T.H.E., Chang, H.J.: 基于互信息的时间差学习用于视频中的人类姿态估计。在CVPR会议中，页码17131–17141
    (2023)'
- en: 'Gai et al. [2023] Gai, D., Feng, R., Min, W., Yang, X., Su, P., Wang, Q., Han,
    Q.: Spatiotemporal learning transformer for video-based human pose estimation.
    TCSVT (2023)'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gai等[2023] Gai, D., Feng, R., Min, W., Yang, X., Su, P., Wang, Q., Han, Q.:
    用于视频基础的人类姿态估计的时空学习变换器。在TCSVT中 (2023)'
- en: 'Amit et al. [2021] Amit, T., Shaharbany, T., Nachmani, E., Wolf, L.: Segdiff:
    Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390
    (2021)'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amit等[2021] Amit, T., Shaharbany, T., Nachmani, E., Wolf, L.: Segdiff: 使用扩散概率模型的图像分割。在arXiv预印本中，arXiv:2112.00390
    (2021)'
- en: 'Chen et al. [2023] Chen, S., Sun, P., Song, Y., Luo, P.: Diffusiondet: Diffusion
    model for object detection. ICCV, 19830–19843 (2023)'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen等[2023] Chen, S., Sun, P., Song, Y., Luo, P.: Diffusiondet: 用于目标检测的扩散模型。在ICCV会议中，页码19830–19843
    (2023)'
- en: 'Feng et al. [2023] Feng, R., Gao, Y., Tse, T.H.E., Ma, X., Chang, H.J.: Diffpose:
    Spatiotemporal diffusion model for video-based human pose estimation. In: ICCV,
    pp. 14861–14872 (2023)'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng等[2023] Feng, R., Gao, Y., Tse, T.H.E., Ma, X., Chang, H.J.: Diffpose:
    用于视频基础的人类姿态估计的时空扩散模型。在ICCV会议中，页码14861–14872 (2023)'
- en: 'Jin et al. [2019] Jin, S., Liu, W., Ouyang, W., Qian, C.: Multi-person articulated
    tracking with spatial and temporal embeddings. In: CVPR, pp. 5664–5673 (2019)'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin等[2019] Jin, S., Liu, W., Ouyang, W., Qian, C.: 多人关节跟踪，具有空间和时间嵌入。在CVPR会议中，页码5664–5673
    (2019)'
- en: 'Li and Chan [2014] Li, S., Chan, A.B.: 3d human pose estimation from monocular
    images with deep convolutional neural network. In: ACCV, pp. 332–347 (2014)'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li和Chan [2014] Li, S., Chan, A.B.: 基于单目图像的3D人类姿态估计，使用深度卷积神经网络。在ACCV会议中，页码332–347
    (2014)'
- en: 'Li et al. [2015] Li, S., Zhang, W., Chan, A.B.: Maximum-margin structured learning
    with deep networks for 3d human pose estimation. In: ICCV, pp. 2848–2856 (2015)'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2015] Li, S., Zhang, W., Chan, A.B.: 使用深度网络进行最大间隔结构学习以进行 3d 人体姿态估计。在：ICCV，页码
    2848–2856（2015）'
- en: 'Tekin et al. [2016] Tekin, B., Katircioglu, I., Salzmann, M., Lepetit, V.,
    Fua, P.: Structured prediction of 3d human pose with deep neural networks. In:
    BMVC, pp. 1–11 (2016)'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tekin 等人 [2016] Tekin, B., Katircioglu, I., Salzmann, M., Lepetit, V., Fua,
    P.: 使用深度神经网络对 3d 人体姿态进行结构化预测。在：BMVC，页码 1–11（2016）'
- en: 'Zhou et al. [2016] Zhou, X., Sun, X., Zhang, W., Liang, S., Wei, Y.: Deep kinematic
    pose regression. In: ECCV, pp. 186–201 (2016)'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2016] Zhou, X., Sun, X., Zhang, W., Liang, S., Wei, Y.: 深度运动学姿态回归。在：ECCV，页码
    186–201（2016）'
- en: 'Mehta et al. [2017] Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko,
    O., Xu, W., Theobalt, C.: Monocular 3d human pose estimation in the wild using
    improved cnn supervision. In: 3DV, pp. 506–516 (2017)'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mehta 等人 [2017] Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko, O.,
    Xu, W., Theobalt, C.: 使用改进的 cnn 监督进行单目 3d 人体姿态估计。在：3DV，页码 506–516（2017）'
- en: 'Zhou et al. [2017] Zhou, X., Huang, Q., Sun, X., Xue, X., Wei, Y.: Towards
    3d human pose estimation in the wild: a weakly-supervised approach. In: ICCV,
    pp. 398–407 (2017)'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2017] Zhou, X., Huang, Q., Sun, X., Xue, X., Wei, Y.: 向野外的 3d 人体姿态估计迈进：一种弱监督方法。在：ICCV，页码
    398–407（2017）'
- en: 'Martinez et al. [2017] Martinez, J., Hossain, R., Romero, J., Little, J.J.:
    A simple yet effective baseline for 3d human pose estimation. In: ICCV, pp. 2640–2649
    (2017)'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Martinez 等人 [2017] Martinez, J., Hossain, R., Romero, J., Little, J.J.: 一种简单却有效的
    3d 人体姿态估计基线。在：ICCV，页码 2640–2649（2017）'
- en: 'Tekin et al. [2017] Tekin, B., Márquez-Neila, P., Salzmann, M., Fua, P.: Learning
    to fuse 2d and 3d image cues for monocular body pose estimation. In: ICCV, pp.
    3941–3950 (2017)'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tekin 等人 [2017] Tekin, B., Márquez-Neila, P., Salzmann, M., Fua, P.: 学习融合 2d
    和 3d 图像线索用于单目体姿态估计。在：ICCV，页码 3941–3950（2017）'
- en: 'Zhou et al. [2019] Zhou, K., Han, X., Jiang, N., Jia, K., Lu, J.: Hemlets pose:
    Learning part-centric heatmap triplets for accurate 3d human pose estimation.
    In: ICCV, pp. 2344–2353 (2019)'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2019] Zhou, K., Han, X., Jiang, N., Jia, K., Lu, J.: Hemlets 姿态：学习以部分为中心的热图三元组以实现准确的
    3d 人体姿态估计。在：ICCV，页码 2344–2353（2019）'
- en: 'Wang et al. [2018] Wang, M., Chen, X., Liu, W., Qian, C., Lin, L., Ma, L.:
    Drpose3d: Depth ranking in 3d human pose estimation. arXiv preprint arXiv:1805.08973
    (2018)'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2018] Wang, M., Chen, X., Liu, W., Qian, C., Lin, L., Ma, L.: Drpose3d:
    在 3d 人体姿态估计中进行深度排序。arXiv 预印本 arXiv:1805.08973（2018）'
- en: 'Carbonera Luvizon et al. [2023] Carbonera Luvizon, D., Tabia, H., Picard, D.:
    SSP-Net: Scalable sequential pyramid networks for real-time 3d human pose regression.
    PR 142, 109714 (2023)'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carbonera Luvizon 等人 [2023] Carbonera Luvizon, D., Tabia, H., Picard, D.: SSP-Net：用于实时
    3d 人体姿态回归的可扩展序列金字塔网络。PR 142, 109714（2023）'
- en: 'Jahangiri and Yuille [2017] Jahangiri, E., Yuille, A.L.: Generating multiple
    diverse hypotheses for human 3d pose consistent with 2d joint detections. In:
    ICCV, pp. 805–814 (2017)'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jahangiri 和 Yuille [2017] Jahangiri, E., Yuille, A.L.: 生成与 2d 关节检测一致的人体 3d
    姿态的多个多样化假设。在：ICCV，页码 805–814（2017）'
- en: 'Sharma et al. [2019] Sharma, S., Varigonda, P.T., Bindal, P., Sharma, A., Jain,
    A.: Monocular 3d human pose estimation by generation and ordinal ranking. In:
    ICCV, pp. 2325–2334 (2019)'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sharma 等人 [2019] Sharma, S., Varigonda, P.T., Bindal, P., Sharma, A., Jain,
    A.: 通过生成和排序排名进行单目 3d 人体姿态估计。在：ICCV，页码 2325–2334（2019）'
- en: 'Li and Lee [2019] Li, C., Lee, G.H.: Generating multiple hypotheses for 3d
    human pose estimation with mixture density network. In: CVPR, pp. 9887–9895 (2019)'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Lee [2019] Li, C., Lee, G.H.: 通过混合密度网络生成多个 3d 人体姿态估计假设。在：CVPR，页码 9887–9895（2019）'
- en: 'Ci et al. [2019] Ci, H., Wang, C., Ma, X., Wang, Y.: Optimizing network structure
    for 3d human pose estimation. In: ICCV, pp. 2262–2271 (2019)'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ci 等人 [2019] Ci, H., Wang, C., Ma, X., Wang, Y.: 优化网络结构用于 3d 人体姿态估计。在：ICCV，页码
    2262–2271（2019）'
- en: 'Zhao et al. [2019] Zhao, L., Peng, X., Tian, Y., Kapadia, M., Metaxas, D.N.:
    Semantic graph convolutional networks for 3d human pose regression. In: CVPR,
    pp. 3425–3435 (2019)'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人 [2019] Zhao, L., Peng, X., Tian, Y., Kapadia, M., Metaxas, D.N.: 用于
    3d 人体姿态回归的语义图卷积网络。在：CVPR，页码 3425–3435（2019）'
- en: 'Choi et al. [2020] Choi, H., Moon, G., Lee, K.M.: Pose2mesh: Graph convolutional
    network for 3d human pose and mesh recovery from a 2d human pose. In: ECCV, pp.
    769–787 (2020)'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choi 等人 [2020] Choi, H., Moon, G., Lee, K.M.: Pose2mesh：用于从 2d 人体姿态恢复 3d 人体姿态和网格的图卷积网络。在：ECCV，页码
    769–787（2020）'
- en: 'Zeng et al. [2020] Zeng, A., Sun, X., Huang, F., Liu, M., Xu, Q., Lin, S.:
    Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine
    approach. In: ECCV, pp. 507–523 (2020)'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等人 [2020] Zeng, A., Sun, X., Huang, F., Liu, M., Xu, Q., Lin, S.: Srnet:
    通过拆分和重新组合方法提高 3d 人体姿态估计的泛化能力。在：ECCV，页码 507–523（2020）'
- en: 'Liu et al. [2020] Liu, K., Ding, R., Zou, Z., Wang, L., Tang, W.: A comprehensive
    study of weight sharing in graph networks for 3d human pose estimation. In: Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part X 16, pp. 318–334 (2020)'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2020] Liu, K., Ding, R., Zou, Z., Wang, L., Tang, W.: 图网络中权重共享的综合研究用于
    3d 人体姿态估计。载于：计算机视觉–ECCV 2020: 第 16 届欧洲会议，英国格拉斯哥，2020 年 8 月 23–28 日，会议录，第 X 部分，第
    318–334 页（2020）'
- en: 'Zou and Tang [2021] Zou, Z., Tang, W.: Modulated graph convolutional network
    for 3d human pose estimation. In: ICCV, pp. 11477–11487 (2021)'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zou 和 Tang [2021] Zou, Z., Tang, W.: 调制图卷积网络用于 3d 人体姿态估计。载于：ICCV，第 11477–11487
    页（2021）'
- en: 'Xu and Takano [2021] Xu, T., Takano, W.: Graph stacked hourglass networks for
    3d human pose estimation. In: CVPR, pp. 16105–16114 (2021)'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 和 Takano [2021] Xu, T., Takano, W.: 图堆叠小时玻璃网络用于 3d 人体姿态估计。载于：CVPR，第 16105–16114
    页（2021）'
- en: 'Shengping et al. [2023] Shengping, Z., Chenyang, W., Liqiang, N., Hongxun,
    Y., Qingming, H., Qi, T.: Learning enriched hop-aware correlation for robust 3d
    human pose estimation. IJCV (6), 1566–1583 (2023)'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shengping 等 [2023] Shengping, Z., Chenyang, W., Liqiang, N., Hongxun, Y., Qingming,
    H., Qi, T.: 学习丰富的跳跃感知相关性以增强 3d 人体姿态估计的鲁棒性。IJCV (6)，1566–1583（2023）'
- en: 'Hassan and Ben Hamza [2023] Hassan, M.T., Ben Hamza, A.: Regular splitting
    graph network for 3d human pose estimation. IEEE Transactions on Image Processing
    32, 4212–4222 (2023)'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hassan 和 Ben Hamza [2023] Hassan, M.T., Ben Hamza, A.: 用于 3d 人体姿态估计的规则分裂图网络。IEEE
    Transactions on Image Processing 32，第 4212–4222 页（2023）'
- en: 'Zhai et al. [2023] Zhai, K., Nie, Q., Ouyang, B., Li, X., Yang, S.: Hopfir:
    Hop-wise graphformer with intragroup joint refinement for 3d human pose estimation.
    In: ICCV (2023)'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhai 等 [2023] Zhai, K., Nie, Q., Ouyang, B., Li, X., Yang, S.: Hopfir：具有组内联合精细化的逐跳图变换器用于
    3d 人体姿态估计。载于：ICCV（2023）'
- en: 'Lin et al. [2021] Lin, K., Wang, L., Liu, Z.: End-to-end human pose and mesh
    reconstruction with transformers. In: CVPR, pp. 1954–1963 (2021)'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 [2021] Lin, K., Wang, L., Liu, Z.: 基于变换器的端到端人体姿态和网格重建。载于：CVPR，第 1954–1963
    页（2021）'
- en: 'Zhao et al. [2022] Zhao, W., Wang, W., Tian, Y.: Graformer: Graph-oriented
    transformer for 3d pose estimation. In: CVPR, pp. 20438–20447 (2022)'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等 [2022] Zhao, W., Wang, W., Tian, Y.: Graformer：面向图的变换器用于 3d 姿态估计。载于：CVPR，第
    20438–20447 页（2022）'
- en: 'Yang et al. [2018] Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., Wang, X.:
    3d human pose estimation in the wild by adversarial learning. In: CVPR, pp. 5255–5264
    (2018)'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等 [2018] Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., Wang, X.: 通过对抗学习进行野外
    3d 人体姿态估计。载于：CVPR，第 5255–5264 页（2018）'
- en: 'Habibie et al. [2019] Habibie, I., Xu, W., Mehta, D., Pons-Moll, G., Theobalt,
    C.: In the wild human pose estimation using explicit 2d features and intermediate
    3d representations. In: CVPR, pp. 10905–10914 (2019)'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Habibie 等 [2019] Habibie, I., Xu, W., Mehta, D., Pons-Moll, G., Theobalt, C.:
    在野外使用显式 2d 特征和中间 3d 表示进行人体姿态估计。载于：CVPR，第 10905–10914 页（2019）'
- en: 'Chen et al. [2019] Chen, C.-H., Tyagi, A., Agrawal, A., Drover, D., Mv, R.,
    Stojanov, S., Rehg, J.M.: Unsupervised 3d pose estimation with geometric self-supervision.
    In: CVPR, pp. 5714–5724 (2019)'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2019] Chen, C.-H., Tyagi, A., Agrawal, A., Drover, D., Mv, R., Stojanov,
    S., Rehg, J.M.: 使用几何自监督的无监督 3d 姿态估计。载于：CVPR，第 5714–5724 页（2019）'
- en: 'Wandt and Rosenhahn [2019] Wandt, B., Rosenhahn, B.: Repnet: Weakly supervised
    training of an adversarial reprojection network for 3d human pose estimation.
    In: CVPR, pp. 7782–7791 (2019)'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wandt 和 Rosenhahn [2019] Wandt, B., Rosenhahn, B.: Repnet：用于 3d 人体姿态估计的弱监督对抗重投影网络训练。载于：CVPR，第
    7782–7791 页（2019）'
- en: 'Iqbal et al. [2020] Iqbal, U., Molchanov, P., Kautz, J.: Weakly-supervised
    3d human pose learning via multi-view images in the wild. In: CVPR, pp. 5243–5252
    (2020)'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Iqbal 等 [2020] Iqbal, U., Molchanov, P., Kautz, J.: 通过多视角图像进行弱监督的 3d 人体姿态学习。载于：CVPR，第
    5243–5252 页（2020）'
- en: 'Kundu et al. [2020] Kundu, J.N., Seth, S., Jampani, V., Rakesh, M., Babu, R.V.,
    Chakraborty, A.: Self-supervised 3d human pose estimation via part guided novel
    image synthesis. In: CVPR, pp. 6152–6162 (2020)'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kundu 等 [2020] Kundu, J.N., Seth, S., Jampani, V., Rakesh, M., Babu, R.V.,
    Chakraborty, A.: 通过部件引导的新图像合成进行自监督 3d 人体姿态估计。载于：CVPR，第 6152–6162 页（2020）'
- en: 'Schmidtke et al. [2021] Schmidtke, L., Vlontzos, A., Ellershaw, S., Lukens,
    A., Arichi, T., Kainz, B.: Unsupervised human pose estimation through transforming
    shape templates. In: CVPR, pp. 2484–2494 (2021)'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schmidtke 等 [2021] Schmidtke, L., Vlontzos, A., Ellershaw, S., Lukens, A.,
    Arichi, T., Kainz, B.: 通过转换形状模板进行无监督人体姿态估计。载于：CVPR，第 2484–2494 页（2021）'
- en: 'Yu et al. [2021] Yu, Z., Ni, B., Xu, J., Wang, J., Zhao, C., Zhang, W.: Towards
    alleviating the modeling ambiguity of unsupervised monocular 3d human pose estimation.
    In: ICCV, pp. 8651–8660 (2021)'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 [2021] Yu, Z., Ni, B., Xu, J., Wang, J., Zhao, C., Zhang, W.: 旨在缓解无监督单目
    3d 人体姿态估计的建模歧义。载于：ICCV，第 8651–8660 页（2021）'
- en: 'Gong et al. [2022] Gong, K., Li, B., Zhang, J., Wang, T., Huang, J., Mi, M.B.,
    Feng, J., Wang, X.: Posetriplet: co-evolving 3d human pose estimation, imitation,
    and hallucination under self-supervision. In: CVPR, pp. 11017–11027 (2022)'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gong 等 [2022] Gong, K., Li, B., Zhang, J., Wang, T., Huang, J., Mi, M.B., Feng,
    J., Wang, X.: Posetriplet: 在自监督下共同演化的 3D 人体姿态估计、模仿和幻觉。会议：CVPR，第 11017–11027 页
    (2022)'
- en: 'Chai et al. [2023] Chai, W., Jiang, Z., Hwang, J.-N., Wang, G.: Global adaptation
    meets local generalization: Unsupervised domain adaptation for 3d human pose estimation.
    In: ICCV (2023)'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chai 等 [2023] Chai, W., Jiang, Z., Hwang, J.-N., Wang, G.: 全球适应与局部泛化的结合：用于
    3D 人体姿态估计的无监督领域适应。会议：ICCV (2023)'
- en: 'Wang et al. [2022] Wang, Z., Nie, X., Qu, X., Chen, Y., Liu, S.: Distribution-aware
    single-stage models for multi-person 3d pose estimation. In: CVPR, pp. 13096–13105
    (2022)'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2022] Wang, Z., Nie, X., Qu, X., Chen, Y., Liu, S.: 分布感知的单阶段模型用于多人
    3D 姿态估计。会议：CVPR，第 13096–13105 页 (2022)'
- en: 'Rogez et al. [2017] Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net: Localization-classification-regression
    for human pose. In: CVPR, pp. 3433–3441 (2017)'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rogez 等 [2017] Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net: 人体姿态的定位-分类-回归。会议：CVPR，第
    3433–3441 页 (2017)'
- en: 'Rogez et al. [2019] Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net++: Multi-person
    2d and 3d pose detection in natural images. TPAMI 42(5), 1146–1161 (2019)'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rogez 等 [2019] Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net++: 自然图像中的多人
    2D 和 3D 姿态检测。TPAMI 42(5)，1146–1161 (2019)'
- en: 'Moon et al. [2019] Moon, G., Chang, J.Y., Lee, K.M.: Camera distance-aware
    top-down approach for 3d multi-person pose estimation from a single rgb image.
    In: ICCV, pp. 10133–10142 (2019)'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moon 等 [2019] Moon, G., Chang, J.Y., Lee, K.M.: 一种基于相机距离感知的自上而下方法，用于从单张 RGB
    图像中进行 3D 多人姿态估计。会议：ICCV，第 10133–10142 页 (2019)'
- en: 'Lin and Lee [2020] Lin, J., Lee, G.H.: Hdnet: Human depth estimation for multi-person
    camera-space localization. In: ECCV, pp. 633–648 (2020)'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 和 Lee [2020] Lin, J., Lee, G.H.: Hdnet: 用于多人相机空间定位的人体深度估计。会议：ECCV，第 633–648
    页 (2020)'
- en: 'Wang et al. [2020] Wang, C., Li, J., Liu, W., Qian, C., Lu, C.: Hmor: Hierarchical
    multi-person ordinal relations for monocular multi-person 3d pose estimation.
    In: ECCV, pp. 242–259 (2020)'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2020] Wang, C., Li, J., Liu, W., Qian, C., Lu, C.: Hmor: 用于单目多人 3D
    姿态估计的分层多人人体序关系。会议：ECCV，第 242–259 页 (2020)'
- en: 'Cha et al. [2022] Cha, J., Saqlain, M., Kim, G., Shin, M., Baek, S.: Multi-person
    3d pose and shape estimation via inverse kinematics and refinement. In: ECCV,
    pp. 660–677 (2022)'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cha 等 [2022] Cha, J., Saqlain, M., Kim, G., Shin, M., Baek, S.: 通过逆运动学和细化进行多人
    3D 姿态和形状估计。会议：ECCV，第 660–677 页 (2022)'
- en: 'Zanfir et al. [2018] Zanfir, A., Marinoiu, E., Zanfir, M., Popa, A.-I., Sminchisescu,
    C.: Deep network for the integrated 3d sensing of multiple people in natural images.
    NIPS 31 (2018)'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zanfir 等 [2018] Zanfir, A., Marinoiu, E., Zanfir, M., Popa, A.-I., Sminchisescu,
    C.: 用于自然图像中多人的集成 3D 传感的深度网络。NIPS 31 (2018)'
- en: 'Fabbri et al. [2020] Fabbri, M., Lanzi, F., Calderara, S., Alletto, S., Cucchiara,
    R.: Compressed volumetric heatmaps for multi-person 3d pose estimation. In: CVPR,
    pp. 7204–7213 (2020)'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri 等 [2020] Fabbri, M., Lanzi, F., Calderara, S., Alletto, S., Cucchiara,
    R.: 用于多人人体 3D 姿态估计的压缩体积热图。会议：CVPR，第 7204–7213 页 (2020)'
- en: 'Kundu et al. [2020] Kundu, J.N., Revanur, A., Waghmare, G.V., Venkatesh, R.M.,
    Babu, R.V.: Unsupervised cross-modal alignment for multi-person 3d pose estimation.
    In: ECCV, pp. 35–52 (2020)'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kundu 等 [2020] Kundu, J.N., Revanur, A., Waghmare, G.V., Venkatesh, R.M., Babu,
    R.V.: 无监督跨模态对齐用于多人 3D 姿态估计。会议：ECCV，第 35–52 页 (2020)'
- en: 'Mehta et al. [2018] Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar,
    S., Pons-Moll, G., Theobalt, C.: Single-shot multi-person 3d pose estimation from
    monocular rgb. In: 3DV, pp. 120–130 (2018)'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mehta 等 [2018] Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar, S.,
    Pons-Moll, G., Theobalt, C.: 单次拍摄的多人 3D 姿态估计。会议：3DV，第 120–130 页 (2018)'
- en: 'Mehta et al. [2020] Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Elgharib,
    M., Fua, P., Seidel, H.-P., Rhodin, H., Pons-Moll, G., Theobalt, C.: Xnect: Real-time
    multi-person 3d motion capture with a single rgb camera. TOG 39(4), 82–1 (2020)'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mehta 等 [2020] Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Elgharib, M.,
    Fua, P., Seidel, H.-P., Rhodin, H., Pons-Moll, G., Theobalt, C.: Xnect: 使用单个 RGB
    相机进行实时多人 3D 动作捕捉。TOG 39(4)，82–1 (2020)'
- en: 'Zhen et al. [2020] Zhen, J., Fang, Q., Sun, J., Liu, W., Jiang, W., Bao, H.,
    Zhou, X.: Smap: Single-shot multi-person absolute 3d pose estimation. In: ECCV,
    pp. 550–566 (2020)'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhen 等 [2020] Zhen, J., Fang, Q., Sun, J., Liu, W., Jiang, W., Bao, H., Zhou,
    X.: Smap: 单次拍摄的多人绝对 3D 姿态估计。会议：ECCV，第 550–566 页 (2020)'
- en: 'Liu et al. [2022] Liu, Q., Zhang, Y., Bai, S., Yuille, A.: Explicit occlusion
    reasoning for multi-person 3d human pose estimation. In: ECCV, pp. 497–517 (2022)'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2022] Liu, Q., Zhang, Y., Bai, S., Yuille, A.: 明确遮挡推理用于多人 3D 人体姿态估计。会议：ECCV，第
    497–517 页 (2022)'
- en: 'Chen et al. [2023] Chen, X., Zhang, J., Wang, K., Wei, P., Lin, L.: Multi-person
    3d pose esitmation with occlusion reasoning. TMM, 1–13 (2023)'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2023] 陈, X., 张, J., 王, K., 魏, P., 林, L.: 多人3D姿态估计与遮挡推理. TMM, 1–13 (2023)'
- en: 'Zhou et al. [2019] Zhou, X., Wang, D., Krähenbühl, P.: Objects as points. arXiv
    preprint arXiv:1904.07850 (2019)'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '周等人 [2019] 周, X., 王, D., 克雷恩布赫尔, P.: 对象作为点. arXiv 预印本 arXiv:1904.07850 (2019)'
- en: 'Wei et al. [2020] Wei, F., Sun, X., Li, H., Wang, J., Lin, S.: Point-set anchors
    for object detection, instance segmentation and pose estimation. In: ECCV, pp.
    527–544 (2020)'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '韦等人 [2020] 韦, F., 孙, X., 李, H., 王, J., 林, S.: 用于目标检测、实例分割和姿态估计的点集锚点. 见: ECCV,
    pp. 527–544 (2020)'
- en: 'Jin et al. [2022] Jin, L., Xu, C., Wang, X., Xiao, Y., Guo, Y., Nie, X., Zhao,
    J.: Single-stage is enough: Multi-person absolute 3d pose estimation. In: CVPR,
    pp. 13086–13095 (2022)'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '金等人 [2022] 金, L., 许, C., 王, X., 肖, Y., 郭, Y., 聂, X., 赵, J.: 单阶段足够：多人绝对3D姿态估计.
    见: CVPR, pp. 13086–13095 (2022)'
- en: 'Qiu et al. [2023] Qiu, Z., Qiu, K., Fu, J., Fu, D.: Weakly-supervised pre-training
    for 3d human pose estimation via perspective knowledge. PR 139, 109497 (2023)'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '邱等人 [2023] 邱, Z., 邱, K., 傅, J., 傅, D.: 通过透视知识进行3D人体姿态估计的弱监督预训练. PR 139, 109497
    (2023)'
- en: 'Tekin et al. [2016] Tekin, B., Rozantsev, A., Lepetit, V., Fua, P.: Direct
    prediction of 3d body poses from motion compensated sequences. In: CVPR, pp. 991–1000
    (2016)'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '特金等人 [2016] 特金, B., 罗赞采夫, A., 勒佩蒂, V., 富阿, P.: 从运动补偿序列中直接预测3D身体姿态. 见: CVPR,
    pp. 991–1000 (2016)'
- en: 'Mehta et al. [2017] Mehta, D., Sridhar, S., Sotnychenko, O., Rhodin, H., Shafiei,
    M., Seidel, H.-P., Xu, W., Casas, D., Theobalt, C.: Vnect: Real-time 3d human
    pose estimation with a single rgb camera. ACM Transactions on Graphics (TOG) 36(4),
    44 (2017)'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '梅赫塔等人 [2017] 梅赫塔, D., 斯里达尔, S., 索特尼琴科, O., 罗丁, H., 沙菲埃, M., 西德尔, H.-P., 徐,
    W., 卡萨斯, D., 塔尔博特, C.: Vnect: 使用单个RGB相机的实时3D人体姿态估计. ACM Transactions on Graphics
    (TOG) 36(4), 44 (2017)'
- en: 'Dabral et al. [2018] Dabral, R., Mundhada, A., Kusupati, U., Afaque, S., Sharma,
    A., Jain, A.: Learning 3d human pose from structure and motion. In: ECCV, pp.
    668–683 (2018)'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '达布拉尔等人 [2018] 达布拉尔, R., 蒙达达, A., 库苏帕提, U., 阿法克, S., 莎玛, A., 贾因, A.: 从结构和运动中学习3D人体姿态.
    见: ECCV, pp. 668–683 (2018)'
- en: 'Qiu et al. [2022] Qiu, Z., Yang, Q., Wang, J., Fu, D.: Ivt: An end-to-end instance-guided
    video transformer for 3d pose estimation. In: ACM MM, pp. 6174–6182 (2022)'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '邱等人 [2022] 邱, Z., 杨, Q., 王, J., 傅, D.: IVT: 用于3D姿态估计的端到端实例引导视频变换器. 见: ACM MM,
    pp. 6174–6182 (2022)'
- en: 'Honari et al. [2023] Honari, S., Constantin, V., Rhodin, H., Salzmann, M.,
    Fua, P.: Temporal representation learning on monocular videos for 3d human pose
    estimation. TPAMI 45(5), 6415–6427 (2023)'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '霍纳里等人 [2023] 霍纳里, S., 康斯坦丁, V., 罗丁, H., 萨尔茨曼, M., 富阿, P.: 单目视频中的时间表示学习用于3D人体姿态估计.
    TPAMI 45(5), 6415–6427 (2023)'
- en: 'Pavllo et al. [2019] Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.:
    3d human pose estimation in video with temporal convolutions and semi-supervised
    training. In: CVPR, pp. 7753–7762 (2019)'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '帕夫洛等人 [2019] 帕夫洛, D., 费希滕霍夫, C., 格朗吉耶, D., 奥利, M.: 通过时间卷积和半监督训练在视频中进行3D人体姿态估计.
    见: CVPR, pp. 7753–7762 (2019)'
- en: 'Cheng et al. [2019] Cheng, Y., Yang, B., Wang, B., Yan, W., Tan, R.T.: Occlusion-aware
    networks for 3d human pose estimation in video. In: CVPR, pp. 723–732 (2019)'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '程等人 [2019] 程, Y., 杨, B., 王, B., 严, W., 谭, R.T.: 用于视频中3D人体姿态估计的遮挡感知网络. 见: CVPR,
    pp. 723–732 (2019)'
- en: 'Liu et al. [2020] Liu, J., Guang, Y., Rojas, J.: Gast-net: Graph attention
    spatio-temporal convolutional networks for 3d human pose estimation in video.
    arXiv preprint arXiv:2003.14179 (2020)'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等人 [2020] 刘, J., 光, Y., 罗哈斯, J.: GAST-NET: 用于视频中3D人体姿态估计的图注意力时空卷积网络. arXiv
    预印本 arXiv:2003.14179 (2020)'
- en: 'Chen et al. [2021] Chen, T., Fang, C., Shen, X., Zhu, Y., Chen, Z., Luo, J.:
    Anatomy-aware 3d human pose estimation with bone-based pose decomposition. TCSVT
    32(1), 198–209 (2021)'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2021] 陈, T., 方, C., 沈, X., 朱, Y., 陈, Z., 罗, J.: 基于骨骼的姿态分解进行解剖学意识3D人体姿态估计.
    TCSVT 32(1), 198–209 (2021)'
- en: 'Cai et al. [2019] Cai, Y., Ge, L., Liu, J., Cai, J., Cham, T.-J., Yuan, J.,
    Thalmann, N.M.: Exploiting spatial-temporal relationships for 3d pose estimation
    via graph convolutional networks. In: ICCV, pp. 2272–2281 (2019)'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '蔡等人 [2019] 蔡, Y., 葛, L., 刘, J., 蔡, J., 查姆, T.-J., 袁, J., 塔尔曼, N.M.: 通过图卷积网络利用时空关系进行3D姿态估计.
    见: ICCV, pp. 2272–2281 (2019)'
- en: 'Zheng et al. [2021] Zheng, C., Zhu, S., Mendieta, M., Yang, T., Chen, C., Ding,
    Z.: 3d human pose estimation with spatial and temporal transformers. In: ICCV,
    pp. 11656–11665 (2021)'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '郑等人 [2021] 郑, C., 朱, S., 门迪塔, M., 杨, T., 陈, C., 丁, Z.: 使用空间和时间变换器的3D人体姿态估计.
    见: ICCV, pp. 11656–11665 (2021)'
- en: 'Zhao et al. [2023] Zhao, Q., Zheng, C., Liu, M., Wang, P., Chen, C.: Poseformerv2:
    Exploring frequency domain for efficient and robust 3d human pose estimation.
    In: CVPR, pp. 8877–8886 (2023)'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao等人[2023] Zhao, Q., Zheng, C., Liu, M., Wang, P., Chen, C.: Poseformerv2:
    探索频域以实现高效且鲁棒的3D人体姿态估计。In: CVPR, pp. 8877–8886 (2023)'
- en: 'Li et al. [2022a] Li, W., Liu, H., Ding, R., Liu, M., Wang, P., Yang, W.: Exploiting
    temporal contexts with strided transformer for 3d human pose estimation. IEEE
    Transactions on Multimedia (2022)'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人[2022a] Li, W., Liu, H., Ding, R., Liu, M., Wang, P., Yang, W.: 使用滑动变换器挖掘时间上下文进行3D人体姿态估计。IEEE
    Transactions on Multimedia (2022)'
- en: 'Li et al. [2022b] Li, W., Liu, H., Tang, H., Wang, P., Van Gool, L.: Mhformer:
    Multi-hypothesis transformer for 3d human pose estimation. In: CVPR, pp. 13147–13156
    (2022)'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人[2022b] Li, W., Liu, H., Tang, H., Wang, P., Van Gool, L.: Mhformer: 多假设变换器用于3D人体姿态估计。In:
    CVPR, pp. 13147–13156 (2022)'
- en: 'Li et al. [2023] Li, W., Liu, H., Tang, H., Wang, P.: Multi-hypothesis representation
    learning for transformer-based 3d human pose estimation. PR 141, 109631 (2023)'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人[2023] Li, W., Liu, H., Tang, H., Wang, P.: 基于变换器的多假设表示学习用于3D人体姿态估计。PR
    141, 109631 (2023)'
- en: 'Holmquist and Wandt [2023] Holmquist, K., Wandt, B.: Diffpose: Multi-hypothesis
    human pose estimation using diffusion models. In: ICCV, pp. 15977–15987 (2023)'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Holmquist和Wandt[2023] Holmquist, K., Wandt, B.: Diffpose: 使用扩散模型的多假设人体姿态估计。In:
    ICCV, pp. 15977–15987 (2023)'
- en: 'Shan et al. [2023] Shan, W., Liu, Z., Zhang, X., Wang, Z., Han, K., Wang, S.,
    Ma, S., Gao, W.: Diffusion-based 3d human pose estimation with multi-hypothesis
    aggregation. In: ICCV (2023)'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shan等人[2023] Shan, W., Liu, Z., Zhang, X., Wang, Z., Han, K., Wang, S., Ma,
    S., Gao, W.: 基于扩散的3D人体姿态估计与多假设聚合。In: ICCV (2023)'
- en: 'Tang et al. [2023] Tang, Z., Qiu, Z., Hao, Y., Hong, R., Yao, T.: 3d human
    pose estimation with spatio-temporal criss-cross attention. In: CVPR, pp. 4790–4799
    (2023)'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang等人[2023] Tang, Z., Qiu, Z., Hao, Y., Hong, R., Yao, T.: 具有时空交叉注意力的3D人体姿态估计。In:
    CVPR, pp. 4790–4799 (2023)'
- en: 'Lin et al. [2017] Lin, M., Lin, L., Liang, X., Wang, K., Cheng, H.: Recurrent
    3d pose sequence machines. In: CVPR, pp. 810–819 (2017)'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin等人[2017] Lin, M., Lin, L., Liang, X., Wang, K., Cheng, H.: 循环3D姿态序列机器。In:
    CVPR, pp. 810–819 (2017)'
- en: 'Rayat Imtiaz Hossain and Little [2018] Rayat Imtiaz Hossain, M., Little, J.J.:
    Exploiting temporal information for 3d human pose estimation. In: ECCV, pp. 68–84
    (2018)'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rayat Imtiaz Hossain和Little[2018] Rayat Imtiaz Hossain, M., Little, J.J.: 利用时间信息进行3D人体姿态估计。In:
    ECCV, pp. 68–84 (2018)'
- en: 'Lee et al. [2018] Lee, K., Lee, I., Lee, S.: Propagating lstm: 3d pose estimation
    based on joint interdependency. In: ECCV, pp. 119–135 (2018)'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee等人[2018] Lee, K., Lee, I., Lee, S.: 传播LSTM: 基于关节相互依赖的3D姿态估计。In: ECCV, pp.
    119–135 (2018)'
- en: 'Katircioglu et al. [2018] Katircioglu, I., Tekin, B., Salzmann, M., Lepetit,
    V., Fua, P.: Learning latent representations of 3d human pose with deep neural
    networks. IJCV 126(12), 1326–1341 (2018)'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katircioglu等人[2018] Katircioglu, I., Tekin, B., Salzmann, M., Lepetit, V.,
    Fua, P.: 利用深度神经网络学习3D人体姿态的潜在表示。IJCV 126(12), 1326–1341 (2018)'
- en: 'Yeh et al. [2019] Yeh, R., Hu, Y.-T., Schwing, A.: Chirality nets for human
    pose regression. In: NIPS, pp. 8161–8171 (2019)'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yeh等人[2019] Yeh, R., Hu, Y.-T., Schwing, A.: 用于人体姿态回归的手性网络。In: NIPS, pp. 8161–8171
    (2019)'
- en: 'Wang et al. [2020] Wang, J., Yan, S., Xiong, Y., Lin, D.: Motion guided 3d
    pose estimation from videos. In: ECCV, pp. 764–780 (2020)'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人[2020] Wang, J., Yan, S., Xiong, Y., Lin, D.: 基于运动引导的视频中的3D姿态估计。In: ECCV,
    pp. 764–780 (2020)'
- en: 'Yu et al. [2023] Yu, B.X., Zhang, Z., Liu, Y., Zhong, S.-h., Liu, Y., Chen,
    C.W.: Gla-gcn: Global-local adaptive graph convolutional network for 3d human
    pose estimation from monocular video. In: ICCV, pp. 8818–8829 (2023)'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu等人[2023] Yu, B.X., Zhang, Z., Liu, Y., Zhong, S.-h., Liu, Y., Chen, C.W.:
    Gla-gcn: 基于单目视频的3D人体姿态估计的全局-局部自适应图卷积网络。In: ICCV, pp. 8818–8829 (2023)'
- en: 'Zhang et al. [2022] Zhang, J., Tu, Z., Yang, J., Chen, Y., Yuan, J.: Mixste:
    Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video. In:
    CVPR, pp. 13232–13242 (2022)'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人[2022] Zhang, J., Tu, Z., Yang, J., Chen, Y., Yuan, J.: Mixste: 用于视频中3D人体姿态估计的序列到序列混合时空编码器。In:
    CVPR, pp. 13232–13242 (2022)'
- en: 'Chen et al. [2023] Chen, H., He, J.-Y., Xiang, W., Cheng, Z.-Q., Liu, W., Liu,
    H., Luo, B., Geng, Y., Xie, X.: Hdformer: High-order directed transformer for
    3d human pose estimation. In: IJCAI, pp. 581–589 (2023)'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen等人[2023] Chen, H., He, J.-Y., Xiang, W., Cheng, Z.-Q., Liu, W., Liu, H.,
    Luo, B., Geng, Y., Xie, X.: Hdformer: 高阶定向变换器用于3D人体姿态估计。In: IJCAI, pp. 581–589
    (2023)'
- en: 'Shuai et al. [2023] Shuai, H., Wu, L., Liu, Q.: Adaptive multi-view and temporal
    fusing transformer for 3d human pose estimation. TPAMI 45(4), 4122–4135 (2023)'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shuai等人[2023] Shuai, H., Wu, L., Liu, Q.: 自适应多视角和时间融合变换器用于3D人体姿态估计。TPAMI 45(4),
    4122–4135 (2023)'
- en: 'Zhu et al. [2022] Zhu, W., Ma, X., Liu, Z., Liu, L., Wu, W., Wang, Y.: Motionbert:
    Unified pretraining for human motion analysis. arXiv preprint arXiv:2210.06551
    (2022)'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等 [2022] Zhu, W., Ma, X., Liu, Z., Liu, L., Wu, W., Wang, Y.: Motionbert:
    统一预训练用于人体运动分析。arXiv 预印本 arXiv:2210.06551 (2022)'
- en: 'Cheng et al. [2021a] Cheng, Y., Wang, B., Yang, B., Tan, R.T.: Graph and temporal
    convolutional networks for 3d multi-person pose estimation in monocular videos.
    In: AAAI, pp. 1157–1165 (2021)'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等 [2021a] Cheng, Y., Wang, B., Yang, B., Tan, R.T.: 用于单目视频中的3d多人姿态估计的图卷积网络和时间卷积网络。见于：AAAI,
    第1157–1165页 (2021)'
- en: 'Cheng et al. [2021b] Cheng, Y., Wang, B., Yang, B., Tan, R.T.: Monocular 3d
    multi-person pose estimation by integrating top-down and bottom-up networks. In:
    CVPR, pp. 7649–7659 (2021)'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等 [2021b] Cheng, Y., Wang, B., Yang, B., Tan, R.T.: 通过整合自上而下和自下而上的网络进行单目3d多人姿态估计。见于：CVPR,
    第7649–7659页 (2021)'
- en: 'Park et al. [2023] Park, S., You, E., Lee, I., Lee, J.: Towards robust and
    smooth 3d multi-person pose estimation from monocular videos in the wild. In:
    ICCV, pp. 14772–14782 (2023)'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等 [2023] Park, S., You, E., Lee, I., Lee, J.: 通过单目视频实现鲁棒和平滑的3d多人姿态估计。见于：ICCV,
    第14772–14782页 (2023)'
- en: 'Zhao et al. [2015] Zhao, L., Gao, X., Tao, D., Li, X.: Tracking human pose
    using max-margin markov models. IEEE Transactions on Image Processing 24(12),
    5274–5287 (2015)'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等 [2015] Zhao, L., Gao, X., Tao, D., Li, X.: 使用最大边际马尔可夫模型跟踪人体姿态。IEEE 图像处理汇刊
    24(12), 第5274–5287页 (2015)'
- en: 'Samanta and Chanda [2016] Samanta, S., Chanda, B.: A data-driven approach for
    human pose tracking based on spatio-temporal pictorial structure. arXiv preprint
    arXiv:1608.00199 (2016)'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Samanta 和 Chanda [2016] Samanta, S., Chanda, B.: 基于时空 pictorial 结构的人体姿态跟踪的数据驱动方法。arXiv
    预印本 arXiv:1608.00199 (2016)'
- en: 'Zhao et al. [2015] Zhao, L., Gao, X., Tao, D., Li, X.: Learning a tracking
    and estimation integrated graphical model for human pose tracking. IEEE transactions
    on neural networks and learning systems 26(12), 3176–3186 (2015)'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等 [2015] Zhao, L., Gao, X., Tao, D., Li, X.: 学习一个用于人体姿态跟踪的跟踪与估计集成图模型。IEEE
    神经网络与学习系统汇刊 26(12), 第3176–3186页 (2015)'
- en: 'Ma et al. [2016] Ma, M., Marturi, N., Li, Y., Stolkin, R., Leonardis, A.: A
    local-global coupled-layer puppet model for robust online human pose tracking.
    Computer Vision and Image Understanding 153, 163–178 (2016)'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等 [2016] Ma, M., Marturi, N., Li, Y., Stolkin, R., Leonardis, A.: 一种局部-全局耦合层木偶模型用于鲁棒的在线人体姿态跟踪。计算机视觉与图像理解
    153, 第163–178页 (2016)'
- en: 'Zhang et al. [2019] Zhang, J., Zhu, Z., Zou, W., Li, P., Li, Y., Su, H., Huang,
    G.: Fastpose: Towards real-time pose estimation and tracking via scale-normalized
    multi-task networks. arXiv preprint arXiv:1908.05593 (2019)'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2019] Zhang, J., Zhu, Z., Zou, W., Li, P., Li, Y., Su, H., Huang,
    G.: Fastpose: 通过尺度归一化多任务网络实现实时姿态估计和跟踪。arXiv 预印本 arXiv:1908.05593 (2019)'
- en: 'Ning et al. [2020] Ning, G., Pei, J., Huang, H.: Lighttrack: A generic framework
    for online top-down human pose tracking. In: CVPRW, pp. 1034–1035 (2020)'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ning 等 [2020] Ning, G., Pei, J., Huang, H.: Lighttrack: 一种用于在线自上而下人体姿态跟踪的通用框架。见于：CVPRW,
    第1034–1035页 (2020)'
- en: 'Rafi et al. [2020] Rafi, U., Doering, A., Leibe, B., Gall, J.: Self-supervised
    keypoint correspondences for multi-person pose estimation and tracking in videos.
    In: ECCV, pp. 36–52 (2020)'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rafi 等 [2020] Rafi, U., Doering, A., Leibe, B., Gall, J.: 用于视频中多人姿态估计和跟踪的自监督关键点对应。见于：ECCV,
    第36–52页 (2020)'
- en: 'Yang et al. [2021] Yang, Y., Ren, Z., Li, H., Zhou, C., Wang, X., Hua, G.:
    Learning dynamics via graph neural networks for human pose estimation and tracking.
    In: CVPR, pp. 8074–8084 (2021)'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等 [2021] Yang, Y., Ren, Z., Li, H., Zhou, C., Wang, X., Hua, G.: 通过图神经网络学习动态进行人体姿态估计和跟踪。见于：CVPR,
    第8074–8084页 (2021)'
- en: 'Doering and Gall [2023] Doering, A., Gall, J.: A gated attention transformer
    for multi-person pose tracking. In: ICCV, pp. 3189–3198 (2023)'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Doering 和 Gall [2023] Doering, A., Gall, J.: 一种用于多人姿态跟踪的门控注意力变换器。见于：ICCV, 第3189–3198页
    (2023)'
- en: 'Iqbal et al. [2017] Iqbal, U., Milan, A., Gall, J.: Posetrack: Joint multi-person
    pose estimation and tracking. In: CVPR, pp. 2011–2020 (2017)'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Iqbal 等 [2017] Iqbal, U., Milan, A., Gall, J.: Posetrack: 联合多人姿态估计与跟踪。见于：CVPR,
    第2011–2020页 (2017)'
- en: 'Raaj et al. [2019] Raaj, Y., Idrees, H., Hidalgo, G., Sheikh, Y.: Efficient
    online multi-person 2d pose tracking with recurrent spatio-temporal affinity fields.
    In: CVPR, pp. 4620–4628 (2019)'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raaj 等 [2019] Raaj, Y., Idrees, H., Hidalgo, G., Sheikh, Y.: 高效的在线多人2d姿态跟踪与递归时空相似场。见于：CVPR,
    第4620–4628页 (2019)'
- en: 'Bridgeman et al. [2019] Bridgeman, L., Volino, M., Guillemaut, J.-Y., Hilton,
    A.: Multi-person 3d pose estimation and tracking in sports. In: CVPRW, pp. 0–0
    (2019)'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bridgeman 等 [2019] Bridgeman, L., Volino, M., Guillemaut, J.-Y., Hilton, A.:
    体育运动中的多人3d姿态估计和跟踪。见于：CVPRW, 第0–0页 (2019)'
- en: 'Zanfir et al. [2018] Zanfir, A., Marinoiu, E., Sminchisescu, C.: Monocular
    3d pose and shape estimation of multiple people in natural scenes-the importance
    of multiple scene constraints. In: CVPR, pp. 2148–2157 (2018)'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zanfir 等 [2018] Zanfir, A., Marinoiu, E., Sminchisescu, C.: 单目 3D 姿态和形状估计多个自然场景中的人物——多个场景约束的重要性。发表于
    CVPR, 页 2148–2157 (2018)'
- en: 'Sun et al. [2019] Sun, X., Li, C., Lin, S.: Explicit spatiotemporal joint relation
    learning for tracking human pose. In: ICCV (2019)'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 [2019] Sun, X., Li, C., Lin, S.: 明确的空间时间关节关系学习用于跟踪人体姿态。发表于 ICCV (2019)'
- en: 'Reddy et al. [2021] Reddy, N.D., Guigues, L., Pishchulin, L., Eledath, J.,
    Narasimhan, S.G.: Tessetrack: End-to-end learnable multi-person articulated 3d
    pose tracking. In: CVPR, pp. 15190–15200 (2021)'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reddy 等 [2021] Reddy, N.D., Guigues, L., Pishchulin, L., Eledath, J., Narasimhan,
    S.G.: Tessetrack: 端到端可学习的多人关节 3D 姿态跟踪。发表于 CVPR, 页 15190–15200 (2021)'
- en: 'Zhang et al. [2022] Zhang, Y., Wang, C., Wang, X., Liu, W., Zeng, W.: Voxeltrack:
    Multi-person 3d human pose estimation and tracking in the wild. TPAMI 45(2), 2613–2626
    (2022)'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2022] Zhang, Y., Wang, C., Wang, X., Liu, W., Zeng, W.: Voxeltrack:
    多人 3D 人体姿态估计与跟踪。TPAMI 45(2), 2613–2626 (2022)'
- en: 'Zou et al. [2023] Zou, S., Xu, Y., Li, C., Ma, L., Cheng, L., Vo, M.: Snipper:
    A spatiotemporal transformer for simultaneous multi-person 3d pose estimation
    tracking and forecasting on a video snippet. TCSVT (2023)'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zou 等 [2023] Zou, S., Xu, Y., Li, C., Ma, L., Cheng, L., Vo, M.: Snipper: 一种用于视频片段中同时进行多人
    3D 姿态估计、跟踪和预测的空间时间变换器。TCSVT (2023)'
- en: 'Rajasegaran et al. [2022] Rajasegaran, J., Pavlakos, G., Kanazawa, A., Malik,
    J.: Tracking people by predicting 3d appearance, location and pose. In: CVPR,
    pp. 2740–2749 (2022)'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajasegaran 等 [2022] Rajasegaran, J., Pavlakos, G., Kanazawa, A., Malik, J.:
    通过预测 3D 外观、位置和姿态跟踪人员。发表于 CVPR, 页 2740–2749 (2022)'
- en: 'Wang and Wang [2017] Wang, H., Wang, L.: Modeling temporal dynamics and spatial
    configurations of actions using two-stream recurrent neural networks. In: CVPR,
    pp. 499–508 (2017)'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 和 Wang [2017] Wang, H., Wang, L.: 使用双流递归神经网络建模动作的时间动态和空间配置。发表于 CVPR, 页
    499–508 (2017)'
- en: 'Caetano et al. [2019] Caetano, C., Brémond, F., Schwartz, W.R.: Skeleton image
    representation for 3d action recognition based on tree structure and reference
    joints. In: SIBGRAPI, pp. 16–23 (2019)'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caetano 等 [2019] Caetano, C., Brémond, F., Schwartz, W.R.: 基于树结构和参考关节的 3D 动作识别的骨架图像表示。发表于
    SIBGRAPI, 页 16–23 (2019)'
- en: 'Yan et al. [2018] Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional
    networks for skeleton-based action recognition. In: AAAI, pp. 7444–7452 (2018)'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan 等 [2018] Yan, S., Xiong, Y., Lin, D.: 基于骨架的动作识别的空间时间图卷积网络。发表于 AAAI, 页 7444–7452
    (2018)'
- en: 'Plizzari et al. [2021] Plizzari, C., Cannici, M., Matteucci, M.: Spatial temporal
    transformer network for skeleton-based action recognition. In: ICPRW, pp. 694–701
    (2021)'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Plizzari 等 [2021] Plizzari, C., Cannici, M., Matteucci, M.: 基于骨架的动作识别的空间时间变换网络。发表于
    ICPRW, 页 694–701 (2021)'
- en: 'Jhuang et al. [2013] Jhuang, H., Gall, J., Zuffi, S., Schmid, C., Black, M.J.:
    Towards understanding action recognition. In: ICCV, pp. 3192–3199 (2013)'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jhuang 等 [2013] Jhuang, H., Gall, J., Zuffi, S., Schmid, C., Black, M.J.: 朝向理解动作识别。发表于
    ICCV, 页 3192–3199 (2013)'
- en: 'Chéron et al. [2015] Chéron, G., Laptev, I., Schmid, C.: P-cnn: Pose-based
    cnn features for action recognition. In: ICCV, pp. 3218–3226 (2015)'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chéron 等 [2015] Chéron, G., Laptev, I., Schmid, C.: P-cnn: 基于姿态的 cnn 特征用于动作识别。发表于
    ICCV, 页 3218–3226 (2015)'
- en: 'Zolfaghari et al. [2017] Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox,
    T.: Chained multi-stream networks exploiting pose, motion, and appearance for
    action classification and detection. In: ICCV, pp. 2904–2913 (2017)'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zolfaghari 等 [2017] Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox, T.:
    链式多流网络利用姿态、运动和外观进行动作分类和检测。发表于 ICCV, 页 2904–2913 (2017)'
- en: 'Choutas et al. [2018] Choutas, V., Weinzaepfel, P., Revaud, J., Schmid, C.:
    Potion: Pose motion representation for action recognition. In: CVPR, pp. 7024–7033
    (2018)'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choutas 等 [2018] Choutas, V., Weinzaepfel, P., Revaud, J., Schmid, C.: Potion:
    姿态运动表示用于动作识别。发表于 CVPR, 页 7024–7033 (2018)'
- en: 'Liu and Yuan [2018] Liu, M., Yuan, J.: Recognizing human actions as the evolution
    of pose estimation maps. In: CVPR, pp. 1159–1168 (2018)'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 和 Yuan [2018] Liu, M., Yuan, J.: 将人体动作识别为姿态估计图的演变。发表于 CVPR, 页 1159–1168
    (2018)'
- en: 'Moon et al. [2021] Moon, G., Kwon, H., Lee, K.M., Cho, M.: Integralaction:
    Pose-driven feature integration for robust human action recognition in videos.
    In: CVPR, pp. 3339–3348 (2021)'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moon 等 [2021] Moon, G., Kwon, H., Lee, K.M., Cho, M.: Integralaction: 驱动姿态特征融合用于视频中的鲁棒人体动作识别。发表于
    CVPR, 页 3339–3348 (2021)'
- en: 'Shah et al. [2022] Shah, A., Mishra, S., Bansal, A., Chen, J.-C., Chellappa,
    R., Shrivastava, A.: Pose and joint-aware action recognition. In: WACV, pp. 3850–3860
    (2022)'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shah等人 [2022] Shah, A., Mishra, S., Bansal, A., Chen, J.-C., Chellappa, R.,
    Shrivastava, A.: 姿态和关节感知的动作识别。在：WACV，页码3850–3860 (2022)'
- en: 'Duan et al. [2022] Duan, H., Zhao, Y., Chen, K., Lin, D., Dai, B.: Revisiting
    skeleton-based action recognition. In: CVPR, pp. 2969–2978 (2022)'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duan等人 [2022] Duan, H., Zhao, Y., Chen, K., Lin, D., Dai, B.: 再访骨架基础的动作识别。在：CVPR，页码2969–2978
    (2022)'
- en: 'Sato et al. [2023] Sato, F., Hachiuma, R., Sekii, T.: Prompt-guided zero-shot
    anomaly action recognition using pretrained deep skeleton features. In: CVPR,
    pp. 6471–6480 (2023)'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sato等人 [2023] Sato, F., Hachiuma, R., Sekii, T.: 使用预训练深度骨架特征的提示引导零样本异常动作识别。在：CVPR，页码6471–6480
    (2023)'
- en: 'Hachiuma et al. [2023] Hachiuma, R., Sato, F., Sekii, T.: Unified keypoint-based
    action recognition framework via structured keypoint pooling. In: CVPR, pp. 22962–22971
    (2023)'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hachiuma等人 [2023] Hachiuma, R., Sato, F., Sekii, T.: 通过结构化关键点池化的统一关键点基础动作识别框架。在：CVPR，页码22962–22971
    (2023)'
- en: 'Luvizon et al. [2018] Luvizon, D.C., Picard, D., Tabia, H.: 2d/3d pose estimation
    and action recognition using multitask deep learning. In: CVPR, pp. 5137–5146
    (2018)'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luvizon等人 [2018] Luvizon, D.C., Picard, D., Tabia, H.: 使用多任务深度学习进行2D/3D姿态估计和动作识别。在：CVPR，页码5137–5146
    (2018)'
- en: 'Foo et al. [2023] Foo, L.G., Li, T., Rahmani, H., Ke, Q., Liu, J.: Unified
    pose sequence modeling. In: CVPR, pp. 13019–13030 (2023)'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Foo等人 [2023] Foo, L.G., Li, T., Rahmani, H., Ke, Q., Liu, J.: 统一姿态序列建模。在：CVPR，页码13019–13030
    (2023)'
- en: 'Du et al. [2015] Du, Y., Fu, Y., Wang, L.: Skeleton based action recognition
    with convolutional neural network. In: ACPR, pp. 579–583 (2015)'
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du等人 [2015] Du, Y., Fu, Y., Wang, L.: 基于骨架的动作识别与卷积神经网络。在：ACPR，页码579–583 (2015)'
- en: 'Wang et al. [2016] Wang, P., Li, Z., Hou, Y., Li, W.: Action recognition based
    on joint trajectory maps using convolutional neural networks. In: ACMMM, pp. 102–106
    (2016)'
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人 [2016] Wang, P., Li, Z., Hou, Y., Li, W.: 基于联合轨迹图的动作识别与卷积神经网络。在：ACMMM，页码102–106
    (2016)'
- en: 'Hou et al. [2016] Hou, Y., Li, Z., Wang, P., Li, W.: Skeleton optical spectra-based
    action recognition using convolutional neural networks. TCSVT 28(3), 807–811 (2016)'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou等人 [2016] Hou, Y., Li, Z., Wang, P., Li, W.: 基于骨架光谱的动作识别与卷积神经网络。TCSVT 28(3),
    807–811 (2016)'
- en: 'Li et al. [2017] Li, C., Hou, Y., Wang, P., Li, W.: Joint distance maps based
    action recognition with convolutional neural networks. IEEE Signal Processing
    Letters 24(5), 624–628 (2017)'
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人 [2017] Li, C., Hou, Y., Wang, P., Li, W.: 基于联合距离图的动作识别与卷积神经网络。IEEE信号处理快报
    24(5), 624–628 (2017)'
- en: 'Liu et al. [2017] Liu, M., Liu, H., Chen, C.: Enhanced skeleton visualization
    for view invariant human action recognition. PR 68, 346–362 (2017)'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等人 [2017] Liu, M., Liu, H., Chen, C.: 增强骨架可视化以实现视角不变的人类动作识别。PR 68, 346–362
    (2017)'
- en: 'Ke et al. [2017] Ke, Q., An, S., Bennamoun, M., Sohel, F., Boussaid, F.: Skeletonnet:
    Mining deep part features for 3d action recognition. IEEE Signal Processing Letters
    (2017)'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ke等人 [2017] Ke, Q., An, S., Bennamoun, M., Sohel, F., Boussaid, F.: Skeletonnet：挖掘深度部件特征用于3D动作识别。IEEE信号处理快报
    (2017)'
- en: 'Li et al. [2019] Li, Y., Xia, R., Liu, X., Huang, Q.: Learning shape-motion
    representations from geometric algebra spatio-temporal model for skeleton-based
    action recognition. In: ICME, pp. 1066–1071 (2019)'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人 [2019] Li, Y., Xia, R., Liu, X., Huang, Q.: 从几何代数时空模型中学习形状-运动表示用于骨架基础的动作识别。在：ICME，页码1066–1071
    (2019)'
- en: 'Ding et al. [2017] Ding, Z., Wang, P., Ogunbona, P.O., Li, W.: Investigation
    of different skeleton features for cnn-based 3d action recognition. In: ICMEW,
    pp. 617–622 (2017)'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding等人 [2017] Ding, Z., Wang, P., Ogunbona, P.O., Li, W.: 对不同骨架特征进行CNN基础的3D动作识别研究。在：ICMEW，页码617–622
    (2017)'
- en: 'Ke et al. [2017] Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: A
    new representation of skeleton sequences for 3d action recognition. In: CVPR (2017)'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ke等人 [2017] Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: 用于3D动作识别的骨架序列新表示法。在：CVPR
    (2017)'
- en: 'Liang et al. [2019] Liang, D., Fan, G., Lin, G., Chen, W., Pan, X., Zhu, H.:
    Three-stream convolutional neural network with multi-task and ensemble learning
    for 3d action recognition. In: CVPRW, pp. 0–0 (2019)'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang等人 [2019] Liang, D., Fan, G., Lin, G., Chen, W., Pan, X., Zhu, H.: 三流卷积神经网络与多任务和集成学习用于3D动作识别。在：CVPRW，页码0–0
    (2019)'
- en: 'Liu et al. [2017] Liu, H., Tu, J., Liu, M.: Two-stream 3d convolutional neural
    network for skeleton-based action recognition. arXiv preprint arXiv:1705.08106
    (2017)'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等人 [2017] Liu, H., Tu, J., Liu, M.: 用于骨架基础的动作识别的双流3D卷积神经网络。arXiv预印本 arXiv:1705.08106
    (2017)'
- en: 'Hernandez Ruiz et al. [2017] Hernandez Ruiz, A., Porzi, L., Rota Bulò, S.,
    Moreno-Noguer, F.: 3d cnns on distance matrices for human action recognition.
    In: ACM MM, pp. 1087–1095 (2017)'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hernandez Ruiz 等人 [2017] Hernandez Ruiz, A., Porzi, L., Rota Bulò, S., Moreno-Noguer,
    F.: 基于距离矩阵的 3D CNNs 用于人体动作识别。发表于 ACM MM, 第 1087–1095 页 (2017)'
- en: 'Du et al. [2015] Du, Y., Wang, W., Wang, L.: Hierarchical recurrent neural
    network for skeleton based action recognition. In: CVPR, pp. 1110–1118 (2015)'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等人 [2015] Du, Y., Wang, W., Wang, L.: 基于骨架的动作识别的层次递归神经网络。发表于 CVPR, 第 1110–1118
    页 (2015)'
- en: 'Du et al. [2016] Du, Y., Fu, Y., Wang, L.: Representation learning of temporal
    dynamics for skeleton-based action recognition. IEEE Transactions on Image Processing
    25(7), 3010–3022 (2016)'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等人 [2016] Du, Y., Fu, Y., Wang, L.: 基于骨架的动作识别的时间动态表示学习。IEEE 图像处理学报 25(7),
    3010–3022 (2016)'
- en: 'Shahroudy et al. [2016] Shahroudy, A., Liu, J., Ng, T.-T., Wang, G.: NTU RGB+
    D: A large scale dataset for 3D human activity analysis. In: CVPR, pp. 1010–1019
    (2016)'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shahroudy 等人 [2016] Shahroudy, A., Liu, J., Ng, T.-T., Wang, G.: NTU RGB+ D:
    一个用于 3D 人体活动分析的大规模数据集。发表于 CVPR, 第 1010–1019 页 (2016)'
- en: 'Song et al. [2017] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An end-to-end
    spatio-temporal attention model for human action recognition from skeleton data.
    In: AAAI, pp. 4263–4270 (2017)'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等人 [2017] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: 一种端到端的时空注意力模型用于从骨架数据中识别人类动作。发表于
    AAAI, 第 4263–4270 页 (2017)'
- en: 'Liu et al. [2017] Liu, J., Wang, G., Hu, P., Duan, L.-Y., Kot, A.C.: Global
    context-aware attention lstm networks for 3d action recognition. In: CVPR, pp.
    1647–1656 (2017)'
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2017] Liu, J., Wang, G., Hu, P., Duan, L.-Y., Kot, A.C.: 用于 3D 动作识别的全局上下文感知注意力
    LSTM 网络。发表于 CVPR, 第 1647–1656 页 (2017)'
- en: 'Song et al. [2018] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: Spatio-temporal
    attention-based lstm networks for 3d action recognition and detection. TIP 27(7),
    3459–3471 (2018)'
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等人 [2018] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: 基于时空注意力的 LSTM
    网络用于 3D 动作识别和检测。TIP 27(7), 3459–3471 (2018)'
- en: 'Zhang et al. [2019] Zhang, P., Xue, J., Lan, C., Zeng, W., Gao, Z., Zheng,
    N.: Eleatt-rnn: Adding attentiveness to neurons in recurrent neural networks.
    IEEE Transactions on Image Processing 29, 1061–1073 (2019)'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2019] Zhang, P., Xue, J., Lan, C., Zeng, W., Gao, Z., Zheng, N.:
    Eleatt-rnn: 在递归神经网络中为神经元添加注意力。IEEE 图像处理学报 29, 1061–1073 (2019)'
- en: 'Si et al. [2019] Si, C., Chen, W., Wang, W., Wang, L., Tan, T.: An attention
    enhanced graph convolutional lstm network for skeleton-based action recognition.
    In: CVPR, pp. 1227–1236 (2019)'
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Si 等人 [2019] Si, C., Chen, W., Wang, W., Wang, L., Tan, T.: 一种增强注意力的图卷积 LSTM
    网络用于骨架动作识别。发表于 CVPR, 第 1227–1236 页 (2019)'
- en: 'Liu et al. [2016] Liu, J., Shahroudy, A., Xu, D., Wang, G.: Spatio-temporal
    LSTM with trust gates for 3D human action recognition. In: ECCV, pp. 816–833 (2016)'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2016] Liu, J., Shahroudy, A., Xu, D., Wang, G.: 带信任门的时空 LSTM 用于 3D
    人体动作识别。发表于 ECCV, 第 816–833 页 (2016)'
- en: 'Zhang et al. [2017] Zhang, S., Liu, X., Xiao, J.: On geometric features for
    skeleton-based action recognition using multilayer lstm networks. In: WACV, pp.
    148–157 (2017)'
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2017] Zhang, S., Liu, X., Xiao, J.: 基于几何特征的骨架动作识别使用多层 LSTM 网络。发表于
    WACV, 第 148–157 页 (2017)'
- en: 'Si et al. [2018] Si, C., Jing, Y., Wang, W., Wang, L., Tan, T.: Skeleton-based
    action recognition with spatial reasoning and temporal stack learning. In: ECCV,
    pp. 103–118 (2018)'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Si 等人 [2018] Si, C., Jing, Y., Wang, W., Wang, L., Tan, T.: 基于骨架的动作识别结合空间推理和时间堆叠学习。发表于
    ECCV, 第 103–118 页 (2018)'
- en: 'Huang et al. [2020] Huang, Z., Shen, X., Tian, X., Li, H., Huang, J., Hua,
    X.-S.: Spatio-temporal inception graph convolutional networks for skeleton-based
    action recognition. In: ACM MM, pp. 2122–2130 (2020)'
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等人 [2020] Huang, Z., Shen, X., Tian, X., Li, H., Huang, J., Hua, X.-S.:
    用于骨架动作识别的时空 inception 图卷积网络。发表于 ACM MM, 第 2122–2130 页 (2020)'
- en: 'Liu et al. [2020] Liu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W.: Disentangling
    and unifying graph convolutions for skeleton-based action recognition. In: CVPR,
    pp. 143–152 (2020)'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2020] Liu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W.: 解耦和统一图卷积用于骨架动作识别。发表于
    CVPR, 第 143–152 页 (2020)'
- en: 'Zhang et al. [2020] Zhang, X., Xu, C., Tao, D.: Context aware graph convolution
    for skeleton-based action recognition. In: CVPR, pp. 14333–14342 (2020)'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2020] Zhang, X., Xu, C., Tao, D.: 用于骨架动作识别的上下文感知图卷积。发表于 CVPR, 第 14333–14342
    页 (2020)'
- en: 'Li et al. [2019] Li, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q.:
    Actional-structural graph convolutional networks for skeleton-based action recognition.
    In: CVPR, pp. 3595–3603 (2019)'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2019] Li, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q.: 用于骨架动作识别的动作结构图卷积网络。发表于
    CVPR, 第 3595–3603 页 (2019)'
- en: 'Shi et al. [2019] Shi, L., Zhang, Y., Cheng, J., Lu, H.: Two-stream adaptive
    graph convolutional networks for skeleton-based action recognition. In: CVPR,
    pp. 12026–12035 (2019)'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人 [2019] Shi, L., Zhang, Y., Cheng, J., Lu, H.：用于骨架动作识别的双流自适应图卷积网络。发表于
    CVPR, 第12026–12035页 (2019)
- en: 'Cheng et al. [2020] Cheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., Lu,
    H.: Skeleton-based action recognition with shift graph convolutional network.
    In: CVPR, pp. 183–192 (2020)'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 [2020] Cheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., Lu, H.：基于骨架的动作识别与移位图卷积网络。发表于
    CVPR, 第183–192页 (2020)
- en: 'Korban and Li [2020] Korban, M., Li, X.: Ddgcn: A dynamic directed graph convolutional
    network for action recognition. In: ECCV, pp. 761–776 (2020)'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Korban 和 Li [2020] Korban, M., Li, X.：Ddgcn: 用于动作识别的动态有向图卷积网络。发表于 ECCV, 第761–776页
    (2020)'
- en: 'Chen et al. [2021] Chen, Y., Zhang, Z., Yuan, C., Li, B., Deng, Y., Hu, W.:
    Channel-wise topology refinement graph convolution for skeleton-based action recognition.
    In: ICCV, pp. 13359–13368 (2021)'
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2021] Chen, Y., Zhang, Z., Yuan, C., Li, B., Deng, Y., Hu, W.：用于骨架动作识别的通道级拓扑优化图卷积。发表于
    ICCV, 第13359–13368页 (2021)
- en: 'Chi et al. [2022] Chi, H.-g., Ha, M.H., Chi, S., Lee, S.W., Huang, Q., Ramani,
    K.: Infogcn: Representation learning for human skeleton-based action recognition.
    In: CVPR, pp. 20186–20196 (2022)'
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chi 等人 [2022] Chi, H.-g., Ha, M.H., Chi, S., Lee, S.W., Huang, Q., Ramani,
    K.：Infogcn: 基于人体骨架的动作识别的表示学习。发表于 CVPR, 第20186–20196页 (2022)'
- en: 'Duan et al. [2022] Duan, H., Wang, J., Chen, K., Lin, D.: Dg-stgcn: dynamic
    spatial-temporal modeling for skeleton-based action recognition. arXiv preprint
    arXiv:2210.05895 (2022)'
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duan 等人 [2022] Duan, H., Wang, J., Chen, K., Lin, D.：Dg-stgcn: 用于骨架动作识别的动态时空建模。arXiv
    预印本 arXiv:2210.05895 (2022)'
- en: 'Wang et al. [2022] Wang, S., Zhang, Y., Wei, F., Wang, K., Zhao, M., Jiang,
    Y.: Skeleton-based action recognition via temporal-channel aggregation. arXiv
    preprint arXiv:2205.15936 (2022)'
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] Wang, S., Zhang, Y., Wei, F., Wang, K., Zhao, M., Jiang, Y.：通过时间通道聚合进行骨架动作识别。arXiv
    预印本 arXiv:2205.15936 (2022)
- en: 'Wen et al. [2023] Wen, Y.-H., Gao, L., Fu, H., Zhang, F.-L., Xia, S., Liu,
    Y.-J.: Motif-gcns with local and non-local temporal blocks for skeleton-based
    action recognition. TPAMI 45(2), 2009–2023 (2023)'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等人 [2023] Wen, Y.-H., Gao, L., Fu, H., Zhang, F.-L., Xia, S., Liu, Y.-J.：Motif-gcns：带有局部和非局部时间块的骨架动作识别。TPAMI
    45(2), 2009–2023 (2023)
- en: 'Lin et al. [2023] Lin, L., Zhang, J., Liu, J.: Actionlet-dependent contrastive
    learning for unsupervised skeleton-based action recognition. In: CVPR, pp. 2363–2372
    (2023)'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2023] Lin, L., Zhang, J., Liu, J.：基于动作片段的对比学习用于无监督骨架动作识别。发表于 CVPR, 第2363–2372页
    (2023)
- en: 'Li et al. [2022] Li, Z., Gong, X., Song, R., Duan, P., Liu, J., Zhang, W.:
    SMAM: Self and mutual adaptive matching for skeleton-based few-shot action recognition.
    TIP 32, 392–402 (2022)'
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2022] Li, Z., Gong, X., Song, R., Duan, P., Liu, J., Zhang, W.：SMAM:
    自适应匹配与互适应匹配用于骨架少样本动作识别。TIP 32, 392–402 (2022)'
- en: 'Dai et al. [2023] Dai, M., Sun, Z., Wang, T., Feng, J., Jia, K.: Global spatio-temporal
    synergistic topology learning for skeleton-based action recognition. PR 140, 109540
    (2023)'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人 [2023] Dai, M., Sun, Z., Wang, T., Feng, J., Jia, K.：用于骨架动作识别的全球时空协同拓扑学习。PR
    140, 109540 (2023)
- en: 'Zhu et al. [2023] Zhu, Y., Shuai, H., Liu, G., Liu, Q.: Multilevel spatial–temporal
    excited graph network for skeleton-based action recognition. TIP 32, 496–508 (2023)'
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2023] Zhu, Y., Shuai, H., Liu, G., Liu, Q.：用于骨架动作识别的多层次时空激发图网络。TIP 32,
    496–508 (2023)
- en: 'Shu et al. [2023] Shu, X., Xu, B., Zhang, L., Tang, J.: Multi-granularity anchor-contrastive
    representation learning for semi-supervised skeleton-based action recognition.
    TPAMI 45(6), 7559–7576 (2023)'
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 等人 [2023] Shu, X., Xu, B., Zhang, L., Tang, J.：用于半监督骨架动作识别的多粒度锚点对比表示学习。TPAMI
    45(6), 7559–7576 (2023)
- en: 'Wu et al. [2023] Wu, L., Zhang, C., Zou, Y.: Spatiotemporal focus for skeleton-based
    action recognition. PR 136, 109231 (2023)'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2023] Wu, L., Zhang, C., Zou, Y.：用于骨架动作识别的时空关注。PR 136, 109231 (2023)
- en: 'Zhang et al. [2020] Zhang, P., Lan, C., Zeng, W., Xing, J., Xue, J., Zheng,
    N.: Semantics-guided neural networks for efficient skeleton-based human action
    recognition. In: CVPR, pp. 1112–1121 (2020)'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2020] Zhang, P., Lan, C., Zeng, W., Xing, J., Xue, J., Zheng, N.：用于高效骨架人体动作识别的语义引导神经网络。发表于
    CVPR, 第1112–1121页 (2020)
- en: 'Ye et al. [2020] Ye, F., Pu, S., Zhong, Q., Li, C., Xie, D., Tang, H.: Dynamic
    gcn: Context-enriched topology learning for skeleton-based action recognition.
    In: ACM MM, pp. 55–63 (2020)'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye 等人 [2020] Ye, F., Pu, S., Zhong, Q., Li, C., Xie, D., Tang, H.：动态 gcn: 基于骨架的动作识别的上下文丰富拓扑学习。发表于
    ACM MM, 第55–63页 (2020)'
- en: 'Wang et al. [2023] Wang, M., Ni, B., Yang, X.: Learning multi-view interactional
    skeleton graph for action recognition. TPAMI 45(6), 6940–6954 (2023)'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2023] Wang, M., Ni, B., Yang, X.: 学习多视角交互骨架图进行动作识别。TPAMI 45(6), 6940–6954
    (2023)'
- en: 'Li et al. [2023] Li, S., He, X., Song, W., Hao, A., Qin, H.: Graph diffusion
    convolutional network for skeleton based semantic recognition of two-person actions.
    TPAMI 45(7), 8477–8493 (2023)'
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2023] Li, S., He, X., Song, W., Hao, A., Qin, H.: 基于图的扩散卷积网络用于两人动作的语义识别。TPAMI
    45(7), 8477–8493 (2023)'
- en: 'Xu et al. [2023] Xu, H., Gao, Y., Hui, Z., Li, J., Gao, X.: Language knowledge-assisted
    representation learning for skeleton-based action recognition. arXiv preprint
    arXiv:2305.12398 (2023)'
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 [2023] Xu, H., Gao, Y., Hui, Z., Li, J., Gao, X.: 语言知识辅助的表示学习用于骨架基础动作识别。arXiv
    预印本 arXiv:2305.12398 (2023)'
- en: 'Wang et al. [2023] Wang, X., Xu, X., Mu, Y.: Neural koopman pooling: Control-inspired
    temporal dynamics encoding for skeleton-based action recognition. In: CVPR, pp.
    10597–10607 (2023)'
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2023] Wang, X., Xu, X., Mu, Y.: 神经 Koopman 池化：受控启发的时间动态编码用于骨架基础动作识别。发表于:
    CVPR, 第10597–10607页 (2023)'
- en: 'Zhou et al. [2023] Zhou, H., Liu, Q., Wang, Y.: Learning discriminative representations
    for skeleton based action recognition. In: CVPR, pp. 10608–10617 (2023)'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2023] Zhou, H., Liu, Q., Wang, Y.: 学习区分性表示用于骨架基础的动作识别。发表于: CVPR, 第10608–10617页
    (2023)'
- en: 'Dosovitskiy et al. [2020] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al.: An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929 (2020)'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dosovitskiy 等人 [2020] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., 等.: 一张图片值16x16个单词：大规模图像识别的变换器。arXiv 预印本 arXiv:2010.11929 (2020)'
- en: 'Shi et al. [2020] Shi, L., Zhang, Y., Cheng, J., Lu, H.: Decoupled spatial-temporal
    attention network for skeleton-based action-gesture recognition. In: ACCV (2020)'
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等人 [2020] Shi, L., Zhang, Y., Cheng, J., Lu, H.: 解耦空间-时间注意网络用于骨架基础动作-手势识别。发表于:
    ACCV (2020)'
- en: 'Wang et al. [2021] Wang, Q., Peng, J., Shi, S., Liu, T., He, J., Weng, R.:
    Iip-transformer: Intra-inter-part transformer for skeleton-based action recognition.
    arXiv preprint arXiv:2110.13385 (2021)'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2021] Wang, Q., Peng, J., Shi, S., Liu, T., He, J., Weng, R.: Iip-transformer:
    用于骨架基础动作识别的内部-交互部分变换器。arXiv 预印本 arXiv:2110.13385 (2021)'
- en: 'Ijaz et al. [2022] Ijaz, M., Diaz, R., Chen, C.: Multimodal transformer for
    nursing activity recognition. In: CVPR, pp. 2065–2074 (2022)'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ijaz 等人 [2022] Ijaz, M., Diaz, R., Chen, C.: 多模态变换器用于护理活动识别。发表于: CVPR, 第2065–2074页
    (2022)'
- en: 'Zhang et al. [2021] Zhang, Y., Wu, B., Li, W., Duan, L., Gan, C.: Stst: Spatial-temporal
    specialized transformer for skeleton-based action recognition. In: ACMMM, pp.
    3229–3237 (2021)'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2021] Zhang, Y., Wu, B., Li, W., Duan, L., Gan, C.: Stst: 用于骨架基础动作识别的空间-时间专用变换器。发表于:
    ACMMM, 第3229–3237页 (2021)'
- en: 'Shi et al. [2021] Shi, F., Lee, C., Qiu, L., Zhao, Y., Shen, T., Muralidhar,
    S., Han, T., Zhu, S.-C., Narayanan, V.: Star: Sparse transformer-based action
    recognition. arXiv preprint arXiv:2107.07089 (2021)'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等人 [2021] Shi, F., Lee, C., Qiu, L., Zhao, Y., Shen, T., Muralidhar, S.,
    Han, T., Zhu, S.-C., Narayanan, V.: Star: 稀疏变换器基础的动作识别。arXiv 预印本 arXiv:2107.07089
    (2021)'
- en: 'Gedamu et al. [2023] Gedamu, K., Ji, Y., Gao, L., Yang, Y., Shen, H.T.: Relation-mining
    self-attention network for skeleton-based human action recognition. PR 139, 109455
    (2023)'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gedamu 等人 [2023] Gedamu, K., Ji, Y., Gao, L., Yang, Y., Shen, H.T.: 关系挖掘自注意网络用于骨架基础人类动作识别。PR
    139, 109455 (2023)'
- en: 'Zhou et al. [2022] Zhou, Y., Li, C., Cheng, Z.-Q., Geng, Y., Xie, X., Keuper,
    M.: Hypergraph transformer for skeleton-based action recognition. arXiv preprint
    arXiv:2211.09590 (2022)'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2022] Zhou, Y., Li, C., Cheng, Z.-Q., Geng, Y., Xie, X., Keuper, M.:
    超图变换器用于骨架基础动作识别。arXiv 预印本 arXiv:2211.09590 (2022)'
- en: 'Qiu et al. [2022] Qiu, H., Hou, B., Ren, B., Zhang, X.: Spatio-temporal tuples
    transformer for skeleton-based action recognition. arXiv preprint arXiv:2201.02849
    (2022)'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu 等人 [2022] Qiu, H., Hou, B., Ren, B., Zhang, X.: 时空元组变换器用于骨架基础动作识别。arXiv
    预印本 arXiv:2201.02849 (2022)'
- en: 'Kong et al. [2022] Kong, J., Bian, Y., Jiang, M.: Mtt: Multi-scale temporal
    transformer for skeleton-based action recognition. IEEE Signal Processing Letters
    29, 528–532 (2022)'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kong 等人 [2022] Kong, J., Bian, Y., Jiang, M.: Mtt: 用于骨架基础动作识别的多尺度时间变换器。IEEE
    信号处理快报 29, 528–532 (2022)'
- en: 'Zhang et al. [2022] Zhang, J., Jia, Y., Xie, W., Tu, Z.: Zoom transformer for
    skeleton-based group activity recognition. TCSVT 32(12), 8646–8659 (2022)'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2022] Zhang, J., Jia, Y., Xie, W., Tu, Z.: 用于骨架基础群体活动识别的缩放变换器。TCSVT
    32(12), 8646–8659 (2022)'
- en: 'Gao et al. [2022] Gao, Z., Wang, P., Lv, P., Jiang, X., Liu, Q., Wang, P.,
    Xu, M., Li, W.: Focal and global spatial-temporal transformer for skeleton-based
    action recognition. In: ACCV, pp. 382–398 (2022)'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2022] 高泽、王鹏、吕平、蒋欣、刘青、王鹏、徐敏、李伟：《用于骨架基动作识别的焦点和全局时空变换器》。在：ACCV, pp. 382–398
    (2022)
- en: 'Liu et al. [2022] Liu, Y., Zhang, H., Xu, D., He, K.: Graph transformer network
    with temporal kernel attention for skeleton-based action recognition. Knowledge-Based
    Systems 240, 108146 (2022)'
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2022] 刘勇、张辉、徐东、贺凯：《具有时间内核注意力的图变换器网络用于基于骨架的动作识别》。知识基础系统 240, 108146 (2022)
- en: 'Pang et al. [2022] Pang, Y., Ke, Q., Rahmani, H., Bailey, J., Liu, J.: Igformer:
    Interaction graph transformer for skeleton-based human interaction recognition.
    In: ECCV, pp. 605–622 (2022)'
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 等人 [2022] 庞毅、柯强、拉赫马尼、贝利、刘军：《Igformer：用于骨架基人类互动识别的交互图变换器》。在：ECCV, pp. 605–622
    (2022)
- en: 'Duan et al. [2023] Duan, H., Xu, M., Shuai, B., Modolo, D., Tu, Z., Tighe,
    J., Bergamo, A.: Skeletr: Towards skeleton-based action recognition in the wild.
    In: ICCV, pp. 13634–13644 (2023)'
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段等人 [2023] 段辉、徐敏、帅博、莫多洛、涂志、泰格、贝尔加莫：《Skeletr：面向野外的骨架基动作识别》。在：ICCV, pp. 13634–13644
    (2023)
- en: 'Kim et al. [2022] Kim, B., Chang, H.J., Kim, J., Choi, J.Y.: Global-local motion
    transformer for unsupervised skeleton-based action learning. In: ECCV, pp. 209–225
    (2022)'
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人 [2022] Kim, B., Chang, H.J., Kim, J., Choi, J.Y.: 全局-局部运动变换器用于无监督骨架基动作学习。在：ECCV,
    pp. 209–225 (2022)'
- en: 'Dong et al. [2023] Dong, J., Sun, S., Liu, Z., Chen, S., Liu, B., Wang, X.:
    Hierarchical contrast for unsupervised skeleton-based action representation learning.
    In: AAAI, pp. 525–533 (2023)'
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人 [2023] 董军、孙思、刘智、陈尚、刘斌、王旭：《无监督骨架基动作表示学习的层次对比》。在：AAAI, pp. 525–533 (2023)
- en: 'Shah et al. [2023] Shah, A., Roy, A., Shah, K., Mishra, S., Jacobs, D., Cherian,
    A., Chellappa, R.: Halp: Hallucinating latent positives for skeleton-based self-supervised
    learning of actions. In: CVPR, pp. 18846–18856 (2023)'
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shah 等人 [2023] Shah, A., Roy, A., Shah, K., Mishra, S., Jacobs, D., Cherian,
    A., Chellappa, R.: Halp：为骨架基自监督动作学习幻觉潜在正样本。在：CVPR, pp. 18846–18856 (2023)'
- en: 'Cheng et al. [2021] Cheng, Y.-B., Chen, X., Zhang, D., Lin, L.: Motion-transformer:
    Self-supervised pre-training for skeleton-based action recognition. In: ACM MM,
    pp. 1–6 (2021)'
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 [2021] 程永标、陈曦、张大伟、林琳：《Motion-transformer：用于骨架基动作识别的自监督预训练》。在：ACM MM,
    pp. 1–6 (2021)
- en: 'Wu et al. [2023] Wu, W., Hua, Y., Zheng, C., Wu, S., Chen, C., Lu, A.: Skeletonmae:
    Spatial-temporal masked autoencoders for self-supervised skeleton action recognition.
    In: ICMEW, pp. 224–229 (2023)'
  id: totrans-1066
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2023] 吴伟、华扬、郑城、吴淑、陈超、陆安：《Skeletonmae：用于自监督骨架动作识别的时空掩蔽自编码器》。在：ICMEW, pp.
    224–229 (2023)
- en: 'Hua et al. [2023] Hua, Y., Wu, W., Zheng, C., Lu, A., Liu, M., Chen, C., Wu,
    S.: Part aware contrastive learning for self-supervised action recognition. In:
    IJCAI, pp. 855–863 (2023)'
  id: totrans-1067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua 等人 [2023] 华扬、吴伟、郑城、陆安、刘敏、陈超、吴淑：《面向自监督动作识别的部件感知对比学习》。在：IJCAI, pp. 855–863
    (2023)
- en: 'Johnson and Everingham [2010] Johnson, S., Everingham, M.: Clustered pose and
    nonlinear appearance models for human pose estimation. In: BMVC, p. 5 (2010)'
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Johnson 和 Everingham [2010] Johnson, S., Everingham, M.: 用于人体姿态估计的聚类姿态和非线性外观模型。在：BMVC,
    p. 5 (2010)'
- en: 'Johnson and Everingham [2011] Johnson, S., Everingham, M.: Learning effective
    human pose estimation from inaccurate annotation. In: CVPR, pp. 1465–1472 (2011)'
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Johnson 和 Everingham [2011] Johnson, S., Everingham, M.: 从不准确的标注中学习有效的人体姿态估计。在：CVPR,
    pp. 1465–1472 (2011)'
- en: 'Sapp and Taskar [2013] Sapp, B., Taskar, B.: Modec: Multimodal decomposable
    models for human pose estimation. In: CVPR, pp. 3674–3681 (2013)'
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sapp 和 Taskar [2013] Sapp, B., Taskar, B.: Modec: 多模态可分解模型用于人体姿态估计。 在：CVPR,
    pp. 3674–3681 (2013)'
- en: 'Andriluka et al. [2014] Andriluka, M., Pishchulin, L., Gehler, P., Schiele,
    B.: 2d human pose estimation: New benchmark and state of the art analysis. In:
    CVPR, pp. 3686–3693 (2014)'
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Andriluka 等人 [2014] Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B.:
    2D人体姿态估计：新的基准和最新分析。在：CVPR, pp. 3686–3693 (2014)'
- en: 'Lin et al. [2014] Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context.
    In: ECCV, pp. 740–755 (2014)'
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2014] 林腾跃、马伊雷、贝隆吉、海斯、佩罗纳、拉马南、杜拉、齐特尼克：《Microsoft COCO：上下文中的常见物体》。在：ECCV,
    pp. 740–755 (2014)
- en: 'Gong et al. [2017] Gong, K., Liang, X., Zhang, D., Shen, X., Lin, L.: Look
    into person: Self-supervised structure-sensitive learning and a new benchmark
    for human parsing. In: CVPR, pp. 932–940 (2017)'
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等人 [2017] 龚凯、梁旭、张大伟、申学、林琳：《Look into person：自监督结构敏感学习和人体解析的新基准》。在：CVPR,
    pp. 932–940 (2017)
- en: 'Li et al. [2019] Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.-S., Lu, C.: Crowdpose:
    Efficient crowded scenes pose estimation and a new benchmark. In: CVPR, pp. 10863–10872
    (2019)'
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2019] Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.-S., Lu, C.: Crowdpose:
    高效的拥挤场景姿态估计和新的基准。在: CVPR, pp. 10863–10872 (2019)'
- en: 'Zhang et al. [2013] Zhang, W., Zhu, M., Derpanis, K.G.: From actemes to action:
    A strongly-supervised representation for detailed action understanding. In: ICCV,
    pp. 2248–2255 (2013)'
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2013] Zhang, W., Zhu, M., Derpanis, K.G.: 从 actemes 到动作：用于详细动作理解的强监督表示。在:
    ICCV, pp. 2248–2255 (2013)'
- en: 'Andriluka et al. [2018] Andriluka, M., Iqbal, U., Insafutdinov, E., Pishchulin,
    L., Milan, A., Gall, J., Schiele, B.: Posetrack: A benchmark for human pose estimation
    and tracking. In: CVPR, pp. 5167–5176 (2018)'
  id: totrans-1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Andriluka 等人 [2018] Andriluka, M., Iqbal, U., Insafutdinov, E., Pishchulin,
    L., Milan, A., Gall, J., Schiele, B.: Posetrack: 人体姿态估计和跟踪的基准测试。在: CVPR, pp. 5167–5176
    (2018)'
- en: 'Doering et al. [2022] Doering, A., Chen, D., Zhang, S., Schiele, B., Gall,
    J.: Posetrack21: A dataset for person search, multi-object tracking and multi-person
    pose tracking. In: CVPR, pp. 20963–20972 (2022)'
  id: totrans-1077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Doering 等人 [2022] Doering, A., Chen, D., Zhang, S., Schiele, B., Gall, J.:
    Posetrack21: 用于行人搜索、多目标跟踪和多人姿态跟踪的数据集。在: CVPR, pp. 20963–20972 (2022)'
- en: 'Sigal et al. [2010] Sigal, L., Balan, A.O., Black, M.J.: Humaneva: Synchronized
    video and motion capture dataset and baseline algorithm for evaluation of articulated
    human motion. IJCV 87(1-2), 4–27 (2010)'
  id: totrans-1078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sigal 等人 [2010] Sigal, L., Balan, A.O., Black, M.J.: Humaneva: 同步视频和运动捕捉数据集及用于评估关节人体运动的基准算法。IJCV
    87(1-2), 4–27 (2010)'
- en: 'Ionescu et al. [2013] Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.:
    Human3\. 6m: Large scale datasets and predictive methods for 3d human sensing
    in natural environments. TPAMI 36(7), 1325–1339 (2013)'
  id: totrans-1079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ionescu 等人 [2013] Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3\.
    6m: 大规模数据集和自然环境下 3D 人体感测的预测方法。TPAMI 36(7), 1325–1339 (2013)'
- en: 'Joo et al. [2017] Joo, H., Simon, T., Li, X., Liu, H., Tan, L., Gui, L., Banerjee,
    S., Godisart, T., Nabbe, B., Matthews, I., et al.: Panoptic studio: A massively
    multiview system for social interaction capture. TPAMI 41(1), 190–204 (2017)'
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joo 等人 [2017] Joo, H., Simon, T., Li, X., Liu, H., Tan, L., Gui, L., Banerjee,
    S., Godisart, T., Nabbe, B., Matthews, I., 等: Panoptic studio: 一个用于社会互动捕捉的大规模多视角系统。TPAMI
    41(1), 190–204 (2017)'
- en: 'von Marcard et al. [2018] Marcard, T., Henschel, R., Black, M.J., Rosenhahn,
    B., Pons-Moll, G.: Recovering accurate 3d human pose in the wild using imus and
    a moving camera. In: ECCV, pp. 601–617 (2018)'
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'von Marcard 等人 [2018] Marcard, T., Henschel, R., Black, M.J., Rosenhahn, B.,
    Pons-Moll, G.: 使用 IMU 和移动相机在野外恢复准确的 3D 人体姿态。在: ECCV, pp. 601–617 (2018)'
- en: 'Sapp et al. [2011] Sapp, B., Weiss, D., Taskar, B.: Parsing human motion with
    stretchable models. In: CVPR 2011, pp. 1281–1288 (2011)'
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sapp 等人 [2011] Sapp, B., Weiss, D., Taskar, B.: 使用可拉伸模型解析人体运动。在: CVPR 2011,
    pp. 1281–1288 (2011)'
- en: 'Berclaz et al. [2011] Berclaz, J., Fleuret, F., Turetken, E., Fua, P.: Multiple
    object tracking using k-shortest paths optimization. TPAMI 33(9), 1806–1819 (2011)'
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Berclaz 等人 [2011] Berclaz, J., Fleuret, F., Turetken, E., Fua, P.: 使用 k 最短路径优化的多目标跟踪。TPAMI
    33(9), 1806–1819 (2011)'
- en: 'Ramakrishna et al. [2013] Ramakrishna, V., Kanade, T., Sheikh, Y.: Tracking
    human pose by tracking symmetric parts. In: CVPR, pp. 3728–3735 (2013)'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramakrishna 等人 [2013] Ramakrishna, V., Kanade, T., Sheikh, Y.: 通过跟踪对称部位跟踪人体姿态。在:
    CVPR, pp. 3728–3735 (2013)'
- en: 'Weiss et al. [2010] Weiss, D., Sapp, B., Taskar, B.: Sidestepping intractable
    inference with structured ensemble cascades. NIPS 23 (2010)'
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Weiss 等人 [2010] Weiss, D., Sapp, B., Taskar, B.: 避免难以处理的推断与结构化集成级联。NIPS 23
    (2010)'
- en: 'Belagiannis et al. [2014] Belagiannis, V., Amin, S., Andriluka, M., Schiele,
    B., Navab, N., Ilic, S.: 3d pictorial structures for multiple human pose estimation.
    In: CVPR, pp. 1669–1676 (2014)'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Belagiannis 等人 [2014] Belagiannis, V., Amin, S., Andriluka, M., Schiele, B.,
    Navab, N., Ilic, S.: 用于多人体姿态估计的 3D 图形结构。在: CVPR, pp. 1669–1676 (2014)'
- en: 'Müller et al. [2007] Müller, M., Röder, T., Clausen, M., Eberhardt, B., Krüger,
    B., Weber, A.: Mocap database hdm05. Institut für Informatik II, Universität Bonn
    2(7) (2007)'
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Müller 等人 [2007] Müller, M., Röder, T., Clausen, M., Eberhardt, B., Krüger,
    B., Weber, A.: Mocap 数据库 hdm05。Institut für Informatik II, Universität Bonn 2(7)
    (2007)'
- en: 'Li et al. [2010] Li, W., Zhang, Z., Liu, Z.: Action recognition based on a
    bag of 3D points. In: CVPRW, pp. 9–14 (2010)'
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2010] Li, W., Zhang, Z., Liu, Z.: 基于 3D 点袋的动作识别。在: CVPRW, pp. 9–14 (2010)'
- en: 'Fothergill et al. [2012] Fothergill, S., Mentis, H., Kohli, P., Nowozin, S.:
    Instructing people for training gestural interactive systems. In: Proceedings
    of the SIGCHI Conference on Human Factors in Computing Systems, pp. 1737–1746
    (2012)'
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fothergill 等人 [2012] Fothergill, S., Mentis, H., Kohli, P., Nowozin, S.: 指导人们进行手势互动系统的培训。在:
    SIGCHI 计算机系统人因会议论文集, pp. 1737–1746 (2012)'
- en: 'Bloom et al. [2012] Bloom, V., Makris, D., Argyriou, V.: G3d: A gaming action
    dataset and real time action recognition evaluation framework. In: CVPR, pp. 7–12
    (2012)'
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布鲁姆等人 [2012] 布鲁姆、马克里斯、阿尔吉留：G3D：一个游戏动作数据集及实时动作识别评估框架。会议论文：CVPR，第 7–12 页 (2012)
- en: 'Yun et al. [2012] Yun, K., Honorio, J., Chattopadhyay, D., Berg, T.L., Samaras,
    D.: Two-person interaction detection using body-pose features and multiple instance
    learning. In: CVPRW, pp. 28–35 (2012)'
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云等人 [2012] 云开、霍诺里奥、查托帕德亚、伯格、萨马拉斯：使用身体姿态特征和多实例学习进行两人交互检测。会议论文：CVPRW，第 28–35 页
    (2012)
- en: 'Xia et al. [2012] Xia, L., Chen, C.-C., Aggarwal, J.: View invariant human
    action recognition using histograms of 3D joints. In: CVPRW, pp. 20–27 (2012)'
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 夏等人 [2012] 夏磊、陈昌鑫、阿加瓦尔：使用 3D 关节直方图进行视角不变的人体动作识别。会议论文：CVPRW，第 20–27 页 (2012)
- en: 'Wang et al. [2014] Wang, J., Nie, X., Xia, Y., Wu, Y., Zhu, S.-C.: Cross-view
    action modeling, learning and recognition. In: CVPR, pp. 2649–2656 (2014)'
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2014] 王建、聂晓、夏阳、吴勇、朱圣慈：跨视角动作建模、学习和识别。会议论文：CVPR，第 2649–2656 页 (2014)
- en: 'Chen et al. [2015] Chen, C., Jafari, R., Kehtarnavaz, N.: Utd-mhad: A multimodal
    dataset for human action recognition utilizing a depth camera and a wearable inertial
    sensor. In: ICIP, pp. 168–172 (2015)'
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2015] 陈诚、贾法里、凯赫塔尔纳瓦兹：UTD-MHAD：一个多模态数据集，用于结合深度相机和可穿戴惯性传感器的人体动作识别。会议论文：ICIP，第
    168–172 页 (2015)
- en: 'Hu et al. [2015] Hu, J.-F., Zheng, W.-S., Lai, J., Zhang, J.: Jointly learning
    heterogeneous features for RGB-D activity recognition. In: CVPR (2015)'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等人 [2015] 胡建丰、郑伟森、赖江、张俊：联合学习异质特征用于 RGB-D 活动识别。会议论文：CVPR (2015)
- en: 'Chunhui et al. [2017] Chunhui, L., Yueyu, H., Yanghao, L., Sijie, S., Jiaying,
    L.: Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding.
    arXiv preprint arXiv:1703.07475 (2017)'
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 春晖等人 [2017] 春晖、岳雨、杨浩、斯杰、佳颖：PKU-MMD：用于连续多模态人体动作理解的大规模基准。arXiv 预印本 arXiv:1703.07475
    (2017)
- en: 'Kay et al. [2017] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier,
    C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.:
    The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017)'
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凯等人 [2017] 凯威、卡雷拉、斯莫尼扬、张博、希利尔、维贾纳拉西曼、维奥拉、格林、巴克、纳特塞夫等：Kinetics 人体动作视频数据集。arXiv
    预印本 arXiv:1705.06950 (2017)
- en: 'Liu et al. [2019] Liu, J., Shahroudy, A., Perez, M., Wang, G., Duan, L.-Y.,
    Kot, A.C.: Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding.
    TPAMI 42(10), 2684–2701 (2019)'
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2019] 刘俊、沙赫鲁迪、佩雷斯、王刚、段林阳、科特：Ntu rgb+ d 120：用于 3D 人体活动理解的大规模基准。TPAMI 42(10)，2684–2701
    (2019)
- en: 'Li et al. [2017] Li, B., Dai, Y., Cheng, X., Chen, H., Lin, Y., He, M.: Skeleton
    based action recognition using translation-scale invariant image mapping and multi-scale
    deep cnn. In: ICMEW, pp. 601–604 (2017)'
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2017] 李斌、戴阳、程鑫、陈浩、林勇、何明：基于骨架的动作识别使用平移尺度不变图像映射和多尺度深度 CNN。会议论文：ICMEW，第 601–604
    页 (2017)
- en: 'Cheng et al. [2020] Cheng, K., Zhang, Y., Cao, C., Shi, L., Cheng, J., Lu,
    H.: Decoupling gcn with dropgraph module for skeleton-based action recognition.
    In: ECCV, pp. 1–18 (2020)'
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程等人 [2020] 程凯、张艳、曹昌、石磊、程建、卢辉：使用 DropGraph 模块解耦 GCN 进行基于骨架的动作识别。会议论文：ECCV，第 1–18
    页 (2020)
- en: 'Jiang et al. [2022] Jiang, Y., Sun, Z., Yu, S., Wang, S., Song, Y.: A graph
    skeleton transformer network for action recognition. Symmetry 14(8), 1547 (2022)'
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姜等人 [2022] 姜阳、孙哲、余硕、王石、宋阳：一种用于动作识别的图骨架变换器网络。对称性 14(8)，1547 (2022)
- en: 'Dong et al. [2019] Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast
    and robust multi-person 3d pose estimation from multiple views. In: CVPR, pp.
    7792–7801 (2019)'
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 董等人 [2019] 董静、姜威、黄强、鲍华、周晓：快速而鲁棒的多视角多人的 3D 姿态估计。会议论文：CVPR，第 7792–7801 页 (2019)
- en: 'Tu et al. [2020] Tu, H., Wang, C., Zeng, W.: Voxelpose: Towards multi-camera
    3d human pose estimation in wild environment. In: ECCV, pp. 197–212 (2020)'
  id: totrans-1103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图等人 [2020] 图辉、王成、曾伟：VoxelPose：面向复杂环境中的多相机 3D 人体姿态估计。会议论文：ECCV，第 197–212 页 (2020)
- en: 'Zhang et al. [2021] Zhang, J., Cai, Y., Yan, S., Feng, J., et al.: Direct multi-view
    multi-person 3d pose estimation. NIPS 34, 13153–13164 (2021)'
  id: totrans-1104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2021] 张静、蔡颖、阎松、冯俊等：直接的多视角多人的 3D 姿态估计。NIPS 34，13153–13164 (2021)
- en: 'Shah et al. [2019] Shah, S., Jain, N., Sharma, A., Jain, A.: On the robustness
    of human pose estimation. In: CVPRW (2019)'
  id: totrans-1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沙赫等人 [2019] 沙赫、简娜、香玛、简安：关于人体姿态估计的鲁棒性。会议论文：CVPRW (2019)
- en: 'Zhang et al. [2020] Zhang, Z., Wang, C., Qin, W., Zeng, W.: Fusing wearable
    imus with multi-view images for human pose estimation: A geometric approach. In:
    CVPR, pp. 2200–2209 (2020)'
  id: totrans-1106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2020] 张志、王成、秦伟、曾伟：融合可穿戴 IMU 和多视角图像进行人体姿态估计：一种几何方法。会议论文：CVPR，第 2200–2209
    页 (2020)
- en: 'Wang et al. [2022a] Wang, C., Zhang, F., Zhu, X., Ge, S.S.: Low-resolution
    human pose estimation. PR 126(10857), 108579 (2022)'
  id: totrans-1107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2022a] Wang, C., Zhang, F., Zhu, X., Ge, S.S.: **Low-resolution human
    pose estimation**. PR 126(10857), 108579 (2022)'
- en: 'Wang et al. [2022b] Wang, Z., Luo, H., Wang, P., Ding, F., Wang, F., Li, H.:
    Vtc-lfc: Vision transformer compression with low-frequency components. NIPS 35,
    13974–13988 (2022)'
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2022b] Wang, Z., Luo, H., Wang, P., Ding, F., Wang, F., Li, H.: Vtc-lfc:
    **Vision transformer compression** with low-frequency components. NIPS 35, 13974–13988
    (2022)'
- en: 'Jiang et al. [2022] Jiang, W., Jin, S., Liu, W., Qian, C., Luo, P., Liu, S.:
    Posetrans: A simple yet effective pose transformation augmentation for human pose
    estimation. In: ECCV, pp. 643–659 (2022)'
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等 [2022] Jiang, W., Jin, S., Liu, W., Qian, C., Luo, P., Liu, S.: **Posetrans**:
    A simple yet effective pose transformation augmentation for human pose estimation.
    In: ECCV, pp. 643–659 (2022)'
- en: 'Zhang et al. [2023] Zhang, J., Gong, K., Wang, X., Feng, J.: Learning to augment
    poses for 3d human pose estimation in images and videos. TPAMI 45(8), 10012–10026
    (2023)'
  id: totrans-1110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2023] Zhang, J., Gong, K., Wang, X., Feng, J.: **Learning to augment
    poses** for 3d human pose estimation in images and videos. TPAMI 45(8), 10012–10026
    (2023)'
- en: 'Jiang et al. [2023] Jiang, Z., Zhou, Z., Li, L., Chai, W., Yang, C.-Y., Hwang,
    J.-N.: Back to optimization: Diffusion-based zero-shot 3d human pose estimation.
    arXiv preprint arXiv:2307.03833 (2023)'
  id: totrans-1111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等 [2023] Jiang, Z., Zhou, Z., Li, L., Chai, W., Yang, C.-Y., Hwang, J.-N.:
    **Back to optimization**: Diffusion-based zero-shot 3d human pose estimation.
    arXiv preprint arXiv:2307.03833 (2023)'
- en: 'Gong et al. [2023] Gong, J., Foo, L.G., Fan, Z., Ke, Q., Rahmani, H., Liu,
    J.: Diffpose: Toward more reliable 3d pose estimation. CVPR (2023)'
  id: totrans-1112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gong 等 [2023] Gong, J., Foo, L.G., Fan, Z., Ke, Q., Rahmani, H., Liu, J.: **Diffpose**:
    Toward more reliable 3d pose estimation. CVPR (2023)'
- en: 'Zhang et al. [2022] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan,
    Z., Luo, P., Liu, W., Wang, X.: Bytetrack: Multi-object tracking by associating
    every detection box. In: ECCV, pp. 1–21 (2022)'
  id: totrans-1113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2022] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan, Z., Luo,
    P., Liu, W., Wang, X.: **Bytetrack**: Multi-object tracking by associating every
    detection box. In: ECCV, pp. 1–21 (2022)'
- en: 'Radford et al. [2021] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh,
    G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning
    transferable visual models from natural language supervision. In: International
    Conference on Machine Learning, pp. 8748–8763 (2021)'
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Radford 等 [2021] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G.,
    Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: **Learning
    transferable visual models** from natural language supervision. In: International
    Conference on Machine Learning, pp. 8748–8763 (2021)'
- en: 'Khirodkar et al. [2023] Khirodkar, R., Bansal, A., Ma, L., Newcombe, R., Vo,
    M., Kitani, K.: Egohumans: An egocentric 3d multi-human benchmark. arXiv preprint
    arXiv:2305.16487 (2023)'
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khirodkar 等 [2023] Khirodkar, R., Bansal, A., Ma, L., Newcombe, R., Vo, M.,
    Kitani, K.: **Egohumans**: An egocentric 3d multi-human benchmark. arXiv preprint
    arXiv:2305.16487 (2023)'
- en: 'Ulhaq et al. [2022] Ulhaq, A., Akhtar, N., Pogrebna, G., Mian, A.: Vision transformers
    for action recognition: A survey. arXiv preprint arXiv:2209.05700 (2022)'
  id: totrans-1116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ulhaq 等 [2022] Ulhaq, A., Akhtar, N., Pogrebna, G., Mian, A.: **Vision transformers
    for action recognition**: A survey. arXiv preprint arXiv:2209.05700 (2022)'
- en: 'Qing et al. [2023] Qing, Z., Zhang, S., Huang, Z., Wang, X., Wang, Y., Lv,
    Y., Gao, C., Sang, N.: Mar: Masked autoencoders for efficient action recognition.
    IEEE Transactions on Multimedia (2023)'
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qing 等 [2023] Qing, Z., Zhang, S., Huang, Z., Wang, X., Wang, Y., Lv, Y., Gao,
    C., Sang, N.: Mar: **Masked autoencoders** for efficient action recognition. IEEE
    Transactions on Multimedia (2023)'
- en: 'Kang et al. [2023] Kang, M.-S., Kang, D., Kim, H.: Efficient skeleton-based
    action recognition via joint-mapping strategies. In: WACV, pp. 3403–3412 (2023)'
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang 等 [2023] Kang, M.-S., Kang, D., Kim, H.: **Efficient skeleton-based action
    recognition** via joint-mapping strategies. In: WACV, pp. 3403–3412 (2023)'
- en: 'Gupta et al. [2021] Gupta, P., Sharma, D., Sarvadevabhatla, R.K.: Syntactically
    guided generative embeddings for zero-shot skeleton action recognition. In: ICIP,
    pp. 439–443 (2021)'
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gupta 等 [2021] Gupta, P., Sharma, D., Sarvadevabhatla, R.K.: **Syntactically
    guided generative embeddings** for zero-shot skeleton action recognition. In:
    ICIP, pp. 439–443 (2021)'
- en: 'Zhou et al. [2023] Zhou, Y., Qiang, W., Rao, A., Lin, N., Su, B., Wang, J.:
    Zero-shot skeleton-based action recognition via mutual information estimation
    and maximization. arXiv preprint arXiv:2308.03950 (2023)'
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 [2023] Zhou, Y., Qiang, W., Rao, A., Lin, N., Su, B., Wang, J.: **Zero-shot
    skeleton-based action recognition** via mutual information estimation and maximization.
    arXiv preprint arXiv:2308.03950 (2023)'
