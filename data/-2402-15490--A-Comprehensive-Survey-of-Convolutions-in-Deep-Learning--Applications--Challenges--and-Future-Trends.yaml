- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:34:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:34:25'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2402.15490] A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2402.15490] 深度学习中的卷积综述：应用、挑战和未来趋势'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15490](https://ar5iv.labs.arxiv.org/html/2402.15490)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15490](https://ar5iv.labs.arxiv.org/html/2402.15490)
- en: 'A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的卷积综述：应用、挑战和未来趋势
- en: 'Abolfazl Younesi1, Mohsen Ansari1, MohammadAmin Fazli1, Alireza Ejlali1, Muhammad
    Shafique2, and Jörg Henkel3 A. Younesi, M. Ansari, A. Ejlali, and M. A. Fazli
    are with the Department of Computer Engineering, Sharif University of Technology,
    Tehran, Iran. E-mail: {abolfazl.yunesi, ansari, ejlali, fazli}@sharif.edu.M. Shafique
    is with the Department of Computer Engineering, New York University (NYU) Abu
    Dhabi, United Arab Emirates. E-mail: muhammad.shafique@nyu.edu).J. Henkel is with
    the Karlsruhe Institute of Technology, Karlsruhe 76131, Germany. E-mail: henkel@kit.edu.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Abolfazl Younesi1, Mohsen Ansari1, MohammadAmin Fazli1, Alireza Ejlali1, Muhammad
    Shafique2 和 Jörg Henkel3 A. Younesi、M. Ansari、A. Ejlali 和 M. A. Fazli 现为伊朗德黑兰谢里夫理工大学计算机工程系的成员。电子邮件：{abolfazl.yunesi,
    ansari, ejlali, fazli}@sharif.edu。M. Shafique 现为阿布扎比纽约大学计算机工程系的成员。电子邮件：muhammad.shafique@nyu.edu。J.
    Henkel 现为德国卡尔斯鲁厄理工学院的成员。电子邮件：henkel@kit.edu。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In today’s digital age, Convolutional Neural Networks (CNNs), a subset of Deep
    Learning (DL), are widely used for various computer vision tasks such as image
    classification, object detection, and image segmentation. There are numerous types
    of CNNs designed to meet specific needs and requirements, including 1D, 2D, and
    3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS,
    among others. Each type of CNN has its unique structure and characteristics, making
    it suitable for specific tasks. It’s crucial to gain a thorough understanding
    and perform a comparative analysis of these different CNN types to understand
    their strengths and weaknesses. Furthermore, studying the performance, limitations,
    and practical applications of each type of CNN can aid in the development of new
    and improved architectures in the future. We also dive into the platforms and
    frameworks that researchers utilize for their research or development from various
    perspectives. Additionally, we explore the main research fields of CNN like 6D
    vision, generative models, and meta-learning. This survey paper provides a comprehensive
    examination and comparison of various CNN architectures, highlighting their architectural
    differences and emphasizing their respective advantages, disadvantages, applications,
    challenges, and future trends.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的数字时代，卷积神经网络（CNN），作为深度学习（DL）的一个子集，被广泛用于各种计算机视觉任务，如图像分类、目标检测和图像分割。为了满足特定需求和要求，设计了许多不同类型的CNN，包括1D、2D和3D
    CNN，以及扩张卷积、分组卷积、注意力卷积和深度卷积等。每种类型的CNN具有其独特的结构和特性，使其适用于特定任务。深入了解这些不同类型的CNN并进行比较分析对于理解它们的优缺点至关重要。此外，研究每种CNN的性能、限制和实际应用可以帮助未来开发新的改进架构。我们还从不同的角度探讨了研究人员用于研究或开发的平台和框架。此外，我们还研究了CNN的主要研究领域，如6D视觉、生成模型和元学习。这篇综述论文全面检查和比较了各种CNN架构，突出它们的架构差异，并强调它们各自的优点、缺点、应用、挑战和未来趋势。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep learning, DNN, CNN, Machine learning, Vision Transformers, GAN, Attention,
    Computer Vision, LLM, Large Language Model, Transformer, Dilated Convolution,
    Depthwise, NAS, NAT, Object Detection, 6D Vision, Vision Language Model
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，DNN，CNN，机器学习，视觉变换器，GAN，注意力机制，计算机视觉，LLM，大型语言模型，Transformer，扩张卷积，深度卷积，NAS，NAT，目标检测，6D视觉，视觉语言模型
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: IN today’s world, as technology continues to evolve, deep learning (DL) has
    become an integral part of our lives [[1](#bib.bib1)]. From voice assistants like
    Siri and Alexa to personalized recommendations on social media platforms, DL algorithms
    are constantly working behind the scenes to understand our preferences and make
    our lives easier [[2](#bib.bib2)]. With advancements in technology, DL is also
    being used in various fields such as healthcare, finance, and transportation,
    revolutionizing the way we approach these industries [[3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)]. As research and development in the field of DL continue to progress,
    even more innovative applications that will further enhance our daily lives can
    be expected. DL has ushered in a transformative era in artificial intelligence,
    empowering machines to assimilate vast datasets and make informed predictions
    [[6](#bib.bib6)][[8](#bib.bib8)]. The development of CNNs has received attention
    among deep learning’s significant advancements. Their impact has been felt in
    some areas, including generative AI, examining medical images, identifying objects
    [[9](#bib.bib9)], and finding anomalies [[10](#bib.bib10)]. CNNs, constituting
    a feedforward neural network, integrate convolution operations into their architecture
    [[7](#bib.bib7)][[11](#bib.bib11)]. These operations enable CNNs to adeptly capture
    intricate spatial and hierarchical patterns, rendering them exceptionally well-suited
    for image analysis tasks [[12](#bib.bib12)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的世界里，随着技术的不断进步，深度学习（DL）已成为我们生活中的一个重要组成部分[[1](#bib.bib1)]。从像Siri和Alexa这样的语音助手到社交媒体平台上的个性化推荐，DL算法不断在幕后工作，以理解我们的偏好并使我们的生活更轻松[[2](#bib.bib2)]。随着技术的进步，DL也被应用于医疗保健、金融和交通等各个领域，彻底改变了我们对这些行业的处理方式[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]。随着DL领域的研究和开发不断进展，我们可以期待更多创新应用，进一步提升我们的日常生活。DL引领了人工智能的变革时代，使机器能够整合大量数据集并做出明智的预测[[6](#bib.bib6)][[8](#bib.bib8)]。CNN的开发在深度学习的重大进展中受到了关注。它们在一些领域产生了影响，包括生成式AI、医学图像检查、物体识别[[9](#bib.bib9)]和异常检测[[10](#bib.bib10)]。CNN作为前馈神经网络，将卷积操作集成到其架构中[[7](#bib.bib7)][[11](#bib.bib11)]。这些操作使CNN能够巧妙地捕捉复杂的空间和层次模式，使其特别适合图像分析任务[[12](#bib.bib12)]。
- en: However, CNNs are often burdened by their computational complexity during training
    and deployment, particularly when operating on resource-constrained devices like
    mobile phones and wearables [[12](#bib.bib12)][[13](#bib.bib13)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CNN在训练和部署过程中经常受到计算复杂性的困扰，特别是在资源受限的设备如手机和可穿戴设备上运行时[[12](#bib.bib12)][[13](#bib.bib13)]。
- en: 'TABLE I: Comparison of existing surveys; +* means conditional cosideration'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：现有调查的比较；+* 表示有条件考虑
- en: '| Ref. | Year | No. of included studies | Research Questions and Objective
    | Taxonomy | Datasets | Challanges | Comparison of Simulators | Evaluation |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 年份 | 包含研究数量 | 研究问题和目标 | 分类 | 数据集 | 挑战 | 模拟器比较 | 评估 |'
- en: '| [[117](#bib.bib117)] | 2023 | 210 | - | +* | - | - | + | - |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | 2023 | 210 | - | +* | - | - | + | - |'
- en: '| [[118](#bib.bib118)] | 2021 | 343 | - | + | + | + | + | - |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118)] | 2021 | 343 | - | + | + | + | + | - |'
- en: '| [[119](#bib.bib119)] | 2022 | 202 | - | + | - | + | - | + |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bib119)] | 2022 | 202 | - | + | - | + | - | + |'
- en: '| [[120](#bib.bib120)] | 2020 | 243 | - | +* | + | +* | - | - |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| [[120](#bib.bib120)] | 2020 | 243 | - | +* | + | +* | - | - |'
- en: '| Our survey | 2024 | 465 | + | + | + | + | + | + |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 我们的调查 | 2024 | 465 | + | + | + | + | + | + |'
- en: 'Two principal avenues have emerged to reinforce the energy efficiency of CNNs:
    Employing Lightweight CNN Architectures: These architectures are deliberately
    engineered to achieve computational efficiency without compromising accuracy.
    For instance, the MobileNet family of CNNs has been meticulously tailored for
    mobile devices and has demonstrated state-of-the-art accuracy across various image
    classification Applications [[13](#bib.bib13)].'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 加强卷积神经网络（CNNs）能源效率的主要途径有两个：采用轻量级CNN架构：这些架构经过精心设计，以在不妨碍准确性的情况下实现计算效率。例如，MobileNet系列CNN经过精细调整，专为移动设备设计，并在各种图像分类应用中展示了最先进的准确性[[13](#bib.bib13)]。
- en: 'Embracing Compression Techniques: These methods facilitate the reduction of
    CNN model size and consequently diminish the volume of data transfers between
    devices. A noteworthy example is the TensorFlow Lite framework, which offers a
    suite of compression techniques tailored for compressing CNN models for mobile
    devices [[14](#bib.bib14)].'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 拥抱压缩技术：这些方法有助于减少CNN模型的大小，从而减少设备之间的数据传输量。一个值得注意的例子是TensorFlow Lite框架，它提供了一套针对移动设备的CNN模型压缩技术[[14](#bib.bib14)]。
- en: The fusion of lightweight CNN architectures and compression techniques yields
    a substantial boost in the energy efficiency of CNNs. Training and deploying CNNs
    on resource-constrained devices become feasible, thereby unlocking novel opportunities
    for employing CNNs in diverse applications like healthcare, agriculture, and environmental
    monitoring [[12](#bib.bib12)][[16](#bib.bib16)].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级CNN架构与压缩技术的融合显著提高了CNN的能效。在资源受限的设备上训练和部署CNN变得可行，从而为在医疗、农业和环境监测等各种应用中使用CNN开辟了新的机会[[12](#bib.bib12)][[16](#bib.bib16)]。
- en: 'How different convolutional techniques cater to various AI applications. Convolutions
    play a fundamental role in contemporary DL architectures and are especially crucial
    when dealing with data organized in grid-like structures, such as images, audio
    signals, and sequential data [[23](#bib.bib23)]. The convolutional operation entails
    moving a small filter, also known as a kernel, across the input data, performing
    element-wise multiplications and aggregations. This process extracts essential
    features from the input data [[24](#bib.bib24)]. The main significance of convolutions
    lies in their capability to efficiently capture local patterns and spatial relationships
    within the data. This localization property makes convolutions highly suitable
    for tasks like image recognition, as objects can be identified based on their
    local structures. Additionally, convolutions introduce parameter sharing, which
    results in a significant reduction in the number of trainable parameters, leading
    to more efficient and scalable models [[25](#bib.bib25)]. Existing surveys: Previous
    survey papers on CNN architectures such as [[118](#bib.bib118)] and [[120](#bib.bib120)]
    provided good overviews of popular architectures from a certain period. However,
    they lacked a clear Research question and objective, evaluation, and challenges
    based on their design patterns. They mostly discussed architectures chronologically.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的卷积技术如何满足各种人工智能应用的需求。卷积在现代深度学习架构中发挥了基础性作用，尤其是在处理以网格状结构组织的数据时（如图像、音频信号和序列数据）尤其重要[[23](#bib.bib23)]。卷积操作包括将一个小滤波器（也称为内核）在输入数据上移动，进行逐元素乘法和聚合。这个过程从输入数据中提取出关键特征[[24](#bib.bib24)]。卷积的主要意义在于它们能够有效地捕捉数据中的局部模式和空间关系。这种定位特性使得卷积特别适合于图像识别等任务，因为可以基于对象的局部结构来识别物体。此外，卷积引入了参数共享，从而显著减少了可训练参数的数量，导致模型更加高效和可扩展[[25](#bib.bib25)]。现有的调查：以前关于CNN架构的调查论文，如[[118](#bib.bib118)]和[[120](#bib.bib120)]，提供了对某一时期流行架构的良好概述。然而，它们缺乏明确的研究问题和目标、评估和基于设计模式的挑战。它们主要按时间顺序讨论了架构。
- en: Earlier surveys like [[119](#bib.bib119)] and [[120](#bib.bib120)] focused on
    explaining core CNN components and popular architectures up to a certain year.
    they also lacked research questions and objectives, analysis of datasets, and
    special types of taxonomy that were not considered complete overviews like large
    vision models, and large language models, and lacked of multipoint view for challenges.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的调查如[[119](#bib.bib119)]和[[120](#bib.bib120)]着重于解释核心CNN组件和流行架构直到某一年。这些调查也缺乏研究问题和目标、数据集分析以及未涵盖的大型视觉模型、大型语言模型等特殊分类的完整概述，并且缺乏多点视角来解决挑战。
- en: Previous work discussed the challenges in some specific concepts and applications
    of CNNs but did not extensively cover the intrinsic taxonomy present in newer
    CNN architectures. So this caused us to write a survey paper that aims to address
    the gaps in previous work by proposing a taxonomy to clearly classify CNN architectures
    based on their intrinsic design patterns rather than release year.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的工作讨论了CNN某些特定概念和应用中的挑战，但没有广泛涵盖新型CNN架构中的内在分类。因此，这促使我们撰写了一篇调查论文，旨在通过提出一个分类法，以清晰地根据内在设计模式而不是发布日期来分类CNN架构，从而填补前人工作的空白。
- en: We focus on architectural innovations from 2012 onwards and discuss the recent
    developments in greater depth than earlier surveys. Discussing the latest trends
    and challenges provides an updated perspective for researchers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于 2012 年以来的架构创新，并比早期的综述更深入地讨论了最新的发展。讨论最新的趋势和挑战为研究人员提供了更新的视角。
- en: This comprehensive survey of CNN’s history, taxonomy, applications, and challenges
    is needed to accelerate research progress in this domain further.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对 CNN 的历史、分类、应用和挑战的全面调查是加速这一领域研究进展所必需的。
- en: 'In this paper, the key questions we seek to address include:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们寻求解答的关键问题包括：
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How do state-of-the-art CNN models like ResNet, Inception, and MobileNet perform
    on the target hardware compared to constrained baselines? What are the impacts
    on accuracy, latency, and memory usage?
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 诸如 ResNet、Inception 和 MobileNet 等最先进的 CNN 模型在目标硬件上的表现如何，与受限基准相比如何？对准确性、延迟和内存使用的影响是什么？
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What techniques like pruning, quantization, distillation, and architecture design
    can help reduce the model size and computational complexity the most while retaining
    prediction quality?
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝、量化、蒸馏和架构设计等技术如何在保持预测质量的同时最大限度地减少模型大小和计算复杂性？
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How do multi-stage optimization approaches that combine different techniques
    compare to single methods? Can we achieve better trade-offs between accuracy,
    latency, and memory?
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结合不同技术的多阶段优化方法与单一方法相比如何？我们能否在准确性、延迟和内存之间实现更好的折中？
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For a target application like embedded vision, what are the best practices for
    benchmarking, tuning, and deploying optimized CNN models considering their unique
    constraints and specifications?
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于像嵌入式视觉这样的目标应用，考虑到其独特的约束和规格，优化 CNN 模型的基准测试、调优和部署的最佳实践是什么？
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Which pruning and quantization techniques work best for our target application
    and hardware? How does this compare to baselines?
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哪些剪枝和量化技术最适合我们的目标应用和硬件？与基准相比如何？
- en: '![Refer to caption](img/4939a02c4d84d1c713199e81aeee249c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4939a02c4d84d1c713199e81aeee249c.png)'
- en: 'Figure 1: Represents the section-by-section structure of the paper that provides
    a clear and organized framework for presenting the research findings.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：表示论文的逐节结构，为展示研究结果提供了一个清晰且有组织的框架。
- en: 'Our overview makes several key contributions to the DL and CV communities:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的综述对深度学习和计算机视觉社区做出了几个关键贡献：
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Analyzing multiple types of existing CNNs: The survey provides a comprehensive
    and detailed analysis of various DL models and algorithms used in CV Applications.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分析多种现有的 CNN：这项调查提供了对各种用于计算机视觉应用的深度学习模型和算法的全面而详细的分析。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comparing the CNN models with various parameters and architectures: The overview
    offers insights into the performance and efficiency trade-offs.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比较具有各种参数和架构的 CNN 模型：综述提供了性能和效率折中的见解。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Identifying the strengths and weaknesses of different CNN models: Aiding researchers
    in selecting the most suitable model for their specific applications.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确定不同 CNN 模型的优缺点：帮助研究人员选择最适合其特定应用的模型。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The overview highlights the challenges and future directions for further improvement
    in the fields of DL and computer vision.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综述突出了深度学习和计算机视觉领域进一步改进的挑战和未来方向。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Exploring the trends in neural network architecture: This emphasizes the practical
    application and exciting nature of the advancements.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索神经网络架构的趋势：强调了这些进展的实际应用和令人兴奋的性质。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'comprehensive overview of the Main research fields: This covers the primary
    fields of research that are actively pursued by researchers.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对主要研究领域的全面概述：涵盖了研究人员积极追求的主要研究领域。
- en: '![Refer to caption](img/ffbfe5858ad65fb12047d8a2b424779f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ffbfe5858ad65fb12047d8a2b424779f.png)'
- en: 'Figure 2: A text-based visual reading map that helps individuals navigate and
    comprehend the paper'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个基于文本的视觉阅读地图，帮助个人导航和理解论文内容。
- en: 'The rest of our review paper follows (See Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")): Section 2 of the paper will delve into the fundamentals
    of convolutions, elucidating their mathematical formulation, operational mechanics,
    and the role they play in the architecture of neural networks. Section 3 describes
    the basic parts of CNNs. In Section 4, The exploration will cover 2D convolutions,
    1D convolutions for sequential data, and 3D convolutions for volumetric data.
    Section 5 of the research paper will investigate advanced convolutional techniques
    that have emerged in recent years. This will encompass topics such as transposed
    convolutions for upsampling, depthwise separable convolutions for efficiency,
    spatial pyramid pooling, and attention mechanisms within convolutions. Section
    6 of the paper will highlight the real-world applications of different convolution
    types, showcasing their utility in image recognition, object detection, NLP, audio
    processing, and medical image analysis. In section 7 we discuss future trends
    and some open questions about CNNs. Section 8 is about the performance consideration
    of CNNs. In Section 9, we are going to talk about the platforms that are mostly
    used by researchers and developers, and in Section 10 about research fields that
    are popular or trending, then we have discussion in Section 11\. By the end of
    this research in Section 8, readers will gain a profound understanding of the
    importance of convolutions in DL and Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends") represents a reader map to visualize the flow of information
    within a text. It shows the connections between various sections, assisting readers
    in comprehending the overall structure of their preferred section following their
    needs.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的综述论文的其余部分如下（见图[1](#S1.F1 "图 1 ‣ I 引言 ‣ 深度学习中卷积的全面调查：应用、挑战与未来趋势")）：论文的第2部分将**深入**卷积的基础知识，阐明它们的数学公式、操作机制以及它们在神经网络结构中的作用。第3部分描述了CNN的基本部分。在第4部分，将涵盖2D卷积、用于序列数据的1D卷积以及用于体积数据的3D卷积。论文第5部分将调查近年来出现的先进卷积技术。这将包括上采样的转置卷积、用于效率的深度可分卷积、空间金字塔池化以及卷积中的注意机制。第6部分将突出不同卷积类型在现实世界中的应用，展示它们在图像识别、目标检测、自然语言处理、音频处理和医学图像分析中的实用性。在第7部分，我们讨论了CNN的未来趋势和一些未解问题。第8部分涉及CNN的性能考虑。在第9部分，我们将讨论研究人员和开发者常用的平台，在第10部分讨论流行或趋势的研究领域，然后在第11部分进行讨论。在第8部分结束时，读者将深入理解卷积在深度学习中的重要性，图[2](#S1.F2
    "图 2 ‣ I 引言 ‣ 深度学习中卷积的全面调查：应用、挑战与未来趋势")表示读者地图，以可视化文本中的信息流。它展示了各部分之间的连接，帮助读者理解根据需求选择的部分的整体结构。
- en: II Fundamentals of Convolutions
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 卷积的基础知识
- en: Convolutions form the foundation of crucial mathematical operations used to
    process data structured in grids, such as images, videos, and time series data
    [[26](#bib.bib26)]. Originally used in signal processing, convolutions were used
    for analyzing and manipulating signals [[27](#bib.bib27)]. In deep learning, convolutions
    serve as powerful feature extractors, enabling neural networks to efficiently
    learn from raw data [[26](#bib.bib26)][[27](#bib.bib27)]. The essence of a convolution
    involves the sliding of a small filter, commonly known as a kernel, over the input
    data. At each position of this sliding operation, the kernel performs element-wise
    multiplication with the corresponding input values [[28](#bib.bib28)]. Through
    this process, local patterns and relationships within the data are captured, enabling
    the model to acquire essential features like edges, textures, and shapes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积构成了处理网格结构数据（如图像、视频和时间序列数据）的关键数学操作的基础[[26](#bib.bib26)]。最初用于信号处理，卷积用于分析和处理信号[[27](#bib.bib27)]。在深度学习中，卷积作为强大的特征提取器，使神经网络能够高效地从原始数据中学习[[26](#bib.bib26)][[27](#bib.bib27)]。卷积的本质是将一个小滤波器，通常称为内核，滑过输入数据。在每个滑动操作的位置，内核与相应的输入值进行逐元素乘法[[28](#bib.bib28)]。通过这一过程，捕捉数据中的局部模式和关系，使模型能够获取诸如边缘、纹理和形状等重要特征。
- en: II-A Mathematical Formulation of Convolutions
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 卷积的数学公式
- en: 'Mathematically, a 2D convolution between an input matrix (often representing
    an image) and a kernel can be represented as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '-   从数学上讲，二维卷积在输入矩阵（通常代表图像）和内核之间可以表示如下：'
- en: '|  | $\text{Output}(i,j)=\sum_{(x,y)}\text{Input}(x,y)\cdot\text{Kernel}(i-x,j-y)$
    |  | (1) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Output}(i,j)=\sum_{(x,y)}\text{Input}(x,y)\cdot\text{Kernel}(i-x,j-y)$
    |  | (1) |'
- en: Here, Output denotes the resulting feature map, and Input represents the input
    matrix. The kernel, usually a small square matrix, defines the convolutional filter’s
    weights. The convolution operation is performed by sliding the kernel over the
    input matrix, and at each position, the element-wise multiplication and summation
    are computed as described in the formula [[29](#bib.bib29)]. For 1D convolutions,
    the mathematical formulation is similar, with the kernel sliding along a one-dimensional
    sequence, such as a time series or text data [[30](#bib.bib30)].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在这里，Output 表示结果特征图，而 Input 表示输入矩阵。内核，通常是一个小的方阵，定义了卷积滤波器的权重。卷积操作通过在输入矩阵上滑动内核来进行，在每个位置，按元素的乘法和求和会按照公式[[29](#bib.bib29)]进行计算。对于一维卷积，数学公式类似，内核在一维序列（如时间序列或文本数据）上滑动[[30](#bib.bib30)]。'
- en: II-B Convolutional Operations in DL
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   II-B 卷积操作在深度学习中的应用'
- en: Convolutional operations form the core of CNNs, a highly prominent class of
    DL models widely utilized for various CV applications. Within a CNN, convolutions
    are typically integrated into specific layers referred to as convolutional layers
    [[31](#bib.bib31)]. These layers are composed of multiple filters, each responsible
    for detecting distinct patterns in the input data [[139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [145](#bib.bib145), [146](#bib.bib146)]. During the training phase, the model
    goes through the process of backpropagation and gradient descent to learn the
    optimal weights of the convolutional filters. This enables the model to automatically
    discern meaningful patterns within the data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '-   卷积操作构成了 CNN 的核心，CNN 是一种广泛用于各种计算机视觉应用的高度突出的深度学习模型。CNN 中，卷积通常集成到被称为卷积层的特定层中[[31](#bib.bib31)]。这些层由多个滤波器组成，每个滤波器负责检测输入数据中的不同模式[[139](#bib.bib139),
    [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143),
    [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146)]。在训练阶段，模型通过反向传播和梯度下降的过程来学习卷积滤波器的最优权重，从而使模型能够自动识别数据中的有意义模式。'
- en: '![Refer to caption](img/4f21392d14824235012fd1bb5ca53792.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4f21392d14824235012fd1bb5ca53792.png)'
- en: 'Figure 3: . A graphical representation of CNN architectures from 1998 to 2023'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '-   图 3：1998 到 2023 年 CNN 架构的图示表示'
- en: 'Moreover, CNN architectures (See Fig. [3](#S2.F3 "Figure 3 ‣ II-B Convolutional
    Operations in DL ‣ II Fundamentals of Convolutions ‣ A Comprehensive Survey of
    Convolutions in Deep Learning: Applications, Challenges, and Future Trends") and
    Fig. [4](#S2.F4 "Figure 4 ‣ II-B Convolutional Operations in DL ‣ II Fundamentals
    of Convolutions ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends")) often incorporate pooling layers following the
    convolutional layers. As a result of pooling layers, feature maps generated by
    convolutions are downsampled, reducing computational complexity. Common pooling
    techniques include max-pooling and average pooling, which we will discuss about
    them in Section 3\. B.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '-   此外，CNN 架构（参见图 [3](#S2.F3 "图 3 ‣ II-B 卷积操作在深度学习中的应用 ‣ II 卷积基础 ‣ 深度学习中卷积的全面调查：应用、挑战和未来趋势")
    和图 [4](#S2.F4 "图 4 ‣ II-B 卷积操作在深度学习中的应用 ‣ II 卷积基础 ‣ 深度学习中卷积的全面调查：应用、挑战和未来趋势")）通常在卷积层后加入池化层。由于池化层的存在，通过卷积生成的特征图会被降采样，从而减少计算复杂度。常见的池化技术包括最大池化和平均池化，我们将在第
    3\. B 节中讨论这些技术。'
- en: '![Refer to caption](img/3c6633a4539462c2db1f1b48c6280160.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3c6633a4539462c2db1f1b48c6280160.png)'
- en: 'Figure 4: The flow of CNN architectures from 1998-2020 with their pros and
    cons represents that each CNN model is efficient for a specific application'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '-   图 4：1998-2020 年 CNN 架构的流程及其优缺点，表示每个 CNN 模型在特定应用中的效率'
- en: II-C Wavelets
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   II-C 小波变换'
- en: Wavelets are an important mathematical tool that has numerous applications in
    fields such as signal processing and computer graphics. At their core, wavelets
    rely on convolution to analyze functions or continuous-time signals [[104](#bib.bib104)].
    By convolving the target function with wavelet basis functions at different scales,
    wavelets are capable of representing data with varying degrees of resolution [[109](#bib.bib109)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 小波是一个重要的数学工具，广泛应用于信号处理和计算机图形学等领域。小波本质上依赖卷积来分析函数或连续时间信号[[104](#bib.bib104)]。通过将目标函数与不同尺度的小波基函数进行卷积，小波能够以不同的分辨率表示数据[[109](#bib.bib109)]。
- en: Wavelet analysis uses small waves, called wavelets, as basis functions instead
    of the sine and cosine functions used in Fourier analysis [[105](#bib.bib105)].
    Wavelets have the advantage of analyzing properties of data locally in time and
    frequency instead of globally. This makes them well-suited for tasks such as edge
    detection, noise removal, and texture identification. The wavelet basis can also
    be adapted to the input signal or data being analyzed [[105](#bib.bib105)][[106](#bib.bib106)].
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 小波分析使用小的波动，称为小波，作为基函数，而不是傅里叶分析中使用的正弦和余弦函数[[105](#bib.bib105)]。小波的优势在于能够在时间和频率上局部分析数据的属性，而不是全局分析。这使得它们非常适合于边缘检测、噪声去除和纹理识别等任务。小波基函数也可以适应被分析的输入信号或数据[[105](#bib.bib105)][[106](#bib.bib106)]。
- en: CNNs naturally lend themselves to wavelet analysis due to their intrinsic use
    of convolution operations [[107](#bib.bib107)][[108](#bib.bib108)]. During training,
    the convolutional filters within CNNs can learn wavelet-like basis functions tailored
    to meaningfully represent the given input data distribution at multiple resolutions.
    By adopting the wavelet bases through gradient descent and backpropagation, CNNs
    gain an efficient multi-scale representation of patterns in the data [[108](#bib.bib108)][[109](#bib.bib109)].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 自然适用于小波分析，因为它们固有地使用卷积操作[[107](#bib.bib107)][[108](#bib.bib108)]。在训练过程中，CNNs
    内的卷积滤波器可以学习类似小波的基函数，以有意义地表示在多个分辨率下的输入数据分布。通过采用小波基函数，CNNs 通过梯度下降和反向传播获得数据模式的高效多尺度表示[[108](#bib.bib108)][[109](#bib.bib109)]。
- en: A key characteristic of wavelets is their ability to decompose a signal into
    different frequency components, with high frequencies corresponding to detailed
    information and low frequencies corresponding to overall trends [[108](#bib.bib108)].
    A single-level wavelet decomposition breaks down the original signal into approximation
    and detail coefficients. The approximation contains lower frequency information,
    while the detail contains higher frequency or detailed information [[109](#bib.bib109)].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 小波的一个关键特性是其将信号分解为不同频率成分的能力，高频对应于详细信息，低频对应于整体趋势[[108](#bib.bib108)]。单层小波分解将原始信号分解为近似系数和细节系数。近似系数包含低频信息，而细节系数包含高频或详细信息[[109](#bib.bib109)]。
- en: CNNs can utilize this multi-resolution decomposition property of wavelets by
    using convolutions to learn wavelet filters at each level [[108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)]. The output of each level becomes the
    input to the next, with the filters extracting more detailed features at higher
    levels after the removal of coarse information. This convolutional learning of
    adapted wavelet bases enables CNNs to hierarchically capture patterns across different
    scales for improved data representation [[110](#bib.bib110)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 可以利用小波的多分辨率分解特性，通过卷积在每个层级学习小波滤波器[[108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110)]。每个层级的输出成为下一个层级的输入，滤波器在去除粗略信息后提取更详细的特征。这种卷积学习适应的小波基函数使 CNNs
    能够在不同尺度上分层捕捉模式，从而提高数据表示能力[[110](#bib.bib110)]。
- en: In various image processing and computer vision tasks, the use of convolutional
    wavelets within CNNs has shown promising results. For applications like denoising,
    super-resolution, and texture synthesis, CNNs equipped with learned wavelet filters
    have achieved state-of-the-art performance by effectively representing key multi-scale
    characteristics of visual data [[110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112),
    [113](#bib.bib113)]. Convolutional wavelets also benefit segmentation, detection,
    and classification when combined with traditional convolutional filters within
    CNNs [[109](#bib.bib109)]. In summary, wavelets provide a powerful tool for multi-scale
    analysis that CNNs can leverage through their inherent ability to learn localized
    basis functions via convolution operations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种图像处理和计算机视觉任务中，CNN中使用卷积小波已显示出有希望的结果。对于去噪、超分辨率和纹理合成等应用，配备了学习的小波滤波器的CNN在有效表示视觉数据的关键多尺度特征方面达到了最先进的性能[[110](#bib.bib110)、[111](#bib.bib111)、[112](#bib.bib112)、[113](#bib.bib113)]。卷积小波在与传统卷积滤波器结合时，也有助于分割、检测和分类[[109](#bib.bib109)]。总之，小波为多尺度分析提供了强大的工具，CNN可以通过其固有的卷积操作学习局部基函数来利用这一点。
- en: III Basic Convolutional Neural Networks
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基本卷积神经网络
- en: '![Refer to caption](img/cca30847185f7f980a4826af630c37e0.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cca30847185f7f980a4826af630c37e0.png)'
- en: 'Figure 5: A graphical representation of Section 3'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：第3节的图形表示
- en: The CNN architecture typically consists of an initial input layer, followed
    by several critical components, including convolutional layers, pooling layers,
    and fully connected layers. This organized structure allows for the systematic
    processing of raw data, such as images, through a series of layers, which in turn
    enables the extraction of relevant features and facilitates making predictions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构通常包括一个初始输入层，接着是几个关键组件，包括卷积层、池化层和全连接层。这种有序结构允许通过一系列层对原始数据（如图像）进行系统处理，从而提取相关特征并促进预测。
- en: The convolutional layers hold a central position in this architecture, as they
    employ learnable filters to process the input data. This operation is instrumental
    in detecting diverse patterns and features, thereby enhancing the network’s ability
    to understand the underlying data. Following the convolutional layers, the pooling
    layers come into play, downsampling the output from the previous layers. This
    downsampling process reduces the spatial dimensions while retaining crucial information.
    By focusing on the most significant details, these layers contribute to translational
    invariance, a valuable aspect in applications like image recognition where object
    positions may vary.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层在此架构中占据中心位置，因为它们使用可学习的滤波器来处理输入数据。此操作对于检测各种模式和特征至关重要，从而增强了网络对底层数据的理解能力。在卷积层之后，池化层发挥作用，对前一层的输出进行下采样。此下采样过程减少了空间维度，同时保留了关键的信息。通过关注最重要的细节，这些层有助于平移不变性，这在图像识别等应用中尤其有价值，因为对象位置可能会有所变化。
- en: 'In Table [II](#S3.T2 "TABLE II ‣ III-A Background of Deep Learning ‣ III Basic
    Convolutional Neural Networks ‣ A Comprehensive Survey of Convolutions in Deep
    Learning: Applications, Challenges, and Future Trends"), a comprehensive overview
    of the core components of basic CNNs is presented (also See Fig. [5](#S3.F5 "Figure
    5 ‣ III Basic Convolutional Neural Networks ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")), encompassing
    convolutional layers, pooling layers, and activation functions. The table provides
    insights into their individual purposes, functionalities, dependencies on input
    size, parameters, feature maps, translational invariance, computational efficiency,
    output size, roles in the CNN architecture, and impact on model performance. Analyzing
    these aspects provides profound insights into the elements that contribute to
    the effectiveness and performance of CNNs, making it a valuable reference for
    researchers and practitioners in the field.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [II](#S3.T2 "表格 II ‣ III-A 深度学习的背景 ‣ III 基本卷积神经网络 ‣ 卷积在深度学习中的综合调查：应用、挑战和未来趋势")
    中，呈现了基本 CNN 的核心组件的全面概述（还请参见图 [5](#S3.F5 "图 5 ‣ III 基本卷积神经网络 ‣ 卷积在深度学习中的综合调查：应用、挑战和未来趋势")），包括卷积层、池化层和激活函数。该表提供了关于它们各自目的、功能、对输入大小的依赖、参数、特征图、平移不变性、计算效率、输出大小、在
    CNN 结构中的作用以及对模型性能的影响的深刻见解。分析这些方面可以深入了解对 CNN 的效果和性能有贡献的元素，使其成为该领域的研究人员和实践者的宝贵参考。
- en: III-A Background of Deep Learning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 深度学习的背景
- en: Deep learning, a prominent form of machine learning, encompasses the use of
    neural networks composed of multiple layers to acquire hierarchical representations
    of data [[17](#bib.bib17)]. Taking inspiration from the intricate workings of
    the human brain, where neurons engage in processing and transmitting information
    to forge elaborate depictions of the world, DL models, also known as deep neural
    networks, showcase remarkable prowess in assimilating hierarchical features from
    raw data. This exceptional ability enables them to discern intricate patterns
    and achieve remarkable precision in predictions [[18](#bib.bib18)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，作为机器学习的一个重要形式，涵盖了使用由多个层组成的神经网络获取数据的分层表示 [[17](#bib.bib17)]。深受人脑错综复杂的工作方式的启发，在那里神经元参与处理和传递信息以形成世界的复杂描述，DL
    模型，也被称为深度神经网络，在从原始数据中吸收分层特征方面展示出了非凡的能力。这种出色的能力使它们能够辨别错综复杂的模式，并在预测中取得显著的精度 [[18](#bib.bib18)]。
- en: 'TABLE II: The Different Aspects of the Basic Convolutional Neural Networks'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：基本卷积神经网络的不同方面
- en: '| Aspect | Convolutional Layers | Pooling Layers | Activation Functions | Batch
    Normalization |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 卷积层 | 池化层 | 激活函数 | 批量归一化 |'
- en: '| Purpose | Feature extraction | Feature reduction | Introduce non-linearity
    | Training stabilization |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 特征提取 | 特征减少 | 引入非线性 | 训练稳定性 |'
- en: '| \hdashlineFunctionality | Detect patterns and textures | Downsample feature
    maps | Add non-linearity | Normalizing activations |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline功能性 | 检测模式和纹理 | 对特征图进行下采样 | 添加非线性 | 规范化激活 |'
- en: '| \hdashlineInput size dependency | Depends on input dimensions | Reduces spatial
    dimensions | Independent of input | Depends on input size |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline输入大小依赖性 | 取决于输入维度 | 减少空间维度 | 独立于输入 | 取决于输入大小 |'
- en: '| \hdashlineParameters | Learnable weights (kernels) | No parameters | No parameters
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline参数 | 可学习的权重（卷积核） | 无参数 | 无参数 |'
- en: '&#124; Learnable scaling & &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可学习的比例调整 & &#124;'
- en: '&#124; shifting parameters &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 位移参数 &#124;'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| \hdashlineFeature maps | Produce feature maps | No feature maps | No feature
    maps | No feature maps |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline特征图 | 产生特征图 | 没有特征图 | 没有特征图 | 没有特征图 |'
- en: '| \hdashlineTranslational invariance | Not inherently invariant | Introduces
    some invariance | Independent of input | No Translational invariance |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline平移不变性 | 并非固有不变 | 引入一些不变性 | 独立于输入 | 没有平移不变性 |'
- en: '| \hdashlineComputational efficiency | Computationally intensive |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline计算效率 | 计算密集型 |'
- en: '&#124; Reduces computation &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 减少计算 &#124;'
- en: '&#124; complexity &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 复杂度 &#124;'
- en: '| Low computation cost | Enhanced training stability |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 低计算成本 | 增强训练稳定性 |'
- en: '| \hdashlineOutput size |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline输出大小 |'
- en: '&#124; May or may not &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可能或可能不 &#124;'
- en: '&#124; match the input size &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 匹配输入大小 &#124;'
- en: '| Reduced size | Unchanged | Unchanged |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 减小的大小 | 保持不变 | 保持不变 |'
- en: '| \hdashline'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '| \hdashline'
- en: '&#124; Role in CNN &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在 CNN 中的作用 &#124;'
- en: '&#124; architecture &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 体系结构 &#124;'
- en: '| Central component |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 中央组件 |'
- en: '&#124; Interposed between &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 介于 &#124;'
- en: '&#124; convolutions &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积 &#124;'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Enable learning &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 启用学习 &#124;'
- en: '&#124; complex relationships &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 复杂关系 &#124;'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Improve convergence, &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 改善收敛性， &#124;'
- en: '&#124; ease of tuning &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 调优难度 &#124;'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| \hdashline'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '| \hdashline'
- en: '&#124; Influence on &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对 &#124;'
- en: '&#124; model performance &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型性能 &#124;'
- en: '|'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Significantly &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 显著 &#124;'
- en: '&#124; impacts performance &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 影响性能 &#124;'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Affects model &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 影响模型 &#124;'
- en: '&#124; efficiency &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 效率 &#124;'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Crucial for &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对于 &#124;'
- en: '&#124; Learning &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '|'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Significantly &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 显著 &#124;'
- en: '&#124; impacts performance &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 影响性能 &#124;'
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| \hdashlineInterpretability | Low | Low | Low | Normal |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline可解释性 | 低 | 低 | 低 | 正常 |'
- en: '| \hdashlineTraining complexity | High | Low | Low | Normal |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline训练复杂性 | 高 | 低 | 低 | 正常 |'
- en: '| \hdashlineMemory usage | Normal | Low | Low | Normal |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline内存使用 | 正常 | 低 | 低 | 正常 |'
- en: The roots of DL can be traced back to the nascent endeavors surrounding artificial
    neural networks in the 1940s. However, the true resurgence and substantial remarkable
    materialized in the 1980s and 1990s, paving the way for its remarkable revival
    in the 21 century [[19](#bib.bib19)]. Key catalysts driving this resurgence were
    the strides made in computational power, the vast availability of datasets, and
    the advent of efficient training algorithms, most notably backpropagation, which
    played a pivotal role [[20](#bib.bib20)]. By harnessing these advancements, DL
    models attained the ability to process and analyze vast repositories of data,
    thus acquiring an aptitude for deciphering intricate patterns and making precise
    predictions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的根源可以追溯到20世纪40年代关于人工神经网络的初期努力。然而，真正的复兴和显著成果出现在80年代和90年代，为其在21世纪的显著复兴铺平了道路
    [[19](#bib.bib19)]。推动这一复兴的关键因素包括计算能力的进步、大量数据集的可用性以及高效训练算法的出现，特别是反向传播算法，它在这一过程中发挥了关键作用
    [[20](#bib.bib20)]。通过利用这些进展，深度学习模型获得了处理和分析大量数据的能力，从而能够解读复杂模式并做出准确预测。
- en: The convergence of powerful hardware and sophisticated algorithms ushered in
    an era of remarkable accomplishments across diverse domains. Computer Vision (CV),
    natural language processing (NLP), and speech recognition (SR), among others,
    have witnessed remarkable strides through the transformative power of DL [[73](#bib.bib73)].
    This dynamic discipline’s capacity to overcome more difficult problems and promote
    innovation across various industries is becoming more and more clear as it develops
    and advances.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 强大硬件和复杂算法的融合引领了各个领域的显著成就。计算机视觉（CV）、自然语言处理（NLP）和语音识别（SR）等领域，通过深度学习的变革性力量，取得了显著进展
    [[73](#bib.bib73)]。随着这一动态学科的发展和进步，它克服更困难问题并推动各行业创新的能力越来越清晰。
- en: III-B Introduction to Convolutional Neural Networks
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 卷积神经网络简介
- en: CNNs, an influential category of DL models, have emerged as a preeminent and
    extensively utilized algorithm within the realm of DL [[21](#bib.bib21)]. Distinctive
    to CNNs is their capacity to engage in convolution calculations and operate proficiently
    on intricate structures. This characteristic has propelled CNNs to achieve remarkable
    breakthroughs in image analysis and feature extraction, bestowing upon them the
    ability to discern and efficiently classify features in images. Moreover, CNNs
    are renowned as shift-invariant artificial neural networks, a nomenclature that
    accentuates their capability to classify input information based on its hierarchical
    arrangement [[22](#bib.bib22)].
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs），作为一种具有影响力的深度学习模型，已成为深度学习领域中一种卓越且广泛使用的算法 [[21](#bib.bib21)]。CNNs的特点在于其进行卷积计算的能力和在复杂结构上高效运行的能力。这一特性使得CNNs在图像分析和特征提取方面取得了显著突破，使其能够识别和有效分类图像中的特征。此外，CNNs被称为位移不变的人工神经网络，这一命名强调了它们根据输入信息的层次结构进行分类的能力
    [[22](#bib.bib22)]。
- en: The hierarchical architecture of CNNs empowers them to process and extract features
    from input data in a shift-invariant manner [[22](#bib.bib22)]. This implies that
    CNNs can adeptly recognize and classify objects within images, irrespective of
    their position or orientation. The realization of this shift-invariant attribute
    is accomplished through the application of convolutional layers, which employ
    filters in a sliding window fashion. These filters acquire the ability to detect
    specific patterns or features at various spatial scales, thereby enabling the
    network to encapsulate both local and global information. Consequently, CNNs exhibit
    profound proficiency in extracting meaningful features from images, facilitating
    a wide array of applications encompassing object detection, image recognition,
    and even image generation [[74](#bib.bib74)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的层次结构使其能够以平移不变的方式处理和提取输入数据中的特征[[22](#bib.bib22)]。这意味着CNN可以熟练地识别和分类图像中的物体，无论它们的位置或方向如何。这一平移不变属性的实现是通过应用卷积层来完成的，这些卷积层采用滑动窗口的方式使用滤波器。这些滤波器能够在不同空间尺度上检测特定模式或特征，从而使网络能够捕捉局部和全局信息。因此，CNN在从图像中提取有意义的特征方面表现出极高的能力，这促进了包括目标检测、图像识别甚至图像生成在内的广泛应用[[74](#bib.bib74)]。
- en: III-C Convolutional Layers and Their Functionality
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 卷积层及其功能
- en: Each convolutional layer comprises multiple filters, also referred to as kernels,
    which are small windows that slide over the input data [[32](#bib.bib32)]. During
    the training phase, the weights of these filters are learned, and they function
    as feature extractors, identifying specific patterns, edges, and textures present
    in the input [[33](#bib.bib33)]. When the filters move across the input, they
    create feature maps that emphasize important parts of the data as region of interest
    (ROI). These maps show where specific patterns in the input become active, helping
    the CNN recognize significant features crucial for later tasks like classification
    or detection [[34](#bib.bib34)].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积层包含多个滤波器，也称为卷积核，这些滤波器是滑过输入数据的小窗口[[32](#bib.bib32)]。在训练阶段，这些滤波器的权重被学习，它们作为特征提取器，识别输入中存在的特定模式、边缘和纹理[[33](#bib.bib33)]。当滤波器在输入数据上移动时，它们生成强调数据中重要部分的特征图，这些部分被称为兴趣区域（ROI）。这些图显示了输入中具体模式变得活跃的地方，帮助CNN识别对后续任务如分类或检测至关重要的显著特征[[34](#bib.bib34)]。
- en: For example, in a CNN trained to identify cats in images, the filters may learn
    to recognize the patterns of fur, whiskers, and ears. As the filters convolve
    across an image of a cat, they generate feature maps that highlight these specific
    regions of interest. These feature maps indicate the activation of these cat-specific
    patterns and aid in accurately classifying the image as containing a cat.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个训练用于识别猫的图像的CNN中，滤波器可能学习识别毛发、胡须和耳朵的模式。当滤波器在猫的图像上卷积时，它们生成突出这些特定感兴趣区域的特征图。这些特征图显示了这些猫特有模式的激活，并有助于准确地将图像分类为包含猫。
- en: III-D Pooling Layers and Feature Reduction
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 池化层与特征减少
- en: Pooling layers are incorporated following convolutional layers to decrease the
    spatial dimensions of the feature maps, thereby reducing the computational complexity
    of the network [[35](#bib.bib35)]. The most frequently utilized pooling techniques
    in CNNs are max-pooling and average-pooling [[37](#bib.bib37)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层之后，池化层被引入以减少特征图的空间维度，从而降低网络的计算复杂性[[35](#bib.bib35)]。在CNN中，最常用的池化技术是最大池化和平均池化[[37](#bib.bib37)]。
- en: 'Max-pooling entails selecting the maximum value from a small region of the
    feature map, while average-pooling computes the average value. Pooling offers
    two primary advantages: first, it effectively reduces the number of parameters
    in the network, resulting in improved computational efficiency. Second, it introduces
    a level of translational invariance, signifying that minor spatial translations
    in the input data do not substantially impact the pooled outputs. This property
    enhances the CNN’s ability to generalize better to variations in the input data.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化涉及从特征图的小区域中选择最大值，而平均池化计算平均值。池化有两个主要优势：首先，它有效地减少了网络中的参数数量，从而提高了计算效率。其次，它引入了一定程度的平移不变性，意味着输入数据中的小幅空间平移不会对池化结果产生显著影响。这一特性增强了卷积神经网络（CNN）对输入数据变化的泛化能力。
- en: For example, in image classification applications, after several convolutional
    and activation layers, a pooling layer can be used to downsample the feature map.
    This downsampling reduces the spatial resolution of the features, making it more
    computationally efficient to process and reducing the risk of overfitting. Additionally,
    because pooling computes either maximum or average values, it can capture the
    dominant features in an image regardless of their exact location, making the network
    more robust to slight variations in object position or orientation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图像分类应用中，经过若干卷积层和激活层之后，可以使用池化层来下采样特征图。这种下采样减少了特征的空间分辨率，使处理更具计算效率，并减少了过拟合的风险。此外，由于池化计算的是最大值或平均值，它能够捕捉图像中的主要特征，而不受其精确位置的影响，使网络对物体位置或方向的轻微变化更具鲁棒性。
- en: III-E Activation Functions in CNNs
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E CNN 中的激活函数
- en: Activation functions play a vital role in CNNs as they are applied to the output
    of each neuron, introducing nonlinearity to the network and facilitating the learning
    of complex relationships between input data and their corresponding features.
    Within CNNs, several commonly used activation functions include Rectified Linear
    Units (ReLU) [[36](#bib.bib36)], which set negative values to zero while preserving
    positive values unchanged. Variants like Leaky ReLU [[36](#bib.bib36)] and Parametric
    ReLU [[39](#bib.bib39)] are also widely employed. The selection of the activation
    function is of great importance as it directly impacts the network’s capacity
    to learn and make accurate predictions. By introducing nonlinearity, activation
    functions allow CNN to model intricate patterns and decision boundaries, thereby
    enhancing its performance across a range of tasks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数在 CNN 中扮演着重要角色，因为它们被应用于每个神经元的输出，引入非线性，从而促进输入数据与其对应特征之间复杂关系的学习。在 CNN 中，一些常用的激活函数包括整流线性单元（ReLU）[[36](#bib.bib36)]，它将负值设为零，而正值保持不变。像
    Leaky ReLU [[36](#bib.bib36)] 和 Parametric ReLU [[39](#bib.bib39)] 等变体也被广泛使用。激活函数的选择至关重要，因为它直接影响网络的学习能力和预测准确性。通过引入非线性，激活函数使
    CNN 能够建模复杂的模式和决策边界，从而提高其在各种任务中的性能。
- en: For example, in image classification applications, the ReLU activation function
    has been shown to effectively remove negative pixel values and emphasize positive
    pixel values, allowing CNN to identify important features and learn discriminative
    patterns. This enables the CNN to accurately classify different objects in images,
    such as correctly identifying whether an image contains a cat or a dog.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图像分类应用中，ReLU 激活函数已被证明能有效去除负像素值并强调正像素值，从而使 CNN 能够识别重要特征并学习判别模式。这使得 CNN 能够准确分类图像中的不同对象，比如正确识别图像中是否包含猫或狗。
- en: III-F Batch Normalization in CNNs
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-F CNN 中的批量归一化
- en: Batch Normalization is a technique that helps stabilize and accelerate the training
    of CNNs [[78](#bib.bib78)]. It normalizes the activations of each layer by centering
    and scaling the values using the mean and variance of each mini-batch during training.
    This process reduces internal covariate shifts, making the optimization process
    smoother and enabling the use of higher learning rates.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是一种有助于稳定和加速 CNN 训练的技术 [[78](#bib.bib78)]。它通过使用每个小批量的均值和方差对每层的激活值进行中心化和缩放，从而规范化激活值。这个过程减少了内部协变量偏移，使优化过程更加平滑，并允许使用更高的学习率。
- en: By normalizing activations, Batch Normalization allows for more aggressive learning
    rates, which leads to faster convergence and improved model generalization. Additionally,
    it acts as a regularizer, reducing the need for other regularization techniques
    like dropout.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过规范化激活值，批量归一化允许使用更激进的学习率，这导致了更快的收敛和改进的模型泛化。此外，它还充当了正则化器，减少了对其他正则化技术（如 dropout）的需求。
- en: Overall, Batch Normalization has become a standard component in CNN architectures,
    contributing to faster training, improved model performance, and increased ease
    of hyperparameter tuning. Its widespread adoption has significantly contributed
    to the success of modern CNNs in various CV and NLP applications. For example,
    in image classification applications, Batch Normalization helps reduce overfitting
    by normalizing the input for each mini-batch during training. This ensures that
    the network learns robust features and avoids relying on specific pixel values
    or noise in the input data. As a result, the model becomes more generalized and
    performs better on unseen data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，批量归一化已成为CNN架构中的标准组件，有助于加快训练速度、提高模型性能，并简化超参数调优。它的广泛应用显著促进了现代CNN在各种计算机视觉（CV）和自然语言处理（NLP）应用中的成功。例如，在图像分类应用中，批量归一化通过对每个小批量的输入进行归一化来帮助减少过拟合。这确保了网络能够学习到稳健的特征，并避免依赖于输入数据中的特定像素值或噪声。因此，模型变得更加通用，并在未见过的数据上表现更好。
- en: IV Types of Convolution in Deep Learning
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习中的卷积类型
- en: '![Refer to caption](img/4145c6ab56a402b2b7df7d520333ac8d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4145c6ab56a402b2b7df7d520333ac8d.png)'
- en: 'Figure 6: An overview of Section 4 structure'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：第4节结构概览
- en: 'In this section, our goal is to comprehensively explore the different convolution
    methods (See Fig. [6](#S4.F6 "Figure 6 ‣ IV Types of Convolution in Deep Learning
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) commonly used in deep learning models. Table LABEL:tab:Characteristics
    presents a condensed overview of these convolution types, providing important
    information such as input data type, dimensionality, receptive field, computational
    cost, primary use case, memory consumption, parallelization capability, consideration
    of temporal information, and computational efficiency.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们的目标是全面探讨在深度学习模型中常用的不同卷积方法（参见图 [6](#S4.F6 "图 6 ‣ 深度学习中的卷积类型 ‣ 深度学习卷积的综合调查：应用、挑战和未来趋势")）。表
    LABEL:tab:Characteristics 提供了这些卷积类型的简明概述，提供了输入数据类型、维度、感受野、计算成本、主要应用场景、内存消耗、并行化能力、时间信息的考虑以及计算效率等重要信息。
- en: '![Refer to caption](img/d987ea6235c57d37d02e224fbba34c11.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d987ea6235c57d37d02e224fbba34c11.png)'
- en: 'Figure 7: The Basic structure of CNN. a) represents CNN without Padding which
    causes the output image to become smaller. b) represents CNN without Padding which
    the output image is the same size as the input image'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：CNN的基本结构。a) 代表没有填充的CNN，这会导致输出图像变小。b) 代表没有填充的CNN，输出图像与输入图像大小相同
- en: It is important to highlight that selecting the appropriate convolutional type
    relies on the particular task and dataset under consideration. For instance, when
    working with diverse data types, such as images or text, it may be necessary to
    employ distinct convolutional types to effectively capture relevant features.
    Moreover, considering the computational efficiency of each convolutional type
    becomes important for real-time applications or settings with limited resources.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，选择适当的卷积类型取决于特定任务和数据集。例如，在处理各种数据类型（如图像或文本）时，可能需要采用不同的卷积类型来有效捕捉相关特征。此外，对于实时应用或资源有限的环境，考虑每种卷积类型的计算效率变得尤为重要。
- en: 'TABLE III: The Comparison Provides an Overview of The Characteristics and Functionalities
    of Different Convolution Types'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：比较提供了不同卷积类型的特征和功能概述
- en: '| Convolution Type | 2D Convolutions | 1D Convolutions | 3D Convolutions |
    Dilated Convolutions | Grouped Convolutions |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 卷积类型 | 2D卷积 | 1D卷积 | 3D卷积 | 膨胀卷积 | 分组卷积 |'
- en: '| Input Data Type | Images |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 输入数据类型 | 图像 |'
- en: '&#124; Sequential Data &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 序列数据 &#124;'
- en: '&#124; (e.g., Text) &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （例如，文本） &#124;'
- en: '|'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Volumetric Data &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 体积数据 &#124;'
- en: '&#124; (e.g., Videos) &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （例如，视频） &#124;'
- en: '| Images | Images |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | 图像 |'
- en: '| \hdashlineDimensionality | 2D | 1D | 3D | 1D, 2D | 2D |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline维度 | 2D | 1D | 3D | 1D, 2D | 2D |'
- en: '| \hdashlineReceptive Field | Local | Local | Volumetric | Local | Local |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline感受野 | 局部 | 局部 | 体积 | 局部 | 局部 |'
- en: '| \hdashlineComputational Cost | Medium | Low | High | Low | High |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline计算成本 | 中等 | 低 | 高 | 低 | 高 |'
- en: '| \hdashlineMain Use Case |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline主要应用场景 |'
- en: '&#124; Image recognition, &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像识别， &#124;'
- en: '&#124; Object detection &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标检测 &#124;'
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Text classification, &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文本分类， &#124;'
- en: '&#124; Sentiment analysis &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 情感分析 &#124;'
- en: '|'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Semantic segmentation, &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语义分割, &#124;'
- en: '&#124; 3D medical imaging &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D医学成像 &#124;'
- en: '|'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image Filtering, &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像滤波, &#124;'
- en: '&#124; Image generation &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像生成 &#124;'
- en: '|'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Large-scale &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大规模 &#124;'
- en: '&#124; CNN architectures &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN架构 &#124;'
- en: '|'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| \hdashlineMemory Consumption | Medium | Low | High | Low | Low |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline内存消耗 | 中等 | 低 | 高 | 低 | 低 |'
- en: '| \hdashlineParallelization | Limited | Limited | Limited | Limited | High
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline并行 | 有限 | 有限 | 有限 | 有限 | 高 |'
- en: '| \hdashline'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '| \hdashline'
- en: '&#124; Use of Temporal &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间使用 &#124;'
- en: '&#124; Information &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息 &#124;'
- en: '| Not applicable |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 不适用 |'
- en: '&#124; Captures temporal &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉时间 &#124;'
- en: '&#124; patterns &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模式 &#124;'
- en: '|'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Captures spatial &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉空间 &#124;'
- en: '&#124; temporal patterns &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间模式 &#124;'
- en: '| Not applicable | Not applicable |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 不适用 | 不适用 |'
- en: '| \hdashline'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '| \hdashline'
- en: '&#124; Computational &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算 &#124;'
- en: '&#124; Efficiency &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 效率 &#124;'
- en: '| Medium | High | Medium | High | High |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 中等 | 高 | 中等 | 高 | 高 |'
- en: IV-A 2D Convolutions
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 2D卷积
- en: '2D convolutions (See Fig. [7](#S4.F7 "Figure 7 ‣ IV Types of Convolution in
    Deep Learning ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends")) serve as the foundational elements in CNNs, particularly
    for applications related to CV. They are predominantly utilized for processing
    two-dimensional data, such as images, which can be represented as a grid of pixels.
    During this convolutional operation, a 2D kernel slides over the input image,
    enabling the capture of local patterns and the extraction of relevant features
    [[27](#bib.bib27)]. The primary application of 2D convolutions lies in image recognition,
    wherein the model learns to identify essential patterns, including edges, textures,
    and object components, thereby facilitating high-level recognition applications
    [[40](#bib.bib40)].'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 2D卷积（见图 [7](#S4.F7 "图 7 ‣ IV 深度学习中的卷积类型 ‣ 深度学习中的卷积：应用、挑战和未来趋势的综合调查")）作为CNN的基础元素，特别是与计算机视觉（CV）相关的应用中。它们主要用于处理二维数据，例如可以表示为像素网格的图像。在此卷积操作中，2D核在输入图像上滑动，从而捕捉局部模式并提取相关特征
    [[27](#bib.bib27)]。2D卷积的主要应用在于图像识别，其中模型学习识别关键模式，包括边缘、纹理和对象组件，从而促进高层次的识别应用 [[40](#bib.bib40)]。
- en: 2D convolutions have found use in a variety of fields, including signal processing,
    CV, and NLP in addition to image recognition. CNNs have completely changed CV
    processes like object detection, image segmentation, and facial recognition. CNNs
    can more accurately and efficiently analyze the spatial relationships and hierarchical
    structures present in images by using 2D convolutions. When learned filters slide
    across the input image, a CNN can learn to find and locate different objects in
    images, such as in object detection tasks. This helps the network accurately detect
    objects even in complicated scenes, as it can identify important patterns of various
    sizes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 2D卷积在图像识别之外的多个领域中也得到了应用，包括信号处理、计算机视觉（CV）和自然语言处理（NLP）。CNNs 完全改变了计算机视觉处理过程，如对象检测、图像分割和面部识别。通过使用2D卷积，CNNs
    可以更准确、更高效地分析图像中存在的空间关系和层次结构。当学习到的滤波器在输入图像上滑动时，CNN可以学习在图像中找到和定位不同的对象，例如在对象检测任务中。这有助于网络在复杂场景中准确检测对象，因为它可以识别各种尺寸的重要模式。
- en: Moreover, CNNs can also be learned to categorize and compare faces by analyzing
    facial features using 2D convolutions in facial recognition. This makes it possible
    to create systems like access control and identity verification.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过使用2D卷积分析面部特征，CNN还可以学习对面部进行分类和比较。这使得能够创建像访问控制和身份验证这样的系统。
- en: IV-B 1D Convolutions for Sequential Data
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 一维卷积用于序列数据
- en: 'One-dimensional (1D) convolutions (See Fig. [8](#S4.F8 "Figure 8 ‣ IV-B 1D
    Convolutions for Sequential Data ‣ IV Types of Convolution in Deep Learning ‣
    A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) are specially designed for working with sequential data like
    time series, audio signals, and natural language. Unlike their two-dimensional
    counterparts, 1D convolutions operate on a single line, allowing them to detect
    patterns that develop over time [[41](#bib.bib41)]. In the field of natural language
    processing, 1D convolutions are widely used in tasks such as classifying text
    and analyzing sentiments. They help the model identify complex patterns in sequences
    of words and understand how these words are related to each other [[42](#bib.bib42)].'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '一维（1D）卷积（见图[8](#S4.F8 "Figure 8 ‣ IV-B 1D Convolutions for Sequential Data
    ‣ IV Types of Convolution in Deep Learning ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")）专门设计用于处理时间序列、音频信号和自然语言等序列数据。与二维卷积不同，1D
    卷积操作在单一线性上，能够检测随时间发展的模式[[41](#bib.bib41)]。在自然语言处理领域，1D 卷积广泛用于文本分类和情感分析等任务。它们帮助模型识别词序列中的复杂模式，并理解这些词之间的关系[[42](#bib.bib42)]。'
- en: '![Refer to caption](img/202c37eda19fd5c0d864a106c6b14bcf.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/202c37eda19fd5c0d864a106c6b14bcf.png)'
- en: 'Figure 8: An overview to simple one-dimensional (1D) Convolution Neural Network
    with Two Convolution layer'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：简单的一维（1D）卷积神经网络的概览，包含两个卷积层
- en: 1D convolutions have also been successfully applied to audio signal processing
    applications such as SR and music analysis. By analyzing the temporal patterns
    of audio signals, these models can extract meaningful features that capture the
    underlying structure and characteristics of the sound. This has proven to be particularly
    useful in applications like speaker identification and emotion recognition, where
    the sequential nature of the audio data is sequential.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一维卷积也已成功应用于音频信号处理，如超分辨率（SR）和音乐分析。通过分析音频信号的时间模式，这些模型可以提取出有意义的特征，从而捕捉声音的基本结构和特征。这在如说话人识别和情感识别等应用中尤为有效，其中音频数据的序列特性至关重要。
- en: For example, in speaker identification, 1D convolution can analyze the sequential
    patterns of an individual’s voice and learn to associate certain patterns with
    specific speakers. This allows the model to accurately identify and differentiate
    between different speakers in an audio recording. In emotion recognition, 1D convolutions
    can analyze the temporal changes in pitch, tone, and intensity of an audio signal
    to classify the emotional state of the speaker, such as happiness, sadness, or
    anger. This helps in detecting and understanding the underlying emotions conveyed
    through speech, which can be useful in various applications like customer sentiment
    analysis, virtual assistants, and mental health monitoring.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在说话人识别中，1D 卷积可以分析个体声音的序列模式，并学习将特定模式与特定说话人关联起来。这使得模型能够准确识别和区分音频录音中的不同说话人。在情感识别中，1D
    卷积可以分析音频信号中音高、音调和强度的时间变化，以分类说话人的情感状态，如快乐、悲伤或愤怒。这有助于检测和理解通过语音传达的潜在情感，适用于客户情感分析、虚拟助手和心理健康监测等各种应用。
- en: IV-C 3D Convolutions for Volumetric Data
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 体积数据的 3D 卷积
- en: Three-dimensional (3D) convolutions are specifically designed to handle volumetric
    data, such as 3D medical images or video data [[43](#bib.bib43)]. 3D convolutions
    possess the capability to simultaneously process spatial and temporal dimensions,
    thereby capturing intricate patterns and distinctive features across all three
    dimensions. In medical imaging, 3D convolutions are vital in jobs like finding
    where tumors are. The model uses 3D medical scans to figure out where the important
    spatial and surrounding details are, which helps accurately locate and describe
    tumors [[44](#bib.bib44)][[45](#bib.bib45)].
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 三维（3D）卷积专门设计用于处理体积数据，如 3D 医学图像或视频数据[[43](#bib.bib43)]。3D 卷积具备同时处理空间和时间维度的能力，从而捕捉所有三个维度中的复杂模式和独特特征。在医学成像中，3D
    卷积对于寻找肿瘤的位置至关重要。模型利用 3D 医学扫描来识别重要的空间及周围细节，帮助准确定位和描述肿瘤[[44](#bib.bib44)][[45](#bib.bib45)]。
- en: The use of 3D convolutions has gone beyond just tumors and is used in various
    medical imaging tasks like picking out different parts of the body, spotting issues,
    and classifying diseases. This method lets the model see the whole volume of a
    medical scan, rather than just individual parts, and consider how different slices
    are related in space. This comprehensive approach allows the model to effectively
    capture the overall structure of the target organ or an anomaly, resulting in
    improved diagnostic accuracy and better patient outcomes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 3D卷积的应用已经超越了肿瘤，还用于各种医学影像任务，如提取身体不同部位、发现问题和分类疾病。这种方法使模型能够查看医学扫描的整个体积，而不仅仅是个别部分，并考虑不同切片在空间中的关系。这种全面的方法使模型能够有效捕捉目标器官或异常的整体结构，从而提高诊断准确性和患者预后。
- en: For instance, in tumor segmentation, 3D convolutions can be used to analyze
    a series of consecutive medical scans to identify the size and location of tumors
    over time, allowing doctors to track their growth and plan targeted treatments.
    This helps improve the accuracy and efficiency of tumor identification, leading
    to better patient outcomes.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在肿瘤分割中，可以使用3D卷积分析一系列连续的医学扫描，以识别肿瘤的大小和位置，帮助医生跟踪其生长并规划针对性的治疗。这有助于提高肿瘤识别的准确性和效率，从而改善患者预后。
- en: In addition to operating on raw medical images and videos, 3D convolutions can
    be applied to process point cloud data through voxelization [[101](#bib.bib101)].
    As point clouds represent 3D geometry as an unordered set of points without connectivity,
    a common approach is to first discretize the continuous 3D space into regular
    volumetric grids called voxels. Each voxel is assigned a feature vector, such
    as the number of points or aggregated point properties within its volume.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 除了处理原始医学图像和视频外，3D卷积还可以通过体素化处理点云数据[[101](#bib.bib101)]。由于点云将3D几何表示为无连接的无序点集，常见的方法是首先将连续的3D空间离散化为称为体素的规则体积网格。每个体素被分配一个特征向量，例如体素内点的数量或聚合的点属性。
- en: Voxelizing the point cloud allows existing 3D convolutional kernel operations
    to be directly applied. Early works divided the spatial domain into coarse voxels
    and maxpooled point features inside each voxel [[101](#bib.bib101)]. More advanced
    methods utilize sparse convolutions over fine-grained voxels or use dilated kernels
    with gaps to control the receptive field size. Multi-scale voxels have also been
    explored to capture both local and global point features [[126](#bib.bib126)][[127](#bib.bib127)].
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对点云进行体素化允许直接应用现有的3D卷积核操作。早期的工作将空间域划分为粗略的体素，并对每个体素中的点特征进行最大池化[[101](#bib.bib101)]。更先进的方法利用稀疏卷积对细粒度体素进行处理，或使用膨胀卷积核控制感受野的大小。还探讨了多尺度体素以捕捉局部和全局点特征[[126](#bib.bib126)][[127](#bib.bib127)]。
- en: After 3D convolution and pooling, the extracted voxel features can be decoded
    back to the original point cloud domain for subsequent 3D fully connected or Transformer
    layers [[130](#bib.bib130)]. Voxel representation serves as an efficient intermediary
    that not only maintains the spatial structure required by CNNs but also allows
    points of variable density[[128](#bib.bib128)][[129](#bib.bib129)][[130](#bib.bib130)].
    This two-stage voxel-based approach enables end-to-end training of 3D CNNs for
    point clouds.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在3D卷积和池化之后，提取的体素特征可以解码回原始点云域，以进行后续的3D全连接或Transformer层[[130](#bib.bib130)]。体素表示作为一个高效的中介，不仅保持了CNN所需的空间结构，还允许点的可变密度[[128](#bib.bib128)][[129](#bib.bib129)][[130](#bib.bib130)]。这种两阶段的体素化方法使得3D
    CNNs对点云进行端到端训练成为可能。
- en: IV-D Dilated Convolutions and Their Advantages
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 膨胀卷积及其优势
- en: 'Dilated convolutions (See Fig. [9](#S4.F9 "Figure 9 ‣ IV-D Dilated Convolutions
    and Their Advantages ‣ IV Types of Convolution in Deep Learning ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends")), also known as atrous convolutions, are a variant of traditional convolutions
    that introduce gaps (dilation) between kernel elements. This gap enables for an
    increased receptive field without increasing the number of parameters, making
    dilated convolutions more computationally efficient [[46](#bib.bib46)].'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积（见图[9](#S4.F9 "图 9 ‣ IV-D 膨胀卷积及其优势 ‣ IV 深度学习中的卷积类型 ‣ 深度学习卷积的全面调查：应用、挑战和未来趋势")），也称为扩张卷积，是传统卷积的一个变体，它在卷积核元素之间引入间隙（膨胀）。这种间隙使得在不增加参数数量的情况下扩大感受野，从而使膨胀卷积在计算上更为高效[[46](#bib.bib46)]。
- en: '![Refer to caption](img/e8e3673677dfbd79cfdd215b598f3fda.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e8e3673677dfbd79cfdd215b598f3fda.png)'
- en: 'Figure 9: Dilation Convolution with multiple dilation rate with 3 x 3 kernel
    size [[74](#bib.bib74)]'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：具有3 x 3核大小的多膨胀率膨胀卷积[[74](#bib.bib74)]
- en: Dilated convolutions find application in applications like semantic segmentation,
    where they enable the model to capture broader contextual information without
    compromising computational efficiency [[47](#bib.bib47)].
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积在诸如语义分割等应用中发挥作用，它使模型能够捕捉更广泛的上下文信息，而不影响计算效率[[47](#bib.bib47)]。
- en: In semantic segmentation applications, dilated convolutions are particularly
    useful because they enable the model to capture broader contextual information.
    By introducing gaps between kernel elements, dilated convolutions increase the
    receptive field without adding more parameters. This means that the model can
    understand the surrounding context of each pixel or object in the image without
    sacrificing computational efficiency. This value is important in applications
    like semantic segmentation, where accurately identifying and classifying objects
    within an image is essential.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义分割应用中，膨胀卷积特别有用，因为它们使模型能够捕捉更广泛的上下文信息。通过在卷积核元素之间引入间隙，膨胀卷积增加了感受野，而不会增加更多的参数。这意味着模型可以理解图像中每个像素或对象的周围上下文，而不会牺牲计算效率。这在语义分割等应用中尤为重要，因为准确识别和分类图像中的对象至关重要。
- en: IV-E Grouped Convolutions for Efficiency
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 提高效率的分组卷积
- en: 'Grouped convolutions (See Fig. [10](#S4.F10 "Figure 10 ‣ IV-E Grouped Convolutions
    for Efficiency ‣ IV Types of Convolution in Deep Learning ‣ A Comprehensive Survey
    of Convolutions in Deep Learning: Applications, Challenges, and Future Trends"))
    involve dividing the input and output channels of a convolutional layer into groups.
    Within each group, separate convolutions are performed, which are then concatenated
    to produce the final output. This technique significantly reduces computational
    cost and memory consumption while promoting model parallelism [[48](#bib.bib48)].
    Grouped convolutions are commonly used in large-scale CNN architectures to reduce
    training time and enhance the scalability of DL models [[49](#bib.bib49)].'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '分组卷积（见图[10](#S4.F10 "Figure 10 ‣ IV-E Grouped Convolutions for Efficiency ‣
    IV Types of Convolution in Deep Learning ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")）涉及将卷积层的输入和输出通道分成若干组。在每组内，执行独立的卷积操作，然后将其连接以生成最终输出。这种技术显著降低了计算成本和内存消耗，同时促进了模型的并行性[[48](#bib.bib48)]。分组卷积在大规模CNN架构中被广泛使用，以减少训练时间并增强DL模型的可扩展性[[49](#bib.bib49)]。'
- en: In addition to reducing computational cost and memory consumption, grouped convolutions
    also offer other advantages. One of the main benefits is improved model parallelism,
    which provides for better utilization of parallel computing resources. This is
    especially important in large-scale CNN architectures where training time can
    be a bottleneck. By dividing the input and output channels into groups, the convolutions
    can be performed in parallel, speeding up the entire training process. Furthermore,
    the scalability of DL models is enhanced with grouped convolutions, making it
    easier to deal with larger datasets and more complex applications.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 除了降低计算成本和内存消耗外，分组卷积还提供了其他优点。主要优点之一是提高了模型的并行性，从而更好地利用并行计算资源。这在大型CNN架构中尤为重要，因为训练时间可能成为瓶颈。通过将输入和输出通道分成组，卷积可以并行执行，从而加快整个训练过程。此外，分组卷积增强了DL模型的可扩展性，使处理更大数据集和更复杂应用变得更加容易。
- en: '![Refer to caption](img/cb515e8024203151c7f5d27635f276d2.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cb515e8024203151c7f5d27635f276d2.png)'
- en: 'Figure 10: Grouped convolution involves dividing the channels of a convolutional
    layer into 3 groups'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：分组卷积涉及将卷积层的通道分成3组
- en: '![Refer to caption](img/106508688b05f0efeac7bb3b6192c26d.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/106508688b05f0efeac7bb3b6192c26d.png)'
- en: 'Figure 11: The detailed overview of advanced convolutions techniques'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：高级卷积技术的详细概述
- en: For example, in image classification applications, a large-scale CNN architecture
    such as ResNet can benefit from model parallelism using grouped convolutions.
    By dividing the input and output channels into groups, different subsets of the
    model can be trained in parallel on multiple GPUs or distributed systems. This
    not only reduces the training time but also allows for better resource utilization,
    eventually improving the scalability of the DL model to handle larger datasets
    and more complex image recognition applications.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图像分类应用中，大规模 CNN 架构如 ResNet 可以通过使用组卷积受益于模型并行性。通过将输入和输出通道分成多个组，可以在多个 GPU 或分布式系统上并行训练模型的不同子集。这不仅减少了训练时间，还允许更好的资源利用，最终提高了深度学习模型的可扩展性，以处理更大的数据集和更复杂的图像识别应用。
- en: In conclusion, DL offers a diverse range of convolutional techniques to accommodate
    different data types and applications. From 2D convolutions for image recognition
    to 1D convolutions for sequential data and 3D convolutions for volumetric data,
    each convolution type has its unique advantages. Additionally, dilated convolutions
    and grouped convolutions serve as efficient alternatives, addressing specific
    challenges in DL models. Understanding the characteristics and applications of
    these convolution types empowers researchers and practitioners to design efficient
    and effective models for a wide array of applications.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，深度学习提供了一系列多样化的卷积技术，以适应不同的数据类型和应用。从用于图像识别的 2D 卷积到用于序列数据的 1D 卷积，再到用于体积数据的 3D
    卷积，每种卷积类型都有其独特的优势。此外，膨胀卷积和组卷积作为高效的替代方案，解决了深度学习模型中的特定挑战。了解这些卷积类型的特性和应用，使研究人员和从业者能够设计出高效且有效的模型，以满足各种应用的需求。
- en: IV-F Evolution of CNN Architectures
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F CNN 架构的演变
- en: 'Since the early origins of CNNs, there has been a rapid evolution in CNN architectures
    (See Fig. [11](#S4.F11 "Figure 11 ‣ IV-E Grouped Convolutions for Efficiency ‣
    IV Types of Convolution in Deep Learning ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")) [[49](#bib.bib49)]
    over the past decade to enhance performance and efficiency [51]. Some key developments
    include:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 CNN 的早期起源以来，CNN 架构在过去十年中迅速演变（见图 [11](#S4.F11 "图 11 ‣ IV-E 组卷积以提高效率 ‣ IV 深度学习中的卷积类型
    ‣ 深度学习中卷积的全面调查：应用、挑战和未来趋势")） [[49](#bib.bib49)]，以提高性能和效率 [51]。一些关键的发展包括：
- en: •
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inception modules (2014) - The Inception architecture introduced convolutional
    blocks with multiple filter sizes to capture features at various scales [[52](#bib.bib52)].
    This improves both accuracy and computational efficiency.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Inception 模块（2014） - Inception 架构引入了具有多个滤波器尺寸的卷积块，以捕捉不同尺度的特征 [[52](#bib.bib52)]。这提高了准确性和计算效率。
- en: •
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ResNets (2015) - Residual networks allow the training of much deeper CNNs through
    shortcut connections that bypass multiple layers [[53](#bib.bib53)]. They reduce
    degradation in very deep models.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ResNets（2015） - 残差网络通过跳过多个层的快捷连接来训练更深的 CNN [[53](#bib.bib53)]。它们减少了非常深层模型中的退化问题。
- en: •
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DenseNets (2016) - These connect each layer to all subsequent layers for maximum
    information flow and feature reuse. This reduces the number of parameters [[54](#bib.bib54)].
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DenseNets（2016） - 这些网络将每一层连接到所有后续层，以实现最大的信息流动和特征重用。这减少了参数的数量 [[54](#bib.bib54)]。
- en: •
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MobileNets (2017) - Designed specifically for mobile applications, they use
    depthwise separable convolutions to minimize model size and latency [[55](#bib.bib55)].
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MobileNets（2017） - 专门为移动应用设计，它们使用深度可分离卷积来最小化模型大小和延迟 [[55](#bib.bib55)]。
- en: •
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: EfficientNets (2019) - By systematically scaling network dimensions, these achieve
    much better efficiency-accuracy trade-offs [[55](#bib.bib55)].
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EfficientNets（2019） - 通过系统地调整网络维度，这些网络实现了更好的效率与准确性之间的权衡 [[55](#bib.bib55)]。
- en: 'The evolution of CNN architectures (See Fig. [11](#S4.F11 "Figure 11 ‣ IV-E
    Grouped Convolutions for Efficiency ‣ IV Types of Convolution in Deep Learning
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) has been crucial to their widespread adoption across vision
    applications.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 架构的演变（见图 [11](#S4.F11 "图 11 ‣ IV-E 组卷积以提高效率 ‣ IV 深度学习中的卷积类型 ‣ 深度学习中卷积的全面调查：应用、挑战和未来趋势")）对它们在视觉应用中的广泛采用至关重要。
- en: V Advanced Convolutional Techniques
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 高级卷积技术
- en: 'This section provides a detailed overview of advanced convolutional techniques
    (See Fig. [12](#S5.F12 "Figure 12 ‣ V Advanced Convolutional Techniques ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends")). A clear and informative summary of these techniques is available in
    Table [IV](#S5.T4 "TABLE IV ‣ V-A Transposed Convolutions and Upsampling ‣ V Advanced
    Convolutional Techniques ‣ A Comprehensive Survey of Convolutions in Deep Learning:
    Applications, Challenges, and Future Trends"). By reviewing this table, readers
    can gain a better understanding of the state-of-the-art convolutional techniques
    and their potential uses.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了先进的卷积技术（参见图 [12](#S5.F12 "图 12 ‣ V 先进卷积技术 ‣ 深度学习中的卷积综合调查：应用、挑战和未来趋势")）。这些技术的清晰和有用的总结可以在表
    [IV](#S5.T4 "表 IV ‣ V-A 转置卷积和上采样 ‣ V 先进卷积技术 ‣ 深度学习中的卷积综合调查：应用、挑战和未来趋势") 中找到。通过审阅此表，读者可以更好地理解最先进的卷积技术及其潜在用途。
- en: '![Refer to caption](img/47c61927d83cfadaec28e28f2371ca42.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/47c61927d83cfadaec28e28f2371ca42.png)'
- en: 'Figure 12: The trend of CNNs over time based on the released year and amount
    of parameters and their types'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 基于发布时间和参数量及其类型的 CNN 趋势'
- en: V-A Transposed Convolutions and Upsampling
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 转置卷积和上采样
- en: Transposed convolutions—also referred to as deconvolutions or fractionally stridden
    convolutions—are sophisticated methods for upsampling feature maps [[57](#bib.bib57)].
    Transposed convolutions, as opposed to conventional convolutions, increase the
    feature map size, enabling the model to reconstruct higher-resolution representations
    from lower-resolution inputs [[58](#bib.bib58)]. Traditional convolutions reduce
    spatial dimensions. In processes like image segmentation [[59](#bib.bib59)], image
    creation [[60](#bib.bib60)], and image-to-image translation [[61](#bib.bib61)],
    they are essential. Transposed convolutions employ padding and stride values to
    regulate the upsampling process and learnable parameters to choose the output
    size.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积——也称为去卷积或分数步幅卷积——是用于上采样特征图的复杂方法 [[57](#bib.bib57)]。与传统卷积不同，转置卷积增加特征图的大小，使模型能够从低分辨率输入重建高分辨率表示
    [[58](#bib.bib58)]。传统卷积减少空间维度。在图像分割 [[59](#bib.bib59)]、图像创建 [[60](#bib.bib60)]
    和图像到图像的转换 [[61](#bib.bib61)] 等过程中，它们是必不可少的。转置卷积使用填充和步幅值来调节上采样过程，并利用可学习参数选择输出大小。
- en: Transposed convolution can create artifacts or checkerboard patterns in generated
    feature maps, due to overlapping receptive fields. To prevent this, stride, padding,
    and dilation are used to control the output resolution and reduce these artifacts.
    In the field of image generation, transposed convolutions are used to upscale
    low-resolution images into high-resolution ones. To ensure the generated images
    are free of artifacts or checkerboard patterns, stride, padding, and dilation
    are adjusted to control the output resolution and enhance the quality of the generated
    images.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积可能会在生成的特征图中产生伪影或棋盘图案，因为感受野重叠。为了防止这种情况，使用步幅、填充和膨胀来控制输出分辨率并减少这些伪影。在图像生成领域，转置卷积用于将低分辨率图像上采样为高分辨率图像。为了确保生成的图像没有伪影或棋盘图案，会调整步幅、填充和膨胀来控制输出分辨率并提升生成图像的质量。
- en: 'TABLE IV: The Comparison Provides an Overview of The Characteristics and Functionalities
    of Different Convolution Types - Part 1'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 比较提供了不同卷积类型的特征和功能概览 - 第 1 部分'
- en: '| Convolution Technique | Transposed Convolutions | DSC | SPP | Attention Mechanism
    | Shift-Invariant |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 卷积技术 | 转置卷积 | DSC | SPP | 注意机制 | 移位不变 |'
- en: '| Purpose | Upsampling | Parameter Reduction |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 上采样 | 参数减少 |'
- en: '&#124; Handling Varying &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 处理变化 &#124;'
- en: '&#124; Input Sizes &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入尺寸 &#124;'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Focus on Relevant &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关注相关 &#124;'
- en: '&#124; Features &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特点 &#124;'
- en: '| Invariance |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 不变性 |'
- en: '| Parameters | Learnable | Learnable | No parameters | Learnable | Learnable
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 可学习 | 可学习 | 无参数 | 可学习 | 可学习 |'
- en: '| Computational Cost | High | Low | Low | Normal | High |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 计算成本 | 高 | 低 | 低 | 普通 | 高 |'
- en: '| Parameter Efficiency | Low | High | High | Low | Normal |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 参数效率 | 低 | 高 | 高 | 低 | 普通 |'
- en: '| Upsampling | Yes | No | No | No | No |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 上采样 | 是 | 否 | 否 | 否 | 否 |'
- en: '| Spatial Handling | Spatially Invariant | Spatially Invariant | Variable regions
    | Spatially Invariant | Spatially Invariant |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 空间处理 | 空间不变 | 空间不变 | 可变区域 | 空间不变 | 空间不变 |'
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Long-range &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长程 &#124;'
- en: '&#124; Dependencies &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 依赖关系 &#124;'
- en: '| No | No | No | Yes | No |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 否 | 否 | 是 | 否 |'
- en: '| Translation Invariance | Yes | Yes | Yes | Yes | Yes |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 翻译不变性 | 是 | 是 | 是 | 是 | 是 |'
- en: '| Rotation Invariance | No | No | No | No | No |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 旋转不变性 | 否 | 否 | 否 | 否 | 否 |'
- en: '| Interpretability | Low | Low | Low | Low | Low |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 可解释性 | 低 | 低 | 低 | 低 | 低 |'
- en: '| Model Size | Large | Small | Small | Small | Large |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小 | 大 | 小 | 小 | 小 | 大 |'
- en: '| Versatility | Normal | High | High | High | Normal |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 多功能性 | 普通 | 高 | 高 | 高 | 普通 |'
- en: '| Practical Applications |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 实际应用 |'
- en: '&#124; Image Segmentation, &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像分割, &#124;'
- en: '&#124; Image Super-Resolution, &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像超分辨率, &#124;'
- en: '&#124; Image Generation &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像生成 &#124;'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mobile Vision Applications, &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 移动视觉应用, &#124;'
- en: '&#124; Real-time Object Detection &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 实时目标检测 &#124;'
- en: '|'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image Classification, &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像分类, &#124;'
- en: '&#124; Object Detection, &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标检测, &#124;'
- en: '&#124; Semantic Segmentation &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语义分割 &#124;'
- en: '|'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image Captioning, &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像描述, &#124;'
- en: '&#124; Visual Question Answering &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉问答 &#124;'
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image Recognition, &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像识别, &#124;'
- en: '&#124; Object Detection, &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标检测, &#124;'
- en: '&#124; Image Filtering &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像滤波 &#124;'
- en: '|'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'TABLE V: The Comparison Provides an Overview of The Characteristics and Functionalities
    of Different Convolution Types - Part 2'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 比较表概述了不同卷积类型的特性和功能 - 第 2 部分'
- en: '| Convolution Technique | Steerable Convolution | Capsule Networks | NAS |
    GAN | VIT |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 卷积技术 | 可调卷积 | 胶囊网络 | NAS | GAN | VIT |'
- en: '| Purpose | Efficiency and Invariance | Invariance | Efficiency | Synthesis
    | Long-range dependencies |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 效率与不变性 | 不变性 | 效率 | 合成 | 长程依赖 |'
- en: '| Parameters | Learnable | Learnable capsules | Architecture search | Learnable
    | Learnable |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 可学习 | 可学习胶囊 | 架构搜索 | 可学习 | 可学习 |'
- en: '| Computational Cost | Low | High | High | High | Higher |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 计算成本 | 低 | 高 | 高 | 高 | 更高 |'
- en: '| Parameter Efficiency | High | Normal | High | Low | Normal |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 参数效率 | 高 | 普通 | 高 | 低 | 普通 |'
- en: '| Upsampling | No | No | No | No | No |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 上采样 | 否 | 否 | 否 | 否 | 否 |'
- en: '| Spatial Handling | Spatially Invariant | Spatially Invariant | Spatially
    variant | Spatially Invariant | Spatially Invariant |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 空间处理 | 空间不变 | 空间不变 | 空间可变 | 空间不变 | 空间不变 |'
- en: '| Long-range Dependencies | No | No | No | No | Yes |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 长程依赖 | 否 | 否 | 否 | 否 | 是 |'
- en: '| Translation Invariance | Yes | Yes | Yes | No | Yes |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 翻译不变性 | 是 | 是 | 是 | 否 | 是 |'
- en: '| Rotation Invariance | Yes | Yes | No | No | Yes |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 旋转不变性 | 是 | 是 | 否 | 否 | 是 |'
- en: '| Interpretability | Low | Low | Low | Low | High |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 可解释性 | 低 | 低 | 低 | 低 | 高 |'
- en: '| Model Size | Normal | Normal | Large | Large | Large |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小 | 普通 | 普通 | 大 | 大 | 大 |'
- en: '| Versatility | Low | Low | Low | Low | High |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 多功能性 | 低 | 低 | 低 | 低 | 高 |'
- en: '| Practical Applications |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 实际应用 |'
- en: '&#124; Image Filtering, &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像滤波, &#124;'
- en: '&#124; Edge Detection, &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边缘检测, &#124;'
- en: '&#124; Pattern Recognition &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模式识别 &#124;'
- en: '|'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Object Recognition, &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标识别, &#124;'
- en: '&#124; Image Segmentation, &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像分割, &#124;'
- en: '&#124; Medical Imaging &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医学成像 &#124;'
- en: '|'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Customized CNN Architectures, &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自定义 CNN 架构, &#124;'
- en: '&#124; Resource-Constrained Devices &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 资源受限设备 &#124;'
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image Synthesis, &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像合成, &#124;'
- en: '&#124; Style Transfer, &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 风格迁移, &#124;'
- en: '&#124; Data Augmentation &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据增强 &#124;'
- en: '|'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image recognition, NLP, &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像识别, NLP, &#124;'
- en: '&#124; diverse tasks &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多样化任务 &#124;'
- en: '|'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: V-B Depthwise Separable Convolutions (DSC)
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 深度可分离卷积（DSC）
- en: 'Depthwise separable convolutions (See the purple box in Fig. [13](#S5.F13 "Figure
    13 ‣ V-B Depthwise Separable Convolutions (DSC) ‣ V Advanced Convolutional Techniques
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) are an efficient alternative to traditional convolutions,
    particularly in resource-constrained environments [[62](#bib.bib62)][[63](#bib.bib63)].
    They split the convolution process into two steps (See Fig. [13](#S5.F13 "Figure
    13 ‣ V-B Depthwise Separable Convolutions (DSC) ‣ V Advanced Convolutional Techniques
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")) depthwise convolutions [[64](#bib.bib64)] and pointwise convolutions
    [[65](#bib.bib65)] [[276](#bib.bib276), [277](#bib.bib277), [278](#bib.bib278),
    [279](#bib.bib279)]. Depthwise convolutions apply a separate kernel to each input
    channel, capturing spatial patterns independently for each channel. Pointwise
    convolutions then use 1x1 convolutions to combine the output channels from the
    depthwise step, effectively aggregating the information [[66](#bib.bib66)]. Depthwise
    separable convolutions significantly reduce the number of parameters and computation
    while maintaining model performance, making them popular in mobile and embedded
    applications [[67](#bib.bib67)].'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积（见图 [13](#S5.F13 "图 13 ‣ V-B 深度可分离卷积（DSC） ‣ V 高级卷积技术 ‣ 深度学习中卷积的全面调查：应用、挑战和未来趋势")
    的紫色框）是传统卷积的高效替代方案，尤其是在资源受限的环境中 [[62](#bib.bib62)][[63](#bib.bib63)]。它们将卷积过程分为两个步骤（见图
    [13](#S5.F13 "图 13 ‣ V-B 深度可分离卷积（DSC） ‣ V 高级卷积技术 ‣ 深度学习中卷积的全面调查：应用、挑战和未来趋势")），即深度卷积
    [[64](#bib.bib64)] 和逐点卷积 [[65](#bib.bib65)] [[276](#bib.bib276), [277](#bib.bib277),
    [278](#bib.bib278), [279](#bib.bib279)]。深度卷积对每个输入通道应用一个单独的卷积核，独立捕捉每个通道的空间模式。逐点卷积则使用
    1x1 卷积来结合来自深度卷积步骤的输出通道，有效地聚合信息 [[66](#bib.bib66)]。深度可分离卷积显著减少了参数数量和计算量，同时保持模型性能，使其在移动和嵌入式应用中非常受欢迎
    [[67](#bib.bib67)]。
- en: '![Refer to caption](img/cd6d866e2a320d92f8e63a6f46e16a8d.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cd6d866e2a320d92f8e63a6f46e16a8d.png)'
- en: 'Figure 13: The Box with Purple color represents the Depthwise Convolution and
    the box with red color represents Pointwise Convolution (in pointwise a 1 x 1
    convolution is used)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：紫色框表示深度卷积，红色框表示逐点卷积（逐点卷积中使用了 1x1 卷积）
- en: By decoupling spatial filtering from cross-channel filtering, depthwise convolution
    achieves higher computational efficiency and is well-suited for resource-constrained
    environments. MobileNet and Xception are popular CNN architectures that use depthwise
    convolution to reduce model size and improve inference speed without compromising
    performance significantly.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将空间滤波与跨通道滤波解耦，深度卷积实现了更高的计算效率，适合资源受限的环境。MobileNet 和 Xception 是使用深度卷积来减小模型大小和提高推理速度而不显著影响性能的流行
    CNN 架构。
- en: V-C Spatial Pyramid Pooling (SPP)
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 空间金字塔池化（SPP）
- en: Spatial pyramid pooling (SPP) is a technique used to handle inputs of varying
    sizes and aspect ratios in CNNs [[68](#bib.bib68)][[280](#bib.bib280), [281](#bib.bib281),
    [282](#bib.bib282), [283](#bib.bib283), [284](#bib.bib284), [285](#bib.bib285)].
    It divides the input feature maps into different regions of interest and applies
    max-pooling or average-pooling to each region independently. The resulting pooled
    features are then concatenated to form a fixed-length representation, which is
    fed into fully connected layers for further processing. SPP enables the CNN to
    accept input images of different sizes and produces consistent feature maps, making
    it useful in object detection and image segmentation applications [[69](#bib.bib69)].
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 空间金字塔池化（SPP）是一种处理 CNN 输入不同尺寸和长宽比的技术 [[68](#bib.bib68)][[280](#bib.bib280), [281](#bib.bib281),
    [282](#bib.bib282), [283](#bib.bib283), [284](#bib.bib284), [285](#bib.bib285)]。它将输入特征图划分为不同的感兴趣区域，并对每个区域独立应用最大池化或平均池化。然后将得到的池化特征连接起来，形成一个固定长度的表示，并输入到全连接层进行进一步处理。SPP
    使 CNN 能够接受不同尺寸的输入图像，并生成一致的特征图，这使得它在物体检测和图像分割应用中非常有用 [[69](#bib.bib69)]。
- en: V-D Attention Mechanisms in Convolutions
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 卷积中的注意力机制
- en: Attention mechanisms in convolutions allow the model to focus on relevant parts
    of the input, emphasizing specific regions during feature extraction [[70](#bib.bib70)].
    These mechanisms assign weights to different spatial locations based on their
    importance. Self-attention mechanisms [[70](#bib.bib70)], like those used in transformers,
    have been adapted for use in convolutions. They enable the network to capture
    long-range dependencies and context, improving the model’s ability to recognize
    complex patterns and relationships.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积中的注意力机制使模型能够关注输入的相关部分，在特征提取过程中强调特定区域[[70](#bib.bib70)]。这些机制根据不同空间位置的重要性分配权重。自注意力机制[[70](#bib.bib70)]，如变换器中使用的那样，已被适应用于卷积中。它们使网络能够捕捉长距离依赖和上下文，改善模型识别复杂模式和关系的能力。
- en: V-E Shift-Invariant and Steerable Convolutions
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 变换不变和可引导卷积
- en: Shift-invariant convolutions are designed to be insensitive to small translations
    in the input data [[71](#bib.bib71)] [[286](#bib.bib286), [287](#bib.bib287),
    [288](#bib.bib288)]. They ensure that the learned features remain consistent regardless
    of the object’s position within the input image. This property is crucial for
    object detection applications, where the object’s location might vary within the
    image [[27](#bib.bib27)]. Steerable convolutions are filters that can be rotated
    to different angles, allowing the model to learn orientation-sensitive features
    in an orientation-invariant manner [[289](#bib.bib289), [290](#bib.bib290), [291](#bib.bib291)].
    These convolutions are often used in applications like text recognition, where
    the orientation of text can vary.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 变换不变卷积旨在对输入数据中的小幅平移不敏感[[71](#bib.bib71)] [[286](#bib.bib286), [287](#bib.bib287),
    [288](#bib.bib288)]。它们确保所学特征在输入图像中物体的位置变化时保持一致。这一属性对于对象检测应用至关重要，因为对象在图像中的位置可能会变化[[27](#bib.bib27)]。可引导卷积是可以旋转到不同角度的滤波器，使模型能够以方向不变的方式学习方向敏感特征[[289](#bib.bib289),
    [290](#bib.bib290), [291](#bib.bib291)]。这些卷积通常用于诸如文本识别等应用，其中文本的方向可能会有所变化。
- en: V-F Recent Advancements and Innovations
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F 最近的进展和创新
- en: V-F1 Capsule Networks
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-F1 胶囊网络
- en: Capsule Networks, introduced by Geoffrey Hinton and his team, is a revolutionary
    advancement in CNNs [[75](#bib.bib75)]. They aim to address the limitations of
    traditional CNNs, particularly in handling spatial hierarchies and viewpoint variations
    [[292](#bib.bib292), [293](#bib.bib293), [294](#bib.bib294), [295](#bib.bib295),
    [296](#bib.bib296), [297](#bib.bib297), [298](#bib.bib298)]. Capsule Networks
    use capsules as fundamental units, which are groups of neurons that represent
    various properties of an entity, such as its pose, deformation, and parts.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**胶囊网络**由**Geoffrey Hinton**及其团队引入，是CNN的革命性进展[[75](#bib.bib75)]。它们旨在解决传统CNN的局限性，特别是在处理空间层次结构和视点变化方面[[292](#bib.bib292),
    [293](#bib.bib293), [294](#bib.bib294), [295](#bib.bib295), [296](#bib.bib296),
    [297](#bib.bib297), [298](#bib.bib298)]。**胶囊网络**使用胶囊作为基本单元，这些胶囊是代表实体各种属性的神经元组，如姿态、变形和部分。'
- en: Capsule Networks offer dynamic routing mechanisms to route information between
    capsules, allowing them to model complex hierarchical relationships more effectively.
    This enables the network to recognize objects with various poses and appearances,
    making Capsule Networks more robust to transformations and occlusions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**胶囊网络**提供了动态路由机制，以在胶囊之间路由信息，使其能更有效地建模复杂的层次关系。这使得网络能够识别具有不同姿态和外观的对象，使**胶囊网络**对变换和遮挡具有更强的鲁棒性。'
- en: V-F2 Neural Architecture Search for Convolutions
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-F2 卷积神经架构搜索
- en: Neural Architecture Search (NAS) is an automated approach to designing CNN architectures
    [[76](#bib.bib76)][[81](#bib.bib81)]. Instead of relying on human-designed architectures,
    NAS employs search algorithms and neural networks to discover architectures that
    perform well on specific applications [[76](#bib.bib76)]. This technique has led
    to the development of state-of-the-art CNNs that outperform hand-crafted models
    [[299](#bib.bib299), [300](#bib.bib300), [301](#bib.bib301), [302](#bib.bib302),
    [303](#bib.bib303), [304](#bib.bib304), [305](#bib.bib305), [306](#bib.bib306),
    [307](#bib.bib307), [308](#bib.bib308), [309](#bib.bib309)].
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索（NAS）是一种自动化设计CNN架构的方法[[76](#bib.bib76)][[81](#bib.bib81)]。NAS利用搜索算法和神经网络来发现在特定应用中表现良好的架构，而不是依赖于人工设计的架构[[76](#bib.bib76)]。这种技术促成了最先进的CNN的发展，其性能超越了手工设计的模型[[299](#bib.bib299),
    [300](#bib.bib300), [301](#bib.bib301), [302](#bib.bib302), [303](#bib.bib303),
    [304](#bib.bib304), [305](#bib.bib305), [306](#bib.bib306), [307](#bib.bib307),
    [308](#bib.bib308), [309](#bib.bib309)]。
- en: NAS for convolutions involves exploring various convolutional designs, including
    different kernel sizes, depths, and connectivity patterns [[82](#bib.bib82)].
    It evaluates each architecture on a validation set, and through a process of evolution
    or optimization, identifies the best-performing architecture.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的 NAS 涉及探索各种卷积设计，包括不同的卷积核大小、深度和连接模式[[82](#bib.bib82)]。它在验证集上评估每个架构，通过进化或优化过程，识别出表现最佳的架构。
- en: In the scenario of self-autonomous vehicle navigation, NAS for convolutions
    could be used to design an optimal convolutional neural network architecture specifically
    tailored for processing and analyzing various types of visual data collected by
    the vehicle’s sensors. By exploring different convolutional designs, such as varying
    kernel sizes, depths, and connectivity patterns, NAS could identify the most effective
    architecture for accurately detecting objects and recognizing road signs in real-time.
    This would ultimately improve the vehicle’s ability to navigate autonomously and
    make informed decisions based on its visual perception.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶车辆导航的场景中，卷积神经网络架构搜索（NAS）可以用于设计一个最佳的卷积神经网络架构，专门针对处理和分析车辆传感器收集的各种视觉数据。通过探索不同的卷积设计，如变化的卷积核大小、深度和连接模式，NAS
    可以识别出最有效的架构，以准确实时地检测物体和识别道路标志。这将最终提高车辆的自主导航能力，并根据其视觉感知做出明智的决策。
- en: V-F3 Generative Adversarial Networks
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-F3 生成对抗网络
- en: 'Generative Adversarial Networks (GANs) are a class of DL models used for generative
    applications, such as image synthesis, style transfer, and data augmentation [[310](#bib.bib310),
    [311](#bib.bib311), [312](#bib.bib312), [313](#bib.bib313), [314](#bib.bib314),
    [315](#bib.bib315), [316](#bib.bib316)]. GANs utilize CNNs as key components to
    model the generator and discriminator (See Fig. [14](#S5.F14 "Figure 14 ‣ V-F3
    Generative Adversarial Networks ‣ V-F Recent Advancements and Innovations ‣ V
    Advanced Convolutional Techniques ‣ A Comprehensive Survey of Convolutions in
    Deep Learning: Applications, Challenges, and Future Trends")) [[77](#bib.bib77)][[83](#bib.bib83)][[84](#bib.bib84)].
    The generator is a CNN that generates new samples, such as realistic images, while
    the discriminator is another CNN that aims to distinguish between real and fake
    samples [[77](#bib.bib77)]. These networks are trained adversarially, where the
    generator’s goal is to produce samples that deceive the discriminator, and the
    discriminator’s goal is to become better at distinguishing real from fake [[71](#bib.bib71)][[84](#bib.bib84)].'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '生成对抗网络（GANs）是一类用于生成应用的深度学习模型，如图像合成、风格迁移和数据增强[[310](#bib.bib310), [311](#bib.bib311),
    [312](#bib.bib312), [313](#bib.bib313), [314](#bib.bib314), [315](#bib.bib315),
    [316](#bib.bib316)]。GANs 利用卷积神经网络（CNNs）作为关键组件来建模生成器和判别器（见图 [14](#S5.F14 "Figure
    14 ‣ V-F3 Generative Adversarial Networks ‣ V-F Recent Advancements and Innovations
    ‣ V Advanced Convolutional Techniques ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")）[[77](#bib.bib77)][[83](#bib.bib83)][[84](#bib.bib84)]。生成器是一个
    CNN，用于生成新的样本，如真实的图像，而判别器是另一个 CNN，旨在区分真实样本和虚假样本[[77](#bib.bib77)]。这些网络以对抗的方式进行训练，其中生成器的目标是生成能欺骗判别器的样本，而判别器的目标是变得更擅长区分真实样本和虚假样本[[71](#bib.bib71)][[84](#bib.bib84)]。'
- en: '![Refer to caption](img/8c7343fd1954b03e4c6b4269b91f9418.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8c7343fd1954b03e4c6b4269b91f9418.png)'
- en: 'Figure 14: A simple GAN architecture represented to detect real and fake data
    which generator has generated'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：一个简单的 GAN 架构，用于检测生成器生成的真实和虚假数据
- en: GANs with convolution have revolutionized the field of image generation and
    have produced impressive results in generating high-quality images and realistic
    textures [[266](#bib.bib266), [267](#bib.bib267), [268](#bib.bib268), [269](#bib.bib269),
    [270](#bib.bib270), [271](#bib.bib271), [272](#bib.bib272), [273](#bib.bib273),
    [274](#bib.bib274), [275](#bib.bib275)]. They have also been extended to other
    domains like NLP, audio generation, and video synthesis. This technology has also
    been applied to other areas such as medical imaging, where GANs have been used
    to generate high-resolution and accurate images for diagnostic purposes. Additionally,
    GANs have shown promising results in the field of data augmentation, where they
    can generate synthetic data to increase the size and diversity of training datasets,
    improving the performance of machine learning models.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积的GANs彻底改变了图像生成领域，并在生成高质量图像和逼真纹理方面取得了令人印象深刻的成果[[266](#bib.bib266), [267](#bib.bib267),
    [268](#bib.bib268), [269](#bib.bib269), [270](#bib.bib270), [271](#bib.bib271),
    [272](#bib.bib272), [273](#bib.bib273), [274](#bib.bib274), [275](#bib.bib275)]。它们也已扩展到其他领域，如自然语言处理、音频生成和视频合成。该技术还被应用于医学成像领域，其中GANs被用于生成高分辨率和准确的诊断图像。此外，GANs在数据增强领域表现出令人期待的成果，能够生成合成数据，以增加训练数据集的大小和多样性，从而提升机器学习模型的性能。
- en: For example, in the field of image generation, GANs with convolutional networks
    have been used to create realistic images of non-existent landscapes. The generator
    network creates visually convincing images, while the discriminator network learns
    to identify any flaws or inconsistencies in these generated images, pushing the
    generator to improve its output. This adversarial training process ultimately
    leads to the creation of high-quality and believable images that are indistinguishable
    from real photographs.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图像生成领域，卷积网络的GANs被用于创建虚构景观的逼真图像。生成器网络创建视觉上令人信服的图像，而判别器网络则学习识别这些生成图像中的任何缺陷或不一致，从而促使生成器改进其输出。这种对抗训练过程最终导致生成高质量和逼真的图像，这些图像与真实照片难以区分。
- en: V-G Vision Transformers and Self-Attention Mechanisms
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-G Vision Transformers 和自注意力机制
- en: Through the use of self-attention mechanisms [[85](#bib.bib85)], Vision Transformers
    [[244](#bib.bib244), [245](#bib.bib245), [246](#bib.bib246), [247](#bib.bib247),
    [248](#bib.bib248), [249](#bib.bib249), [250](#bib.bib250), [251](#bib.bib251),
    [252](#bib.bib252), [253](#bib.bib253), [254](#bib.bib254), [255](#bib.bib255),
    [256](#bib.bib256), [257](#bib.bib257), [258](#bib.bib258), [259](#bib.bib259),
    [260](#bib.bib260), [261](#bib.bib261), [262](#bib.bib262), [263](#bib.bib263),
    [264](#bib.bib264), [265](#bib.bib265)] represent an important evolutionary step
    away from traditional computer vision architectures [[86](#bib.bib86), [87](#bib.bib87)]
    . Rather than solely relying on convolutional filters to process visual inputs,
    as has predominantly been the case, they segment images into distinct finite parts
    known as patches [[87](#bib.bib87)]. Each patch focuses on and extracts features
    from a different localized region of the photographic scene. This division of
    images into discrete patches is a major conceptual divergence from how most previous
    approaches operate.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用自注意力机制[[85](#bib.bib85)]，Vision Transformers[[244](#bib.bib244), [245](#bib.bib245),
    [246](#bib.bib246), [247](#bib.bib247), [248](#bib.bib248), [249](#bib.bib249),
    [250](#bib.bib250), [251](#bib.bib251), [252](#bib.bib252), [253](#bib.bib253),
    [254](#bib.bib254), [255](#bib.bib255), [256](#bib.bib256), [257](#bib.bib257),
    [258](#bib.bib258), [259](#bib.bib259), [260](#bib.bib260), [261](#bib.bib261),
    [262](#bib.bib262), [263](#bib.bib263), [264](#bib.bib264), [265](#bib.bib265)]代表了从传统计算机视觉架构向前发展的重要演变步骤[[86](#bib.bib86),
    [87](#bib.bib87)]。与以往主要依赖卷积滤波器处理视觉输入不同，它们将图像分割成称为“补丁”的离散有限部分[[87](#bib.bib87)]。每个补丁关注并提取来自摄影场景不同局部区域的特征。这种将图像划分为离散补丁的方式在概念上与大多数以往方法的操作有着显著的差异。
- en: In conclusion, advanced convolutional techniques have significantly expanded
    the capabilities of CNNs and revolutionized various fields like CV, image synthesis,
    and NLP. From transposed convolution for upsampling to capsule networks for handling
    spatial hierarchies, these innovations have enhanced the efficiency, robustness,
    and expressiveness of CNNs, making them powerful tools for a wide range of applications.
    Moreover, recent advancements, such as NAS and GANs, continue to drive progress
    in the field of DL and hold promise for further breakthroughs in the future.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，先进的卷积技术显著扩展了 CNN 的能力，并彻底改变了计算机视觉、图像合成和自然语言处理等领域。从用于上采样的转置卷积到处理空间层次结构的胶囊网络，这些创新提升了
    CNN 的效率、鲁棒性和表现力，使其成为广泛应用的强大工具。此外，最近的进展，如 NAS 和 GANs，继续推动深度学习领域的发展，并对未来进一步突破充满希望。
- en: VI Applications of Different Convolution Types
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 不同卷积类型的应用
- en: 'We provide a thorough overview of the numerous applications of different convolutional
    types in this section (See Fig. [15](#S6.F15 "Figure 15 ‣ VI Applications of Different
    Convolution Types ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends")). Table [VI](#S6.T6 "TABLE VI ‣ VI-A Image Recognition
    and Classification ‣ VI Applications of Different Convolution Types ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends") provides a brief but comprehensive overview of these applications. Convolutions
    of various types are used in a variety of contexts, demonstrating the flexibility
    and strength of CNNs. Convolutional techniques enable machines to understand and
    interact with complex data, facilitating advancements in a variety of fields and
    enhancing our daily lives. Examples include image recognition, object detection,
    NLP, and medical image analysis.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了不同卷积类型广泛应用的详细概述（见图 [15](#S6.F15 "图 15 ‣ VI 不同卷积类型的应用 ‣ 深度学习中卷积的全面调查：应用、挑战和未来趋势")）。表
    [VI](#S6.T6 "表 VI ‣ VI-A 图像识别与分类 ‣ VI 不同卷积类型的应用 ‣ 深度学习中卷积的全面调查：应用、挑战和未来趋势") 提供了这些应用的简要而全面的概述。各种类型的卷积在多种背景下使用，展示了
    CNN 的灵活性和强大功能。卷积技术使机器能够理解和互动复杂数据，促进了多个领域的进步，提升了我们的日常生活。示例包括图像识别、物体检测、NLP 和医学图像分析。
- en: '![Refer to caption](img/56472502fac659f26439f7933dc48cc5.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/56472502fac659f26439f7933dc48cc5.png)'
- en: 'Figure 15: The applications of CNN techniques which we have discussed in Section
    VI'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：我们在第 VI 节中讨论的 CNN 技术的应用
- en: VI-A Image Recognition and Classification
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 图像识别与分类
- en: There are many uses for CNNs, including image recognition and classification.
    Traditional 2D convolutions are especially useful in these applications. They
    make it possible for deep learning models to accurately classify images into various
    groups and learn crucial features from images. The network’s convolutional layers
    recognize edges, textures, and shapes. The pooling layers reduce the size of the
    image while preserving the data needed for classification. Image recognition and
    classification are used for various tasks, including optical character recognition
    (OCR) [[203](#bib.bib203), [204](#bib.bib204), [205](#bib.bib205), [206](#bib.bib206),
    [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210),
    [211](#bib.bib211)], classifying different animal species, and recognizing handwritten
    numbers [[88](#bib.bib88)]. In competitions like ImageNet, CNNs have displayed
    impressive results, showcasing their abilities for handling wide image classification
    [[89](#bib.bib89)].
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 有许多用途，包括图像识别与分类。传统的二维卷积在这些应用中尤为有用。它们使深度学习模型能够准确地将图像分类为不同的组，并从图像中学习关键特征。网络的卷积层识别边缘、纹理和形状。池化层在保持分类所需的数据的同时，减少了图像的大小。图像识别与分类用于各种任务，包括光学字符识别
    (OCR) [[203](#bib.bib203), [204](#bib.bib204), [205](#bib.bib205), [206](#bib.bib206),
    [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210),
    [211](#bib.bib211)]，不同动物种类的分类，以及手写数字的识别 [[88](#bib.bib88)]。在像 ImageNet 这样的比赛中，CNN
    展示了令人印象深刻的结果，展示了它们处理广泛图像分类的能力 [[89](#bib.bib89)]。
- en: 'TABLE VI: The Compact Table Highlights the Main Applications of Each Convolution
    Type'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：紧凑的表格突出显示了每种卷积类型的主要应用
- en: '| Convolution Type | Traditional 2D Convolutions | 1D Convolutions | 3D Convolutions
    | Dilated Convolutions | Grouped Convolutions |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 卷积类型 | 传统 2D 卷积 | 1D 卷积 | 3D 卷积 | 膨胀卷积 | 分组卷积 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Image Recognition | Image categorization | Time series analysis | Action
    recognition | Image segmentation | Real-time recognition |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 图像识别 | 图像分类 | 时间序列分析 | 行为识别 | 图像分割 | 实时识别 |'
- en: '| \hdashlineObject Detection | Object detection | Event detection | 3D object
    detection | Semantic segmentation | Efficient detection |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline目标检测 | 目标检测 | 事件检测 | 3D 目标检测 | 语义分割 | 高效检测 |'
- en: '| \hdashlineNLP | Sentiment analysis | Text classification | Textual entailment
    |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineNLP | 情感分析 | 文本分类 | 文本蕴含 |'
- en: '&#124; Hierarchical &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 层次结构 &#124;'
- en: '&#124; document classification &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文档分类 &#124;'
- en: '| Parameter reduction |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 参数减少 |'
- en: '| \hdashlineASPR |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineASPR |'
- en: '&#124; Voice activity &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语音活动 &#124;'
- en: '&#124; detection &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '| Speech recognition |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 |'
- en: '&#124; Environmental sound &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 环境声音 &#124;'
- en: '&#124; classification &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '|'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Robust speech &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲁棒语音 &#124;'
- en: '&#124; recognition &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 识别 &#124;'
- en: '|'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Low-latency &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低延迟 &#124;'
- en: '&#124; speech recognition &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语音识别 &#124;'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| \hdashlineMedical Image Analysis | Tumor segmentation | ECG signal processing
    |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline医学图像分析 | 肿瘤分割 | ECG信号处理 |'
- en: '&#124; Brain Tumor &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 脑肿瘤 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '|'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Enhanced image &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强图像 &#124;'
- en: '&#124; segmentation &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '| Faster medical analysis |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 更快的医学分析 |'
- en: VI-B Object Detection and Localization
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 目标检测与定位
- en: Multiple objects within an image must be located and identified during object
    detection [[90](#bib.bib90)]. In this application, both conventional 2D convolutions
    and 3D convolutions are crucial [[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180),
    [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183), [184](#bib.bib184),
    [185](#bib.bib185), [186](#bib.bib186), [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)]. While 3D convolutions
    are used for video object detection, 2D convolutions are used to process individual
    image frames. CNNs can detect objects at different scales and aspect ratios thanks
    to their region proposal mechanisms and anchor-based methods [[192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201), [202](#bib.bib202)].
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测过程中，必须定位和识别图像中的多个对象[[90](#bib.bib90)]。在这一应用中，传统的2D卷积和3D卷积都是至关重要的[[178](#bib.bib178),
    [179](#bib.bib179), [180](#bib.bib180), [181](#bib.bib181), [182](#bib.bib182),
    [183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185), [186](#bib.bib186),
    [187](#bib.bib187), [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190),
    [191](#bib.bib191)]。虽然3D卷积用于视频目标检测，但2D卷积用于处理单独的图像帧。得益于区域提议机制和基于锚点的方法，CNN能够在不同的尺度和纵横比下检测对象[[192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201), [202](#bib.bib202)]。
- en: Accurate localization of object bounding boxes is made possible by the use of
    pooling layers and convolutional sliding windows. Robotics, surveillance technology,
    and autonomous vehicles all use object detection to better understand and interact
    with their surroundings [[91](#bib.bib91)][[92](#bib.bib92)].
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用池化层和卷积滑动窗口，可以实现对对象边界框的精确定位。机器人技术、监控技术和自动驾驶车辆都使用目标检测来更好地理解和与周围环境互动[[91](#bib.bib91)][[92](#bib.bib92)]。
- en: VI-C Natural Language Processing
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 自然语言处理
- en: For sequential data, such as text processing and sentiment analysis, NLP uses
    1D convolutions. 1D convolutions are used in NLP applications to extract pertinent
    patterns and relationships from sentences, enabling models to understand semantic
    meaning and context [[212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214),
    [215](#bib.bib215), [216](#bib.bib216)]. Sentiment analysis for understanding
    customer opinions, named entity recognition to extract specific information from
    text, and text classification to classify news articles or product reviews are
    examples of NLP applications using 1D convolutions. Applications like machine
    translation and text summarization have benefited from the successful integration
    of CNNs and recurrent neural networks (RNNs).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 对于顺序数据，如文本处理和情感分析，自然语言处理（NLP）使用1D卷积。1D卷积用于NLP应用中，以提取句子中的相关模式和关系，使模型能够理解语义意义和上下文
    [[212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214), [215](#bib.bib215),
    [216](#bib.bib216)]。情感分析用于理解客户意见，命名实体识别用于从文本中提取特定信息，以及文本分类用于分类新闻文章或产品评论，都是使用1D卷积的NLP应用示例。机器翻译和文本摘要等应用已经受益于CNN和递归神经网络（RNN）的成功整合。
- en: VI-D Audio Processing and Speech Recognition
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 音频处理和语音识别
- en: Audio Processing and Speech Recognition (APSR) benefit from 1D convolutions,
    which analyze and process sequential audio data such as speech signals or audio
    waveforms [[217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219), [220](#bib.bib220),
    [221](#bib.bib221), [222](#bib.bib222), [223](#bib.bib223)]. By extracting temporal
    patterns and acoustic features, CNNs can learn to recognize spoken words and transcribe
    audio into text. SR systems, often built upon convolutional and recurrent neural
    networks, enable voice assistants like Siri and Google Assistant to understand
    and respond to user commands.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 音频处理和语音识别（APSR）受益于1D卷积，这些卷积分析和处理如语音信号或音频波形等顺序音频数据 [[217](#bib.bib217), [218](#bib.bib218),
    [219](#bib.bib219), [220](#bib.bib220), [221](#bib.bib221), [222](#bib.bib222),
    [223](#bib.bib223)]。通过提取时间模式和声学特征，CNN能够学习识别口语单词并将音频转录为文本。语音识别系统，通常基于卷积神经网络和递归神经网络，使得像Siri和Google
    Assistant这样的语音助手能够理解并回应用户命令。
- en: VI-E Medical Image Analysis
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E 医学图像分析
- en: Medical image analysis involves the examination and interpretation of medical
    images, such as MRI scans, CT scans, and X-rays [[92](#bib.bib92)][[224](#bib.bib224),
    [225](#bib.bib225), [226](#bib.bib226), [227](#bib.bib227), [228](#bib.bib228),
    [229](#bib.bib229), [230](#bib.bib230), [231](#bib.bib231), [232](#bib.bib232),
    [233](#bib.bib233), [234](#bib.bib234), [235](#bib.bib235), [236](#bib.bib236),
    [237](#bib.bib237), [238](#bib.bib238), [239](#bib.bib239), [240](#bib.bib240),
    [241](#bib.bib241)]. In this domain, 3D convolutions and dilated convolutions
    are frequently used. 3D convolutions process volumetric medical data, allowing
    CNNs to extract spatial and contextual information for applications like tumor
    segmentation, organ localization, and disease classification [[92](#bib.bib92)][[93](#bib.bib93)].
    Dilated convolutions enhance feature extraction and semantic segmentation in medical
    images, enabling precise identification of abnormal tissues and structures. The
    applications of convolution types in medical image analysis have led to significant
    advancements in healthcare, assisting doctors in diagnosis and treatment planning.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分析涉及对医学图像的检查和解释，如MRI扫描、CT扫描和X光 [[92](#bib.bib92)][[224](#bib.bib224), [225](#bib.bib225),
    [226](#bib.bib226), [227](#bib.bib227), [228](#bib.bib228), [229](#bib.bib229),
    [230](#bib.bib230), [231](#bib.bib231), [232](#bib.bib232), [233](#bib.bib233),
    [234](#bib.bib234), [235](#bib.bib235), [236](#bib.bib236), [237](#bib.bib237),
    [238](#bib.bib238), [239](#bib.bib239), [240](#bib.bib240), [241](#bib.bib241)]。在这一领域中，3D卷积和膨胀卷积被广泛使用。3D卷积处理体积医学数据，使CNN能够提取空间和上下文信息，用于肿瘤分割、器官定位和疾病分类等应用
    [[92](#bib.bib92)][[93](#bib.bib93)]。膨胀卷积增强了医学图像中的特征提取和语义分割，能够精确识别异常组织和结构。卷积类型在医学图像分析中的应用已显著推动了医疗保健的发展，帮助医生进行诊断和治疗计划。
- en: VII Future Trends in CNN
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII CNN的未来趋势
- en: CNNs continue to be a hot topic of research and have achieved remarkable success
    in various CV applications. Future trends and open research questions in the field
    of CNNs are emerging as technology develops and deep learning techniques become
    increasingly complex.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 继续成为研究的热门话题，并在各种计算机视觉应用中取得了显著的成功。随着技术的发展和深度学习技术的日益复杂，CNN领域的未来趋势和未解研究问题也在不断涌现。
- en: The investigation of more effective architectures that can achieve comparable
    performance with fewer parameters and computational resources is one future trend
    in CNN research. How to make CNNs more interpretable is another unanswered research
    question, as the reasoning behind CNN decisions is frequently difficult to comprehend
    due to the internal complexity of these systems. Another crucial area for future
    research is finding ways to strengthen CNNs and make them less vulnerable to hostile
    attacks.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 调查能够以更少的参数和计算资源实现可比性能的更有效架构是 CNN 研究中的一个未来趋势。如何使 CNN 更具可解释性是另一个未解答的研究问题，因为 CNN
    决策背后的推理由于这些系统的内部复杂性而常常难以理解。另一个未来研究的重要领域是寻找增强 CNN 并使其更不易受到恶意攻击的方法。
- en: One active area of research looks at designing efficient CNN architectures optimized
    for edge and mobile computing. As CV moves from data centers to cameras, smartphones,
    and IoT at the network’s edge, models need to operate within strict constraints
    on latency, memory, and power. Techniques including network pruning, compact operators,
    knowledge distillation, and adaptive quantization help derive lightweight CNN
    variants suitable for these low-resource scenarios [[121](#bib.bib121)]. This
    focus on efficiency ties into work on improving CNN interpretability.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 一个活跃的研究领域致力于设计高效的 CNN 架构，以优化边缘计算和移动计算。随着计算机视觉（CV）从数据中心转向摄像头、智能手机和网络边缘的物联网，模型需要在延迟、内存和功耗的严格限制下运行。包括网络剪枝、紧凑算子、知识蒸馏和自适应量化在内的技术有助于推导出适合这些低资源场景的轻量级
    CNN 变体 [[121](#bib.bib121)]。对效率的关注也与提升 CNN 可解释性的工作相关联。
- en: While today’s complex CNNs achieve top accuracy, their decision-making remains
    poorly understood. Work on saliency mapping, activation clustering, modular CNNs,
    and other explanatory methods aims to shine light into the ”black box” and address
    concerns around reliability, bias, and accountability - important considerations
    for safety-critical domains like healthcare. New types of CNN modules also aim
    to expand what these models can represent by incorporating flexible self-attention
    and capturing non-Euclidean structures.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管今天的复杂 CNN 实现了最高的准确率，但其决策过程仍然理解不足。对显著性映射、激活聚类、模块化 CNN 和其他解释性方法的研究旨在揭示“黑箱”并解决可靠性、偏见和问责等问题，这些问题在像医疗保健这样的安全关键领域尤为重要。新的
    CNN 模块类型还旨在通过引入灵活的自注意力和捕捉非欧几里得结构来扩展这些模型的表示能力。
- en: A particularly compelling avenue involves tackling large-scale vision multimodal
    (LVM) challenges, which builds upon this work on expanding CNN capabilities. Vast
    datasets merging diverse visual media with language, audio, and other inputs present
    unprecedented complexity. However, they also offer unprecedented opportunities
    to develop general, comprehensive models of multisensory scene understanding.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特别引人注目的方向是解决大规模视觉多模态（LVM）挑战，这在扩展 CNN 能力的工作基础上进行。融合多种视觉媒体与语言、音频和其他输入的大型数据集呈现出前所未有的复杂性。然而，它们也提供了前所未有的机会来发展通用、全面的多感官场景理解模型。
- en: VII-A Interpretability and Explainability of CNNs
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A CNN 的可解释性和解释性
- en: The interpretability and explainability of CNNs is a significant open research
    question. Understanding the decision-making process of these models gets harder
    as CNNs get deeper and more complex. Particularly in critical applications like
    healthcare and autonomous systems, researchers are investigating ways to interpret
    and explain CNN predictions. To increase trust and reliability in CNN-based systems,
    methods such as attention visualization, saliency maps, and attribution methods
    seek to reveal which areas of the input contribute most to the model’s conclusion.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的可解释性和解释性是一个重要的开放研究问题。随着 CNN 的深度和复杂性增加，理解这些模型的决策过程变得更加困难。特别是在像医疗保健和自动驾驶系统等关键应用中，研究人员正在探讨解释和解释
    CNN 预测的方法。为了增加对基于 CNN 系统的信任和可靠性，诸如注意力可视化、显著性图和归因方法等方法试图揭示输入的哪些区域对模型的结论贡献最大。
- en: VII-B Incorporating Domain Knowledge
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 融入领域知识
- en: Incorporating domain knowledge into CNN architectures is another important research
    direction. While CNNs have shown exceptional generalization abilities, they may
    not fully exploit domain-specific characteristics. Research focuses on developing
    architectures that can efficiently utilize domain knowledge or constraints, such
    as physics-based priors in medical imaging or geometric constraints in robotics,
    to improve performance and reduce data requirements.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 将领域知识融入CNN架构是另一个重要的研究方向。虽然CNN已表现出卓越的泛化能力，但它们可能未能充分利用特定领域的特征。研究重点是开发能够有效利用领域知识或约束的架构，如医学成像中的基于物理的先验或机器人中的几何约束，以提高性能并减少数据需求。
- en: VII-C Robustness and Adversarial Defense
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 鲁棒性与对抗防御
- en: Enhancing the robustness of CNNs against adversarial attacks remains a significant
    challenge. Adversarial attacks involve adding carefully crafted perturbations
    to inputs, leading to incorrect predictions by the CNN model. Researchers are
    investigating techniques for adversarial defense, such as adversarial training,
    robust optimization, and input transformations, to make CNNs more resilient against
    these attacks.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 提高卷积神经网络（CNN）对对抗性攻击的鲁棒性仍然是一个重大挑战。对抗性攻击涉及向输入中添加精心设计的扰动，导致CNN模型做出错误预测。研究人员正在探索对抗性防御技术，如对抗性训练、鲁棒优化和输入变换，以增强CNN对这些攻击的抵御能力。
- en: VII-D Efficient Model Design
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 高效模型设计
- en: When using CNNs on devices with limited resources, such as smartphones and edge
    devices, efficiency in terms of computation, memory, and power consumption is
    important [[242](#bib.bib242), [243](#bib.bib243)]. Creating lightweight architectures,
    knowledge distillation methods, and effective model compression techniques will
    be future trends in CNN research to decrease the model size and increase inference
    speed while maintaining accuracy.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源有限的设备上使用CNN时，如智能手机和边缘设备，计算、内存和功耗的效率非常重要[[242](#bib.bib242), [243](#bib.bib243)]。未来的CNN研究趋势将包括创建轻量级架构、知识蒸馏方法和有效的模型压缩技术，以减小模型大小并提高推理速度，同时保持准确性。
- en: 'TABLE VII: Comparison of Pruning Technique'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：剪枝技术比较
- en: '| Technique | Sparsity Type | Pruning Granularity | Hardware Friendly | Accuracy
    Impact | Compression Ratio | Iterative Training | Requires Retraining |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 稀疏类型 | 剪枝粒度 | 硬件友好 | 准确性影响 | 压缩比 | 迭代训练 | 是否需要重新训练 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Magnitude Pruning | Unstructured | Weight level | No | Medium | 2-10x | Yes
    | No |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 量纲剪枝 | 非结构化 | 权重级别 | 否 | 中等 | 2-10倍 | 是 | 否 |'
- en: '| Filter Pruning | Channel-wise | Filter level | Yes | Low | 5-10x | No | Yes
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 滤波器剪枝 | 通道级别 | 滤波器级别 | 是 | 低 | 5-10倍 | 否 | 是 |'
- en: '| Block Pruning | Block-level | Block level | Yes | Low | 2-5x | No | Yes |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 块剪枝 | 块级别 | 块级别 | 是 | 低 | 2-5倍 | 否 | 是 |'
- en: '| Network Slimming | Channel-wise | Channel level | Yes | Low | 2-5x | Yes
    | Yes |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 网络精简 | 通道级别 | 通道级别 | 是 | 低 | 2-5倍 | 是 | 是 |'
- en: '| Lottery Ticket | Unstructured | Weight level | No | Low | 2-10x | Yes | Yes
    |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 彩票剪枝 | 非结构化 | 权重级别 | 否 | 低 | 2-10倍 | 是 | 是 |'
- en: '| Iterative Magnitude | Unstructured | Weight level | No | Medium | 2-5x |
    Yes | No |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 迭代量纲 | 非结构化 | 权重级别 | 否 | 中等 | 2-5倍 | 是 | 否 |'
- en: '| Pruning-at-Init | Channel-wise | Filter level | Yes | Low | 5-10x | No |
    No |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 初始剪枝 | 通道级别 | 滤波器级别 | 是 | 低 | 5-10倍 | 否 | 否 |'
- en: '| One-Shot Pruning | Channel-wise | Filter level | Yes | Low | 5-10x | No |
    No |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 一次性剪枝 | 通道级别 | 滤波器级别 | 是 | 低 | 5-10倍 | 否 | 否 |'
- en: 'Model compression techniques play a crucial role in designing efficient deep
    learning models suitable for deployment on resource-constrained edge devices.
    Several methods (See Table [VII](#S7.T7 "TABLE VII ‣ VII-D Efficient Model Design
    ‣ VII Future Trends in CNN ‣ A Comprehensive Survey of Convolutions in Deep Learning:
    Applications, Challenges, and Future Trends")) have been proposed to reduce model
    size and computations without significantly impacting predictive performance.
    Network pruning and quantization are two widely used compression approaches[[102](#bib.bib102)][[103](#bib.bib103)].'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '模型压缩技术在设计适合在资源受限的边缘设备上部署的高效深度学习模型中发挥了关键作用。已经提出了多种方法（见表 [VII](#S7.T7 "TABLE
    VII ‣ VII-D Efficient Model Design ‣ VII Future Trends in CNN ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends")）来减少模型大小和计算量，而不会显著影响预测性能。网络剪枝和量化是两种广泛使用的压缩方法[[102](#bib.bib102)][[103](#bib.bib103)]。'
- en: Pruning techniques aim to sparsify neural networks by removing redundant connections
    with minimal impact on functionality [[121](#bib.bib121)]. Early methods relied
    on unstructured pruning where connections were simply set to zero based on their
    magnitude or importance ranking. However, such arbitrary pruning leads to non-standard
    sparse matrices thereby preventing hardware acceleration. More recent structured
    pruning techniques induce channel-wise, filter-wise, or block-wise sparsity to
    yield compact models amendable to efficient implementations [[121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124)].
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪技术旨在通过移除冗余连接来稀疏化神经网络，对功能的影响最小 [[121](#bib.bib121)]。早期方法依赖于无结构的修剪，其中连接仅基于其大小或重要性排名被置为零。然而，这种任意修剪导致了非标准的稀疏矩阵，从而阻碍了硬件加速。最近的结构化修剪技术则引入了通道级、滤波器级或块级稀疏性，以生成适合高效实现的紧凑模型
    [[121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124)]。
- en: 'TABLE VIII: Comparison of Quantization Technique'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 表VIII：量化技术比较
- en: '| Technique | Quantization Level | Bit Width | Hardware Friendly | Accuracy
    Impact | Compression Ratio | Iterative Training | Requires Calibration |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 量化级别 | 位宽 | 硬件友好 | 准确性影响 | 压缩比 | 迭代训练 | 需要校准 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Weight Quantization | Weight values | 8-bit | Yes | Low | Up to 8x | No |
    Yes |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 权重量化 | 权重值 | 8位 | 是 | 低 | 高达8倍 | 否 | 是 |'
- en: '|'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Activation &#124;'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 激活 &#124;'
- en: '&#124; Quantization &#124;'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 量化 &#124;'
- en: '| Activations | 8-bit | Yes | Low | Up to 8x | No | Yes |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 激活 | 8位 | 是 | 低 | 高达8倍 | 否 | 是 |'
- en: '| Tensor Quantization | Tensors | 4-8 bit | Yes | Low | Up to 32x | No | Yes
    |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 张量量化 | 张量 | 4-8位 | 是 | 低 | 高达32倍 | 否 | 是 |'
- en: '| Tensor Decomposition | Tensors | 4-bit | Yes | Medium | Up to 32x | No |
    No |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 张量分解 | 张量 | 4位 | 是 | 中等 | 高达32倍 | 否 | 否 |'
- en: '| Huffman Coding | Weights | Variable | No | Low | Up to 10x | No | No |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 霍夫曼编码 | 权重 | 可变 | 否 | 低 | 高达10倍 | 否 | 否 |'
- en: '| Log Quantization | Activations | 1 bit | Yes | Low | Up to 16x | No | No
    |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 对数量化 | 激活 | 1位 | 是 | 低 | 高达16倍 | 否 | 否 |'
- en: '| BNN Quantization |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| BNN量化 |'
- en: '&#124; Weights/ &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 权重/ &#124;'
- en: '&#124; Activations &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 激活 &#124;'
- en: '| 1 bit | Yes | High | Up to 32x | Yes | Yes |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 1位 | 是 | 高 | 高达32倍 | 是 | 是 |'
- en: '|'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Floating Point &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 浮点数 &#124;'
- en: '&#124; Quantization &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 量化 &#124;'
- en: '|'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Weights/ &#124;'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 权重/ &#124;'
- en: '&#124; Activations &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 激活 &#124;'
- en: '| 16-bit | Yes | Low | Up to 2x | No | No |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 16位 | 是 | 低 | 高达2倍 | 否 | 否 |'
- en: Filter pruning refers to removing entire convolutional filters, thereby achieving
    channel-wise sparsity [[116](#bib.bib116)][[123](#bib.bib123)]. It has been shown
    that up to 90% of filters can be removed from VGG16 without accuracy degradation.
    One method, termed “Pruning-at-Initialization” prunes filters with the lowest
    sum values at the start of training itself. Alternatively, “One-Shot” prunes filters
    once based on their first-order Taylor expansion. These filter-level pruning methods
    lead to uniform sparsity across layers and reduce computation by 5̃x.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器修剪指的是移除整个卷积滤波器，从而实现通道级稀疏性 [[116](#bib.bib116)][[123](#bib.bib123)]。研究表明，VGG16中的高达90%的滤波器可以被移除而不会降低准确性。一种方法，称为“初始化时修剪”，在训练开始时移除具有最低总和值的滤波器。另一种方法，“一次性修剪”，基于滤波器的一阶泰勒展开一次性修剪滤波器。这些滤波器级修剪方法导致各层之间的稀疏性均匀，并将计算减少了约5倍。
- en: Another structured approach is to prune blocks of connections rather than individual
    weights [[124](#bib.bib124)]. For example, in “Block Level Pruning”, a number
    of convolution blocks are removed from blocks 1, 2, and 3 of ResNet50, reducing
    computations without retraining. The block structure ensures layout sparsity,
    maintaining original convolution block shapes for hardware friendliness. Network
    slimming is a channel-pruning method that enforces L1-norm regularization during
    training itself to gradually remove channels with low importance scores.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种结构化方法是修剪连接块，而不是单个权重 [[124](#bib.bib124)]。例如，在“块级修剪”中，从ResNet50的块1、2和3中移除了一些卷积块，从而减少计算量而无需重新训练。块结构确保了布局稀疏性，保持了原始卷积块的形状，以便硬件友好。网络瘦身是一种通道修剪方法，它在训练过程中强制执行L1范数正则化，以逐渐移除重要性分数较低的通道。
- en: In unstructured variants, magnitude-based pruning removes weights below a threshold
    while iterative magnitude pruning alternates between weight updates and pruning
    based on a dynamic threshold [[121](#bib.bib121)][[125](#bib.bib125)]. These maintain
    sparsity throughout the architecture but induce non-zero filler weights. Lottery
    ticket hypothesis experiments have demonstrated that dense, randomly-initialized,
    sub-networks can achieve the accuracy of their original networks if trained in
    isolation.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在非结构化变体中，基于幅度的剪枝移除低于阈值的权重，而迭代幅度剪枝则在基于动态阈值的权重更新和剪枝之间交替进行 [[121](#bib.bib121)][[125](#bib.bib125)]。这些方法在整个架构中保持稀疏性，但会引入非零填充权重。彩票票据假设实验表明，密集的随机初始化子网络如果单独训练，可以达到其原始网络的准确性。
- en: 'Apart from pruning, quantization is another effective technique to compress
    models (See Table [VIII](#S7.T8 "TABLE VIII ‣ VII-D Efficient Model Design ‣ VII
    Future Trends in CNN ‣ A Comprehensive Survey of Convolutions in Deep Learning:
    Applications, Challenges, and Future Trends")). Weight and activation quantization
    methods map weights/activations to a small set of discrete values, reducing the
    number of bits required for representation [[114](#bib.bib114)][[115](#bib.bib115)].
    For example, 8-bit quantization reduces model size by 4x without accuracy loss
    for many architectures. Tensor decomposition-based quantization further compresses
    models by decomposing weight tensors into low-rank approximations.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '除了剪枝，量化是压缩模型的另一种有效技术（参见表格 [VIII](#S7.T8 "TABLE VIII ‣ VII-D Efficient Model
    Design ‣ VII Future Trends in CNN ‣ A Comprehensive Survey of Convolutions in
    Deep Learning: Applications, Challenges, and Future Trends")）。权重和激活量化方法将权重/激活映射到一组小的离散值，从而减少表示所需的比特数
    [[114](#bib.bib114)][[115](#bib.bib115)]。例如，8位量化可以将模型大小减少4倍，而不会导致许多架构的准确性损失。基于张量分解的量化通过将权重张量分解为低秩近似进一步压缩模型。'
- en: Some recent works have combined multiple compression approaches in a multi-stage
    pipeline. One example jointly employs weight quantization, pruning, and Huffman
    coding on ResNet50, achieving over 10x compression with a minor accuracy drop.
    Another uses a two-phase pipeline consisting of filtering-based pruning followed
    by quantization to design efficient MobileNet variants. Such composite methods
    achieve better accuracy-efficiency tradeoffs than individual techniques alone.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 一些近期的研究结合了多种压缩方法在多阶段管道中。例如，一个实例联合采用权重量化、剪枝和霍夫曼编码在ResNet50上实现了超过10倍的压缩，同时准确性略有下降。另一个使用了一个两阶段管道，包括基于过滤的剪枝和量化，以设计高效的MobileNet变体。这些复合方法在准确性和效率的权衡上优于单一技术。
- en: In conclusion, network pruning and quantization offer promising avenues to design
    compact models for edge and mobile applications. While early methods relied on
    unstructured sparsing, recent techniques induce structure for hardware friendliness.
    Looking ahead, continued research on model compression holds the key to facilitating
    the adoption of deep learning across myriad resource-constrained environments.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，网络剪枝和量化为设计紧凑的边缘和移动应用模型提供了有希望的途径。尽管早期的方法依赖于非结构化稀疏，最近的技术则引入了结构以适应硬件友好性。展望未来，对模型压缩的持续研究是推动深度学习在各种资源受限环境中广泛应用的关键。
- en: VII-E Multi-Task Learning and Transfer Learning
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-E 多任务学习与迁移学习
- en: CNNs are well suited for multi-task learning, in which a single model is trained
    to carry out several related applications concurrently [[162](#bib.bib162), [163](#bib.bib163),
    [164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167),
    [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171),
    [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177)]. The need for large amounts of labeled
    data for each individual task is being reduced as researchers investigate ways
    to take advantage of shared representations across applications and enhance generalization
    by transferring knowledge learned from one task to another [[147](#bib.bib147),
    [148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151),
    [152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159),
    [160](#bib.bib160), [161](#bib.bib161)].
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 很适合用于多任务学习，其中一个模型被训练以同时执行多个相关的任务[[162](#bib.bib162), [163](#bib.bib163),
    [164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167),
    [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171),
    [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177)]。随着研究人员探索利用跨应用的共享表示来减少每个任务所需的大量标记数据，并通过将一个任务中学到的知识转移到另一个任务中来增强泛化能力，这一需求正在减少[[147](#bib.bib147),
    [148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151),
    [152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159),
    [160](#bib.bib160), [161](#bib.bib161)]。
- en: VII-F Integration with Uncertainty Estimation
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-F 与不确定性估计的集成
- en: Understanding model uncertainty is essential for safety-critical applications.
    Integrating uncertainty estimation into CNNs would allow models to quantify their
    confidence in predictions and prevent costly errors, which is an area of open
    research. To improve the uncertainty measures in CNNs, researchers are investigating
    Bayesian neural networks (BNNs), dropout-based uncertainty estimation, and Bayesian
    optimization techniques.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 理解模型的不确定性对安全关键应用至关重要。将不确定性估计集成到 CNN 中将允许模型量化其预测的信心并防止代价高昂的错误，这是一个开放的研究领域。为了改进
    CNN 中的不确定性度量，研究人员正在研究贝叶斯神经网络（BNNs）、基于 dropout 的不确定性估计和贝叶斯优化技术。
- en: VII-G Generalization to Small Data Regimes
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-G 小数据集的泛化
- en: A constant problem in the CNN research area is the generalization to small data
    regimes, where labeled training data are hard to come by. Essentially using data
    from related applications or domains, techniques like transfer learning, few-shot
    learning, and meta-learning work to increase CNNs’ capacity to learn from sparse
    data.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNN 研究领域，一个常见的问题是对小数据集的泛化，其中标记的训练数据很难获得。本质上，利用来自相关应用或领域的数据，像迁移学习、少样本学习和元学习这样的技术致力于提高
    CNN 从稀疏数据中学习的能力。
- en: VII-H Evolution of Language Models and Multimodal LLMs
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-H 语言模型和多模态 LLMs 的演变
- en: In recent epochs, the domain of large language models (LLMs) for natural language
    processing has witnessed a precipitous progression. Prototypes such as BERT, GPT-3,
    and PaLM have demonstrated exceptional aptitude in language apprehension and generation,
    courtesy of self-supervised pretraining on voluminous text corpora [[85](#bib.bib85)].
    As LLMs expand in magnitude and range, incorporating additional modalities beyond
    text is a burgeoning field of study. Multimodal LLMs strive to amalgamate language,
    vision, and other sensory inputs within a singular model architecture. They hold
    the potential to attain a more holistic understanding of the world by concurrently
    learning representations across diverse data types [[96](#bib.bib96)]. A significant
    hurdle is the effective fusion of the strengths of CNNs for computer vision and
    transformer architectures for language modeling.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 最近时期，大型语言模型（LLMs）在自然语言处理领域经历了急剧的进展。诸如 BERT、GPT-3 和 PaLM 这样的原型在语言理解和生成方面表现出色，这得益于在大量文本语料库上进行的自监督预训练[[85](#bib.bib85)]。随着
    LLMs 在规模和范围上的扩展，除了文本之外，整合额外的模态是一个新兴的研究领域。多模态 LLMs 努力在一个单一的模型架构中融合语言、视觉和其他感官输入。它们有潜力通过同时学习不同数据类型的表示来获得对世界的更全面的理解[[96](#bib.bib96)]。一个重大挑战是如何有效地融合
    CNN 在计算机视觉方面的优势和变压器架构在语言建模方面的优势。
- en: One strategy involves employing a dual-stream architecture with distinct CNN
    and transformer encoders interacting via co-attentional transformer layers [[97](#bib.bib97)].
    The CNN extracts visual features from images, providing contextual information
    that can guide language generation and comprehension. The transformer architecture
    models the semantics and syntax of text. Their interaction enables the generation
    of captions based on image content or the retrieval of pertinent images for textual
    queries. Alternative methods directly incorporate CNNs within the transformer
    architecture as visual token encoders that operate with text token encoders [[98](#bib.bib98)].
    The CNN projections of image patches are appended to text token embeddings as
    inputs to the transformer layers. This unified architecture allows for end-to-end
    optimization of parameters for both vision and language tasks. Self-supervised
    pretraining continues to be vital for multimodal LLMs to learn effective joint
    representations before downstream task tuning. Contrastive learning objectives
    that predict associations between modalities have proven highly effective [[99](#bib.bib99)].
    Models pre-trained on large datasets of image-text pairs have demonstrated robust
    zero-shot transfer performance on multimodal tasks.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 一种策略是采用双流架构，使用不同的CNN和变换器编码器通过共注意力变换器层进行交互[[97](#bib.bib97)]。CNN从图像中提取视觉特征，提供可以指导语言生成和理解的上下文信息。变换器架构建模文本的语义和语法。它们的交互使得能够基于图像内容生成标题或为文本查询检索相关图像。另一种方法是将CNN直接整合进变换器架构中，作为与文本标记编码器一起工作的视觉标记编码器[[98](#bib.bib98)]。图像块的CNN投影被附加到文本标记嵌入上，作为变换器层的输入。这种统一架构允许对视觉和语言任务的参数进行端到端的优化。自监督预训练对于多模态大语言模型在下游任务调优之前学习有效的联合表示仍然至关重要。预测模态之间关联的对比学习目标已被证明非常有效[[99](#bib.bib99)]。在大规模图像-文本对数据集上预训练的模型在多模态任务中展示了强大的零样本迁移性能。
- en: As multimodal LLMs increase in scale, the efficient combination of diverse convolution
    types and attention mechanisms will be crucial. Compact CNN architectures could
    help to reduce the cost of computing. Sparse attention and memory compression
    techniques can assist with scalability.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 随着多模态大语言模型规模的扩大，高效结合不同卷积类型和注意力机制将变得至关重要。紧凑的卷积神经网络（CNN）架构可能有助于降低计算成本。稀疏注意力和内存压缩技术可以帮助提高可扩展性。
- en: VIII Performance and Efficiency Consideration
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 性能和效率考虑
- en: Considerations for performance and efficiency (See Figs. 17-20) in CNNs are
    critical in developing high-performing and resource-efficient models. Researchers
    can make informed decisions about optimizing their CNN architectures for various
    applications and deployment scenarios by analyzing computational complexity, trade-offs
    between accuracy and speed, memory requirements, and benchmarking on standard
    datasets.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中考虑性能和效率（参见图17-20）对于开发高性能和资源高效的模型至关重要。通过分析计算复杂度、准确性和速度之间的权衡、内存要求以及在标准数据集上的基准测试，研究人员可以就优化其CNN架构以适应各种应用和部署场景做出明智的决策。
- en: VIII-A Computational Complexity of Different Convolutions
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 不同卷积的计算复杂度
- en: 'The computational complexity of different convolutional techniques (See Table
    [IX](#S8.T9 "TABLE IX ‣ VIII-A Computational Complexity of Different Convolutions
    ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends")) is a critical
    aspect to consider when designing CNNs. It refers to the amount of computation
    required to perform a convolution operation on input data. The computational complexity
    is influenced by various factors, including the size of the input data, the size
    of the convolutional filters, and the number of channels in the feature maps.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '不同卷积技术的计算复杂度（参见表格 [IX](#S8.T9 "TABLE IX ‣ VIII-A Computational Complexity of
    Different Convolutions ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends")）是设计CNN时需要考虑的关键方面。它指的是对输入数据执行卷积操作所需的计算量。计算复杂度受多种因素的影响，包括输入数据的大小、卷积滤波器的大小和特征图中的通道数量。'
- en: Traditional convolutional layers, such as the standard convolution and depthwise
    separable convolution, generally have higher computational complexity compared
    to other techniques. This is because they involve a large number of convolution
    operations, especially when dealing with high-resolution images or complex data.
    On the other hand, techniques like pointwise convolution and transposed convolution
    tend to have lower computational complexity, making them more suitable for certain
    resource-constrained applications.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 传统卷积层，如标准卷积和深度可分卷积，通常具有比其他技术更高的计算复杂性。这是因为它们涉及大量的卷积操作，特别是在处理高分辨率图像或复杂数据时。另一方面，点卷积和转置卷积等技术往往具有较低的计算复杂性，使它们更适合某些资源受限的应用。
- en: Understanding the computational complexity of different convolution types is
    crucial for optimizing the performance of CNNs. By selecting convolution techniques
    that align with the available computational resources, researchers can build efficient
    models that achieve a good balance between accuracy and speed.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 理解不同卷积类型的计算复杂性对于优化CNN的性能至关重要。通过选择与可用计算资源相匹配的卷积技术，研究人员可以构建高效的模型，在准确性和速度之间达到良好的平衡。
- en: 'As illustrated in Figs. [17](#S8.F17 "Figure 17 ‣ VIII-B Trade-offs between
    Accuracy and Speed ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends") to [19](#S8.F19 "Figure 19 ‣ VIII-B Trade-offs between Accuracy and Speed
    ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive Survey of Convolutions
    in Deep Learning: Applications, Challenges, and Future Trends") the Adam optimizer
    performed well, as evidenced by key observations ① through ⑥, in both accuracy
    and loss metrics. Overall, the use of CNN techniques such as VGG, ResNet, and
    LeNet resulted in improved accuracy and reduced loss.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[17](#S8.F17 "图17 ‣ VIII-B 准确性和速度之间的权衡 ‣ VIII 性能与效率考虑 ‣ 深度学习中卷积的全面调查：应用、挑战与未来趋势")至[19](#S8.F19
    "图19 ‣ VIII-B 准确性和速度之间的权衡 ‣ VIII 性能与效率考虑 ‣ 深度学习中卷积的全面调查：应用、挑战与未来趋势")所示，Adam优化器在准确性和损失指标方面表现良好，如关键观察点①至⑥所示。总体而言，使用VGG、ResNet和LeNet等CNN技术提高了准确性并降低了损失。
- en: 'Also, as depicted in Figure [20](#S8.F20 "Figure 20 ‣ VIII-C Memory and Storage
    Requirements ‣ VIII Performance and Efficiency Consideration ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends"), and based on key observation ①,②, and ③, it is evident that the Adam
    optimizer exhibits less CPU usage in comparison to five other optimizers - RMSprop,
    Adamax, Adagrad, SGD, and Nadam. This observation holds true when using LeNet-5,
    VGG16, and ResNet-50\. Additionally, the memory usage of the Adam optimizer is
    among the lowest (See key observation ④).'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如图[20](#S8.F20 "图20 ‣ VIII-C 内存和存储需求 ‣ VIII 性能与效率考虑 ‣ 深度学习中卷积的全面调查：应用、挑战与未来趋势")所示，并基于关键观察点①、②和③，Adam优化器的CPU使用量明显低于其他五种优化器
    - RMSprop、Adamax、Adagrad、SGD和Nadam。这一观察结果在使用LeNet-5、VGG16和ResNet-50时都成立。此外，Adam优化器的内存使用量也在最低范围内（见关键观察点④）。
- en: '![Refer to caption](img/e87d17eb1851cedb0402c62ba54ecba4.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e87d17eb1851cedb0402c62ba54ecba4.png)'
- en: 'Figure 16: The trade-off curve between accuracy and speed of a deep learning
    model [[75](#bib.bib75)]'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：深度学习模型在准确性和速度之间的权衡曲线 [[75](#bib.bib75)]
- en: 'TABLE IX: Comparison on LeNet-5, VGG16, and ResNet-50 with 7 types of optimizers
    on Cifar-10 dataset, CU: CPU Utilization, MU: Memory Utilization'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 表IX：在Cifar-10数据集上，使用7种优化器对LeNet-5、VGG16和ResNet-50的比较，CU：CPU利用率，MU：内存利用率
- en: '| Optimizer Type | CNN Model | Accuracy | Loss | CU | MU |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| 优化器类型 | CNN模型 | 准确率 | 损失 | CU | MU |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | LeNet-5 | 0.547 | 1.277 | 71 | 50.7 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '|  | LeNet-5 | 0.547 | 1.277 | 71 | 50.7 |'
- en: '|  | VGG16 | 0.87 | 0.776 | 57 | 55.7 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | VGG16 | 0.87 | 0.776 | 57 | 55.7 |'
- en: '| SGD | ResNet-50 | 0.789 | 1.1212 | 63 | 53.4 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| SGD | ResNet-50 | 0.789 | 1.1212 | 63 | 53.4 |'
- en: '|  | LeNet-5 | 0.629 | 1.153 | 46.2 | 44.4 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | LeNet-5 | 0.629 | 1.153 | 46.2 | 44.4 |'
- en: '|  | VGG16 | 0.805 | 0.821 | 54.2 | 51.4 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  | VGG16 | 0.805 | 0.821 | 54.2 | 51.4 |'
- en: '| Adam | ResNet-50 | 0.760 | 1.016 | 60.5 | 51.9 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| Adam | ResNet-50 | 0.760 | 1.016 | 60.5 | 51.9 |'
- en: '|  | LeNet-5 | 0.624 | 1.22 | 58.3 | 57.6 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  | LeNet-5 | 0.624 | 1.22 | 58.3 | 57.6 |'
- en: '|  | VGG16 | 0.776 | 1.109 | 61.1 | 63.5 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  | VGG16 | 0.776 | 1.109 | 61.1 | 63.5 |'
- en: '| NAdam | ResNet-50 | 0.789 | 0.89 | 66.4 | 57.8 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| NAdam | ResNet-50 | 0.789 | 0.89 | 66.4 | 57.8 |'
- en: '|  | LeNet-5 | 0.605 | 1.288 | 50.3 | 42.9 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|  | LeNet-5 | 0.605 | 1.288 | 50.3 | 42.9 |'
- en: '|  | VGG16 | 0.755 | 22.286 | 61.2 | 49.7 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | VGG16 | 0.755 | 22.286 | 61.2 | 49.7 |'
- en: '| RSMProp | ResNet-50 | 0.78 | 1.151 | 61.7 | 49.4 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| RSMProp | ResNet-50 | 0.78 | 1.151 | 61.7 | 49.4 |'
- en: '|  | LeNet-5 | 0.603 | 1.132 | 69.8 | 56.7 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  | LeNet-5 | 0.603 | 1.132 | 69.8 | 56.7 |'
- en: '|  | VGG16 | 0.8506 | 0.885 | 55.8 | 64.2 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | VGG16 | 0.8506 | 0.885 | 55.8 | 64.2 |'
- en: '| Adamax | ResNet-50 | 0.8123 | 1.002 | 62.1 | 56.1 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| Adamax | ResNet-50 | 0.8123 | 1.002 | 62.1 | 56.1 |'
- en: '|  | LeNet-5 | 0.412 | 1.65 | 67.6 | 44.4 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | LeNet-5 | 0.412 | 1.65 | 67.6 | 44.4 |'
- en: '|  | VGG16 | 0.822 | 0.708 | 55.3 | 50.3 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  | VGG16 | 0.822 | 0.708 | 55.3 | 50.3 |'
- en: '| AdaGrad | ResNet-50 | 0.75 | 0.999 | 62.4 | 50.6 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| AdaGrad | ResNet-50 | 0.75 | 0.999 | 62.4 | 50.6 |'
- en: VIII-B Trade-offs between Accuracy and Speed
  id: totrans-504
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 准确性和速度之间的权衡
- en: One of the key challenging aspects of designing CNNs is balancing model accuracy
    and inference speed (see Fig. 16). The inference time increases as the complexity
    of convolutional layers increases to capture more complex features. Using simpler
    convolutional techniques, on the other hand, may result in lower accuracy. The
    depth and width of the network, the number of parameters, the choice of convolutional
    techniques, and the hardware on which the model is deployed all have an impact
    on the trade-offs between accuracy and speed. For real-time applications or resource-constrained
    environments, sacrificing some accuracy to achieve faster inference may be necessary.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 设计 CNN 的一个关键挑战是平衡模型的准确性和推断速度（见图16）。随着卷积层复杂特征的捕捉增加，推断时间也会增加。另一方面，使用更简单的卷积技术可能会导致准确性降低。网络的深度和宽度、参数数量、卷积技术的选择以及模型部署的硬件都会影响准确性和速度之间的权衡。对于实时应用或资源受限环境，可能需要牺牲一些准确性来获得更快的推断速度。
- en: '![Refer to caption](img/ec1997a7188290ec8bd8a5d623ad5c9e.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ec1997a7188290ec8bd8a5d623ad5c9e.png)'
- en: 'Figure 17: Comparison of various optimizers on LeNet-5 with Cifar-10 dataset.
    a) represents the accuracy of LeNet-5 architecture, b) represents loss of LeNet-5
    architecture'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：LeNet-5 在 Cifar-10 数据集上的各种优化器比较。a) 表示 LeNet-5 结构的准确性，b) 表示 LeNet-5 结构的损失
- en: 'Model pruning, quantization, and low-rank approximations are commonly used
    by researchers to reduce model size (See Section [VII](#S7 "VII Future Trends
    in CNN ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications,
    Challenges, and Future Trends") -¿ Subsection D) and improve inference speed without
    significantly compromising accuracy. Furthermore, attention-based convolutions
    and other techniques that prioritize important regions of the input can be used
    to focus computational efforts where they are most needed, improving the balance
    between accuracy and speed even further.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '模型修剪、量化和低秩近似通常被研究人员用来减小模型尺寸（见第 [VII](#S7 "VII Future Trends in CNN ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends") -¿ 子章节 D）并提高推断速度，而不显著影响准确性。此外，基于注意力的卷积和其他优先考虑输入重要区域的技术可以用来将计算工作重点放在最需要的地方，进一步提高准确性和速度的平衡。'
- en: '![Refer to caption](img/7d6e7c606b589069871faebd320f4395.png)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7d6e7c606b589069871faebd320f4395.png)'
- en: 'Figure 18: Comparison of various optimizers on VGG16 with Cifar-10 dataset.
    a) represents the accuracy of VGG16 architecture, b) represents loss of VGG16
    architecture with various range of optimizers'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：VGG16 在 Cifar-10 数据集上的各种优化器比较。a) 表示 VGG16 结构的准确性，b) 表示不同范围优化器下 VGG16 结构的损失
- en: '![Refer to caption](img/d1234d55fe1057ef41b681851fbeb6a6.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d1234d55fe1057ef41b681851fbeb6a6.png)'
- en: 'Figure 19: Comparison of various optimizers on ResNet-50 with Cifar-10 dataset.
    a) represents the accuracy of ResNet-50 architecture, b) represents loss of ResNet
    -50 architecture'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：ResNet-50 在 Cifar-10 数据集上的各种优化器比较。a) 表示 ResNet-50 结构的准确性，b) 表示 ResNet-50
    结构的损失
- en: VIII-C Memory and Storage Requirements
  id: totrans-513
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 内存和存储要求
- en: '![Refer to caption](img/9ee5916de1d599d32554281f0281f2bc.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9ee5916de1d599d32554281f0281f2bc.png)'
- en: 'Figure 20: The CPU and Memory Utilization used by each model. a) The Average
    CPU Utilization of LeNet-5, VGG16, and ResNet-50 with six types of optimizer (Better
    value Recognition depends on use-case), b) The Average Memory Utilization of LeNet-5,
    VGG16, and ResNet-50 with six types of optimizer (Better value Recognition depends
    on Usecase)'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：每个模型所使用的 CPU 和内存利用率。a) LeNet-5、VGG16 和 ResNet-50 使用六种优化器的平均 CPU 利用率（更好的价值识别取决于使用情况），b)
    LeNet-5、VGG16 和 ResNet-50 使用六种优化器的平均内存利用率（更好的价值识别取决于使用情况）
- en: Memory and storage requirements are crucial considerations in deep learning,
    especially when deploying models on edge devices or in cloud environments with
    limited resources. Convolutional models, particularly those with a large number
    of layers and parameters, can demand substantial memory and storage resources
    during training and inference.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 内存和存储需求是深度学习中的关键考虑因素，特别是在将模型部署到边缘设备或资源有限的云环境时。卷积模型，特别是那些具有大量层和参数的模型，在训练和推理过程中可能需要大量的内存和存储资源。
- en: Traditional convolutional layers often have higher memory requirements due to
    the need to store intermediate feature maps and gradients during backpropagation.
    Depthwise separable convolutions and pointwise convolutions can reduce memory
    usage by reducing the number of parameters and intermediate feature maps. Memory-efficient
    CNN design involves strategies like using smaller batch sizes, employing mixed-precision
    training, and optimizing memory usage during inference. Additionally, model compression
    techniques, such as knowledge distillation and model quantization, can significantly
    reduce the size of the model without significant loss in performance.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 传统卷积层通常具有较高的内存需求，因为在反向传播期间需要存储中间特征图和梯度。深度可分离卷积和点卷积可以通过减少参数数量和中间特征图来减少内存使用。内存高效的CNN设计涉及使用较小的批量大小、采用混合精度训练以及优化推理过程中的内存使用等策略。此外，模型压缩技术，如知识蒸馏和模型量化，可以显著减少模型的大小，而不会显著影响性能。
- en: VIII-D Benchmarking on Standard Datasets
  id: totrans-518
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-D 在标准数据集上的基准测试
- en: Benchmarking convolutional techniques on standard datasets is a crucial step
    in evaluating their performance and efficiency. Standard datasets, such as ImageNet
    [[95](#bib.bib95)] for image recognition or COCO [[94](#bib.bib94)] for object
    detection, provide a common ground for fair comparison of different models and
    techniques. By benchmarking convolutional techniques, researchers can objectively
    assess their effectiveness in various applications and compare their performance
    with state-of-the-art models. The benchmarks consider metrics like accuracy, inference
    speed, memory usage, and energy efficiency, allowing for a comprehensive evaluation
    of the models.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准数据集上对卷积技术进行基准测试是评估其性能和效率的重要步骤。标准数据集，例如用于图像识别的ImageNet [[95](#bib.bib95)]
    或用于物体检测的COCO [[94](#bib.bib94)]，提供了公平比较不同模型和技术的共同基础。通过基准测试卷积技术，研究人员可以客观地评估其在各种应用中的有效性，并将其性能与最先进的模型进行比较。基准测试考虑了准确率、推理速度、内存使用和能效等指标，从而允许对模型进行全面评估。
- en: Benchmarking helps the DL community identify the strengths and weaknesses of
    different convolutional techniques, paving the way for improvements and advancements.
    It also aids practitioners in selecting the most suitable convolutional techniques
    for their specific use cases and desired trade-offs between performance and efficiency.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试帮助深度学习社区识别不同卷积技术的优缺点，为改进和进步铺平道路。它还帮助从业者选择最适合他们特定用例和在性能与效率之间权衡的卷积技术。
- en: IX Frameworks and Libraries
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 框架和库
- en: 'This section will provide an overview of some of the popular platforms (See
    Table [X](#S9.T10 "TABLE X ‣ IX Frameworks and Libraries ‣ A Comprehensive Survey
    of Convolutions in Deep Learning: Applications, Challenges, and Future Trends"))
    available for developing deep learning applications. We will compare the frameworks
    from aspects like their architecture, programming models, supported hardware,
    and key features. Choosing the right tool is crucial for deep learning success.
    That’s why exploring framework capabilities is key for researchers and engineers'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '本节将概述一些用于开发深度学习应用的流行平台（参见表 [X](#S9.T10 "TABLE X ‣ IX Frameworks and Libraries
    ‣ A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges,
    and Future Trends")）。我们将从其架构、编程模型、支持的硬件和关键特性等方面比较这些框架。选择正确的工具对深度学习成功至关重要。这就是为什么探索框架能力对于研究人员和工程师来说是关键的原因。'
- en: 'TABLE X: Comparison of existing popular frameworks and libraries'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 表 X：现有流行框架和库的比较
- en: '| Aspect | Caffe | TensorFlow | Keras | PyTorch | OpenCV | Deeplearning4j |
    MXNet | Chainer |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | Caffe | TensorFlow | Keras | PyTorch | OpenCV | Deeplearning4j | MXNet
    | Chainer |'
- en: '| Year released | 2013 | 2015 | 2015 | 2016 | 1999 | 2014 | 2015 | 2015 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 发布年份 | 2013 | 2015 | 2015 | 2016 | 1999 | 2014 | 2015 | 2015 |'
- en: '|'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Programming &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编程 &#124;'
- en: '&#124; language &#124;'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语言 &#124;'
- en: '| C++/Python | Python, C++ | Python | Python | C++, Python, Java | Java, Scala
    |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| C++/Python | Python, C++ | Python | Python | C++, Python, Java | Java, Scala
    |'
- en: '&#124; Python, C++, R, &#124;'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Python, C++, R, &#124;'
- en: '&#124; Scala, Perl, Julia &#124;'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Scala, Perl, Julia &#124;'
- en: '| Python |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| Python |'
- en: '| License | BSD 3-Clause | Apache 2.0 | MIT | BSD 3-Clause | BSD 3-Clause |
    Apache 2.0 | Apache 2.0 | MIT |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| 许可证 | BSD 3-Clause | Apache 2.0 | MIT | BSD 3-Clause | BSD 3-Clause | Apache
    2.0 | Apache 2.0 | MIT |'
- en: '| Model definition | Layered | Graph-based |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 模型定义 | 分层 | 图形化 |'
- en: '&#124; Sequential & &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 顺序和 &#124;'
- en: '&#124; functional &#124;'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 函数式 &#124;'
- en: '|'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dynamic &#124;'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 动态 &#124;'
- en: '&#124; computations graphs &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算图 &#124;'
- en: '| N/A |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 不适用 |'
- en: '&#124; Sequential, compute &#124;'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 顺序、计算 &#124;'
- en: '&#124; graphs &#124;'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图 &#124;'
- en: '| Symbolic |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 符号式 |'
- en: '&#124; Imperative and &#124;'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 命令式和 &#124;'
- en: '&#124; declarative &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 声明式 &#124;'
- en: '|'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ease of use | Intermediate | Intermediate | High | High | Low | Intermediate
    | Intermediate | High |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 易用性 | 中等 | 中等 | 高 | 高 | 低 | 中等 | 中等 | 高 |'
- en: '| Speed | Fast | Fast | Intermediate | Fast | Very fast | Fast | Fast | Fast
    |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 速度 | 快 | 快 | 中等 | 快 | 非常快 | 快 | 快 | 快 |'
- en: '|'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Support for &#124;'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对 &#124;'
- en: '&#124; computer vision &#124;'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算机视觉 &#124;'
- en: '| Very good | Excellent | Good | Excellent | Excellent (library) | Good | Goo
    | Good |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 非常好 | 优秀 | 好 | 优秀 | 优秀（库） | 好 | 好 | 好 |'
- en: '| Focus |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| 关注 |'
- en: '&#124; Research &#124;'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究 &#124;'
- en: '&#124; prototyping &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原型设计 &#124;'
- en: '|'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Production & &#124;'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生产和 &#124;'
- en: '&#124; research &#124;'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究 &#124;'
- en: '|'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; User-friendly &#124;'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户友好 &#124;'
- en: '&#124; research &#124;'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究 &#124;'
- en: '|'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Research &#124;'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究 &#124;'
- en: '&#124; prototyping &#124;'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原型设计 &#124;'
- en: '|'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Traditional &#124;'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传统 &#124;'
- en: '&#124; algorithms &#124;'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 算法 &#124;'
- en: '|'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Enterprise &#124;'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 企业级 &#124;'
- en: '&#124; production &#124;'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生产 &#124;'
- en: '|'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Distributed training &#124;'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分布式训练 &#124;'
- en: '&#124; at scale &#124;'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大规模 &#124;'
- en: '|'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Intuitive high-level &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直观的高级 &#124;'
- en: '&#124; APIs for research &#124;'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究用 API &#124;'
- en: '|'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Distributed training | No | Yes | No | No | No | Yes | Yes | No |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| 分布式训练 | 否 | 是 | 否 | 否 | 否 | 是 | 是 | 否 |'
- en: '|'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Model &#124;'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '&#124; deployment &#124;'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 部署 &#124;'
- en: '| No | Yes | Yes | Yes | No | Yes | Yes | Limited |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 是 | 是 | 是 | 否 | 是 | 是 | 有限 |'
- en: '|'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hardware &#124;'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬件 &#124;'
- en: '&#124; support &#124;'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持 &#124;'
- en: '| CPU, GPU | CPU, GPU, TPU | CPU, GPU | CPU, GPU, TPU | CPU, GPU | CPU, GPU
    |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| CPU, GPU | CPU, GPU, TPU | CPU, GPU | CPU, GPU, TPU | CPU, GPU | CPU, GPU
    |'
- en: '&#124; CPU, GPU, &#124;'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CPU, GPU, &#124;'
- en: '&#124; TensorFlow &#124;'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow &#124;'
- en: '| CPU, GPU |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| CPU, GPU |'
- en: '|'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Documentation &#124;'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文档 &#124;'
- en: '&#124; quality &#124;'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 质量 &#124;'
- en: '| Good | Excellent | Good | Excellent | Excellent | Good | Good | Good |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| 好 | 优秀 | 好 | 优秀 | 优秀 | 好 | 好 | 好 |'
- en: '|'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Community &#124;'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 社区 &#124;'
- en: '&#124; support &#124;'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持 &#124;'
- en: '| Limited | Very active | Very active | Very active | Very active | Active
    | Active | Active |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| 有限 | 非常活跃 | 非常活跃 | 非常活跃 | 非常活跃 | 活跃 | 活跃 | 活跃 |'
- en: 'Table [X](#S9.T10 "TABLE X ‣ IX Frameworks and Libraries ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends") provides a comparison of several popular frameworks and libraries used
    in deep learning. It evaluates key aspects such as the year of release, programming
    languages supported, license type, model definition approaches, ease of use, speed,
    and focus or strength of each framework.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [X](#S9.T10 "TABLE X ‣ IX Frameworks and Libraries ‣ A Comprehensive Survey
    of Convolutions in Deep Learning: Applications, Challenges, and Future Trends")
    提供了几种流行的深度学习框架和库的比较。它评估了关键方面，如发布年份、支持的编程语言、许可证类型、模型定义方法、易用性、速度以及每个框架的重点或优势。'
- en: IX-A Caffe
  id: totrans-599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-A Caffe
- en: Caffe was one of the earliest and most influential deep learning frameworks
    developed specifically for CV tasks [[131](#bib.bib131)]. Released in 2013 by
    the Berkeley Vision and Learning Center (BVLC), Caffe made training convolutional
    neural networks much faster and more accessible. It has an easy-to-use C++/Python
    interface and was designed for speed and modularity. Caffe adopted a layered structure
    that greatly simplified model definition and training. This helped drive wider
    adoption and enabled researchers to rapidly iterate on vision models. While development
    has slowed in recent years, Caffe laid important groundwork and is still used
    for CV research.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: Caffe 是最早和最具影响力的深度学习框架之一，专门为计算机视觉任务开发 [[131](#bib.bib131)]。由伯克利视觉与学习中心（BVLC）于
    2013 年发布，Caffe 使卷积神经网络的训练变得更快、更容易。它拥有易于使用的 C++/Python 接口，并且设计注重速度和模块化。Caffe 采用了分层结构，大大简化了模型定义和训练。这有助于推动更广泛的应用，使研究人员能够快速迭代视觉模型。虽然近年来开发进展有所减缓，但
    Caffe 为计算机视觉研究奠定了重要基础，仍在使用中。
- en: IX-B TensorFlow
  id: totrans-601
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-B TensorFlow
- en: TensorFlow is an end-to-end open-source machine learning platform developed
    by Google [[132](#bib.bib132)]. While not strictly a CV library, it has become
    one of the most popular and full-featured frameworks for building and training
    complex deep learning models. TensorFlow has excellent support for CV including
    pre-trained models, image loading and preprocessing utilities, object detection
    APIs, and more. Its flexibility has led to it being used for a very wide range
    of applications from image classification to semantic segmentation. TensorFlow
    also works seamlessly across CPUs and GPUs and can be easily deployed to production.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个由谷歌开发的端到端开源机器学习平台 [[132](#bib.bib132)]。虽然它并不严格是一个计算机视觉库，但它已成为构建和训练复杂深度学习模型最流行和功能最全的框架之一。TensorFlow
    对计算机视觉提供了出色的支持，包括预训练模型、图像加载和预处理工具、目标检测 API 等。它的灵活性使其适用于从图像分类到语义分割的各种应用。TensorFlow
    还可以在 CPU 和 GPU 上无缝工作，并且可以轻松部署到生产环境中。
- en: IX-C Keras
  id: totrans-603
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-C Keras
- en: Keras is a high-level deep learning API that runs on top of popular frameworks
    like TensorFlow and CNTK [[133](#bib.bib133)]. Keras was developed with a focus
    on user-friendliness, modularity and extensibility. It provides excellent abstractions
    and tools for developing and evaluating deep learning models quickly. For CV,
    Keras ships with the ImageDataGenertator for real-time data augmentation as well
    as pre-defined models like VGG16\. It also supports popular CV tasks like image
    segmentation, object detection, and feature extraction through convenient APIs.
    Keras’ simplicity has made it very approachable for developers.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个高层次的深度学习 API，它运行在流行的框架如 TensorFlow 和 CNTK 之上 [[133](#bib.bib133)]。Keras
    的开发重点在于用户友好性、模块化和扩展性。它提供了出色的抽象和工具，能够快速开发和评估深度学习模型。在计算机视觉（CV）领域，Keras 配备了用于实时数据增强的
    ImageDataGenerator 以及像 VGG16 这样的预定义模型。它还支持流行的 CV 任务，如图像分割、目标检测和特征提取，通过便捷的 API
    实现。Keras 的简洁性使得它对开发者非常友好。
- en: IX-D PyTorch
  id: totrans-605
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-D PyTorch
- en: PyTorch is an open-source deep learning platform developed by Facebook’s AI
    Research Lab (FAIR) [[134](#bib.bib134)]. In recent years it has emerged as a
    leading alternative to TensorFlow especially for CV and NLP applications. PyTorch
    has a strong focus on dynamic neural networks and shares similarities to MATLAB
    and Numpy. This makes for an intuitive, Pythonic interface that is well-suited
    to CV prototyping and experimentation. PyTorch also supports GPU/TPU training
    along with production deployment. It has a growing ecosystem of 3rd party libraries
    and community support. Like Keras, PyTorch integrates tightly with common CV tasks
    and datasets.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个由 Facebook 的 AI 研究实验室（FAIR）开发的开源深度学习平台 [[134](#bib.bib134)]。近年来，它已成为
    TensorFlow 的主要替代品，特别是在计算机视觉和自然语言处理应用中。PyTorch 强调动态神经网络，并与 MATLAB 和 Numpy 有相似之处。这使得它具有直观的、符合
    Python 风格的接口，非常适合计算机视觉原型设计和实验。PyTorch 还支持 GPU/TPU 训练以及生产部署。它拥有一个不断增长的第三方库和社区支持生态系统。像
    Keras 一样，PyTorch 也与常见的计算机视觉任务和数据集紧密集成。
- en: IX-E OpenCV
  id: totrans-607
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-E OpenCV
- en: OpenCV (Open Source Computer Vision Library) is a popular CV and machine learning
    software library [[135](#bib.bib135)]. While not specifically designed for deep
    learning, OpenCV contains many traditional CV algorithms and an extensive collection
    of image processing functions. These include capabilities like image filtering,
    morphological operations, feature detection and extraction, object segmentation,
    and face and gesture recognition among others. OpenCV integrates with deep learning
    frameworks and is frequently used for simpler CV tasks or as a pre-processing
    step before feeding data into neural networks.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV（开源计算机视觉库）是一个流行的计算机视觉和机器学习软件库 [[135](#bib.bib135)]。尽管它并非专门为深度学习设计，但 OpenCV
    包含许多传统的计算机视觉算法和丰富的图像处理功能。这些功能包括图像滤波、形态学操作、特征检测和提取、物体分割、以及人脸和手势识别等。OpenCV 与深度学习框架集成，并常用于较简单的计算机视觉任务或作为将数据输入神经网络之前的预处理步骤。
- en: IX-F MXNet
  id: totrans-609
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-F MXNet
- en: MXNet is a flexible, efficient, and scalable deep learning framework [[136](#bib.bib136)].
    Similar to TensorFlow, it supports a wide variety of programming languages and
    hardware environments. MXNet excels at distributed training and supports training
    models containing billions of parameters across hundreds of GPUs. It also includes
    algorithms for CV like image recognition, object detection, and semantic segmentation.
    Overall, MXNet strikes a good balance between flexibility, performance, and ease
    of use making it suitable for large-scale CV problems.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 是一个灵活、高效且可扩展的深度学习框架 [[136](#bib.bib136)]。类似于 TensorFlow，它支持多种编程语言和硬件环境。MXNet
    在分布式训练方面表现突出，支持在数百个 GPU 上训练包含数十亿参数的模型。它还包括用于计算机视觉的算法，如图像识别、物体检测和语义分割。总体而言，MXNet
    在灵活性、性能和易用性之间取得了良好的平衡，使其适用于大规模的计算机视觉问题。
- en: IX-G Chainer
  id: totrans-611
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-G Chainer
- en: Chainer is an open-source deep learning framework created by preferred networks
    in Japan [[137](#bib.bib137)]. It provides straightforward neural network abstraction
    similar to Keras with imperative and declarative model definitions. Chainer focuses
    on intuitive high-level APIs combined with low-level performance. It includes
    CV functionality like image loading, augmentation, pre-trained models, and model
    export. Chainer supports GPU and multi-GPU training and deployment. Overall it
    provides a performant and productive environment for CV development.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: Chainer 是由日本的 preferred networks 创建的开源深度学习框架 [[137](#bib.bib137)]。它提供了类似于 Keras
    的直接神经网络抽象，具有命令式和声明式模型定义。Chainer 专注于直观的高层 API 与底层性能相结合。它包括图像加载、增强、预训练模型和模型导出等计算机视觉功能。Chainer
    支持 GPU 和多 GPU 训练与部署。总体而言，它为计算机视觉开发提供了一个高效且富有成效的环境。
- en: IX-H Deeplearning4j
  id: totrans-613
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-H Deeplearning4j
- en: Deeplearning4j (Dl4j) was launched in 2014 as an open-source deep learning library
    for Java and Scala on the JVM [[138](#bib.bib138)]. It enables large-scale distributed
    training on GPUs and CPUs. For CV tasks, Deeplearning4j offers tools like image
    loading, pre-trained models, model import from Keras and ONNX, and the samediff
    for dynamic model construction. Deeplearning4j focuses on production-ready deployment
    with capabilities like model serving, online prediction, and on-device inference
    via Android or iOS apps.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: Deeplearning4j（Dl4j）于 2014 年推出，是一个用于 Java 和 Scala 的开源深度学习库 [[138](#bib.bib138)]。它支持在
    GPU 和 CPU 上进行大规模分布式训练。对于计算机视觉任务，Deeplearning4j 提供了图像加载、预训练模型、从 Keras 和 ONNX 导入模型以及用于动态模型构建的
    samediff 等工具。Deeplearning4j 专注于生产就绪的部署，具有模型服务、在线预测和通过 Android 或 iOS 应用进行设备端推理等功能。
- en: Overall, these libraries and frameworks represent the forefront of open-source
    tools transforming CV through deep learning. Each offers different strengths and
    tradeoffs between flexibility, performance, ease of use, and supported features.
    As CV tasks continue advancing, we can expect these projects to further incorporate
    state-of-the-art research while also lowering the barrier to development through
    improved tools and abstractions. CV is sure to remain a major application domain
    for deep learning innovation in both research and industry.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些库和框架代表了通过深度学习转变计算机视觉的开源工具的前沿。每个工具在灵活性、性能、易用性和支持特性之间提供了不同的优势和权衡。随着计算机视觉任务的不断发展，我们可以期待这些项目进一步融入最先进的研究，同时通过改进的工具和抽象降低开发门槛。计算机视觉必将继续成为深度学习创新的重要应用领域，无论在研究还是行业中。
- en: X Main Research Fields
  id: totrans-616
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X 主要研究领域
- en: X-A Image Classification
  id: totrans-617
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-A 图像分类
- en: Image classification was one of the earliest successes of CNNs. The seminal
    AlexNet achieved record-breaking results on the ImageNet challenge in 2012 by
    drastically improving upon prior techniques. Today, state-of-the-art CNNs for
    image classification routinely achieve human-level or better accuracy on standardized
    datasets. Architectures like ResNet, Inception, Xception, and EfficientNets optimize
    parameters, layer connectivity, and computation to classify thousands of object
    categories at superhuman performance levels [[52](#bib.bib52), [53](#bib.bib53),
    [56](#bib.bib56), [276](#bib.bib276)]. Beyond static images, video classification
    CNNs also extract spatial-temporal features to recognize complex activities and
    events.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是 CNNs 最早的成功之一。开创性的 AlexNet 在 2012 年的 ImageNet 挑战中通过大幅改进之前的技术，达到了破纪录的结果。如今，最先进的
    CNNs 在图像分类中通常能在标准数据集上实现与人类水平相当或更高的准确性。像 ResNet、Inception、Xception 和 EfficientNets
    这样的架构优化了参数、层连接和计算，以超人类的性能水平分类成千上万的物体类别 [[52](#bib.bib52), [53](#bib.bib53), [56](#bib.bib56),
    [276](#bib.bib276)]。除了静态图像，视频分类 CNNs 还提取时空特征以识别复杂的活动和事件。
- en: X-B Object Detection
  id: totrans-619
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-B 目标检测
- en: Object detection is another major CV application that relies heavily on convolutional
    modeling. Two-stage detectors like Faster R-CNN and one-stage detectors like YOLO
    leverage region proposal networks and anchor boxes trained via priors to simultaneously
    localize and classify objects within images [[317](#bib.bib317), [318](#bib.bib318),
    [319](#bib.bib319), [320](#bib.bib320), [321](#bib.bib321), [322](#bib.bib322),
    [323](#bib.bib323), [324](#bib.bib324), [325](#bib.bib325), [326](#bib.bib326),
    [327](#bib.bib327), [328](#bib.bib328), [329](#bib.bib329), [330](#bib.bib330),
    [331](#bib.bib331)]. Recent works further optimize speed and accuracy, enabling
    real-time object detection on billions of parameter models. Techniques like mobile
    object detection address embedded constraints by designing lightweight CNN backbones
    and feature extractors optimized for on-device inference [[332](#bib.bib332)].
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是另一个主要的 CV 应用，严重依赖于卷积建模。像 Faster R-CNN 这样的两阶段检测器和像 YOLO 这样的单阶段检测器利用区域提议网络和通过先验训练的锚框来同时定位和分类图像中的物体
    [[317](#bib.bib317), [318](#bib.bib318), [319](#bib.bib319), [320](#bib.bib320),
    [321](#bib.bib321), [322](#bib.bib322), [323](#bib.bib323), [324](#bib.bib324),
    [325](#bib.bib325), [326](#bib.bib326), [327](#bib.bib327), [328](#bib.bib328),
    [329](#bib.bib329), [330](#bib.bib330), [331](#bib.bib331)]。最近的研究进一步优化了速度和准确性，使实时目标检测成为可能，适用于亿级参数的模型。像移动目标检测这样的技术通过设计轻量级的
    CNN 背骨和优化设备上的推理功能来解决嵌入式限制 [[332](#bib.bib332)]。
- en: X-C Image Segmentation
  id: totrans-621
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-C 图像分割
- en: Semantic segmentation tasks require dense pixel-level labeling of image content.
    FCN and U-Net CNNs employ skip connections and encoder-decoder mirrors to preserve
    spatial information across resolutions [[333](#bib.bib333), [334](#bib.bib334),
    [335](#bib.bib335), [336](#bib.bib336), [337](#bib.bib337), [338](#bib.bib338),
    [339](#bib.bib339), [340](#bib.bib340), [341](#bib.bib341), [342](#bib.bib342),
    [343](#bib.bib343), [344](#bib.bib344), [345](#bib.bib345), [346](#bib.bib346),
    [347](#bib.bib347), [348](#bib.bib348)]. PSPNet and DeepLab introduce pyramid
    spatial pooling modules to capture multi-scale contextual cues [[349](#bib.bib349)].
    GANs and conditional random fields further refine coarse segmentations from CNNs.
    Advances in medical imaging also apply segmentation CNNs to understand organ structures,
    localize pathologies, and aid diagnosis.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割任务需要对图像内容进行密集的像素级标记。FCN 和 U-Net CNN 采用跳跃连接和编码器-解码器镜像来保持跨分辨率的空间信息 [[333](#bib.bib333),
    [334](#bib.bib334), [335](#bib.bib335), [336](#bib.bib336), [337](#bib.bib337),
    [338](#bib.bib338), [339](#bib.bib339), [340](#bib.bib340), [341](#bib.bib341),
    [342](#bib.bib342), [343](#bib.bib343), [344](#bib.bib344), [345](#bib.bib345),
    [346](#bib.bib346), [347](#bib.bib347), [348](#bib.bib348)]。PSPNet 和 DeepLab 引入了金字塔空间池化模块以捕捉多尺度的上下文信息
    [[349](#bib.bib349)]。GANs 和条件随机场进一步精细化来自 CNNs 的粗略分割。医学成像的进展也应用分割 CNNs 来理解器官结构、定位病变，并辅助诊断。
- en: X-D Vision transformers
  id: totrans-623
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-D 视觉变换器
- en: Vision transformers have also emerged as a compelling alternative to traditional
    CNNs for CV tasks. Inspired by the success of language models, vision transformers
    divide images into discrete patches which are embedded and processed with self-attention.
    This allows them to capture long-range dependencies and multi-scale contextual
    information more effectively than CNNs. Models like ViT, DeiT, and Visual BERT
    demonstrate state-of-the-art results in tasks like image classification when pre-trained
    on large datasets [[350](#bib.bib350), [351](#bib.bib351), [352](#bib.bib352),
    [353](#bib.bib353), [354](#bib.bib354), [355](#bib.bib355), [356](#bib.bib356),
    [357](#bib.bib357)]. Research now focuses on optimizing transformer efficiency
    for real-time CV applications.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器也成为传统 CNNs 在计算机视觉任务中的一种引人注目的替代方案。受到语言模型成功的启发，视觉变换器将图像划分为离散的补丁，这些补丁通过自注意力机制进行嵌入和处理。这使得它们能够比
    CNNs 更有效地捕捉长距离依赖和多尺度上下文信息。像 ViT、DeiT 和 Visual BERT 等模型在经过大规模数据集预训练后，在图像分类等任务中展示了最先进的结果
    [[350](#bib.bib350), [351](#bib.bib351), [352](#bib.bib352), [353](#bib.bib353),
    [354](#bib.bib354), [355](#bib.bib355), [356](#bib.bib356), [357](#bib.bib357)]。目前的研究集中在优化变换器在实时计算机视觉应用中的效率。
- en: X-E One-shot/few-shot/Zero-shot learning
  id: totrans-625
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-E 一次性/少样本/零样本学习
- en: One-shot and few-shot learning aim to address challenges posed by limited labeled
    training examples. Through metric learning and prototypical networks that learn
    robust representations from extensive base classes, models can effectively recognize
    new concepts from just one or a handful of examples without catastrophic forgetting
    [[358](#bib.bib358), [359](#bib.bib359), [360](#bib.bib360), [361](#bib.bib361),
    [362](#bib.bib362), [363](#bib.bib363), [364](#bib.bib364), [365](#bib.bib365),
    [366](#bib.bib366), [367](#bib.bib367), [368](#bib.bib368), [369](#bib.bib369),
    [370](#bib.bib370), [371](#bib.bib371), [372](#bib.bib372)]. This opens up CV
    to new long-tailed and incremental learning paradigms. Matching networks and prototypical
    networks efficiently compare test samples to prototype representations of base
    classes to generalize from limited exposures.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性学习和少样本学习旨在解决标注训练示例有限带来的挑战。通过度量学习和原型网络，这些网络从广泛的基础类中学习稳健的表示，模型能够有效地从仅有的一个或几个示例中识别新概念而不会发生灾难性遗忘
    [[358](#bib.bib358), [359](#bib.bib359), [360](#bib.bib360), [361](#bib.bib361),
    [362](#bib.bib362), [363](#bib.bib363), [364](#bib.bib364), [365](#bib.bib365),
    [366](#bib.bib366), [367](#bib.bib367), [368](#bib.bib368), [369](#bib.bib369),
    [370](#bib.bib370), [371](#bib.bib371), [372](#bib.bib372)]。这为计算机视觉打开了新的长尾和增量学习范式。匹配网络和原型网络通过有效比较测试样本与基础类的原型表示，从有限的暴露中进行泛化。
- en: Zero-shot learning emerges as a promising area where CNNs imagine possibilities
    beyond the limitations of labeled data [[373](#bib.bib373), [374](#bib.bib374),
    [375](#bib.bib375), [376](#bib.bib376), [377](#bib.bib377)]. Descriptors like
    attributes or semantic relationships introduce inductive biases facilitating generalization
    without example. SAE, DeViSE, and contemporary models transfer knowledge by aligning
    embeddings between seen and unseen categories connected through auxiliary descriptors.
    Knowledge graphs also provide structural inductive biases through entity and relation
    modeling.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习作为一个有前景的领域出现，其中 CNNs 想象超越标注数据限制的可能性 [[373](#bib.bib373), [374](#bib.bib374),
    [375](#bib.bib375), [376](#bib.bib376), [377](#bib.bib377)]。像属性或语义关系这样的描述符引入了促进在没有示例情况下泛化的归纳偏差。SAE、DeViSE
    和当代模型通过对齐在辅助描述符下的已见和未见类别之间的嵌入来转移知识。知识图谱还通过实体和关系建模提供结构性归纳偏差。
- en: X-F Weakly-supervised learning
  id: totrans-628
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-F 弱监督学习
- en: Weakly supervised learning techniques also help alleviate dependence on labor-intensive
    annotations [[378](#bib.bib378), [379](#bib.bib379), [380](#bib.bib380), [381](#bib.bib381),
    [382](#bib.bib382), [383](#bib.bib383)]. Models can be trained end-to-end from
    weaker input signals like image-level tags or bounding box object locations instead
    of explicit pixel-level segmentation maps. Multi-instance learning approaches
    cluster image regions corresponding to each label to iteratively refine local
    predictions. Expectation-maximization (EM) and multiple instance learning jointly
    infer labels and recognize discriminative regions, enabling training from cheaper
    forms of weak supervision.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督学习技术还帮助减少对劳动密集型标注的依赖 [[378](#bib.bib378), [379](#bib.bib379), [380](#bib.bib380),
    [381](#bib.bib381), [382](#bib.bib382), [383](#bib.bib383)]。模型可以通过图像级标签或边界框对象位置等较弱的输入信号进行端到端训练，而不是显式的像素级分割图。多实例学习方法将图像区域与每个标签对应进行聚类，以迭代精炼局部预测。期望最大化（EM）和多实例学习共同推断标签并识别区分区域，从而使从更便宜的弱监督形式中进行训练成为可能。
- en: X-G Self-supervised/unsupervised learning
  id: totrans-630
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-G 自监督/无监督学习
- en: Self-supervised learning has also gained vast attention in CV by enabling pre-training
    from sheer ubiquity of unlabeled visual data [[384](#bib.bib384), [385](#bib.bib385),
    [386](#bib.bib386), [387](#bib.bib387), [388](#bib.bib388), [389](#bib.bib389),
    [390](#bib.bib390), [391](#bib.bib391), [392](#bib.bib392), [393](#bib.bib393)].
    Pretext tasks like predicting image rotations, solving jigsaw puzzles, or counting
    pixel colors allow models to learn rich visual representations applicable to downstream
    tasks. Recent contrastive self-supervised models like SimCLR, SwAV, and MoCo demonstrate
    that unlabeled pre-training rivals or exceeds supervised pre-training in various
    vision benchmarks, enabling more data-efficient fine-tuning or transfer to new
    problems.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习在计算机视觉中也受到了极大关注，通过利用大量未标记的视觉数据进行预训练[[384](#bib.bib384), [385](#bib.bib385),
    [386](#bib.bib386), [387](#bib.bib387), [388](#bib.bib388), [389](#bib.bib389),
    [390](#bib.bib390), [391](#bib.bib391), [392](#bib.bib392), [393](#bib.bib393)]。像预测图像旋转、解决拼图谜题或计数像素颜色这样的预置任务使模型能够学习适用于下游任务的丰富视觉表示。最近的对比自监督模型，如SimCLR、SwAV和MoCo表明，无标记的预训练在各种视觉基准测试中能够与或超过监督预训练，从而实现更具数据效率的微调或转移到新问题。
- en: X-H Lifelong/Continual learning
  id: totrans-632
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-H 终身/持续学习
- en: Lifelong and continual learning aim to simulate open-world scenarios where models
    learn lifelong with non-stationary data distributions [[51](#bib.bib51)]. Models
    must avoid catastrophic forgetting when presented with new classes or shifts in
    existing class definitions without revisiting historical data [[394](#bib.bib394),
    [395](#bib.bib395), [396](#bib.bib396), [397](#bib.bib397), [398](#bib.bib398),
    [399](#bib.bib399), [400](#bib.bib400), [401](#bib.bib401), [402](#bib.bib402),
    [403](#bib.bib403)]. Elastic weight consolidation and incremental moment matching
    regularization preserve knowledge while accommodating new tasks. Research now
    explores task-aware architectures, dual-memory systems, and replay buffers that
    emulate memory reconsolidation to model lifelong visual learning.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 终身学习和持续学习旨在模拟开放世界场景，在这些场景中模型学习终身，面对非静态数据分布[[51](#bib.bib51)]。模型在面对新的类别或现有类别定义的变化时必须避免灾难性遗忘，而无需再次访问历史数据[[394](#bib.bib394),
    [395](#bib.bib395), [396](#bib.bib396), [397](#bib.bib397), [398](#bib.bib398),
    [399](#bib.bib399), [400](#bib.bib400), [401](#bib.bib401), [402](#bib.bib402),
    [403](#bib.bib403)]。弹性权重巩固和增量时刻匹配正则化在容纳新任务的同时保持知识。目前的研究探索了面向任务的架构、双存储系统以及模拟记忆重构的重播缓冲区，从而对终身视觉学习进行建模。
- en: X-I Vision language model
  id: totrans-634
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-I 视觉语言模型
- en: Vision-language models (VLMs) have also emerged at the intersection of NLP and
    CV by grounding language in visual contexts. Models fuse multimodal inputs through
    attention and generate captions conditioned on images, or localize and describe
    visual entities based on linguistic context. Large pre-trained models such as
    CLIP, ALIGN, and Oscar demonstrate exciting capabilities like zero-shot classification,
    question-answering (QA), and visual dialog with potential applications in education,
    assistive technologies, and more.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语言模型（VLMs）也出现在自然语言处理和计算机视觉的交叉领域，通过将语言基于视觉背景进行落地。模型通过注意力融合多模态输入，并生成依赖于图像的标题，或根据语言背景定位和描述视觉实体。像CLIP、ALIGN和Oscar这样的大型预训练模型展示了令人兴奋的能力，如零样本分类、问答和视觉对话，在教育、辅助技术等领域具有潜在应用。
- en: X-J Medical image analysis
  id: totrans-636
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-J 医学图像分析
- en: Medical imaging epitomizes the necessity of collaboration between deep learning
    and domain experts. Segmenting organs in volumetric scans, localizing anomalies
    across imaging modalities, and tracking patients longitudinally all leverage 3D/2D
    CNNs [[404](#bib.bib404), [405](#bib.bib405), [406](#bib.bib406), [407](#bib.bib407),
    [408](#bib.bib408), [409](#bib.bib409), [410](#bib.bib410), [411](#bib.bib411),
    [412](#bib.bib412), [413](#bib.bib413), [414](#bib.bib414), [415](#bib.bib415),
    [416](#bib.bib416), [417](#bib.bib417), [418](#bib.bib418)]. Advanced models exploit
    anatomical priors by enforcing smoothness, and preservation of edges and surfaces
    in predictions. Self-supervision further enables pre-training from non-private
    data before fine-tuning target tasks. Model interpretation especially matters
    here to ensure trust among clinicians [[410](#bib.bib410), [411](#bib.bib411),
    [412](#bib.bib412), [413](#bib.bib413)]. Beyond diagnosis, CNNs can also simulate
    novel views to aid surgical planning. Efficiency additionally matters for on-device
    deployment and assisting underserved populations lacking infrastructure.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 医学成像体现了深度学习与领域专家之间协作的必要性。在体积扫描中分割器官、在成像模式中定位异常以及对患者进行纵向追踪，所有这些都利用了 3D/2D CNN[[404](#bib.bib404),
    [405](#bib.bib405), [406](#bib.bib406), [407](#bib.bib407), [408](#bib.bib408),
    [409](#bib.bib409), [410](#bib.bib410), [411](#bib.bib411), [412](#bib.bib412),
    [413](#bib.bib413), [414](#bib.bib414), [415](#bib.bib415), [416](#bib.bib416),
    [417](#bib.bib417), [418](#bib.bib418)]。先进的模型通过强制平滑以及在预测中保留边缘和表面，利用解剖学先验。自监督进一步使得从非私人数据中进行预训练成为可能，然后微调目标任务。在这里，模型解释尤其重要，以确保临床医生的信任[[410](#bib.bib410),
    [411](#bib.bib411), [412](#bib.bib412), [413](#bib.bib413)]。除了诊断，CNN 还可以模拟新视角以辅助外科手术规划。效率对于设备上的部署以及辅助缺乏基础设施的服务不足人群也非常重要。
- en: X-K Video understanding
  id: totrans-638
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-K 视频理解
- en: Beyond images, video understanding presents unique challenges in modeling spatial-temporal
    relationships across consecutive frames. C3D and I3D CNNs introduce 3D convolutions
    directly learning from video volumes. Advanced techniques in video captioning
    and action recognition fuse language models and attention to jointly reason about
    visual content and linguistic semantics over time. Self-supervised learning from
    large unlabeled video repositories also emerges as a promising pretraining paradigm
    before fine-tuning downstream tasks.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像之外，视频理解在建模连续帧间的时空关系方面面临独特的挑战。C3D 和 I3D CNN 直接从视频体积中学习 3D 卷积。视频字幕生成和动作识别中的先进技术融合了语言模型和注意力机制，以共同推理视觉内容和语言语义。自监督学习从大规模未标记的视频库中也成为了在微调下游任务之前的一个有前景的预训练范式。
- en: X-L Multi-task learning
  id: totrans-640
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-L 多任务学习
- en: Multi-task learning aims to improve generalization by jointly training CNNs
    on multiple related tasks using shared representations. This has proven successful
    across numerous applications by leveraging commonalities while mitigating overfitting
    individual tasks’ limited data [[419](#bib.bib419), [420](#bib.bib420), [421](#bib.bib421),
    [422](#bib.bib422)]. For example, YOLO trains object detection alongside other
    auxiliary predictions like segmentation and counting.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习旨在通过在多个相关任务上共同训练 CNN 来提高泛化能力，使用共享表示。这在众多应用中已经证明了成功，通过利用共性同时减轻了单个任务有限数据的过拟合[[419](#bib.bib419),
    [420](#bib.bib420), [421](#bib.bib421), [422](#bib.bib422)]。例如，YOLO 在进行物体检测的同时，也训练其他辅助预测如分割和计数。
- en: 'Multi-task CNNs outperform independent models in low-data regimes (See Section
    [VII](#S7 "VII Future Trends in CNN ‣ A Comprehensive Survey of Convolutions in
    Deep Learning: Applications, Challenges, and Future Trends") -¿ Sub-section G.)
    by borrowing statistical strength across related problems. Dense captioning localizes
    objects and describes scenes simultaneously. A single network predicts keypoints,
    normals, and semantic part segmentation. Deeper tasks benefit substantially from
    representations learned for more general shallow tasks.'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '多任务 CNN 在低数据条件下优于独立模型（见第 [VII](#S7 "VII Future Trends in CNN ‣ A Comprehensive
    Survey of Convolutions in Deep Learning: Applications, Challenges, and Future
    Trends") -¿ 子章节 G。）通过借用相关问题的统计力量。密集字幕生成同时定位物体并描述场景。一个单独的网络预测关键点、法线和语义部分分割。更深层的任务从为更通用的浅层任务学习的表示中受益匪浅。'
- en: Progressively growing into new problem spaces via related auxiliary objectives
    also prevents catastrophic forgetting. Self-supervised pre-training establishes
    features broadly useful across downstream tasks, including those without annotations.
    Measuring and maximizing modularity in multi-task architectures additionally reduces
    interference between domains.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 通过相关的辅助目标逐步扩展到新的问题空间也防止了灾难性遗忘。自监督预训练建立了在下游任务中广泛有用的特征，包括那些没有标注的任务。测量和最大化多任务架构中的模块化还可以减少领域间的干扰。
- en: Techniques like multi-granularity, multi-level, and heterogeneous multi-task
    learning further craft diverse objectives to progressively refine semantics captured
    at differing levels of granularity [[423](#bib.bib423), [424](#bib.bib424), [425](#bib.bib425),
    [426](#bib.bib426)]. Task relations range from independent, and cooperative where
    tasks improve each other, to completely shared exploiting identical representations.
    Properly designed, multi-task CNNs deliver state-of-the-art performance while
    improving generalizability, efficiency, and real-world applicability.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 技术如多粒度、多层次和异构多任务学习进一步设计多样的目标，以逐步细化在不同粒度级别捕获的语义[[423](#bib.bib423), [424](#bib.bib424),
    [425](#bib.bib425), [426](#bib.bib426)]。任务关系从独立、互相改善的合作到完全共享利用相同表示。设计得当，多任务CNN能够提供最先进的性能，同时提高泛化能力、效率和现实世界的适用性。
- en: Multi-task models combine CNNs with other modalities like language. For captioning,
    CNN-RNN fusion grounds generated text within visual contexts. For retrieval, ranking
    loss trains CNN-LSTM encoders to map semantically aligned vision-text pairs to
    nearby embeddings. Multi-modal pre-training on enormous unlabeled multimedia collections
    has proven highly beneficial via self-supervised alignment of domains.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务模型将CNN与语言等其他模态结合。对于图像描述，CNN-RNN融合将生成的文本与视觉上下文对接。对于检索，排名损失训练CNN-LSTM编码器将语义对齐的视觉-文本对映射到相近的嵌入。多模态的自监督预训练在巨大的未标记多媒体集合中证明了其通过自监督对齐领域的高度效用。
- en: X-M 6D vision
  id: totrans-646
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-M 6D视觉
- en: 6D vision aims to recover the full 6D pose (3D position, 3D orientation) of
    objects directly from monocular RGB images. This is a challenging problem due
    to the loss of depth information when projecting 3D scenes onto 2D images [[427](#bib.bib427),
    [428](#bib.bib428), [429](#bib.bib429), [430](#bib.bib430), [431](#bib.bib431),
    [432](#bib.bib432)]. Early works relied on CAD models and rendered synthetic data
    which lacked photorealism, while more recent approaches leverage large amounts
    of real training data.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 6D视觉旨在直接从单目RGB图像中恢复完整的6D姿态（3D位置、3D方向）。由于将3D场景投影到2D图像时丢失了深度信息，这是一项具有挑战性的任务[[427](#bib.bib427),
    [428](#bib.bib428), [429](#bib.bib429), [430](#bib.bib430), [431](#bib.bib431),
    [432](#bib.bib432)]。早期的工作依赖于CAD模型和缺乏照片现实主义的合成数据，而较新的方法则利用大量真实训练数据。
- en: CNN-based regression networks are commonly used which take images as input and
    directly predict the 6D pose values. PoseCNN showed this can achieve competitive
    accuracy to model-based regression if trained end-to-end on real data. Due to
    the complex, multi-modal nature of the target distribution, losses that ensure
    consistent predictions under different poses like reprojection or angular are
    beneficial.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: CNN基础的回归网络通常用于将图像作为输入，并直接预测6D姿态值。PoseCNN展示了如果在真实数据上端到端训练，这可以实现与基于模型的回归相媲美的精度。由于目标分布的复杂性和多模态性质，确保在不同姿态下预测一致的损失（如重投影或角度）是有益的。
- en: Iterative refinement approaches first detect the object, then iteratively update
    the pose estimate based on 2D-3D correspondences. DeepIM predicts shape coefficients
    and refines using PnP. DPOD leverages deep features combined with geometric constraints
    in a RANSAC framework. Dense representations also help by reasoning about object
    parts independently.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代细化方法首先检测对象，然后基于2D-3D对应关系迭代更新姿态估计。DeepIM预测形状系数并使用PnP进行细化。DPOD结合深度特征和几何约束，在RANSAC框架中进行操作。密集表示也通过独立推理对象部分提供帮助。
- en: Multi-view and RGB-D sensors provide additional cues to leverage. MVD helps
    constrain the problem by training separate networks for each view and fusing results.
    Using both RGB and depth as input allows Depth-PoseNet to lift 2D predictions
    to 3D space. Multitask models predicting bounding boxes, keypoints, and poses
    jointly demonstrate accuracies approaching marker-based motion capture.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角和RGB-D传感器提供了额外的线索来利用。MVD通过为每个视角训练独立的网络并融合结果来帮助约束问题。使用RGB和深度作为输入，Depth-PoseNet可以将2D预测提升到3D空间。多任务模型联合预测边界框、关键点和姿态，准确度接近基于标记的动作捕捉。
- en: X-N Neural Architecture Search
  id: totrans-651
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-N 神经架构搜索
- en: Neural architecture search (NAS) aims to automate the design of neural networks
    leveraging the power of evolution and reinforcement learning. Rather than relying
    on human experts to laboriously craft CNN architectures, NAS approaches evolve
    architectures directly on target datasets and tasks. This has led to state-of-the-art
    vision models developed without human design choices [[433](#bib.bib433), [434](#bib.bib434),
    [435](#bib.bib435), [436](#bib.bib436), [437](#bib.bib437), [438](#bib.bib438),
    [439](#bib.bib439), [440](#bib.bib440)].
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索（NAS）旨在利用进化和强化学习的力量自动设计神经网络。NAS方法直接在目标数据集和任务上演变架构，而不是依赖人工专家费力地设计CNN架构。这导致了在没有人工设计选择的情况下开发出的最先进的视觉模型[[433](#bib.bib433),
    [434](#bib.bib434), [435](#bib.bib435), [436](#bib.bib436), [437](#bib.bib437),
    [438](#bib.bib438), [439](#bib.bib439), [440](#bib.bib440)]。
- en: Early NAS works explored various search spaces defined by units, operations,
    and connections between them. Combining concepts like pruning, sharing weights
    across child models during evolution helped scaling search to larger spaces [[295](#bib.bib295),
    [299](#bib.bib299)]. Performance predictors further reduced costs by guiding search
    towards promising regions. Novel methods evolved filters, activation functions,
    and batch normalization layers for particular domains.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的NAS工作探讨了由单元、操作和它们之间的连接定义的各种搜索空间。结合像剪枝、在演化过程中在子模型之间共享权重等概念，帮助将搜索扩展到更大的空间[[295](#bib.bib295),
    [299](#bib.bib299)]。性能预测器通过引导搜索到有前景的区域进一步减少了成本。新方法为特定领域演变了过滤器、激活函数和批归一化层。
- en: Recent efforts evolve entire sections or blocks, expanding applicable search
    spaces. Single-path one-shot approaches drastically sped up search without compromising
    quality. ProxylessNAS found efficient mobile architectures directly on target
    devices. NAS approaches also discover non-CNN models suiting problems beyond CV.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的努力演变了整个部分或块，扩展了适用的搜索空间。单路径一体化方法极大地加快了搜索速度而不妥协质量。ProxylessNAS直接在目标设备上找到高效的移动架构。NAS方法还发现了适用于超越计算机视觉问题的非CNN模型。
- en: Once identified, the best architectures can be trained from scratch to further
    improve upon proxy accuracies predicted during the search. Late phase evolution
    also enhances architectures initially identified, while architecture parameters
    themselves may evolve. Overall, NAS technologies continuously push forward state-of-the-art
    for vision tasks given diverse data, constraints, or objectives.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了最佳架构，可以从头开始训练这些架构，以进一步提高搜索过程中预测的代理准确率。晚期演化还增强了最初确定的架构，同时架构参数本身也可能会演变。总体而言，NAS技术在面对多样的数据、约束或目标时，不断推动视觉任务的最新技术。
- en: X-O Neural Architecture Transformer
  id: totrans-656
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-O 神经架构变换器
- en: Neural architecture transformers (NAT) replace CNNs’ fixed topology with self-attention,
    replacing convolutional filtering with axial self-attention [[436](#bib.bib436),
    [441](#bib.bib441)]. This increased flexibility allows modeling long-range pixel
    dependencies crucial for vision tasks like segmentation. Vil-BERT introduced a
    multi-stage training procedure enabling pre-trained models to learn visual representations
    as well as natural language tasks.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构变换器（NAT）用自注意力替代了CNN的固定拓扑，将卷积滤波替换为轴向自注意力[[436](#bib.bib436), [441](#bib.bib441)]。这种增加的灵活性允许建模对视觉任务如分割至关重要的长距离像素依赖。Vil-BERT引入了一个多阶段的训练程序，使预训练模型能够学习视觉表示以及自然语言任务。
- en: Early works divided input images into uniform patches processed independently
    by attention layers. More sophisticated designs aim to capture visual locality
    through hierarchical patch divisions better. Rotary positional embeddings and
    attention patterns help encode translation equivariance. Architectures like CoAtNet
    cascade blocks with increased resolution, improving accuracy and interpretability.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的工作将输入图像分成均匀的补丁，由注意力层独立处理。更复杂的设计旨在通过层次化的补丁划分更好地捕捉视觉局部性。旋转位置嵌入和注意力模式有助于编码平移等变性。像CoAtNet这样的架构将块级联，增加分辨率，提高了准确性和可解释性。
- en: Multi-scale vision transformers (MViT) incorporate prior convolutional inductive
    biases in hybrid models jointly benefiting from attention and translation equivariance.
    Combining vision transformers with convolutional networks particularly benefits
    medical image segmentation leveraging anatomical priors. Swin Transformers introduces
    a shifted window mechanism to focus computation locally across higher-resolution
    feature maps.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度视觉变换器（MViT）在混合模型中融入了先前的卷积归纳偏置，利用注意力和翻译等变性带来了共同的好处。将视觉变换器与卷积网络结合，特别有利于利用解剖学先验进行医学图像分割。Swin变换器引入了一个位移窗口机制，以在更高分辨率的特征图上局部集中计算。
- en: Though still an emerging direction, neural architecture transformers open new
    pathways for CV by bringing the full generality of self-attention to bear on visual
    problems. Their continued development will surely impact future CV research by
    unlocking novel representational abilities. Alongside NAS, they hold promise for
    pushing boundaries through data-driven discovery operating directly within much
    broader algorithmic search spaces.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管仍然是一个新兴方向，神经网络架构变换器通过将自注意力的全部普遍性应用于视觉问题，开启了计算机视觉的新路径。它们的持续发展肯定会通过解锁新的表征能力对未来的计算机视觉研究产生影响。与NAS一起，它们有望通过数据驱动的发现直接在更广泛的算法搜索空间内推动边界。
- en: X-P Generative Models
  id: totrans-661
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-P 生成模型
- en: Generative models have made large strides in the area of CV through techniques
    like GANs and diffusion models [[442](#bib.bib442), [443](#bib.bib443), [444](#bib.bib444)].
    GANs pair a generator network against a discriminator network in an adversarial
    training procedure. This drives the generator to synthesize increasingly realistic
    fake images that can fool the discriminator.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型在计算机视觉领域取得了重大进展，采用了GANs和扩散模型等技术[[442](#bib.bib442), [443](#bib.bib443),
    [444](#bib.bib444)]。GANs将生成器网络与鉴别器网络配对进行对抗训练。这驱使生成器合成越来越真实的假图像，从而欺骗鉴别器。
- en: GANs have produced impressive results generating photos that are near-indistinguishable
    from real images. Applications include image-to-image translation, super-resolution,
    and manipulating image attributes like style [[444](#bib.bib444), [445](#bib.bib445),
    [446](#bib.bib446), [447](#bib.bib447)]. However, GAN training remains tricky
    to stabilize. Issues like mode collapse require careful architecture and hyperparameter
    choices.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: GANs在生成与真实图像几乎无法区分的照片方面取得了令人印象深刻的成果。应用包括图像到图像的翻译、超分辨率以及操控图像属性如风格[[444](#bib.bib444),
    [445](#bib.bib445), [446](#bib.bib446), [447](#bib.bib447)]。然而，GAN训练仍然难以稳定。模式崩溃等问题需要精心设计架构和超参数选择。
- en: Diffusion models provide an alternative generative framework gaining popularity.
    They utilize denoising diffusion probabilistic models (DDPMs) which gradually
    corrupt data with Gaussian noise before reversing the process [[442](#bib.bib442),
    [443](#bib.bib443), [444](#bib.bib444), [447](#bib.bib447), [448](#bib.bib448),
    [449](#bib.bib449)]. During generation, the model adds noise to a blank canvas
    and then predicts the noise-reduced output iteratively. This diffusion process
    proves more stable than adversarial training.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型提供了一种备受欢迎的生成框架。它们利用去噪扩散概率模型（DDPMs），在将数据逐渐添加高斯噪声后，再反向处理[[442](#bib.bib442),
    [443](#bib.bib443), [444](#bib.bib444), [447](#bib.bib447), [448](#bib.bib448),
    [449](#bib.bib449)]。在生成过程中，模型向空白画布中添加噪声，然后迭代地预测去噪后的输出。这个扩散过程证明比对抗训练更稳定。
- en: Sampling from DDPMs follows an ancestral sampling approach regressing the noise
    at each step conditioned on the previous denoised output. Advanced techniques
    like score-based sampling further improve sample quality by maximizing the model’s
    density rather than following ancestral noise. Generative diffusion models (GDMs)
    also maximize a denoising score objective specifically for a generation [[449](#bib.bib449)].
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 从DDPM进行采样遵循祖先采样方法，根据先前去噪输出的条件回归噪音。像基于得分的采样这样的先进技术通过最大化模型的密度而不是遵循祖先噪音进一步提高样本质量。生成扩散模型（GDMs）还特别为生成最大化去噪得分目标[[449](#bib.bib449)]。
- en: Diffusion models have proven highly effective at synthesizing crisp, detailed
    images across varied datasets. Large-scale vision diffusion models (LVMs) like
    DALL-E 2 and DALL-E 3 demonstrate unparalleled capabilities of generating images
    from text prompts, and can even fuse language and vision to answer trivia questions
    about synthetic images.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型已被证明在各种数据集中合成清晰、详细的图像方面非常有效。像DALL-E 2和DALL-E 3这样的大规模视觉扩散模型（LVMs）展示了从文本提示生成图像的无与伦比的能力，甚至可以融合语言和视觉回答关于合成图像的琐事问题。
- en: By generating synthetic training data, generative models also benefit downstream
    classification, detection, and segmentation tasks through data augmentation. As
    generative diffusion models continue advancing, they will surely establish new
    frontiers in CV domains ranging from image editing to scientific discovery through
    computational experimentation.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成合成训练数据，生成模型还可以通过数据增强受益于下游分类、检测和分割任务。随着生成扩散模型的不断进步，它们必定会在从图像编辑到通过计算实验进行科学发现的CV领域建立新的前沿。
- en: X-Q Meta Learning
  id: totrans-668
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-Q 元学习
- en: Meta-learning, also known as learning to learn, aims to develop models that
    can rapidly adapt to new tasks and environments using only a few training examples.
    This is achieved by learning inductive biases about learning itself on a variety
    of related tasks during a meta-training phase. These biases are then leveraged
    during meta-test time on novel tasks [[450](#bib.bib450), [451](#bib.bib451)].
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习，也被称为学会学习，旨在开发能够仅使用少量训练样例就能快速适应新任务和环境的模型。通过在元训练阶段在各种相关任务上学习关于学习本身的归纳偏见来实现这一目标。然后在元测试时间在新颖任务上利用这些偏见[[450](#bib.bib450),
    [451](#bib.bib451)]。
- en: In CV, meta-learning enables CNNs to generalize beyond the restrictions of limited
    labeled examples through fast adaptation. Model-agnostic meta-learning (MAML)
    trains initial model parameters such that a few gradient steps fine-tune into
    new tasks. This learns efficient parameter initialization rather than solutions
    for any specific task [[450](#bib.bib450), [451](#bib.bib451), [452](#bib.bib452),
    [453](#bib.bib453), [454](#bib.bib454), [455](#bib.bib455), [456](#bib.bib456),
    [457](#bib.bib457)].
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 在CV领域，元学习使CNN能够通过快速适应超越有限标记样例的限制。模型无关元学习（MAML）训练初始模型参数，使得少量梯度步骤可以微调成为新任务。这学习了高效的参数初始化，而不是针对特定任务的解决方案[[450](#bib.bib450),
    [451](#bib.bib451), [452](#bib.bib452), [453](#bib.bib453), [454](#bib.bib454),
    [455](#bib.bib455), [456](#bib.bib456), [457](#bib.bib457)]。
- en: Metric-based approaches represent classes using prototypes that summarize inter/intra-class
    relationships independent of tasks [[450](#bib.bib450), [451](#bib.bib451), [452](#bib.bib452)].
    Matching networks compare new examples to prototypes, providing fast adaptation
    through learned metric space similarities. Meta-Dataset consolidates many few-shot
    image classification datasets, advancing state-of-the-art and evaluation protocols
    in this challenging zero/few-shot regime[[450](#bib.bib450), [451](#bib.bib451),
    [452](#bib.bib452), [457](#bib.bib457)].
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 基于度量的方法使用原型来表示类，总结独立于任务的跨/内类关系[[450](#bib.bib450), [451](#bib.bib451), [452](#bib.bib452)]。匹配网络将新的示例与原型进行比较，通过学习的度量空间相似性提供快速适应。Meta-Dataset整合了许多少样本图像分类数据集，在这个具有挑战性的零/少样本范式上推动了最新技术和评估协议[[450](#bib.bib450),
    [451](#bib.bib451), [452](#bib.bib452), [457](#bib.bib457)]。
- en: Self-supervised auxiliary tasks like prediction, rotation, and context modeling
    further enhance generalization when used alongside supervised meta-learning objectives.
    Temporal ensemble models aggregate diverse predictions over time from a generator
    network, improving robustness to noise and outliers. Reinforcement meta-learning
    successfully trains visuomotor policies for robotic control from only a handful
    of demonstrations.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督的辅助任务，如预测、旋转和上下文建模，当与监督的元学习目标一起使用时，会进一步增强泛化能力。时间集成模型通过从生成网络中聚合不同的预测，提升对噪声和异常值的鲁棒性。强化元学习成功地仅通过少量示范训练视觉运动控制策略。
- en: X-R Federated Learning
  id: totrans-673
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-R 联邦学习
- en: Federated learning (FL) enables distributed training across decentralized edge
    devices without exchanging private user data like images, videos or medical scans
    . It aims to collaboratively learn a shared global model tailored to non-IID user
    distributions through coordinated local updates. This paradigm attracts increased
    interest due to growing concerns around data privacy and security.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习（FL）使得在去中心化的边缘设备上进行分布式训练成为可能，无需交换私人用户数据如图像、视频或医疗扫描。它旨在通过协调的本地更新，协作学习一个适应非
    IID 用户分布的共享全球模型。由于对数据隐私和安全性的日益关注，这一范式引起了越来越多的兴趣。
- en: FL trains a centralized CNN model through an iterative process where devices
    download the latest parameters, contribute updates computed over shards of local
    data, and then push weights back. A parameter server aggregates updates to globally
    improve the model. A key challenge arises from heterogeneity in non-IID data distributions,
    devices, and unreliable network connectivity. FedVision applies FL to object detection
    directly over fragmented client videos.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: FL 通过一个迭代过程训练一个集中式的 CNN 模型，其中设备下载最新的参数，贡献计算过的局部数据分片的更新，然后推送权重回服务器。参数服务器聚合更新以全球性地改进模型。一个关键挑战来自于非
    IID 数据分布、设备和不可靠的网络连接的异质性。FedVision 直接在分段的客户端视频上应用 FL 进行对象检测。
- en: Techniques like personalized, multi-task, and meta-learning help address statistical
    heterogeneity in FL. Continual learning aspects prevent catastrophic forgetting
    when populations change over disseminated rounds. Differentially private algorithms
    and secure aggregation schemes ensure strong privacy in collaborative updates,
    advancing FL under stringent privacy constraints beyond vision to sensitive domains
    like healthcare.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化、多任务和元学习等技术有助于解决 FL 中的统计异质性。持续学习方面防止在传播轮次中人口变化时的灾难性遗忘。差分隐私算法和安全聚合方案确保了在协作更新中的强隐私性，使
    FL 在严格的隐私约束下，超越视觉应用到像医疗保健这样的敏感领域。
- en: XI Discussion
  id: totrans-677
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XI 讨论
- en: We have methodically explored the various CNN variations that have become more
    and more popular in recent years across a wide range of application sectors through
    this thorough survey. Our goal in this discussion part is to summarize the most
    significant findings from our evaluation of the literature and offer an analytical
    viewpoint on significant problems regarding the development and prospects of this
    area of study.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这次详尽的调查，我们系统地探索了近年来在各个应用领域变得越来越流行的各种 CNN 变体。我们在讨论部分的目标是总结我们对文献评估的最重要发现，并提供对该研究领域发展和前景的重要问题的分析视角。
- en: Convolutional layers are well-suited for grid-like data types, like images because
    they have proven highly capable of capturing spatial relationships and extracting
    hierarchical patterns. At the core of CNNs, commonly used for computer vision
    tasks such as object identification and image classification, remain traditional
    2D convolutions. However, as the field has evolved, additional specialized convolution
    approaches have emerged to handle different data modalities more effectively.
    One notable application of 1D convolutions is in sequential data domains including
    time series analysis and natural language processing. Their ability to capture
    temporal dependencies has enabled state-of-the-art accuracy on various language
    and audio processing problems. Likewise, 3D convolutions allow CNNs to effectively
    model volumetric medical images and video inputs by accounting for both spatial
    and temporal dimensions.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层非常适合网格状数据类型，如图像，因为它们在捕捉空间关系和提取层次模式方面表现出了很高的能力。在CNN的核心，通常用于计算机视觉任务如对象识别和图像分类的，仍然是传统的2D卷积。然而，随着领域的发展，出现了额外的专门卷积方法，以更有效地处理不同的数据模式。其中，1D卷积在时间序列分析和自然语言处理等序列数据领域有着显著的应用。它们捕捉时间依赖性的能力使得在各种语言和音频处理问题上取得了最先进的准确度。同样，3D卷积使CNN能够通过考虑空间和时间维度，有效地对体积医学图像和视频输入进行建模。
- en: While basic convolution varieties such as 2D and 3D continue powering many top
    models, more efficient variants have also been developed. Dilated convolutions
    utilize dilations to widen receptive fields without loss of resolution, aiding
    high-level semantic tasks such as segmentation. Grouped convolutions offer a means
    of factorizing convolutions to dramatically reduce computation and memory usage,
    enabling large, deep architectures. However, their representational abilities
    may remain limited compared to standard convolutions for advanced analysis. Depthwise
    separable convolutions, as used in MobileNets, have achieved tremendous success
    in deploying efficient CNNs on embedded and mobile devices via their channel-wise
    decomposition.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基本的卷积变种如2D和3D卷积仍然推动着许多顶级模型，但更高效的变体也已经被开发出来。膨胀卷积利用膨胀来扩展感受野而不损失分辨率，从而帮助完成高层次的语义任务，如分割。分组卷积提供了一种将卷积分解以显著减少计算和内存使用的方法，使得大型深度架构成为可能。然而，与标准卷积相比，它们在高级分析中的表示能力可能仍然有限。深度可分离卷积，如在MobileNets中使用的，通过其按通道分解的特性，在嵌入式和移动设备上部署高效CNN取得了巨大的成功。
- en: In addition to novel convolution designs, the field is witnessing increasingly
    innovative integration of concepts from parallel research areas. For example,
    vision transformer models incorporate attention mechanisms to replace convolutional
    building blocks entirely, achieving strong results, especially on large datasets.
    Techniques like capsule networks aim to overcome CNN limitations through dynamic
    routing between feature vectors. Generative models such as Pix2Pix employ convolutional
    decoders to generate high-fidelity images from semantic maps or sketches. Advances
    in self-supervised learning provide alternative pretraining paradigms bypassing
    the need for vast annotated datasets.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 除了新型卷积设计外，该领域还在不断创新地融合来自平行研究领域的概念。例如，视觉变换器模型通过引入注意力机制完全替代卷积构建块，特别是在大数据集上取得了强劲的结果。像胶囊网络这样的技术旨在通过特征向量之间的动态路由来克服CNN的局限性。生成模型如Pix2Pix采用卷积解码器从语义图或草图中生成高保真图像。自监督学习的进展提供了替代的预训练范式，绕过了对大量注释数据集的需求。
- en: Further combining of deep learning techniques seems poised to yield fruitful
    synergies. For instance, incorporating attention into convolutional pipelines
    could endow them with the benefits of both approaches. Moreover, self-supervised
    mechanisms may help the unsupervised discovery of interpretable convolutional
    filters well-suited to specific domains. Despite remarkable achievements, open
    challenges remain regarding robustness, sparse data scenarios, model interpretability,
    and trustworthiness. Future progress relies on close collaboration between academia
    and industry to define real-world needs and expand deep learning’s positive societal
    impact.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术的进一步结合似乎有望产生富有成效的协同效应。例如，将注意力机制融入卷积流程中，可能会赋予它们两种方法的优点。此外，自监督机制可能有助于无监督地发现适合特定领域的可解释卷积滤波器。尽管取得了显著成就，但在鲁棒性、稀疏数据情境、模型可解释性和可信度方面仍然存在开放挑战。未来的进展依赖于学术界和工业界的紧密合作，以定义现实世界的需求并扩大深度学习对社会的积极影响。
- en: Some convolution types have proven more enduring than others based on their
    flexibility and ability to adaptively fit diverse applications. While LeNet certainly
    played an instrumental pioneering role, more recent architectures better capture
    inherent data properties through principled network designs and optimizations.
    Meanwhile, innovation continues on all fronts, suggesting no single solution has
    emerged as definitive. Success hinges on judiciously combining innovations tailored
    to particular contexts rather than wholesale replacement of existing paradigms.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 一些卷积类型由于其灵活性和适应性，已被证明比其他类型更持久。虽然LeNet无疑在早期发挥了重要的开创性作用，但近年来的架构通过原则性的网络设计和优化更好地捕捉了数据的固有特性。与此同时，创新在各个方面持续进行，暗示尚未出现单一的解决方案。成功依赖于根据特定情境明智地结合创新，而不是完全替代现有的范式。
- en: A promising outlook envisions continued refinement of core CNN building blocks
    and their harmonious integration with new algorithmic concepts from self-supervised
    learning, attention mechanisms, and generative models. In conclusion, this survey
    highlights both the remarkable advances of convolutional neural networks to date
    and their vast unrealized potential through the future intersection of ideas across
    deep learning’s constantly evolving landscape.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 一个充满希望的前景设想了核心CNN构建模块的持续优化及其与自监督学习、注意力机制和生成模型中的新算法概念的和谐结合。总之，本综述突出了卷积神经网络迄今为止的显著进展及其在深度学习不断演变的领域中未来的广阔未实现潜力。
- en: XII Conclusion
  id: totrans-685
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XII 结论
- en: In this comprehensive study of different convolution types in deep learning,
    we have gained valuable insights into these techniques’ diverse applications and
    strengths. CNNs have proven to be highly effective in various domains, ranging
    from image recognition to natural language processing. We compared various types
    of CNNS in various aspects, allowing us to understand their unique characteristics
    and advantages for specific tasks. Overall, this study emphasizes the importance
    of convolution in deep learning and its potential for future advances and improvements
    in artificial intelligence. Furthermore, the findings suggest that CNNs’ versatility
    makes them suitable for various applications beyond traditional computer vision
    tasks. Furthermore, the study emphasizes the importance of additional research
    and development to optimize and refine these techniques for specific domains and
    tasks.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 在对深度学习中不同卷积类型的综合研究中，我们获得了对这些技术多样化应用和优势的宝贵见解。CNN在各种领域，如图像识别和自然语言处理，已经被证明非常有效。我们在多个方面比较了各种类型的CNN，使我们能够理解它们在特定任务中的独特特性和优势。总体而言，这项研究强调了卷积在深度学习中的重要性及其在人工智能领域未来进步和改进的潜力。此外，研究结果表明，CNN的多功能性使其适用于除传统计算机视觉任务之外的各种应用。此外，研究还强调了进一步研究和开发以优化和改进这些技术以适应特定领域和任务的重要性。
- en: Acknowledgements
  id: totrans-687
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This work was partially supported by the NYUAD Center for Artificial Intelligence
    and Robotics (CAIR), funded by Tamkeen under the NYUAD Research Institute Award
    CG010\. This work was also partially supported by the project “eDLAuto: An Automated
    Framework for Energy-Efficient Embedded Deep Learning in Autonomous Systems”,
    funded by the NYUAD Research Enhancement Fund (REF).'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了NYUAD人工智能与机器人中心（CAIR）的资助，该中心由Tamkeen资助，属于NYUAD研究院奖项CG010。该研究还部分得到了项目“eDLAuto：一个用于自主系统中节能嵌入式深度学习的自动化框架”的资助，该项目由NYUAD研究提升基金（REF）资助。
- en: References
  id: totrans-689
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] I. H. Sarker, ”Deep Learning: A Comprehensive Overview on Techniques, Taxonomy,
    Applications and Research Directions,” SN Computer Science, vol. 2, no. 6, Aug.
    2021, doi: 10.1007/s42979-021-00815-1.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] I. H. Sarker，”深度学习：技术、分类、应用和研究方向的全面概述，” SN 计算机科学，第2卷，第6期，2021年8月，doi: 10.1007/s42979-021-00815-1。'
- en: '[2] G. Hong, A. Folcarelli, J. Less, C. Wang, N. Erbasi, and S. Lin, ”Voice
    Assistants and Cancer Screening: A Comparison of Alexa, Siri, Google Assistant,
    and Cortana,” The Annals of Family Medicine, vol. 19, no. 5, pp. 447–449, Sep.
    2021, doi: 10.1370/afm.2713.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] G. Hong, A. Folcarelli, J. Less, C. Wang, N. Erbasi 和 S. Lin，”语音助手与癌症筛查：Alexa、Siri、Google
    Assistant 和 Cortana 的比较，” 家庭医学年鉴，第19卷，第5期，第447–449页，2021年9月，doi: 10.1370/afm.2713。'
- en: '[3] A. Kumar, S. Gadag, and U. Y. Nayak, ”The Beginning of a New Era: Artificial
    Intelligence in Healthcare,” Advanced Pharmaceutical Bulletin, vol. 11, no. 3,
    pp. 414–425, Jul. 2020, doi: 10.34172/apb.2021.049.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Kumar, S. Gadag 和 U. Y. Nayak，”新时代的开始：人工智能在医疗中的应用，” 高级制药公告，第11卷，第3期，第414–425页，2020年7月，doi:
    10.34172/apb.2021.049。'
- en: '[4] J. B. Heaton and N. Polson, ”Deep Learning for Finance: Deep Portfolios,”
    SSRN Electronic Journal, 2016, doi: 10.2139/ssrn.2838013.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. B. Heaton 和 N. Polson，”金融领域的深度学习：深度投资组合，” SSRN 电子期刊，2016年，doi: 10.2139/ssrn.2838013。'
- en: '[5] M. Veres and M. Moussa, ”Deep Learning for Intelligent Transportation Systems:
    A Survey of Emerging Trends,” IEEE Transactions on Intelligent Transportation
    Systems, vol. 21, no. 8, pp. 3152-3168, Aug. 2020, doi: 10.1109/TITS.2019.2929020.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Veres 和 M. Moussa，”智能交通系统中的深度学习：新兴趋势调查，” IEEE 智能交通系统汇刊，第21卷，第8期，第3152-3168页，2020年8月，doi:
    10.1109/TITS.2019.2929020。'
- en: '[6] M. Ghaderzadeh and F. Asadi, ”Deep Learning in the Detection and Diagnosis
    of COVID-19 Using Radiology Modalities: A Systematic Review,” Journal of Healthcare
    Engineering, vol. 2021, pp. 1-10, Mar. 2021, doi: 10.1155/2021/6677314.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Ghaderzadeh 和 F. Asadi，”深度学习在 COVID-19 检测和诊断中的应用：系统评估，” 医疗工程杂志，第2021卷，第1-10页，2021年3月，doi:
    10.1155/2021/6677314。'
- en: '[7] I. Goodfellow, Y. Bengio, and A. Courville, ”Deep learning,” MIT press,
    2016.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] I. Goodfellow, Y. Bengio 和 A. Courville，”深度学习，” MIT出版社，2016年。'
- en: '[8] J. Schmidhuber, ”Deep learning in neural networks: An overview,” Neural
    Networks, vol. 61, pp. 85–117, 2015.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Schmidhuber，”神经网络中的深度学习：概述，” 神经网络，第61卷，第85–117页，2015年。'
- en: '[9] N. K. Logothetis and D. L. Sheinberg, ”Visual Object Recognition,” Mar.
    1996.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] N. K. Logothetis 和 D. L. Sheinberg，”视觉对象识别，” 1996年3月。'
- en: '[10] H. Zhou et al., ”A deep learning approach for medical waste classification,”
    Feb. 2022.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] H. Zhou 等，”一种用于医疗废物分类的深度学习方法，” 2022年2月。'
- en: '[11] S. Yang and J. Anjie, ”Recognition of Oil and Gas Reservoir Space Based
    on Deep Learning,” Jan. 2021.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Yang 和 J. Anjie，”基于深度学习的油气储层空间识别，” 2021年1月。'
- en: '[12] D. Jimenez-Carretero et al., ”Tox_(R)CNN: Deep learning-based nuclei profiling
    tool for drug toxicity screening,” Plos Computational Biology, vol. 14, no. 11,
    pp. e1006238, Nov. 2018, doi: 10.1371/journal.pcbi.1006238.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. Jimenez-Carretero 等，”Tox_(R)CNN: 基于深度学习的细胞核分析工具用于药物毒性筛选，” PLoS 计算生物学，第14卷，第11期，第e1006238页，2018年11月，doi:
    10.1371/journal.pcbi.1006238。'
- en: '[13] A. G. Howard et al., ”Mobilenets: Efficient convolutional neural networks
    for mobile vision applications,” arXiv preprint arXiv:1704.04861, 2017.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. G. Howard 等，”Mobilenets: 高效的卷积神经网络用于移动视觉应用，” arXiv 预印本 arXiv:1704.04861，2017年。'
- en: '[14] TensorFlow Lite. https://www.tensorflow.org/lite/'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] TensorFlow Lite. https://www.tensorflow.org/lite/'
- en: '[15] R. Kavuluru, ”An end-to-end deep learning architecture for extracting
    protein–protein interactions affected by genetic mutations,” Jan. 2018.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] R. Kavuluru，”一种端到端的深度学习架构用于提取受遗传突变影响的蛋白质–蛋白质相互作用，” 2018年1月。'
- en: '[16] A. Kamilaris and F. X. Prenafeta-Boldú, ”A review of the use of convolutional
    neural networks in agriculture,” The Journal of Agricultural Science, vol. 156,
    no. 3, pp. 312–322, Apr. 2018, doi: 10.1017/s0021859618000436.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] A. Kamilaris 和 F. X. Prenafeta-Boldú，”卷积神经网络在农业中的应用综述，” 农业科学杂志，第156卷，第3期，第312–322页，2018年4月，doi:
    10.1017/s0021859618000436。'
- en: '[17] S. Klos and J. Patalas-Maliszewska, ”A Model for the Intelligent Supervision
    of Production for Industry 4.0,” May. 2022.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Klos 和 J. Patalas-Maliszewska，”工业4.0智能生产监督模型，” 2022年5月。'
- en: '[18] R. Nakajo, S. Murata, H. Arie, and T. Ogata, ”Acquisition of Viewpoint
    Transformation and Action Mappings via Sequence to Sequence Imitative Learning
    by Deep Neural Networks,” Jul. 2018.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] R. Nakajo, S. Murata, H. Arie 和 T. Ogata，”通过深度神经网络的序列到序列模仿学习获得视点变换和动作映射，”
    2018年7月。'
- en: '[19] V. D. Veksler, B. E. Hoffman, and N. Buchler, ”Symbolic Deep Networks:
    A Psychologically Inspired Lightweight and Efficient Approach to Deep Learning,”
    Topics in Cognitive Science, Oct. 2021.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] V. D. Veksler, B. E. Hoffman 和 N. Buchler，“符号深度网络：一种心理学启发的轻量级高效深度学习方法，”
    *认知科学专题*，2021年10月。'
- en: '[20] C. Xue et al., ”Detection of dementia on voice recordings using deep learning:
    a Framingham Heart Study,” Aug. 2021.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. Xue 等，“使用深度学习在声音录音中检测痴呆：一项弗雷明汉心脏研究，” 2021年8月。'
- en: '[21] C. Lai et al., ”The Use of Convolutional Neural Networks and Digital Camera
    Images in Cataract Detection,” Mar. 2022.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. Lai 等，“卷积神经网络和数字相机图像在白内障检测中的应用，” 2022年3月。'
- en: '[22] H. Ahmed and M. Kashmola, ”A proposed architecture for convolutional neural
    networks to detect skin cancers,” Iaes International Journal of Artificial Intelligence
    (Ij-Ai), vol. 11, no. 2, pp. 485, Jun. 2022, doi: 10.11591/ijai.v11.i2.pp485-493.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Ahmed 和 M. Kashmola，“一种用于检测皮肤癌的卷积神经网络提议架构，” *Iaes国际人工智能期刊*（Ij-Ai），第11卷，第2期，第485页，2022年6月，doi:
    10.11591/ijai.v11.i2.pp485-493。'
- en: '[23] X. Jiang, L. Teng and L. Teng, ”A novel Gauss-Laplace operator based on
    multi-scale convolution for dance motion image enhancement,” Jul. 2018.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] X. Jiang, L. Teng 和 L. Teng，“一种基于多尺度卷积的高斯-拉普拉斯算子用于舞蹈动作图像增强，” 2018年7月。'
- en: '[24] S. Shuai, Y. Zhou and X. Song, ”A Stochastic Max Pooling Strategy for
    Convolutional Neural Network Trained by Noisy Samples,” International Journal
    of Computers Communications & Control, vol. 15, no. 1, Feb. 2020, doi: 10.15837/ijccc.2020.1.3712.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Shuai, Y. Zhou 和 X. Song，“一种用于噪声样本训练的卷积神经网络的随机最大池化策略，” *计算机通信与控制国际期刊*，第15卷，第1期，2020年2月，doi:
    10.15837/ijccc.2020.1.3712。'
- en: '[25] Y. Zhang, J. Hou, Q. Wang, A. Hou and Y. Liu, ”Application of Transfer
    Learning and Feature Fusion Algorithms to Improve the Identification and Prediction
    Efficiency of Premature Ovarian Failure,” Mar. 2022.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Zhang, J. Hou, Q. Wang, A. Hou 和 Y. Liu，“迁移学习和特征融合算法在提高早发性卵巢衰竭识别与预测效率中的应用，”
    2022年3月。'
- en: '[26] A. Younesi, R. Afrouzian, and Y. Seyfari, ”A transfer learning approach
    with convolutional neural network for Face Mask Detection,” Journal of Advanced
    Signal Processing, vol. 5, no. 1, Jan. 2021, doi: 10.22034/jasp.2022.48447.1167.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Younesi, R. Afrouzian 和 Y. Seyfari，“一种基于卷积神经网络的迁移学习方法用于口罩检测，” *高级信号处理杂志*，第5卷，第1期，2021年1月，doi:
    10.22034/jasp.2022.48447.1167。'
- en: '[27] H. M. Jalajamony, H. M. Jalajamony and R. E. Fernandez, ”Deep-Learning
    Based Estimation of Dielectrophoretic Force,” Micromachines, vol. 13, no. 1, pp.
    41, Dec. 2021, doi: 10.3390/mi13010041.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] H. M. Jalajamony, H. M. Jalajamony 和 R. E. Fernandez，“基于深度学习的介电泳力估计，”
    *微型机械*，第13卷，第1期，第41页，2021年12月，doi: 10.3390/mi13010041。'
- en: '[28] W. Liang, H. Zhang, G. Zhang and H. Cao, ”Rice Blast Disease Recognition
    Using a Deep Convolutional Neural Network,” Feb. 2019.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] W. Liang, H. Zhang, G. Zhang 和 H. Cao，“使用深度卷积神经网络进行稻瘟病识别，” 2019年2月。'
- en: '[29] G. Yang, X. Liang, S. Deng and X. D. Chen, ”Principal Component Research
    of the Teaching Model Based on Multimodal Neural Network Algorithm,” Jun. 2022.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] G. Yang, X. Liang, S. Deng 和 X. D. Chen，“基于多模态神经网络算法的教学模型主成分研究，” 2022年6月。'
- en: '[30] J. Rong, Y. Chen and J. Yang, ”CNN-LSTM Hybrid Model for Kinematic Feature
    Analysis and Parabolic Radian Prediction in Basketball Videos,” Sep. 2021.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Rong, Y. Chen 和 J. Yang，“用于篮球视频中的运动特征分析和抛物线弯度预测的CNN-LSTM混合模型，” 2021年9月。'
- en: '[31] A. Tiwari, M. Silver and A. Karnieli, ”A deep learning approach for automatic
    identification of ancient agricultural water harvesting systems,” Apr. 2023.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] A. Tiwari, M. Silver 和 A. Karnieli，“一种自动识别古代农业水收集系统的深度学习方法，” 2023年4月。'
- en: '[32] I. Ahmed, B. T. Hammad and N. Jamil, ”A comparative analysis of image
    copy-move forgery detection algorithms based on hand and machine-crafted features,”
    *Indonesian Journal of Electrical Engineering and Computer Science*, vol. 22,
    no. 2, pp. 1177, May. 2021, doi: 10.11591/ijeecs.v22.i2.pp1177-1190.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] I. Ahmed, B. T. Hammad 和 N. Jamil，“基于手工和机器制作特征的图像拷贝-移动伪造检测算法的比较分析，” *印度尼西亚电气工程与计算机科学杂志*，第22卷，第2期，第1177页，2021年5月，doi:
    10.11591/ijeecs.v22.i2.pp1177-1190。'
- en: '[33] E. Setyati, S. Az, S. A. Hudiono and F. Kurniawan, ”CNN based Face Recognition
    System for Patients with Down and William Syndrome,” *Knowledge Engineering and
    Data Science*, vol. 4, no. 2, pp. 138, Dec. 2021, doi: 10.17977/um018v4i22021p138-144.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] E. Setyati, S. Az, S. A. Hudiono 和 F. Kurniawan，“基于CNN的面部识别系统用于唐氏综合症和威廉姆氏综合症患者，”
    *知识工程与数据科学*，第4卷，第2期，第138页，2021年12月，doi: 10.17977/um018v4i22021p138-144。'
- en: '[34] G. Hu, K. Wang and L. Liu, ”Underwater Acoustic Target Recognition Based
    on Depthwise Separable Convolution Neural Networks,” *Sensors*, vol. 21, no. 4,
    pp. 1429, Feb. 2021, doi: 10.3390/s21041429.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] G. Hu, K. Wang 和 L. Liu，《基于深度可分离卷积神经网络的水下声学目标识别》，*Sensors*，第21卷，第4期，页码1429，2021年2月，doi:
    10.3390/s21041429。'
- en: '[35] Z. Jiang, Z. Dong, L. Wang and W. Jiang, ”Method for Diagnosis of Acute
    Lymphoblastic Leukemia Based on ViT-CNN Ensemble Model,” Aug. 2021.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. Jiang, Z. Dong, L. Wang 和 W. Jiang，《基于ViT-CNN集成模型的急性淋巴细胞白血病诊断方法》，2021年8月。'
- en: '[36] L. Wang, W. Zhou, Q. Chang, J. Chen and X. Zhou, ”Deep Ensemble Detection
    of Congestive Heart Failure Using Short-Term RR Intervals,” *IEEE Access*, vol.
    7, pp. 69559-69574, Jan. 2019, doi: 10.1109/access.2019.2912226.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] L. Wang, W. Zhou, Q. Chang, J. Chen 和 X. Zhou，《基于短期RR间期的充血性心力衰竭深度集成检测》，*IEEE
    Access*，第7卷，页码69559-69574，2019年1月，doi: 10.1109/access.2019.2912226。'
- en: '[37] A. Zafar et al., “A Comparison of Pooling Methods for Convolutional Neural
    Networks,” *Applied Sciences*, vol. 12, no. 17, p. 8643, Jan. 2022, doi: 10.3390/app12178643.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Zafar 等人，《卷积神经网络的池化方法比较》，*Applied Sciences*，第12卷，第17期，页码8643，2022年1月，doi:
    10.3390/app12178643。'
- en: '[38] J. Zhu, J. Jang-Jaccard and P. A. Watters, ”Multi-Loss Siamese Neural
    Network With Batch Normalization Layer for Malware Detection,” Jan. 2020.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Zhu, J. Jang-Jaccard 和 P. A. Watters，《带有批量归一化层的多损失Siamese神经网络用于恶意软件检测》，2020年1月。'
- en: '[39] A. Maniatopoulos and N. Mitianoudis, ”Learnable Leaky ReLU (LeLeLU): An
    Alternative Accuracy-Optimized Activation Function,” Dec. 2021.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. Maniatopoulos 和 N. Mitianoudis，《可学习的漏斗ReLU（LeLeLU）：一种替代的精度优化激活函数》，2021年12月。'
- en: '[40] *”Delving Deep into Rectifiers: Surpassing Human-Level …”*, arxiv.org,
    (Accessed 18 Jul. 2023).'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] *《深入探讨整流器：超越人类水平……》*，arxiv.org，（访问日期：2023年7月18日）。'
- en: '[41] L. E. V. Dyck, R. Kwitt, S. J. Denzler and W. R. Gruber, ”Comparing Object
    Recognition in Humans and Deep Convolutional Neural Networks—An Eye Tracking Study,”
    *Frontiers in Neuroscience*, vol. 15, Oct. 2021, doi: 10.3389/fnins.2021.750639.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] L. E. V. Dyck, R. Kwitt, S. J. Denzler 和 W. R. Gruber，《人类与深度卷积神经网络的目标识别比较——眼动追踪研究》，*Frontiers
    in Neuroscience*，第15卷，2021年10月，doi: 10.3389/fnins.2021.750639。'
- en: '[42] Z. Wu, Y. Guo, W. Lin, S. Yu and Y. Ji, ”A Weighted Deep Representation
    Learning Model for Imbalanced Fault Diagnosis in Cyber-Physical Systems,” *Sensors*,
    vol. 18, no. 4, pp. 1096, Apr. 2018, doi: 10.3390/s18041096.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Wu, Y. Guo, W. Lin, S. Yu 和 Y. Ji，《一种加权深度表示学习模型用于网络物理系统中不平衡故障诊断》，*Sensors*，第18卷，第4期，页码1096，2018年4月，doi:
    10.3390/s18041096。'
- en: '[43] H. Xia, C. Ding and Y. Liu, ”Sentiment Analysis Model Based on Self-Attention
    and Character-Level Embedding,” *IEEE Access*, vol. 8, pp. 184614-184620, Jan.
    2020, doi: 10.1109/access.2020.3029694.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H. Xia, C. Ding 和 Y. Liu，《基于自注意力和字符级嵌入的情感分析模型》，*IEEE Access*，第8卷，页码184614-184620，2020年1月，doi:
    10.1109/access.2020.3029694。'
- en: '[44] Z. Zhao, X. Xie, W. Liu and Q. Pan, ”A Hybrid-3D Convolutional Network
    for Video Compressive Sensing,” Jan. 2020.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Z. Zhao, X. Xie, W. Liu 和 Q. Pan，《用于视频压缩感知的混合3D卷积网络》，2020年1月。'
- en: '[45] Z. Liu et al., ”Deep learning based brain tumor segmentation: a survey,”
    Jul. 2022.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Z. Liu 等人，《基于深度学习的脑肿瘤分割：综述》，2022年7月。'
- en: '[46] H. Chen et al., ”A Supervised Video Hashing Method Based on a Deep 3D
    Convolutional Neural Network for Large-Scale Video Retrieval,” *Sensors*, vol.
    21, no. 9, pp. 3094, Apr. 2021, doi: 10.3390/s21093094.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Chen 等人，《基于深度3D卷积神经网络的大规模视频检索的监督视频哈希方法》，*Sensors*，第21卷，第9期，页码3094，2021年4月，doi:
    10.3390/s21093094。'
- en: '[47] G. Li, S. Jiang, I. Yun and J. Kim, ”Depth-Wise Asymmetric Bottleneck
    With Point-Wise Aggregation Decoder for Real-Time Semantic Segmentation in Urban
    Scenes,” *IEEE Access*, vol. 8, pp. 27495-27506, Jan. 2020, doi: 10.1109/access.2020.2971760.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] G. Li, S. Jiang, I. Yun 和 J. Kim，《用于城市场景实时语义分割的深度非对称瓶颈与逐点聚合解码器》，*IEEE
    Access*，第8卷，页码27495-27506，2020年1月，doi: 10.1109/access.2020.2971760。'
- en: '[48] Z. Lu et al., ”Fast Single Image Super-Resolution Via Dilated Residual
    Networks,” Jan. 2019.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Z. Lu 等人，《通过膨胀残差网络的快速单图像超分辨率》，2019年1月。'
- en: '[49] R. Kolaghassi, M. K. Al-Hares and K. Sirlantzis, ”Systematic Review of
    Intelligent Algorithms in Gait Analysis and Prediction for Lower Limb Robotic
    Systems,” Jan. 2021.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] R. Kolaghassi, M. K. Al-Hares 和 K. Sirlantzis，《下肢机器人系统步态分析和预测中智能算法的系统综述》，2021年1月。'
- en: '[50] M. Capra, B. Bussolino, A. Marchisio, G. Masera, M. Martina and M. Shafique,
    ”Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey
    of Current Trends, Challenges, and the Road Ahead,” in *IEEE Access*, vol. 8,
    pp. 225134-225180, 2020, doi: 10.1109/ACCESS.2020.3039858.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. Capra, B. Bussolino, A. Marchisio, G. Masera, M. Martina 和 M. Shafique,
    “加速深度神经网络的硬件和软件优化：当前趋势、挑战及未来发展概述，” *IEEE Access*，第8卷，第225134-225180页，2020年，doi:
    10.1109/ACCESS.2020.3039858。'
- en: '[51] K. Shaheen, M. A. Hanif, O. Hasan, and M. Shafique, “Continual Learning
    for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks,” *Journal
    of Intelligent & Robotic Systems*, vol. 105, no. 1, Apr. 2022, doi: 1007/s10846-022-01603-6.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] K. Shaheen, M. A. Hanif, O. Hasan 和 M. Shafique, “面向真实世界自主系统的持续学习：算法、挑战和框架，”
    *Journal of Intelligent & Robotic Systems*，第105卷，第1期，2022年4月，doi: 1007/s10846-022-01603-6。'
- en: '[52] C. Szegedy et al., “Going deeper with convolutions,” *CVPR 2015*.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] C. Szegedy 等, “通过卷积深入发展，” *CVPR 2015*。'
- en: '[53] K. He et al., “Deep residual learning for image recognition,” *CVPR 2016*.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] K. He 等, “用于图像识别的深度残差学习，” *CVPR 2016*。'
- en: '[54] G. Huang et al., “Densely connected convolutional networks,” *CVPR 2017*.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] G. Huang 等, “密集连接卷积网络，” *CVPR 2017*。'
- en: '[55] A. Howard et al., “MobileNets: Efficient CNNs for mobile vision applications,”
    *arXiv 2017*.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] A. Howard 等, “MobileNets：用于移动视觉应用的高效 CNN，” *arXiv 2017*。'
- en: '[56] M. Tan and Q. Le, “EfficientNet: Rethinking model scaling for CNNs,” *ICML
    2019*.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M. Tan 和 Q. Le, “EfficientNet：重新思考 CNN 模型扩展，” *ICML 2019*。'
- en: '[57] B. Jin, L. Cruz and N. Goncalves, ”Deep Facial Diagnosis: Deep Transfer
    Learning From Face Recognition to Facial Diagnosis,” *IEEE Access*, vol. 8, pp.
    123649-123661, Jan. 2020, doi: 10.1109/access.2020.3005687.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] B. Jin, L. Cruz 和 N. Goncalves, “深度面部诊断：从面部识别到面部诊断的深度迁移学习，” *IEEE Access*，第8卷，第123649-123661页，2020年1月，doi:
    10.1109/access.2020.3005687。'
- en: '[58] Y. Yang, D. Kim and B. H. Oh, ”Deep Convolutional Grid Warping Network
    for Joint Depth Map Upsampling,” Jan. 2020.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Yang, D. Kim 和 B. H. Oh, “用于联合深度图上采样的深度卷积网格扭曲网络，” 2020年1月。'
- en: '[59] P. Lang, X. Fu, C. Feng, J. Dong, R. Qin and M. Martorella, ”LW-CMDANet:
    A Novel Attention Network for SAR Automatic Target Recognition,” *IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing*, vol. 15,
    pp. 6615-6630, Jan. 2022, doi: 10.1109/jstars.2022.3195074.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] P. Lang, X. Fu, C. Feng, J. Dong, R. Qin 和 M. Martorella, “LW-CMDANet：一种新型的注意力网络用于
    SAR 自动目标识别，” *IEEE Journal of Selected Topics in Applied Earth Observations and
    Remote Sensing*，第15卷，第6615-6630页，2022年1月，doi: 10.1109/jstars.2022.3195074。'
- en: '[60] L. Li, J. Wang and X. Li, ”Efficiency Analysis of Machine Learning Intelligent
    Investment Based on K-Means Algorithm,” *IEEE Access*, vol. 8, pp. 147463-147470,
    Jan. 2020, doi: 10.1109/access.2020.3011366.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] L. Li, J. Wang 和 X. Li, “基于 K-Means 算法的机器学习智能投资效率分析，” *IEEE Access*，第8卷，第147463-147470页，2020年1月，doi:
    10.1109/access.2020.3011366。'
- en: '[61] H. Tomosada, T. Kudo, T. Fujisawa and M. Ikehara, ”GAN-Based Image Deblurring
    Using DCT Loss With Customized Datasets,” Jan. 2021.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] H. Tomosada, T. Kudo, T. Fujisawa 和 M. Ikehara, “基于 GAN 的图像去模糊，使用定制数据集的
    DCT 损失，” 2021年1月。'
- en: '[62] T. Wang *et al.*, ”Pretraining is All You Need for Image-to-Image Translation,”
    May. 2022.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] T. Wang *et al.*, “预训练是图像到图像翻译所需的一切，” 2022年5月。'
- en: '[63] B. Kaddar, H. Fizazi, M. Hernandez-Cabronero, V. Sanchez and J. Serra-Sagrista,
    ”DivNet: Efficient Convolutional Neural Network via Multilevel Hierarchical Architecture
    Design,” Jan. 2021.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] B. Kaddar, H. Fizazi, M. Hernandez-Cabronero, V. Sanchez 和 J. Serra-Sagrista,
    “DivNet：通过多级层次结构设计实现高效卷积神经网络，” 2021年1月。'
- en: '[64] R. Yu and J. Sun, ”Learning Polynomial-Based Separable Convolution for
    3D Point Cloud Analysis,” Jun. 2021.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Yu 和 J. Sun, “学习基于多项式的可分卷积用于 3D 点云分析，” 2021年6月。'
- en: '[65] A. Kara, ”A Hybrid Prognostic Approach Based on Deep Learning for the
    Degradation Prediction of Machinery,” *Sakarya University Journal of Computer
    and Information Sciences*, vol. 4, no. 2, pp. 216-226, Aug. 2021, doi: 10.35377/saucis.04.02.912154.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. Kara, “基于深度学习的混合预测方法用于机器设备的退化预测，” *Sakarya University Journal of Computer
    and Information Sciences*，第4卷，第2期，第216-226页，2021年8月，doi: 10.35377/saucis.04.02.912154。'
- en: '[66] X. Zhang, J. Zhou, W. Sun and S. K. Jha, ”A Lightweight CNN Based on Transfer
    Learning for COVID-19 Diagnosis,” *Computers Materials & Continua*, vol. 72, no.
    1, pp. 1123-1137, Jan. 2022, doi: 10.32604/cmc.2022.024589.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] X. Zhang, J. Zhou, W. Sun 和 S. K. Jha, “基于迁移学习的轻量级 CNN 用于 COVID-19 诊断，”
    *Computers Materials & Continua*，第72卷，第1期，第1123-1137页，2022年1月，doi: 10.32604/cmc.2022.024589。'
- en: '[67] F. Sultonov, J. Park, S. Yun, D. Lim and J. Kang, ”Mixer U-Net: An Improved
    Automatic Road Extraction from UAV Imagery,” Feb. 2022.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] F. Sultonov, J. Park, S. Yun, D. Lim 和 J. Kang, “Mixer U-Net：一种改进的无人机影像自动道路提取方法，”
    2022年2月。'
- en: '[68] J. Shao, C. Qu, J. Li and S. Peng, ”A Lightweight Convolutional Neural
    Network Based on Visual Attention for SAR Image Target Classification,” *Sensors*,
    vol. 18, no. 9, pp. 3039, Sep. 2018, doi: 10.3390/s18093039.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Shao, C. Qu, J. Li 和 S. Peng，“基于视觉注意力的轻量级卷积神经网络用于 SAR 图像目标分类，” *传感器*，第
    18 卷，第 9 期，第 3039 页，2018 年 9 月，doi: 10.3390/s18093039。'
- en: '[69] D. AL-Alimi, Y. Shao, R. Feng, M. A. A. Al-qaness, T. Seppänen and S.
    Kim, ”Multi-Scale Geospatial Object Detection Based on Shallow-Deep Feature Extraction,”
    *Remote Sensing*, vol. 11, no. 21, pp. 2525, Oct. 2019, doi: 10.3390/rs11212525.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] D. AL-Alimi, Y. Shao, R. Feng, M. A. A. Al-qaness, T. Seppänen 和 S. Kim，“基于浅层-深层特征提取的多尺度地理空间对象检测，”
    *遥感*，第 11 卷，第 21 期，第 2525 页，2019 年 10 月，doi: 10.3390/rs11212525。'
- en: '[70] X. Yang, A. Chen, G. Zhou, W. Chen, Y. Gao and R. Jiang, ”Instance Segmentation
    and Classification Method for Plant Leaf Images Based on ISC-MRCNN and APS-DCCNN,”
    Jan. 2020.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] X. Yang, A. Chen, G. Zhou, W. Chen, Y. Gao 和 R. Jiang，“基于 ISC-MRCNN 和
    APS-DCCNN 的植物叶片图像实例分割与分类方法，”2020 年 1 月。'
- en: '[71] Q. Liu *et al.*, ”Hybrid Attention Based Residual Network for Pansharpening,”
    *Remote Sensing*, vol. 13, no. 10, pp. 1962, May. 2021, doi: 10.3390/rs13101962.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Q. Liu *等*，“基于混合注意力的残差网络用于全色锐化，” *遥感*，第 13 卷，第 10 期，第 1962 页，2021 年 5
    月，doi: 10.3390/rs13101962。'
- en: '[72] A. Ju and Z. Wang, ”Convolutional block attention module based on visual
    mechanism for robot image edge detection,” *Icst Transactions on Scalable Information
    Systems*, pp. 172214, Jul. 2018, doi: 10.4108/eai.19-11-2021.172214.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Ju 和 Z. Wang，“基于视觉机制的卷积块注意力模块用于机器人图像边缘检测，” *Icst 可扩展信息系统交易*，第 172214
    页，2018 年 7 月，doi: 10.4108/eai.19-11-2021.172214。'
- en: '[73] M. Alam, M. D. Samad, L. Vidyaratne, A. Glandon, and K. M. Iftekharuddin,
    “Survey on Deep Neural Networks in Speech and Vision Systems,” *Neurocomputing*,
    vol. 417, pp. 302–321, Dec. 2020.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Alam, M. D. Samad, L. Vidyaratne, A. Glandon 和 K. M. Iftekharuddin，“深度神经网络在语音和视觉系统中的调查，”
    *神经计算*，第 417 卷，第 302–321 页，2020 年 12 月。'
- en: '[74] Q. Zhang, X. Wang, Y. Wu, H. Zhou, and S.-C. Zhu, “Interpretable CNNs
    for Object Classification,” vol. 43, no. 10, pp. 3416–3431, Oct. 2021.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Q. Zhang, X. Wang, Y. Wu, H. Zhou 和 S.-C. Zhu，“可解释的 CNN 用于对象分类，”第 43 卷，第
    10 期，第 3416–3431 页，2021 年 10 月。'
- en: '[75] M. Kwabena Patrick, A. Felix Adekoya, A. Abra Mighty, and B. Y. Edward,
    “Capsule Networks – A survey,” *Journal of King Saud University - Computer and
    Information Sciences*, Sep. 2019, doi: 10.1016/j.jksuci.2019.09.014.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Kwabena Patrick, A. Felix Adekoya, A. Abra Mighty 和 B. Y. Edward，“胶囊网络
    – 综述，” *国王沙乌地大学计算机与信息科学期刊*，2019 年 9 月，doi: 10.1016/j.jksuci.2019.09.014。'
- en: '[76] C. White *et al.*, “Neural Architecture Search: Insights from 1000 Papers,”
    Jan. 2023, doi: 10.48550/arxiv.2301.08727.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C. White *等*，“神经架构搜索：来自 1000 篇论文的见解，”2023 年 1 月，doi: 10.48550/arxiv.2301.08727。'
- en: '[77] I. J. Goodfellow *et al.*, “Generative Adversarial Networks,” *arXiv (Cornell
    University)*, Jun. 2014, doi: 10.48550/arxiv.1406.2661.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] I. J. Goodfellow *等*，“生成对抗网络，” *arXiv (康奈尔大学)*，2014 年 6 月，doi: 10.48550/arxiv.1406.2661。'
- en: '[78] X. Yuan, Z. Feng, M. Norton, and X. Li, “Generalized Batch Normalization:
    Towards Accelerating Deep Neural Networks,” *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 33, pp. 1682–1689, Jul. 2019, doi: 10.1609/aaai.v33i01.33011682.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] X. Yuan, Z. Feng, M. Norton 和 X. Li，“广义批归一化：加速深度神经网络的探索，” *AAAI 人工智能会议论文集*，第
    33 卷，第 1682–1689 页，2019 年 7 月，doi: 10.1609/aaai.v33i01.33011682。'
- en: '[79] Y. Wang, G. Wang, C. Chen, and Z. Pan, “Multi-scale dilated convolution
    of convolutional neural network for image denoising,” *Multimedia Tools and Applications*,
    Feb. 2019, doi: 10.1007/s11042-019-7377-y.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y. Wang, G. Wang, C. Chen 和 Z. Pan，“用于图像去噪的多尺度膨胀卷积卷积神经网络，” *多媒体工具与应用*，2019
    年 2 月，doi: 10.1007/s11042-019-7377-y。'
- en: '[80] J. Huang *et al.*, “Speed/accuracy trade-offs for modern convolutional
    object detectors,” *arXiv (Cornell University)*, Nov. 2016, doi: 10.48550/arxiv.1611.10012.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] J. Huang *等*，“现代卷积对象检测器的速度/准确性权衡，” *arXiv (康奈尔大学)*，2016 年 11 月，doi: 10.48550/arxiv.1611.10012。'
- en: '[81] H. Zhu, H. Zhang, and Y. Jin, “From federated learning to federated neural
    architecture search: a survey,” *Complex & Intelligent Systems*, Jan. 2021, doi:
    10.1007/s40747-020-00247-z.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H. Zhu, H. Zhang 和 Y. Jin，“从联邦学习到联邦神经架构搜索：综述，” *复杂与智能系统*，2021 年 1 月，doi:
    10.1007/s40747-020-00247-z。'
- en: '[82] O. N. Oyelade and A. E. Ezugwu, “A bioinspired neural architecture search
    based convolutional neural network for breast cancer detection using histopathology
    images,” *Scientific Reports*, vol. 11, no. 1, Oct. 2021, doi: 10.1038/s41598-021-98978-7.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] O. N. Oyelade 和 A. E. Ezugwu，“基于生物启发的神经架构搜索卷积神经网络用于乳腺癌检测的组织病理图像，” *科学报告*，第
    11 卷，第 1 期，2021 年 10 月，doi: 10.1038/s41598-021-98978-7。'
- en: '[83] F. Zhan, H. Zhu, and S. Lu, “Spatial Fusion GAN for Image Synthesis,”
    Jun. 2019, doi: 10.1109/cvpr.2019.00377.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] F. Zhan, H. Zhu 和 S. Lu，“用于图像合成的空间融合GAN，” 2019年6月，doi: 10.1109/cvpr.2019.00377。'
- en: '[84] A. Singh *et al.*, “Neural Style Transfer: A Critical Review,” *IEEE Access*,
    vol. 9, pp. 131583–131613, 2021, doi: 10.1109/access.2021.3112996.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A. Singh *et al.*，“神经风格迁移：批判性综述，” *IEEE Access*，第9卷，第131583–131613页，2021年，doi:
    10.1109/access.2021.3112996。'
- en: '[85] A. Vaswani *et al.*, “Attention Is All You Need,” *arXiv.org*, Dec. 05,
    2017.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. Vaswani *et al.*，“注意力即是你所需，” *arXiv.org*，2017年12月5日。'
- en: '[86] K. Han *et al.*, ”A Survey on Vision Transformer,” in *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, vol. 45, no. 1, pp. 87-110, 1 Jan.
    2023, doi: 10.1109/TPAMI.2022.3152247.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] K. Han *et al.*，“视觉变换器综述，” *IEEE Transactions on Pattern Analysis and
    Machine Intelligence*，第45卷，第1期，第87-110页，2023年1月1日，doi: 10.1109/TPAMI.2022.3152247。'
- en: '[87] A. Khan *et al.*, “A survey of the Vision Transformers and its CNN-Transformer
    based Variants,” *arXiv.org*, Aug. 08, 2023.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] A. Khan *et al.*，“视觉变换器及其基于CNN-变换器的变体综述，” *arXiv.org*，2023年8月8日。'
- en: '[88] J. Memon, M. Sami, R. A. Khan and M. Uddin, ”Handwritten Optical Character
    Recognition (OCR): A Comprehensive Systematic Literature Review (SLR),” in *IEEE
    Access*, vol. 8, pp. 142642-142668, 2020, doi: 10.1109/ACCESS.2020.3012542.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] J. Memon, M. Sami, R. A. Khan 和 M. Uddin，“手写光学字符识别（OCR）：全面的系统文献综述（SLR），”
    *IEEE Access*，第8卷，第142642-142668页，2020年，doi: 10.1109/ACCESS.2020.3012542。'
- en: '[89] Tal Ridnik, E. Ben-Baruch, A. Noy, and Lihi Zelnik-Manor, “ImageNet-21K
    Pretraining for the Masses,” *arXiv (Cornell University)*, Apr. 2021.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Tal Ridnik, E. Ben-Baruch, A. Noy 和 Lihi Zelnik-Manor，“ImageNet-21K 大规模预训练，”
    *arXiv (Cornell University)*，2021年4月。'
- en: '[90] M. Hnewa and H. Radha, ”Object Detection Under Rainy Conditions for Autonomous
    Vehicles: A Review of State-of-the-Art and Emerging Techniques,” in *IEEE Signal
    Processing Magazine*, vol. 38, no. 1, pp. 53-67, Jan. 2021, doi: 10.1109/MSP.2020.2984801.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Hnewa 和 H. Radha，“自主车辆在雨天条件下的目标检测：前沿和新兴技术综述，” *IEEE Signal Processing
    Magazine*，第38卷，第1期，第53-67页，2021年1月，doi: 10.1109/MSP.2020.2984801。'
- en: '[91] M. Elhoseny, “Multi-object Detection and Tracking (MODT) Machine Learning
    Model for Real-Time Video Surveillance Systems,” *Circuits, Systems, and Signal
    Processing*, Aug. 2019, doi: 10.1007/s00034-019-01234-7.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Elhoseny, “多目标检测与跟踪（MODT）机器学习模型用于实时视频监控系统，” *Circuits, Systems, and
    Signal Processing*，2019年8月，doi: 10.1007/s00034-019-01234-7。'
- en: '[92] S. Thakur and A. Kumar, “X-ray and CT-scan-based automated detection and
    classification of covid-19 using convolutional neural networks (CNN),” *Biomedical
    Signal Processing and Control*, vol. 69, p. 102920, Aug. 2021, doi: 10.1016/j.bspc.2021.102920.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Thakur 和 A. Kumar，“基于X射线和CT扫描的COVID-19自动检测与分类，使用卷积神经网络（CNN），” *Biomedical
    Signal Processing and Control*，第69卷，第102920页，2021年8月，doi: 10.1016/j.bspc.2021.102920。'
- en: '[93] F. Ramzan, M. U. G. Khan, S. Iqbal, T. Saba and A. Rehman, ”Volumetric
    Segmentation of Brain Regions From MRI Scans Using 3D Convolutional Neural Networks,”
    in *IEEE Access*, vol. 8, pp. 103697-103709, 2020, doi: 10.1109/ACCESS .2020.2998901.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] F. Ramzan, M. U. G. Khan, S. Iqbal, T. Saba 和 A. Rehman，“利用3D卷积神经网络对MRI扫描的脑区进行体积分割，”
    *IEEE Access*，第8卷，第103697-103709页，2020年，doi: 10.1109/ACCESS.2020.2998901。'
- en: '[94] T.-Y. Lin *et al.*, “Microsoft COCO: Common Objects in Context,” *arXiv
    (Cornell University)*, May 2014, doi: 10.48550/arxiv.1405.0312.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] T.-Y. Lin *et al.*，“Microsoft COCO：上下文中的常见对象，” *arXiv (Cornell University)*，2014年5月，doi:
    10.48550/arxiv.1405.0312。'
- en: '[95] J. Deng *et al.*, ”ImageNet: A large-scale hierarchical image database,”
    *2009 IEEE Conference on Computer Vision and Pattern Recognition*, Miami, FL,
    USA, 2009, pp. 248-255, doi: 10.1109/CVPR.2009.5206848.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Deng *et al.*，“ImageNet：大规模分层图像数据库，” *2009 IEEE Conference on Computer
    Vision and Pattern Recognition*，美国佛罗里达州迈阿密，2009年，第248-255页，doi: 10.1109/CVPR.2009.5206848。'
- en: '[96] Y. Li *et al.*, ”Unified multimodal transformers for joint video-language
    modeling,” 2021.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Y. Li *et al.*，“统一的多模态变换器用于联合视频-语言建模，” 2021年。'
- en: '[97] H. Tan and M. Bansal, “LXMERT: Learning Cross-Modality Encoder Representations
    from Transformers,” 2019.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. Tan 和 M. Bansal，“LXMERT：从变换器中学习跨模态编码器表示，” 2019年。'
- en: '[98] A. Radford *et al.*, “Learning Transferable Visual Models From Natural
    Language Supervision,” 2021.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] A. Radford *et al.*，“从自然语言监督中学习可转移的视觉模型，” 2021年。'
- en: '[99] J.-B. Alayrac *et al.*, “Self-Supervised Multimodal Versatile Networks,”
    2022.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] J.-B. Alayrac *et al.*，“自监督多模态通用网络，” 2022年。'
- en: '[100] G. O. Young, “Synthetic structure of industrial plastics,” in Plastics,
    vol. 3, Polymers of Hexadromicon, J. Peters, Ed., 2^(nd) ed. New York, NY, USA:
    McGraw-Hill, 1964, pp. 15-64. [Online]. Available: http://www.bookref.com.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] G. O. Young，“工业塑料的合成结构，”收录于《塑料》，第3卷，Hexadromicon聚合物，J. Peters主编，第二版。美国纽约：McGraw-Hill，1964年，pp.
    15-64。 [在线]. 可用网址: http://www.bookref.com。'
- en: '[101] Y. Zhou and Oncel Tuzel, “VoxelNet: End-to-End Learning for Point Cloud
    Based 3D Object Detection,” Nov. 2017, doi: 10.48550/arxiv.1711.06396.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Y. Zhou 和 Oncel Tuzel，“VoxelNet：基于点云的3D目标检测的端到端学习，”2017年11月，doi: 10.48550/arxiv.1711.06396。'
- en: '[102] Y. He and L. Xiao, ”Structured Pruning for Deep Convolutional Neural
    Networks: A Survey,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    doi: 10.1109/TPAMI.2023.3334614.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Y. He 和 L. Xiao，“深度卷积神经网络的结构化剪枝：综述，”《IEEE模式分析与机器智能汇刊》，doi: 10.1109/TPAMI.2023.3334614。'
- en: '[103] I. Oguntola, S. Olubeko and C. Sweeney, ”SlimNets: An Exploration of
    Deep Model Compression and Acceleration,” 2018 IEEE High Performance extreme Computing
    Conference (HPEC), Waltham, MA, USA, 2018, pp. 1-6, doi: 10.1109/HPEC.2018.8547604.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] I. Oguntola, S. Olubeko 和 C. Sweeney，“SlimNets：深度模型压缩与加速的探索，”2018 IEEE高性能极限计算会议（HPEC），美国马萨诸塞州沃尔瑟姆，2018年，pp.
    1-6，doi: 10.1109/HPEC.2018.8547604。'
- en: '[104] T. Guo, T. Zhang, E. Lim, M. López-Benítez, F. Ma and L. Yu, ”A Review
    of Wavelet Analysis and Its Applications: Challenges and Opportunities,” in IEEE
    Access, vol. 10, pp. 58869-58903, 2022, doi: 10.1109/ACCESS.2022.3179517.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] T. Guo, T. Zhang, E. Lim, M. López-Benítez, F. Ma 和 L. Yu，“小波分析及其应用综述：挑战与机遇，”《IEEE
    Access》，第10卷，pp. 58869-58903，2022年，doi: 10.1109/ACCESS.2022.3179517。'
- en: '[105] G. Othman and D. Q. Zeebaree, “The Applications of Discrete Wavelet Transform
    in Image Processing: A Review”, jscdm, vol. 1, no. 2, pp. 31–43, Dec. 2020.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] G. Othman 和 D. Q. Zeebaree，“离散小波变换在图像处理中的应用：综述”，jscdm，第1卷，第2期，pp. 31–43，2020年12月。'
- en: '[106] A. Saxena, A. Khanna, and D. Gupta, “Emotion Recognition and Detection
    Methods: A Comprehensive Survey,” Journal of Artificial Intelligence and Systems,
    vol. 2, no. 1, pp. 53–79, 2020, doi: 10.33969/ais.2020.21005.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Saxena, A. Khanna 和 D. Gupta，“情感识别和检测方法：全面调查，”《人工智能与系统杂志》，第2卷，第1期，pp.
    53–79，2020年，doi: 10.33969/ais.2020.21005。'
- en: '[107] T. Williams and R. Li, ”Advanced Image Classification Using Wavelets
    and Convolutional Neural Networks,” 2016 15th IEEE International Conference on
    Machine Learning and Applications (ICMLA), Anaheim, CA, USA, 2016, pp. 233-239,
    doi: 10.1109/ICMLA.2016.0046.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] T. Williams 和 R. Li，“使用小波变换和卷积神经网络的高级图像分类，”2016年第15届IEEE国际机器学习与应用会议（ICMLA），美国加州安纳海姆，2016年，pp.
    233-239，doi: 10.1109/ICMLA.2016.0046。'
- en: '[108] P. Liu, H. Zhang, W. Lian and W. Zuo, ”Multi-Level Wavelet Convolutional
    Neural Networks,” in IEEE Access, vol. 7, pp. 74973-74985, 2019, doi: 10.1109/ACCESS.2019.2921451.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] P. Liu, H. Zhang, W. Lian 和 W. Zuo，“多层次小波卷积神经网络，”《IEEE Access》，第7卷，pp.
    74973-74985，2019年，doi: 10.1109/ACCESS.2019.2921451。'
- en: '[109] S. Fujieda, K. Takayama, and T. Hachisuka, “Wavelet Convolutional Neural
    Networks,” arXiv.org, May 20, 2018\. https://arxiv.org/abs/1805.08620 (accessed
    Nov. 08, 2023).'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] S. Fujieda, K. Takayama 和 T. Hachisuka，“小波卷积神经网络，”arXiv.org，2018年5月20日。
    https://arxiv.org/abs/1805.08620（访问日期：2023年11月8日）。'
- en: '[110] W. Zhang et al., ”A Wavelet-Based Asymmetric Convolution Network for
    Single Image Super-Resolution,” in IEEE Access, vol. 9, pp. 28976-28986, 2021,
    doi: 10.1109/ACCESS.2021.3058648.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] W. Zhang 等，“一种基于小波的非对称卷积网络用于单幅图像超分辨率，”《IEEE Access》，第9卷，pp. 28976-28986，2021年，doi:
    10.1109/ACCESS.2021.3058648。'
- en: '[111] Y. Wu, P. Qian and X. Zhang, ”Two-Level Wavelet-Based Convolutional Neural
    Network for Image Deblurring,” in IEEE Access, vol. 9, pp. 45853-45863, 2021,
    doi: 10.1109/ACCESS.2021.3067055.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Y. Wu, P. Qian 和 X. Zhang，“基于两级小波的卷积神经网络用于图像去模糊，”《IEEE Access》，第9卷，pp.
    45853-45863，2021年，doi: 10.1109/ACCESS.2021.3067055。'
- en: '[112] Z. Tao, T. Wei and J. Li, ”Wavelet Multi-Level Attention Capsule Network
    for Texture Classification,” in IEEE Signal Processing Letters, vol. 28, pp. 1215-1219,
    2021, doi: 10.1109/LSP.2021.3088052.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Z. Tao, T. Wei 和 J. Li，“小波多层注意力胶囊网络用于纹理分类，”《IEEE信号处理快报》，第28卷，pp. 1215-1219，2021年，doi:
    10.1109/LSP.2021.3088052。'
- en: '[113] M. C. Kim, J. H. Park, and M. H. Sunwoo, ”Multilevel Feature Extraction
    Using Wavelet Attention for Deep Joint Demosaicking and Denoising,” in IEEE Access,
    vol. 10, pp. 77099-77109, 2022, doi: 10.1109/ACCESS.2022.3192451.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. C. Kim, J. H. Park 和 M. H. Sunwoo，“使用小波注意力进行多级特征提取以实现深度联合去马赛克和去噪，”《IEEE
    Access》，第10卷，pp. 77099-77109，2022年，doi: 10.1109/ACCESS.2022.3192451。'
- en: '[114] Z. Xie, Z. Wen, J. Liu, Z. Liu, X. Wu, and M. Tan, “Deep Transferring
    Quantization,” Lecture Notes in Computer Science, pp. 625–642, Jan. 2020, doi:
    https://doi.org/10.1007/978-3-030-58598-3_37.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Z. Xie, Z. Wen, J. Liu, Z. Liu, X. Wu 和 M. Tan, “深度迁移量化，” 计算机科学讲义，页 625–642，2020
    年 1 月，doi: https://doi.org/10.1007/978-3-030-58598-3_37。'
- en: '[115] H. Li et al., “Hard Sample Matters a Lot in Zero-Shot Quantization,”
    Jun. 2023, doi: 10.1109/cvpr52729.2023.02339.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] H. Li 等, “硬样本在零样本量化中的重要性，” 2023 年 6 月，doi: 10.1109/cvpr52729.2023.02339。'
- en: '[116] M. Lin et al., “HRank: Filter Pruning Using High-Rank Feature Map,” Jun.
    2020, doi: 10.1109/cvpr42600.2020.00160.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] M. Lin 等, “HRank: 使用高阶特征图的滤波器剪枝，” 2020 年 6 月，doi: 10.1109/cvpr42600.2020.00160。'
- en: '[117] Moez Krichen, “Convolutional Neural Networks: A Survey,” Computers, vol.
    12, no. 8, pp. 151–151, Jul. 2023, doi: 10.3390/computers12080151'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Moez Krichen, “卷积神经网络：一项调查，” Computers，卷 12，第 8 期，页 151–151，2023 年 7
    月，doi: 10.3390/computers12080151。'
- en: '[118] L. Alzubaidi et al., “Review of deep learning: concepts, CNN architectures,
    challenges, applications, future directions,” Journal of Big Data, vol. 8, no.
    1, Mar. 2021, doi: 10.1186/s40537-021-00444-8'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] L. Alzubaidi 等, “深度学习综述：概念、CNN 架构、挑战、应用、未来方向，” Journal of Big Data，卷
    8，第 1 期，2021 年 3 月，doi: 10.1186/s40537-021-00444-8。'
- en: '[119] Z. Li, F. Liu, W. Yang, S. Peng and J. Zhou, ”A Survey of Convolutional
    Neural Networks: Analysis, Applications, and Prospects,” in IEEE Transactions
    on Neural Networks and Learning Systems, vol. 33, no. 12, pp. 6999-7019, Dec.
    2022, doi: 10.1109/TNNLS.2021.3084827.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. Li, F. Liu, W. Yang, S. Peng 和 J. Zhou, “卷积神经网络调查：分析、应用与前景，” 发表在 IEEE
    Transactions on Neural Networks and Learning Systems，卷 33，第 12 期，页 6999-7019，2022
    年 12 月，doi: 10.1109/TNNLS.2021.3084827。'
- en: '[120] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi, “A survey of the recent
    architectures of deep convolutional neural networks,” Artificial Intelligence
    Review, vol. 53, Apr. 2020, doi: 10.1007/s10462-020-09825-6'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Khan, A. Sohail, U. Zahoora 和 A. S. Qureshi, “深度卷积神经网络的近期架构调查，” Artificial
    Intelligence Review，卷 53，2020 年 4 月，doi: 10.1007/s10462-020-09825-6。'
- en: '[121] S. Xu, A. Huang, L. Chen and B. Zhang, ”Convolutional Neural Network
    Pruning: A Survey,” 2020 39th Chinese Control Conference (CCC), Shenyang, China,
    2020, pp. 7458-7463, doi: 10.23919/CCC50068.2020.9189610.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] S. Xu, A. Huang, L. Chen 和 B. Zhang, “卷积神经网络剪枝：一项调查，” 2020 第 39 届中国控制会议（CCC），中国沈阳，2020
    年，页 7458-7463，doi: 10.23919/CCC50068.2020.9189610。'
- en: '[122] Y. He and L. Xiao, ”Structured Pruning for Deep Convolutional Neural
    Networks: A Survey,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    doi: 10.1109/TPAMI.2023.3334614.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Y. He 和 L. Xiao, “深度卷积神经网络的结构化剪枝：一项调查，” 发表在 IEEE Transactions on Pattern
    Analysis and Machine Intelligence，doi: 10.1109/TPAMI.2023.3334614。'
- en: '[123] U. Kulkarni, S. S. Hallad, A. Patil, T. Bhujannavar, S. Kulkarni and
    S. M. Meena, ”A Survey on Filter Pruning Techniques for Optimization of Deep Neural
    Networks,” 2022 Sixth International Conference on I-SMAC (IoT in Social, Mobile,
    Analytics and Cloud) (I-SMAC), Dharan, Nepal, 2022, pp. 610-617, doi: 10.1109/I-SMAC55078.2022.9987264.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] U. Kulkarni, S. S. Hallad, A. Patil, T. Bhujannavar, S. Kulkarni 和 S.
    M. Meena, “深度神经网络优化的滤波器剪枝技术调查，” 2022 第六届 I-SMAC 国际会议（社交、移动、分析与云中的物联网）（I-SMAC），尼泊尔达朗，2022
    年，页 610-617，doi: 10.1109/I-SMAC55078.2022.9987264。'
- en: '[124] S. Vadera and S. Ameen, ”Methods for Pruning Deep Neural Networks,” in
    IEEE Access, vol. 10, pp. 63280-63300, 2022, doi: 10.1109/ACCESS.2022.3182659.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] S. Vadera 和 S. Ameen, “深度神经网络的剪枝方法，” 发表在 IEEE Access，卷 10，页 63280-63300，2022
    年，doi: 10.1109/ACCESS.2022.3182659。'
- en: '[125] A. R. Aswani, C. R and A. P. James, ”Unstructured Weight Pruning in Variability-Aware
    Memristive Crossbar Neural Networks,” 2022 IEEE International Symposium on Circuits
    and Systems (ISCAS), Austin, TX, USA, 2022, pp. 3458-3462, doi: 10.1109/ISCAS48785.2022.9937284.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] A. R. Aswani, C. R 和 A. P. James, “在变异感知忆阻交叉条形神经网络中进行非结构化权重剪枝，” 2022
    IEEE 国际电路与系统研讨会（ISCAS），美国德克萨斯州奥斯汀，2022 年，页 3458-3462，doi: 10.1109/ISCAS48785.2022.9937284。'
- en: '[126] A. Liang, H. Zhang, H. Hua and W. Chen, ”To Drop or to Select: Reduce
    the Negative Effects of Disturbance Features for Point Cloud Classification From
    an Interpretable Perspective,” in IEEE Access, vol. 11, pp. 36184-36202, 2023,
    doi: 10.1109/ACCESS.2023.3266340.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] A. Liang, H. Zhang, H. Hua 和 W. Chen, “是丢弃还是选择：从可解释的角度减少扰动特征对点云分类的负面影响，”
    发表在 IEEE Access，卷 11，页 36184-36202，2023 年，doi: 10.1109/ACCESS.2023.3266340。'
- en: '[127] Y. Peng et al., ”Sparse-to-Dense Multi-Encoder Shape Completion of Unstructured
    Point Cloud,” in IEEE Access, vol. 8, pp. 30969-30978, 2020, doi: 10.1109/ACCESS.2020.2973003.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Y. Peng 等, “稀疏到密集的多编码器未结构化点云形状补全，” 发表在 IEEE Access，卷 8，页 30969-30978，2020
    年，doi: 10.1109/ACCESS.2020.2973003。'
- en: '[128] A. Zhang, S. Li, J. Wu, S. Li and B. Zhang, ”Exploring Semantic Information
    Extraction From Different Data Forms in 3D Point Cloud Semantic Segmentation,”
    in IEEE Access, vol. 11, pp. 61929-61949, 2023, doi: 10.1109/ACCESS.2023.3287940.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Zhang, S. Li, J. Wu, S. Li 和 B. Zhang，“探索从不同数据形式中提取语义信息在3D点云语义分割中的应用，”
    载于 IEEE Access，第11卷，第61929-61949页，2023年，doi: 10.1109/ACCESS.2023.3287940。'
- en: '[129] Y. Wang, X. Tang, and C. Yue, ”Enhancing the Local Graph Semantic Feature
    for 3D Point Cloud Classification and Segmentation,” in IEEE Access, vol. 10,
    pp. 74620-74628, 2022, doi: 10.1109/ACCESS.2022.3190966.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. Wang, X. Tang 和 C. Yue，“增强局部图语义特征用于3D点云分类和分割，” 载于 IEEE Access，第10卷，第74620-74628页，2022年，doi:
    10.1109/ACCESS.2022.3190966。'
- en: '[130] J. Zeng, D. Wang, and P. Chen, ”A Survey on Transformers for Point Cloud
    Processing: An Updated Overview,” in IEEE Access, vol. 10, pp. 86510-86527, 2022,
    doi: 10.1109/ACCESS.2022.3198999.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] J. Zeng, D. Wang 和 P. Chen，“点云处理中的变换器调查：更新概述，” 载于 IEEE Access，第10卷，第86510-86527页，2022年，doi:
    10.1109/ACCESS.2022.3198999。'
- en: '[131] “Caffe — Deep Learning Framework,” Berkeleyvision.org, 2012\. https://caffe.berkeleyvision.org/'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] “Caffe — 深度学习框架，” Berkeleyvision.org，2012\. https://caffe.berkeleyvision.org/'
- en: '[132] PyTorch, “PyTorch,” Pytorch.org, 2023\. https://pytorch.org/'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] PyTorch，“PyTorch，” Pytorch.org，2023\. https://pytorch.org/'
- en: '[133] TensorFlow, “TensorFlow,” TensorFlow, 2019\. https://www.tensorflow.org/'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] TensorFlow，“TensorFlow，” TensorFlow，2019\. https://www.tensorflow.org/'
- en: '[134] Keras, “Home - Keras Documentation,” Keras.io, 2019\. https://keras.io/'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Keras，“首页 - Keras文档，” Keras.io，2019\. https://keras.io/'
- en: '[135] OpenCV, “OpenCV library,” Opencv.org, 2019\. https://opencv.org/'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] OpenCV，“OpenCV库，” Opencv.org，2019\. https://opencv.org/'
- en: '[136] “apache/mxnet,” GitHub, Jan. 09, 2024\. https://github.com/apache/mxnet
    (accessed Jan. 09, 2024).'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] “apache/mxnet，” GitHub，2024年1月9日\. https://github.com/apache/mxnet（访问于2024年1月9日）。'
- en: '[137] “Chainer: A flexible framework for neural networks,” Chainer. https://chainer.org/'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] “Chainer：一个灵活的神经网络框架，” Chainer。 https://chainer.org/'
- en: '[138] “Eclipse DeepLearning4J,” deeplearning4j.konduit.ai. https://deeplearning4j.konduit.ai/'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] “Eclipse DeepLearning4J，” deeplearning4j.konduit.ai。 https://deeplearning4j.konduit.ai/'
- en: '[139] F. Ullah, I. Ullah, R. U. Khan, S. Khan, K. Khan and G. Pau, ”Conventional
    to Deep Ensemble Methods for Hyperspectral Image Classification: A Comprehensive
    Survey,” in IEEE Journal of Selected Topics in Applied Earth Observations and
    Remote Sensing, doi: 10.1109/JSTARS.2024.3353551.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] F. Ullah, I. Ullah, R. U. Khan, S. Khan, K. Khan 和 G. Pau，“从传统到深度集成方法的高光谱图像分类：全面调查，”
    载于 IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing，doi:
    10.1109/JSTARS.2024.3353551。'
- en: '[140] V. A. Ashwath, O. K. Sikha and R. Benitez, ”TS-CNN: A Three-Tier Self-Interpretable
    CNN for Multi-Region Medical Image Classification,” in IEEE Access, vol. 11, pp.
    78402-78418, 2023, doi: 10.1109/ACCESS.2023.3299850.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] V. A. Ashwath, O. K. Sikha 和 R. Benitez，“TS-CNN：一种三层自解释CNN用于多区域医学图像分类，”
    载于 IEEE Access，第11卷，第78402-78418页，2023年，doi: 10.1109/ACCESS.2023.3299850。'
- en: '[141] X. He and Y. Chen, ”Optimized Input for CNN-Based Hyperspectral Image
    Classification Using Spatial Transformer Network,” in IEEE Geoscience and Remote
    Sensing Letters, vol. 16, no. 12, pp. 1884-1888, Dec. 2019, doi: 10.1109/LGRS.2019.2911322.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] X. He 和 Y. Chen，“使用空间变换网络优化CNN基于高光谱图像分类的输入，” 载于 IEEE Geoscience and Remote
    Sensing Letters，第16卷，第12期，第1884-1888页，2019年12月，doi: 10.1109/LGRS.2019.2911322。'
- en: '[142] Y. Pei, Y. Huang, Q. Zou, X. Zhang and S. Wang, ”Effects of Image Degradation
    and Degradation Removal to CNN-Based Image Classification,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 43, no. 4, pp. 1239-1253, 1
    April 2021, doi: 10.1109/TPAMI.2019.2950923.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Y. Pei, Y. Huang, Q. Zou, X. Zhang 和 S. Wang，“图像退化及退化去除对基于CNN的图像分类的影响，”
    载于 IEEE Transactions on Pattern Analysis and Machine Intelligence，第43卷，第4期，第1239-1253页，2021年4月1日，doi:
    10.1109/TPAMI.2019.2950923。'
- en: '[143] L. Song et al., ”A Deep Multi-Modal CNN for Multi-Instance Multi-Label
    Image Classification,” in IEEE Transactions on Image Processing, vol. 27, no.
    12, pp. 6025-6038, Dec. 2018, doi: 10.1109/TIP.2018.2864920.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] L. Song 等，“一种深度多模态CNN用于多实例多标签图像分类，” 载于 IEEE Transactions on Image Processing，第27卷，第12期，第6025-6038页，2018年12月，doi:
    10.1109/TIP.2018.2864920。'
- en: '[144] C. Shi, L. Fang and H. Shen, ”Convolutional Neural Networks With Class-Driven
    Loss for Multiscale VHR Remote Sensing Image Classification,” in IEEE Access,
    vol. 8, pp. 149162-149175, 2020, doi: 10.1109/ACCESS.2020.3014975.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] C. Shi, L. Fang 和 H. Shen，“具有类别驱动损失的卷积神经网络用于多尺度VHR遥感图像分类，” 载于 IEEE Access，第8卷，第149162-149175页，2020年，doi:
    10.1109/ACCESS.2020.3014975。'
- en: '[145] D. Wang, J. Zhang, B. Du, L. Zhang and D. Tao, ”DCN-T: Dual Context Network
    With Transformer for Hyperspectral Image Classification,” in IEEE Transactions
    on Image Processing, vol. 32, pp. 2536-2551, 2023, doi: 10.1109/TIP.2023.3270104.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] D. Wang, J. Zhang, B. Du, L. Zhang 和 D. Tao, ”DCN-T：用于高光谱图像分类的双上下文网络与变换器，”
    见《IEEE 图像处理汇刊》，第32卷，pp. 2536-2551，2023年，doi: 10.1109/TIP.2023.3270104。'
- en: '[146] S. Khan, M. Sajjad, T. Hussain, A. Ullah and A. S. Imran, ”A Review on
    Traditional Machine Learning and Deep Learning Models for WBCs Classification
    in Blood Smear Images,” in IEEE Access, vol. 9, pp. 10657-10673, 2021, doi: 10.1109/ACCESS.2020.3048172.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] S. Khan, M. Sajjad, T. Hussain, A. Ullah 和 A. S. Imran, ”关于传统机器学习和深度学习模型在血液涂片图像中白细胞分类的综述，”
    见《IEEE Access》，第9卷，pp. 10657-10673，2021年，doi: 10.1109/ACCESS.2020.3048172。'
- en: '[147] S. A. H. Minoofam, A. Bastanfard and M. R. Keyvanpour, ”TRCLA: A Transfer
    Learning Approach to Reduce Negative Transfer for Cellular Learning Automata,”
    in IEEE Transactions on Neural Networks and Learning Systems, vol. 34, no. 5,
    pp. 2480-2489, May 2023, doi: 10.1109/TNNLS.2021.3106705.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. A. H. Minoofam, A. Bastanfard 和 M. R. Keyvanpour, ”TRCLA：一种减少负迁移的迁移学习方法用于细胞学习自动机，”
    见《IEEE 神经网络与学习系统汇刊》，第34卷，第5期，pp. 2480-2489，2023年5月，doi: 10.1109/TNNLS.2021.3106705。'
- en: '[148] J. Hao, ”Deep learning-based medical image analysis with explainable
    transfer learning,” 2023 International Conference on Computer Engineering and
    Distance Learning (CEDL), Shanghai, China, 2023, pp. 106-109, doi: 10.1109/CEDL60560.2023.00029.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Hao, ”基于深度学习的医学图像分析与可解释的迁移学习，” 2023 国际计算机工程与远程学习会议（CEDL），中国上海，2023年，pp.
    106-109，doi: 10.1109/CEDL60560.2023.00029。'
- en: '[149] L. Shao, F. Zhu and X. Li, ”Transfer Learning for Visual Categorization:
    A Survey,” in IEEE Transactions on Neural Networks and Learning Systems, vol.
    26, no. 5, pp. 1019-1034, May 2015, doi: 10.1109/TNNLS.2014.2330900.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] L. Shao, F. Zhu 和 X. Li, ”视觉分类的迁移学习：一项综述，” 见《IEEE 神经网络与学习系统汇刊》，第26卷，第5期，pp.
    1019-1034，2015年5月，doi: 10.1109/TNNLS.2014.2330900。'
- en: '[150] E. Chalmers, E. B. Contreras, B. Robertson, A. Luczak and A. Gruber,
    ”Learning to Predict Consequences as a Method of Knowledge Transfer in Reinforcement
    Learning,” in IEEE Transactions on Neural Networks and Learning Systems, vol.
    29, no. 6, pp. 2259-2270, June 2018, doi: 10.1109/TNNLS.2017.2690910.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] E. Chalmers, E. B. Contreras, B. Robertson, A. Luczak 和 A. Gruber, ”学习预测后果作为强化学习中的知识迁移方法，”
    见《IEEE 神经网络与学习系统汇刊》，第29卷，第6期，pp. 2259-2270，2018年6月，doi: 10.1109/TNNLS.2017.2690910。'
- en: '[151] T. V. Phan, S. Sultana, T. G. Nguyen and T. Bauschert, ”$Q$ - TRANSFER:
    A Novel Framework for Efficient Deep Transfer Learning in Networking,” 2020 International
    Conference on Artificial Intelligence in Information and Communication (ICAIIC),
    Fukuoka, Japan, 2020, pp. 146-151, doi: 10.1109/ICAIIC48513.2020.9065240.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] T. V. Phan, S. Sultana, T. G. Nguyen 和 T. Bauschert, ”$Q$ - TRANSFER：一种用于网络中高效深度迁移学习的新框架，”
    2020 国际人工智能与信息通信会议（ICAIIC），日本福冈，2020年，pp. 146-151，doi: 10.1109/ICAIIC48513.2020.9065240。'
- en: '[152] T. T. Chungath, A. M. Nambiar and A. Mittal, ”Transfer Learning and Few-Shot
    Learning Based Deep Neural Network Models for Underwater Sonar Image Classification
    With a Few Samples,” in IEEE Journal of Oceanic Engineering, doi: 10.1109/JOE.2022.3221127.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] T. T. Chungath, A. M. Nambiar 和 A. Mittal, ”基于迁移学习和少样本学习的深度神经网络模型用于水下声纳图像分类，”
    见《IEEE 海洋工程期刊》，doi: 10.1109/JOE.2022.3221127。'
- en: '[153] A. M. Nagib, H. Abou-zeid and H. S. Hassanein, ”Safe and Accelerated
    Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach,”
    in IEEE Journal on Selected Areas in Communications, doi: 10.1109/JSAC.2023.3336191.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] A. M. Nagib, H. Abou-zeid 和 H. S. Hassanein, ”基于深度强化学习的O-RAN切片安全加速：一种混合迁移学习方法，”
    见《IEEE 选择性通讯领域期刊》，doi: 10.1109/JSAC.2023.3336191。'
- en: '[154] M. Biehler, Y. Sun, S. Kode, J. Li and J. Shi, ”PLURAL: 3D Point Cloud
    Transfer Learning via Contrastive Learning With Augmentations,” in IEEE Transactions
    on Automation Science and Engineering, doi: 10.1109/TASE.2023.3345807.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] M. Biehler, Y. Sun, S. Kode, J. Li 和 J. Shi, ”PLURAL：通过对比学习与增强进行的3D点云迁移学习，”
    见《IEEE 自动化科学与工程汇刊》，doi: 10.1109/TASE.2023.3345807。'
- en: '[155] H. Li, Z. Wang, C. Lan, P. Wu and N. Zeng, ”A Novel Dynamic Multiobjective
    Optimization Algorithm With Non-Inductive Transfer Learning Based on Multi-Strategy
    Adaptive Selection,” in IEEE Transactions on Neural Networks and Learning Systems,
    doi: 10.1109/TNNLS.2023.3295461.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] H. Li, Z. Wang, C. Lan, P. Wu 和 N. Zeng, ”一种基于多策略自适应选择的非归纳迁移学习的新型动态多目标优化算法，”
    见《IEEE 神经网络与学习系统汇刊》，doi: 10.1109/TNNLS.2023.3295461。'
- en: '[156] H. Chen, H. Luo, B. Huang, B. Jiang and O. Kaynak, ”Transfer Learning-Motivated
    Intelligent Fault Diagnosis Designs: A Survey, Insights, and Perspectives,” in
    IEEE Transactions on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3290974.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] H. Chen, H. Luo, B. Huang, B. Jiang 和 O. Kaynak，”基于转移学习的智能故障诊断设计：综述、洞察和观点，”
    发表在 IEEE Transactions on Neural Networks and Learning Systems，doi: 10.1109/TNNLS.2023.3290974。'
- en: '[157] H. S. Mputu, A. Abdel-Mawgood, A. Shimada and M. S. Sayed, ”Tomato Quality
    Classification based on Transfer Learning Feature Extraction and Machine Learning
    Algorithm Classifiers,” in IEEE Access, doi: 10.1109/ACCESS.2024.3352745.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] H. S. Mputu, A. Abdel-Mawgood, A. Shimada 和 M. S. Sayed，”基于转移学习特征提取和机器学习算法分类器的番茄质量分类，”
    发表在 IEEE Access，doi: 10.1109/ACCESS.2024.3352745。'
- en: '[158] T. Zhou et al., ”Cooperative Multi-Agent Transfer Learning with Coalition
    Pattern Decomposition,” in IEEE Transactions on Games, doi: 10.1109/TG.2023.3272386.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] T. Zhou 等，”基于联盟模式分解的合作多智能体转移学习，” 发表在 IEEE Transactions on Games，doi:
    10.1109/TG.2023.3272386。'
- en: '[159] L. He, Q. Wei, M. Gong, X. Yang and J. Wei, ”Transfer Learning Based
    Center-of-mass Positioning Methods for Cultural Relics,” in IEEE Access, doi:
    10.1109/ACCESS.2023.3349017.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] L. He, Q. Wei, M. Gong, X. Yang 和 J. Wei，”基于转移学习的质心定位方法用于文物，” 发表在 IEEE
    Access，doi: 10.1109/ACCESS.2023.3349017。'
- en: '[160] M. S. Azari, F. Flammini, S. Santini and M. Caporuscio, ”A Systematic
    Literature Review on Transfer Learning for Predictive Maintenance in Industry
    4.0,” in IEEE Access, vol. 11, pp. 12887-12910, 2023, doi: 10.1109/ACCESS.2023.3239784.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] M. S. Azari, F. Flammini, S. Santini 和 M. Caporuscio，”关于工业 4.0 中预测性维护的转移学习的系统文献综述，”
    发表在 IEEE Access，卷 11，第 12887-12910 页，2023年，doi: 10.1109/ACCESS.2023.3239784。'
- en: '[161] D. Onita, ”Active Learning Based on Transfer Learning Techniques for
    Text Classification,” in IEEE Access, vol. 11, pp. 28751-28761, 2023, doi: 10.1109/ACCESS.2023.3260771.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] D. Onita，”基于转移学习技术的主动学习用于文本分类，” 发表在 IEEE Access，卷 11，第 28751-28761 页，2023年，doi:
    10.1109/ACCESS.2023.3260771。'
- en: '[162] Q. Li et al., ”A multi-task learning based approach to biomedical entity
    relation extraction,” 2018 IEEE International Conference on Bioinformatics and
    Biomedicine (BIBM), Madrid, Spain, 2018, pp. 680-682, doi: 10.1109/BIBM.2018.8621284.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Q. Li 等，”一种基于多任务学习的生物医学实体关系提取方法，” 2018 IEEE 国际生物信息学与生物医学会议（BIBM），西班牙马德里，2018年，第
    680-682 页，doi: 10.1109/BIBM.2018.8621284。'
- en: '[163] H. Li and J. Qi, ”A Multi-Task Learning and Data Augmentation-Based Pose
    Estimation Algorithm,” 2023 8th International Conference on Information Systems
    Engineering (ICISE), Dalian, China, 2023, pp. 358-361, doi: 10.1109/ICISE60366.2023.00082.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] H. Li 和 J. Qi，”一种基于多任务学习和数据增强的姿态估计算法，” 2023年第八届国际信息系统工程会议（ICISE），中国大连，2023年，第
    358-361 页，doi: 10.1109/ICISE60366.2023.00082。'
- en: '[164] N. Jin, J. Wu, X. Ma, K. Yan and Y. Mo, ”Multi-Task Learning Model Based
    on Multi-Scale CNN and LSTM for Sentiment Classification,” in IEEE Access, vol.
    8, pp. 77060-77072, 2020, doi: 10.1109/ACCESS.2020.2989428.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] N. Jin, J. Wu, X. Ma, K. Yan 和 Y. Mo，”基于多尺度 CNN 和 LSTM 的多任务学习模型用于情感分类，”
    发表在 IEEE Access，卷 8，第 77060-77072 页，2020年，doi: 10.1109/ACCESS.2020.2989428。'
- en: '[165] S. Liu, F. Yang, F. Kang and J. Yang, ”A Multi-Task Learning Method for
    Weakly Supervised Sound Event Detection,” ICASSP 2022 - 2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore,
    2022, pp. 8802-8806, doi: 10.1109/ICASSP43922.2022.9746947.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. Liu, F. Yang, F. Kang 和 J. Yang，”一种用于弱监督声音事件检测的多任务学习方法，” 2022 IEEE
    国际声学、语音与信号处理会议（ICASSP），新加坡，新加坡，2022年，第 8802-8806 页，doi: 10.1109/ICASSP43922.2022.9746947。'
- en: '[166] X. Ouyang et al., ”A 3D-CNN and LSTM Based Multi-Task Learning Architecture
    for Action Recognition,” in IEEE Access, vol. 7, pp. 40757-40770, 2019, doi: 10.1109/ACCESS.2019.2906654.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] X. Ouyang 等，”基于 3D-CNN 和 LSTM 的多任务学习架构用于动作识别，” 发表在 IEEE Access，卷 7，第
    40757-40770 页，2019年，doi: 10.1109/ACCESS.2019.2906654。'
- en: '[167] L. Yunxiang and Z. Kexin, ”Design of Efficient Speech Emotion Recognition
    Based on Multi Task Learning,” in IEEE Access, vol. 11, pp. 5528-5537, 2023, doi:
    10.1109/ACCESS.2023.3237268.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] L. Yunxiang 和 Z. Kexin，”基于多任务学习的高效语音情感识别设计，” 发表在 IEEE Access，卷 11，第 5528-5537
    页，2023年，doi: 10.1109/ACCESS.2023.3237268。'
- en: '[168] Q. Zhou and Q. Zhao, ”Flexible Clustered Multi-Task Learning by Learning
    Representative Tasks,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 38, no. 2, pp. 266-278, 1 Feb. 2016, doi: 10.1109/TPAMI.2015.2452911.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Q. Zhou 和 Q. Zhao，”通过学习代表任务的灵活集群多任务学习，” 发表在 IEEE Transactions on Pattern
    Analysis and Machine Intelligence，卷 38，第 2 期，第 266-278 页，2016年2月1日，doi: 10.1109/TPAMI.2015.2452911。'
- en: '[169] Y. Yan, E. Ricci, R. Subramanian, G. Liu, O. Lanz and N. Sebe, ”A Multi-Task
    Learning Framework for Head Pose Estimation under Target Motion,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 38, no. 6, pp. 1070-1083, 1
    June 2016, doi: 10.1109/TPAMI.2015.2477843.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Y. Yan, E. Ricci, R. Subramanian, G. Liu, O. Lanz 和 N. Sebe, “一种用于目标运动下头部姿态估计的多任务学习框架，”
    发表在 IEEE 模式分析与机器智能汇刊，卷 38，第 6 期，页码 1070-1083，2016年6月1日，doi: 10.1109/TPAMI.2015.2477843。'
- en: '[170] A. -A. Liu, Y. -T. Su, W. -Z. Nie and M. Kankanhalli, ”Hierarchical Clustering
    Multi-Task Learning for Joint Human Action Grouping and Recognition,” in IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 1, pp.
    102-114, 1 Jan. 2017, doi: 10.1109/TPAMI.2016.2537337.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] A. -A. Liu, Y. -T. Su, W. -Z. Nie 和 M. Kankanhalli, “分层聚类多任务学习用于联合人体动作分组和识别，”
    发表在 IEEE 模式分析与机器智能汇刊，卷 39，第 1 期，页码 102-114，2017年1月1日，doi: 10.1109/TPAMI.2016.2537337。'
- en: '[171] Q. Chen, W. Liu and X. Yu, ”A Viewpoint Aware Multi-Task Learning Framework
    for Fine-Grained Vehicle Recognition,” in IEEE Access, vol. 8, pp. 171912-171923,
    2020, doi: 10.1109/ACCESS.2020.3024658.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Q. Chen, W. Liu 和 X. Yu, “一种视角感知的多任务学习框架用于细粒度车辆识别，” 发表在 IEEE Access，卷
    8，页码 171912-171923，2020年，doi: 10.1109/ACCESS.2020.3024658。'
- en: '[172] G. Buroni, B. Lebichot and G. Bontempi, ”AST-MTL: An Attention-Based
    Multi-Task Learning Strategy for Traffic Forecasting,” in IEEE Access, vol. 9,
    pp. 77359-77370, 2021, doi: 10.1109/ACCESS.2021.3083412.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] G. Buroni, B. Lebichot 和 G. Bontempi, “AST-MTL：一种基于注意力的多任务学习策略用于交通预测，”
    发表在 IEEE Access，卷 9，页码 77359-77370，2021年，doi: 10.1109/ACCESS.2021.3083412。'
- en: '[173] C. Ding, Z. Lu, S. Wang, R. Cheng and V. N. Boddeti, ”Mitigating Task
    Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable
    Primitives,” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), Vancouver, BC, Canada, 2023, pp. 7756-7765, doi: 10.1109/CVPR52729.2023.00749.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] C. Ding, Z. Lu, S. Wang, R. Cheng 和 V. N. Boddeti, “通过非学习原语的显式任务路由来缓解多任务学习中的任务干扰，”
    2023 IEEE/CVF 计算机视觉与模式识别会议（CVPR），加拿大不列颠哥伦比亚省温哥华，2023年，页码 7756-7765，doi: 10.1109/CVPR52729.2023.00749。'
- en: '[174] W. Choi and S. Im, ”Dynamic Neural Network for Multi-Task Learning Searching
    across Diverse Network Topologies,” 2023 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 3779-3788, doi:
    10.1109/CVPR52729.2023.00368.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] W. Choi 和 S. Im, “用于多任务学习的动态神经网络在多样网络拓扑中的搜索，” 2023 IEEE/CVF 计算机视觉与模式识别会议（CVPR），加拿大不列颠哥伦比亚省温哥华，2023年，页码
    3779-3788，doi: 10.1109/CVPR52729.2023.00368。'
- en: '[175] R. Ranjan, V. M. Patel and R. Chellappa, ”HyperFace: A Deep Multi-Task
    Learning Framework for Face Detection, Landmark Localization, Pose Estimation,
    and Gender Recognition,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 41, no. 1, pp. 121-135, 1 Jan. 2019, doi: 10.1109/TPAMI.2017.2781233.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] R. Ranjan, V. M. Patel 和 R. Chellappa, “HyperFace：一个深度多任务学习框架用于人脸检测、地标定位、姿态估计和性别识别，”
    发表在 IEEE 模式分析与机器智能汇刊，卷 41，第 1 期，页码 121-135，2019年1月1日，doi: 10.1109/TPAMI.2017.2781233。'
- en: '[176] H. Han, A. K. Jain, F. Wang, S. Shan and X. Chen, ”Heterogeneous Face
    Attribute Estimation: A Deep Multi-Task Learning Approach,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 40, no. 11, pp. 2597-2609,
    1 Nov. 2018, doi: 10.1109/TPAMI.2017.2738004.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] H. Han, A. K. Jain, F. Wang, S. Shan 和 X. Chen, “异质人脸属性估计：一种深度多任务学习方法，”
    发表在 IEEE 模式分析与机器智能汇刊，卷 40，第 11 期，页码 2597-2609，2018年11月1日，doi: 10.1109/TPAMI.2017.2738004。'
- en: '[177] G. He, Y. Huo, M. He, H. Zhang and J. Fan, ”A Novel Orthogonality Loss
    for Deep Hierarchical Multi-Task Learning,” in IEEE Access, vol. 8, pp. 67735-67744,
    2020, doi: 10.1109/ACCESS.2020.2985991.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] G. He, Y. Huo, M. He, H. Zhang 和 J. Fan, “一种新颖的正交性损失用于深度层次多任务学习，” 发表在
    IEEE Access，卷 8，页码 67735-67744，2020年，doi: 10.1109/ACCESS.2020.2985991。'
- en: '[178] Z. -Q. Zhao, P. Zheng, S. -T. Xu and X. Wu, ”Object Detection With Deep
    Learning: A Review,” in IEEE Transactions on Neural Networks and Learning Systems,
    vol. 30, no. 11, pp. 3212-3232, Nov. 2019, doi: 10.1109/TNNLS.2018.2876865.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Z. -Q. Zhao, P. Zheng, S. -T. Xu 和 X. Wu, “深度学习的物体检测：综述，” 发表在 IEEE 神经网络与学习系统汇刊，卷
    30，第 11 期，页码 3212-3232，2019年11月，doi: 10.1109/TNNLS.2018.2876865。'
- en: '[179] K. He, G. Gkioxari, P. Dollár and R. Girshick, ”Mask R-CNN,” in IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 2, pp.
    386-397, 1 Feb. 2020, doi: 10.1109/TPAMI.2018.2844175.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] K. He, G. Gkioxari, P. Dollár 和 R. Girshick, “Mask R-CNN，” 发表在 IEEE 模式分析与机器智能汇刊，卷
    42，第 2 期，页码 386-397，2020年2月1日，doi: 10.1109/TPAMI.2018.2844175。'
- en: '[180] R. Girshick, J. Donahue, T. Darrell and J. Malik, ”Region-Based Convolutional
    Networks for Accurate Object Detection and Segmentation,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 38, no. 1, pp. 142-158, 1 Jan.
    2016, doi: 10.1109/TPAMI.2015.2437384.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] R. Girshick, J. Donahue, T. Darrell 和 J. Malik，"基于区域的卷积网络用于准确的目标检测与分割"，发表于《IEEE
    Transactions on Pattern Analysis and Machine Intelligence》，第38卷，第1期，第142-158页，2016年1月1日，doi:
    10.1109/TPAMI.2015.2437384。'
- en: '[181] S. -H. Gao, M. -M. Cheng, K. Zhao, X. -Y. Zhang, M. -H. Yang and P. Torr,
    ”Res2Net: A New Multi-Scale Backbone Architecture,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 43, no. 2, pp. 652-662, 1 Feb. 2021, doi:
    10.1109/TPAMI.2019.2938758.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] S. -H. Gao, M. -M. Cheng, K. Zhao, X. -Y. Zhang, M. -H. Yang 和 P. Torr，"Res2Net：一种新的多尺度骨干网络架构"，发表于《IEEE
    Transactions on Pattern Analysis and Machine Intelligence》，第43卷，第2期，第652-662页，2021年2月1日，doi:
    10.1109/TPAMI.2019.2938758。'
- en: '[182] Z. Wu, J. Wen, Y. Xu, J. Yang, X. Li and D. Zhang, ”Enhanced Spatial
    Feature Learning for Weakly Supervised Object Detection,” in IEEE Transactions
    on Neural Networks and Learning Systems, vol. 35, no. 1, pp. 961-972, Jan. 2024,
    doi: 10.1109/TNNLS.2022.3178180.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Z. Wu, J. Wen, Y. Xu, J. Yang, X. Li 和 D. Zhang，"增强空间特征学习用于弱监督目标检测"，发表于《IEEE
    Transactions on Neural Networks and Learning Systems》，第35卷，第1期，第961-972页，2024年1月，doi:
    10.1109/TNNLS.2022.3178180。'
- en: '[183] L. Jiao et al., ”A Survey of Deep Learning-Based Object Detection,” in
    IEEE Access, vol. 7, pp. 128837-128868, 2019, doi: 10.1109/ACCESS.2019.2939201.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] L. Jiao 等，"基于深度学习的目标检测综述"，发表于《IEEE Access》，第7卷，第128837-128868页，2019年，doi:
    10.1109/ACCESS.2019.2939201。'
- en: '[184] J. Kang, S. Tariq, H. Oh and S. S. Woo, ”A Survey of Deep Learning-Based
    Object Detection Methods and Datasets for Overhead Imagery,” in IEEE Access, vol.
    10, pp. 20118-20134, 2022, doi: 10.1109/ACCESS.2022.3149052.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Kang, S. Tariq, H. Oh 和 S. S. Woo，"基于深度学习的目标检测方法及其数据集概述"，发表于《IEEE
    Access》，第10卷，第20118-20134页，2022年，doi: 10.1109/ACCESS.2022.3149052。'
- en: '[185] S. Hoque, M. Y. Arafat, S. Xu, A. Maiti and Y. Wei, ”A Comprehensive
    Review on 3D Object Detection and 6D Pose Estimation With Deep Learning,” in IEEE
    Access, vol. 9, pp. 143746-143770, 2021, doi: 10.1109/ACCESS.2021.3114399.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] S. Hoque, M. Y. Arafat, S. Xu, A. Maiti 和 Y. Wei，"基于深度学习的3D目标检测与6D姿态估计的综合评述"，发表于《IEEE
    Access》，第9卷，第143746-143770页，2021年，doi: 10.1109/ACCESS.2021.3114399。'
- en: '[186] Y. -L. Li and S. Wang, ”HAR-Net: Joint Learning of Hybrid Attention for
    Single-Stage Object Detection,” in IEEE Transactions on Image Processing, vol.
    29, pp. 3092-3103, 2020, doi: 10.1109/TIP.2019.2957850.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Y. -L. Li 和 S. Wang，"HAR-Net：单阶段目标检测的混合注意力联合学习"，发表于《IEEE Transactions
    on Image Processing》，第29卷，第3092-3103页，2020年，doi: 10.1109/TIP.2019.2957850。'
- en: '[187] Z. Yuan, X. Song, L. Bai, Z. Wang and W. Ouyang, ”Temporal-Channel Transformer
    for 3D Lidar-Based Video Object Detection for Autonomous Driving,” in IEEE Transactions
    on Circuits and Systems for Video Technology, vol. 32, no. 4, pp. 2068-2078, April
    2022, doi: 10.1109/TCSVT.2021.3082763.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Z. Yuan, X. Song, L. Bai, Z. Wang 和 W. Ouyang，"基于时间通道变换器的3D激光雷达视频目标检测用于自动驾驶"，发表于《IEEE
    Transactions on Circuits and Systems for Video Technology》，第32卷，第4期，第2068-2078页，2022年4月，doi:
    10.1109/TCSVT.2021.3082763。'
- en: '[188] H. Ibrahem, A. D. A. Salem and H. -S. Kang, ”Real-Time Weakly Supervised
    Object Detection Using Center-of-Features Localization,” in IEEE Access, vol.
    9, pp. 38742-38756, 2021, doi: 10.1109/ACCESS.2021.3064372.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] H. Ibrahem, A. D. A. Salem 和 H. -S. Kang，"实时弱监督目标检测使用特征中心定位"，发表于《IEEE
    Access》，第9卷，第38742-38756页，2021年，doi: 10.1109/ACCESS.2021.3064372。'
- en: '[189] A. B. Amjoud and M. Amrouch, ”Object Detection Using Deep Learning, CNNs,
    and Vision Transformers: A Review,” in IEEE Access, vol. 11, pp. 35479-35516,
    2023, doi: 10.1109/ACCESS.2023.3266093.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] A. B. Amjoud 和 M. Amrouch，"使用深度学习、CNN和视觉变换器的目标检测：综述"，发表于《IEEE Access》，第11卷，第35479-35516页，2023年，doi:
    10.1109/ACCESS.2023.3266093。'
- en: '[190] H. Wang, Q. Wang, H. Zhang, Q. Hu, and W. Zuo, ”CrabNet: Fully Task-Specific
    Feature Learning for One-Stage Object Detection,” in IEEE Transactions on Image
    Processing, vol. 31, pp. 2962-2974, 2022, doi: 10.1109/TIP.2022.3162099.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] H. Wang, Q. Wang, H. Zhang, Q. Hu 和 W. Zuo，"CrabNet：全任务特定特征学习用于单阶段目标检测"，发表于《IEEE
    Transactions on Image Processing》，第31卷，第2962-2974页，2022年，doi: 10.1109/TIP.2022.3162099。'
- en: '[191] T. Gao, H. Pan and H. Gao, ”Monocular 3D Object Detection With Sequential
    Feature Association and Depth Hint Augmentation,” in IEEE Transactions on Intelligent
    Vehicles, vol. 7, no. 2, pp. 240-250, June 2022, doi: 10.1109/TIV.2022.3143954.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] T. Gao, H. Pan 和 H. Gao，"基于单目相机的3D目标检测与序列特征关联及深度提示增强"，发表于《IEEE Transactions
    on Intelligent Vehicles》，第7卷，第2期，第240-250页，2022年6月，doi: 10.1109/TIV.2022.3143954。'
- en: '[192] Z. Zhang and T. D. Bui, ”Attention-based Selection Strategy for Weakly
    Supervised Object Localization,” 2020 25th International Conference on Pattern
    Recognition (ICPR), Milan, Italy, 2021, pp. 10305-10311, doi: 10.1109/ICPR48806.2021.9412173.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Z. Zhang 和 T. D. Bui，"基于注意力的弱监督目标定位选择策略"，2020年第25届国际模式识别大会（ICPR），意大利米兰，2021年，第10305-10311页，doi:
    10.1109/ICPR48806.2021.9412173。'
- en: '[193] J. Wei, Q. Wang, Z. Li, S. Wang, S. K. Zhou and S. Cui, ”Shallow Feature
    Matters for Weakly Supervised Object Localization,” 2021 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp.
    5989-5997, doi: 10.1109/CVPR46437.2021.00593.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] J. Wei, Q. Wang, Z. Li, S. Wang, S. K. Zhou 和 S. Cui，"浅层特征对弱监督目标定位的重要性"，2021
    IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国田纳西州纳什维尔，2021年，第5989-5997页，doi: 10.1109/CVPR46437.2021.00593。'
- en: '[194] L. Zhu, Q. She, Q. Chen, Y. You, B. Wang and Y. Lu, ”Weakly Supervised
    Object Localization as Domain Adaption,” 2022 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 14617-14626,
    doi: 10.1109/CVPR52688.2022.01423.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] L. Zhu, Q. She, Q. Chen, Y. You, B. Wang 和 Y. Lu，"将弱监督目标定位视为领域适应"，2022
    IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国路易斯安那州新奥尔良，2022年，第14617-14626页，doi: 10.1109/CVPR52688.2022.01423。'
- en: '[195] W. Gao et al., ”TS-CAM: Token Semantic Coupled Attention Map for Weakly
    Supervised Object Localization,” 2021 IEEE/CVF International Conference on Computer
    Vision (ICCV), Montreal, QC, Canada, 2021, pp. 2866-2875, doi: 10.1109/ICCV48922.2021.00288.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] W. Gao 等，"TS-CAM: 用于弱监督目标定位的令牌语义耦合注意力图"，2021 IEEE/CVF 国际计算机视觉会议（ICCV），加拿大魁北克省蒙特利尔，2021年，第2866-2875页，doi:
    10.1109/ICCV48922.2021.00288。'
- en: '[196] X. Pan et al., ”Unveiling the Potential of Structure Preserving for Weakly
    Supervised Object Localization,” 2021 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp. 11637-11646, doi: 10.1109/CVPR46437.2021.01147.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] X. Pan 等，"揭示结构保持对弱监督目标定位的潜力"，2021 IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国田纳西州纳什维尔，2021年，第11637-11646页，doi:
    10.1109/CVPR46437.2021.01147。'
- en: '[197] G. Guo, J. Han, F. Wan and D. Zhang, ”Strengthen Learning Tolerance for
    Weakly Supervised Object Localization,” 2021 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp. 7399-7408, doi:
    10.1109/CVPR46437.2021.00732.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] G. Guo, J. Han, F. Wan 和 D. Zhang，"增强弱监督目标定位的学习容忍度"，2021 IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国田纳西州纳什维尔，2021年，第7399-7408页，doi:
    10.1109/CVPR46437.2021.00732。'
- en: '[198] J. Xie, C. Luo, X. Zhu, Z. Jin, W. Lu and L. Shen, ”Online Refinement
    of Low-level Feature Based Activation Map for Weakly Supervised Object Localization,”
    2021 IEEE/CVF International Conference on Computer Vision (ICCV), Montreal, QC,
    Canada, 2021, pp. 132-141, doi: 10.1109/ICCV48922.2021.00020.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] J. Xie, C. Luo, X. Zhu, Z. Jin, W. Lu 和 L. Shen，"弱监督目标定位中基于低级特征的激活图的在线优化"，2021
    IEEE/CVF 国际计算机视觉会议（ICCV），加拿大魁北克省蒙特利尔，2021年，第132-141页，doi: 10.1109/ICCV48922.2021.00020。'
- en: '[199] J. Xu et al., ”CREAM: Weakly Supervised Object Localization via Class
    RE-Activation Mapping,” 2022 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 9427-9436, doi: 10.1109/CVPR52688.2022.00922.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Xu 等，"CREAM: 通过类别重新激活映射进行弱监督目标定位"，2022 IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国路易斯安那州新奥尔良，2022年，第9427-9436页，doi:
    10.1109/CVPR52688.2022.00922。'
- en: '[200] Z. Min, B. Zhuang, S. Schulter, B. Liu, E. Dunn and M. Chandraker, ”NeurOCS:
    Neural NOCS Supervision for Monocular 3D Object Localization,” 2023 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023,
    pp. 21404-21414, doi: 10.1109/CVPR52729.2023.02050.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Z. Min, B. Zhuang, S. Schulter, B. Liu, E. Dunn 和 M. Chandraker，"NeurOCS:
    用于单目3D目标定位的神经NOCS监督"，2023 IEEE/CVF 计算机视觉与模式识别会议（CVPR），加拿大不列颠哥伦比亚省温哥华，2023年，第21404-21414页，doi:
    10.1109/CVPR52729.2023.02050。'
- en: '[201] X. Yu et al., ”Object Localization under Single Coarse Point Supervision,”
    2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New
    Orleans, LA, USA, 2022, pp. 4858-4867, doi: 10.1109/CVPR52688.2022.00482.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] X. Yu 等，"在单个粗略点监督下的目标定位"，2022 IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国路易斯安那州新奥尔良，2022年，第4858-4867页，doi:
    10.1109/CVPR52688.2022.00482。'
- en: '[202] V. Gaudillière, L. Pauly, A. Rathinam, A. G. Sanchez, M. A. Musallam
    and D. Aouada, ”3D-Aware Object Localization using Gaussian Implicit Occupancy
    Function,” 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems
    (IROS), Detroit, MI, USA, 2023, pp. 5858-5863, doi: 10.1109/IROS55552.2023.10342399.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] V. Gaudillière, L. Pauly, A. Rathinam, A. G. Sanchez, M. A. Musallam
    和 D. Aouada，“使用高斯隐式占用函数进行 3D 感知对象定位，”2023 年 IEEE/RSJ 智能机器人与系统国际会议（IROS），美国密歇根州底特律，2023
    年，第 5858-5863 页，doi: 10.1109/IROS55552.2023.10342399。'
- en: '[203] Gang Lv, Y. Sun, Fudong Nian, M. Zhu, W. Tang, and Z. Hu, “COME: Clip-OCR
    and Master ObjEct for text image captioning,” Image and Vision Computing, vol.
    136, pp. 104751–104751, Aug. 2023, doi: 10.1016/j.imavis.2023.104751.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Gang Lv, Y. Sun, Fudong Nian, M. Zhu, W. Tang 和 Z. Hu，“COME：Clip-OCR
    和 Master ObjEct 用于文本图像标题生成，”Image and Vision Computing，第 136 卷，第 104751–104751
    页，2023 年 8 月，doi: 10.1016/j.imavis.2023.104751。'
- en: '[204] M. R. Gupta, N. P. Jacobson, and E. K. Garcia, “OCR binarization and
    image pre-processing for searching historical documents,” Pattern Recognition,
    vol. 40, no. 2, pp. 389–397, Feb. 2007, doi: 10.1016/j.patcog.2006.04.043.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] M. R. Gupta, N. P. Jacobson 和 E. K. Garcia，“OCR 二值化和图像预处理用于历史文献检索，”Pattern
    Recognition，第 40 卷，第 2 期，第 389–397 页，2007 年 2 月，doi: 10.1016/j.patcog.2006.04.043。'
- en: '[205] I. Bazzi, R. Schwartz and J. Makhoul, ”An omnifont open-vocabulary OCR
    system for English and Arabic,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 21, no. 6, pp. 495-504, June 1999, doi: 10.1109/34.771314.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] I. Bazzi, R. Schwartz 和 J. Makhoul，“用于英语和阿拉伯语的全字体开放词汇 OCR 系统，”发表于 IEEE
    Transactions on Pattern Analysis and Machine Intelligence，第 21 卷，第 6 期，第 495-504
    页，1999 年 6 月，doi: 10.1109/34.771314。'
- en: '[206] Jaehwa Park, V. Govindaraju and S. N. Srihari, ”OCR in a hierarchical
    feature space,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 22, no. 4, pp. 400-407, April 2000, doi: 10.1109/34.845383.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Jaehwa Park, V. Govindaraju 和 S. N. Srihari，“分层特征空间中的 OCR，”发表于 IEEE Transactions
    on Pattern Analysis and Machine Intelligence，第 22 卷，第 4 期，第 400-407 页，2000 年 4
    月，doi: 10.1109/34.845383。'
- en: '[207] J. Memon, M. Sami, R. A. Khan and M. Uddin, ”Handwritten Optical Character
    Recognition (OCR): A Comprehensive Systematic Literature Review (SLR),” in IEEE
    Access, vol. 8, pp. 142642-142668, 2020, doi: 10.1109/ACCESS.2020.3012542.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] J. Memon, M. Sami, R. A. Khan 和 M. Uddin，“手写光学字符识别（OCR）：全面的系统文献综述（SLR），”发表于
    IEEE Access，第 8 卷，第 142642-142668 页，2020 年，doi: 10.1109/ACCESS.2020.3012542。'
- en: '[208] G. Nagy, S. Seth and K. Einspahr, ”Decoding Substitution Ciphers by Means
    of Word Matching with Application to OCR,” in IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. PAMI-9, no. 5, pp. 710-715, Sept. 1987, doi: 10.1109/TPAMI.1987.4767969.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] G. Nagy, S. Seth 和 K. Einspahr，“通过词匹配解码替代密码，并应用于 OCR，”发表于 IEEE Transactions
    on Pattern Analysis and Machine Intelligence，第 PAMI-9 卷，第 5 期，第 710-715 页，1987
    年 9 月，doi: 10.1109/TPAMI.1987.4767969。'
- en: '[209] Agung Yuwono Sugiyono, Kendricko Adrio, K. Tanuwijaya, and Kristien Margi
    Suryaningrum, “Extracting Information from Vehicle Registration Plate using OCR
    Tesseract,” Procedia Computer Science, vol. 227, pp. 932–938, Jan. 2023, doi:
    10.1016/j.procs.2023.10.600.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Agung Yuwono Sugiyono, Kendricko Adrio, K. Tanuwijaya 和 Kristien Margi
    Suryaningrum，“使用 OCR Tesseract 从车辆登记牌中提取信息，”Procedia Computer Science，第 227 卷，第
    932–938 页，2023 年 1 月，doi: 10.1016/j.procs.2023.10.600。'
- en: '[210] C. C. Paglinawan, M. Hannah M. Caliolio and J. B. Frias, ”Medicine Classification
    Using YOLOv4 and Tesseract OCR,” 2023 15th International Conference on Computer
    and Automation Engineering (ICCAE), Sydney, Australia, 2023, pp. 260-263, doi:
    10.1109/ICCAE56788.2023.10111387.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] C. C. Paglinawan, M. Hannah M. Caliolio 和 J. B. Frias，“使用 YOLOv4 和 Tesseract
    OCR 的医学分类，”2023 年第 15 届计算机与自动化工程国际会议（ICCAE），澳大利亚悉尼，2023 年，第 260-263 页，doi: 10.1109/ICCAE56788.2023.10111387。'
- en: '[211] T. Thapliyal, S. Bhatt, V. Rawat and S. Maurya, ”Automatic License Plate
    Recognition (ALPR) using YOLOv5 model and Tesseract OCR engine,” 2023 First International
    Conference on Advances in Electrical, Electronics and Computational Intelligence
    (ICAEECI), Tiruchengode, India, 2023, pp. 1-5, doi: 10.1109/ICAEECI58247.2023.10370919.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] T. Thapliyal, S. Bhatt, V. Rawat 和 S. Maurya，“使用 YOLOv5 模型和 Tesseract
    OCR 引擎的自动车牌识别（ALPR），”2023 年第 一届电气、电子与计算智能领域国际会议（ICAEECI），印度 Tiruchengode，2023
    年，第 1-5 页，doi: 10.1109/ICAEECI58247.2023.10370919。'
- en: '[212] S. K. Ladi, G. K. Panda, R. Dash and P. K. Ladi, ”A Pioneering Approach
    of Hyperspectral Image Classification Employing the Cooperative Efforts of 3D,
    2D and Depthwise Separable-1D Convolutions,” 2022 IEEE 2nd International Symposium
    on Sustainable Energy, Signal Processing and Cyber Security (iSSSC), Gunupur,
    Odisha, India, 2022, pp. 1-6, doi: 10.1109/iSSSC56467.2022.10051566.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] S. K. Ladi, G. K. Panda, R. Dash 和 P. K. Ladi，“采用 3D、2D 和深度可分离 1D 卷积的高光谱图像分类开创性方法”，2022
    IEEE 第二届国际可持续能源、信号处理与网络安全研讨会 (iSSSC)，印度奥里萨邦古努普尔，2022年，第1-6页，doi: 10.1109/iSSSC56467.2022.10051566。'
- en: '[213] K. J. Han, R. Prieto and T. Ma, ”State-of-the-Art Speech Recognition
    Using Multi-Stream Self-Attention with Dilated 1D Convolutions,” 2019 IEEE Automatic
    Speech Recognition and Understanding Workshop (ASRU), Singapore, 2019, pp. 54-61,
    doi: 10.1109/ASRU46091.2019.9003730.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] K. J. Han, R. Prieto 和 T. Ma，“使用多流自注意力与膨胀 1D 卷积的最先进语音识别”，2019 IEEE 自动语音识别与理解研讨会
    (ASRU)，新加坡，2019年，第54-61页，doi: 10.1109/ASRU46091.2019.9003730。'
- en: '[214] S. Kiranyaz, T. Ince, O. Abdeljaber, O. Avci and M. Gabbouj, ”1-D Convolutional
    Neural Networks for Signal Processing Applications,” ICASSP 2019 - 2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK,
    2019, pp. 8360-8364, doi: 10.1109/ICASSP.2019.8682194.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] S. Kiranyaz, T. Ince, O. Abdeljaber, O. Avci 和 M. Gabbouj，“用于信号处理应用的
    1-D 卷积神经网络”，ICASSP 2019 - 2019 IEEE 国际声学、语音与信号处理会议 (ICASSP)，英国布赖顿，2019年，第8360-8364页，doi:
    10.1109/ICASSP.2019.8682194。'
- en: '[215] W. Huang and G. Liu, ”Hierarchical Text Classification Based on the End-to-End
    MCHA-BERT,” 2021 IEEE 3rd International Conference on Frontiers Technology of
    Information and Computer (ICFTIC), Greenville, SC, USA, 2021, pp. 301-306, doi:
    10.1109/ICFTIC54370.2021.9647279.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] W. Huang 和 G. Liu，“基于端到端 MCHA-BERT 的层次化文本分类”，2021 IEEE 第三届国际信息与计算技术前沿会议
    (ICFTIC)，美国南卡罗来纳州格林维尔，2021年，第301-306页，doi: 10.1109/ICFTIC54370.2021.9647279。'
- en: '[216] J. Abdelnour, J. Rouat and G. Salvi, ”NAAQA: A Neural Architecture for
    Acoustic Question Answering,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 45, no. 4, pp. 4997-5009, 1 April 2023, doi: 10.1109/TPAMI.2022.3194311.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] J. Abdelnour, J. Rouat 和 G. Salvi，“NAAQA: 一种用于声学问答的神经架构”，发表于《IEEE模式分析与机器智能学报》，第45卷，第4期，第4997-5009页，2023年4月1日，doi:
    10.1109/TPAMI.2022.3194311。'
- en: '[217] J. Rämö and V. Välimäki, ”Optimizing a High-Order Graphic Equalizer for
    Audio Processing,” in IEEE Signal Processing Letters, vol. 21, no. 3, pp. 301-305,
    March 2014, doi: 10.1109/LSP.2014.2301557.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] J. Rämö 和 V. Välimäki，“高阶图形均衡器在音频处理中的优化”，发表于《IEEE信号处理快报》，第21卷，第3期，第301-305页，2014年3月，doi:
    10.1109/LSP.2014.2301557。'
- en: '[218] Y. Yamazaki, C. Premachandra and C. J. Perea, ”Audio-Processing-Based
    Human Detection at Disaster Sites With Unmanned Aerial Vehicle,” in IEEE Access,
    vol. 8, pp. 101398-101405, 2020, doi: 10.1109/ACCESS.2020.2998776.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Y. Yamazaki, C. Premachandra 和 C. J. Perea，“基于音频处理的人类检测在灾难现场与无人机”，发表于《IEEE
    Access》，第8卷，第101398-101405页，2020年，doi: 10.1109/ACCESS.2020.2998776。'
- en: '[219] K. Kumar, R. Pandey, S. S. Bhattacharjee and N. V. George, ”Exponential
    Hyperbolic Cosine Robust Adaptive Filters for Audio Signal Processing,” in IEEE
    Signal Processing Letters, vol. 28, pp. 1410-1414, 2021, doi: 10.1109/LSP.2021.3093862.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] K. Kumar, R. Pandey, S. S. Bhattacharjee 和 N. V. George，“用于音频信号处理的指数双曲余弦鲁棒自适应滤波器”，发表于《IEEE信号处理快报》，第28卷，第1410-1414页，2021年，doi:
    10.1109/LSP.2021.3093862。'
- en: '[220] D. Comminiello, M. Scarpiniti, R. Parisi and A. Uncini, ”Frequency-domain
    Adaptive Filtering: from Real to Hypercomplex Signal Processing,” ICASSP 2019
    - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), Brighton, UK, 2019, pp. 7745-7749, doi: 10.1109/ICASSP.2019.8683403.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] D. Comminiello, M. Scarpiniti, R. Parisi 和 A. Uncini，“频域自适应滤波：从实数到超复数信号处理”，ICASSP
    2019 - 2019 IEEE 国际声学、语音与信号处理会议 (ICASSP)，英国布赖顿，2019年，第7745-7749页，doi: 10.1109/ICASSP.2019.8683403。'
- en: '[221] W. Fan, K. Chen, J. Lu and J. Tao, ”Effective Improvement of Under-Modeling
    Frequency-Domain Kalman Filter,” in IEEE Signal Processing Letters, vol. 26, no.
    2, pp. 342-346, Feb. 2019, doi: 10.1109/LSP.2019.2890965.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] W. Fan, K. Chen, J. Lu 和 J. Tao，“对欠建模频域卡尔曼滤波器的有效改进”，发表于《IEEE信号处理快报》，第26卷，第2期，第342-346页，2019年2月，doi:
    10.1109/LSP.2019.2890965。'
- en: '[222] É. Bavu, A. Ramamonjy, H. Pujol and A. Garcia, ”Timescalenet : A Multiresolution
    Approach for Raw Audio Recognition,” ICASSP 2019 - 2019 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 5686-5690,
    doi: 10.1109/ICASSP.2019.8682378.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] É. Bavu, A. Ramamonjy, H. Pujol 和 A. Garcia，“Timescalenet: 一种用于原始音频识别的多分辨率方法”，ICASSP
    2019 - 2019 IEEE 国际声学、语音与信号处理会议 (ICASSP)，英国布莱顿，2019 年，页码 5686-5690，doi: 10.1109/ICASSP.2019.8682378。'
- en: '[223] Ç. Bilen, A. Ozerov and P. Pérez, ”Solving Time-Domain Audio Inverse
    Problems Using Nonnegative Tensor Factorization,” in IEEE Transactions on Signal
    Processing, vol. 66, no. 21, pp. 5604-5617, 1 Nov.1, 2018, doi: 10.1109/TSP.2018.2869113.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Ç. Bilen, A. Ozerov 和 P. Pérez，“利用非负张量分解解决时域音频反问题”，发表于 IEEE 信号处理学报，第
    66 卷，第 21 期，页码 5604-5617，2018 年 11 月 1 日，doi: 10.1109/TSP.2018.2869113。'
- en: '[224] W. Cai, L. Xie, W. Yang, Y. Li, Y. Gao and T. Wang, ”DFTNet: Dual-Path
    Feature Transfer Network for Weakly Supervised Medical Image Segmentation,” in
    IEEE/ACM Transactions on Computational Biology and Bioinformatics, vol. 20, no.
    4, pp. 2530-2540, 1 July-Aug. 2023, doi: 10.1109/TCBB.2022.3198284.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] W. Cai, L. Xie, W. Yang, Y. Li, Y. Gao 和 T. Wang，“DFTNet: 用于弱监督医学图像分割的双路径特征转移网络”，发表于
    IEEE/ACM 计算生物学与生物信息学学报，第 20 卷，第 4 期，页码 2530-2540，2023 年 7 月-8 月，doi: 10.1109/TCBB.2022.3198284。'
- en: '[225] X. Dai, T. Ma, H. Cai and Y. Wen, ”Unsupervised Hierarchical Translation-Based
    Model for Multi-Modal Medical Image Registration,” ICASSP 2022 - 2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore,
    2022, pp. 1261-1265, doi: 10.1109/ICASSP43922.2022.9746324.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] X. Dai, T. Ma, H. Cai 和 Y. Wen，“用于多模态医学图像配准的无监督分层翻译模型”，ICASSP 2022 -
    2022 IEEE 国际声学、语音与信号处理会议 (ICASSP)，新加坡，新加坡，2022 年，页码 1261-1265，doi: 10.1109/ICASSP43922.2022.9746324。'
- en: '[226] L. Xie, W. Cai and Y. Gao, ”DMCGNet: A Novel Network for Medical Image
    Segmentation With Dense Self-Mimic and Channel Grouping Mechanism,” in IEEE Journal
    of Biomedical and Health Informatics, vol. 26, no. 10, pp. 5013-5024, Oct. 2022,
    doi: 10.1109/JBHI.2022.3192277.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] L. Xie, W. Cai 和 Y. Gao，“DMCGNet: 一种用于医学图像分割的新型网络，具有密集自我模仿和通道分组机制”，发表于
    IEEE 生物医学与健康信息学期刊，第 26 卷，第 10 期，页码 5013-5024，2022 年 10 月，doi: 10.1109/JBHI.2022.3192277。'
- en: '[227] Z. Gu et al., ”CE-Net: Context Encoder Network for 2D Medical Image Segmentation,”
    in IEEE Transactions on Medical Imaging, vol. 38, no. 10, pp. 2281-2292, Oct.
    2019, doi: 10.1109/TMI.2019.2903562.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Z. Gu 等人，“CE-Net: 用于 2D 医学图像分割的上下文编码器网络”，发表于 IEEE 医学影像学期刊，第 38 卷，第 10
    期，页码 2281-2292，2019 年 10 月，doi: 10.1109/TMI.2019.2903562。'
- en: '[228] X. Bing, W. Zhang, L. Zheng and Y. Zhang, ”Medical Image Super Resolution
    Using Improved Generative Adversarial Networks,” in IEEE Access, vol. 7, pp. 145030-145038,
    2019, doi: 10.1109/ACCESS.2019.2944862.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] X. Bing, W. Zhang, L. Zheng 和 Y. Zhang，“使用改进的生成对抗网络进行医学图像超分辨率”，发表于 IEEE
    Access，第 7 卷，页码 145030-145038，2019 年，doi: 10.1109/ACCESS.2019.2944862。'
- en: '[229] M. Z. Khan, M. K. Gajendran, Y. Lee and M. A. Khan, ”Deep Neural Architectures
    for Medical Image Semantic Segmentation: Review,” in IEEE Access, vol. 9, pp.
    83002-83024, 2021, doi: 10.1109/ACCESS.2021.3086530.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] M. Z. Khan, M. K. Gajendran, Y. Lee 和 M. A. Khan，“医学图像语义分割的深度神经架构: 综述”，发表于
    IEEE Access，第 9 卷，页码 83002-83024，2021 年，doi: 10.1109/ACCESS.2021.3086530。'
- en: '[230] J. Duan, S. Mao, J. Jin, Z. Zhou, L. Chen and C. L. P. Chen, ”A Novel
    GA-Based Optimized Approach for Regional Multimodal Medical Image Fusion With
    Superpixel Segmentation,” in IEEE Access, vol. 9, pp. 96353-96366, 2021, doi:
    10.1109/ACCESS.2021.3094972.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] J. Duan, S. Mao, J. Jin, Z. Zhou, L. Chen 和 C. L. P. Chen，“一种基于遗传算法优化的方法，用于具有超像素分割的区域多模态医学图像融合”，发表于
    IEEE Access，第 9 卷，页码 96353-96366，2021 年，doi: 10.1109/ACCESS.2021.3094972。'
- en: '[231] Y. Weng, T. Zhou, Y. Li and X. Qiu, ”NAS-Unet: Neural Architecture Search
    for Medical Image Segmentation,” in IEEE Access, vol. 7, pp. 44247-44257, 2019,
    doi: 10.1109/ACCESS.2019.2908991.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Y. Weng, T. Zhou, Y. Li 和 X. Qiu，“NAS-Unet: 用于医学图像分割的神经架构搜索”，发表于 IEEE
    Access，第 7 卷，页码 44247-44257，2019 年，doi: 10.1109/ACCESS.2019.2908991。'
- en: '[232] C. You, Y. Zhou, R. Zhao, L. Staib and J. S. Duncan, ”SimCVD: Simple
    Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical
    Image Segmentation,” in IEEE Transactions on Medical Imaging, vol. 41, no. 9,
    pp. 2228-2237, Sept. 2022, doi: 10.1109/TMI.2022.3161829.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] C. You, Y. Zhou, R. Zhao, L. Staib 和 J. S. Duncan，“SimCVD: 用于半监督医学图像分割的简单对比体素级表示蒸馏”，发表于
    IEEE 医学影像学期刊，第 41 卷，第 9 期，页码 2228-2237，2022 年 9 月，doi: 10.1109/TMI.2022.3161829。'
- en: '[233] N. Wang et al., ”MISSU: 3D Medical Image Segmentation via Self-Distilling
    TransUNet,” in IEEE Transactions on Medical Imaging, vol. 42, no. 9, pp. 2740-2750,
    Sept. 2023, doi: 10.1109/TMI.2023.3264433.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] N. Wang 等, ”MISSU：通过自我蒸馏的 TransUNet 进行 3D 医学图像分割，” 发表在 IEEE 医学成像期刊，卷42，第9期，页码2740-2750，2023年9月，doi:
    10.1109/TMI.2023.3264433。'
- en: '[234] Y. Zhao, K. Lu, J. Xue, S. Wang and J. Lu, ”Semi-Supervised Medical Image
    Segmentation With Voxel Stability and Reliability Constraints,” in IEEE Journal
    of Biomedical and Health Informatics, vol. 27, no. 8, pp. 3912-3923, Aug. 2023,
    doi: 10.1109/JBHI.2023.3273609.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Y. Zhao, K. Lu, J. Xue, S. Wang 和 J. Lu, ”基于体素稳定性和可靠性约束的半监督医学图像分割，” 发表在
    IEEE 生物医学与健康信息学期刊，卷27，第8期，页码3912-3923，2023年8月，doi: 10.1109/JBHI.2023.3273609。'
- en: '[235] L. Wang, J. Zhang, Y. Liu, J. Mi and J. Zhang, ”Multimodal Medical Image
    Fusion Based on Gabor Representation Combination of Multi-CNN and Fuzzy Neural
    Network,” in IEEE Access, vol. 9, pp. 67634-67647, 2021, doi: 10.1109/ACCESS.2021.3075953.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] L. Wang, J. Zhang, Y. Liu, J. Mi 和 J. Zhang, ”基于 Gabor 表示结合的多模态医学图像融合：多
    CNN 和模糊神经网络，” 发表在 IEEE Access，卷9，页码67634-67647，2021年，doi: 10.1109/ACCESS.2021.3075953。'
- en: '[236] N. Hilmizen, A. Bustamam and D. Sarwinda, ”The Multimodal Deep Learning
    for Diagnosing COVID-19 Pneumonia from Chest CT-Scan and X-Ray Images,” 2020 3rd
    International Seminar on Research of Information Technology and Intelligent Systems
    (ISRITI), Yogyakarta, Indonesia, 2020, pp. 26-31, doi: 10.1109/ISRITI51436.2020.9315478.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] N. Hilmizen, A. Bustamam 和 D. Sarwinda, ”用于诊断 COVID-19 肺炎的多模态深度学习：胸部
    CT 扫描和 X 光图像，” 2020 第三届信息技术与智能系统研究国际研讨会（ISRITI），日惹，印度尼西亚，2020年，页码26-31，doi: 10.1109/ISRITI51436.2020.9315478。'
- en: '[237] T. Anwar and S. Zakir, ”Deep learning based diagnosis of COVID-19 using
    chest CT-scan images,” 2020 IEEE 23rd International Multitopic Conference (INMIC),
    Bahawalpur, Pakistan, 2020, pp. 1-5, doi: 10.1109/INMIC50486.2020.9318212.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] T. Anwar 和 S. Zakir, ”基于深度学习的 COVID-19 胸部 CT 扫描图像诊断，” 2020 IEEE 第23届国际多主题会议（INMIC），巴哈瓦尔布尔，巴基斯坦，2020年，页码1-5，doi:
    10.1109/INMIC50486.2020.9318212。'
- en: '[238] Y. F. Riti, H. A. Nugroho, S. Wibirama, B. Windarta and L. Choridah,
    ”Feature extraction for lesion margin characteristic classification from CT Scan
    lungs image,” 2016 1st International Conference on Information Technology, Information
    Systems and Electrical Engineering (ICITISEE), Yogyakarta, Indonesia, 2016, pp.
    54-58, doi: 10.1109/ICITISEE.2016.7803047.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Y. F. Riti, H. A. Nugroho, S. Wibirama, B. Windarta 和 L. Choridah, ”用于
    CT 扫描肺部图像的病变边缘特征分类的特征提取，” 2016 第一次国际信息技术、信息系统和电气工程会议（ICITISEE），日惹，印度尼西亚，2016年，页码54-58，doi:
    10.1109/ICITISEE.2016.7803047。'
- en: '[239] A. Seum, A. H. Raj, S. Sakib and T. Hossain, ”A Comparative Study of
    CNN Transfer Learning Classification Algorithms with Segmentation for COVID-19
    Detection from CT Scan Images,” 2020 11th International Conference on Electrical
    and Computer Engineering (ICECE), Dhaka, Bangladesh, 2020, pp. 234-237, doi: 10.1109/ICECE51571.2020.9393129.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] A. Seum, A. H. Raj, S. Sakib 和 T. Hossain, ”基于 CNN 转移学习分类算法与分割技术的 COVID-19
    检测比较研究，” 2020 第11届国际电气与计算机工程会议（ICECE），达卡，孟加拉国，2020年，页码234-237，doi: 10.1109/ICECE51571.2020.9393129。'
- en: '[240] N. Vani and D. Vinod, ”A Comparative Analysis on Random Forest Algorithm
    Over K-Means for Identifying the Brain Tumor Anomalies Using Novel CT Scan with
    MRI Scan,” 2022 International Conference on Business Analytics for Technology
    and Security (ICBATS), Dubai, United Arab Emirates, 2022, pp. 1-6, doi: 10.1109/ICBATS54253.2022.9759036.'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] N. Vani 和 D. Vinod, ”基于随机森林算法与 K-均值算法的比较分析：使用新型 CT 扫描与 MRI 扫描识别脑肿瘤异常，”
    2022 国际技术与安全商业分析会议（ICBATS），迪拜，阿拉伯联合酋长国，2022年，页码1-6，doi: 10.1109/ICBATS54253.2022.9759036。'
- en: '[241] A. Hoque, A. K. M. A. Farabi, F. Ahmed and M. Z. Islam, ”Automated Detection
    of Lung Cancer Using CT Scan Images,” 2020 IEEE Region 10 Symposium (TENSYMP),
    Dhaka, Bangladesh, 2020, pp. 1030-1033, doi: 10.1109/TENSYMP50017.2020.9230861.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] A. Hoque, A. K. M. A. Farabi, F. Ahmed 和 M. Z. Islam, ”基于 CT 扫描图像的肺癌自动检测，”
    2020 IEEE 第10区研讨会（TENSYMP），达卡，孟加拉国，2020年，页码1030-1033，doi: 10.1109/TENSYMP50017.2020.9230861。'
- en: '[242] W. Cao et al., ”CNN-based intelligent safety surveillance in green IoT
    applications,” in China Communications, vol. 18, no. 1, pp. 108-119, Jan. 2021,
    doi: 10.23919/JCC.2021.01.010.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] W. Cao 等, ”基于 CNN 的绿色物联网应用智能安全监控，” 发表在 China Communications，卷18，第1期，页码108-119，2021年1月，doi:
    10.23919/JCC.2021.01.010。'
- en: '[243] J. Xu, W. Zhou, Z. Fu, H. Zhang, and L. Li, “A Survey on Green Deep Learning,”
    arXiv (Cornell University), Nov. 2021, doi: 10.48550/arxiv.2111.05193.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] J. Xu, W. Zhou, Z. Fu, H. Zhang 和 L. Li, “绿色深度学习综述，” arXiv（康奈尔大学），2021年11月，doi:
    10.48550/arxiv.2111.05193。'
- en: '[244] Y. Rao, Z. Liu, W. Zhao, J. Zhou and J. Lu, ”Dynamic Spatial Sparsification
    for Efficient Vision Transformers and Convolutional Neural Networks,” in IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp.
    10883-10897, 1 Sept. 2023, doi: 10.1109/TPAMI.2023.3263826.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Y. Rao, Z. Liu, W. Zhao, J. Zhou 和 J. Lu，"动态空间稀疏化：用于高效视觉变换器和卷积神经网络"，发表于
    IEEE Transactions on Pattern Analysis and Machine Intelligence，第 45 卷，第 9 期，第
    10883-10897 页，2023 年 9 月 1 日，doi: 10.1109/TPAMI.2023.3263826。'
- en: '[245] Z. Li, M. Chen, J. Xiao and Q. Gu, ”PSAQ-ViT V2: Toward Accurate and
    General Data-Free Quantization for Vision Transformers,” in IEEE Transactions
    on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3301007.'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] Z. Li, M. Chen, J. Xiao 和 Q. Gu，"PSAQ-ViT V2：面向准确和通用的数据无关量化的视觉变换器"，发表于
    IEEE Transactions on Neural Networks and Learning Systems，doi: 10.1109/TNNLS.2023.3301007。'
- en: '[246] R. Garcia-Martin and R. Sanchez-Reillo, ”Vision Transformers for Vein
    Biometric Recognition,” in IEEE Access, vol. 11, pp. 22060-22080, 2023, doi: 10.1109/ACCESS.2023.3252009.'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] R. Garcia-Martin 和 R. Sanchez-Reillo，"用于静脉生物识别的视觉变换器"，发表于 IEEE Access，第
    11 卷，第 22060-22080 页，2023 年，doi: 10.1109/ACCESS.2023.3252009。'
- en: '[247] K. L. Ong, C. P. Lee, H. S. Lim, K. M. Lim and A. Alqahtani, ”Mel-MViTv2:
    Enhanced Speech Emotion Recognition With Mel Spectrogram and Improved Multiscale
    Vision Transformers,” in IEEE Access, vol. 11, pp. 108571-108579, 2023, doi: 10.1109/ACCESS.2023.3321122.'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] K. L. Ong, C. P. Lee, H. S. Lim, K. M. Lim 和 A. Alqahtani，"Mel-MViTv2：结合梅尔频谱图和改进的多尺度视觉变换器的增强语音情感识别"，发表于
    IEEE Access，第 11 卷，第 108571-108579 页，2023 年，doi: 10.1109/ACCESS.2023.3321122。'
- en: '[248] L. Meng et al., ”AdaViT: Adaptive Vision Transformers for Efficient Image
    Recognition,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), New Orleans, LA, USA, 2022, pp. 12299-12308, doi: 10.1109/CVPR52688.2022.01199.'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] L. Meng 等人，"AdaViT：用于高效图像识别的自适应视觉变换器"，2022 IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，美国新奥尔良，2022
    年，第 12299-12308 页，doi: 10.1109/CVPR52688.2022.01199。'
- en: '[249] J. Liu, X. Huang, J. Zheng, Y. Liu and H. Li, ”MixMAE: Mixed and Masked
    Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers,” 2023
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver,
    BC, Canada, 2023, pp. 6252-6261, doi: 10.1109/CVPR52729.2023.00605.'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] J. Liu, X. Huang, J. Zheng, Y. Liu 和 H. Li，"MixMAE：用于高效预训练层次化视觉变换器的混合和遮蔽自编码器"，2023
    IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，加拿大温哥华，2023 年，第 6252-6261 页，doi: 10.1109/CVPR52729.2023.00605。'
- en: '[250] J. -N. Chen, S. Sun, J. He, P. Torr, A. Yuille and S. Bai, ”TransMix:
    Attend to Mix for Vision Transformers,” 2022 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 12125-12134, doi:
    10.1109/CVPR52688.2022.01182.'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] J. -N. Chen, S. Sun, J. He, P. Torr, A. Yuille 和 S. Bai，"TransMix：面向视觉变换器的混合注意机制"，2022
    IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，美国新奥尔良，2022 年，第 12125-12134 页，doi: 10.1109/CVPR52688.2022.01182。'
- en: '[251] J. -N. Chen, S. Sun, J. He, P. Torr, A. Yuille and S. Bai, ”TransMix:
    Attend to Mix for Vision Transformers,” 2022 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 12125-12134, doi:
    10.1109/CVPR52688.2022.01182.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] J. -N. Chen, S. Sun, J. He, P. Torr, A. Yuille 和 S. Bai，"TransMix：面向视觉变换器的混合注意机制"，2022
    IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，美国新奥尔良，2022 年，第 12125-12134 页，doi: 10.1109/CVPR52688.2022.01182。'
- en: '[252] Y. Tang et al., ”Patch Slimming for Efficient Vision Transformers,” 2022
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans,
    LA, USA, 2022, pp. 12155-12164, doi: 10.1109/CVPR52688.2022.01185.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] Y. Tang 等人，"Patch Slimming：用于高效视觉变换器的补丁瘦身"，2022 IEEE/CVF 计算机视觉与模式识别会议
    (CVPR)，美国新奥尔良，2022 年，第 12155-12164 页，doi: 10.1109/CVPR52688.2022.01185。'
- en: '[253] A. Hatamizadeh et al., ”GradViT: Gradient Inversion of Vision Transformers,”
    2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New
    Orleans, LA, USA, 2022, pp. 10011-10020, doi: 10.1109/CVPR52688.2022.00978.'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] A. Hatamizadeh 等人，"GradViT：视觉变换器的梯度反演"，2022 IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，美国新奥尔良，2022
    年，第 10011-10020 页，doi: 10.1109/CVPR52688.2022.00978。'
- en: '[254] Y. He et al., ”BiViT: Extremely Compressed Binary Vision Transformers,”
    2023 IEEE/CVF International Conference on Computer Vision (ICCV), Paris, France,
    2023, pp. 5628-5640, doi: 10.1109/ICCV51070.2023.00520.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Y. He 等人，"BiViT：极度压缩的二值视觉变换器"，2023 IEEE/CVF 国际计算机视觉会议 (ICCV)，法国巴黎，2023
    年，第 5628-5640 页，doi: 10.1109/ICCV51070.2023.00520。'
- en: '[255] Y. Li et al., ”Rethinking Vision Transformers for MobileNet Size and
    Speed,” 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Paris,
    France, 2023, pp. 16843-16854, doi: 10.1109/ICCV51070.2023.01549.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] Y. Li 等人，"重新思考视觉变换器的 MobileNet 尺寸和速度"，2023 IEEE/CVF 国际计算机视觉会议 (ICCV)，法国巴黎，2023
    年，第 16843-16854 页，doi: 10.1109/ICCV51070.2023.01549。'
- en: '[256] T. Sun, C. Zhang, Y. Ji and Z. Hu, ”MSnet: Multi-Head Self-Attention
    Network for Distantly Supervised Relation Extraction,” in IEEE Access, vol. 7,
    pp. 54472-54482, 2019, doi: 10.1109/ACCESS.2019.2913316.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] 孙涛, 张超, 季燕和胡志伟, "MSnet: 用于远程监督关系抽取的多头自注意网络," 发表于《IEEE Access》，第7卷，第54472-54482页，2019年，doi:
    10.1109/ACCESS.2019.2913316。'
- en: '[257] Z. Xie, G. Zheng, L. Miao and W. Huang, ”STGL-GCN: Spatial–Temporal Mixing
    of Global and Local Self-Attention Graph Convolutional Networks for Human Action
    Recognition,” in IEEE Access, vol. 11, pp. 16526-16532, 2023, doi: 10.1109/ACCESS.2023.3246127.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] 谢志，郑刚，苗力和黄伟, "STGL-GCN:用于人体动作识别的全局和局部自注意力图卷积网络的时空混合," 发表于《IEEE Access》，第11卷，第16526-16532页，2023年，doi:
    10.1109/ACCESS.2023.3246127。'
- en: '[258] C. -X. Zhang, Y. -L. Zhang and X. -Y. Gao, ”Multi-Head Self-Attention
    Gated-Dilated Convolutional Neural Network for Word Sense Disambiguation,” in
    IEEE Access, vol. 11, pp. 14202-14210, 2023, doi: 10.1109/ACCESS.2023.3243574.'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] 张陈信，张玉莲和高晓瑜, "用于词义消歧的多头自注意力门控扩张卷积神经网络," 发表于《IEEE Access》，第11卷，第14202-14210页，2023年，doi:
    10.1109/ACCESS.2023.3243574。'
- en: '[259] K. Zhao, W. Guo, F. Qin and X. Wang, ”D3-SACNN: DGA Domain Detection
    With Self-Attention Convolutional Network,” in IEEE Access, vol. 10, pp. 69250-69263,
    2022, doi: 10.1109/ACCESS.2021.3127913.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] 赵凯, 郭伟, 秦峰和王晓, "D3-SACNN: 使用自注意力卷积网络进行DGA域检测," 发表于《IEEE Access》，第10卷，第69250-69263页，2022年，doi:
    10.1109/ACCESS.2021.3127913。'
- en: '[260] F. Zhang, A. Panahi and G. Gao, ”FsaNet: Frequency Self-Attention for
    Semantic Segmentation,” in IEEE Transactions on Image Processing, vol. 32, pp.
    4757-4772, 2023, doi: 10.1109/TIP.2023.3305090.'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] 张峰, 潘啊和高根高, "FsaNet:用于语义分割的频率自注意," 发表于《IEEE Transactions on Image Processing》，第32卷，第4757-4772页，2023年，doi:
    10.1109/TIP.2023.3305090。'
- en: '[261] B. Kelenyi and L. Tamas, ”D3GATTEN: Dense 3D Geometric Features Extraction
    and Pose Estimation Using Self-Attention,” in IEEE Access, vol. 11, pp. 7947-7958,
    2023, doi: 10.1109/ACCESS.2023.3238901.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] B. Kelenyi 和 L. Tamas, "D3GATTEN: 使用自注意力进行稠密三维几何特征提取和姿态估计," 发表于《IEEE
    Access》，第11卷，第7947-7958页，2023年，doi: 10.1109/ACCESS.2023.3238901。'
- en: '[262] Y. Wang, Y. Xie, X. Ji, Z. Liu and X. Liu, ”RacPixGAN: An Enhanced Sketch-to-Face
    Synthesis GAN Based on Residual modules, Multi-Head Self-Attention Mechanisms,
    and CLIP Loss,” 2023 4th International Conference on Electronic Communication
    and Artificial Intelligence (ICECAI), Guangzhou, China, 2023, pp. 336-342, doi:
    10.1109/ICECAI58670.2023.10176715.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] 王勇，谢阳，季旭，刘征和刘鑫, "RacPixGAN:一种基于残差模块、多头自注意力机制和CLIP损失的增强版素描到人脸综合GAN," 2023年第4届电子通信和人工智能国际会议(ICECAI)，中国广州，2023年，第336-342页，doi:
    10.1109/ICECAI58670.2023.10176715。'
- en: '[263] C. Yang et al., ”Lite Vision Transformer with Enhanced Self-Attention,”
    2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New
    Orleans, LA, USA, 2022, pp. 11988-11998, doi: 10.1109/CVPR52688.2022.01169.'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] C. Yang 等, "增强自注意力的轻量Vision Transformer," 2022年IEEE/CVF计算机视觉和模式识别会议(CVPR)，美国新奥尔良，2022年，第11988-11998页，doi:
    10.1109/CVPR52688.2022.01169。'
- en: '[264] S. Park and Y. S. Choi, ”Image Super-Resolution Using Dilated Window
    Transformer,” in IEEE Access, vol. 11, pp. 60028-60039, 2023, doi: 10.1109/ACCESS.2023.3284539.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] 朴树和崔英守, "使用扩张窗口变换器进行图像超分辨率," 发表于《IEEE Access》，第11卷，第60028-60039页，2023年，doi:
    10.1109/ACCESS.2023.3284539。'
- en: '[265] F. Wang, X. Wang, D. Lv, L. Zhou and G. Shi, ”Separable Self-Attention
    Mechanism for Point Cloud Local and Global Feature Modeling,” in IEEE Access,
    vol. 10, pp. 129823-129831, 2022, doi: 10.1109/ACCESS.2022.3228044.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] 王飞, 王欣, 吕丹, 周亮和石光, "可分离自注意力机制用于点云局部和全局特征建模," 发表于《IEEE Access》，第10卷，第129823-129831页，2022年，doi:
    10.1109/ACCESS.2022.3228044。'
- en: '[266] S. Lamba, A. Baliyan and V. Kukreja, ”GAN based image augmentation for
    increased CNN performance in Paddy leaf disease classification,” 2022 2nd International
    Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),
    Greater Noida, India, 2022, pp. 2054-2059, doi: 10.1109/ICACITE53722.2022.9823799.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] S. Lamba, A. Baliyan 和 V. Kukreja, "基于GAN的图像增强用于判断稻谷叶病分类中CNN性能的提升," 2022年第2届印度大尼达先进计算和创新技术国际会议(ICACITE)，印度大尼达，2022年，第2054-2059页，doi:
    10.1109/ICACITE53722.2022.9823799。'
- en: '[267] B. Sandhiya, R. Priyatharshini, B. Ramya, S. Monish and G. R. Sai Raja,
    ”Reconstruction, Identification and Classification of Brain Tumor Using Gan and
    Faster Regional-CNN,” 2021 3rd International Conference on Signal Processing and
    Communication (ICPSC), Coimbatore, India, 2021, pp. 238-242, doi: 10.1109/ICSPC51351.2021.9451747.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] B. Sandhiya, R. Priyatharshini, B. Ramya, S. Monish 和 G. R. Sai Raja,
    "使用GAN和更快的区域-CNN进行脑肿瘤的重建、识别和分类," 2021年第3届信号处理与通讯国际会议(ICPSC)，印度哥印拜陀，2021年，第238-242页，doi:
    10.1109/ICSPC51351.2021.9451747。'
- en: '[268] N. Sasipriyaa, P. Natesan, R. S. Mohana, P. Arvindkumar, R. S. Arwin
    Prakadis and K. Aswin Surya, ”Recognizing Handwritten Offline Tamil Character
    using VAE-GAN & CNN,” 2023 International Conference on Computer Communication
    and Informatics (ICCCI), Coimbatore, India, 2023, pp. 1-7, doi: 10.1109/ICCCI56745.2023.10128520.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] N. Sasipriyaa, P. Natesan, R. S. Mohana, P. Arvindkumar, R. S. Arwin
    Prakadis 和 K. Aswin Surya, “使用 VAE-GAN 和 CNN 识别手写的离线泰米尔字符,” 2023 年国际计算机通信与信息学大会
    (ICCCI), 印度科印巴托尔, 2023 年, 第 1-7 页, doi: 10.1109/ICCCI56745.2023.10128520.'
- en: '[269] S. K. Singh, M. H. Anisi, S. Clough, T. Blyth and D. Jarchi, ”CNN-BiLSTM
    based GAN for Anamoly Detection from Multivariate Time Series Data,” 2023 24th
    International Conference on Digital Signal Processing (DSP), Rhodes (Rodos), Greece,
    2023, pp. 1-4, doi: 10.1109/DSP58604.2023.10167937.'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] S. K. Singh, M. H. Anisi, S. Clough, T. Blyth 和 D. Jarchi, “基于 CNN-BiLSTM
    的 GAN 用于多变量时间序列数据的异常检测,” 2023 年第 24 届数字信号处理国际会议 (DSP), 希腊罗德岛, 2023 年, 第 1-4 页,
    doi: 10.1109/DSP58604.2023.10167937.'
- en: '[270] Y. Fu, T. Sun, X. Jiang, K. Xu and P. He, ”Robust GAN-Face Detection
    Based on Dual-Channel CNN Network,” 2019 12th International Congress on Image
    and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), Suzhou,
    China, 2019, pp. 1-5, doi: 10.1109/CISP-BMEI48845.2019.8965991.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] Y. Fu, T. Sun, X. Jiang, K. Xu 和 P. He, “基于双通道 CNN 网络的鲁棒 GAN 人脸检测,” 2019
    年第 12 届国际图像与信号处理、生物医学工程与信息学大会 (CISP-BMEI), 中国苏州, 2019 年, 第 1-5 页, doi: 10.1109/CISP-BMEI48845.2019.8965991.'
- en: '[271] A. Akram, N. Wang, X. Gao and J. Li, ”Integrating GAN with CNN for Face
    Sketch Synthesis,” 2018 IEEE 4th International Conference on Computer and Communications
    (ICCC), Chengdu, China, 2018, pp. 1483-1487, doi: 10.1109/CompComm.2018.8780648.'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] A. Akram, N. Wang, X. Gao 和 J. Li, “将 GAN 与 CNN 结合用于人脸素描合成,” 2018 年 IEEE
    第四届计算机与通信国际会议 (ICCC), 中国成都, 2018 年, 第 1483-1487 页, doi: 10.1109/CompComm.2018.8780648.'
- en: '[272] C. -H. Rhee and C. H. Lee, ”Estimating Physically-Based Reflectance Parameters
    From a Single Image With GAN-Guided CNN,” in IEEE Access, vol. 10, pp. 13259-13269,
    2022, doi: 10.1109/ACCESS.2022.3147483.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] C. -H. Rhee 和 C. H. Lee, “通过 GAN 指导 CNN 从单幅图像中估计基于物理的反射参数,” 发表在《IEEE
    Access》, 第 10 卷，第 13259-13269 页, 2022 年, doi: 10.1109/ACCESS.2022.3147483.'
- en: '[273] C. Mao, L. Huang, Y. Xiao, F. He and Y. Liu, ”Target Recognition of SAR
    Image Based on CN-GAN and CNN in Complex Environment,” in IEEE Access, vol. 9,
    pp. 39608-39617, 2021, doi: 10.1109/ACCESS.2021.3064362.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] C. Mao, L. Huang, Y. Xiao, F. He 和 Y. Liu, “基于 CN-GAN 和 CNN 的 SAR 图像目标识别在复杂环境中的应用,”
    发表在《IEEE Access》, 第 9 卷，第 39608-39617 页, 2021 年, doi: 10.1109/ACCESS.2021.3064362.'
- en: '[274] J. Wang, M. Xu, X. Deng, L. Shen and Y. Song, ”MW-GAN+ for Perceptual
    Quality Enhancement on Compressed Video,” in IEEE Transactions on Circuits and
    Systems for Video Technology, vol. 32, no. 7, pp. 4224-4237, July 2022, doi: 10.1109/TCSVT.2021.3128275.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] J. Wang, M. Xu, X. Deng, L. Shen 和 Y. Song, “MW-GAN+ 用于压缩视频的感知质量增强,”
    发表在《IEEE Transactions on Circuits and Systems for Video Technology》, 第 32 卷，第
    7 期，第 4224-4237 页, 2022 年 7 月, doi: 10.1109/TCSVT.2021.3128275.'
- en: '[275] L. Wei, S. Zhang, W. Gao and Q. Tian, ”Person Transfer GAN to Bridge
    Domain Gap for Person Re-identification,” 2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, Salt Lake City, UT, USA, 2018, pp. 79-88, doi:
    10.1109/CVPR.2018.00016.'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] L. Wei, S. Zhang, W. Gao 和 Q. Tian, “人员转移 GAN 桥接领域差距以实现人员重新识别,” 2018
    年 IEEE/CVF 计算机视觉与模式识别会议, 美国犹他州盐湖城, 2018 年, 第 79-88 页, doi: 10.1109/CVPR.2018.00016.'
- en: '[276] F. Chollet, ”Xception: Deep Learning with Depthwise Separable Convolutions,”
    2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu,
    HI, USA, 2017, pp. 1800-1807, doi: 10.1109/CVPR.2017.195.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] F. Chollet, “Xception: 使用深度可分离卷积的深度学习,” 2017 年 IEEE 计算机视觉与模式识别会议 (CVPR),
    美国夏威夷檀香山, 2017 年, 第 1800-1807 页, doi: 10.1109/CVPR.2017.195.'
- en: '[277] S. Ma, W. Liu, W. Cai, Z. Shang and G. Liu, ”Lightweight Deep Residual
    CNN for Fault Diagnosis of Rotating Machinery Based on Depthwise Separable Convolutions,”
    in IEEE Access, vol. 7, pp. 57023-57036, 2019, doi: 10.1109/ACCESS.2019.2912072.'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] S. Ma, W. Liu, W. Cai, Z. Shang 和 G. Liu, “基于深度可分离卷积的轻量级深度残差 CNN 用于旋转机械故障诊断,”
    发表在《IEEE Access》, 第 7 卷，第 57023-57036 页, 2019 年, doi: 10.1109/ACCESS.2019.2912072.'
- en: '[278] D. Haase and M. Amthor, ”Rethinking Depthwise Separable Convolutions:
    How Intra-Kernel Correlations Lead to Improved MobileNets,” in 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp.
    14588-14597, doi: 10.1109/CVPR42600.2020.01461.'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] D. Haase 和 M. Amthor, “重新思考深度可分离卷积：内核间相关性如何改善 MobileNets,” 2020 年 IEEE/CVF
    计算机视觉与模式识别会议 (CVPR), 美国华盛顿州西雅图, 2020 年, 第 14588-14597 页, doi: 10.1109/CVPR42600.2020.01461.'
- en: '[279] A. Batool and Y. -C. Byun, ”Lightweight EfficientNetB3 Model Based on
    Depthwise Separable Convolutions for Enhancing Classification of Leukemia White
    Blood Cell Images,” in IEEE Access, vol. 11, pp. 37203-37215, 2023, doi: 10.1109/ACCESS.2023.3266511.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] A. Batool 和 Y. -C. Byun，“基于深度可分离卷积的轻量级EfficientNetB3模型用于增强白血病白细胞图像的分类，”发表于《IEEE
    Access》，第11卷，第37203-37215页，2023年，doi: 10.1109/ACCESS.2023.3266511。'
- en: '[280] N. A. Mohamed, M. A. Zulkifley and S. R. Abdani, ”Spatial Pyramid Pooling
    with Atrous Convolutional for MobileNet,” 2020 IEEE Student Conference on Research
    and Development (SCOReD), Batu Pahat, Malaysia, 2020, pp. 333-336, doi: 10.1109/SCOReD50371.2020.9250928.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] N. A. Mohamed, M. A. Zulkifley 和 S. R. Abdani，“用于MobileNet的空间金字塔池化与空洞卷积，”2020年IEEE学生研究与发展会议（SCOReD），马来西亚峇株巴辖，2020年，第333-336页，doi:
    10.1109/SCOReD50371.2020.9250928。'
- en: '[281] S. Sriram, R. Vinayakumar, V. Sowmya, M. Alazab and K. P. Soman, ”Multi-scale
    Learning based Malware Variant Detection using Spatial Pyramid Pooling Network,”
    IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM
    WKSHPS), Toronto, ON, Canada, 2020, pp. 740-745, doi: 10.1109/INFOCOMWKSHPS50562.2020.9162661.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] S. Sriram, R. Vinayakumar, V. Sowmya, M. Alazab 和 K. P. Soman，“基于多尺度学习的恶意软件变体检测使用空间金字塔池化网络，”IEEE
    INFOCOM 2020 - IEEE计算机通信研讨会（INFOCOM WKSHPS），加拿大安大略省多伦多，2020年，第740-745页，doi: 10.1109/INFOCOMWKSHPS50562.2020.9162661。'
- en: '[282] K. He, X. Zhang, S. Ren and J. Sun, ”Spatial Pyramid Pooling in Deep
    Convolutional Networks for Visual Recognition,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 37, no. 9, pp. 1904-1916, 1 Sept. 2015,
    doi: 10.1109/TPAMI.2015.2389824.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] K. He, X. Zhang, S. Ren 和 J. Sun，“深度卷积网络中的空间金字塔池化用于视觉识别，”发表于《IEEE模式分析与机器智能汇刊》，第37卷，第9期，第1904-1916页，2015年9月1日，doi:
    10.1109/TPAMI.2015.2389824。'
- en: '[283] E. Prasetyo, N. Suciati and C. Fatichah, ”Yolov4-tiny and Spatial Pyramid
    Pooling for Detecting Head and Tail of Fish,” 2021 International Conference on
    Artificial Intelligence and Computer Science Technology (ICAICST), Yogyakarta,
    Indonesia, 2021, pp. 157-161, doi: 10.1109/ICAICST53116.2021.9497822.'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] E. Prasetyo, N. Suciati 和 C. Fatichah，“Yolov4-tiny 和空间金字塔池化用于检测鱼的头部和尾部，”2021年国际人工智能与计算机科学技术会议（ICAICST），印度尼西亚，日惹，2021年，第157-161页，doi:
    10.1109/ICAICST53116.2021.9497822。'
- en: '[284] Y. Tian, F. Chen, H. Wang and S. Zhang, ”Real-Time Semantic Segmentation
    Network Based on Lite Reduced Atrous Spatial Pyramid Pooling Module Group,” 2020
    5th International Conference on Control, Robotics and Cybernetics (CRC), Wuhan,
    China, 2020, pp. 139-143, doi: 10.1109/CRC51253.2020.9253492.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] Y. Tian, F. Chen, H. Wang 和 S. Zhang，“基于Lite Reduced Atrous Spatial Pyramid
    Pooling模块组的实时语义分割网络，”2020年第五届国际控制、机器人与网络会议（CRC），中国武汉，2020年，第139-143页，doi: 10.1109/CRC51253.2020.9253492。'
- en: '[285] A. Qayyum, I. Ahmad, W. Mumtaz, M. O. Alassafi, R. Alghamdi and M. Mazher,
    ”Automatic Segmentation Using a Hybrid Dense Network Integrated With an 3D-Atrous
    Spatial Pyramid Pooling Module for Computed Tomography (CT) Imaging,” in IEEE
    Access, vol. 8, pp. 169794-169803, 2020, doi: 10.1109/ACCESS.2020.3024277.'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] A. Qayyum, I. Ahmad, W. Mumtaz, M. O. Alassafi, R. Alghamdi 和 M. Mazher，“使用集成有3D-Atrous
    Spatial Pyramid Pooling模块的混合密集网络进行自动分割的计算机断层扫描（CT）成像，”发表于《IEEE Access》，第8卷，第169794-169803页，2020年，doi:
    10.1109/ACCESS.2020.3024277。'
- en: '[286] Chi-Man Pun and Moon-Chuen Lee, ”Extraction of shift invariant wavelet
    features for classification of images with different sizes,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 26, no. 9, pp. 1228-1233, Sept.
    2004, doi: 10.1109/TPAMI.2004.67.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] Chi-Man Pun 和 Moon-Chuen Lee，“提取移位不变小波特征用于分类不同尺寸的图像，”发表于《IEEE模式分析与机器智能汇刊》，第26卷，第9期，第1228-1233页，2004年9月，doi:
    10.1109/TPAMI.2004.67。'
- en: '[287] L. Liang and H. Liu, ”Dual-Tree Cosine-Modulated Filter Bank With Linear-Phase
    Individual Filters: An Alternative Shift-Invariant and Directional-Selective Transform,”
    in IEEE Transactions on Image Processing, vol. 22, no. 12, pp. 5168-5180, Dec.
    2013, doi: 10.1109/TIP.2013.2283146.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] L. Liang 和 H. Liu，“具有线性相位单滤波器的双树余弦调制滤波器组：一种替代的移位不变和方向选择性变换，”发表于《IEEE图像处理汇刊》，第22卷，第12期，第5168-5180页，2013年12月，doi:
    10.1109/TIP.2013.2283146。'
- en: '[288] L. -D. Kuang, Q. -H. Lin, X. -F. Gong, F. Cong, Y. -P. Wang and V. D.
    Calhoun, ”Shift-Invariant Canonical Polyadic Decomposition of Complex-Valued Multi-Subject
    fMRI Data With a Phase Sparsity Constraint,” in IEEE Transactions on Medical Imaging,
    vol. 39, no. 4, pp. 844-853, April 2020, doi: 10.1109/TMI.2019.2936046.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] L. -D. Kuang, Q. -H. Lin, X. -F. Gong, F. Cong, Y. -P. Wang 和 V. D. Calhoun，“具有相位稀疏约束的复值多主体fMRI数据的移位不变典型多元分解，”发表于《IEEE医学影像学汇刊》，第39卷，第4期，第844-853页，2020年4月，doi:
    10.1109/TMI.2019.2936046。'
- en: '[289] G. Papari, P. Campisi and N. Petkov, ”New Families of Fourier Eigenfunctions
    for Steerable Filtering,” in IEEE Transactions on Image Processing, vol. 21, no.
    6, pp. 2931-2943, June 2012, doi: 10.1109/TIP.2011.2179060.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] G. Papari, P. Campisi 和 N. Petkov，"用于可转动滤波的新型傅里叶特征函数"，发表于 IEEE 图像处理汇刊，卷
    21，第 6 期，第 2931-2943 页，2012年6月，doi: 10.1109/TIP.2011.2179060。'
- en: '[290] T. Zhao and T. Blu, ”The Fourier-Argand Representation: An Optimal Basis
    of Steerable Patterns,” in IEEE Transactions on Image Processing, vol. 29, pp.
    6357-6371, 2020, doi: 10.1109/TIP.2020.2990483.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] T. Zhao 和 T. Blu，"傅里叶-阿尔冈德表示：可转动模式的最佳基底"，发表于 IEEE 图像处理汇刊，卷 29，第 6357-6371
    页，2020年，doi: 10.1109/TIP.2020.2990483。'
- en: '[291] A. Depeursinge, Z. Püspöki, J. P. Ward and M. Unser, ”Steerable Wavelet
    Machines (SWM): Learning Moving Frames for Texture Classification,” in IEEE Transactions
    on Image Processing, vol. 26, no. 4, pp. 1626-1636, April 2017, doi: 10.1109/TIP.2017.2655438.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] A. Depeursinge, Z. Püspöki, J. P. Ward 和 M. Unser，"可转动小波机器 (SWM)：学习用于纹理分类的移动帧"，发表于
    IEEE 图像处理汇刊，卷 26，第 4 期，第 1626-1636 页，2017年4月，doi: 10.1109/TIP.2017.2655438。'
- en: '[292] A. M. Alhassan and W. M. N. W. Zainon, ”BAT Algorithm With fuzzy C-Ordered
    Means (BAFCOM) Clustering Segmentation and Enhanced Capsule Networks (ECN) for
    Brain Cancer MRI Images Classification,” in IEEE Access, vol. 8, pp. 201741-201751,
    2020, doi: 10.1109/ACCESS.2020.3035803.'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] A. M. Alhassan 和 W. M. N. W. Zainon，"带有模糊 C-有序均值 (BAFCOM) 聚类分割和增强型胶囊网络
    (ECN) 的脑癌 MRI 图像分类"，发表于 IEEE Access，卷 8，第 201741-201751 页，2020年，doi: 10.1109/ACCESS.2020.3035803。'
- en: '[293] P. Haridas, G. Chennupati, N. Santhi, P. Romero and S. Eidenbenz, ”Code
    Characterization With Graph Convolutions and Capsule Networks,” in IEEE Access,
    vol. 8, pp. 136307-136315, 2020, doi: 10.1109/ACCESS.2020.3011909.'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] P. Haridas, G. Chennupati, N. Santhi, P. Romero 和 S. Eidenbenz，"通过图卷积和胶囊网络进行代码特征化"，发表于
    IEEE Access，卷 8，第 136307-136315 页，2020年，doi: 10.1109/ACCESS.2020.3011909。'
- en: '[294] B. Kakillioglu, A. Ren, Y. Wang and S. Velipasalar, ”3D Capsule Networks
    for Object Classification With Weight Pruning,” in IEEE Access, vol. 8, pp. 27393-27405,
    2020, doi: 10.1109/ACCESS.2020.2971950.'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] B. Kakillioglu, A. Ren, Y. Wang 和 S. Velipasalar，"用于对象分类的 3D 胶囊网络与权重剪枝"，发表于
    IEEE Access，卷 8，第 27393-27405 页，2020年，doi: 10.1109/ACCESS.2020.2971950。'
- en: '[295] A. Marchisio, V. Mrazek, A. Massa, B. Bussolino, M. Martina and M. Shafique,
    ”RoHNAS: A Neural Architecture Search Framework With Conjoint Optimization for
    Adversarial Robustness and Hardware Efficiency of Convolutional and Capsule Networks,”
    in IEEE Access, vol. 10, pp. 109043-109055, 2022, doi: 10.1109/ACCESS.2022.3214312.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] A. Marchisio, V. Mrazek, A. Massa, B. Bussolino, M. Martina 和 M. Shafique，"RoHNAS：一种用于对抗性鲁棒性和卷积及胶囊网络硬件效率的联合优化神经架构搜索框架"，发表于
    IEEE Access，卷 10，第 109043-109055 页，2022年，doi: 10.1109/ACCESS.2022.3214312。'
- en: '[296] Y. Zhao, T. Birdal, H. Deng and F. Tombari, ”3D Point Capsule Networks,”
    2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long
    Beach, CA, USA, 2019, pp. 1009-1018, doi: 10.1109/CVPR.2019.00110.'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] Y. Zhao, T. Birdal, H. Deng 和 F. Tombari，"3D 点胶囊网络"，2019 IEEE/CVF 计算机视觉与模式识别会议
    (CVPR)，美国加州长滩，2019年，第 1009-1018 页，doi: 10.1109/CVPR.2019.00110。'
- en: '[297] M. -H. Ha and O. T. -C. Chen, ”Deep Neural Networks Using Capsule Networks
    and Skeleton-Based Attentions for Action Recognition,” in IEEE Access, vol. 9,
    pp. 6164-6178, 2021, doi: 10.1109/ACCESS.2020.3048741.'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] M. -H. Ha 和 O. T. -C. Chen，"使用胶囊网络和基于骨架的注意力进行动作识别的深度神经网络"，发表于 IEEE Access，卷
    9，第 6164-6178 页，2021年，doi: 10.1109/ACCESS.2020.3048741。'
- en: '[298] J. Rajasegaran, V. Jayasundara, S. Jayasekara, H. Jayasekara, S. Seneviratne
    and R. Rodrigo, ”DeepCaps: Going Deeper With Capsule Networks,” 2019 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA,
    USA, 2019, pp. 10717-10725, doi: 10.1109/CVPR.2019.01098.'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] J. Rajasegaran, V. Jayasundara, S. Jayasekara, H. Jayasekara, S. Seneviratne
    和 R. Rodrigo，"DeepCaps：利用胶囊网络更深入"，2019 IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，美国加州长滩，2019年，第
    10717-10725 页，doi: 10.1109/CVPR.2019.01098。'
- en: '[299] Z. Yan, X. Dai, P. Zhang, Y. Tian, B. Wu and M. Feiszli, ”FP-NAS: Fast
    Probabilistic Neural Architecture Search,” 2021 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp. 15134-15143,
    doi: 10.1109/CVPR46437.2021.01489.'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] Z. Yan, X. Dai, P. Zhang, Y. Tian, B. Wu 和 M. Feiszli，"FP-NAS：快速概率神经架构搜索"，2021
    IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，美国田纳西州纳什维尔，2021年，第 15134-15143 页，doi: 10.1109/CVPR46437.2021.01489。'
- en: '[300] M. Zhang, H. Li, S. Pan, X. Chang, and S. Su, ”Overcoming Multi-Model
    Forgetting in One-Shot NAS With Diversity Maximization,” 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp.
    7806-7815, doi: 10.1109/CVPR42600.2020.00783.'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] M. Zhang, H. Li, S. Pan, X. Chang 和 S. Su，”通过多样性最大化克服一-shot NAS中的多模型遗忘，”
    2020 IEEE/CVF 计算机视觉与模式识别大会 (CVPR)，美国华盛顿州西雅图，2020，页码 7806-7815，doi: 10.1109/CVPR42600.2020.00783。'
- en: '[301] X. Li et al., ”Improving One-Shot NAS by Suppressing the Posterior Fading,”
    2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle,
    WA, USA, 2020, pp. 13833-13842, doi: 10.1109/CVPR42600.2020.01385.'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] X. Li 等，”通过抑制后验衰减来改进一-shot NAS，” 2020 IEEE/CVF 计算机视觉与模式识别大会 (CVPR)，美国华盛顿州西雅图，2020，页码
    13833-13842，doi: 10.1109/CVPR42600.2020.01385。'
- en: '[302] Z. Li, T. Xi, J. Deng, G. Zhang, S. Wen and R. He, ”GP-NAS: Gaussian
    Process Based Neural Architecture Search,” 2020 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp. 11930-11939,
    doi: 10.1109/CVPR42600.2020.01195.'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] Z. Li, T. Xi, J. Deng, G. Zhang, S. Wen 和 R. He，”GP-NAS: 基于高斯过程的神经架构搜索，”
    2020 IEEE/CVF 计算机视觉与模式识别大会 (CVPR)，美国华盛顿州西雅图，2020，页码 11930-11939，doi: 10.1109/CVPR42600.2020.01195。'
- en: '[303] C. Jiang, H. Xu, W. Zhang, X. Liang and Z. Li, ”SP-NAS: Serial-to-Parallel
    Backbone Search for Object Detection,” 2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp. 11860-11869, doi:
    10.1109/CVPR42600.2020.01188.'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] C. Jiang, H. Xu, W. Zhang, X. Liang 和 Z. Li，”SP-NAS: 面向目标检测的串行到并行骨干搜索，”
    2020 IEEE/CVF 计算机视觉与模式识别大会 (CVPR)，美国华盛顿州西雅图，2020，页码 11860-11869，doi: 10.1109/CVPR42600.2020.01188。'
- en: '[304] Y. Gao, H. Bai, Z. Jie, J. Ma, K. Jia and W. Liu, ”MTL-NAS: Task-Agnostic
    Neural Architecture Search Towards General-Purpose Multi-Task Learning,” 2020
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle,
    WA, USA, 2020, pp. 11540-11549, doi: 10.1109/CVPR42600.2020.01156.'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] Y. Gao, H. Bai, Z. Jie, J. Ma, K. Jia 和 W. Liu，”MTL-NAS: 面向通用多任务学习的任务无关神经架构搜索，”
    2020 IEEE/CVF 计算机视觉与模式识别大会 (CVPR)，美国华盛顿州西雅图，2020，页码 11540-11549，doi: 10.1109/CVPR42600.2020.01156。'
- en: '[305] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen and K. C. Tan, ”A Survey
    on Evolutionary Neural Architecture Search,” in IEEE Transactions on Neural Networks
    and Learning Systems, vol. 34, no. 2, pp. 550-570, Feb. 2023, doi: 10.1109/TNNLS.2021.3100554.'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen 和 K. C. Tan，”进化神经架构搜索综述，”
    发表在 IEEE 神经网络与学习系统汇刊，第 34 卷，第 2 期，页码 550-570，2023年2月，doi: 10.1109/TNNLS.2021.3100554。'
- en: '[306] K. T. Chitty-Venkata, M. Emani, V. Vishwanath and A. K. Somani, ”Neural
    Architecture Search Benchmarks: Insights and Survey,” in IEEE Access, vol. 11,
    pp. 25217-25236, 2023, doi: 10.1109/ACCESS.2023.3253818.'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] K. T. Chitty-Venkata, M. Emani, V. Vishwanath 和 A. K. Somani，”神经架构搜索基准：洞察与综述，”
    发表在 IEEE Access，第 11 卷，页码 25217-25236，2023，doi: 10.1109/ACCESS.2023.3253818。'
- en: '[307] Y. Weng, T. Zhou, L. Liu and C. Xia, ”Automatic Convolutional Neural
    Architecture Search for Image Classification Under Different Scenes,” in IEEE
    Access, vol. 7, pp. 38495-38506, 2019, doi: 10.1109/ACCESS.2019.2906369.'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] Y. Weng, T. Zhou, L. Liu 和 C. Xia，”不同场景下的图像分类自动卷积神经架构搜索，” 发表在 IEEE Access，第
    7 卷，页码 38495-38506，2019，doi: 10.1109/ACCESS.2019.2906369。'
- en: '[308] X. Zheng et al., ”MIGO-NAS: Towards Fast and Generalizable Neural Architecture
    Search,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
    43, no. 9, pp. 2936-2952, 1 Sept. 2021, doi: 10.1109/TPAMI.2021.3065138.'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] X. Zheng 等，”MIGO-NAS: 面向快速和通用神经架构搜索，” 发表在 IEEE 模式分析与机器智能汇刊，第 43 卷，第 9
    期，页码 2936-2952，2021年9月1日，doi: 10.1109/TPAMI.2021.3065138。'
- en: '[309] C. Wei, C. Niu, Y. Tang, Y. Wang, H. Hu and J. Liang, ”NPENAS: Neural
    Predictor Guided Evolution for Neural Architecture Search,” in IEEE Transactions
    on Neural Networks and Learning Systems, vol. 34, no. 11, pp. 8441-8455, Nov.
    2023, doi: 10.1109/TNNLS.2022.3151160.'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] C. Wei, C. Niu, Y. Tang, Y. Wang, H. Hu 和 J. Liang，”NPENAS: 基于神经预测器的神经架构搜索演化，”
    发表在 IEEE 神经网络与学习系统汇刊，第 34 卷，第 11 期，页码 8441-8455，2023年11月，doi: 10.1109/TNNLS.2022.3151160。'
- en: '[310] Y. Li, S. Tang, R. Zhang, Y. Zhang, J. Li and S. Yan, ”Asymmetric GAN
    for Unpaired Image-to-Image Translation,” in IEEE Transactions on Image Processing,
    vol. 28, no. 12, pp. 5881-5896, Dec. 2019, doi: 10.1109/TIP.2019.2922854.'
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] Y. Li, S. Tang, R. Zhang, Y. Zhang, J. Li 和 S. Yan，”非对称 GAN 用于无配对图像到图像转换，”
    发表在 IEEE 图像处理汇刊，第 28 卷，第 12 期，页码 5881-5896，2019年12月，doi: 10.1109/TIP.2019.2922854。'
- en: '[311] C. D. Prakash and L. J. Karam, ”It GAN Do Better: GAN-Based Detection
    of Objects on Images With Varying Quality,” in IEEE Transactions on Image Processing,
    vol. 30, pp. 9220-9230, 2021, doi: 10.1109/TIP.2021.3124155.'
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] C. D. Prakash 和 L. J. Karam，“它 GAN 能做得更好：基于 GAN 的图像对象检测”，发表于 IEEE 图像处理汇刊，卷
    30，第 9220-9230 页，2021 年，doi: 10.1109/TIP.2021.3124155。'
- en: '[312] Y. Huang, F. Zheng, R. Cong, W. Huang, M. R. Scott and L. Shao, ”MCMT-GAN:
    Multi-Task Coherent Modality Transferable GAN for 3D Brain Image Synthesis,” in
    IEEE Transactions on Image Processing, vol. 29, pp. 8187-8198, 2020, doi: 10.1109/TIP.2020.3011557.'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] Y. Huang, F. Zheng, R. Cong, W. Huang, M. R. Scott 和 L. Shao，“MCMT-GAN：用于
    3D 脑部图像合成的多任务一致模态可转移 GAN”，发表于 IEEE 图像处理汇刊，卷 29，第 8187-8198 页，2020 年，doi: 10.1109/TIP.2020.3011557。'
- en: '[313] T. Chen, Y. Zhang, X. Huo, S. Wu, Y. Xu and H. S. Wong, ”SphericGAN:
    Semi-supervised Hyper-spherical Generative Adversarial Networks for Fine-grained
    Image Synthesis,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), New Orleans, LA, USA, 2022, pp. 9991-10000, doi: 10.1109/CVPR52688.2022.00976.'
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] T. Chen, Y. Zhang, X. Huo, S. Wu, Y. Xu 和 H. S. Wong，“SphericGAN：用于细粒度图像合成的半监督超球面生成对抗网络”，2022
    IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国新奥尔良，2022 年，第 9991-10000 页，doi: 10.1109/CVPR52688.2022.00976。'
- en: '[314] Z. Liu et al., ”Fine-Grained Face Swapping Via Regional GAN Inversion,”
    2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver,
    BC, Canada, 2023, pp. 8578-8587, doi: 10.1109/CVPR52729.2023.00829.'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] Z. Liu 等人，“通过区域 GAN 反演进行细粒度面部交换”，2023 IEEE/CVF 计算机视觉与模式识别会议（CVPR），加拿大温哥华，2023
    年，第 8578-8587 页，doi: 10.1109/CVPR52729.2023.00829。'
- en: '[315] Y. Dong, W. Tan, D. Tao, L. Zheng, and X. Li, ”CartoonLossGAN: Learning
    Surface and Coloring of Images for Cartoonization,” in IEEE Transactions on Image
    Processing, vol. 31, pp. 485-498, 2022, doi: 10.1109/TIP.2021.3130539.'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] Y. Dong, W. Tan, D. Tao, L. Zheng 和 X. Li，“CartoonLossGAN：学习图像的表面和着色以实现卡通化”，发表于
    IEEE 图像处理汇刊，卷 31，第 485-498 页，2022 年，doi: 10.1109/TIP.2021.3130539。'
- en: '[316] Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan and Y. Zheng, ”Recent Progress
    on Generative Adversarial Networks (GANs): A Survey,” in IEEE Access, vol. 7,
    pp. 36322-36333, 2019, doi: 10.1109/ACCESS.2019.2905015.'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan 和 Y. Zheng，“生成对抗网络（GAN）的最新进展：一项综述”，发表于
    IEEE Access，卷 7，第 36322-36333 页，2019 年，doi: 10.1109/ACCESS.2019.2905015。'
- en: '[317] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-Time
    Object Detection with Region Proposal Networks,” arXiv (Cornell University), Jun.
    2015, doi: 10.48550/arxiv.1506.01497.'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] S. Ren, K. He, R. Girshick 和 J. Sun，“Faster R-CNN：基于区域提议网络的实时目标检测”，arXiv（康奈尔大学），2015
    年 6 月，doi: 10.48550/arxiv.1506.01497。'
- en: '[318] R. Joseph, Divvala Santosh, G. Ross, and F. Ali, “You Only Look Once:
    Unified, Real-Time Object Detection,” arXiv (Cornell University), Jan. 2016, doi:
    10.48550/arxiv.1506.02640.'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] R. Joseph, Divvala Santosh, G. Ross 和 F. Ali，“你只看一次：统一的实时目标检测”，arXiv（康奈尔大学），2016
    年 1 月，doi: 10.48550/arxiv.1506.02640。'
- en: '[319] J. Terven, D.-M. Córdova-Esparza, and J.-A. Romero-González, “A Comprehensive
    Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS,”
    Machine Learning and Knowledge Extraction, vol. 5, no. 4, pp. 1680–1716, Dec.
    2023, doi: 10.3390/make5040083.'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] J. Terven, D.-M. Córdova-Esparza 和 J.-A. Romero-González，“计算机视觉中 YOLO
    架构的全面回顾：从 YOLOv1 到 YOLOv8 和 YOLO-NAS”，机器学习与知识提取，卷 5，第 4 期，第 1680-1716 页，2023 年
    12 月，doi: 10.3390/make5040083。'
- en: '[320] Y. Lu, L. Zhang and W. Xie, ”YOLO-compact: An Efficient YOLO Network
    for Single Category Real-time Object Detection,” 2020 Chinese Control And Decision
    Conference (CCDC), Hefei, China, 2020, pp. 1931-1936, doi: 10.1109/CCDC49329.2020.9164580.'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] Y. Lu, L. Zhang 和 W. Xie，“YOLO-compact：一种高效的 YOLO 网络用于单类别实时目标检测”，2020
    中国控制与决策会议（CCDC），中国合肥，2020 年，第 1931-1936 页，doi: 10.1109/CCDC49329.2020.9164580。'
- en: '[321] T. -H. Wu, T. -W. Wang and Y. -Q. Liu, ”Real-Time Vehicle and Distance
    Detection Based on Improved Yolo v5 Network,” 2021 3rd World Symposium on Artificial
    Intelligence (WSAI), Guangzhou, China, 2021, pp. 24-28, doi: 10.1109/WSAI51899.2021.9486316.'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] T. -H. Wu, T. -W. Wang 和 Y. -Q. Liu，“基于改进的 Yolo v5 网络的实时车辆及距离检测”，2021
    第三届世界人工智能研讨会（WSAI），中国广州，2021 年，第 24-28 页，doi: 10.1109/WSAI51899.2021.9486316。'
- en: '[322] H. Yu, Y. Li and D. Zhang, ”An Improved YOLO v3 Small-Scale Ship Target
    Detection Algorithm,” 2021 6th International Conference on Smart Grid and Electrical
    Automation (ICSGEA), Kunming, China, 2021, pp. 560-563, doi: 10.1109/ICSGEA53208.2021.00132.'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] H. Yu, Y. Li 和 D. Zhang，“改进的 YOLO v3 小规模船只目标检测算法”，2021 第六届智能电网与电气自动化国际会议（ICSGEA），中国昆明，2021
    年，第 560-563 页，doi: 10.1109/ICSGEA53208.2021.00132。'
- en: '[323] W. Lan, J. Dang, Y. Wang and S. Wang, ”Pedestrian Detection Based on
    YOLO Network Model,” 2018 IEEE International Conference on Mechatronics and Automation
    (ICMA), Changchun, China, 2018, pp. 1547-1551, doi: 10.1109/ICMA.2018.8484698.'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] W. Lan, J. Dang, Y. Wang 和 S. Wang，”基于 YOLO 网络模型的行人检测，” 2018 IEEE 机械电子与自动化国际会议（ICMA），中国长春，2018
    年，页码 1547-1551，doi: 10.1109/ICMA.2018.8484698。'
- en: '[324] J. -H. Kim, N. Kim and C. S. Won, ”High-Speed Drone Detection Based On
    Yolo-V8,” ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-2, doi: 10.1109/ICASSP49357.2023.10095516.'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] J. -H. Kim, N. Kim 和 C. S. Won，”基于 Yolo-V8 的高速无人机检测，” ICASSP 2023 - 2023
    IEEE 国际声学、语音与信号处理大会（ICASSP），希腊罗德岛，2023 年，页码 1-2，doi: 10.1109/ICASSP49357.2023.10095516。'
- en: '[325] Z. Li, Z. Liu and X. Wang, ”On-Board Real-Time Pedestrian Detection for
    Micro Unmanned Aerial Vehicles Based on YOLO-v8,” 2023 2nd International Conference
    on Machine Learning, Cloud Computing and Intelligent Mining (MLCCIM), Jiuzhaigou,
    China, 2023, pp. 250-255, doi: 10.1109/MLCCIM60412.2023.00042.'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] Z. Li, Z. Liu 和 X. Wang，”基于 YOLO-v8 的微型无人机实时行人检测，” 2023 第二届国际机器学习、云计算与智能挖掘大会（MLCCIM），中国九寨沟，2023
    年，页码 250-255，doi: 10.1109/MLCCIM60412.2023.00042。'
- en: '[326] N. Madhasu and S. D. Pande, ”Chrometect GAYO: Classification and Colorization
    using PIX2PIX and YOLOV8,” 2023 7th International Conference on Computation System
    and Information Technology for Sustainable Solutions (CSITSS), Bangalore, India,
    2023, pp. 1-6, doi: 10.1109/CSITSS60515.2023.10384657.'
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] N. Madhasu 和 S. D. Pande，”Chrometect GAYO：使用 PIX2PIX 和 YOLOV8 的分类与着色，”
    2023 第七届国际计算系统与可持续解决方案信息技术大会（CSITSS），印度班加罗尔，2023 年，页码 1-6，doi: 10.1109/CSITSS60515.2023.10384657。'
- en: '[327] Y. Wang, H. Wang and Z. Xin, ”Efficient Detection Model of Steel Strip
    Surface Defects Based on YOLO-V7,” in IEEE Access, vol. 10, pp. 133936-133944,
    2022, doi: 10.1109/ACCESS.2022.3230894.'
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] Y. Wang, H. Wang 和 Z. Xin，”基于 YOLO-V7 的钢带表面缺陷高效检测模型，” 载于 IEEE Access，卷
    10，页码 133936-133944，2022 年，doi: 10.1109/ACCESS.2022.3230894。'
- en: '[328] F. Fang, L. Li, H. Zhu and J. -H. Lim, ”Combining Faster R-CNN and Model-Driven
    Clustering for Elongated Object Detection,” in IEEE Transactions on Image Processing,
    vol. 29, pp. 2052-2065, 2020, doi: 10.1109/TIP.2019.2947792.'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] F. Fang, L. Li, H. Zhu 和 J. -H. Lim，”将 Faster R-CNN 与模型驱动的聚类结合用于长条形物体检测，”
    载于 IEEE 图像处理汇刊，卷 29，页码 2052-2065，2020 年，doi: 10.1109/TIP.2019.2947792。'
- en: '[329] X. Chen et al., ”Fast and Accurate Craniomaxillofacial Landmark Detection
    via 3D Faster R-CNN,” in IEEE Transactions on Medical Imaging, vol. 40, no. 12,
    pp. 3867-3878, Dec. 2021, doi: 10.1109/TMI.2021.3099509.'
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] X. Chen 等人，”通过 3D Faster R-CNN 实现快速准确的颅面标志检测，” 载于 IEEE 医学成像汇刊，卷 40，第
    12 期，页码 3867-3878，2021 年 12 月，doi: 10.1109/TMI.2021.3099509。'
- en: '[330] S. -L. Chen et al., ”Detection of Various Dental Conditions on Dental
    Panoramic Radiography Using Faster R-CNN,” in IEEE Access, vol. 11, pp. 127388-127401,
    2023, doi: 10.1109/ACCESS.2023.3332269.'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] S. -L. Chen 等人，”利用 Faster R-CNN 检测牙科全景 X 光影像中的各种牙科状况，” 载于 IEEE Access，卷
    11，页码 127388-127401，2023 年，doi: 10.1109/ACCESS.2023.3332269。'
- en: '[331] F. Selamet, S. Cakar and M. Kotan, ”Automatic Detection and Classification
    of Defective Areas on Metal Parts by Using Adaptive Fusion of Faster R-CNN and
    Shape From Shading,” in IEEE Access, vol. 10, pp. 126030-126038, 2022, doi: 10.1109/ACCESS.2022.3224037.'
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] F. Selamet, S. Cakar 和 M. Kotan，”通过自适应融合 Faster R-CNN 和 Shape From Shading
    实现金属部件缺陷区域的自动检测与分类，” 载于 IEEE Access，卷 10，页码 126030-126038，2022 年，doi: 10.1109/ACCESS.2022.3224037。'
- en: '[332] A. Omid-Zohoor, C. Young, D. Ta and B. Murmann, ”Toward Always-On Mobile
    Object Detection: Energy Versus Performance Tradeoffs for Embedded HOG Feature
    Extraction,” in IEEE Transactions on Circuits and Systems for Video Technology,
    vol. 28, no. 5, pp. 1102-1115, May 2018, doi: 10.1109/TCSVT.2017.2653187.'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] A. Omid-Zohoor, C. Young, D. Ta 和 B. Murmann，”面向始终在线的移动物体检测：嵌入式 HOG 特征提取的能量与性能权衡，”
    载于 IEEE 视听技术电路与系统汇刊，卷 28，第 5 期，页码 1102-1115，2018 年 5 月，doi: 10.1109/TCSVT.2017.2653187。'
- en: '[333] Y. Guo and B. Yang, ”A Survey of Semantic Segmentation Methods in Traffic
    Scenarios,” 2022 International Conference on Machine Learning, Cloud Computing
    and Intelligent Mining (MLCCIM), Xiamen, China, 2022, pp. 452-457, doi: 10.1109/MLCCIM55934.2022.00083.'
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] Y. Guo 和 B. Yang，”交通场景中的语义分割方法综述，” 2022 国际机器学习、云计算与智能挖掘大会（MLCCIM），中国厦门，2022
    年，页码 452-457，doi: 10.1109/MLCCIM55934.2022.00083。'
- en: '[334] X. Xu, S. Huang and H. Lai, ”Lightweight Semantic Segmentation Network
    Leveraging Class-Aware Contextual Information,” in IEEE Access, vol. 11, pp. 144722-144734,
    2023, doi: 10.1109/ACCESS.2023.3345790.'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] X. Xu, S. Huang 和 H. Lai， ”利用类别感知上下文信息的轻量级语义分割网络，” 发表在 IEEE Access，第11卷，第144722-144734页，2023年，doi:
    10.1109/ACCESS.2023.3345790。'
- en: '[335] S. Abdigapporov, S. Miraliev, V. Kakani and H. Kim, ”Joint Multiclass
    Object Detection and Semantic Segmentation for Autonomous Driving,” in IEEE Access,
    vol. 11, pp. 37637-37649, 2023, doi: 10.1109/ACCESS.2023.3266284.'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] S. Abdigapporov, S. Miraliev, V. Kakani 和 H. Kim， ”用于自动驾驶的联合多类别目标检测和语义分割，”
    发表在 IEEE Access，第11卷，第37637-37649页，2023年，doi: 10.1109/ACCESS.2023.3266284。'
- en: '[336] Y. Zheng, Y. Xu, S. Qiu, W. Li, G. Zhong and M. Sarem, ”A Novel Semantic
    Segmentation Algorithm for RGB-D Images Based on Non-Symmetry and Anti-Packing
    Pattern Representation Model,” in IEEE Access, vol. 11, pp. 36290-36299, 2023,
    doi: 10.1109/ACCESS.2023.3266251.'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] Y. Zheng, Y. Xu, S. Qiu, W. Li, G. Zhong 和 M. Sarem， ”基于非对称和抗打包模式表示模型的
    RGB-D 图像新型语义分割算法，” 发表在 IEEE Access，第11卷，第36290-36299页，2023年，doi: 10.1109/ACCESS.2023.3266251。'
- en: '[337] Z. Cao et al., ”Meta-Seg: A Generalized Meta-Learning Framework for Multi-Class
    Few-Shot Semantic Segmentation,” in IEEE Access, vol. 7, pp. 166109-166121, 2019,
    doi: 10.1109/ACCESS.2019.2953465.'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] Z. Cao 等， ”Meta-Seg：用于多类别少样本语义分割的广义元学习框架，” 发表在 IEEE Access，第7卷，第166109-166121页，2019年，doi:
    10.1109/ACCESS.2019.2953465。'
- en: '[338] F. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson, J. M. Alvarez
    and S. Gould, ”Incorporating Network Built-in Priors in Weakly-Supervised Semantic
    Segmentation,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 40, no. 6, pp. 1382-1396, 1 June 2018, doi: 10.1109/TPAMI.2017.2713785.'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] F. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson, J. M. Alvarez
    和 S. Gould， ”在弱监督语义分割中融入网络内建先验，” 发表在 IEEE Transactions on Pattern Analysis and
    Machine Intelligence，第40卷，第6期，第1382-1396页，2018年6月1日，doi: 10.1109/TPAMI.2017.2713785。'
- en: '[339] P. Anilkumar et al., ”An Adaptive DeepLabv3+ for Semantic Segmentation
    of Aerial Images Using Improved Golden Eagle Optimization Algorithm,” in IEEE
    Access, vol. 11, pp. 106688-106705, 2023, doi: 10.1109/ACCESS.2023.3318867.'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[339] P. Anilkumar 等， ”用于航空图像语义分割的自适应 DeepLabv3+ 及改进的金鹰优化算法，” 发表在 IEEE Access，第11卷，第106688-106705页，2023年，doi:
    10.1109/ACCESS.2023.3318867。'
- en: '[340] W. Zhou, J. Liu, J. Lei, L. Yu and J. -N. Hwang, ”GMNet: Graded-Feature
    Multilabel-Learning Network for RGB-Thermal Urban Scene Semantic Segmentation,”
    in IEEE Transactions on Image Processing, vol. 30, pp. 7790-7802, 2021, doi: 10.1109/TIP.2021.3109518.'
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[340] W. Zhou, J. Liu, J. Lei, L. Yu 和 J. -N. Hwang， ”GMNet：用于 RGB-热图城市场景语义分割的分级特征多标签学习网络，”
    发表在 IEEE Transactions on Image Processing，第30卷，第7790-7802页，2021年，doi: 10.1109/TIP.2021.3109518。'
- en: '[341] A. Sohail, N. A. Nawaz, A. A. Shah, S. Rasheed, S. Ilyas and M. K. Ehsan,
    ”A Systematic Literature Review on Machine Learning and Deep Learning Methods
    for Semantic Segmentation,” in IEEE Access, vol. 10, pp. 134557-134570, 2022,
    doi: 10.1109/ACCESS.2022.3230983.'
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[341] A. Sohail, N. A. Nawaz, A. A. Shah, S. Rasheed, S. Ilyas 和 M. K. Ehsan，
    ”关于机器学习和深度学习方法用于语义分割的系统文献综述，” 发表在 IEEE Access，第10卷，第134557-134570页，2022年，doi:
    10.1109/ACCESS.2022.3230983。'
- en: '[342] W. Ji et al., ”Multispectral Video Semantic Segmentation: A Benchmark
    Dataset and Baseline,” 2023 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 1094-1104, doi: 10.1109/CVPR52729.2023.00112.'
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[342] W. Ji 等， ”多光谱视频语义分割：基准数据集和基线，” 2023年 IEEE/CVF 计算机视觉与模式识别会议（CVPR），加拿大温哥华，2023年，第1094-1104页，doi:
    10.1109/CVPR52729.2023.00112。'
- en: '[343] J. Zhang, X. Zhao, Z. Chen and Z. Lu, ”A Review of Deep Learning-Based
    Semantic Segmentation for Point Cloud,” in IEEE Access, vol. 7, pp. 179118-179133,
    2019, doi: 10.1109/ACCESS.2019.2958671.'
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[343] J. Zhang, X. Zhao, Z. Chen 和 Z. Lu， ”基于深度学习的点云语义分割综述，” 发表在 IEEE Access，第7卷，第179118-179133页，2019年，doi:
    10.1109/ACCESS.2019.2958671。'
- en: '[344] B. Woo and M. Lee, ”Comparison of tissue segmentation performance between
    2D U-Net and 3D U-Net on brain MR Images,” 2021 International Conference on Electronics,
    Information, and Communication (ICEIC), Jeju, Korea (South), 2021, pp. 1-4, doi:
    10.1109/ICEIC51217.2021.9369797.'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[344] B. Woo 和 M. Lee， ”2D U-Net 和 3D U-Net 在脑 MR 图像上的组织分割性能比较，” 2021年电子信息与通信国际会议（ICEIC），韩国济州，2021年，第1-4页，doi:
    10.1109/ICEIC51217.2021.9369797。'
- en: '[345] P. Harsh, R. Chakraborty, S. Tripathi and K. Sharma, ”Attention U-Net
    Architecture for Dental Image Segmentation,” 2021 International Conference on
    Intelligent Technologies (CONIT), Hubli, India, 2021, pp. 1-5, doi: 10.1109/CONIT51480.2021.9498422.'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[345] P. Harsh, R. Chakraborty, S. Tripathi 和 K. Sharma， ”用于牙科图像分割的 Attention
    U-Net 架构，” 2021年智能技术国际会议（CONIT），印度赫布利，2021年，第1-5页，doi: 10.1109/CONIT51480.2021.9498422。'
- en: '[346] G. B. Kande et al., ”MSR U-Net: An Improved U-Net Model for Retinal Blood
    Vessel Segmentation,” in IEEE Access, vol. 12, pp. 534-551, 2024, doi: 10.1109/ACCESS.2023.3347196.'
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[346] G. B. Kande 等，“MSR U-Net: 一种改进的 U-Net 模型用于视网膜血管分割，”发表于 IEEE Access，第
    12 卷，第 534-551 页，2024 年，doi: 10.1109/ACCESS.2023.3347196。'
- en: '[347] N. Siddique, S. Paheding, C. P. Elkin and V. Devabhaktuni, ”U-Net and
    Its Variants for Medical Image Segmentation: A Review of Theory and Applications,”
    in IEEE Access, vol. 9, pp. 82031-82057, 2021, doi: 10.1109/ACCESS.2021.3086020.'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[347] N. Siddique, S. Paheding, C. P. Elkin 和 V. Devabhaktuni，“U-Net 及其变体在医学图像分割中的应用：理论与应用综述，”发表于
    IEEE Access，第 9 卷，第 82031-82057 页，2021 年，doi: 10.1109/ACCESS.2021.3086020。'
- en: '[348] Y. -J. Huang et al., ”HL-FCN: Hybrid loss guided FCN for colorectal cancer
    segmentation,” 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
    2018), Washington, DC, USA, 2018, pp. 195-198, doi: 10.1109/ISBI.2018.8363553.'
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[348] Y. -J. Huang 等，“HL-FCN: 混合损失引导的 FCN 用于结直肠癌分割，”2018 IEEE 第 15 届生物医学成像国际研讨会
    (ISBI 2018)，美国华盛顿特区，2018 年，第 195-198 页，doi: 10.1109/ISBI.2018.8363553。'
- en: '[349] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid Scene Parsing Network,”
    arXiv.org, Apr. 27, 2017.'
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[349] H. Zhao, J. Shi, X. Qi, X. Wang 和 J. Jia，“金字塔场景解析网络，”arXiv.org，2017 年
    4 月 27 日。'
- en: '[350] H. Tang, ”Vision Question Answering System Based on Roberta and Vit Model,”
    2022 International Conference on Image Processing, Computer Vision and Machine
    Learning (ICICML), Xi’an, China, 2022, pp. 258-261, doi: 10.1109/ICICML57342.2022.10009711.'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[350] H. Tang，“基于 Roberta 和 Vit 模型的视觉问答系统，”2022 年国际图像处理、计算机视觉和机器学习会议 (ICICML)，中国西安，2022
    年，第 258-261 页，doi: 10.1109/ICICML57342.2022.10009711。'
- en: '[351] F. Bao et al., ”All are Worth Words: A ViT Backbone for Diffusion Models,”
    2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver,
    BC, Canada, 2023, pp. 22669-22679, doi: 10.1109/CVPR52729.2023.02171.'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[351] F. Bao 等，“所有都值得一提：一种用于扩散模型的 ViT 骨干网，”2023 IEEE/CVF 计算机视觉与模式识别会议 (CVPR)，加拿大温哥华，2023
    年，第 22669-22679 页，doi: 10.1109/CVPR52729.2023.02171。'
- en: '[352] Z. Li and Q. Gu, ”I-ViT: Integer-only Quantization for Efficient Vision
    Transformer Inference,” 2023 IEEE/CVF International Conference on Computer Vision
    (ICCV), Paris, France, 2023, pp. 17019-17029, doi: 10.1109/ICCV51070.2023.01565.'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[352] Z. Li 和 Q. Gu，“I-ViT: 高效视觉变换器推理的整数量化，”2023 IEEE/CVF 计算机视觉国际会议 (ICCV)，法国巴黎，2023
    年，第 17019-17029 页，doi: 10.1109/ICCV51070.2023.01565。'
- en: '[353] H. Dong, C. Chen, J. Wang, F. Shen and Y. Pang, ”ViT-SAPS: Detail-Aware
    Transformer for Mechanical Assembly Semantic Segmentation,” in IEEE Access, vol.
    11, pp. 41467-41479, 2023, doi: 10.1109/ACCESS.2023.3270807.'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[353] H. Dong, C. Chen, J. Wang, F. Shen 和 Y. Pang，“ViT-SAPS: 细节感知变换器用于机械装配语义分割，”发表于
    IEEE Access，第 11 卷，第 41467-41479 页，2023 年，doi: 10.1109/ACCESS.2023.3270807。'
- en: '[354] L. Zou, Z. Huang, N. Gu and G. Wang, ”6D-ViT: Category-Level 6D Object
    Pose Estimation via Transformer-Based Instance Representation Learning,” in IEEE
    Transactions on Image Processing, vol. 31, pp. 6907-6921, 2022, doi: 10.1109/TIP.2022.3216980.'
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[354] L. Zou, Z. Huang, N. Gu 和 G. Wang，“6D-ViT: 基于变换器的实例表示学习进行类别级 6D 物体姿势估计，”发表于
    IEEE Transactions on Image Processing，第 31 卷，第 6907-6921 页，2022 年，doi: 10.1109/TIP.2022.3216980。'
- en: '[355] H. Bao, L. Dong, and F. Wei, “BEiT: BERT Pre-Training of Image Transformers,”
    arXiv (Cornell University), Jun. 2021, doi: 10.48550/arxiv.2106.08254.'
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[355] H. Bao, L. Dong 和 F. Wei，“BEiT: 图像变换器的 BERT 预训练，”arXiv（康奈尔大学），2021 年
    6 月，doi: 10.48550/arxiv.2106.08254。'
- en: '[356] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, “Transformer in Transformer,”
    arXiv (Cornell University), Feb. 2021, doi: 10.48550/arxiv.2103.00112.'
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[356] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu 和 Y. Wang，“变换器中的变换器，”arXiv（康奈尔大学），2021
    年 2 月，doi: 10.48550/arxiv.2103.00112。'
- en: '[357] S. Mehta and M. Rastegari, “MobileViT: Light-weight, General-purpose,
    and Mobile-friendly Vision Transformer,” arXiv (Cornell University), Oct. 2021,
    doi: 10.48550/arxiv.2110.02178.'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[357] S. Mehta 和 M. Rastegari，“MobileViT: 轻量级、通用且适用于移动设备的视觉变换器，”arXiv（康奈尔大学），2021
    年 10 月，doi: 10.48550/arxiv.2110.02178。'
- en: '[358] Z. Hu, Z. Gan, W. Li, J. Z. Wen, D. Zhou and X. Wang, ”Two-Stage Model-Agnostic
    Meta-Learning With Noise Mechanism for One-Shot Imitation,” in IEEE Access, vol.
    8, pp. 182720-182730, 2020, doi: 10.1109/ACCESS.2020.3029220.'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[358] Z. Hu, Z. Gan, W. Li, J. Z. Wen, D. Zhou 和 X. Wang，“带噪声机制的两阶段模型无关元学习用于一次性模仿，”发表于
    IEEE Access，第 8 卷，第 182720-182730 页，2020 年，doi: 10.1109/ACCESS.2020.3029220。'
- en: '[359] L. Xie, Y. Yang, Z. Fu and S. M. Naqvi, ”One-Shot Medical Action Recognition
    With A Cross-Attention Mechanism And Dynamic Time Warping,” ICASSP 2023 - 2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10097186.'
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[359] L. Xie, Y. Yang, Z. Fu 和 S. M. Naqvi, “基于交叉注意力机制和动态时间规整的单次医学动作识别,” ICASSP
    2023 - 2023 IEEE 国际声学, 语音与信号处理会议 (ICASSP), 希腊罗德岛, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10097186。'
- en: '[360] C. Huang, Y. Dang, P. Chen, X. Yang and K. -T. Cheng, ”One-Shot Imitation
    Drone Filming of Human Motion Videos,” in IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. 44, no. 9, pp. 5335-5348, 1 Sept. 2022, doi: 10.1109/TPAMI.2021.3067359.'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[360] C. Huang, Y. Dang, P. Chen, X. Yang 和 K. -T. Cheng, “单次模仿无人机拍摄人体动作视频,”
    见于 IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no.
    9, pp. 5335-5348, 2022年9月1日, doi: 10.1109/TPAMI.2021.3067359。'
- en: '[361] S. K. Biswas and P. Milanfar, ”One Shot Detection with Laplacian Object
    and Fast Matrix Cosine Similarity,” in IEEE Transactions on Pattern Analysis and
    Machine Intelligence, vol. 38, no. 3, pp. 546-562, 1 March 2016, doi: 10.1109/TPAMI.2015.2453950.'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[361] S. K. Biswas 和 P. Milanfar, “基于拉普拉斯对象和快速矩阵余弦相似性的单次检测,” 见于 IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 38, no. 3, pp. 546-562, 2016年3月1日,
    doi: 10.1109/TPAMI.2015.2453950。'
- en: '[362] S. -. Y. Huang and W. -. Ta Chu, ”Searching by Generating: Flexible and
    Efficient One-Shot NAS with Architecture Generator,” 2021 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp.
    983-992, doi: 10.1109/CVPR46437.2021.00104.'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[362] S. -. Y. Huang 和 W. -. Ta Chu, “通过生成进行搜索: 灵活高效的单次 NAS 与架构生成器,” 2021 IEEE/CVF
    计算机视觉与模式识别会议 (CVPR), 美国田纳西州纳什维尔, 2021, pp. 983-992, doi: 10.1109/CVPR46437.2021.00104。'
- en: '[363] Y. Chen, T. Huang, Y. Niu, X. Ke and Y. Lin, ”Pose-Guided Spatial Alignment
    and Key Frame Selection for One-Shot Video-Based Person Re-Identification,” in
    IEEE Access, vol. 7, pp. 78991-79004, 2019, doi: 10.1109/ACCESS.2019.2922679.'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[363] Y. Chen, T. Huang, Y. Niu, X. Ke 和 Y. Lin, “姿态引导的空间对齐和关键帧选择用于单次视频人重识别,”
    见于 IEEE Access, vol. 7, pp. 78991-79004, 2019, doi: 10.1109/ACCESS.2019.2922679。'
- en: '[364] S. K. Roy, P. Kar, M. E. Paoletti, J. M. Haut, R. Pastor-Vargas and A.
    Robles-Gómez, ”SiCoDeF² Net: Siamese Convolution Deconvolution Feature Fusion
    Network for One-Shot Classification,” in IEEE Access, vol. 9, pp. 118419-118434,
    2021, doi: 10.1109/ACCESS.2021.3107626.'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[364] S. K. Roy, P. Kar, M. E. Paoletti, J. M. Haut, R. Pastor-Vargas 和 A.
    Robles-Gómez, “SiCoDeF² Net: 孪生卷积解卷积特征融合网络用于单次分类,” 见于 IEEE Access, vol. 9, pp.
    118419-118434, 2021, doi: 10.1109/ACCESS.2021.3107626。'
- en: '[365] Y. Song, T. Wang, Subrota Kumar Mondal, and Jyoti Prakash Sahoo, “A Comprehensive
    Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities,”
    arXiv (Cornell University), May 2022, doi: 10.48550/arxiv.2205.06743.'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[365] Y. Song, T. Wang, Subrota Kumar Mondal 和 Jyoti Prakash Sahoo, “少样本学习的综合调查:
    发展, 应用, 挑战和机遇,” arXiv (康奈尔大学), 2022年5月, doi: 10.48550/arxiv.2205.06743。'
- en: '[366] Y. Wang, Q. Yao, J. Kwok, and L. M. Ni, “Generalizing from a Few Examples:
    A Survey on Few-Shot Learning,” arXiv.org, Mar. 29, 2020.'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[366] Y. Wang, Q. Yao, J. Kwok 和 L. M. Ni, “从少量示例中泛化: 少样本学习综述,” arXiv.org,
    2020年3月29日。'
- en: '[367] W. Wang, V. W. Zheng, H. Yu, and C. Miao, “A Survey of Zero-Shot Learning,”
    ACM Transactions on Intelligent Systems and Technology, vol. 10, no. 2, pp. 1–37,
    Jan. 2019, doi: 10.1145/3293318.'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[367] W. Wang, V. W. Zheng, H. Yu 和 C. Miao, “零样本学习综述,” ACM Transactions on
    Intelligent Systems and Technology, vol. 10, no. 2, pp. 1–37, 2019年1月, doi: 10.1145/3293318。'
- en: '[368] R. Xu et al., ”GCT: Graph Co-Training for Semi-Supervised Few-Shot Learning,”
    in IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no.
    12, pp. 8674-8687, Dec. 2022, doi: 10.1109/TCSVT.2022.3196550.'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[368] R. Xu 等, “GCT: 图协同训练用于半监督少样本学习,” 见于 IEEE Transactions on Circuits and
    Systems for Video Technology, vol. 32, no. 12, pp. 8674-8687, 2022年12月, doi: 10.1109/TCSVT.2022.3196550。'
- en: '[369] G. Cheng, C. Lang and J. Han, ”Holistic Prototype Activation for Few-Shot
    Segmentation,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 45, no. 4, pp. 4650-4666, 1 April 2023, doi: 10.1109/TPAMI.2022.3193587.'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[369] G. Cheng, C. Lang 和 J. Han, “全面原型激活用于少样本分割,” 见于 IEEE Transactions on
    Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4650-4666, 2023年4月1日,
    doi: 10.1109/TPAMI.2022.3193587。'
- en: '[370] L. Zhu and Y. Yang, ”Label Independent Memory for Semi-Supervised Few-Shot
    Video Classification,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 44, no. 1, pp. 273-285, 1 Jan. 2022, doi: 10.1109/TPAMI.2020.3007511.'
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[370] L. Zhu 和 Y. Yang, “标签独立记忆用于半监督少样本视频分类,” 见于 IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 44, no. 1, pp. 273-285, 2022年1月1日, doi:
    10.1109/TPAMI.2020.3007511。'
- en: '[371] Z. Yang, C. Zhang, R. Li, Y. Xu and G. Lin, ”Efficient Few-Shot Object
    Detection via Knowledge Inheritance,” in IEEE Transactions on Image Processing,
    vol. 32, pp. 321-334, 2023, doi: 10.1109/TIP.2022.3228162.'
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[371] Z. Yang, C. Zhang, R. Li, Y. Xu 和 G. Lin，”通过知识继承的高效少样本物体检测，”发表于 IEEE
    图像处理汇刊，第 32 卷，第 321-334 页，2023 年，doi: 10.1109/TIP.2022.3228162。'
- en: '[372] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li and J. Jia, ”Prior Guided Feature
    Enrichment Network for Few-Shot Segmentation,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 44, no. 2, pp. 1050-1065, 1 Feb. 2022,
    doi: 10.1109/TPAMI.2020.3013717.'
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[372] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li 和 J. Jia，”基于先验引导的特征增强网络用于少样本分割，”发表于
    IEEE 模式分析与机器智能汇刊，第 44 卷，第 2 期，第 1050-1065 页，2022 年 2 月 1 日，doi: 10.1109/TPAMI.2020.3013717。'
- en: '[373] C. Yan, X. Chang, M. Luo, H. Liu, X. Zhang and Q. Zheng, ”Semantics-Guided
    Contrastive Network for Zero-Shot Object detection,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2021.3140070.'
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[373] C. Yan, X. Chang, M. Luo, H. Liu, X. Zhang 和 Q. Zheng，”语义引导的对比网络用于零样本物体检测，”发表于
    IEEE 模式分析与机器智能汇刊，doi: 10.1109/TPAMI.2021.3140070。'
- en: '[374] S. Chen et al., ”GNDAN: Graph Navigated Dual Attention Network for Zero-Shot
    Learning,” in IEEE Transactions on Neural Networks and Learning Systems, doi:
    10.1109/TNNLS.2022.3155602.'
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[374] S. Chen 等人，”GNDAN: 图导航双重注意力网络用于零样本学习，”发表于 IEEE 神经网络与学习系统汇刊，doi: 10.1109/TNNLS.2022.3155602。'
- en: '[375] P. Ma, Z. He, W. Ran and H. Lu, ”A Transferable Generative Framework
    for Multi-Label Zero-Shot Learning,” in IEEE Transactions on Circuits and Systems
    for Video Technology, doi: 10.1109/TCSVT.2023.3324648.'
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[375] P. Ma, Z. He, W. Ran 和 H. Lu，”多标签零样本学习的可迁移生成框架，”发表于 IEEE 视频技术电路与系统汇刊，doi:
    10.1109/TCSVT.2023.3324648。'
- en: '[376] L. Feng and C. Zhao, ”Transfer Increment for Generalized Zero-Shot Learning,”
    in IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 6,
    pp. 2506-2520, June 2021, doi: 10.1109/TNNLS.2020.3006322.'
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[376] L. Feng 和 C. Zhao，”用于广义零样本学习的迁移增量，”发表于 IEEE 神经网络与学习系统汇刊，第 32 卷，第 6 期，第
    2506-2520 页，2021 年 6 月，doi: 10.1109/TNNLS.2020.3006322。'
- en: '[377] Y. Fu, T. M. Hospedales, T. Xiang and S. Gong, ”Transductive Multi-View
    Zero-Shot Learning,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 37, no. 11, pp. 2332-2345, 1 Nov. 2015, doi: 10.1109/TPAMI.2015.2408354.'
  id: totrans-1066
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[377] Y. Fu, T. M. Hospedales, T. Xiang 和 S. Gong，”传导性多视图零样本学习，”发表于 IEEE 模式分析与机器智能汇刊，第
    37 卷，第 11 期，第 2332-2345 页，2015 年 11 月 1 日，doi: 10.1109/TPAMI.2015.2408354。'
- en: '[378] C. Gong, J. Yang, J. You and M. Sugiyama, ”Centroid Estimation With Guaranteed
    Efficiency: A General Framework for Weakly Supervised Learning,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 44, no. 6, pp. 2841-2855, 1
    June 2022, doi: 10.1109/TPAMI.2020.3044997.'
  id: totrans-1067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[378] C. Gong, J. Yang, J. You 和 M. Sugiyama，”具有保证效率的中心点估计：一个用于弱监督学习的一般框架，”发表于
    IEEE 模式分析与机器智能汇刊，第 44 卷，第 6 期，第 2841-2855 页，2022 年 6 月 1 日，doi: 10.1109/TPAMI.2020.3044997。'
- en: '[379] D. Ienco, Y. J. E. Gbodjo, R. Gaetano and R. Interdonato, ”Weakly Supervised
    Learning for Land Cover Mapping of Satellite Image Time Series via Attention-Based
    CNN,” in IEEE Access, vol. 8, pp. 179547-179560, 2020, doi: 10.1109/ACCESS.2020.3024133.'
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[379] D. Ienco, Y. J. E. Gbodjo, R. Gaetano 和 R. Interdonato，”基于注意力的 CNN 的弱监督学习用于卫星图像时间序列的土地覆盖映射，”发表于
    IEEE Access，第 8 卷，第 179547-179560 页，2020 年，doi: 10.1109/ACCESS.2020.3024133。'
- en: '[380] J. Han, Y. Yang, D. Zhang, D. Huang, D. Xu and F. De La Torre, ”Weakly-Supervised
    Learning of Category-Specific 3D Object Shapes,” in IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 43, no. 4, pp. 1423-1437, 1 April 2021,
    doi: 10.1109/TPAMI.2019.2949562.'
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[380] J. Han, Y. Yang, D. Zhang, D. Huang, D. Xu 和 F. De La Torre，”类别特定三维物体形状的弱监督学习，”发表于
    IEEE 模式分析与机器智能汇刊，第 43 卷，第 4 期，第 1423-1437 页，2021 年 4 月 1 日，doi: 10.1109/TPAMI.2019.2949562。'
- en: '[381] Y. Feng, L. Wang and M. Zhang, ”Weakly-Supervised Learning of a Deep
    Convolutional Neural Networks for Semantic Segmentation,” in IEEE Access, vol.
    7, pp. 91009-91018, 2019, doi: 10.1109/ACCESS.2019.2926972.'
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[381] Y. Feng, L. Wang 和 M. Zhang，”用于语义分割的深度卷积神经网络的弱监督学习，”发表于 IEEE Access，第
    7 卷，第 91009-91018 页，2019 年，doi: 10.1109/ACCESS.2019.2926972。'
- en: '[382] A. Prest, C. Schmid and V. Ferrari, ”Weakly Supervised Learning of Interactions
    between Humans and Objects,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 34, no. 3, pp. 601-614, March 2012, doi: 10.1109/TPAMI.2011.158.'
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[382] A. Prest, C. Schmid 和 V. Ferrari，”人类与物体之间互动的弱监督学习，”发表于 IEEE 模式分析与机器智能汇刊，第
    34 卷，第 3 期，第 601-614 页，2012 年 3 月，doi: 10.1109/TPAMI.2011.158。'
- en: '[383] R. Cong et al., ”A Weakly Supervised Learning Framework for Salient Object
    Detection via Hybrid Labels,” in IEEE Transactions on Circuits and Systems for
    Video Technology, vol. 33, no. 2, pp. 534-548, Feb. 2023, doi: 10.1109/TCSVT.2022.3205182.'
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[383] R. Cong 等，“通过混合标签进行显著目标检测的弱监督学习框架”，发表于《IEEE 视频技术电路与系统汇刊》，第 33 卷，第 2 期，页码
    534-548，2023 年 2 月，doi: 10.1109/TCSVT.2022.3205182。'
- en: '[384] X. Zhou, A. Sun, Y. Liu, J. Zhang, and C. Miao, “SelfCF: A Simple Framework
    for Self-supervised Collaborative Filtering,” ACM Transactions on Recommender
    Systems, Apr. 2023, doi: 10.1145/3591469. ‌'
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[384] X. Zhou, A. Sun, Y. Liu, J. Zhang 和 C. Miao，“SelfCF：一个简单的自监督协同过滤框架”，发表于《ACM
    推荐系统汇刊》，2023 年 4 月，doi: 10.1145/3591469。'
- en: '[385] L. Tian, Z. Tu, D. Zhang, J. Liu, B. Li and J. Yuan, ”Unsupervised Learning
    of Optical Flow With CNN-Based Non-Local Filtering,” in IEEE Transactions on Image
    Processing, vol. 29, pp. 8429-8442, 2020, doi: 10.1109/TIP.2020.3013168.'
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[385] L. Tian, Z. Tu, D. Zhang, J. Liu, B. Li 和 J. Yuan，“基于 CNN 的非局部滤波的光流无监督学习”，发表于《IEEE
    图像处理汇刊》，第 29 卷，页码 8429-8442，2020 年，doi: 10.1109/TIP.2020.3013168。'
- en: '[386] Y. A. D. Djilali, T. Krishna, K. McGuinness and N. E. O’Connor, ”Rethinking
    360° Image Visual Attention Modelling with Unsupervised Learning,” 2021 IEEE/CVF
    International Conference on Computer Vision (ICCV), Montreal, QC, Canada, 2021,
    pp. 15394-15404, doi: 10.1109/ICCV48922.2021.01513.'
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[386] Y. A. D. Djilali, T. Krishna, K. McGuinness 和 N. E. O’Connor，“用无监督学习重新思考
    360° 图像视觉注意建模”，2021 IEEE/CVF 国际计算机视觉会议（ICCV），加拿大魁北克省蒙特利尔，2021 年，页码 15394-15404，doi:
    10.1109/ICCV48922.2021.01513。'
- en: '[387] J. Xu, Z. Zhang and X. Hu, ”Extracting Semantic Knowledge From GANs With
    Unsupervised Learning,” in IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 45, no. 8, pp. 9654-9668, Aug. 2023, doi: 10.1109/TPAMI.2023.3262140.'
  id: totrans-1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[387] J. Xu, Z. Zhang 和 X. Hu，“从 GANs 中提取语义知识的无监督学习”，发表于《IEEE 模式分析与机器智能汇刊》，第
    45 卷，第 8 期，页码 9654-9668，2023 年 8 月，doi: 10.1109/TPAMI.2023.3262140。'
- en: '[388] Y. Shan, H. S. Sawhney and R. Kumar, ”Unsupervised Learning of Discriminative
    Edge Measures for Vehicle Matching between Nonoverlapping Cameras,” in IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 30, no. 4, pp. 700-711, April
    2008, doi: 10.1109/TPAMI.2007.70728.'
  id: totrans-1077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[388] Y. Shan, H. S. Sawhney 和 R. Kumar，“用于非重叠摄像头之间车辆匹配的无监督学习的判别边缘度量”，发表于《IEEE
    模式分析与机器智能汇刊》，第 30 卷，第 4 期，页码 700-711，2008 年 4 月，doi: 10.1109/TPAMI.2007.70728。'
- en: '[389] W. Kim, A. Kanezaki and M. Tanaka, ”Unsupervised Learning of Image Segmentation
    Based on Differentiable Feature Clustering,” in IEEE Transactions on Image Processing,
    vol. 29, pp. 8055-8068, 2020, doi: 10.1109/TIP.2020.3011269.'
  id: totrans-1078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[389] W. Kim, A. Kanezaki 和 M. Tanaka，“基于可微特征聚类的图像分割无监督学习”，发表于《IEEE 图像处理汇刊》，第
    29 卷，页码 8055-8068，2020 年，doi: 10.1109/TIP.2020.3011269。'
- en: '[390] K. Song, J. Xie, S. Zhang and Z. Luo, ”Multi-Mode Online Knowledge Distillation
    for Self-Supervised Visual Representation Learning,” 2023 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023,
    pp. 11848-11857, doi: 10.1109/CVPR52729.2023.01140.'
  id: totrans-1079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[390] K. Song, J. Xie, S. Zhang 和 Z. Luo，“用于自监督视觉表示学习的多模态在线知识蒸馏”，2023 IEEE/CVF
    计算机视觉与模式识别会议（CVPR），加拿大不列颠哥伦比亚省温哥华，2023 年，页码 11848-11857，doi: 10.1109/CVPR52729.2023.01140。'
- en: '[391] S. Zare and H. Van Nguyen, ”Evaluating and Improving Domain Invariance
    in Contrastive Self-Supervised Learning by Extrapolating the Loss Function,” in
    IEEE Access, vol. 11, pp. 137758-137768, 2023, doi: 10.1109/ACCESS.2023.3339775.'
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[391] S. Zare 和 H. Van Nguyen，“通过外推损失函数评估和改进对比自监督学习中的领域不变性”，发表于《IEEE Access》，第
    11 卷，页码 137758-137768，2023 年，doi: 10.1109/ACCESS.2023.3339775。'
- en: '[392] R. Li, C. Zhang, G. Lin, Z. Wang and C. Shen, ”RigidFlow: Self-Supervised
    Scene Flow Learning on Point Clouds by Local Rigidity Prior,” 2022 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022,
    pp. 16938-16947, doi: 10.1109/CVPR52688.2022.01645.'
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[392] R. Li, C. Zhang, G. Lin, Z. Wang 和 C. Shen，“RigidFlow：通过局部刚性先验在点云上进行自监督场景流学习”，2022
    IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国路易斯安那州新奥尔良，2022 年，页码 16938-16947，doi: 10.1109/CVPR52688.2022.01645。'
- en: '[393] F. D. Pup and M. Atzori, ”Applications of Self-Supervised Learning to
    Biomedical Signals: A Survey,” in IEEE Access, vol. 11, pp. 144180-144203, 2023,
    doi: 10.1109/ACCESS.2023.3344531.'
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[393] F. D. Pup 和 M. Atzori，“自监督学习在生物医学信号中的应用：综述”，发表于《IEEE Access》，第 11 卷，页码
    144180-144203，2023 年，doi: 10.1109/ACCESS.2023.3344531。'
- en: '[394] L. Wang, X. Zhang, H. Su, and J. Zhu, “A Comprehensive Survey of Continual
    Learning: Theory, Method and Application,” arXiv.org, Jun. 10, 2023\. https://arxiv.org/abs/2302.00487
    (accessed Sep. 04, 2023).'
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[394] L. Wang, X. Zhang, H. Su 和 J. Zhu，“持续学习的全面调查：理论、方法与应用”，arXiv.org，2023
    年 6 月 10 日。 https://arxiv.org/abs/2302.00487（访问于 2023 年 9 月 4 日）。'
- en: '[395] J. He and F. Zhu, ”Out-Of-Distribution Detection In Unsupervised Continual
    Learning,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops (CVPRW), New Orleans, LA, USA, 2022, pp. 3849-3854, doi: 10.1109/CVPRW56347.2022.00430.'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[395] J. He 和 F. Zhu， “在无监督持续学习中的分布外检测，” 2022 IEEE/CVF 计算机视觉与模式识别研讨会 (CVPRW)，美国路易斯安那州新奥尔良，2022年，页码
    3849-3854，doi: 10.1109/CVPRW56347.2022.00430。'
- en: '[396] B. Kwon and T. Kim, ”Toward an Online Continual Learning Architecture
    for Intrusion Detection of Video Surveillance,” in IEEE Access, vol. 10, pp. 89732-89744,
    2022, doi: 10.1109/ACCESS.2022.3201139.'
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[396] B. Kwon 和 T. Kim， “面向视频监控入侵检测的在线持续学习架构，” 见 IEEE Access，卷 10，页码 89732-89744，2022年，doi:
    10.1109/ACCESS.2022.3201139。'
- en: '[397] Z. Cai, O. Sener and V. Koltun, ”Online Continual Learning with Natural
    Distribution Shifts: An Empirical Study with Visual Data,” 2021 IEEE/CVF International
    Conference on Computer Vision (ICCV), Montreal, QC, Canada, 2021, pp. 8261-8270,
    doi: 10.1109/ICCV48922.2021.00817.'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[397] Z. Cai, O. Sener 和 V. Koltun， “具有自然分布变化的在线持续学习：基于视觉数据的实证研究，” 2021 IEEE/CVF
    计算机视觉国际会议 (ICCV)，加拿大蒙特利尔，2021年，页码 8261-8270，doi: 10.1109/ICCV48922.2021.00817。'
- en: '[398] Y. Zhao, D. Saxena and J. Cao, ”AdaptCL: Adaptive Continual Learning
    for Tackling Heterogeneity in Sequential Datasets,” in IEEE Transactions on Neural
    Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3341841.'
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[398] Y. Zhao, D. Saxena 和 J. Cao， “AdaptCL：应对序列数据集异质性的自适应持续学习，” 见 IEEE 神经网络与学习系统汇刊，doi:
    10.1109/TNNLS.2023.3341841。'
- en: '[399] X. Li and W. Wang, ”GopGAN: Gradients Orthogonal Projection Generative
    Adversarial Network With Continual Learning,” in IEEE Transactions on Neural Networks
    and Learning Systems, vol. 34, no. 1, pp. 215-227, Jan. 2023, doi: 10.1109/TNNLS.2021.3093319.'
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[399] X. Li 和 W. Wang， “GopGAN：具有持续学习的梯度正交投影生成对抗网络，” 见 IEEE 神经网络与学习系统汇刊，卷 34，第
    1 期，页码 215-227，2023年1月，doi: 10.1109/TNNLS.2021.3093319。'
- en: '[400] M. De Lange et al., ”A Continual Learning Survey: Defying Forgetting
    in Classification Tasks,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 44, no. 7, pp. 3366-3385, 1 July 2022, doi: 10.1109/TPAMI.2021.3057446.'
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[400] M. De Lange 等， “持续学习调查：在分类任务中挑战遗忘，” 见 IEEE 模式分析与机器智能汇刊，卷 44，第 7 期，页码
    3366-3385，2022年7月1日，doi: 10.1109/TPAMI.2021.3057446。'
- en: '[401] L. Wang, B. Lei, Q. Li, H. Su, J. Zhu and Y. Zhong, ”Triple-Memory Networks:
    A Brain-Inspired Method for Continual Learning,” in IEEE Transactions on Neural
    Networks and Learning Systems, vol. 33, no. 5, pp. 1925-1934, May 2022, doi: 10.1109/TNNLS.2021.3111019.'
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[401] L. Wang, B. Lei, Q. Li, H. Su, J. Zhu 和 Y. Zhong， “Triple-Memory Networks：一种脑启发的持续学习方法，”
    见 IEEE 神经网络与学习系统汇刊，卷 33，第 5 期，页码 1925-1934，2022年5月，doi: 10.1109/TNNLS.2021.3111019。'
- en: '[402] M. Xue, H. Zhang, J. Song and M. Song, ”Meta-attention for ViT-backed
    Continual Learning,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), New Orleans, LA, USA, 2022, pp. 150-159, doi: 10.1109/CVPR52688.2022.00025.'
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[402] M. Xue, H. Zhang, J. Song 和 M. Song， “ViT支持的持续学习中的元注意力，” 2022 IEEE/CVF
    计算机视觉与模式识别会议 (CVPR)，美国路易斯安那州新奥尔良，2022年，页码 150-159，doi: 10.1109/CVPR52688.2022.00025。'
- en: '[403] X. Wang, L. Yao, X. Wang, H. -Y. Paik and S. Wang, ”Uncertainty Estimation
    With Neural Processes for Meta-Continual Learning,” in IEEE Transactions on Neural
    Networks and Learning Systems, vol. 34, no. 10, pp. 6887-6897, Oct. 2023, doi:
    10.1109/TNNLS.2022.3215633.'
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[403] X. Wang, L. Yao, X. Wang, H. -Y. Paik 和 S. Wang， “基于神经过程的元持续学习中的不确定性估计，”
    见 IEEE 神经网络与学习系统汇刊，卷 34，第 10 期，页码 6887-6897，2023年10月，doi: 10.1109/TNNLS.2022.3215633。'
- en: '[404] B. Sistaninejhad, H. Rasi, and P. Nayeri, “A Review Paper about Deep
    Learning for Medical Image Analysis,” Computational and Mathematical Methods in
    Medicine, vol. 2023, p. e7091301, May 2023, doi: 10.1155/2023/7091301.'
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[404] B. Sistaninejhad, H. Rasi, 和 P. Nayeri，“关于深度学习在医学图像分析中的综述论文，” 计算与数学医学方法，卷
    2023，页 e7091301，2023年5月，doi: 10.1155/2023/7091301。'
- en: '[405] Giorgos Papanastasiou, $\text{Ni}\kappa\text{o}\lambda\alpha\text{o}\varsigma\
    \Delta\text{i}\kappa\alpha\text{i}\text{o}\varsigma$, J. Huang, C. Wang, and G.
    Yang, “Is attention all you need in medical image analysis? A review,” arXiv (Cornell
    University), Jul. 2023, doi: 10.48550/arxiv.2307.12775.'
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[405] Giorgos Papanastasiou, $\text{Ni}\kappa\text{o}\lambda\alpha\text{o}\varsigma\
    \Delta\text{i}\kappa\alpha\text{i}\text{o}\varsigma$, J. Huang, C. Wang, 和 G.
    Yang， “在医学图像分析中注意力是否是你所需要的全部？一项综述，” arXiv (康奈尔大学)，2023年7月，doi: 10.48550/arxiv.2307.12775。'
- en: '[406] Z. M. C. Baum, Y. Hu and D. C. Barratt, ”Meta-Learning Initializations
    for Interactive Medical Image Registration,” in IEEE Transactions on Medical Imaging,
    vol. 42, no. 3, pp. 823-833, March 2023, doi: 10.1109/TMI.2022.3218147.'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[406] Z. M. C. Baum、Y. Hu 和 D. C. Barratt，“用于交互式医学图像配准的元学习初始化方法，”发表于《IEEE医学成像》,
    第42卷，第3期，页码823-833，2023年3月，doi: 10.1109/TMI.2022.3218147。'
- en: '[407] M. T. Irshad and H. U. Rehman, ”Gradient Compass-Based Adaptive Multimodal
    Medical Image Fusion,” in IEEE Access, vol. 9, pp. 22662-22670, 2021, doi: 10.1109/ACCESS.2021.3054843.'
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[407] M. T. Irshad 和 H. U. Rehman，“基于梯度指针的自适应多模态医学图像融合，”发表于《IEEE Access》, 第9卷，页码22662-22670，2021年，doi:
    10.1109/ACCESS.2021.3054843。'
- en: '[408] J. S. Duncan and N. Ayache, ”Medical image analysis: progress over two
    decades and the challenges ahead,” in IEEE Transactions on Pattern Analysis and
    Machine Intelligence, vol. 22, no. 1, pp. 85-106, Jan. 2000, doi: 10.1109/34.824822.'
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[408] J. S. Duncan 和 N. Ayache，“医学图像分析：二十年来的进展与未来挑战，”发表于《IEEE模式分析与机器智能》, 第22卷，第1期，页码85-106，2000年1月，doi:
    10.1109/34.824822。'
- en: '[409] J. Ker, L. Wang, J. Rao and T. Lim, ”Deep Learning Applications in Medical
    Image Analysis,” in IEEE Access, vol. 6, pp. 9375-9389, 2018, doi: 10.1109/ACCESS.2017.2788044.'
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[409] J. Ker、L. Wang、J. Rao 和 T. Lim，“深度学习在医学图像分析中的应用，”发表于《IEEE Access》, 第6卷，页码9375-9389，2018年，doi:
    10.1109/ACCESS.2017.2788044。'
- en: '[410] G. Litjens et al., “A Survey on Deep Learning in Medical Image Analysis,”
    Medical Image Analysis, vol. 42, pp. 60–88, Dec. 2017, doi: 10.1016/j.media.2017.07.005.'
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[410] G. Litjens 等，“深度学习在医学图像分析中的应用综述，”《医学图像分析》，第42卷，页码60-88，2017年12月，doi:
    10.1016/j.media.2017.07.005。'
- en: '[411] S. M. Anwar, M. Majid, A. Qayyum, M. Awais, M. Alnowami, and M. K. Khan,
    “Medical Image Analysis using Convolutional Neural Networks: A Review,” Journal
    of Medical Systems, vol. 42, no. 11, Oct. 2018, doi: 10.1007/s10916-018-1088-1.'
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[411] S. M. Anwar、M. Majid、A. Qayyum、M. Awais、M. Alnowami 和 M. K. Khan，“使用卷积神经网络的医学图像分析：综述，”《医学系统杂志》，第42卷，第11期，2018年10月，doi:
    10.1007/s10916-018-1088-1。'
- en: '[412] D. Shen, G. Wu, and H.-I. Suk, “Deep Learning in Medical Image Analysis,”
    Annual Review of Biomedical Engineering, vol. 19, no. 1, pp. 221–248, Jun. 2017,
    doi: 10.1146/annurev-bioeng-071516-044442.'
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[412] D. Shen、G. Wu 和 H.-I. Suk，“深度学习在医学图像分析中的应用，”《生物医学工程年鉴》，第19卷，第1期，页码221-248，2017年6月，doi:
    10.1146/annurev-bioeng-071516-044442。'
- en: '[413] J. Jiang, P. Trundle, and J. Ren, “Medical image analysis with artificial
    neural networks,” Computerized Medical Imaging and Graphics, vol. 34, no. 8, pp.
    617–631, Dec. 2010, doi: 10.1016/j.compmedimag.2010.07.003.'
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[413] J. Jiang、P. Trundle 和 J. Ren，“基于人工神经网络的医学图像分析，”《计算医学成像与图形》, 第34卷，第8期，页码617-631，2010年12月，doi:
    10.1016/j.compmedimag.2010.07.003。'
- en: '[414] Z. Gu et al., ”CE-Net: Context Encoder Network for 2D Medical Image Segmentation,”
    in IEEE Transactions on Medical Imaging, vol. 38, no. 10, pp. 2281-2292, Oct.
    2019, doi: 10.1109/TMI.2019.2903562.'
  id: totrans-1103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[414] Z. Gu 等，“CE-Net：用于2D医学图像分割的上下文编码网络，”发表于《IEEE医学成像》, 第38卷，第10期，页码2281-2292，2019年10月，doi:
    10.1109/TMI.2019.2903562。'
- en: '[415] M. Z. Khan, M. K. Gajendran, Y. Lee and M. A. Khan, ”Deep Neural Architectures
    for Medical Image Semantic Segmentation: Review,” in IEEE Access, vol. 9, pp.
    83002-83024, 2021, doi: 10.1109/ACCESS.2021.3086530.'
  id: totrans-1104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[415] M. Z. Khan、M. K. Gajendran、Y. Lee 和 M. A. Khan，“医学图像语义分割的深度神经网络架构：综述，”发表于《IEEE
    Access》, 第9卷，页码83002-83024，2021年，doi: 10.1109/ACCESS.2021.3086530。'
- en: '[416] J. Duan, S. Mao, J. Jin, Z. Zhou, L. Chen and C. L. P. Chen, ”A Novel
    GA-Based Optimized Approach for Regional Multimodal Medical Image Fusion With
    Superpixel Segmentation,” in IEEE Access, vol. 9, pp. 96353-96366, 2021, doi:
    10.1109/ACCESS.2021.3094972.'
  id: totrans-1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[416] J. Duan、S. Mao、J. Jin、Z. Zhou、L. Chen 和 C. L. P. Chen，“一种新颖的基于遗传算法的优化方法用于区域多模态医学图像融合与超像素分割，”发表于《IEEE
    Access》, 第9卷，页码96353-96366，2021年，doi: 10.1109/ACCESS.2021.3094972。'
- en: '[417] C. You, Y. Zhou, R. Zhao, L. Staib and J. S. Duncan, ”SimCVD: Simple
    Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical
    Image Segmentation,” in IEEE Transactions on Medical Imaging, vol. 41, no. 9,
    pp. 2228-2237, Sept. 2022, doi: 10.1109/TMI.2022.3161829.'
  id: totrans-1106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[417] C. You、Y. Zhou、R. Zhao、L. Staib 和 J. S. Duncan，“SimCVD：简单对比体素级表示蒸馏用于半监督医学图像分割，”发表于《IEEE医学成像》,
    第41卷，第9期，页码2228-2237，2022年9月，doi: 10.1109/TMI.2022.3161829。'
- en: '[418] S. Ye, T. Wang, M. Ding and X. Zhang, ”F-DARTS: Foveated Differentiable
    Architecture Search Based Multimodal Medical Image Fusion,” in IEEE Transactions
    on Medical Imaging, vol. 42, no. 11, pp. 3348-3361, Nov. 2023, doi: 10.1109/TMI.2023.3283517.'
  id: totrans-1107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[418] S. Ye、T. Wang、M. Ding 和 X. Zhang，“F-DARTS：基于视网膜差异化架构搜索的多模态医学图像融合，”发表于《IEEE医学成像》,
    第42卷，第11期，页码3348-3361，2023年11月，doi: 10.1109/TMI.2023.3283517。'
- en: '[419] S. Liu, E. Johns and A. J. Davison, ”End-To-End Multi-Task Learning With
    Attention,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), Long Beach, CA, USA, 2019, pp. 1871-1880, doi: 10.1109/CVPR.2019.00197.'
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[419] S. Liu, E. Johns 和 A. J. Davison， “端到端多任务学习与注意力机制，” 2019年IEEE/CVF计算机视觉与模式识别会议（CVPR），美国加利福尼亚州长滩，2019年，pp.
    1871-1880，doi: 10.1109/CVPR.2019.00197。'
- en: '[420] Y. Zhang and Q. Yang, ”A Survey on Multi-Task Learning,” in IEEE Transactions
    on Knowledge and Data Engineering, vol. 34, no. 12, pp. 5586-5609, 1 Dec. 2022,
    doi: 10.1109/TKDE.2021.3070203.'
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[420] Y. Zhang 和 Q. Yang，“关于多任务学习的调查，” 发表在IEEE知识与数据工程学报，卷34，第12期，pp. 5586-5609，2022年12月1日，doi:
    10.1109/TKDE.2021.3070203。'
- en: '[421] I. Misra, A. Shrivastava, A. Gupta and M. Hebert, ”Cross-Stitch Networks
    for Multi-task Learning,” 2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 3994-4003, doi: 10.1109/CVPR.2016.433.'
  id: totrans-1110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[421] I. Misra, A. Shrivastava, A. Gupta 和 M. Hebert， “多任务学习的交叉缝合网络，” 2016年IEEE计算机视觉与模式识别会议（CVPR），美国内华达州拉斯维加斯，2016年，pp.
    3994-4003，doi: 10.1109/CVPR.2016.433。'
- en: '[422] S. Vandenhende, S. Georgoulis, W. Van Gansbeke, M. Proesmans, D. Dai
    and L. Van Gool, ”Multi-Task Learning for Dense Prediction Tasks: A Survey,” in
    IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7,
    pp. 3614-3633, 1 July 2022, doi: 10.1109/TPAMI.2021.3054719.'
  id: totrans-1111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[422] S. Vandenhende, S. Georgoulis, W. Van Gansbeke, M. Proesmans, D. Dai
    和 L. Van Gool，“密集预测任务的多任务学习：综述，” 发表在IEEE模式分析与机器智能学报，卷44，第7期，pp. 3614-3633，2022年7月1日，doi:
    10.1109/TPAMI.2021.3054719。'
- en: '[423] F. Zhao, Y. Li, L. Bai, Z. Tian and X. Wang, ”Semi-Supervised Multi-Granularity
    CNNs for Text Classification: An Application in Human-Car Interaction,” in IEEE
    Access, vol. 8, pp. 68000-68012, 2020, doi: 10.1109/ACCESS.2020.2985098.'
  id: totrans-1112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[423] F. Zhao, Y. Li, L. Bai, Z. Tian 和 X. Wang， “半监督多粒度CNN用于文本分类：在人车交互中的应用，”
    发表在IEEE Access，卷8，pp. 68000-68012，2020年，doi: 10.1109/ACCESS.2020.2985098。'
- en: '[424] H. Chen, Y. Wang and Q. Hu, ”Multi-Granularity Regularized Re-Balancing
    for Class Incremental Learning,” in IEEE Transactions on Knowledge and Data Engineering,
    vol. 35, no. 7, pp. 7263-7277, 1 July 2023, doi: 10.1109/TKDE.2022.3188335.'
  id: totrans-1113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[424] H. Chen, Y. Wang 和 Q. Hu，“多粒度正则化重新平衡用于类别增量学习，” 发表在IEEE知识与数据工程学报，卷35，第7期，pp.
    7263-7277，2023年7月1日，doi: 10.1109/TKDE.2022.3188335。'
- en: '[425] K. Niu, Y. Huang, W. Ouyang and L. Wang, ”Improving Description-Based
    Person Re-Identification by Multi-Granularity Image-Text Alignments,” in IEEE
    Transactions on Image Processing, vol. 29, pp. 5542-5556, 2020, doi: 10.1109/TIP.2020.2984883.'
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[425] K. Niu, Y. Huang, W. Ouyang 和 L. Wang，“通过多粒度图像-文本对齐提高基于描述的人物再识别，” 发表在IEEE图像处理学报，卷29，pp.
    5542-5556，2020年，doi: 10.1109/TIP.2020.2984883。'
- en: '[426] J. -F. Hu, W. -S. Zheng, J. Lai and J. Zhang, ”Jointly Learning Heterogeneous
    Features for RGB-D Activity Recognition,” in IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. 39, no. 11, pp. 2186-2200, 1 Nov. 2017, doi: 10.1109/TPAMI.2016.2640292.'
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[426] J. -F. Hu, W. -S. Zheng, J. Lai 和 J. Zhang，“共同学习异质特征用于RGB-D活动识别，” 发表在IEEE模式分析与机器智能学报，卷39，第11期，pp.
    2186-2200，2017年11月1日，doi: 10.1109/TPAMI.2016.2640292。'
- en: '[427] C. Sahin, G. Garcia-Hernando, J. Sock, and T.-K. Kim, “A review on object
    pose recovery: From 3D bounding box detectors to full 6D pose estimators,” Image
    and Vision Computing, vol. 96, p. 103898, Apr. 2020, doi: https://doi.org/10.1016/j.imavis.2020.103898.'
  id: totrans-1116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[427] C. Sahin, G. Garcia-Hernando, J. Sock 和 T.-K. Kim，“物体姿态恢复综述：从3D边界框检测器到完整的6D姿态估计器，”
    发表在《图像与视觉计算》，卷96，p. 103898，2020年4月，doi: https://doi.org/10.1016/j.imavis.2020.103898。'
- en: '[428] T. Hodaň, D. Baráth and J. Matas, ”EPOS: Estimating 6D Pose of Objects
    With Symmetries,” 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), Seattle, WA, USA, 2020, pp. 11700-11709, doi: 10.1109/CVPR42600.2020.01172.'
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[428] T. Hodaň, D. Baráth 和 J. Matas， “EPOS: 估计具有对称性的物体的6D姿态，” 2020年IEEE/CVF计算机视觉与模式识别会议（CVPR），美国华盛顿州西雅图，2020年，pp.
    11700-11709，doi: 10.1109/CVPR42600.2020.01172。'
- en: '[429] W. Kehl, F. Manhardt, F. Tombari, S. Ilic and N. Navab, ”SSD-6D: Making
    RGB-Based 3D Detection and 6D Pose Estimation Great Again,” 2017 IEEE International
    Conference on Computer Vision (ICCV), Venice, Italy, 2017, pp. 1530-1538, doi:
    10.1109/ICCV.2017.169.'
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[429] W. Kehl, F. Manhardt, F. Tombari, S. Ilic 和 N. Navab， “SSD-6D: 让基于RGB的3D检测和6D姿态估计再度辉煌，”
    2017年IEEE国际计算机视觉会议（ICCV），意大利威尼斯，2017年，pp. 1530-1538，doi: 10.1109/ICCV.2017.169。'
- en: '[430] T. Vaudrey, A. Wedel, C. Rabe, J. Klappstein and R. Klette, ”Evaluation
    of moving object segmentation comparing 6D-vision and monocular motion constraints,”
    2008 23rd International Conference Image and Vision Computing New Zealand, Christchurch,
    New Zealand, 2008, pp. 1-6, doi: 10.1109/IVCNZ.2008.4762126.'
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[430] T. Vaudrey, A. Wedel, C. Rabe, J. Klappstein 和 R. Klette，“移动物体分割的评估：比较
    6D 视觉和单目运动约束”，2008 年第 23 届国际图像与视觉计算新西兰会议，基督城，新西兰，2008 年，第 1-6 页，doi: 10.1109/IVCNZ.2008.4762126。'
- en: '[431] Z. He, W. Feng, X. Zhao, and Y. Lv, “6D Pose Estimation of Objects: Recent
    Technologies and Challenges,” Applied Sciences, vol. 11, no. 1, p. 228, Jan. 2021,
    doi: 10.3390/app11010228.'
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[431] Z. He, W. Feng, X. Zhao 和 Y. Lv，“物体的 6D 姿态估计：最新技术和挑战”，应用科学，第 11 卷，第 1
    期，第 228 页，2021 年 1 月，doi: 10.3390/app11010228。'
- en: '[432] Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox, “DeepIM: Deep Iterative
    Matching for 6D Pose Estimation,” International Journal of Computer Vision, vol.
    128, no. 3, pp. 657–678, Nov. 2019, doi: 10.1007/s11263-019-01250-9.'
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[432] Y. Li, G. Wang, X. Ji, Y. Xiang 和 D. Fox，“DeepIM: 深度迭代匹配用于 6D 姿态估计”，国际计算机视觉杂志，第
    128 卷，第 3 期，第 657–678 页，2019 年 11 月，doi: 10.1007/s11263-019-01250-9。'
- en: '[433] T. Elsken, Jan Hendrik Metzen, and F. Hutter, “Neural Architecture Search:
    A Survey,” arXiv (Cornell University), vol. 20, no. 55, pp. 1–21, Jan. 2019.'
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[433] T. Elsken, Jan Hendrik Metzen 和 F. Hutter，“神经架构搜索：综述”，arXiv（康奈尔大学），第
    20 卷，第 55 期，第 1–21 页，2019 年 1 月。'
- en: '[434] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, “Neural Architecture
    Search without Training,” arXiv (Cornell University), Jun. 2020.'
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[434] J. Mellor, J. Turner, A. Storkey 和 E. J. Crowley，“无训练的神经架构搜索”，arXiv（康奈尔大学），2020
    年 6 月。'
- en: '[435] L. Sekanina, “Neural Architecture Search and Hardware Accelerator Co-Search:
    A Survey,” IEEE Access, vol. 9, pp. 151337–151362, 2021, doi: 10.1109/access.2021.3126685.'
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[435] L. Sekanina，“神经架构搜索与硬件加速器共同搜索：综述”，IEEE Access，第 9 卷，第 151337–151362 页，2021
    年，doi: 10.1109/access.2021.3126685。'
- en: '[436] K. T. Chitty-Venkata, M. Emani, V. Vishwanath and A. K. Somani, ”Neural
    Architecture Search for Transformers: A Survey,” in IEEE Access, vol. 10, pp.
    108374-108412, 2022, doi: 10.1109/ACCESS.2022.3212767.'
  id: totrans-1125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[436] K. T. Chitty-Venkata, M. Emani, V. Vishwanath 和 A. K. Somani，“变压器的神经架构搜索：综述”，发表于
    IEEE Access，第 10 卷，第 108374-108412 页，2022 年，doi: 10.1109/ACCESS.2022.3212767。'
- en: '[437] K. G. Mills et al., ”Exploring Neural Architecture Search Space via Deep
    Deterministic Sampling,” in IEEE Access, vol. 9, pp. 110962-110974, 2021, doi:
    10.1109/ACCESS.2021.3101975.'
  id: totrans-1126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[437] K. G. Mills 等，“通过深度确定性采样探索神经架构搜索空间”，发表于 IEEE Access，第 9 卷，第 110962-110974
    页，2021 年，doi: 10.1109/ACCESS.2021.3101975。'
- en: '[438] Z. Ding, Y. Chen, N. Li, D. Zhao, Z. Sun and C. L. P. Chen, ”BNAS: Efficient
    Neural Architecture Search Using Broad Scalable Architecture,” in IEEE Transactions
    on Neural Networks and Learning Systems, vol. 33, no. 9, pp. 5004-5018, Sept.
    2022, doi: 10.1109/TNNLS.2021.3067028.'
  id: totrans-1127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[438] Z. Ding, Y. Chen, N. Li, D. Zhao, Z. Sun 和 C. L. P. Chen，“BNAS: 使用广泛可扩展架构的高效神经架构搜索”，发表于
    IEEE Transactions on Neural Networks and Learning Systems，第 33 卷，第 9 期，第 5004-5018
    页，2022 年 9 月，doi: 10.1109/TNNLS.2021.3067028。'
- en: '[439] Z. Ma, Z. Zhou, Y. Liu, Y. Lei and H. Yan, ”Auto-ORVNet: Orientation-Boosted
    Volumetric Neural Architecture Search for 3D Shape Classification,” in IEEE Access,
    vol. 8, pp. 12942-12954, 2020, doi: 10.1109/ACCESS.2019.2961715.'
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[439] Z. Ma, Z. Zhou, Y. Liu, Y. Lei 和 H. Yan，"Auto-ORVNet: 3D 形状分类的方向增强体积神经架构搜索"，发表于
    IEEE Access，第 8 卷，第 12942-12954 页，2020 年，doi: 10.1109/ACCESS.2019.2961715。'
- en: '[440] X. Zhang, Z. Huang, N. Wang, S. Xiang and C. Pan, ”You Only Search Once:
    Single Shot Neural Architecture Search via Direct Sparse Optimization,” in IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 9, pp.
    2891-2904, 1 Sept. 2021, doi: 10.1109/TPAMI.2020.3020300.'
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[440] X. Zhang, Z. Huang, N. Wang, S. Xiang 和 C. Pan，“你只搜索一次：通过直接稀疏优化的单次神经架构搜索”，发表于
    IEEE Transactions on Pattern Analysis and Machine Intelligence，第 43 卷，第 9 期，第
    2891-2904 页，2021 年 9 月 1 日，doi: 10.1109/TPAMI.2020.3020300。'
- en: '[441] Y. Guo et al., ”Towards Accurate and Compact Architectures via Neural
    Architecture Transformer,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 44, no. 10, pp. 6501-6516, 1 Oct. 2022, doi: 10.1109/TPAMI.2021.3086914.'
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[441] Y. Guo 等，“通过神经架构转换器实现准确紧凑的架构”，发表于 IEEE Transactions on Pattern Analysis
    and Machine Intelligence，第 44 卷，第 10 期，第 6501-6516 页，2022 年 10 月 1 日，doi: 10.1109/TPAMI.2021.3086914。'
- en: '[442] H. Cao, C. Tan, Z. Gao, G. Chen, P. Heng, and S. Z. Li, “A Survey on
    Generative Diffusion Model,” arXiv (Cornell University), Sep. 2022, doi: 10.48550/arxiv.2209.02646.'
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[442] H. Cao, C. Tan, Z. Gao, G. Chen, P. Heng 和 S. Z. Li，“生成扩散模型综述”，arXiv（康奈尔大学），2022
    年 9 月，doi: 10.48550/arxiv.2209.02646。'
- en: '[443] W. Mao, B. Han and Z. Wang, ”Sketchffusion: Sketch-Guided Image Editing
    with Diffusion Model,” 2023 IEEE International Conference on Image Processing
    (ICIP), Kuala Lumpur, Malaysia, 2023, pp. 790-794, doi: 10.1109/ICIP49359.2023.10222365.'
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[443] W. Mao, B. Han 和 Z. Wang，《Sketchffusion：基于草图的扩散模型图像编辑》，2023 IEEE 国际图像处理会议（ICIP），马来西亚吉隆坡，2023
    年，第 790-794 页，doi: 10.1109/ICIP49359.2023.10222365。'
- en: '[444] X. P. Ooi and C. Seng Chan, ”LLDE: Enhancing Low-Light Images with Diffusion
    Model,” 2023 IEEE International Conference on Image Processing (ICIP), Kuala Lumpur,
    Malaysia, 2023, pp. 1305-1309, doi: 10.1109/ICIP49359.2023.10222446.'
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[444] X. P. Ooi 和 C. Seng Chan，《LLDE：利用扩散模型增强低光照图像》，2023 IEEE 国际图像处理会议（ICIP），马来西亚吉隆坡，2023
    年，第 1305-1309 页，doi: 10.1109/ICIP49359.2023.10222446。'
- en: '[445] T. Roque et al., ”A DCE-MRI Driven 3-D Reaction-Diffusion Model of Solid
    Tumor Growth,” in IEEE Transactions on Medical Imaging, vol. 37, no. 3, pp. 724-732,
    March 2018, doi: 10.1109/TMI.2017.2779811.'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[445] T. Roque 等，《基于 DCE-MRI 的 3D 反应扩散模型用于固体肿瘤生长》，发表于 IEEE Transactions on
    Medical Imaging，卷 37，第 3 期，第 724-732 页，2018 年 3 月，doi: 10.1109/TMI.2017.2779811。'
- en: '[446] T. Roque et al., ”A DCE-MRI Driven 3-D Reaction-Diffusion Model of Solid
    Tumor Growth,” in IEEE Transactions on Medical Imaging, vol. 37, no. 3, pp. 724-732,
    March 2018, doi: 10.1109/TMI.2017.2779811.'
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[446] T. Roque 等，《基于 DCE-MRI 的 3D 反应扩散模型用于固体肿瘤生长》，发表于 IEEE Transactions on
    Medical Imaging，卷 37，第 3 期，第 724-732 页，2018 年 3 月，doi: 10.1109/TMI.2017.2779811。'
- en: '[447] D. Kim, E. Lee, D. Yoo and H. Lee, ”Fine-Grained Human Hair Segmentation
    Using a Text-to-Image Diffusion Model,” in IEEE Access, doi: 10.1109/ACCESS.2024.3355542.'
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[447] D. Kim, E. Lee, D. Yoo 和 H. Lee，《使用文本到图像扩散模型的细粒度人类头发分割》，发表于 IEEE Access，doi:
    10.1109/ACCESS.2024.3355542。'
- en: '[448] A. Karnewar, A. Vedaldi, D. Novotny and N. J. Mitra, ”HOLODIFFUSION:
    Training a 3D Diffusion Model Using 2D Images,” 2023 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 18423-18433,
    doi: 10.1109/CVPR52729.2023.01767.'
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[448] A. Karnewar, A. Vedaldi, D. Novotny 和 N. J. Mitra，《HOLODIFFUSION：使用 2D
    图像训练 3D 扩散模型》，2023 IEEE/CVF 计算机视觉与模式识别会议（CVPR），加拿大温哥华，2023 年，第 18423-18433 页，doi:
    10.1109/CVPR52729.2023.01767。'
- en: '[449] W. Ran, W. Yuan and R. Shibasaki, ”Few-Shot Depth Completion Using Denoising
    Diffusion Probabilistic Model,” 2023 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition Workshops (CVPRW), Vancouver, BC, Canada, 2023, pp. 6559-6567,
    doi: 10.1109/CVPRW59228.2023.00697.'
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[449] W. Ran, W. Yuan 和 R. Shibasaki，《使用去噪扩散概率模型的少样本深度补全》，2023 IEEE/CVF 计算机视觉与模式识别会议研讨会（CVPRW），加拿大温哥华，2023
    年，第 6559-6567 页，doi: 10.1109/CVPRW59228.2023.00697。'
- en: '[450] T. Hospedales, A. Antoniou, P. Micaelli and A. Storkey, ”Meta-Learning
    in Neural Networks: A Survey,” in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 44, no. 9, pp. 5149-5169, 1 Sept. 2022, doi: 10.1109/TPAMI.2021.3079209.'
  id: totrans-1139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[450] T. Hospedales, A. Antoniou, P. Micaelli 和 A. Storkey，《神经网络中的元学习：综述》，发表于
    IEEE Transactions on Pattern Analysis and Machine Intelligence，卷 44，第 9 期，第 5149-5169
    页，2022 年 9 月 1 日，doi: 10.1109/TPAMI.2021.3079209。'
- en: '[451] I. Khan, X. Zhang, M. Rehman and R. Ali, ”A Literature Survey and Empirical
    Study of Meta-Learning for Classifier Selection,” in IEEE Access, vol. 8, pp.
    10262-10281, 2020, doi: 10.1109/ACCESS.2020.2964726.'
  id: totrans-1140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[451] I. Khan, X. Zhang, M. Rehman 和 R. Ali，《分类器选择的元学习文献综述与实证研究》，发表于 IEEE Access，卷
    8，第 10262-10281 页，2020 年，doi: 10.1109/ACCESS.2020.2964726。'
- en: '[452] P. Zhang, C. Liu, X. Chang, Y. Li and M. Li, ”Metric-based Meta-Learning
    Model for Few-Shot PolSAR Image Terrain Classification,” 2021 CIE International
    Conference on Radar (Radar), Haikou, Hainan, China, 2021, pp. 2529-2533, doi:
    10.1109/Radar53847.2021.10027883.'
  id: totrans-1141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[452] P. Zhang, C. Liu, X. Chang, Y. Li 和 M. Li，《基于度量的元学习模型用于少样本 PolSAR 图像地形分类》，2021
    CIE 国际雷达会议（Radar），中国海南海口，2021 年，第 2529-2533 页，doi: 10.1109/Radar53847.2021.10027883。'
- en: '[453] X. Zhang, D. Meng, H. Gouk and T. Hospedales, ”Shallow Bayesian Meta
    Learning for Real-World Few-Shot Recognition,” 2021 IEEE/CVF International Conference
    on Computer Vision (ICCV), Montreal, QC, Canada, 2021, pp. 631-640, doi: 10.1109/ICCV48922.2021.00069.'
  id: totrans-1142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[453] X. Zhang, D. Meng, H. Gouk 和 T. Hospedales，《用于现实世界少样本识别的浅层贝叶斯元学习》，2021
    IEEE/CVF 计算机视觉国际会议（ICCV），加拿大蒙特利尔，2021 年，第 631-640 页，doi: 10.1109/ICCV48922.2021.00069。'
- en: '[454] K. Gao, B. Liu, X. Yu and A. Yu, ”Unsupervised Meta Learning With Multiview
    Constraints for Hyperspectral Image Small Sample set Classification,” in IEEE
    Transactions on Image Processing, vol. 31, pp. 3449-3462, 2022, doi: 10.1109/TIP.2022.3169689.'
  id: totrans-1143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[454] K. Gao, B. Liu, X. Yu 和 A. Yu，《带多视图约束的无监督元学习用于高光谱图像小样本分类》，发表于 IEEE Transactions
    on Image Processing，卷 31，第 3449-3462 页，2022 年，doi: 10.1109/TIP.2022.3169689。'
- en: '[455] H. Cho, Y. Cho, J. Yu and J. Kim, ”Camera Distortion-aware 3D Human Pose
    Estimation in Video with Optimization-based Meta-Learning,” 2021 IEEE/CVF International
    Conference on Computer Vision (ICCV), Montreal, QC, Canada, 2021, pp. 11149-11158,
    doi: 10.1109/ICCV48922.2021.01098.'
  id: totrans-1144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[455] H. Cho, Y. Cho, J. Yu 和 J. Kim, ”基于优化的元学习的视频相机失真感知3D人体姿态估计,” 2021年IEEE/CVF国际计算机视觉大会（ICCV），加拿大魁北克蒙特利尔,
    pp. 11149-11158, doi: 10.1109/ICCV48922.2021.01098.'
- en: '[456] L. Zhang, Z. Liu, W. Zhang and D. Zhang, ”Style Uncertainty Based Self-Paced
    Meta Learning for Generalizable Person Re-Identification,” in IEEE Transactions
    on Image Processing, vol. 32, pp. 2107-2119, 2023, doi: 10.1109/TIP.2023.3263112.'
  id: totrans-1145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[456] L. Zhang, Z. Liu, W. Zhang 和 D. Zhang, ”基于风格不确定性的自主化元学习用于泛化人员重识别,” in
    《IEEE图像处理学刊》，第32卷，pp. 2107-2119, 2023年, doi: 10.1109/TIP.2023.3263112.'
- en: '[457] H. Coskun et al., ”Domain-Specific Priors and Meta Learning for Few-Shot
    First-Person Action Recognition,” in IEEE Transactions on Pattern Analysis and
    Machine Intelligence, vol. 45, no. 6, pp. 6659-6673, 1 June 2023, doi: 10.1109/TPAMI.2021.3058606.'
  id: totrans-1146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[457] H. Coskun 等，”领域特定的假设和元学习用于少样本第一人称行为识别,” in 《IEEE模式分析与机器智能学刊》，第45卷，第6期，pp.
    6659-6673, 2023年6月1日, doi: 10.1109/TPAMI.2021.3058606.'
- en: '[458] Y. Deng, T. Han and N. Ansari, ”FedVision: Federated Video Analytics
    With Edge Computing,” in IEEE Open Journal of the Computer Society, vol. 1, pp.
    62-72, 2020, doi: 10.1109/OJCS.2020.2996184.'
  id: totrans-1147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[458] Y. Deng, T. Han 和 N. Ansari, “FedVision：边缘计算中的联邦视频分析,” in 《IEEE计算机协会开放学刊》，第1卷，pp.
    62-72, 2020年, doi: 10.1109/OJCS.2020.2996184.'
- en: '[459] K. Doshi and Y. Yilmaz, ”Privacy-Preserving Video Understanding via Transformer-based
    Federated Learning,” 2023 IEEE Conference on Dependable and Secure Computing (DSC),
    Tampa, FL, USA, 2023, pp. 1-8, doi: 10.1109/DSC61021.2023.10354099.'
  id: totrans-1148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[459] K. Doshi 和 Y. Yilmaz, ”基于变压器的联邦学习的隐私保护视频理解,” 2023年IEEE可信与安全计算（DSC）大会，美国佛罗里达州坦帕,
    pp. 1-8, doi: 10.1109/DSC61021.2023.10354099.'
- en: '[460] Y. Liu et al., “Federated Learning-Powered Visual Object Detection for
    Safety Monitoring,” AI Magazine, vol. 42, no. 2, pp. 19–27, Oct. 2021, doi: 10.1609/aimag.v42i2.15095.
    ‌'
  id: totrans-1149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[460] Y. Liu 等，“联邦学习实施的安全监控视觉对象检测,” 《AI杂志》，第42卷，第2期，pp. 19–27, 2021年10月, doi:
    10.1609/aimag.v42i2.15095. '
- en: '[461] Y. Liu et al., “FedVision: An Online Visual Object Detection Platform
    Powered by Federated Learning,” Proceedings of the AAAI Conference on Artificial
    Intelligence, vol. 34, no. 08, pp. 13172–13179, Apr. 2020, doi: 10.1609/aaai.v34i08.7021.'
  id: totrans-1150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[461] Y. Liu 等，“FedVision: 由联合学习提供动力的在线视觉对象检测平台,” in 《AAAI人工智能大会论文集》，第34卷，第08期，pp.
    13172–13179, 2020年4月, doi: 10.1609/aaai.v34i08.7021.'
- en: '[462] K. Zhou and Xin Eric Wang, “FedVLN: Privacy-preserving Federated Vision-and-Language
    Navigation,” arXiv (Cornell University), Mar. 2022, doi: 10.48550/arxiv.2203.14936.'
  id: totrans-1151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[462] K. Zhou 和 Xin Eric Wang, “FedVLN：隐私保护的联邦视觉与语言导航,” arXiv (康奈尔大学)，2022年3月,
    doi: 10.48550/arxiv.2203.14936.'
- en: '[463] H. Li, K. Yin, X. Ji, Y. Liu, T. Huang and G. Yin, ”Improved YOLOV3 Surveillance
    Device Object Detection Method Based on Federated Learning,” 2022 4th International
    Conference on Data-driven Optimization of Complex Systems (DOCS), Chengdu, China,
    2022, pp. 1-6, doi: 10.1109/DOCS55193.2022.9967481.'
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[463] H. Li, K. Yin, X. Ji, Y. Liu, T. Huang 和 G. Yin, ”基于联邦学习的改进YOLOV3监控设备对象检测方法,”
    2022年第四届数据驱动复杂系统优化国际会议（DOCS）, 中国成都, pp. 1-6, doi: 10.1109/DOCS55193.2022.9967481.'
- en: '[464] M. Alazab, S. P. RM, P. M, P. K. R. Maddikunta, T. R. Gadekallu and Q.
    -V. Pham, ”Federated Learning for Cybersecurity: Concepts, Challenges, and Future
    Directions,” in IEEE Transactions on Industrial Informatics, vol. 18, no. 5, pp.
    3501-3509, May 2022, doi: 10.1109/TII.2021.3119038.'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[464] M. Alazab, S. P. RM, P. M, P. K. R. Maddikunta, T. R. Gadekallu 和 Q.
    -V. Pham, ”联邦学习用于网络安全：概念，挑战和未来方向,” in 《IEEE工业信息学刊》，第18卷，第5期，pp. 3501-3509, 2022年5月,
    doi: 10.1109/TII.2021.3119038.'
- en: '[465] P. K. Mandal, Carter De Leo, and C. Hurley, “Horizontal Federated Computer
    Vision,” arXiv (Cornell University), Dec. 2023, doi: 10.48550/arxiv.2401.00390.'
  id: totrans-1154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[465] P. K. Mandal, Carter De Leo, 和 C. Hurley, “水平联邦计算机视觉,” arXiv (康奈尔大学)，2023年12月,
    doi: 10.48550/arxiv.2401.00390.'
- en: '| ![[Uncaptioned image]](img/ac14f5baae3653fe8d2a513f332d2931.png) | Abolfazl
    Younesi received a B.Sc. degree in computer engineering from the Tabriz University,
    Tabriz, Iran, in 2021\. He is currently working toward an M.Sc. degree in computer
    engineering at Sharif University of Technology (SUT), Tehran, Iran, from Oct.
    2021 until now. He is currently a member of the Embedded Systems Research Laboratory
    (ESR-LAB) at the Department of Computer Engineering, Sharif University of Technology.
    His research interests include the Internet of Things (IoT) and Cyber-Physical
    Systems (CPS), low-power design, machine learning, and computer vision. |'
  id: totrans-1155
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标题图像]](img/ac14f5baae3653fe8d2a513f332d2931.png) | Abolfazl Younesi 于
    2021 年获得了伊朗塔布里兹大学计算机工程学士学位。他目前在 Sharif University of Technology（SUT）攻读计算机工程硕士学位，自
    2021 年 10 月以来。他目前是 Sharif University of Technology 计算机工程系嵌入式系统研究实验室（ESR-LAB）的成员。他的研究兴趣包括物联网（IoT）和网络物理系统（CPS）、低功耗设计、机器学习和计算机视觉。
    |'
- en: '| ![[Uncaptioned image]](img/01164c65ef5a2b63f5742725459f9797.png) | Mohsen
    Ansari received the Ph.D. degree in computer engineering from the Sharif University
    of Technology, Tehran, Iran, in 2021\. He is currently an Assistant Professor
    of computer engineering at the Sharif University of Technology, Tehran, Iran.
    He was a visiting researcher with the chair for Embedded Systems (CES), Karlsruhe
    Institute of Technology (KIT), Germany, from 2019 to 2021\. His research interests
    include cyber-physical systems, embedded machine learning, edge, fog, and cloud
    computing, and thermal and low-power design of CPSs. |'
  id: totrans-1156
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标题图像]](img/01164c65ef5a2b63f5742725459f9797.png) | Mohsen Ansari 于 2021
    年获得了伊朗德黑兰的Sharif University of Technology计算机工程博士学位。他目前是Sharif University of Technology的计算机工程助理教授。他曾于2019至2021年在德国卡尔斯鲁厄理工学院（KIT）的嵌入式系统（CES）主席担任访问研究员。他的研究兴趣包括网络物理系统、嵌入式机器学习、边缘、雾计算和云计算，以及CPS的热设计和低功耗设计。
    |'
- en: '| ![[Uncaptioned image]](img/dca3ea20497b602fe404bad5820db6df.png) | MohammadAmin
    Fazli received the B.Sc. degree in hardware engineering and the M.Sc. and Ph.D.
    degrees in software engineering from the Sharif University of Technology, Tehran,
    Iran, in 2009, 2011, and 2015, respectively. He is currently an Assistant Professor
    at the Sharif University of Technology, where he is also a Research and Development
    Supervisor with the Intelligent Information Solutions Center. His current research
    interests include game theory, combinatorial optimization, computational business
    and economics, graphs and combinatorics, complex networks, and dynamical systems.
    |'
  id: totrans-1157
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标题图像]](img/dca3ea20497b602fe404bad5820db6df.png) | MohammadAmin Fazli
    于 2009 年获得了伊朗德黑兰 Sharif University of Technology 的硬件工程学士学位，2011 年和 2015 年分别获得了软件工程硕士和博士学位。他目前是
    Sharif University of Technology 的助理教授，同时也是智能信息解决方案中心的研发主管。他目前的研究兴趣包括博弈论、组合优化、计算商业与经济学、图论与组合学、复杂网络和动态系统。
    |'
- en: '| ![[Uncaptioned image]](img/c023ba39b014f151e794963e9f1a81df.png) | Alireza
    Ejlali received the Ph.D. degree in computer engineering from the Sharif University
    of Technology (SUT), Tehran, Iran, in 2006, where he is currently an Associate
    Professor of computer engineering. From 2005 to 2006, he was a Visiting Researcher
    with the Electronic Systems Design Group, University of Southampton, Southampton,
    U.K. He is currently the Director of the Embedded Systems Research Laboratory
    with the Department of Computer Engineering, Sharif University of Technology.
    His research interests include low-power design, real-time, and fault-tolerant
    embedded systems. |'
  id: totrans-1158
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标题图像]](img/c023ba39b014f151e794963e9f1a81df.png) | Alireza Ejlali 于 2006
    年获得了伊朗 Sharif University of Technology 的计算机工程博士学位，目前是该校计算机工程系的副教授。2005 至 2006
    年期间，他曾是英国南安普顿大学电子系统设计组的访问研究员。他目前是 Sharif University of Technology 计算机工程系嵌入式系统研究实验室的主任。他的研究兴趣包括低功耗设计、实时和容错嵌入式系统。'
- en: '| ![[Uncaptioned image]](img/9f84fb5f5f8a635efb69ea0a53b4acce.png) | Muhammad
    Shafique (Senior Member, IEEE) received the Ph.D. degree in computer science from
    Karlsruhe Institute of Technology, Karlsruhe, Germany, in 2011\. He was a Full
    Professor at the Institute of Computer Engineering, TU Wien, Vienna, Austria,
    from October 2016 to August 2020\. Since September 2020, he has been with the
    Division of Engineering, New York University Abu Dhabi, Abu Dhabi, UAE, and is
    a Global Network Faculty Member of the NYU Tandon School of Engineering, Brooklyn,
    NY, USA. His research interests are in system-level design for brain-inspired
    computing, AI/machine learning hardware, wearables, autonomous systems, energy-efficient
    and robust computing, IoT, and smart CPS. Dr. Shafique received the 2015 ACM/SIGDA
    Outstanding New Faculty Award, the AI 2000 Chip Technology Most Influential Scholar
    Award in 2020, 2022, and 2023, six gold medals, and several best paper awards
    and nominations. He has given several keynotes, talks, and tutorials and organized
    special sessions at premier venues. He has served as the PC chair, general chair,
    track chair, and PC member for several conferences. |'
  id: totrans-1159
  prefs: []
  type: TYPE_TB
  zh: '![[未标注图像]](img/9f84fb5f5f8a635efb69ea0a53b4acce.png) | Muhammad Shafique（IEEE
    高级会员）于 2011 年在卡尔斯鲁厄理工学院获得计算机科学博士学位。他曾在 2016 年 10 月至 2020 年 8 月期间担任奥地利维也纳工业大学计算机工程学院的全职教授。自
    2020 年 9 月以来，他在阿布扎比纽约大学工程学院工作，并且是 NYU Tandon School of Engineering 的全球网络教员，位于纽约布鲁克林。他的研究兴趣包括脑启发计算的系统级设计、AI/机器学习硬件、可穿戴设备、自动化系统、能源高效和稳健计算、物联网和智能网络物理系统。Shafique
    博士获得了 2015 年 ACM/SIGDA 杰出新教师奖、2020、2022 和 2023 年 AI 2000 芯片技术最具影响力学者奖、六枚金牌以及多个最佳论文奖和提名。他在多个重要场合做过几次主题演讲、讲座和教程，并组织过特别会议。他曾担任多个会议的程序委员会主席、总主席、分会主席和程序委员会成员。'
- en: '| ![[Uncaptioned image]](img/67acb84df6d491c378695937997f4373.png) | Jörg Henkel
    (Fellow, IEEE) received the Diploma and Ph.D. degrees (summa cum laude) from the
    Technical University of Braunschweig. He is currently the Chair Professor of embedded
    systems with the Karlsruhe Institute of Technology (KIT). Before that, he was
    a Research Staff Member with NEC Laboratories, Princeton, NJ, USA. His research
    interests include co-design for embedded hardware/software systems with respect
    to power security and means of embedded machine learning. He is the Vice President
    of Publications at IEEE CEDA and a fellow of the ACM. He has led several conferences
    as the General Chair, including ICCAD and ESWeek, and is currently the General
    of DAC’60\. He serves as a steering committee chair/member for leading conferences
    and journals for embedded and cyber-physical systems. He has coordinated the DFG
    Program SPP 1500 “Dependable Embedded Systems” and is a Site Coordinator of the
    DFG TR89 Collaborative Research Center on “Invasive Computing.” He is the Chairman
    of the IEEE Computer Society, Germany Chapter. He has received six best paper
    awards throughout his career from, among others, ICCAD, ESWeek, and DATE. For
    two consecutive terms each, he served as the Editor-in-Chief for both the ACM
    Transactions on Embedded Computing Systems and the IEEE Design & Test magazine.
    |'
  id: totrans-1160
  prefs: []
  type: TYPE_TB
  zh: '![[未标注图像]](img/67acb84df6d491c378695937997f4373.png) | Jörg Henkel（IEEE Fellow）获得了布伦瑞克技术大学的文凭和博士学位（以优异成绩）。他目前是卡尔斯鲁厄理工学院（KIT）嵌入式系统讲席教授。在此之前，他曾是
    NEC Laboratories, Princeton, NJ, USA 的研究员。他的研究兴趣包括嵌入式硬件/软件系统的协同设计，特别是电力安全和嵌入式机器学习方法。他是
    IEEE CEDA 的出版副主席和 ACM 的院士。他曾作为总主席主持过多个会议，包括 ICCAD 和 ESWeek，并且目前是 DAC''60 的总主席。他还担任多个领先会议和期刊的指导委员会主席/成员，专注于嵌入式和网络物理系统。他协调了
    DFG 计划 SPP 1500 “可靠嵌入式系统”，并且是 DFG TR89 “侵入式计算”协作研究中心的现场协调员。他是 IEEE 计算机协会德国分会的主席。在他的职业生涯中，他曾获得六项最佳论文奖，包括
    ICCAD、ESWeek 和 DATE 等。他曾连续两届担任 ACM Transactions on Embedded Computing Systems
    和 IEEE Design & Test 杂志的主编。'
