- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:04:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:04:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1912.00271] Biometrics Recognition Using Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1912.00271] 基于深度学习的生物识别：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.00271](https://ar5iv.labs.arxiv.org/html/1912.00271)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1912.00271](https://ar5iv.labs.arxiv.org/html/1912.00271)
- en: ∎
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '¹¹institutetext: Shervin Minaee ²²institutetext: Snapchat, Machine Learning
    R&D ³³institutetext: Amirali Abdolrashidi ⁴⁴institutetext: University of California,
    Riverside ⁵⁵institutetext: Hang Su ⁶⁶institutetext: Facebook Research ⁷⁷institutetext:
    Mohammed Bennamoun ⁸⁸institutetext: The University of Western Australia ⁹⁹institutetext:
    David Zhang ^(10)^(10)institutetext: Chinese University of Hong Kong'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构文本：Shervin Minaee ²²机构文本：Snapchat，机器学习研发 ³³机构文本：Amirali Abdolrashidi ⁴⁴机构文本：加州大学河滨分校
    ⁵⁵机构文本：Hang Su ⁶⁶机构文本：Facebook Research ⁷⁷机构文本：Mohammed Bennamoun ⁸⁸机构文本：西澳大学
    ⁹⁹机构文本：David Zhang ^(10)^(10)机构文本：香港中文大学
- en: 'Biometrics Recognition Using Deep Learning: A Survey'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的生物识别：综述
- en: Shervin Minaee    Amirali Abdolrashidi    Hang Su    Mohammed Bennamoun    David
    Zhang
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Shervin Minaee    Amirali Abdolrashidi    Hang Su    Mohammed Bennamoun    David
    Zhang
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning-based models have been very successful in achieving state-of-the-art
    results in many of the computer vision, speech recognition, and natural language
    processing tasks in the last few years. These models seem a natural fit for handling
    the ever-increasing scale of biometric recognition problems, from cellphone authentication
    to airport security systems. Deep learning-based models have increasingly been
    leveraged to improve the accuracy of different biometric recognition systems in
    recent years. In this work, we provide a comprehensive survey of more than 120
    promising works on biometric recognition (including face, fingerprint, iris, palmprint,
    ear, voice, signature, and gait recognition), which deploy deep learning models,
    and show their strengths and potentials in different applications. For each biometric,
    we first introduce the available datasets that are widely used in the literature
    and their characteristics. We will then talk about several promising deep learning
    works developed for that biometric, and show their performance on popular public
    benchmarks. We will also discuss some of the main challenges while using these
    models for biometric recognition, and possible future directions to which research
    in this area is headed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的模型在过去几年中，在计算机视觉、语音识别和自然语言处理任务中取得了最先进的成果。这些模型似乎非常适合处理不断增加的生物识别问题规模，从手机认证到机场安全系统。近年来，基于深度学习的模型被越来越多地用于提高各种生物识别系统的准确性。在这项工作中，我们提供了一个全面的综述，涵盖了120多个有前途的生物识别工作（包括面部、指纹、虹膜、掌纹、耳朵、语音、签名和步态识别），这些工作部署了深度学习模型，并展示了它们在不同应用中的优势和潜力。对于每种生物特征，我们首先介绍了文献中广泛使用的可用数据集及其特点。然后我们将讨论为该生物特征开发的几项有前景的深度学习工作，并展示它们在流行的公共基准测试中的表现。我们还将讨论在使用这些模型进行生物识别时的一些主要挑战，以及这一领域未来的可能研究方向。
- en: 'Keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Biometric Recognition, Deep Learning, Face Recognition, Fingerprint Recognition,
    Iris Recognition, Palmprint Recognition.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 生物识别，深度学习，人脸识别，指纹识别，虹膜识别，掌纹识别。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Biometric features¹¹1In this paper, we commonly refer to a biometric characteristic
    as biometric for short. hold a unique place when it comes to recognition, authentication,
    and security applications [jain2000biometric](#bib.bib1) , [zhang2013automated](#bib.bib2)
    . They cannot get lost, unlike token-based features such as keys and ID cards,
    and they cannot be forgotten, unlike knowledge-based features, such as passwords
    or answers to security questions [zhang2018advanced](#bib.bib3) . In addition,
    they are almost impossible to perfectly imitate or duplicate. Even though there
    have been recent attempts to generate and forge various biometric features [galbally2008fake](#bib.bib4)
    , [eskimez2018generating](#bib.bib5) , there have also been methods proposed to
    distinguish fake biometric features from authentic ones [deepfake](#bib.bib6)
    , [mo2018fake](#bib.bib7) , [li2018exposing](#bib.bib8) . Changes over time for
    many biometric features are also extremely little. For these reasons, they have
    been utilized in many applications, including cellphone authentication, airport
    security, and forensic science. Biometric features can be physiological, which
    are features possessed by any person, such as fingerprints [jain2004introduction](#bib.bib9)
    , palmprints [lu2003palmprint](#bib.bib10) , [zhang1999two](#bib.bib11) , facial
    features [face_Chellappa](#bib.bib12) , ears [mu2004shape](#bib.bib13) , irises
    [daugman2009iris](#bib.bib14) , [bowyer2016handbook](#bib.bib15) , and retinas
    [borgen_retina](#bib.bib16) , or behavioral, which are apparent in a person’s
    interaction with the environment, such as signatures [elhoseny2018hybrid](#bib.bib17)
    , gaits [gait_review](#bib.bib18) , and keystroke [monrose2000keystroke](#bib.bib19)
    . Voice/Speech contains both behavioral features, such as accent, and physiological
    features, such as voice pitch [speech_rec](#bib.bib20) .
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生物特征¹¹1在本文中，我们通常将生物特征简称为生物识别。 在识别、认证和安全应用中，生物特征占据了独特的位置[jain2000biometric](#bib.bib1)
    ，[zhang2013automated](#bib.bib2) 。 与基于令牌的特征（如钥匙和身份证）不同，它们不会丢失，也不同于基于知识的特征（如密码或安全问题的答案），它们也不会被遗忘[zhang2018advanced](#bib.bib3)
    。 此外，它们几乎不可能被完美模仿或复制。 尽管最近有尝试生成和伪造各种生物特征[galbally2008fake](#bib.bib4) ，[eskimez2018generating](#bib.bib5)
    ，也提出了区分伪造生物特征和真实特征的方法[deepfake](#bib.bib6) ，[mo2018fake](#bib.bib7) ，[li2018exposing](#bib.bib8)
    。 许多生物特征随着时间的变化也极少。 正因如此，它们被广泛应用于许多领域，包括手机认证、机场安全和法医科学。 生物特征可以是生理的，如指纹[jain2004introduction](#bib.bib9)
    ，掌纹[lu2003palmprint](#bib.bib10) ，[zhang1999two](#bib.bib11) ，面部特征[face_Chellappa](#bib.bib12)
    ，耳朵[mu2004shape](#bib.bib13) ，虹膜[daugman2009iris](#bib.bib14) ，[bowyer2016handbook](#bib.bib15)
    ，和视网膜[borgen_retina](#bib.bib16) ，或行为的，如签名[elhoseny2018hybrid](#bib.bib17) ，步态[gait_review](#bib.bib18)
    ，和击键[monrose2000keystroke](#bib.bib19) 。 语音/言语包含行为特征，如口音，以及生理特征，如语音音调[speech_rec](#bib.bib20)
    。
- en: 'Face and fingerprint are arguably the most commonly used physiological biometric
    feature. Fingerprint is the oldest, dating back to 1893 when it was used to convict
    a murder suspect in Argentina [hawthorne2017fingerprints](#bib.bib21) . Face has
    many discriminative features which can be used for recognition tasks [jain2011handbook](#bib.bib22)
    . However, its susceptibility to change due to factors such as expression or aging,
    may present a challenge [guo2016ei3d](#bib.bib23) , [park2010age](#bib.bib24)
    . Fingerprint consists of ridges and valleys, which form unique patterns. Minutiae
    are major local portions of the fingerprint which can be used to determine the
    uniqueness of the fingerprint, two of the most important ones of which being ridge
    endings and ridge bifurcations [jain1997line](#bib.bib25) . Palmprint is another
    alternative used for authentication purposes. In addition to minutiae features,
    palmprints also consist of geometry-based features, delta points, principal lines,
    and wrinkles [multispectral_palm](#bib.bib26) . Iris and retina are the two most
    popular biometrics that are present in the eye, and can be used for recognition
    through the texture of the iris or the pattern of blood vessels in the retina.
    One interesting point worth noting is that even the two eyes in the same person
    have different patterns [iris_survey_old](#bib.bib27) . Ears can also be used
    as a biometric through the shape of their lobes, and helix, and unlike most biometric
    features, do not need the person’s direct interaction. The right and left ears
    are symmetrical in a person in most cases. However, their sizes are subject to
    change over time [emervsivc2017ear](#bib.bib28) . Among the behavioral features,
    signatures are arguably the most widely used today. The strokes in the signature
    can be examined for the pressure of the pen throughout the signature as well as
    the speed, which is a factor in the thickness of the stroke [galbally2015line](#bib.bib29)
    . Gait refers to the manner of walking, which has been gaining more attention
    in the recent years. Due to the involvement of many joints and body parts in the
    process of walking, gait can also be used to uniquely identify a person from a
    distance [wang2004fusion](#bib.bib30) . Samples of various biometrics are shown
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Biometrics Recognition Using
    Deep Learning: A Survey").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '面部和指纹可以说是最常用的生理生物特征。指纹是最古老的，早在1893年就用于阿根廷的一起谋杀案件的定罪 [hawthorne2017fingerprints](#bib.bib21)。面部有许多可用于识别任务的区分特征
    [jain2011handbook](#bib.bib22)。然而，面部特征由于表达或衰老等因素的变化，可能会带来挑战 [guo2016ei3d](#bib.bib23)，[park2010age](#bib.bib24)。指纹由脊线和沟壑组成，形成独特的图案。细节是指纹的主要局部部分，可以用来确定指纹的唯一性，其中最重要的两个是脊线的终点和脊线分叉
    [jain1997line](#bib.bib25)。掌纹是另一种用于身份验证的替代方法。除了细节特征外，掌纹还包括基于几何的特征、三角点、主要线条和皱纹
    [multispectral_palm](#bib.bib26)。虹膜和视网膜是眼睛中最受欢迎的两种生物特征，可以通过虹膜的纹理或视网膜中的血管图案进行识别。值得注意的是，即使是同一个人的两只眼睛也有不同的图案
    [iris_survey_old](#bib.bib27)。耳朵也可以通过耳垂和耳廓的形状作为生物特征，与大多数生物特征不同，耳朵不需要被测试者的直接互动。大多数情况下，右耳和左耳在一个人身上是对称的。然而，它们的大小会随着时间而变化
    [emervsivc2017ear](#bib.bib28)。在行为特征中，签名无疑是今天使用最广泛的。签名中的笔划可以检查整个签名中的笔压力以及速度，这会影响笔划的厚度
    [galbally2015line](#bib.bib29)。步态指的是走路的方式，近年来获得了越来越多的关注。由于走路过程中涉及许多关节和身体部位，步态也可以用来从远处唯一识别一个人
    [wang2004fusion](#bib.bib30)。各种生物特征的样本如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Biometrics Recognition Using Deep Learning: A Survey") 所示。'
- en: '![Refer to caption](img/9aac9a2366e11d48f6caf2ccfbdeaab6.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9aac9a2366e11d48f6caf2ccfbdeaab6.png)'
- en: 'Figure 1: Sample images for various biometrics. The images in the first to
    sixth rows denote samples from face, fingerprint, iris, palmprint, ear, and gait
    respectively [casia_gaitA](#bib.bib31) ; [yaleb](#bib.bib32) ; [polyU_finger](#bib.bib33)
    ; [polyu_palm](#bib.bib34) ; [kumar2010comparison](#bib.bib35) ; [iit_ear](#bib.bib36)
    .'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：各种生物特征的样本图像。第一到第六行的图像分别表示面部、指纹、虹膜、掌纹、耳朵和步态的样本 [casia_gaitA](#bib.bib31)；[yaleb](#bib.bib32)；[polyU_finger](#bib.bib33)；[polyu_palm](#bib.bib34)；[kumar2010comparison](#bib.bib35)；[iit_ear](#bib.bib36)。
- en: 'Traditionally, the biometric recognition process involved several key steps.
    Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Biometrics Recognition Using Deep
    Learning: A Survey") shows the block-diagram of traditional biometric recognition
    systems. Firstly, the image data are acquired via (various) camera or optical
    sensors, and are then pre-processed so as to make the algorithm work on as much
    useful data as possible. Then, features are extracted from each image. Classical
    biometric recognition works were mostly based on hand-crafted features (designed
    by computer vision experts) to work with a certain type of data [ahonen2004face](#bib.bib37)
    , [2d_3d_face](#bib.bib38) , [zhang2009advanced](#bib.bib39) . Many of the hand-crafted
    features were based on the distribution of edges (SIFT [lowe2004distinctive](#bib.bib40)
    , HOG [dalal2005histograms](#bib.bib41) ), or where derived from transform domain,
    such as Gabor [kong2003palmprint](#bib.bib42) , Fourier [lai2001face](#bib.bib43)
    , and wavelet [jin2004efficient](#bib.bib44) . Principal component analysis is
    also used in many works to reduce the dimensionality of the features [abdi2010principal](#bib.bib45)
    , [yang2004two](#bib.bib46) . Once the features are extracted, they are fed into
    a classifier to perform recognition.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，生物特征识别过程包括几个关键步骤。图[2](#S1.F2 "图2 ‣ 1 介绍 ‣ 使用深度学习的生物特征识别：综述")显示了传统生物特征识别系统的框图。首先，通过（各种）摄像机或光学传感器获取图像数据，然后进行预处理，以便算法能够在尽可能多的有用数据上进行工作。接着，从每张图像中提取特征。经典的生物特征识别工作大多基于手工特征（由计算机视觉专家设计）以适应特定类型的数据
    [ahonen2004face](#bib.bib37) , [2d_3d_face](#bib.bib38) , [zhang2009advanced](#bib.bib39)
    。许多手工特征基于边缘的分布（SIFT [lowe2004distinctive](#bib.bib40) , HOG [dalal2005histograms](#bib.bib41)），或来源于变换域，如Gabor
    [kong2003palmprint](#bib.bib42) , Fourier [lai2001face](#bib.bib43) 和小波 [jin2004efficient](#bib.bib44)。主成分分析也在许多工作中用于降低特征的维度
    [abdi2010principal](#bib.bib45) , [yang2004two](#bib.bib46)。一旦特征被提取出来，它们会被输入到分类器中进行识别。
- en: '![Refer to caption](img/630cc1dfd6e6432f5292266fa934c6d2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/630cc1dfd6e6432f5292266fa934c6d2.png)'
- en: 'Figure 2: The block-diagram of most of classical biometric recognition algorithms.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：大多数经典生物特征识别算法的框图。
- en: Many challenges arise in a traditional biometric recognition task. For example,
    the hand-crafted features that are suitable for one biometric, will not necessarily
    perform well on others. Therefore, it would take a great number of experiments
    to find and choose the most efficient set of hand-crafted features for a certain
    biometric. Also many of the classical models were based on multi-class SVM trained
    in an one-vs-one fashion, which will not scale well when the number of classes
    is large.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的生物特征识别任务中，面临许多挑战。例如，适用于一种生物特征的手工特征，未必在其他生物特征上表现良好。因此，需要大量实验来寻找和选择最有效的手工特征集以适应某种生物特征。此外，许多经典模型基于多类支持向量机（SVM），采用一对一的训练方式，当类别数量较多时，这种方法的扩展性较差。
- en: 'However, a paradigm shift started to occur in 2012, when a deep learning-based
    model, AlexNet [alexnet](#bib.bib47) , won the ImageNet competition by a large
    margin. Since then, deep learning models have been applied to a wide range of
    problems in computer vision and Natural Language Processing (NLP), and achieved
    promising results. Not surprisingly, biometric recognition methods were not an
    exception, and were taken over by deep learning models (with a few years delay).
    Deep learning based models provide an end-to-end learning framework, which can
    jointly learn the feature representation while performing classification/regression.
    This is achieved through a multi-layer neural networks, also known as Deep Neural
    Networks (DNNs), to learn multiple levels of representations that correspond to
    different levels of abstraction, which is better suited to uncover underlying
    patterns of the data (as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Biometrics Recognition Using Deep Learning: A Survey")). The idea of a multi-layer
    neural network dates back to the 1960s [ivakhnenko1966cybernetic](#bib.bib48)
    ; [schmidhuber2015deep](#bib.bib49) . However, their feasible implementation was
    a challenge in itself, as the training time would be too large (due to lack of
    powerful computers at that time). The progresses made in processor technology,
    and especially the development of General-Purpose GPUs (GPGPUs), as well as development
    of new techniques (such as Dropout) for training neural networks with a lower
    chance of over-fitting, enabled scientists to train very deep neural networks
    much faster [lecun2015deep](#bib.bib50) . The main idea of a neural network is
    to pass the (raw) data through a series of interconnected neurons or nodes, each
    of which emulates a linear or non-linear function based on its own weights and
    biases. These weights and biases would change during the training through back-propagation
    of the gradients from the output [backprop](#bib.bib51) , usually resulted from
    the differences between the expected output and the actual current output, aimed
    to minimized a loss function or cost function (difference between the predicted
    and actual outputs according to some metric) [bottou1991stochastic](#bib.bib52)
    . We will talk about different deep architectures in more details in Section 2.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，2012年开始出现了范式转变，当时基于深度学习的模型AlexNet [alexnet](#bib.bib47) 以巨大优势赢得了ImageNet竞赛。从那时起，深度学习模型被应用于计算机视觉和自然语言处理（NLP）等广泛的问题，并取得了令人期待的成果。毫不奇怪，生物特征识别方法也不例外，虽然有几年的延迟，但也被深度学习模型所取代。基于深度学习的模型提供了一个端到端的学习框架，可以在进行分类/回归的同时共同学习特征表示。这是通过多层神经网络实现的，也称为深度神经网络（DNNs），以学习多个层次的表示，这些表示对应于不同的抽象层次，这更适合揭示数据的潜在模式（如图[3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Biometrics Recognition Using Deep Learning: A Survey")所示）。多层神经网络的理念可以追溯到1960年代
    [ivakhnenko1966cybernetic](#bib.bib48) ; [schmidhuber2015deep](#bib.bib49) 。然而，它们的可行实施本身就是一个挑战，因为训练时间会过长（由于当时缺乏强大的计算机）。处理器技术的进步，特别是通用目的GPU（GPGPUs）的发展，以及新技术的发展（如Dropout），使得科学家能够更快地训练非常深的神经网络
    [lecun2015deep](#bib.bib50) 。神经网络的主要思想是将（原始）数据通过一系列互连的神经元或节点，每个神经元或节点根据自己的权重和偏差模拟线性或非线性函数。这些权重和偏差在训练过程中会通过从输出[backprop](#bib.bib51)
    的梯度反向传播进行调整，通常是由于期望输出和实际当前输出之间的差异，旨在最小化损失函数或成本函数（根据某些指标的预测和实际输出之间的差异） [bottou1991stochastic](#bib.bib52)
    。我们将在第2节中更详细地讨论不同的深度架构。'
- en: 'Using deep models for biometric recognition, one can learn a hierarchy of concepts
    as we go deeper in the network. Looking at face recognition for example, as shown
    in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Biometrics Recognition Using
    Deep Learning: A Survey"), starting from the first few layers of the deep neural
    network, we can observe learned patterns similar to the Gabor feature (oriented
    edges with different scales). The next few layers can learn more complex texture
    features and part of the face. The following layers are able to catch more complex
    pattern, such as high-bridged nose and big eyes. Finally the last few layers can
    learn very abstract concepts and certain facial attribute (such as smile, roar,
    and even eye color faces).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度模型进行生物特征识别时，随着我们在网络中的深入，可以学习到概念的层次结构。例如，面部识别如图[3](#S1.F3 "图3 ‣ 1 引言 ‣ 使用深度学习的生物特征识别：综述")所示，从深度神经网络的前几层开始，我们可以观察到类似于Gabor特征（具有不同尺度的定向边缘）的学习模式。接下来的几层可以学习更复杂的纹理特征和部分面部特征。进一步的层能够捕捉到更复杂的模式，如高挺的鼻子和大眼睛。最后几层可以学习非常抽象的概念和某些面部属性（如微笑、咆哮，甚至眼睛颜色）。
- en: '![Refer to caption](img/12a26b83f39076df1ce517e79418b6c7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/12a26b83f39076df1ce517e79418b6c7.png)'
- en: 'Figure 3: Illustration of the hierarchical concepts learned by a deep learning
    models trained for face recognition. Courtesy of [deep_face_survey](#bib.bib53)
    .'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：面部识别训练的深度学习模型所学的层次概念示意图。感谢[deep_face_survey](#bib.bib53)。
- en: In this paper, we present a comprehensive review of the recent advances in biometric
    recognition using deep learning frameworks. For each work, we provide an overview
    of the the key contributions, network architecture, and loss functions, developed
    to push state-of-the-art performance in biometric recognition. We have gathered
    more than 150 papers, which appeared between 2014 and 2019, in leading computer
    vision, biometric recognition, and machine learning conferences and journals.
    For each biometric, we provide some of the most popular datasets used by the computer
    vision community, and the most promising state-of-the-art deep learning works
    utilized in the area of biometric recognition. We then provide a quantitative
    analysis of well-known models for each biometric. Finally, we explore the challenges
    associated with deep learning-based methods in biometric recognition and research
    opportunities for the future.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对使用深度学习框架进行生物特征识别的最新进展进行了全面综述。对于每项工作，我们提供了关键贡献、网络架构和损失函数的概述，这些都旨在推动生物特征识别的前沿性能。我们收集了2014年至2019年间在计算机视觉、生物特征识别和机器学习领域的主要会议和期刊上发表的150多篇论文。对于每种生物特征，我们提供了一些计算机视觉社区使用的最受欢迎的数据集，以及该领域中最有前景的深度学习工作。接着，我们对每种生物特征的知名模型进行了定量分析。最后，我们探讨了基于深度学习的生物特征识别方法的挑战以及未来的研究机会。
- en: 'The goal of this survey is to help new researchers in this field to navigate
    through the progress of deep learning-based biometric recognition models, particularly
    with the growing interest of multi-modal biometrics systems [multimodal_challenge](#bib.bib54)
    . Compared to the existing literature, the main contributions of this paper are
    as follow:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查的目标是帮助该领域的新研究人员了解基于深度学习的生物特征识别模型的进展，特别是随着对多模态生物特征系统[多模态挑战](#bib.bib54)的兴趣日益增加。与现有文献相比，本文的主要贡献如下：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the only review paper which provides an
    overview of eight popular biometrics proposed before and in 2019, including face,
    fingerprint, iris, palmprint, ear, voice, signature, and gait.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这篇综述文章是唯一一篇提供2019年及之前八种流行生物特征概述的文献，包括面部、指纹、虹膜、掌纹、耳朵、声音、签名和步态。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We cover the contemporary literature with respect to this area. We present a
    comprehensive review of more than 150 methods, which have appeared since 2014.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们覆盖了这一领域的当代文献。我们提供了自2014年以来出现的150多种方法的全面综述。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive review and an insightful analysis of different aspects
    of biometric recognition using deep learning, including the training data, the
    choice of network architectures, training strategies, and their key contributions.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了关于使用深度学习进行生物特征识别的各个方面的全面评估和深入分析，包括训练数据、网络架构选择、训练策略及其关键贡献。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comparative summary of the properties and performance of the reviewed
    methods for biometric recognition.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了审查过的方法在生物特征识别中的属性和性能的比较总结。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide seven challenges and potential future direction for deep learning-based
    biometric recognition models.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了七个挑战和基于深度学习的生物特征识别模型的潜在未来方向。
- en: The structure of the rest of this paper is as follows. In Section 2, we provide
    an overview of popular deep neural networks architectures, which serve as the
    backbone of many biometric recognition algorithms, including convolutional neural
    networks, recurrent neural networks, auto-encoders, and generative adversarial
    networks. Then in Section 3, we provide an introduction to each of the eight biometrics
    (Face, Fingerprint, Iris, Palmprint, Ear, Voice, Signature, and Gait), some of
    the popular datasets for each of them, as well as the promising deep learning
    based works developed for them. The quantitative results and experimental performance
    of these models for all biometrics are provided in Section 4\. Finally in Section
    5, we explore the challenges and future directions for deep learning-based biometric
    recognition.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构如下。第2节中，我们概述了流行的深度神经网络架构，这些架构作为许多生物特征识别算法的基础，包括卷积神经网络、递归神经网络、自编码器和生成对抗网络。然后在第3节中，我们介绍了八种生物特征（面部、指纹、虹膜、掌纹、耳朵、声音、签名和步态），每种特征的一些流行数据集，以及为它们开发的有前景的基于深度学习的工作。第4节提供了这些模型在所有生物特征上的定量结果和实验性能。最后在第5节中，我们探讨了基于深度学习的生物特征识别的挑战和未来方向。
- en: 2 Deep Neural Network Overview
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度神经网络概述
- en: In this section, we provide an overview of some of the most promising deep learning
    architectures used by the computer vision community, including convolutional neural
    networks (CNN) [CNN](#bib.bib55) , recurrent neural networks (RNN) and one of
    their specific version called long short term memory (LSTM) [lstm](#bib.bib56)
    , auto-encoders, and generative adversarial networks (GANs) [gan](#bib.bib57)
    . It is noteworthy that with the popularity of deep learning in recent years,
    there are several other deep neural architectures proposed (such as Transformers,
    Capsule Network, GRU, and spatial transformer networks), which we will not cover
    in this work.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了计算机视觉领域中一些最具前景的深度学习架构，包括卷积神经网络（CNN）[CNN](#bib.bib55) 、递归神经网络（RNN）及其一个特定版本，称为长短期记忆网络（LSTM）[lstm](#bib.bib56)
    、自编码器和生成对抗网络（GANs）[gan](#bib.bib57) 。值得注意的是，随着近年来深度学习的普及，还有其他几种深度神经网络架构被提出（如Transformers、胶囊网络、GRU和空间变换网络），这些我们将在本工作中不作介绍。
- en: 2.1 Convolutional Neural Networks (CNN)
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 卷积神经网络（CNN）
- en: 'Convolutional Neural Networks (CNN) (inspired by the mammalian visual cortex)
    are one of the most successful and widely used architectures in deep learning
    community (specially for computer vision tasks). CNN was initially proposed by
    Fukushima in a seminal paper, called ”Neocognitron” [neocog](#bib.bib58) , based
    on the model of human visual system proposed by Nobel laureates Hubel and Wiesel.
    Later on Yann Lecun and colleagues developed an optimization framework (based
    on back-propagation) to efficiently learn the model weights for a CNN architecture
    [CNN](#bib.bib55) . The block-diagram of one of the first CNN models developed
    by Lecun et al. is shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Convolutional Neural
    Networks (CNN) ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition Using
    Deep Learning: A Survey").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '卷积神经网络（CNN）（受哺乳动物视觉皮层的启发）是深度学习领域中最成功和广泛使用的架构之一（特别是用于计算机视觉任务）。CNN最初由福岛在一篇开创性论文“Neocognitron”[neocog](#bib.bib58)中提出，基于诺贝尔奖得主Hubel和Wiesel提出的人类视觉系统模型。后来，Yann
    Lecun及其同事开发了一个优化框架（基于反向传播）来有效地学习CNN架构的模型权重[CNN](#bib.bib55) 。Lecun等人开发的第一个CNN模型的框图见于图[4](#S2.F4
    "Figure 4 ‣ 2.1 Convolutional Neural Networks (CNN) ‣ 2 Deep Neural Network Overview
    ‣ Biometrics Recognition Using Deep Learning: A Survey")。'
- en: 'CNNs mainly consist of three type of layers: convolutional layers, where a
    sliding kernel is applied to the image (as in image convolution operation) in
    order to extract features; nonlinear layers (usually applied in an element-wise
    fashion), which apply an activation function on the features in order to enable
    the modeling of non-linear functions by the network; and pooling layers, which
    takes a small neighborhood of the feature map and replaces it with some statistical
    information (mean, max, etc.) of the neighborhood. Nodes in the CNN layers are
    locally connected; that is, each unit in a layer receives input from a small neighborhood
    of the previous layer (known as the receptive field). The main advantage of CNN
    is the weight sharing mechanism through the use of the sliding kernel, which goes
    through the images, and aggregates the local information to extract the features.
    Since the kernel weights are shared across the entire image, CNNs have a significantly
    smaller number of parameters than a similar fully connected neural network. Also
    by stacking multiple convolution layers, the higher-level layers learn features
    from increasingly wider receptive fields.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 主要由三种类型的层组成：卷积层，通过对图像应用滑动卷积核（如图像卷积操作）来提取特征；非线性层（通常以逐元素的方式应用），在特征上应用激活函数，以使网络能够建模非线性函数；以及池化层，它从特征图的小邻域中提取一些统计信息（均值、最大值等）进行替代。CNN
    层中的节点是局部连接的；即，每个层中的单元从前一层的小邻域（称为感受野）接收输入。CNN 的主要优势是通过使用滑动卷积核来进行权重共享，该卷积核遍历图像并汇总局部信息以提取特征。由于卷积核权重在整个图像上共享，因此
    CNN 的参数数量显著少于类似的全连接神经网络。此外，通过堆叠多个卷积层，高层能够从越来越宽的感受野中学习特征。
- en: '![Refer to caption](img/125bacd1df96012853184de2e7a77b04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/125bacd1df96012853184de2e7a77b04.png)'
- en: 'Figure 4: Architecture of a Convolutional Neural Network (CNN), showing the
    main two operations of convolution and pooling. Courtesy of Yann LeCun.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：卷积神经网络（CNN）的架构，展示了卷积和池化的主要两个操作。图片来自 Yann LeCun。
- en: 'CNNs have been applied to various computer vision tasks such as: semantic segmentation
    [seman_seg](#bib.bib59) , medical image segmentation [med_seg](#bib.bib60) , object
    detection [faster_rcnn](#bib.bib61) , super-resolution [sisr](#bib.bib62) , image
    enhancement [enhance](#bib.bib63) , caption generation for image and videos [caption](#bib.bib64)
    , and many more. Some of the most well-known CNN architectures include AlexNet
    [alexnet](#bib.bib47) , ZFNet [zfnet](#bib.bib65) , VGGNet [vggnet](#bib.bib66)
    , ResNet [resnet](#bib.bib67) , GoogLenet [googlenet](#bib.bib68) , MobileNet
    [mobilenet](#bib.bib69) , and DenseNet [densenet](#bib.bib70) .'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 已被应用于各种计算机视觉任务，例如：语义分割 [seman_seg](#bib.bib59)，医学图像分割 [med_seg](#bib.bib60)，目标检测
    [faster_rcnn](#bib.bib61)，超分辨率 [sisr](#bib.bib62)，图像增强 [enhance](#bib.bib63)，图像和视频的标题生成
    [caption](#bib.bib64) 等等。一些最著名的 CNN 架构包括 AlexNet [alexnet](#bib.bib47)，ZFNet [zfnet](#bib.bib65)，VGGNet
    [vggnet](#bib.bib66)，ResNet [resnet](#bib.bib67)，GoogLenet [googlenet](#bib.bib68)，MobileNet
    [mobilenet](#bib.bib69) 和 DenseNet [densenet](#bib.bib70)。
- en: 2.2 Recurrent Neural Networks and LSTM
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 循环神经网络与 LSTM
- en: 'Recurrent Neural Networks (RNNs) [RNN](#bib.bib71) are widely used for processing
    sequential data like speech, text, video, and time-series (such as stock prices),
    where data at any given time/position depends on the previously encountered data.
    A high-level architecture of a simple RNN is shown in Figure [5](#S2.F5 "Figure
    5 ‣ 2.2 Recurrent Neural Networks and LSTM ‣ 2 Deep Neural Network Overview ‣
    Biometrics Recognition Using Deep Learning: A Survey"). As we can see at each
    time-stamp, the model gets the input from the current time $X_{i}$ and the hidden
    state from the previous step $h_{i-1}$ and outputs the hidden state (and possibly
    an output value). The hidden state from the very last time-stamp (or a weighted
    average of all hidden states) can then be used to perform a task.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）[RNN](#bib.bib71) 被广泛用于处理序列数据，如语音、文本、视频和时间序列（如股票价格），其中任何给定时间/位置的数据依赖于之前遇到的数据。图
    [5](#S2.F5 "图 5 ‣ 2.2 循环神经网络与 LSTM ‣ 2 深度神经网络概述 ‣ 使用深度学习进行生物特征识别：综述") 显示了简单 RNN
    的高层架构。如图所示，在每个时间戳，模型接收当前时间的输入 $X_{i}$ 和前一步的隐藏状态 $h_{i-1}$，并输出隐藏状态（以及可能的输出值）。最后一个时间戳的隐藏状态（或所有隐藏状态的加权平均）可以用于执行任务。
- en: '![Refer to caption](img/2444bbe48c1611815e150bbd586e7d85.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2444bbe48c1611815e150bbd586e7d85.png)'
- en: 'Figure 5: Architecture of a Recurrent Neural Network (RNN).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：循环神经网络（RNN）的架构。
- en: RNNs usually suffer when dealing with long sequences, as they cannot capture
    the long-term dependencies of many real application (although in theory there
    is nothing limiting them from doing so). However, there is a variation of RNNs,
    called LSTM, which is designed to better capture long-term dependencies.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 在处理长序列时通常会遇到困难，因为它们无法捕捉许多实际应用中的长期依赖（尽管理论上并没有限制它们这样做）。然而，有一种 RNN 的变体称为 LSTM，它旨在更好地捕捉长期依赖。
- en: 'Long Short Term Memory (LSTM): LSTM is a popular recurrent neural network architecture
    for modeling sequential data, which is designed to have a better ability to capture
    long term dependencies than the vanilla RNN model [lstm](#bib.bib56) . As mentioned
    above, the vanilla RNN often suffers from the gradient vanishing or exploding
    problems, and LSTM network tries to overcome this issue by introducing some internal
    gates. In the LSTM architecture, there are three gates (input gate, output gate,
    forget gate) and a memory cell. The cell remembers values over arbitrary time
    intervals and the other three gates regulate the flow of information into and
    out of the cell. Figure [6](#S2.F6 "Figure 6 ‣ 2.2 Recurrent Neural Networks and
    LSTM ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition Using Deep Learning:
    A Survey") illustrates the inner architecture of a single LSTM module.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '长短期记忆（LSTM）：LSTM 是一种流行的递归神经网络架构，用于建模序列数据，其设计目的是比普通 RNN 模型具有更好的捕捉长期依赖的能力[lstm](#bib.bib56)。如上所述，普通
    RNN 常常遭遇梯度消失或爆炸问题，而 LSTM 网络通过引入一些内部门控来克服这一问题。在 LSTM 架构中，有三个门（输入门、输出门、遗忘门）和一个记忆单元。记忆单元在任意时间间隔内记住值，其他三个门则调节信息的流入和流出。图
    [6](#S2.F6 "Figure 6 ‣ 2.2 Recurrent Neural Networks and LSTM ‣ 2 Deep Neural
    Network Overview ‣ Biometrics Recognition Using Deep Learning: A Survey") 说明了单个
    LSTM 模块的内部结构。'
- en: '![Refer to caption](img/cc19c8962b628e4565c8da701e33f195.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cc19c8962b628e4565c8da701e33f195.png)'
- en: 'Figure 6: The architecture of a standard LSTM module, courtesy of Andrej Karpathy
    [lstm_cell](#bib.bib72) .'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：标准 LSTM 模块的架构，感谢 Andrej Karpathy [lstm_cell](#bib.bib72)。
- en: 'The relationship between input, hidden states, and different gates is shown
    in Equation [1](#S2.E1 "In 2.2 Recurrent Neural Networks and LSTM ‣ 2 Deep Neural
    Network Overview ‣ Biometrics Recognition Using Deep Learning: A Survey"):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '输入、隐藏状态和不同门之间的关系在公式 [1](#S2.E1 "In 2.2 Recurrent Neural Networks and LSTM ‣
    2 Deep Neural Network Overview ‣ Biometrics Recognition Using Deep Learning: A
    Survey") 中展示：'
- en: '|  | $\displaystyle f_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(f)}x_{t}+\textbf{U}^{(f)}h_{t-1}+b^{(f)})$
    |  | (1) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(f)}x_{t}+\textbf{U}^{(f)}h_{t-1}+b^{(f)})$
    |  | (1) |'
- en: '|  | $\displaystyle i_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(i)}x_{t}+\textbf{U}^{(i)}h_{t-1}+b^{(i)})$
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle i_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(i)}x_{t}+\textbf{U}^{(i)}h_{t-1}+b^{(i)})$
    |  |'
- en: '|  | $\displaystyle o_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(o)}x_{t}+\textbf{U}^{(o)}h_{t-1}+b^{(o)})$
    |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(o)}x_{t}+\textbf{U}^{(o)}h_{t-1}+b^{(o)})$
    |  |'
- en: '|  | $\displaystyle c_{t}$ | $\displaystyle=f_{t}\odot c_{t-1}$ |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c_{t}$ | $\displaystyle=f_{t}\odot c_{t-1}$ |  |'
- en: '|  |  | $\displaystyle+i_{t}\odot\text{tanh}(\textbf{W}^{(c)}x_{t}+\textbf{U}^{(c)}h_{t-1}+b^{(c)})$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+i_{t}\odot\text{tanh}(\textbf{W}^{(c)}x_{t}+\textbf{U}^{(c)}h_{t-1}+b^{(c)})$
    |  |'
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\text{tanh}(c_{t})$ |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\text{tanh}(c_{t})$ |  |'
- en: where $x_{t}\in R^{d}$ is the input at time-step t, and $d$ denotes the feature
    dimension for each word, $\sigma$ denotes the element-wise sigmoid function (to
    squash/map the values within $[0,1]$), $\odot$ denotes the element-wise product.
    $c_{t}$ denotes the memory cell designed to lower the risk of vanishing/exploding
    gradient, and therefore enabling the learning of dependencies over larger periods
    of time, which is infeasible with traditional recurrent networks. The forget gate,
    $f_{t}$ is to reset the memory cell. $i_{t}$ and $o_{t}$ denote the input and
    output gates, and essentially control the input and output of the memory cell.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{t}\in R^{d}$ 是时间步 t 的输入，$d$ 表示每个词的特征维度，$\sigma$ 表示逐元素的 sigmoid 函数（将值压缩/映射到
    $[0,1]$ 之间），$\odot$ 表示逐元素乘积。$c_{t}$ 表示设计用来降低梯度消失/爆炸风险的记忆单元，从而实现对更长时间段的依赖学习，这在传统递归网络中是不可能的。遗忘门
    $f_{t}$ 用于重置记忆单元。$i_{t}$ 和 $o_{t}$ 表示输入门和输出门，主要控制记忆单元的输入和输出。
- en: 2.3 Auto-Encoders
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 自编码器
- en: 'Auto-encoders are a family of neural network models used to learn efficient
    data encoding in an unsupervised manner. They achieve this by compressing the
    input data into a latent-space representation, and then reconstructing the output
    (which is usually the same as the input) from this representation. Auto-encoders
    are composed of two parts: Encoder: This is the part of the network that compresses
    the input into a latent-space representation. It can be represented by an encoding
    function $z=f(x)$. Decoder: This part aims to reconstruct the input from the latent
    space representation. It can be represented by a decoding function $y=g(z)$.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一类神经网络模型，用于以无监督的方式学习高效的数据编码。它们通过将输入数据压缩成潜在空间表示，然后从这个表示中重构输出（通常与输入相同）来实现这一目标。自编码器由两个部分组成：编码器：这是网络中将输入压缩成潜在空间表示的部分。它可以用编码函数
    $z=f(x)$ 表示。解码器：这部分旨在从潜在空间表示中重构输入。它可以用解码函数 $y=g(z)$ 表示。
- en: 'The architecture of a simple auto-encoder model is demonstrated in Figure [7](#S2.F7
    "Figure 7 ‣ 2.3 Auto-Encoders ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition
    Using Deep Learning: A Survey").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [7](#S2.F7 "图 7 ‣ 2.3 自编码器 ‣ 2 深度神经网络概述 ‣ 使用深度学习的生物特征识别：一项调查") 展示了一个简单自编码器模型的架构。
- en: '![Refer to caption](img/c0b5336fb93d2eed2585347326937af1.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c0b5336fb93d2eed2585347326937af1.png)'
- en: 'Figure 7: Architecture of a standard Auto-Encoder Model.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 标准自编码器模型的架构。'
- en: Auto-encoders are usually trained by minimizing the reconstruction error, $L(x,\hat{x})$
    (unsupervised, i.e. no need for labeled data), which measures the differences
    between our original input $x$, and the consequent reconstruction $\hat{x}$. Mean
    square error, and mean absolute deviation are popular choices for the reconstruction
    loss in many applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器通常通过最小化重构误差 $L(x,\hat{x})$ （无监督，即不需要标记数据）来进行训练，这个误差衡量了原始输入 $x$ 与重构结果 $\hat{x}$
    之间的差异。均方误差和平均绝对偏差是许多应用中常用的重构损失选择。
- en: There are several variations of auto-encoders where were proposed in the past.
    One of the popular ones is the stacked denoising auto-encoder (SDAE) which stacks
    several auto-encoders and uses them for image denoising [stac_ae](#bib.bib73)
    . Another popular variation of autoencoders is ”variational auto-encoder (VAE)”
    which imposes a prior distribution on the latent representation [vr_ae](#bib.bib74)
    . Variational auto-encoders are able to generate realistic samples from a data
    distribution. Another variation of auto-encoders is the adversarial auto-encoders,
    which introduces an adversarial loss on the latent representation to encourage
    them to be close to a prior distribution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 过去提出了几种变体的自编码器。其中一种流行的变体是堆叠去噪自编码器（SDAE），它堆叠了几个自编码器并用于图像去噪 [stac_ae](#bib.bib73)。另一个流行的自编码器变体是“变分自编码器（VAE）”，它对潜在表示施加了先验分布
    [vr_ae](#bib.bib74)。变分自编码器能够从数据分布中生成真实的样本。另一种自编码器的变体是对抗自编码器，它在潜在表示上引入了对抗损失，以鼓励其接近先验分布。
- en: 2.4 Generative Adversarial Networks (GAN)
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 生成对抗网络（GAN）
- en: 'Generative Adversarial Networks (GANs) are a newer family of deep learning
    models, which consists of two networks, one generator, and one discriminator [gan](#bib.bib57)
    . On a high level, the generator’s job is to generate samples from a distribution
    which are close enough to real samples with the objective to fool the discriminator,
    while the discriminator’s job is to distinguish the generated samples (fakes)
    from the authentic ones. The general architecture of a vanilla GAN model is demonstrated
    in Figure [8](#S2.F8 "Figure 8 ‣ 2.4 Generative Adversarial Networks (GAN) ‣ 2
    Deep Neural Network Overview ‣ Biometrics Recognition Using Deep Learning: A Survey").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）是一类较新的深度学习模型，它由两个网络组成，一个生成器和一个判别器 [gan](#bib.bib57)。从高层次看，生成器的工作是生成接近真实样本的样本，以欺骗判别器，而判别器的工作是区分生成的样本（伪样本）和真实样本。图
    [8](#S2.F8 "图 8 ‣ 2.4 生成对抗网络（GAN） ‣ 2 深度神经网络概述 ‣ 使用深度学习的生物特征识别：一项调查") 展示了一个基础
    GAN 模型的总体架构。
- en: '![Refer to caption](img/acc8370fd09080415685544b15e82d96.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/acc8370fd09080415685544b15e82d96.png)'
- en: 'Figure 8: The architecture of generative adversarial network.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 生成对抗网络的架构。'
- en: 'The generator network in vanilla GAN learns a mapping from noise $z$ (with
    a prior distribution, such as Gaussian) to a target distribution $y$, $G=z\rightarrow
    y$, which look similar to the real samples, while the discriminator network, $D$,
    tries to distinguish the samples generated by the generator models from the real
    ones. The loss function of GAN can be written as Equation [2](#S2.E2 "In 2.4 Generative
    Adversarial Networks (GAN) ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition
    Using Deep Learning: A Survey"):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'vanilla GAN中的生成器网络学习从噪声$z$（具有先验分布，如高斯分布）到目标分布$y$的映射，$G=z\rightarrow y$，这些映射与真实样本相似，而判别器网络$D$则尝试区分生成器模型生成的样本与真实样本。GAN的损失函数可以写作方程[2](#S2.E2
    "In 2.4 Generative Adversarial Networks (GAN) ‣ 2 Deep Neural Network Overview
    ‣ Biometrics Recognition Using Deep Learning: A Survey")：'
- en: '|  | $\displaystyle\mathcal{L}_{GAN}$ | $\displaystyle=\mathbb{E}_{x\sim p_{data}(x)}[\text{log}D(x)]$
    |  | (2) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{GAN}$ | $\displaystyle=\mathbb{E}_{x\sim p_{data}(x)}[\text{log}D(x)]$
    |  | (2) |'
- en: '|  |  | $\displaystyle+\mathbb{E}_{z\sim p_{z}(z)}[\text{log}(1-D(G(z)))]$
    |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\mathbb{E}_{z\sim p_{z}(z)}[\text{log}(1-D(G(z)))]$
    |  |'
- en: 'in which we can think of GAN, as a minimax game between $D$ and $G$, where
    $D$ is trying to minimize its classification error in detecting fake samples from
    the real ones (maximize the above loss function), and $G$ is trying to maximize
    the discriminator network’s error (minimize the above loss function). After training
    this model, the trained generator model would be:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，我们可以将GAN视为$D$和$G$之间的极小极大游戏，其中$D$试图最小化其在检测真假样本中的分类错误（最大化上述损失函数），而$G$试图最大化判别器网络的错误（最小化上述损失函数）。训练完成后，训练后的生成器模型将是：
- en: '|  | $\displaystyle G^{*}=\text{arg}\ \text{min}_{G}\text{max}_{D}\ \mathcal{L}_{GAN}$
    |  | (3) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G^{*}=\text{arg}\ \text{min}_{G}\text{max}_{D}\ \mathcal{L}_{GAN}$
    |  | (3) |'
- en: 'In practice, the loss function in Equation [3](#S2.E3 "In 2.4 Generative Adversarial
    Networks (GAN) ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition Using
    Deep Learning: A Survey") may not provide enough gradient for $G$ to get trained
    well, specially at the beginning where $D$ can easily detect fake samples from
    the real ones. One solution is to maximize $\mathbb{E}_{z\sim p_{z}(z)}[\text{log}(D(G(z)))]$.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '实际操作中，方程[3](#S2.E3 "In 2.4 Generative Adversarial Networks (GAN) ‣ 2 Deep Neural
    Network Overview ‣ Biometrics Recognition Using Deep Learning: A Survey")中的损失函数可能无法提供足够的梯度，使得$G$无法得到良好的训练，特别是在开始阶段，$D$可以轻松区分真假样本。一个解决方案是最大化$\mathbb{E}_{z\sim
    p_{z}(z)}[\text{log}(D(G(z)))]$。'
- en: Since the invention of GAN, there have been several works trying to improve/modify
    GAN in different aspects. For a detailed list of works relevant to GAN, please
    refer to [GanZoo](#bib.bib75) .
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自从GAN的发明以来，已经有一些工作尝试在不同方面改进/修改GAN。有关GAN的详细工作列表，请参见[GanZoo](#bib.bib75)。
- en: 2.5 Transfer Learning Approach
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 迁移学习方法
- en: Now that we talked about some of the popular deep learning architectures, let
    us briefly talk about how these models are applied to new applications. Of course,
    these models can always be trained from scratch on new applications, assuming
    they are provided with sufficient labeled data. But depending on the depth of
    the model (i.e. how large if the number of parameters), it may not be very straightforward
    to make the model converge to a good local minimum. Also, for many applications,
    there may not be enough labeled data available to train a deep model from scratch.
    For these situations, transfer learning approach can be used to better handle
    labeled data limitations and the local-minimum problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们讨论了一些流行的深度学习架构，让我们简要讨论一下这些模型如何应用于新的应用领域。当然，这些模型总是可以从头开始在新应用上进行训练，前提是提供足够的标记数据。但根据模型的深度（即参数数量的多少），可能不容易使模型收敛到良好的局部最小值。此外，对于许多应用，可能没有足够的标记数据来从头开始训练深度模型。在这些情况下，可以使用迁移学习方法来更好地处理标记数据的限制和局部最小值问题。
- en: In transfer learning, a model trained on one task is re-purposed on another
    related task, usually by some adaptation toward the new task. For example, one
    can imagine using an image classification model trained on ImageNet, to be used
    for a different task such as texture classification, or iris recognition. There
    are two main ways in which the pre-trained model is used for a different task.
    In one approach, the pre-trained model, e.g. a language model, is treated as a
    feature extractor, and a classifier is trained on top of it to perform classification
    (e.g. sentiment classification). Here the internal weights of the pre-trained
    model are not adapted to the new task. In the other approach, the whole network,
    or a subset of it, is fine-tuned on the new task. Therefore the pre-trained model
    weights are treated as the initial values for the new task, and are updated during
    the training stage.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，经过一个任务训练的模型被重新用于另一个相关任务，通常是通过对新任务的某种适应。例如，可以设想使用在ImageNet上训练的图像分类模型，用于不同的任务，如纹理分类或虹膜识别。预训练模型用于不同任务的主要方式有两种。一种方法是将预训练模型（例如语言模型）视为特征提取器，在其上训练一个分类器以进行分类（例如情感分类）。在这种情况下，预训练模型的内部权重不会适应新任务。另一种方法是对整个网络或其子集进行微调以适应新任务。因此，预训练模型的权重被视为新任务的初始值，并在训练阶段进行更新。
- en: Many of the deep learning-based models for biometric recognition are based on
    transfer learning (except for voice because of the difference in the nature of
    the data, and face because of the availability of large-scale datasets), which
    we are going to explain in the following section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于深度学习的生物特征识别模型都是基于迁移学习的（语音除外，因为数据性质的不同，人脸除外，因为大规模数据集的可用性），这一点我们将在接下来的章节中解释。
- en: 3 Deep Learning Based Works on Biometric Recognition
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于深度学习的生物特征识别工作
- en: In this section we provide an overview of some of the most promising deep learning
    works for various biometric recognition works. Within each subsection, we also
    provide a summary of some of the most popular datasets for each biometric.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一些最有前景的深度学习在各种生物特征识别工作的概述。在每个子节中，我们还提供了每种生物特征的一些最受欢迎的数据集的总结。
- en: 3.1 Face Recognition
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 人脸识别
- en: Face is perhaps one of the most popular biometrics (and the most researched
    one during the last few years). It has a wide range of applications, from security
    cameras in airports and government offices, to daily usage for cellphone authentication
    (such as in FaceID in iPhones). Various hand-crafted features were used for recognition
    in the past, such as the LBP, Gabor Wavelet, SIFT, HoG, and also sparsity-based
    representations [deng2013defense](#bib.bib76) , [cao2013similarity](#bib.bib77)
    , [face_sparse](#bib.bib78) , [yang2012regularized](#bib.bib79) , [yi2013towards](#bib.bib80)
    . Both 2D and 3D versions of faces are used for recognition [mian2007efficient](#bib.bib81)
    , but most people have focused on 2D face recognition so far. One of the main
    challenges for facial recognition is the face’s susceptibility to change over
    time due to aging or external factors, such as scars, or medical conditions [park2010age](#bib.bib24)
    . We will introduce some of the most widely used face recognition datasets in
    the next section, and then talk about the promising deep learning-based face recognition
    models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸可能是最受欢迎的生物特征之一（也是近年来研究最多的）。它有广泛的应用，从机场和政府办公室的安全摄像头，到日常的手机认证（如iPhone中的FaceID）。过去，使用了各种手工设计的特征进行识别，如LBP、Gabor
    Wavelet、SIFT、HoG，以及基于稀疏表示的特征[deng2013defense](#bib.bib76)、[cao2013similarity](#bib.bib77)、[face_sparse](#bib.bib78)、[yang2012regularized](#bib.bib79)、[yi2013towards](#bib.bib80)。人脸的2D和3D版本都用于识别[mian2007efficient](#bib.bib81)，但大多数人迄今为止主要关注2D人脸识别。人脸识别的主要挑战之一是人脸随时间的变化，如衰老、伤疤或医疗状况[park2010age](#bib.bib24)。我们将在下一节介绍一些最广泛使用的人脸识别数据集，然后讨论有前途的深度学习基础的人脸识别模型。
- en: 3.1.1 Face Datasets
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 人脸数据集
- en: Due to the wide application of face recognition in the industry, a large number
    of datasets are proposed for that purpose. We will introduce some of the most
    popular ones here.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人脸识别在工业中的广泛应用，提出了大量的数据集。我们将在这里介绍一些最受欢迎的数据集。
- en: 'Yale and Yale Face Database B: Yale face dataset is perhaps one of the earliest
    face recognition datasets [yale](#bib.bib82) . It Contains 165 grayscale images
    of 15 individuals. There are 11 images per subject, one per different facial expression
    or configuration (center-light, w/glasses, happy, left-light, w/no glasses, normal,
    right-light, sad, sleepy, surprised, and wink).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Yale 和 Yale Face Database B：Yale 面部数据集可能是最早的面部识别数据集之一 [yale](#bib.bib82)。它包含了15个人的165张灰度图像。每个人有11张图像，分别对应不同的面部表情或配置（中心光、带眼镜、开心、左侧光、无眼镜、正常、右侧光、伤心、困倦、惊讶和眨眼）。
- en: 'It is extended version, Yale Face Database B [yaleb](#bib.bib32) , contains
    5760 single light source images of 10 subjects each seen under 576 viewing conditions
    (9 poses x 64 illumination conditions). For every subject in a particular pose,
    an image with ambient (background) illumination was also captured. Ten example
    images from Yale face B dataset are shown in Figure [9](#S3.F9 "Figure 9 ‣ 3.1.1
    Face Datasets ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric
    Recognition ‣ Biometrics Recognition Using Deep Learning: A Survey").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '这是扩展版的 Yale Face Database B [yaleb](#bib.bib32)，包含了5760张10名受试者在576种观看条件下（9种姿势
    x 64种光照条件）下的单一光源图像。对于每个特定姿势的受试者，还捕捉了一张带有环境（背景）光照的图像。图 [9](#S3.F9 "Figure 9 ‣ 3.1.1
    Face Datasets ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric
    Recognition ‣ Biometrics Recognition Using Deep Learning: A Survey") 显示了Yale Face
    B 数据集中的十个示例图像。'
- en: '![Refer to caption](img/ea1d6bf59370fa442524649b792965ea.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ea1d6bf59370fa442524649b792965ea.png)'
- en: 'Figure 9: Ten example images from Yale Face B Dataset.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：Yale Face B 数据集中的十个示例图像。
- en: 'CMU Multi-PIE: The CMU Multi-PIE face database contains more than 750,000 images
    of 337 people [cmu](#bib.bib83) , [gross2010multi](#bib.bib84) . Subjects were
    imaged under 15 view points and 19 illumination conditions while displaying a
    range of facial expressions.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CMU Multi-PIE：CMU Multi-PIE 面部数据库包含了337个人的超过750,000张图像 [cmu](#bib.bib83)，[gross2010multi](#bib.bib84)。受试者在15个视角和19种光照条件下被拍摄，展现了一系列面部表情。
- en: 'Labeled Face in The Wild (LFW): Labeled Faces in the Wild is a database of
    face images designed for studying unconstrained face recognition. The database
    contains more than 13,000 images of faces collected from the web. Each face has
    been labeled with the name of the person pictured [lfw](#bib.bib85) . 1680 of
    the people pictured have two or more distinct photos in the database. The only
    constraint on these faces is that they were detected by the Viola-Jones face detector.
    For more details on this dataset, we refer the readers to the database web-page.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Labeled Face in The Wild (LFW)：Labeled Faces in the Wild 是一个用于研究无约束面部识别的面部图像数据库。该数据库包含了来自网络的超过13,000张面部图像。每张面部图像都标注了图像中人的名字
    [lfw](#bib.bib85)。其中1680人有两张或更多不同的照片。唯一的约束是这些面部图像是通过Viola-Jones面部检测器检测到的。有关此数据集的更多详细信息，请参阅数据库网页。
- en: 'PolyU NIR Face Database: The Biometric Research Centre at The Hong Kong Polytechnic
    University developed a NIR face capture device and used it to construct a large-scale
    NIR face database [polyu_face](#bib.bib86) . By using the self-designed data acquisition
    device, they collected NIR face images from 335 subjects. In each recording, 100
    images from each subject is captured, and in total about 34,000 images were collected
    in the PolyU-NIRFD database.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: PolyU NIR Face Database：香港理工大学的生物识别研究中心开发了一种NIR面部捕捉设备，并利用它构建了一个大规模的NIR面部数据库
    [polyu_face](#bib.bib86)。通过使用自设计的数据采集设备，他们从335名受试者那里收集了NIR面部图像。在每次记录中，捕捉了每个受试者的100张图像，总共在PolyU-NIRFD数据库中收集了约34,000张图像。
- en: 'YouTube Faces: This data set contains 3,425 videos of 1,595 different people.
    All videos were downloaded from YouTube. An average of 2.15 videos are available
    for each subject. The goal of this dataset was to produce a large scale collection
    of videos along with labels indicating the identities of a person appearing in
    each video [youtube](#bib.bib87) . In addition, they published benchmark tests,
    intended to measure the performance of video pair-matching techniques on these
    videos.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube Faces：该数据集包含了3,425段1,595名不同人的视频。所有视频均从YouTube下载。每个受试者平均有2.15段视频。此数据集的目标是生成一个大规模的视频集合，并标注出每段视频中出现的人的身份
    [youtube](#bib.bib87)。此外，他们发布了基准测试，旨在衡量这些视频上的视频配对匹配技术的性能。
- en: 'VGGFace2: VGGFace2 is a large-scale face recognition dataset [vggface2](#bib.bib88)
    . Images are downloaded from Google Image Search and have a large variations in
    pose, age, illumination, ethnicity and profession. It contains 3.31 million images
    of 9131 subjects (identities), with an average of 362.6 images for each subject.
    Face distribution for different identities is varied, from 87 to 843.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'VGGFace2: VGGFace2是一个大规模面部识别数据集 [vggface2](#bib.bib88)。图像从Google图像搜索下载，具有很大的姿态、年龄、光照、种族和职业变化。它包含3.31百万张9131个身份的图像，每个身份平均有362.6张图像。不同身份的面部分布变化，从87到843张。'
- en: 'CASIA-WebFace: CASIA WebFace Facial dataset of 453,453 images over 10,575 identities
    after face detection [casiawebface](#bib.bib89) . This is one of the largest publicly
    available face datasets.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 'CASIA-WebFace: CASIA WebFace人脸数据集包含453,453张图像，覆盖10,575个身份，经过人脸检测处理 [casiawebface](#bib.bib89)。这是最大的一些公开可用的人脸数据集之一。'
- en: 'MS-Celeb: Microsoft Celeb is a dataset of 10 million face images harvested
    from the Internet for the purpose of developing face recognition technologies,
    from nearly 100,000 individuals [msceleb1m](#bib.bib90) .'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'MS-Celeb: Microsoft Celeb是一个由来自互联网的1000,000个个体收集的1000万张面部图像的数据集 [msceleb1m](#bib.bib90)。'
- en: 'CelebA: CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes
    dataset with more than 200K celebrity images [celeba](#bib.bib91) . CelebA has
    a large diversity, large quantities, and rich annotations, including more than
    10,000 identities, more than 202,599 face images, 5 landmark locations, 40 binary
    attributes annotations per image. The dataset can be employed as the training
    and test sets for the following computer vision tasks: face attribute recognition,
    face detection, landmark (or facial part) localization, and face editing & synthesis.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'CelebA: CelebFaces属性数据集（CelebA）是一个大规模的面部属性数据集，包含超过200K名名人图像 [celeba](#bib.bib91)。CelebA具有极大的多样性、大量的数据以及丰富的注释，包括超过10,000个身份、202,599张面部图像、5个标志点位置、每张图像40个二元属性注释。该数据集可用于以下计算机视觉任务的训练和测试：面部属性识别、面部检测、标志点（或面部部位）定位以及面部编辑与合成。'
- en: 'IJB-C: The IJB-C dataset [ijbc](#bib.bib92) contains about 3,500 identities
    with a total of 31,334 still facial images and 117,542 unconstrained video frames.
    The entire IJB-C testing protocols are designed to test detection, identification,
    verification and clustering of faces. In the 1:1 verification protocol, there
    are 19,557 positive matches and 15,638,932 negative matches.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'IJB-C: IJB-C数据集 [ijbc](#bib.bib92) 包含约3,500个身份，共31,334张静态面部图像和117,542张无约束的视频帧。整个IJB-C测试协议旨在测试面部的检测、识别、验证和聚类。在1:1验证协议中，有19,557个正匹配和15,638,932个负匹配。'
- en: 'MegaFace: MegaFace Challenge [megaface](#bib.bib93) is a publicly available
    benchmark, which is widely used to test the performance of facial recognition
    algorithms (for both identification and verification). The gallery set of MegaFace
    contains over 1 million images from 690K identities collected from Flickr [flicker](#bib.bib94)
    . The probe sets are two existing databases: FaceScrub and FGNet. The FaceScrub
    dataset contains 106,863 face images of 530 celebrities. The FGNet dataset is
    mainly used for testing age invariant face recognition, with 1002 face images
    from 82 persons.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'MegaFace: MegaFace Challenge [megaface](#bib.bib93) 是一个公开可用的基准测试，广泛用于测试面部识别算法的性能（包括识别和验证）。MegaFace的库集包含超过100万张来自690K个身份的Flickr图像
    [flicker](#bib.bib94)。探测集包括两个现有数据库：FaceScrub和FGNet。FaceScrub数据集包含530位名人的106,863张面部图像。FGNet数据集主要用于测试年龄不变的面部识别，包含82人共1002张面部图像。'
- en: 'Other Datasets: It is worth mentioning that there are several other datasets
    which we skipped the details due to being private or less popularity, such as
    DeepFace (Facebook private dataset of 4.4M photos of 4k subjects), NTechLab (a
    private dataset of 18.4M photos of 200k subjects), FaceNet (Google private dataset
    of more than 500M photos of more than 10M subjects), WebFaces (a dataset of 80M
    photos crawled from web) [megaface](#bib.bib93) , and Disguised Faces in the Wild
    (DFW) [DFW](#bib.bib95) which contains over 11,000 images of 1,000 identities
    with variations across different types of disguise accessories.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '其他数据集: 值得一提的是，还有一些我们跳过了细节的其他数据集，因为它们是私有的或不太受欢迎，例如DeepFace（Facebook的私有数据集，包含4.4M张4k个受试者的照片）、NTechLab（一个私有数据集，包含18.4M张200k个受试者的照片）、FaceNet（Google的私有数据集，包含超过500M张超过10M个受试者的照片）、WebFaces（一个从网络爬取的80M张照片的数据集）
    [megaface](#bib.bib93) 以及Disguised Faces in the Wild (DFW) [DFW](#bib.bib95)，其中包含超过11,000张1,000个身份的图像，具有不同类型的伪装配件。'
- en: 3.1.2 Deep Learning Works on Face Recognition
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 深度学习在人脸识别中的应用
- en: There are countless number of works using deep learning for face recognition.
    In this survey, we provide an overview of some of the most promising works developed
    for face verification and/or identification.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习进行人脸识别的工作不胜枚举。在这次调查中，我们提供了一些最具前景的人脸验证和/或识别工作的概述。
- en: 'In 2014, Taigman and colleagues proposed one of the earliest deep learning
    work for face recognition in a paper called DeepFace [DeepFace](#bib.bib96) ,
    and achieved the state-of-the-art accuracy on the LFW benchmark [lfw](#bib.bib85)
    , approaching human performance on the unconstrained condition for the first time
    ever (DeepFace: 97.35% vs. Human: 97.53%). DeepFace was trained on 4 million facial
    images. This work was a milestone on face recognition, and after that several
    researchers started using deep learning for face recognition.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '2014 年，Taigman 和同事在名为 DeepFace 的论文中提出了早期的深度学习人脸识别工作 [DeepFace](#bib.bib96)，并在
    LFW 基准测试 [lfw](#bib.bib85) 上达到了最先进的准确率，首次在无约束条件下接近人类表现（DeepFace: 97.35% vs. Human:
    97.53%）。DeepFace 在 400 万张面孔图像上进行了训练。这项工作是人脸识别领域的一个里程碑，此后，许多研究人员开始使用深度学习进行人脸识别。'
- en: In another promising work in the same year, Sun et al. proposed DeepID (Deep
    hidden IDentity features) [DeepID](#bib.bib97) , for face verification. DeepID
    features were taken from the last hidden layer of a deep convolutional network,
    which is trained to recognize about 10,000 face identities in the training set.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一年中，Sun 等人提出了 DeepID（深度隐藏身份特征）[DeepID](#bib.bib97)，用于人脸验证。DeepID 特征取自深度卷积网络的最后一层隐藏层，该网络被训练来识别训练集中约
    10,000 个面孔身份。
- en: 'In a follow up work, Sun et al. extended DeepID for joint face identification
    and verification called DeepID2 [DeepID2](#bib.bib98) . By training the model
    for joint identification and verification, they showed that the face identification
    task increases the inter-personal variations by drawing DeepID2 features extracted
    from different identities apart, while the face verification task reduces the
    intra-personal variations by pulling DeepID2 features extracted from the same
    identity together. For identification, cross-entropy is used as the loss function
    (as defined in the Equation [4](#S3.E4 "In 3.1.2 Deep Learning Works on Face Recognition
    ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric Recognition
    ‣ Biometrics Recognition Using Deep Learning: A Survey")), while for verification
    they proposed to use the loss function of Equation [5](#S3.E5 "In 3.1.2 Deep Learning
    Works on Face Recognition ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works
    on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning: A Survey")
    to reduce the intra-class distances on the features and increase the inter-class
    distances.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在后续工作中，Sun 等人扩展了 DeepID，提出了用于人脸识别和验证的 DeepID2 [DeepID2](#bib.bib98)。通过联合训练模型进行识别和验证，他们展示了人脸识别任务通过将不同身份的
    DeepID2 特征分开，从而增加了个体间的变化，而人脸验证任务通过将相同身份的 DeepID2 特征拉在一起，减少了个体内的变化。对于识别任务，使用交叉熵作为损失函数（如方程
    [4](#S3.E4 "In 3.1.2 Deep Learning Works on Face Recognition ‣ 3.1 Face Recognition
    ‣ 3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey") 所定义），而对于验证任务，他们建议使用方程 [5](#S3.E5 "In 3.1.2 Deep
    Learning Works on Face Recognition ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based
    Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning: A
    Survey") 中的损失函数来减少特征的类内距离并增加类间距离。'
- en: '|  | $\mathbf{L}_{Ident}(f,t,\theta_{id})=-\sum_{i}p_{i}\log\hat{p}_{i}$ |  |
    (4) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{L}_{Ident}(f,t,\theta_{id})=-\sum_{i}p_{i}\log\hat{p}_{i}$ |  |
    (4) |'
- en: '|  | <math   alttext="\begin{split}\mathbf{L}_{Verif}&amp;(f_{i},f_{j},y_{ij},\theta_{vr})=\\
    &amp;\begin{cases}\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},&amp;\text{if }y_{ij}=1\\'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\mathbf{L}_{Verif}&amp;(f_{i},f_{j},y_{ij},\theta_{vr})=\\
    &amp;\begin{cases}\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},&amp;\text{if }y_{ij}=1\\'
- en: \frac{1}{2}max(1-\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},0),&amp;\text{otherwise}\end{cases}\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  columnalign="right" ><msub ><mi  >𝐋</mi><mrow ><mi >V</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi  >e</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >r</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >f</mi></mrow></msub></mtd><mtd columnalign="left"  ><mrow ><mrow ><mo stretchy="false"  >(</mo><msub
    ><mi >f</mi><mi  >i</mi></msub><mo >,</mo><msub ><mi  >f</mi><mi >j</mi></msub><mo
    >,</mo><msub ><mi  >y</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >j</mi></mrow></msub><mo >,</mo><msub ><mi  >θ</mi><mrow ><mi >v</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >r</mi></mrow></msub><mo stretchy="false"  >)</mo></mrow><mo
    >=</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><mstyle displaystyle="false"  ><mfrac ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><msubsup ><mrow ><mo stretchy="false"  >‖</mo><mrow
    ><msub ><mi >f</mi><mi >i</mi></msub><mo >−</mo><msub ><mi >f</mi><mi >j</mi></msub></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn >2</mn><mn >2</mn></msubsup></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >y</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow></msub></mrow><mo
    >=</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mrow
    ><mstyle displaystyle="false"  ><mfrac ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >x</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mn >1</mn><mo >−</mo><mrow ><mstyle displaystyle="false"  ><mfrac
    ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em"  >​</mo><msubsup
    ><mrow ><mo stretchy="false"  >‖</mo><mrow ><msub ><mi >f</mi><mi >i</mi></msub><mo
    >−</mo><msub ><mi >f</mi><mi >j</mi></msub></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >2</mn><mn >2</mn></msubsup></mrow></mrow><mo >,</mo><mn >0</mn><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mtext >otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐋</ci><apply ><ci  >𝑉</ci><ci >𝑒</ci><ci >𝑟</ci><ci  >𝑖</ci><ci >𝑓</ci></apply></apply><vector
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑓</ci><ci  >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑓</ci><ci >𝑗</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑦</ci><apply ><ci  >𝑖</ci><ci
    >𝑗</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><apply ><ci >𝑣</ci><ci >𝑟</ci></apply></apply></vector></apply><apply ><csymbol
    cd="latexml" >cases</csymbol><apply ><apply ><cn type="integer"  >1</cn><cn type="integer"  >2</cn></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="latexml"  >norm</csymbol><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑓</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑓</ci><ci >𝑗</ci></apply></apply></apply><cn type="integer"  >2</cn></apply><cn
    type="integer"  >2</cn></apply></apply><apply ><apply ><ci ><mtext >if </mtext></ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><apply ><ci >𝑖</ci><ci
    >𝑗</ci></apply></apply></apply><cn type="integer"  >1</cn></apply><apply ><apply
    ><cn type="integer"  >1</cn><cn type="integer"  >2</cn></apply><ci >𝑚</ci><ci
    >𝑎</ci><ci >𝑥</ci><interval closure="open"  ><apply ><cn type="integer"  >1</cn><apply
    ><apply ><cn type="integer"  >1</cn><cn type="integer"  >2</cn></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="latexml"  >norm</csymbol><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑓</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑓</ci><ci >𝑗</ci></apply></apply></apply><cn type="integer"  >2</cn></apply><cn
    type="integer"  >2</cn></apply></apply></apply><cn type="integer"  >0</cn></interval></apply><ci
    ><mtext >otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{split}\mathbf{L}_{Verif}&(f_{i},f_{j},y_{ij},\theta_{vr})=\\ &\begin{cases}\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},&\text{if
    }y_{ij}=1\\ \frac{1}{2}max(1-\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},0),&\text{otherwise}\end{cases}\end{split}</annotation></semantics></math>
    |  | (5) |
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{L}_{Verif}&(f_{i},f_{j},y_{ij},\theta_{vr})=\\ &\begin{cases}\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},&\text{如果
    }y_{ij}=1\\ \frac{1}{2}max(1-\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},0),&\text{否则}\end{cases}\end{split}
- en: As an extension of DeepID2, in DeepID3 [DeepID3](#bib.bib99) Sun et al proposed
    a new model which has higher dimensional hidden representation, and deploys VGGNet
    and GoogleNet as the main architectures.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 DeepID2 的扩展，DeepID3 [DeepID3](#bib.bib99) 中，Sun 等人提出了一个新模型，该模型具有更高维度的隐藏表示，并采用了
    VGGNet 和 GoogleNet 作为主要架构。
- en: 'In 2015, FaceNet [FaceNet](#bib.bib100) trained a GoogLeNet model on a large
    private dataset. This work tried to learn a mapping from face images to a compact
    Euclidean space where distances directly corresponds to a measure of face similarity.
    It adopted a triplet loss function based on triplets of roughly aligned matching/non-matching
    face patches generated by a novel online triplet mining method and achieved good
    performance on LFW dataset (99.63%). Given features for a given sample $f(x_{i}^{a})$,
    a positive sample $f(x_{i}^{p})$ (matching $x_{i}^{a}$), and a negative sample
    $f(x_{i}^{n})$, the triplet loss for a given margin $\alpha$ is defined as Equation
    [6](#S3.E6 "In 3.1.2 Deep Learning Works on Face Recognition ‣ 3.1 Face Recognition
    ‣ 3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey"):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '在 2015 年，FaceNet [FaceNet](#bib.bib100) 在一个大型私有数据集上训练了一个 GoogLeNet 模型。这项工作试图学习一个从人脸图像到紧凑欧几里得空间的映射，其中距离直接对应于人脸相似性的度量。它采用了一种基于粗略对齐的匹配/非匹配人脸补丁的三元组损失函数，这些补丁通过一种新颖的在线三元组挖掘方法生成，并在
    LFW 数据集上取得了良好的表现（99.63%）。给定样本 $f(x_{i}^{a})$，正样本 $f(x_{i}^{p})$（匹配 $x_{i}^{a}$）和负样本
    $f(x_{i}^{n})$，给定边际 $\alpha$ 的三元组损失定义如公式 [6](#S3.E6 "In 3.1.2 Deep Learning Works
    on Face Recognition ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric
    Recognition ‣ Biometrics Recognition Using Deep Learning: A Survey")：'
- en: '|  | $\mathbf{L}_{triplet}=\sum\bigg{[}\&#124;f(x_{i}^{a})-f(x_{i}^{p})\&#124;_{2}^{2}-\&#124;f(x_{i}^{a})-f(x_{i}^{n})\&#124;_{2}^{2}+\alpha\bigg{]}_{+}$
    |  | (6) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{L}_{triplet}=\sum\bigg{[}\|f(x_{i}^{a})-f(x_{i}^{p})\|_{2}^{2}-\|f(x_{i}^{a})-f(x_{i}^{n})\|_{2}^{2}+\alpha\bigg{]}_{+}$
    |  | (6) |'
- en: In the same year, Parkhi et al. proposed a model called VGGface [VGGface_model](#bib.bib101)
    (trained on a large-scale dataset collected from the Internet). It trained the
    VGGNet on this dataset and fine-tuned the networks via a triplet loss function,
    Similar to FaceNet. VGGface obtained a very high accuracy rate of 98.95%.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一年，Parkhi 等人提出了一种名为 VGGface 的模型 [VGGface_model](#bib.bib101)（在从互联网收集的大规模数据集上训练）。它在该数据集上训练了
    VGGNet，并通过三元组损失函数对网络进行了微调，类似于 FaceNet。VGGface 获得了非常高的准确率 98.95%。
- en: In 2016, Liu and colleagues developed a ”Large-Margin Softmax Loss” for CNNs
    [Largesoftmax](#bib.bib102) , and showed its promise on multiple computer vision
    datasets, including LFW. They claimed that, cross-entropy does not explicitly
    encourage discriminative learning of features, and proposed a generalized large-margin
    softmax loss, which explicitly encourages intra-class compactness and inter-class
    separability between learned features.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2016 年，Liu 和同事开发了一种“Large-Margin Softmax Loss”用于 CNNs [Largesoftmax](#bib.bib102)，并在多个计算机视觉数据集上展示了其前景，包括
    LFW。他们声称，交叉熵并没有明确鼓励特征的判别学习，并提出了一种广义的大间隔 softmax 损失，它明确鼓励学习特征之间的类别内紧凑性和类别间可分离性。
- en: In the same year, Wen et al. proposed a new supervision signal, called ”center
    loss”, for face recognition task [wen2016discriminative](#bib.bib103) . The center
    loss simultaneously learns a center for deep features of each class and penalizes
    the distances between the deep features and their corresponding class centers.
    With the joint supervision of softmax loss and center loss, they trained a CNN
    to obtain the deep features with the two key learning objectives, inter-class
    dispension and intra-class compactness as much as possible.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一年，Wen 等人提出了一种新的监督信号，称为“中心损失”，用于人脸识别任务 [wen2016discriminative](#bib.bib103)。中心损失同时为每个类别的深度特征学习一个中心，并惩罚深度特征与其对应类别中心之间的距离。在
    softmax 损失和中心损失的联合监督下，他们训练了一个 CNN，以尽可能获得具有两个关键学习目标的深度特征，即类别间分散性和类别内紧凑性。
- en: In another work in 2016, Sun et al. proposed a face recognition model using
    a convolutional network with sparse neural connections [sun2016sparsifying](#bib.bib104)
    . This sparse ConvNet is learned in an iterative fashion, where each time one
    additional layer is sparsified and the entire model is re-trained given the initial
    weights learned in previous iterations (they found out training the sparse ConvNet
    from scratch usually fails to find good solutions for face recognition).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年的另一项工作中，Sun 等人提出了一种使用具有稀疏神经连接的卷积网络的面部识别模型 [sun2016sparsifying](#bib.bib104)
    。这种稀疏ConvNet以迭代方式学习，每次稀疏化一个额外的层，并根据先前迭代中学习到的初始权重重新训练整个模型（他们发现从头开始训练稀疏ConvNet通常无法找到面部识别的良好解决方案）。
- en: In 2017, in [RangeLoss](#bib.bib105) , Zhang and colleagues developed a range
    loss to reduce the overall intra-personal variations while increasing inter-personal
    differences simultaneously. In the same year, Ranjan and colleagues developed
    an ”L2-constraint softmax loss function” and used it for face verification [L2softmax](#bib.bib106)
    . This loss function restricts the feature descriptors to lie on a hyper-sphere
    of a fixed radius. This work achieved state-of-the-art performance on LFW dataset
    with an accuracy of 99.78% at the time. In [cocoloss](#bib.bib107) , Liu and colleagues
    developed a face recognition model based on the intuition that the cosine distance
    of face features in high-dimensional space should be close enough within one class
    and far away across categories. They proposed the congenerous cosine (COCO) algorithm
    to simultaneously optimize the cosine similarity among data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，在 [RangeLoss](#bib.bib105) 中，Zhang 和同事们开发了一种范围损失，以同时减少整体的个人内变异并增加个人间差异。同年，Ranjan
    和同事们开发了一种“L2约束softmax损失函数”，并将其用于面部验证 [L2softmax](#bib.bib106) 。该损失函数限制特征描述符位于固定半径的超球面上。这项工作在LFW数据集上取得了当时99.78%的最先进性能。在
    [cocoloss](#bib.bib107) 中，Liu 和同事们开发了一种基于这样一种直觉的面部识别模型：即高维空间中的面部特征的余弦距离在一个类别内应足够接近，而在不同类别间应足够远。他们提出了同源余弦（COCO）算法，以同时优化数据之间的余弦相似度。
- en: In the same year, Liu et al. developed SphereFace [Sphereface](#bib.bib108)
    , a deep hypersphere embedding for face recognition. They proposed an angular
    softmax (A-Softmax) loss function that enables CNNs to learn angular discriminative
    features. Geometrically, A-Softmax loss can be viewed as imposing discriminative
    constraints on a hypersphere manifold, which intrinsically matches the prior that
    faces also lie on a manifold. They showed promising face recognition accuracy
    on LFW, MegaFace, and Youtube Face databases.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 同年，Liu 等人开发了 SphereFace [Sphereface](#bib.bib108) ，这是一种用于面部识别的深度超球体嵌入。他们提出了一种角度Softmax（A-Softmax）损失函数，使得CNN能够学习角度上区分特征。从几何上看，A-Softmax损失可以视为对超球体流形施加区分约束，这本质上匹配了面部也位于流形上的先验。他们在LFW、MegaFace和Youtube
    Face数据库上展示了有前景的面部识别准确性。
- en: In 2018, in [amsloss](#bib.bib109) Wang et al. developed a simple and geometrically
    interpretable objective function, called additive margin Softmax (AM-Softmax),
    for deep face verification. This work is heavily inspired by two previous works,
    Large-margin Softmax [Largesoftmax](#bib.bib102) , and Angular Softmax in [Sphereface](#bib.bib108)
    .
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年，在 [amsloss](#bib.bib109) 中，Wang 等人开发了一种简单且几何上可解释的目标函数，称为加性边距Softmax（AM-Softmax），用于深度面部验证。这项工作受到两项早期工作的强烈启发，分别是大型边距Softmax
    [Largesoftmax](#bib.bib102) 和 [Sphereface](#bib.bib108) 中的角度Softmax。
- en: CosFace [CosFace](#bib.bib110) and ArcFace [Arcface](#bib.bib111) are two other
    promising face recognition works developed in 2018. In [CosFace](#bib.bib110)
    , Wang et al. proposed a novel loss function, namely large margin cosine loss
    (LM-CL). More specifically, they reformulate the softmax loss as a cosine loss
    by L2 normalizing both features and weight vectors to remove radial variations,
    based on which a cosine margin term is introduced to further maximize the decision
    margin in the angular space. As a result, minimum intra-class variance and maximum
    inter-class variance are achieved by virtue of normalization and cosine decision
    margin maximization.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: CosFace [CosFace](#bib.bib110) 和 ArcFace [Arcface](#bib.bib111) 是2018年开发的另外两种有前景的面部识别方法。在
    [CosFace](#bib.bib110) 中，Wang 等人提出了一种新颖的损失函数，即大边距余弦损失（LM-CL）。更具体地，他们通过对特征和权重向量进行L2标准化，将softmax损失重新公式化为余弦损失，以消除径向变化，并引入了一个余弦边距项来进一步最大化角空间中的决策边距。因此，通过标准化和余弦决策边距最大化，实现了最小类内方差和最大类间方差。
- en: 'Ring-Loss [RingLoss](#bib.bib112) is another work focused on designing a new
    loss function, which applies soft normalization, where it gradually learns to
    constrain the norm to the scaled unit circle while preserving convexity leading
    to more robust features. The comparison of learned features by regular softmax
    and the Ring-loss function is shown in Figure [10](#S3.F10 "Figure 10 ‣ 3.1.2
    Deep Learning Works on Face Recognition ‣ 3.1 Face Recognition ‣ 3 Deep Learning
    Based Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning:
    A Survey").'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Ring-Loss [RingLoss](#bib.bib112) 是另一个专注于设计新损失函数的工作，该函数应用了软归一化，它逐渐学习将范数约束到缩放的单位圆，同时保持凸性，从而得到更强健的特征。通过常规软最大值和
    Ring-loss 函数学习到的特征的比较见图 [10](#S3.F10 "图 10 ‣ 3.1.2 深度学习在人脸识别中的应用 ‣ 3.1 人脸识别 ‣
    基于深度学习的生物特征识别 ‣ 使用深度学习进行生物特征识别：调查")。
- en: '![Refer to caption](img/3203cb722b1e7df3cf4fa17e0d23905e.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3203cb722b1e7df3cf4fa17e0d23905e.png)'
- en: 'Figure 10: Sample MNIST features trained using (a) Softmax and (b) Ring loss
    on top of Softmax. Courtesy of [RingLoss](#bib.bib112) .'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：使用 (a) Softmax 和 (b) Ring loss 训练的 MNIST 特征样本。感谢 [RingLoss](#bib.bib112)
    提供。
- en: AdaCos [adacos](#bib.bib113) , P2SGrad [p2sgrad](#bib.bib114) , UniformFace
    [uniface](#bib.bib115) , and AdaptiveFace [adapface](#bib.bib116) are among the
    most promising works proposed in 2019. In AdaCos [adacos](#bib.bib113) , Zhang
    et al. proposed a novel cosine-based softmax loss, AdaCos, which is hyperparameter-free
    and leverages an adaptive scale parameter to automatically strengthen the training
    supervisions during the training process. In [p2sgrad](#bib.bib114) , Zhang et
    al. claimed that cosine based losses always include sensitive hyper-parameters
    which can make training process unstable, and it is very tricky to set suitable
    hyperparameters for a specific dataset. They addressed this challenge by directly
    designing the gradients for training in an adaptive manner. P2SGrad was able to
    achieves state-of-the-art performance on all three face recognition benchmarks,
    LFW, MegaFace, and IJB-C. There are several other works proposed for face recognition.
    For more detailed overview of deep learning-based face recognition, we refer the
    readers to [deep_face_survey](#bib.bib53) .
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: AdaCos [adacos](#bib.bib113) 、P2SGrad [p2sgrad](#bib.bib114) 、UniformFace [uniface](#bib.bib115)
    和 AdaptiveFace [adapface](#bib.bib116) 是 2019 年提出的最有前景的工作之一。在 AdaCos [adacos](#bib.bib113)
    中，Zhang 等人提出了一种新型的基于余弦的 softmax 损失函数 AdaCos，该函数无需超参数，利用自适应缩放参数在训练过程中自动增强训练监督。在
    [p2sgrad](#bib.bib114) 中，Zhang 等人声称基于余弦的损失总是包含敏感的超参数，这可能导致训练过程不稳定，而且为特定数据集设置合适的超参数非常棘手。他们通过以自适应方式直接设计训练梯度来解决这一挑战。P2SGrad
    能够在所有三个人脸识别基准测试中实现最先进的性能，即 LFW、MegaFace 和 IJB-C。还有其他一些工作也提出了用于人脸识别的方法。有关基于深度学习的人脸识别的更详细概述，请参阅
    [deep_face_survey](#bib.bib53)。
- en: 'There have also been several works on using generative models for face image
    generation. To show the results of one promising model, in Progressive-GAN [prog_gan](#bib.bib117)
    , Karras et al developed a framework to grow both the generator and discriminator
    of GAN progressively, which can learn to generate high-resolution realistic images.
    Figure [11](#S3.F11 "Figure 11 ‣ 3.1.2 Deep Learning Works on Face Recognition
    ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric Recognition
    ‣ Biometrics Recognition Using Deep Learning: A Survey") shows 8 sample face images
    generated by this Progressive-GAN model trained on CELEB-A dataset.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用生成模型进行人脸图像生成的研究也有一些成果。为了展示一个有前景的模型的结果，在 Progressive-GAN [prog_gan](#bib.bib117)
    中，Karras 等人开发了一个框架，逐步增加 GAN 的生成器和判别器的规模，这样可以学习生成高分辨率的真实图像。图 [11](#S3.F11 "图 11
    ‣ 3.1.2 深度学习在人脸识别中的应用 ‣ 3.1 人脸识别 ‣ 基于深度学习的生物特征识别 ‣ 使用深度学习进行生物特征识别：调查") 展示了通过在
    CELEB-A 数据集上训练的 Progressive-GAN 模型生成的 8 张样本人脸图像。
- en: '![Refer to caption](img/2ccfe3a0b1a88e3bb2cbcda907ec49da.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ccfe3a0b1a88e3bb2cbcda907ec49da.png)'
- en: 'Figure 11: 8 sample images (of 1024 x 1024) generated by progressive GAN, using
    the CELEBA-HQ dataset. Courtesy of [prog_gan](#bib.bib117) .'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：使用 CELEBA-HQ 数据集生成的 8 张样本图像（1024 x 1024）。感谢 [prog_gan](#bib.bib117) 提供。
- en: .
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'Figure [12](#S3.F12 "Figure 12 ‣ 3.1.2 Deep Learning Works on Face Recognition
    ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric Recognition
    ‣ Biometrics Recognition Using Deep Learning: A Survey") illustrates the timeline
    of popular face recognition models since 2012. The listed models after 2014 are
    all deep learning based models. DeepFace and DeepID mark the beginning of deep
    learning based face recognition. As we can see many of the models after 2017 have
    focused on developing new loss functions for more discriminative feature learning.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12](#S3.F12 "图 12 ‣ 3.1.2 深度学习在人脸识别中的应用 ‣ 3.1 人脸识别 ‣ 3 深度学习基础的生物特征识别 ‣ 使用深度学习的生物特征识别：综述")
    展示了自 2012 年以来流行的人脸识别模型的时间线。2014 年之后列出的模型都是基于深度学习的模型。DeepFace 和 DeepID 标志着基于深度学习的人脸识别的开始。我们可以看到，2017
    年之后的许多模型都集中在开发新的损失函数以获得更具区分性的特征学习。
- en: '![Refer to caption](img/1bf96d31ba95ba0e3b0406022b171be2.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/1bf96d31ba95ba0e3b0406022b171be2.png)'
- en: 'Figure 12: A timeline of face recognition methods.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：人脸识别方法的时间线。
- en: 3.2 Fingerprint Recognition
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 指纹识别
- en: Fingerprint is arguably the most commonly used physiological biometric feature.
    It consists of ridges and valleys, which form unique patterns. Minutiae are major
    local portions of the fingerprint which can be used to determine the uniqueness
    of the fingerprint [jain1997line](#bib.bib25) . Important features exist in a
    fingerprint include ridge endings, bifurcations, islands, bridges, crossovers,
    and dots. [hrechak1990automated](#bib.bib118) .
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹可以说是最常用的生理生物特征。它由脊和谷组成，形成独特的模式。细节点是指纹的主要局部部分，可以用来确定指纹的唯一性 [jain1997line](#bib.bib25)。指纹中的重要特征包括脊端、分叉、岛屿、桥梁、交叉点和点
    [hrechak1990automated](#bib.bib118)。
- en: A fingerprint needs to be captured by a special device in its close proximity.
    This makes making a dataset of fingerprints more time-consuming than some other
    biometrics, such as faces and ears. Nevertheless, there are quite a few remarkable
    fingerprint datasets that are being used around the world. Fingerprint recognition
    has always been a very active area with wide applications in industry, such as
    smartphone authentication, border security, and forensic science. As one of the
    classical works, Lee et al [lee1999fingerprint](#bib.bib119) used Gabor filtering
    on partitioned fingerprint images to extract features, followed by a k-NN classifier
    for the recognition, achieving 97.2% recognition rate. In addition, using the
    magnitude of the filter output with eight orientations added a degree of shift-invariance
    to the recognition scheme. Tico et al [tico2001wavelet](#bib.bib120) extracted
    wavelet features from the fingerprint to use in a k-NN classifier.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹需要通过特殊设备在其近距离内捕捉。这使得制作指纹数据集比某些其他生物特征（如面部和耳朵）更为耗时。尽管如此，世界各地仍有许多出色的指纹数据集在使用。指纹识别一直是一个非常活跃的领域，广泛应用于工业，如智能手机认证、边境安全和法医学。作为经典的工作之一，Lee
    等人 [lee1999fingerprint](#bib.bib119) 使用 Gabor 滤波器对分割的指纹图像进行特征提取，然后使用 k-NN 分类器进行识别，达到了
    97.2% 的识别率。此外，使用具有八种方向的滤波器输出的幅度为识别方案增加了一个平移不变性。Tico 等人 [tico2001wavelet](#bib.bib120)
    从指纹中提取了小波特征，并在 k-NN 分类器中使用。
- en: 3.2.1 Fingerprint Datasets
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 指纹数据集
- en: 'There are several datasets developed for fingerprint recognition. Some of the
    most popular ones include:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 已开发了几个用于指纹识别的数据集。一些最受欢迎的数据集包括：
- en: 'FVC Fingerprint Database: Fingerprint Verification Competition (FVC) is widely
    used for fingerprint evaluation [fvc_finger](#bib.bib121) . FVC 2002 consists
    of three fingerprint datasets (DB1, DB2, and DB3) collected using different sensors.
    Each of these datasets consists of two sets: (i) Set A with 100 subjects and 8
    impressions per subject, (ii) Set B with 10 subjects and 8 impressions per subject.
    FVC 2004 adds another dataset (DB4) and contains more deliberate noise, e.g. skin
    distortions, skin moisture, and rotation.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: FVC 指纹数据库：指纹验证竞赛 (FVC) 广泛用于指纹评估 [fvc_finger](#bib.bib121)。FVC 2002 包含三个指纹数据集
    (DB1、DB2 和 DB3)，这些数据集使用不同的传感器收集。每个数据集由两个子集组成：(i) A 集合有 100 个受试者，每个受试者 8 次印象，(ii)
    B 集合有 10 个受试者，每个受试者 8 次印象。FVC 2004 增加了另一个数据集 (DB4)，并包含更多故意噪声，例如皮肤变形、皮肤湿度和旋转。
- en: 'PolyU High-resolution Fingerprint Database: This dataset contains two high
    resolution fingerprint image databases (denoted as DBI and DBII), provided by
    the Hong Kong Polytechnic University [polyU_finger](#bib.bib33) . It contains
    1480 images of 148 fingers.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: PolyU 高分辨率指纹数据库：该数据集包含两个高分辨率指纹图像数据库（标记为 DBI 和 DBII），由香港理工大学提供 [polyU_finger](#bib.bib33)
    。它包含 1480 张来自 148 个手指的图像。
- en: 'CASIA Fingerprint Dataset: CASIA Fingerprint Image Database V5 contains 20,000
    fingerprint images of 500 subjects [casia_finger](#bib.bib122) . Each volunteer
    contributed 40 fingerprint images of his eight fingers (left and right thumb,
    second, third, fourth finger), i.e., 5 images per finger. The volunteers were
    asked to rotate their fingers with various levels of pressure to generate significant
    intra-class variations.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: CASIA 指纹数据集：CASIA 指纹图像数据库 V5 包含了 20,000 张来自 500 名受试者的指纹图像 [casia_finger](#bib.bib122)
    。每位志愿者提供了 40 张他/她八个手指的指纹图像（左右手拇指、食指、中指、无名指），即每个手指 5 张图像。志愿者被要求用不同的压力旋转手指，以生成显著的同类内部变化。
- en: 'NIST Fingerprint Dataset: NIST SD27 consists of 258 latent fingerprints and
    corresponding reference fingerprints [nist_finger](#bib.bib123) .'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: NIST 指纹数据集：NIST SD27 包含 258 张潜指纹及其对应的参考指纹 [nist_finger](#bib.bib123) 。
- en: 3.2.2 Deep Learning Works on Fingerprint Recognition
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 深度学习在指纹识别中的应用
- en: There have been numerous works on using deep learning for fingerprint recognition.
    Here we provide a summary of some of the prominent works in this area.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用深度学习进行指纹识别的研究已经有很多。在这里，我们总结了一些该领域的突出工作。
- en: 'In [finger_1](#bib.bib124) , Darlow et al. proposed a fingerprint minutiae
    extraction algorithm based on deep learning models, called MENet, and achieve
    promising results on fingerprint images from FVC datasets. In [finger_2](#bib.bib125)
    , Tang and colleagues proposed another deep learning-based model for fingerprint
    minutiae extraction, called FingerNet. This model jointly performs feature extraction,
    orientation estimation, segmentation, and uses them to estimate the minutiae maps.
    The block-diagram of this model is shown in Figure [13](#S3.F13 "Figure 13 ‣ 3.2.2
    Deep Learning Works on Fingerprint Recognition ‣ 3.2 Fingerprint Recognition ‣
    3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey").'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [finger_1](#bib.bib124) 中，Darlow 等人提出了一种基于深度学习模型的指纹特征点提取算法，称为 MENet，并在 FVC
    数据集中的指纹图像上取得了良好的结果。在 [finger_2](#bib.bib125) 中，Tang 及其同事提出了另一种基于深度学习的指纹特征点提取模型，称为
    FingerNet。该模型联合执行特征提取、方向估计、分割，并利用这些信息来估计特征点图。该模型的框图如图 [13](#S3.F13 "Figure 13
    ‣ 3.2.2 Deep Learning Works on Fingerprint Recognition ‣ 3.2 Fingerprint Recognition
    ‣ 3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey") 所示。'
- en: '![Refer to caption](img/d2fa993c6465c25cf0cd6d815f5a6f2b.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d2fa993c6465c25cf0cd6d815f5a6f2b.png)'
- en: 'Figure 13: The block-diagram of the proposed FingerNet model for minutiae extraction.
    Courtesy of [finger_2](#bib.bib125) .'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：用于特征点提取的提出的 FingerNet 模型的框图。感谢 [finger_2](#bib.bib125) 提供。
- en: .
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: In another work [finger_3](#bib.bib126) , Lin and Kumar proposed a multi-view
    deep representation (based on CNNs) for contact-less and partial 3D fingerprint
    recognition. The proposed model includes one fully convolutional network for fingerprint
    segmentation and three Siamese networks to learn multi-view 3D fingerprint feature
    representation. They show promising results on several 3D fingerprint databases.
    In [finger_4](#bib.bib127) , the authors develop a fingerprint texture learning
    using a deep learning framework. They evaluate their models on several benchmarks,
    and achieve verification accuracies of 100, 98.65, 100 and 98% on the four databases
    of PolyU2D, IITD, CASIA-BLU and CASIA-WHT, respectively. In [finger_5](#bib.bib128)
    , Minaee et al. proposed a deep transfer learning approach to perform fingerprint
    recognition with a very high accuracy. They fine-tuned a pre-trained ResNet model
    on a popular fingerprint dataset, and are able to achieve very high recognition
    rate.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个工作 [finger_3](#bib.bib126) 中，Lin 和 Kumar 提出了一个基于 CNN 的多视角深度表示，用于无接触和部分 3D
    指纹识别。所提出的模型包括一个全卷积网络用于指纹分割和三个 Siamese 网络用于学习多视角 3D 指纹特征表示。他们在多个 3D 指纹数据库上展示了有希望的结果。在
    [finger_4](#bib.bib127) 中，作者使用深度学习框架开发了一种指纹纹理学习方法。他们在多个基准上评估了他们的模型，并在 PolyU2D、IITD、CASIA-BLU
    和 CASIA-WHT 四个数据库上分别达到了 100%、98.65%、100% 和 98%的验证准确率。在 [finger_5](#bib.bib128)
    中，Minaee 等人提出了一种深度迁移学习方法，以非常高的准确率进行指纹识别。他们在一个流行的指纹数据集上微调了一个预训练的 ResNet 模型，并能够达到非常高的识别率。
- en: In [finger_8](#bib.bib129) , Lin and Kumar proposed a multi-Siamese network
    to accurately match contactless to contact-based fingerprint images. In addition
    to the fingerprint images, hand-crafted fingerprint features, e.g., minutiae and
    core point, are also incorporated into the proposed architecture. This multi-Siamese
    CNN is trained using the fingerprint images and extracted features.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [finger_8](#bib.bib129) 中，Lin 和 Kumar 提出了一个多Siamese网络，以准确匹配非接触式与接触式指纹图像。除了指纹图像，手工制作的指纹特征，如*细节*和*核心点*，也被纳入到所提议的架构中。这个多Siamese
    CNN 使用指纹图像和提取的特征进行训练。
- en: There are also some works using deep learning models for fingerprint segmentation.
    In [finger_6](#bib.bib130) , Stojanovic and colleagues proposed a fingerprint
    ROI segmentation algorithm based on convolutional neural networks. In another
    work [finger_7](#bib.bib131) , Zhu et al. proposed a new latent fingerprint segmentation
    method based on convolutional neural networks (”ConvNets”). The latent fingerprint
    segmentation problem is formulated as a classification system, in which a set
    of ConvNets are trained to classify each patch as either fingerprint or background.
    Then, a score map is calculated based on the classification results to evaluate
    the possibility of a pixel belonging to the fingerprint foreground. Finally, a
    segmentation mask is generated by thresholding the score map and used to delineate
    the latent fingerprint boundary.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些工作利用深度学习模型进行指纹分割。在 [finger_6](#bib.bib130) 中，Stojanovic 和同事提出了一种基于卷积神经网络的指纹ROI分割算法。在另一项工作
    [finger_7](#bib.bib131) 中，Zhu 等人提出了一种基于卷积神经网络（“ConvNets”）的新型潜在指纹分割方法。潜在指纹分割问题被表述为分类系统，其中一组ConvNets被训练来将每个补丁分类为指纹或背景。然后，基于分类结果计算评分图，以评估像素属于指纹前景的可能性。最后，通过对评分图进行阈值处理生成分割掩码，用于描绘潜在指纹的边界。
- en: There have also been some works for fake fingerprint detection. In [finger_9](#bib.bib132)
    , Kim et al. proposed a fingerprint liveliness detection based on statistical
    features learned from deep belief network (DBN). This method achieves good accuracy
    on various sensor datasets of the LivDet2013 test. In [finger_10](#bib.bib133)
    , Nogueira and colleagues proposed a model to detect fingerprint liveliness (where
    they are real or fake) using a convolutional neural network, which achieved an
    accuracy of 95.5% on fingerprint liveness detection competition 2015.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些工作用于假指纹检测。在 [finger_9](#bib.bib132) 中，Kim 等人提出了一种基于深度信念网络（DBN）学习的统计特征的指纹活跃度检测方法。该方法在LivDet2013测试的各种传感器数据集上实现了良好的准确性。在
    [finger_10](#bib.bib133) 中，Nogueira 和同事提出了一种使用卷积神经网络检测指纹活跃度（即它们是真实的还是伪造的）的方法，该方法在2015年指纹活跃度检测比赛中实现了95.5%的准确率。
- en: There have also been some works on using generative models for fingerprint image
    generation. In [finger_11](#bib.bib134) , Minaee et al proposed an algorithm for
    fingerprint image generation based on an extension of GAN, called ”Connectivity
    Imposed GAN”. This model adds total variation of the generated image to the GAN
    loss function, to promote the connectivity of generated fingerprint images. In
    [anil_finger](#bib.bib135) , Tabassi et al. developed a framework to synthesize
    altered fingerprints whose characteristics are similar to true altered fingerprints,
    and used them to train a classifier to detect ”Fingerprint alteration/obfuscation
    presentation attack” (i.e. intentional tamper or damage to the real friction ridge
    patterns to avoid identification).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些工作使用生成模型进行指纹图像生成。在 [finger_11](#bib.bib134) 中，Minaee 等人提出了一种基于GAN扩展的指纹图像生成算法，称为“Connectivity
    Imposed GAN”。该模型将生成图像的总变差添加到GAN损失函数中，以促进生成指纹图像的连通性。在 [anil_finger](#bib.bib135)
    中，Tabassi 等人开发了一个框架来合成与真实变更指纹特征相似的伪造指纹，并用它们训练分类器以检测“指纹更改/伪装呈现攻击”（即有意篡改或损坏真实摩擦脊模式以避免识别）。
- en: 3.3 Iris Recognition
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 虹膜识别
- en: Iris images contain a rich set of features embedded in their texture and patterns
    which do not change over time, such as rings, corona, ciliary processes, freckles,
    and the striated trabecular meshwork of chromatophore and fibroblast cells, which
    is the most prevailing under visible light [daugman1993high](#bib.bib136) . Iris
    recognition has gained a lot of attention in recent years in different security-related
    fields.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虹膜图像包含丰富的特征，这些特征嵌入在其纹理和模式中，不随时间变化，如环形、冠状物、睫毛过程、雀斑以及色素细胞和纤维母细胞的条纹状小梁网，这是在可见光下最为明显的
    [daugman1993high](#bib.bib136)。近年来，虹膜识别在不同的安全相关领域引起了很多关注。
- en: 'John Daugman developed one of the first modern iris recognition frameworks
    using 2D Gabor wavelet transform [kumar2010comparison](#bib.bib35) . Iris recognition
    started to rise in popularity in the 1990s. In 1994, Wildes et al [wildes1994system](#bib.bib137)
    introduced a device using iris recognition for personnel authentication. After
    that, many researchers started looking at iris recognition problem. Early works
    have used a variety of methods to extract hand-crafted features from the iris.
    Williams et al [williams1996iris](#bib.bib138) converted all iris entries to an
    “IrisCode” and used Hamming’s distance of an input iris image’s IrisCode from
    those of the irises in the database as a metric for recognition. In [iris_0](#bib.bib139)
    , the authors proposed an iris recognition system based on ”deep scattering convolutional
    features”, which achieved a significantly high accuracy rate on IIT Delhi dataset.
    This work is not exactly using deep learning, but is using a deep scattering convolutional
    network, to extract hierarchical features from the image. The output images at
    different nodes of scattering network denote the transformed image along different
    orientation and scales. The transformed images of the first and second layers
    of scattering transform for a sample iris image are shown in Figures [14](#S3.F14
    "Figure 14 ‣ 3.3 Iris Recognition ‣ 3 Deep Learning Based Works on Biometric Recognition
    ‣ Biometrics Recognition Using Deep Learning: A Survey"). These images are derived
    by applying bank of filters of 5 different scales and 6 orientations.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: John Daugman 开发了第一个现代虹膜识别框架之一，使用了 2D Gabor 小波变换 [kumar2010comparison](#bib.bib35)。虹膜识别在1990年代开始受到关注。1994年，Wildes
    等人 [wildes1994system](#bib.bib137) 引入了一种用于人员认证的虹膜识别设备。之后，许多研究人员开始关注虹膜识别问题。早期的工作使用了各种方法从虹膜中提取手工特征。Williams
    等人 [williams1996iris](#bib.bib138) 将所有虹膜条目转换为“IrisCode”，并使用输入虹膜图像的 IrisCode 与数据库中虹膜的
    IrisCode 的汉明距离作为识别的度量。在 [iris_0](#bib.bib139) 中，作者提出了一种基于“深度散射卷积特征”的虹膜识别系统，在 IIT
    Delhi 数据集上实现了显著的高准确率。这项工作并非完全使用深度学习，而是使用深度散射卷积网络从图像中提取层次特征。散射网络中不同节点的输出图像表示沿不同方向和尺度的变换图像。散射变换的第一层和第二层的变换图像如图[14](#S3.F14
    "图 14 ‣ 3.3 虹膜识别 ‣ 3 基于深度学习的生物识别工作 ‣ 使用深度学习的生物识别：综述")所示。这些图像是通过应用5种不同尺度和6种方向的滤波器组得到的。
- en: '![Refer to caption](img/229e2b3ee2f78aa8687446cfebc345af.png)![Refer to caption](img/ddaf8ba82d1fcd7ddf3467cb6149c6b3.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/229e2b3ee2f78aa8687446cfebc345af.png)![参见说明](img/ddaf8ba82d1fcd7ddf3467cb6149c6b3.png)'
- en: 'Figure 14: The images from the first (on the left) and second (on the right)
    layers of the scattering transform.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：散射变换的第一层（左侧）和第二层（右侧）的图像。
- en: 'It is worth mentioning that many of the classical iris recognition models perform
    several pre-processing steps such as iris detection, normalization, and enhancement,
    as shown in Figure [15](#S3.F15 "Figure 15 ‣ 3.3 Iris Recognition ‣ 3 Deep Learning
    Based Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning:
    A Survey"). They then extract features from the normalized or enhanced image.
    Many of the modern works on iris recognition skip normalization and enhancement,
    and yet, they are still able to achieve very high recognition accuracy. One reason
    is the ability of deep models to capture high-level semantic a features from original
    iris images, which are discriminative enough to perform well for iris recognition.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，许多经典的虹膜识别模型执行多个预处理步骤，如虹膜检测、标准化和增强，如图[15](#S3.F15 "图 15 ‣ 3.3 虹膜识别 ‣ 3
    基于深度学习的生物识别工作 ‣ 使用深度学习的生物识别：综述")所示。然后，它们从标准化或增强后的图像中提取特征。许多现代虹膜识别工作跳过了标准化和增强，但仍能实现非常高的识别准确率。其中一个原因是深度模型能够从原始虹膜图像中捕捉高层次的语义特征，这些特征具有足够的区分度，以在虹膜识别中表现良好。
- en: '![Refer to caption](img/84cef33b9ed0dffea02bbe5435836013.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/84cef33b9ed0dffea02bbe5435836013.png)'
- en: 'Figure 15: Illustration of some of the key pre-processing steps for iris recognition,
    courtesy of [iris_12](#bib.bib140) .'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：虹膜识别的一些关键预处理步骤的示意图，感谢 [iris_12](#bib.bib140)。
- en: 3.3.1 Iris Datasets
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 虹膜数据集
- en: 'Various datasets have been proposed for iris recognition in the past. Some
    of the most popular ones include:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 过去提出了各种虹膜识别数据集。其中一些最受欢迎的数据集包括：
- en: 'CASIA-Iris-1000 Database: CASIA-Iris-1000 contains 20,000 iris images from
    1,000 subjects, which were collected using an IKEMB-100 camera. The main sources
    of intra-class variations in CASIA-Iris-1000 are eyeglasses and specular reflections
    [casia_iris](#bib.bib142) .'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CASIA-Iris-1000 数据库：CASIA-Iris-1000 包含来自1,000名受试者的20,000张虹膜图像，这些图像是使用IKEMB-100相机采集的。CASIA-Iris-1000
    的主要类内变异来源是眼镜和镜面反射 [casia_iris](#bib.bib142)。
- en: 'UBIRIS Dataset: The UBIRIS database has two distinct versions, UBIRIS.v1 and
    UBIRIS.v2. The first version of this database is composed of 1877 images collected
    from 241 eyes in two distinct sessions. It simulates less constrained imaging
    conditions [ubiris_iris](#bib.bib143) . The second version of the UBIRIS database
    has over 11000 images (and continuously growing) and more realistic noise factors.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: UBIRIS 数据集：UBIRIS 数据库有两个不同的版本，UBIRIS.v1 和 UBIRIS.v2。第一个版本包含来自241只眼睛的1877张图像，分两次采集。它模拟了较少受限的成像条件
    [ubiris_iris](#bib.bib143)。第二个版本的 UBIRIS 数据库有超过11,000张图像（并且持续增长），噪声因素更为真实。
- en: 'IIT Delhi Iris Dataset: IIT Delhi iris database contains 2240 iris images captured
    from 224 different people. The resolution of these images is 320x240 pixels [iit_iris](#bib.bib144)
    . Iris images in this dataset have variable color distribution, and different
    (iris) sizes.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: IIT Delhi Iris 数据集：IIT Delhi iris 数据库包含来自224个不同人的2240张虹膜图像。这些图像的分辨率为320x240像素
    [iit_iris](#bib.bib144)。该数据集中的虹膜图像具有不同的颜色分布和不同的（虹膜）尺寸。
- en: 'ND Datasets: ND-CrossSensor-Iris-2013 consists of two iris databases, taken
    with two iris sensors: LG2200 and LG4000\. The LG2200 dataset consists of 116,564
    iris images, and LG4000 consists of 29,986 iris images of 676 subjects [lg_iris](#bib.bib145)
    .'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ND 数据集：ND-CrossSensor-Iris-2013 包含两个虹膜数据库，分别使用两个虹膜传感器：LG2200 和 LG4000。LG2200
    数据集包含116,564张虹膜图像，而 LG4000 包含29,986张虹膜图像，涉及676名受试者 [lg_iris](#bib.bib145)。
- en: 'MICHE Dataset: Mobile Iris Challenge Evaluation (MICHE) consists of iris images
    acquired under unconstrained conditions using smartphones. It consists of more
    than 3,732 images acquired from 92 subjects using three different smartphones
    [miche_iris](#bib.bib146) .'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: MICHE 数据集：Mobile Iris Challenge Evaluation (MICHE) 包含在无约束条件下使用智能手机采集的虹膜图像。该数据集包括来自92名受试者的3,732张图像，使用了三种不同的智能手机
    [miche_iris](#bib.bib146)。
- en: 3.3.2 Deep Learning Works on Iris Recognition
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 深度学习在虹膜识别中的应用
- en: Compared to face recognition, deep learning models made their ways to iris recognition
    with a few years delay.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与人脸识别相比，深度学习模型在虹膜识别中的应用有所滞后。
- en: As one of the first works using deep learning for iris recognition, in [iris_1](#bib.bib147)
    Minaee et al. showed that features extracted from a pre-trained CNN model trained
    on ImageNet are able to achieve a reasonably high accuracy rate for iris recognition.
    In this work, they used features derived from different layers of VGGNet [vggnet](#bib.bib66)
    , and trained a multi-class SVM on top of it, and showed that the trained model
    can achieve state-of-the-art accuracy on two iris recognition benchmarks, CASIA-1000
    and IIT Delhi databases. They also showed that features extracted from the mid-layers
    of VGGNet achieve slightly higher accuracy from the the very last layers. In another
    work [iris_1_2](#bib.bib148) , Gangwar and Joshi proposed an iris recognition
    network based on convolutional neural network, which provides robust, discriminative,
    compact resulting in very high accuracy rate, and can work pretty well in cross-sensor
    recognition of iris images.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用深度学习进行虹膜识别的早期工作之一，在 [iris_1](#bib.bib147) 中，Minaee 等人展示了从在 ImageNet 上训练的预训练
    CNN 模型中提取的特征能够实现相当高的虹膜识别准确率。在这项工作中，他们使用了来自 VGGNet [vggnet](#bib.bib66) 不同层的特征，并在其上训练了一个多类
    SVM，结果表明，训练后的模型在两个虹膜识别基准（CASIA-1000 和 IIT Delhi 数据库）上能够达到最先进的准确率。他们还展示了从 VGGNet
    中层提取的特征相比于最后几层具有稍高的准确率。在另一项工作 [iris_1_2](#bib.bib148) 中，Gangwar 和 Joshi 提出了一个基于卷积神经网络的虹膜识别网络，该网络提供了强大的、具有区分性的、紧凑的结果，实现了非常高的准确率，并且在虹膜图像的跨传感器识别中表现良好。
- en: In [iris_1_3](#bib.bib149) , Baqar and colleagues proposed an iris recognition
    framework based on deep belief networks, as well as contour information of iris
    images. Contour based feature vector has been used to discriminate samples belonging
    to different classes i.e., difference of sclera-iris and iris-pupil contours,
    and is named as “Unique Signature”. Once the features extracted, deep belief network
    (DBN) with modified back-propagation algorithm based feed-forward neural network
    (RVLR-NN) has been used for classification.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在[iris_1_3](#bib.bib149)中，Baqar及其同事提出了一种基于深度置信网络和虹膜图像轮廓信息的虹膜识别框架。基于轮廓的特征向量已被用于区分属于不同类别的样本，即巩膜-虹膜轮廓和虹膜-瞳孔轮廓的差异，并被称为“独特签名”。特征提取后，使用具有修改反向传播算法的前馈神经网络（RVLR-NN）进行分类。
- en: 'In [iris_12](#bib.bib140) , Zhao and Kumar proposed an iris recognition model
    based on ”Deeply Learned Spatially Corresponding Features”. The proposed framework
    is based on a fully convolutional network (FCN), which outputs spatially corresponding
    iris feature descriptors. They also introduce a specially designed ”Extended Triplet
    Loss (ETL)” function to incorporate the bit-shifting and non-iris masking. The
    triplet network is illustrated in Figure [16](#S3.F16 "Figure 16 ‣ 3.3.2 Deep
    Learning Works on Iris Recognition ‣ 3.3 Iris Recognition ‣ 3 Deep Learning Based
    Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning: A
    Survey"). They also developed a sub-network to provide appropriate information
    for identifying meaningful iris regions, which serves as essential input for the
    newly developed ETL. They were able to outperform several classic and state-of-the-art
    iris recognition approaches on a few iris databases.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '在[iris_12](#bib.bib140)中，Zhao和Kumar提出了一种基于“深度学习空间对应特征”的虹膜识别模型。该框架基于全卷积网络（FCN），输出空间对应的虹膜特征描述符。他们还引入了一种特别设计的“扩展三元组损失（ETL）”函数，以结合位移和非虹膜掩膜。三元组网络如图[16](#S3.F16
    "Figure 16 ‣ 3.3.2 Deep Learning Works on Iris Recognition ‣ 3.3 Iris Recognition
    ‣ 3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey")所示。他们还开发了一个子网络，以提供识别有意义虹膜区域的适当信息，这为新开发的ETL提供了必要的输入。他们在一些虹膜数据库上超越了几种经典和最先进的虹膜识别方法。'
- en: '![Refer to caption](img/557c4fbb7894aa90bcb8ffcf04c0cb66.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/557c4fbb7894aa90bcb8ffcf04c0cb66.png)'
- en: 'Figure 16: The block-diagram of triplet network used for iris recognition,
    courtesy of [iris_12](#bib.bib140) .'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：用于虹膜识别的三元组网络的框图，感谢[iris_12](#bib.bib140)提供。
- en: In another work [iris_2](#bib.bib150) , Alaslani et al. developed an iris recognition
    system, based on deep features extracted from AlexNet, followed by a multi-class
    classification, and achieved high accuracy rates on CASIA-Iris-V1, CASIA-Iris-1000
    and, CASIA-Iris-V3 Interval databases. In [menon2018iris](#bib.bib151) , Menon
    shows the applications of convolutional features from a fine-tuned pre-trained
    model for both identification and verification problems. In [iris_4](#bib.bib152)
    , Hofbauer and colleagues proposed a CNN based algorithm for segmentation of iris
    images, which can results in higher accuracies than previous models. In another
    work [iris_5](#bib.bib153) , Ahmad and Fuller developed an iris recognition model
    based on triplet network, call ThirdEye. Their work directly uses the segmented,
    un-normalized iris images, and is shown to achieve equal error rates of 1.32%,
    9.20%, and 0.59% on the ND-0405, UbirisV2, and IITD datasets respectively. In
    a more recent work [iris_8](#bib.bib154) , Minaee and colleagues proposed an algorithm
    for iris recognition based using a deep transfer learning approach. They trained
    a CNN model (by fine-tuning a pre-trained ResNet model) on an iris dataset, and
    achieved very accurate recognition on the test set.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项工作[iris_2](#bib.bib150)中，Alaslani等人开发了一种虹膜识别系统，该系统基于从AlexNet提取的深度特征，随后进行多类分类，并在CASIA-Iris-V1、CASIA-Iris-1000和CASIA-Iris-V3
    Interval数据库上达到了高准确率。在[menon2018iris](#bib.bib151)中，Menon展示了从经过微调的预训练模型中提取的卷积特征在识别和验证问题上的应用。在[iris_4](#bib.bib152)中，Hofbauer及其同事提出了一种基于CNN的算法，用于虹膜图像的分割，这可以获得比以前的模型更高的准确率。在另一项工作[iris_5](#bib.bib153)中，Ahmad和Fuller开发了一种基于三元组网络的虹膜识别模型，称为ThirdEye。他们的工作直接使用分割的、未归一化的虹膜图像，并在ND-0405、UbirisV2和IITD数据集上分别达到了1.32%、9.20%和0.59%的等错误率。在更近期的工作[iris_8](#bib.bib154)中，Minaee及其同事提出了一种基于深度迁移学习方法的虹膜识别算法。他们在虹膜数据集上训练了一个CNN模型（通过微调预训练的ResNet模型），并在测试集上实现了非常准确的识别。
- en: 'With the rise of deep generative models, there have been works that apply them
    to iris recognition. In [iris_6](#bib.bib155) , Minaee et al proposed an algorithm
    for iris image generation based on convolutional GAN, which can generate realistic
    iris images. These images can be used for augmenting the training set, resulting
    in better feature representation and higher accuracy. Four sample iris images
    generated by this work (over different training epochs) are shown in Figure [17](#S3.F17
    "Figure 17 ‣ 3.3.2 Deep Learning Works on Iris Recognition ‣ 3.3 Iris Recognition
    ‣ 3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度生成模型的兴起，已经有研究将其应用于虹膜识别。在 [iris_6](#bib.bib155) 中，Minaee 等人提出了一种基于卷积 GAN
    的虹膜图像生成算法，该算法可以生成逼真的虹膜图像。这些图像可以用于增强训练集，从而获得更好的特征表示和更高的准确率。图 [17](#S3.F17 "图 17
    ‣ 3.3.2 深度学习在虹膜识别中的应用 ‣ 3.3 虹膜识别 ‣ 3 基于深度学习的生物识别研究 ‣ 基于深度学习的生物识别：综述") 显示了这项工作生成的四个样本虹膜图像（经过不同的训练周期）。
- en: '![Refer to caption](img/fa8a042282b4bedf3562177a3daa9ecf.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa8a042282b4bedf3562177a3daa9ecf.png)'
- en: 'Figure 17: The generated iris images for 4 input latent vectors, over 140 epochs
    (on every 10 epochs), using the trained model on IIT Delhi Iris database. Courtesy
    of [iris_6](#bib.bib155) .'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：使用训练好的 IIT Delhi Iris 数据库模型，针对 4 个输入潜在向量生成的虹膜图像（经过 140 个周期，每 10 个周期一次）。图片来源：[iris_6](#bib.bib155)。
- en: In [iris_7](#bib.bib156) , Lee and colleagues proposed a data augmentation technique
    based on GAN to augment the training data for iris recognition, resulting in a
    higher accuracy rate. They claim that historical data augmentation techniques
    such as geometric transformations and brightness adjustment result in samples
    with very high correlation with the original ones, but using augmentation based
    on a conditional generative adversarial network can result in higher test accuracy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [iris_7](#bib.bib156) 中，Lee 和同事提出了一种基于 GAN 的数据增强技术，用于增加虹膜识别的训练数据，从而提高了准确率。他们声称，历史数据增强技术如几何变换和亮度调整会产生与原始样本高度相关的样本，但使用基于条件生成对抗网络的增强方法可以获得更高的测试准确度。
- en: 3.4 Palmprint Recognition
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 掌纹识别
- en: Palmprint is another biometric which is gaining more attention recently. In
    addition to minutiae features, palmprints also consist of geometry-based features,
    delta points, principal lines, and wrinkles [zhang1999two](#bib.bib11) ; [zhang2012comparative](#bib.bib157)
    . Each part of a palmprint has different features, including texture, ridges,
    lines and creases. An advantage of palmprints is that the creases in palmprint
    virtually do not change over time and are easy to extract [chen2001palmprint](#bib.bib158)
    . However, sampling palmprints requires special devices, making their collection
    not as easy as other biometrics such as fingerprint, iris and face. Classical
    works on palmprint recognition have explored a wide range of hand-carfted features
    such as as PCA and ICA [connie2003palmprint](#bib.bib159) , Fourier transform
    [li2002palmprint](#bib.bib160) , wavelet transform [wu2002wavelet](#bib.bib161)
    , line feature matching [shu1998palmprint](#bib.bib162) , and deep scaterring
    features [palm_2](#bib.bib163) .
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 掌纹是另一种最近越来越受到关注的生物特征。除了纹点特征外，掌纹还包括基于几何的特征、三角点、主线和皱纹 [zhang1999two](#bib.bib11)；[zhang2012comparative](#bib.bib157)。掌纹的每个部分具有不同的特征，包括纹理、脊线、线条和褶皱。掌纹的一个优点是掌纹中的褶皱几乎不会随时间变化，且容易提取
    [chen2001palmprint](#bib.bib158)。然而，采集掌纹需要特殊设备，这使得其收集不如指纹、虹膜和面部等其他生物特征那么容易。经典的掌纹识别研究探讨了多种手工制作的特征，如
    PCA 和 ICA [connie2003palmprint](#bib.bib159)、傅里叶变换 [li2002palmprint](#bib.bib160)、小波变换
    [wu2002wavelet](#bib.bib161)、线条特征匹配 [shu1998palmprint](#bib.bib162) 和深度散射特征 [palm_2](#bib.bib163)。
- en: 3.4.1 Palmprint Datasets
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 掌纹数据集
- en: 'Several datasets have been proposed for palmprint recognition dataset. Some
    of the most widely used datasets include:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了几个用于掌纹识别的数据集。其中一些最广泛使用的数据集包括：
- en: 'PolyU Multispectral Palmprint Dataset: The images from PolyU dataset were collected
    from 250 volunteers, including 195 males and 55 females. In total, the database
    contains 6,000 images from 500 different palms for one illumination [polyu_palm](#bib.bib34)
    . Samples are collected in two separate sessions. In each session, the subject
    was asked to provide 6 images for each palm. Therefore, 24 images of each illumination
    from 2 palms were collected from each subject.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: PolyU 多光谱手掌纹数据集：PolyU 数据集中的图像是从 250 名志愿者中收集的，其中包括 195 名男性和 55 名女性。该数据库总共包含来自
    500 个不同手掌的 6,000 张图像，光照条件为单一 [polyu_palm](#bib.bib34)。样本分两次收集。在每次收集中，受试者需提供每只手掌
    6 张图像。因此，每个受试者的 2 只手掌在每个光照条件下共收集了 24 张图像。
- en: 'CASIA Palmprint Database: CASIA Palmprint Image Database contains of 5,502
    palmprint images captured from 312 subjects. For each subject, they collect palmprint
    images from both left and right palms [casia_palm](#bib.bib164) . All palmprint
    images are 8-bit gray-level JPEG files by their self-developed palmprint recognition
    device.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: CASIA 手掌纹数据库：CASIA 手掌纹图像数据库包含 5,502 张从 312 个受试者处获取的手掌纹图像。对于每个受试者，收集了左右手掌的图像
    [casia_palm](#bib.bib164)。所有手掌纹图像均为由其自开发的手掌纹识别设备拍摄的 8 位灰度 JPEG 文件。
- en: 'IIT Delhi Touchless Palmprint Database: The IIT Delhi palmprint image database
    consists of the hand images collected from the students and staff at IIT Delhi,
    New Delhi, India [iit_palm](#bib.bib165) . This database has been acquired using
    a simple and touchless imaging setup. The currently available database is from
    235 users. Seven images from each subject, from each of the left and right hand,
    are acquired in varying hand pose variations. Each image has a size of 800x600
    pixels.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: IIT 德里无接触手掌纹数据库：IIT 德里手掌纹图像数据库由从印度新德里 IIT 德里学生和工作人员处收集的手部图像组成 [iit_palm](#bib.bib165)。该数据库使用简单的无接触成像设备获取。当前可用的数据库来自
    235 位用户。每个受试者的左右手各获取七张图像，手部姿势各异。每张图像的大小为 800x600 像素。
- en: 3.4.2 Deep Learning Works on Palmprint Recognition
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 深度学习在手掌纹识别中的应用
- en: In [palm_1](#bib.bib166) , Xin et al. proposed one of the early works on palmprint
    recognition using a deep learning framework. The authors built a deep belief net
    by top-to-down unsupervised training, and tuned the model parameters toward a
    robust accuracy on the validation set. Their experimental analysis showed a performance
    gain over classical models that are based on LBP, and PCA, and other other hand-crafted
    features.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [palm_1](#bib.bib166) 中，Xin 等人提出了使用深度学习框架进行手掌纹识别的早期工作之一。作者通过自上而下的无监督训练建立了深度信念网络，并调优模型参数以在验证集上实现稳健的准确性。他们的实验分析显示，相较于基于
    LBP、PCA 和其他手工特征的经典模型，性能有显著提升。
- en: In another work, Samai et al. proposed a deep learning-based model for 2D and
    3D palmprint recognition [palm_3](#bib.bib167) . They proposed an efficient biometric
    identification system combining 2D and 3D palmprint by fusing them at matching
    score level. To exploit the 3D palmprint data, they converted them to grayscale
    images by using the Mean Curvature (MC) and the Gauss Curvature (GC). They then
    extracted features from images using Discrete Cosine Transform Net (DCT Net).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项工作中，Samai 等人提出了一种基于深度学习的二维和三维手掌纹识别模型 [palm_3](#bib.bib167)。他们提出了一种高效的生物识别系统，通过在匹配分数级别融合
    2D 和 3D 手掌纹来实现。为了利用 3D 手掌纹数据，他们通过使用均值曲率 (MC) 和高斯曲率 (GC) 将其转换为灰度图像。然后，他们使用离散余弦变换网络
    (DCT Net) 从图像中提取特征。
- en: Zhong et al. proposed a palmprint recognition algorithm using Siamese network
    [palm_4](#bib.bib168) . Two VGG-16 networks (with shared parameters) were employed
    to extract features for two input palmprint images, and another network is used
    on top of them to directly obtain the similarity of two input palmprints according
    to their convolutional features. This method achieved an Equal Error Rate (EER)
    of 0.2819% on on PolyU dataset. In [palm_5](#bib.bib169) , Izadpanahkakhk et al.
    proposed a transfer learning approach towards palmprint verification, which jointly
    extracts regions of interests and features from the images. They use a pre-trained
    convolutional network, along with SVM to make prediction. They achieved an IoU
    score of 93% and EER of 0.0125 on Hong Kong Polytechnic University Palmprint (HKPU)
    database.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Zhong等人提出了一种使用Siamese网络的掌纹识别算法[palm_4](#bib.bib168)。使用了两个VGG-16网络（共享参数）来提取两个输入掌纹图像的特征，并在它们之上使用另一个网络根据其卷积特征直接获得两个输入掌纹的相似性。该方法在PolyU数据集上实现了0.2819%的等错误率（EER）。在[palm_5](#bib.bib169)中，Izadpanahkakhk等人提出了一种针对掌纹验证的迁移学习方法，该方法共同提取图像的感兴趣区域和特征。他们使用预训练的卷积网络以及SVM进行预测。在香港理工大学掌纹（HKPU）数据库上，他们达到了93%的IoU分数和0.0125的EER。
- en: 'In [palm_6](#bib.bib170) , Shao and Zhong proposed a few-shot palmprint recognition
    model using a graph neural network. In this work, the palmprint features extracted
    by a convolutional neural network are processed into nodes in the GNN. The edges
    in the GNN are used to represent similarities between image nodes. In a more recent
    work [palm_7](#bib.bib171) , Shao and colleagues proposed a deep palmprint recognition
    approach by combining hash coding and knowledge distillation. Deep hashing network
    are used to convert palmprint images to binary codes to save storage space and
    speed up the matching process. The architecture of the proposed deep hashing network
    is shown in Figure [18](#S3.F18 "Figure 18 ‣ 3.4.2 Deep Learning Works on Palmprint
    Recognition ‣ 3.4 Palmprint Recognition ‣ 3 Deep Learning Based Works on Biometric
    Recognition ‣ Biometrics Recognition Using Deep Learning: A Survey").'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在[palm_6](#bib.bib170)中，Shao和Zhong提出了一种使用图神经网络的少样本掌纹识别模型。在这项工作中，由卷积神经网络提取的掌纹特征被处理成GNN中的节点。GNN中的边用于表示图像节点之间的相似性。在更近期的工作[palm_7](#bib.bib171)中，Shao及其同事提出了一种结合哈希编码和知识蒸馏的深度掌纹识别方法。深度哈希网络用于将掌纹图像转换为二进制代码，以节省存储空间并加速匹配过程。所提议的深度哈希网络的架构如图[18](#S3.F18
    "图18 ‣ 3.4.2 深度学习在掌纹识别中的应用 ‣ 3.4 掌纹识别 ‣ 3 基于深度学习的生物识别研究 ‣ 基于深度学习的生物识别：综述")所示。
- en: '![Refer to caption](img/9d8434caa80baea2e62fcd7382bb7d08.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9d8434caa80baea2e62fcd7382bb7d08.png)'
- en: 'Figure 18: The block-diagram of the proposed deep hashing network, courtesy
    of [palm_7](#bib.bib171) .'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：所提议的深度哈希网络的框图，感谢[palm_7](#bib.bib171)。
- en: They also proposed a database for unconstrained palmprint recognition, which
    consists of more than 30,000 images collected by 5 different mobile phones, and
    achieved promising results on that dataset. In [palm_8](#bib.bib172) , Shao et
    al. proposed a cross-domain palmprint recognition based on transfer convolutional
    autoencoder. Convolutional autoencoders were firstly used to extract low-dimensional
    features. A discriminator was then introduced to reduce the gap of two domains.
    The auto-encoders and discriminator were alternately trained, and finally the
    features with the same distribution were extracted.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还提出了一个用于无约束掌纹识别的数据库，该数据库包含超过30,000张由5部不同手机采集的图像，并在该数据集上取得了令人满意的结果。在[palm_8](#bib.bib172)中，Shao等人提出了一种基于迁移卷积自编码器的跨域掌纹识别方法。首先使用卷积自编码器提取低维特征。然后引入了判别器以减少两个域之间的差距。自编码器和判别器交替训练，最终提取出具有相同分布的特征。
- en: In [palm_9](#bib.bib173) , Zhao and colleagues proposed a joint deep convolutional
    feature representation for hyperspectral palmprint recognition. A CNN stack is
    constructed to extract its features from the entire spectral bands and generate
    a joint convolutional feature. They evaluated their model on a hyperspectral palmprint
    dataset consisting of 53 spectral bands with 110,770 images. They achieved an
    EER of 0.01%. In [palm_10](#bib.bib174) , Xie et al. proposed a gender classification
    framework using convolutional neural network on plamprint images. They fine-tuned
    the pre-trained VGGNet on a palmprint dataset and showed that the proposed structure
    could achieve a good performance for gender classification.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在[palm_9](#bib.bib173)中，赵及其同事提出了一种用于高光谱掌纹识别的联合深度卷积特征表示方法。构建了一个CNN堆叠，从所有光谱带中提取特征，并生成联合卷积特征。他们在一个包含53个光谱带和110,770张图像的高光谱掌纹数据集上评估了他们的模型。他们取得了0.01%的EER。在[palm_10](#bib.bib174)中，谢等人提出了一种基于卷积神经网络的掌纹图像性别分类框架。他们对掌纹数据集中的预训练VGGNet进行了微调，并展示了所提出的结构在性别分类方面可以取得良好的性能。
- en: 3.5 Ear Recognition
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 耳朵识别
- en: 'Ear recognition is a more recent problem that scientists are exploring, and
    the volume of biometric recognition works involving ears is expected to increase
    in the coming years. One of the more prominent aspects of ear recognition is the
    fact that the subject can be photographed from either side of their head and the
    ears are almost identical (suitable when subject is not cooperating, or hiding
    his/her face). Also, since there is no need for the subject’s proximity, images
    may be taken from the ear more easily. However, ears of the subject may still
    be occluded by factors such as hair, hat, and jewelry, making it difficult to
    detect and use the ear image [cintas2019automatic](#bib.bib175) . There are multiple
    classical methods to perform ear recognition: geometric methods, which try to
    extract the shape of the ear; holistic methods, which extract the features from
    the ear image as a whole; local methods, which specifically use a portion of the
    image; and hybrid methods, which use a combination of the others [emervsivc2017training](#bib.bib176)
    , [naseem2008sparse](#bib.bib177) .'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 耳朵识别是科学家们正在探索的一个较新的问题，预计涉及耳朵的生物识别工作量在未来几年将会增加。耳朵识别的一个显著特点是可以从受试者头部的任一侧拍摄，耳朵几乎是相同的（适用于受试者不配合或隐藏面部的情况）。此外，由于不需要受试者的接近，拍摄耳朵图像会更容易。然而，受试者的耳朵可能仍会被头发、帽子和珠宝等因素遮挡，使得检测和使用耳朵图像变得困难
    [cintas2019automatic](#bib.bib175)。有多种经典方法可以进行耳朵识别：几何方法，尝试提取耳朵的形状；整体方法，从耳朵图像中提取整体特征；局部方法，专门使用图像的一部分；以及混合方法，结合其他方法
    [emervsivc2017training](#bib.bib176)， [naseem2008sparse](#bib.bib177)。
- en: 3.5.1 Ear Datasets
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 耳朵数据集
- en: The datasets below are some of the popular 2D ear recognition datasets, which
    are used by researchers.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下数据集是一些流行的2D耳朵识别数据集，供研究人员使用。
- en: 'IIT Ear Database: The IIT Delhi ear image database contains 471 images, acquired
    from the 121 different subjects and each subject has at least three ear images
    [iit_ear](#bib.bib36) . All the subjects in the database are in the age group
    14-58 years. The resolution of these images is 272x204 pixels and all these images
    are available in jpeg format.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: IIT耳朵数据库：IIT德里耳朵图像数据库包含471张图像，来自121个不同的受试者，每个受试者至少有三张耳朵图像 [iit_ear](#bib.bib36)。数据库中的所有受试者年龄在14至58岁之间。这些图像的分辨率为272x204像素，所有图像均为jpeg格式。
- en: 'AWE Ear Dataset: This database contains 1,000 images of 100 persons. Images
    were collected from the web using a semi-automatic procedure, and contain the
    following annotations: gender, ethnicity, accessories, occlusions, head pitch,
    head roll, head yaw, head side, and central tragus point [awe_ear](#bib.bib178)
    .'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: AWE耳朵数据集：该数据库包含1,000张100个人的图像。这些图像通过半自动程序从网上收集，并包含以下注释：性别、民族、配饰、遮挡、头部俯仰角、头部滚动角、头部偏航角、头部侧面和中央耳廓点
    [awe_ear](#bib.bib178)。
- en: 'Multi-PIE Ear Dataset: This dataset was created in 2017 [eyiokur2017domain](#bib.bib179)
    based on the Multi-PIE face dataset [gross2010multi](#bib.bib84) . There are 17,000
    ear images extracted from the profile and near-profile images of 205 subjects
    present in the face dataset. The ears in the images are in different illuminations,
    angles, and conditions, making it a decent dataset for a more generalized ear
    recognition approach.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Multi-PIE耳朵数据集：这个数据集是在2017年创建的 [eyiokur2017domain](#bib.bib179)，基于Multi-PIE面部数据集
    [gross2010multi](#bib.bib84)。数据集中从205名被试的侧面和近侧面图像中提取了17,000张耳朵图像。图像中的耳朵在不同的光照、角度和条件下，提供了一个适用于更广泛耳朵识别方法的良好数据集。
- en: 'USTB Ear Database: This dataset contains ear images of 60 volunteers captured
    in 2002 [ustb_ear](#bib.bib180) . Every volunteer is photographed three different
    images. They are normal frontal image, frontal image with trivial angle rotation
    and image under different lighting condition.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: USTB耳朵数据库：这个数据集包含了2002年拍摄的60名志愿者的耳朵图像 [ustb_ear](#bib.bib180)。每名志愿者拍摄了三种不同的图像：正常的正面图像、略微角度旋转的正面图像和不同光照条件下的图像。
- en: 'UERC Ear Dataset: The ear images in this dataset [emervsivc2017unconstrained](#bib.bib181)
    are collected from the Internet in unconstrained conditions, i.e., from the wild.
    There is a total of 11,804 images from 3,706 subjects, of which 2,304 images from
    166 subjects are for training, and the rest are for testing.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: UERC耳朵数据集：这个数据集中的耳朵图像 [emervsivc2017unconstrained](#bib.bib181) 是从互联网中以无约束条件（即野外条件）收集的。总共有11,804张来自3,706名被试的图像，其中2,304张来自166名被试的图像用于训练，其余用于测试。
- en: 'AMI Ear Dataset: This dataset [gonzalez2008biometria](#bib.bib182) contains
    700 images of size 492 x 702 from 100 subjects in the age range of 19 to 65 years
    old. The images are all in the same lighting condition and distance, and from
    both sides of the subject’s head. The images, however, differ in focal lengths,
    and the direction the subject is looking (up, down, left, right).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: AMI耳朵数据集：这个数据集 [gonzalez2008biometria](#bib.bib182) 包含了来自100名年龄在19到65岁之间的被试的700张尺寸为492
    x 702的图像。这些图像都在相同的光照条件和距离下拍摄，且来自被试头部的两侧。然而，这些图像在焦距和被试的视线方向（向上、向下、向左、向右）上有所不同。
- en: 'CP Ear Dataset: One of the older datasets in this area, the Carreira-Perpinan
    dataset [perpinan1995compression](#bib.bib183) contains 102 left ear images taken
    from 17 subjects in the same conditions.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: CP耳朵数据集：这是该领域较早的数据集之一，Carreira-Perpinan数据集 [perpinan1995compression](#bib.bib183)
    包含了在相同条件下拍摄的17名被试的102张左耳图像。
- en: 'WPUT Ear Dataset: The West Pomeranian University of Technology (WPUT) dataset
    [frejlichowski2010west](#bib.bib184) contains 2,071 images from 501 subjects (247
    male and 254 female subjects), from different age groups and ethnicities. The
    images are taken in different lighting conditions, from various distances and
    two angles, and include ears with and without accessories, including earrings,
    glasses, scarves, and hearing aids.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: WPUT耳朵数据集：西波美拉尼亚技术大学（WPUT）数据集 [frejlichowski2010west](#bib.bib184) 包含了来自501名被试（247名男性和254名女性）的2,071张图像，涉及不同的年龄组和民族。这些图像在不同的光照条件、不同的距离和两个角度下拍摄，并包括佩戴和不佩戴配件（如耳环、眼镜、围巾和助听器）的耳朵。
- en: 3.5.2 Deep Learning Works on Ear Recognition
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 深度学习在耳朵识别中的应用
- en: Ear recognition is not as popular as face, iris, and fingerprint recognition
    yet. Therefore, datasets used for this procedure are still limited in size. Based
    on this, Zhang et al [zhang2019few](#bib.bib185) proposed few-shot learning methods,
    where the network use the limited training and quickly learn to recognize the
    images. Dodge et al [dodge2018unconstrained](#bib.bib186) , who proposed using
    transfer learning with deep networks for unconstrained ear recognition Emersic
    and colleagues [emervsivc2017unconstrained](#bib.bib181) , also proposed a deep
    learning-based averaging system to mitigate the overfitting caused by the small
    size of the datasets. In [emersic2017training](#bib.bib187) , the authors proposed
    the first publicly available CNN-based ear recognition method. They explored different
    strategies, such as different architectures, selective learning on pre-trained
    data and aggressive data augmentation to find the best configurations for their
    work.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 耳部识别尚未像面部、虹膜和指纹识别那样流行。因此，用于该过程的数据集仍然有限。基于此，Zhang等人[zhang2019few](#bib.bib185)提出了少样本学习方法，其中网络使用有限的训练数据快速学习识别图像。Dodge等人[dodge2018unconstrained](#bib.bib186)提出了使用迁移学习与深度网络进行无约束耳部识别。Emersic及其同事[emervsivc2017unconstrained](#bib.bib181)也提出了一种基于深度学习的平均系统，以减轻由于数据集规模小而造成的过拟合。在[emersic2017training](#bib.bib187)中，作者提出了第一个公开的基于CNN的耳部识别方法。他们探索了不同的策略，如不同的架构、在预训练数据上选择性学习和积极的数据增强，以寻找最佳的工作配置。
- en: In [emervsivc2018towards](#bib.bib188) , the authors showed how ear accessories
    can disrupt the recognition process and even be used for spoofing, especially
    in a CNN-based method, e.g., VGG-16, against a traditional method, e.g., local
    binary patterns (LBP), and proposed methods to remove such accessories and improve
    the performance, such as ”inprinting” and area coloring. Sinha et al [sinha2019convolutional](#bib.bib189)
    proposed a framework which localizes the outer ear image using HOG and SVMs, and
    then uses CNNs to perform ear recognition. It aims to resolve the issues usually
    associated with feature extraction appearance-based techniques, namely the conditions
    in which the image was taken, such as illumination, angle, contrast, and scale,
    which are also present in other biometric recognition systems, e.g. for face.
    Omara et al [omara2018learning](#bib.bib190) proposed extracting hierarchical
    deep features from ear images, fusing the features using discriminant correlation
    analysis (DCA) Haghighat et al [haghighat2016discriminant](#bib.bib191) to reduce
    their dimensions, and due to the lack of ear images per person, creating pairwise
    samples and using pairwise SVM [brunner2012pairwise](#bib.bib192) to perform the
    matching (since regular SVM would not perform well due to the small size of the
    datasets). Hansley et al [hansley2018employing](#bib.bib193) used a fusion of
    CNNs and handcrafted features for ear recognition which outperformed other state-of-the-art
    CNN-based works, reaching to the conclusion that handcrafted features can complement
    deep learning methods.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在[emervsivc2018towards](#bib.bib188)中，作者展示了耳饰如何干扰识别过程，甚至被用于欺骗，特别是在基于CNN的方法（例如VGG-16）与传统方法（例如局部二值模式（LBP））对比时，并提出了去除这些饰品和提高性能的方法，如“印刷”和区域着色。Sinha等人[sinha2019convolutional](#bib.bib189)提出了一个框架，通过HOG和SVM定位外耳图像，然后使用CNN进行耳部识别。该框架旨在解决通常与基于外观的特征提取技术相关的问题，例如图像拍摄时的条件，如照明、角度、对比度和尺度，这些问题在其他生物识别系统中也存在，例如面部识别。Omara等人[omara2018learning](#bib.bib190)提出从耳部图像中提取层次深度特征，通过判别相关分析（DCA）Haghighat等人[haghighat2016discriminant](#bib.bib191)将特征融合以减少其维度，并由于每人耳部图像的不足，创建成对样本并使用成对SVM[brunner2012pairwise](#bib.bib192)进行匹配（因为常规SVM由于数据集的规模小而效果不佳）。Hansley等人[hansley2018employing](#bib.bib193)使用CNN和手工特征的融合进行耳部识别，表现超越了其他最先进的基于CNN的工作，得出手工特征可以补充深度学习方法的结论。
- en: 3.6 Voice Recognition
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 语音识别
- en: Voice Recognition (also known as speaker recognition) is the task of determining
    a person’s ID using the characteristics of one’s voice. In a way, speaker recognition
    includes both behavioral and physiological features, such as accent and pitch
    respectively. Using automatic ways to perform speaker recognition dates back to
    1960s when Bell Laboratories were approached by law enforcement agencies about
    the possibility of identifying callers who had made verbal bomb threats over the
    telephone [shaver2016brief](#bib.bib194) . Over the years, researchers have developed
    many models that can perform this task effectively, especially with the help of
    deep learning. In addition to security applications, it is also being used in
    virtual personal assistants, such as Google Assistant, so they can recognize and
    distinguish the phone owner’s voice from the others [yoffie2018voice](#bib.bib195)
    .
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别（也称为说话人识别）是指利用一个人的声音特征来确定其身份。在某种程度上，说话人识别包括行为特征和生理特征，例如口音和音调。使用自动化方式进行说话人识别可以追溯到
    1960 年代，当时贝尔实验室受到执法机构的咨询，探讨识别那些通过电话发出口头炸弹威胁的呼叫者的可能性 [shaver2016brief](#bib.bib194)
    。多年来，研究人员开发了许多能够有效执行这一任务的模型，特别是借助深度学习。除了安全应用外，它还被用于虚拟个人助手，例如 Google Assistant，以便识别和区分手机主人的声音与其他声音
    [yoffie2018voice](#bib.bib195) 。
- en: Speaker recognition can be classified into speaker identification and speaker
    verification. speaker identification is the process of determining a person’s
    ID from a set of registered voice using a given utterance [naseem2010sparse](#bib.bib196)
    , whereas speaker verification is the process of accepting or rejecting a proposed
    identity claimed for a speaker [Furui2008](#bib.bib197) . Since these two tasks
    usually share the same evaluation process under commonly-used metrics, the terms
    are sometimes used interchangeably in referenced papers. Speaker recognition is
    also closely related to speaker diarization, where an input audio stream is partitioned
    into homogeneous segments according to the speaker identity [garcia2017speaker](#bib.bib198)
    .
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 说话人识别可以分为说话人识别和说话人验证。**说话人识别** 是指从一组注册的语音中通过给定的发声确定一个人的身份 [naseem2010sparse](#bib.bib196)
    ，而**说话人验证** 是指接受或拒绝为说话人声明的提议身份 [Furui2008](#bib.bib197) 。由于这两个任务通常在常用的评估指标下共享相同的评估过程，因此这些术语在参考文献中有时可以互换使用。说话人识别还与说话人分段密切相关，其中输入音频流根据说话人身份被划分为同质段
    [garcia2017speaker](#bib.bib198) 。
- en: 3.6.1 Voice Datasets
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1 语音数据集
- en: 'Some of the popular datasets on voice/speaker recognition are:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的语音/说话人识别数据集包括：
- en: 'NIST SRE: Starting in 1996, the National Institute of Standards and Technology
    (NIST) has organized a series of evaluations for speaker recognition research
    [martin2001nist](#bib.bib199) . The Speaker Recognition Evaluation (SRE) datasets
    compiled by NIST have thus become the most widely used datasets for evaluation
    of speaker recognition systems. These datasets are collected in an evolving fashion,
    and each evaluation plan has a slightly different focus. These evaluation datasets
    differ in audio lengths [nist2010sre](#bib.bib200) , recording devices (telephone,
    handsets, and video) [nist2018sre](#bib.bib201) , data origination (in North America
    or outside) [nist2016sre](#bib.bib202) , and match/mismatch scenarios. In recent
    years, SRE 2016 and SRE 2018 are the most popular datasets in this area.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 'NIST SRE: 从 1996 年开始，美国国家标准与技术研究院（NIST）组织了一系列说话人识别研究的评估 [martin2001nist](#bib.bib199)
    。NIST 编制的说话人识别评估（SRE）数据集因此成为评估说话人识别系统最广泛使用的数据集。这些数据集是以不断发展的方式收集的，每个评估计划的重点略有不同。这些评估数据集在音频长度
    [nist2010sre](#bib.bib200) 、录音设备（电话、手持设备和视频） [nist2018sre](#bib.bib201) 、数据来源（北美或其他地区）
    [nist2016sre](#bib.bib202) 和匹配/不匹配场景上存在差异。近年来，SRE 2016 和 SRE 2018 是该领域最受欢迎的数据集。'
- en: 'SITW: The Speakers in the Wild (SITW) dataset was acquired across unconstrained
    conditions [mclaren2016speakers](#bib.bib203) . Unlike the SRE datasets, this
    data was not collected under controlled conditions and thus contains real noise
    and reverberation. The database consists of recordings of 299 speakers, with an
    average of eight different sessions per person.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'SITW: **The Speakers in the Wild (SITW)** 数据集是在无约束条件下获取的 [mclaren2016speakers](#bib.bib203)
    。与 SRE 数据集不同，这些数据不是在受控条件下收集的，因此包含了真实的噪声和混响。该数据库包含了 299 名说话人的录音，每人平均有八个不同的会话。'
- en: 'VoxCeleb: The VoxCeleb dataset [nagrani2017voxceleb](#bib.bib204) and VoxCeleb2
    dataset [chung2018voxceleb2](#bib.bib205) are public datasets compiled from interview
    videos uploaded to YouTube to emphasize the lack of large scale unconstrained
    data for speaker recognition. These data are collected using a fully automated
    pipeline. A two-stream synchronization CNN is used to estimate the correlation
    between the audio track and the mouth motion of the video, and then CNN-based
    facial recognition techniques are used to identify speakers for speech annotation.
    VoxCeleb1 contains over 100,000 utterances for 1,251 celebrities, and VoxCeleb2
    contains over a million utterances for 6,112 identities.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'VoxCeleb: VoxCeleb 数据集 [nagrani2017voxceleb](#bib.bib204) 和 VoxCeleb2 数据集 [chung2018voxceleb2](#bib.bib205)
    是公共数据集，从上传到 YouTube 的采访视频中编制，旨在强调缺乏大规模非约束数据用于说话人识别。这些数据通过完全自动化的流程收集。使用双流同步 CNN
    来估计音频轨道和视频的嘴部运动之间的相关性，然后使用基于 CNN 的面部识别技术来识别说话人进行语音注释。VoxCeleb1 包含 100,000 多个说话样本，涉及
    1,251 位名人，而 VoxCeleb2 包含超过一百万个说话样本，涉及 6,112 个身份。'
- en: Apart from datasets designed purely for speaker recognition tasks, many datasets
    collected for automatic speech recognition can also be used for training or evaluation
    of speaker recognition systems. For example, the Switchboard dataset [godfrey1997switchboard](#bib.bib206)
    and the Fisher Corpus [cieri2004fisher](#bib.bib207) , which were originally collected
    for speech recognition tasks, are also used for model training in NIST Speaker
    Recognition Evaluations. On the other hand, researchers may utilize existing speech
    recognition datasets to prepare their own speaker recognition evaluation dataset
    to prove the effectiveness of their research. For example, Librispeech dataset
    [panayotov2015librispeech](#bib.bib208) and the TIMIT dataset [zue1990speech](#bib.bib209)
    are pre-processed by the author in [ravanelli2018learning](#bib.bib210) to serve
    as evaluation set for speaker recognition task.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 除了专门为说话人识别任务设计的数据集外，许多为自动语音识别收集的数据集也可以用于说话人识别系统的训练或评估。例如，Switchboard 数据集 [godfrey1997switchboard](#bib.bib206)
    和 Fisher 语料库 [cieri2004fisher](#bib.bib207) 原本是为语音识别任务收集的，也被用于 NIST 说话人识别评估中的模型训练。另一方面，研究人员可能利用现有的语音识别数据集来准备自己的说话人识别评估数据集，以证明他们研究的有效性。例如，Librispeech
    数据集 [panayotov2015librispeech](#bib.bib208) 和 TIMIT 数据集 [zue1990speech](#bib.bib209)
    被作者在 [ravanelli2018learning](#bib.bib210) 中预处理，以作为说话人识别任务的评估集。
- en: 3.6.2 Deep Learning Works on Voice Recognition
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2 深度学习在语音识别中的应用
- en: Before the era of deep learning, most state-of-the-art speaker recognition systems
    are built with the i-vectors approach [dehak2010front](#bib.bib211) , which uses
    factor analysis to define a low-dimensional space that models both speaker and
    channel variabilities. In recent years, it has become more and more popular to
    explore deep learning approaches for speaker recognition. One of the first approach
    among these efforts is to incorporate DNN-based acoustic models into the i-vector
    framework [lei2014novel](#bib.bib212) . This method uses an DNN acoustic model
    trained for Automatic Speech Recognition (ASR) to gather speaker statistics for
    i-vector model training. It has been shown that this improvement leads to a 30%
    relative reduction in equal error rate.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习时代之前，大多数最先进的说话人识别系统都是基于 i-vectors 方法 [dehak2010front](#bib.bib211) 建立的，该方法使用因子分析定义一个低维空间，以建模说话人和频道的变异性。近年来，探索深度学习方法进行说话人识别变得越来越流行。在这些努力中，最早的一种方法是将基于
    DNN 的声学模型整合到 i-vector 框架中 [lei2014novel](#bib.bib212)。这种方法使用为自动语音识别（ASR）训练的 DNN
    声学模型来收集说话人统计数据，以进行 i-vector 模型训练。研究表明，这种改进导致了 30% 的相对等错误率减少。
- en: Around the same time, d-vector was proposed in [variani2014deep](#bib.bib213)
    to tackle text-dependent speaker recognition using neural network. In this approach,
    a DNN is trained to classify speakers at the frame-level. During enrollment and
    testing, the trained DNN is used to extract speaker specific features from the
    last hidden layer. “d-vectors” are then computed by averaging these features and
    used as speaker embeddings for recognition. This method shows 14% and 25% relative
    improvement over an i-vector system under clean and noisy conditions, respectively.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一时期，d-vector 被提出用于解决基于神经网络的文本相关说话人识别问题，参见 [variani2014deep](#bib.bib213)。在这种方法中，DNN
    被训练以帧级别分类说话人。在注册和测试过程中，训练好的 DNN 用于从最后一层隐藏层中提取说话人特征。然后通过对这些特征取平均计算出“d-vectors”，并将其用作说话人的嵌入进行识别。这种方法在干净和噪声条件下分别比
    i-vector 系统有 14% 和 25% 的相对改进。
- en: In [snyder2018x](#bib.bib214) , a time-delay neural network is trained to extract
    segment level “x-vectors” for text-independent speech recognition. This network
    takes in features of speech segments and passes them through a few non-linear
    layers followed by a pooling layer to classify speakers at segment-level. X-vectors
    are then extracted from the pooling layer for enrollment and testing. It is shown
    that an x-vector system can achieve a better speaker recognition performance compared
    to the traditional i-vector approach, with the help of data augmentation.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [snyder2018x](#bib.bib214) 中，训练了一个时间延迟神经网络来提取用于文本无关语音识别的段级“x-vectors”。该网络接收语音片段的特征，并将其通过几个非线性层，随后经过一个池化层，以在段级别对说话者进行分类。然后从池化层中提取x-vectors进行注册和测试。结果表明，x-vector系统在数据增强的帮助下，相较于传统的i-vector方法，能够实现更好的说话者识别性能。
- en: End-to-end approaches based on neural networks are also explored in various
    papers. In [heigold2016end](#bib.bib215) and [zhang2016end](#bib.bib216) , neural
    networks are designed to take in pairs of speech segments, and are trained to
    classify match/mismatch targets. A specially designed triplet loss function is
    proposed in [zhang2017end](#bib.bib217) to substitute a binary classification
    loss function. Generalized end-to-end (GE2E) loss, which is similar to triplet
    loss, is proposed in [wan2018generalized](#bib.bib218) for text-dependent speaker
    recognition on an in-house dataset.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络的端到端方法在各种论文中也得到了探索。在 [heigold2016end](#bib.bib215) 和 [zhang2016end](#bib.bib216)
    中，设计了神经网络来接收一对语音片段，并训练其分类匹配/不匹配目标。在 [zhang2017end](#bib.bib217) 中，提出了一种特殊设计的三元组损失函数来替代二元分类损失函数。在
    [wan2018generalized](#bib.bib218) 中，提出了一种类似于三元组损失的广义端到端（GE2E）损失，用于在内部数据集上进行文本相关说话者识别。
- en: In [le2018robust](#bib.bib219) , a complementary optimizing goal called intra-class
    loss is proposed to improve deep speaker embeddings learned with triplet loss.
    It is shown in the paper that models trained using intra-class loss can yield
    a significant relative reduction of 30% in equal error rate (EER) compared to
    the original triplet loss. The effectiveness is evaluated on both VoxCeleb and
    VoxForge datasets.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [le2018robust](#bib.bib219) 中，提出了一种称为类内损失的补充优化目标，以改善使用三元组损失学习的深度说话者嵌入。论文中显示，使用类内损失训练的模型相较于原始三元组损失，能够显著降低30%的等错误率（EER）。该方法在VoxCeleb和VoxForge数据集上进行了有效性评估。
- en: In [ravanelli2018learning](#bib.bib210) , the authors proposed a method for
    learning speaker embeddings from raw waveform by maximizing the mutual information.
    This approach uses an encoder-discriminator architecture similar to that of Generative
    Adversarial Networks (GANs) to optimize mutual information implicitly. The authors
    show that this approach effectively learns useful speaker representations, leading
    to a superior performance on the VoxCeleb corpus when compared with i-vector baseline
    and CNN-based triples loss systems.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [ravanelli2018learning](#bib.bib210) 中，作者提出了一种通过最大化互信息来从原始波形中学习说话者嵌入的方法。这种方法使用类似于生成对抗网络（GANs）的编码器-鉴别器架构来隐式优化互信息。作者展示了该方法有效地学习了有用的说话者表示，相比于i-vector基线和基于CNN的三元组损失系统，在VoxCeleb语料库上表现出色。
- en: In [bhattacharya2019deep](#bib.bib220) , the authors combine a deep convolutional
    feature extractor, self-attentive pooling and large-margin loss functions into
    their end-to-end deep speaker recognizers. The individual and ensemble models
    from this approach achieved state-of-the-art performance on VoxCeleb with a relative
    improvement of 70% and 82%, respectively, over the best reported results. The
    authors also proposed to use a neural network to subsitute PLDA classifier, which
    enables them to get the state-of-the-art results on NIST-SRE 2016 dataset.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [bhattacharya2019deep](#bib.bib220) 中，作者将深度卷积特征提取器、自注意力池化和大边际损失函数结合到其端到端深度说话者识别器中。该方法中的个体模型和集成模型在VoxCeleb数据集上达到了最先进的性能，相较于最好的报告结果，分别有70%和82%的相对提升。作者还建议使用神经网络替代PLDA分类器，这使得他们在NIST-SRE
    2016数据集上获得了最先进的结果。
- en: 3.7 Signature Recognition
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 签名识别
- en: Signature is considered a behavioral biometric. It is widely used in traditional
    and digital formats to verify the user’s identity for the purposes of security,
    transactions, agreements, etc. Therefore, being able to distinguish an authentic
    signature from a forged one is of utmost importance. Signature forgery can be
    performed as either a random forgery, where no attempt is made to make an authentic
    signature (e.g., merely writing the name [radhika2011approach](#bib.bib221) ),
    or a skilled forgery, where the signature is made to look like the original and
    is performed with the genuine signature in mind [soleimani2016deep](#bib.bib222)
    .
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 签名被认为是一种行为生物特征。它在传统和数字格式中广泛用于验证用户身份，以确保安全、交易、协议等。因此，能够区分真实签名和伪造签名至关重要。签名伪造可以是随机伪造，即没有尝试模仿真实签名（例如，仅仅是书写名字
    [radhika2011approach](#bib.bib221) ），或者是熟练伪造，即使签名看起来像原始签名，并且是在真实签名的基础上进行的 [soleimani2016deep](#bib.bib222)
    。
- en: In order to distinguish an authentic signature from a forged one, one may either
    store merely signature samples to compare against (offline verification), or also
    the features of the written signature such as the thickness of a stroke and the
    speed of the pen during the signing [impedovo2008automatic](#bib.bib223) . For
    verification, there are writer-dependent (WD) and writer-independent (WI) methods.
    In WD methods, a classifier is trained for each signature owner, whereas, in WI
    methods, one is trained for all owners [srihari2004learning](#bib.bib224) .
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分真实签名和伪造签名，可以仅存储签名样本以供比较（离线验证），也可以存储书写签名的特征，如笔画的粗细和签名时的笔速 [impedovo2008automatic](#bib.bib223)
    。在验证方面，有依赖作者的方法（WD）和不依赖作者的方法（WI）。在WD方法中，为每个签名所有者训练一个分类器，而在WI方法中，为所有所有者训练一个分类器
    [srihari2004learning](#bib.bib224) 。
- en: 3.7.1 Signature Datasets
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.1 签名数据集
- en: 'Some of the popular signature verification datasets include:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的签名验证数据集包括：
- en: 'ICDAR 2009 SVC: ICDAR 2009 Signature Verification Competition contains simultaneously
    acquired online and offline signature samples [icdar_svc](#bib.bib225) . The online
    dataset is called ”NFI-online” and was processed and segmented by Louis Vuurpijl.
    The offline dataset is called ”NFI-offline” and was scanned by Vivian Blankers
    from the NFI. The collection contains: authentic signatures from 100 writers,
    and forged signatures from 33 writers. The NLDCC-online signature collection contains
    in total 1953 online and 1953 offline signature files.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ICDAR 2009 SVC：ICDAR 2009签名验证竞赛包含同时获得的在线和离线签名样本 [icdar_svc](#bib.bib225) 。在线数据集称为“
    NFI-online”，由Louis Vuurpijl处理和分割。离线数据集称为“ NFI-offline”，由Vivian Blankers从NFI扫描。该数据集包含：来自100个作者的真实签名，以及来自33个作者的伪造签名。NLDCC-online签名集合总共有1953个在线签名和1953个离线签名文件。
- en: 'SVC 2004: Signature Verification Competition 2004 consists of two datasets
    for two verification tasks: one for pen-based input devices like PDAs and another
    one for digitizing tablets [SVC2004](#bib.bib226) . Each dataset consists of 100
    sets of signatures with each set containing 20 genuine signatures and 20 skilled
    forgeries.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: SVC 2004：签名验证竞赛2004年包括两个数据集，分别用于两个验证任务：一个用于如PDA等基于笔的输入设备，另一个用于数字化平板 [SVC2004](#bib.bib226)
    。每个数据集包含100组签名，每组包含20个真实签名和20个熟练伪造签名。
- en: 'Offline GPDS-960 Corpus: This offline signature dataset [vargas2007off](#bib.bib227)
    includes signatures from 960 subjects. There are 24 authentic signatures for each
    person, and 30 forgeries performed by other people not in the original 960 (1920
    forgers in total). Some works have used a subset of this public dataset, usually
    the images for the first 160 or 300 subjects, dubbing them GPDS-160 and GPDS-300
    respectively.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 离线 GPDS-960 数据集：这个离线签名数据集 [vargas2007off](#bib.bib227) 包含来自960个对象的签名。每个人有24个真实签名，以及30个由不在原始960人中的其他人伪造的签名（总共1920个伪造者）。一些研究工作使用了这个公共数据集的一个子集，通常是前160或300个对象的图像，分别称之为
    GPDS-160 和 GPDS-300。
- en: 3.7.2 Deep Learning Works on Signature Recognition
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.2 签名识别中的深度学习研究
- en: Before the rise of deep learning to its current popularity, there were a few
    works seeking to use it. For example, Ribeiro et al [ribeiro2011deep](#bib.bib228)
    proposed a deep learning-based method to both identify a signature’s owner and
    distinguish an authentic signature from a fake, making use of the Restricted Boltzmann
    Machine (RBM) [ackley1985learning](#bib.bib229) . With more powerful computer
    and massively parallel architectures making deep learning mainstream, the number
    of deep learning-based works increased dramatically, including those involving
    signature recognition. Rantzsch et al [rantzsch2016signature](#bib.bib230) proposed
    an embedding-based WI offline signature verification, in which the input signatures
    are embedded in a high-dimensional space using a specific training pattern, and
    the Euclidean distance between the input and the embedded signatures will determine
    the outcome. Soleimani et al [soleimani2016deep](#bib.bib222) proposed Deep Multitask
    Metric Learning (DMML), a deep neural network used for offline signature verification,
    mixing WD methods, WI methods, and transfer learning. Zhang et al [zhang2016multi](#bib.bib231)
    proposed a hybrid WD-WI classifier in conjuction with a DC-GAN network in order
    to learn to extract the signature features in an unsupervised manner. With signature
    being a behavioral biometric, it is imperative to learn the best features to distinguish
    an authentic signature from a forged one. Hafemann et al [hafemann2017learning](#bib.bib232)
    proposed a WI CNN-based system to learn features of forgeries from multiple datasets,
    which greatly reduced the error equal rate compared to that of the state-of-the-art.
    Wang et al [wang2019signature](#bib.bib233) proposed signature identification
    using a special GAN network (SIGAN) in which the loss value from the discriminator
    network is utilized as the threshold for the identification process. Tolosana
    et al [tolosana2018exploring](#bib.bib234) proposed an online writer-independent
    signature verification method using Siamese recurrent neural networks (RNNs),
    including long short term memory (LSTM) and gated recurrent units (GRUs).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习崛起并获得当前普及之前，已有一些研究尝试利用这一技术。例如，Ribeiro 等人 [ribeiro2011deep](#bib.bib228)
    提出了基于深度学习的方法来识别签名的拥有者并区分真实签名和伪造签名，使用了受限玻尔兹曼机（RBM） [ackley1985learning](#bib.bib229)。随着更强大的计算机和大规模并行架构使深度学习成为主流，基于深度学习的研究数量急剧增加，包括涉及签名识别的研究。Rantzsch
    等人 [rantzsch2016signature](#bib.bib230) 提出了基于嵌入的WI离线签名验证方法，其中输入签名通过特定的训练模式嵌入到高维空间中，输入签名与嵌入签名之间的欧几里得距离将决定结果。Soleimani
    等人 [soleimani2016deep](#bib.bib222) 提出了深度多任务度量学习（DMML），一种用于离线签名验证的深度神经网络，结合了WD方法、WI方法和迁移学习。Zhang
    等人 [zhang2016multi](#bib.bib231) 提出了一个混合WD-WI分类器，结合DC-GAN网络，以无监督方式提取签名特征。由于签名是行为生物识别的一种，因此学习最佳特征以区分真实签名和伪造签名至关重要。Hafemann
    等人 [hafemann2017learning](#bib.bib232) 提出了基于WI CNN的系统，从多个数据集中学习伪造特征，与最先进的技术相比，显著降低了错误等率。Wang
    等人 [wang2019signature](#bib.bib233) 提出了使用特殊的GAN网络（SIGAN）进行签名识别，其中判别网络的损失值作为识别过程的阈值。Tolosana
    等人 [tolosana2018exploring](#bib.bib234) 提出了使用Siamese递归神经网络（RNNs），包括长短期记忆（LSTM）和门控递归单元（GRUs）的在线签名验证方法。
- en: 3.8 Gait Recognition
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 步态识别
- en: Gait recognition is a popular pattern recognition problem and attracts a lot
    of researchers from different communities such as computer vision, machine learning,
    biomedical, forensic studying and robotics. This problem has also great potential
    in industries such as visual surveillance, since gait can be observed from a distance
    without the need for the subject’s cooperation. Similar to other behavioral biometrics,
    it is difficult, however possible, to try to imitate someone else’s gait [zhang2016siamese](#bib.bib235)
    . It is also possible for the gait to change due to factors such as the carried
    load, injuries, clothing, walking speed, viewing angle, and weather conditions,
    [alotaibi2017improved](#bib.bib236) , [wolf2016multi](#bib.bib237) . It is also
    a challenge to recognize a person among a group of walking people [chen2017multi](#bib.bib238)
    . Gait recognition can be model-based, in which the the structure of the subject’s
    body is extracted (meaning more compute demand), or appearance-based, in which
    features are extracted from the person’s movement in the images [wolf2016multi](#bib.bib237)
    , [zhang2016siamese](#bib.bib235) .
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 步态识别是一个受欢迎的模式识别问题，吸引了来自计算机视觉、机器学习、生物医学、法医学研究和机器人技术等不同领域的大量研究人员。由于步态可以从远处观察而不需要被观察者的配合，这一问题在视觉监控等行业中具有很大潜力。然而，与其他行为生物特征一样，试图模仿他人的步态是困难的，但并非不可能
    [zhang2016siamese](#bib.bib235)。此外，步态可能会由于携带负重、受伤、穿着、行走速度、视角和天气条件等因素发生变化 [alotaibi2017improved](#bib.bib236)
    ，[wolf2016multi](#bib.bib237)。在一群行走的人中识别一个人也是一种挑战 [chen2017multi](#bib.bib238)。步态识别可以是基于模型的，其中提取了被观察者身体的结构（这意味着更多的计算需求），也可以是基于外观的，其中从图像中的人物运动中提取特征
    [wolf2016multi](#bib.bib237)，[zhang2016siamese](#bib.bib235)。
- en: 3.8.1 Gait Datasets
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.8.1 步态数据集
- en: 'Some of the widely used gait recognition datasets include:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一些广泛使用的步态识别数据集包括：
- en: 'CASIA Gait Database: This CASIA Gait Recognition Dataset contains 4 subsets:
    Dataset A (standard dataset) [casia_gaitA](#bib.bib31) , Dataset B (multi-view
    gait dataset), Dataset C (infrared gait dataset), and Dataset D (gait and its
    corresponding footprint dataset) [CASIA_gait](#bib.bib239) . Here we give details
    of CASIA B dataset, which is very popular. Dataset B is a large multi-view gait
    database, which is created in 2005\. There are 124 subjects, and the gait data
    was captured from 11 views. Three variations, namely view angle, clothing and
    carrying condition changes, are separately considered. Besides the video files,
    they also provide human silhouettes extracted from video files. The reader is
    referred to [casia_gaitB](#bib.bib240) for more detailed information about Dataset
    B.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: CASIA 步态数据库：该 CASIA 步态识别数据集包含 4 个子集：数据集 A（标准数据集） [casia_gaitA](#bib.bib31)，数据集
    B（多视角步态数据集），数据集 C（红外步态数据集），以及数据集 D（步态及其对应足迹数据集） [CASIA_gait](#bib.bib239)。这里我们详细介绍了非常受欢迎的
    CASIA B 数据集。数据集 B 是一个大型多视角步态数据库，于 2005 年创建。共有 124 名受试者，步态数据从 11 个视角捕获。分别考虑了视角、穿着和携带条件的三种变化。除了视频文件外，他们还提供了从视频文件中提取的人体轮廓。有关数据集
    B 的更多详细信息，请参见 [casia_gaitB](#bib.bib240)。
- en: 'Osaka Treadmill Dataset: This dataset has been collected in March 2007 at the
    Institute of Scientific and Industrial Research (ISIR), Osaka University (OU)
    [osaka_url](#bib.bib241) . The dataset consists of 4,007 persons walking on a
    treadmill surrounded by the 25 cameras at 60 fps, 640 by 480 pixels. The datasets
    are basically distributed in a form of silhouette sequences registered and size-normalized
    to 88x128 pixels size. They have four subsets of this dataset, dataset A: Speed
    variation, dataset B: Clothes variation, dataset C: view variations, and dataset
    D: Gait fluctuation. The dataset B is composed of gait silhouette sequences of
    68 subjects from the side view with clothes variations of up to 32 combinations.
    Detailed descriptions about all these datasets can be found in this technical
    note [osaka_gait](#bib.bib242) .'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 大阪跑步机数据集：该数据集于 2007 年 3 月在大阪大学（OU）科学与工业研究所（ISIR）收集 [osaka_url](#bib.bib241)。数据集包含
    4,007 人在 25 台相机监控下以 60 fps 的速度走在跑步机上，分辨率为 640x480 像素。数据集基本上以轮廓序列的形式分发，并被注册和大小标准化为
    88x128 像素。该数据集有四个子集，数据集 A：速度变化，数据集 B：衣物变化，数据集 C：视角变化，数据集 D：步态波动。数据集 B 包含 68 名受试者从侧视图拍摄的步态轮廓序列，衣物变化达到
    32 种组合。有关所有这些数据集的详细描述可以在这份技术说明 [osaka_gait](#bib.bib242) 中找到。
- en: 'Osaka University Large Population (OULP) Dataset: This dataset [iwama2012isir](#bib.bib243)
    includes images from 4,016 subjects from different ages (up to 94 years old) taken
    from two surrounding cameras and 4 observation angles. The images are normalized
    to 88x128 pixels.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 大阪大学大规模人群（OULP）数据集：该数据集[iwama2012isir](#bib.bib243)包括来自4,016名不同年龄（最高达94岁）的受试者的图像，这些图像是从两个相邻的摄像头和4个观察角度拍摄的。图像被归一化为88x128像素。
- en: 3.8.2 Deep Learning Works on Gait Recognition
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.8.2 深度学习在步态识别中的应用
- en: Research on gait recognition based on deep learning has only taken off in the
    past few years. In one of the older works, Wolf et al [wolf2016multi](#bib.bib237)
    proposed a gait recognition system using 3D convolutional neural networks which
    learns the gait from multiple viewing angles. This model consists of multiple
    layers of 3D convolutions, max pooling and ReLUs, followed by fully-connected
    layers.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的步态识别研究在过去几年才刚刚起步。在一项较早的工作中，Wolf等人[wolf2016multi](#bib.bib237)提出了一种使用3D卷积神经网络进行步态识别的系统，该系统从多个视角学习步态。该模型由多个3D卷积层、最大池化层和ReLU层组成，最后是全连接层。
- en: 'Zhang et al [zhang2016siamese](#bib.bib235) proposed a Siamese neural network
    for gait recognition, in which the sequences of images are converted into gait
    energy images (GEI) [han2005individual](#bib.bib244) . Next, they are fed to the
    twin CNN networks and their contrastive losses are also calculated. This allows
    the system to minimize the loss for similar inputs and maximize it for different
    ones. The network for this work is shown in Figure [19](#S3.F19 "Figure 19 ‣ 3.8.2
    Deep Learning Works on Gait Recognition ‣ 3.8 Gait Recognition ‣ 3 Deep Learning
    Based Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning:
    A Survey").'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人[zhang2016siamese](#bib.bib235)提出了一种用于步态识别的孪生神经网络，其中图像序列被转换为步态能量图像（GEI）[han2005individual](#bib.bib244)。接下来，这些图像被输入到两个卷积神经网络（CNN）中，并计算它们的对比损失。这使得系统能够最小化相似输入的损失，并最大化不同输入的损失。这项工作的网络结构如图[19](#S3.F19
    "图 19 ‣ 3.8.2 深度学习在步态识别中的应用 ‣ 3.8 步态识别 ‣ 3 基于深度学习的生物识别识别工作 ‣ 使用深度学习的生物识别识别：综述")所示。
- en: '![Refer to caption](img/e29d9c494ed29a9e006d15fbf58ee435.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e29d9c494ed29a9e006d15fbf58ee435.png)'
- en: 'Figure 19: Siamese network for gait recognition, courtesy of [zhang2016siamese](#bib.bib235)
    .'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：步态识别的孪生网络，感谢[zhang2016siamese](#bib.bib235)提供。
- en: Battistone et al [battistone2019tglstm](#bib.bib245) proposed gait recognition
    through a time-based graph LSTM network, which uses alternating recursive LSTM
    layers and dense layers to extract skeletons from the person’s images and learn
    their joint features. Zou et al [zou2018deep](#bib.bib246) proposed a hybrid CNN-RNN
    network which uses the data from smartphone sensors for gait recognition, particularly
    from the accelerometer and the gyroscope, and the subjects are not restricted
    in their walking in any way.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Battistone等人[battistone2019tglstm](#bib.bib245)提出了一种通过基于时间的图LSTM网络进行步态识别的方法，该方法使用交替的递归LSTM层和全连接层从个人的图像中提取骨架并学习它们的关节特征。Zou等人[zou2018deep](#bib.bib246)提出了一种混合CNN-RNN网络，该网络利用智能手机传感器中的数据进行步态识别，特别是加速度计和陀螺仪的数据，并且受试者在行走时没有任何限制。
- en: 4 Performance of Different Models on Different Datasets
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 不同模型在不同数据集上的性能
- en: In this section, we are going to present the performance of different biometric
    recognition models developed over the past few years. We are going to present
    the results of each biometric recognition model separately, by providing the performance
    of several promising works on one or two widely used dataset of that biometric.
    Before getting into the quantitative analysis, we are going to first briefly introduce
    some of the popular metrics that are used for evaluating biometric recognition
    models.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示过去几年开发的不同生物识别识别模型的性能。我们将单独展示每个生物识别识别模型的结果，通过提供几项有前途的工作在一两个广泛使用的数据集上的表现。在进行定量分析之前，我们将首先简要介绍一些用于评估生物识别识别模型的流行指标。
- en: 4.1 Popular Metrics For Evaluating Biometrics Recognition Systems
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 生物识别识别系统的流行评估指标
- en: Various metrics are designed to evaluate the performance a biometric recognition
    systems. Here we provide an overview of some of the popular metrics for evaluation
    verification and identification algorithms.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 各种指标被设计用于评估生物识别识别系统的性能。在这里，我们提供了一些流行评估验证和识别算法的指标概述。
- en: Biometric verification is relevant to the problem of re-identification, where
    we want to see if a given data matches a registered sample. In many cases the
    performance is measured in terms of verification accuracy, specially when a test
    dataset is provided. Equal error rate (EER) is another popular metric, which is
    the rate of error decided by a threshold that yields equal false negative rate
    and false positive rate. Receiver operating characteristic (ROC) is also another
    classical metric used for verification performance. ROC essentially measures the
    true positive rate (TPR), which is the fraction of genuine comparisons that correctly
    exceeds the threshold, and the false positive rate (FPR), which is the fraction
    of impostor comparisons that incorrectly exceeds the threshold, at different thresholds.
    ACC (classification accuracy) is another metric used by LFW, which is simply the
    percentage of correct classifications. Many works also use TPR for a certain FPR.
    For example IJB-A focuses TPR@FAR=$10^{-3}$, while Megaface uses TPR@FPR= $10^{-6}$.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 生物特征验证与再识别问题相关，其中我们希望查看给定数据是否与注册的样本匹配。在许多情况下，性能是根据验证准确性来衡量的，特别是当提供了一个测试数据集时。等错误率（EER）是另一个流行的指标，它是通过产生相等的误报率和漏报率来决定的阈值的错误率。接收者操作特征（ROC）也是另一个用于验证性能的经典指标。ROC主要衡量真正率（TPR），即正确超过阈值的真实比较的比例，以及错误超过阈值的冒名比较的假阳率（FPR），在不同的阈值下。ACC（分类准确性）是LFW使用的另一个指标，它简单地是正确分类的百分比。许多作品还使用特定FPR的TPR。例如，IJB-A关注TPR@FAR=$10^{-3}$，而Megaface使用TPR@FPR=$10^{-6}$。
- en: Closed-set identification can be measured in terms of closed-set identification
    accuracy, as well as rank-N detection and identification rate. Rank-N measures
    the percentage of probe searches return the samples from probe’s gallery within
    the top N rank-ordered results (e.g. IJB-A/B/C focuses on the rank-1 and rank-5
    recognition rates). The cumulative match characteristic (CMC) is another popular
    metric, which measures the percentage of probes identified within a given rank.
    Confusion matrix is also a popular metric for smaller datasets.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 封闭式身份识别可以用封闭式身份识别准确性以及排名N的检测和识别率来衡量。排名N测量在前N个排序结果中是否有来自探测器画廊的样本的比例（例如，IJB-A/B/C关注排名1和排名5的识别率）。累积匹配特性（CMC）是另一个流行的指标，它测量了在给定排序中识别的探针的百分比。混淆矩阵也是较小数据集的流行指标。
- en: Open-set identification deals with the cases where the recognition system should
    reject unknown/unseen subjects (probes which are not present in gallery) at the
    test time. At present, there are very few databases covering the task of open-set
    biometric recognition. Open-set identification accuracy is a popular metrics for
    this task. Some benchmarks also suggested to use the decision error trade-off
    (DET) curve to characterize the FNIR (false-negative identification rate) as a
    function of FPIR (false-positive identification rate).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 开放式身份识别处理的是识别系统在测试时应该拒绝未知/未见主体（不在画廊中的探针）。目前，几乎没有数据库涵盖开放式生物识别识别任务。开放式身份识别准确性是此任务的流行指标。一些基准还建议使用决策错误权衡（DET）曲线将FNIR（错误负面识别率）描述为FPIR（错误正面识别率）的函数。
- en: 'Performance of Models for Face Recognition: For face recognition, various metrics
    are used for verification and identification. For face verification, EER is one
    of the most popular metrics. For identification, various metrics are used such
    as close-set identification accuracy, open-set identification accuracy. For open-set
    performance, many works used detection and identification accuracy at a certain
    false-alarm rate (mostly 1%).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸识别模型的性能：对于人脸识别，使用各种指标进行验证和识别。对于人脸验证，等错误率是最流行的指标之一。对于识别，使用多种指标，如封闭式识别准确性和开放式识别准确性。对于开放式性能，许多作品使用在特定错误报警率下的检测和识别准确性（通常为1%）。
- en: Due to the popularity of face recognition, there are a large number of algorithms
    and datasets available. Here, we are going to provide the performance of some
    of the most promising deep learning-based face recognition models, and their comparison
    with some of the promising classical face recognition models on three popular
    datasets.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人脸识别技术的流行，目前有大量的算法和数据集可供使用。在这里，我们将提供一些最有希望的基于深度学习的人脸识别模型的性能，并将它们与一些有希望的经典人脸识别模型在三个流行数据集上进行比较。
- en: 'As mentioned earlier, LFW is one of the most widely used for face recognition.
    The performance of some of the most prominent deep learning-based face verification
    models on this dataset is provided in Table [1](#S4.T1 "Table 1 ‣ 4.1 Popular
    Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different
    Models on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey").
    We have also included the results of two very well-known classical face verification
    works. As we can see, models based on deep learning algorithms achieve superior
    performance over classical techniques with a large margin. In fact, many deep
    learning approaches have surpassed human performance and are already close to
    100% (For verification task, not identification).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，LFW 是最广泛使用的面部识别数据集之一。表 [1](#S4.T1 "Table 1 ‣ 4.1 Popular Metrics For Evaluating
    Biometrics Recognition Systems ‣ 4 Performance of Different Models on Different
    Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey") 提供了一些最突出的深度学习人脸验证模型在该数据集上的表现。我们还包括了两项非常著名的经典人脸验证工作的结果。正如我们所见，基于深度学习算法的模型在性能上明显优于经典技术。实际上，许多深度学习方法已超过人类表现，并且已经接近100%（用于验证任务，而非识别）。'
- en: 'Table 1: Accuracy of different face recognition models for face verification
    on LFW dataset.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同面部识别模型在LFW数据集上的面部验证准确率。
- en: '| Method | Architecture | Used Dataset | Accuracy on LFW |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 架构 | 使用的数据集 | LFW上的准确率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Joint Bayesian [jointbayes](#bib.bib247) | Classical | - | 92.4 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Joint Bayesian [jointbayes](#bib.bib247) | 经典 | - | 92.4 |'
- en: '| Tom-vs-Pete [Tom-vs-Pete](#bib.bib248) | Classical | - | 93.3 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Tom-vs-Pete [Tom-vs-Pete](#bib.bib248) | Classical | - | 93.3 |'
- en: '| DeepFace [DeepFace](#bib.bib96) | AlexNet | Facebook (4.4M,4K) | 97.35 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| DeepFace [DeepFace](#bib.bib96) | AlexNet | Facebook (4.4M,4K) | 97.35 |'
- en: '| DeepID2 [DeepID2](#bib.bib98) | AlexNet | CelebFaces+ (0.2M,10K) | 99.15
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| DeepID2 [DeepID2](#bib.bib98) | AlexNet | CelebFaces+ (0.2M,10K) | 99.15
    |'
- en: '| VGGface [VGGface_model](#bib.bib101) | VGGNet-16 | VGGface (2.6M,2.6K) |
    98.95 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| VGGface [VGGface_model](#bib.bib101) | VGGNet-16 | VGGface (2.6M,2.6K) |
    98.95 |'
- en: '| DeepID3 [DeepID3](#bib.bib99) | VGGNet-10 | CelebFaces+ (0.2M,10K) | 99.53
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| DeepID3 [DeepID3](#bib.bib99) | VGGNet-10 | CelebFaces+ (0.2M,10K) | 99.53
    |'
- en: '| FaceNet [FaceNet](#bib.bib100) | GoogleNet-24 | Google (500M,10M) | 99.63
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| FaceNet [FaceNet](#bib.bib100) | GoogleNet-24 | Google (500M,10M) | 99.63
    |'
- en: '| Range Loss [RangeLoss](#bib.bib105) | VGGNet-16 | MS-Celeb-1M, CASIA-WebFace
    | 99.52 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Range Loss [RangeLoss](#bib.bib105) | VGGNet-16 | MS-Celeb-1M, CASIA-WebFace
    | 99.52 |'
- en: '| L2-softmax [L2softmax](#bib.bib106) | ResNet-101 | MS-Celeb-1M (3.7M,58K)
    | 99.87 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| L2-softmax [L2softmax](#bib.bib106) | ResNet-101 | MS-Celeb-1M (3.7M,58K)
    | 99.87 |'
- en: '| Marginal Loss [marginalloss](#bib.bib249) | ResNet-27 | MS-Celeb-1M (4M,80K)
    | 99.48 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Marginal Loss [marginalloss](#bib.bib249) | ResNet-27 | MS-Celeb-1M (4M,80K)
    | 99.48 |'
- en: '| SphereFace [Sphereface](#bib.bib108) | ResNet-64 | CASIA-WebFace (0.49M,10K)
    | 99.42 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| SphereFace [Sphereface](#bib.bib108) | ResNet-64 | CASIA-WebFace (0.49M,10K)
    | 99.42 |'
- en: '| AMS loss [amsloss](#bib.bib109) | ResNet-20 | CASIA-WebFace (0.49M,10K) |
    99.12 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| AMS loss [amsloss](#bib.bib109) | ResNet-20 | CASIA-WebFace (0.49M,10K) |
    99.12 |'
- en: '| Cos Face [CosFace](#bib.bib110) | ResNet-64 | CASIA-WebFace (0.49M,10K) |
    99.33 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Cos Face [CosFace](#bib.bib110) | ResNet-64 | CASIA-WebFace (0.49M,10K) |
    99.33 |'
- en: '| Ring loss [RingLoss](#bib.bib112) | ResNet-64 | CelebFaces+ (0.2M,10K) |
    99.50 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Ring loss [RingLoss](#bib.bib112) | ResNet-64 | CelebFaces+ (0.2M,10K) |
    99.50 |'
- en: '| Arcface [Arcface](#bib.bib111) | ResNet-100 | MS-Celeb-1M (3.8M,85K) | 99.45
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Arcface [Arcface](#bib.bib111) | ResNet-100 | MS-Celeb-1M (3.8M,85K) | 99.45
    |'
- en: '| AdaCos [adacos](#bib.bib113) | ResNet-50 | WebFace | 99.71 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| AdaCos [adacos](#bib.bib113) | ResNet-50 | WebFace | 99.71 |'
- en: '| P2SGrad [p2sgrad](#bib.bib114) | ResNet-50 | CASIAWebFace | 99.82 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| P2SGrad [p2sgrad](#bib.bib114) | ResNet-50 | CASIAWebFace | 99.82 |'
- en: 'As mentioned earlier, closed-set identification is another popular face recognition
    task. Table [2](#S4.T2 "Table 2 ‣ 4.1 Popular Metrics For Evaluating Biometrics
    Recognition Systems ‣ 4 Performance of Different Models on Different Datasets
    ‣ Biometrics Recognition Using Deep Learning: A Survey"), provides the summary
    of the performance of some of the recent state-of-the-art deep learning-based
    works on the MegaFace challenge 1 (for both identification and verification tasks).
    MegaFace challenge evaluates rank1 recognition rate as a function of an increasing
    number of gallery distractors (going from 10 to 1 million) for identification
    accuracy. For verification, they report TPR at FAR= $10^{-6}$. Some of these reported
    accuracies are taken from [adapface](#bib.bib116) , where they implemented the
    Softmax, A-Softmax, CosFace, ArcFace and the AdaptiveFace models with the same
    50-layer CNN, for fair comparison. As we can see the deep learning-based models
    in recent years achieve very high Rank-1 identification accuracy even in the case
    where 1 million distractors are included in the gallery.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，封闭集识别是另一个流行的面部识别任务。表[2](#S4.T2 "Table 2 ‣ 4.1 Popular Metrics For Evaluating
    Biometrics Recognition Systems ‣ 4 Performance of Different Models on Different
    Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey")提供了一些最新最先进的深度学习模型在
    MegaFace 挑战 1 中（无论是识别还是验证任务）的性能总结。MegaFace 挑战评估在识别准确度方面，随着画廊干扰项数量的增加（从 10 到 100
    万），Rank1 识别率的变化。对于验证，他们报告了在 FAR= $10^{-6}$ 下的 TPR。这些报告的准确率部分来自 [adapface](#bib.bib116)，他们在相同的
    50 层 CNN 上实现了 Softmax、A-Softmax、CosFace、ArcFace 和 AdaptiveFace 模型，以公平比较。可以看出，近年来基于深度学习的模型即使在画廊中包含
    100 万个干扰项的情况下，Rank-1 识别准确率也达到了非常高的水平。'
- en: 'Table 2: Face identification and verification evaluation on MegaFace Challenge
    1.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 MegaFace 挑战 1 中的面部识别和验证评估。
- en: '| Method | Protocol | Rank1 Identification Accuracy | (TPR@$10^{-6}$FPR) Verification
    Accuracy |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 协议 | Rank1 识别准确率 | (TPR@$10^{-6}$FPR) 验证准确率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Beijing FaceAll Norm 1600, from [adapface](#bib.bib116) | Large | 64.8 |
    67.11 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 北京 FaceAll Norm 1600, from [adapface](#bib.bib116) | 大型 | 64.8 | 67.11 |'
- en: '| Softmax [adapface](#bib.bib116) | Large | 71.36 | 73.04 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Softmax [adapface](#bib.bib116) | 大型 | 71.36 | 73.04 |'
- en: '| Google - FaceNet v8 [FaceNet](#bib.bib100) | Large | 70.49 | 86.47 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Google - FaceNet v8 [FaceNet](#bib.bib100) | 大型 | 70.49 | 86.47 |'
- en: '| YouTu Lab, from [adapface](#bib.bib116) | Large | 83.29 | 91.34 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| YouTu Lab, from [adapface](#bib.bib116) | 大型 | 83.29 | 91.34 |'
- en: '| DeepSense V2, from [adapface](#bib.bib116) | Large | 81.29 | 95.99 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| DeepSense V2, from [adapface](#bib.bib116) | 大型 | 81.29 | 95.99 |'
- en: '| Cos Face (Single-patch) [CosFace](#bib.bib110) | Large | 82.72 | 96.65 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Cos Face（单补丁）[CosFace](#bib.bib110) | 大型 | 82.72 | 96.65 |'
- en: '| Cos Face (3-patch ensemble) [CosFace](#bib.bib110) | Large | 84.26 | 97.96
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| Cos Face（三补丁集成）[CosFace](#bib.bib110) | 大型 | 84.26 | 97.96 |'
- en: '| SphereFace [Sphereface](#bib.bib108) | Large | 92.241 | 93.423 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| SphereFace [Sphereface](#bib.bib108) | 大型 | 92.241 | 93.423 |'
- en: '| Arcface [Arcface](#bib.bib111) | Large | 94.637 | 94.850 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Arcface [Arcface](#bib.bib111) | 大型 | 94.637 | 94.850 |'
- en: '| AdaptiveFace [adapface](#bib.bib116) | Large | 95.023 | 95.608 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| AdaptiveFace [adapface](#bib.bib116) | 大型 | 95.023 | 95.608 |'
- en: Deep learning-based models have achieved great performance on other facial analysis
    tasks too, such as facial landmark detection, facial expression recognition, face
    tracking, age prediction from face, face aging, part of face tracking, and many
    more. As this paper is mostly focused on biometric recognition, we skip the details
    of models developed for those works here.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的模型在其他面部分析任务上也取得了出色的表现，如面部标志检测、面部表情识别、面部跟踪、从面部预测年龄、面部衰老、部分面部跟踪等。由于本文主要集中于生物识别，我们在此省略了这些工作的模型详细信息。
- en: 'Performance of Models for Fingerprint Recognition: It is common for fingerprint
    recognition models to report their results using either the accuracy or equal
    error rate (EER). Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating Biometrics
    Recognition Systems ‣ 4 Performance of Different Models on Different Datasets
    ‣ Biometrics Recognition Using Deep Learning: A Survey") provides the accuracy
    of some of the recent fingerprint recognition works on PolyU, FVC, and CASIA databases.
    As we can see, deep learning-based models achieve very high accuracy rate on these
    benchmarks.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '指纹识别模型的性能：指纹识别模型通常使用准确率或等错误率（EER）报告其结果。表[4.1](#S4.SS1 "4.1 Popular Metrics
    For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different Models
    on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey")提供了一些最近在
    PolyU、FVC 和 CASIA 数据库上进行的指纹识别工作的准确率。从中可以看出，基于深度学习的模型在这些基准测试中达到了非常高的准确率。'
- en: 'Table 3: Accuracy of several fingerprint recognition algorithms.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 几种指纹识别算法的准确率。'
- en: \csvreader
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: \csvreader
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
- en: ']csv/fingerprint.csv\csvlinetotablerow'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ']csv/fingerprint.csv\csvlinetotablerow'
- en: 'Performance of Models for Iris Recognition: Many of the recent iris recognition
    works have reported their accuracy rates on different iris databases, making it
    hard to compare all of them on a single benchmark. The performance of deep learning-based
    iris recognition algorithms, and their comparison with some of the promising classical
    iris recognition models are provided in Table [4](#S4.T4 "Table 4 ‣ 4.1 Popular
    Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different
    Models on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey").
    As we can see models based on deep learning algorithms achieve superior performance
    over classical techniques. Some of these numbers are taken from [iris_survey](#bib.bib251)
    and [iris_2019](#bib.bib252) .'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '虹膜识别模型的性能：许多最近的虹膜识别工作在不同的虹膜数据库上报告了其准确率，使得在单一基准上进行比较变得困难。表 [4](#S4.T4 "Table
    4 ‣ 4.1 Popular Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance
    of Different Models on Different Datasets ‣ Biometrics Recognition Using Deep
    Learning: A Survey") 提供了基于深度学习的虹膜识别算法的性能以及它们与一些有前景的经典虹膜识别模型的比较。可以看出，基于深度学习算法的模型在性能上优于经典技术。一些数据来自
    [iris_survey](#bib.bib251) 和 [iris_2019](#bib.bib252)。'
- en: 'Table 4: The performance of iris recognition models on some of the most popular
    datasets.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 一些最受欢迎的数据集上虹膜识别模型的性能。'
- en: '| Method | Dataset | Model/Feature | Performance |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 数据集 | 模型/特征 | 性能 |'
- en: '| Elastic Graph Matching [elastic](#bib.bib253) | IITD | - | Acc= 98% |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 弹性图匹配 [elastic](#bib.bib253) | IITD | - | 准确率= 98% |'
- en: '| SIFT Based Model [sift_iris](#bib.bib254) | CASIA, MMU, UBIRIS | SIFT features
    | Acc= 99.05% EER=3.5% |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 基于SIFT的模型 [sift_iris](#bib.bib254) | CASIA, MMU, UBIRIS | SIFT特征 | 准确率= 99.05%
    EER=3.5% |'
- en: '| Deep CNN [menon2018iris](#bib.bib151) | IITD | - | Acc=99.8% |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 深度卷积神经网络 [menon2018iris](#bib.bib151) | IITD | - | 准确率=99.8% |'
- en: '| Deep CNN [menon2018iris](#bib.bib151) | UBIRIS v2 | - | Acc=95.36% |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 深度卷积神经网络 [menon2018iris](#bib.bib151) | UBIRIS v2 | - | 准确率=95.36% |'
- en: '| Deep Scattering [iris_0](#bib.bib139) | IITD | ScatNet3+Texture features
    | Acc= 99.2% |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 深度散射 [iris_0](#bib.bib139) | IITD | ScatNet3+纹理特征 | 准确率= 99.2% |'
- en: '| Deep Features [iris_1](#bib.bib147) | IITD | VGG-16 | Acc= 99.4% |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 深度特征 [iris_1](#bib.bib147) | IITD | VGG-16 | 准确率= 99.4% |'
- en: '| SCNN [iris_scnn](#bib.bib255) | CASIA-v4, FRGC FOCS | Semantics-assisted
    convolutional networks | R1-ACC= 98.4 (CASIA-v4) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| SCNN [iris_scnn](#bib.bib255) | CASIA-v4, FRGC FOCS | 语义辅助卷积网络 | R1-ACC=
    98.4 (CASIA-v4) |'
- en: 'Performance of Models for Palmprint Recognition: It is common for palmprint
    recognition papers to compare their work against others using the accuracy rate
    or equal error rate (EER). Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating
    Biometrics Recognition Systems ‣ 4 Performance of Different Models on Different
    Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey") displays the
    accuracy of some of the palmprint recognition works. As we can see, deep learning-based
    models achieve very high accuracy rate on PolyU palmprint dataset.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '手掌纹识别模型的性能：手掌纹识别论文常用准确率或等错误率（EER）来与其他研究进行比较。表 [4.1](#S4.SS1 "4.1 Popular Metrics
    For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different Models
    on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey")
    显示了一些手掌纹识别工作的准确率。可以看出，基于深度学习的模型在PolyU手掌纹数据集上实现了非常高的准确率。'
- en: 'Table 5: Accuracy of various palmprint recognition systems.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 各种手掌纹识别系统的准确率。'
- en: \csvreader
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \csvreader
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
- en: ']csv/palmprint.csv\csvlinetotablerow'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ']csv/palmprint.csv\csvlinetotablerow'
- en: 'Performance of Models for Ear Recognition: The results of some of the recent
    ear recognition models are provided in Table [4.1](#S4.SS1 "4.1 Popular Metrics
    For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different Models
    on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey").
    Besides recognition accuracy, some of the works have also reported their rank-5
    accuracy, i.e. if one of the first 5 outputs of the algorithm is correct, the
    algorithm has succeeded. Different deep learning-based models for ear recognition
    report their accuracy on different benchmarks. Therefore, we list some of the
    promising works, along with the respective datasets that they are evaluated on,
    in Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey").'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '模型在耳识别中的表现：最近一些耳识别模型的结果见表[4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating
    Biometrics Recognition Systems ‣ 4 Performance of Different Models on Different
    Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey")。除了识别准确率外，一些研究还报告了它们的
    rank-5 准确率，即如果算法的前 5 个输出中有一个是正确的，算法就算成功。不同基于深度学习的耳识别模型在不同基准上的准确率有所报告。因此，我们在表[4.1](#S4.SS1
    "4.1 Popular Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance
    of Different Models on Different Datasets ‣ Biometrics Recognition Using Deep
    Learning: A Survey")中列出了一些有前景的工作，以及它们所评估的相应数据集。'
- en: 'Table 6: Accuracy of select ear recognition algorithms.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：选择的耳识别算法的准确率。
- en: \csvreader
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: \csvreader
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格=—c—c—c—，无标题，列数=3，表头=，行后延迟='
- en: ']csv/ear.csv\csvlinetotablerow'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ']csv/ear.csv\csvlinetotablerow'
- en: 'Performance of Models for Voice Recognition: The most widely used metric for
    evaluation of speaker recognition systems is Equal Error Rate (EER). Apart from
    EER, other metrics are also used for system evaluation. For example, detection
    error trade-off curve (DET curve) is used in SRE performance evaluations to compare
    different systems. A DET curve is created by plotting the false negative rate
    versus false positive rate, with logarithmic scale on the x- and y-axes. (EER
    corresponds to the point on a DET curve where false negative rate and false positive
    rate are equal.) Minimum detection cost is another metric that is frequently used
    in speaker recognition tasks [van2007introduction](#bib.bib261) . This cost is
    defined as a weighted average of two normalized error rates. Not all of these
    metrics are reported in every research papers, but EER is the most important metric
    to compare different systems.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在语音识别中的表现：评价说话人识别系统的最广泛使用的指标是等错误率（EER）。除了 EER，还有其他指标用于系统评价。例如，检测错误权衡曲线（DET
    曲线）用于 SRE 性能评估中，以比较不同系统。DET 曲线是通过绘制假阴性率与假阳性率的对数尺度图来创建的。（EER 对应于 DET 曲线上的点，其中假阴性率和假阳性率相等。）最小检测成本是另一个在说话人识别任务中经常使用的指标[van2007introduction](#bib.bib261)。这个成本定义为两个标准化错误率的加权平均值。并非所有这些指标都会在每篇研究论文中报告，但
    EER 是比较不同系统的最重要指标。
- en: 'Table [7](#S4.T7 "Table 7 ‣ 4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey") records the performance of some of
    the best deep leaning based spearker recognition systems on VoxCeleb1 dataset.
    As is shown in the table, the progress made by researchers over the last two years
    are prominent. All these systems shown in Table [7](#S4.T7 "Table 7 ‣ 4.1 Popular
    Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different
    Models on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey")
    are single systems, which means the performance can be boosted further with system
    combination or ensembles.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '表[7](#S4.T7 "Table 7 ‣ 4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey")记录了在 VoxCeleb1 数据集上表现最好的深度学习基础说话人识别系统的一些性能。正如表中所示，研究人员在过去两年取得的进展显著。表[7](#S4.T7
    "Table 7 ‣ 4.1 Popular Metrics For Evaluating Biometrics Recognition Systems ‣
    4 Performance of Different Models on Different Datasets ‣ Biometrics Recognition
    Using Deep Learning: A Survey")中显示的所有这些系统都是单一系统，这意味着通过系统组合或集成，性能可以进一步提升。'
- en: 'Table 7: Accuracy of different speaker recognition systems on VoxCeleb1 dataset'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同说话人识别系统在 VoxCeleb1 数据集上的准确率
- en: '| Model | Loss | Training set |   EER |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 损失 | 训练集 |   EER |'
- en: '| --- | --- | --- | --- |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| i-vector + PLDA [shon2018frame](#bib.bib262) | - | VoxCeleb1 | 5.39 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| i-vector + PLDA [shon2018frame](#bib.bib262) | - | VoxCeleb1 | 5.39 |'
- en: '| SincNet+LIM (raw audio) [ravanelli2018learning](#bib.bib210) | - | VoxCeleb1
    | 5.80 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| SincNet+LIM (原始音频) [ravanelli2018learning](#bib.bib210) | - | VoxCeleb1 |
    5.80 |'
- en: '| x-vector* [shon2018frame](#bib.bib262) | Softmax | VoxCeleb1 | 6.00 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| x-vector* [shon2018frame](#bib.bib262) | Softmax | VoxCeleb1 | 6.00 |'
- en: '| ResNet-34 [cai2018exploring](#bib.bib263) | A-Softmax + GNLL | VoxCeleb1
    | 4.46 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-34 [cai2018exploring](#bib.bib263) | A-Softmax + GNLL | VoxCeleb1
    | 4.46 |'
- en: '| x-vector [okabe2018attentive](#bib.bib264) | Softmax | VoxCeleb1 | 3.85 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| x-vector [okabe2018attentive](#bib.bib264) | Softmax | VoxCeleb1 | 3.85 |'
- en: '| ResNet-20 [hajibabaei2018unified](#bib.bib265) | AM-Softmax | VoxCeleb1 |
    4.30 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-20 [hajibabaei2018unified](#bib.bib265) | AM-Softmax | VoxCeleb1 |
    4.30 |'
- en: '| ResNet-50 [chung2018voxceleb2](#bib.bib205) | Softmax + Contrastive | VoxCeleb2
    | 3.95 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 [chung2018voxceleb2](#bib.bib205) | Softmax + Contrastive | VoxCeleb2
    | 3.95 |'
- en: '| Thin ResNet-34 [xie2019utterance](#bib.bib266) | Softmax | VoxCeleb2 | 3.22
    |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| Thin ResNet-34 [xie2019utterance](#bib.bib266) | Softmax | VoxCeleb2 | 3.22
    |'
- en: '| ResNet-28 [bhattacharya2019deep](#bib.bib220) | AAM | VoxCeleb1 | 0.95 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-28 [bhattacharya2019deep](#bib.bib220) | AAM | VoxCeleb1 | 0.95 |'
- en: For SRE datasets, due to the large number of its series and complexity of different
    evaluation conditions, it is hard to compile all results into one table. Also
    different papers may present results on different sets or conditions, making it
    hard to compare the performance across different approaches.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SRE数据集，由于其系列数量庞大和各种评估条件的复杂性，难以将所有结果汇总到一个表格中。此外，不同的论文可能在不同的数据集或条件下呈现结果，这使得在不同方法之间比较性能变得困难。
- en: The deep learning-based approaches discussed above have also been applied to
    other related areas, e.g. speech diarization, replay attack detection and language
    identification. Since this paper focuses on biometric recognition, we skip the
    details for these tasks.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 上述基于深度学习的方法也已应用于其他相关领域，例如语音分段、重放攻击检测和语言识别。由于本文专注于生物识别，我们跳过了这些任务的细节。
- en: 'Performance of Models for Signature Recognition: Most signature recognition
    works use EER as the performance metric, but sometimes, they also report accuracy.
    Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey") summarizes the EER of several signature
    verification methods on GPDS dataset, where there are 12 authentic signature samples
    used for each person (except in [soleimani2016deep](#bib.bib222) where it is 10
    samples). In addition, Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating
    Biometrics Recognition Systems ‣ 4 Performance of Different Models on Different
    Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey") provides the
    reported accuracy of a few other works on other datasets.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '签名识别模型的性能：大多数签名识别工作使用EER作为性能指标，但有时也会报告准确率。表[4.1](#S4.SS1 "4.1 Popular Metrics
    For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different Models
    on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey")总结了GPDS数据集上几种签名验证方法的EER，其中每个人使用了12个真实签名样本（除了[soleimani2016deep](#bib.bib222)中使用了10个样本）。此外，表[4.1](#S4.SS1
    "4.1 Popular Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance
    of Different Models on Different Datasets ‣ Biometrics Recognition Using Deep
    Learning: A Survey")提供了其他几项工作的报告准确率。'
- en: 'Table 8: Reported EER of selected signature recognition models on GPDS dataset
    (using 10-12 genuine samples).'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：GPDS数据集上选定签名识别模型的报告EER（使用10-12个真实样本）。
- en: \csvreader
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \csvreader
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
- en: ']csv/signature_eer.csv\csvlinetotablerow'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ']csv/signature_eer.csv\csvlinetotablerow'
- en: 'Table 9: Accuracy reported by some signature recognition models.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：一些签名识别模型报告的准确率。
- en: \csvreader
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: \csvreader
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
- en: ']csv/signature.csv\csvlinetotablerow'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ']csv/signature.csv\csvlinetotablerow'
- en: 'Performance of Models for Gait Recognition: Likely due to the different configurations
    of the existing gait datasets, it is difficult to compare the deep learning-based
    gait recognition works. The results are reported in the form of accuracies and
    EER across different gallery view angles and cross-view settings. For Gait recognition,
    it is common to compare rank-5 statistics as well as the normal rank-1 ones. We
    have gathered some of the averaged accuracy results reported in [sundararajan2018deep](#bib.bib270)
    in Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey"). Note that results using CASIA-B are
    collected in various scene and viewing conditions, while, using OU-ISIR, they
    are for cross-view conditions.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '步态识别模型的性能：由于现有步态数据集的配置不同，比较基于深度学习的步态识别工作变得困难。结果以不同画廊视角和跨视角设置下的准确率和等错误率（EER）形式报告。对于步态识别，通常比较排名前5的统计数据以及普通的排名前1的统计数据。我们在表格[4.1](#S4.SS1
    "4.1 Popular Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance
    of Different Models on Different Datasets ‣ Biometrics Recognition Using Deep
    Learning: A Survey")中汇总了[sundararajan2018deep](#bib.bib270)报告的一些平均准确率结果。注意，使用CASIA-B的数据是在不同场景和视角条件下收集的，而使用OU-ISIR的数据则是针对跨视角条件的。'
- en: 'Table 10: Accuracy of select gait recognition models.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：选择的步态识别模型的准确率。
- en: \csvreader
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: \csvreader
- en: '[tabular=—c—c—c—c—, no head,column count=4, table head=, late after line='
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[tabular=—c—c—c—c—, no head,column count=4, table head=, late after line='
- en: ']csv/gait.csv\csvlinetotablerow'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ']csv/gait.csv\csvlinetotablerow'
- en: 5 Challenges and Opportunities
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 挑战与机遇
- en: Biometric recognition systems have undergone great progress with the help of
    deep learning-based models, in the past few years, but there are still several
    challenges ahead which may be tackled in the few years and decades.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，借助基于深度学习的模型，生物特征识别系统取得了巨大的进展，但仍然面临一些挑战，这些挑战可能在未来几年或几十年内得到解决。
- en: 5.1 More Challenging Datasets
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 更具挑战性的数据集
- en: Although some of the current biometric recognition datasets (such as MegaFace,
    MS-Celeb-1M) contain a very large number of candidates, they are still far from
    representing all the real-world scenarios. Although state-of-the-art algorithms
    can achieve accuracy rates of over 99.9% on LFW and Megaface databases, fundamental
    challenges such as matching faces/biometrics across ages, different poses, partial-data,
    different sensor types still remain challenging. Also the number of subjects/people
    in real-world scenarios should be in the order of tens of millions. Therefore
    biometrics dataset which contain a much larger number of classes (10M-100M), as
    well as a lot more intra-class variations, would be another big step towards supporting
    all real-world conditions.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些当前的生物特征识别数据集（如MegaFace、MS-Celeb-1M）包含了非常大量的候选者，但它们仍然远未涵盖所有真实世界场景。尽管最先进的算法可以在LFW和Megaface数据库上实现超过99.9%的准确率，但匹配不同年龄、不同姿势、部分数据、不同传感器类型的面孔/生物特征等基本挑战仍然存在。同时，现实世界场景中的受试者/人员数量应在数千万级别。因此，包含更多类别（10M-100M）以及更多类别内变异的生物特征数据集将是支持所有真实世界条件的又一大步。
- en: 5.2 Interpretable Deep Models
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 可解释的深度模型
- en: It is true that deep learning-based models achieved an astonishing performance
    on many of the challenging benchmarks, but there are still several open questions
    about these models. For example, what exactly are deep learning models learning?
    Why are these models easily fooled by adversarial examples (while human can detect
    many of those examples easily)? What is a minimal neural architecture which can
    achieve a certain recognition accuracy on a given dataset?
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，基于深度学习的模型在许多具有挑战性的基准测试中表现出惊人的性能，但这些模型仍然存在几个未解的问题。例如，深度学习模型究竟在学习什么？为什么这些模型容易被对抗样本欺骗（而人类可以轻松检测到许多这些样本）？在给定数据集上，能达到某种识别准确率的最小神经网络架构是什么？
- en: 5.3 Few Shot Learning, and Self-Supervised Learning
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 少样本学习与自监督学习
- en: Many of the successful models developed for biometric recognition are trained
    on large datasets with enough samples for each class. One of the interesting future
    trend will be to develop recognition models which can learn a powerful models
    from very few shots (zero/one shot in extreme case). This would enable training
    discriminative models without the need to provide several samples for each person/identity.
    Self-supervised learning [selfsuper](#bib.bib279) is also another recent popular
    topic in deep learning, which has not been explored enough for biometrics recognition.
    One way to use it would be to learn discriminative biometric feature from local
    patches of an image, and then aggregating those features and used for classification.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 许多成功的生物识别模型是在具有足够每类样本的大型数据集上训练的。一个有趣的未来趋势将是开发可以从极少量样本（在极端情况下为零/一个样本）中学习强大模型的识别模型。这将使得训练判别模型不需要为每个人/身份提供多个样本。自监督学习
    [selfsuper](#bib.bib279) 也是深度学习中的一个新兴热门话题，在生物识别中尚未得到充分探索。一种应用方法是从图像的局部区域学习判别生物识别特征，然后汇总这些特征用于分类。
- en: 5.4 Biometric Fusion
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 生物识别融合
- en: Single biometric recognition by itself is far from sufficient to solve all biometric/forensic
    tasks. For example distinguishing identical twins may not be possible from face
    only, or matching an identity from face with disguise, or after surgery may not
    be that easy. Fusing the information from multiple biometrics can provide a more
    reliable solution/system in many of these cases (for example using voice+face
    or voice+gait can potentially solve the identical twin detection) [ross2004multimodal](#bib.bib280)
    ; [ross2003information](#bib.bib281) . A good neural architecture which can jointly
    encode and aggregate different biometrics would be an interesting problem (information
    fusion can happen at the data level, feature level, score level, or decision level).
    Image set classification could also be useful in this direction [imageset](#bib.bib282)
    . There have been some works on biometric fusion, but most of them are far from
    the real-world scale of biometric recognition, and are mostly in their infancy.
    For some of the challenges of multi-modal machine learning, we refer the reader
    to [multimodal_challenge](#bib.bib54) .
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 单一生物识别技术本身远不足以解决所有生物识别/司法任务。例如，仅通过面部特征区分同卵双胞胎可能不可行，或者从面部识别伪装下的身份，或在手术后识别身份可能不会那么容易。融合来自多种生物特征的信息可以在许多情况下提供更可靠的解决方案/系统（例如，使用声音+面部或声音+步态可能有助于解决同卵双胞胎的检测）
    [ross2004multimodal](#bib.bib280) ; [ross2003information](#bib.bib281) 。一个能够共同编码和聚合不同生物特征的良好神经架构将是一个有趣的问题（信息融合可以在数据级、特征级、分数级或决策级发生）。图像集分类在这方面也可能会有用
    [imageset](#bib.bib282) 。虽然已有一些关于生物识别融合的研究，但大多数仍远未达到现实世界生物识别的规模，并且大多处于起步阶段。关于多模态机器学习的一些挑战，我们建议读者参考
    [multimodal_challenge](#bib.bib54) 。
- en: 5.5 Realtime Models for Various Applications
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 各种应用的实时模型
- en: In many applications, accuracy is the most important factor; however, there
    are many applications in which it is also crucial to have a near real-time biometric
    recognition model. This could be very useful for on-device solutions, such as
    the one for cellphone and tablet authentication. Some of the current deep models
    for biometrics recognition are far from this speed requirement, and developing
    near real-time models yet accurate models would be very valuable.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，准确性是最重要的因素；然而，在许多应用中，拥有接近实时的生物识别模型也是至关重要的。这对于设备上的解决方案，如手机和平板电脑身份验证，将非常有用。目前的一些深度模型对于生物识别的速度要求仍相差甚远，因此开发接近实时且准确的模型将具有很高的价值。
- en: 5.6 Memory Efficient Models
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 内存高效模型
- en: Many of the deep learning-based models require a significant amount of memory
    even during inference. So far, most of the effort has focused on improving the
    accuracy of these models, but in order to fit these models in devices, the networks
    must be simplified. This can be done either by using a simpler model, using model
    compression techniques, or training a complex model and then using knowledge distillation
    techniques to compress that into a smaller network mimicking the initial complex
    model. Having a memory-efficient model opens up the door for these models to be
    used even on consumer devices.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于深度学习的模型即使在推理过程中也需要大量的内存。目前，大多数努力集中在提高这些模型的准确性上，但为了使这些模型适应设备，网络必须被简化。这可以通过使用更简单的模型、使用模型压缩技术，或训练复杂模型然后使用知识蒸馏技术将其压缩成一个较小的网络来模仿初始复杂模型来实现。拥有一个内存高效的模型将使这些模型能够在消费者设备上使用。
- en: 5.7 Security and Privacy Issues
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 安全性和隐私问题
- en: Security is of great importance in biometric recognition systems. Presentation
    attack, template attack, and adversarial attack threaten the security of deep
    biometric recognition systems, and challenge the current anti-spoofing methods.
    Although some attempts have been done for adversarial example detection, there
    is still a long way to robust/reliable anti-spoofing capabilities.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物识别系统中，安全性至关重要。呈现攻击、模板攻击和对抗攻击威胁深度生物识别系统的安全性，并挑战当前的防伪方法。尽管已经有一些对抗样本检测的尝试，但在建立稳健/可靠的防伪能力方面仍有很长的路要走。
- en: With the leakage of biological/biometrics data nowadays, privacy concerns are
    rising. Some information about the user identity/age/gender can be decoded from
    the neural feature representation of their images. Research on visual cryptography,
    to protect users’ privacy on stored biometrics templates are essential for addressing
    public concern on privacy.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生物/生物特征数据的泄露，隐私问题越来越引起关注。用户身份/年龄/性别的一些信息可以从其图像的神经特征表示中解码出来。研究视觉密码学，以保护存储的生物特征模板的用户隐私，对于应对公众对隐私的关注至关重要。
- en: 6 Conclusions
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we provided a summary of the recent deep learning-based models
    (till 2019) for biometric recognition. As opposed to the other surveys, it provides
    an overview of most used biometrics. Deep neural models have shown promising improvement
    over classical models for various biometrics. Some biometrics have attracted a
    lot more attention (such as face) due to the wider industrial applications, and
    availability of large-scale datasets, but other biometrics seem to be following
    the same trend. Although deep learning research in biometrics has achieved promising
    results, there is still a great room for improvement in different directions,
    such as creating larger and more challenging datasets, addressing model interpretation,
    fusing multiple biometrics, and addressing security and privacy issues.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们总结了最近基于深度学习的生物识别模型（截至2019年）。与其他调查不同，它提供了最常用生物识别技术的概述。深度神经模型在各种生物识别领域相较于经典模型表现出了有希望的改进。一些生物识别技术由于更广泛的工业应用和大规模数据集的可用性（如面部识别）受到更多关注，但其他生物识别技术似乎也在跟随相同的趋势。尽管深度学习在生物识别中的研究取得了有希望的结果，但在不同方向上仍有很大的改进空间，例如创建更大且更具挑战性的数据集，解决模型解释问题，融合多种生物识别技术，以及解决安全性和隐私问题。
- en: Acknowledgements.
  id: totrans-374
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: We would like to thank Prof. Rama Chellappa, and Dr. Nalini Ratha for reviewing
    this work, and providing very helpful comments and suggestions.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢Rama Chellappa教授和Nalini Ratha博士审阅这项工作，并提供了非常有用的评论和建议。
- en: References
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Anil Jain, Lin Hong, and Sharath Pankanti. Biometric identification. Communications
    of the ACM, 43(2):90–98, 2000.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Anil Jain, Lin Hong, 和 Sharath Pankanti. 生物识别识别。ACM通讯, 43(2):90–98, 2000.'
- en: '[2] David Zhang. Automated biometrics: Technologies and systems, volume 7.
    Springer Science & Business Media, 2000.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] David Zhang. Automated biometrics: Technologies and systems, volume 7.
    Springer Science & Business Media, 2000.'
- en: '[3] David Zhang, Guangming Lu, and Lei Zhang. Advanced biometrics. Springer,
    2018.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] David Zhang, Guangming Lu, 和 Lei Zhang. 高级生物识别技术。Springer, 2018.'
- en: '[4] Javier Galbally, Raffaele Cappelli, Alessandra Lumini, Davide Maltoni,
    and Julian Fierrez. Fake fingertip generation from a minutiae template. In International
    Conference on Pattern Recognition. IEEE, 2008.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Javier Galbally, Raffaele Cappelli, Alessandra Lumini, Davide Maltoni,
    和 Julian Fierrez. 从特征模板生成假指尖。在国际模式识别会议上。IEEE, 2008.'
- en: '[5] Sefik Eskimez, Ross K Maddox, Chenliang Xu, and Zhiyao Duan. Generating
    talking face landmarks from speech. In Conference on Latent Variable Analysis
    and Signal Separation. Springer, 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Sefik Eskimez, Ross K Maddox, Chenliang Xu 和 Zhiyao Duan. 从语音生成说话面部标志。第十二届潜变量分析与信号分离会议。Springer,
    2018。'
- en: '[6] https://deepfakedetectionchallenge.ai/.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] https://deepfakedetectionchallenge.ai/。'
- en: '[7] Huaxiao Mo, Bolin Chen, and Weiqi Luo. Fake faces identification via convolutional
    neural network. In Information Hiding and Multimedia Security. ACM, 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Huaxiao Mo, Bolin Chen 和 Weiqi Luo. 通过卷积神经网络识别虚假面孔。在信息隐藏与多媒体安全会议。ACM, 2018。'
- en: '[8] Yuezun Li and Siwei Lyu. Exposing deepfake videos by detecting face warping
    artifacts. arXiv preprint arXiv:1811.00656, 2, 2018.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Yuezun Li 和 Siwei Lyu. 通过检测面部变形伪影来揭露深度伪造视频。arXiv预印本 arXiv:1811.00656, 2,
    2018。'
- en: '[9] Anil K Jain, Arun Ross, Salil Prabhakar, et al. An introduction to biometric
    recognition. IEEE Transactions on circuits and systems for video technology, 14(1),
    2004.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Anil K Jain, Arun Ross, Salil Prabhakar 等. 生物识别识别简介。IEEE视频技术电路与系统汇刊, 14(1),
    2004。'
- en: '[10] Guangming Lu, David Zhang, and Kuanquan Wang. Palmprint recognition using
    eigenpalms features. Pattern Recognition Letters, 24(9-10):1463–1467, 2003.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Guangming Lu, David Zhang 和 Kuanquan Wang. 使用特征值掌纹进行掌纹识别。模式识别通讯, 24(9-10):1463–1467,
    2003。'
- en: '[11] Dapeng Zhang and Wei Shu. Two novel characteristics in palmprint verification:
    datum point invariance and line feature matching. Pattern recognition, 32(4):691–702,
    1999.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Dapeng Zhang 和 Wei Shu. 掌纹验证中的两个新特征：数据点不变性和线条特征匹配。模式识别, 32(4):691–702,
    1999。'
- en: '[12] Wenyi Zhao, Rama Chellappa, P Jonathon Phillips, and Azriel Rosenfeld.
    Face recognition: A literature survey. ACM computing surveys (CSUR), 35(4):399–458,
    2003.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Wenyi Zhao, Rama Chellappa, P Jonathon Phillips 和 Azriel Rosenfeld. 面部识别：文献综述。ACM计算机调查
    (CSUR), 35(4):399–458, 2003。'
- en: '[13] Zhichun Mu, Li Yuan, Zhengguang Xu, Dechun Xi, and Shuai Qi. Shape and
    structural feature based ear recognition. In Chinese Conference on Biometric Recognition,
    pages 663–670. Springer, 2004.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Zhichun Mu, Li Yuan, Zhengguang Xu, Dechun Xi 和 Shuai Qi. 基于形状和结构特征的耳朵识别。在中国生物识别识别会议,
    页663–670。Springer, 2004。'
- en: '[14] John Daugman. How iris recognition works. In The essential guide to image
    processing, pages 715–739. Elsevier, 2009.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] John Daugman. 虹膜识别的工作原理。在图像处理基础指南, 页715–739。Elsevier, 2009。'
- en: '[15] Kevin W Bowyer and Mark J Burge. Handbook of iris recognition. Springer,
    2016.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Kevin W Bowyer 和 Mark J Burge. 虹膜识别手册。Springer, 2016。'
- en: '[16] Halvor Borgen, Patrick Bours, and Stephen D Wolthusen. Visible-spectrum
    biometric retina recognition. In Conference on Intelligent Information Hiding
    and Multimedia Signal Processing. IEEE, 2008.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Halvor Borgen, Patrick Bours 和 Stephen D Wolthusen. 可见光谱生物识别视网膜识别。在智能信息隐藏与多媒体信号处理会议。IEEE,
    2008。'
- en: '[17] Mohamed Elhoseny, Amir Nabil, Aboul Hassanien, and Diego Oliva. Hybrid
    rough neural network model for signature recognition. In Advances in Soft Computing
    and Machine Learning in Image Processing, pages 295–318\. Springer, 2018.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Mohamed Elhoseny, Amir Nabil, Aboul Hassanien 和 Diego Oliva. 用于签名识别的混合粗糙神经网络模型。在图像处理中的软计算与机器学习进展,
    页295–318。Springer, 2018。'
- en: '[18] Jin Wang, Mary She, Saeid Nahavandi, and Abbas Kouzani. A review of vision-based
    gait recognition methods for human identification. In 2010 international conference
    on digital image computing: techniques and applications, pages 320–327\. IEEE,
    2010.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Jin Wang, Mary She, Saeid Nahavandi 和 Abbas Kouzani. 基于视觉的步态识别方法综述。在2010年国际数字图像计算：技术与应用会议,
    页320–327。IEEE, 2010。'
- en: '[19] Fabian Monrose and Aviel D Rubin. Keystroke dynamics as a biometric for
    authentication. Future Generation computer systems, 16(4):351–359, 2000.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Fabian Monrose 和 Aviel D Rubin. 键盘动态作为生物识别认证的方法。未来一代计算机系统, 16(4):351–359,
    2000。'
- en: '[20] Joseph P Campbell. Speaker recognition: A tutorial. Proceedings of the
    IEEE, 85(9):1437–1462, 1997.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Joseph P Campbell. 说话人识别：教程。IEEE会议录, 85(9):1437–1462, 1997。'
- en: '[21] Mark Hawthorne. Fingerprints: analysis and understanding. CRC Press, 2017.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Mark Hawthorne. 指纹：分析与理解。CRC出版社, 2017。'
- en: '[22] Anil K Jain and Stan Z Li. Handbook of face recognition. Springer, 2011.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Anil K Jain 和 Stan Z Li. 面部识别手册。Springer, 2011。'
- en: '[23] Yulan Guo, Yinjie Lei, Li Liu, Yan Wang, Mohammed Bennamoun, and Ferdous
    Sohel. Ei3d: Expression-invariant 3d face recognition based on feature and shape
    matching. Pattern Recognition Letters, 83, 2016.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yulan Guo, Yinjie Lei, Li Liu, Yan Wang, Mohammed Bennamoun 和 Ferdous
    Sohel. Ei3d: 基于特征和形状匹配的表情不变3D面部识别。模式识别通讯, 83, 2016。'
- en: '[24] Unsang Park, Yiying Tong, and Anil K Jain. Age-invariant face recognition.
    IEEE transactions on pattern analysis and machine intelligence, 32(5), 2010.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Unsang Park, Yiying Tong, 和 Anil K Jain. 年龄不变的面部识别. IEEE 模式分析与机器智能汇刊,
    32(5), 2010.'
- en: '[25] Anil Jain, Lin Hong, and Ruud Bolle. On-line fingerprint verification.
    IEEE transactions on pattern analysis and machine intelligence, 19(4):302–314,
    1997.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Anil Jain, Lin Hong, 和 Ruud Bolle. 在线指纹验证. IEEE 模式分析与机器智能汇刊, 19(4):302–314,
    1997.'
- en: '[26] David Zhang, Zhenhua Guo, and Yazhuo Gong. Multispectral Biometrics: Systems
    and Applications. Springer, 2015.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] David Zhang, Zhenhua Guo, 和 Yazhuo Gong. 多光谱生物特征识别：系统与应用. Springer, 2015.'
- en: '[27] Maria De Marsico, Alfredo Petrosino, and Stefano Ricciardi. Iris recognition
    through machine learning techniques: A survey. Pattern Recognition Letters, 82:106–115,
    2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Maria De Marsico, Alfredo Petrosino, 和 Stefano Ricciardi. 通过机器学习技术的虹膜识别：综述.
    模式识别信件, 82:106–115, 2016.'
- en: '[28] Žiga Emeršič, Vitomir Štruc, and Peter Peer. Ear recognition: More than
    a survey. Neurocomputing, 255:26–39, 2017.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Žiga Emeršič, Vitomir Štruc, 和 Peter Peer. 耳部识别：不仅仅是综述. 神经计算, 255:26–39,
    2017.'
- en: '[29] Javier Galbally, Moises Diaz-Cabrera, Miguel A Ferrer, Marta Gomez-Barrero,
    Aythami Morales, and Julian Fierrez. On-line signature recognition through the
    combination of real dynamic data and synthetically generated static data. Pattern
    Recognition, 48(9), 2015.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Javier Galbally, Moises Diaz-Cabrera, Miguel A Ferrer, Marta Gomez-Barrero,
    Aythami Morales, 和 Julian Fierrez. 通过结合真实动态数据和合成生成的静态数据进行在线签名识别. 模式识别, 48(9),
    2015.'
- en: '[30] Liang Wang, Huazhong Ning, Tieniu Tan, and Weiming Hu. Fusion of static
    and dynamic body biometrics for gait recognition. IEEE Transactions on circuits
    and systems for video technology, 14(2):149–158, 2004.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Liang Wang, Huazhong Ning, Tieniu Tan, 和 Weiming Hu. 静态和动态身体生物特征融合用于步态识别.
    IEEE 视频技术电路与系统汇刊, 14(2):149–158, 2004.'
- en: '[31] Liang Wang, Tieniu Tan, Huazhong Ning, and Weiming Hu. Silhouette analysis-based
    gait recognition for human identification. IEEE transactions on pattern analysis
    and machine intelligence, 25(12):1505–1518, 2003.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Liang Wang, Tieniu Tan, Huazhong Ning, 和 Weiming Hu. 基于轮廓分析的人体步态识别. IEEE
    模式分析与机器智能汇刊, 25(12):1505–1518, 2003.'
- en: '[32] Extended yale face database b (b+). http://vision.ucsd.edu/content/extended-yale-face-database-b-b.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 扩展的耶鲁面部数据库 B (B+). http://vision.ucsd.edu/content/extended-yale-face-database-b-b.'
- en: '[33] Polyu fingerprint dataset. http://www4.comp.polyu.edu.hk/~biometrics/HRF/HRF_old.htm.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Polyu 指纹数据集. http://www4.comp.polyu.edu.hk/~biometrics/HRF/HRF_old.htm.'
- en: '[34] Polyu palmprint dataset. https://www4.comp.polyu.edu.hk/~biometrics/MultispectralPalmprint/MSP.htm.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Polyu palmprint 数据集. https://www4.comp.polyu.edu.hk/~biometrics/MultispectralPalmprint/MSP.htm.'
- en: '[35] Ajay Kumar and Arun Passi. Comparison and combination of iris matchers
    for reliable personal authentication. Pattern recognition, 43(3):1016–1026, 2010.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ajay Kumar 和 Arun Passi. 虹膜匹配器的比较与组合以实现可靠的个人认证. 模式识别, 43(3):1016–1026,
    2010.'
- en: '[36] Ajay Kumar and Chenye Wu. Automated human identification using ear imaging.
    Pattern Recognition, 45(3):956–968, 2012.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Ajay Kumar 和 Chenye Wu. 使用耳部成像的自动化人类识别. 模式识别, 45(3):956–968, 2012.'
- en: '[37] Timo Ahonen, Abdenour Hadid, and Matti Pietikäinen. Face recognition with
    local binary patterns. In European conference on computer vision, pages 469–481.
    Springer, 2004.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Timo Ahonen, Abdenour Hadid, 和 Matti Pietikäinen. 使用局部二值模式的面部识别. 在欧洲计算机视觉会议中,
    页码 469–481. Springer, 2004.'
- en: '[38] Andrea F Abate, Michele Nappi, Daniel Riccio, and Gabriele Sabatino. 2d
    and 3d face recognition: A survey. Pattern recognition letters, 28(14):1885–1906,
    2007.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Andrea F Abate, Michele Nappi, Daniel Riccio, 和 Gabriele Sabatino. 2D
    和 3D 面部识别：综述. 模式识别信件, 28(14):1885–1906, 2007.'
- en: '[39] David Zhang, Fengxi Song, Yong Xu, and Zhizhen Liang. Advanced pattern
    recognition technologies with applications to biometrics. IGI Global Hershey,
    PA, 2009.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] David Zhang, Fengxi Song, Yong Xu, 和 Zhizhen Liang. 先进的模式识别技术及其在生物特征识别中的应用.
    IGI Global Hershey, PA, 2009.'
- en: '[40] David G Lowe. Distinctive image features from scale-invariant keypoints.
    International journal of computer vision, 60(2):91–110, 2004.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] David G Lowe. 从尺度不变关键点中提取的独特图像特征. 国际计算机视觉杂志, 60(2):91–110, 2004.'
- en: '[41] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human
    detection. 2005.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Navneet Dalal 和 Bill Triggs. 面向人类检测的方向梯度直方图. 2005.'
- en: '[42] Wai Kin Kong, David Zhang, and Wenxin Li. Palmprint feature extraction
    using 2-d gabor filters. Pattern recognition, 36(10):2339–2347, 2003.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Wai Kin Kong, David Zhang, 和 Wenxin Li. 使用 2D Gabor 滤波器的掌纹特征提取. 模式识别,
    36(10):2339–2347, 2003.'
- en: '[43] Jian Huang Lai, Pong C Yuen, and Guo Can Feng. Face recognition using
    holistic fourier invariant features. Pattern Recognition, 34(1):95–109, 2001.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Jian Huang Lai, Pong C Yuen 和 Guo Can Feng. 使用整体傅里叶不变特征的面部识别. 模式识别, 34(1):95–109,
    2001.'
- en: '[44] Andrew Teoh Beng Jin, David Ngo Chek Ling, and Ong Thian Song. An efficient
    fingerprint verification system using integrated wavelet and fourier–mellin invariant
    transform. Image and Vision Computing, 22(6):503–513, 2004.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Andrew Teoh Beng Jin, David Ngo Chek Ling 和 Ong Thian Song. 使用集成小波和傅里叶-梅林不变变换的高效指纹验证系统.
    图像与视觉计算, 22(6):503–513, 2004.'
- en: '[45] Hervé Abdi and Lynne J Williams. Principal component analysis. Wiley interdisciplinary
    reviews: computational statistics, 2(4):433–459, 2010.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Hervé Abdi 和 Lynne J Williams. 主成分分析. Wiley 跨学科评论：计算统计, 2(4):433–459,
    2010.'
- en: '[46] Jian Yang, David Zhang, Alejandro F Frangi, and Jing-yu Yang. Two-dimensional
    pca: a new approach to appearance-based face representation and recognition. IEEE
    transactions on pattern analysis and machine intelligence, 26(1):131–137, 2004.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Jian Yang, David Zhang, Alejandro F Frangi 和 Jing-yu Yang. 二维 PCA：基于外观的面部表示与识别的新方法.
    IEEE 图像模式分析与机器智能杂志, 26(1):131–137, 2004.'
- en: '[47] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems, pages 1097–1105, 2012.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E Hinton. 使用深度卷积神经网络的 Imagenet
    分类. 载于神经信息处理系统进展, 页1097–1105, 2012.'
- en: '[48] Aleksei Grigorevich Ivakhnenko and Valentin Grigorevich Lapa. Cybernetic
    predicting devices. Technical report, Purdue Univ, School of Electrical Engineering,
    1966.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Aleksei Grigorevich Ivakhnenko 和 Valentin Grigorevich Lapa. 控制预测装置. 技术报告,
    普渡大学, 电气工程学院, 1966.'
- en: '[49] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural
    networks, 61:85–117, 2015.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Jürgen Schmidhuber. 神经网络中的深度学习：概述. 神经网络, 61:85–117, 2015.'
- en: '[50] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436, 2015.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Yann LeCun, Yoshua Bengio 和 Geoffrey Hinton. 深度学习. 自然, 521(7553):436,
    2015.'
- en: '[51] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning
    representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams 等. 通过反向传播错误来学习表示.
    认知建模, 5(3):1, 1988.'
- en: '[52] Léon Bottou. Stochastic gradient learning in neural networks. Proceedings
    of Neuro-Nımes, 91(8):12, 1991.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Léon Bottou. 神经网络中的随机梯度学习. Neuro-Nîmes 会议录, 91(8):12, 1991.'
- en: '[53] Mei Wang and Weihong Deng. Deep face recognition: A survey. arXiv preprint
    arXiv:1804.06655, 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Mei Wang 和 Weihong Deng. 深度人脸识别：综述. arXiv 预印本 arXiv:1804.06655, 2018.'
- en: '[54] Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal
    machine learning: A survey and taxonomy. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 41(2), 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Tadas Baltrusaitis, Chaitanya Ahuja 和 Louis-Philippe Morency. 多模态机器学习：综述与分类.
    IEEE 图像模式分析与机器智能杂志, 41(2), 2018.'
- en: '[55] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based
    learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324,
    1998.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner 等. 基于梯度的学习在文档识别中的应用.
    IEEE 会议录, 86(11):2278–2324, 1998.'
- en: '[56] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Sepp Hochreiter 和 Jürgen Schmidhuber. 长短期记忆. 神经计算, 9(8):1735–1780, 1997.'
- en: '[57] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Advances in neural information processing systems, pages 2672–2680, 2014.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville 和 Yoshua Bengio. 生成对抗网络. 载于神经信息处理系统进展, 页2672–2680,
    2014.'
- en: '[58] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model
    for a mechanism of pattern recognition unaffected by shift in position. Biological
    cybernetics, 36(4):193–202, 1980.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Kunihiko Fukushima. Neocognitron：一种自组织神经网络模型，用于不受位置偏移影响的模式识别机制. 生物计算机学,
    36(4):193–202, 1980.'
- en: '[59] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Jonathan Long, Evan Shelhamer 和 Trevor Darrell. 用于语义分割的全卷积网络. 载于 IEEE
    计算机视觉与模式识别会议论文集, 页3431–3440, 2015.'
- en: '[60] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional
    networks for biomedical image segmentation. In International Conference on Medical
    image computing and computer-assisted intervention, pages 234–241\. Springer,
    2015.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Olaf Ronneberger, Philipp Fischer, 和 Thomas Brox. U-net: 用于生物医学图像分割的卷积网络。在医学图像计算与计算机辅助手术国际会议，页234–241。Springer，2015年。'
- en: '[61] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards
    real-time object detection with region proposal networks. In Advances in neural
    information processing systems, pages 91–99, 2015.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun. Faster r-cnn: 向实时物体检测迈进，采用区域提议网络。神经信息处理系统进展，页91–99，2015年。'
- en: '[62] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep
    convolutional network for image super-resolution. In European conference on computer
    vision, pages 184–199. Springer, 2014.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Chao Dong, Chen Change Loy, Kaiming He, 和 Xiaoou Tang. 学习深度卷积网络用于图像超分辨率。在欧洲计算机视觉会议，页184–199。Springer，2014年。'
- en: '[63] Kin Gwn Lore, Adedotun Akintayo, and Soumik Sarkar. Llnet: A deep autoencoder
    approach to natural low-light image enhancement. Pattern Recognition, 61:650–662,
    2017.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Kin Gwn Lore, Adedotun Akintayo, 和 Soumik Sarkar. Llnet: 一种深度自编码器方法用于自然低光图像增强。模式识别，61:650–662，2017年。'
- en: '[64] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image
    captioning with semantic attention. In IEEE conference on computer vision and
    pattern recognition, pages 4651–4659, 2016.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, 和 Jiebo Luo. 带有语义注意的图像描述。在IEEE计算机视觉与模式识别会议，页4651–4659，2016年。'
- en: '[65] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional
    networks. In European conference on computer vision. Springer, 2014.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Matthew D Zeiler 和 Rob Fergus. 可视化和理解卷积网络。在欧洲计算机视觉会议。Springer，2014年。'
- en: '[66] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks
    for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Karen Simonyan 和 Andrew Zisserman. 用于大规模图像识别的深度卷积网络。arXiv预印本 arXiv:1409.1556，2014年。'
- en: '[67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 图像识别的深度残差学习。在IEEE计算机视觉与模式识别会议论文集，页770–778，2016年。'
- en: '[68] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In IEEE conference on computer vision and pattern recognition,
    2015.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, 和 Andrew Rabinovich. 通过卷积网络深入探索。在IEEE计算机视觉与模式识别会议，2015年。'
- en: '[69] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional
    neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861,
    2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, 和 Hartwig Adam. Mobilenets: 用于移动视觉应用的高效卷积神经网络。arXiv预印本
    arXiv:1704.04861，2017年。'
- en: '[70] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
    Densely connected convolutional networks. In IEEE conference on computer vision
    and pattern recognition, pages 4700–4708, 2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, 和 Kilian Q Weinberger.
    密集连接卷积网络。在IEEE计算机视觉与模式识别会议，页4700–4708，2017年。'
- en: '[71] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning
    representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, 等. 通过反向传播错误来学习表示。认知建模，5(3):1，1988年。'
- en: '[72] http://colah.github.io/posts/2015-08-Understanding-LSTMs/.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。'
- en: '[73] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre
    Manzagol. Stacked denoising autoencoders: Learning useful representations in a
    deep network with a local denoising criterion. Journal of machine learning research,
    11(Dec):3371–3408, 2010.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, 和 Pierre
    Manzagol. 堆叠去噪自编码器：通过局部去噪准则在深度网络中学习有用的表示。机器学习研究期刊，11(12):3371–3408，2010年。'
- en: '[74] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv
    preprint arXiv:1312.6114, 2013.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Diederik P Kingma 和 Max Welling. 自编码变分贝叶斯。arXiv预印本 arXiv:1312.6114，2013年。'
- en: '[75] https://github.com/hindupuravinash/the-gan-zoo.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)。'
- en: '[76] Weihong Deng, Jiani Hu, and Jun Guo. In defense of sparsity based face
    recognition. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pages 399–406, 2013.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Weihong Deng、Jiani Hu 和 Jun Guo。在稀疏性基础面部识别的防御。发表于 IEEE 计算机视觉与模式识别会议论文集，页码
    399–406，2013 年。'
- en: '[77] Qiong Cao, Yiming Ying, and Peng Li. Similarity metric learning for face
    recognition. In Proceedings of the IEEE International Conference on Computer Vision,
    pages 2408–2415, 2013.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Qiong Cao、Yiming Ying 和 Peng Li。面部识别的相似度度量学习。发表于 IEEE 国际计算机视觉会议论文集，页码
    2408–2415，2013 年。'
- en: '[78] John Wright, Allen Y Yang, Arvind Ganesh, S Shankar Sastry, and Yi Ma.
    Robust face recognition via sparse representation. IEEE transactions on pattern
    analysis and machine intelligence, 31(2):210–227, 2008.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] John Wright、Allen Y Yang、Arvind Ganesh、S Shankar Sastry 和 Yi Ma。通过稀疏表示实现鲁棒的面部识别。IEEE
    模式分析与机器智能汇刊，31(2):210–227，2008 年。'
- en: '[79] Meng Yang, Lei Zhang, Jian Yang, and David Zhang. Regularized robust coding
    for face recognition. IEEE transactions on image processing, 22(5), 2012.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Meng Yang、Lei Zhang、Jian Yang 和 David Zhang。面部识别的正则化鲁棒编码。IEEE 图像处理汇刊，22(5)，2012
    年。'
- en: '[80] Dong Yi, Zhen Lei, and Stan Z Li. Towards pose robust face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 3539–3545, 2013.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Dong Yi、Zhen Lei 和 Stan Z Li。面向姿态鲁棒的面部识别。发表于 IEEE 计算机视觉与模式识别会议论文集，页码 3539–3545，2013
    年。'
- en: '[81] Ajmal Mian, Mohammed Bennamoun, and Robyn Owens. An efficient multimodal
    2d-3d hybrid approach to automatic face recognition. IEEE transactions on pattern
    analysis and machine intelligence, 29(11), 2007.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Ajmal Mian、Mohammed Bennamoun 和 Robyn Owens。一种高效的多模态 2D-3D 混合自动面部识别方法。IEEE
    模式分析与机器智能汇刊，29(11)，2007 年。'
- en: '[82] Yale face database. http://vision.ucsd.edu/content/yale-face-database.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] 耶鲁面孔数据库。 http://vision.ucsd.edu/content/yale-face-database。'
- en: '[83] The cmu multi-pie face database. http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] CMU Multi-PIE 面孔数据库。 http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html。'
- en: '[84] Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, and Simon Baker.
    Multi-pie. Image and Vision Computing, 28(5):807–813, 2010.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Ralph Gross、Iain Matthews、Jeffrey Cohn、Takeo Kanade 和 Simon Baker。Multi-PIE。图像与视觉计算，28(5):807–813，2010
    年。'
- en: '[85] Labeled faces in the wild. http://vis-www.cs.umass.edu/lfw/.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] 标记的野外面孔。 http://vis-www.cs.umass.edu/lfw/。'
- en: '[86] Polyu nir face database. http://www4.comp.polyu.edu.hk/~biometrics/polyudb_face.htm.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] PolyU NIR 面孔数据库。 http://www4.comp.polyu.edu.hk/~biometrics/polyudb_face.htm。'
- en: '[87] Youtube faces db. http://www.cs.tau.ac.il/~wolf/ytfaces/.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Youtube 面孔数据库。 http://www.cs.tau.ac.il/~wolf/ytfaces/。'
- en: '[88] Vggface2. http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Vggface2。 http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/。'
- en: '[89] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation
    from scratch. arXiv preprint arXiv:1411.7923, 2014.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Dong Yi、Zhen Lei、Shengcai Liao 和 Stan Z Li。从头开始学习面部表示。arXiv 预印本 arXiv:1411.7923，2014
    年。'
- en: '[90] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m:
    A dataset and benchmark for large-scale face recognition. In European Conference
    on Computer Vision, pages 87–102. Springer, 2016.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Yandong Guo、Lei Zhang、Yuxiao Hu、Xiaodong He 和 Jianfeng Gao。MS-Celeb-1M：大规模面部识别的数据集和基准。发表于欧洲计算机视觉会议，页码
    87–102。Springer，2016 年。'
- en: '[91] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face
    attributes in the wild. In Proceedings of the IEEE international conference on
    computer vision, pages 3730–3738, 2015.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Ziwei Liu、Ping Luo、Xiaogang Wang 和 Xiaoou Tang。深度学习野外面部属性。发表于 IEEE 国际计算机视觉会议论文集，页码
    3730–3738，2015 年。'
- en: '[92] Brianna Maze, Jocelyn Adams, James A Duncan, Nathan Kalka, Tim Miller,
    Charles Otto, Anil K Jain, W Tyler Niggel, Janet Anderson, Jordan Cheney, et al.
    Iarpa janus benchmark-c: Face dataset and protocol. In 2018 International Conference
    on Biometrics (ICB), pages 158–165\. IEEE, 2018.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Brianna Maze、Jocelyn Adams、James A Duncan、Nathan Kalka、Tim Miller、Charles
    Otto、Anil K Jain、W Tyler Niggel、Janet Anderson、Jordan Cheney 等。IARPA Janus Benchmark-C：面部数据集和协议。发表于
    2018 年国际生物特征识别大会（ICB），页码 158–165。IEEE，2018 年。'
- en: '[93] Ira Kemelmacher-Shlizerman, Steven Seitz, D Miller, and Evan Brossard.
    The megaface benchmark: 1 million faces for recognition at scale. In IEEE Conference
    on Computer Vision and Pattern Recognition, 2016.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Ira Kemelmacher-Shlizerman、Steven Seitz、D Miller 和 Evan Brossard。Megaface
    基准测试：用于大规模识别的 100 万张面孔。发表于 IEEE 计算机视觉与模式识别会议，2016 年。'
- en: '[94] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl
    Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia
    research. arXiv preprint arXiv:1503.01817, 2015.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Bart Thomee、David A Shamma、Gerald Friedland、Benjamin Elizalde、Karl Ni、Douglas
    Poland、Damian Borth 和 Li-Jia Li。YFCC100M：多媒体研究中的新数据。arXiv 预印本 arXiv:1503.01817，2015
    年。'
- en: '[95] Vineet Kushwaha, Maneet Singh, Richa Singh, Mayank Vatsa, Nalini Ratha,
    and Rama Chellappa. Disguised faces in the wild. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, pages 1–9, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Vineet Kushwaha、Maneet Singh、Richa Singh、Mayank Vatsa、Nalini Ratha 和 Rama
    Chellappa. 野外伪装面孔。在 IEEE 计算机视觉与模式识别会议研讨会论文集, 页码 1–9, 2018。'
- en: '[96] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface:
    Closing the gap to human-level performance in face verification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 1701–1708,
    2014.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Yaniv Taigman、Ming Yang、Marc’Aurelio Ranzato 和 Lior Wolf. Deepface: 缩小面部验证中的人类水平性能差距。在
    IEEE 计算机视觉与模式识别会议论文集, 页码 1701–1708, 2014。'
- en: '[97] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation
    from predicting 10,000 classes. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 1891–1898, 2014.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Yi Sun、Xiaogang Wang 和 Xiaoou Tang. 从预测 10,000 个类别中进行深度学习面部表示。在 IEEE 计算机视觉与模式识别会议论文集,
    页码 1891–1898, 2014。'
- en: '[98] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face
    representation by joint identification-verification. In Advances in neural information
    processing systems, pages 1988–1996, 2014.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Yi Sun、Yuheng Chen、Xiaogang Wang 和 Xiaoou Tang. 通过联合识别-验证进行深度学习面部表示。在神经信息处理系统进展,
    页码 1988–1996, 2014。'
- en: '[99] Yi Sun, Ding Liang, Xiaogang Wang, and Xiaoou Tang. Deepid3: Face recognition
    with very deep neural networks. arXiv preprint arXiv:1502.00873, 2015.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Yi Sun、Ding Liang、Xiaogang Wang 和 Xiaoou Tang. Deepid3: 用于面部识别的非常深的神经网络。arXiv
    预印本 arXiv:1502.00873, 2015。'
- en: '[100] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified
    embedding for face recognition and clustering. In IEEE conference on computer
    vision and pattern recognition, pages 815–823, 2015.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Florian Schroff、Dmitry Kalenichenko 和 James Philbin. Facenet: 面部识别与聚类的统一嵌入。在
    IEEE 计算机视觉与模式识别会议, 页码 815–823, 2015。'
- en: '[101] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. Deep face recognition.
    In bmvc, volume 1, 2015.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Omkar M Parkhi、Andrea Vedaldi、Andrew Zisserman 等。深度面部识别。在 bmvc, 第 1 卷,
    2015。'
- en: '[102] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax
    loss for convolutional neural networks. In ICML, volume 2, page 7, 2016.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Weiyang Liu、Yandong Wen、Zhiding Yu 和 Meng Yang. 用于卷积神经网络的大边际软最大损失。在 ICML,
    第 2 卷, 页码 7, 2016。'
- en: '[103] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative
    feature learning approach for deep face recognition. In European conference on
    computer vision, pages 499–515. Springer, 2016.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Yandong Wen、Kaipeng Zhang、Zhifeng Li 和 Yu Qiao. 用于深度面部识别的辨别特征学习方法。在欧洲计算机视觉会议,
    页码 499–515。Springer, 2016。'
- en: '[104] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Sparsifying neural network connections
    for face recognition. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 4856–4864, 2016.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Yi Sun、Xiaogang Wang 和 Xiaoou Tang. 稀疏化神经网络连接用于面部识别。在 IEEE 计算机视觉与模式识别会议论文集,
    页码 4856–4864, 2016。'
- en: '[105] Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, and Yu Qiao. Range
    loss for deep face recognition with long-tailed training data. In IEEE International
    Conference on Computer Vision, pages 5409–5418, 2017.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Xiao Zhang、Zhiyuan Fang、Yandong Wen、Zhifeng Li 和 Yu Qiao. 长尾训练数据的深度面部识别范围损失。在
    IEEE 国际计算机视觉会议, 页码 5409–5418, 2017。'
- en: '[106] Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-constrained
    softmax loss for discriminative face verification. arXiv preprint arXiv:1703.09507,
    2017.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Rajeev Ranjan、Carlos D Castillo 和 Rama Chellappa. 用于辨别面部验证的 L2 约束软最大损失。arXiv
    预印本 arXiv:1703.09507, 2017。'
- en: '[107] Yu Liu, Hongyang Li, and Xiaogang Wang. Rethinking feature discrimination
    and polymerization for large-scale recognition. preprint, arXiv:1710.00870, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Yu Liu、Hongyang Li 和 Xiaogang Wang. 重新思考大规模识别的特征辨别与聚合。预印本, arXiv:1710.00870,
    2017。'
- en: '[108] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song.
    Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of
    the IEEE conference on computer vision and pattern recognition, pages 212–220,
    2017.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Weiyang Liu、Yandong Wen、Zhiding Yu、Ming Li、Bhiksha Raj 和 Le Song. Sphereface:
    深度超球体嵌入用于面部识别。在 IEEE 计算机视觉与模式识别会议论文集, 页码 212–220, 2017。'
- en: '[109] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Additive margin softmax
    for face verification. IEEE Signal Processing Letters, 25(7):926–930, 2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Feng Wang、Jian Cheng、Weiyang Liu 和 Haijun Liu. 用于面部验证的加性边际软最大。IEEE 信号处理快报,
    25(7):926–930, 2018。'
- en: '[110] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou,
    Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5265–5274, 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou,
    Zhifeng Li 和 Wei Liu. Cosface: 大边距余弦损失用于深度人脸识别。发表于 IEEE 计算机视觉与模式识别会议论文集，第5265–5274页，2018年。'
- en: '[111] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface:
    Additive angular margin loss for deep face recognition. In IEEE Conference on
    Computer Vision and Pattern Recognition, 2019.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Jiankang Deng, Jia Guo, Niannan Xue 和 Stefanos Zafeiriou. Arcface: 添加角度边距损失用于深度人脸识别。发表于
    IEEE 计算机视觉与模式识别会议论文集，2019年。'
- en: '[112] Yutong Zheng, Dipan K Pal, and Marios Savvides. Ring loss: Convex feature
    normalization for face recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 5089–5097, 2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Yutong Zheng, Dipan K Pal 和 Marios Savvides. Ring loss: 用于人脸识别的凸特征归一化。发表于
    IEEE 计算机视觉与模式识别会议论文集，第5089–5097页，2018年。'
- en: '[113] Xiao Zhang, Rui Zhao, Yu Qiao, Xiaogang Wang, and Hongsheng Li. Adacos:
    Adaptively scaling cosine logits for effectively learning deep face representations.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 10823–10832, 2019.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Xiao Zhang, Rui Zhao, Yu Qiao, Xiaogang Wang 和 Hongsheng Li. Adacos:
    自适应缩放余弦对数以有效学习深度人脸表示。发表于 IEEE 计算机视觉与模式识别会议论文集，第10823–10832页，2019年。'
- en: '[114] Xiao Zhang, Rui Zhao, Junjie Yan, Mengya Gao, Yu Qiao, Xiaogang Wang,
    and Hongsheng Li. P2sgrad: Refined gradients for optimizing deep face models.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 9906–9914, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Xiao Zhang, Rui Zhao, Junjie Yan, Mengya Gao, Yu Qiao, Xiaogang Wang
    和 Hongsheng Li. P2sgrad: 精炼梯度用于优化深度人脸模型。发表于 IEEE 计算机视觉与模式识别会议论文集，第9906–9914页，2019年。'
- en: '[115] Yueqi Duan, Jiwen Lu, and Jie Zhou. Uniformface: Learning deep equidistributed
    representation for face recognition. In IEEE Conference on Computer Vision and
    Pattern Recognition, pages 3415–3424, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Yueqi Duan, Jiwen Lu 和 Jie Zhou. Uniformface: 学习深度等分布表示用于人脸识别。发表于 IEEE
    计算机视觉与模式识别会议论文集，第3415–3424页，2019年。'
- en: '[116] Hao Liu, Xiangyu Zhu, Zhen Lei, and Stan Z Li. Adaptiveface: Adaptive
    margin and sampling for face recognition. In IEEE Conference on Computer Vision
    and Pattern Recognition, pages 11947–11956, 2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Hao Liu, Xiangyu Zhu, Zhen Lei 和 Stan Z Li. Adaptiveface: 自适应边距和采样用于人脸识别。发表于
    IEEE 计算机视觉与模式识别会议论文集，第11947–11956页，2019年。'
- en: '[117] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive
    growing of gans for improved quality, stability, and variation. arXiv preprint
    arXiv:1710.10196, 2017.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Tero Karras, Timo Aila, Samuli Laine 和 Jaakko Lehtinen. 生成对抗网络的渐进式成长以改善质量、稳定性和变化性。arXiv
    预印本 arXiv:1710.10196，2017年。'
- en: '[118] Andrew K Hrechak and James A McHugh. Automated fingerprint recognition
    using structural matching. Pattern Recognition, 23(8):893–904, 1990.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Andrew K Hrechak 和 James A McHugh. 使用结构匹配的自动化指纹识别。模式识别，23(8):893–904，1990年。'
- en: '[119] Chih-Jen Lee and Sheng-De Wang. Fingerprint feature extraction using
    gabor filters. Electronics Letters, 35(4):288–290, 1999.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Chih-Jen Lee 和 Sheng-De Wang. 使用 Gabor 滤波器进行指纹特征提取。电子信件，35(4):288–290，1999年。'
- en: '[120] Marius Tico, Pauli Kuosmanen, and Jukka Saarinen. Wavelet domain features
    for fingerprint recognition. Electronics Letters, 37(1):21–22, 2001.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Marius Tico, Pauli Kuosmanen 和 Jukka Saarinen. 指纹识别的波域特征。电子信件，37(1):21–22，2001年。'
- en: '[121] Fvc fingerprint dataset. http://bias.csr.unibo.it/fvc2002/.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Fvc 指纹数据集。 http://bias.csr.unibo.it/fvc2002/。'
- en: '[122] Casia fingerprint dataset. http://biometrics.idealtest.org/dbDetailForUser.do?id=7.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Casia 指纹数据集。 http://biometrics.idealtest.org/dbDetailForUser.do?id=7。'
- en: '[123] Michael D Garris and R Michael McCabe. Fingerprint minutiae from latent
    and matching tenprint images. In Tenprint Images”, National Institute of Standards
    and Technology. Citeseer, 2000.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Michael D Garris 和 R Michael McCabe. 从潜在和匹配的十指印图像中提取指纹细节。发表于 “十指印图像”，国家标准与技术研究院。Citeseer，2000年。'
- en: '[124] Luke Nicholas Darlow and Benjamin Rosman. Fingerprint minutiae extraction
    using deep learning. In 2017 IEEE International Joint Conference on Biometrics
    (IJCB), pages 22–30\. IEEE, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Luke Nicholas Darlow 和 Benjamin Rosman. 使用深度学习进行指纹细节提取。发表于 2017 年 IEEE
    国际联合生物识别会议 (IJCB) 论文集，第22–30页，IEEE，2017年。'
- en: '[125] Yao Tang, Fei Gao, Jufu Feng, and Yuhang Liu. Fingernet: An unified deep
    network for fingerprint minutiae extraction. In 2017 IEEE International Joint
    Conference on Biometrics (IJCB), pages 108–116\. IEEE, 2017.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Yao Tang, Fei Gao, Jufu Feng 和 Yuhang Liu. Fingernet: 一种统一的深度网络用于指纹细节提取。发表于
    2017 年 IEEE 国际联合生物识别会议 (IJCB) 论文集，第108–116页，IEEE，2017年。'
- en: '[126] Chenhao Lin and Ajay Kumar. Contactless and partial 3d fingerprint recognition
    using multi-view deep representation. Pattern Recognition, 83:314–327, 2018.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Chenhao Lin 和 Ajay Kumar. 使用多视角深度表示的非接触式和部分3D指纹识别。《模式识别》，83:314–327，2018年。'
- en: '[127] Raid Omar, Tingting Han, Saadoon AM Al-Sumaidaee, and Taolue Chen. Deep
    finger texture learning for verifying people. IET Biometrics, 8(1):40–48, 2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Raid Omar, Tingting Han, Saadoon AM Al-Sumaidaee, 和 Taolue Chen. 深度指纹纹理学习用于人员验证。《IET生物识别》，8(1):40–48，2018年。'
- en: '[128] Shervin Minaee, Elham Azimi, and Amirali Abdolrashidi. Fingernet: Pushing
    the limits of fingerprint recognition using convolutional neural network. arXiv
    preprint arXiv:1907.12956, 2019.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Shervin Minaee, Elham Azimi, 和 Amirali Abdolrashidi. Fingernet: 使用卷积神经网络推进指纹识别的极限。arXiv预印本
    arXiv:1907.12956，2019年。'
- en: '[129] Chenhao Lin and Ajay Kumar. Multi-siamese networks to accurately match
    contactless to contact-based fingerprint images. In International Joint Conference
    on Biometrics (IJCB), pages 277–285\. IEEE, 2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Chenhao Lin 和 Ajay Kumar. 多孪生网络精确匹配非接触式与接触式指纹图像。在国际生物识别联合会议（IJCB），页码277–285。IEEE，2017年。'
- en: '[130] Branka Stojanović, Oge Marques, Aleksandar Nešković, and Snežana Puzović.
    Fingerprint roi segmentation based on deep learning. In 2016 24th Telecommunications
    Forum (TELFOR), pages 1–4. IEEE, 2016.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Branka Stojanović, Oge Marques, Aleksandar Nešković, 和 Snežana Puzović.
    基于深度学习的指纹区域分割。在2016年第24届电信论坛（TELFOR），页码1–4。IEEE，2016年。'
- en: '[131] Yanming Zhu, Xuefei Yin, Xiuping Jia, and Jiankun Hu. Latent fingerprint
    segmentation based on convolutional neural networks. In Workshop on Information
    Forensics and Security (WIFS), pages 1–6\. IEEE, 2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Yanming Zhu, Xuefei Yin, Xiuping Jia, 和 Jiankun Hu. 基于卷积神经网络的潜在指纹分割。在信息取证与安全研讨会（WIFS），页码1–6。IEEE，2017年。'
- en: '[132] Soowoong Kim, Bogun Park, Bong Seop Song, and Seungjoon Yang. Deep belief
    network based statistical feature learning for fingerprint liveness detection.
    Pattern Recognition Letters, 77:58–65, 2016.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Soowoong Kim, Bogun Park, Bong Seop Song, 和 Seungjoon Yang. 基于深度信念网络的指纹活体检测统计特征学习。《模式识别快报》，77:58–65，2016年。'
- en: '[133] Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo, and Rubens Campos
    Machado. Fingerprint liveness detection using convolutional neural networks. IEEE
    transactions on information forensics and security, 11(6):1206–1213, 2016.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo, 和 Rubens Campos
    Machado. 使用卷积神经网络进行指纹活体检测。《IEEE信息取证与安全交易》，11(6):1206–1213，2016年。'
- en: '[134] Shervin Minaee and Amirali Abdolrashidi. Finger-gan: Generating realistic
    fingerprint images using connectivity imposed gan. preprint, arXiv:1812.10482,
    2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Shervin Minaee 和 Amirali Abdolrashidi. Finger-gan: 使用连接约束生成对抗网络生成真实的指纹图像。预印本，arXiv:1812.10482，2018年。'
- en: '[135] Elham Tabassi, Tarang Chugh, Debayan Deb, and Anil K Jain. Altered fingerprints:
    Detection and localization. In 2018 IEEE 9th International Conference on Biometrics
    Theory, Applications and Systems (BTAS), pages 1–9\. IEEE, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Elham Tabassi, Tarang Chugh, Debayan Deb, 和 Anil K Jain. 修改指纹：检测与定位。在2018年IEEE第9届生物识别理论、应用与系统国际会议（BTAS），页码1–9。IEEE，2018年。'
- en: '[136] John G Daugman. High confidence visual recognition of persons by a test
    of statistical independence. IEEE transactions on pattern analysis and machine
    intelligence, 15(11):1148–1161, 1993.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] John G Daugman. 通过统计独立性测试实现高置信度的视觉人物识别。《IEEE模式分析与机器智能交易》，15(11):1148–1161，1993年。'
- en: '[137] Richard Wildes, Jane Asmuth, Gilbert Green, Stephen Hsu, Raymond Kolczynski,
    James Matey, and Sterling McBride. A system for automated iris recognition. In
    Workshop on Applications of Computer Vision, pages 121–128. IEEE, 1994.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Richard Wildes, Jane Asmuth, Gilbert Green, Stephen Hsu, Raymond Kolczynski,
    James Matey, 和 Sterling McBride. 一个自动化虹膜识别系统。在计算机视觉应用研讨会，页码121–128。IEEE，1994年。'
- en: '[138] Gerald O Williams. Iris recognition technology. In 1996 30th Annual International
    Carnahan Conference on Security Technology, pages 46–59\. IEEE, 1996.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Gerald O Williams. 虹膜识别技术。在1996年第30届国际卡纳汉安全技术会议，页码46–59。IEEE，1996年。'
- en: '[139] Shervin Minaee, AmirAli Abdolrashidi, and Yao Wang. Iris recognition
    using scattering transform and textural features. In signal processing and signal
    processing education workshop, pages 37–42\. IEEE, 2015.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Shervin Minaee, AmirAli Abdolrashidi, 和 Yao Wang. 使用散射变换和纹理特征进行虹膜识别。在信号处理与信号处理教育研讨会，页码37–42。IEEE，2015年。'
- en: '[140] Zijing Zhao and Ajay Kumar. Towards more accurate iris recognition using
    deeply learned spatially corresponding features. In IEEE International Conference
    on Computer Vision, pages 3809–3818, 2017.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Zijing Zhao 和 Ajay Kumar. 通过深度学习空间对应特征实现更准确的虹膜识别。在IEEE计算机视觉国际会议，页码3809–3818，2017年。'
- en: '[141] http://biometrics.idealtest.org/.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] http://biometrics.idealtest.org/。'
- en: '[142] Casia iris dataset. http://biometrics.idealtest.org/findTotalDbByMode.do?mode=Iris.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Casia 虹膜数据集。http://biometrics.idealtest.org/findTotalDbByMode.do?mode=Iris。'
- en: '[143] Ubiris iris dataset. http://iris.di.ubi.pt/.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Ubiris 虹膜数据集。http://iris.di.ubi.pt/。'
- en: '[144] Iit iris dataset. https://www4.comp.polyu.edu.hk/~csajaykr/IITD/Database_Iris.htm.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] IIT 虹膜数据集。https://www4.comp.polyu.edu.hk/~csajaykr/IITD/Database_Iris.htm。'
- en: '[145] Lg iris. https://cvrl.nd.edu/projects/data/.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] LG 虹膜。https://cvrl.nd.edu/projects/data/。'
- en: '[146] Maria De Marsico, Michele Nappi, Daniel Riccio, and Harry Wechsler. Mobile
    iris challenge evaluation (miche)-i, biometric iris dataset and protocols. Pattern
    Recognition Letters, 57:17–23, 2015.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Maria De Marsico, Michele Nappi, Daniel Riccio 和 Harry Wechsler. 移动虹膜挑战评估
    (MICHE)-I，生物识别虹膜数据集和协议。模式识别快报，57:17–23，2015。'
- en: '[147] Shervin Minaee, Amirali Abdolrashidiy, and Yao Wang. An experimental
    study of deep convolutional features for iris recognition. In signal processing
    in medicine and biology symposium, pages 1–6\. IEEE, 2016.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Shervin Minaee, Amirali Abdolrashidiy 和 Yao Wang. 深度卷积特征在虹膜识别中的实验研究。在医学与生物学信号处理研讨会论文集中，页
    1–6。IEEE，2016。'
- en: '[148] Abhishek Gangwar and Akanksha Joshi. Deepirisnet: Deep iris representation
    with applications in iris recognition and cross-sensor iris recognition. In 2016
    IEEE International Conference on Image Processing (ICIP), pages 2301–2305\. IEEE,
    2016.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Abhishek Gangwar 和 Akanksha Joshi. Deepirisnet: 深度虹膜表示及其在虹膜识别和跨传感器虹膜识别中的应用。在
    2016 年 IEEE 国际图像处理大会 (ICIP) 会议论文集中，页 2301–2305。IEEE，2016。'
- en: '[149] Mohtashim Baqar, Azfar Ghani, Azeem Aftab, Saira Arbab, and Sajid Yasin.
    Deep belief networks for iris recognition based on contour detection. In 2016
    International Conference on Open Source Systems & Technologies (ICOSST), pages
    72–77\. IEEE, 2016.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Mohtashim Baqar, Azfar Ghani, Azeem Aftab, Saira Arbab 和 Sajid Yasin.
    基于轮廓检测的深度信念网络虹膜识别。在 2016 年开源系统与技术国际会议 (ICOSST) 会议论文集中，页 72–77。IEEE，2016。'
- en: '[150] Maram Alaslani and Lamiaa Elrefaei. Convolutional neural network-based
    feature extraction for iris recognition. Int. J. Comp. Sci. Info. Tech., 10:65–78,
    2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Maram Alaslani 和 Lamiaa Elrefaei. 基于卷积神经网络的虹膜识别特征提取。国际计算机科学与信息技术期刊，10:65–78，2018。'
- en: '[151] Hrishikesh Menon and Anirban Mukherjee. Iris biometrics using deep convolutional
    networks. In 2018 IEEE International Instrumentation and Measurement Technology
    Conference (I2MTC), pages 1–5\. IEEE, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Hrishikesh Menon 和 Anirban Mukherjee. 基于深度卷积网络的虹膜生物识别。在 2018 年 IEEE 国际仪器与测量技术会议
    (I2MTC) 会议论文集中，页 1–5。IEEE，2018。'
- en: '[152] Heinz Hofbauer, Ehsaneddin Jalilian, and Andreas Uhl. Exploiting superior
    cnn-based iris segmentation for better recognition accuracy. Pattern Recognition
    Letters, 120:17–23, 2019.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Heinz Hofbauer, Ehsaneddin Jalilian 和 Andreas Uhl. 利用优越的 CNN 基于虹膜分割提高识别准确率。模式识别快报，120:17–23，2019。'
- en: '[153] Sohaib Ahmad and Benjamin Fuller. Thirdeye: Triplet based iris recognition
    without normalization. arXiv preprint arXiv:1907.06147, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Sohaib Ahmad 和 Benjamin Fuller. Thirdeye: 基于三元组的虹膜识别，无需归一化。arXiv 预印本
    arXiv:1907.06147，2019。'
- en: '[154] Shervin Minaee and Amirali Abdolrashidi. Deepiris: Iris recognition using
    a deep learning approach. arXiv preprint arXiv:1907.09380, 2019.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Shervin Minaee 和 Amirali Abdolrashidi. Deepiris: 使用深度学习方法进行虹膜识别。arXiv
    预印本 arXiv:1907.09380，2019。'
- en: '[155] Shervin Minaee and Amirali Abdolrashidi. Iris-gan: Learning to generate
    realistic iris images using convolutional gan. arXiv preprint arXiv:1812.04822,
    2018.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Shervin Minaee 和 Amirali Abdolrashidi. Iris-gan: 使用卷积 gan 生成逼真的虹膜图像。arXiv
    预印本 arXiv:1812.04822，2018。'
- en: '[156] Min Beom Lee, Yu Hwan Kim, and Kang Ryoung Park. Conditional generative
    adversarial network-based data augmentation for enhancement of iris recognition
    accuracy. IEEE Access, 7:122134–122152, 2019.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Min Beom Lee, Yu Hwan Kim 和 Kang Ryoung Park. 基于条件生成对抗网络的数据增强以提升虹膜识别准确率。IEEE
    Access，7:122134–122152，2019。'
- en: '[157] David Zhang, Wangmeng Zuo, and Feng Yue. A comparative study of palmprint
    recognition algorithms. ACM computing surveys (CSUR), 44(1):2, 2012.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] David Zhang, Wangmeng Zuo 和 Feng Yue. 掌纹识别算法的比较研究。ACM 计算调查 (CSUR)，44(1):2，2012。'
- en: '[158] Jun Chen, Changshui Zhang, and Gang Rong. Palmprint recognition using
    crease. In Proceedings 2001 International Conference on Image Processing (Cat.
    No. 01CH37205), volume 3, pages 234–237\. IEEE, 2001.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Jun Chen, Changshui Zhang 和 Gang Rong. 基于褶皱的掌纹识别。在 2001 年国际图像处理会议论文集中（Cat.
    No. 01CH37205），第 3 卷，页 234–237。IEEE，2001。'
- en: '[159] Tee Connie, Andrew Teoh, Michael Goh, and David Ngo. Palmprint recognition
    with pca and ica. In Proc. Image and Vision Computing, New Zealand, 2003.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Tee Connie, Andrew Teoh, Michael Goh 和 David Ngo. 基于 PCA 和 ICA 的掌纹识别。在新西兰图像与视觉计算会议论文集中，2003。'
- en: '[160] Wenxin Li, David Zhang, and Zhuoqun Xu. Palmprint identification by fourier
    transform. International Journal of Pattern Recognition and Artificial Intelligence,
    16(04):417–432, 2002.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] 李文鑫、张大伟和徐卓群。通过傅里叶变换进行掌纹识别。《国际模式识别与人工智能期刊》，16(04):417–432，2002年。'
- en: '[161] Xiang-Qian Wu, Kuan-Quan Wang, and David Zhang. Wavelet based palm print
    recognition. In Proceedings. International Conference on Machine Learning and
    Cybernetics, volume 3, pages 1253–1257\. IEEE, 2002.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] 吴向前、王宽宽 和 张大伟。基于小波的掌纹识别。在会议录。国际机器学习与控制会议，第3卷，页1253–1257。IEEE，2002年。'
- en: '[162] Wei Shu and David Zhang. Palmprint verification: an implementation of
    biometric technology. In Fourteenth International Conference on Pattern Recognition,
    volume 1, pages 219–221\. IEEE, 1998.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] 施伟和张大伟。掌纹验证：生物识别技术的实现。第十四届国际模式识别大会，第1卷，页219–221。IEEE，1998年。'
- en: '[163] Shervin Minaee and Yao Wang. Palmprint recognition using deep scattering
    network. In International Symposium on Circuits and Systems (ISCAS). IEEE, 2017.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Shervin Minaee 和 Yao Wang。利用深度散射网络进行掌纹识别。国际电路与系统会议（ISCAS）。IEEE，2017年。'
- en: '[164] Casia palmprint dataset. http://www.cbsr.ia.ac.cn/english/Palmprint\%20Databases.asp.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Casia掌纹数据集。http://www.cbsr.ia.ac.cn/english/Palmprint%20Databases.asp。'
- en: '[165] Iit palmprint dataset. https://www4.comp.polyu.edu.hk/~csajaykr/IITD/Database_Palm.htm.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] IIT掌纹数据集。https://www4.comp.polyu.edu.hk/~csajaykr/IITD/Database_Palm.htm。'
- en: '[166] Zhao Dandan Pan Xin, Pan Xin, Luo Xiaoling, and Gao Xiaojing. Palmprint
    recognition based on deep learning. 2015.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] 赵丹丹、潘鑫、潘鑫、罗小玲 和 高小静。基于深度学习的掌纹识别。2015年。'
- en: '[167] Djamel Samai, Khaled Bensid, Abdallah Meraoumia, Abdelmalik Taleb-Ahmed,
    and Mouldi Bedda. 2d and 3d palmprint recognition using deep learning method.
    In IInternational Conference on Pattern Analysis and Intelligent Systems, pages
    1–6\. IEEE, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Djamel Samai、Khaled Bensid、Abdallah Meraoumia、Abdelmalik Taleb-Ahmed
    和 Mouldi Bedda。使用深度学习方法进行2D和3D掌纹识别。在国际模式分析与智能系统会议，第1–6页。IEEE，2018年。'
- en: '[168] Dexing Zhong, Yuan Yang, and Xuefeng Du. Palmprint recognition using
    siamese network. In Chinese Conference on Biometric Recognition, pages 48–55.
    Springer, 2018.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] 钟德兴、袁洋 和 杜雪峰。使用孪生网络进行掌纹识别。在中国生物识别大会，第48–55页。Springer，2018年。'
- en: '[169] Mahdieh Izadpanahkakhk, Seyyed Razavi, Mehran Gorjikolaie, Seyyed Zahiri,
    and Aurelio Uncini. Deep region of interest and feature extraction models for
    palmprint verification using convolutional neural networks transfer learning.
    Applied Sciences, 8(7), 2018.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Mahdieh Izadpanahkakhk、Seyyed Razavi、Mehran Gorjikolaie、Seyyed Zahiri
    和 Aurelio Uncini。使用卷积神经网络迁移学习的深度兴趣区域和特征提取模型进行掌纹验证。《应用科学》，8(7)，2018年。'
- en: '[170] Huikai Shao and Dexing Zhong. Few-shot palmprint recognition via graph
    neural networks. Electronics Letters, 55(16):890–892, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] 邵辉凯 和 钟德兴。通过图神经网络进行少样本掌纹识别。《电子通讯信函》，55(16):890–892，2019年。'
- en: '[171] Huikai Shao, Dexing Zhong, and Xuefeng Du. Efficient deep palmprint recognition
    via distilled hashing coding. In IEEE Conference on Computer Vision and Pattern
    Recognition Workshops, 2019.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] 邵辉凯、钟德兴和杜雪峰。通过蒸馏哈希编码进行高效的深度掌纹识别。在IEEE计算机视觉与模式识别研讨会，2019年。'
- en: '[172] Huikai Shao, Dexing Zhong, and Xuefeng Du. Cross-domain palmprint recognition
    based on transfer convolutional autoencoder. In International Conference on Image
    Processing, pages 1153–1157\. IEEE, 2019.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] 邵辉凯、钟德兴 和 杜雪峰。基于迁移卷积自编码器的跨领域掌纹识别。在国际图像处理会议，第1153–1157页。IEEE，2019年。'
- en: '[173] Shuping Zhao, Bob Zhang, and CL Philip Chen. Joint deep convolutional
    feature representation for hyperspectral palmprint recognition. Information Sciences,
    489:167–181, 2019.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] 赵书平、张博和CL Philip Chen。用于高光谱掌纹识别的联合深度卷积特征表示。《信息科学》，489:167–181，2019年。'
- en: '[174] Zhihuai Xie, Zhenhua Guo, and Chengshan Qian. Palmprint gender classification
    by convolutional neural network. IET Computer Vision, 12(4):476–483, 2018.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] 谢志怀、郭振华和钱成山。通过卷积神经网络进行掌纹性别分类。《IET计算机视觉》，12(4):476–483，2018年。'
- en: '[175] Celia Cintas, Claudio Delrieux, Pablo Navarro, Mirsha Quinto-Sanchez,
    Bruno Pazos, and Rolando Gonzalez-Jose. Automatic ear detection and segmentation
    over partially occluded profile face images. Journal of Computer Science & Technology,
    19, 2019.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Celia Cintas、Claudio Delrieux、Pablo Navarro、Mirsha Quinto-Sanchez、Bruno
    Pazos 和 Rolando Gonzalez-Jose。自动耳部检测和分割在部分遮挡的侧脸图像上。《计算机科学与技术期刊》，19，2019年。'
- en: '[176] Žiga Emeršič, Dejan Štepec, Vitomir Štruc, and Peter Peer. Training convolutional
    neural networks with limited training data for ear recognition in the wild. arXiv
    preprint arXiv:1711.09952, 2017.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Žiga Emeršič, Dejan Štepec, Vitomir Štruc, 和 Peter Peer. 使用有限训练数据训练卷积神经网络进行野外耳朵识别。arXiv
    预印本 arXiv:1711.09952，2017年。'
- en: '[177] Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. Sparse representation
    for ear biometrics. In International Symposium on Visual Computing, pages 336–345.
    Springer, 2008.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Imran Naseem, Roberto Togneri, 和 Mohammed Bennamoun. 稀疏表示用于耳朵生物识别。在国际视觉计算研讨会，页码336–345。Springer，2008年。'
- en: '[178] Awe ear dataset. http://awe.fri.uni-lj.si/home.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Awe ear dataset. http://awe.fri.uni-lj.si/home。'
- en: '[179] Fevziye Irem Eyiokur, Dogucan Yaman, and Hazım Kemal Ekenel. Domain adaptation
    for ear recognition using deep convolutional neural networks. iet Biometrics,
    7(3):199–206, 2017.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Fevziye Irem Eyiokur, Dogucan Yaman, 和 Hazım Kemal Ekenel. 使用深度卷积神经网络的耳朵识别领域自适应。IET
    生物识别，7(3):199–206，2017年。'
- en: '[180] Ustb ear dataset. http://www1.ustb.edu.cn/resb/en/visit/visit.htm.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Ustb ear dataset. http://www1.ustb.edu.cn/resb/en/visit/visit.htm。'
- en: '[181] Ziga Emersic, Dejan Stepec, Vitomir Struc, Peter Peer, Anjith George,
    Adii Ahmad, Elshibani Omar, Terranee E Boult, Reza Safdaii, Yuxiang Zhou, et al.
    The unconstrained ear recognition challenge. In 2017 IEEE International Joint
    Conference on Biometrics (IJCB), pages 715–724\. IEEE, 2017.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Ziga Emersic, Dejan Stepec, Vitomir Struc, Peter Peer, Anjith George,
    Adii Ahmad, Elshibani Omar, Terranee E Boult, Reza Safdaii, Yuxiang Zhou 等. 无约束耳朵识别挑战。在2017年
    IEEE 国际联合生物识别会议 (IJCB)，页码715–724。IEEE，2017年。'
- en: '[182] E Gonzalez-Sanchez. Biometria de la oreja. PhD thesis, Ph. D. thesis,
    Universidad de Las Palmas de Gran Canaria, Spain, 2008.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] E González-Sanchez. 耳朵生物识别。博士论文，拉斯帕尔马斯大学，西班牙，2008年。'
- en: '[183] Carreira Perpinan. Compression neural networks for feature extraction:
    Application to human recognition from ear images. Master’s thesis, Faculty of
    Informatics, Technical University of Madrid, Spain, 1995.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Carreira Perpinan. 压缩神经网络用于特征提取：在人耳图像识别中的应用。硕士论文，马德里技术大学信息学学院，西班牙，1995年。'
- en: '[184] Dariusz Frejlichowski and Natalia Tyszkiewicz. The west pomeranian university
    of technology ear database–a tool for testing biometric algorithms. In International
    Conference Image Analysis and Recognition, pages 227–234\. Springer, 2010.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Dariusz Frejlichowski 和 Natalia Tyszkiewicz. 西波美拉尼亚技术大学耳朵数据库——测试生物识别算法的工具。在国际图像分析与识别会议，页码227–234。Springer，2010年。'
- en: '[185] Jie Zhang, Wen Yu, Xudong Yang, and Fang Deng. Few-shot learning for
    ear recognition. In Proceedings of the 2019 International Conference on Image,
    Video and Signal Processing, pages 50–54\. ACM, 2019.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Jie Zhang, Wen Yu, Xudong Yang, 和 Fang Deng. 用于耳朵识别的少样本学习。在2019年国际图像、视频与信号处理会议论文集中，页码50–54。ACM，2019年。'
- en: '[186] Samuel Dodge, Jinane Mounsef, and Lina Karam. Unconstrained ear recognition
    using deep neural networks. IET Biometrics, 7(3):207–214, 2018.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Samuel Dodge, Jinane Mounsef, 和 Lina Karam. 使用深度神经网络进行无约束耳朵识别。IET 生物识别，7(3):207–214，2018年。'
- en: '[187] Ziga Emersic, Dejan Stepec, Vitomir Struc, and Peter Peer. Training convolutional
    neural networks with limited training data for ear recognition in the wild. In
    International Conference on Automatic Face & Gesture Recognition, pages 987–994\.
    IEEE, 2017.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Ziga Emersic, Dejan Stepec, Vitomir Struc, 和 Peter Peer. 使用有限训练数据训练卷积神经网络进行野外耳朵识别。在国际自动面部与手势识别会议，页码987–994。IEEE，2017年。'
- en: '[188] Žiga Emeršič, Nil Oleart Playà, Vitomir Štruc, and Peter Peer. Towards
    accessories-aware ear recognition. In 2018 IEEE International Work Conference
    on Bioinspired Intelligence (IWOBI), pages 1–8\. IEEE, 2018.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Žiga Emeršič, Nil Oleart Playà, Vitomir Štruc, 和 Peter Peer. 朝着配件感知耳朵识别迈进。在2018年
    IEEE 国际生物启发智能工作会议 (IWOBI)，页码1–8。IEEE，2018年。'
- en: '[189] Harsh Sinha, Raunak Manekar, Yash Sinha, and Pawan K Ajmera. Convolutional
    neural network-based human identification using outer ear images. In Soft Computing
    for Problem Solving, pages 707–719. Springer, 2019.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Harsh Sinha, Raunak Manekar, Yash Sinha, 和 Pawan K Ajmera. 基于卷积神经网络的外耳图像人类识别。在解决问题的软计算，页码707–719。Springer，2019年。'
- en: '[190] Ibrahim Omara, Xiaohe Wu, Hongzhi Zhang, Yong Du, and Wangmeng Zuo. Learning
    pairwise svm on hierarchical deep features for ear recognition. IET Biometrics,
    7(6):557–566, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Ibrahim Omara, Xiaohe Wu, Hongzhi Zhang, Yong Du, 和 Wangmeng Zuo. 在层次深度特征上学习配对
    SVM 进行耳朵识别。IET 生物识别，7(6):557–566，2018年。'
- en: '[191] Mohammad Haghighat, Mohamed Abdel-Mottaleb, and Wadee Alhalabi. Discriminant
    correlation analysis: Real-time feature level fusion for multimodal biometric
    recognition. IEEE Transactions on Information Forensics and Security, 11(9):1984–1996,
    2016.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Mohammad Haghighat, Mohamed Abdel-Mottaleb 和 Wadee Alhalabi。判别相关分析：多模态生物识别的实时特征级融合。IEEE信息取证与安全学报，11（9）：1984–1996，2016年。'
- en: '[192] Carl Brunner, Andreas Fischer, Klaus Luig, and Thorsten Thies. Pairwise
    support vector machines and their application to large scale problems. Journal
    of Machine Learning Research, 13(Aug):2279–2292, 2012.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Carl Brunner, Andreas Fischer, Klaus Luig 和 Thorsten Thies。成对支持向量机及其在大规模问题中的应用。机器学习研究杂志，13（8月）：2279–2292，2012年。'
- en: '[193] Earnest E Hansley, Maurício Pamplona Segundo, and Sudeep Sarkar. Employing
    fusion of learned and handcrafted features for unconstrained ear recognition.
    IET Biometrics, 7(3):215–223, 2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Earnest E Hansley, Maurício Pamplona Segundo 和 Sudeep Sarkar。采用学习和手工特征融合的无约束耳朵识别。IET
    生物识别，7（3）：215–223，2018年。'
- en: '[194] Clark D Shaver and John M Acken. A brief review of speaker recognition
    technology. 2016.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Clark D Shaver 和 John M Acken。说话人识别技术的简要回顾。2016年。'
- en: '[195] David B Yoffie, Liang Wu, Jodie Sweitzer, Denzil Eden, and Karan Ahuja.
    Voice war: Hey google vs. alexa vs. siri. 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] David B Yoffie, Liang Wu, Jodie Sweitzer, Denzil Eden 和 Karan Ahuja。语音大战：嘿
    Google 对比 Alexa 对比 Siri。2018年。'
- en: '[196] Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. Sparse representation
    for speaker identification. In 2010 20th International Conference on Pattern Recognition,
    pages 4460–4463\. IEEE, 2010.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Imran Naseem, Roberto Togneri 和 Mohammed Bennamoun。稀疏表示用于说话人识别。在2010年第20届国际模式识别大会，页面4460–4463。IEEE，2010年。'
- en: '[197] S. Furui. Speaker recognition. Scholarpedia, 3(4):3715, 2008. revision
    #64889.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] S. Furui。说话人识别。学者百科，3（4）：3715，2008年。修订版 #64889。'
- en: '[198] Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey, and Alan
    McCree. Speaker diarization using deep neural network embeddings. In International
    Conference on Acoustics, Speech and Signal Processing, pages 4930–4934\. IEEE,
    2017.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey 和 Alan
    McCree。使用深度神经网络嵌入进行说话人分段。在国际声学、语音与信号处理会议，页面4930–4934。IEEE，2017年。'
- en: '[199] Alvin F Martin and Mark A Przybocki. The nist speaker recognition evaluations:
    1996-2001. In 2001: A Speaker Odyssey-The Speaker Recognition Workshop, 2001.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Alvin F Martin 和 Mark A Przybocki。nist语音识别评估：1996-2001。在2001年：一个说话人的奥德赛-说话人识别研讨会，2001年。'
- en: '[200] The 2010 nist speaker recognition evaluation. 2010.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] 2010年nist语音识别评估。2010年。'
- en: '[201] The 2018 nist speaker recognition evaluation. 2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] 2018年nist语音识别评估。2018年。'
- en: '[202] The 2016 nist speaker recognition evaluation. 2016.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] 2016年nist语音识别评估。2016年。'
- en: '[203] Mitchell McLaren, Luciana Ferrer, Diego Castan, and Aaron Lawson. The
    speakers in the wild (sitw) speaker recognition database. In Interspeech, pages
    818–822, 2016.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Mitchell McLaren, Luciana Ferrer, Diego Castan 和 Aaron Lawson。野外说话人（sitw）识别数据库。在Interspeech，页面818–822，2016年。'
- en: '[204] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale
    speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Arsha Nagrani, Joon Son Chung 和 Andrew Zisserman。Voxceleb：大规模说话人识别数据集。arXiv
    预印本 arXiv:1706.08612，2017年。'
- en: '[205] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep
    speaker recognition. arXiv preprint arXiv:1806.05622, 2018.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Joon Son Chung, Arsha Nagrani 和 Andrew Zisserman。Voxceleb2：深度说话人识别。arXiv
    预印本 arXiv:1806.05622，2018年。'
- en: '[206] J Godfrey and E Holliman. Switchboard-1 release 2: Linguistic data consortium.
    SWITCHBOARD: A User’s Manual, 1997.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] J Godfrey 和 E Holliman。Switchboard-1 发布 2：语言数据联盟。SWITCHBOARD：用户手册，1997年。'
- en: '[207] Christopher Cieri, David Miller, and Kevin Walker. The fisher corpus:
    a resource for the next generations of speech-to-text. In LREC, volume 4, pages
    69–71, 2004.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Christopher Cieri, David Miller 和 Kevin Walker。费舍语料库：下一代语音转文本资源。在LREC，卷4，页面69–71，2004年。'
- en: '[208] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech:
    an asr corpus based on public domain audio books. In 2015 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), pages 5206–5210\. IEEE, 2015.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Vassil Panayotov, Guoguo Chen, Daniel Povey 和 Sanjeev Khudanpur。Librispeech：基于公共领域有声书的ASR语料库。在2015
    IEEE国际声学、语音与信号处理会议（ICASSP），页面5206–5210。IEEE，2015年。'
- en: '[209] Victor Zue, Stephanie Seneff, and James Glass. Speech database development
    at mit: Timit and beyond. Speech communication, 9(4):351–356, 1990.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Victor Zue, Stephanie Seneff 和 James Glass。麻省理工学院的语音数据库开发：Timit及其他。语音通信，9（4）：351–356，1990年。'
- en: '[210] Mirco Ravanelli and Yoshua Bengio. Learning speaker representations with
    mutual information. arXiv preprint arXiv:1812.00271, 2018.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] 米尔科·拉瓦内利和约书亚·本吉奥。通过互信息学习说话人表征。arXiv预印本 arXiv:1812.00271，2018年。'
- en: '[211] Najim Dehak, Patrick J Kenny, Réda Dehak, Pierre Dumouchel, and Pierre
    Ouellet. Front-end factor analysis for speaker verification. IEEE Transactions
    on Audio, Speech, and Language Processing, 19(4):788–798, 2010.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] 纳吉姆·德哈克、帕特里克·J·肯尼、雷达·德哈克、皮埃尔·杜穆谢尔和皮埃尔·乌勒。用于说话人验证的前端因子分析。IEEE音频、语音和语言处理交易，19(4):788–798，2010年。'
- en: '[212] Yun Lei, Nicolas Scheffer, Luciana Ferrer, and Mitchell McLaren. A novel
    scheme for speaker recognition using a phonetically-aware deep neural network.
    In International Conference on Acoustics, Speech and Signal Processing, pages
    1695–1699\. IEEE, 2014.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] 云磊、尼古拉斯·谢费尔、露西安娜·费雷尔和米切尔·麦克拉伦。使用语音学感知深度神经网络进行说话人识别的新方案。见于《国际声学、语音和信号处理会议》，第1695–1699页。IEEE，2014年。'
- en: '[213] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier
    Gonzalez-Dominguez. Deep neural networks for small footprint text-dependent speaker
    verification. In International Conference on Acoustics, Speech and Signal Processing
    (ICASSP). IEEE, 2014.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] 埃赫桑·瓦里亚尼、辛·雷、埃里克·麦克德莫特、伊格纳西奥·洛佩斯·莫雷诺和哈维尔·冈萨雷斯·多明戈斯。用于小型文本相关说话人验证的深度神经网络。见于《国际声学、语音和信号处理会议（ICASSP）》。IEEE，2014年。'
- en: '[214] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev
    Khudanpur. X-vectors: Robust dnn embeddings for speaker recognition. In International
    Conference on Acoustics, Speech and Signal Processing. IEEE, 2018.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] 大卫·斯奈德、丹尼尔·加西亚-罗梅罗、格雷戈里·塞尔和丹尼尔·波维，以及桑吉夫·库丹普尔。X-vectors：用于说话人识别的鲁棒DNN嵌入。见于《国际声学、语音和信号处理会议》。IEEE，2018年。'
- en: '[215] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end
    text-dependent speaker verification. In International Conference on Acoustics,
    Speech and Signal Processing (ICASSP). IEEE, 2016.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] 乔治·海戈尔德、伊格纳西奥·莫雷诺、萨米·本吉奥和诺姆·沙泽尔。端到端文本相关说话人验证。见于《国际声学、语音和信号处理会议（ICASSP）》。IEEE，2016年。'
- en: '[216] Shi-Xiong Zhang, Zhuo Chen, Yong Zhao, Jinyu Li, and Yifan Gong. End-to-end
    attention based text-dependent speaker verification. In Spoken Language Technology
    Workshop (SLT), pages 171–178. IEEE, 2016.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] 施雄张、卓陈、永赵、金玉李和一凡宫。基于注意力机制的端到端文本相关说话人验证。见于《语音语言技术研讨会（SLT）》，第171–178页。IEEE，2016年。'
- en: '[217] Chunlei Zhang and Kazuhito Koishida. End-to-end text-independent speaker
    verification with triplet loss on short utterances. In Interspeech, 2017.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] 张春雷和小石田和人。基于短语音的端到端文本无关说话人验证，采用三元组损失。见于《国际语音交流大会》，2017年。'
- en: '[218] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. Generalized
    end-to-end loss for speaker verification. In International Conference on Acoustics,
    Speech and Signal Processing. IEEE, 2018.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] 李万、全王、艾伦·帕皮尔和伊格纳西奥·洛佩斯·莫雷诺。用于说话人验证的广义端到端损失。见于《国际声学、语音和信号处理会议》。IEEE，2018年。'
- en: '[219] Nam Le and Jean-Marc Odobez. Robust and discriminative speaker embedding
    via intra-class distance variance regularization. In Interspeech, pages 2257–2261,
    2018.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] 南·黎和让-马克·奥多贝兹。通过类内距离方差正则化实现鲁棒且具有区分性的说话人嵌入。见于《国际语音交流》，第2257–2261页，2018年。'
- en: '[220] Gautam Bhattacharya, Jahangir Alam, and Patrick Kenny. Deep speaker recognition:
    Modular or monolithic? Proc. Interspeech 2019, pages 1143–1147, 2019.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] 高塔姆·巴塔查里亚、贾汉吉尔·阿拉姆和帕特里克·肯尼。深度说话人识别：模块化还是整体式？见于《国际语音交流2019》，第1143–1147页，2019年。'
- en: '[221] KR Radhika, MK Venkatesha, and GN Sekhar. An approach for on-line signature
    authentication using zernike moments. Pattern Recognition Letters, 32(5), 2011.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] KR 拉迪卡、MK 文卡特沙和GN 塞卡尔。使用泽尔尼克矩进行在线签名认证的方法。模式识别快报，32(5)，2011年。'
- en: '[222] Amir Soleimani, Babak N Araabi, and Kazim Fouladi. Deep multitask metric
    learning for offline signature verification. Pattern Recognition Letters, 80:84–90,
    2016.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] 阿米尔·索雷马尼、巴巴克·纳·阿拉比和卡齐姆·福拉迪。用于离线签名验证的深度多任务度量学习。模式识别快报，80:84–90，2016年。'
- en: '[223] Donato Impedovo and Giuseppe Pirlo. Automatic signature verification:
    The state of the art. IEEE Transactions on Systems, Man, and Cybernetics, Part
    C (Applications and Reviews), 38(5):609–635, 2008.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] 多纳托·因佩多沃和朱塞佩·皮尔洛。自动签名验证：现状。IEEE系统、人类和控制论交易，部分C（应用与评论），38(5):609–635，2008年。'
- en: '[224] Sargur Srihari, Aihua Xu, and Meenakshi Kalera. Learning strategies and
    classification methods for off-line signature verification. In Workshop on Frontiers
    in Handwriting Recognition, pages 161–166\. IEEE, 2004.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] 萨古尔·斯里哈里、艾华·徐和米纳克希·卡莱拉。离线签名验证的学习策略和分类方法。见于《手写识别前沿研讨会》，第161–166页。IEEE，2004年。'
- en: '[225] Icdar svc 2009. http://tc11.cvc.uab.es/datasets/SigComp2009_1.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] Icdar svc 2009。http://tc11.cvc.uab.es/datasets/SigComp2009_1。'
- en: '[226] Dit-Yan Yeung, Hong Chang, Yimin Xiong, Susan George, Ramanujan Kashi,
    Takashi Matsumoto, and Gerhard Rigoll. Svc2004: First international signature
    verification competition. In International conference on biometric authentication,
    pages 16–22\. Springer, 2004.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Dit-Yan Yeung, Hong Chang, Yimin Xiong, Susan George, Ramanujan Kashi,
    Takashi Matsumoto 和 Gerhard Rigoll。Svc2004：第一次国际签名验证比赛。在国际生物识别认证会议上，第16–22页。Springer，2004。'
- en: '[227] Francisco Vargas, M Ferrer, Carlos Travieso, and J Alonso. Off-line handwritten
    signature gpds-960 corpus. In International Conference on Document Analysis and
    Recognition, volume 2, pages 764–768\. IEEE, 2007.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Francisco Vargas, M Ferrer, Carlos Travieso 和 J Alonso。离线手写签名GPDS-960语料库。在国际文档分析与识别会议上，第2卷，第764–768页。IEEE，2007。'
- en: '[228] Bernardete Ribeiro, Ivo Gonçalves, Sérgio Santos, and Alexander Kovacec.
    Deep learning networks for off-line handwritten signature recognition. In Iberoamerican
    Congress on Pattern Recognition. Springer, 2011.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Bernardete Ribeiro, Ivo Gonçalves, Sérgio Santos 和 Alexander Kovacec。用于离线手写签名识别的深度学习网络。在伊比利亚美洲模式识别大会上。Springer，2011。'
- en: '[229] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning
    algorithm for boltzmann machines. Cognitive science, 9(1):147–169, 1985.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] David H Ackley, Geoffrey E Hinton 和 Terrence J Sejnowski。Boltzmann机的学习算法。《认知科学》，9(1)：147–169，1985年。'
- en: '[230] Hannes Rantzsch, Haojin Yang, and Christoph Meinel. Signature embedding:
    Writer independent offline signature verification with deep metric learning. In
    International symposium on visual computing, 2016.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Hannes Rantzsch, Haojin Yang 和 Christoph Meinel。签名嵌入：使用深度度量学习的与作者无关的离线签名验证。在视觉计算国际研讨会上，2016年。'
- en: '[231] Zehua Zhang, Xiangqian Liu, and Yan Cui. Multi-phase offline signature
    verification system using deep convolutional generative adversarial networks.
    In 2016 9th international Symposium on Computational Intelligence and Design,
    volume 2, pages 103–107\. IEEE, 2016.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Zehua Zhang, Xiangqian Liu 和 Yan Cui。使用深度卷积生成对抗网络的多阶段离线签名验证系统。在2016年第9届国际计算智能与设计研讨会上，第2卷，第103–107页。IEEE，2016。'
- en: '[232] Luiz G Hafemann, Robert Sabourin, and Luiz S Oliveira. Learning features
    for offline handwritten signature verification using deep convolutional neural
    networks. Pattern Recognition, 70:163–176, 2017.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] Luiz G Hafemann, Robert Sabourin 和 Luiz S Oliveira。使用深度卷积神经网络进行离线手写签名验证的特征学习。《模式识别》，70：163–176，2017年。'
- en: '[233] Siyue Wang and Shijie Jia. Signature handwriting identification based
    on generative adversarial networks. In Journal of Physics: Conference Series,
    number 4, 2019.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] Siyue Wang 和 Shijie Jia。基于生成对抗网络的签名手写识别。在《物理学杂志：会议系列》第4期，2019年。'
- en: '[234] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, and Javier Ortega-Garcia.
    Exploring recurrent neural networks for on-line handwritten signature biometrics.
    IEEE Access, 6:5128–5138, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez 和 Javier Ortega-Garcia。探索递归神经网络用于在线手写签名生物识别。《IEEE
    Access》，6：5128–5138，2018年。'
- en: '[235] Cheng Zhang, Wu Liu, Huadong Ma, and Huiyuan Fu. Siamese neural network
    based gait recognition for human identification. In International Conference on
    Acoustics, Speech and Signal Processing. IEEE, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Cheng Zhang, Wu Liu, Huadong Ma 和 Huiyuan Fu。基于Siamese神经网络的人体步态识别。在国际声学、语音和信号处理会议上。IEEE，2016。'
- en: '[236] Munif Alotaibi and Ausif Mahmood. Improved gait recognition based on
    specialized deep convolutional neural network. Computer Vision and Image Understanding,
    164:103–110, 2017.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] Munif Alotaibi 和 Ausif Mahmood。基于专业深度卷积神经网络的改进步态识别。《计算机视觉与图像理解》，164：103–110，2017年。'
- en: '[237] Thomas Wolf, Mohammadreza Babaee, and Gerhard Rigoll. Multi-view gait
    recognition using 3d convolutional neural networks. In International Conference
    on Image Processing, pages 4165–4169\. IEEE, 2016.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Thomas Wolf, Mohammadreza Babaee 和 Gerhard Rigoll。使用3D卷积神经网络的多视角步态识别。在国际图像处理会议上，第4165–4169页。IEEE，2016。'
- en: '[238] Xin Chen, Jian Weng, Wei Lu, and Jiaming Xu. Multi-gait recognition based
    on attribute discovery. IEEE transactions on pattern analysis and machine intelligence,
    40(7):1697–1710, 2017.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Xin Chen, Jian Weng, Wei Lu 和 Jiaming Xu。基于属性发现的多步态识别。《IEEE模式分析与机器智能交易》，40(7)：1697–1710，2017年。'
- en: '[239] Casia gait database. http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Casia步态数据库。http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71。'
- en: '[240] Shuai Zheng, Junge Zhang, Kaiqi Huang, Ran He, and Tieniu Tan. Robust
    view transformation model for gait recognition. In International Conference on
    Image Processing. IEEE, 2011.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Shuai Zheng, Junge Zhang, Kaiqi Huang, Ran He, 和 Tieniu Tan. 用于步态识别的鲁棒视图变换模型。在国际图像处理会议上。IEEE，2011年。'
- en: '[241] Osaka gait database. http://www.am.sanken.osaka-u.ac.jp/BiometricDB/GaitTM.html.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] 大阪步态数据库。http://www.am.sanken.osaka-u.ac.jp/BiometricDB/GaitTM.html。'
- en: '[242] Y. Makihara, H. Mannami, A. Tsuji, M.A. Hossain, K. Sugiura, A. Mori,
    and Y. Yagi. The ou-isir gait database comprising the treadmill dataset. IPSJ
    Trans. on Computer Vision and Applications, 4:53–62, 2012.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] Y. Makihara, H. Mannami, A. Tsuji, M.A. Hossain, K. Sugiura, A. Mori,
    和 Y. Yagi. 包含跑步机数据集的ou-isir步态数据库。《信息处理学会计算机视觉与应用学报》，4:53–62, 2012年。'
- en: '[243] Haruyuki Iwama, Mayu Okumura, Yasushi Makihara, and Yasushi Yagi. The
    ou-isir gait database comprising the large population dataset and performance
    evaluation of gait recognition. IEEE Transactions on Information Forensics and
    Security, 7(5):1511–1521, 2012.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Haruyuki Iwama, Mayu Okumura, Yasushi Makihara, 和 Yasushi Yagi. 包含大规模人群数据集的ou-isir步态数据库及步态识别性能评估。《IEEE信息取证与安全学报》，7(5):1511–1521,
    2012年。'
- en: '[244] Ju Han and Bir Bhanu. Individual recognition using gait energy image.
    IEEE transactions on pattern analysis and machine intelligence, 28(2):316–322,
    2005.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Ju Han 和 Bir Bhanu. 使用步态能量图进行个体识别。《IEEE模式分析与机器智能学报》，28(2):316–322, 2005年。'
- en: '[245] Francesco Battistone and Alfredo Petrosino. Tglstm: A time based graph
    deep learning approach to gait recognition. Pattern Recognition Letters, 126:132–138,
    2019.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] Francesco Battistone 和 Alfredo Petrosino. Tglstm：一种基于时间的图深度学习步态识别方法。《模式识别快报》，126:132–138,
    2019年。'
- en: '[246] Qin Zou, Yanling Wang, Yi Zhao, Qian Wang, Chao Shen, and Qingquan Li.
    Deep learning based gait recognition using smartphones in the wild. arXiv preprint
    arXiv:1811.00338, 2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] Qin Zou, Yanling Wang, Yi Zhao, Qian Wang, Chao Shen, 和 Qingquan Li.
    使用智能手机在实际环境中进行基于深度学习的步态识别。arXiv预印本 arXiv:1811.00338, 2018年。'
- en: '[247] Dong Chen, Xudong Cao, Liwei Wang, Fang Wen, and Jian Sun. Bayesian face
    revisited: A joint formulation. In European conference on computer vision, pages
    566–579. Springer, 2012.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] Dong Chen, Xudong Cao, Liwei Wang, Fang Wen, 和 Jian Sun. 贝叶斯人脸再探：一种联合公式。在欧洲计算机视觉会议上，页面566–579。Springer，2012年。'
- en: '[248] Thomas Berg and Peter N Belhumeur. Tom-vs-pete classifiers and identity-preserving
    alignment for face verification. In BMVC, volume 2, page 7\. Citeseer, 2012.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] Thomas Berg 和 Peter N Belhumeur. Tom-vs-pete分类器和用于人脸验证的身份保留对齐。在BMVC，卷2，页面7。Citeseer，2012年。'
- en: '[249] Jiankang Deng, Yuxiang Zhou, and Stefanos Zafeiriou. Marginal loss for
    deep face recognition. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition Workshops, pages 60–68, 2017.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] Jiankang Deng, Yuxiang Zhou, 和 Stefanos Zafeiriou. 深度人脸识别的边际损失。在IEEE计算机视觉与模式识别研讨会论文集中，页面60–68，2017年。'
- en: '[250] Bhavesh Pandya, Georgina Cosma, Ali A Alani, Aboozar Taherkhani, Vinayak
    Bharadi, and TM McGinnity. Fingerprint classification using a deep convolutional
    neural network. In 2018 4th International Conference on Information Management
    (ICIM), pages 86–91\. IEEE, 2018.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] Bhavesh Pandya, Georgina Cosma, Ali A Alani, Aboozar Taherkhani, Vinayak
    Bharadi, 和 TM McGinnity. 使用深度卷积神经网络的指纹分类。在2018年第4届国际信息管理会议（ICIM）上，页面86–91。IEEE，2018年。'
- en: '[251] J Jenkin Winston and D Jude Hemanth. A comprehensive review on iris image-based
    biometric system. Soft Computing, 23(19):9361–9384, 2019.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] J Jenkin Winston 和 D Jude Hemanth. 基于虹膜图像的生物识别系统综述。《软计算》，23(19):9361–9384,
    2019年。'
- en: '[252] Punam Kumari and KR Seeja. Periocular biometrics: A survey. Journal of
    King Saud University-Computer and Information Sciences, 2019.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] Punam Kumari 和 KR Seeja. 周边生物识别技术：综述。《沙特大学计算机与信息科学学报》，2019年。'
- en: '[253] RM Farouk. Iris recognition based on elastic graph matching and gabor
    wavelets. Computer Vision and Image Understanding, 115(8):1239–1244, 2011.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] RM Farouk. 基于弹性图匹配和Gabor小波的虹膜识别。《计算机视觉与图像理解》，115(8):1239–1244, 2011年。'
- en: '[254] Yuniol Alvarez-Betancourt and Miguel Garcia-Silvente. A keypoints-based
    feature extraction method for iris recognition under variable image quality conditions.
    Knowledge-Based Systems, 92:169–182, 2016.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Yuniol Alvarez-Betancourt 和 Miguel Garcia-Silvente. 一种基于关键点的特征提取方法，用于在图像质量变化条件下进行虹膜识别。《知识基系统》，92:169–182,
    2016年。'
- en: '[255] Zijing Zhao and Ajay Kumar. Accurate periocular recognition under less
    constrained environment using semantics-assisted convolutional neural network.
    IEEE Transactions on Information Forensics and Security, 12(5):1017–1030, 2016.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] Zijing Zhao 和 Ajay Kumar. 在较少约束环境下，使用语义辅助卷积神经网络进行准确的周边识别。《IEEE信息取证与安全学报》，12(5):1017–1030,
    2016年。'
- en: '[256] Imad Rida, Romain Herault, Gian Luca Marcialis, and Gilles Gasso. Palmprint
    recognition with an efficient data driven ensemble classifier. Pattern Recognition
    Letters, 126:21–30, 2019.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] Imad Rida、Romain Herault、Gian Luca Marcialis 和 Gilles Gasso。使用高效数据驱动集成分类器的掌纹识别。《模式识别快报》，126:21–30，2019年。'
- en: '[257] Xiangyu Xu, Nuoya Xu, Huijie Li, and Qi Zhu. Multi-spectral palmprint
    recognition with deep multi-view representation learning. In International Conference
    on Machine Learning and Intelligent Communications, pages 748–758\. Springer,
    2019.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Xiangyu Xu、Nuoya Xu、Huijie Li 和 Qi Zhu。利用深度多视角表示学习的多光谱掌纹识别。在《国际机器学习与智能通信会议》，页748–758。Springer，2019年。'
- en: '[258] Aurelia Michele, Vincent Colin, and Diaz D Santika. Mobilenet convolutional
    neural networks and support vector machines for palmprint recognition. Procedia
    Computer Science, 157:110–117, 2019.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] Aurelia Michele、Vincent Colin 和 Diaz D Santika。用于掌纹识别的Mobilenet卷积神经网络和支持向量机。《计算机科学程序》，157:110–117，2019年。'
- en: '[259] Amin Jalali, Rommohan Mallipeddi, and Minho Lee. Deformation invariant
    and contactless palmprint recognition using convolutional neural network. In Proceedings
    of the 3rd International Conference on Human-Agent Interaction, pages 209–212\.
    ACM, 2015.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] Amin Jalali、Rommohan Mallipeddi 和 Minho Lee。使用卷积神经网络的变形不变和无接触掌纹识别。在第3届国际人机交互会议论文集，页209–212。ACM，2015年。'
- en: '[260] Liang Tian and Zhichun Mu. Ear recognition based on deep convolutional
    network. In International Congress on Image and Signal Processing, BioMedical
    Engineering and Informatics, pages 437–441\. IEEE, 2016.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] Liang Tian 和 Zhichun Mu。基于深度卷积网络的耳部识别。在《国际图像与信号处理、生物医学工程与信息学大会》，页437–441。IEEE，2016年。'
- en: '[261] David A Van Leeuwen and Niko Brümmer. An introduction to application-independent
    evaluation of speaker recognition systems. In Speaker classification I, pages
    330–353\. Springer, 2007.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] David A Van Leeuwen 和 Niko Brümmer。说话人识别系统应用无关评估介绍。在《说话人分类 I》，页330–353。Springer，2007年。'
- en: '[262] Suwon Shon, Hao Tang, and James Glass. Frame-level speaker embeddings
    for text-independent speaker recognition and analysis of end-to-end model. In
    Spoken Language Technology Workshop (SLT). IEEE, 2018.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] Suwon Shon、Hao Tang 和 James Glass。用于文本无关说话人识别的帧级说话人嵌入及端到端模型分析。在《口语语言技术研讨会》（SLT）。IEEE，2018年。'
- en: '[263] Weicheng Cai, Jinkun Chen, and Ming Li. Exploring the encoding layer
    and loss function in end-to-end speaker and language recognition system. arXiv
    preprint arXiv:1804.05160, 2018.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] Weicheng Cai、Jinkun Chen 和 Ming Li。探索端到端说话人和语言识别系统中的编码层和损失函数。arXiv 预印本
    arXiv:1804.05160，2018年。'
- en: '[264] Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda. Attentive statistics
    pooling for deep speaker embedding. arXiv preprint arXiv:1803.10963, 2018.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] Koji Okabe、Takafumi Koshinaka 和 Koichi Shinoda。用于深度说话人嵌入的注意力统计汇聚。arXiv
    预印本 arXiv:1803.10963，2018年。'
- en: '[265] Mahdi Hajibabaei and Dengxin Dai. Unified hypersphere embedding for speaker
    recognition. arXiv preprint arXiv:1807.08312, 2018.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] Mahdi Hajibabaei 和 Dengxin Dai。用于说话人识别的统一超球体嵌入。arXiv 预印本 arXiv:1807.08312，2018年。'
- en: '[266] Weidi Xie, Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Utterance-level
    aggregation for speaker recognition in the wild. In ICASSP 2019-2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5791–5795\.
    IEEE, 2019.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] Weidi Xie、Arsha Nagrani、Joon Son Chung 和 Andrew Zisserman。野外说话人识别的发话层级聚合。在ICASSP
    2019-2019 IEEE国际声学、语音与信号处理会议（ICASSP），页5791–5795。IEEE，2019年。'
- en: '[267] Luiz G Hafemann, Robert Sabourin, and Luiz S Oliveira. Writer-independent
    feature learning for offline signature verification using deep convolutional neural
    networks. In International Joint Conference on Neural Networks, pages 2576–2583\.
    IEEE, 2016.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] Luiz G Hafemann、Robert Sabourin 和 Luiz S Oliveira。使用深度卷积神经网络的无作者特征学习用于离线签名验证。在国际联合神经网络会议，页2576–2583。IEEE，2016年。'
- en: '[268] Mustafa Berkay Yılmaz and Berrin Yanıkoğlu. Score level fusion of classifiers
    in off-line signature verification. Information Fusion, 32:109–119, 2016.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] Mustafa Berkay Yılmaz 和 Berrin Yanıkoğlu。离线签名验证中分类器的分数级融合。《信息融合》，32:109–119，2016年。'
- en: '[269] Victor LF Souza, Adriano LI Oliveira, and Robert Sabourin. A writer-independent
    approach for offline signature verification using deep convolutional neural networks
    features. In Brazilian Conference on Intelligent Systems (BRACIS), pages 212–217\.
    IEEE, 2018.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] Victor LF Souza、Adriano LI Oliveira 和 Robert Sabourin。基于深度卷积神经网络特征的无作者离线签名验证方法。在巴西智能系统会议（BRACIS），页212–217。IEEE，2018年。'
- en: '[270] Kalaivani Sundararajan and Damon L Woodard. Deep learning for biometrics:
    a survey. ACM Computing Surveys (CSUR), 51(3):65, 2018.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] Kalaivani Sundararajan 和 Damon L Woodard. 生物识别中的深度学习：综述。ACM 计算调查 (CSUR)，51(3):65,
    2018。'
- en: '[271] Worapan Kusakunniran. Recognizing gaits on spatio-temporal feature domain.
    IEEE Transactions on Information Forensics and Security, 9(9):1416–1423, 2014.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] Worapan Kusakunniran. 在时空特征域中识别步态。IEEE 信息取证与安全交易，9(9):1416–1423, 2014。'
- en: '[272] Worapan Kusakunniran, Qiang Wu, Jian Zhang, Hongdong Li, and Liang Wang.
    Recognizing gaits across views through correlated motion co-clustering. IEEE Transactions
    on Image Processing, 23(2):696–709, 2013.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] Worapan Kusakunniran、Qiang Wu、Jian Zhang、Hongdong Li 和 Liang Wang. 通过相关运动共聚类识别跨视角步态。IEEE
    图像处理交易，23(2):696–709, 2013。'
- en: '[273] Daigo Muramatsu, Yasushi Makihara, and Yasushi Yagi. View transformation
    model incorporating quality measures for cross-view gait recognition. IEEE transactions
    on cybernetics, 46(7):1602–1615, 2015.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] Daigo Muramatsu、Yasushi Makihara 和 Yasushi Yagi. 跨视角步态识别的视角变换模型，结合质量度量。IEEE
    网络学报，46(7):1602–1615, 2015。'
- en: '[274] Daigo Muramatsu, Yasushi Makihara, and Yasushi Yagi. Cross-view gait
    recognition by fusion of multiple transformation consistency measures. IET Biometrics,
    4(2):62–73, 2015.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] Daigo Muramatsu、Yasushi Makihara 和 Yasushi Yagi. 通过融合多重变换一致性度量进行跨视角步态识别。IET
    生物识别，4(2):62–73, 2015。'
- en: '[275] Chao Yan, Bailing Zhang, and Frans Coenen. Multi-attributes gait identification
    by convolutional neural networks. In International Congress on Image and Signal
    Processing (CISP), pages 642–647\. IEEE, 2015.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] Chao Yan、Bailing Zhang 和 Frans Coenen. 通过卷积神经网络进行多属性步态识别。在国际图像与信号处理大会
    (CISP)，页面 642–647。IEEE, 2015。'
- en: '[276] Zifeng Wu, Yongzhen Huang, Liang Wang, Xiaogang Wang, and Tieniu Tan.
    A comprehensive study on cross-view gait based human identification with deep
    cnns. IEEE transactions on pattern analysis and machine intelligence, 39(2):209–226,
    2016.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] Zifeng Wu、Yongzhen Huang、Liang Wang、Xiaogang Wang 和 Tieniu Tan. 基于深度卷积神经网络的跨视角步态人类识别的综合研究。IEEE
    模式分析与机器智能交易，39(2):209–226, 2016。'
- en: '[277] Chao Li, Xin Min, Shouqian Sun, Wenqian Lin, and Zhichuan Tang. Deepgait:
    a learning deep convolutional representation for view-invariant gait recognition
    using joint bayesian. Applied Sciences, 7(3):210, 2017.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] Chao Li、Xin Min、Shouqian Sun、Wenqian Lin 和 Zhichuan Tang. Deepgait：使用联合贝叶斯进行视角不变步态识别的深度卷积表示学习。应用科学，7(3):210,
    2017。'
- en: '[278] Kohei Shiraga, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, and Yasushi
    Yagi. Geinet: View-invariant gait recognition using a convolutional neural network.
    In 2016 international conference on biometrics (ICB), pages 1–8\. IEEE, 2016.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] Kohei Shiraga、Yasushi Makihara、Daigo Muramatsu、Tomio Echigo 和 Yasushi
    Yagi. Geinet：使用卷积神经网络的视角不变步态识别。在2016年国际生物识别会议 (ICB)，页面 1–8。IEEE, 2016。'
- en: '[279] Longlong Jing and Yingli Tian. Self-supervised visual feature learning
    with deep neural networks: A survey. arXiv preprint arXiv:1902.06162, 2019.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] Longlong Jing 和 Yingli Tian. 基于深度神经网络的自监督视觉特征学习：综述。arXiv 预印本 arXiv:1902.06162,
    2019。'
- en: '[280] Arun Ross and Anil K Jain. Multimodal biometrics: an overview. In 2004
    12th European Signal Processing Conference, pages 1221–1224\. IEEE, 2004.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] Arun Ross 和 Anil K Jain. 多模态生物识别：概述。在2004年第十二届欧洲信号处理大会，页面 1221–1224。IEEE,
    2004。'
- en: '[281] Arun Ross and Anil Jain. Information fusion in biometrics. Pattern recognition
    letters, 24(13):2115–2125, 2003.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] Arun Ross 和 Anil Jain. 生物识别中的信息融合。模式识别通讯，24(13):2115–2125, 2003。'
- en: '[282] Munawar Hayat, Mohammed Bennamoun, and Senjian An. Deep reconstruction
    models for image set classification. IEEE transactions on pattern analysis and
    machine intelligence, 37(4):713–727, 2014.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] Munawar Hayat、Mohammed Bennamoun 和 Senjian An. 用于图像集分类的深度重建模型。IEEE 模式分析与机器智能交易，37(4):713–727,
    2014。'
