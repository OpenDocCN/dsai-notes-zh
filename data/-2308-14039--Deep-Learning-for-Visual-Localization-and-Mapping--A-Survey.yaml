- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:37:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2308.14039] Deep Learning for Visual Localization and Mapping: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2308.14039] 深度学习在视觉定位与地图构建中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.14039](https://ar5iv.labs.arxiv.org/html/2308.14039)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.14039](https://ar5iv.labs.arxiv.org/html/2308.14039)
- en: 'Deep Learning for Visual Localization and Mapping: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在视觉定位与地图构建中的应用：综述
- en: 'Changhao Chen*, Bing Wang*, Chris Xiaoxuan Lu, Niki Trigoni, Andrew Markham
    Changhao Chen is with the College of Intelligence Science and Technology, National
    University of Defense Technology, Changsha, 410073, China Bing Wang is with the
    Department of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic
    University, HKSAR, China Chris Xiaoxuan Lu is with the School of Informatics,
    University of Edinburgh, Edinburgh, EH8 9AB, United Kingdom Niki Trigoni and Andrew
    Markham are with the Department of Computer Science, University of Oxford, Oxford
    OX1 3QD, United Kingdom *Changhao Chen and Bing Wang are co-first-authors. Corresponding
    author: Changhao Chen (changhao.chen66@outlook.com) This work was supported by
    National Natural Science Foundation of China (NFSC) under the Grant Number of
    62103427 and 42301520, and EPSRC Program “ACE-OPS: From Autonomy to Cognitive
    assistance in Emergency OPerationS” (Grant number: EP/S030832/1) . Changhao Chen
    is sponsored by the Young Elite Scientist Sponsorship Program by CAST (No. YESS20220181)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Changhao Chen*、Bing Wang*、Chris Xiaoxuan Lu、Niki Trigoni、Andrew Markham Changhao
    Chen 现任职于国防科技大学智能科学与技术学院，中国长沙，410073 Bing Wang 现任职于香港理工大学航空航天工程系，中国香港特别行政区 Chris
    Xiaoxuan Lu 现任职于爱丁堡大学信息学学院，英国爱丁堡，EH8 9AB Niki Trigoni 和 Andrew Markham 现任职于牛津大学计算机科学系，英国牛津，OX1
    3QD *Changhao Chen 和 Bing Wang 为共同第一作者。通讯作者：Changhao Chen (changhao.chen66@outlook.com)
    本工作得到了中国国家自然科学基金（NFSC）资助，资助编号为62103427和42301520，以及EPSRC计划“ACE-OPS: 从自主到紧急操作中的认知辅助”（资助编号：EP/S030832/1）。Changhao
    Chen 还获得了中国科学院青年精英科学家资助计划（编号：YESS20220181）的资助。'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning based localization and mapping approaches have recently emerged
    as a new research direction and receive significant attentions from both industry
    and academia. Instead of creating hand-designed algorithms based on physical models
    or geometric theories, deep learning solutions provide an alternative to solve
    the problem in a data-driven way. Benefiting from the ever-increasing volumes
    of data and computational power on devices, these learning methods are fast evolving
    into a new area that shows potentials to track self-motion and estimate environmental
    model accurately and robustly for mobile agents. In this work, we provide a comprehensive
    survey, and propose a taxonomy for the localization and mapping methods using
    deep learning. This survey aims to discuss two basic questions: whether deep learning
    is promising to localization and mapping; how deep learning should be applied
    to solve this problem. To this end, a series of localization and mapping topics
    are investigated, from the learning based visual odometry, global relocalization,
    to mapping, and simultaneous localization and mapping (SLAM). It is our hope that
    this survey organically weaves together the recent works in this vein from robotics,
    computer vision and machine learning communities, and serves as a guideline for
    future researchers to apply deep learning to tackle the problem of visual localization
    and mapping.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的定位与地图构建方法近年来作为一种新的研究方向出现，并受到工业界和学术界的广泛关注。与基于物理模型或几何理论创建的手工设计算法不同，深度学习解决方案提供了一种数据驱动的替代方案来解决问题。得益于数据和设备计算能力的持续增长，这些学习方法正快速演变成一个新领域，展示了在移动体上跟踪自我运动和准确、稳健地估计环境模型的潜力。在这项工作中，我们提供了一项全面的综述，并提出了基于深度学习的定位与地图构建方法的分类法。本文旨在讨论两个基本问题：深度学习在定位与地图构建中是否有前景；深度学习应如何应用于解决这一问题。为此，调查了一系列定位与地图构建的主题，从基于学习的视觉里程计、全球重定位，到地图构建和同时定位与地图构建（SLAM）。我们希望这项综述能够将机器人技术、计算机视觉和机器学习领域中的近期工作有机地结合起来，并为未来的研究人员提供指导，以便利用深度学习解决视觉定位和地图构建的问题。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep Learning, Visual SLAM, Visual Odometry, Visual-inertial Odometry, Global
    Localization
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、视觉SLAM、视觉里程计、视觉惯性里程计、全球定位
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Localization and mapping serve as essential requirements for both human beings
    and mobile agents. As a motivating example, humans possess the remarkable ability
    to perceive their own motion and the surrounding environment through multisensory
    perception. They heavily rely on this awareness to determine their location and
    navigate through intricate three-dimensional spaces. In a similar vein, mobile
    agents, encompassing a diverse range of robots like self-driving vehicles, delivery
    drones, and home service robots, must possess the capability to perceive their
    environment and estimate positional states through onboard sensors. These agents
    actively engage in sensing their surroundings and autonomously make decisions
    [[1](#bib.bib1)]. Equivalently, the integration of emerging technologies like
    Augmented Reality (AR) and Virtual Reality (VR) intertwines the virtual and physical
    realms, making it imperative for machines to possess perceptual awareness. This
    awareness forms the foundation for seamless interaction between humans and machines.
    Furthermore, the applications of these concepts extend to mobile and wearable
    devices such as smartphones, wristbands, and Internet-of-Things (IoT) devices.
    These devices offer a wide array of location-based services, ranging from pedestrian
    navigation and sports/activity monitoring to emergency response.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 定位和地图绘制是人类和移动智能体的基本要求。作为一个激励性的例子，人类具备通过多感官感知来感知自身运动和周围环境的非凡能力。他们在决定位置和在复杂的三维空间中导航时，严重依赖这种意识。同样，移动智能体，包括自驾车、配送无人机和家用服务机器人等多种机器人，必须具备通过车载传感器感知环境和估计位置状态的能力。这些智能体积极参与环境感知，并自主做出决策[[1](#bib.bib1)]。同样，增强现实（AR）和虚拟现实（VR）等新兴技术的集成将虚拟与现实领域紧密交织，使得机器具备感知意识变得至关重要。这种意识构成了人类与机器之间无缝互动的基础。此外，这些概念的应用扩展到智能手机、手环和物联网（IoT）设备等移动和可穿戴设备。这些设备提供了广泛的基于位置的服务，从步行导航和运动/活动监测到紧急响应。
- en: '![Refer to caption](img/de080c0ebfd06192212c589ef40cf89a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/de080c0ebfd06192212c589ef40cf89a.png)'
- en: 'Figure 1: A localization and mapping system exploits on-board sensors to perceive
    self-motion, global pose, scene geometry, and semantics. (a) Traditional model-based
    solutions employ hand-designed algorithms to transform input sensor data into
    desired output values. (c) Data-driven solutions, on the other hand, leverage
    learning models to construct this mapping function. (b) Hybrid approaches integrate
    both hand-designed algorithms and learning models for improved performance.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：定位和地图绘制系统利用车载传感器感知自我运动、全球姿态、场景几何和语义。（a）传统的基于模型的解决方案使用手工设计的算法将输入传感器数据转换为期望的输出值。（c）数据驱动的解决方案则利用学习模型来构建这种映射函数。（b）混合方法将手工设计的算法和学习模型相结合，以提高性能。
- en: Enabling a high level of autonomy for these and other digital agents requires
    precise and robust localization, while incrementally building and maintaining
    a world model, with the capability to continuously process new information and
    adapt to various scenarios. In this work, localization broadly refers to the ability
    to obtain internal system states of robot motion, including locations, orientations
    and velocities, whilst mapping indicates the capacity to perceive external environmental
    states, including scene geometry, appearance and semantics. They can act individually
    to sense internal or external states respectively, or can operate jointly as a
    simultaneous localization and mapping (SLAM) system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为这些以及其他数字智能体提供高水平的自主性需要精准而强大的定位，同时逐步建立和维护一个世界模型，具备持续处理新信息和适应各种场景的能力。在这项工作中，定位广泛指的是获取机器人运动的内部系统状态的能力，包括位置、方向和速度，而地图绘制则指感知外部环境状态的能力，包括场景几何、外观和语义。它们可以单独作用于内部或外部状态，或者作为一个同时定位与地图绘制（SLAM）系统共同操作。
- en: 'The problem of localization and mapping has been studied for decades, with
    a range of algorithms and systems being developed, for example, visual odometry
    [[2](#bib.bib2)], visual-inertial odometry [[3](#bib.bib3)], image-based relocalization[[4](#bib.bib4)],
    place recognition[[5](#bib.bib5)], SLAM [[6](#bib.bib6)]. These algorithms and
    systems have demonstrated their efficacy in supporting a wide range of real-world
    applications, such as delivery robots, self-driving vehicles, and VR devices.
    However, the deployment of these systems is not without challenges. Factors such
    as imperfect sensor measurements, dynamic scenes, adverse lighting conditions,
    and real-world constraints somewhat hinder their practical implementation. In
    light of these limitations, recent advancements in machine learning, particularly
    deep learning, have prompted researchers to explore data-driven approaches as
    an alternative solution. Unlike conventional model-based approaches that rely
    on concrete and explicit algorithms tailored to specific application domains,
    learning-based methods leverage the power of deep neural networks to extract features
    and construct implicit neural models, as shown in Figure [1](#S1.F1 "Figure 1
    ‣ I Introduction ‣ Deep Learning for Visual Localization and Mapping: A Survey").
    By training these networks on large datasets, they learn to obtain ability to
    generate poses and describe scenes, even in challenging environments such as those
    characterized by high dynamics and poor lighting conditions. Consequently, deep
    learning-based localization and mapping methods exhibit good robustness and accuracy
    compared to their traditional counterparts. Deep learning-based localization and
    mapping remain active areas of research, and further investigations are necessary
    to fully understand the strengths and limitations of different approaches.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '定位与建图的问题已经研究了几十年，开发了各种算法和系统，例如视觉里程计 [[2](#bib.bib2)]、视觉惯性里程计 [[3](#bib.bib3)]、基于图像的重定位
    [[4](#bib.bib4)]、地点识别 [[5](#bib.bib5)]、SLAM [[6](#bib.bib6)]。这些算法和系统在支持广泛的现实世界应用方面表现出了其有效性，如送货机器人、自动驾驶车辆和虚拟现实设备。然而，这些系统的部署并非没有挑战。传感器测量不准确、动态场景、不良的光照条件和现实世界的约束等因素在一定程度上阻碍了它们的实际应用。鉴于这些限制，最近机器学习特别是深度学习的进展促使研究人员探索数据驱动的方法作为替代解决方案。与依赖于具体和明确算法的传统模型方法不同，基于学习的方法利用深度神经网络的力量来提取特征和构建隐式神经模型，如图
    [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning for Visual Localization
    and Mapping: A Survey") 所示。通过在大规模数据集上训练这些网络，它们学习到生成姿态和描述场景的能力，即使在高动态和光照条件差等具有挑战性的环境中也是如此。因此，与传统方法相比，基于深度学习的定位和建图方法表现出良好的鲁棒性和准确性。基于深度学习的定位和建图仍然是活跃的研究领域，需要进一步的研究以充分了解不同方法的优缺点。'
- en: 'In this article, we extensively review the existing deep learning based visual
    localization and mapping approaches, and try to explore the answers to the following
    two questions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们广泛回顾了现有的基于深度学习的视觉定位与建图方法，并尝试探索以下两个问题的答案：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 1) Is deep learning promising to visual localization and mapping?
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1) 深度学习对视觉定位与建图是否有前景？
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 2) How can deep learning be applied to solve the problem of visual localization
    and mapping?
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2) 如何将深度学习应用于解决视觉定位与建图的问题？
- en: '![Refer to caption](img/0bc03f12c9501cd9af828e66fb9153d8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0bc03f12c9501cd9af828e66fb9153d8.png)'
- en: 'Figure 2: The taxonomy of deep learning based visual localization and mapping.
    Individual modules can be integrated together into a complete deep learning based
    SLAM system. It is not mandatory to include all modules for the system to function
    effectively. In the diagram, rounded rectangles represent function modules, and
    arrow lines depict the connections between these modules for data input and output.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：基于深度学习的视觉定位与建图的分类。各个模块可以集成到一个完整的基于深度学习的 SLAM 系统中。系统的有效运行并不一定需要包括所有模块。在图中，圆角矩形表示功能模块，箭头线描绘了这些模块之间的数据输入和输出连接。
- en: The two questions will be revisited by the end of this survey. As vision is
    major information source for most mobile agents, this work will focus on vision
    based solutions. The field of deep learning based localization and mapping is
    still relatively new, and there are a growing number of different approaches and
    techniques that have been proposed in recent years. Notably, although the problem
    of localization and mapping falls into the key notion of robotics, the incorporation
    of learning methods progresses in tandem with other research areas such as machine
    learning, computer vision and even natural language processing. This cross-disciplinary
    area thus imposes non-trivial difficulty when comprehensively summarizing related
    works into a survey paper. We hope that our survey can help to promote collaboration
    and knowledge sharing within the research community, and foster new ideas and
    facilitate interdisciplinary research on deep learning based localization and
    mapping. In addition, this survey can help to identify key research challenges
    and open problems in the field, guide future research efforts, and provide guidance
    for researchers and practitioners who are interested in using deep learning solutions
    in their works. To the best of our knowledge, this is the first survey article
    that thoroughly and extensively covers existing work on deep learning for visual
    localization and mapping.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题将在本调查的末尾重新审视。由于视觉是大多数移动代理的主要信息来源，这项工作将重点关注基于视觉的解决方案。深度学习基础的定位和映射领域仍然相对较新，近年来提出了越来越多的不同方法和技术。值得注意的是，虽然定位和映射的问题属于机器人学的核心概念，但学习方法的融入与机器学习、计算机视觉甚至自然语言处理等其他研究领域同步进展。因此，当将相关工作全面总结成调查论文时，这一跨学科领域带来了非凡的困难。我们希望我们的调查能够促进研究社区的合作与知识共享，激发新想法，并促进基于深度学习的定位和映射的跨学科研究。此外，本调查还可以帮助识别该领域的关键研究挑战和开放问题，指导未来的研究工作，并为对深度学习解决方案感兴趣的研究人员和从业者提供指导。据我们所知，这是第一篇彻底且广泛覆盖视觉定位和映射中深度学习现有工作的调查文章。
- en: 'TABLE I: A summary of relevant surveys and tutorials'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：相关调查和教程的总结
- en: '| Year | Content | Reference |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 内容 | 参考文献 |'
- en: '| --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2005 | probalistic SLAM | Thrun et al. [[7](#bib.bib7)] |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 2005 | 概率SLAM | Thrun等 [[7](#bib.bib7)] |'
- en: '| 2006 | SLAM tutorial | Durrant-whyte et al. [[8](#bib.bib8)] |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 2006 | SLAM教程 | Durrant-whyte等 [[8](#bib.bib8)] |'
- en: '| 2010 | pose-graph SLAM | Grisetti et al. [[9](#bib.bib9)] |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | 位姿图SLAM | Grisetti等 [[9](#bib.bib9)] |'
- en: '| 2011 | visual odometry tutorial | Scaramuzza et al. [[10](#bib.bib10)] |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2011 | 视觉里程计教程 | Scaramuzza等 [[10](#bib.bib10)] |'
- en: '| 2015 | visual place recognition | Lowry et al. [[5](#bib.bib5)] |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 视觉地点识别 | Lowry等 [[5](#bib.bib5)] |'
- en: '| 2016 | SLAM in robust-perception age | Cadena et al. [[11](#bib.bib11)] |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | 强鲁棒感知时代的SLAM | Cadena等 [[11](#bib.bib11)] |'
- en: '| 2018 | dynamic SLAM | Saputra et al.[[12](#bib.bib12)] |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 动态SLAM | Saputra等 [[12](#bib.bib12)] |'
- en: '| 2018 | deep learning for robotics | Sunderhauf et al.[[1](#bib.bib1)] |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 机器人学中的深度学习 | Sunderhauf等 [[1](#bib.bib1)] |'
- en: '| 2022 | perception and navigation | Tang et al. [[13](#bib.bib13)] |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | 感知与导航 | Tang等 [[13](#bib.bib13)] |'
- en: '| 2023 | deep learning based SLAM | This survey |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | 基于深度学习的SLAM | 本调查 |'
- en: I-A Comparison with Other Surveys
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 与其他调查的比较
- en: As an established field, the development of SLAM problem has been well summarized
    by several survey papers in literature [[8](#bib.bib8), [14](#bib.bib14)], with
    their focus lying in the conventional model-based localization and mapping approaches.
    The seminal survey [[11](#bib.bib11)] provides a thorough discussion on existing
    SLAM works, reviews the history of development and charts several future directions.
    Although this paper contains a section which briefly discusses deep learning models,
    it does not overview this field comprehensively, especially due to the explosion
    of research in this area of the past five years. Other SLAM survey papers only
    focus on individual flavours of SLAM systems, including the probabilistic formulation
    of SLAM [[7](#bib.bib7)], visual odometry [[10](#bib.bib10)], pose-graph SLAM
    [[9](#bib.bib9)], and SLAM in dynamic environments [[12](#bib.bib12)]. We refer
    readers to these surveys for a better understanding of the conventional solutions
    to SLAM systems. On the other hand, [[1](#bib.bib1)] has a discussion on the applications
    of deep learning to robotics research; however, its main focus is not on localization
    and mapping specifically, but a more general perspective towards the potentials
    and limits of deep learning in a broad context of robotic policy learning, reasoning
    and planning. A recent survey [[13](#bib.bib13)] discusses deep learning based
    perception and navigation. Compared to [[13](#bib.bib13)] that throws a broader
    view on environment perception, motion estimation and reinforcement learning based
    control for autonomous systems, we provide a more comprehensive review and deep
    analysis on odometry estimation, relocalization, mapping and other aspects of
    visual SLAM.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个成熟的领域，SLAM 问题的发展已经在文献中由几篇综述论文进行了很好的总结[[8](#bib.bib8), [14](#bib.bib14)]，它们的重点在于传统的基于模型的定位和地图构建方法。开创性的综述[[11](#bib.bib11)]对现有的
    SLAM 工作进行了深入讨论，回顾了发展历史，并绘制了几个未来方向的图谱。虽然这篇论文包含了一个简要讨论深度学习模型的章节，但并未全面概述这一领域，尤其是由于过去五年该领域研究的爆炸性增长。其他
    SLAM 综述论文只关注 SLAM 系统的个别特性，包括 SLAM 的概率性表述[[7](#bib.bib7)]、视觉里程计[[10](#bib.bib10)]、姿态图
    SLAM [[9](#bib.bib9)]和动态环境中的 SLAM [[12](#bib.bib12)]。我们建议读者参考这些综述，以更好地理解 SLAM
    系统的传统解决方案。另一方面，[[1](#bib.bib1)]讨论了深度学习在机器人研究中的应用；然而，它的主要关注点不是在定位和地图构建上，而是对深度学习在机器人策略学习、推理和规划广泛背景下的潜力和限制的更一般性的观点。最近的综述[[13](#bib.bib13)]讨论了基于深度学习的感知和导航。相比于[[13](#bib.bib13)]对环境感知、运动估计和基于强化学习的控制进行的更广泛视角的探讨，我们提供了对视觉
    SLAM 中的里程计估计、重定位、地图构建和其他方面的更全面的回顾和深入分析。
- en: I-B Survey Organization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 综述组织
- en: 'The remainder of the paper is organized as follows: Section 2 offers an overview
    and presents a taxonomy of existing deep learning based localization and mapping;
    Sections 3, 4, 5, 6 discuss the existing deep learning approaches on incremental
    motion (odometry) estimation, global relocalization, mapping, and SLAM back-ends
    respectively; Sections 7 and 8 review the learning based uncertainty estimation
    and sensor fusion methods; and finally Section 9 concludes the article, and discusses
    the limitations and future prospects.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：第 2 节提供了对现有基于深度学习的定位和地图构建的概述和分类；第 3、4、5、6 节分别讨论了增量运动（里程计）估计、全局重定位、地图构建和
    SLAM 后端的现有深度学习方法；第 7 和 8 节回顾了基于学习的不确定性估计和传感器融合方法；最后，第 9 节总结了文章，并讨论了局限性和未来前景。
- en: II Taxonomy of Existing Approaches
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 现有方法的分类
- en: 'From the perspective of learning approaches, we provide a taxonomy of existing
    deep learning based visual localization and mapping, to connect the fields of
    robotics, computer vision and machine learning. Based on their main technical
    contributions towards a complete SLAM system, related approaches can be broadly
    categorized into four main types in our context: incremental motion estimation
    (visual odometry), global relocalization, mapping, and loop-closing and SLAM Back-ends,
    as illustrated by the taxonomy shown in Figure [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Deep Learning for Visual Localization and Mapping: A Survey"):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '从学习方法的角度来看，我们提供了现有基于深度学习的视觉定位和地图构建的分类，以连接机器人、计算机视觉和机器学习领域。根据它们对完整 SLAM 系统的主要技术贡献，相关方法可以在我们的背景下广泛地分为四大类：增量运动估计（视觉里程计）、全局重定位、地图构建，以及闭环和
    SLAM 后端，如图 [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning for Visual Localization
    and Mapping: A Survey") 所示。'
- en: A) Incremental Motion Estimation concerns the calculation of the incremental
    change in pose, in terms of translation and rotation, between two or more frames
    of sensor data. It continuously tracks self-motion, and is followed by a process
    to integrate these pose changes with respect to an initial state to derive global
    pose. Incremental motion estimation, i.e. visual odometry (VO), can be used in
    providing pose information in a scenario without a pre-built map or as odometry
    motion model to assist the feedback loop of robot control. Deep learning is applied
    to estimate motion transformations from various sensor measurements in an end-to-end
    fashion or extract useful features to support a hybrid system.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: A) 增量运动估计关注于计算两个或更多传感器数据帧之间姿态的增量变化，包括平移和旋转。它持续跟踪自我运动，并随后通过将这些姿态变化与初始状态结合的过程来推导全局姿态。增量运动估计，即视觉里程计（VO），可以在没有预构建地图的情况下提供姿态信息，或者作为里程计运动模型来辅助机器人控制的反馈环。深度学习被应用于从各种传感器测量中以端到端的方式估计运动变换，或提取有用特征以支持混合系统。
- en: B) Global Relocalization retrieves the global pose of mobile agents in a known
    scene with prior knowledge. This is achieved by matching the inquiry input data
    with a pre-built map or other spatial references. It can be leveraged to reduce
    the pose drift of a dead reckoning system or retrieve the absolute pose when motion
    tracking is lost [[7](#bib.bib7)]. Deep learning is used to tackle the tricky
    data association problem that is complicated by the changes in views, illumination,
    weather and scene dynamics, between the inquiry data and map.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: B) 全球重定位通过匹配查询输入数据与预构建地图或其他空间参考来检索已知场景中的移动代理的全球姿态。这可以用来减少死记系统的姿态漂移，或在运动跟踪丢失时检索绝对姿态[[7](#bib.bib7)]。深度学习用于解决由于视角、光照、天气和场景动态变化而使数据关联问题变得复杂的难题。
- en: C) Mapping builds and reconstructs a consistent environmental model to describe
    the surroundings. Mapping can be used to provide environment information for human
    operators or high-level robot tasks, constrain the error drifts of self-motion
    tracking, and retrieve the inquiry observation for global localization [[11](#bib.bib11)].
    Deep learning is leveraged as a useful tool to discover scene geometry and semantics
    from high-dimensional raw data for mapping. Deep learning based mapping methods
    are sub-divided into geometric, semantic, and implicit mapping, depending on whether
    the neural network learns the explicit geometry, or semantics of a scene, or encodes
    the scene into implicit neural representation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: C) 建图构建并重建一个一致的环境模型来描述周围环境。建图可以用于为人类操作员或高级机器人任务提供环境信息，限制自我运动跟踪的误差漂移，并检索全球定位所需的查询观察[[11](#bib.bib11)]。深度学习被作为一种有用工具，利用高维原始数据发现场景几何和语义。基于深度学习的建图方法被细分为几何建图、语义建图和隐式建图，取决于神经网络是否学习场景的显式几何或语义，或将场景编码为隐式神经表示。
- en: 'D) Loop-closing and SLAM Back-ends detect loop closures and optimize the aforementioned
    incremental motion estimation, global localization and mapping modules to boost
    the performance in a simultaneous localization and mapping (SLAM) system. These
    modules perform to ensure the consistency of entire system as follows: *local
    optimization* ensures the local consistency of camera motion and scene geometry;
    once a loop closure is detected by *loop-closing module*, system error drifts
    can be mitigated by *global optimization*.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: D) 回环闭合和SLAM后端检测回环闭合，并优化上述增量运动估计、全局定位和建图模块，以提升同时定位与建图（SLAM）系统的性能。这些模块执行以下功能，以确保整个系统的一致性：*局部优化*确保相机运动和场景几何的一致性；一旦*回环闭合模块*检测到回环闭合，*全局优化*可以减轻系统误差漂移。
- en: 'Besides the modules mentioned above, other modules that also contribute to
    a SLAM system include:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除上述模块外，还有其他对SLAM系统做出贡献的模块包括：
- en: E) Uncertainty Estimation provides a metric of belief in the learned poses and
    mapping, critical to probabilistic sensor fusion and back-end optimization in
    a SLAM system.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: E) 不确定性估计提供了对学习到的姿态和地图的信心度量，这对SLAM系统中的概率传感器融合和后端优化至关重要。
- en: F) Sensor Fusion exploits the complementary properties of each sensor modality,
    and aims to discover the suitable data fusion strategy such that more accurate
    and robust localization and mapping can be achieved.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: F) 传感器融合利用每种传感器模态的互补特性，旨在发现合适的数据融合策略，以实现更准确和鲁棒的定位与建图。
- en: In the following sections, we will discuss these components in details.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细讨论这些组件。
- en: III Incremental Motion Estimation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 增量运动估计
- en: We begin with incremental motion (odometry) estimation, i.e. visual odometry
    (VO), which continuously tracks camera egomotion and yields motion transformations.
    Given an initial state, global trajectories are reconstructed by integrating these
    incremental poses. Thus, it is critical to keep the estimate of each motion transformation
    accurate enough to ensure high-prevision localization in a global scale. This
    section presents deep learning approaches to achieve visual odometry.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从增量运动（里程计）估计开始，即视觉里程计（VO），它持续跟踪相机自运动并产生运动变换。在给定初始状态的情况下，通过整合这些增量姿态来重建全局轨迹。因此，保持每个运动变换的估计准确是至关重要的，以确保在全球尺度上的高精度定位。本节介绍了实现视觉里程计的深度学习方法。
- en: 'Deep learning is capable of extracting high-level feature representations from
    raw images directly, and thereby provides an alternative to solve visual odometry
    (VO) problem, without requiring hand-crafted feature detectors. Existing deep
    learning based VO models can be categorized into *end-to-end VO* and *hybrid VO*,
    depending on whether they are purely DNN based or a combination of classical VO
    algorithms and DNNs. Depending on the availability of ground-truth labels in the
    training phase, end-to-end VO systems are further classified into *supervised*
    VO and *unsupervised* VO. Table [II](#S3.T2 "TABLE II ‣ III-A Supervised Learning
    of Visual Odometry ‣ III Incremental Motion Estimation ‣ Deep Learning for Visual
    Localization and Mapping: A Survey") lists and compares deep learning based visual
    odometry methods.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习能够直接从原始图像中提取高级特征表示，因此提供了解决视觉里程计（VO）问题的替代方案，而无需手工制作特征检测器。现有的基于深度学习的 VO 模型可以根据是否完全基于
    DNN 或者是经典 VO 算法与 DNN 的结合，分为*端到端 VO*和*混合 VO*。根据训练阶段的真实标签的可用性，端到端 VO 系统进一步分为*监督*
    VO 和*无监督* VO。表 [II](#S3.T2 "TABLE II ‣ III-A Supervised Learning of Visual Odometry
    ‣ III Incremental Motion Estimation ‣ Deep Learning for Visual Localization and
    Mapping: A Survey") 列出了并比较了基于深度学习的视觉里程计方法。'
- en: '![Refer to caption](img/8891c04ba30181e556e811f64f1d80a1.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8891c04ba30181e556e811f64f1d80a1.png)'
- en: 'Figure 3: The typical structure of supervised learning of visual odometry (reprint
    from DeepVO [[15](#bib.bib15)]), self-supervised learning of visual odometry,
    (reprint from SfmLearner [[16](#bib.bib16)]) and hybrid visual odometry, (reprint
    from D3VO [[17](#bib.bib17)]).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：典型的视觉里程计监督学习结构（摘自 DeepVO [[15](#bib.bib15)]）、自监督学习的视觉里程计（摘自 SfmLearner [[16](#bib.bib16)]）以及混合视觉里程计（摘自
    D3VO [[17](#bib.bib17)]）。
- en: III-A Supervised Learning of Visual Odometry
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 视觉里程计的监督学习
- en: Supervised learning based visual odometry (VO) methods aim to train a deep neural
    network model on labelled datasets to construct a function from consecutive images
    to motion transformations, instead of exploiting the geometric structures of images
    as in conventional VO algorithms [[10](#bib.bib10)]. At its most basic, the input
    to the deep neural network (DNN) consists of a pair of consecutive images, while
    the output corresponds to the estimated translation and rotation between the two
    frames of images.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基于监督学习的视觉里程计（VO）方法旨在通过在标注数据集上训练深度神经网络模型，来构建从连续图像到运动变换的函数，而不是像传统 VO 算法那样利用图像的几何结构
    [[10](#bib.bib10)]。最基本的输入是深度神经网络（DNN）的连续图像对，而输出则是两帧图像之间的估计平移和旋转。
- en: One of the early works in this area is Konda et al. [[18](#bib.bib18)]. Their
    approach formulates visual odometry (VO) as a classification problem, and predicts
    the discrete changes of direction and velocity from input images using a convolutional
    neural network (ConvNet). However, this method is limited in its ability to estimate
    the full camera trajectory, and relies on a series of discrete motion estimates
    instead. Costante et al. [[19](#bib.bib19)] propose a method that overcomes some
    of the limitations of the Konda et al. [[18](#bib.bib18)] approach by using dense
    optical flow to extract visual features, and then using a ConvNet to estimate
    the frame-to-frame motion of the camera. This method shows performance improvements
    over the Konda et al. approach, and can generate smoother and more accurate camera
    trajectories. Despite the promising results of both approaches, they are not strictly
    an end-to-end learning model from images to motion estimates, and still fall short
    of traditional VO algorithms, e.g. VISO2 [[20](#bib.bib20)] in terms of accuracy
    and robustness. One limitation of both methods is that they do not fully exploit
    the rich geometric information contained in the input images, which is crucial
    for accurate motion estimation. Furthermore, the datasets used to train and evaluate
    these approaches are limited in their diversity and may not generalize well to
    different scenarios.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一领域的早期工作之一是Konda等人[[18](#bib.bib18)]。他们的方法将视觉里程计（VO）公式化为分类问题，并利用卷积神经网络（ConvNet）从输入图像中预测方向和速度的离散变化。然而，这种方法在估计完整相机轨迹的能力上存在局限，而是依赖于一系列离散的运动估计。Costante等人[[19](#bib.bib19)]提出了一种方法，通过使用稠密光流来提取视觉特征，从而克服了Konda等人[[18](#bib.bib18)]方法的一些局限性，然后利用ConvNet估计相机的帧间运动。这种方法在性能上优于Konda等人方法，并能够生成更平滑、更准确的相机轨迹。尽管这两种方法都显示出令人鼓舞的结果，但它们并不完全是从图像到运动估计的端到端学习模型，并且在准确性和鲁棒性方面仍然不及传统的VO算法，例如VISO2[[20](#bib.bib20)]。这两种方法的一个局限是没有充分利用输入图像中包含的丰富几何信息，而这些信息对于准确的运动估计至关重要。此外，用于训练和评估这些方法的数据集在多样性上有限，可能无法很好地推广到不同的场景。
- en: 'To enable end-to-end learning of visual odometry, DeepVO [[15](#bib.bib15)]
    utilizes a combination of convolutional neural network (ConvNet) and recurrent
    neural network (RNN). Figure [3](#S3.F3 "Figure 3 ‣ III Incremental Motion Estimation
    ‣ Deep Learning for Visual Localization and Mapping: A Survey") (a) shows the
    architecture of this typical RNN+ConvNet based VO model, which extracts visual
    features from pairs of images via a ConvNet, and passes features through RNNs
    to model the temporal correlation of features. Its ConvNet encoder is based on
    a FlowNet [[21](#bib.bib21)] structure to extract visual features suitable for
    optical flow and self-motion estimation. The recurrent model summarizes history
    information into its hidden states, so that the output is inferred from both past
    experience and current ConvNet features from sensor observations. DeepVO is trained
    on datasets with groundtruthed poses as training labels. To recover the optimal
    parameters $\bm{\theta}^{*}$ of this framework, the optimization target is to
    minimize the Mean Square Error (MSE) of the estimated translations $\mathbf{\hat{p}}\in\mathbb{R}^{3}$
    and Euler angle based rotations $\hat{\bm{\varphi}}\in\mathbb{R}^{3}$:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '为了实现视觉里程计的端到端学习，DeepVO[[15](#bib.bib15)]利用了卷积神经网络（ConvNet）和递归神经网络（RNN）的组合。图[3](#S3.F3
    "Figure 3 ‣ III Incremental Motion Estimation ‣ Deep Learning for Visual Localization
    and Mapping: A Survey")（a）展示了这种典型的基于RNN+ConvNet的VO模型的架构，该模型通过ConvNet从图像对中提取视觉特征，并通过RNN传递特征以建模特征的时间相关性。其ConvNet编码器基于FlowNet[[21](#bib.bib21)]结构来提取适合光流和自运动估计的视觉特征。递归模型将历史信息总结到其隐藏状态中，从而使输出既基于过去的经验，也基于当前ConvNet从传感器观察中得到的特征。DeepVO在具有地面真实姿态作为训练标签的数据集上进行训练。为了恢复这一框架的最佳参数$\bm{\theta}^{*}$，优化目标是最小化估计平移$\mathbf{\hat{p}}\in\mathbb{R}^{3}$和基于欧拉角的旋转$\hat{\bm{\varphi}}\in\mathbb{R}^{3}$的均方误差（MSE）：'
- en: '|  | $\bm{\theta}^{*}=\operatorname*{arg\,min}_{\bm{\theta}}\frac{1}{N}\displaystyle\sum_{i=1}^{N}\displaystyle\sum_{t=1}^{T}\&#124;\hat{\mathbf{p}}_{t}-\mathbf{p}_{t}\&#124;_{2}^{2}+\&#124;\hat{\bm{\varphi}}_{t}-\bm{\varphi}_{t}\&#124;_{2}^{2},$
    |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\theta}^{*}=\operatorname*{arg\,min}_{\bm{\theta}}\frac{1}{N}\displaystyle\sum_{i=1}^{N}\displaystyle\sum_{t=1}^{T}\&#124;\hat{\mathbf{p}}_{t}-\mathbf{p}_{t}\&#124;_{2}^{2}+\&#124;\hat{\bm{\varphi}}_{t}-\bm{\varphi}_{t}\&#124;_{2}^{2},$
    |  | (1) |'
- en: where $(\hat{\mathbf{p}}_{t},\hat{\bm{\varphi}}_{t})$ are the estimates of relative
    pose from DNN at the timestep $t$, $(\mathbf{p},\bm{\varphi})$ are the corresponding
    groundtruth values, $\bm{\theta}$ are the parameters of the DNN framework, and
    $N$ is the number of samples. This data-driven solution reports good results on
    estimating the pose of driving vehicles on several benchmarks. On the KITTI odometry
    dataset[[22](#bib.bib22)], it shows competitive performance over conventional
    monocular VO, e.g. VISO2 [[20](#bib.bib20)] and ORB-SLAM (without loop closure)
    [[6](#bib.bib6)]. It is worth noting that supervised VO naturally produces trajectory
    with absolute scale from monocular camera, while classical monocular VO algorithm
    is scale-ambiguous. This is probably because DNN implicitly learns and maintains
    the global scale from large collections of images. Although DeepVO reports good
    results in experimental scenarios, its performance has still not been extensively
    evaluated by large-scale datasets (e.g., across cities) or real-world experiments/demonstrations
    in the wild.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\hat{\mathbf{p}}_{t},\hat{\bm{\varphi}}_{t})$ 是在时间步 $t$ 从 DNN 估计的相对姿态，$(\mathbf{p},\bm{\varphi})$
    是对应的真实值，$\bm{\theta}$ 是 DNN 框架的参数，$N$ 是样本数量。该数据驱动的解决方案在多个基准上对驾驶车辆的姿态估计报告了良好的结果。在
    KITTI 里程计数据集[[22](#bib.bib22)]上，它显示了相对于传统的单目视觉里程计（如 VISO2 [[20](#bib.bib20)] 和
    ORB-SLAM（不包括回环检测） [[6](#bib.bib6)]] 的竞争力表现。值得注意的是，监督式视觉里程计自然地从单目相机产生具有绝对尺度的轨迹，而经典的单目视觉里程计算法则存在尺度歧义。这可能是因为
    DNN 从大量图像中隐式地学习和维护全局尺度。尽管 DeepVO 在实验场景中报告了良好的结果，但其性能仍未通过大规模数据集（例如跨城市）或现实世界实验/演示进行广泛评估。
- en: 'TABLE II: A summary of deep learning based visual odometry (incremental motion
    estimation) methods (Section [III](#S3 "III Incremental Motion Estimation ‣ Deep
    Learning for Visual Localization and Mapping: A Survey")).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II：基于深度学习的视觉里程计（增量运动估计）方法的总结（见第 [III](#S3 "III Incremental Motion Estimation
    ‣ Deep Learning for Visual Localization and Mapping: A Survey) 节）。'
- en: '| Model | Year | Sensor | Scale | Performance | Contributions |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 传感器 | 尺度 | 性能 | 贡献 |'
- en: '| Seq09 | Seq10 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Seq09 | Seq10 |'
- en: '| Supervised | Konda et al.[[18](#bib.bib18)] | 2015 | MC | Yes | - | - | formulate
    VO as a classification problem |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | Konda et al.[[18](#bib.bib18)] | 2015 | MC | 是 | - | - | 将视觉里程计形式化为分类问题
    |'
- en: '| Costante et al.[[19](#bib.bib19)] | 2016 | MC | Yes | 6.75 | 21.23 | extract
    features from optical flow for VO estimates |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Costante et al.[[19](#bib.bib19)] | 2016 | MC | 是 | 6.75 | 21.23 | 从光流中提取特征用于视觉里程计估计
    |'
- en: '| DeepVO[[15](#bib.bib15)] | 2017 | MC | Yes | - | 8.11 | combine RNN and ConvNet
    for end-to-end learning |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| DeepVO[[15](#bib.bib15)] | 2017 | MC | 是 | - | 8.11 | 结合RNN和ConvNet进行端到端学习
    |'
- en: '| Zhao et al.[[23](#bib.bib23)] | 2018 | MC | Yes | - | 4.38 | generate dense
    3D flow for VO and mapping |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al.[[23](#bib.bib23)] | 2018 | MC | 是 | - | 4.38 | 为视觉里程计和地图生成生成密集的3D流
    |'
- en: '| Saputra et al.[[24](#bib.bib24)] | 2019 | MC | Yes | - | 8.29 | curriculum
    learning and geometric loss constraints |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Saputra et al.[[24](#bib.bib24)] | 2019 | MC | 是 | - | 8.29 | 课程学习和几何损失约束
    |'
- en: '| Xue et al.[[25](#bib.bib25)] | 2019 | MC | Yes | - | 3.47 | memory and refinement
    module |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Xue et al.[[25](#bib.bib25)] | 2019 | MC | 是 | - | 3.47 | 记忆和精炼模块 |'
- en: '| Saputra et al.[[26](#bib.bib26)] | 2019 | MC | Yes | - | - | knowledge distilling
    to compress deep VO model |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Saputra et al.[[26](#bib.bib26)] | 2019 | MC | 是 | - | - | 知识蒸馏以压缩深度视觉里程计模型
    |'
- en: '| Koumis et al.[[27](#bib.bib27)] | 2019 | MC | Yes | - | - | 3D convolutional
    networks |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Koumis et al.[[27](#bib.bib27)] | 2019 | MC | 是 | - | - | 3D 卷积网络 |'
- en: '|  | DAVO [[28](#bib.bib28)] | 2020 | MC | Yes | - | 5.37 | Use attention to
    weight semantics and optical flow |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | DAVO [[28](#bib.bib28)] | 2020 | MC | 是 | - | 5.37 | 使用注意力加权语义和光流 |'
- en: '| Self-supervised | SfmLearner[[16](#bib.bib16)] | 2017 | MC | No | 17.84 |
    37.91 | novel view synthesis for self-supervised learning |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 自监督 | SfmLearner[[16](#bib.bib16)] | 2017 | MC | 否 | 17.84 | 37.91 | 自监督学习的新视图合成
    |'
- en: '| UnDeepVO[[29](#bib.bib29)] | 2018 | SC | Yes | 7.01 | 10.63 | use fixed stereo
    line to recover scale metric |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| UnDeepVO[[29](#bib.bib29)] | 2018 | SC | 是 | 7.01 | 10.63 | 使用固定的立体线恢复尺度度量
    |'
- en: '| GeoNet[[30](#bib.bib30)] | 2018 | MC | No | 43.76 | 35.6 | geometric consistency
    loss and 2D flow generator |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| GeoNet[[30](#bib.bib30)] | 2018 | MC | 否 | 43.76 | 35.6 | 几何一致性损失和2D流生成器
    |'
- en: '| Zhan et al.[[31](#bib.bib31)] | 2018 | SC | Yes | 11.92 | 12.45 | use fixed
    stereo line for scale recovery |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Zhan et al.[[31](#bib.bib31)] | 2018 | SC | 是 | 11.92 | 12.45 | 使用固定的立体线进行尺度恢复
    |'
- en: '| Struct2Depth[[32](#bib.bib32)] | 2019 | MC | No | 10.2 | 28.9 | introduce
    3D geometry structure during learning |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Struct2Depth[[32](#bib.bib32)] | 2019 | MC | 否 | 10.2 | 28.9 | 在学习过程中引入3D几何结构
    |'
- en: '| GANVO[[33](#bib.bib33)] | 2019 | MC | No | - | - | adversarial learning to
    generate depth |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GANVO[[33](#bib.bib33)] | 2019 | MC | 否 | - | - | 对抗学习以生成深度 |'
- en: '| Wang et al.[[34](#bib.bib34)] | 2019 | MC | Yes | 9.30 | 7.21 | integrate
    RNN and flow consistency constraint |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人[[34](#bib.bib34)] | 2019 | MC | 是 | 9.30 | 7.21 | 将 RNN 和流一致性约束结合
    |'
- en: '| Li et al.[[35](#bib.bib35)] | 2019 | MC | No | - | - | global optimization
    for pose graph |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人[[35](#bib.bib35)] | 2019 | MC | 否 | - | - | 姿态图的全局优化 |'
- en: '| Gordon[[36](#bib.bib36)] | 2019 | MC | No | 2.7 | 6.8 | camera matrix learning
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Gordon[[36](#bib.bib36)] | 2019 | MC | 否 | 2.7 | 6.8 | 相机矩阵学习 |'
- en: '| Bian et al.[[37](#bib.bib37)] | 2019 | MC | No | 11.2 | 10.1 | consistent
    scale from monocular images |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Bian 等人[[37](#bib.bib37)] | 2019 | MC | 否 | 11.2 | 10.1 | 从单目图像中得到一致尺度 |'
- en: '|  | Li et al.[[38](#bib.bib38)] | 2020 | MC | No | 5.89 | 4.79 | meta learning
    to adapt into new environment |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | Li 等人[[38](#bib.bib38)] | 2020 | MC | 否 | 5.89 | 4.79 | 元学习以适应新环境 |'
- en: '|  | Zou et al.[[39](#bib.bib39)] | 2020 | MC | No | 3.49 | 5.81 | model the
    long-term dependency |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | Zou 等人[[39](#bib.bib39)] | 2020 | MC | 否 | 3.49 | 5.81 | 模拟长期依赖 |'
- en: '|  | Zhao et al.[[40](#bib.bib40)] | 2021 | MC | No | 8.71 | 9.63 | introduce
    masked GAN to remove inconsistency |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhao 等人[[40](#bib.bib40)] | 2021 | MC | 否 | 8.71 | 9.63 | 引入掩膜 GAN 以去除不一致性
    |'
- en: '|  | Chi et al.[[41](#bib.bib41)] | 2021 | MC | No | 2.02 | 1.81 | collaborative
    learning of optical flow, depth and motion |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | Chi 等人[[41](#bib.bib41)] | 2021 | MC | 否 | 2.02 | 1.81 | 光流、深度和运动的协作学习
    |'
- en: '|  | Li et al.[[42](#bib.bib42)] | 2021 | MC | No | 1.87 | 1.93 | online adaptation
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | Li 等人[[42](#bib.bib42)] | 2021 | MC | 否 | 1.87 | 1.93 | 在线适应 |'
- en: '|  | Sun et al.[[40](#bib.bib40)] | 2022 | MC | No | 7.14 | 7.72 | introduce
    cover and filter masks |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | Sun 等人[[40](#bib.bib40)] | 2022 | MC | 否 | 7.14 | 7.72 | 引入覆盖和滤波掩膜 |'
- en: '|  | Dai et al. [[43](#bib.bib43)] | 2022 | MC | No | 3.24 | 1.03 | introduce
    attention and pose graph optimization |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | Dai 等人[[43](#bib.bib43)] | 2022 | MC | 否 | 3.24 | 1.03 | 引入注意力和姿态图优化 |'
- en: '|  | VRVO[[44](#bib.bib44)] | 2022 | MC | Yes | 1.55 | 2.75 | use virtual data
    to recover scale |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | VRVO[[44](#bib.bib44)] | 2022 | MC | 是 | 1.55 | 2.75 | 使用虚拟数据来恢复尺度 |'
- en: '| Hybrid | Backprop KF[[45](#bib.bib45)] | 2016 | MC | Yes | - | - | a differentiable
    Kalman filter based VO |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Hybrid | Backprop KF[[45](#bib.bib45)] | 2016 | MC | 是 | - | - | 基于可微卡尔曼滤波器的
    VO |'
- en: '| Yin et al.[[46](#bib.bib46)] | 2017 | MC | Yes | 4.14 | 1.70 | introduce
    learned depth to recover scale metric |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Yin 等人[[46](#bib.bib46)] | 2017 | MC | 是 | 4.14 | 1.70 | 引入学习的深度来恢复尺度度量 |'
- en: '| Barnes et al.[[47](#bib.bib47)] | 2018 | MC | Yes | - | - | integrate learned
    depth and ephemeral masks |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Barnes 等人[[47](#bib.bib47)] | 2018 | MC | 是 | - | - | 结合学习的深度和短期掩膜 |'
- en: '| DPF[[48](#bib.bib48)] | 2018 | MC | Yes | - | - | a differentiable particle
    filter based VO |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| DPF[[48](#bib.bib48)] | 2018 | MC | 是 | - | - | 基于可微粒子滤波器的 VO |'
- en: '| Yang et al.[[49](#bib.bib49)] | 2018 | MC | Yes | 0.83 | 0.74 | use learned
    depth into classical VO |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等人[[49](#bib.bib49)] | 2018 | MC | 是 | 0.83 | 0.74 | 将学习的深度应用于经典 VO
    |'
- en: '| CNN-SVO[[50](#bib.bib50)] | 2019 | MC | Yes | 10.69 | 4.84 | use learned
    depth to initialize SVO |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| CNN-SVO[[50](#bib.bib50)] | 2019 | MC | 是 | 10.69 | 4.84 | 使用学习的深度来初始化 SVO
    |'
- en: '| Zhan et al.[[51](#bib.bib51)] | 2020 | MC | Yes | 2.61 | 2.29 | integrate
    learned optical flow and depth |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Zhan 等人[[51](#bib.bib51)] | 2020 | MC | 是 | 2.61 | 2.29 | 结合学习的光流和深度 |'
- en: '| Wagstaff et al.[[52](#bib.bib52)] | 2020 | MC | Yes | 2.82 | 3.81 | integrate
    classical VO with learned pose corrections |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Wagstaff 等人[[52](#bib.bib52)] | 2020 | MC | 是 | 2.82 | 3.81 | 将经典 VO 与学习的姿态修正相结合
    |'
- en: '|  | D3VO[[17](#bib.bib17)] | 2020 | MC | Yes | 0.78 | 0.62 | integrate learned
    depth, uncertainty and pose |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | D3VO[[17](#bib.bib17)] | 2020 | MC | 是 | 0.78 | 0.62 | 结合学习的深度、不确定性和姿态
    |'
- en: '|  | Sun et al.[[53](#bib.bib53)] | 2022 | MC | Yes | - | - | integrate learned
    depth into DSO |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | Sun 等人[[53](#bib.bib53)] | 2022 | MC | 是 | - | - | 将学习的深度集成到 DSO 中 |'
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Year indicates the publication year (e.g. the date of conference) of each work.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 年份表示每项工作的出版年份（例如会议日期）。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sensor: MC and SC represent monocular camera and stereo camera respectively.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传感器：MC 和 SC 分别表示单目相机和立体相机。
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Supervision represents whether it is a supervised or unsupervised end-to-end
    model, or a hybrid model
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监督表示是否为有监督或无监督的端到端模型，或混合模型
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scale indicates whether a trajectory with a global scale can be produced.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尺度表示是否可以生成具有全局尺度的轨迹。
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Performance reports the localization error (a small number is better), i.e.
    the averaged translational RMSE drift (%) on lengths of 100m-800m on the KITTI
    odometry dataset[[22](#bib.bib22)]. Most works were evaluated on the Sequence
    09 and 10, and thus we took the results on these two sequences from their original
    papers for a performance comparison. Note that the training sets may be different
    in each work.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能报告了本地化误差（数值越小越好），即在KITTI里程计数据集[[22](#bib.bib22)]上长度为100m-800m的平均平移RMSE漂移（%）。大多数工作是在序列09和10上进行评估的，因此我们从这些原始论文中提取了这两个序列的结果以进行性能比较。注意，每项工作的训练集可能有所不同。
- en: Enhancing the generalization capability of supervised Visual Odometry (VO) models
    and improving their efficacy for operating in real-time on devices with limited
    resources are still formidable challenges. While supervised learning-based VO
    is trained on extensive datasets of image sequences with ground-truth poses, not
    all sequences are equally informative or challenging for the model to learn. Curriculum
    learning is a technique that gradually elevates the complexity of the training
    data by initially presenting simple sequences and progressively introducing more
    challenging ones. In [[24](#bib.bib24)], curriculum learning is integrated into
    the supervised VO model by increasing the amount of motion and rotation in the
    training sequences, enabling the model to learn to estimate camera motion more
    robustly and generalize better to new data. Knowledge distillation is another
    approach that can be introduced to improve the efficiency of supervised VO models
    by compressing a large model through teaching a smaller one. This method is applied
    in [[26](#bib.bib26)], reducing the number of network parameters and making the
    model more suitable for real-time operation on mobile devices. Compared to pure
    supervised VO without knowledge distillation, this method significantly reduces
    network parameters by 92.95% and enhances computation speed by 2.12 times.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 提升监督视觉里程计（VO）模型的泛化能力以及改善其在资源有限的设备上实时操作的效果仍然是巨大的挑战。虽然基于监督学习的VO在大量图像序列及其真实姿态数据上进行训练，但并非所有序列对模型学习都是同等有用或具有挑战性的。课程学习是一种技术，通过最初呈现简单序列并逐渐引入更具挑战性的序列来逐步提高训练数据的复杂性。在[[24](#bib.bib24)]中，通过增加训练序列中的运动和旋转量，将课程学习集成到监督VO模型中，使模型能够更稳健地学习估计相机运动，并更好地泛化到新数据。知识蒸馏是另一种方法，可以通过将大模型压缩为较小的模型来提高监督VO模型的效率。这种方法在[[26](#bib.bib26)]中应用，减少了网络参数的数量，使模型更适合在移动设备上实时操作。与没有知识蒸馏的纯监督VO相比，这种方法显著减少了92.95%的网络参数，并提高了2.12倍的计算速度。
- en: Furthermore, to enhance the localization performance, a memory module that stores
    global information about the scene and camera motion is introduced in [[25](#bib.bib25)].
    The background information is then utilized by a refining module that enhances
    the accuracy of the predicted camera poses. Additionally, attention mechanisms
    have been implemented to weigh the inputs from different sources and enhance the
    efficacy of supervised VO models. For example, DAVO [[28](#bib.bib28)] integrates
    an attention module to weigh the inputs from semantic segmentation, optical flow,
    and RGB images, leading to improved odometry estimation performance. Despite the
    promising end-to-end learning performance achieved on publicly available datasets
    by these supervised VO frameworks, their deployment performance in real-world
    scenarios remains to be further verified as of the writing of this survey.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了提升本地化性能，在[[25](#bib.bib25)]中引入了一个存储关于场景和相机运动的全局信息的记忆模块。背景信息随后由一个细化模块利用，该模块提高了预测相机姿态的准确性。此外，已实施注意力机制以权衡来自不同来源的输入，并提高监督视觉里程计（VO）模型的效果。例如，DAVO
    [[28](#bib.bib28)] 集成了一个注意力模块，以权衡来自语义分割、光流和RGB图像的输入，从而改善了里程计估计性能。尽管这些监督VO框架在公开数据集上取得了有前景的端到端学习性能，但在实际场景中的部署性能仍需进一步验证。
- en: Overall, supervised learning-based visual odometry models primarily rely on
    ConvNet or RNN to learn pose transformations automatically from raw images. Recent
    advancements in machine learning, including attention mechanisms, GANs, and knowledge
    distillation, have allowed these models to extract more expressive visual features
    and accurately model motion. However, these learning methods often require a vast
    amount of training data with precise poses as labels to optimize model parameters
    and improve robustness. While supervised learning-based VO models have demonstrated
    promising end-to-end learning performance on publicly available datasets, their
    deployment performance in real-world scenarios requires further validation. Additionally,
    obtaining labeled data is often time-consuming and costly, and inaccurate labels
    can occur. In the following section, we will discuss recent efforts to address
    the issue of label scarcity through self-supervised learning techniques.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，基于监督学习的视觉里程计模型主要依赖于ConvNet或RNN从原始图像中自动学习姿态变换。机器学习的最新进展，包括注意力机制、GANs和知识蒸馏，使这些模型能够提取更具表现力的视觉特征并准确建模运动。然而，这些学习方法通常需要大量标注精确姿态的训练数据，以优化模型参数并提高鲁棒性。尽管基于监督学习的VO模型在公开数据集上展示了有前景的端到端学习性能，但其在实际场景中的部署性能仍需进一步验证。此外，获取标注数据通常耗时且成本高昂，并且可能出现不准确的标签。在接下来的部分，我们将讨论通过自监督学习技术解决标签稀缺问题的最新努力。
- en: III-B Self-supervised Learning of Visual Odometry
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 自监督学习的视觉里程计
- en: There are growing interests in exploring self-supervised learning of visual
    odometry (VO). Self-supervised solutions are capable of exploiting unlabelled
    sensor data, and thus it saves human efforts. Compared with supervised approaches,
    they normally show better adaptation ability in new scenarios, where no labelled
    data are available. This has been achieved in a self-supervised framework that
    jointly learns camera ego-motion and depth from video sequences, by utilizing
    view synthesis as a self-supervisory signal [[16](#bib.bib16)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于探索自监督学习的视觉里程计（VO），兴趣日益增长。自监督解决方案能够利用未标注的传感器数据，从而节省了人工努力。与监督方法相比，它们通常在没有标注数据的新场景中表现出更好的适应能力。这在一个自监督框架中得以实现，该框架通过利用视图合成作为自监督信号，从视频序列中共同学习相机自运动和深度[[16](#bib.bib16)]。
- en: 'As shown in Figure [3](#S3.F3 "Figure 3 ‣ III Incremental Motion Estimation
    ‣ Deep Learning for Visual Localization and Mapping: A Survey") (b), a typical
    self-supervised VO framework [[16](#bib.bib16)] consists of a depth network to
    predict depth maps, and a pose network to produce motion transformations between
    images. The entire framework takes consecutive images as input, and the supervision
    signal is based on novel view synthesis - given a source image $\mathbf{I}_{s}$,
    the view synthesis task is to generate a synthetic target image $\mathbf{I}_{t}$.
    A pixel of source image $\mathbf{I}_{s}(p_{s})$ is projected onto a target view
    $\mathbf{I}_{t}(p_{t})$ via:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S3.F3 "图 3 ‣ III 增量运动估计 ‣ 深度学习在视觉定位与映射中的应用：综述") (b) 所示，典型的自监督VO框架[[16](#bib.bib16)]包括一个深度网络来预测深度图和一个姿态网络来生成图像之间的运动变换。整个框架将连续图像作为输入，监督信号基于新视图合成——给定源图像
    $\mathbf{I}_{s}$，视图合成任务是生成合成目标图像 $\mathbf{I}_{t}$。源图像 $\mathbf{I}_{s}(p_{s})$
    的一个像素通过以下公式投影到目标视图 $\mathbf{I}_{t}(p_{t})$：
- en: '|  | $p_{s}\sim\mathbf{K}\mathbf{T}_{t\to s}\mathbf{D}_{t}(p_{t})\mathbf{K}^{-1}p_{t}$
    |  | (2) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{s}\sim\mathbf{K}\mathbf{T}_{t\to s}\mathbf{D}_{t}(p_{t})\mathbf{K}^{-1}p_{t}$
    |  | (2) |'
- en: 'where $\mathbf{K}$ is the camera’s intrinsic matrix, $\mathbf{T}_{t\to s}$
    denotes the camera motion matrix from target frame to source frame, and $\mathbf{D}_{t}(p_{t})$
    denotes the per-pixel depth maps in the target frame. The training objective is
    to ensure the consistency of the scene geometry by optimizing the photometric
    reconstruction loss between the real target image and the synthetic one:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{K}$ 是相机的内参矩阵，$\mathbf{T}_{t\to s}$ 表示从目标帧到源帧的相机运动矩阵，$\mathbf{D}_{t}(p_{t})$
    表示目标帧中的每像素深度图。训练目标是通过优化真实目标图像和合成图像之间的光度重建损失来确保场景几何的一致性：
- en: '|  | $\mathcal{L}_{\text{photo}}=\sum_{<\mathbf{I}_{1},...,\mathbf{I}_{N}>\in
    S}\sum_{p}&#124;\mathbf{I}_{t}(p)-\hat{\mathbf{I}}_{s}(p)&#124;,$ |  | (3) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{photo}}=\sum_{<\mathbf{I}_{1},...,\mathbf{I}_{N}>\in
    S}\sum_{p}&#124;\mathbf{I}_{t}(p)-\hat{\mathbf{I}}_{s}(p)&#124;,$ |  | (3) |'
- en: where $p$ denotes pixel coordinates, $\mathbf{I}_{t}$ is the target image, and
    $\hat{\mathbf{I}}_{s}$ is the synthetic target image generated from the source
    image $\mathbf{I}_{s}$.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p$ 表示像素坐标，$\mathbf{I}_{t}$ 是目标图像，$\hat{\mathbf{I}}_{s}$ 是从源图像 $\mathbf{I}_{s}$
    生成的合成目标图像。
- en: 'However, there are basically two main problems that remain unsolved in the
    original work [[16](#bib.bib16)]: 1) this monocular image based approach is not
    able to provide pose estimates in a consistent global scale. Due to the scale
    ambiguity, no physically meaningful global trajectory can be reconstructed, limiting
    its real usage; 2) the photometric loss assumes that the scene is static and without
    camera occlusions. Although the authors propose the use of an explainability mask
    to remove scene dynamics, the influence of these environmental factors is still
    not addressed completely, which violates the assumption.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，原始工作中仍然存在两个主要未解决的问题[[16](#bib.bib16)]：1）这种基于单目图像的方法无法提供一致的全局尺度姿态估计。由于尺度歧义，无法重建出具有物理意义的全局轨迹，从而限制了其实际应用；2）光度损失假设场景是静态的且没有相机遮挡。虽然作者提出使用可解释性掩码来去除场景动态，但这些环境因素的影响仍未完全解决，这违背了假设。
- en: To solve global scale problem, [[29](#bib.bib29), [31](#bib.bib31)] propose
    to utilize stereo image pairs to recover the absolute scale of pose estimation.
    They introduce an additional spatial photometric loss between the left and right
    pairs of images, as the stereo baseline (i.e. motion transformation between the
    left and right images) is fixed and known throughout the dataset. Once the training
    is complete, the network produces pose predictions using only monocular images.
    Compared with [[16](#bib.bib16)], they are able to produce camera poses with global
    metric scale and higher accuracy. Another approach is to use virtual stereo data
    from simulator to recover the absolute scale of pose estimation in VRVO[[44](#bib.bib44)].
    It utilizes a generative adversarial network (GAN) to generate virtual stereo
    data that is similar to real-world data. By bridging the gap between virtual and
    real data using adversarial learning, the pose network is then trained using the
    virtual data to recover the absolute scale of pose estimation. [[37](#bib.bib37)]
    tackles the scale issue by introducing a geometric consistency loss, that enforces
    the consistency between predicted depth maps and reconstructed depth maps. The
    framework transforms the predicted depth maps into a 3D space, and projects them
    back to produce reconstructed depth maps. By doing so, the depth predictions can
    remain scale-consistent over consecutive frames, enabling pose estimates to be
    scale-consistent as well. Different from previous works that either use stereo
    images [[29](#bib.bib29), [31](#bib.bib31)] or virtual data [[44](#bib.bib44)],
    this work successfully produces scale-consistent camera poses and depth estimates
    only using monocular images.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决全局尺度问题，[[29](#bib.bib29), [31](#bib.bib31)] 提出了利用立体图像对来恢复姿态估计的绝对尺度。他们在左右图像对之间引入了额外的空间光度损失，因为立体基线（即左右图像之间的运动变换）在整个数据集中是固定且已知的。一旦训练完成，网络仅使用单目图像生成姿态预测。与
    [[16](#bib.bib16)] 相比，他们能够生成具有全局度量尺度和更高精度的相机姿态。另一种方法是使用来自模拟器的虚拟立体数据来恢复 VRVO[[44](#bib.bib44)]
    中姿态估计的绝对尺度。它利用生成对抗网络（GAN）生成类似于现实世界数据的虚拟立体数据。通过使用对抗学习弥合虚拟数据和真实数据之间的差距，然后使用虚拟数据训练姿态网络，以恢复姿态估计的绝对尺度。[[37](#bib.bib37)]
    通过引入几何一致性损失来解决尺度问题，该损失强制执行预测深度图与重建深度图之间的一致性。该框架将预测的深度图转换为 3D 空间，并将其投影回去以生成重建深度图。通过这种方式，深度预测可以在连续帧之间保持尺度一致，从而使姿态估计也保持尺度一致。不同于之前使用立体图像
    [[29](#bib.bib29), [31](#bib.bib31)] 或虚拟数据 [[44](#bib.bib44)] 的工作，这项工作仅使用单目图像成功地生成了尺度一致的相机姿态和深度估计。
- en: The photometric consistency constraint is based on the assumption that the entire
    scene consists only of rigid static structures such as buildings and lanes. However,
    in real-world applications, the presence of environmental dynamics such as pedestrians
    and vehicles can cause distortion in the photometric projection, leading to reduced
    accuracy in pose estimation. To address this concern, GeoNet [[30](#bib.bib30)]
    divides its learning process into two sub-tasks by estimating static scene structures
    and motion dynamics separately through a rigid structure reconstructor and a non-rigid
    motion localizer. In addition, GeoNet enforces a geometric consistency loss to
    mitigate the issues caused by camera occlusions and non-Lambertian surfaces. [[23](#bib.bib23)]
    adds a 2D flow generator along with a depth network to generate 3D flow. Benefiting
    from better 3D understanding of environment, this framework is able to produce
    more accurate camera poses, along with a point cloud map. GANVO [[33](#bib.bib33)]
    employs a generative adversarial learning paradigm for depth generation and introduces
    a temporal recurrent module for pose regression. This method improves accuracy
    in depth map and pose estimation, as well as tolerating environmental dynamics.
    [[54](#bib.bib54)] also utilizes a generative adversarial network (GAN) to generate
    more realistic depth maps and poses, and further encourages more accurate synthetic
    images in the target frame. Unlike hand-crafted metrics, a discriminator is used
    to evaluate the quality of synthetic image generation. In doing so, the generative
    adversarial setup facilitates the generated depth maps to be more texture-rich
    and crisper. In this way, high-level scene perception and representation are accurately
    captured and environmental dynamics are implicitly tolerated. [[40](#bib.bib40)]
    introduces a masked GAN into joint learning of depth and visual odometry (VO)
    estimation, addressing influences from light-condition changes and occlusions.
    By incorporating MaskNet and a Boolean mask scheme, it mitigates the impacts of
    occlusions and visual field changes, improving adversarial loss and image reconstruction.
    A scale-consistency loss ensures accurate pose estimation in long monocular sequences.
    Similarly, [[55](#bib.bib55)] introduces hybrid masks to mitigate the negative
    impact of dynamic environments. Cover masks and filter masks alleviate adverse
    effects on VO estimation and view reconstruction processes. Both approaches demonstrate
    competitive depth prediction and globally consistent VO estimation in car-driving
    scenarios.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 光度一致性约束基于整个场景仅由建筑物和车道等刚性静态结构组成的假设。然而，在实际应用中，环境动态如行人和车辆的存在可能会导致光度投影的扭曲，从而降低姿态估计的准确性。为了解决这一问题，GeoNet
    [[30](#bib.bib30)] 将其学习过程分为两个子任务，通过刚性结构重建器和非刚性运动定位器分别估计静态场景结构和运动动态。此外，GeoNet 强制执行几何一致性损失，以减轻相机遮挡和非朗伯表面引起的问题。[[23](#bib.bib23)]
    增加了一个 2D 流生成器和一个深度网络，以生成 3D 流。得益于对环境的更好 3D 理解，这一框架能够生成更准确的相机姿态以及点云地图。GANVO [[33](#bib.bib33)]
    采用生成对抗学习范式进行深度生成，并引入了一个时间递归模块用于姿态回归。这种方法提高了深度图和姿态估计的准确性，并能容忍环境动态。[[54](#bib.bib54)]
    还利用生成对抗网络（GAN）生成更逼真的深度图和姿态，并进一步鼓励目标帧中的合成图像更准确。与手工设计的度量标准不同，使用了一个鉴别器来评估合成图像生成的质量。这样，生成对抗设置使得生成的深度图更具纹理丰富度和清晰度。通过这种方式，高级场景感知和表示得以准确捕捉，环境动态被隐式容忍。[[40](#bib.bib40)]
    将掩码 GAN 引入深度和视觉里程计（VO）估计的联合学习中，解决了光照条件变化和遮挡的影响。通过引入 MaskNet 和布尔掩码方案，它减轻了遮挡和视野变化的影响，提高了对抗损失和图像重建。尺度一致性损失确保了在长单目序列中的准确姿态估计。同样，[[55](#bib.bib55)]
    引入混合掩码来减轻动态环境的负面影响。覆盖掩码和滤波掩码缓解了对 VO 估计和视图重建过程的负面影响。这两种方法在汽车驾驶场景中展示了有竞争力的深度预测和全局一致的
    VO 估计。
- en: Recent attempts [[38](#bib.bib38), [42](#bib.bib42)] design online learning
    strategies that enable the learned model to adapt into new environments. These
    approaches allow the learning model to automatically update its parameters and
    learn from new data without forgetting the previously learned knowledge. Collaborative
    learning of multiple learning tasks, such as optical flow, depth, and camera motion
    estimation, has also been shown to improve the performance of self-supervised
    VO [[41](#bib.bib41)]. By jointly optimizing the different learning targets, it
    exploits the complementary information between them so that learns more robust
    representations for pose estimation. To further improve visual odometry, [[43](#bib.bib43)]
    proposes a self-supervised VO with an attention mechanism and pose graph optimization.
    The introduced attention mechanism is sensitive to geometrical structure and helps
    to accurately regress the rotation matrix.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的尝试[[38](#bib.bib38), [42](#bib.bib42)]设计了在线学习策略，使得学习模型能够适应新环境。这些方法允许学习模型自动更新其参数并从新数据中学习，而不会遗忘先前学习的知识。多个学习任务的协同学习，例如光流、深度和相机运动估计，也已被证明能提高自监督视觉里程计的性能[[41](#bib.bib41)]。通过联合优化不同的学习目标，它利用了它们之间的互补信息，从而学习出更稳健的位姿估计表示。为了进一步改善视觉里程计，[[43](#bib.bib43)]提出了一种具有注意机制和位姿图优化的自监督视觉里程计。引入的注意机制对几何结构非常敏感，有助于准确回归旋转矩阵。
- en: Overall, self-supervised learning-based visual odometry methods have emerged
    as a promising approach for estimating camera poses and scene depths without the
    need for labeled data during training. They normally consist of two ConvNet based
    neural networks- one for depth estimation and the other for pose estimation. Compared
    to supervised learning-based methods, self-supervised approaches offer several
    advantages, including the ability to handle non-rigid dynamics and adapt to new
    environments in real-time. However, despite these benefits, self-supervised VO
    methods still underperform compared to their supervised counterparts, and there
    remain challenges with scaling and scene dynamics.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，基于自监督学习的视觉里程计方法作为估计相机位姿和场景深度的一种有前途的方法，不需要在训练过程中使用标注数据。它们通常由两个基于卷积网络的神经网络组成——一个用于深度估计，另一个用于位姿估计。与基于监督学习的方法相比，自监督方法提供了若干优势，包括处理非刚性动态和实时适应新环境的能力。然而，尽管有这些优势，自监督视觉里程计方法的表现仍不及其监督对手，并且在尺度和场景动态方面仍存在挑战。
- en: 'As demonstrated in Table [II](#S3.T2 "TABLE II ‣ III-A Supervised Learning
    of Visual Odometry ‣ III Incremental Motion Estimation ‣ Deep Learning for Visual
    Localization and Mapping: A Survey"), self-supervised VO still cannot compete
    with supervised VO in performance, its concerns of scale metric and scene dynamics
    problem have been largely resolved with the efforts of many researchers. With
    the benefits of self-supervised learning and ever-increasing improvement on performance,
    self-supervised VO would be a promising solution to deep learning based SLAM.
    Currently, end-to-end learning based VOs have not been proved to surpass the state-of-the-art
    model-based VOs in performance. Next section will show how to combine the benefits
    from both sides to construct hybrid approaches.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[II](#S3.T2 "TABLE II ‣ III-A Supervised Learning of Visual Odometry ‣ III
    Incremental Motion Estimation ‣ Deep Learning for Visual Localization and Mapping:
    A Survey")所示，自监督视觉里程计（VO）的性能仍无法与监督视觉里程计相媲美，不过其在尺度度量和场景动态问题上的顾虑在许多研究人员的努力下已经得到很大程度的解决。借助自监督学习的优势和性能的不断提升，自监督视觉里程计有望成为基于深度学习的SLAM的一个有前景的解决方案。目前，基于端到端学习的视觉里程计尚未被证明在性能上超越最先进的模型驱动视觉里程计。下一节将展示如何结合双方的优势来构建混合方法。'
- en: III-C Hybrid Visual Odometry
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 混合视觉里程计
- en: Unlike end-to-end approaches that rely solely on a deep neural network to interpret
    pose from data, hybrid approaches combine classical geometric models with a deep
    learning framework. The deep neural network is used to replace part of a geometry
    model, which allows for more expressive representations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与完全依赖深度神经网络从数据中解释位姿的端到端方法不同，混合方法将经典几何模型与深度学习框架相结合。深度神经网络被用来替代几何模型的部分，从而允许更具表现力的表示。
- en: 'One of the key challenges in traditional monocular visual odometry (VO) is
    the scale-ambiguity problem, where monocular VOs can only estimate relative scale.
    This poses a problem in scenarios where absolute scale is required. One way to
    solve this issue is to integrate learned depth estimates into a classical visual
    odometry algorithm, which helps to recover the absolute scale metric of poses.
    Depth estimation is a well-established research area in computer vision, and various
    methods have been proposed to tackle this problem. For instance, Godard et al.
    [[56](#bib.bib56)] proposed a deep neural model that predicts per-pixel depths
    in an absolute scale. The details of depth learning are discussed in Section [V-A1](#S5.SS1.SSS1
    "V-A1 Raw Dense Representations ‣ V-A Geometric Mapping ‣ V Mapping ‣ Deep Learning
    for Visual Localization and Mapping: A Survey").'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '传统单目视觉里程计（VO）中的一个关键挑战是尺度模糊问题，其中单目视觉里程计只能估计相对尺度。在需要绝对尺度的场景中，这会造成问题。解决这一问题的一种方法是将学习到的深度估计集成到经典视觉里程计算法中，这有助于恢复姿态的绝对尺度度量。深度估计是计算机视觉中的一个成熟研究领域，已提出了各种方法来解决这个问题。例如，Godard
    等人 [[56](#bib.bib56)] 提出了一个深度神经网络模型，该模型可以在绝对尺度下预测每个像素的深度。深度学习的细节在[第 V-A1 节](#S5.SS1.SSS1
    "V-A1 Raw Dense Representations ‣ V-A Geometric Mapping ‣ V Mapping ‣ Deep Learning
    for Visual Localization and Mapping: A Survey")中进行了讨论。'
- en: In [[46](#bib.bib46)], a ConvNet produces coarse depth values from raw images,
    which are then refined by conditional random fields. The scale factor is calculated
    by comparing the estimated depth predictions with the observed point positions.
    Once the scale factor is obtained, the ego-motions with absolute scale are obtained
    by multiplying the scale-factor and estimated translations from a monocular VO
    algorithm. This approach mitigates the scale problem by incorporating depth information.
    Additionally, [[47](#bib.bib47)] proposes the integration of predicted ephemeral
    masks (i.e., the area of moving objects) with depth maps in a traditional VO system
    to enhance its robustness to moving objects. This method enables the system to
    produce metric-scale pose estimates using a single camera, even when a significant
    portion of the image is obscured by dynamic objects. [[52](#bib.bib52)] proposes
    to combine a classical VO with learned pose corrections, that largely reduces
    the error drifts of classical VOs. Compared with pure learning based VOs, instead
    of directly regressing inter-frame pose changes, this approach regresses pose
    corrections from data, without the need of pose ground truth as training data.
    Similarly, [[53](#bib.bib53)] proposes to improve classical monocular VO with
    learned depth estimates. This framework consists of a monocular depth estimation
    module with two separate working modes to assist localization and mapping, and
    it demonstrates strong generalization ability to diverse scenes, compared with
    existing learning based VOs. Furthermore, [[51](#bib.bib51)] integrates learned
    depth and optical flow predictions into a conventional VO model. Specifically,
    this framework uses optical flow and single-view depth predictions from deep ConvNets
    as intermediate outputs to establish 2D-2D/3D-2D correspondences, and the depth
    estimates with consistent scale can mitigate the scale drift issue in monocular
    VO/SLAM systems. By integrating deep predictions with geometry-based methods,
    the study shows that deep VO models can complement standard VO/SLAM systems.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[46](#bib.bib46)] 中，卷积网络从原始图像中生成粗略的深度值，然后通过条件随机场进行细化。通过将估计的深度预测与观察到的点位置进行比较来计算尺度因子。一旦获得尺度因子，就可以通过将尺度因子与单目视觉里程计算法估计的平移量相乘来获得具有绝对尺度的自我运动。这种方法通过结合深度信息来缓解尺度问题。此外，[[47](#bib.bib47)]
    提出了将预测的瞬态掩码（即移动物体的区域）与传统视觉里程计系统中的深度图结合起来，以增强其对移动物体的鲁棒性。该方法使系统能够使用单个摄像头生成度量尺度的姿态估计，即使图像的大部分被动态物体遮挡。[[52](#bib.bib52)]
    提出了将经典视觉里程计与学习到的姿态校正相结合，大大减少了经典视觉里程计的误差漂移。与纯学习基础的视觉里程计相比，该方法不是直接回归帧间姿态变化，而是从数据中回归姿态校正，不需要姿态真值作为训练数据。类似地，[[53](#bib.bib53)]
    提出了用学习到的深度估计来改进经典的单目视觉里程计。该框架包括一个具有两种不同工作模式的单目深度估计模块，以协助定位和地图构建，并且与现有的学习基础视觉里程计相比，展示了对各种场景的强大泛化能力。此外，[[51](#bib.bib51)]
    将学习到的深度和光流预测集成到传统视觉里程计模型中。具体而言，该框架使用深度卷积网络的光流和单视图深度预测作为中间输出，以建立 2D-2D/3D-2D 对应关系，并且具有一致尺度的深度估计可以缓解单目视觉里程计/SLAM
    系统中的尺度漂移问题。通过将深度预测与基于几何的方法结合，该研究表明深度视觉里程计模型可以补充标准的视觉里程计/SLAM 系统。
- en: 'D3VO [[17](#bib.bib17)] is proposed to incorporate the predictions of depth,
    pose, and photometric uncertainty from deep neural networks into direct visual
    odometry (DVO) [[57](#bib.bib57)]. In D3VO, a self-supervised framework is employed
    to learn depth and ego-motion jointly, similar to the approaches discussed in
    Section [III-B](#S3.SS2 "III-B Self-supervised Learning of Visual Odometry ‣ III
    Incremental Motion Estimation ‣ Deep Learning for Visual Localization and Mapping:
    A Survey"). D3VO employs the uncertainty estimation method proposed by [[58](#bib.bib58)]
    to generate a photometric uncertainty map that indicates which parts of the visual
    observations can be trusted. As illustrated in Fig. [3](#S3.F3 "Figure 3 ‣ III
    Incremental Motion Estimation ‣ Deep Learning for Visual Localization and Mapping:
    A Survey") (c), the learned depth and pose estimates are integrated into the front-end
    of a VO algorithm, and the uncertainties are used in the system back-end. This
    method shows impressive results on the KITTI [[22](#bib.bib22)] and EuroC [[59](#bib.bib59)]
    benchmarks, surpassing several popular conventional VO/VIO systems, e.g. DSO [[60](#bib.bib60)],
    ORB-SLAM [[6](#bib.bib6)] and VINS-Mono [[3](#bib.bib3)]. This indicates the promise
    of integrating learning methods with geometric models.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'D3VO [[17](#bib.bib17)] 被提出用于将深度神经网络的深度、姿态和光度不确定性的预测融入直接视觉里程计（DVO）[[57](#bib.bib57)]中。在
    D3VO 中，采用自监督框架来联合学习深度和自我运动，类似于在第 [III-B](#S3.SS2 "III-B Self-supervised Learning
    of Visual Odometry ‣ III Incremental Motion Estimation ‣ Deep Learning for Visual
    Localization and Mapping: A Survey") 节中讨论的方法。D3VO 采用了 [[58](#bib.bib58)] 提出的不确定性估计方法来生成光度不确定性图，以指示哪些视觉观测部分是可信的。如图
    Fig. [3](#S3.F3 "Figure 3 ‣ III Incremental Motion Estimation ‣ Deep Learning
    for Visual Localization and Mapping: A Survey") (c) 所示，学习到的深度和姿态估计被整合到 VO 算法的前端，而不确定性则在系统后端使用。这种方法在
    KITTI [[22](#bib.bib22)] 和 EuroC [[59](#bib.bib59)] 基准测试中显示了令人印象深刻的结果，超越了几种流行的传统
    VO/VIO 系统，例如 DSO [[60](#bib.bib60)]、ORB-SLAM [[6](#bib.bib6)] 和 VINS-Mono [[3](#bib.bib3)]。这表明将学习方法与几何模型结合的前景非常光明。'
- en: In addition to geometric models, there have been studies that combine physical
    motion models with deep neural networks, such as through a differentiable Kalman
    filter [[45](#bib.bib45), [61](#bib.bib61)] or a differentiable particle filter
    [[48](#bib.bib48)]. In [[45](#bib.bib45)], Kalman filter is transformed into a
    differentiable module that is combined with deep neural networks for an end-to-end
    training. [[61](#bib.bib61)] proposes DynaNet, a hybrid model integrating deep
    neural networks (DNNs) and state-space models (SSMs) to leverage their strengths.
    DynaNet enhances interpretability and robustness in car-driving scenarios by combining
    powerful feature representation from DNNs with explicit modeling of physical processes
    from SSMs. The incorporation of a recursive Kalman filter enables optimal filtering
    on the feature state space, facilitating accurate positioning estimation, and
    showcasing its ability to detect failures through internal filtering model parameters
    such as the rate of innovation (Kalman gain). Instead of Kalman filter, [[48](#bib.bib48)]
    presents a differentiable particle filter with learnable motion and measurement
    models. The proposed differentiable particle filter can approximate complex nonlinear
    functions, allowing for efficient training of motion models by optimizing state
    estimation performance. Both two works incorporate the physical motion model of
    visual odometry into the state update process of filtering. Thus, the physical
    model serves as an algorithmic prior in the learning process. Compared with ConvNet
    or LSTM based models, differentiable filters improve the data efficiency and generalization
    ability of the learning based motion estimation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了几何模型，还有一些研究将物理运动模型与深度神经网络相结合，例如通过可微分的卡尔曼滤波器 [[45](#bib.bib45), [61](#bib.bib61)]
    或可微分的粒子滤波器 [[48](#bib.bib48)]。在 [[45](#bib.bib45)] 中，卡尔曼滤波器被转化为一个可微分模块，与深度神经网络结合进行端到端训练。[[61](#bib.bib61)]
    提出了 DynaNet，一个结合深度神经网络（DNNs）和状态空间模型（SSMs）的混合模型，以利用它们的优势。DynaNet 通过将 DNNs 强大的特征表示与
    SSMs 对物理过程的明确建模相结合，在汽车驾驶场景中提升了解释性和鲁棒性。递归卡尔曼滤波器的引入使得在特征状态空间上进行最佳滤波，从而促进准确的定位估计，并通过内部滤波模型参数（如创新率（卡尔曼增益））展示了检测失败的能力。[[48](#bib.bib48)]
    提出了一个具有可学习运动和测量模型的可微分粒子滤波器。所提出的可微分粒子滤波器能够近似复杂的非线性函数，允许通过优化状态估计性能来高效训练运动模型。这两项工作都将视觉里程计的物理模型纳入了滤波的状态更新过程。因此，物理模型在学习过程中作为算法先验。与基于
    ConvNet 或 LSTM 的模型相比，可微分滤波器提高了数据效率和基于学习的运动估计的泛化能力。
- en: 'In summary, hybrid models that combine geometric or physical priors with deep
    learning techniques are generally more accurate than end-to-end VO/SLAM systems
    and can even outperform conventional monocular VO systems on common benchmarks.
    Geometry-based models integrate deep neural networks into VO/SLAM pipelines to
    improve depth and egomotion estimation, as well as increase robustness to dynamic
    objects. Physical motion-based models combine deep neural networks with physical
    motion models, such as the Kalman filter or particle filter, to integrate the
    physical motion model of VO/SLAM systems into the learning process. Combining
    the benefits from combining geometric or physical priors with deep learning, hybrid
    models are normally more accurate than end-to-end VO at this stage, as shown in
    Table [II](#S3.T2 "TABLE II ‣ III-A Supervised Learning of Visual Odometry ‣ III
    Incremental Motion Estimation ‣ Deep Learning for Visual Localization and Mapping:
    A Survey"). It is notable that recent hybrid models even outperform some representative
    conventional monocular VO systems on common benchmarks [[17](#bib.bib17)]. This
    demonstrates the rapid rate of progress in this area.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '总结而言，将几何或物理先验与深度学习技术相结合的混合模型通常比端到端的 VO/SLAM 系统更准确，甚至在常见基准测试中超越了传统的单目 VO 系统。基于几何的模型将深度神经网络整合到
    VO/SLAM 流水线中，以改善深度和自我运动估计，并提高对动态物体的鲁棒性。基于物理运动的模型将深度神经网络与物理运动模型（如卡尔曼滤波器或粒子滤波器）结合起来，将
    VO/SLAM 系统的物理运动模型融入学习过程。结合几何或物理先验与深度学习的优势，混合模型在这一阶段通常比端到端 VO 更为准确，如表 [II](#S3.T2
    "TABLE II ‣ III-A Supervised Learning of Visual Odometry ‣ III Incremental Motion
    Estimation ‣ Deep Learning for Visual Localization and Mapping: A Survey") 所示。值得注意的是，最近的混合模型甚至在一些常见基准测试中超越了代表性的传统单目
    VO 系统 [[17](#bib.bib17)]。这展示了该领域的快速进展。'
- en: III-D Performance Comparison of Deep Learning based Visual Odometry (VO) Methods
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 基于深度学习的视觉里程计（VO）方法性能比较
- en: 'Table [II](#S3.T2 "TABLE II ‣ III-A Supervised Learning of Visual Odometry
    ‣ III Incremental Motion Estimation ‣ Deep Learning for Visual Localization and
    Mapping: A Survey") presents a comprehensive comparison of existing works focusing
    on deep learning-based visual odometry (VO). The table includes information regarding
    the sensor type utilized, the model employed, whether the method produces a trajectory
    with an absolute scale, and the performance evaluation conducted on the KITTI
    dataset. A concise overview of the contribution made by each model is also provided.
    The KITTI dataset [[22](#bib.bib22)] serves as a widely recognized benchmark for
    visual odometry estimation and comprises a collection of sensor data captured
    during car-driving scenarios. As most deep learning based approaches use the trajectory
    09 and 10 of the KITTI dataset to test a trained model, we compared them according
    to the averaged Root Mean Square Error (RMSE) of the translation for all the subsequences
    of lengths (100, 200, .., 800) meters, which is provided by the official KITTI
    VO/SLAM evaluation metrics.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [II](#S3.T2 "TABLE II ‣ III-A Supervised Learning of Visual Odometry ‣ III
    Incremental Motion Estimation ‣ Deep Learning for Visual Localization and Mapping:
    A Survey") 提供了一个关于基于深度学习的视觉里程计（VO）现有工作的全面比较。该表包含了所用传感器类型、所采用模型、方法是否生成带有绝对尺度的轨迹以及在
    KITTI 数据集上的性能评估等信息。每个模型的贡献也得到了简要概述。KITTI 数据集 [[22](#bib.bib22)] 是一个广泛认可的视觉里程计估计基准数据集，包含在汽车驾驶场景中捕获的传感器数据。由于大多数基于深度学习的方法使用
    KITTI 数据集的轨迹 09 和 10 来测试训练模型，我们根据官方 KITTI VO/SLAM 评估指标提供的所有长度（100、200、……、800）米子序列的平均均方根误差（RMSE）进行比较。'
- en: Hybrid VO models demonstrate superior performance compared to both supervised
    and unsupervised VO approaches. This is attributed to the hybrid model’s ability
    to leverage the well-established geometry models of traditional VO algorithms
    alongside the powerful feature extraction capabilities offered by deep learning
    methods. Although supervised VO models still outperform unsupervised approaches,
    the performance gap between them is diminishing as the limitations of self-supervised
    VO methods are gradually addressed. Notably, recent advancements have shown that
    self-supervised VO can now recover scale-consistent poses from monocular images
    [[37](#bib.bib37)]. Overall, data-driven visual odometry shows a remarkable increase
    in model performance, indicating the potentials of deep learning approaches in
    achieving more accurate visual odometry estimation in the future. However, it
    is worth noting that this upward trend is not always consistent, as several published
    papers focus on addressing intrinsic issues within learning frameworks rather
    than solely aiming to achieve the best performance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 混合 VO 模型表现出比监督式和无监督 VO 方法更优越的性能。这归因于混合模型能够利用传统 VO 算法中成熟的几何模型以及深度学习方法提供的强大特征提取能力。尽管监督式
    VO 模型仍然优于无监督方法，但由于自监督 VO 方法的局限性逐渐得到解决，它们之间的性能差距在缩小。值得注意的是，近期的进展表明，自监督 VO 现在可以从单目图像中恢复尺度一致的姿态
    [[37](#bib.bib37)]。总体而言，数据驱动的视觉里程计在模型性能上表现出显著的提升，表明深度学习方法在实现更精确的视觉里程计估计方面具有潜力。然而，值得注意的是，这一上升趋势并不总是稳定的，因为一些已发布的论文专注于解决学习框架中的固有问题，而不仅仅是追求最佳性能。
- en: 'TABLE III: A summary on existing methods on deep learning for relocalization
    in 2D map (Section [IV-A](#S4.SS1 "IV-A Relocalization in a 2D Map ‣ IV Global
    Relocalization ‣ Deep Learning for Visual Localization and Mapping: A Survey"))'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III：现有方法在 2D 地图重定位中的总结（第 [IV-A](#S4.SS1 "IV-A Relocalization in a 2D Map
    ‣ IV Global Relocalization ‣ Deep Learning for Visual Localization and Mapping:
    A Survey") 节）'
- en: '| Model | Year | Agnostic | Performance (m/degree) | Contributions |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 无关 | 性能（m/度） | 贡献 |'
- en: '| 7Scenes | Cambridge |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 7Scenes | 剑桥 |'
- en: '| Relocalization in 2D Map | Explicit Map | NN-Net [[62](#bib.bib62)] | 2017
    | Yes | 0.21/9.30 | - | combine retrieval and relative pose estimation |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 2D 地图中的重定位 | 显式地图 | NN-Net [[62](#bib.bib62)] | 2017 | 是 | 0.21/9.30 | -
    | 结合检索和相对位姿估计 |'
- en: '| DeLS-3D [[63](#bib.bib63)] | 2018 | No | - | - | jointly learn with semantics
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| DeLS-3D [[63](#bib.bib63)] | 2018 | 否 | - | - | 与语义共同学习 |'
- en: '| AnchorNet [[64](#bib.bib64)] | 2018 | Yes | 0.09/6.74 | 0.84/2.10 | anchor
    point allocation |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| AnchorNet [[64](#bib.bib64)] | 2018 | 是 | 0.09/6.74 | 0.84/2.10 | 锚点分配 |'
- en: '| RelocNet [[65](#bib.bib65)] | 2018 | Yes | 0.21/6.73 | - | camera frustum
    overlap loss |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| RelocNet [[65](#bib.bib65)] | 2018 | 是 | 0.21/6.73 | - | 相机视锥体重叠损失 |'
- en: '| CamNet [[66](#bib.bib66)] | 2019 | Yes | 0.04/1.69 | - | multi-stage image
    retrieval |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| CamNet [[66](#bib.bib66)] | 2019 | 是 | 0.04/1.69 | - | 多阶段图像检索 |'
- en: '| PixLoc [[67](#bib.bib67)] | 2021 | Yes | 0.03/0.98 | 0.15/0.25 | cast camera
    localization as metric learning |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| PixLoc [[67](#bib.bib67)] | 2021 | 是 | 0.03/0.98 | 0.15/0.25 | 将相机定位视作度量学习
    |'
- en: '| Implicit Map | PoseNet [[68](#bib.bib68)] | 2015 | No | 0.44/10.44 | 2.09/6.84
    | first neural network in global pose regression |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Implicit Map | PoseNet [[68](#bib.bib68)] | 2015 | 否 | 0.44/10.44 | 2.09/6.84
    | 全球姿态回归中的第一个神经网络 |'
- en: '| Bayesian PoseNet [[69](#bib.bib69)] | 2016 | No | 0.47/9.81 | 1.92/6.28 |
    estimate Bayesian uncertainty for global pose |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Bayesian PoseNet [[69](#bib.bib69)] | 2016 | 否 | 0.47/9.81 | 1.92/6.28 |
    估计全局姿态的贝叶斯不确定性 |'
- en: '| BranchNet [[70](#bib.bib70)] | 2017 | No | 0.29/8.30 | - | multi-task learning
    for orientation and translation |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| BranchNet [[70](#bib.bib70)] | 2017 | 否 | 0.29/8.30 | - | 面向方向和位移的多任务学习 |'
- en: '| VidLoc [[71](#bib.bib71)] | 2017 | No | 0.25/- | - | efficient localization
    from image sequences |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| VidLoc [[71](#bib.bib71)] | 2017 | 否 | 0.25/- | - | 从图像序列中高效定位 |'
- en: '| Geometric PoseNet [[72](#bib.bib72)] | 2017 | No | 0.23/8.12 | 1.63/2.86
    | geometry-aware loss |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Geometric PoseNet [[72](#bib.bib72)] | 2017 | 否 | 0.23/8.12 | 1.63/2.86 |
    几何感知损失 |'
- en: '| SVS-Pose [[73](#bib.bib73)] | 2017 | No | - | 1.33/5.17 | data augmentation
    in 3D space |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| SVS-Pose [[73](#bib.bib73)] | 2017 | 否 | - | 1.33/5.17 | 3D空间中的数据增强 |'
- en: '| LSTM PoseNet [[74](#bib.bib74)] | 2017 | No | 0.31/9.85 | 1.30/5.52 | spatial
    correlation |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| LSTM PoseNet [[74](#bib.bib74)] | 2017 | 否 | 0.31/9.85 | 1.30/5.52 | 空间相关性
    |'
- en: '| Hourglass PoseNet [[75](#bib.bib75)] | 2017 | No | 0.23/9.53 | - | hourglass-shaped
    architecture |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Hourglass PoseNet [[75](#bib.bib75)] | 2017 | 否 | 0.23/9.53 | - | 沙漏形状的架构
    |'
- en: '| MapNet [[76](#bib.bib76)] | 2018 | No | 0.21/7.77 | 1.63/3.64 | impose spatial
    and temporal constraints |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| MapNet [[76](#bib.bib76)] | 2018 | 否 | 0.21/7.77 | 1.63/3.64 | 强加空间和时间约束
    |'
- en: '| SPP-Net [[77](#bib.bib77)] | 2018 | No | 0.18/6.20 | 1.24/2.68 | synthetic
    data augmentation |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| SPP-Net [[77](#bib.bib77)] | 2018 | 否 | 0.18/6.20 | 1.24/2.68 | 合成数据增强 |'
- en: '| GPoseNet [[78](#bib.bib78)] | 2018 | No | 0.30/9.90 | 2.00/4.60 | hybrid
    model with Gaussian Process Regressor |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| GPoseNet [[78](#bib.bib78)] | 2018 | 否 | 0.30/9.90 | 2.00/4.60 | 与高斯过程回归器的混合模型
    |'
- en: '| LSG [[79](#bib.bib79)] | 2019 | No | 0.19/7.47 | - | odometry-aided localization
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| LSG [[79](#bib.bib79)] | 2019 | 否 | 0.19/7.47 | - | 里程计辅助定位 |'
- en: '| PVL [[80](#bib.bib80)] | 2019 | No | - | 1.60/4.21 | prior-guided dropout
    mask to improve robustness |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| PVL [[80](#bib.bib80)] | 2019 | 否 | - | 1.60/4.21 | 先验指导的丢弃掩码以提高鲁棒性 |'
- en: '| AdPR [[81](#bib.bib81)] | 2019 | No | 0.22/8.8 | - | adversarial architecture
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| AdPR [[81](#bib.bib81)] | 2019 | 否 | 0.22/8.8 | - | 对抗性架构 |'
- en: '| AtLoc [[82](#bib.bib82)] | 2019 | No | 0.20/7.56 | - | attention-guided spatial
    correlation |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| AtLoc [[82](#bib.bib82)] | 2019 | 否 | 0.20/7.56 | - | 基于注意力的空间相关性 |'
- en: '| GR-Net [[83](#bib.bib83)] | 2020 | No | 0.19/6.33 | 1.12/2.40 | construct
    a view graph |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| GR-Net [[83](#bib.bib83)] | 2020 | 否 | 0.19/6.33 | 1.12/2.40 | 构建视图图 |'
- en: '| MS-Transformer [[84](#bib.bib84)] | 2021 | Yes | 0.18/ 7.28 | 1.28/2.73 |
    extend to multiple scenes with transformers |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| MS-Transformer [[84](#bib.bib84)] | 2021 | 是 | 0.18/7.28 | 1.28/2.73 | 扩展到多个场景的变压器
    |'
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Year indicates the publication year (e.g. the date of conference) of each work.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 年份表示每项工作的出版年份（例如，会议日期）。
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Agnostic indicates whether it can generalize to new scenarios.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Agnostic 指示是否可以推广到新场景。
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Performance reports the position (m) and orientation (degree) error (a small
    number is better) on the 7-Scenes (Indoor)[[85](#bib.bib85)] and Cambridge (Outdoor)
    dataset[[68](#bib.bib68)]. Both datasets are split into training and testing set.
    We report the averaged error on the testing set.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能报告了在7-Scenes (室内)[[85](#bib.bib85)]和Cambridge (室外)数据集[[68](#bib.bib68)]上的位置（米）和方向（度）误差（数字越小越好）。两个数据集都被划分为训练集和测试集。我们报告了测试集上的平均误差。
- en: IV Global Relocalization
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 全球重定位
- en: 'Global relocalization is the process of determining the absolute camera pose
    within a known scene. Different from incremental motion estimation (visual odometry)
    that can perform in unfamiliar environments, global relocalization relies on prior
    knowledge of the scene and utilizes a 2D or 3D scene model. Basically, it establishes
    the relation between sensor observations and the map by matching a query image
    or view against a pre-built model, followed by returning an estimate of the global
    pose. According to the type of map used, deep learning-based methods for global
    relocalization can be categorized into two categories: *Relocalization in a 2D
    Map*, where input 2D images are matched against a database of geo-referenced images
    or an implicit neural map; *Relocalization in a 3D Map*, where correspondences
    are established between 2D image pixels and 3D points from an explicit or implicit
    scene model. Table [III](#S3.T3 "TABLE III ‣ III-D Performance Comparison of Deep
    Learning based Visual Odometry (VO) Methods ‣ III Incremental Motion Estimation
    ‣ Deep Learning for Visual Localization and Mapping: A Survey") and [IV](#S4.T4
    "TABLE IV ‣ IV-B1 Local Descriptor Based Relocalization ‣ IV-B Relocalization
    in a 3D Map ‣ IV Global Relocalization ‣ Deep Learning for Visual Localization
    and Mapping: A Survey") summarize the existing approaches in deep learning based
    global relocalization within a 2D map or a 3D map, respectively.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '全局重定位是确定已知场景中的绝对相机姿态的过程。与能够在陌生环境中执行的增量运动估计（视觉里程计）不同，全局重定位依赖于对场景的先验知识，并利用2D或3D场景模型。基本上，它通过将查询图像或视图与预构建的模型进行匹配，建立传感器观测与地图之间的关系，然后返回全局姿态的估计值。根据所使用的地图类型，基于深度学习的全局重定位方法可以分为两类：*2D地图中的重定位*，其中输入的2D图像与地理参考图像数据库或隐式神经网络地图进行匹配；*3D地图中的重定位*，其中在显式或隐式场景模型中建立2D图像像素与3D点之间的对应关系。表[III](#S3.T3
    "TABLE III ‣ III-D Performance Comparison of Deep Learning based Visual Odometry
    (VO) Methods ‣ III Incremental Motion Estimation ‣ Deep Learning for Visual Localization
    and Mapping: A Survey")和[IV](#S4.T4 "TABLE IV ‣ IV-B1 Local Descriptor Based Relocalization
    ‣ IV-B Relocalization in a 3D Map ‣ IV Global Relocalization ‣ Deep Learning for
    Visual Localization and Mapping: A Survey")分别总结了基于深度学习的2D地图或3D地图中的全局重定位的现有方法。'
- en: IV-A Relocalization in a 2D Map
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 2D地图中的重定位
- en: Relocalization in a 2D map involves estimating the image pose relative to a
    2D map. This type of map can be created explicitly using a geo-referenced database
    or implicitly encoded within a neural network.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在2D地图中的重定位涉及相对于2D地图估计图像姿态。这种类型的地图可以通过使用地理参考数据库显式创建，或通过神经网络隐式编码。
- en: '![Refer to caption](img/7b36fc3820b735bbcfb3e78fba95cb93.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7b36fc3820b735bbcfb3e78fba95cb93.png)'
- en: 'Figure 4: The typical architectures of relocalization in 2D map through (a)
    explicit map, i.e. RelocNet [[65](#bib.bib65)] and (b) implicit map, i.e. e.g.
    PoseNet [[68](#bib.bib68)]'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：通过(a)显式地图，即RelocNet [[65](#bib.bib65)] 和 (b)隐式地图，例如PoseNet [[68](#bib.bib68)]的2D地图中的典型重定位架构
- en: IV-A1 Explicit 2D Map Based Relocalization
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 显式2D地图基础的重定位
- en: 'Explicit 2D map based relocalization typically represents a scene by a database
    of geo-tagged images (references) [[86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88)].
    Figure [4](#S4.F4 "Figure 4 ‣ IV-A Relocalization in a 2D Map ‣ IV Global Relocalization
    ‣ Deep Learning for Visual Localization and Mapping: A Survey") (a) illustrates
    the two stages of this relocalization process with 2D references: image retrieval
    and pose regression.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '显式2D地图基础的重定位通常通过地理标记图像（参考）[[86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88)]的数据库来表示场景。图[4](#S4.F4
    "Figure 4 ‣ IV-A Relocalization in a 2D Map ‣ IV Global Relocalization ‣ Deep
    Learning for Visual Localization and Mapping: A Survey") (a) 展示了这种重定位过程的两个阶段：图像检索和姿态回归。'
- en: In the first stage, the goal is to determine the most relevant part of the scene
    represented by reference images to the visual query. This is achieved by finding
    suitable image descriptors for image retrieval, which is a challenging task. Deep
    learning-based approaches [[89](#bib.bib89), [90](#bib.bib90)] use pre-trained
    convolutional neural networks (ConvNets) to extract image-level features that
    are invariant to changes in viewpoint, lighting, and other factors that can affect
    image appearance. In challenging situations, local descriptors are extracted and
    aggregated to obtain robust global descriptors. For instance, NetVLAD [[91](#bib.bib91)]
    uses a trainable generalized VLAD layer (Vector of Locally Aggregated Descriptors
    [[92](#bib.bib92)], a descriptor vector used in image retrieval), while CamNet
    [[66](#bib.bib66)] applies a two-stage retrieval approach that combines image-based
    coarse retrieval and pose-based fine retrieval to select the most similar reference
    frames for the final precise pose estimation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，目标是确定由参考图像表示的场景中与视觉查询最相关的部分。这是通过寻找适合图像检索的图像描述符来实现的，这是一项具有挑战性的任务。基于深度学习的方法[[89](#bib.bib89),
    [90](#bib.bib90)]使用预训练的卷积神经网络（ConvNets）来提取对视角、光照以及其他可能影响图像外观的因素不变的图像级特征。在具有挑战性的情况下，提取并聚合局部描述符以获得稳健的全局描述符。例如，NetVLAD
    [[91](#bib.bib91)]使用可训练的广义VLAD层（Vector of Locally Aggregated Descriptors [[92](#bib.bib92)]，一种用于图像检索的描述符向量），而CamNet
    [[66](#bib.bib66)]应用两阶段检索方法，结合基于图像的粗略检索和基于姿态的精细检索，以选择最相似的参考帧进行最终的精确姿态估计。
- en: The second stage of explicit 2D map-based relocalization aims to obtain more
    precise poses of the queries by performing additional relative pose estimation
    with respect to the retrieved images. Traditionally, this is tackled by epipolar
    geometry, relying on the 2D-2D correspondences determined by local descriptors
    [[93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]. In contrast, deep learning-based
    approaches regress the relative poses directly from pairwise images. For example,
    NN-Net [[62](#bib.bib62)] uses a neural network to estimate the pairwise relative
    poses between the query and the top N ranked references, followed by a triangulation-based
    fusion algorithm that coalesces the predicted N relative poses and the ground
    truth of 3D geometry poses to obtain the absolute query pose. Alternatively, RelocNet
    [[65](#bib.bib65)] introduces a frustum overlap loss to assist global descriptors
    learning that are suitable for camera localization.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 明确的2D地图基础重定位的第二阶段旨在通过对检索到的图像执行额外的相对姿态估计，获得查询的更精确姿态。传统上，这通过极线几何来解决，依赖于由局部描述符确定的2D-2D对应关系[[93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95)]。相比之下，基于深度学习的方法直接从成对图像回归相对姿态。例如，NN-Net [[62](#bib.bib62)]使用神经网络来估计查询图像和排名前N的参考图像之间的成对相对姿态，随后使用基于三角测量的融合算法将预测的N个相对姿态与3D几何姿态的真实值结合，以获得绝对查询姿态。或者，RelocNet
    [[65](#bib.bib65)]引入了一个视锥体重叠损失来辅助全局描述符的学习，使其适用于相机定位。
- en: Explicit 2D map-based relocalization is scalable and flexible, as it does not
    require training on specific scenarios. However, maintaining a database of geo-tagged
    images and accurate image retrieval can be challenging, making it difficult to
    scale to large-scale scenarios. Moreover, explicit 2D map-based relocalization
    is normally time-consuming compared to implicit-map-based counterparts, which
    will be discussed in the next section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 明确的2D地图基础重定位是可扩展和灵活的，因为它不需要在特定场景上进行训练。然而，维护一个地理标记图像的数据库和准确的图像检索可能具有挑战性，这使得在大规模场景中扩展变得困难。此外，与隐式地图基础的对应方法相比，明确的2D地图基础重定位通常比较耗时，这将在下一节中讨论。
- en: IV-A2 Implicit 2D Map Based Relocalization
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 隐式2D地图基础重定位
- en: 'Implicit 2D map based relocalization directly regresses camera pose from single
    images, by implicitly representing a 2D map inside a deep neural network. The
    common pipeline is illustrated in Figure [4](#S4.F4 "Figure 4 ‣ IV-A Relocalization
    in a 2D Map ‣ IV Global Relocalization ‣ Deep Learning for Visual Localization
    and Mapping: A Survey") (b) - the input to a neural network is single images,
    while the output is the global position and orientation of query images.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '隐式2D地图基础重定位通过在深度神经网络内部隐式表示2D地图，从单张图像中直接回归相机姿态。常见的流程如图[4](#S4.F4 "Figure 4 ‣
    IV-A Relocalization in a 2D Map ‣ IV Global Relocalization ‣ Deep Learning for
    Visual Localization and Mapping: A Survey") (b)所示——神经网络的输入是单张图像，而输出是查询图像的全局位置和方向。'
- en: PoseNet [[68](#bib.bib68)] is the first approach to tackle the camera relocalization
    problem by training a ConvNet to predict camera pose from single RGB images in
    an end-to-end manner. It leverages the main structure of GoogleNet [[96](#bib.bib96)]
    to extract visual features and removes the last softmax layers. Instead, a fully
    connected layer is introduced to output a 7 dimensional global pose, which consists
    of position and orientation vectors in 3 and 4 dimensions, respectively. However,
    PoseNet has some limitations. It is designed with a naive regression loss function
    that does not take into account the underlying geometry of the problem. This leads
    to hyper-parameters requiring expensive hand-engineering to be tuned, and it may
    not generalize well to new scenes. Additionally, due to the high dimensionality
    of the feature embedding and limited training data, PoseNet suffers from overfitting
    problems.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: PoseNet [[68](#bib.bib68)] 是首个通过训练卷积神经网络（ConvNet）从单张RGB图像中预测相机姿态来解决相机重定位问题的方案。它利用GoogleNet
    [[96](#bib.bib96)] 的主要结构来提取视觉特征，并去除了最后的softmax层。取而代之的是引入了一个全连接层来输出一个7维的全局姿态，其中包括3维和4维的位置和方向向量。然而，PoseNet有一些局限性。它设计了一个简单的回归损失函数，没有考虑到问题的底层几何结构。这导致了超参数需要昂贵的人工调整，并且可能对新场景的泛化能力较差。此外，由于特征嵌入的高维度和有限的训练数据，PoseNet存在过拟合问题。
- en: Various extensions are proposed to enhance the original pipeline, for example,
    by exploiting LSTM units to reduce the dimensionality [[74](#bib.bib74)], applying
    synthetic generation to augment training data [[73](#bib.bib73), [70](#bib.bib70),
    [77](#bib.bib77), [97](#bib.bib97)], replacing the backbone [[75](#bib.bib75)],
    modelling pose uncertainty [[69](#bib.bib69), [78](#bib.bib78), [98](#bib.bib98)],
    introducing geometry-aware loss function [[72](#bib.bib72)] and associating features
    via an attention mechanism [[82](#bib.bib82)]. A prior guided dropout mask is
    additionally adopted in RVL [[80](#bib.bib80)] to further eliminate the uncertainty
    caused by dynamic objects. VidLoc [[71](#bib.bib71)] incorporates temporal constraints
    of image sequences to model the temporal connections of input images for visual
    localization. Moreover, additional motion constraints, including spatial constraints
    and other sensor constraints from GPS or SLAM systems are exploited in MapNet
    [[76](#bib.bib76)], to enforce the motion consistency between predicted poses.
    Similar motion constraints are also introduced by jointly optimizing a relocalization
    network and visual odometry network [[99](#bib.bib99), [79](#bib.bib79), [100](#bib.bib100)].
    However, being application-specific, scene representations learned from localization
    tasks may ignore some useful features they are not designed for. To this end,
    VLocNet++ [[101](#bib.bib101)] additionally exploits the inter-task relationship
    between learning semantics and regressing poses, achieving impressive results.
    More recently, Graph Neural Networks (GNNs) are introduced to tackle the multi-view
    camera relocalization task in GR-Net [[83](#bib.bib83)] and PoGO-Net [[102](#bib.bib102)],
    enabling the messages of different frames to be transferred beyond temporal connections.
    MS-Transformer [[84](#bib.bib84)] extends the absolute pose regression paradigm
    for learning a single model on multiple scenes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强原始管道，提出了各种扩展方案，例如，通过利用LSTM单元来降低维度 [[74](#bib.bib74)]，应用合成生成来扩充训练数据 [[73](#bib.bib73),
    [70](#bib.bib70), [77](#bib.bib77), [97](#bib.bib97)]，更换主干网络 [[75](#bib.bib75)]，建模姿态不确定性
    [[69](#bib.bib69), [78](#bib.bib78), [98](#bib.bib98)]，引入几何感知损失函数 [[72](#bib.bib72)]
    和通过注意力机制关联特征 [[82](#bib.bib82)]。在RVL [[80](#bib.bib80)] 中，还额外采用了先验引导的丢弃掩码，以进一步消除由动态物体引起的不确定性。VidLoc
    [[71](#bib.bib71)] 结合了图像序列的时间约束来建模输入图像的时间连接，以进行视觉定位。此外，MapNet [[76](#bib.bib76)]
    利用包括空间约束和来自GPS或SLAM系统的其他传感器约束在内的额外运动约束，以强制预测姿态之间的运动一致性。类似的运动约束也通过联合优化重定位网络和视觉里程计网络
    [[99](#bib.bib99), [79](#bib.bib79), [100](#bib.bib100)] 引入。然而，由于应用特定性，从定位任务中学到的场景表示可能忽略一些它们没有设计的有用特征。为此，VLocNet++
    [[101](#bib.bib101)] 额外利用了学习语义和回归姿态之间的任务间关系，取得了令人印象深刻的结果。最近，图神经网络（GNNs）被引入到GR-Net
    [[83](#bib.bib83)] 和PoGO-Net [[102](#bib.bib102)] 中，处理多视角相机重定位任务，使得不同帧的信息能够超越时间连接进行传递。MS-Transformer
    [[84](#bib.bib84)] 扩展了绝对姿态回归范式，用于在多个场景上学习单一模型。
- en: Both explicit and implicit 2D map-based relocalization methods exploit the benefits
    of deep learning in automatically extracting crucial features for global relocalization
    in environments lacking distinctive features. Implicit map-based learning approaches
    directly regress the absolute pose of a camera through a DNN, making them easier
    to implement and more efficient than explicit map-based learning approaches. However,
    current implicit map-based approaches exhibit performance limitations, and their
    dependence on scene-specific training prevents them from generalizing to unfamiliar
    scenes without necessitating retraining. In the next section, we will introduce
    the concept of learning to match images against a 3D model for global relocalization.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 显式和隐式的2D地图基础重定位方法都利用深度学习自动提取关键特征的优势，用于在缺乏独特特征的环境中进行全局重定位。隐式地图基础学习方法通过DNN直接回归相机的绝对姿态，使其比显式地图基础学习方法更易于实现和更高效。然而，当前的隐式地图基础方法表现出性能限制，它们对特定场景的训练依赖使得它们无法在没有重新训练的情况下推广到不熟悉的场景。在下一节中，我们将介绍学习如何将图像与3D模型匹配以进行全局重定位的概念。
- en: IV-B Relocalization in a 3D Map
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 3D地图中的重定位
- en: 'Relocalization in a 3D map involves recovering the camera pose of a 2D image
    with respect to a pre-built 3D scene model. This 3D map is constructed from color
    images using approaches such as structure-from-motion (SfM) [[12](#bib.bib12)]
    or range images using approaches such as truncated-signed-distance-function (TSDF)
    [[103](#bib.bib103)]. As depicted in Figure [5](#S4.F5 "Figure 5 ‣ IV-B1 Local
    Descriptor Based Relocalization ‣ IV-B Relocalization in a 3D Map ‣ IV Global
    Relocalization ‣ Deep Learning for Visual Localization and Mapping: A Survey"),
    3D map based methods establish 2D-3D correspondences between the 2D pixels of
    a query image and the 3D points using local descriptors [[104](#bib.bib104), [105](#bib.bib105),
    [106](#bib.bib106), [107](#bib.bib107)] or scene coordinate regression [[108](#bib.bib108),
    [109](#bib.bib109), [85](#bib.bib85), [110](#bib.bib110)]. These 2D-3D matches
    are then used to compute the camera pose by applying a Perspective-n-Point (PnP)
    solver [[111](#bib.bib111)] within a RANSAC loop [[112](#bib.bib112)].'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '在3D地图中进行重定位涉及根据预构建的3D场景模型恢复2D图像的相机姿态。该3D地图是使用如结构从运动（SfM）[[12](#bib.bib12)]或使用如截断签名距离函数（TSDF）[[103](#bib.bib103)]的方法从彩色图像或范围图像构建的。如图[5](#S4.F5
    "Figure 5 ‣ IV-B1 Local Descriptor Based Relocalization ‣ IV-B Relocalization
    in a 3D Map ‣ IV Global Relocalization ‣ Deep Learning for Visual Localization
    and Mapping: A Survey")所示，基于3D地图的方法通过使用局部描述符[[104](#bib.bib104), [105](#bib.bib105),
    [106](#bib.bib106), [107](#bib.bib107)]或场景坐标回归[[108](#bib.bib108), [109](#bib.bib109),
    [85](#bib.bib85), [110](#bib.bib110)]在查询图像的2D像素与3D点之间建立2D-3D对应关系。然后，这些2D-3D匹配被用于通过在RANSAC循环[[112](#bib.bib112)]中应用透视-n-点（PnP）求解器[[111](#bib.bib111)]来计算相机姿态。'
- en: IV-B1 Local Descriptor Based Relocalization
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 基于局部描述符的重定位
- en: 'Local descriptor based relocalization relies on establishing correspondences
    between 2D map inputs and the given explicit 3D model using feature descriptors.
    As the learning of feature descriptor is typically coupled with keypoint detection,
    existing learning methods can be divided into three types: detect-then-describe,
    detect-and-describe, and describe-then-detect, according to the role of detector
    and descriptor in the learning process.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 基于局部描述符的重定位依赖于使用特征描述符在2D地图输入和给定的显式3D模型之间建立对应关系。由于特征描述符的学习通常与关键点检测相关联，现有的学习方法可以根据检测器和描述符在学习过程中的角色分为三种类型：检测-然后-描述、检测和描述以及描述-然后-检测。
- en: 'TABLE IV: A summary on existing methods on deep learning based relocalization
    in a 3D map (Section [IV-B](#S4.SS2 "IV-B Relocalization in a 3D Map ‣ IV Global
    Relocalization ‣ Deep Learning for Visual Localization and Mapping: A Survey"))'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '表IV：现有的基于深度学习的3D地图重定位方法总结（第[IV-B](#S4.SS2 "IV-B Relocalization in a 3D Map
    ‣ IV Global Relocalization ‣ Deep Learning for Visual Localization and Mapping:
    A Survey")节）'
- en: '| Model | Year | Agnostic | Performance (m/degree) | Contributions |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 无关 | 性能（m/度） | 贡献 |'
- en: '| 7Scenes | Cambridge |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 7Scenes | 剑桥 |'
- en: '| Relocalization in 3D Map | Descriptor Based | NetVLAD [[91](#bib.bib91)]
    | 2016 | Yes | - | - | differentiable VLAD layer |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 3D地图中的重定位 | 基于描述符 | NetVLAD [[91](#bib.bib91)] | 2016 | 是 | - | - | 可微分的VLAD层
    |'
- en: '| DELF [[113](#bib.bib113)] | 2017 | Yes | - | - | attentive local feature
    descriptor |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| DELF [[113](#bib.bib113)] | 2017 | 是 | - | - | 注意力局部特征描述符 |'
- en: '| InLoc [[114](#bib.bib114)] | 2018 | Yes | 0.04/1.38 | 0.31/0.73 | dense data
    association |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| InLoc [[114](#bib.bib114)] | 2018 | 是 | 0.04/1.38 | 0.31/0.73 | 密集数据关联 |'
- en: '| SVL [[115](#bib.bib115)] | 2018 | No | - | - | leverage a generative model
    for descriptor learning |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| SVL [[115](#bib.bib115)] | 2018 | 否 | - | - | 利用生成模型进行描述符学习 |'
- en: '| SuperPoint [[116](#bib.bib116)] | 2018 | Yes | - | - | jointly extract interest
    points and descriptors |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| SuperPoint [[116](#bib.bib116)] | 2018 | 是 | - | - | 联合提取兴趣点和描述符 |'
- en: '| Sarlin et al. [[117](#bib.bib117)] | 2018 | Yes | - | - | hierarchical localization
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Sarlin 等 [[117](#bib.bib117)] | 2018 | 是 | - | - | 分层定位 |'
- en: '| NC-Net [[118](#bib.bib118)] | 2018 | Yes | - | - | neighbourhood consensus
    constraints |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| NC-Net [[118](#bib.bib118)] | 2018 | 是 | - | - | 邻域共识约束 |'
- en: '| 2D3D-MatchNet [[119](#bib.bib119)] | 2019 | Yes | - | - | jointly learn the
    descriptors for 2D and 3D keypoints |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 2D3D-MatchNet [[119](#bib.bib119)] | 2019 | 是 | - | - | 联合学习 2D 和 3D 关键点的描述符
    |'
- en: '| HF-Net [[120](#bib.bib120)] | 2019 | Yes | 0.042/1.3 | 0.356/0.31 | coarse-to-fine
    localization |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| HF-Net [[120](#bib.bib120)] | 2019 | 是 | 0.042/1.3 | 0.356/0.31 | 粗到细的定位
    |'
- en: '| D2-Net [[121](#bib.bib121)] | 2019 | Yes | - | - | jointly learn keypoints
    and descriptors |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| D2-Net [[121](#bib.bib121)] | 2019 | 是 | - | - | 联合学习关键点和描述符 |'
- en: '| Speciale et al [[122](#bib.bib122)] | 2019 | No | - | - | privacy preserving
    localization |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Speciale 等 [[122](#bib.bib122)] | 2019 | 否 | - | - | 隐私保护定位 |'
- en: '| OOI-Net [[123](#bib.bib123)] | 2019 | No | - | - | objects-of-interest annotations
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| OOI-Net [[123](#bib.bib123)] | 2019 | 否 | - | - | 关注对象标注 |'
- en: '| Camposeco et al. [[124](#bib.bib124)] | 2019 | Yes | - | 0.56/0.66 | hybrid
    scene compression for localization |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Camposeco 等 [[124](#bib.bib124)] | 2019 | 是 | - | 0.56/0.66 | 用于定位的混合场景压缩
    |'
- en: '| Cheng et al. [[125](#bib.bib125)] | 2019 | Yes | - | - | cascaded parallel
    filtering |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Cheng 等 [[125](#bib.bib125)] | 2019 | 是 | - | - | 级联并行过滤 |'
- en: '| Taira et al. [[126](#bib.bib126)] | 2019 | Yes | - | - | comprehensive analysis
    of pose verification |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Taira 等 [[126](#bib.bib126)] | 2019 | 是 | - | - | 姿态验证的综合分析 |'
- en: '| R2D2 [[127](#bib.bib127)] | 2019 | Yes | - | - | learn a predictor of the
    descriptor discriminativeness |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| R2D2 [[127](#bib.bib127)] | 2019 | 是 | - | - | 学习描述符的区分性预测器 |'
- en: '| ASLFeat [[128](#bib.bib128)] | 2020 | Yes | - | - | leverage deformable convolutional
    networks |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[128](#bib.bib128)] | 2020 | 是 | - | - | 利用可变形卷积网络 |'
- en: '| CD-VLM [[129](#bib.bib129)] | 2021 | Yes | - | - | cross-descriptor matching
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| CD-VLM [[129](#bib.bib129)] | 2021 | 是 | - | - | 跨描述符匹配 |'
- en: '| VS-Net [[130](#bib.bib130)] | 2021 | No | 0.024/0.8 | 0.136/0.24 | vote by
    segmentation |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| VS-Net [[130](#bib.bib130)] | 2021 | 否 | 0.024/0.8 | 0.136/0.24 | 通过分割进行投票
    |'
- en: '| Scene Coordinate Regression | DSAC [[131](#bib.bib131)] | 2017 | No | 0.20/6.3
    | 0.32/0.78 | differentiable RANSAC |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 场景坐标回归 | DSAC [[131](#bib.bib131)] | 2017 | 否 | 0.20/6.3 | 0.32/0.78 | 可微分的
    RANSAC |'
- en: '| DSAC++ [[132](#bib.bib132)] | 2018 | No | 0.08/2.40 | 0.19/0.50 | without
    using a 3D model of the scene |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| DSAC++ [[132](#bib.bib132)] | 2018 | 否 | 0.08/2.40 | 0.19/0.50 | 不使用场景的 3D
    模型 |'
- en: '| Angle DSAC++ [[133](#bib.bib133)] | 2018 | No | 0.06/1.47 | 0.17/0.50 | angle-based
    reprojection loss |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Angle DSAC++ [[133](#bib.bib133)] | 2018 | 否 | 0.06/1.47 | 0.17/0.50 | 基于角度的重投影损失
    |'
- en: '| Dense SCR [[134](#bib.bib134)] | 2018 | No | 0.04/1.4 | - | full frame scene
    coordinate regression |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Dense SCR [[134](#bib.bib134)] | 2018 | 否 | 0.04/1.4 | - | 全帧场景坐标回归 |'
- en: '| Confidence SCR [[135](#bib.bib135)] | 2018 | No | 0.06/3.1 | - | model uncertainty
    of correspondences |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Confidence SCR [[135](#bib.bib135)] | 2018 | 否 | 0.06/3.1 | - | 对应关系的不确定性模型
    |'
- en: '| ESAC [[136](#bib.bib136)] | 2019 | No | 0.034/1.50 | - | integrates DSAC
    in a Mixture of Experts |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| ESAC [[136](#bib.bib136)] | 2019 | 否 | 0.034/1.50 | - | 在专家混合模型中集成 DSAC |'
- en: '| NG-RANSAC [[137](#bib.bib137)] | 2019 | No | - | 0.24/0.30 | prior-guided
    model hypothesis search |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| NG-RANSAC [[137](#bib.bib137)] | 2019 | 否 | - | 0.24/0.30 | 基于先验的模型假设搜索 |'
- en: '| SANet [[138](#bib.bib138)] | 2019 | Yes | 0.05/1.68 | 0.23/0.53 | scene agnostic
    architecture for camera localization |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[138](#bib.bib138)] | 2019 | 是 | 0.05/1.68 | 0.23/0.53 | 用于相机定位的场景无关架构
    |'
- en: '| MV-SCR [[139](#bib.bib139)] | 2019 | No | 0.05/1.63 | 0.17/0.40 | multi-view
    constraints |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| MV-SCR [[139](#bib.bib139)] | 2019 | 否 | 0.05/1.63 | 0.17/0.40 | 多视角约束 |'
- en: '| HSC-Net [[140](#bib.bib140)] | 2020 | No | 0.03/0.90 | 0.13/0.30 | hierarchical
    scene coordinate network |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| HSC-Net [[140](#bib.bib140)] | 2020 | 否 | 0.03/0.90 | 0.13/0.30 | 分层场景坐标网络
    |'
- en: '| KFNet [[141](#bib.bib141)] | 2020 | No | 0.03/0.88 | 0.13/0.30 | extends
    the problem to the time domain |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| KFNet [[141](#bib.bib141)] | 2020 | 否 | 0.03/0.88 | 0.13/0.30 | 扩展到时间域的问题
    |'
- en: '| DSM [[142](#bib.bib142)] | 2021 | Yes | 0.027/0.92 | 0.27/0.52 | dense coordinates
    prediction |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| DSM [[142](#bib.bib142)] | 2021 | 是 | 0.027/0.92 | 0.27/0.52 | 密集坐标预测 |'
- en: •
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Year indicates the publication year (e.g. the date of conference) of each work.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 年份表示每项工作的出版年份（例如会议日期）。
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Agnostic indicates whether it can generalize to new scenarios.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表示是否可以推广到新场景的能力。
- en: •
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Performance reports the position (m) and orientation (degree) error (a small
    number is better) on the 7-Scenes (Indoor)[[85](#bib.bib85)] and Cambridge (Outdoor)
    dataset[[68](#bib.bib68)]. Both datasets are split into training and testing set.
    We report the averaged error on the testing set.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能报告了7-Scenes（室内）[[85](#bib.bib85)]和Cambridge（室外）数据集[[68](#bib.bib68)]中的位置（m）和方向（度数）误差（误差越小越好）。这两个数据集都被划分为训练集和测试集。我们报告了测试集上的平均误差。
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贡献总结了每项工作的主要贡献，相较于以往的研究。
- en: Detect-then-describe is a common pipeline for local descriptor-based relocalization.
    This approach first performs feature detection and then extracts a feature descriptor
    from a patch centered around each keypoint [[143](#bib.bib143), [144](#bib.bib144)].
    The keypoint detector is responsible for providing robustness or invariance against
    possible real issues such as scale transformation, rotation, or viewpoint changes
    by normalizing the patch accordingly. However, some of these responsibilities
    might also be delegated to the descriptor. The common pipeline varies from using
    hand-crafted detectors [[145](#bib.bib145), [146](#bib.bib146)] and descriptors
    [[147](#bib.bib147), [148](#bib.bib148)], replacing either the descriptor [[149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [118](#bib.bib118), [152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156),
    [157](#bib.bib157)] or detector [[158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)]
    with a learned alternative, or learning both the detector and descriptor [[161](#bib.bib161),
    [162](#bib.bib162), [163](#bib.bib163), [164](#bib.bib164)]. For efficiency, the
    feature detector often considers only small image regions and typically focuses
    on low-level structures such as corners or blobs [[165](#bib.bib165)], while the
    descriptor often captures higher level information in a larger patch around the
    keypoint.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Detect-then-describe是一种常见的基于局部描述符的重新定位流程。这种方法首先进行特征检测，然后从围绕每个关键点的补丁中提取特征描述符[[143](#bib.bib143),
    [144](#bib.bib144)]。关键点检测器负责提供对可能的实际问题（如尺度变化、旋转或视角变化）的鲁棒性或不变性，通过相应地标准化补丁。然而，这些职责中的一些也可能被委托给描述符。常见的流程包括使用手工制作的检测器[[145](#bib.bib145),
    [146](#bib.bib146)]和描述符[[147](#bib.bib147), [148](#bib.bib148)]，用学习的替代品替换描述符[[149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [118](#bib.bib118), [152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156),
    [157](#bib.bib157)]或检测器[[158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)]，或者同时学习检测器和描述符[[161](#bib.bib161),
    [162](#bib.bib162), [163](#bib.bib163), [164](#bib.bib164)]。为了提高效率，特征检测器通常只考虑小的图像区域，通常集中于低级结构，如角点或斑点[[165](#bib.bib165)]，而描述符通常在关键点周围较大的补丁中捕捉更高级的信息。
- en: In contrast, detect-and-describe approaches advance the description stage. By
    sharing a representation from deep neural network, SuperPoint [[116](#bib.bib116)]
    and R2D2 [[127](#bib.bib127)] attempt to learn a dense feature descriptor and
    a feature detector. However, they rely on different decoder branches which are
    trained independently with specific losses. On the contrary, D2-net [[121](#bib.bib121)]
    and ASLFeat [[128](#bib.bib128)] share all parameters between detection and description
    and use a joint formulation that simultaneously optimizes for both tasks. Different
    from these works, which purely rely on image features, P2-Net [[166](#bib.bib166)]
    proposes a unified descriptor between 2D and 3D representations for pixel and
    point matching.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，detect-and-describe方法推进了描述阶段。通过共享来自深度神经网络的表示，SuperPoint [[116](#bib.bib116)]
    和 R2D2 [[127](#bib.bib127)] 尝试学习稠密的特征描述符和特征检测器。然而，它们依赖于不同的解码器分支，这些分支独立训练，具有特定的损失函数。相反，D2-net
    [[121](#bib.bib121)] 和 ASLFeat [[128](#bib.bib128)] 在检测和描述之间共享所有参数，并使用联合公式同时优化这两个任务。与这些纯粹依赖于图像特征的工作不同，P2-Net
    [[166](#bib.bib166)] 提出了2D和3D表示之间的统一描述符，用于像素和点匹配。
- en: Alternatively, the describe-then-detect approach, e.g. D2D [[167](#bib.bib167)],
    postpones the detection to a later stage but applies such detector on pre-learned
    dense descriptors to extract a sparse set of keypoints and corresponding descriptors.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，describe-then-detect方法，例如D2D [[167](#bib.bib167)]，将检测推迟到后期阶段，但在预先学习的稠密描述符上应用这种检测器，以提取稀疏的关键点集和相应的描述符。
- en: '![Refer to caption](img/a74de10fa8f1d156749d7844e9847a97.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/a74de10fa8f1d156749d7844e9847a97.png)'
- en: 'Figure 5: The typical architectures of 3D Map based relocalization through
    (a) descriptor matching, i.e. HF-Net [[120](#bib.bib120)] and (b) scene coordinate
    regression, i.e. Confidence SCR [[135](#bib.bib135)].'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于3D地图的重定位的典型架构，包括（a）描述符匹配，即HF-Net [[120](#bib.bib120)] 和（b）场景坐标回归，即Confidence
    SCR [[135](#bib.bib135)]。
- en: In practice, descriptors are commonly used to perform sparse feature extraction
    and matching for the requirement of efficiency with keypoint detector. Moreover,
    by disabling the function of keypoint detector, dense feature extraction and matching
    [[168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170), [115](#bib.bib115),
    [114](#bib.bib114), [171](#bib.bib171), [172](#bib.bib172)], show better matching
    results than sparse feature matching, particularly under strong variations in
    illumination [[173](#bib.bib173)]. More recently, new approaches have been proposed
    to establish correspondence for visual localization. For example, CD-VLM [[129](#bib.bib129)]
    uses cross-descriptor matching to overcome challenges in cross-seasonal and cross-domain
    visual localization. VS-Net [[130](#bib.bib130)] proposes a scene-specific landmark-based
    approach, which uses a set of keyframe-based landmarks to establish correspondences
    in visual localization. These new approaches offer promising alternatives for
    robust and accurate visual localization.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中，描述符通常用于进行稀疏特征提取和匹配，以满足与关键点检测器的效率要求。此外，通过禁用关键点检测器的功能，密集特征提取和匹配[[168](#bib.bib168),
    [169](#bib.bib169), [170](#bib.bib170), [115](#bib.bib115), [114](#bib.bib114),
    [171](#bib.bib171), [172](#bib.bib172)]显示出比稀疏特征匹配更好的匹配结果，尤其是在光照变化剧烈的情况下[[173](#bib.bib173)]。最近，提出了新的方法来建立视觉定位的对应关系。例如，CD-VLM
    [[129](#bib.bib129)]通过交叉描述符匹配来克服跨季节和跨领域视觉定位中的挑战。VS-Net [[130](#bib.bib130)]提出了一种基于场景特定地标的方法，该方法利用一组基于关键帧的地标来建立视觉定位中的对应关系。这些新方法为鲁棒且准确的视觉定位提供了有希望的替代方案。
- en: IV-B2 Scene Coordinate Regression Based Localization
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 场景坐标回归基于的定位
- en: Different from local descriptor-based relocalization which relies on matching
    descriptors between images and an explicit 3D map to establish 2D-3D correspondences,
    scene coordinate regression approaches eliminate the need for explicit 3D map
    construction and descriptor extraction, making it relatively more efficient. Instead
    of relying on explicit 3D maps, these methods learn an implicit transformation
    from 2D pixel coordinates to 3D point coordinates. By estimating the 3D coordinates
    of each pixel in the query image within the world coordinate system (i.e., the
    scene coordinates [[85](#bib.bib85), [174](#bib.bib174)]), these approaches allow
    for more flexibility in dealing with different environments and scene structures.
    This makes scene coordinate regression a promising alternative for relocalization
    tasks, especially in scenarios where explicit 3D maps may not be available or
    accurate enough.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖于图像之间的描述符匹配和显式3D地图来建立2D-3D对应关系的局部描述符基重定位不同，场景坐标回归方法消除了对显式3D地图构建和描述符提取的需求，使其相对更高效。这些方法并不依赖显式3D地图，而是学习从2D像素坐标到3D点坐标的隐式变换。通过估计查询图像中每个像素在世界坐标系统中的3D坐标（即场景坐标[[85](#bib.bib85),
    [174](#bib.bib174)]），这些方法在处理不同环境和场景结构时提供了更多的灵活性。这使得场景坐标回归成为重定位任务的一个有前途的替代方案，尤其是在没有显式3D地图或地图不够准确的情况下。
- en: DSAC [[131](#bib.bib131)] is a relocalization pipeline that leverages a ConvNet
    to regress scene coordinates and incorporates a novel differentiable RANSAC algorithm
    to allow for end-to-end training of the pipeline. This approach has been extended
    in several ways to improve its performance and applicability. For example, reprojection
    loss [[132](#bib.bib132), [175](#bib.bib175), [133](#bib.bib133)] and multi-view
    geometric constraints [[139](#bib.bib139)] have been introduced to enable unsupervised
    learning and joint learning of observation confidences [[135](#bib.bib135), [137](#bib.bib137)]
    to enhance sampling efficiency and accuracy. Other strategies, such as Mixture
    of Experts (MoE) [[136](#bib.bib136)] and hierarchical coarse-to-fine [[140](#bib.bib140),
    [176](#bib.bib176)], have been integrated to eliminate environment ambiguities.
    Different from these, KFNet [[141](#bib.bib141)] extends the scene coordinate
    regression problem to the time domain, effectively bridging the performance gap
    between temporal and one-shot relocalization approaches. However, these methods
    are still limited to a specific scene and cannot be generalized to unseen scenes
    without retraining. To address this limitation, SANet [[138](#bib.bib138)] regresses
    the scene coordinate map of the query by interpolating the 3D points associated
    with the retrieved scene images, making it a scene-agnostic method. Unlike aforementioned
    methods which are trained in a sparse manner, Dense SCR and DSM [[134](#bib.bib134),
    [142](#bib.bib142)] perform scene coordinate regression in a dense manner, making
    the computation more efficient during testing. Moreover, they incorporate global
    context into the regression process to improve robustness. Overall, these advances
    in scene coordinate regression and relocalization techniques offer promising avenues
    for improving localization accuracy in diverse scenarios.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: DSAC [[131](#bib.bib131)] 是一个重新定位管道，它利用 ConvNet 回归场景坐标，并结合了一种新颖的可微分 RANSAC 算法，从而实现了管道的端到端训练。这种方法已在多个方面进行了扩展，以提高其性能和适用性。例如，已经引入了重投影损失
    [[132](#bib.bib132), [175](#bib.bib175), [133](#bib.bib133)] 和多视角几何约束 [[139](#bib.bib139)]，以实现无监督学习，并联合学习观测置信度
    [[135](#bib.bib135), [137](#bib.bib137)]，以提高采样效率和准确性。其他策略，如专家混合模型 (MoE) [[136](#bib.bib136)]
    和分层粗到细 [[140](#bib.bib140), [176](#bib.bib176)]，也已被集成以消除环境歧义。与这些方法不同，KFNet [[141](#bib.bib141)]
    将场景坐标回归问题扩展到时间域，有效地缩小了时间性和单次定位方法之间的性能差距。然而，这些方法仍然局限于特定场景，无法在不重新训练的情况下推广到未见过的场景。为了解决这个限制，SANet
    [[138](#bib.bib138)] 通过插值与检索场景图像相关联的 3D 点来回归查询的场景坐标图，使其成为场景无关的方法。与上述方法不同的是，Dense
    SCR 和 DSM [[134](#bib.bib134), [142](#bib.bib142)] 以密集方式进行场景坐标回归，从而在测试过程中使计算更加高效。此外，它们将全局上下文融入回归过程，以提高鲁棒性。总体而言，这些场景坐标回归和重新定位技术的进展为在多样化场景中提高定位准确性提供了有希望的途径。
- en: Scene coordinate regression-based methods can be more efficient than local descriptor-based
    methods as they eliminate the need for descriptor extraction and matching. These
    methods can directly regress the corresponding 3D point for a given 2D pixel,
    thus generating 2D-3D correspondences efficiently. Additionally, implicit 3D map-based
    relocalization methods have shown promising results, exhibiting robust and accurate
    performance in small indoor environments and achieving comparable, if not better,
    performance than explicit 3D map-based methods. It is worth noting, however, that
    the effectiveness of these implicit methods in large-scale outdoor scenes has
    not been demonstrated. This is due to their dependence on learning a regression
    function that maps 2D image coordinates to 3D scene coordinates, which may not
    generalize well to outdoor scenes with diverse illumination, weather conditions,
    and scene layouts.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 基于场景坐标回归的方法比基于局部描述符的方法更高效，因为它们消除了描述符提取和匹配的需求。这些方法可以直接回归给定 2D 像素的相应 3D 点，从而高效地生成
    2D-3D 对应关系。此外，隐式 3D 地图基的重新定位方法显示出良好的结果，在小型室内环境中表现出鲁棒且准确的性能，并且表现与显式 3D 地图基的方法相当，甚至更好。然而，需要注意的是，这些隐式方法在大规模户外场景中的有效性尚未得到证明。这是因为它们依赖于学习一个将
    2D 图像坐标映射到 3D 场景坐标的回归函数，这可能无法很好地推广到具有多样化光照、天气条件和场景布局的户外场景。
- en: V Mapping
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 映射
- en: Mapping refers to the ability of a mobile agent to perceive and build a consistent
    environmental model to describe surroundings. Deep learning has fostered a set
    of tools for scene perception and understanding, with applications ranging from
    depth prediction, object detection, to semantic labelling and 3D geometry reconstruction.
    This section provides an overview of existing works relevant to deep learning
    based mapping (scene perception) methods. We categorize them into geometric mapping,
    semantic mapping, and implicit mapping.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 映射指的是移动代理感知并构建一致环境模型以描述周围环境的能力。深度学习促成了一套用于场景感知和理解的工具，这些工具的应用范围从深度预测、物体检测，到语义标记和三维几何重建。本节概述了与基于深度学习的映射（场景感知）方法相关的现有工作。我们将它们分为几何映射、语义映射和隐式映射。
- en: V-A Geometric Mapping
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 几何映射
- en: 'Broadly, geometric mapping captures the shape and structural description of
    a scene. The classical mapping algorithms can be categorized into sparse features
    or dense methods. As deep learning based approaches mostly represent scene with
    dense representations, this section focuses on introducing relevant works in this
    area. Typical choices of dense scene representations include depth, point, boundary,
    mesh and voxel. Figure [6](#S5.F6 "Figure 6 ‣ V-A Geometric Mapping ‣ V Mapping
    ‣ Deep Learning for Visual Localization and Mapping: A Survey") visualizes these
    representative geometric representations on the Stanford Bunny benchmark. Inspired
    by [[11](#bib.bib11)], we further divide the learning approaches into two parts:
    raw dense representations and boundary dense representations.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，几何映射捕捉场景的形状和结构描述。经典的映射算法可以分为稀疏特征或密集方法。由于基于深度学习的方法大多用密集表示场景，本节重点介绍这一领域的相关工作。典型的密集场景表示选择包括深度、点、边界、网格和体素。图[6](#S5.F6
    "图 6 ‣ V-A 几何映射 ‣ V 映射 ‣ 基于深度学习的视觉定位和映射：综述")可视化了这些代表性几何表示在斯坦福兔基准上的表现。受到[[11](#bib.bib11)]的启发，我们进一步将学习方法分为两个部分：原始密集表示和边界密集表示。
- en: '![Refer to caption](img/efa5153a847dee75f69ad072b382c76d.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/efa5153a847dee75f69ad072b382c76d.png)'
- en: 'Figure 6: An illustrations of scene representations on the Stanford Bunny benchmark:
    (a) original model, (b) depth , (c) voxel (d) point and (e) mesh representation.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：斯坦福兔基准上的场景表示插图：（a）原始模型，（b）深度，（c）体素，（d）点和（e）网格表示。
- en: V-A1 Raw Dense Representations
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 原始密集表示
- en: Conditioned on input images, deep learning approaches are able to generate 2.5D
    depth maps or 3D points as raw dense representations that express scene geometry
    in high resolution. Such raw representations serve as fundamental components to
    constitute a scene that is well-suited to robotic tasks, such as obstacle avoidance.
    In SLAM (Simultaneous Localization and Mapping) systems, these raw dense mapping
    methods are jointly used with motion tracking. For example, dense scene reconstruction
    can be achieved by fusing per-pixel depth and RGB images, such as DTAM [[177](#bib.bib177)]
    and [[178](#bib.bib178), [179](#bib.bib179)].
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 基于输入图像，深度学习方法能够生成2.5D深度图或3D点作为原始密集表示，这些表示以高分辨率表达场景几何。这些原始表示作为构成适用于机器人任务（如避障）的场景的基础组件。在SLAM（同步定位与地图构建）系统中，这些原始密集映射方法与运动跟踪联合使用。例如，通过融合每个像素的深度和RGB图像可以实现密集场景重建，如DTAM
    [[177](#bib.bib177)] 和 [[178](#bib.bib178), [179](#bib.bib179)]。
- en: '1) 2.5D depth representation: Learning depth from raw images is a fast evolving
    area in computer vision community. There are generally three main categories:
    supervised learning based, self-supervised learning with spatial consistency based
    and self-supervised learning with temporal consistency based depth estimation.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 2.5D深度表示：从原始图像中学习深度是计算机视觉领域一个快速发展的领域。通常有三种主要类别：基于监督学习、基于空间一致性的自监督学习和基于时间一致性的自监督学习深度估计。
- en: One of the earliest approaches is [[180](#bib.bib180)] that takes a single image
    as input, and processes to output per-pixel depths. It uses two deep neural networks,
    i.e. one for coarse global prediction and the other for local refinement, and
    applies scale-invariant error to measure depth relations. This method achieves
    new state-of-the-art performance on NYU Depth and KITTI datasets. More accurate
    depth prediction is achieved by jointly optimizing the depth and self-motion estimation
    [[181](#bib.bib181)]. This work learns to produce depth and camera motion from
    unconstrained image pairs via ConvNet based encoder-decoder structure and an iterative
    network that improves predictions. The network estimates surface normals, optical
    flow, and matching confidence, with a training loss based on spatial relative
    differences. Compared to traditional depth estimation methods, this approach achieves
    higher accuracy and robustness, and outperforms single-image-based depth learning
    network [[181](#bib.bib181)] by better generalizing to unseen structures. [[182](#bib.bib182)]
    proposes a ConvNet based neural model to estimate depth from monocular images
    by using continuous conditional random field (CRF) learning and a structured learning
    scheme that learns the unary and pairwise potentials of continuous CRF in a unified
    deep CNN framework. This model improves upon supervised learning based depth estimation
    and is relatively more efficient. While these supervised learning methods have
    shown superior performance compared to traditional structure-based methods, such
    as [[183](#bib.bib183)], their effectiveness is limited by the availability of
    labeled data during model training, making generalization to new scenarios difficult.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的方法之一是[[180](#bib.bib180)]，它将单张图像作为输入，处理后输出每个像素的深度值。该方法使用了两个深度神经网络，一个用于粗略的全局预测，另一个用于局部细化，并应用尺度不变误差来测量深度关系。该方法在NYU
    Depth和KITTI数据集上达到了新的最先进的性能。通过联合优化深度和自我运动估计[[181](#bib.bib181)]，实现了更准确的深度预测。这项工作通过基于卷积网络的编码器-解码器结构和改进预测的迭代网络，学习从非约束的图像对中生成深度和相机运动。该网络估计表面法线、光流和匹配置信度，训练损失基于空间相对差异。与传统的深度估计方法相比，这种方法在准确性和鲁棒性方面表现更高，并且通过更好地推广到未见过的结构，超越了基于单图像的深度学习网络[[181](#bib.bib181)]。[[182](#bib.bib182)]提出了一种基于卷积网络的神经模型，通过使用连续条件随机场（CRF）学习和一种结构化学习方案来估计单眼图像的深度，该方案在统一的深度卷积神经网络框架中学习连续CRF的单项和配对势。该模型在有监督学习的深度估计上有所改进，且相对更高效。虽然这些有监督学习方法相较于传统结构基础的方法，如[[183](#bib.bib183)]，显示了优越的性能，但其效果受限于模型训练期间标注数据的可用性，使得在新场景中的泛化变得困难。
- en: On the other side, recent advances in this field focus on unsupervised solutions,
    by reformulating depth prediction as a novel view synthesis problem. [[184](#bib.bib184)]
    utilizes photometric consistency loss as a self-supervision signal for training
    neural models. With stereo images and a known camera baseline, it synthesizes
    the left view from the right image, and the predicted depth maps of the left view.
    By minimizing the distance between synthesized images and real images, i.e. the
    spatial consistency, the parameters of the networks are recovered via this self-supervision
    in an end-to-end manner. Similarly, [[56](#bib.bib56)] proposes a single image
    depth estimation model that uses binocular stereo footage instead of ground truth
    depth data. Their approach utilizes an image reconstruction loss to generate disparity
    images and enforces consistency between disparities produced relative to both
    the left and right images to improve performance and robustness, outperforming
    [[182](#bib.bib182)] and [[184](#bib.bib184)].
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，近年来该领域的进展集中在无监督解决方案上，通过将深度预测重新表述为一种新颖的视图合成问题。[[184](#bib.bib184)]利用光度一致性损失作为训练神经模型的自监督信号。通过立体图像和已知的相机基线，它从右图像及左视图的预测深度图中合成左视图。通过最小化合成图像与真实图像之间的距离，即空间一致性，网络的参数通过这种自监督以端到端的方式恢复。类似地，[[56](#bib.bib56)]提出了一种单图像深度估计模型，该模型使用双目立体图像代替真实深度数据。他们的方法利用图像重建损失生成视差图像，并强制施加相对左图像和右图像生成的视差之间的一致性，以提高性能和鲁棒性，优于[[182](#bib.bib182)]和[[184](#bib.bib184)]。
- en: 'In addition to spatial consistency, temporal consistency can also be used as
    a self-supervised signal[[16](#bib.bib16)]. These approaches synthesize the image
    in the target time frame from the source time frame, while simultaneously recovering
    egomotion and depth estimation. Importantly, this framework only requires monocular
    images to learn both depth maps and egomotion. As we have discussed this part
    in Section [III-B](#S3.SS2 "III-B Self-supervised Learning of Visual Odometry
    ‣ III Incremental Motion Estimation ‣ Deep Learning for Visual Localization and
    Mapping: A Survey"), we refer the readers to Section [III-B](#S3.SS2 "III-B Self-supervised
    Learning of Visual Odometry ‣ III Incremental Motion Estimation ‣ Deep Learning
    for Visual Localization and Mapping: A Survey") for more details.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 除了空间一致性，时间一致性也可以作为自监督信号[[16](#bib.bib16)]。这些方法在目标时间框架中合成图像，同时恢复自运动和深度估计。重要的是，这种框架只需要单目图像来学习深度图和自运动。正如我们在[III-B](#S3.SS2
    "III-B 自监督视觉里程计学习 ‣ III 增量运动估计 ‣ 深度学习在视觉定位与映射中的应用")节中讨论的那样，我们建议读者查阅[III-B](#S3.SS2
    "III-B 自监督视觉里程计学习 ‣ III 增量运动估计 ‣ 深度学习在视觉定位与映射中的应用")节以获取更多细节。
- en: The learned depth information can be integrated into SLAM systems to address
    some limitations of classical monocular solution. For example, CNN-SLAM [[185](#bib.bib185)]
    utilizes the learned depths from single images into a monocular SLAM framework
    (i.e. LSD-SLAM [[186](#bib.bib186)]). It shows how learned depth maps contribute
    to mitigating the absolute scale recovery problem in pose estimates and scene
    reconstruction. With the dense depth maps predicted by ConvNets, CNN-SLAM provides
    dense scene predictions in texture-less areas, which is normally hard for a conventional
    SLAM system.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的深度信息可以集成到SLAM系统中，以解决经典单目解决方案的一些局限性。例如，CNN-SLAM [[185](#bib.bib185)] 将从单幅图像中学到的深度信息集成到单目SLAM框架中（即LSD-SLAM
    [[186](#bib.bib186)]）。这表明学到的深度图如何有助于缓解姿态估计和场景重建中的绝对尺度恢复问题。借助ConvNets预测的密集深度图，CNN-SLAM在无纹理区域提供了密集的场景预测，这通常是传统SLAM系统难以做到的。
- en: '2) 3D Points Representation: Deep learning techniques have also been introduced
    to generate 3D points from raw images. The point-based formulation represents
    the 3-dimensional coordinates (x, y, z) of points in 3D space. While this formulation
    is straightforward and easily manipulated, it encounters the challenge of ambiguity,
    wherein different configurations of point clouds can represent the same underlying
    geometry.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 3D点表示：深度学习技术也已被引入以从原始图像生成3D点。基于点的表示方式表示3D空间中点的三维坐标（x, y, z）。虽然这种表示方式直观且易于操作，但它面临模糊性挑战，不同的点云配置可能代表相同的底层几何形状。
- en: The pioneer work in this domain is PointNet [[187](#bib.bib187)] that directly
    operates on point clouds, without the need for unnecessary conversion to regular
    3D voxel grids or image collections. PointNet is specifically designed to handle
    the permutation invariance of points in the input, and its applications span various
    tasks, such as object classification, part segmentation, and scene semantic parsing.
    Furthermore, [[188](#bib.bib188)] develops a deep generative model that can generate
    3D geometry in point-based formulation from single images. In their work, a loss
    function based on Earth Mover’s distance is introduced to tackle the problem of
    data ambiguity. However, their method has only been validated on the reconstruction
    task of single objects. As of now, no research on point generation for scene reconstruction
    has been found, primarily due to the large computational burden associated with
    such endeavors.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的开创性工作是PointNet [[187](#bib.bib187)]，它直接在点云上操作，无需将其转换为常规的3D体素网格或图像集合。PointNet专门设计用于处理输入点的排列不变性，其应用范围涵盖了各种任务，如物体分类、部件分割和场景语义解析。此外，[[188](#bib.bib188)]
    开发了一种深度生成模型，可以从单幅图像生成基于点的3D几何形状。在他们的工作中，引入了一种基于地球移动者距离的损失函数来解决数据模糊性问题。然而，他们的方法仅在单个物体的重建任务上进行了验证。目前尚未找到针对场景重建的点生成研究，主要是因为这种工作涉及的计算负担很大。
- en: V-A2 Boundary and Spatial-Partitioning Representations
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 边界和空间划分表示
- en: Beyond unstructured raw dense representations (i.e. 2.5D depth maps and 3D points),
    boundary representations express the 3D scene with explicit surfaces and spatial-partitioning
    (i.e. boundaries).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 除了无结构的原始密集表示（即2.5D深度图和3D点），边界表示通过显式表面和空间划分（即边界）来表达3D场景。
- en: '1) Surface mesh representation: Mesh-based formulation naturally captures the
    surface of 3D shape. It encodes the underlying surface structure of 3D models,
    such as edges, vertices and faces. Several works consider the problem of learning
    mesh generation from images [[189](#bib.bib189), [190](#bib.bib190)] or point
    clouds data [[191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193)]. However,
    these approaches are only able to reconstruct single objects, and limited to generating
    models with simple structures or from familiar classes. To tackle the problem
    of scene reconstruction in mesh representation, [[194](#bib.bib194)] integrates
    the sparse features from monocular SLAM with the dense depth maps from ConvNets
    to the update 3D mesh representation. In this work, SLAM-measured sparse features
    and CNN-predicted dense depth maps are fused to obtain a more accurate 3D reconstruction,
    a 3D mesh representation is updated by integrating accurately tracked sparse features
    points. The proposed work shows a reduction in the mean residual error of 38%
    compared to ConvNet-based depth map prediction alone in 3D reconstruction. To
    allow efficient computation and flexible information fusion, [[195](#bib.bib195)]
    utilizes 2.5D mesh to represent scene geometry. In this approach, the image plane
    coordinates of mesh vertices are learned by deep neural networks, while depth
    maps are optimized as free variables. A factor graph is utilized to integrate
    information in a flexible and continuous manner through the use of learnable residuals.
    Experimental evaluation on synthetic and real data shows the effectiveness and
    practicability of the proposed approach.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 表面网格表示：基于网格的表达自然地捕捉了三维形状的表面。它编码了三维模型的基础表面结构，如边缘、顶点和面。已有一些研究考虑了从图像[[189](#bib.bib189)、[190](#bib.bib190)]或点云数据[[191](#bib.bib191)、[192](#bib.bib192)、[193](#bib.bib193)]中学习网格生成的问题。然而，这些方法仅能重建单一对象，并且局限于生成结构简单或来自熟悉类别的模型。为了解决网格表示中的场景重建问题，[[194](#bib.bib194)]
    将单目SLAM的稀疏特征与ConvNets的密集深度图结合起来，以更新三维网格表示。在这项工作中，通过融合SLAM测量的稀疏特征和CNN预测的密集深度图，获得了更准确的三维重建，三维网格表示通过准确跟踪的稀疏特征点进行更新。与仅使用ConvNet基础的深度图预测相比，所提出的方法在三维重建中显示出38%的均值残差误差降低。为了实现高效的计算和灵活的信息融合，[[195](#bib.bib195)]
    利用2.5D网格表示场景几何。在这种方法中，网格顶点的图像平面坐标由深度神经网络学习，同时深度图作为自由变量进行优化。通过使用可学习的残差，因子图被用来以灵活和连续的方式整合信息。对合成数据和真实数据的实验评估显示了所提出方法的有效性和实用性。
- en: '2) Surface function representation: This representation describes the surface
    as the zero-crossing of an implicit function. A popular choice is signed distance
    function, a continuous volumetric field, in which the magnitude of a point is
    the distance to the surface boundary and the sign determines whether it is inside
    or outside. DeepSDF is proposed learn to generate such a continuous field by a
    classifier, indicating which boundary is the shape surface [[196](#bib.bib196)].
    Specifically, DeepSDF is a learned continuous Signed Distance Function (SDF) representation
    of a class of shapes that enables high quality shape representation, interpolation
    and completion from partial and noisy 3D input data. It represents a shape’s surface
    by a continuous volumetric field and explicitly represents the classification
    of space as being part of the shapes interior or not. DeepSDF can represent an
    entire class of shapes and has impressive performance on learning 3D shape representation
    and completion while reducing the model size by an order of magnitude compared
    with previous works. Another approach, Occupancy Networks generate a continuous
    3D occupancy function with deep neural networks, representing the decision boundary
    with neural classifier[[197](#bib.bib197)], a description of the 3D output at
    infinite resolution without excessive memory footprint. The effectiveness of this
    approach has been validated for 3D reconstruction from single images, noisy point
    clouds, and coarse discrete voxel grids, and demonstrate competitive results over
    baselines. To further improve Occupancy Networks, Convolutional Occupancy Networks
    [[198](#bib.bib198)] combines Convolutional encoders with implicit occupancy decoders.
    This method is empirically validated through experiments reconstructing complex
    geometry from noisy point clouds and low-resolution voxel representations. In
    addition, [[199](#bib.bib199)] leverages deep fully-connected neural network to
    optimize a radiance field function to represent a scene. Their experiments demonstrate
    good performance in novel view synthesis task. Compared with raw representations,
    surface function representation reduces storage memory significantly. Different
    from aforementioned methods that are limited to closed surfaces, NDF [[200](#bib.bib200)]
    is proposed to predict unsigned distance fields for arbitrary 3D shapes, which
    is more flexible in practical usages.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表面函数表示：这种表示方式将表面描述为隐式函数的零交叉点。一个流行的选择是带符号距离函数，它是一个连续的体积场，其中点的大小是到表面边界的距离，而符号决定了点是否在表面内外。DeepSDF
    被提出用来通过分类器生成这样的连续场，指示哪个边界是形状的表面 [[196](#bib.bib196)]。具体来说，DeepSDF 是一种学习到的连续带符号距离函数（SDF）表示，它用于表示一类形状，能够高质量地进行形状表示、插值和从部分及噪声3D输入数据中完成。它通过一个连续的体积场表示形状的表面，并明确表示空间的分类是否属于形状内部。DeepSDF
    可以表示整个形状类别，并且在学习3D形状表示和完成方面表现出色，同时相较于之前的工作，模型大小减少了一个数量级。另一种方法，Occupancy Networks
    生成一个连续的3D占用函数，通过深度神经网络表示决策边界，并用神经分类器[[197](#bib.bib197)]进行描述，能够在无限分辨率下生成3D输出，且不会占用过多内存。该方法在从单张图像、噪声点云和粗糙离散体素网格中进行3D重建方面效果显著，并且在基线测试中展示了竞争力的结果。为了进一步改进
    Occupancy Networks，卷积占用网络 [[198](#bib.bib198)] 结合了卷积编码器和隐式占用解码器。该方法通过实验在从噪声点云和低分辨率体素表示中重建复杂几何形状上得到了实证验证。此外，[[199](#bib.bib199)]
    利用深度全连接神经网络来优化辐射场函数以表示场景。他们的实验在新视角合成任务中表现良好。与原始表示相比，表面函数表示显著减少了存储内存。与上述限于闭合表面的方法不同，NDF
    [[200](#bib.bib200)] 被提出用于预测任意3D形状的无符号距离场，这在实际应用中更加灵活。
- en: '3) Voxel representation: Similar to the usage of pixel (i.e. 2D element) in
    images, voxel is a volume element in a three-dimensional space. Previous works
    explore to use multiple input views, to reconstruct the volumetric representation
    of a scene [[201](#bib.bib201), [202](#bib.bib202)] and objects [[203](#bib.bib203)].
    For example, SurfaceNet [[201](#bib.bib201)] learns to predict the confidence
    of a voxel to determine whether it is on surface or not, and reconstruct the 2D
    surface of a scene. SurfaceNet is based on a 3D convolutional network that encodes
    the camera parameters together with the images in a 3D voxel representation, allowing
    for the direct learning of both photo-consistency and geometric relations of the
    surface structure. This framework is evaluated on the large-scale scene reconstruction
    dataset, demonstrating its effectiveness for multiview stereopsis. RayNet [[202](#bib.bib202)]
    reconstructs the scene geometry by extracting view-invariant features while imposing
    geometric constraints. It encodes the physics of perspective projection and occlusion
    via Markov Random Fields while utilizing a ConvNet to learn view-invariant feature
    representations. Some works focus on generating high-resolution 3D volumetric
    models [[204](#bib.bib204), [205](#bib.bib205)]. For example, [[205](#bib.bib205)]
    designes a convolutional decoder based on octree-based formulation to enable scene
    reconstruction in much higher resolution. This network predicts the structure
    of the octree and the occupancy values of individual cells, making it valuable
    for generating complex 3D shapes. Unlike standard decoders with cubic complexity,
    this architecture allows for higher resolution outputs with limited memory budget.
    Others can be found on scene completion from RGB-D data [[206](#bib.bib206), [207](#bib.bib207)].
    One limitation of voxel representation is its high computational requirement,
    especially when attempting to reconstruct a scene in high resolution.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 体素表示：类似于图像中使用的像素（即二维元素），体素是三维空间中的体积元素。以往的工作探索了使用多个输入视图，重建场景的体积表示[[201](#bib.bib201)、[202](#bib.bib202)]和物体[[203](#bib.bib203)]。例如，SurfaceNet[[201](#bib.bib201)]学会预测体素的置信度，以确定其是否在表面上，并重建场景的二维表面。SurfaceNet基于3D卷积网络，该网络将相机参数与图像编码到3D体素表示中，从而直接学习表面结构的光照一致性和几何关系。该框架在大规模场景重建数据集上进行了评估，证明了其在多视角立体视觉中的有效性。RayNet[[202](#bib.bib202)]通过提取视图不变特征并施加几何约束来重建场景几何。它通过马尔可夫随机场编码透视投影和遮挡的物理学，同时利用卷积网络学习视图不变的特征表示。一些工作专注于生成高分辨率的3D体积模型[[204](#bib.bib204)、[205](#bib.bib205)]。例如，[[205](#bib.bib205)]设计了一种基于八叉树公式的卷积解码器，以实现场景的更高分辨率重建。该网络预测八叉树的结构和单个单元的占用值，使其在生成复杂的3D形状时具有重要价值。与具有立方复杂度的标准解码器不同，该架构允许在有限的内存预算下实现更高分辨率的输出。其他工作可以在RGB-D数据的场景完成中找到[[206](#bib.bib206)、[207](#bib.bib207)]。体素表示的一个限制是其高计算要求，特别是在尝试以高分辨率重建场景时。
- en: Choosing optimal representation for mapping is still an open question. The choice
    of scene representation for SLAM depends on a range of factors, including the
    sensor modality, the level of detail required, the computational resources available,
    and the size and complexity of the environment. In general, dense representations,
    such as depth maps or point clouds, offer a comprehensive and detailed view of
    the scene but incur a high computational and memory cost. This renders them more
    suitable for small-scale scenes. On the other hand, boundary representations,
    such as mesh and surface function-based formulations, are preferred for large-scale
    outdoor environments due to their ability to capture the scene’s structure and
    geometry while keeping memory and computational requirements within feasible limits.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的映射表示仍然是一个未解的问题。SLAM的场景表示选择取决于一系列因素，包括传感器的模式、所需的细节级别、可用的计算资源以及环境的大小和复杂性。一般而言，密集表示，如深度图或点云，提供了对场景的全面和详细的视图，但计算和内存成本较高。这使得它们更适用于小规模场景。另一方面，边界表示，如网格和基于表面函数的公式，由于其能够捕捉场景的结构和几何形状，同时保持内存和计算要求在可行范围内，因此更适合大规模的户外环境。
- en: V-B Semantic Map
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 语义地图
- en: '![Refer to caption](img/baae91547f8f24b84bc5ebd888b160d5.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/baae91547f8f24b84bc5ebd888b160d5.png)'
- en: 'Figure 7: (a) Raw image (b) semantic segmentation, (c) instance segmentation
    and (d) panoptic segmentation for semantic mapping [[208](#bib.bib208)].'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：（a）原始图像，（b）语义分割，（c）实例分割和（d）全景分割用于语义映射[[208](#bib.bib208)]。
- en: Semantic mapping connects semantic concepts (i.e. object classification, material
    composition etc) with environment geometry. The advances in deep learning greatly
    foster the developments of object recognition and semantic segmentation. Maps
    with semantic meanings enable mobile agents to have a high-level understanding
    of their environments beyond pure geometry, and allow for a greater range of functionality
    and autonomy.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 语义映射将语义概念（即对象分类、材料组成等）与环境几何相连接。深度学习的进步极大地促进了对象识别和语义分割的发展。具有语义意义的地图使移动代理能够对其环境有超越纯几何的高层次理解，并允许更广泛的功能性和自主性。
- en: SemanticFusion [[209](#bib.bib209)] is one of the early contributions that combines
    semantic segmentation labels obtained from deep ConvNet with dense scene geometry
    derived from a SLAM system. This integration is achieved by probabilistically
    associating 2D frames with a 3D map, thereby incrementally incorporating per-frame
    semantic segmentation predictions into the dense 3D map. The combined framework
    not only generates a map enriched with useful semantic information, but also shows
    that the integration with a SLAM system enhances single-frame segmentation. However,
    in SemanticFusion, the two modules, i.e. semantic segmentation and SLAM, are loosely
    coupled. [[210](#bib.bib210)] proposes a self-supervised network that predicts
    consistent semantic labels for a map, by imposing constraints on the coherence
    of semantic predictions across across different viewpoints. DA-RNN [[211](#bib.bib211)]
    introduces recurrent models into the semantic segmentation framework, enabling
    the learning of temporal connections across multiple view frames, producing more
    accurate and consistent semantic labelling for volumetric maps. Another recent
    work [[212](#bib.bib212)] proposes a framework that builds a compact semantic
    map using crowd-sourced visual data. Localization is achieved by matching current
    feature points against the built semantic map via the iterative closest point
    (ICP) method. Unlike previous approaches that are evaluated on a room level, this
    work provides a lightweight semantic mapping and localization that performs well
    in large-scale city scenes. Yet it is worth noting that these semantic segmentation-based
    methods do not provide information about object instances. Therefore, they are
    unable to distinguish between different objects belonging to the same category.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: SemanticFusion [[209](#bib.bib209)] 是早期将从深度卷积网络获得的语义分割标签与从SLAM系统中获得的密集场景几何结合起来的贡献之一。通过概率性地将2D帧与3D地图关联，实现了这种集成，从而逐步将每帧的语义分割预测纳入密集的3D地图中。这个综合框架不仅生成了一个充满有用语义信息的地图，还显示了与SLAM系统的集成提升了单帧分割。然而，在SemanticFusion中，两个模块，即语义分割和SLAM，连接松散。[[210](#bib.bib210)]
    提出了一个自监督网络，通过对不同视角间语义预测的一致性施加约束，来预测地图的一致语义标签。DA-RNN [[211](#bib.bib211)] 将递归模型引入语义分割框架，使其能够学习多个视角帧之间的时间连接，为体积地图生成更准确、一致的语义标签。另一个近期工作
    [[212](#bib.bib212)] 提出了一个利用众包视觉数据构建紧凑语义地图的框架。通过将当前特征点与构建的语义地图进行匹配，实现定位，采用迭代最近点（ICP）方法。与之前在房间级别上评估的方法不同，这项工作提供了一种在大规模城市场景中表现良好的轻量级语义映射和定位。然而，需要注意的是，这些基于语义分割的方法并不提供对象实例的信息，因此它们无法区分属于同一类别的不同对象。
- en: With the advances in instance segmentation, semantic mapping has evolved operate
    at the instance level. A notable example is [[213](#bib.bib213)] that offers object-level
    semantic mapping by employing a bounding box detection module and an unsupervised
    geometric segmentation module to identify individual objects. [[214](#bib.bib214)]
    presents a framework that achieves instance-aware semantic mapping, and enables
    novel object discovery within the mapped environment. Unlike other dense semantic
    mapping approaches, Fusion++ [[215](#bib.bib215)] builds a semantic graph-based
    map that specifically predicts object instances and maintains a consistent map
    via loop closure detection, pose-graph optimization and further refinement. In
    order to leverage learned object information more effectively, [[216](#bib.bib216)]
    presents a probabilistic framework within the context of SLAM. It introduces object
    detectors as semantic landmarks into a factor graph, enabling the joint optimization
    of pose estimation, landmark positions/classes and data association. This integration
    helps address ambiguous data association challenges encountered in the mapping
    process.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 随着实例分割技术的发展，语义映射已发展到实例级别。一个显著的例子是 [[213](#bib.bib213)]，它通过采用边界框检测模块和无监督几何分割模块来实现对象级别的语义映射。[[214](#bib.bib214)]
    提出了一个框架，实现了实例感知的语义映射，并在映射环境中启用了新对象发现。与其他密集语义映射方法不同，Fusion++ [[215](#bib.bib215)]
    构建了一个基于语义图的地图，专门预测对象实例，并通过闭环检测、姿态图优化和进一步的精细化来保持一致的地图。为了更有效地利用学到的对象信息，[[216](#bib.bib216)]
    在 SLAM 背景下提出了一个概率框架。它将对象检测器作为语义地标引入因子图，从而实现姿态估计、地标位置/类别和数据关联的联合优化。这种集成有助于解决映射过程中遇到的模糊数据关联挑战。
- en: 'Recently, panoptic segmentation [[208](#bib.bib208)] attracts attentions. PanopticFusion
    [[217](#bib.bib217)] represents an advancement in semantic mapping that extends
    to the level of stuff and things classification. In this context, stuff classes
    encompass static objects such as walls, doors, and lanes, while things classes
    include accountable objects like moving vehicles, humans, and tables. Figure [7](#S5.F7
    "Figure 7 ‣ V-B Semantic Map ‣ V Mapping ‣ Deep Learning for Visual Localization
    and Mapping: A Survey") provides a visual comparison between semantic segmentation,
    instance segmentation, and panoptic segmentation.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，全面分割[[208](#bib.bib208)]引起了关注。PanopticFusion [[217](#bib.bib217)]代表了语义映射的进步，扩展到物体和事物分类的层面。在这种背景下，stuff
    类别包括墙壁、门和车道等静态物体，而 things 类别包括移动的车辆、人类和桌子等可计数物体。图 [7](#S5.F7 "Figure 7 ‣ V-B Semantic
    Map ‣ V Mapping ‣ Deep Learning for Visual Localization and Mapping: A Survey")
    提供了语义分割、实例分割和全面分割的视觉对比。'
- en: In summary, semantic mapping generates high-level representations of the environment
    by incorporating information about the semantic meaning, material composition,
    and geometry of objects, along with the specifics of their instance-level characteristics.
    These methods employs probabilistic models to gradually fuse per-frame semantic
    segmentation predictions into a dense 3D map or employs object detectors as semantic
    landmarks to optimize pose and scene estimation together. They provide a comprehensive
    understanding of the environment, enabling mobile agents to achieve higher levels
    of functionality and autonomy.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，语义映射通过整合有关物体语义意义、材料组成和几何形状的信息以及它们的实例级特征，生成环境的高级表示。这些方法采用概率模型，将每帧的语义分割预测逐渐融合为密集的
    3D 地图，或者将对象检测器作为语义地标，以优化姿态和场景估计。它们提供了对环境的全面理解，使移动代理能够实现更高水平的功能和自主性。
- en: V-C Implicit Map
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 隐式地图
- en: In addition to explicit geometric and semantic map representations, deep learning
    models are able to encode the entire scene into an implicit representation, known
    as a neural map. This neural map representation captures the underlying scene
    geometry and appearance in an implicit manner.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 除了显式的几何和语义地图表示外，深度学习模型还能够将整个场景编码为隐式表示，即神经地图。这种神经地图表示以隐式方式捕捉了场景的基础几何和外观。
- en: '1) Autoencoder based Scene Representation: Deep autoencoders offer the capability
    to automatically discover high-level compact representations of high-dimensional
    image data. A notable example is CodeSLAM [[218](#bib.bib218)] that encodes observed
    images into a compact and optimizable representation to contain the essential
    information of a dense scene. The learned implicit representation is then utilized
    within a keyframe-based SLAM system to infer both camera poses and depth maps.
    The reduced size of learned representation in CodeSLAM enables efficient optimization
    of camera motion tracking and scene geometry, facilitating global consistency
    in visual localization and mapping.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基于自编码器的场景表示：深度自编码器提供了自动发现高维图像数据高层紧凑表示的能力。一个显著的例子是 CodeSLAM [[218](#bib.bib218)]，它将观察到的图像编码为紧凑且可优化的表示，以包含密集场景的核心信息。然后，将学习到的隐式表示用于基于关键帧的
    SLAM 系统，以推断相机姿态和深度图。CodeSLAM 中学习表示的减小尺寸使得相机运动跟踪和场景几何的优化变得高效，从而促进了视觉定位和映射中的全局一致性。
- en: '2) Neural Rendering based Scene Representation: Neural rendering models form
    a distinct category of research that leverages view synthesis as a self-supervision
    signal to implicitly learn and model the 3D structure of a scene. These models
    aim to reconstruct a new scene from an unknown viewpoint.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 基于神经渲染的场景表示：神经渲染模型形成了一类独特的研究，它利用视图合成作为自我监督信号，隐式学习和建模场景的3D结构。这些模型旨在从未知视点重建新场景。
- en: A notable example is the Generative Query Network (GQN) [[219](#bib.bib219)]
    that learns to capture a neural implicit representation and utilizes it to render
    new scenes. GQN consists of a representation network and a generation network.
    The representation network encodes observations from reference views into a scene
    representation, while the generation network, based on a recurrent model, reconstructs
    the scene from a new view conditioned on the scene representation and a stochastic
    latent variable. By taking observed images from multiple viewpoints and the camera
    pose of a new view as inputs, GQN predicts the physical scene of the new view.
    Through end-to-end training, the representation network can capture the necessary
    and important factors of 3D environment for the scene reconstruction task via
    the generation network. GQN has been extended to incorporate a geometric-aware
    attention mechanism to allow more complex environment modelling [[220](#bib.bib220)].
    Furthermore, the integration of multimodal data for scene inference has been explored
    to enhance the capabilities of GQN [[221](#bib.bib221)].
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显著的例子是生成查询网络（GQN）[[219](#bib.bib219)]，它学习捕捉神经隐式表示，并利用它来渲染新场景。GQN 由表示网络和生成网络组成。表示网络将来自参考视图的观察编码为场景表示，而生成网络基于递归模型，根据场景表示和随机潜变量从新视图重建场景。通过将来自多个视点的观察图像和新视图的相机姿态作为输入，GQN
    预测新视图的物理场景。通过端到端训练，表示网络可以捕捉到用于场景重建任务的3D环境中必要且重要的因素。GQN 已扩展到结合几何感知注意机制，以允许更复杂的环境建模
    [[220](#bib.bib220)]。此外，已探索多模态数据集成以增强 GQN 的能力 [[221](#bib.bib221)]。
- en: 'Recently, NeRF [[199](#bib.bib199)] is proposed to explicitly encode the radiance
    fields of complicated 3D scenes into the weights of MLPs. It delivers an impressive
    realism for demanding 3D situations by utilizing volume rendering to generate
    new views for 2D supervision. However, there are three main limitations: 1) because
    each 3D scene is stored into all MLP weights, the trained network (i.e., a learned
    radiance field) can only represent a single scene and is hard to generalize to
    novel circumstances; 2) as a single camera ray requires tens or even hundreds
    of the evaluations of the 3D neural scene representation, NeRF-based approaches
    are highly computational, leading to slow rendering time; 3) due to the fact that
    each spatial 3D location along a light ray is only optimized by the available
    pixel RGBs, the learned implicit representations of that site lack the general
    geometric patterns, resulting in less photo-realistic synthetic images. To address
    these limitations, several works have been proposed, including those that focus
    on generalization [[222](#bib.bib222), [223](#bib.bib223)], efficiency [[224](#bib.bib224),
    [225](#bib.bib225)], and geometry [[226](#bib.bib226), [227](#bib.bib227)]. NeRF
    can also be combined with a semantic map, as seen in Semantic-NeRF [[228](#bib.bib228)],
    which jointly encodes semantics with appearance and geometry, exploiting the intrinsic
    multi-view consistency and smoothness of NeRF to benefit semantics.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，NeRF [[199](#bib.bib199)] 被提出，以显式编码复杂三维场景的辐射场到 MLP 的权重中。通过利用体积渲染生成新的视图进行二维监督，它为要求高的三维场景提供了令人印象深刻的真实感。然而，存在三大主要限制：1）由于每个三维场景都存储在所有
    MLP 权重中，因此训练后的网络（即学习到的辐射场）只能表示单一场景，难以推广到新的情况；2）由于单个相机光线需要对三维神经场景表示进行数十次甚至数百次评估，基于
    NeRF 的方法计算量大，导致渲染时间缓慢；3）由于沿光线的每个空间三维位置仅通过可用的像素 RGB 优化，因此该位置学习到的隐式表示缺乏通用的几何模式，导致合成图像的照片真实感较差。为了解决这些限制，已经提出了几项工作，包括关注于泛化
    [[222](#bib.bib222), [223](#bib.bib223)]、效率 [[224](#bib.bib224), [225](#bib.bib225)]
    和几何 [[226](#bib.bib226), [227](#bib.bib227)]。NeRF 还可以与语义地图结合，例如在 Semantic-NeRF
    [[228](#bib.bib228)] 中，联合编码语义与外观和几何，利用 NeRF 的内在多视图一致性和平滑性来改善语义。
- en: Additionally, NeRF is also introduced to build SLAM systems, such as iMAP [[229](#bib.bib229)]
    and NICE-SLAM [[230](#bib.bib230)]. Specifically, iMAP [[229](#bib.bib229)] employs
    a multilayer perceptron (MLP) as the sole scene representation in a SLAM system,
    which is trained in live operation without prior data. iMAP designs a keyframe
    structure, multi-processing computation flow, and dynamic information-guided pixel
    sampling for speed, achieving tracking at 10 Hz and global map updating at 2 Hz.
    Compared to standard dense SLAMs, iMAP has efficient geometry representation with
    automatic detail control and smooth filling-in of unobserved regions. To overcome
    the limitations of over-smoothed scene reconstructions and difficulty in scaling
    up to large scenes in SLAM, NICE-SLAM [[230](#bib.bib230)] has been proposed as
    an efficient and robust dense SLAM system. It incorporates multi-level local information
    through a hierarchical scene representation and is optimized with pre-trained
    geometric priors, resulting in more detailed reconstruction on large indoor scenes.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，NeRF 也被引入到构建 SLAM 系统中，例如 iMAP [[229](#bib.bib229)] 和 NICE-SLAM [[230](#bib.bib230)]。具体来说，iMAP
    [[229](#bib.bib229)] 使用多层感知器（MLP）作为 SLAM 系统中的唯一场景表示，该模型在实际操作中进行训练，而不依赖于先前的数据。iMAP
    设计了关键帧结构、多处理计算流和动态信息引导的像素采样，以提高速度，实现了 10 Hz 的跟踪和 2 Hz 的全局地图更新。与标准的密集 SLAM 相比，iMAP
    具有高效的几何表示，自动控制细节，并平滑填充未观测区域。为了克服 SLAM 中过度平滑场景重建和难以扩展到大型场景的局限性，提出了 NICE-SLAM [[230](#bib.bib230)]
    作为一个高效且稳健的密集 SLAM 系统。它通过层次化场景表示来融合多级局部信息，并通过预训练的几何先验进行优化，从而在大型室内场景中实现更详细的重建。
- en: '3) Reinforcement Learning based Scene Representation: Last but not least, in
    the quest of ‘map-less’ navigation, task-driven maps emerge as a novel map representation.
    This representation is jointly modelled by deep neural networks with respect to
    the task at hand. Generally those tasks leverage location information, such as
    navigation or path planning, requiring mobile agents to understand the geometry
    and semantics of environment. Navigation in unstructured environments (even in
    a city scale) is formulated as a policy learning problem in these works [[231](#bib.bib231),
    [232](#bib.bib232), [233](#bib.bib233), [234](#bib.bib234)], and solved by deep
    reinforcement learning. Different from traditional solutions that follow a procedure
    of building an explicit map, planning path and making decisions, these learning
    based techniques predict control signals directly from sensor observations in
    an end-to-end manner, without explicitly modelling the environment. The model
    parameters are optimized via sparse reward signals, for example, whenever agents
    reach a destination, a positive reward will be given to tune the neural network.
    Once a model is trained, the actions of agents can be determined conditioned on
    the current observations of environment, i.e. images. In this case, all of environmental
    factors, such as the geometry, appearance and semantics of a scene, are embedded
    inside the neurons of a deep neural network and suitable to solving the task at
    hand. Interestingly, the visualization of the neurons inside a neural model that
    is trained on the navigation task via reinforcement learning, has similar patterns
    as the grid and place cells inside human brain [[235](#bib.bib235)]. This provides
    cognitive cues to support the effectiveness of neural map representation.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 基于强化学习的场景表示：最后但同样重要的是，在“无地图”导航的探索中，以任务驱动的地图作为一种新型地图表示。这种表示是通过深度神经网络与当前任务共同建模的。通常这些任务利用位置信息，如导航或路径规划，要求移动代理理解环境的几何形状和语义。在这些工作中，未结构化环境（甚至是城市规模）的导航被形式化为一个策略学习问题[[231](#bib.bib231),
    [232](#bib.bib232), [233](#bib.bib233), [234](#bib.bib234)]，并通过深度强化学习解决。与传统方法不同，后者遵循建立明确地图、规划路径和做出决策的程序，这些基于学习的技术直接从传感器观测中以端到端的方式预测控制信号，而无需明确建模环境。模型参数通过稀疏奖励信号进行优化，例如，当代理达到目的地时，将给予正奖励以调整神经网络。一旦模型训练完成，代理的动作可以根据当前的环境观测，即图像，来确定。在这种情况下，所有的环境因素，如场景的几何形状、外观和语义，都被嵌入在深度神经网络的神经元中，并适合解决当前任务。有趣的是，通过强化学习在导航任务上训练的神经模型中，神经元的可视化具有与人脑中的网格细胞和位置细胞类似的模式[[235](#bib.bib235)]。这为神经地图表示的有效性提供了认知线索。
- en: VI Loop-closing and SLAM Back-ends
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 回环闭合与SLAM后端
- en: Simultaneously tracking self-motion and building environmental structures construct
    a simultaneous localization and mapping (SLAM) system. The localization and mapping
    methods discussed in the preceding sections can be considered as individual modules
    within a comprehensive SLAM frameworks. This section overviews deep learning based
    loop closure detection and SLAM back-ends.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 同时追踪自我运动和构建环境结构构成了一个同时定位与建图（SLAM）系统。前面讨论的定位和建图方法可以被视为综合SLAM框架中的独立模块。本节概述了基于深度学习的回环闭合检测和SLAM后端。
- en: VI-A Loop-closure Detection
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 回环闭合检测
- en: Loop-closing (or place recognition) module determines whether a particular location
    has been visited previously. Upon detecting a loop-closure, global optimization
    is performed to ensure the overall consistency of motion tracking and the map.
    For a more comprehensive discussion on this topic, readers are referred to the
    survey [[5](#bib.bib5)].
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 回环闭合（或位置识别）模块用于确定特定位置是否已被访问过。在检测到回环闭合后，执行全局优化以确保运动追踪和地图的一致性。有关该主题的更全面讨论，读者可以参考调查[[5](#bib.bib5)]。
- en: Conventional works typically rely on the bag-of-words (BoW) to store and use
    visual features extracted from hand-designed detectors. However, real-world scenarios
    often introduce complications such as changes in illumination, weather conditions,
    viewpoints, and the presence of moving objects. To address these challenges, researchers
    have proposed to use the ConvNet features, that are from pre-trained neural models
    on large-scale generic image processing dataset. In [[236](#bib.bib236)], by adapting
    object proposal techniques and utilizing convolutional neural network features,
    potential landmarks within an image can be identified for place recognition. This
    method does not require any form of training, and the system’s components are
    generic enough to be used off-the-shelf, resulting in performance improvement
    over current state-of-the-art techniques. Other representative works, e.g. [[237](#bib.bib237),
    [238](#bib.bib238), [239](#bib.bib239)] are built on a deep auto-encoder structure
    to extract a compact representation, that compresses scene in an unsupervised
    manner. Specifically, [[237](#bib.bib237)] utilizes a stacked denoising auto-encoder
    (SDA) that learns a compressed representation from raw input data in an unsupervised
    manner, allowing for complex inner structures in image data to be learned without
    the need for manual visual feature design. [[238](#bib.bib238)] leverages an unsupervised
    autoencoder architecture, trained with randomized projective transformations to
    emulate natural viewpoint changes and histogram of oriented gradients (HOG) descriptors
    for illumination invariance. It is without the need for labeled training data
    or environment-specific training, and is capable of closing loops in real time
    with no dimensionality reduction. [[239](#bib.bib239)] is based on a super dictionary,
    which is more memory-efficient than traditional BoW dictionaries. The proposed
    model uses two deep neural networks to speed up the loop closure detection and
    to ignore the effect of mobile objects. Experimental results show that it performs
    robustly and is significantly faster.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法通常依赖于词袋模型（BoW）来存储和使用从手工设计的检测器中提取的视觉特征。然而，现实世界场景常常引入复杂因素，如光照变化、天气条件、视角变化以及移动物体的存在。为了解决这些挑战，研究人员提出使用来自大规模通用图像处理数据集上预训练神经模型的ConvNet特征。在[[236](#bib.bib236)]中，通过适应物体提议技术并利用卷积神经网络特征，可以识别图像中的潜在地标进行地点识别。此方法不需要任何形式的训练，并且系统组件足够通用，可以直接使用，从而在性能上超越了当前的最先进技术。其他代表性工作，如[[237](#bib.bib237)、[238](#bib.bib238)、[239](#bib.bib239)]，基于深度自编码器结构提取紧凑表示，以无监督的方式压缩场景。具体而言，[[237](#bib.bib237)]利用堆叠去噪自编码器（SDA）从原始输入数据中学习压缩表示，允许在图像数据中学习复杂的内部结构，而无需手动视觉特征设计。[[238](#bib.bib238)]采用了无监督自编码器架构，经过随机投影变换训练以模拟自然视角变化，并使用方向梯度直方图（HOG）描述符以实现光照不变性。该方法无需标记训练数据或环境特定训练，能够实时闭环且无维度降减。[[239](#bib.bib239)]基于超字典，比传统BoW字典更节省内存。所提模型使用两个深度神经网络加速环闭合检测，并忽略移动物体的影响。实验结果表明，它表现出强大的鲁棒性，并且显著更快。
- en: Deep learning-based loop closure methods offer more robust and effective visual
    features by leveraging high-level representations learned from deep neural networks.
    These approaches have demonstrated improved performance in place recognition and
    loop-closure detection and can be integrated into SLAM systems.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的环闭合方法通过利用从深度神经网络中学习到的高级表示，提供了更强大和有效的视觉特征。这些方法在地点识别和环闭合检测方面表现出改善，并且可以集成到SLAM系统中。
- en: VI-B Local Optimization
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 本地优化
- en: When jointly optimizing estimated camera motion and scene geometry, SLAM systems
    enforce them to satisfy a certain constraint. It is done by minimizing a geometric
    or photometric loss to ensure their consistency in the local area - the surroundings
    of camera poses. This is bundle adjustment (BA) problem [[240](#bib.bib240)].
    Learning based approaches predict depth maps and ego-motion through two individual
    networks trained above large datasets [[16](#bib.bib16)]. During the testing procedure
    when deployed online, there is a requirement that enforces the predictions to
    satisfy some local constraints. To enable local optimization, traditionally, the
    second-order solvers, e.g. Gauss-Newton (GN) method or Levenberg-Marquadt (LM)
    algorithm [[241](#bib.bib241)], are applied to optimize motion transformations
    and per-pixel depth maps.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在联合优化估计的相机运动和场景几何时，SLAM 系统强制它们满足一定的约束。这是通过最小化几何或光度损失来确保它们在局部区域（即相机姿态的周围环境）的一致性来完成的。这就是束调整（BA）问题[[240](#bib.bib240)]。基于学习的方法通过训练于大型数据集的两个独立网络来预测深度图和自我运动[[16](#bib.bib16)]。在在线部署的测试过程中，需要确保预测满足一些局部约束。为了实现局部优化，传统上使用二阶求解器，如高斯-牛顿（GN）方法或列文伯格-马夸特（LM）算法[[241](#bib.bib241)]，来优化运动变换和每像素深度图。
- en: To this end, LS-Net [[242](#bib.bib242)] tackles this problem via a learning
    based optimizer by integrating analytical solvers into its learning process. It
    learns a data-driven prior, followed by refining neural network predictions with
    an analytical optimizer to ensure photometric consistency. It can optimize sum-of-squares
    objective functions in SLAM algorithms, which are often difficult to optimize
    due to violated assumptions and ill-posed problems. BA-Net [[243](#bib.bib243)]
    integrates a differentiable second-order optimizer (LM algorithm) into a deep
    neural network for an end-to-end learning. Instead of minimizing geometric or
    photometric error, BA-Net is performed on feature space to optimize the consistency
    loss of features from multiview images extracted by ConvNets. This feature-level
    optimizer can mitigate the fundamental problems of geometric or photometric solution
    (e.g. some information may be lost in the geometric optimization, while environmental
    dynamics and lighting changes may impact the photometric optimization). This work
    combines domain knowledge of SLAM with deep learning and achieves successful results
    on large-scale real data, outperforming conventional SLAM with geometric or photometric
    BA and deep learning based methods, e.g. Zhou et al.[[16](#bib.bib16)].
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，LS-Net [[242](#bib.bib242)] 通过将分析求解器集成到学习过程中来解决这个问题。它学习数据驱动的先验，然后用分析优化器来细化神经网络预测，以确保光度一致性。它可以优化
    SLAM 算法中的平方和目标函数，这些目标函数由于违反假设和病态问题往往很难优化。BA-Net [[243](#bib.bib243)] 将可微分的二阶优化器（LM
    算法）集成到深度神经网络中，实现端到端学习。BA-Net 不再最小化几何或光度误差，而是在特征空间上进行操作，以优化由卷积网络提取的多视图图像特征的一致性损失。这种特征级优化器可以缓解几何或光度解的基本问题（例如，在几何优化中可能会丢失一些信息，而环境动态和光照变化可能会影响光度优化）。这项工作结合了
    SLAM 的领域知识和深度学习，并在大规模真实数据上取得了成功的结果，超越了传统的几何或光度 BA 和基于深度学习的方法，如 Zhou 等人[[16](#bib.bib16)]。
- en: These learning based optimizers provide an alternative to solve local bundle
    adjustment problem. By integrating analytical solvers and differentiable second-order
    optimizers into their learning processes, these methods have demonstrated the
    potential to improve SLAM performance by mitigating challenges such as violated
    assumptions and ill-posed problems or information loss during optimization. Consequently,
    they are able to offer promising results for enhancing the accuracy and robustness
    of local optimization in SLAM systems.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基于学习的优化器提供了解决局部束调整问题的替代方案。通过将分析求解器和可微分的二阶优化器集成到它们的学习过程中，这些方法展示了通过减轻假设违反、病态问题或优化过程中信息丢失等挑战来提高
    SLAM 性能的潜力。因此，它们能够提供有希望的结果，以增强 SLAM 系统局部优化的准确性和鲁棒性。
- en: VI-C Global Optimization
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 全局优化
- en: Incremental motion estimation (visual odometry) suffers from accumulative error
    drifts during long-term operation. This issue stems from the inherent problem
    of path integration, where the system’s errors progressively accumulate without
    effective constraints. To address this challenge, graph-SLAM [[9](#bib.bib9)]
    constructs a topological graph to represent camera poses or scene features as
    graph nodes, which are connected by edges (measured by sensors) to constrain the
    poses. This graph-based formulation can be optimized to ensure the global consistency
    of graph nodes and edges, mitigating the possible errors on pose estimates and
    the inherent sensor measurement noise. A popular solver for global optimization
    is through Levenberg-Marquardt (LM) algorithm.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 增量运动估计（视觉里程计）在长期操作中会受到累积误差漂移的影响。这个问题源自路径积分的固有问题，在这一过程中，系统的误差逐渐累积而没有有效的约束。为了解决这一挑战，图优化SLAM
    [[9](#bib.bib9)] 构建了一个拓扑图，将相机姿态或场景特征表示为图节点，这些节点通过传感器测量的边连接，以约束姿态。这个基于图的公式化方法可以进行优化，以确保图节点和边的全局一致性，从而减轻姿态估计的可能误差和固有的传感器测量噪声。一个流行的全局优化解算器是通过Levenberg-Marquardt（LM）算法。
- en: In the era of deep learning, deep neural networks excel at extracting features,
    and constructing functions from observations to poses and scene representations.
    A global optimization upon the DNN predictions is necessary to reducing the drifts
    of global trajectories and supporting large-scale mapping. Compared with a variety
    of well-researched solutions in classical SLAM, optimizing deep predictions globally
    is underexplored.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的时代，深度神经网络擅长提取特征，并从观测数据构建函数来映射姿态和场景表示。对深度神经网络预测进行全局优化是必要的，以减少全局轨迹的漂移并支持大规模映射。与经典SLAM中的各种研究解决方案相比，优化深度预测的全局方法仍未得到充分探索。
- en: Various studies have explored the integration of learning modules into classical
    SLAM systems at different levels. At the front-end, deep neural networks (DNNs)
    generate predictions, which are then incorporated into the back-end for optimization
    and refinement. One good example is CNN-SLAM [[185](#bib.bib185)], which uses
    learned per-pixel depths to support loop closing and graph optimization in LSD-SLAM,
    a complete SLAM system [[186](#bib.bib186)]. The joint optimization of camera
    poses, scene representations, and depth maps in CNN-SLAM produces consistent scale
    metrics. This method has been evaluated for estimating the absolute scale of the
    reconstruction and fusing semantic labels, which results in semantically coherent
    scene reconstruction from a single view. CNN-SLAM is capable of producing pose
    and depth estimates consistently in low-textured areas where traditional SLAM
    systems tend to fail by utilizing depth predictions from neural networks. In DeepTAM
    [[244](#bib.bib244)], the depth and pose predictions from deep neural networks
    are integrated into a classical DTAM system[[177](#bib.bib177)], where the system
    estimates small pose increments and accumulates information in a cost volume to
    update the depth prediction. Depth measurements and image-based priors are combined
    for optimization, which results in more accurate scene reconstruction and camera
    motion tracking. Few images are required, and the system is robust to noisy camera
    poses. Similarly, in [[35](#bib.bib35)], unsupervised learning-based VO is combined
    with a graph optimization back-end. This method generates a windowed pose graph
    consisting of multi-view constraints and uses a novel pose cycle consistency loss
    to improve performance and robustness. Conversely, DeepFactors [[245](#bib.bib245)]
    integrates the learned optimizable scene representation (their so-called code
    representation) into a probabilistic factor graph-based back-end for global optimization.
    The advantage of the factor-graph-based formulation is its flexibility to include
    sensor measurements, state estimates, and constraints. It is comparably easy and
    convenient to add new sensor modalities, pairwise constraints, and system states
    into the graph for optimization.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 各种研究探讨了将学习模块集成到经典 SLAM 系统中的不同层次。在前端，深度神经网络（DNNs）生成预测，这些预测随后被纳入后端进行优化和精细化。一个很好的例子是
    CNN-SLAM [[185](#bib.bib185)]，它利用学习的每像素深度来支持 LSD-SLAM 中的环路闭合和图优化，这是一个完整的 SLAM
    系统 [[186](#bib.bib186)]。CNN-SLAM 中相机姿态、场景表示和深度图的联合优化产生了一致的尺度指标。这种方法已被评估用于估计重建的绝对尺度和融合语义标签，从而实现从单视角的语义一致的场景重建。CNN-SLAM
    能够在传统 SLAM 系统往往失败的低纹理区域中通过利用神经网络的深度预测持续产生姿态和深度估计。在 DeepTAM [[244](#bib.bib244)]
    中，深度神经网络的深度和姿态预测被集成到经典的 DTAM 系统[[177](#bib.bib177)]中，系统估计小的姿态增量并在成本体积中积累信息以更新深度预测。深度测量和基于图像的先验结合进行优化，从而实现更准确的场景重建和相机运动跟踪。所需图像很少，系统对噪声相机姿态具有鲁棒性。同样，在
    [[35](#bib.bib35)] 中，无监督学习的 VO 与图优化后端相结合。这种方法生成一个由多视图约束组成的窗口化姿态图，并使用一种新颖的姿态循环一致性损失来提高性能和鲁棒性。相反，DeepFactors
    [[245](#bib.bib245)] 将可优化的场景表示（他们所谓的代码表示）集成到基于概率因子图的后端进行全局优化。因子图基于的公式的优势在于它能够灵活地包含传感器测量、状态估计和约束。将新的传感器模式、成对约束和系统状态添加到图中进行优化是相对容易和方便的。
- en: In summary, these methods integrate deep neural networks with SLAM back-ends,
    resulting in numerous benefits, such as improved accuracy and robustness in scene
    reconstruction and camera motion tracking, handling of low-textured areas, and
    flexibility in the factor-graph-based formulation for adding new sensor modalities,
    pairwise constraints, and system states for optimization. It is worth noting,
    however, that the back-end optimizers employed in these methods are not entirely
    differentiable at present.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，这些方法将深度神经网络与 SLAM 后端集成，带来了诸多好处，如提高了场景重建和相机运动跟踪的准确性和鲁棒性，处理了低纹理区域，并在因子图基于的公式中提供了添加新传感器模式、成对约束和系统状态进行优化的灵活性。然而，值得注意的是，这些方法中使用的后端优化器目前尚不完全可微分。
- en: VII Uncertainty Estimation
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 不确定性估计
- en: 'Safety and interpretability are an critical step towards the practical deployment
    of mobile agents in real-world applications: the former enables robots to live
    and act with human reliably, while the latter allows users to have better understanding
    over model behaviours. Although deep learning models achieve impressive performance
    in many visual regression and classification tasks, once failure cases occur,
    errors from one component inevitably propagate to other downstream modules, causing
    catastrophic consequences. To this end, there is an emerging need to estimate
    the uncertainty of DNN predictions. This section introduces deep learning approaches
    to estimating uncertainty for localization and mapping, i.e. to capture the uncertainty
    with the purpose of motion tracking or scene understanding. The estimated uncertainty
    plays a vital role in probabilistic sensor fusion or the back-end optimization
    of SLAM systems.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和可解释性是将移动代理实际部署到现实世界应用中的关键步骤：前者使机器人能够可靠地与人类共同生活和行动，而后者则使用户能够更好地理解模型行为。尽管深度学习模型在许多视觉回归和分类任务中取得了令人印象深刻的表现，但一旦出现故障情况，一个组件的错误不可避免地会传播到其他下游模块，从而造成灾难性的后果。为此，估计深度神经网络（DNN）预测的不确定性成为了一个新兴的需求。本节介绍了用于定位和地图绘制的不确定性估计的深度学习方法，即捕捉不确定性以实现运动跟踪或场景理解。估计的不确定性在概率传感器融合或SLAM系统的后端优化中起着至关重要的作用。
- en: 'Deep learning models normally produce the mean values of target predictions,
    for example, the output of a DNN-based visual odometry model is a 6-dimensional
    pose vector. In order to capture the uncertainty of deep predictions, deep learning
    models can be augmented into a Bayesian model [[246](#bib.bib246), [58](#bib.bib58)].
    The Bayesian uncertainties are broadly categorized into Aleatoric and epistemic
    uncertainty: Aleatoric uncertainty reflects observation noises, e.g. sensor measurement
    or motion noises; epistemic uncertainty captures the belief in model parameters
    [[58](#bib.bib58)]. Bayesian models have been applied to solve global localization
    problem. As illustrated in [[69](#bib.bib69), [71](#bib.bib71)], the uncertainty
    from deep models are able to reflect the global location errors, in which the
    unreliable pose estimates are avoided with this belief metric. Estimating the
    uncertainty of DNN-based incremental motion estimation has been explored by [[247](#bib.bib247)].
    It adopts a strategy to convert target predictions into a Gaussian distribution,
    conditioned on the mean value of pose estimates and its covariance. The parameters
    inside the framework are optimized via a loss function with a combination of mean
    and covariance. By minimizing the error function to find the best combination,
    the uncertainty of motion transformation is automatically recovered in an unsupervised
    fashion. [[247](#bib.bib247)] integrates the learned uncertainty into a graph
    based SLAM as the covariances of odometry edges. It validates that the learned
    uncertainty further improves the performance of a SLAM system over the baseline
    with a fixed predefined value of covariance.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常生成目标预测的均值，例如，基于DNN的视觉里程计模型的输出是一个6维姿态向量。为了捕捉深度预测的不确定性，深度学习模型可以扩展为贝叶斯模型[[246](#bib.bib246),
    [58](#bib.bib58)]。贝叶斯不确定性大致分为偶然不确定性和认知不确定性：偶然不确定性反映了观察噪声，例如传感器测量或运动噪声；认知不确定性捕捉对模型参数的信念[[58](#bib.bib58)]。贝叶斯模型已被应用于解决全局定位问题。如[[69](#bib.bib69),
    [71](#bib.bib71)]所示，深度模型中的不确定性能够反映全局位置误差，其中通过这种信念度量避免了不可靠的姿态估计。[[247](#bib.bib247)]探讨了基于DNN的增量运动估计的不确定性估计。它采用了一种将目标预测转换为高斯分布的策略，条件是姿态估计的均值及其协方差。框架内的参数通过包含均值和协方差的损失函数进行优化。通过最小化误差函数以找到最佳组合，运动变换的不确定性以无监督的方式自动恢复。[[247](#bib.bib247)]将学习到的不确定性整合到基于图的SLAM中，作为里程计边缘的协方差。它验证了学习到的不确定性进一步提高了SLAM系统的性能，相较于使用固定预定义协方差值的基线。
- en: The uncertainty for scene understanding also contributes to SLAM systems. The
    scene uncertainty offers a belief metric in to what extent the environmental perception
    and scene structure should be trusted. For example, in the semantic segmentation
    and depth estimation tasks, uncertainty estimation provides per-pixel uncertainties
    for the DNN predictions of semantics and depth maps [[248](#bib.bib248), [58](#bib.bib58),
    [249](#bib.bib249)]. Further more, scene uncertainty is applicable to building
    a hybrid SLAM system. For example, photometric uncertainty can be learned to capture
    the variance of intensity on each image pixel, and hence enhances the robustness
    of a SLAM system towards observation noises [[17](#bib.bib17)].
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 场景理解的不确定性也对 SLAM 系统有影响。场景的不确定性提供了一种信念度量，表明环境感知和场景结构在多大程度上应该被信任。例如，在语义分割和深度估计任务中，不确定性估计为
    DNN 对语义和深度图的预测提供了每像素的不确定性[[248](#bib.bib248), [58](#bib.bib58), [249](#bib.bib249)]。此外，场景不确定性适用于构建混合
    SLAM 系统。例如，光度不确定性可以通过学习来捕捉每个图像像素上强度的方差，从而增强 SLAM 系统对观测噪声的鲁棒性[[17](#bib.bib17)]。
- en: VIII Sensor Fusion
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 传感器融合
- en: Sensor fusion stands as a fundamental challenge in the field of robotics. In
    this section, we discuss learning based sensor fusion strategy for motion tracking.
    We focus on visual-inertial sensor fusion as a representative example. Integrating
    visual and inertial data as visual-inertial odometry (VIO) is a well-defined problem.
    Accurate estimation of pose heavily relies on the effective fusion of measurements
    from these two complementary sensors. In recent years, data-driven approaches
    have emerged to directly learn 6-DoF poses from visual and inertial measurements.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器融合是机器人领域的一个基本挑战。在这一部分，我们讨论了用于运动跟踪的学习基传感器融合策略。我们以视觉惯性传感器融合为代表例进行重点讨论。将视觉和惯性数据整合为视觉惯性里程计（VIO）是一个明确定义的问题。精确的姿态估计严重依赖于这两个互补传感器测量的有效融合。近年来，数据驱动的方法已经出现，直接从视觉和惯性测量中学习
    6-DoF 姿态。
- en: VIII-A Supervised learning based VIO
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 基于监督学习的视觉惯性里程计
- en: VINet [[250](#bib.bib250)] treats visual-inertial odometry as a sequential learning
    problem, and proposes an end-to-end DNN framework to solve this problem. VINet
    uses a ConvNet based visual encoder to extract visual features from two consecutive
    RGB images, and an inertial encoder to extract inertial features from a sequence
    of IMU data using a long short-term memory (LSTM) network. The deep sensor fusion
    is achieved by concatenating visual and inertial features and passing them through
    an LSTM module to predict relative poses. While this approach is more robust to
    calibration and timing offset errors, it has not addressed the issue of learning
    a meaningful sensor fusion strategy. To tackle the deep sensor fusion problem,
    [[251](#bib.bib251), [252](#bib.bib252)] propose selective sensor fusion, that
    selectively learns context-dependent representations for visual inertial pose
    estimation. Their intuition is that the importance of features from different
    modalities should be considered according to the exterior (i.e., environmental)
    and interior (i.e., device/sensor) dynamics, by fully exploiting the complementary
    behaviors of two sensors. Their approach outperforms VINet and other models without
    a fusion strategy, avoiding catastrophic failures.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: VINet [[250](#bib.bib250)] 将视觉惯性里程计视为一个序列学习问题，并提出了一个端到端的 DNN 框架来解决这个问题。VINet
    使用基于卷积网络的视觉编码器从两个连续的 RGB 图像中提取视觉特征，并使用长短期记忆（LSTM）网络从 IMU 数据序列中提取惯性特征。深度传感器融合通过将视觉和惯性特征串联并通过
    LSTM 模块来预测相对位姿来实现。虽然这种方法对校准和时间偏差误差更具鲁棒性，但尚未解决学习有意义传感器融合策略的问题。为了解决深度传感器融合问题，[[251](#bib.bib251),
    [252](#bib.bib252)] 提出了选择性传感器融合，选择性地学习视觉惯性位姿估计的上下文相关表示。他们的直觉是，根据外部（即环境）和内部（即设备/传感器）动态，应该考虑来自不同模态的特征的重要性，通过充分利用两个传感器的互补行为。他们的方法优于
    VINet 和其他没有融合策略的模型，避免了灾难性失败。
- en: VIII-B Self-supervised learning based VIO
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 基于自监督学习的视觉惯性里程计
- en: Learning visual and inertial sensor fusion can be tackled in a self-supervised
    manner using novel view synthesis. VIOLearner [[253](#bib.bib253)] constructs
    motion transformations from raw inertial data and converts source images into
    target images with the camera matrix and depth maps. An online error correction
    module is introduced to correct intermediate errors, and the network parameters
    are optimized using a photometric loss. Similarly, DeepVIO [[254](#bib.bib254)]
    uses an unsupervised learning framework to incorporate inertial data and stereo
    images, training with a dedicated loss to reconstruct trajectories on a global
    scale. A recent unsupervised visual-inertial odometry framework, UnVIO [[255](#bib.bib255)],
    predicts per-frame depth maps and self-adaptively fuses visual-inertial motion
    features for pose estimation. To overcome error accumulation and scale ambiguity
    issues, UnVIO introduces a sliding window optimization strategy. Thanks to this
    strategy, UnVIO outperforms both DeepVIO [[254](#bib.bib254)] and VIOLearner [[253](#bib.bib253)].
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 学习视觉和惯性传感器融合可以通过新颖的视图合成以自监督的方式进行。VIOLearner [[253](#bib.bib253)] 从原始惯性数据构建运动变换，并利用相机矩阵和深度图将源图像转换为目标图像。引入了一个在线误差修正模块来纠正中间误差，并使用光度损失优化网络参数。同样，DeepVIO
    [[254](#bib.bib254)] 使用无监督学习框架结合惯性数据和立体图像，利用专门的损失函数训练，以在全球范围内重建轨迹。最近的无监督视觉惯性里程计框架UnVIO
    [[255](#bib.bib255)] 预测每帧深度图，并自适应融合视觉惯性运动特征以进行姿态估计。为了克服误差累积和尺度模糊问题，UnVIO 引入了滑动窗口优化策略。得益于这一策略，UnVIO
    超越了 DeepVIO [[254](#bib.bib254)] 和 VIOLearner [[253](#bib.bib253)]。
- en: Overall, though current learning-based VIO models are not able to surpass the
    state-of-the-art classical model-based VIOs, the experiments conducted in the
    works discussed in the previous paragraph indicate that they offer greater robustness
    against real-world issues such as measurement noises and bad time synchronization.
    This improved performance can be attributed to the ability of DNNs to extract
    useful features, learn implicit multimodal fusion, and accurately model motion,
    providing a significant advantage over classical models.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，尽管目前基于学习的VIO模型尚未超越最先进的经典模型VIO，但在前述工作的实验表明，它们在面对实际问题如测量噪声和时间同步不良时提供了更大的鲁棒性。这种改进的性能归因于DNN提取有用特征、学习隐式多模态融合和准确建模运动的能力，相较于经典模型具有显著优势。
- en: IX Discussions
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 讨论
- en: This survey comprehensively overviews the area of deep learning for visual localization
    and mapping, and provides a taxonomy to cover the relevant existing approaches
    from robotics, computer vision and machine learning communities. The fast development
    of deep learning provides an alternative to solve this problem in a data-driven
    way, and meanwhile paves the road towards the next-generation AI based spatial
    perception solution.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查全面概述了视觉定位和映射领域的深度学习，并提供了一个分类体系，涵盖了来自机器人技术、计算机视觉和机器学习社区的相关现有方法。深度学习的快速发展为以数据驱动的方式解决此问题提供了替代方案，同时为下一代基于AI的空间感知解决方案铺平了道路。
- en: Both localization and mapping modules are critical components of SLAM systems
    and can function independently or jointly. In instances where our discussion pertains
    to either localization or mapping separately, we will explicitly state ”localization”
    or ”mapping”. Conversely, if the benefits or limitations apply to both localization
    and mapping, we will use ”SLAM” or ”localization and mapping”. The two questions
    posted at the beginning of this article are visited here, and the limitations
    of current learning based approaches are summarized as follows.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 定位和映射模块是SLAM系统的关键组件，可以独立或共同工作。当我们讨论定位或映射时，会明确说明“定位”或“映射”。相反，如果讨论适用于定位和映射，我们将使用“SLAM”或“定位和映射”。本文开头提出的两个问题在此得到讨论，并总结了当前基于学习的方法的局限性。
- en: 1) Is deep learning promising to visual localization and mapping?
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 深度学习对视觉定位和映射有前景吗？
- en: 'TABLE V: A summary of how deep learning can be applied to tackle localization
    and mapping'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：深度学习如何应用于解决定位和映射的总结
- en: '| How to apply deep learning to solve localization and mapping | Methods |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 如何应用深度学习解决定位和映射 | 方法 |'
- en: '| --- | --- |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Deep learning is used as a universal approximator | [[15](#bib.bib15), [23](#bib.bib23),
    [25](#bib.bib25), [24](#bib.bib24), [26](#bib.bib26)] |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习被用作通用逼近器 | [[15](#bib.bib15), [23](#bib.bib23), [25](#bib.bib25), [24](#bib.bib24),
    [26](#bib.bib26)] |'
- en: '| Deep learning is applied to solve the association problem | [[65](#bib.bib65),
    [209](#bib.bib209), [210](#bib.bib210), [236](#bib.bib236)] |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习被应用于解决关联问题 | [[65](#bib.bib65), [209](#bib.bib209), [210](#bib.bib210),
    [236](#bib.bib236)] |'
- en: '| Deep learning can automatically discover relevant features | [[243](#bib.bib243),
    [251](#bib.bib251), [231](#bib.bib231), [232](#bib.bib232), [233](#bib.bib233)]
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习可以自动发现相关特征 | [[243](#bib.bib243), [251](#bib.bib251), [231](#bib.bib231),
    [232](#bib.bib232), [233](#bib.bib233)] |'
- en: '| Self-learning framework can be set up to automatically update parameters
    | [[16](#bib.bib16), [37](#bib.bib37), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [49](#bib.bib49), [23](#bib.bib23), [33](#bib.bib33), [35](#bib.bib35), [54](#bib.bib54),
    [256](#bib.bib256)] |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 自学习框架可以设置为自动更新参数 | [[16](#bib.bib16), [37](#bib.bib37), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [49](#bib.bib49), [23](#bib.bib23), [33](#bib.bib33),
    [35](#bib.bib35), [54](#bib.bib54), [256](#bib.bib256)] |'
- en: '| Deep learning can tackle some intrinsic problems of conventional algorithms
    | [[185](#bib.bib185), [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50), [17](#bib.bib17)]
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习可以解决传统算法的一些内在问题 | [[185](#bib.bib185), [47](#bib.bib47), [49](#bib.bib49),
    [50](#bib.bib50), [17](#bib.bib17)] |'
- en: 'SLAM systems have progressed fast over the past decades and shown great successes
    in real-world deployment. Examples can be witnessed from delivery robots to mobile
    and wearable devices. Admittedly, predominant SLAM systems without embracing deep
    learning has already meet many needs in certain conditions by exploiting physical
    laws or geometry heuristics to build up models and algorithms. Nevertheless, the
    final answer to the promise of deep learning for SLAM depends on application scenarios
    from a general view. We believe that three particular properties listed below
    could make deep learning a unique direction towards a general-purpose SLAM system
    in the future:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM系统在过去几十年里发展迅速，并在实际应用中取得了巨大成功。可以从送货机器人到移动和可穿戴设备中看到实例。诚然，主流的SLAM系统在没有深度学习的情况下，已经通过利用物理定律或几何启发式来建立模型和算法，在某些条件下满足了许多需求。然而，从总体上看，深度学习在SLAM中的最终承诺取决于应用场景。我们相信，以下列出的三个特定属性可以使深度学习在未来成为通用SLAM系统的独特方向：
- en: (1) First, deep learning offers powerful perception tools that can be integrated
    into the visual SLAM front-end to extract features in challenging areas for odometry
    estimation or relocalization, and provide dense depth [[180](#bib.bib180), [16](#bib.bib16)],
    and semantic labelling [[209](#bib.bib209), [210](#bib.bib210)] for mapping. Deep
    learning has been largely embraced by the computer vision community, leading to
    state-of-the-art methods in a number of computer vision tasks, e.g. object detection,
    image recognition and semantic segmentation. Some works have already introduced
    learning algorithms as a ’black box’ module to solve important and useful perception
    problems for SLAM [[244](#bib.bib244), [17](#bib.bib17)].
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 首先，深度学习提供了强大的感知工具，可以集成到视觉SLAM前端中，以提取困难区域的特征，用于里程计估计或重新定位，并提供稠密深度 [[180](#bib.bib180),
    [16](#bib.bib16)]，以及语义标记 [[209](#bib.bib209), [210](#bib.bib210)]用于映射。深度学习已被计算机视觉社区广泛采纳，导致在许多计算机视觉任务中出现了最先进的方法，例如目标检测、图像识别和语义分割。一些研究已经引入了学习算法作为“黑箱”模块，以解决SLAM的关键和有用的感知问题
    [[244](#bib.bib244), [17](#bib.bib17)]。
- en: (2) Second, deep learning enables high-level understanding and interaction for
    robots. Neural networks are known to be powerful in connecting abstract elements
    with human understandable terms[[209](#bib.bib209), [210](#bib.bib210)], such
    as labelling scene semantics in a mapping or SLAM system, which is normally hard
    to describe in a formal mathematical way. Deep-learning enabled scene understanding,
    on the other hand, is able to support high-level robotic tasks, for example, a
    service robot searches for an apple in kitchen room by leveraging fine-grained
    indoor semantics.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 其次，深度学习使得机器人能够进行高层次的理解和互动。神经网络在将抽象元素与人类可理解的术语相连接方面被认为非常强大[[209](#bib.bib209),
    [210](#bib.bib210)]，例如在映射或SLAM系统中标记场景语义，这通常很难用正式的数学方式描述。另一方面，深度学习支持的场景理解能够支持高层次的机器人任务，例如，一个服务机器人通过利用精细的室内语义在厨房中寻找一个苹果。
- en: (3) Third, learning methods allow SLAM systems or individual localization/mapping
    algorithms to learn from past experience, and actively exploit new information
    for self-learning and adapting to new environment. Beyond performing in restricted
    areas, future SLAM systems are believed to undertake more indispensable roles
    in unseen scenarios, e.g. nuclear waste disposal. By leveraging self-supervised
    learning [[16](#bib.bib16)], or reinforcement learning [[231](#bib.bib231), [232](#bib.bib232),
    [233](#bib.bib233)], it would offer opportunities to self-update system (neural
    network) parameters, and be promising to enhancing the adaptation ability of mobile
    agents to unseen scenarios without human intervention.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 第三，学习方法允许SLAM系统或单个定位/制图算法从过去的经验中学习，并积极利用新信息进行自学习并适应新环境。除了在限制区域内执行任务，未来的SLAM系统被认为将在未见过的场景中承担更为重要的角色，例如核废料处理。通过利用自监督学习[[16](#bib.bib16)]或强化学习[[231](#bib.bib231),
    [232](#bib.bib232), [233](#bib.bib233)]，可以为自我更新系统（神经网络）参数提供机会，并有望提升移动代理对未见场景的适应能力而无需人工干预。
- en: 2) How can deep learning be applied to solve visual localization and mapping?
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 深度学习如何应用于解决视觉定位和制图问题？
- en: 'After reviewing the existing works in above sections, predominant methods of
    adopting deep learning in SLAM systems can be summarized in Table [V](#S9.T5 "TABLE
    V ‣ IX Discussions ‣ Deep Learning for Visual Localization and Mapping: A Survey").'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '在以上章节中回顾现有的工作后，采用深度学习的SLAM系统的主要方法可以在表格[V](#S9.T5 "TABLE V ‣ IX Discussions
    ‣ Deep Learning for Visual Localization and Mapping: A Survey")中总结。'
- en: (1) Deep learning is used as a universal approximator to describe certain functions
    of SLAM or individual localization/mapping algorithms. For example, visual odometry
    can be achieved by building an end-to-end deep neural network model to directly
    approximate the function from images to pose [[15](#bib.bib15), [23](#bib.bib23),
    [25](#bib.bib25), [24](#bib.bib24), [26](#bib.bib26)]. The advantage here is that
    the learned models can be inherently incorporated and resilient to certain circumstances,
    e.g. featureless areas, dynamic lightning conditions and motion blur which are
    typically difficult to model.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 深度学习被用作通用近似器来描述SLAM或单个定位/制图算法的某些功能。例如，可以通过构建一个端到端的深度神经网络模型来直接近似从图像到位姿的函数，从而实现视觉里程计[[15](#bib.bib15),
    [23](#bib.bib23), [25](#bib.bib25), [24](#bib.bib24), [26](#bib.bib26)]。这里的优势在于，学习到的模型可以内在地融入并对某些情况具有弹性，例如特征稀缺区域、动态光照条件和运动模糊，这些通常难以建模。
- en: (2) Deep learning is applied to solve the association problem in SLAM. Relocalization
    needs to connect an image with a pre-built map, and retrieves its pose[[65](#bib.bib65)].
    Semantic mapping or SLAM needs to tackle the complex semantics labelling that
    associates pixels with its semantic meaning [[209](#bib.bib209), [210](#bib.bib210)].
    Loop-closure detection requires to recognize whether observed scene is relevant
    to the place visited previously [[236](#bib.bib236)].
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 深度学习被应用于解决SLAM中的关联问题。重新定位需要将图像与预先构建的地图连接起来，并检索其位姿[[65](#bib.bib65)]。语义制图或SLAM需要处理复杂的语义标注，将像素与其语义含义关联起来[[209](#bib.bib209),
    [210](#bib.bib210)]。回环检测需要识别观察到的场景是否与先前访问的地点相关[[236](#bib.bib236)]。
- en: (3) Deep learning is leveraged to automatically discover features relevant to
    the task of interest. For example, features suitable to bundle adjustment are
    extracted to SLAM, showing performance improvement [[243](#bib.bib243)]. In [[251](#bib.bib251)],
    features relevant to sensor fusion are extracted for visual-inertial odometry.
    Reinforcement learning based navigation also utilizes the discovered features
    to constitute an implicit map for path planning and task-driven navigation [[231](#bib.bib231),
    [232](#bib.bib232), [233](#bib.bib233)].
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 深度学习被用来自动发现与感兴趣任务相关的特征。例如，提取适用于束调整的特征到SLAM中，显示出性能提升[[243](#bib.bib243)]。在[[251](#bib.bib251)]中，提取与传感器融合相关的特征用于视觉惯性里程计。基于强化学习的导航还利用发现的特征来构建路径规划和任务驱动导航的隐式地图[[231](#bib.bib231),
    [232](#bib.bib232), [233](#bib.bib233)]。
- en: (4) By exploiting prior knowledge, e.g. the geometry constraints, a self-learning
    framework can be set up for SLAM to automatically update parameters based on input
    images. For instance, novel view synthesis can serve as a self-supervision signal
    to recover self-motion and depth from unlabelled videos [[16](#bib.bib16), [37](#bib.bib37),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [49](#bib.bib49), [23](#bib.bib23),
    [33](#bib.bib33), [35](#bib.bib35), [54](#bib.bib54), [256](#bib.bib256)], thereby
    supporting localization tasks.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 通过利用先验知识，例如几何约束，可以建立一个自学习框架来自动更新 SLAM 的参数，基于输入图像。例如，新颖视图合成可以作为自我监督信号，从未标记的视频中恢复自我运动和深度
    [[16](#bib.bib16), [37](#bib.bib37), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [49](#bib.bib49), [23](#bib.bib23), [33](#bib.bib33), [35](#bib.bib35), [54](#bib.bib54),
    [256](#bib.bib256)]，从而支持定位任务。
- en: (5) Deep learning can be utilized to tackle some intrinsic problems of conventional
    SLAM or localization/mapping algorithms. For instance, the scale-ambiguity problem
    of monocular SLAM is mitigated by using learned depth estimates with absolute
    scale from deep neural networks [[185](#bib.bib185), [47](#bib.bib47), [49](#bib.bib49),
    [50](#bib.bib50)]. Furthermore, the photometric uncertainties of scenes produced
    by deep neural networks can be introduced into visual odometry (VO) in order to
    encourage the framework to leverage features that can be trusted and thus further
    enhance pose estimation performance [[17](#bib.bib17)].
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 深度学习可以用于解决传统 SLAM 或定位/映射算法的一些固有问题。例如，单目 SLAM 的尺度歧义问题通过使用来自深度神经网络的带有绝对尺度的学习深度估计来减轻
    [[185](#bib.bib185), [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50)]。此外，深度神经网络产生的场景的光度不确定性可以引入到视觉里程计
    (VO) 中，以鼓励框架利用可以信赖的特征，从而进一步提升姿态估计性能 [[17](#bib.bib17)]。
- en: 3) The limitations and Future Directions
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 限制与未来方向
- en: 'It must also be pointed out that these learning techniques are reliant on massive
    datasets, preferably with accurate labels, to extract statistically meaningful
    patterns and may struggle to generalize to out-of-set environments. Futhermore,
    there is a lack of sufficiency interpretability of model behaviors as deep learning
    to date is still a black box model. Additionally, although highly parallelizable,
    deep learning based localization and mapping systems are also typically more computationally
    costly than simpler conventional models if model compression techniques are not
    used. We further discuss the limitations of existing works as follows:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 还必须指出，这些学习技术依赖于大量数据集，最好是具有准确标签的数据集，以提取统计上有意义的模式，并且可能难以推广到集外环境。此外，模型行为的解释性仍然不足，因为深度学习至今仍然是一个黑箱模型。此外，尽管深度学习具有高度的并行性，但基于深度学习的定位和地图构建系统通常也比更简单的传统模型计算成本更高，如果不使用模型压缩技术。我们进一步讨论了现有工作的局限性如下：
- en: (1) Real-world deployment. Deploying deep learning models in real-world environments
    is a systematic research problem. In existing research, the prediction accuracy
    is always chosen as the ‘rule of thumb’ to follow, while other crucial issues
    are overlooked, such as the optimality of model structure and sizes. The computational
    and energy consumption have to be considered on resource-constrained systems,
    e.g., miniaturized insect robots or VR/AR/MR devices. The parallerization opportunities,
    such as convolutional filters or other parallel neural network modules should
    be further investigated in order to fully exploit the potential of GPUs.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 现实世界部署。在现实环境中部署深度学习模型是一个系统性的研究问题。在现有研究中，预测准确性总是被选择作为遵循的“经验法则”，而其他关键问题，如模型结构和大小的最优性，往往被忽视。计算和能源消耗必须在资源受限的系统上考虑，例如微型昆虫机器人或
    VR/AR/MR 设备。并行化的机会，如卷积滤波器或其他并行神经网络模块，应进一步研究，以充分发挥 GPU 的潜力。
- en: (2) Scalability. Deep learning based localization and mapping models have now
    achieved promising results on the evaluation benchmark. However, they are restricted
    to some scenarios. For example, odometry estimation or localization is always
    evaluated in the city area or on the road. Whether these techniques could be applied
    to other environments, e.g. rural area or forest area remains as an open problem.
    Moreover, existing works on mapping or scene reconstruction are restricted to
    single-objects, synthetic data or only at a room level. It is worth exploring
    the opportunity to scale these methods to more complex, large-scale and realistic
    problems.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 可扩展性。基于深度学习的定位和建图模型现在在评估基准上取得了有希望的结果。然而，它们在某些场景下受到限制。例如，里程计估计或定位总是在城市区域或道路上进行评估。这些技术是否可以应用于其他环境，例如乡村或森林区域仍然是一个未解的问题。此外，现有的建图或场景重建工作仅限于单一物体、合成数据或仅在房间级别进行。值得探索将这些方法扩展到更复杂、大规模和现实问题的机会。
- en: (3) Safety, reliability and interpretability. Safety and reliability are critical
    to many applications in practice, e.g. autonomous driving, surgical robots and
    delivery robots. In these scenarios, even a small error of pose or scene estimates
    will cause disasters to the entire system. Deep neural networks have been long-critisized
    as ’black-box’, exacerbating the safety concerns for critical tasks. Some initial
    efforts explore the interpretability on deep models. For example, uncertainty
    estimation [[246](#bib.bib246), [58](#bib.bib58)] offers a belief metric, representing
    to what extent we trust our models. In this way, unreliable predictions (with
    low uncertainty) should be avoided or alleviated in order to ensure system’s safety
    and reliability.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 安全性、可靠性和可解释性。安全性和可靠性对于许多实际应用至关重要，例如自动驾驶、外科手术机器人和送货机器人。在这些场景中，即使是姿态或场景估计上的小错误也会对整个系统造成灾难。深度神经网络长期以来被批评为“黑箱”，加剧了对关键任务的安全性担忧。一些初步的努力探讨了深度模型的可解释性。例如，不确定性估计[[246](#bib.bib246)、[58](#bib.bib58)]提供了一种信任度度量，表示我们对模型的信任程度。通过这种方式，应避免或缓解不可靠的预测（具有低不确定性），以确保系统的安全性和可靠性。
- en: (4) Trade-Offs. Although deep learning shows impressive results in solving localization
    and mapping tasks, there are trade-offs came with them. The metric to evaluate
    these learning methods is still dominated by the accuracy which can not comprehensively
    examine the performance of a method. Most works only focus on improving the prediction
    accuracy. Sometimes a larger DNN model that requires more computation and storage
    contributes to a higher prediction accuracy, but the hardware and system constraints
    in practice are rarely considered, such as model efficiency, memory storage and
    I/O bandwidth. This problem is increasingly pronounced with localization and mapping
    systems becoming increasingly adopted on resource-constrained platforms, e.g.,
    a drone or an AR headset. In addition, it is hard to choose the optimal parameters
    e.g. the size of hidden states or the number of layers, inside a DNN model. Another
    trade-off is between training a model to be overfitted in one domain or to generalize
    in new domains. Although a model performs well on test data, its performance could
    be degraded when deployed in application domains that are different from training
    sets.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 权衡。尽管深度学习在解决定位和建图任务方面表现出色，但这些方法也存在权衡。评估这些学习方法的指标仍然以准确度为主，这无法全面评估方法的性能。大多数工作只关注提高预测准确度。有时，一个需要更多计算和存储的大型DNN模型会导致更高的预测准确度，但实际中很少考虑硬件和系统约束，例如模型效率、内存存储和I/O带宽。随着定位和建图系统越来越多地应用于资源受限的平台（如无人机或AR头显），这一问题变得越来越突出。此外，很难选择DNN模型内的最佳参数，例如隐状态的大小或层数。另一个权衡是在一个领域过拟合模型与在新领域泛化之间。尽管模型在测试数据上表现良好，但在部署到与训练集不同的应用领域时，其性能可能会下降。
- en: References
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, B. Upcroft,
    P. Abbeel, W. Burgard, M. Milford, and P. Corke, “The Limits and Potentials of
    Deep Learning for Robotics,” International Journal of Robotics Research, vol. 37,
    no. 4-5, 2018.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, B.
    Upcroft, P. Abbeel, W. Burgard, M. Milford, and P. Corke, “深度学习在机器人领域的局限性与潜力”，《国际机器人研究杂志》，第37卷，第4-5期，2018年。'
- en: '[2] C. Forster, M. Pizzoli, and D. Scaramuzza, “SVO: Fast Semi-Direct Monocular
    Visual Odometry,” in The IEEE International Conference on Robotics and Automation
    (ICRA), pp. 15–22, 2014.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] C. Forster, M. Pizzoli, 和 D. Scaramuzza, “SVO：快速半直接单目视觉里程计，” 发表在 IEEE 国际机器人与自动化会议
    (ICRA)，页 15–22，2014。'
- en: '[3] T. Qin, P. Li, and S. Shen, “VINS-Mono: A Robust and Versatile Monocular
    Visual-Inertial State Estimator,” IEEE Transactions on Robotics, vol. 34, no. 4,
    pp. 1004–1020, 2018.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] T. Qin, P. Li, 和 S. Shen, “VINS-Mono：一种鲁棒且多功能的单目视觉-惯性状态估计器，” IEEE 机器人学杂志，卷
    34，第 4 期，页 1004–1020，2018。'
- en: '[4] T. Sattler, B. Leibe, and L. Kobbelt, “Fast image-based localization using
    direct 2d-to-3d matching,” in The International Conference on Computer Vision
    (ICCV), pp. 667–674, IEEE, 2011.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Sattler, B. Leibe, 和 L. Kobbelt, “使用直接 2D 到 3D 匹配的快速基于图像的定位，” 发表在国际计算机视觉会议
    (ICCV)，页 667–674，IEEE，2011。'
- en: '[5] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, and
    M. J. Milford, “Visual place recognition: A survey,” IEEE Transactions on Robotics,
    vol. 32, no. 1, pp. 1–19, 2015.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, 和
    M. J. Milford, “视觉位置识别：综述，” IEEE 机器人学杂志，卷 32，第 1 期，页 1–19，2015。'
- en: '[6] R. Mur-Artal, J. Montiel, and J. D. Tardos, “ORB-SLAM : A Versatile and
    Accurate Monocular SLAM System,” IEEE Transactions on Robotics, vol. 31, no. 5,
    pp. 1147–1163, 2015.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] R. Mur-Artal, J. Montiel, 和 J. D. Tardos, “ORB-SLAM：一种多功能且准确的单目 SLAM 系统，”
    IEEE 机器人学杂志，卷 31，第 5 期，页 1147–1163，2015。'
- en: '[7] S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics. MIT press, 2005.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] S. Thrun, W. Burgard, 和 D. Fox, 《概率机器人》，MIT 出版社，2005。'
- en: '[8] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and mapping:
    part i,” IEEE robotics & automation magazine, vol. 13, no. 2, pp. 99–110, 2006.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Durrant-Whyte 和 T. Bailey, “同时定位与地图构建：第一部分，” IEEE 机器人与自动化杂志，卷 13，第 2
    期，页 99–110，2006。'
- en: '[9] G. Grisetti, R. Kummerle, C. Stachniss, and W. Burgard, “A tutorial on
    graph-based slam,” IEEE Intelligent Transportation Systems Magazine, vol. 2, no. 4,
    pp. 31–43, 2010.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] G. Grisetti, R. Kummerle, C. Stachniss, 和 W. Burgard, “基于图的 SLAM 教程，” IEEE
    智能交通系统杂志，卷 2，第 4 期，页 31–43，2010。'
- en: '[10] D. Scaramuzza and F. Fraundorfer, “Visual odometry [tutorial],” IEEE robotics
    & automation magazine, vol. 18, no. 4, pp. 80–92, 2011.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] D. Scaramuzza 和 F. Fraundorfer, “视觉里程计 [教程]，” IEEE 机器人与自动化杂志，卷 18，第 4
    期，页 80–92，2011。'
- en: '[11] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, and J. J. Leonard, “Past, present, and future of simultaneous localization
    and mapping: Toward the robust-perception age,” IEEE Transactions on robotics,
    vol. 32, no. 6, pp. 1309–1332, 2016.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, 和 J. J. Leonard, “同时定位与地图构建的过去、现在和未来：迈向鲁棒感知时代，” IEEE 机器人学交易，卷 32，第 6
    期，页 1309–1332，2016。'
- en: '[12] M. R. U. Saputra, A. Markham, and N. Trigoni, “Visual slam and structure
    from motion in dynamic environments: A survey,” ACM Computing Surveys (CSUR),
    vol. 51, no. 2, pp. 1–36, 2018.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. R. U. Saputra, A. Markham, 和 N. Trigoni, “动态环境中的视觉 SLAM 和运动结构：综述，”
    ACM 计算调查 (CSUR)，卷 51，第 2 期，页 1–36，2018。'
- en: '[13] Y. Tang, C. Zhao, J. Wang, C. Zhang, Q. Sun, W. X. Zheng, W. Du, F. Qian,
    and J. Kurths, “Perception and navigation in autonomous systems in the era of
    learning: A survey,” IEEE Transactions on Neural Networks and Learning Systems,
    2022.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Tang, C. Zhao, J. Wang, C. Zhang, Q. Sun, W. X. Zheng, W. Du, F. Qian,
    和 J. Kurths, “学习时代的自主系统中的感知与导航：综述，” IEEE 神经网络与学习系统交易，2022。'
- en: '[14] T. Bailey and H. Durrant-Whyte, “Simultaneous localization and mapping
    (slam): Part ii,” IEEE robotics & automation magazine, vol. 13, no. 3, pp. 108–117,
    2006.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] T. Bailey 和 H. Durrant-Whyte, “同时定位与地图构建 (slam)：第二部分，” IEEE 机器人与自动化杂志，卷
    13，第 3 期，页 108–117，2006。'
- en: '[15] S. Wang, R. Clark, H. Wen, and N. Trigoni, “DeepVO : Towards End-to-End
    Visual Odometry with Deep Recurrent Convolutional Neural Networks,” in The IEEE
    International Conference on Robotics and Automation (ICRA), 2017.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Wang, R. Clark, H. Wen, 和 N. Trigoni, “DeepVO：基于深度递归卷积神经网络的端到端视觉里程计，”
    发表在 IEEE 国际机器人与自动化会议 (ICRA)，2017。'
- en: '[16] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised Learning
    of Depth and Ego-Motion from Video,” in IEEE/CVF International Conference on Computer
    Vision and Pattern Recognition (CVPR), 2017.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] T. Zhou, M. Brown, N. Snavely, 和 D. G. Lowe, “从视频中无监督学习深度和自我运动，” 发表在 IEEE/CVF
    国际计算机视觉与模式识别会议 (CVPR)，2017。'
- en: '[17] N. Yang, L. von Stumberg, R. Wang, and D. Cremers, “D3vo: Deep depth,
    deep pose and deep uncertainty for monocular visual odometry,” IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] N. Yang, L. von Stumberg, R. Wang, 和 D. Cremers，“D3vo：用于单目视觉里程计的深度深度、深度姿态和深度不确定性”，IEEE/CVF国际计算机视觉与模式识别会议（CVPR），2020年。'
- en: '[18] K. Konda and R. Memisevic, “Learning Visual Odometry with a Convolutional
    Network,” in International Conference on Computer Vision Theory and Applications,
    pp. 486–490, 2015.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] K. Konda 和 R. Memisevic，“使用卷积网络学习视觉里程计”，发表于计算机视觉理论与应用国际会议，第486–490页，2015年。'
- en: '[19] G. Costante, M. Mancini, P. Valigi, and T. A. Ciarfuglia, “Exploring representation
    learning with cnns for frame-to-frame ego-motion estimation,” IEEE robotics and
    automation letters, vol. 1, no. 1, pp. 18–25, 2015.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] G. Costante, M. Mancini, P. Valigi, 和 T. A. Ciarfuglia，“探索使用CNN进行帧间自我运动估计的表征学习”，IEEE机器人与自动化快报，第1卷，第1期，第18–25页，2015年。'
- en: '[20] A. Geiger, J. Ziegler, and C. Stiller, “Stereoscan: Dense 3d reconstruction
    in real-time,” in 2011 IEEE Intelligent Vehicles Symposium (IV), pp. 963–968,
    Ieee, 2011.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Geiger, J. Ziegler, 和 C. Stiller，“Stereoscan：实时密集3D重建”，发表于2011 IEEE智能车辆研讨会（IV），第963–968页，IEEE，2011年。'
- en: '[21] P. Fischer, E. Ilg, H. Philip, C. Hazırbas, P. V. D. Smagt, D. Cremers,
    and T. Brox, “FlowNet: Learning Optical Flow with Convolutional Networks,” in
    The International Conference on Computer Vision (ICCV), 2015.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] P. Fischer, E. Ilg, H. Philip, C. Hazırbas, P. V. D. Smagt, D. Cremers,
    和 T. Brox，“FlowNet：使用卷积网络学习光流”，发表于国际计算机视觉会议（ICCV），2015年。'
- en: '[22] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The KITTI dataset,” The International Journal of Robotics Research, vol. 32, no. 11,
    pp. 1231–1237, 2013.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Geiger, P. Lenz, C. Stiller, 和 R. Urtasun，“视觉遇见机器人：KITTI数据集”，《国际机器人研究期刊》，第32卷，第11期，第1231–1237页，2013年。'
- en: '[23] C. Zhao, L. Sun, P. Purkait, T. Duckett, and R. Stolkin, “Learning monocular
    visual odometry with dense 3d mapping from dense 3d flow,” in The IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 6864–6871, IEEE, 2018.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] C. Zhao, L. Sun, P. Purkait, T. Duckett, 和 R. Stolkin，“通过密集3D流的密集3D映射学习单目视觉里程计”，发表于IEEE/RSJ国际智能机器人与系统会议（IROS），第6864–6871页，IEEE，2018年。'
- en: '[24] M. R. U. Saputra, P. P. de Gusmao, S. Wang, A. Markham, and N. Trigoni,
    “Learning monocular visual odometry through geometry-aware curriculum learning,”
    in The IEEE International Conference on Robotics and Automation (ICRA), pp. 3549–3555,
    IEEE, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] M. R. U. Saputra, P. P. de Gusmao, S. Wang, A. Markham, 和 N. Trigoni，“通过几何感知课程学习学习单目视觉里程计”，发表于IEEE国际机器人与自动化会议（ICRA），第3549–3555页，IEEE，2019年。'
- en: '[25] F. Xue, X. Wang, S. Li, Q. Wang, J. Wang, and H. Zha, “Beyond tracking:
    Selecting memory and refining poses for deep visual odometry,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8575–8583, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] F. Xue, X. Wang, S. Li, Q. Wang, J. Wang, 和 H. Zha，“超越跟踪：选择记忆和优化姿态以进行深度视觉里程计”，发表于IEEE/CVF国际计算机视觉与模式识别会议（CVPR），第8575–8583页，2019年。'
- en: '[26] M. R. U. Saputra, P. P. de Gusmao, Y. Almalioglu, A. Markham, and N. Trigoni,
    “Distilling knowledge from a deep pose regressor network,” in The International
    Conference on Computer Vision (ICCV), pp. 263–272, 2019.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. R. U. Saputra, P. P. de Gusmao, Y. Almalioglu, A. Markham, 和 N. Trigoni，“从深度姿态回归网络中提炼知识”，发表于国际计算机视觉会议（ICCV），第263–272页，2019年。'
- en: '[27] A. S. Koumis, J. A. Preiss, and G. S. Sukhatme, “Estimating metric scale
    visual odometry from videos using 3d convolutional networks,” in The IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS), pp. 265–272,
    IEEE, 2019.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. S. Koumis, J. A. Preiss, 和 G. S. Sukhatme，“使用3D卷积网络从视频中估计度量尺度视觉里程计”，发表于IEEE/RSJ国际智能机器人与系统会议（IROS），第265–272页，IEEE，2019年。'
- en: '[28] X. Kuo, C. Liu, K. Lin, E. Luo, Y. Chen, and C. Lee, “Dynamic attention-based
    visual odometry,” in The IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), pp. 5753–5760, IEEE, 2020.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] X. Kuo, C. Liu, K. Lin, E. Luo, Y. Chen, 和 C. Lee，“基于动态注意力的视觉里程计”，发表于IEEE/RSJ国际智能机器人与系统会议（IROS），第5753–5760页，IEEE，2020年。'
- en: '[29] R. Li, S. Wang, Z. Long, and D. Gu, “Undeepvo: Monocular visual odometry
    through unsupervised deep learning,” in The IEEE International Conference on Robotics
    and Automation (ICRA), pp. 7286–7291, IEEE, 2018.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] R. Li, S. Wang, Z. Long, 和 D. Gu，“Undeepvo：通过无监督深度学习进行单目视觉里程计”，发表于IEEE国际机器人与自动化会议（ICRA），第7286–7291页，IEEE，2018年。'
- en: '[30] Z. Yin and J. Shi, “GeoNet: Unsupervised Learning of Dense Depth, Optical
    Flow and Camera Pose,” in IEEE/CVF International Conference on Computer Vision
    and Pattern Recognition (CVPR), 2018.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Z. Yin 和 J. Shi, “GeoNet：无监督学习密集深度、光流和相机姿态”，在《IEEE/CVF计算机视觉与模式识别国际会议》(CVPR)上，2018年。'
- en: '[31] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and I. Reid, “Unsupervised
    Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction,”
    in IEEE/CVF International Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 340–349, 2018.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, 和 I. Reid, “通过深度特征重建进行无监督单目深度估计和视觉里程计学习”，在《IEEE/CVF计算机视觉与模式识别国际会议》(CVPR)上，pp.
    340–349，2018年。'
- en: '[32] V. Casser, S. Pirk, R. Mahjourian, and A. Angelova, “Depth prediction
    without the sensors: Leveraging structure for unsupervised learning from monocular
    videos,” in The Conference on Artificial Intelligence (AAAI), vol. 33, pp. 8001–8008,
    2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] V. Casser, S. Pirk, R. Mahjourian, 和 A. Angelova, “无需传感器的深度预测：利用结构进行无监督单目视频学习”，在《人工智能会议》(AAAI)上，vol.
    33，pp. 8001–8008，2019年。'
- en: '[33] Y. Almalioglu, M. R. U. Saputra, P. P. de Gusmao, A. Markham, and N. Trigoni,
    “Ganvo: Unsupervised deep monocular visual odometry and depth estimation with
    generative adversarial networks,” in The IEEE International Conference on Robotics
    and Automation (ICRA), pp. 5474–5480, IEEE, 2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Almalioglu, M. R. U. Saputra, P. P. de Gusmao, A. Markham, 和 N. Trigoni,
    “Ganvo：基于生成对抗网络的无监督深度单目视觉里程计和深度估计”，在《IEEE国际机器人与自动化大会》(ICRA)上，pp. 5474–5480，IEEE，2019年。'
- en: '[34] R. Wang, S. M. Pizer, and J.-M. Frahm, “Recurrent neural network for (un-)
    supervised learning of monocular video visual odometry and depth,” in IEEE/CVF
    International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5555–5564,
    2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] R. Wang, S. M. Pizer, 和 J.-M. Frahm, “用于（无）监督学习单目视频视觉里程计和深度的递归神经网络”，在《IEEE/CVF计算机视觉与模式识别国际会议》(CVPR)上，pp.
    5555–5564，2019年。'
- en: '[35] Y. Li, Y. Ushiku, and T. Harada, “Pose graph optimization for unsupervised
    monocular visual odometry,” in The IEEE International Conference on Robotics and
    Automation (ICRA), pp. 5439–5445, IEEE, 2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Li, Y. Ushiku, 和 T. Harada, “用于无监督单目视觉里程计的姿态图优化”，在《IEEE国际机器人与自动化大会》(ICRA)上，pp.
    5439–5445，IEEE，2019年。'
- en: '[36] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, “Depth from videos
    in the wild: Unsupervised monocular depth learning from unknown cameras,” in The
    International Conference on Computer Vision (ICCV), pp. 8977–8986, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] A. Gordon, H. Li, R. Jonschkowski, 和 A. Angelova, “来自野外视频的深度：从未知相机进行无监督单目深度学习”，在《国际计算机视觉会议》(ICCV)上，pp.
    8977–8986，2019年。'
- en: '[37] J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M.-M. Cheng, and I. Reid, “Unsupervised
    scale-consistent depth and ego-motion learning from monocular video,” in Neural
    Information Processing Systems (NeurIPS), pp. 35–45, 2019.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M.-M. Cheng, 和 I. Reid, “从单目视频中学习无监督的尺度一致深度和自我运动”，在《神经信息处理系统》(NeurIPS)上，pp.
    35–45，2019年。'
- en: '[38] S. Li, X. Wang, Y. Cao, F. Xue, Z. Yan, and H. Zha, “Self-supervised deep
    visual odometry with online adaptation,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 6338–6347, 2020.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] S. Li, X. Wang, Y. Cao, F. Xue, Z. Yan, 和 H. Zha, “自监督深度视觉里程计与在线适应”，在《IEEE/CVF计算机视觉与模式识别国际会议》(CVPR)上，pp.
    6338–6347，2020年。'
- en: '[39] Y. Zou, P. Ji, Q. Tran, J. Huang, and M. Chandraker, “Learning monocular
    visual odometry via self-supervised long-term modeling,” in European Conference
    on Computer Vision (ECCV), vol. 12359, pp. 710–727, Springer, 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Zou, P. Ji, Q. Tran, J. Huang, 和 M. Chandraker, “通过自监督长期建模学习单目视觉里程计”，在《欧洲计算机视觉会议》(ECCV)上，vol.
    12359，pp. 710–727，Springer，2020年。'
- en: '[40] C. Zhao, G. G. Yen, Q. Sun, C. Zhang, and Y. Tang, “Masked gan for unsupervised
    depth and pose prediction with scale consistency,” IEEE Transactions on Neural
    Networks and Learning Systems, vol. 32, no. 12, pp. 5392–5403, 2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C. Zhao, G. G. Yen, Q. Sun, C. Zhang, 和 Y. Tang, “用于无监督深度和姿态预测的遮蔽GAN与尺度一致性”，《IEEE神经网络与学习系统汇刊》，vol.
    32，no. 12，pp. 5392–5403，2020年。'
- en: '[41] C. Chi, Q. Wang, T. Hao, P. Guo, and X. Yang, “Feature-level collaboration:
    Joint unsupervised learning of optical flow, stereo depth and camera motion,”
    in IEEE/CVF International Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 2463–2473, 2021.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] C. Chi, Q. Wang, T. Hao, P. Guo, 和 X. Yang, “特征级协作：光流、立体深度和相机运动的联合无监督学习”，在《IEEE/CVF计算机视觉与模式识别国际会议》(CVPR)上，pp.
    2463–2473，2021年。'
- en: '[42] S. Li, X. Wu, Y. Cao, and H. Zha, “Generalizing to the open world: Deep
    visual odometry with online adaptation,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 13184–13193, 2021.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. Li, X. Wu, Y. Cao, 和 H. Zha，“向开放世界推广：具有在线适应的深度视觉里程计”，发表于《IEEE/CVF国际计算机视觉与模式识别会议（CVPR）》，第13184–13193页，2021年。'
- en: '[43] J. Dai, X. Gong, Y. Li, J. Wang, and M. Wei, “Self-supervised deep visual
    odometry based on geometric attention model,” IEEE Transactions on Intelligent
    Transportation Systems, 2022.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Dai, X. Gong, Y. Li, J. Wang, 和 M. Wei，“基于几何注意力模型的自监督深度视觉里程计”，《IEEE智能交通系统汇刊》，2022年。'
- en: '[44] S. Zhang, J. Zhang, and D. Tao, “Towards scale consistent monocular visual
    odometry by learning from the virtual world,” The International Conference on
    Robotics and Automation (ICRA), 2022.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Zhang, J. Zhang, 和 D. Tao，“通过从虚拟世界中学习实现尺度一致的单目视觉里程计”，发表于《国际机器人与自动化会议（ICRA）》，2022年。'
- en: '[45] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop KF: Learning
    Discriminative Deterministic State Estimators,” in Neural Information Processing
    Systems (NeurIPS), 2016.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] T. Haarnoja, A. Ajay, S. Levine, 和 P. Abbeel，“Backprop KF：学习判别性确定性状态估计器”，发表于《神经信息处理系统（NeurIPS）》，2016年。'
- en: '[46] X. Yin, X. Wang, X. Du, and Q. Chen, “Scale recovery for monocular visual
    odometry using depth estimated with deep convolutional neural fields,” in The
    International Conference on Computer Vision (ICCV), pp. 5870–5878, 2017.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] X. Yin, X. Wang, X. Du, 和 Q. Chen，“利用深度卷积神经场估计的深度恢复单目视觉里程计中的尺度”，发表于《国际计算机视觉会议（ICCV）》，第5870–5878页，2017年。'
- en: '[47] D. Barnes, W. Maddern, G. Pascoe, and I. Posner, “Driven to distraction:
    Self-supervised distractor learning for robust monocular visual odometry in urban
    environments,” in The IEEE International Conference on Robotics and Automation
    (ICRA), pp. 1894–1900, IEEE, 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] D. Barnes, W. Maddern, G. Pascoe, 和 I. Posner，“分心驾驶：用于城市环境中鲁棒单目视觉里程计的自监督干扰学习”，发表于《IEEE国际机器人与自动化会议（ICRA）》，第1894–1900页，IEEE，2018年。'
- en: '[48] R. Jonschkowski, D. Rastogi, and O. Brock, “Differentiable Particle Filters:
    End-to-End Learning with Algorithmic Priors,” in Robotics: Science and Systems,
    2018.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] R. Jonschkowski, D. Rastogi, 和 O. Brock，“可微分粒子滤波器：具有算法先验的端到端学习”，发表于《机器人学：科学与系统》，2018年。'
- en: '[49] N. Yang, R. Wang, J. Stuckler, and D. Cremers, “Deep virtual stereo odometry:
    Leveraging deep depth prediction for monocular direct sparse odometry,” in The
    European Conference on Computer Vision (ECCV), pp. 817–833, 2018.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] N. Yang, R. Wang, J. Stuckler, 和 D. Cremers，“深度虚拟立体视觉里程计：利用深度预测进行单目直接稀疏里程计”，发表于《欧洲计算机视觉会议（ECCV）》，第817–833页，2018年。'
- en: '[50] S. Y. Loo, A. J. Amiri, S. Mashohor, S. H. Tang, and H. Zhang, “Cnn-svo:
    Improving the mapping in semi-direct visual odometry using single-image depth
    prediction,” in The IEEE International Conference on Robotics and Automation (ICRA),
    pp. 5218–5223, IEEE, 2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Y. Loo, A. J. Amiri, S. Mashohor, S. H. Tang, 和 H. Zhang，“Cnn-svo：通过单幅图像深度预测改进半直接视觉里程计中的映射”，发表于《IEEE国际机器人与自动化会议（ICRA）》，第5218–5223页，IEEE，2019年。'
- en: '[51] H. Zhan, C. S. Weerasekera, J. Bian, and I. Reid, “Visual odometry revisited:
    What should be learnt?,” The IEEE International Conference on Robotics and Automation
    (ICRA), 2020.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Zhan, C. S. Weerasekera, J. Bian, 和 I. Reid，“视觉里程计再探：应学习什么？”，发表于《IEEE国际机器人与自动化会议（ICRA）》，2020年。'
- en: '[52] B. Wagstaff, V. Peretroukhin, and J. Kelly, “Self-supervised deep pose
    corrections for robust visual odometry,” in The IEEE International Conference
    on Robotics and Automation (ICRA), pp. 2331–2337, IEEE, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] B. Wagstaff, V. Peretroukhin, 和 J. Kelly，“用于鲁棒视觉里程计的自监督深度姿态校正”，发表于《IEEE国际机器人与自动化会议（ICRA）》，第2331–2337页，IEEE，2020年。'
- en: '[53] L. Sun, W. Yin, E. Xie, Z. Li, C. Sun, and C. Shen, “Improving monocular
    visual odometry using learned depth,” IEEE Transactions on Robotics, vol. 38,
    no. 5, pp. 3173–3186, 2022.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] L. Sun, W. Yin, E. Xie, Z. Li, C. Sun, 和 C. Shen，“利用学习到的深度改进单目视觉里程计”，《IEEE机器人学汇刊》，第38卷，第5期，第3173–3186页，2022年。'
- en: '[54] S. Li, F. Xue, X. Wang, Z. Yan, and H. Zha, “Sequential adversarial learning
    for self-supervised deep visual odometry,” in The International Conference on
    Computer Vision (ICCV), pp. 2851–2860, 2019.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Li, F. Xue, X. Wang, Z. Yan, 和 H. Zha，“用于自监督深度视觉里程计的序列对抗学习”，发表于《国际计算机视觉会议（ICCV）》，第2851–2860页，2019年。'
- en: '[55] Q. Sun, Y. Tang, C. Zhang, C. Zhao, F. Qian, and J. Kurths, “Unsupervised
    estimation of monocular depth and vo in dynamic environments via hybrid masks,”
    IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 5, pp. 2023–2033,
    2021.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Q. Sun, Y. Tang, C. Zhang, C. Zhao, F. Qian 和 J. Kurths，“通过混合掩模在动态环境中进行无监督单目深度和
    VO 估计”，《IEEE 神经网络与学习系统汇刊》，第 33 卷，第 5 期，页码 2023–2033，2021 年。'
- en: '[56] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth
    estimation with left-right consistency,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 270–279, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] C. Godard, O. Mac Aodha 和 G. J. Brostow，“具有左右一致性的无监督单目深度估计”，在 IEEE/CVF
    国际计算机视觉与模式识别会议（CVPR），页码 270–279，2017 年。'
- en: '[57] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE transactions
    on pattern analysis and machine intelligence, vol. 40, no. 3, pp. 611–625, 2017.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. Engel, V. Koltun 和 D. Cremers，“直接稀疏里程计”，《IEEE 模式分析与机器智能汇刊》，第 40 卷，第
    3 期，页码 611–625，2017 年。'
- en: '[58] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?,” in Neural Information Processing Systems (NeurIPS),
    pp. 5574–5584, 2017.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Kendall 和 Y. Gal，“我们在计算机视觉的贝叶斯深度学习中需要什么不确定性？”，在神经信息处理系统（NeurIPS），页码
    5574–5584，2017 年。'
- en: '[59] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.
    Achtelik, and R. Siegwart, “The EuRoC micro aerial vehicle datasets,” International
    Journal of Robotics Research, vol. 35, no. 10, pp. 1157–1163, 2016.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.
    Achtelik 和 R. Siegwart，“EuRoC 微型飞行器数据集”，《国际机器人研究杂志》，第 35 卷，第 10 期，页码 1157–1163，2016
    年。'
- en: '[60] L. Von Stumberg, V. Usenko, and D. Cremers, “Direct sparse visual-inertial
    odometry using dynamic marginalization,” in The IEEE International Conference
    on Robotics and Automation (ICRA), pp. 2510–2517, IEEE, 2018.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] L. Von Stumberg, V. Usenko 和 D. Cremers，“使用动态边际化的直接稀疏视觉惯性里程计”，在 IEEE 国际机器人与自动化会议（ICRA），页码
    2510–2517，IEEE，2018 年。'
- en: '[61] C. Chen, C. X. Lu, B. Wang, N. Trigoni, and A. Markham, “Dynanet: Neural
    kalman dynamical model for motion estimation and prediction,” IEEE Transactions
    on Neural Networks and Learning Systems, vol. 32, no. 12, pp. 5479–5491, 2021.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] C. Chen, C. X. Lu, B. Wang, N. Trigoni 和 A. Markham，“Dynanet: 神经 Kalman
    动态模型用于运动估计和预测”，《IEEE 神经网络与学习系统汇刊》，第 32 卷，第 12 期，页码 5479–5491，2021 年。'
- en: '[62] Z. Laskar, I. Melekhov, S. Kalia, and J. Kannala, “Camera relocalization
    by computing pairwise relative poses using convolutional neural network,” in The
    International Conference on Computer Vision (ICCV) Workshops, pp. 929–938, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Z. Laskar, I. Melekhov, S. Kalia 和 J. Kannala，“通过卷积神经网络计算成对相对姿态进行相机重定位”，在国际计算机视觉大会（ICCV）研讨会，页码
    929–938，2017 年。'
- en: '[63] P. Wang, R. Yang, B. Cao, W. Xu, and Y. Lin, “Dels-3d: Deep localization
    and segmentation with a 3d semantic map,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 5860–5869, 2018.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] P. Wang, R. Yang, B. Cao, W. Xu 和 Y. Lin，“Dels-3d: 基于 3D 语义地图的深度定位与分割”，在
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），页码 5860–5869，2018 年。'
- en: '[64] S. Saha, G. Varma, and C. Jawahar, “Improved visual relocalization by
    discovering anchor points,” British Machine Vision Conference (BMVC), 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S. Saha, G. Varma 和 C. Jawahar，“通过发现锚点改善视觉重定位”，英国机器视觉会议（BMVC），2018 年。'
- en: '[65] V. Balntas, S. Li, and V. Prisacariu, “Relocnet: Continuous metric learning
    relocalisation using neural nets,” in The European Conference on Computer Vision
    (ECCV), pp. 751–767, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] V. Balntas, S. Li 和 V. Prisacariu，“Relocnet: 使用神经网络的连续度量学习重定位”，在欧洲计算机视觉会议（ECCV），页码
    751–767，2018 年。'
- en: '[66] M. Ding, Z. Wang, J. Sun, J. Shi, and P. Luo, “Camnet: Coarse-to-fine
    retrieval for camera re-localization,” in The International Conference on Computer
    Vision (ICCV), pp. 2871–2880, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] M. Ding, Z. Wang, J. Sun, J. Shi 和 P. Luo，“Camnet: 相机重新定位的粗到细检索”，在国际计算机视觉大会（ICCV），页码
    2871–2880，2019 年。'
- en: '[67] P.-E. Sarlin, A. Unagar, M. Larsson, H. Germain, C. Toft, V. Larsson,
    M. Pollefeys, V. Lepetit, L. Hammarstrand, F. Kahl, et al., “Back to the feature:
    Learning robust camera localization from pixels to pose,” in Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3247–3257,
    2021.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] P.-E. Sarlin, A. Unagar, M. Larsson, H. Germain, C. Toft, V. Larsson,
    M. Pollefeys, V. Lepetit, L. Hammarstrand, F. Kahl 等人，“回到特征：从像素到姿态学习鲁棒相机定位”，在
    IEEE/CVF 计算机视觉与模式识别会议论文集，页码 3247–3257，2021 年。'
- en: '[68] A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional network
    for real-time 6-dof camera relocalization,” in The International Conference on
    Computer Vision (ICCV), pp. 2938–2946, 2015.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Kendall, M. Grimes, 和 R. Cipolla, “Posenet: 一种用于实时 6-DoF 相机重定位的卷积网络，”
    发表在国际计算机视觉大会（ICCV），第 2938–2946 页，2015 年。'
- en: '[69] A. Kendall and R. Cipolla, “Modelling uncertainty in deep learning for
    camera relocalization,” in The IEEE International Conference on Robotics and Automation
    (ICRA), pp. 4762–4769, IEEE, 2016.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Kendall 和 R. Cipolla, “在深度学习中建模相机重定位的不确定性，” 发表在 IEEE 国际机器人与自动化大会（ICRA），第
    4762–4769 页，IEEE，2016 年。'
- en: '[70] J. Wu, L. Ma, and X. Hu, “Delving deeper into convolutional neural networks
    for camera relocalization,” in The IEEE International Conference on Robotics and
    Automation (ICRA), pp. 5644–5651, IEEE, 2017.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Wu, L. Ma, 和 X. Hu, “深入探讨卷积神经网络用于相机重定位，” 发表在 IEEE 国际机器人与自动化大会（ICRA），第
    5644–5651 页，IEEE，2017 年。'
- en: '[71] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen, “VidLoc: A Deep
    Spatio-Temporal Model for 6-DoF Video-Clip Relocalization,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] R. Clark, S. Wang, A. Markham, N. Trigoni, 和 H. Wen, “VidLoc: 一种用于 6-DoF
    视频片段重定位的深度时空模型，” 发表在 IEEE/CVF 国际计算机视觉与模式识别大会（CVPR），2017 年。'
- en: '[72] A. Kendall and R. Cipolla, “Geometric loss functions for camera pose regression
    with deep learning,” in IEEE/CVF International Conference on Computer Vision and
    Pattern Recognition (CVPR), pp. 5974–5983, 2017.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Kendall 和 R. Cipolla, “用于相机姿态回归的几何损失函数与深度学习，” 发表在 IEEE/CVF 国际计算机视觉与模式识别大会（CVPR），第
    5974–5983 页，2017 年。'
- en: '[73] T. Naseer and W. Burgard, “Deep regression for monocular camera-based
    6-dof global localization in outdoor environments,” in The IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 1525–1530, IEEE, 2017.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] T. Naseer 和 W. Burgard, “用于户外环境中单目相机的 6-DoF 全局定位的深度回归，” 发表在 IEEE/RSJ 国际智能机器人与系统大会（IROS），第
    1525–1530 页，IEEE，2017 年。'
- en: '[74] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, and D. Cremers,
    “Image-based localization using lstms for structured feature correlation,” in
    The International Conference on Computer Vision (ICCV), pp. 627–637, 2017.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, 和 D.
    Cremers, “基于图像的定位使用 LSTMs 进行结构化特征关联，” 发表在国际计算机视觉大会（ICCV），第 627–637 页，2017 年。'
- en: '[75] I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu, “Image-based localization
    using hourglass networks,” in The International Conference on Computer Vision
    (ICCV) Workshops, pp. 879–886, 2017.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] I. Melekhov, J. Ylioinas, J. Kannala, 和 E. Rahtu, “基于图像的定位使用沙漏网络，” 发表在国际计算机视觉大会（ICCV）工作坊，第
    879–886 页，2017 年。'
- en: '[76] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz, “Geometry-Aware Learning
    of Maps for Camera Localization,” in IEEE/CVF International Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 2616–2625, 2018.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, 和 J. Kautz, “面向几何的相机定位地图学习，” 发表在
    IEEE/CVF 国际计算机视觉与模式识别大会（CVPR），第 2616–2625 页，2018 年。'
- en: '[77] P. Purkait, C. Zhao, and C. Zach, “Synthetic view generation for absolute
    pose regression and image synthesis.,” in British Machine Vision Conference (BMVC),
    p. 69, 2018.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] P. Purkait, C. Zhao, 和 C. Zach, “用于绝对姿态回归和图像合成的合成视图生成，” 发表在英国机器视觉会议（BMVC），第
    69 页，2018 年。'
- en: '[78] M. Cai, C. Shen, and I. D. Reid, “A hybrid probabilistic model for camera
    relocalization,” in British Machine Vision Conference (BMVC), vol. 1, p. 8, 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Cai, C. Shen, 和 I. D. Reid, “一种用于相机重定位的混合概率模型，” 发表在英国机器视觉会议（BMVC），第
    1 卷，第 8 页，2018 年。'
- en: '[79] F. Xue, X. Wang, Z. Yan, Q. Wang, J. Wang, and H. Zha, “Local supports
    global: Deep camera relocalization with sequence enhancement,” in The International
    Conference on Computer Vision (ICCV), pp. 2841–2850, 2019.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] F. Xue, X. Wang, Z. Yan, Q. Wang, J. Wang, 和 H. Zha, “局部支持全局：通过序列增强的深度相机重定位，”
    发表在国际计算机视觉大会（ICCV），第 2841–2850 页，2019 年。'
- en: '[80] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, “Prior guided
    dropout for robust visual localization in dynamic environments,” in The International
    Conference on Computer Vision (ICCV), pp. 2791–2800, 2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao, 和 G. Zhang, “在动态环境中进行稳健视觉定位的先验指导
    dropout，” 发表在国际计算机视觉大会（ICCV），第 2791–2800 页，2019 年。'
- en: '[81] M. Bui, C. Baur, N. Navab, S. Ilic, and S. Albarqouni, “Adversarial networks
    for camera pose regression and refinement,” in The International Conference on
    Computer Vision (ICCV) Workshops, pp. 0–0, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Bui, C. Baur, N. Navab, S. Ilic, 和 S. Albarqouni, “用于相机姿态回归和细化的对抗网络，”
    发表在国际计算机视觉大会（ICCV）工作坊，第 0–0 页，2019 年。'
- en: '[82] B. Wang, C. Chen, C. X. Lu, P. Zhao, N. Trigoni, and A. Markham, “Atloc:
    Attention guided camera localization,” The Conference on Artificial Intelligence
    (AAAI), 2020.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] B. Wang, C. Chen, C. X. Lu, P. Zhao, N. Trigoni 和 A. Markham，“Atloc：基于注意力的相机定位”，人工智能大会
    (AAAI)，2020 年。'
- en: '[83] F. Xue, X. Wu, S. Cai, and J. Wang, “Learning multi-view camera relocalization
    with graph neural networks,” in IEEE/CVF International Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 11372–11381, IEEE, 2020.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] F. Xue, X. Wu, S. Cai 和 J. Wang，“使用图神经网络学习多视角相机重新定位”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议
    (CVPR)，第 11372–11381 页，IEEE，2020 年。'
- en: '[84] Y. Shavit, R. Ferens, and Y. Keller, “Learning multi-scene absolute pose
    regression with transformers,” in Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp. 2733–2742, 2021.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Shavit, R. Ferens 和 Y. Keller，“使用变换器学习多场景绝对姿态回归”，发表于 IEEE/CVF 国际计算机视觉会议，第
    2733–2742 页，2021 年。'
- en: '[85] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. Fitzgibbon,
    “Scene coordinate regression forests for camera relocalization in rgb-d images,”
    in IEEE/CVF International Conference on Computer Vision and Pattern Recognition,
    pp. 2930–2937, 2013.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi 和 A. Fitzgibbon，“用于
    rgb-d 图像中相机重新定位的场景坐标回归森林”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议，第 2930–2937 页，2013 年。'
- en: '[86] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi, and T. Pajdla, “24/7
    place recognition by view synthesis,” in IEEE/CVF International Conference on
    Computer Vision and Pattern Recognition (CVPR), pp. 1808–1817, 2015.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi 和 T. Pajdla，“通过视图合成进行
    24/7 位置识别”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议 (CVPR)，第 1808–1817 页，2015 年。'
- en: '[87] Y. Ge, H. Wang, F. Zhu, R. Zhao, and H. Li, “Self-supervising fine-grained
    region similarities for large-scale image localization,” in European Conference
    on Computer Vision, pp. 369–386, Springer, 2020.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Y. Ge, H. Wang, F. Zhu, R. Zhao 和 H. Li，“用于大规模图像定位的自我监督细粒度区域相似性”，发表于欧洲计算机视觉会议，第
    369–386 页，Springer，2020 年。'
- en: '[88] J. Thoma, D. P. Paudel, and L. V. Gool, “Soft contrastive learning for
    visual localization,” Advances in Neural Information Processing Systems, vol. 33,
    pp. 11119–11130, 2020.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] J. Thoma, D. P. Paudel 和 L. V. Gool，“用于视觉定位的软对比学习”，神经信息处理系统进展，卷 33，第 11119–11130
    页，2020 年。'
- en: '[89] Z. Chen, O. Lam, A. Jacobson, and M. Milford, “Convolutional neural network-based
    place recognition,” Australasian Conference on Robotics and Automation, 2014.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Z. Chen, O. Lam, A. Jacobson 和 M. Milford，“基于卷积神经网络的位置识别”，澳大利亚机器人与自动化会议，2014
    年。'
- en: '[90] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford, “On
    the performance of convnet features for place recognition,” in The IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 4297–4304, IEEE, 2015.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft 和 M. Milford，“卷积网络特征在位置识别中的性能”，发表于
    IEEE/RSJ 国际智能机器人与系统会议 (IROS)，第 4297–4304 页，IEEE，2015 年。'
- en: '[91] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “Netvlad:
    Cnn architecture for weakly supervised place recognition,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5297–5307, 2016.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla 和 J. Sivic，“Netvlad：用于弱监督位置识别的
    CNN 架构”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议 (CVPR)，第 5297–5307 页，2016 年。'
- en: '[92] H. Jégou, M. Douze, C. Schmid, and P. Pérez, “Aggregating local descriptors
    into a compact image representation,” in IEEE/CVF International Conference on
    Computer Vision and Pattern Recognition (CVPR), pp. 3304–3311, IEEE, 2010.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] H. Jégou, M. Douze, C. Schmid 和 P. Pérez，“将局部描述符聚合成紧凑的图像表示”，发表于 IEEE/CVF
    国际计算机视觉与模式识别会议 (CVPR)，第 3304–3311 页，IEEE，2010 年。'
- en: '[93] Q. Zhou, T. Sattler, M. Pollefeys, and L. Leal-Taixe, “To learn or not
    to learn: Visual localization from essential matrices,” The IEEE International
    Conference on Robotics and Automation (ICRA), 2019.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Q. Zhou, T. Sattler, M. Pollefeys 和 L. Leal-Taixe，“学习还是不学习：从本质矩阵中进行视觉定位”，IEEE
    国际机器人与自动化会议 (ICRA)，2019 年。'
- en: '[94] I. Melekhov, A. Tiulpin, T. Sattler, M. Pollefeys, E. Rahtu, and J. Kannala,
    “Dgc-net: Dense geometric correspondence network,” in IEEE Winter Conference on
    Applications of Computer Vision (WACV), pp. 1034–1042, IEEE, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] I. Melekhov, A. Tiulpin, T. Sattler, M. Pollefeys, E. Rahtu 和 J. Kannala，“Dgc-net：密集几何对应网络”，发表于
    IEEE 冬季计算机视觉应用会议 (WACV)，第 1034–1042 页，IEEE，2019 年。'
- en: '[95] B. Zhuang and M. Chandraker, “Fusing the old with the new: Learning relative
    camera pose with geometry-guided uncertainty,” in Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 32–42, 2021.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] B. Zhuang 和 M. Chandraker, “将旧技术与新技术融合：使用几何引导的不确定性学习相对相机姿态，” 见于 IEEE/CVF
    计算机视觉与模式识别会议论文集, pp. 32–42, 2021.'
- en: '[96] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in IEEE/CVF
    International Conference on Computer Vision and Pattern Recognition (CVPR), 2015.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich, “通过卷积深入学习，” 见于 IEEE/CVF 国际计算机视觉与模式识别会议 (CVPR),
    2015.'
- en: '[97] Y. Zhu, R. Gao, S. Huang, S.-C. Zhu, and Y. N. Wu, “Learning neural representation
    of camera pose with matrix representation of pose shift via view synthesis,” in
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 9959–9968, 2021.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Y. Zhu, R. Gao, S. Huang, S.-C. Zhu, 和 Y. N. Wu, “通过视图合成学习相机姿态的神经表示与姿态位移的矩阵表示，”
    见于 IEEE/CVF 计算机视觉与模式识别会议论文集, pp. 9959–9968, 2021.'
- en: '[98] M. Bui, T. Birdal, H. Deng, S. Albarqouni, L. Guibas, S. Ilic, and N. Navab,
    “6d camera relocalization in ambiguous scenes via continuous multimodal inference,”
    in European Conference on Computer Vision, pp. 139–157, Springer, 2020.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Bui, T. Birdal, H. Deng, S. Albarqouni, L. Guibas, S. Ilic, 和 N. Navab,
    “在模糊场景中通过连续的多模态推理进行 6D 相机重定位，” 见于 欧洲计算机视觉会议, pp. 139–157, Springer, 2020.'
- en: '[99] A. Valada, N. Radwan, and W. Burgard, “Deep auxiliary learning for visual
    localization and odometry,” in The IEEE International Conference on Robotics and
    Automation (ICRA), pp. 6939–6946, IEEE, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Valada, N. Radwan, 和 W. Burgard, “深度辅助学习用于视觉定位和里程计，” 见于 IEEE 国际机器人与自动化会议
    (ICRA), pp. 6939–6946, IEEE, 2018.'
- en: '[100] M. Tian, Q. Nie, and H. Shen, “3d scene geometry-aware constraint for
    camera localization with deep learning,” in 2020 IEEE International Conference
    on Robotics and Automation (ICRA), pp. 4211–4217, IEEE, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] M. Tian, Q. Nie, 和 H. Shen, “用于相机定位的 3D 场景几何感知约束与深度学习，” 见于 2020 IEEE
    国际机器人与自动化会议 (ICRA), pp. 4211–4217, IEEE, 2020.'
- en: '[101] N. Radwan, A. Valada, and W. Burgard, “Vlocnet++: Deep multitask learning
    for semantic visual localization and odometry,” IEEE Robotics and Automation Letters,
    vol. 3, no. 4, pp. 4407–4414, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] N. Radwan, A. Valada, 和 W. Burgard, “Vlocnet++: 深度多任务学习用于语义视觉定位和里程计，”
    IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4407–4414, 2018.'
- en: '[102] X. Li and H. Ling, “Pogo-net: Pose graph optimization with graph neural
    networks,” in Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp. 5895–5905, 2021.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] X. Li 和 H. Ling, “Pogo-net: 使用图神经网络的姿态图优化，” 见于 IEEE/CVF 国际计算机视觉会议论文集,
    pp. 5895–5905, 2021.'
- en: '[103] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial Discriminative
    Domain Adaptation,” in IEEE/CVF International Conference on Computer Vision and
    Pattern Recognition (CVPR), 2017.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] E. Tzeng, J. Hoffman, K. Saenko, 和 T. Darrell, “对抗性判别域自适应，” 见于 IEEE/CVF
    国际计算机视觉与模式识别会议 (CVPR), 2017.'
- en: '[104] Y. Li, N. Snavely, and D. P. Huttenlocher, “Location recognition using
    prioritized feature matching,” in The European Conference on Computer Vision (ECCV),
    pp. 791–804, Springer, 2010.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Li, N. Snavely, 和 D. P. Huttenlocher, “使用优先特征匹配进行位置识别，” 见于 欧洲计算机视觉会议
    (ECCV), pp. 791–804, Springer, 2010.'
- en: '[105] Y. Li, N. Snavely, D. Huttenlocher, and P. Fua, “Worldwide pose estimation
    using 3d point clouds,” in The European Conference on Computer Vision (ECCV),
    pp. 15–29, Springer, 2012.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Li, N. Snavely, D. Huttenlocher, 和 P. Fua, “使用 3D 点云进行全球姿态估计，” 见于
    欧洲计算机视觉会议 (ECCV), pp. 15–29, Springer, 2012.'
- en: '[106] B. Zeisl, T. Sattler, and M. Pollefeys, “Camera pose voting for large-scale
    image-based localization,” in The International Conference on Computer Vision
    (ICCV), pp. 2704–2712, 2015.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] B. Zeisl, T. Sattler, 和 M. Pollefeys, “大规模基于图像的定位的相机姿态投票，” 见于 国际计算机视觉会议
    (ICCV), pp. 2704–2712, 2015.'
- en: '[107] H. Germain, V. Lepetit, and G. Bourmaud, “Neural reprojection error:
    Merging feature learning and camera pose estimation,” in Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 414–423, 2021.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Germain, V. Lepetit, 和 G. Bourmaud, “神经重投影误差：融合特征学习与相机姿态估计，” 见于 IEEE/CVF
    计算机视觉与模式识别会议论文集, pp. 414–423, 2021.'
- en: '[108] T. Cavallari, S. Golodetz, N. A. Lord, J. Valentin, L. Di Stefano, and
    P. H. Torr, “On-the-fly adaptation of regression forests for online camera relocalisation,”
    in IEEE/CVF International Conference on Computer Vision and Pattern Recognition,
    pp. 4457–4466, 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] T. Cavallari, S. Golodetz, N. A. Lord, J. Valentin, L. Di Stefano, 和
    P. H. Torr，“在线相机重新定位的回归森林即时适应”，在 IEEE/CVF 国际计算机视觉与模式识别大会，第 4457–4466 页，2017 年。'
- en: '[109] A. Guzman-Rivera, P. Kohli, B. Glocker, J. Shotton, T. Sharp, A. Fitzgibbon,
    and S. Izadi, “Multi-output learning for camera relocalization,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1114–1121, 2014.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] A. Guzman-Rivera, P. Kohli, B. Glocker, J. Shotton, T. Sharp, A. Fitzgibbon,
    和 S. Izadi，“相机重新定位的多输出学习”，在 IEEE/CVF 国际计算机视觉与模式识别大会（CVPR），第 1114–1121 页，2014 年。'
- en: '[110] D. Massiceti, A. Krull, E. Brachmann, C. Rother, and P. H. Torr, “Random
    forests versus neural networks—what’s best for camera localization?,” in The IEEE
    International Conference on Robotics and Automation (ICRA), pp. 5118–5125, IEEE,
    2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] D. Massiceti, A. Krull, E. Brachmann, C. Rother, 和 P. H. Torr，“随机森林与神经网络——哪种更适合相机定位？”，在
    IEEE 国际机器人与自动化会议（ICRA），第 5118–5125 页，IEEE，2017 年。'
- en: '[111] X.-S. Gao, X.-R. Hou, J. Tang, and H.-F. Cheng, “Complete solution classification
    for the perspective-three-point problem,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 25, no. 8, pp. 930–943, 2003.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] X.-S. Gao, X.-R. Hou, J. Tang, 和 H.-F. Cheng，“透视三点问题的完整解法分类”，IEEE 模式分析与机器智能汇刊，第
    25 卷，第 8 期，第 930–943 页，2003 年。'
- en: '[112] J. Wald, T. Sattler, S. Golodetz, T. Cavallari, and F. Tombari, “Beyond
    controlled environments: 3d camera re-localization in changing indoor scenes,”
    in European Conference on Computer Vision, pp. 467–487, Springer, 2020.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Wald, T. Sattler, S. Golodetz, T. Cavallari, 和 F. Tombari，“超越受控环境：在变化的室内场景中进行
    3d 相机重新定位”，在欧洲计算机视觉大会，第 467–487 页，Springer，2020 年。'
- en: '[113] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han, “Large-scale image
    retrieval with attentive deep local features,” in The International Conference
    on Computer Vision (ICCV), pp. 3456–3465, 2017.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] H. Noh, A. Araujo, J. Sim, T. Weyand, 和 B. Han，“使用注意力深度局部特征的大规模图像检索”，在国际计算机视觉大会（ICCV），第
    3456–3465 页，2017 年。'
- en: '[114] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,
    T. Pajdla, and A. Torii, “Inloc: Indoor visual localization with dense matching
    and view synthesis,” in IEEE/CVF International Conference on Computer Vision and
    Pattern Recognition (CVPR), pp. 7199–7209, 2018.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,
    T. Pajdla, 和 A. Torii，“Inloc: 室内视觉定位与密集匹配和视图合成”，在 IEEE/CVF 国际计算机视觉与模式识别大会（CVPR），第
    7199–7209 页，2018 年。'
- en: '[115] J. L. Schönberger, M. Pollefeys, A. Geiger, and T. Sattler, “Semantic
    visual localization,” in IEEE/CVF International Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 6896–6906, 2018.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. L. Schönberger, M. Pollefeys, A. Geiger, 和 T. Sattler，“语义视觉定位”，在 IEEE/CVF
    国际计算机视觉与模式识别大会（CVPR），第 6896–6906 页，2018 年。'
- en: '[116] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superpoint: Self-supervised
    interest point detection and description,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 224–236, 2018.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] D. DeTone, T. Malisiewicz, 和 A. Rabinovich，“Superpoint: 自监督兴趣点检测与描述”，在
    IEEE/CVF 国际计算机视觉与模式识别大会（CVPR）研讨会，第 224–236 页，2018 年。'
- en: '[117] P.-E. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, and C. Cadena, “Leveraging
    deep visual descriptors for hierarchical efficient localization,” The Annual Conference
    on Robot Learning (CoRL), 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] P.-E. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, 和 C. Cadena，“利用深度视觉描述符进行层次化高效定位”，年度机器人学习大会（CoRL），2018
    年。'
- en: '[118] I. Rocco, M. Cimpoi, R. Arandjelović, A. Torii, T. Pajdla, and J. Sivic,
    “Neighbourhood consensus networks,” in Neural Information Processing Systems (NeurIPS),
    pp. 1651–1662, 2018.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] I. Rocco, M. Cimpoi, R. Arandjelović, A. Torii, T. Pajdla, 和 J. Sivic，“邻域共识网络”，在神经信息处理系统大会（NeurIPS），第
    1651–1662 页，2018 年。'
- en: '[119] M. Feng, S. Hu, M. H. Ang, and G. H. Lee, “2d3d-matchnet: learning to
    match keypoints across 2d image and 3d point cloud,” in The IEEE International
    Conference on Robotics and Automation (ICRA), pp. 4790–4796, IEEE, 2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Feng, S. Hu, M. H. Ang, 和 G. H. Lee，“2d3d-matchnet: 学习在 2d 图像和 3d
    点云之间匹配关键点”，在 IEEE 国际机器人与自动化会议（ICRA），第 4790–4796 页，IEEE，2019 年。'
- en: '[120] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk, “From coarse to
    fine: Robust hierarchical localization at large scale,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12716–12725,
    2019.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] P.-E. Sarlin, C. Cadena, R. Siegwart, 和 M. Dymczyk，“从粗到细: 大规模鲁棒分层定位”，发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第12716–12725页，2019年。'
- en: '[121] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and
    T. Sattler, “D2-net: A trainable cnn for joint description and detection of local
    features,” in IEEE/CVF International Conference on Computer Vision and Pattern
    Recognition, pp. 8092–8101, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, 和
    T. Sattler，“D2-Net: 一种可训练的 CNN，用于局部特征的联合描述和检测”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议，第8092–8101页，2019年。'
- en: '[122] P. Speciale, J. L. Schonberger, S. B. Kang, S. N. Sinha, and M. Pollefeys,
    “Privacy preserving image-based localization,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 5493–5503, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] P. Speciale, J. L. Schonberger, S. B. Kang, S. N. Sinha, 和 M. Pollefeys，“隐私保护的基于图像的定位”，发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第5493–5503页，2019年。'
- en: '[123] P. Weinzaepfel, G. Csurka, Y. Cabon, and M. Humenberger, “Visual localization
    by learning objects-of-interest dense match regression,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5634–5643, 2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] P. Weinzaepfel, G. Csurka, Y. Cabon, 和 M. Humenberger，“通过学习兴趣物体的密集匹配回归进行视觉定位”，发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第5634–5643页，2019年。'
- en: '[124] F. Camposeco, A. Cohen, M. Pollefeys, and T. Sattler, “Hybrid scene compression
    for visual localization,” in IEEE/CVF International Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 7653–7662, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] F. Camposeco, A. Cohen, M. Pollefeys, 和 T. Sattler，“用于视觉定位的混合场景压缩”，发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第7653–7662页，2019年。'
- en: '[125] W. Cheng, W. Lin, K. Chen, and X. Zhang, “Cascaded parallel filtering
    for memory-efficient image-based localization,” in The International Conference
    on Computer Vision (ICCV), pp. 1032–1041, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] W. Cheng, W. Lin, K. Chen, 和 X. Zhang，“内存高效的图像定位的级联并行过滤”，发表于国际计算机视觉大会（ICCV），第1032–1041页，2019年。'
- en: '[126] H. Taira, I. Rocco, J. Sedlar, M. Okutomi, J. Sivic, T. Pajdla, T. Sattler,
    and A. Torii, “Is this the right place? geometric-semantic pose verification for
    indoor visual localization,” in The International Conference on Computer Vision
    (ICCV), pp. 4373–4383, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] H. Taira, I. Rocco, J. Sedlar, M. Okutomi, J. Sivic, T. Pajdla, T. Sattler,
    和 A. Torii，“这是正确的地方吗？室内视觉定位的几何-语义姿态验证”，发表于国际计算机视觉大会（ICCV），第4373–4383页，2019年。'
- en: '[127] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon,
    and M. Humenberger, “R2d2: Repeatable and reliable detector and descriptor,” Neural
    Information Processing Systems (NeurIPS), 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon,
    和 M. Humenberger，“R2D2: 可重复和可靠的检测器与描述符”，发表于神经信息处理系统会议（NeurIPS），2019年。'
- en: '[128] Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang, and
    L. Quan, “Aslfeat: Learning local features of accurate shape and localization,”
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang, 和
    L. Quan，“ASLFeat: 学习准确形状和定位的局部特征”，发表于 IEEE/CVF 计算机视觉与模式识别大会（CVPR），2020年。'
- en: '[129] M. Dusmanu, O. Miksik, J. L. Schönberger, and M. Pollefeys, “Cross-descriptor
    visual localization and mapping,” in Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 6058–6067, 2021.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] M. Dusmanu, O. Miksik, J. L. Schönberger, 和 M. Pollefeys，“跨描述符视觉定位与映射”，发表于
    IEEE/CVF 国际计算机视觉会议论文集，第6058–6067页，2021年。'
- en: '[130] Z. Huang, H. Zhou, Y. Li, B. Yang, Y. Xu, X. Zhou, H. Bao, G. Zhang,
    and H. Li, “Vs-net: Voting with segmentation for visual localization,” in Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6101–6111,
    2021.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Z. Huang, H. Zhou, Y. Li, B. Yang, Y. Xu, X. Zhou, H. Bao, G. Zhang,
    和 H. Li，“VS-Net: 用于视觉定位的分割投票”，发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第6101–6111页，2021年。'
- en: '[131] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel, S. Gumhold,
    and C. Rother, “Dsac-differentiable ransac for camera localization,” in IEEE/CVF
    International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6684–6692,
    2017.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel, S. Gumhold,
    和 C. Rother，“Dsac-可微的 RANSAC 用于相机定位”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第6684–6692页，2017年。'
- en: '[132] E. Brachmann and C. Rother, “Learning less is more-6d camera localization
    via 3d surface regression,” in IEEE/CVF International Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 4654–4662, 2018.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] E. Brachmann 和 C. Rother，"学习更少即是更多——通过 3D 表面回归实现 6D 摄像机定位"，在 IEEE/CVF
    国际计算机视觉与模式识别会议（CVPR）中，第 4654–4662 页，2018 年。'
- en: '[133] X. Li, J. Ylioinas, J. Verbeek, and J. Kannala, “Scene coordinate regression
    with angle-based reprojection loss for camera relocalization,” in The European
    Conference on Computer Vision (ECCV), pp. 0–0, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] X. Li, J. Ylioinas, J. Verbeek 和 J. Kannala，"基于角度的重投影损失的场景坐标回归用于摄像机重定位"，在《欧洲计算机视觉会议》（ECCV）中，第
    0–0 页，2018 年。'
- en: '[134] X. Li, J. Ylioinas, and J. Kannala, “Full-frame scene coordinate regression
    for image-based localization,” Robotics: Science and Systems, 2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] X. Li, J. Ylioinas 和 J. Kannala，"全帧场景坐标回归用于基于图像的定位"，《机器人学：科学与系统》，2018
    年。'
- en: '[135] M. Bui, S. Albarqouni, S. Ilic, and N. Navab, “Scene coordinate and correspondence
    learning for image-based localization,” British Machine Vision Conference (BMVC),
    2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] M. Bui, S. Albarqouni, S. Ilic 和 N. Navab，"基于图像的定位的场景坐标和对应学习"，英国机器视觉会议（BMVC），2018
    年。'
- en: '[136] E. Brachmann and C. Rother, “Expert sample consensus applied to camera
    re-localization,” in The International Conference on Computer Vision (ICCV), pp. 7525–7534,
    2019.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] E. Brachmann 和 C. Rother，"应用专家样本共识于摄像机重定位"，在《国际计算机视觉会议》（ICCV）中，第 7525–7534
    页，2019 年。'
- en: '[137] E. Brachmann and C. Rother, “Neural-guided ransac: Learning where to
    sample model hypotheses,” in The International Conference on Computer Vision (ICCV),
    pp. 4322–4331, 2019.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] E. Brachmann 和 C. Rother，"神经引导 RANSAC：学习在哪里采样模型假设"，在《国际计算机视觉会议》（ICCV）中，第
    4322–4331 页，2019 年。'
- en: '[138] L. Yang, Z. Bai, C. Tang, H. Li, Y. Furukawa, and P. Tan, “Sanet: Scene
    agnostic network for camera localization,” in The International Conference on
    Computer Vision (ICCV), pp. 42–51, 2019.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] L. Yang, Z. Bai, C. Tang, H. Li, Y. Furukawa 和 P. Tan，"SANet：用于摄像机定位的场景无关网络"，在《国际计算机视觉会议》（ICCV）中，第
    42–51 页，2019 年。'
- en: '[139] M. Cai, H. Zhan, C. Saroj Weerasekera, K. Li, and I. Reid, “Camera relocalization
    by exploiting multi-view constraints for scene coordinates regression,” in The
    International Conference on Computer Vision (ICCV) Workshops, pp. 0–0, 2019.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] M. Cai, H. Zhan, C. Saroj Weerasekera, K. Li 和 I. Reid，"通过利用多视图约束进行场景坐标回归的摄像机重定位"，在《国际计算机视觉会议》（ICCV）研讨会中，第
    0–0 页，2019 年。'
- en: '[140] X. Li, S. Wang, Y. Zhao, J. Verbeek, and J. Kannala, “Hierarchical scene
    coordinate classification and regression for visual localization,” in IEEE/CVF
    International Conference on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] X. Li, S. Wang, Y. Zhao, J. Verbeek 和 J. Kannala，"用于视觉定位的层次化场景坐标分类和回归"，在
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR）中，2020 年。'
- en: '[141] L. Zhou, Z. Luo, T. Shen, J. Zhang, M. Zhen, Y. Yao, T. Fang, and L. Quan,
    “Kfnet: Learning temporal camera relocalization using kalman filtering,” IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] L. Zhou, Z. Luo, T. Shen, J. Zhang, M. Zhen, Y. Yao, T. Fang 和 L. Quan，"Kfnet：使用卡尔曼滤波学习时间摄像机重定位"，IEEE/CVF
    计算机视觉与模式识别会议（CVPR），2020 年。'
- en: '[142] S. Tang, C. Tang, R. Huang, S. Zhu, and P. Tan, “Learning camera localization
    via dense scene matching,” in Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 1831–1841, 2021.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] S. Tang, C. Tang, R. Huang, S. Zhu 和 P. Tan，"通过密集场景匹配学习摄像机定位"，在 IEEE/CVF
    计算机视觉与模式识别会议论文集中，第 1831–1841 页，2021 年。'
- en: '[143] K. Mikolajczyk and C. Schmid, “Scale & affine invariant interest point
    detectors,” International journal of computer vision, vol. 60, no. 1, pp. 63–86,
    2004.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] K. Mikolajczyk 和 C. Schmid，"尺度和仿射不变兴趣点检测器"，《国际计算机视觉杂志》，第 60 卷，第 1 期，第
    63–86 页，2004 年。'
- en: '[144] S. Leutenegger, M. Chli, and R. Y. Siegwart, “Brisk: Binary robust invariant
    scalable keypoints,” in The International Conference on Computer Vision (ICCV),
    pp. 2548–2555, Ieee, 2011.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] S. Leutenegger, M. Chli 和 R. Y. Siegwart，"Brisk：二进制鲁棒不变可扩展关键点"，在《国际计算机视觉会议》（ICCV）中，第
    2548–2555 页，IEEE，2011 年。'
- en: '[145] H. Bay, T. Tuytelaars, and L. Van Gool, “Surf: Speeded up robust features,”
    in The European Conference on Computer Vision (ECCV), pp. 404–417, Springer, 2006.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] H. Bay, T. Tuytelaars 和 L. Van Gool，"SURF：加速鲁棒特征"，在《欧洲计算机视觉会议》（ECCV）中，第
    404–417 页，Springer，2006 年。'
- en: '[146] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    International journal of computer vision, vol. 60, no. 2, pp. 91–110, 2004.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] D. G. Lowe，"来自尺度不变关键点的独特图像特征"，《国际计算机视觉杂志》，第 60 卷，第 2 期，第 91–110 页，2004
    年。'
- en: '[147] M. Calonder, V. Lepetit, C. Strecha, and P. Fua, “Brief: Binary robust
    independent elementary features,” in The European Conference on Computer Vision
    (ECCV), pp. 778–792, Springer, 2010.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] M. Calonder, V. Lepetit, C. Strecha, 和 P. Fua，“Brief：二进制鲁棒独立基本特征”，发表于
    欧洲计算机视觉会议（ECCV），第 778–792 页，Springer，2010 年。'
- en: '[148] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An efficient
    alternative to sift or surf,” in The International Conference on Computer Vision
    (ICCV), pp. 2564–2571, 2011.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] E. Rublee, V. Rabaud, K. Konolige, 和 G. Bradski，“Orb：对 SIFT 或 SURF 的高效替代”，发表于
    国际计算机视觉会议（ICCV），第 2564–2571 页，2011 年。'
- en: '[149] V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk, “Learning local feature
    descriptors with triplets and shallow convolutional neural networks.,” in British
    Machine Vision Conference (BMVC), vol. 1, p. 3, 2016.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] V. Balntas, E. Riba, D. Ponsa, 和 K. Mikolajczyk，“使用三元组和浅层卷积神经网络学习局部特征描述符”，发表于
    英国机器视觉会议（BMVC），卷 1，第 3 页，2016 年。'
- en: '[150] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer,
    “Discriminative learning of deep convolutional feature point descriptors,” in
    The International Conference on Computer Vision (ICCV), pp. 118–126, 2015.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, 和 F. Moreno-Noguer，“深度卷积特征点描述符的辨别学习”，发表于
    国际计算机视觉会议（ICCV），第 118–126 页，2015 年。'
- en: '[151] K. Simonyan, A. Vedaldi, and A. Zisserman, “Learning local feature descriptors
    using convex optimisation,” IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 36, no. 8, pp. 1573–1585, 2014.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] K. Simonyan, A. Vedaldi, 和 A. Zisserman，“使用凸优化学习局部特征描述符”，发表于 IEEE 模式分析与机器智能学报，卷
    36，第 8 期，第 1573–1585 页，2014 年。'
- en: '[152] K. Moo Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua, “Learning
    to find good correspondences,” in IEEE/CVF International Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 2666–2674, 2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] K. Moo Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, 和 P. Fua，“学习寻找良好对应关系”，发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第 2666–2674 页，2018 年。'
- en: '[153] P. Ebel, A. Mishchuk, K. M. Yi, P. Fua, and E. Trulls, “Beyond cartesian
    representations for local descriptors,” in The International Conference on Computer
    Vision (ICCV), pp. 253–262, 2019.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] P. Ebel, A. Mishchuk, K. M. Yi, P. Fua, 和 E. Trulls，“超越笛卡尔表示的局部描述符”，发表于
    国际计算机视觉会议（ICCV），第 253–262 页，2019 年。'
- en: '[154] M. Larsson, E. Stenborg, C. Toft, L. Hammarstrand, T. Sattler, and F. Kahl,
    “Fine-grained segmentation networks: Self-supervised segmentation for improved
    long-term visual localization,” in The International Conference on Computer Vision
    (ICCV), pp. 31–41, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] M. Larsson, E. Stenborg, C. Toft, L. Hammarstrand, T. Sattler, 和 F. Kahl，“细粒度分割网络：自监督分割以改善长期视觉定位”，发表于
    国际计算机视觉会议（ICCV），第 31–41 页，2019 年。'
- en: '[155] R. Pautrat, V. Larsson, M. R. Oswald, and M. Pollefeys, “Online invariance
    selection for local feature descriptors,” in European Conference on Computer Vision,
    pp. 707–724, Springer, 2020.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] R. Pautrat, V. Larsson, M. R. Oswald, 和 M. Pollefeys，“局部特征描述符的在线不变性选择”，发表于
    欧洲计算机视觉会议，第 707–724 页，Springer，2020 年。'
- en: '[156] Q. Wang, X. Zhou, B. Hariharan, and N. Snavely, “Learning feature descriptors
    using camera pose supervision,” in European Conference on Computer Vision, pp. 757–774,
    Springer, 2020.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Q. Wang, X. Zhou, B. Hariharan, 和 N. Snavely，“使用相机姿态监督学习特征描述符”，发表于 欧洲计算机视觉会议，第
    757–774 页，Springer，2020 年。'
- en: '[157] Y. Tian, A. Barroso Laguna, T. Ng, V. Balntas, and K. Mikolajczyk, “Hynet:
    Learning local descriptor with hybrid similarity measure and triplet loss,” Advances
    in Neural Information Processing Systems, vol. 33, pp. 7401–7412, 2020.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Y. Tian, A. Barroso Laguna, T. Ng, V. Balntas, 和 K. Mikolajczyk，“Hynet：通过混合相似度测量和三元组损失学习局部描述符”，发表于神经信息处理系统进展，卷
    33，第 7401–7412 页，2020 年。'
- en: '[158] N. Savinov, A. Seki, L. Ladicky, T. Sattler, and M. Pollefeys, “Quad-networks:
    unsupervised learning to rank for interest point detection,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1822–1830, 2017.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] N. Savinov, A. Seki, L. Ladicky, T. Sattler, 和 M. Pollefeys，“Quad-networks：用于兴趣点检测的无监督排名学习”，发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第 1822–1830 页，2017 年。'
- en: '[159] L. Zhang and S. Rusinkiewicz, “Learning to detect features in texture
    images,” in IEEE/CVF International Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 6325–6333, 2018.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] L. Zhang 和 S. Rusinkiewicz，“学习检测纹理图像中的特征”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第
    6325–6333 页，2018 年。'
- en: '[160] A. B. Laguna, E. Riba, D. Ponsa, and K. Mikolajczyk, “Key. net: Keypoint
    detection by handcrafted and learned cnn filters,” The IEEE/CVF International
    Conference on Computer Vision (ICCV), 2019.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. B. Laguna, E. Riba, D. Ponsa 和 K. Mikolajczyk，“Key.net：通过手工制作和学习的
    CNN 滤波器进行关键点检测，”发表于 IEEE/CVF 国际计算机视觉会议（ICCV），2019 年。'
- en: '[161] Y. Ono, E. Trulls, P. Fua, and K. M. Yi, “Lf-net: learning local features
    from images,” in Neural Information Processing Systems (NeurIPS), pp. 6234–6244,
    2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Y. Ono, E. Trulls, P. Fua 和 K. M. Yi，“Lf-net：从图像中学习局部特征，”发表于神经信息处理系统会议（NeurIPS），第
    6234–6244 页，2018 年。'
- en: '[162] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua, “Lift: Learned invariant
    feature transform,” in The European Conference on Computer Vision (ECCV), pp. 467–483,
    Springer, 2016.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] K. M. Yi, E. Trulls, V. Lepetit 和 P. Fua，“Lift：学习的不变特征变换，”发表于欧洲计算机视觉会议（ECCV），第
    467–483 页，Springer，2016 年。'
- en: '[163] Y. Zhou, G. Wan, S. Hou, L. Yu, G. Wang, X. Rui, and S. Song, “Da4ad:
    End-to-end deep attention-based visual localization for autonomous driving,” in
    European Conference on Computer Vision, pp. 271–289, Springer, 2020.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Y. Zhou, G. Wan, S. Hou, L. Yu, G. Wang, X. Rui 和 S. Song，“Da4ad：端到端深度注意力视觉定位用于自动驾驶，”发表于欧洲计算机视觉会议，第
    271–289 页，Springer，2020 年。'
- en: '[164] F. Lu, G. Chen, Y. Liu, Z. Qu, and A. Knoll, “Rskdd-net: Random sample-based
    keypoint detector and descriptor,” Advances in Neural Information Processing Systems,
    vol. 33, pp. 21297–21308, 2020.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] F. Lu, G. Chen, Y. Liu, Z. Qu 和 A. Knoll，“Rskdd-net：基于随机样本的关键点检测器和描述符，”发表于神经信息处理系统进展，卷
    33，第 21297–21308 页，2020 年。'
- en: '[165] C. G. Harris, M. Stephens, et al., “A combined corner and edge detector.,”
    in Alvey vision conference, vol. 15, pp. 10–5244, Citeseer, 1988.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] C. G. Harris, M. Stephens 等人，“一种结合角点和边缘检测器，”发表于 Alvey 视觉会议，卷 15，第 10–5244
    页，Citeseer，1988 年。'
- en: '[166] B. Wang, C. Chen, Z. Cui, J. Qin, C. X. Lu, Z. Yu, P. Zhao, Z. Dong,
    F. Zhu, N. Trigoni, et al., “P2-net: Joint description and detection of local
    features for pixel and point matching,” IEEE/CVF International Conference on Computer
    Vision (ICCV), 2021.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] B. Wang, C. Chen, Z. Cui, J. Qin, C. X. Lu, Z. Yu, P. Zhao, Z. Dong,
    F. Zhu, N. Trigoni 等人，“P2-net：用于像素和点匹配的局部特征联合描述和检测，”发表于 IEEE/CVF 国际计算机视觉会议（ICCV），2021
    年。'
- en: '[167] Y. Tian, V. Balntas, T. Ng, A. Barroso-Laguna, Y. Demiris, and K. Mikolajczyk,
    “D2d: Keypoint extraction with describe to detect approach,” The Asian Conference
    on Computer Vision (ACCV), 2020.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Y. Tian, V. Balntas, T. Ng, A. Barroso-Laguna, Y. Demiris 和 K. Mikolajczyk，“D2d：用描述来检测的方法进行关键点提取，”发表于亚洲计算机视觉会议（ACCV），2020
    年。'
- en: '[168] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker, “Universal correspondence
    network,” in Neural Information Processing Systems (NeurIPS), pp. 2414–2422, 2016.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] C. B. Choy, J. Gwak, S. Savarese 和 M. Chandraker，“通用对应网络，”发表于神经信息处理系统会议（NeurIPS），第
    2414–2422 页，2016 年。'
- en: '[169] M. E. Fathy, Q.-H. Tran, M. Zeeshan Zia, P. Vernaza, and M. Chandraker,
    “Hierarchical metric learning and matching for 2d and 3d geometric correspondences,”
    in The European Conference on Computer Vision (ECCV), pp. 803–819, 2018.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] M. E. Fathy, Q.-H. Tran, M. Zeeshan Zia, P. Vernaza 和 M. Chandraker，“用于
    2d 和 3d 几何对应的层次度量学习和匹配，”发表于欧洲计算机视觉会议（ECCV），第 803–819 页，2018 年。'
- en: '[170] N. Savinov, L. Ladicky, and M. Pollefeys, “Matching neural paths: transfer
    from recognition to correspondence search,” in Neural Information Processing Systems
    (NeurIPS), pp. 1205–1214, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] N. Savinov, L. Ladicky 和 M. Pollefeys，“匹配神经路径：从识别到对应搜索的转移，”发表于神经信息处理系统会议（NeurIPS），第
    1205–1214 页，2017 年。'
- en: '[171] J. Hyeon, J. Kim, and N. Doh, “Pose correction for highly accurate visual
    localization in large-scale indoor spaces,” in Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 15974–15983, 2021.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] J. Hyeon, J. Kim 和 N. Doh，“大规模室内空间中的高度准确视觉定位的姿态校正，”发表于 IEEE/CVF 国际计算机视觉会议论文集，第
    15974–15983 页，2021 年。'
- en: '[172] G. Berton, C. Masone, V. Paolicelli, and B. Caputo, “Viewpoint invariant
    dense matching for visual geolocalization,” in Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 12169–12178, 2021.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] G. Berton, C. Masone, V. Paolicelli 和 B. Caputo，“视点不变的密集匹配用于视觉地理定位，”发表于
    IEEE/CVF 国际计算机视觉会议论文集，第 12169–12178 页，2021 年。'
- en: '[173] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
    D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al., “Benchmarking 6dof outdoor
    visual localization in changing conditions,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 8601–8610, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
    D. Safari, M. Okutomi, M. Pollefeys, J. Sivic 等人，“在变化条件下的 6dof 户外视觉定位基准测试，”发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第 8601–8610 页，2018 年。'
- en: '[174] S. Dong, Q. Fan, H. Wang, J. Shi, L. Yi, T. Funkhouser, B. Chen, and
    L. J. Guibas, “Robust neural routing through space partitions for camera relocalization
    in dynamic indoor environments,” in Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 8544–8554, 2021.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] S. Dong, Q. Fan, H. Wang, J. Shi, L. Yi, T. Funkhouser, B. Chen, 和 L.
    J. Guibas，“通过空间分割进行鲁棒的神经路由，用于动态室内环境中的相机重定位，”发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第 8544–8554
    页，2021 年。'
- en: '[175] E. Brachmann and C. Rother, “Visual camera re-localization from rgb and
    rgb-d images using dsac,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
    2021.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] E. Brachmann 和 C. Rother，“使用 DSAC 从 RGB 和 RGB-D 图像中进行视觉相机重定位，”IEEE 模式分析与机器智能汇刊，2021
    年。'
- en: '[176] S. Wang, Z. Laskar, I. Melekhov, X. Li, and J. Kannala, “Continual learning
    for image-based camera localization,” in Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 3252–3262, 2021.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] S. Wang, Z. Laskar, I. Melekhov, X. Li, 和 J. Kannala，“基于图像的相机定位的持续学习，”发表于
    IEEE/CVF 国际计算机视觉会议论文集，第 3252–3262 页，2021 年。'
- en: '[177] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison, “DTAM : Dense Tracking
    and Mapping in Real-Time,” in The International Conference on Computer Vision
    (ICCV), pp. 2320–2327, 2011.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] R. A. Newcombe, S. J. Lovegrove, 和 A. J. Davison，“DTAM : 实时密集跟踪与地图构建，”发表于
    国际计算机视觉会议 (ICCV)，第 2320–2327 页，2011 年。'
- en: '[178] C. Kerl, J. Sturm, and D. Cremers, “Dense visual slam for rgb-d cameras,”
    in The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
    pp. 2100–2106, IEEE, 2013.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] C. Kerl, J. Sturm, 和 D. Cremers，“针对 RGB-D 相机的密集视觉 SLAM，”发表于 IEEE/RSJ
    国际智能机器人与系统会议 (IROS)，第 2100–2106 页，IEEE，2013 年。'
- en: '[179] T. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, and J. McDonald,
    “Real-time large-scale dense rgb-d slam with volumetric fusion,” The International
    Journal of Robotics Research, vol. 34, no. 4-5, pp. 598–626, 2015.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] T. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, 和 J. McDonald，“实时大规模密集
    RGB-D SLAM 与体积融合，”《国际机器人研究杂志》，第 34 卷，第 4-5 期，第 598–626 页，2015 年。'
- en: '[180] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single
    image using a multi-scale deep network,” in Neural Information Processing Systems
    (NeurIPS), pp. 2366–2374, 2014.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] D. Eigen, C. Puhrsch, 和 R. Fergus，“使用多尺度深度网络从单幅图像中预测深度图，”发表于 神经信息处理系统会议
    (NeurIPS)，第 2366–2374 页，2014 年。'
- en: '[181] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and
    T. Brox, “Demon: Depth and motion network for learning monocular stereo,” in IEEE/CVF
    International Conference on Computer Vision and Pattern Recognition, pp. 5038–5047,
    2017.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, 和
    T. Brox，“Demon: 深度与运动网络用于学习单目立体视觉，”发表于 IEEE/CVF 国际计算机视觉与模式识别会议，第 5038–5047 页，2017
    年。'
- en: '[182] F. Liu, C. Shen, G. Lin, and I. Reid, “Learning depth from single monocular
    images using deep convolutional neural fields,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 38, no. 10, pp. 2024–2039, 2015.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] F. Liu, C. Shen, G. Lin, 和 I. Reid，“使用深度卷积神经场从单幅单目图像中学习深度，”IEEE 模式分析与机器智能汇刊，第
    38 卷，第 10 期，第 2024–2039 页，2015 年。'
- en: '[183] K. Karsch, C. Liu, and S. B. Kang, “Depth transfer: Depth extraction
    from video using non-parametric sampling,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 36, no. 11, pp. 2144–2158, 2014.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] K. Karsch, C. Liu, 和 S. B. Kang，“深度传输：使用非参数采样从视频中提取深度，”IEEE 模式分析与机器智能汇刊，第
    36 卷，第 11 期，第 2144–2158 页，2014 年。'
- en: '[184] R. Garg, V. K. BG, G. Carneiro, and I. Reid, “Unsupervised cnn for single
    view depth estimation: Geometry to the rescue,” in The European Conference on
    Computer Vision (ECCV), pp. 740–756, Springer, 2016.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] R. Garg, V. K. BG, G. Carneiro, 和 I. Reid，“无监督 CNN 用于单视图深度估计：几何学的救援，”发表于
    欧洲计算机视觉会议 (ECCV)，第 740–756 页，Springer，2016 年。'
- en: '[185] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time dense
    monocular slam with learned depth prediction,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 6243–6252, 2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] K. Tateno, F. Tombari, I. Laina, 和 N. Navab，“Cnn-slam: 实时密集单目 SLAM 与学习的深度预测，”发表于
    IEEE/CVF 国际计算机视觉与模式识别会议 (CVPR)，第 6243–6252 页，2017 年。'
- en: '[186] J. Engel, T. Schöps, and D. Cremers, “Lsd-slam: Large-scale direct monocular
    slam,” in The European Conference on Computer Vision (ECCV), pp. 834–849, Springer,
    2014.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] J. Engel, T. Schöps, 和 D. Cremers，“LSD-SLAM: 大规模直接单目 SLAM，”发表于 欧洲计算机视觉会议
    (ECCV)，第 834–849 页，Springer，2014 年。'
- en: '[187] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 652–660, 2017.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas， “Pointnet：深度学习点集用于 3D 分类和分割”，发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第 652–660 页，2017年。'
- en: '[188] H. Fan, H. Su, and L. J. Guibas, “A point set generation network for
    3d object reconstruction from a single image,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 605–613, 2017.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] H. Fan, H. Su, 和 L. J. Guibas， “一种点集生成网络用于从单幅图像重建 3D 物体”，发表于 IEEE/CVF
    国际计算机视觉与模式识别会议（CVPR），第 605–613 页，2017年。'
- en: '[189] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, “A papier-mâché
    approach to learning 3d surface generation,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 216–224, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, 和 M. Aubry， “一种纸浆方法用于学习
    3D 表面生成”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第 216–224 页，2018年。'
- en: '[190] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang, “Pixel2mesh:
    Generating 3d mesh models from single rgb images,” in The European Conference
    on Computer Vision (ECCV), pp. 52–67, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, 和 Y.-G. Jiang， “Pixel2mesh：从单幅
    RGB 图像生成 3D 网格模型”，发表于 欧洲计算机视觉会议（ECCV），第 52–67 页，2018年。'
- en: '[191] L. Ladicky, O. Saurer, S. Jeong, F. Maninchedda, and M. Pollefeys, “From
    point clouds to mesh using regression,” in The International Conference on Computer
    Vision (ICCV), pp. 3893–3902, 2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] L. Ladicky, O. Saurer, S. Jeong, F. Maninchedda, 和 M. Pollefeys， “从点云到网格的回归方法”，发表于
    国际计算机视觉会议（ICCV），第 3893–3902 页，2017年。'
- en: '[192] A. Dai and M. Nießner, “Scan2mesh: From unstructured range scans to 3d
    meshes,” in IEEE/CVF International Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 5574–5583, 2019.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] A. Dai 和 M. Nießner， “Scan2mesh：从非结构化范围扫描到 3D 网格”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第
    5574–5583 页，2019年。'
- en: '[193] S. Peng, C. Jiang, Y. Liao, M. Niemeyer, M. Pollefeys, and A. Geiger,
    “Shape as points: A differentiable poisson solver,” Advances in Neural Information
    Processing Systems, vol. 34, 2021.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] S. Peng, C. Jiang, Y. Liao, M. Niemeyer, M. Pollefeys, 和 A. Geiger， “形状作为点：一种可微的泊松求解器”，发表于
    神经信息处理系统进展，卷 34，2021年。'
- en: '[194] T. Mukasa, J. Xu, and B. Stenger, “3d scene mesh from cnn depth predictions
    and sparse monocular slam,” in The International Conference on Computer Vision
    (ICCV) Workshops, pp. 921–928, 2017.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] T. Mukasa, J. Xu, 和 B. Stenger， “从 CNN 深度预测和稀疏单目 SLAM 生成 3D 场景网格”，发表于
    国际计算机视觉会议（ICCV）研讨会，第 921–928 页，2017年。'
- en: '[195] M. Bloesch, T. Laidlow, R. Clark, S. Leutenegger, and A. J. Davison,
    “Learning meshes for dense visual slam,” in IEEE/CVF International Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 5855–5864, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] M. Bloesch, T. Laidlow, R. Clark, S. Leutenegger, 和 A. J. Davison， “用于密集视觉
    SLAM 的网格学习”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第 5855–5864 页，2019年。'
- en: '[196] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, “Deepsdf:
    Learning continuous signed distance functions for shape representation,” in IEEE/CVF
    International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 165–174,
    2019.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] J. J. Park, P. Florence, J. Straub, R. Newcombe, 和 S. Lovegrove， “Deepsdf：学习连续符号距离函数用于形状表示”，发表于
    IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第 165–174 页，2019年。'
- en: '[197] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, “Occupancy
    networks: Learning 3d reconstruction in function space,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4460–4470, 2019.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, 和 A. Geiger， “占用网络：在函数空间中学习
    3D 重建”，发表于 IEEE/CVF 国际计算机视觉与模式识别会议（CVPR），第 4460–4470 页，2019年。'
- en: '[198] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger, “Convolutional
    occupancy networks,” in European Conference on Computer Vision, pp. 523–540, Springer,
    2020.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, 和 A. Geiger， “卷积占用网络”，发表于
    欧洲计算机视觉会议， 第 523–540 页，Springer，2020年。'
- en: '[199] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    in The European Conference on Computer Vision (ECCV), pp. 405–421, Springer, 2020.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    和 R. Ng， “Nerf：将场景表示为神经辐射场用于视图合成”，发表于 欧洲计算机视觉会议（ECCV），第 405–421 页，Springer，2020年。'
- en: '[200] J. Chibane, G. Pons-Moll, et al., “Neural unsigned distance fields for
    implicit function learning,” Advances in Neural Information Processing Systems,
    vol. 33, pp. 21638–21652, 2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] J. Chibane, G. Pons-Moll 等, “神经无符号距离场用于隐式函数学习，” 《神经信息处理系统进展》，第 33 卷，第
    21638–21652 页，2020 年。'
- en: '[201] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, “Surfacenet: An end-to-end
    3d neural network for multiview stereopsis,” in The International Conference on
    Computer Vision (ICCV), pp. 2307–2315, 2017.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] M. Ji, J. Gall, H. Zheng, Y. Liu, 和 L. Fang, “Surfacenet：用于多视角立体视觉的端到端
    3D 神经网络，” 发表在国际计算机视觉大会（ICCV），第 2307–2315 页，2017 年。'
- en: '[202] D. Paschalidou, O. Ulusoy, C. Schmitt, L. Van Gool, and A. Geiger, “Raynet:
    Learning volumetric 3d reconstruction with ray potentials,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3897–3906, 2018.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] D. Paschalidou, O. Ulusoy, C. Schmitt, L. Van Gool, 和 A. Geiger, “Raynet：通过光线潜力学习体积
    3D 重建，” 发表在 IEEE/CVF 国际计算机视觉与模式识别大会（CVPR），第 3897–3906 页，2018 年。'
- en: '[203] A. Kar, C. Häne, and J. Malik, “Learning a multi-view stereo machine,”
    in Neural Information Processing Systems, pp. 365–376, 2017.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A. Kar, C. Häne, 和 J. Malik, “学习多视角立体机器，” 发表在神经信息处理系统，第 365–376 页，2017
    年。'
- en: '[204] C. Häne, S. Tulsiani, and J. Malik, “Hierarchical surface prediction
    for 3d object reconstruction,” in International Conference on 3D Vision (3DV),
    pp. 412–420, 2017.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] C. Häne, S. Tulsiani, 和 J. Malik, “用于 3D 物体重建的分层表面预测，” 发表在国际 3D 视觉大会（3DV），第
    412–420 页，2017 年。'
- en: '[205] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Octree generating networks:
    Efficient convolutional architectures for high-resolution 3d outputs,” in The
    International Conference on Computer Vision (ICCV), pp. 2088–2096, 2017.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox, “八叉树生成网络：高分辨率 3D 输出的高效卷积架构，”
    发表在国际计算机视觉大会（ICCV），第 2088–2096 页，2017 年。'
- en: '[206] A. Dai, C. Ruizhongtai Qi, and M. Nießner, “Shape completion using 3d-encoder-predictor
    cnns and shape synthesis,” in IEEE/CVF International Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 5868–5877, 2017.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] A. Dai, C. Ruizhongtai Qi, 和 M. Nießner, “使用 3D 编码器-预测器 CNNs 和形状合成的形状完成，”
    发表在 IEEE/CVF 国际计算机视觉与模式识别大会（CVPR），第 5868–5877 页，2017 年。'
- en: '[207] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger, “Octnetfusion: Learning
    depth fusion from data,” in 2017 International Conference on 3D Vision (3DV),
    pp. 57–66, IEEE, 2017.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] G. Riegler, A. O. Ulusoy, H. Bischof, 和 A. Geiger, “Octnetfusion：从数据中学习深度融合，”
    发表在 2017 年国际 3D 视觉大会（3DV），第 57–66 页，IEEE，2017 年。'
- en: '[208] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollár, “Panoptic
    segmentation,” in IEEE/CVF International Conference on Computer Vision and Pattern
    Recognition (CVPR), pp. 9404–9413, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] A. Kirillov, K. He, R. Girshick, C. Rother, 和 P. Dollár, “全景分割，” 发表在
    IEEE/CVF 国际计算机视觉与模式识别大会（CVPR），第 9404–9413 页，2019 年。'
- en: '[209] J. McCormac, A. Handa, A. Davison, and S. Leutenegger, “Semanticfusion:
    Dense 3d semantic mapping with convolutional neural networks,” in The IEEE International
    Conference on Robotics and Automation (ICRA), pp. 4628–4635, IEEE, 2017.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] J. McCormac, A. Handa, A. Davison, 和 S. Leutenegger, “Semanticfusion：基于卷积神经网络的密集
    3D 语义映射，” 发表在 IEEE 国际机器人与自动化大会（ICRA），第 4628–4635 页，IEEE，2017 年。'
- en: '[210] L. Ma, J. Stückler, C. Kerl, and D. Cremers, “Multi-view deep learning
    for consistent semantic mapping with rgb-d cameras,” in The IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 598–605, IEEE, 2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] L. Ma, J. Stückler, C. Kerl, 和 D. Cremers, “用于一致语义映射的多视角深度学习与 rgb-d 相机，”
    发表在 IEEE/RSJ 国际智能机器人与系统大会（IROS），第 598–605 页，IEEE，2017 年。'
- en: '[211] Y. Xiang and D. Fox, “Da-rnn: Semantic mapping with data associated recurrent
    neural networks,” Robotics: Science and Systems, 2017.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Y. Xiang 和 D. Fox, “Da-rnn：基于数据关联的递归神经网络进行语义映射，” 《机器人学：科学与系统》，2017 年。'
- en: '[212] T. Qin, Y. Zheng, T. Chen, Y. Chen, and Q. Su, “A light-weight semantic
    map for visual localization towards autonomous driving,” in 2021 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 11248–11254, IEEE, 2021.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] T. Qin, Y. Zheng, T. Chen, Y. Chen, 和 Q. Su, “面向自动驾驶的轻量级语义地图用于视觉定位，”
    发表在 2021 IEEE 国际机器人与自动化大会（ICRA），第 11248–11254 页，IEEE，2021 年。'
- en: '[213] N. Sünderhauf, T. T. Pham, Y. Latif, M. Milford, and I. Reid, “Meaningful
    maps with object-oriented semantic mapping,” in The IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS), pp. 5079–5085, IEEE, 2017.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] N. Sünderhauf, T. T. Pham, Y. Latif, M. Milford, 和 I. Reid, “基于面向对象的语义映射的有意义地图，”
    发表在 IEEE/RSJ 国际智能机器人与系统大会（IROS），第 5079–5085 页，IEEE，2017 年。'
- en: '[214] M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Siegwart,
    and J. Nieto, “Volumetric instance-aware semantic mapping and 3d object discovery,”
    IEEE Robotics and Automation Letters, vol. 4, no. 3, pp. 3037–3044, 2019.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Siegwart,
    和 J. Nieto， “体积实例感知语义映射和3D对象发现”，IEEE机器人与自动化快报，卷4，第3期，页3037–3044，2019。'
- en: '[215] J. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger, “Fusion++:
    Volumetric object-level slam,” in International Conference on 3D Vision (3DV),
    pp. 32–41, IEEE, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] J. McCormac, R. Clark, M. Bloesch, A. Davison, 和 S. Leutenegger， “Fusion++:
    体积对象级SLAM”，在国际3D视觉会议（3DV），页32–41，IEEE，2018。'
- en: '[216] K. Doherty, D. Fourie, and J. Leonard, “Multimodal semantic slam with
    probabilistic data association,” in 2019 international conference on robotics
    and automation (ICRA), pp. 2419–2425, IEEE, 2019.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] K. Doherty, D. Fourie, 和 J. Leonard， “具有概率数据关联的多模态语义SLAM”，在2019年国际机器人与自动化会议（ICRA），页2419–2425，IEEE，2019。'
- en: '[217] G. Narita, T. Seno, T. Ishikawa, and Y. Kaji, “Panopticfusion: Online
    volumetric semantic mapping at the level of stuff and things,” The IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), 2019.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] G. Narita, T. Seno, T. Ishikawa, 和 Y. Kaji， “Panopticfusion: 级别为物品和事物的在线体积语义映射”，IEEE/RSJ国际智能机器人与系统会议（IROS），2019。'
- en: '[218] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison,
    “CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM,”
    in IEEE/CVF International Conference on Computer Vision and Pattern Recognition
    (CVPR), 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, 和 A. J. Davison，
    “CodeSLAM — 学习一种紧凑、可优化的密集视觉SLAM表示”，在IEEE/CVF国际计算机视觉与模式识别会议（CVPR），2018。'
- en: '[219] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo,
    A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor, et al., “Neural scene representation
    and rendering,” Science, vol. 360, no. 6394, pp. 1204–1210, 2018.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo,
    A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor, 等等，“神经场景表示与渲染”，科学，卷360，第6394期，页1204–1210，2018。'
- en: '[220] J. Tobin, W. Zaremba, and P. Abbeel, “Geometry-aware neural rendering,”
    in Neural Information Processing System (NeurIPS), pp. 11555–11565, 2019.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] J. Tobin, W. Zaremba, 和 P. Abbeel， “几何感知神经渲染”，在神经信息处理系统会议（NeurIPS），页11555–11565，2019。'
- en: '[221] J. H. Lim, P. O. Pinheiro, N. Rostamzadeh, C. Pal, and S. Ahn, “Neural
    multisensory scene inference,” in Neural Information Processing Systems (NeurIPS),
    pp. 8994–9004, 2019.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] J. H. Lim, P. O. Pinheiro, N. Rostamzadeh, C. Pal, 和 S. Ahn， “神经多感官场景推断”，在神经信息处理系统会议（NeurIPS），页8994–9004，2019。'
- en: '[222] A. Trevithick and B. Yang, “Grf: Learning a general radiance field for
    3d scene representation and rendering,” 2020.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] A. Trevithick 和 B. Yang， “Grf: 学习用于3D场景表示与渲染的通用辐射场”，2020。'
- en: '[223] K. Schwarz, Y. Liao, M. Niemeyer, and A. Geiger, “Graf: Generative radiance
    fields for 3d-aware image synthesis,” Neural Information Processing Systems (NeurIPS),
    2020.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] K. Schwarz, Y. Liao, M. Niemeyer, 和 A. Geiger， “Graf: 用于3D感知图像合成的生成辐射场”，神经信息处理系统会议（NeurIPS），2020。'
- en: '[224] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, and A. Kanazawa, “Plenoctrees
    for real-time rendering of neural radiance fields,” IEEE/CVF International Conference
    on Computer Vision (ICCV), 2021.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, 和 A. Kanazawa， “Plenoctrees用于神经辐射场的实时渲染”，IEEE/CVF国际计算机视觉会议（ICCV），2021。'
- en: '[225] D. Lindell, J. Martel, and G. Wetzstein, “AutoInt: Automatic integration
    for fast neural volume rendering,” IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR), 2020.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] D. Lindell, J. Martel, 和 G. Wetzstein， “AutoInt: 用于快速神经体积渲染的自动集成”，IEEE/CVF计算机视觉与模式识别会议（CVPR），2020。'
- en: '[226] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller, C. R. A. Chaitanya,
    A. S. Kaplanyan, and M. Steinberger, “DONeRF: Towards Real-Time Rendering of Compact
    Neural Radiance Fields using Depth Oracle Networks,” Computer Graphics Forum,
    vol. 40, no. 4, 2021.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller, C. R. A. Chaitanya,
    A. S. Kaplanyan, 和 M. Steinberger， “DONeRF: 迈向使用深度预言网络的紧凑神经辐射场实时渲染”，计算机图形论坛，卷40，第4期，2021。'
- en: '[227] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang, “NeuS:
    Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction,”
    Neural Information Processing Systems (NeurIPS), 2021.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, 和 W. Wang， “NeuS: 通过体积渲染学习神经隐式表面以进行多视角重建”，神经信息处理系统会议（NeurIPS），2021。'
- en: '[228] S. Zhi, T. Laidlow, S. Leutenegger, and A. J. Davison, “In-place scene
    labelling and understanding with implicit scene representation,” in Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 15838–15847,
    2021.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] S. Zhi, T. Laidlow, S. Leutenegger, 和 A. J. Davison，“使用隐式场景表示进行现场场景标记和理解，”
    在 IEEE/CVF 国际计算机视觉会议论文集，第15838–15847页，2021年。'
- en: '[229] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, “imap: Implicit mapping
    and positioning in real-time,” in IEEE/CVF International Conference on Computer
    Vision (ICCV), pp. 6229–6238, 2021.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] E. Sucar, S. Liu, J. Ortiz, 和 A. J. Davison，“imap：实时隐式映射与定位，” 在 IEEE/CVF
    国际计算机视觉会议（ICCV），第6229–6238页，2021年。'
- en: '[230] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and
    M. Pollefeys, “Nice-slam: Neural implicit scalable encoding for slam,” in Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12786–12796,
    2022.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, 和 M.
    Pollefeys，“Nice-slam：用于SLAM的神经隐式可扩展编码，” 在 IEEE/CVF 计算机视觉与模式识别会议论文集，第12786–12796页，2022年。'
- en: '[231] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
    M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al., “Learning to navigate
    in complex environments,” International Conference on Learning Representations
    (ICLR), 2017.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
    M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, 等，“在复杂环境中学习导航，” 国际学习表示会议（ICLR），2017年。'
- en: '[232] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in The IEEE International Conference on Robotics and Automation (ICRA), pp. 3357–3364,
    IEEE, 2017.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, 和 A.
    Farhadi，“使用深度强化学习进行室内场景的目标驱动视觉导航，” 在 IEEE 国际机器人与自动化会议（ICRA），第3357–3364页，IEEE，2017年。'
- en: '[233] P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann, K. Anderson, D. Teplyashin,
    K. Simonyan, A. Zisserman, R. Hadsell, et al., “Learning to navigate in cities
    without a map,” in Neural Information Processing Systems (NeurIPS), pp. 2419–2430,
    2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann, K. Anderson, D.
    Teplyashin, K. Simonyan, A. Zisserman, R. Hadsell, 等，“在没有地图的情况下学习在城市中导航，” 在神经信息处理系统（NeurIPS），第2419–2430页，2018年。'
- en: '[234] H. Li, Q. Zhang, and D. Zhao, “Deep reinforcement learning-based automatic
    exploration for navigation in unknown environment,” IEEE transactions on neural
    networks and learning systems, vol. 31, no. 6, pp. 2064–2076, 2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] H. Li, Q. Zhang, 和 D. Zhao，“基于深度强化学习的自动探索以进行未知环境导航，” IEEE 神经网络与学习系统交易，第31卷，第6期，第2064–2076页，2019年。'
- en: '[235] A. Banino, C. Barry, B. Uria, C. Blundell, T. Lillicrap, P. Mirowski,
    A. Pritzel, M. J. Chadwick, T. Degris, J. Modayil, et al., “Vector-based navigation
    using grid-like representations in artificial agents,” Nature, vol. 557, no. 7705,
    pp. 429–433, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] A. Banino, C. Barry, B. Uria, C. Blundell, T. Lillicrap, P. Mirowski,
    A. Pritzel, M. J. Chadwick, T. Degris, J. Modayil, 等，“使用网格状表示的基于向量的导航在人工智能代理中，”
    《自然》, 第557卷，第7705期，第429–433页，2018年。'
- en: '[236] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    and M. Milford, “Place recognition with convnet landmarks: Viewpoint-robust, condition-robust,
    training-free,” Proceedings of Robotics: Science and Systems XII, 2015.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    和 M. Milford，“使用卷积网络地标进行位置识别：视角鲁棒、条件鲁棒、无需训练，” 机器人科学与系统第十二届会议论文集，2015年。'
- en: '[237] X. Gao and T. Zhang, “Unsupervised learning to detect loops using deep
    neural networks for visual slam system,” Autonomous robots, vol. 41, no. 1, pp. 1–18,
    2017.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] X. Gao 和 T. Zhang，“使用深度神经网络进行回环检测的无监督学习用于视觉SLAM系统，” 自主机器人，第41卷，第1期，第1–18页，2017年。'
- en: '[238] N. Merrill and G. Huang, “Lightweight unsupervised deep loop closure,”
    Robotics: Science and Systems, 2018.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] N. Merrill 和 G. Huang，“轻量级无监督深度回环闭合，” 机器人科学与系统，2018年。'
- en: '[239] A. R. Memon, H. Wang, and A. Hussain, “Loop closure detection using supervised
    and unsupervised deep neural networks for monocular slam systems,” Robotics and
    Autonomous Systems, vol. 126, p. 103470, 2020.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] A. R. Memon, H. Wang, 和 A. Hussain，“使用监督和无监督深度神经网络进行回环闭合检测用于单目SLAM系统，”
    机器人与自主系统，第126卷，第103470页，2020年。'
- en: '[240] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, “Bundle
    adjustment—a modern synthesis,” in International workshop on vision algorithms,
    pp. 298–372, Springer, 1999.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] B. Triggs, P. F. McLauchlan, R. I. Hartley, 和 A. W. Fitzgibbon，“束调整——现代综合，”
    在国际视觉算法研讨会，第298–372页，Springer，1999年。'
- en: '[241] J. Nocedal and S. Wright, Numerical optimization. Springer Science &
    Business Media, 2006.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] J. Nocedal 和 S. Wright, 《数值优化》。Springer Science & Business Media，2006年。'
- en: '[242] R. Clark, M. Bloesch, J. Czarnowski, S. Leutenegger, and A. J. Davison,
    “Learning to solve nonlinear least squares for monocular stereo,” in The European
    Conference on Computer Vision (ECCV), pp. 284–299, 2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] R. Clark, M. Bloesch, J. Czarnowski, S. Leutenegger 和 A. J. Davison，“学习解决单目立体视觉的非线性最小二乘问题，”
    在欧洲计算机视觉会议（ECCV），pp. 284–299，2018年。'
- en: '[243] C. Tang and P. Tan, “Ba-net: Dense bundle adjustment network,” International
    Conference on Learning Representations, 2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] C. Tang 和 P. Tan，“Ba-net: 密集束调整网络，” 国际学习表示会议，2019年。'
- en: '[244] H. Zhou, B. Ummenhofer, and T. Brox, “Deeptam: Deep tracking and mapping
    with convolutional neural networks,” International Journal of Computer Vision,
    vol. 128, no. 3, pp. 756–769, 2020.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] H. Zhou, B. Ummenhofer 和 T. Brox，“Deeptam: 利用卷积神经网络进行深度跟踪和映射，” 国际计算机视觉杂志，vol.
    128，no. 3，pp. 756–769，2020年。'
- en: '[245] J. Czarnowski, T. Laidlow, R. Clark, and A. J. Davison, “Deepfactors:
    Real-time probabilistic dense monocular slam,” IEEE Robotics and Automation Letters,
    2020.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] J. Czarnowski, T. Laidlow, R. Clark 和 A. J. Davison，“Deepfactors: 实时概率密度单目SLAM，”
    IEEE Robotics and Automation Letters，2020年。'
- en: '[246] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Representing
    model uncertainty in deep learning,” in international conference on machine learning,
    pp. 1050–1059, 2016.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] Y. Gal 和 Z. Ghahramani，“Dropout作为贝叶斯近似：在深度学习中表示模型不确定性，” 在国际机器学习会议，pp.
    1050–1059，2016年。'
- en: '[247] S. Wang, R. Clark, H. Wen, and N. Trigoni, “End-to-end, sequence-to-sequence
    probabilistic visual odometry through deep neural networks,” The International
    Journal of Robotics Research, vol. 37, no. 4-5, pp. 513–542, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] S. Wang, R. Clark, H. Wen 和 N. Trigoni，“端到端、序列到序列的概率视觉里程计通过深度神经网络，” 国际机器人研究杂志，vol.
    37，no. 4-5，pp. 513–542，2018年。'
- en: '[248] A. Kendall, V. Badrinarayanan, and R. Cipolla, “Bayesian segnet: Model
    uncertainty in deep convolutional encoder-decoder architectures for scene understanding,”
    in British Machine Vision Conference (BMVC), 2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] A. Kendall, V. Badrinarayanan 和 R. Cipolla，“Bayesian segnet: 场景理解中深度卷积编码-解码架构的模型不确定性，”
    在英国机器视觉会议（BMVC），2017年。'
- en: '[249] M. Klodt and A. Vedaldi, “Supervising the new with the old: learning
    sfm from sfm,” in The European Conference on Computer Vision (ECCV), pp. 698–713,
    2018.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] M. Klodt 和 A. Vedaldi，“用旧知识指导新知识：从SFM中学习SFM，” 在欧洲计算机视觉会议（ECCV），pp. 698–713，2018年。'
- en: '[250] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “VINet : Visual-Inertial
    Odometry as a Sequence-to-Sequence Learning Problem,” in The Conference on Artificial
    Intelligence (AAAI), pp. 3995–4001, 2017.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] R. Clark, S. Wang, H. Wen, A. Markham 和 N. Trigoni，“VINet: 视觉-惯性里程计作为序列到序列学习问题，”
    在人工智能会议（AAAI），pp. 3995–4001，2017年。'
- en: '[251] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham, and N. Trigoni,
    “Selective sensor fusion for neural visual-inertial odometry,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10542–10551,
    2019.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham 和 N. Trigoni，“用于神经视觉惯性里程计的选择性传感器融合，”
    在IEEE/CVF国际计算机视觉与模式识别会议（CVPR），pp. 10542–10551，2019年。'
- en: '[252] C. Chen, S. Rosa, C. X. Lu, B. Wang, N. Trigoni, and A. Markham, “Learning
    selective sensor fusion for state estimation,” IEEE Transactions on Neural Networks
    and Learning Systems, 2022.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] C. Chen, S. Rosa, C. X. Lu, B. Wang, N. Trigoni 和 A. Markham，“学习状态估计的选择性传感器融合，”
    IEEE Transactions on Neural Networks and Learning Systems，2022年。'
- en: '[253] E. J. Shamwell, K. Lindgren, S. Leung, and W. D. Nothwang, “Unsupervised
    deep visual-inertial odometry with online error correction for rgb-d imagery,”
    IEEE transactions on pattern analysis and machine intelligence, 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] E. J. Shamwell, K. Lindgren, S. Leung 和 W. D. Nothwang，“无监督深度视觉惯性里程计与RGB-D图像的在线误差校正，”
    IEEE Transactions on Pattern Analysis and Machine Intelligence，2019年。'
- en: '[254] L. Han, Y. Lin, G. Du, and S. Lian, “Deepvio: Self-supervised deep learning
    of monocular visual inertial odometry using 3d geometric constraints,” The IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS), 2019.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] L. Han, Y. Lin, G. Du 和 S. Lian，“Deepvio: 使用3D几何约束自监督深度学习单目视觉惯性里程计，”
    IEEE/RSJ国际智能机器人与系统会议（IROS），2019年。'
- en: '[255] P. Wei, G. Hua, W. Huang, F. Meng, and H. Liu, “Unsupervised monocular
    visual-inertial odometry network,” in Proceedings of the Twenty-Ninth International
    Conference on International Joint Conferences on Artificial Intelligence, pp. 2347–2354,
    2021.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] P. Wei, G. Hua, W. Huang, F. Meng 和 H. Liu，“无监督单目视觉惯性里程计网络，” 在第二十九届国际人工智能联合会议论文集，pp.
    2347–2354，2021年。'
- en: '[256] L. Sheng, D. Xu, W. Ouyang, and X. Wang, “Unsupervised collaborative
    learning of keyframe detection and visual odometry towards monocular deep slam,”
    in The International Conference on Computer Vision (ICCV), pp. 4302–4311, 2019.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] L. Sheng, D. Xu, W. Ouyang, 和 X. Wang, “无监督的关键帧检测与视觉里程计协作学习，面向单目深度SLAM，”
    在《国际计算机视觉大会》（ICCV），第4302–4311页，2019年。 |'
- en: '| ![[Uncaptioned image]](img/19853889941d88e5501b9e1ca4f2e5aa.png) | Changhao
    Chen is a Lecturer at College of Intelligence Science and Technology, National
    University of Defense Technology. Before that, he obtained his Ph.D. degree at
    University of Oxford (UK), M.Eng. degree at National University of Defense Technology
    (China), and B.Eng. degree at Tongji University (China). His research interest
    lies in robotics, computer vision and cyberphysical systems. |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/19853889941d88e5501b9e1ca4f2e5aa.png) | Changhao Chen 是国防科技大学智能科学与技术学院的讲师。在此之前，他在牛津大学（英国）获得博士学位，在国防科技大学（中国）获得硕士学位，在同济大学（中国）获得学士学位。他的研究兴趣在于机器人技术、计算机视觉和网络物理系统。
    |'
- en: '| ![[Uncaptioned image]](img/5311a5a1f6028016eb22139e7e9b05fc.png) | Bing Wang
    is an Assistant Professor in the Department of Aeronautical and Aviation Engineering,
    The Hong Kong Polytechnic University. Before that, he obtained his Ph.D. degree
    from University of Oxford. His research interests broadly lie in the design of
    intelligent perception solutions for autonomous systems, and the development of
    reliable 3D scene understanding algorithms on mobile robotics operating in the
    real world. |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/5311a5a1f6028016eb22139e7e9b05fc.png) | Bing Wang 是香港理工大学航空航天工程系的助理教授。在此之前，他在牛津大学获得博士学位。他的研究兴趣广泛包括智能感知解决方案的设计用于自主系统，以及开发在现实世界中运行的移动机器人上可靠的3D场景理解算法。
    |'
- en: '| ![[Uncaptioned image]](img/85c0c3496f1feeb09153ecf321d522d6.png) | Chris
    Xiaoxuan Lu is an Assistant Professor at School of Informatics, University of
    Edinburgh. Before that, he obtained his Ph.D degree at University of Oxford, and
    MEng degree at Nanyang Technology University, Singapore. His research interest
    lies in Cyber Physical Systems, which use networked smart devices to sense and
    interact with the physical world. |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/85c0c3496f1feeb09153ecf321d522d6.png) | Chris Xiaoxuan Lu
    是爱丁堡大学信息学学院的助理教授。在此之前，他在牛津大学获得博士学位，在南洋理工大学（新加坡）获得硕士学位。他的研究兴趣在于网络化智能设备感知并与物理世界互动的网络物理系统。
    |'
- en: '| ![[Uncaptioned image]](img/a437b5692a0619b1eccc1a45ee717a6c.png) | Niki Trigoni
    is a Professor at the Department of Computer Science, University of Oxford. She
    is currently the director of the EPSRC Centre for Doctoral Training on Autonomous
    Intelligent Machines and Systems, and leads the Cyber Physical Systems Group.
    Her research interests lie in intelligent and autonomous sensor systems with applications
    in positioning, healthcare, environmental monitoring and smart cities. |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/a437b5692a0619b1eccc1a45ee717a6c.png) | Niki Trigoni 是牛津大学计算机科学系的教授。她目前是EPSRC自主智能机器与系统博士培训中心的主任，并领导网络物理系统组。她的研究兴趣在于智能和自主传感器系统，应用于定位、医疗保健、环境监测和智慧城市。
    |'
- en: '| ![[Uncaptioned image]](img/a865bcbab2cf25c5c260f28424699761.png) | Andrew
    Markham is a Professor at the Department of Computer Science, University of Oxford.
    He obtained his BSc (2004) and PhD (2008) degrees from the University of Cape
    Town, South Africa. He is the Director of the MSc in Software Engineering. He
    works on resource-constrained systems, positioning systems, in particular magneto-inductive
    positioning and machine intelligence. |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/a865bcbab2cf25c5c260f28424699761.png) | Andrew Markham 是牛津大学计算机科学系的教授。他在开普敦大学（南非）获得了本科（2004年）和博士（2008年）学位。他是软件工程硕士课程的主任。他研究资源受限系统、定位系统，特别是磁感应定位和机器智能。
    |'
