- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:47:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:47:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2202.13589] Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2202.13589] 无监督点云表示学习与深度神经网络：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2202.13589](https://ar5iv.labs.arxiv.org/html/2202.13589)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2202.13589](https://ar5iv.labs.arxiv.org/html/2202.13589)
- en: 'Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督点云表示学习与深度神经网络：综述
- en: 'Aoran Xiao^∗, Jiaxing Huang^∗, Dayan Guan, Xiaoqin Zhang, Shijian Lu,and Ling Shao
    Aoran Xiao and Jiaxing Huang are co-first authors. Aoran Xiao, Jiaxing Huang and
    Shijian Lu are with the School of Computer Science and Engineering, Nanyang Technological
    University, Singapore. Dayan Guan is with Mohamed bin Zayed University of Artificial
    Intelligence, United Arab Emirates. Xiaoqin Zhang is with Key Laboratory of Intelligent
    Informatics for Safety & Emergency of Zhejiang Province, Wenzhou University, China.
    Ling Shao is with UCAS-Terminus AI Lab, UCAS. Corresponding authors: Shijian Lu
    (shijian.lu@ntu.edu.sg) and Xiaoqin Zhang (zhangxiaoqinnan@gmail.com)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 小奥然^∗，黄家兴^∗，关大言，张小琴，陆世坚，邵玲 小奥然和黄家兴为共同第一作者。小奥然、黄家兴和陆世坚在新加坡南洋理工大学计算机科学与工程学院工作。关大言在阿联酋穆罕默德·本·扎耶德人工智能大学工作。张小琴在中国温州大学浙江省安全与应急智能信息重点实验室工作。邵玲在中国科学院终端人工智能实验室工作。通讯作者：陆世坚（shijian.lu@ntu.edu.sg）和张小琴（zhangxiaoqinnan@gmail.com）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Point cloud data have been widely explored due to its superior accuracy and
    robustness under various adverse situations. Meanwhile, deep neural networks (DNNs)
    have achieved very impressive success in various applications such as surveillance
    and autonomous driving. The convergence of point cloud and DNNs has led to many
    deep point cloud models, largely trained under the supervision of large-scale
    and densely-labelled point cloud data. Unsupervised point cloud representation
    learning, which aims to learn general and useful point cloud representations from
    unlabelled point cloud data, has recently attracted increasing attention due to
    the constraint in large-scale point cloud labelling. This paper provides a comprehensive
    review of unsupervised point cloud representation learning using DNNs. It first
    describes the motivation, general pipelines as well as terminologies of the recent
    studies. Relevant background including widely adopted point cloud datasets and
    DNN architectures is then briefly presented. This is followed by an extensive
    discussion of existing unsupervised point cloud representation learning methods
    according to their technical approaches. We also quantitatively benchmark and
    discuss the reviewed methods over multiple widely adopted point cloud datasets.
    Finally, we share our humble opinion about several challenges and problems that
    could be pursued in the future research in unsupervised point cloud representation
    learning. A project associated with this survey has been built at [https://github.com/xiaoaoran/3d_url_survey](https://github.com/xiaoaoran/3d_url_survey).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 点云数据因其在各种不利情况下的优越准确性和鲁棒性而得到广泛探索。与此同时，深度神经网络（DNNs）在监控和自动驾驶等各种应用中取得了非常令人印象深刻的成功。点云与DNN的融合催生了许多深度点云模型，这些模型大多在大规模和密集标注的点云数据的监督下进行训练。由于大规模点云标注的限制，无监督点云表示学习，即从未标注的点云数据中学习通用且有用的点云表示，近年来受到越来越多的关注。本文全面回顾了使用DNNs进行的无监督点云表示学习。首先描述了近期研究的动机、一般流程及术语。接着简要介绍了相关背景，包括广泛采用的点云数据集和DNN架构。随后，根据技术方法对现有的无监督点云表示学习方法进行了广泛讨论。我们还对所评审的方法在多个广泛采用的点云数据集上进行了定量基准测试和讨论。最后，我们分享了对无监督点云表示学习未来研究中几个挑战和问题的谦虚看法。与本调查相关的项目已在[https://github.com/xiaoaoran/3d_url_survey](https://github.com/xiaoaoran/3d_url_survey)上建立。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Point cloud, unsupervised representation learning, self-supervised learning,
    deep learning, transfer learning, 3D vision, pre-training, deep neural network
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 点云，无监督表示学习，自监督学习，深度学习，迁移学习，3D视觉，预训练，深度神经网络
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 3D acquisition technologies have experienced fast development in recent years.
    This can be witnessed by different 3D sensors that have become increasingly popular
    in both industrial and our daily lives such as LiDAR sensors in autonomous vehicles,
    RGB-D cameras in Kinect and Apple devices, 3D scanners in various reconstruction
    tasks, etc. Meanwhile, 3D data of different modalities such as meshes, point clouds,
    depth images and volumetric grids, which capture accurate geometric information
    for both objects and scenes, have been collected and widely applied in different
    areas such as autonomous driving, robotics, medical treatment, remote sensing,
    etc.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，3D获取技术发展迅速。这可以通过各种3D传感器的普及来见证，如自动驾驶车辆中的LiDAR传感器、Kinect和Apple设备中的RGB-D摄像头、各种重建任务中的3D扫描仪等。与此同时，捕捉准确几何信息的不同模态3D数据，如网格、点云、深度图像和体积网格，已被收集并广泛应用于不同领域，如自动驾驶、机器人技术、医疗治疗、遥感等。
- en: '![Refer to caption](img/01c28ef2de4ec45f95014268bd339db7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/01c28ef2de4ec45f95014268bd339db7.png)'
- en: 'Figure 1: The general pipeline of unsupervised representation learning on point
    clouds: Deep neural networks are first pre-trained with unannotated point clouds
    via unsupervised learning over certain pre-text tasks. The learned unsupervised
    point cloud representations are then transferred to various downstream tasks to
    provide network initialization, with which the pre-trained networks are fine-tuned
    with a small amount of annotated task-specific point cloud data.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：点云上无监督表示学习的通用流程：深度神经网络首先通过无监督学习在某些预文本任务上使用未标注的点云进行预训练。然后，将学习到的无监督点云表示转移到各种下游任务中，为网络初始化提供支持，接着用少量标注的任务特定点云数据对预训练的网络进行微调。
- en: Point cloud as one source of ubiquitous and widely used 3D data can be directly
    captured with entry-level depth sensors before triangulating into meshes or converting
    to voxels. This makes it easily applicable to various 3D scene understanding tasks [[1](#bib.bib1)]
    such as 3D object detection and shape analysis, semantic segmentation, etc. With
    the advance of deep neural networks (DNNs), point cloud understanding has attracted
    increasing attention as observed by a large number of deep architectures and deep
    models developed in recent years [[2](#bib.bib2)]. On the other hand, effective
    training of deep networks requires large-scale human-annotated training data such
    as 3D bounding boxes for object detection and point-wise annotations for semantic
    segmentation, which are usually laborious and time-consuming to collect due to
    3D view changes and visual inconsistency between human perception and point cloud
    display. Efficient collection of large-scale annotated point clouds has become
    one bottleneck for effective design, evaluations, and deployment of deep networks
    while handling various real-world tasks [[3](#bib.bib3)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 点云作为一种普遍且广泛使用的3D数据来源，可以直接通过入门级深度传感器捕获，然后进行三角化成网格或转换为体素。这使其在各种3D场景理解任务中具有广泛的应用[[1](#bib.bib1)]，如3D物体检测和形状分析、语义分割等。随着深度神经网络（DNN）的进步，点云理解引起了越来越多的关注，这从近年来开发的大量深度架构和深度模型中可见一斑[[2](#bib.bib2)]。另一方面，深度网络的有效训练需要大规模的人工标注训练数据，如物体检测的3D边界框和语义分割的点级标注，这通常因3D视角变化和人类感知与点云显示之间的视觉不一致而收集起来费时费力。高效收集大规模标注点云已成为处理各种真实世界任务时有效设计、评估和部署深度网络的一个瓶颈[[3](#bib.bib3)]。
- en: '![Refer to caption](img/98272c6d65096f2f15aa86424da5b083.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/98272c6d65096f2f15aa86424da5b083.png)'
- en: 'Figure 2: Taxonomy of existing unsupervised methods in point cloud representation
    learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：现有无监督方法在点云表示学习中的分类。
- en: 'Unsupervised representation learning (URL), which aims to learn robust and
    general feature representations from unlabelled data, has recently been studied
    intensively for mitigating the laborious and time-consuming data annotation challenge.
    As Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") shows, URL works in a similar way
    to pre-training which learns useful knowledge from unlabelled data and transfers
    the learned knowledge to various downstream tasks [[4](#bib.bib4)]. More specifically,
    URL can provide useful network initialization with which well-performing network
    models can be trained with a small amount of labelled and task-specific training
    data without suffering from much over-fitting as compared with training from random
    initialization. URL can thus help reduce training data and annotations which has
    demonstrated great effectiveness in the areas of natural language processing (NLP) [[5](#bib.bib5),
    [6](#bib.bib6)], 2D computer vision [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10)], etc.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '无监督表示学习（URL）旨在从未标记的数据中学习鲁棒和通用的特征表示，近年来已被深入研究以缓解繁琐且耗时的数据标注挑战。如图 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey") 所示，URL 的工作原理类似于预训练，通过从未标记的数据中学习有用的知识，并将所学知识转移到各种下游任务中 [[4](#bib.bib4)]。更具体地说，URL
    可以提供有用的网络初始化，通过这种初始化，可以在少量标记和任务特定的训练数据下训练出表现良好的网络模型，相较于从随机初始化训练，更不容易出现过拟合。因此，URL
    可以帮助减少训练数据和注释，这在自然语言处理（NLP） [[5](#bib.bib5)、[6](#bib.bib6)]、二维计算机视觉 [[7](#bib.bib7)、[8](#bib.bib8)、[9](#bib.bib9)、[10](#bib.bib10)]
    等领域已经显示出很大的有效性。'
- en: Similar to URL from other types of data such as texts and 2D images, URL of
    point clouds has recently attracted increasing attention in the computer vision
    research community. A number of URL techniques have been reported which are typically
    achieved by designing different pre-text tasks such as 3D object reconstruction [[11](#bib.bib11)],
    partial object completion [[12](#bib.bib12)], 3D jigsaws solving [[13](#bib.bib13)],
    etc. However, URL of point clouds still lags far behind as compared with its counterparts
    in NLP and 2D computer vision tasks. For the time being, training from scratch
    on various target new data is still the prevalent approach in most existing 3D
    scene understanding development. At the other end, URL from point cloud data is
    facing increasing problems and challenges, largely due to the lack of large-scale
    and high-quality point cloud data, unified deep backbone architectures, generalizable
    technical approaches, as well as comprehensive public benchmarks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于来自其他数据类型（如文本和二维图像）的 URL，点云的 URL 最近在计算机视觉研究领域引起了越来越多的关注。已有许多 URL 技术被报道，这些技术通常通过设计不同的预训练任务来实现，例如
    3D 物体重建 [[11](#bib.bib11)]、部分物体完成 [[12](#bib.bib12)]、3D 拼图解决 [[13](#bib.bib13)]
    等。然而，与自然语言处理（NLP）和二维计算机视觉任务相比，点云的 URL 仍然落后许多。目前，大多数现有的 3D 场景理解开发仍然是从头开始在各种目标新数据上进行训练。另一方面，点云数据的
    URL 正面临越来越多的问题和挑战，这在很大程度上是由于缺乏大规模和高质量的点云数据、统一的深度骨干网络架构、可泛化的技术方法以及全面的公共基准。
- en: 'In addition, URL for point clouds is still short of systematic survey that
    can offer a clear big picture about this new yet challenging task. To fill up
    this gap, this paper presents a comprehensive survey on the recent progress in
    unsupervised point cloud representation learning from the perspective of datasets,
    network architectures, technical approaches, performance benchmarking, and future
    research directions. As shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣
    Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A
    Survey"), we broadly group existing methods into four categories based on their
    pretext tasks, including URL methods using data generation, global and local contexts,
    multimodality data and local descriptors, more details to be discussed in the
    ensuing subsections.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，关于点云的 URL 仍然缺乏系统性的调查，无法提供关于这一新兴但具有挑战性的任务的清晰全貌。为填补这一空白，本文从数据集、网络架构、技术方法、性能基准和未来研究方向的角度，全面回顾了无监督点云表示学习的最新进展。如图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") 所示，我们根据预训练任务将现有方法大致分为四类，包括使用数据生成、全局和局部上下文、多模态数据和局部描述符的
    URL 方法，具体细节将在后续小节中讨论。'
- en: 'The major contributions of this work are threefold:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献有三方面：
- en: '1.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: It presents a comprehensive review of the recent development in unsupervised
    point cloud representation learning. To the best of our knowledge, it is the first
    survey that provides an overview and big picture for this exciting research topic.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它提供了对无监督点云表示学习最近发展的全面综述。据我们所知，这是首个提供此激动人心的研究主题概览和全貌的综述。
- en: '2.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: It studies the most recent progress of unsupervised point cloud representation
    learning, including a comprehensive benchmarking and discussion of existing methods
    over multiple public datasets.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它研究了无监督点云表示学习的最新进展，包括对多个公共数据集上现有方法的全面基准测试和讨论。
- en: '3.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: It shares several research challenges and potential research directions that
    could be pursued in unsupervised point cloud representation learning.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它分享了无监督点云表示学习中若干研究挑战和潜在的研究方向。
- en: 'The rest of this survey is organized as follows: In Section [2](#S2 "2 Background
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), we introduce background knowledge of unsupervised point cloud learning
    including term definition, common tasks of point cloud understanding and relevant
    surveys to this work. Section [3](#S3 "3 Point cloud datasets ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey") introduces
    widely-used datasets and their characteristics. Section [4](#S4 "4 Common deep
    architectures ‣ Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey") introduces commonly used deep point cloud architectures with
    typical models that are frequently used for point cloud URL. In Section [5](#S5
    "5 Unsupervised point cloud representation learning ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey") we systematically
    review the methods for point cloud URL. Section [6](#S6 "6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") summarizes and compares the performances of existing methods on multiple
    benchmark datasets. At last, we list several promising future directions for unsupervised
    point cloud representation learning in Section [7](#S7 "7 Future direction ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本文其余部分的组织结构如下：在[第2节](#S2 "2 背景 ‣ 无监督点云表示学习与深度神经网络：综述")中，我们介绍了无监督点云学习的背景知识，包括术语定义、点云理解的常见任务以及与此工作相关的调查。在[第3节](#S3
    "3 点云数据集 ‣ 无监督点云表示学习与深度神经网络：综述")中，我们介绍了广泛使用的数据集及其特征。在[第4节](#S4 "4 常见深度架构 ‣ 无监督点云表示学习与深度神经网络：综述")中，我们介绍了常用的深度点云架构及典型的点云URL模型。在[第5节](#S5
    "5 无监督点云表示学习 ‣ 无监督点云表示学习与深度神经网络：综述")中，我们系统回顾了点云URL方法。在[第6节](#S6 "6 基准性能 ‣ 无监督点云表示学习与深度神经网络：综述")中，我们总结并比较了现有方法在多个基准数据集上的性能。最后，在[第7节](#S7
    "7 未来方向 ‣ 无监督点云表示学习与深度神经网络：综述")中，我们列出了无监督点云表示学习的几个有前途的未来方向。
- en: 2 Background
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Basic concepts
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基本概念
- en: We first define all relevant terms and concepts that are to be used in the ensuing
    sections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了在接下来的章节中将使用的所有相关术语和概念。
- en: 'Point cloud data: A point cloud $P$ is a set of vectors $P=\{p_{1},...,p_{N}\}$
    where each vector represents one point $p_{i}=[C_{i},A_{i}]$. Here, $C_{i}\in\mathbf{R}^{1\times
    3}$ refers to 3D coordinate $(x_{i},y_{i},z_{i})$ of the point, and $A_{i}$ refers
    to feature attributes of the point such as RGB values, LiDAR intensity, normal
    values, etc., which are optional and variational depending on 3D sensors as well
    as applications.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 点云数据：点云 $P$ 是一个向量集合 $P=\{p_{1},...,p_{N}\}$，其中每个向量表示一个点 $p_{i}=[C_{i},A_{i}]$。这里，$C_{i}\in\mathbf{R}^{1\times
    3}$ 表示点的3D坐标 $(x_{i},y_{i},z_{i})$，而 $A_{i}$ 表示点的特征属性，如RGB值、LiDAR强度、法线值等，这些都是可选的并且会因3D传感器及应用而有所不同。
- en: 'Supervised learning: Under the paradigm of deep learning, supervised learning
    aims to train deep network models by using labelled training data.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习：在深度学习的范式下，监督学习旨在通过使用标记的训练数据来训练深度网络模型。
- en: 'Unsupervised learning: Unsupervised learning aims to train networks by using
    unlabelled training data.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习：无监督学习旨在通过使用未标记的训练数据来训练网络。
- en: 'Unsupervised representation learning: URL is a subset of unsupervised learning.
    It aims to learn meaningful representations from data without using any data labels/annotations,
    where the learned representations can be transferred to different downstream tasks.
    Some literature alternatively uses the term “self-supervised learning”.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督表示学习：URL是无监督学习的一个子集。它旨在从数据中学习有意义的表示，而不使用任何数据标签/注释，所学到的表示可以转移到不同的下游任务中。一些文献中使用“自监督学习”这一术语。
- en: 'Semi-supervised learning: In semi-supervised learning, deep networks are trained
    with a small amount of labelled data and a large amount of unlabelled data. It
    aims to mitigate data annotation constraints by learning from a small amount of
    labelled data and a large amount of unlabelled data that have similar distributions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习：在半监督学习中，深度网络使用少量标记数据和大量未标记数据进行训练。它旨在通过从少量标记数据和分布相似的大量未标记数据中学习，以减轻数据注释的约束。
- en: 'Pre-training: Network pre-training learns with certain pre-text tasks over
    other datasets. The learned parameters are often employed for model initialization
    for further fine-tuning with various task-specific data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练：网络预训练在其他数据集上通过特定的预文本任务进行学习。学到的参数通常用于模型初始化，以便进一步使用各种任务特定的数据进行微调。
- en: 'Transfer learning: Transfer learning aims to transfer knowledge across tasks,
    modalities or datasets. A typical scenario related to this survey is to perform
    unsupervised learning for pre-training for transferring the learned knowledge
    from unlabelled data to various downstream networks.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习：迁移学习旨在跨任务、模态或数据集转移知识。与本调查相关的典型场景是进行无监督学习以进行预训练，将从未标记数据中学到的知识转移到各种下游网络中。
- en: 2.2 Common 3D understanding tasks
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 常见的3D理解任务
- en: 'This subsection introduces common 3D understanding tasks including object-level
    tasks in object classification and object part segmentation and scene-level tasks
    in 3D object detection, semantic segmentation and instance segmentation. These
    tasks have been widely adopted to evaluate the quality of point cloud representations
    that are learned via various unsupervised learning methods, which will be discussed
    in detail in Section [6](#S6 "6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '本小节介绍了常见的3D理解任务，包括对象分类和对象部分分割中的对象级任务，以及3D对象检测、语义分割和实例分割中的场景级任务。这些任务已被广泛采用，以评估通过各种无监督学习方法学习到的点云表示的质量，这将在第[6](#S6
    "6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning with
    Deep Neural Networks: A Survey")节中详细讨论。'
- en: '![Refer to caption](img/0eec5bef1faa87c2916fcbfe84a65ca9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0eec5bef1faa87c2916fcbfe84a65ca9.png)'
- en: 'Figure 3: Illustration of object part segmentation: The first row shows a few
    object samples including airplane, motorcycle, and table from the ShapeNetPart
    dataset [[14](#bib.bib14)]. The second row shows segmentation ground truth with
    different parts as highlighted by different colors.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：对象部分分割的示意图：第一行展示了来自ShapeNetPart数据集的几个对象样本，包括飞机、摩托车和桌子。第二行展示了不同部分的分割地面真值，不同颜色突出显示了不同的部分。
- en: 2.2.1 Object classification
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 对象分类
- en: 'Object classification aims to classify point cloud objects into a number of
    pre-defined categories. Two evaluation metrics are most frequently used: The overall
    Accuracy (OA) represents the averaged accuracy for all instances in the test set;
    The mean class accuracy (mAcc) represents the mean accuracy of all object classes
    for the test set.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对象分类的目标是将点云对象分类到若干预定义的类别中。最常用的两个评估指标是：总体准确率（OA）代表测试集中所有实例的平均准确率；平均类别准确率（mAcc）代表测试集中所有对象类别的平均准确率。
- en: 2.2.2 Object part segmentation
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 对象部分分割
- en: 'Object part segmentation is an important task for point cloud representation
    learning. It aims to assign a part category label (e.g., airplane wing, table
    leg, etc.) to each point as illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Common
    3D understanding tasks ‣ 2 Background ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). The mean Intersection over Union
    (mIoU) [[15](#bib.bib15)] is the most widely adopted evaluation metric. For each
    instance, IoU is computed for each part belonging to that object category. The
    mean of the part IoUs represents the IoU of that object instance. The overall
    IoU is computed as the average of IoUs over all test instances while category-wise
    IoU (or class IoU) is calculated as the mean over instances under that category.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '对象部件分割是点云表示学习中的一项重要任务。它的目标是将部件类别标签（例如飞机翼、桌腿等）分配给每个点，如图 [3](#S2.F3 "Figure 3
    ‣ 2.2 Common 3D understanding tasks ‣ 2 Background ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey") 所示。平均交并比（mIoU）[[15](#bib.bib15)]
    是最广泛采用的评估指标。对于每个实例，计算属于该对象类别的每个部件的IoU。部件IoU的平均值表示该对象实例的IoU。总体IoU计算为所有测试实例的IoU的平均值，而类别-wise
    IoU（或类 IoU）则计算为该类别下实例的平均值。'
- en: 2.2.3 3D object detection
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 3D 对象检测
- en: '3D object detection on point clouds is a crucial and indispensable task for
    many real-world applications, such as autonomous driving and domestic robots.
    The task aims to localize objects in the 3D space, i.e. 3D object bounding boxes
    as illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ 2.2.3 3D object detection ‣ 2.2
    Common 3D understanding tasks ‣ 2 Background ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). The average precision (AP) metric
    has been widely used for evaluations in 3D object detection [[16](#bib.bib16),
    [17](#bib.bib17)].'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '点云上的 3D 对象检测是许多实际应用中至关重要且不可或缺的任务，如自动驾驶和家用机器人。该任务旨在在 3D 空间中定位对象，即 3D 对象边界框，如图
    [4](#S2.F4 "Figure 4 ‣ 2.2.3 3D object detection ‣ 2.2 Common 3D understanding
    tasks ‣ 2 Background ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey") 所示。平均精度 (AP) 指标已广泛用于 3D 对象检测的评估 [[16](#bib.bib16),
    [17](#bib.bib17)]。'
- en: '![Refer to caption](img/68f6f21346e738050cbcefdd3599f402.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/68f6f21346e738050cbcefdd3599f402.png)'
- en: (a) ScanNet-V2 dataset
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ScanNet-V2 数据集
- en: '![Refer to caption](img/da28e0a769580951dc8da07370f0253f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da28e0a769580951dc8da07370f0253f.png)'
- en: (b) KITTI dataset
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: (b) KITTI 数据集
- en: 'Figure 4: Illustration of 3D bounding boxes in point cloud object detection:
    The two graphs show 3D bounding boxes in datasets ScanNet-V2 [[18](#bib.bib18)]
    and KITTI [[19](#bib.bib19)] which are cropped from [[16](#bib.bib16)] and [[20](#bib.bib20)],
    respectively.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：点云对象检测中的 3D 边界框示例：这两幅图展示了从 [[16](#bib.bib16)] 和 [[20](#bib.bib20)] 中裁剪的
    ScanNet-V2 [[18](#bib.bib18)] 和 KITTI [[19](#bib.bib19)] 数据集中的 3D 边界框。
- en: '![Refer to caption](img/a7db22cb8112a95c7753db79dfb191cd.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a7db22cb8112a95c7753db79dfb191cd.png)'
- en: (a) A raw sample
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始样本
- en: '![Refer to caption](img/f57619e9a8e6953799617b4c9b0794c8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f57619e9a8e6953799617b4c9b0794c8.png)'
- en: (b) Semantic annotations
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 语义注释
- en: 'Figure 5: Illustration of semantic point cloud segmentation: For the point
    cloud sample from S3DIS [[21](#bib.bib21)] on the left, the graph on the right
    shows the corresponding ground truth where different categories are highlighted
    by different colors.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：语义点云分割的示例：左侧是来自 S3DIS [[21](#bib.bib21)] 的点云样本，右侧的图展示了对应的真实标签，其中不同的类别用不同的颜色突出显示。
- en: 2.2.4 3D semantic segmentation
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 3D 语义分割
- en: '3D semantic segmentation on point clouds is another critical task for 3D understanding
    as illustrated in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2.3 3D object detection ‣ 2.2
    Common 3D understanding tasks ‣ 2 Background ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). Different from the object part
    segmentation that segments point cloud objects, 3D semantic segmentation aims
    to assign a category label to each point in scene-level point clouds with much
    higher complexity. The widely adopted evaluation metrics includes OA, mIoU over
    semantic categories and mAcc.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '点云上的 3D 语义分割是 3D 理解中的另一个关键任务，如图 [5](#S2.F5 "Figure 5 ‣ 2.2.3 3D object detection
    ‣ 2.2 Common 3D understanding tasks ‣ 2 Background ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey") 所示。与分割点云对象的对象部件分割不同，3D
    语义分割旨在为场景级点云中的每个点分配一个类别标签，其复杂性要高得多。广泛采用的评估指标包括 OA、语义类别上的 mIoU 和 mAcc。'
- en: 2.2.5 3D instance segmentation
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5 3D 实例分割
- en: '3D instance segmentation aims to detect and delineate each distinct object
    of interest in scene-level point clouds as illustrated in Fig. [6](#S2.F6 "Figure
    6 ‣ 2.2.5 3D instance segmentation ‣ 2.2 Common 3D understanding tasks ‣ 2 Background
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"). On top of semantic segmentation that considers the semantic category
    only, instance segmentation assigns each object a unique identity. Mean Average
    Precision (mAP) has been widely adopted for the quantitative evaluation of this
    task.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 实例分割旨在检测并描绘场景级点云中的每个独特对象，如图 [6](#S2.F6 "图 6 ‣ 2.2.5 3D 实例分割 ‣ 2.2 常见的 3D
    理解任务 ‣ 2 背景 ‣ 基于深度神经网络的无监督点云表示学习：综述") 所示。除了只考虑语义类别的语义分割外，实例分割还为每个对象分配唯一的身份。均值平均精度
    (mAP) 已广泛用于此任务的定量评估。
- en: '![Refer to caption](img/73c2e1ef70510ad21722234db5bb0018.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/73c2e1ef70510ad21722234db5bb0018.png)'
- en: (a) A raw sample
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始样本
- en: '![Refer to caption](img/374e39ad295bfb345b3dc9c9b94e2450.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/374e39ad295bfb345b3dc9c9b94e2450.png)'
- en: (b) Instance annotations
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 实例注释
- en: 'Figure 6: Illustration of instance segmentation on point clouds: For the point
    cloud sample from ScanNet-V2 [[18](#bib.bib18)] on the left, the graph on the
    right shows the corresponding ground truth with different instances highlighted
    by different colors.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 点云上的实例分割示意图：左侧的点云样本来自 ScanNet-V2 [[18](#bib.bib18)]，右侧的图显示了对应的真实数据，不同实例用不同颜色突出显示。'
- en: 2.3 Relevant surveys
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 相关调查
- en: 'To the best of our knowledge, this paper is the first survey that reviews unsupervised
    point cloud learning comprehensively. Several relevant but different surveys have
    been performed. For example, several papers reviewed recent advances for deep
    supervised learning on point clouds: Ioannidou et al. [[22](#bib.bib22)] reviewed
    deep learning approaches on 3D data; Xie et al. [[23](#bib.bib23)] provided a
    literature review on point cloud segmentation task; Guo et al. [[2](#bib.bib2)]
    provided a comprehensive and detailed survey on deep learning of point cloud for
    multiple tasks including classification, detection, tracking, and segmentation.
    In addition, several works reviewed unsupervised representation learning on other
    data modalities: Jing et al. [[24](#bib.bib24)] introduced advances on unsupervised
    representation learning in 2D computer vision; Liu et al. [[25](#bib.bib25)] looked
    into latest progress about unsupervised representation learning methods in 2D
    computer vision, NLP, and graph learning; Qi et al. [[26](#bib.bib26)] introduced
    recent progress on small data learning including unsupervised- and semi-supervised
    methods.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，本文是首个全面回顾无监督点云学习的调查。已有几项相关但不同的调查。例如，一些论文回顾了点云深度监督学习的最新进展：Ioannidou 等人 [[22](#bib.bib22)]
    回顾了 3D 数据的深度学习方法；Xie 等人 [[23](#bib.bib23)] 提供了点云分割任务的文献综述；Guo 等人 [[2](#bib.bib2)]
    提供了有关点云深度学习的综合详细调查，涵盖了分类、检测、跟踪和分割等多个任务。此外，还有一些工作回顾了其他数据模态上的无监督表示学习：Jing 等人 [[24](#bib.bib24)]
    介绍了 2D 计算机视觉中无监督表示学习的进展；Liu 等人 [[25](#bib.bib25)] 研究了 2D 计算机视觉、NLP 和图学习中的无监督表示学习方法的最新进展；Qi
    等人 [[26](#bib.bib26)] 介绍了包括无监督和半监督方法在内的小数据学习的最新进展。
- en: 3 Point cloud datasets
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 点云数据集
- en: 'TABLE I: Summary of commonly used datasets for training and evaluations in
    prior URL studies with point clouds.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 先前 URL 研究中用于训练和评估的点云常用数据集的总结。'
- en: '| Dataset | Year | #Samples | #Classes | Type | Representation | Label |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | #样本 | #类别 | 类型 | 表示 | 标签 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| KITTI [[19](#bib.bib19)] | 2013 | 15K frames | 8 | Outdoor driving | RGB
    & LiDAR | Bounding box |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| KITTI [[19](#bib.bib19)] | 2013 | 15K 帧 | 8 | 户外驾驶 | RGB & LiDAR | 边界框 |'
- en: '| ModelNet10 [[27](#bib.bib27)] | 2015 | 4,899 objects | 10 | Synthetic object
    | Mesh | Object category label |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet10 [[27](#bib.bib27)] | 2015 | 4,899 个对象 | 10 | 合成对象 | 网格 | 对象类别标签
    |'
- en: '| ModelNet40 [[27](#bib.bib27)] | 2015 | 12,311 objects | 40 | Synthetic object
    | Mesh | Object category label |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet40 [[27](#bib.bib27)] | 2015 | 12,311 个对象 | 40 | 合成对象 | 网格 | 对象类别标签
    |'
- en: '| ShapeNet [[14](#bib.bib14)] | 2015 | 51,190 objects | 55 | Synthetic object
    | Mesh | Object/part category label |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet [[14](#bib.bib14)] | 2015 | 51,190 个对象 | 55 | 合成对象 | 网格 | 对象/部分类别标签
    |'
- en: '| SUN RGB-D [[28](#bib.bib28)] | 2015 | 5K frames | 37 | Indoor scene | RGB-D
    | Bounding box |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGB-D [[28](#bib.bib28)] | 2015 | 5K 帧 | 37 | 室内场景 | RGB-D | 边界框 |'
- en: '| S3DIS [[21](#bib.bib21)] | 2016 | 272 scans | 13 | Indoor scene | RGB-D |
    Point category label |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| S3DIS [[21](#bib.bib21)] | 2016 | 272 次扫描 | 13 | 室内场景 | RGB-D | 点类别标签 |'
- en: '| ScanNet [[18](#bib.bib18)] | 2017 | 1,513 scans | 20 | Indoor scene | RGB-D
    & mesh | Point category label & Bounding box |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet [[18](#bib.bib18)] | 2017 | 1,513 次扫描 | 20 | 室内场景 | RGB-D & 网格 |
    点类别标签 & 边界框 |'
- en: '| ScanObjectNN [[29](#bib.bib29)] | 2019 | 2,902 objects | 15 | Real-world
    object | Points | Object category label |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ScanObjectNN [[29](#bib.bib29)] | 2019 | 2,902 个对象 | 15 | 真实世界物体 | 点 | 物体类别标签
    |'
- en: '| ONCE [[30](#bib.bib30)] | 2021 | 1M scenes | 5 | Outdoor driving | RGB &
    LiDAR | Bounding box |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ONCE [[30](#bib.bib30)] | 2021 | 1M 场景 | 5 | 室外驾驶 | RGB & LiDAR | 边界框 |'
- en: 'In this section, we summarize the commonly used datasets for training and evaluating
    unsupervised point cloud representation learning. As listed in Table [I](#S3.T1
    "TABLE I ‣ 3 Point cloud datasets ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey"), existing work learns unsupervised point
    cloud representations mainly from 1) synthetic object datasets including ModelNet [[27](#bib.bib27)]
    and ShapeNet [[14](#bib.bib14)], or 2) real scene datasets including ScanNet [[18](#bib.bib18)]
    and KITTI [[19](#bib.bib19)]. In addition, various tasks-specific datasets have
    been collected which can be used for fine-tuning downstream models, such as ScanObjectNN [[29](#bib.bib29)],
    ModelNet40 [[27](#bib.bib27)], and ShapeNet [[14](#bib.bib14)] for point cloud
    classification, ShapeNetPart [[14](#bib.bib14)] for part segmentation, S3DIS [[21](#bib.bib21)],
    ScanNet [[18](#bib.bib18)], or Synthia4D [[31](#bib.bib31)] for semantic segmentation,
    indoor datasets SUNRGB-D [[28](#bib.bib28)] and ScanNet [[18](#bib.bib18)] as
    well as outdoor dataset ONCE [[30](#bib.bib30)] for object detection.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们总结了用于训练和评估无监督点云表示学习的常用数据集。如表 [I](#S3.T1 "TABLE I ‣ 3 Point cloud datasets
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") 所示，现有工作主要从 1) 合成物体数据集（包括 ModelNet [[27](#bib.bib27)] 和 ShapeNet [[14](#bib.bib14)]）或
    2) 真实场景数据集（包括 ScanNet [[18](#bib.bib18)] 和 KITTI [[19](#bib.bib19)]）中学习无监督点云表示。此外，还收集了各种特定任务的数据集，可用于微调下游模型，例如用于点云分类的
    ScanObjectNN [[29](#bib.bib29)]、ModelNet40 [[27](#bib.bib27)] 和 ShapeNet [[14](#bib.bib14)]，用于部件分割的
    ShapeNetPart [[14](#bib.bib14)]，用于语义分割的 S3DIS [[21](#bib.bib21)]、ScanNet [[18](#bib.bib18)]
    或 Synthia4D [[31](#bib.bib31)]，以及用于对象检测的室内数据集 SUNRGB-D [[28](#bib.bib28)] 和 ScanNet [[18](#bib.bib18)]
    以及室外数据集 ONCE [[30](#bib.bib30)]。'
- en: '$\bullet$ModelNet10/ModelNet40 [[27](#bib.bib27)]: ModelNet is a synthetic
    object-level dataset for 3D classification. The original ModelNet provides CAD
    models represented by vertices and faces. Point clouds are generated by sampling
    from the models uniformly. ModelNet40 contains 13,834 objects of 40 categories,
    among which 9,843 objects form the training set and the rest form the test set.
    ModelNet10 consists of 3,377 samples of 10 categories, which are split into 2,468
    training samples and 909 testing samples.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ModelNet10/ModelNet40 [[27](#bib.bib27)]: ModelNet 是一个用于 3D 分类的合成物体级数据集。原始的
    ModelNet 提供了由顶点和面表示的 CAD 模型。点云通过从模型中均匀采样生成。ModelNet40 包含 13,834 个 40 类别的对象，其中
    9,843 个对象组成训练集，其余的组成测试集。ModelNet10 由 3,377 个 10 类别的样本组成，这些样本被分为 2,468 个训练样本和 909
    个测试样本。'
- en: '$\bullet$ShapeNet [[14](#bib.bib14)]: ShapeNet contains synthetic 3D objects
    of 55 categories. It was curated by collecting CAD models from online open-sourced
    3D repositories. Similar to ModelNet, synthetic objects in ShapeNet are complete,
    aligned, and with no occlusion or background. Its extension ShapeNetPart has 16,881
    objects of 16 categories and is represented by point clouds. Each object consists
    of 2 to 6 parts, and in total there are 50 part categories in the dataset.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ShapeNet [[14](#bib.bib14)]: ShapeNet 包含 55 类别的合成 3D 对象。它通过从在线开源 3D
    库中收集 CAD 模型来策划。与 ModelNet 类似，ShapeNet 中的合成对象是完整的、对齐的，并且没有遮挡或背景。它的扩展版 ShapeNetPart
    包含 16,881 个 16 类别的对象，并由点云表示。每个对象由 2 到 6 个部件组成，数据集中总共有 50 个部件类别。'
- en: '$\bullet$ScanObjectNN [[29](#bib.bib29)]: ScanObjectNN is a real object-level
    dataset, where 2,902 3D point cloud objects of 15 categories are constructed from
    the scans captured in real indoor scenes. Different from synthetic object datasets,
    point cloud objects in ScanObjectNN are noisy (including background points, occlusions,
    and holes in objects) and not axis-aligned.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ScanObjectNN [[29](#bib.bib29)]: ScanObjectNN 是一个真实物体级数据集，其中 2,902
    个 3D 点云对象来自于真实室内场景的扫描。与合成物体数据集不同，ScanObjectNN 中的点云对象具有噪声（包括背景点、遮挡和物体的孔洞），并且未对齐轴。'
- en: '$\bullet$S3DIS [[21](#bib.bib21)]: Stanford Large-Scale 3D Indoor Spaces (S3DIS)
    dataset contains over 215 million points scanned from 6 large-scale indoor areas
    in 3 office buildings, where each area is 6,000 square meters. The scans are represented
    as point clouds with point-wise semantic labels of 13 object categories.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$S3DIS [[21](#bib.bib21)]: 斯坦福大规模3D室内空间（S3DIS）数据集包含了来自3幢办公楼6个大型室内区域的超过2.15亿个点，每个区域为6000平方米。这些扫描被表示为带有13种物体类别点级语义标签的点云。'
- en: '$\bullet$ScanNet-V2 [[18](#bib.bib18)]: ScanNet-V2 is an RGB-D video dataset
    containing 2.5 million views in more than 1500 scans, which are captured in indoor
    scenes such as offices and living rooms and annotated with 3D camera poses, surface
    reconstructions, as well as semantic and instance labels for segmentation.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ScanNet-V2 [[18](#bib.bib18)]: ScanNet-V2 是一个包含超过1500个扫描的250万个视图的RGB-D视频数据集，这些扫描是在办公室和客厅等室内场景中捕获的，并用三维摄像机姿势、表面重建以及语义和实例标签进行了注释，用于分割。'
- en: '$\bullet$SUN RGB-D [[28](#bib.bib28)]: SUN RGB-D dataset is a collection of
    single view RGB-D images collected from indoor environments. There are in total
    10,335 RGB-D images annotated with amodal, and 3D oriented object bounding boxes
    of 37 categories.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$SUN RGB-D [[28](#bib.bib28)]: SUN RGB-D 数据集是从室内环境收集的单视图RGB-D图像的集合。总共有10335个带有全视角和3D定向物体边界框标记的RGB-D图像，覆盖了37个物体类别。'
- en: '$\bullet$KITTI [[19](#bib.bib19)]: KITTI is a pioneer outdoor dataset providing
    dense point clouds from a LiDAR sensor together with other modalities including
    front-facing stereo images and GPS/IMU data. It provides 200k 3D boxes over 22
    scenes for 3D object detection.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$KITTI [[19](#bib.bib19)]: KITTI是一个首创的室外数据集，提供了来自激光雷达传感器的密集点云以及其他模态，包括前置立体图像和GPS/IMU数据。它为22个场景提供了20万个3D盒子，用于3D物体检测。'
- en: '$\bullet$ONCE [[30](#bib.bib30)]: ONCE dataset has 1 million LiDAR scenes and
    7 million corresponding camera images. There are 581 sequences in total, where
    560 sequences are unlabelled and used for unsupervised learning, and 10 sequences
    are annotated and used for testing. It provides an unsupervised learning benchmark
    for object detection in outdoor environments.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ONCE [[30](#bib.bib30)]: ONCE 数据集包含100万个激光雷达场景和700万张对应的摄像机图像。共有581个序列，其中560个序列无标签用于无监督学习，10个序列有标签用于测试。它为室外环境中的目标检测提供了一个无监督学习的基准。'
- en: 'The publicly available datasets for URL of point clouds are still limited in
    both data size and scene variety, especially compared with the image and text
    datasets that have been used for 2D computer vision and NLP research. For example,
    there are 800 million words in BooksCorpus and 2,500 million words in English
    Wikipedia that is able to provide comprehensive data sources for unsupervised
    representation learning in NLP [[32](#bib.bib32)]; ImageNet [[33](#bib.bib33)]
    has more than 10 million images for unsupervised visual representation learning.
    Large-scale and high-quality point cloud data are highly demanded for future research
    on this topic, and we provide a detailed discussion of this issue in Section [7](#S7
    "7 Future direction ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '关于点云的公开可用数据集在数据大小和场景多样性方面仍然受到限制，特别是与用于2D计算机视觉和NLP研究的图像和文本数据集相比。例如，BooksCorpus中有8亿个单词，英文维基百科中有25亿个单词，可以为NLP的无监督表示学习提供全面的数据来源 [[32](#bib.bib32)]；ImageNet[[33](#bib.bib33)]拥有1000万多张图像用于无监督视觉表示学习。未来研究对大规模和高质量的点云数据需求急剧增长。我们在第7节中对这个问题进行了详细讨论，参见第[7](#S7"7
    Future direction ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey")节。'
- en: 4 Common deep architectures
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4种常见的深度学习架构
- en: Over the last decade, deep learning has been playing a more important role in
    point-cloud processing and understanding. This can be observed by the abundance
    of deep architectures that have been developed in recent years. Different from
    traditional 3D vision that transforms point clouds to structures like Octrees
    [[34](#bib.bib34)] or Hashed Voxel Lists [[35](#bib.bib35)], deep learning favors
    more amenable structures for differentiability and/or efficient neural processing
    which have achieved very impressive performance over various 3D tasks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，深度学习在点云处理和理解中发挥了越来越重要的作用。这可以从最近几年开发的丰富的深度架构中看出。与将点云转换为像八叉树[[34](#bib.bib34)]或哈希体素列表[[35](#bib.bib35)]这样的结构的传统3D视觉不同，深度学习更倾向于对不同的可区分性和/或高效的神经处理更加友好的结构，这在各种3D任务中取得了非常出色的性能。
- en: At the other end, DNN-based point cloud processing and understanding lags far
    behind as compared with its counterparts in NLP and 2D computer vision. This is
    especially true for the task of unsupervised representation learning, largely
    due to the lack of regular representations in point cloud data. Specifically,
    word embeddings and 2D images have regular and well-defined structures, but point
    clouds represented by unordered point sets have no such universal and structural
    data format.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与自然语言处理（NLP）和二维计算机视觉相比，基于DNN的点云处理和理解还远远滞后。这在无监督表示学习任务中尤其明显，主要是由于点云数据缺乏规律性表示。具体而言，词嵌入和二维图像具有规律且明确定义的结构，但由无序点集表示的点云没有这种通用且结构化的数据格式。
- en: In this section, we introduce deep architectures that have been explored for
    the URL of point clouds. Deep learning for point clouds achieved significant progress
    during the last decade and we see the abundance of 3D deep architectures and 3D
    models being proposed. However, we do not have universal and ubiquitous “3D backbones”
    like VGG [[36](#bib.bib36)] or ResNet [[37](#bib.bib37)] in 2D computer vision.
    We thus focus on those frequently used architectures in the URL of point clouds
    in this survey. For clarity of description, we group them into five categories
    broadly, namely, point-based architectures, graph-based architectures, sparse
    voxel-based architectures, spatial CNN-based architectures, and Transformer-based
    architectures. Note other deep architectures also exist for various 3D tasks as
    discussed in [[2](#bib.bib2)], such as projection-based networks [[38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)],
    recurrent neural networks [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)],
    3D capsule networks [[47](#bib.bib47)], etc. However, they were not often employed
    for the URL task and thus are not detailed in this survey.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了为点云的URL探索的深度架构。过去十年中，点云的深度学习取得了显著进展，我们看到提出了大量3D深度架构和3D模型。然而，我们并没有像2D计算机视觉中的VGG
    [[36](#bib.bib36)] 或 ResNet [[37](#bib.bib37)]这样的通用“3D骨干网络”。因此，本调查集中在点云URL中常用的架构。为清晰起见，我们将其大致分为五类，即基于点的架构、基于图的架构、稀疏体素架构、空间CNN架构和基于Transformer的架构。请注意，还存在其他用于各种3D任务的深度架构，如[[2](#bib.bib2)]中讨论的投影网络[[38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]，递归神经网络[[44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46)]，3D胶囊网络[[47](#bib.bib47)]等。然而，它们不常用于URL任务，因此在本调查中未详细介绍。
- en: 4.1 Point-based deep architectures
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于点的深度架构
- en: Point-based networks were designed to process raw point clouds directly without
    point data transformations beforehand. Independent point features are usually
    first extracted by stacking networks with Multi-Layer Perceptrons (MLPs), which
    are then aggregated into global features with symmetric aggregation functions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于点的网络旨在直接处理原始点云，无需事先进行点数据转换。独立的点特征通常首先通过堆叠多层感知机（MLPs）网络提取，然后通过对称聚合函数聚合成全局特征。
- en: 'PointNet [[15](#bib.bib15)] is a pioneer point-based network as shown in Fig.
    [7](#S4.F7 "Figure 7 ‣ 4.1 Point-based deep architectures ‣ 4 Common deep architectures
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"). It stacks several MLP layers to learn point-wise features independently
    and forwards the learned features to a max-pooling layer to extract global features
    for permutation invariance. To improve PointNet, Qi et al. proposed PointNet++
    [[48](#bib.bib48)] to learn local geometry details from the neighborhood of points,
    where the set abstraction level includes sampling layer, grouping layer, and PointNet
    layer for learning local and hierarchical features. PointNet++ achieves great
    success in multiple 3D tasks including object classification and semantic segmentation.
    By taking PointNet++ as the backbone, Qi et al. designed VoteNet [[16](#bib.bib16)],
    the first point-based 3D object detection network. VoteNet adopts the Hough voting
    strategy, which generates new points around object centers and groups them with
    the surrounding points to produce 3D box proposals.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'PointNet [[15](#bib.bib15)] 是一种开创性的基于点的网络，如图 [7](#S4.F7 "Figure 7 ‣ 4.1 Point-based
    deep architectures ‣ 4 Common deep architectures ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") 所示。它堆叠了多个 MLP 层来独立学习点级特征，并将学习到的特征传递到最大池化层，以提取全局特征以实现排列不变性。为了改进
    PointNet，Qi 等人提出了 PointNet++ [[48](#bib.bib48)]，从点的邻域中学习局部几何细节，其中集合抽象层包括采样层、分组层和
    PointNet 层，用于学习局部和层次特征。PointNet++ 在包括物体分类和语义分割在内的多个 3D 任务中取得了巨大的成功。以 PointNet++
    作为骨干网络，Qi 等人设计了 VoteNet [[16](#bib.bib16)]，这是第一个基于点的 3D 物体检测网络。VoteNet 采用了霍夫投票策略，它在物体中心周围生成新点，并将这些点与周围点分组，以生成
    3D 框建议。'
- en: '![Refer to caption](img/97ae10bcc632279441afaa90cbf683d6.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/97ae10bcc632279441afaa90cbf683d6.png)'
- en: 'Figure 7: A simplified architecture of PointNet [[15](#bib.bib15)] for point
    cloud object classification, where parameters $n$ and $m$ denote point number
    and feature dimension, respectively.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：点云物体分类的 PointNet 简化架构 [[15](#bib.bib15)]，其中参数 $n$ 和 $m$ 分别表示点数和特征维度。
- en: 4.2 Graph-based deep architectures
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于图的深度架构
- en: 'Graph-based networks treat point clouds as graphs in Euclidean space with vertexes
    being points and edges capturing neighboring point relations as illustrated in
    Fig. [8](#S4.F8 "Figure 8 ‣ 4.2 Graph-based deep architectures ‣ 4 Common deep
    architectures ‣ Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey"). It works with graph convolution where filter weights are
    conditioned on edge labels and dynamically generated for individual input samples.
    This allows to reduce the degrees of freedom in the learned models by enforcing
    weight sharing and extracting localized features that can capture dependencies
    among neighboring points.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '基于图的网络将点云视为欧几里得空间中的图，顶点是点，边缘捕捉邻近点的关系，如图 [8](#S4.F8 "Figure 8 ‣ 4.2 Graph-based
    deep architectures ‣ 4 Common deep architectures ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") 所示。它使用图卷积，其中滤波器权重根据边缘标签进行条件化，并为每个输入样本动态生成。这允许通过强制权重共享来减少学习模型的自由度，并提取能够捕捉邻近点之间依赖关系的局部特征。'
- en: The Dynamic Graph Convolutional Neural Network (DGCNN) [[49](#bib.bib49)] is
    a typical graph-based network that has been frequently used for URL for point
    clouds. It is stacked with a graph convolution module named EdgeConv that performs
    convolution on graph dynamically in the feature space. DGCNN integrates EdgeConv
    into the basic version of PointNet structures for learning global shape properties
    and semantic characteristics for point cloud understanding.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 动态图卷积神经网络（DGCNN） [[49](#bib.bib49)] 是一种典型的基于图的网络，已被广泛用于点云的 URL。它堆叠了一个名为 EdgeConv
    的图卷积模块，该模块在特征空间中对图进行动态卷积。DGCNN 将 EdgeConv 集成到 PointNet 结构的基本版本中，以学习点云理解的全局形状属性和语义特征。
- en: '![Refer to caption](img/4d562b2a1a6505d9ce8b4c5298d6a749.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4d562b2a1a6505d9ce8b4c5298d6a749.png)'
- en: 'Figure 8: Schematic depiction of graph convolutional network (GCN): Each graph
    consists of multiple vertexes representing points $X_{i}$ or features $Z_{i}$
    (highlighted by circular dots), as well as edges connecting the vertexes representing
    point relations (shown as black lines). $C$ denotes input channels, $F$ denotes
    output feature dimensions, and $Y_{i}$ denotes labels.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：图卷积网络（GCN）的示意图：每个图由多个顶点组成，表示点 $X_{i}$ 或特征 $Z_{i}$（由圆点突出显示），以及连接顶点的边表示点之间的关系（以黑线显示）。$C$
    表示输入通道，$F$ 表示输出特征维度，$Y_{i}$ 表示标签。
- en: 4.3 Sparse voxel-based deep architectures
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 稀疏体素基础深度架构
- en: The voxel-based architecture voxelizes point clouds into 3D grids before applying
    3D CNN on the volumetric representations. Due to the sparseness of point cloud
    data, It often involves huge computation redundancy or sacrifices the representation
    accuracy while processing a large number of points. To overcome this constrain,
     [[50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)] adopt
    sparse tensor as the basic unit where point clouds are represented with a data
    list and an index list. Unlike standard convolution operation that employs sliding
    windows (im2col function in PyTorch and TensorFlow) to build the computational
    pipeline, sparse convolution [[50](#bib.bib50)] collects all atomic operations
    including convolution kernel elements and saves them in a Rulebook as computation
    instructions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 体素基础架构在对体积表示应用 3D CNN 之前，将点云体素化为 3D 网格。由于点云数据的稀疏性，这通常会涉及巨大的计算冗余或在处理大量点时牺牲表示准确性。为了克服这一限制，
    [[50](#bib.bib50)、[51](#bib.bib51)、[52](#bib.bib52)、[53](#bib.bib53)] 采用稀疏张量作为基本单元，其中点云用数据列表和索引列表表示。与标准卷积操作不同，后者使用滑动窗口（PyTorch
    和 TensorFlow 中的 im2col 函数）来构建计算管道，稀疏卷积 [[50](#bib.bib50)] 收集所有原子操作，包括卷积核元素，并将其保存到规则书中作为计算指令。
- en: 'Recently, Choy et al. proposed Minkowski Engine [[51](#bib.bib51)] that introduces
    generalized sparse convolution and an auto-differentiation library for sparse
    tensors. On top of that, Xie et al. [[54](#bib.bib54)] adopted a unified U-Net [[55](#bib.bib55)]
    architecture and built a backbone network (SR-UNet as shown in Fig. [9](#S4.F9
    "Figure 9 ‣ 4.3 Sparse voxel-based deep architectures ‣ 4 Common deep architectures
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey")) for unsupervised pre-training. The learned encoder can be transferred
    to different downstream tasks such as classification, object detection, and semantic
    segmentation.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Choy 等人提出了 Minkowski Engine [[51](#bib.bib51)]，引入了广义稀疏卷积和用于稀疏张量的自动微分库。此基础上，Xie
    等人 [[54](#bib.bib54)] 采用了统一 U-Net [[55](#bib.bib55)] 架构，并建立了一个主干网络（SR-UNet，如图
    [9](#S4.F9 "图 9 ‣ 4.3 稀疏体素基础深度架构 ‣ 4 常见深度架构 ‣ 无监督点云表示学习的深度神经网络：综述")）用于无监督预训练。学习到的编码器可以迁移到不同的下游任务，如分类、物体检测和语义分割。
- en: '![Refer to caption](img/1a2cd74a19831c9355ef79c38e606b91.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1a2cd74a19831c9355ef79c38e606b91.png)'
- en: 'Figure 9: An illustration of SR-UNet [[54](#bib.bib54)] that adopts a unified
    U-Net [[55](#bib.bib55)] architecture for sparse convolution. The graph is reproduced
    based on [[54](#bib.bib54)].'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：展示了采用统一 U-Net [[55](#bib.bib55)] 架构进行稀疏卷积的 SR-UNet [[54](#bib.bib54)]。该图基于
    [[54](#bib.bib54)] 重新制作。
- en: 4.4 Spatial CNN-based deep architectures
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 空间 CNN 基础深度架构
- en: 'Spatial CNN-based networks have been developed to extend the capabilities of
    regular-grid CNNs to analyze irregularly spaced point clouds. They can be divided
    into continuous and discrete convolutional networks according to the convolutional
    kernels [[2](#bib.bib2)]. As Fig. [10](#S4.F10 "Figure 10 ‣ 4.4 Spatial CNN-based
    deep architectures ‣ 4 Common deep architectures ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") shows, continuous convolutional
    networks define the convolutional kernels in a continuous space, where the weights
    of neighboring points are determined by their spatial distribution relative to
    the center point. Differently, discrete convolutional networks operate on regular
    grids and define the convolutional kernels in a discrete space where neighboring
    points have fixed offsets relative to the center point. One typical example of
    continuous convolution models is RS-CNN [[56](#bib.bib56)] which has been widely
    adopted for URL of point clouds. Specifically, RS-CNN extracts geometric topology
    relations among local centers with their surrounding points, and it learns dynamic
    weights for convolutions.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '基于空间 CNN 的网络已被开发，以扩展常规网格 CNN 的能力，以分析不规则间隔的点云。根据卷积核，它们可以分为连续卷积网络和离散卷积网络 [[2](#bib.bib2)]。如图
    [10](#S4.F10 "Figure 10 ‣ 4.4 Spatial CNN-based deep architectures ‣ 4 Common
    deep architectures ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey") 所示，连续卷积网络在连续空间中定义卷积核，其中邻近点的权重由它们相对于中心点的空间分布决定。不同的是，离散卷积网络在规则网格上操作，并在离散空间中定义卷积核，其中邻近点相对于中心点有固定的偏移量。连续卷积模型的一个典型例子是
    RS-CNN [[56](#bib.bib56)]，它已被广泛应用于点云的 URL。具体来说，RS-CNN 提取局部中心及其周围点之间的几何拓扑关系，并为卷积学习动态权重。'
- en: '![Refer to caption](img/fc717c3f21c36980ab5666688fd3b575.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fc717c3f21c36980ab5666688fd3b575.png)'
- en: 'Figure 10: An illustration of 3D spatial convolution including continuous and
    discrete convolutions. Parameters $p$ and $q_{i}$ denote the center point and
    its neighboring points, respectively. The graph is reproduced based on [[2](#bib.bib2)].'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：3D 空间卷积的示意图，包括连续卷积和离散卷积。参数 $p$ 和 $q_{i}$ 分别表示中心点及其邻近点。该图基于 [[2](#bib.bib2)]
    重制。
- en: 4.5 Transformer-based deep architectures
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 基于 Transformer 的深度架构
- en: '![Refer to caption](img/ce546d4b1ec2c4f089f76dcbb85f2857.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ce546d4b1ec2c4f089f76dcbb85f2857.png)'
- en: 'Figure 11: The architecture of point cloud Transformer that was used for unsupervised
    pre-training in Point-BERT [[57](#bib.bib57)]. More network details can be found
    in [[57](#bib.bib57)]. The figure is reproduced based on [[57](#bib.bib57), [58](#bib.bib58)].'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：用于点云无监督预训练的 Transformer 架构 [[57](#bib.bib57)]。更多网络细节见 [[57](#bib.bib57)]。该图基于
    [[57](#bib.bib57), [58](#bib.bib58)] 重制。
- en: 'Over the last few years, Transformers have made astounding progress in the
    research areas of NLP [[59](#bib.bib59), [32](#bib.bib32)] and 2D image processing [[58](#bib.bib58),
    [60](#bib.bib60)] due to their structural superiority and versatility. They have
    also been introduced into the area of point cloud processing [[61](#bib.bib61),
    [57](#bib.bib57)] recently. Fig. [11](#S4.F11 "Figure 11 ‣ 4.5 Transformer-based
    deep architectures ‣ 4 Common deep architectures ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") shows a standard Transformer architecture
    for URL of point clouds [[57](#bib.bib57)], which contains a stack of Transformer
    blocks [[59](#bib.bib59)] and each block consists of a multi-head self-attention
    layer and a feed-forward network. The unsupervised pre-trained Transformer encoder
    can be used for fine-tuning downstream tasks such as object classification and
    semantic segmentation, etc.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '在过去几年中，由于其结构优越性和多功能性，Transformers 在 NLP 研究领域 [[59](#bib.bib59), [32](#bib.bib32)]
    和 2D 图像处理 [[58](#bib.bib58), [60](#bib.bib60)] 取得了惊人的进展。最近，它们也被引入到点云处理领域 [[61](#bib.bib61),
    [57](#bib.bib57)]。图 [11](#S4.F11 "Figure 11 ‣ 4.5 Transformer-based deep architectures
    ‣ 4 Common deep architectures ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") 显示了用于点云的标准 Transformer 架构 [[57](#bib.bib57)]，其中包含一系列
    Transformer 块 [[59](#bib.bib59)]，每个块由多头自注意力层和前馈网络组成。无监督预训练的 Transformer 编码器可用于微调下游任务，如对象分类和语义分割等。'
- en: 5 Unsupervised point cloud representation learning
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 无监督点云表示学习
- en: 'In this section, we review existing URL methods for point clouds. As shown
    in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"), we broadly group existing methods
    into four categories according to their pretext tasks, including generative-based
    methods, context-based methods, multiple modal-based methods, and local descriptor-based
    methods. With this taxonomy, we sort out existing methods and systematically introduce
    them in the ensuing subsections of this section.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了现有的点云URL方法。如图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 无监督点云表示学习与深度神经网络：综述") 所示，我们根据其前提任务将现有方法大致分为四类，包括基于生成的方法、基于上下文的方法、基于多模态的方法和基于局部描述符的方法。借助这一分类法，我们整理了现有方法，并在本节的后续小节中系统介绍它们。
- en: 5.1 Generation-based methods
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于生成的方法
- en: 'TABLE II: Summary of generation-based methods for unsupervised representation
    learning of point clouds.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 点云无监督表示学习的生成方法总结。'
- en: '| Method | Published in | Category | Contribution |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发布于 | 类别 | 贡献 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| VConv-DAE [[62](#bib.bib62)] | ECCV 2016 | Completion | Learning by predicting
    missing parts in 3D grids |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| VConv-DAE [[62](#bib.bib62)] | ECCV 2016 | 完成 | 通过预测3D网格中的缺失部分进行学习 |'
- en: '| TL-Net [[63](#bib.bib63)] | ECCV 2016 | Reconstruction | Learning by 3D generation
    and 2D prediction |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| TL-Net [[63](#bib.bib63)] | ECCV 2016 | 重建 | 通过3D生成和2D预测进行学习 |'
- en: '| 3D-GAN [[64](#bib.bib64)] | NeurIPS 2016 | GAN | Pioneer GAN for 3D voxels
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 3D-GAN [[64](#bib.bib64)] | NeurIPS 2016 | GAN | 3D体素的先锋GAN |'
- en: '| 3D-DescriptorNet [[65](#bib.bib65)] | CVPR 2018 | Completion | learning with
    energy-based models for point cloud completion |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 3D-DescriptorNet [[65](#bib.bib65)] | CVPR 2018 | 完成 | 使用基于能量的模型进行点云完成的学习
    |'
- en: '| FoldingNet [[66](#bib.bib66)] | CVPR 2018 | Reconstruction | learning by
    folding 3D object surfaces |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| FoldingNet [[66](#bib.bib66)] | CVPR 2018 | 重建 | 通过折叠3D物体表面进行学习 |'
- en: '| SO-Net [[67](#bib.bib67)] | CVPR 2018 | Reconstruction | Performing hierarchical
    feature extraction on individual points and SOM nodes |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SO-Net [[67](#bib.bib67)] | CVPR 2018 | 重建 | 对单个点和SOM节点执行层次特征提取 |'
- en: '| Latent-GAN [[68](#bib.bib68)] | ICML 2018 | GAN | Pioneer GAN for raw point
    clouds and latent embeddings |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Latent-GAN [[68](#bib.bib68)] | ICML 2018 | GAN | 原始点云和潜在嵌入的先锋GAN |'
- en: '| MRT [[69](#bib.bib69)] | ECCV 2018 | Reconstruction | A new point cloud autoencoder
    with multi-grid architecture |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| MRT [[69](#bib.bib69)] | ECCV 2018 | 重建 | 一种具有多网格架构的新型点云自编码器 |'
- en: '| VIP-GAN [[70](#bib.bib70)] | AAAI 2019 | GAN | Learning by solving multi-views
    inter-prediction tasks for objects |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| VIP-GAN [[70](#bib.bib70)] | AAAI 2019 | GAN | 通过解决对象的多视角互预测任务进行学习 |'
- en: '| G-GAN [[11](#bib.bib11)] | ICLR 2019 | GAN | Pioneer GAN with graph convolution
    for point clouds |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| G-GAN [[11](#bib.bib11)] | ICLR 2019 | GAN | 具有图卷积的先锋GAN |'
- en: '| 3DCapsuleNet [[47](#bib.bib47)] | CVPR 2019 | Reconstruction | Learning with
    3D point-capsule network |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 3DCapsuleNet [[47](#bib.bib47)] | CVPR 2019 | 重建 | 使用3D点胶囊网络进行学习 |'
- en: '| L2G-AE [[71](#bib.bib71)] | ACM MM 2019 | Reconstruction | Learning by global
    and local reconstruction of point clouds |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| L2G-AE [[71](#bib.bib71)] | ACM MM 2019 | 重建 | 通过点云的全局和局部重建进行学习 |'
- en: '| MAP-VAE [[72](#bib.bib72)] | ICCV 2019 | Reconstruction | Learning by 3D
    reconstruction and half-to-half prediction |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| MAP-VAE [[72](#bib.bib72)] | ICCV 2019 | 重建 | 通过3D重建和半对半预测进行学习 |'
- en: '| PointFlow [[73](#bib.bib73)] | ICCV 2019 | Reconstruction | Learning by modeling
    point clouds as a distribution of distributions |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| PointFlow [[73](#bib.bib73)] | ICCV 2019 | 重建 | 通过将点云建模为分布的分布进行学习 |'
- en: '| PDL [[74](#bib.bib74)] | CVPR 2020 | reconstruction | A probabilistic framework
    for point distribution learning |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| PDL [[74](#bib.bib74)] | CVPR 2020 | 重建 | 点分布学习的概率框架 |'
- en: '| GraphTER [[75](#bib.bib75)] | CVPR 2020 | Reconstruction | Proposed a graph-based
    autoencoder for point clouds |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| GraphTER [[75](#bib.bib75)] | CVPR 2020 | 重建 | 提出了用于点云的图基自编码器 |'
- en: '| SA-Net [[76](#bib.bib76)] | CVPR 2020 | Completion | Learning by completing
    point cloud objects with a skip-attention mechanism |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SA-Net [[76](#bib.bib76)] | CVPR 2020 | 完成 | 通过跳跃注意机制完成点云对象进行学习 |'
- en: '| PointGrow [[77](#bib.bib77)] | WACV 2020 | Reconstruction | An autoregressive
    model that can recurrently generate point cloud samples. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| PointGrow [[77](#bib.bib77)] | WACV 2020 | 重建 | 一种可以递归生成点云样本的自回归模型。 |'
- en: '| PSG-Net [[78](#bib.bib78)] | ICCV 2021 | Reconstruction | Learning by reconstruct
    point cloud objects with seed generation |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| PSG-Net [[78](#bib.bib78)] | ICCV 2021 | 重建 | 通过种子生成重建点云对象进行学习 |'
- en: '| OcCo [[12](#bib.bib12)] | ICCV 2021 | Completion | Learning by completing
    occluded point cloud objects |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | ICCV 2021 | 完成 | 通过补全遮蔽的点云对象进行学习 |'
- en: '| Point-Bert [[57](#bib.bib57)] | CVPR 2022 | Reconstruction | Learning for
    Transformers by recovering masked tokens of 3D objects |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Point-Bert [[57](#bib.bib57)] | CVPR 2022 | 重建 | 通过恢复3D对象的遮蔽标记来学习变换器 |'
- en: '| Point-MAE [[79](#bib.bib79)] | ECCV 2022 | Reconstruction | Autoencoder transformer
    recovers masked parts from input data |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Point-MAE [[79](#bib.bib79)] | ECCV 2022 | 重建 | 自编码器变换器从输入数据中恢复被遮蔽的部分 |'
- en: '| Point-M2AE [[80](#bib.bib80)] | NeurIPS 2022 | Reconstruction | Masked autoencoder
    with hierarchical point cloud encoding and reconstruction. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Point-M2AE [[80](#bib.bib80)] | NeurIPS 2022 | 重建 | 具有分层点云编码和重建的遮蔽自编码器 |'
- en: 'Generation-based URL methods for point clouds involve the process of generating
    point cloud objects in training. According to the employed pre-text tasks, they
    can be further grouped into four subcategories including point cloud self-reconstruction
    (for generating point cloud objects that are the same as the input), point cloud
    GAN (for generating fake point cloud objects), point cloud up-sampling (for generating
    objects with denser point clouds but similar shapes) and point cloud completion
    (for predicting missing parts from incomplete point cloud objects). The ground
    truth of these URL methods are point clouds themselves. Hence, these methods require
    no human annotations and can learn in an unsupervised manner. Table [II](#S5.T2
    "TABLE II ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") shows a list of generation-based methods.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '基于生成的URL方法涉及在训练过程中生成点云对象。根据所使用的前置任务，它们可以进一步分为四个子类别，包括点云自我重建（用于生成与输入相同的点云对象）、点云GAN（用于生成虚假的点云对象）、点云上采样（用于生成形状相似但点云更密集的对象）和点云补全（用于预测不完整点云对象中的缺失部分）。这些URL方法的真实数据是点云本身。因此，这些方法不需要人工标注，并且可以以无监督的方式进行学习。表[II](#S5.T2
    "TABLE II ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey")显示了一些基于生成的方法。'
- en: 5.1.1 Learning through point cloud self-reconstruction
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 通过点云自我重建进行学习
- en: 'Networks for self-reconstruction usually encode point cloud samples into representation
    vectors and decode them back to the original input data, where shape information
    and semantic structures are extracted during this process. It belongs to one typical
    URL approach since it does not involve any human annotations. One representative
    network is autoencoder [[81](#bib.bib81)] which has an encoder network and a decoder
    network as illustrated in Fig. [12](#S5.F12 "Figure 12 ‣ 5.1.1 Learning through
    point cloud self-reconstruction ‣ 5.1 Generation-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). The encoder compresses and encodes
    a point cloud object into a low-dimensional embedding vector (i.e., codeword) [[66](#bib.bib66)],
    which is then decoded back to the 3D space by the decoder.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '自我重建的网络通常将点云样本编码成表示向量，并将其解码回原始输入数据，在此过程中提取形状信息和语义结构。这属于典型的URL方法，因为它不涉及任何人工标注。一个代表性网络是自编码器[[81](#bib.bib81)]，它具有如图[12](#S5.F12
    "Figure 12 ‣ 5.1.1 Learning through point cloud self-reconstruction ‣ 5.1 Generation-based
    methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey")所示的编码器网络和解码器网络。编码器将点云对象压缩并编码成低维嵌入向量（即代码字）[[66](#bib.bib66)]，然后由解码器解码回3D空间。'
- en: '![Refer to caption](img/d3a4a1377d70663115269ea1aaac5f1a.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d3a4a1377d70663115269ea1aaac5f1a.png)'
- en: 'Figure 12: An illustration of AutoEncoder in unsupervised point cloud representation
    learning: The Encoder learns to represent a point cloud object by a Codeword vector
    while the Decoder reconstructs the Output Object from the Codeword.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：自编码器在无监督点云表示学习中的示意图：编码器通过代码字向量学习表示点云对象，而解码器从代码字中重建输出对象。
- en: 'The model is optimized by forcing the final output to be the same as the input.
    During this process, the encoding is validated and learns by attempting to regenerate
    the input from the encoding whereas the autoencoder learns low-dimension representations
    by training the network to ignore insignificant data (“noise”) [[82](#bib.bib82)].
    Permutation invariant losses [[83](#bib.bib83)] are widely adopted as the training
    objective to describe how the input and output point cloud objects are similar
    to each other. They can be measured by Chamfer Distance $L_{\mathrm{CD}}$ or Earth
    Mover’s Distance $L_{\mathrm{EMD}}$ as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型通过强制最终输出与输入相同来进行优化。在此过程中，编码被验证并通过尝试从编码中再生输入来学习，而自编码器通过训练网络忽略不重要的数据（“噪声”）来学习低维表示
    [[82](#bib.bib82)]。排列不变的损失 [[83](#bib.bib83)] 被广泛采用作为训练目标，以描述输入和输出点云对象之间的相似性。它们可以通过
    Chamfer 距离 $L_{\mathrm{CD}}$ 或地球搬运工距离 $L_{\mathrm{EMD}}$ 来测量，如下所示：
- en: '|  | $L_{\mathrm{CD}}=\sum_{p\in P}\min_{p^{\prime}\in P^{\prime}}{&#124;&#124;p-p^{\prime}&#124;&#124;}^{2}+\sum_{p^{\prime}\in
    P^{\prime}}\min_{p\in P}{&#124;&#124;p^{\prime}-p&#124;&#124;}^{2}$ |  | (1) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\mathrm{CD}}=\sum_{p\in P}\min_{p^{\prime}\in P^{\prime}}{\|p-p^{\prime}\|^{2}}+\sum_{p^{\prime}\in
    P^{\prime}}\min_{p\in P}{\|p^{\prime}-p\|^{2}}$ |  | (1) |'
- en: '|  | $L_{\mathrm{EMD}}=\min_{\phi:P\rightarrow P^{\prime}}\sum_{x\in P}{&#124;&#124;p-\phi(p)_{2}&#124;&#124;}_{2}$
    |  | (2) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\mathrm{EMD}}=\min_{\phi:P\rightarrow P^{\prime}}\sum_{x\in P}{\|p-\phi(p)\|_{2}}$
    |  | (2) |'
- en: Where $P$ and $P^{\prime}$ denote input and output point clouds of the same
    size, $\phi:P\rightarrow P^{\prime}$ is bijection, and $p$ & $p^{\prime}$ are
    points.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P$ 和 $P^{\prime}$ 表示相同大小的输入和输出点云，$\phi:P\rightarrow P^{\prime}$ 是双射，$p$
    和 $p^{\prime}$ 是点。
- en: Self-reconstruction has been one of the most widely adopted pre-text tasks for
    URL from point clouds over the last decade. By assuming that point cloud representations
    should be generative in 3D space and predictable from 2D space, Girdhar et al.
    proposed TL-Net [[63](#bib.bib63)] that employs a 3D autoencoder to reconstruct
    3D volumetric grids and a 2D convolutional network to learn 2D features from the
    projected images. Yang et al. designed FoldingNet [[66](#bib.bib66)] that introduces
    a folding-based decoder that deforms a canonical 2D grid onto the underlying 3D
    object surface of a point cloud object. Li et al. proposed SO-Net [[67](#bib.bib67)]
    that introduces self-organizing map to learn hierarchical features of point clouds
    via self-reconstruction. Zhao et al. [[47](#bib.bib47)] extended the capsule network [[84](#bib.bib84)]
    into 3D point cloud processing and the designed 3D capsule network can learn generic
    representations from unstructured 3D data. Gao et al. [[75](#bib.bib75)] proposed
    a graph-based autoencoder that can learn intrinsic patterns of point-cloud structures
    under both global and local transformations. Chen et al. [[85](#bib.bib85)] designed
    a deep autoencoder that exploits graph topology inference and filtering for extracting
    compact representations from 3D point clouds.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 自我重建已成为过去十年中最广泛采用的从点云中获取 URL 的前置任务之一。通过假设点云表示应该在 3D 空间中是生成性的，并且可以从 2D 空间中预测，Girdhar
    等人提出了 TL-Net [[63](#bib.bib63)]，该方法使用 3D 自编码器重建 3D 体积网格，并使用 2D 卷积网络从投影图像中学习 2D
    特征。Yang 等人设计了 FoldingNet [[66](#bib.bib66)]，它引入了一种基于折叠的解码器，将规范的 2D 网格变形到点云对象的底层
    3D 物体表面。Li 等人提出了 SO-Net [[67](#bib.bib67)]，它引入了自组织映射，通过自我重建来学习点云的层次特征。Zhao 等人
    [[47](#bib.bib47)] 将胶囊网络 [[84](#bib.bib84)] 扩展到 3D 点云处理，设计的 3D 胶囊网络可以从非结构化的 3D
    数据中学习通用表示。Gao 等人 [[75](#bib.bib75)] 提出了基于图的自编码器，可以在全局和局部变换下学习点云结构的内在模式。Chen 等人
    [[85](#bib.bib85)] 设计了一个深度自编码器，利用图拓扑推断和过滤来从 3D 点云中提取紧凑的表示。
- en: Several studies explore global and local geometries to learn robust representations
    from point cloud objects [[71](#bib.bib71), [72](#bib.bib72)]. For example, [[71](#bib.bib71)]
    introduces hierarchical self-attention in the encoder for information aggregation,
    and a recurrent neural network (RNN) as the decoder for point cloud reconstruction
    locally and globally. [[72](#bib.bib72)] presents MAP-VAE that introduces a half-to-half
    prediction task that first splits a point cloud object into a front half and a
    back half and then trains an RNN to predict the back half sequence from the corresponding
    front half sequence. Several studies instead formulate point cloud reconstruction
    as a point distribution learning task [[73](#bib.bib73), [74](#bib.bib74), [77](#bib.bib77)].
    For example, [[73](#bib.bib73)] presents PointFlow which generates 3D point clouds
    by modelling the distribution of shapes and that of points given shapes. [[74](#bib.bib74)]
    presents a probabilistic framework that extracts unsupervised shape descriptors
    via point distribution learning, which associates each point with a Gaussian and
    models point clouds as the distribution of points. [[77](#bib.bib77)] presents
    an autoregressive model Pointgrow that generates diverse and realistic point cloud
    samples either from scratch or conditioned on semantic contexts.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究探讨了全球和局部几何以从点云对象中学习鲁棒的表示[[71](#bib.bib71), [72](#bib.bib72)]。例如，[[71](#bib.bib71)]
    在编码器中引入了层次自注意力用于信息聚合，并使用递归神经网络（RNN）作为解码器来进行点云的局部和全球重建。[[72](#bib.bib72)] 提出了MAP-VAE，它引入了一个半对半预测任务，首先将点云对象分成前半部分和后半部分，然后训练RNN从相应的前半部分序列预测后半部分序列。一些研究则将点云重建公式化为点分布学习任务[[73](#bib.bib73),
    [74](#bib.bib74), [77](#bib.bib77)]。例如，[[73](#bib.bib73)] 提出了PointFlow，它通过建模形状的分布和给定形状的点的分布来生成3D点云。[[74](#bib.bib74)]
    提出了一个概率框架，通过点分布学习提取无监督的形状描述符，该框架将每个点与一个高斯分布关联，并将点云建模为点的分布。[[77](#bib.bib77)] 提出了一个自回归模型Pointgrow，它可以从头开始生成多样且逼真的点云样本，或者在语义上下文中生成。
- en: Further, several studies learn point cloud representations from different object
    resolutions [[69](#bib.bib69), [78](#bib.bib78), [86](#bib.bib86)]. For example,
    Gadelha et al. [[69](#bib.bib69)] designed an autoencoder with a multi-resolution
    tree structure that learns point cloud representations via coarse-to-fine analysis.
    Yang et al. [[78](#bib.bib78)] proposed an autoencoder with a seed generation
    module that allows extraction of input-dependent point-wise features in multiple
    stages with gradually increasing resolution. Chen et al. [[86](#bib.bib86)] proposed
    to learn sampling-invariant features by reconstructing point cloud objects of
    different resolutions and minimizing Chamfer distances between them.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究从不同的对象分辨率中学习点云表示[[69](#bib.bib69), [78](#bib.bib78), [86](#bib.bib86)]。例如，Gadelha等人[[69](#bib.bib69)]
    设计了一个具有多分辨率树结构的自编码器，通过粗到细的分析学习点云表示。Yang等人[[78](#bib.bib78)] 提出了一个具有种子生成模块的自编码器，该模块允许在多个阶段提取依赖于输入的点特征，分辨率逐渐增加。Chen等人[[86](#bib.bib86)]
    提出了通过重建不同分辨率的点云对象来学习采样不变特征，并最小化它们之间的Chamfer距离。
- en: '![Refer to caption](img/6d343adcec318d5acc8946106fa82c70.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6d343adcec318d5acc8946106fa82c70.png)'
- en: 'Figure 13: An illustration of GAN which typically consists of a generator $G$
    and a discriminator $D$ that fight with each other during the training process
    (in the form of a zero-sum game, where one agent’s gain is another agent’s loss).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：GAN的示意图，通常由一个生成器$G$和一个判别器$D$组成，在训练过程中彼此对抗（以零和博弈的形式，其中一个代理的收益是另一个代理的损失）。
- en: 5.1.2 Learning through point cloud GAN
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 通过点云GAN进行学习
- en: 'Generative and Adversarial Network (GAN) [[87](#bib.bib87)] is a typical deep
    generative network. As demonstrated in Fig. [13](#S5.F13 "Figure 13 ‣ 5.1.1 Learning
    through point cloud self-reconstruction ‣ 5.1 Generation-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"), it consists of a generator and
    a discriminator. The generator aims to synthesize as realistic data samples as
    possible while the discriminator tries to differentiate real samples and synthesized
    samples. GAN thus learns to generate new data with the same statistics as the
    training set and the modeling can be formulated as a min-max problem:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）[[87](#bib.bib87)]是一个典型的深度生成网络。如图[13](#S5.F13 "图 13 ‣ 5.1.1 通过点云自重建学习
    ‣ 5.1 基于生成的方法 ‣ 5 无监督点云表示学习 ‣ 无监督点云表示学习与深度神经网络：综述")所示，它由生成器和判别器组成。生成器旨在合成尽可能逼真的数据样本，而判别器则尝试区分真实样本和合成样本。因此，GAN
    学习生成具有与训练集相同统计特征的新数据，建模可以被表述为一个最小-最大问题：
- en: '|  | $\min_{G}\max_{D}L_{GAN}=\log D(x)+\log(1-D(G(z))),$ |  | (3) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{G}\max_{D}L_{GAN}=\log D(x)+\log(1-D(G(z))),$ |  | (3) |'
- en: where $G$ is the generator and $D$ represents the discriminator. $x$ and $z$
    represent a real sample and a randomly sampled noise vector from a distribution
    $p(z)$, respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G$ 是生成器，$D$ 代表判别器。$x$ 和 $z$ 分别代表真实样本和从分布 $p(z)$ 随机抽样的噪声向量。
- en: When training GANs for URL of point clouds, the generator learns from either
    a sampled vector or a latent embedding to generate point cloud instances, while
    the discriminator tries to distinguish whether input point clouds are from real
    data distribution or generated data distribution. The two sub-networks fight with
    each other during the training process and the discriminator learns to extract
    useful feature representations for point cloud object recognition. The learning
    process involves no human annotations thus the networks can be trained in an unsupervised
    learning manner. After that, the learned discriminator is extended into various
    downstream tasks such as object classification or part segmentation by fine-tuning
    the model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在对点云进行 URL 的 GAN 训练中，生成器从采样向量或潜在嵌入中学习以生成点云实例，而判别器则尝试区分输入点云是否来自真实数据分布或生成的数据分布。在训练过程中，这两个子网络相互对抗，判别器学习提取有用的特征表示以进行点云对象识别。学习过程无需人工标注，因此网络可以以无监督的方式进行训练。之后，通过微调模型，将学习到的判别器扩展到各种下游任务，如对象分类或部件分割。
- en: Several networks employ GAN for URL for point clouds successfully [[64](#bib.bib64),
    [68](#bib.bib68), [11](#bib.bib11), [88](#bib.bib88)]. For example, Wu et al. [[64](#bib.bib64)]
    proposed the first GAN model applying for 3D voxels. However, the voxelization
    process either sacrifices the representation accuracy or incurs huge redundancies.
    Achlioptas et al. proposed Latent-GAN [[68](#bib.bib68)] as the first GAN model
    for raw point clouds. Li et al. [[88](#bib.bib88)] further proposed a point cloud
    GAN model with a hierarchical sampling and inference network that learns a stochastic
    procedure to generate new point cloud objects. Valsesia et al. [[11](#bib.bib11)]
    designed the first graph-based GAN model to extract localized features from point
    clouds. These methods evaluated the generalization of the learned representations
    by fine-tuning them to the high-level downstream 3D tasks.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 几个网络成功地将 GAN 应用于点云的 URL [[64](#bib.bib64), [68](#bib.bib68), [11](#bib.bib11),
    [88](#bib.bib88)]。例如，Wu 等人[[64](#bib.bib64)] 提出了第一个应用于 3D 体素的 GAN 模型。然而，体素化过程要么牺牲表示精度，要么产生巨大的冗余。Achlioptas
    等人提出了 Latent-GAN [[68](#bib.bib68)]，这是第一个用于原始点云的 GAN 模型。Li 等人[[88](#bib.bib88)]
    进一步提出了一种具有层次采样和推理网络的点云 GAN 模型，该模型学习一种随机过程以生成新的点云对象。Valsesia 等人[[11](#bib.bib11)]
    设计了第一个基于图的 GAN 模型，用于从点云中提取局部特征。这些方法通过将学习到的表示微调到高级下游 3D 任务来评估其泛化能力。
- en: 5.1.3 Learning through point cloud up-sampling
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 通过点云上采样进行学习
- en: 'As shown in Fig. [14](#S5.F14 "Figure 14 ‣ 5.1.3 Learning through point cloud
    up-sampling ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), given a set of points, point cloud up-sampling aims to generate a
    denser set of points with similar geometries. This task requires deep point cloud
    networks to learn underlying geometries of 3D shapes without any supervision,
    and the learned representations can be used for fine-tuning in 3D downstream tasks.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[14](#S5.F14 "图14 ‣ 5.1.3 通过点云上采样进行学习 ‣ 5.1 基于生成的方法 ‣ 5 无监督点云表示学习 ‣ 使用深度神经网络的无监督点云表示学习综述")所示，给定一组点，点云上采样旨在生成一个具有相似几何形状的更密集的点集。此任务需要深度点云网络在没有任何监督的情况下学习3D形状的潜在几何形状，学习到的表示可以用于3D下游任务中的微调。
- en: '![Refer to caption](img/65f332a3f713430b0d616cd556897676.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/65f332a3f713430b0d616cd556897676.png)'
- en: 'Figure 14: An illustration of point cloud up-sampling: The network DNN learns
    point cloud representations by solving a pre-text task that reproduces an object
    with the same geometry but denser point distribution.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：点云上采样的示意图：网络DNN通过解决一个再现具有相同几何形状但点分布更密集的对象的前置任务来学习点云表示。
- en: 'Li et al. [[89](#bib.bib89)] introduced GAN into the point cloud up-sampling
    task and presented PU-GAN to learn a variety of point distributions from the latent
    space by up-sampling points over patches on object surfaces. The generator aims
    to produce up-sampled point clouds while the discriminator tries to distinguish
    whether its input point cloud is produced by the generator or the real one. Similar
    to GANs introduced in Section [5.1.2](#S5.SS1.SSS2 "5.1.2 Learning through point
    cloud GAN ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), the learned discriminator can be transferred in downstream tasks.
    Remelli et al. [[90](#bib.bib90)] designed an autoencoder that can up-sample sparse
    point clouds into dense representations. The learned weight of the encoder can
    also be used as initialization weights for downstream tasks as described in Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Learning through point cloud self-reconstruction ‣ 5.1 Generation-based
    methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey"). Though point
    cloud up-sampling is attracting increasing attention in recent years [[91](#bib.bib91),
    [92](#bib.bib92), [89](#bib.bib89), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)],
    it is largely evaluated by the quality of generated point clouds while its performance
    in transfer learning has not been well studied.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人[[89](#bib.bib89)]将GAN引入到点云上采样任务中，并提出了PU-GAN，通过在物体表面上的补丁上对点进行上采样，从潜在空间中学习各种点分布。生成器旨在生成上采样的点云，而判别器则试图区分其输入的点云是由生成器生成的还是实际的点云。类似于第[5.1.2](#S5.SS1.SSS2
    "5.1.2 通过点云GAN进行学习 ‣ 5.1 基于生成的方法 ‣ 5 无监督点云表示学习 ‣ 使用深度神经网络的无监督点云表示学习综述")节中介绍的GAN，学习到的判别器可以转移到下游任务中。Remelli等人[[90](#bib.bib90)]设计了一种自编码器，可以将稀疏点云上采样为密集表示。编码器学习到的权重也可以用作下游任务的初始化权重，如第[5.1.1](#S5.SS1.SSS1
    "5.1.1 通过点云自重建进行学习 ‣ 5.1 基于生成的方法 ‣ 5 无监督点云表示学习 ‣ 使用深度神经网络的无监督点云表示学习综述")节中所述。尽管近年来点云上采样受到越来越多的关注[[91](#bib.bib91),
    [92](#bib.bib92), [89](#bib.bib89), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]，但其性能主要通过生成的点云质量来评估，而在迁移学习中的表现尚未得到充分研究。
- en: '![Refer to caption](img/198422e52973d56812dce34bdd4336c1.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/198422e52973d56812dce34bdd4336c1.png)'
- en: 'Figure 15: The pipeline of OcCo [[12](#bib.bib12)]. Taking occluded point cloud
    objects as input, an encoder-decoder model is trained to complete the occluded
    point clouds, where the encoder learns point cloud representations and the decoder
    learns to generate complete objects. The learned encoder weights can be used for
    network initialization for downstream tasks. The figure is from [[12](#bib.bib12)]
    with author’s permission.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：OcCo[[12](#bib.bib12)]的流程图。以遮挡的点云对象作为输入，训练一个编码器-解码器模型来完成遮挡的点云，其中编码器学习点云表示，解码器学习生成完整对象。学习到的编码器权重可以用于下游任务的网络初始化。图来自[[12](#bib.bib12)]，经作者许可使用。
- en: 5.1.4 Learning through point cloud completion
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 通过点云补全进行学习
- en: 'TABLE III: Summary of context-based methods for unsupervised representation
    learning of point clouds.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：基于上下文的方法在无监督点云表示学习中的总结。
- en: '| Method | Published in | Category | Contribution |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表在 | 类别 | 贡献 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MultiTask [[96](#bib.bib96)] | ICCV 2019 | Hybrid | Learning by clustering,
    reconstruction, and self-supervised classification |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| MultiTask [[96](#bib.bib96)] | ICCV 2019 | 混合型 | 通过聚类、重建和自监督分类进行学习 |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | NeurIPS 2019 | Spatial-context | Learning by
    solving 3D jigsaws |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | NeurIPS 2019 | 空间上下文 | 通过解决3D拼图进行学习 |'
- en: '| Constrast&Cluster [[97](#bib.bib97)] | 3DV 2019 | Hybrid | Learning by contrasting
    and clustering with GNN |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Constrast&Cluster [[97](#bib.bib97)] | 3DV 2019 | 混合型 | 通过对比和聚类与GNN进行学习 |'
- en: '| GLR [[98](#bib.bib98)] | CVPR 2020 | Hybrid | Learning by global-local reasoning
    for 3D objects |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| GLR [[98](#bib.bib98)] | CVPR 2020 | 混合型 | 通过全球-局部推理进行3D物体学习 |'
- en: '| Info3D [[99](#bib.bib99)] | ECCV 2020 | Context-similarity | Learning by
    contrasting global and local parts of objects |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Info3D [[99](#bib.bib99)] | ECCV 2020 | 上下文相似性 | 通过对比物体的全局和局部部分进行学习 |'
- en: '| PointContrast [[54](#bib.bib54)] | ECCV 2020 | Context-similarity | Learning
    by contrasting different views of scene point clouds |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| PointContrast [[54](#bib.bib54)] | ECCV 2020 | 上下文相似性 | 通过对比场景点云的不同视角进行学习
    |'
- en: '| ACD [[100](#bib.bib100)] | ECCV 2020 | Context-similarity | Learning by contrasting
    convex components decomposed from 3D objects |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ACD [[100](#bib.bib100)] | ECCV 2020 | 上下文相似性 | 通过对比从3D物体中分解出的凸成分进行学习 |'
- en: '| Rotation3D [[101](#bib.bib101)] | 3DV 2020 | Spatial-context | Learning by
    predicting rotation angles |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Rotation3D [[101](#bib.bib101)] | 3DV 2020 | 空间上下文 | 通过预测旋转角度进行学习 |'
- en: '| HNS [[102](#bib.bib102)] | ACM MM 2021 | Context-similarity | Learning by
    contrasting local patches of 3D objects with hard negative sampling |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| HNS [[102](#bib.bib102)] | ACM MM 2021 | 上下文相似性 | 通过对比3D物体的局部补丁与困难负样本进行学习
    |'
- en: '| CSC [[3](#bib.bib3)] | CVPR 2021 | Context-similarity | Techniques to improve
    contrasting scene point cloud views |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| CSC [[3](#bib.bib3)] | CVPR 2021 | 上下文相似性 | 提高场景点云视角对比的技术 |'
- en: '| STRL [[1](#bib.bib1)] | ICCV 2021 | Temporal-context | Learning spatio-temporal
    data invariance from point cloud sequences |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| STRL [[1](#bib.bib1)] | ICCV 2021 | 时间上下文 | 从点云序列中学习时空数据的不变性 |'
- en: '| RandomRooms [[103](#bib.bib103)] | ICCV 2021 | Context-similarity | Constructing
    pseudo scenes with synthetic objects for contrastive learning |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| RandomRooms [[103](#bib.bib103)] | ICCV 2021 | 上下文相似性 | 构建带有合成物体的伪场景以进行对比学习
    |'
- en: '| DepthContrast [[104](#bib.bib104)] | ICCV 2021 | Context-similarity | Joint
    contrastive learning with points and voxels |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| DepthContrast [[104](#bib.bib104)] | ICCV 2021 | 上下文相似性 | 通过点和体素的联合对比学习 |'
- en: '| SelfCorrection [[105](#bib.bib105)] | ICCV 2021 | Hybrid | Learning by distinguishing
    and restoring destroyed objects |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| SelfCorrection [[105](#bib.bib105)] | ICCV 2021 | 混合型 | 通过区分和恢复破坏的物体进行学习
    |'
- en: '| PC-FractalDB [[106](#bib.bib106)] | CVPR 2022 | Context-similarity | Leveraging
    fractal geometry to generate high-quality pre-training data |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| PC-FractalDB [[106](#bib.bib106)] | CVPR 2022 | 上下文相似性 | 利用分形几何生成高质量的预训练数据
    |'
- en: '| 4dcontrast [[107](#bib.bib107)] | ECCV 2022 | Temporal-context | Learning
    by contrasting dynamic correspondences from 3D scene sequences |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 4dcontrast [[107](#bib.bib107)] | ECCV 2022 | 时间上下文 | 通过对比3D场景序列中的动态对应关系进行学习
    |'
- en: '| DPCo [[108](#bib.bib108)] | ECCV 2022 | Context-similarity | A unified contrastive-learning
    framework for point cloud pre-training |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| DPCo [[108](#bib.bib108)] | ECCV 2022 | 上下文相似性 | 一个用于点云预训练的统一对比学习框架 |'
- en: '| ProposalContrast [[109](#bib.bib109)] | ECCV 2022 | Context-similarity |
    Pre-training 3D detectors by contrasting region proposals |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| ProposalContrast [[109](#bib.bib109)] | ECCV 2022 | 上下文相似性 | 通过对比区域提议进行3D探测器的预训练
    |'
- en: '| MaskPoint [[110](#bib.bib110)] | ECCV 2022 | Context-similarity | Learning
    by discriminating masked object points and sampled noise points |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| MaskPoint [[110](#bib.bib110)] | ECCV 2022 | 上下文相似性 | 通过区分遮挡物体点和采样噪声点进行学习
    |'
- en: '| FAC [[111](#bib.bib111)] | CVPR 2023 | Context-similarity | Learning by contrasting
    between grouped foreground and background |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| FAC [[111](#bib.bib111)] | CVPR 2023 | 上下文相似性 | 通过对比分组的前景和背景进行学习 |'
- en: Point cloud completion is a task to predict arbitrary missing parts based on
    the rest of the 3D point clouds. To achieve this target, deep networks need to
    learn inner geometric structures and semantic knowledge of the 3D objects so as
    to correctly predict missing parts. On top of that, the learned representations
    can be transferred to downstream tasks. The whole process involves no human annotations
    and thus belongs to unsupervised representation learning.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 点云完成是一项基于其余3D点云预测任意缺失部分的任务。为了实现这一目标，深度网络需要学习3D物体的内部几何结构和语义知识，以便正确预测缺失部分。除此之外，学习到的表示可以迁移到下游任务。整个过程无需人工标注，因此属于无监督表示学习。
- en: 'Point cloud completion has been an active research area over the past decade [[112](#bib.bib112),
    [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115), [76](#bib.bib76),
    [116](#bib.bib116), [117](#bib.bib117)] with evaluation in different URL benchmarks
    [[62](#bib.bib62), [65](#bib.bib65), [76](#bib.bib76), [12](#bib.bib12)]. A pioneer
    work VConv-DAE [[62](#bib.bib62)] voxelizes point cloud objects into volumetric
    grids and learns object shape distributions with an autoencoder by predicting
    the missing voxels from the rest parts. Xie et al. [[65](#bib.bib65)] designed
    3D-DescriptorNet for probabilistic modeling of volumetric shape patterns. Achlioptas
    et al. [[68](#bib.bib68)] introduced the first DNN for raw point cloud completion
    which is a point-based network with an encoder-decoder structure. Yuan et al. [[113](#bib.bib113)]
    proposed a Point Completion Network, an autoencoder structured network for learning
    useful representations by repairing incomplete point cloud objects. Wen et al. [[76](#bib.bib76)]
    proposed SA-Net, which introduces a skip-attention mechanism in the encoder that
    selectively transfers geometric information from the local regions to the decoder
    for generating complete point cloud objects. Wang et al. [[12](#bib.bib12)] proposed
    to learn an encoder-decoder model that recovers the occluded points by different
    camera views as shown in Fig. [15](#S5.F15 "Figure 15 ‣ 5.1.3 Learning through
    point cloud up-sampling ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point
    cloud representation learning ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey"). The encoder parameters are used as initialization
    for downstream tasks including classification, part segmentation, and semantic
    segmentation.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '点云完成在过去十年中一直是一个活跃的研究领域[[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [76](#bib.bib76), [116](#bib.bib116), [117](#bib.bib117)]，并在不同的URL基准测试中进行评估[[62](#bib.bib62),
    [65](#bib.bib65), [76](#bib.bib76), [12](#bib.bib12)]。一项开创性的工作是VConv-DAE [[62](#bib.bib62)]，它将点云对象体素化为体积网格，并通过从其余部分预测缺失的体素来学习物体形状分布。Xie等人[[65](#bib.bib65)]设计了3D-DescriptorNet，用于体积形状模式的概率建模。Achlioptas等人[[68](#bib.bib68)]介绍了首个用于原始点云完成的深度神经网络，这是一种具有编码器-解码器结构的基于点的网络。Yuan等人[[113](#bib.bib113)]提出了Point
    Completion Network，这是一种具有自动编码器结构的网络，通过修复不完整的点云对象来学习有用的表示。Wen等人[[76](#bib.bib76)]提出了SA-Net，它在编码器中引入了跳跃注意机制，选择性地将几何信息从局部区域转移到解码器，以生成完整的点云对象。Wang等人[[12](#bib.bib12)]提出学习一个编码器-解码器模型，通过不同的相机视角恢复被遮挡的点，如图[15](#S5.F15
    "Figure 15 ‣ 5.1.3 Learning through point cloud up-sampling ‣ 5.1 Generation-based
    methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey")所示。编码器参数被用作下游任务的初始化，包括分类、部分分割和语义分割。'
- en: Recently, recovering missing parts from masked input as the pre-text task of
    URL has been proved remarkably successful in NLP [[5](#bib.bib5), [6](#bib.bib6)]
    and 2D computer vision [[10](#bib.bib10)]. Such idea has also been investigated
    in 3D point cloud learning [[57](#bib.bib57), [79](#bib.bib79), [110](#bib.bib110),
    [118](#bib.bib118)]. For example, Yu et al. [[57](#bib.bib57)] proposed a Point-BERT
    paradigm that pre-trains point cloud Transformers through a masked point modeling
    task. They use a discrete variational autoencoder to generate tokens for object
    patches and randomly masked out the tokens to train the Transformer to recover
    the original complete point tokens. The representations learned by Point-BERT
    can be well transferred to new tasks and domains such as object classification
    and object part segmentation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，利用掩码输入恢复缺失部分作为点云URL的前文本任务在自然语言处理[[5](#bib.bib5), [6](#bib.bib6)]和二维计算机视觉[[10](#bib.bib10)]中已经证明了其显著成功。这一思想也在三维点云学习中得到了探讨[[57](#bib.bib57),
    [79](#bib.bib79), [110](#bib.bib110), [118](#bib.bib118)]。例如，Yu等人[[57](#bib.bib57)]提出了一种Point-BERT范式，通过掩码点建模任务对点云Transformer进行预训练。他们使用离散变分自编码器生成对象补丁的标记，并随机掩盖这些标记以训练Transformer恢复原始完整的点标记。Point-BERT学习到的表示可以很好地迁移到新的任务和领域，如对象分类和对象部件分割。
- en: 5.2 Context-based methods
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基于上下文的方法
- en: 'Context-based methods are another important category of URL of point clouds
    that has attracted increasing attention in recent years. Different from generation-based
    methods that learn representations in a generative way, these methods employ discriminative
    pre-text tasks to learn different contexts of point clouds including context similarity,
    spatial context structures, and temporal context structures. The designed pre-text
    tasks require no human annotations and Table [III](#S5.T3 "TABLE III ‣ 5.1.4 Learning
    through point cloud completion ‣ 5.1 Generation-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") lists the recent methods.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上下文的方法是另一类重要的点云URL，近年来受到越来越多的关注。这些方法不同于以生成方式学习表示的生成方法，它们采用区分性的前文本任务来学习点云的不同上下文，包括上下文相似性、空间上下文结构和时间上下文结构。设计的前文本任务无需人工标注，表[III](#S5.T3
    "表 III ‣ 5.1.4 通过点云完成学习 ‣ 5.1 生成方法 ‣ 5 无监督点云表示学习 ‣ 基于深度神经网络的无监督点云表示学习：综述")列出了最近的方法。
- en: 5.2.1 Learning with context similarity
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 基于上下文相似性的学习
- en: '![Refer to caption](img/504beb3b6807f02d1435621d8a00d2a1.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/504beb3b6807f02d1435621d8a00d2a1.png)'
- en: 'Figure 16: An illustration of instance contrastive learning that learns locally
    smooth representations by self-discrimination, which pulls Query (from the Anchor
    sample) close to Positive Key (from Positive Samples) and pushes it away from
    Negative Keys (from Negative Samples).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：一个实例对比学习的示意图，通过自我区分学习局部平滑的表示，将查询（来自Anchor样本）拉近到正键（来自正样本）并将其推远离负键（来自负样本）。
- en: 'This type of method learns unsupervised representations of point clouds by
    exploring underlying context similarities between samples. A typical approach
    is contrastive learning, which has demonstrated superior performances in both
    2D vision [[7](#bib.bib7), [8](#bib.bib8), [119](#bib.bib119)] and 3D vision [[54](#bib.bib54),
    [3](#bib.bib3), [104](#bib.bib104)] in recent years. Fig. [16](#S5.F16 "Figure
    16 ‣ 5.2.1 Learning with context similarity ‣ 5.2 Context-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") provides an illustration of instance-wise
    contrastive learning. Given one input point cloud object instance as the anchor,
    its augmented views are defined as the positive samples while other different
    instances are negative samples. The network learns representations of point clouds
    by optimizing a self-discrimination task, i.e. query (feature of the anchor) should
    be close to the positive keys (features of positive samples) and faraway from
    its negative keys (features of negative samples). This learning strategy groups
    representations of similar samples together in an unsupervised manner and helps
    networks to learn semantic structures from unlabelled data distribution. The InfoNCE
    loss [[120](#bib.bib120)] defined below and its variants are often employed as
    the objective function in training:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '这种方法通过探索样本之间的潜在上下文相似性来学习点云的无监督表示。一种典型的方法是对比学习，它在近年来在2D视觉 [[7](#bib.bib7), [8](#bib.bib8),
    [119](#bib.bib119)] 和3D视觉 [[54](#bib.bib54), [3](#bib.bib3), [104](#bib.bib104)]
    中表现出色。图 [16](#S5.F16 "Figure 16 ‣ 5.2.1 Learning with context similarity ‣ 5.2
    Context-based methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey") 展示了实例级对比学习的示例。以一个输入点云对象实例作为锚点，其增强视图被定义为正样本，而其他不同的实例则为负样本。网络通过优化自我辨别任务来学习点云的表示，即查询（锚点的特征）应该接近正键（正样本的特征），而远离负键（负样本的特征）。这种学习策略以无监督的方式将相似样本的表示聚集在一起，并帮助网络从未标记的数据分布中学习语义结构。以下定义的
    InfoNCE 损失 [[120](#bib.bib120)] 及其变体通常用作训练中的目标函数：'
- en: '|  | $\mathcal{L}_{\mathrm{InfoNCE}}=-\log\frac{\exp{(q\cdot k_{+}/\tau)}}{\sum_{i=0}^{K}{\exp(q\cdot
    k_{i}/\tau)}},$ |  | (4) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathrm{InfoNCE}}=-\log\frac{\exp{(q\cdot k_{+}/\tau)}}{\sum_{i=0}^{K}{\exp(q\cdot
    k_{i}/\tau)}},$ |  | (4) |'
- en: where $q$ is encoded query, $\{k_{0},k_{1},k_{2},...\}$ are keys with $k_{+}$
    being the positive key, $\tau$ is a temperature hyper-parameter that controls
    how the distribution concentrates.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q$ 是编码的查询，$\{k_{0},k_{1},k_{2},...\}$ 是键，其中 $k_{+}$ 是正键，$\tau$ 是控制分布集中度的温度超参数。
- en: Similar to generation-based methods, different contrastive learning methods
    [[99](#bib.bib99), [121](#bib.bib121), [100](#bib.bib100), [102](#bib.bib102),
    [122](#bib.bib122)] have been proposed to learn representations on synthetic single
    objects. For example, Sanghi et al. [[99](#bib.bib99)] proposed to learn useful
    feature representations by maximizing mutual information between synthetic objects
    and their local parts. Wang et al. [[121](#bib.bib121)] proposed a hybrid contrastive
    learning strategy that uses objects of different resolutions for instance-level
    contrast for capturing hierarchical global representations and simultaneously
    contrasted points and instances for learning local features. Gadelha et al. [[100](#bib.bib100)]
    decompose 3D objects into convex components and construct positive pairs among
    the same components and negative pairs among different components for contrastive
    learning. Du et al. [[102](#bib.bib102)] introduced a hard negative sampling strategy
    into the contrastive learning between instances and local parts. Besides, Rao
    et al. [[98](#bib.bib98)] unified contrastive learning, normal estimation, and
    self-reconstruction into the same framework and formulated a multi-task learning
    method.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于生成基础的方法，不同的对比学习方法 [[99](#bib.bib99), [121](#bib.bib121), [100](#bib.bib100),
    [102](#bib.bib102), [122](#bib.bib122)] 被提出用于学习合成单个对象的表示。例如，Sanghi 等人 [[99](#bib.bib99)]
    提出了通过最大化合成对象及其局部部分之间的互信息来学习有用的特征表示。Wang 等人 [[121](#bib.bib121)] 提出了一种混合对比学习策略，该策略使用不同分辨率的对象进行实例级对比，以捕获层次化的全局表示，同时对比点和实例以学习局部特征。Gadelha
    等人 [[100](#bib.bib100)] 将3D对象分解为凸组件，并在相同组件之间构建正对和在不同组件之间构建负对用于对比学习。Du 等人 [[102](#bib.bib102)]
    将困难负样本采样策略引入实例和局部部分之间的对比学习。此外，Rao 等人 [[98](#bib.bib98)] 将对比学习、法线估计和自我重建统一到同一框架中，并制定了一种多任务学习方法。
- en: '![Refer to caption](img/87fc7b061dd3aa400e4441d136febf82.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/87fc7b061dd3aa400e4441d136febf82.png)'
- en: 'Figure 17: The pipeline of PointContrast [[54](#bib.bib54)]: Two scans $x^{1}$
    and $x^{2}$ of the same scene captured from two different viewpoints are transformed
    by $T_{1}$ and $T_{2}$ for data augmentation. The correspondence mapping between
    the two views is computed to minimize the distance for matched point features
    and maximize the distance for unmatched point features for contrastive learning.
    The graph is extracted from [[54](#bib.bib54)] with authors’ permission.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：PointContrast [[54](#bib.bib54)] 的流程：从两个不同视角捕获的同一场景的两个扫描 $x^{1}$ 和 $x^{2}$
    通过 $T_{1}$ 和 $T_{2}$ 进行数据增强。计算两个视角之间的对应映射，以最小化匹配点特征的距离并最大化不匹配点特征的距离，以进行对比学习。该图摘自
    [[54](#bib.bib54)]，经作者许可。
- en: 'Recently, Xie et. al proposed PointContrast [[54](#bib.bib54)], a contrastive
    learning framework that learns representations of scene point clouds as illustrated
    in Fig. [17](#S5.F17 "Figure 17 ‣ 5.2.1 Learning with context similarity ‣ 5.2
    Context-based methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey"). The
    work shows, for the first time, that network weights pre-trained on 3D scene partial
    frames can lead to performance boosts when fine-tuned on multiple 3D high-level
    tasks including object classification, semantic segmentation, and object detection.
    Firstly, dense correspondences are extracted between two aligned views of ScanNet [[18](#bib.bib18)]
    to build point pairs and point-level contrastive learning is then conducted with
    a unified backbone (SR-UNet). Finally, the learned model was transferred to multiple
    downstream 3D tasks including classification, semantic segmentation, and object
    detection with consistent performance gains.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Xie 等人提出了 PointContrast [[54](#bib.bib54)]，一个对比学习框架，学习场景点云的表示，如图 [17](#S5.F17
    "图 17 ‣ 5.2.1 上下文相似度学习 ‣ 5.2 基于上下文的方法 ‣ 5 无监督点云表示学习 ‣ 使用深度神经网络进行无监督点云表示学习：综述")
    所示。这项工作首次表明，网络权重在 3D 场景局部帧上预训练后，可以在多个 3D 高级任务（包括物体分类、语义分割和物体检测）上进行微调，从而提升性能。首先，在
    ScanNet [[18](#bib.bib18)] 的两个对齐视图之间提取密集的对应关系以构建点对，然后使用统一的骨干网（SR-UNet）进行点级对比学习。最后，将学习到的模型转移到包括分类、语义分割和物体检测在内的多个下游
    3D 任务中，取得了一致的性能提升。
- en: Since PointContrast brought new insights that the unsupervised representation
    learned from scene-level point clouds can generalize across domains and boost
    high-level scene understanding tasks, several unsupervised pre-training works
    are proposed for scene-level 3D tasks. Considering that PointContrast focuses
    on point-level alignment without capturing spatial configurations and contexts
    in scenes, Hou et al. [[3](#bib.bib3)] integrated spatial contexts into the pre-training
    objective by partitioning the space into spatially inhomogeneous cells for correspondence
    matching. Hou et al. [[123](#bib.bib123)] built a multi-modal contrastive learning
    framework that models 2D multi-view correspondences as well as 2D-3D correspondences
    with geometry-to-image alignment. While the aforementioned works [[54](#bib.bib54),
    [3](#bib.bib3), [123](#bib.bib123)] require 3D data captured from multiple camera
    views, Zhang et al. [[104](#bib.bib104)] proposed DepthContrast that can work
    with single-view data. Instead of using real point clouds as previous methods,
    Rao et al. [[103](#bib.bib103)] generated synthetic scenes and objects from ShapeNet [[14](#bib.bib14)]for
    network pre-training.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 PointContrast 带来了新的见解，表明从场景级点云学习的无监督表示可以跨领域泛化并提升高级场景理解任务，几项无监督预训练工作已被提出用于场景级
    3D 任务。考虑到 PointContrast 专注于点级对齐而没有捕捉场景中的空间配置和背景，Hou 等人 [[3](#bib.bib3)] 通过将空间划分为空间不均匀的单元来进行对应匹配，将空间背景集成到预训练目标中。Hou
    等人 [[123](#bib.bib123)] 建立了一个多模态对比学习框架，该框架模型化 2D 多视角对应关系以及 2D-3D 对应关系与几何到图像对齐。虽然上述工作
    [[54](#bib.bib54), [3](#bib.bib3), [123](#bib.bib123)] 需要从多个相机视角捕获的 3D 数据，但 Zhang
    等人 [[104](#bib.bib104)] 提出了 DepthContrast，可以处理单视角数据。与之前的方法使用真实点云不同，Rao 等人 [[103](#bib.bib103)]
    从 ShapeNet [[14](#bib.bib14)] 生成了合成场景和对象用于网络预训练。
- en: Another unsupervised approach to learn context similarity is clustering. In
    this approach, samples are first grouped into clusters by clustering algorithms
    such as K-Means [[124](#bib.bib124)] and each sample is assigned a cluster ID
    as pseudo-label. Then networks are trained in a supervised manner to learn semantic
    structures of data distribution. The learned parameters are used for model initialization
    for fine-tuning various downstream tasks. A typical example is DeepClustering [[125](#bib.bib125)]
    which is the first unsupervised clustering method for 2D visual representation
    learning. However, no prior studies adopted a purely clustering strategy for URL
    of point clouds. Instead, hybrid approaches are proposed by integrating clustering
    with other unsupervised learning approaches (e.g., self-reconstruction [[96](#bib.bib96)]
    or contrastive learning [[97](#bib.bib97)]) for learning more robust representations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种无监督的上下文相似性学习方法是聚类。在这种方法中，样本首先通过诸如K-Means [[124](#bib.bib124)]的聚类算法被分组到不同的聚类中，并且每个样本被分配一个聚类ID作为伪标签。然后，网络以监督方式进行训练，以学习数据分布的语义结构。学习到的参数用于模型初始化，以微调各种下游任务。一个典型的例子是DeepClustering
    [[125](#bib.bib125)]，这是第一个用于二维视觉表示学习的无监督聚类方法。然而，之前的研究并没有采用纯粹的聚类策略来处理点云的URL。相反，提出了将聚类与其他无监督学习方法（例如，自我重建
    [[96](#bib.bib96)] 或对比学习 [[97](#bib.bib97)]) 结合的混合方法，以学习更鲁棒的表示。
- en: 5.2.2 Learning with spatial context structure
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 利用空间上下文结构进行学习
- en: Point clouds with spatial coordinates provides accurate geometric description
    of 3D shapes of objects and scenes. The rich spatial contexts in point clouds
    can be exploited in pre-text tasks for URL. For example, networks can be trained
    to sort out the relation of different object parts. Likewise, the learned parameters
    can be used for model initialization for downstream tasks. Since no human annotations
    are required in training, the key is to design effective pre-text tasks to exploit
    spatial contexts as URL objectives.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 带有空间坐标的点云提供了对象和场景的三维形状的准确几何描述。点云中的丰富空间上下文可以在URL的预文本任务中加以利用。例如，可以训练网络来理清不同对象部分之间的关系。同样，学习到的参数可以用于下游任务的模型初始化。由于训练中不需要人工标注，关键是设计有效的预文本任务以利用空间上下文作为URL目标。
- en: '![Refer to caption](img/e48ac3aa0357c58b7d3a84fea99b13e7.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e48ac3aa0357c58b7d3a84fea99b13e7.png)'
- en: 'Figure 18: The pipeline of 3DJigsaw [[13](#bib.bib13)]: An object is split
    into voxels where each point is assigned with a voxel label. The split voxels
    are randomly rearranged via pre-processing, and a deep neural network is trained
    to predict the voxel label for each point. The graph is reproduced based on [[13](#bib.bib13)].'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：3DJigsaw [[13](#bib.bib13)] 的流程：一个对象被拆分成体素，每个点被分配一个体素标签。拆分的体素通过预处理被随机重新排列，深度神经网络被训练以预测每个点的体素标签。该图表基于
    [[13](#bib.bib13)] 进行再现。
- en: 'The method Jigsaw3D [[13](#bib.bib13)] proposed by Sauder et al. is one of
    the pioneer works that use spatial context for URL of point clouds. As illustrated
    in Fig. [18](#S5.F18 "Figure 18 ‣ 5.2.2 Learning with spatial context structure
    ‣ 5.2 Context-based methods ‣ 5 Unsupervised point cloud representation learning
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), objects are first split into voxels where each point is assigned a
    voxel label. The network is then fed with randomly rearranged point clouds and
    optimized by predicting correct voxel label for each point. During the training,
    the network aims to extract spatial relations and geometric information from point
    clouds. In their following work [[126](#bib.bib126)], another pre-text task was
    designed to predict one of ten spatial relationships of two local parts from the
    same object. Inspired by the 2D method that predicts image rotations [[127](#bib.bib127)],
    Poursaeed et al. [[101](#bib.bib101)] proposed to learn representations by predicting
    rotation angles of 3D objects. Thabet et al. [[128](#bib.bib128)] designed a pre-text
    task that predicts the next point in a point sequence defined by Morton-order
    Space Filling Curve. Chen et al. [[105](#bib.bib105)] proposed to learn the spatial
    context of objects by distinguishing the distorted parts of a shape from the correct
    ones. Sun et al. [[129](#bib.bib129)] introduced a mix-and-disentangle task to
    exploit spatial context cues.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sauder等人提出的Jigsaw3D [[13](#bib.bib13)]方法是利用点云的空间上下文的开创性工作之一。如图[18](#S5.F18
    "Figure 18 ‣ 5.2.2 Learning with spatial context structure ‣ 5.2 Context-based
    methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey")所示，物体首先被分割成体素，每个点被分配一个体素标签。然后将随机重新排列的点云输入网络，通过预测每个点的正确体素标签来优化网络。在训练过程中，网络旨在从点云中提取空间关系和几何信息。在其后续工作[[126](#bib.bib126)]中，设计了另一个预文本任务，用于预测同一物体的两个局部部分之间的十种空间关系之一。受2D方法的启发，该方法预测图像旋转[[127](#bib.bib127)]，Poursaeed等人[[101](#bib.bib101)]提出通过预测3D物体的旋转角度来学习表示。Thabet等人[[128](#bib.bib128)]设计了一个预文本任务，用于预测由Morton-order
    Space Filling Curve定义的点序列中的下一个点。Chen等人[[105](#bib.bib105)]提出通过区分形状中扭曲部分与正确部分来学习物体的空间上下文。Sun等人[[129](#bib.bib129)]引入了混合与解耦任务，以利用空间上下文线索。'
- en: 5.2.3 Learning with temporal context structure
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 利用时间上下文结构进行学习
- en: Point cloud sequence is a common type of point cloud data that consists of consecutive
    point cloud frames. For example, there are indoor point cloud sequences transformed
    from RGB-D video frames  [[18](#bib.bib18)] and LiDAR sequential data [[19](#bib.bib19),
    [130](#bib.bib130), [131](#bib.bib131)] with continuous point cloud scans with
    each scan collected by one sweep of LiDAR sensors. Point cloud sequences contain
    rich temporal information that can be extracted by designing pre-text tasks and
    used as supervision signals to train DNNs. The learned representations can be
    transferred to downstream tasks.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 点云序列是由连续的点云帧组成的一种常见点云数据类型。例如，有从RGB-D视频帧[[18](#bib.bib18)]转化而来的室内点云序列，以及具有每次扫描由一次LiDAR传感器扫描收集的连续点云扫描的LiDAR序列数据[[19](#bib.bib19),
    [130](#bib.bib130), [131](#bib.bib131)]。点云序列包含丰富的时间信息，可以通过设计预文本任务来提取，并用作监督信号来训练深度神经网络。学习到的表示可以转移到下游任务。
- en: '![Refer to caption](img/5b4713a6478aa972cf346ea7f25a8990.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/5b4713a6478aa972cf346ea7f25a8990.png)'
- en: 'Figure 19: The pipeline of STRL [[1](#bib.bib1)]: An Online Network learns
    spatial and temporal structures from two neighbouring point cloud frames $X^{u}$
    and $X^{v}$. The figure is adopted from [[1](#bib.bib1)] with authors’ permission.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '图19: STRL [[1](#bib.bib1)]的流程图：一个在线网络从两个相邻的点云帧$X^{u}$和$X^{v}$中学习空间和时间结构。该图采用了[[1](#bib.bib1)]的图示，并获得了作者许可。'
- en: 'Recently, Huang et al. [[1](#bib.bib1)] proposed a Spatio-Temporal Representation
    Learning (STRL) framework as illustrated in Fig. [19](#S5.F19 "Figure 19 ‣ 5.2.3
    Learning with temporal context structure ‣ 5.2 Context-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). STRL extends BYOL [[8](#bib.bib8)]
    from 2D vision to 3D vision and extracts spatial and temporal representation from
    point clouds. It treats two neighboring point cloud frames as positive pairs and
    minimizes the mean squared error between the learned feature representations of
    sample pairs. Chen et al. [[107](#bib.bib107)] exploit synthetic 3D shapes moving
    in static 3D environments to create dynamic scenarios and sample pairs in the
    temporal order. They conduct contrastive learning to learn 3D representations
    with dynamic understanding.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，Huang 等人[[1](#bib.bib1)]提出了一个时空表示学习（STRL）框架，如图[19](#S5.F19 "Figure 19 ‣
    5.2.3 Learning with temporal context structure ‣ 5.2 Context-based methods ‣ 5
    Unsupervised point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey")所示。STRL 将 BYOL [[8](#bib.bib8)]
    从二维视觉扩展到三维视觉，并从点云中提取空间和时间表示。它将两个相邻的点云帧视为正样本对，并最小化样本对学习特征表示之间的均方误差。Chen 等人[[107](#bib.bib107)]利用在静态三维环境中移动的合成三维形状来创建动态场景和时间顺序中的样本对。他们进行对比学习，以获得具有动态理解的三维表示。'
- en: 'Unsupervised learning with temporal context structures has proved its effectiveness
    in both 2D computer vision tasks [[132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134),
    [135](#bib.bib135)] and 3D computer vision tasks [[1](#bib.bib1), [107](#bib.bib107)].
    As discussed in Section [7](#S7 "7 Future direction ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey"), this direction
    is very promising but more research is needed for better harvesting the temporal
    contextual information.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '具有时间上下文结构的无监督学习在二维计算机视觉任务[[132](#bib.bib132)、[133](#bib.bib133)、[134](#bib.bib134)、[135](#bib.bib135)]和三维计算机视觉任务[[1](#bib.bib1)、[107](#bib.bib107)]中都证明了其有效性。如第[7](#S7
    "7 Future direction ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey")节所述，这一方向非常有前景，但仍需要更多的研究以更好地利用时间上下文信息。'
- en: 5.3 Multiple modal-based methods
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 基于多模态的方法
- en: Different modalities such as images [[19](#bib.bib19)] and natural language
    descriptions [[136](#bib.bib136)] can provide additional information for point-cloud
    data. Modeling relationships across modalities can be designed as pre-text tasks
    for URL which helps networks to learn more robust and comprehensive representations.
    Likewise, the learned parameters can be used as initialization weights for various
    downstream tasks.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图像[[19](#bib.bib19)]和自然语言描述[[136](#bib.bib136)]等不同模态可以为点云数据提供额外的信息。跨模态建模关系可以被设计为URL的前置任务，这有助于网络学习更鲁棒和全面的表示。同样，学习到的参数可以用作各种下游任务的初始化权重。
- en: '![Refer to caption](img/4e8ec1363b5d0584672bd5e8cb4a9fb7.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4e8ec1363b5d0584672bd5e8cb4a9fb7.png)'
- en: 'Figure 20: The pipeline CMCV [[4](#bib.bib4)]: CMCV employs a 2D CNN to extract
    2D features from rendered views of 3D objects and a 3D GCN to extract 3D features
    from point clouds. The two types of features are concatenated by a two-layer fully
    connected network (FCN) to predict cross-modality correspondences. The graph is
    reproduced based on [[4](#bib.bib4)].'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：管道 CMCV [[4](#bib.bib4)]：CMCV 使用 2D CNN 从 3D 物体的渲染视图中提取 2D 特征，并使用 3D GCN
    从点云中提取 3D 特征。这两种特征通过一个两层全连接网络（FCN）进行连接，以预测跨模态的对应关系。图表基于 [[4](#bib.bib4)] 复制。
- en: 'Several recent work [[4](#bib.bib4), [137](#bib.bib137)] exploits the correspondences
    across 3D point cloud objects and 2D images for URL. For example, Jing et al. [[4](#bib.bib4)]
    render 3D objects with different camera views into 2D images for learning from
    multi-modality data. As Fig. [20](#S5.F20 "Figure 20 ‣ 5.3 Multiple modal-based
    methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey") shows, they
    employ a 2D CNN and a 3D GCN to extract image features and point cloud features,
    respectively, and then conduct contrastive learning on intra-modal correspondences
    and cross-modal correspondences. Their study shows that both pre-trained 2D CNN
    and 3D GCN achieved better classification as compared with random initialization.
    Differently, Wang et al. [[138](#bib.bib138)] project point clouds into colored
    images and then feed them into an image pre-trained model with frozen weights
    to extract representative features for downstream tasks. However, how to learn
    unsupervised point cloud representations with other modalities such as text descriptions
    and audio data remains an under-explored field. We expect more studies in this
    promising research direction.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '一些近期工作[[4](#bib.bib4), [137](#bib.bib137)]利用3D点云对象和2D图像之间的对应关系进行URL。例如，Jing等人[[4](#bib.bib4)]将不同视角的3D对象渲染成2D图像，以便从多模态数据中学习。如图[20](#S5.F20
    "Figure 20 ‣ 5.3 Multiple modal-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey")所示，他们分别使用2D CNN和3D GCN提取图像特征和点云特征，然后对模态内对应关系和模态间对应关系进行对比学习。他们的研究表明，预训练的2D
    CNN和3D GCN在分类性能上都优于随机初始化。不同的是，Wang等人[[138](#bib.bib138)]将点云投影为彩色图像，然后将其输入到冻结权重的图像预训练模型中，以提取下游任务所需的代表性特征。然而，如何与其他模态（如文本描述和音频数据）一起学习无监督点云表示仍是一个待探索的领域。我们期待在这个有前景的研究方向上看到更多研究。'
- en: 5.4 Local descriptor-based methods
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 基于局部描述符的方法
- en: The aforementioned methods aim to learn semantic structures of point clouds
    for high-level understanding, while the local descriptor-based methods focus on
    learning representations for low-level tasks. For example, Deng et al. [[139](#bib.bib139)]
    introduced PPF-FoldNet that extracts rotation-invariant 3D local descriptors for
    3D matching [[140](#bib.bib140)]. Several works [[141](#bib.bib141), [142](#bib.bib142)]
    exploit non-rigid shape correspondence extraction as pre-text tasks for URL of
    point clouds, aiming to find the point-to-point correspondence of two deformable
    3D shapes. Jiang et al. [[143](#bib.bib143)] explore unsupervised 3D registration
    for finding the optimal rigid transformation that can align the source point cloud
    to the target precisely.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法旨在学习点云的语义结构以进行高级理解，而基于局部描述符的方法则专注于学习低级任务的表示。例如，Deng等人[[139](#bib.bib139)]引入了PPF-FoldNet，该方法提取旋转不变的3D局部描述符用于3D匹配[[140](#bib.bib140)]。一些工作[[141](#bib.bib141),
    [142](#bib.bib142)]利用非刚性形状对应提取作为点云URL的前文本任务，旨在找到两个可变形3D形状之间的点对点对应关系。Jiang等人[[143](#bib.bib143)]探索了无监督的3D配准，以找到能够精确对齐源点云到目标的最优刚性变换。
- en: The performances of existing local descriptor-based methods are mainly evaluated
    on low-level tasks. However, how to adapt the learned feature representations
    toward other high-level tasks is rarely discussed. We expect more related research
    in the future.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的基于局部描述符的方法主要在低级任务上进行评估。然而，如何将学习到的特征表示适应于其他高级任务却很少讨论。我们期待未来更多相关研究。
- en: 5.5 Pros and Cons
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 优缺点
- en: Generation-based methods have been extensively studied in 3D URL, thanks to
    their ability to recover the original data distribution without assuming any downstream
    tasks. However, most existing research focuses on object-level point clouds, characterized
    by limited point numbers and data variability, restricting their applicability
    to object classification and part segmentation tasks. Additionally, these methods
    demonstrate limited effectiveness in scene-level tasks, such as 3D object detection
    and semantic segmentation, due to the difficulty of generating scene-level point
    clouds with complex distribution, rich noises and sparsity variation, and various
    occlusions. Nonetheless, generation-based methods achieve very impressive progress
    in 2D images [[10](#bib.bib10)] recently, demonstrating their great potential
    for handling 3D point-cloud data. More efforts are expected in scene-level tasks
    as well as various downstream applications.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成的方法在 3D URL 中得到了广泛研究，得益于它们在不假设任何下游任务的情况下恢复原始数据分布的能力。然而，大多数现有研究集中在对象级点云上，其特点是点数有限和数据变异性，限制了它们在对象分类和部件分割任务中的应用。此外，这些方法在场景级任务（如
    3D 目标检测和语义分割）中的效果有限，因为生成具有复杂分布、丰富噪声和稀疏变化的场景级点云以及各种遮挡情况十分困难。尽管如此，基于生成的方法在 2D 图像上最近取得了非常显著的进展
    [[10](#bib.bib10)]，展示了它们处理 3D 点云数据的巨大潜力。预计在场景级任务以及各种下游应用中会有更多的努力。
- en: Context-based methods have recently become a prevalent approach in scene-level
    tasks, such as 3D semantic segmentation, 3D instance segmentation, and 3D object
    detection, thanks to their ability in addressing complex real-world data. However,
    they are still facing several challenges. The first is hard-example mining which
    is crucial to effective contrastive learning. Beyond that, designing effective
    self-supervision is also challenging for context-based methods, especially while
    considering generalization across various tasks and applications.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上下文的方法由于能够处理复杂的真实世界数据，最近已成为场景级任务（如 3D 语义分割、3D 实例分割和 3D 目标检测）的普遍方法。然而，它们仍面临一些挑战。首先是困难样本挖掘，这对有效的对比学习至关重要。除此之外，为上下文方法设计有效的自监督也是一个挑战，特别是在考虑各种任务和应用的泛化时。
- en: Multiple modal-based methods allow leveraging additional data modalities for
    enriching the distribution of point clouds. Pair-wise correspondences between
    point clouds and other data modalities also offer additional supervision, thereby
    enhancing the learned unsupervised point cloud representations. However, multi-modality
    methods are still facing several challenges. For example, acquiring large-scale
    pair-wise data is often a non-trivial task, and so does the design of effective
    cross-domain tasks. In addition, how to learn an effective homogeneous representation
    space across multiple modalities remains a very open research problem.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 基于多模态的方法允许利用额外的数据模态来丰富点云的分布。点云与其他数据模态之间的配对对应关系还提供了额外的监督，从而增强了学习到的无监督点云表示。然而，多模态方法仍面临多个挑战。例如，获取大规模配对数据通常是一项复杂的任务，设计有效的跨领域任务也是如此。此外，如何在多个模态之间学习有效的同质表示空间仍然是一个非常开放的研究问题。
- en: Local descriptor-based methods offer distinct advantages in capturing detailed
    spatial cues and exploiting low-level position information. However, these methods
    are limited in their ability of transferring learned representations to high-level
    recognition models, which restricts their application scope in more complex and
    abstract recognition tasks.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 基于局部描述符的方法在捕捉详细的空间线索和利用低级位置信息方面具有明显优势。然而，这些方法在将学到的表示转移到高级识别模型中的能力受到限制，这限制了它们在更复杂和抽象的识别任务中的应用范围。
- en: 6 Benchmark performances
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 基准性能
- en: 'TABLE IV: Comparing linear shape classification on ModelNet10 and ModelNet40
    [[27](#bib.bib27)]: Linear SVM classifiers are trained with representations learned
    by different unsupervised methods. Accuracy highlighted by ^* was obtained by
    pre-training with multi-modal data. [T] denotes models with modified Transformers.
    [ST] denotes models with standard Transformers.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 比较在 ModelNet10 和 ModelNet40 上的线性形状分类 [[27](#bib.bib27)]：线性 SVM 分类器使用通过不同的无监督方法学习的表示进行训练。标记为
    ^* 的准确率是通过多模态数据预训练获得的。[T] 表示修改过的 Transformer 模型。[ST] 表示标准 Transformer 模型。'
- en: '| Method | Year | Pre-text task | Backbone | Pre-train dataset | ModelNet10
    | ModelNet40 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 预处理任务 | 主干 | 预训练数据集 | ModelNet10 | ModelNet40 |'
- en: '| Supervised learning | 2017 | N.A. | PointNet [[15](#bib.bib15)] | N.A. |
    - | 89.2 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 有监督学习 | 2017 | 不适用 | PointNet [[15](#bib.bib15)] | 不适用 | - | 89.2 |'
- en: '| 2017 | PointNet++ [[48](#bib.bib48)] | - | 90.7 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | PointNet++ [[48](#bib.bib48)] | - | 90.7 |'
- en: '| 2019 | DGCNN [[49](#bib.bib49)] | - | 93.5 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | DGCNN [[49](#bib.bib49)] | - | 93.5 |'
- en: '| 2019 | RSCNN [[56](#bib.bib56)] | - | 93.6 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | RSCNN [[56](#bib.bib56)] | - | 93.6 |'
- en: '| 2021 | [T]PointTransformer [[144](#bib.bib144)] |  | - | 93.7 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | [T]PointTransformer [[144](#bib.bib144)] |  | - | 93.7 |'
- en: '| 2022 | [ST]Transformer [[57](#bib.bib57)] |  | - | 91.4 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | [ST]Transformer [[57](#bib.bib57)] |  | - | 91.4 |'
- en: '| SPH [[145](#bib.bib145)] | 2003 | Generation | - | ShapeNet | 79.8 | 68.2
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| SPH [[145](#bib.bib145)] | 2003 | 生成 | - | ShapeNet | 79.8 | 68.2 |'
- en: '| LFD [[146](#bib.bib146)] | 2003 | Generation | - | ShapeNet | 79.9 | 75.5
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| LFD [[146](#bib.bib146)] | 2003 | 生成 | - | ShapeNet | 79.9 | 75.5 |'
- en: '| TL-Net [[63](#bib.bib63)] | 2016 | Generation | - | ShapeNet | - | 74.4 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| TL-Net [[63](#bib.bib63)] | 2016 | 生成 | - | ShapeNet | - | 74.4 |'
- en: '| VConv-DAE [[62](#bib.bib62)] | 2016 | Generation | - | ShapeNet | 80.5 |
    75.5 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| VConv-DAE [[62](#bib.bib62)] | 2016 | 生成 | - | ShapeNet | 80.5 | 75.5 |'
- en: '| 3D-GAN [[64](#bib.bib64)] | 2016 | Generation | - | ShapeNet | 91.0 | 83.3
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 3D-GAN [[64](#bib.bib64)] | 2016 | 生成 | - | ShapeNet | 91.0 | 83.3 |'
- en: '| 3D DescriptorNet [[65](#bib.bib65)] | 2018 | Generation | - | ShapeNet |
    - | 92.4 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 3D DescriptorNet [[65](#bib.bib65)] | 2018 | 生成 | - | ShapeNet | - | 92.4
    |'
- en: '| FoldingNet [[66](#bib.bib66)] | 2018 | Generation | - | ModelNet40 | 91.9
    | 84.4 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| FoldingNet [[66](#bib.bib66)] | 2018 | 生成 | - | ModelNet40 | 91.9 | 84.4
    |'
- en: '| FoldingNet [[66](#bib.bib66)] | 2018 | Generation | - | ShapeNet | 94.4 |
    88.4 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| FoldingNet [[66](#bib.bib66)] | 2018 | 生成 | - | ShapeNet | 94.4 | 88.4 |'
- en: '| Latent-GAN [[68](#bib.bib68)] | 2018 | Generation | - | ModelNet40 | 92.2
    | 87.3 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Latent-GAN [[68](#bib.bib68)] | 2018 | 生成 | - | ModelNet40 | 92.2 | 87.3
    |'
- en: '| Latent-GAN [[68](#bib.bib68)] | 2018 | Generation | - | ShapeNet | 95.3 |
    85.7 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Latent-GAN [[68](#bib.bib68)] | 2018 | 生成 | - | ShapeNet | 95.3 | 85.7 |'
- en: '| MRTNet [[69](#bib.bib69)] | 2018 | Generation | - | ShapeNet | 86.4 | - |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| MRTNet [[69](#bib.bib69)] | 2018 | 生成 | - | ShapeNet | 86.4 | - |'
- en: '| VIP-GAN [[70](#bib.bib70)] | 2019 | Generation | - | ShapeNet | 94.1 | 92.0
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| VIP-GAN [[70](#bib.bib70)] | 2019 | 生成 | - | ShapeNet | 94.1 | 92.0 |'
- en: '| 3DCapsuleNet [[47](#bib.bib47)] | 2019 | Generation | - | ShapeNet | - |
    88.9 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 3DCapsuleNet [[47](#bib.bib47)] | 2019 | 生成 | - | ShapeNet | - | 88.9 |'
- en: '| PC-GAN [[88](#bib.bib88)] | 2019 | Generation | - | ModelNet40 | - | 87.8
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| PC-GAN [[88](#bib.bib88)] | 2019 | 生成 | - | ModelNet40 | - | 87.8 |'
- en: '| L2G-AE [[71](#bib.bib71)] | 2019 | Generation | - | ShapeNet | 95.4 | 90.6
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| L2G-AE [[71](#bib.bib71)] | 2019 | 生成 | - | ShapeNet | 95.4 | 90.6 |'
- en: '| MAP-VAE [[72](#bib.bib72)] | 2019 | Generation | - | ShapeNet | 94.8 | 90.2
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| MAP-VAE [[72](#bib.bib72)] | 2019 | 生成 | - | ShapeNet | 94.8 | 90.2 |'
- en: '| PointFlow [[73](#bib.bib73)] | 2019 | Generation | - | ShapeNet | 93.7 |
    86.8 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| PointFlow [[73](#bib.bib73)] | 2019 | 生成 | - | ShapeNet | 93.7 | 86.8 |'
- en: '| MultiTask [[96](#bib.bib96)] | 2019 | Hybrid | - | ShapeNet | - | 89.1 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| MultiTask [[96](#bib.bib96)] | 2019 | 混合 | - | ShapeNet | - | 89.1 |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | 2019 | Context | PointNet | ShapeNet | 91.6
    | 87.3 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | 2019 | 上下文 | PointNet | ShapeNet | 91.6 | 87.3
    |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | 2019 | Context | DGCNN | ShapeNet | 94.5 |
    90.6 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | 2019 | 上下文 | DGCNN | ShapeNet | 94.5 | 90.6
    |'
- en: '| ClusterNet [[97](#bib.bib97)] | 2019 | Context | DGCNN | ShapeNet | 93.8
    | 86.8 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| ClusterNet [[97](#bib.bib97)] | 2019 | 上下文 | DGCNN | ShapeNet | 93.8 | 86.8
    |'
- en: '| CloudContext [[126](#bib.bib126)] | 2019 | Context | DGCNN | ShapeNet | 94.5
    | 89.3 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| CloudContext [[126](#bib.bib126)] | 2019 | 上下文 | DGCNN | ShapeNet | 94.5
    | 89.3 |'
- en: '| NeuralSampler [[90](#bib.bib90)] | 2019 | Generation | - | ShapeNet | 95.3
    | 88.7 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| NeuralSampler [[90](#bib.bib90)] | 2019 | 生成 | - | ShapeNet | 95.3 | 88.7
    |'
- en: '| PointGrow [[77](#bib.bib77)] | 2020 | Generation | - | ShapeNet | 85.8 |
    - |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| PointGrow [[77](#bib.bib77)] | 2020 | 生成 | - | ShapeNet | 85.8 | - |'
- en: '| Info3D [[99](#bib.bib99)] | 2020 | Context | PointNet | ShapeNet | - | 89.8
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Info3D [[99](#bib.bib99)] | 2020 | 上下文 | PointNet | ShapeNet | - | 89.8 |'
- en: '| Info3D [[99](#bib.bib99)] | 2020 | Context | DGCNN | ShapeNet | - | 91.6
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Info3D [[99](#bib.bib99)] | 2020 | 上下文 | DGCNN | ShapeNet | - | 91.6 |'
- en: '| ACD [[100](#bib.bib100)] | 2020 | Context | PointNet++ | ShapeNet | - | 89.8
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| ACD [[100](#bib.bib100)] | 2020 | 上下文 | PointNet++ | ShapeNet | - | 89.8
    |'
- en: '| PDL [[74](#bib.bib74)] | 2020 | Generation | - | ShapeNet | - | 84.7 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| PDL [[74](#bib.bib74)] | 2020 | 生成 | - | ShapeNet | - | 84.7 |'
- en: '| GLR [[98](#bib.bib98)] | 2020 | Hybrid | PointNet++ | ShapeNet | 94.8 | 92.2
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| GLR [[98](#bib.bib98)] | 2020 | 混合 | PointNet++ | ShapeNet | 94.8 | 92.2
    |'
- en: '| GLR [[98](#bib.bib98)] | 2020 | Hybrid | RSCNN | ShapeNet | 94.6 | 92.2 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| GLR [[98](#bib.bib98)] | 2020 | 混合 | RSCNN | ShapeNet | 94.6 | 92.2 |'
- en: '| SA-Net-cls [[76](#bib.bib76)] | 2020 | Generation | - | ShapeNet | - | 90.6
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| SA-Net-cls [[76](#bib.bib76)] | 2020 | 生成 | - | ShapeNet | - | 90.6 |'
- en: '| GraphTER [[75](#bib.bib75)] | 2020 | Generation | - | ModelNet40 | - | 89.1
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| GraphTER [[75](#bib.bib75)] | 2020 | 生成 | - | ModelNet40 | - | 89.1 |'
- en: '| Rotation3D [[101](#bib.bib101)] | 2020 | Context | PointNet | ShapeNet |
    - | 88.6 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Rotation3D [[101](#bib.bib101)] | 2020 | 上下文 | PointNet | ShapeNet | - |
    88.6 |'
- en: '| Rotation3D [[101](#bib.bib101)] | 2020 | Context | DGCNN | ShapeNet | - |
    90.8 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Rotation3D [[101](#bib.bib101)] | 2020 | 上下文 | DGCNN | ShapeNet | - | 90.8
    |'
- en: '| MID [[121](#bib.bib121)] | 2020 | Context | HRNet | ShapeNet | - | 90.3 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| MID [[121](#bib.bib121)] | 2020 | 上下文 | HRNet | ShapeNet | - | 90.3 |'
- en: '| GTIF [[85](#bib.bib85)] | 2020 | Generation | HRNet | ShapeNet | 95.9 | 89.6
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| GTIF [[85](#bib.bib85)] | 2020 | 生成 | HRNet | ShapeNet | 95.9 | 89.6 |'
- en: '| HNS [[102](#bib.bib102)] | 2021 | Context | DGCNN | ShapeNet | - | 89.6 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| HNS [[102](#bib.bib102)] | 2021 | 上下文 | DGCNN | ShapeNet | - | 89.6 |'
- en: '| ParAE [[147](#bib.bib147)] | 2021 | Generation | PointNet | ShapeNet | -
    | 90.3 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| ParAE [[147](#bib.bib147)] | 2021 | 生成 | PointNet | ShapeNet | - | 90.3 |'
- en: '| ParAE [[147](#bib.bib147)] | 2021 | Generation | DGCNN | ShapeNet | - | 91.6
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| ParAE [[147](#bib.bib147)] | 2021 | 生成 | DGCNN | ShapeNet | - | 91.6 |'
- en: '| CMCV [[4](#bib.bib4)] | 2021 | Multi-modal | DGCNN | ShapeNet | - | 89.8^*
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| CMCV [[4](#bib.bib4)] | 2021 | 多模态 | DGCNN | ShapeNet | - | 89.8^* |'
- en: '| GSIR [[86](#bib.bib86)] | 2021 | Context | DGCNN | ModelNet40 | - | 90.4
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| GSIR [[86](#bib.bib86)] | 2021 | 上下文 | DGCNN | ModelNet40 | - | 90.4 |'
- en: '| STRL [[1](#bib.bib1)] | 2021 | Context | PointNet | ShapeNet | - | 88.3 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| STRL [[1](#bib.bib1)] | 2021 | 上下文 | PointNet | ShapeNet | - | 88.3 |'
- en: '| STRL [[1](#bib.bib1)] | 2021 | Context | DGCNN | ShapeNet | - | 90.9 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| STRL [[1](#bib.bib1)] | 2021 | 上下文 | DGCNN | ShapeNet | - | 90.9 |'
- en: '| PSG-Net [[78](#bib.bib78)] | 2021 | Generation | PointNet++ | ShapeNet |
    - | 90.9 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| PSG-Net [[78](#bib.bib78)] | 2021 | 生成 | PointNet++ | ShapeNet | - | 90.9
    |'
- en: '| SelfCorrection [[105](#bib.bib105)] | 2021 | Hybrid | PointNet | ShapeNet
    | 93.3 | 89.9 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| SelfCorrection [[105](#bib.bib105)] | 2021 | 混合 | PointNet | ShapeNet | 93.3
    | 89.9 |'
- en: '| SelfCorrection [[105](#bib.bib105)] | 2021 | Hybrid | RSCNN | ShapeNet |
    95.0 | 92.4 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| SelfCorrection [[105](#bib.bib105)] | 2021 | 混合 | RSCNN | ShapeNet | 95.0
    | 92.4 |'
- en: '| OcCo [[12](#bib.bib12)] | 2021 | Generation | [ST]Transformer | ShapeNet
    | - | 92.1 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | 2021 | 生成 | [ST]Transformer | ShapeNet | - | 92.1
    |'
- en: '| CrossPoint [[137](#bib.bib137)] | 2022 | Multi-modal | PointNet | ShapeNet
    | - | 89.1^* |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| CrossPoint [[137](#bib.bib137)] | 2022 | 多模态 | PointNet | ShapeNet | - |
    89.1^* |'
- en: '| CrossPoint [[137](#bib.bib137)] | 2022 | Multi-modal | DGCNN | ShapeNet |
    - | 91.2^* |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| CrossPoint [[137](#bib.bib137)] | 2022 | 多模态 | DGCNN | ShapeNet | - | 91.2^*
    |'
- en: '| Point-BERT [[57](#bib.bib57)] | 2022 | Generation | [ST]Transformer | ShapeNet
    | - | 93.2 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Point-BERT [[57](#bib.bib57)] | 2022 | 生成 | [ST]Transformer | ShapeNet |
    - | 93.2 |'
- en: '| Point-MAE [[79](#bib.bib79)] | 2022 | Generation | [ST]Transformer | ShapeNet
    | - | 93.8 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Point-MAE [[79](#bib.bib79)] | 2022 | 生成 | [ST]Transformer | ShapeNet | -
    | 93.8 |'
- en: We benchmark representative 3D URL methods with two widely adopted evaluation
    metrics. The benchmarking is performed over public point-cloud data, where all
    performances are extracted from the corresponding papers.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个广泛采用的评估指标对代表性的3D URL方法进行基准测试。基准测试在公共点云数据上进行，所有性能数据均来自相应的论文。
- en: 6.1 Evaluation Criteria
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 评估标准
- en: There are two metrics that have been widely adopted for evaluating the quality
    of the learned unsupervised point-cloud representations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种指标被广泛用于评估学习到的无监督点云表示的质量。
- en: $\bullet$ Linear classification first applies a pre-trained unsupervised model
    to extract features from certain labelled data. It then trains a supervised linear
    classifier with the extracted features together with the corresponding labels,
    where the quality of the pre-learned unsupervised representations is evaluated
    by the performance of the trained linear classifier over test data. Hence, the
    linear classification can be viewed as a type of representation learning metric
    which provides cluster analysis in an implicit way.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 线性分类首先应用预训练的无监督模型从某些标记数据中提取特征。然后，它用提取的特征及相应的标签训练一个监督线性分类器，其中预学习的无监督表示的质量通过训练好的线性分类器在测试数据上的表现来评估。因此，线性分类可以被视为一种表示学习度量，它以隐式方式提供了聚类分析。
- en: $\bullet$ Fine-tuning optimizes a pre-trained unsupervised model using labelled
    data from downstream tasks. It can assess the quality of the pre-learned unsupervised
    representations by evaluating the performance of the fine-tuned model over downstream
    test data, i.e. how much performance gains could be obtained by unsupervised pre-training
    compared to the random initialization.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 微调通过使用下游任务的标记数据优化预训练的无监督模型。它可以通过评估微调模型在下游测试数据上的表现来评估预学习的无监督表示的质量，即通过无监督预训练相比于随机初始化能获得多少性能提升。
- en: 'TABLE V: Comparisons of unsupervised pre-training performance over the object
    classification datasets ModelNet40 and OBJ-BG split in ScanObjecNN. Performance
    numbers are presented in the format of ”A/B”, with ”A” indicating training classification
    models from scratch with random initialization and ”B” indicating fine-tuning
    classification models that are initialized with unsupervised pre-trained models.
    Performance under “A” may vary due to different implementations as reported in
    the corresponding papers.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：对对象分类数据集 ModelNet40 和 ScanObjectNN 的无监督预训练性能比较。性能数据以“A/B”的格式呈现，其中“A”表示从头开始训练的分类模型，具有随机初始化，而“B”表示用无监督预训练模型初始化的分类模型进行微调。由于不同的实现，性能在“A”下可能有所不同，具体情况请参考相关论文。
- en: '| Method | Backbone | ModelNet40 | ScanObjectNN |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干 | ModelNet40 | ScanObjectNN |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | PointNet [[15](#bib.bib15)] | 89.2/89.6(+0.4)
    | 73.5/76.5(+3.0) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | PointNet [[15](#bib.bib15)] | 89.2/89.6(+0.4)
    | 73.5/76.5(+3.0) |'
- en: '| Info3D [[99](#bib.bib99)] | PointNet [[15](#bib.bib15)] | 89.2/90.2(+1.0)
    | -/- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Info3D [[99](#bib.bib99)] | PointNet [[15](#bib.bib15)] | 89.2/90.2(+1.0)
    | -/- |'
- en: '| SelfCorrection [[105](#bib.bib105)] | PointNet [[15](#bib.bib15)] | 89.1/90.0(+0.9)
    | -/- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| SelfCorrection [[105](#bib.bib105)] | PointNet [[15](#bib.bib15)] | 89.1/90.0(+0.9)
    | -/- |'
- en: '| OcCo [[12](#bib.bib12)] | PointNet [[15](#bib.bib15)] | 89.2/90.1(+0.9) |
    73.5/80.0(+6.5) |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | PointNet [[15](#bib.bib15)] | 89.2/90.1(+0.9) |
    73.5/80.0(+6.5) |'
- en: '| ParAE [[147](#bib.bib147)] | PointNet [[15](#bib.bib15)] | 89.2/90.5(+1.3)
    | -/- |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| ParAE [[147](#bib.bib147)] | PointNet [[15](#bib.bib15)] | 89.2/90.5(+1.3)
    | -/- |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | PCN [[113](#bib.bib113)] | 89.3/89.6(+0.3)
    | 78.3/78.2(-0.1) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | PCN [[113](#bib.bib113)] | 89.3/89.6(+0.3)
    | 78.3/78.2(-0.1) |'
- en: '| OcCo [[12](#bib.bib12)] | PCN [[113](#bib.bib113)] | 89.3/90.3(+1.0) | 78.3/80.4(+2.1)
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | PCN [[113](#bib.bib113)] | 89.3/90.3(+1.0) | 78.3/80.4(+2.1)
    |'
- en: '| GLR [[98](#bib.bib98)] | RSCNN [[56](#bib.bib56)] | 91.8/92.2(+0.5) | -/-
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| GLR [[98](#bib.bib98)] | RSCNN [[56](#bib.bib56)] | 91.8/92.2(+0.5) | -/-
    |'
- en: '| SelfCorrection [[105](#bib.bib105)] | RSCNN [[56](#bib.bib56)] | 91.7/93.0(+1.3)
    | -/- |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| SelfCorrection [[105](#bib.bib105)] | RSCNN [[56](#bib.bib56)] | 91.7/93.0(+1.3)
    | -/- |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | DGCNN [[49](#bib.bib49)] | 92.2/92.4(+0.2)
    | 82.4/82.7(+0.3) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | DGCNN [[49](#bib.bib49)] | 92.2/92.4(+0.2)
    | 82.4/82.7(+0.3) |'
- en: '| Info3D [[99](#bib.bib99)] | DGCNN [[49](#bib.bib49)] | 93.5/93.0(-0.5) |
    -/- |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Info3D [[99](#bib.bib99)] | DGCNN [[49](#bib.bib49)] | 93.5/93.0(-0.5) |
    -/- |'
- en: '| OcCo [[12](#bib.bib12)] | DGCNN [[49](#bib.bib49)] | 92.5/93.0(+0.5) | 82.4/83.9(+1.6)
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | DGCNN [[49](#bib.bib49)] | 92.5/93.0(+0.5) | 82.4/83.9(+1.6)
    |'
- en: '| ParAE [[147](#bib.bib147)] | DGCNN [[49](#bib.bib49)] | 92.2/92.9(+0.7) |
    -/- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| ParAE [[147](#bib.bib147)] | DGCNN [[49](#bib.bib49)] | 92.2/92.9(+0.7) |
    -/- |'
- en: '| STRL [[1](#bib.bib1)] | DGCNN [[49](#bib.bib49)] | 92.2/93.1(+0.9) | -/-
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| STRL [[1](#bib.bib1)] | DGCNN [[49](#bib.bib49)] | 92.2/93.1(+0.9) | -/-
    |'
- en: '| OcCo [[12](#bib.bib12)] | Transformer [[57](#bib.bib57)] | 91.2/92.2(+1.0)
    | 79.9/84.9(+5.0) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | Transformer [[57](#bib.bib57)] | 91.2/92.2(+1.0)
    | 79.9/84.9(+5.0) |'
- en: '| Point-BERT [[57](#bib.bib57)] | Transformer [[57](#bib.bib57)] | 91.2/93.4(+2.2)
    | 79.9/87.4(+7.5) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Point-BERT [[57](#bib.bib57)] | Transformer [[57](#bib.bib57)] | 91.2/93.4(+2.2)
    | 79.9/87.4(+7.5) |'
- en: Note URL can be evaluated with other quantitative metrics. For example, reconstruction
    error [[66](#bib.bib66)] can tell how well the learned representations encode
    the raw point clouds. Different clustering metrics such as Normalized Mutual Information [[96](#bib.bib96)]
    could complement the linear-classification metric. However, these metrics are
    mostly task-specific, e.g., the reconstruction error may not evaluate the representation
    of scene-level point clouds well due to their inherent noise, occlusion, and sparsity.
    In fact, few generic metrics can directly and explicitly evaluate the quality
    of the learned 3D unsupervised representations despite its critical importance
    to 3D URL studies. More research along this direction is needed to advance this
    research field further.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，URL可以通过其他定量指标进行评估。例如，重建误差 [[66](#bib.bib66)] 可以显示学习到的表示如何编码原始点云。不同的聚类指标，例如标准化互信息 [[96](#bib.bib96)]，可以补充线性分类指标。然而，这些指标大多是特定于任务的，例如，重建误差可能无法很好地评估场景级点云的表示，因为它们固有的噪声、遮挡和稀疏性。实际上，尽管学习到的3D无监督表示对3D
    URL研究至关重要，但很少有通用指标可以直接和明确地评估这些表示的质量。需要更多的研究来推动这一研究领域的发展。
- en: Beyond quantitative metrics, unsupervised feature representations can be evaluated
    in a qualitative manner. For example, t-SNE (t-Distributed Stochastic Neighbor
    Embedding) [[148](#bib.bib148)] has been widely adopted to compress the dimension
    of the learned feature representations and visualize the compressed feature embeddings.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定量指标之外，无监督特征表示还可以以定性方式进行评估。例如，广泛采用t-SNE（t分布的随机邻居嵌入）[[148](https://wiki.example.org/t-SNE)]来压缩所学特征表示的维度并可视化压缩后的特征嵌入。
- en: 6.2 Object-level tasks
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 对象级任务
- en: 6.2.1 Object classification
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 对象分类
- en: 'Object classification is the most widely used task in evaluations since the
    majority of existing works learn point cloud representations on object-level point
    cloud datasets. As described in Section [6.1](#S6.SS1 "6.1 Evaluation Criteria
    ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey"), both two types of protocols are widely
    adopted including the linear classification protocol and the fine-tuning protocol.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 对象分类是评估中最常用的任务，因为现有的大部分工作都是在对象级点云数据集上学习点云表示。如[6.1](https://wiki.example.org/evaluation_criteria/unsupervised_point_cloud_representation_learning_with_deep_neural_networks)部分所述，线性分类协议和微调协议都被广泛采用。
- en: 'TABLE VI: Comparison of 3D URL methods for shape part segmentation over ShapeNetPart [[14](#bib.bib14)].
    ”Unsup.” denotes linear classification of the learned unsupervised point features.
    ”Trans.” is presented in a format of ”A/B”, where ”A” is obtained with segmentation
    models trained from scratch with random initialization, and ”B” is obtained by
    fine-tuning segmentation models that are initialized with unsupervised pre-trained
    models. We also provide supervised performances (”Sup.”) of different backbone
    models with random initialization (extracted from the original papers).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 表VI：基于ShapeNetPart [[14](https://wiki.example.org/ShapeNetPart)] 的3D URL方法的比较。
    "无监督" 表示对学习的无监督点特征进行线性分类。 "转换" 是以“A/B”格式呈现，其中“A”是使用随机初始化从头训练的分割模型的结果，而“B”是通过使用无监督预训练模型初始化的分割模型进行微调的结果。我们还提供了具有随机初始化的不同主干模型的监督性能（“监督”）的结果（从原始论文中提取）。
- en: '| URL Method | Type | Backbone | class mIoU | instance mIoU |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| URL 方法 | 类型 | 主干 | 类别 mIoU | 实例 mIoU |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| N.A. | Sup. | PointNet | 80.4 | 83.7 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 监督 | PointNet | 80.4 | 83.7 |'
- en: '| Sup. | PointNet++ | 81.9 | 85.1 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | PointNet++ | 81.9 | 85.1 |'
- en: '| Sup. | DGCNN | 82.3 | 85.1 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | DGCNN | 82.3 | 85.1 |'
- en: '| Sup. | RSCNN | 84.0 | 86.2 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | RSCNN | 84.0 | 86.2 |'
- en: '| Sup. | Transformer | 83.4 | 85.1 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | Transformer | 83.4 | 85.1 |'
- en: '| Latent-GAN [[68](#bib.bib68)] | Unsup. | - | 57.0 | - |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Latent-GAN [[68](https://wiki.example.org/Latent-GAN)] | 无监督 | - | 57.0 |
    - |'
- en: '| MAP-VAE [[72](#bib.bib72)] | Unsup. | - | 68.0 | - |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| MAP-VAE [[72](https://wiki.example.org/MAP-VAE)] | 无监督 | - | 68.0 | - |'
- en: '| CloudContext [[126](#bib.bib126)] | Unsup. | DGCNN | - | 81.5 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| CloudContext [[126](https://wiki.example.org/CloudContext)] | 无监督 | DGCNN
    | - | 81.5 |'
- en: '| GraphTER [[75](#bib.bib75)] | Unsup. | - | 78.1 | 81.9 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| GraphTER [[75](https://wiki.example.org/GraphTER)] | 无监督 | - | 78.1 | 81.9
    |'
- en: '| MID [[121](#bib.bib121)] | Unsup. | HRNet | 83.4 | 84.6 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| MID [[121](https://wiki.example.org/MID)] | 无监督 | HRNet | 83.4 | 84.6 |'
- en: '| HNS [[102](#bib.bib102)] | Unsup. | DGCNN | 79.9 | 82.3 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| HNS [[102](https://wiki.example.org/HNS)] | 无监督 | DGCNN | 79.9 | 82.3 |'
- en: '| CMCV [[4](#bib.bib4)] | Unsup. | DGCNN | 74.7 | 80.8 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| CMCV [[4](https://wiki.example.org/CMCV)] | 无监督 | DGCNN | 74.7 | 80.8 |'
- en: '| SO-Net [[67](#bib.bib67)] | Trans. | SO-Net | -/- | 84.6/84.9(+0.3) |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| SO-Net [[67](https://wiki.example.org/SO-Net)] | 转换 | SO-Net | -/- | 84.6/84.9(+0.3)
    |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | Trans. | DGCNN | 82.3/83.1(+0.8) | 85.1/85.3(+0.2)
    |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](https://wiki.example.org/Jigsaw3D)] | 转换 | DGCNN | 82.3/83.1(+0.8)
    | 85.1/85.3(+0.2) |'
- en: '| MID [[121](#bib.bib121)] | Trans. | HRNet | 84.6/85.2(+0.6) | 85.5/85.8(+0.3)
    |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| MID [[121](https://wiki.example.org/MID)] | 转换 | HRNet | 84.6/85.2(+0.6)
    | 85.5/85.8(+0.3) |'
- en: '| CMCV [[4](#bib.bib4)] | Trans. | DGCNN | 77.6/79.1(+1.5) | 83.0/83.7(+0.7)
    |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| CMCV [[4](https://wiki.example.org/CMCV)] | 转换 | DGCNN | 77.6/79.1(+1.5)
    | 83.0/83.7(+0.7) |'
- en: '| OcCo [[12](#bib.bib12)] | Trans. | PointNet | 82.2/83.4(+1.2) | -/- |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](https://wiki.example.org/OcCo)] | 转换 | PointNet | 82.2/83.4(+1.2)
    | -/- |'
- en: '| OcCo [[12](#bib.bib12)] | Trans. | DGCNN | 84.4/85.0(+0.6) | -/- |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](https://wiki.example.org/OcCo)] | 转换 | DGCNN | 84.4/85.0(+0.6)
    | -/- |'
- en: '| OcCo [[12](#bib.bib12)] | Trans. | Transformer | 83.4/83.4(+0.0) | 85.1/85.1(+0.0)
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](https://wiki.example.org/OcCo)] | 转换 | Transformer | 83.4/83.4(+0.0)
    | 85.1/85.1(+0.0) |'
- en: '| Point-BERT [[57](#bib.bib57)] | Trans. | Transformer | 83.4/84.1(+0.7) |
    85.1/85.6(+0.5) |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Point-BERT [[57](https://wiki.example.org/Point-BERT)] | 转换 | Transformer
    | 83.4/84.1(+0.7) | 85.1/85.6(+0.5) |'
- en: 'Table [IV](#S6.T4 "TABLE IV ‣ 6 Benchmark performances ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey") summarizes
    the performance of the linear classification by existing methods. Specifically,
    linear classifiers are trained with the representations learned by different unsupervised
    methods on the ShapeNet or ModelNet40 dataset, and the classification results
    over the testing set over ModelNet10 and ModelNet40 are reported. For comparison,
    we also list supervised learning performances of the same backbone models over
    the same datasets. It can be seen that the performances of unsupervised learning
    methods keep improving and some methods have even surpassed supervised learning
    methods, demonstrating the effectiveness and great potential of URL of point clouds.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [IV](#S6.T4 "TABLE IV ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey") 总结了现有方法的线性分类性能。具体来说，线性分类器使用不同无监督方法在
    ShapeNet 或 ModelNet40 数据集上学习到的表示进行训练，并报告了 ModelNet10 和 ModelNet40 上的测试集分类结果。为了比较，我们还列出了相同骨干模型在相同数据集上的监督学习表现。可以看出，无监督学习方法的表现不断提升，有些方法甚至超越了监督学习方法，展示了点云
    URL 的有效性和巨大潜力。'
- en: 'TABLE VII: Semantic segmentation on S3DIS [[21](#bib.bib21)]: It compares supervised
    training with random weight initialization and fine-tuning with pre-trained weights
    learned from unsupervised pre-training tasks. It uses DGCNN as the segmentation
    model, which is trained on different single Areas and tested on Area 5 (upper
    part) and Area 6 (lower part).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：S3DIS 上的语义分割 [[21](#bib.bib21)]：它比较了带有随机权重初始化的监督训练和使用从无监督预训练任务中学习到的预训练权重进行微调的效果。它使用
    DGCNN 作为分割模型，模型在不同的单个区域上训练，并在区域 5（上部）和区域 6（下部）上测试。
- en: '| Method | OA on area 5 with different train area | mIoU on area 5 with different
    train area |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 在不同训练区域的区域 5 的 OA | 在不同训练区域的区域 5 的 mIoU |'
- en: '| --- | --- | --- |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Area1 | Area2 | Area3 | Area4 | Area6 | Area1 | Area2 | Area3 | Area4 | Area6
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 区域1 | 区域2 | 区域3 | 区域4 | 区域6 | 区域1 | 区域2 | 区域3 | 区域4 | 区域6 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| from scratch | 82.9 | 81.2 | 82.8 | 82.8 | 83.1 | 43.6 | 34.6 | 39.9 | 39.4
    | 43.9 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | 82.9 | 81.2 | 82.8 | 82.8 | 83.1 | 43.6 | 34.6 | 39.9 | 39.4 | 43.9
    |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | 83.5(+0.6) | 81.2(+0.0) | 84.0(+1.2) | 82.9(+0.1)
    | 83.3(+0.2) | 44.7(+1.1) | 34.9(+0.3) | 42.4(+2.5) | 39.9(+0.5) | 43.9(+0.0)
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | 83.5(+0.6) | 81.2(+0.0) | 84.0(+1.2) | 82.9(+0.1)
    | 83.3(+0.2) | 44.7(+1.1) | 34.9(+0.3) | 42.4(+2.5) | 39.9(+0.5) | 43.9(+0.0)
    |'
- en: '| ParAE [[147](#bib.bib147)] | 91.8(+8.9) | 82.3(+1.1) | 89.5(+6.7) | 88.2(+5.4)
    | 86.4(+3.3) | 53.5(+9.9) | 38.5(+3.9) | 48.4(+8.5) | 45.0(+5.6) | 49.2(+5.3)
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| ParAE [[147](#bib.bib147)] | 91.8(+8.9) | 82.3(+1.1) | 89.5(+6.7) | 88.2(+5.4)
    | 86.4(+3.3) | 53.5(+9.9) | 38.5(+3.9) | 48.4(+8.5) | 45.0(+5.6) | 49.2(+5.3)
    |'
- en: '| Method | OA on area 6 with different train area | mIoU on area 6 with different
    train area |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 在不同训练区域的区域 6 的 OA | 在不同训练区域的区域 6 的 mIoU |'
- en: '| Area1 | Area2 | Area3 | Area4 | Area5 | Area1 | Area2 | Area3 | Area4 | Area5
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 区域1 | 区域2 | 区域3 | 区域4 | 区域5 | 区域1 | 区域2 | 区域3 | 区域4 | 区域5 |'
- en: '| from scratch | 84.6 | 70.6 | 77.7 | 73.6 | 76.9 | 57.9 | 38.9 | 49.5 | 38.5
    | 48.6 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | 84.6 | 70.6 | 77.7 | 73.6 | 76.9 | 57.9 | 38.9 | 49.5 | 38.5 | 48.6
    |'
- en: '| STRL [[1](#bib.bib1)] | 85.3(+0.7) | 72.4(+1.8) | 79.1(+1.4) | 73.8(+0.2)
    | 77.3(+0.4) | 59.2(+1.3) | 39.2(+0.8) | 51.9(+2.4) | 39.3(+0.8) | 49.5(+0.9)
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| STRL [[1](#bib.bib1)] | 85.3(+0.7) | 72.4(+1.8) | 79.1(+1.4) | 73.8(+0.2)
    | 77.3(+0.4) | 59.2(+1.3) | 39.2(+0.8) | 51.9(+2.4) | 39.3(+0.8) | 49.5(+0.9)
    |'
- en: 'Table [V](#S6.T5 "TABLE V ‣ 6.1 Evaluation Criteria ‣ 6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") lists fine-tuning performance on the ModelNet40 and ScanObjectNN datasets.
    We can see that classification models initialized with unsupervised pre-trained
    weights always achieve better classification performances as compared with random
    initialization, regardless of backbone architectures. On the other hand, the performance
    gaps are still limited, largely due to the limited size and diversity of the pre-training
    datasets (i.e., ShapeNet and ModelNet40) and the simplicity of existing backbone
    models. In comparison, thanks to the much larger pre-training datasets ImageNet [[33](#bib.bib33)]
    and the more powerful backbone network ResNet  [[37](#bib.bib37)], the state-of-the-art
    methods for unsupervised pre-training of 2D images are able to achieve more significant
    performance gains in the classification task. As discussed in Section [7](#S7
    "7 Future direction ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey"), we expect more diverse datasets and more advanced
    and generous backbone models that can set stronger foundations for this field.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [V](#S6.T5 "TABLE V ‣ 6.1 Evaluation Criteria ‣ 6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") 列出了 ModelNet40 和 ScanObjectNN 数据集上的微调性能。我们可以看到，与随机初始化相比，使用无监督预训练权重初始化的分类模型始终能够获得更好的分类性能，无论主干网络架构如何。另一方面，性能差距仍然有限，主要由于预训练数据集（即
    ShapeNet 和 ModelNet40）的大小和多样性有限以及现有主干模型的简单性。相比之下，由于预训练数据集 ImageNet [[33](#bib.bib33)]
    规模更大以及更强大的主干网络 ResNet [[37](#bib.bib37)]，2D 图像的无监督预训练最先进方法能够在分类任务中获得更显著的性能提升。如第
    [7](#S7 "7 Future direction ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") 节讨论，我们期待更多样的数据集以及更先进、更强大的主干模型为这一领域奠定更强的基础。'
- en: 6.2.2 Object part segmentation
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 对象部件分割
- en: 'Table [VI](#S6.T6 "TABLE VI ‣ 6.2.1 Object classification ‣ 6.2 Object-level
    tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") presents the benchmarking of object part
    segmentation on the ShapeNetPart dataset [[14](#bib.bib14)] using the linear classification
    protocol (i.e., ”Unsup.” in Table [VI](#S6.T6 "TABLE VI ‣ 6.2.1 Object classification
    ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey")) and the fine-tuning
    protocol (i.e., ”Trans.” in Table [VI](#S6.T6 "TABLE VI ‣ 6.2.1 Object classification
    ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey")) as described in
    Section [6.1](#S6.SS1 "6.1 Evaluation Criteria ‣ 6 Benchmark performances ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey"). As
    the table shows, the performance gaps between unsupervised and supervised learning
    (i.e., ”Unsup.” vs. ”Sup.”) are decreasing. In addition, unsupervised pre-training
    achieves better performance in most cases under the fine-tuning protocol (i.e.,
    ”Trans.” vs. ”Sup.”), though the improvement is still limited.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [VI](#S6.T6 "TABLE VI ‣ 6.2.1 Object classification ‣ 6.2 Object-level tasks
    ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") 展示了使用线性分类协议（即表 [VI](#S6.T6 "TABLE VI ‣ 6.2.1
    Object classification ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey") 中的“Unsup.”）和微调协议（即表
    [VI](#S6.T6 "TABLE VI ‣ 6.2.1 Object classification ‣ 6.2 Object-level tasks ‣
    6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning with
    Deep Neural Networks: A Survey") 中的“Trans.”）在 ShapeNetPart 数据集 [[14](#bib.bib14)]
    上的对象部件分割基准测试。如表所示，无监督学习和监督学习（即“Unsup.” 与 “Sup.”）之间的性能差距正在缩小。此外，无监督预训练在大多数情况下在微调协议下（即“Trans.”
    与 “Sup.”）表现更好，尽管改进仍然有限。'
- en: 'TABLE VIII: Performances for semantic segmentation on S3DIS [[21](#bib.bib21)].
    Upper part: Models are tested on Area5 (Fold#1) and trained on the rest of the
    data. Lower part: Six-fold cross-validation over three runs.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VIII：S3DIS 上的语义分割性能 [[21](#bib.bib21)]。上半部分：模型在 Area5（Fold#1）上进行测试，其余数据上进行训练。下半部分：三次运行的六折交叉验证。
- en: '| Method | Backbone | mACC | mIoU |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | mACC | mIoU |'
- en: '| from scratch | SR-UNet | 75.5 | 68.2 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | SR-UNet | 75.5 | 68.2 |'
- en: '| PointConstrast [[54](#bib.bib54)] | 77.0 | 70.9 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| PointConstrast [[54](#bib.bib54)] | 77.0 | 70.9 |'
- en: '| DepthContrast [[104](#bib.bib104)] | - | 70.6 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| DepthContrast [[104](#bib.bib104)] | - | 70.6 |'
- en: '| Method | Backbone | OA | mIoU |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 骨干网络 | OA | mIoU |'
- en: '| from scratch | PointNet | 78.2 | 47.0 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | PointNet | 78.2 | 47.0 |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | 80.1 | 52.6 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | 80.1 | 52.6 |'
- en: '| OcCo [[12](#bib.bib12)] | 82.0 | 54.9 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | 82.0 | 54.9 |'
- en: '| from scratch | PCN | 82.9 | 51.1 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | PCN | 82.9 | 51.1 |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | 83.7 | 52.2 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | 83.7 | 52.2 |'
- en: '| OcCo [[12](#bib.bib12)] | 85.1 | 53.4 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | 85.1 | 53.4 |'
- en: '| from scratch | DGCNN | 83.7 | 54.9 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | DGCNN | 83.7 | 54.9 |'
- en: '| Jigsaw3D [[13](#bib.bib13)] | 84.1 | 55.6 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Jigsaw3D [[13](#bib.bib13)] | 84.1 | 55.6 |'
- en: '| OcCo [[12](#bib.bib12)] | 84.6 | 58.0 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| OcCo [[12](#bib.bib12)] | 84.6 | 58.0 |'
- en: 6.3 Scene-level tasks
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 场景级任务
- en: 'As discussed in Section [5.2](#S5.SS2 "5.2 Context-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"), unsupervised pre-training in scene-level
    tasks has recently become prevalent due to its enormous potential in various applications.
    This comes with a series of 3D URL studies that investigate the effectiveness
    of pre-training over different scene-level point cloud datasets. We provide a
    comprehensive benchmarking of these methods with respect to different 3D tasks.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[5.2节](#S5.SS2 "5.2 基于上下文的方法 ‣ 5 无监督点云表示学习 ‣ 使用深度神经网络的无监督点云表示学习：综述")中讨论的那样，由于在各种应用中的巨大潜力，无监督预训练在场景级任务中最近变得越来越普遍。这伴随着一系列
    3D URL 研究，调查了在不同场景级点云数据集上预训练的有效性。我们提供了这些方法在不同 3D 任务上的综合基准测试。
- en: 'TABLE IX: Comparison of pre-training effects by different unsupervised learning
    methods. The benchmarking is 3D object detection task over datasets SUN RGB-D [[28](#bib.bib28)]
    and ScanNet-V2 [[18](#bib.bib18)]. “@0.25“ and “@0.5“ represent per-category results
    of average precision (AP) with IoU threshold 0.25 (mAP@0.25) and 0.5 (mAP@0.5),
    respectively.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：不同无监督学习方法的预训练效果比较。基准是对数据集 SUN RGB-D [[28](#bib.bib28)] 和 ScanNet-V2 [[18](#bib.bib18)]
    的 3D 目标检测任务。“@0.25” 和 “@0.5” 分别表示 IoU 阈值 0.25 (mAP@0.25) 和 0.5 (mAP@0.5) 的每类结果的平均精度
    (AP)。
- en: '| Method | Backbone | SUN RGB-D | ScanNet-V2 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 骨干网络 | SUN RGB-D | ScanNet-V2 |'
- en: '| @0.5 | @0.25 | @0.5 | @0.25 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| @0.5 | @0.25 | @0.5 | @0.25 |'
- en: '| from scratch | SR-UNet | 31.7 | 55.6 | 35.4 | 56.7 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | SR-UNet | 31.7 | 55.6 | 35.4 | 56.7 |'
- en: '| PointConstrast [[54](#bib.bib54)] | 34.8 | 57.5 | 38.0 | 58.5 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| PointConstrast [[54](#bib.bib54)] | 34.8 | 57.5 | 38.0 | 58.5 |'
- en: '| PC-FractalDB [[106](#bib.bib106)] | 35.9 | 57.1 | 37.0 | 59.4 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| PC-FractalDB [[106](#bib.bib106)] | 35.9 | 57.1 | 37.0 | 59.4 |'
- en: '| from scratch | VoteNet | 32.9 | 57.7 | 33.5 | 58.6 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | VoteNet | 32.9 | 57.7 | 33.5 | 58.6 |'
- en: '| STRL [[1](#bib.bib1)] | - | 58.2 | - | - |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| STRL [[1](#bib.bib1)] | - | 58.2 | - | - |'
- en: '| RandRooms [[103](#bib.bib103)] | 35.4 | 59.2 | 36.2 | 61.3 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| RandRooms [[103](#bib.bib103)] | 35.4 | 59.2 | 36.2 | 61.3 |'
- en: '| DepthContrast [[104](#bib.bib104)] | - | - | - | 62.2 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| DepthContrast [[104](#bib.bib104)] | - | - | - | 62.2 |'
- en: '| CSC [[3](#bib.bib3)] | 33.6 | - | - | - |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| CSC [[3](#bib.bib3)] | 33.6 | - | - | - |'
- en: '| PointContrast [[54](#bib.bib54)] | 34.0 | - | 38.0 | - |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| PointContrast [[54](#bib.bib54)] | 34.0 | - | 38.0 | - |'
- en: '| 4DContrast [[107](#bib.bib107)] | 34.4 | - | 39.3 | - |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 4DContrast [[107](#bib.bib107)] | 34.4 | - | 39.3 | - |'
- en: '| from scratch | PointNet++ | - | 57.5 | - | 58.6 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | PointNet++ | - | 57.5 | - | 58.6 |'
- en: '| PointContrast [[54](#bib.bib54)] | - | 57.9 | - | 58.5 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| PointContrast [[54](#bib.bib54)] | - | 57.9 | - | 58.5 |'
- en: '| RandRooms [[103](#bib.bib103)] | - | 59.2 | - | 61.3 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| RandRooms [[103](#bib.bib103)] | - | 59.2 | - | 61.3 |'
- en: '| DepthContrast [[104](#bib.bib104)] | - | 60.7 | - | - |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| DepthContrast [[104](#bib.bib104)] | - | 60.7 | - | - |'
- en: '| PC-FractalDB [[106](#bib.bib106)] | 33.9 | 59.4 | 38.3 | 61.9 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| PC-FractalDB [[106](#bib.bib106)] | 33.9 | 59.4 | 38.3 | 61.9 |'
- en: '| DPCo [[108](#bib.bib108)] | 35.6 | 59.8 | 41.5 | 64.2 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| DPCo [[108](#bib.bib108)] | 35.6 | 59.8 | 41.5 | 64.2 |'
- en: '| from scratch | H3DNet | 39.0 | 60.1 | 48.1 | 67.3 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | H3DNet | 39.0 | 60.1 | 48.1 | 67.3 |'
- en: '| RandRooms [[103](#bib.bib103)] | 43.1 | 61.6 | 51.5 | 68.6 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| RandRooms [[103](#bib.bib103)] | 43.1 | 61.6 | 51.5 | 68.6 |'
- en: 'TABLE X: Object detection performance on dataset ONCE [[30](#bib.bib30)]. The
    baseline is trained from scratch. Unsupervised learning methods are used for pre-training
    models. $U_{small}$, $U_{median}$, and $U_{large}$ represent small, medium, and
    large amounts of unlabelled data that are used for unsupervised learning, respectively.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 表 X：数据集 ONCE [[30](#bib.bib30)] 上的目标检测性能。基线模型是从头开始训练的。无监督学习方法用于预训练模型。$U_{small}$、$U_{median}$
    和 $U_{large}$ 分别表示用于无监督学习的小、中、大量未标记数据。
- en: '| Method | Vehicle | Pedestrian | Cyclist | mAP |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 车辆 | 行人 | 骑行者 | mAP |'
- en: '| Baseline [[149](#bib.bib149)] | 69.7 | 26.1 | 59.9 | 51.9 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 基线 [[149](#bib.bib149)] | 69.7 | 26.1 | 59.9 | 51.9 |'
- en: '| $U_{small}$ |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| $U_{small}$ |'
- en: '| BYOL [[8](#bib.bib8)] | 67.6 | 17.2 | 53.4 | 46.1 (-5.8) |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| BYOL [[8](#bib.bib8)] | 67.6 | 17.2 | 53.4 | 46.1 (-5.8) |'
- en: '| PointContrast [[54](#bib.bib54)] | 71.5 | 22.7 | 58.0 | 50.8 (-0.1) |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| PointContrast [[54](#bib.bib54)] | 71.5 | 22.7 | 58.0 | 50.8 (-0.1) |'
- en: '| SwAV [[150](#bib.bib150)] | 72.3 | 25.1 | 60.7 | 52.7 (+0.8) |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| SwAV [[150](#bib.bib150)] | 72.3 | 25.1 | 60.7 | 52.7 (+0.8) |'
- en: '| DeepCluster [[125](#bib.bib125)] | 72.1 | 27.6 | 50.3 | 53.3 (+1.4) |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| DeepCluster [[125](#bib.bib125)] | 72.1 | 27.6 | 50.3 | 53.3 (+1.4) |'
- en: '| $U_{median}$ |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| $U_{median}$ |'
- en: '| BYOL [[8](#bib.bib8)] | 69.7 | 27.3 | 57.2 | 51.4 (-0.5) |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| BYOL [[8](#bib.bib8)] | 69.7 | 27.3 | 57.2 | 51.4 (-0.5) |'
- en: '| PointContrast [[54](#bib.bib54)] | 70.2 | 29.2 | 58.9 | 52.8 (+0.9) |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| PointContrast [[54](#bib.bib54)] | 70.2 | 29.2 | 58.9 | 52.8 (+0.9) |'
- en: '| SwAV [[150](#bib.bib150)] | 72.1 | 28.0 | 60.2 | 53.4 (+1.5) |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| SwAV [[150](#bib.bib150)] | 72.1 | 28.0 | 60.2 | 53.4 (+1.5) |'
- en: '| DeepCluster [[125](#bib.bib125)] | 72.1 | 30.1 | 60.5 | 54.2 (+2.3) |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| DeepCluster [[125](#bib.bib125)] | 72.1 | 30.1 | 60.5 | 54.2 (+2.3) |'
- en: '| $U_{large}$ |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| $U_{large}$ |'
- en: '| BYOL [[8](#bib.bib8)] | 72.2 | 23.6 | 60.5 | 52.1 (+0.2) |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| BYOL [[8](#bib.bib8)] | 72.2 | 23.6 | 60.5 | 52.1 (+0.2) |'
- en: '| PointContrast [[54](#bib.bib54)] | 73.2 | 27.5 | 58.3 | 53.0 (+1.1) |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| PointContrast [[54](#bib.bib54)] | 73.2 | 27.5 | 58.3 | 53.0 (+1.1) |'
- en: '| SwAV [[150](#bib.bib150)] | 72.0 | 30.6 | 60.3 | 54.3 (+2.4) |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| SwAV [[150](#bib.bib150)] | 72.0 | 30.6 | 60.3 | 54.3 (+2.4) |'
- en: '| DeepCluster [[125](#bib.bib125)] | 71.9 | 30.5 | 60.4 | 54.3 (+2.4) |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| DeepCluster [[125](#bib.bib125)] | 71.9 | 30.5 | 60.4 | 54.3 (+2.4) |'
- en: 'TABLE XI: Performances of instance segmentation on datasets S3DIS [[21](#bib.bib21)]
    and ScanNet-V2 [[18](#bib.bib18)]. It reports the mean of average precision (mAP)
    across all semantic classes with a 3D IoU threshold of 0.25.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XI: 实例分割在数据集 S3DIS [[21](#bib.bib21)] 和 ScanNet-V2 [[18](#bib.bib18)] 上的性能。报告了所有语义类别的平均精度（mAP）的均值，3D
    IoU 阈值为 0.25。'
- en: '| Method | Backbone | S3DIS | ScanNet |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 骨干网络 | S3DIS | ScanNet |'
- en: '| --- | --- | --- | --- |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| from scratch | SR-UNet | 59.3 | 53.4 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始 | SR-UNet | 59.3 | 53.4 |'
- en: '| PointContrast [[54](#bib.bib54)] | 60.5 | 55.8 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| PointContrast [[54](#bib.bib54)] | 60.5 | 55.8 |'
- en: '| CSC [[3](#bib.bib3)] | 63.4 | 56.5 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| CSC [[3](#bib.bib3)] | 63.4 | 56.5 |'
- en: '| 4DContrast [[107](#bib.bib107)] | - | 57.6 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 4DContrast [[107](#bib.bib107)] | - | 57.6 |'
- en: 'Tables [VII](#S6.T7 "TABLE VII ‣ 6.2.1 Object classification ‣ 6.2 Object-level
    tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") and [VIII](#S6.T8 "TABLE VIII ‣ 6.2.2 Object
    part segmentation ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey") show
    the performances of semantic segmentation on the S3DIS [[21](#bib.bib21)] dataset.
    We summarized them separately since different fine-tuning setups have been used
    in prior works. In Table [VII](#S6.T7 "TABLE VII ‣ 6.2.1 Object classification
    ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey"), the unsupervised
    pre-trained DGCNN is fine-tuned on every single area of S3DIS and tested on either
    Area 5 (the upper part of table) or Area 6 (the lower part of the table). Table
    [VIII](#S6.T8 "TABLE VIII ‣ 6.2.2 Object part segmentation ‣ 6.2 Object-level
    tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") instead shows the performance of fine-tuning
    different segmentation networks with the whole dataset by following the one-fold
    (in the upper part of the table) and six-fold cross-validation setups (in the
    lower part of the table), respectively.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [VII](#S6.T7 "TABLE VII ‣ 6.2.1 物体分类 ‣ 6.2 物体级任务 ‣ 6 基准性能 ‣ 无监督点云表示学习与深度神经网络：一项综述")
    和 [VIII](#S6.T8 "TABLE VIII ‣ 6.2.2 物体部件分割 ‣ 6.2 物体级任务 ‣ 6 基准性能 ‣ 无监督点云表示学习与深度神经网络：一项综述")
    展示了在 S3DIS [[21](#bib.bib21)] 数据集上的语义分割性能。我们将它们分别总结，因为以前的工作使用了不同的微调设置。在表 [VII](#S6.T7
    "TABLE VII ‣ 6.2.1 物体分类 ‣ 6.2 物体级任务 ‣ 6 基准性能 ‣ 无监督点云表示学习与深度神经网络：一项综述") 中，无监督预训练的
    DGCNN 在 S3DIS 的每个区域上进行了微调，并在 Area 5（表的上部）或 Area 6（表的下部）上进行测试。表 [VIII](#S6.T8 "TABLE
    VIII ‣ 6.2.2 物体部件分割 ‣ 6.2 物体级任务 ‣ 6 基准性能 ‣ 无监督点云表示学习与深度神经网络：一项综述") 则展示了微调不同分割网络在整个数据集上的性能，分别遵循一折（表的上部）和六折交叉验证设置（表的下部）。
- en: 'We also summarize existing works that handle unsupervised pre-training for
    object detection. Tables [IX](#S6.T9 "TABLE IX ‣ 6.3 Scene-level tasks ‣ 6 Benchmark
    performances ‣ Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey") and [X](#S6.T10 "TABLE X ‣ 6.3 Scene-level tasks ‣ 6 Benchmark
    performances ‣ Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey") show their performances over indoor datasets including SUN
    RGB-D [[28](#bib.bib28)] and ScanNet-V2 [[18](#bib.bib18)] as well as outdoor
    LiDAR dataset ONCE [[30](#bib.bib30)], respectively. In addition, several works
    investigated unsupervised pre-training for instance segmentation. We summarize
    their performance over S3DIS [[21](#bib.bib21)] and ScanNet-V2 [[18](#bib.bib18)]
    in Table [XI](#S6.T11 "TABLE XI ‣ 6.3 Scene-level tasks ‣ 6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey").'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还总结了处理物体检测的无监督预训练的现有工作。表 [IX](#S6.T9 "TABLE IX ‣ 6.3 Scene-level tasks ‣
    6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning with
    Deep Neural Networks: A Survey") 和 [X](#S6.T10 "TABLE X ‣ 6.3 Scene-level tasks
    ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") 展示了它们在室内数据集（包括 SUN RGB-D [[28](#bib.bib28)]
    和 ScanNet-V2 [[18](#bib.bib18)]）以及室外 LiDAR 数据集 ONCE [[30](#bib.bib30)] 上的性能。此外，一些工作还研究了用于实例分割的无监督预训练。我们在表
    [XI](#S6.T11 "TABLE XI ‣ 6.3 Scene-level tasks ‣ 6 Benchmark performances ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey") 中总结了它们在
    S3DIS [[21](#bib.bib21)] 和 ScanNet-V2 [[18](#bib.bib18)] 上的性能。'
- en: It is inspiring to see that unsupervised learning representation can generalize
    across domains and boost performances over multiple high-level 3D tasks as compared
    with training from scratch. These experiments demonstrate the huge potential of
    URL of point clouds in saving expensive human annotations. However, the improvements
    are still limited and we expect more research in this area.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 令人振奋的是，无监督学习表示可以跨领域泛化，并在多个高层次 3D 任务中提升性能，相比于从头开始训练。这些实验展示了点云 URL 在节省昂贵的人工标注方面的巨大潜力。然而，改进仍然有限，我们期待在这一领域有更多的研究。
- en: 7 Future direction
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来方向
- en: URL of point clouds has achieved significant progress during the last decade.
    We share several potential future research directions of this research field in
    this section.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 点云的 URL 在过去十年中取得了显著进展。在本节中，我们分享了该研究领域的若干潜在未来研究方向。
- en: 'Unified 3D backbones are needed: One major reason of the great success of deep
    learning in 2D computer vision is the standardization of CNN architectures with
    VGG [[36](#bib.bib36)], ResNet [[37](#bib.bib37)], etc. For example, the unified
    backbone structures greatly facilitate knowledge transfer across different datasets
    and tasks. For 3D point clouds, similar development is far under-explored, despite
    a variety of 3D deep architectures that have been recently reported. This can
    be observed from the URL methods in tables in Section [6](#S6 "6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") most of which adopted very different backbone models. This impedes
    the development of 3D point cloud networks in scalable design and efficient deployment
    in various new tasks. Designing certain universal backbones that can be as ubiquitous
    as ResNet in 2D computer vision is crucial for the advance of 3D point cloud networks
    including unsupervised point cloud representation learning.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '需要统一的 3D 骨干网：深度学习在 2D 计算机视觉中取得巨大成功的一个主要原因是 CNN 架构的标准化，如 VGG [[36](#bib.bib36)]、ResNet
    [[37](#bib.bib37)] 等。例如，统一的骨干网结构大大促进了不同数据集和任务之间的知识迁移。对于 3D 点云，尽管最近报告了各种 3D 深度架构，但类似的发展尚未得到充分探索。这可以从第
    [6](#S6 "6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") 节中的 URL 方法表中观察到，其中大多数采用了非常不同的骨干网模型。这阻碍了
    3D 点云网络在可扩展设计和在各种新任务中的高效部署的发展。设计某些可以像 2D 计算机视觉中的 ResNet 一样普遍的通用骨干网，对于 3D 点云网络的发展，包括无监督点云表示学习，至关重要。'
- en: 'Larger datasets are needed: As described in Section [3](#S3 "3 Point cloud
    datasets ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), most existing URL datasets were originally collected for the task
    of supervised learning. Since point cloud annotation is laborious and time-consuming,
    these datasets are severely constrained in data size and data diversity and are
    not suitable for URL with point clouds which usually requires large amounts of
    point clouds of good size and diversity. This issue well explains the trivial
    improvements by URL in tables in Section [6](#S6 "6 Benchmark performances ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey"). Hence,
    it is urgent to collect large-scale and high-quality unlabelled point cloud datasets
    of sufficient diversity in terms of object-level and scene-level point clouds,
    indoor and outdoor point clouds, etc.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 需要更大的数据集：如第[3节](#S3 "3 点云数据集 ‣ 基于深度神经网络的无监督点云表示学习：综述")所述，大多数现有的URL数据集最初是为监督学习任务收集的。由于点云注释繁琐且耗时，这些数据集在数据规模和数据多样性方面受到严重限制，不适合URL，因为URL通常需要大量具有良好规模和多样性的点云。这一问题很好地解释了第[6节](#S6
    "6 基准性能 ‣ 基于深度神经网络的无监督点云表示学习：综述")中表格中URL的微小改进。因此，迫切需要收集大规模、高质量的未标注点云数据集，这些数据集在物体级和场景级点云、室内和室外点云等方面具有足够的多样性。
- en: 'Unsupervised pre-training for scene-level tasks: As described in Section [5.2](#S5.SS2
    "5.2 Context-based methods ‣ 5 Unsupervised point cloud representation learning
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), most earlier research focuses on object-level point cloud processing
    though several pioneer studies [[54](#bib.bib54), [3](#bib.bib3), [123](#bib.bib123),
    [103](#bib.bib103), [1](#bib.bib1)] explored how to pre-train DNNs on scene-level
    point clouds for improving various scene-level downstream tasks such as object
    detection and instance segmentation. Prior studies show that the learned unsupervised
    representations can effectively generalize across domains and tasks. Hence, URL
    of scene-level point clouds deserves more attention as a new direction due to
    its great potential in a variety of applications. On the other hand, the research
    along this line remains at a nascent stage, largely due to the constraints in
    network architectures and datasets. We foresee that more related research will
    be conducted in the near future.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 场景级任务的无监督预训练：如第[5.2节](#S5.SS2 "5.2 基于上下文的方法 ‣ 5 无监督点云表示学习 ‣ 基于深度神经网络的无监督点云表示学习：综述")所述，大多数早期研究关注于物体级点云处理，尽管一些开创性的研究[[54](#bib.bib54),
    [3](#bib.bib3), [123](#bib.bib123), [103](#bib.bib103), [1](#bib.bib1)]探讨了如何在场景级点云上进行DNN的预训练，以提升物体检测和实例分割等各种场景级下游任务。之前的研究表明，学到的无监督表示可以有效地在领域和任务之间进行泛化。因此，场景级点云的URL因其在各种应用中的巨大潜力值得更多关注。另一方面，这方面的研究仍处于初期阶段，主要由于网络架构和数据集的限制。我们预见到在不久的将来会有更多相关研究进行。
- en: 'Learning representations from multi-modal data: 3D sensors are often equipped
    with other sensors that can capture additional and complementary information to
    point clouds. For example, depth cameras are often equipped with optical sensors
    for capturing better appearance information. LiDAR sensors, optical sensors, GPU,
    and IMU are often installed together as a sensor suite to capture complementary
    information and provide certain redundancy in autonomous vehicles and mobile robot
    navigation. Unsupervised learning from such multi-modal data has attracted increasing
    attention in recent years. For example, learning correspondences among multi-modal
    data has been explored as pre-text tasks for unsupervised learning as described
    in Section [5.3](#S5.SS3 "5.3 Multiple modal-based methods ‣ 5 Unsupervised point
    cloud representation learning ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey"). However, the study along this line of research
    remains under-investigated and we expect more related research point clouds, RGB
    images, depth maps, etc.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '从多模态数据中学习表示：3D传感器通常配备其他传感器，这些传感器可以捕捉点云的额外和补充信息。例如，深度相机通常配备光学传感器以捕获更好的外观信息。LiDAR传感器、光学传感器、GPU和IMU常常被一起安装作为传感器套件，以捕获互补信息并在自主车辆和移动机器人导航中提供一定的冗余。近年来，从这种多模态数据中进行无监督学习引起了越来越多的关注。例如，已探索将多模态数据之间的对应关系作为无监督学习的预文本任务，如第[5.3](#S5.SS3
    "5.3 Multiple modal-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey")节所述。然而，这方面的研究仍然不够深入，我们期待更多关于点云、RGB图像、深度图等的相关研究。'
- en: 'Learning Spatio-temporal representations: 3D sensors that support capturing
    sequential point clouds are becoming increasingly popular nowadays. Rich temporal
    information from point cloud streams can be extracted as useful supervision signals
    for unsupervised learning while most of the existing works still focus on static
    point clouds. We expect that more effective pretext tasks will be designed that
    can effectively learn spatio-temporal representations from unlabelled sequential
    point cloud frames.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 学习时空表示：支持捕捉序列点云的3D传感器在如今变得越来越受欢迎。可以从点云流中提取丰富的时间信息，作为无监督学习的有用监督信号，而现有的大多数工作仍然集中在静态点云上。我们期望设计出更多有效的预文本任务，以有效地从未标记的序列点云帧中学习时空表示。
- en: 8 Conclusion
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: Unsupervised representation learning aims to learn effective representations
    from unannotated data, which has demonstrated impressive progress in the research
    with point cloud data. This paper presents a contemporary survey of unsupervised
    representation learning of point clouds. It first introduces the widely adopted
    datasets and deep network architectures. A comprehensive taxonomy and detailed
    review of methods are then presented. Following that, representative methods are
    discussed and benchmarked over multiple 3D point cloud tasks. Finally, we share
    our humble opinions about several potential future research directions. We hope
    that this work can lay a strong and sound foundation for future research in unsupervised
    representation learning from point cloud data.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督表示学习旨在从未标注的数据中学习有效的表示，这在点云数据研究中取得了显著进展。本文对点云的无监督表示学习进行了现代化综述。首先介绍了广泛采用的数据集和深度网络架构。接着，提供了方法的全面分类法和详细回顾。随后，讨论并基准测试了多种3D点云任务中的代表性方法。最后，我们分享了对几个潜在未来研究方向的谦虚看法。我们希望这项工作能够为未来在点云数据中的无监督表示学习奠定坚实和可靠的基础。
- en: Acknowledgments
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This project is funded in part by the Ministry of Education Singapore, under
    the Tier-1 scheme with project number RG18/22\. It is also supported in part under
    the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP)
    Funding Initiative, as well as cash and in-kind contributions from Singapore Telecommunications
    Limited (Singtel), through Singtel Cognitive and Artificial Intelligence Lab for
    Enterprises (SCALE@NTU).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目部分由新加坡教育部资助，属于Tier-1计划，项目编号为RG18/22。它还部分得到RIE2020行业对接基金——行业合作项目（IAF-ICP）资助，并通过新加坡电信有限公司（Singtel）在Singtel认知与人工智能企业实验室（SCALE@NTU）的现金和实物贡献得到支持。
- en: References
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Huang, Y. Xie, S.-C. Zhu, and Y. Zhu, “Spatio-temporal self-supervised
    representation learning for 3d point clouds,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 6535–6545.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Huang, Y. Xie, S.-C. Zhu 和 Y. Zhu，“用于3D点云的时空自监督表征学习，”载于*IEEE/CVF国际计算机视觉大会论文集*，2021年，第6535–6545页。'
- en: '[2] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep learning
    for 3d point clouds: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, 2020.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu 和 M. Bennamoun，“3D点云的深度学习：综述，”*IEEE模式分析与机器智能汇刊*，2020年。'
- en: '[3] J. Hou, B. Graham, M. Nießner, and S. Xie, “Exploring data-efficient 3d
    scene understanding with contrastive scene contexts,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 15 587–15 597.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Hou, B. Graham, M. Nießner 和 S. Xie，“利用对比场景上下文探索数据高效的3D场景理解，”载于*IEEE/CVF计算机视觉与模式识别大会论文集*，2021年，第15 587–15 597页。'
- en: '[4] L. Jing, L. Zhang, and Y. Tian, “Self-supervised feature learning by cross-modality
    and cross-view correspondences,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2021, pp. 1581–1591.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] L. Jing, L. Zhang 和 Y. Tian，“通过跨模态和跨视角对应进行自监督特征学习，”载于*IEEE/CVF计算机视觉与模式识别大会论文集*，2021年，第1581–1591页。'
- en: '[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *等*，“语言模型是无监督的多任务学习者，”*OpenAI博客*，第1卷，第8期，第9页，2019年。'
- en: '[6] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep
    bidirectional transformers for language understanding,” in *Proceedings of NAACL-HLT*,
    2019, pp. 4171–4186.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. D. M.-W. C. Kenton 和 L. K. Toutanova，“Bert：用于语言理解的深度双向变换器的预训练，”载于*NAACL-HLT会议论文集*，2019年，第4171–4186页。'
- en: '[7] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised
    visual representation learning,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2020, pp. 9729–9738.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] K. He, H. Fan, Y. Wu, S. Xie 和 R. Girshick，“用于无监督视觉表征学习的动量对比，”载于*IEEE/CVF计算机视觉与模式识别大会论文集*，2020年，第9729–9738页。'
- en: '[8] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya,
    C. Doersch, B. Pires, Z. Guo, M. Azar *et al.*, “Bootstrap your own latent: A
    new approach to self-supervised learning,” in *Neural Information Processing Systems*,
    2020.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya,
    C. Doersch, B. Pires, Z. Guo, M. Azar *等*，“自引导潜在：一种新的自监督学习方法，”载于*神经信息处理系统*，2020年。'
- en: '[9] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with momentum
    contrastive learning,” *arXiv preprint arXiv:2003.04297*, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] X. Chen, H. Fan, R. Girshick 和 K. He，“通过动量对比学习改进基线，”*arXiv预印本 arXiv:2003.04297*，2020年。'
- en: '[10] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders
    are scalable vision learners,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 16 000–16 009.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] K. He, X. Chen, S. Xie, Y. Li, P. Dollár 和 R. Girshick，“掩码自编码器是可扩展的视觉学习者，”载于*IEEE/CVF计算机视觉与模式识别大会论文集*，2022年，第16 000–16 009页。'
- en: '[11] D. Valsesia, G. Fracastoro, and E. Magli, “Learning localized generative
    models for 3d point clouds via graph convolution,” in *International conference
    on learning representations*, 2018.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] D. Valsesia, G. Fracastoro 和 E. Magli，“通过图卷积学习局部生成模型用于3D点云，”载于*国际学习表征会议*，2018年。'
- en: '[12] H. Wang, Q. Liu, X. Yue, J. Lasenby, and M. J. Kusner, “Unsupervised point
    cloud pre-training via occlusion completion,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision (ICCV)*, October 2021, pp. 9782–9792.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] H. Wang, Q. Liu, X. Yue, J. Lasenby 和 M. J. Kusner，“通过遮挡补全进行无监督点云预训练，”载于*IEEE/CVF国际计算机视觉大会论文集（ICCV）*，2021年10月，第9782–9792页。'
- en: '[13] J. Sauder and B. Sievers, “Self-supervised deep learning on point clouds
    by reconstructing space,” *Advances in Neural Information Processing Systems*,
    vol. 32, pp. 12 962–12 972, 2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Sauder 和 B. Sievers，“通过重建空间进行点云的自监督深度学习，”*神经信息处理系统进展*，第32卷，第12 962–12 972页，2019年。'
- en: '[14] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3d model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S.
    Savarese, M. Savva, S. Song, H. Su *等*，“Shapenet：一个信息丰富的3D模型库，”*arXiv预印本 arXiv:1512.03012*，2015年。'
- en: '[15] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. R. Qi, H. Su, K. Mo 和 L. J. Guibas，“PointNet：用于 3D 分类和分割的点集深度学习，”在
    *IEEE计算机视觉与模式识别会议论文集*，2017年，第652–660页。'
- en: '[16] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for 3d
    object detection in point clouds,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 9277–9286.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. R. Qi, O. Litany, K. He 和 L. J. Guibas，“用于点云中 3D 目标检测的深度 Hough 投票，”在
    *IEEE/CVF国际计算机视觉会议论文集*，2019年，第9277–9286页。'
- en: '[17] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation
    and detection from point cloud,” in *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*, 2019, pp. 770–779.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Shi, X. Wang 和 H. Li，“PointRCNN：从点云生成和检测 3D 目标提案，”在 *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第770–779页。'
- en: '[18] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2017, pp.
    5828–5839.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser 和 M. Nießner，“ScanNet：丰富注释的室内场景
    3D 重建，”在 *IEEE计算机视觉与模式识别会议论文集*，2017年，第5828–5839页。'
- en: '[19] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The kitti dataset,” *The International Journal of Robotics Research*, vol. 32,
    no. 11, pp. 1231–1237, 2013.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Geiger, P. Lenz, C. Stiller 和 R. Urtasun，“视觉遇见机器人：Kitti 数据集，” *国际机器人研究杂志*，第32卷，第11期，第1231–1237页，2013年。'
- en: '[20] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for
    3d object detection from rgb-d data,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2018, pp. 918–927.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. R. Qi, W. Liu, C. Wu, H. Su 和 L. J. Guibas，“用于 RGB-D 数据的 Frustum PointNets
    进行 3D 目标检测，”在 *IEEE计算机视觉与模式识别会议论文集*，2018年，第918–927页。'
- en: '[21] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and
    S. Savarese, “3d semantic parsing of large-scale indoor spaces,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2016, pp.
    1534–1543.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer 和
    S. Savarese，“大规模室内空间的 3D 语义解析，”在 *IEEE计算机视觉与模式识别会议论文集*，2016年，第1534–1543页。'
- en: '[22] A. Ioannidou, E. Chatzilari, S. Nikolopoulos, and I. Kompatsiaris, “Deep
    learning advances in computer vision with 3d data: A survey,” *ACM Computing Surveys
    (CSUR)*, vol. 50, no. 2, pp. 1–38, 2017.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Ioannidou, E. Chatzilari, S. Nikolopoulos 和 I. Kompatsiaris，“基于 3D
    数据的计算机视觉深度学习进展：综述，” *ACM计算机调查*，第50卷，第2期，第1–38页，2017年。'
- en: '[23] Y. Xie, J. Tian, and X. X. Zhu, “Linking points with labels in 3d: A review
    of point cloud semantic segmentation,” *IEEE Geoscience and Remote Sensing Magazine*,
    vol. 8, no. 4, pp. 38–59, 2020.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Xie, J. Tian 和 X. X. Zhu，“在 3D 中用标签连接点：点云语义分割综述，” *IEEE地球科学与遥感杂志*，第8卷，第4期，第38–59页，2020年。'
- en: '[24] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, 2020.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Jing 和 Y. Tian，“使用深度神经网络的自监督视觉特征学习：综述，” *IEEE模式分析与机器智能汇刊*，2020年。'
- en: '[25] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, “Self-supervised
    learning: Generative or contrastive,” *IEEE Transactions on Knowledge and Data
    Engineering*, 2021.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang 和 J. Tang，“自监督学习：生成性还是对比性，”
    *IEEE知识与数据工程汇刊*，2021年。'
- en: '[26] G.-J. Qi and J. Luo, “Small data challenges in big data era: A survey
    of recent progress on unsupervised and semi-supervised methods,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2020.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] G.-J. Qi 和 J. Luo，“大数据时代的小数据挑战：无监督和半监督方法的最新进展综述，” *IEEE模式分析与机器智能汇刊*，2020年。'
- en: '[27] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
    shapenets: A deep representation for volumetric shapes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 1912–1920.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang 和 J. Xiao，“3D ShapeNets：体积形状的深度表示，”在
    *IEEE计算机视觉与模式识别会议论文集*，2015年，第1912–1920页。'
- en: '[28] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2015, pp. 567–576.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Song, S. P. Lichtenberg 和 J. Xiao，“Sun rgb-d：一个 RGB-D 场景理解基准套件，”在 *IEEE计算机视觉与模式识别会议论文集*，2015年，第567–576页。'
- en: '[29] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019, pp. 1588–1597.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, 和 S.-K. Yeung，“重新审视点云分类：一个新的基准数据集和基于真实世界数据的分类模型，”
    见 *IEEE/CVF 国际计算机视觉会议论文集*，2019，页 1588–1597。'
- en: '[30] J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W. Zhang,
    Z. Li *et al.*, “One million scenes for autonomous driving: Once dataset,” *arXiv
    preprint arXiv:2106.11037*, 2021.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W.
    Zhang, Z. Li *等人*，“百万场景用于自动驾驶：一次数据集，” *arXiv 预印本 arXiv:2106.11037*，2021。'
- en: '[31] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
    synthia dataset: A large collection of synthetic images for semantic segmentation
    of urban scenes,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2016, pp. 3234–3243.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, 和 A. M. Lopez，“Synthia
    数据集：一个用于城市场景语义分割的大型合成图像集合，” 见 *IEEE 计算机视觉与模式识别会议论文集*，2016，页 3234–3243。'
- en: '[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“Bert：用于语言理解的深度双向变换器的预训练，”
    *arXiv 预印本 arXiv:1810.04805*，2018。'
- en: '[33] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE conference on computer
    vision and pattern recognition*.   Ieee, 2009, pp. 248–255.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei，“Imagenet：一个大规模分层图像数据库，”
    见 *2009 IEEE 计算机视觉与模式识别会议*。Ieee，2009，页 248–255。'
- en: '[34] A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard, “Octomap:
    An efficient probabilistic 3d mapping framework based on octrees,” *Autonomous
    robots*, vol. 34, no. 3, pp. 189–206, 2013.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, 和 W. Burgard，“Octomap：基于八叉树的高效概率
    3D 映射框架，” *自主机器人*，第 34 卷，第 3 期，页 189–206，2013。'
- en: '[35] M. Nießner, M. Zollhöfer, S. Izadi, and M. Stamminger, “Real-time 3d reconstruction
    at scale using voxel hashing,” *ACM Transactions on Graphics (ToG)*, vol. 32,
    no. 6, pp. 1–11, 2013.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Nießner, M. Zollhöfer, S. Izadi, 和 M. Stamminger，“基于体素哈希的实时 3D 重建，”
    *ACM 图形学交易*（ToG），第 32 卷，第 6 期，页 1–11，2013。'
- en: '[36] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *International Conference on Learning Representations*,
    2015.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深卷积网络，” 见 *学习表征国际会议*，2015。'
- en: '[37] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，” 见 *IEEE 计算机视觉与模式识别会议论文集*，2016，页
    770–778。'
- en: '[38] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view convolutional
    neural networks for 3d shape recognition,” in *Proceedings of the IEEE international
    conference on computer vision*, 2015, pp. 945–953.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] H. Su, S. Maji, E. Kalogerakis, 和 E. Learned-Miller，“用于 3D 形状识别的多视角卷积神经网络，”
    见 *IEEE 国际计算机视觉会议论文集*，2015，页 945–953。'
- en: '[39] T. Yu, J. Meng, and J. Yuan, “Multi-view harmonized bilinear network for
    3d object recognition,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 186–194.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] T. Yu, J. Meng, 和 J. Yuan，“用于 3D 物体识别的多视角协调双线性网络，” 见 *IEEE 计算机视觉与模式识别会议论文集*，2018，页
    186–194。'
- en: '[40] B. Yang, W. Luo, and R. Urtasun, “Pixor: Real-time 3d object detection
    from point clouds,” in *Proceedings of the IEEE conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 7652–7660.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] B. Yang, W. Luo, 和 R. Urtasun，“Pixor：基于点云的实时 3D 物体检测，” 见 *IEEE 计算机视觉与模式识别会议论文集*，2018，页
    7652–7660。'
- en: '[41] Z. Yang and L. Wang, “Learning relationships for multi-view 3d object
    recognition,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 7505–7514.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Z. Yang 和 L. Wang，“多视角 3D 物体识别的关系学习，” 见 *IEEE/CVF 国际计算机视觉会议论文集*，2019，页
    7505–7514。'
- en: '[42] X. Wei, R. Yu, and J. Sun, “View-gcn: View-based graph convolutional network
    for 3d shape analysis,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 1850–1859.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] X. Wei, R. Yu, 和 J. Sun，“View-gcn：基于视角的图卷积网络用于 3D 形状分析，” 见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020，页
    1850–1859。'
- en: '[43] A. Xiao, X. Yang, S. Lu, D. Guan, and J. Huang, “Fps-net: A convolutional
    fusion network for large-scale lidar point cloud segmentation,” *ISPRS Journal
    of Photogrammetry and Remote Sensing*, vol. 176, pp. 237–249, 2021.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 肖安，杨旭，陆思，管大洪和黄静，“FPS-Net：用于大规模激光雷达点云分割的卷积融合网络”，*ISPRS摄影测量与遥感杂志*，vol .
    176，pp. 237-249，2021年。'
- en: '[44] Q. Huang, W. Wang, and U. Neumann, “Recurrent slice networks for 3d segmentation
    of point clouds,” in *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition*, 2018, pp. 2626–2635.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] 黄强，王伟和U. Neumann，“用于三维点云分割的循环切片网络”，在*IEEE计算机视觉与模式识别会议论文集*中，2018年，第2626-2635页。'
- en: '[45] X. Ye, J. Li, H. Huang, L. Du, and X. Zhang, “3d recurrent neural networks
    with context fusion for point cloud semantic segmentation,” in *Proceedings of
    the European conference on computer vision (ECCV)*, 2018, pp. 403–417.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] X. Ye，J. Li，H. Huang，L. Du和X. Zhang，“具有上下文融合的3D循环神经网络用于点云语义分割”，在*欧洲计算机视觉会议（ECCV）论文集*中，2018年，第403-417页。'
- en: '[46] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem, “3d-prnn: Generating
    shape primitives with recurrent neural networks,” in *Proceedings of the IEEE
    International Conference on Computer Vision*, 2017, pp. 900–909.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] C. Zou，E. Yumer，J. Yang，D. Ceylan和D. Hoiem，“3D-PRNN：用于生成形状基元的循环神经网络”，在*IEEE国际计算机视觉会议论文集*中，2017年，第900-909页。'
- en: '[47] Y. Zhao, T. Birdal, H. Deng, and F. Tombari, “3d point capsule networks,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 1009–1018.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] 赵勇，T. Birdal，H. Deng和F. Tombari，“3D点胶囊网络”，在*IEEE/CVF计算机视觉与模式识别会议论文集*中，2019年，第1009-1018页。'
- en: '[48] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. R. Qi，L. Yi，H. Su和L. J. Guibas，“PointNet ++：度量空间中点集的深层逐级特征学习”，*神经信息处理系统的进展*，vol.
    30，2017年。'
- en: '[49] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
    “Dynamic graph cnn for learning on point clouds,” *Acm Transactions On Graphics
    (tog)*, vol. 38, no. 5, pp. 1–12, 2019.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 王耀，孙洋，刘泽，S. E. Sarma，M. M. Bronstein和J. M. Solomon，“用于点云学习的动态图形CNN”，*图形学ACM事务（TOG）*，vol.
    38，no. 5，pp. 1-12，2019年。'
- en: '[50] B. Graham, M. Engelcke, and L. Van Der Maaten, “3d semantic segmentation
    with submanifold sparse convolutional networks,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 9224–9232.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] B. Graham， M. Engelcke和L. Van Der Maaten，“具有子流形稀疏卷积网络的三维语义分割”，在*IEEE计算机视觉和模式识别会议论文集*中，2018年，第9224-9232页。'
- en: '[51] C. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets: Minkowski
    convolutional neural networks,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2019, pp. 3075–3084.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] C. Choy, J. Gwak和S. Savarese，“4D时空卷积网络：Minkowski卷积神经网络”，在*IEEE/CVF计算机视觉与模式识别会议论文集*中，2019年，第3075-3084页。'
- en: '[52] H. Tang, Z. Liu, X. Li, Y. Lin, and S. Han, “TorchSparse: Efficient Point
    Cloud Inference Engine,” in *Conference on Machine Learning and Systems (MLSys)*,
    2022.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] H. Tang，Z. Liu，X. Li，Y. Lin和S. Han，“TorchSparse：高效点云推理引擎”，在*机器学习和系统（MLSys）会议*上，2022年。'
- en: '[53] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han, “Searching
    efficient 3d architectures with sparse point-voxel convolution,” in *European
    conference on computer vision*.   Springer, 2020, pp. 685–702.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. Tang，Z. Liu，S. Zhao，Y. Lin，J. Lin，H. Wang和S. Han，“搜索高效的三维架构与稀疏的点-体素卷积”，在*欧洲计算机视觉会议*上。Springer，2020年，第685-702页。'
- en: '[54] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany, “Pointcontrast:
    Unsupervised pre-training for 3d point cloud understanding,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 574–591.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Xie，J. Gu，D. Guo，C. R. Qi，L. Guibas和O. Litany，“PointContrast：面向3D点云理解的无监督预训练”，在*欧洲计算机视觉会议*上。Springer，2020年，第574-591页。'
- en: '[55] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] O. Ronneberger, P. Fischer和T. Brox，“U-Net：用于生物医学图像分割的卷积网络”，在*医学图像计算与计算辅助干预国际会议*上。Springer，2015年，第234-241页。'
- en: '[56] Y. Liu, B. Fan, S. Xiang, and C. Pan, “Relation-shape convolutional neural
    network for point cloud analysis,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 8895–8904.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Liu，B. Fan，S. Xiang和C. Pan，“Relation-Shape卷积神经网络用于点云分析”，在*IEEE/CVF计算机视觉与模式识别会议论文集*中，2019年，第8895-8904页。'
- en: '[57] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, “Point-bert: Pre-training
    3d point cloud transformers with masked point modeling,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 19 313–19 322.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou 和 J. Lu，“Point-bert：通过掩蔽点建模预训练3D点云变换器，”
    发表在*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第19 313–19 322页。'
- en: '[58] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    *ICLR*, 2021.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit 和 N. Houlsby，“一张图像值16x16个词：用于大规模图像识别的变换器，”
    *ICLR*，2021年。'
- en: '[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser 和 I. Polosukhin，“注意力即你所需，” 发表在*神经信息处理系统进展*，第30卷，2017年。'
- en: '[60] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
    transformer: Hierarchical vision transformer using shifted windows,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 10 012–10 022.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin 和 B. Guo，“Swin
    transformer：使用移动窗口的分层视觉变换器，” 发表在*IEEE/CVF国际计算机视觉会议论文集*，2021年，第10 012–10 022页。'
- en: '[61] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point transformer,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*,
    October 2021, pp. 16 259–16 268.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] H. Zhao, L. Jiang, J. Jia, P. H. Torr 和 V. Koltun，“Point transformer，”
    发表在*IEEE/CVF国际计算机视觉会议（ICCV）论文集*，2021年10月，第16 259–16 268页。'
- en: '[62] A. Sharma, O. Grau, and M. Fritz, “Vconv-dae: Deep volumetric shape learning
    without object labels,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 236–250.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. Sharma, O. Grau 和 M. Fritz，“Vconv-dae：无对象标签的深度体积形状学习，” 发表在*欧洲计算机视觉会议论文集*，Springer，2016年，第236–250页。'
- en: '[63] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta, “Learning a predictable
    and generative vector representation for objects,” in *European Conference on
    Computer Vision*.   Springer, 2016, pp. 484–499.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] R. Girdhar, D. F. Fouhey, M. Rodriguez 和 A. Gupta，“学习可预测和生成的对象向量表示，” 发表在*欧洲计算机视觉会议论文集*，Springer，2016年，第484–499页。'
- en: '[64] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum, “Learning
    a probabilistic latent space of object shapes via 3d generative-adversarial modeling,”
    in *Proceedings of the 30th International Conference on Neural Information Processing
    Systems*, 2016, pp. 82–90.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Wu, C. Zhang, T. Xue, W. T. Freeman 和 J. B. Tenenbaum，“通过3D生成对抗建模学习对象形状的概率潜在空间，”
    发表在*第30届国际神经信息处理系统会议论文集*，2016年，第82–90页。'
- en: '[65] J. Xie, Z. Zheng, R. Gao, W. Wang, S.-C. Zhu, and Y. N. Wu, “Learning
    descriptor networks for 3d shape synthesis and analysis,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2018, pp. 8629–8638.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] J. Xie, Z. Zheng, R. Gao, W. Wang, S.-C. Zhu 和 Y. N. Wu，“学习描述符网络用于3D形状合成与分析，”
    发表在*IEEE计算机视觉与模式识别会议论文集*，2018年，第8629–8638页。'
- en: '[66] Y. Yang, C. Feng, Y. Shen, and D. Tian, “Foldingnet: Point cloud auto-encoder
    via deep grid deformation,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018, pp. 206–215.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Y. Yang, C. Feng, Y. Shen 和 D. Tian，“Foldingnet：通过深度网格变形的点云自动编码器，” 发表在*IEEE计算机视觉与模式识别会议论文集*，2018年，第206–215页。'
- en: '[67] J. Li, B. M. Chen, and G. H. Lee, “So-net: Self-organizing network for
    point cloud analysis,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 9397–9406.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Li, B. M. Chen 和 G. H. Lee，“So-net：自组织网络用于点云分析，” 发表在*IEEE计算机视觉与模式识别会议论文集*，2018年，第9397–9406页。'
- en: '[68] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learning representations
    and generative models for 3d point clouds,” in *International conference on machine
    learning*.   PMLR, 2018, pp. 40–49.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] P. Achlioptas, O. Diamanti, I. Mitliagkas 和 L. Guibas，“学习3D点云的表示和生成模型，”
    发表在*国际机器学习会议*，PMLR，2018年，第40–49页。'
- en: '[69] M. Gadelha, R. Wang, and S. Maji, “Multiresolution tree networks for 3d
    point cloud processing,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 103–118.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. Gadelha, R. Wang 和 S. Maji，“多分辨率树网络用于3D点云处理，” 发表在*欧洲计算机视觉会议（ECCV）论文集*，2018年，第103–118页。'
- en: '[70] Z. Han, M. Shang, Y.-S. Liu, and M. Zwicker, “View inter-prediction gan:
    Unsupervised representation learning for 3d shapes by learning global shape memories
    to support local view predictions,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 33, no. 01, 2019, pp. 8376–8384.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Z. Han, M. Shang, Y.-S. Liu, 和 M. Zwicker, “视图间预测生成对抗网络：通过学习全球形状记忆支持局部视图预测进行3D形状的无监督表示学习，”
    发表在 *AAAI 人工智能会议论文集*，第33卷，第01期，2019年，页码8376–8384。'
- en: '[71] X. Liu, Z. Han, X. Wen, Y.-S. Liu, and M. Zwicker, “L2g auto-encoder:
    Understanding point clouds by local-to-global reconstruction with hierarchical
    self-attention,” in *Proceedings of the 27th ACM International Conference on Multimedia*,
    2019, pp. 989–997.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] X. Liu, Z. Han, X. Wen, Y.-S. Liu, 和 M. Zwicker, “L2g 自编码器：通过局部到全局重建与层次自注意力理解点云，”
    发表在 *第27届ACM国际多媒体会议论文集*，2019年，页码989–997。'
- en: '[72] Z. Han, X. Wang, Y.-S. Liu, and M. Zwicker, “Multi-angle point cloud-vae:
    Unsupervised feature learning for 3d point clouds from multiple angles by joint
    self-reconstruction and half-to-half prediction,” in *2019 IEEE/CVF International
    Conference on Computer Vision (ICCV)*.   IEEE, 2019, pp. 10 441–10 450.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Z. Han, X. Wang, Y.-S. Liu, 和 M. Zwicker, “多角度点云变分自编码器：通过联合自重建和半对半预测进行多角度3D点云的无监督特征学习，”
    发表在 *2019 IEEE/CVF 国际计算机视觉会议（ICCV）*。IEEE，2019年，页码10 441–10 450。'
- en: '[73] G. Yang, X. Huang, Z. Hao, M.-Y. Liu, S. Belongie, and B. Hariharan, “Pointflow:
    3d point cloud generation with continuous normalizing flows,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2019, pp. 4541–4550.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] G. Yang, X. Huang, Z. Hao, M.-Y. Liu, S. Belongie, 和 B. Hariharan, “Pointflow:
    使用连续归一化流生成 3D 点云，” 发表在 *IEEE/CVF 国际计算机视觉会议论文集*，2019年，页码4541–4550。'
- en: '[74] Y. Shi, M. Xu, S. Yuan, and Y. Fang, “Unsupervised deep shape descriptor
    with point distribution learning,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 9353–9362.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y. Shi, M. Xu, S. Yuan, 和 Y. Fang, “无监督深度形状描述符与点分布学习，” 发表在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，页码9353–9362。'
- en: '[75] X. Gao, W. Hu, and G.-J. Qi, “Graphter: Unsupervised learning of graph
    transformation equivariant representations via auto-encoding node-wise transformations,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 7163–7172.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] X. Gao, W. Hu, 和 G.-J. Qi, “Graphter: 通过自编码节点级变换无监督学习图转换等变表示，” 发表在 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2020年，页码7163–7172。'
- en: '[76] X. Wen, T. Li, Z. Han, and Y.-S. Liu, “Point cloud completion by skip-attention
    network with hierarchical folding,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 1939–1948.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] X. Wen, T. Li, Z. Han, 和 Y.-S. Liu, “通过跳跃注意力网络和层次折叠的点云完成，” 发表在 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2020年，页码1939–1948。'
- en: '[77] Y. Sun, Y. Wang, Z. Liu, J. Siegel, and S. Sarma, “Pointgrow: Autoregressively
    learned point cloud generation with self-attention,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2020, pp. 61–70.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. Sun, Y. Wang, Z. Liu, J. Siegel, 和 S. Sarma, “Pointgrow: 自回归学习的点云生成与自注意力，”
    发表在 *IEEE/CVF 冬季计算机视觉应用会议论文集*，2020年，页码61–70。'
- en: '[78] J. Yang, P. Ahn, D. Kim, H. Lee, and J. Kim, “Progressive seed generation
    auto-encoder for unsupervised point cloud learning,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 6413–6422.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. Yang, P. Ahn, D. Kim, H. Lee, 和 J. Kim, “渐进种子生成自编码器用于无监督点云学习，” 发表在
    *IEEE/CVF 国际计算机视觉会议论文集*，2021年，页码6413–6422。'
- en: '[79] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, “Masked autoencoders
    for point cloud self-supervised learning,” in *Computer Vision–ECCV 2022: 17th
    European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
    II*.   Springer, 2022, pp. 604–621.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, 和 L. Yuan, “点云自监督学习的掩码自编码器，”
    发表在 *计算机视觉–ECCV 2022: 第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第II部分*。Springer，2022年，页码604–621。'
- en: '[80] R. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, and H. Li,
    “Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training,”
    *Advances in neural information processing systems*, 2022.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] R. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, 和 H. Li,
    “Point-m2ae: 用于层次点云预训练的多尺度掩码自编码器，” *神经信息处理系统进展*，2022年。'
- en: '[81] M. A. Kramer, “Nonlinear principal component analysis using autoassociative
    neural networks,” *AIChE journal*, vol. 37, no. 2, pp. 233–243, 1991.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. A. Kramer, “使用自联想神经网络的非线性主成分分析，” *AIChE 期刊*，第37卷，第2期，页码233–243，1991年。'
- en: '[82] Autoencoder, “Autoencoder — Wikipedia, the free encyclopedia,” 2022, [Online;
    accessed 16-Feb-2022]. [Online]. Available: [https://en.wikipedia.org/wiki/Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] 自编码器，“自编码器 — 维基百科，自由百科全书，” 2022年，[在线；访问日期2022年2月16日]。[在线]。可用：[https://en.wikipedia.org/wiki/Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)'
- en: '[83] H. Fan, H. Su, and L. J. Guibas, “A point set generation network for 3d
    object reconstruction from a single image,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 605–613.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] H. Fan, H. Su, 和 L. J. Guibas，“用于从单张图像重建3D物体的点集生成网络，” 见于 *IEEE 计算机视觉与模式识别会议论文集*，2017年，页码605–613。'
- en: '[84] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,”
    *Advances in neural information processing systems*, vol. 30, 2017.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S. Sabour, N. Frosst, 和 G. E. Hinton，“胶囊之间的动态路由，” *神经信息处理系统进展*，第30卷，2017年。'
- en: '[85] S. Chen, C. Duan, Y. Yang, D. Li, C. Feng, and D. Tian, “Deep Unsupervised
    Learning of 3D Point Clouds via Graph Topology Inference and Filtering,” *IEEE
    Transactions on Image Processing*, vol. 29, pp. 3183–3198, 2020.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Chen, C. Duan, Y. Yang, D. Li, C. Feng, 和 D. Tian，“通过图拓扑推断和过滤进行深度无监督3D点云学习，”
    *IEEE 图像处理学报*，第29卷，页码3183–3198，2020年。'
- en: '[86] H. Chen, S. Luo, X. Gao, and W. Hu, “Unsupervised learning of geometric
    sampling invariant representations for 3d point clouds,” in *2021 IEEE/CVF International
    Conference on Computer Vision Workshops (ICCVW)*.   IEEE Computer Society, 2021,
    pp. 893–903.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] H. Chen, S. Luo, X. Gao, 和 W. Hu，“无监督学习几何采样不变表示用于3D点云，” 见于 *2021 IEEE/CVF
    国际计算机视觉会议研讨会 (ICCVW)*。 IEEE计算机学会，2021年，页码893–903。'
- en: '[87] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *Advances in neural
    information processing systems*, vol. 27, 2014.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络，” *神经信息处理系统进展*，第27卷，2014年。'
- en: '[88] C.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, and R. Salakhutdinov, “Point
    cloud gan,” *arXiv preprint arXiv:1810.05795*, 2018.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, 和 R. Salakhutdinov，“点云生成对抗网络，”
    *arXiv 预印本 arXiv:1810.05795*，2018年。'
- en: '[89] R. Li, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, “Pu-gan: a point
    cloud upsampling adversarial network,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 7203–7212.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] R. Li, X. Li, C.-W. Fu, D. Cohen-Or, 和 P.-A. Heng，“Pu-gan: 点云上采样对抗网络，”
    见于 *IEEE/CVF 国际计算机视觉会议论文集*，2019年，页码7203–7212。'
- en: '[90] E. Remelli, P. Baque, and P. Fua, “Neuralsampler: Euclidean point cloud
    auto-encoder and sampler,” *arXiv preprint arXiv:1901.09394*, 2019.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] E. Remelli, P. Baque, 和 P. Fua，“Neuralsampler: 欧几里得点云自编码器和采样器，” *arXiv
    预印本 arXiv:1901.09394*，2019年。'
- en: '[91] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, “Pu-net: Point cloud
    upsampling network,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 2790–2799.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, 和 P.-A. Heng，“Pu-net: 点云上采样网络，” 见于
    *IEEE 计算机视觉与模式识别会议论文集*，2018年，页码2790–2799。'
- en: '[92] W. Yifan, S. Wu, H. Huang, D. Cohen-Or, and O. Sorkine-Hornung, “Patch-based
    progressive 3d point set upsampling,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 5958–5967.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] W. Yifan, S. Wu, H. Huang, D. Cohen-Or, 和 O. Sorkine-Hornung，“基于补丁的渐进式3D点集上采样，”
    见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2019年，页码5958–5967。'
- en: '[93] Y. Qian, J. Hou, S. Kwong, and Y. He, “Pugeo-net: A geometry-centric network
    for 3d point cloud upsampling,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 752–769.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Y. Qian, J. Hou, S. Kwong, 和 Y. He，“Pugeo-net: 一种以几何为中心的3D点云上采样网络，” 见于
    *欧洲计算机视觉会议*。 Springer，2020年，页码752–769。'
- en: '[94] G. Qian, A. Abualshour, G. Li, A. Thabet, and B. Ghanem, “Pu-gcn: Point
    cloud upsampling using graph convolutional networks,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 11 683–11 692.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] G. Qian, A. Abualshour, G. Li, A. Thabet, 和 B. Ghanem，“Pu-gcn: 使用图卷积网络进行点云上采样，”
    见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021年，页码11 683–11 692。'
- en: '[95] R. Li, X. Li, P.-A. Heng, and C.-W. Fu, “Point cloud upsampling via disentangled
    refinement,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 344–353.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] R. Li, X. Li, P.-A. Heng, 和 C.-W. Fu，“通过解耦细化进行点云上采样，” 见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021年，页码344–353。'
- en: '[96] K. Hassani and M. Haley, “Unsupervised multi-task feature learning on
    point clouds,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 8160–8171.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] K. Hassani 和 M. Haley， “点云上的无监督多任务特征学习，” 见 *IEEE/CVF 国际计算机视觉会议论文集*，2019年，页码
    8160–8171。'
- en: '[97] L. Zhang and Z. Zhu, “Unsupervised feature learning for point cloud understanding
    by contrasting and clustering using graph convolutional neural networks,” in *2019
    International Conference on 3D Vision (3DV)*.   IEEE, 2019, pp. 395–404.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] L. Zhang 和 Z. Zhu， “通过对比和聚类使用图卷积神经网络进行点云理解的无监督特征学习，” 见 *2019 国际 3D 视觉会议
    (3DV)*。   IEEE，2019年，页码 395–404。'
- en: '[98] Y. Rao, J. Lu, and J. Zhou, “Global-local bidirectional reasoning for
    unsupervised representation learning of 3d point clouds,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp. 5376–5385.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Rao, J. Lu, 和 J. Zhou， “3D 点云的无监督表示学习的全局-局部双向推理，” 见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，页码
    5376–5385。'
- en: '[99] A. Sanghi, “Info3d: Representation learning on 3d objects using mutual
    information maximization and contrastive learning,” in *European Conference on
    Computer Vision*.   Springer, 2020, pp. 626–642.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Sanghi， “Info3d：使用互信息最大化和对比学习进行 3D 对象表示学习，” 见 *欧洲计算机视觉会议*。   Springer，2020年，页码
    626–642。'
- en: '[100] M. Gadelha, A. RoyChowdhury, G. Sharma, E. Kalogerakis, L. Cao, E. Learned-Miller,
    R. Wang, and S. Maji, “Label-efficient learning on point clouds using approximate
    convex decompositions,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 473–491.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] M. Gadelha, A. RoyChowdhury, G. Sharma, E. Kalogerakis, L. Cao, E. Learned-Miller,
    R. Wang, 和 S. Maji， “使用近似凸分解进行点云的标签高效学习，” 见 *欧洲计算机视觉会议*。   Springer，2020年，页码 473–491。'
- en: '[101] O. Poursaeed, T. Jiang, H. Qiao, N. Xu, and V. G. Kim, “Self-supervised
    learning of point clouds via orientation estimation,” in *2020 International Conference
    on 3D Vision (3DV)*.   IEEE, 2020, pp. 1018–1028.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] O. Poursaeed, T. Jiang, H. Qiao, N. Xu, 和 V. G. Kim， “通过方向估计进行点云的自监督学习，”
    见 *2020 国际 3D 视觉会议 (3DV)*。   IEEE，2020年，页码 1018–1028。'
- en: '[102] B. Du, X. Gao, W. Hu, and X. Li, “Self-contrastive learning with hard
    negative sampling for self-supervised point cloud learning,” in *Proceedings of
    the 29th ACM International Conference on Multimedia*, 2021, pp. 3133–3142.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] B. Du, X. Gao, W. Hu, 和 X. Li， “用于自监督点云学习的自对比学习与困难负样本采样，” 见 *第 29 届 ACM
    国际多媒体会议论文集*，2021年，页码 3133–3142。'
- en: '[103] Y. Rao, B. Liu, Y. Wei, J. Lu, C.-J. Hsieh, and J. Zhou, “Randomrooms:
    Unsupervised pre-training from synthetic shapes and randomized layouts for 3d
    object detection,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2021, pp. 3283–3292.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Rao, B. Liu, Y. Wei, J. Lu, C.-J. Hsieh, 和 J. Zhou， “Randomrooms：从合成形状和随机布局中进行无监督预训练，用于
    3D 对象检测，” 见 *IEEE/CVF 国际计算机视觉会议论文集*，2021年，页码 3283–3292。'
- en: '[104] Z. Zhang, R. Girdhar, A. Joulin, and I. Misra, “Self-supervised pretraining
    of 3d features on any point-cloud,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, October 2021, pp. 10 252–10 263.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Z. Zhang, R. Girdhar, A. Joulin, 和 I. Misra， “在任何点云上自监督预训练 3D 特征，” 见
    *IEEE/CVF 国际计算机视觉会议论文集 (ICCV)*，2021年10月，页码 10 252–10 263。'
- en: '[105] Y. Chen, J. Liu, B. Ni, H. Wang, J. Yang, N. Liu, T. Li, and Q. Tian,
    “Shape self-correction for unsupervised point cloud understanding,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 8382–8391.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Chen, J. Liu, B. Ni, H. Wang, J. Yang, N. Liu, T. Li, 和 Q. Tian， “用于无监督点云理解的形状自我修正，”
    见 *IEEE/CVF 国际计算机视觉会议论文集*，2021年，页码 8382–8391。'
- en: '[106] R. Yamada, H. Kataoka, N. Chiba, Y. Domae, and T. Ogata, “Point cloud
    pre-training with natural 3d structures,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp. 21 283–21 293.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] R. Yamada, H. Kataoka, N. Chiba, Y. Domae, 和 T. Ogata， “带有自然 3D 结构的点云预训练，”
    见 *IEEE/CVF 计算机视觉与模式识别会议论文集 (CVPR)*，2022年6月，页码 21 283–21 293。'
- en: '[107] Y. Chen, M. Nießner, and A. Dai, “4dcontrast: Contrastive learning with
    dynamic correspondences for 3d scene understanding,” in *Computer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXXII*.   Springer, 2022, pp. 543–560.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Y. Chen, M. Nießner, 和 A. Dai， “4dcontrast：用于 3d 场景理解的动态对应的对比学习，” 见 *计算机视觉–ECCV
    2022：第十七届欧洲会议，特拉维夫，以色列，2022年10月23–27日，会议论文集，第 XXXII 部分*。   Springer，2022年，页码 543–560。'
- en: '[108] L. Li and M. Heizmann, “A closer look at invariances in self-supervised
    pre-training for 3d vision,” in *Computer Vision–ECCV 2022: 17th European Conference,
    Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXX*.   Springer, 2022,
    pp. 656–673.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] L. Li 和 M. Heizmann，"深入分析3D视觉自监督预训练中的不变性"，发表于 *计算机视觉–ECCV 2022：第17届欧洲会议，以色列特拉维夫，2022年10月23–27日，论文集，第XXX部分*。   Springer，2022，第656–673页。'
- en: '[109] J. Yin, D. Zhou, L. Zhang, J. Fang, C.-Z. Xu, J. Shen, and W. Wang, “Proposalcontrast:
    Unsupervised pre-training for lidar-based 3d object detection,” in *Computer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXXIX*.   Springer, 2022, pp. 17–33.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] J. Yin, D. Zhou, L. Zhang, J. Fang, C.-Z. Xu, J. Shen, 和 W. Wang，"Proposalcontrast：基于激光雷达的3D目标检测的无监督预训练"，发表于
    *计算机视觉–ECCV 2022：第17届欧洲会议，以色列特拉维夫，2022年10月23–27日，论文集，第XXXIX部分*。   Springer，2022，第17–33页。'
- en: '[110] H. Liu, M. Cai, and Y. J. Lee, “Masked discrimination for self-supervised
    learning on point clouds,” in *Computer Vision–ECCV 2022: 17th European Conference,
    Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part II*.   Springer, 2022,
    pp. 657–675.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] H. Liu, M. Cai, 和 Y. J. Lee，"点云的自监督学习中的掩码判别"，发表于 *计算机视觉–ECCV 2022：第17届欧洲会议，以色列特拉维夫，2022年10月23–27日，论文集，第II部分*。   Springer，2022，第657–675页。'
- en: '[111] K. Liu, A. Xiao, X. Zhang, S. Lu, and L. Shao, “Fac: 3d representation
    learning via foreground aware feature contrast,” in *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2023.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] K. Liu, A. Xiao, X. Zhang, S. Lu, 和 L. Shao，"Fac：通过前景感知特征对比进行3D表示学习"，发表于
    *IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2023。'
- en: '[112] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, “A papier-mâché
    approach to learning 3d surface generation,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 216–224.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, 和 M. Aubry，"一种纸浆法用于学习3D表面生成"，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2018，第216–224页。'
- en: '[113] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, “Pcn: Point completion
    network,” in *2018 International Conference on 3D Vision (3DV)*.   IEEE, 2018,
    pp. 728–737.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] W. Yuan, T. Khot, D. Held, C. Mertz, 和 M. Hebert，"Pcn：点云完成网络"，发表于 *2018年国际3D视觉会议（3DV）*。   IEEE，2018，第728–737页。'
- en: '[114] Z. Huang, Y. Yu, J. Xu, F. Ni, and X. Le, “Pf-net: Point fractal network
    for 3d point cloud completion,” in *2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2020, pp. 7659–7667.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Z. Huang, Y. Yu, J. Xu, F. Ni, 和 X. Le，"Pf-net：用于3D点云完成的点分形网络"，发表于 *2020
    IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2020，第7659–7667页。'
- en: '[115] M. Liu, L. Sheng, S. Yang, J. Shao, and S.-M. Hu, “Morphing and sampling
    network for dense point cloud completion,” in *Proceedings of the AAAI conference
    on artificial intelligence*, vol. 34, no. 07, 2020, pp. 11 596–11 603.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] M. Liu, L. Sheng, S. Yang, J. Shao, 和 S.-M. Hu，"用于密集点云完成的形态学和采样网络"，发表于
    *AAAI人工智能会议论文集*，第34卷，第07期，2020，第11,596–11,603页。'
- en: '[116] W. Zhang, Q. Yan, and C. Xiao, “Detail preserved point cloud completion
    via separated feature aggregation,” in *Computer Vision–ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16*.   Springer,
    2020, pp. 512–528.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] W. Zhang, Q. Yan, 和 C. Xiao，"通过分离特征聚合保持细节的点云完成"，发表于 *计算机视觉–ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XXV部分
    16*。   Springer，2020，第512–528页。'
- en: '[117] C. Xie, C. Wang, B. Zhang, H. Yang, D. Chen, and F. Wen, “Style-based
    point generator with adversarial rendering for point cloud completion,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 4619–4628.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] C. Xie, C. Wang, B. Zhang, H. Yang, D. Chen, 和 F. Wen，"基于风格的点生成器与对抗渲染用于点云完成"，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2021，第4619–4628页。'
- en: '[118] K. Fu, P. Gao, S. Liu, R. Zhang, Y. Qiao, and M. Wang, “Pos-bert: Point
    cloud one-stage bert pre-training,” *arXiv preprint arXiv:2204.00989*, 2022.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] K. Fu, P. Gao, S. Liu, R. Zhang, Y. Qiao, 和 M. Wang，"Pos-bert：点云一阶段BERT预训练"，*arXiv预印本
    arXiv:2204.00989*，2022。'
- en: '[119] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
    for contrastive learning of visual representations,” in *International conference
    on machine learning*.   PMLR, 2020, pp. 1597–1607.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] T. Chen, S. Kornblith, M. Norouzi, 和 G. Hinton，"一种简单的视觉表示对比学习框架"，发表于
    *国际机器学习会议*。   PMLR，2020，第1597–1607页。'
- en: '[120] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive
    predictive coding,” *arXiv preprint arXiv:1807.03748*, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. v. d. Oord, Y. Li, 和 O. Vinyals，"通过对比预测编码进行表示学习"，*arXiv预印本 arXiv:1807.03748*，2018。'
- en: '[121] P.-S. Wang, Y.-Q. Yang, Q.-F. Zou, Z. Wu, Y. Liu, and X. Tong, “Unsupervised
    3d learning for shape analysis via multiresolution instance discrimination,” *ACM
    Trans. Graphic*, 2020.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] P.-S. Wang, Y.-Q. Yang, Q.-F. Zou, Z. Wu, Y. Liu, 和 X. Tong，“通过多分辨率实例区分进行无监督
    3d 学习以分析形状，” *ACM 图形学交易*，2020年。'
- en: '[122] J. Jiang, X. Lu, W. Ouyang, and M. Wang, “Unsupervised representation
    learning for 3d point cloud data,” *arXiv preprint arXiv:2110.06632*, 2021.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] J. Jiang, X. Lu, W. Ouyang, 和 M. Wang，“用于 3d 点云数据的无监督表示学习，” *arXiv 预印本
    arXiv:2110.06632*，2021年。'
- en: '[123] J. Hou, S. Xie, B. Graham, A. Dai, and M. Nießner, “Pri3d: Can 3d priors
    help 2d representation learning?” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, October 2021, pp. 5693–5702.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] J. Hou, S. Xie, B. Graham, A. Dai, 和 M. Nießner，“Pri3d: 3d 先验是否有助于 2d
    表示学习？”在 *IEEE/CVF 国际计算机视觉大会 (ICCV)*，2021年10月，pp. 5693–5702。'
- en: '[124] J. A. Hartigan and M. A. Wong, “Algorithm as 136: A k-means clustering
    algorithm,” *Journal of the royal statistical society. series c (applied statistics)*,
    vol. 28, no. 1, pp. 100–108, 1979.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. A. Hartigan 和 M. A. Wong，“算法 136: 一个 k-means 聚类算法，” *皇家统计学会杂志，C 系列（应用统计学）*，第28卷，第1期，pp.
    100–108，1979年。'
- en: '[125] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
    unsupervised learning of visual features,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 132–149.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] M. Caron, P. Bojanowski, A. Joulin, 和 M. Douze，“深度聚类用于无监督视觉特征学习，”在 *欧洲计算机视觉会议（ECCV）论文集*，2018年，pp.
    132–149。'
- en: '[126] J. Sauder and B. Sievers, “Context prediction for unsupervised deep learning
    on point clouds,” *arXiv preprint arXiv:1901.08396*, vol. 2, no. 4, p. 5, 2019.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Sauder 和 B. Sievers，“点云无监督深度学习的上下文预测，” *arXiv 预印本 arXiv:1901.08396*，第2卷，第4期，第5页，2019年。'
- en: '[127] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation
    learning by predicting image rotations,” in *International Conference on Learning
    Representations*, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] S. Gidaris, P. Singh, 和 N. Komodakis，“通过预测图像旋转进行无监督表示学习，”在 *国际学习表示会议*，2018年。'
- en: '[128] A. Thabet, H. Alwassel, and B. Ghanem, “Self-supervised learning of local
    features in 3d point clouds,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops*, 2020, pp. 938–939.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Thabet, H. Alwassel, 和 B. Ghanem，“3d 点云中局部特征的自监督学习，”在 *IEEE/CVF 计算机视觉与模式识别大会研讨会*，2020年，pp.
    938–939。'
- en: '[129] C. Sun, Z. Zheng, X. Wang, M. Xu, and Y. Yang, “Point cloud pre-training
    by mixing and disentangling,” *arXiv preprint arXiv:2109.00452*, 2021.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] C. Sun, Z. Zheng, X. Wang, M. Xu, 和 Y. Yang，“通过混合和解缠结进行点云预训练，” *arXiv
    预印本 arXiv:2109.00452*，2021年。'
- en: '[130] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    and J. Gall, “Semantickitti: A dataset for semantic scene understanding of lidar
    sequences,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 9297–9307.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    和 J. Gall，“Semantickitti: 用于 lidar 序列的语义场景理解数据集，”在 *IEEE/CVF 国际计算机视觉大会论文集*，2019年，pp.
    9297–9307。'
- en: '[131] A. Xiao, J. Huang, D. Guan, F. Zhan, and S. Lu, “Transfer learning from
    synthetic to real lidar point cloud for semantic segmentation,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 36, no. 3, 2022, pp.
    2795–2803.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] A. Xiao, J. Huang, D. Guan, F. Zhan, 和 S. Lu，“从合成到真实 lidar 点云的迁移学习，用于语义分割，”在
    *AAAI 人工智能会议论文集*，第36卷，第3期，2022年，pp. 2795–2803。'
- en: '[132] C. Feichtenhofer, H. Fan, B. Xiong, R. Girshick, and K. He, “A large-scale
    study on unsupervised spatiotemporal representation learning,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 3299–3309.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] C. Feichtenhofer, H. Fan, B. Xiong, R. Girshick, 和 K. He，“大规模无监督时空表示学习研究，”在
    *IEEE/CVF 计算机视觉与模式识别大会*，2021年，pp. 3299–3309。'
- en: '[133] X. Song, S. Zhao, J. Yang, H. Yue, P. Xu, R. Hu, and H. Chai, “Spatio-temporal
    contrastive domain adaptation for action recognition,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 9787–9795.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] X. Song, S. Zhao, J. Yang, H. Yue, P. Xu, R. Hu, 和 H. Chai，“用于动作识别的时空对比领域适应，”在
    *IEEE/CVF 计算机视觉与模式识别大会论文集*，2021年，pp. 9787–9795。'
- en: '[134] K. Hu, J. Shao, Y. Liu, B. Raj, M. Savvides, and Z. Shen, “Contrast and
    order representations for video self-supervised learning,” in *Proceedings of
    the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 7939–7949.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] K. Hu, J. Shao, Y. Liu, B. Raj, M. Savvides, 和 Z. Shen，“视频自监督学习的对比和顺序表示，”在
    *IEEE/CVF 国际计算机视觉大会论文集*，2021年，pp. 7939–7949。'
- en: '[135] H. Kuang, Y. Zhu, Z. Zhang, X. Li, J. Tighe, S. Schwertfeger, C. Stachniss,
    and M. Li, “Video contrastive learning with global context,” in *Proceedings of
    the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 3195–3204.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] H. Kuang、Y. Zhu、Z. Zhang、X. Li、J. Tighe、S. Schwertfeger、C. Stachniss
    和 M. Li，“视频对比学习与全球上下文”，见于 *IEEE/CVF 国际计算机视觉会议论文集*，2021 年，第 3195–3204 页。'
- en: '[136] D. Z. Chen, A. X. Chang, and M. Nießner, “Scanrefer: 3d object localization
    in rgb-d scans using natural language,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 202–221.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] D. Z. Chen、A. X. Chang 和 M. Nießner，“Scanrefer：使用自然语言在 rgb-d 扫描中进行三维物体定位”，见于
    *欧洲计算机视觉会议*。Springer，2020 年，第 202–221 页。'
- en: '[137] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna,
    and R. Rodrigo, “Crosspoint: Self-supervised cross-modal contrastive learning
    for 3d point cloud understanding,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 9902–9912.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] M. Afham、I. Dissanayake、D. Dissanayake、A. Dharmasiri、K. Thilakarathna
    和 R. Rodrigo，“Crosspoint：用于三维点云理解的自监督跨模态对比学习”，见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022
    年，第 9902–9912 页。'
- en: '[138] Z. Wang, X. Yu, Y. Rao, J. Zhou, and J. Lu, “P2p: Tuning pre-trained
    image models for point cloud analysis with point-to-pixel prompting,” *Advances
    in neural information processing systems*, 2022.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Z. Wang、X. Yu、Y. Rao、J. Zhou 和 J. Lu，“P2p：通过点对像素提示调整预训练图像模型以进行点云分析”，见于
    *神经信息处理系统进展*，2022 年。'
- en: '[139] H. Deng, T. Birdal, and S. Ilic, “Ppf-foldnet: Unsupervised learning
    of rotation invariant 3d local descriptors,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 602–618.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] H. Deng、T. Birdal 和 S. Ilic，“Ppf-foldnet：旋转不变三维局部描述符的无监督学习”，见于 *欧洲计算机视觉会议论文集（ECCV）*，2018
    年，第 602–618 页。'
- en: '[140] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser,
    “3dmatch: Learning local geometric descriptors from rgb-d reconstructions,” in
    *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2017, pp. 1802–1811.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] A. Zeng、S. Song、M. Nießner、M. Fisher、J. Xiao 和 T. Funkhouser，“3dmatch：从
    rgb-d 重建中学习局部几何描述符”，见于 *IEEE 计算机视觉与模式识别会议论文集*，2017 年，第 1802–1811 页。'
- en: '[141] Y. Zeng, Y. Qian, Z. Zhu, J. Hou, H. Yuan, and Y. He, “Corrnet3d: unsupervised
    end-to-end learning of dense correspondence for 3d point clouds,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 6052–6061.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. Zeng、Y. Qian、Z. Zhu、J. Hou、H. Yuan 和 Y. He，“Corrnet3d：三维点云密集对应关系的无监督端到端学习”，见于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021 年，第 6052–6061 页。'
- en: '[142] I. Lang, D. Ginzburg, S. Avidan, and D. Raviv, “Dpc: Unsupervised deep
    point correspondence via cross and self construction,” in *2021 International
    Conference on 3D Vision (3DV)*.   IEEE, 2021, pp. 1442–1451.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] I. Lang、D. Ginzburg、S. Avidan 和 D. Raviv，“Dpc：通过交叉和自我构建的无监督深度点匹配”，见于
    *2021 年国际三维视觉会议（3DV）*。IEEE，2021 年，第 1442–1451 页。'
- en: '[143] H. Jiang, Y. Shen, J. Xie, J. Li, J. Qian, and J. Yang, “Sampling network
    guided cross-entropy method for unsupervised point cloud registration,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 6128–6137.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] H. Jiang、Y. Shen、J. Xie、J. Li、J. Qian 和 J. Yang，“用于无监督点云配准的采样网络引导交叉熵方法”，见于
    *IEEE/CVF 国际计算机视觉会议论文集*，2021 年，第 6128–6137 页。'
- en: '[144] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point transformer,”
    in *Proceedings of the IEEE/CVF international conference on computer vision*,
    2021, pp. 16 259–16 268.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] H. Zhao、L. Jiang、J. Jia、P. H. Torr 和 V. Koltun，“点变换器”，见于 *IEEE/CVF 国际计算机视觉会议论文集*，2021
    年，第 16 259–16 268 页。'
- en: '[145] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz, “Rotation invariant spherical
    harmonic representation of 3 d shape descriptors,” in *Symposium on geometry processing*,
    vol. 6, 2003, pp. 156–164.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. Kazhdan、T. Funkhouser 和 S. Rusinkiewicz，“旋转不变的球面谐波表示三维形状描述符”，见于 *几何处理研讨会*，第
    6 卷，2003 年，第 156–164 页。'
- en: '[146] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung, “On visual similarity
    based 3d model retrieval,” in *Computer graphics forum*, vol. 22, no. 3.   Wiley
    Online Library, 2003, pp. 223–232.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] D.-Y. Chen、X.-P. Tian、Y.-T. Shen 和 M. Ouhyoung，“基于视觉相似性的三维模型检索”，见于 *计算机图形论坛*，第
    22 卷，第 3 期。Wiley 在线图书馆，2003 年，第 223–232 页。'
- en: '[147] B. Eckart, W. Yuan, C. Liu, and J. Kautz, “Self-supervised learning on
    3d point clouds by learning discrete generative models,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 8248–8257.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] B. Eckart、W. Yuan、C. Liu 和 J. Kautz，“通过学习离散生成模型对三维点云进行自监督学习”，见于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2021 年，第 8248–8257 页。'
- en: '[148] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” *Journal
    of machine learning research*, vol. 9, no. 11, 2008.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] L. Van der Maaten 和 G. Hinton，"使用 t-sne 进行数据可视化。" *机器学习研究期刊*，第 9 卷，第
    11 期，2008 年。'
- en: '[149] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional detection,”
    *Sensors*, vol. 18, no. 10, p. 3337, 2018.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Yan, Y. Mao, 和 B. Li，"Second: 稀疏嵌入卷积检测"，*传感器*，第 18 卷，第 10 期，页码 3337，2018
    年。'
- en: '[150] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,
    “Unsupervised learning of visual features by contrasting cluster assignments,”
    *Advances in Neural Information Processing Systems*, vol. 33, pp. 9912–9924, 2020.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, 和 A. Joulin，"通过对比聚类分配进行无监督学习的视觉特征"，*神经信息处理系统进展*，第
    33 卷，页码 9912–9924，2020 年。'
- en: '| ![[Uncaptioned image]](img/dc667d3bd71eb0fcf6152d59da45d9fd.png) | Aoran
    Xiao received his B.Sc. and M.Sc. degree from Wuhan University, China in 2016
    and 2019, respectively. He is currently pursuing the Ph.D. degree with the school
    of computer science and engineering at Nanyang Technological University, Singapore.
    His research interests lie in point cloud processing, computer vision, and remote
    sensing. |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/dc667d3bd71eb0fcf6152d59da45d9fd.png) | Aoran Xiao 于 2016
    年和 2019 年分别从中国武汉大学获得了学士和硕士学位。他目前在南洋理工大学计算机科学与工程学院攻读博士学位。他的研究兴趣包括点云处理、计算机视觉和遥感。
    |'
- en: '| ![[Uncaptioned image]](img/519ed1d04f38970f4ec322b419d2464b.png) | Jiaxing
    Huang received his B.Eng. and M.Sc. in EEE from the University of Glasgow, UK,
    and the Nanyang Technological University (NTU), Singapore, respectively. He is
    currently a Research Associate and Ph.D. student with School of Computer Science
    and Engineering, NTU, Singapore. His research interests include computer vision
    and machine learning. |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/519ed1d04f38970f4ec322b419d2464b.png) | Jiaxing Huang 在英国格拉斯哥大学和新加坡南洋理工大学分别获得了电子与电气工程学士和硕士学位。他目前是南洋理工大学计算机科学与工程学院的研究助理和博士生。他的研究兴趣包括计算机视觉和机器学习。
    |'
- en: '| ![[Uncaptioned image]](img/32a1f6341548c7dd617e1f0326bdae09.png) | Dayan
    Guan is currently a Research Scientist at Mohamed bin Zayed University of Artificial
    Intelligence, United Arab Emirates. Before that, he had been a Research Fellow
    at Nanyang Technological University from Nov 2019 to Mar 2022\. In Sep 2019, he
    received his Ph.D. from Zhejiang University, China. His research interests include
    computer vision, pattern recognition and deep learning. |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/32a1f6341548c7dd617e1f0326bdae09.png) | Dayan Guan 目前是阿联酋穆罕默德·本·扎耶德人工智能大学的研究科学家。在此之前，他曾在南洋理工大学担任研究员，时间为
    2019 年 11 月至 2022 年 3 月。2019 年 9 月，他获得了中国浙江大学的博士学位。他的研究兴趣包括计算机视觉、模式识别和深度学习。 |'
- en: '| ![[Uncaptioned image]](img/08bee97017240cb66de44a95b2d61c41.png) | Xiaoqin
    Zhang is a senior member of the IEEE. He received the B.Sc. degree in electronic
    information science and technology from Central South University, China, in 2005,
    and the Ph.D. degree in pattern recognition and intelligent system from the National
    Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of
    Sciences, China, in 2010\. He is currently a Professor with Wenzhou University,
    China. He has published more than 100 papers in international and national journals,
    and international conferences, including IEEE T-PAMI, IJCV, IEEE T-IP, IEEE T-NNLS,
    IEEE T-C, ICCV, CVPR, NIPS, IJCAI, AAAI, and among others. His research interests
    include in pattern recognition, computer vision, and machine learning. |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/08bee97017240cb66de44a95b2d61c41.png) | Xiaoqin Zhang 是 IEEE
    的高级会员。他于 2005 年从中国中南大学获得电子信息科学与技术学士学位，并于 2010 年从中国科学院自动化研究所模式识别国家重点实验室获得模式识别与智能系统博士学位。他目前是中国温州大学的教授。他在国际和国内期刊以及国际会议上发表了
    100 多篇论文，包括 IEEE T-PAMI、IJCV、IEEE T-IP、IEEE T-NNLS、IEEE T-C、ICCV、CVPR、NIPS、IJCAI、AAAI
    等。他的研究兴趣包括模式识别、计算机视觉和机器学习。 |'
- en: '| ![[Uncaptioned image]](img/c02bcf06ed26db965b2457ebcd13290d.png) | Shijian
    Lu is an Associate Professor with the School of Computer Science and Engineering
    at the Nanyang Technological University, Singapore. He received his PhD in electrical
    and computer engineering from the National University of Singapore. His major
    research interests include image and video analytics, visual intelligence, and
    machine learning. |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/c02bcf06ed26db965b2457ebcd13290d.png) | Shijian Lu 是新加坡南洋理工大学计算机科学与工程学院的副教授。他在新加坡国立大学获得了电气与计算机工程博士学位。他的主要研究兴趣包括图像和视频分析、视觉智能以及机器学习。
    |'
- en: '| ![[Uncaptioned image]](img/a69193d2971afcd51c063197002e7771.png) | Ling Shao
    is the Chief Scientist of Terminus Group and the President of Terminus International.
    He was the founding CEO and Chief Scientist of the Inception Institute of Artificial
    Intelligence, Abu Dhabi, UAE. His research interests include computer vision,
    deep learning, medical imaging and vision and language. He is a fellow of the
    IEEE, the IAPR, the BCS and the IET. |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/a69193d2971afcd51c063197002e7771.png) | Ling Shao 是Termin
    Group的首席科学家及Termin International的总裁。他曾是阿布扎比人工智能创新研究院的创始首席执行官和首席科学家。他的研究兴趣包括计算机视觉、深度学习、医学成像以及视觉和语言。他是IEEE、IAPR、BCS和IET的会士。
    |'
