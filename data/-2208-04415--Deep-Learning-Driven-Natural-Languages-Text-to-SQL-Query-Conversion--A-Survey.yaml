- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:44:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:44:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2208.04415] Deep Learning Driven Natural Languages Text to SQL Query Conversion:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2208.04415] 深度学习驱动的自然语言文本到 SQL 查询转换：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.04415](https://ar5iv.labs.arxiv.org/html/2208.04415)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2208.04415](https://ar5iv.labs.arxiv.org/html/2208.04415)
- en: 'Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习驱动的自然语言文本到 SQL 查询转换：综述
- en: Ayush Kumar, Parth Nagarkar, Prabhav Nalhe, and Sanjeev Vijayakumar
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ayush Kumar, Parth Nagarkar, Prabhav Nalhe 和 Sanjeev Vijayakumar
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the future striving toward data-centric decision-making, seamless access
    to databases is of utmost importance. There is extensive research on creating
    an efficient text-to-sql (TEXT2SQL) model to access data from the database. Using
    a Natural language is one of the best interfaces that can bridge the gap between
    the data and results by accessing the database efficiently, especially for non-technical
    users. It will open the doors and create tremendous interest among users who are
    well versed in technical skills or not very skilled in query languages. Even if
    numerous deep learning-based algorithms are proposed or studied, there still is
    very challenging to have a generic model to solve the data query issues using
    natural language in a real-work scenario. The reason is the use of different datasets
    in different studies, which comes with its limitations and assumptions. At the
    same time, we do lack a thorough understanding of these proposed models and their
    limitations with the specific dataset it is trained on. In this paper, we try
    to present a holistic overview of 24 recent neural network models studied in the
    last couple of years, including their architectures involving convolutional neural
    networks, recurrent neural networks, pointer networks, reinforcement learning,
    generative models, etc. We also give an overview of the 11 datasets that are widely
    used to train the models for TEXT2SQL technologies. We also discuss the future
    application possibilities of TEXT2SQL technologies for seamless data queries.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着未来向数据驱动决策的努力，访问数据库的无缝连接显得尤为重要。已有大量研究致力于创建高效的文本到 SQL（TEXT2SQL）模型以访问数据库中的数据。使用自然语言是一种极佳的接口，能够有效地连接数据与结果，尤其适合非技术用户。它将打开大门，激发对技术熟练或不熟练于查询语言的用户的极大兴趣。尽管已有许多基于深度学习的算法被提出或研究，但在实际工作场景中使用自然语言解决数据查询问题仍然非常具有挑战性。原因在于不同研究使用了不同的数据集，这些数据集各有其局限性和假设。同时，我们对这些提议的模型及其在特定数据集上的局限性了解不足。本文试图提供最近几年对24种神经网络模型的全面概述，包括卷积神经网络、递归神经网络、指针网络、强化学习、生成模型等。我们还概述了用于训练
    TEXT2SQL 技术模型的11个广泛使用的数据集。我们还讨论了 TEXT2SQL 技术在无缝数据查询中的未来应用可能性。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Natural language processing, deep learning, SQL Query, machine translation
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理、深度学习、SQL 查询、机器翻译
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: In today’s world, a considerable portion of data is saved in relational databases
    for applications ranging from finance and e-commerce to medicine. As a result,
    it is not surprising that utilizing natural language to query a database has a
    wide range of uses. It also opens up the possibility of self-serving dashboards
    and dynamic analytics, where individuals unfamiliar with the SQL language may
    utilize it to obtain the most relevant information for their organization. Many
    activities are associated with converting natural language to SQL [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)], including code creation and schema
    construction. However, creating SQL is more complex than the typical semantic
    parsing problem. A brief natural language inquiry may necessitate combining numerous
    tables or having multiple filtering requirements. This needs more context-based
    techniques. In recent years, with the widespread development of deep learning
    techniques, particularly convolutions and recurrent neural networks, the outcomes
    have drastically improved for this purpose.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，相当一部分数据被保存在关系数据库中，这些应用范围从金融和电子商务到医学。因此，利用自然语言查询数据库有广泛的用途也就不足为奇。这还打开了自服务仪表盘和动态分析的可能性，那些不熟悉SQL语言的个人可以利用它来获取最相关的信息。许多活动与将自然语言转换为SQL相关[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)]，包括代码创建和模式构建。然而，创建SQL比典型的语义解析问题更复杂。简短的自然语言询问可能需要合并多个表或具有多个过滤要求。这需要更多基于上下文的技术。近年来，随着深度学习技术的广泛发展，特别是卷积神经网络和递归神经网络的应用，结果在这方面有了显著改善。
- en: 'In general TEXT2SQL algorithms involve converting a natural language statement
    or texts into SQL query [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)] to access the desired dataset from the respective database. The
    input in the form of natural language text from the users is fed through various
    TEXT2SQL algorithms using baseline algorithms such as convolutional neural networks,
    recurrent neural networks, pointer networks, reinforcement learning, and generative
    model. Using these algorithms and input text queries, desired SQL queries are
    generated by concatenating various conditions used in the input. These SQL queries
    are then used to access the required dataset from the respective databases or
    a combination of various databases for many applications. It might eventually
    be integrated to make a broader goal of translating natural language into a fully
    functional application, combining with different forms or visualizations [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)] or visual analytic tools [[13](#bib.bib13),
    [14](#bib.bib14)]. One such sample for data query from natural language is shown
    in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning Driven Natural
    Languages Text to SQL Query Conversion: A Survey") by Brunner et al. [[15](#bib.bib15)].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，TEXT2SQL算法涉及将自然语言声明或文本转换为SQL查询[[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]，以访问相应数据库中的所需数据集。用户输入的自然语言文本通过各种TEXT2SQL算法进行处理，这些算法使用卷积神经网络、递归神经网络、指针网络、强化学习和生成模型等基线算法。利用这些算法和输入文本查询，通过连接输入中使用的各种条件生成所需的SQL查询。这些SQL查询随后用于从相应的数据库或多个数据库的组合中访问所需的数据集。最终，它可能会被整合以实现将自然语言翻译为完全功能应用的更广泛目标，与不同形式或可视化[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)]或可视化分析工具[[13](#bib.bib13), [14](#bib.bib14)]结合起来。图[1](#S1.F1
    "图1 ‣ 介绍 ‣ 基于深度学习的自然语言文本到SQL查询转换：综述")展示了从自然语言进行数据查询的一个示例[[15](#bib.bib15)]。
- en: '![Refer to caption](img/9d0274ea39ee3d2e606daf18c2fdab14.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9d0274ea39ee3d2e606daf18c2fdab14.png)'
- en: 'Figure 1: A generic NLP TEXT2SQL schema that generated a SQL query given a
    natural language question and a database schema [[15](#bib.bib15)]. Other than
    understanding the requirement for a valid SQL query (non-highlighted words), the
    NLP algorithm also has to select the correct tables, columns, and values from
    where it will fetch the result'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个通用的NLP TEXT2SQL架构，它根据自然语言问题和数据库架构生成SQL查询[[15](#bib.bib15)]。除了理解有效SQL查询的需求（非高亮的词），NLP算法还必须从中选择正确的表、列和值来获取结果。
- en: The availability of massive annotated datasets containing questions and database
    queries has speeded up and caused substantial progress in the field by allowing
    the construction and deployment of supervised learning models for the job. This
    feat has been achieved by improving the test sets’ accuracy given with the datasets
    and focusing on developing the problem formulation toward higher complications
    more closely approaching applications that can be used in the real world. New
    datasets such as WikiSQL [[16](#bib.bib16)] and Spider [[17](#bib.bib17)] pose
    the real-life challenge of generalization to unseen database schemas, where Stack
    Exchange data [[4](#bib.bib4)] is taken from real-world example itself, which
    makes it equally challenging. There is a multi-database schema to which each query
    is mapped, and there is no overlap among the databases between the training and
    test sets. Multiple reasons make the generalization of schema challenging.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大量注释数据集的可用性，包括问题和数据库查询，加快并推动了该领域的重大进展，这要归功于能够构建和部署用于该工作的监督学习模型。这一成就通过提高数据集中提供的测试集的准确性以及将问题表述的复杂性提升到更接近现实世界应用的水平来实现。新数据集如
    WikiSQL [[16](#bib.bib16)] 和 Spider [[17](#bib.bib17)] 真实地挑战了对未见过的数据库模式的泛化能力，而
    Stack Exchange 数据 [[4](#bib.bib4)] 来自真实世界的示例，使其同样具有挑战性。每个查询都映射到一个多数据库模式，训练集和测试集之间的数据库没有重叠。各种原因使得模式的泛化变得具有挑战性。
- en: There are multiple annotated datasets used for the Text2SQL [[4](#bib.bib4)]
    task. The most commonly used datasets are Spider [[17](#bib.bib17)] and WikiSQL [[16](#bib.bib16)].
    Spider is a large-scale, complex, cross-domain semantic parsing and text-to-SQL
    dataset annotated by 11 college students. It comprises 10,181 questions and 5,693
    distinct complex SQL queries on 200 databases with numerous tables spanning 138
    domains. The WikiSQL corpus contains 87,726 hand-annotated SQL queries and natural
    language question pairings. These SQL queries are further classified as training
    (61297 examples), development (9145 examples), and test sets (17,284 examples).
    It may be used for relational database-related natural language inference problems.
    Multiple approaches have been used for the Text2SQL task, such as semantic parsing,
    IRNET, RuleSQL, etc., which use sophisticated methods to solve the Text2SQL task
    and reduce the error rate.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 Text2SQL [[4](#bib.bib4)] 任务的注释数据集有很多。其中最常用的数据集是 Spider [[17](#bib.bib17)]
    和 WikiSQL [[16](#bib.bib16)]。Spider 是一个大规模、复杂的跨领域语义解析和文本到 SQL 数据集，由 11 名大学生注释。它包含
    10,181 个问题和 5,693 个不同的复杂 SQL 查询，涉及 200 个数据库，涵盖 138 个领域。WikiSQL 语料库包含 87,726 个手工注释的
    SQL 查询和自然语言问题配对。这些 SQL 查询进一步分类为训练集（61297 个示例）、开发集（9145 个示例）和测试集（17,284 个示例）。它可以用于与关系数据库相关的自然语言推理问题。为
    Text2SQL 任务使用了多种方法，如语义解析、IRNET、RuleSQL 等，这些方法利用复杂的技术来解决 Text2SQL 任务并降低错误率。
- en: 'This overview paper presents comprehensive research on the most often used
    $11$ datasets, as shown in Table [I](#S3.T1 "TABLE I ‣ III Benchmark Dataset ‣
    Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey"),
    and 24 most recent (2018-2022) algorithms used on these datasets to address the
    challenge of synthesizing SQL queries from natural language texts. We looked into
    all the major NLP conferences such as ACL, EMNLP, IJCNLP, SIGKDD, ICLR, Computational
    Linguistics, and many other conferences and journals. The primary goal of this
    work is to offer a complete description and analysis of the most recent approaches
    for dealing with the issue of producing SQL using natural language, as well as
    the many datasets and evaluation methodologies that are constantly being improved.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '本概述论文提供了对最常用的 $11$ 个数据集的全面研究，如表 [I](#S3.T1 "TABLE I ‣ III Benchmark Dataset
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey")
    所示，并介绍了在这些数据集上使用的 24 种最新（2018-2022）算法，以应对从自然语言文本中合成 SQL 查询的挑战。我们调查了所有主要的 NLP 会议，如
    ACL、EMNLP、IJCNLP、SIGKDD、ICLR、计算语言学以及许多其他会议和期刊。此项工作的主要目标是提供对处理自然语言生成 SQL 问题的最新方法、各种数据集和评估方法的完整描述和分析，并不断进行改进。'
- en: II Terminology
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 术语
- en: 'LSTM: Long Short-Term Memory (LSTM) [[18](#bib.bib18)] networks are a type
    of recurrent neural network (RNN) [[19](#bib.bib19)] capable of gauging the dependence
    of order in problems that require the prediction of sequential data. They are
    required in convoluted problem domains like machine translation, recognition of
    speech, etc. Long Short Term Memory Networks are well-suited to analyzing, processing,
    and predicting based on data in the time-series format. This is because many times,
    there are lags of unrevealed time scales between two or more events that might
    be significant.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM：长短期记忆（LSTM） [[18](#bib.bib18)] 网络是一种递归神经网络（RNN） [[19](#bib.bib19)]，能够处理需要预测顺序数据的问题的顺序依赖性。它们在复杂问题领域如机器翻译、语音识别等中是必需的。长短期记忆网络非常适合分析、处理和预测基于时间序列格式的数据。这是因为许多时候，两个或多个事件之间可能存在未被揭示的时间延迟，这些延迟可能是重要的。
- en: 'Encoder: Encoder [[20](#bib.bib20), [21](#bib.bib21)] is a pile of multiple
    recurrent units such as LSTM, where each gets input in the form of a single element
    of the input sequence, gathering data for that specific element and generating
    it forward. It follows a procedure to transform the relevant text into number/vector
    representation to conserve the conditions and connection between words and sentences,
    such that a machine can comprehend the pattern associated with any text and make
    out the context of the sentences.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器：编码器 [[20](#bib.bib20), [21](#bib.bib21)] 是由多个递归单元（如 LSTM）堆叠而成，每个单元以输入序列中的单个元素为输入，收集该特定元素的数据并向前生成。它遵循一种将相关文本转换为数字/向量表示的过程，以保存单词和句子之间的条件和连接，使机器能够理解与任何文本相关的模式，并辨别句子的上下文。
- en: 'Decoder: Decoder [[20](#bib.bib20), [21](#bib.bib21)] is a pile of multiple
    recurrent units in which an output y for every time step is predicted. The current
    recurrent unit accepts a hidden state from the earlier recurrent unit. This is
    generally used in question-answering problems where the sequence of outputs is
    a collection of answers’ words.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器：解码器 [[20](#bib.bib20), [21](#bib.bib21)] 是由多个递归单元堆叠而成的，在每个时间步预测一个输出 y。当前递归单元接受来自前一个递归单元的隐藏状态。这通常用于问答问题，其中输出序列是答案词的集合。
- en: 'BERT: BERT [[22](#bib.bib22)] is an open source machine learning framework
    for natural language processing (NLP). It is designed to assist machines with
    comprehending vague language in the text by using nearby text to organize context.
    A Transformer is used along with the attention mechanism [[23](#bib.bib23)] to
    learn the relations with respect to their context between multiple words in a
    sentence.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: BERT：BERT [[22](#bib.bib22)] 是一个开源的自然语言处理（NLP）机器学习框架。它旨在通过使用附近的文本来组织上下文，帮助机器理解文本中的模糊语言。使用Transformer及其注意力机制 [[23](#bib.bib23)]
    来学习句子中多个词语之间相对于上下文的关系。
- en: 'Semantic Parsing: Semantic Parsing [[24](#bib.bib24)] is used for transforming
    a natural language like English into a form that machines can comprehend in a
    logical form [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. The transformed languages can include SQL or any other conceptual
    representations.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语义解析：语义解析 [[24](#bib.bib24)] 用于将像英语这样的自然语言转换为机器可以以逻辑形式理解的形式 [[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]。转换后的语言可以包括 SQL
    或任何其他概念表示。
- en: 'Baseline Model: A baseline model is a model that is very simple to implement
    or configure and, in most cases, provides satisfactory results. Trying to run
    any experiments with baseline models is fast and requires minimal cost. Usually,
    researchers use baseline models and leverage them to try to describe how their
    own trained model is better. The score of the baseline model is generally kept
    as a threshold.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基线模型：基线模型是一个非常简单实现或配置的模型，并且在大多数情况下提供令人满意的结果。尝试使用基线模型进行实验速度快且成本最低。研究人员通常使用基线模型，并利用它们来描述自己训练的模型如何更好。基线模型的得分通常作为阈值。
- en: 'Incremental Decoding: Incremental decoding [[25](#bib.bib25)] is a method in
    which translations are done dynamically as a set of input is parsed to the model.
    This approach refrains from waiting for the entirety of input to be parsed; instead
    produces output instantaneously as input is received.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 增量解码：增量解码 [[25](#bib.bib25)] 是一种方法，其中翻译是在输入集被解析到模型时动态完成的。这种方法避免了等待全部输入被解析，而是随着输入的接收即时生成输出。
- en: 'Exacting matching accuracy: It is the percentage of questions that outputs
    the exact SQL query as expected.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 精确匹配准确率：它是输出与预期完全相同的 SQL 查询的问题的百分比。
- en: Component matching F1:It is the cumulative F1 scores.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 组件匹配 F1：它是累积的 F1 分数。
- en: III Benchmark Dataset
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基准数据集
- en: The datasets designed for semantic parsing of natural language phrases to SQL
    queries consist of a combination of Annotated complicated questions and SQL queries.
    The sentences are inquiries for a certain area. In contrast, the answers come
    from existing databases resulting in a link between the question to its corresponding
    SQL query such that on the execution of the SQL query, the result is obtained
    from existing databases. Several semantic parsing datasets for SQL query been
    generated recently, each with different yet significant characteristics. Table
    1 from Kalajdjieski et.al [[26](#bib.bib26)] contains extensive statistics of
    the most popular datasets utilized by researchers. ATIS [[27](#bib.bib27)], GeoQuery [[28](#bib.bib28)],
    Restaurants [[29](#bib.bib29)], Academic [[30](#bib.bib30)], IMDB [[31](#bib.bib31)],
    Scholar [[32](#bib.bib32)], Yelp [[33](#bib.bib33)], Stack Exchange Data [[26](#bib.bib26)],
    and Advising [[34](#bib.bib34)] are examples of early datasets that focused on
    a single topic and database. WikiSQL2 [[16](#bib.bib16)] and Spider [[17](#bib.bib17)]
    are newer datasets that are context-agnostic and greater in size across domains.
    Additionally, new datasets include more queries and in-depth research, thus catering
    to effective model evaluation. To assess the model generalization capabilities,
    previously unknown advanced queries may be used in the test sets. The authors
    of Advising demonstrate that standard data splits increase the generalizability
    of the systems. Even though the WikiSQL dataset comprises many questions and SQL
    queries, the SQL queries are short and limited to certain tables. On the other
    hand, WikiSQL consists of more questions and SQL queries than the Spider dataset,
    which is composed of questions that include various SQL expressions such as table
    join and nested query, making them of added complexity in comparison. Spider dataset
    extensions SParC [[35](#bib.bib35)] and CoSQL [[36](#bib.bib36)] are developed
    for contextual cross-domain semantical parsing and conversational dialog text-to-SQL
    systems. As a result, these fresh aspects provide new and significant issues for
    future research in this sector.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 设计用于将自然语言短语解析为 SQL 查询的数据集由一系列注释的复杂问题和 SQL 查询组成。这些句子是针对特定领域的询问。相比之下，答案来自现有的数据库，从而在问题与其对应的
    SQL 查询之间建立了联系，使得在执行 SQL 查询时，结果从现有数据库中获得。最近生成了多个用于 SQL 查询的语义解析数据集，每个数据集都有不同但重要的特征。Kalajdjieski
    等人[[26](#bib.bib26)]的表 1 包含了研究人员使用的最受欢迎的数据集的详细统计数据。ATIS [[27](#bib.bib27)]、GeoQuery [[28](#bib.bib28)]、Restaurants [[29](#bib.bib29)]、Academic [[30](#bib.bib30)]、IMDB [[31](#bib.bib31)]、Scholar [[32](#bib.bib32)]、Yelp [[33](#bib.bib33)]、Stack
    Exchange Data [[26](#bib.bib26)] 和 Advising [[34](#bib.bib34)] 是早期专注于单一主题和数据库的数据集的例子。WikiSQL2 [[16](#bib.bib16)]
    和 Spider [[17](#bib.bib17)] 是更为新颖的数据集，不依赖于特定上下文，并且跨领域的规模更大。此外，新数据集包括更多的查询和深入的研究，从而有助于有效的模型评估。为了评估模型的泛化能力，可能会在测试集中使用之前未知的高级查询。Advising
    的作者表明，标准的数据划分提高了系统的泛化能力。尽管 WikiSQL 数据集包含了许多问题和 SQL 查询，但这些 SQL 查询较短且仅限于某些表。另一方面，WikiSQL
    包含的问题和 SQL 查询数量多于 Spider 数据集，而 Spider 数据集则包含了包括表连接和嵌套查询等各种 SQL 表达式的问题，这使得其复杂性增加。Spider
    数据集扩展的 SParC [[35](#bib.bib35)] 和 CoSQL [[36](#bib.bib36)] 是为了上下文跨领域语义解析和对话文本到
    SQL 系统而开发的。因此，这些新鲜的方面为未来该领域的研究提供了新的重要问题。
- en: 'ATIS (Air Travel Information System) / GeoQuery: ATIS is also known as Airline
    Travel Information Systems, a dataset [[27](#bib.bib27)] consisting of audio recordings
    and hand transcripts of individuals utilizing automated travel inquiry systems
    seeking information about flights. The data is categorized into main purpose categories
    where the train, development, and test sets are composed of 4478, 500, and 893
    intent-labeled reference utterances, respectively. ATIS, most commonly used for
    semantic parsing, is a methodology for facilitating the conversion of natural
    language queries into a formal meaning representation. On the other hand, GeoQuery
    consists of seven tables from the US geography database and 880 natural language
    to SQL pairings. Unlike WikiSQL, all queries in ATIS and GeoQuery are on a single
    domain. Resultantly utilizing them to train a deep learning model makes the model
    work only in one domain. Both these benchmarks contain varied queries with join
    and nested queries inclusive, whereas GeoQuery possesses a grouping and ordering
    query that the latter, i.e., ATIS, does not.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'ATIS (Air Travel Information System) / GeoQuery: ATIS 也被称为航空旅行信息系统，是一个包含音频录音和手工转录的个人使用自动旅行查询系统以获取航班信息的数据集[[27](#bib.bib27)]。数据被分类为主要目的类别，其中训练、开发和测试集分别由4478、500和893条意图标记的参考语句组成。ATIS最常用于语义解析，是一种将自然语言查询转换为正式意义表示的方法。另一方面，GeoQuery包含来自美国地理数据库的七个表和880个自然语言到SQL的配对。与WikiSQL不同，ATIS和GeoQuery中的所有查询都在一个领域内。因此，利用它们训练深度学习模型使模型仅在一个领域内工作。这两个基准都包含了各种查询，包括连接和嵌套查询，而GeoQuery拥有ATIS所没有的分组和排序查询。'
- en: 'IMDb: The IMDb dataset [[31](#bib.bib31)] is a massive collection of 50K IMDb
    reviews. Each film is limited to 30 reviews. There are an equal amount of favorable
    and negative reviews in the dataset. The dataset developers took into account
    extremely polarized reviews, with a negative review receiving a score of 4 out
    of 10 and a favorable review receiving a score of 7 out of 10. When constructing
    the dataset, neural reviews are not taken into account. The dataset is distributed
    evenly between training and testing.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'IMDb: IMDb 数据集[[31](#bib.bib31)]是一个包含5万条IMDb评论的大型集合。每部电影限制为30条评论。数据集中包含了相等数量的好评和差评。数据集开发者考虑了极端分化的评论，其中负面评论的得分为10分中的4分，正面评论的得分为10分中的7分。在构建数据集时，神经网络评论未被考虑。数据集在训练和测试之间均匀分配。'
- en: 'Advising: The advising dataset [[34](#bib.bib34)] was intended to suggest improvements
    to text2SQL systems. The dataset’s designers compare human-authored versus computer-produced
    inquiries, noting query features relevant to real-world applications. The dataset
    comprises university students’ questions concerning courses that lead to complicated
    queries. The database contains fictitious student records. The dataset consists
    of student profile information such as suggested courses, grades, and introductory
    courses taken by the student. Students with knowledge of the database developed
    inquiries and were instructed to frame queries they may ask in an academic advising
    appointment. Many searches in the dataset were similar to those in the ATIS, GeoQuery,
    and Scholar databases.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'Advising: Advising 数据集[[34](#bib.bib34)]旨在为text2SQL系统建议改进。数据集设计者比较了人类撰写的查询与计算机生成的查询，注意到与实际应用相关的查询特征。数据集包括大学生关于课程的问题，这些问题涉及复杂的查询。数据库包含虚构的学生记录。数据集包括学生档案信息，如建议课程、成绩以及学生所修的入门课程。具有数据库知识的学生开发了查询，并被指示构造他们可能在学术咨询预约中提出的查询。数据集中许多搜索类似于ATIS、GeoQuery和Scholar数据库中的搜索。'
- en: 'MAS : Microsoft Academic Search includes a database of academic and social
    networks and a collection of queries. MAS [[37](#bib.bib37)], like ATIS and GeoQuery,
    operates on a single domain. It has 17 tables in its database and 196 natural
    languages to SQL pairs. MAS contains a variety of SQL queries that include join,
    grouping, and nested queries but no ordering queries. The following limitations
    apply to each natural language query in MAS. To begin, a natural language question
    starts with ”return me.” In real-world scenarios, a user may inquire using an
    interrogative statement or a collection of keywords; however, MAS does not include
    such cases. Second, each natural language inquiry follows proper grammatical conventions.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'MAS : Microsoft Academic Search 包括一个学术和社交网络的数据库以及一组查询。MAS [[37](#bib.bib37)]，类似于
    ATIS 和 GeoQuery，操作在单一领域内。它的数据库中有 17 个表格和 196 对自然语言到 SQL 的配对。MAS 包含各种 SQL 查询，包括连接、分组和嵌套查询，但不包含排序查询。每个自然语言查询都有以下限制。首先，自然语言问题以“return
    me.” 开始。在现实世界中，用户可能使用疑问句或一系列关键字进行询问；然而，MAS 不包括这些情况。其次，每个自然语言查询都遵循适当的语法规范。'
- en: 'Spider : Spider [[17](#bib.bib17)] is a large-scale, complicated, cross-domain
    semantic parsing and text-to-SQL dataset that 11 Yale students annotated. The
    Spider challenge’s purpose is to provide natural language interfaces to cross-domain
    databases. It comprises 10,181 questions and 5,693 distinct sophisticated SQL
    queries on 200 databases with numerous tables across 138 domains. In Spider 1.0,
    train and test sets contain a variety of complicated SQL queries and databases.
    To perform effectively, systems must generalize not just to new SQL queries but
    also to new database structures.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'Spider : Spider [[17](#bib.bib17)] 是一个大规模、复杂的跨领域语义解析和文本到 SQL 数据集，由 11 名耶鲁大学学生注释。Spider
    挑战的目的是为跨领域数据库提供自然语言接口。它包含 10,181 个问题和 5,693 个不同的复杂 SQL 查询，这些查询分布在 200 个数据库上，涵盖
    138 个领域的众多表格。在 Spider 1.0 中，训练集和测试集包含各种复杂的 SQL 查询和数据库。为了有效地执行，系统不仅必须对新的 SQL 查询进行泛化，还必须对新的数据库结构进行泛化。'
- en: 'WikiSQL : WikiSQL [[16](#bib.bib16)] is the most popular and most extensive
    benchmark, with 24,241 tables and 80,654 natural languages to SQL pairings in
    a single table. Tables are taken from Wikipedia’s HTML tables. Then, for a given
    table, each SQL query is automatically created under the constraint that the query
    yields a non-empty result set. Each natural language inquiry is made using a basic
    template and then paraphrased by Amazon Mechanical Turk. All SQL queries in WikiSQL
    follow the same syntactic pattern: SELECT FROM T [WHERE (and)*], where T is a
    single table. This allows for a single projected column and conjunction option.
    It is worth noting that this syntax conveys no grouping, sorting, join, or nested
    queries.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'WikiSQL : WikiSQL [[16](#bib.bib16)] 是最流行和最广泛的基准测试，拥有 24,241 个表格和 80,654 对自然语言到
    SQL 的配对。表格取自维基百科的 HTML 表格。然后，对于给定的表格，每个 SQL 查询在约束下自动生成，即查询会生成非空的结果集。每个自然语言查询使用基本模板制作，然后由
    Amazon Mechanical Turk 进行释义。WikiSQL 中的所有 SQL 查询都遵循相同的语法模式：SELECT FROM T [WHERE
    (and)*]，其中 T 是单个表格。这允许一个投影列和连词选项。值得注意的是，这种语法不包含分组、排序、连接或嵌套查询。'
- en: The significant distinction between WikiSQL and Spider is that SQL queries in
    Spider are more complicated than those in WikiSQL. Table 1 is a complex SQL example
    from Spider in which the query appears theoretically easy but contains multiple
    elements of database structure and SQL clause. Aside from that, the Spider database
    has many tables, but the WikiSQL database has only one. The presence of many tables
    adds column and table name disambiguation issues to Spider, which do not exist
    in WikiSQL.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: WikiSQL 和 Spider 之间的主要区别在于，Spider 中的 SQL 查询比 WikiSQL 中的 SQL 查询更复杂。表 1 是一个来自
    Spider 的复杂 SQL 示例，其中查询在理论上看起来简单，但包含多个数据库结构和 SQL 子句的元素。除此之外，Spider 数据库有许多表格，而 WikiSQL
    数据库只有一个。大量表格的存在为 Spider 增加了列名和表名消歧的问题，而 WikiSQL 中没有这些问题。
- en: While WikiSQL and Spider are cross-domain settings, most SQL queries do not
    need domain expertise during the creation process. Domain knowledge is a consensus
    on just one issue and will not be articulated clearly in the inquiry. For example,
    when domain knowledge is necessary, asking for a ’good restaurant’ might correspond
    to a WHERE condition ’star’ greater than $3.5$ since this domain assesses places
    from 0 to 5 stars? Some domain examples replace words associated with schema item
    names while keeping the same phrase structure. Furthermore, rather than utilizing
    synonyms, most sentences use words directly connected to schema item names, allowing
    the model to discover the schema items using word matching.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然WikiSQL和Spider是跨领域设置，但大多数SQL查询在创建过程中不需要领域专业知识。领域知识对一个问题达成共识，并不会在查询中明确表达。例如，当需要领域知识时，询问‘好餐馆’可能对应于一个WHERE条件‘星级’大于$3.5$，因为这个领域评估的地方从0到5星？一些领域示例用与模式项名称相关的词替换词语，同时保持相同的短语结构。此外，大多数句子使用直接与模式项名称相关的词，而不是同义词，从而使模型通过词匹配发现模式项。
- en: 'Stack Exchange : Stack Exchange Data Explorer (SEDE) [[4](#bib.bib4)] SEDE
    is an online question and answers community with over 3 million questions; it
    recently released a benchmark dataset of SQL queries consisting of 29 tables and
    211 columns. This dataset is collected from real usage on the Stack Exchange website
    of common utterance topics such as published posts, comments, votes, tags, awards,
    etc. These datasets are also challenging to handle while semantically parsing
    as they have various real-world questions. We show that these pairs contain a
    variety of real-world challenges, which were rarely reflected so far in any other
    semantic parsing dataset. 1,714 questions out of 12,023 questions (clean) asked
    on the platform are verified by humans, making it an excellent choice for validation
    and test set while training the model.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'Stack Exchange : Stack Exchange 数据探测器（SEDE） [[4](#bib.bib4)] SEDE是一个在线问答社区，拥有超过300万的问题；它最近发布了一个SQL查询的基准数据集，包含29个表和211列。这个数据集来自Stack
    Exchange网站上的实际使用，涉及常见的发言主题，如已发布的帖子、评论、投票、标签、奖项等。这些数据集在语义解析时也很具挑战性，因为它们包含各种现实世界的问题。我们展示了这些对包含各种现实世界挑战，这些挑战在任何其他语义解析数据集中都很少反映。平台上12,023个（清洁）问题中的1,714个经过人工验证，使其成为训练模型时进行验证和测试的绝佳选择。'
- en: '| Dataset | Year | Tables | Questions | Unique Queries | Domain |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年 | 表格 | 问题 | 唯一查询 | 领域 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ATIS [[27](#bib.bib27)] | 1994 | 25 | 5280 | 947 | Air Travel Information
    System |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ATIS [[27](#bib.bib27)] | 1994 | 25 | 5280 | 947 | 航空旅行信息系统 |'
- en: '| GeoQuery [[28](#bib.bib28)] | 2001 | 8 | 877 | 246 | US geography database
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| GeoQuery [[28](#bib.bib28)] | 2001 | 8 | 877 | 246 | 美国地理数据库 |'
- en: '| Restaurants [[29](#bib.bib29)] | 2000 | 3 | 378 | 23 | Restaurants, Food
    Type, Locations |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 餐馆 [[29](#bib.bib29)] | 2000 | 3 | 378 | 23 | 餐馆、食品类型、地点 |'
- en: '| Academic [[30](#bib.bib30)] | 2014 | 15 | 196 | 185 | Microsoft Academic
    Search |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 学术 [[30](#bib.bib30)] | 2014 | 15 | 196 | 185 | Microsoft Academic Search
    |'
- en: '| IMDB [[31](#bib.bib31)] | 2015 | 16 | 131 | 89 | Internet Movie Database
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| IMDB [[31](#bib.bib31)] | 2015 | 16 | 131 | 89 | 互联网电影数据库 |'
- en: '| Scholar [[32](#bib.bib32)] | 2017 | 7 | 817 | 193 | Academic Publications
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Scholar [[32](#bib.bib32)] | 2017 | 7 | 817 | 193 | 学术出版物 |'
- en: '| Yelp [[33](#bib.bib33)] | 2017 | 7 | 128 | 110 | Yelp Movie Website |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Yelp [[33](#bib.bib33)] | 2017 | 7 | 128 | 110 | Yelp 电影网站 |'
- en: '| WikiSQL [[16](#bib.bib16)] | 2017 | 24241 | 80654 | 77840 | Wikipedia |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| WikiSQL [[16](#bib.bib16)] | 2017 | 24241 | 80654 | 77840 | 维基百科 |'
- en: '| Advising [[34](#bib.bib34)] | 2018 | 10 | 4570 | 211 | Student Course Information
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Advising [[34](#bib.bib34)] | 2018 | 10 | 4570 | 211 | 学生课程信息 |'
- en: '| Spider [[17](#bib.bib17)] | 2018 | 645 | 10181 | 5693 | 138 Different Domains
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Spider [[17](#bib.bib17)] | 2018 | 645 | 10181 | 5693 | 138 个不同领域 |'
- en: '| SEDE [[4](#bib.bib4)] | 2021 | 29 | 12,023 | 11767 | Stack Exchange |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| SEDE [[4](#bib.bib4)] | 2021 | 29 | 12,023 | 11767 | Stack Exchange |'
- en: 'TABLE I: Tabulation of Benchmark Datasets [[26](#bib.bib26), [4](#bib.bib4)]'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 基准数据集汇总 [[26](#bib.bib26), [4](#bib.bib4)]'
- en: IV Algorithmic explanation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 算法解释
- en: In this section, we focus on explaining 24 recent (2018 -2022) Text2SQL algorithms
    that introduce an improvement or adapt existing text2SQL techniques to address
    all common challenges researchers face.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将重点解释24种最近（2018 - 2022年）的Text2SQL算法，这些算法要么引入了改进，要么适应了现有的text2SQL技术，以解决研究人员面临的所有常见挑战。
- en: IV-A SQLNet [[1](#bib.bib1)]
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A SQLNet [[1](#bib.bib1)]
- en: Synthesizing SQL queries from natural language has been a long-standing open
    subject lately garnered significant interest. This paper suggests a unique solution,
    SQLNet, to fundamentally tackle the problem of synthesizing SQL queries by bypassing
    the sequence-to-sequence structure where the order is entirely irrelevant. They
    use a sketch-based technique in particular, where the sketch incorporates a dependency
    network, such that a single prediction may be made by considering just the prior
    forecasts on which it is dependent. Additionally, this paper presents a sequence-to-set
    model and column attention technique for generating the query from the sketch.
    By integrating all of these unique strategies, the authors have demonstrated that
    SQLNet outperforms the previous state of state-of-the-art SQL challenge by 9%
    to 13%.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从自然语言合成 SQL 查询一直是一个长期存在的开放问题，最近引起了广泛关注。本文提出了一种独特的解决方案——SQLNet，旨在从根本上解决 SQL 查询合成的问题，绕过序列到序列的结构，因为顺序完全无关紧要。他们特别使用了一种基于草图的技术，其中草图包含一个依赖网络，使得通过仅考虑其依赖的先前预测即可进行单次预测。此外，本文还提出了一种序列到集合的模型和列注意力技术，用于从草图生成查询。通过整合这些独特的策略，作者展示了
    SQLNet 相比之前的最先进 SQL 挑战提高了 9% 到 13% 的性能。
- en: To achieve this, the authors have used a sketch strongly aligned with the SQL
    language. The sketch is intended to be broad enough that it may be used to represent
    any SQL queries that interest the developer. The sketch illustrates the interdependence
    of the predictions to be made. The approaches are combined to create an SQLNet
    neural network, which may be used to synthesize a SQL query from a natural language
    inquiry and a database table structure. The SQLNet and training specifics are
    also mentioned to outperform the prior state-of-the-art technique without using
    reinforcement learning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，作者使用了与 SQL 语言强烈对齐的草图。草图的设计足够宽泛，可以表示开发者感兴趣的任何 SQL 查询。草图展示了预测之间的相互依赖。这些方法结合起来创建了一个
    SQLNet 神经网络，该网络可用于从自然语言查询和数据库表结构中合成 SQL 查询。还提到了 SQLNet 和训练细节，显示其在不使用强化学习的情况下超越了之前的最先进技术。
- en: '![Refer to caption](img/731e032a392ba8015b7718371ad6f0ec.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/731e032a392ba8015b7718371ad6f0ec.png)'
- en: 'Figure 2: Sketch Syntax and dependency in a sketch [[1](#bib.bib1)]'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 草图语法和草图中的依赖 [[1](#bib.bib1)]'
- en: IV-A1 Sketch Based query synthesis
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 基于草图的查询合成
- en: A box represents each slot whose value is to be predicted, and a directed edge
    represents the relationship between each slot and its dependencies. For example,
    the box of OP1 has two incoming edges, one from Column1 and the other from the
    natural language query. These edges suggest that the prediction of the value for
    OP1 is influenced by both the values of Column1 and the natural language query
    provided. This dependency network serves as the foundation for Their model, which
    They may consider as a graphical model and the query synthesis issue as an inference
    problem based on the graph. When They look at it from this viewpoint, they can
    see that the prediction of one constraint is entirely independent of the prediction
    of another constraint. Therefore, Their technique can altogether ”order matters”
    problem in a sequence-to-sequence model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个槽位由一个框表示，其值需预测，而有向边表示每个槽位及其依赖之间的关系。例如，OP1 的框有两条入边，一条来自 Column1，另一条来自自然语言查询。这些边表明
    OP1 的值预测受到 Column1 和自然语言查询值的影响。这个依赖网络作为他们模型的基础，他们可以将其视为一个图形模型，将查询合成问题视为基于图的推断问题。从这个视角看，他们可以发现一个约束的预测与另一个约束的预测完全独立。因此，他们的技术可以完全解决序列到序列模型中的“顺序重要性”问题。
- en: IV-A2 Sequence-To-Set Prediction Using Column Attention
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 使用列注意力的序列到集合预测
- en: 'Sequence-to-set: From an intuitive standpoint, the column names appearing in
    the WHERE clause represent a subset of the total number of column names present
    in the database. Consequently, rather than creating a series of column names,
    they can anticipate which column names will appear in this subset of interest,
    saving time and effort. This concept is referred to as ”sequence-to-set prediction.”
    Notably, in this case, they compute the probability Pwherecol(col—Q), where col
    is the column’s name, and Q is the query in plain language. One approach to achieving
    this goal is to compute Pwherecol(col—Q) as where uc and uq are two column vectors
    of trainable variables, Ecol and EQ are the embeddings of the column name and
    the natural language question, and uc and uq are the embeddings of the column
    name and the natural language question, respectively. When the sequences of col
    and Q are used as inputs to an LSTM, it is possible to calculate the embeddings
    Ecol and EQ by computing the hidden states of a bi-directional LSTM running on
    top of each sequence.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到集合：从直观的角度来看，WHERE子句中出现的列名表示数据库中所有列名的一个子集。因此，他们可以预测在这个感兴趣的子集中会出现哪些列名，从而节省时间和精力，这一概念被称为“序列到集合预测”。值得注意的是，在这种情况下，他们计算概率Pwherecol(col—Q)，其中col是列名，Q是自然语言查询。实现这一目标的一种方法是计算Pwherecol(col—Q)，其中uc和uq是两个可训练的列向量，Ecol和EQ分别是列名和自然语言问题的嵌入。当将col和Q的序列作为LSTM的输入时，可以通过计算运行在每个序列上的双向LSTM的隐藏状态来计算Ecol和EQ的嵌入。
- en: For each token in the query, they compute the attention weights w for that token.
    Specifically, w is an L-dimension column vector, which can be represented mathematically
    as
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于查询中的每个标记，他们计算该标记的注意力权重w。具体来说，w是一个L维列向量，可以用数学表示为
- en: 'Column attention: They design the column attention mechanism to compute EQ—col
    instead of EQ. In particular, they assume HQ is a matrix of d×L, where L is the
    length of the natural language question. The i-th column of HQ represents the
    hidden state’s output of the LSTM corresponding to the i-th token of the question.n'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列注意力：他们设计了列注意力机制来计算EQ—col，而不是EQ。特别地，他们假设HQ是一个d×L的矩阵，其中L是自然语言问题的长度。HQ的第i列表示LSTM对应于问题第i个标记的隐藏状态的输出。
- en: IV-A3 SQLNet Model and Training Details
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 SQLNet模型和训练细节
- en: 'Predicting the WHERE clause: Column Slots: SQLNet must determine which columns
    should be included in the WHERE clause. An alternative strategy is establishing
    a threshold of 0 or 1 such that all columns with Pwherecol(col—Q) 0 or 1 are picked.
    They see that the WHERE clauses of most searches contain just a small number of
    columns, as They will see below. As a result, they establish an upper bound of
    N on the number of columns to pick from, and They formulate the issue of predicting
    the number of columns as an (N + 1)-way classification problem to begin with (from
    0 to N).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 预测WHERE子句：列插槽：SQLNet必须确定应该在WHERE子句中包含哪些列。另一种策略是设定一个0或1的阈值，使得所有Pwherecol(col—Q)为0或1的列被选中。他们发现大多数查询的WHERE子句只包含少量列。如下面所示，他们为此设定了一个N的列上限，并将预测列数的问题最初表述为一个（N
    + 1）分类问题（从0到N）。
- en: 'Predicting The Select Clause: The SELECT clause contains an aggregator as well
    as a column designation. The prediction of the column name in the SELECT clause
    is similar to the prediction in the WHERE clause. The most significant distinction
    is that with the Choose clause, they only need to select one column from among
    all of the other possible choices. As a result, they calculate'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 预测SELECT子句：SELECT子句包含一个聚合器和一个列指定。SELECT子句中列名的预测类似于WHERE子句中的预测。最重要的区别是，在Choose子句中，他们只需要从所有可能的选择中选择一个列。因此，他们计算
- en: Training Details To parse the statement, the Stanford CoreNLP tokenizer is employed.
    To accomplish this, they use the GloVe word embedding [[38](#bib.bib38)] to represent
    each token as a one-hot vector.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 训练细节 为了解析声明，使用了斯坦福CoreNLP的分词器。为此，他们使用GloVe词嵌入 [[38](#bib.bib38)] 将每个标记表示为一个独热向量。
- en: IV-B Slot-Filling Approach [[39](#bib.bib39)]
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 插槽填充方法 [[39](#bib.bib39)]
- en: In concordance with the name, a slot-filling approach imbibes the meaning of
    the sentence by a divide and conquer methodology [[39](#bib.bib39)]. Individual
    words encountered in the human language query are categorized into sub-domain,s
    also known as slots. With reference from the model TypeSQL [[2](#bib.bib2)], the
    essential and sufficient slots are declared to be $AGG, $SELECT_COL under the
    SELECT clause, which signifies the aggregation and selected column, allowing to
    capture the data required from a particular column and $COND_COL, $OP, $COND_VAL
    under the WHERE clause and AND clause (if needed) which assists in filtering the
    data being extracted according to the user. The three slots aforementioned under
    the WHERE and AND clause signify the column subjected to conditioning, an operator,
    and the condition value, respectively. To fit individual words in these slots,
    an input encoder comprising two bi-directional LSTMs [[40](#bib.bib40)] is utilized.
    The encoder classifies words based on predefined scilicet INTEGER, FLOAT, DATE,
    YEAR, and NAMED ENTITIES such as PERSON, PLACE, COUNTRY, ORGANIZATION, AND SPORT.
    The LSTMs are programmed accordingly to iterate over each column. After extensive
    research, the model was fabricated to work efficaciously with a sum of 3 models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与名称一致，槽填充方法通过分而治之的方法来吸收句子的含义 [[39](#bib.bib39)]。在自然语言查询中遇到的单词被分类到子领域，也称为槽。参考模型
    TypeSQL [[2](#bib.bib2)]，在 SELECT 子句中，$AGG 和 $SELECT_COL 被声明为基本和必要的槽，表示聚合和选择的列，从而允许捕获所需的特定列中的数据；而在
    WHERE 子句和 AND 子句（如需要）中，$COND_COL、$OP 和 $COND_VAL 帮助根据用户过滤提取的数据。上述 WHERE 和 AND
    子句中的三个槽分别表示施加条件的列、一个操作符和条件值。为了将单词适配到这些槽中，使用了包含两个双向 LSTM 的输入编码器 [[40](#bib.bib40)]。编码器基于预定义的
    INTEGER、FLOAT、DATE、YEAR 和 NAMED ENTITIES（如 PERSON、PLACE、COUNTRY、ORGANIZATION 和
    SPORT）对单词进行分类。LSTM 被相应编程以遍历每一列。经过广泛研究，模型被制造出来以有效地与三个模型一起工作。
- en: '|  | Dev | Test |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | 开发 | 测试 |'
- en: '| Content Insensitive |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 内容不敏感 |'
- en: '|  | $Acc_{If}$ | $Acc_{qm}$ | $Acc_{ex}$ | $Acc_{If}$ | $Acc_{qm}$ | $Acc_{ex}$
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $Acc_{If}$ | $Acc_{qm}$ | $Acc_{ex}$ | $Acc_{If}$ | $Acc_{qm}$ | $Acc_{ex}$
    |'
- en: '| Dong and Lapata (2016) | 23.3% | - | 37% | 23.4% | - | 35.9% |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Dong 和 Lapata（2016年） | 23.3% | - | 37% | 23.4% | - | 35.9% |'
- en: '| Augmented Pointer Network (Zhong et al., 2017) | 44.1% | - | 53.8% | 42.8%
    | - | 52.8% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 增强型指针网络（Zhong 等，2017年） | 44.1% | - | 53.8% | 42.8% | - | 52.8% |'
- en: '| Seq2SQL (Zhong et al., 2017) | 49.5% | - | 60.8% | 48.3% | - | 59.4% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Seq2SQL（Zhong 等，2017年） | 49.5% | - | 60.8% | 48.3% | - | 59.4% |'
- en: '| SQLNet (Xu et al., 2017) | - | 63.2% | 69.8% | - | 61.3% | 68.0% |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| SQLNet（Xu 等，2017年） | - | 63.2% | 69.8% | - | 61.3% | 68.0% |'
- en: '| TypeSQL w/o type-awareness (self-made) | - | 66.5% | 72.8% | - | 64.9% |
    71.7% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| TypeSQL 无类型感知（自制） | - | 66.5% | 72.8% | - | 64.9% | 71.7% |'
- en: '| TypeSQL (self-made) | - | 68.0% | 74.5% | - | 66.7% | 73.5% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| TypeSQL（自制） | - | 68.0% | 74.5% | - | 66.7% | 73.5% |'
- en: '| Content Sensitive |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 内容敏感 |'
- en: '| Wang et al. (2017a) | 59.6% | - | 65.2% | 59.5% | - | 65.1% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等（2017年a） | 59.6% | - | 65.2% | 59.5% | - | 65.1% |'
- en: '| TypeSQL+TC (self-made) | - | 79.2% | 85.5% | - | 75.4% | 82.6% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| TypeSQL+TC（自制） | - | 79.2% | 85.5% | - | 75.4% | 82.6% |'
- en: 'TABLE II: Experimental Result of Slot Filling Approach - TypeSQL derived from
    paper [[2](#bib.bib2)]'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 槽填充方法的实验结果 - TypeSQL 源自论文 [[2](#bib.bib2)]'
- en: 'There are three evaluation metrics utilized to display the performance of the
    model, namely accuracies of canonical representation matches on AGGREGATOR, SELECT
    COLUMN, and WHERE CLAUSES depicted by Acc_If, Acc_qm and Acc_ex respectively.
    The model outperforms the baseline models by 5.5% based on executing accuracy—similarly,
    the accuracy based on SELECT and WHERE clauses are improved by 1.3% and 5.9%,
    respectively. When given complete access to the database, the model achieves 82.6%
    based on executing accuracy and 17.5% on the content-aware system. The results
    have been summarized in table [II](#S4.T2 "TABLE II ‣ IV-B Slot-Filling Approach
    [39] ‣ IV Algorithmic explanation ‣ Deep Learning Driven Natural Languages Text
    to SQL Query Conversion: A Survey").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '模型性能展示利用了三种评估指标，即在 AGGREGATOR、SELECT COLUMN 和 WHERE CLAUSES 上的规范表示匹配准确度，分别由
    Acc_If、Acc_qm 和 Acc_ex 表示。根据执行准确度，模型比基线模型高出 5.5%；类似地，基于 SELECT 和 WHERE 子句的准确度分别提高了
    1.3% 和 5.9%。在完全访问数据库的情况下，模型在执行准确度上达到了 82.6%，在内容感知系统上为 17.5%。结果已在表 [II](#S4.T2
    "TABLE II ‣ IV-B Slot-Filling Approach [39] ‣ IV Algorithmic explanation ‣ Deep
    Learning Driven Natural Languages Text to SQL Query Conversion: A Survey") 中总结。'
- en: IV-C Sequence-to-Tree model [[41](#bib.bib41)]
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 序列到树模型 [[41](#bib.bib41)]
- en: This model’s prime goal is that this approach’s output is returned in the form
    of a tree. Considering the intricacy of returning a visual tree and its limitations
    in parsing its structure, a pre-order traversal of the tree is returned. The model
    uses a neural semantic parsing method of Yu et al. [[17](#bib.bib17)] as the baseline
    model. CHISP [[41](#bib.bib41)], parses the input question by encoding in a LSTM
    sequence encoder. After parsing the input, the model understands the input by
    recognizing individual entities and classifies information based on three categories.
    As it is essential and adequate for a SQL query to have data for SELECT, WHERE,
    EXCEPT, the model follows a similar basket approach. Ultimately, the tree nodes
    comprise keyword nodes as described above and columns of tables such as Name,
    City, etc.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的主要目标是将这种方法的输出以树形结构的形式返回。考虑到返回视觉树的复杂性及其在解析结构时的局限性，返回了树的前序遍历。该模型使用了Yu等人的神经语义解析方法[[17](#bib.bib17)]作为基线模型。CHISP
    [[41](#bib.bib41)]通过在LSTM序列编码器中对输入问题进行编码来解析输入。在解析输入后，模型通过识别单独的实体来理解输入，并根据三类信息进行分类。由于SQL查询对于SELECT、WHERE、EXCEPT来说至关重要且足够，模型采用了类似的篮子方法。最终，树节点包括上述描述的关键词节点和表的列，如Name、City等。
- en: In addition to this, a stack is utilized for incremental decoding, as explained
    in section 2.2\. As log data is generated from previous accumulations of results,
    the model uses this data to decide the term that is supposed to be developed in
    the next iteration.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用堆栈进行增量解码，如第2.2节所述。由于日志数据是从先前结果的累积中生成的，模型使用这些数据来决定在下一次迭代中要开发的术语。
- en: Exacting match accuracy and component matching F1 are used for evaluation for
    SELECT, GROUP BY, WHERE, and ORDER BY.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SELECT、GROUP BY、WHERE和ORDER BY，采用精确匹配准确度和组件匹配F1进行评估。
- en: '|  | Easy | Medium | Hard | Extra Hard | All |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 简单 | 中等 | 难 | 特别难 | 全部 |'
- en: '| ENG | 31.8% | 11.3% | 9.5% | 2.7% | 14.1% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ENG | 31.8% | 11.3% | 9.5% | 2.7% | 14.1% |'
- en: '| HT | C-ML | 27.3% | 9.9% | 7.5% | 2.3% | 12.1% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| HT | C-ML | 27.3% | 9.9% | 7.5% | 2.3% | 12.1% |'
- en: '| C-S | 23.1% | 7.7% | 6.2% | 1.7% | 9.9% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| C-S | 23.1% | 7.7% | 6.2% | 1.7% | 9.9% |'
- en: '| WY-ML | 21.4% | 8.1% | 8.0% | 1.7% | 10.0% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| WY-ML | 21.4% | 8.1% | 8.0% | 1.7% | 10.0% |'
- en: '| WY-S | 20.2% | 6.4% | 6.7% | 2.0% | 8.9% |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| WY-S | 20.2% | 6.4% | 6.7% | 2.0% | 8.9% |'
- en: '| WJ-ML | 19.8% | 8.6% | 5.0% | 1.3% | 9.2% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| WJ-ML | 19.8% | 8.6% | 5.0% | 1.3% | 9.2% |'
- en: '| WJ-S | 20.1% | 5.0% | 5.7% | 1.7% | 8.2% |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| WJ-S | 20.1% | 5.0% | 5.7% | 1.7% | 8.2% |'
- en: '| MT | C-ML | 18.1% | 4.6% | 5.2% | 0.3% | 7.9% |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| MT | C-ML | 18.1% | 4.6% | 5.2% | 0.3% | 7.9% |'
- en: '| WY-ML | 17.9% | 4.7% | 4.5% | 0.3% | 7.6% |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| WY-ML | 17.9% | 4.7% | 4.5% | 0.3% | 7.6% |'
- en: 'TABLE III: Experimental Result of Sequence to Tree Model [[41](#bib.bib41)]'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：序列到树模型的实验结果[[41](#bib.bib41)]
- en: 'Summarizing of exact matching results has been depicted in table [III](#S4.T3
    "TABLE III ‣ IV-C Sequence-to-Tree model [41] ‣ IV Algorithmic explanation ‣ Deep
    Learning Driven Natural Languages Text to SQL Query Conversion: A Survey"). As
    it can be seen, The accuracy of each dataset under their respective categories
    has been populated.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '精确匹配结果的总结已在表[III](#S4.T3 "TABLE III ‣ IV-C Sequence-to-Tree model [41] ‣ IV
    Algorithmic explanation ‣ Deep Learning Driven Natural Languages Text to SQL Query
    Conversion: A Survey")中描绘。如图所示，每个数据集在各自类别下的准确率已经填充。'
- en: IV-D Zero-Shot-Semantic Parsing [[42](#bib.bib42)]
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 零样本语义解析[[42](#bib.bib42)]
- en: This particular text to SQL parser has been extended from its predecessor known
    as the encoder-decoder semantic parser [[42](#bib.bib42)]. In an encoder-decoder [[20](#bib.bib20)]
    semantic parser, the training data is initially passed through a graph neural
    network [[43](#bib.bib43)] to initialize the DB constants in a soft selection
    manner. After this, an auto-regressive model is utilized to imbibe the top-K queries
    and rank the list based on global matching. Global matching in this setting signifies
    the process of considering words in the corpus in correspondence to the database
    rather than just focusing attention on the target word. The approach utilized
    in this model plays a significant part in improving the efficiency of the model
    when compared to the baseline papers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特定的文本到SQL解析器是从其前身——编码器-解码器语义解析器[[42](#bib.bib42)]扩展而来的。在编码器-解码器[[20](#bib.bib20)]语义解析器中，训练数据最初通过图神经网络[[43](#bib.bib43)]以软选择方式初始化DB常量。之后，使用自回归模型来吸收前K个查询，并根据全局匹配对列表进行排序。在这种设置中，全局匹配意味着考虑语料库中的词语与数据库的对应关系，而不仅仅是关注目标词。该模型中使用的方法在提高模型效率方面发挥了重要作用，相比于基线论文有所改善。
- en: 'To delineate the zero-shot-semantic parsing, the corresponding datasets and
    equations are employed. The training set resembles the following structure $(x^{k},y^{k},S^{k})$
    where $x^{k},y^{k},S^{k}$ signifies a input question, translation to its corresponding
    SQL query, and the schema of the corresponding database. $S^{k}$ in this scenario
    includes three important parameters: the set of DB tables, a set of columns for
    each table, and a set of foreign key-primary key column pairs wherein a pair represents
    a relation from a foreign-key table to a primary-key table. The input question,
    $x^{i}$, is encoded using a Bi-LSTM [[40](#bib.bib40)] and the output query, $y^{i}$,
    is decoded at each step by decoding grammar using SQL grammar.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明零样本语义解析，采用了相应的数据集和方程。训练集的结构如下 $(x^{k},y^{k},S^{k})$，其中 $x^{k},y^{k},S^{k}$
    分别表示输入问题、其对应的 SQL 查询的翻译，以及相关数据库的模式。在这种情况下，$S^{k}$ 包含三个重要参数：DB 表的集合、每个表的列集合，以及外键-主键列对的集合，其中每对表示从外键表到主键表的关系。输入问题
    $x^{i}$ 通过 Bi-LSTM 编码[[40](#bib.bib40)]，输出查询 $y^{i}$ 在每一步通过使用 SQL 语法的解码语法进行解码。
- en: '|  | $s_{v}=\Sigma_{i}\alpha_{i}s_{link}(v,x_{i})$ |  | (1) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{v}=\Sigma_{i}\alpha_{i}s_{link}(v,x_{i})$ |  | (1) |'
- en: '|  | $output=softmax([s_{v}]_{v\in V})$ |  | (2) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $output=softmax([s_{v}]_{v\in V})$ |  | (2) |'
- en: '|  | $h_{v}^{(0)}=p_{v}\textperiodcentered r_{v}$ |  | (3) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{v}^{(0)}=p_{v}\textperiodcentered r_{v}$ |  | (3) |'
- en: 'To compute a DB constant, first attention scores [[44](#bib.bib44)] are calculated
    as $\alpha^{|x|}_{i=1}$. Sequel to it, the local similarity score $s_{link}$ is
    computed from an edit distance between the two inputs, the fraction of string
    overlap between them, and the learned embeddings [[45](#bib.bib45)] of the word
    and DB constant. Proceeding further, equation [1](#S4.E1 "In IV-D Zero-Shot-Semantic
    Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning Driven Natural Languages
    Text to SQL Query Conversion: A Survey") is employed to compute $s_{v}$, and equation
    [2](#S4.E2 "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey")
    is used to circumscribe it with a softmax layer to obtain the desired output.
    Upon continuous research, it was concluded that a representation $h_{v}$ associated
    with every DB constant was used in the paper [[42](#bib.bib42)]. This representation
    is used in the prescribed Graph Convolutional Network (GCN) [[46](#bib.bib46)]
    in a node-to-node manner. Considering the excessive number of nodes present in
    one GCN, a probability factor known as relevance probability, $p_{v}$, was computed
    for every node and served as a gate for input to the network. The mathematical
    representation of the aforementioned has been depicted in equation [4](#S4.E4
    "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey"). In addition,
    $p_{v}$ is derived based on local information rather than from a global perspective.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '要计算 DB 常量，首先计算注意力分数 [[44](#bib.bib44)]，其为 $\alpha^{|x|}_{i=1}$。随后，从两个输入之间的编辑距离、它们之间的字符串重叠率以及单词和
    DB 常量的学习嵌入 [[45](#bib.bib45)] 中计算局部相似度分数 $s_{link}$。进一步地，使用方程 [1](#S4.E1 "In IV-D
    Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning Driven
    Natural Languages Text to SQL Query Conversion: A Survey") 计算 $s_{v}$，并使用方程 [2](#S4.E2
    "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey") 通过 softmax 层对其进行限制以获得期望的输出。经过持续研究，得出结论，文献
    [[42](#bib.bib42)] 中使用了与每个 DB 常量相关的表示 $h_{v}$。该表示在规定的图卷积网络（GCN） [[46](#bib.bib46)]
    中以节点到节点的方式使用。考虑到一个 GCN 中存在大量节点，计算了每个节点的相关性概率因子 $p_{v}$，并作为输入网络的门控。上述数学表示在方程 [4](#S4.E4
    "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey") 中描绘。此外，$p_{v}$
    是基于局部信息而非全球视角推导的。'
- en: '|  | $g_{v}^{(0)}=FF([r_{v};h_{v};p_{v}])$ |  | (4) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $g_{v}^{(0)}=FF([r_{v};h_{v};p_{v}])$ |  | (4) |'
- en: '|  | $p_{global}=\sigma(FF(g_{v}^{(L)})$ |  | (5) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{global}=\sigma(FF(g_{v}^{(L)})$ |  | (5) |'
- en: '|  | $f_{U_{\hat{y}}}=f^{(L_{v_{global}})}$ |  | (6) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{U_{\hat{y}}}=f^{(L_{v_{global}})}$ |  | (6) |'
- en: 'As previously suggested, the approach adopted utilizes a global gating methodology
    to allot the context of the question in contrast to its contemporary baselines.
    A new node $v_{global}$ is added to the GCN to accommodate this. $p_{global}$
    is initialized randomly which is then evolved based on equation [4](#S4.E4 "In
    IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey") and equation
    [5](#S4.E5 "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey").
    The FF in the equations [4](#S4.E4 "In IV-D Zero-Shot-Semantic Parsing [42] ‣
    IV Algorithmic explanation ‣ Deep Learning Driven Natural Languages Text to SQL
    Query Conversion: A Survey") and [5](#S4.E5 "In IV-D Zero-Shot-Semantic Parsing
    [42] ‣ IV Algorithmic explanation ‣ Deep Learning Driven Natural Languages Text
    to SQL Query Conversion: A Survey") denotes the feedforward neural network and
    concatenation of the described columns. On a further note, to re-rank the desired
    output list, a quotidian approach is to use an autoregressive model to carry the
    job. The approach is set to train a separate discriminative model to account for
    the global superposition of words. Here, the sub-graph created by the node of
    interest $U_{\hat{y}}$, and the global node $v_{global}$ is fed as input according
    to the equation [6](#S4.E6 "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic
    explanation ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion:
    A Survey").'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，所采用的方法利用了全球门控方法论，以便将问题的上下文与其当前基线进行对比。为了容纳这一点，GCN中添加了一个新节点$v_{global}$。$p_{global}$被随机初始化，然后根据方程[4](#S4.E4
    "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey")和方程[5](#S4.E5
    "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey")进行演变。方程[4](#S4.E4
    "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey")和[5](#S4.E5 "In
    IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey")中的FF表示前馈神经网络和描述列的连接。进一步说，为了对期望的输出列表进行重新排序，日常方法是使用自回归模型来完成这一任务。该方法设置为训练一个单独的判别模型，以考虑词汇的全球叠加。在这里，由感兴趣的节点$U_{\hat{y}}$和全球节点$v_{global}$创建的子图作为输入，按照方程[6](#S4.E6
    "In IV-D Zero-Shot-Semantic Parsing [42] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey")进行处理。'
- en: The model attains an accuracy of 47.4%. The basis of experimentation is summed
    up by two broad domains when queries use a single table and multiple tables. Assuming
    a global gating increases the accuracy to 63.2% in contrast to its contemporaries.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的准确率达到47.4%。实验基础分为两大类领域，即查询使用单表和多表。假设全球门控将准确率提高到63.2%，与其同期方法相比。
- en: IV-E Zero-Shot-Semantic Parsing with induced dynamic gating [[47](#bib.bib47)]
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 零-shot-语义解析与诱导的动态门控[[47](#bib.bib47)]
- en: 'As an evolved version of the zero-shot-semantic parsing aforementioned, this
    approach inherits an additional trait known as dynamic gating to fill a new entity
    in the GCN (Graph Convolutional Network). As already specified, a typical zero-shot-semantic
    parser can be classified into three the NL decoder that maps questions to word
    embeddings, a schema encoder that builds a relation-aware entity representation
    for every entity in the schema. Finally, a grammar decoder generates an Abstract
    Syntax Tree (AST) [[48](#bib.bib48)] using an autoregressive LSTM model. There
    arise two particular cases when the AST tree is about to generate:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为上述零-shot-语义解析的进化版本，该方法继承了一个称为动态门控的额外特性，以在GCN（图卷积网络）中填充一个新实体。如前所述，典型的零-shot-语义解析器可以分为三个部分：将问题映射到词嵌入的NL解码器，构建一个关系感知的实体表示的模式编码器。最后，语法解码器使用自回归LSTM模型生成抽象语法树（AST）[[48](#bib.bib48)]。当AST树即将生成时，出现两个特定的情况：
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: to generate a new rule that is responsible for inducing the leftmost non-terminal
    node after each iteration
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成一个新规则，该规则负责在每次迭代后引入最左侧的非终结节点
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: to generate a DB schema according to a rule spawned by the above point in the
    previous iteration
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据上一轮迭代中上述点生成一个数据库模式
- en: 'There are two techniques been deciphered in the research study [[47](#bib.bib47)],
    which are as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究中解读了两种技术[[47](#bib.bib47)]，如下所示：
- en: •
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Schema Linking: Compiles the output of NL encoder and chooses an apt entity
    based on string matching and embedding-matching.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模式链接：汇编NL编码器的输出并根据字符串匹配和嵌入匹配选择合适的实体。
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Structural Linking: Surfs through the generated entities and finds a perfect
    fit for the entity under consideration.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结构链接：浏览生成的实体并找到与考虑中的实体完全匹配的实体。
- en: The whole structure of the aforementioned is already at play. The dynamic gating
    process assists in choosing between the two probabilities according to the situation.
    One denotes the likelihood of choosing Schema Linking, and the other represents
    the likelihood of selecting Structural Linking. The reason for a direct relation
    is because the required data lies with the desired entity and, in hindsight. Hence
    relying entirely on this parameter establishes control over the model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上述整个结构已经在运行中。动态门控过程根据情况帮助选择两个概率之间的一个。一个表示选择模式链接的可能性，另一个表示选择结构链接的可能性。直接关系的原因在于所需的数据在期望的实体中，回顾起来也是如此。因此，完全依赖这个参数可以控制模型。
- en: IV-F Zero-shot Text-to-SQL Learning with Auxiliary Task [[49](#bib.bib49)]
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 零-shot文本到SQL学习与辅助任务 [[49](#bib.bib49)]
- en: This work begins by diagnosing the bottleneck in the text-to-SQL job by introducing
    an entirely novel testing environment where it is discovered that current models
    have weak generalization capacity on data that has only been viewed a few times.
    The authors are encouraged by the initial study’s results to develop a simple
    but effective auxiliary task that acts as both a supporting model and a regularization
    term to force generation jobs to boost the generality of the models. On the overall
    dataset, this model outperforms a robust baseline coarse-to-fine model by more
    than 3% in absolute accuracy compared to the baseline model. Interesting to note
    is that the model described by the author outperformed the baseline on a zero-shot
    subset test of WikiSQL, exhibiting a 5 percent absolute accuracy improvement over
    the baseline and proving its improved ability to generalize.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作通过引入一种全新的测试环境来诊断文本到SQL任务中的瓶颈，发现当前模型在仅查看过几次的数据上具有较弱的泛化能力。初步研究的结果鼓励作者开发一种简单但有效的辅助任务，作为支持模型和正则化项，强制生成任务提升模型的泛化能力。在整体数据集上，该模型在绝对准确性上比鲁棒基线粗细模型高出超过3%，相较于基线模型的表现更为优越。值得注意的是，作者描述的模型在WikiSQL的零-shot子集测试中超越了基线，展示了5%的绝对准确性提升，证明了其改进的泛化能力。
- en: '![Refer to caption](img/9ae25e2d9ea49cad2ab3fac209aa7fd3.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9ae25e2d9ea49cad2ab3fac209aa7fd3.png)'
- en: 'Figure 3: Zero-shot Text-to-SQL Learning with Auxiliary Task [[49](#bib.bib49)]'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：零-shot文本到SQL学习与辅助任务 [[49](#bib.bib49)]
- en: IV-F1 Encoder
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-F1 编码器
- en: 'The encoder retrieves the question’s hidden representations and the schema,
    Hq, and Hc. For the final question and table schema representation, bi-attention
    is employed to strengthen the interaction between the question terms q and the
    column name c. The formula is:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器检索问题的隐藏表示和模式，Hq 和 Hc。为了最终的问题和表模式表示，采用双重注意力机制来增强问题术语q与列名c之间的交互。公式为：
- en: IV-F2 AGG and SEL Decoder
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-F2 AGG和SEL解码器
- en: 'AGG and SEL are generated using an attentive pooling layer over H q, combined
    with an attentive pooling layer to provide a final hidden representation q SEL.
    They input qSEL into the CLS layer, which generates the aggregation operation
    AGG, and They assess the similarity score between qSEL and each column name C
    j, which allows the PT layer to forecast SEL in the following ways:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: AGG和SEL是使用对H q的注意力池化层生成的，与注意力池化层结合以提供最终的隐藏表示q SEL。它们将qSEL输入到CLS层，该层生成聚合操作AGG，并评估qSEL与每个列名C
    j之间的相似度分数，从而允许PT层以以下方式预测SEL：
- en: IV-F3 WHERE Decoder
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-F3 WHERE 解码器
- en: To do this, They used the WHERE decoder from the current state-of-the-art model,
    which first creates a slot sketch of the WHERE clause before converting the SQL
    generation into a slot filling issue. WikiSQL has 35 categories of WHERE clauses,
    each of which is a subsequence of a WHERE clause that skips the COND COL and COND
    VAL. Examples include ”WHERE = AND ¿ ”, a sketch of a WHERE clause that consists
    of two criteria. They begin by predicting the drawing q based on H q where qWhere
    = [h¯q1, h¯q —Q—].
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，他们使用了当前最先进模型中的 WHERE 解码器，该解码器首先创建 WHERE 子句的槽草图，然后将 SQL 生成转换为槽填充问题。 WikiSQL
    有 35 种 WHERE 子句类别，每种子句都是一个跳过 COND COL 和 COND VAL 的 WHERE 子句的子序列。示例包括“WHERE = AND
    ¿”，这是一个包含两个标准的 WHERE 子句草图。他们首先基于 H q 预测绘图 q，其中 qWhere = [h¯q1, h¯q —Q—]。
- en: IV-F4 Auxiliary Mapping Model
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-F4 辅助映射模型
- en: The three components of a SQL query condition are COND COL, COND OP, and COND
    VAL. Each state is composed of three parts. Its mapping model aims to figure out
    how to transfer the condition column to the condition value in a given situation.
    Hq and Hc are two representations of the question and table schema shared with
    the generation model. An intuitive mapping method is to immediately learn a mapping
    function from each word in question to the names of the columns in question. A
    two-step mapping model is proposed to handle this issue. First, a detector to
    filter out condition values is learned; then, a mapping function from condition
    values to column names is learned.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 查询条件的三个组成部分是 COND COL、COND OP 和 COND VAL。每个状态由三个部分组成。其映射模型旨在弄清楚如何在给定情况下将条件列转换为条件值。Hq
    和 Hc 是与生成模型共享的问题和表格模式的两个表示。一个直观的映射方法是立即从问题中的每个词到列名学习一个映射函数。提出了一种两步映射模型来解决这个问题。首先，学习一个过滤条件值的检测器；然后，学习一个从条件值到列名的映射函数。
- en: IV-G Air-Concierge [[50](#bib.bib50)]
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-G Air-Concierge [[50](#bib.bib50)]
- en: AirConcierge, an end-to-end trainable text-to-SQL guided framework to learn
    a neural agent that interacts with KBs using the generated SQL queries. Specifically,
    the neural agent first learns to ask and confirm the customer’s intent during
    the multi-turn interactions, then dynamically determining when to ground the user
    constraints into executable SQL queries to fetch relevant information from KBs.
    AirConcierge system addresses the following challenges in developing an effective
    task-oriented dialogue system, including • When should the system access the KBs
    to obtain task-relevant information during a conversation? • How does the system
    formulate a query that retrieves task-relevant data from the KBs
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: AirConcierge 是一个端到端可训练的文本到 SQL 引导框架，用于学习一个与知识库（KBs）交互的神经代理，该代理通过生成的 SQL 查询进行交互。具体来说，神经代理首先在多轮交互中学习询问并确认客户的意图，然后动态确定何时将用户约束转换为可执行的
    SQL 查询，以从知识库中获取相关信息。AirConcierge 系统解决了开发有效的任务导向对话系统中的以下挑战，包括 • 系统在对话过程中应何时访问知识库以获取与任务相关的信息？
    • 系统如何制定查询以从知识库中检索与任务相关的数据
- en: IV-G1 Dialogue Encoder
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-G1 对话编码器
- en: '|  | $h_{t}^{e}=GRU(W_{emb}(x_{t}-1),h_{t-1}^{e})$ |  | (7) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{t}^{e}=GRU(W_{emb}(x_{t}-1),h_{t-1}^{e})$ |  | (7) |'
- en: 'The Dialogue Encoder is encoded using a Recurrent Neural Network (RNN) [[51](#bib.bib51),
    [52](#bib.bib52)]. Subsequent to embedding by the $W_{emb}$ matrix, equation [7](#S4.E7
    "In IV-G1 Dialogue Encoder ‣ IV-G Air-Concierge [50] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey")
    is used to model the sequence of input conversation history X = {x_1, x_2 , x_3,
    …x_t}'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对话编码器使用递归神经网络（RNN）进行编码 [[51](#bib.bib51), [52](#bib.bib52)]。在通过 $W_{emb}$ 矩阵进行嵌入后，使用方程
    [7](#S4.E7 "在 IV-G1 对话编码器 ‣ IV-G Air-Concierge [50] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到
    SQL 查询转换：一项调查") 来建模输入对话历史 X = {x_1, x_2 , x_3, …x_t}
- en: IV-G2 Dialogue State Tracker
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-G2 对话状态追踪器
- en: '|  | $P(s&#124;h_{T}^{e},x_{1:J}^{col})=\sigma(W_{2}^{s}(W_{1}^{s}h_{T}^{e}+\Sigma
    U_{2}W_{emb}(x_{1:J}^{col})))$ |  | (8) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(s&#124;h_{T}^{e},x_{1:J}^{col})=\sigma(W_{2}^{s}(W_{1}^{s}h_{T}^{e}+\Sigma
    U_{2}W_{emb}(x_{1:J}^{col})))$ |  | (8) |'
- en: Upon multiple iterations performed, there arises a situation where the model
    needs to determine if the data available with it is enough for the further steps
    or not. In detail, the system switches from the ”greeting state” to the ”problem-solving
    state”. The dialogue state tracker is utilized to facilitate this task, wherein
    the schema from the database is taken as input data. Specifically, it takes the
    parameter as input and returns a binary value between 0 to 1 using bidirectional
    LSTM, with a fully-connected layers and a sigmoid function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行多次迭代后，出现了一种情况，即模型需要确定其手头的数据是否足够用于进一步的步骤。具体而言，系统从“问候状态”切换到“问题解决状态”。对话状态跟踪器被用来促进这一任务，其中数据库中的模式作为输入数据。具体来说，它将参数作为输入，并使用双向
    LSTM、全连接层和 sigmoid 函数返回 0 到 1 之间的二进制值。
- en: IV-G3 SQL Generator
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-G3 SQL 生成器
- en: '|  | $P_{col}(x_{j}^{col}&#124;h_{j}^{col},h_{T}^{e})=\sigma(W_{1}^{col}h_{j}^{col}+W_{2}^{col}h_{T}^{e})$
    |  | (9) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{col}(x_{j}^{col}&#124;h_{j}^{col},h_{T}^{e})=\sigma(W_{1}^{col}h_{j}^{col}+W_{2}^{col}h_{T}^{e})$
    |  | (9) |'
- en: '|  | $P_{op}(x_{j}^{op}&#124;h_{j}^{col},h_{T}^{e})=\sigma(W_{1}^{op}h_{j}^{col}+W_{2}^{op}h_{T}^{e})$
    |  | (10) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{op}(x_{j}^{op}&#124;h_{j}^{col},h_{T}^{e})=\sigma(W_{1}^{op}h_{j}^{col}+W_{2}^{op}h_{T}^{e})$
    |  | (10) |'
- en: '|  | $P_{val}(v_{i}^{j}&#124;h_{j}^{col},h_{T}^{e})=Softmax(W_{1}^{val}(W_{2}^{val}h_{T}^{e}+W_{3}^{val}h_{j}^{col}))$
    |  | (11) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{val}(v_{i}^{j}&#124;h_{j}^{col},h_{T}^{e})=Softmax(W_{1}^{val}(W_{2}^{val}h_{T}^{e}+W_{3}^{val}h_{j}^{col}))$
    |  | (11) |'
- en: 'Once the model switches to the ”problem-solving state”, the SQL generator fires
    up to predict the values of three broad clauses, the COLUMN NAME, OPERATOR, and
    VALUE in the desired column in concordance with a SQL query. Equations [9](#S4.E9
    "In IV-G3 SQL Generator ‣ IV-G Air-Concierge [50] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey"),
    [10](#S4.E10 "In IV-G3 SQL Generator ‣ IV-G Air-Concierge [50] ‣ IV Algorithmic
    explanation ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion:
    A Survey"), and [11](#S4.E11 "In IV-G3 SQL Generator ‣ IV-G Air-Concierge [50]
    ‣ IV Algorithmic explanation ‣ Deep Learning Driven Natural Languages Text to
    SQL Query Conversion: A Survey") are used respectively to compute the values aforementioned.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型切换到“问题解决状态”，SQL 生成器将启动，预测所需列中的三个主要子句的值，即列名、操作符和值，以符合 SQL 查询。方程 [9](#S4.E9
    "在 IV-G3 SQL 生成器 ‣ IV-G Air-Concierge [50] ‣ IV 算法说明 ‣ 基于深度学习的自然语言文本到 SQL 查询转换：综述")、[10](#S4.E10
    "在 IV-G3 SQL 生成器 ‣ IV-G Air-Concierge [50] ‣ IV 算法说明 ‣ 基于深度学习的自然语言文本到 SQL 查询转换：综述")
    和 [11](#S4.E11 "在 IV-G3 SQL 生成器 ‣ IV-G Air-Concierge [50] ‣ IV 算法说明 ‣ 基于深度学习的自然语言文本到
    SQL 查询转换：综述") 分别用于计算上述值。
- en: IV-G4 Knowledge base memory encoder
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-G4 知识库记忆编码器
- en: '|  | $c_{i}^{k}=B(C^{k}(f_{i}))$ |  | (12) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{i}^{k}=B(C^{k}(f_{i}))$ |  | (12) |'
- en: '|  | $p_{i}^{k}=Softmax((q^{k})^{T}c_{i}^{k})$ |  | (13) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}^{k}=Softmax((q^{k})^{T}c_{i}^{k})$ |  | (13) |'
- en: '|  | $o^{k}=\sum_{i=1}^{F}p_{i}^{k}c_{i}^{k+1}$ |  | (14) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $o^{k}=\sum_{i=1}^{F}p_{i}^{k}c_{i}^{k+1}$ |  | (14) |'
- en: '|  | $q^{k+1}=q^{k}+o^{k}$ |  | (15) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $q^{k+1}=q^{k}+o^{k}$ |  | (15) |'
- en: '|  | $g_{i}^{K}=Softmax((q^{K})^{T}c_{i}^{K})$ |  | (16) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $g_{i}^{K}=Softmax((q^{K})^{T}c_{i}^{K})$ |  | (16) |'
- en: 'In addition to encoding the retrieved data from the existing Knowledge base
    (KB), the knowledge base memory encoder filters out irrelevant data from the KBs.
    With the help of $C=\{C^{1},C^{2},....C^{K+1}\}$, the trainable embedding matrices,
    the data retrieved from KBs are converted to memory vectors $\{m_{1},m_{2}....m_{F}\}$
    where K is the number of hops. Simultaneously a vector qk̂, known as the attention
    weights, is computed for each memory vector m_i. With the help of equations [12](#S4.E12
    "In IV-G4 Knowledge base memory encoder ‣ IV-G Air-Concierge [50] ‣ IV Algorithmic
    explanation ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion:
    A Survey") and [13](#S4.E13 "In IV-G4 Knowledge base memory encoder ‣ IV-G Air-Concierge
    [50] ‣ IV Algorithmic explanation ‣ Deep Learning Driven Natural Languages Text
    to SQL Query Conversion: A Survey"), each memory vector’s probability of higher
    relevance is calculated.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对从现有知识库 (KB) 中检索到的数据进行编码外，知识库记忆编码器还过滤掉 KB 中的不相关数据。在 $C=\{C^{1},C^{2},....C^{K+1}\}$
    的帮助下，经过训练的嵌入矩阵将从 KB 中检索的数据转换为记忆向量 $\{m_{1},m_{2}....m_{F}\}$，其中 K 是跳数。同时，为每个记忆向量
    $m_i$ 计算一个向量 $qk̂$，称为注意力权重。利用方程 [12](#S4.E12 "在 IV-G4 知识库记忆编码器 ‣ IV-G Air-Concierge
    [50] ‣ IV 算法说明 ‣ 基于深度学习的自然语言文本到 SQL 查询转换：综述") 和 [13](#S4.E13 "在 IV-G4 知识库记忆编码器
    ‣ IV-G Air-Concierge [50] ‣ IV 算法说明 ‣ 基于深度学习的自然语言文本到 SQL 查询转换：综述")，计算每个记忆向量的更高相关性的概率。
- en: 'Further, a parameter ok̂ is calculated by equation [14](#S4.E14 "In IV-G4 Knowledge
    base memory encoder ‣ IV-G Air-Concierge [50] ‣ IV Algorithmic explanation ‣ Deep
    Learning Driven Natural Languages Text to SQL Query Conversion: A Survey") which
    is used to update qK̂ that can be visualized in equation [15](#S4.E15 "In IV-G4
    Knowledge base memory encoder ‣ IV-G Air-Concierge [50] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey").
    Finally, the vector/pointer G = (g_1, g_2,…g_F) is used to pick out the most relevant
    data points amongst the heap and filter out non-essential data points. Individual
    entities of the pointer G can be calculated with the help of equation [16](#S4.E16
    "In IV-G4 Knowledge base memory encoder ‣ IV-G Air-Concierge [50] ‣ IV Algorithmic
    explanation ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion:
    A Survey").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过方程 [14](#S4.E14 "在 IV-G4 知识库记忆编码器 ‣ IV-G Air-Concierge [50] ‣ IV 算法解释 ‣
    深度学习驱动的自然语言文本到 SQL 查询转换：综述") 计算的参数 ok̂ 用于更新 qK̂，可以在方程 [15](#S4.E15 "在 IV-G4 知识库记忆编码器
    ‣ IV-G Air-Concierge [50] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述") 中可视化。最后，向量/指针
    G = (g_1, g_2,…g_F) 用于在堆中挑选出最相关的数据点，并过滤掉非必要的数据点。指针 G 的各个实体可以借助方程 [16](#S4.E16
    "在 IV-G4 知识库记忆编码器 ‣ IV-G Air-Concierge [50] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述")
    进行计算。
- en: IV-G5 Dialogue decoder
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-G5 对话解码器
- en: '|  | $h_{t}^{d}=GRU(W_{emb}(\hat{y_{t}}-1),h_{t-1}^{d})$ |  | (17) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{t}^{d}=GRU(W_{emb}(\hat{y_{t}}-1),h_{t-1}^{d})$ |  | (17) |'
- en: '|  | $P(\hat{y_{t}})=Softmax((q^{K})^{T}c_{i}^{K})$ |  | (18) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(\hat{y_{t}})=Softmax((q^{K})^{T}c_{i}^{K})$ |  | (18) |'
- en: 'A dialogue decoder is utilized to decipher the agent’s output at every step.
    This is accomplished with the help of a GRU model for a seamless output. Equations
    [17](#S4.E17 "In IV-G5 Dialogue decoder ‣ IV-G Air-Concierge [50] ‣ IV Algorithmic
    explanation ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion:
    A Survey") and [18](#S4.E18 "In IV-G5 Dialogue decoder ‣ IV-G Air-Concierge [50]
    ‣ IV Algorithmic explanation ‣ Deep Learning Driven Natural Languages Text to
    SQL Query Conversion: A Survey") are responsible for the same.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对话解码器用于解密每一步的代理输出。这是通过使用 GRU 模型来实现的，从而实现无缝的输出。方程 [17](#S4.E17 "在 IV-G5 对话解码器
    ‣ IV-G Air-Concierge [50] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述") 和 [18](#S4.E18
    "在 IV-G5 对话解码器 ‣ IV-G Air-Concierge [50] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述")
    负责相同的任务。
- en: IV-H Machine Reading Comprehension model [[53](#bib.bib53)]
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-H 机器阅读理解模型 [[53](#bib.bib53)]
- en: The MRC (Machine Reading Comprehension) model [[53](#bib.bib53)] is based on
    the BERT-based MRC model that allows efficient conversion of text span in the
    model. It formulates the task as a question-answering problem. A unified MRC model
    is used to predict different slots with intermediate training on the dataset,
    achieving comparable performance on WikiSQL base state of the art.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MRC（机器阅读理解）模型 [[53](#bib.bib53)] 基于 BERT 模型的 MRC 模型，允许模型中文本跨度的高效转换。它将任务表述为问答问题。统一的
    MRC 模型用于预测不同的槽位，通过对数据集的中间训练，实现了与 WikiSQL 基础状态的艺术水平相当的性能。
- en: '|  | $H^{Q},H^{C}=BERT([Q,C])$ |  | (19) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $H^{Q},H^{C}=BERT([Q,C])$ |  | (19) |'
- en: '|  | $p_{start}(i)=softmax(H_{i}^{C}v_{start})$ |  | (20) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{start}(i)=softmax(H_{i}^{C}v_{start})$ |  | (20) |'
- en: '|  | $p_{end}(i)=softmax(H_{i}^{C}v_{end})$ |  | (21) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{end}(i)=softmax(H_{i}^{C}v_{end})$ |  | (21) |'
- en: Initially, the question parsed to the model is denoted by Q, and the output/answer
    is concatenated to generate the training data. [CLS], q_1, q_2, .., q_L, [SEP],
    c_1, c_2, …, c_M would be an apt representation in a mathematical form, where
    [CLS] signifies the start and [SEP] is a unique character inserted to differentiate
    the question and the corresponding answer. As a result, the model outputs the
    matrix H. In this trial, two additional parameters, v _start and v_end wrapped
    under a softmax layer are used to compute the probability of the start and end
    token positions, respectively. The above model is suitable for predicting only
    one set of WHERE, OPERATOR, and VALUE. To overcome this problem, an alternate
    approach is enforced.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，解析到模型的问题用 Q 表示，输出/答案被连接生成训练数据。[CLS]、q_1、q_2、..、q_L、[SEP]、c_1、c_2、…、c_M 将是一个适当的数学表示，其中
    [CLS] 表示开始，[SEP] 是一个独特的字符，用于区分问题和对应的答案。因此，模型输出矩阵 H。在此试验中，使用两个额外的参数 v_start 和 v_end，通过
    softmax 层来计算开始和结束标记位置的概率。上述模型适用于仅预测一组 WHERE、OPERATOR 和 VALUE。为了解决这个问题，采用了另一种方法。
- en: '|  | $T=CRF(W^{L}H^{C}),&#124;T&#124;=&#124;C&#124;$ |  | (22) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $T=CRF(W^{L}H^{C}),&#124;T&#124;=&#124;C&#124;$ |  | (22) |'
- en: 'Sequence labeling using a BIO tag set is used for multiple sets of predictions.
    In this scenario, each token in the context, HĈ, is fed to a conditional random
    field to yield output labels T, as specified in equation [22](#S4.E22 "In IV-H
    Machine Reading Comprehension model [53] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey"). This approach
    is similar to an MRC-based entity-relation extraction and is under development.
    To enhance the efficiency of this study, an experimental approach was employed.
    The method known as STILTs involves utilizing a pre-trained model for intermediate
    tasks before the final deployed model.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BIO 标签集的序列标注用于多组预测。在这种情况下，背景中的每个标记 HĈ 被输入到条件随机场中，以产生输出标签 T，如公式 [22](#S4.E22
    "在 IV-H 机器阅读理解模型 [53] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述") 所指定。这种方法类似于基于 MRC
    的实体关系提取，正在开发中。为了提高研究的效率，采用了实验方法。称为 STILTs 的方法涉及利用预训练模型进行中间任务，然后再部署最终模型。
- en: IV-I Seq2SQL [[54](#bib.bib54)]
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-I Seq2SQL [[54](#bib.bib54)]
- en: It is a deep neural network for translating questions about a natural language
    into their respective queries in the form of Structured Query Language. The developed
    model uses the formation of queries in Structured Query Language to reduce the
    output space concerning the generated questions remarkably. It is trained using
    a mixed objective, combining cross-entropy losses and RL rewards from in-the-loop
    query execution on a database. Seq2SQL outperforms a previously state-of-the-art
    semantic parsing model on the WikiSQL benchmark dataset. It achieves 59.4% execution
    accuracy compared to the previous 35.9% and 53.3% execution accuracy.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个深度神经网络，用于将自然语言中的问题翻译成结构化查询语言形式的相应查询。开发的模型通过结构化查询语言的查询形成显著减少了生成问题的输出空间。它使用混合目标进行训练，结合了交叉熵损失和来自数据库上循环查询执行的强化学习奖励。Seq2SQL
    在 WikiSQL 基准数据集上超越了先前最先进的语义解析模型。它的执行准确率达到了 59.4%，相比之前的 35.9% 和 53.3% 执行准确率。
- en: Concerning the model Sequence to SQL [[55](#bib.bib55)], the model is segregated
    into three individual aspects. The AGGREGATION operator, the SELECT column, and
    the WHERE clause
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型 Sequence to SQL [[55](#bib.bib55)]，该模型被分为三个独立的方面。AGGREGATION 操作符、SELECT
    列和 WHERE 子句。
- en: IV-I1 AGGREGATION
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-I1 聚合
- en: '|  | $\alpha_{t}^{inp}=W^{inp}h_{t}^{enc}$ |  | (23) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{t}^{inp}=W^{inp}h_{t}^{enc}$ |  | (23) |'
- en: '|  | $\alpha^{inp}=[\alpha_{1}^{inp},\alpha_{2}^{inp},...]$ |  | (24) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha^{inp}=[\alpha_{1}^{inp},\alpha_{2}^{inp},...]$ |  | (24) |'
- en: '|  | $\beta^{inp}=softmax(\alpha^{inp})$ |  | (25) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta^{inp}=softmax(\alpha^{inp})$ |  | (25) |'
- en: '|  | $k^{agg}=\sum_{t}\beta_{t}^{inp}h_{t}^{enc}$ |  | (26) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $k^{agg}=\sum_{t}\beta_{t}^{inp}h_{t}^{enc}$ |  | (26) |'
- en: '|  | $\alpha^{agg}=W^{agg}tanh(V^{agg}k^{agg}+b^{agg})+c^{agg}$ |  | (27) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha^{agg}=W^{agg}tanh(V^{agg}k^{agg}+b^{agg})+c^{agg}$ |  | (27) |'
- en: '|  | $\beta^{agg}=softmax(\alpha^{agg})$ |  | (28) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta^{agg}=softmax(\alpha^{agg})$ |  | (28) |'
- en: 'Initially, the scalar attention scores are calculated according to equation
    [23](#S4.E23 "In IV-I1 AGGREGATION ‣ IV-I Seq2SQL [54] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey")
    which is further normalized as depicted in equation [24](#S4.E24 "In IV-I1 AGGREGATION
    ‣ IV-I Seq2SQL [54] ‣ IV Algorithmic explanation ‣ Deep Learning Driven Natural
    Languages Text to SQL Query Conversion: A Survey"). The individual scores for
    the aggregation operators, COUNT, MIN, MAX and NULL can be calculated from equations
    [25](#S4.E25 "In IV-I1 AGGREGATION ‣ IV-I Seq2SQL [54] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey"),
    [26](#S4.E26 "In IV-I1 AGGREGATION ‣ IV-I Seq2SQL [54] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey"),
    and [27](#S4.E27 "In IV-I1 AGGREGATION ‣ IV-I Seq2SQL [54] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey").
    Finally, a softmax layer is applied over the result to obtain the distribution
    over the possible aggregation operations.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，标量注意力分数是根据方程[23](#S4.E23 "在 IV-I1 聚合 ‣ IV-I Seq2SQL [54] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到
    SQL 查询转换：综述")计算的，随后如方程[24](#S4.E24 "在 IV-I1 聚合 ‣ IV-I Seq2SQL [54] ‣ IV 算法解释 ‣
    深度学习驱动的自然语言文本到 SQL 查询转换：综述")所示进行归一化。聚合操作符 COUNT、MIN、MAX 和 NULL 的个别分数可以从方程[25](#S4.E25
    "在 IV-I1 聚合 ‣ IV-I Seq2SQL [54] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述")、[26](#S4.E26
    "在 IV-I1 聚合 ‣ IV-I Seq2SQL [54] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述")和[27](#S4.E27
    "在 IV-I1 聚合 ‣ IV-I Seq2SQL [54] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述")计算得出。最后，将
    softmax 层应用于结果，以获得可能聚合操作的分布。
- en: IV-I2 SELECT
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-I2 选择
- en: '|  | $h_{j,t}^{c}=LSTM(emb(x_{j,t}^{c}),h_{j,t-1}^{c})$ |  | (29) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{j,t}^{c}=LSTM(emb(x_{j,t}^{c}),h_{j,t-1}^{c})$ |  | (29) |'
- en: '|  | $e_{j}^{c}=h_{j,T_{j}}^{c}$ |  | (30) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{j}^{c}=h_{j,T_{j}}^{c}$ |  | (30) |'
- en: '|  | $\alpha_{j}^{sel}=W^{sel}tanh(V^{sel}k^{sel}+V^{c}e_{j}^{c})$ |  | (31)
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{j}^{sel}=W^{sel}tanh(V^{sel}k^{sel}+V^{c}e_{j}^{c})$ |  | (31)
    |'
- en: 'Each column names are encoded with a LSTM as shown in equation [29](#S4.E29
    "In IV-I2 SELECT ‣ IV-I Seq2SQL [54] ‣ IV Algorithmic explanation ‣ Deep Learning
    Driven Natural Languages Text to SQL Query Conversion: A Survey"), where hĉ_j,t
    denotes the tt̂h encoder state of the jt̂h column. Another representation kŝel
    is fabricated using united weights, which is further used to wrap over a multi-layer
    perceptron over the column representations, to compute a score as shown in equation
    [30](#S4.E30 "In IV-I2 SELECT ‣ IV-I Seq2SQL [54] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey")
    and [31](#S4.E31 "In IV-I2 SELECT ‣ IV-I Seq2SQL [54] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey").
    Finally, a softmax layer is applied over the output Bŝel to normalize the scores.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 每列名称通过 LSTM 编码，如方程[29](#S4.E29 "在 IV-I2 选择 ‣ IV-I Seq2SQL [54] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到
    SQL 查询转换：综述")所示，其中 hĉ_j,t 表示第 jt̂ 列的 tt̂h 编码器状态。另一个表示 kŝel 是使用联合权重构造的，进一步用于对列表示进行多层感知器计算分数，如方程[30](#S4.E30
    "在 IV-I2 选择 ‣ IV-I Seq2SQL [54] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述")和[31](#S4.E31
    "在 IV-I2 选择 ‣ IV-I Seq2SQL [54] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到 SQL 查询转换：综述")所示。最后，将
    softmax 层应用于输出 Bŝel 以归一化分数。
- en: IV-I3 WHERE
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-I3 WHERE
- en: To avoid the mismatch of queries, reinforcement learning is utilized to learn
    a policy to optimize and minimize the error produced. If an invalid query is parsed,
    the score is -2, whereas if the query is valid, but the query executes to an incorrect
    result, then the score is -1, and finally, for the perfect result, where the query
    parsed is right as well as the corresponding correct output, then the score would
    turn out to be +1\. Simultaneously a loss function is declared to calculate to
    optimize the expected reward.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免查询不匹配，利用强化学习学习策略以优化和最小化产生的错误。如果解析了无效查询，分数为 -2，而如果查询有效但执行结果不正确，则分数为 -1，最后，对于完美结果，即解析的查询和对应的正确输出都正确时，分数将为
    +1。与此同时，声明一个损失函数来计算和优化期望奖励。
- en: IV-J IRNET [[56](#bib.bib56)]
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-J IRNET [[56](#bib.bib56)]
- en: 'IRNet is a neural network technique for text-to-SQL conversion that is complicated
    and cross-domain. IRNet solves two challenges: 1) the misalignment between the
    intents represented in natural language (NL) and the implementation details in
    SQL, and 2) the difficulty in anticipating columns due to the massive number of
    out-of-domain terms in the natural language. Then, using a grammar-based neural
    network, IRNet synthesizes a SemQL query, an intermediate form that They created
    to bridge the gap between NL and SQL queries. Regarding the difficult Text-to-SQL
    test Spider, IRNet scores 46.7 percent accuracy, which is a 19.5 percent absolute
    gain over the previously available state-of-the-art methods.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: IRNet 是一种复杂的跨领域文本到 SQL 转换的神经网络技术。IRNet 解决了两个挑战：1) 自然语言（NL）中表示的意图与 SQL 中的实现细节之间的不匹配，以及
    2) 由于自然语言中大量的领域外术语，预测列的难度。然后，使用基于语法的神经网络，IRNet 合成了一个 SemQL 查询，这是他们创建的中间形式，用于弥合
    NL 和 SQL 查询之间的差距。关于困难的文本到 SQL 测试 Spider，IRNet 的准确率为 46.7%，比之前的最先进方法高出 19.5%。
- en: IV-J1 Intermediate Representation
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-J1 中间表示
- en: The solution provided by the authors is to create a domain-specific language
    called SemQL that acts as an intermediary representation between NL and SQL, thereby
    eliminating the misalignment. Whenever a SQL query is inferred from a SemQL query,
    an assumption is made that the specification of a database schema is exact and
    comprehensive. As an illustration, consider the inference of the FROM clause in
    a SQL query. The algorithm begins by determining the shortest path between all
    the stated tables in a SemQL query in the schema.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的解决方案是创建一种名为 SemQL 的领域特定语言，它充当 NL 和 SQL 之间的中间表示，从而消除不匹配。当从 SemQL 查询推断出 SQL
    查询时，假设数据库模式的规范是准确和全面的。作为例子，考虑 SQL 查询中 FROM 子句的推断。算法首先确定 SemQL 查询中所有陈述的表之间的最短路径。
- en: IV-J2 Schema Linking
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-J2 模式链接
- en: The purpose of schema linking in IRNet is to detect the columns and tables referenced
    in a question and assign various types to the columns based on their placement
    in the question. Schema linking is an instance of entity linking in the context
    of Text-to-SQL, where entity refers to columns, tables, and cell values in a database.
    Schema Linking starts by identifying and recognizing all of the entities stated
    in a question and then creating a non-overlap n-gram sequence of the question
    by combining those identified n-grams with the remaining 1-grams Each n-gram in
    the sequence is referred to as a span, and each span is assigned a type based
    on the thing it represents.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: IRNet 中的模式链接旨在检测问题中引用的列和表，并根据列在问题中的位置为这些列分配不同的类型。模式链接是在文本到 SQL 的背景下进行实体链接的一个实例，其中实体指的是数据库中的列、表和单元格值。模式链接首先识别和识别问题中陈述的所有实体，然后通过将这些识别出的
    n-gram 与剩余的 1-gram 结合，创建一个非重叠的 n-gram 序列。序列中的每个 n-gram 称为跨度，每个跨度根据它表示的内容分配一个类型。
- en: '![Refer to caption](img/a3e154b79f5e6367b71e958895c1f767.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a3e154b79f5e6367b71e958895c1f767.png)'
- en: 'Figure 4: IRNET [[56](#bib.bib56)]'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: IRNET [[56](#bib.bib56)]'
- en: IV-J3 NL Encoder
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-J3 NL 编码器
- en: The NL encoder accepts an input of x and encodes it into a sequence of hidden
    states denoted by the letter Hx. It is transformed into an embedding vector for
    each word in xi, and each word’s type (in xi) is also made into an embedding vector.
    Then, as the span embedding e I x, the NL encoder takes the average of the type
    and word embeddings and uses it as the span embedding. Finally, the NL encoder
    applies a bi-directional LSTM to all the span embeddings. When the forward and
    backward LSTMs are finished, the hidden output states are concatenated to form
    Hx.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: NL 编码器接受输入 x，并将其编码为由字母 Hx 表示的隐藏状态序列。它被转换为 xi 中每个词的嵌入向量，每个词的类型（在 xi 中）也被转换为嵌入向量。然后，作为跨度嵌入
    e I x，NL 编码器取类型和词嵌入的平均值，并将其用作跨度嵌入。最后，NL 编码器对所有跨度嵌入应用双向 LSTM。当前向和反向 LSTM 完成后，隐藏输出状态被连接以形成
    Hx。
- en: IV-J4 Schema Encoder
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-J4 模式编码器
- en: Input data is sent to the schema encoder, producing output representing columns
    Ec and tables, Et. Except for the fact that They do not provide a type to a table
    during schema linking, the generation of table representations is identical. Each
    word in ci is first turned into its embedding vector, then type I is translated
    into an embedding vector of its own, as shown in the following diagram. Next,
    the schema encoder uses the mean of all word embeddings as the starting point
    for the column’s initial representations (e). A second step is taken by the schema
    encoder, which is attention over the span embeddings, which results in the generation
    of a context vector called cic. Last, the schema encoder computes the column representation
    eic by adding together the initial embedding, the context vector, and the type
    embedding. The representations for column ci are calculated in the following manner.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据被发送到模式编码器，生成表示列Ec和表Et的输出。除了在模式链接过程中不为表提供类型外，表表示的生成是相同的。ci中的每个单词首先被转换为其嵌入向量，然后类型I被翻译为其自身的嵌入向量，如下图所示。接下来，模式编码器使用所有单词嵌入的平均值作为列初始表示(e)的起点。模式编码器采取的第二步是对跨度嵌入进行注意，这会生成一个称为cic的上下文向量。最后，模式编码器通过将初始嵌入、上下文向量和类型嵌入相加来计算列表示eic。列ci的表示计算如下。
- en: IV-J5 Decoder
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-J5 解码器
- en: The decoder’s primary objective is synthesizing SemQL queries from their constituent
    parts. To describe the creation process of a SemQL query through sequential applications
    of actions, they use a grammar-based decoder that utilizes an LSTM and is based
    on the tree structure of SemQL. It is possible to formalize the generating process
    of a SemQL query y in the following manner. Where ai is an action performed at
    time step i. a¡i is the sequence of actions performed before I, and t is the total
    number of time steps spent performing the whole action sequence.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的主要目标是从其组成部分合成SemQL查询。为了描述通过顺序应用操作创建SemQL查询的过程，他们使用了一种基于语法的解码器，该解码器利用LSTM，并基于SemQL的树结构。可以用以下方式形式化SemQL查询y的生成过程。其中ai是在时间步i执行的操作，a¡i是在I之前执行的操作序列，t是执行整个操作序列所花费的时间步数。
- en: IV-K Model Based Interactive Semantic Parsing [[57](#bib.bib57)]
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-K 基于模型的互动语义解析 [[57](#bib.bib57)]
- en: A model-based intelligent agent has been proposed in this paper in which an
    agent takes percept as the world model. In this, the parser decides when and where
    the interactive input is needed by adding an explanation for the same. They demonstrate
    two text-to-SQL datasets, which are WikiSQL and Spider. Despite lesser use intervention,
    this model scores a higher accuracy.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种基于模型的智能代理，其中代理将感知视为世界模型。在此模型中，解析器通过添加解释来决定何时以及在哪里需要交互输入。他们展示了两个文本到SQL的数据集，即WikiSQL和Spider。尽管使用干预较少，但该模型的准确率更高。
- en: '![Refer to caption](img/2d04847fa3c714d0bc4b4180efc20524.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2d04847fa3c714d0bc4b4180efc20524.png)'
- en: 'Figure 5: Model Based Interactive Semantic Parsing [[57](#bib.bib57)]'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '图5: 基于模型的互动语义解析 [[57](#bib.bib57)]'
- en: IV-K1 Agent State
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-K1 代理状态
- en: Agent state st is defined as a partial SQL query, for instance, st=o1, o2, …,
    ot, where it is the predicted SQL component at time step t, such as SELECT place.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 代理状态st被定义为部分SQL查询，例如st=o1, o2, …, ot，其中it是时间步t的预测SQL组件，如SELECT place。
- en: IV-K2 Environment
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-K2 环境
- en: The environment consists of a user with a purpose, which correlates to a semantic
    parse that the user expects the agent to provide in response to the user’s intent.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 环境由一个具有目的的用户组成，这与用户期望代理根据用户意图提供的语义解析相关。
- en: IV-K3 World Model
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-K3 世界模型
- en: Among the essential components of a MISP agent is its world model, which compacts
    the past perceptions throughout the interaction and forecasts the future based
    on the agent’s knowledge of the surrounding environment.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: MISP代理的核心组件之一是其世界模型，它压缩了在互动过程中过去的感知，并根据代理对周围环境的知识预测未来。
- en: IV-K4 Error Detector
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-K4 错误检测器
- en: The job of this module is to detect errors which it does introspective and greedy
    way. Two uncertainty measures are experimented with in the error detector.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块的工作是检测错误，其方式是自省的和贪婪的。错误检测器实验了两种不确定性度量。
- en: IV-K5 Actuator, a natural language generator
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-K5 执行器，自然语言生成器
- en: An actuator records the actions of the agent through a user interface.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器通过用户界面记录代理的操作。
- en: 'Given that the MISP-SQL agent executes its function by asking users binary
    questions, the actuator is also known as a natural language generator. A Natural
    language generator based on rules is defined, consisting of a seed lexicon and
    a grammar for generating questions from the seeds. The seed lexicon specifies
    an essential SQL element. Regarding MISP-SQL, there are four syntactic categories
    to consider: AGG for aggregates, OP for operators, COL for columns, and Q for
    produced queries.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 MISP-SQL 代理通过向用户提出二元问题来执行其功能，执行器也被称为自然语言生成器。基于规则的自然语言生成器由种子词汇和生成问题的语法组成。种子词汇指定了一个基本的
    SQL 元素。关于 MISP-SQL，需要考虑四种语法类别：AGG（聚合函数）、OP（操作符）、COL（列）和 Q（生成的查询）。
- en: The grammar describes the rules that must be followed to generate questions.
    Each column is detailed in its own right (i.e., the column name). The rules connected
    with each Q-typed item create an NL question. The Clause provides the required
    framework for asking meaningful questions in a meaningful way.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 语法描述了生成问题时必须遵循的规则。每一列都有详细的描述（即列名）。与每个 Q 类型条目相关的规则生成自然语言问题。子句提供了以有意义的方式提出有意义问题所需的框架。
- en: IV-K6 World Model
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-K6 世界模型
- en: The agent takes into account user input and adjusts its state by a model of
    the world. The MISP-SQL agent leverages the basic semantic parser to transition
    states, saving the user time and money by eliminating the need for further training.
    The agent asks the user a binary inquiry regarding whether an anticipated SQL
    component exists. Using the NL question generator, a Q-typed item is used to generate
    an NL question regarding the aggregator max in the clause ”SELECT max(age)”. The
    response either confirms or disproves the forecast made.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 代理会考虑用户输入，并通过一个世界模型调整其状态。MISP-SQL 代理利用基本的语义解析器来过渡状态，通过消除进一步培训的需求来节省用户的时间和金钱。代理向用户提出一个关于是否存在预期
    SQL 组件的二元查询。使用 NL 问题生成器，Q 类型的条目用于生成有关聚合器 max 在“SELECT max(age)”子句中的自然语言问题。响应要么确认，要么反驳所做的预测。
- en: IV-L Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic
    Parsing [[58](#bib.bib58)]
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-L 跨领域文本到 SQL 语义解析中的文本和表格数据的桥接 [[58](#bib.bib58)]
- en: 'The authors introduce BRIDGE, an architecture bridging dependencies between
    natural language questions and relational databases in cross-DB semantic parsing
    to represent dependencies between natural language questions and relational databases.
    A pointer generation decoder combined with schema-consistency-driven search space
    pruning enabled BRIDGE to achieve state-of-the-art performance on two popular
    text-to-SQL benchmarks: Spider (71.1% development and 67.5 percent test with ensemble
    model) and WikiSQL (71.1% development and 67.5 percent test with ensemble model)
    (92.6 percent dev, 91.9 percent test). Their investigation demonstrates that BRIDGE
    efficiently captures the intended cross-modal interdependence and can generalize
    to other text-DB-related jobs.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 作者介绍了 BRIDGE，这是一种在跨数据库语义解析中桥接自然语言问题与关系数据库之间依赖性的架构，用于表示自然语言问题与关系数据库之间的依赖性。结合模式一致性驱动的搜索空间剪枝的指针生成解码器使
    BRIDGE 在两个流行的文本到 SQL 基准测试中实现了最先进的性能：Spider（开发集 71.1% 和测试集 67.5% 使用集成模型）和 WikiSQL（开发集
    71.1% 和测试集 67.5% 使用集成模型）（开发集 92.6%，测试集 91.9%）。他们的研究表明 BRIDGE 有效捕捉了预期的跨模态相互依赖性，并可以推广到其他与文本-数据库相关的任务。
- en: '![Refer to caption](img/c37d9788f882cf952dd35658f864dd09.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c37d9788f882cf952dd35658f864dd09.png)'
- en: 'Figure 6: Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic
    Parsing [[58](#bib.bib58)]'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：跨领域文本到 SQL 语义解析中的文本和表格数据的桥接 [[58](#bib.bib58)]
- en: 'The following is a formal definition of their cross-DB text-to-SQL job. Given
    a natural language question Q and the relational database schema S = hT, Ci, the
    parser must construct the SQL query Y that corresponds to the natural language
    question Q. In the schema, there are three types of tables: T1 (the first one),
    T2 (the second one), and T3 (the third one). There are also three types of fields:
    C (the last one), T1 (the first one), T2 (the second one), and T3 (the third one).
    A textual name is assigned to each table ti and each field cij.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是他们的跨数据库文本到 SQL 任务的正式定义。给定一个自然语言问题 Q 和关系数据库模式 S = hT, Ci，解析器必须构建一个对应于自然语言问题
    Q 的 SQL 查询 Y。在模式中，有三种类型的表：T1（第一个）、T2（第二个）和 T3（第三个）。还有三种类型的字段：C（最后一个）、T1（第一个）、T2（第二个）和
    T3（第三个）。每个表 ti 和每个字段 cij 都分配了一个文本名称。
- en: IV-L1 Question-Schema Serialisation and Encoding
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-L1 问题-模式序列化和编码
- en: 'As seen in Figure [6](#S4.F6 "Figure 6 ‣ IV-L Bridging Textual and Tabular
    Data for Cross-Domain Text-to-SQL Semantic Parsing [58] ‣ IV Algorithmic explanation
    ‣ Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey"),
    each table is represented by its table name followed by the fields that make up
    that table. To the left of each table, the name is the unique token [T], and to
    the right of each field, the name is the unique token [C]. Several tables’ representations
    are concatenated to the question to produce a serialization of the schema. The
    serialization of the schema is surrounded by two [SEP] tokens and concatenated
    to the query. Finally, by the BERT input format, the question is preceded by [CLS]
    to form the hybrid question-schema serialization format.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [6](#S4.F6 "图 6 ‣ IV-L 跨领域文本到 SQL 语义解析中的文本与表格数据桥接 [58] ‣ IV 算法解释 ‣ 深度学习驱动的自然语言文本到
    SQL 查询转换：调查") 所示，每个表由其表名表示，后跟构成该表的字段。每个表的左侧，名称为唯一标记 [T]，每个字段的右侧，名称为唯一标记 [C]。多个表的表示被连接到问题中，产生方案的序列化。方案的序列化被两个
    [SEP] 标记包围，并连接到查询中。最后，通过 BERT 输入格式，问题前加上 [CLS]，形成混合问题-方案序列化格式。
- en: IV-L2 Bridging
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-L2 桥接
- en: A fuzzy string match between Q and the picklist of each field in the DB to determine
    which field is correct. The matching field values (anchor texts) are entered into
    the question-schema representation X after the respective field names and separated
    by the special character [V], in the same order as the corresponding field names.
    If more than one value were found to match for a single field, they would concatenate
    all values in the matching order. The question is considered answered if a question
    is compared with values in many fields. They combine all matches and allow the
    model to learn how to resolve ambiguity.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Q 和 DB 中每个字段的选择列表进行模糊字符串匹配，以确定哪个字段是正确的。匹配的字段值（锚文本）在各自字段名称后输入到问题-方案表示 X 中，并用特殊字符
    [V] 分隔，顺序与对应字段名称一致。如果发现一个字段有多个匹配值，则将所有值按匹配顺序连接。若问题与多个字段中的值进行比较，则视为已回答。他们结合所有匹配，并允许模型学习如何解决歧义。
- en: IV-L3 Decoder
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-L3 解码器
- en: 'Authors have used an LSTM-based pointer-generator in conjunction with multi-head
    attention. The decoder uses the question encoder’s end state as a point of departure.
    When the decoder reaches a new stage, it executes one of the following actions:
    creating a new token from the vocabulary V, copying a new token from the question
    Q, or copying an element from the schema S. Mathematically, at each step t, given
    the decoder state and the encoder representation they compute the multi-head attention
    as define.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们使用了基于 LSTM 的指针生成器，并结合多头注意力机制。解码器使用问题编码器的结束状态作为起点。当解码器进入新阶段时，它会执行以下操作之一：从词汇表
    V 中创建一个新标记，从问题 Q 中复制一个新标记，或从方案 S 中复制一个元素。在每一步 t，给定解码器状态和编码器表示，他们计算多头注意力，如定义。
- en: IV-L4 Schema-Consistency Guided Decoding
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-L4 方案一致性引导解码
- en: Using SQL syntax limitations and the fact that the DB fields appearing in each
    SQL clause must only originate from the tables specified in the FROM clause, they
    present simple heuristics for trimming the search space of the sequence decoders.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 SQL 语法限制和每个 SQL 子句中出现的 DB 字段必须仅来源于 FROM 子句中指定的表这一事实，他们提出了用于修剪序列解码器搜索空间的简单启发式方法。
- en: IV-M Encoding Database Schemas with Relation-Aware Self-Attention for Text-to-SQL
    Parsers [[59](#bib.bib59)]
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-M 使用关系感知自注意力编码数据库模式以供文本到 SQL 解析器使用 [[59](#bib.bib59)]
- en: The authors’ goal is to have approaches generalizable to domains and database
    schemas other than the training set while converting natural language inquiries
    into SQL queries to answer questions from a database. For a neural encoder-decoder
    paradigm to effectively handle complicated questions and database schemas, it
    is essential to appropriately encode the schema as part of the input along with
    the query. Specifically, they use relation-aware self-attention inside the encoder
    to allow it to reason about the relationships between the tables and columns in
    the supplied schema and utilize this knowledge in understanding the query. On
    the newly released Spider dataset, they obtained considerable improvements in
    exact match accuracy, with 42.94 percent precise match accuracy, compared to the
    18.96 percent reported in previous work.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的目标是使方法能够推广到训练集以外的领域和数据库模式，同时将自然语言查询转换为SQL查询，以回答数据库中的问题。为了使神经编码器-解码器范式有效地处理复杂的问题和数据库模式，将模式作为输入的一部分与查询一起适当地编码是至关重要的。具体来说，他们在编码器内部使用关系感知自注意力，使其能够推理提供的模式中表和列之间的关系，并利用这些知识来理解查询。在新发布的Spider数据集上，他们在精确匹配准确率上取得了显著提升，精确匹配准确率为42.94%，而之前工作的报告为18.96%。
- en: IV-M1 Encoding the Schema as a Graph
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-M1 将模式编码为图
- en: The authors have started by constructing a directed graph G that represents
    the database schema and labels each node and edge on the graph to create an encoder.
    This enables reasoning about links between schema components in the encoder. The
    above figure illustrates the example graph.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 作者首先构建了一个有向图G，该图表示数据库模式，并为图中的每个节点和边贴上标签，以创建一个编码器。这使得编码器能够推理模式组件之间的链接。上述图示例图展示了示例图。
- en: IV-M2 Initial Encoding of the Input
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-M2 输入的初步编码
- en: After that, an initial representation is extracted for each node in the graph
    and an initial representation for each of the words in the input query. A bidirectional
    LSTM over the words included in the label is used as a node label for the graph.
    To create the embedding for the node, they concatenate the output of the start
    and end time steps of this LSTM and concatenate them together. In addition, they
    employ a bidirectional LSTM across the words to answer the query.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，从图中的每个节点提取初步表示，并为输入查询中的每个词提取初步表示。使用双向LSTM对标签中的词进行处理，作为图的节点标签。为了创建节点的嵌入，他们将此LSTM的开始和结束时间步骤的输出连接在一起。此外，他们还使用双向LSTM处理词汇以回答查询。
- en: IV-M3 Relation-Aware Self-Attention
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-M3 关系感知自注意力
- en: At this point, they would want to infuse the information contained in the schema
    graph into these representations. They use self-attention that is also relation
    aware to attain this aim.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，他们希望将模式图中包含的信息融入这些表示中。他们使用同样是关系感知的自注意力来实现这一目标。
- en: IV-M4 Decoder
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-M4 解码器
- en: After obtaining an encoding of the input, the method utilizes a decoder developed
    by Yin and Neubig [1] to construct the SQL query for the data. It constructs the
    SQL query as an abstract syntax tree in depth-first traversal order by generating
    a series of production rules that extend the last created node in the tree, as
    shown in the following diagram. Because the decoder is limited to selecting only
    syntactically valid production rules, it always delivers syntactically correct
    outputs. The following adjustments have been made to Yin and Neubig [[60](#bib.bib60)]
    to conserve readers’ space.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得输入的编码后，该方法利用Yin和Neubig [1] 开发的解码器来构造数据的SQL查询。它通过生成一系列扩展树中最后创建节点的生成规则，将SQL查询构建为深度优先遍历顺序的抽象语法树，如下图所示。由于解码器仅限于选择语法上有效的生成规则，因此它始终生成语法正确的输出。对Yin和Neubig
    [[60](#bib.bib60)] 做了以下调整以节省读者空间。
- en: •
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When a column is to be given as output by the decoder, a pointer network [[61](#bib.bib61)]
    developed on scaled dot-product attention which points to c_final_i and t_final_i
    is used.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当解码器需要输出一列时，使用了基于缩放点积注意力的指针网络[[61](#bib.bib61)]，该网络指向`c_final_i`和`t_final_i`。
- en: •
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The decoder accesses the encoder outputs at each step which are c_final_i, t_final_i,
    and q_final_i using multi-head attention. The original decoder in Yin and Neubig [[60](#bib.bib60)]
    uses a simpler form of attention.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一步通过多头注意力访问编码器的输出，这些输出为`c_final_i`、`t_final_i`和`q_final_i`。Yin和Neubig [[60](#bib.bib60)]中的原始解码器使用了更简单的注意力形式。
- en: 'IV-N SeaD: Schema-aware Denoising [[62](#bib.bib62)]'
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'IV-N SeaD: 语义感知去噪[[62](#bib.bib62)]'
- en: Based on the Transformer design, the authors examine the questions in this study.
    Instead of constructing additional modules or imposing constraints on model output,
    they offer schema-aware denoising goals that are trained concurrently with the
    original S2S task. These denoising aims deal with the inherent property of logical
    form and, as a result, ease the schema linking necessary for the text-to-SQL operation.
    Erosion is used in the S2S job that trains a model to create a corrupted SQL sequence
    from NL and eroded schema. These suggested denoising goals and the origin S2S
    job are combined to train a SeaD model. In addition, to overcome the limitations
    of execution-guided (EG) decoding, the authors present a clause-sensitive EG technique
    that determines beam size based on the projected clause token. The findings demonstrate
    that Their model outperforms earlier work and provides a new benchmark for WikiSQL.
    It reflects the efficiency of the schema-aware denoising technique and highlights
    the significance of the task-oriented denoising aim.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer设计，作者研究了本研究中的问题。他们没有构建额外的模块或对模型输出施加约束，而是提供了与原始S2S任务同时训练的模式感知去噪目标。这些去噪目标处理逻辑形式的固有特性，从而简化了文本到SQL操作所需的模式链接。侵蚀用于S2S任务，该任务训练模型从NL和侵蚀模式创建损坏的SQL序列。这些建议的去噪目标与原始S2S任务结合，以训练SeaD模型。此外，为了克服执行引导（EG）解码的局限性，作者提出了一种基于投影子句令牌确定束大小的子句敏感EG技术。研究结果表明，他们的模型优于早期工作，并为WikiSQL提供了新的基准。这反映了模式感知去噪技术的效率，并突出了任务导向去噪目标的重要性。
- en: They generate the SQL sequence token by token using auto-aggressive generation.
    The transformer is a widely used S2S translation and generation architecture.
    In this part, they first give an example formulation that transforms text-to-SQL
    into a standard S2S work, then They introduce the Transformer architecture with
    a pointer generator. Then They discuss schema-aware denoising and clause-sensitive
    EG decoding.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 他们通过自回归生成逐个生成SQL序列令牌。Transformer是一种广泛使用的S2S翻译和生成架构。在这一部分，他们首先给出了一个示例公式，将文本到SQL转换为标准的S2S工作，然后介绍了带有指针生成器的Transformer架构。接着，他们讨论了模式感知去噪和子句敏感EG解码。
- en: IV-N1 Sample Formulation
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-N1 示例公式
- en: 'S2S creation requires re-formatting the structural target sequence and unordered
    schema set. Each schema column name is preceded with a specific token, where I
    signify the i-th column. The column type is also included in the name sequence
    [col name]: [col type]. The representative sequence for the schema is obtained
    by concatenating all columns in the schema. For model input, the schema sequence
    is combined with the NL sequence. They initiate SQL sequence with raw SQL query
    and modify it: 1) Surrounding SQL entities and values with a ”‘” token and deleting
    other tokens; 2) Replacing col entities with their matching schema token; 3) Inserting
    spaces between punctuation and words.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'S2S创建需要重新格式化结构目标序列和无序模式集。每个模式列名之前加上一个特定令牌，其中“I”表示第i列。列类型也包含在名称序列[col name]:
    [col type]中。模式的代表序列是通过连接模式中的所有列获得的。对于模型输入，模式序列与NL序列结合。他们用原始SQL查询初始化SQL序列并进行修改：1)
    用“‘”令牌包围SQL实体和值，并删除其他令牌；2) 用匹配的模式令牌替换列实体；3) 在标点符号和单词之间插入空格。'
- en: IV-N2 Transformer with Pointer
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-N2 带指针的Transformer
- en: In the decoding procedure, each input sequence Isource is encoded using the
    transformer encoder into the hidden states Htarget. First, the transformer decoder
    generates the hidden states ht in step t based on the previously generated sequence
    and encoded output. Next, an affine transformation is applied to obtain scores.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码过程中，每个输入序列Isource使用transformer编码器编码为隐藏状态Htarget。首先，transformer解码器基于先前生成的序列和编码输出，在步骤t生成隐藏状态ht。接下来，应用仿射变换以获得得分。
- en: IV-N3 Schema Aware Denoising
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-N3 模式感知去噪
- en: Two schema-aware objectives are proposed, namely, erosion and shuffle, that
    train the model to either rebuild the original sequence from applying noise to
    the input or otherwise predict the corrupted output.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了两个模式感知目标，即侵蚀（erosion）和洗牌（shuffle），它们训练模型从对输入施加噪声中重建原始序列或预测损坏的输出。
- en: IV-N4 Clause-sensitive EG Decoding
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-N4 子句敏感EG解码
- en: The projected SQL may include mistakes resulting from improper schema linking
    or grammar during text-to-SQL inference. Through an executor-in-loop iteration,
    EG decoding is suggested to rectify these problems. It is carried out by sequentially
    feeding SQL queries from the candidate list to the executor and eliminating those
    that fail to execute or produce an empty response. Such a decoding technique,
    although successful, shows that the primary dispute in the candidate list is centered
    on schema linking or grammar. They are directly applying EG to the candidates
    obtained by beam search results in a negligible improvement since they consist
    of redundant variants with selection or schema naming as the primary emphasis,
    etc. This issue may be resolved by changing the beam length of most expected tokens
    to 1 and releasing tokens associated with schema linking (e.g., WHERE).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的SQL可能由于不正确的模式链接或语法问题在文本到SQL推理过程中出现错误。通过一个执行器循环迭代，建议使用EG解码来纠正这些问题。这是通过将候选列表中的SQL查询顺序地输入到执行器中，并消除那些无法执行或产生空响应的查询来完成的。尽管这种解码技术成功，但显示出候选列表中的主要争议集中在模式链接或语法上。他们直接将EG应用于通过束搜索获得的候选项，改进非常有限，因为它们包含的冗余变体主要强调选择或模式命名等。可以通过将大多数预期令牌的束长度更改为1，并释放与模式链接相关的令牌（例如，WHERE）来解决这个问题。
- en: IV-O Learning to Synthesize Data for Semantic Parsing [[63](#bib.bib63)]
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-O 学习合成数据以进行语义解析 [[63](#bib.bib63)]
- en: The authors utilize text-to-SQL as an example problem and propose a generative
    model to generate pair-wise utterance-SQL representations. Learning to Synthesize
    Data for Semantic Parsing, the authors first simulate the distribution of SQL
    queries using a probabilistic context-free grammar (PCFG). Then, with the aid
    of a SQL-to-text translation model, the relevant SQL query expressions are constructed.
    In their scenario, the ’target language’ is a formal language whose underlying
    grammar is well understood. Like the training of a semantic parser, data synthesizer
    training needs a collection of utterance-SQL pairings. Their two-stage data synthesis
    strategy, consisting of the PCFG and the translation model, is more sample efficient
    than a neural semantic parser, which is very similar to back-translation [[64](#bib.bib64)].
    They sample synthetic data from the generative model to train a semantic parser.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们以文本到SQL为例问题，提出了一种生成模型来生成成对的发话- SQL 表示。为了合成数据进行语义解析，作者们首先使用概率上下文无关文法（PCFG）来模拟SQL查询的分布。然后，在SQL到文本的翻译模型的帮助下，构建了相关的SQL查询表达式。在他们的场景中，“目标语言”是一个其底层语法已被很好理解的形式语言。类似于语义解析器的训练，数据合成器的训练也需要一组发话-
    SQL 对。其由PCFG和翻译模型组成的两阶段数据合成策略，比神经语义解析器更具样本效率，这与回译非常相似 [[64](#bib.bib64)]。他们从生成模型中采样合成数据来训练语义解析器。
- en: IV-P Awakening Latent Grounding from Pretrained Language Models for Semantic
    Parsing [[65](#bib.bib65)]
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-P 从预训练语言模型中觉醒潜在基础以进行语义解析 [[65](#bib.bib65)]
- en: The authors propose the new Erasing-then-Awakening method (ETA). Recent developments
    influence it in interpretable machine learning [[66](#bib.bib66)], where the significance
    of individual pixels may be evaluated in the classification decision. Similarly,
    their method first assesses the importance of each word to each idea by deleting
    it and examining the variance of concept prediction judgments (elaborated later).
    Then, it leverages these contributions as pseudo labels to reawaken PLMs’ dormant
    grounding. In contrast to previous research, their technique requires supervision
    of concept prediction, which may be readily extracted by downstream activities
    (e.g., text-to-SQL). Four empirical datasets illustrate that their method may
    uncover latent grounding that is understandable to human experts. In training,
    their method is not subjected to any human-annotated grounding label. Hence this
    problem is very complex. Significantly, they discover that the grounding may be
    readily connected with downstream models to increase their performance by as much
    as 9.8 percent.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们提出了新的擦除-再唤醒方法（ETA）。最近的进展影响了可解释机器学习 [[66](#bib.bib66)]，在这种方法中，单个像素在分类决策中的重要性可能会被评估。同样，他们的方法首先通过删除每个词并检查概念预测判断的方差（后文详述），来评估每个词对每个思想的重要性。然后，它利用这些贡献作为伪标签来重新唤醒预训练语言模型（PLMs）的潜在基础。与以往的研究相比，他们的技术需要对概念预测进行监督，这可以通过下游活动（例如，文本到SQL）轻松提取。四个实证数据集表明，他们的方法可能揭示人类专家可以理解的潜在基础。在训练中，他们的方法不受任何人工标注的基础标签的限制。因此，这个问题非常复杂。显著的是，他们发现基础可以很容易地与下游模型连接，以提高其性能最多达9.8%。
- en: 'The author’s model comprises PLM, CP, and grounding modules. This section begins
    with an overview of ETA’s training program, which consists of three steps: (1)
    Train an auxiliary module for idea prediction. (2) Remove tokens from a question
    to derive concept prediction confidence differences as pseudo alignment. (3) Awaken
    dormant grounding in PLMs by supervising them with pseudo alignment. Then, the
    process for producing grounding pairs in inference is introduced.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的模型包括PLM、CP和基础模块。本节开始介绍ETA的训练程序，该程序包括三个步骤：（1）训练一个辅助模块用于思想预测。（2）从问题中移除令牌，以得出概念预测置信度差异作为伪对齐。（3）通过伪对齐来监督PLMs，以唤醒其潜在基础。然后，介绍了在推理中生成基础对的过程。
- en: IV-Q SyntaxSQLNet [[67](#bib.bib67)]
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-Q SyntaxSQLNet [[67](#bib.bib67)]
- en: A syntax tree network to deal with the complex and cross-domain task of text-to-SQL
    generation SyntaxSQLNet can handle a substantially higher number of sophisticated
    SQL examples (such as which can handle nested queries on unknown databases) than
    earlier research, beating the previous state-of-the-art model in precise matching
    accuracy by 7.3 %. SyntaxSQLNet may enhance speed by an additional 7.5 %utilizing
    a cross-domain augmentation strategy, for a total improvement of 14.8 %. On specific
    complicated text-to-SQL benchmarks, such as ATIS and GeoQuery, Seq2Seq encoder-decoder
    designs may achieve more than 80% accurate matching accuracy. Although these models
    appear to have addressed the majority of challenges in this field, most learn
    to match semantic parsing outcomes rather than genuinely understanding the meanings
    of inputs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 一个处理文本到SQL生成的复杂和跨领域任务的语法树网络。SyntaxSQLNet能够处理比早期研究更多的复杂SQL示例（例如，可以处理对未知数据库的嵌套查询），并且在精确匹配准确率上比之前的最先进模型提高了7.3%。SyntaxSQLNet通过利用跨领域增强策略进一步提高了速度7.5%，总改进幅度达到14.8%。在特定的复杂文本到SQL基准测试中，如ATIS和GeoQuery，Seq2Seq编码器-解码器设计可能实现了超过80%的准确匹配率。虽然这些模型似乎解决了这一领域的大部分挑战，但大多数模型学习的是匹配语义解析结果，而非真正理解输入的含义。
- en: SyntaxSQLNet is a SQL-specific syntax tree network that may be used to solve
    the Spider problem. Researchers build a syntax tree-based decoder with SQL generation
    path history to construct complicated SQL queries with numerous clauses, selects,
    and sub-queries. They also create a table-aware column encoder to help Their model
    learn to generalize to new databases with new tables and columns.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: SyntaxSQLNet 是一个针对SQL的语法树网络，可以用来解决Spider问题。研究人员构建了一个基于语法树的解码器，并结合SQL生成路径历史，以构造包含多个子句、选择和子查询的复杂SQL查询。他们还创建了一个表感知的列编码器，以帮助他们的模型学习如何推广到具有新表和新列的新数据库。
- en: IV-Q1 Approach
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-Q1 方法
- en: SyntaxSQLNet divides the SQL decoding process into nine modules, each responsible
    for predicting distinct SQL components such as keywords, operators, and variables.
    The IUEN Module, which predicts INTERSECT, UNION, EXCEPT, and NONE, decides whether
    They need to call themselves again to construct nested queries. WHERE, GROUP BY,
    and ORDER BY keywords are predicted by the KW Module, whereas SELECT is used in
    every query. COL Module is used for predicting table columns and OP Module. MAX,
    MIN, SUM, COUNT, AVG, and NONE are used in the AGG Module to anticipate aggregators.
    Predicting the ROOT of a new subquery or terminal value is done by the Root/Terminal
    Module. AND/OR Module predicts whether an AND or OR operator exists between two
    conditions. The ORDER BY keywords is predicted by the DESC/ASC/LIMIT module. It
    is only used when ORDER BY is anticipated first. The HAVING Module does predict
    the presence of HAVING for the GROUP BY clause. It is only used when GROUP BY
    has been expected before.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: SyntaxSQLNet 将 SQL 解码过程划分为九个模块，每个模块负责预测不同的 SQL 组件，如关键字、操作符和变量。IUEN 模块预测 INTERSECT、UNION、EXCEPT
    和 NONE，决定是否需要再次调用自己以构建嵌套查询。WHERE、GROUP BY 和 ORDER BY 关键字由 KW 模块预测，而 SELECT 在每个查询中使用。COL
    模块用于预测表列，OP 模块。MAX、MIN、SUM、COUNT、AVG 和 NONE 在 AGG 模块中用于预测聚合器。预测新子查询或终端值的 ROOT
    由 Root/Terminal 模块完成。AND/OR 模块预测两个条件之间是否存在 AND 或 OR 操作符。ORDER BY 关键字由 DESC/ASC/LIMIT
    模块预测。只有在首先预测 ORDER BY 时才使用它。HAVING 模块预测 GROUP BY 子句的 HAVING 存在。仅在之前预测 GROUP BY
    时使用。
- en: IV-Q2 SQL Grammar
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-Q2 SQL 语法
- en: SyntaxSQLNet chooses which module to run and forecast the next SQL token to
    build depending on the current SQL token and the SQL history (the tokens they’ve
    gone over to arrive at the current token) during the decoding process. During
    decoding, the model checks the current token instance type and whether the previously
    decoded SQL token is GROUP for HAVING and WHERE or HAVING for OP.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: SyntaxSQLNet 根据当前 SQL 令牌和 SQL 历史（到达当前令牌所经过的令牌）选择运行哪个模块，并预测下一个 SQL 令牌以进行构建。在解码过程中，模型检查当前令牌实例的类型以及之前解码的
    SQL 令牌是否为 GROUP，用于 HAVING 和 WHERE，或 HAVING 用于 OP。
- en: IV-Q3 Input Encoder
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-Q3 输入编码器
- en: 'Each module’s inputs consist of three data types: a query, a database structure,
    and the current SQL decoding history path. A bi-directional LSTM, BiLSTMQ, is
    used to encode a question sentence.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块的输入包括三种数据类型：查询、数据库结构和当前 SQL 解码历史路径。使用双向 LSTM，即 BiLSTMQ，来编码问题句子。
- en: IV-Q4 Table-Aware Column Representation
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-Q4 表感知列表示
- en: In particular, given a database, as an initial input to each column, SyntaxSQLNet
    obtains a list of words in the table name, words in the column name, and the type
    of information of the column (string, integer, primary/foreign key). The table-aware
    column representation of a particular column is then computed as the final hidden
    state of a BiLSTM executing on top of this sequence, just as SQLNet. In this manner,
    the encoding scheme can interpret a natural language inquiry in the context of
    the supplied database by capturing both global (table names) and local (column
    names and types) information in the database design.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是，给定一个数据库，作为每列的初始输入，SyntaxSQLNet 获得表名中的词汇、列名中的词汇以及列的信息类型（字符串、整数、主键/外键）列表。然后，特定列的表感知列表示被计算为在此序列上执行的
    BiLSTM 的最终隐藏状态，就像 SQLNet 一样。通过这种方式，编码方案可以在提供的数据库上下文中解释自然语言查询，通过捕捉数据库设计中的全球（表名）和局部（列名和类型）信息来实现。
- en: Each SQLNet module, by contrast, ignores the preceding decoded SQL history.
    As a result, if SyntaxSQLNet had applied it straight to their recursive SQL decoding
    stages, each module would anticipate the same outcome each time it was called.
    Each module can forecast a different output based on the history each time it
    is called during the recursive SQL generation process by supplying the SQL history.
    Additionally, the SQL history can assist each module in performing better on long
    and complicated queries by allowing the model to record the relationships between
    clauses. During test decoding, predicted SQL history is utilized. To build gold
    SQL route history for each training sample, SyntaxSQLNet first explores each node
    in the gold query tree in pre-order.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 每个SQLNet模块相反地忽略了前面的解码SQL历史。因此，如果SyntaxSQLNet将其直接应用于递归SQL解码阶段，每个模块在每次调用时都会预测相同的结果。通过提供SQL历史记录，每个模块可以基于历史记录预测不同的输出。在递归SQL生成过程中，SQL历史记录可以帮助每个模块在处理长而复杂的查询时表现更好，因为它允许模型记录子句之间的关系。在测试解码期间，使用预测的SQL历史记录。为了为每个训练样本建立黄金SQL路径历史记录，SyntaxSQLNet首先以先序方式探索黄金查询树中的每个节点。
- en: IV-Q5 Recursive SQL Generation
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-Q5 递归SQL生成
- en: The SQL creation procedure involves recursively activating many modules. To
    structure Their decoding procedure, as shown in Figure 2, They SyntaxSQLNet uses
    a stack. Each decoding step consists in removing one SQL token instance from the
    stack, using a grammar-based module to predict the next token instance, and then
    pushing the anticipated instance into the stack. Decoding continues until the
    stack is depleted. In the first decoding phase, this method deliberately creates
    a stack with only ROOT. The stack then pops ROOT in the following stage. The ROOT
    uses the IUEN module to determine whether there is an EXCEPT, INTERSECT, or UNION.
    If that’s the case, the next step is to create two subqueries. If the model predicts
    NONE, it will be placed at the bottom of the stack. At the next step, the stack
    pops NONE. In Figure 2, the currently popped token is SELECT, a keyword (KW) type
    instance. It uses the COL module to forecast the column’s name that will be pushed
    to the stack.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: SQL创建过程涉及递归激活多个模块。为了构建它们的解码过程，如图2所示，SyntaxSQLNet使用一个栈。每个解码步骤包括从栈中移除一个SQL标记实例，使用基于语法的模块预测下一个标记实例，然后将预测的实例推入栈中。解码持续进行，直到栈被耗尽。在第一个解码阶段，此方法故意创建一个仅包含ROOT的栈。接下来的阶段栈将弹出ROOT。ROOT使用IUEN模块确定是否存在EXCEPT、INTERSECT或UNION。如果是这样，下一步是创建两个子查询。如果模型预测NONE，它将被放置在栈底部。在下一步中，栈弹出NONE。在图2中，当前弹出的标记是SELECT，一个关键字（KW）类型的实例。它使用COL模块预测将推送到栈中的列名。
- en: IV-Q6 Comparison to Existing Models
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-Q6 与现有模型的比较
- en: Even though the individual modules are similar to SQLNet and TypeSQL, the syntax-aware
    decoder allows the modules to generate complex SQL queries recursively based on
    the SQL grammar. This result suggests that the syntax and history information
    benefit this complex text-to-SQL task. Specifically, SyntaxSQLNet outperforms
    the previous best, SQLNet, even without the data augmentation technique, by 7.3%.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管各个模块类似于SQLNet和TypeSQL，但语法感知解码器允许这些模块基于SQL语法递归生成复杂的SQL查询。这一结果表明，语法和历史信息对这一复杂的文本到SQL任务是有益的。具体而言，即使没有数据增强技术，SyntaxSQLNet也比之前的最佳模型SQLNet提高了7.3%。
- en: IV-R PointerSQL [[68](#bib.bib68)]
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-R PointerSQL [[68](#bib.bib68)]
- en: The PointerSQL solves the challenge of neural semantic parsing, which converts
    natural language inquiries into SQL queries that can be executed. The authors
    propose a novel approach for using SQL semantics called execution guidance. By
    conditioning the execution of a partially created program, it discovers and rejects
    erroneous programs during the decoding step. The method may be employed with any
    autoregressive generating model, as demonstrated by the authors using four cutting-edge
    recurrent or template-based semantic parsing models. They also show that execution
    guidance increases model performance across the board on various text-to-SQL datasets
    with varying sizes and query complexity, including WikiSQL, ATIS, and GeoQuery.
    Consequently, they reached a new state-of-the-art execution accuracy on WikiSQL
    of 83.8%.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: PointerSQL 解决了神经语义解析的挑战，该技术将自然语言查询转换为可执行的 SQL 查询。作者提出了一种称为执行指导的 SQL 语义新方法。通过对部分创建的程序的执行进行条件判断，它在解码步骤中发现并拒绝错误程序。该方法可以与任何自回归生成模型配合使用，作者通过四种前沿的递归或基于模板的语义解析模型进行了演示。他们还展示了执行指导在不同的文本到
    SQL 数据集上，包括 WikiSQL、ATIS 和 GeoQuery，提升了模型的整体表现。结果，他们在 WikiSQL 上达到了 83.8% 的新最先进执行准确率。
- en: Developing effective semantic parsers to translate natural language questions
    into logical programs has been a long-standing goal. The PointerSQL focuses on
    the semantic parsing task of translating natural language queries into executable
    SQL programs. It shows how to condition such models to avoid whole classes of
    errors to generate syntactically valid queries. Execution guidance is the idea
    that a partially generated query can already be executed in languages such as
    SQL. The results of that execution can be used to guide the generation procedure.
    In other words, execution guidance extends standard autoregressive decoders to
    condition on non-differentiable partial execution results at appropriate timesteps
    additionally. It also shows the effectiveness of execution guidance by extending
    a range of existing models with execution guidance and evaluating the resulting
    models on various text-to-SQL tasks.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 开发有效的语义解析器以将自然语言问题翻译为逻辑程序一直是一个长期目标。PointerSQL 侧重于将自然语言查询翻译为可执行 SQL 程序的语义解析任务。它展示了如何对这些模型进行条件判断，以避免生成语法上有效的查询时的整类错误。执行指导的理念是，部分生成的查询已经可以在如
    SQL 这样的语言中执行。这些执行结果可以用于指导生成过程。换句话说，执行指导扩展了标准自回归解码器，使其能够在适当的时间步上对不可微分的部分执行结果进行条件判断。它还通过扩展一系列现有模型并在各种文本到
    SQL 任务上评估结果模型，展示了执行指导的有效性。
- en: 'IV-R1 Execution-Guided Decoding:'
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-R1 执行指导解码：
- en: The authors of PointerSQL combine the query generation method with a SQL execution
    component to prevent creating queries that result in Execution Errors, and extending
    a model with execution guidance, therefore, necessitates deciding which stages
    of the generating method to execute the partial result at, and then using the
    outcome to modify the remaining generation procedure. The pseudocode of an execution-guided
    expansion of a typical autoregressive recurrent decoder is demonstrated. It’s
    a model-specific decoder cell called DECODE, an extension of ordinary beam search.
    The technique keeps just the top k states in the beam that correspond to the partial
    programs without execution faults or empty outputs whenever possible (where the
    result at the current timestep t corresponds to a partial executable program).
    Execution guidance can be employed as a filtering step after decoding in non-autoregressive
    models based on feedforward networks, for example, by removing result programs
    that produce execution errors. Any autoregressive decoder may be used similarly
    at the end of beam decoding. In many application areas (including SQL creation),
    execution checks may be applied to partially decoded programs rather than only
    after beam decoding. For example, right after the token ’Haugar’ is emitted, this
    helps to remove an improperly produced string-to-string inequality comparison
    ”… WHERE opponent ¿ ’Haugar’… ” from the beam. This considerably enhances the
    efficacy of execution guidance, as demonstrated by the research.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: PointerSQL 的作者将查询生成方法与 SQL 执行组件结合起来，以防止创建导致执行错误的查询，并扩展了一个具有执行指导的模型，因此，必须决定在生成方法的哪些阶段执行部分结果，然后使用结果来修改其余的生成过程。演示了一个典型的自回归递归解码器的执行指导扩展的伪代码。这是一个特定于模型的解码器单元，称为
    DECODE，是普通束搜索的扩展。这项技术在束中仅保留与部分程序对应的前 k 个状态，这些状态尽可能没有执行故障或空输出（其中当前时间步 t 的结果对应于一个部分可执行程序）。执行指导可以作为非自回归模型中基于前馈网络的解码后的过滤步骤来使用，例如，通过删除产生执行错误的结果程序。任何自回归解码器也可以类似地用于束解码的末尾。在许多应用领域（包括
    SQL 创建）中，执行检查可以应用于部分解码程序，而不仅仅是在束解码后。例如，在发出标记 ’Haugar’ 之后，这有助于从束中删除一个不正确生成的字符串到字符串的不等式比较
    ”… WHERE opponent ¿ ’Haugar’… ”。这大大提高了执行指导的有效性，如研究所示。
- en: Wang et al. [[69](#bib.bib69)] introduced the Pointer-SQL model, which extends
    and specializes in the sequence-to-sequence architecture of the WikiSQL dataset.
    As inputs, it takes a natural language question and a single table of its schema.
    To learn a joint representation, a bidirectional RNN with LSTM cells processes
    the concatenation of the table header (column names) of the queried table and
    the question as input. Another RNN, the decoder, can look over and copy the encoded
    input sequence. The decoder uses three separate output modules to correspond to
    three decoding types, a vital feature of this model. One module generates SQL
    keywords, while the other is for copying a column. Extending the Coarse2Fine model
    with an execution-guided decoder improves its accuracy by 5.4% on the WikiSQL
    test, which beats state-of-the-art on this task.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[69](#bib.bib69)] 引入了 Pointer-SQL 模型，该模型扩展并专门针对 WikiSQL 数据集的序列到序列架构。作为输入，它接受自然语言问题和一个表格的模式。为了学习联合表示，一个具有
    LSTM 单元的双向 RNN 处理查询表格的表头（列名）和问题的连接作为输入。另一个 RNN，解码器，可以查看并复制编码的输入序列。解码器使用三个单独的输出模块来对应三种解码类型，这是该模型的一个重要特性。一个模块生成
    SQL 关键字，而另一个用于复制列。将 Coarse2Fine 模型扩展为具有执行指导的解码器，将其在 WikiSQL 测试上的准确率提高了 5.4%，击败了该任务上的最先进水平。
- en: IV-S Content Enhanced BERT-based Text-to-SQL Generation [[70](#bib.bib70)]
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-S 内容增强的基于 BERT 的文本到 SQL 生成 [[70](#bib.bib70)]
- en: BERT is a transformer-based model with an intense level of complexity. It uses
    the mask language model loss and the next-sentence loss to pre-train on a vast
    corpus. Then They could fine-tune BERT for various tasks, including text categorization,
    matching, and natural language inference, to achieve new state-of-the-art performance.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一个基于变换器的模型，具有高度复杂性。它使用掩码语言模型损失和下一个句子损失在大规模语料库上进行预训练。然后，他们可以对 BERT 进行微调，以处理各种任务，包括文本分类、匹配和自然语言推理，从而实现新的最先进的性能。
- en: 'The NL2SQL model depends on the following three things:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: NL2SQL 模型依赖于以下三点：
- en: '1.'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To mark the query, they take the match information from all of the table columns
    and the query string to create a feature vector that has the same length as the
    query.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了标记查询，他们从所有表格列和查询字符串中提取匹配信息，创建一个与查询长度相同的特征向量。
- en: '2.'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: To mark the column, they utilize the match info of all the table column names
    and question strings to create a feature vector that has the same length as the
    table header.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了标记列，他们利用所有表格列名和问题字符串的匹配信息，创建一个与表格头部长度相同的特征向量。
- en: '3.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The whole BERT-based model is designed with the two feature vectors above as
    external inputs.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 整个基于BERT的模型被设计为将上述两个特征向量作为外部输入。
- en: IV-T GRAPPA [[71](#bib.bib71)]
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-T GRAPPA [[71](#bib.bib71)]
- en: The authors provide a unique grammar-augmented pre-training approach for table
    semantic parsing in this research (GRAPPA). The authors infer a synchronous context-free
    grammar (SCFG) for natural mapping language to SQL queries from existing text-to-SQL
    datasets that account for most question-SQL patterns. The authors may generate
    a question-SQL template from a text-to-SQL example by excluding references to
    schema components (tables and fields), values, and SQL actions. Using a unique
    text-schema linking goal that predicts the syntactic function of a table column
    in the SQL for each pair, The authors train GRAPPA on these synthetic question-SQL
    pairings and their related tables. To pre-train GRAPPA, the authors use 475k synthetic
    cases and 391.5k examples from existing table-and-language datasets. Their method
    significantly decreases training time and GPU costs. The authors examine four
    major semantic parsing benchmarks under both strong and light supervision. GRAPPA
    routinely obtains new state-of-the-art results on all of them, exceeding all previously
    published results by a wide margin.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究为表格语义解析提供了一种独特的语法增强预训练方法（GRAPPA）。作者从现有的文本到SQL数据集中推断出自然映射语言到SQL查询的同步上下文无关文法（SCFG），这些数据集涵盖了大多数问题-SQL模式。作者通过排除对模式组件（表和字段）、值和SQL动作的引用，从文本到SQL示例生成一个问题-SQL模板。通过一种独特的文本-模式链接目标，该目标预测SQL中表格列的句法功能，作者在这些合成问题-SQL配对及其相关表格上训练GRAPPA。为了预训练GRAPPA，作者使用了475k个合成案例和391.5k个来自现有表格和语言数据集的示例。他们的方法显著减少了训练时间和GPU成本。作者在强监督和轻监督下对四个主要的语义解析基准进行了评估。GRAPPA在所有基准测试中都获得了新的最先进结果，超越了所有之前发布的结果。
- en: IV-T1 Methodology
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-T1 方法论
- en: Increasing research demonstrates that using enhanced data does not always result
    in a significant performance improvement in cross-domain semantic parsing end
    jobs. The most obvious explanation is that models tend to overfit the canonical
    input distribution, producing notably distinct utterances from the originals.
    In addition, instead of immediately training semantic parsers on the enriched
    data, the author’s study is the first to employ synthetic instances in pre-training
    to inject an inductive compositional bias into LMs and demonstrate that it truly
    works if the overfitting issue is treated with care. To combat overfitting, the
    author’s pre-training data also contains a tiny number of table-related utterances.
    As a regularization element, the authors apply an MLM loss to them, which challenges
    the model to balance actual and synthetic cases during pre-training. This reliably
    enhances the performance of all semantic parsing jobs that follow.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的研究表明，使用增强数据并不总能在跨域语义解析终端任务中带来显著的性能提升。最明显的解释是模型往往过拟合标准输入分布，产生明显不同于原始数据的语句。此外，作者的研究是首个在预训练中使用合成实例以将归纳组合偏置注入语言模型（LMs）中的研究，并且证明了如果处理好过拟合问题，这种方法确实有效。为了对抗过拟合，作者的预训练数据还包含少量与表格相关的语句。作为正则化元素，作者对这些语句应用了MLM损失，挑战模型在预训练期间平衡真实和合成案例。这可靠地提高了所有后续语义解析任务的性能。
- en: IV-U TRIAGESQL [[72](#bib.bib72)]
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-U TRIAGESQL [[72](#bib.bib72)]
- en: TRIAGESQL is a benchmark for cross-domain text-to-SQL question intention categorization.
    The authors describe four categories of unanswerable questions and distinguish
    them from answerable ones, then generate a benchmark dataset consisting of 34,000
    databases and 390,000 questions from 21 text-to-SQL, question answering, and table-to-text
    existing datasets. The authors reviewed and annotated data to create a high-quality
    test set with 500 instances of each kind. The fact that a Transformer-based model
    RoBERTa [[73](#bib.bib73)] got a 60 percent F1 score on their benchmark for a
    five-class classification model demonstrates the difficulty of this endeavor.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: TRIAGESQL 是一个跨领域文本到 SQL 问题意图分类的基准。作者描述了四类无法回答的问题，并将它们与可回答的问题区分开来，然后生成了一个基准数据集，包含来自
    21 个文本到 SQL、问题回答和表到文本现有数据集的 34,000 个数据库和 390,000 个问题。作者审查和注释数据，创建了一个高质量的测试集，每种类型有
    500 个实例。基于 Transformer 的模型 RoBERTa [[73](#bib.bib73)] 在其五类分类模型的基准测试中获得了 60% 的
    F1 分数，这表明了这一工作的难度。
- en: IV-V ValueNet [[15](#bib.bib15)]
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-V ValueNet [[15](#bib.bib15)]
- en: Brunner et al [[15](#bib.bib15)] proposed two end-to-end texts to SQL algorithm
    ValueNet and ValueNet light. The idea of Valuenet is based on combining the metadata
    information from the database with the information on base data from the spider
    dataset. The ValueNet is based on encode-decoder architecture to generate the
    SQL query by extracting values from a user question which helps them to generate
    other possible value candidates which are not used or mentioned in the question
    asked in the form of natural language text.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Brunner 等人 [[15](#bib.bib15)] 提出了两种端到端的文本到 SQL 算法 ValueNet 和 ValueNet light。ValueNet
    的思想基于将数据库中的元数据与来自蜘蛛数据集的基础数据进行结合。ValueNet 基于编码-解码器架构，通过从用户问题中提取值来生成 SQL 查询，这有助于生成其他可能的值候选，这些值在以自然语言文本形式提问的问题中没有被使用或提及。
- en: ValueNet light selects the correct values from a given list of possible ground
    truth values and then synthesizes a full query including the chosen values. Whereas
    ValueNet goes one step ahead with extracting and generating value candidates from
    the basic question format in the natural language and the content from the database
    only using Name Entity Recognition (NER) and heuristics. The next step uses those
    value candidates to generate SQL queries using encoder-decoder. However, between
    ValueNet and ValueNet light, a performance gap of $3\%-4\%$ is expected.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ValueNet light 从给定的可能真实值列表中选择正确的值，然后综合生成一个包括所选值的完整查询。而 ValueNet 则进一步从自然语言的基本问题格式和数据库内容中提取和生成值候选，仅使用命名实体识别
    (NER) 和启发式方法。下一步使用这些值候选来生成 SQL 查询，使用编码器-解码器。然而，在 ValueNet 和 ValueNet light 之间，预计会有
    $3\%-4\%$ 的性能差距。
- en: IV-W RYANSQL [[74](#bib.bib74)]
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-W RYANSQL [[74](#bib.bib74)]
- en: RYANSQL (Recursively Yielding Annotation Network for SQL) is another approach
    to solving cross-domain complex Text-to-SQL problems by generating multiple nested
    queries and recursively predicting its component SELECT statements using Statement
    Position Code (SPC) and sketch-based slot filling. RYANSQL improves the previous
    state-of-the-art system by 3.2% in terms of the exact matching accuracy test using
    BERT. The two simple input manipulations are used to improve the performance of
    RYANSQL. One is that some tables are used only to make a “link” between other
    tables in a FROM clause. And in the second manipulation, the table names are supplemented
    with their column names which help the architecture to distinguish between the
    same column names but from different tables.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: RYANSQL（递归生成 SQL 注释网络）是另一种解决跨领域复杂文本到 SQL 问题的方法，通过生成多个嵌套查询并递归预测其组件 SELECT 语句，使用语句位置编码
    (SPC) 和基于草图的插槽填充。RYANSQL 在使用 BERT 的精确匹配准确度测试中比之前的最新技术系统提高了 3.2%。RYANSQL 的性能通过两个简单的输入操作得到了提升。一个是某些表仅用于在
    FROM 子句中“链接”其他表。在第二个操作中，表名与其列名一起补充，这有助于架构区分来自不同表的相同列名。
- en: IV-X F-SQL [[75](#bib.bib75)]
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-X F-SQL [[75](#bib.bib75)]
- en: F-SQL focus on solving the problem of table content utilization. It employs
    the gate mechanism to fuse table schemas and contents and get a different representation
    of table schemas. It uses sketch-based [[1](#bib.bib1)] technique to synthesize
    SQL queries from natural language questions.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: F-SQL 关注解决表内容利用的问题。它采用门控机制来融合表模式和内容，并获得表模式的不同表示。它使用基于草图的 [[1](#bib.bib1)] 技术从自然语言问题中综合
    SQL 查询。
- en: Since sketch-based techniques require neural network models to predict the slots
    to assemble SQL, F-SQL uses multiple sub-models in its architecture to predict
    the slot in the sketch. The selection of sub-models also depends on the dataset
    and the pre-trained encoder. They achieved better SQL query synthesis by training
    all the slots together. To overcome the challenge of column prediction, they used
    a simple gate mechanism to fuse table schemas and table contents and get were
    able to predict distinct columns.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于草图的技术需要神经网络模型来预测SQL的插槽，F-SQL在其架构中使用多个子模型来预测草图中的插槽。子模型的选择还取决于数据集和预训练编码器。他们通过将所有插槽一起训练来实现更好的SQL查询合成。为了克服列预测的挑战，他们使用了简单的门控机制来融合表模式和表内容，并成功预测了不同的列。
- en: V Conclusion
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this profound work-in-progress study, besides the overview of various approaches
    and methods for TEXT to SQL language models, we also focus on terminologies and
    definitions frequently used in their summarized form for the reader to gain deep
    insight into natural language processing. This was further stratified based on
    their domain of application. Proceeding, upon extensive research and study, a
    variety of union of benchmarks were discovered. Adding to that, a multitude of
    models were described in detail. The stance toward individual models has been
    categorized based on a broad division of approaches. This allows a simplified
    overview of the evolution of a specific approach. Following this, a detailed summary
    of the multiple experiments, evaluation, and accuracy of the models mentioned
    above. In our future study, we want to cover all the mentioned TEXT2SQL approaches
    in full detail with our own set of benchmark datasets. This will also help to
    have a standard benchmark of all the models on a similar benchmark dataset which
    is not the case currently. This will also help in accessing the applicability
    of specific models and their limitations on different datasets.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项深奥的研究中，除了概述TEXT到SQL语言模型的各种方法和方法外，我们还专注于术语和定义，并以总结的形式呈现，以便读者深入了解自然语言处理。这进一步根据其应用领域进行分层。经过广泛的研究和学习，发现了多种基准的联合。除此之外，还详细描述了多种模型。对个别模型的立场进行了基于广泛方法划分的分类。这使得特定方法演变的概述更加简化。接着，对上述模型的多个实验、评估和准确性进行了详细总结。在未来的研究中，我们希望详细涵盖所有提到的TEXT2SQL方法，并使用我们自己的基准数据集。这也有助于对所有模型在类似基准数据集上的标准基准进行评估，而这在目前尚未实现。这还将有助于评估特定模型在不同数据集上的适用性和局限性。
- en: References
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] X. Xu, C. Liu, and D. Song, “Sqlnet: Generating structured queries from
    natural language without reinforcement learning,” 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] X. Xu, C. Liu, 和 D. Song，“Sqlnet: 从自然语言生成结构化查询无需强化学习，” 2018年。'
- en: '[2] T. Yu, Z. Li, Z. Zhang, R. Zhang, and D. Radev, “Typesql: Knowledge-based
    type-aware neural text-to-sql generation,” in *Proceedings of the 2018 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 2 (Short Papers)*, 2018, pp. 588–594.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] T. Yu, Z. Li, Z. Zhang, R. Zhang, 和 D. Radev，“Typesql: 基于知识的类型感知神经文本到SQL生成，”
    见于*2018年北美计算语言学协会人类语言技术会议论文集：短论文卷*，2018年，第588–594页。'
- en: '[3] B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson, “Rat-sql: Relation-aware
    schema encoding and linking for text-to-sql parsers,” in *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics*, 2020, pp. 7567–7578.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] B. Wang, R. Shin, X. Liu, O. Polozov, 和 M. Richardson，“Rat-sql: 关系感知模式编码与链接用于文本到SQL解析器，”
    见于*第58届计算语言学协会年会论文集*，2020年，第7567–7578页。'
- en: '[4] M. Hazoom, V. Malik, and B. Bogin, “Text-to-sql in the wild: A naturally-occurring
    dataset based on stack exchange data,” in *Proceedings of the 1st Workshop on
    Natural Language Processing for Programming (NLP4Prog 2021)*, 2021, pp. 77–87.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Hazoom, V. Malik, 和 B. Bogin，“野外的文本到SQL：基于Stack Exchange数据的自然发生数据集，”
    见于*第1届编程自然语言处理研讨会（NLP4Prog 2021）*，2021年，第77–87页。'
- en: '[5] D. H. Warren and F. C. Pereira, “An efficient easily adaptable system for
    interpreting natural language queries,” *American journal of computational linguistics*,
    vol. 8, no. 3-4, pp. 110–122, 1982.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. H. Warren 和 F. C. Pereira，“一种高效且易于适应的自然语言查询解释系统，” *美国计算语言学期刊*，第8卷，第3-4期，第110–122页，1982年。'
- en: '[6] A.-M. Popescu, O. Etzioni, and H. Kautz, “Towards a theory of natural language
    interfaces to databases,” in *Proceedings of the 8th international conference
    on Intelligent user interfaces*, 2003, pp. 149–157.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A.-M. Popescu, O. Etzioni 和 H. Kautz，“关于自然语言接口与数据库的理论探索”，发表于 *第8届国际智能用户界面会议论文集*，2003年，第149–157页。'
- en: '[7] Y. Li, H. Yang, and H. Jagadish, “Constructing a generic natural language
    interface for an xml database,” in *International Conference on Extending Database
    Technology*.   Springer, 2006, pp. 737–754.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Li, H. Yang 和 H. Jagadish，“为 XML 数据库构建通用自然语言接口”，发表于 *国际扩展数据库技术会议*。Springer，2006，第737–754页。'
- en: '[8] A. Giordani and A. Moschitti, “Translating questions to sql queries with
    generative parsers discriminatively reranked,” in *Proceedings of COLING 2012:
    Posters*, 2012, pp. 401–410.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Giordani 和 A. Moschitti，“通过生成解析器将问题翻译为 SQL 查询并进行判别性重新排序”，发表于 *COLING
    2012 会议论文集：海报*，2012年，第401–410页。'
- en: '[9] C. Wang, A. Cheung, and R. Bodik, “Synthesizing highly expressive sql queries
    from input-output examples,” in *Proceedings of the 38th ACM SIGPLAN Conference
    on Programming Language Design and Implementation*, 2017, pp. 452–466.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] C. Wang, A. Cheung 和 R. Bodik，“从输入输出示例中合成高度表达的 SQL 查询”，发表于 *第38届 ACM SIGPLAN
    程序设计语言设计与实现会议论文集*，2017年，第452–466页。'
- en: '[10] A. Kumar, M. Burch, and K. Mueller, “Visually comparing eye movements
    over space and time,” in *Proceedings of the 11th ACM Symposium on Eye Tracking
    Research & Applications*, ser. ETRA ’19.   New York, NY, USA: Association for
    Computing Machinery, 2019.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Kumar, M. Burch 和 K. Mueller，“视觉比较空间和时间上的眼动”，发表于 *第11届 ACM 眼动研究与应用研讨会论文集*，ETRA
    ’19 系列。纽约，NY，美国：计算机协会，2019年。'
- en: '[11] A. Kumar, “Visual analytics methodology for multivariate volumetric and
    spatio-temporal data,” Ph.D. dissertation, State University of New York at Stony
    Brook, 2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Kumar，“多变量体积数据和时空数据的视觉分析方法”，博士学位论文，纽约州立大学石溪分校，2020年。'
- en: '[12] A. Kumar, R. Netzel, M. Burch, D. Weiskopf, and K. Mueller, “Visual multi-metric
    grouping of eye-tracking data,” *Journal of eye movement research*, vol. 10, no. 5,
    pp. 10–16 910, 2018.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Kumar, R. Netzel, M. Burch, D. Weiskopf 和 K. Mueller，“视觉化多度量眼动数据分组”，*眼动研究期刊*，第10卷，第5期，第10–16 910页，2018年。'
- en: '[13] A. Kumar, B. Goel, K. R. Premkumar, M. Burch, and K. Mueller, “Eyefix:
    An interactive visual analytics interface for eye movement analysis,” in *The
    14th International Symposium on Visual Information Communication and Interaction*,
    2021, pp. 1–5.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. Kumar, B. Goel, K. R. Premkumar, M. Burch 和 K. Mueller，“Eyefix：一个用于眼动分析的互动视觉分析接口”，发表于
    *第14届国际视觉信息通信与互动研讨会*，2021年，第1–5页。'
- en: '[14] A. Kumar, D. Mohanty, K. Kurzhals, F. Beck, D. Weiskopf, and K. Mueller,
    “Demo of the eyesac system for visual synchronization, cleaning, and annotation
    of eye movement data,” in *ACM Symposium on Eye Tracking Research and Applications*,
    2020, pp. 1–3.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. Kumar, D. Mohanty, K. Kurzhals, F. Beck, D. Weiskopf 和 K. Mueller，“眼动数据的视觉同步、清理和注释的眼动系统演示”，发表于
    *ACM 眼动研究与应用研讨会*，2020年，第1–3页。'
- en: '[15] U. Brunner and K. Stockinger, “Valuenet: A natural language-to-sql system
    that learns from database information,” in *2021 IEEE 37th International Conference
    on Data Engineering (ICDE)*.   IEEE, 2021, pp. 2177–2182.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] U. Brunner 和 K. Stockinger，“Valuenet：一个从数据库信息中学习的自然语言到 SQL 系统”，发表于 *2021
    IEEE 第37届国际数据工程会议（ICDE）*。IEEE，2021，第2177–2182页。'
- en: '[16] V. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured queries
    from natural language using reinforcement learning,” *CoRR*, vol. abs/1709.00103,
    2017.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] V. Zhong, C. Xiong 和 R. Socher，“Seq2sql：使用强化学习从自然语言生成结构化查询”，*CoRR*，第 abs/1709.00103
    卷，2017年。'
- en: '[17] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao,
    S. Roman *et al.*, “Spider: A large-scale human-labeled dataset for complex and
    cross-domain semantic parsing and text-to-sql task,” in *Proceedings of the 2018
    Conference on Empirical Methods in Natural Language Processing*, 2018, pp. 3911–3921.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q.
    Yao, S. Roman *等*，“Spider：一个大规模人类标注的数据集，用于复杂和跨领域的语义解析及文本到 SQL 任务”，发表于 *2018年自然语言处理经验方法会议论文集*，2018年，第3911–3921页。'
- en: '[18] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, pp. 1735–1780, 1997.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Hochreiter 和 J. Schmidhuber，“长短期记忆”，*神经计算*，第9卷，第1735–1780页，1997年。'
- en: '[19] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” *Advances in neural information processing systems*, vol. 27,
    2014.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] I. Sutskever, O. Vinyals 和 Q. V. Le，“使用神经网络进行序列到序列学习”，*神经信息处理系统进展*，第27卷，2014年。'
- en: '[20] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder–decoder for
    statistical machine translation,” in *Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, 2014, pp. 1724–1734.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H.
    Schwenk, 和 Y. Bengio, “使用rnn编码器-解码器学习短语表示用于统计机器翻译”，发表于 *2014年自然语言处理经验方法会议论文集（EMNLP）*，2014,
    pp. 1724–1734。'
- en: '[21] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” *Advances in neural information processing systems*, vol. 27,
    2014.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] I. Sutskever, O. Vinyals, 和 Q. V. Le, “使用神经网络的序列到序列学习”，*神经信息处理系统进展*，vol.
    27, 2014。'
- en: '[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, 2019,
    pp. 4171–4186.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova, “Bert：深度双向变换器的预训练用于语言理解”，发表于
    *2019年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长篇和短篇论文）*，2019, pp. 4171–4186。'
- en: '[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, 和 I. Polosukhin, “注意力即你所需”，*神经信息处理系统进展*，vol. 30, 2017。'
- en: '[24] J. Krishnamurthy, P. Dasigi, and M. Gardner, “Neural semantic parsing
    with type constraints for semi-structured tables,” in *Proceedings of the 2017
    Conference on Empirical Methods in Natural Language Processing*, 2017, pp. 1516–1526.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Krishnamurthy, P. Dasigi, 和 M. Gardner, “带类型约束的神经语义解析用于半结构化表格”，发表于
    *2017年自然语言处理经验方法会议论文集*，2017, pp. 1516–1526。'
- en: '[25] L. Huang and H. Mi, “Efficient incremental decoding for tree-to-string
    translation,” in *Proceedings of the 2010 Conference on Empirical Methods in Natural
    Language Processing*, 2010, pp. 273–283.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] L. Huang 和 H. Mi, “树到字符串翻译的高效增量解码”，发表于 *2010年自然语言处理经验方法会议论文集*，2010, pp.
    273–283。'
- en: '[26] J. Kalajdjieski, M. Toshevska, and F. Stojanovska, “Recent advances in
    sql query generation: A survey,” in *Conference for Informatics and Information
    Technology*, 2020, p. 16.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Kalajdjieski, M. Toshevska, 和 F. Stojanovska, “SQL查询生成的最新进展：一项调查”，发表于
    *信息学与信息技术会议*，2020, p. 16。'
- en: '[27] D. A. Dahl, M. Bates, M. K. Brown, W. M. Fisher, K. Hunicke-Smith, D. S.
    Pallett, C. Pao, A. Rudnicky, and E. Shriberg, “Expanding the scope of the atis
    task: The atis-3 corpus,” in *Human Language Technology: Proceedings of a Workshop
    held at Plainsboro, New Jersey, March 8-11, 1994*, 1994.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] D. A. Dahl, M. Bates, M. K. Brown, W. M. Fisher, K. Hunicke-Smith, D.
    S. Pallett, C. Pao, A. Rudnicky, 和 E. Shriberg, “扩展atis任务的范围：atis-3语料库”，发表于 *人类语言技术：1994年3月8-11日在新泽西州Plainsboro举行的研讨会论文集*，1994。'
- en: '[28] L. R. Tang and R. J. Mooney, “Using multiple clause constructors in inductive
    logic programming for semantic parsing,” in *European Conference on Machine Learning*.   Springer,
    2001, pp. 466–477.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] L. R. Tang 和 R. J. Mooney, “在归纳逻辑编程中使用多重子句构造器进行语义解析”，发表于 *欧洲机器学习会议*，Springer,
    2001, pp. 466–477。'
- en: '[29] L. R. Tang and R. Mooney, “Automated construction of database interfaces:
    Intergrating statistical and relational learning for semantic parsing,” in *2000
    Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and
    Very Large Corpora*, 2000, pp. 133–141.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] L. R. Tang 和 R. Mooney, “自动化构建数据库接口：整合统计和关系学习进行语义解析”，发表于 *2000年SIGDAT自然语言处理与大规模语料库联合会议*，2000,
    pp. 133–141。'
- en: '[30] F. Li and H. V. Jagadish, “Constructing an interactive natural language
    interface for relational databases,” *Proceedings of the VLDB Endowment*, vol. 8,
    no. 1, pp. 73–84, 2014.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] F. Li 和 H. V. Jagadish, “为关系数据库构建交互式自然语言接口”，*VLDB捐赠会论文集*，vol. 8, no. 1,
    pp. 73–84, 2014。'
- en: '[31] M. P. Wasserman, “Properties and applications of the imdb film connections
    network,” Ph.D. dissertation, Northwestern University, 2015.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. P. Wasserman, “imdb电影连接网络的属性与应用”，博士论文，西北大学，2015。'
- en: '[32] S. Iyer, I. Konstas, A. Cheung, J. Krishnamurthy, and L. Zettlemoyer,
    “Learning a neural semantic parser from user feedback,” in *Proceedings of the
    55th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, 2017, pp. 963–973.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Iyer, I. Konstas, A. Cheung, J. Krishnamurthy, 和 L. Zettlemoyer, “从用户反馈中学习神经语义解析器”，收录于*第55届计算语言学协会年会（卷1：长篇论文）*，2017，第963–973页。'
- en: '[33] N. Yaghmazadeh, Y. Wang, I. Dillig, and T. Dillig, “Sqlizer: query synthesis
    from natural language,” *Proceedings of the ACM on Programming Languages*, vol. 1,
    no. OOPSLA, pp. 1–26, 2017.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] N. Yaghmazadeh, Y. Wang, I. Dillig, 和 T. Dillig, “Sqlizer: 从自然语言合成查询”，*ACM编程语言会议论文集*，第1卷，第OOPSLA期，第1–26页，2017。'
- en: '[34] C. Finegan-Dollak, J. K. Kummerfeld, L. Zhang, K. Ramanathan, S. Sadasivam,
    R. Zhang, and D. Radev, “Improving text-to-sql evaluation methodology,” in *Proceedings
    of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, 2018, pp. 351–360.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C. Finegan-Dollak, J. K. Kummerfeld, L. Zhang, K. Ramanathan, S. Sadasivam,
    R. Zhang, 和 D. Radev, “改进文本到SQL评估方法”，收录于*第56届计算语言学协会年会（卷1：长篇论文）*，2018，第351–360页。'
- en: '[35] T. Yu, R. Zhang, M. Yasunaga, Y. C. Tan, X. V. Lin, S. Li, H. Er, I. Li,
    B. Pang, T. Chen *et al.*, “Sparc: Cross-domain semantic parsing in context,”
    in *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, 2019, pp. 4511–4523.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Yu, R. Zhang, M. Yasunaga, Y. C. Tan, X. V. Lin, S. Li, H. Er, I. Li,
    B. Pang, T. Chen *等*，“Sparc: 上下文中的跨领域语义解析”，收录于*第57届计算语言学协会年会*，2019，第4511–4523页。'
- en: '[36] T. Yu, R. Zhang, H. Er, S. Li, E. Xue, B. Pang, X. V. Lin, Y. C. Tan,
    T. Shi, Z. Li *et al.*, “Cosql: A conversational text-to-sql challenge towards
    cross-domain natural language interfaces to databases,” in *Proceedings of the
    2019 Conference on Empirical Methods in Natural Language Processing and the 9th
    International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    2019, pp. 1962–1979.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Yu, R. Zhang, H. Er, S. Li, E. Xue, B. Pang, X. V. Lin, Y. C. Tan,
    T. Shi, Z. Li *等*，“Cosql: 面向跨领域自然语言接口的对话式文本到SQL挑战”，收录于*2019年自然语言处理经验方法会议与第9届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*，2019，第1962–1979页。'
- en: '[37] S. B. Roy, M. De Cock, V. Mandava, S. Savanna, B. Dalessandro, C. Perlich,
    W. Cukierski, and B. Hamner, “The microsoft academic search dataset and kdd cup
    2013,” in *Proceedings of the 2013 KDD cup 2013 workshop*, 2013, pp. 1–6.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. B. Roy, M. De Cock, V. Mandava, S. Savanna, B. Dalessandro, C. Perlich,
    W. Cukierski, 和 B. Hamner, “微软学术搜索数据集与KDD Cup 2013”，收录于*2013年KDD Cup 2013研讨会论文集*，2013，第1–6页。'
- en: '[38] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for
    word representation,” in *Proceedings of the 2014 conference on empirical methods
    in natural language processing (EMNLP)*, 2014, pp. 1532–1543.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Pennington, R. Socher, 和 C. D. Manning, “Glove: 全球词向量表示”，收录于*2014年自然语言处理（EMNLP）会议论文集*，2014，第1532–1543页。'
- en: '[39] C.-W. Goo, G. Gao, Y.-K. Hsu, C.-L. Huo, T.-C. Chen, K.-W. Hsu, and Y.-N.
    Chen, “Slot-gated modeling for joint slot filling and intent prediction,” in *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 2 (Short Papers)*, 2018, pp.
    753–757.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C.-W. Goo, G. Gao, Y.-K. Hsu, C.-L. Huo, T.-C. Chen, K.-W. Hsu, 和 Y.-N.
    Chen, “用于联合槽填充和意图预测的槽门控建模”，收录于*2018年北美计算语言学协会会议：人类语言技术（卷2：短篇论文）*，2018，第753–757页。'
- en: '[40] A. Graves, S. Fernandez, and J. Schmidhuber, “Bidirectional lstm networks
    for improved phoneme classification and recognition,” in *Proceedings of the 15th
    international conference on Artificial neural networks: formal models and their
    applications-Volume Part II*, 2005, pp. 799–804.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Graves, S. Fernandez, 和 J. Schmidhuber, “双向LSTM网络用于改进音素分类和识别”，收录于*第15届国际人工神经网络会议：正式模型及其应用-第II卷*，2005，第799–804页。'
- en: '[41] Q. Min, Y. Shi, and Y. Zhang, “A pilot study for chinese sql semantic
    parsing,” in *Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)*, 2019, pp. 3652–3658.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Q. Min, Y. Shi, 和 Y. Zhang, “中文SQL语义解析的初步研究”，收录于*2019年自然语言处理经验方法会议与第9届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*，2019，第3652–3658页。'
- en: '[42] B. Bogin, J. Berant, and M. Gardner, “Representing schema structure with
    graph neural networks for text-to-sql parsing,” in *Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics*, 2019, pp. 4560–4565.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] B. Bogin，J. Berant和M. Gardner，“使用图神经网络表示模式结构以进行文本到SQL解析”，在*计算语言学年会第57届年会论文集*中，2019年，第4560-4565页。'
- en: '[43] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE transactions on neural networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] F. Scarselli，M. Gori，A. C. Tsoi，M. Hagenbuchner和G. Monfardini，“图神经网络模型”，*IEEE神经网络交易*，第20卷，第1期，2008年，第61-80页。'
- en: '[44] X. Sun and W. Lu, “Understanding attention for text classification,” in
    *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*,
    2020, pp. 3418–3428.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] X. Sun和W. Lu，“理解文本分类的注意力”，在*计算语言学年会第58届年会论文集*中，2020年，第3418-3428页。'
- en: '[45] A. Elekes, A. Englhardt, M. Schaler, and K. Bohm, “Toward meaningful notions
    of similarity in nlp embedding models,” *International Journal on Digital Libraries*,
    vol. 21, no. 2, pp. 109–128, 2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Elekes，A. Englhardt，M. Schaler和K. Bohm，“NLP嵌入模型中有意义相似性概念的发展”，*国际数字图书馆杂志*，第21卷，第2期，2020年，第109-128页。'
- en: '[46] H. Gao, Z. Wang, and S. Ji, “Large-scale learnable graph convolutional
    networks,” in *Proceedings of the 24th ACM SIGKDD international conference on
    knowledge discovery & data mining*, 2018, pp. 1416–1424.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Gao，Z. Wang和S. Ji，“大规模可学习图卷积网络”，在*第24届ACM SIGKDD国际会议数据挖掘与知识发现*中，2018年，第1416-1424页。'
- en: '[47] S. Chen, A. San, X. Liu, and Y. Ji, “A tale of two linkings: Dynamically
    gating between schema linking and structural linking for text-to-sql parsing,”
    in *Proceedings of the 28th International Conference on Computational Linguistics*,
    2020, pp. 2900–2912.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] S. Chen，A. San，X. Liu和Y. Ji，“从结构链接到文本到SQL解析的动态门控：两个链接之间的故事”，在*计算语言学国际会议第28届论文集*中，2020年，第2900-2912页。'
- en: '[48] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, “A novel neural
    source code representation based on abstract syntax tree,” in *2019 IEEE/ACM 41st
    International Conference on Software Engineering (ICSE)*.   IEEE, 2019, pp. 783–794.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Zhang，X. Wang，H. Zhang，H. Sun，K. Wang和X. Liu，“基于抽象语法树的新型神经源代码表示”，在*2019
    IEEE / ACM 41届国际软件工程大会（ICSE）*中。 IEEE，2019年，第783-794页。'
- en: '[49] S. Chang, P. Liu, Y. Tang, J. Huang, X. He, and B. Zhou, “Zero-shot text-to-sql
    learning with auxiliary task,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 34, no. 05, 2020, pp. 7488–7495.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] S. Chang，P. Liu，Y. Tang，J. Huang，X. He和B. Zhou，“辅助任务下的零样本文本到SQL学习”，在*人工智能AAAI大会论文集*中，第34卷，第05期，2020年，第7488-7495页。'
- en: '[50] C.-Y. Chen, P.-H. Wang, S.-C. Chang, D.-C. Juan, W. Wei, and J.-Y. Pan,
    “Airconcierge: Generating task-oriented dialogue via efficient large-scale knowledge
    retrieval,” in *Findings of the Association for Computational Linguistics: EMNLP
    2020*, 2020, pp. 884–897.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] C.-Y. Chen，P.-H. Wang，S.-C. Chang，D.-C. Juan，W. Wei和J.-Y. Pan，“Airconcierge：通过高效的大规模知识检索生成面向任务的对话”，在*计算语言学协会会议发现EMNLP
    2020*中，2020年，第884-897页。'
- en: '[51] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder–decoder for
    statistical machine translation,” in *Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, 2014, pp. 1724–1734.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] K. Cho，B. van Merrienboer，C. Gulcehre，D. Bahdanau，F. Bougares，H. Schwenk和Y. Bengio，“使用RNN编码器-解码器学习短语表示进行统计机器翻译”，在*2014年经验方法会议自然语言处理（EMNLP）*中，2014年，第1724-1734页。'
- en: '[52] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint arXiv:1412.3555*,
    2014.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Chung，C. Gulcehre，K. Cho和Y. Bengio，“序列建模中门控循环神经网络的经验评估”，*arXiv预印本arXiv:1412.3555*，2014年。'
- en: '[53] Z. Yan, J. Ma, Y. Zhang, and J. Shen, “Sql generation via machine reading
    comprehension,” in *Proceedings of the 28th International Conference on Computational
    Linguistics*, 2020, pp. 350–356.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Z. Yan，J. Ma，Y. Zhang和J. Shen，“通过机器阅读理解生成SQL”，在*计算语言学国际会议第28届论文集*中，2020年，第350-356页。'
- en: '[54] V. Zhong, C. Xiong, and R. Socher, “Seq2SQL: Generating structured queries
    from natural language using reinforcement learning,” 2018\. [Online]. Available:
    https://openreview.net/forum?id=Syx6bz-Ab'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] V. Zhong，C. Xiong和R. Socher，“Seq2SQL：使用强化学习从自然语言生成结构化查询”，2018年。[链接](https://openreview.net/forum?id=Syx6bz-Ab)'
- en: '[55] L. Dong and M. Lapata, “Language to logical form with neural attention,”
    in *Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, 2016, pp. 33–43.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L. Dong 和 M. Lapata，“通过神经注意力将语言转化为逻辑形式，”发表在*第54届计算语言学协会年会（第一卷：长篇论文）*，2016年，页码
    33–43。'
- en: '[56] J. Guo, Z. Zhan, Y. Gao, Y. Xiao, J.-G. Lou, T. Liu, and D. Zhang, “Towards
    complex text-to-sql in cross-domain database with intermediate representation,”
    in *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, 2019, pp. 4524–4535.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Guo, Z. Zhan, Y. Gao, Y. Xiao, J.-G. Lou, T. Liu, 和 D. Zhang，“在跨域数据库中使用中间表示进行复杂的文本到SQL，”发表在*第57届计算语言学协会年会*，2019年，页码
    4524–4535。'
- en: '[57] Z. Yao, Y. Su, H. Sun, and W.-t. Yih, “Model-based interactive semantic
    parsing: A unified framework and a text-to-sql case study,” in *Proceedings of
    the 2019 Conference on Empirical Methods in Natural Language Processing and the
    9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    2019, pp. 5447–5458.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Z. Yao, Y. Su, H. Sun, 和 W.-t. Yih，“基于模型的交互式语义解析：一个统一框架和一个文本到SQL的案例研究，”发表在*2019年自然语言处理实证方法会议和第9届国际联合自然语言处理会议（EMNLP-IJCNLP）*，2019年，页码
    5447–5458。'
- en: '[58] X. V. Lin, R. Socher, and C. Xiong, “Bridging textual and tabular data
    for cross-domain text-to-sql semantic parsing,” in *Findings of the Association
    for Computational Linguistics: EMNLP 2020*, 2020, pp. 4870–4888.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] X. V. Lin, R. Socher, 和 C. Xiong，“桥接文本和表格数据进行跨域文本到SQL语义解析，”发表在*计算语言学协会发现：EMNLP
    2020*，2020年，页码 4870–4888。'
- en: '[59] R. Shin, “Encoding database schemas with relation-aware self-attention
    for text-to-sql parsers,” 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] R. Shin，“利用关系感知自注意力对文本到SQL解析器进行数据库模式编码，”2019年。'
- en: '[60] P. Yin and G. Neubig, “A syntactic neural model for general-purpose code
    generation,” in *Proceedings of the 55th Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, 2017, pp. 440–450.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] P. Yin 和 G. Neubig，“用于通用代码生成的句法神经模型，”发表在*第55届计算语言学协会年会（第一卷：长篇论文）*，2017年，页码
    440–450。'
- en: '[61] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” *Advances
    in neural information processing systems*, vol. 28, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] O. Vinyals, M. Fortunato, 和 N. Jaitly，“指针网络，”*神经信息处理系统进展*，第28卷，2015年。'
- en: '[62] K. Xuan, Y. Wang, Y. Wang, Z. Wen, and Y. Dong, “Sead: End-to-end text-to-sql
    generation with schema-aware denoising,” *arXiv preprint arXiv:2105.07911*, 2021.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. Xuan, Y. Wang, Y. Wang, Z. Wen, 和 Y. Dong，“Sead：具有模式感知去噪的端到端文本到SQL生成，”*arXiv预印本
    arXiv:2105.07911*，2021年。'
- en: '[63] B. Wang, W. Yin, X. V. Lin, and C. Xiong, “Learning to synthesize data
    for semantic parsing,” in *Proceedings of the 2021 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*,
    2021, pp. 2760–2766.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] B. Wang, W. Yin, X. V. Lin, 和 C. Xiong，“学习合成数据以进行语义解析，”发表在*2021年北美计算语言学协会会议：人类语言技术*，2021年，页码
    2760–2766。'
- en: '[64] R. Sennrich, B. Haddow, and A. Birch, “Improving neural machine translation
    models with monolingual data,” in *Proceedings of the 54th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers)*.   Berlin,
    Germany: Association for Computational Linguistics, 2016, pp. 86–96\. [Online].
    Available: https://aclanthology.org/P16-1009'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Sennrich, B. Haddow, 和 A. Birch，“利用单语数据改进神经机器翻译模型，”发表在*第54届计算语言学协会年会（第一卷：长篇论文）*。
    柏林，德国：计算语言学协会，2016年，页码 86–96。 [在线]。可用链接：https://aclanthology.org/P16-1009'
- en: '[65] Q. Liu, D. Yang, J. Zhang, J. Guo, B. Zhou, and J.-G. Lou, “Awakening
    latent grounding from pretrained language models for semantic parsing,” in *Findings
    of the Association for Computational Linguistics: ACL-IJCNLP 2021*, 2021, pp.
    1174–1189.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Q. Liu, D. Yang, J. Zhang, J. Guo, B. Zhou, 和 J.-G. Lou，“从预训练语言模型中唤醒潜在基础以进行语义解析，”发表在*计算语言学协会发现：ACL-IJCNLP
    2021*，2021年，页码 1174–1189。'
- en: '[66] A. Kumar, P. Howlader, R. Garcia, D. Weiskopf, and K. Mueller, “Challenges
    in interpretability of neural networks for eye movement data,” in *ACM Symposium
    on Eye Tracking Research and Applications*, 2020, pp. 1–5.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Kumar, P. Howlader, R. Garcia, D. Weiskopf, 和 K. Mueller，“眼动数据神经网络可解释性挑战，”发表在*ACM眼动研究与应用研讨会*，2020年，页码
    1–5。'
- en: '[67] T. Yu, M. Yasunaga, K. Yang, R. Zhang, D. Wang, Z. Li, and D. Radev, “Syntaxsqlnet:
    Syntax tree networks for complex and cross-domain text-to-sql task,” in *Proceedings
    of the 2018 Conference on Empirical Methods in Natural Language Processing*, 2018,
    pp. 1653–1663.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] T. Yu, M. Yasunaga, K. Yang, R. Zhang, D. Wang, Z. Li, 和 D. Radev，“Syntaxsqlnet:
    复杂及跨领域文本到 SQL 任务的语法树网络，” 见 *2018 年自然语言处理经验方法会议论文集*，2018 年，页 1653–1663。'
- en: '[68] C. Wang, K. Tatwawadi, M. Brockschmidt, P.-S. Huang, Y. Mao, O. Polozov,
    and R. Singh, “Robust text-to-sql generation with execution-guided decoding,”
    *arXiv preprint arXiv:1807.03100*, 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] C. Wang, K. Tatwawadi, M. Brockschmidt, P.-S. Huang, Y. Mao, O. Polozov,
    和 R. Singh，“基于执行引导解码的鲁棒文本到 SQL 生成，” *arXiv 预印本 arXiv:1807.03100*，2018 年。'
- en: '[69] C. Wang, M. Brockschmidt, and R. Singh, “Pointing out sql queries from
    text.”'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] C. Wang, M. Brockschmidt, 和 R. Singh，“从文本中提取 SQL 查询。”'
- en: '[70] T. Guo and H. Gao, “Content enhanced bert-based text-to-sql generation,”
    2019\. [Online]. Available: https://arxiv.org/abs/1910.07179'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] T. Guo 和 H. Gao，“内容增强的基于 BERT 的文本到 SQL 生成，” 2019\. [在线]. 网址: https://arxiv.org/abs/1910.07179'
- en: '[71] T. Yu, C.-S. Wu, X. V. Lin, B. Wang, Y. C. Tan, X. Yang, D. R. Radev,
    R. Socher, and C. Xiong, “Grappa: Grammar-augmented pre-training for table semantic
    parsing,” in *ICLR*, 2021.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] T. Yu, C.-S. Wu, X. V. Lin, B. Wang, Y. C. Tan, X. Yang, D. R. Radev,
    R. Socher, 和 C. Xiong，“Grappa: 语法增强的表语义解析预训练，” 见 *ICLR*，2021 年。'
- en: '[72] Y. Zhang, X. Dong, S. Chang, T. Yu, P. Shi, and R. Zhang, “Did you ask
    a good question? a cross-domain question intention classification benchmark for
    text-to-sql,” *arXiv preprint arXiv:2010.12634*, 2020.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Zhang, X. Dong, S. Chang, T. Yu, P. Shi, 和 R. Zhang，“你问了一个好的问题吗？一个跨领域问题意图分类基准用于文本到
    SQL，” *arXiv 预印本 arXiv:2010.12634*，2020 年。'
- en: '[73] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining
    approach,” 2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, 和 V. Stoyanov，“Roberta: 一种鲁棒优化的 BERT 预训练方法，” 2019 年。'
- en: '[74] D. Choi, M. C. Shin, E. Kim, and D. R. Shin, “Ryansql: Recursively applying
    sketch-based slot fillings for complex text-to-sql in cross-domain databases,”
    *Computational Linguistics*, vol. 47, no. 2, pp. 309–332, 2021.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] D. Choi, M. C. Shin, E. Kim, 和 D. R. Shin，“Ryansql: 递归应用基于草图的槽位填充用于复杂的跨领域文本到
    SQL 任务，” *计算语言学*，第 47 卷，第 2 期，页 309–332，2021 年。'
- en: '[75] X. Zhang, F. Yin, G. Ma, B. Ge, and W. Xiao, “F-sql: fuse table schema
    and table content for single-table text2sql generation,” *IEEE Access*, vol. 8,
    pp. 136 409–136 420, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] X. Zhang, F. Yin, G. Ma, B. Ge, 和 W. Xiao，“F-sql: 融合表模式和表内容用于单表文本到 SQL
    生成，” *IEEE Access*，第 8 卷，页 136 409–136 420，2020 年。'
