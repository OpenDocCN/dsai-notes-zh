- en: 'Machine Learning 1: Lesson 9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第9课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-9-689bbc828fd2](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-9-689bbc828fd2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-9-689bbc828fd2](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-9-689bbc828fd2)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从*[*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*中的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*[*Jeremy*](https://twitter.com/jeremyphoward)*和*[*Rachel*](https://twitter.com/math_rachel)*给了我这个学习的机会。*'
- en: Students’ work [[0:00](https://youtu.be/PGC0UxakTvM)]
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学生的作品[[0:00](https://youtu.be/PGC0UxakTvM)]
- en: 'Welcome back to machine learning! I am really excited to be able to share some
    amazing stuff that University of San Francisco students have built or written
    about during the week. Quite a few things I’m going to show you have already spread
    around the internet quite a bit: lots of tweets and posts and all kinds of stuff
    happening.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到机器学习！我非常兴奋能够分享一些由旧金山大学学生在这一周内构建或撰写的惊人内容。我将向你展示的许多东西已经在互联网上广泛传播：大量的推文和帖子以及各种各样的事情发生。
- en: '[Coloring With Random Forests](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)'
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[用随机森林着色](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)'
- en: by [Tyler White](https://twitter.com/Tyler_V_White)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Tyler White](https://twitter.com/Tyler_V_White)提供
- en: He started out by saying what if I create a synthetic dataset where the independent
    variables are the x and the y, and the dependent variable was color. Interestingly,
    he showed me an earlier version of this where he wasn’t using color. He was just
    putting the actual numbers in here.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 他开始说，如果我创建一个合成数据集，其中自变量是x和y，因变量是颜色，会怎样。有趣的是，他向我展示了一个早期版本，那时他没有使用颜色。他只是把实际的数字放在这里。
- en: '![](../Images/c99b65f92789190055215515bf529170.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c99b65f92789190055215515bf529170.png)'
- en: And this thing wasn’t really working at all. As soon as he started using color,
    it started working really well. So I wanted to mention that one of the things
    that unfortunately we don’t teach you at USF is a theory of human perception,
    perhaps we should. Because actually when it comes to visualization, the most important
    thing to know is what is the human eye or brain good at perceiving. There is a
    whole area of academic study on this. And one of the things that we’re best at
    perceiving is differences in color. So that’s why as soon as we look at this picture
    of this synthetic data he created, you can immediately see, oh there is kind of
    four areas of lighter red color. What he did was, he said okay, what if we tried
    to create a machine learning model of this synthetic dataset and specifically
    he created a tree. And the cool thing is that you can actually draw the tree.
    So after he created the tree, he did this all in matplotlib. Matplotlib is very
    flexible. He actually drew the tree boundaries which is already a pretty neat
    trick — to be able to actually draw the tree.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个东西一开始根本不起作用。一旦他开始使用颜色，它就开始运行得非常好。所以我想提一下，不幸的是我们在USF没有教给你的一件事是人类感知的理论，也许我们应该。因为实际上，当涉及到可视化时，最重要的事情是了解人眼或大脑擅长感知的是什么。关于这个有一个整个学术研究领域。我们最擅长感知的事情之一就是颜色的差异。这就是为什么当我们看这张他创建的合成数据的图片时，你可以立刻看到，哦，这里有四个较浅红色的区域。他所做的是，他说好，如果我们尝试创建一个关于这个合成数据集的机器学习模型，具体来说他创建了一棵树。而酷的事情是你实际上可以绘制这棵树。所以在他创建了这棵树之后，他在matplotlib中完成了所有这些。Matplotlib非常灵活。他实际上绘制了树的边界，这已经是一个相当不错的技巧——能够实际绘制这棵树。
- en: '![](../Images/a707efcb16cb9661c302cb899f6c2c8a.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a707efcb16cb9661c302cb899f6c2c8a.png)'
- en: Then he did something even cleverer which is he said okay, so what predictions
    does the tree make? Well, it’s the average of each of these areas and so to do
    that, we can actually draw the average color. That is actually pretty. Here are
    the predictions the tree makes. Now here is where it gets really interesting.
    You can, as you know, randomly generate trees through resampling and so here are
    four trees generated through resampling. They are all pretty similar but a little
    bit different.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后他做了更聪明的事情，他说好的，那么树做出了什么预测？嗯，这是每个区域的平均值，所以为了做到这一点，我们实际上可以绘制平均颜色。这实际上相当漂亮。这是树所做的预测。现在这里变得非常有趣。你可以，如你所知，通过重新采样随机生成树，所以这里有通过重新采样生成的四棵树。它们都非常相似但略有不同。
- en: '![](../Images/a33d2b53173180f3e482991393161888.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a33d2b53173180f3e482991393161888.png)'
- en: 'So now we can actually visualize bagging and to visualize bagging, we literally
    take the average of the four pictures. That’s what bagging is. And there it is:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实际上可以可视化装袋，为了可视化装袋，我们简单地取四张图片的平均值。这就是装袋。就是这样：
- en: '![](../Images/1a9339d0c880167afa28878d32123328.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a9339d0c880167afa28878d32123328.png)'
- en: So here is the fuzzy decision boundaries of a random forest and I think this
    is kind of amazing. Because it’s like, I wish I had this actually when I started
    teaching you all random forest because I could have skipped a couple of classes.
    It’s just like “okay, that’s what we do”. We create the decision boundaries, we
    average each area, then we do it a few times and average all of them. So that’s
    what a random forest does and I think this is just such a great example of making
    the complex easy through pictures. So congrats to Tyler for that. It actually
    turns out that he actually reinvented something that somebody else has already
    done. A guy called Christian Innie(?) who went on to be one of the world’s foremost
    machine learning researchers actually included almost exactly this technique in
    a book he wrote about decision forests. So it’s actually kind of cool that Tyler
    ended up reinventing something that one of the world’s foremost authorities on
    decision forest actually has created. So I thought that was neat. It’s nice because
    when we posted this on Twitter, got a lot of attention and finally somebody was
    able to say “oh, you know what, this actually already exists.” So Tyler has gone
    away and started reading that book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个随机森林的模糊决策边界，我觉得这很神奇。因为这就像，我真希望在我开始教你们随机森林的时候有这个，我本来可以跳过几节课的。就像“好的，这就是我们做的”。我们创建决策边界，对每个区域进行平均，然后重复几次并对所有结果进行平均。这就是随机森林的做法，我认为这只是一个通过图片将复杂问题变得简单的很好的例子。所以恭喜Tyler。事实上，他实际上重新发明了别人已经做过的事情。一个叫Christian
    Innie(?)的人，他后来成为了世界上最重要的机器学习研究者之一，实际上在他写的一本关于决策森林的书中几乎完全包含了这个技术。所以Tyler最终重新发明了一个世界上最重要的决策森林专家创造的东西，这实际上很酷。我觉得这很有趣。很好，因为当我们在Twitter上发布这个时，引起了很多关注，最终有人能够说“哦，你知道吗，这实际上已经存在了”。所以Tyler已经开始阅读那本书。
- en: '[Parfit — quick and powerful hyper-parameter optimization with visualizations](/mlreview/parfit-hyper-parameter-optimization-77253e7e175e)
    [[4:16](https://youtu.be/PGC0UxakTvM?t=256)]'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Parfit - 快速而强大的超参数优化与可视化](/mlreview/parfit-hyper-parameter-optimization-77253e7e175e)
    [[4:16](https://youtu.be/PGC0UxakTvM?t=256)]'
- en: by [Jason Carpenter](/@jmcarpenter2)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Jason Carpenter](/@jmcarpenter2)
- en: Something else which is super cool is Jason Carpenter created a whole new library
    called Parfit. Parfit is a parallelized fitting of multiple models for the purpose
    of selecting hyper parameters. There’s a lot I really like about this. He’s shown
    a clear example of how to use it, and the API looks very similar to other grid
    search based approaches, but it uses the validation techniques that Rachel wrote
    about and that we learned about a couple weeks ago of using a good validation
    set. And what he’s done here is, in his blog post that introduces it, he’s gone
    right back and said what are hyper parameters, why do we have to train them, and
    he’s kind of explained every step. And then the module itself is very polished.
    He added documentation to it, he’s added a nice README to it. And it’s kind of
    interesting when you actually look at the code, you realize it’s very simple which
    is definitely not a bad thing ,that’s a good thing to make things simple. But
    writing this little bit of code and then packaging it up so nicely, he’s made
    it really easy for other people to use this technique which is great.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件很酷的事情是Jason Carpenter创建了一个全新的库叫做Parfit。Parfit是为了选择超参数而并行拟合多个模型。我真的很喜欢这个。他展示了如何使用它的清晰示例，API看起来非常类似于其他基于网格搜索的方法，但它使用了Rachel写的验证技术，我们几周前学到的使用一个好的验证集。他在介绍它的博客文章中，回顾了什么是超参数，为什么我们必须训练它们，并解释了每一步。然后这个模块本身非常完善。他为其添加了文档，为其添加了一个很好的README。当你实际看代码时，你会发现它非常简单，这绝对不是坏事，简单是好事。通过编写这段代码并将其打包得如此完美，他使其他人使用这个技术变得非常容易，这很棒。
- en: '[How to make SGD Classifier perform as well as Logistic Regression using parfit](https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4)
    [[5:40](https://youtu.be/PGC0UxakTvM?t=340)]'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[如何使用parfit使SGD分类器表现得和逻辑回归一样好](https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4)
    [[5:40](https://youtu.be/PGC0UxakTvM?t=340)]'
- en: by [Vinay Patlolla](https://towardsdatascience.com/@vinnsvinay)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Vinay Patlolla](https://towardsdatascience.com/@vinnsvinay)
- en: 'One of the things I’ve been really thrilled to see is then Vinay went along
    and combined two things from our class: one was to take Parfit and then the other
    was to take the accelerated SGD approach to classification we learned about in
    the last lesson and combine the two to say “okay, let’s now use Parfit to help
    us find the parameters of a SGD logistic regression.” So I think that’s really
    a great idea.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的很高兴看到的一件事是Vinay继续结合了我们课堂上学到的两件事：一是使用Parfit，另一是使用我们在上一课中学到的加速SGD分类方法，将两者结合起来，说“好的，现在让我们使用Parfit来帮助我们找到SGD逻辑回归的参数”。我认为这真的是一个很好的主意。
- en: '[Intuitive Interpretation of Random Forest](/usf-msds/intuitive-interpretation-of-random-forest-2238687cae45)
    [[6:14](https://youtu.be/PGC0UxakTvM?t=374)]'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[随机森林的直观解释](/usf-msds/intuitive-interpretation-of-random-forest-2238687cae45)
    [[6:14](https://youtu.be/PGC0UxakTvM?t=374)]'
- en: by [Prince Grover](/@pgrover3)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Prince Grover](/@pgrover3)
- en: Something else which I thought was terrific is Prince basically went through
    and summarized pretty much all the stuff we learnt in the random forest interpretation
    plus. He went even further than that as he described each of the different approaches
    to random forest interpretation. He described how it’s done so here , for example,
    is feature importance through variable permutation, a little picture of each one,
    and then super cool, here is the code to implement it from scratch. I think this
    is a really nice post describing something that not many people understand and
    showing exactly how it works both with pictures and with code that implements
    it from scratch. So I think that’s really great. One of the things I really like
    here is that for the tree interpreter, he actually showed how you can take the
    tree interpreter output and feed it into the new waterfall chart package that
    Chris, a USF student, built to show how you can actually visualize the contribution
    of the tree interpreter in a water fall chart. So again, kind of a nice combination
    of multiple pieces of technology we both learned about and built as a group.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为很棒的另一件事是，Prince基本上总结了我们在随机森林解释中学到的几乎所有内容。他甚至比那更进一步，因为他描述了随机森林解释的每种不同方法。他描述了如何做到这一点，例如，通过变量置换来计算特征重要性，每个方法都有一个小图片，然后非常酷，这里是从头开始实现它的代码。我认为这是一篇非常好的文章，描述了很多人不理解的东西，并且准确展示了它是如何工作的，既有图片又有实现代码。所以我认为这真的很棒。我在这里真的很喜欢的一件事是，对于树解释器，他实际上展示了如何将树解释器的输出输入到由USF学生Chris构建的新瀑布图包中，以展示如何实际上在瀑布图中可视化树解释器的贡献。所以再次，这是一种很好的结合我们学习和作为一个团队构建的多种技术。
- en: '[Keras Model for Beginners (0.210 on LB)+EDA+R&D](https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d)
    [[7:37](https://youtu.be/PGC0UxakTvM?t=457)]'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Keras Model for Beginners (0.210 on LB)+EDA+R&D](https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d)
    [[7:37](https://youtu.be/PGC0UxakTvM?t=457)]'
- en: by [Davesh Maheshwari](https://www.kaggle.com/devm2024)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Davesh Maheshwari](https://www.kaggle.com/devm2024)提供
- en: There’s been a few interesting kernels share and I’ll share more next week,
    Davesh wrote this really nice kernel showing, this is quite challenging Kaggle
    competition on detecting icebergs vs. ships. And it’s kind of a weird two channel
    satellite data which is very hard to visualize and he actually went through and
    basically described the formulas for how these radar scattering things actually
    work, then actually managed to come up with a code taht allowed him to recreate
    the actual 3D icebergs or ships. I have not seen that done before. It’s quite
    challenging to know how to visualize this data. Then he went on to show how to
    build a neural net to try to interpret this so that was pretty fantastic as well.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些有趣的内核分享，下周我会分享更多，Davesh写了这篇非常好的内核，展示了在检测冰山与船只的Kaggle竞赛中的挑战。这是一种很难可视化的奇怪的双通道卫星数据，他实际上通过并基本上描述了这些雷达散射的工作原理的公式，然后实际上设法编写了一个代码，使他能够重新创建实际的3D冰山或船只。我以前没有见过这样做。如何可视化这些数据是非常具有挑战性的。然后他继续展示如何构建神经网络来尝试解释这一点，这也非常棒。
- en: SGD [[9:53](https://youtu.be/PGC0UxakTvM?t=593)]
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SGD [[9:53](https://youtu.be/PGC0UxakTvM?t=593)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
- en: Let’s go back to SGD. So we are going back through this notebook which Rachel
    put together basically taking us through SGD from scratch for the purpose of digit
    recognition. Actually quite a lot of stuff we look at today is going to be closely
    following part of the computational linear algebra course which you can both find
    the [MOOCs](https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY)
    on fast.ai or at USF it’ll be an elective next year. So if you find some of this
    stuff interesting and I hope you do, then please consider signing up for the elective
    or checking out the video online.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到SGD。所以我们正在回顾这份笔记本，Rachel基本上带领我们从头开始学习SGD，目的是进行数字识别。实际上，今天我们看的很多东西都将紧随计算线性代数课程的一部分，你可以在fast.ai上找到[MOOCs](https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY)，或者在USF，它将成为明年的选修课。所以如果你觉得这些东西有趣，我希望你会考虑报名参加选修课或在线观看视频。
- en: So we are building neural networks. And we are starting with an assumption that
    we’ve downloaded the MNIST data, we’ve normalized it by subtracting the main and
    divided by the standard deviation. The data is slightly unusual in that although
    they represent images, they were downloaded as each image being 784 long rank
    1 tensor, so it’s been flattened out. For the purpose of drawing pictures of it,
    we had to resize it to 28 by 28\. But the actual data we’ve got is not 28 by 28,
    it’s 784 long flattened out.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们正在构建神经网络。我们假设已经下载了MNIST数据，通过减去平均值并除以标准差对其进行了标准化。这些数据略有不同，虽然它们代表图像，但它们被下载为每个图像都是784个长的秩为1的张量，因此已经被展平。为了绘制它的图片，我们必须将其调整为28x28。但实际的数据不是28x28，而是784个长的展平数据。
- en: 'The basic steps we’re going to take here is to start out with training the
    world’s simplest neural network basically a logistic regression [[11:43](https://youtu.be/PGC0UxakTvM?t=703)].
    So no hidden layers. And we’re going to train it using a library, Fast AI, and
    we’re going to build the network using a library PyTorch. Then we are going to
    gradually get rid of all the libraries. So first of all, we’ll get rid of the
    `nn` (neural net) library in PyTorch and write that ourselves. Then we’ll get
    rid of the Fast AI `fit` function and write that ourselves. And then we’ll get
    rid of the PyTorch optimizer and write that ourselves. So by the end of this notebook,
    we’ll have written all the pieces ourselves. The only thing that we’ll end up
    relying on is the two key things that PyTorch gives us which is:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要采取的基本步骤是从训练世界上最简单的神经网络开始，基本上是一个逻辑回归。因此没有隐藏层。我们将使用一个名为Fast AI的库进行训练，并使用一个名为PyTorch的库构建网络。然后我们将逐渐摆脱所有库。首先，我们将摆脱PyTorch中的`nn`（神经网络）库，并自己编写。然后我们将摆脱Fast
    AI的`fit`函数，并自己编写。然后我们将摆脱PyTorch的优化器，并自己编写。因此，在本笔记本的最后，我们将自己编写所有部分。我们最终依赖的唯一两个PyTorch给我们的关键事物是：
- en: the ability to write Python code and have it run it on the GPU
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有编写Python代码并在GPU上运行的能力
- en: the ability to write Python code and have it automatically differentiated for
    us.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有编写Python代码并使其自动为我们进行微分的能力。
- en: So they are the two things we are not going to attempt to write ourselves because
    it’s boring and pointless. But everything else, we’ll try and write ourselves
    on top of those two things.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这两件事我们不打算自己尝试编写，因为这很无聊且毫无意义。但除此之外，我们将尝试在这两件事的基础上自己编写其他所有内容。
- en: Our starting point is not doing anything ourselves. It’s basically having it
    all done for us. So PyTorch has `nn` library which is where the neural net stuff
    lives. You can create a multi-layer neural network by using the `Sequential` function
    and then passing in a list of the layers that you want and we asked for a linear
    layer followed by a softmax layer and that defines our logistic regression.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的起点不是自己做任何事情。基本上所有事情都已经为我们完成。因此，PyTorch有一个`nn`库，其中包含神经网络的内容。您可以通过使用`Sequential`函数创建一个多层神经网络，然后传入您想要的层的列表，我们要求一个线性层，然后是一个softmax层，这定义了我们的逻辑回归。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The input to our linear layer is 28 by 28 as we just discussed, the output is
    10 because we want a probability for each of the numbers naught through 9 for
    each of our images. `.cuda()` sticks it on the GPU and then `fit` fits a model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们线性层的输入是28乘以28，输出是10，因为我们希望为我们的图像中的每个数字0到9之间的每个数字获得一个概率。`.cuda()`将其放在GPU上，然后`fit`拟合模型。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So we start out with a random set of weights then fit uses gradient descent
    to make it better. We had to tell the fit function what criterion to use, in other
    words, what counts as better and we told it to use negative log likelihood. We’ll
    learn about that in the next lesson what that is exactly. We had to tell it what
    optimizer to use and we said please use `optim.Adam`, the details of that we won’t
    cover in this course. We are going to build something simpler called SGD. If you
    are interested in Adam, we just covered that in the deep learning course. And
    what metrics do you want to print out, we decided to print out accuracy. So that
    was that. So after we fit it, we get an accuracy of generally somewhere around
    91, 92%.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从一组随机权重开始，然后使用梯度下降进行拟合以使其更好。我们必须告诉拟合函数要使用什么标准，换句话说，什么算作更好，我们告诉它使用负对数似然。我们将在下一课中了解这是什么。我们必须告诉它要使用什么优化器，我们说请使用`optim.Adam`，这方面的细节我们在本课程中不会涉及。我们将构建一个更简单的称为SGD的东西。如果您对Adam感兴趣，我们刚刚在深度学习课程中涵盖了这一点。您想要打印出什么指标，我们决定打印出准确率。就是这样。所以在我们拟合后，我们通常会得到大约91、92%的准确率。
- en: Defining Module [[14:47](https://youtu.be/PGC0UxakTvM?t=887)]
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模块
- en: What we are going to do from here is, we are going to repeat this exact same
    thing, so we are going to rebuild this model 4 or 5 times, building it and fitting
    it with less and less libraries. So the second thing that we did lat time was
    to try to start to define the module ourselves. So instead of saying the network
    is a sequential bunch of these layers, let’s not use that library at all and try
    and define it ourselves from scratch. To do that, we have to use OO because that’s
    how we build everything in PyTorch. And we have to create a class which inherits
    from `nn.Module`. So `nn.Module` is a PyTorch class that takes our class and turns
    it into a neural network module, which basically means anything that you inherit
    from `nn.Module` like this, you can pretty much insert into a neural network as
    a layer or you can treat it as a neural network. It is going to get all the stuff
    that it needs automatically to work as a part of or a full neural network. Now
    we’ll talk about exactly what that means today and the next lesson.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要做的是，我们将重复这完全相同的事情，所以我们将重建这个模型4到5次，逐渐减少使用的库。我们上次做的第二件事是尝试开始自己定义模块。所以我们不再使用那个库，而是尝试从头开始自己定义它。为了做到这一点，我们必须使用面向对象，因为这是我们在PyTorch中构建所有东西的方式。我们必须创建一个类，该类继承自`nn.Module`。所以`nn.Module`是一个PyTorch类，它接受我们的类并将其转换为神经网络模块，这基本上意味着您从`nn.Module`继承的任何东西，您几乎可以将其插入到神经网络中作为一个层，或者您可以将其视为一个神经网络。它将自动获得作为神经网络的一部分或整个神经网络运行所需的所有内容。现在我们将在今天和下一课中详细讨论这意味着什么。
- en: So we need to construct the object so that means we need to define the constructor
    dunder init. And importantly, this is a Python thing, if you inherit from some
    other object, then you have to create the thing you inherit from first. So when
    you say `super().__init__()` , that says construct the `nn.Module` piece of that
    first. If you don’t do that, then the `nn.Module` stuff never gets a chance to
    actually get constructed. So this is just like a standard Python OO subclass constructor.
    If any of that’s unclear to you, then you know this is where you definitely want
    to just grab a Python intro to OO because this is the standard approach.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们需要构建这个对象，这意味着我们需要定义构造函数dunder init。重要的是，这是一个Python的东西，如果你继承自其他对象，那么你首先必须创建你继承的东西。因此当你说`super().__init__()`时，这意味着首先构建那个`nn.Module`部分。如果你不这样做，那么`nn.Module`的东西就永远没有机会被实际构建。因此这就像一个标准的Python面向对象子类构造函数。如果其中有任何地方让你感到困惑，那么你知道这就是你绝对需要抓住一个Python面向对象的入门，因为这是标准的方法。
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So inside our constructor, we want to do the equivalent of `nn.Linear` [[17:06](https://youtu.be/PGC0UxakTvM?t=1026)].
    What `nn.Linear` is doing is it’s taking our 28 by 28 vector, so 784 long vector,
    and that’s going to be the input to a matrix multiplication. So now we need to
    create a something with 784 rows and 10 columns. Because the input to this is
    going to be a mini batch of size 64 by 784\. So we are going to do this matrix
    product. So when we say in PyTorch `nn.Linear`, it’s going to construct 784 by
    10 matrix for us. So since we are not using that, we are doing things from scratch,
    we need to make it ourselves. To make it ourselves, we can say generate normal
    random numbers with this dimensionality `torch.randn(dims)` which we passed in
    here `28*28, 10`. So that gets us our randomly initialized matrix.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的构造函数中，我们想要做的相当于`nn.Linear`。`nn.Linear`所做的是，它将我们的28乘以28的向量，即784个元素的向量，作为矩阵乘法的输入。因此，现在我们需要创建一个具有784行和10列的矩阵。因为这个的输入将是一个大小为64乘以784的小批量数据。因此我们将进行这个矩阵乘法运算。因此当我们在PyTorch中说`nn.Linear`时，它将为我们构建一个784乘以10的矩阵。因此，由于我们没有使用它，我们是从头开始做事情，我们需要自己制作它。为了自己制作它，我们可以说生成具有这种维度的正态随机数`torch.randn(dims)`，我们在这里传入了`28*28,
    10`。这样我们就得到了我们随机初始化的矩阵。
- en: Then we want to add on to this. We don’t just want *y = ax*, we want *y = ax
    + b*. So we need to add on what we call in neural nets a bbias vector. So we create
    here a bias vector of length 10 `self.l1_b = get_weights(10)`, again randomly
    initialized and so now here we are our two randomly initialized weight tensors.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们想要添加到这个。我们不只是想要 *y = ax*，我们想要 *y = ax + b*。所以我们需要添加在神经网络中称为偏置向量的东西。因此，我们在这里创建一个长度为10的偏置向量
    `self.l1_b = get_weights(10)`，同样是随机初始化的，所以现在我们有了两个随机初始化的权重张量。
- en: '![](../Images/21ca5b1ad41c51e5086c197245455b34.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21ca5b1ad41c51e5086c197245455b34.png)'
- en: So that’s our constructor. Now we need to define forward. Why do we need define
    forward? This is a PyTorch specific thing. What’s going to happen is when you
    create a module in PyTorch, the object that you get back behave as if it’s a function.
    You can call it with parentheses which we will do in a moment. And so you need
    to somehow define what happens when you call it as if it’s a function and the
    answer is PyTorch calls a method called forward. That’s the PyTorch approach they
    picked. So when it calls forward, we need to do our actual calculation of the
    output of this module or a layer. So here is the thing that actually gets calculated
    in a logistic regression. So basically we take our input x which gets passed to
    forward — that’s basically how forward works, it gets passed the mini batch, and
    we matrix multiply it by the layer one weights which we defined in the constructor.
    Then we add on the layer one bias which we also defined in the constructor. Actually
    nowadays we can define this a little bit more elegantly using the Python 3 matrix
    multiplication operator which is the `@` sign.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的构造函数。现在我们需要定义前向传播。为什么我们需要定义前向传播？这是一个PyTorch特有的东西。当你在PyTorch中创建一个模块时，你得到的对象会表现得好像它是一个函数。你可以用括号调用它，我们马上就会这样做。因此，你需要以某种方式定义当你调用它时会发生什么，就好像它是一个函数，答案是PyTorch调用一个叫做forward的方法。这是他们选择的PyTorch方法。所以当它调用forward时，我们需要做这个模块或层的输出的实际计算。这就是在逻辑回归中实际计算的东西。基本上我们取得我们的输入x，它被传递给forward
    —— 这基本上是forward的工作原理，它接收到小批量数据，并且我们将其与构造函数中定义的第一层权重进行矩阵乘法运算。然后我们加上构造函数中也定义的第一层偏置。实际上，现在我们可以更加优雅地使用Python
    3的矩阵乘法运算符`@`来定义这个。
- en: '![](../Images/add1946d0bb9c9ac4348a570bcb6535f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add1946d0bb9c9ac4348a570bcb6535f.png)'
- en: When you use that, I think you kind of end up with something that looks closer
    to what the mathematical notation looked like and so I find that nicer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用它时，我认为你最终会得到更接近数学符号的样子，所以我觉得这更好看。
- en: Alright, so that’s our linear layer in our logistic regression (i.e. our zero
    hidden layer neural net). So then the next thing we do to that is softmax. We
    get the output of this matrix multiply which has the dimension 64 by 10\. We get
    this matrix of outputs and we put this through a softmax. Why do we put it through
    a softmax? We put it through a softmax because in the end, for every image, we
    want a probability that is a 0, a 1, or a 2, or 3, or 4\. So we want a bunch of
    probabilities that add up to 1 where each of those probability is between 0 and
    1\. So a softmax does exactly that for us.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这就是我们逻辑回归中的线性层（即我们的零隐藏层神经网络）。然后我们对其进行softmax。我们得到这个矩阵乘法的输出，它的维度是64乘以10。我们得到这个输出矩阵，然后我们通过softmax函数处理它。为什么我们要通过softmax函数处理它？我们要通过softmax函数处理它是因为最终，对于每个图像，我们希望得到一个概率，它是0、1、2、3或4。所以我们希望得到一堆概率，它们加起来等于1，其中每个概率都在0到1之间。所以softmax函数正好为我们做到了这一点。
- en: For example, if we weren’t picking out numbers from nought to 10, but in stead,
    we are picking out cat, dog, plane, fish, or building, the output of that matrix
    multiply for one particular image might look like that (output column)[[22:38](https://youtu.be/PGC0UxakTvM?t=1361)].
    These are just some random numbers. And to turn that into a softmax, I first go
    *e* to the power of each of those numbers. I sum up those *e* to the power of’s.
    Then I take each of those *e* to the power of’s and divide it by the sum. And
    that’s softmax. That’s the definition of softmax.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们不是从零到10中挑选数字，而是挑选猫、狗、飞机、鱼或建筑物，那么一个特定图像的矩阵乘积的输出可能看起来像这样（输出列）。这些只是一些随机数。为了将其转换为softmax，我首先对这些数字中的每一个进行*e*的幂运算。我将这些*e*的幂相加。然后我将这些*e*的幂中的每一个除以总和。这就是softmax。这就是softmax的定义。
- en: '![](../Images/a12c6ae4c296d037b033b6fc120b2a62.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a12c6ae4c296d037b033b6fc120b2a62.png)'
- en: Because it was *e* to the power of, it means that it’s always positive. Because
    it was divided by the sum, it means that it’s always between 0 and 1 and it also
    means they always add up to 1\. So by applying this softmax activation function,
    anytime we have a layer of a layer of outputs which we call activations and then
    we apply some nonlinear function to that which maps one scaler to one scaler like
    softmax (we call that an activation function). So the softmax activation function
    takes our outputs and turns it into something which behaves like a probability.
    We don’t, strictly speaking, need it. We could still try and train something which
    where the output directly is the probabilities. But by using this function that
    automatically makes them always behave like probabilities, it means there’s less
    for the network to learn, so it’s going to learn better. So generally speaking,
    whenever we design an architecture, we try to design it in a way where it’s as
    easy as possible for it to create something of the form we want. So that’s why
    we use softmax.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它是*e*的幂，这意味着它始终是正的。因为它被总和除以，这意味着它始终在0和1之间，并且还意味着它们总是加起来等于1。因此，通过应用这个softmax激活函数，每当我们有一层输出，我们称之为激活，然后我们对其应用一些非线性函数，将一个标量映射到一个标量，比如softmax（我们称之为激活函数）。因此，softmax激活函数将我们的输出转换为类似概率的东西。严格来说，我们不需要它。我们仍然可以尝试训练直接输出概率的东西。但是通过使用这个函数，它自动使它们始终表现得像概率，这意味着网络需要学习的内容更少，因此它会学得更好。因此，一般来说，每当我们设计一个架构时，我们都会尽可能地设计它，以便它尽可能地创建我们想要的形式。这就是为什么我们使用softmax的原因。
- en: That’s the basic steps [[24:40](https://youtu.be/PGC0UxakTvM?t=1480)]. We have
    our input which is a bunch of images, it gets multiplied by a weight matrix, we
    also add on a bias to get an output of the linear function. We put it through
    a nonlinear activation function, in this case softmax, and that gives us our probabilities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是基本步骤。我们有我们的输入，它是一堆图像，它被一个权重矩阵相乘，我们还添加一个偏置以获得线性函数的输出。我们将其通过一个非线性激活函数，这种情况下是softmax，这给我们带来了我们的概率。
- en: So there that all is. PyTorch also tends to use the log of softmax for reasons
    that don’t particularly need to bother us now. It’s basically a numerical stability
    convenience. So to make this the same as our version up here that you saw `nn.LogSoftmax()`,
    I’m going to use log here as well. Okay, so we can now instantiate this class
    (i.e. create an object of this class)[[25:33](https://youtu.be/PGC0UxakTvM?t=1533)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是全部。PyTorch也倾向于使用softmax的对数，原因并不需要现在困扰我们。基本上是为了数值稳定性的便利。因此，为了使这与我们在上面看到的`nn.LogSoftmax()`版本相同，我也将在这里使用对数。好的，现在我们可以实例化这个类（即创建这个类的对象）。
- en: '![](../Images/573498089aadc672e0634eb74a8af6e9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/573498089aadc672e0634eb74a8af6e9.png)'
- en: '**Question**: I have a question back for the probabilities where we were before.
    If we were to have a photo with a cat and a dog together, would that change the
    way that works? Or does it work in the same basic [[25:43](https://youtu.be/PGC0UxakTvM?t=1543)].
    That’s a great question. So if you had a photo with a cat and a dog together and
    you wanted it to spit out both cat and dog, this would be a very poor choice.
    Softmax is specifically the activation function we use for categorical predictions
    where we only ever want to predict one of those things. So part of the reason
    why is that as you can see, because we are using *e* to the power of, *e* to the
    slightly bigger numbers create much bigger numbers. As a result of which, we generally
    have just one or two things large and everything else is pretty small. So if I
    recalculate these random numbers (in the excel sheet), you’ll see it tends to
    be a bunch of zeros and one or two high numbers. So it’s really designed to try
    to make it easy to predict this one thing i s the thing I want. If you are doing
    multi-label prediction, so I want to just find all the things in this image, rather
    than using softmax, we would instead use sigmoid. Sigmoid would cause each of
    these things to be between 0 and 1, but they would no longer add to 1.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我有一个关于之前的概率的问题。如果我们有一张照片上有一只猫和一只狗，那会改变它的工作方式吗？或者它的基本工作方式是一样的。这是一个很好的问题。所以如果你有一张照片上有一只猫和一只狗，你希望它同时输出猫和狗，这将是一个非常糟糕的选择。Softmax是我们专门用于分类预测的激活函数，我们只想预测其中一种东西。因此，部分原因是因为正如你所看到的，因为我们使用*e*的幂，稍微更大的*e*会产生更大的数字。因此，通常我们只有一两个大的东西，其他东西都很小。因此，如果我重新计算这些随机数（在Excel表中），你会看到它往往是一堆零和一两个高数字。因此，它真的是设计成试图让预测这一件事变得容易。如果你正在进行多标签预测，所以我只想找到这张图片中的所有东西，而不是使用softmax，我们将使用sigmoid。Sigmoid会导致这些东西中的每一个都在0和1之间，但它们不再加起来等于1。
- en: '![](../Images/42572813ccbd7ab85d871310d13ec998.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42572813ccbd7ab85d871310d13ec998.png)'
- en: A lot of these details about best practices are things that we cover in the
    deep learning course and we won’t cover heaps of them here in the machine learning
    course. We are more interested in the mechanics. But we are trying to them if
    they are quick.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最佳实践的许多细节是我们在深度学习课程中涵盖的内容，我们在机器学习课程中不会涵盖大量这些内容。我们更感兴趣的是机制。但是如果它们很快，我们会尝试涵盖它们。
- en: So now that we’ve got that, we can instantiate an object of that class [[27:30](https://youtu.be/PGC0UxakTvM?t=1650)].
    And of course we want to copy it over to the GPU. So we can do computations over
    there. Again, we need an optimizer which we are talking about what this is shortly.
    But you see here, we’ve called a function on our class called `parameters`. But
    we never defined a method called parameters and the reason that is going to work
    is because it actually was defined for us inside `nn.Module`. So `nn.Module` automatically
    go through the attributes we’ve created and finds anything that basically we said
    this is a parameter. The way you say something is a parameter is you wrap it in
    `nn.Parameter`. This is just the way that you tell PyTorch this is something that
    I want to optimize. So when we created the weight matrix, we just wrapped it with
    n`n.Parameter`, it’s exactly the same as a regular PyTorch variable which we will
    learn about shortly. It’s just a little flag to say hey you should optimize this.
    So when you call `net2.parameters()` on our `net2` object we created, it goes
    through everything that we created in the constructor, checks to see if any of
    them are of type `Parameter` and if so, it sets all of those being things that
    we want to train with the optimizer. We will be implementing the optimizer from
    scratch later.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了这个，我们可以实例化该类的一个对象[[27:30](https://youtu.be/PGC0UxakTvM?t=1650)]。当然我们想将其复制到GPU上。这样我们可以在那里进行计算。再次，我们需要一个优化器，我们很快会讨论这是什么。但你看到这里，我们在我们的类上调用了一个名为`parameters`的函数。但我们从未定义过一个叫做parameters的方法，这将会起作用的原因是因为它实际上是在`nn.Module`内部为我们定义的。所以`nn.Module`会自动遍历我们创建的属性，并找到任何我们说这是一个参数的东西。你说某个东西是参数的方式是将它包装在`nn.Parameter`中。这只是告诉PyTorch这是我想要优化的东西的方式。所以当我们创建权重矩阵时，我们只是用`nn.Parameter`包装它，这与我们很快要学习的常规PyTorch变量完全相同。这只是一个小标志，告诉PyTorch你应该优化这个。所以当你在我们创建的`net2`对象上调用`net2.parameters()`时，它会遍历我们在构造函数中创建的所有东西，检查是否有任何一个是`Parameter`类型，如果是，它会将所有这些东西设置为我们要用优化器训练的东西。我们稍后将从头开始实现优化器。
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Having done that, we can fit [[28:51](https://youtu.be/PGC0UxakTvM?t=1731)].
    And we should get basically the same answer as before (i.e. 91 ish). So that looks
    good.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 做完这些，我们可以拟合[[28:51](https://youtu.be/PGC0UxakTvM?t=1731)]。我们应该基本上得到与之前相同的答案（即91左右）。看起来不错。
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Data Loader [[29:05](https://youtu.be/PGC0UxakTvM?t=1745)]
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载器[[29:05](https://youtu.be/PGC0UxakTvM?t=1745)]
- en: 'So what have we actually built here? Well, what we’ve actually built as I said
    is something that can behave like a regular function. So I want to show you how
    we can actually call this as a function. To be able to call it as a function,
    we need to be able to pass data to it. To be able to pass data to it, I’m going
    to need to grab a mini batch of MNIST images. For convenience, we used the `ImageClassifierData.from_arrays`
    method from Fast AI and what that does is it creates a PyTorch DataLoader for
    us. A PyTorch DataLoader is something that grabs a few images and sticks them
    into a mini batch and makes them available. And you can basically say give me
    another mini batch, give me another mini batch, give me another mini batch. So
    in Python, we call these things generators. Generators are things where you can
    basically say I want another, I want another, I want another. There’s this very
    close connection between iterators and generators, I’m not going to worry about
    the difference between them right now. But you’ll see basically to get hold of
    something which we can say please give me another of, in order to grab something
    that we can use to generate mini batches, we have to take our data loader so you
    can ask for the training data from our model data object. You will see there’s
    a bunch of different data loader you can ask for: test data loader, train data
    loader, validation data loader, augmented images data loader, and so forth.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们实际上在这里构建了什么？嗯，正如我所说的，我们实际上构建的是可以像常规函数一样运行的东西。所以我想向你展示如何实际上将其作为一个函数调用。为了能够将其作为一个函数调用，我们需要能够向其传递数据。为了能够向其传递数据，我需要获取一个MNIST图像的小批量。为了方便起见，我们使用了Fast
    AI的`ImageClassifierData.from_arrays`方法，它会为我们创建一个PyTorch DataLoader。PyTorch DataLoader是一种获取几张图像并将它们放入一个小批量并使其可用的东西。你基本上可以说给我另一个小批量，给我另一个小批量，给我另一个小批量。所以在Python中，我们称这些东西为生成器。生成器是一种东西，你基本上可以说我想要另一个，我想要另一个，我想要另一个。迭代器和生成器之间有非常紧密的联系，我现在不打算担心它们之间的区别。但你会看到，为了获得我们可以说请给我另一个的东西，为了获取我们可以用来生成小批量的东西，我们必须取出我们的数据加载器，这样你就可以从我们的模型数据对象中请求训练数据。你会看到有很多不同的数据加载器可以请求：测试数据加载器、训练数据加载器、验证数据加载器、增强图像数据加载器等等。
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So we’re going to grab the training data loader that was created for us. This
    is a standard PyTorch data loader, well slightly optimized by us, but same idea.
    You can then say this (`iter`) is a standard Python thing, we can say turn that
    into an iterator i.e. something where we can grab another one at a time from.
    Once you’ve done that, we’ve not got something that we can iterate through. You
    can use the standard Python `next` function to grab one more thing from that generator.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要获取为我们创建的训练数据加载器。这是一个标准的PyTorch数据加载器，稍微被我们优化了一下，但是思路是一样的。然后你可以说这个（`iter`）是一个标准的Python东西，我们可以说将其转换为一个迭代器，即我们可以一次从中获取另一个的东西。一旦你这样做了，我们就有了一个可以迭代的东西。你可以使用标准的Python
    `next`函数从生成器中获取一个更多的东西。
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So that is returning x’s from a mini-batch and the y’s from our mini-batch.
    The other way that you can use generators and iterators in Python is with a `for`
    loop. I could have also said x mini-batch comma y mini-batch in data loader, and
    then do something:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是从一个小批量返回x和y。在Python中，您可以使用`for`循环来使用生成器和迭代器。我也可以说x小批量逗号y小批量在数据加载器中，然后做一些事情：
- en: '![](../Images/1bd4323f592b38ab8fd2f7e16cdff645.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bd4323f592b38ab8fd2f7e16cdff645.png)'
- en: So when you do that, it’s actually behind the scenes, it’s basically syntactic
    sugar for calling `next` lots of times. So this is all standard Python stuff.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当你这样做时，实际上是在幕后，它基本上是调用`next`很多次的语法糖。所以这都是标准的Python东西。
- en: So that returns a tensor of size 64 by 784 as we would expect. The Fast AI library
    we used defaults to a mini batch size of 64, that’s why it’s that long. These
    are all of the background zero pixels but they are not actually zero. In this
    case, why aren’t they zero? Because they are normalized. So we subtracted the
    mean, divided by the standard deviation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它返回了一个大小为64乘以784的张量，这是我们所期望的。我们使用的Fast AI库默认使用小批量大小为64，这就是为什么它这么长。这些都是背景零像素，但它们实际上并不是零。在这种情况下，为什么它们不是零呢？因为它们被标准化了。所以我们减去了平均值，除以标准差。
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now what we want to do is we want to pass that into our logistic regression.
    So what we might do is we’ll go `vxmb` (variable x mini-batch), I can take my
    x mini-batch, I can move it onto the GPU because remember my `net2` object is
    on the GPU, so our data for it also has to be on the GPU. Then the second thing
    I do is, I have to wrap it in `Variable`. So what does variable do? This is how
    we get for free automatic differentiation. PyTorch can automatically differentiate
    pretty much any tensor. But to do so takes memory and time, so it’s not going
    to always keep track. To do automatic differentiation, it has to keep track of
    exactly how something was calculated. We added these things together, we multiplied
    it by that, we then took the sign blah blah blah. You have to know all of the
    steps because then to do the automatic differentiation, it has to take the derivative
    of each step using the chain rule, multiply them all together. So that’s slow
    and memory intensive. So we have to opt in to saying “okay, this particular thing,
    we’re going to be taking the derivative later so please keep track of all of those
    operations for us.” So the way we opt-in is by wrapping a tensor in `Variable`.
    That’s how we do it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要做的是将其传递给我们的逻辑回归。所以我们可能会这样做，我们将使用`vxmb`（变量x小批量），我可以取出我的x小批量，我可以将其移动到GPU上，因为记住我的`net2`对象在GPU上，所以我们的数据也必须在GPU上。然后我要做的第二件事是，我必须将其包装在`Variable`中。那么变量是做什么的呢？这是我们免费获得自动微分的方式。PyTorch可以自动微分几乎任何张量。但这需要内存和时间，所以它不会总是跟踪。要进行自动微分，它必须跟踪确切的计算方式。我们将这些东西相加，我们将其乘以那个，然后我们取了这个的符号等等。你必须知道所有的步骤，因为然后要进行自动微分，它必须使用链式法则对每个步骤求导，然后将它们相乘。所以这是缓慢和占用内存的。所以我们必须选择说“好的，这个特定的东西，我们以后会对其进行求导，所以请为我们跟踪所有这些操作。”我们选择的方式是将一个张量包装在`Variable`中。这就是我们的做法。
- en: And you’ll see that it looks almost exactly like a tensor but it now says “variable
    containing” this tensor. So in PyTorch a variable has exactly identical API to
    a tensor, or actually more specifically a superset of the API of a tensor. Anything
    we can do to a tensor, we can do to a variable. But it’s going to keep track of
    exactly what we did so we can later on take the derivative.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现它看起来几乎和一个张量一样，但现在它说“包含这个张量的变量”。所以在PyTorch中，一个变量的API与张量完全相同，或者更具体地说，是张量API的超集。我们对张量可以做的任何事情，我们也可以对变量做。但它会跟踪我们做了什么，以便我们以后可以求导。
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So we can now pass that into our `net2` object [[34:37](https://youtu.be/PGC0UxakTvM?t=2077)].
    Remember I said you can treat this as if it’s a function. So notice we’re not
    calling `.forward()` we’re just treating it as a function. And then remember,
    we took the log so to undo that, I’m taking the `.exp()` and that will give me
    my probabilities. So there’s my probabilities and it returns something of size
    64 by 10 so for each image in the mini batch, we’ve got 10 probabilities. And
    you’ll see, most probabilities are pretty close to zero. And a few of them are
    quite a bit bigger which is exactly what we would have hoped. It’s like okay,
    it’s not a zero, it’s not a 1, it’s not a 2, it **is** a 3, it’s not a 4 and so
    forth.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们现在可以将其传递给我们的`net2`对象。记住我说过你可以将其视为函数。所以请注意，我们没有调用`.forward()`，我们只是将其视为函数。然后记住，我们取了对数，为了撤销这个操作，我正在使用`.exp()`，这将给我概率。所以这是我的概率，它返回的大小是64乘以10，所以对于小批量中的每个图像，我们有10个概率。你会看到，大多数概率都非常接近零。而其中一些则要大得多，这正是我们所希望的。就像好吧，它不是零，不是1，不是2，它**是**3，不是4等等。
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We could call `net2.forward(vxmb)` and it will do exactly the same thing. But
    that’s not how all of the PyTorch mechanics actually work. They actually call
    it as if it’s a function. This is actually a really important idea because it
    means that when we define our own architectures or whatever, anywhere that you
    would put in a function, you could put in a layer; anywhere you put in a layer,
    you can put in a neural net; anywhere you put in a neural net, you can put in
    a function. Because as far as PyTorch is concerned, they are all just things that
    it’s going to call just like as if they are functions. So they are all interchangeable
    and this is really important because that’s how we create really good neural nets
    is by mixing and matching lots of pieces and putting them all together.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用`net2.forward(vxmb)`，它会做完全相同的事情。但这并不是PyTorch的所有机制实际上是如何工作的。他们实际上将其称为函数。这实际上是一个非常重要的想法，因为这意味着当我们定义自己的架构或其他内容时，任何你想要放入函数的地方，你都可以放入一个层；任何你想要放入一个层的地方，你都可以放入一个神经网络；任何你想要放入一个神经网络的地方，你都可以放入一个函数。因为就PyTorch而言，它们都只是它将调用的东西，就像它们是函数一样。所以它们是可以互换的，这是非常重要的，因为这就是我们通过混合和匹配许多部分并将它们组合在一起来创建非常好的神经网络的方式。
- en: Let me give you an example [[36:54](https://youtu.be/PGC0UxakTvM?t=2214)]. Here
    is my logistic regression which got 91 and a bit % accuracy. I’m now going to
    turn it into a neural network with one hidden layer .
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我举个例子。这是我的逻辑回归，准确率达到了91%多一点。我现在要把它转换成一个带有一个隐藏层的神经网络。
- en: '![](../Images/8e2b5ca11c8fb87cebe04a6278420f88.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e2b5ca11c8fb87cebe04a6278420f88.png)'
- en: And the way I’m going to do that is I’m going to create more layer. I’m going
    to change this so it spits out a hundred rather than 10 which means this one’s
    input is going to be a hundred rather than 10\. Now this as it is can’t possibly
    make things any better at all yet. Why is this definitely not going to be better
    than what I had before? Because a combination of two linear layers is just the
    same as one linear layer but different parameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我要做的是创建更多的层。我要改变这个，使其输出100而不是10，这意味着这个输入将是100而不是10。现在这样还不能让事情变得更好。为什么这肯定不会比之前更好呢？因为两个线性层的组合只是一个线性层，但参数不同。
- en: '![](../Images/a821328c63cef76d02c6fa82afd244ae.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a821328c63cef76d02c6fa82afd244ae.png)'
- en: So we’ve got two linear layers which is just a linear layer. To make things
    interesting, I’m going to replace all the negatives from the first layer with
    zeros. Because that’s a nonlinear transformation, and that nonlinear transformation
    is called a rectified linear unit (`ReLU`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有两个线性层，这只是一个线性层。为了使事情变得有趣，我将用零替换第一层中的所有负数。因为这是一个非线性转换，这个非线性转换被称为修正线性单元（ReLU）。
- en: '![](../Images/72221e96d78665ab9745d93bb5d47104.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72221e96d78665ab9745d93bb5d47104.png)'
- en: So `nn.Sequential` simply is going to call each of these layers in turn for
    each mini batch. So do a Linear layer, replace all of the negatives with zero,
    do another Linear layer and do a softmax. This is now a neural network with one
    hidden layer. So let’s try training that instead. The accuracy has now been up
    to 96%.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所以`nn.Sequential`简单地会依次调用每个层对每个小批量进行操作。所以做一个线性层，用零替换所有负数，再做一个线性层，最后做一个softmax。这现在是一个有一个隐藏层的神经网络。所以让我们尝试训练这个。准确率现在已经提高到96%。
- en: '![](../Images/d1810921fe4995436121d3d562b4e487.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1810921fe4995436121d3d562b4e487.png)'
- en: So the idea is that the basic techniques we are learning in this lesson become
    powerful at the point where you start stacking them together.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个想法是，我们在这节课中学习的基本技术在你开始将它们堆叠在一起时变得强大。
- en: '**Question**: Why did you pick 100 [[38:55](https://youtu.be/PGC0UxakTvM?t=2335)]?
    No reason. It was easier to type an extra zero. This question of how many activations
    should I have in a neural network layer is part of the scale of a deep learning
    practitioner, we covered in deep learning course and not in this course.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么你选择了100？没有原因。输入一个额外的零更容易。神经网络层中应该有多少激活是深度学习从业者的规模问题，我们在深度学习课程中讨论过，而不是在这门课程中。'
- en: '**Question**: When adding the additional layer, does it matter if you would
    have done two softmax’s or is that something you cannot do? You can absolutely
    use the softmax there. But it’s probably not going to give you what you want.
    The reason why is that a softmax tends to push most of its activations to zero.
    An activation, just to be clear as I’ve had a lot of questions in deep learning
    course about what is an activation, an activation is the value that is calculated
    in a layer. So this is an activation:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在添加额外的层时，如果你做了两个softmax，这会有影响吗，或者这是你不能做的事情？你绝对可以在那里使用softmax。但这可能不会给你想要的结果。原因是softmax倾向于将大部分激活推向零。激活，只是为了明确，因为在深度学习课程中我收到了很多关于什么是激活的问题，激活是在一个层中计算出来的值。这就是一个激活：'
- en: '![](../Images/dda14ae53c216b5f6d4a99d52a8bf672.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dda14ae53c216b5f6d4a99d52a8bf672.png)'
- en: It’s not a weight. A weight is not an activation. It’s the value that you calculate
    from a layer. So softmax will tend to make most of its activations pretty close
    to zero and that’s the opposite of what you want. You generally want your activations
    to be as rich and diverse and used as possible. So nothing to stop you doing it,
    but it probably won’t work very well. Basically pretty much all of your layers
    will be followed by nonlinear activation functions that will nearly always be
    ReLU except for the last layer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个权重。权重不是一个激活。它是你从一个层计算出来的值。所以softmax会倾向于使大部分激活接近于零，这与你想要的相反。通常你希望你的激活尽可能丰富、多样且被使用。所以没有什么能阻止你这样做，但它可能不会工作得很好。基本上，你的所有层几乎都会跟随非线性激活函数，通常是ReLU，除了最后一层。
- en: '**Question**: When doing multiple layers, so let’s say 2 or 3, do you want
    to switch up these activation layers [[40:41](https://youtu.be/PGC0UxakTvM?t=2441)]?
    No. So if I wanted to go deeper, I would just do that.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在做多层时，比如说2或3层，你想要改变这些激活层吗？不。所以如果我想要更深，我会直接这样做。'
- en: '![](../Images/c8aba27c41cb86df3007dce502aa8d2a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8aba27c41cb86df3007dce502aa8d2a.png)'
- en: That’s now a two hidden layer network.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是一个两个隐藏层的网络。
- en: '**Question**: So I think I heard you said that there are a couple of different
    activation functions like that rectified linear units. What are some examples
    and why would you use each [[41:09](https://youtu.be/PGC0UxakTvM?t=2469)]? Yes,
    great question. So basically as you add more linear layers, your input comes in
    and you put it through a linear layer and then a nonlinear layer, linear layer,
    nonlinear layer, linear layer and the final nonlinear layer. The final nonlinear
    layer as we’ve discussed if it’s a multi category classification but you only
    ever pick one of them, you would use softmax. If it’s a binary classification
    or a multi-label classification where you are predicting multiple things, you
    would use sigmoid. If it’s a regression, you would often have nothing at all,
    although we learnt in last night’s DL course where sometimes you can use sigmoid
    there as well. So they are basically the main options for the final layer. For
    the hidden layers, you pretty much always use ReLU but there is another one you
    can pick which is kind of interesting called leaky ReLU. Basically if it’s above
    zero, it’s *y = x*, and if it’s below zero, it’s like *y = 0.1x*. So it’s very
    similar to ReLU but rather than being equal to 0 it’s something close to that.
    So they are the main two: ReLU and Leaky ReLU.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所以我想我听到你说有几种不同的激活函数，比如修正线性单元。有一些例子，为什么会使用每一个呢？是的，很好的问题。所以基本上当你添加更多的线性层时，你的输入进来，你把它通过一个线性层然后一个非线性层，线性层，非线性层，线性层和最终的非线性层。最终的非线性层正如我们讨论过的，如果它是多类别分类但你只选择其中一个，你会使用softmax。如果是二元分类或多标签分类，你会使用sigmoid。如果是回归，通常你根本不会有，尽管我们在昨晚的深度学习课程中学到有时你也可以在那里使用sigmoid。所以它们基本上是最终层的主要选项。对于隐藏层，你几乎总是使用ReLU，但还有另一个你可以选择的，有点有趣，叫做leaky
    ReLU。基本上如果它大于零，它是*y = x*，如果小于零，它就像*y = 0.1x*。所以它与ReLU非常相似，但不是等于0，而是接近于0。所以它们是主要的两种：ReLU和Leaky
    ReLU。
- en: '![](../Images/bf6ba4240ceea4633489ae74e5b8edb5.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf6ba4240ceea4633489ae74e5b8edb5.png)'
- en: There are various others, but they are kind of like things that just look very
    close to that. For example, there’s something called ELU which is quite popular,
    but the details don’t matter too much honestly. Like ELU is something like ReLU
    but slightly more curvy in the middle. It’s not generally something that you so
    much pick based on the dataset. It’s more like over time we just find better activation
    functions. So two or three years ago, everybody used ReLU. A year ago, pretty
    much everybody used Leaky ReLU. Today, I guess probably most people are starting
    to move towards ELU. But honestly the choice of activation function doesn’t matter
    terribly much actually. People have actually showed that you can use pretty arbitrary
    nonlinear activation functions, like even a sine wave, and it still works.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些，但它们有点像那样。例如，有一种叫做ELU的东西相当受欢迎，但细节并不太重要。像ELU这样的东西有点像ReLU，但在中间稍微弯曲一些。它通常不是基于数据集选择的东西。随着时间的推移，我们发现了更好的激活函数。所以两三年前，每个人都使用ReLU。一年前，几乎每个人都使用Leaky
    ReLU。今天，我想大多数人开始转向ELU。但老实说，激活函数的选择实际上并不太重要。人们实际上已经表明，你可以使用相当任意的非线性激活函数，甚至是正弦波，它仍然有效。
- en: So although what we are going to do today is showing how to create this network
    with no hidden layers, to turn it into that network (below) which is 96% ish accurate
    will be trivial [[44:35](https://youtu.be/PGC0UxakTvM?t=2675)]. It’s something
    you should probably try and do during the week to create this version.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以尽管今天我们要做的是展示如何创建这个没有隐藏层的网络，将其转变为下面这个网络（下面）准确率为96%左右将会很简单。这是你可能应该在这一周尝试做的事情，创建这个版本。
- en: '![](../Images/d76a8f940270860deeb1c8e1039ea336.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d76a8f940270860deeb1c8e1039ea336.png)'
- en: 'So now that we’ve got something where we can take our network, passing our
    variable, and get back some predictions, that’s basically all that happened when
    we called `fit` [[45:11](https://youtu.be/PGC0UxakTvM?t=2711)]. So we are going
    to see how that approach can be used to create this stochastic gradient descent.
    One thing to note is that to turn the predicted probabilities into a predicted
    which digit is it, we would nee to use argmax. Unfortunately, PyTorch doesn’t
    call it argmax. Instead, PyTorch just calls it max and max returns two things:
    it returns the actual max across the given axis (so `max(1)` will return max across
    the columns), and the second thing it returns is the index of that maximum. So
    the equivalent of argmax is to call max and then get the first indexed thing:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个可以传递我们的变量并得到一些预测的网络，这基本上就是我们调用`fit`时发生的事情。所以我们将看看这种方法如何用于创建这种随机梯度下降。需要注意的一件事是将预测的概率转换为预测的数字是，我们需要使用argmax。不幸的是，PyTorch并不称之为argmax。相反，PyTorch只是称之为max，并且max返回两个东西：它返回给定轴上的实际最大值（所以`max(1)`将返回列的最大值），它返回的第二件事是该最大值的索引。所以argmax的等价物是调用max然后获取第一个索引的东西：
- en: '![](../Images/4375c4746bc633c37eea2765866b888c.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4375c4746bc633c37eea2765866b888c.png)'
- en: So there’s our predictions. If this was numpy, we would instead use `np.argmax()`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的预测。如果这是numpy，我们将使用`np.argmax()`。
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/08b7fbb29369fcf2ae9b19332f2eb125.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08b7fbb29369fcf2ae9b19332f2eb125.png)'
- en: So here are the predictions from our hand created logistic regression and in
    this case, looks like we’ve got all but one correct [[46:25](https://youtu.be/PGC0UxakTvM?t=2785)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我们手动创建的逻辑回归的预测，在这种情况下，看起来我们几乎全部正确。
- en: The next thing we are going to try and get rid of in terms of using libraries
    is we will try to avoid using the matrix multiplication operator. Instead we are
    going to try and write that by hand.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要尝试摆脱使用库的是我们将尝试避免使用矩阵乘法运算符。相反，我们将尝试手动编写。
- en: Broadcasting [[46:58](https://youtu.be/PGC0UxakTvM?t=2818)]
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广播[[46:58](https://youtu.be/PGC0UxakTvM?t=2818)]
- en: So this next part, we are going to learn about something which kind of seems
    like a minor little kind of programming idea. But actually it’s going to turn
    out that, at least in my opinion, it’s the most important programming concept
    that we will teach in this course, and it’s possibly the most important programming
    concept in all the things you need to build machine learning algorithms. And it’s
    the idea of broadcasting. And the idea I will show by example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，接下来，我们将学习一些似乎是一个小的编程概念的东西。但实际上，至少在我看来，这将是我们在这门课程中教授的最重要的编程概念，也可能是你需要构建机器学习算法的所有重要编程概念。这就是广播的概念。我将通过示例展示这个概念。
- en: If we create an array of 10, 6, -4 and an array of 2, 8, 7 and then add the
    two together, it adds each of the components of those two arrays in turn — we
    call that “element-wise”.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建一个数组10、6、-4和一个数组2、8、7，然后将它们相加，它会依次添加这两个数组的每个分量——我们称之为“逐元素”。
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In other words, we didn’t have to write a loop. Back in the old days, we would
    have to have looped through each one and added them, and then concatenated them
    together. We don’t have to do that today. It happens for us automatically. So
    in numpy, we automatically get element-wise operations. We can do the same thing
    with PyTorch [[48:17](https://youtu.be/PGC0UxakTvM?t=2897)]. In Fast AI, we just
    add a little capital T to turn something into a PyTorch tensor. And if we add
    those together, exactly the same thing.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们不必编写循环。在过去，我们必须循环遍历每一个并将它们相加，然后将它们连接在一起。今天我们不必这样做。它会自动为我们发生。因此，在numpy中，我们自动获得逐元素操作。我们可以用PyTorch做同样的事情。在Fast
    AI中，我们只需添加一个大写T来将某物转换为PyTorch张量。如果我们将它们相加，结果完全相同。
- en: '![](../Images/ad51a52ee9c9f03f89f3338910ef8deb.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad51a52ee9c9f03f89f3338910ef8deb.png)'
- en: So element-wise operations are pretty standard in these kind of libraries. It’s
    interesting not just because we don’t have to write the for loop, but it’s actually
    much more interesting because of the performance things that are happening here.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些库中的逐元素操作在这种情况下是相当标准的。有趣的不仅仅是因为我们不必编写for循环，而且实际上更有趣的是由于这里发生的性能问题。
- en: Performance [[48:49](https://youtu.be/PGC0UxakTvM?t=2929)]
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: The first is if we were doing a for loop, that would happen in Python. Even
    when you use PyTorch, it still does the for loop in Python. It has no way of optimizing
    a for loop. So a for loop in Python is something like 10,000 times slower than
    in C. So that’s your first problem. I can’t remember it’s like 1,000 or 10,000.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是如果我们在Python中进行for循环，那将会发生。即使你使用PyTorch，它仍然在Python中执行for循环。它没有优化for循环的方法。因此，在Python中，for循环的速度大约比在C中慢10,000倍。这是你的第一个问题。我记不清是1,000还是10,000。
- en: The second problem, then, is that you don’t just want it to be optimized in
    C but you want C to take advantage of the thing that all of your CPUs do, something
    called SIMD, Single Instruction Multiple Data. Your CPU is capable of taking 8
    things at a time in a vector and adding them up to another vector with 8 things
    in, in a single CPU instruction. So if you can take advantage of SIMD, you are
    immediately 8 time s faster. It depends on how big the data type is, it might
    be 4, might be 8.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，第二个问题是，你不仅希望它在C中得到优化，而且你希望C利用你所有CPU所做的事情，这被称为SIMD，单指令多数据。你的CPU能够一次处理8个向量中的8个元素，并将它们相加到另一个包含8个元素的向量中，在一个CPU指令中。因此，如果你能利用SIMD，你立即就会快8倍。这取决于数据类型有多大，可能是4，可能是8。
- en: The other thing you’ve got in your computer is you’ve got multiple processes
    (multiple cores). So if the vector addition is happening in one core, you’ve probably
    got about 4 of those. So if you’re using SIMD, you are 8 times faster, if you
    can use multiple cores, then you are 32 times faster. Then if you are doing that
    in C, you might be something like 32k times faster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你的计算机中还有多个进程（多个核心）。因此，如果向量相加发生在一个核心中，你可能有大约4个核心。因此，如果你使用SIMD，你会快8倍，如果你可以使用多个核心，那么你会快32倍。然后如果你在C中这样做，你可能会快32k倍。
- en: So the nice thing is when we do `a + b` , it’s taking advantage of all of these
    things.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以好处是当我们执行`a + b`时，它利用了所有这些东西。
- en: '![](../Images/d766cb4e5b4d62d4a306f75b5ffef2ad.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d766cb4e5b4d62d4a306f75b5ffef2ad.png)'
- en: Better still, if you do it in PyTorch and your data was created with `.cuda()`
    to to stick it on the GPU, then your GPU can do about 10,000 things at a time
    [[50:40](https://youtu.be/PGC0UxakTvM?t=3040)]. So that’ll be another hundred
    times faster than C. So this is critical to getting good performance. You have
    to learn how to write loop-less code by taking advantage of these element-wise
    operations. And it’s a lot more than just plus (`+`). I can also use less than
    (`<`), and that’s going to return 0, 1, 1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，如果你在PyTorch中执行这个操作，并且你的数据是用`.cuda()`创建的，然后将其放在GPU上，那么你的GPU可以一次执行大约10,000个操作。因此，这将比C快100倍。因此，这对于获得良好的性能至关重要。你必须学会如何通过利用这些逐元素操作来编写无循环的代码。这不仅仅是加号（`+`）。我还可以使用小于号（`<`），这将返回0，1，1。
- en: '![](../Images/b63558070aada53d6e9695a6b222fc25.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b63558070aada53d6e9695a6b222fc25.png)'
- en: Or if we go back to numpy, False, True, True.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 或者如果我们回到numpy，False，True，True。
- en: '![](../Images/5b47aae269951970d4a0dc5931e29c1d.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b47aae269951970d4a0dc5931e29c1d.png)'
- en: 'So you can use this to do all kinds of things without looping. So for example,
    I could now multiply that by *a* and here are all of the values of *a* as long
    as they are less than *b*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以使用这个来做各种事情而不需要循环。例如，我现在可以将其乘以*a*，这里是所有小于*b*的*a*的值：
- en: '![](../Images/8462596727a131fb69c01a8ca51dc025.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8462596727a131fb69c01a8ca51dc025.png)'
- en: 'Or we could take the mean:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以取平均值：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is the percentage of values in *a* that are less than *b*. So there’s a
    lot of stuff you can do with this simple idea.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*a*中小于*b*的值的百分比。因此，你可以用这个简单的想法做很多事情。
- en: Taking it further [[52:04](https://youtu.be/PGC0UxakTvM?t=3124)]
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步
- en: But to take it further, to take it further than just this element-wise operation,
    we are going to have to go to the next step to something called broadcasting.
    Let’s start by looking at an example of broadcasting [[52:43](https://youtu.be/PGC0UxakTvM?t=3163)].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 但要进一步，要进一步超越这种逐元素操作，我们将不得不走到下一步，到一种称为广播的东西。让我们从看一个广播的例子开始。
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'a is an array with one dimension, also known as a rank 1 tensor, also known
    as a vector. We can say `a` greater than zero:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: a是一个具有一维的数组，也称为秩1张量，也称为向量。我们可以说`a`大于零：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'So here, we have a rank 1 tensor (`a`) and a rank 0 tensor (`0`). A rank 0
    tensor is also called a scalar, and rank 1 tensor is also called a vector. And
    we’ve got an operation between the two:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有一个秩1张量（`a`）和一个秩0张量（`0`）。秩0张量也称为标量，秩1张量也称为向量。我们之间有一个操作：
- en: '![](../Images/1d5c63282e23f91fce767c03a10d3800.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d5c63282e23f91fce767c03a10d3800.png)'
- en: Now you’ve probably done it a thousand times without even noticing that’s kind
    of weird. You’ve got these things of different ranks and different sizes. So what
    is it actually doing? What it’s actually doing is it’s taking that scaler and
    copying it 3 times (i.e. [0, 0, 0]) and actually going element-wise and giving
    us back the three answers. That’s called broadcasting. Broadcasting means copying
    one of more axes of my tensor to allow it to be the same shape as the other tensor.
    It doesn’t really copy it though. What it actually does is it stores this kind
    of internal indicator that says pretend that this is a vector of three zeros,
    but it actually rather than going to the next row or going to the next scaler,
    it goes back to where it came from. If you are interested in learning about this
    specifically, it’s they set the stride on that axis to be zero. That’s a minor
    advanced concept for those who are curious.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能已经做了一千次，甚至没有注意到这有点奇怪。你有不同等级和不同大小的这些东西。那么它实际上在做什么呢？它实际上是将那个标量复制3次（即[0, 0,
    0]），并实际上逐个元素地给我们三个答案。这就是所谓的广播。广播意味着复制我的张量的一个或多个轴，以使其与另一个张量的形状相同。但它实际上并没有复制。它实际上是存储了一种内部指示器，告诉它假装这是一个三个零的向量，但实际上它不是去下一行或下一个标量，而是回到它来的地方。如果你对这个特别感兴趣，它们将该轴上的步幅设置为零。这对于那些好奇的人来说是一个较为高级的概念。
- en: So we could do a + 1 [[54:52](https://youtu.be/PGC0UxakTvM?t=3292)]. It’s going
    to broadcast the scalar 1 to be [1, 1, 1] and then do element wise addition.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以做a + 1[[54:52](https://youtu.be/PGC0UxakTvM?t=3292)]。它将广播标量1为[1, 1, 1]，然后进行逐元素加法。
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We could do the same with a matrix. Here is our matrix.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对一个矩阵做同样的操作。这是我们的矩阵。
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 2 times that matrix is going to broadcast 2 to be [[2, 2, 2], [2,2,2], [2,2,2]],
    and then do element-wise multiplication. So that’s our most simple version of
    broadcasting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 那个矩阵的2倍将广播2为[[2, 2, 2]，[2,2,2]，[2,2,2]]，然后进行逐元素乘法。这就是我们广播的最简单版本。
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Broadcasting a vector to a matrix [[55:27](https://youtu.be/PGC0UxakTvM?t=3327)]
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将向量广播到矩阵[[55:27](https://youtu.be/PGC0UxakTvM?t=3327)]
- en: Here is a slightly more complex version of broadcasting. Here is an array called
    `c`. This is a rank 1 tensor.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是广播的一个稍微复杂的版本。这里有一个名为`c`的数组。这是一个秩1张量。
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: And here is our matrix `m` from before — rank 2 tensor. We can add `m + c`.
    So what’s going on here?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是之前的矩阵`m`——秩2张量。我们可以添加`m + c`。那么这里发生了什么？
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can see that what it’s done is to add [10, 20, 30] to each row.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到它所做的是将[10, 20, 30]添加到每一行。
- en: '![](../Images/a9fd0fce34448412f9a2654042ca9c56.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9fd0fce34448412f9a2654042ca9c56.png)'
- en: So we can kind of figure it seems to have done the same kind of idea as broadcasting
    a scaler, it’s like made copies of it. And then it treats those as if it’s a rank
    2 matrix. And now we can do element-wise addition.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以想象它似乎做了与广播标量相同类型的想法，就像复制了它。然后将这些视为秩2矩阵。现在我们可以进行逐元素加法。
- en: '![](../Images/8564ca32bf50a1786efb59a41fdf5d15.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8564ca32bf50a1786efb59a41fdf5d15.png)'
- en: '**Question**: By looking at this example, it copies it down making new rows.
    How would we want to do it if we wanted to get new columns [[56:50](https://youtu.be/PGC0UxakTvM?t=3410)]?
    I’m so glad you asked. So instead, we would do this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：通过查看这个例子，它将其复制下来生成新的行。如果我们想要获得新的列，我们应该如何做[[56:50](https://youtu.be/PGC0UxakTvM?t=3410)]？我很高兴你问了。所以我们会这样做：'
- en: '![](../Images/bc2677111cc4e584315b72c3164ebed0.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc2677111cc4e584315b72c3164ebed0.png)'
- en: And now treat that as our matrix. To get numpy to do that, we need to not pass
    in a vector but to pass in a matrix with one column (i.e. a rank 2 tensor). So
    basically, it turns out that numpy is going to think of a rank 1 tensor for these
    purposes as if it was a rank 2 tensor which represents a row. So in other words,
    that it is 1 by 3\. So we want to create a tensor which is 3 by 1\. There is a
    couple ways to do that. One is to use `np.expand_dims(c,1)` and if you then pass
    in this argument it says “please insert a length 1 axis here.” So in our case,
    we want to turn it into a 3 by 1, so if we said `expand_dims(c,1)` it changes
    the shape to (3, 1). So if we look at what that looks like, that looks like a
    column.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将其视为我们的矩阵。要让numpy这样做，我们需要传入一个矩阵而不是一个向量，而是传入一个具有一列的矩阵（即秩2张量）。基本上，numpy会将这个秩1张量视为秩2张量，表示一行。换句话说，它是1乘3。所以我们想要创建一个3乘1的张量。有几种方法可以做到这一点。一种方法是使用`np.expand_dims(c,1)`，如果你传入这个参数，它会说“请在这里插入一个长度为1的轴”。所以在我们的情况下，我们想将其转换为3乘1，所以如果我们说`expand_dims(c,1)`，它会将形状改变为(3,
    1)。所以如果我们看看它是什么样子的，它看起来像一列。
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'So if we now go that plus `m`, you can see it’s doing exactly what we hoped
    it would do which is to add 10, 20, 30 to the column [[58:50](https://youtu.be/PGC0UxakTvM?t=3530)]:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们现在加上`m`，你可以看到它确实做了我们希望它做的事情，即将10、20、30添加到列[[58:50](https://youtu.be/PGC0UxakTvM?t=3530)]：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now because the location of a unit axis turns out to be so important, it’s really
    helpful to experiment with creating these extra unit axes and know how to do it
    easily. `np.expand_dims` isn’t in my opinion the easiest way to do this. The easiest
    way is to index into the tensor with a special index `None`. What `None` does
    is it creates a new axis in that location of length 1\. So this is going to add
    a new axis at the start of length one.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在由于单位轴的位置是如此重要，所以通过实验创建这些额外的单位轴并知道如何轻松地做到这一点是非常有帮助的。在我看来，`np.expand_dims`并不是最容易的方法。最简单的方法是使用一个特殊的索引`None`来索引张量。`None`的作用是在那个位置创建一个长度为1的新轴。因此，这将在开始添加一个新的长度为1的轴。
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This is going to add a new axis at the end of length one.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在末尾添加一个新的长度为1的轴。
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Or why not do both
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 或者为什么不两者都做呢
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: So if you think about it, a tensor which has like 3 things in it, could be of
    any rank you like, you can just add unit axis all over the place. That way, we
    can decide how we want our broadcasting to work. So there is a pretty convenient
    thing in numpy called `broadcast_to` and what that does is it takes our vector
    and broadcasts it to that shape and shows us what that would look like.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你考虑一下，一个张量中有3个元素，可以是任何你喜欢的阶数，你可以随意添加单位轴。这样，我们可以决定我们希望广播的方式。所以在numpy中有一个非常方便的东西叫做`broadcast_to`，它的作用是将我们的向量广播到那个形状，并展示给我们看看那会是什么样子。
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So if you are ever unsure of what’s going on in some broadcasting operation,
    you can say `broadcast_to`. So for example here, we could say rather than (3,3),
    we could say `m.shape` and see exactly what’s going to happen.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你对某个广播操作中发生的事情感到不确定，你可以使用`broadcast_to`。例如，在这里，我们可以说，而不是(3,3)，我们可以说`m.shape`，看看将会发生什么。
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And that’s what’s going to happen before we add it to `m`. So if we said turn
    it into a column, that’s what that looks like:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是在我们将其添加到`m`之前会发生的事情。所以如果我们说将其转换为列，那就是它的样子：
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So that’s kind of the intuitive definition of broadcasting. And so now hopefully
    we can go back to that numpy documentation and understand what it means.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是广播的直观定义。现在希望我们可以回到那个numpy文档并理解它的含义。
- en: 'From the [Numpy Documentation](https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html)
    [[1:01:37](https://youtu.be/PGC0UxakTvM?t=3697)]:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从[Numpy文档](https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html)
    [[1:01:37](https://youtu.be/PGC0UxakTvM?t=3697)]：
- en: The term broadcasting describes how numpy treats arrays with different shapes
    during arithmetic operations. Subject to certain constraints, the smaller array
    (lower rank tensor) is “broadcast” across the larger array so that they have compatible
    shapes. Broadcasting provides a means of vectorizing array operations so that
    looping occurs in C instead of Python. It does this without making needless copies
    of data and usually leads to efficient algorithm implementations.
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 广播这个术语描述了numpy在算术运算期间如何处理具有不同形状的数组。在一定的约束条件下，较小的数组（较低秩的张量）被“广播”到较大的数组上，以便它们具有兼容的形状。广播提供了一种向量化数组操作的方法，使循环发生在C而不是Python中。它可以在不进行不必要的数据复制的情况下实现这一点，并通常导致高效的算法实现。
- en: “Vectorizing” generally means using SIMD and stuff like that so that multiple
    things happen at the same time. It doesn’t actually make needless copies of data,
    it just acts as if it had. So there is our definition.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: “向量化”通常意味着使用SIMD等技术，以便多个操作同时进行。它实际上并不会进行不必要的数据复制，只是表现得好像进行了复制。所以这就是我们的定义。
- en: Now in deep learning, you very often deal with tensors of rank 4 or more, and
    you very often combine them with tenors of rank 1 or 2, and trying to just rely
    on intuition to do that correctly is nearly impossible. So you really need to
    know the rules.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在深度学习中，你经常处理4阶或更高阶的张量，并且经常将它们与1阶或2阶的张量结合在一起，仅凭直觉正确地执行这些操作几乎是不可能的。所以你真的需要了解规则。
- en: '![](../Images/375a9b8c17006a72d4106b90521b8e86.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/375a9b8c17006a72d4106b90521b8e86.png)'
- en: Here is `m.shape` and `c.shape` [[1:02:45](https://youtu.be/PGC0UxakTvM?t=3765)].
    So the rules are that we are going to compare the shapes of our two tensors element-wise.
    We are going to look at one at a time, and we are going to start at the end and
    go towards the front. Two dimensions are going to be compatible when one of these
    two things is true. So let’s check if our `m` and `c` are compatible. So we are
    going to start at the end (trailing dimensions first) and check “are they compatible?”
    They are compatible if the dimensions are equal. So these ones are equal, so they
    are compatible. Let’s go to the next one. Uh-oh, we are missing. `c` is missing
    something. So what happens if something is missing is we insert a 1\. That’s the
    rule. So let’s now check — are these compatible? One of them is one, yes, they
    are compatible. So now you can see why it is that numpy treats the one dimensional
    array as if it is a rank 2 tensor which is representing a row. It’s because we
    are basically inserting a 1 at the front. So that’s the rule.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`m.shape`和`c.shape` [[1:02:45](https://youtu.be/PGC0UxakTvM?t=3765)]。所以规则是我们将逐个元素地比较我们两个张量的形状。我们将一次查看一个，并且我们将从末尾开始向前移动。当这两个条件之一为真时，两个维度将是兼容的。所以让我们检查一下我们的`m`和`c`是否兼容。所以我们将从末尾开始（首先是尾部维度）并检查“它们是否兼容？”如果维度相等，则它们是兼容的。让我们继续下一个。哦，我们缺少了。`c`缺少一些东西。如果有东西缺失会发生什么，我们会插入一个1。这就是规则。所以现在让我们检查一下——这些是否兼容？其中一个是1，是的，它们是兼容的。所以现在你可以看到为什么numpy将一维数组视为一个代表行的2阶张量。这是因为我们基本上在前面插入了一个1。这就是规则。
- en: '![](../Images/a6e19ad9b3ef1edfedac008f87638bc5.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6e19ad9b3ef1edfedac008f87638bc5.png)'
- en: When operating on two arrays, Numpy/PyTorch compares their shapes element-wise.
    It starts with the **trailing dimensions**, and works its way forward. Two dimensions
    are **compatible** when
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在对两个数组进行操作时，Numpy/PyTorch会逐个元素地比较它们的形状。它从**尾部维度**开始，逐步向前推进。当两个维度**兼容**时
- en: they are equal, or
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是相等的，或者
- en: one of them is 1
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中之一是1
- en: 'Arrays do not need to have the same number of dimensions. For example, if you
    have a `256*256*3` array of RGB values, and you want to scale each color in the
    image by a different value, you can multiply the image by a one-dimensional array
    with 3 values. Lining up the sizes of the trailing axes of these arrays according
    to the broadcast rules, shows that they are compatible:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 数组不需要具有相同数量的维度。例如，如果你有一个`256*256*3`的RGB值数组，并且你想要按不同的值缩放图像中的每种颜色，你可以将图像乘以一个具有3个值的一维数组。根据广播规则对齐这些数组的尾部轴的大小，显示它们是兼容的：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For example, above is something that you very commonly have to do which is you
    start with an image, 256 pixel by 256 pixel by 3 channels [[1:04:11](https://youtu.be/PGC0UxakTvM?t=3851)].
    And you want to subtract the mean of each channel. So you’ve got 256 by 256 by
    3 and you want to subtract something of length 3\. So yes, you can do that. Absolutely.
    Because 3 and 3 are compatible because they are the same. 256 and empty is compatible
    because it’s going to insert a 1\. 256 and empty is compatible because it’s going
    to insert a 1\. So you are going to end up with this (the mean of each channel)
    is going to be broadcast over all of this axis (second from the right) and then
    that whole thing will be broadcast over this left most axis and so we will end
    up with a 256 by 256 by 3 effective tensor here.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，上面是你经常需要做的事情，即你从一幅图像开始，256像素乘以256像素乘以3个通道。你想要减去每个通道的平均值。所以你有256乘以256乘以3，你想要减去长度为3的东西。是的，你可以做到。绝对可以。因为3和3是兼容的，因为它们是相同的。256和空是兼容的，因为它会插入一个1。256和空是兼容的，因为它会插入一个1。所以你最终会得到这个（每个通道的平均值）会在所有这个轴（从右边数第二个）上广播，然后整个东西将在这个最左边的轴上广播，所以我们最终会得到一个256乘以256乘以3的有效张量。
- en: So interestingly [[1:05:15](https://youtu.be/PGC0UxakTvM?t=3915)], very few
    people in the data science or machine learning communities understand broadcasting
    and the vast majority of the time, for example, when I see people doing pre-processing
    for computer vision like subtracting the mean, they always write loops over the
    channels. And I think it’s so handy to not have to do that and it’s often so much
    faster to not have to do that. So if you get good at broadcasting, you’ll have
    this super useful skill that very very few people have. And it’s an ancient skill.
    It goes all the way back to the days of APL. So APL was from the late 50’s which
    stands for A Programming Language, and Kenneth Iverson wrote this paper called
    notation as a tool for thought in which he proposed a new math notation. And he
    proposed that if we use this new math notation, it gives us new tools for thought
    and allows us to think things we couldn’t before. One of his ideas was broadcasting,
    not as a computer programming tool but as a piece of math notation. So he ended
    up implementing this notation as a tool for thought as a programming language
    called APL. And his son has gone on to further develop that into a piece of software
    called [J](http://jsoftware.com/) which is basically what you get when you put
    60 years of very smart people working on this idea. And with this programming
    language, you can express very complex mathematical ideas often just with a line
    of code or two. So it’s great that we have J but it’s even greater that these
    ideas have found their ways into the languages we all use like in Python the numpy
    and PyTorch libraries. These are not just little niche ideas, it’s like fundamental
    ways to think about math and to do programming.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，数据科学或机器学习社区中很少有人理解广播，大多数时候，例如，当我看到人们为计算机视觉进行预处理时，比如减去平均值，他们总是在通道上写循环。我认为不必这样做非常方便，而且通常不必这样做速度更快。所以如果你擅长广播，你将拥有这种非常少数人拥有的超级有用的技能。这是一种古老的技能。它可以追溯到
    APL 的时代。APL 是上世纪50年代的，代表着 A Programming Language，肯尼斯·艾弗森写了一篇名为“符号作为思维工具”的论文，他在其中提出了一种新的数学符号。他提出，如果我们使用这种新的数学符号，它会给我们提供新的思维工具，让我们能够思考以前无法思考的事情。他的一个想法就是广播，不是作为计算机编程工具，而是作为数学符号的一部分。因此，他最终将这种符号实现为一种思维工具，作为一种名为
    APL 的编程语言。他的儿子继续将其进一步发展为一种名为 J 的软件，这基本上是当你将60年来非常聪明的人们致力于这个想法时得到的结果。通过这种编程语言，你可以用一两行代码表达非常复杂的数学思想。我们有
    J 是很棒的，但更棒的是这些想法已经进入我们所有人使用的语言中，比如在 Python 中的 numpy 和 PyTorch 库。这些不仅仅是一些小众的想法，它们是思考数学和进行编程的基本方式。
- en: 'Let me give an example of this kind of notation as a tool for thought [[1:07:33](https://youtu.be/PGC0UxakTvM?t=4053)].
    Here we’ve got c:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我举个例子，这种符号作为思维工具的例子。这里我们有 c：
- en: '![](../Images/491c3354e57f50f63d0ae696ae7eb588.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/491c3354e57f50f63d0ae696ae7eb588.png)'
- en: 'Here we’ve got `c[None]`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有 `c[None]`：
- en: '![](../Images/3dfa10097462cc7dc03f85e0ad576b0d.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dfa10097462cc7dc03f85e0ad576b0d.png)'
- en: 'Notice this is now two square brackets. So this is kind of like a one row vector
    tensor. Here it is a little column:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意现在有两个方括号。这有点像一个一行向量张量。这里是一个小列：
- en: '![](../Images/b6629f8ed57863049a862aeb2fb8bb53.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6629f8ed57863049a862aeb2fb8bb53.png)'
- en: So what is this going to do?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这将会做什么呢？
- en: '![](../Images/69aa535456c1103077c2bf7440e2d375.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69aa535456c1103077c2bf7440e2d375.png)'
- en: So to think of this in terms of like those broadcasting rules [[1:09:13](https://youtu.be/PGC0UxakTvM?t=4153)],
    we are basically taking this column which is of dimension (3, 1) and this row
    which is of dimension (1, 3). So make these compatible with our broadcasting rules,
    the column has to be duplicated 3 times because it needs to match 3\. The row
    has to be duplicated 3 times to match 3\. So now I’ve got two matrices to do an
    element wise product of.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 所以从广播规则的角度来看，我们基本上是将这个列（维度为（3，1））和这个行（维度为（1，3））进行操作。为了使这些符合我们的广播规则，列必须被复制3次，因为它需要匹配3。行必须被复制3次以匹配3。所以现在我有两个矩阵可以进行逐元素乘积。
- en: '![](../Images/7f12961ea749e7d59e5dc551a7eb81a7.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f12961ea749e7d59e5dc551a7eb81a7.png)'
- en: So as you say, there is our outer product.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 所以正如你所说，这是我们的外积。
- en: '![](../Images/833ad728efd356e3ffa6ec77d1ad956d.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/833ad728efd356e3ffa6ec77d1ad956d.png)'
- en: 'Now the interesting thing here is that suddenly now that this is not a special
    mathematical case but just a specific version of the general idea of broadcasting,
    we can do like an outer plus:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有趣的是，突然间这不再是一个特殊的数学案例，而只是广播这个一般概念的一个特定版本，我们可以像外加一样：
- en: '![](../Images/716af41d721f4ab7c3c93770968b512f.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/716af41d721f4ab7c3c93770968b512f.png)'
- en: 'Or outer greater than:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 或者外大于：
- en: '![](../Images/788870b82dda6f95b4fc22012e16e867.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/788870b82dda6f95b4fc22012e16e867.png)'
- en: Or whatever. So it’s suddenly we’ve got this concept that we can use build new
    ideas and then we can start to experiment with those new ideas.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 或者其他。所以突然间我们有了这个概念，我们可以用它来构建新的想法，然后我们可以开始尝试这些新的想法。
- en: 'Interestingly, numpy actually uses this sometimes [[1:10:52](https://youtu.be/PGC0UxakTvM?t=4252)].
    For example, if you want to create a grid, this is how numpy does it:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，numpy 有时会使用这个方法。例如，如果你想创建一个网格，numpy 就是这样做的：
- en: '![](../Images/d17e901488fdf40dd79f4f1549013d03.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d17e901488fdf40dd79f4f1549013d03.png)'
- en: 'It actually returns 0, 1, 2, 3, 4; one as a column, one as a row. So we could
    say okay that’s x grid (`xg`) comma y grid (`yg`) and now you could do something
    like that:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上返回的是0、1、2、3、4；一个作为列，一个作为行。所以我们可以说好的，这是x网格（`xg`）逗号y网格（`yg`），现在你可以做类似这样的事情：
- en: '![](../Images/4dd46bdf51376cc71e8595d12d435575.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dd46bdf51376cc71e8595d12d435575.png)'
- en: So suddenly we’ve expanded that out into a grid. So it’s kind of interesting
    how some of these simple little concepts get built on and built on and built on.
    So if you lose something like APL or J, it’s this whole environment of layers
    and layers of this. We don’t have such a deep environment in numpy but you can
    certainly see these idea of broadcasting coming through in simple things like
    how do we create a grid in numpy.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 所以突然间我们把它扩展成了一个网格。所以有趣的是一些简单的概念是如何被不断地建立和发展的。所以如果你熟悉APL或J，这是一个由许多层层叠加而成的整个环境。虽然在numpy中我们没有这样深层次的环境，但你肯定可以看到这种广播的想法在简单的事情中体现出来，比如我们如何在numpy中创建一个网格。
- en: Implementing matrix multiplication [[1:12:30](https://youtu.be/PGC0UxakTvM?t=4350)]
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现矩阵乘法[[1:12:30](https://youtu.be/PGC0UxakTvM?t=4350)]
- en: So that’s broadcasting and so what we can do with this now is use this to implement
    matrix multiplication ourselves. Now why would we want to do that? Well, obviously
    we don’t. Matrix multiplication has already been handled perfectly nicely for
    us by our libraries. But very often you’ll find in all kinds of areas in machine
    learning and particularly in deep learning that there’ll be particular types of
    linear function that you want to do that aren’t quite done for you. For example,
    there’s whole areas called tensor regression and tensor decomposition which are
    really being developed a lot at the moment and they are talking about how do we
    take higher rank tensors and turn them into combinations of rows, columns, and
    faces. It turns out that when you do this, you can basically deal with really
    high dimensional data structures with not much memory and not much computation
    time. For example, there is a really terrific library called [TensorLy](https://github.com/tensorly/tensorly)
    which does a whole lot of this kind of stuff for you. So it’s a really really
    important area. It covers all of deep learning, lots of modern machine learning
    in general. So even though you’re not going to define matrix multiplication, you
    are very likely to wanting to find some other slightly different tensor product.
    So it’s really useful to understand how to do that.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是广播，现在我们可以使用这个来实现矩阵乘法。那么为什么我们要这样做呢？显然我们不需要。矩阵乘法已经被我们的库完美地处理了。但是很多时候你会发现在各种领域，特别是在深度学习中，会有一些特定类型的线性函数，你想要做的事情并没有完全为你做好。例如，有一个叫做张量回归和张量分解的领域，目前正在得到很大的发展，它们讨论的是如何将高阶张量转化为行、列和面的组合。事实证明，当你这样做时，你基本上可以处理非常高维的数据结构，而不需要太多的内存和计算时间。例如，有一个非常棒的库叫做[TensorLy](https://github.com/tensorly/tensorly)，它为你做了很多这样的事情。所以这是一个非常重要的领域。它涵盖了所有的深度学习，还有很多现代机器学习。所以即使你不会定义矩阵乘法，你很可能会想要找到一些其他略有不同的张量积。所以了解如何做这个是非常有用的。
- en: Let’s go back and look at our 2D array and 1D array, rank 2 tensor and rank
    1 tensor [[1:14:27](https://youtu.be/PGC0UxakTvM?t=4467)].
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回过头来看看我们的二维数组和一维数组，秩为2的张量和秩为1的张量[[1:14:27](https://youtu.be/PGC0UxakTvM?t=4467)]。
- en: '[PRE29]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Remember, we can do a matrix multiplication using the @ sign or the old way,
    `np.matmul`. What that’s actually doing when we do that is we are basically saying
    `1*10 + 2*20 + 3*30 = 140`, so we do that for each row and we can go through and
    do the same thing for the next one, and so on to get our result.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们可以使用@符号或旧方法`np.matmul`进行矩阵乘法。当我们这样做时，实际上我们基本上是在说`1*10 + 2*20 + 3*30 = 140`，所以我们对每一行都这样做，然后我们可以继续对下一行做同样的事情，依此类推以获得我们的结果。
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You could do that in PyTorch as well
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在PyTorch中这样做
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: But this (`m * c`) is not matrix multiplication. What’s that? Element-wise with
    broadcasting. But notice, the numbers it has created [10, 40, 90] are the exact
    three numbers that I needed to calculate when I did that first piece of my matrix
    multiplication (`1*10 + 2*20 + 3*30`).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 但是（`m * c`）这不是矩阵乘法。那是什么？逐元素广播。但请注意，它创建的数字[10, 40, 90]正是我在做矩阵乘法的第一部分时需要计算的三个确切数字（`1*10
    + 2*20 + 3*30`）。
- en: '[PRE32]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'So in other words, if we sum this over the columns which is axis equals 1,
    we get our matrix vector product:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果我们对列求和，即轴等于1，我们就得到了矩阵向量乘积：
- en: '[PRE33]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So we can do this stuff without special help from our library. So now let’s
    expand this out to a matrix matrix product.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以在不依赖库的特殊帮助下做这些事情。现在让我们将这扩展到矩阵矩阵乘积。
- en: '![](../Images/0cdab326d71ab351c60a2c871cb144dc.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cdab326d71ab351c60a2c871cb144dc.png)'
- en: '[http://matrixmultiplication.xyz/](http://matrixmultiplication.xyz/)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://matrixmultiplication.xyz/](http://matrixmultiplication.xyz/)'
- en: 'So matrix matrix product looks like this. This is a great site called [matrixmultiplication.xyz](http://matrixmultiplication.xyz/)
    and it shows us this is what happens when we multiply two matrices. That’s what
    matrix multiplication is, operationally speaking. So in other words, what we just
    did there was we first of all took the first column with the first row to get
    this 15:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵矩阵乘积看起来是这样的。有一个很棒的网站叫做[matrixmultiplication.xyz](http://matrixmultiplication.xyz/)，它向我们展示了当我们将两个矩阵相乘时会发生什么。从操作的角度来看，这就是矩阵乘法。换句话说，我们刚刚做的是首先取第一列和第一行相乘得到15：
- en: '![](../Images/2fd468f2db46e43dbe3c837d72d3e38c.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fd468f2db46e43dbe3c837d72d3e38c.png)'
- en: 'Then we took the second column with the first row to get 27:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们取第二列和第一行得到27：
- en: '![](../Images/976c482ea5f617efd85b8b956b6c289f.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/976c482ea5f617efd85b8b956b6c289f.png)'
- en: 'So we are basically doing the thing we just did, the matrix vector product,
    we are just doing it twice. Once with this column (left), once with that column
    (right), and then we concatenate the two together. So we can now go ahead and
    do that like so:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们基本上是在做我们刚刚做的事情，矩阵向量乘积，我们只是做了两次。一次用这一列（左边），一次用那一列（右边），然后我们把这两个连接在一起。所以我们现在可以继续这样做：
- en: '[PRE34]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: So there are the two columns of our matrix multiplication.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们矩阵乘法的两列。
- en: I didn’t want to make our code too messy so I’m not going to actually use that,
    but we have it there now if we want to. We don’t need to use torch or numpy matrix
    multiplication anymore. We’ve got our own that we can use, using nothing but element
    wise operations, broadcasting, and sum.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想让我们的代码太乱，所以我不打算真的使用那个，但是现在我们有了它，如果我们想要的话。我们不再需要使用torch或numpy矩阵乘法。我们有自己的方法可以使用，只使用逐元素操作、广播和求和。
- en: Tis is our logistic regression from scratch class again [[1:18:37](https://youtu.be/PGC0UxakTvM?t=4717)].
    I just copied it here.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们从头开始的逻辑回归类[[1:18:37](https://youtu.be/PGC0UxakTvM?t=4717)]。我只是把它复制到这里。
- en: '[PRE35]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here is where we instantiate the object, copy to the GPU. We create an optimizer
    which we will learn about in a moment. And we call fit.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们实例化对象的地方，复制到GPU。我们创建一个优化器，我们将在稍后学习。然后我们调用fit。
- en: '[PRE36]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Writing Our Own Training Loop [[1:18:53](https://youtu.be/PGC0UxakTvM?t=4733)]
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写我们自己的训练循环[[1:18:53](https://youtu.be/PGC0UxakTvM?t=4733)]
- en: So the goal is to now repeat this without needing to call fit. To do that, we
    are going to need a loop which grabs a mini batch of data at a time. And with
    each mini batch of data, we need to pass it to the optimizer and say “please try
    to come up with a slightly better set of predictions for this mini batch.”
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 所以目标是现在重复这一过程，而无需调用fit。为此，我们需要一个循环，每次抓取一个小批量的数据。对于每个小批量的数据，我们需要将其传递给优化器，并说“请尝试为这个小批量提供稍微更好的预测”。
- en: As we learnt, in order to grab a mini batch of the training set at a time, we
    have to ask the model data object for the training data loader. We have to wrap
    it in `iter` to create an iterator or a generator. So that gives us our data loader.
    So PyTorch call this a data loader. We actually wrote our own Fast AI data loader,
    but it’s basically the same idea.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们学到的，为了一次抓取训练集的一个小批次，我们必须向模型数据对象请求训练数据加载器。我们必须将其包装在`iter`中以创建一个迭代器或生成器。这样就给我们了我们的数据加载器。所以PyTorch将其称为数据加载器。我们实际上编写了自己的Fast
    AI数据加载器，但基本上是相同的思路。
- en: '[PRE37]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So the next thing we do is we grab the x and the y tensor, the next one from
    our data loader. Wrap it in a `Variable` to say I need to be able to take the
    derivative of the calculations using this. Because if I can’t take the derivative,
    then I can’t get the gradients and I can’t update the weights. And I need to put
    it on the GPU because my module is on the GPU (`net2 = LogReg().cuda()`). So we
    can now take that variable and pass it to the object that we instantiated (i.e.
    our logistic regression). Remember, our module, we can use it as if it’s a function
    because that’s how PyTorch works. And that gives us a set of predictions as we’ve
    seen before.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要做的是获取x和y张量，从我们的数据加载器中获取下一个。将其包装在`Variable`中以表明我需要能够对使用此计算的导数进行求导。因为如果我不能求导，那么我就无法得到梯度，也无法更新权重。而且我需要将其放在GPU上，因为我的模块在GPU上（`net2
    = LogReg().cuda()`）。所以现在我们可以将该变量传递给我们实例化的对象（即我们的逻辑回归）。记住，我们的模块，我们可以将其用作函数，因为这就是PyTorch的工作原理。这给我们提供了一组预测，就像我们以前看到的那样。
- en: '[PRE38]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: So now we can check the loss [[1:20:41](https://youtu.be/PGC0UxakTvM?t=4841)].
    And the loss, we defined as being a negative log likelihood loss object. We are
    going to learn about how that’s calculated in the next lesson and for now, think
    of it just like a root mean squared error but for classification problems. So
    we can call that also just like a function. So you can kind of see this is very
    general idea in PyTorch that treat everything ideally like it’s a function. In
    this case, we have a negative log likelihood loss object, we could treat it like
    a function. We pass in our predictions and we pass in our actuals. Again, the
    actuals need to be turned into a variable and put on the GPU because the loss
    is specifically the thing that we actually want to take the derivative of. So
    that gives us our loss, and there it is.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查损失[[1:20:41](https://youtu.be/PGC0UxakTvM?t=4841)]。我们定义损失为负对数似然损失对象。我们将在下一课中学习如何计算它，现在，只需将其视为分类问题的均方根误差。所以我们也可以像调用函数一样调用它。所以你可以看到这在PyTorch中是一个非常普遍的想法，将一切都理想地视为函数。在这种情况下，我们有一个负对数似然损失对象，我们可以将其视为函数。我们传入我们的预测和实际值。同样，实际值需要转换为变量并放在GPU上，因为损失是我们实际想要求导的东西。这给我们带来了我们的损失，就是这样。
- en: '[PRE39]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'That’s our loss 2.43\. So it’s a variable and because it’s a variable, it knows
    how it was calculated. It knows it was calculated with this loss function (`loss`).
    It knows that the predictions were calculated with this network (`net2`). It knows
    that this network consisted of these operations:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的损失2.43。所以它是一个变量，因为它是一个变量，它知道它是如何计算的。它知道它是用这个损失函数（`loss`）计算的。它知道预测是用这个网络（`net2`）计算的。它知道这个网络由这些操作组成：
- en: '![](../Images/30b0a7a4cd47670ff3f3826c30b85141.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30b0a7a4cd47670ff3f3826c30b85141.png)'
- en: So we can get the gradient automatically. To get the gradient, we call `l.backward()`.
    Remember `l` is the thing that contains our loss. So `l.backward()` is something
    which is added to anything that’s a variable. You then call `.backward()` and
    that says please calculate the gradients. So that calculates the gradients and
    stores them inside, basically for each of the weights/parameters that was used
    to calculate that, it’s now stored in `.grad` — we will see it later but it’s
    basically stored the gradients. So we can then call `optimizer.step()` and we
    are going to do this step manually shortly. And that’s the bit that says please
    make the weights a little bit better.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以自动获取梯度。要获取梯度，我们调用`l.backward()`。记住`l`是包含我们损失的东西。所以`l.backward()`是添加到任何变量的东西。然后调用`.backward()`，这表示请计算梯度。这样就计算了梯度并将其存储在内部，基本上对于用于计算的每个权重/参数，现在都存储在`.grad`中，我们稍后会看到，但基本上存储了梯度。然后我们可以调用`optimizer.step()`，我们很快将手动执行这一步。这部分表示请让权重变得更好一点。
- en: '![](../Images/7accbc7a8d8f6330380d946374051bb0.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7accbc7a8d8f6330380d946374051bb0.png)'
- en: optimizer.step [[1:22:49](https://youtu.be/PGC0UxakTvM?t=4969)]
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: optimizer.step [[1:22:49](https://youtu.be/PGC0UxakTvM?t=4969)]
- en: So what `optimizer.step()` is doing is if you had a really simple function like
    this, what the optimizer does is it says okay, let’s pick a starting point, let’s
    calculate the value of the loss, let’s take the derivative which tells us which
    way is down. So it tells us we need to go that direction. And we take a small
    step.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`optimizer.step()`正在做的是，如果你有一个非常简单的函数像这样，优化器所做的就是说好的，让我们选择一个起始点，计算损失的值，计算导数告诉我们哪个方向是向下的。因此，它告诉我们我们需要朝那个方向走。然后我们迈出一小步。
- en: '![](../Images/6e5325322a942296c07c18f1bbc93642.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e5325322a942296c07c18f1bbc93642.png)'
- en: And then we take the derivative again, we take a small step, and repeat until
    eventually we are taking such small steps that we stop.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们再次取导数，采取一个小步骤，并重复，直到最终我们采取的步骤如此之小以至于停止。
- en: '![](../Images/31143403e8c38db561b897ebee2fada2.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31143403e8c38db561b897ebee2fada2.png)'
- en: So that’s what gradient descent does. How big a step is a small step? Well,
    basically take the derivative here, so let’s say derivative there is like 8\.
    And we multiply it by a small number, say, 0.01 and that tells us what step size
    to take. This small number here is called the learning rate, and it’s the most
    important hyper parameter to set. If you pick too small a learning rate, then
    your steps down are going to be tiny and it’s going to take you forever. Too big
    a learning rate and you jump too far and then you jump too far and you will diverge
    rather than converge.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是梯度下降的作用。小步骤有多大？基本上在这里取导数，所以让我们说导数是8。然后我们乘以一个小数，比如0.01，这告诉我们要采取什么步骤大小。这里的这个小数被称为学习率，它是设置的最重要的超参数。如果你选择的学习率太小，那么你的下降步骤将会很小，而且会花费很长时间。学习率太大，你会跳得太远，然后你会跳得太远，最终会发散而不是收敛。
- en: '![](../Images/af7a595aa044a5e86a3a3ee224799f71.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af7a595aa044a5e86a3a3ee224799f71.png)'
- en: We are not going to talk about how to pick a learning rate in this class, but
    in the deep learning class, we actually show you a specific technique that very
    reliably picks a very good learning rate.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这节课中我们不会讨论如何选择学习率，但在深度学习课程中，我们实际上向你展示了一种非常可靠地选择一个非常好的学习率的特定技术。
- en: So that’s basically what’s happening. We calculate the derivatives, we call
    the optimizer that does `step`, in other words update the weights based on the
    gradients and the learning rate.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基本上正在发生的是，我们计算导数，我们调用执行`step`的优化器，换句话说，根据梯度和学习率更新权重。
- en: We should hopefully find that after doing that we have a better loss than we
    did before [[1:25:03](https://youtu.be/PGC0UxakTvM?t=5103)]. So I just re-ran
    this and got a loss here of 4.16.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 希望在这样做之后，我们的损失比之前更好。因此，我刚刚重新运行了这个，得到了一个4.16的损失。
- en: '![](../Images/e9aa05a83d5ad78069fab38cec5478b1.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9aa05a83d5ad78069fab38cec5478b1.png)'
- en: After one step, it’s now 4.03.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一步之后，现在是4.03。
- en: '![](../Images/f6d620e458ae1a8f33853a897c4f4d52.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6d620e458ae1a8f33853a897c4f4d52.png)'
- en: So it worked the way we hoped it would based on this mini batch, it updated
    all of the weights in our network to be a little better than they were. As a result
    of which our loss went down.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它按照我们希望的方式运行，基于这个小批量，它更新了我们网络中的所有权重，使它们比之前更好。因此，我们的损失下降了。
- en: Training loop [[1:25:28](https://youtu.be/PGC0UxakTvM?t=5128)]
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'So let’s turn that into a training loop. We are going to go through a hundred
    steps:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们将其转化为一个训练循环。我们将进行一百步：
- en: Grab one more mini batch of data from the data loader
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据加载器中获取另一个小批量数据
- en: Calculate our predictions from our network
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从我们的网络中计算预测
- en: Calculate our loss from the predictions and the actuals
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从预测和实际值计算我们的损失
- en: Every 10 goes, we’ll print out the accuracy just take the mean of whether they
    are equal or not.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每10次，我们将打印出准确率，只需取平均值，看它们是否相等。
- en: One PyTorch specific thing, you have to zero the gradients. Basically you can
    have networks where you’ve got lots of different loss functions that you might
    want to add all of the gradients together. So you have to tell PyTorch when to
    set the gradients back to zero. So this just says set all the gradients to zero.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个PyTorch特定的事情，你必须将梯度清零。基本上，你可以有许多不同的损失函数的网络，你可能想要将所有的梯度加在一起。因此，你必须告诉PyTorch何时将梯度设置为零。因此，这只是说将所有的梯度设置为零。
- en: Calculate the gradients that’s called backward
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算梯度，这被称为反向传播
- en: Then take one step of the optimizer, so update the weights using the gradients
    and the learning rate
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后进行一步优化器，使用梯度和学习率更新权重
- en: '[PRE40]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once we run it, you can can see the loss goes down and the accuracy goes up.
    So that’s the basic approach. Next lesson, we will see what `optimizer.step()`
    does. We’ll look at it in detail. We are not going to look inside `l.backward()`
    as I said we are going to basically take the calculation of the derivative as
    given. But basically what’s happening there, in any kind of deep network, you
    have a function that’s like a linear function and then you pass the output of
    that into another function that might be like a ReLU. And you pass the output
    of that into another function that might be another linear layer, and so forth:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行它，你会看到损失下降，准确率上升。所以这是基本的方法。下一课，我们将看到`optimizer.step()`做了什么。我们将详细看一下。我们不会深入研究`l.backward()`，因为我说过我们基本上会将导数的计算视为给定的。但基本上，在任何深度网络中，你有一个类似于线性函数的函数，然后将其输出传递到另一个可能类似于ReLU的函数中。然后将其输出传递到可能是另一个线性层的函数中，依此类推：
- en: '*i( h( g( f(x) ) ) )*'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*i( h( g( f(x) ) ) )*'
- en: 'So these deep networks are just functions of functions of functions. So you
    could write them mathematically like that. So all back prop does is it says (let’s
    just simplify this down to the depth two version), we can say okay:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些深度网络只是函数的函数的函数。因此，你可以用数学方式写出它们。因此，反向传播所做的就是说（让我们将其简化为深度为二的版本），我们可以说：
- en: '*g( f(x) )*'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '*g( f(x) )*'
- en: '*u = f(x)*'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*u = f(x)*'
- en: 'Therefore, the derivative of *g(f(x))* we can calculate with the chain rule
    as:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以用链式法则计算*g(f(x))*的导数：
- en: '*g’(u) f’(x)*'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*g’(u) f’(x)*'
- en: So you can see, we can do the same thing for the functions of the functions
    of the functions. So when you apply a function to a function of a function, you
    can take the derivative just by taking the product of the derivatives of each
    of those layers. In neural networks, we call this back propagation. So when you
    hear back propagation, it just means use the chain rule to calculate the derivatives.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，我们可以对函数的函数的函数做同样的事情。因此，当你将一个函数应用于一个函数的函数时，你可以通过将这些层的导数的乘积来计算导数。在神经网络中，我们称之为反向传播。因此，当你听到反向传播时，它只是意味着使用链式法则来计算导数。
- en: 'So when you see a neural network defined like here:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当你看到一个神经网络像这样定义时：
- en: '![](../Images/c09eef012da5144d753b7adff5be8eab.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c09eef012da5144d753b7adff5be8eab.png)'
- en: If it’s defined sequentially, literally, all this means is apply this function
    to the input, apply this function to that, apply this function to that, etc. So
    this is just defining a composition of a function to a function to a function
    to a function. So although we are not going to bother with calculating the gradients
    ourselves, you can now see why it can do it as long as it internally knows what’s
    the derivative of to-the-power-of (`^`), what’s the derivative of sine, what’s
    the derivative of plus, and so forth. Then our Python code in here is just combining
    those things together. So it just needs to know how to compose them together with
    the chain rule and away it goes.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果按顺序定义，字面上，所有这意味着将这个函数应用于输入，将这个函数应用于那个，将这个函数应用于那个，依此类推。因此，这只是定义了一个函数到一个函数到一个函数到一个函数的组合。因此，虽然我们不打算自己计算梯度，但现在你可以看到为什么它可以这样做，只要它内部知道幂函数的导数是什么，正弦函数的导数是什么，加法的导数是什么，依此类推。然后我们在这里的Python代码只是将这些东西组合在一起。因此，它只需要知道如何用链式法则将它们组合在一起，然后就可以运行了。
- en: So I think we can leave it there for now and in the next class, we’ll go and
    see how to write our own optimizer and then we’ll have solved MNIST from scratch
    ourselves. See you then!
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为我们现在可以把它留在这里，在下一堂课中，我们将看看如何编写我们自己的优化器，然后我们将自己从头解决MNIST问题。到时见！
