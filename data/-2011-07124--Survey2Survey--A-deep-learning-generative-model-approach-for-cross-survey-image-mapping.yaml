- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:58:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:31
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2011.07124] Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2011.07124] Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.07124](https://ar5iv.labs.arxiv.org/html/2011.07124)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2011.07124](https://ar5iv.labs.arxiv.org/html/2011.07124)
- en: 'Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射'
- en: Brandon Buncher¹, Awshesh Nath Sharma², and Matias Carrasco Kind^(3,4,5)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Brandon Buncher¹，Awshesh Nath Sharma² 和 Matias Carrasco Kind^(3,4,5)
- en: ¹Department of Physics, University of Illinois, Champaign, IL 61820 USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹伊利诺伊大学物理系，香槟，IL 61820 美国
- en: ²Department of Earth Sciences, Indian Institute of Technology Roorkee, Roorkee,
    Uttarakhand 247667 India
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²印度理工学院鲁尔基分校，鲁尔基，乌塔拉坎德 247667 印度
- en: ³Department of Astronomy, University of Illinois, Urbana, IL 61801 USA
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³伊利诺伊大学天文学系，厄尔巴纳，IL 61801 美国
- en: ⁴National Center for Supercomputing Applications, Urbana, IL 61801 USA
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴国家超级计算应用中心，厄尔巴纳，IL 61801 美国
- en: ⁵Center for Astrophysical Surveys, Urbana, IL 61801 USA buncher2@illinois.edu(Accepted
    XXX. Received YYY; in original form ZZZ)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵天体物理调查中心，厄尔巴纳，IL 61801 美国 buncher2@illinois.edu（接受 XXX。收到 YYY；原始形式 ZZZ）
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: During the last decade, there has been an explosive growth in survey data and
    deep learning techniques, both of which have enabled great advances for astronomy.
    The amount of data from various surveys from multiple epochs with a wide range
    of wavelengths, albeit with varying brightness and quality, is overwhelming, and
    leveraging information from overlapping observations from different surveys has
    limitless potential in understanding galaxy formation and evolution. Synthetic
    galaxy image generation using physical models has been an important tool for survey
    data analysis, while deep learning generative models show great promise. In this
    paper, we present a novel approach for robustly expanding and improving survey
    data through cross survey feature translation. We trained two types of neural
    networks to map images from the Sloan Digital Sky Survey (SDSS) to corresponding
    images from the Dark Energy Survey (DES). This map was used to generate false
    DES representations of SDSS images, increasing the brightness and S/N while retaining
    important morphological information. We substantiate the robustness of our method
    by generating DES representations of SDSS images from outside the overlapping
    region, showing that the brightness and quality are improved even when the source
    images are of lower quality than the training images. Finally, we highlight several
    images in which the reconstruction process appears to have removed large artifacts
    from SDSS images. While only an initial application, our method shows promise
    as a method for robustly expanding and improving the quality of optical survey
    data and provides a potential avenue for cross-band reconstruction.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，调查数据和深度学习技术经历了爆炸式增长，这两者都极大地推动了天文学的发展。来自多个时代、不同波长的各种调查数据量庞大，尽管亮度和质量各异，但利用来自不同调查的重叠观测信息具有无限的潜力来理解银河形成和演化。使用物理模型生成合成银河图像已成为调查数据分析的重要工具，而深度学习生成模型展现了巨大的潜力。在本文中，我们提出了一种通过跨调查特征转换来稳健扩展和改进调查数据的新方法。我们训练了两种神经网络，将斯隆数字天空调查（SDSS）的图像映射到与之对应的暗能量调查（DES）图像。这一映射被用来生成SDSS图像的虚假DES表示，增加了亮度和信噪比，同时保留了重要的形态信息。我们通过生成来自重叠区域之外的SDSS图像的DES表示来证明我们方法的稳健性，显示出即使源图像质量低于训练图像时，亮度和质量仍得到改善。最后，我们突出显示了几个重建过程中似乎去除了SDSS图像中大块伪影的图像。虽然仅为初步应用，我们的方法在稳健扩展和提高光学调查数据质量方面显示出潜力，并为跨波段重建提供了可能的途径。
- en: 'keywords:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'galaxies: formation – galaxies: evolution – techniques: image processing –
    surveys – virtual observatory tools^†^†pubyear: 2020^†^†pagerange: Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping–[A](#A1
    "Appendix A Additional Image Samples ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '银河：形成 – 银河：演化 – 技术：图像处理 – 调查 – 虚拟观测工具^†^†pubyear: 2020^†^†pagerange: Survey2Survey:
    一种深度学习生成模型方法用于跨调查图像映射–[A](#A1 "附录 A 额外图像样本 ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射")'
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The analysis of optical data at a wide frequency range collected by various
    astronomical surveys is a critical component used to study the origin and evolution
    of galaxies. Data on galaxy shape (Wang et al., [2019](#bib.bib57)) and luminosity
    (Padmanabhan & Loeb, [2020](#bib.bib36); Cortese et al., [2017](#bib.bib12)) in
    various bands provide information about the evolution of galaxies at different
    cosmic times. As each band provides information about different characteristics
    of each object, stronger conclusions may be drawn from studies that incorporate
    data from a wide range of wavelengths. While a large range of optical wavelengths
    is covered by most modern surveys, such as the Dark Energy Survey (DES; Abbott
    et al., [2018](#bib.bib3)) and the Sloan Digital Sky Survey (SDSS; Abazajian et al.,
    [2009](#bib.bib2); Jiang et al., [2014a](#bib.bib23)), the depth, the footprint,
    and signal-to-noise ratio (S/N) varies from survey to survey. Inparticular, these
    will be vastly improved with future surveys like the Legacy Survey of Space and
    Time (LSST) (Ivezić et al., [2019](#bib.bib21)). As a result, feature extraction
    in a particular band may be difficult in certain regions due to incomplete field
    coverage by surveys with high-quality data within that band.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过各种天文调查收集的宽频率范围内的光学数据分析是研究星系起源和演化的重要组成部分。关于星系形状的数据（Wang et al., [2019](#bib.bib57)）和光度（Padmanabhan
    & Loeb, [2020](#bib.bib36); Cortese et al., [2017](#bib.bib12)）在不同波段提供了有关星系在不同宇宙时代演化的信息。由于每个波段提供了有关每个物体不同特征的信息，结合来自广泛波长范围的数据的研究可能会得出更强的结论。虽然大多数现代调查（如暗能量调查（DES；Abbott
    et al., [2018](#bib.bib3)）和斯隆数字天空调查（SDSS；Abazajian et al., [2009](#bib.bib2);
    Jiang et al., [2014a](#bib.bib23)））覆盖了大范围的光学波长，但调查的深度、覆盖区域和信噪比（S/N）因调查而异。特别是，未来的调查如空间与时间遗产调查（LSST）（Ivezić
    et al., [2019](#bib.bib21)）将大大改善这些情况。因此，由于某些波段中高质量数据的调查覆盖不完全，可能会在特定区域提取特征变得困难。
- en: Prior Work
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先前的工作
- en: In order to understand the underlying galaxy formation model and physics behind
    galaxy properties, simulations are required to mimic observations; however, their
    systematics are computationally expensive. Synthetic image generation of individual
    objects via deep learning is an alternative method for synthetic sky catalog generation
    that avoids the time and computational expense of other physically driven simulations.
    Various neural network architectures have been used for this purpose, including
    variational autoencoders (Regier et al., [2015a](#bib.bib42), [b](#bib.bib43);
    Lanusse et al., [2020](#bib.bib26); Spindler et al., [2020](#bib.bib48)) and generative
    adversarial networks (GANs) (Smith & Geach, [2019](#bib.bib47); Ullmo et al.,
    [2020](#bib.bib50)). While these methods efficiently generate mock galaxy images,
    the accuracy of the output images depends on that of the input images. As a result,
    their physical information is fundamentally limited by the quality of the survey
    data they are trained with.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解潜在的星系形成模型及其背后的物理学，模拟是必需的，以模拟观测结果；然而，它们的系统性在计算上是昂贵的。通过深度学习生成单个物体的合成图像是一种替代方法，用于生成合成天空目录，这避免了其他物理驱动模拟所需的时间和计算开销。各种神经网络架构已被用于此目的，包括变分自编码器（Regier
    et al., [2015a](#bib.bib42), [b](#bib.bib43); Lanusse et al., [2020](#bib.bib26);
    Spindler et al., [2020](#bib.bib48)）和生成对抗网络（GANs）（Smith & Geach, [2019](#bib.bib47);
    Ullmo et al., [2020](#bib.bib50)）。尽管这些方法能够高效生成模拟星系图像，但输出图像的准确性取决于输入图像的质量。因此，它们的物理信息在根本上受到它们所训练的调查数据质量的限制。
- en: In addition to image generation, autoencoders of various types have been used
    for a number of purposes in astronomy, including anomaly detection (Villar et al.,
    [2020a](#bib.bib53)) and object classification (Ralph et al., [2019](#bib.bib41);
    Spindler et al., [2020](#bib.bib48); Villar et al., [2020b](#bib.bib54)). GANs
    have also been utilized for feature extraction (Shirasaki et al., [2019](#bib.bib46))
    and anomaly detection (Storey-Fisher et al., [2020](#bib.bib49)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像生成之外，各种类型的自编码器已被用于天文学中的多种目的，包括异常检测（Villar et al., [2020a](#bib.bib53)）和物体分类（Ralph
    et al., [2019](#bib.bib41); Spindler et al., [2020](#bib.bib48); Villar et al.,
    [2020b](#bib.bib54)）。GANs也已被用于特征提取（Shirasaki et al., [2019](#bib.bib46)）和异常检测（Storey-Fisher
    et al., [2020](#bib.bib49)）。
- en: Two autoencoder architectures that are of particular importance for this work
    are convolutional autoencoders (CAEs) (Masci et al., [2011](#bib.bib31)) and denoising
    autoencoders (DAEs) (Vincent et al., [2008](#bib.bib55)). CAEs have been utilized
    in astronomy for purposes including classification/feature extraction (Cai et al.,
    [2020](#bib.bib9); Cheng et al., [2020](#bib.bib10); Patel & Upla, [2019](#bib.bib37))
    and anomaly detection (Storey-Fisher et al., [2020](#bib.bib49)). DAEs take as
    input an artificially corrupted input and are trained to reconstruct a distortion-free
    representation of that input. DAEs are primarily used to eliminate noise from
    images (Graff et al., [2014](#bib.bib16)) and data (Shen et al., [2017](#bib.bib45)),
    as well as for feature extraction (Frontera-Pons et al., [2017](#bib.bib15); Wang
    et al., [2020](#bib.bib58)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这项工作特别重要的两种自编码器架构是卷积自编码器（CAEs）（Masci et al., [2011](#bib.bib31)）和去噪自编码器（DAEs）（Vincent
    et al., [2008](#bib.bib55)）。CAEs 已在天文学中用于分类/特征提取（Cai et al., [2020](#bib.bib9)；Cheng
    et al., [2020](#bib.bib10)；Patel & Upla, [2019](#bib.bib37)）和异常检测（Storey-Fisher
    et al., [2020](#bib.bib49)）。DAEs 接受人为损坏的输入，并被训练以重建该输入的无失真表示。DAEs 主要用于从图像（Graff
    et al., [2014](#bib.bib16)）和数据（Shen et al., [2017](#bib.bib45)）中消除噪声，以及进行特征提取（Frontera-Pons
    et al., [2017](#bib.bib15)；Wang et al., [2020](#bib.bib58)）。
- en: One little explored alternative for improving the size and quality of survey
    datasets is through the use of feature transfer techniques across survey data.
    A feature transfer model is trained to recognize differences between features
    in corresponding image pairs $\mathcal{X}$ and $\mathcal{Y}$ from datasets $\mathbb{X}$
    and $\mathbb{Y}$. Using an image from $\mathcal{X}^{\prime}\in\mathbb{X}$ as input,
    the trained neural network can then be used to construct a representation of this
    image with the features characteristic of images in $\mathbb{Y}$.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 改善调查数据集大小和质量的一个鲜有探索的替代方法是通过在调查数据之间使用特征迁移技术。特征迁移模型被训练以识别来自数据集 $\mathbb{X}$ 和
    $\mathbb{Y}$ 的相应图像对 $\mathcal{X}$ 和 $\mathcal{Y}$ 中特征的差异。使用来自 $\mathcal{X}^{\prime}\in\mathbb{X}$
    的图像作为输入，训练好的神经网络可以用于构建具有 $\mathbb{Y}$ 中图像特征的图像表示。
- en: 'In the context of astronomy and astrophysics, feature transfer learning using
    conditional GANs has recently found application for data analysis and feature
    extraction. Moriwaki et al. ([2020](#bib.bib33)) developed a method to extract/reconstruct
    H$\alpha$ line intensity maps from noisy hydrodynamic simulation data. In addition,
    Shirasaki et al. ([2019](#bib.bib46)) used feature transfer techniques to extract
    information from weak lensing maps. However, other modified GAN architectures
    can be used for feature transfer learning; in particular, cycle-consistent generative
    adversarial networks (CycleGANs; these are described in Section [3](#S3 "3 Methodology
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")) are particularly suited for image analysis and generation. Developed
    by Zhu et al. ([2017](#bib.bib62)) and Isola et al. ([2016](#bib.bib20)), CycleGANs
    have been used for image-to-image translation (feature transfer between paired
    or unpaired sets of images) (Jia et al., [2020](#bib.bib22); Liu et al., [2020](#bib.bib28);
    Luo et al., [2021](#bib.bib29); Osakabe et al., [2020](#bib.bib35); Maziarka et al.,
    [2019](#bib.bib32)). However, there has been minimal exploration of generative
    models using feature transfer learning in astronomy and astrophysics.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '在天文学和天体物理学的背景下，利用条件生成对抗网络（GANs）的特征迁移学习最近在数据分析和特征提取方面找到了应用。Moriwaki et al. ([2020](#bib.bib33))
    开发了一种从噪声的流体动力学模拟数据中提取/重建 H$\alpha$ 线强度图的方法。此外，Shirasaki et al. ([2019](#bib.bib46))
    使用特征迁移技术从弱透镜图中提取信息。然而，其他修改过的 GAN 架构也可以用于特征迁移学习；特别是，循环一致生成对抗网络（CycleGANs；这些在第 [3](#S3
    "3 Methodology ‣ Survey2Survey: A deep learning generative model approach for
    cross-survey image mapping") 节中描述）特别适用于图像分析和生成。CycleGANs 由 Zhu et al. ([2017](#bib.bib62))
    和 Isola et al. ([2016](#bib.bib20)) 开发，已用于图像到图像的转换（成对或未配对的图像集之间的特征迁移）（Jia et al.,
    [2020](#bib.bib22)；Liu et al., [2020](#bib.bib28)；Luo et al., [2021](#bib.bib29)；Osakabe
    et al., [2020](#bib.bib35)；Maziarka et al., [2019](#bib.bib32)）。然而，在天文学和天体物理学中对使用特征迁移学习的生成模型的探索仍然很有限。'
- en: Recently, Lin et al. ([2021](#bib.bib27)) used image-to-image translation to
    reconstruct high-frequency noise patterns characteristic to different astronomical
    surveys. The authors used several modified CycleGAN architectures with a semi-supervised
    training scheme using unpaired images to separate the signal and noise in images
    from two distinct surveys. A noise emulator is then used to reconstruct the noise
    patterns from each survey. The noise emulator can then be used to reconstruct
    images from a target dataset with said characteristic noise patterns. While several
    of their models were successful at emulating noise patterns, training using unpaired
    images hindered the reconstruction of small-scale features of the signal. Save
    for this work, the authors have been unable to identify any other use of CycleGANs
    in astronomy and astrophysics.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Lin 等人（[2021](#bib.bib27)）使用图像到图像的翻译技术重建了不同天文调查所特有的高频噪声模式。作者使用了几种经过修改的 CycleGAN
    架构，并采用了半监督训练方案，利用未配对的图像来分离来自两个不同调查的图像中的信号和噪声。然后使用噪声模拟器来重建每个调查的噪声模式。噪声模拟器随后可以用于重建具有这些特征噪声模式的目标数据集中的图像。尽管他们的几个模型在模拟噪声模式方面取得了成功，但使用未配对图像的训练妨碍了对信号的小尺度特征的重建。除了这项工作，作者未能识别出
    CycleGAN 在天文学和天体物理学中的其他应用。
- en: Methods other than feature transfer ones can hypothetically be used to generate
    representations of galaxies with altered parameters. In particular, fader networks
    (Lample et al., [2017](#bib.bib25); Perarnau et al., [2016](#bib.bib38)) have
    been used by Schawinski et al. ([2018](#bib.bib44)) for the purpose of testing
    hypotheses about mechanisms that drive galaxy formation. While this could be used
    as a method to transfer individual physical parameters of galaxies from one dataset
    to another, image reconstruction would not be feasible using this method because
    a large number of parameters must be known ab initio to generate faithful representations
    of images in the target dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征转移的方法外，理论上还可以使用其他方法生成具有不同参数的星系表示。特别是，**fader networks**（Lample 等，[2017](#bib.bib25)；Perarnau
    等，[2016](#bib.bib38)）已被 Schawinski 等人（[2018](#bib.bib44)）用于测试驱动星系形成机制的假设。虽然这可以作为将星系的个别物理参数从一个数据集转移到另一个数据集的方法，但由于需要知道大量参数才能生成目标数据集中图像的真实表示，因此使用这种方法进行图像重建是不可行的。
- en: We propose a novel method of feature transfer between galaxy surveys using CAEs
    and CycleGANs that can be used to expand galaxy image catalogs and can be adopted
    to multiple wavelengths and resolutions. By training these architectures with
    images from DES DR1 (Abbott et al., [2018](#bib.bib3)) paired with corresponding
    images from SDSS DR16 (Ahumada et al., [2020](#bib.bib5)), we demonstrate that
    information from DES images may be transferred to SDSS images, improving their
    S/N, contrast, and brightness. We show that the synthetic DES images reconstructed
    from SDSS images share the same characteristics as the true DES images, and that
    this consistency is retained when performing reconstructions using images from
    a separate set of lower quality SDSS images which do not have a counterpart in
    the DES catalog.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的星系调查之间的特征转移方法，结合 CAEs 和 CycleGANs，可以用于扩展星系图像目录，并可应用于多种波长和分辨率。通过使用来自
    DES DR1（Abbott 等，[2018](#bib.bib3)）的图像与来自 SDSS DR16（Ahumada 等，[2020](#bib.bib5)）的相应图像进行训练，我们证明了
    DES 图像中的信息可以转移到 SDSS 图像中，从而提高了它们的 S/N、对比度和亮度。我们展示了从 SDSS 图像重建的合成 DES 图像与真实 DES
    图像具有相同的特征，并且这种一致性在使用来自一组质量较低的 SDSS 图像进行重建时得以保留，这些图像在 DES 目录中没有对应项。
- en: 'While other works have demonstrated that variational autoencoders (Regier et al.,
    [2015a](#bib.bib42), [b](#bib.bib43); Lanusse et al., [2020](#bib.bib26); Spindler
    et al., [2020](#bib.bib48)) and GANs (Smith & Geach, [2019](#bib.bib47); Ullmo
    et al., [2020](#bib.bib50)) are effective at generating realistic synthetic galaxy
    images and improving the S/N, these models both train and validate using images
    from the same dataset. Our method utilizes techniques that are generally similar
    to these; however, by using different data to train (SDSS) and validate (DES),
    we are able to generate false images that share the same morphological features
    of the SDSS images, but with a brightness and S/N more characteristic of the DES
    images. Like other generative models, this can be used to increase the size of
    survey datasets; however, this method generates false representations of real
    observed galaxies. This provides benefit when studying the properties of galaxies
    in a specific region that has not yet be covered by high quality surveys. More
    importantly, transfer learning may allow for cross-band reconstruction: all surveys
    cover a limited range of wavelengths at sufficiently high quality for effective
    analysis, making feature extraction from particular bands impossible in certain
    regions of the sky. By training using images with fewer bands than the validation
    data, a feature transfer-based generative model may be able to generate synthetic
    representations of galaxies with a greater range of wavelength bands than the
    input image. This provides a method to allow more thorough analysis of galaxies
    in regions that lack sufficient band coverage.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其他研究表明变分自编码器（Regier et al., [2015a](#bib.bib42), [b](#bib.bib43); Lanusse
    et al., [2020](#bib.bib26); Spindler et al., [2020](#bib.bib48)）和GAN（Smith & Geach,
    [2019](#bib.bib47); Ullmo et al., [2020](#bib.bib50)）在生成真实合成星系图像和提高信噪比方面是有效的，但这些模型都在相同的数据集上进行训练和验证。我们的方法使用了与这些模型相似的技术；然而，通过使用不同的数据进行训练（SDSS）和验证（DES），我们能够生成与SDSS图像具有相同形态特征但亮度和信噪比更具DES图像特征的虚假图像。像其他生成模型一样，这可以用于增加调查数据集的规模；然而，这种方法生成了真实观测星系的虚假表示。当研究尚未被高质量调查覆盖的特定区域中的星系特性时，这提供了好处。更重要的是，迁移学习可能允许跨波段重建：所有调查覆盖了在有效分析中具有足够高质量的有限波长范围，使得在某些天空区域提取特定波段的特征变得不可能。通过使用波段少于验证数据的图像进行训练，基于特征转移的生成模型可能能够生成具有比输入图像更大波长范围的星系合成表示。这为在缺乏足够波段覆盖的区域中提供了更彻底分析星系的方法。
- en: 'In this work, we demonstrate the creation of Survey2Survey, a neural network
    architecture used to transfer features between SDSS and DES galaxy images that
    can be easily generalized to other optical surveys or even across multiple wavelengths.
    The parameters of the SDSS and DES datasets used for training and validation are
    described in Section [2](#S2 "2 Data ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). In Section [3](#S3 "3 Methodology
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), we detail the CAE and CycleGAN architectures used. In Section [4](#S4
    "4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), we present qualitative and quantitative metrics
    of the accuracy of the reconstructed image, then summarize our findings in Section
    [5](#S5 "5 Conclusions ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们展示了Survey2Survey的创建，这是一种神经网络架构，用于在SDSS和DES星系图像之间转移特征，且该方法可以轻松地推广到其他光学调查或甚至跨多个波长。用于训练和验证的SDSS和DES数据集的参数在第[2节](#S2
    "2 数据 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")中进行了描述。在第[3节](#S3 "3 方法论 ‣ Survey2Survey:
    一种用于跨调查图像映射的深度学习生成模型方法")中，我们详细介绍了使用的CAE和CycleGAN架构。在第[4节](#S4 "4 结果与分析 ‣ Survey2Survey:
    一种用于跨调查图像映射的深度学习生成模型方法")中，我们展示了重建图像的定性和定量准确性指标，然后在第[5节](#S5 "5 结论 ‣ Survey2Survey:
    一种用于跨调查图像映射的深度学习生成模型方法")中总结了我们的发现。'
- en: 2 Data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据
- en: In this section we describe the datasets used to carry out this study. We focused
    on optical data from the SDSS and DES surveys and the overlapping region in the
    Stripe82 (Jiang et al., [2014b](#bib.bib24)). All of the data used in this paper
    is publicly accessible via their respective websites.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们描述了用于进行本研究的数据集。我们重点关注来自SDSS和DES调查的光学数据以及Stripe82（Jiang et al., [2014b](#bib.bib24)）中的重叠区域。本文中使用的所有数据均可通过其各自的网站公开访问。
- en: All images consisted of three layers (one layer for each RGB channel), where
    the brightness of each pixel $P_{i}$ was represented by an 32-bit float, $0\leq
    P_{i}\leq 1$. Each SDSS image was 150 $\times$ 150 $\mathrm{\,pix}$, and each
    DES image was 228 $\times$ 228 $\mathrm{\,pix}$; the DES images were downscaled
    to match the dimensions of the SDSS images prior to training and reconstruction.
    After the reconstruction and prior to the analysis, each three-layer 150 $\times$
    150 $\mathrm{\,pix}$ image was reduced to a single layer by averaging over the
    RGB channels.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像由三层组成（每个 RGB 通道一层），每个像素 $P_{i}$ 的亮度由 32 位浮点数表示，$0\leq P_{i}\leq 1$。每张 SDSS
    图像为 150 $\times$ 150 $\mathrm{\,pix}$，每张 DES 图像为 228 $\times$ 228 $\mathrm{\,pix}$；在训练和重建之前，DES
    图像被缩小以匹配 SDSS 图像的尺寸。重建后，在分析之前，每张三层 150 $\times$ 150 $\mathrm{\,pix}$ 的图像通过对 RGB
    通道取平均值减少为单层图像。
- en: SDSS
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SDSS
- en: SDSS images were captured by the Ritchey-Chrètien altitude-azimuth telescope
    (Gunn et al., [2006](#bib.bib17)), the Irènèe du Pont Telescope (Bowen & Vaughan,
    [1973](#bib.bib7)), and the NMSU 1-Meter Telescope (Holtzman et al., [2010](#bib.bib18)).
    We selected a sample of galaxies in Stripe82 that overlapped with the DES footprint,
    and randomly sampled data from outside that region and within the northern cap
    for a total of 25,000 galaxies. We chose galaxies with band Petrosian magnitude
    limits $14<R<17.77$, $z<0.25$ and a resolution of $0.396\mathrm{\,arcsec}/\!\mathrm{\,pix}$,
    using the galaxy flag produced by SDSS to select high confidence galaxy images.
    Images of these galaxies were obtained from the SDSS cutout server¹¹1http://casjobs.sdss.org/ImgCutoutDR7.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: SDSS 图像由 Ritchey-Chrètien 高度-方位望远镜（Gunn 等，[2006](#bib.bib17)）、Irènèe du Pont
    望远镜（Bowen & Vaughan，[1973](#bib.bib7)）和 NMSU 1 米望远镜（Holtzman 等，[2010](#bib.bib18)）捕捉。我们选择了
    Stripe82 中与 DES 脚印重叠的星系样本，并从该区域外和北极帽区域内随机抽样数据，总计 25,000 个星系。我们选择了带有 Petrosian
    亮度限制 $14<R<17.77$、$z<0.25$ 和分辨率 $0.396\mathrm{\,arcsec}/\!\mathrm{\,pix}$ 的星系，使用
    SDSS 生成的星系标志来选择高置信度的星系图像。这些星系的图像来自 SDSS 剪切服务器¹¹1http://casjobs.sdss.org/ImgCutoutDR7。
- en: DES and Overlap Region
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DES 和重叠区域
- en: DES uses the Dark Energy Camera (DECam; Flaugher et al., [2015](#bib.bib14))
    mounted at the Blanco 4m telescope at the Cerro Tololo Inter-American Observatory
    (CTIO) in Chile to observe ${\sim}\,5000\mathrm{\,deg}^{2}$ of the southern sky
    in the $g$, $r$, $i$, $z$, and $Y$ broadband filters ranging from ${\sim}\,400\mathrm{\,nm}$
    to ${\sim}\,1000\mathrm{\,nm}$ in wavelength.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DES 使用安装在智利 Cerro Tololo Inter-American Observatory (CTIO) 的 Blanco 4 米望远镜上的
    Dark Energy Camera (DECam; Flaugher 等，[2015](#bib.bib14)) 观察南天 ${\sim}\,5000\mathrm{\,deg}^{2}$
    的 $g$、$r$、$i$、$z$ 和 $Y$ 广泛滤光片，波长范围从 ${\sim}\,400\mathrm{\,nm}$ 到 ${\sim}\,1000\mathrm{\,nm}$。
- en: We used images from the Dark Energy Survey DR1 release (Abbott et al., [2018](#bib.bib3)),
    which is comprised of over 10,000 co-added tiles of $0.534\mathrm{\,deg}^{2}$
    with a resolution of $0.263\mathrm{\,arcsec}/\mathrm{\,pix}$ and a depth reaching
    S/N ${\sim}\,10$ for extended objects up to $i_{AB}\,{\sim}\,23.1$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 Dark Energy Survey DR1 发布的图像（Abbott 等，[2018](#bib.bib3)），其由超过 10,000 张
    $0.534\mathrm{\,deg}^{2}$ 的共添加图像组成，分辨率为 $0.263\mathrm{\,arcsec}/\mathrm{\,pix}$，深度达到
    S/N ${\sim}\,10$，适用于扩展对象，最大 $i_{AB}\,{\sim}\,23.1$。
- en: 'We selected DES galaxies using a combination of filtered criteria in terms
    of the concentration and error in the magnitude model as recommended²²2https://des.ncsa.illinois.edu/releases/dr1/dr1-faq
    with $g<17$ located in the Stripe 82 region (Jiang et al., [2014b](#bib.bib24))
    corresponding to roughly $300\mathrm{\,deg}^{2}$ near the celestial equator. We
    selected all images from Stripe 82 that have an SDSS counterpart (Abazajian et al.,
    [2009](#bib.bib2); Jiang et al., [2014a](#bib.bib23)). These images were obtained
    using the public DES cutout service³³3https://des.ncsa.illinois.edu/desaccess.
    We removed images with incomplete coverage and cleaned the images of anomalies
    and contaminants such as stars using visual inspection. Each DES image was scaled
    to 150 $\times$ 150 $\mathrm{\,pix}$ to match the resolution of the SDSS images.
    We aligned the orientation and central pixels of each DES/SDSS image pair, and
    the final RGB composite was generated using the Lupton et al. ([2004](#bib.bib30))
    prescription in order to closely match the SDSS colors. Figure [1](#S2.F1 "Figure
    1 ‣ DES and Overlap Region ‣ 2 Data ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") shows examples of the galaxies
    selected, where we can see that the DES images appear brighter and more detailed
    than the SDSS images.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用了按照浓度和亮度模型误差过滤的标准选择了DES星系，如推荐的²²2https://des.ncsa.illinois.edu/releases/dr1/dr1-faq，$g<17$位于条纹82区域（Jiang
    et al., [2014b](#bib.bib24)），对应约$300\mathrm{\,deg}^{2}$的赤道附近区域。我们选择了所有具有SDSS对应图像的条纹82区域的图像（Abazajian
    et al., [2009](#bib.bib2)；Jiang et al., [2014a](#bib.bib23)）。这些图像是通过公共的DES剪裁服务³³3https://des.ncsa.illinois.edu/desaccess获得的。我们删除了覆盖不完整的图像，并通过人工检查清理了星星等异常和污染物。每张DES图像被缩放到150
    $\times$ 150 $\mathrm{\,pix}$以匹配SDSS图像的分辨率。我们对齐了每对DES/SDSS图像的方向和中心像素，最终的RGB合成图像使用Lupton
    et al. ([2004](#bib.bib30))的配方生成，以尽可能匹配SDSS的颜色。图[1](#S2.F1 "图 1 ‣ DES 和重叠区域 ‣
    2 数据 ‣ Survey2Survey: 跨调查图像映射的深度学习生成模型方法")展示了所选星系的示例，其中可以看到DES图像比SDSS图像更亮、更详细。'
- en: The overlap region was used for training and validation; each SDSS image in
    the overlap region had a DES counterpart. In total, there were 5,538 RGB images
    in the overlap region. 5,000 SDSS/DES image pairs were used for training the models,
    while the remaining 538 were used as the validation dataset. Because of the large
    variation in the brightness and spatial extent of objects in the SDSS and DES
    datasets, we chose to use a training dataset that was $\approx 5\times$ larger
    than those used by both (Isola et al., [2016](#bib.bib20)) and (Zhu et al., [2017](#bib.bib62)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重叠区域用于训练和验证；重叠区域中的每张SDSS图像都有一个DES对应图像。总共有5,538张重叠区域的RGB图像。5,000对SDSS/DES图像用于训练模型，而剩余的538对则用于验证数据集。由于SDSS和DES数据集中对象亮度和空间范围的巨大变化，我们选择使用一个约为两者使用的数据集$\approx
    5\times$大的训练数据集（Isola et al., [2016](#bib.bib20)；Zhu et al., [2017](#bib.bib62)）。
- en: The external dataset, which consisted of 25,076 from outside of the Stripe 82
    region, was used to provide evidence for the robustness of our methodology. These
    images were fainter and of lower S/N than the training and validation datasets.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 外部数据集，包括25,076张来自条纹82区域之外的图像，用于提供我们方法论鲁棒性的证据。这些图像比训练和验证数据集中的图像更暗，S/N比更低。
- en: '![Refer to caption](img/26c4ee7e60106311e8a5c89ab0f5856f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/26c4ee7e60106311e8a5c89ab0f5856f.png)'
- en: 'Figure 1: Sample images used from the Dark Energy Survey (DES) (top row) and
    Sloan Digital Sky Survey (SDSS) (bottom row) datasets. More examples can be seen
    throughout the text.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：来自暗能量调查（DES）（顶部行）和斯隆数字天空调查（SDSS）（底部行）数据集的样本图像。更多示例可见于全文。
- en: 3 Methodology
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'Convolutional Autoencoders (CAE) (Masci et al., [2011](#bib.bib31)) and Cycle-Consistent
    Generative Adversarial Networks (CycleGAN) (Durugkar et al., [2016](#bib.bib13))
    were used to generate synthetic galaxy images from the SDSS input images. Since
    the images were scaled, rotated, and centered so that each pair of pixels in a
    given image pair corresponded with one another, minimizing the loss function used
    for both models corresponded with minimizing the pixel-to-pixel differences between
    the reconstructed image and the DES target image. These two types of models differ
    in their implementation and objective function as described below. We did not
    perform any methods that have traditionally been used to reduce overfitting and
    provide data augmentation, such as image rotations and translations, for either
    the CAE or CycleGAN. Spatial transformations would have likely led to failure:
    as we intend to perform pixel-to-pixel translations, any misalignment of pixels
    would lead to the creation of an invalid mapping function. While this may not
    cause an issue in many other cases, spatial transformations on SDSS and DES images
    could drastically reduce the accuracy of the mapping function given the small
    spatial extent of the signal region relative to the background in many images.
    While this may have led to overfitting, the analysis of the external reconstructions
    provides evidence of the robustness of our method. As an initial application of
    image-to-image translation for false image generation, we chose to minimize the
    number of factors that could affect the pixel-to-pixel map; future research should
    be dedicated to establishing methods to ensure that overfitting does not occur.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积自编码器（CAE）（Masci 等人，[2011](#bib.bib31)）和循环一致生成对抗网络（CycleGAN）（Durugkar 等人，[2016](#bib.bib13)）被用于从
    SDSS 输入图像中生成合成星系图像。由于图像经过了缩放、旋转和居中处理，使得每对图像对中的像素彼此对应，因此最小化两个模型的损失函数与最小化重建图像与 DES
    目标图像之间的像素差异相对应。这两种模型在实现和目标函数上有所不同，如下所述。我们没有对 CAE 或 CycleGAN 进行传统上用于减少过拟合和提供数据增强的方法，例如图像旋转和翻译。空间变换可能会导致失败：由于我们打算进行像素对像素的翻译，任何像素的不对齐都会导致无效的映射函数的生成。虽然在许多其他情况下这可能不会造成问题，但在
    SDSS 和 DES 图像上的空间变换可能会由于信号区域相对于背景的空间范围较小而大幅降低映射函数的准确性。虽然这可能会导致过拟合，但外部重建的分析提供了我们方法鲁棒性的证据。作为对假图像生成的图像到图像翻译的初步应用，我们选择最小化可能影响像素对像素映射的因素；未来的研究应致力于建立方法，以确保不会发生过拟合。
- en: 3.1 Convolutional Autoencoders (CAE)
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 卷积自编码器（CAE）
- en: 'An autoencoder is a neural network architecture typically used for classification
    that is comprised of an encoder/decoder pair. The encoder compresses data from
    an input image using one or more hidden layers to isolate important features from
    that image, generating a latent space representation of that image with lower
    dimensionality. The decoder uses the information in the latent space to reconstruct
    a representation of the input image. The autoencoder is trained to optimize a
    loss function to minimize the difference between the source and reconstructed
    image. A convolutional autoencoder performs encoding and decoding using convolution
    filters: during the encoding stage, convolution filters are used to extract information
    from and decrease the dimensionality of the input image. Additional convolution
    filters are used to map the latent space representation to a reconstruction of
    the input image. Training is performed by iteratively modifying the weights of
    the convolution filters to minimize the differences between the source image and
    its reconstruction.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种神经网络架构，通常用于分类，由一个编码器/解码器对组成。编码器使用一个或多个隐藏层压缩来自输入图像的数据，以从该图像中隔离重要特征，生成具有较低维度的图像潜在空间表示。解码器使用潜在空间中的信息来重建输入图像的表示。自编码器被训练以优化损失函数，从而最小化源图像和重建图像之间的差异。卷积自编码器使用卷积滤波器进行编码和解码：在编码阶段，卷积滤波器用于从输入图像中提取信息并减少其维度。额外的卷积滤波器用于将潜在空间表示映射到输入图像的重建。训练通过迭代地修改卷积滤波器的权重，以最小化源图像及其重建之间的差异。
- en: 'Our CAE was implemented using Keras (Chollet et al., [2015](#bib.bib11)) using
    a Tensorflow (Abadi et al., [2015](#bib.bib1)) backend, and was run on a 32 GB
    Tesla V1000-PCIE GPU. Training over the course of 100 epochs (a value chosen via
    early stopping) took ${\sim}\,30$ minutes. Details about our architecture are
    shown in Table [1](#S3.T1 "Table 1 ‣ 3.1 Convolutional Autoencoders (CAE) ‣ 3
    Methodology ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"). We intentionally did not substantially decrease the dimensionality
    of the latent space of each layer because of the complexity of the images we aimed
    to reproduce. The SDSS images were generally less bright and noisier, and objects
    from the DES dataset often had a greater number of pixels distinguishable from
    the background noise (i.e. the signal in DES images had a larger spatial extent)
    than the SDSS images, so it is unlikely that a low-dimensionality latent space
    would be capable of producing sufficiently detailed false images.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的CAE使用Keras（Chollet等，[2015](#bib.bib11)）和Tensorflow（Abadi等，[2015](#bib.bib1)）后端实现，并在32
    GB Tesla V1000-PCIE GPU上运行。经过100个epoch的训练（该值通过提前停止选择）花费了约30分钟。关于我们架构的详细信息见表[1](#S3.T1
    "Table 1 ‣ 3.1 Convolutional Autoencoders (CAE) ‣ 3 Methodology ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping")。由于我们旨在重现的图像复杂性，我们故意没有大幅减少每一层的潜在空间维度。SDSS图像通常较暗且噪声较大，而来自DES数据集的对象通常有更多的像素与背景噪声区分开（即DES图像中的信号具有更大的空间范围），因此低维度的潜在空间不太可能生成足够详细的伪图像。'
- en: '| Stage | Output Shape | Activation | $\bm{N_{\textnormal{{P}}}}$ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 阶段 | 输出形状 | 激活函数 | $\bm{N_{\textnormal{{P}}}}$ |'
- en: '| Input | 150 $\times$ 150 $\times$ 3 | N/A |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 150 $\times$ 150 $\times$ 3 | N/A |'
- en: '| Encoder | 150 $\times$ 150 $\times$ 128 | ReLU | 3584 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 编码器 | 150 $\times$ 150 $\times$ 128 | ReLU | 3584 |'
- en: '| 150 $\times$ 150 $\times$ 64 | ReLU | 73792 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 150 $\times$ 150 $\times$ 64 | ReLU | 73792 |'
- en: '| 150 $\times$ 150 $\times$ 32 | ReLU | 18464 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 150 $\times$ 150 $\times$ 32 | ReLU | 18464 |'
- en: '| Decoder | 150 $\times$ 150 $\times$ 32 | ReLU | 9248 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 解码器 | 150 $\times$ 150 $\times$ 32 | ReLU | 9248 |'
- en: '| 150 $\times$ 150 $\times$ 64 | ReLU | 18496 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 150 $\times$ 150 $\times$ 64 | ReLU | 18496 |'
- en: '| 150 $\times$ 150 $\times$ 128 | ReLU | 73856 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 150 $\times$ 150 $\times$ 128 | ReLU | 73856 |'
- en: '| 150 $\times$ 150 $\times$ 3 | Sigmoid | 3459 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 150 $\times$ 150 $\times$ 3 | Sigmoid | 3459 |'
- en: 'Table 1: CAE architecture used for image reconstruction. The initial input
    and final output images were 150 $\times$ 150 $\mathrm{\,pix}$ with 3 color channels;
    the output shape of each image in the encoder and decoder stages is length $\times$ width
    $\times$ no. filters. Each row in the encoder and decoder stages represents a
    single convolution layer with the specified activation function; convolution was
    performed using 3 $\times$ 3 kernel with a stride of 1 and zero padding. The image
    passed to the subsequent convolution layer had dimensions corresponding to that
    row’s output shape. $N_{\textnormal{P}}$ is the number of training features in
    that layer. The number of filters and activation functions used were chosen through
    manual tuning.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 用于图像重建的CAE架构。初始输入和最终输出图像的尺寸为150 $\times$ 150 $\mathrm{\,pix}$，具有3个颜色通道；编码器和解码器阶段中每个图像的输出形状为长度
    $\times$ 宽度 $\times$ 滤波器数量。编码器和解码器阶段中的每一行代表一个具有指定激活函数的卷积层；卷积使用3 $\times$ 3的核，步幅为1，填充为零。传递到后续卷积层的图像尺寸与该行的输出形状相对应。$N_{\textnormal{P}}$是该层中训练特征的数量。使用的滤波器数量和激活函数通过手动调节确定。'
- en: The RGB data from each image was separated into three layers, each of which
    were used to generate a unique set of filters. The encoder and decoder both consisted
    of three hidden layers, each of which filtered the image data from the previous
    layer using 150 3 $\times$ 3 $\mathrm{\,pix}$ convolution filters. These filters
    were initialized using randomly generated weights. Rectified Linear Unit (ReLU)
    activation functions were used for each layer of the encoder and decoder, and
    a sigmoid activation function was used during the final reconstruction phase.
    For each epoch, the input image $\textbf{{x}}_{0}$ was an image from the SDSS
    catalog, while the target image $\textbf{{x}}_{T}$ was the same object taken from
    the DES catalog. The difference between the reconstructed image $\textbf{{x}}^{\prime}_{0}$
    and the target image was calculated using the mean squared error loss function
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图像的RGB数据被分成三个层，每一层用于生成一组独特的滤波器。编码器和解码器都由三个隐藏层组成，每一层使用150个3 $\times$ 3 $\mathrm{\,pix}$的卷积滤波器对来自前一层的图像数据进行过滤。这些滤波器使用随机生成的权重进行初始化。每层的编码器和解码器使用了修正线性单元（ReLU）激活函数，而在最终重建阶段使用了sigmoid激活函数。每个周期中，输入图像$\textbf{{x}}_{0}$来自SDSS目录，而目标图像$\textbf{{x}}_{T}$则来自DES目录。使用均方误差损失函数计算重建图像$\textbf{{x}}^{\prime}_{0}$与目标图像之间的差异。
- en: '|  | $\displaystyle\mathcal{L}\left(\textbf{{x}}^{\prime}_{0},\textbf{{x}}_{T}\right)$
    | $\displaystyle=\left\lVert\textbf{{x}}_{T}-\textbf{{x}}^{\prime}_{0}\right\rVert$
    |  | (1) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}\left(\textbf{{x}}^{\prime}_{0},\textbf{{x}}_{T}\right)$
    | $\displaystyle=\left\lVert\textbf{{x}}_{T}-\textbf{{x}}^{\prime}_{0}\right\rVert$
    |  | (1) |'
- en: The Adadelta (Zeiler, [2012](#bib.bib60)) optimizer was used to determine filter
    weights. At the conclusion of 100 training epochs, the trained algorithm was used
    to reconstruct the DES validation images from their corresponding SDSS image.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Adadelta（Zeiler，[2012](#bib.bib60)）优化器被用于确定滤波器权重。在100个训练周期结束时，训练后的算法被用来从对应的SDSS图像中重建DES验证图像。
- en: 3.2 Cycle-Consistent Generative Adversarial Networks (CycleGAN)
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 循环一致性生成对抗网络（CycleGAN）
- en: '![Refer to caption](img/4afbfe47a6065e497c4acad885773cc0.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4afbfe47a6065e497c4acad885773cc0.png)'
- en: 'Figure 2: A representation of the architecture of a CycleGAN. 1. A false image
    representation $\hat{x}$ is generated from $y$, a member of the target dataset,
    via the mapping function $F$. 2. $\hat{x}$ is mapped to a false image $\hat{y}$
    via the mapping function $G$. 3. The GAN loss function $\mathcal{L}_{\textnormal{GAN}}^{X}$
    for discriminator $D_{\mathcal{X}}$ is calculated by comparing $x$ and $\hat{x}$.
    4. The backward cycle-consistency loss function $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{b}}$
    is calculated by comparing $\hat{y}$ to the true target image $y$ (the forward
    cycle-consistency loss function $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    is calculated similarly using $\hat{x}$ and $x$). In our case, to calculate $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$,
    we generate a DES representation of an SDSS image $x$, then use $F$ to generate
    a false SDSS representation of that image. $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    quantifies the differences between the source SDSS image and the false SDSS image;
    its combination with $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{b}}$ quantifies
    the error accumulated when the SDSS image completes a full “cycle” between the
    SDSS and DES image spaces ($\mathbb{X}\to\mathbb{Y}\to\mathbb{X}$). 5. These loss
    functions are combined with $\mathcal{L}_{\textnormal{GAN}}^{Y}$ and $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    to calculate the total loss function $\mathcal{L}$. $F$ and $G$ are then updated
    to minimize $\mathcal{L}$. This process is repeated to optimize the neural network.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：CycleGAN 架构的示意图。1. 通过映射函数 $F$ 从目标数据集中的成员 $y$ 生成一个虚假图像表示 $\hat{x}$。2. 通过映射函数
    $G$ 将 $\hat{x}$ 映射到虚假图像 $\hat{y}$。3. 判别器 $D_{\mathcal{X}}$ 的 GAN 损失函数 $\mathcal{L}_{\textnormal{GAN}}^{X}$
    通过比较 $x$ 和 $\hat{x}$ 计算得出。4. 反向循环一致性损失函数 $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{b}}$
    通过比较 $\hat{y}$ 和真实目标图像 $y$ 计算得出（前向循环一致性损失函数 $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    通过使用 $\hat{x}$ 和 $x$ 计算得出）。在我们的案例中，为了计算 $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$，我们生成一个
    SDSS 图像 $x$ 的 DES 表示，然后使用 $F$ 生成该图像的虚假 SDSS 表示。$\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    量化源 SDSS 图像和虚假 SDSS 图像之间的差异；它与 $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{b}}$
    的结合量化了当 SDSS 图像在 SDSS 和 DES 图像空间之间完成一次完整的“循环”时积累的误差（$\mathbb{X}\to\mathbb{Y}\to\mathbb{X}$）。5.
    这些损失函数与 $\mathcal{L}_{\textnormal{GAN}}^{Y}$ 和 $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    结合计算总损失函数 $\mathcal{L}$。然后更新 $F$ 和 $G$ 以最小化 $\mathcal{L}$。这一过程重复进行，以优化神经网络。
- en: A Generative Adversarial Network (GAN) (Durugkar et al., [2016](#bib.bib13))
    is an unsupervised or semi-supervised generative model consisting of a generator
    $G$ and discriminator $D$. $D$ is trained to distinguish between images from a
    training dataset of “true” images ($\mathcal{Y}$) and those generated by sampling
    from the latent space of $G$ ($\mathcal{X}$). Backpropagation of error from $D$
    is used to generate a map $g:\mathcal{X}\to\mathcal{Y}$ from the latent space
    of $G$ to the “true” image dataset by minimizing a loss function $\mathcal{L}(G,D,\mathcal{X},\mathcal{Y})$.
    After training, the GAN may be used to generate false images that replicate the
    features of $\mathcal{Y}$.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）（Durugkar 等人，[2016](#bib.bib13)）是一种无监督或半监督的生成模型，由生成器 $G$ 和判别器 $D$
    组成。$D$ 被训练用来区分来自“真实”图像训练数据集 ($\mathcal{Y}$) 的图像和通过从 $G$ 的潜在空间采样生成的图像 ($\mathcal{X}$)。通过最小化损失函数
    $\mathcal{L}(G,D,\mathcal{X},\mathcal{Y})$，使用 $D$ 的误差反向传播来生成从 $G$ 的潜在空间到“真实”图像数据集的映射
    $g:\mathcal{X}\to\mathcal{Y}$。训练后，GAN 可以用来生成复制 $\mathcal{Y}$ 特征的虚假图像。
- en: 'A CycleGAN (Zhu et al., [2017](#bib.bib62); Isola et al., [2016](#bib.bib20))
    is a variation of a traditional GAN that minimizes cycle-consistency loss through
    the additional of a second generator/discriminator pair; a diagram of this architecture
    is shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Cycle-Consistent Generative Adversarial
    Networks (CycleGAN) ‣ 3 Methodology ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). Images from $\mathcal{X}$ ($\mathcal{Y}$)
    are used to train discriminators $D_{\mathcal{X}}$ ($D_{\mathcal{Y}}$). The generators
    $F:\mathcal{X}\to\mathcal{Y}$ and $G:\mathcal{Y}\to\mathcal{X}$ are trained to
    extremize the adversarial loss function $\mathcal{L}(H,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})$
    for generator $G$, discriminator $D_{\mathcal{Y}}$, and datasets $\mathcal{X}$
    and $\mathcal{Y}$. For the purposes of this project, we chose to use the loss
    function used by Zhu et al. ([2017](#bib.bib62)):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'CycleGAN（Zhu 等，[2017](#bib.bib62)；Isola 等，[2016](#bib.bib20)）是传统 GAN 的一种变体，通过增加第二对生成器/判别器来最小化循环一致性损失；该架构的图示见图
    [2](#S3.F2 "Figure 2 ‣ 3.2 Cycle-Consistent Generative Adversarial Networks (CycleGAN)
    ‣ 3 Methodology ‣ Survey2Survey: A deep learning generative model approach for
    cross-survey image mapping")。来自 $\mathcal{X}$（$\mathcal{Y}$）的图像用于训练判别器 $D_{\mathcal{X}}$（$D_{\mathcal{Y}}$）。生成器
    $F:\mathcal{X}\to\mathcal{Y}$ 和 $G:\mathcal{Y}\to\mathcal{X}$ 被训练以极端化对抗损失函数 $\mathcal{L}(H,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})$
    对于生成器 $G$、判别器 $D_{\mathcal{Y}}$ 和数据集 $\mathcal{X}$ 和 $\mathcal{Y}$。为了本项目的目的，我们选择使用
    Zhu 等（[2017](#bib.bib62)）使用的损失函数：'
- en: '|  | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})\kern
    2.0pt=\kern 2.0pt$ | $\displaystyle\mathbb{E}_{\kern 0.5pty\sim p_{\textnormal{data}}(y)}\left[\kern
    0.5pt\log D_{\mathcal{Y}}(y)\kern 0.5pt\right]+$ |  | (2) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})\kern
    2.0pt=\kern 2.0pt$ | $\displaystyle\mathbb{E}_{\kern 0.5pty\sim p_{\textnormal{data}}(y)}\left[\kern
    0.5pt\log D_{\mathcal{Y}}(y)\kern 0.5pt\right]+$ |  | (2) |'
- en: '|  |  | $\displaystyle\mathbb{E}_{\kern 0.5ptx\sim p_{\textnormal{data}}(X)}\left[\kern
    0.5pt\log(1-D_{\mathcal{Y}}(G(x))\kern 0.5pt\right]$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}_{\kern 0.5ptx\sim p_{\textnormal{data}}(X)}\left[\kern
    0.5pt\log(1-D_{\mathcal{Y}}(G(x))\kern 0.5pt\right]$ |  |'
- en: for images $x\in\mathcal{X}$ and $y\in\mathcal{Y}$, where $p_{\textnormal{data}}$
    is the true data distribution. $G$ was trained to maximize $\mathcal{L}_{\textnormal{GAN}}$
    ($\max_{G}\max_{D_{\mathcal{Y}}}\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})$),
    while $F$ was trained to minimize it ($\min_{F}\max_{D_{\mathcal{X}}}\mathcal{L}_{\textnormal{GAN}}(F,D_{\mathcal{X}},\mathcal{Y},\mathcal{X})$).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像 $x\in\mathcal{X}$ 和 $y\in\mathcal{Y}$，其中 $p_{\textnormal{data}}$ 是真实数据分布。$G$
    被训练以最大化 $\mathcal{L}_{\textnormal{GAN}}$ ($\max_{G}\max_{D_{\mathcal{Y}}}\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})$)，而
    $F$ 被训练以最小化它 ($\min_{F}\max_{D_{\mathcal{X}}}\mathcal{L}_{\textnormal{GAN}}(F,D_{\mathcal{X}},\mathcal{Y},\mathcal{X})$)。
- en: To constrain the space of possible mapping functions, a CycleGAN optimizes $F$
    and $G$ by minimizing the forward and backward cycle consistency error. For images
    $x\in\mathcal{X}$ and $y\in\mathcal{Y}$, let $x^{\prime}=F(G(x))$ and $y^{\prime}=G(F(y))$.
    Forward cycle consistency is achieved when the difference between $x^{\prime}$
    and $x$ is minimized (i.e. $F=G^{-1}+\varepsilon_{x}$ for some small error $\varepsilon_{x}$),
    indicating that the full translation cycle beginning in $\mathcal{X}$ reproduces
    a close approximation of $x$; backward cycle consistency is defined identically
    for images $y\in\mathcal{Y}$ and $G=F^{-1}+\varepsilon_{y}$ for some small error
    $\varepsilon_{y}$. An optimized CycleGAN will simultaneously minimize the forward
    and backward cycle-consistency error; this is equivalent to ensuring that $F$
    and $G$ are bijective inverses of one another, limiting the size of the set of
    possible mapping functions. This improves the robustness of the neural network
    and decreases the amount of training required relative to many other GAN variations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了约束可能映射函数的空间，CycleGAN 通过最小化前向和反向循环一致性误差来优化 $F$ 和 $G$。对于图像 $x\in\mathcal{X}$
    和 $y\in\mathcal{Y}$，令 $x^{\prime}=F(G(x))$ 和 $y^{\prime}=G(F(y))$。当 $x^{\prime}$
    和 $x$ 之间的差异被最小化时（即 $F=G^{-1}+\varepsilon_{x}$，其中 $\varepsilon_{x}$ 是某个小误差），前向循环一致性得以实现，这表明以
    $\mathcal{X}$ 开始的完整翻译循环重现了 $x$ 的接近近似；反向循环一致性在图像 $y\in\mathcal{Y}$ 上定义相同，且 $G=F^{-1}+\varepsilon_{y}$，其中
    $\varepsilon_{y}$ 是某个小误差。优化后的 CycleGAN 将同时最小化前向和反向循环一致性误差；这等同于确保 $F$ 和 $G$ 是彼此的双射逆，限制了可能映射函数集合的大小。这提高了神经网络的鲁棒性，并减少了相对于许多其他
    GAN 变体所需的训练量。
- en: We note that for our particular case we used the architecture described in Isola
    et al. ([2016](#bib.bib20)) which is adapted from the unsupervised representation
    learning GAN architecture introduced in Radford et al. ([2016](#bib.bib40)). In
    particular, we highlight the use of a generator with skips and a Markovian discriminator.
    These additions helped with the translation process and limited the GAN discriminator
    to high-frequency structures, reducing the potential for artifacts.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，在我们的特定案例中，我们使用了Isola等人描述的架构（[2016](#bib.bib20)），该架构改编自Radford等人提出的无监督表示学习GAN架构（[2016](#bib.bib40)）。特别是，我们强调使用了带有跳跃连接的生成器和马尔可夫判别器。这些附加功能有助于翻译过程，并将GAN判别器限制在高频结构上，减少了伪影的潜在风险。
- en: The cycle-consistency loss function $\mathcal{L}_{\textnormal{cyc}}(G,F)$ we
    used is defined as
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的循环一致性损失函数$\mathcal{L}_{\textnormal{cyc}}(G,F)$定义为
- en: '|  | $\displaystyle\mathcal{L}_{\textnormal{cyc}}(G,F)=\kern 3.0pt$ | $\displaystyle\mathbb{E}_{x\sim
    p_{\textnormal{data}}(x)}\left[\kern 0.5pt\left\lvert F(G(x))-x\right\rvert_{1}\right]+$
    |  | (3) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\textnormal{cyc}}(G,F)=\kern 3.0pt$ | $\displaystyle\mathbb{E}_{x\sim
    p_{\textnormal{data}}(x)}\left[\kern 0.5pt\left\lvert F(G(x))-x\right\rvert_{1}\right]+$
    |  | (3) |'
- en: '|  |  | $\displaystyle\mathbb{E}_{y\sim p_{\textnormal{data}}(y)}\left[\kern
    0.5pt\left\lvert G(F(y))-y\right\rvert_{1}\right],$ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}_{y\sim p_{\textnormal{data}}(y)}\left[\kern
    0.5pt\left\lvert G(F(y))-y\right\rvert_{1}\right],$ |  |'
- en: where $\left\lvert A-B\right\rvert_{1}=\sum\limits_{i}\hskip 2.0pt\left\lvert
    A_{i}-B_{i}\right\rvert$ is the pixel-to-pixel $L^{1}$-norm between images $A$
    (SDSS) and $B$ (DES).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\left\lvert A-B\right\rvert_{1}=\sum\limits_{i}\hskip 2.0pt\left\lvert A_{i}-B_{i}\right\rvert$是图像$A$（SDSS）和$B$（DES）之间的像素到像素的$L^{1}$-范数。
- en: The full loss function used for training $F$ and $G$ was
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练$F$和$G$的完整损失函数是
- en: '|  | $\displaystyle\mathcal{L}(G,F,D_{\mathcal{X}},D_{\mathcal{Y}})\kern 2.0pt=\kern
    2.0pt$ | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})\kern
    2.0pt+$ |  | (4) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}(G,F,D_{\mathcal{X}},D_{\mathcal{Y}})\kern 2.0pt=\kern
    2.0pt$ | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})\kern
    2.0pt+$ |  | (4) |'
- en: '|  |  | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(F,D_{\mathcal{X}},\mathcal{Y},\mathcal{X})\kern
    2.0pt+$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(F,D_{\mathcal{X}},\mathcal{Y},\mathcal{X})\kern
    2.0pt+$ |  |'
- en: '|  |  | $\displaystyle\lambda\kern 0.5pt\mathcal{L}_{\textnormal{cyc}}(G,F)$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\lambda\kern 0.5pt\mathcal{L}_{\textnormal{cyc}}(G,F)$
    |  |'
- en: for some parameter $\lambda$, which describes the relative importance of the
    optimization of the adversarial and cycle consistency errors. For this work, we
    set $\lambda=0.2$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些参数$\lambda$，它描述了对抗性和循环一致性误差优化的相对重要性。在这项工作中，我们将$\lambda$设置为$0.2$。
- en: Image translation using a CycleGAN architecture provides benefit over a traditional
    GAN by constraining the allowed mapping functions by ensuring that the discriminator
    pair $F$ and $G$ are inverses. This benefits the translation between noisy images
    by making sure that the differences in noise patterns in $\mathcal{X}$ and $\mathcal{Y}$
    is taken into account, helping distinguish between the signal and noise more easily
    after training on many images.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CycleGAN架构进行图像翻译相比传统GAN具有优势，通过确保判别对$F$和$G$是互为逆函数来限制允许的映射函数。这有利于噪声图像之间的翻译，因为它确保了$\mathcal{X}$和$\mathcal{Y}$中的噪声模式差异被考虑到，从而在训练多张图像后更容易区分信号和噪声。
- en: 4 Results and Analysis
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与分析
- en: Here, we demonstrate that we can transfer information from DES images to their
    SDSS counterparts, generating synthetic images that are brighter, of higher quality,
    and have less noise, yet retain the morphological information contained within
    the source image. We begin with a qualitative analysis to understand properties
    of the reconstructed images, then quantify the brightness and noise level of the
    image datasets. We then use correlations between the light profiles of the source
    and reconstructed objects to establish the small-scale differences between the
    datasets. Finally, we combine this information with comparative quality assessments
    to establish that the image reconstruction process improves the image quality,
    brightens objects, and reduces background noise. We provide evidence for the robustness
    of the reconstruction process by comparing the statistics of the validation and
    external datasets.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了我们可以将信息从DES图像转移到其SDSS对应图像，生成更亮、更高质量且噪声更少的合成图像，同时保留源图像中的形态信息。我们首先进行定性分析，以理解重建图像的属性，然后量化图像数据集的亮度和噪声水平。接着，我们利用源图像和重建图像的光谱剖面之间的相关性，确定数据集之间的小规模差异。最后，我们结合比较质量评估的信息，确认图像重建过程提高了图像质量，使物体变亮，并减少了背景噪声。我们通过比较验证数据集和外部数据集的统计数据，提供了重建过程鲁棒性的证据。
- en: 4.1 Qualitative Analysis
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 定性分析
- en: '![Refer to caption](img/6050b54db1b0d18498f75da7caaccaca.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6050b54db1b0d18498f75da7caaccaca.png)'
- en: 'Figure 3: Examples of galaxy images from the validation dataset (from the Stripe82
    region). Each column shows an SDSS galaxy (row A), its DES counterpart (row B),
    and the DES image reconstruction by the CAE (row C) and CycleGAN (row D) methods.
    CAE and CycleGAN residuals (reconstruction - DES) are shown in rows E and F respectively,
    wile the CAE and CycleGAN pixel-to-pixel brightness increases (reconstruction
    - SDSS) are shown in rows G and H, respectively. Note that to increase visibility,
    images in rows E, F, G and H were artificially enhanced with a power law transform
    ($P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$ for each pixel $P_{i}$). In rows
    E and F, $\gamma=0.3$, while in rows G and H, $\gamma=0.5$. Additional galaxy
    samples can be found in Appendix [A](#A1 "Appendix A Additional Image Samples
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping").'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：来自验证数据集的银河图像示例（来自Stripe82区域）。每列展示一个SDSS银河（A行）、其对应的DES图像（B行），以及由CAE（C行）和CycleGAN（D行）方法重建的DES图像。CAE和CycleGAN的残差（重建
    - DES）分别显示在E行和F行，而CAE和CycleGAN的逐像素亮度增加（重建 - SDSS）分别显示在G行和H行。注意，为了提高可视性，E、F、G和H行的图像经过了功率法则变换（$P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$，对于每个像素$P_{i}$）。在E和F行中，$\gamma=0.3$，而在G和H行中，$\gamma=0.5$。额外的银河样本可以在附录[A](#A1
    "附录A 额外图像样本 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")中找到。
- en: '![Refer to caption](img/e88870814ca1b03dbd0887e0dc20ea83.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e88870814ca1b03dbd0887e0dc20ea83.png)'
- en: 'Figure 4: Examples of galaxy images from the external dataset (from outside
    of the Stripe82 overlap region). Each column shows an SDSS galaxy (row A) and
    its DES image reconstruction by the CAE (row B) and CycleGAN (row C) methods.
    The CAE and CycleGAN pixel-to-pixel brightness increases (reconstruction - SDSS)
    are shown in rows D and E, respectively. Note that to increase visibility, images
    in rows D and E were artificially enhanced with a power law transform ($P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$
    for each pixel $P_{i}$, where $\gamma=0.5$). Additional galaxy samples can be
    found in Appendix [A](#A1 "Appendix A Additional Image Samples ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：来自外部数据集的银河图像示例（来自Stripe82重叠区域外）。每列展示一个SDSS银河（A行）及其由CAE（B行）和CycleGAN（C行）方法重建的DES图像。CAE和CycleGAN的逐像素亮度增加（重建
    - SDSS）分别显示在D行和E行。注意，为了提高可视性，D和E行的图像经过了功率法则变换（$P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$，对于每个像素$P_{i}$，其中$\gamma=0.5$）。额外的银河样本可以在附录[A](#A1
    "附录A 额外图像样本 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")中找到。
- en: 'Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") shows several examples of false images generated by the neural networks
    paired with their corresponding SDSS and DES images from the overlap region. These
    images were selected to demonstrate the wide variety of galaxy types and structures
    included in the validation sample which were not including during the training.
    Row A contains images from the SDSS catalog; the corresponding DES images are
    located in row B. Rows C and D contain the reconstructed CAE and CycleGAN images,
    respectively. We can observe that the DES images and the synthetic images in rows
    C and D are remarkably similar, where the small differences come from the the
    lack of structure resolution of the reconstructed objects.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")展示了神经网络生成的一些虚假图像的示例，并与它们相应的SDSS和DES图像进行了配对。这些图像被选中以展示验证样本中包含的各种类型和结构的银河，这些银河在训练期间未被包含。A行包含SDSS目录中的图像；相应的DES图像位于B行。C行和D行分别包含了重建的CAE和CycleGAN图像。我们可以观察到，DES图像和C行及D行中的合成图像非常相似，差异主要来自重建对象的结构分辨率不足。'
- en: Qualitatively, the reconstructed images are generally blurrier than the corresponding
    DES images. However, images reconstructed by both the models are generally brighter
    than their SDSS counterparts. In addition, the false images, particularly the
    those generated by the CAE, are often less noisy than their SDSS and/or DES counterparts.
    Image residuals for the CAE and CycleGAN reconstructions are shown in rows E and
    F, respectively. These show the pixel-to-pixel brightness differences between
    the reconstructed and DES images; note that these images were artificially enhanced
    using a power law transform ($P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$ for
    each pixel $P_{i}$; in rows E and F, $\gamma=0.3$, while in rows G and H, $\gamma=0.5$).
    This was done so that the residual structure was visible. It appears that both
    neural networks isolated and enhanced the galaxy signal while affecting the background
    minimally or try to reduce the noise. Both networks were also able to distinguish
    between separate structures in each image; this is particularly evident in the
    second column.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度看，重建的图像通常比相应的DES图像更模糊。然而，两种模型重建的图像通常比它们的SDSS图像更明亮。此外，虚假图像，特别是由CAE生成的图像，通常比它们的SDSS和/或DES图像噪声更少。CAE和CycleGAN重建的图像残差分别显示在E行和F行。这些显示了重建图像与DES图像之间的逐像素亮度差异；请注意，这些图像经过了功率律变换（$P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$
    对于每个像素 $P_{i}$；在E行和F行中，$\gamma=0.3$，而在G行和H行中，$\gamma=0.5$）。这样做是为了使残差结构可见。看来两种神经网络都隔离并增强了银河信号，同时对背景的影响最小化或试图减少噪声。两个网络还能够区分每张图像中的独立结构；这一点在第二列中尤为明显。
- en: 'Rows G and H show the pixel-to-pixel brightness increase provided by the CAE
    and CycleGAN reconstructions relative to the corresponding SDSS galaxies, respectively.
    Qualitatively, the CAE reconstructions are brighter than the CycleGAN images,
    and provided greater amplification to the internal structure of each galaxy. Interestingly,
    both networks consistently amplified the galaxy center more than other regions.
    This amplification was not exclusive to the central galaxy; rather, it was present
    in most regions the network identified as a signal region. Other example galaxies
    are included in Appendix [A](#A1 "Appendix A Additional Image Samples ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'G行和H行显示了相对于对应的SDSS银河，CAE和CycleGAN重建提供的逐像素亮度增益。从定性角度看，CAE重建的图像比CycleGAN图像更亮，并且对每个银河的内部结构提供了更大的放大。值得注意的是，两种网络都一致地对银河中心的放大效果超过了其他区域。这种放大并不仅限于银河中心，而是在网络识别为信号区域的大多数区域都存在。其他示例银河可在附录[A](#A1
    "Appendix A Additional Image Samples ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")中查看。'
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") shows examples of images from the external dataset (from outside of
    the Stripe82 region). These images are generally of lower quality; however, both
    reconstruction models succeeded in selecting and amplifying the objects of interest
    with little effect on the background, even maintaining much of the small-scale
    detail of the images (particularly in the fourth and seventh columns). As in Fig.
    [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"), the
    reconstructions generally increased the spatial extent of objects in the image.
    Notably, the CAE reconstruction appears to have removed an artifact from the SDSS
    image in the final column; this phenomenon is discussed in greater detail in Section
    [4.4](#S4.SS4 "4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S4.F4 "图 4 ‣ 4.1 质量分析 ‣ 4 结果与分析 ‣ Survey2Survey: 用于跨调查图像映射的深度学习生成模型方法")展示了来自外部数据集（Stripe82区域之外）的图像示例。这些图像通常质量较低；然而，两个重建模型都成功选择并放大了感兴趣的物体，对背景的影响较小，甚至保持了图像的许多小尺度细节（特别是在第四列和第七列）。如图[3](#S4.F3
    "图 3 ‣ 4.1 质量分析 ‣ 4 结果与分析 ‣ Survey2Survey: 用于跨调查图像映射的深度学习生成模型方法")所示，重建一般增加了图像中物体的空间范围。值得注意的是，CAE重建在最后一列中似乎去除了SDSS图像中的一个伪影；这一现象在第[4.4节](#S4.SS4
    "4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey: 用于跨调查图像映射的深度学习生成模型方法")中进行了更详细的讨论。'
- en: 4.2 Dataset Properties
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据集属性
- en: Here, we quantify the brightness and quality of images from each dataset to
    use as baseline comparison metrics between the original input SDSS images, the
    DES target images, and the reconstructions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们量化了来自每个数据集的图像亮度和质量，以用作原始输入SDSS图像、DES目标图像和重建图像之间的基准比较指标。
- en: Pseudo-Flux Magnitude
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 伪通量大小
- en: In this work we have used the RGB images from SDSS and DES to test our architectures.
    Image brightness was quantified using the average pseudo-flux magnitude $F$ of
    each image. We refer to $F$ as the “pseudo-flux magnitude” because, while $F$
    does not represent the physical flux magnitude (our images consisted solely of
    (r, g, b) channel pixel values), it acts as a proxy for this quantity due to the
    similarities between the two measurements. The pseudo-flux magnitude $F$ was defined
    by
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们使用了来自SDSS和DES的RGB图像来测试我们的架构。图像亮度通过每幅图像的平均伪通量大小$F$来量化。我们称$F$为“伪通量大小”，因为虽然$F$不代表物理通量大小（我们的图像仅由(r,
    g, b)通道像素值组成），但由于这两种测量之间的相似性，它充当了该量的代理。伪通量大小$F$的定义如下：
- en: '|  | $\displaystyle F$ | $\displaystyle=30-2.5\log\left(\sum_{{\hskip 6.0ptr_{i}\,<\,r_{\textnormal{max}}}}\beta_{i}\hskip
    2.0pt\right)$ |  | (5) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F$ | $\displaystyle=30-2.5\log\left(\sum_{{\hskip 6.0ptr_{i}\,<\,r_{\textnormal{max}}}}\beta_{i}\hskip
    2.0pt\right)$ |  | (5) |'
- en: '|  |  | $\displaystyle=30-2.5\log\,\beta^{\circ}.$ |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=30-2.5\log\,\beta^{\circ}.$ |  |'
- en: Here, the pixel brightness $\beta_{i}$ describes the average of the red, green,
    and blue channel values in $P_{i}$ and $\beta^{\circ}$ is the total pixel brightness
    contained within an aperture of radius $r_{\textnormal{max}}=75\mathrm{\,pix}$.
    A constant factor (zero point) of 30 was added to approximate the appearance of
    a physical magnitude distribution.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，像素亮度$\beta_{i}$描述了$P_{i}$中红色、绿色和蓝色通道值的平均值，而$\beta^{\circ}$是半径$r_{\textnormal{max}}=75\mathrm{\,pix}$的孔径内包含的总像素亮度。为了逼近物理量分布的外观，添加了一个常数因子（零点）30。
- en: 'Gaussian kernel density estimates (KDEs) for histograms of the pseudo-flux
    magnitudes are shown in Fig. [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2
    Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). The first, second, and third
    quartiles were used as a conservative estimate of the spread of the distribution
    data; this was chosen due to the heavy skew of the distributions of the external
    data. However, they cannot be used to determine whether there was a significant
    difference between the S/N of different datasets. This is because the differences
    in pseudo-flux magnitude must be evaluated on an image-to-image basis, not by
    the relative frequency of each S/N value. The pseudo-flux magnitude values for
    the reconstructions in the validation datasets were comparable to those of the
    DES dataset, showing an improvement in the brightness relative to the SDSS dataset.
    In the external dataset, the pseudo-flux magnitude distributions for both reconstructions
    were shifted to the left of the SDSS distribution, indicating that the reconstruction
    process successfully increased the brightness of images from the external dataset.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '伪通量幅度直方图的高斯核密度估计（KDEs）如图 [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2
    Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") 所示。使用了第一、第二和第三四分位数作为数据分布的保守估计；由于外部数据分布的严重偏斜，因此选择了这些值。然而，它们不能用来确定不同数据集之间的信噪比是否存在显著差异。这是因为伪通量幅度的差异必须在图像到图像的基础上评估，而不是通过每个信噪比值的相对频率来评估。验证数据集中的重建伪通量幅度值与DES数据集相当，显示出相对于SDSS数据集的亮度有所改善。在外部数据集中，两个重建的伪通量幅度分布都向左偏离了SDSS分布，这表明重建过程成功提高了来自外部数据集的图像亮度。'
- en: To quantify the image-to-image brightnesses and provide an error estimate, define
    the mean relative flux difference $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$ between datasets $i$ and $j$ as
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化图像之间的亮度并提供误差估计，定义数据集 $i$ 和 $j$ 之间的均值相对通量差 $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$ 为
- en: '|  | $\displaystyle\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}=\frac{1}{N_{\textnormal{img}}}\sum_{m}^{N_{\textnormal{img}}}\frac{F_{m}^{\kern
    1.0ptj}-F_{m}^{\kern 1.0pti}}{F_{m}^{\kern 1.0pti}},$ |  | (6) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}=\frac{1}{N_{\textnormal{img}}}\sum_{m}^{N_{\textnormal{img}}}\frac{F_{m}^{\kern
    1.0ptj}-F_{m}^{\kern 1.0pti}}{F_{m}^{\kern 1.0pti}},$ |  | (6) |'
- en: where $F_{m}^{\kern 1.0pti}$ and $F_{m}^{\kern 1.0ptj}$ are the pseudo-flux
    magnitudes of corresponding images from the $N_{\textnormal{img}}$ image in datasets
    $\mathbb{X}_{i}$ and $\mathbb{X}_{j}$, respectively.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $F_{m}^{\kern 1.0pti}$ 和 $F_{m}^{\kern 1.0ptj}$ 是数据集 $\mathbb{X}_{i}$ 和 $\mathbb{X}_{j}$
    中 $N_{\textnormal{img}}$ 张图像的对应伪通量幅度。
- en: 'The values of $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$for the external and validation datasets are shown in Table [2](#S4.T2
    "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") (scaled by a factor of $10^{3}$ to enhance readability); the error was
    estimated using the standard deviation of $\frac{F_{m}^{\kern 1.0ptj}-F_{m}^{\kern
    1.0pti}}{F_{m}^{\kern 1.0pti}}$. Quartiles were used to estimate the error of
    the pseudo-flux magnitude plot (Figure [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping")) due to the clear skew
    of the data, so the variance would not provide an adequate representation of the
    spread of the data. However, the distribution of $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$was more symmetric than those of $F$, allowing the use of the standard
    deviation as an estimate of error.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '外部和验证数据集的$\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$值见表[2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping")（为了提高可读性，缩放因子为$10^{3}$）；误差是通过$\frac{F_{m}^{\kern
    1.0ptj}-F_{m}^{\kern 1.0pti}}{F_{m}^{\kern 1.0pti}}$的标准差估算的。由于数据的明显偏斜，伪通量幅度图（图[5](#S4.F5
    "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")）的误差是使用四分位数估算的，因此方差不能提供数据分布的充分表示。然而，$\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$的分布比$F$的分布更对称，因此可以使用标准差作为误差的估算。'
- en: 'The only dataset pair in which there was not a significant difference between
    the fluxes was for CycleGAN vs. DES. This implies that the CycleGAN reconstructions
    decreased the flux of the SDSS images to match that of the DES images. It should
    be noted that the results from this table seem to be in conflict with those from
    Figure [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣
    4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"). This is not unexpected: Figure [5](#S4.F5 "Figure
    5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") show the distribution of the relative frequency of individual pseudo-flux
    magnitudes, while the values in Table [2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping") show an image-to-image
    comparison of the pseudo-flux magnitudes. Hence, the values of Table [2](#S4.T2
    "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") provide an appropriate measure of the differences in the pseudo-flux
    magnitudes of the reconstructions relative to their source images.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '唯一一个在通量之间没有显著差异的数据集对是CycleGAN与DES。这意味着CycleGAN的重建将SDSS图像的通量降低，以匹配DES图像的通量。需要注意的是，这张表中的结果似乎与图[5](#S4.F5
    "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")中的结果相冲突。这并不意外：图[5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset
    Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")展示了各个伪通量幅度的相对频率分布，而表[2](#S4.T2
    "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")中的值展示了伪通量幅度的图像对图像比较。因此，表[2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude ‣
    4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping")中的值提供了一个适当的度量，用于比较重建图像与源图像之间伪通量幅度的差异。'
- en: 'Notably, there was not a statistically significant difference between the values
    of $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS, CAE}$ in the validation
    and external datasets; the same is also true for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS,
    CycleGAN}$. Hence, the decrease in flux provided by the reconstructions were similar
    for both the validation and external datasets. This provides evidence for the
    robustness of our method: the increase in the brightnesses of the false images
    was the same regardless of the brightnesses of the input images.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在验证和外部数据集中，$\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$在 $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS, CAE}$ 之间没有统计学上的显著差异；同样，$\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS,
    CycleGAN}$ 也是如此。因此，重建提供的通量减少在验证和外部数据集中是类似的。这提供了我们方法稳健性的证据：伪图像的亮度增加与输入图像的亮度无关。
- en: '![Refer to caption](img/fa3ecd8ea6c17321b76892e62fc07187.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa3ecd8ea6c17321b76892e62fc07187.png)'
- en: 'Figure 5: Top: Pseudo-flux magnitudes (defined in Eqn. ([5](#S4.E5 "In Pseudo-Flux
    Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping"))) for
    the validation and external data. Bottom: The first, second, and third quartiles
    of the corresponding datasets, providing a measure of spread. SDSS images tended
    to be fainter than the DES and reconstructed images. Note that the quartiles cannot
    be used as a measure of error/statistical significance because this plot does
    not provide a representation of the image-to-image differences in $F$; this is
    discussed in greater detail in the text.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5：上图：验证和外部数据的伪通量幅度（定义见公式 ([5](#S4.E5 "在伪通量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey:
    深度学习生成模型方法用于跨调查图像映射")))。下图：相应数据集的第一、第二和第三四分位数，提供了分布范围的度量。SDSS 图像通常比 DES 和重建图像更暗。请注意，四分位数不能用作误差/统计显著性的度量，因为此图并未提供
    $F$ 的图像对图像差异的表示；这在文本中有更详细的讨论。'
- en: '| $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}\times 10^{3}$ | External | Validation |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}\times 10^{3}$ | 外部 | 验证 |'
- en: '| <svg version="1.1" height="25.53" width="150" overflow="visible"><g transform="translate(0,25.53)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,13.52) scale(1,
    -1)"><foreignobject width="24.16" height="13.52" overflow="visible">$\mathbb{X}_{j}$</foreignobject></g></g>
    <g  transform="translate(124.04,13.52)"><g transform="translate(0,12.01) scale(1,
    -1)"><foreignobject width="25.96" height="12.01" overflow="visible">$\mathbb{X}_{i}$</foreignobject></g></g></g></svg>
    | SDSS | SDSS | DES |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="25.53" width="150" overflow="visible"><g transform="translate(0,25.53)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,13.52) scale(1,
    -1)"><foreignobject width="24.16" height="13.52" overflow="visible">$\mathbb{X}_{j}$</foreignobject></g></g>
    <g  transform="translate(124.04,13.52)"><g transform="translate(0,12.01) scale(1,
    -1)"><foreignobject width="25.96" height="12.01" overflow="visible">$\mathbb{X}_{i}$</foreignobject></g></g></g></svg>
    | SDSS | SDSS | DES |'
- en: '| CAE | $-40.65\pm 5.59$ | $-41.90\pm 5.54$ | $-9.36\pm 6.72$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| CAE | $-40.65\pm 5.59$ | $-41.90\pm 5.54$ | $-9.36\pm 6.72$ |'
- en: '| CycleGAN | $-30.44\pm 7.28$ | $-29.33\pm 6.28$ | $\mathit{3.65\pm 8.22}$
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| CycleGAN | $-30.44\pm 7.28$ | $-29.33\pm 6.28$ | $\mathit{3.65\pm 8.22}$
    |'
- en: '| DES | N/A | $-32.80\pm 9.13$ | N/A |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| DES | 不适用 | $-32.80\pm 9.13$ | 不适用 |'
- en: 'Table 2: The mean proportional difference $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$(scaled by a factor of $10^{3}$) in the pseudo-flux magnitudes (defined
    in Eq. ([6](#S4.E6 "In Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"))) between each of the image sets; the standard deviation was used
    to estimate the error. The only dataset pair that does not show a significant
    difference in $F$ is CycleGAN vs. DES.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：伪通量幅度中的平均比例差异 $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$(按$10^{3}$的因子缩放)（定义见公式 ([6](#S4.E6 "在伪通量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey:
    深度学习生成模型方法用于跨调查图像映射"))) 在每组图像之间；标准差用于估计误差。唯一一个没有显示出显著差异的$F$数据集对是 CycleGAN 和 DES。'
- en: '![Refer to caption](img/7db6a19f80ed6e83161246cdf695d17a.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7db6a19f80ed6e83161246cdf695d17a.png)'
- en: 'Figure 6: Top: KDEs of the histograms of the mean S/N (defined in Eqn. ([7](#S4.E7
    "In Signal-to-Noise Ratio ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"))) for the validation and external data. Bottom: The first, second, and
    third quartiles of the distributions. Both reconstruction models were effective
    at increasing the S/N. Note that, as described in Figure [5](#S4.F5 "Figure 5
    ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"), the
    quartile bars cannot be used to determine statistical significance; see the text
    for further explanation.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6：顶部：验证和外部数据中均值 S/N 的直方图的 KDEs（定义见方程 ([7](#S4.E7 "信噪比 ‣ 4.2 数据集属性 ‣ 4 结果与分析
    ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法"))）。底部：分布的第一、第二和第三四分位数。两种重建模型都有效地提高了 S/N。请注意，如图
    [5](#S4.F5 "图 5 ‣ 伪通量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")
    所述，四分位数条不能用于确定统计显著性；有关进一步解释，请参见正文。'
- en: Signal-to-Noise Ratio
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 信噪比
- en: As metric for image quality, we measured the average signal-to-noise ratio (S/N)
    of images in each dataset. We define the mean S/N as
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为图像质量的度量，我们测量了每个数据集中图像的平均信噪比 (S/N)。我们将均值 S/N 定义为
- en: '|  | S/N | $\displaystyle=\frac{\mu^{\circ}_{\beta}}{\sigma^{\circ}_{\beta}},$
    |  | (7) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | S/N | $\displaystyle=\frac{\mu^{\circ}_{\beta}}{\sigma^{\circ}_{\beta}},$
    |  | (7) |'
- en: where $\mu^{\circ}_{\beta}$ $\left(\sigma^{\circ}_{\beta}\right)$ is the mean
    (standard deviation) of the pixel brightness $\beta$ for pixels within a radius
    of $r_{\textnormal{max}}=75\mathrm{\,pix}$.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu^{\circ}_{\beta}$ $\left(\sigma^{\circ}_{\beta}\right)$ 是半径为 $r_{\textnormal{max}}=75\mathrm{\,pix}$
    的像素亮度 $\beta$ 的均值（标准差）。
- en: 'In Fig. [6](#S4.F6 "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), we show KDEs for histograms of the mean S/N,
    along with the first, second, and third quartiles, which are used as a measure
    of spread; however, as discussed in the analysis of Table [2](#S4.T2 "Table 2
    ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"), they
    cannot be used as a measure of error. On average, both reconstruction models were
    effective at boosting the S/N relative to the SDSS images, and the S/N for the
    CycleGAN reconstructions nearly matched that of the DES images. Denoising autoencoders
    have been used to reduce the amount of noise in images (Vincent et al., [2008](#bib.bib55)),
    so it is not surprising that the S/N in CAE images was greater than that of the
    target images.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [6](#S4.F6 "图 6 ‣ 伪通量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")
    中，我们展示了均值 S/N 的直方图的 KDEs，以及第一、第二和第三四分位数，这些都用作扩展度量；然而，如表 [2](#S4.T2 "表 2 ‣ 伪通量幅度
    ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法") 的分析所讨论的，它们不能用作误差度量。平均而言，两种重建模型在提高
    S/N 相对于 SDSS 图像方面都有效，而 CycleGAN 重建的 S/N 几乎与 DES 图像匹配。去噪自编码器已被用于减少图像中的噪声（Vincent
    et al., [2008](#bib.bib55)），因此 CAE 图像中的 S/N 大于目标图像并不令人惊讶。'
- en: 'In Table [3](#S4.T3 "Table 3 ‣ Signal-to-Noise Ratio ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), we list the mean proportional differences in
    the signal-to-noise ratios between image sets $i$ and $j$ to summarize the results
    from Figure [6](#S4.F6 "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"). As in Eq. ([6](#S4.E6 "In Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping")), we define the mean
    proportional difference between image set $i$ and $j$ as'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[3](#S4.T3 "Table 3 ‣ 信噪比 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射")中，我们列出了图像集$i$和$j$之间的信噪比的平均比例差异，以总结来自图[6](#S4.F6
    "Figure 6 ‣ 伪流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射")的结果。正如在方程式([6](#S4.E6
    "在伪流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射"))中定义的，我们定义图像集$i$和$j$之间的平均比例差异为'
- en: '|  | $\displaystyle\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}=\frac{1}{N_{\textnormal{img}}}\sum_{m}^{N_{\textnormal{img}}}\frac{\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0ptj}-\left[\textnormal{S/N}\right]_{m}^{\kern 1.0pti}}{\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0pti}}$ |  | (8) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}=\frac{1}{N_{\textnormal{img}}}\sum_{m}^{N_{\textnormal{img}}}\frac{\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0ptj}-\left[\textnormal{S/N}\right]_{m}^{\kern 1.0pti}}{\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0pti}}$ |  | (8) |'
- en: 'where $\left[\textnormal{S/N}\right]_{m}^{\kern 1.0pti}$ and $\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0ptj}$ are the signal-to-noise ratios (as defined in Eqn. ([7](#S4.E7 "In Signal-to-Noise
    Ratio ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) of corresponding
    image pairs from among the $N_{\textnormal{img}}$ images in datasets $\mathbb{X}_{i}$
    and $\mathbb{X}_{j}$, respectively. In Table [3](#S4.T3 "Table 3 ‣ Signal-to-Noise
    Ratio ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"), we can see
    that, for the validation dataset, the CycleGAN reconstructions did not provide
    a significant increase in the S/N relative to the DES images ($\Delta\hbox to0.0pt{\hskip
    0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}-\sigma<0<\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$ for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS, CycleGAN}$, where
    $\sigma$ is the standard deviation of $\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$). However, the CAE reconstructions did provide a significant increase
    over the DES images. The second and third columns from the left ($\mathbb{X}_{i}=\textnormal{SDSS}$
    for the external and validation data, respectively) indicate that the reconstructions
    provided a significant increase in the S/N of their corresponding SDSS images.
    Moreover, there was not a significant difference between the value of $\Delta\hbox
    to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS, CAE}$ in the validation
    and external datasets; this relationship is the same for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS,
    CycleGAN}$. This implies that image reconstruction via feature translation using
    our architectures provides a robust method to generate false galaxy images that
    share the same S/N as DES galaxies in this study.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\left[\textnormal{S/N}\right]_{m}^{\kern 1.0pti}$和$\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0ptj}$是信噪比（如方程([7](#S4.E7 "在信噪比 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")）所定义）对于来自数据集$\mathbb{X}_{i}$和$\mathbb{X}_{j}$的$N_{\textnormal{img}}$张图像中的对应图像对。在表[3](#S4.T3
    "表 3 ‣ 信噪比 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")中，我们可以看到，对于验证数据集，CycleGAN重建并未显著提高相对于DES图像的S/N（$\Delta\hbox
    to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}-\sigma<0<\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$，其中$\sigma$是$\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$的标准差）。然而，CAE重建确实显著提高了相对于DES图像的S/N。左侧的第二列和第三列（分别是外部和验证数据中的$\mathbb{X}_{i}=\textnormal{SDSS}$）表明，重建在其对应的SDSS图像中的S/N显著增加。此外，$\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS,
    CAE}$在验证和外部数据集中$\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$的值之间没有显著差异；这一关系在$\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS, CycleGAN}$中也是一样。这意味着，通过使用我们的架构进行特征转换的图像重建提供了一种强大的方法来生成与本研究中的DES银河相同S/N的虚假银河图像。
- en: '| $\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$ | External | Validation |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$ | 外部 | 验证 |'
- en: '| <svg version="1.1" height="25.53" width="150" overflow="visible"><g transform="translate(0,25.53)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,13.52) scale(1,
    -1)"><foreignobject width="24.16" height="13.52" overflow="visible">$\mathbb{X}_{j}$</foreignobject></g></g>
    <g  transform="translate(124.04,13.52)"><g transform="translate(0,12.01) scale(1,
    -1)"><foreignobject width="25.96" height="12.01" overflow="visible">$\mathbb{X}_{i}$</foreignobject></g></g></g></svg>
    | SDSS | SDSS | DES |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="25.53" width="150" overflow="visible"><g transform="translate(0,25.53)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,13.52) scale(1,
    -1)"><foreignobject width="24.16" height="13.52" overflow="visible">$\mathbb{X}_{j}$</foreignobject></g></g>
    <g  transform="translate(124.04,13.52)"><g transform="translate(0,12.01) scale(1,
    -1)"><foreignobject width="25.96" height="12.01" overflow="visible">$\mathbb{X}_{i}$</foreignobject></g></g></g></svg>
    | SDSS | SDSS | DES |'
- en: '| CAE | $5.138\pm 2.719$ | $2.893\pm 1.819$ | $0.710\pm 0.317$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| CAE | $5.138\pm 2.719$ | $2.893\pm 1.819$ | $0.710\pm 0.317$ |'
- en: '| CycleGAN | $2.067\pm 1.001$ | $1.334\pm 0.716$ | $\mathit{0.069\pm 0.125}$
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| CycleGAN | $2.067\pm 1.001$ | $1.334\pm 0.716$ | $\mathit{0.069\pm 0.125}$
    |'
- en: '| DES | N/A | $1.224\pm 0.779$ | N/A |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| DES | N/A | $1.224\pm 0.779$ | N/A |'
- en: 'Table 3: The mean proportional difference in the signal-to-noise ratios (see
    Eqn. ([8](#S4.E8 "In Signal-to-Noise Ratio ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"))) between each of the image sets; the standard deviation was used
    to estimate the error. As in Table [2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping"), the only dataset pair
    that does not show a significant difference in S/N is CycleGAN vs. DES.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：各图像集之间信噪比的平均比例差异（见公式 ([8](#S4.E8 "在信噪比 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")))；标准差用于估算误差。如表
    [2](#S4.T2 "表 2 ‣ 伪流量大小 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")
    所示，唯一没有显著信噪比差异的数据集对是 CycleGAN 与 DES。
- en: 4.3 Pseudo-Luminosity Profile
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 伪亮度分布
- en: '![Refer to caption](img/827d79fd9a6d229751fb897377e94e40.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/827d79fd9a6d229751fb897377e94e40.png)'
- en: 'Figure 7: Pseudo-luminosity profiles $\frac{dF}{dS}$ for the validation (left)
    and external (right) data; $\frac{dF}{dS}$ is defined in Eqn. ([9](#S4.E9 "In
    4.3 Pseudo-Luminosity Profile ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping")). The solid
    line represents $\frac{dF}{dS}$, while the dotted lines show $\frac{dF}{dS}\pm\hat{\sigma}$,
    where $\hat{\sigma}$ is the sample standard deviation. There was no statistically
    significant difference between the pseudo-luminosity profiles for any dataset.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：验证数据（左）和外部数据（右）的伪亮度分布 $\frac{dF}{dS}$；$\frac{dF}{dS}$ 的定义见公式 ([9](#S4.E9
    "在 4.3 伪亮度分布 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")). 实线表示 $\frac{dF}{dS}$，而虚线显示了
    $\frac{dF}{dS}\pm\hat{\sigma}$，其中 $\hat{\sigma}$ 是样本标准差。对于任何数据集，伪亮度分布之间没有统计学上显著的差异。
- en: 'In Section [4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), we showed that the image reconstructions provided a significant increase
    in the pseudo-flux magnitude and S/N of their corresponding SDSS images that matched
    that of the DES images. We also demonstrate that the improvement in $F$ and S/N
    were not heavily dependent on the dataset from which the source image was taken,
    providing evidence for the robustness of our method. Now, we will compare the
    pseudo-luminosity profiles of the objects in these images to characterize the
    structure of the objects themselves.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [4.2](#S4.SS2 "4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")
    节中，我们展示了图像重建显著提高了其对应 SDSS 图像的伪流量大小和信噪比，与 DES 图像相匹配。我们还证明了 $F$ 和信噪比的改进不严重依赖于源图像的数据集，为我们方法的稳健性提供了证据。现在，我们将比较这些图像中对象的伪亮度分布，以描述对象本身的结构。
- en: The pseudo-luminosity profile $\frac{dF}{dS}$, which is analogous to the luminosity
    profile in observed data, is defined by
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 伪亮度分布 $\frac{dF}{dS}$，类似于观测数据中的亮度分布，由以下公式定义
- en: '|  | $\displaystyle\frac{dF(r)}{dS}$ | $\displaystyle=\frac{1}{2\pi r{\Delta
    r}}\sum_{\hskip 4.0ptr_{i}\in\textnormal{Ann}(r)}F_{i}$ |  | (9) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{dF(r)}{dS}$ | $\displaystyle=\frac{1}{2\pi r{\Delta
    r}}\sum_{\hskip 4.0ptr_{i}\in\textnormal{Ann}(r)}F_{i}$ |  | (9) |'
- en: '|  |  | $\displaystyle=\frac{F_{\textnormal{Ann}(r)}}{2\pi r{\Delta r}}$ |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{F_{\textnormal{Ann}(r)}}{2\pi r{\Delta r}}$ |  |'
- en: where $F_{\textnormal{Ann}(r)}$ is the total flux contained within an annulus-shaped
    aperture $\textnormal{Ann}(r)$ with central radius $r$ and area $S=2\pi r{\Delta
    r}$, where ${\Delta r}=1\mathrm{\,pix}$
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $F_{\textnormal{Ann}(r)}$ 是包含在一个中心半径为 $r$、面积为 $S=2\pi r{\Delta r}$ 的环形孔径
    $\textnormal{Ann}(r)$ 内的总通量，其中 ${\Delta r}=1\mathrm{\,pix}$
- en: 'Plots of the pseudo-luminosity profile for the validation and external datasets
    are shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Pseudo-Luminosity Profile ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"). Bootstrapping was used to estimate the sample variance $\hat{\sigma}^{2}$;
    the dotted lines represent $\frac{dF}{dS}\pm\hat{\sigma}$. $\hat{\sigma}^{2}$
    was estimated by resampling each dataset 1000 times; for the validation dataset,
    the sample size was 50, while for the external dataset, the sample size was 2500\.
    In both the external and validation datasets, there was no significant difference
    between the pseudo-luminosity profiles of any image datasets, and from the pseudo-flux
    magnitude results (Figure [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset
    Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")), we know that the reconstructions
    were generally brighter than their SDSS counterparts. This implies that the reconstructions
    improved the brightness quality of the SDSS images without losing information
    about the object’s brightness profile distribution.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '验证集和外部数据集的伪亮度曲线图如图 [7](#S4.F7 "Figure 7 ‣ 4.3 Pseudo-Luminosity Profile ‣ 4
    Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping") 所示。使用自助法估计样本方差 $\hat{\sigma}^{2}$；虚线表示 $\frac{dF}{dS}\pm\hat{\sigma}$。$\hat{\sigma}^{2}$
    通过对每个数据集进行 1000 次重采样来估计；验证数据集的样本量为 50，而外部数据集的样本量为 2500。在外部和验证数据集中，任何图像数据集的伪亮度曲线图之间没有显著差异，并且从伪通量幅度结果（图
    [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping")）中，我们知道重建图像通常比其 SDSS 对应图像更亮。这意味着重建图像在不丢失对象亮度分布信息的情况下改善了 SDSS 图像的亮度质量。'
- en: 4.4 Image Quality Comparison
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 图像质量比较
- en: '![Refer to caption](img/ef20c93e639fb70d1dd1296ed867b2f3.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ef20c93e639fb70d1dd1296ed867b2f3.png)'
- en: 'Figure 8: Top: Mean luminance index $\bar{\ell}$ (defined in Eqn. ([4.4](#S4.Ex7
    "4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) for the
    validation and external data. Bottom: The first, second, and third quartiles of
    each distribution. $\bar{\ell}$ describes the similarities in brightness between
    two images at small scales ($\sim 10\mathrm{\,pix}$). The robustness of the method
    is indicated by the similarities in the validation and external distributions
    in the left-hand plot. In the right-hand plot, both reconstruction models increased
    $\bar{\ell}$ by a similar amount, indicating that, at small scales, the brightness
    increase provided by the two models were similar. This supports the conclusions
    drawn from the pseudo-flux magnitude in Fig. [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux
    Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping") and Table
    [2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"). Note that unlike in Figures [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux
    Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping") and [6](#S4.F6
    "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), the quartiles can be used to determine statistical significance because
    $\ell$ calculations provide direct image-to-image comparisons.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '图8：顶部：平均亮度指数 $\bar{\ell}$（定义在方程 ([4.4](#S4.Ex7 "4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey:
    一种用于跨调查图像映射的深度学习生成模型方法")）中）用于验证和外部数据。底部：每个分布的第一个、第二个和第三个四分位数。$\bar{\ell}$ 描述了在小尺度下（$\sim
    10\mathrm{\,pix}$）两幅图像之间的亮度相似性。方法的鲁棒性通过左侧图中的验证和外部分布的相似性来表示。在右侧图中，两种重建模型都将 $\bar{\ell}$
    提高了相似的量，表明在小尺度下，两种模型提供的亮度增加是类似的。这支持了图 [5](#S4.F5 "图 5 ‣ 伪流量幅度 ‣ 4.2 数据集属性 ‣ 4
    结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法") 和表 [2](#S4.T2 "表 2 ‣ 伪流量幅度 ‣ 4.2
    数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法") 中的结论。请注意，与图 [5](#S4.F5
    "图 5 ‣ 伪流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法") 和
    [6](#S4.F6 "图 6 ‣ 伪流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")
    不同，四分位数可以用来确定统计显著性，因为 $\ell$ 计算提供了直接的图像对图像比较。'
- en: '![Refer to caption](img/474df107226942783921e50f55420156.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/474df107226942783921e50f55420156.png)'
- en: 'Figure 9: Top: Mean contrast index $\bar{c}$ (defined in Eqn. ([4.4](#S4.Ex7
    "4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) for the
    validation and external data. Bottom: The first, second, and third quartiles of
    each distribution. $\bar{c}$ describes the relative sharpness of two images at
    small scales ($\sim 10\mathrm{\,pix}$). The robustness of the method is indicated
    by the similarities in the validation and external distributions in the left-hand
    plot. In the right-hand plot, $\bar{c}$ was generally lower for the CAE reconstructions
    than for the CycleGAN reconstructions, implying that the CAE images were generally
    blurrier than the CycleGAN images. This confirms the qualitative observations
    about the images described in Section [4.1](#S4.SS1 "4.1 Qualitative Analysis
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping") (see Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")). Note that unlike in Figures
    [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping") and [6](#S4.F6 "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset
    Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"), the quartiles can be used to
    determine statistical significance because $c$ calculations provide direct image-to-image
    comparisons.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：上图：均值对比指数 $\bar{c}$（定义见 Eqn. ([4.4](#S4.Ex7 "4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey：一种用于跨调查图像映射的深度学习生成模型方法")))
    用于验证和外部数据。下图：每个分布的第一个、第二个和第三个四分位数。$\bar{c}$ 描述了在小尺度（$\sim 10\mathrm{\,pix}$）下两幅图像的相对清晰度。方法的稳健性由左侧图中的验证和外部分布的相似性表明。在右侧图中，CAE
    重建的 $\bar{c}$ 通常低于 CycleGAN 重建的 $\bar{c}$，这表明 CAE 图像通常比 CycleGAN 图像模糊。这确认了第 [4.1](#S4.SS1
    "4.1 定性分析 ‣ 4 结果与分析 ‣ Survey2Survey：一种用于跨调查图像映射的深度学习生成模型方法") 节中关于图像的定性观察（见图 [3](#S4.F3
    "图 3 ‣ 4.1 定性分析 ‣ 4 结果与分析 ‣ Survey2Survey：一种用于跨调查图像映射的深度学习生成模型方法")）。请注意，与图 [5](#S4.F5
    "图 5 ‣ 伪流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey：一种用于跨调查图像映射的深度学习生成模型方法") 和图
    [6](#S4.F6 "图 6 ‣ 伪流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey：一种用于跨调查图像映射的深度学习生成模型方法")
    不同，四分位数可以用来确定统计显著性，因为 $c$ 计算提供了直接的图像对比。
- en: '![Refer to caption](img/ad45a2cb30553c8ec3db5849829568b3.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ad45a2cb30553c8ec3db5849829568b3.png)'
- en: 'Figure 10: Top: Mean cross-correlation index $\bar{s}$ (defined in Eqn. ([4.4](#S4.Ex7
    "4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) for the
    validation and external data. Bottom: The first, second, and third quartiles of
    each distribution. $\bar{s}$ describes the similarities between the structure
    of two images at small scales ($\sim 10\mathrm{\,pix}$), providing a measure of
    the faithfulness of the reconstruction. The robustness of the method is indicated
    by the similarities in the validation and external distributions in the left-hand
    plot. In the right-hand plot, $\bar{s}$ was generally lower for the CycleGAN reconstructions
    than for the CAE reconstructions, implying that the CAE architecture more accurately
    recreated small-scale details of the DES images, providing a more accurate reconstruction
    of the morphological properties of the image. Note that unlike in Figures [5](#S4.F5
    "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") and [6](#S4.F6 "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), the quartiles can be used to determine statistical
    significance because $s$ calculations provide direct image-to-image comparisons.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10：上图：用于验证和外部数据的均值交叉相关指数 $\bar{s}$（定义见方程 ([4.4](#S4.Ex7 "4.4 图像质量比较 ‣ 4 结果与分析
    ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射"))）。下图：每个分布的第一、第二和第三四分位数。$\bar{s}$ 描述了两个图像在小尺度下（$\sim
    10\mathrm{\,pix}$）的结构相似性，提供了重建忠实度的度量。方法的稳健性由左图中验证和外部分布的相似性表示。在右图中，CycleGAN 重建的
    $\bar{s}$ 一般低于 CAE 重建的 $\bar{s}$，这表明 CAE 架构更准确地重建了 DES 图像的小尺度细节，提供了对图像形态特征的更准确重建。请注意，与图
    [5](#S4.F5 "图 5 ‣ 假流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射")
    和 [6](#S4.F6 "图 6 ‣ 假流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射")
    不同的是，这里的四分位数可以用于确定统计显著性，因为 $s$ 计算提供了直接的图像对图像比较。'
- en: '![Refer to caption](img/6d36018af3c0d40382433e6ee08cc5e4.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/6d36018af3c0d40382433e6ee08cc5e4.png)'
- en: 'Figure 11: Top: Mean structural similarity index (defined in Eqn. ([12](#S4.E12
    "In 4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) for the
    validation and external data. Bottom: The first, second, and third quartiles of
    each distribution. The MSSIM, which is the mean of the product of $\ell$, $c$,
    and $s$, provides a metric for the overall relative image quality. The robustness
    of the method is indicated by the similarities in the validation and external
    distributions in the left-hand plot. From the right-hand plot, we can see that
    the overall quality of the CAE images was similar to that of the DES images, while
    the quality of the CycleGAN reconstructions was further removed from that of the
    DES images. Note that unlike in Figures [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping") and [6](#S4.F6 "Figure
    6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), the quartiles can be used to determine statistical significance because
    MSSIM calculations provide direct image-to-image comparisons.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11：上图：用于验证和外部数据的均值结构相似度指数（定义见方程 ([12](#S4.E12 "在 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey:
    一种深度学习生成模型方法用于跨调查图像映射"))）。下图：每个分布的第一、第二和第三四分位数。MSSIM，即 $\ell$、$c$ 和 $s$ 的乘积的均值，提供了整体相对图像质量的度量。方法的稳健性由左图中验证和外部分布的相似性表示。从右图中可以看出，CAE
    图像的整体质量与 DES 图像相似，而 CycleGAN 重建的质量则远低于 DES 图像。请注意，与图 [5](#S4.F5 "图 5 ‣ 假流量幅度 ‣
    4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射") 和 [6](#S4.F6 "图 6
    ‣ 假流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型方法用于跨调查图像映射") 不同的是，这里的四分位数可以用于确定统计显著性，因为
    MSSIM 计算提供了直接的图像对图像比较。'
- en: 'As shown in Section [4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), the brightnesses and S/N of the reconstructions were greater than or
    not significantly different from those of the DES images, and in [4.3](#S4.SS3
    "4.3 Pseudo-Luminosity Profile ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"), we show that
    the brightness increase provided by the reconstruction has little effect on the
    radial profiles of the objects. Now, we will characterize how effective each reconstruction
    model is at amplifying the image signal, reducing background noise, improving
    image quality, and retaining the morphological information contained within the
    original image. We also highlight several notable images from the external dataset
    that show that CAE reconstructions may help remove image artifact.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '如[4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping")节所示，重建图像的亮度和信噪比（S/N）大于或与
    DES 图像没有显著差异，而在[4.3](#S4.SS3 "4.3 Pseudo-Luminosity Profile ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")节中，我们展示了重建所提供的亮度增加对对象的径向轮廓影响甚微。现在，我们将深入研究每个重建模型在增强图像信号、减少背景噪声、改善图像质量和保留原始图像中的形态信息方面的效果。我们还重点介绍了外部数据集中几个显著的图像，这些图像表明
    CAE 重建可能有助于去除图像伪影。'
- en: The mean structural similarity index (MSSIM) (Zhou Wang et al., [2004](#bib.bib61))
    is a method used to compare image quality that takes into account differences
    in brightness, sharpness, and small-scale features. The MSSIM is defined by the
    product of the luminance index $\ell$, contrast index $c$, and cross-correlation
    index $s$. For a pair of images X and Y, where each respective entry $\textbf{{X}}_{ij}$
    and $\textbf{{Y}}_{ij}$ is the pixel brightness $\beta_{ij}$ of pixel $P_{ij}$,
    let $\textbf{{x}}_{ij}$ $\left(\textbf{{y}}_{ij}\right)$ be an 11 $\times$ 11
    window centered around pixel $x_{ij}$ $\left(y_{ij}\right)$. After smoothing $\textbf{{x}}_{ij}$
    $\left(\textbf{{y}}_{ij}\right)$ by an 11-tap Gaussian filter, define $\ell$,
    $c$, and $s$ as
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 平均结构相似性指数（MSSIM）（Zhou Wang 等，[2004](#bib.bib61)）是一种用于比较图像质量的方法，考虑了亮度、清晰度和小尺度特征的差异。MSSIM
    定义为亮度指数 $\ell$、对比度指数 $c$ 和互相关指数 $s$ 的乘积。对于一对图像 X 和 Y，其中每个相应条目 $\textbf{{X}}_{ij}$
    和 $\textbf{{Y}}_{ij}$ 是像素 $P_{ij}$ 的像素亮度 $\beta_{ij}$，设 $\textbf{{x}}_{ij}$ $\left(\textbf{{y}}_{ij}\right)$
    为以像素 $x_{ij}$ $\left(y_{ij}\right)$ 为中心的 11 $\times$ 11 窗口。在用 11 次抽头的高斯滤波器平滑 $\textbf{{x}}_{ij}$
    $\left(\textbf{{y}}_{ij}\right)$ 之后，定义 $\ell$、$c$ 和 $s$ 如下
- en: '|  | $\displaystyle\ell\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ |
    $\displaystyle=\frac{2\mu_{x}\mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$ |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ell\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ |
    $\displaystyle=\frac{2\mu_{x}\mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$ |  |'
- en: '|  | $\displaystyle c\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ | $\displaystyle=\frac{2\sigma_{x}\sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}},$
    |  | (10) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ | $\displaystyle=\frac{2\sigma_{x}\sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}},$
    |  | (10) |'
- en: '|  | $\displaystyle s\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ | $\displaystyle=\frac{2\sigma_{xy}+C_{2}}{2\sigma_{x}\sigma_{y}+C_{2}}.$
    |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ | $\displaystyle=\frac{2\sigma_{xy}+C_{2}}{2\sigma_{x}\sigma_{y}+C_{2}}.$
    |  |'
- en: Then structural similarity index SSIM can be calculated as
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后结构相似性指数 SSIM 可以计算为
- en: '|  | $\displaystyle\textnormal{SSIM}\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$
    | $\displaystyle=\ell\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)c\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)s\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$
    |  | (11) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textnormal{SSIM}\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$
    | $\displaystyle=\ell\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)c\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)s\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$
    |  | (11) |'
- en: '|  |  | $\displaystyle=\frac{\left(2\mu_{x}\mu_{y}+C_{1}\right)\left(2\sigma_{xy}+C_{2}\right)}{\left(\mu_{x}^{2}+\mu_{y}^{2}+C_{1}\right)\left(\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}\right)},$
    |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{\left(2\mu_{x}\mu_{y}+C_{1}\right)\left(2\sigma_{xy}+C_{2}\right)}{\left(\mu_{x}^{2}+\mu_{y}^{2}+C_{1}\right)\left(\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}\right)},$
    |  |'
- en: Here, $\mu_{x}$ ($\mu_{y}$) is the mean of $\textbf{{x}}_{ij}$ $\left(\textbf{{y}}_{ij}\right)$,
    $\sigma_{x}^{2}$ $\left(\sigma_{y}^{2}\right)$ is the variance of $\textbf{{x}}_{ij}$
    $\left(\textbf{{y}}_{ij}\right)$, $\sigma_{xy}^{2}$ is the covariance, $C_{1}=(0.01R_{D})^{2}$,
    and $C_{2}=(0.03R_{D})^{2}$ are stabilization constants for which $R_{D}$ is the
    dynamic range of the image (in our case, $R_{D}=1$). Then the MSSIM is defined
    by
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mu_{x}$ ($\mu_{y}$) 是 $\textbf{{x}}_{ij}$ $\left(\textbf{{y}}_{ij}\right)$
    的均值，$\sigma_{x}^{2}$ $\left(\sigma_{y}^{2}\right)$ 是 $\textbf{{x}}_{ij}$ $\left(\textbf{{y}}_{ij}\right)$
    的方差，$\sigma_{xy}^{2}$ 是协方差，$C_{1}=(0.01R_{D})^{2}$ 和 $C_{2}=(0.03R_{D})^{2}$ 是稳定常数，其中
    $R_{D}$ 是图像的动态范围（在我们的情况下，$R_{D}=1$）。然后，MSSIM 被定义为
- en: '|  | $\displaystyle\textnormal{MSSIM}=\frac{1}{N_{P}^{2}}\sum_{i,j}^{N_{P}}\textnormal{SSIM}\left(\textbf{{x}}_{i,j},\textbf{{y}}_{i,j}\right)$
    |  | (12) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textnormal{MSSIM}=\frac{1}{N_{P}^{2}}\sum_{i,j}^{N_{P}}\textnormal{SSIM}\left(\textbf{{x}}_{i,j},\textbf{{y}}_{i,j}\right)$
    |  | (12) |'
- en: and the mean luminance, contrast, and cross-correlation indices ($\bar{\ell}$,
    $\bar{c}$, and $\bar{s}$, respectively) are defined similarly.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 平均亮度、对比度和互相关指标（分别为 $\bar{\ell}$、$\bar{c}$ 和 $\bar{s}$）也以类似的方式定义。
- en: 'KDEs of histograms for $\bar{\ell}$, $\bar{c}$, $\bar{s}$, and MSSIM for the
    overlap and external data are shown in Figures [8](#S4.F8 "Figure 8 ‣ 4.4 Image
    Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"), [9](#S4.F9 "Figure 9 ‣ 4.4 Image
    Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"), [10](#S4.F10 "Figure 10 ‣ 4.4
    Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping"), and [11](#S4.F11 "Figure
    11 ‣ 4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping"), respectively.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bar{\ell}$、$\bar{c}$、$\bar{s}$ 和 MSSIM 的直方图的 KDEs 在重叠数据和外部数据中如图 [8](#S4.F8
    "图 8 ‣ 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")、[9](#S4.F9
    "图 9 ‣ 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")、[10](#S4.F10
    "图 10 ‣ 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法") 和 [11](#S4.F11
    "图 11 ‣ 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法") 所示。'
- en: As the SDSS galaxies are substantially different in brightness and radii, it
    is not valid to use $\bar{\ell}$, $\bar{c}$, $\bar{s}$, and MSSIM as image quality
    metrics for DES/SDSS and reconstruction/SDSS image pairs. However, if the reconstruction
    process is robust, the distributions for DES/SDSS pairs and reconstruction/SDSS
    pairs should be consistent in the validation and external datasets. Hence, we
    will use reconstruction/DES and reconstruction/reconstruction measurements to
    quantify the reconstruction quality and reconstruction/SDSS measurements as metrics
    for robustness.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 SDSS 星系在亮度和半径上有显著差异，因此不适合使用 $\bar{\ell}$、$\bar{c}$、$\bar{s}$ 和 MSSIM 作为 DES/SDSS
    和重建/SDSS 图像对的图像质量指标。然而，如果重建过程是稳健的，那么 DES/SDSS 对和重建/SDSS 对的分布在验证和外部数据集中应该是一致的。因此，我们将使用重建/DES
    和重建/重建测量来量化重建质量，并将重建/SDSS 测量作为稳健性的指标。
- en: 'The mean luminance index $\bar{\ell}$ is a measure of the differences in the
    pixel-to-pixel brightness of two (smoothed) images. The reconstruction/DES distributions
    for $\bar{\ell}$ were similar in shape, and there was not a significant difference
    between their medians, indicating that they had similar brightness qualities to
    one another; this is consistent with the pseudo-flux magnitude results (Figure
    [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping") and Table [2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset
    Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")). The brightness quality of the
    reconstructions relative to their SDSS counterparts were extremely similar to
    one another in both the validation and external distributions, implying that both
    were equally effective at increasing the image brightnesses.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '平均亮度指数$\bar{\ell}$是两张（平滑）图像之间像素到像素亮度差异的度量。重建/DES的$\bar{\ell}$分布形状相似，它们的中位数之间没有显著差异，表明它们之间的亮度质量相似；这与伪流量幅度结果一致（图[5](#S4.F5
    "图 5 ‣ 伪流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")和表[2](#S4.T2
    "表 2 ‣ 伪流量幅度 ‣ 4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")）。相对于其SDSS对应图像，重建图像的亮度质量在验证和外部分布中极其相似，这意味着两者在提高图像亮度方面效果相当。'
- en: While the $\bar{\ell}$ values for the external dataset cannot be interpreted
    as measures of the image brightness qualities, they can be used to support the
    robustness of the reconstruction process. The shapes of the CAE/SDSS $\bar{\ell}$
    distributions for the validation and external datasets were similar to one another,
    and there was not a significant difference between their medians; the same is
    true for CycleGAN/SDSS. This implies that the brightness quality improvement was
    consistent for both the validation and external SDSS datasets, providing evidence
    for the robustness of this method.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然外部数据集的$\bar{\ell}$值不能解释为图像亮度质量的度量，但它们可以用来支持重建过程的稳健性。CAE/SDSS $\bar{\ell}$分布的形状在验证数据集和外部数据集中相似，它们的中位数之间没有显著差异；CycleGAN/SDSS也是如此。这意味着亮度质量的改善在验证和外部SDSS数据集中是一致的，为该方法的稳健性提供了证据。
- en: 'The mean contrast index $\bar{c}$ describes the average difference in smoothness
    between small cut-outs of image pairs. For the validation data, the CycleGAN reconstructions
    had a significantly higher contrast index than the CAE reconstructions, implying
    that the sharpness of the CycleGAN images was more consistent with that of the
    DES images. This confirms that the CAE reconstructions tended to smooth the images,
    leading to the blurriness seen in Figures [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") and [4](#S4.F4 "Figure 4 ‣ 4.1
    Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping"). The robustness of
    the reconstructions can again be seen by the lack of a significant difference
    between the test and external distributions for the reconstruction/SDSS $\bar{c}$
    distributions. Note that the differences between the S/N for each image set likely
    contributed to the value of $\bar{c}$; however, the qualitative sharpness of the
    reconstructions are consistent with the conclusions drawn from $\bar{c}$.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '平均对比度指数$\bar{c}$描述了图像对小切片之间的平滑度差异。对于验证数据，CycleGAN重建的对比度指数显著高于CAE重建，表明CycleGAN图像的清晰度与DES图像的清晰度更为一致。这证实了CAE重建往往会使图像变得平滑，从而导致图像模糊，如图[3](#S4.F3
    "图 3 ‣ 4.1 质量分析 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")和图[4](#S4.F4
    "图 4 ‣ 4.1 质量分析 ‣ 4 结果与分析 ‣ Survey2Survey: 一种用于跨调查图像映射的深度学习生成模型方法")所示。重建的稳健性可以通过重建/SDSS
    $\bar{c}$分布与外部分布之间没有显著差异再次体现。注意，每个图像集的S/N差异可能影响了$\bar{c}$的值；然而，重建图像的定性清晰度与$\bar{c}$得出的结论是一致的。'
- en: The mean cross-correlation index $\bar{s}$ is a measure of the deviations in
    the small-scale structure between two images; large values of $\bar{s}$ indicate
    that, after normalizing for the brightness and sharpness, the morphological features
    of the images at small scales are similar to (strongly correlated with) one another.
    The $\bar{s}$ distributions for the validation data indicates that the CAE reconstructions
    are significantly more closely correlated with their DES counterparts at small
    scales than the CycleGAN reconstructions. This implies that CAE reconstruction
    preserves more information at small scales than CycleGAN.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 平均互相关指数 $\bar{s}$ 是衡量两幅图像之间小尺度结构偏差的指标；较大的 $\bar{s}$ 值表明，在对亮度和清晰度进行标准化后，图像在小尺度上的形态特征彼此相似（强相关）。验证数据的
    $\bar{s}$ 分布表明，CAE 重建在小尺度上与其 DES 对应物的相关性显著高于 CycleGAN 重建。这意味着 CAE 重建在小尺度上保留的信息比
    CycleGAN 更多。
- en: 'The combination of these quantities yields the MSSIM distributions seen in
    Fig. [11](#S4.F11 "Figure 11 ‣ 4.4 Image Quality Comparison ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"). This metric indicates that the overall quality of the CAE images was
    comparable to that of the CycleGAN reconstructions; however, the breakdown in
    terms of $\bar{\ell}$, $\bar{c}$, and $\bar{s}$ suggests that the reconstruction
    methods provide differing benefits. Specifically, CycleGAN reconstructions are
    generally sharper than their CAE counterparts, while CAE reconstructions preserve
    more information at small scales in the image.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些量的组合产生了图 [11](#S4.F11 "图 11 ‣ 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")
    中看到的 MSSIM 分布。这一指标表明，CAE 图像的整体质量与 CycleGAN 重建的质量相当；然而，$\bar{\ell}$、$\bar{c}$ 和
    $\bar{s}$ 的分解表明重建方法提供了不同的好处。具体而言，CycleGAN 重建通常比 CAE 对应物更清晰，而 CAE 重建在图像的小尺度上保留了更多信息。
- en: '![Refer to caption](img/016eed935e10f08987035c24784c45ca.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/016eed935e10f08987035c24784c45ca.png)'
- en: 'Figure 12: A selection of several notable objects from the external dataset.
    In each of these images, it appears that the CAE reconstructions may have removed
    artifacts from the image. The reconstructed objects may have been generated through
    inpainting.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：外部数据集中多个显著对象的选择。在这些图像中，CAE 重建可能已经去除了图像中的伪影。重建的对象可能通过填补技术生成。
- en: '![Refer to caption](img/30091a24c2501684c7b05093ac318fc3.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/30091a24c2501684c7b05093ac318fc3.png)'
- en: 'Figure 13: A selection of several notable objects from the validation dataset.
    In each of these images, it appears that the CAE reconstructions may have removed
    large artifacts from the image. Note that both reconstructions may have used inpainting
    to generate the images in column 1, while the CAE reconstruction of the central
    object in column 2 appears to have removed the corrupted region of that object.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：验证数据集中几个显著对象的选择。在这些图像中，CAE 重建可能已经去除了图像中的大伪影。请注意，两种重建方法可能都使用了填补技术来生成第一列中的图像，而第二列中中央对象的
    CAE 重建似乎去除了该对象的损坏区域。
- en: 'Finally, we would like to highlight several unique images from the external
    dataset; these are shown in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality
    Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). These images were found through
    visual inspection of images with the lowest reconstruction/SDSS $\bar{\ell}$,
    $\bar{c}$, $\bar{s}$, and/or MSSIM values in the external dataset.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想突出显示外部数据集中几个独特的图像；这些图像如图 [12](#S4.F12 "图 12 ‣ 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")
    所示。这些图像是通过对外部数据集中重建/SDSS $\bar{\ell}$、$\bar{c}$、$\bar{s}$ 和/或 MSSIM 值最低的图像进行视觉检查找到的。
- en: 'Each image in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping") is heavily corrupted by artifacts; however, the
    CAE reconstructions appear to have removed these artifacts at the cost of blurring
    the objects in the image. These results are consistent with studies of denoising
    autoencoders (Vincent et al., [2008](#bib.bib55)), which proven effective at smoothing
    brightness/color variations, removing artifacts, and restoring corrupted images.
    As the base architecture of a denoising autoencoder is similar to that of our
    encoder/decoder pair, it is not surprising that the image reconstructions were
    effective at removing these artifacts. The CycleGAN reconstructions, however,
    fail to consistently remove these artifacts, though do succeed in amplifying the
    brightness of these objects.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '图[12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")中的每个图像都被伪影严重污染；然而，CAE重建似乎已经去除了这些伪影，但代价是模糊了图像中的物体。这些结果与去噪自编码器（Vincent
    et al., [2008](#bib.bib55)）的研究一致，这些去噪自编码器在平滑亮度/颜色变化、去除伪影和恢复受损图像方面被证明是有效的。由于去噪自编码器的基本架构类似于我们的编码器/解码器对，因此图像重建在去除这些伪影方面的效果并不令人惊讶。然而，CycleGAN重建未能一致地去除这些伪影，但确实成功地增强了这些物体的亮度。'
- en: 'Figure [13](#S4.F13 "Figure 13 ‣ 4.4 Image Quality Comparison ‣ 4 Results and
    Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping") shows several images from the validation dataset that contain
    artifacts. The images in row 1 were found due to their extreme reconstruction/SDSS
    $\bar{\ell}$, $\bar{c}$, $\bar{s}$, and MSSIM values; however, those in row 2
    were found via manual inspection of the validation dataset. This was to be expected
    because the Stripe82 dataset is generally of higher quality than the external
    dataset.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '图[13](#S4.F13 "Figure 13 ‣ 4.4 Image Quality Comparison ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")显示了包含伪影的验证数据集中的几个图像。第1行的图像由于其极端的重建/SDSS $\bar{\ell}$, $\bar{c}$, $\bar{s}$
    和 MSSIM 值被发现；然而，第2行的图像则通过对验证数据集的人工检查发现。这是可以预料的，因为Stripe82数据集通常比外部数据集的质量更高。'
- en: 'In row 1, it appears that the CAE reconstruction removed the artifact, albeit
    at the cost of blurring the central object. The artifact in row 2 consists of
    a blue streak passing through the upper-left edge of the central object. Like
    column 4 in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison ‣ 4
    Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), there is no signal in this region of the CAE
    reconstruction, implying that little or no inpainting was performed.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '在第1行中，CAE重建似乎去除了伪影，但代价是模糊了中心物体。第2行中的伪影是一个蓝色条纹穿过中心物体的左上边缘。像图[12](#S4.F12 "Figure
    12 ‣ 4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping")中的第4列一样，CAE重建的这一区域没有信号，这意味着几乎没有进行填补。'
- en: 'As the validation data and training data were taken from the same population,
    it is likely that the training data had a similar incidence of corrupted images
    as the validation data. As a result, it is unlikely that either neural network
    was trained sufficiently to accurately extract the signal from the heavily corrupted
    images in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"), implying that objects recovered in these images likely resulted
    from inpainting. While outside the scope of this work, the improvement in the
    quality of the images in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), especially given the lack of training on corrupted
    images, warrants a more thorough analysis of the effectiveness of corrupted image
    reconstruction using our CAE architecture.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于验证数据和训练数据来自相同的总体，因此训练数据中可能存在与验证数据类似的图像损坏情况。因此，不太可能这两个神经网络都经过充分训练以准确提取来自图像[12](#S4.F12
    "图 12 ‣ 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")的严重损坏图像中的信号，这意味着这些图像中恢复的物体很可能是通过图像修复得到的。尽管超出了本工作的范围，但考虑到缺乏对损坏图像的训练，图像[12](#S4.F12
    "图 12 ‣ 4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey：一种深度学习生成模型方法用于跨调查图像映射")中图像质量的改善，值得对我们CAE架构用于损坏图像重建的效果进行更深入的分析。
- en: 5 Conclusions
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this work, we demonstrated the viability of robust cross-survey galaxy image
    translation using neural networks and generative models. Using the pseudo-flux
    magnitude (Section [4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")) and mean luminance index $\bar{\ell}$ (Section [4.4](#S4.SS4 "4.4 Image
    Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")), we show that the average brightnesses
    of the reconstructions more closely match DES images than their SDSS source images
    while preserving the structural information contained within the source galaxy
    (Section [4.3](#S4.SS3 "4.3 Pseudo-Luminosity Profile ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")). In Section [4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), we also demonstrated that the reconstruction process improved the signal-to-noise
    ratio of the source images. The signal-to-noise ratio of the CycleGAN images closely
    correlated with that of the DES images, while the CAE images improved this quantity
    relative to the DES images; this behavior is expected because autoencoders have
    been shown to be effective at reducing the amount of noise in images (Vincent
    et al., [2008](#bib.bib55)). Together, these imply that our method can be used
    to improve image brightness and signal strength using image-to-image translation.
    In Section [4.4](#S4.SS4 "4.4 Image Quality Comparison ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), we discuss the pros and cons of each reconstruction method using the
    mean contrast index $\bar{c}$ and cross-correlation index $\bar{s}$. We found
    that CycleGAN reconstructions were sharper, while CAE reconstructions more accurately
    reproduced the structure of DES galaxies at length scales on the order of several
    pixels at the cost of being slightly blurrier. Finally, we highlighted several
    instances in which the reconstructions appear to have removed large artifacts.
    We find evidence for the robustness of our method by performing reconstructions
    on images from the SDSS catalog in the external region, which contains objects
    without a DES counterpart. Though these images were fainter and had lower S/N
    than images from the overlap region (Stripe82), the large- and small-scale statistics
    of these image reconstructions were similar to those in the overlap region, implying
    that the reconstruction process accurately created DES representation of these
    objects. However, there is the possibility that our model was overfitted due to
    our choice to avoid factors that may impact the accuracy of the map between SDSS
    and DES images in the Stripe82 region.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们展示了使用神经网络和生成模型进行鲁棒跨调查星系图像转换的可行性。通过使用伪流量幅度（第[4.2节](#S4.SS2 "4.2 数据集属性
    ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型的方法用于跨调查图像映射")）和平均亮度指数 $\bar{\ell}$（第[4.4节](#S4.SS4
    "4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型的方法用于跨调查图像映射")），我们展示了重建图像的平均亮度比它们的SDSS源图像更接近DES图像，同时保留了源星系中的结构信息（第[4.3节](#S4.SS3
    "4.3 伪亮度轮廓 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型的方法用于跨调查图像映射")）。在第[4.2节](#S4.SS2
    "4.2 数据集属性 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型的方法用于跨调查图像映射")中，我们还展示了重建过程改善了源图像的信噪比。CycleGAN图像的信噪比与DES图像的信噪比密切相关，而CAE图像在相对于DES图像的信噪比上有所改善；这种行为是预期的，因为自编码器已经被证明在减少图像噪声方面是有效的（Vincent
    et al., [2008](#bib.bib55)）。综合来看，这表明我们的方法可以通过图像到图像的转换来改善图像亮度和信号强度。在第[4.4节](#S4.SS4
    "4.4 图像质量比较 ‣ 4 结果与分析 ‣ Survey2Survey: 一种深度学习生成模型的方法用于跨调查图像映射")中，我们使用平均对比度指数 $\bar{c}$
    和交叉相关指数 $\bar{s}$ 讨论了每种重建方法的优缺点。我们发现，CycleGAN重建的图像更清晰，而CAE重建则在几像素数量级上更准确地再现了DES星系的结构，代价是稍微模糊。最后，我们强调了几个重建图像看起来已去除大范围伪影的实例。我们通过对来自SDSS目录的外部区域图像进行重建来寻找我们方法鲁棒性的证据，该区域包含没有DES对应物的对象。尽管这些图像比重叠区域（Stripe82）中的图像更暗且信噪比更低，但这些图像重建的大尺度和小尺度统计特征与重叠区域的图像类似，这意味着重建过程准确地创建了这些对象的DES表示。然而，由于我们选择避免可能影响SDSS和DES图像之间映射准确性的因素，因此存在我们的模型可能过拟合的可能性。'
- en: 'While this only constitutes an initial application, our results show that feature
    transfer learning shows promise as a method for false galaxy image generation.
    This has great implications for the analysis of astronomical survey data: assuming
    that there is a sufficiently large sample of corresponding SDSS and DES image
    pairs, one could improve the brightness and S/N of many images from the SDSS catalog,
    decreasing the amount of error and improving the statistical power of analyses.
    Additionally, this provides an important advantage over other generative models
    used supplement survey data: while other methods generate false images that share
    the properties of the images in the data set of interest, feature-to-feature translation
    provides representations of observed galaxies, providing a way to extend both
    the size and the sky coverage of galaxy surveys.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这仅仅是初步应用，我们的结果显示，特征迁移学习作为生成虚假星系图像的方法展现了前景。这对天文调查数据的分析具有重大意义：假设有足够大的SDSS和DES图像对样本，可以提高SDSS目录中许多图像的亮度和信噪比，减少误差并提高分析的统计功效。此外，这相较于其他用于补充调查数据的生成模型具有重要优势：虽然其他方法生成的虚假图像具有与数据集中图像相似的属性，但特征对特征的翻译提供了观察到的星系的表示，提供了一种扩展星系调查规模和天空覆盖范围的方法。
- en: The reconstruction pipeline we developed solely constitutes a initial exploration,
    but the efficiency and robustness of the reconstruction process shows promise
    as a method for generating or improving survey data. While SDSS and DES data were
    used in this work, we expect that this may be applicable to other surveys, particularly
    for deeper surveys such as LSST (Ivezić et al., [2019](#bib.bib21)). All quantities
    calculated were derived solely from the mean of the (r, g, b) channel pixel values
    of survey images; however, we anticipate that similar methods could be used for
    the generation of false images with physical observables consistent with those
    of survey images. In addition, our methodology could be expanded to enable cross-wavelength
    or band-to-band translation. A neural network could be trained with a feature
    set containing fewer bands than the target dataset, generating a map between each
    pair of bands in the training and target data. The trained network could be used
    to supplement survey data by generating realistic reconstructions of image data
    in frequency bands not probed by that survey. We intend to explore these applications
    in future work using DES DR2 data, which contains more images and has a greater
    field depth than DES DR1 (Abbott et al., [2021](#bib.bib4)).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发的重建管道仅构成初步探索，但重建过程的效率和稳健性展现了生成或改进调查数据的方法的前景。尽管本工作中使用了SDSS和DES数据，我们预计这可能适用于其他调查，特别是对于LSST这样的深层次调查（Ivezić
    et al., [2019](#bib.bib21)）。所有计算的量均仅来自调查图像的（r, g, b）通道像素值的均值；然而，我们预期类似的方法可以用于生成具有与调查图像一致的物理可观测量的虚假图像。此外，我们的方法可以扩展到实现跨波长或带到带的翻译。可以用包含比目标数据集更少带的特征集来训练神经网络，生成训练数据和目标数据中每对带之间的映射。训练好的网络可以用于补充调查数据，通过生成在该调查未探测的频率带中的真实重建图像数据。我们打算在未来的工作中使用DES
    DR2数据进行这些应用探索，该数据包含更多图像，并且比DES DR1（Abbott et al., [2021](#bib.bib4)）具有更大的视场深度。
- en: 6 Acknowledgments
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 致谢
- en: This material is based upon work supported by the National Science Foundation
    Graduate Research Fellowship Program under Grant No. DGE — 1746047\. M. Carrasco
    Kind has been supported by NSF Grant AST-1536171.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本材料基于国家科学基金会研究生研究奖学金项目（Grant No. DGE — 1746047）的支持。M. Carrasco Kind得到了NSF Grant
    AST-1536171的资助。
- en: Author contribution
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作者贡献
- en: 'B. Buncher: Data analysis, figure creation, writing, and editing.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'B. Buncher: 数据分析、图形创建、写作和编辑。'
- en: 'A. N. Sharma: AI model creation, data collection, figure creation, writing,
    and editing.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 'A. N. Sharma: AI模型创建、数据收集、图形创建、写作和编辑。'
- en: 'M. Carrasco Kind: Oversight, data collection, writing, and editing.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 'M. Carrasco Kind: 监督、数据收集、写作和编辑。'
- en: Softwares Used
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用的软件
- en: This research made us of matplotlib (Hunter, [2007](#bib.bib19)), numpy (Oliphant,
    [2006](#bib.bib34); Van Der Walt et al., [2011](#bib.bib51)), scikit-image (Van der
    Walt et al., [2014](#bib.bib52)), SciPY (Virtanen et al., [2019](#bib.bib56)),
    and seaborn (Waskom et al., [2017](#bib.bib59)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用了matplotlib（Hunter, [2007](#bib.bib19)）、numpy（Oliphant, [2006](#bib.bib34)；Van
    Der Walt et al., [2011](#bib.bib51)）、scikit-image（Van der Walt et al., [2014](#bib.bib52)）、SciPY（Virtanen
    et al., [2019](#bib.bib56)）和seaborn（Waskom et al., [2017](#bib.bib59)）。
- en: This research made use of Astropy,⁴⁴4http://www.astropy.org a community-developed
    core Python package for Astronomy (Astropy Collaboration et al., [2013](#bib.bib6);
    Price-Whelan et al., [2018](#bib.bib39)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用了 Astropy，⁴⁴4http://www.astropy.org 这是一个由社区开发的核心 Python 天文学包（Astropy 合作组等，[2013](#bib.bib6)；Price-Whelan
    等人，[2018](#bib.bib39)）。
- en: This research made use of Photutils, an Astropy package for detection and photometry
    of astronomical sources (Bradley et al., [2019](#bib.bib8)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用了 Photutils，这是一个用于天文源检测和光度测量的 Astropy 包（Bradley 等人，[2019](#bib.bib8)）。
- en: Data Availability Statement
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可用性声明
- en: The data underlying this article will be shared on reasonable request to the
    corresponding author.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本文所用数据将在合理要求下与对应作者共享。
- en: References
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abadi et al. (2015) Abadi M., et al., 2015, TensorFlow: Large-Scale Machine
    Learning on Heterogeneous Systems, [http://tensorflow.org/](http://tensorflow.org/)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abadi 等人（2015） Abadi M. 等，2015，TensorFlow: 大规模机器学习异构系统，[http://tensorflow.org/](http://tensorflow.org/)'
- en: Abazajian et al. (2009) Abazajian K. N., et al., 2009, [ApJS](http://dx.doi.org/10.1088/0067-0049/182/2/543),
    [182, 543](https://ui.adsabs.harvard.edu/abs/2009ApJS..182..543A)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abazajian 等人（2009） Abazajian K. N. 等，2009，[ApJS](http://dx.doi.org/10.1088/0067-0049/182/2/543)，[182,
    543](https://ui.adsabs.harvard.edu/abs/2009ApJS..182..543A)
- en: Abbott et al. (2018) Abbott T. M. C., et al., 2018, [ApJS](http://dx.doi.org/10.3847/1538-4365/aae9f0),
    [239, 18](https://ui.adsabs.harvard.edu/abs/2018ApJS..239...18A)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbott 等人（2018） Abbott T. M. C. 等，2018，[ApJS](http://dx.doi.org/10.3847/1538-4365/aae9f0)，[239,
    18](https://ui.adsabs.harvard.edu/abs/2018ApJS..239...18A)
- en: Abbott et al. (2021) Abbott T. M. C., et al., 2021, arXiv e-prints, [p. arXiv:2101.05765](https://ui.adsabs.harvard.edu/abs/2021arXiv210105765A)
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbott 等人（2021） Abbott T. M. C. 等，2021，arXiv 电子预印本，[p. arXiv:2101.05765](https://ui.adsabs.harvard.edu/abs/2021arXiv210105765A)
- en: Ahumada et al. (2020) Ahumada R., et al., 2020, [ApJS](http://dx.doi.org/10.3847/1538-4365/ab929e),
    [249, 3](https://ui.adsabs.harvard.edu/abs/2020ApJS..249....3A)
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahumada 等人（2020） Ahumada R. 等，2020，[ApJS](http://dx.doi.org/10.3847/1538-4365/ab929e)，[249,
    3](https://ui.adsabs.harvard.edu/abs/2020ApJS..249....3A)
- en: Astropy Collaboration et al. (2013) Astropy Collaboration et al., 2013, [A&A](http://dx.doi.org/10.1051/0004-6361/201322068),
    [558, A33](http://adsabs.harvard.edu/abs/2013A%26A...558A..33A)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Astropy 合作组等（2013） Astropy 合作组等，2013，[A&A](http://dx.doi.org/10.1051/0004-6361/201322068)，[558,
    A33](http://adsabs.harvard.edu/abs/2013A%26A...558A..33A)
- en: Bowen & Vaughan (1973) Bowen I. S., Vaughan A. H., 1973, [Appl. Opt.](http://dx.doi.org/10.1364/AO.12.001430),
    12, 1430
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowen & Vaughan（1973） Bowen I. S.，Vaughan A. H.，1973，[Appl. Opt.](http://dx.doi.org/10.1364/AO.12.001430)，12，1430
- en: 'Bradley et al. (2019) Bradley L., et al., 2019, astropy/photutils: v0.6, [doi:10.5281/zenodo.2533376](http://dx.doi.org/10.5281/zenodo.2533376),
    [https://doi.org/10.5281/zenodo.2533376](https://doi.org/10.5281/zenodo.2533376)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bradley 等人（2019） Bradley L. 等，2019，astropy/photutils: v0.6，[doi:10.5281/zenodo.2533376](http://dx.doi.org/10.5281/zenodo.2533376)，[https://doi.org/10.5281/zenodo.2533376](https://doi.org/10.5281/zenodo.2533376)'
- en: Cai et al. (2020) Cai M. X., Bédorf J., Saletore V. A., Codreanu V., Podareanu
    D., Chaibi A., Qian P. X., 2020, arXiv e-prints, [p. arXiv:2010.11630](https://ui.adsabs.harvard.edu/abs/2020arXiv201011630C)
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人（2020） Cai M. X.，Bédorf J.，Saletore V. A.，Codreanu V.，Podareanu D.，Chaibi
    A.，Qian P. X.，2020，arXiv 电子预印本，[p. arXiv:2010.11630](https://ui.adsabs.harvard.edu/abs/2020arXiv201011630C)
- en: Cheng et al. (2020) Cheng T.-Y., Li N., Conselice C. J., Aragón-Salamanca A.,
    Dye S., Metcalf R. B., 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/staa1015),
    [494, 3750](https://ui.adsabs.harvard.edu/abs/2020MNRAS.494.3750C)
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人（2020） Cheng T.-Y.，Li N.，Conselice C. J.，Aragón-Salamanca A.，Dye S.，Metcalf
    R. B.，2020，[MNRAS](http://dx.doi.org/10.1093/mnras/staa1015)，[494, 3750](https://ui.adsabs.harvard.edu/abs/2020MNRAS.494.3750C)
- en: Chollet et al. (2015) Chollet F., et al., 2015, Keras, [https://keras.io](https://keras.io)
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet 等人（2015） Chollet F. 等，2015，Keras，[https://keras.io](https://keras.io)
- en: Cortese et al. (2017) Cortese L., Catinella B., Janowiecki S., 2017, [ApJL](http://dx.doi.org/10.3847/2041-8213/aa8cc3),
    [848, L7](https://ui.adsabs.harvard.edu/abs/2017ApJ...848L...7C)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cortese 等人（2017） Cortese L.，Catinella B.，Janowiecki S.，2017，[ApJL](http://dx.doi.org/10.3847/2041-8213/aa8cc3)，[848,
    L7](https://ui.adsabs.harvard.edu/abs/2017ApJ...848L...7C)
- en: Durugkar et al. (2016) Durugkar I. P., Gemp I., Mahadevan S., 2016, CoRR, abs/1611.01673
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durugkar 等人（2016） Durugkar I. P.，Gemp I.，Mahadevan S.，2016，CoRR，abs/1611.01673
- en: Flaugher et al. (2015) Flaugher B., Diehl H. T., Honscheid K., et al., 2015,
    [AJ](http://dx.doi.org/10.1088/0004-6256/150/5/150), [150, 150](http://adsabs.harvard.edu/abs/2015AJ....150..150F)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flaugher 等人（2015） Flaugher B.，Diehl H. T.，Honscheid K. 等，2015，[AJ](http://dx.doi.org/10.1088/0004-6256/150/5/150)，[150,
    150](http://adsabs.harvard.edu/abs/2015AJ....150..150F)
- en: Frontera-Pons et al. (2017) Frontera-Pons J., Sureau F., Bobin J., Le Floc’h
    E., 2017, [A&A](http://dx.doi.org/10.1051/0004-6361/201630240), [603, A60](https://ui.adsabs.harvard.edu/abs/2017A&A...603A..60F)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frontera-Pons 等 (2017) Frontera-Pons J., Sureau F., Bobin J., Le Floc’h E.,
    2017, [A&A](http://dx.doi.org/10.1051/0004-6361/201630240), [603, A60](https://ui.adsabs.harvard.edu/abs/2017A&A...603A..60F)
- en: Graff et al. (2014) Graff P., Feroz F., Hobson M. P., Lasenby A., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu642),
    [441, 1741](https://ui.adsabs.harvard.edu/abs/2014MNRAS.441.1741G)
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graff 等 (2014) Graff P., Feroz F., Hobson M. P., Lasenby A., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu642),
    [441, 1741](https://ui.adsabs.harvard.edu/abs/2014MNRAS.441.1741G)
- en: Gunn et al. (2006) Gunn J. E., et al., 2006, [Astron. J.](http://dx.doi.org/10.1086/500975),
    131, 2332
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunn 等 (2006) Gunn J. E., 等, 2006, [Astron. J.](http://dx.doi.org/10.1086/500975),
    131, 2332
- en: Holtzman et al. (2010) Holtzman J., Harrison T., Coughlin J., 2010, [Advances
    in Astronomy](http://dx.doi.org/10.1155/2010/193086), 2010
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman 等 (2010) Holtzman J., Harrison T., Coughlin J., 2010, [天文学进展](http://dx.doi.org/10.1155/2010/193086),
    2010
- en: Hunter (2007) Hunter J. D., 2007, Computing in Science & Engineering, 9, 90
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hunter (2007) Hunter J. D., 2007, 《计算科学与工程》，9, 90
- en: Isola et al. (2016) Isola P., Zhu J., Zhou T., Efros A. A., 2016, CoRR, abs/1611.07004
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola 等 (2016) Isola P., Zhu J., Zhou T., Efros A. A., 2016, CoRR, abs/1611.07004
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivezić 等 (2019) Ivezić Ž., 等, 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
- en: Jia et al. (2020) Jia Z., Yuan B., Wang K., Wu H., Clifford D., Yuan Z., Su
    H., 2020, arXiv e-prints, [p. arXiv:2012.04932](https://ui.adsabs.harvard.edu/abs/2020arXiv201204932J)
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等 (2020) Jia Z., Yuan B., Wang K., Wu H., Clifford D., Yuan Z., Su H., 2020,
    arXiv e-prints, [p. arXiv:2012.04932](https://ui.adsabs.harvard.edu/abs/2020arXiv201204932J)
- en: Jiang et al. (2014a) Jiang L., et al., 2014a, [Astrophys. J. Suppl.](http://dx.doi.org/10.1088/0067-0049/213/1/12),
    213, 12
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2014a) Jiang L., 等, 2014a, [Astrophys. J. Suppl.](http://dx.doi.org/10.1088/0067-0049/213/1/12),
    213, 12
- en: Jiang et al. (2014b) Jiang L., et al., 2014b, [ApJS](http://dx.doi.org/10.1088/0067-0049/213/1/12),
    [213, 12](https://ui.adsabs.harvard.edu/abs/2014ApJS..213...12J)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2014b) Jiang L., 等, 2014b, [ApJS](http://dx.doi.org/10.1088/0067-0049/213/1/12),
    [213, 12](https://ui.adsabs.harvard.edu/abs/2014ApJS..213...12J)
- en: Lample et al. (2017) Lample G., Zeghidour N., Usunier N., Bordes A., Denoyer
    L., Ranzato M., 2017, arXiv e-prints, [p. arXiv:1706.00409](https://ui.adsabs.harvard.edu/abs/2017arXiv170600409L)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample 等 (2017) Lample G., Zeghidour N., Usunier N., Bordes A., Denoyer L.,
    Ranzato M., 2017, arXiv e-prints, [p. arXiv:1706.00409](https://ui.adsabs.harvard.edu/abs/2017arXiv170600409L)
- en: Lanusse et al. (2020) Lanusse F., Mandelbaum R., Ravanbakhsh S., Li C.-L., Freeman
    P., Poczos B., 2020, arXiv e-prints, [p. arXiv:2008.03833](https://ui.adsabs.harvard.edu/abs/2020arXiv200803833L)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lanusse 等 (2020) Lanusse F., Mandelbaum R., Ravanbakhsh S., Li C.-L., Freeman
    P., Poczos B., 2020, arXiv e-prints, [p. arXiv:2008.03833](https://ui.adsabs.harvard.edu/abs/2020arXiv200803833L)
- en: Lin et al. (2021) Lin Q., Fouchez D., Pasquet J., 2021, arXiv e-prints, [p.
    arXiv:2101.07389](https://ui.adsabs.harvard.edu/abs/2021arXiv210107389L)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2021) Lin Q., Fouchez D., Pasquet J., 2021, arXiv e-prints, [p. arXiv:2101.07389](https://ui.adsabs.harvard.edu/abs/2021arXiv210107389L)
- en: Liu et al. (2020) Liu H., Liu J., Tao T., Hou S., Han J., 2020, arXiv e-prints,
    [p. arXiv:2012.14142](https://ui.adsabs.harvard.edu/abs/2020arXiv201214142L)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2020) Liu H., Liu J., Tao T., Hou S., Han J., 2020, arXiv e-prints, [p.
    arXiv:2012.14142](https://ui.adsabs.harvard.edu/abs/2020arXiv201214142L)
- en: Luo et al. (2021) Luo H., Liao G., Hou X., Liu B., Zhou F., Qiu G., 2021, arXiv
    e-prints, [p. arXiv:2101.02384](https://ui.adsabs.harvard.edu/abs/2021arXiv210102384L)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等 (2021) Luo H., Liao G., Hou X., Liu B., Zhou F., Qiu G., 2021, arXiv e-prints,
    [p. arXiv:2101.02384](https://ui.adsabs.harvard.edu/abs/2021arXiv210102384L)
- en: Lupton et al. (2004) Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane
    W., Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116,
    133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lupton 等 (2004) Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane W.,
    Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116, 133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
- en: Masci et al. (2011) Masci J., Meier U., Cireşan D., Schmidhuber J., 2011, in
    Honkela T., Duch W., Girolami M., Kaski S., eds, Artificial Neural Networks and
    Machine Learning – ICANN 2011\. Springer Berlin Heidelberg, Berlin, Heidelberg,
    pp 52–59
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masci 等 (2011) Masci J., Meier U., Cireşan D., Schmidhuber J., 2011, 在 Honkela
    T., Duch W., Girolami M., Kaski S., 主编, 《人工神经网络与机器学习 – ICANN 2011》。Springer Berlin
    Heidelberg, Berlin, Heidelberg, pp 52–59
- en: Maziarka et al. (2019) Maziarka Ł., Pocha A., Kaczmarczyk J., Rataj K., Warchoł
    M., 2019, arXiv e-prints, [p. arXiv:1902.02119](https://ui.adsabs.harvard.edu/abs/2019arXiv190202119M)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maziarka 等 (2019) Maziarka Ł., Pocha A., Kaczmarczyk J., Rataj K., Warchoł M.,
    2019, arXiv e-prints, [p. arXiv:1902.02119](https://ui.adsabs.harvard.edu/abs/2019arXiv190202119M)
- en: Moriwaki et al. (2020) Moriwaki K., Shirasaki M., Yoshida N., 2020, arXiv e-prints,
    [p. arXiv:2010.00809](https://ui.adsabs.harvard.edu/abs/2020arXiv201000809M)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moriwaki 等 (2020) Moriwaki K., Shirasaki M., Yoshida N., 2020, arXiv e-prints,
    [p. arXiv:2010.00809](https://ui.adsabs.harvard.edu/abs/2020arXiv201000809M)
- en: 'Oliphant (2006) Oliphant T., 2006, NumPy: A guide to NumPy, USA: Trelgol Publishing'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Oliphant (2006) Oliphant T., 2006, NumPy: A guide to NumPy, USA: Trelgol Publishing'
- en: Osakabe et al. (2020) Osakabe T., Tanaka M., Kinoshita Y., Kiya H., 2020, arXiv
    e-prints, [p. arXiv:2012.00287](https://ui.adsabs.harvard.edu/abs/2020arXiv201200287O)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osakabe 等 (2020) Osakabe T., Tanaka M., Kinoshita Y., Kiya H., 2020, arXiv e-prints,
    [p. arXiv:2012.00287](https://ui.adsabs.harvard.edu/abs/2020arXiv201200287O)
- en: Padmanabhan & Loeb (2020) Padmanabhan H., Loeb A., 2020, arXiv e-prints, [p.
    arXiv:2002.01489](https://ui.adsabs.harvard.edu/abs/2020arXiv200201489P)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Padmanabhan 和 Loeb (2020) Padmanabhan H., Loeb A., 2020, arXiv e-prints, [p.
    arXiv:2002.01489](https://ui.adsabs.harvard.edu/abs/2020arXiv200201489P)
- en: Patel & Upla (2019) Patel H., Upla K. P., 2019, in Arora C., Mitra K., eds,
    Computer Vision Applications. Springer Singapore, Singapore, pp 115–128
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel 和 Upla (2019) Patel H., Upla K. P., 2019, 在 Arora C., Mitra K., eds, 计算机视觉应用。Springer
    Singapore, Singapore, pp 115–128
- en: Perarnau et al. (2016) Perarnau G., van de Weijer J., Raducanu B., Álvarez J. M.,
    2016, arXiv e-prints, [p. arXiv:1611.06355](https://ui.adsabs.harvard.edu/abs/2016arXiv161106355P)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perarnau 等 (2016) Perarnau G., van de Weijer J., Raducanu B., Álvarez J. M.,
    2016, arXiv e-prints, [p. arXiv:1611.06355](https://ui.adsabs.harvard.edu/abs/2016arXiv161106355P)
- en: Price-Whelan et al. (2018) Price-Whelan A. M., et al., 2018, [AJ](http://dx.doi.org/10.3847/1538-3881/aabc4f),
    [156, 123](https://ui.adsabs.harvard.edu/#abs/2018AJ....156..123T)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Price-Whelan 等 (2018) Price-Whelan A. M., 等, 2018, [AJ](http://dx.doi.org/10.3847/1538-3881/aabc4f),
    [156, 123](https://ui.adsabs.harvard.edu/#abs/2018AJ....156..123T)
- en: Radford et al. (2016) Radford A., Metz L., Chintala S., 2016, Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks ([arXiv:1511.06434](http://arxiv.org/abs/1511.06434))
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2016) Radford A., Metz L., Chintala S., 2016, 无监督表征学习与深度卷积生成对抗网络
    ([arXiv:1511.06434](http://arxiv.org/abs/1511.06434))
- en: Ralph et al. (2019) Ralph N. O., et al., 2019, [Publications of the Astronomical
    Society of the Pacific](http://dx.doi.org/10.1088/1538-3873/ab213d), 131, 108011
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ralph 等 (2019) Ralph N. O., 等, 2019, [Publications of the Astronomical Society
    of the Pacific](http://dx.doi.org/10.1088/1538-3873/ab213d), 131, 108011
- en: 'Regier et al. (2015a) Regier J., McAuliffe J., Prabhat M., 2015a, in NIPS Workshop:
    Advances in Approximate Bayesian Inference.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Regier 等 (2015a) Regier J., McAuliffe J., Prabhat M., 2015a, 在 NIPS Workshop:
    Advances in Approximate Bayesian Inference.'
- en: Regier et al. (2015b) Regier J., Miller A., McAuliffe J., Adams R., Hoffman
    M., Lang D., Schlegel D., Prabhat 2015b, arXiv e-prints, [p. arXiv:1506.01351](https://ui.adsabs.harvard.edu/abs/2015arXiv150601351R)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Regier 等 (2015b) Regier J., Miller A., McAuliffe J., Adams R., Hoffman M., Lang
    D., Schlegel D., Prabhat 2015b, arXiv e-prints, [p. arXiv:1506.01351](https://ui.adsabs.harvard.edu/abs/2015arXiv150601351R)
- en: Schawinski et al. (2018) Schawinski K., Turp M. D., Zhang C., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201833800),
    [616, L16](https://ui.adsabs.harvard.edu/abs/2018A&A...616L..16S)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schawinski 等 (2018) Schawinski K., Turp M. D., Zhang C., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201833800),
    [616, L16](https://ui.adsabs.harvard.edu/abs/2018A&A...616L..16S)
- en: Shen et al. (2017) Shen H., George D., Huerta E. A., Zhao Z., 2017, arXiv e-prints,
    [p. arXiv:1711.09919](https://ui.adsabs.harvard.edu/abs/2017arXiv171109919S)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 (2017) Shen H., George D., Huerta E. A., Zhao Z., 2017, arXiv e-prints,
    [p. arXiv:1711.09919](https://ui.adsabs.harvard.edu/abs/2017arXiv171109919S)
- en: Shirasaki et al. (2019) Shirasaki M., Yoshida N., Ikeda S., Oogi T., Nishimichi
    T., 2019, arXiv e-prints, [p. arXiv:1911.12890](https://ui.adsabs.harvard.edu/abs/2019arXiv191112890S)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shirasaki 等 (2019) Shirasaki M., Yoshida N., Ikeda S., Oogi T., Nishimichi T.,
    2019, arXiv e-prints, [p. arXiv:1911.12890](https://ui.adsabs.harvard.edu/abs/2019arXiv191112890S)
- en: Smith & Geach (2019) Smith M. J., Geach J. E., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz2886),
    [490, 4985](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.4985S)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 和 Geach (2019) Smith M. J., Geach J. E., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz2886),
    [490, 4985](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.4985S)
- en: Spindler et al. (2020) Spindler A., Geach J. E., Smith M. J., 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/staa3670),
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spindler 等 (2020) Spindler A., Geach J. E., Smith M. J., 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/staa3670)
- en: Storey-Fisher et al. (2020) Storey-Fisher K., Huertas-Company M., Ramachandra
    N., Lanusse F., Leauthaud A., Luo Y., Huang S., 2020, arXiv e-prints, [p. arXiv:2012.08082](https://ui.adsabs.harvard.edu/abs/2020arXiv201208082S)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storey-Fisher 等 (2020) Storey-Fisher K., Huertas-Company M., Ramachandra N.,
    Lanusse F., Leauthaud A., Luo Y., Huang S., 2020, arXiv e-prints, [p. arXiv:2012.08082](https://ui.adsabs.harvard.edu/abs/2020arXiv201208082S)
- en: Ullmo et al. (2020) Ullmo M., Decelle A., Aghanim N., 2020, arXiv e-prints,
    [p. arXiv:2011.05244](https://ui.adsabs.harvard.edu/abs/2020arXiv201105244U)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ullmo 等 (2020) Ullmo M., Decelle A., Aghanim N., 2020, arXiv e-prints, [p. arXiv:2011.05244](https://ui.adsabs.harvard.edu/abs/2020arXiv201105244U)
- en: Van Der Walt et al. (2011) Van Der Walt S., Colbert S. C., Varoquaux G., 2011,
    Computing in Science & Engineering, 13, 22
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Der Walt 等（2011）Van Der Walt S., Colbert S. C., Varoquaux G., 2011，计算科学与工程，13，22
- en: Van der Walt et al. (2014) Van der Walt S., Schönberger J. L., Nunez-Iglesias
    J., Boulogne F., Warner J. D., Yager N., Gouillart E., Yu T., 2014, PeerJ, 2,
    e453
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van der Walt 等（2014）Van der Walt S., Schönberger J. L., Nunez-Iglesias J., Boulogne
    F., Warner J. D., Yager N., Gouillart E., Yu T., 2014，PeerJ，2，e453
- en: Villar et al. (2020a) Villar V. A., Cranmer M., Contardo G., Ho S., Yao-Yu Lin
    J., 2020a, arXiv e-prints, [p. arXiv:2010.11194](https://ui.adsabs.harvard.edu/abs/2020arXiv201011194V)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villar 等（2020a）Villar V. A., Cranmer M., Contardo G., Ho S., Yao-Yu Lin J.,
    2020a，arXiv 预印本，[p. arXiv:2010.11194](https://ui.adsabs.harvard.edu/abs/2020arXiv201011194V)
- en: Villar et al. (2020b) Villar V. A., et al., 2020b, [ApJ](http://dx.doi.org/10.3847/1538-4357/abc6fd),
    [905, 94](https://ui.adsabs.harvard.edu/abs/2020ApJ...905...94V)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villar 等（2020b）Villar V. A., 等，2020b，[ApJ](http://dx.doi.org/10.3847/1538-4357/abc6fd)，[905,
    94](https://ui.adsabs.harvard.edu/abs/2020ApJ...905...94V)
- en: Vincent et al. (2008) Vincent P., Larochelle H., Bengio Y., Manzagol P.-A.,
    2008, in Proceedings of the 25th International Conference on Machine Learning.
    ICML ’08. Association for Computing Machinery, New York, NY, USA, p. 1096–1103,
    [doi:10.1145/1390156.1390294](http://dx.doi.org/10.1145/1390156.1390294), [https://doi.org/10.1145/1390156.1390294](https://doi.org/10.1145/1390156.1390294)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent 等（2008）Vincent P., Larochelle H., Bengio Y., Manzagol P.-A., 2008，发表于第25届国际机器学习会议论文集。ICML
    ’08。计算机协会，纽约，NY，USA，第1096–1103页，[doi:10.1145/1390156.1390294](http://dx.doi.org/10.1145/1390156.1390294)，[https://doi.org/10.1145/1390156.1390294](https://doi.org/10.1145/1390156.1390294)
- en: Virtanen et al. (2019) Virtanen P., et al., 2019, arXiv e-prints, [p. arXiv:1907.10121](https://ui.adsabs.harvard.edu/abs/2019arXiv190710121V)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Virtanen 等（2019）Virtanen P., 等，2019，arXiv 预印本，[p. arXiv:1907.10121](https://ui.adsabs.harvard.edu/abs/2019arXiv190710121V)
- en: Wang et al. (2019) Wang Y., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz2907),
    [490, 5722](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.5722W)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019）Wang Y., 等，2019，[MNRAS](http://dx.doi.org/10.1093/mnras/stz2907)，[490,
    5722](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.5722W)
- en: Wang et al. (2020) Wang Y.-C., Xie Y.-B., Zhang T.-J., Huang H.-C., Zhang T.,
    Liu K., 2020, arXiv e-prints, [p. arXiv:2005.10628](https://ui.adsabs.harvard.edu/abs/2020arXiv200510628W)
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020）Wang Y.-C., Xie Y.-B., Zhang T.-J., Huang H.-C., Zhang T., Liu K.,
    2020，arXiv 预印本，[p. arXiv:2005.10628](https://ui.adsabs.harvard.edu/abs/2020arXiv200510628W)
- en: 'Waskom et al. (2017) Waskom M., et al., 2017, mwaskom/seaborn: v0.8.1 (September
    2017), [doi:10.5281/zenodo.883859](http://dx.doi.org/10.5281/zenodo.883859), [https://doi.org/10.5281/zenodo.883859](https://doi.org/10.5281/zenodo.883859)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Waskom 等（2017）Waskom M., 等，2017，mwaskom/seaborn: v0.8.1 (2017年9月)，[doi:10.5281/zenodo.883859](http://dx.doi.org/10.5281/zenodo.883859)，[https://doi.org/10.5281/zenodo.883859](https://doi.org/10.5281/zenodo.883859)'
- en: Zeiler (2012) Zeiler M. D., 2012, arXiv e-prints, [p. arXiv:1212.5701](https://ui.adsabs.harvard.edu/abs/2012arXiv1212.5701Z)
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler (2012) Zeiler M. D., 2012，arXiv 预印本，[p. arXiv:1212.5701](https://ui.adsabs.harvard.edu/abs/2012arXiv1212.5701Z)
- en: Zhou Wang et al. (2004) Zhou Wang Bovik A. C., Sheikh H. R., Simoncelli E. P.,
    2004, IEEE Transactions on Image Processing, 13, 600
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou Wang 等（2004）Zhou Wang Bovik A. C., Sheikh H. R., Simoncelli E. P., 2004，IEEE
    图像处理学报，13，600
- en: Zhu et al. (2017) Zhu J.-Y., Park T., Isola P., Efros A. A., 2017, arXiv e-prints,
    [p. arXiv:1703.10593](https://ui.adsabs.harvard.edu/abs/2017arXiv170310593Z)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2017）Zhu J.-Y., Park T., Isola P., Efros A. A., 2017，arXiv 预印本，[p. arXiv:1703.10593](https://ui.adsabs.harvard.edu/abs/2017arXiv170310593Z)
- en: Appendix A Additional Image Samples
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附加图像样本
- en: 'Here, we show additional examples of SDSS, DES, and reconstructed images from
    the validation dataset similar to Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). They were randomly selected to
    provide examples of objects with a variety of types, brightnesses, and extents.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了来自验证数据集的 SDSS、DES 和重建图像的附加示例，与图 [3](#S4.F3 "图 3 ‣ 4.1 定性分析 ‣ 4 结果与分析
    ‣ Survey2Survey：一种用于跨调查图像映射的深度学习生成模型方法") 类似。它们是随机选择的，以提供各种类型、亮度和范围的对象示例。
- en: 'The rows in Figs. [14](#A1.F14 "Figure 14 ‣ Appendix A Additional Image Samples
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") - [18](#A1.F18 "Figure 18 ‣ Appendix A Additional Image Samples ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping") represent
    the following quantities:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [14](#A1.F14 "图 14 ‣ 附录 A 附加图像样本 ‣ Survey2Survey：一种用于跨调查图像映射的深度学习生成模型方法")
    - [18](#A1.F18 "图 18 ‣ 附录 A 附加图像样本 ‣ Survey2Survey：一种用于跨调查图像映射的深度学习生成模型方法") 中的行表示以下量：
- en: (A)
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (A)
- en: SDSS representation
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SDSS 表示
- en: (B)
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (B)
- en: DES representation
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DES 表示
- en: (C)
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (C)
- en: CAE reconstruction
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CAE 重建
- en: (D)
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (D)
- en: CycleGAN reconstruction
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CycleGAN 重建
- en: (E)
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (E)
- en: CAE residuals (CAE - DES)
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CAE 残差 (CAE - DES)
- en: (F)
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (F)
- en: CycleGAN residuals (CycleGAN - DES)
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CycleGAN 残差 (CycleGAN - DES)
- en: (G)
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (G)
- en: CAE gain (CAE - SDSS)
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CAE 增益 (CAE - SDSS)
- en: (H)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (H)
- en: CycleGAN gain (CycleGAN - SDSS),
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CycleGAN 增益 (CycleGAN - SDSS),
- en: 'while in Fig. [19](#A1.F19 "Figure 19 ‣ Appendix A Additional Image Samples
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), they represent the following quantities:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [19](#A1.F19 "Figure 19 ‣ Appendix A Additional Image Samples ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping") 中，它们表示以下量：'
- en: (A)
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (A)
- en: SDSS representation
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SDSS 表示
- en: (B)
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (B)
- en: CAE reconstruction
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CAE 重建
- en: (C)
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (C)
- en: CycleGAN reconstruction
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CycleGAN 重建
- en: (D)
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (D)
- en: CAE gain (CAE - SDSS)
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CAE 增益 (CAE - SDSS)
- en: (E)
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (E)
- en: CycleGAN gain (CycleGAN - SDSS),
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CycleGAN 增益 (CycleGAN - SDSS),
- en: 'Note that to increase visibility, the residual and gain images were artificially
    enhanced with a power law transform (see Figs. [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") and [4](#S4.F4 "Figure 4 ‣ 4.1
    Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping") for details).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，为了提高可见性，残差和增益图像经过了功率法变换的人工增强（详情见图 [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") 和 [4](#S4.F4 "Figure 4 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")）。'
- en: '![Refer to caption](img/5ba6c01e0eaedf3c04019db15b936099.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ba6c01e0eaedf3c04019db15b936099.png)'
- en: 'Figure 14: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 来自验证数据集的源图像、目标图像和重建图像的附加示例；格式与图 [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") 相同。请注意，行 E、F、G 和 H 中的图像经过了清晰度增强。'
- en: '![Refer to caption](img/5ef12532696ee42dec60ea4030bbc2b9.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ef12532696ee42dec60ea4030bbc2b9.png)'
- en: 'Figure 15: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: 来自验证数据集的源图像、目标图像和重建图像的附加示例；格式与图 [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") 相同。请注意，行 E、F、G 和 H 中的图像经过了清晰度增强。'
- en: '![Refer to caption](img/8fec0db5fed2038ef8433542dd24f962.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8fec0db5fed2038ef8433542dd24f962.png)'
- en: 'Figure 16: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: 来自验证数据集的源图像、目标图像和重建图像的附加示例；格式与图 [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") 相同。请注意，行 E、F、G 和 H 中的图像经过了清晰度增强。'
- en: '![Refer to caption](img/0d24e20651b57b75c8155b2288c3f39f.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d24e20651b57b75c8155b2288c3f39f.png)'
- en: 'Figure 17: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '图 17: 来自验证数据集的源图像、目标图像和重建图像的附加示例；格式与图 [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") 相同。请注意，行 E、F、G 和 H 中的图像经过了清晰度增强。'
- en: '![Refer to caption](img/4dadfcaa733131eab6fce6820784f1d8.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4dadfcaa733131eab6fce6820784f1d8.png)'
- en: 'Figure 18: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18：验证数据集中的源图像、目标图像和重建图像的附加示例；格式与图 [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") 中的相同。请注意，E、F、G 和 H 行的图像已进行清晰度增强。'
- en: '![Refer to caption](img/48def07828a454ec4fb41f2edfe6b51f.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/48def07828a454ec4fb41f2edfe6b51f.png)'
- en: 'Figure 19: Additional examples of source, target, and reconstructed images
    from the external dataset; the formatting is the same as in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows D and E were enhanced for clarity.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '图 19：外部数据集中的源图像、目标图像和重建图像的附加示例；格式与图 [4](#S4.F4 "Figure 4 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") 中的相同。请注意，D 和 E 行的图像已进行清晰度增强。'
