- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:30:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:30:28'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2408.15276] A Survey of Deep Learning for Group-level Emotion Recognition'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2408.15276] 深度学习在群体级情感识别中的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.15276](https://ar5iv.labs.arxiv.org/html/2408.15276)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.15276](https://ar5iv.labs.arxiv.org/html/2408.15276)
- en: A Survey of Deep Learning for Group-level Emotion Recognition
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在群体级情感识别中的调查
- en: 'Xiaohua Huang, Jinke Xu, Wenming Zheng, , Qirong Mao,Abhinav Dhall X. Huang
    is with the School of Computer Engineering, Nanjing Institute of Technology, China,
    the Key Laboratory of Child Development and Learning Science (Southeast University),
    Ministry of Education, Southeast University, Nanjing 210096, China, and also with
    Research Center for Learning Science, Southeast University, China. (email: xiaohuahwang@gmail.com)J.
    Xu is with the School of Computer Engineering, Nanjing Institute of Technology,
    China. (email: y00450220246@njit.edu.cn)W. Zheng is with the Key Laboratory of
    Child Development and Learning Science (Southeast University), Ministry of Education,
    Southeast University, Nanjing 210096, China and also with the School of Biological
    Science and Medical Engineering, Southeast University, Nanjing 210096, Jiangsu,
    China. (E-mail: wenming_zheng@seu.edu.cn)Q. Mao is with the School of Computer
    Science and Communication Engineering, Jiangsu University, Zhenjiang, Jiangsu,
    China. (E-mail: mao_qr@ujs.edu.cn)A. Dhall is with College of Science & Engineering,
    Flinders University, Adelaide, Australia and Indian Institute of Technology Ropar,
    Hussainpur, Rupnagar 140001, Punjab. (E-mail: abhinav.dhall@flinders.edu.au)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '黄晓华，徐金科，郑文铭，毛奇荣，阿宾纳夫·达尔。黄晓华现任南京工业大学计算机工程学院教授，中国南京210096；东南大学儿童发展与学习科学重点实验室（教育部），中国南京210096；东南大学学习科学研究中心，中国。
    (电子邮件: xiaohuahwang@gmail.com) 徐金科现任南京工业大学计算机工程学院教授，中国。 (电子邮件: y00450220246@njit.edu.cn)
    郑文铭现任东南大学儿童发展与学习科学重点实验室（教育部），中国南京210096，及东南大学生物科学与医学工程学院教授，中国江苏南京210096。 (电子邮件:
    wenming_zheng@seu.edu.cn) 毛奇荣现任江苏大学计算机科学与通信工程学院教授，中国江苏镇江。 (电子邮件: mao_qr@ujs.edu.cn)
    阿宾纳夫·达尔现任弗林德斯大学科学与工程学院教授，澳大利亚阿德莱德，及印度理工学院罗帕尔校区，印度旁遮普140001。 (电子邮件: abhinav.dhall@flinders.edu.au)'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the advancement of artificial intelligence (AI) technology, group-level
    emotion recognition (GER) has emerged as an important area in analyzing human
    behavior. Early GER methods are primarily relied on handcrafted features. However,
    with the proliferation of Deep Learning (DL) techniques and their remarkable success
    in diverse tasks, neural networks have garnered increasing interest in GER. Unlike
    individual’s emotion, group emotions exhibit diversity and dynamics. Presently,
    several DL approaches have been proposed to effectively leverage the rich information
    inherent in group-level image and enhance GER performance significantly. In this
    survey, we present a comprehensive review of DL techniques applied to GER, proposing
    a new taxonomy for the field cover all aspects of GER based on DL. The survey
    overviews datasets, the deep GER pipeline, and performance comparisons of the
    state-of-the-art methods past decade. Moreover, it summarizes and discuss the
    fundamental approaches and advanced developments for each aspect. Furthermore,
    we identify outstanding challenges and suggest potential avenues for the design
    of robust GER systems. To the best of our knowledge, thus survey represents the
    first comprehensive review of deep GER methods, serving as a pivotal references
    for future GER research endeavors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能（AI）技术的进步，群体级情感识别（GER）已成为分析人类行为的重要领域。早期的GER方法主要依赖于手工特征。然而，随着深度学习（DL）技术的普及及其在各种任务中的显著成功，神经网络在GER中引起了越来越多的关注。与个体情感不同，群体情感表现出多样性和动态性。目前，已有几种DL方法被提出，以有效利用群体级图像中的丰富信息，并显著提升GER性能。在这项调查中，我们提供了对应用于GER的DL技术的全面回顾，提出了一种新的分类法，涵盖了基于DL的GER领域的所有方面。调查概述了数据集、深度GER流程以及过去十年最先进方法的性能比较。此外，还总结和讨论了每个方面的基本方法和先进发展。此外，我们还识别了突出的挑战，并建议了设计稳健的GER系统的潜在途径。据我们所知，这项调查代表了对深度GER方法的首次全面回顾，为未来的GER研究工作提供了关键参考。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Group-level Emotion Recognition, Deep Learning, Feature representation learning,
    Attention mechanism, Fusion scheme.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 群体级情感识别、深度学习、特征表示学习、注意力机制、融合方案。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Emotio exhibits a profound influence on human perception, attention, memory,
    and decision-making, directly impacting both physical and mental well-being [[1](#bib.bib1)].
    Consequently, the comprehension and perception of emotions not only enhance interpersonal
    communication but also implicitly contribute to the regulation of human physical
    health. With the evolution of big data technology, the broadening of application
    scenarios, and the progress in artificial intelligence technology, group-level
    emotion recognition (GER) has become a focal point for researchers [[2](#bib.bib2),
    [3](#bib.bib3)]. Unlike individual-level emotion recognition, which analyzes the
    emotions of individuals through facial expressions, speech, and postures, GER
    focuses on identifying the collective emotions of a group of people. Existing
    GER technology integrates diverse information, encompassing facial expressions,
    postures, social interactions, etc., to predict the emotions of a group. Taking
    facial expression data as an example, GER typically involves three key steps:
    first, given an image, detecting and extracting information such as faces and
    postures from images or videos; secondly, employing handcrafted descriptor or
    neural networks to extract facial features and other relevant information; finally,
    inputting these features as sequential data into another model like recurrent
    neural network to classify the group-level emotion. However, GER presents three
    key challenges. Firstly, the groups and contexts involved in GER may exhibit diversity
    and complexity. Secondly, the labeling of group-level emotion demands more nuanced
    considerations for the emotions expressed by the primary group, introducing additional
    complexities compared to individual-level emotion labeling. Lastly, due to the
    involvement of multiple individuals and diverse contexts, the recognition process
    becomes more complex than individual-level emotion recognition. Despite these
    challenges, GER remains an essential and formidable research area. It would be
    utilized to analyze emotion changes of a group of people, detecting abnormal behaviors
    and potential dangers in a timely manner in surveillance videos, or understanding
    students’ learning status in collaborative learning [[4](#bib.bib4)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 情感对人类的感知、注意力、记忆和决策有深远的影响，直接影响到身心健康[[1](#bib.bib1)]。因此，对情感的理解和感知不仅增强了人际沟通，还隐含地有助于调节人类的身体健康。随着大数据技术的发展、应用场景的拓展和人工智能技术的进步，群体级情感识别（GER）已成为研究人员关注的重点[[2](#bib.bib2),
    [3](#bib.bib3)]。与通过面部表情、语音和姿势分析个人情感的个人级情感识别不同，GER专注于识别一群人的集体情感。现有的GER技术整合了各种信息，包括面部表情、姿势、社交互动等，以预测群体的情感。以面部表情数据为例，GER通常涉及三个关键步骤：首先，给定图像，从图像或视频中检测和提取面部和姿势等信息；其次，使用手工制作的描述符或神经网络提取面部特征和其他相关信息；最后，将这些特征作为序列数据输入到另一个模型，如递归神经网络，以对群体级情感进行分类。然而，GER面临三个主要挑战。首先，GER涉及的群体和背景可能表现出多样性和复杂性。其次，群体级情感的标注需要对主要群体表达的情感进行更细致的考虑，与个人级情感标注相比引入了额外的复杂性。最后，由于涉及多个个体和多样化的背景，识别过程变得比个人级情感识别更复杂。尽管面临这些挑战，GER仍然是一个重要且艰巨的研究领域。它可用于分析一群人的情感变化，及时检测监控视频中的异常行为和潜在危险，或了解学生在协作学习中的学习状态[[4](#bib.bib4)]。
- en: This article provides a comprehensive survey of deep learning approaches for
    GER. Different from existing review [[5](#bib.bib5)], this paper elaborates on
    the methods of GER with a unique focus on deep learning architecture, providing
    insights into the current state and technical challenges associated with deep
    learning method in GER. In the beginning,we commence by providing an in-depth
    analysis of groups and emotions from a social perspective, offering a concise
    concept for GER. Subsequently, we present a thorough description of the image-based
    and video-based group-level emotion databases currently available for GER. Moreover,
    we explore recently developed deep learning methods for GER and scrutinize their
    technical challenges. In conclusion, we discuss the development trends of GER,
    offering valuable insights and guidance for future research and applications.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了关于GER的深度学习方法的全面调查。与现有的综述[[5](#bib.bib5)]不同，本文详细阐述了GER的方法，独特地聚焦于深度学习架构，提供了有关深度学习方法在GER中的现状和技术挑战的见解。开始时，我们从社会角度对群体和情感进行深入分析，为GER提供了简明的概念。随后，我们详细描述了目前可用于GER的基于图像和视频的群体级情感数据库。此外，我们探讨了最近开发的GER深度学习方法，并审视了其技术挑战。最后，我们讨论了GER的发展趋势，为未来的研究和应用提供有价值的见解和指导。
- en: II Group-level emotion
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 群体级情感
- en: Emotions, traditionally viewed as individual-level phenomena in common parlance,
    have increasingly received attention in the field of social psychology as being
    potentially group-level [[6](#bib.bib6)]. Niedenthal and Brauer offer a broad
    definition of group-level emotions, characterizing them as emotions experienced
    by individuals on behalf of a group with which they identify [[7](#bib.bib7)].
    This perspective suggests that group-level emotions are inherently more complex
    than their individual-level counterparts. Adopting both sociological and psychological
    perspectives, we will proceed by elucidating the concepts of groups and emotions
    before providing a precise definition of group emotions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 情感在传统上被视为个体级现象，但在社会心理学领域，它们越来越受到关注，作为潜在的群体级现象[[6](#bib.bib6)]。Niedenthal和Brauer提供了群体级情感的广泛定义，将其描述为个体代表他们认同的群体所体验的情感[[7](#bib.bib7)]。这一观点表明，群体级情感本质上比个体级情感更为复杂。我们将从社会学和心理学角度出发，阐明群体和情感的概念，然后提供对群体情感的准确定义。
- en: The fundamental disparity between group-level emotion and individual-level emotion
    arises from the distinction between the concepts of a group and an individual.
    While an individual pertains to an singular, independent entity, a group transcends
    mere aggregation, constituting a social phenomenon that amalgamates individuals
    into a collective entity. Various perspectives and definitions of the concept
    of a group have been proposed in scholarly literature [[8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)]. Shaw et al. [[8](#bib.bib8)] defines a group
    as comprising two or more individuals engaged in mutual interaction and influence,
    emphasizing the significance of interaction among group members. Szilagi and Wallance [[9](#bib.bib9)]
    characterize a group as a collection of two or more individuals who interact and
    depend on each other to achieve a common goal. Additionally, Barron et al. [[10](#bib.bib10)]
    further elaborate on this definition by conceptualizing a group as individuals
    connected by some kind of bond, exhibiting varying degrees of cohesion. Moreover,
    Dasgupta et al. [[11](#bib.bib11)] describe a group as a group of people closely
    interconnected in some manner. According to these studies, a group consists of
    multiple individuals with a shared objective and direct or indirect interactions
    between them during a certain period of time. Thus, only when these conditions
    are met can a combination of multiple individuals be deemed a group.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 群体层面的情感与个体层面的情感之间的根本差异源于群体与个体概念的不同。个体指的是一个单一的、独立的实体，而群体超越了单纯的聚合，构成了一个将个体融合成集体实体的社会现象。学术文献中提出了各种关于群体概念的观点和定义[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]。Shaw等人[[8](#bib.bib8)]定义群体为两个或更多个体进行相互互动和影响的集合，强调群体成员之间互动的重要性。Szilagi和Wallance[[9](#bib.bib9)]将群体描述为两个或更多个体互动并依赖于彼此以实现共同目标的集合。此外，Barron等人[[10](#bib.bib10)]进一步阐述了这个定义，将群体概念化为通过某种纽带连接的个体，表现出不同程度的凝聚力。而Dasgupta等人[[11](#bib.bib11)]将群体描述为以某种方式紧密相互联系的人群。根据这些研究，群体由多个个体组成，这些个体在一定时间内有共同目标和直接或间接的互动。因此，只有当这些条件得到满足时，多个个体的组合才可以被视为一个群体。
- en: Numerous scholars have investigated the concept of emotion within the sociological
    and psychological field. Schachter et al. [[12](#bib.bib12)] proposed that emotion
    encompasses both a physiological arousal state and a cognitive state adapted to
    this physiological state. Kleinginna et al. [[13](#bib.bib13)] conducted a comprehensive
    review categorizing 92 different definitions of emotion from various literature
    sources. Ekman in [[14](#bib.bib14)] suggested that “emotions evolve from the
    adaptive value of human beings in handling basic life tasks”, indicating that
    emotions arise as adaptive responses to different situations encountered during
    social activities. Averill regards emotion as a complex of impulsive motivation
    in higher cognition, involving various psychological processes and physiological
    responses [[15](#bib.bib15)]. Cabanac defines emotion as any psychological experience
    with high intensity and high pleasure content, emphasizing the relationship between
    emotion and psychological experience [[16](#bib.bib16)]. Barrett posits that emotions
    are constructs of the brain’s interpretation of external and internal stimuli [[17](#bib.bib17)].
    Scherer views emotion as a biopsychological phenomenon resulting from the interaction
    of specific neural systems and physiological responses, cognitive assessments,
    and social-cultural factors [[18](#bib.bib18)]. Therefore, emotions can be understood
    as physiological and psychological responses to stimuli in our environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 许多学者在社会学和心理学领域探讨了情感的概念。Schachter等人[[12](#bib.bib12)]提出情感包含生理激发状态和适应这一生理状态的认知状态。Kleinginna等人[[13](#bib.bib13)]进行了全面的综述，将92种不同的情感定义从各种文献来源中进行分类。Ekman在[[14](#bib.bib14)]中建议“情感源于人类在处理基本生活任务中的适应性价值”，这表明情感作为对社会活动中遇到的不同情况的适应性反应而产生。Averill认为情感是高级认知中冲动动机的复杂体，涉及各种心理过程和生理反应[[15](#bib.bib15)]。Cabanac定义情感为任何具有高强度和高愉悦内容的心理体验，强调情感与心理体验之间的关系[[16](#bib.bib16)]。Barrett认为情感是大脑对外部和内部刺激的解读构建[[17](#bib.bib17)]。Scherer视情感为一种生物心理现象，由特定神经系统和生理反应、认知评估及社会文化因素的互动产生[[18](#bib.bib18)]。因此，情感可以被理解为对环境刺激的生理和心理反应。
- en: Based on the foregoing elucidation of group and emotion, group-level emotion
    denotes the physiological and psychological responses elicited by multiple interacting
    individuals over a defined period. Scholars have proffered diverse definitions
    of group-level emotion in their studies. In the beginning, Hatfield et al. [[19](#bib.bib19)]
    characterize group-level emotion as the process whereby group members reciprocally
    influence each other through emotional contagion and empathy, culminating in emotional
    synchronization and consistency. Essentially, group-level emotion entails the
    reciprocal transmission and influence of emotions among individuals, leading to
    emotional consistency. Furthermore, group emotion is perceived as the emotional
    state propagated among members of a social group, cultivated and diffused through
    social interaction and shared experiences [[20](#bib.bib20)]. Barsäde and Gibson
    stressed the importance for researchers in the social science community to approach
    group-level emotions from both a “top-down approach” and a “bottom-up approach” [[21](#bib.bib21)].
    A “top-down approach” suggests that emotion exhibited by group is represented
    at the group level and is felt by individual members, while the ”bottom-up approach”
    highlights the unique compositional effects of individual-level group member emotions.
    According to the framework [[21](#bib.bib21)], Kelly and Barsäde further proposed
    that group-level emotion comprises affective compositional effects and affective
    context [[22](#bib.bib22)]. In essence, group-level emotion emerges from a combination
    of individual-level affective factors posed by group members and group-level factors
    that shape the emotional experience of the group. Additionally, Barsäde and Gibson
    further explore group-level emotion as the emotional state disseminated and shared
    among members of an organization [[23](#bib.bib23)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前述的组和情感的阐述，组级情感指的是在定义的时间段内，由多个互动个体引发的生理和心理反应。学者们在研究中提供了不同的组级情感定义。一开始，Hatfield等人[[19](#bib.bib19)]
    将组级情感描述为组成员通过情感传染和同理心互相影响的过程，最终实现情感的同步和一致。本质上，组级情感涉及个体之间情感的相互传递和影响，从而导致情感一致。此外，组情感被认为是通过社会互动和共享经历在社会群体成员之间传播和扩散的情感状态[[20](#bib.bib20)]。Barsäde
    和 Gibson 强调社会科学领域的研究人员需要从“自上而下的方法”和“自下而上的方法”来研究组级情感[[21](#bib.bib21)]。“自上而下的方法”表明，组所表现出的情感在组层面上表现，并被个体成员感受到，而“自下而上的方法”则强调个体层面的组成员情感的独特组成效应。根据该框架[[21](#bib.bib21)]，Kelly
    和 Barsäde 进一步提出，组级情感包括情感组成效应和情感背景[[22](#bib.bib22)]。本质上，组级情感源于组成员的个体层面情感因素与塑造组情感体验的组级因素的组合。此外，Barsäde
    和 Gibson 进一步探讨了组级情感作为在组织成员之间传播和共享的情感状态[[23](#bib.bib23)]。
- en: III Group-level emotion dataset
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 组级情感数据集
- en: 'The rise of social media platforms has led to a surge in the volume of uploaded
    photos and videos, driving advancements in big data technologies and affective
    computing domains, especially GER. In recent years, numerous group-level emotion
    datasets are established, attracting attention from researchers in the domains
    of affective computing and computer vision. However, the quality of annotation
    on images and videos plays an critical role in determining the efficacy of GER
    models. Thus, this section aims to provide an exhaustive examination of existing
    group-level emotion datasets, classifying them into two main types: image-based
    and video-based types.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体平台的兴起导致上传的照片和视频量激增，推动了大数据技术和情感计算领域，尤其是 GER 的进展。近年来，许多组级情感数据集被建立，吸引了情感计算和计算机视觉领域研究者的关注。然而，图像和视频的注释质量在决定
    GER 模型的有效性方面起着关键作用。因此，本节旨在对现有的组级情感数据集进行详尽的审查，将其分为两种主要类型：基于图像的和基于视频的类型。
- en: III-A Image-based datasets
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于图像的数据集
- en: In contrast to individual-level emotion datasets, group-level emotion datasets
    necessitate annotation for a group. Image-based datasets began to emerge since
    2013, with notable representations including MultiEmoVA [[24](#bib.bib24)], Happiness
    Image database (HAPPEI) [[25](#bib.bib25)], Group-level Affect database (GAF) [[2](#bib.bib2),
    [26](#bib.bib26), [27](#bib.bib27)], Group cohesion dataset [[28](#bib.bib28)],
    GroupEmoW [[29](#bib.bib29)], and SiteGroEmo [[3](#bib.bib3)]. Among these database,
    HAPPEI, GAF, and the Group cohesion dataset, which were utilized in the EmotiW
    sub-challenge, have received significant attention and adoption by researchers
    in the fields of computer vision and affective computing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与个体级情感数据集相比，群体级情感数据集需要对一个群体进行注释。基于图像的数据集自 2013 年以来开始出现，其中包括 MultiEmoVA [[24](#bib.bib24)]、Happiness
    Image 数据库 (HAPPEI) [[25](#bib.bib25)]、Group-level Affect 数据库 (GAF) [[2](#bib.bib2),
    [26](#bib.bib26), [27](#bib.bib27)]、Group cohesion 数据集 [[28](#bib.bib28)]、GroupEmoW
    [[29](#bib.bib29)] 和 SiteGroEmo [[3](#bib.bib3)]。在这些数据库中，HAPPEI、GAF 和在 EmotiW
    子挑战中使用的 Group cohesion 数据集，受到了计算机视觉和情感计算领域研究人员的广泛关注和采用。
- en: Mou et al. [[24](#bib.bib24)] introduced a multi-dimensional group emotion image
    dataset, namely MultiEmoVA. This dataset was primarily constructed by scouring
    various social media platforms for real-life photos. Initially, 400 color images
    were collected, and after manually filtering images with ambiguous emotional expressions,
    250 images meeting the criteria were remained. Furthermore, these images were
    categorized into positive, neutral, and negative. Additionally, Mou et al. also
    incorporated arousal-level annotation into the dataset, providing more explicit
    representations of the intensity for each emotion category.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Mou 等人 [[24](#bib.bib24)] 引入了一个多维群体情感图像数据集，即 MultiEmoVA。这个数据集主要通过搜集各种社交媒体平台上的现实照片来构建。最初收集了
    400 张彩色图像，经过人工筛选去除情感表达模糊的图像后，剩下符合标准的图像有 250 张。此外，这些图像被分类为积极、中性和消极。此外，Mou 等人还在数据集中加入了唤醒水平的注释，为每个情感类别的强度提供了更明确的表示。
- en: 'The HAPPEI database contains images captured from social media platform and
    categorizes group-level emotions into neutral, small smile, big smile, smile,
    big laugh, and thrilled, totaling 2,638 images. In contrast, the GAF datbase delineates
    three emotion categories along the valence dimension: positive, neutral, and negative.
    The data collection process involved web search using keywords corresponding to
    various scenarios, such as weddings, birthday parties, and sports events, etc.,
    to obtain images depicting group-level emotions in those contexts. Subsequently,
    these images were annotated by 2 to 3 experts in affective computing domain. The
    GAF database has undergone three iterations, namely GAF [[26](#bib.bib26)], GAF2.0 [[2](#bib.bib2)],
    and GAF3.0 [[27](#bib.bib27)]. The initial version, GAF includes 504 images [[26](#bib.bib26)],
    which is relatively small in scale and has been limited usage by researcher for
    evaluating the performance of deep GER models. However, the subsequent iterations,
    GAF2.0 and GAF3.0, witnessed a substantial increase in data size, featuring 6,467 [[2](#bib.bib2)]
    and 17,172 images [[27](#bib.bib27)], respectively.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: HAPPEI 数据库包含从社交媒体平台捕获的图像，并将群体级情感分为中性、小笑容、大笑容、微笑、大笑和兴奋，共计 2,638 张图像。相比之下，GAF
    数据库在情感价值维度上划分为三个情感类别：积极、中性和消极。数据收集过程通过使用与各种场景（如婚礼、生日派对、体育赛事等）相关的关键词进行网络搜索，以获取描绘这些情境中的群体情感的图像。随后，这些图像由
    2 到 3 名情感计算领域的专家进行注释。GAF 数据库经历了三次迭代，分别是 GAF [[26](#bib.bib26)]、GAF2.0 [[2](#bib.bib2)]
    和 GAF3.0 [[27](#bib.bib27)]。初始版本 GAF 包含 504 张图像 [[26](#bib.bib26)]，规模相对较小，研究人员在评估深度
    GER 模型性能时使用有限。然而，后续迭代的 GAF2.0 和 GAF3.0 数据量大幅增加，分别包含 6,467 张图像 [[2](#bib.bib2)]
    和 17,172 张图像 [[27](#bib.bib27)]。
- en: In addition to emotion category, the cohesiveness of a group serves as a crucial
    indicator of the emotional state, structure and success of a group of individuals.
    Group Cohesion database was derived from GAF3.0 by adding cohesion labels [[28](#bib.bib28)].
    Each image in this database was annotated with a cohesion score ranging from 0
    to 3 by five annotators, where 0 represents no cohesion and 3 means strong cohesion.
    This database was utilized in the Emotiw2019 challenge [[30](#bib.bib30)], with
    9,300 images allocated for training, 4,244 images for validation and 2,899 images
    for testing purposes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了情感类别之外，群体的凝聚力作为情感状态、结构和成功的关键指标。Group Cohesion 数据库是通过添加凝聚力标签从 GAF3.0 派生而来的[[28](#bib.bib28)]。该数据库中的每张图片都由五位标注者标注了一个从
    0 到 3 的凝聚力评分，其中 0 代表无凝聚力，3 代表强凝聚力。该数据库在 Emotiw2019 挑战中得到了应用[[30](#bib.bib30)]，其中
    9,300 张图片用于训练，4,244 张图片用于验证，2,899 张图片用于测试。
- en: Besides the above-mentioned databases, another databases have recently proposed
    by Guo et al. [[29](#bib.bib29)] and Wang et al. [[3](#bib.bib3)], that is GroupEmoW
    and SiteGroEmo. GroupEmoW was created with a stringent criterion mandating that
    each image contains 2 to 9 individuals engaged in a specific activity, thereby
    forming distinct groups. According to this criterion, they collected 15,894 images
    from the internet, creating a diverse dataset with varying image resolutions and
    in-the-wild. Subsequently, these images were categorized into positive, neutral,
    and negative. On the other hand, the SiteGroEmo dataset diverges from existing
    database by capturing image across tourism scenes worldwide. This dataset not
    only contains rich geographic information and scene variations but also randomly
    captures the facial and body movements of individuals at a specific moment. Comprising
    a total of 10,034 images, the dataset is labeled with valence to denote emotions,
    specifically categorized as negative, neutral, and positive.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述数据库，Guo 等人[[29](#bib.bib29)]和 Wang 等人[[3](#bib.bib3)] 最近提出了另外两个数据库，即 GroupEmoW
    和 SiteGroEmo。GroupEmoW 是在严格标准下创建的，要求每张图片包含 2 到 9 人参与特定活动，从而形成不同的群体。根据这一标准，他们从互联网收集了
    15,894 张图片，创建了一个具有不同图像分辨率和自然环境的多样化数据集。随后，这些图片被分为正面、中性和负面。另一方面，SiteGroEmo 数据集通过捕捉全球旅游场景的图片而与现有数据库有所不同。该数据集不仅包含丰富的地理信息和场景变化，还随机捕捉到个人在特定时刻的面部和身体动作。数据集总共有
    10,034 张图片，用于标注情感的价值，包括负面、中性和正面。
- en: It is important to note that the aforementioned image-based datasets predominantly
    rely on internet keywords searches for data collection. While this method may
    expedite the establishment of satisfactory datasets, the existing datasets, with
    the exception of the MultiEmoVA database, solely employ valence-level annotations
    for images, lacking arousal-level annotations and more specific emotion categories
    such as anger and surprise. This limitation may arise from the diverse expression
    exhibited by participants in a group, making it challenging to annotate group-level
    images with a comprehensive emotion taxonomy. Additionally, manual annotation
    may introduce discrepancies in labeling due to cultural differences. Moreover,
    many images in these datasets may suffer from quality issues such as poor lighting,
    incomplete capture of facial expressions, or obstructions obscuring certain individuals.
    Finally, given their static nature, these images lack information about emotional
    dynamics. In the domain of emotion recognition research, scholars have emphasized
    that dynamic changes in facial expressions can offer crucial clues for both humans
    and computers to discern emotions or emotional processes [[31](#bib.bib31)]. Therefore,
    these uncontrollable circumstances and the absence of dynamic information may
    have a discernible impact on the accuracy of GER based on image.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，前述的基于图像的数据集主要依赖互联网关键词搜索进行数据采集。虽然这种方法可能加快满意数据集的建立，但现有的数据集（除了MultiEmoVA数据库）仅使用了图像的情感价值层面标注，缺乏激发层面的标注以及如愤怒和惊讶等更具体的情感类别。这种局限性可能源于参与者在群体中表现出的多样化表情，使得用全面的情感分类系统对群体图像进行标注变得困难。此外，人工标注可能由于文化差异引入标签不一致的问题。此外，这些数据集中的许多图像可能存在质量问题，如光线不足、面部表情捕捉不完整或遮挡某些个体等。最后，由于这些图像是静态的，它们缺乏情感动态信息。在情感识别研究领域，学者们强调，面部表情的动态变化可以为人类和计算机辨别情感或情感过程提供关键线索[[31](#bib.bib31)]。因此，这些不可控的情况和缺乏动态信息可能对基于图像的情感识别准确性产生明显影响。
- en: III-B Video-based datasets
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于视频的数据集
- en: 'In contrast to image-based datasets, video-based datasets not only capture
    the temporal dynamics of emotional expressions but also provide additional contextual
    information, facilitating a more comprehensive and accurate portrayal of changes
    in emotional states. However, due to the demanding collection and annotation process,
    only two video-based datasets have emerged recently since 2019: the VGAF dataset
    introduced by Sharma et al. [[32](#bib.bib32)] and the GECV dataset by Quach et
    al. [[33](#bib.bib33)].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于图像的数据集相比，基于视频的数据集不仅捕捉了情感表达的时间动态，还提供了额外的上下文信息，从而有助于更全面、准确地描绘情感状态的变化。然而，由于数据采集和标注过程要求高，自2019年以来，只有两个基于视频的数据集新近出现：Sharma等人介绍的VGAF数据集[[32](#bib.bib32)]和Quach等人提出的GECV数据集[[33](#bib.bib33)]。
- en: 'The VAGF dataset comprises videos sourced from the YouTube platform, each featuring
    a varying number of individuals forming groups of different sizes. The dataset
    is partitioned into training, validation, and test sets, encompassing 2,661, 766,
    and 756 samples, respectively. Alongside valence annotation, the dataset includes
    cohesion metrics among individuals in the group. The corresponding dataset has
    also been used in EmotiW2023 [[34](#bib.bib34)]. Conversely, the GECV dataset
    contains videos captured in leisure and crowded scenes, amounting to a total of
    627 videos. This dataset is further subdivided into three subsets: GEVC-SingleImg,
    GEVC-GroupImg, and GEVC-GroupVid. The latter two subsets are tailored to capture
    group emotions more effectively by showcasing various various group behaviors
    across different scenarios.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: VAGF数据集包含来自YouTube平台的视频，每个视频中展示了不同数量的个体组成的各种规模的群体。该数据集被划分为训练集、验证集和测试集，分别包含2,661、766和756个样本。除了情感价值标注，数据集还包括群体中个体之间的凝聚度指标。该数据集还被用于EmotiW2023[[34](#bib.bib34)]。相反，GECV数据集包含在休闲和拥挤场景中拍摄的视频，总计627个视频。该数据集进一步细分为三个子集：GEVC-SingleImg、GEVC-GroupImg和GEVC-GroupVid。后两个子集旨在通过展示不同场景中的各种群体行为，更有效地捕捉群体情感。
- en: While video-based databases offer richer semantic and contextual information
    compared to image-based databases, facilitating more discriminant criteria for
    data annotation, they are sourced from media platforms featuring complex scenes
    that often depict real-life scenarios. Nonetheless, they present limitations such
    as the absence of physiological signals akin to multi-modal emotion recognition,
    thereby posing challenges in data collection.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于视频的数据库提供比基于图像的数据库更丰富的语义和上下文信息，有助于更具辨别性的注释标准，但它们来源于包含复杂场景的媒体平台，这些场景通常描绘了真实生活中的情境。然而，它们也存在一些限制，例如缺乏类似于多模态情感识别的生理信号，从而在数据收集上带来了挑战。
- en: III-C Dataset summary
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 数据集总结
- en: The specific comparisons of the existing group emotion datasets are presented
    in Table [I](#S3.T1 "TABLE I ‣ III-C Dataset summary ‣ III Group-level emotion
    dataset ‣ A Survey of Deep Learning for Group-level Emotion Recognition"). Despite
    all datasets collecting images or videos from diverse real-world scenarios, their
    sample size is comparably small when compare with comprehensive datasets like
    AffectNet [[35](#bib.bib35)]. This limited data size impedes the robust learning
    of group-level features. Recently, in the domain of micro-expression recognition,
    a composite dataset consolidating various micro-expression databases has become
    popular [[36](#bib.bib36)]. This approach corroborates the generalization capacity
    of the method across datasets with disparate characteristics, mitigating the issue
    of data scarcity. Such a strategy holds promise for GER by potentially augmenting
    data volumes, particularly for video-based datasets, and enhancing the generalization
    ability of GER methods.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现有群体情感数据集的具体比较见表[I](#S3.T1 "TABLE I ‣ III-C Dataset summary ‣ III Group-level
    emotion dataset ‣ A Survey of Deep Learning for Group-level Emotion Recognition")。尽管所有数据集都从各种真实场景中收集了图像或视频，但与如
    AffectNet [[35](#bib.bib35)] 这样的大型数据集相比，它们的样本量相对较小。这一数据量限制阻碍了群体级特征的稳健学习。最近，在微表情识别领域，整合各种微表情数据库的复合数据集变得流行[[36](#bib.bib36)]。这种方法证实了在特征各异的数据集上方法的泛化能力，缓解了数据稀缺的问题。这种策略有望通过增加数据量，特别是视频数据集的数量，提高
    GER 方法的泛化能力。
- en: State-of-the-art approaches, particularly those showcased in the EmotiW challenge,
    primarily undergo evaluation using image-based databases. All databases adopt
    an annotation strategy categorizing images into three valence-level categories,
    as certain nuanced emotions such as fear and contempt pose challenges in data
    collection, resulting in limited samples for these categories, which are insufficient
    for robust learning. As seen from Table [I](#S3.T1 "TABLE I ‣ III-C Dataset summary
    ‣ III Group-level emotion dataset ‣ A Survey of Deep Learning for Group-level
    Emotion Recognition"), except SiteGroEmo, each database has a balanced distribution
    of data across each class. Furthermore, in practical experiments, apart from the
    HAPPEI and MultiEmoVA databases, other databases follow an official protocol where
    the train, validation, and test sets remain strictly fixed without random validation.
    Such rigid dataset partitioning may not be facilitate accurate assessments of
    GER method performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的方法，特别是在 EmotiW 挑战中展示的方法，主要通过基于图像的数据库进行评估。所有数据库采用将图像分类为三种情感强度级别的注释策略，因为某些细微的情感如恐惧和蔑视在数据收集上存在挑战，导致这些类别的样本有限，不足以进行稳健的学习。从表[I](#S3.T1
    "TABLE I ‣ III-C Dataset summary ‣ III Group-level emotion dataset ‣ A Survey
    of Deep Learning for Group-level Emotion Recognition")可见，除 SiteGroEmo 外，每个数据库在每个类别中数据分布均衡。此外，在实际实验中，除了
    HAPPEI 和 MultiEmoVA 数据库外，其他数据库遵循官方协议，其中训练、验证和测试集保持严格固定，不进行随机验证。这种严格的数据集划分可能不利于准确评估
    GER 方法的性能。
- en: 'TABLE I: Group-level emotion database.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：群体级情感数据库。
- en: '| Dataset | Type | Sample size | Category | Task | Protocol |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 样本量 | 类别 | 任务 | 协议 |'
- en: '| HAPPEI [[26](#bib.bib26)]* | Image | 2,638 | Neural (92), Small smile (147),
    Large smile (774), Small laugh (1256), Large laugh (331), Thrilled (38) | Regression
    | 4-fold cross validation train (1500), val (1138), test (496) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| HAPPEI [[26](#bib.bib26)]* | 图像 | 2,638 | 神经（92）、小笑容（147）、大笑容（774）、小笑声（1256）、大笑声（331）、兴奋（38）
    | 回归 | 4折交叉验证训练（1500），验证（1138），测试（496） |'
- en: '| MultiEmoVA [[24](#bib.bib24)] | Image | 250 | High-pos (46), Medium-pos (64),
    High-neg (31), medium-neg (27), low-neg (10), neu (72) | Classification | 5-fold
    cross validation |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| MultiEmoVA [[24](#bib.bib24)] | 图像 | 250 | 高正向（46）、中正向（64）、高负向（31）、中负向（27）、低负向（10）、中性（72）
    | 分类 | 5折交叉验证 |'
- en: '| GAF2.0 [[2](#bib.bib2)] | Image | 6,467 | pos (2,356), neu (2,092), neg (2,019)
    | Classification | train (3,630), val (2,068), test (772) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| GAF2.0 [[2](#bib.bib2)] | 图像 | 6,467 | pos (2,356), neu (2,092), neg (2,019)
    | 分类 | 训练集 (3,630), 验证集 (2,068), 测试集 (772) |'
- en: '| GAF3.0 [[27](#bib.bib27)] | Image | 17,172 | pos (6,553), neu (5,364), neg
    (5,256) | Classification | train (9,836), val (4346), test (3011) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| GAF3.0 [[27](#bib.bib27)] | 图像 | 17,172 | pos (6,553), neu (5,364), neg (5,256)
    | 分类 | 训练集 (9,836), 验证集 (4,346), 测试集 (3,011) |'
- en: '| Group Cohesion [[30](#bib.bib30), [28](#bib.bib28), [37](#bib.bib37)] | Image
    | 16,433 | [0, 3] | Regression | train (9,300), val (4,244), test (2,899) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Group Cohesion [[30](#bib.bib30), [28](#bib.bib28), [37](#bib.bib37)] | 图像
    | 16,433 | [0, 3] | 回归 | 训练集 (9,300), 验证集 (4,244), 测试集 (2,899) |'
- en: '| SiteGroEmo [[3](#bib.bib3)] | Image | 10,034 | pos (4,660), neu (4,355),
    neg (1,019) | Classification | train (6,096), val (1,972), test (1,966) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SiteGroEmo [[3](#bib.bib3)] | 图像 | 10,034 | pos (4,660), neu (4,355), neg
    (1,019) | 分类 | 训练集 (6,096), 验证集 (1,972), 测试集 (1,966) |'
- en: '| GroupEmoW [[29](#bib.bib29)] | Image | 15,894 | pos (6,636), neu (4,947),
    neg (4,311) | Classification | train (11,127), val (3,178), test (1,589) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| GroupEmoW [[29](#bib.bib29)] | 图像 | 15,894 | pos (6,636), neu (4,947), neg
    (4,311) | 分类 | 训练集 (11,127), 验证集 (3,178), 测试集 (1,589) |'
- en: '| GECV [[33](#bib.bib33)] | Video | 627 | pos (204), neu (221), neg (202) |
    Classification | train (90%), test (10%) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| GECV [[33](#bib.bib33)] | 视频 | 627 | pos (204), neu (221), neg (202) | 分类
    | 训练集 (90%), 测试集 (10%) |'
- en: '| VGAF [[38](#bib.bib38)] | Video | 4,183 | pos (1,104), neu (1,203), neg (1,120)
    | Classification | train (2,661), val (766), test (756) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| VGAF [[38](#bib.bib38)] | 视频 | 4,183 | pos (1,104), neu (1,203), neg (1,120)
    | 分类 | 训练集 (2,661), 验证集 (766), 测试集 (756) |'
- en: '| pos: Positive; neg: Negative; neu: Neutral |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| pos: 积极; neg: 消极; neu: 中性 |  |'
- en: '| val: validation. |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| val: 验证集。 |  |'
- en: '| *For HAPPEI database, we only offer the database volume of each class for
    train and validation sets. |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| *对于HAPPEI数据库，我们只提供每个类别的训练集和验证集的数据库容量。 |  |'
- en: IV Input modality
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 输入方式
- en: Recognizing group-level emotions poses significant challenges due to the diversity
    in group dynamic, individual emotion expressions, and limited data availability,
    deep learning (DL) methods, while promising, exhibit varied performance depending
    on the various modalities utilized. In this section, we describe the diverse information
    cues based on static image or video sequence utilized for GER and outline their
    respectively strengths and limitations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 识别群体级情感面临重大挑战，因为群体动态、个体情感表达的多样性以及数据可用性的限制。虽然深度学习（DL）方法具有潜力，但其性能因所使用的不同模式而异。在本节中，我们将详细描述用于GER的静态图像或视频序列的各种信息提示，并概述它们各自的优势和局限性。
- en: IV-A Static image
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 静态图像
- en: Due to the abundant availability of facial images online such as AffectNet database [[35](#bib.bib35)],
    numerous existing studies in FER are conducted on static images. Leveraging the
    efficiency demonstrated by FER with static images, numerous researchers have extended
    their efforts to GER, incorporating various additional cues such as face, scene,
    pose, even objects. Convolutional Neural Networks (CNNs), notably VGG, ResNet
    and their variants, are commonly employed to analyze static images in GER research.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在线面部图像的丰富资源，如AffectNet数据库 [[35](#bib.bib35)]，许多现有的面部情感识别（FER）研究都基于静态图像。利用FER在静态图像上展示的效率，许多研究者已将其工作扩展到群体情感识别（GER），融入了诸如面部、场景、姿势甚至物体等各种附加信息。卷积神经网络（CNNs），尤其是VGG、ResNet及其变体，通常被用来分析GER研究中的静态图像。
- en: Cue aggregation as input. Given the flexible nature of group sizes, GER faces
    the challenge of effectively aggregating features from multiple individuals within
    a group to derive an overall emotional state. Existing methods are broadly categorized
    into two approaches. The first category involves averaging or weighted sum all
    individuals’ emotion scores [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)], which are typically outputted by a classifier such as Support
    Vector Machine (SVM). For example, Rassadin et al. [[39](#bib.bib39)] normalized
    detected faces and fed them into VGGFace and ImageNet. They constructed an ensemble
    of four Random Forest classifier trained on the features outputted by VGGFace,
    ImageNet, and landmark features. Finally, they employed a weighted sum approach
    to fuse the score outputted by the random forest classifiers. The second approach
    employs some machine learning algorithms such as bag-of-words and clustering to
    aggregate all individuals’ features into a single feature vector. Balaji et al. [[43](#bib.bib43)],
    for example, utilized CNNs to extract face information. Subsequently, Fisher vector
    and VLAD encoding techniques were applied to compress all individual features
    in the image, yielding bottom-up features capable of representing group emotions.
    This method effectively compresses features, reduces computational complexity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示聚合作为输入。鉴于组大小的灵活性，GER面临的挑战是如何有效地聚合来自组内多个个体的特征，以推导出总体情感状态。现有的方法大致分为两类。第一类包括对所有个体的情感评分进行平均或加权求和[[39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42)]，这些评分通常由如支持向量机（SVM）等分类器输出。例如，Rassadin等[[39](#bib.bib39)]对检测到的面孔进行归一化，并将其输入到VGGFace和ImageNet中。他们构建了一个由四个随机森林分类器组成的集成模型，这些分类器是在VGGFace、ImageNet和地标特征输出的特征上训练的。最终，他们采用加权求和方法来融合随机森林分类器输出的评分。第二类方法采用一些机器学习算法，如词袋模型和聚类，将所有个体的特征聚合成一个单一的特征向量。例如，Balaji等[[43](#bib.bib43)]利用CNNs提取面部信息。随后，应用了Fisher向量和VLAD编码技术来压缩图像中所有个体的特征，从而产生能够表示群体情感的自下而上的特征。这种方法有效地压缩了特征，减少了计算复杂性。
- en: Multimodality as input. Group are often considered as “emotional entities and
    a rich source of varied manifestations of affect”. Earlier discussions by Barsäde
    and Gibson in [[23](#bib.bib23)] emphasized the necessity for GER researchers
    to adopt both a “top-down approach” and a “bottom-up approach”. Consequently,
    GER research has concentrated on integrating both bottom-up and top-down components.
    Typically, bottom-up refers to individual emotions, while top-down encompasses
    contextual factors such as background information in an image. Recognizing the
    benefits of incorporating both bottom-up and top-down components, several studies [[44](#bib.bib44),
    [43](#bib.bib43), [45](#bib.bib45), [46](#bib.bib46), [24](#bib.bib24), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)] have explored to fuse features
    from various cues in group-level images. For example, in Garg’s work [[46](#bib.bib46)],
    a deep convolutional neural network is utilized to identify facial expressions
    within an image, while a Bayesian network leverages scene descriptors to extract
    visual features of the image content, thereby inferring the overall emotion of
    the image. In addition to these modalities, body information [[24](#bib.bib24),
    [51](#bib.bib51)] and object [[49](#bib.bib49), [50](#bib.bib50)] have also been
    incorporated into certain studies. In summary, GER research has commonly explored
    one or more cues extracted from face, pose/skeleton information, object, and scene
    context. Cues combination offers the advantage of enabling successful recognition
    of group-level emotions in the challenging environment when one cue is lacking.
    However, even employing multiple cues, a key concern arises regarding how to effectively
    establish connection among these cues to enhance the robustness of GER in real-world
    scenarios.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态作为输入。群体通常被视为“情感实体和多样化情感表现的丰富来源”。Barsäde 和 Gibson 早期的讨论[[23](#bib.bib23)]强调了GER研究人员采纳“自上而下的方法”和“自下而上的方法”的必要性。因此，GER研究集中在整合自下而上和自上而下的组件上。通常，自下而上指的是个体情感，而自上而下则包括图像中的背景信息等上下文因素。认识到整合自下而上和自上而下组件的好处，若干研究[[44](#bib.bib44),
    [43](#bib.bib43), [45](#bib.bib45), [46](#bib.bib46), [24](#bib.bib24), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)]探讨了如何融合来自不同线索的特征，以分析群体级图像。例如，在Garg的工作[[46](#bib.bib46)]中，利用深度卷积神经网络来识别图像中的面部表情，同时使用贝叶斯网络利用场景描述符提取图像内容的视觉特征，从而推断图像的整体情感。除了这些模态外，身体信息[[24](#bib.bib24),
    [51](#bib.bib51)]和对象[[49](#bib.bib49), [50](#bib.bib50)]也被纳入了一些研究中。总之，GER研究通常探讨从面部、姿态/骨架信息、对象和场景上下文中提取的一个或多个线索。线索的组合可以在缺少某一线索时提供成功识别群体级情感的优势。然而，即使使用多个线索，如何有效建立这些线索之间的连接以增强GER在现实场景中的鲁棒性仍然是一个关键问题。
- en: IV-B Dynamic image sequence
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 动态图像序列
- en: As emotions are temporal in nature, such automatic tools in environments like
    factories, companies, and offices can help identifying interventions to maintain
    a healthy work culture. Facial expressions, being dynamic cues, evolve and change,
    revealing their effective signals over time. The visual information captured in
    videos plays a pivotal role in discerning the emotions depicted within them [[52](#bib.bib52)].
    The temporal variations across video frames provides additional information to
    be exploited, albeit encoding these variations introduces complexity to emotion
    recognition. Training a network to comprehend the overall affect of a group of
    people shown across frames poses challenges. In this subsection, we delineate
    various dynamic inputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于情感具有时间性，因此在工厂、公司和办公室等环境中的自动工具可以帮助识别干预措施，以维护健康的工作文化。面部表情作为动态线索，会随时间变化，揭示其有效信号。视频中捕捉的视觉信息在辨别其所描绘的情感中起着关键作用[[52](#bib.bib52)]。视频帧之间的时间变化提供了额外的信息，尽管编码这些变化会增加情感识别的复杂性。训练网络理解在各帧中展示的群体情感整体性面临挑战。在这一小节中，我们详细描述了各种动态输入。
- en: Temporal information as input. Temporal information encapsulate the dynamics
    of an entire video sequence in a single instance. The temporal information, modeled
    by using algorithms like LSTM, has been successfully employed in GER to model
    scene dynamics and appearance in a video [[32](#bib.bib32)]. Similarly, active
    image encapsulated spatial and temporal information from video sequences into
    a single instance by estimating and accumulating changes in each pixel component.
    Sun et al. [[53](#bib.bib53)] utilized temporal segment networks to extract RGB
    information, optical flow frame, and warped optical flow frame for each video,
    incorporating a temporal shift module to model dynamic scene information. Quach et
    al. [[33](#bib.bib33)] introduced a fusion mechanism called Non Volume Preserving
    Fusion (NVPF) to better model spatial relationships between facial emotions in
    each frame, effectively addressing emotional ambiguity caused by insufficient
    facial resolution or undetectable emotions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将时间信息作为输入。时间信息将整个视频序列的动态封装成一个实例。使用LSTM等算法建模的时间信息已成功应用于GER中，以建模视频中的场景动态和外观[[32](#bib.bib32)]。类似地，主动图像通过估计和累积每个像素组件的变化，将视频序列的空间和时间信息封装成一个实例。Sun等人[[53](#bib.bib53)]
    利用时间段网络提取每个视频的RGB信息、光流帧和变形光流帧，并结合时间位移模块来建模动态场景信息。Quach等人[[33](#bib.bib33)] 引入了一种称为非体积保持融合（NVPF）的融合机制，以更好地建模每帧中面部情感的空间关系，有效解决了由于面部分辨率不足或情感不可检测造成的情感模糊。
- en: Frame aggregation as input. Dynamic image sequence collected in the wild often
    include complex scene background. Petrova et al. [[54](#bib.bib54)] designed a
    method based on VGG-19 framework for each frame, capturing global emotions, followed
    by using score averaging and accumulation across all frames. Li et al. [[55](#bib.bib55)]
    proposed leveraging multi-task learning theory to aggregate frame features, while
    Liu et al. [[56](#bib.bib56)] employed four aggregation methods including maximum,
    minimum, average, and standard deviation to consolidate all individual’s face
    feature in a group image.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将帧聚合作为输入。在自然环境中收集的动态图像序列通常包含复杂的场景背景。Petrova等人[[54](#bib.bib54)] 设计了一种基于VGG-19框架的方法来处理每一帧，捕捉全球情感，然后对所有帧进行评分平均和累积。Li等人[[55](#bib.bib55)]
    提出了利用多任务学习理论来聚合帧特征，而Liu等人[[56](#bib.bib56)] 则采用了包括最大值、最小值、平均值和标准差在内的四种聚合方法来整合组图像中的所有个体面部特征。
- en: Multimodality as input. In analyzing group affect, audio features play a crucial
    role alongside facial image, as relying solely on facial expressions may lead
    to inaccuracies in estimating overall group affect. Pitch, speech rate, and duration,
    etc. have been found relevant to affect analysis. In group settings, these features
    are vital for distinguishing between situations like arguments and discussions,
    where the visual model may falter. Visual-audio fusion models have been proposed
    to enhance visual-based models [[32](#bib.bib32), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [55](#bib.bib55)]. For example, Wang et al. [[57](#bib.bib57)]
    introduced a network called K-injection audiovisual network, which employs a multi-head
    cross-attention mechanism to jointly model audio and video data, integrating previous
    emotion knowledge to improve the model’s generalization ability. Recent study
    indicate that human gesture can convery emotion [[59](#bib.bib59)]. Several researchers
    have incorporated human gesture features fused with scene and face cues for video-level
    GER [[53](#bib.bib53), [56](#bib.bib56)]. For example, Sun et al. [[53](#bib.bib53)]
    utilized CenterNet for human detection and pose estimation, followed by ResNetSt
    for extracting body feature. For prediction, the average probability of each frame’s
    prediction was calculated, yielding the video’s class prediction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态作为输入。在分析群体情感时，音频特征与面部图像一样起着关键作用，因为仅仅依赖面部表情可能会导致对整体群体情感的估计不准确。音高、语速和时长等被发现与情感分析相关。在群体设置中，这些特征对于区分争论和讨论等情况至关重要，因为视觉模型可能会出现不足。为了增强基于视觉的模型，已经提出了视觉-音频融合模型[[32](#bib.bib32),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [55](#bib.bib55)]。例如，Wang等[[57](#bib.bib57)]介绍了一种名为K-injection的视听网络，该网络采用了多头交叉注意机制来联合建模音频和视频数据，整合了先前的情感知识以提高模型的泛化能力。最近的研究表明，人类手势可以传达情感[[59](#bib.bib59)]。一些研究者将人类手势特征与场景和面部线索融合用于视频级别的GER[[53](#bib.bib53),
    [56](#bib.bib56)]。例如，Sun等[[53](#bib.bib53)]利用CenterNet进行人体检测和姿势估计，然后使用ResNetSt提取身体特征。对于预测，计算了每帧预测的平均概率，从而得出视频的类别预测。
- en: IV-C Discussion
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 讨论
- en: The primary challenge faced by these methods lies in establishing relationships
    between modalities and effectively fuse them. As evident from the discussed methods,
    GER has increasingly emphasized mining the dynamic information present in videos.
    This trend is expected to drive further advancements in Recurrent Neural Networks
    (RNN) and its derivatives in group-level emotion recognition. Additionally, these
    studies have ventured beyong single-modal information, paving the way for research
    to better comprehend and leverage multimodal information. This broader perspective
    holds promise for enrich the understanding and utilization of diverse sources
    of information in group-level emotion recognition tasks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法面临的主要挑战在于建立模态之间的关系并有效融合它们。正如讨论的方法所示，GER越来越强调挖掘视频中的动态信息。这一趋势预计将推动递归神经网络（RNN）及其衍生物在群体情感识别中的进一步发展。此外，这些研究已经超越了单模态信息，为研究提供了更好地理解和利用多模态信息的途径。这种更广泛的视角有望丰富对群体情感识别任务中多样化信息来源的理解和利用。
- en: V Deep Networks for GER
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 深度网络用于GER
- en: Since the inception of artificial neurons in the 1940s, deep learning has been
    undergone extensive exploration and implementation. Evolving from single-layer
    perceptrons to multi-layer neural networks, Convolutional neural networks (CNNs),
    Recurrent neural networks (RNNs), Cascade networks, Graph convolutional networks
    (GCNs), attention mechanisms, and beyond, deep learning has witnessed rapid evolution.
    So far, various deep learning approaches have emerged to discern the collective
    emotions of a group of people, leveraging diverse information such as facial expression,
    gestures, social interactions, and more. Figure [1](#S5.F1 "Figure 1 ‣ V Deep
    Networks for GER ‣ A Survey of Deep Learning for Group-level Emotion Recognition")
    illustrates literature spanning database, emotion competition, method, and survey
    categories of academic papers employing deep learning based methods for GER over
    the past decade. It is noteworthy that the publication trend has exhibited a noticeable
    increase, especially attributed to the EmotiW competition. In this section, we
    delve into the approaches from the perspectives of specialized blocks, network
    architecture, fusion stage and scheme, and loss function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 自20世纪40年代人工神经元的出现以来，深度学习经历了广泛的探索和实施。从单层感知器到多层神经网络，卷积神经网络（CNN）、递归神经网络（RNN）、级联网络、图卷积网络（GCN）、注意机制等，深度学习见证了迅速的演变。至今，各种深度学习方法已经出现，用于识别群体的集体情感，利用面部表情、手势、社交互动等多种信息。图
    [1](#S5.F1 "Figure 1 ‣ V Deep Networks for GER ‣ A Survey of Deep Learning for
    Group-level Emotion Recognition") 展示了过去十年中采用深度学习方法进行 GER 的数据库、情感竞赛、方法和调查类别的学术文献。值得注意的是，出版趋势出现了明显增长，特别是由于
    EmotiW 竞赛。在本节中，我们将从专用块、网络架构、融合阶段和方案以及损失函数的角度深入探讨这些方法。
- en: '![Refer to caption](img/f799ea049767d520e0608ac165231e40.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f799ea049767d520e0608ac165231e40.png)'
- en: 'Figure 1: The overview of deep learning based technical papers and survey papers
    for group-level emotion recognition. Viewed in color is BEST.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：深度学习技术论文和调查论文的概述，针对群体级情感识别。以彩色查看效果最佳。
- en: V-A Basic Network Block
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 基础网络块
- en: '![Refer to caption](img/69cdf7089073e36dbf3f80f233f0403c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/69cdf7089073e36dbf3f80f233f0403c.png)'
- en: (a) CNN
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CNN
- en: '![Refer to caption](img/b77a06569ffba9074756be0244aa6fea.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b77a06569ffba9074756be0244aa6fea.png)'
- en: (b) RNN
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (b) RNN
- en: '![Refer to caption](img/d3993262c559d6a6ce711b1ae213f9db.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d3993262c559d6a6ce711b1ae213f9db.png)'
- en: (c) The cascade network
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 级联网络
- en: '![Refer to caption](img/54627f60d7ea00ffd34f027f779f5ece.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/54627f60d7ea00ffd34f027f779f5ece.png)'
- en: (d) GCN
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (d) GCN
- en: 'Figure 2: Basic blocks for GER: (1) Convolution block; (b) Recurrent neural
    network (RNN); (c) Cascade network; (d) Graph Convolutional Network (GCN).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：GER 的基础块：（1）卷积块；（b）递归神经网络（RNN）；（c）级联网络；（d）图卷积网络（GCN）。
- en: Before describing the network architecture, Figure [2](#S5.F2 "Figure 2 ‣ V-A
    Basic Network Block ‣ V Deep Networks for GER ‣ A Survey of Deep Learning for
    Group-level Emotion Recognition") first introduce basic network block widely used
    in GER, including CNN, RNN, GCN.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述网络架构之前，图 [2](#S5.F2 "Figure 2 ‣ V-A Basic Network Block ‣ V Deep Networks
    for GER ‣ A Survey of Deep Learning for Group-level Emotion Recognition") 首先介绍了在
    GER 中广泛使用的基础网络块，包括 CNN、RNN 和 GCN。
- en: CNN. Due to the limitations of traditional machine learning in addressing complex
    environments, many researchers have explored more in-depth research methods. LeCun et
    al. [[60](#bib.bib60)] first proposed a convolutional neural network model based
    on the backpropagation algorithm. However, due to hardware limitations at the
    time, the research progress of CNN was relatively slow. It was not until 2012
    when Krizhevsky et al. [[61](#bib.bib61)] proposed the AlexNet CNN model in the
    ImageNet Large Scale Visual Recognition Challenge, which significantly surpassed
    traditional machine learning methods in accuracy and promoted the development
    of deep learning in the field of computer vision. Since then, various models based
    on CNN have been proposed, such as VGGNet, GoogLeNet, ResNet, DenseNet, and MobileNet.
    Meanwhile, CNN models have also shown their superiority in GER.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: CNN。由于传统机器学习在应对复杂环境方面的局限性，许多研究人员探索了更深入的研究方法。LeCun等人[[60](#bib.bib60)]首次提出了基于反向传播算法的卷积神经网络模型。然而，由于当时的硬件限制，CNN的研究进展相对缓慢。直到2012年，Krizhevsky等人[[61](#bib.bib61)]在ImageNet大规模视觉识别挑战赛中提出了AlexNet
    CNN模型，这一模型在准确性上显著超越了传统机器学习方法，并推动了深度学习在计算机视觉领域的发展。从那时起，基于CNN的各种模型相继提出，如VGGNet、GoogLeNet、ResNet、DenseNet和MobileNet。同时，CNN模型在GER中也显示了其优越性。
- en: RNN. In Convolutional Neural Networks (CNNs), each input and output are independent
    of each other, but this ignores the relationship between them. Although CNNs can
    extract good features when processing image datasets, they are not ideal for datasets
    with time series data, such as speech, audio, and video. Rumelhart et al. [[62](#bib.bib62)]
    proposed a method of Recurrent Neural Networks (RNN) that learns and trains through
    the backpropagation algorithm, and applied it to process data with time series.
    RNN has the characteristic of introducing a recurrent structure, allowing the
    network to remember and utilize previous information, thereby extracting better
    time series features. Various improved network architectures based on RNN have
    also been widely used, such as Simple Recurrent Neural Network (SRNN), Bidirectional
    Recurrent Neural Network (BRNN), Long Short-Term Memory Network (LSTM), and Gated
    Recurrent Unit (GRU), which have achieved good results based on RNN. Normally,
    RNN is always combined with CNN, leading to the cascade network for GER, as shown
    in Figure [2(c)](#S5.F2.sf3 "In Figure 2 ‣ V-A Basic Network Block ‣ V Deep Networks
    for GER ‣ A Survey of Deep Learning for Group-level Emotion Recognition").
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RNN。在卷积神经网络（CNNs）中，每个输入和输出彼此独立，但这忽略了它们之间的关系。虽然CNN在处理图像数据集时能提取出很好的特征，但对于时间序列数据集，如语音、音频和视频，CNN并不理想。Rumelhart等人[[62](#bib.bib62)]提出了一种递归神经网络（RNN）的方法，通过反向传播算法进行学习和训练，并将其应用于处理时间序列数据。RNN具有引入递归结构的特点，使网络能够记住和利用先前的信息，从而提取更好的时间序列特征。基于RNN的各种改进网络架构也得到了广泛应用，如简单递归神经网络（SRNN）、双向递归神经网络（BRNN）、长短期记忆网络（LSTM）和门控递归单元（GRU），这些方法在RNN的基础上取得了良好的效果。通常，RNN总是与CNN结合，从而形成级联网络用于GER，如图[2(c)](#S5.F2.sf3
    "在图2 ‣ V-A基本网络块 ‣ V GER的深度网络 ‣ 关于群体情感识别的深度学习综述")所示。
- en: GCN. Graph data has a wide presence in the real world, such as social networks,
    biological networks, recommendation networks, and chemical molecules. However,
    previous deep learning models based on CNN and RNN mainly deal with vector and
    matrix data, which neglects the topological structure of graphs and the relationships
    between nodes, leading to possible information loss and performance degradation.
    In order to solve this problem, Scarselli et al. [[63](#bib.bib63)] first proposed
    a new neural network model, namely, the graph neural network model, which extends
    existing neural network methods to handle data represented in the graph domain.
    Recent studies demonstrate that the effectiveness of graph convolutional networks
    (GCNs) in modeling semantic relationships, making them valuable for facial expression
    recognition (FER) tasks [[64](#bib.bib64)], as depicted in Figure [2(d)](#S5.F2.sf4
    "In Figure 2 ‣ V-A Basic Network Block ‣ V Deep Networks for GER ‣ A Survey of
    Deep Learning for Group-level Emotion Recognition"). With the success of GCNs
    in FER, [[65](#bib.bib65)] introduced GCNs for GER, aiming to enhance performance
    by capturing inter-individual relationships.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: GCN。图数据在现实世界中广泛存在，如社交网络、生物网络、推荐网络和化学分子。然而，以往基于 CNN 和 RNN 的深度学习模型主要处理向量和矩阵数据，忽略了图的拓扑结构和节点之间的关系，可能导致信息丢失和性能下降。为了解决这个问题，Scarselli
    等[[63](#bib.bib63)] 首次提出了一种新的神经网络模型，即图神经网络模型，该模型扩展了现有的神经网络方法，以处理图域中的数据。最近的研究表明，图卷积网络（GCNs）在建模语义关系方面的有效性，使其在面部表情识别（FER）任务中具有重要价值[[64](#bib.bib64)]，如图
    [2(d)](#S5.F2.sf4 "在图 2 ‣ V-A 基本网络块 ‣ V 深度网络用于 GER ‣ 深度学习在群体情感识别中的应用") 所示。随着 GCN
    在 FER 中的成功，[[65](#bib.bib65)] 引入了 GCN 用于 GER，旨在通过捕捉个体间的关系来提升性能。
- en: V-B Network architecture
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 网络架构
- en: 'The efficacy of FER neural units depends on how multiple networks are integrated.
    GER methods typically adopt one of five network architectures: single-stream,
    multi-stream, cascade, graph convolutional network, and attention mechanism. In
    this section, we delve into the specifics of each architecture.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: FER 神经单元的效果取决于多个网络如何集成。GER 方法通常采用五种网络架构中的一种：单流、多流、级联、图卷积网络和注意力机制。在本节中，我们将深入探讨每种架构的具体细节。
- en: V-B1 Single-stream networks
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 单流网络
- en: Typical deep GER methods adopt single CNN with individual input. In single-stream
    2D CNNs, the primary input is facial images, while single-stream 3D CNNs directly
    extract spatial and temporal features from video sequences. Many studies [[66](#bib.bib66),
    [39](#bib.bib39), [40](#bib.bib40)] employ transfer learning strategy on deep
    networks pretrained on large-scale face datasets to mitigate the overfitting issues.
    For example, Rassadin et al. [[39](#bib.bib39)] employ a pre-trained VGGFace model
    on detected faces, followed by a weighted sum to obtain the final result using
    a random forest classifier. Similarly, Lu et al. [[40](#bib.bib40)] utilize a
    VGG model pretrained on the VGGFace dataset to extract facial features for GER.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的深度 GER 方法采用具有单独输入的单个 CNN。在单流 2D CNN 中，主要输入是面部图像，而单流 3D CNN 直接从视频序列中提取空间和时间特征。许多研究[[66](#bib.bib66),
    [39](#bib.bib39), [40](#bib.bib40)] 采用在大规模面部数据集上预训练的深度网络的迁移学习策略，以减轻过拟合问题。例如，Rassadin
    等[[39](#bib.bib39)] 在检测到的面部上采用了预训练的 VGGFace 模型，然后通过加权求和使用随机森林分类器获得最终结果。类似地，Lu
    等[[40](#bib.bib40)] 利用在 VGGFace 数据集上预训练的 VGG 模型提取面部特征用于 GER。
- en: In addition to transfer learning methods, several works design cascade network [[67](#bib.bib67),
    [68](#bib.bib68)] or kernel methods [[69](#bib.bib69)] on single-stream shallow
    CNNs. Sun et al. [[67](#bib.bib67)] explore various handcrafted feature (LBP),
    AlexNet, Reduced AlexNet and ResNet, followed by group-expression model or LSTM
    for group-level happiness intensity estimation. Building on this, Wei et al. [[68](#bib.bib68)]
    extend the group-level intensity estimation with VGGFace pretrained VGG-Face dataset.
    Furthermore, GER with ResNet18, ResNet34, MobileNet, DenseNet, Resnet50, Inception,
    GoogleNet, and VGG19 pretrained on Imagenet for scene-level information is explored
    in [[54](#bib.bib54), [41](#bib.bib41)]. The results demonstrate that VGG surpasses
    other architectures in GER and excels at distinguishing the complex hidden information
    in data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了迁移学习方法外，几项工作还在单流浅层CNN上设计了级联网络[[67](#bib.bib67), [68](#bib.bib68)]或核方法[[69](#bib.bib69)]。Sun等人[[67](#bib.bib67)]探索了各种手工特征（LBP）、AlexNet、Reduced
    AlexNet和ResNet，随后使用群体表达模型或LSTM进行群体级幸福强度估计。基于此，Wei等人[[68](#bib.bib68)]利用预训练的VGG-Face数据集扩展了群体级强度估计。此外，GER使用在Imagenet上预训练的ResNet18、ResNet34、MobileNet、DenseNet、Resnet50、Inception、GoogleNet和VGG19进行场景级信息的探索[[54](#bib.bib54),
    [41](#bib.bib41)]。结果表明，VGG在GER中超越了其他架构，特别擅长区分数据中复杂的隐藏信息。
- en: While the aforementioned works are based on 2D CNN with image input, several
    works employ 3D CNN variants [[58](#bib.bib58), [53](#bib.bib53), [53](#bib.bib53)]
    or cascade network [[70](#bib.bib70)] to directly extract spatial and temporal
    features from video sequences. Inflated ResNet-3D [[58](#bib.bib58)], Temporal
    shift module (TSM) [[53](#bib.bib53)], and Temporal Binding Network (TBN) [[53](#bib.bib53)]
    are introduced in GER. Additionally, a end-to-end cross-attention cascade network [[70](#bib.bib70)]
    combined the idea of ClipBERT [[71](#bib.bib71)] modules with the temporal sequence
    to enhance the representation in spatial and temporal dimensions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述工作基于图像输入的2D CNN，但也有几项工作采用了3D CNN变体[[58](#bib.bib58), [53](#bib.bib53), [53](#bib.bib53)]或级联网络[[70](#bib.bib70)]，以直接从视频序列中提取空间和时间特征。引入了Inflated
    ResNet-3D[[58](#bib.bib58)]、Temporal shift module (TSM)[[53](#bib.bib53)]和Temporal
    Binding Network (TBN)[[53](#bib.bib53)]。此外，一种端到端的交叉注意力级联网络[[70](#bib.bib70)]结合了ClipBERT[[71](#bib.bib71)]模块的思想与时间序列，以增强空间和时间维度的表示。
- en: V-B2 Multi-stream Networks
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 多流网络
- en: Single-stream model represent a basic structure in GER, extracting features
    solely from a single viewpoint such as the face or scene within group-level image.
    However, since group-level images encompass diverse and rich information, a single
    view may not provide sufficient insight. As we discussed in Section [IV](#S4 "IV
    Input modality ‣ A Survey of Deep Learning for Group-level Emotion Recognition"),
    employing various inputs from different perspectives can effectively explore spatial
    and temporal information. Hence, multi-stream networks have been adopted in GER
    to extract features through multiple inputs. Generally, multi-stream networks
    can be categorized into networks with two inputs, more than two inputs, and handcrafted
    features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 单流模型在群体级情感识别（GER）中代表了一种基本结构，仅从单一视角（如面部或场景）提取特征。然而，由于群体级图像包含多样而丰富的信息，单一视角可能无法提供足够的洞察。如我们在第[IV](#S4
    "IV Input modality ‣ A Survey of Deep Learning for Group-level Emotion Recognition")节中讨论的，采用来自不同视角的多种输入可以有效探索空间和时间信息。因此，多流网络已被应用于GER，通过多个输入提取特征。一般而言，多流网络可以分为两个输入的网络、多个输入的网络和手工特征。
- en: Multi-stream networks with two inputs. According to [[72](#bib.bib72)], the
    face plays a crucial role in expressing the emotion. Therefore, among multi-stream
    networks with local and global inputs, the face remains a primary input. Several
    studies incorporated the face as local information and the scene as global information.
    They combined a convolution neural network for face while another neural network
    on scene descriptors for GER [[42](#bib.bib42), [44](#bib.bib44), [66](#bib.bib66),
    [73](#bib.bib73), [45](#bib.bib45)]. Experiment results of these studies demonstrate
    that models based on face outperforms those on other methods and other modality
    is still competitive and useful to GER. Moreover, the multi-stream networks have
    shown improvement in GER. Furthermore, Zhang et al. [[74](#bib.bib74)] presented
    a semi-supervised group-level emotion recognition (SSGER) framework based on contrastive
    learning, learning efficient features from both labeled and unlabeled images,
    where face images and scene images are utilized. In addition to visual features,
    audio feature are also being considered in dynamic GER. Augusma et al. [[75](#bib.bib75)]
    proposed branches for both video and audio, with cross-attention between modalities
    for GER. In their work, the video branch is based on a fine-tuned ViT architecture,
    while the audio branch extracts Mel-spectrograms and feeds them through CNN blocks
    into a transformer encoder.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 带有两个输入的多流网络。根据[[72](#bib.bib72)]，面部在表达情感中起着关键作用。因此，在具有局部和全局输入的多流网络中，面部仍然是主要输入。几个研究将面部作为局部信息，场景作为全局信息进行整合。他们结合了用于面部的卷积神经网络和用于场景描述符的另一种神经网络以进行GER[[42](#bib.bib42),
    [44](#bib.bib44), [66](#bib.bib66), [73](#bib.bib73), [45](#bib.bib45)]。这些研究的实验结果表明，基于面部的模型优于其他方法，而其他模态仍然对GER具有竞争力和实用性。此外，多流网络在GER中表现出了改进。此外，张等人[[74](#bib.bib74)]提出了一种基于对比学习的半监督组级情感识别（SSGER）框架，从标记和未标记的图像中学习高效特征，其中利用了面部图像和场景图像。除了视觉特征之外，音频特征也在动态GER中被考虑。Augusma等人[[75](#bib.bib75)]提出了视频和音频的分支，并在模态之间进行交叉注意以进行GER。在他们的工作中，视频分支基于经过微调的ViT架构，而音频分支则提取Mel频谱图，并通过CNN块输入到transformer编码器中。
- en: Multi-stream networks with more than two inputs. As mentioned in the previous
    section, bottom-up components involve individual emotions, while top-down components
    consider contextual factors such as the background of an image [[23](#bib.bib23)].
    Therefore, to enhance group feature representation, some works [[51](#bib.bib51),
    [56](#bib.bib56), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [48](#bib.bib48),
    [53](#bib.bib53), [79](#bib.bib79), [65](#bib.bib65)] have investigated combinations
    of more than one top-down component and bottom-up components. Guo et al. [[51](#bib.bib51)]
    designed a hybrid network that incorporates scene features, skeleton features,
    and local facial features with deep convolutional neural networks. Additionally,
    they [[76](#bib.bib76)] further used visual attention attention mechanism to fuse
    face, scene, skeletons, and salient regions. Different from the aforementioned
    studies, Fujii et al. [[78](#bib.bib78), [48](#bib.bib48)] proposed a two-stage
    architecture for GER. The first stage performs binary classification based on
    facial expression to distinguish “Positive” labels, including discriminative facial
    expressions from others. For second stage, they considered exploiting object-wise
    semantic information and scene background for the second classification. Recently,
    several researchers [[53](#bib.bib53), [56](#bib.bib56)] investigated multi-stream
    network for dynamic GER. Spatio-temporal features and static features were exploited
    by Sun et al. [[53](#bib.bib53)]. The fusion of several spatio-temporal modality
    adopted RGB, RGB difference, optical flow, warped optical flow, and audio, while
    image-level CNNs were designed based on face and body images. Moreover, a hybrid
    network fusing audio stream, facial emotion stream, environmental object statistics
    stream (EOS), and video stream are designed with temporal shift module and SVM
    for GER [[56](#bib.bib56)].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 多流网络具有两个以上的输入。如前一节所述，底层组件涉及个体情感，而顶层组件考虑上下文因素，例如图像的背景[[23](#bib.bib23)]。因此，为了增强组特征表示，一些研究[[51](#bib.bib51)、[56](#bib.bib56)、[76](#bib.bib76)、[77](#bib.bib77)、[78](#bib.bib78)、[48](#bib.bib48)、[53](#bib.bib53)、[79](#bib.bib79)、[65](#bib.bib65)]调查了多于一个顶层组件和底层组件的组合。郭等人[[51](#bib.bib51)]设计了一种混合网络，该网络结合了场景特征、骨架特征和局部面部特征与深度卷积神经网络。此外，他们[[76](#bib.bib76)]进一步使用了视觉注意机制来融合面部、场景、骨架和显著区域。与上述研究不同，藤井等人[[78](#bib.bib78)、[48](#bib.bib48)]提出了一种两阶段的GER架构。第一阶段基于面部表情进行二分类，以区分“积极”标签，包括与其他表情区分的面部表情。对于第二阶段，他们考虑利用对象级语义信息和场景背景进行第二次分类。最近，一些研究者[[53](#bib.bib53)、[56](#bib.bib56)]研究了动态GER的多流网络。孙等人[[53](#bib.bib53)]利用了时空特征和静态特征。多种时空模态的融合采用了RGB、RGB差异、光流、扭曲光流和音频，同时基于面部和身体图像设计了图像级CNN。此外，融合音频流、面部情感流、环境对象统计流（EOS）和视频流的混合网络被设计用于GER[[56](#bib.bib56)]，并配有时间转移模块和SVM。
- en: Multi-stream networks with handcafted features. According to the analysis, the
    facial emotion or movements of group-level emotion are highly related to face
    textures, while scene information contains more abundant information for GER,
    the handcrafted features for low-level representation also plays an important
    role in GER. Multiple works [[68](#bib.bib68), [43](#bib.bib43)] combined deep
    features for face-level and handcrafted features for scene-level to leverage the
    low-level and high-level information for robust GER.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 具有手工特征的多流网络。根据分析，群体级情感的面部表情或动作与面部纹理高度相关，而场景信息包含了更多的GER信息，手工特征在低级表示中也起着重要作用。多个研究[[68](#bib.bib68)、[43](#bib.bib43)]结合了面部级的深度特征和场景级的手工特征，以利用低级和高级信息来实现鲁棒的GER。
- en: V-B3 Cascade network
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3级联网络
- en: For GER, dealing with varying numbers of face between group-level images poses
    a significant challenge. As discussed in Section [IV-A](#S4.SS1 "IV-A Static image
    ‣ IV Input modality ‣ A Survey of Deep Learning for Group-level Emotion Recognition"),
    current cascade networks can be categorized to two types. The first category utilizes
    variants of CNN such as ResNet, VGG, followed by decision-level score fusion.
    In contrast, the second category combines various CNN and RNNs to address the
    inconsistency in the number of faces between two group-level image. In this section,
    we will discuss the details of the second category [[67](#bib.bib67), [76](#bib.bib76),
    [80](#bib.bib80), [70](#bib.bib70), [81](#bib.bib81)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GER（群体情感识别），处理组级图像之间面孔数量的变化是一个重大挑战。如在第[IV-A](#S4.SS1 "IV-A Static image ‣
    IV Input modality ‣ A Survey of Deep Learning for Group-level Emotion Recognition")节中讨论的，当前的级联网络可以分为两种类型。第一类利用
    CNN 的变体，如 ResNet、VGG，之后进行决策级别的评分融合。相反，第二类将各种 CNN 和 RNN 结合起来，以解决两个组级图像之间面孔数量的不一致。在这一节中，我们将深入讨论第二类[[67](#bib.bib67)、[76](#bib.bib76)、[80](#bib.bib80)、[70](#bib.bib70)、[81](#bib.bib81)]。
- en: In GER, a critical issue is how to effectively model the variability in the
    number of faces. LSTM is a primary method. Sun et al. [[67](#bib.bib67)] were
    the first to investigate the combination of CNNs and LSTM. They explored the use
    of AlexNet, Reduced AlexNet, and ResNet to extract the individual faces in a group.
    Subsequently, a weighted LSTM was used to assign different weights to each facial
    feature based on factors such as the size of the face and the distance between
    them. Furthermore, visual attention mechanisms were incorporated in LSTM for GER [[76](#bib.bib76),
    [70](#bib.bib70)]. In [[70](#bib.bib70)], a cascade network containing CNN and
    LSTM was employed to extract image-level and audio-level features, respectively.
    An attention mechanism was then introduced to compute important features at each
    time step. Additionally, Li et al. [[82](#bib.bib82)] employed a similar architecture
    at the face-level for GER. However, in contrast to [[67](#bib.bib67)], skeletons
    and scene features were directly fused at the end. Moreover, skeletons information
    extracted by OpenPose toolkit was considered in cascade network proposed by Slogrove et
    al. [[80](#bib.bib80)]. In their approach, the coordinates and confidence information
    of all individual key points were fed into an LSTM as a sequence for modeling,
    resulting in a group-level emotion classification. Additionally, speech signals
    were investigated for GER in [[81](#bib.bib81)]. They proposed a method incorporating
    a cascade network with multi-task learning for GER, using deep spectral features
    on speech signal. The cascade network based on CNN and RNN, was designed to extract
    discriminative deep spectral features, while multi-task learning combines emotion
    recognition and speaker identification tasks during the model training process.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GER 中，一个关键问题是如何有效地建模面孔数量的变异性。LSTM 是一种主要的方法。Sun 等人[[67](#bib.bib67)]首次研究了 CNN
    和 LSTM 的组合。他们探索了使用 AlexNet、Reduced AlexNet 和 ResNet 来提取组中的单独面孔。随后，使用加权 LSTM 根据面孔的大小和面孔之间的距离等因素给每个面部特征分配不同的权重。此外，在
    GER 中将视觉注意机制纳入 LSTM[[76](#bib.bib76)、[70](#bib.bib70)]。在[[70](#bib.bib70)]中，使用包含
    CNN 和 LSTM 的级联网络分别提取图像级别和音频级别的特征。然后引入了注意机制，以计算每个时间步的重要特征。此外，Li 等人[[82](#bib.bib82)]在面部级别采用了类似的架构用于
    GER。然而，与[[67](#bib.bib67)]相比，骨架和场景特征在最后直接融合。此外，Slogrove 等人[[80](#bib.bib80)] 提出的级联网络中考虑了
    OpenPose 工具包提取的骨架信息。在他们的方法中，所有单独关键点的坐标和置信度信息作为序列输入到 LSTM 中进行建模，从而实现组级情感分类。此外，在[[81](#bib.bib81)]中研究了语音信号对
    GER 的影响。他们提出了一种将级联网络与多任务学习结合的方法，使用语音信号的深度谱特征。基于 CNN 和 RNN 的级联网络旨在提取具有辨别力的深度谱特征，而多任务学习则在模型训练过程中结合情感识别和说话人识别任务。
- en: V-B4 GCN based network
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B4 基于 GCN 的网络
- en: Individuals within a group often exhibit diverse social relationships with others.
    To highlight this social aspect, several works [[29](#bib.bib29), [3](#bib.bib3)]
    have utilized graph convolutional networks (GCNs) to model both visual features
    and social context within group-level images. In these approaches, the emotional
    states of individuals are treated as node features, while the interactions between
    individuals are represented as graph edges, thus forming a graph structure. Guo et
    al. [[29](#bib.bib29)] introduced a group-level emotion recognition method based
    on four cues, where faces, bodies, objects, and the entire image are transformed
    into a graph structure. This graph represents the relationships within the group
    based on these four cues, facilitating group-level emotion recognition. Additionally,
    Wang et al. [[3](#bib.bib3)] proposed a context-consistent cross-graph neural
    network to mitigate emotional biases resulting from different cues in multi-cue
    emotion recognition.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 群体中的个体通常与他人表现出多样的社会关系。为了突出这一社会方面，几个研究 [[29](#bib.bib29), [3](#bib.bib3)] 利用图卷积网络（GCNs）来建模群体级图像中的视觉特征和社会背景。在这些方法中，个体的情感状态被视为节点特征，而个体之间的互动则表示为图边，从而形成图结构。Guo
    等人 [[29](#bib.bib29)] 提出了一种基于四个线索的群体级情感识别方法，其中面孔、身体、物体和整个图像被转化为图结构。这个图表示基于这四个线索的群体内部关系，从而促进群体级情感识别。此外，Wang
    等人 [[3](#bib.bib3)] 提出了一个上下文一致的跨图神经网络，以减轻多线索情感识别中由不同线索引起的情感偏差。
- en: V-B5 Attention based network
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B5 基于注意力的网络
- en: In order to prioritize important character or object features that play a pivotal
    role in group emotions, five studies have incorporated attention mechanisms. Gupta et
    al. [[83](#bib.bib83)] detect local facial emotions by employing an attention
    mechanism to concentrate on more pertinent local information on the face, generating
    probability attention weights through the Softmax function. The weighted sum of
    facial features is then calculated based on these attention weights to produce
    a single facial feature vector representation. Additionall, Guo et al. [[76](#bib.bib76)]
    and Khan et al. [[79](#bib.bib79)] also integrated attention mechanisms. They
    introduced a novel region attention network (RAN) to detect and extract crucial
    features in facial regions. The region attention mechanism comprises an attention
    generator and an attention applier, where the former generates adaptive region
    attention weights to emphasize important facial features in different regions,
    while the latter applies these generated region attention weights to the output
    of the feature extractor to enhance the distinction of facial features. Furthermore,
    Wang et al. [[84](#bib.bib84)] proposed a cascaded attention network, which leverages
    the importance of each face in the image to generate a global representation based
    on all faces, effectively focusing on the feature information of the most important
    face.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优先考虑在群体情感中发挥关键作用的重要人物或物体特征，五项研究引入了注意力机制。Gupta 等人 [[83](#bib.bib83)] 通过使用注意力机制集中在面部的相关局部信息上，从而检测局部面部情感，并通过
    Softmax 函数生成概率注意力权重。然后，根据这些注意力权重计算面部特征的加权和，生成单一的面部特征向量表示。此外，Guo 等人 [[76](#bib.bib76)]
    和 Khan 等人 [[79](#bib.bib79)] 也整合了注意力机制。他们引入了一种新颖的区域注意力网络（RAN），以检测和提取面部区域中的关键特征。区域注意力机制包括一个注意力生成器和一个注意力应用器，其中前者生成自适应区域注意力权重，以突出不同区域中的重要面部特征，而后者将这些生成的区域注意力权重应用于特征提取器的输出，以增强面部特征的区别性。此外，Wang
    等人 [[84](#bib.bib84)] 提出了一个级联注意力网络，该网络利用图像中每张面孔的重要性生成基于所有面孔的全局表示，有效地关注最重要面孔的特征信息。
- en: Moreover, Transformer-based architectures have found extensive applications
    in NLP [[85](#bib.bib85)] and computer vision [[86](#bib.bib86)] tasks. Inspired
    by the significant success of Transformer architecture in various tasks, two recent
    studies [[75](#bib.bib75), [87](#bib.bib87)] have explored the application of
    transformers to the GER task. Augusma et al. [[75](#bib.bib75)] initially utilized
    the visual Transformer mechanism and the BERT framework to extract features from
    global images and speech, respectively. They employed a cross-attention mechanism
    to learn the weights of the two modes and subsequently performed weight fusion.
    Moreover, a dual-branch cross-patch attention Transformer (DCAT) was proposed
    to incorporate the psychological concept of the Most Important Person (MIP) and
    the global image.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于Transformer的架构在自然语言处理[[85](#bib.bib85)]和计算机视觉[[86](#bib.bib86)]任务中得到了广泛应用。受到Transformer架构在各种任务中取得显著成功的启发，最近的两项研究[[75](#bib.bib75),
    [87](#bib.bib87)]探索了将transformer应用于GER任务。Augusma等人[[75](#bib.bib75)]最初利用视觉Transformer机制和BERT框架分别从全局图像和语音中提取特征。他们采用了交叉注意力机制来学习这两种模式的权重，并随后进行了权重融合。此外，提出了一种双分支交叉补丁注意力Transformer（DCAT），将心理学中的最重要人物（MIP）概念和全局图像结合起来。
- en: In summary, GER network architectures can be broadly categorized into single-stream,
    multi-stream, cascade networks, GNN-based networks, and attention-based networks.
    While single-stream serves as the fundamental model, it only considers a single
    view of the group-level image. To leverage more information, multi-stream networks
    learn features from multiple perspectives for robust GER. Additionally, as the
    group size fluctuates, cascade networks sequentially incorporate various modules
    like RNNs and LSTMs to construct an end-to-end GER network. GNN effectively models
    interactions between individuals based on social relationships. Conversely, the
    attention mechanism, inspired by the psychological concept of the most important
    person, focuses on extracting key features from all individuals. In the future,
    combining more effective modules in multi-stream, cascade, and attention-based
    approaches could further enhance GER performance.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，GER网络架构可以大致分为单流、多个流、级联网络、基于GNN的网络和基于注意力的网络。单流网络作为基础模型，只考虑组级图像的单一视角。为了利用更多信息，多流网络从多个角度学习特征，以实现稳健的GER。此外，由于组规模的波动，级联网络依次集成RNN和LSTM等各种模块，构建端到端的GER网络。GNN有效地基于社会关系建模个体间的互动。相反，注意力机制受到心理学中最重要的人的概念启发，关注从所有个体中提取关键特征。未来，结合多流、级联和基于注意力的更有效模块可能进一步提升GER性能。
- en: V-C Fusion stage and scheme
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C融合阶段和方案
- en: In GER, the fusion stage plays a crucial role in integrating information from
    multiple cues to improve the accuracy of emotion recognition. It encompass various
    methods for combining features extracted from different modalities, such as facial
    expressions, scene, skeleton etc. One common fusion approach is score-level, as
    used in studies like  [[76](#bib.bib76), [49](#bib.bib49), [47](#bib.bib47)].
    In their approaches, the output scores from individual modalities are combined
    using techniques like averaging or mean voting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在GER中，融合阶段在整合来自多种线索的信息以提高情感识别准确性方面发挥着关键作用。它包含了各种将从不同模态（如面部表情、场景、骨架等）提取的特征进行组合的方法。一种常见的融合方法是评分级别的融合，如在[[76](#bib.bib76),
    [49](#bib.bib49), [47](#bib.bib47)]的研究中使用的方法。在这些方法中，来自单一模态的输出评分通过平均或均值投票等技术进行组合。
- en: Alternatively, feature-level fusion used in studies like those by [[78](#bib.bib78),
    [48](#bib.bib48), [33](#bib.bib33)], integrates raw feature representations extracted
    from each modality before feeding them into a classifier. This approach allows
    the model to learn more complex relationships between different modalities but
    may be more computationally intensive. Besides score-level and feature-level fusion,
    kernel-based [[88](#bib.bib88), [69](#bib.bib69)] and loss function-based [[50](#bib.bib50),
    [74](#bib.bib74)] fusion are two alternative ways to fuse multi-modality. For
    example, [[74](#bib.bib74)] proposed a weight cross-entropy loss function on Scene-Face
    network by combining face and scene information.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，像[[78](#bib.bib78)、[48](#bib.bib48)、[33](#bib.bib33)]等研究中使用的特征级融合，在将原始特征表示提取自每种模态后进行融合，然后再送入分类器。这种方法允许模型学习不同模态之间更复杂的关系，但可能计算开销较大。除了分数级和特征级融合外，基于核的[[88](#bib.bib88)、[69](#bib.bib69)]和基于损失函数的[[50](#bib.bib50)、[74](#bib.bib74)]融合是融合多模态的两种替代方法。例如，[[74](#bib.bib74)]在
    Scene-Face 网络上提出了一个权重交叉熵损失函数，通过结合面部和场景信息。
- en: V-D Loss function
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 损失函数
- en: Different from classical methods, where the feature extraction and classification
    are independent, deep networks can perform end-to-end classification through loss
    functions by penalizing the deviation between predicted and true labels during
    training. Most GER works directly apply the commonly used softmax cross-entropy
    loss [[39](#bib.bib39)]. The softmax loss is typically effective at correctly
    classifying known categories. However, in practical classification tasks, the
    classification of unknown samples is also essential. Therefore, to achieve better
    generalization ability, it is crucial to further enhance inter-class difference
    and reduce intra-class variation, especailly for data scarcity. Metric learning
    techniques, such as contrastive loss [[89](#bib.bib89)], have been developed to
    ensure intra-class compactness and inter-class separability by measuring the relative
    distances between inputs. Wang et al. [[90](#bib.bib90)] proposed a contrastive
    learning-based self-attentive network. In this approach, different features are
    embedded into a vector space, and the similarity and difference of features are
    learned by enhancing the similarity between samples of the same class and reducing
    the similarity between samples of different classes. Then, adaptive weight calculation
    and weighted average fusion are performed to adaptively fuse features at different
    levels. Although the above two methods have achieved good performance, they are
    still limited to static images. Additionally, metric learning loss often requires
    effective sample mining strategies for robust recognition performance. Metric
    learning alone may not suffice for learning a discriminative metric space for
    GER. To address these challenges, Zhang et al. [[74](#bib.bib74)] proposed a semi-supervised
    group-level emotion recognition framework based on contrastive learning to learn
    efficient features from both labeled and unlabeled images. To alleviate the uncertainty
    of given pseudo-labels, they introduce Weight Cross-Entropy Loss (WCE-Loss) to
    suppress the influence of samples with unreliable pseudo-labels in the training
    process.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于经典方法中，特征提取和分类是独立的，深度网络可以通过损失函数实现端到端的分类，通过在训练过程中惩罚预测标签与真实标签之间的偏差。大多数 GER 研究直接应用常用的
    softmax 交叉熵损失[[39](#bib.bib39)]。Softmax 损失通常对正确分类已知类别有效。然而，在实际分类任务中，未知样本的分类也同样重要。因此，为了实现更好的泛化能力，进一步增强类别间差异和减少类别内变异至关重要，特别是在数据稀缺的情况下。对比损失[[89](#bib.bib89)]等度量学习技术已经被开发出来，以通过测量输入之间的相对距离来确保类别内紧凑性和类别间可分性。Wang
    等人[[90](#bib.bib90)] 提出了基于对比学习的自注意网络。在这种方法中，不同的特征被嵌入到一个向量空间中，通过增强同一类别样本之间的相似性和减少不同类别样本之间的相似性来学习特征的相似性和差异。然后，进行自适应权重计算和加权平均融合，以自适应地融合不同级别的特征。尽管上述两种方法已取得了良好的性能，但它们仍然局限于静态图像。此外，度量学习损失通常需要有效的样本挖掘策略以获得鲁棒的识别性能。仅凭度量学习可能不足以为
    GER 学习一个具有区分性的度量空间。为了解决这些挑战，Zhang 等人[[74](#bib.bib74)] 提出了一个基于对比学习的半监督组级情感识别框架，以从标记和未标记图像中学习有效特征。为了减轻伪标签的不确定性，他们引入了权重交叉熵损失（WCE-Loss），以抑制训练过程中伪标签不可靠样本的影响。
- en: In summary, although most current GER approaches utilize the standard softmax
    cross-entropy loss, only a limited number of studies have explored alternative
    loss functions such as contrastive learning loss or introduced novel loss functions
    to enhance inter-class separability, intra-class compactness, and achieve well-balanced
    learning. Looking ahead, investigating more effective loss functions targeting
    discriminative representations for group-level emotion features holds significant
    promise as a direction for future research in GER.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，尽管当前大多数 GER 方法采用标准的 softmax 交叉熵损失函数，但只有少数研究探索了对比学习损失等替代损失函数，或引入了新型损失函数，以提高类间分离度、类内紧凑度，并实现良好的平衡学习。展望未来，研究更有效的损失函数，以针对组级情感特征的判别表示，将作为
    GER 未来研究的重要方向。
- en: VI Experiments
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 实验
- en: VI-A Performance metric
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 性能指标
- en: The standard evaluation metric for GER typically involves using accuracy for
    group-level emotion recognition and mean square error for group happiness estimation.
    Accuracy assesses the proportion of correct predictions relative to the total
    number of evaluated samples, providing a measure of the model’s overall performance
    in correctly identifying group-level emotion. Conversely, mean square error quantifies
    the average squared difference between the predicted and true happiness values,
    offering a measure of the model’s accuracy in estimating the happiness levels
    of a group.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: GER 的标准评估指标通常包括使用准确率进行组级情感识别，以及使用均方误差进行组幸福度估计。准确率评估正确预测相对于总评估样本数的比例，为模型在正确识别组级情感方面的整体性能提供衡量标准。相反，均方误差量化了预测值和真实幸福值之间的平均平方差异，提供了模型在估计组幸福水平方面的准确度衡量。
- en: VI-B Model Evaluation Protocols
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 模型评估协议
- en: Cross-validation stands as a widely utilized protocol for evaluating GER performance.
    This protocol involves dividing the dataset into train, validation, and test sets,
    ensuring fair verification of deep learning architectures on group emotion datasets.
    Cross-validation in the GER contains fixed partition validation and K-fold cross
    validation. The first kind of cross-validation is commonly used as the official
    evaluation method in the competitions such as the Emotion Recognition in the wild
    challenge (EmotiW) [[27](#bib.bib27)]. Here, the train, validation, and test sets
    are pre-determined and remain fixed throughout the competition, eliminating randomization.
    Participants receive the train and validation sets at the competition’s outset
    for model development, while the test set is revealed later for final performance
    assessment and ranking. On the other hand, K-fold cross-validation protocol is
    also prevalent in GER research. This protocol involves randomly dividing the dataset
    into $k$ equally sized parts, with each part serving as a test set in turn while
    the remaining portions constitute the training data. The process repeats $k$ times,
    with each partition serving as the test set once. The choice for $k$, typically
    4 or 5 in GER, can significantly impact evaluating time while ensuring robust
    performance assessment.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证作为评估 GER 性能的广泛使用协议。这一协议涉及将数据集划分为训练集、验证集和测试集，确保对组情感数据集上的深度学习架构进行公平验证。GER中的交叉验证包括固定分区验证和K折交叉验证。第一种交叉验证常作为
    Emotion Recognition in the Wild Challenge (EmotiW) [[27](#bib.bib27)] 等竞赛中的官方评估方法。在这种方法中，训练集、验证集和测试集在整个比赛期间是预先确定并固定不变的，从而消除了随机化。参赛者在比赛开始时获得训练集和验证集以进行模型开发，而测试集则在稍后揭示，以进行最终性能评估和排名。另一方面，K折交叉验证协议在
    GER 研究中也很常见。该协议涉及将数据集随机划分为 $k$ 个相等大小的部分，每个部分依次作为测试集，而其余部分构成训练数据。该过程重复 $k$ 次，每个分区各作为一次测试集。在
    GER 中，$k$ 的选择通常为 4 或 5，这可能显著影响评估时间，同时确保稳健的性能评估。
- en: VI-C Performance analysis
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 性能分析
- en: Table [II](#S6.T2 "TABLE II ‣ VI-C Performance analysis ‣ VI Experiments ‣ A
    Survey of Deep Learning for Group-level Emotion Recognition") reports the performance
    of various deep learning models for GER as reported in EmotiW since 2016\. With
    the exceptions of [[54](#bib.bib54), [56](#bib.bib56)], most methods proposed
    in these competition aim to fuse more than two cues such as face, scene, and skeleton,
    among others. Notably, established neural network architectures such as VGG have
    been commonly employed for extracting facial expression feature. Additionally,
    to accommodate the variable number of faces/objects, variations of RNN have been
    prevalent in many algorithms. Furthermore, since 2019, attention modules, facilitated
    by the successful application of transformer architectures, have gained widespread
    adoption in Emotion Recognition in the Wild competitions. Regarding performance
    metrics, in the HAPPEI dataset, the lowest reported RMSE is 0.822\. In the GAF2.0
    competition, the highest reported accuracy is 80.9%, while in GAF3.0, it is 68.08%.
    This suggests that GAF3.0 introduced more competitive samples and increased the
    challenge level. Additionally, a new track called Group Cohesion, derived from
    GAF3.0, was introduced to evaluate group cohesion. Despite improvements in team
    performance between EmotionW2016 and EmotiW2019, there is still significant room
    for enhancing overall performance. Since 2020, the utilization of video-based
    datasets has become prevalent, indicating a growing consideration for temporal
    information in algorithm development. Moreover, traditional feature extraction
    techniques have been gradually supplanted by deep learning methods in recent years.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表[II](#S6.T2 "TABLE II ‣ VI-C Performance analysis ‣ VI Experiments ‣ A Survey
    of Deep Learning for Group-level Emotion Recognition")报告了自2016年以来EmotiW中各种深度学习模型在GER（群体级情感识别）中的表现。除了[[54](#bib.bib54),
    [56](#bib.bib56)]之外，这些比赛中提出的大多数方法旨在融合两个以上的线索，如面部、场景和骨架等。值得注意的是，像VGG这样的成熟神经网络架构已被广泛用于提取面部表情特征。此外，为了适应面部/物体数量的变化，RNN的变体在许多算法中得到广泛应用。此外，自2019年以来，由于变压器架构的成功应用，注意力模块在Emotion
    Recognition in the Wild比赛中得到了广泛采用。关于性能指标，在HAPPEI数据集中，最低报告的RMSE为0.822。在GAF2.0比赛中，最高报告的准确率为80.9%，而在GAF3.0中为68.08%。这表明GAF3.0引入了更具竞争力的样本，增加了挑战性。此外，源于GAF3.0的新赛道“群体凝聚力”被引入以评估群体凝聚力。尽管EmotionW2016和EmotiW2019之间团队表现有所提升，但整体性能仍有显著提升空间。自2020年以来，基于视频的数据集的使用变得越来越普遍，表明在算法开发中对时间信息的关注不断增加。此外，近年来，传统的特征提取技术逐渐被深度学习方法取代。
- en: Beyond competition, many researchers have also explored GER using group-level
    emotion databases like HAPPEI and GAF. Tables [III](#S6.T3 "TABLE III ‣ VI-C Performance
    analysis ‣ VI Experiments ‣ A Survey of Deep Learning for Group-level Emotion
    Recognition") and [IV](#S6.T4 "TABLE IV ‣ VI-C Performance analysis ‣ VI Experiments
    ‣ A Survey of Deep Learning for Group-level Emotion Recognition") offer detailed
    comparison of method or model accuracies proposed by researchers over the last
    decade, excluding those participating in EmotionW competition. Notably, except
    for [[91](#bib.bib91), [69](#bib.bib69), [88](#bib.bib88), [33](#bib.bib33)],
    all methods fused more than two cues. Among these, face, pose/skeleton, and object
    are regarded as local component, while scene information serves as global information.
    This approach aligns with the concept of bottom-up and top-down components mentioned
    in group emotion theory. Furthermore, widely recognized network blocks such as
    VGG, Xception, ResNet and AlexNet have found extensive use in GER due to their
    promising performance in image and face recognition tasks. Additionally, the fusion
    of various networks enables better exploitation of complementary information between
    them. Common fusion schemes include Average and Feature concatenation, which provide
    straightforward solutions to the fusion problem. However, some researchers have
    proposed novel approaches to fuse multi-modality features. For example, Guo et
    al. [[29](#bib.bib29)] introduced a graph convolutional network to facilitate
    information exchange among features extracted from different models. Zhu et al. [[50](#bib.bib50)]
    proposed a uncertain-aware learning to extract more robust representation from
    face, object, and scene modalities for GER. Moreover, unlike competitions, recently
    proposed GER methods have been evaluated across various databases, such as GroupEmoW,
    GAF, and GECV-GroupImg to access their generalization ability. With the introduction
    of databases like VGAF and GECV-GroupVid, researchers have started exploring the
    spatiotemporal GER.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了竞赛之外，许多研究人员还使用像HAPPEI和GAF这样的组级情感数据库来探索GER。表格 [III](#S6.T3 "TABLE III ‣ VI-C
    Performance analysis ‣ VI Experiments ‣ A Survey of Deep Learning for Group-level
    Emotion Recognition")和 [IV](#S6.T4 "TABLE IV ‣ VI-C Performance analysis ‣ VI
    Experiments ‣ A Survey of Deep Learning for Group-level Emotion Recognition")
    提供了过去十年中研究人员提出的方法或模型准确性的详细比较，排除了参与EmotionW竞赛的那些。值得注意的是，除了[[91](#bib.bib91), [69](#bib.bib69),
    [88](#bib.bib88), [33](#bib.bib33)]，所有方法都融合了两个以上的提示。在这些方法中，面部、姿态/骨架和对象被视为局部组件，而场景信息则作为全局信息。这种方法符合组情感理论中提到的自下而上的组件和自上而下的组件的概念。此外，由于在图像和面部识别任务中表现出色，VGG、Xception、ResNet和AlexNet等广泛认可的网络模块在GER中得到了广泛应用。此外，各种网络的融合使得更好地利用它们之间的互补信息成为可能。常见的融合方案包括平均和特征拼接，这些方案为融合问题提供了直接的解决方案。然而，一些研究人员提出了新颖的方法来融合多模态特征。例如，Guo等人[[29](#bib.bib29)]引入了图卷积网络，以促进不同模型提取的特征之间的信息交换。Zhu等人[[50](#bib.bib50)]提出了一种不确定性感知学习方法，从面部、对象和场景模态中提取更稳健的表征以进行GER。此外，与竞赛不同，最近提出的GER方法已在各种数据库上进行评估，如GroupEmoW、GAF和GECV-GroupImg，以评估其泛化能力。随着VGAF和GECV-GroupVid等数据库的引入，研究人员已开始探索时空GER。
- en: In general, modality fusion can yield promising results across all datasets.
    Different modalities contribute diverse information, allowing for more comprehensive
    exploration of limited GER samples. Since the combined inputs offer robust GER
    solutions, multi-stream networks are recommended to effectively learn representations
    from available modalities. In contrast, single-modality approaches perform worse
    due to limited information and redundancy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，模态融合在所有数据集上都能产生令人期待的结果。不同的模态提供了多样的信息，从而更全面地探索有限的GER样本。由于组合的输入提供了稳健的GER解决方案，因此推荐使用多流网络来有效学习来自可用模态的表征。相比之下，由于信息有限和冗余，单模态方法表现较差。
- en: From Tables [III](#S6.T3 "TABLE III ‣ VI-C Performance analysis ‣ VI Experiments
    ‣ A Survey of Deep Learning for Group-level Emotion Recognition") and [IV](#S6.T4
    "TABLE IV ‣ VI-C Performance analysis ‣ VI Experiments ‣ A Survey of Deep Learning
    for Group-level Emotion Recognition"), it is clear that fusion of scores and features
    is a common approach in integrating multiple modalities. Additionally, there is
    a growing trend towards using loss functions for multi-modality fusion. Fusion
    schemes like cross-attention, GCN, ECL, and NVPF have demonstrated state-of-the-art
    results across all databases. This is likely due to the challenges posed bythat
    the limited number of GER samples and group sizes, making leveraging additional
    data sources as reasonable and effective solution.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格 [III](#S6.T3 "TABLE III ‣ VI-C Performance analysis ‣ VI Experiments ‣ A
    Survey of Deep Learning for Group-level Emotion Recognition") 和 [IV](#S6.T4 "TABLE
    IV ‣ VI-C Performance analysis ‣ VI Experiments ‣ A Survey of Deep Learning for
    Group-level Emotion Recognition") 中可以看出，评分和特征的融合是一种在整合多模态时常见的方法。此外，使用损失函数进行多模态融合的趋势也在增长。像交叉注意力、GCN、ECL
    和 NVPF 等融合方案在所有数据库中都展示了最先进的结果。这可能是由于 GER 样本和组大小有限带来的挑战，使得利用额外的数据源成为一个合理且有效的解决方案。
- en: Presently, scene information based on LSTM and averaging features across all
    faces are commonly employed in video-based GER studies. This is likely because
    flexible group sizes pose the main challenge for video-based GER. Recently, Quach et
    al. [[33](#bib.bib33)] proposed a Non-Volume Preserving Fusion (NVPF) mechanism
    with LSTM to model spatial representation between groups of multiple faces and
    temporal relationships between multiple video frames. However, the small-sample
    GE dataset limits the ability to model group-level features for video-based GER.
    The combination of transfer learning and graph-based methods is anticipated to
    be a promising direction for future GER studies.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，基于 LSTM 的场景信息和对所有面部特征的平均处理在视频基础的 GER 研究中被广泛采用。这可能是因为灵活的组大小对视频基础的 GER 构成了主要挑战。最近，Quach
    等人 [[33](#bib.bib33)] 提出了一个非体积保持融合（NVPF）机制，结合 LSTM 来建模多面部组之间的空间表示和多个视频帧之间的时间关系。然而，小样本
    GE 数据集限制了对视频基础 GER 的组级特征建模。结合迁移学习和基于图的方法被预期将是未来 GER 研究的一个有前景的方向。
- en: 'TABLE II: Performance comparison of remarkable deep learning techniques published
    in ACM Library in Group emotion competition in Emotion Recognition in the Wild
    (EmotiW) challenge [[92](#bib.bib92), [2](#bib.bib2), [27](#bib.bib27), [30](#bib.bib30),
    [38](#bib.bib38), [34](#bib.bib34)]. The best performance for specific database
    on the test set is bold and red color.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：在 Emotion Recognition in the Wild（EmotiW）挑战中的组情感竞赛中，ACM 图书馆发布的显著深度学习技术的性能比较
    [[92](#bib.bib92), [2](#bib.bib2), [27](#bib.bib27), [30](#bib.bib30), [38](#bib.bib38),
    [34](#bib.bib34)]。特定数据库的测试集最佳性能以粗体和红色标出。
- en: '| Dataset (Year) | Cate. | Ref. | Modality | Network architecture | Fusion
    scheme | Fusion stage | Pre-train | Prot | Perf. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 数据集（年份） | 类别 | 参考文献 | 模态 | 网络架构 | 融合方案 | 融合阶段 | 预训练 | 原型 | 性能 |'
- en: '| F | S | P | A | T |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| F | S | P | A | T |'
- en: '| HAPPEI* (2016) | 6 | [[42](#bib.bib42)] | ✓ | ✓ |  |  |  | ResNet for F CENTRIST+PCA
    for S | LSTM | Feature | FER2013 | val | 0.494 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| HAPPEI*（2016） | 6 | [[42](#bib.bib42)] | ✓ | ✓ |  |  |  | ResNet 用于 F CENTRIST+PCA
    用于 S | LSTM | 特征 | FER2013 | 验证 | 0.494 |'
- en: '| test | 0.822 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 0.822 |'
- en: '| [[67](#bib.bib67)] | ✓ |  |  |  |  | AlexNet | LSTM | Feature | FER2013 |
    val | 0.4942 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| [[67](#bib.bib67)] | ✓ |  |  |  |  | AlexNet | LSTM | 特征 | FER2013 | 验证 |
    0.4942 |'
- en: '| test | 0.836 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 0.836 |'
- en: '| [[93](#bib.bib93)] | ✓ | ✓ |  |  |  | ResNet+LSTM for F CENTRIST/VGG for
    S | Concat | Feature | - | val | 0.55 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] | ✓ | ✓ |  |  |  | ResNet+LSTM 用于 F CENTRIST/VGG 用于 S
    | 拼接 | 特征 | - | 验证 | 0.55 |'
- en: '| test | 0.865 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 0.865 |'
- en: '| GAF2.0 (2017) | 3 | [[39](#bib.bib39)] | ✓ | ✓ |  |  |  | VGG for F ImageNet
    for S | Soft aggregation &weighting | Score | VGG face ImageNet | val | 75.39%
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GAF2.0（2017） | 3 | [[39](#bib.bib39)] | ✓ | ✓ |  |  |  | VGG 用于 F ImageNet
    用于 S | 软聚合 & 权重调整 | 评分 | VGG face ImageNet | 验证 | 75.39% |'
- en: '| test | 78.53% |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 78.53% |'
- en: '| [[43](#bib.bib43)] | ✓ | ✓ |  |  |  | HOG+FV for S VGG+VLAD for F | Cont
    | Feature | - | val | 65% |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bib43)] | ✓ | ✓ |  |  |  | HOG+FV 用于 S VGG+VLAD 用于 F | 连续 | 特征
    | - | 验证 | 65% |'
- en: '| test | 75.10% |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 75.10% |'
- en: '| [[44](#bib.bib44)] | ✓ | ✓ |  |  |  | AlexNet for F Context for S | Bayesian
    Network | Score | - | val | 67.75% |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [[44](#bib.bib44)] | ✓ | ✓ |  |  |  | AlexNet 用于 F Context for S | 贝叶斯网络
    | 评分 | - | 验证 | 67.75% |'
- en: '| test | 64.68% |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 64.68% |'
- en: '| [[51](#bib.bib51)] | ✓ | ✓ | ✓ |  |  | VGG for F Inception&ResNet for P Inception&VGG
    for S | SVM | Score | FER2013 GENKI-4K | val | 80.05% |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51)] | ✓ | ✓ | ✓ |  |  | VGG 用于 F Inception&ResNet 用于 P Inception&VGG
    用于 S | SVM | 评分 | FER2013 GENKI-4K | 验证 | 80.05% |'
- en: '| test | 80.61% |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 80.61% |'
- en: '| [[66](#bib.bib66)] | ✓ | ✓ |  |  |  | Xception for F VGG for S | Cont | Feature
    | FER2013 | val | 72.38% |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [[66](#bib.bib66)] | ✓ | ✓ |  |  |  | Xception 用于 F VGG 用于 S | 持续 | 特征 |
    FER2013 | 验证 | 72.38% |'
- en: '| test | 63.43% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 63.43% |'
- en: '| [[68](#bib.bib68)] | ✓ | ✓ |  |  |  | VGG+DCNN for F CENTRIST+VGG for S |
    LSTM/SVM | Feature | - | val | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[68](#bib.bib68)] | ✓ | ✓ |  |  |  | VGG+DCNN 用于 F CENTRIST+VGG 用于 S | LSTM/SVM
    | 特征 | - | 验证 | - |'
- en: '| test | 79.78% |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 79.78% |'
- en: '| [[73](#bib.bib73)] | ✓ | ✓ |  |  |  | 4-layer CNN for F ResNet for S | Average
    | Score | FERPlus Places | val | 83.7% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[73](#bib.bib73)] | ✓ | ✓ |  |  |  | 4层 CNN 用于 F ResNet 用于 S | 平均 | 得分 |
    FERPlus Places | 验证 | 83.7% |'
- en: '|  |  | test | 80.9% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 测试 | 80.9% |'
- en: '| GAF3.0 (2018) | 3 | [[76](#bib.bib76)] | ✓ | ✓ | ✓ |  |  | VGG for F Inception/SE-ResNet
    for S ResNet for P | Weight Average | Score | FER2013 GENKI-4K | val | 78.98%
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| GAF3.0 (2018) | 3 | [[76](#bib.bib76)] | ✓ | ✓ | ✓ |  |  | VGG 用于 F Inception/SE-ResNet
    用于 S ResNet 用于 P | 权重平均 | 得分 | FER2013 GENKI-4K | 验证 | 78.98% |'
- en: '| test | 68.08% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 68.08% |'
- en: '| [[77](#bib.bib77)] | ✓ | ✓ |  |  |  | ResNet for F VGG for S | Weight average
    | Score | FER2013 RAF-DB | val | 78.39% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| [[77](#bib.bib77)] | ✓ | ✓ |  |  |  | ResNet 用于 F VGG 用于 S | 权重平均 | 得分 |
    FER2013 RAF-DB | 验证 | 78.39% |'
- en: '| test | 65.59% |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 65.59% |'
- en: '| [[83](#bib.bib83)] | ✓ | ✓ | ✓ |  |  | DenseNet for S SphereFace for F |
    Cont | Feature | ImageNet CASIA-Webface | val | 80.98% |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| [[83](#bib.bib83)] | ✓ | ✓ | ✓ |  |  | DenseNet 用于 S SphereFace 用于 F | 持续
    | 特征 | ImageNet CASIA-Webface | 验证 | 80.98% |'
- en: '| test | 64.83% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 64.83% |'
- en: '| [[84](#bib.bib84)] | ✓ | ✓ |  |  |  | CAN for F, ResNet for S, SE-net for
    P | Average | Score | FERPlus | val | 86.7% |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| [[84](#bib.bib84)] | ✓ | ✓ |  |  |  | CAN 用于 F，ResNet 用于 S，SE-net 用于 P |
    平均 | 得分 | FERPlus | 验证 | 86.7% |'
- en: '| test | 67.48% |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 67.48% |'
- en: '| Group Cohesion* (2019) | 4 | [[94](#bib.bib94)] | ✓ | ✓ | ✓ |  |  | CAN for
    F SE-Net for S/P | Average | Score | FERPlus ImageNet | val | 0.5588 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 组内一致性* (2019) | 4 | [[94](#bib.bib94)] | ✓ | ✓ | ✓ |  |  | CAN 用于 F SE-Net
    用于 S/P | 平均 | 得分 | FERPlus ImageNet | 验证 | 0.5588 |'
- en: '| test | 0.4382 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 0.4382 |'
- en: '| [[95](#bib.bib95)] | ✓ | ✓ | ✓ |  |  | DensePose for S ResNet/Inception/NasNet
    for S ResNet for F | Average | Score | VGG Face2 RAF-DB | val | 0.517 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| [[95](#bib.bib95)] | ✓ | ✓ | ✓ |  |  | DensePose 用于 S ResNet/Inception/NasNet
    用于 S ResNet 用于 F | 平均 | 得分 | VGG Face2 RAF-DB | 验证 | 0.517 |'
- en: '| test | 0.416 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 0.416 |'
- en: '| [[96](#bib.bib96)] | ✓ | ✓ | ✓ |  |  | VGG+SVR for F Efficient+SVR for P
    Densenet+SVR for S | Grid Search | Score | FER2013 Emotic | val | 0.672 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | ✓ | ✓ | ✓ |  |  | VGG+SVR 用于 F Efficient+SVR 用于 P Densenet+SVR
    用于 S | 网格搜索 | 得分 | FER2013 Emotic | 验证 | 0.672 |'
- en: '| test | 0.444 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 0.444 |'
- en: '| VGAF (2020) | 3 | [[54](#bib.bib54)] |  | ✓ |  |  |  | VGG+ML | - | - | ImageNet
    | val | 57.18% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| VGAF (2020) | 3 | [[54](#bib.bib54)] |  | ✓ |  |  |  | VGG+ML | - | - | ImageNet
    | 验证 | 57.18% |'
- en: '| test | 59.13% |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 59.13% |'
- en: '| [[81](#bib.bib81)] |  |  | ✓ |  |  | DeepSpectrum+AlexNet +VGG+DenseNet |
    Mean | Score | - | val | 58.09% |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| [[81](#bib.bib81)] |  |  | ✓ |  |  | DeepSpectrum+AlexNet +VGG+DenseNet |
    平均 | 得分 | - | 验证 | 58.09% |'
- en: '| test | 62.70% |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 62.70% |'
- en: '| [[56](#bib.bib56)] | ✓ | ✓ | ✓ | ✓ | ✓ | TSM for T Dense for F, OpenSmile
    for A | Average | Score | FER2013 | val | 74.28% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| [[56](#bib.bib56)] | ✓ | ✓ | ✓ | ✓ | ✓ | TSM 用于 T Dense 用于 F，OpenSmile 用于
    A | 平均 | 得分 | FER2013 | 验证 | 74.28% |'
- en: '| test | 76.85% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 76.85% |'
- en: '| [[57](#bib.bib57)] |  | ✓ |  | ✓ | ✓ | K-injection | Cont | Feature | - |
    val | 66.19% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| [[57](#bib.bib57)] |  | ✓ |  | ✓ | ✓ | K-injection | 持续 | 特征 | - | 验证 | 66.19%
    |'
- en: '| test | 66.40% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 66.40% |'
- en: '| [[53](#bib.bib53)] | ✓ | ✓ | ✓ |  | ✓ | TSM TBN | Weight sum | Score | ImageNet
    | val | 71.93% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| [[53](#bib.bib53)] | ✓ | ✓ | ✓ |  | ✓ | TSM TBN | 权重和 | 得分 | ImageNet | 验证
    | 71.93% |'
- en: '| test | 70.77% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 70.77% |'
- en: '| VGAF (2023) | 3 | [[55](#bib.bib55)] | ✓ | ✓ |  | ✓ | ✓ | ResNet for F, SeNet
    for S Hubert large for A | Cont | Feature | FER2013 ImageNet | val | 68.41% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| VGAF (2023) | 3 | [[55](#bib.bib55)] | ✓ | ✓ |  | ✓ | ✓ | ResNet 用于 F，SeNet
    用于 S Hubert large 用于 A | 持续 | 特征 | FER2013 ImageNet | 验证 | 68.41% |'
- en: '| test | 72% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 72% |'
- en: '| [[75](#bib.bib75)] |  | ✓ |  | ✓ | ✓ | ViT-large for V CNN+Transformer for
    A | Average | Feature | ImageNet | val | 78.72% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] |  | ✓ |  | ✓ | ✓ | ViT-large 用于 V CNN+Transformer 用于
    A | 平均 | 特征 | ImageNet | 验证 | 78.72% |'
- en: '| test | 75.13% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 75.13% |'
- en: '| Prot.: Protocol; Cate.: Category. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Prot.: 协议；Cate.: 类别。 |'
- en: '| TSM: Temporal Shift Module; TBN: Temporal Binding Network; CAN: Cascade Attention
    Network. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| TSM: 时间位移模块；TBN: 时间绑定网络；CAN: 级联注意网络。 |'
- en: '| F: Face, S: Scene, P: Pose/skeleton, A: Audio, T: Temporal. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| F: 面部，S: 场景，P: 姿态/骨架，A: 音频，T: 时间。 |'
- en: '| Concat: Concatenation. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 连接：连接。 |'
- en: '| * For HAPPEI and Group Coheison, RMSE is used as performance metric, while
    for other databases, accuracy is used. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| * 对于 HAPPEI 和组内一致性，使用 RMSE 作为性能指标，而对于其他数据库，则使用准确率。 |'
- en: 'TABLE III: Performance comparison of remarkable deep learning techniques in
    image-based group-level emotion databases.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：基于图像的组级情感数据库中显著深度学习技术的性能比较。
- en: '| Method | Database | Modality | Network | Fusion Scheme | Fusion Stage | Prot.
    | Cate. | Perf. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 数据库 | 模态 | 网络 | 融合方案 | 融合阶段 | Prot. | Cate. | 性能 |'
- en: '| F | S | P | O |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| F | S | P | O |'
- en: '| [[91](#bib.bib91)] | GAF2.0 | ✓ |  |  |  | AlexNet | HeatMap | Feature |
    val | 3 | 55.23% |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [[91](#bib.bib91)] | GAF2.0 | ✓ |  |  |  | AlexNet | 热力图 | 特征 | 验证 | 3 |
    55.23% |'
- en: '| [[45](#bib.bib45)] | ✓ | ✓ |  |  | Xception for face VGG for scene | Concat
    | Feature | val | 3 | 71.83% |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| [[45](#bib.bib45)] | ✓ | ✓ |  |  | 面部的 Xception 场景的 VGG | 拼接 | 特征 | 验证 |
    3 | 71.83% |'
- en: '| [[69](#bib.bib69)] | ✓ |  |  |  | CNN, RVLBP | DMKL | Kernel | val | 3 |
    79.49% |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)] | ✓ |  |  |  | CNN，RVLBP | DMKL | 核 | 验证 | 3 | 79.49%
    |'
- en: '| [[49](#bib.bib49)] | ✓ | ✓ |  |  | MobileNet for scene LSTM for face | Average
    | Score | 1-fold | 3 | 78% |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| [[49](#bib.bib49)] | ✓ | ✓ |  |  | 场景的 MobileNet 面部的 LSTM | 平均 | 分数 | 1-fold
    | 3 | 78% |'
- en: '| [[47](#bib.bib47)] | GAF3.0 | ✓ |  |  |  | Inception/VGG for scene VGG for
    face | SVM | Score | val | 3 | 70.1% |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| [[47](#bib.bib47)] | GAF3.0 | ✓ |  |  |  | 场景的 Inception/VGG 面部的 VGG | SVM
    | 分数 | 验证 | 3 | 70.1% |'
- en: '| [[82](#bib.bib82)] | ✓ | ✓ | ✓ |  | VGG+LSTM for face Dense for Skeleton
    Attention for Scene | Concat | Feature | val | 3 | 62.90% |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| [[82](#bib.bib82)] | ✓ | ✓ | ✓ |  | 面部的 VGG+LSTM 骨架的 Dense 场景的注意力 | 拼接 |
    特征 | 验证 | 3 | 62.90% |'
- en: '| [[88](#bib.bib88)] | MultiEmoVA HAPPEI* GAF2.0 | ✓ |  |  |  | RVLBP, VGG
    | SVM-CGAK | Kernel | 5-fold 4-fold val | 5 6 3 | 54.40% 0.4920 72.17% |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| [[88](#bib.bib88)] | MultiEmoVA HAPPEI* GAF2.0 | ✓ |  |  |  | RVLBP，VGG |
    SVM-CGAK | 核 | 5-fold 4-fold 验证 | 5 6 3 | 54.40% 0.4920 72.17% |'
- en: '| [[48](#bib.bib48)] | GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | VGG, attention for face
    VGG, attention for object VGG for scene | Hierarchical | Feature | val | 3 | 80.41%
    76.61% |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| [[48](#bib.bib48)] | GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | VGG，面部的注意力 VGG，对象的注意力
    VGG，场景的 VGG | 分层的 | 特征 | 验证 | 3 | 80.41% 76.61% |'
- en: '| [[50](#bib.bib50)] | MultiEmoVA GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | ResNet for
    face VGG for object VGG for scene | UAL | Loss | 5-fold val val | 5 3 3 | 61.22%
    79.19% 77.10% |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| [[50](#bib.bib50)] | MultiEmoVA GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | 面部的 ResNet
    对象的 VGG 场景的 VGG | UAL | 损失 | 5-fold 验证 验证 | 5 3 3 | 61.22% 79.19% 77.10% |'
- en: '| [[29](#bib.bib29)] | GroupEmoW GAF2.0 SocEID | ✓ | ✓ | ✓ | ✓ | VGG for face
    SE-ResNet for skeleton SENet for object Inception for scene | GNN | Score | test
    val test | 3 3 8 | 89.14% 78.16% 91.61% |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] | GroupEmoW GAF2.0 SocEID | ✓ | ✓ | ✓ | ✓ | 面部的 VGG 骨架的
    SE-ResNet 对象的 SENet 场景的 Inception | GNN | 分数 | 测试 验证 测试 | 3 3 8 | 89.14% 78.16%
    91.61% |'
- en: '| [[79](#bib.bib79)] | GAF2.0 GroupEmoW | ✓ | ✓ |  | ✓ | Resnet for all modalities
    Attention module | CARAN | Loss | val test | 3 | 67.61% 90.18% |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| [[79](#bib.bib79)] | GAF2.0 GroupEmoW | ✓ | ✓ |  | ✓ | Resnet 适用于所有模态 注意力模块
    | CARAN | 损失 | 验证 测试 | 3 | 67.61% 90.18% |'
- en: '| [[74](#bib.bib74)] | GroupEmoW GAF2.0 GAF3.0 | ✓ | ✓ |  |  | ResNet for all
    modalities | FusionNet | Loss | test val val | 3 | 88.67% 78.51% 77.01% |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| [[74](#bib.bib74)] | GroupEmoW GAF2.0 GAF3.0 | ✓ | ✓ |  |  | 所有模态的 ResNet
    | FusionNet | 损失 | 测试 验证 验证 | 3 | 88.67% 78.51% 77.01% |'
- en: '| [[3](#bib.bib3)] | GroupEmoW GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | ResNet+LSTM+GNN
    for face SE-ResNet+GNN for object SE-ResNet+GNN for scene | ECL | Loss | test
    val val | 3 | 90.06% 79.45% 79.95% |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| [[3](#bib.bib3)] | GroupEmoW GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | 面部的 ResNet+LSTM+GNN
    对象的 SE-ResNet+GNN 场景的 SE-ResNet+GNN | ECL | 损失 | 测试 验证 验证 | 3 | 90.06% 79.45%
    79.95% |'
- en: '| [[87](#bib.bib87)] | GAF3.0 GroupEmoW | ✓ | ✓ |  |  | Multi-scale Transformer
    | DCAT | Feature | val test | 3 | 79.20% 90.47% |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| [[87](#bib.bib87)] | GAF3.0 GroupEmoW | ✓ | ✓ |  |  | 多尺度 Transformer | DCAT
    | 特征 | 验证 测试 | 3 | 79.20% 90.47% |'
- en: '| [[33](#bib.bib33)] | GECV-GroupImg GAF3.0 | ✓ |  |  |  | EmoNet | NVPF |
    Feature | val | 3 | 77.02% 76.12% |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| [[33](#bib.bib33)] | GECV-GroupImg GAF3.0 | ✓ |  |  |  | EmoNet | NVPF |
    特征 | 验证 | 3 | 77.02% 76.12% |'
- en: '| Prot.: Protocol; Cate.: Category. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Prot.: 协议；Cate.: 类别。 |'
- en: '| NVPF: Non-volume Preserving-based Fusion; DMKL: Deep Multiple Kernel Learning;
    GNN: Graph Neural Network. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| NVPF: 非体积保持融合；DMKL: 深度多核学习；GNN: 图神经网络。 |'
- en: '| UAL: Uncertain-aware Learning; CARAN: Context-aware Regional Attention Network;
    Concat: Concatenation. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| UAL: 不确定性感知学习；CARAN: 上下文感知区域注意力网络；Concat: 拼接。 |'
- en: '| ECL: Emotion context-consistent learning; DCAT: Dual-branch Cross-Patch Attention
    Transformer. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| ECL: 情感上下文一致学习；DCAT: 双分支跨补丁注意力变换器。 |'
- en: '| F: Face, S: Scene, P: Pose/skeleton, O: Object. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| F: 面部，S: 场景，P: 姿势/骨架，O: 对象。 |'
- en: '| * For HAPPEI, RMSE is used as performance metric, while for other databases,
    accuracy is used. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| * 对于 HAPPEI，使用 RMSE 作为性能指标，而对于其他数据库，使用准确率。 |'
- en: 'TABLE IV: Performance comparison of remarkable deep learning techniques in
    video-based group-level emotion (VGAF) database, where F, S, P, and A mean face,
    scene, pose, and audio modality, respectively.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE IV: 基于视频的群体级情感（VGAF）数据库中显著深度学习技术的性能比较，其中 F、S、P 和 A 分别表示面部、场景、姿势和音频模态。'
- en: '| Method | Dataset | Modality | Network | Fusion Scheme | Fusion Stage | Prot.
    | Cate. | Accuracy |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 数据集 | 模态 | 网络 | 融合方案 | 融合阶段 | Prot. | Cate. | 准确率 |'
- en: '| F | S | P | A |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| F | S | P | A |'
- en: '| Sharma+[[32](#bib.bib32)] | VGAF |  | ✓ |  | ✓ | LSTM for scene OpenSMILE
    for audio | Concat | Feature | val | 3 | 47.50% |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Sharma+[[32](#bib.bib32)] | VGAF |  | ✓ |  | ✓ | LSTM用于场景 OpenSMILE用于音频 |
    连接 | 特征 | val | 3 | 47.50% |'
- en: '| Pinto+[[58](#bib.bib58)] |  | ✓ |  | ✓ | ResNet for scene Bi-LSTM for audio
    | SVM | Score | val | 3 | 65.74% |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Pinto+[[58](#bib.bib58)] |  | ✓ |  | ✓ | ResNet用于场景 Bi-LSTM用于音频 | SVM | 分数
    | val | 3 | 65.74% |'
- en: '| Evtodienko+[[70](#bib.bib70)] |  | ✓ |  | ✓ | Hubert+Attention for audio
    ResNet+Attention for scene | Concat | Feature | val | 3 | 60.37% |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Evtodienko+[[70](#bib.bib70)] |  | ✓ |  | ✓ | Hubert+注意力用于音频 ResNet+注意力用于场景
    | 连接 | 特征 | val | 3 | 60.37% |'
- en: '| Quach+[[33](#bib.bib33)] | GECV -GroupVid | ✓ |  |  |  | EmoNet | TNVPF |
    Feature | test | 3 | 70.97% |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Quach+[[33](#bib.bib33)] | GECV -GroupVid | ✓ |  |  |  | EmoNet | TNVPF |
    特征 | 测试 | 3 | 70.97% |'
- en: '| Prot.: Protocol; Cate.: Category. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Prot.: 协议; Cate.: 类别。 |'
- en: '| TNVPF: Temporal Non-volume Preserving-based Fusion. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| TNVPF: 基于时间的非体积保持融合。 |'
- en: '| Concat: Concatenation. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 连接: 级联。 |'
- en: '| F: Face, S: Scene, P: Pose/skeleton, A: Audio. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| F: 面部, S: 场景, P: 姿势/骨架, A: 音频。 |'
- en: VII Challenges and future direction
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 挑战和未来方向
- en: 'Group-level emotion recognition in unconstrained environments has become significant
    attention within the computer vision community, offering substantial implications
    for social public security and education. This article delves into the intricate
    concepts of group dynamics and emotion, along with methodologies for recognizing
    group-level emotion, and pertient datasets, aiming to provide a comprehensive
    analysis of the current landscape and future trajectories of GER. This endeavor
    furnishes a robust theoretical foundation for potential applications of GER in
    domains such as social psychology, human-computer interaction, and smart cities.
    In this section, we summarize and discuss the future prospects of GER from three
    pivotal dimensions: database-level, technique-level, multimodality, and evaluation
    metric.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在不受限制的环境中，组级情感识别在计算机视觉领域中已经引起了显著关注，为社会公共安全和教育提供了重要意义。本文深入探讨了组动态和情感的复杂概念，以及组级情感识别的方法和相关数据集，旨在提供对GER当前现状和未来发展趋势的全面分析。这一努力为GER在社会心理学、人机交互和智能城市等领域的潜在应用奠定了坚实的理论基础。在这一部分中，我们从数据库层面、技术层面、多模态和评估指标这三个关键维度总结和讨论了GER的未来前景。
- en: GER encounters three primary technical challenges. Firstly, the accurate discernment
    of emotions inherently presents complex, compounded by the potential bias introduced
    by human subjective labeling in annotating datasets. Secondly, the fluctuating
    number of individuals and diverse scenes within a group necessitates enhanced
    generalization and robustness of features. Furthermore, different extracted features
    may convey inconsistent emotions. Lastly, the cross-fusion of diverse features
    in a multimodal model and achieving end-to-end feature learning pose significant
    challenges. Despite these obstacles hindering GER advancement, its potential applications
    span a wide spectrum.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: GER面临三个主要技术挑战。首先，情感的准确辨别本质上是复杂的，且数据集注释中人为主观标记可能引入的偏差使情况更加复杂。其次，组内个体数量的波动和多样化的场景要求特征具有更强的泛化能力和鲁棒性。此外，不同提取的特征可能传达不一致的情感。最后，在多模态模型中，不同特征的交叉融合和实现端到端的特征学习构成了显著挑战。尽管这些障碍阻碍了GER的发展，但其潜在应用涵盖了广泛的领域。
- en: VII-A GER Database
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A GER数据库
- en: While currently available group emotion databases primarily collect images and
    videos from websites and media platforms, their sample size remain relatively
    small. On the other hand, Smith et al. [[97](#bib.bib97)] emphasize the significance
    of the changes in group emotions along time, noting that individuals may react
    differently to external group members based on their prevailing emotion states.
    Pantic et al. [[31](#bib.bib31)] found that dynamic videos provide more discriminative
    information to extract temporal changes in emotions. Therefore, the collection
    of richer and more realistic video data holds promise for furnishing contextual
    semantic insights into group interactions and dynamic processes, facilitating
    a more nuanced observation of emotional dynamics within team.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然目前可用的群体情感数据库主要从网站和媒体平台收集图像和视频，但其样本量仍相对较小。另一方面，Smith 等人 [[97](#bib.bib97)]
    强调了群体情感随时间变化的重要性，指出个人可能会根据他们的情感状态对外部群体成员做出不同的反应。Pantic 等人 [[31](#bib.bib31)] 发现动态视频提供了更多的区分信息来提取情感的时间变化。因此，收集更丰富、更现实的视频数据有望为群体互动和动态过程提供情境语义洞察，从而更细致地观察团队内部的情感动态。
- en: Presently, database annotations are typically derived from independent observers
    rating the images, lacking subjective evaluation like self-reporting measurements.
    This limitation may lead to the challenges in acquiring such measurements from
    online media platforms. However, the absence of subjective evaluation may potentially
    yield considerable performance in group-level emotion recognition. By integrating
    subjective evaluation and other auxiliary information, computer scientists can
    devise advanced methodologies for analyzing data, bridging the gap between computational
    methods and social science research. Moreover, existing databases primarily reply
    on methods such as multi-observer cross-calibration to categorize images or videos,
    which may introduce calibration biases due to cultural disparities and overlook
    the fundamental tenets of group emotion posited by social psychologists [[23](#bib.bib23)].
    Therefore, it is crucial to expanding the repertoire of basic emotion categories
    to encompass more generalized emotion states in affective computing is imperative.
    Additionally, involving social psychologists in the data collection and annotation
    process can furnish more rational and valuable calibration information.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，数据库的注释通常来自独立观察者对图像的评分，缺乏诸如自我报告测量等主观评估。这一限制可能导致从在线媒体平台获取此类测量数据时出现挑战。然而，缺乏主观评估可能会对群体级情感识别产生显著的性能影响。通过整合主观评估和其他辅助信息，计算机科学家可以设计先进的数据分析方法，弥合计算方法与社会科学研究之间的差距。此外，现有数据库主要依赖于多观察者交叉校准等方法来对图像或视频进行分类，这可能由于文化差异引入校准偏差，并忽视社会心理学家
    [[23](#bib.bib23)] 提出的群体情感基本理论。因此，扩大基础情感类别的范围以涵盖更多的通用情感状态在情感计算中显得尤为重要。此外，参与社会心理学家进行数据收集和注释过程可以提供更合理和有价值的校准信息。
- en: VII-B GER Technique
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B GER 技术
- en: The methods ranging from Convolutional Neural Networks (CNN) to Recurrent Neural
    Networks (RNN), Hybrid Networks combining CNN and RNN, and Graph Convolutional
    Networks (GCN), have already demonstrated remarkable success in GER. However,
    the challenge of limited sample sizes necessitates the exploration of unsupervised
    and self-supervised learning techniques. Unsupervised learning technique like
    Generative adversarial networks [[98](#bib.bib98)] and generative AI [[99](#bib.bib99)]
    can be valuable for learning meaningful representations from unannotated data,
    enabling the development of robust deep GER models. These techniques help in extracting
    rich features from data, even when labeled examples are scarce. Similarly, self-supervised
    learning paradigms, such as contrastive learning [[100](#bib.bib100)], offer a
    way to learn representations from auxiliary tasks, enhancing the generalization
    capabilities of GER models across diverse group contexts.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 从卷积神经网络（CNN）到递归神经网络（RNN）、结合 CNN 和 RNN 的混合网络，以及图卷积网络（GCN）的方法，已经在 GER 中展示了显著的成功。然而，有限样本量的挑战要求探索无监督和自监督学习技术。无监督学习技术如生成对抗网络[[98](#bib.bib98)]和生成
    AI[[99](#bib.bib99)]可以从未标注的数据中学习有意义的表示，从而推动稳健的深度 GER 模型的开发。这些技术有助于从数据中提取丰富的特征，即使标记样本稀少。同样，自监督学习范式，如对比学习[[100](#bib.bib100)]，提供了一种从辅助任务中学习表示的方法，增强了
    GER 模型在多样化群体背景下的泛化能力。
- en: As GER systems move towards real-world deployment, the demand for continual
    learning and adaptive capabilities become increasingly critical. Deep learning
    architectures will need to evolve to accommodate dynamic changes in group compositions,
    social contexts, and environmental conditions over time. Incremental learning
    strategies, lifelong learning approaches, and adaptive neural networks will play
    crucial roles in enabling models to adapt and refine their representations based
    on incoming data streams. Moreover, techniques for mitigating catastrophic forgetting
    and domain adaptation will be essential for ensuring the long-term stability and
    effectiveness of GER systems. These methods help models retain previously learned
    knowledge while adapting to new information, thereby enhancing their robustness
    in real-world scenarios.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 GER 系统向实际部署迈进，对持续学习和适应能力的需求变得越来越关键。深度学习架构需要发展以适应群体组成、社会背景和环境条件的动态变化。增量学习策略、终身学习方法和适应性神经网络将在使模型能够根据输入数据流进行调整和优化其表示方面发挥关键作用。此外，缓解灾难性遗忘和领域适应的技术将对确保
    GER 系统的长期稳定性和有效性至关重要。这些方法有助于模型保留之前学到的知识，同时适应新信息，从而提高其在实际场景中的稳健性。
- en: By embracing unsupervised and self-supervised learning techniques, as well as
    continual learning and adaptive methodologies, future deep learning systems will
    be better equipped to capture the nuanced dynamics of group-level emotions across
    diverse scenarios and domains. This holistic approach holds the potential to significantly
    advance the field of Group Emotion Recognition and its applications in various
    domains.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用无监督和自监督学习技术，以及持续学习和适应性方法，未来的深度学习系统将更好地捕捉不同情境和领域中群体情感的细微动态。这种整体方法有可能显著推动群体情感识别领域及其在各种领域中的应用。
- en: VII-C Multi-modal architecture for GER
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 多模态架构用于 GER
- en: In the research process of GER, we have observed a continuous evolution towards
    richer and more diverse in feature representations. Initially, the focus was predominantly
    on a single facial feature. However, as research progressed, there was a shift
    towards utilizing multiple features concurrently, including facial features, local
    object features, and scene features. This enrichment of feature diversity has
    contributed to an enhanced accuracy of group-level emotion recognition to a certain
    extent. Moreover, researcher have begun to explore the integration of different
    types of data sources. For example, studies by Sharma et al. [[32](#bib.bib32)],
    Wang et al. [[57](#bib.bib57)], Liu et al. [[101](#bib.bib101)], and Pinto et
    al. [[58](#bib.bib58)] have incorporated both video frame features and audio features,
    while Liu et al. [[56](#bib.bib56)] and Sun et al. [[53](#bib.bib53)] have combined
    both facial and audio features. These approaches, known as multimodal methods,
    offer advantages such as robustness against interference, high interpretability,
    and broad applicability compared to single-modal methods.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在群体情感识别（GER）的研究过程中，我们观察到了特征表示向更加丰富和多样化的持续演变。最初，研究重点主要集中在单一的面部特征上。然而，随着研究的深入，研究方向转向同时利用多个特征，包括面部特征、局部对象特征和场景特征。这种特征多样性的丰富在一定程度上提升了群体层面情感识别的准确性。此外，研究者们开始探索不同类型数据源的整合。例如，Sharma等人的研究[[32](#bib.bib32)]、Wang等人的研究[[57](#bib.bib57)]、Liu等人的研究[[101](#bib.bib101)]以及Pinto等人的研究[[58](#bib.bib58)]已将视频帧特征和音频特征结合在一起，而Liu等人[[56](#bib.bib56)]和Sun等人[[53](#bib.bib53)]则结合了面部和音频特征。这些被称为多模态方法的方法，相较于单模态方法，具有抗干扰性强、高可解释性和广泛适用性等优点。
- en: Traditionally, GER relied primarily on single-modal information. However, as
    the field has progressed and dataset forms have diversified, researchers have
    increasingly delved into multi-modal methods. These methods leverage various data
    forms including images, videos, and sounds. The adoption of multimodal fusion
    techniques in group-level emotion recognition not only facilitates a more comprehensive
    understanding of emotions but also enhances the accuracy and robustness of the
    recognition process.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，群体情感识别（GER）主要依赖于单一模态信息。然而，随着该领域的发展和数据集形式的多样化，研究者们越来越多地深入探讨多模态方法。这些方法利用包括图像、视频和声音在内的各种数据形式。在群体层面情感识别中采用多模态融合技术不仅有助于更全面地理解情感，而且提升了识别过程的准确性和鲁棒性。
- en: Nevertheless, the application of multi-modal fusion in GER poses several challenges.
    Obtaining and annotating datasets for group-level emotion recognition, particularly
    those encompassing different modalities, can be arduous. Furthermore, variations
    in feature extraction and fusion methods across modalities present additional
    complexities, making it challenging to harmonize features between different modalities.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多模态融合在群体情感识别（GER）中的应用面临若干挑战。获取和注释群体层面情感识别的数据集，尤其是那些涵盖不同模态的数据集，可能非常困难。此外，不同模态间的特征提取和融合方法的差异带来了额外的复杂性，使得在不同模态之间协调特征变得具有挑战性。
- en: Overall, while multi-modal fusion encounters challenges in GER, its potential
    and advantages are undeniable. With advancements in technology and ongoing research
    efforts, it is anticipated that these challenges will be gradually addressed,
    and multimodal methods will emerge as the primary research direction in the field
    of GER. Future research endeavors will likely focus on effectively integrating
    information from diverse modalities and constructing larger and more diverse datasets
    for GER, thereby presenting both challenges and opportunities in this dynamic
    field.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，虽然多模态融合在群体情感识别（GER）中面临挑战，但其潜力和优势不可否认。随着技术进步和持续的研究努力，预计这些挑战将逐步得到解决，多模态方法将成为GER领域的主要研究方向。未来的研究可能会集中在有效整合来自不同模态的信息以及构建更大、更具多样性的数据集，以便在这一动态领域中呈现出挑战与机遇。
- en: VII-D Evaluation metric of GER
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D GER的评估指标
- en: It is worth noting that while accuracy is a common metric in GER, it can be
    influenced by biased data. To address this issue, F1-score offers a more comprehensive
    evaluation by considering True Positives (TP), False Positives (FP), and False
    Negatives (FN). This metric provides a balanced assessment of the true classification
    performance.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管准确性是群体情感识别（GER）中的一个常见指标，但它可能会受到偏倚数据的影响。为解决这一问题，F1分数提供了更全面的评估，它考虑了真正例（TP）、假正例（FP）和假负例（FN）。这一指标提供了对真正分类性能的平衡评估。
- en: Although the data may not exhibit severe imbalance, as indicated in Table [I](#S3.T1
    "TABLE I ‣ III-C Dataset summary ‣ III Group-level emotion dataset ‣ A Survey
    of Deep Learning for Group-level Emotion Recognition"), it may be more suitable
    to employ metrics such as Unweighted F1-score (UF1) and Unweighted Average Recall
    (UAR) to assess method performance. UF1, also known as macro-averaged F1-score,
    calculates the average F1-score across all classes, offering equal weighting to
    each class in multi-class scenarios. Conversely, UAR computes the average accuracy
    per class, normalized by the total number of classes. UAR helps mitigate bias
    arising from class imbalance existing in some databases,e.g., SiteGroEmo and is
    often referred to as balanced accuracy.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据可能没有表现出严重的不平衡，正如表[I](#S3.T1 "TABLE I ‣ III-C Dataset summary ‣ III Group-level
    emotion dataset ‣ A Survey of Deep Learning for Group-level Emotion Recognition")所示，但使用诸如未加权
    F1 分数（UF1）和未加权平均召回率（UAR）等指标来评估方法性能可能更为合适。UF1，也称为宏平均 F1 分数，计算所有类别的平均 F1 分数，在多类别情境中对每个类别给予相等的权重。相反，UAR
    计算每个类别的平均准确率，并按类别总数进行归一化。UAR 有助于减轻某些数据库中存在的类别不平衡所带来的偏差，例如 SiteGroEmo，并且通常被称为平衡准确率。
- en: References
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. Brosch, K. Scherer, D. Grandjean, and D. Sander, “The impact of emotion
    on perception, attention, memory, and decision-making,” *Swiss medical weekly*,
    vol. 143, no. 1920, pp. w13 786–w13 786, 2013.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] T. Brosch, K. Scherer, D. Grandjean 和 D. Sander，“情感对感知、注意、记忆和决策的影响，” *瑞士医学周刊*，第
    143 卷，第 1920 期，第 w13 786–w13 786 页，2013。'
- en: '[2] A. Dhall, R. Goecke, S. Ghosh, J. Joshi, J. Hoey, and T. Gedeon, “From
    individual to group-level emotion recognition: Emotiw 5.0,” in *Proceedings of
    the 19th ACM international conference on multimodal interaction*, 2017, pp. 524–528.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Dhall, R. Goecke, S. Ghosh, J. Joshi, J. Hoey 和 T. Gedeon，“从个体到群体级别的情感识别：Emotiw
    5.0，”在*第 19 届 ACM 国际多模态互动会议论文集*，2017，第 524–528 页。'
- en: '[3] Y. Wang, S. Zhou, Y. Liu, K. Wang, F. Fang, and H. Qian, “Congnn: Context-consistent
    cross-graph neural network for group emotion recognition in the wild,” *Information
    Sciences*, vol. 610, pp. 707–724, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Wang, S. Zhou, Y. Liu, K. Wang, F. Fang 和 H. Qian，“Congnn: 用于群体情感识别的上下文一致跨图神经网络，”
    *信息科学*，第 610 卷，第 707–724 页，2022。'
- en: '[4] M. Dindar, S. Järvelä, S. Ahola, X. Huang, and G. Zhao, “Leaders and followers
    identified by emotional mimicry during collaborative learning: A facial expression
    recognition study on emotional valence,” *IEEE Transactions on Affective Computing*,
    vol. 13, no. 3, pp. 1390–1400, 2020.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Dindar, S. Järvelä, S. Ahola, X. Huang 和 G. Zhao，“通过情感模仿识别的领导者和跟随者：关于情感价值的面部表情识别研究，”
    *IEEE 情感计算汇刊*，第 13 卷，第 3 期，第 1390–1400 页，2020。'
- en: '[5] E. A. Veltmeijer, C. Gerritsen, and K. V. Hindriks, “Automatic emotion
    recognition for groups: a review,” *IEEE Transactions on Affective Computing*,
    vol. 14, no. 1, pp. 89–107, 2021.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] E. A. Veltmeijer, C. Gerritsen 和 K. V. Hindriks，“群体自动情感识别：综述，” *IEEE 情感计算汇刊*，第
    14 卷，第 1 期，第 89–107 页，2021。'
- en: '[6] E. R. Smith and D. M. Mackie, “Group-level emotions,” *Current Opinion
    in Psychology*, vol. 11, pp. 15–19, 2016.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] E. R. Smith 和 D. M. Mackie，“群体情感，”*当前心理学评论*，第 11 卷，第 15–19 页，2016。'
- en: '[7] P. M. Niedenthal and M. Brauer, “Social functionality of human emotion,”
    *Annual review of psychology*, vol. 63, pp. 259–285, 2012.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] P. M. Niedenthal 和 M. Brauer，“人类情感的社会功能，” *心理学年鉴*，第 63 卷，第 259–285 页，2012。'
- en: '[8] M. E. Shaw, “Group dynamics,” *Annual Review of Psychology*, vol. 12, no. 1,
    pp. 129–156, 1961.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. E. Shaw，“群体动态，” *心理学年鉴*，第 12 卷，第 1 期，第 129–156 页，1961。'
- en: '[9] A. D. Szilagyi and M. J. Wallace, *Organizational Behavior and Performance*.   JAI
    Press, 1983.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. D. Szilagyi 和 M. J. Wallace，*组织行为与绩效*。JAI Press，1983。'
- en: '[10] D. N. Barron, E. West, and M. T. Hannan, “A time to grow and a time to
    die: Growth and mortality of credit unions in new york city, 1914-1990,” *American
    Journal of Sociology*, vol. 100, no. 2, pp. 381–421, 1994.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] D. N. Barron, E. West 和 M. T. Hannan，“生长与衰亡：1914-1990 年纽约市信用合作社的增长与死亡，”*美国社会学杂志*，第
    100 卷，第 2 期，第 381–421 页，1994。'
- en: '[11] N. Dasgupta, M. R. Banaji, and R. P. Abelson, “Group entitativity and
    group perception: Associations between physical features and psychological judgment.”
    *Journal of personality and social psychology*, vol. 77, no. 5, p. 991, 1999.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] N. Dasgupta, M. R. Banaji 和 R. P. Abelson，“群体实体性与群体感知：物理特征与心理判断之间的关联。”
    *人格与社会心理学杂志*，第 77 卷，第 5 期，第 991 页，1999。'
- en: '[12] S. Schachter and J. Singer, “Cognitive, social, and physiological determinants
    of emotional state.” *Psychological review*, vol. 69, no. 5, p. 379, 1962.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Schachter 和 J. Singer，“情感状态的认知、社会和生理决定因素。” *心理学评论*，第 69 卷，第 5 期，第 379
    页，1962。'
- en: '[13] P. R. Kleinginna Jr and A. M. Kleinginna, “A categorized list of emotion
    definitions, with suggestions for a consensual definition,” *Motivation and emotion*,
    vol. 5, no. 4, pp. 345–379, 1981.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] P. R. Kleinginna Jr 和 A. M. Kleinginna，“情感定义的分类列表，并对共识定义提出建议，” *动机与情感*，第5卷，第4期，第345–379页，1981年。'
- en: '[14] P. Ekman, “An argument for basic emotions,” *Cognition & emotion*, vol. 6,
    no. 3-4, pp. 169–200, 1992.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] P. Ekman，“关于基本情感的论证，” *认知与情感*，第6卷，第3-4期，第169–200页，1992年。'
- en: '[15] J. R. Averill, “What are emotions, really?” *Cognition & Emotion*, vol. 12,
    no. 6, pp. 849–855, 1998.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. R. Averill，“情感究竟是什么？” *认知与情感*，第12卷，第6期，第849–855页，1998年。'
- en: '[16] M. Cabanac, “What is emotion?” *Behavioural processes*, vol. 60, no. 2,
    pp. 69–83, 2002.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Cabanac，“什么是情感？” *行为过程*，第60卷，第2期，第69–83页，2002年。'
- en: '[17] L. F. Barrett, “Are emotions natural kinds?” *Perspectives on psychological
    science*, vol. 1, no. 1, pp. 28–58, 2006.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. F. Barrett，“情感是自然类别吗？” *心理科学视角*，第1卷，第1期，第28–58页，2006年。'
- en: '[18] B. W. Schuller, B. Vlasenko, F. Eyben, M. Wöllmer, A. Stuhlsatz, A. Wendemuth,
    and G. Rigoll, “Cross-corpus acoustic emotion recognition: Variances and strategies,”
    *IEEE Trans. Affect. Comput.*, vol. 1, no. 2, pp. 119–131, 2010.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] B. W. Schuller, B. Vlasenko, F. Eyben, M. Wöllmer, A. Stuhlsatz, A. Wendemuth,
    和 G. Rigoll，“跨语料库的声学情感识别：方差与策略，” *IEEE情感计算汇刊*，第1卷，第2期，第119–131页，2010年。'
- en: '[19] E. Hatfield, J. T. Cacioppo, and R. L. Rapson, “Emotional contagion,”
    *Current directions in psychological science*, vol. 2, no. 3, pp. 96–100, 1993.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] E. Hatfield, J. T. Cacioppo, 和 R. L. Rapson，“情感传染，” *当前心理科学动态*，第2卷，第3期，第96–100页，1993年。'
- en: '[20] R. P. Bagozzi and U. M. Dholakia, “Antecedents and purchase consequences
    of customer participation in small group brand communities,” *International Journal
    of research in Marketing*, vol. 23, no. 1, pp. 45–61, 2006.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] R. P. Bagozzi 和 U. M. Dholakia，“顾客参与小型品牌社区的前因和购买后果，” *国际市场研究期刊*，第23卷，第1期，第45–61页，2006年。'
- en: '[21] S. G. Barsade and D. E. Gibson, “Group emotion: A view from top and bottom.”
    *Res. Manag. Group Teamss*, vol. 41, pp. 81–102, 1998.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. G. Barsade 和 D. E. Gibson，“群体情感：从上到下的视角。” *研究管理：群体与团队*，第41卷，第81–102页，1998年。'
- en: '[22] J. R. Kelly and S. G. Barsade, “Mood and emotions in small groups and
    work teams,” *Organizational behavior and human decision processes*, vol. 86,
    no. 1, pp. 99–130, 2001.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. R. Kelly 和 S. G. Barsade，“小组和工作团队中的情绪和心情，” *组织行为与人类决策过程*，第86卷，第1期，第99–130页，2001年。'
- en: '[23] S. G. Barsade and D. E. Gibson, “Why does affect matter in organizations?”
    *Academy of management perspectives*, vol. 21, no. 1, pp. 36–59, 2007.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. G. Barsade 和 D. E. Gibson，“为什么情感在组织中重要？” *管理学会观点*，第21卷，第1期，第36–59页，2007年。'
- en: '[24] W. Mou, O. Celiktutan, and H. Gunes, “Group-level arousal and valence
    recognition in static images: Face, body and context,” in *2015 11th IEEE International
    Conference and Workshops on Automatic Face and Gesture Recognition (FG)*, vol. 5.   IEEE,
    2015, pp. 1–6.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] W. Mou, O. Celiktutan, 和 H. Gunes，“静态图像中的群体级激发和情感识别：面孔、身体和背景，” 收录于 *2015年第11届IEEE国际自动面部与手势识别会议（FG）*，第5卷。
    IEEE，2015年，第1–6页。'
- en: '[25] A. Dhall, J. Joshi, I. Radwan, and R. Goecke, “Finding happiest moments
    in a social context,” in *Computer Vision–ACCV 2012: 11th Asian Conference on
    Computer Vision, Daejeon, Korea, November 5-9, 2012, Revised Selected Papers,
    Part II 11*.   Springer, 2013, pp. 613–626.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Dhall, J. Joshi, I. Radwan, 和 R. Goecke，“在社会背景中寻找最快乐的时刻，” 收录于 *计算机视觉–ACCV
    2012：第11届亚洲计算机视觉大会，韩国大田，2012年11月5-9日，修订的精选论文，第II部分*。 Springer，2013年，第613–626页。'
- en: '[26] A. Dhall, J. Joshi, K. Sikka, R. Goecke, and N. Sebe, “The more the merrier:
    Analysing the affect of a group of people in images,” in *2015 11th IEEE international
    conference and workshops on automatic face and gesture recognition (FG)*, vol. 1.   IEEE,
    2015, pp. 1–8.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Dhall, J. Joshi, K. Sikka, R. Goecke, 和 N. Sebe，“更多就是更好：分析图像中群体的情感，”
    收录于 *2015年第11届IEEE国际自动面部与手势识别会议（FG）*，第1卷。 IEEE，2015年，第1–8页。'
- en: '[27] A. Dhall, A. Kaur, R. Goecke, and T. Gedeon, “Emotiw 2018: Audio-video,
    student engagement and group-level affect prediction,” in *Proceedings of the
    20th ACM International Conference on Multimodal Interaction*, 2018, pp. 653–656.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Dhall, A. Kaur, R. Goecke, 和 T. Gedeon，“Emotiw 2018：音视频、学生参与和群体级情感预测，”
    收录于 *第20届ACM国际多模态互动会议论文集*，2018年，第653–656页。'
- en: '[28] S. Ghosh, A. Dhall, N. Sebe, and T. Gedeon, “Predicting group cohesiveness
    in images,” in *2019 International Joint Conference on Neural Networks (IJCNN)*.   IEEE,
    2019, pp. 1–8.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Ghosh, A. Dhall, N. Sebe, 和 T. Gedeon，“预测图像中的群体凝聚力，” 收录于 *2019年国际神经网络联合会议（IJCNN）*。
    IEEE，2019年，第1–8页。'
- en: '[29] X. Guo, L. Polania, B. Zhu, C. Boncelet, and K. Barner, “Graph neural
    networks for image understanding based on multiple cues: Group emotion recognition
    and event recognition as use cases,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2020, pp. 2921–2930.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] X. Guo, L. Polania, B. Zhu, C. Boncelet 和 K. Barner，“基于多种线索的图神经网络图像理解：群体情感识别和事件识别作为应用案例”，见
    *IEEE/CVF冬季计算机视觉应用会议论文集*，2020年，pp. 2921–2930。'
- en: '[30] A. Dhall, “Emotiw 2019: Automatic emotion, engagement and cohesion prediction
    tasks,” in *2019 International Conference on Multimodal Interaction*, 2019, pp.
    546–550.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] A. Dhall，“Emotiw 2019：自动情感、参与度和凝聚力预测任务”，见 *2019国际多模态交互会议*，2019年，pp. 546–550。'
- en: '[31] M. Pantic and I. Patras, “Dynamics of facial expression: recognition of
    facial actions and their temporal segments from face profile image sequences,”
    *IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)*, vol. 36,
    no. 2, pp. 433–449, 2006.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. Pantic 和 I. Patras，“面部表情的动态：从面部侧影图像序列中识别面部动作及其时间段”，*IEEE系统、人类和控制论学报，B部分（控制论）*，第36卷，第2期，pp.
    433–449，2006年。'
- en: '[32] G. Sharma, S. Ghosh, and A. Dhall, “Automatic group level affect and cohesion
    prediction in videos,” in *2019 8th International Conference on Affective Computing
    and Intelligent Interaction Workshops and Demos (ACIIW)*.   IEEE, 2019, pp. 161–167.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] G. Sharma, S. Ghosh 和 A. Dhall，“视频中群体级情感和凝聚力的自动预测”，见 *2019年第8届情感计算与智能交互国际会议研讨会与演示（ACIIW）*，IEEE，2019年，pp.
    161–167。'
- en: '[33] K. G. Quach, N. Le, C. N. Duong, I. Jalata, K. Roy, and K. Luu, “Non-volume
    preserving-based fusion to group-level emotion recognition on crowd videos,” *Pattern
    Recognition*, vol. 128, p. 108646, 2022.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] K. G. Quach, N. Le, C. N. Duong, I. Jalata, K. Roy 和 K. Luu，“基于非体积保持的融合方法进行人群视频的群体情感识别”，*模式识别*，第128卷，p.
    108646，2022年。'
- en: '[34] A. Dhall, M. Singh, R. Goecke, T. Gedeon, D. Zeng, Y. Wang, and K. Ikeda,
    “Emotiw 2023: Emotion recognition in the wild challenge,” in *Proceedings of the
    25th International Conference on Multimodal Interaction*, 2023, pp. 746–749.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Dhall, M. Singh, R. Goecke, T. Gedeon, D. Zeng, Y. Wang 和 K. Ikeda，“Emotiw
    2023：野外情感识别挑战”，见 *第25届国际多模态交互会议论文集*，2023年，pp. 746–749。'
- en: '[35] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database
    for facial expression, valence, and arousal computing in the wild,” *IEEE Transactions
    on Affective Computing*, vol. 10, no. 1, pp. 18–31, 2017.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] A. Mollahosseini, B. Hasani 和 M. H. Mahoor，“Affectnet：一个用于面部表情、情感价值和唤醒计算的数据库”，*IEEE情感计算学报*，第10卷，第1期，pp.
    18–31，2017年。'
- en: '[36] J. See, M. H. Yap, J. Li, X. Hong, and S.-J. Wang, “Megc 2019–the second
    facial micro-expressions grand challenge,” in *2019 14th IEEE International Conference
    on Automatic Face & Gesture Recognition (FG 2019)*.   IEEE, 2019, pp. 1–5.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. See, M. H. Yap, J. Li, X. Hong 和 S.-J. Wang，“Megc 2019–第二届面部微表情大挑战”，见
    *2019年第14届IEEE国际自动面部与手势识别会议（FG 2019）*，IEEE，2019年，pp. 1–5。'
- en: '[37] S. Ghosh, A. Dhall, N. Sebe, and T. Gedeon, “Automatic prediction of group
    cohesiveness in images,” *IEEE Transactions on Affective Computing*, vol. 13,
    no. 3, pp. 1677–1690, 2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Ghosh, A. Dhall, N. Sebe 和 T. Gedeon，“图像中群体凝聚力的自动预测”，*IEEE情感计算学报*，第13卷，第3期，pp.
    1677–1690，2020年。'
- en: '[38] A. Dhall, G. Sharma, R. Goecke, and T. Gedeon, “Emotiw 2020: Driver gaze,
    group emotion, student engagement and physiological signal based challenges,”
    in *Proceedings of the 2020 International Conference on Multimodal Interaction*,
    2020, pp. 784–789.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Dhall, G. Sharma, R. Goecke 和 T. Gedeon，“Emotiw 2020：驾驶员注视、群体情感、学生参与度和生理信号挑战”，见
    *2020国际多模态交互会议论文集*，2020年，pp. 784–789。'
- en: '[39] A. Rassadin, A. Gruzdev, and A. Savchenko, “Group-level emotion recognition
    using transfer learning from face identification,” in *Proceedings of the 19th
    ACM international conference on multimodal interaction*, 2017, pp. 544–548.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. Rassadin, A. Gruzdev 和 A. Savchenko，“基于面部识别的组级情感识别的迁移学习”，见 *第19届ACM国际多模态交互会议论文集*，2017年，pp.
    544–548。'
- en: '[40] G. Lu and W. Zhang, “Happiness intensity estimation for a group of people
    in images using convolutional neural networks,” in *2019 3rd international conference
    on electronic information technology and computer engineering (EITCE)*.   IEEE,
    2019, pp. 1707–1710.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] G. Lu 和 W. Zhang，“使用卷积神经网络对图像中一组人的幸福强度进行估计”，见 *2019年第3届国际电子信息技术与计算机工程会议（EITCE）*，IEEE，2019年，pp.
    1707–1710。'
- en: '[41] C. Pan, D. Yu, L. Sijiang, G. Zhen, and Y. Lei, “Group emotion recognition
    based on multilayer hybrid network,” in *2018 IEEE 3rd International Conference
    on Image, Vision and Computing (ICIVC)*.   IEEE, 2018, pp. 173–177.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] C. Pan, D. Yu, L. Sijiang, G. Zhen, 和 Y. Lei，“基于多层次混合网络的群体情感识别，”收录于*2018年第3届IEEE国际图像、视觉与计算会议（ICIVC）*。
    IEEE，2018年，第173–177页。'
- en: '[42] J. Li, S. Roy, J. Feng, and T. Sim, “Happiness level prediction with sequential
    inputs via multiple regressions,” in *Proceedings of the 18th ACM international
    conference on multimodal interaction*, 2016, pp. 487–493.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Li, S. Roy, J. Feng, 和 T. Sim，“通过多重回归预测幸福水平，”收录于*第18届ACM国际多模态互动会议论文集*，2016年，第487–493页。'
- en: '[43] B. Balaji and V. R. M. Oruganti, “Multi-level feature fusion for group-level
    emotion recognition,” in *Proceedings of the 19th ACM international conference
    on multimodal interaction*, 2017, pp. 583–586.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] B. Balaji 和 V. R. M. Oruganti，“用于群体级情感识别的多层次特征融合，”收录于*第19届ACM国际多模态互动会议论文集*，2017年，第583–586页。'
- en: '[44] L. Surace, M. Patacchiola, E. Battini Sönmez, W. Spataro, and A. Cangelosi,
    “Emotion recognition in the wild using deep neural networks and bayesian classifiers,”
    in *Proceedings of the 19th ACM international conference on multimodal interaction*,
    2017, pp. 593–597.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] L. Surace, M. Patacchiola, E. Battini Sönmez, W. Spataro, 和 A. Cangelosi，“使用深度神经网络和贝叶斯分类器在自然环境中进行情感识别，”收录于*第19届ACM国际多模态互动会议论文集*，2017年，第593–597页。'
- en: '[45] N. Liu, Y. Fang, and Y. Guo, “Enhancing feature correlation for bi-modal
    group emotion recognition,” in *Advances in Multimedia Information Processing–PCM
    2018: 19th Pacific-Rim Conference on Multimedia, Hefei, China, September 21-22,
    2018, Proceedings, Part II 19*.   Springer, 2018, pp. 24–34.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] N. Liu, Y. Fang, 和 Y. Guo，“通过增强特征关联进行双模态群体情感识别，”收录于*多媒体信息处理进展–PCM 2018:
    第19届环太平洋多媒体会议，合肥，中国，2018年9月21-22日，会议录，第二部分 19*。 Springer，2018年，第24–34页。'
- en: '[46] S. Garg, “Group emotion recognition using machine learning,” *arXiv preprint
    arXiv:1905.01118*, 2019.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Garg，“使用机器学习进行群体情感识别，”*arXiv 预印本 arXiv:1905.01118*，2019年。'
- en: '[47] B. Nagarajan and V. R. M. Oruganti, “Group emotion recognition in adverse
    face detection,” in *2019 14th IEEE International Conference on Automatic Face
    & Gesture Recognition (FG 2019)*.   IEEE, 2019, pp. 1–5.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] B. Nagarajan 和 V. R. M. Oruganti，“在逆境面部检测中的群体情感识别，”收录于*2019年第14届IEEE国际自动面部与姿态识别会议（FG
    2019）*。 IEEE，2019年，第1–5页。'
- en: '[48] K. Fujii, D. Sugimura, and T. Hamamoto, “Hierarchical group-level emotion
    recognition,” *IEEE Transactions on Multimedia*, vol. 23, pp. 3892–3906, 2020.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] K. Fujii, D. Sugimura, 和 T. Hamamoto，“层次化群体级情感识别，”*IEEE多媒体学报*，第23卷，第3892–3906页，2020年。'
- en: '[49] D. Yu, L. Xingyu, D. Shuzhan, and Y. Lei, “Group emotion recognition based
    on global and local features,” *IEEE Access*, vol. 7, pp. 111 617–111 624, 2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] D. Yu, L. Xingyu, D. Shuzhan, 和 Y. Lei，“基于全局和局部特征的群体情感识别，”*IEEE Access*，第7卷，第111 617–111 624页，2019年。'
- en: '[50] Q. Zhu, Q. Mao, J. Zhang, X. Huang, and W. Zheng, “Towards a robust group-level
    emotion recognition via uncertainty-aware learning,” *arXiv preprint arXiv:2310.04306*,
    2023.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Q. Zhu, Q. Mao, J. Zhang, X. Huang, 和 W. Zheng，“通过不确定性感知学习实现鲁棒的群体级情感识别，”*arXiv
    预印本 arXiv:2310.04306*，2023年。'
- en: '[51] X. Guo, L. F. Polanía, and K. E. Barner, “Group-level emotion recognition
    using deep models on image scene, faces, and skeletons,” in *Proceedings of the
    19th ACM International Conference on Multimodal Interaction*, 2017, pp. 603–608.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Guo, L. F. Polanía, 和 K. E. Barner，“使用深度模型进行图像场景、面部和骨架上的群体级情感识别，”收录于*第19届ACM国际多模态互动会议论文集*，2017年，第603–608页。'
- en: '[52] E. G. Krumhuber, L. I. Skora, H. C. Hill, and K. Lander, “The role of
    facial movements in emotion recognition,” *Nature Reviews Psychology*, vol. 2,
    no. 5, pp. 283–296, 2023.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] E. G. Krumhuber, L. I. Skora, H. C. Hill, 和 K. Lander，“面部运动在情感识别中的作用，”*自然评论心理学*，第2卷，第5期，第283–296页，2023年。'
- en: '[53] M. Sun, J. Li, H. Feng, W. Gou, H. Shen, J. Tang, Y. Yang, and J. Ye,
    “Multi-modal fusion using spatio-temporal and static features for group emotion
    recognition,” in *Proceedings of the 2020 International Conference on Multimodal
    Interaction*, 2020, pp. 835–840.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. Sun, J. Li, H. Feng, W. Gou, H. Shen, J. Tang, Y. Yang, 和 J. Ye，“利用时空和静态特征进行多模态融合以实现群体情感识别，”收录于*2020年国际多模态互动会议论文集*，2020年，第835–840页。'
- en: '[54] A. Petrova, D. Vaufreydaz, and P. Dessus, “Group-level emotion recognition
    using a unimodal privacy-safe non-individual approach,” in *Proceedings of the
    2020 International Conference on Multimodal Interaction*, 2020, pp. 813–820.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Petrova, D. Vaufreydaz, 和 P. Dessus，“使用单模态隐私安全非个体化方法进行群体级情感识别，”收录于*2020年国际多模态互动会议论文集*，2020年，第813–820页。'
- en: '[55] S. Li, H. Lian, C. Lu, Y. Zhao, C. Tang, Y. Zong, and W. Zheng, “Audio-visual
    group-based emotion recognition using local and global feature aggregation based
    multi-task learning,” in *Proceedings of the 25th International Conference on
    Multimodal Interaction*, 2023, pp. 741–745.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] S. Li, H. Lian, C. Lu, Y. Zhao, C. Tang, Y. Zong, 和 W. Zheng，“基于局部和全局特征聚合的多任务学习音视频群体情感识别”，收录于
    *第25届国际多模态交互会议论文集*，2023年，第741–745页。'
- en: '[56] C. Liu, W. Jiang, M. Wang, and T. Tang, “Group level audio-video emotion
    recognition using hybrid networks,” in *Proceedings of the 2020 International
    Conference on Multimodal Interaction*, 2020, pp. 807–812.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] C. Liu, W. Jiang, M. Wang, 和 T. Tang，“基于混合网络的群体级音视频情感识别”，收录于 *2020年国际多模态交互会议论文集*，2020年，第807–812页。'
- en: '[57] Y. Wang, J. Wu, P. Heracleous, S. Wada, R. Kimura, and S. Kurihara, “Implicit
    knowledge injectable cross attention audiovisual model for group emotion recognition,”
    in *Proceedings of the 2020 international conference on multimodal interaction*,
    2020, pp. 827–834.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Wang, J. Wu, P. Heracleous, S. Wada, R. Kimura, 和 S. Kurihara，“用于群体情感识别的隐式知识注入式交叉注意力视听模型”，收录于
    *2020年国际多模态交互会议论文集*，2020年，第827–834页。'
- en: '[58] J. R. Pinto, T. Gonçalves, C. Pinto, L. Sanhudo, J. Fonseca, F. Gonçalves,
    P. Carvalho, and J. S. Cardoso, “Audiovisual classification of group emotion valence
    using activity recognition networks,” in *2020 IEEE 4th International Conference
    on Image Processing, Applications and Systems (IPAS)*.   IEEE, 2020, pp. 114–119.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. R. Pinto, T. Gonçalves, C. Pinto, L. Sanhudo, J. Fonseca, F. Gonçalves,
    P. Carvalho, 和 J. S. Cardoso，“使用活动识别网络进行群体情感效价的视听分类”，收录于 *2020 IEEE第4届图像处理、应用与系统国际会议（IPAS）*。IEEE，2020年，第114–119页。'
- en: '[59] F. Noroozi, C. A. Corneanu, D. Kamińska, T. Sapiński, S. Escalera, and
    G. Anbarjafari, “Survey on emotional body gesture recognition,” *IEEE transactions
    on affective computing*, vol. 12, no. 2, pp. 505–523, 2018.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] F. Noroozi, C. A. Corneanu, D. Kamińska, T. Sapiński, S. Escalera, 和 G.
    Anbarjafari，“情感体态识别调查”，*IEEE情感计算汇刊*，第12卷，第2期，第505–523页，2018年。'
- en: '[60] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    *Neural computation*, vol. 1, no. 4, pp. 541–551, 1989.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    和 L. D. Jackel，“应用反向传播进行手写邮政编码识别”，*神经计算*，第1卷，第4期，第541–551页，1989年。'
- en: '[61] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Proceedings of NeurIPS*, 2012, pp.
    1106–1114.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络进行Imagenet分类”，收录于
    *NeurIPS会议论文集*，2012年，第1106–1114页。'
- en: '[62] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
    by back-propagating errors,” *nature*, vol. 323, no. 6088, pp. 533–536, 1986.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] D. E. Rumelhart, G. E. Hinton, 和 R. J. Williams，“通过反向传播错误学习表示”，*自然*，第323卷，第6088期，第533–536页，1986年。'
- en: '[63] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE transactions on neural networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, 和 G. Monfardini，“图神经网络模型”，*IEEE神经网络汇刊*，第20卷，第1期，第61–80页，2008年。'
- en: '[64] Z. Liu, J. Dong, C. Zhang, L. Wang, and J. Dang, “Relation modeling with
    graph convolutional networks for facial action unit detection,” in *Proceedings
    of the International Conference MultiMedia Modeling*, 2020, pp. 489–501.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Z. Liu, J. Dong, C. Zhang, L. Wang, 和 J. Dang，“用于面部动作单元检测的图卷积网络关系建模”，收录于
    *国际多媒体建模会议论文集*，2020年，第489–501页。'
- en: '[65] X. Wang, D. Zhang, and D.-J. Lee, “Implementing the affective mechanism
    for group emotion recognition with a new graph convolutional network architecture,”
    *IEEE Transactions on Affective Computing*, 2023.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] X. Wang, D. Zhang, 和 D.-J. Lee，“通过一种新的图卷积网络架构实现群体情感识别的情感机制”，*IEEE情感计算汇刊*，2023年。'
- en: '[66] A. Abbas and S. K. Chalup, “Group emotion recognition in the wild by combining
    deep neural networks for facial expression classification and scene-context analysis,”
    in *Proceedings of the 19th ACM international conference on multimodal interaction*,
    2017, pp. 561–568.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Abbas 和 S. K. Chalup，“通过结合深度神经网络进行面部表情分类和场景上下文分析来识别野外的群体情感”，收录于 *第19届ACM国际多模态交互会议论文集*，2017年，第561–568页。'
- en: '[67] B. Sun, Q. Wei, L. Li, Q. Xu, J. He, and L. Yu, “Lstm for dynamic emotion
    and group emotion recognition in the wild,” in *Proceedings of the 18th ACM international
    conference on multimodal interaction*, 2016, pp. 451–457.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] B. Sun, Q. Wei, L. Li, Q. Xu, J. He, 和 L. Yu，“用于动态情感和群体情感识别的LSTM”，收录于
    *第18届ACM国际多模态交互会议论文集*，2016年，第451–457页。'
- en: '[68] Q. Wei, Y. Zhao, Q. Xu, L. Li, J. He, L. Yu, and B. Sun, “A new deep-learning
    framework for group emotion recognition,” in *Proceedings of the 19th ACM International
    Conference on Multimodal Interaction*, 2017, pp. 587–592.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Q. Wei, Y. Zhao, Q. Xu, L. Li, J. He, L. Yu, 和 B. Sun，“一种新的深度学习框架用于群体情感识别，”在*第19届ACM国际多模态互动会议论文集*，2017年，页码587–592。'
- en: '[69] X. Huang, “Group-level human affect recognition with multiple graph kernel
    fusion,” in *INFORMS International Conference on Service Science*.   Springer,
    2022, pp. 127–140.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] X. Huang，“使用多图谱核融合的群体级人类情感识别，”在*INFORMS国际服务科学会议*。Springer，2022年，页码127–140。'
- en: '[70] L. Evtodienko, “Multimodal end-to-end group emotion recognition using
    cross-modal attention,” *arXiv preprint arXiv:2111.05890*, 2021.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] L. Evtodienko，“使用跨模态注意的多模态端到端群体情感识别，”*arXiv预印本arXiv:2111.05890*，2021年。'
- en: '[71] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, “Less
    is more: Clipbert for video-and-language learning via sparse sampling,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2021,
    pp. 7331–7341.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, 和 J. Liu，“少即是多：通过稀疏采样进行视频与语言学习的Clipbert，”在*IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，页码7331–7341。'
- en: '[72] D. Bombari, P. C. Schmid, M. Schmid Mast, S. Birri, F. W. Mast, and J. S.
    Lobmaier, “Emotion recognition: The role of featural and configural face information,”
    *Quarterly Journal of Experimental Psychology*, vol. 66, no. 12, pp. 2426–2442,
    2013.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] D. Bombari, P. C. Schmid, M. Schmid Mast, S. Birri, F. W. Mast, 和 J. S.
    Lobmaier，“情感识别：特征信息与结构信息的作用，”*实验心理学季刊*，第66卷，第12期，页码2426–2442，2013年。'
- en: '[73] L. Tan, K. Zhang, K. Wang, X. Zeng, X. Peng, and Y. Qiao, “Group emotion
    recognition with individual facial emotion cnns and global image based cnns,”
    in *Proceedings of the 19th ACM international conference on multimodal interaction*,
    2017, pp. 549–552.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] L. Tan, K. Zhang, K. Wang, X. Zeng, X. Peng, 和 Y. Qiao，“使用个人面部情感CNN和基于全局图像的CNN进行群体情感识别，”在*第19届ACM国际多模态互动会议论文集*，2017年，页码549–552。'
- en: '[74] J. Zhang, X. Wang, D. Zhang, and D.-J. Lee, “Semi-supervised group emotion
    recognition based on contrastive learning,” *Electronics*, vol. 11, no. 23, p.
    3990, 2022.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Zhang, X. Wang, D. Zhang, 和 D.-J. Lee，“基于对比学习的半监督群体情感识别，”*电子学*，第11卷，第23期，页码3990，2022年。'
- en: '[75] A. Augusma, D. Vaufreydaz, and F. Letué, “Multimodal group emotion recognition
    in-the-wild using privacy-compliant features,” in *Proceedings of the 25th International
    Conference on Multimodal Interaction*, 2023, pp. 750–754.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] A. Augusma, D. Vaufreydaz, 和 F. Letué，“使用符合隐私要求的特征进行现实环境中的多模态群体情感识别，”在*第25届国际多模态互动会议论文集*，2023年，页码750–754。'
- en: '[76] X. Guo, B. Zhu, L. F. Polanía, C. Boncelet, and K. E. Barner, “Group-level
    emotion recognition using hybrid deep models based on faces, scenes, skeletons
    and visual attentions,” in *Proceedings of the 20th ACM International Conference
    on Multimodal Interaction*, 2018, pp. 635–639.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] X. Guo, B. Zhu, L. F. Polanía, C. Boncelet, 和 K. E. Barner，“基于面部、场景、骨架和视觉注意的混合深度模型用于群体级情感识别，”在*第20届ACM国际多模态互动会议论文集*，2018年，页码635–639。'
- en: '[77] A.-S. Khan, Z. Li, J. Cai, Z. Meng, J. O’Reilly, and Y. Tong, “Group-level
    emotion recognition using deep models with a four-stream hybrid network.” in *ICMI*,
    2018, pp. 623–629.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] A.-S. Khan, Z. Li, J. Cai, Z. Meng, J. O’Reilly, 和 Y. Tong，“基于四流混合网络的深度模型用于群体级情感识别。”
    在*ICMI*，2018年，页码623–629。'
- en: '[78] K. Fujii, D. Sugimura, and T. Hamamoto, “Hierarchical group-level emotion
    recognition in the wild,” in *2019 14th IEEE International Conference on Automatic
    Face & Gesture Recognition (FG 2019)*.   IEEE, 2019, pp. 1–5.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] K. Fujii, D. Sugimura, 和 T. Hamamoto，“现实环境中分层的群体级情感识别，”在*2019年第14届IEEE国际自动面部与手势识别会议（FG
    2019）*。IEEE，2019年，页码1–5。'
- en: '[79] A. S. Khan, Z. Li, J. Cai, and Y. Tong, “Regional attention networks with
    context-aware fusion for group emotion recognition,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2021, pp. 1150–1159.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] A. S. Khan, Z. Li, J. Cai, 和 Y. Tong，“具有上下文感知融合的区域注意网络用于群体情感识别，”在*IEEE/CVF计算机视觉应用冬季会议论文集*，2021年，页码1150–1159。'
- en: '[80] K. Slogrove and D. van der Haar, “Group emotion recognition in the wild
    using pose estimation and lstm neural networks,” in *2022 International Conference
    on Artificial Intelligence, Big Data, Computing and Data Communication Systems
    (icABCD)*.   IEEE, 2022, pp. 1–6.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] K. Slogrove 和 D. van der Haar，“使用姿态估计和LSTM神经网络进行现实环境中的群体情感识别，”在*2022年人工智能、大数据、计算和数据通信系统国际会议（icABCD）*。IEEE，2022年，页码1–6。'
- en: '[81] S. Ottl, S. Amiriparian, M. Gerczuk, V. Karas, and B. Schuller, “Group-level
    speech emotion recognition utilising deep spectrum features,” in *Proceedings
    of the 2020 International Conference on Multimodal Interaction*, 2020, pp. 821–826.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] S. Ottl, S. Amiriparian, M. Gerczuk, V. Karas 和 B. Schuller，“利用深度频谱特征进行群体级语音情感识别”，见
    *2020年国际多模态交互会议论文集*，2020年，第821–826页。'
- en: '[82] D. Li, R. Luo, and S. Sun, “Group-level emotion recognition based on faces,
    scenes, skeletons features,” in *Eleventh International Conference on Graphics
    and Image Processing (ICGIP 2019)*, vol. 11373.   SPIE, 2020, pp. 46–51.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] D. Li, R. Luo 和 S. Sun，“基于面部、场景、骨架特征的群体级情感识别”，见 *第十一届国际图形与图像处理会议（ICGIP
    2019）*，第11373卷。 SPIE，2020年，第46–51页。'
- en: '[83] A. Gupta, D. Agrawal, H. Chauhan, J. Dolz, and M. Pedersoli, “An attention
    model for group-level emotion recognition,” in *Proceedings of the 20th ACM International
    Conference on Multimodal Interaction*, 2018, pp. 611–615.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] A. Gupta, D. Agrawal, H. Chauhan, J. Dolz 和 M. Pedersoli，“用于群体级情感识别的注意力模型”，见
    *第20届ACM国际多模态交互会议论文集*，2018年，第611–615页。'
- en: '[84] K. Wang, X. Zeng, J. Yang, D. Meng, K. Zhang, X. Peng, and Y. Qiao, “Cascade
    attention networks for group emotion recognition with face, body and image cues,”
    in *Proceedings of the 20th ACM international conference on multimodal interaction*,
    2018, pp. 640–645.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] K. Wang, X. Zeng, J. Yang, D. Meng, K. Zhang, X. Peng 和 Y. Qiao，“用于群体情感识别的级联注意力网络，结合面部、身体和图像线索”，见
    *第20届ACM国际多模态交互会议论文集*，2018年，第640–645页。'
- en: '[85] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser 和 I. Polosukhin，“注意力机制是你所需的一切”，*神经信息处理系统进展*，第30卷，2017年。'
- en: '[86] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu,
    Y. Xu *et al.*, “A survey on vision transformer,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 45, no. 1, pp. 87–110, 2022.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C.
    Xu, Y. Xu *等*，“关于视觉变换器的调查”，*IEEE模式分析与机器智能汇刊*，第45卷，第1期，第87–110页，2022年。'
- en: '[87] H. Xie, M.-X. Lee, T.-J. Chen, H.-J. Chen, H.-I. Liu, H.-H. Shuai, and
    W.-H. Cheng, “Most important person-guided dual-branch cross-patch attention for
    group affect recognition,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2023, pp. 20 598–20 608.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. Xie, M.-X. Lee, T.-J. Chen, H.-J. Chen, H.-I. Liu, H.-H. Shuai 和 W.-H.
    Cheng，“用于群体情感识别的最重要人员引导的双分支交叉补丁注意力”，见 *IEEE/CVF国际计算机视觉会议论文集*，2023年，第20 598–20 608页。'
- en: '[88] X. Huang, A. Dhall, R. Goecke, M. Pietikäinen, and G. Zhao, “Analyzing
    group-level emotion with global alignment kernel based approach,” *IEEE Transactions
    on Affective Computing*, vol. 13, no. 2, pp. 713–728, 2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] X. Huang, A. Dhall, R. Goecke, M. Pietikäinen 和 G. Zhao，“基于全局对齐核的方法分析群体级情感”，*IEEE情感计算汇刊*，第13卷，第2期，第713–728页，2019年。'
- en: '[89] F. Wang and H. Liu, “Understanding the behaviour of contrastive loss,”
    in *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    2021, pp. 2495–2504.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] F. Wang 和 H. Liu，“理解对比损失的行为”，见 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第2495–2504页。'
- en: '[90] X. Wang, D. Zhang, H.-Z. Tan, and D.-J. Lee, “A self-fusion network based
    on contrastive learning for group emotion recognition,” *IEEE Transactions on
    Computational Social Systems*, vol. 10, no. 2, pp. 458–469, 2022.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] X. Wang, D. Zhang, H.-Z. Tan 和 D.-J. Lee，“基于对比学习的自融合网络用于群体情感识别”，*IEEE计算社会系统汇刊*，第10卷，第2期，第458–469页，2022年。'
- en: '[91] S. N. Shamsi, B. P. Singh, and M. Wadhwa, “Group affect prediction using
    multimodal distributions,” in *2018 IEEE Winter Applications of Computer Vision
    Workshops (WACVW)*.   IEEE, 2018, pp. 77–83.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] S. N. Shamsi, B. P. Singh 和 M. Wadhwa，“使用多模态分布进行群体情感预测”，见 *2018年IEEE计算机视觉冬季应用研讨会（WACVW）*。
    IEEE，2018年，第77–83页。'
- en: '[92] A. Dhall, R. Goecke, J. Joshi, J. Hoey, and T. Gedeon, “Emotiw 2016: Video
    and group-level emotion recognition challenges,” in *Proceedings of the 18th ACM
    international conference on multimodal interaction*, 2016, pp. 427–432.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] A. Dhall, R. Goecke, J. Joshi, J. Hoey 和 T. Gedeon，“Emotiw 2016：视频和群体级情感识别挑战”，见
    *第18届ACM国际多模态交互会议论文集*，2016年，第427–432页。'
- en: '[93] A. Cerekovic, “A deep look into group happiness prediction from images,”
    in *Proceedings of the 18th ACM international conference on multimodal interaction*,
    2016, pp. 437–444.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. Cerekovic，“对图像中群体幸福预测的深入探讨”，见 *第18届ACM国际多模态交互会议论文集*，2016年，第437–444页。'
- en: '[94] D. Guo, K. Wang, J. Yang, K. Zhang, X. Peng, and Y. Qiao, “Exploring regularizations
    with face, body and image cues for group cohesion prediction,” in *2019 International
    Conference on Multimodal Interaction*, 2019, pp. 557–561.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] D. Guo，K. Wang，J. Yang，K. Zhang，X. Peng，和 Y. Qiao，“探索面部、身体和图像线索的正则化用于群体凝聚力预测”，发表于*2019国际多模态交互会议*，2019年，第557–561页。'
- en: '[95] T. Xuan Dang, S.-H. Kim, H.-J. Yang, G.-S. Lee, and T.-H. Vo, “Group-level
    cohesion prediction using deep learning models with a multi-stream hybrid network,”
    in *2019 International Conference on Multimodal Interaction*, 2019, pp. 572–576.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] T. Xuan Dang，S.-H. Kim，H.-J. Yang，G.-S. Lee，和 T.-H. Vo，“使用多流混合网络的深度学习模型进行群体级凝聚力预测”，发表于*2019国际多模态交互会议*，2019年，第572–576页。'
- en: '[96] B. Zhu, X. Guo, K. Barner, and C. Boncelet, “Automatic group cohesiveness
    detection with multi-modal features,” in *2019 International Conference on Multimodal
    Interaction*, 2019, pp. 577–581.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] B. Zhu，X. Guo，K. Barner，和 C. Boncelet，“基于多模态特征的自动群体凝聚力检测”，发表于*2019国际多模态交互会议*，2019年，第577–581页。'
- en: '[97] E. R. Smith and D. M. Mackie, “Dynamics of group-based emotions: Insights
    from intergroup emotions theory,” *Emotion Review*, vol. 7, no. 4, pp. 349–354,
    2015.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] E. R. Smith 和 D. M. Mackie，“基于群体的情感动态：来自群体情感理论的见解”，*情感评论*，第7卷，第4期，第349–354页，2015年。'
- en: '[98] Y. Xia, W. Zheng, Y. Wang, H. Yu, J. Dong, and F.-Y. Wang, “Local and
    global perception generative adversarial network for facial expression synthesis,”
    *IEEE Transactions on Circuits and Systems for Video Technology*, vol. 32, no. 3,
    pp. 1443–1452, 2021.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Xia，W. Zheng，Y. Wang，H. Yu，J. Dong，和 F.-Y. Wang，“用于面部表情合成的局部和全局感知生成对抗网络”，*IEEE电路与系统视频技术汇刊*，第32卷，第3期，第1443–1452页，2021年。'
- en: '[99] S. Latif, A. Shahid, and J. Qadir, “Generative emotional ai for speech
    emotion recognition: The case for synthetic emotional speech augmentation,” *Applied
    Acoustics*, vol. 210, p. 109425, 2023.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] S. Latif，A. Shahid，和 J. Qadir，“用于语音情感识别的生成性情感 AI：合成情感语音增强的案例”，*应用声学*，第210卷，第109425页，2023年。'
- en: '[100] D. Kim and B. C. Song, “Emotion-aware multi-view contrastive learning
    for facial emotion recognition,” in *European Conference on Computer Vision*.   Springer,
    2022, pp. 178–195.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] D. Kim 和 B. C. Song，"情感感知的多视角对比学习用于面部情感识别"，发表于*欧洲计算机视觉会议*。  Springer，2022年，第178–195页。'
- en: '[101] J. Liu, S. Chen, L. Wang, Z. Liu, Y. Fu, L. Guo, and J. Dang, “Multimodal
    emotion recognition with capsule graph convolutional based representation fusion,”
    in *ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2021, pp. 6339–6343.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] J. Liu，S. Chen，L. Wang，Z. Liu，Y. Fu，L. Guo，和 J. Dang，“基于胶囊图卷积的表示融合的多模态情感识别”，发表于*ICASSP
    2021-2021 IEEE国际声学、语音与信号处理会议（ICASSP）*。  IEEE，2021年，第6339–6343页。'
