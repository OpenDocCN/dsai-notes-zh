- en: 'Machine Learning 1: Lesson 12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第12课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-12-6c2512e005a3](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-12-6c2512e005a3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-12-6c2512e005a3](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-12-6c2512e005a3)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从* [*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*中的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: '[Video](https://youtu.be/5_xFdhfUnvQ) / [Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[视频](https://youtu.be/5_xFdhfUnvQ) / [笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)'
- en: I thought what we might do today is to finish off where we were in this Rossmann
    notebook looking at time series forecasting and structured data analysis. Then
    we might do a little mini review of everything we’ve learnt because believe it
    or not, this is the end. There is nothing more to know about machine learning
    rather than everything that you’re going to learn next semester and for the rest
    of your life. But anyway, I got nothing else to teach. So I’ll do a little review
    and then we’ll cover the most important part of the course which is like thinking
    about how are ways to think about how to use this kind of technology appropriately,
    and effectively in a way it’ll be a positive impact on society.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我想今天我们可以完成在这个Rossmann笔记本中的工作，看一下时间序列预测和结构化数据分析。然后我们可能会对我们学到的一切进行一个小小的复习，因为信不信由你，这就是结尾。关于机器学习没有更多需要知道的东西，只有你将在下个学期和余生中学到的一切。但无论如何，我没有别的要教的了。所以我会做一个小小的复习，然后我们将涵盖课程中最重要的部分，那就是思考如何正确、有效地使用这种技术，以及如何让它对社会产生积极影响的方式。
- en: Last time, we got to the point where we talked a bit about this idea that when
    we were looking at building this CompetitionMonthsOpen derived variable but we
    actually truncated it down to be no more than 24 months and we talked about the
    reason why being that we actually wanted to use it as a categorical variable because
    categorical variables, thanks to embeddings, have more flexibility in how the
    neural net can use them. And so that was kind of where we left off.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 上次，我们谈到了这样一个想法，当我们试图构建这个CompetitionMonthsOpen派生变量时，实际上我们将其截断为不超过24个月，我们谈到了原因，因为我们实际上希望将其用作分类变量，因为分类变量，由于嵌入，具有更多的灵活性，神经网络可以如何使用它们。所以这就是我们离开的地方。
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s keep working through this. Because what’s happening in this notebook is
    stuff which is probably going to apply to most time series datasets that you work
    with. As we talked about although we used `df.apply` here, this is something where
    it’s running a piece of Python code over every row and that’s terrifically slow.
    So we only do that if we can’t find a vectorized pandas or numpy function that
    can do it to the whole column at once. But in this case, I couldn’t find a way
    to convert a year and a week number into a date without using arbitrary Python.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行下去。因为这个笔记本中发生的事情可能适用于你处理的大多数时间序列数据集。正如我们所讨论的，虽然我们在这里使用了`df.apply`，但这是在每一行上运行一段Python代码，速度非常慢。所以只有在找不到可以一次对整列进行操作的矢量化pandas或numpy函数时才这样做。但在这种情况下，我找不到一种方法可以在不使用任意Python的情况下将年份和周数转换为日期。
- en: Also worth remembering this idea of a lambda function. Anytime you’re trying
    to apply a function to every row of something or every element of a tensor, if
    there isn’t a vectorized version already, you are going to have to call something
    like `DataFrame.apply` which will run a function you pass to every element. So
    this is basically a map in functional programming since very often the function
    you want to pass to it is something you’re just going to use once and then throw
    it away. It’s really common to use this lambda approach. So this lambda is creating
    a function just for the purpose of telling `df.apply` what to use.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得记住这个lambda函数的概念。每当你尝试将一个函数应用到某个东西的每一行或张量的每个元素时，如果没有已经存在的矢量化版本，你将不得不调用像`DataFrame.apply`这样的东西，它将运行你传递给每个元素的函数。所以这基本上是函数式编程中的映射，因为很多时候你想要传递给它的函数是你只会使用一次然后丢弃的东西。使用这种lambda方法非常常见。所以这个lambda是为了告诉`df.apply`要使用什么而创建的函数。
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We could also have written this in a different way [[3:16](https://youtu.be/5_xFdhfUnvQ?t=196)].
    The following two cells are the same thing:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用不同的方式来写这个 [[3:16](https://youtu.be/5_xFdhfUnvQ?t=196)]。以下两个单元格是相同的：
- en: '![](../Images/ae1503a2ae88a35b80650db9137bf7af.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae1503a2ae88a35b80650db9137bf7af.png)'
- en: One approach is to define the function (`create_promo2since(x)`) and then pass
    it by name, or the other is to define the function in place using lambda. So if
    you are not comfortable creating and using lambda, it’s a good thing to practice
    and playing around with `df.apply` is a good way to practice it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是定义函数（`create_promo2since(x)`），然后通过名称传递它，另一种方法是使用lambda在现场定义函数。所以如果你不熟悉创建和使用lambda，练习和玩弄`df.apply`是一个很好的练习方法。
- en: Durations [[4:32](https://youtu.be/5_xFdhfUnvQ?t=272)]
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续时间 [[4:32](https://youtu.be/5_xFdhfUnvQ?t=272)]
- en: 'Let’s talk about this durations section which may at first seem a little specific
    but actually it turns out not to be. What we are going to do is we’re going to
    look at three fields: `“Promo”`, `“StateHoliday”`, `“SchoolHoliday”`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈这个持续时间部分，起初可能看起来有点具体，但实际上并不是。我们要做的是看三个字段：`“促销”`、`“州假期”`、`“学校假期”`
- en: 'So basically what we have is a table of:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上我们有一个表格：
- en: for each store for each date, does that store have a promo going on that date
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个商店，对于每个日期，那个商店在那天有促销活动
- en: is there a school holiday in that region of that store that date
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那个地区的那家店铺在那天有学校假期吗
- en: is there a state holiday in that region for that store that date
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那个地区的那家店铺在那天有州假期吗
- en: These kind of things are events. And time series with events are very very common.
    If you are looking at oil and gas drilling data, you’re trying to say the flow
    through this pipe, here is an event representing when it set off some alarm, or
    here is an event where the drill got stuck, or whatever. So like most time series,
    at some level, will tend to represent some events. The fact that an event happened
    at at time is interesting itself, but very often a time series will also show
    something happening before and after the event. For example, in this case, we
    are doing grocery sales prediction. If there’s a holiday coming up, it’s quite
    likely that sales will be higher before and after the holiday, and lower during
    the holiday if this if this is a city based store. Because you have to stock up
    before you go away to bring things with you, then when you come back, you’ll have
    to refill the fridge, for instance. Although we don’t have to do this kind of
    feature engineering to create features specifically about this is before or after
    a holiday, the neural net, the more we can give the neural net the kind of information
    it needs, the less it’s going to have to learn it. The less it’s going to have
    to learn it, the more we can do with the data we already have and the more we
    can do with the size architecture we already have. So feature engineering even
    with stuff like neural nets is still important because it means that we will be
    able to get better results with whatever limited data we have, whatever limited
    computation we have.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事情是事件。带有事件的时间序列非常常见。如果你正在查看石油和天然气钻探数据，你试图说的是通过这根管道的流量，这里是一个代表何时触发了某个警报的事件，或者这里是一个钻头卡住的事件，或者其他。所以像大多数时间序列一样，某种程度上会倾向于代表一些事件。事件发生在某个时间点本身就很有趣，但很多时候时间序列也会显示事件发生前后的情况。例如，在这种情况下，我们正在进行杂货销售预测。如果即将到来一个假期，销售额在假期前后很可能会更高，在假期期间会更低，如果这是一个城市店铺的话。因为你要在离开前备货带东西，然后回来时，你就得重新填满冰箱，例如。虽然我们不必进行这种特征工程来专门创建关于假期前后的特征，但是神经网络，我们能够给神经网络提供它需要的信息，它就不必学习这些信息。它学习的越少，我们就能够利用我们已有的数据做更多事情，利用我们已有的规模架构做更多事情。因此，即使对于神经网络这样的东西，特征工程仍然很重要，因为这意味着我们将能够利用我们拥有的有限数据取得更好的结果，利用我们拥有的有限计算能力取得更好的结果。
- en: 'So the basic idea here, therefore, is when we have events in our time series,
    we want to create two new columns for each event [[7:20](https://youtu.be/5_xFdhfUnvQ?t=440)]:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里的基本思想是，当我们的时间序列中有事件时，我们希望为每个事件创建两个新列[[7:20](https://youtu.be/5_xFdhfUnvQ?t=440)]：
- en: How long is it going to be until the next time this event happens.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有多久这个事件再次发生。
- en: How long has it been since the last time that event happened.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上次那个事件发生已经多久了。
- en: So in other words, how long until the next state holiday, how long since the
    previous state holiday. So that’s not something which I am aware of as existing
    as a library or anything like that. So I wrote up here by hand.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，距离下一个州假期还有多久，距离上一个州假期已经多久了。所以这不是我知道存在的库或任何其他东西。所以我手工写在这里。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So importantly, I need to do this by store. So I want to say, for this store,
    when was this store’s last promo (i.e. how long has it been since the last time
    it had a promo), how long it will be until the next time it has a promo, for instance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，重要的是，我需要按店铺来做这个。所以我想说，对于这家店铺，上次促销是什么时候（即自上次促销以来多长时间），下次促销还有多长时间，例如。
- en: 'Here is what I’m going to do. I’m going to create a little function that’s
    going to take a field name and I’m going to pass it each of `Promo` and then `StateHoliday`,
    and then `SchoolHoliday`. So let’s do school holiday for example. So we say field
    equals school holiday, and then we’ll say `get_elapsed(''SchoolHoliday'', ''After'')`.
    So let me show you what that’s going to do. We are going to first of all sort
    by store and date. Now when we loop through this, we are going to be looping through
    within a store. So store #1, January the first, January the second, January the
    third, and so forth.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我要做的是这样的。我将创建一个小函数，它将接受一个字段名，然后我将依次传递`Promo`、`StateHoliday`和`SchoolHoliday`。让我们以学校假期为例。所以我们说字段等于学校假期，然后我们说`get_elapsed('SchoolHoliday',
    'After')`。让我告诉你这将会做什么。我们首先按店铺和日期排序。现在当我们循环遍历时，我们将在店铺内循环遍历。所以店铺＃1，1月1日，1月2日，1月3日，依此类推。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we loop through each store, we are basically going to say is this row a school
    holiday or not [[8:56](https://youtu.be/5_xFdhfUnvQ?t=536)]. If it is a school
    holiday, then we’ll keep track of this variable called `last_date` which says
    this is the last date where we saw a school holiday. So then we are going to append
    to our result the number of days since the last school holiday.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们循环遍历每家店铺时，我们基本上会说这一行是学校假期还是不是[[8:56](https://youtu.be/5_xFdhfUnvQ?t=536)]。如果是学校假期，那么我们将跟踪名为`last_date`的变量，表示我们看到学校假期的最后日期。然后我们将追加到我们的结果中自上次学校假期以来的天数。
- en: '![](../Images/3c11910ad3e0819b2440bec2bb150f54.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c11910ad3e0819b2440bec2bb150f54.png)'
- en: Importance of using `zip` [[9:26](https://youtu.be/5_xFdhfUnvQ?t=566)]
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`zip`的重要性[[9:26](https://youtu.be/5_xFdhfUnvQ?t=566)]
- en: There are a few interesting features. One is the use of [zip](https://docs.python.org/3/library/functions.html#zip).
    I could actually write this much more simply by writing `for row in df.iterrows():`
    then grab the fields we want from each row. It turns out this is 300 times slower
    than the version that I have. Basically, iterating through a DataFrame and extracting
    specific fields out of a row has a lot of overhead. What’s much faster is to iterate
    through a numpy array. So if you take a Series (e.g. `df.Store`), and add `.values`
    after it, that grabs a numpy array of that series.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些有趣的特性。其中一个是使用[zip](https://docs.python.org/3/library/functions.html#zip)。我实际上可以通过编写`for
    row in df.iterrows():`然后从每行中获取我们想要的字段来更简单地编写这个。结果表明，这比我现在的版本慢300倍。基本上，遍历DataFrame并从行中提取特定字段具有很多开销。更快的方法是遍历numpy数组。因此，如果您取一个Series（例如`df.Store`），然后在其后添加`.values`，那么就会获取该系列的numpy数组。
- en: '![](../Images/c7efabffa15d38364b495f448ee6cd52.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7efabffa15d38364b495f448ee6cd52.png)'
- en: 'So here are three numpy arrays. One is the store IDs, one is whatever `fld`
    is (in this case, that’s a school holiday), and what is the date. So now what
    I want to do is loop through the first one, the second one, and the third one
    of each of those lists. And this is a really really common pattern. I need to
    do something like this in basically every notebook I write. And the way to do
    it is with zip. So `zip` means loop through each of these lists one at a time.
    Then this here is where we can grab the element out of the first list, the second
    list, and the third list:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有三个numpy数组。一个是商店ID，一个是`fld`是什么（在这种情况下，那是学校假期），还有日期。因此，现在我想要循环遍历每个列表的第一个、第二个和第三个。这是一个非常常见的模式。我基本上在我写的每个笔记本中都需要做类似的事情。而要做到这一点的方法就是使用zip。因此，`zip`意味着逐个循环遍历这些列表。然后这里是我们可以从第一个列表、第二个列表和第三个列表中获取元素的地方：
- en: '![](../Images/54117f6fc69c914ee027d172ffc0434b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54117f6fc69c914ee027d172ffc0434b.png)'
- en: So if you haven’t played around much with zip, that’s a really important function
    to practice with. Like I said, I use it in pretty much every notebook I write
    — all the time you have to loop through a bunch of lists at the same time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您还没有使用zip进行过多尝试，那么这是一个非常重要的函数需要练习。就像我说的，我几乎在我写的每个笔记本中都使用它——每次您都必须同时循环遍历一堆列表。
- en: So we are going to loop through every store, every school holiday, and every
    date [[11:34](https://youtu.be/5_xFdhfUnvQ?t=694)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将循环遍历每个商店、每个学校假期和每个日期。
- en: '**Question**: Is it looping through all the possible combination of each of
    those [[11:44](https://youtu.be/5_xFdhfUnvQ?t=704)]? No. Just 111, 222, etc.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：它是否循环遍历了所有可能的组合？不是。只有111、222等。'
- en: So in this case, we basically want to say let’s grab the first store, the first
    school holiday, and the first date. So for store 1, January the first, school
    holiday was true or false. So if it is a school holiday, I’ll keep track of that
    fact by saying the last time I saw a school was that date, and append how long
    has it been since the last school holiday. And if the store ID is different to
    the last store ID, then I’ve now got to a whole new store, in which case, I have
    to basically reset everything.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，我们基本上想要说让我们获取第一个商店、第一个学校假期和第一个日期。因此，对于商店1，1月1日，学校假期是真还是假。因此，如果是学校假期，我会通过记录上次看到学校的日期来跟踪这一事实，并附加自上次学校假期以来的时间长度。如果商店ID与上一个商店ID不同，那么我现在已经到了一个全新的商店，这种情况下，我基本上必须重置一切。
- en: '**Question**: What will happen to the first points that we don’t have a last
    holiday [[12:39](https://youtu.be/5_xFdhfUnvQ?t=759)]? Yeah, so I just set this
    to some arbitrary starting point (`np.datetime64()`), it’s going to end up with,
    I can’t remember, either the largest or the smallest possible date. You may need
    to replace this with a missing value afterwards or zeros. The nice thing is though,
    thanks to ReLU’s, it’s very easy for a neural net to cut off extreme values. So
    in this case, I didn’t do anything special with it. I ended up with these like
    negative billion date time stamps and it still worked fine.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：对于我们没有最后一个假期的第一个点会发生什么？是的，所以我只是将其设置为一些任意的起始点（`np.datetime64()`），最终会得到，我记不清了，要么是最大的日期，要么是最小的日期。您可能需要在之后用缺失值或零替换它。不过好处是，由于ReLU的存在，神经网络很容易截断极端值。因此，在这种情况下，我没有对其做任何特殊处理。我最终得到了这些像负十亿日期时间戳，但仍然可以正常工作。'
- en: 'The next thing to note is there’s a bunch of stuff that I need to do to both
    the training set and the test set [[13:35](https://youtu.be/5_xFdhfUnvQ?t=815)].
    So in the previous section, I actually added this loop where I go for each of
    the training DataFrame and the test DataFrame, do these things:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要注意的是，我需要对训练集和测试集进行一些处理。在前一节中，我实际上添加了一个循环，对训练DataFrame和测试DataFrame进行以下操作：
- en: '![](../Images/decac3a5cc5d72d2b336a25dcdb1bfa3.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/decac3a5cc5d72d2b336a25dcdb1bfa3.png)'
- en: 'Each cell, I did for each of the data frames:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据框中的每个单元格，我都进行了以下操作：
- en: '![](../Images/515ab2a9132d75084f9044818ed56c81.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/515ab2a9132d75084f9044818ed56c81.png)'
- en: 'Coming up, there are a whole series of cells that I want to run first of all
    for the training set and for the test set. In this case, the way I did that was
    I have two different cells here: one which sets df to be the training set, one
    which set it to be the test set.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，有一系列单元格我首先要为训练集和测试集运行。在这种情况下，我有两个不同的单元格：一个将df设置为训练集，一个将其设置为测试集。
- en: '![](../Images/e2099d26b6139dbf30e052738b6f2776.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2099d26b6139dbf30e052738b6f2776.png)'
- en: The way I use this is, I run just the first cell (i.e. skip the `df=test[columns]`)
    then I run all the cells underneath, so it does it all to the training set. Then
    I come back and run the second cell, then run all the cells underneath. So this
    notebook is not designed to be just run from top to bottom. But it’s designed
    to be run in this particular way. I mentioned that because this can be a handy
    trick to know. You could, of course, put all the stuff underneath in a function
    that you pass the data frame to and call it once with a test set, once with a
    training set. But I kind of like to experiment a bit, more interactively look
    at each step as I go. So this way is an easy way to run something on different
    data frames without turning it into a function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的方法是，我只运行第一个单元格（即跳过 `df=test[columns]`），然后运行下面的所有单元格，这样就可以对训练集进行全部操作。然后我回来运行第二个单元格，然后运行下面的所有单元格。因此，这个笔记本不是设计为从头到尾顺序运行的。但是它被设计为以这种特定方式运行。我提到这一点是因为这可能是一个有用的技巧。当然，你可以将下面的所有内容放在一个函数中，将数据框传递给它，并在测试集上调用一次，在训练集上调用一次。但我更喜欢有点实验，更交互地看着每一步。因此，这种方式是在不将其转换为函数的情况下在不同数据框上运行某些内容的简单方法。
- en: 'If I sort by store and by date, then this is keeping track of the last time
    something happened [[15:11](https://youtu.be/5_xFdhfUnvQ?t=911)]. So `d — last_date`
    is, therefore, going to end up telling me how many days was it since the last
    school holiday:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我按店铺和日期排序，那么这就是在追踪上次发生某事的时间[[15:11](https://youtu.be/5_xFdhfUnvQ?t=911)]。因此
    `d - last_date` 最终会告诉我距离上次学校假期有多少天：
- en: '![](../Images/9a8f08693a5e98d404c3d5713a42561b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a8f08693a5e98d404c3d5713a42561b.png)'
- en: 'So now if I sort date descending and call the exact same function, then it’s
    going to say how long until the next holiday:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我按日期降序排列并调用完全相同的函数，那么它会告诉我距离下一个假期还有多久：
- en: '![](../Images/a8047280b7db4974e8a2b7f347b55a9f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8047280b7db4974e8a2b7f347b55a9f.png)'
- en: So that’s kind of a nice little trick for adding arbitrary event times into
    your time series models. If you are doing, for example, the Ecuadorean groceries
    competition right now, maybe this kind of approach would be useful for various
    events in that as well.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个很好的技巧，可以将任意事件时间添加到你的时间序列模型中。例如，如果你现在正在进行厄瓜多尔杂货比赛，也许这种方法对其中的各种事件也会有用。
- en: 'Do it for state holiday, do it for promo, there we go:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了国家假期，为了促销，我们来做一下：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Rolling function [[16:11](https://youtu.be/5_xFdhfUnvQ?t=971)]
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滚动函数[[16:11](https://youtu.be/5_xFdhfUnvQ?t=971)]
- en: The next thing that we look at here is rolling functions. Rolling in pandas
    is how we create what we call windowing functions. Let’s say I had some data like
    this. What I could do is to say okay let’s create a window around this point of
    like 7 days.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看的下一件事是滚动函数。在 pandas 中，滚动是我们创建所谓的窗口函数的方式。假设我有这样的一些数据。我可以说好，让我们在这个点周围创建一个大约7天的窗口。
- en: '![](../Images/23f779cfc5d51d60c4467304ac6c1cd4.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23f779cfc5d51d60c4467304ac6c1cd4.png)'
- en: Then I could take the average sales in that seven day window. Then I could do
    the same thing over here, take the average sales over that seven day window.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可以取得那七天窗口内的平均销售额。然后我可以在这里做同样的事情，取得那七天窗口内的平均销售额。
- en: '![](../Images/8e68427552fe8d2be1b721ec55c7ccbc.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e68427552fe8d2be1b721ec55c7ccbc.png)'
- en: 'So if do that for every point and join up those averages, you are going to
    end up with a moving average:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果对每个点都这样做，并连接起那些平均值，你最终会得到一个移动平均值：
- en: '![](../Images/f3f7e1161ef2c9bff3be29b194cecb14.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3f7e1161ef2c9bff3be29b194cecb14.png)'
- en: 'The more generic version of moving average is a window function i.e. something
    where you apply to some function to some window of data around each point. Very
    often that windows that I’ve shown here are not actually what you want. If you’re
    trying to build a predictive model, you can’t include the future as part of a
    moving average. So quite often you actually need a window that ends at a point
    (rather than a point being in the middle of the window). So that’ll be our window
    function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 移动平均值的更通用版本是窗口函数，即将某个函数应用于每个点周围的一些数据窗口。很多时候，我在这里展示的窗口实际上并不是你想要的。如果你试图构建一个预测模型，你不能将未来作为移动平均的一部分。因此，通常你实际上需要一个在某个点结束的窗口（而不是点位于窗口中间）。那就是我们的窗口函数：
- en: '![](../Images/f4bbce94fd99e2b398ee09b117546738.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4bbce94fd99e2b398ee09b117546738.png)'
- en: 'Pandas lets you create window arbitrary window functions using this rolling
    here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas允许你使用这里的滚动来创建任意窗口函数：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first argument says how many time steps do I want to apply the function
    to. The second argument says if I’m at the edge, in other words, if I’m at the
    left edge of the above graph, should you make that a missing value because I don’t
    have seven days to average over, or what’s the minimum number of time periods
    to use. So here, I said 1\. Then optionally you can also say do you want to set
    the window at the start of a period, the end of a period, or the middle of a period.
    Then within that, you can then apply whatever function you like. So here, I’ve
    got my weekly, by store, sums. So there’s a nice easy way of getting moving average
    or whatever else.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数表示我想将函数应用到多少个时间步。第二个参数表示如果我处于边缘，换句话说，如果我处于上图的左边缘，你应该将其设置为缺失值，因为我没有七天的平均值，或者要使用的最小时间段数是多少。所以这里，我设置为1。然后你还可以选择设置窗口在周期的开始、结束或中间。然后在其中，你可以应用任何你喜欢的函数。所以这里，我有我的按店铺每周求和。所以有一个很简单的方法来得到移动平均值或其他内容。
- en: I should mention [[19:20](https://youtu.be/5_xFdhfUnvQ?t=1160)], if you go to
    the [time series page on Pandas](https://pandas.pydata.org/pandas-docs/stable/timeseries.html),
    there’s a long list of indices on the left. There’s lots because Wes McKinney
    who created this, he was originally in hedge fund trading, I believe. And his
    work was all about time series. So I think Pandas originally was very focused
    on time series and still it’s perhaps the strongest part of Pandas. So if you’re
    playing around with time series computations, you definitely owe it to yourself
    to try to learn this entire API. And there’s a lot of conceptual pieces around
    time stamps, date offsets, resampling and stuff like that to get your head around.
    But it’s totally worth it because otherwise you’ll be writing this stuff as loops
    by hand. It’s going to take you a lot longer than leveraging what Pandas already
    does. And of course Pandas will do it in highly optimized vectorized C code for
    you, whereas your version is going to loop in Python. So definitely worth, if
    you are doing stuff in time series, learning the full Pandas time series API.
    They are just about as strong as any time series API out there.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该提到，如果你去 [Pandas 的时间序列页面](https://pandas.pydata.org/pandas-docs/stable/timeseries.html)，左侧有一个很长的索引列表。这是因为
    Wes McKinney 创造了这个，他最初是在对冲基金交易中，我相信。他的工作都是关于时间序列的。所以我认为 Pandas 最初非常专注于时间序列，而且现在它可能仍然是
    Pandas 最强大的部分。所以如果你在处理时间序列计算，你绝对应该尝试学习整个 API。关于时间戳、日期偏移、重采样等方面有很多概念性的内容需要理解。但这绝对值得，否则你将手动编写这些循环。这将比利用
    Pandas 已经做的事情花费更多时间。当然，Pandas 将为你使用高度优化的向量化 C 代码，而你的版本将在 Python 中循环。所以如果你在处理时间序列的工作，学习完整的
    Pandas 时间序列 API 是绝对值得的。它们几乎和其他任何时间序列 API 一样强大。
- en: Okay, so at the end of all that, you can see here’s those starting point values
    I mentioned [[20:56](https://youtu.be/5_xFdhfUnvQ?t=1256)] — slightly on the extreme
    side. So you can see here, 17th of September, store 1 was 13 days after the last
    school holiday. The 16th was 12, 11, 10, so forth.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，经过所有这些，你可以看到这些起始值，我提到的 —— 稍微偏向极端。所以你可以看到，9月17日，商店1距上次学校假期13天。16日是12，11，10，依此类推。
- en: '![](../Images/17132c9974c3677e960115e78ef1a444.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17132c9974c3677e960115e78ef1a444.png)'
- en: 'We are currently in a promotion. Here, this is one day before a promotion:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前处于促销期。这里，这是促销前一天：
- en: '![](../Images/871cfbe40f3f7e4445e0d18486e6c23c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/871cfbe40f3f7e4445e0d18486e6c23c.png)'
- en: And the left of it, we’ve got 9 days after the last promotion and so forth.
    So that’s how we can add kind of event counters to our time series and probably
    always a good idea when you are doing work with time series.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在它的左边，我们在上次促销之后有9天等等。这就是我们如何可以向我们的时间序列添加事件计数器的方式，当你在处理时间序列时，这通常是一个好主意。
- en: Categorical versus continuous [[21:46](https://youtu.be/5_xFdhfUnvQ?t=1306)]
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类与连续 [[21:46](https://youtu.be/5_xFdhfUnvQ?t=1306)]
- en: 'So now we’ve done that, we’ve got lots of columns in our dataset and so we
    split them out into categorical versus continuous columns. We’ll talk more about
    that in the review section, but these are going to be all the things I’m going
    to create an embedding for:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经做到了，我们的数据集中有很多列，所以我们将它们分成分类和连续列。我们将在回顾部分更多地讨论这一点，但这些将是我将为其创建嵌入的所有内容：
- en: '![](../Images/cd1216ef8099e872b94aa95db58efeb4.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd1216ef8099e872b94aa95db58efeb4.png)'
- en: And `contin_vars` are all the things that I’m going to feed directly into the
    model. So for example, we’ve got `CompetitionDistance` so that’s distance to the
    nearest competitor, maximum temperature, and we have a categorical value `DayOfWeek`.
    So here, we’ve got maximum temperature, maybe like 22.1 because they use centigrade
    in Germany, we’ve got distance to nearest competitor, might be 321.7km. Then we’ve
    got day of week, maybe Saturday is a 6\. So the first two numbers are going to
    go straight into our vector that we are going to be feeding into our neural net.
    We will see in a little moment, but we’ll actually normalize them, but more or
    less. But this categorical variable, we are not. We need to put it through an
    embedding. So we will have some embedding matrix of 7 by 4 (e.g. dimension 4 embedding).
    So this will look up the 6th row to get back the four items. So day of week 6
    will turn into length 4 vector which will then get added here.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 而 `contin_vars` 是我将直接输入模型的所有东西。例如，我们有 `CompetitionDistance`，这是到最近竞争对手的距离，最高温度，以及我们有一个分类值
    `DayOfWeek`。所以这里，我们有最高温度，可能是22.1，因为德国使用摄氏度，我们有到最近竞争对手的距离，可能是321.7公里。然后我们有星期几，也许星期六是6。前两个数字将直接进入我们要输入神经网络的向量中。我们将在稍后看到，但实际上我们会对它们进行归一化，或多或少。但这个分类变量，我们不会。我们需要将它通过一个嵌入层。所以我们将有一个
    7x4 的嵌入矩阵（例如，维度为4的嵌入）。这将查找第6行以获取四个项目。所以星期六将变成长度为4的向量，然后添加到这里。
- en: '![](../Images/9bb8c105deda092e15b288cb685fa67f.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bb8c105deda092e15b288cb685fa67f.png)'
- en: So that’s how our continuous and categorical variables are going to work.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的连续和分类变量将如何工作。
- en: 'Then all of our categorical variables will turn them into Pandas categorical
    variables in the same way that we’ve done before [[24:21](https://youtu.be/5_xFdhfUnvQ?t=1461)]:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将所有的分类变量转换为 Pandas 的分类变量，方式与之前相同：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then we are going to apply the same mappings to the test set. If Saturday is
    6 in the training set, this `apply_cats` makes sure that Saturday is also 6 in
    the test set:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将应用相同的映射到测试集。如果在训练集中星期六是6，`apply_cats` 确保在测试集中星期六也是6：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For the continuous variables, make sure they’re all floats because PyTorch expects
    everything to be a float.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续变量，确保它们都是浮点数，因为 PyTorch 期望所有东西都是浮点数。
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So then this is another little trick that I use.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这是我使用的另一个小技巧。
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Both of these cells (above and below) define something called `joined_samp`.
    One of them defines them as the whole training set, one of them defines them as
    a random subset. So the idea is that I do all of my work on the sample, make sure
    it all works well, play around with different hyper parameters and architectures.
    And then when I’m happy with it, I then go back and run this line of code (below)
    to say, okay now make the whole dataset be the sample, then rerun it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个单元格（上面和下面）都定义了一个叫做`joined_samp`的东西。其中一个将它们定义为整个训练集，另一个将它们定义为一个随机子集。所以我的想法是，我在样本上做所有的工作，确保一切都运行良好，尝试不同的超参数和架构。然后当我对此满意时，我会回过头来运行下面这行代码，说，好，现在让整个数据集成为样本，然后重新运行它。
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is a good way, again similar to what I showed you before, it lets you use
    the same cells in your notebook to run first of al on the sample and then go back
    later and run it on the full dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的方法，与我之前向您展示的类似，它让您可以在笔记本中使用相同的单元格首先在样本上运行，然后稍后回来并在完整数据集上运行。
- en: Normalizing data [[25:51](https://youtu.be/5_xFdhfUnvQ?t=1551)]
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标准化
- en: Now that we’ve got that `joined_samp`, we can then pass it to proc_df as we’ve
    done before to grab the dependent variable to deal with missing values. In this
    case, we pass one more thing which is `do_scale=True`. This will subtract the
    mean and divide by the standard deviation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`joined_samp`，我们可以像以前一样将其传递给proc_df来获取因变量以处理缺失值。在这种情况下，我们传递了一个额外的参数`do_scale=True`。这将减去均值并除以标准差。
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The reason for that is that if our first layer, it’s just a matrix multiply.
    So here is our set of weights. And our input has something like 0.001 and something
    else which is 10⁶, for example, then our weight matrix has been initialized to
    be random numbers between 0 and 1\. Then basically10⁶ is going to have gradients
    that are 9 orders of magnitude bigger than 0.001 which is not going to be good
    for optimization. So by normalizing everything to be mean of zero standard deviation
    of 1 to start with, then that means that all of the gradients are going to be
    on the same kind of scale.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为如果我们的第一层只是一个矩阵乘法。这是我们的权重集。我们的输入大约是0.001和另一个是10⁶，例如，然后我们的权重矩阵已经初始化为0到1之间的随机数。然后基本上10⁶的梯度将比0.001大9个数量级，这对优化不利。因此，通过将所有内容标准化为均值为零标准差为1开始，这意味着所有的梯度将在同一种规模上。
- en: '![](../Images/45641142f6dbe5315b026aee21fe76ad.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45641142f6dbe5315b026aee21fe76ad.png)'
- en: We didn’t have to do that in random forests because in random forests, we only
    cared about the sort order. We didn’t care about the values at all. But with linear
    models and things that are built out of layers of linear models i.e. neural nets,
    we care very much about the scale. So `do_scale=True` normalizes our data for
    us. Now since it normalizes our data for us, it returns one extra object `mapper`
    which is an object that contains for each continuous variable what was the mean
    and standard deviation it was normalized with. The reason being that we are going
    to have to use the same mean and standard deviation on the test set because we
    need our test set and our training set to be scaled in the exact same way; otherwise
    they are going to have different meanings.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在随机森林中不需要这样做，因为在随机森林中，我们只关心排序。我们根本不关心值。但是对于线性模型和由线性模型层构建而成的东西，即神经网络，我们非常关心规模。因此，`do_scale=True`为我们归一化我们的数据。现在，由于它为我们归一化了数据，它会返回一个额外的对象`mapper`，其中包含了每个连续变量被归一化时的均值和标准差。原因是我们将不得不在测试集上使用相同的均值和标准差，因为我们需要我们的测试集和训练集以完全相同的方式进行缩放；否则它们将具有不同的含义。
- en: '![](../Images/daddc98cf18f5a74447601279b81bcd1.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/daddc98cf18f5a74447601279b81bcd1.png)'
- en: So these details about making sure that your tests and training set have the
    same categorical codings, the same missing value replacement, and the same scaling
    normalization are really important to get right because if you don’t get it right,
    then your test set is not going to work at all. But if you follow these steps,
    it’ll work fine. We also take the log of the dependent variable and that’s because
    in this Kaggle competition, the evaluation metric was root mean squared percent
    error. Root mean squared percent error means we are being penalized based on the
    ratio between our answer and the correct answer. We don’t have a loss function
    in PyTorch called root mean squared percent error. We could write one, but easier
    is just to take the log of the dependent because the difference between logs is
    the same as the ratio. So by taking the log, we kind of get that for free.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确保您的测试集和训练集具有相同的分类编码、相同的缺失值替换和相同的缩放归一化的细节非常重要，因为如果您没有做对，那么您的测试集根本不会起作用。但是如果您按照这些步骤操作，它将正常工作。我们还对因变量取对数，这是因为在这个Kaggle竞赛中，评估指标是均方根百分比误差。均方根百分比误差意味着我们根据我们的答案和正确答案之间的比率受到惩罚。我们在PyTorch中没有一个叫做均方根百分比误差的损失函数。我们可以编写一个，但更简单的方法是对因变量取对数，因为对数之间的差异与比率相同。因此，通过取对数，我们可以轻松地得到这个效果。
- en: You’ll notice the vast majority of regression competitions on Kaggle use either
    root mean squared percent error or a root mean squared error of the log as their
    evaluation metric [[29:23](https://youtu.be/5_xFdhfUnvQ?t=1763)]. That’s because
    in real world problems, most of the time, we care more about ratios than about
    raw differences. So if you are designing your own project, it’s quite likely that
    you want to think about using log of your dependent variable.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到Kaggle上绝大多数的回归竞赛要么使用均方根百分比误差，要么使用对数的均方根误差作为他们的评估指标。这是因为在现实世界的问题中，大多数时候，我们更关心比率而不是原始差异。因此，如果您正在设计自己的项目，很可能您会考虑使用因变量的对数。
- en: 'So then we create a validation set and as we’ve learned before, most of the
    time if you’ve got a problem involving a time component, your validation set probably
    wants to be the most recent time period rather than a random subset [[30:00](https://youtu.be/5_xFdhfUnvQ?t=1800)].
    So that’s what I do here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一个验证集，正如我们之前学到的，大多数情况下，如果你的问题涉及时间因素，你的验证集可能应该是最近的时间段，而不是一个随机子集。所以这就是我在这里做的：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When I finished modeling and I found an architecture and a set of hyper parameters
    and a number of epochs and all that stuff that works really well, if I want to
    make my model as good as possible, I’ll retrain on the whole thing — including
    the validation set. Now, currently at least, Fast AI assumes that you do have
    a validation set, so my kind of hacky workaround is to set my validation set to
    just be one index which is the first row:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我完成建模并找到一个架构、一组超参数、一定数量的epochs以及所有能够很好工作的东西时，如果我想让我的模型尽可能好，我会重新在整个数据集上进行训练
    — 包括验证集。现在，至少目前为止，Fast AI假设你有一个验证集，所以我的一种折中方法是将我的验证集设置为只有一个索引，即第一行：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That way all the code keeps working but there’s no real validation set. Obviously
    if you do this, you need to make sure that your final training is like the exact
    same hyper parameters, the exact same number of epochs, exactly the same as the
    thing that worked because you don’t actually have a proper validation set now
    to check against.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这样所有的代码都能继续运行，但实际上没有真正的验证集。显然，如果你这样做，你需要确保你的最终训练与之前的完全相同，包括相同的超参数、相同数量的epochs，因为现在你实际上没有一个正确的验证集来进行检查。
- en: '**Question**: I have a question regarding get_elapsed function which we discussed
    before. In get_elapsed function, we are trying to find how many days away is the
    next holiday. So every year, the holidays are more or less fixed, like there will
    be holiday on 4th of July, 25th of December, and there is hardly any change. So
    can’t we just look from previous years and just get a list of all the holidays
    that are going to occur this year [[31:09](https://youtu.be/5_xFdhfUnvQ?t=1869)]?
    Maybe. I mean, in this case, I guess that’s not true for `Promo` and some holidays
    change, like Easter. So this way, I get to write one piece of code that works
    for all of them. And it doesn’t take very long to run. If your dataset was so
    big that this took too long, you could maybe do it on one year and then kind of
    somehow copy it. But in this case, there was no need to. And I always value my
    time over my computer’s time, so I try to keep things as simple as i can.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我有一个关于之前讨论过的get_elapsed函数的问题。在get_elapsed函数中，我们试图找出下一个假期还有多少天。所以每年，假期基本上是固定的，比如7月4日、12月25日都会有假期，几乎没有变化。那么我们不能从以前的年份查找，然后列出今年将要发生的所有假期吗？也许可以。我的意思是，在这种情况下，我猜对于`Promo`和一些假期会改变，比如复活节，这种方法可以适用于所有情况。而且运行起来也不会花太长时间。如果你的数据集太大，导致运行时间太长，你可以在一年内运行一次，然后以某种方式复制。但在这种情况下，没有必要。我总是把我的时间看得比电脑的时间更重要，所以我尽量保持事情尽可能简单。'
- en: Creating a model [[32:31](https://youtu.be/5_xFdhfUnvQ?t=1951)]
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个模型
- en: So now we can create our model. To create our model, we have to create a model
    data object as we always do with Fast AI. So a columnar model data object is just
    a model data object that represents training set, a validation set, and an optional
    test set of standard columnar structured data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建我们的模型。要创建我们的模型，我们必须像在Fast AI中一样创建一个模型数据对象。所以一个列模型数据对象只是一个代表训练集、验证集和可选测试集的标准列结构化数据的模型数据对象。
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We just have to tell it which of the variables should we treat as categorical.
    Then pass in our data frames.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要告诉它哪些变量应该被视为分类变量。然后传入我们的数据框。
- en: For each of our categorical variables, here is the number of categories it has.
    So for each of our embedding matrices, this tells us the number of rows in that
    embedding matrix. Then we define what embedding dimensionality we want. If you
    are doing natural language processing, then the number of dimensions you need
    to capture all the nuance of what a word means and how it’s used has been found
    empirically to be about 600\. It turns out when you do NLP models with embedding
    matrices that are smaller than 600, you don’t get as good a result as you do with
    the size 600\. Beyond 600, it doesn’t seem to improve much. I would say that human
    language is one of the most complex things that we model, so I wouldn’t expect
    you to come across many if any categorical variables that need embedding matrices
    with more than 600 dimensions. At the other end, some things may have pretty simple
    kind of causality. So for example, `StateHoliday` — maybe if something is a holiday
    then it’s just a case like okay at stores that are in the city, there’s some behavior,
    the stores in the country, there’s some other behavior and that’s about it. Maybe
    it’s a pretty simple relationship. So ideally, when you decide what embedding
    size to use, you would kind of use your knowledge about the domain to decide how
    complex is the relationship and so how big embedding do I need. In practice, you
    almost never know that. You only know that because maybe somebody else has previously
    done that research and figured it out like in NLP. So in practice, you probably
    need to use some rule of thumb, and having tried a rule of thumb, you could then
    maybe try a little bit higher, a little bit lower and see what helps. So it’s
    kind of experimental.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的每个分类变量，这里是它所拥有的类别数量。因此，对于我们的每个嵌入矩阵，这告诉我们该嵌入矩阵中的行数。然后我们定义我们想要的嵌入维度。如果你在进行自然语言处理，那么需要捕捉一个词的含义和使用方式的所有细微差别的维度数量经验性地被发现大约是600。事实证明，当你使用小于600的嵌入矩阵进行自然语言处理模型时，结果不如使用大小为600的好。超过600后，似乎没有太大的改进。我会说人类语言是我们建模的最复杂的事物之一，所以我不会指望你会遇到许多或任何需要超过600维度的嵌入矩阵的分类变量。另一方面，有些事物可能具有相当简单的因果关系。例如，`StateHoliday`
    ——也许如果某事是假日，那么在城市中的商店会有一些行为，在乡村中的商店会有一些其他行为，就是这样。也许这是一个相当简单的关系。因此，理想情况下，当你决定使用什么嵌入大小时，你应该利用你对领域的知识来决定关系有多复杂，因此我需要多大的嵌入。实际上，你几乎永远不会知道这一点。你只知道这一点，因为也许别人以前已经做过这方面的研究并找到了答案，就像在自然语言处理中一样。因此，在实践中，你可能需要使用一些经验法则，并尝试一些经验法则后，你可以尝试再高一点，再低一点，看看哪种方法有帮助。所以这有点像实验。
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So here is my rule of thumb [[35:45](https://youtu.be/5_xFdhfUnvQ?t=2145)].
    My rule of thumb is look at how many discrete values the category has (i.e. the
    number of rows in the embedding matrix) and make the dimensionality of the embedding
    half of that. So if . day of week which is the second one, eight rows and four
    columns. Here it is `(c+1)//2` — the number of columns divided by two. But then
    I say don’t go more than 50\. Here you can see for Store (first row), there’s
    116 stores, only have a dimensionality of 50\. Why 50? I don’t know. It seems
    to have worked okay so far. You may find you need something a little different.
    Actually for the Ecuadorian groceries competition, I haven’t really tried playing
    with this but I think we may need some larger embedding sizes. But it’s something
    to fiddle with.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我的经验法则。我的经验法则是看看该类别有多少个离散值（即嵌入矩阵中的行数），并使嵌入的维度为该值的一半。所以如果是星期几，第二个，有八行和四列。这里是`(c+1)//2`
    ——列数除以二。但是我说不要超过50。在这里你可以看到对于商店（第一行），有116家商店，只有一个维度为50。为什么是50？我不知道。到目前为止似乎效果还不错。你可能会发现你需要一些稍微不同的东西。实际上，对于厄瓜多尔杂货比赛，我还没有真正尝试过调整这个，但我认为我们可能需要一些更大的嵌入大小。但这是可以摆弄的东西。
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Question**: As your cardinality size becomes larger and larger, you are creating
    wider and wider embedding matrices. Aren’t you therefore massively risking overfitting
    because if you are choosing 70 parameters, the model can never possibly capture
    all that variations that your data is actually huge [[36:44](https://youtu.be/5_xFdhfUnvQ?t=2204)]?
    That’s a great question and so let me remind you about my golden rule of the difference
    between modern machine learning and old machine learning. In old machine learning,
    we control complexity by reducing the number of parameters. In modern machine
    learning, we control complexity by regularization. So a short answer is no. I’m
    not concerned about overfitting because the way I avoid overfitting is not by
    reducing the number of parameters but by increasing my dropout or increasing my
    weight decay. Now having said that, there’s no point using more parameters for
    a particular embedding than i need. Because regularization is penalizing a model
    by giving it more random data or by actually penalizing weights. So we’d rather
    not use more than we have to. But they are kind of my general rule of thumb for
    designing an architecture is to be generous on the side of the number of parameters.
    In this case, if after doing some work, we kind of felt like the store doesn’t
    actually seem to be that important. Then I might manually go and make change to
    this to make it smaller. Or if I was really finding there’s not enough data here,
    I’m either overfitting or I’m using more regularization than I’m comfortable with,
    then you might go back. But I would always start with being generous with parameters.
    In this case, this model turned out pretty good.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：随着基数大小变得越来越大，您正在创建越来越宽的嵌入矩阵。因此，您是否会因为选择了70个参数而极大地增加过拟合的风险，因为模型永远不可能捕捉到数据实际巨大的所有变化[[36:44](https://youtu.be/5_xFdhfUnvQ?t=2204)]？这是一个很好的问题，所以让我提醒您一下现代机器学习和旧机器学习之间的区别的黄金法则。在旧的机器学习中，我们通过减少参数数量来控制复杂性。在现代机器学习中，我们通过正则化来控制复杂性。所以简短的答案是不。我不担心过拟合，因为我避免过拟合的方式不是通过减少参数数量，而是通过增加丢弃率或增加权重衰减。现在说到这一点，对于特定的嵌入，没有必要使用比我需要的更多的参数。因为正则化是通过给模型更多的随机数据或实际上对权重进行惩罚来惩罚模型。所以我们宁愿不使用比必要更多的参数。但是在设计架构时，我的一般经验法则是在参数数量方面慷慨一些。在这种情况下，如果经过一些工作后，我们觉得商店实际上似乎并不那么重要。那么我可能会手动去修改它，使其更小。或者如果我真的发现这里的数据不够，我要么过拟合了，要么使用的正则化比我感到舒适的要多，那么您可能会回去。但我总是会从参数慷慨的角度开始。在这种情况下，这个模型表现得相当不错。'
- en: 'Okay, now we’ve got a list of tuples containing the number of rows and columns
    of each of our embedding matrices [[38:41](https://youtu.be/5_xFdhfUnvQ?t=2321)].
    And so when we call `get_learner` to create our neural net, that’s the first thing
    we pass in:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们有一个包含每个嵌入矩阵的行数和列数的元组列表[[38:41](https://youtu.be/5_xFdhfUnvQ?t=2321)]。所以当我们调用`get_learner`来创建我们的神经网络时，这是我们传入的第一件事：
- en: '`emb_szs`: how big is each of our embeddings'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emb_szs`: 我们的每个嵌入有多大'
- en: '`len(df.columns)-len(cat_vars)`: how many continuous variables we have'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`len(df.columns)-len(cat_vars)`: 我们有多少连续变量'
- en: '`[1000,500]`: how many activations to create for each layer'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[1000,500]`: 每一层要创建多少个激活'
- en: '`[0.001,0.01]`: what dropout to use for each layer'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[0.001,0.01]`: 每一层使用的丢弃率是多少'
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Then we can go ahead and call `fit`. We fit for a while and we are getting something
    around the 0.1 mark.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以继续调用`fit`。我们训练了一段时间，得到了大约0.1的分数。
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'So I tried running this on the test set and I submitted it to Kaggle last week,
    and here it is [[39:25](https://youtu.be/5_xFdhfUnvQ?t=2365)]:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我尝试在测试集上运行这个，并且上周我把它提交到了Kaggle，这里是结果[[39:25](https://youtu.be/5_xFdhfUnvQ?t=2365)]：
- en: '![](../Images/f5437601f03ff97a21fcbcbef68abc52.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5437601f03ff97a21fcbcbef68abc52.png)'
- en: 'Private score .107, public score .103\. So let’s have a look and see how that
    would go. Let’s start on public:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 私人分数0.107，公共分数0.103。所以让我们看看这将如何进行。让我们从公共排行榜开始：
- en: '![](../Images/3e8aa03fa5c5888126487365aab82984.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e8aa03fa5c5888126487365aab82984.png)'
- en: 340th out of 3000\. That’s not good. Let’s try the private leader board which
    is .107.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在3000个参赛者中排名第340。这不太好。让我们试试私人排行榜，排名是0.107。
- en: '![](../Images/d610fb647c79437c7ebb71c97c777508.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d610fb647c79437c7ebb71c97c777508.png)'
- en: Oh, 5th [[40:30](https://youtu.be/5_xFdhfUnvQ?t=2430)]. So hopefully you are
    now thinking oh, there are some Kaggle competition finishing soon which I entered
    and I spent a lot of time trying to get good results on the public leaderboard.
    I wonder if that was a good idea. The answer is no, that wasn’t. Kaggle public
    leaderboard is not meant to be a replacement for your carefully developed validation
    set. So for example, if you are doing the iceberg competition (which ones are
    ships, which ones are icebergs), then they’ve actually put something like 4,000
    synthetic images into the public leaderboard and none into the private leaderboard.
    So this is one of the really good kind of things that tests you out on Kaggle
    is “are you creating a good validation set and are you trusting it?” Because if
    you are trusting your leaderboard feedback more than your validation feedback,
    then you may find yourself in 350th place when you thought you are in 5th. In
    this case, we actually had a pretty good validation set because as you can see,
    it’s saying somewhere around 0.1 and we actually did get somewhere around 0.1\.
    So in this case, the public leaderboard in this competition was entirely useless.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，第5名[[40:30](https://youtu.be/5_xFdhfUnvQ?t=2430)]。所以希望您现在在想，哦，有一些Kaggle比赛即将结束，我参加了，并且花了很多时间在公共排行榜上取得好成绩。我想知道那是否是个好主意。答案是否定的，那不是。Kaggle公共排行榜并不是要取代您精心开发的验证集。例如，如果您正在进行冰山比赛（哪些是船只，哪些是冰山），那么他们实际上在公共排行榜中放入了大约4000张合成图像，而在私人排行榜中没有放入任何图像。所以这是Kaggle测试您的一个非常好的方面，“您是否创建了一个好的验证集并且是否信任它？”因为如果您更信任排行榜的反馈而不是验证反馈，那么当您认为自己排名第5时，您可能会发现自己排名第350。在这种情况下，我们实际上有一个相当不错的验证集，因为正如您所看到的，它大约是0.1，而我们实际上确实得到了大约0.1。所以在这种情况下，这个比赛的公共排行榜完全没有用。
- en: '**Question**: So in regards to that, how much does the top of the public leaderboard
    actually correspond to the top of a private leaderboard? Because in the churn
    prediction challenge, there’s like 4 people who are just completely above everyone
    else [[42:07](https://youtu.be/5_xFdhfUnvQ?t=2527)]. It totally depends. If they
    randomly sample the public and private leaderboard, then it should be extremely
    indicative. But it might not be. So in this case, the person who was second on
    the public leaderboard did end up winning. The first place on the public leaderboard
    came in 7th. In fact, you can see the little green thing here. Where else, this
    guy jumped 96 places.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：那么，公共排行榜的前几名实际上与私人排行榜的前几名有多少对应呢？因为在流失预测挑战中，有4个人完全超过了其他人[[42:07](https://youtu.be/5_xFdhfUnvQ?t=2527)]。这完全取决于情况。如果他们随机抽取公共和私人排行榜，那么这应该是非常有指示性的。但也可能不是。所以在这种情况下，公共排行榜上的第二名最终赢得了比赛。公共排行榜上的第一名排在第七位。事实上，你可以看到这里的小绿色标记。而另外一个人则跃升了96个名次。'
- en: '![](../Images/3663a4a8e5695d52fd40ae072e9a297f.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3663a4a8e5695d52fd40ae072e9a297f.png)'
- en: If we had entered with the neural net we just looked at, we would have jumped
    350 places. So it just depends. Sometimes they will tell you the public leaderboard
    was randomly sampled. Sometimes they will tell you it’s not. Generally you have
    to figure it out by looking at the correlation between your validation set results
    and the public leaderboard results to see how well they are correlated. Sometimes
    if 2 or 3 people are way ahead of everybody else, they may have found some kind
    of leakage or something like that. That’s often a sign that there’s some trick.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用刚刚看过的神经网络进入，我们将跃升350个名次。所以这取决于情况。有时他们会告诉你公共排行榜是随机抽样的。有时他们会告诉你不是。通常你必须通过查看验证集结果和公共排行榜结果之间的相关性来判断它们之间的相关性有多好。有时，如果有2或3个人远远领先于其他人，他们可能已经发现了某种泄漏或类似的情况。这通常是一种存在某种技巧的迹象。
- en: Okay, so that’s Rossmann and that brings us to the end of all of our material.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这就是Rossmann，这也是我们所有材料的结束。
- en: Review [[44:21](https://youtu.be/5_xFdhfUnvQ?t=44m21s)]
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾[[44:21](https://youtu.be/5_xFdhfUnvQ?t=44m21s)]
- en: 'We’ve learnt two ways to train a model. One is by building a tree and one is
    with SGD. So the SGD approach is a way we can train a model which is a linear
    model or stack of linear layers with nonlinearities between them. Whereas tree
    building specifically will give us a tree. Then tree building, we can combine
    with bagging to create a random forest or with boosting to create a GBM or various
    other slight variations such as extremely randomized trees. So it’s worth reminding
    ourselves of what these things do. So let’s look at some data. Actually, let’s
    look specifically at categorical data. So categorical data, there’s a couple of
    possibilities of what categorical data might look like. Let’s say we got zip code,
    so we’ve got 94003 is our zip code. Then we’ve got like sales, say 50\. For 94131,
    sales of 22, and so forth. So we’ve got some categorical variable. There’s a couple
    of ways we could represent that categorical variable. One would be just to use
    the number. Maybe it wasn’t a number to start. Maybe it wasn’t a number at all.
    Maybe a categorical variable is like San Francisco, New York, Mumbai, and Sydney.
    But we can turn it into a number just by arbitrarily deciding to give them numbers.
    So it ends up being a number. We could just use that kind of arbitrary number.
    So if it turns out that zip codes that are numerically next to each other have
    somewhat similar behavior then the zip code versus sales chart might look something
    like this, for example:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学会了两种训练模型的方法。一种是通过构建树，另一种是使用SGD。因此，SGD方法是一种可以训练线性模型或具有非线性层之间的线性层堆叠的模型的方法。而树构建具体将给我们一棵树。然后，我们可以将树构建与装袋结合起来创建随机森林，或者与提升结合起来创建GBM，或者其他一些略有不同的变体，比如极端随机树。因此，值得提醒自己这些东西是做什么的。所以让我们看一些数据。实际上，让我们具体看看分类数据。因此，分类数据可能看起来有几种可能性。比如我们有邮政编码，所以我们有94003作为我们的邮政编码。然后我们有销售额，比如50。对于94131，销售额为22，等等。因此，我们有一些分类变量。我们可以表示这种分类变量的几种方式。一种是只使用数字。也许一开始它不是一个数字。也许根本不是一个数字。也许一个分类变量是像旧金山、纽约、孟买和悉尼这样的城市。但我们可以通过任意决定给它们编号来将其转换为数字。因此，它最终成为一个数字。我们可以使用这种任意的数字。因此，如果发现相邻的邮政编码具有相似的行为，那么邮政编码与销售额的图表可能看起来像这样，例如：
- en: '![](../Images/3f624e2fa5c2cc3ec071a5f543eca810.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f624e2fa5c2cc3ec071a5f543eca810.png)'
- en: 'Or alternatively, if the two zip codes next each other didn’t have in any ways
    similar sales behavior, you would expect to see something that look more like
    this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果相邻的两个邮政编码在任何方面都没有相似的销售行为，你会期望看到更像这样的情况：
- en: '![](../Images/971be0dda04257dec34dada567b05cc5.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/971be0dda04257dec34dada567b05cc5.png)'
- en: Kind of just all over the place. So they are two possibilities. So what a random
    forest would do if we just encoded zip in this way is it’s going to say, alright,
    I need to find my single best split point — the split point that’s going to make
    the two sides have as smaller standard deviation as possible or mathematically
    equivalently have the lowest root mean squared error. So in this case, it might
    pick here as a first split point because on the left side, there’s one average
    and on the other side, there’s the other average [[48:07](https://youtu.be/5_xFdhfUnvQ?t=2887)].
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有点到处都是。所以有两种可能性。如果我们只是用这种方式对邮政编码进行编码，那么随机森林会做什么是，它会说，好的，我需要找到我的最佳分割点——使两侧的标准差尽可能小或在数学上等效地具有最低均方根误差的分割点。因此，在这种情况下，它可能会选择这里作为第一个分割点，因为在左侧有一个平均值，在另一侧有另一个平均值[[48:07](https://youtu.be/5_xFdhfUnvQ?t=2887)]。
- en: '![](../Images/26253c04a70ff3e78fb8698bf38dd31c.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26253c04a70ff3e78fb8698bf38dd31c.png)'
- en: 'Then for its second split point, it’s going to say how do I split the right
    hand side, and it’s probably going to say I would split here because now we’ve
    got this average and versus this average:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于它的第二个分割点，它会说如何分割右侧，它可能会说我会在这里分割，因为现在我们有了这个平均值和这个平均值：
- en: '![](../Images/b1e07c19147b503bec824f4da5a2162d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1e07c19147b503bec824f4da5a2162d.png)'
- en: Then finally, it’s going to say how do we split the middle it, and it’s going
    to say okay I’ll split right in the middle. So you can see that it’s able to hone
    in on the set of splits it needs even though it kind of does it greedily top down
    one at a time. The only reason it wouldn’t be able to do this is if it was just
    such bad luck that the two halves were always exactly balanced. But even if that
    happens, it’s not going to be the end of the world. It’ll split on something else,
    some other variable and next time around, it’s very unlikely that it’s still going
    to be exactly balanced in both parts of the tree. So in practice, this works just
    fine.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它会说我们如何拆分中间部分，然后它会说好的，我会在中间拆分。所以你可以看到，即使它贪婪地自上而下一次一次地进行拆分，它仍然能够专注于它需要的拆分集合。唯一的原因是如果两半总是完全平衡，那么它就无法做到这一点。但即使发生这种情况，也不会是世界末日。它会在其他地方拆分，下一次，两部分树仍然完全平衡的可能性非常小。因此，在实践中，这完全没问题。
- en: 'In the second case, it can do exactly the same thing [[49:25](https://youtu.be/5_xFdhfUnvQ?t=2965)].
    It’ll say okay which is my best first split even though there’s no relationship
    between one zip code and its neighboring zip code numerically. We can still see
    here, if it splits here, there’s the average on one side and the average on the
    other side is probably about here:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，它可以做同样的事情。即使一个邮政编码与其相邻的邮政编码之间在数字上没有关系。我们仍然可以看到，如果在这里拆分，一侧的平均值，另一侧的平均值可能大约在这里：
- en: '![](../Images/9d0771992a3ed43ddfa349192d10135a.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d0771992a3ed43ddfa349192d10135a.png)'
- en: Then where would it split next? Probably here, because here is the average on
    one side, here’s the average on the other side.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 那么接下来它会在哪里拆分？可能是在这里，因为这一侧的平均值，另一侧的平均值在这里。
- en: '![](../Images/caaae2bf5dcaed1e41cd6a744a886f70.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caaae2bf5dcaed1e41cd6a744a886f70.png)'
- en: So again, can do the same thing. It’s going to need more splits because it’s
    going to end up having to narrow down on each individual large zip code and each
    individual small zip code. But it’s still going to be fine. So when we are dealing
    with building decision trees for random forests or GBM’s or whatever, we tend
    to encode our variables just as ordinals.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，可以做同样的事情。它将需要更多的拆分，因为它将需要缩小到每个单独的大邮政编码和每个单独的小邮政编码。但这仍然没问题。所以当我们处理为随机森林或GBM构建决策树时，我们
    tend to encode our variables just as ordinals.
- en: 'On the other hand [[50:26](https://youtu.be/5_xFdhfUnvQ?t=3026)], if we are
    doing a neural network or like a simplest version like a linear regression or
    logistic regression, the best it could do is that (in green) which is no good
    at all:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们正在做神经网络或者像线性回归或逻辑回归这样的最简单版本，它能做的最好就是（绿色），这一点一点也不好：
- en: '![](../Images/7fbfd684c9198dc825e627685fde6d33.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fbfd684c9198dc825e627685fde6d33.png)'
- en: 'And ditto with this one. It’s going to be like that:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 而且这个也是一样的：
- en: '![](../Images/76cd902836bd22f3071c44ad3d979e0c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76cd902836bd22f3071c44ad3d979e0c.png)'
- en: 'So an ordinal is not going to be a useful encoding for a linear model or something
    that stacks linear and nonlinear models together. So instead, what we do is we
    create a one hot encoding. Like so:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一个序数对于线性模型或将线性和非线性模型堆叠在一起的模型来说并不是一个有用的编码。所以，我们创建一个独热编码。就像这样：
- en: '![](../Images/b8d980a92b2c3dbd0545a4158e5de102.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8d980a92b2c3dbd0545a4158e5de102.png)'
- en: With that encoding, that can effectively create like a little histogram where
    it’s going to have a different coefficient for each level. So that way, it can
    do exactly what it needs to do.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种编码，可以有效地创建一个小直方图，其中每个级别都会有一个不同的系数。这样，它可以做到它需要做的事情。
- en: '![](../Images/979b86ea5b6e40c6de2b53c36c53c1a2.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/979b86ea5b6e40c6de2b53c36c53c1a2.png)'
- en: '**Question**: At what point does that become too tedious for your system [[51:36](https://youtu.be/5_xFdhfUnvQ?t=3096)]?
    Pretty much never. Because remember, in real life, we don’t actually have to create
    that matrix. Instead, we can just have the four coefficients and just do an index
    lookup which is mathematically equivalent to multiply on the one hot encoding.
    So that’s no problem.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在什么时候这对你的系统变得太繁琐？几乎从来没有。因为请记住，在现实生活中，我们实际上不需要创建那个矩阵。相反，我们可以只有四个系数，然后进行索引查找，这在数学上等同于在独热编码上进行乘法。所以这不是问题。
- en: One thing to mention [[52:14](https://youtu.be/5_xFdhfUnvQ?t=3134)]. I know
    you guys have been taught quite a bit of more analytical solutions to things.
    And in analytical solutions to like a linear regression, you can’t solve something
    with this amount of collinearity. In other words, you know something is in Sydney
    if it’s not Mumbai, New York, or San Francisco. So in other words, there’s a hundred
    percent collinearity between the fourth of these classes versus the other three.
    So if you try to solve a linear regression analytically that way, the whole thing
    falls apart. Now note with SGD, we have no such problem. Like SGD why would it
    care? We’re just taking one step along the derivative. It cares a little because
    in the end, the main problem with collinearity is that there’s an infinite number
    of equally good solutions. So in other words, we could increase all of these on
    the left, and decrease this. Or decrease all of these and increase this. And they
    are going to balance out.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有一件事要提到。我知道你们已经学到了更多关于事物的分析解决方案。在像线性回归这样的分析解决方案中，你无法解决这种程度的共线性问题。换句话说，如果不是孟买、纽约或旧金山，你就知道某样东西在悉尼。换句话说，这四个类别中的第四个与其他三个之间存在百分之百的共线性。因此，如果你尝试以这种方式在分析上解决线性回归问题，整个事情就会崩溃。现在请注意，对于
    SGD，我们没有这样的问题。像 SGD 为什么会在乎呢？我们只是沿着导数走一步。它会在乎一点，因为最终，共线性的主要问题在于有无限数量的同样好的解决方案。换句话说，我们可以增加左边的所有这些，减少这个。或者减少所有这些，增加这个。它们会平衡。
- en: '![](../Images/fc8d84c33e90196458df664873579f78.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc8d84c33e90196458df664873579f78.png)'
- en: And when there’s an infinitely large number of good solutions, it means there’s
    a lot of flat spots in the loss surface and it can be harder to optimize. So the
    really easy way to get rid of all those flat spots is to add a little bit of regularization.
    So if we added a little bit of weight decay, like 1e-7 even, then that says these
    are not all equally good anymore, the one which is the best is the one where the
    parameters are the smallest and the most similar to each other, and so that’ll
    again move it back to being a nice loss function.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在无限多个好的解决方案时，意味着损失曲面上有很多平坦区域，这可能会使优化变得更加困难。因此，摆脱所有这些平坦区域的真正简单方法是添加一点正则化。因此，如果我们添加了一点权重衰减，比如1e-7，那么这就表示这些解决方案不再是完全相同的，最好的解决方案是参数最小且彼此最相似的解决方案，这将使其再次成为一个良好的损失函数。
- en: '**Question**: Could you clarify that point you made about why one hot encoding
    wouldn’t be that tedious [[54:03](https://youtu.be/5_xFdhfUnvQ?t=3243)]? Sure.
    If we have one hot encoded vector and we are multiplying it by a set of coefficients,
    then that’s exactly the same thing as simply saying let’s grab the thing where
    the one is. In other words, if we had stored this (`1000`) as a zero, `0100` as
    a one, `0020` as a two, then it’s exactly the same as just saying hey, look up
    that thing in the array.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：您能澄清一下您提到为什么独热编码不会那么繁琐的那一点吗？当然。如果我们有一个独热编码向量，并且将其乘以一组系数，那么这完全等同于简单地说让我们找到其中值为1的那个值。换句话说，如果我们将`1000`存储为零，`0100`存储为一，`0020`存储为二，那么这完全等同于只是说嘿，查找数组中的那个值。'
- en: '![](../Images/a7df1ae3e1d5db68f640173c0a00f8ed.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7df1ae3e1d5db68f640173c0a00f8ed.png)'
- en: So we call that version an embedding. So an embedding is a weight matrix you
    can multiply by one hot encoding. And it’s just a computational shortcut. But
    it’s mathematically the same.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们称这个版本为嵌入。因此，嵌入是一个权重矩阵，您可以将其与独热编码相乘。这只是一个计算快捷方式。但从数学上讲，它是一样的。
- en: There is a key difference between solving linear type model analytically versus
    with SGD [[55:03](https://youtu.be/5_xFdhfUnvQ?t=3303)]. With SGD, we don’t have
    to worry about collinearity and stuff, at least not nearly to the same degree.
    Then the difference between solving a linear or single layer or multi-layer model
    with SGD versus a tree; a tree is going to complain about less things. So in particular,
    you can just use ordinals as your categorical variables and as we learnt just
    before, we also don’t have to worry about normalizing continuous variables for
    a tree, but we do have to worry about it for these SGD trained models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 解决线性模型的解析方法与使用SGD解决的方法之间存在关键差异。使用SGD，我们不必担心共线性等问题，至少不像解析方法那样。然后使用SGD解决线性模型或单层或多层模型与使用树的区别；树会对更少的事情提出异议。特别是，您可以将序数作为您的分类变量，并且正如我们之前学到的，对于树，我们也不必担心对连续变量进行归一化，但是对于这些使用SGD训练的模型，我们必须担心。
- en: Then we also learnt a lot about interpreting, random forests in particular.
    And if you are interested, you may be interested in trying to use those same techniques
    to interpret neural nets. If you want to know which of my features are important
    in a neural net, you could try the same thing; try shuffling each column in turn
    and see how much it changes your accuracy. That’s going to be your feature importance
    for your neural net. Then if you really want to have fun, recognize, then, that
    shuffling that column is just a way of calculating how sensitive the output is
    to that input which in other words is the derivative of the output with respect
    to that input. So therefore, maybe you could just ask PyTorch to give you the
    derivatives with respect to the input directly and see if that gives you the same
    kind of answers.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们还学到了很多关于解释随机森林的知识。如果您感兴趣，您可能会尝试使用相同的技术来解释神经网络。如果您想知道在神经网络中哪些特征很重要，您可以尝试同样的方法；依次对每列进行洗牌，看看它对准确性的影响有多大。这将是您神经网络的特征重要性。然后，如果您真的想玩得开心，认识到，那么，对该列进行洗牌只是计算输出对该输入的敏感性的一种方式，换句话说，就是输出对该输入的导数。因此，也许您可以直接要求PyTorch给您输出关于输入的导数，看看是否会得到相同类型的答案。
- en: You could do the same kind of thing for partial dependence plot. You could try
    doing the exact same thing with your neural net; replace everything in the column
    with the same value, do it for 1960, 1961, 1962, plot that. I don’t know if anybody
    who’s done these things before, not because it’s rocket science but just because
    maybe no one thought of it or it’s not in a library, I don’t know. But if somebody
    tried it, I think you should find it useful. It would make a great blog post.
    Maybe even the paper if you wanted to take it a bit further. So there’s a thought
    on something you can do. So most of those interpretational techniques are not
    particularly specific to random forests. Things like the tree interpreter certainly
    are because they are all about what’s inside the tree.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以对偏依赖图做同样的事情。您可以尝试使用神经网络做完全相同的事情；用相同的值替换列中的所有内容，对1960、1961、1962进行绘图。我不知道有没有人在以前做过这些事情，不是因为这是火箭科学，而只是可能没有人想到或者不在库中，我不知道。但如果有人尝试过，我认为您会发现它很有用。这将成为一篇很棒的博客文章。甚至如果您想进一步，也可以写成论文。所以这是一个您可以尝试的想法。因此，大多数这些解释技术并不特别适用于随机森林。像树解释器这样的东西当然适用，因为它们都是关于树内部的东西。
- en: '**Question**: In tree interpreter, we are looking at the paths and their contributions
    of the features. In neural net case, it will be same with activations, I guess
    the contributions of each activation on their path [[57:42](https://youtu.be/5_xFdhfUnvQ?t=3462)]?
    Yeah, maybe. I don’t know. I haven’t thought about it. **Question continued**:
    How can we make inference out of the activations? **Jeremy**: Be careful saying
    the word “inference” because people normally use the word inference specifically
    to mean the same as the test time prediction. You mean kind of interrogate the
    model. I’m not sure. We should think about that. Actually, Hinton and one of his
    students just published a paper on how to approximate a neural net with a tree
    for this exact reason. I haven’t read the paper yet.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在树解释器中，我们正在查看特征的路径及其贡献。在神经网络的情况下，我猜每个激活在其路径上的贡献会是相同的，对吗？是的，也许。我不知道。我还没有考虑过这个。**问题继续**：我们如何从激活中推断出结论？**Jeremy**：说“推断”这个词要小心，因为人们通常使用“推断”这个词来特指测试时间的预测。你的意思是对模型进行一种询问。我不确定。我们应该考虑一下。实际上，Hinton和他的一位学生刚刚发表了一篇关于如何用树来近似神经网络的论文，就是出于这个原因。我还没有看过这篇论文。'
- en: '**Question**: In linear regression and traditional statistics, one of the things
    that we focused on was statistical significance of the changes and things like
    that. So when thinking about a tree interpreter or even like the waterfall chart
    which I guess is just a visualization. I guess where does that fit in? Because
    we can see like oh yeah this looks important in the sense that it causes large
    changes but how do we know that it’s traditionally statistically significant [[58:43](https://youtu.be/5_xFdhfUnvQ?t=3523)]?
    So most of the time, I don’t care about the traditional statistical significance
    and the reason why is that nowadays, the main driver of statistical significance
    is data volume, not kind of practical importance. And nowadays most of the models
    you build will have so much data that every tiny thing will be statistically significant
    but most of them won’t be practically significant. So my main focus, therefore,
    is practical significance which is does the size of this influence impact your
    business? Statistical significance was much more important when we had a lot less
    data to work with. If you do need to know statistical significance because, for
    example, you have a very small dataset because it’s really expensive to label
    or hard to collect or whatever, or it’s a medical dataset for a rare disease,
    you can always get statistical significance by bootstrapping which is to say that
    you can randomly resample your dataset a number of times, train your model a number
    of times, and you can then see the actual variation in predictions. So that’s
    with bootstrapping, you can turn any model into something that gives you confidence
    intervals. There is [a paper by Michael Jordan](https://arxiv.org/abs/1112.5016)
    which has a technique called the bag of little bootstraps which actually kind
    of takes this a little further and well worth reading if you are interested.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在线性回归和传统统计学中，我们关注的一件事是变化的统计显著性之类的东西。所以在考虑树解释器或者瀑布图，我猜这只是一种可视化。我猜这在哪里适用？因为我们可以看到，哦，是的，这看起来很重要，因为它导致了很大的变化，但我们怎么知道它在传统上是否具有统计显著性？所以大多数时候，我不关心传统的统计显著性，原因是现在，统计显著性的主要驱动因素是数据量，而不是实际重要性。而且现在，您构建的大多数模型都会有如此多的数据，以至于每一个微小的事情都会在统计上显著，但其中大多数在实际上并不重要。因此，我的主要关注点是实际重要性，即这种影响的大小是否影响您的业务？在我们处理的数据较少时，统计显著性更为重要。如果您确实需要了解统计显著性，例如，因为您有一个非常小的数据集，因为标记成本很高或者很难收集，或者是一个罕见疾病的医疗数据集，您总是可以通过自助法来获得统计显著性，也就是说，您可以随机重新对数据集进行多次抽样，多次训练您的模型，然后您可以看到预测的实际变化。因此，通过自助法，您可以将任何模型转化为能够给出置信区间的东西。有一篇由Michael
    Jordan撰写的论文，其中有一种被称为小自助袋的技术，实际上将这一点推进了一步，如果您感兴趣，那么这篇论文是值得一读的。'
- en: '**Question**: You said we don’t need one hot encoding matrix if you are doing
    random forests. What will happen if we do that and how bad can a model be [[1:00:46](https://youtu.be/5_xFdhfUnvQ?t=3646)]?
    We actually did do it. Remember we had that maximum category size and we did create
    one hot encodings, and the reason why we did it was that then our feature importance
    would tell us the importance of the individual levels and our partial dependence
    plot, we could include the individual levels. So it doesn’t necessarily make the
    model worse, it may make it better, but it probably won’t change it much at all.
    In this case, it hardly changed it. **Question continued**: This is something
    that we have noticed on real data also that if cardinality is higher let’s say
    50 levels and if you do one hot encoding, the random forest performs very badly?
    **Jeremy**: Yes, thats right. That’s why in Fast.AI, we have maximum categorical
    size because at some point, your one hot encoded variables become too sparse.
    So I generally cut it off at 6 or 7\. Also because when you get past that, it
    becomes less useful because for the feature importance, there is going to be too
    many levels to really look at. **Question continued**: can it just not look at
    those levels which are not important and just give those significant feature as
    important? **Jeremy**: Yeah, it will be okay. It’s just like once the cardinality
    increases too high, you’re just splitting your data up too much basically, and
    so in practice your ordinal version is likely to be better.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你说如果你在做随机森林时不需要一个独热编码矩阵。如果我们这样做会发生什么，模型会有多糟糕？我们实际上确实这样做了。记得我们有那个最大类别大小，我们确实创建了一个独热编码，我们这样做的原因是我们的特征重要性会告诉我们个别级别的重要性，我们的部分依赖图，我们可以包括个别级别。所以这并不一定会使模型变得更糟，它可能会使它变得更好，但它可能根本不会改变太多。在这种情况下，它几乎没有改变。**问题继续**：这也是我们在真实数据中注意到的一点，如果基数更高，比如说有50个级别，如果你做独热编码，随机森林表现得非常糟糕？**Jeremy**：是的，没错。这就是为什么在Fast.AI中，我们有最大分类大小，因为在某个时候，你的独热编码变量会变得太稀疏。所以我通常在6或7处截断。另外，当你超过那个点时，它变得不那么有用，因为对于特征重要性来说，要看的级别太多了。**问题继续**：它是否可以只查看那些不重要的级别，然后将那些显著的特征视为重要？**Jeremy**：是的，这样也可以。就像一旦基数增加得太高，你基本上就是把你的数据分割得太多了，所以在实践中，你的有序版本可能会更好。'
- en: There is no time to review everything but that’s the key concepts and then remembering
    that the embedding matrix that we can use is likely to have more than just one
    coefficient, we will actually have a dimensionality of a few coefficients which
    isn’t going to be useful for most linear models [[1:02:42](https://youtu.be/5_xFdhfUnvQ?t=3762)].
    But once you’ve got multi-layer models, that’s now creating a representation of
    your category which is quite a lot richer and you can do a lot more with it.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 没有时间来审查所有内容，但这是关键概念，然后记住我们可以使用的嵌入矩阵可能不只有一个系数，实际上我们将有几个系数的维度，这对于大多数线性模型来说并不实用。但一旦你有了多层模型，那么现在就创建了一个相当丰富的类别表示，你可以用它做更多的事情。
- en: Ethics and Data Science [[1:03:13](https://youtu.be/5_xFdhfUnvQ?t=1h3m13s)]
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伦理与数据科学
- en: '[Powerpoint](https://github.com/fastai/fastai/blob/master/courses/ml1/ppt/2017-12-ethics.pptx)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[幻灯片](https://github.com/fastai/fastai/blob/master/courses/ml1/ppt/2017-12-ethics.pptx)'
- en: Let’s now talk about the most important bit. We started off early in this course
    talking about how actually a lot of machine learning is kind of misplaced. People
    focus on predictive accuracy like Amazon has a collaborative filtering algorithm
    for recommending books and they end up the book which it thinks you’re most likely
    to rate highly. So what they end up doing is probably recommending a took that
    you already have or that you already know about and would have bought anyway which
    isn’t very valuable. What they should instead have done is to figure out which
    book can I recommend that would cause you to change your behavior. That way, we
    actually maximize our lift in sales due to recommendations. So this idea of the
    difference between optimizing influencing your actions versus just improving predictive
    accuracy. Improving predictive accuracy is a really important distinction which
    is very rarely discussed in academia or industry, crazily enough. It’s more discussed
    in industry, it’s particularly ignored in most academia. So it’s a really important
    idea which is that in the end the goal of your model, presumably, is to influence
    behavior. So remember, I actually mentioned [a whole paper I have about this](https://www.oreilly.com/ideas/drivetrain-approach-data-products)
    where I introduced this thing called the drivetrain approach where I talk about
    ways to think about how to incorporate machine learning into how do we actually
    influence behavior. So that’s a starting point, but then the next question is
    okay if we are trying to influence behavior, what kind of behavior should we be
    influencing and how and what might it mean when we start influencing behavior.
    Because nowadays a lot of the companies that you are going to end up working at
    a big arse companies and you’ll be building stuff that can influence millions
    of people. So what does that mean?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们谈谈最重要的部分。我们在这门课程的早期谈到了很多机器学习实际上是有些误导的。人们关注预测准确性，比如亚马逊有一个协同过滤算法用于推荐书籍，最终推荐出它认为你最有可能高评的书。结果他们最终可能会推荐一本你已经有或者你已经知道并且本来就会购买的书，这并没有太大价值。他们应该做的是找出哪本书可以推荐给你，从而改变你的行为。这样，我们实际上最大化了由于推荐而带来的销售增长。所以这种优化影响你的行为与仅仅提高预测准确性之间的区别的想法。提高预测准确性是一个非常重要的区别，在学术界或行业中很少被讨论，令人惊讶。在行业中更多地被讨论，但在大多数学术界中被忽视。所以这是一个非常重要的想法，即最终你的模型的目标，想必是影响行为。所以请记住，我实际上提到了[我写的一篇关于这个的论文](https://www.oreilly.com/ideas/drivetrain-approach-data-products)，在那里我介绍了一种叫做传动系统方法的东西，我谈到了如何将机器学习纳入到我们如何实际影响行为的方式中。这是一个起点，但接下来的问题是，如果我们试图影响行为，我们应该影响什么样的行为，以及如何影响，当我们开始影响行为时可能意味着什么。因为现在很多公司，你最终会在那里工作，都是大公司，你将会构建能够影响数百万人的东西。那意味着什么呢？
- en: 'Actually I’m not going to tell you what it means because I don’t know [[1:05:34](https://youtu.be/5_xFdhfUnvQ?t=3934)].
    All I’m going to try and do is make you aware of some of the issues and make you
    believe two things about them:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上我不会告诉你这意味着什么，因为我不知道[[1:05:34](https://youtu.be/5_xFdhfUnvQ?t=3934)]。我要做的只是让你意识到一些问题，并让你相信其中两件事：
- en: You should care.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该关心。
- en: They are big current issues.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些是重大的当前问题。
- en: The main reason I want you to care is because I want you to want to be a good
    person and show you that not thinking about these things will make you a bad person.
    But if you don’t find that convincing, I will tell you this. Volkswagen were found
    to be cheating on their emissions tests. The person who was sent to jail for it
    was the programmer that implemented that piece of code. They did exactly what
    they were told to do. So if you are coming in here thinking hey, I’m just a techie,
    I’ll just do what I’m told, that’s my job. I’m telling you, if you do that, you
    can be sent to jail for doing what you are told, So a) don’t just do what you’re
    told because you can be a bad person and b) you can go to jail.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你关心的主要原因是因为我希望你想成为一个好人，并向你展示不考虑这些事情会让你成为一个坏人。但如果你不觉得有说服力，我会告诉你这个事实。大众汽车被发现在排放测试中作弊。最终被判刑的人是实施那段代码的程序员。他们只是按照指示去做。所以如果你认为自己只是一个技术人员，只是按照指示去做，那是我的工作。我告诉你，如果你这样做，你可能会因为听从指示而被送进监狱，所以a)不要只是听从指示，因为你可能会成为一个坏人，b)你可能会被送进监狱。
- en: Second thing to realize is, in the head of the moment, you’re in a meeting with
    twenty people at work and you’re all talking about how you’re going to implement
    this new feature and everybody is discussing it [[1:06:49](https://youtu.be/5_xFdhfUnvQ?t=4009)].
    And everybody’s like “we could do this and here’s a way of modeling it and then
    we can implement it and here’s these constraints” and there’s some part of you
    that’s thinking am I sure if we should be doing this? That’s not the right time
    to be thinking about that because it’s really hard to step up then and say “excuse
    me I’m not sure this is a good idea”. You actually need to think about how you
    would handle that situation ahead of time. So I want you to think about these
    issues now and realize by the time you’re in the middle of it, you might not even
    realize it’s happening. It’ll just be a meeting like every other meeting and a
    bunch of people will be talking about how to solve this technical question. You
    need to be able to recognize oh, this is actually something with ethical implications.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 第二件要意识到的事情是，在当下，你在工作中与二十个人开会，大家都在讨论如何实施这个新功能，每个人都在讨论[[1:06:49](https://youtu.be/5_xFdhfUnvQ?t=4009)]。每个人都在说“我们可以这样做，这是一种建模的方式，然后我们可以实施它，这是这些约束条件”，你心里有一部分在想我们是否应该这样做？那不是考虑这个问题的正确时机，因为在那时很难站出来说“对不起，我不确定这是一个好主意”。你实际上需要提前考虑如何处理这种情况。所以我希望你现在考虑这些问题，并意识到当你陷入其中时，你可能甚至意识不到正在发生什么。那只是一个像其他会议一样的会议，一群人在讨论如何解决这个技术问题。你需要能够认识到哦，这实际上是一个具有道德影响的事情。
- en: Rachel actually wrote all of these slides. I’m sorry she can’t be here to present
    this because she has studied this in depth. She’s actually being in difficult
    environments herself where she’s seen these things happening, an we know how hard
    it is. But let me give you a sense of what happens.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Rachel实际上写了所有这些幻灯片。很抱歉她不能在这里展示，因为她对这个问题进行了深入研究。她实际上曾身处困难环境，亲眼目睹这些事情发生，我们知道这是多么困难。但让我给你们一个了解发生的情况。
- en: '![](../Images/2ac5562c65d4e7c3b53572a2c250b17f.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ac5562c65d4e7c3b53572a2c250b17f.png)'
- en: So engineers trying to solve engineering problems and causing problems is not
    a new thing. In Nazi Germany, IBM, the group known as Hollerith, Hollerith was
    the original name of IBM and it comes from the guy who actually invented the use
    of punch cards for tracking the US Census. The first mass wide-scale use of punch
    cards for data collection in the world. And that turned into IBM, so at this point,
    was still called Hollerith. So Hollerith sold a punch card system to Nazi Germany
    and so each punch card would code like this is Jew, 8, gypsy, 12, general execution
    for death by gas chamber, 6\. So here is one of these cards describing the right
    way to kill these various people. So a Swiss judge ruled that IBM’s technical
    assistance facilitated the tasks of Nazis and commission of their crimes against
    humanity. This led to the death of something like twenty million civilians. So
    according to the Jewish virtual library where I got these pictures and quotes
    from, their view is that “the destruction of the Jewish people became even less
    important because of the invigorating nature of IBM’s technical achievement was
    only heightened by the fantastical profits to be made”. So this was a long time
    ago and hopefully you won’t end up working at companies that facilitate genocide.
    But perhaps you will [[1:09:59](https://youtu.be/5_xFdhfUnvQ?t=4199)].
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 所以工程师试图解决工程问题并导致问题并不是一件新事。在纳粹德国，IBM，被称为Hollerith的集团，Hollerith是IBM的原始名称，它来自于实际发明用于跟踪美国人口普查的打孔卡的人。这是世界上第一次大规模使用打孔卡进行数据收集。这变成了IBM，所以在这一点上，仍然被称为Hollerith。因此，Hollerith向纳粹德国出售了一个打孔卡系统，每张打孔卡都会编码，比如这是犹太人，8，吉普赛人，12，用毒气室处决，6。这里有一张描述如何杀死这些不同人的卡片。因此，一名瑞士法官裁定IBM的技术协助促成了纳粹分子的任务，并犯下了反人类罪行。这导致了大约二千万平民的死亡。根据我从犹太虚拟图书馆得到这些图片和引用的观点，他们认为“犹太人民的毁灭变得更不重要，因为IBM的技术成就的振奋性质只会因为可以获得的奇幻利润而进一步提高”。这是很久以前的事情，希望你们不会最终在促成种族灭绝的公司工作。但也许你们会。
- en: '![](../Images/479fb1e31f2ee60c47f92e0752c42b08.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/479fb1e31f2ee60c47f92e0752c42b08.png)'
- en: '[https://www.nytimes.com/2017/10/27/world/asia/myanmar-government-facebook-rohingya.html](https://www.nytimes.com/2017/10/27/world/asia/myanmar-government-facebook-rohingya.html)
    [https://www.nytimes.com/2017/10/24/world/asia/myanmar-rohingya-ethnic-cleansing.html](https://www.nytimes.com/2017/10/24/world/asia/myanmar-rohingya-ethnic-cleansing.html)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.nytimes.com/2017/10/27/world/asia/myanmar-government-facebook-rohingya.html](https://www.nytimes.com/2017/10/27/world/asia/myanmar-government-facebook-rohingya.html)
    [https://www.nytimes.com/2017/10/24/world/asia/myanmar-rohingya-ethnic-cleansing.html](https://www.nytimes.com/2017/10/24/world/asia/myanmar-rohingya-ethnic-cleansing.html)'
- en: Because perhaps you’ll go to Facebook who are facilitating genocide right now.
    And I know people at Facebook who are doing this and they had no idea they were
    doing this. Right now, in Facebook, the Rohingya are in the middle of a genocide,
    a Muslim population of Myanmar. Babies are grabbed out of their mother’s arms
    and thrown into fires, people are being killed, hundreds and thousands of refugees.
    When interviewed, the Myanmar generals doing this said we are so grateful to Facebook
    for letting us know about the “Rohingya fake news” that these people are actually
    not human that they are actually animals. Now Facebook did not set out to enable
    the genocide of the Rohingya in Myanmar, no. Instead, what happened is they wanted
    to maximize impression in clicks. So it turns out that for the data scientists
    at Facebook, their algorithms learned that if you take the kinds of stuff people
    are interested in and feed them slightly more extreme versions of that, you are
    actually going to get a lot more impressions and the project managers are saying
    maximize these impressions and people are clicking, and it creates this thing.
    So the potential implications are extraordinary and global. And this is something
    that is literally happening. This is October 2017\. It’s happening now.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你们会去Facebook，他们现在正在促成种族灭绝。我认识在Facebook工作的人，他们根本不知道他们在做什么。现在，在Facebook，罗辛亚人正处于缅甸的种族灭绝中，他们是缅甸的一个穆斯林族群。婴儿被从母亲怀中抢走扔进火里，人们被杀害，数以百千计的难民。在接受采访时，进行这些行为的缅甸将军表示，我们非常感谢Facebook让我们知道“罗辛亚假新闻”，这些人实际上不是人类，他们实际上是动物。现在，Facebook并不是要促成缅甸罗辛亚人的种族灭绝，不是。相反，发生的是他们想要最大化印象和点击量。结果是，Facebook的数据科学家发现，如果你拿人们感兴趣的东西并向他们提供稍微更极端的版本，你实际上会得到更多的印象，项目经理们说要最大化这些印象，人们点击了，这就产生了这种情况。潜在的影响是非常巨大和全球性的。这实际上正在发生。这是2017年10月。正在发生。
- en: '**Question**: I just want to clarify what was happening here. So it was the
    facilitation of fake news or inaccurate media [[1:11:48](https://youtu.be/5_xFdhfUnvQ?t=4308)]?
    Yeah, let me go into it in more detail, so what happened was in mid 2016, Facebook
    fired its human editors. So it was humans that decided how to order things on
    your homepage. Those people got fired and replaced with machine learning algorithms.
    So the machine learning algorithm written by data scientists like you, they had
    nice clear metrics and they were trying to maximize their predictive accuracy
    and be like okay we think if we put this thing higher than this thing, we’ll get
    more clicks. It turned out that these algorithms for putting things on the Facebook
    newsfeed had a tendency to say oh, human nature is that we tend to click on things
    which stimulate our views and are therefore like more extreme versions of things
    we’ve already seen. This is great for the Facebook revenue model of maximizing
    engagement, it looked good on all of their KPIs. At that time, there was some
    negative press about I’m not sure that the stuff that Facebook is now putting
    on their trending section is actually that accurate, but from the point of view
    of the metrics that people are optimizing at Facebook, it looked terrific. So
    way back to October 2016, people started noticing some serious problems.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我只是想澄清这里发生了什么。所以这是在促进虚假新闻或不准确的媒体吗？是的，让我详细介绍一下，发生的事情是在2016年中期，Facebook解雇了它的人类编辑。所以是人类决定如何在你的主页上排序的。这些人被解雇，被机器学习算法取代。所以这些机器学习算法是由像你们这样的数据科学家编写的，他们有清晰的指标，他们试图最大化他们的预测准确性，并且像“我们认为如果我们把这个东西放在比这个东西更高的位置，我们会得到更多的点击”。结果表明，这些用于在Facebook新闻源上放置内容的算法倾向于说“哦，人类的本性是我们倾向于点击那些刺激我们观点的东西，因此更极端版本的我们已经看到的东西”。这对于最大化参与度的Facebook收入模型来说是很好的，它在他们所有的关键绩效指标上看起来都很好。那时，有一些负面新闻，我不确定Facebook现在放在他们的热门部分的东西是否真的那么准确，但从人们在Facebook优化的指标的角度来看，这看起来很棒。所以回到2016年10月，人们开始注意到一些严重的问题。'
- en: '![](../Images/5fee4ecc1d73ffa6aa214b8e773130d4.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fee4ecc1d73ffa6aa214b8e773130d4.png)'
- en: For example, it is illegal to target housing to people of certain races in America.
    That is illegal, and yet a news organization discovered that Facebook was doing
    exactly that in October 2016\. Again, not because somebody in that data science
    team said “let’s make sure black people can’t live in nice neighborhood.” But
    instead, they found that their automatic clustering and segmentation algorithm
    found there was a cluster of people who didn’t like African Americans and if you
    targeted them with these kinds of ads, then they would be more likely to select
    this kind of housing or whatever. But the interesting thing is that even after
    being told about this three times, Facebook still hasn’t fixed it. And that is
    to say these are not just technical issues. They are also economic issues. When
    you start saying the thing that you get paid for (that is ads), you have to change
    the ways that you structure those so that you know you either use more people
    that cost money, or less aggressive on your algorithms to target people based
    on minority group status or whatever, that can impact revenues. The reason I mention
    this is you will at some point in your career find yourself in a conversation
    where you’re thinking “I’m not confident that this is morally okay”, the person
    you are talking to is thinking in their head “this is going to make us a lot of
    money”, you don’t quite ever manage to have a successful conversation because
    you’re talking about different things. So when you are talking to somebody who
    may be more experienced and more senior than you and they may sound like they
    know what they are talking about, just realize that their incentives are not necessarily
    going to be focused on how so I be a good person. Like they are not thinking how
    do I be a bad person but the more time you spend in an industry in my experience,
    the more desensitized you get to this stuff of like maybe getting promotions and
    making money isn’t the most important thing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在美国，将住房针对某些种族是非法的。这是非法的，然而一个新闻机构发现Facebook在2016年10月确实在做这件事。再次强调，并不是因为数据科学团队中的某个人说“让黑人不能住在好社区”。而是，他们发现他们的自动聚类和分割算法发现了一群不喜欢非裔美国人的人，如果你用这种广告针对他们，那么他们更有可能选择这种住房或其他什么。但有趣的是，即使被告知了三次，Facebook仍然没有解决这个问题。这就是说这些不仅仅是技术问题。它们也是经济问题。当你开始说你得到报酬的事情（也就是广告），你必须改变你构建这些方式的方式，这样你要么使用更多花钱的人，要么减少对少数群体或其他基于算法的人的针对性，这可能会影响收入。我提到这个原因是因为在你的职业生涯中，你会在某个时候发现自己在一次对话中，你会想“我不确定这在道德上是否可以接受”，而你正在与之交谈的人在心里想“这会让我们赚很多钱”，你永远不会成功地进行对话，因为你在谈论不同的事情。所以当你与比你更有经验和更资深的人交谈时，他们可能听起来像他们知道他们在谈论什么，只要意识到他们的激励并不一定会集中在如何成为一个好人上。就像他们并不是在想如何成为一个坏人，但在我看来，你在这个行业中花的时间越多，你对这些事情的麻木程度就越高，比如也许得到晋升和赚钱并不是最重要的事情。
- en: '![](../Images/005076e5130b472dc3a7c7cb478e6d4b.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/005076e5130b472dc3a7c7cb478e6d4b.png)'
- en: So for example [[1:15:45](https://youtu.be/5_xFdhfUnvQ?t=4545)], I’ve got a
    lot of friends who are very good at computer vision and some of them have gone
    on to create startups that seem like they are almost handmade to help authoritarian
    governments surveil their citizens. When I ask my friends like have you thought
    about how this could be used in that way, they are generally kind of offended
    that I ask. But I’m asking you to think about this. Wherever you end up working,
    if you end up creating a startup, tools can be used for good or for evil. So I’m
    not saying like don’t create excellent object tracking and detection tools from
    computer vision because you could go on and create a much better surgical intervention
    robot tool kit. I’m just saying be aware of it, think about it, talk about it.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我有很多擅长计算机视觉的朋友，其中一些人已经创建了似乎几乎是为了帮助威权政府监视他们的公民而量身定制的初创公司。当我问我的朋友们，你们有没有考虑过这种方式的使用，他们通常会对我提出的问题感到有点冒犯。但我要求你们考虑这个问题。无论你最终在哪里工作，如果你最终创建了一个初创公司，工具可以被用于善良或邪恶。所以我并不是说不要创建优秀的计算机视觉目标跟踪和检测工具，因为你可以继续创建一个更好的外科手术干预机器人工具包。我只是说要意识到这一点，考虑一下，谈论一下。
- en: '![](../Images/bc4f6d6f340a31ba450f1c9ea3dce8e7.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc4f6d6f340a31ba450f1c9ea3dce8e7.png)'
- en: So here is what I’d find fascinating [[1:16:50](https://youtu.be/5_xFdhfUnvQ?t=4610)].
    There is this really cool thing actually meetup.com did. They think about this.
    They actually thought about this. They thought you know what, if we built a collaborative
    filtering system like we learned about in class to help people decide what meetup
    to go to. It might notice that on the whole in San Francisco, a few more men than
    women tend to go to techie meetups and so it might then start to decide to recommend
    techie meetups to more men than women. As a result of which more men will go to
    techie meetup, as a result of which when women go to techie meetups, they will
    be like oh, this is like all men, I don’t really want to go to techie meetups.
    As a result of which the algorithm will get new data saying that men like techie
    meetups better, so it continues. So a little bit of that initial push from the
    algorithm can create this runaway feedback loop. And you end up with almost all
    male techie meetups, for instance. So this kind of feedback loop is a subtle issue
    that you really want to think about when you are thinking about like what is the
    behavior that I’m changing with this algorithm that I’m building.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我发现这个很有趣。这实际上是一个很酷的事情，meetup.com做了。他们考虑到了这一点。他们实际上考虑到了这一点。他们认为你知道，如果我们建立一个像我们在课堂上学到的协同过滤系统来帮助人们决定去哪个meetup。它可能会注意到在旧金山，比起女性，更多的男性倾向于参加技术meetup，因此它可能会开始决定向更多男性推荐技术meetup。结果是更多男性会参加技术meetup，结果是当女性参加技术meetup时，她们会觉得哦，这里都是男性，我真的不想去技术meetup。结果是算法会得到新的数据，表明男性更喜欢技术meetup，所以继续下去。因此，算法的这种初步推动可能会产生这种恶性循环。最终，你可能会得到几乎全是男性的技术meetup。因此，这种反馈循环是一个微妙的问题，当你考虑构建算法时，你真的要考虑到你正在改变的行为是什么。
- en: So another example which is kind of terrifying is in [this paper](https://arxiv.org/abs/1706.09847)
    where the authors describe how a lot of departments in the US are now using predictive
    policing algorithms [[1:18:18](https://youtu.be/5_xFdhfUnvQ?t=4698)]. So where
    can we go to find somebody who is about to commit a crime. So you know the algorithm
    simply feeds back to you basically the data that you’ve given it. So if your police
    department has engaged in racial profiling at all in the past, then it might suggest
    slightly more often, maybe you should go to the black neighborhoods to check for
    people committing crimes. As a result of which more of your police officers go
    to the black neighborhoods, as a result of which they arrest more black people,
    as a result of which the data says that the black neighborhoods are less safe.
    As a result of which the algorithm says to policeman maybe you should go to the
    black neighborhoods more often, and so forth.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所以另一个有点可怕的例子是在[这篇论文](https://arxiv.org/abs/1706.09847)中，作者描述了美国许多部门现在正在使用预测性执法算法。那么我们去哪里找即将犯罪的人。你知道，算法简单地反馈给你基本上你给它的数据。所以如果你的警察局过去曾经进行过种族歧视，那么它可能会稍微更频繁地建议你去黑人社区检查是否有人犯罪。结果是你的警察会更多地去黑人社区，结果是他们逮捕更多黑人，结果是数据显示黑人社区更不安全。结果是算法告诉警察，也许你应该更频繁地去黑人社区，依此类推。
- en: This is not like vague possibilities of something that might happen in the future.
    This is documented work from top academics who have carefully studied the data
    and the theory. This serious scholarly work says, no this is happening right now.
    Again, I’m sure the people that started creating this predictive policing algorithm
    didn’t think like how do we arrest more black people. Hopefully they were thinking
    gosh, I’d like my children to be safer on the streets, how do I create a safer
    society. But they didn’t think about this nasty runaway feedback loop.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这不像是未来可能发生的模糊可能性。这是来自认真研究数据和理论的顶尖学者的记录工作。这些严肃的学术工作表明，不，这正在发生。再次，我相信开始创建这种预测性执法算法的人并没有想过如何逮捕更多黑人。希望他们在想，天哪，我希望我的孩子在街上更安全，我该如何创造一个更安全的社会。但他们没有考虑到这种恶性循环。
- en: So [this one](https://www.fastcompany.com/3059742/social-network-algorithms-are-distorting-reality-by-boosting-conspiracy-theories)
    about social network algorithms is actually an article in the New York Times recently
    by one of my friends, Renee Diresta, and she did something that was kind of amazing
    [1:20:08]. She set up a second Facebook account, a fake account. She was very
    interested in the anti-vaccination movement at that time, so she started following
    a couple of anti-vaxxers and visited a couple of anti-vaxxer links. And suddenly,
    her newsfeed starts getting full of anti-vaxxer news along with other stuff like
    chemtrails and deep state conspiracy theories and all this stuff. So she starts
    clicking on those and the more she clicked, the more hardcore far-out conspiracy
    stuff Facebook recommended. So now when Renee goes to that Facebook account, the
    whole thing is just full of angry, crazy, far-out conspiracy stuff. That’s all
    she sees. So if that was your world, then as far as you’re concerned is just like
    this continuous reminder and proof of all this stuff. So again, this is the kind
    of runaway feedback loop that ends up telling Myanmar generals throughout their
    Facebook homepage that Rohingya are animals, fake news, and whatever else.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 关于社交网络算法的[这篇文章](https://www.fastcompany.com/3059742/social-network-algorithms-are-distorting-reality-by-boosting-conspiracy-theories)实际上是最近《纽约时报》上我一个朋友Renee
    Diresta写的，她做了一些令人惊讶的事情。她建立了一个第二个Facebook账号，一个假账号。那时她对反疫苗运动非常感兴趣，所以她开始关注一些反疫苗者，并访问了一些反疫苗链接。突然间，她的新闻动态开始充斥着反疫苗者的新闻，以及其他一些东西，比如化学尾迹和深层国家阴谋论等等。于是她开始点击这些内容，她点击得越多，Facebook推荐的极端离谱的阴谋论就越多。所以现在当Renee去那个Facebook账号时，整个页面都充满了愤怒、疯狂、极端的阴谋论内容。这就是她看到的一切。所以如果这是你的世界，那么在你看来，这就像是一个不断提醒和证明所有这些东西的连续。所以再次强调，这种失控的反馈循环最终告诉缅甸将军们，他们的Facebook主页上罗兴亚人是动物、假新闻，以及其他一切。
- en: '![](../Images/f1927e72259b6294c18bb746b078ea76.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1927e72259b6294c18bb746b078ea76.png)'
- en: A lot of this comes also from bias [[1:21:51](https://youtu.be/5_xFdhfUnvQ?t=4911)].
    So let’s talk about bias specifically. Bias in image software comes from bias
    in data. And so most of the folks I know at Google brain building computer vision
    algorithms, very few of them are people of color. So when they are training the
    algorithms with photos of their families and friends, they are training them with
    very few people of color. So when FaceApp decided we’re going to try looking at
    lots of Instagram photos to see which ones are upvoted the most, without them
    necessarily realizing it, the answer was light colored faces. So they built a
    generative model to make you more “hot” and so this is the actual photos and here
    is the hotter version. So the hotter version is more white, less nostrils, more
    European-looking. This did not go down well to say the least. Again, I don’t think
    anybody at FaceApp said “let’s create something that makes people look more white.
    They just trained it on a bunch of images of the people that they had around them.
    And this has serious commercial implications as well. They had to pull this feature.
    They had a huge amount of negative pushback as they should.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 很多这些也来自偏见。所以让我们具体谈谈偏见。图像软件中的偏见来自数据中的偏见。所以我认识的大多数在谷歌大脑构建计算机视觉算法的人，很少有人是有色人种。所以当他们用家人和朋友的照片来训练算法时，他们用的是很少有色人种的照片。所以当FaceApp决定我们要尝试查看很多Instagram照片，看看哪些被点赞最多时，他们并没有意识到，答案是浅色面孔。所以他们建立了一个生成模型让你看起来更“性感”，这是实际照片，这是更性感的版本。所以更性感的版本更白，鼻孔更少，看起来更欧洲化。这显然不受欢迎。再次强调，我不认为FaceApp的任何人会说“让我们创造一些让人看起来更白的东西”。他们只是用周围的人的一堆图像来训练它。这也有严重的商业影响。他们不得不撤回这个功能。他们受到了大量的负面反弹，这是应该的。
- en: Here is another example. Google photos created this photo classifier, airplanes,
    skyscrapers, cars, graduations, and gorillas. So think about how this looks to
    most people. To most people, they look at this, they don’t know about machine
    learning, they say “what the f@#$ somebody at Google wrote some code to take black
    people and call them gorillas”. That’s what it looks like. We know that’s not
    what happened. We know what happened is the team of folks at Google computer vision
    experts who have none or a few people of color working in the team built a classifier
    using all the photos they had available to them so when the system came across
    a person with dark skin it was like oh, I’ve only mainly seen that before amongst
    gorillas so I’ll put it in that category. Again, it’s the bias in the data creates
    a bias in the software and again, the commercial implications were very significant.
    Google really got a lot of bad PR from this as they should. This was a photo that
    somebody put in their Twitter feed. They said like look what Google photos just
    decided to do.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有另一个例子。谷歌照片创建了这个照片分类器，飞机、摩天大楼、汽车、毕业典礼和大猩猩。所以想想这对大多数人来说是什么样子。对大多数人来说，他们看到这个，他们不了解机器学习，他们会说“搞什么鬼，谷歌的某个人写了一些代码把黑人称为大猩猩”。这就是它看起来的样子。我们知道那不是发生的事情。我们知道发生的是谷歌计算机视觉专家团队中没有或只有少数有色人种的人建立了一个分类器，使用他们可以获得的所有照片，所以当系统遇到皮肤较黑的人时，它会认为“哦，我之前主要只见过大猩猩中的这种情况，所以我会把它放在那个类别中”。再次强调，数据中的偏见会导致软件中的偏见，商业影响也非常显著。谷歌因此受到了很多负面公关影响，这是应该的。有人在他们的Twitter上发布了这张照片。他们说看看谷歌照片刚刚决定做的事情。
- en: You can imagine what happened with the first international beauty contest judged
    by artificial intelligence. Basically it turns out all the beautiful people are
    white. So you kind of see this bias in image software, thanks to bias in the data,
    thanks to by lack of diversity in the teams building it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象第一届由人工智能评选的国际选美大赛发生了什么。基本上结果是所有美丽的人都是白人。所以你可以看到这种图像软件中的偏见，这要归功于数据中的偏见，也要归功于构建团队缺乏多样性。
- en: '![](../Images/3d5249925b7f97b99aa98e45ab322b16.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d5249925b7f97b99aa98e45ab322b16.png)'
- en: You see the same thing in natural language processing [[1:25:18](https://youtu.be/5_xFdhfUnvQ?t=5118)].
    Here is Turkish. O is the pronoun in Turkish which has no gender. There is no
    he vs. she. But of course in English, we don’t really have a widely used un-gendered
    singular pronoun, so Google Translate converts it to this. Now there are plenty
    of people who saw this online and said literally “so what?” it is correctly feeding
    back the usual usage in English. I know how this is trained, this is like Word2vec
    word vectors, it was trained on Google News corpus, Google books corpus, it’s
    just telling us how things are. From a point of view, that is entirely true. The
    biased data to create this biased algorithm is the actual data of how people have
    written books and newspaper articles for decades. But does that mean that this
    is the product that you want to create? Does this mean this is the product you
    have to create? Just because the particular way you’ve trained the model means
    it ends up doing this, is this actually the design you want?And can you think
    of potential negative implications and feedback loops this could create? If any
    of these things bother you, then now, lucky you, you have a new cool engineering
    problem to work on. How do I create unbiased NLP solutions? And now there are
    some startups starting to do that and starting to make some money. So these are
    opportunities for you. It’s like hey here’s some stuff where people are creating
    screwed up societal outcomes because of their shitty models. You can go and build
    something better. Another example of the bias in Word2vec word vectors is restaurant
    reviews ranked Mexican restaurants worse because the word “Mexican” tend to be
    associated with criminal words in the US press and books more often. Again, this
    is a real problem that is happening.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中也可以看到同样的情况。这里是土耳其语。O是土耳其语中没有性别的代词。没有他和她之分。但当然在英语中，我们并没有一个广泛使用的无性别单数代词，所以谷歌翻译将其转换为这个。现在有很多人在网上看到这个并且字面上说“那又怎样？”它正确地反馈了英语中的通常用法。我知道这是如何训练的，就像Word2vec词向量一样，它是在谷歌新闻语料库、谷歌图书语料库上训练的，它只是告诉我们事情是如何的。从某种角度来看，这是完全正确的。用来创建这种有偏见算法的有偏见数据实际上是人们几十年来写书和报纸文章的实际数据。但这是否意味着这是你想要创建的产品？这是否意味着这是你必须创建的产品？仅仅因为你训练模型的特定方式导致它最终做出这样的结果，这真的是你想要的设计吗？你能想到这可能会产生什么负面影响和反馈循环吗？如果这些事情中的任何一件让你感到不安，那么现在，幸运的是，你有一个新的很酷的工程问题要解决。我如何创建无偏见的自然语言处理解决方案？现在有一些初创公司开始做这个，并开始赚钱。所以这对你来说是机会。就像嘿，这里有一些人因为他们糟糕的模型而创造出了扭曲的社会结果。你可以去建立一些更好的东西。Word2vec词向量中的偏见的另一个例子是餐厅评论中排名较低的墨西哥餐厅，因为“墨西哥”这个词在美国新闻和书籍中更常与犯罪相关的词语联系在一起。同样，这是一个正在发生的真实问题。
- en: '![](../Images/3897abbbec407e3674911145df0196a0.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3897abbbec407e3674911145df0196a0.png)'
- en: Rachel actually did some interesting analysis of just the plain Word2vec word
    vectors [[1:27:46](https://youtu.be/5_xFdhfUnvQ?t=5266)]. She basically pulled
    them out and looked at these analogies based on some research that had been done
    elsewhere. So you can see, Word2vec vector directions show that father is to doctor
    is the mother is to nurse. Man is to computer programmer as woman is to homemaker,
    and so forth. So it’s really easy to see what’s in these word vectors. They are
    kind of fundamental to just about all the NLP software we use today.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Rachel实际上对普通的Word2vec词向量进行了一些有趣的分析。她基本上把它们拿出来，根据一些在其他地方进行的研究来看这些类比。所以你可以看到，Word2vec向量方向显示父亲对医生就像母亲对护士一样。男人对计算机程序员就像女人对家庭主妇一样，等等。所以很容易看出这些词向量中包含了什么。它们在我们今天使用的几乎所有自然语言处理软件中都是基础的。
- en: '![](../Images/4fa22bd28651303459b9ab464aba5cb7.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fa22bd28651303459b9ab464aba5cb7.png)'
- en: Here is a great example [[1:28:30](https://youtu.be/5_xFdhfUnvQ?t=5310)]. ProPublica
    has actually done a lot of good work in this area. Many judges now have access
    to sentencing guideline software. So sentencing guideline software says to the
    judge for this individual, we would recommend this kind of sentence. Now of course
    a judge doesn’t understand machine learning so they have two choices which is
    either do what it says or ignore it entirely and some people fall into each category.
    For the ones that fall into the do what it says category, here is what happens.
    For those that were labeled higher risk, the subset of those that were labeled
    high risk actually turned out not to re-offend was about quoter of whites and
    about a half of African Americans. So nearly twice as often, people who didn’t
    reoffend were marked as higher risk if they are African American and vice versa.
    Amongst those labeled lower risk but actually did reoffend turn out to be about
    half of the whites and only 28% of the African Americans. So this is data which
    I would like to think nobody is setting out to create something that does this.
    But when you start with biased data and the data says that whites and black smoke
    marijuana at about the same rate, but the blacks are jailed at, I think something
    like, 5 times more often than whites, the nature of the justice system in America
    or at least at the moment is that it’s not equal, it’s not fair. And therefore,
    the data that’s fed into the machine learning model is going to basically support
    that status quo. And then because of the negative feedback loop, it’s just going
    to get worse and worse.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个很好的例子。ProPublica实际上在这个领域做了很多有益的工作。现在许多法官都可以访问判决指南软件。因此，判决指南软件告诉法官对于这个个体，我们建议这种类型的刑罚。当然，法官不懂机器学习，所以他们有两个选择，要么按照软件的建议行事，要么完全忽视它，有些人属于每个类别。对于那些属于按照软件建议行事的人，这里是发生的事情。对于那些被标记为高风险的人，实际上没有再次犯罪的人中，白人约四分之一，非裔美国人约一半。所以，那些没有再次犯罪的人中，如果是非裔美国人，他们被标记为高风险的情况几乎是白人的两倍。而那些被标记为低风险但实际上再次犯罪的人中，白人约一半，非裔美国人只有28%。所以这是数据，我希望没有人会刻意创造这样的情况。但是当你从有偏见的数据开始，数据显示白人和黑人吸食大麻的比率大致相同，但黑人被监禁的频率大约是白人的5倍，美国的司法系统的性质，至少目前来看，是不平等的，不公平的。因此，输入到机器学习模型中的数据基本上会支持这种现状。然后由于负面反馈循环，情况只会变得越来越糟。
- en: Now I’ll tell you something else interesting about this one which researcher
    Abe Gong has pointed out [[1:30:35](https://youtu.be/5_xFdhfUnvQ?t=5435)]. Here
    are some of the questions that are being asked. So let’s take one. Was your father
    ever arrested? Your answer to that question is going to decide whether you’re
    locked up and for how long. Now as a machine learning researcher, do you think
    that might improve the predictive accuracy of your algorithm and get you a better
    R². It could well but I don’t know. Maybe it does. You try it out so oh, I’ve
    got a better R². So does that mean you should use it? There’s another question.
    Do you think it’s reasonable to lock somebody up for longer because of who their
    dad was. And yet, these are actually the examples of questions that we are asking
    right now to offenders and then putting into a machine learning system to decide
    what happens to them. Again, whoever designed this presumably they were like laser
    focused on technical excellence, getting the maximum area under the ROC curve
    and I found these great predictors that give me another 0.02\. And I guess didn’t
    stop to think like well, is that a reasonable way to decide who goes to jail for
    longer.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我要告诉你关于这个有趣的事情，研究员阿贝·龚指出了一些问题。以下是一些正在被问到的问题。所以让我们来看一个。你的父亲是否曾被逮捕过？你对这个问题的回答将决定你是否被关押以及关押多久。现在作为一个机器学习研究员，你认为这可能会提高你算法的预测准确性并获得更好的R²。可能会，但我不知道。也许会。你试试看，哦，我有了更好的R²。那么这意味着你应该使用它吗？还有另一个问题。你认为因为一个人的父亲是谁而把他关押更长时间是合理的吗？然而，这些实际上是我们现在问罪犯的问题的例子，然后将其放入一个机器学习系统中来决定他们的命运。再次，设计这个的人可能他们是专注于技术卓越，获得ROC曲线下面积的最大值，我发现了这些很好的预测因子，让我又多了0.02。我猜他们没有停下来思考，这样决定谁应该更长时间被关押是否合理。
- en: '![](../Images/c30f964553f4c3b0fd2c26c711955e4e.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c30f964553f4c3b0fd2c26c711955e4e.png)'
- en: So putting this together, you can kind of see how this can get more and more
    scary [[1:32:03](https://youtu.be/5_xFdhfUnvQ?t=5523)]. We take a company like
    Taser and tasers are these devices that give you a big electric shock basically
    and Tasers manage to do a great job of creating strong relationships with some
    academic researchers who seem to say whatever they tell them to say to the extent
    where now if you look at the data it turns out that there’s a pretty high probability
    that if you get tased that you’ll die. It happens not unusually. And yet the researchers
    who they’ve paid to look into this have consistently come back and said “oh no,
    it was nothing to do with the taser. The fact that they died immediately afterwards
    was totally unrelated. It was just a random, things happen”. So this company now
    owns 80% of the market for body cameras. And they’ve started buying computer vision
    AI companies. And they are going to do try and now use these police body camera
    videos to anticipate criminal activity. So what does that mean? Is that like okay
    I now have some augmented reality display saying tase this person because they
    are about to do something bad? It’s kind of like a worrying direction and so I’m
    sure nobody who’s a data scientist at Taser or at the companies that they bought
    out is thinking like this is the world I want to help create, but they could find
    themselves in or you could find yourself in the middle of this kind of discussion
    where it’s not explicitly about that topic but there’s a part of you that’s just
    like “I wonder if this is how this could be used.” And I don’t know exactly what
    the right thing to do in that situation is because you can ask and of course people
    are going to be like “no no no.” So what could you do? You could ask for some
    kind of written promise, you could decide to leave, you could start doing some
    research into the legality of things to say like oh, I would at least protect
    my own legal situation. I don’t know. Have a think about how you would respond
    to that.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将这些放在一起，你可以看到这会变得越来越可怕。我们以Taser这样的公司为例，Taser是一种会给你一个大电击的设备，Taser设法与一些学术研究人员建立了良好的关系，这些研究人员似乎会说他们要他们说的话，以至于现在如果你看数据，结果表明如果你被电击，你可能会死亡的概率相当高。这并不罕见。然而，他们支付给研究这个问题的研究人员一直回来说“哦不，这与电击无关。他们之后立即死亡完全无关。这只是一个偶然事件，事情发生了”。因此，这家公司现在拥有80%的身体摄像机市场份额。他们开始收购计算机视觉AI公司。他们将尝试使用这些警察身体摄像机视频来预测犯罪活动。那意味着什么？这就像是我现在有一些增强现实显示，告诉我电击这个人，因为他们即将做一些坏事？这是一个令人担忧的方向，所以我确信没有人在Taser或他们收购的公司中的数据科学家会认为这是他们想要帮助创造的世界，但他们可能会发现自己或你可能会发现自己处于这种讨论的中心，虽然这不是明确讨论的话题，但你内心可能会有一部分在想“我想知道这可能会被如何使用”。我不知道在这种情况下应该做什么才是正确的，因为你可以询问，当然人们会说“不不不”。那你能做什么？你可以要求一些书面承诺，你可以决定离开，你可以开始研究事情的合法性，比如说，至少保护自己的法律情况。我不知道。想一想你会如何应对。
- en: '![](../Images/f3a21861d23c1f8826b9db19280241e8.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3a21861d23c1f8826b9db19280241e8.png)'
- en: So these are some questions that Rachel created as being things to think about
    [[1:34:39](https://youtu.be/5_xFdhfUnvQ?t=5679)]. If you are looking at building
    a data product or using a model, if you are building machine learning model, it’s
    for a reason. You’re trying to do something. So what bias may be in that data?
    Because whatever bias is in that data ends up being a bias in your predictions,
    potentially then biases the actions you are influencing, potentially then biases
    the data that you come back, and you may get a feedback loop.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是Rachel提出的一些问题，作为需要考虑的事情。如果你正在考虑构建一个数据产品或使用模型，如果你正在构建机器学习模型，那是有原因的。你正在尝试做某事。那么数据中可能存在什么偏见？因为无论数据中存在什么偏见，最终都会成为你预测的偏见，潜在地影响你正在影响的行动，潜在地影响你返回的数据，你可能会得到一个反馈循环。
- en: If the team that built it isn’t diverse, what might you be missing? For example,
    one senior executive at Twitter called the alarm about major Russian bot problems
    at Twitter way back well before the election. That was the one black person in
    the exec team at Twitter. The one. And shortly afterwards, they lost their job.
    So definitely having a more diverse team that means having a more diverse set
    of opinions and beliefs and ideas and things to look for and so forth. So non-diverse
    team seem to make more of these bad mistakes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果构建它的团队不够多样化，你可能会错过什么？例如，Twitter的一位高级执行官在选举之前很久就警告了Twitter存在严重的俄罗斯机器人问题。那时Twitter高管团队中唯一的黑人。唯一的一个。不久之后，他们失去了工作。因此，拥有一个更多样化的团队意味着拥有更多样化的观点、信仰、想法和寻找的事物等等。因此，非多样化的团队似乎会犯更多这样的错误。
- en: Can we audit the code? Is it open source? Check for the different error rates
    amongst different groups. Is there a simple rule we could use instead that’s extremely
    interpretable and easy to communicate? And if something goes wrong, do we have
    a good way to deal with it?
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能审计代码吗？它是开源的吗？检查不同群体之间的不同错误率。我们是否可以使用一个极易解释和易于沟通的简单规则？如果出了问题，我们是否有一个处理问题的好方法？
- en: '![](../Images/fa455344d985ddcea6165cf7ef902e13.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa455344d985ddcea6165cf7ef902e13.png)'
- en: When we’ve talked to people about this and a lot of people have come to Rachel
    and said I’m concerned about something my organization is doing, what do I do
    [1:36:21]? Or I’m just concerned about my toxic workplace, what do I do? And very
    often Rachel will say have you considered leaving? And they will say I don’t want
    to lose my job. But actually you can code, you’re in like 0.3% of the population.
    If you can code and do machine learning, you’re in probably like 0.01% of the
    population. You are massively massively in demand. So realistically, obviously
    an organization does not want you to feel like you are somebody who could just
    leave and get another job, that’s not in their interest. But that’s absolutely
    true. So one of the things I hope you’ll leave this course with is enough self-confidence
    to recognize that you have the skills to get a job and particularly once you’ve
    got your first your job, your second job is an order of magnitude easier. So this
    is important not just so that you feel like you actually have the ability to act
    ethically, but it’s also important to realize if you find yourself in a toxic
    environment which is pretty damn common unfortunately. There’s a lot of shitty
    tech cultures/environments particularly in the Bay Area. If you find yourself
    in one of those environments, the best thing to do is to get the hell out. And
    if you don’t have the self-confidence to think you can get another job, you can
    get trapped. So it’s really important. It’s really important to know that you
    are leaving this program with very in demand skills and particularly after you
    have that first job, you’re now somebody with in-demand skills and a track record
    of being employed in that area.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们与人们谈论这个问题时，很多人都去找Rachel说我对我的组织正在做的事情感到担忧，我该怎么办[1:36:21]？或者我只是担心我的 toxic workplace，我该怎么办？而
    Rachel 往往会说你有考虑过离开吗？他们会说我不想失去工作。但实际上，如果你会编程，你是整个人口的 0.3%。如果你会编程和做机器学习，你可能是整个人口的
    0.01%。你是极其极其受欢迎的。所以实际上，显然一个组织不希望你觉得自己可以随时离开找另一份工作，这不符合他们的利益。但这绝对是真的。所以我希望你们在这门课程中能够获得足够的自信，认识到你有能力找到工作，特别是一旦你有了第一份工作，第二份工作就会容易很多。所以这很重要，不仅是为了让你觉得你实际上有能力去做出道德行为，而且也很重要意识到如果你发现自己处在一个
    toxic 环境中，这是相当普遍的不幸。在湾区尤其存在很多糟糕的科技文化/环境。如果你发现自己处在这样的环境中，最好的做法就是赶紧离开。如果你没有自信认为自己可以找到另一份工作，你就会被困住。所以这真的很重要。很重要的是要知道你在结束这个项目时拥有非常受欢迎的技能，特别是在你有了第一份工作之后，你现在是一个拥有受欢迎技能和在该领域就业记录的人。
- en: '**Question**: This is kind of just a broad question but what are some things
    that you know of that people are doing to treat bias in data [[1:38:41](https://youtu.be/5_xFdhfUnvQ?t=5921)]?
    You know, it’s kind of like a bit of controversial subject at the moment and there
    are people trying to use an algorithmic approach where they are basically trying
    to say how can we identify the bias and kind of subtract it out. But the most
    effective ways I know of are the ones that are trying to treat it at the data
    level. So start with a more diverse team particularly a team includes people from
    the humanities like sociologists, psychologists, economists, people that understand
    feedback loops and implications for human behavior and they tend to be equipped
    with good tools for identifying and tracking these kinds of problems. And then
    try to incorporate the solutions into the process itself. But there isn’t some
    standard process I can point you to and say here’s how to solve it. If there’s
    such a thing, we haven’t found it yet. It requires a diverse team of smart people
    to be aware of the problems and work hard at them is the short answer.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：这只是一个广泛的问题，你知道人们正在做一些什么来处理数据中的偏见吗[1:38:41]？你知道，这目前是一个有争议的话题，有人试图使用算法方法，他们基本上试图说我们如何识别偏见并将其减去。但我知道的最有效的方法是试图在数据层面上处理它。所以从一个更多元化的团队开始，特别是一个团队包括来自人文学科的人，比如社会学家、心理学家、经济学家，了解反馈循环和对人类行为的影响，他们往往配备了识别和跟踪这类问题的良好工具。然后尝试将解决方案纳入到过程中。但我无法告诉你有一个标准的处理方法，告诉你如何解决它。如果有这样的东西，我们还没有找到。简短的答案是，需要一个多样化的聪明团队意识到问题并努力解决。'
- en: '**Comment**: This is just kind of a general thing for the whole class, if you
    are interested in this stuff, I read a pretty cool book, Jeremy you’ve probably
    heard of it, Weapons of Math Destruction by Cathy O’Neil. It covers a lot of the
    same stuff [[1:40:09](https://youtu.be/5_xFdhfUnvQ?t=6009)]. Yeah, thanks for
    the recommendation. Kathy is great. She’s also got a TED talk. I didn’t manage
    to finish the book. It’s so damn depressing. I was just like “no more”. But yeah,
    it’s very good.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**：这只是针对整个班级的一个普遍性建议，如果你对这些感兴趣，我读过一本很酷的书，Jeremy 你可能听说过，Cathy O’Neil 的《Weapons
    of Math Destruction》。它涵盖了很多相同的内容[1:40:09]。是的，谢谢你的推荐。Kathy 很棒。她还有一个 TED 演讲。我没能完成这本书。它太令人沮丧了。我只是说“不要再看了”。但是，它确实很好。'
- en: All right. That’s it. Thank you, everybody. This has been really intense for
    me. Obviously this was meant to be something that I was sharing with Rachel. So
    I’ve ended up doing one of the hardest things in my life which is to teach two
    peoples worth of course on my own and also look after a sick wife and have a toddler
    and also do a deep learning course. And also do all this with a new library that
    I just wrote. So I’m looking forward to getting some sleep. But it’s been totally
    worth it. Because you’ve been amazing. I’m thrilled with how you’ve reacted to
    the opportunities I’ve given you and also to the feedback that I’ve given you.
    So congratulations.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。就这样了。谢谢大家。对我来说，这真的很紧张。显然，这原本是我和Rachel分享的事情。所以我最终做了我一生中最困难的事情之一，那就是独自教授两个人的课程，照顾生病的妻子和一个幼儿，还要学习深度学习课程。而且还要用我刚写的新库来完成所有这些。所以我期待着能好好休息一下。但这一切都是完全值得的。因为你们太棒了。我对你们对我给予的机会作出的反应以及我给予你们的反馈感到非常高兴。所以恭喜。
