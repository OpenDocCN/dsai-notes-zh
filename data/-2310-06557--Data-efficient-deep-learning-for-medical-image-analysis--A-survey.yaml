- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:36:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:36:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2310.06557] Data efficient deep learning for medical image analysis: A survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2310.06557] 数据高效深度学习在医学图像分析中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06557](https://ar5iv.labs.arxiv.org/html/2310.06557)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06557](https://ar5iv.labs.arxiv.org/html/2310.06557)
- en: 'Data efficient deep learning for medical image analysis: A survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据高效深度学习在医学图像分析中的应用：综述
- en: Suruchi Kumari [suruchi_k@cs.iitr.ac.in](mailto:suruchi_k@cs.iitr.ac.in) Pravendra Singh
    [pravendra.singh@cs.iitr.ac.in](mailto:pravendra.singh@cs.iitr.ac.in) Department
    of Computer Science and Engineering, Indian Institute of Technology Roorkee, India
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Suruchi Kumari [suruchi_k@cs.iitr.ac.in](mailto:suruchi_k@cs.iitr.ac.in) Pravendra
    Singh [pravendra.singh@cs.iitr.ac.in](mailto:pravendra.singh@cs.iitr.ac.in) 印度理工学院鲁尔基计算机科学与工程系
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid evolution of deep learning has significantly advanced the field of
    medical image analysis. However, despite these achievements, the further enhancement
    of deep learning models for medical image analysis faces a significant challenge
    due to the scarcity of large, well-annotated datasets. To address this issue,
    recent years have witnessed a growing emphasis on the development of data-efficient
    deep learning methods. This paper conducts a thorough review of data-efficient
    deep learning methods for medical image analysis. To this end, we categorize these
    methods based on the level of supervision they rely on, encompassing categories
    such as *no supervision*, *inexact supervision*, *incomplete supervision*, *inaccurate
    supervision*, and *only limited supervision*. We further divide these categories
    into finer subcategories. For example, we categorize *inexact supervision* into
    *multiple instance learning* and *learning with weak annotations*. Similarly,
    we categorize *incomplete supervision* into *semi-supervised learning*, *active
    learning*, and *domain-adaptive learning* and so on. Furthermore, we systematically
    summarize commonly used datasets for data efficient deep learning in medical image
    analysis and investigate future research directions to conclude this survey.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的快速发展显著推动了医学图像分析领域的进步。然而，尽管取得了这些成就，深度学习模型在医学图像分析中的进一步提升仍面临着由于大规模、良好标注的数据集稀缺而带来的重大挑战。为了解决这个问题，近年来越来越重视开发数据高效的深度学习方法。本文对医学图像分析中的数据高效深度学习方法进行了全面的综述。为此，我们根据这些方法所依赖的监督程度对其进行分类，包括*无监督*、*不精确监督*、*不完全监督*、*不准确监督*和*仅限监督*等类别。我们进一步将这些类别细分。例如，我们将*不精确监督*细分为*多实例学习*和*弱标注学习*。类似地，我们将*不完全监督*细分为*半监督学习*、*主动学习*和*领域自适应学习*等。进一步地，我们系统总结了医学图像分析中用于数据高效深度学习的常用数据集，并调查了未来的研究方向，以结束本综述。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Data efficient deep learning , Medical image analysis , Inexact supervision
    , Incomplete supervision , Inaccurate supervision , Only limited supervision ,
    No supervision.\UseRawInputEncoding
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据高效深度学习，医学图像分析，*不精确监督*，*不完全监督*，*不准确监督*，*仅限监督*，*无监督*。\UseRawInputEncoding
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep learning has significantly influenced various medical fields, particularly
    medical imaging, with its influence expected to further expand [[1](#bib.bib1)].
    In the context of medical image analysis (MIA), deep learning methods have demonstrated
    remarkable performance across various tasks, including disease classification
    [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)], medical object
    detection [[6](#bib.bib6), [7](#bib.bib7)], ROI segmentation [[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)], and image registration [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]. Initially, supervised learning was widely
    adopted in MIA. Despite its success in numerous applications, the broader use
    of supervised models faces a significant challenge due to the typically small
    size of most medical datasets. Medical image datasets are often considerably smaller
    than standard computer vision datasets. The initial amount of available data is
    limited, and obtaining additional data is hindered by factors such as patient
    confidentiality and institutional policies. Furthermore, in many instances, only
    a small fraction of the images are annotated by domain experts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在各种医学领域，特别是医学影像学中产生了重大影响，并预计其影响将进一步扩大[[1](#bib.bib1)]。在医学图像分析（MIA）的背景下，深度学习方法在各种任务中表现出色，包括疾病分类[[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]、医学目标检测[[6](#bib.bib6), [7](#bib.bib7)]、ROI分割[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]以及图像配准[[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14)]。最初，MIA广泛采用了监督学习。尽管在许多应用中取得了成功，但由于大多数医学数据集的规模通常较小，监督模型的广泛使用面临重大挑战。医学图像数据集通常比标准计算机视觉数据集要小得多。初始可用数据量有限，而额外数据的获取受到患者隐私和机构政策等因素的阻碍。此外，在许多情况下，只有一小部分图像由领域专家标注。
- en: '![Refer to caption](img/798d7d9d633ce5ba0f9fcc4755341549.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/798d7d9d633ce5ba0f9fcc4755341549.png)'
- en: 'Figure 1: Taxonomy of data efficient deep learning approaches for medical image
    analysis.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：医学图像分析的数据高效深度学习方法分类。
- en: 'Typically, researchers rely on domain experts, such as radiologists or pathologists,
    to create task-specific annotations for image data. Labeling a sufficiently large
    dataset can be time-consuming [[15](#bib.bib15)]. For example, training deep learning
    systems for radiology, especially when involving 3D data, requires meticulous
    slice-by-slice annotations, which can be particularly time-intensive [[12](#bib.bib12)].
    Some research efforts have involved numerous experts in annotating extensive medical
    image datasets [[16](#bib.bib16), [17](#bib.bib17)]. However, such initiatives
    demand substantial financial and logistical resources, which are often not readily
    available across various domains. Other investigations have resorted to crowd-sourcing
    approaches for obtaining labels from non-experts [[18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)]. Although this method may have potential in specific cases,
    its applicability is limited because non-experts typically cannot provide meaningful
    labels for most medical applications. To overcome these limitations, there is
    a growing trend among researchers to develop data-efficient deep learning approaches
    for medical image analysis. We broadly categorize these approaches into the following
    groups: no supervision, inexact supervision, incomplete supervision, inaccurate
    supervision, and limited supervision, as shown in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis: A
    survey").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，研究人员依赖领域专家，如放射科医生或病理学家，为图像数据创建特定任务的注释。标注足够大的数据集可能非常耗时[[15](#bib.bib15)]。例如，为放射学训练深度学习系统，尤其是涉及3D数据时，需要细致的逐切片注释，这可能特别耗时[[12](#bib.bib12)]。一些研究工作涉及众多专家对大规模医学图像数据集进行注释[[16](#bib.bib16),
    [17](#bib.bib17)]。然而，这些举措需要大量的财政和后勤资源，这些资源在各个领域通常并不容易获得。其他研究则求助于众包方法，从非专家那里获取标签[[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20)]。尽管这种方法在特定情况下可能具有潜力，但其适用性有限，因为非专家通常不能为大多数医学应用提供有意义的标签。为了克服这些限制，研究人员正在日益发展数据高效的深度学习方法用于医学图像分析。我们将这些方法大致分类为以下几类：无监督、近似监督、不完全监督、不准确监督和有限监督，如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis:
    A survey")所示。'
- en: This survey covers more than 250 papers, with the majority published in recent
    years (2020-2023). These papers span a diverse range of applications of deep learning
    in medical image analysis and have been presented in conference proceedings for
    MICCAI, EMBC, and ISBI, as well as various journals such as TMI, Medical Image
    Analysis, and Computers in Biology and Medicine, among others.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述涵盖了超过250篇论文，其中大多数是在近几年（2020-2023年）发表的。这些论文涉及深度学习在医学图像分析中的多种应用，并已在MICCAI、EMBC和ISBI的会议论文集以及TMI、Medical
    Image Analysis和Computers in Biology and Medicine等各类期刊中发表。
- en: 'Several related review articles have already been published summarizing a few
    specific categories of data efficient learning in the domain of medical image
    analysis. Cheplygina et al. [[21](#bib.bib21)] provided an overview of semi-supervised
    learning, multiple instance learning, and transfer learning within the context
    of medical imaging, addressing both diagnostic and segmentation tasks. Meanwhile,
    Tajbakhsh et al. [[22](#bib.bib22)] explored numerous strategies for handling
    dataset limitations, such as cases involving scarce or weak annotations, with
    a particular focus on medical image segmentation. Chen et al. [[13](#bib.bib13)]
    present a summary of the latest developments in deep learning, encompassing supervised,
    unsupervised, and semi-supervised methodologies. More recently, Jin et al. [[23](#bib.bib23)]
    provide an overview of semi-supervised, self-supervised, multi-instance learning,
    active learning, and annotation-efficient techniques. However, it’s worth noting
    that their review does not delve into subjects such as domain-adaptive learning
    or few-shot learning, among others. Also, in the previously discussed surveys,
    their coverage is either restricted concerning data-efficient methods in MIA or
    not up to date with the current trends. To tackle this challenge, we undertake
    a systematic review of recent data-efficient methodologies, as outlined in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis:
    A survey"). Our goal is to offer a thorough review of data-efficient learning
    methods in medical image analysis and outline future challenges. We also provide
    an overview of several widely used available datasets in the field of medical
    imaging, as illustrated in Table [1](#S1.T1b "Table 1 ‣ 1 Introduction ‣ Data
    efficient deep learning for medical image analysis: A survey"). The major contributions
    of our work can be summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '已经有几篇相关的综述文章总结了医学图像分析领域中一些特定类别的数据高效学习方法。Cheplygina 等人 [[21](#bib.bib21)] 提供了在医学成像背景下半监督学习、多实例学习和迁移学习的概述，涵盖了诊断和分割任务。同时，Tajbakhsh
    等人 [[22](#bib.bib22)] 探讨了处理数据集限制的多种策略，如涉及稀缺或弱标注的案例，特别关注医学图像分割。Chen 等人 [[13](#bib.bib13)]
    总结了深度学习的最新进展，包括监督学习、无监督学习和半监督学习方法。最近，Jin 等人 [[23](#bib.bib23)] 提供了半监督、自监督、多实例学习、主动学习和注释高效技术的概述。然而，值得注意的是，他们的综述未涉及领域自适应学习或少样本学习等主题。此外，在之前讨论的调查中，其覆盖范围要么对医学图像分析中的数据高效方法有所限制，要么未跟上当前的趋势。为应对这一挑战，我们对近期的数据高效方法进行了系统综述，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis:
    A survey")所示。我们的目标是提供医学图像分析中数据高效学习方法的全面综述，并勾勒未来的挑战。我们还提供了医学成像领域中几种广泛使用的可用数据集的概述，如表[1](#S1.T1b
    "Table 1 ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis:
    A survey")所示。我们工作的主要贡献总结如下：'
- en: '1.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: This is the first survey paper that summarizes recent advances in data efficient
    deep learning for medical image analysis. Specifically, we present a comprehensive
    overview of more than 250 relevant papers to cover the recent progress.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是第一篇总结医学图像分析中数据高效深度学习最新进展的综述论文。具体来说，我们提供了超过250篇相关论文的全面概述，以涵盖最近的进展。
- en: '2.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'We systematically categorize these methods into five distinct groups: incomplete
    supervision, no supervision, inaccurate supervision, inexact supervision, and
    only limited supervision.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将这些方法系统地分类为五个不同的组别：不完全监督、无监督、不准确的监督、不精确的监督和仅有有限的监督。
- en: '3.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: Lastly, we explore several potential future directions for further research
    and development for data-efficient deep learning methods in MIA.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们探索了数据高效深度学习方法在医学图像分析中的几个潜在未来研究和开发方向。
- en: 'The remainder of this survey is organized as follows. In Section [2](#S2 "2
    No supervision ‣ Data efficient deep learning for medical image analysis: A survey"),
    we delve into techniques falling under the category of No Supervision, which we
    further subdivide into Predictive Self-Supervision (Subsection [2.1](#S2.SS1 "2.1
    Predictive self-supervision ‣ 2 No supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")), Generative Self-Supervision (Subsection [2.2](#S2.SS2
    "2.2 Generative self-supervision ‣ 2 No supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")), Contrastive Self-Supervision (Subsection [2.3](#S2.SS3
    "2.3 Contrastive self-supervision ‣ 2 No supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")), and Multi-Self Supervised Learning (Subsection [2.4](#S2.SS4
    "2.4 Multi-self supervised learning: combining multiple SSL pretext tasks into
    one framework ‣ 2 No supervision ‣ Data efficient deep learning for medical image
    analysis: A survey")). In Section [3](#S3 "3 Inexact supervision ‣ Data efficient
    deep learning for medical image analysis: A survey"), we explore Inexact Supervision
    techniques, further classified into Multiple Instance Learning (Subsection [3.1](#S3.SS1
    "3.1 Multiple instance learning ‣ 3 Inexact supervision ‣ Data efficient deep
    learning for medical image analysis: A survey")) and Learning with Weak Annotations
    (Subsection [3.2](#S3.SS2 "3.2 Learning with weak annotations ‣ 3 Inexact supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). Section [4](#S4
    "4 Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey") is dedicated to Incomplete Supervision methods, which we further categorize
    as Semi-Supervised Learning (Subsection [4.1](#S4.SS1 "4.1 Semi-supervised learning
    ‣ 4 Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")), Active Learning (Subsection [4.2](#S4.SS2 "4.2 Active learning ‣
    4 Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")), and Domain-Adaptive Learning (Subsection [4.3](#S4.SS3 "4.3 Domain-adaptive
    learning ‣ 4 Incomplete supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")). Similarly, Section [5](#S5 "5 Inaccurate supervision
    ‣ Data efficient deep learning for medical image analysis: A survey") deals with
    Inaccurate Supervision techniques, which we further categorize as Robust Loss
    Design (Subsection [5.1](#S5.SS1 "5.1 Robust loss design ‣ 5 Inaccurate supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")), Data reweighting
    (Subsection [5.2](#S5.SS2 "5.2 Data re-weighting ‣ 5 Inaccurate supervision ‣
    Data efficient deep learning for medical image analysis: A survey")), and Training
    procedures (Subsection [5.3](#S5.SS3 "5.3 Training procedures ‣ 5 Inaccurate supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). Moving
    on to Section [6](#S6 "6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey"), we focus on Only Limited Supervision techniques,
    which are classified into Data Augmentation (Subsection [6.1](#S6.SS1 "6.1 Data
    Augmentation ‣ 6 Only limited supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")), Few-Shot Learning (Subsection [6.2](#S6.SS2 "6.2
    Few shot learning ‣ 6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")), and Transfer Learning (Subsection [6.3](#S6.SS3
    "6.3 Transfer learning ‣ 6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")). Additionally, we outline potential future
    research directions in Section [7](#S7 "7 Future research scope ‣ Data efficient
    deep learning for medical image analysis: A survey") before concluding this survey
    in Section [8](#S8 "8 Conclusion ‣ Data efficient deep learning for medical image
    analysis: A survey"). The structural overview of this survey is presented in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Data efficient deep learning for medical image analysis:
    A survey").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分组织如下。在第[2](#S2 "2 无监督 ‣ 医学图像分析的数据高效深度学习：综述")节中，我们将深入探讨无监督类别下的技术，我们进一步细分为预测性自监督（第[2.1](#S2.SS1
    "2.1 预测性自监督 ‣ 2 无监督 ‣ 医学图像分析的数据高效深度学习：综述")节）、生成性自监督（第[2.2](#S2.SS2 "2.2 生成性自监督
    ‣ 2 无监督 ‣ 医学图像分析的数据高效深度学习：综述")节）、对比性自监督（第[2.3](#S2.SS3 "2.3 对比性自监督 ‣ 2 无监督 ‣ 医学图像分析的数据高效深度学习：综述")节）和多自监督学习（第[2.4](#S2.SS4
    "2.4 多自监督学习：将多个SSL预任务结合到一个框架中 ‣ 2 无监督 ‣ 医学图像分析的数据高效深度学习：综述")节）。在第[3](#S3 "3 不完全监督
    ‣ 医学图像分析的数据高效深度学习：综述")节中，我们探讨了不完全监督技术，进一步分类为多实例学习（第[3.1](#S3.SS1 "3.1 多实例学习 ‣
    3 不完全监督 ‣ 医学图像分析的数据高效深度学习：综述")节）和带弱注释的学习（第[3.2](#S3.SS2 "3.2 带弱注释的学习 ‣ 3 不完全监督
    ‣ 医学图像分析的数据高效深度学习：综述")节）。第[4](#S4 "4 不完整监督 ‣ 医学图像分析的数据高效深度学习：综述")节专门讨论了不完整监督方法，我们进一步将其分类为半监督学习（第[4.1](#S4.SS1
    "4.1 半监督学习 ‣ 4 不完整监督 ‣ 医学图像分析的数据高效深度学习：综述")节）、主动学习（第[4.2](#S4.SS2 "4.2 主动学习 ‣
    4 不完整监督 ‣ 医学图像分析的数据高效深度学习：综述")节）和领域自适应学习（第[4.3](#S4.SS3 "4.3 领域自适应学习 ‣ 4 不完整监督
    ‣ 医学图像分析的数据高效深度学习：综述")节）。类似地，第[5](#S5 "5 不准确监督 ‣ 医学图像分析的数据高效深度学习：综述")节处理了不准确监督技术，我们进一步分类为鲁棒损失设计（第[5.1](#S5.SS1
    "5.1 鲁棒损失设计 ‣ 5 不准确监督 ‣ 医学图像分析的数据高效深度学习：综述")节）、数据重标定（第[5.2](#S5.SS2 "5.2 数据重标定
    ‣ 5 不准确监督 ‣ 医学图像分析的数据高效深度学习：综述")节）和训练程序（第[5.3](#S5.SS3 "5.3 训练程序 ‣ 5 不准确监督 ‣ 医学图像分析的数据高效深度学习：综述")节）。接着，在第[6](#S6
    "6 仅有限监督 ‣ 医学图像分析的数据高效深度学习：综述")节中，我们专注于仅有限监督技术，这些技术被分类为数据增强（第[6.1](#S6.SS1 "6.1
    数据增强 ‣ 6 仅有限监督 ‣ 医学图像分析的数据高效深度学习：综述")节）、少样本学习（第[6.2](#S6.SS2 "6.2 少样本学习 ‣ 6 仅有限监督
    ‣ 医学图像分析的数据高效深度学习：综述")节）和迁移学习（第[6.3](#S6.SS3 "6.3 迁移学习 ‣ 6 仅有限监督 ‣ 医学图像分析的数据高效深度学习：综述")节）。此外，我们在第[7](#S7
    "7 未来研究方向 ‣ 医学图像分析的数据高效深度学习：综述")节中概述了潜在的未来研究方向，然后在第[8](#S8 "8 结论 ‣ 医学图像分析的数据高效深度学习：综述")节总结了本调查。图[1](#S1.F1
    "图1 ‣ 1 引言 ‣ 医学图像分析的数据高效深度学习：综述")展示了本调查的结构概览。
- en: 'Table 1: Commonly used datasets for data efficient deep learning in medical
    image analysis.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：医学图像分析中用于数据高效深度学习的常用数据集。
- en: '| Dataset | Organ | Types | Task | Description | Link |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 器官 | 类型 | 任务 | 描述 | 链接 |'
- en: '| JSRT Database (2000) [[24](#bib.bib24)] | Brain | Chest radiographs | Classification
    | The database includes 154 conventional chest radiographs with a lung nodule
    (100 malignant and 54 benign nodules) and 93 radiographs without a nodule. | [http://db.jsrt.or.jp/eng.php](http://db.jsrt.or.jp/eng.php)
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| JSRT 数据库 (2000) [[24](#bib.bib24)] | 大脑 | 胸部 X 光片 | 分类 | 数据库包括 154 张有肺结节的常规胸部
    X 光片（100 张恶性和 54 张良性结节）以及 93 张无结节的 X 光片。 | [http://db.jsrt.or.jp/eng.php](http://db.jsrt.or.jp/eng.php)
    |'
- en: '| ADNI-3 dataset [[25](#bib.bib25), [26](#bib.bib26)] | Brain | MRI, PET, fMRI,
    etc.. | Alzheimer’s Disease identification | 697 subjects from ADNI-2 and additional
    133 CN, 151 amnestic MCI and 87 AD subjects were added (371 total new subjects)
    | [https://adni.loni.usc.edu/adni-3/](https://adni.loni.usc.edu/adni-3/) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| ADNI-3 数据集 [[25](#bib.bib25), [26](#bib.bib26)] | 大脑 | MRI, PET, fMRI 等 |
    阿尔茨海默病识别 | 从 ADNI-2 中获取的 697 名受试者以及额外添加的 133 名 CN、151 名遗忘性 MCI 和 87 名 AD 受试者（共
    371 名新受试者） | [https://adni.loni.usc.edu/adni-3/](https://adni.loni.usc.edu/adni-3/)
    |'
- en: '| BraTS 2012 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: 30 datasets(pre- and post-therapy images) Synthetic data: 50 simulated
    datasets; Test: 15 clinical and 15 simulated datasets | [http://www.imm.dtu.dk/projects/BRATS2012/data.html](http://www.imm.dtu.dk/projects/BRATS2012/data.html)
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| BraTS 2012 [[27](#bib.bib27)] | 大脑 | MR 图像 | 脑肿瘤分割 | 训练：30 个数据集（治疗前后图像）；合成数据：50
    个模拟数据集；测试：15 个临床数据集和 15 个模拟数据集 | [http://www.imm.dtu.dk/projects/BRATS2012/data.html](http://www.imm.dtu.dk/projects/BRATS2012/data.html)
    |'
- en: '| BraTS 2013 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: Clinical dataset from BraTS12 training data; Test: 15 clinical test
    images from BraTS12 and 10 new test dataset | [https://www.smir.ch/BRATS/Start2013#!#download](https://www.smir.ch/BRATS/Start2013#!#download)
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| BraTS 2013 [[27](#bib.bib27)] | 大脑 | MR 图像 | 脑肿瘤分割 | 训练：来自 BraTS12 训练数据的临床数据集；测试：来自
    BraTS12 的 15 张临床测试图像和 10 个新测试数据集 | [https://www.smir.ch/BRATS/Start2013#!#download](https://www.smir.ch/BRATS/Start2013#!#download)
    |'
- en: '| BraTS 2014 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: 200 datasets from both BraTS12 and BraTS13 and TCIA [16] including
    longitudinal datasets; Test: 38 unseen datasets from both BraTS12 and BraTS13
    test datasets and TCIA | [https://www.smir.ch/BRATS/Start2014](https://www.smir.ch/BRATS/Start2014)
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| BraTS 2014 [[27](#bib.bib27)] | 大脑 | MR 图像 | 脑肿瘤分割 | 训练：来自 BraTS12 和 BraTS13
    以及 TCIA [16] 的 200 个数据集，包括纵向数据集；测试：来自 BraTS12 和 BraTS13 测试数据集及 TCIA 的 38 个未见数据集
    | [https://www.smir.ch/BRATS/Start2014](https://www.smir.ch/BRATS/Start2014) |'
- en: '| BraTS 2015 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: Identical to the BraTS14 training dataset; Test: 53 unseen datasets
    from both BraTS12 and BraTS13 test datasets and TCIA | [https://www.smir.ch/BRATS/Start2015](https://www.smir.ch/BRATS/Start2015)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| BraTS 2015 [[27](#bib.bib27)] | 大脑 | MR 图像 | 脑肿瘤分割 | 训练：与 BraTS14 训练数据集相同；测试：来自
    BraTS12 和 BraTS13 测试数据集及 TCIA 的 53 个未见数据集 | [https://www.smir.ch/BRATS/Start2015](https://www.smir.ch/BRATS/Start2015)
    |'
- en: '| TCIA (2015) | Brain | MR images | Segmentation | 20 subjects with primary
    newly diagnosed glioblastoma who were treated with surgery and standard concomitant
    chemo-radiation therapy (CRT) followed by adjuvant chemotherapy. | [https://www.cancerimagingarchive.net/](https://www.cancerimagingarchive.net/)
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| TCIA (2015) | 大脑 | MR 图像 | 分割 | 20 名新诊断为胶质母细胞瘤的患者接受手术和标准的联合化疗放疗（CRT），随后接受辅助化疗。
    | [https://www.cancerimagingarchive.net/](https://www.cancerimagingarchive.net/)
    |'
- en: '| BraTS 2016 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: Identical to the BraTS14 training dataset; Test: 191 unseen datasets
    from both BraTS12 and BraTS13 test datasets and TCIA | [https://www.smir.ch/BRATS/Start2016](https://www.smir.ch/BRATS/Start2016)
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| BraTS 2016 [[27](#bib.bib27)] | 大脑 | MR 图像 | 脑肿瘤分割 | 训练：与 BraTS14 训练数据集相同；测试：来自
    BraTS12 和 BraTS13 测试数据集及 TCIA 的 191 个未见数据集 | [https://www.smir.ch/BRATS/Start2016](https://www.smir.ch/BRATS/Start2016)
    |'
- en: '| ABIDE-II (2016) | Brain | fMRI sequences | Autism spectrum disorder classification
    | 1114 datasets from 521 individuals with ASD and 593 controls | [https://fcon1000.projects.nitrc.org/indi/abide/](https://fcon1000.projects.nitrc.org/indi/abide/)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ABIDE-II (2016) | 大脑 | fMRI 序列 | 自闭症谱系障碍分类 | 来自 521 名 ASD 个体和 593 名对照的 1114
    个数据集 | [https://fcon1000.projects.nitrc.org/indi/abide/](https://fcon1000.projects.nitrc.org/indi/abide/)
    |'
- en: '| BraTS 2017 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: 285 training datasets from BraTS12 and BraTS13 + pre-operative MRI
    scans from 19 institution; Validation: 6 unseen datasets from different institution;
    Test: 146 unseen datasets from both BraTS13 test datasets and different institutions
    | [https://sites.google.com/site/braintumorsegmentation/](https://sites.google.com/site/braintumorsegmentation/)
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| BraTS 2017 [[27](#bib.bib27)] | 大脑 | MR 图像 | 脑肿瘤分割 | 训练：来自 BraTS12 和 BraTS13
    的 285 个训练数据集 + 19 个机构的术前 MRI 扫描；验证：来自不同机构的 6 个未见数据集；测试：来自 BraTS13 测试数据集和不同机构的
    146 个未见数据集 | [https://sites.google.com/site/braintumorsegmentation/](https://sites.google.com/site/braintumorsegmentation/)
    |'
- en: '| BraTS 2018 [[27](#bib.bib27)] | Brain | MR images | Brain tumor segmentation
    | Training: Identical to the BraTS17 dataset; Validation: 6 unseen datasets from
    different institution; Test: 191 unseen datasets from both BraTS13 test datasets
    and different institutions | [https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=37224922](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=37224922)
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| BraTS 2018 [[27](#bib.bib27)] | 大脑 | MR 图像 | 脑肿瘤分割 | 训练：与 BraTS17 数据集相同；验证：来自不同机构的
    6 个未见数据集；测试：来自 BraTS13 测试数据集和不同机构的 191 个未见数据集 | [https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=37224922](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=37224922)
    |'
- en: '| dHCP 2018 [[28](#bib.bib28)] | Brain | MRI | Cortical and sub-cortical volume
    segmentation, cortical surface extraction, and inflation | 465 subjects ranging
    from 28 to 45 weeks post-menstrual age. | [http://www.developingconnectome.org/data-release/](http://www.developingconnectome.org/data-release/)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| dHCP 2018 [[28](#bib.bib28)] | 大脑 | MRI | 皮层和皮下体积分割、皮层表面提取和膨胀 | 465 名受试者，年龄范围为
    28 到 45 周产后年龄。 | [http://www.developingconnectome.org/data-release/](http://www.developingconnectome.org/data-release/)
    |'
- en: '| Calgary-Campinas-359 (CC-359) [[29](#bib.bib29)] | Brain | MR images | Skull
    stripping or Brain segmentation | 359 subjects on scanners from three different
    vendors (GE, Philips, and Siemens) and at two magnetic field strengths (1.5 T
    and 3 T) | [https://www.ccdataset.com/download](https://www.ccdataset.com/download)
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Calgary-Campinas-359 (CC-359) [[29](#bib.bib29)] | 大脑 | MR 图像 | 颅骨去除或脑分割
    | 359 名受试者，使用三种不同厂商（GE、Philips 和 Siemens）的扫描仪，且在两种磁场强度（1.5 T 和 3 T）下进行扫描 | [https://www.ccdataset.com/download](https://www.ccdataset.com/download)
    |'
- en: '| MICCAI WMH Challenge [[30](#bib.bib30)] | Brain | MR images | White matter
    hyperintensities (WMH) segmentation | Training: 60 images; Test: 110 images |
    [https://wmh.isi.uu.nl/#_Toc122355662](https://wmh.isi.uu.nl/#_Toc122355662) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| MICCAI WMH Challenge [[30](#bib.bib30)] | 大脑 | MR 图像 | 白质高信号 (WMH) 分割 | 训练：60
    张图像；测试：110 张图像 | [https://wmh.isi.uu.nl/#_Toc122355662](https://wmh.isi.uu.nl/#_Toc122355662)
    |'
- en: '| REST-meta-MDD Consortium [[31](#bib.bib31)] | Brain | Resting-state functional
    MRI (R-fMRI) | Major Depressive Disorder (MDD) classification | Neuroimaging data
    of 1,300 depressed patients and 1,128 normal controls from 25 research groups
    | [http://rfmri.org/REST-meta-MDD](http://rfmri.org/REST-meta-MDD) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| REST-meta-MDD Consortium [[31](#bib.bib31)] | 大脑 | 静息态功能 MRI (R-fMRI) | 重度抑郁症
    (MDD) 分类 | 来自 25 个研究小组的 1,300 名抑郁患者和 1,128 名正常对照的神经影像数据 | [http://rfmri.org/REST-meta-MDD](http://rfmri.org/REST-meta-MDD)
    |'
- en: '| BraTS (2021) | Brain | MR images | Segmentation; Classification | 2,000 cases
    (8,000 mpMRI scans) | [http://braintumorsegmentation.org/](http://braintumorsegmentation.org/)
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| BraTS (2021) | 大脑 | MR 图像 | 分割；分类 | 2,000 个病例（8,000 个 mpMRI 扫描） | [http://braintumorsegmentation.org/](http://braintumorsegmentation.org/)
    |'
- en: '| MM-WHS challenge dataset (2017) [[32](#bib.bib32), [33](#bib.bib33)] | Heart
    | MR and CT images | Whole heart segmentation | 20 labeled and 40 unlabeled CT
    volumes; 20 labeled and 40 unlabeled MR volumes. | [https://zmiclab.github.io/zxh/0/mmwhs](https://zmiclab.github.io/zxh/0/mmwhs)
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| MM-WHS challenge dataset (2017) [[32](#bib.bib32), [33](#bib.bib33)] | 心脏
    | MR 和 CT 图像 | 全心脏分割 | 20 个标注 CT 体积和 40 个未标注 CT 体积；20 个标注 MR 体积和 40 个未标注 MR 体积。
    | [https://zmiclab.github.io/zxh/0/mmwhs](https://zmiclab.github.io/zxh/0/mmwhs)
    |'
- en: '| ACDC (2018) [[34](#bib.bib34)] | Heart | Cine MR images | Classification
    and segmentation | Training: 100 patients; Test: 50 patients | [https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html](https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html)
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ACDC (2018) [[34](#bib.bib34)] | 心脏 | 动态 MR 图像 | 分类和分割 | 训练：100 名患者；测试：50
    名患者 | [https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html](https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html)
    |'
- en: '| Atrial LGE-MRI dataset (2018) [[35](#bib.bib35)] | Heart | Cardiac (LA) segmentation
    | Late gadolinium-enhanced magnetic resonance images (LGE-MRI) | Training: 100
    LGE-MRI; Test: 54 LGE-MRI | [http://atriaseg2018.cardiacatlas.org](http://atriaseg2018.cardiacatlas.org)
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 房颤LGE-MRI数据集 (2018) [[35](#bib.bib35)] | 心脏 | 心房（LA）分割 | 晚期钆增强磁共振图像（LGE-MRI）
    | 训练：100张LGE-MRI；测试：54张LGE-MRI | [http://atriaseg2018.cardiacatlas.org](http://atriaseg2018.cardiacatlas.org)
    |'
- en: '| MSCMRseg (2019) [[36](#bib.bib36)] | Heart | MR images | Cardiac(MYO, RV
    and LV) segmentation | Data was collected from 45 patients, who underwent cardiomyopathy.
    | [https://zmiclab.github.io/zxh/0/mscmrseg19](https://zmiclab.github.io/zxh/0/mscmrseg19)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| MSCMRseg (2019) [[36](#bib.bib36)] | 心脏 | MR图像 | 心脏（MYO、RV和LV）分割 | 数据来自45名接受心肌病治疗的患者。
    | [https://zmiclab.github.io/zxh/0/mscmrseg19](https://zmiclab.github.io/zxh/0/mscmrseg19)
    |'
- en: '| M&Ms (2020) [[37](#bib.bib37)] | Heart | MR images | Cardiac segmentation
    | Training: 175; Validation: 40; Test: 160 MR images | [https://www.ub.edu/mnms/](https://www.ub.edu/mnms/)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| M&Ms (2020) [[37](#bib.bib37)] | 心脏 | MR图像 | 心脏分割 | 训练：175；验证：40；测试：160张MR图像
    | [https://www.ub.edu/mnms/](https://www.ub.edu/mnms/) |'
- en: '| STARE | Eye | Fundus images | Blood vessel segmentation | 20 equal-sized
    (700×605) color fundus images | [https://cecas.clemson.edu/~ahoover/stare/](https://cecas.clemson.edu/~ahoover/stare/)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| STARE | 眼睛 | 眼底图像 | 血管分割 | 20张相同大小（700×605）的彩色眼底图像 | [https://cecas.clemson.edu/~ahoover/stare/](https://cecas.clemson.edu/~ahoover/stare/)
    |'
- en: '| DRIVE (2004) | Eye | Images captured withCanon CR5 non-mydriatic 3CCD camera
    | Vasculature segmentation | Training: 20 images; Test: 20 images | [https://drive.grand-challenge.org/](https://drive.grand-challenge.org/)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| DRIVE (2004) | 眼睛 | 使用Canon CR5非散瞳3CCD相机拍摄的图像 | 血管分割 | 训练：20张图像；测试：20张图像
    | [https://drive.grand-challenge.org/](https://drive.grand-challenge.org/) |'
- en: '| DRISHTI-GS (2014) [[38](#bib.bib38)] | Eye | Fundus images | Optic disc (OD)
    and (OC) cup segmentation | Training: 50 images; Test: 51 images | [https://ieeexplore.ieee.org/document/6867807](https://ieeexplore.ieee.org/document/6867807)
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| DRISHTI-GS (2014) [[38](#bib.bib38)] | 眼睛 | 眼底图像 | 视盘（OD）和视杯（OC）分割 | 训练：50张图像；测试：51张图像
    | [https://ieeexplore.ieee.org/document/6867807](https://ieeexplore.ieee.org/document/6867807)
    |'
- en: '| continued on the next page |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 续下页 |'
- en: 'Table 1: Commonly used datasets for data efficient deep learning in medical
    image analysis (continued).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：用于医学图像分析中数据高效深度学习的常用数据集（续）。 |
- en: '| Dataset | Organ | Types | Task | Description | Link |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 器官 | 类型 | 任务 | 描述 | 链接 |'
- en: '| ReTOUCH (2017) [[39](#bib.bib39)] | Eye | OCT volumes | Fluid detection and
    fluid segmentation | Training: 70 OCT volumes; Test: 42 OCT volumes | [https://retouch.grand-challenge.org](https://retouch.grand-challenge.org)
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ReTOUCH (2017) [[39](#bib.bib39)] | 眼睛 | OCT体积 | 液体检测和液体分割 | 训练：70个OCT体积；测试：42个OCT体积
    | [https://retouch.grand-challenge.org](https://retouch.grand-challenge.org) |'
- en: '| RetinalOCT (2018) [[40](#bib.bib40)] | Eye | Optical Coherence Tomography
    (OCT) Images | Classification | 207,130 OCT images | [https://www.kaggle.com/datasets/paultimothymooney/kermany2018](https://www.kaggle.com/datasets/paultimothymooney/kermany2018)
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| RetinalOCT (2018) [[40](#bib.bib40)] | 眼睛 | 光学相干断层扫描（OCT）图像 | 分类 | 207,130张OCT图像
    | [https://www.kaggle.com/datasets/paultimothymooney/kermany2018](https://www.kaggle.com/datasets/paultimothymooney/kermany2018)
    |'
- en: '| LDLOCTCXR (2018) [[40](#bib.bib40)] | Eye | OCT and Chest X-Ray images |
    Classification | 108,312 images(37,206 with choroidal neovascularization, 11,349
    with diabetic macular edema, 8,617 with drusen, and 51,140 normal) from 4,686
    patient | [https://data.mendeley.com/datasets/rscbjbr9sj/3](https://data.mendeley.com/datasets/rscbjbr9sj/3)
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| LDLOCTCXR (2018) [[40](#bib.bib40)] | 眼睛 | OCT和胸部X射线图像 | 分类 | 108,312张图像（37,206张有脉络膜新生血管，11,349张有糖尿病黄斑水肿，8,617张有视网膜结节，51,140张正常）来自4,686名患者
    | [https://data.mendeley.com/datasets/rscbjbr9sj/3](https://data.mendeley.com/datasets/rscbjbr9sj/3)
    |'
- en: '| PALM (2019) [[41](#bib.bib41)] | Eye | Images captured with Zeiss Visucam
    500 | Classification of normal and myopia fundus; lesion segmentation in pathologic
    myopia. | Training: 400 images, Validation: 400 images; Test: 400 images | [https://palm.grand-challenge.org](https://palm.grand-challenge.org)
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| PALM (2019) [[41](#bib.bib41)] | 眼睛 | 使用蔡司Visucam 500拍摄的图像 | 正常和近视眼底图像分类；病理性近视的病灶分割。
    | 训练：400张图像，验证：400张图像；测试：400张图像 | [https://palm.grand-challenge.org](https://palm.grand-challenge.org)
    |'
- en: '| REFUGE challenge dataset [[42](#bib.bib42)] | Eye | Fundus images | Classification
    of clinical Glaucoma; OD and OC segmentation; Localization of Fovea | 1200 fundus
    images with ground truth segmentations and clinical glaucoma labels | [https://refuge.grand-challenge.org/](https://refuge.grand-challenge.org/)
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| REFUGE challenge dataset [[42](#bib.bib42)] | 眼睛 | 视网膜图像 | 临床青光眼分类；视盘和视杯分割；黄斑定位
    | 1200 张视网膜图像，附有真实分割和临床青光眼标签 | [https://refuge.grand-challenge.org/](https://refuge.grand-challenge.org/)
    |'
- en: '| ADAM (2020) [[43](#bib.bib43)] | Eye | Fundus images captured using a Zeiss
    Visucam 500 fundus camera | Classification; Optic disc detection and segmentation;
    Fovea localization and Lesion detection and segmentation | 1200 retinal fundus
    images | [https://amd.grand-challenge.org/](https://amd.grand-challenge.org/)
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ADAM (2020) [[43](#bib.bib43)] | 眼睛 | 使用 Zeiss Visucam 500 视网膜相机拍摄的视网膜图像
    | 分类；视盘检测与分割；黄斑定位与病变检测与分割 | 1200 张视网膜图像 | [https://amd.grand-challenge.org/](https://amd.grand-challenge.org/)
    |'
- en: '| RIGA+ dataset (2022) [[44](#bib.bib44)] | Eye | Fundus images | Segmentation
    of Optic Disc (OD) and Cup (OC) | 744 labeled samples and 717 Unlabeled samples
    | [https://zenodo.org/record/6325549](https://zenodo.org/record/6325549) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| RIGA+ dataset (2022) [[44](#bib.bib44)] | 眼睛 | 视网膜图像 | 视盘（OD）和视杯（OC）分割 |
    744 张标记样本和 717 张未标记样本 | [https://zenodo.org/record/6325549](https://zenodo.org/record/6325549)
    |'
- en: '| ISIC (2016) | Skin | Dermoscopic lesion images | 1.Lesion Segmentation; 2.Dermoscopic
    Feature Classification and segmentation; 3.Disease Classification | 1.Training:900,
    Test:379 images; 2.Training:807, Test:335 images; 3.Training:900, Test:379 images
    | [https://challenge.isic-archive.com/data/#2016](https://challenge.isic-archive.com/data/#2016)
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ISIC (2016) | 皮肤 | 皮肤镜下病变图像 | 1. 病变分割；2. 皮肤镜特征分类与分割；3. 疾病分类 | 1. 训练：900 张，测试：379
    张图像；2. 训练：807 张，测试：335 张图像；3. 训练：900 张，测试：379 张图像 | [https://challenge.isic-archive.com/data/#2016](https://challenge.isic-archive.com/data/#2016)
    |'
- en: '| HAM10000 (2018) | Skin | Dermatoscopic images | Lesion classification and
    segmentation | 10000 training images | [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T)
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| HAM10000 (2018) | 皮肤 | 皮肤镜图像 | 病变分类与分割 | 10000 张训练图像 | [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T)
    |'
- en: '| MITOS12 [[45](#bib.bib45)] | Breast | Histological Images | Breast cancer
    grading | 50 high power fields (HPF) coming from 5 different slides scanned at
    ×40 magnification | [http://ludo17.free.fr/mitos_2012/dataset.html](http://ludo17.free.fr/mitos_2012/dataset.html)
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| MITOS12 [[45](#bib.bib45)] | 乳腺 | 组织学图像 | 乳腺癌分级 | 来自 5 张不同切片的 50 张高倍视野（HPF）图像，扫描倍率为
    ×40 | [http://ludo17.free.fr/mitos_2012/dataset.html](http://ludo17.free.fr/mitos_2012/dataset.html)
    |'
- en: '| MITOS14 | Breast | Histological Images | Breast cancer grading | Training
    data set there are 284 frames at X20 magnification and 1,136 frames at X40 magnification.
    | [https://mitos-atypia-14.grand-challenge.org/Dataset/](https://mitos-atypia-14.grand-challenge.org/Dataset/)
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| MITOS14 | 乳腺 | 组织学图像 | 乳腺癌分级 | 训练数据集中有 284 张 X20 放大倍数图像和 1136 张 X40 放大倍数图像。
    | [https://mitos-atypia-14.grand-challenge.org/Dataset/](https://mitos-atypia-14.grand-challenge.org/Dataset/)
    |'
- en: '| MIAS (2015) | Breast | Mammograms | Detection; Classification | 322 images
    (161 pairs) at 50 micron resolution in *Portable Gray Map* format | [https://www.kaggle.com/datasets/kmader/mias-mammography](https://www.kaggle.com/datasets/kmader/mias-mammography)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| MIAS (2015) | 乳腺 | 乳腺X光图像 | 检测；分类 | 322 张图像（161 对），分辨率为 50 微米，格式为 *便携灰度图*
    | [https://www.kaggle.com/datasets/kmader/mias-mammography](https://www.kaggle.com/datasets/kmader/mias-mammography)
    |'
- en: '| TUPAC (2016) [[46](#bib.bib46)] | Breast | Whole-slide histopathology images
    | Automatic prediction of tumor proliferation scores of breast tumors | Training:
    500 WSIs; Test: 321 WSIs | [https://github.com/CODAIT/deep-histopath](https://github.com/CODAIT/deep-histopath)
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| TUPAC (2016) [[46](#bib.bib46)] | 乳腺 | 整片组织病理图像 | 自动预测乳腺肿瘤的肿瘤增殖评分 | 训练：500
    张整片图像；测试：321 张整片图像 | [https://github.com/CODAIT/deep-histopath](https://github.com/CODAIT/deep-histopath)
    |'
- en: '| CAMELYON (2016) [[47](#bib.bib47)] | Breast | Whole-slide images (WSIs) |
    Detection and classification of breast cancer metastases | Training: 270 WSI;
    Test: 130 WSI | [https://camelyon16.grand-challenge.org/Data](https://camelyon16.grand-challenge.org/Data)
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| CAMELYON (2016) [[47](#bib.bib47)] | 乳腺 | 整片图像（WSI） | 乳腺癌转移的检测与分类 | 训练：270
    张 WSI；测试：130 张 WSI | [https://camelyon16.grand-challenge.org/Data](https://camelyon16.grand-challenge.org/Data)
    |'
- en: '| CAMELYON (2017) [[47](#bib.bib47)] | Breast | Whole-slide images (WSIs) |
    Detection and classification of breast cancer metastases | Training: 500 WSI;
    Test: 500 WSI | [https://camelyon17.grand-challenge.org/Data](https://camelyon17.grand-challenge.org/Data)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| CAMELYON (2017) [[47](#bib.bib47)] | 乳房 | 全切片图像（WSI） | 乳腺癌转移的检测和分类 | 训练：500
    WSI；测试：500 WSI | [https://camelyon17.grand-challenge.org/Data](https://camelyon17.grand-challenge.org/Data)
    |'
- en: '| CBIS-DDSM (2017) | Breast | Mammograms | Segmentation | Data set contains
    753 calcification cases and 891 mass cases | [https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset](https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset)
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| CBIS-DDSM (2017) | 乳房 | 乳腺X光照片 | 分割 | 数据集包含753例钙化病例和891例肿块病例 | [https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset](https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset)
    |'
- en: '| BACH (2018) [[48](#bib.bib48)] | Breast | Microscopy and Whole-slide images
    | Breast cancer classification | Microscopy: 400 images; WSI: 30 images | [https://iciar2018-challenge.grand-challenge.org/Dataset/](https://iciar2018-challenge.grand-challenge.org/Dataset/)
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| BACH (2018) [[48](#bib.bib48)] | 乳房 | 显微镜和全切片图像 | 乳腺癌分类 | 显微镜：400张图像；WSI：30张图像
    | [https://iciar2018-challenge.grand-challenge.org/Dataset/](https://iciar2018-challenge.grand-challenge.org/Dataset/)
    |'
- en: '| TNBC (2018) | Breast | Histopathology images stained with H&E | Nuclei segmentation
    | Data Set1: 50 images with a total of 4022 annotated cells; Data Set2: 30 images
    from 7 different organs with a total of 21 623 annotated nuclei | [https://ega-archive.org/datasets/EGAD00001000063](https://ega-archive.org/datasets/EGAD00001000063)
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| TNBC (2018) | 乳房 | H&E染色的组织病理图像 | 细胞核分割 | 数据集1：50张图像，共4022个标注细胞；数据集2：30张图像，来自7个不同器官，共21,623个标注细胞核
    | [https://ega-archive.org/datasets/EGAD00001000063](https://ega-archive.org/datasets/EGAD00001000063)
    |'
- en: '| FNAC (2019) [[49](#bib.bib49)] | Breast | Cytology images | Classification
    | 212 images in two classes: benign (99) and malignant (113) | [https://1drv.ms/u/s!Al-T6d-_ENf6axsEbvhbEc2gUFs](https://1drv.ms/u/s!Al-T6d-_ENf6axsEbvhbEc2gUFs)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| FNAC (2019) [[49](#bib.bib49)] | 乳房 | 细胞学图像 | 分类 | 212张图像，分为两类：良性（99张）和恶性（113张）
    | [https://1drv.ms/u/s!Al-T6d-_ENf6axsEbvhbEc2gUFs](https://1drv.ms/u/s!Al-T6d-_ENf6axsEbvhbEc2gUFs)
    |'
- en: '| NYUBCS (2019) | Breast | Mammograms | Segmentation | 29,426 digital screening
    mammography exams (1,001,093 images) from 141,473 patients | [https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf](https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| NYUBCS (2019) | 乳房 | 乳腺X光照片 | 分割 | 29,426次数字筛查乳腺X光检查（1,001,093张图像），来自141,473名患者
    | [https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf](https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf)
    |'
- en: '| BreastPathQ (2019) [[50](#bib.bib50)] | Breast | Whole slide images stained
    with H&E | Estimation of tumor cellularity (TC) | Training: 2,579 patches extracted
    from 69 WSIs; Test: 1,121 patches extracted from 25 WSIs | [https://breastpathq.grand-challenge.org/Overview/](https://breastpathq.grand-challenge.org/Overview/)
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| BreastPathQ (2019) [[50](#bib.bib50)] | 乳房 | H&E染色的全切片图像 | 肿瘤细胞密度（TC）的估计
    | 训练：从69个WSI提取的2579个补丁；测试：从25个WSI提取的1121个补丁 | [https://breastpathq.grand-challenge.org/Overview/](https://breastpathq.grand-challenge.org/Overview/)
    |'
- en: '| CERVIX93 (2018) [[51](#bib.bib51)] | Cervix | Cytology images | Classification;
    detection | 93 stacks of images (2705 nuclei) | [https://github.com/parhamap/cytology_dataset](https://github.com/parhamap/cytology_dataset)
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| CERVIX93 (2018) [[51](#bib.bib51)] | 子宫颈 | 细胞学图像 | 分类；检测 | 93堆图像（2705个细胞核）
    | [https://github.com/parhamap/cytology_dataset](https://github.com/parhamap/cytology_dataset)
    |'
- en: '| LBC (2020) [[52](#bib.bib52)] | Cervix | Cytology images | Classification
    | 963 LBC images in classes of NILM, LSIL, HSIL, and SCC | [https://data.mendeley.com/datasets/zddtpgzv63/4](https://data.mendeley.com/datasets/zddtpgzv63/4)
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| LBC (2020) [[52](#bib.bib52)] | 子宫颈 | 细胞学图像 | 分类 | 963张LBC图像，分类为NILM、LSIL、HSIL和SCC
    | [https://data.mendeley.com/datasets/zddtpgzv63/4](https://data.mendeley.com/datasets/zddtpgzv63/4)
    |'
- en: '| CHAOS (2021) [[53](#bib.bib53)] | Abdomen | CT and MR images | Liver and
    Abdominal segmentation | CT: 40 images; MRI: 120 DICOM data sets | [https://chaos.grand-challenge.org/](https://chaos.grand-challenge.org/)
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| CHAOS (2021) [[53](#bib.bib53)] | 腹部 | CT和MR图像 | 肝脏和腹部分割 | CT: 40 张图像; MRI:
    120 个DICOM数据集 | [https://chaos.grand-challenge.org/](https://chaos.grand-challenge.org/)
    |'
- en: '| KiTS (2023) | Kidney | CT scan | Kidney Tumor Segmentation | Training: 489
    cases; Test: 110 cases | [https://kits-challenge.org/kits23/](https://kits-challenge.org/kits23/)
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| KiTS (2023) | 肾脏 | CT扫描 | 肾肿瘤分割 | 训练：489例；测试：110例 | [https://kits-challenge.org/kits23/](https://kits-challenge.org/kits23/)
    |'
- en: '| LiTS (2017) | Liver | CT scans | Liver lesions segmentation | Training: 130
    CT scans; Test: 70 CT scans | [https://competitions.codalab.org/competitions/17094](https://competitions.codalab.org/competitions/17094)
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LiTS (2017) | 肝脏 | CT 扫描 | 肝脏病变分割 | 训练：130个CT扫描；测试：70个CT扫描 | [https://competitions.codalab.org/competitions/17094](https://competitions.codalab.org/competitions/17094)
    |'
- en: '| Asciteps (2020) [[54](#bib.bib54)] | Stomach | Classification; detection
    | Cytology images | 487 images for classification: malignant(18,558) and benign(6089);
    176 images for detection (6573 bounding boxes) | [https://pan.baidu.com/s/1r0cd0PVm5DiUmaNozMSxgg](https://pan.baidu.com/s/1r0cd0PVm5DiUmaNozMSxgg)
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Asciteps (2020) [[54](#bib.bib54)] | 胃 | 分类；检测 | 细胞学图像 | 487张分类图像：恶性(18,558)和良性(6089)；176张检测图像（6573个边界框）
    | [https://pan.baidu.com/s/1r0cd0PVm5DiUmaNozMSxgg](https://pan.baidu.com/s/1r0cd0PVm5DiUmaNozMSxgg)
    |'
- en: '| continued on the next page |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 续见下一页 |'
- en: 'Table 1: Commonly used datasets for data efficient deep learning in medical
    image analysis (continued).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：用于医学图像分析中数据高效深度学习的常用数据集（续）。
- en: '| Dataset | Organ | Types | Task | Description | Link |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 器官 | 类型 | 任务 | 描述 | 链接 |'
- en: '| MoNuSeg (2017) [[55](#bib.bib55)] | Multi-organ | H&E stained tissue images
    | Nuclei segmentation | Training: 30 images and around 22,000 nuclear boundary
    annotations; Test: 7000 nuclear boundary annotations | [https://monuseg.grand-challenge.org/](https://monuseg.grand-challenge.org/)
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| MoNuSeg (2017) [[55](#bib.bib55)] | 多脏器 | H&E 染色组织图像 | 细胞核分割 | 训练：30张图像和约22,000个核边界标注；测试：7000个核边界标注
    | [https://monuseg.grand-challenge.org/](https://monuseg.grand-challenge.org/)
    |'
- en: '| BTCV (2017) [[56](#bib.bib56)] | Multi-organ | CT images | Multi-organ segmentation
    | 90 abdominal CT images | [nhttps://zenodo.org/record/1169361#.Y8Ud-OxBwUE](nhttps://zenodo.org/record/1169361#.Y8Ud-OxBwUE)
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| BTCV (2017) [[56](#bib.bib56)] | 多脏器 | CT 图像 | 多脏器分割 | 90张腹部CT图像 | [nhttps://zenodo.org/record/1169361#.Y8Ud-OxBwUE](nhttps://zenodo.org/record/1169361#.Y8Ud-OxBwUE)
    |'
- en: '| DeepLesion (2018) [[57](#bib.bib57)] | Multi-organ | CT slices | For different
    applications | 32,735 lesions in 32,120 CT slices | [https://nihcc.app.box.com/v/DeepLesion](https://nihcc.app.box.com/v/DeepLesion)
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DeepLesion (2018) [[57](#bib.bib57)] | 多脏器 | CT 切片 | 用于不同应用 | 32,735个病变在32,120个CT切片中
    | [https://nihcc.app.box.com/v/DeepLesion](https://nihcc.app.box.com/v/DeepLesion)
    |'
- en: '| DECATHLON (2019) | Multi-organ | CT and MRI | Segmentation | Brain: 750 MRI;
    Heart: 30 MRI; Liver: 201 CT images; Hippocampus: 195 MRI; Prostate: 48 MRI; Lung:
    96 CT scans; Pancreas: 420 CT scans; HepaticVessel: 443 CT scans; Spleen: 61 CT
    scans; Colon: 190 CT scans | [http://medicaldecathlon.com/](http://medicaldecathlon.com/)
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| DECATHLON (2019) | 多脏器 | CT 和 MRI | 分割 | 大脑：750个MRI；心脏：30个MRI；肝脏：201张CT图像；海马体：195个MRI；前列腺：48个MRI；肺：96个CT扫描；胰腺：420个CT扫描；肝血管：443个CT扫描；脾脏：61个CT扫描；结肠：190个CT扫描
    | [http://medicaldecathlon.com/](http://medicaldecathlon.com/) |'
- en: '| MIDOG [[58](#bib.bib58)] | Multi-organ | Whole Slide Images | Segmentation
    | Canine Lung Cancer: 44 cases; Human Breast Cancer: 150 cases; Canine Lymphoma:
    55 cases; Human neuroendocrine tumor: 55 cases; Canine Cutaneous Mast Cell Tumor:
    50 cases; Human melanoma: 49 cases | [https://imig.science/midog/the-dataset/](https://imig.science/midog/the-dataset/)
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| MIDOG [[58](#bib.bib58)] | 多脏器 | 全幻灯片图像 | 分割 | 犬类肺癌：44例；人类乳腺癌：150例；犬类淋巴瘤：55例；人类神经内分泌肿瘤：55例；犬类皮肤肥大细胞瘤：50例；人类黑色素瘤：49例
    | [https://imig.science/midog/the-dataset/](https://imig.science/midog/the-dataset/)
    |'
- en: '| CRCHistoPhenotypes (2016) [[59](#bib.bib59)] | Colon | Histology images |
    Cancer classification | 100 H&E stained histology images of colorectal adenocarcinomas
    | [https://warwick.ac.uk/fac/crossfac/tia/data/crchistolabelednucleihe](https://warwick.ac.uk/fac/crossfac/tia/data/crchistolabelednucleihe)
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| CRCHistoPhenotypes (2016) [[59](#bib.bib59)] | 结肠 | 组织学图像 | 癌症分类 | 100张H&E染色的结直肠腺癌组织学图像
    | [https://warwick.ac.uk/fac/crossfac/tia/data/crchistolabelednucleihe](https://warwick.ac.uk/fac/crossfac/tia/data/crchistolabelednucleihe)
    |'
- en: '| KATHER (2018) [[60](#bib.bib60)] | Colon | Histological images | Cancer classification
    | 100,000 histological images of human colorectal cancer and healthy tissue |
    [https://zenodo.org/record/1214456#.Y8fgV-zP1hE](https://zenodo.org/record/1214456#.Y8fgV-zP1hE)
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| KATHER (2018) [[60](#bib.bib60)] | 结肠 | 组织学图像 | 癌症分类 | 100,000张人类结直肠癌和健康组织的组织学图像
    | [https://zenodo.org/record/1214456#.Y8fgV-zP1hE](https://zenodo.org/record/1214456#.Y8fgV-zP1hE)
    |'
- en: '| PROMISE12 challenge dataset [[61](#bib.bib61)] | Prostate | MR images | Prostate
    segmentation | Training: 50; Test: 30; Live challenge: 20 datasets | [promise12.grand-challenge.org/](promise12.grand-challenge.org/)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| PROMISE12 挑战数据集 [[61](#bib.bib61)] | 前列腺 | MR 图像 | 前列腺分割 | 训练：50；测试：30；现场挑战：20个数据集
    | [promise12.grand-challenge.org/](promise12.grand-challenge.org/) |'
- en: '| TMA-Zurich (2018) [[62](#bib.bib62)] | Prostate | Histopathology images |
    Gleason grading of prostate cancer | Training: 641 patients; Test: 245 patients
    | [https://www.nature.com/articles/s41598-018-30535-1?source=app#data-availability](https://www.nature.com/articles/s41598-018-30535-1?source=app#data-availability)
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| TMA-Zurich (2018) [[62](#bib.bib62)] | 前列腺 | 组织病理图像 | 前列腺癌的Gleason分级 | 训练：641名患者；测试：245名患者
    | [https://www.nature.com/articles/s41598-018-30535-1?source=app#data-availability](https://www.nature.com/articles/s41598-018-30535-1?source=app#data-availability)
    |'
- en: '| The Cancer Genome Atlas (TCGA) dataset | Prostate | Histopathology WSIs |
    Cancer tumour classification based on gleason scores | 20,000 patient samples
    spanning 33 cancer types | [https://portal.gdc.cancer.gov/repository](https://portal.gdc.cancer.gov/repository)
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 癌症基因组图谱 (TCGA) 数据集 | 前列腺 | 组织病理WSI | 基于Gleason评分的癌症肿瘤分类 | 20,000个患者样本，涵盖33种癌症类型
    | [https://portal.gdc.cancer.gov/repository](https://portal.gdc.cancer.gov/repository)
    |'
- en: '| PANDA (2020) [[63](#bib.bib63)] | Prostate | Whole-slide images | Gleason
    grading of prostate cancer | Development set: 10,616 biopsies; Tuning set: 393;
    Internal validation set: 545; External validation: 1071 | [https://www.kaggle.com/c/prostate-cancer-grade-assessment/data](https://www.kaggle.com/c/prostate-cancer-grade-assessment/data)
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| PANDA (2020) [[63](#bib.bib63)] | 前列腺 | 全片图像 | 前列腺癌的Gleason分级 | 开发集：10,616个活检；调优集：393；内部验证集：545；外部验证集：1071
    | [https://www.kaggle.com/c/prostate-cancer-grade-assessment/data](https://www.kaggle.com/c/prostate-cancer-grade-assessment/data)
    |'
- en: '| SCGM dataset [[64](#bib.bib64)] | Spinal Cord | MRI images | Spinal cord
    gray matter segmentation | Training: 40 images; Test: 40 images | [http://niftyweb.cs.ucl.ac.uk/program.php?p=CHALLENGE](http://niftyweb.cs.ucl.ac.uk/program.php?p=CHALLENGE)
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SCGM数据集 [[64](#bib.bib64)] | 脊髓 | MRI图像 | 脊髓灰质分割 | 训练：40张图像；测试：40张图像 | [http://niftyweb.cs.ucl.ac.uk/program.php?p=CHALLENGE](http://niftyweb.cs.ucl.ac.uk/program.php?p=CHALLENGE)
    |'
- en: '| Montgomery (2014) [[65](#bib.bib65)] | Chest | Chest X-rays | Segmentation
    | 138 images in two classes: normal (80) and manifestations of TB (58) | [https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-montgomery](https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-montgomery)
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Montgomery (2014) [[65](#bib.bib65)] | 胸部 | 胸部X光 | 分割 | 138张图像分为两类：正常（80）和TB表现（58）
    | [https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-montgomery](https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-montgomery)
    |'
- en: '| Shenzhen (2014) [[65](#bib.bib65)] | Chest | Chest X-rays | Segmentation
    | 662 images in two classes: normal (326) and manifestations of TB (336) | [https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen](https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen)
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 深圳 (2014) [[65](#bib.bib65)] | 胸部 | 胸部X光 | 分割 | 662张图像分为两类：正常（326）和TB表现（336）
    | [https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen](https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen)
    |'
- en: '| NIH Chest X-ray (2017) [[66](#bib.bib66)] | Chest | Chest X-rays | Classification
    | 112,120 X-ray images with disease labels from 30,805 unique patients. | [https://www.kaggle.com/datasets/nih-chest-xrays/data](https://www.kaggle.com/datasets/nih-chest-xrays/data)
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| NIH胸部X光 (2017) [[66](#bib.bib66)] | 胸部 | 胸部X光 | 分类 | 112,120张X光图像，来自30,805名独特患者的疾病标签。
    | [https://www.kaggle.com/datasets/nih-chest-xrays/data](https://www.kaggle.com/datasets/nih-chest-xrays/data)
    |'
- en: '| ChestX-ray8 (2017) [[66](#bib.bib66)] | Chest | Chest x-ray images | Classification
    and Localization of Common Thorax Diseases | 108,948 frontal-view X-ray images
    of 32,717 unique patients with the text-mined eight disease image labels | [https://nihcc.app.box.com/v/ChestXray-NIHCC/](https://nihcc.app.box.com/v/ChestXray-NIHCC/)
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ChestX-ray8 (2017) [[66](#bib.bib66)] | 胸部 | 胸部X光图像 | 常见胸部疾病的分类和定位 | 108,948张正面X光图像，来自32,717名独特患者，带有文本挖掘的八种疾病图像标签
    | [https://nihcc.app.box.com/v/ChestXray-NIHCC/](https://nihcc.app.box.com/v/ChestXray-NIHCC/)
    |'
- en: '| MIMIC-CXR (2019) [[67](#bib.bib67)] | Chest | Chest x-ray images | Detection
    | Total of 377,110 images with semi-structured free-text radiology report that
    describes the radiological findings of the images | [https://physionet.org/content/mimic-cxr/2.0.0/](https://physionet.org/content/mimic-cxr/2.0.0/)
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| MIMIC-CXR (2019) [[67](#bib.bib67)] | 胸部 | 胸部X光图像 | 检测 | 总计377,110张带有半结构化自由文本放射报告的图像，描述了图像的放射学发现
    | [https://physionet.org/content/mimic-cxr/2.0.0/](https://physionet.org/content/mimic-cxr/2.0.0/)
    |'
- en: '| ChestX-ray14 (2019) | Chest | Chest x-ray images | Classification and Localization
    of Common Thorax Diseases | 112,120 frontal chest radiographs from 30,805 distinct
    patients with 14 binary labels | [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ChestX-ray14 (2019) | 胸部 | 胸部 X 光图像 | 常见胸部疾病的分类和定位 | 112,120 张来自 30,805 名不同患者的正面胸部
    X 光片，带有 14 个二元标签 | [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)
    |'
- en: '| CC-COVID (2020) [[68](#bib.bib68)] | Chest | CT images | Lung-lesion segmentation
    | 532,506 CT images from NCP, common pneumonia, and normal controls | [https://ncov-ai.big.ac.cn/download?lang=en](https://ncov-ai.big.ac.cn/download?lang=en)
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| CC-COVID (2020) [[68](#bib.bib68)] | 胸部 | CT 图像 | 肺部病变分割 | 532,506 张来自 NCP、普通肺炎和正常对照的
    CT 图像 | [https://ncov-ai.big.ac.cn/download?lang=en](https://ncov-ai.big.ac.cn/download?lang=en)
    |'
- en: '| SegTHOR (2020) [[69](#bib.bib69)] | Chest | CT images | Segmentation of Thoracic
    Organs | Training: 40 CT scans; Test: 20 CT scans | [https://competitions.codalab.org/competitions/21145](https://competitions.codalab.org/competitions/21145)
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SegTHOR (2020) [[69](#bib.bib69)] | 胸部 | CT 图像 | 胸部器官分割 | 训练：40 张 CT 扫描；测试：20
    张 CT 扫描 | [https://competitions.codalab.org/competitions/21145](https://competitions.codalab.org/competitions/21145)
    |'
- en: '| VinDr-CXR (2021) [[70](#bib.bib70)] | Chest | Chest x-ray images | Classification;
    Detection | Training: 15000 scans; Test: 3000 scans | [https://vindr.ai/datasets/cxr](https://vindr.ai/datasets/cxr)
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| VinDr-CXR (2021) [[70](#bib.bib70)] | 胸部 | 胸部 X 光图像 | 分类；检测 | 训练：15000 张扫描；测试：3000
    张扫描 | [https://vindr.ai/datasets/cxr](https://vindr.ai/datasets/cxr) |'
- en: '| ChestXR (2021) | Chest | Chest x-ray images | Classification | 20,000+ images
    and 3 classes: COVID-19, Pneumonia and Normal cases | [https://cxr-covid19.grand-challenge.org/Dataset/](https://cxr-covid19.grand-challenge.org/Dataset/)
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ChestXR (2021) | 胸部 | 胸部 X 光图像 | 分类 | 20,000+ 张图像和 3 个类别：COVID-19、肺炎和正常病例
    | [https://cxr-covid19.grand-challenge.org/Dataset/](https://cxr-covid19.grand-challenge.org/Dataset/)
    |'
- en: '| MICCAI2018 IVDM3Seg dataset | Intervertebral Disc | MRI images | Intervertebral
    discs (IVD) localization and segmentation | 24 3D multi-modality MRI data sets
    each data set contains four aligned high-resolution 3D volumes, so total 96 high-resolution
    3D MRI volume data | [https://ivdm3seg.weebly.com/data.html](https://ivdm3seg.weebly.com/data.html)
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MICCAI2018 IVDM3Seg 数据集 | 椎间盘 | MRI 图像 | 椎间盘 (IVD) 定位和分割 | 24 个 3D 多模态 MRI
    数据集，每个数据集包含四个对齐的高分辨率 3D 卷积，因此总共 96 个高分辨率 3D MRI 卷积数据 | [https://ivdm3seg.weebly.com/data.html](https://ivdm3seg.weebly.com/data.html)
    |'
- en: 2 No supervision
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 无监督
- en: 'Learning with no supervision, commonly referred to as unsupervised learning,
    involves the challenge of obtaining supervision signals in the absence of explicit
    guidance. One primary technique used for this purpose is self-supervised learning
    (SSL). In SSL, representations are acquired by training on an auxiliary pretext
    task and later transferred to a target downstream task of interest. The effectiveness
    of SSL relies significantly on the design of well-crafted pretext tasks. These
    pretext tasks introduce implicit inductive biases into the model, making it crucial
    to select them thoughtfully to ensure their relevance to the specific domain of
    interest. Self-supervised learning can be divided into four broad categories:
    predictive, generative, contrastive, and multi self-supervision [[71](#bib.bib71)].
    A summary of recent methods for learning with no supervision is provided in Table [2](#S2.T2
    "Table 2 ‣ 2.4 Multi-self supervised learning: combining multiple SSL pretext
    tasks into one framework ‣ 2 No supervision ‣ Data efficient deep learning for
    medical image analysis: A survey").'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '无监督学习，通常称为自监督学习，涉及在没有明确指导的情况下获取监督信号的挑战。一个主要的技术是自监督学习 (SSL)。在 SSL 中，通过训练辅助的前置任务来获取表示，然后将其转移到感兴趣的目标下游任务。SSL
    的有效性在很大程度上依赖于精心设计的前置任务。这些前置任务为模型引入了隐含的归纳偏差，因此选择它们时必须慎重，以确保它们与特定领域的相关性。自监督学习可以分为四大类：预测性、自生成、对比性和多重自监督
    [[71](#bib.bib71)]。表[2](#S2.T2 "Table 2 ‣ 2.4 Multi-self supervised learning:
    combining multiple SSL pretext tasks into one framework ‣ 2 No supervision ‣ Data
    efficient deep learning for medical image analysis: A survey") 提供了无监督学习的最新方法的总结。'
- en: 2.1 Predictive self-supervision
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 预测性自监督
- en: 'In this section, we explore predictive self-supervision, where the pretext
    task is cast as either a classification or regression task. Specifically, each
    unlabeled image is assigned a pseudo label, which is generated directly from the
    data itself. These pseudo labels can take on categorical or numerical values,
    depending on the design specifications of the pretext task. Common transformation-based
    predictive tasks involve aspects such as assessing relative position [[72](#bib.bib72)],
    solving jigsaw puzzles [[73](#bib.bib73)], and determining rotation angles [[74](#bib.bib74)],
    among others. These traditional pretext tasks, and their variations, have been
    explored in MIA and have demonstrated their effectiveness. For instance, Bai et
    al. [[75](#bib.bib75)] introduced an approach for segmenting cardiac MRI scans
    by proposing a pretext task focused on predicting anatomical positions. This pretext
    task aimed to utilize the various cardiac views available in the MRI scans, such
    as short-axis, 2CH long-axis, and 4CH long-axis, to represent different cardiac
    anatomical regions, including the left and right atrium and ventricle. To accomplish
    this, the authors defined a series of bounding boxes corresponding to specific
    anatomical positions within a given view and trained their network to predict
    these positions. Taleb et al. [[76](#bib.bib76)] introduced a novel approach inspired
    by Jigsaw puzzle-solving, which makes use of multiple imaging modalities. In this
    method, an input image is composed of disordered patches from different modalities,
    and the model’s task is to reconstruct the original image by correctly assembling
    these patches. Their work represents a notable enhancement over the traditional
    Jigsaw puzzle approach. Zhuang et al. [[77](#bib.bib77)] proposed a self-supervised
    task called Rubik cube recovery, inspired by the early work on Jigsaw puzzle solving
    for 2D natural images. The task involves two operations: cube rearrangement and
    cube rotation, as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Predictive self-supervision
    ‣ 2 No supervision ‣ Data efficient deep learning for medical image analysis:
    A survey"). The Rubik cube recovery task uses 3D input, where a Rubik cube is
    divided into a 3D grid of 2×2×2 sub-cubes. The addition of the cube rotation task
    ensures learning of rotation invariant features, going beyond the original Jigsaw
    puzzle task, which only focuses on learning translation-invariant features. Rubik
    cube+ [[78](#bib.bib78)] improves upon the Rubik cube recovery pretext task by
    using cube masking operation along with both cube rearrangement and cube rotation
    operations. Nguyen et al. [[79](#bib.bib79)] introduced a spatial awareness pretext
    task with the aim of acquiring semantic and spatial representations from volumetric
    images. This concept of a spatial pretext task was influenced by Chen et al.’s
    [[80](#bib.bib80)] context restoration framework; however, it was formulated here
    into a classification problem. Recently, Zhou et al. [[81](#bib.bib81)] performed
    multi-scale pixel restoration and siamese feature comparison within the feature
    pyramid. This approach effectively retains semantic, pixel-level, and scale information
    all at once.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们探讨预测自我监督，其中前置任务被设定为分类或回归任务。具体而言，每个未标记的图像都会被分配一个伪标签，该标签直接从数据本身生成。这些伪标签可以是分类的或数值的，取决于前置任务的设计规格。常见的基于变换的预测任务包括评估相对位置
    [[72](#bib.bib72)]、解决拼图难题 [[73](#bib.bib73)] 和确定旋转角度 [[74](#bib.bib74)] 等。这些传统的前置任务及其变体在MIA中得到了探讨，并展示了其有效性。例如，Bai等人
    [[75](#bib.bib75)] 提出了一个用于分割心脏MRI扫描的方案，提出了一个以预测解剖位置为重点的前置任务。该前置任务旨在利用MRI扫描中可用的不同心脏视图，如短轴、2CH长轴和4CH长轴，来表示不同的心脏解剖区域，包括左心房、右心房和心室。为此，作者定义了一系列与特定解剖位置对应的边界框，并训练网络以预测这些位置。Taleb等人
    [[76](#bib.bib76)] 提出了一种受拼图难题启发的新方法，该方法利用多种成像模态。在这种方法中，输入图像由来自不同模态的无序块组成，模型的任务是通过正确地拼装这些块来重建原始图像。他们的工作代表了对传统拼图方法的显著提升。Zhuang等人
    [[77](#bib.bib77)] 提出了一个自监督任务，称为魔方恢复，灵感来源于早期针对2D自然图像的拼图解决工作。该任务涉及两个操作：魔方重新排列和魔方旋转，如图 [2](#S2.F2
    "Figure 2 ‣ 2.1 Predictive self-supervision ‣ 2 No supervision ‣ Data efficient
    deep learning for medical image analysis: A survey")所示。魔方恢复任务使用3D输入，其中魔方被划分为2×2×2子立方体的3D网格。增加魔方旋转任务确保学习旋转不变特征，超越了原始的拼图任务，后者仅关注学习平移不变特征。Rubik
    cube+ [[78](#bib.bib78)] 通过结合魔方掩模操作以及魔方重新排列和魔方旋转操作，改进了魔方恢复前置任务。Nguyen等人 [[79](#bib.bib79)]
    提出了一个空间感知前置任务，旨在从体积图像中获取语义和空间表示。这个空间前置任务的概念受到Chen等人 [[80](#bib.bib80)] 上下文恢复框架的影响；然而，在这里它被形成了一个分类问题。最近，Zhou等人
    [[81](#bib.bib81)] 在特征金字塔内进行了多尺度像素恢复和孪生特征比较。这种方法有效地同时保留了语义、像素级别和尺度信息。'
- en: '![Refer to caption](img/8a823e006cf90f1a49ca0991edc59689.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8a823e006cf90f1a49ca0991edc59689.png)'
- en: 'Figure 2: Illustration of the Rubik’s Cube pretext task: A Siamese network
    with M (representing the number of cubes) shared-weight branches, referred to
    as Siamese-Octad, is employed to solve the Rubik’s Cube. The backbone network
    for each branch can be any well-known 3D CNN. The feature maps derived from the
    final fully-connected or convolutional layer of all branches are concatenated
    and used as input for separate tasks’ fully-connected layers, namely cube ordering
    and orientation. These tasks are supervised by the permutation loss ($L_{P}$)
    and rotation loss ($L_{R}$), respectively (image from [[77](#bib.bib77)]).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：魔方预文本任务的示意图：一个具有 M（表示魔方数量）共享权重分支的孪生网络，称为Siamese-Octad，用于解决魔方问题。每个分支的主干网络可以是任何著名的
    3D CNN。所有分支的最终全连接或卷积层提取的特征图被连接在一起，作为单独任务全连接层的输入，即魔方排序和方向。这些任务分别通过排列损失（$L_{P}$）和旋转损失（$L_{R}$）进行监督（图片来源
    [[77](#bib.bib77)]）。
- en: 2.2 Generative self-supervision
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 生成性自监督
- en: 'The generative self-supervised learning approach seeks to learn underlying
    features in the input data by framing pretext tasks as generative problems [[71](#bib.bib71)].
    The idea behind generative pretext tasks is that the model can acquire valuable
    representations from unlabeled data by either learning to reconstruct the input
    data itself or by generating new examples that follow the same distribution as
    the input data. Ross et al. [[82](#bib.bib82)] utilized the image colorization
    pretext task to address the segmentation of endoscopic medical instruments in
    endoscopic video data. However, instead of using the original architecture employed
    in the colorization task, they opted for a conditional Generative Adversarial
    Network (GAN) architecture. This choice aimed to promote the generation of more
    realistic colored images. The authors evaluated their approach on six datasets
    from both medical and natural domains to assess its effectiveness in downstream
    tasks. Chen et al. [[80](#bib.bib80)] introduced a new generative pretext task
    that involves randomly selecting two isolated patches from an input image and
    swapping their positions. This swapping process is repeated iteratively, resulting
    in a corrupted version of the original image while preserving its overall distribution.
    Subsequently, a generative model is used to restore the corrupted image back to
    its original version (see Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Generative self-supervision
    ‣ 2 No supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")). Building upon earlier context-restoration-based studies, Zhou et
    al. [[83](#bib.bib83)] incorporated four data transformations (non-linear transformation,
    local-shuffling, outer-cutout, and inner-cutout) into a cohesive reconstruction
    model called *Model Genesis*. Harvella et al. [[10](#bib.bib10)] introduced a
    self-supervised multi-modal reconstruction task for retinal anatomy learning.
    They assumed that distinct modalities of the same organ could offer complementary
    knowledge, leading to valuable representations for subsequent tasks.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性自监督学习方法试图通过将预文本任务构建为生成问题来学习输入数据中的潜在特征 [[71](#bib.bib71)]。生成性预文本任务的想法是，通过学习重建输入数据本身或生成符合输入数据相同分布的新示例，模型可以从未标记的数据中获得有价值的表示。Ross
    等人 [[82](#bib.bib82)] 利用图像着色预文本任务来解决内窥镜视频数据中内窥镜医疗仪器的分割问题。然而，他们选择了条件生成对抗网络（GAN）架构，而不是使用图像着色任务中使用的原始架构。这个选择旨在促进生成更逼真的彩色图像。作者在来自医学和自然领域的六个数据集上评估了他们的方法，以评估其在下游任务中的有效性。Chen
    等人 [[80](#bib.bib80)] 引入了一种新的生成性预文本任务，涉及从输入图像中随机选择两个孤立的区域并交换其位置。这个交换过程反复进行，产生了一个破坏的原始图像版本，同时保留了其整体分布。随后，生成模型用于将破坏的图像恢复到其原始版本（见图
    [3](#S2.F3 "图 3 ‣ 2.2 生成性自监督 ‣ 2 无监督 ‣ 用于医学图像分析的数据高效深度学习：综述")）。在早期基于上下文恢复的研究基础上，Zhou
    等人 [[83](#bib.bib83)] 将四种数据变换（非线性变换、局部打乱、外部切割和内部切割）融入到一个名为 *Model Genesis* 的一致重建模型中。Harvella
    等人 [[10](#bib.bib10)] 引入了一种自监督的多模态重建任务用于视网膜解剖学习。他们假设同一器官的不同模态可以提供互补的知识，从而为后续任务提供有价值的表示。
- en: 'In the medical domain, conventional pretext tasks that heavily rely on the
    existence of bigger objects in natural images are inadequate because disease-related
    features are usually found in smaller regions of the medical image. To address
    this, Holmberg et al. [[84](#bib.bib84)] introduced a pretext task, cross-modal
    self-supervised retinal thickness prediction, for ophthalmic disease diagnosis.
    This task involves the utilization of two distinct modalities: infrared fundus
    images and optical coherence tomography scans (OCT). Initially, they extracted
    retinal thickness maps from OCT scans by training a segmentation model with the
    limited annotated dataset, which served as ground-truth annotations for the preliminary
    task. Then, a model was trained to predict the thickness maps utilizing unlabeled
    fundus images and the previously predicted thickness maps as labels. Other examples
    of generative self-supervised pretext tasks include the image denoising method
    proposed by Prakash et al. [[85](#bib.bib85)] and the Rubik cube++ (introduced
    by Tao et al. [[86](#bib.bib86)]). In the Rubik cube++ approach, significant modifications
    were made to the earlier Rubik cube method [[77](#bib.bib77)]. Instead of treating
    it as a classification task, they approached it as a generative problem using
    a GAN-based framework. The generator’s task was to bring back the initial arrangement
    of the Rubik cube before applying transformations, whereas the discriminator was
    responsible for distinguishing between correct and incorrect arrangements of the
    generated cubes.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学领域，传统的前置任务过于依赖自然图像中较大物体的存在，而这些任务通常不适用，因为与疾病相关的特征通常出现在医学图像的较小区域。为了解决这个问题，Holmberg等人[[84](#bib.bib84)]提出了一种前置任务，即跨模态自监督视网膜厚度预测，用于眼科疾病诊断。该任务涉及使用两种不同的模态：红外视网膜图像和光学相干断层扫描（OCT）。最初，他们通过使用有限的标注数据集训练一个分割模型，提取OCT扫描中的视网膜厚度图，这些图作为初步任务的真实注释。然后，训练一个模型来预测厚度图，利用未标记的视网膜图像和之前预测的厚度图作为标签。其他生成式自监督前置任务的例子包括Prakash等人[[85](#bib.bib85)]提出的图像去噪方法以及Tao等人[[86](#bib.bib86)]引入的Rubik
    cube++。在Rubik cube++方法中，对早期的Rubik cube方法[[77](#bib.bib77)]进行了显著修改。他们没有将其视为分类任务，而是采用了基于GAN的框架作为生成问题。生成器的任务是恢复Rubik
    cube的初始排列，然后再进行变换，而判别器则负责区分生成的立方体的正确和错误排列。
- en: '![Refer to caption](img/242038d660542e7e5b10dae4949632bb.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/242038d660542e7e5b10dae4949632bb.png)'
- en: 'Figure 3: CNN architecture for self-supervised context restoration learning,
    where blue, green, and orange strides indicate convolutional units, downsampling
    units, and upsampling units, respectively. The specific structure of the CNN in
    the reconstruction part may vary based on the subsequent task (image from [[80](#bib.bib80)]).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：用于自监督上下文恢复学习的CNN架构，其中蓝色、绿色和橙色的步幅分别表示卷积单元、下采样单元和上采样单元。CNN在重建部分的具体结构可能会根据后续任务有所变化（图像来源于[[80](#bib.bib80)]）。
- en: '![Refer to caption](img/044bac0f629c564d3a0eeed267225fa3.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/044bac0f629c564d3a0eeed267225fa3.png)'
- en: 'Figure 4: Illustration of the enhanced SimCLR for 3D medical image segmentation:
    (i) Outline of the global contrastive loss employed for pre-training the encoder
    $e$ using dense layers $g_{1}$. (ii) Outline of the local contrastive loss utilized
    for pre-training the decoder $d_{l}$ with 1 × 1 convolutional layers $g_{2}$,
    with frozen weights of encoder $e$ obtained from the previous training stage (image
    from [[87](#bib.bib87)]).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：增强SimCLR在3D医学图像分割中的示意图：（i）用于预训练编码器$e$的全局对比损失的轮廓，使用稠密层$g_{1}$。 （ii）用于预训练解码器$d_{l}$的局部对比损失的轮廓，使用1
    × 1卷积层$g_{2}$，编码器$e$的权重在前一训练阶段被冻结（图像来源于[[87](#bib.bib87)]）。
- en: 2.3 Contrastive self-supervision
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 对比自监督
- en: Contrastive learning is designed to maximize the mutual information between
    positive image pairs and, if needed, minimize the representation similarity of
    negative image pairs. Positive pairs consist of two augmented views of the same
    instance, whereas negative pairs come from different instances. This allows the
    network to learn discriminative representations of instances, which are beneficial
    for pattern recognition tasks. In contrastive learning, the effectiveness of learned
    representations heavily depends on the choice of positive and negative pairs.
    However, the conventional pair generation methods used for natural images might
    not be suitable for medical images with intricate semantic concepts, leading to
    potentially meaningless representations. To tackle this challenge, researchers
    have dedicated considerable effort to meticulously devising pair selection strategies
    within widely used contrastive learning frameworks [[88](#bib.bib88)]. These strategies
    aim to retain the pathological semantics present in medical images, resulting
    in significant performance enhancements for medical datasets compared to traditional
    methods.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习旨在最大化正图像对之间的互信息，并在需要时最小化负图像对的表示相似性。正图像对由同一实例的两个增强视图组成，而负图像对则来自不同的实例。这使得网络能够学习实例的区分表示，这对模式识别任务非常有利。在对比学习中，学习到的表示的有效性严重依赖于正负图像对的选择。然而，传统的图像对生成方法可能不适用于具有复杂语义概念的医学图像，这可能导致无意义的表示。为了解决这个问题，研究人员投入了大量精力在广泛使用的对比学习框架中精心设计图像对选择策略[[88](#bib.bib88)]。这些策略旨在保留医学图像中的病理语义，从而显著提升医学数据集的性能，相较于传统方法。
- en: 'Contig [[89](#bib.bib89)] employs a contrastive loss to align images and various
    genetic modalities within the feature space. The approach is devised to seamlessly
    incorporate multiple modalities from each individual into a single end-to-end
    model, even when the modalities available may differ among individuals. Sowrirajan
    et al. [[90](#bib.bib90)] asserted that the augmentations used in MOCO [[91](#bib.bib91)]
    are not suitable for gray-scale medical images. Specifically, blurring and random
    crop could potentially remove important lesions. To address this issue, they introduced
    MoCo-CXR, a modified version of MOCO, specifically tailored for chest X-ray images
    by adapting the augmentations to better suit this medical imaging context. Vu
    et al. [[92](#bib.bib92)] introduced a SSL technique called MedAug, inspired by
    MoCo-CXR. In their method, positive pairs are generated from diverse images of
    a single patient based on their metadata. Azizi et al. [[5](#bib.bib5)] presented
    a similar work to MedAug, which was based on the SimCLR framework [[93](#bib.bib93)].
    They introduced a method called *Multi-Instance Contrastive Learning* to create
    more informative positive pairs from various images of a similar patient. Chaitanya
    et al. [[87](#bib.bib87)] enhanced SimCLR for 3D medical image segmentation (see
    Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Generative self-supervision ‣ 2 No supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). They introduced
    a novel contrasting strategy that leveraged the structural similarity of volumetric
    medical images. Additionally, they introduced a local contrastive loss to facilitate
    the learning of more detailed and fine-grained representations. Ciga et al. [[94](#bib.bib94)]
    introduce a contrastive SSL approach for digital histopathology. They conducted
    training on 57 unlabeled histopathology datasets. Their findings reveal that enhancing
    the feature quality is achievable by combining multiple multi-organ datasets with
    diverse staining and resolution characteristics. Some techniques leverage anatomical
    priors within contrastive methods to further enhance performance across various
    tasks [[6](#bib.bib6), [95](#bib.bib95)]. Specifically, He et al. [[95](#bib.bib95)]
    introduce Geometric Visual Similarity Learning (GVSL). GVSL incorporates the concept
    of topological invariance into the metric, ensuring a dependable assessment of
    inter-image similarity. This approach aims to learn a consistent representation
    for equivalent semantic regions across different images.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Contig [[89](#bib.bib89)] 采用对比损失来对齐图像和各种基因模式在特征空间中的位置。该方法旨在将每个个体的多个模式无缝地整合到一个端到端模型中，即使这些模式在个体之间可能有所不同。Sowrirajan
    等人 [[90](#bib.bib90)] 断言，MOCO [[91](#bib.bib91)] 中使用的增强方法不适用于灰度医学图像。具体来说，模糊和随机裁剪可能会去除重要的病变。为了解决这一问题，他们引入了
    MoCo-CXR，这是一种针对胸部 X 射线图像的 MOCO 修改版，通过调整增强方法来更好地适应这一医学影像环境。Vu 等人 [[92](#bib.bib92)]
    引入了一种名为 MedAug 的 SSL 技术，该技术受到 MoCo-CXR 的启发。在他们的方法中，基于单个患者的元数据，从多样的图像中生成正样本对。Azizi
    等人 [[5](#bib.bib5)] 提出了类似于 MedAug 的工作，这一工作基于 SimCLR 框架 [[93](#bib.bib93)]。他们引入了一种名为
    *多实例对比学习* 的方法，从相似患者的各种图像中创建更具信息性的正样本对。Chaitanya 等人 [[87](#bib.bib87)] 为 3D 医学图像分割改进了
    SimCLR（见图 [4](#S2.F4 "图 4 ‣ 2.2 生成自监督 ‣ 2 无监督 ‣ 数据高效深度学习医学图像分析：调查")）。他们引入了一种新颖的对比策略，利用体积医学图像的结构相似性。此外，他们引入了一种局部对比损失，以促进更详细和细粒度表示的学习。Ciga
    等人 [[94](#bib.bib94)] 介绍了一种针对数字组织病理学的对比 SSL 方法。他们在 57 个未标记的组织病理数据集上进行了训练。他们的研究结果表明，通过结合多个具有不同染色和分辨率特征的多脏器数据集，可以提升特征质量。一些技术利用对比方法中的解剖学先验，以进一步提升在各种任务中的性能
    [[6](#bib.bib6), [95](#bib.bib95)]。具体来说，He 等人 [[95](#bib.bib95)] 引入了几何视觉相似性学习（GVSL）。GVSL
    将拓扑不变性的概念纳入度量中，确保对图像间相似性的可靠评估。该方法旨在学习不同图像中等效语义区域的一致表示。
- en: '2.4 Multi-self supervised learning: combining multiple SSL pretext tasks into
    one framework'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 多自监督学习：将多个 SSL 前置任务合并到一个框架中
- en: 'Multi-SSL integrates various types of pretext tasks, including predictive,
    generative, and contrastive tasks. By doing so, it aims to overcome the limitation
    of single pretext tasks, which might learn task-specific features. By employing
    different self-supervision signals during network training, multi-SSL aims to
    extract more robust and generalizable representations. Taleb et al. [[9](#bib.bib9)]
    proposed that medical images with a 3D nature offer the potential to learn rich
    representations compared to 2D images. To accommodate this, they employed five
    predesigned pretext tasks, namely contrastive predictive coding (CPC), exemplar
    CNN, rotation prediction, relative position prediction, and Jigsaw puzzle, to
    adapt to the characteristics of 3D medical images. Haghighi et al. [[96](#bib.bib96)]
    introduced Semantic Genesis, building upon the Model Genesis approach [[83](#bib.bib83)].
    This framework comprises three modules: self-classification, self-restoration,
    and self-discovery, aimed at learning semantics-enriched representations. In a
    further extension of Model Genesis, Zhang et al. [[97](#bib.bib97)] incorporated
    a scale-aware proxy task for predicting the input’s scale. This addition allows
    for the learning of multi-level representations. Zhou et al. [[98](#bib.bib98)]
    combined generative and contrastive SSL into a Preservational Contrastive Representation
    Learning (PCRL) framework, where preservational learning is introduced for the
    generative SSL to keep more information. Tang et al. [[99](#bib.bib99)] introduce
    a novel 3D transformer-based architecture known as Swin UNEt TRansformers (Swin
    UNETR), with a hierarchical encoder for self-supervised pre-training. In their
    proposed pre-training framework, input CT images undergo random cropping into
    sub-volumes and are augmented with random inner cutout and rotation operations.
    Subsequently, they are inputted into the Swin UNETR encoder. The authors employ
    masked volume inpainting, contrastive learning, and rotation prediction as proxy
    tasks to facilitate the learning of contextual representations from input images,
    as shown in Figure [5](#S2.F5 "Figure 5 ‣ 2.4 Multi-self supervised learning:
    combining multiple SSL pretext tasks into one framework ‣ 2 No supervision ‣ Data
    efficient deep learning for medical image analysis: A survey"). CS-CO [[100](#bib.bib100)],
    designed specifically for histopathological images, combines the strengths of
    generative and discriminative approaches. This method comprises two self-supervised
    learning phases: cross-stain prediction (CS) and contrastive learning (CO). Yan
    et al. [[101](#bib.bib101)] employ Masked Autoencoders (MAE) but demonstrate that
    directly applying MAE is suboptimal for dense downstream prediction tasks such
    as multi-organ segmentation. To address this limitation, they propose a self-supervised
    pre-training approach on large-scale unlabeled medical datasets, leveraging both
    contrastive and generative modeling techniques.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'Multi-SSL 集成了各种类型的前置任务，包括预测任务、生成任务和对比任务。这样做旨在克服单一前置任务的局限性，这些任务可能会学习到特定任务的特征。通过在网络训练过程中使用不同的自监督信号，multi-SSL
    旨在提取更为稳健和具有普遍性的表示。Taleb 等人 [[9](#bib.bib9)] 提出了与 2D 图像相比，具有 3D 特性的医学图像提供了学习丰富表示的潜力。为此，他们采用了五个预设计的前置任务，即对比预测编码（CPC）、示例
    CNN、旋转预测、相对位置预测和拼图，以适应 3D 医学图像的特点。Haghighi 等人 [[96](#bib.bib96)] 引入了 Semantic
    Genesis，基于 Model Genesis 方法 [[83](#bib.bib83)]。该框架包括三个模块：自分类、自恢复和自发现，旨在学习丰富语义的表示。在
    Model Genesis 的进一步扩展中，Zhang 等人 [[97](#bib.bib97)] 纳入了一个尺度感知的代理任务，用于预测输入的尺度。这一补充允许学习多层次的表示。Zhou
    等人 [[98](#bib.bib98)] 将生成式和对比式 SSL 结合成一个保留对比表示学习（PCRL）框架，其中引入了保留学习以使生成式 SSL 保留更多信息。Tang
    等人 [[99](#bib.bib99)] 介绍了一种新的基于 3D Transformer 的架构，称为 Swin UNEt TRansformers（Swin
    UNETR），具有用于自监督预训练的层次编码器。在他们提出的预训练框架中，输入的 CT 图像经过随机裁剪成子体积，并通过随机内部切割和旋转操作进行增强。随后，这些图像被输入到
    Swin UNETR 编码器中。作者使用了掩膜体积修复、对比学习和旋转预测作为代理任务，以促进从输入图像中学习上下文表示，如图 [5](#S2.F5 "Figure
    5 ‣ 2.4 Multi-self supervised learning: combining multiple SSL pretext tasks into
    one framework ‣ 2 No supervision ‣ Data efficient deep learning for medical image
    analysis: A survey") 所示。CS-CO [[100](#bib.bib100)] 专为组织病理图像设计，结合了生成式和判别式方法的优点。该方法包括两个自监督学习阶段：交叉染色预测（CS）和对比学习（CO）。Yan
    等人 [[101](#bib.bib101)] 使用了掩膜自编码器（MAE），但表明直接应用 MAE 对于密集下游预测任务（如多脏器分割）并不理想。为了解决这一限制，他们提出了一种在大规模未标记医学数据集上进行自监督预训练的方法，利用了对比和生成建模技术。'
- en: Yan et al. [[101](#bib.bib101)] used Masked Autoencoders (MAE) but demonstrated
    that directly applying MAE is suboptimal for dense downstream prediction tasks,
    such as multi-organ segmentation. To address this limitation, they proposed a
    self-supervised pre-training approach on large-scale unlabeled medical datasets,
    leveraging both contrastive and generative modeling techniques.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Yan 等人 [[101](#bib.bib101)] 使用了掩模自编码器（MAE），但证明直接应用 MAE 对于密集的下游预测任务（如多脏器分割）效果不佳。为了解决这个限制，他们提出了一种在大规模未标记医学数据集上进行自监督预训练的方法，利用了对比学习和生成建模技术。
- en: '![Refer to caption](img/4717c35abc7c02900da61c9ff0b7be39.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4717c35abc7c02900da61c9ff0b7be39.png)'
- en: 'Figure 5: The pre-training framework’s [[99](#bib.bib99)] outline begins with
    the random cropping of input CT images into sub-volumes, followed by the application
    of random inner cutout and rotation augmentations. These processed images are
    then utilized as input for the Swin UNETR encoder. The framework leverages masked
    volume inpainting, contrastive learning, and rotation prediction as proxy tasks
    aimed at acquiring contextual representations from the input images (image from
    [[99](#bib.bib99)]).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：预训练框架的 [[99](#bib.bib99)] 概述开始于将输入 CT 图像随机裁剪成子体积，随后应用随机内切割和旋转数据增强。这些处理后的图像随后作为
    Swin UNETR 编码器的输入。该框架利用了掩模体积修复、对比学习和旋转预测作为代理任务，旨在从输入图像中获取上下文表示（图像来自 [[99](#bib.bib99)]）。
- en: 'Table 2: Overview of recent methods in *No Supervision* category.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：*无监督*类别中近期方法的概述。
- en: '| Reference | Task | Pretext task | Dataset | Result |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 任务 | 前置任务 | 数据集 | 结果 |'
- en: '| [[75](#bib.bib75)] | Cardiac segmentation | Anatomical Position Prediction
    | Private Dataset: 3825 Subjects | DSC: 0.93 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] | 心脏分割 | 解剖位置预测 | 私有数据集：3825 个受试者 | DSC: 0.93 |'
- en: '| [[77](#bib.bib77)] | Brain tumor segmentation Brain hemorrhage classification
    | Rubik’s Cube Recovery | BraTS 2018; Private Dataset: 1,486 Images | BraTS 2018:
    mIoU: 0.773; Private: Acc: 0.838 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| [[77](#bib.bib77)] | 脑肿瘤分割 脑出血分类 | 魔方恢复 | BraTS 2018；私有数据集：1,486 张图像 | BraTS
    2018：mIoU: 0.773；私有：准确率：0.838 |'
- en: '| [[78](#bib.bib78)] | Brain tumor segmentation Brain hemorrhage classification
    | Rubik cube+ (cube ordering, cube orientation and masking identification) | BraTS-2018;
    Private Dataset: 1,486 CT volumes | BraTS 2018: Mean Dice: 81.70; Private: Acc:
    87.84 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| [[78](#bib.bib78)] | 脑肿瘤分割 脑出血分类 | 魔方+（魔方排序、魔方方向和掩模识别） | BraTS-2018；私有数据集：1,486
    个 CT 图像 | BraTS 2018：平均 Dice：81.70；私有：准确率：87.84 |'
- en: '| [[80](#bib.bib80)] | Fetal image classification Abdominal multi-organ localization
    Brain tumour segmentation | Image Context Restoration | Private Fetus Dataset:
    2,694 Images; Private Multi-organ Dataset: 150 Images; BraTS 2017 | Private Fetus
    Dataset: F1: 0.8942; Private Multi-organ Dataset: Mean Distance: 2.90; BraTS 2017:
    DSC: 0.8557 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| [[80](#bib.bib80)] | 胎儿图像分类 腹部多脏器定位 脑肿瘤分割 | 图像上下文恢复 | 私有胎儿数据集：2,694 张图像；私有多脏器数据集：150
    张图像；BraTS 2017 | 私有胎儿数据集：F1: 0.8942；私有多脏器数据集：平均距离：2.90；BraTS 2017：DSC: 0.8557
    |'
- en: '| [[10](#bib.bib10)] | Optic disc segmentation | Multi-modal Reconstruction
    | Isfahan MISP | AUC: 0.818 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [[10](#bib.bib10)] | 视盘分割 | 多模态重建 | 伊斯法罕 MISP | AUC: 0.818 |'
- en: '| [[86](#bib.bib86)] | Pancreas and Brain Tissue segmentation | Rubik cube
    ++ | NIH PCT; MRBrainS18 | NIH PCT: DSC: 0.8408; MRBrainS18: DSC: 0.7756 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| [[86](#bib.bib86)] | 胰腺和脑组织分割 | 魔方 ++ | NIH PCT；MRBrainS18 | NIH PCT：DSC:
    0.8408；MRBrainS18：DSC: 0.7756 |'
- en: '| [[5](#bib.bib5)] | Chest X-ray classification Skin lesions classification
    | Multi-Instance Contrastive Learning (SimCLR) | Priavte Dermatology Dataset;
    CheXpert | Private: Top-1 Acc: 0.7002; CheXpert: AUC: 0.772 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[5](#bib.bib5)] | 胸部 X 光分类 皮肤病变分类 | 多实例对比学习（SimCLR） | 私有皮肤科数据集；CheXpert
    | 私有：Top-1 准确率：0.7002；CheXpert：AUC: 0.772 |'
- en: '| [[102](#bib.bib102)] | Lung | Contrastive Learning | CheXpert | AUC: 0.889
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[102](#bib.bib102)] | 肺部 | 对比学习 | CheXpert | AUC: 0.889 |'
- en: '| [[6](#bib.bib6)] | 2D and 3D landmark detection; 3D Lesion matching | Global
    and Local Contrastive Learning | DeepLesion; NIH LN; Private Dataset: 94 Patients
    | Mean Radial Error: 4.3; Maximum Radial Error: 16.4 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[6](#bib.bib6)] | 2D 和 3D 标志物检测；3D 病灶匹配 | 全球和局部对比学习 | DeepLesion；NIH LN；私有数据集：94
    名患者 | 平均径向误差：4.3；最大径向误差：16.4 |'
- en: '| [[96](#bib.bib96)] | Lung | Self-Discovery + Self-Classification + Self-Restoration
    | LUNA; LiTS; CAD-PE; BraTS 2018; ChestX-ray14; LIDC-IDRI; SIIM-ACR | Classification:
    LUNA: AUC: 0.9847; Segmentation: IoU: LiTS: 0.8560; BraTS 2018: 0.6882 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | 肺 | 自我发现 + 自我分类 + 自我修复 | LUNA; LiTS; CAD-PE; BraTS 2018;
    ChestX-ray14; LIDC-IDRI; SIIM-ACR | 分类: LUNA: AUC: 0.9847; 分割: IoU: LiTS: 0.8560;
    BraTS 2018: 0.6882 |'
- en: '| [[9](#bib.bib9)] | Brain tumors segmentation pancreas tumor segmentation
    | CPC Jigsaw puzzle Exemplar CNN Rotation Prediction Relative position prediction
    | BraTS 2018; DECATHLON; DRD | BraTS 2018: DSC: 0.9080; DECATHLON: DSC $\approx$
    0.635; DRD DRD: DSC $\approx$ 0.80 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[9](#bib.bib9)] | 脑肿瘤分割胰腺肿瘤分割 | CPC 拼图示例 CNN 旋转预测相对位置预测 | BraTS 2018; DECATHLON;
    DRD | BraTS 2018: DSC: 0.9080; DECATHLON: DSC $\approx$ 0.635; DRD DRD: DSC $\approx$
    0.80 |'
- en: '| [[101](#bib.bib101)] | Multi-organ segmentation | Masked Autoencoders + contrastive
    and generative modeling | Pre-training Dataset: Abdomen-1K; Fine-tuning Dataset:
    ABD-110; Thorax-85; HaN | ABD-110: Dice score: 84.67; Thorax-85: Dice score: 90.37;
    HaN: Dice score: 77.31 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[101](#bib.bib101)] | 多器官分割 | 掩码自编码器 + 对比性和生成建模 | 预训练数据集: Abdomen-1K; 微调数据集:
    ABD-110; Thorax-85; HaN | ABD-110: Dice score: 84.67; Thorax-85: Dice score: 90.37;
    HaN: Dice score: 77.31 |'
- en: 3 Inexact supervision
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 不精确监督
- en: 'Inexact supervision pertains to situations where some form of supervision information
    is available but lacks the exactness desired for the task. In this context, we
    classify inexact supervision into two categories: Multiple Instance Learning (MIL)
    and learning with weak annotations (Figure [6](#S3.F6 "Figure 6 ‣ 3 Inexact supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). In the
    MIL framework (Subsection [3.1](#S3.SS1 "3.1 Multiple instance learning ‣ 3 Inexact
    supervision ‣ Data efficient deep learning for medical image analysis: A survey")),
    each image is treated as a *bag*, and the patches extracted from it are regarded
    as *instances*. When a bag is labeled as negative, it implies that all instances
    within it are also considered negative. Conversely, if a bag is labeled as positive,
    it indicates the presence of at least one positive instance within it. This labeling
    strategy at the bag level significantly reduces the labeling burden compared to
    labeling each individual instance separately, which proves advantageous across
    various tasks. Learning with weak annotations (Subsection [3.2](#S3.SS2 "3.2 Learning
    with weak annotations ‣ 3 Inexact supervision ‣ Data efficient deep learning for
    medical image analysis: A survey")) refers to a scenario in which the available
    training data is annotated with labels that are less detailed or less precise
    than what might be ideal for a particular task. In many medical imaging tasks,
    obtaining precise annotations at a fine-grained level (such as pixel-level annotations)
    can be highly valuable but also costly and time-consuming. Weak annotations offer
    an alternative approach where the labels provided for the training data are of
    a coarser or less specific nature, making them easier and more cost-effective
    to obtain. These weak annotations can take various forms, including image-level,
    point-level, scribble-level, or box-level. In all of these scenarios, the provided
    annotations are less detailed and precise compared to comprehensive pixel-level
    annotations. A summary of recent methods for learning with inexact supervision
    is provided in Table [3](#S3.T3 "Table 3 ‣ 3.2.2 Learning with point annotation
    ‣ 3.2 Learning with weak annotations ‣ 3 Inexact supervision ‣ Data efficient
    deep learning for medical image analysis: A survey").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不精确监督涉及到一些形式的监督信息可用，但缺乏任务所需的精确性。在这种情况下，我们将不精确监督分为两类：多实例学习（MIL）和弱标注学习（图 [6](#S3.F6
    "图 6 ‣ 3 不精确监督 ‣ 医学图像分析中的数据高效深度学习：综述")）。在MIL框架下（子节 [3.1](#S3.SS1 "3.1 多实例学习 ‣
    3 不精确监督 ‣ 医学图像分析中的数据高效深度学习：综述")），每张图像被视为一个*袋*，从中提取的图块被视为*实例*。当一个袋被标记为负面时，这意味着其中所有的实例也被认为是负面的。相反，如果一个袋被标记为正面，则表示其中至少有一个正面的实例。这种袋级别的标记策略相比于单独标记每个实例显著减少了标记负担，这在各种任务中都显示出其优势。弱标注学习（子节
    [3.2](#S3.SS2 "3.2 弱标注学习 ‣ 3 不精确监督 ‣ 医学图像分析中的数据高效深度学习：综述")）指的是在训练数据上标注的标签不如特定任务所需的理想详细或精确的场景。在许多医学成像任务中，获得精细级别（如像素级标注）的精确标注可能非常有价值，但也很昂贵和耗时。弱标注提供了一种替代方法，其中提供的训练数据标签是较粗糙或较不具体的，使得它们更容易获得且成本更低。这些弱标注可以采取多种形式，包括图像级、点级、涂鸦级或框级。在所有这些情况下，提供的标注相较于全面的像素级标注都较为简略和不精确。最近关于不精确监督学习方法的总结见表
    [3](#S3.T3 "表 3 ‣ 3.2.2 点标注学习 ‣ 3.2 弱标注学习 ‣ 3 不精确监督 ‣ 医学图像分析中的数据高效深度学习：综述")。
- en: '![Refer to caption](img/02d325496f39cbe142ed18298548a993.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02d325496f39cbe142ed18298548a993.png)'
- en: 'Figure 6: Taxonomy of *Inexact Supervision* methods.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: *不精确监督*方法的分类。'
- en: 3.1 Multiple instance learning
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 多实例学习
- en: 'Multiple-instance learning (MIL) [[103](#bib.bib103)] arises when obtaining
    detailed annotations for individual pixels or patches in an image becomes impractical,
    time-consuming, or infeasible. Instead, global labels representing the overall
    image condition are more readily available. However, these global labels do not
    directly correspond to every pixel or patch within the image. MIL extends supervised
    learning to train classifiers using weakly labeled data. In MIL, every image is
    viewed as a *bag* containing numerous patches, also referred to as *instances*.
    If an image, or *bag*, is classified as disease-positive, it implies that at least
    one patch, or *instance*, within that image is disease-positive. Conversely, if
    an image is labeled as disease-negative, it signifies that all patches, or *instances*,
    in that image are negative instances. The current approaches within deep MIL can
    be classified into two categories: instance-based methods and bag-based methods.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当为图像中的单个像素或补丁获取详细注释变得不切实际、耗时或不可行时，**多实例学习**（MIL）[[103](#bib.bib103)] 应运而生。相反，代表整体图像条件的全局标签更容易获得。然而，这些全局标签并不直接对应图像中的每个像素或补丁。MIL
    扩展了有监督学习，使用弱标注数据来训练分类器。在 MIL 中，每个图像被视为一个*包*，包含许多补丁，也称为*实例*。如果一个图像或*包*被分类为疾病阳性，则意味着该图像中的至少一个补丁或*实例*是疾病阳性。相反，如果图像标记为疾病阴性，则表示该图像中的所有补丁或*实例*都是阴性实例。深度
    MIL 的当前方法可以分为两类：基于实例的方法和基于包的方法。
- en: 3.1.1 Instance-based methods
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 基于实例的方法
- en: The main concept behind the instance-based method is to train an effective instance
    classifier to predict the possible labels for individual instances (e.g., image
    patches) within each bag. Subsequently, the MIL-pooling (the aggregation process
    is commonly referred to as MIL-pooling) method is applied to combine the predictions
    of all instances within each bag, ultimately generating the bag’s prediction.
    Given that the actual labels of individual instances are unknown, these approaches
    typically begin by assigning pseudo-labels to each instance based on their respective
    bags (i.e., all instances within a positive bag are assigned positive labels,
    and all instances within a negative bag are assigned negative labels). Subsequently,
    the instance classifier is trained using pseudo-labels in a supervised manner
    until it converges [[103](#bib.bib103)]. Various MIL pooling techniques are employed
    in this process, including Mean-pooling [[104](#bib.bib104)], Max-pooling [[104](#bib.bib104)],
    Average-pooling [[105](#bib.bib105)] log-sum-exp-pooling [[106](#bib.bib106)],
    Noisy-or-pooling [[107](#bib.bib107)], Noisy-and-pooling [[108](#bib.bib108)],
    and Dynamic pooling [[109](#bib.bib109)], among others. Couture et al. [[110](#bib.bib110)]
    propose an improved MIL aggregation approach that employs a quantile function
    as the pooling mechanism. This innovative technique allows for a comprehensive
    representation of the variations within each sample, leading to improved global
    classification accuracy. In the recent study by Qu et al. [[111](#bib.bib111)],
    they applied instance-level contrastive learning to aggregate various tumor features
    for the purpose of diagnosing pancreatic cancer.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 基于实例的方法的主要概念是训练一个有效的实例分类器，以预测每个包中单个实例（例如，图像补丁）的可能标签。随后，应用 MIL-pooling（聚合过程通常称为
    MIL-pooling）方法，将每个包中所有实例的预测结果结合起来，最终生成包的预测结果。由于单个实例的实际标签未知，这些方法通常通过基于各自包分配伪标签（即所有正包中的实例被分配正标签，所有负包中的实例被分配负标签）来开始。然后，使用伪标签以有监督的方式训练实例分类器，直到其收敛[[103](#bib.bib103)]。在此过程中使用了各种
    MIL pooling 技术，包括均值池化 [[104](#bib.bib104)]、最大池化 [[104](#bib.bib104)]、平均池化 [[105](#bib.bib105)]、对数和指数池化
    [[106](#bib.bib106)]、噪声或池化 [[107](#bib.bib107)]、噪声与池化 [[108](#bib.bib108)] 和动态池化
    [[109](#bib.bib109)] 等。Couture 等人 [[110](#bib.bib110)] 提出了改进的 MIL 聚合方法，采用分位数函数作为池化机制。这一创新技术允许对每个样本的变异进行全面表示，从而提高了全局分类准确性。在
    Qu 等人 [[111](#bib.bib111)] 的最新研究中，他们应用实例级对比学习来聚合各种肿瘤特征，以诊断胰腺癌。
- en: 3.1.2 Bag-based methods
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 基于包的方法
- en: Bag-based methods rely on shared instance-level feature extractors to capture
    the features of each instance within a bag. These features are then aggregated
    using MIL-pooling to obtain bag-level features, followed by supervised training
    of the bag classifier until convergence is achieved. In bag-based methods, MIL-pooling
    aggregates instance features rather than instance predictions, as is the case
    in instance-based methods. Bag-based methods excel in bag classification because
    they have access to true bag labels, making their training process free from noise
    and more accurate than instance-based methods. However, they are less suitable
    for localization tasks, and their instance feature aggregation lacks flexibility
    in showcasing the contributions of individual instances to bag classification.
    These methods are suitable when the target pattern is expected to be visible at
    the whole-bag level rather than being localized to specific instances within the
    bag [[21](#bib.bib21)].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基于包的方法依赖共享的实例级特征提取器来捕获包内每个实例的特征。这些特征随后通过MIL-pooling聚合以获得包级特征，然后对包分类器进行监督训练，直到收敛。在基于包的方法中，MIL-pooling
    聚合的是实例特征而非实例预测，这与基于实例的方法不同。基于包的方法在包分类上表现优秀，因为它们可以访问真实的包标签，使得训练过程免于噪声，比基于实例的方法更准确。然而，这些方法不太适用于定位任务，其实例特征聚合在展示单个实例对包分类贡献的灵活性方面有所欠缺。当目标模式预计在整个包级别上可见而非局限于包内的特定实例时，这些方法是合适的[[21](#bib.bib21)]。
- en: '![Refer to caption](img/e1e3c037a40d1d871b9c2032c4a2c2fb.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e1e3c037a40d1d871b9c2032c4a2c2fb.png)'
- en: 'Figure 7: Illustration of the framework from Zhao et al.’s work [[112](#bib.bib112)]:
    VAE-GAN functions as the instance-level feature extractor. The feature selection
    process identifies and selects discriminative instance-level features. A Graph
    Convolutional Network (GCN) is employed to synthesize the selected instance-level
    features, responsible for generating bag representations and performing the final
    classification (image adapted from [[112](#bib.bib112)]).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：赵等人的研究框架示意图[[112](#bib.bib112)]：VAE-GAN 作为实例级特征提取器。特征选择过程识别并选择出具有区分性的实例级特征。采用图卷积网络（GCN）合成所选择的实例级特征，负责生成包表示并执行最终分类（图像来源于[[112](#bib.bib112)]）。
- en: 'Bag-based methods primarily vary in three key components: the first being the
    instance-level feature extraction module, the second involving instance-level
    feature selection, and lastly, the method by which the instance features are aggregated
    to produce bag-level features.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 基于包的方法主要在三个关键组件上有所不同：首先是实例级特征提取模块，其次是实例级特征选择，最后是聚合实例特征以生成包级特征的方法。
- en: 'Concerning the instance-level feature extractor, the majority of methods utilize
    CNNs to automatically extract robust features from patches or employ pre-trained
    models [[3](#bib.bib3)]. Recently, there has been an emergence of methods that
    utilize unsupervised learning to extract features at the patch level. In this
    context, [[112](#bib.bib112)] train the feature extractor using a combination
    model that includes both a variational autoencoder and a generative adversarial
    network (VAE-GAN) as shown in Figure [7](#S3.F7 "Figure 7 ‣ 3.1.2 Bag-based methods
    ‣ 3.1 Multiple instance learning ‣ 3 Inexact supervision ‣ Data efficient deep
    learning for medical image analysis: A survey"). Various methods employ a self-supervised
    contrastive learning approach to obtain instance-level feature representations.
    For instance, [[113](#bib.bib113)] uses contrastive predictive coding (CPC) from
    [[114](#bib.bib114)], while [[115](#bib.bib115)] utilizes SimCLR from [[93](#bib.bib93)].
    Additionally, Chikontwe et al. [[116](#bib.bib116)] integrate an unsupervised
    contrastive loss with their proposed MIL method to enhance the learning of instance-level
    features.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 关于实例级特征提取器，大多数方法利用CNN自动从补丁中提取鲁棒特征或使用预训练模型[[3](#bib.bib3)]。最近出现了一些利用无监督学习在补丁级别提取特征的方法。在这一背景下，[[112](#bib.bib112)]
    使用包括变分自编码器和生成对抗网络（VAE-GAN）在内的组合模型来训练特征提取器，如图[7](#S3.F7 "图 7 ‣ 3.1.2 基于包的方法 ‣ 3.1
    多实例学习 ‣ 3 不准确的监督 ‣ 医学图像分析的数据高效深度学习：综述")所示。各种方法采用自监督对比学习方法以获取实例级特征表示。例如，[[113](#bib.bib113)]
    使用来自[[114](#bib.bib114)]的对比预测编码（CPC），而[[115](#bib.bib115)] 则利用来自[[93](#bib.bib93)]的SimCLR。此外，Chikontwe等人[[116](#bib.bib116)]
    将无监督对比损失与他们提出的MIL方法相结合，以增强实例级特征的学习。
- en: Regarding the feature selection, the high resolution of medical images poses
    a challenge when applying deep Multiple Instance Learning (MIL) methods since
    only a limited number of patches can be selected from these images for MIL. To
    address this, some approaches use techniques such as random patch selection [[117](#bib.bib117)],
    intelligent sampling using weakly supervised discriminator [[118](#bib.bib118)]
    and discriminative patch selection [[119](#bib.bib119), [112](#bib.bib112)]. Additionally,
    patch clustering methods [[120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122)]
    have been employed. Patch clustering serves the purpose of ensuring the representativeness
    of the selected patches to a certain degree, as a few patches chosen from a cluster
    can approximately represent the entire cluster. Ultimately, representative clusters
    are utilized to make the final prediction. Sharma et al. [[120](#bib.bib120)]
    employ clustering and sampling on the patch features extracted through the feature
    extractor. Subsequently, they integrate these features using an adaptive attention
    mechanism to facilitate end-to-end training. To enhance the feature space learning,
    Lu et al. [[121](#bib.bib121)] select instances with the highest and lowest attention
    scores within the current bag for clustering. To advance upon these prior techniques,
    Yan et al. [[122](#bib.bib122)] introduce a patch clustering approach based on
    unsupervised and self-supervised learning methods.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 关于特征选择，高分辨率医学图像在应用深度多实例学习（MIL）方法时面临挑战，因为只能从这些图像中选择有限数量的补丁用于MIL。为了解决这一问题，一些方法采用了随机补丁选择[[117](#bib.bib117)]、使用弱监督鉴别器进行智能采样[[118](#bib.bib118)]和鉴别性补丁选择[[119](#bib.bib119),
    [112](#bib.bib112)]等技术。此外，还采用了补丁聚类方法[[120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122)]。补丁聚类的目的是确保所选补丁在一定程度上的代表性，因为从一个簇中选择的几个补丁可以大致代表整个簇。最终，代表性的簇被用于做出最终预测。Sharma等人[[120](#bib.bib120)]对通过特征提取器提取的补丁特征进行聚类和采样。随后，他们使用自适应注意机制整合这些特征，以便进行端到端训练。为了增强特征空间学习，Lu等人[[121](#bib.bib121)]在当前袋中选择具有最高和最低注意力分数的实例进行聚类。为了在这些先前技术的基础上进一步改进，Yan等人[[122](#bib.bib122)]引入了一种基于无监督和自监督学习方法的补丁聚类方法。
- en: For the Bag level representation, pooling methods such as max pooling, average
    pooling, and log-sum-exp pooling [[106](#bib.bib106)] are typically adopted in
    this step. However, these pooling methods are not trainable, which can restrict
    their usefulness. To address this limitation, Ilse et al. [[123](#bib.bib123)]
    introduced a fully trainable approach that uses the attention mechanism to assign
    weights to instances, thus indicating the contribution of individual instances
    to bag classification. This work has spurred a wave of research into attention-based
    aggregation methods [[113](#bib.bib113), [124](#bib.bib124), [2](#bib.bib2), [125](#bib.bib125),
    [115](#bib.bib115)]. Hashimoto et al. [[2](#bib.bib2)] utilized the attention
    mechanism to combine instance features at various resolutions. Li et al. [[115](#bib.bib115)]
    introduced a dual-stream aggregator that relies on masked non-local operations
    for conducting instance-level classification as well as bag-level classification.
    In contrast to the methods mentioned earlier, their model computes attention explicitly
    using a trainable distance measurement. It’s not just important to consider the
    contribution of various instances to bag classification; the relationships among
    these instances should also be fully explored. To address this, several methods
    proposed to use Transformer to aggregate instance features [[126](#bib.bib126),
    [3](#bib.bib3)]. Shao et al. [[3](#bib.bib3)] introduced Vision Transformer (ViT)
    into MIL for gigapixel Whole Slide Images (WSIs) because ViT offers significant
    benefits in capturing long-distance information and correlations among instances
    in a sequence. Wang et al. [[126](#bib.bib126)] aimed to improve lymph node metastasis
    prediction by incorporating a pruned Transformer model into MIL. To address the
    issue of limited samples in the original dataset and prevent overfitting, they
    also developed a knowledge distillation mechanism using data from similar datasets.
    Different from the approaches mentioned above, [[112](#bib.bib112)] work builds
    the bag representation with a Graph Convolutional Network.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于袋级别的表示，通常采用最大池化、平均池化和对数和指数池化[[106](#bib.bib106)]等池化方法。然而，这些池化方法不可训练，这限制了它们的有效性。为了应对这一限制，Ilse等人[[123](#bib.bib123)]提出了一种完全可训练的方法，使用注意力机制为实例分配权重，从而指示每个实例对袋分类的贡献。这项工作催生了一波基于注意力的聚合方法的研究[[113](#bib.bib113),
    [124](#bib.bib124), [2](#bib.bib2), [125](#bib.bib125), [115](#bib.bib115)]。Hashimoto等人[[2](#bib.bib2)]利用注意力机制在不同分辨率下结合实例特征。Li等人[[115](#bib.bib115)]引入了一种双流聚合器，依赖于遮蔽的非局部操作进行实例级别和袋级别的分类。与之前提到的方法不同，他们的模型通过可训练的距离度量显式计算注意力。考虑到各种实例对袋分类的贡献固然重要，但这些实例之间的关系也应当充分探索。为了解决这一问题，提出了几种使用Transformer聚合实例特征的方法[[126](#bib.bib126),
    [3](#bib.bib3)]。Shao等人[[3](#bib.bib3)]将Vision Transformer (ViT)引入MIL以处理吉帕像素全切片图像（WSIs），因为ViT在捕捉序列中实例的长距离信息和相关性方面具有显著优势。Wang等人[[126](#bib.bib126)]通过将剪枝的Transformer模型引入MIL来提高淋巴结转移预测的准确性。为了应对原始数据集中样本有限的问题并防止过拟合，他们还开发了一种使用类似数据集的知识蒸馏机制。不同于上述方法，[[112](#bib.bib112)]的工作通过图卷积网络构建袋表示。
- en: 3.2 Learning with weak annotations
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用弱标注进行学习
- en: 'Learning with weak annotations refers to a scenario where the available training
    data is annotated with labels that are less detailed or less precise than what
    might be ideal for a particular medical imaging task. In many MIA applications,
    obtaining precise annotations at a fine-grained level, such as pixel-level annotations,
    can be challenging, or expensive. Weak annotations provide a cost-effective alternative
    with coarser labels. These weak annotations can take various forms, including:
    ([3.2.1](#S3.SS2.SSS1 "3.2.1 Learning with image-level supervision ‣ 3.2 Learning
    with weak annotations ‣ 3 Inexact supervision ‣ Data efficient deep learning for
    medical image analysis: A survey")) Image-level annotations: Only category labels
    are provided for each training image, lacking precise instance-level information.
    ([3.2.2](#S3.SS2.SSS2 "3.2.2 Learning with point annotation ‣ 3.2 Learning with
    weak annotations ‣ 3 Inexact supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")) Point-level annotations: A single specific location
    or coordinate within an image is marked to highlight a key feature. ([3.2.3](#S3.SS2.SSS3
    "3.2.3 Learning with scribble-level supervision ‣ 3.2 Learning with weak annotations
    ‣ 3 Inexact supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")) Scribble-level annotations: A subset of pixels within each training
    image is annotated. ([3.2.4](#S3.SS2.SSS4 "3.2.4 Learning with box-level supervision
    ‣ 3.2 Learning with weak annotations ‣ 3 Inexact supervision ‣ Data efficient
    deep learning for medical image analysis: A survey")) Box-level annotations: Object
    bounding boxes are annotated for each training image, offering coarse localization
    information but not pixel-level accuracy (see Figure [8](#S3.F8 "Figure 8 ‣ 3.2
    Learning with weak annotations ‣ 3 Inexact supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")). In each of these cases, the annotations
    are less detailed or less precise than full pixel-level annotations, which presents
    challenges but also reduces the labeling effort compared to exhaustive pixel-level
    annotation requirements.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用弱标注进行学习指的是在某些医疗影像任务中，现有的训练数据使用的标签不如理想中的那样详细或精确。在许多医疗影像分析应用中，获取像素级标注等细粒度的精确标注可能是具有挑战性的，或者成本较高。弱标注提供了一种具有成本效益的替代方案，使用较粗略的标签。这些弱标注可以采取多种形式，包括：
    ([3.2.1](#S3.SS2.SSS1 "3.2.1 使用图像级监督进行学习 ‣ 3.2 使用弱标注进行学习 ‣ 3 不精确监督 ‣ 医疗影像分析的数据高效深度学习：综述"))
    图像级标注：每个训练图像仅提供类别标签，缺乏精确的实例级信息。 ([3.2.2](#S3.SS2.SSS2 "3.2.2 使用点标注进行学习 ‣ 3.2 使用弱标注进行学习
    ‣ 3 不精确监督 ‣ 医疗影像分析的数据高效深度学习：综述")) 点级标注：在图像中标记一个特定位置或坐标，以突出一个关键特征。 ([3.2.3](#S3.SS2.SSS3
    "3.2.3 使用涂鸦级监督进行学习 ‣ 3.2 使用弱标注进行学习 ‣ 3 不精确监督 ‣ 医疗影像分析的数据高效深度学习：综述")) 涂鸦级标注：每个训练图像中的一部分像素被标注。
    ([3.2.4](#S3.SS2.SSS4 "3.2.4 使用框级监督进行学习 ‣ 3.2 使用弱标注进行学习 ‣ 3 不精确监督 ‣ 医疗影像分析的数据高效深度学习：综述"))
    框级标注：为每个训练图像标注对象的边界框，提供粗略的定位信息，但没有像素级的精度（参见图 [8](#S3.F8 "图 8 ‣ 3.2 使用弱标注进行学习 ‣
    3 不精确监督 ‣ 医疗影像分析的数据高效深度学习：综述")）。在这些情况下，标注的详细程度或精确度低于完全的像素级标注，这带来了挑战，但相比全面的像素级标注要求，标注工作量也有所减少。
- en: '![Refer to caption](img/9a715b4ca428ef3d567b97d831b8b85b.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9a715b4ca428ef3d567b97d831b8b85b.png)'
- en: 'Figure 8: Illustration of fully supervised mask annotation, weakly supervised
    box annotation, scribble annotation and point annotation (image from [[127](#bib.bib127)]).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：完全监督的掩膜标注、弱监督的框标注、涂鸦标注和点标注的示意图（图像来自 [[127](#bib.bib127)])。
- en: 3.2.1 Learning with image-level supervision
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 使用图像级监督进行学习
- en: In this section, we examine approaches that exclusively rely on image-level
    supervision for tasks like image detection and segmentation. It’s worth noting
    that image-level supervision is commonly employed to train models for image classification.
    The challenge here arises from the substantial gap in supervision between the
    high-level information provided by image-level labels and the detailed pixel-level
    predictions required for tasks like detection and segmentation [[128](#bib.bib128)].
    In most cases, the Class Activation Maps (CAMs) [[129](#bib.bib129)] are commonly
    used as the standard approach for producing initial regions of interest using
    classification models. Essentially, CAMs leverage prior of cross-label constraints
    to identify these initial regions within an image based on the information derived
    from a classification model. Nonetheless, the accuracy of localizing using CAMs
    is relatively limited. To tackle this challenge, researchers have devised multiple
    strategies aimed at enhancing CAMs to enable tasks such as segmentation with only
    image-level supervision. For example, Li et al. [[130](#bib.bib130)] introduce
    an approach named CAM-deep level set (CAM-DLS). In this method, they integrate
    the DLS loss into the classification loss during the training of the classification
    network. This DLS loss leverages CAMs to emphasize regions within breast tumors.
    Similarly, Chen et al. [[131](#bib.bib131)] present a causal CAM approach for
    organ segmentation. This method employs the concept of causal inference, incorporating
    a category-causality chain and an anatomy-causality chain.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了完全依赖图像级监督的任务，如图像检测和分割。值得注意的是，图像级监督通常用于训练图像分类模型。挑战在于图像级标签提供的高层次信息与检测和分割等任务所需的详细像素级预测之间存在较大差距[[128](#bib.bib128)]。在大多数情况下，Class
    Activation Maps (CAMs) [[129](#bib.bib129)]通常作为使用分类模型生成初步兴趣区域的标准方法。CAMs本质上利用跨标签约束的先验信息，根据分类模型所获得的信息来识别图像中的这些初步区域。然而，使用CAMs进行定位的准确性相对有限。为了应对这一挑战，研究人员提出了多种策略，旨在增强CAMs，使其能够在仅有图像级监督的情况下完成分割等任务。例如，Li等人[[130](#bib.bib130)]提出了一种名为CAM-deep
    level set (CAM-DLS)的方法。在这种方法中，他们将DLS损失集成到分类网络的分类损失中。这个DLS损失利用CAMs来强调乳腺肿瘤中的区域。同样，Chen等人[[131](#bib.bib131)]提出了一种用于器官分割的因果CAM方法。这种方法采用因果推理的概念，结合了类别因果链和解剖因果链。
- en: 3.2.2 Learning with point annotation
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 带点注释的学习
- en: 'Point annotation involves marking a single specific location or coordinate
    within an image to indicate a key feature or point of interest. Some works [[132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134)] concentrate on employing extreme points
    as annotations for accomplishing pixel-level segmentation. Specifically, Khan
    et al. [[132](#bib.bib132)] investigate a method designed to extract information
    from extreme points and create a confidence map. This map serves as a guide for
    neural networks to comprehend the precise object location within the boundaries
    set by the extreme points. Similarly, Roth et al. [[133](#bib.bib133)] utilize
    a network that takes two types of input: an image channel and a point channel
    representing user-defined extreme points. This point channel is subsequently integrated
    into the network to provide additional guidance during segmentation training.
    Specifically, it is used as an extra input for attention gates and is incorporated
    into the loss function, effectively enhancing the segmentation process. Nevertheless,
    these methods demand annotators to identify the object’s boundary, a task that
    remains labor-intensive in practical applications. In comparison, some methods
    [[135](#bib.bib135), [136](#bib.bib136), [127](#bib.bib127)] employ center point
    annotation to accomplish pixel-level segmentation. To achieve this, certain studies
    employ the Voronoi diagram [[137](#bib.bib137)] and clustering algorithms to create
    initial coarse pixel-level labels. Subsequently, various techniques are applied
    to enhance the segmentation outcomes, including iterative optimization [[135](#bib.bib135)]
    and co-training [[136](#bib.bib136), [138](#bib.bib138)]. Zhao et al. [[127](#bib.bib127)]
    employ a framework that combines self-training and co-training to address cell
    segmentation. They introduce a divergence loss to mitigate overfitting and a consistency
    loss to ensure agreement among multiple co-trained networks.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 点注释涉及在图像中标记一个特定的位置或坐标，以指示关键特征或兴趣点。一些工作[[132](#bib.bib132), [133](#bib.bib133),
    [134](#bib.bib134)]专注于使用极端点作为注释来完成像素级分割。具体来说，Khan 等人[[132](#bib.bib132)]研究了一种从极端点提取信息并创建置信度图的方法。该图作为神经网络的指导，以理解在极端点设定的边界内的精确物体位置。类似地，Roth
    等人[[133](#bib.bib133)]利用一个网络，该网络接收两种类型的输入：图像通道和表示用户定义的极端点的点通道。这个点通道随后被集成到网络中，以提供额外的指导，用于分割训练。具体而言，它作为额外输入用于注意力门，并被纳入损失函数中，从而有效地增强了分割过程。然而，这些方法要求注释者识别物体的边界，这在实际应用中仍然是劳动密集型的。相比之下，一些方法[[135](#bib.bib135),
    [136](#bib.bib136), [127](#bib.bib127)]使用中心点注释来完成像素级分割。为此，某些研究使用Voronoi图[[137](#bib.bib137)]和聚类算法来创建初始粗略像素级标签。随后，应用各种技术来改善分割结果，包括迭代优化[[135](#bib.bib135)]和共同训练[[136](#bib.bib136),
    [138](#bib.bib138)]。Zhao 等人[[127](#bib.bib127)]使用一个结合自我训练和共同训练的框架来解决细胞分割问题。他们引入了一种发散损失来减轻过拟合，并引入了一种一致性损失来确保多个共同训练网络之间的一致性。
- en: 'Table 3: Overview of recent methods in *Inexact supervision* category.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：*不精确监督*类别的近期方法概述。
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 任务 | 算法设计 | 数据集 | 结果 |'
- en: '| [[2](#bib.bib2)] | Cancer subtype classification | Domain Adversarial + Multi-scale
    MIL | Private Dataset: 196 Images | Acc: 0.871 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| [[2](#bib.bib2)] | 癌症亚型分类 | 域对抗 + 多尺度 MIL | 私有数据集：196 张图像 | 准确率：0.871 |'
- en: '| [[117](#bib.bib117)] | Colorectal cancer staging, | Graph Attention MIL |
    MCO | Acc: 0.811; F1: 0.798 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | 结直肠癌分期 | 图注意力 MIL | MCO | 准确率：0.811；F1：0.798 |'
- en: '| [[3](#bib.bib3)] | Whole slide image classification | Transformer-based MIL
    | CAMELYON 2016; TCGA-NSCLC; TCGA-RCC | Acc: CAMELYON: 0.8837; TCGA-NSCLC: 0.8835;
    TCGA-RCC: 0.9466 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| [[3](#bib.bib3)] | 全切片图像分类 | 基于 Transformer 的 MIL | CAMELYON 2016; TCGA-NSCLC;
    TCGA-RCC | 准确率：CAMELYON：0.8837；TCGA-NSCLC：0.8835；TCGA-RCC：0.9466 |'
- en: '| [[126](#bib.bib126)] | Lymph node metastasis prediction | Transformer-based
    MIL + Knowledge Distillation | Private Dataset: 595 Images | AUC: 0.9835; P: 0.9482;
    R: 0.9151; F1: 0.9297 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| [[126](#bib.bib126)] | 淋巴结转移预测 | 基于 Transformer 的 MIL + 知识蒸馏 | 私有数据集：595
    张图像 | AUC：0.9835；精度：0.9482；召回率：0.9151；F1：0.9297 |'
- en: '| [[139](#bib.bib139)] | Histopathology whole slide image classification |
    Double-Tier Feature Distillation MIL | CAMELYON 2016; TCGA-Lung | CAMELYON 2016:
    AUC: 0.946; TCGA-Lung: AUC: 0.961 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| [[139](#bib.bib139)] | 组织病理学全切片图像分类 | 双层特征蒸馏 MIL | CAMELYON 2016; TCGA-Lung
    | CAMELYON 2016：AUC：0.946；TCGA-Lung：AUC：0.961 |'
- en: '| [[105](#bib.bib105)] | Chest X-rays classification | Jointly Classification
    and Localization | RSNA-Lung; MIMIC-CXR; Private Dataset: 1,003 Images | AUC:
    0.93 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| [[105](#bib.bib105)] | 胸部 X 光分类 | 联合分类与定位 | RSNA-Lung; MIMIC-CXR; 私人数据集:
    1,003 张图像 | AUC: 0.93 |'
- en: '| [[110](#bib.bib110)] | Breast cancer classification | Quantile Function-based
    MIL | CBCS3 | Acc: 0.952 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] | 乳腺癌分类 | 基于分位数函数的 MIL | CBCS3 | 准确率: 0.952 |'
- en: '| [[123](#bib.bib123)] | Cancer classification | Attention-based MIL | TMA-UCSB;
    CRCHistoPhenotypes | TMA-UCSB: Acc: 0.755; CRCHistoPhenotypes: Acc: 0.898 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| [[123](#bib.bib123)] | 癌症分类 | 基于注意力的 MIL | TMA-UCSB; CRCHistoPhenotypes |
    TMA-UCSB: 准确率: 0.755; CRCHistoPhenotypes: 准确率: 0.898 |'
- en: '| [[124](#bib.bib124)] | Pancreatic ductal adenocarcinoma classification and
    segmentation | Jointly Global-level Classification and Local-level Segmentation
    | Private Dataset: 800 Images | DSC: 0.6029; Sens: 0.9975 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [[124](#bib.bib124)] | 胰腺导管腺癌分类和分割 | 联合全局级别分类和局部级别分割 | 私人数据集: 800 张图像 | DSC:
    0.6029; 灵敏度: 0.9975 |'
- en: '| [[7](#bib.bib7)] | Detection of lymph node metastases | Hybrid MIL | MSK
    breast cancer | AUC: 0.965 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| [[7](#bib.bib7)] | 淋巴结转移检测 | 混合 MIL | MSK 乳腺癌 | AUC: 0.965 |'
- en: '| [[140](#bib.bib140)] | Breast Cancer (HER2 scoring: negative, equivocal and
    positive) | Hybrid MIL | Private dataset: 1105 cases | Accuracy: 0.8970 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| [[140](#bib.bib140)] | 乳腺癌（HER2评分：阴性、可疑和阳性） | 混合 MIL | 私人数据集: 1105 个病例 |
    准确率: 0.8970 |'
- en: '| [[130](#bib.bib130)] | Breast tumor segmentation | CAM + Level-Set | Private
    dataset: 3062 BUS images | DSC: fat 0.830 ± 0.118; mammary gland 0.843 ± 0.100;
    muscle 0.807 ± 0.154; thorax layers 0.910 ± 0.114 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| [[130](#bib.bib130)] | 乳腺肿瘤分割 | CAM + Level-Set | 私人数据集: 3062 BUS 图像 | DSC:
    脂肪 0.830 ± 0.118; 乳腺 0.843 ± 0.100; 肌肉 0.807 ± 0.154; 胸部层 0.910 ± 0.114 |'
- en: '| [[131](#bib.bib131)] | Segmentation | Causal Inference; CAM | ACDC; ProMRI;
    CHAOS | ProMRI DSC: 0.864±0.004; ASD: 3.86±1.20; MSD: 3.85±1.33 Abdominal Organ
    ACDC DSC: 0.875±0.008; ASD: 1.62±0.41; MSD: 1.17±0.24 CHAOS DSC: 0.781 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| [[131](#bib.bib131)] | 分割 | 因果推断; CAM | ACDC; ProMRI; CHAOS | ProMRI DSC:
    0.864±0.004; ASD: 3.86±1.20; MSD: 3.85±1.33 腹部器官 ACDC DSC: 0.875±0.008; ASD: 1.62±0.41;
    MSD: 1.17±0.24 CHAOS DSC: 0.781 |'
- en: '| [[132](#bib.bib132)] | Multi-organ segmentation | Confidence Map Supervision
    | SegTHOR | DSC Aorta: 0.9441 ± 0.0187; Esophagus 0.8983 ± 0.0416 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | 多脏器分割 | 置信度图监督 | SegTHOR | DSC 主动脉: 0.9441 ± 0.0187;
    食道 0.8983 ± 0.0416 |'
- en: '| [[133](#bib.bib133)] | Multi-organ segmentation | Random Walker + Iterative
    Training | BTCV; MSD; CT-ORG | MO-Liver 0.956 ± 0.010; MO-Pancreas 0.747 ± 0.082;
    DSC: MSD-spleen 0.958 ± 0.007; MO-Spleen 0.954 ± 0.027 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| [[133](#bib.bib133)] | 多脏器分割 | 随机游走 + 迭代训练 | BTCV; MSD; CT-ORG | MO-肝脏 0.956
    ± 0.010; MO-胰腺 0.747 ± 0.082; DSC: MSD-脾脏 0.958 ± 0.007; MO-脾脏 0.954 ± 0.027 |'
- en: '| [[134](#bib.bib134)] | Brain tumor segmentation | CNN + CRF | Vestibular-Schwannoma-SEG
    | DSC: 0.819±0.080; HD95: 3.7±7.4; P: 0.929±0.059 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bib134)] | 脑肿瘤分割 | CNN + CRF | 前庭神经瘤分割 | DSC: 0.819±0.080; HD95:
    3.7±7.4; P: 0.929±0.059 |'
- en: '| [[136](#bib.bib136)] | Multi-organ segmentation | Co-/Self-Training | MoNuSeg;
    CPM | MoNuSeg DSC: 0.7441; AJI: 0.5620; CPM DSC: 0.7337; AJI: 0.5132 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| [[136](#bib.bib136)] | 多脏器分割 | 协同/自我训练 | MoNuSeg; CPM | MoNuSeg DSC: 0.7441;
    AJI: 0.5620; CPM DSC: 0.7337; AJI: 0.5132 |'
- en: '| [[127](#bib.bib127)] | Cell segmentation | Self-/Co-/Hybrid-Training | PHC;
    Phase100 | DSC PHC: 0.871; Phase 100: 0.811 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| [[127](#bib.bib127)] | 细胞分割 | 自我/协同/混合训练 | PHC; Phase100 | DSC PHC: 0.871;
    Phase 100: 0.811 |'
- en: 3.2.3 Learning with scribble-level supervision
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 通过草图级别监督进行学习
- en: In this section, we examine techniques related to scribble-based supervision,
    where annotations are given for a limited number of pixels, often in the form
    of manually drawn scribbles. These scribbles essentially act as seed regions.
    The key challenge is to extend semantic information from these sparsely annotated
    scribbles to all other pixels that lack labels. Some approaches address this challenge
    by aiming to expand the scribbles or reconstruct the complete mask for model training
    [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143)]. Nevertheless, the
    iterative training necessary for the pixel-relabeling process is time-consuming
    and susceptible to the introduction of noisy labels. To eliminate the necessity
    for relabeling, several approaches have utilized conditional random fields for
    refining segmentation results, either in post-processing [[144](#bib.bib144)]
    or as a trainable layer [[145](#bib.bib145)]. Specifically, Can et al. [[144](#bib.bib144)]
    use region growing to create seed areas. They apply a random walk-based segmentation
    method that generates per-pixel probability maps for each label, assigning values
    only when the probability exceeds a specific threshold. However, these methods
    failed to provide more effective guidance for model training. Conversely, alternative
    techniques [[146](#bib.bib146), [147](#bib.bib147)] introduced new modules to
    assess the quality of segmentation masks, thereby encouraging the generation of
    realistic predictions. For instance, Gabriele et al. [[147](#bib.bib147)] proposed
    an adversarial training and an attention gating mechanism to produce segmentation
    masks, leading to enhanced object localization across multiple resolutions, while
    Zhang et al. [[148](#bib.bib148)] leveraged the PatchGAN discriminator to incorporate
    shape priors. However, these methods required additional data source of complete
    masks. On the other hand, Zhang et al. [[149](#bib.bib149)] utilize mix augmentation
    and cycle consistency within the Scribble-Pixel approach. This demonstrates enhancements
    in both weakly and fully supervised segmentation methodologies. Several studies
    utilize consistency learning for scribble-based supervision [[150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)]. Scribble2Label [[152](#bib.bib152)] combines
    guidance signals from scribble annotations and pseudo labels using exponential
    moving averages for cell segmentation. Based on the teacher-student framework,
    Gao et al. [[150](#bib.bib150)] propose SOUSA, where the student model receives
    weak supervision through scribbles and a Geodesic distance map created from those
    scribbles. Simultaneously, a substantial volume of unlabeled data containing different
    forms of perturbations is provided to both the student and teacher models. The
    alignment of their output predictions is enforced using a combination of Mean
    Square Error (MSE) loss and a Multi-angle Projection Reconstruction (MPR) loss.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨与涂鸦基础监督相关的技术，其中对有限数量的像素进行标注，通常以手动绘制的涂鸦形式出现。这些涂鸦实际上充当了种子区域。主要挑战在于将这些稀疏标注的涂鸦中的语义信息扩展到所有缺乏标签的其他像素。一些方法通过扩展涂鸦或重建完整的掩膜来应对这一挑战，以用于模型训练[[141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143)]。然而，像素重新标注过程所需的迭代训练耗时且容易引入噪声标签。为了消除重新标注的必要性，一些方法利用条件随机场来优化分割结果，无论是在后处理[[144](#bib.bib144)]还是作为可训练的层[[145](#bib.bib145)]。具体来说，Can等人[[144](#bib.bib144)]使用区域生长来创建种子区域。他们应用基于随机游走的分割方法，为每个标签生成逐像素概率图，仅在概率超过特定阈值时分配值。然而，这些方法未能为模型训练提供更有效的指导。相反，其他技术[[146](#bib.bib146),
    [147](#bib.bib147)]引入了新的模块来评估分割掩膜的质量，从而鼓励生成更真实的预测。例如，Gabriele等人[[147](#bib.bib147)]提出了对抗训练和注意力门控机制来生成分割掩膜，从而提高了在多分辨率下的目标定位，而张等人[[148](#bib.bib148)]利用PatchGAN判别器来结合形状先验。然而，这些方法需要额外的完整掩膜数据源。另一方面，张等人[[149](#bib.bib149)]在Scribble-Pixel方法中利用混合增强和循环一致性。这表明在弱监督和完全监督分割方法中都有所改进。一些研究利用一致性学习进行涂鸦基础监督[[150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)]。Scribble2Label[[152](#bib.bib152)]结合了来自涂鸦标注和伪标签的指导信号，使用指数移动平均进行细胞分割。基于教师-学生框架，Gao等人[[150](#bib.bib150)]提出了SOUSA，其中学生模型通过涂鸦及从这些涂鸦中创建的测地距离图获得弱监督。同时，提供了大量包含不同形式扰动的未标记数据给学生和教师模型。通过结合均方误差（MSE）损失和多角度投影重建（MPR）损失来强制对齐其输出预测。
- en: 3.2.4 Learning with box-level supervision
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 使用框级监督进行学习
- en: In this section, we evaluate approaches for semantic segmentation guided by
    box-level supervision. Utilizing box-level supervision proves to be a more robust
    substitute for image-level guidance, as it inherently reduces the exploration
    area for object detection. For object segmentation, Rajchl et al. [[153](#bib.bib153)]
    recover pixel-wise annotations given a database of images with corresponding bounding
    boxes. To achieve this goal, they devise an iterative energy minimization problem
    within a densely connected conditional random field framework to adjust and refine
    the parameters of a CNN model throughout the iterative process. Wang et al. [[154](#bib.bib154)]
    utilize MIL and a smooth maximum approximation method based on the concept of
    bounding box tightness. In this context, bounding box tightness implies that an
    object instance should have contact with all four sides of its bounding box. Consequently,
    if there is a vertical or horizontal crossing line within the box, it results
    in a positive bag classification because it covers at least one foreground pixel.
    In the work presented by [[155](#bib.bib155)], they introduce a fusion filter
    sampling (FFS) module designed to create pixel-level pseudo labels from box annotations
    while minimizing noise.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了由框级监督指导的语义分割方法。利用框级监督被证明是一种比图像级指导更为稳健的替代方法，因为它在本质上减少了目标检测的探索区域。对于目标分割，Rajchl
    等人 [[153](#bib.bib153)] 在具有相应边界框的图像数据库中恢复像素级标注。为实现这一目标，他们设计了一个迭代的能量最小化问题，在密集连接的条件随机场框架内调整和优化
    CNN 模型的参数。在 Wang 等人 [[154](#bib.bib154)] 的方法中，他们利用 MIL 和基于边界框紧密度的平滑最大近似方法。在这种情况下，边界框紧密度意味着一个对象实例应该与其边界框的四个边接触。因此，如果框内有垂直或水平交叉线，则会导致正袋分类，因为它覆盖了至少一个前景像素。在
    [[155](#bib.bib155)] 提出的工作中，他们引入了一个融合滤波采样 (FFS) 模块，旨在从框标注中创建像素级伪标签，同时最小化噪声。
- en: 4 Incomplete supervision
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 不完全监督
- en: 'Incomplete supervision refers to a scenario where we have access to a limited
    quantity of labeled data, which is inadequate for training an effective learner,
    while there exists a large pool of unlabeled data. We categorize incomplete supervision
    into three broad subcategories: Semi-supervised Learning, Active Learning, and
    Domain-adaptive Learning (Figure [9](#S4.F9 "Figure 9 ‣ 4 Incomplete supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). Semi-supervised
    learning aims to enhance learning performance by leveraging both labeled and unlabeled
    data automatically. In Domain-adaptive Learning, a domain shift occurs between
    labeled and unlabeled data. Conversely, Active learning operates on the assumption
    that there is an *oracle*, like a human expert, who can be consulted to obtain
    ground-truth labels for specific unlabeled instances. A summary of recent methods
    for learning with incomplete supervision is provided in Table [4](#S4.T4 "Table
    4 ‣ 4.3.6 Hybrid methods ‣ 4.3 Domain-adaptive learning ‣ 4 Incomplete supervision
    ‣ Data efficient deep learning for medical image analysis: A survey").'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 不完全监督指的是我们只能访问到有限的标注数据，这些数据不足以训练一个有效的学习模型，而同时存在大量未标注的数据。我们将不完全监督分为三个大类：半监督学习、主动学习和领域自适应学习（图
    [9](#S4.F9 "图 9 ‣ 4 不完全监督 ‣ 用于医学图像分析的数据高效深度学习：综述")）。半监督学习旨在通过自动利用标注和未标注数据来提高学习性能。在领域自适应学习中，标注数据和未标注数据之间发生领域迁移。相反，主动学习的假设是存在一个*预言家*，如人类专家，可以咨询以获得特定未标注实例的真实标签。最近关于不完全监督学习的方法总结见表
    [4](#S4.T4 "表 4 ‣ 4.3.6 混合方法 ‣ 4.3 领域自适应学习 ‣ 4 不完全监督 ‣ 用于医学图像分析的数据高效深度学习：综述")。
- en: '![Refer to caption](img/7596483b93ff04cf9c229c59220c2e44.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7596483b93ff04cf9c229c59220c2e44.png)'
- en: 'Figure 9: Taxonomy of *Incomplete Supervision* methods.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: *不完全监督* 方法的分类。'
- en: 4.1 Semi-supervised learning
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 半监督学习
- en: In this section, we will examine techniques used in semi-supervised learning
    (Semi-SL). In this approach, only a small portion of the training images have
    annotations, while the majority of training images remain unannotated. The goal
    of semi-supervised learning is to incorporate the vast number of unlabeled training
    images into the training process in order to enhance model performance [[156](#bib.bib156),
    [157](#bib.bib157)]. Semi-supervised Learning can be categorized into Consistency
    regularization, Generative, Pseudo-labeling, and Hybrid methods.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨在半监督学习（Semi-SL）中使用的技术。在这种方法中，只有一小部分训练图像有注释，而大多数训练图像没有注释。半监督学习的目标是将大量未标记的训练图像纳入训练过程，以提升模型性能
    [[156](#bib.bib156), [157](#bib.bib157)]。半监督学习可以分为一致性正则化、生成、伪标签和混合方法。
- en: 4.1.1 Consistency regularization methods
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 一致性正则化方法
- en: 'Consistency regularization methods rely on the concept of smoothness or manifold
    assumption, suggesting that perturbing data points should not alter the model’s
    predictions. Importantly, this approach does not rely on label information, making
    it an effective constraint for learning from unlabeled data. Within this framework,
    various perturbations are available and can be classified into two categories:
    input perturbations and feature map perturbations. These perturbations must be
    relevant and meaningful for the specific task at hand. Commonly employed input
    perturbations encompass random rotation, Gaussian blurring, Gaussian noise, contrast
    variations, and scaling. Notably, Bortsova et al. [[158](#bib.bib158)] and Li
    et al. [[159](#bib.bib159)] employ consistency learning by applying different
    transformations to input images. Another widely adopted form of consistency is
    mix-up consistency [[160](#bib.bib160), [161](#bib.bib161)], where the segmentation
    of interpolation of two inputs is encouraged to remain consistent with the interpolation
    of segmentation results for those inputs. Moreover, recent investigations by [[162](#bib.bib162)]
    and [[163](#bib.bib163)] delve into perturbations at the feature map level. Zheng
    et al. [[162](#bib.bib162)] propose a method that introduces random noise into
    the parameter calculations of the teacher model. Li et al. [[163](#bib.bib163)]
    introduce seven distinct feature perturbations, each associated with an additional
    decoder, all conditioned on maintaining consistency with the primary decoder.
    Furthermore, there are studies that simultaneously apply perturbations at both
    the input and feature map levels [[164](#bib.bib164), [165](#bib.bib165)].'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性正则化方法依赖于光滑性或流形假设，认为扰动数据点不应改变模型的预测。重要的是，这种方法不依赖于标签信息，使其成为从未标记数据中学习的有效约束。在这个框架下，有各种扰动可供选择，可以分为两类：输入扰动和特征图扰动。这些扰动必须与特定任务相关且有意义。常见的输入扰动包括随机旋转、高斯模糊、高斯噪声、对比度变化和缩放。值得注意的是，Bortsova等人
    [[158](#bib.bib158)] 和 Li等人 [[159](#bib.bib159)] 通过对输入图像应用不同的变换来采用一致性学习。另一种广泛采用的一致性形式是混合一致性
    [[160](#bib.bib160), [161](#bib.bib161)]，其中鼓励两个输入的插值分割与这些输入的插值分割结果保持一致。此外，最近的研究
    [[162](#bib.bib162)] 和 [[163](#bib.bib163)] 深入探讨了特征图级别的扰动。Zheng等人 [[162](#bib.bib162)]
    提出了一种方法，通过在教师模型的参数计算中引入随机噪声。Li等人 [[163](#bib.bib163)] 引入了七种不同的特征扰动，每种特征扰动都与一个额外的解码器相关，所有解码器都基于与主解码器保持一致。此外，还有研究同时在输入和特征图级别应用扰动
    [[164](#bib.bib164), [165](#bib.bib165)]。
- en: 'In contrast to incorporating perturbations, alternative consistency learning
    techniques are also available. For instance, the $\pi$-model [[166](#bib.bib166)]
    is a straightforward yet powerful approach that utilizes a shared encoder to generate
    various views of the input sample through augmentation. It enforces the classifier
    to provide consistent predictions for different augmentations of the same input.
    Simultaneously, the training process incorporates label information to enhance
    the classifier’s overall performance. Li et al. [[167](#bib.bib167)] developed
    a semi-supervised algorithm for skin lesion segmentation based on the $\pi$-model
    approach. Temporal ensembling [[168](#bib.bib168)] was created with the aim of
    enhancing the prediction stability of the $\pi$-model. This is achieved by incorporating
    an exponentially moving average module to update predictions. Several researchers
    have adopted this module to tackle MIA related challenges [[169](#bib.bib169),
    [170](#bib.bib170)]. To achieve precise breast mass segmentation, Cao et al. [[169](#bib.bib169)]
    incorporate uncertainty into the temporal ensembling model. They utilize uncertainty
    maps as guidance for the neural network to ensure the reliability of the generated
    predictions. Likewise, Luo et al. [[170](#bib.bib170)] suggest an uncertainty-aware
    temporal ensembling method for chest X-ray disease screening. In the training
    process of temporal ensembling, the activation of each training sample is updated
    only once in one epoch. Mean teacher (MT) [[171](#bib.bib171)] overcomes this
    limitation by applying exponentially moving average on model parameters instead
    of network activations. Several methods enhance the MT framework for its application
    in MIA contexts [[172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175)].
    To enhance the performance of the MT, Yu et al. [[172](#bib.bib172)] introduced
    the Uncertainty-Aware Mean Teacher (UA-MT) framework (see Figure [10](#S4.F10
    "Figure 10 ‣ 4.1.1 Consistency regularization methods ‣ 4.1 Semi-supervised learning
    ‣ 4 Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey")) for 3D left atrium segmentation. In this approach, the teacher model,
    in addition to producing target outputs, also assesses the uncertainty associated
    with each target prediction using Monte Carlo sampling. This allows the removal
    of unreliable predictions, retaining only those with low uncertainty for consistency
    loss calculations. This process offers more reliable guidance to the student model,
    promoting the teacher model to produce higher-quality target predictions. Wang
    et al. [[174](#bib.bib174)] incorporated multi-task learning into the mean teacher
    framework including segmentation, reconstruction, and SDF prediction tasks to
    enhance data, model, and task consistency. Additionally, they introduced an uncertainty-weighted
    integration (UWI) approach to assess uncertainty across all tasks and created
    a triple-uncertainty method to guide the student model to learn reliable information
    from the teacher.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '与引入扰动的方法相对，其他一致性学习技术也是可用的。例如，$\pi$-模型 [[166](#bib.bib166)] 是一种简单但强大的方法，它利用共享编码器通过数据增强生成输入样本的各种视图。它要求分类器对相同输入的不同增强进行一致的预测。同时，训练过程中结合了标签信息，以提高分类器的整体性能。Li
    等人 [[167](#bib.bib167)] 基于$\pi$-模型方法开发了一种用于皮肤病变分割的半监督算法。为了提高$\pi$-模型的预测稳定性，创建了时间集成
    [[168](#bib.bib168)]。这是通过引入指数移动平均模块来更新预测实现的。一些研究人员采用了这一模块来解决与MIA相关的挑战 [[169](#bib.bib169),
    [170](#bib.bib170)]。为了实现精确的乳腺肿块分割，Cao 等人 [[169](#bib.bib169)] 在时间集成模型中融入了不确定性。他们利用不确定性图作为神经网络的指导，以确保生成预测的可靠性。同样，Luo
    等人 [[170](#bib.bib170)] 提出了一个针对胸部X射线疾病筛查的不确定性感知时间集成方法。在时间集成的训练过程中，每个训练样本的激活仅在一个周期内更新一次。均值教师（MT）
    [[171](#bib.bib171)] 通过对模型参数应用指数移动平均来克服这一限制，而不是网络激活。多个方法增强了MT框架在MIA背景下的应用 [[172](#bib.bib172),
    [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175)]。为了提高MT的性能，Yu 等人 [[172](#bib.bib172)]
    引入了不确定性感知均值教师（UA-MT）框架（参见图 [10](#S4.F10 "Figure 10 ‣ 4.1.1 Consistency regularization
    methods ‣ 4.1 Semi-supervised learning ‣ 4 Incomplete supervision ‣ Data efficient
    deep learning for medical image analysis: A survey")）用于3D左心房分割。在这种方法中，教师模型除了产生目标输出外，还使用蒙特卡罗采样评估每个目标预测的不确定性。这使得可以去除不可靠的预测，只保留那些不确定性低的预测用于一致性损失计算。这个过程为学生模型提供了更可靠的指导，促使教师模型产生更高质量的目标预测。Wang
    等人 [[174](#bib.bib174)] 将多任务学习融入均值教师框架，包括分割、重建和SDF预测任务，以增强数据、模型和任务的一致性。此外，他们引入了一种不确定性加权集成（UWI）方法来评估所有任务中的不确定性，并创建了一种三重不确定性方法，以引导学生模型从教师那里学习可靠的信息。'
- en: Recently, Xu et al. [[176](#bib.bib176)] present a dual uncertainty-guided mixing
    consistency network for precise 3D semi-supervised segmentation, emphasizing the
    consideration of context information at the volume level. To segment surgical
    images, Lou et al. [[177](#bib.bib177)] propose a Min-Max Similarity (MMS) method.
    This approach adopts a dual-view training strategy, utilizing classifiers and
    projectors to construct pairs of all-negative features and positive/negative feature
    pairs. This formulation transforms the learning process into solving an MMS problem.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Xu等人[[176](#bib.bib176)]提出了一种双重不确定性引导的混合一致性网络，用于精确的3D半监督分割，强调在体积级别上考虑上下文信息。为了分割手术图像，Lou等人[[177](#bib.bib177)]提出了一种最小-最大相似性（MMS）方法。这种方法采用双视图训练策略，利用分类器和投影仪构建全负特征对和正/负特征对。这种表述将学习过程转化为解决MMS问题。
- en: '![Refer to caption](img/26fed374465422e319bf966f5be17d4c.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/26fed374465422e319bf966f5be17d4c.png)'
- en: 'Figure 10: Illustration of the Uncertainty-Aware Mean Teacher (UA-MT) framework.
    The student model is trained by minimizing the supervised loss $L_{s}$ on labeled
    data and the consistency loss $L_{c}$ on both unlabeled and labeled data. The
    teacher model’s estimated uncertainty is used to instruct the student in learning
    from the more dependable teacher-provided targets (image courtesy of Yu et al.
    [[172](#bib.bib172)]).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：不确定性感知均值教师（UA-MT）框架的示意图。学生模型通过最小化标记数据上的监督损失$L_{s}$和未标记及标记数据上的一致性损失$L_{c}$进行训练。教师模型估计的不确定性用于指导学生从更可靠的教师提供的目标中学习（图像由Yu等人提供[[172](#bib.bib172)]）。
- en: 4.1.2 Generative methods
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 生成方法
- en: 'The generative adversarial network (GAN) has shown potential performance on
    semi-supervised learning [[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180)].
    GANs consist of two main parts: a generator and a discriminator. The generator’s
    goal is to deceive the discriminator by producing fake data that appears real,
    while the discriminator aims to distinguish between real and synthetic data (see
    Figure [11](#S4.F11 "Figure 11 ‣ 4.1.2 Generative methods ‣ 4.1 Semi-supervised
    learning ‣ 4 Incomplete supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")(B)). These two networks engage in a zero-sum game,
    where any gain made by one network comes at the expense of the other. There are
    different ways to use GANs in Semi-SL settings. One such approach involves employing
    adversarial techniques to encourage the outputs of unlabeled images to closely
    resemble those of the labeled images [[181](#bib.bib181), [182](#bib.bib182)].
    Peiris et al. [[182](#bib.bib182)] incorporate a critic network into their segmentation
    architecture. This network engages in a min-max game by distinguishing between
    the predicted masks and the actual ground truth masks. The outcomes of their experiments
    indicate that this approach can enhance the definition of boundaries in the prediction
    masks. Additionally, the discriminator can be employed to generate pixel-wise
    confidence maps, facilitating the selection of reliable pixel predictions for
    consistency learning. The study by Wu et al. [[179](#bib.bib179)] introduces a
    pair of discriminators to anticipate confidence maps and differentiate between
    segmentation outcomes originating from labeled or unlabeled data. Constrained
    Adversarial Training (CAT) [[180](#bib.bib180)] focuses on generating anatomically
    accurate segmentations. This method incorporates unlabeled samples into an adversarial
    training framework, which serves to regularize the network and facilitate constraint
    learning.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '生成对抗网络（GAN）在半监督学习中显示出潜在的性能 [[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180)]。GAN
    由两个主要部分组成：生成器和判别器。生成器的目标是通过生成看起来真实的假数据来欺骗判别器，而判别器旨在区分真实数据和合成数据（见图 [11](#S4.F11
    "Figure 11 ‣ 4.1.2 Generative methods ‣ 4.1 Semi-supervised learning ‣ 4 Incomplete
    supervision ‣ Data efficient deep learning for medical image analysis: A survey")(B)）。这两个网络进行零和博弈，其中一个网络的任何收益都以另一个网络的损失为代价。在半监督学习环境中，有不同的方法来使用
    GAN。其中一种方法涉及使用对抗技术来鼓励未标记图像的输出与标记图像的输出尽可能接近 [[181](#bib.bib181), [182](#bib.bib182)]。Peiris
    等人 [[182](#bib.bib182)] 在他们的分割架构中引入了一个批评网络。这个网络通过区分预测掩模和实际真实掩模进行最小-最大博弈。他们实验的结果表明，这种方法可以增强预测掩模中边界的定义。此外，判别器还可以用于生成像素级置信度图，促进选择可靠的像素预测以进行一致性学习。Wu
    等人 [[179](#bib.bib179)] 引入了一对判别器，以预测置信度图并区分来自标记或未标记数据的分割结果。受限对抗训练（CAT） [[180](#bib.bib180)]
    侧重于生成解剖学上准确的分割。这种方法将未标记样本融入对抗训练框架中，以对网络进行正则化并促进约束学习。'
- en: 'Hou et al. [[183](#bib.bib183)] use a GAN-based framework with three enhancements:
    First, a U-Net style network is employed as the discriminator. Second, a *polluted
    discriminator* is introduced, incorporating auxiliary *leaking links* from the
    generator to encourage the generation of moderate, though unrealistic, samples,
    thereby enhancing semi-supervised learning. Third, the discriminator undergoes
    regularization via the mean-teacher mechanism, enhancing segmentation generalization
    through input and weight perturbations. Certain approaches employ GANs as a method
    for data augmentation within the context of Semi-SL. For instance, Chaitanya et
    al. [[184](#bib.bib184)] integrate unlabeled data directly into GAN’s adversarial
    training process to enhance the generator’s performance for improving medical
    data augmentation. They assert that incorporating unlabeled samples enables greater
    diversity in terms of shape and intensity, thereby enhancing the model’s robustness
    and guiding the optimization process.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Hou 等人 [[183](#bib.bib183)] 使用了一种基于 GAN 的框架，并进行了三项改进：首先，采用了 U-Net 风格的网络作为判别器。其次，引入了一个*污染判别器*，结合了来自生成器的辅助*泄漏链接*，以鼓励生成适度但不现实的样本，从而增强半监督学习。第三，判别器通过均值教师机制进行正则化，通过输入和权重扰动提高分割泛化。某些方法将
    GAN 作为数据增强的一种方法，在半监督学习的背景下。例如，Chaitanya 等人 [[184](#bib.bib184)] 将未标记数据直接融入 GAN
    的对抗训练过程中，以提高生成器在医疗数据增强方面的表现。他们声称，纳入未标记样本可以在形状和强度方面实现更大的多样性，从而增强模型的鲁棒性并指导优化过程。
- en: 'A Variational Autoencoder (VAE) [[185](#bib.bib185)] consists of two main components:
    an encoder that transforms input data into a latent representation and a decoder
    that reconstructs the latent representation into the original data space. In order
    to regularize the encoder of the VAE, a prior over the latent distribution is
    commonly introduced (see Figure [11](#S4.F11 "Figure 11 ‣ 4.1.2 Generative methods
    ‣ 4.1 Semi-supervised learning ‣ 4 Incomplete supervision ‣ Data efficient deep
    learning for medical image analysis: A survey")(A)). As one of the initial attempts
    to apply VAE to semi-supervised segmentation tasks, Sedai et al. [[178](#bib.bib178)]
    employed a dual-VAE approach for segmenting the optic cup in retinal fundus images.
    This method involved two VAEs, where one VAE learned the data distribution from
    unlabeled data and transferred its acquired knowledge to the other VAE responsible
    for segmentation using labeled data. Wang et al. [[186](#bib.bib186)] extended
    the VAE architecture to 3D medical image segmentation by introducing a mean vector
    and covariance matrix to account for correlations across different slices within
    an input volume.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）[[185](#bib.bib185)]由两个主要组件组成：一个编码器将输入数据转换为潜在表示，和一个解码器将潜在表示重构到原始数据空间。为了对VAE的编码器进行正则化，通常会引入对潜在分布的先验（见图[11](#S4.F11
    "图 11 ‣ 4.1.2 生成方法 ‣ 4.1 半监督学习 ‣ 4 不完全监督 ‣ 数据高效的医学图像分析：综述")(A)）。作为将VAE应用于半监督分割任务的初步尝试之一，Sedai等[[178](#bib.bib178)]采用了双VAE方法来分割视网膜眼底图像中的视杯。这种方法涉及两个VAE，其中一个VAE从未标记数据中学习数据分布，并将其获得的知识转移给另一个负责使用标记数据进行分割的VAE。Wang等[[186](#bib.bib186)]通过引入均值向量和协方差矩阵，将VAE架构扩展到3D医学图像分割，以考虑输入体积内不同切片之间的相关性。
- en: '![Refer to caption](img/b2c6d142286bf8133c4d977a61207f2c.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2c6d142286bf8133c4d977a61207f2c.png)'
- en: 'Figure 11: Illustration of VAE and GAN architectures: (A) In the VAE architecture,
    there is an encoder-decoder structure. Here, $Z_{\mu}$ represents the mean vector,
    $Z_{\sigma}$ denotes the standard deviation vector, and $Z$ is the sampled latent
    vector. (B) In the GAN architecture, there are both a generator and a discriminator
    (image courtesy of [[187](#bib.bib187)]).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：VAE和GAN架构的示意图：（A）在VAE架构中，存在一个编码器-解码器结构。这里，$Z_{\mu}$表示均值向量，$Z_{\sigma}$表示标准差向量，而$Z$是采样的潜在向量。（B）在GAN架构中，既有生成器也有判别器（图片来源于[[187](#bib.bib187)]）。
- en: 4.1.3 Pseudo-labeling methods
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 伪标签方法
- en: 'In pseudo-labeling, a model is trained on the available labeled data. It then
    predicts labels for unlabeled samples with high confidence, effectively creating
    pseudo-labels. Finally, the model is retrained using both the labeled data and
    these newly generated pseudo-labeled samples, improving its performance through
    the utilization of additional unlabeled data. Pseudo-labeling methods can be mainly
    categorized into two sub-categories: Self-training methods and Co-training learning
    methods.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪标签方法中，模型在可用的标记数据上进行训练。然后，它为高置信度的未标记样本预测标签，有效地创建伪标签。最后，模型使用标记数据和这些新生成的伪标签样本进行再训练，通过利用额外的未标记数据来提高其性能。伪标签方法主要可以分为两大类：自训练方法和协同训练学习方法。
- en: '![Refer to caption](img/acd2bb5dbf73c41d177ef62f6da9235f.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/acd2bb5dbf73c41d177ef62f6da9235f.png)'
- en: 'Figure 12: Illustration of the Inf-net framework: CT images are initially processed
    through two convolutional layers for the extraction of high-resolution (i.e.,
    low-level) features. An edge attention module is incorporated to enhance the representation
    of boundaries within the region of interest. Subsequently, the obtained low-level
    features, denoted as $f_{2}$, undergo three convolutional layers to extract high-level
    features. These high-level features serve two primary purposes. Firstly, they
    are used to feed a parallel partial decoder (PPD), which aggregates these features
    and generates a global map denoted as $S_{g}$. This global map aids in the coarse
    localization of lung infections. Secondly, these high-level features, along with
    $f_{2}$, are directed through multiple cascaded reverse attention (RA) modules
    under the guidance of $S_{g}$. The RA module $R_{4}$ depends on the output of
    another RA module, $R_{5}$. Finally, the output of the last RA module, denoted
    as $S_{3}$, is passed through a sigmoid activation function for the final prediction
    of lung infection regions (image from [[188](#bib.bib188)]).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：Inf-net框架的示意图：CT图像最初通过两个卷积层进行处理，以提取高分辨率（即低级）特征。引入了一个边缘注意模块，以增强感兴趣区域内边界的表现。随后，获得的低级特征，记作
    $f_{2}$，经过三个卷积层提取高级特征。这些高级特征有两个主要用途。首先，它们用于喂给一个并行部分解码器（PPD），该解码器汇总这些特征并生成一个全球地图，记作
    $S_{g}$。这个全球地图有助于粗略定位肺部感染。其次，这些高级特征与 $f_{2}$ 一起，通过多个级联反向注意（RA）模块，在 $S_{g}$ 的指导下进行处理。RA模块
    $R_{4}$ 依赖于另一个RA模块 $R_{5}$ 的输出。最后，最后一个RA模块的输出，记作 $S_{3}$，通过一个 sigmoid 激活函数进行处理，以最终预测肺部感染区域（图片来源于
    [[188](#bib.bib188)]）。
- en: 'Self-training models: In the self-training framework, an initial model is trained
    using limited labeled data. Then, this initial model is utilized to generate pseudo
    labels for the unlabeled data. Subsequently, the labeled dataset is combined with
    the pseudo-labeled dataset to update the initial model. The training process iteratively
    alternates between these two steps until a predetermined number of iterations
    is reached. Self-training approaches primarily vary in terms of model initialization,
    pseudo label generation, and their strategies for addressing pseudo label noise.
    According to the study by [[189](#bib.bib189)], pseudo labels with higher confidence
    tend to be more effective. Consequently, various methods that take into account
    confidence or uncertainty in pseudo labels have been introduced to generate more
    consistent and reliable pseudo labels, such as refining pseudo labels through
    conditional random fields [[190](#bib.bib190)], uncertainty-aware confidence evaluation
    [[191](#bib.bib191)]. Similarly, Ke et al. [[192](#bib.bib192)] proposed a three-stage
    self-training framework to refine pseudo labels in a stage-wise manner. It reduces
    the uncertainty in the predicted probability for the pseudo-masks using a multi-task
    model. Inf-net [[188](#bib.bib188)] addresses the shortage of well-annotated data
    for segmentation of COVID-19 lung infections in CT images. Further, a parallel
    partial decoder (PPD), reverse attention (RA), and edge attention were further
    added to improve the performance of the model, as shown in Figure [12](#S4.F12
    "Figure 12 ‣ 4.1.3 Pseudo-labeling methods ‣ 4.1 Semi-supervised learning ‣ 4
    Incomplete supervision ‣ Data efficient deep learning for medical image analysis:
    A survey"). In contrast to conventional pseudo-labeling techniques, which rely
    on a threshold to pick confidently classified samples, Liu et al. [[193](#bib.bib193)]
    propose the Anti-Curriculum Pseudo-labeling (ACPL) method. ACPL utilizes a mechanism
    known as *cross-distribution sample informativeness* to identify highly informative
    unlabeled samples for pseudo-labeling. It also employs an ensemble of classifiers
    to generate precise pseudo-labels. This approach enables ACPL to effectively handle
    multi-class and multi-label imbalanced classification issues in the field of MIA.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 自训练模型：在自训练框架中，初始模型通过有限的标注数据进行训练。然后，利用这个初始模型为未标注数据生成伪标签。随后，将标注数据集与伪标注数据集结合起来，以更新初始模型。训练过程在这两个步骤之间交替进行，直到达到预定的迭代次数。自训练方法主要在模型初始化、伪标签生成及处理伪标签噪声的策略上有所不同。根据[[189](#bib.bib189)]的研究，置信度较高的伪标签通常更为有效。因此，已经提出了各种考虑伪标签置信度或不确定性的方法，以生成更一致和可靠的伪标签，如通过条件随机场[[190](#bib.bib190)]来精炼伪标签，或使用不确定性感知的置信度评估[[191](#bib.bib191)]。类似地，Ke
    等人[[192](#bib.bib192)]提出了一种三阶段自训练框架，以阶段性方式精炼伪标签。该方法利用多任务模型减少伪掩膜预测概率的不确定性。Inf-net
    [[188](#bib.bib188)]解决了COVID-19肺部感染CT图像分割中标注数据不足的问题。此外，模型性能得到了进一步提高，增加了平行部分解码器（PPD）、反向注意力（RA）和边缘注意力，具体见图[12](#S4.F12
    "图12 ‣ 4.1.3 伪标签方法 ‣ 4.1 半监督学习 ‣ 4 不完全监督 ‣ 医学图像分析的数据高效深度学习：综述")。与传统的伪标签技术依赖阈值选择置信度较高的样本不同，Liu
    等人[[193](#bib.bib193)]提出了反课程伪标签（ACPL）方法。ACPL利用一种称为*跨分布样本信息量*的机制来识别具有高信息量的未标注样本进行伪标签标注。它还使用分类器集成来生成准确的伪标签。这种方法使ACPL能够有效处理MIA领域中的多类和多标签不平衡分类问题。
- en: Recently, Chen et al. [[194](#bib.bib194)] introduced a teacher-student framework
    for multi-organ segmentation in CT scans. They proposed a learning paradigm involving
    $N^{3}$ small cubes extracted from each CT scan, called magic-cubes. Two data
    augmentation strategies were designed. First, labeled and unlabeled data cubes
    were mixed to teach unlabeled data organ semantics in their relative positions.
    Second, for smaller organs, data cubes were shuffled and fed into the student
    network. Finally, the original magic-cubes were reconstructed to align with the
    ground-truth or teacher’s supervision. Further, the teacher network’s predicted
    pseudo labels are improved by blending them with the learned representations of
    the small cubes. This blending strategy considers local attributes like texture,
    luster, and boundary smoothness, addressing the lower performance observed for
    smaller organs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，陈等人[[194](#bib.bib194)]提出了一种用于 CT 扫描中多器官分割的师生框架。他们提出了一种学习范式，其中涉及从每个 CT 扫描中提取的
    $N^{3}$ 个小立方体，称为魔法立方体。设计了两种数据增强策略。首先，将标记和未标记的数据立方体混合，以教授未标记数据中器官的语义及其相对位置。其次，对于较小的器官，将数据立方体进行洗牌并输入到学生网络中。最后，将原始的魔法立方体重建，以与真实标注或教师的监督对齐。此外，通过将教师网络预测的伪标签与小立方体的学习表示进行混合，来改进这些伪标签。这种混合策略考虑了纹理、光泽和边界平滑度等局部属性，解决了在较小器官上观察到的性能较低的问题。
- en: 'Co-training models: In the Co-training framework [[195](#bib.bib195)], a model
    is trained on a dataset with two or more views or representations of the data.
    These views are typically different but complementary. The key idea is that if
    each view provides unique information about the data, the model can learn more
    effectively from the combined knowledge of all views. In contrast to the self-training
    framework, which expands the labeled dataset based on a single model’s confidence,
    co-training iteratively selects instances on which the model is confident based
    on different views, expanding the labeled dataset with complementary information.
    The essence of co-training lies in the process of creating two or more deep models
    that can effectively capture distinct and nearly independent perspectives. These
    approaches typically involve utilizing diverse data sources, implementing various
    network architectures, and applying specialized training techniques to acquire
    a range of diverse deep models [[156](#bib.bib156)]. In the context of medical
    images, data can originate from various modalities or medical centers, resulting
    in distinct distributions. In this regard, [[196](#bib.bib196)] and [[197](#bib.bib197)]
    make use of different views derived from diverse modalities within the co-training
    framework. Some approaches employ different network architectures as distinct
    views. For instance, Luo et al. [[198](#bib.bib198)] propose cross-teaching between
    CNN and Transformer models, which implicitly promotes consistency and complementarity
    between these distinct networks. Peng et al. [[199](#bib.bib199)] generate adversarial
    examples as an alternative view. Similarly, for 3D images, Zhao et al. [[200](#bib.bib200)]
    utilize coronal, sagittal, and axial views of images as diverse input views. Recently,
    Wang et al. [[201](#bib.bib201)] address the issue of imbalanced class distribution
    in Semi-SL methods using the Dual-debiased Heterogeneous Co-training (DHC) framework.
    They introduce two loss weighting techniques called Distribution-aware Debiased
    Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW). These strategies
    utilize pseudo labels dynamically to help the model address data and learning
    biases effectively.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 协同训练模型：在协同训练框架[[195](#bib.bib195)]中，一个模型在具有两个或更多数据视图或表示的数据集上进行训练。这些视图通常是不同的但互补的。关键思想是，如果每个视图提供有关数据的独特信息，模型可以从所有视图的综合知识中更有效地学习。与自我训练框架不同，自我训练框架基于单个模型的信心扩展标记数据集，而协同训练则迭代地选择模型在不同视图上自信的实例，用互补的信息扩展标记数据集。协同训练的本质在于创建两个或更多能够有效捕捉不同且几乎独立视角的深度模型的过程。这些方法通常涉及利用多样的数据源，实施各种网络架构，并应用专业的训练技术以获得一系列多样的深度模型[[156](#bib.bib156)]。在医学图像的背景下，数据可能来源于不同的模态或医疗中心，导致分布差异。在这方面，[[196](#bib.bib196)]和[[197](#bib.bib197)]利用协同训练框架中从不同模态派生的不同视图。一些方法采用不同的网络架构作为不同的视图。例如，Luo等人[[198](#bib.bib198)]提出了CNN和Transformer模型之间的交叉教学，这隐含地促进了这些不同网络之间的一致性和互补性。Peng等人[[199](#bib.bib199)]生成对抗性样本作为替代视图。类似地，对于3D图像，Zhao等人[[200](#bib.bib200)]利用图像的冠状面、矢状面和轴面视图作为多样的输入视图。最近，Wang等人[[201](#bib.bib201)]在使用Dual-debiased
    Heterogeneous Co-training (DHC)框架解决Semi-SL方法中的类不平衡问题。他们引入了两种损失加权技术，分别是Distribution-aware
    Debiased Weighting (DistDW)和Difficulty-aware Debiased Weighting (DiffDW)。这些策略动态地利用伪标签，帮助模型有效地解决数据和学习偏差。
- en: 4.1.4 Hybrid models
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 混合模型
- en: An emerging area of research in Semi-SL involves integrating the previously
    mentioned methods into a unified framework to achieve improved performance. These
    combined approaches are referred to as hybrid methods [[202](#bib.bib202), [203](#bib.bib203),
    [204](#bib.bib204)]. Several studies have explored the combination of pseudo-labeling
    and contrastive learning methods [[205](#bib.bib205), [206](#bib.bib206), [207](#bib.bib207),
    [208](#bib.bib208)] for different tasks. Specifically, both Chaitanya et al. [[205](#bib.bib205)]
    and Basak et al. [[206](#bib.bib206)] introduce a self-training method based on
    local contrastive learning, guided by pseudo-labels, and demonstrate its effectiveness
    across various medical segmentation datasets. For COVID-19 Screening and Lesion
    Segmentation, Zeng et al. [[208](#bib.bib208)] present a double-threshold pseudo-labeling
    approach and a novel inter-slice consistency regularization technique designed
    specifically for CT images. Wang et al. [[202](#bib.bib202)] utilize self-training
    with consistency regularization to efficiently extract valuable information from
    unlabeled data, and they incorporate virtual adversarial training to enhance the
    model’s generalization capability. ASE-Net [[209](#bib.bib209)] comprises segmentation
    networks and a discriminator network. The segmentation network is constructed
    using the MT framework, while the discriminator network employs an adversarial
    consistency training strategy (ACTS) with two discriminators focused on consistency
    learning. This strategy helps establish prior relationships between labeled and
    unlabeled data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习（Semi-SL）中的一个新兴研究领域涉及将前述方法整合到一个统一的框架中以实现性能提升。这些结合的方法被称为混合方法 [[202](#bib.bib202),
    [203](#bib.bib203), [204](#bib.bib204)]。一些研究探索了伪标签和对比学习方法的组合 [[205](#bib.bib205),
    [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208)] 用于不同的任务。具体而言，Chaitanya
    等 [[205](#bib.bib205)] 和 Basak 等 [[206](#bib.bib206)] 介绍了一种基于局部对比学习的自训练方法，该方法由伪标签指导，并展示了其在各种医学分割数据集中的有效性。对于
    COVID-19 筛查和病变分割，Zeng 等 [[208](#bib.bib208)] 提出了一种双阈值伪标签方法和一种针对 CT 图像设计的新型切片间一致性正则化技术。Wang
    等 [[202](#bib.bib202)] 利用具有一致性正则化的自训练方法从未标记数据中高效提取有价值的信息，并结合虚拟对抗训练以增强模型的泛化能力。ASE-Net
    [[209](#bib.bib209)] 包含分割网络和判别网络。分割网络采用 MT 框架构建，而判别网络使用具有两个关注一致性学习的判别器的对抗一致性训练策略（ACTS）。这一策略有助于建立标记数据和未标记数据之间的先验关系。
- en: 4.2 Active learning
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主动学习
- en: 'Active learning (AL) [[210](#bib.bib210)] operates on the assumption that the
    ground-truth labels of unlabeled instances can be obtained by querying an expert
    annotators (see Figure [13](#S4.F13 "Figure 13 ‣ 4.2 Active learning ‣ 4 Incomplete
    supervision ‣ Data efficient deep learning for medical image analysis: A survey")).
    Assuming that the labeling cost is solely based on the number of queries, the
    objective of active learning is to minimize the number of queries required while
    still achieving effective model training with minimized labeling costs. In situations
    where there is a limited set of labeled data but an abundance of unlabeled data,
    active learning aims to identify the most valuable unlabeled instance for querying.
    There are two commonly used selection criteria: informativeness and representativeness.
    Informativeness assesses how effectively an unlabeled instance reduces the uncertainty
    of a statistical model, while representativeness calculates how well an instance
    represents the structure of input patterns.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '主动学习（AL）[[210](#bib.bib210)] 基于这样一个假设：可以通过询问专家注释者来获得未标记实例的真实标签（见图 [13](#S4.F13
    "Figure 13 ‣ 4.2 Active learning ‣ 4 Incomplete supervision ‣ Data efficient deep
    learning for medical image analysis: A survey")）。假设标注成本仅基于查询次数，主动学习的目标是最小化所需查询的次数，同时以最小的标注成本实现有效的模型训练。在标记数据有限但未标记数据丰富的情况下，主动学习旨在识别最有价值的未标记实例进行查询。常用的选择标准有两个：信息量和代表性。信息量评估未标记实例在多大程度上有效地减少了统计模型的不确定性，而代表性则计算实例在多大程度上代表了输入模式的结构。'
- en: '![Refer to caption](img/8aebfa067709b452a760b2f9cdb002a5.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8aebfa067709b452a760b2f9cdb002a5.png)'
- en: 'Figure 13: Overview of the active learning paradigm: In a cycle, a deep learning
    model is trained on a labeled medical dataset. Then, active sampling strategies
    are implemented to select the data that is most valuable to the model from an
    unlabeled medical dataset. Finally, oracles are used to annotate the selected
    data. Image courtesy of Peng and Wang [[211](#bib.bib211)].'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：主动学习范式概述：在一个循环中，深度学习模型在标记的医学数据集上进行训练。然后，实施主动采样策略，从未标记的医学数据集中选择对模型最有价值的数据。最后，使用*oracle*对选择的数据进行标注。图片由
    Peng 和 Wang 提供 [[211](#bib.bib211)]。
- en: 4.2.1 Evaluating informativeness
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 评估信息量
- en: The primary category of informativeness measures revolves around calculating
    uncertainty. The key idea is that including the ground truth for samples with
    higher uncertainty in the training set can provide more valuable information.
    In the deep learning area, uncertainty-based sampling has seen widespread usage
    in recent active learning methods [[212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214)].
    Specifically, Wen et al. [[213](#bib.bib213)] introduce an active learning approach
    that employs uncertainty sampling to facilitate quality control of nucleus segmentation
    in pathology images. Wu et al. [[214](#bib.bib214)] use both the network loss
    and diversity condition as the uncertainty metric for sampling from a loss prediction
    network. They apply this method to the COVID-19 classification task. Zhou et al.
    [[215](#bib.bib215)] introduce the concept of active selection policies, where
    the highest confidence is determined based on the entropy and diversity of the
    sampled data in the mean prediction outcomes. For classifying radiology images,
    Balram et al. [[216](#bib.bib216)] introduce an integrated end-to-end solution
    that merges consistency-driven semi-supervised learning with uncertainty-guided
    active learning, aiming to alleviate the need for extensive manual annotations.
    Another prevalent approach for estimating informativeness involves assessing the
    agreement among various models executing the same task. The reasoning is that
    greater disagreement observed between predictions on similar data points indicates
    a higher degree of uncertainty. These techniques are commonly employed in situations
    where ensembling is utilized to enhance performance. Ensembling involves training
    multiple models to execute the same task with slight variations in parameters
    or settings [[217](#bib.bib217), [218](#bib.bib218)]. Beluch Bcai et al. [[218](#bib.bib218)]
    showcase the effectiveness of ensembles in active learning and compare them to
    alternative approaches. Kuo et al. [[217](#bib.bib217)] employed an ensemble technique
    to assess uncertainty in the context of intracranial hemorrhage segmentation,
    utilizing the Jensen-Shannon divergence. Additionally, they made an effort to
    predict the time required for manual delineation using a log-linear model. Their
    approach involved selecting examples for manual segmentation based on maximizing
    the cumulative uncertainty within a specified time constraint. Atzeni et al. [[219](#bib.bib219)]
    adopt an iterative method, requesting manual delineation for a single Region of
    Interest (ROI) on a single slice per iteration rather than labeling all structures
    within a slice or volume. They update a segmentation CNN that generates dense
    segmentations for all slices using mixed-cross entropy loss, effectively utilizing
    partially annotated images. Similar to Kuo et al. [[217](#bib.bib217)], they use
    tracing time, based on boundary length, as a practical measure of effort. However,
    in contrast to Kuo et al. [[217](#bib.bib217)], they also account for multiple
    ROIs and their spatial relationships.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 信息量测量的主要类别围绕计算不确定性展开。关键思想是，将不确定性较高的样本的真实情况包括在训练集中，可以提供更有价值的信息。在深度学习领域，基于不确定性的采样在最近的主动学习方法中得到了广泛使用
    [[212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214)]。具体来说，Wen 等人 [[213](#bib.bib213)]
    引入了一种主动学习方法，利用不确定性采样来促进病理图像中细胞核分割的质量控制。Wu 等人 [[214](#bib.bib214)] 使用网络损失和多样性条件作为从损失预测网络中采样的不确定性度量，并将该方法应用于
    COVID-19 分类任务。Zhou 等人 [[215](#bib.bib215)] 引入了主动选择策略的概念，其中最高置信度是基于样本数据在平均预测结果中的熵和多样性来确定的。对于放射影像的分类，Balram
    等人 [[216](#bib.bib216)] 引入了一种集成的端到端解决方案，将一致性驱动的半监督学习与不确定性引导的主动学习结合起来，旨在减少对大量手工标注的需求。另一种常见的估计信息量的方法是评估执行相同任务的不同模型之间的一致性。其推理是，如果在相似数据点上的预测之间的分歧较大，则表明不确定性较高。这些技术通常用于利用集成来提升性能的情况。集成涉及训练多个模型来执行相同任务，参数或设置略有不同
    [[217](#bib.bib217), [218](#bib.bib218)]。Beluch Bcai 等人 [[218](#bib.bib218)] 展示了集成在主动学习中的有效性，并与其他方法进行了比较。Kuo
    等人 [[217](#bib.bib217)] 采用集成技术评估颅内出血分割中的不确定性，利用了 Jensen-Shannon 散度。此外，他们还尝试使用对数线性模型预测手动划定所需的时间。他们的方法涉及基于最大化指定时间限制内的累计不确定性来选择手动分割的样本。Atzeni
    等人 [[219](#bib.bib219)] 采用了迭代方法，每次迭代要求对单个切片上的单个感兴趣区域（ROI）进行手动划定，而不是标注切片或体积中的所有结构。他们更新了一个分割
    CNN，该网络使用混合交叉熵损失生成所有切片的密集分割，有效利用部分标注的图像。类似于 Kuo 等人 [[217](#bib.bib217)]，他们使用基于边界长度的追踪时间作为实际的工作量度量。然而，与
    Kuo 等人 [[217](#bib.bib217)] 不同的是，他们还考虑了多个 ROI 及其空间关系。
- en: Bayesian neural networks have gained significant interest due to their capacity
    to represent and propagate the probability of deep learning models. Gal et al.
    [[220](#bib.bib220)] introduce the concept of using Bayesian CNNs for AL, specifically
    employing *Bayesian Active Learning by Disagreement* (BALD). Their study demonstrates
    the superior performance of Bayesian CNNs compared to deterministic CNNs within
    the context of AL. Mahapatra et al. [[221](#bib.bib221)] employ a conditional
    GAN to generate chest X-ray images based on a real image. Additionally, they use
    a Bayesian neural network to assess the informativeness of each generated sample,
    determining whether it should be utilized as training data. If selected, the sample
    is used to fine-tune the network. Their study demonstrates that this method achieves
    comparable performance to training on fully labeled data, even when working with
    a dataset where only 33 % of the pixels in the training set have annotations.
    This provides significant time, effort, and cost savings for annotators. Dai et
    al. [[222](#bib.bib222)] proposed a distinctive method for brain tumor segmentation.
    Instead of traditional approaches, they adopted a novel strategy to select the
    most informative example. This involved moving through the image space along the
    gradient direction of the Dice loss and identifying the nearest neighbor of this
    image within a lower-dimensional latent space, which was learned using a variational
    autoencoder. Certain studies address the challenge of the cold start problem in
    Active Learning, which pertains to the initial selection of images for labeling
    when no labeled data is available as a starting point [[223](#bib.bib223), [224](#bib.bib224)].
    Nath et al. [[223](#bib.bib223)] address the issue of cold start by introducing
    a proxy task and subsequently leveraging the uncertainty generated from this proxy
    task to prioritize the annotation of unlabeled data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络因其在深度学习模型中表示和传播概率的能力而受到广泛关注。Gal等人[[220](#bib.bib220)]引入了使用贝叶斯CNN进行主动学习的概念，特别是采用*贝叶斯主动学习通过争议*（BALD）。他们的研究表明，在主动学习的背景下，贝叶斯CNN的表现优于确定性CNN。Mahapatra等人[[221](#bib.bib221)]使用条件GAN生成基于真实图像的胸部X光图像。此外，他们还使用贝叶斯神经网络评估每个生成样本的信息量，决定是否将其用作训练数据。如果被选择，该样本将用于微调网络。他们的研究表明，即使在训练集中只有33%的像素有注释的情况下，这种方法也能实现与全标记数据训练相当的性能。这为标注人员节省了大量时间、精力和成本。Dai等人[[222](#bib.bib222)]提出了一种独特的大脑肿瘤分割方法。他们没有采用传统方法，而是采用了一种新策略来选择最具信息量的样本。这包括沿Dice损失的梯度方向移动图像空间，并在使用变分自编码器学习的低维潜在空间中识别该图像的最近邻。某些研究解决了主动学习中的冷启动问题，这涉及到在没有标记数据作为起点的情况下进行初始图像选择[[223](#bib.bib223),
    [224](#bib.bib224)]。Nath等人[[223](#bib.bib223)]通过引入代理任务来解决冷启动问题，并利用该代理任务生成的不确定性来优先标注未标记的数据。
- en: 4.2.2 Representativeness
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 代表性
- en: 'These approaches go beyond relying solely on uncertainty-based methods and
    instead focus on evaluating the diversity within chosen samples to minimize repetitive
    annotations. By introducing a representativeness measure, these strategies aim
    to promote the selection of samples from various areas of the distribution, leading
    to greater sample diversity and ultimately enhancing the performance of AL. To
    this end, Yang et al. [[225](#bib.bib225)] introduce Suggestive Annotation, a
    deep active learning framework designed for medical image segmentation. This framework
    utilizes a different approach to uncertainty sampling and incorporates a form
    of representativeness density weighting. The method involves training multiple
    models, each of which excludes a portion of the training data. These models are
    then leveraged to calculate an ensemble-based uncertainty measure. Ozdemir et
    al. [[226](#bib.bib226)] create a Bayesian network and utilize Monte Carlo dropout
    to derive variance information as a measure of model uncertainty. In addition,
    they employ infoVAE [[227](#bib.bib227)] to build a representativeness metric,
    which aids in the selection of samples through maximum likelihood sampling within
    the latent space. Li et al. [[228](#bib.bib228)] adopt k-means clustering and
    curriculum classification (CC) techniques, leveraging CurriculumNet [[229](#bib.bib229)],
    to estimate uncertainty and representativeness in their approach. Li et al. [[224](#bib.bib224)]
    tackle the challenge of the cold start problem by employing representativeness
    sampling that relies on the distance matrix to choose an initial dataset that
    is representative. They also introduce a hybrid sample selection approach that
    incorporates pixel entropy, region consistency, and image diversity scores to
    filter the samples. These three scores reflect informativeness at different levels:
    pixel, region, and image. This strategy, which combines these three levels of
    scores, proves to be more effective in selecting the most valuable samples compared
    to using a simple pixel uncertainty score alone. Wang et al. [[230](#bib.bib230)]
    utilize model ensembles to guide user labeling, focusing on cells that optimize
    a blend of uncertainty, diversity (evaluated using a clustering algorithm), and
    representativeness assessed through cosine similarity of features.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法超越了单纯依赖基于不确定性的方法，转而关注评估所选样本中的多样性，以减少重复标注。通过引入代表性度量，这些策略旨在促进从分布的各个领域中选择样本，从而实现更大的样本多样性，最终提升主动学习（AL）的性能。为此，Yang
    等人 [[225](#bib.bib225)] 引入了建议性标注，这是一种针对医学图像分割的深度主动学习框架。该框架采用了不同于不确定性采样的方法，并结合了一种代表性密度加权形式。该方法涉及训练多个模型，每个模型排除了部分训练数据。然后利用这些模型计算基于集成的、不确定性度量。Ozdemir
    等人 [[226](#bib.bib226)] 创建了一个贝叶斯网络，并利用蒙特卡罗 dropout 来推导方差信息作为模型不确定性的度量。此外，他们采用了
    infoVAE [[227](#bib.bib227)] 来构建代表性度量，这有助于通过在潜在空间内的最大似然采样来选择样本。Li 等人 [[228](#bib.bib228)]
    采用了 k-means 聚类和课程分类（CC）技术，利用 CurriculumNet [[229](#bib.bib229)] 来估计不确定性和代表性。Li
    等人 [[224](#bib.bib224)] 通过利用代表性采样来应对冷启动问题，该方法依赖于距离矩阵来选择具有代表性的初始数据集。他们还引入了一种混合样本选择方法，该方法结合了像素熵、区域一致性和图像多样性分数来过滤样本。这三种分数反映了不同层次的信息量：像素、区域和图像。这种结合了这三种层次的分数的策略，在选择最有价值的样本方面，比单独使用简单的像素不确定性分数更为有效。Wang
    等人 [[230](#bib.bib230)] 采用模型集成来指导用户标注，重点关注优化不确定性、多样性（通过聚类算法评估）和代表性（通过特征的余弦相似度评估）的细胞。
- en: 4.3 Domain-adaptive learning
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 域适应学习
- en: Domain adaptive learning, also known as domain adaptation [[231](#bib.bib231)],
    is a learning paradigm focused on improving the performance of a model on a target
    domain by leveraging knowledge learned from a source domain. In this context,
    a domain refers to a specific distribution of data, which can vary in terms of
    characteristics like data collection settings, sensor types, lighting conditions,
    or other factors that affect the data’s distribution. The main challenge addressed
    by domain adaptive learning is the domain shift problem. This problem arises when
    there is a mismatch between the source domain (where the model is trained) and
    the target domain (where the model needs to perform well). Due to this mismatch,
    a model trained on one domain may not generalize effectively to another domain.
    Domain adaptive learning methods aim to bridge this gap by adapting the model
    to the target domain. Unsupervised Domain Adaptation (UDA) is a specific case
    of domain adaptation where you only have labeled data in the source domain and
    no labeled data in the target domain. The adaptation process is entirely unsupervised,
    meaning it relies solely on unlabeled data in the target domain. These methods
    encompass various approaches, including feature alignment, image translation-based
    methods, learning disentangled representations, pseudo-labeling approaches, self-supervision,
    and hybrid methods.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 域适应学习，也称为领域适应 [[231](#bib.bib231)]，是一种学习范式，旨在通过利用从源领域中学习到的知识来提高模型在目标领域上的性能。在这种背景下，领域指的是数据的特定分布，这些分布可能在数据收集设置、传感器类型、光照条件或其他影响数据分布的因素方面有所不同。领域适应学习解决的主要挑战是领域偏移问题。这个问题出现于源领域（模型训练所在的领域）与目标领域（模型需要在其中表现良好的领域）之间存在不匹配时。由于这种不匹配，在一个领域上训练的模型可能无法有效地推广到另一个领域。领域适应学习方法旨在通过将模型适应目标领域来弥合这一差距。无监督领域适应（UDA）是领域适应的一种特例，其中你只有源领域中的标记数据而在目标领域没有标记数据。适应过程完全是无监督的，意味着它仅依赖于目标领域中的未标记数据。这些方法包括各种方法，如特征对齐、基于图像翻译的方法、学习解缠表示、伪标签方法、自监督和混合方法。
- en: 4.3.1 Feature alignment
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 特征对齐
- en: The fundamental idea behind feature alignment in UDA is to lessen the distinction
    between the source and target domains by learning domain-invariant representations.
    Various UDA approaches map images from both domains onto a common latent space
    to mitigate disparities. This can be accomplished directly by reducing a disparity
    measure that quantifies domain dissimilarities. Alternatively, it can be realized
    implicitly through adversarial learning techniques. The objective is to align
    the feature distributions of both the source and target domains, ensuring that
    the learned representations can be smoothly transferred and effectively utilized
    in diverse domains.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: UDA（无监督领域适应）中的特征对齐的基本理念是通过学习领域不变的表示来减少源领域和目标领域之间的差异。各种 UDA 方法将来自两个领域的图像映射到一个共同的潜在空间，以减轻差异。这可以通过直接减少量化领域差异的差异度量来实现。或者，它也可以通过对抗性学习技术间接实现。目标是对齐源领域和目标领域的特征分布，确保所学习的表示能够平滑地转移并在不同领域中有效利用。
- en: 'Explicit discrepancy minimization: Methods focused on explicitly minimizing
    discrepancies usually create a measure or loss function that calculates how different
    the source and target distributions are from each other. This measure is then
    reduced during training to encourage the development of features that work well
    in both domains. Different measures, like Maximum Mean Discrepancy (MMD) [[232](#bib.bib232),
    [233](#bib.bib233)], Kullback-Leibler (KL) divergence [[234](#bib.bib234)], and
    Contrastive Loss (CL) [[235](#bib.bib235), [236](#bib.bib236)], can be employed
    for this purpose. Specifically, Yu et al. [[232](#bib.bib232)] use two separate
    feature encoders for both the target and source domains. They integrate an attention
    technique to focus on particular brain regions and employ MMD to acquire features
    that work well across domains for the prediction of subjective cognitive decline.
    Another explicit measurement used in UDA is the Characteristic Function (CF) distance
    [[237](#bib.bib237)]. This metric calculates the distinction between the distributions
    of latent features in the frequency domain instead of the spatial domain.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 显式差异最小化：专注于显式最小化差异的方法通常创建一个度量或损失函数，计算源领域和目标领域的分布之间的差异。然后，在训练过程中减少该度量，以鼓励开发在两个领域都能有效的特征。可以使用不同的度量，例如最大均值差异（MMD）[[232](#bib.bib232),
    [233](#bib.bib233)]、Kullback-Leibler（KL）散度[[234](#bib.bib234)]和对比损失（CL）[[235](#bib.bib235),
    [236](#bib.bib236)]。具体来说，Yu等人[[232](#bib.bib232)]使用两个独立的特征编码器分别针对目标领域和源领域。他们集成了一种注意机制，聚焦于特定的脑区，并使用MMD来获取在不同领域之间都能有效的特征，用于预测主观认知下降。UDA中使用的另一种显式度量是特征函数（CF）距离[[237](#bib.bib237)]。该度量计算频域中潜在特征分布之间的区别，而不是空间域中的区别。
- en: 'Implicit discrepancy minimization: Implicit methods for reducing differences
    in UDA mainly rely on the concepts of adversarial learning. To ensure that feature
    distributions are comparable between different domains, a technique called domain-adversarial
    neural network (DANN) [[238](#bib.bib238)] is used. This approach involves incorporating
    a gradient reversal layer (GRL) into the framework of Generative Adversarial Networks
    (GANs), as illustrated in Figure [14](#S4.F14 "Figure 14 ‣ 4.3.1 Feature alignment
    ‣ 4.3 Domain-adaptive learning ‣ 4 Incomplete supervision ‣ Data efficient deep
    learning for medical image analysis: A survey"). The network comprises two classifiers
    and shared feature extraction layers. With the help of GRL, DANN aims to maximize
    the loss due to domain confusion while minimizing the loss associated with label
    prediction for source samples and domain confusion loss for all samples. DANN
    serves as a foundational model for different UDA methods that are built upon adversarial
    learning principles.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '隐式差异最小化：隐式方法用于减少UDA中的差异，主要依赖于对抗学习的概念。为了确保不同领域之间的特征分布具有可比性，使用了一种称为领域对抗神经网络（DANN）的技术[[238](#bib.bib238)]。这种方法涉及将梯度反转层（GRL）融入生成对抗网络（GANs）的框架中，如图[14](#S4.F14
    "Figure 14 ‣ 4.3.1 Feature alignment ‣ 4.3 Domain-adaptive learning ‣ 4 Incomplete
    supervision ‣ Data efficient deep learning for medical image analysis: A survey")所示。该网络包含两个分类器和共享的特征提取层。在GRL的帮助下，DANN旨在最大化由于领域混淆造成的损失，同时最小化源样本的标签预测损失和所有样本的领域混淆损失。DANN作为不同UDA方法的基础模型，这些方法基于对抗学习原则构建。'
- en: '![Refer to caption](img/d2139cc2cab0a276970c455bddb02613.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d2139cc2cab0a276970c455bddb02613.png)'
- en: 'Figure 14: The figure demonstrates the Domain Adversarial Neural Network (DANN)
    framework, a classic and effective model designed for learning domain-invariant
    features using adversarial training (image courtesy of Ganin [[238](#bib.bib238)]).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：该图展示了领域对抗神经网络（DANN）框架，这是一种经典且有效的模型，用于通过对抗训练学习领域不变的特征（图像由Ganin提供[[238](#bib.bib238)]）。
- en: Different research studies employ implicit techniques for a range of classification
    issues. For instance, Ren et al. [[239](#bib.bib239)] utilize an adversarial loss
    along with siamese architecture for whole slide images. Zhang et al. [[240](#bib.bib240)]
    leverage adversarial learning and introduce focal loss to tackle the problem of
    class imbalance in histopathology images. More recently, Feng et al. [[241](#bib.bib241)]
    engage in binary and multi-class classification tasks related to diagnosing pneumonia.
    They make use of a conditional domain adversarial network to narrow the domain
    discrepancy and implement a contrastive loss to address the challenge of limited
    data in the target domain. Certain investigations have combined self-training
    and adversarial learning for the task of medical image segmentation [[242](#bib.bib242),
    [243](#bib.bib243), [244](#bib.bib244)]. Specifically, Liu et al. [[243](#bib.bib243)]
    proposed the Self-cleansing UDA (S-cuda) technique, which is specifically designed
    to address the issue of domain shift and handle noisy labels in the source domain.
    This method utilizes self-training to produce accurate pseudo-labels for both
    the noisy source and unlabeled target domains. Beyond image classification and
    segmentation, various other applications also make use of implicit discrepancy
    methods. For instance, these methods are applied in bronchoscopic depth estimation
    [[245](#bib.bib245)], reconstructing precise high-resolution (HR) representations
    from low-resolution (LR) OCTA images [[246](#bib.bib246)], and automating sleep
    staging [[247](#bib.bib247)].
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的研究采用隐式技术来处理各种分类问题。例如，Ren 等人 [[239](#bib.bib239)] 利用对抗性损失以及孪生网络架构来处理全切片图像。Zhang
    等人 [[240](#bib.bib240)] 结合对抗学习，并引入焦点损失来解决组织病理图像中的类别不平衡问题。最近，Feng 等人 [[241](#bib.bib241)]
    从事与诊断肺炎相关的二分类和多分类任务。他们利用条件领域对抗网络来缩小领域间的差异，并实施对比损失来解决目标领域数据有限的挑战。某些研究将自训练和对抗学习结合起来用于医学图像分割任务
    [[242](#bib.bib242), [243](#bib.bib243), [244](#bib.bib244)]。具体而言，Liu 等人 [[243](#bib.bib243)]
    提出了自清洁UDA（S-cuda）技术，专门设计用于解决领域偏移问题并处理源领域中的噪声标签。这种方法利用自训练生成源领域和无标签目标领域的准确伪标签。除了图像分类和分割之外，各种其他应用也利用隐式差异方法。例如，这些方法被应用于支气管镜深度估计
    [[245](#bib.bib245)]、从低分辨率（LR）OCTA 图像中重建精确的高分辨率（HR）表示 [[246](#bib.bib246)] 和自动化睡眠分期
    [[247](#bib.bib247)]。
- en: 4.3.2 Image translation based methods
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 图像翻译方法
- en: 'Image translation techniques achieve domain alignment by altering the pixel-level
    appearance of source data to match the characteristics of a target domain. Generative
    Adversarial Networks (GANs) are often used for tasks involving direct mapping
    between pixels for image translation. A widely used approach in this category
    is CycleGAN [[248](#bib.bib248)], which operates as an image-to-image translation
    architecture (see Figure [15](#S4.F15 "Figure 15 ‣ 4.3.2 Image translation based
    methods ‣ 4.3 Domain-adaptive learning ‣ 4 Incomplete supervision ‣ Data efficient
    deep learning for medical image analysis: A survey")). It transforms features
    from one image domain into another without relying on paired training examples.
    In the medical field, several approaches apply CycleGAN for unsupervised domain
    adaptation (UDA). However, CycleGAN’s emphasis on pixel-level mapping might not
    consistently ensure the preservation of semantic information in medical images.
    To overcome this limitation, multiple studies have integrated semantic understanding
    into the framework. Various works [[249](#bib.bib249), [250](#bib.bib250), [251](#bib.bib251)]
    have incorporated task-specific losses within the UDA context. These task-specific
    losses are designed to enhance the UDA procedure by introducing extra constraints
    aligned with the unique requirements of the task at hand.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图像翻译技术通过改变源数据的像素级外观以匹配目标域的特征，从而实现领域对齐。生成对抗网络（GANs）通常用于涉及图像翻译的像素直接映射任务。在这一类别中，广泛使用的方法是
    CycleGAN [[248](#bib.bib248)]，它作为一种图像到图像的翻译架构（见图 [15](#S4.F15 "图 15 ‣ 4.3.2 图像翻译方法
    ‣ 4.3 领域自适应学习 ‣ 4 不完全监督 ‣ 医学图像分析的数据高效深度学习：综述")）。它将一个图像域的特征转换为另一个图像域，而不依赖于配对的训练样本。在医学领域，几种方法应用了
    CycleGAN 进行无监督领域适应（UDA）。然而，CycleGAN 对像素级映射的重视可能无法始终确保医学图像语义信息的保留。为了解决这一限制，多项研究已将语义理解融入到框架中。各种研究
    [[249](#bib.bib249), [250](#bib.bib250), [251](#bib.bib251)] 在 UDA 过程中整合了任务特定的损失。这些任务特定的损失旨在通过引入额外的约束来增强
    UDA 过程，以满足特定任务的独特要求。
- en: '![Refer to caption](img/59735d8cfe0ebe235dc6c9e98cf8df78.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/59735d8cfe0ebe235dc6c9e98cf8df78.png)'
- en: 'Figure 15: Illustration of CycleGAN framework: (a) CycleGAN comprises two mapping
    functions $G:X\rightarrow Y$ and $F:Y\rightarrow X$, accompanied by adversarial
    discriminators $D_{Y}$ and $D_{X}$. $D_{Y}$ encourages $G$ to translate $X$ into
    outputs indistinguishable from domain $Y$, while $D_{X}$ performs the reverse
    task. (b) The forward cycle-consistency loss is represented as: $x\rightarrow
    G(x)\rightarrow F(G(x))\approx x$. (c) The backward cycle-consistency loss is
    represented as: $y\rightarrow F(y)\rightarrow G(F(y))\approx y$ (image courtesy
    of Zhu et al. [[248](#bib.bib248)]).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：CycleGAN 框架示意图：(a) CycleGAN 包括两个映射函数 $G:X\rightarrow Y$ 和 $F:Y\rightarrow
    X$，以及对抗性判别器 $D_{Y}$ 和 $D_{X}$。$D_{Y}$ 鼓励 $G$ 将 $X$ 转换为与域 $Y$ 无法区分的输出，而 $D_{X}$
    则执行相反的任务。(b) 正向循环一致性损失表示为：$x\rightarrow G(x)\rightarrow F(G(x))\approx x$。(c)
    反向循环一致性损失表示为：$y\rightarrow F(y)\rightarrow G(F(y))\approx y$（图片由 Zhu 等人提供 [[248](#bib.bib248)]）。
- en: 'Certain works employ attention mechanisms to capture distant relationships
    [[252](#bib.bib252), [253](#bib.bib253)]. In the context of cross-modality domain
    adaptation, Tomar et al. [[252](#bib.bib252)] employ a dual cycle consistency
    loss to maintain semantic content while performing image translation. They propose
    a self-attentive spatial adaptive normalization technique that comprises two components:
    the synthesis module and the attention module. The synthesis module’s intermediate
    layers receive semantic layout information from the attention module, aiding in
    the learning of the translation process. Certain studies exploring UDA in image
    detection also employ image translation techniques. For instance, Xing et al.
    [[254](#bib.bib254)] delve into UDA for cell detection across different data modalities.
    They leverage the CycleGAN framework to adjust source images to align with the
    target domain. Their methodology involves training a structured regression-based
    object detector using these adapted source images. Furthermore, they refine the
    detector by incorporating pseudo-labels derived from the target training dataset.
    Extending their earlier study, Xing et al. [[255](#bib.bib255)] enhance their
    method by introducing bidirectional mapping. This involves translating images
    both from the source to the target and vice versa. They also expand this framework
    to address the semi-supervised scenario. In a subsequent extension of their research,
    Xing and Cornish [[256](#bib.bib256)] tackle not just the UDA challenges in cell/nucleus
    detection but also address the challenge of having scarce training data in the
    target domain.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 某些研究采用注意力机制来捕捉远程关系[[252](#bib.bib252)、[253](#bib.bib253)]。在跨模态领域适应的背景下，Tomar
    等人[[252](#bib.bib252)]使用双循环一致性损失来保持语义内容，同时进行图像翻译。他们提出了一种自注意空间自适应归一化技术，包括两个组件：合成模块和注意力模块。合成模块的中间层从注意力模块接收语义布局信息，帮助学习翻译过程。一些研究探索了图像检测中的UDA，也使用了图像翻译技术。例如，Xing
    等人[[254](#bib.bib254)]深入研究了跨不同数据模态的细胞检测UDA。他们利用CycleGAN框架调整源图像以与目标领域对齐。他们的方法包括使用这些调整后的源图像训练结构化回归基础的目标检测器。此外，他们通过整合来自目标训练数据集的伪标签来改进检测器。扩展他们的早期研究，Xing
    等人[[255](#bib.bib255)]通过引入双向映射来增强他们的方法。这涉及从源到目标以及从目标到源的图像翻译。他们还将这一框架扩展到处理半监督场景。在他们研究的后续扩展中，Xing
    和 Cornish [[256](#bib.bib256)]不仅解决了细胞/细胞核检测中的UDA挑战，还解决了目标领域训练数据稀缺的问题。
- en: 4.3.3 Learning disentangled representations
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 学习解耦表示
- en: Rather than imposing the demanding requirement of making the entire model or
    features domain-invariant, an alternative approach is to ease this constraint
    by permitting certain components to be domain-specific [[257](#bib.bib257)]. This
    essentially involves acquiring disentangled representations. The key idea of disentangled
    representation is to differentiate between the content and style of an image.
    The underlying premise is that the content, which refers to anatomical information,
    remains uniform across domains, whereas the style, encompassing attributes like
    texture and lighting, is specific to each domain. In the process of achieving
    disentangled representation [[258](#bib.bib258)], initial steps involve extracting
    style and content codes from both the source and target images using specialized
    encoders. Subsequently, generators are employed to create images in the opposite
    domains by combining content codes from one domain with style codes from the other.
    This interplay of generators aims to deceive discriminators by generating images
    that confuse the domains’ distinguishing features, leading to the desired disentangled
    representation. Wang et al. [[259](#bib.bib259)] incorporated the segmentation
    stage and diverse image translation stage into a cohesive end-to-end approach.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 与其强制要求整个模型或特征具有领域不变性，不如通过允许某些组件具有领域特定性来缓解这一约束[[257](#bib.bib257)]。这本质上涉及获取解耦表示。解耦表示的关键思想是区分图像的内容和风格。基本前提是，内容（指解剖信息）在各个领域中保持一致，而风格（包括纹理和光照等属性）是特定于每个领域的。在实现解耦表示的过程中[[258](#bib.bib258)]，初步步骤涉及使用专门的编码器从源图像和目标图像中提取风格和内容编码。随后，生成器被用于通过将一个领域的内容编码与另一个领域的风格编码结合起来生成对立领域的图像。这种生成器的相互作用旨在通过生成混淆领域区分特征的图像来欺骗判别器，从而实现期望的解耦表示。Wang
    等人[[259](#bib.bib259)]将分割阶段和多样化的图像翻译阶段整合为一个连贯的端到端方法。
- en: Sun et al. [[260](#bib.bib260)] employ a combination of the attention mechanism
    and disentanglement to further mitigate the disparities between domains. Specifically,
    they adopt a preliminary alignment phase to address issues like variations in
    brightness between MRI and CT images. Following this, they introduce an improved
    approach to disentanglement that leverages the Hilbert-Schmidt independence criterion
    to encourage independence and complementary characteristics between content and
    style attributes. Lastly, they integrate an attention bias mechanism to emphasize
    the alignment of regions relevant to the task of cardiac segmentation. Several
    studies enhance disentanglement learning by employing various approaches. For
    example, Xie et al. [[261](#bib.bib261)] utilize a zero loss to ensure that the
    domain-specific encoder only captures information from its corresponding domain.
    Similarly, Yang et al. [[262](#bib.bib262)] implement a coarse-to-fine prototype
    alignment process before feature disentanglement to enhance the separation of
    features.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Sun等人[[260](#bib.bib260)]结合了注意力机制和解耦来进一步缓解领域间的差异。他们特别采用了初步对齐阶段，以解决MRI和CT图像之间的亮度变化等问题。随后，他们引入了一种改进的解耦方法，利用Hilbert-Schmidt独立性准则来促进内容和风格属性之间的独立性和互补特征。最后，他们整合了一种注意力偏差机制，以强调与心脏分割任务相关区域的对齐。若干研究通过采用不同方法增强了解耦学习。例如，Xie等人[[261](#bib.bib261)]利用零损失来确保领域特定的编码器仅捕获其对应领域的信息。类似地，Yang等人[[262](#bib.bib262)]在特征解耦之前实施了粗到细的原型对齐过程，以增强特征的分离。
- en: 4.3.4 Pseudo-labeling approach
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 伪标签方法
- en: Pseudo-labeling is a widely used strategy in UDA to make use of unlabeled data
    in the target domain. This method involves generating pseudo-labels to unlabeled
    data in the target domain using a model trained on labeled data from the source
    domain. Nevertheless, these pseudo-labels can be inaccurate due to the domain
    gap, leading to noise. Thus, a crucial aspect of pseudo-labeling is how various
    networks reduce the uncertainty and eliminate noise from the pseudo-labels to
    enhance their precision. To reduce the uncertainty of pseudo labels, Wu et al.
    [[263](#bib.bib263)] introduce an uncertainty-aware model that integrates Monte
    Carlo dropout layers into a U-Net architecture. Likewise, the Strudel approach
    [[264](#bib.bib264)] involves incorporating uncertainty details into the training
    process through an uncertainty-guided loss function. This aids in eliminating
    labels with low level of certainty. Some studies adopt a curriculum learning strategy,
    beginning with simpler instances to facilitate the model’s learning process and
    gradually introducing more complex cases over time [[265](#bib.bib265), [266](#bib.bib266)].
    When facing a situation where classes are imbalanced, pseudo-labels frequently
    demonstrate an uneven distribution because the model tends to have greater confidence
    in dominant or less complex classes. To address this, Mottaghi et al. [[267](#bib.bib267)]
    introduce a new strategy for pseudo-label selection. This involves using a subset
    of pseudo-labels based on the reciprocal of class frequency, favoring less common
    or challenging classes. This technique effectively addresses label distribution
    imbalance, boosting the surgical activity recognition model’s reliability and
    performance.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 伪标签是UDA中广泛使用的策略，用于利用目标领域中的未标记数据。该方法涉及使用在源领域的标记数据上训练的模型，为目标领域中的未标记数据生成伪标签。然而，由于领域差异，这些伪标签可能不准确，从而导致噪声。因此，伪标签的关键在于各种网络如何减少不确定性并消除伪标签中的噪声，以提高其精度。为了减少伪标签的不确定性，吴等人[[263](#bib.bib263)]引入了一种不确定性感知模型，该模型将Monte
    Carlo dropout层集成到U-Net架构中。同样，Strudel方法[[264](#bib.bib264)]通过不确定性引导的损失函数将不确定性细节融入训练过程中。这有助于消除不确定性较低的标签。一些研究采用了课程学习策略，从简单实例开始，以促进模型的学习过程，并逐步引入更复杂的案例[[265](#bib.bib265),
    [266](#bib.bib266)]。面对类别不平衡的情况时，伪标签通常表现出分布不均，因为模型往往对主导或较简单的类别更有信心。为了解决这个问题，Mottaghi等人[[267](#bib.bib267)]提出了一种新的伪标签选择策略。这种策略通过基于类别频率的倒数来使用伪标签的子集，偏向于较少见或具有挑战性的类别。这种技术有效解决了标签分布不平衡的问题，提高了手术活动识别模型的可靠性和性能。
- en: 4.3.5 Self-supervision
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 自我监督
- en: Certain studies address UDA by employing the self-supervision strategy. In this
    approach, alignment is achieved by concurrently conducting auxiliary self-supervised
    tasks in both domains. Each self-supervised task aims to bring the domains closer
    by focusing on relevant directions. Successfully training these self-supervised
    tasks alongside the primary task in the source domain has proven effective in
    generalizing to the unlabeled target domain [[268](#bib.bib268)]. Various auxiliary
    self-supervised tasks are available, but not all are suitable for UDA. Consequently,
    the primary challenge in the self-supervision method is to identify an appropriate
    self-supervised task that enables the model to learn valuable representations
    from the data and promote alignment between the domains. Koohbanani et al. [[269](#bib.bib269)]
    present a method called Self-Path for histology image classification. In this
    approach, they propose three innovative domain-specific self-supervision tasks.
    These tasks involve predicting the magnification level, solving a magnification
    jigsaw puzzle, and predicting the Hematoxylin channel. These tasks are strategically
    designed to utilize the contextual, multi-resolution, and semantic features inherent
    in histopathology images. The Self-rule to multi-adapt (SRMA) technique [[270](#bib.bib270)]
    is applied in the detection of cancer tissue. This method uses a limited set of
    labeled images from the source domain and integrates structural details from both
    domains by identifying visual similarities using self-supervision within each
    domain and across domains. Additional self-supervised tasks include the jigsaw
    puzzle auxiliary task, where the spatial correlation in an image is learned by
    reconstructing a CT scan from shuffled patches [[271](#bib.bib271)], and an auxiliary
    task focused on edge generation [[272](#bib.bib272)].
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究通过采用自监督策略来解决UDA问题。在这种方法中，通过同时进行两个领域的辅助自监督任务来实现对齐。每个自监督任务的目标是通过关注相关方向使领域更接近。成功训练这些自监督任务与源领域中的主要任务一起进行，已被证明对在未标记目标领域中的泛化有效[[268](#bib.bib268)]。虽然有各种辅助自监督任务，但并非所有任务都适合UDA。因此，自监督方法的主要挑战在于识别一个合适的自监督任务，使模型能够从数据中学习有价值的表示，并促进领域之间的对齐。Koohbanani等人[[269](#bib.bib269)]提出了一种称为Self-Path的组织学图像分类方法。在这种方法中，他们提出了三个创新的领域特定自监督任务。这些任务涉及预测放大倍数、解决放大拼图以及预测苏木精通道。这些任务经过战略性设计，以利用组织病理图像中固有的上下文、多分辨率和语义特征。Self-rule到多适配（SRMA）技术[[270](#bib.bib270)]应用于癌症组织的检测。该方法使用源领域中有限的标记图像，并通过使用自监督在各领域之间以及跨领域识别视觉相似性来整合两个领域的结构细节。其他自监督任务包括拼图辅助任务，其中通过从打乱的补丁中重建CT扫描来学习图像中的空间相关性[[271](#bib.bib271)]，以及一个关注边缘生成的辅助任务[[272](#bib.bib272)]。
- en: 4.3.6 Hybrid methods
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.6 混合方法
- en: 'Various works employ feature alignment and image translation methods together
    to enhance the performance of UDA. These are called hybrid methods. Hybrid methods
    encompass a two-step procedure: initially, image transformation modifies the source
    images to align them with the target domain’s appearance, and subsequently, feature
    adaptation is applied to narrow the remaining disparity between the generated
    target-like images and the real target images [[273](#bib.bib273)]. The benefit
    of employing hybrid techniques lies in their ability to retain pixel-level, feature-level,
    and semantic information. The Cycle-Consistent Adversarial Domain Adaptation technique
    (CyCADA), introduced by Hoffman et al. [[274](#bib.bib274)], is a hybrid learning
    method developed for natural images. It consists of two stages: image adaptation
    and feature adaptation, both of which undergo sequential training without direct
    interactions. CyCADA has found extensive application as a fundamental model in
    different medical imaging scenarios [[275](#bib.bib275), [276](#bib.bib276)].'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 各种研究将特征对齐和图像翻译方法结合使用，以提升UDA的性能。这些方法称为混合方法。混合方法包括两个步骤：首先，图像变换将源图像修改为与目标领域的外观对齐，其次，应用特征适配来缩小生成的目标图像与实际目标图像之间的剩余差异[[273](#bib.bib273)]。使用混合技术的好处在于能够保留像素级、特征级和语义信息。Hoffman等人提出的循环一致对抗领域适配技术（CyCADA）[[274](#bib.bib274)]是一种为自然图像开发的混合学习方法。它包括两个阶段：图像适配和特征适配，这两个阶段进行顺序训练而没有直接交互。CyCADA已在不同的医学成像场景中广泛应用[[275](#bib.bib275),
    [276](#bib.bib276)]。
- en: In contrast to CyCADA, Chen et al. [[11](#bib.bib11), [273](#bib.bib273)] introduce
    an alternative technique known as Synergistic Image and Feature Alignment (SIFA),
    which facilitates concurrent image and feature translation. In particular, the
    feature encoder is shared, enabling it to simultaneously alter the image’s appearance
    and extract domain-invariant representations for the segmentation task. To enhance
    domain adaptation accuracy further, certain research studies incorporate attention
    mechanisms alongside image and feature alignment techniques [[277](#bib.bib277),
    [278](#bib.bib278)]. Chen et al. [[278](#bib.bib278)] employ the same framework
    as SIFA. However, in their approach, the alignment of the feature space is directed
    by the dual adversarial attention mechanism. This mechanism concentrates on specific
    regions identified by the spatial and class attention mechanisms rather than treating
    all semantic feature components uniformly. Label-efficient UDA (LE-UDA) [[279](#bib.bib279)]
    tackles both domain shift and source label scarcity. The approach utilizes a hybrid
    method to handle domain shift, while for source label scarcity, it incorporates
    two teacher models. These models leverage information within domains as well as
    across domains from diverse datasets. In a recent study, Li et al. [[280](#bib.bib280)]
    introduce a self-training adversarial learning framework for retinal OCT fluid
    segmentation tasks that utilize a hybrid approach.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于CyCADA，Chen等人[[11](#bib.bib11), [273](#bib.bib273)]引入了一种称为协同图像和特征对齐（SIFA）的替代技术，便于同时进行图像和特征的转换。特别地，共享特征编码器使其能够同时改变图像的外观，并提取用于分割任务的领域不变表示。为了进一步提高领域适应精度，某些研究结合了注意力机制与图像和特征对齐技术[[277](#bib.bib277),
    [278](#bib.bib278)]。Chen等人[[278](#bib.bib278)]使用与SIFA相同的框架。然而，在他们的方法中，特征空间的对齐由双重对抗注意力机制指导。该机制集中于由空间和类别注意力机制识别的特定区域，而不是均匀地处理所有语义特征组件。标签高效UDA（LE-UDA）[[279](#bib.bib279)]解决了领域偏移和源标签稀缺问题。该方法利用混合方法来处理领域偏移，而对于源标签稀缺，它结合了两个教师模型。这些模型利用来自不同数据集的领域内部和跨领域的信息。在一项最新研究中，Li等人[[280](#bib.bib280)]引入了一种用于视网膜OCT液体分割任务的自我训练对抗学习框架，采用了混合方法。
- en: 'Table 4: Overview of recent methods in *Incomplete Supervision* category.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：*不完全监督*类别下近期方法概览。
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 任务 | 算法设计 | 数据集 | 结果 |'
- en: '| [[181](#bib.bib181)] | Gland segmentation | Deep adversarial network | 2015
    MICCAI Gland Challenge dataset | F1: 0.916; ObjectDice: 0.903 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| [[181](#bib.bib181)] | 腺体分割 | 深度对抗网络 | 2015 MICCAI腺体挑战数据集 | F1: 0.916; 目标Dice:
    0.903 |'
- en: '| [[179](#bib.bib179)] | Polyp segmentation | Adversarial learning | Kvasir-SEG;
    CVC-Clinic DB | Kvasir-SEG: Dice: 15% label: 0.7676, 30% label: 0.8095; CVC-Clinic
    DB: Dice: 15% label: 0.8218, 30% label: 0.8929 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| [[179](#bib.bib179)] | 息肉分割 | 对抗学习 | Kvasir-SEG; CVC-Clinic DB | Kvasir-SEG:
    Dice: 15% 标签: 0.7676, 30% 标签: 0.8095; CVC-Clinic DB: Dice: 15% 标签: 0.8218, 30%
    标签: 0.8929 |'
- en: '| [[184](#bib.bib184)] | Heart; Prostate; Pancreas segmentation | Semi-supervised
    GAN | ACDC; DECATHLON | ACDC: DSC (Dice coefficient): 0.834; DECATHLON: DSC: 0.529
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| [[184](#bib.bib184)] | 心脏; 前列腺; 胰腺分割 | 半监督GAN | ACDC; DECATHLON | ACDC: DSC（Dice系数）:
    0.834; DECATHLON: DSC: 0.529 |'
- en: '| [[183](#bib.bib183)] | Fundus segmentation | Leaking GAN | DRIVE, STARE,
    CHASE DB1 | DRIVE: Acc: 95.74 Sp: 86.72 Se: 97.50; STARE: Acc: 95.65 Sp: 91.86
    Se: 91.02; CHASE DB1: Acc: 96.83 Sp:92.21 Se:94.72 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| [[183](#bib.bib183)] | 视网膜分割 | 漏水GAN | DRIVE, STARE, CHASE DB1 | DRIVE: 准确率:
    95.74 特异度: 86.72 敏感度: 97.50; STARE: 准确率: 95.65 特异度: 91.86 敏感度: 91.02; CHASE DB1:
    准确率: 96.83 特异度: 92.21 敏感度: 94.72 |'
- en: '| [[178](#bib.bib178)] | Optic cup segmentation | Teacher-student VAE | DRD
    | DSC: 0.80 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| [[178](#bib.bib178)] | 视神经杯分割 | 教师-学生VAE | DRD | DSC: 0.80 |'
- en: '| [[186](#bib.bib186)] | Kidney; Heart; Liver | Generative Bayesian Deep Learning
    | KiTS; ASG; DECATHLON | DSC: KiTS: 0.898; ASG: 0.884; DECATHLON: 0.935 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| [[186](#bib.bib186)] | 肾脏; 心脏; 肝脏 | 生成贝叶斯深度学习 | KiTS; ASG; DECATHLON | DSC:
    KiTS: 0.898; ASG: 0.884; DECATHLON: 0.935 |'
- en: '| [[167](#bib.bib167)] | Skin lesion segmentation | $\Pi$-model | ISIC 2017
    | DSC: 0.874; Acc: 0.943 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| [[167](#bib.bib167)] | 皮肤病变分割 | $\Pi$-模型 | ISIC 2017 | DSC: 0.874; 准确率: 0.943
    |'
- en: '| [[158](#bib.bib158)] | Chest X-ray segmentation | Elastic deformations perturbations
    for CL | JSRT dataset | MeanIOU: 5 labeled samples: 85.0 $\pm$ 2.8; 10 labeled
    samples: 87.9 $\pm$ 0.8 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| [[158](#bib.bib158)] | 胸部X光分割 | 弹性变形扰动用于CL | JSRT数据集 | MeanIOU: 5个标记样本: 85.0
    $\pm$ 2.8; 10个标记样本: 87.9 $\pm$ 0.8 |'
- en: '| [[169](#bib.bib169)] | Breast | Uncertainty-aware Temporal Ensembling | Private
    Dataset: 170 Volumes; ISIC 2017 | Private Dataset: DSC: 0.7287; ISIC 2017: DSC:
    0.8178 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| [[169](#bib.bib169)] | 乳腺 | 不确定性意识时间集成 | 私有数据集: 170 个体积; ISIC 2017 | 私有数据集:
    DSC: 0.7287; ISIC 2017: DSC: 0.8178 |'
- en: '| [[176](#bib.bib176)] | Brain Tumor and Left Atrial Segmentation | Dual Uncertainty-Guided
    Mixing Consistency | BraTS2020; LA2018 | BraTS: Dice: 85.94 %; LA2018: Dice: 89.28
    % |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| [[176](#bib.bib176)] | 脑肿瘤和左心房分割 | 双重不确定性引导混合一致性 | BraTS2020; LA2018 | BraTS:
    骰子系数: 85.94 %; LA2018: 骰子系数: 89.28 % |'
- en: '| [[190](#bib.bib190)] | Heart | CRF-based Self-training | Private Dataset:
    8050 | Images DSC: 0.920 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| [[190](#bib.bib190)] | 心脏 | 基于 CRF 的自训练 | 私有数据集: 8050 | 图像 DSC: 0.920 |'
- en: '| [[193](#bib.bib193)] | Thorax Disease and Skin lesion classification | Anti-Curriculum
    Pseudo-labeling | Chest X-Ray14; ISIC 2018 | Chest X-Ray14: AUC: 81.77; ISIC 2018:
    AUC: 94.36 Sensitivity: 72.14 F1: 62.23 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| [[193](#bib.bib193)] | 胸部疾病和皮肤病变分类 | 反课程伪标签 | 胸部 X 光14; ISIC 2018 | 胸部 X
    光14: AUC: 81.77; ISIC 2018: AUC: 94.36 灵敏度: 72.14 F1: 62.23 |'
- en: '| [[197](#bib.bib197)] | Multi-organ abdominal segmentation | Co-training using
    different modalities | BTCV; CHAOS | BTCV: Mean Dice score: 10 % labels: 81.3;
    CHAOS: Mean Dice score: 10 % labels: 82.1 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| [[197](#bib.bib197)] | 多脏器腹部分割 | 使用不同模态的共同训练 | BTCV; CHAOS | BTCV: 平均骰子系数:
    10 % 标签: 81.3; CHAOS: 平均骰子系数: 10 % 标签: 82.1 |'
- en: '| [[198](#bib.bib198)] | Cardiac segmentation | Co-training using different
    network architectures | ACDC dataset | Mean DSC: 0.848 (0.085); Mean HD95: 7.6
    (10.8) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| [[198](#bib.bib198)] | 心脏分割 | 使用不同网络架构的共同训练 | ACDC 数据集 | 平均 DSC: 0.848 (0.085);
    平均 HD95: 7.6 (10.8) |'
- en: '| [[200](#bib.bib200)] | Cardiac segmentation | Co-training using different
    transformations | MM-WHS dataset | 10% labeled data: Dice: 0.743, mIOU: 0.601,
    PixAcc: 0.973; 20% labeled data: Dice: 0.828, mIOU: 0.714, PixAcc: 0.979; 40%
    labeled data: Dice: 0.849, mIOU: 0.746, PixAcc: 0.985; |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| [[200](#bib.bib200)] | 心脏分割 | 使用不同变换的共同训练 | MM-WHS 数据集 | 10% 标注数据: 骰子系数:
    0.743, mIOU: 0.601, 像素准确率: 0.973; 20% 标注数据: 骰子系数: 0.828, mIOU: 0.714, 像素准确率: 0.979;
    40% 标注数据: 骰子系数: 0.849, mIOU: 0.746, 像素准确率: 0.985; |'
- en: '| [[202](#bib.bib202)] | Breast; Retina | Self-training + Virtual Adversarial
    Training | RetinalOCT; Private Dataset: 39,904 Images | Acc: 0.9513; Macro-R (Macro-Recall):
    0.9330 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| [[202](#bib.bib202)] | 乳腺; 视网膜 | 自训练 + 虚拟对抗训练 | RetinalOCT; 私有数据集: 39,904
    张图像 | 准确率: 0.9513; 宏召回率 (Macro-R): 0.9330 |'
- en: '| [[203](#bib.bib203)] | Lung detection | MixMatch + Focal Loss | LUNA; NLST
    | LUNA: CPM: 0.872 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| [[203](#bib.bib203)] | 肺部检测 | MixMatch + 焦点损失 | LUNA; NLST | LUNA: CPM: 0.872
    |'
- en: '| [[204](#bib.bib204)] | Metastatic epidural spinal cord classification | Consistency
    Regularization + Pseudo-labeling + Active Learning | Private Dataset: 7,295 Images;
    | Acc: 0.9582; Macro-P (Macro-Precision): 0.8609 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| [[204](#bib.bib204)] | 转移性硬膜外脊髓分类 | 一致性正则化 + 伪标签 + 主动学习 | 私有数据集: 7,295 张图像;
    | 准确率: 0.9582; 宏精度 (Macro-P): 0.8609 |'
- en: '| [[205](#bib.bib205)] | Cardiac and Prostate segmentation | Self-training
    + Contrastive loss | ACDC; Prostate; MMWHS dataset | ACDC: DSC: 0.881; Prostate:
    DSC: 0.693; MMWHS: DSC: 0.803 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| [[205](#bib.bib205)] | 心脏和前列腺分割 | 自训练 + 对比损失 | ACDC; 前列腺; MMWHS 数据集 | ACDC:
    DSC: 0.881; 前列腺: DSC: 0.693; MMWHS: DSC: 0.803 |'
- en: '| [[206](#bib.bib206)] | Cardiac, Tumour and histopathology images segmentation
    | Self-training + Contrastive loss | ACDC; KiTS19; CRAG | ACDC: DSC: 0.891; KiTS19:
    DSC: 0.919; CRAG: 0.882 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| [[206](#bib.bib206)] | 心脏、肿瘤和组织病理图像分割 | 自训练 + 对比损失 | ACDC; KiTS19; CRAG |
    ACDC: DSC: 0.891; KiTS19: DSC: 0.919; CRAG: 0.882 |'
- en: '| [[215](#bib.bib215)] | Colon | Traditional Data Augmentation Entropy + Diversity
    | Private Dataset: 6 colonoscopy videos 38 polyp videos + 121 CTPA datasets |
    Classification: 4 % input: AUC: 0.9204; Detection: 2.04 % input: AUC: 0.9615 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| [[215](#bib.bib215)] | 结肠 | 传统数据增强熵 + 多样性 | 私有数据集: 6 个结肠镜视频 38 个息肉视频 + 121
    个 CTPA 数据集 | 分类: 4 % 输入: AUC: 0.9204; 检测: 2.04 % 输入: AUC: 0.9615 |'
- en: '| [[214](#bib.bib214)] | Lung Classification | Loss Prediction Network | CC-CCII
    Dataset | 42 % Chest X-Ray input: Acc: 86.6% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| [[214](#bib.bib214)] | 肺部分类 | 损失预测网络 | CC-CCII 数据集 | 42 % 胸部 X 光输入: 准确率:
    86.6% |'
- en: '| [[220](#bib.bib220)] | Skin disease classification | BALD + KL-divergence
    | ISIC 2016 | 22 % image input: AUC: 0.75 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| [[220](#bib.bib220)] | 皮肤疾病分类 | BALD + KL 散度 | ISIC 2016 | 22 % 图像输入: AUC:
    0.75 |'
- en: '| [[221](#bib.bib221)] | Chest | Bayesian Neural Network + cGAN Data Augmentation
    | JSRT Database; ChestX-ray8 | Classification: 35 % input: AUC: 0.953; Segmentation:
    35 % input: DSC: 0.910 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| [[221](#bib.bib221)] | 胸部 | 贝叶斯神经网络 + cGAN 数据增强 | JSRT 数据库; ChestX-ray8 |
    分类: 35 % 输入: AUC: 0.953; 分割: 35 % 输入: DSC: 0.910 |'
- en: '| [[225](#bib.bib225)] | Gland; Lymph | Cosine Similarity + Bootstrapping +
    FCN | GlaS 2015; Private Dataset: 80 US images | MICCAI 2015: 50 % input: F1:
    0.921; Private Dataset: 50 % input: F1: 0.871 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| [[225](#bib.bib225)] | 腺体；淋巴 | 余弦相似性 + 自助法 + FCN | GlaS 2015；私人数据集: 80 张
    US 图像 | MICCAI 2015: 50% 输入: F1: 0.921；私人数据集: 50% 输入: F1: 0.871 |'
- en: '| [[226](#bib.bib226)] | Shoulder | BNN + MMD Divergence | Private Dataset:
    36 Volume of MRIs | 48 % MRI input: DSC $\approx$ 0.85 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| [[226](#bib.bib226)] | 肩部 | BNN + MMD 散度 | 私人数据集: 36 个 MRI 体积 | 48% MRI 输入:
    DSC $\approx$ 0.85 |'
- en: '| [[233](#bib.bib233)] | Major depressive order identification | Feature (MMD)
    | REST-meta-MDD Consortium | Site/Hospital - 20 → Site/Hospital - 1: ACC (%):
    59.73 $\pm$ 1.63; AUC (%): 62.50 $\pm$ 2.50; SEN (%): 69.46 $\pm$ 6.43; SPE (%):
    50.00 $\pm$ 9.63; PRE (%): 58.49 $\pm$ 2.58 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| [[233](#bib.bib233)] | 重度抑郁症识别 | 特征 (MMD) | REST-meta-MDD 联盟 | 站点/医院 - 20
    → 站点/医院 - 1: ACC (%): 59.73 $\pm$ 1.63；AUC (%): 62.50 $\pm$ 2.50；SEN (%): 69.46
    $\pm$ 6.43；SPE (%): 50.00 $\pm$ 9.63；PRE (%): 58.49 $\pm$ 2.58 |'
- en: '| [[234](#bib.bib234)] | 3D Medical Image Synthesis | KL divergence | BraTS
    2019 dataset (2 subsets are used CBICA and TCIA) | CBICA → TCIA: Dice: 0.773;
    TCIA → CBICA: Dice: 0.874 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| [[234](#bib.bib234)] | 3D 医学图像合成 | KL 散度 | BraTS 2019 数据集（使用了 CBICA 和 TCIA
    的 2 个子集） | CBICA → TCIA: Dice: 0.773；TCIA → CBICA: Dice: 0.874 |'
- en: '| [[236](#bib.bib236)] | Segmentation of retinal fluids in 3D OCT images |
    Contrastive and supervised loss | Two large OCT datasets (Spectralis and Cirrus)
    | Spectralis → Cirrus: Dice: 62.33; UVD: 10.88 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| [[236](#bib.bib236)] | 3D OCT 图像中的视网膜液体分割 | 对比和监督损失 | 两个大型 OCT 数据集（Spectralis
    和 Cirrus） | Spectralis → Cirrus: Dice: 62.33；UVD: 10.88 |'
- en: '| continued on the next page |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 续下页 |'
- en: 'Table 4: Overview of recent methods in *Incomplete Supervision* category (continued).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: *不完全监督* 类别的近期方法概述（续）。'
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 任务 | 算法设计 | 数据集 | 结果 |'
- en: '| [[240](#bib.bib240)] | Histopathology cancer classification | Adversarial
    learning + Entropy loss + Focal loss | Private cross-modality dataset | WSI →
    Microscopy images(MSIs): Accuracy: 90.48; Precision: 90.67; Recall: 90.35; F1-measure:
    90.50 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| [[240](#bib.bib240)] | 组织病理学癌症分类 | 对抗学习 + 熵损失 + 焦点损失 | 私人跨模态数据集 | WSI → 显微图像
    (MSIs): 准确率: 90.48；精确度: 90.67；召回率: 90.35；F1 评分: 90.50 |'
- en: '| [[241](#bib.bib241)] | Automated pneumonia diagnosis | Conditional domain
    adversarial network + Contrastive loss | RSNA dataset (Stage I); Child X-ray dataset
    | RSNA dataset → Child X-ray: AUC score: 90.57; RSNA + COVID → TTSH dataset: weighted
    AUC: 88.27 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| [[241](#bib.bib241)] | 自动化肺炎诊断 | 条件域对抗网络 + 对比损失 | RSNA 数据集（第一阶段）；儿童 X 光数据集
    | RSNA 数据集 → 儿童 X 光: AUC 分数: 90.57；RSNA + COVID → TTSH 数据集: 加权 AUC: 88.27 |'
- en: '| [[245](#bib.bib245)] | Bronchoscopic Depth Estimation | Adversarial learning
    | Synthetic dataset and human pulmonary dataset | Mean abs. rel. diff: 0.379;
    RMSE: 7.532; Accuracy: 0.856 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| [[245](#bib.bib245)] | 支气管镜深度估计 | 对抗学习 | 合成数据集和人类肺数据集 | 平均绝对相对差异: 0.379；RMSE:
    7.532；准确率: 0.856 |'
- en: '| [[250](#bib.bib250)] | Lung Cancer Segmentation | CycleGan + Tumor-aware
    loss | The Cancer Imaging Archive (TCIA) CT dataset and Private MRI dataset |
    CT → MRI: Validation set (Unsupervised): DSC: 0.62 $\pm$ 0.26 HD95: 7.47 $\pm$
    4.66; Test set (Unsupervised): DSC: 0.74 $\pm$ 0.15 HD95: 8.88 $\pm$ 4.8 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| [[250](#bib.bib250)] | 肺癌分割 | CycleGan + 肿瘤感知损失 | 癌症影像档案 (TCIA) CT 数据集和私人
    MRI 数据集 | CT → MRI: 验证集（无监督）：DSC: 0.62 $\pm$ 0.26 HD95: 7.47 $\pm$ 4.66；测试集（无监督）：DSC:
    0.74 $\pm$ 0.15 HD95: 8.88 $\pm$ 4.8 |'
- en: '| [[252](#bib.bib252)] | Brain tumor and cardiac segmentation | Dual CycleGan
    + Self-attentive spatial adaptive normalization | MM-WHS challenge; BraTS | MM-WHS:
    MRI → CT: Mean Dice: 0.78 $\pm$ 0.10, Mean ASSD: 4.9 $\pm$ 1.5 CT → MR: Mean Dice:
    0.70 $\pm$ 0.11, Mean ASSD: 9.5 $\pm$ 3.2; BraTS: MRI-T2 → MRI-T1: 0.50 $\pm$
    0.06 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| [[252](#bib.bib252)] | 脑肿瘤和心脏分割 | 双 CycleGan + 自注意力空间自适应归一化 | MM-WHS 挑战；BraTS
    | MM-WHS: MRI → CT: 平均 Dice: 0.78 $\pm$ 0.10，平均 ASSD: 4.9 $\pm$ 1.5 CT → MR: 平均
    Dice: 0.70 $\pm$ 0.11，平均 ASSD: 9.5 $\pm$ 3.2；BraTS: MRI-T2 → MRI-T1: 0.50 $\pm$
    0.06 |'
- en: '| [[258](#bib.bib258)] | Liver segmentation | Disentangled Representation |
    LiTS challenge 2017 dataset (CT slices) and multi-phasic (MRI slices) | CT → MR:
    Dice: 0.81 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| [[258](#bib.bib258)] | 肝脏分割 | 解耦表示 | LiTS 挑战 2017 数据集（CT 切片）和多相（MRI 切片） |
    CT → MR: Dice: 0.81 |'
- en: '| [[259](#bib.bib259)] | Cardiac (LV and MYO) segmentation | Disentangled Representation
    + Semantic consistency loss | MS-CMRSeg; MM-WHS challenge | MS-CMRSeg: bSSFP CMR
    → LGE CMR images: DSC: 79.08, ASSD: 1.68; MM-WHS: CT → MRI: DSC: 84.51, ASSD:
    1.00, MRI → CT: DSC: 84.77 ASSD: 0.98 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| [[259](#bib.bib259)] | 心脏（左心室和心肌）分割 | 解耦表示 + 语义一致性损失 | MS-CMRSeg；MM-WHS 挑战
    | MS-CMRSeg: bSSFP CMR → LGE CMR 图像: DSC: 79.08，ASSD: 1.68；MM-WHS: CT → MRI: DSC:
    84.51，ASSD: 1.00，MRI → CT: DSC: 84.77 ASSD: 0.98 |'
- en: '| [[260](#bib.bib260)] | Cardiac segmentation | Disentangled Representation
    + HSIC + Attention bias | MMWHS challenge 2017 dataset | MRI → CT: Dice: 80.2,
    ASD: 5.1; CT → MRI: Dice: 66.3, ASD: 4.9 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| [[260](#bib.bib260)] | 心脏分割 | 解耦表示 + HSIC + 注意力偏置 | MMWHS挑战赛2017数据集 | MRI
    → CT: Dice: 80.2，ASD: 5.1；CT → MRI: Dice: 66.3，ASD: 4.9 |'
- en: '| [[264](#bib.bib264)] | White Matter Hyperintensity Segmentation | Self training
    + Uncertainty-guided loss | WMH; ADNI-2 | WMH → ADNI-2: DSC: 0.69 $\pm$ 0.18,
    H95: 11.2 $\pm$ 14.5 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| [[264](#bib.bib264)] | 白质高信号分割 | 自我训练 + 不确定性引导的损失 | WMH；ADNI-2 | WMH → ADNI-2:
    DSC: 0.69 $\pm$ 0.18，H95: 11.2 $\pm$ 14.5 |'
- en: '| [[265](#bib.bib265)] | Epithelial-stroma (ES) classi- fication | Curriculum
    learning | Netherland Cancer Institute’s (NKI) dataset, Vancouver General Hospital’s
    (VGH) and IHC dataset | Accuracy: VGH → NKI: 91.50, IHC → NKI: 82.51, NKI → VGH:
    92.62, IHC → VGH: 80.49, VGH → IHC: 88.15, NKI → IHC: 81.90 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| [[265](#bib.bib265)] | 上皮-基质（ES）分类 | 课程学习 | 荷兰癌症研究所（NKI）数据集，温哥华总医院（VGH）和IHC数据集
    | 准确率: VGH → NKI: 91.50，IHC → NKI: 82.51，NKI → VGH: 92.62，IHC → VGH: 80.49，VGH
    → IHC: 88.15，NKI → IHC: 81.90 |'
- en: '| [[267](#bib.bib267)] | Pseudo labeling | Surgical activity recognition models
    across operating rooms | Dataset of full-length surgery videos from two robotic
    ORs (OR1 and OR2) | OR1 → OR2: Accuracy: 70.76 mAP: 83.71, OR2 → OR1: Accuracy:
    73.53, mAP: 89.96 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| [[267](#bib.bib267)] | 伪标签 | 手术活动识别模型在不同手术室之间的应用 | 来自两个机器人手术室（OR1 和 OR2）的完整手术视频数据集
    | OR1 → OR2: 准确率: 70.76 mAP: 83.71，OR2 → OR1: 准确率: 73.53，mAP: 89.96 |'
- en: '| [[269](#bib.bib269)] | Classification of Pathology Images | Prediction of
    magnification level and Hematoxylin channel + Solving jigsaw puzzle | WSIs: Camelyon16
    and In house dataset(LNM-OSCC) | Camelyon16: AUC-ROC: 93.7 % LNM-OSCC: 97.4 %
    |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| [[269](#bib.bib269)] | 病理图像分类 | 放大倍数预测和苏木精通道 + 解决拼图 | WSIs: Camelyon16 和内部数据集（LNM-OSCC）
    | Camelyon16: AUC-ROC: 93.7 % LNM-OSCC: 97.4 % |'
- en: '| [[270](#bib.bib270)] | Colorectal tissue type classification; Multi-source
    patch classification | Intra-domain and cross-domain self-supervision | Kather-16
    [242], Kather-19 [243], Colorectal cancer tissue phenotyping dataset (CRC-TP)[244]
    and In-house dataset | Kather-19 → Kather-16: overall weighted F1 score: 87.7;
    Kather-19 + Kather-16 → CRC-TP: weighted F1 score: 83.6 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| [[270](#bib.bib270)] | 结直肠组织类型分类；多源补丁分类 | 内领域和跨领域自我监督 | Kather-16 [242]，Kather-19
    [243]，结直肠癌组织表型数据集（CRC-TP）[244] 和内部数据集 | Kather-19 → Kather-16: 整体加权F1分数: 87.7；Kather-19
    + Kather-16 → CRC-TP: 加权F1分数: 83.6 |'
- en: '| [[272](#bib.bib272)] | Cardiac segmentation | Edge generation task + Adversarial
    learning | MM-WHS challenge dataset (2017) | MRI $\rightarrow$ CT: Dice: 76.98,
    ASD: 4.6 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| [[272](#bib.bib272)] | 心脏分割 | 边缘生成任务 + 对抗学习 | MM-WHS挑战赛数据集（2017） | MRI $\rightarrow$
    CT: Dice: 76.98，ASD: 4.6 |'
- en: '| [[275](#bib.bib275)] | Cone-beam computed tomography (CBCT) segmentation
    | Image and feature alignment | Private Dataset: 90 patients | CT $\rightarrow$
    CBCT: DSC: 83.6 % |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| [[275](#bib.bib275)] | 锥束CT（CBCT）分割 | 图像和特征对齐 | 私有数据集：90名患者 | CT $\rightarrow$
    CBCT: DSC: 83.6 % |'
- en: '| [[11](#bib.bib11)] | Cardiac segmentation | Synergistic image and feature
    alignment (SIFA) | MM-WHS challenge dataset | MRI $\rightarrow$ CT: Dice: 73.0,
    ASD: 8.1 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| [[11](#bib.bib11)] | 心脏分割 | 协同图像和特征对齐（SIFA） | MM-WHS挑战赛数据集 | MRI $\rightarrow$
    CT: Dice: 73.0，ASD: 8.1 |'
- en: '| [[278](#bib.bib278)] | Skull segmentation and Cardiac segmentation | CycleGan
    + Feature space alignment is led by the dual adversarial attention mechanism |
    CQ500; ADNI; MM-WHS challenge | Skull: CT $\rightarrow$ MRI: DSC: 84.07 %, ASSD:
    1.18; Cardiac: MRI $\rightarrow$ CT: DSC: 76.7 %, ASSD: 5.1 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| [[278](#bib.bib278)] | 头骨分割和心脏分割 | CycleGan + 特征空间对齐，由双重对抗注意机制主导 | CQ500；ADNI；MM-WHS挑战赛
    | 头骨：CT $\rightarrow$ MRI: DSC: 84.07 %, ASSD: 1.18；心脏：MRI $\rightarrow$ CT: DSC:
    76.7 %, ASSD: 5.1 |'
- en: '| [[279](#bib.bib279)] | Multi-organ and Cardiac segmentation | CycleGan and
    feature alignment | MICCAI 2015 Multi-Atlas Abdomen Labeling(CT images), ISBI
    2019 CHAOS Challenge (MR images); MM-WHS | Cardiac: MRI $\rightarrow$ CT: Dice:
    70.8, ASD: 9.6; CT $\rightarrow$ MRI: Dice: 66.5, ASD: 4.0; Multi-organ: MRI $\rightarrow$
    CT: Dice: 82.8, ASD: 2.3; CT $\rightarrow$ MRI: Dice: 87.7, ASD: 1.0 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| [[279](#bib.bib279)] | 多脏器和心脏分割 | CycleGan 和特征对齐 | MICCAI 2015 多图谱腹部标注（CT图像），ISBI
    2019 CHAOS挑战（MR图像）；MM-WHS | 心脏：MRI $\rightarrow$ CT: Dice: 70.8，ASD: 9.6；CT $\rightarrow$
    MRI: Dice: 66.5，ASD: 4.0；多脏器：MRI $\rightarrow$ CT: Dice: 82.8，ASD: 2.3；CT $\rightarrow$
    MRI: Dice: 87.7，ASD: 1.0 |'
- en: 5 Inaccurate supervision
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 不准确的监督
- en: 'Inaccurate supervision refers to a scenario where the provided supervision
    information isn’t always entirely accurate, meaning that errors may be present
    in some of the label information. Such noisy labels [[281](#bib.bib281)] can originate
    from various sources, including human errors in the labeling process, inter-observer
    variability among medical experts, or reliance on non-experts or automated systems
    for data labeling (see Figure [16](#S5.F16 "Figure 16 ‣ 5 Inaccurate supervision
    ‣ Data efficient deep learning for medical image analysis: A survey")). Since
    noisy labels can significantly harm the generalization capabilities of deep neural
    networks, it is imperative to develop robust techniques to handle and mitigate
    the impact of noisy labels. This is particularly vital in the field of MIA, where
    precision and accuracy are critical for medical diagnosis and treatment. A summary
    of recent approaches for learning with inaccurate supervision is provided in Table [5](#S5.T5
    "Table 5 ‣ 5.3 Training procedures ‣ 5 Inaccurate supervision ‣ Data efficient
    deep learning for medical image analysis: A survey"). We categorize inaccurate
    supervision approaches into three broad groups: Robust loss Design, data re-weighting
    and training procedures.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '不准确的监督指的是提供的监督信息并不总是完全准确的，这意味着某些标签信息中可能存在错误。这种噪声标签 [[281](#bib.bib281)] 可能来自多种来源，包括标注过程中人为错误、医学专家之间的观察者变异，或依赖于非专家或自动化系统进行数据标注（见图 [16](#S5.F16
    "Figure 16 ‣ 5 Inaccurate supervision ‣ Data efficient deep learning for medical
    image analysis: A survey")）。由于噪声标签会显著影响深度神经网络的泛化能力，因此开发鲁棒的技术来处理和减轻噪声标签的影响是至关重要的。这在医学影像分析领域尤为重要，因为精度和准确性对医学诊断和治疗至关重要。有关不准确监督的最新方法的总结见表 [5](#S5.T5
    "Table 5 ‣ 5.3 Training procedures ‣ 5 Inaccurate supervision ‣ Data efficient
    deep learning for medical image analysis: A survey")。我们将不准确监督的方法分为三大类：鲁棒损失设计、数据重加权和训练程序。'
- en: '![Refer to caption](img/7728fd0a766114b0715785a3dfb49b35.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7728fd0a766114b0715785a3dfb49b35.png)'
- en: 'Figure 16: The major sources of label noise encompass variations among different
    observers, mistakes made by human annotators, and inaccuracies in computer-generated
    labels. The impact of label noise in medical datasets is expected to grow as larger
    datasets are curated for deep learning purposes (image courtesy of Karimi [[281](#bib.bib281)]).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：标签噪声的主要来源包括不同观察者之间的差异、人为标注者的错误以及计算机生成标签的误差。随着为深度学习目的策划的大型数据集的增加，医学数据集中的标签噪声影响预计将会增加（图片来源：Karimi
    [[281](#bib.bib281)]）。
- en: 5.1 Robust loss design
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 鲁棒损失设计
- en: Certain investigations modify the loss function as a strategy to mitigate the
    impact of noisy labels. Researchers have introduced novel loss functions like
    Mean Absolute Error (MAE) [[282](#bib.bib282)] and Generalized Cross Entropy [[283](#bib.bib283)]
    to tackle the issue of label noise in natural images. In addition, some works
    choose to adapt loss functions specifically for tasks in medical imaging [[284](#bib.bib284),
    [285](#bib.bib285), [286](#bib.bib286)]. For COVID-19 Pneumonia Lesion segmentation,
    Wang et al. [[285](#bib.bib285)] proposed an enhanced Dice loss that addresses
    noise-related challenges. This improved loss function is an extended version of
    the traditional Dice loss, tailored for segmentation tasks, and incorporates the
    Mean Absolute Error (MAE) loss to enhance its robustness against noisy data. Chen
    et al. [[286](#bib.bib286)] introduce a new and versatile loss function called
    Adaptive Cross Entropy (ACE), designed to handle noise in labels without requiring
    hyperparameter fine-tuning during training. They provide both theoretical and
    practical evaluations of the ACE loss and demonstrate its efficacy across various
    publicly available datasets. Previous segmentation methods that handle noisy labels
    have typically focused on preserving semantics in a pixel-wise manner, which involves
    actions like pixel-wise label correction. However, they often overlook the potential
    benefits of considering pairwise relationships between pixels [[287](#bib.bib287)].
    Notably, it has been observed that capturing these pairwise affinities can significantly
    decrease label noise. Building on this insight, Guo et al. [[287](#bib.bib287)]
    introduce a joint class-affinity segmentation model that takes into account both
    pixel-wise label correction and pairwise pixel relationships in order to reduce
    label noise. To further reduce the impact of label noise, they introduce a strategy
    called class-affinity loss correction (CALC), which includes class-level and affinity-level
    loss correction.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 某些研究通过修改损失函数来减轻噪声标签的影响。研究人员引入了诸如**均方绝对误差**（MAE）[[282](#bib.bib282)]和**广义交叉熵**[[283](#bib.bib283)]等新型损失函数，以解决自然图像中的标签噪声问题。此外，一些研究选择特别为医学成像任务调整损失函数[[284](#bib.bib284),
    [285](#bib.bib285), [286](#bib.bib286)]。对于COVID-19肺炎病灶分割，**Wang**等人[[285](#bib.bib285)]提出了一种增强的**Dice损失**，以应对噪声相关的挑战。这种改进的损失函数是传统Dice损失的扩展版本，专门针对分割任务，并结合了**均方绝对误差**（MAE）损失，以增强其对噪声数据的鲁棒性。**Chen**等人[[286](#bib.bib286)]介绍了一种新的多功能损失函数，称为**自适应交叉熵**（ACE），旨在处理标签噪声，而不需要在训练过程中进行超参数微调。他们提供了ACE损失的理论和实践评估，并展示了其在各种公开数据集上的有效性。以往处理噪声标签的分割方法通常专注于像素级语义保留，例如像素级标签修正。然而，它们往往忽略了考虑像素对之间关系的潜在好处[[287](#bib.bib287)]。值得注意的是，捕捉这些像素对的关联可以显著减少标签噪声。在此基础上，**Guo**等人[[287](#bib.bib287)]引入了一种联合类亲和分割模型，该模型考虑了像素级标签修正和像素对关系，以减少标签噪声。为了进一步减少标签噪声的影响，他们提出了一种称为**类亲和损失修正**（CALC）的策略，包括类级和亲和级损失修正。
- en: 5.2 Data re-weighting
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 数据重标定
- en: Broadly speaking, these methods aim at down-weighting those training samples
    that are more likely to have incorrect labels. In this context, Xue et al. [[4](#bib.bib4)]
    introduced an approach for classifying skin lesions with noisy labels. Their method
    involved a data re-weighting technique, effectively excluding data samples with
    significant loss values in each training batch. To predict pancreatic cancer regions
    in whole-slide images (WSIs), Le et al. [[288](#bib.bib288)] utilized a noisy
    label classification technique. This approach incorporates a limited set of clean
    training samples and dynamically assigns weights to training samples to address
    sample noise. These weights are assigned in real time to align the network loss
    with that of the clean samples. For multi-organ segmentation, Zhu et al. [[8](#bib.bib8)]
    introduced an approach known as *pick and learn*. In this method, a deep learning
    model is trained to identify incorrect labels and assign weights to each sample
    within a training batch. The goal is to reduce the impact of samples with inaccurate
    labels. Simultaneously, the primary segmentation model is trained in parallel,
    incorporating these weights into its loss function.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛来说，这些方法的目标是降低那些可能具有错误标签的训练样本的权重。在这种情况下，Xue 等人 [[4](#bib.bib4)] 提出了一个用于分类带噪声标签皮肤病变的方法。他们的方法涉及一种数据重新加权技术，有效地排除了每个训练批次中具有显著损失值的数据样本。为了预测全片图像（WSI）中的胰腺癌区域，Le
    等人 [[288](#bib.bib288)] 采用了带噪声标签分类技术。这种方法结合了一小部分干净的训练样本，并动态地为训练样本分配权重以解决样本噪声。这些权重实时分配，以使网络损失与干净样本的损失对齐。对于多脏器分割，Zhu
    等人 [[8](#bib.bib8)] 提出了一个被称为*pick and learn*的方法。在此方法中，一个深度学习模型被训练以识别错误标签，并为每个训练批次中的样本分配权重。目标是减少具有不准确标签的样本的影响。同时，主分割模型与这些权重一起进行并行训练，将这些权重纳入其损失函数中。
- en: Strategies involving resampling and reweighting at the pixel level are intended
    to focus the segmentation model on learning from reliable pixels. For example,
    Mirikharaji et al. [[289](#bib.bib289)] introduced a method for skin lesion segmentation
    that incorporates pixel-wise weighting. This approach learns weight maps that
    adapt spatially and adjusts the influence of each pixel using a meta-reweighting
    framework. The Tri-network approach by Zhang et al. [[290](#bib.bib290)] employs
    three cooperating networks and dynamically identifies informative samples based
    on the consensus among predictions generated by these distinct networks. Meanwhile,
    Wang et al. [[291](#bib.bib291)] employ meta-learning techniques to automatically
    estimate an importance map, allowing them to extract reliable information from
    crucial pixels.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及像素级重采样和重加权的策略旨在使分割模型专注于从可靠像素中学习。例如，Mirikharaji 等人 [[289](#bib.bib289)] 提出了一个用于皮肤病变分割的方法，该方法结合了像素级加权。这种方法学习空间适应的权重图，并使用元重加权框架调整每个像素的影响。Zhang
    等人 [[290](#bib.bib290)] 的三网络方法使用三个协作网络，并根据这些不同网络生成的预测之间的一致性动态识别信息样本。与此同时，Wang
    等人 [[291](#bib.bib291)] 使用元学习技术自动估计重要性图，从关键像素中提取可靠信息。
- en: 5.3 Training procedures
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 训练程序
- en: The methods in this category are very diverse. Several approaches in this category
    rely on Multi-network Learning, which frequently employs techniques like collaborative
    learning and co-training to train multiple networks simultaneously. Meanwhile,
    others follow the Multi-round Learning paradigm, which iteratively enhances the
    chosen set of clean examples without the need for maintaining additional DNNs.
    This improvement is achieved by repeating the training process in multiple rounds.
    Min et al. [[292](#bib.bib292)] adapted concepts from Malach and Shalev-Shwartz
    [[293](#bib.bib293)] to create label-noise-resistant techniques for medical image
    segmentation. They simultaneously trained two distinct models and exclusively
    updated these models using data samples where the predictions of the two models
    disagreed. Rather than solely relying on final layer predictions, Min et al. [[292](#bib.bib292)]
    incorporated attention modules at different network depths, allowing them to utilize
    gradient information from various feature maps to identify and reduce the influence
    of samples with incorrect labels. They demonstrated encouraging outcomes in MRI-based
    cardiac and glioma segmentation tasks. For medical image classification, Xue et
    al. [[294](#bib.bib294)] utilize a self-ensemble model along with a noisy label
    filter to effectively identify clean and noisy samples. Subsequently, they employ
    a collaborative training approach to train the clean samples, aiming to mitigate
    the impact of imperfect labels. Additionally, they introduce an innovative global
    and local representation learning scheme, which serves as an implicit regularization
    method for enabling the networks to make use of noisy samples in a self-supervised
    fashion. For COVID-19 pneumonia lesion segmentation, Yang et al. [[295](#bib.bib295)]
    propose a dual-branch network that learns from both accurate and noisy annotations
    separately. They introduce the Divergence-Aware Selective Training (DAST) strategy
    to distinguish between severely noisy and slightly noisy annotations. For severely
    noisy samples, they apply regularization through dual-branch consistency between
    predictions from the two branches. Additionally, they refine slightly noisy samples
    and incorporate them as supplementary data for the clean branch to prevent overfitting.
    Li et al. [[296](#bib.bib296)] focus on selecting training pixels with reliable
    annotations from pixels with uncertain network predictions. They introduce the
    online prototypical soft label correction (PSLC) method to estimate pseudo-labels
    for label-unreliable pixels. They then calibrate the total segmentation loss using
    the segmentation loss of label-reliable and label-unreliable pixels. For hepatic
    vessel segmentation, Xu et al. [[297](#bib.bib297), [298](#bib.bib298)] utilized
    a small set of accurately labeled data alongside a larger set of noisily labeled
    data. They employed confident learning with the help of a weight-averaged teacher
    model. This strategy involved progressively refining the noisy labels in the low-quality
    dataset through pixel-wise soft correction. Shi et al. [[299](#bib.bib299)] present
    a framework designed to address noisy labels by extracting valuable supervision
    information from both pixel-level and image-level sources. Specifically, they
    make explicit estimations of pixel-wise uncertainty, treating it as a measure
    of noise at the pixel level. They then propose a robust learning approach at the
    pixel level, utilizing both the original labels and pseudo-labels. Additionally,
    they present a complementary image-level robust learning method to incorporate
    more information alongside pixel-level learning.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 本类别中的方法非常多样化。这个类别中的几种方法依赖于多网络学习，它通常使用协作学习和共训练等技术来同时训练多个网络。与此同时，其他方法则遵循多轮学习范式，该方法通过在多个轮次中重复训练过程来逐步增强选择的干净示例集，而无需维护额外的深度神经网络（DNN）。Min
    等人 [[292](#bib.bib292)] 采用了 Malach 和 Shalev-Shwartz [[293](#bib.bib293)] 的概念，创建了抗标签噪声的医学图像分割技术。他们同时训练了两个不同的模型，并仅使用两个模型预测不一致的数据样本来更新这些模型。Min
    等人 [[292](#bib.bib292)] 通过在不同网络深度引入注意力模块，而不是仅依赖最终层预测，从而利用来自不同特征图的梯度信息来识别和减少标签错误样本的影响。他们在基于
    MRI 的心脏和胶质瘤分割任务中展示了令人鼓舞的结果。对于医学图像分类，Xue 等人 [[294](#bib.bib294)] 利用自集成模型和带噪声标签过滤器来有效识别干净样本和噪声样本。随后，他们采用协作训练方法对干净样本进行训练，以减轻不完美标签的影响。此外，他们引入了一种创新的全局和局部表征学习方案，作为一种隐式正则化方法，使网络能够以自监督的方式利用带噪声的样本。对于
    COVID-19 肺炎病灶分割，Yang 等人 [[295](#bib.bib295)] 提出了一个双分支网络，该网络分别从准确和带噪声的注释中学习。他们引入了分歧感知选择训练（DAST）策略，以区分严重噪声和轻微噪声的注释。对于严重噪声样本，他们通过两个分支的预测一致性进行正则化。此外，他们对轻微噪声样本进行精细化处理，并将其作为干净分支的补充数据，以防止过拟合。Li
    等人 [[296](#bib.bib296)] 重点选择具有可靠注释的训练像素，而不是网络预测不确定的像素。他们引入了在线原型软标签校正（PSLC）方法来估计标签不可靠像素的伪标签。然后，他们使用标签可靠和标签不可靠像素的分割损失来校准总分割损失。对于肝脏血管分割，Xu
    等人 [[297](#bib.bib297), [298](#bib.bib298)] 利用一小部分准确标注的数据和一大部分带噪声标注的数据。他们借助权重平均教师模型采用了自信学习。这一策略包括通过逐像素软校正来逐步精炼低质量数据集中带噪声的标签。Shi
    等人 [[299](#bib.bib299)] 提出了一个框架，旨在通过从像素级和图像级来源提取有价值的监督信息来解决带噪声标签的问题。具体来说，他们对像素级不确定性进行明确估计，将其视为像素级噪声的度量。然后，他们提出了一种在像素级的稳健学习方法，利用原始标签和伪标签。此外，他们还提出了一种补充的图像级稳健学习方法，以结合更多信息与像素级学习。
- en: 'Table 5: Overview of recent methods in *Inaccurate Supervision* category.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：*不准确监督*类别近期方法概览。
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 任务 | 算法设计 | 数据集 | 结果 |'
- en: '| [[8](#bib.bib8)] | Multi-organ segmentation | Loss Re-weighting | JSRT |
    25 % noise: Dice: 0.895; 50 % noise: Dice: 0.898; 75 % noise: Dice: 0.895 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| [[8](#bib.bib8)] | 多脏器分割 | 损失重标定 | JSRT | 25 % 噪声：Dice：0.895；50 % 噪声：Dice：0.898；75
    % 噪声：Dice：0.895 |'
- en: '| [[289](#bib.bib289)] | Skin Lesions segmentation | Example reweighting |
    ISIC 2017 | Unsupervised noise: Dice: 73.55 % |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| [[289](#bib.bib289)] | 皮肤病变分割 | 示例重标定 | ISIC 2017 | 无监督噪声：Dice：73.55 % |'
- en: '| [[4](#bib.bib4)] | Skin lesion classification | Sample reweighting + Online
    Uncertainty Sample Mining | ISIC 2017 | 5% noise: Acc: 84.5; 10%: Acc: 83.6; 20%:
    ;Acc: 80.7; 40%: Acc: 80.7; |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| [[4](#bib.bib4)] | 皮肤病变分类 | 样本重标定 + 在线不确定性样本挖掘 | ISIC 2017 | 5% 噪声：准确率：84.5；10%：准确率：83.6；20%：准确率：80.7；40%：准确率：80.7；
    |'
- en: '| [[290](#bib.bib290)] | Clinical stroke lesion and multi organ segmentation
    | Tri-teaching network | Private clinical stroke dataset; JSRT | Clinical stroke:
    Dice: 68.12 %; JSRT: Dice: 80.43 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| [[290](#bib.bib290)] | 临床中风病灶及多脏器分割 | 三重教学网络 | 私有临床中风数据集；JSRT | 临床中风：Dice：68.12
    %；JSRT：Dice：80.43 |'
- en: '| [[292](#bib.bib292)] | Cardiac and Brain tumour segmentation | Two-Stream
    Mutual Attention Network | HVSMR 2016; BRATS 2015 | HVSMR 2016: Myocardium: Dice:
    0.820 ADB: 0.824 HDD: 4.73, Blood Pool: Dice: 0.926 ADB: 0.957 HDD: 8.81; BRATS
    2015: Mean Dice: 0.792 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| [[292](#bib.bib292)] | 心脏和脑肿瘤分割 | 双流互注意网络 | HVSMR 2016；BRATS 2015 | HVSMR
    2016：心肌：Dice：0.820 ADB：0.824 HDD：4.73，血池：Dice：0.926 ADB：0.957 HDD：8.81；BRATS 2015：平均Dice：0.792
    |'
- en: '| [[297](#bib.bib297)] | Hepatic Vessel Segmentation | Mean-Teacher-assisted
    Confident Learning | 3DIRCADb; MSD8 (Used for training) | 3DIRCADb: Dice: 0.7245
    PRE: 0.7570 ASD: 1.1718 HD: 7.2111 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| [[297](#bib.bib297)] | 肝血管分割 | 平均教师辅助自信学习 | 3DIRCADb；MSD8（用于训练） | 3DIRCADb：Dice：0.7245
    PRE：0.7570 ASD：1.1718 HD：7.2111 |'
- en: '| [[299](#bib.bib299)] | Left Atrial(LA) and cervical cancer Segmentation |
    Pixel-wise and Image-level Noise Tolerant learning | Left Atrial(LA); Private
    dataset | LA: 25 % Noise : Dice(%): ASD: 1.60 50 % Noise: Dice(%): 89.04 ASD:
    1.92 75 % Noise: Dice(%): 76.25 ASD: 4.56; Private dataset: Dice(%): 75.31 ASD:
    1.76 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| [[299](#bib.bib299)] | 左心房（LA）和宫颈癌分割 | 像素级和图像级噪声容忍学习 | 左心房（LA）；私有数据集 | LA：25
    % 噪声：Dice(%)：ASD：1.60 50 % 噪声：Dice(%)：89.04 ASD：1.92 75 % 噪声：Dice(%)：76.25 ASD：4.56；私有数据集：Dice(%)：75.31
    ASD：1.76 |'
- en: '| [[287](#bib.bib287)] | Surgical instrument segmentation | Pixel-wise label
    correction and pairwise pixel relationships in order to reduce label noise. |
    Endovis18 | Average: Ellipse noise: Dice (%): 71.384 Jac (%): 58.452; Symmetric
    noise: Dice (%): 74.058 Jac (%): 62.667; Asymmetric noise: Dice (%): 74.410 Jac
    (%): 63.029 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| [[287](#bib.bib287)] | 外科器械分割 | 像素级标签修正和配对像素关系，以减少标签噪声。 | Endovis18 | 平均值：椭圆噪声：Dice
    (%)：71.384 Jac (%)：58.452；对称噪声：Dice (%)：74.058 Jac (%)：62.667；非对称噪声：Dice (%)：74.410
    Jac (%)：63.029 |'
- en: 6 Only limited supervision
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 仅限监督
- en: '*Only Limited Supervision* refers to a set of methods in which the available
    supervision or labeling for training data is constrained or limited in nature.
    Furthermore, unlabeled data is also unavailable. These methods are typically applied
    in scenarios where acquiring extensive or detailed annotations is challenging
    or resource-intensive. Instead, they employ alternative strategies such as few-shot
    learning, transfer learning, and data augmentation to maximize the utility of
    the limited available supervision (Figure [17](#S6.F17 "Figure 17 ‣ 6 Only limited
    supervision ‣ Data efficient deep learning for medical image analysis: A survey")).
    These approaches enhance model performance and facilitate tasks like segmentation,
    classification, or detection with minimal labeled data. A summary of recent approaches
    for learning with only limited supervision is provided in Table [6](#S6.T6 "Table
    6 ‣ 6.3 Transfer learning ‣ 6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey").'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*仅限监督* 指的是一套方法，其中可用于训练数据的监督或标签是受限的。此外，未标记的数据也不可用。这些方法通常应用于获取广泛或详细注释具有挑战性或资源密集的场景中。相反，它们采用替代策略，如少样本学习、迁移学习和数据增强，以最大化有限可用监督的效用（见图
    [17](#S6.F17 "Figure 17 ‣ 6 Only limited supervision ‣ Data efficient deep learning
    for medical image analysis: A survey")）。这些方法提高了模型性能，并在最少标记数据的情况下促进了分割、分类或检测等任务。最近仅限监督学习方法的总结见表
    [6](#S6.T6 "Table 6 ‣ 6.3 Transfer learning ‣ 6 Only limited supervision ‣ Data
    efficient deep learning for medical image analysis: A survey")。'
- en: '![Refer to caption](img/a2b2a9190722310ce00af0d3b4b91ea0.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a2b2a9190722310ce00af0d3b4b91ea0.png)'
- en: 'Figure 17: Taxonomy of *Only Limited Supervision* methods.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：*仅有限监督*方法的分类。
- en: 6.1 Data Augmentation
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 数据增强
- en: 'Data augmentation offers a means to significantly increase the quantity and
    diversity of training data, all while avoiding the need for additional sample
    collection. These augmentation techniques encompass both straightforward yet remarkably
    impactful transformations like cropping, padding, and flipping, as well as more
    intricate generative models [[300](#bib.bib300)]. The efficacy of data augmentation
    strategies varies based on factors like input nature and visual tasks. Therefore,
    the field of medical imaging might necessitate distinct augmentation approaches
    that yield plausible data instances and effectively enhance the regularization
    of deep neural networks. Moreover, data augmentation can also address the issue
    of underrepresented classes by generating additional instances, such as generating
    synthetic lesions. Following earlier surveys [[300](#bib.bib300), [301](#bib.bib301)],
    we have categorized data augmentation methods into three broad groups: transformation
    of original data, generation of artificial data and other categories.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强提供了一种显著增加训练数据数量和多样性的方法，同时避免了额外采集样本的需求。这些增强技术包括简单却极具影响力的变换，如裁剪、填充和翻转，以及更复杂的生成模型[[300](#bib.bib300)]。数据增强策略的有效性因输入特性和视觉任务等因素而异。因此，医学影像领域可能需要不同的增强方法，以生成可信的数据实例，并有效增强深度神经网络的正则化。此外，数据增强还可以通过生成额外实例（如生成合成病灶）来解决类别不足的问题。参考早期的调查[[300](#bib.bib300),
    [301](#bib.bib301)]，我们将数据增强方法分为三大类：原始数据的转换、人工数据的生成和其他类别。
- en: 6.1.1 Transformation of original data
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 原始数据的转换
- en: 'The first data augmentation category involves applying various image manipulation
    techniques to existing samples. This can be divided into three subcategories:
    (1) Affine Transformations are geometric changes that retain lines and parallelism,
    though not necessarily distances and angles. This is ensured by transformation
    constraints, typically preserving the image’s aspect ratio along axes of symmetry.
    The transformations include translation, rotation, flipping, scaling, cropping,
    and shearing [[302](#bib.bib302), [303](#bib.bib303)]. (2) Elastic transformations
    involve applying a spatial deformation field to an image. Unlike affine transformations,
    they don’t enforce the preservation of collinearity or aspect ratio. As a result,
    elastic transformations can introduce shape variations and can be employed to
    enhance the robustness of segmentation algorithms [[301](#bib.bib301)]. and (3)
    Pixel-Level Transformations alter pixel values to modify characteristics like
    saturation, contrast, noise, and brightness [[304](#bib.bib304)]. Given that medical
    imaging is often grayscale, color-based changes are rare. Pixel-level transformations
    aid deep neural networks’ robustness across different scanners and protocols that
    might affect pixel distribution.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据增强类别涉及对现有样本应用各种图像处理技术。这可以细分为三个子类别：（1）仿射变换是保持线条和平行性的几何变化，但不一定保持距离和角度。这由变换约束保证，通常在对称轴上保持图像的纵横比。这些变换包括平移、旋转、翻转、缩放、裁剪和剪切[[302](#bib.bib302),
    [303](#bib.bib303)]。 （2）弹性变换涉及将空间变形场应用于图像。与仿射变换不同，它们不强制保持共线性或纵横比。因此，弹性变换可以引入形状变化，并可用于增强分割算法的鲁棒性[[301](#bib.bib301)]。
    （3）像素级变换改变像素值以修改饱和度、对比度、噪声和亮度等特征[[304](#bib.bib304)]。鉴于医学影像通常是灰度图像，基于颜色的变化较少。像素级变换有助于深度神经网络在不同扫描仪和协议中保持鲁棒性，这些扫描仪和协议可能会影响像素分布。
- en: The majority of transformations within this category are quite straightforward
    to apply and are either readily available in deep learning frameworks or can be
    easily incorporated using versatile libraries [[305](#bib.bib305)]. Recently,
    there has been a surge in frameworks and libraries tailored for the medical field,
    like the Medical Open Network for AI (MONAI) [[306](#bib.bib306)]. However, it’s
    important to note that since these techniques rely on altering the original samples,
    they can’t enhance the network’s ability to generalize beyond its initial training
    data. They often generate samples that are highly correlated.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别中的大多数变换方法都相当简单易用，通常在深度学习框架中可以直接获得，或者可以使用多功能库轻松集成[[305](#bib.bib305)]。最近，针对医学领域的框架和库也出现了激增，如医学开放网络人工智能（MONAI）[[306](#bib.bib306)]。然而，需要注意的是，由于这些技术依赖于改变原始样本，它们无法增强网络超越初始训练数据的泛化能力。它们通常生成高度相关的样本。
- en: 6.1.2 Generation of artificial data
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 人工数据生成
- en: Creating artificial or synthesized samples can provide a broader range of diverse
    and complex examples, effectively addressing the limitations of methods based
    on transformations. The prevalent method for medical image synthesis is through
    generative networks, particularly generative adversarial networks (GANs) [[307](#bib.bib307)].
    However, generating synthetic images can also involve techniques like combining
    features or employing specialized modeling approaches designed for specific medical
    imaging tasks or modalities. While these approaches offer increased diversity,
    they often require higher computational resources and introduce complexity. Additionally,
    artificially generated samples might not fully capture the visual attributes or
    distribution of genuine data instances.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 创建人工或合成样本可以提供更广泛的多样性和复杂性的示例，有效解决基于变换方法的局限性。医学图像合成的普遍方法是通过生成网络，特别是生成对抗网络（GANs）[[307](#bib.bib307)]。然而，生成合成图像还可以涉及将特征结合或采用为特定医学成像任务或模态设计的专门建模方法。虽然这些方法提供了更大的多样性，但它们通常需要更高的计算资源，并引入复杂性。此外，人工生成的样本可能无法完全捕捉真实数据实例的视觉属性或分布。
- en: Depending on the specific domain, dataset, and task at hand, certain families
    of GANs may be more suitable, while others may be entirely impractical. Translation-based
    GANs, which encompass models like CGAN [[308](#bib.bib308)], pix2pix [[309](#bib.bib309)],
    CycleGan [[248](#bib.bib248)], and SPADE [[310](#bib.bib310)], specialize in learning
    how to transform various types of images. For instance, they can convert a segmentation
    mask into a newly synthesized input or transform a non-contrast CT-scan into a
    contrast CT-scan. In contrast, noise-based generation models like DC-GAN [[311](#bib.bib311)],
    StyleGAN2 [[312](#bib.bib312)], and PGAN [[313](#bib.bib313)] offer greater flexibility.
    However, noise-based generation techniques may encounter challenges when dealing
    with small training datasets, necessitating mitigation strategies such as patch
    extractions and traditional data augmentation. Although translation based models
    are known for producing images of exceptionally high quality, they are restricted
    in terms of the quantity of images they can create because they rely on the use
    of segmentation masks or different image modalities as input requirements. Noise-based
    approaches, on the other hand, do not face such limitations but often yield images
    with lower visual quality and run the risk of reproducing artifacts (such as vignettes
    or rulers) that could reinforce biases present in the dataset [[314](#bib.bib314)].
    These frameworks and other extended versions of GANs have been widely employed
    for augmenting various types of organ images, including liver [[315](#bib.bib315),
    [316](#bib.bib316)], skin [[317](#bib.bib317)], chest [[318](#bib.bib318)], eye
    [[319](#bib.bib319)], lung [[320](#bib.bib320)], breast [[321](#bib.bib321), [322](#bib.bib322)],
    brain [[323](#bib.bib323), [324](#bib.bib324)], and more. Readers interested in
    a comprehensive review of GANs for medical image augmentation can refer to the
    work of Chen et al. [[325](#bib.bib325)].
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 根据具体领域、数据集和任务的不同，某些类型的生成对抗网络（GAN）可能更为适用，而其他类型可能完全不适用。基于转换的GAN，例如CGAN [[308](#bib.bib308)]、pix2pix
    [[309](#bib.bib309)]、CycleGan [[248](#bib.bib248)] 和 SPADE [[310](#bib.bib310)]，专注于学习如何转换各种类型的图像。例如，它们可以将分割掩模转换为新合成的输入，或将非对比CT扫描转换为对比CT扫描。相比之下，基于噪声的生成模型，如DC-GAN
    [[311](#bib.bib311)]、StyleGAN2 [[312](#bib.bib312)] 和 PGAN [[313](#bib.bib313)]，则提供了更大的灵活性。然而，当处理小规模训练数据集时，基于噪声的生成技术可能会遇到挑战，需要采取诸如补丁提取和传统数据增强等缓解策略。尽管基于转换的模型以生成极高质量的图像而闻名，但由于依赖于分割掩模或不同图像模态作为输入要求，它们在生成图像的数量上受到限制。另一方面，基于噪声的方法没有这种限制，但通常会生成视觉质量较低的图像，并且有可能复现数据集中存在的伪影（如渐晕或标尺），这些伪影可能会强化数据集中存在的偏差
    [[314](#bib.bib314)]。这些框架和其他扩展版本的GAN已广泛用于增强各种类型的器官图像，包括肝脏 [[315](#bib.bib315),
    [316](#bib.bib316)]、皮肤 [[317](#bib.bib317)]、胸部 [[318](#bib.bib318)]、眼睛 [[319](#bib.bib319)]、肺部
    [[320](#bib.bib320)]、乳腺 [[321](#bib.bib321), [322](#bib.bib322)]、大脑 [[323](#bib.bib323),
    [324](#bib.bib324)] 等。有兴趣全面了解GAN在医学图像增强中的应用的读者，可以参考Chen等人的研究 [[325](#bib.bib325)]。
- en: Other than GANs, Copy-paste methods have been utilized to generate artificial
    data. Copy-paste is a simple yet effective data augmentation technique and, it
    has demonstrated the potential to amplify the generalization power of deep neural
    networks. In essence, copy-paste involves copying portions of one image and pasting
    them onto another. Notably, the mix-up technique by [[326](#bib.bib326)], and
    CutMix [[327](#bib.bib327)] are well-known approaches for mixing entire images
    and mixing image crops, respectively. Several studies have extended these methods
    to address specific objectives in MIA. For example, TumorCP [[328](#bib.bib328)]
    employs lesion masks to extract lesions from scans and paste them onto another
    scan at appropriate locations, guided by the lesion masks in the target scan.
    In the context of nuclei segmentation, InsMix [[329](#bib.bib329)] follows a Copy-Smooth-Paste
    principle and conducts morphology-constrained generative instance augmentation.
    SelfMix [[330](#bib.bib330)] leverages both tumor and non-tumor information for
    lesion segmentation. Given a pair of annotated training images, CarveMix [[331](#bib.bib331)]
    combines a region of interest (ROI) based on lesion location and geometry, replacing
    the corresponding voxels in other labeled images. TensorMixup [[332](#bib.bib332)]
    is a method that merges two image patches using a tensor and has been utilized
    to enhance the precision of tumor segmentation. Certain alternative approaches
    concentrate on medical datasets that frequently exhibit skewness towards negative
    cases. These methods encompass the creation and incorporation of fabricated lesions
    into individuals who are otherwise healthy. For example, these methods have been
    applied to simulate lesions resembling multiple sclerosis in brain MR images [[333](#bib.bib333)]
    or to introduce cancer indicators into breast mammography images [[334](#bib.bib334)].
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GANs之外，复制-粘贴方法也被用来生成人工数据。复制-粘贴是一种简单却有效的数据增强技术，它展示了提升深度神经网络泛化能力的潜力。实际上，复制-粘贴涉及将一张图像的一部分复制并粘贴到另一张图像上。特别是，[[326](#bib.bib326)]的mix-up技术和[[327](#bib.bib327)]的CutMix技术分别是混合整张图像和混合图像裁剪的知名方法。许多研究已经扩展这些方法以应对MIA中的具体目标。例如，TumorCP
    [[328](#bib.bib328)]使用病灶掩码从扫描中提取病灶，并将其粘贴到另一张扫描图像的适当位置，指导原则是目标扫描中的病灶掩码。在核分割的背景下，InsMix
    [[329](#bib.bib329)]遵循复制-平滑-粘贴原则，并进行形态学约束的生成实例增强。SelfMix [[330](#bib.bib330)]利用肿瘤和非肿瘤信息进行病灶分割。给定一对标注的训练图像，CarveMix
    [[331](#bib.bib331)]基于病灶位置和几何形状结合感兴趣区域（ROI），并替换其他标记图像中的相应体素。TensorMixup [[332](#bib.bib332)]是一种使用张量合并两个图像补丁的方法，已被用来提高肿瘤分割的精度。某些替代方法集中于那些经常对负案例存在偏斜的医学数据集。这些方法包括在健康个体中创建和加入虚拟病灶。例如，这些方法已被应用于模拟脑MR图像中的多发性硬化病灶[[333](#bib.bib333)]或在乳腺X线图像中引入癌症指标[[334](#bib.bib334)]。
- en: 6.1.3 Others
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 其他
- en: Apart from the aforementioned categories, there exist augmentation techniques
    designed for specific purposes. Notably, modalities such as CT and MRI possess
    a volumetric nature. Leveraging 3D convolutions presents the advantage of incorporating
    information from neighboring slices, leading to enhanced performance given a large
    dataset. While several image transformations, particularly affine transformations,
    are well-established for 2D images, extending them to 3D settings may pose challenges
    in terms of computational efficiency. Examples of augmentations designed with
    3D data augmentation techniques include 3D GANs [[335](#bib.bib335)], 3D affine
    transformations [[336](#bib.bib336)], and multiplanar image synthesis [[337](#bib.bib337)].
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述类别之外，还存在为特定目的设计的增强技术。特别是，CT和MRI等模态具有体积性质。利用3D卷积的优点在于可以结合邻近切片的信息，从而在大型数据集上获得更好的性能。尽管多种图像变换，特别是仿射变换，对于2D图像来说已相当成熟，但将其扩展到3D设置可能会在计算效率上面临挑战。针对3D数据增强技术设计的增强示例包括3D
    GANs [[335](#bib.bib335)]、3D仿射变换 [[336](#bib.bib336)] 和多平面图像合成 [[337](#bib.bib337)]。
- en: 'Learnable data augmentation, also known as neural data augmentation, is an
    advanced technique in deep learning where the augmentation parameters are learned
    by the neural network during the training process. Unlike traditional data augmentation
    methods that apply fixed transformations to input data, learnable data augmentation
    allows the model to adaptively determine the augmentation parameters based on
    the data and the task at hand. Methods following this strategy involve training
    two networks simultaneously: one network learns to solve a specific task, while
    the second network learns how to augment the data for the first one. One of the
    most common approaches for learnable data augmentation is autoaugment [[338](#bib.bib338)].
    This method aims to optimize network performance by identifying the most effective
    combination of established transformations (like affine transformations, pixel-level
    modifications, etc.). The ideal augmentation policy is determined through a neural
    network, which can be trained using adversarial training [[339](#bib.bib339)],
    evolutionary algorithms [[340](#bib.bib340)], or reinforcement learning [[338](#bib.bib338)].'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 可学习的数据增强，也称为神经数据增强，是深度学习中的一种高级技术，其中增强参数在训练过程中由神经网络学习。与应用固定变换到输入数据的传统数据增强方法不同，可学习的数据增强允许模型根据数据和当前任务自适应地确定增强参数。根据这种策略的方法涉及同时训练两个网络：一个网络学习解决特定任务，而第二个网络学习为第一个网络数据进行增强。可学习数据增强的最常见方法之一是autoaugment
    [[338](#bib.bib338)]。该方法旨在通过确定已建立的变换的最有效组合（如仿射变换、像素级修改等），来优化网络性能。理想的增强策略是通过神经网络确定的，可以使用对抗性训练[[339](#bib.bib339)]、进化算法[[340](#bib.bib340)]或强化学习[[338](#bib.bib338)]来训练。
- en: 6.2 Few shot learning
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 少样本学习
- en: 'Few-shot learning (FSL) takes inspiration from human-like robust reasoning
    and analytical abilities. Wang et al. [[341](#bib.bib341)] provided a standard
    definition for machine learning based on experience (E), task (T), and performance
    (P): A computer program is considered to learn from experience E with respect
    to certain classes of task T and performance measure P if its performance can
    enhance with E on T as measured by P. It’s important to note that E, the experience
    in FSL, is quite limited. Formally, within each few-shot task, we are given three
    sets: a support set denoted as S, a query set referred to as Q, and an auxiliary
    set labeled as A. The support set S encompasses C distinct categories, with each
    category comprising K training samples, essentially forming a C-way K-shot configuration.
    The query set Q comprises unlabeled query data. We categorize the current deep
    FSL methods into enlarging the training data, metric-learning based methods, meta-learning
    based methods and others.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习（FSL）的灵感来源于人类强大的推理和分析能力。王等人[[341](#bib.bib341)]提供了一个以经验（E）、任务（T）和性能（P）为基础的机器学习标准定义：如果一个计算机程序在某些类别的任务T上的性能能够通过经验E在性能度量P上提高，那么它被认为是从经验E中学习。重要的是要注意，FSL中的经验E非常有限。形式上，在每个少样本任务中，我们被给定三组数据：一个支持集，记为S，一个查询集，称为Q，和一个辅助集，标记为A。支持集S包括C个不同的类别，每个类别包括K个训练样本，从本质上讲形成了一个C-way
    K-shot的配置。查询集Q包括未标记的查询数据。我们将当前的深度FSL方法归类为扩大训练数据、基于度量学习的方法、基于元学习的方法和其他方法。
- en: 'Enlarging the training data: These approaches augment the training data to
    enlarge the number of samples, enabling the utilization of standard deep learning
    models and algorithms on the augmented dataset to attain a more accurate model.
    For multi-modal medical image segmentation, Mondal et al. [[342](#bib.bib342)]
    expand the training set through the use of GANs. On the other hand, Zhao et al.
    [[343](#bib.bib343)] present a learning-based technique for data augmentation.
    Specifically, their approach starts with a single labeled image and a set of unlabeled
    examples. By employing learning-based registration methods, they model the spatial
    and appearance transformations between the labeled and unlabeled examples. These
    transformations encompass effects such as non-linear deformations and variations
    in imaging intensity. Subsequently, they generate new labeled examples by sampling
    these transformations and applying them to the labeled example, resulting in a
    diverse range of realistic images. These synthesized examples are then used to
    train a supervised segmentation model. For the segmentation of new WM tracts in
    a few-shot scenario, [[344](#bib.bib344), [345](#bib.bib345)] have proposed efficient
    data augmentation techniques. Specifically, Lu et al. [[344](#bib.bib344)] introduce
    an efficient data augmentation technique that creates synthetic annotated images
    through tract-aware image mixing. Additionally, they employ a transfer learning
    method for few-shot segmentation. [[346](#bib.bib346)]'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展训练数据：这些方法通过增加样本数量来扩充训练数据，使得可以在扩充的数据集上利用标准深度学习模型和算法，从而获得更准确的模型。对于多模态医学图像分割，Mondal等人[[342](#bib.bib342)]通过使用生成对抗网络（GANs）来扩展训练集。另一方面，赵等人[[343](#bib.bib343)]提出了一种基于学习的数据增强技术。具体来说，他们的方法从一张标记图像和一组未标记样本开始。通过采用基于学习的配准方法，他们对标记图像和未标记样本之间的空间和外观变换进行建模。这些变换包括非线性变形和成像强度变化等效果。随后，他们通过对这些变换进行采样并将其应用于标记样本，从而生成新的标记样本，形成多样化的真实图像。这些合成样本随后用于训练监督分割模型。在少量样本场景中对新的白质束进行分割，[[344](#bib.bib344),
    [345](#bib.bib345)]提出了高效的数据增强技术。具体而言，Lu等人[[344](#bib.bib344)]引入了一种高效的数据增强技术，通过束意识图像混合生成合成标注图像。此外，他们还采用了迁移学习方法进行少量样本分割。[[346](#bib.bib346)]
- en: 'Metric-learning based approaches: These approaches offer a straightforward
    and adaptable framework where they directly assess the similarities or dissimilarities
    between query images in the query set and labeled images in the support set. A
    classical metric-learning technique, known as the Prototypical Network (ProtoNet)
    by Snell et al. [[347](#bib.bib347)], illustrates this concept. ProtoNet computes
    prototype representations for each base class by averaging the feature vectors
    and subsequently measures the distances between these prototype representations
    and each query image. Importantly, metric-learning based methods do not involve
    data-independent parameters in their classifiers, which means fine-tuning is unnecessary
    during the testing phase. Moreover, some researchers, such as Ali et al. [[348](#bib.bib348)],
    have introduced an innovative additive angular margin metric to enhance the original
    ProtoNet’s ability to classify challenging samples, especially in scenarios involving
    multi-center, underrepresented, and difficult-to-classify endoscopy data. To enhance
    prototype-based few-shot segmentation model for abdominal organs, Wang et al.
    [[349](#bib.bib349)] introduce a regularization technique. This enhancement involves
    two key elements: self-reference and contrastive learning. Self-reference regularization
    ensures that a class prototype accurately represents the entire organ within a
    support image. Contrastive learning aids in the understanding of similarity between
    foreground and background features.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 基于度量学习的方法：这些方法提供了一个简单且灵活的框架，通过直接评估查询集中查询图像与支持集中标记图像之间的相似性或差异性。一个经典的度量学习技术是由
    Snell 等人提出的原型网络（ProtoNet）[[347](#bib.bib347)]，这展示了这个概念。ProtoNet 通过平均特征向量计算每个基础类别的原型表示，然后测量这些原型表示与每个查询图像之间的距离。重要的是，基于度量学习的方法在其分类器中不涉及数据无关的参数，这意味着在测试阶段不需要微调。此外，一些研究人员，如
    Ali 等人[[348](#bib.bib348)]，引入了一种创新的附加角度边际度量，以增强原始 ProtoNet 在分类具有挑战性样本时的能力，特别是在涉及多中心、样本不足和难以分类的内镜数据的情况下。为了增强基于原型的腹部器官少样本分割模型，Wang
    等人[[349](#bib.bib349)] 引入了一种正则化技术。这一改进包括两个关键要素：自参考和对比学习。自参考正则化确保一个类别原型准确代表支持图像中的整个器官。对比学习有助于理解前景和背景特征之间的相似性。
- en: 'In contrast to natural images, there is a lack of extensive publicly available
    datasets for pre-training medical image segmentation models. Consequently, some
    self-supervised learning approaches have emerged in the domain of medical image
    few-shot segmentation, relying on unlabeled data. To this end, Ouyang et al. [[350](#bib.bib350),
    [351](#bib.bib351)] presented SSL-ALPNet, a self-supervised learning method based
    on superpixels. In this approach, for every unlabeled image, pseudolabels are
    created at the superpixel level. During each training iteration, a randomly chosen
    pseudolabel, along with the original image, is used as both the support and query.
    Random transformations are introduced between the support and query images. The
    primary objective of this self-supervision task is to segment the pseudolabel
    on the query image, using the support image as a reference, despite the applied
    transformations. Additionally, they incorporated an adaptive local prototype pooling
    module into prototypical networks to address the prevalent issue of foreground-background
    class imbalance in medical image segmentation, as shown in Figure [18](#S6.F18
    "Figure 18 ‣ 6.2 Few shot learning ‣ 6 Only limited supervision ‣ Data efficient
    deep learning for medical image analysis: A survey"). Hansen et al. [[352](#bib.bib352)]
    expanded on this concept by extending the self-supervision task to supervoxels,
    effectively incorporating 3D information from image volumes. They introduced ADNet,
    a prototypical segmentation network inspired by anomaly detection, which avoids
    modeling the large and diverse background class with prototypes. Building on this
    work, their recent contribution, ADNet++ [[353](#bib.bib353)], presents a one-step
    multi-class medical image segmentation framework. The model notably enhances the
    current 3D FSS model for MRI and CT-based abdominal organ and cardiac segmentation.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '与自然图像相比，缺乏大量公开可用的数据集用于预训练医学图像分割模型。因此，一些自监督学习方法在医学图像少样本分割领域应运而生，这些方法依赖于未标记的数据。为此，欧阳等人[[350](#bib.bib350),
    [351](#bib.bib351)]提出了SSL-ALPNet，这是一种基于超像素的自监督学习方法。在这种方法中，为每个未标记的图像，在超像素级别创建伪标签。在每次训练迭代中，随机选择的伪标签和原始图像一起被用作支持和查询。支持图像和查询图像之间引入随机变换。这一自监督任务的主要目标是分割查询图像上的伪标签，尽管应用了变换，仍以支持图像作为参考。此外，他们在原型网络中加入了自适应局部原型池模块，以解决医学图像分割中常见的前景-背景类别不平衡问题，如图[18](#S6.F18
    "Figure 18 ‣ 6.2 Few shot learning ‣ 6 Only limited supervision ‣ Data efficient
    deep learning for medical image analysis: A survey")所示。汉森等人[[352](#bib.bib352)]通过将自监督任务扩展到超体素，有效地结合了图像体积中的3D信息。他们介绍了ADNet，这是一种受到异常检测启发的原型分割网络，避免了使用原型对大型多样背景类别进行建模。在此基础上，他们的最新贡献ADNet++
    [[353](#bib.bib353)]，提出了一种一步式多类医学图像分割框架。该模型显著提升了当前基于MRI和CT的腹部器官及心脏分割的3D FSS模型。'
- en: '![Refer to caption](img/8590d1749f6ec8d68a0412e4875470ba.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8590d1749f6ec8d68a0412e4875470ba.png)'
- en: 'Figure 18: Illustration of the SSL-ALPNet framework [[350](#bib.bib350)]: (a)
    The self-supervision task involves segmenting the pseudolabel on the query image
    with reference to the support image, despite the applied transformations (shown
    in blue boxes). (b) The ALPNet method addresses the challenge of class imbalance
    by adaptively extracting multiple local representations of the large background
    class (in blue), each of which represents a distinct local background region (image
    from [[350](#bib.bib350)]).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：SSL-ALPNet框架的示意图[[350](#bib.bib350)]：(a) 自监督任务涉及在查询图像上分割伪标签，以支持图像为参考，尽管应用了变换（用蓝色框显示）。(b)
    ALPNet方法通过自适应提取大型背景类别（蓝色）多个局部表示来解决类别不平衡问题，每个局部表示代表一个不同的局部背景区域（图像来自[[350](#bib.bib350)]）。
- en: In medical images, a significant imbalance exists between the foreground and
    background. Medical images typically feature a diverse background comprising numerous
    tissues and organs, whereas the foreground is typically uniform and occupies a
    relatively small area. Applying the same global operation, like masked average
    pooling, directly to both foreground and background, as is commonly done in the
    processing of natural images, can result in the loss of local information. To
    address this issue, recent studies in the field of prototypical FSS have introduced
    more adaptive prototype extraction modules to mitigate the impact of complex backgrounds.
    Specifically, these studies have incorporated additional priors, such as spatial
    location [[354](#bib.bib354)], and neighborhood correlations [[355](#bib.bib355)],
    into the prototypes to preserve spatial and shape information.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像中前景与背景之间存在显著的不平衡。医学图像通常具有由许多组织和器官组成的多样背景，而前景通常是均匀的并且占据的区域相对较小。将像自然图像处理中常用的全局操作（如掩膜平均池化）直接应用于前景和背景，可能会导致局部信息的丢失。为了解决这个问题，最近在原型少样本学习（FSS）领域的研究引入了更具适应性的原型提取模块，以减轻复杂背景的影响。具体而言，这些研究在原型中加入了额外的先验信息，如空间位置[[354](#bib.bib354)]和邻域相关性[[355](#bib.bib355)]，以保持空间和形状信息。
- en: 'Meta-learning based approaches: Meta-learning methods typically employ a meta-training
    approach, where they train a model on a sequence of few-shot tasks derived from
    the base classes during the training phase. The objective is to equip the pre-trained
    model with the capability to quickly adapt to entirely new tasks during the testing
    phase. One well-known representative of meta-learning methods is Model-Agnostic
    Meta-Learning (MAML) [[356](#bib.bib356)]. MAML achieves this adaptation by pre-training
    the initial model parameters using second-order gradients, enabling the model
    to rapidly adapt to new tasks with only a limited number of gradient steps. Several
    studies in medical imaging employ MAML and its extensions for various few-shot
    learning tasks. These tasks include rare disease classification [[357](#bib.bib357)],
    classifying whole-genome doubling (WGD) across 17 cancer types using digitized
    histopathology slide images [[358](#bib.bib358)], brain tumor segmentation [[359](#bib.bib359)],
    and more. Further, To address issues related to vanishing high-order meta-gradients
    in MAML, Khadka et al. [[360](#bib.bib360)] utilize the Implicit Model Agnostic
    Meta-Learning (iMAML) optimization strategy [[361](#bib.bib361)] for few-shot
    lesion segmentation. In this approach, inner optimization focuses on computing
    weights using a CNN model, while an analytic solution is used to estimate the
    outer meta-gradients.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 基于元学习的方法：元学习方法通常采用元训练的方法，在训练阶段，模型在一系列源自基础类别的少样本任务上进行训练。其目标是使预训练模型具备在测试阶段快速适应全新任务的能力。元学习方法的一个著名代表是模型无关的元学习（Model-Agnostic
    Meta-Learning, MAML）[[356](#bib.bib356)]。MAML通过使用二阶梯度对初始模型参数进行预训练，实现了这种适应，使模型能够在仅有有限数量的梯度步骤的情况下迅速适应新任务。多个医学影像研究采用了MAML及其扩展来处理各种少样本学习任务。这些任务包括罕见疾病分类[[357](#bib.bib357)]、使用数字化的组织病理学幻灯片图像对17种癌症类型进行全基因组倍增（WGD）分类[[358](#bib.bib358)]、脑肿瘤分割[[359](#bib.bib359)]等。此外，为了解决MAML中高阶元梯度消失的问题，Khadka等人[[360](#bib.bib360)]利用了隐式模型无关元学习（iMAML）优化策略[[361](#bib.bib361)]进行少样本病灶分割。在这种方法中，内部优化集中于使用CNN模型计算权重，而外部元梯度的估计则使用解析解。
- en: 'Zhao et al. [[362](#bib.bib362)] suggest that meta-learning can enhance the
    generator’s ability to *learn to hallucinate* meaningful images, leading to improved
    segmentation models in few-shot unsupervised domain adaptation. In this regard,
    they introduce a meta-hallucinator to generate valuable samples, enhancing model
    adaptability on the target domain with limited source annotations. For diagnosis
    of glioblastoma multiforme progression, Song et al. [[363](#bib.bib363)] propose
    an interpretable structure-constrained graph neural network (ISGNN). The ISGNN
    used a meta-learning strategy for aggregating class-specific graph nodes to enhance
    classification performance on small-scale datasets while maintaining interpretability.
    Recently, Gao et al. [[364](#bib.bib364)] introduced a discriminative ensemble
    meta-learning approach for the diagnosis of rare fundus diseases. They introduced
    a co-regulation loss during the pre-training of the meta-learning backbone. Subsequently,
    ensemble-learning techniques were employed to improve performance, taking advantage
    of the hierarchical features within the backbone network. They explored three
    ensemble strategies: uniform averaging, majority voting, and stacking, to identify
    low-shot rare fundus diseases.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人 [[362](#bib.bib362)] 提出，元学习可以增强生成器*学习生成*有意义图像的能力，从而在少样本无监督领域适应中改善分割模型。在这方面，他们引入了一种元生成器来生成有价值的样本，提高了模型在目标领域的适应性，同时仅使用有限的源标注。对于多形性胶质母细胞瘤的诊断，Song
    等人 [[363](#bib.bib363)] 提出了一个可解释的结构约束图神经网络（ISGNN）。ISGNN 使用了元学习策略来聚合类特定的图节点，以提升在小规模数据集上的分类性能，同时保持可解释性。最近，Gao
    等人 [[364](#bib.bib364)] 引入了一种用于稀有眼底疾病诊断的判别性集成元学习方法。他们在元学习主干网络的预训练过程中引入了共同调节损失。随后，集成学习技术被用于提高性能，利用了主干网络中的层次特征。他们探索了三种集成策略：均匀平均、投票多数和堆叠，以识别低样本的稀有眼底疾病。
- en: 'Others: One approach to accomplishing few-shot learning involves utilizing
    the encoder-decoder framework to explore the connection between a query and a
    support set. In this regard, Roy et al. [[365](#bib.bib365)] introduce the Squeeze
    and Excitep framework, which was the first implementation of a two-branch architecture
    for medical image few-shot segmentation. One branch, referred to as the conditioner
    arm, focuses on extracting foreground information from the support set. The other
    branch, known as the segmenter arm, engages with the conditioner arm through the
    spatial SE module to rectify the query feature. MprNet [[366](#bib.bib366)], as
    an enhancement of SENet, introduces a fusion module based on cosine similarity
    to facilitate information exchange between these two branches. Similarly, Kim
    et al. [[367](#bib.bib367)] introduce a U-Net like network tailored for segmentation
    tasks. This network is designed to predict segmentation by capturing the relationship
    between 2D slices from the support data and a query image. It incorporates a bidirectional
    gated recurrent unit (GRU) to learn the coherence of encoded features among adjacent
    slices. Recently, Feng et al. [[368](#bib.bib368)] employ a hybrid method in which
    they introduce a segmenter built upon the encoder-decoder architecture. They incorporate
    spatial and prototypical priors as extra sources of supervisory information. Experimental
    results in multi-modalities and multi-organs segmentation showcase that the method
    they propose significantly surpasses previous state-of-the-art techniques. FSL
    is typically trained using the episode training method. Zhu et al. [[369](#bib.bib369)]
    introduced a Query-Relative (QR) loss, which is more effective when combined with
    the episode training approach than the Cross-Entropy loss for FSL.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 其他：一种实现少样本学习的方法涉及利用编码器-解码器框架来探索查询与支持集之间的关系。在这方面，Roy 等人 [[365](#bib.bib365)]
    介绍了 Squeeze and Excitep 框架，这是第一个用于医学图像少样本分割的双分支架构实现。一个分支，被称为条件器臂，专注于从支持集中提取前景信息。另一个分支，称为分割器臂，通过空间
    SE 模块与条件器臂互动，以校正查询特征。MprNet [[366](#bib.bib366)] 作为 SENet 的增强版，引入了基于余弦相似度的融合模块，以促进这两个分支之间的信息交换。同样，Kim
    等人 [[367](#bib.bib367)] 介绍了一种类似于 U-Net 的网络，专门用于分割任务。该网络旨在通过捕捉支持数据的 2D 切片和查询图像之间的关系来预测分割。它结合了双向门控递归单元（GRU），以学习相邻切片之间编码特征的一致性。最近，Feng
    等人 [[368](#bib.bib368)] 采用了一种混合方法，介绍了基于编码器-解码器架构的分割器。他们引入了空间和原型先验作为额外的监督信息来源。多模态和多器官分割的实验结果显示，他们提出的方法显著超过了之前的最先进技术。FSL
    通常使用情节训练方法进行训练。Zhu 等人 [[369](#bib.bib369)] 引入了一种 Query-Relative (QR) 损失，当与情节训练方法结合使用时，比交叉熵损失对
    FSL 更有效。
- en: 6.3 Transfer learning
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 转移学习
- en: To address the challenge of limited training data and enhance model performance,
    another common approach known as transfer learning is frequently employed. In
    this scenario, the goal is to harness knowledge acquired from similar learning
    tasks. The supervised transfer learning technique [[370](#bib.bib370)] has proven
    valuable in addressing various issues in medical image analysis. These approaches
    typically involve initial pre-training of standard architectures like ResNet [[371](#bib.bib371)]
    or VGG [[372](#bib.bib372)] on a source domain containing abundant data, such
    as natural images from sources like ImageNet [[373](#bib.bib373)] or medical images.
    Subsequently, these pre-trained models are transferred to the target domain and
    fine-tuned using a significantly smaller set of training examples. Tajbakhsh et
    al. [[370](#bib.bib370)] demonstrated that pre-trained CNNs, when appropriately
    fine-tuned, achieved performance levels at least comparable to CNNs trained entirely
    from the beginning. This has established transfer learning as a fundamental technique
    for image classification tasks across diverse modalities, spanning CT [[374](#bib.bib374)],
    mammography [[375](#bib.bib375)] MRI [[376](#bib.bib376)], X-ray [[377](#bib.bib377)],
    and more.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决有限训练数据的问题并提升模型性能，另一种常用的方法是转移学习。在这种情况下，目标是利用从类似学习任务中获得的知识。监督性转移学习技术[[370](#bib.bib370)]在解决医疗图像分析中的各种问题方面已被证明具有价值。这些方法通常涉及对标准架构如ResNet[[371](#bib.bib371)]或VGG[[372](#bib.bib372)]进行初步预训练，这些架构在包含大量数据的源领域上进行预训练，例如来自ImageNet[[373](#bib.bib373)]的自然图像或医疗图像。随后，这些预训练模型被转移到目标领域，并使用显著较小的训练样本集进行微调。Tajbakhsh等人[[370](#bib.bib370)]证明，经过适当微调的预训练CNN，其性能水平至少与从头开始训练的CNN相当。这使得转移学习成为一种基础技术，广泛应用于CT[[374](#bib.bib374)]、乳腺X线摄影[[375](#bib.bib375)]、MRI[[376](#bib.bib376)]、X射线[[377](#bib.bib377)]等多种图像分类任务中。
- en: Aside from classification, the use of transfer learning in addressing different
    medical image challenges, such as image segmentation and localization, has been
    limited [[22](#bib.bib22), [378](#bib.bib378), [211](#bib.bib211)]. This trend
    can be attributed, in part, to the inherent 3D characteristics of medical images,
    which present challenges when adapting 2D models trained on natural images. Additionally,
    it is influenced by the effective performance of shallower segmentation networks
    in medical imaging, which may not gain significant advantages from fine-tuning
    in contrast to deep models. However, certain studies have attempted transfer learning
    for image segmentation. For instance, Ma et al. [[379](#bib.bib379)] performed
    fine-tuning on an autoencoder that was originally pre-trained for image segmentation
    tasks in natural images. Similarly, other researchers like Qin et al. [[380](#bib.bib380)]
    utilized an encoder pre-trained for the task of image classification in natural
    images and added a randomly initialized decoder to it to address the task of prostate
    MRI segmentation. Liu et al. [[381](#bib.bib381)] perform a two-stage transfer
    learning framework for segmenting COVID-19 lung infections from CT images. Nguyen
    et al. [[382](#bib.bib382)] perform task agnostic transfer learning for skin attribute
    detection.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分类任务外，转移学习在解决不同医疗图像挑战（如图像分割和定位）中的应用还较为有限[[22](#bib.bib22), [378](#bib.bib378),
    [211](#bib.bib211)]。这种趋势部分归因于医疗图像的固有3D特性，这在将训练于自然图像的2D模型进行适配时会带来挑战。此外，较浅的分割网络在医疗影像中的有效表现也影响了其对微调的显著获益，相比于深度模型，微调效果可能不如预期。然而，某些研究已尝试将转移学习应用于图像分割任务。例如，Ma等人[[379](#bib.bib379)]对一个最初为自然图像分割任务预训练的自编码器进行了微调。类似地，Qin等人[[380](#bib.bib380)]使用了一个为自然图像分类任务预训练的编码器，并添加了一个随机初始化的解码器，以解决前列腺MRI分割任务。Liu等人[[381](#bib.bib381)]进行了一种两阶段的转移学习框架，用于从CT图像中分割COVID-19肺部感染。Nguyen等人[[382](#bib.bib382)]则进行了与任务无关的转移学习，用于皮肤属性检测。
- en: Some studies have attempted to transfer knowledge from 2D models pre-trained
    on natural images to models intended for 3D medical applications. For example,
    Yu et al. [[383](#bib.bib383)] adapted models trained on natural scene videos,
    treating the third dimension of medical scans as a temporal axis. However, this
    approach may not effectively capture the 3D context of medical scans. In contrast,
    Liu et al. [[384](#bib.bib384)] proposed a method to transform a 2D model into
    a 3D network by expanding 2D convolution filters into 3D separable anisotropic
    filters. Recently, Messaoudi et al. [[385](#bib.bib385)] introduced two transfer
    learning strategies. Firstly, they introduced weight transfer learning, an effective
    method for leveraging the weights of a pre-trained 2D classifier network by incorporating
    it into a network of the same or higher dimension. The second approach they proposed
    is dimensional transfer learning, which relies on extrapolating 3D weights from
    a pre-trained 2D network. Empirical evidence demonstrates that their methods outperform
    current state-of-the-art techniques.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究尝试将预训练的2D模型的知识迁移到用于3D医学应用的模型中。例如，Yu等人[[383](#bib.bib383)]将训练于自然场景视频的模型进行了适应，将医学扫描的第三维度视为时间轴。然而，这种方法可能无法有效捕捉医学扫描的3D上下文。相比之下，Liu等人[[384](#bib.bib384)]提出了一种通过将2D卷积滤波器扩展为3D可分离各向异性滤波器，将2D模型转化为3D网络的方法。最近，Messaoudi等人[[385](#bib.bib385)]提出了两种迁移学习策略。首先，他们引入了权重迁移学习，这是一种有效的方法，通过将预训练的2D分类器网络的权重整合到相同或更高维度的网络中来利用这些权重。他们提出的第二种方法是维度迁移学习，这依赖于从预训练的2D网络中外推3D权重。实证证据表明，他们的方法优于当前的最新技术。
- en: 'Table 6: Overview of recent methods in *Only Limited Supervision* category.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：*仅限有限监督*类别下近期方法的概述。
- en: '| Reference | Task | Algorithm Design | Dataset | Result |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 任务 | 算法设计 | 数据集 | 结果 |'
- en: '| [[328](#bib.bib328)] | kidney tumor segmentation | Data Augmentation | KiTS19
    | Mean Dice: 77.44 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| [[328](#bib.bib328)] | 肾肿瘤分割 | 数据增强 | KiTS19 | 平均Dice: 77.44 |'
- en: '| [[343](#bib.bib343)] | Brain tumour segmentation | Learning-based technique
    for data augmentation | T1-weighted MRI brain scans described in [[386](#bib.bib386)]
    | Dice score: 0.815 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| [[343](#bib.bib343)] | 脑肿瘤分割 | 基于学习的数据增强技术 | T1加权MRI脑部扫描，见[[386](#bib.bib386)]
    | Dice得分: 0.815 |'
- en: '| [[348](#bib.bib348)] | Clinical endoscopy image classification | Angular
    margin metric to ProtoNet | miniEndoGI classification datase | 5-way: 1-shot:
    58.76 $\pm$ 1.64, 5-shot: 66.72 $\pm$ 1.35; 3-way: 1-shot: 75.06 $\pm$ 1.87, 5-shot:
    81.20 $\pm$ 1.72; 2-way: 1-shot: 85.60 $\pm$ 2.21, 5-shot: 90.60 $\pm$ 1.70 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| [[348](#bib.bib348)] | 临床内镜图像分类 | Angular margin metric to ProtoNet | miniEndoGI
    分类数据集 | 5-way: 1-shot: 58.76 $\pm$ 1.64, 5-shot: 66.72 $\pm$ 1.35; 3-way: 1-shot:
    75.06 $\pm$ 1.87, 5-shot: 81.20 $\pm$ 1.72; 2-way: 1-shot: 85.60 $\pm$ 2.21, 5-shot:
    90.60 $\pm$ 1.70 |'
- en: '| [[351](#bib.bib351)] | Cardiac and organ segmentation | Prototype-based network
    + Self-supervision | Abdominal CT; Abdominal T2-SPIR MRI; Cardiac bSSFP MRI |
    Abdominal CT: 1-shot: Dice: 67.62, 5-shot: Dice: 75.91; Abdominal MRI: 1-shot:
    Dice: 76.81, 5-shot: Dice: 80.16; Cardiac: 1-shot: Dice: 77.94, 5-shot: Dice:
    81.66 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| [[351](#bib.bib351)] | 心脏和器官分割 | 基于原型的网络 + 自监督 | 腹部CT; 腹部T2-SPIR MRI; 心脏bSSFP
    MRI | 腹部CT: 1-shot: Dice: 67.62, 5-shot: Dice: 75.91; 腹部MRI: 1-shot: Dice: 76.81,
    5-shot: Dice: 80.16; 心脏: 1-shot: Dice: 77.94, 5-shot: Dice: 81.66 |'
- en: '| [[352](#bib.bib352)] | Abdomen and cardiac segmentation | Anomaly detection-inspired
    FS + Self-supervision | MS-CMRSeg; CHAOS | CHAOS: Mean DSC: 72.41; MS-CMRSeg:
    Mean DSC: 69.62 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| [[352](#bib.bib352)] | 腹部和心脏分割 | 异常检测启发的FS + 自监督 | MS-CMRSeg; CHAOS | CHAOS:
    平均DSC: 72.41; MS-CMRSeg: 平均DSC: 69.62 |'
- en: '| [[353](#bib.bib353)] | Abdomen and cardiac segmentation | Prototype-based
    network + Self-supervision | MS-CMRSeg; CHAOS; BTCV | CHAOS: Mean 95 HD: 12.5;
    Mean DSC: 80.99; BTCV: Mean 95 HD: 23.60; Mean DSC: 60.94; MS-CMRSeg: Mean 95
    HD: 6.08; Mean DSC: 69.68 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| [[353](#bib.bib353)] | 腹部和心脏分割 | 基于原型的网络 + 自监督 | MS-CMRSeg; CHAOS; BTCV |
    CHAOS: 平均95 HD: 12.5; 平均DSC: 80.99; BTCV: 平均95 HD: 23.60; 平均DSC: 60.94; MS-CMRSeg:
    平均95 HD: 6.08; 平均DSC: 69.68 |'
- en: '| [[355](#bib.bib355)] | Abdomen segmentation | Context relation encoder +
    Recurrent mask refine- ment module + Prototypical network | ABD-110; BTCV; CHAOS
    | ABD-110: DSC: 81.91; BTCV: DSC: 72.48; CHAOS: DSC: 79.26 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| [[355](#bib.bib355)] | 腹部分割 | 上下文关系编码器 + 递归掩膜精化模块 + 原型网络 | ABD-110; BTCV;
    CHAOS | ABD-110: DSC: 81.91; BTCV: DSC: 72.48; CHAOS: DSC: 79.26 |'
- en: '| [[369](#bib.bib369)] | Skin Disease Classification | FSL with Query-Relative
    loss | Dermatology images | 5-way 1-shot: ACC%: 52.41 Precision%: 53.21 F1%: 49.52;
    5-way 5-shot: ACC%: 71.99 Precision%: 74.23 F1%: 70.30 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| [[369](#bib.bib369)] | 皮肤病分类 | FSL 与查询相关损失 | 皮肤科图像 | 5-way 1-shot：ACC%：52.41
    精度%：53.21 F1%：49.52；5-way 5-shot：ACC%：71.99 精度%：74.23 F1%：70.30 |'
- en: '| [[359](#bib.bib359)] | Brain tumor segmentation | Meta-learning | BraTS2021
    | 1-way 1-shot: DSC($\mu$ ± std)% : 0.57 ± 0.19; 1-way 1-shot: DSC($\mu$ ± std)%
    : 0.63 ± 0.16; 1-way 1-shot: DSC($\mu$ ± std)% : 0.65 ± 0.17 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| [[359](#bib.bib359)] | 脑肿瘤分割 | 元学习 | BraTS2021 | 1-way 1-shot：DSC($\mu$ ±
    std)%：0.57 ± 0.19；1-way 1-shot：DSC($\mu$ ± std)%：0.63 ± 0.16；1-way 1-shot：DSC($\mu$
    ± std)%：0.65 ± 0.17 |'
- en: '| [[358](#bib.bib358)] | Classification of whole-genome doubling across 17
    cancer types | Model-Agnostic Meta-Learning | TCGA | AUC (average ± 1 standard
    deviation): 0.6944 ± 0.0773 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| [[358](#bib.bib358)] | 17种癌症类型的全基因组倍增分类 | 模型无关元学习 | TCGA | AUC（平均 ± 1 标准差）：0.6944
    ± 0.0773 |'
- en: '| [[363](#bib.bib363)] | Tumour classification | Interpretable structure- constrained
    graph neural network | Private dataset: 150 patients | ACC: 83.3, AUC: 81.9, SEN:
    67.2 and SPE: 85.7 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| [[363](#bib.bib363)] | 肿瘤分类 | 可解释的结构约束图神经网络 | 私有数据集：150名患者 | ACC：83.3，AUC：81.9，SEN：67.2
    和 SPE：85.7 |'
- en: '| [[362](#bib.bib362)] | Cardiac segmentation | Gradient-based meta-hallucination
    learning | MM-WHS 2017 | 4-shots: Average Dice: 75.6, Average ASD: 4.8; 1-shots:
    Average Dice: 51.8, Average ASD: 14.1 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| [[362](#bib.bib362)] | 心脏分割 | 基于梯度的元幻想学习 | MM-WHS 2017 | 4-shot：平均 Dice：75.6，平均
    ASD：4.8；1-shot：平均 Dice：51.8，平均 ASD：14.1 |'
- en: '| [[364](#bib.bib364)] | Rare fundus diseases diagnosis | Meta learning + Co-regularization
    loss + Ensemble-learning | FundusData-FS [[364](#bib.bib364)] | Accuracy(%): 2-way:
    1-shot: 71.53, 3-shot: 78.20, 5-shot: 81.47; Accuracy(%): 3-way: 1-shot: 56.69,
    3-shot: 62.62, 5-shot: 66.78; Accuracy(%): 4-way: 1-shot: 48.17, 3-shot: 56.65,
    5-shot: 58.60 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| [[364](#bib.bib364)] | 罕见眼底疾病诊断 | 元学习 + 协同正则化损失 + 集成学习 | FundusData-FS [[364](#bib.bib364)]
    | 准确率（%）：2-way：1-shot：71.53，3-shot：78.20，5-shot：81.47；准确率（%）：3-way：1-shot：56.69，3-shot：62.62，5-shot：66.78；准确率（%）：4-way：1-shot：48.17，3-shot：56.65，5-shot：58.60
    |'
- en: '| [[365](#bib.bib365)] | Multi-organ segmentation | Squeeze and excitep framework
    | Visceral dataset | Mean Dice score on validation set: 0.567 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| [[365](#bib.bib365)] | 多脏器分割 | Squeeze and excitep 框架 | 内脏数据集 | 验证集上的平均 Dice
    分数：0.567 |'
- en: '| [[368](#bib.bib368)] | Multi-organ segmentation | Encoder-decoder architecture
    + Spatial and prototypical priors | CHAOS | Left atrium (LA): 1-shot: Mean DSC:
    86.37; 5-shot: Mean DSC: 88.02 Left ventricle (LV): 1-shot: Mean DSC: 87.06; 5-shot:
    Mean DSC: 87.87 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| [[368](#bib.bib368)] | 多脏器分割 | 编码器-解码器架构 + 空间和原型先验 | CHAOS | 左心房（LA）：1-shot：平均
    DSC：86.37；5-shot：平均 DSC：88.02 左心室（LV）：1-shot：平均 DSC：87.06；5-shot：平均 DSC：87.87
    |'
- en: 7 Future research scope
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来研究范围
- en: 7.1 Continual/lifelong learning
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 持续/终身学习
- en: In healthcare, most intelligent diagnosis systems are limited in their scope,
    often capable of diagnosing only a few diseases. Expanding their capabilities
    after deployment is challenging, preventing them from achieving the breadth of
    diagnoses that medical specialists can. Collecting data for all diseases poses
    significant challenges due to privacy concerns and data sharing constraints. Consequently,
    training a single system to diagnose all diseases simultaneously is impractical.
    One potential solution is to make the system with the ability for continual learning.
    This would allow the system to progressively acquire the capacity to diagnose
    more diseases over time without needing extensive new data for previously learned
    diseases. Continual learning, also known as lifelong learning or incremental learning,
    is a learning paradigm in which a model learns and adapts to new information and
    tasks over time without forgetting previously acquired knowledge [[387](#bib.bib387)].
    Unlike traditional deep learning approaches that assume a fixed dataset and task,
    continual learning addresses scenarios where data arrives continuously, and the
    nature of tasks can evolve over time, including the possibility of introducing
    new classes.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健中，大多数智能诊断系统的范围有限，通常只能诊断少数几种疾病。部署后扩展其功能具有挑战性，阻止了它们达到医疗专家可以诊断的广度。由于隐私问题和数据共享限制，收集所有疾病的数据面临重大挑战。因此，训练一个系统以同时诊断所有疾病是不切实际的。一种可能的解决方案是使系统具备持续学习的能力。这将使系统能够随着时间的推移逐步获得诊断更多疾病的能力，而无需大量新的数据来处理之前学习过的疾病。持续学习，也称为终身学习或增量学习，是一种学习范式，其中模型在不断学习和适应新信息和任务的同时，不忘记之前获得的知识
    [[387](#bib.bib387)]。与传统的深度学习方法假设固定数据集和任务不同，持续学习处理数据不断到达的情境，并且任务的性质可能随着时间的推移而演变，包括引入新类别的可能性。
- en: Despite its potential, there has been a limited exploration of continual learning
    in medical contexts. Current research has primarily focused on this paradigm in
    specific areas such as image segmentation [[388](#bib.bib388), [389](#bib.bib389)],
    disease classification [[390](#bib.bib390), [391](#bib.bib391), [392](#bib.bib392),
    [393](#bib.bib393)], and domain adaptation [[394](#bib.bib394), [395](#bib.bib395)].
    Moreover, there is currently no unified framework in continual learning capable
    of accommodating diverse types of annotations in medical applications. We look
    forward to the development of an integrated framework for continual learning that
    can encompass the various settings and challenges highlighted in this paper. Such
    a framework would significantly advance the application of continual learning
    in the medical field, providing a more comprehensive approach to managing evolving
    datasets and diverse annotations.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有潜力，但在医疗领域对持续学习的探索仍然有限。目前的研究主要集中在特定领域，例如图像分割 [[388](#bib.bib388), [389](#bib.bib389)]、疾病分类
    [[390](#bib.bib390), [391](#bib.bib391), [392](#bib.bib392), [393](#bib.bib393)]
    和领域适应 [[394](#bib.bib394), [395](#bib.bib395)]。此外，目前尚无统一的持续学习框架能够适应医疗应用中的各种注释类型。我们期待一个集成的持续学习框架的发展，它能够涵盖本文中突出显示的各种设置和挑战。这样的框架将显著推进持续学习在医疗领域的应用，提供更全面的方法来管理不断变化的数据集和多样的注释。
- en: 7.2 Incorporating domain knowledge
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 融入领域知识
- en: Incorporating additional information beyond the existing medical datasets has
    emerged as a more promising strategy to tackle the issue of limited-sized medical
    datasets. Within this context, domain knowledge plays a vital role in guiding
    the development of effective deep learning algorithms for MIA. While many models
    used in medical vision are adapted from those designed for natural images, it’s
    worth noting that medical images typically present more complex challenges, such
    as high inter-class similarity, a scarcity of labeled data, and label noise. When
    applied effectively, domain knowledge can mitigate these challenges with reduced
    time and computational requirements. Integrating domain knowledge into deep learning
    algorithms can be achieved by leveraging anatomical details from MRI and CT images
    [[83](#bib.bib83)], exploiting multi-instance data from the same patient [[5](#bib.bib5)],
    incorporating patient metadata [[92](#bib.bib92)], utilizing radiomic features,
    and considering textual reports that accompany the images [[396](#bib.bib396)].
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有医疗数据集之外引入额外信息已经成为应对医疗数据集有限规模问题的更有前景的策略。在这一背景下，领域知识在指导有效的深度学习算法开发中扮演着至关重要的角色。虽然许多用于医疗视觉的模型是从为自然图像设计的模型中改编而来的，但值得注意的是，医疗图像通常呈现出更复杂的挑战，例如高类别间相似性、标记数据稀缺和标签噪声。有效应用领域知识可以在减少时间和计算需求的情况下缓解这些挑战。将领域知识整合到深度学习算法中可以通过利用MRI和CT图像的解剖细节[[83](#bib.bib83)]、利用同一患者的多实例数据[[5](#bib.bib5)]、整合患者元数据[[92](#bib.bib92)]、利用放射组学特征以及考虑与图像一起提供的文本报告[[396](#bib.bib396)]来实现。
- en: While the utilization of medical domain knowledge in deep learning models is
    a prevalent practice, it is not without its challenges. These challenges involve
    the selection, representation, and integration methods for medical domain knowledge
    [[397](#bib.bib397)]. Identifying such knowledge is a complex task primarily because
    the experiences of medical professionals tend to be subjective and ambiguous.
    It’s often difficult for medical practitioners to provide precise and objective
    descriptions of the experiences they draw upon to complete specific tasks. Currently,
    the identification of medical domain knowledge relies on manual processes, and
    there is no existing method for automatically identifying medical domain knowledge
    within a given field. Medical professionals typically draw from various types
    of domain knowledge simultaneously. Furthermore, Most existing approaches, however,
    incorporate only a single type or a few types of medical domain knowledge, often
    from the same modality. Consequently, simultaneously integrating multiple forms
    of medical domain knowledge has the potential to provide more robust support for
    deep learning models across various medical applications.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在深度学习模型中使用医疗领域知识是一种普遍做法，但它并非没有挑战。这些挑战涉及到医疗领域知识的选择、表示和整合方法[[397](#bib.bib397)]。识别这些知识是一项复杂的任务，主要因为医疗专业人员的经验往往是主观和模糊的。医疗从业者很难提供精确和客观的描述来完成特定任务。当前，医疗领域知识的识别依赖于人工过程，并且没有现有的方法可以自动识别特定领域内的医疗领域知识。医疗专业人员通常同时利用各种类型的领域知识。然而，现有的大多数方法通常只整合了单一类型或几种类型的医疗领域知识，通常来自相同的模态。因此，同时整合多种形式的医疗领域知识有可能为深度学习模型在各种医疗应用中提供更强有力的支持。
- en: 7.3 Label-efficient learning by vision transformers
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 通过视觉变换器进行标签高效学习
- en: Current label-efficient segmentation techniques primarily rely on convolutional
    neural networks (CNNs). However, there has been a recent transformation in computer
    vision, driven by the introduction of the transformer module [[398](#bib.bib398)].
    This innovation has given rise to vision transformers (ViT) [[399](#bib.bib399)]
    and their adaptations [[400](#bib.bib400)], leading to significant advancements
    in numerous medical applications, including segmentation, detection, and classification
    tasks. Transformer-based models can achieve higher performance when trained on
    extensive datasets, but their effectiveness diminishes when data or annotations
    are scarce. To overcome this challenge, self-supervised transformers offer a promising
    solution. By utilizing unlabeled data and employing proxy tasks like contrastive
    learning and reconstruction, these transformers can enhance their representation
    learning capabilities [[401](#bib.bib401)]. For instance, the Self-Supervised
    SwinUNETR [[99](#bib.bib99)] and unified pre-training [[402](#bib.bib402)] frameworks
    in the medical domain demonstrate that training with large-scale unlabeled 2D
    or 3D images is advantageous for fine-tuning models with smaller datasets. However,
    it’s worth noting that the utilization of pre-training can be computationally
    demanding. Future research directions may aim to simplify and assess the efficiency
    of the pre-training framework, especially regarding its applicability to smaller
    datasets.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的标签高效分割技术主要依赖于卷积神经网络（CNN）。然而，计算机视觉领域最近发生了变化，这一变化由transformer模块的引入推动[[398](#bib.bib398)]。这一创新催生了视觉transformers（ViT）[[399](#bib.bib399)]及其变体[[400](#bib.bib400)]，在许多医疗应用中，包括分割、检测和分类任务中，取得了显著进展。基于transformer的模型在大规模数据集上训练时可以实现更高的性能，但当数据或注释稀缺时，其效果会减弱。为了解决这个挑战，自监督transformers提供了一个有前途的解决方案。通过利用未标记数据和采用对比学习和重建等代理任务，这些transformers可以增强其表示学习能力[[401](#bib.bib401)]。例如，医疗领域的自监督SwinUNETR[[99](#bib.bib99)]和统一预训练[[402](#bib.bib402)]框架表明，使用大规模未标记的2D或3D图像进行训练对于用较小数据集微调模型是有利的。然而，值得注意的是，预训练的利用可能会计算开销大。未来的研究方向可能会着重简化和评估预训练框架的效率，特别是其在较小数据集上的适用性。
- en: 7.4 Flexible target model design
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 灵活的目标模型设计
- en: To develop better architectures for data-efficient deep learning architectures,
    one promising avenue is the field of automated architectural engineering. Presently,
    the architectures predominantly in use are crafted by human experts through iterative
    processes that are susceptible to errors. To circumvent the need for manual design,
    researchers have put forward the concept of automating architectural engineering,
    with one relevant domain being neural architecture search (NAS), introduced by
    Zoph and Le [[403](#bib.bib403)]. However, it’s essential to note that the majority
    of NAS investigations have been concentrated on image classification tasks [[404](#bib.bib404)].
    Regrettably, this focus has yet to yield truly transformative models capable of
    instigating fundamental shifts [[405](#bib.bib405)]. Nevertheless, the exploration
    of NAS for data-efficient learning in MIA remains a promising avenue.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发更好的数据高效深度学习架构，一个有前景的方向是自动化架构工程领域。目前，主要使用的架构是由人类专家通过迭代过程创建的，这些过程容易出现错误。为了避免手动设计的需求，研究人员提出了自动化架构工程的概念，其中一个相关领域是神经架构搜索（NAS），由Zoph和Le提出[[403](#bib.bib403)]。然而，值得注意的是，大多数NAS研究集中在图像分类任务上[[404](#bib.bib404)]。遗憾的是，这种关注尚未产生真正能够引发根本性变化的模型[[405](#bib.bib405)]。尽管如此，在MIA中探索NAS以实现数据高效学习仍然是一个有前景的方向。
- en: 7.5 Federated learning
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 联邦学习
- en: 'Modern healthcare systems collect significant amounts of medical data, yet
    the complete utilization of this data by deep learning is hindered. This limitation
    stems from the data being isolated within silos and privacy concerns that limit
    data access [[20](#bib.bib20)]. To tackle this challenge, federated learning (FL)
    emerges as a learning paradigm seeking to address the problem of data governance
    and privacy. It achieves this by training algorithms collaboratively without the
    need to exchange the data itself [[406](#bib.bib406)]. FL preserves data privacy
    while collectively improving model efficiency, making it a valuable tool for data-efficient
    deep learning in MIA. FL has produced valuable outcomes in the MIA domain [[407](#bib.bib407),
    [408](#bib.bib408), [409](#bib.bib409)]. Nevertheless, existing FL algorithms
    are predominantly trained using supervised methods. When implementing FL in real-world
    MIA situations, a critical issue arises: *label scarcity* can occur in local healthcare
    datasets. Different medical centers may have varying degrees of missing labels,
    or the label granularity may differ. A potential avenue for research is the development
    of label-efficient federated learning techniques to tackle this challenge.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 现代医疗系统收集了大量的医疗数据，但深度学习对这些数据的充分利用受到阻碍。这种限制源于数据被孤立在不同的数据孤岛中，以及隐私问题限制了数据访问[[20](#bib.bib20)]。为了应对这一挑战，联邦学习（FL）作为一种学习范式，旨在解决数据治理和隐私问题。它通过协作训练算法而不需要交换数据本身[[406](#bib.bib406)]，从而实现这一目标。FL在保护数据隐私的同时，集体提高了模型效率，使其成为在MIA中数据高效深度学习的宝贵工具。FL在MIA领域取得了宝贵的成果[[407](#bib.bib407),
    [408](#bib.bib408), [409](#bib.bib409)]。然而，现有的FL算法主要使用监督方法进行训练。在实际的MIA场景中实施FL时，一个关键问题是：*标签稀缺*可能发生在本地医疗数据集中。不同的医疗中心可能有不同程度的标签缺失，或者标签的粒度可能不同。一个潜在的研究方向是开发标签高效的联邦学习技术以应对这一挑战。
- en: 7.6 Data-efficient learning with text supervision
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6 文本监督的数据高效学习
- en: Text supervision involves using textual descriptions or labels as additional
    sources of information during training. This can include utilizing clinical reports,
    medical terminology, or textual metadata associated with images or patient records.
    By incorporating text supervision, MIA models can learn to associate medical text
    with visual patterns in images, facilitating improved generalization and a deeper
    understanding of medical data. Some studies have explored this approach [[396](#bib.bib396),
    [410](#bib.bib410)]. We encourage future investigations to further explore and
    expand upon this area of study.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 文本监督涉及在训练过程中使用文本描述或标签作为额外的信息来源。这可以包括利用临床报告、医学术语或与图像或患者记录相关的文本元数据。通过结合文本监督，MIA模型可以学习将医学文本与图像中的视觉模式关联，从而促进更好的泛化和对医学数据的更深入理解。一些研究已经探索了这种方法[[396](#bib.bib396),
    [410](#bib.bib410)]。我们鼓励未来的研究进一步探索和扩展这一领域的研究。
- en: 8 Conclusion
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: The challenge of acquiring high-quality labels remains a significant hurdle
    for supervised learning in Medical Image Analysis (MIA). This challenge has fueled
    interest in alternative approaches that enhance labeling efficiency to reduce
    the labeled data requirement. In recent years, extensive research efforts have
    been dedicated to advancing data-efficient learning within the realm of medical
    images, resulting in the development of numerous techniques applicable across
    diverse application domains. In this paper, we have provided a comprehensive review
    that explores the recent progress in data-efficient deep learning for MIA. Specifically,
    we conducted a thorough examination of deep learning-based data-efficient methodologies
    and categorized them into five distinct groups. These categorizations are rooted
    in the varying degrees of supervision they depend on, covering a spectrum from
    scenarios with no supervision to those involving inexact, incomplete, inaccurate,
    and only limited supervision. Finally, we highlight several potential future directions
    for research and development in this area. We hope that this survey serves as
    a valuable resource, offering insights into the current state of data-efficient
    deep learning in medical imaging and inspiring further progress in this domain.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 获取高质量标签的挑战仍然是医学图像分析（MIA）中监督学习的一个重要障碍。这个挑战激发了对提高标注效率的替代方法的兴趣，以减少对标注数据的需求。近年来，广泛的研究工作致力于在医学图像领域推进数据高效学习，开发了许多适用于不同应用领域的技术。在这篇论文中，我们提供了对数据高效深度学习在MIA领域最新进展的全面综述。具体来说，我们对基于深度学习的数据高效方法进行了深入的研究，并将其分为五个不同的类别。这些分类根植于它们所依赖的监督程度的不同，涵盖了从无监督到不准确、不完整、有限监督的不同情况。最后，我们突出了一些未来研究和开发的潜在方向。我们希望这项调查能成为一个宝贵的资源，为医学影像中的数据高效深度学习的现状提供见解，并激发该领域的进一步进展。
- en: References
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] E. J. Topol, High-performance medicine: the convergence of human and artificial
    intelligence, Nature medicine 25 (1) (2019) 44–56.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] E. J. Topol, 高性能医学：人类与人工智能的融合，《自然医学》25（1）（2019）44–56。'
- en: '[2] N. Hashimoto, D. Fukushima, R. Koga, Y. Takagi, K. Ko, K. Kohno, M. Nakaguro,
    S. Nakamura, H. Hontani, I. Takeuchi, Multi-scale domain-adversarial multiple-instance
    cnn for cancer subtype classification with unannotated histopathological images,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2020, pp. 3852–3861.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] N. Hashimoto, D. Fukushima, R. Koga, Y. Takagi, K. Ko, K. Kohno, M. Nakaguro,
    S. Nakamura, H. Hontani, I. Takeuchi, 多尺度领域对抗多实例CNN用于无注释组织病理图像的癌症亚型分类，见：IEEE/CVF计算机视觉与模式识别会议论文集，2020，第3852–3861页。'
- en: '[3] Z. Shao, H. Bian, Y. Chen, Y. Wang, J. Zhang, X. Ji, et al., Transmil:
    Transformer based correlated multiple instance learning for whole slide image
    classification, Advances in neural information processing systems 34 (2021) 2136–2147.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Z. Shao, H. Bian, Y. Chen, Y. Wang, J. Zhang, X. Ji, 等，Transmil：基于Transformer的相关多实例学习用于全幻灯片图像分类，《神经信息处理系统进展》34（2021）2136–2147。'
- en: '[4] C. Xue, Q. Dou, X. Shi, H. Chen, P.-A. Heng, Robust learning at noisy labeled
    medical images: Applied to skin lesion classification, in: 2019 IEEE 16th International
    Symposium on Biomedical Imaging (ISBI 2019), IEEE, 2019, pp. 1280–1283.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] C. Xue, Q. Dou, X. Shi, H. Chen, P.-A. Heng, 在噪声标记医学图像上的稳健学习：应用于皮肤病变分类，见：2019年IEEE第16届生物医学成像国际研讨会（ISBI
    2019），IEEE，2019，第1280–1283页。'
- en: '[5] S. Azizi, B. Mustafa, F. Ryan, Z. Beaver, J. Freyberg, J. Deaton, A. Loh,
    A. Karthikesalingam, S. Kornblith, T. Chen, et al., Big self-supervised models
    advance medical image classification, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2021, pp. 3478–3488.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Azizi, B. Mustafa, F. Ryan, Z. Beaver, J. Freyberg, J. Deaton, A. Loh,
    A. Karthikesalingam, S. Kornblith, T. Chen, 等，大型自监督模型推动医学图像分类的发展，见：IEEE/CVF国际计算机视觉会议论文集，2021，第3478–3488页。'
- en: '[6] K. Yan, J. Cai, D. Jin, S. Miao, D. Guo, A. P. Harrison, Y. Tang, J. Xiao,
    J. Lu, L. Lu, Sam: Self-supervised learning of pixel-wise anatomical embeddings
    in radiological images, IEEE Transactions on Medical Imaging 41 (10) (2022) 2658–2669.
    [doi:10.1109/TMI.2022.3169003](https://doi.org/10.1109/TMI.2022.3169003).'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] K. Yan, J. Cai, D. Jin, S. Miao, D. Guo, A. P. Harrison, Y. Tang, J. Xiao,
    J. Lu, L. Lu, Sam：自监督学习的放射图像像素级解剖嵌入，IEEE医学影像学会会刊 41（10）（2022）2658–2669。 [doi:10.1109/TMI.2022.3169003](https://doi.org/10.1109/TMI.2022.3169003)。'
- en: '[7] G. Campanella, M. G. Hanna, L. Geneslaw, A. Miraflor, V. Werneck Krauss Silva,
    K. J. Busam, E. Brogi, V. E. Reuter, D. S. Klimstra, T. J. Fuchs, Clinical-grade
    computational pathology using weakly supervised deep learning on whole slide images,
    Nature medicine 25 (8) (2019) 1301–1309.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Campanella, M. G. Hanna, L. Geneslaw, A. Miraflor, V. Werneck Krauss
    Silva, K. J. Busam, E. Brogi, V. E. Reuter, D. S. Klimstra, T. J. Fuchs, 利用弱监督深度学习在全切片图像上的临床级计算病理学，《自然医学》25
    (8) (2019) 1301–1309。'
- en: '[8] H. Zhu, J. Shi, J. Wu, Pick-and-learn: Automatic quality evaluation for
    noisy-labeled image segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October
    13–17, 2019, Proceedings, Part VI 22, Springer, 2019, pp. 576–584.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Zhu, J. Shi, J. Wu, Pick-and-learn: 自动质量评估噪声标签图像分割，发表于：医学图像计算与计算机辅助干预–MICCAI
    2019: 第22届国际会议，中国深圳，2019年10月13–17日，论文集，第六部分22，Springer，2019年，第576–584页。'
- en: '[9] A. Taleb, W. Loetzsch, N. Danz, J. Severin, T. Gaertner, B. Bergner, C. Lippert,
    3d self-supervised methods for medical imaging, Advances in neural information
    processing systems 33 (2020) 18158–18172.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Taleb, W. Loetzsch, N. Danz, J. Severin, T. Gaertner, B. Bergner, C.
    Lippert, 医学影像的3D自监督方法，《神经信息处理系统进展》33 (2020) 18158–18172。'
- en: '[10] Á. S. Hervella, J. Rouco, J. Novo, M. Ortega, Learning the retinal anatomy
    from scarce annotated data using self-supervised multimodal reconstruction, Applied
    Soft Computing 91 (2020) 106210.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Á. S. Hervella, J. Rouco, J. Novo, M. Ortega, 从稀缺标注数据中学习视网膜解剖结构，利用自监督的多模态重建，《应用软计算》91
    (2020) 106210。'
- en: '[11] C. Chen, Q. Dou, H. Chen, J. Qin, P.-A. Heng, Synergistic image and feature
    adaptation: Towards cross-modality domain adaptation for medical image segmentation,
    in: Proceedings of the AAAI conference on artificial intelligence, Vol. 33, 2019,
    pp. 865–872.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] C. Chen, Q. Dou, H. Chen, J. Qin, P.-A. Heng, 协同图像与特征适应：面向医学图像分割的跨模态领域适应，发表于：AAAI人工智能会议论文集，第33卷，2019年，第865–872页。'
- en: '[12] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, C. I. Sánchez, A survey on deep learning
    in medical image analysis, Medical image analysis 42 (2017) 60–88.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, C. I. Sánchez, 深度学习在医学图像分析中的综述，《医学图像分析》42
    (2017) 60–88。'
- en: '[13] X. Chen, X. Wang, K. Zhang, K.-M. Fung, T. C. Thai, K. Moore, R. S. Mannel,
    H. Liu, B. Zheng, Y. Qiu, Recent advances and clinical applications of deep learning
    in medical image analysis, Medical Image Analysis 79 (2022) 102444.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Chen, X. Wang, K. Zhang, K.-M. Fung, T. C. Thai, K. Moore, R. S. Mannel,
    H. Liu, B. Zheng, Y. Qiu, 深度学习在医学图像分析中的最新进展及临床应用，《医学图像分析》79 (2022) 102444。'
- en: '[14] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for
    biomedical image segmentation, in: Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18, Springer, 2015, pp. 234–241.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] O. Ronneberger, P. Fischer, T. Brox, U-net: 用于生物医学图像分割的卷积网络，发表于：医学图像计算与计算机辅助干预–MICCAI
    2015: 第18届国际会议，德国慕尼黑，2015年10月5-9日，论文集，第三部分18，Springer，2015年，第234–241页。'
- en: '[15] M. J. Willemink, W. A. Koszek, C. Hardell, J. Wu, D. Fleischmann, H. Harvey,
    L. R. Folio, R. M. Summers, D. L. Rubin, M. P. Lungren, Preparing medical imaging
    data for machine learning, Radiology 295 (1) (2020) 4–15.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. J. Willemink, W. A. Koszek, C. Hardell, J. Wu, D. Fleischmann, H. Harvey,
    L. R. Folio, R. M. Summers, D. L. Rubin, M. P. Lungren, 准备医学影像数据以用于机器学习，《放射学》295
    (1) (2020) 4–15。'
- en: '[16] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros, et al., Development and validation
    of a deep learning algorithm for detection of diabetic retinopathy in retinal
    fundus photographs, jama 316 (22) (2016) 2402–2410.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros, 等，基于深度学习算法的糖尿病视网膜病变检测的开发与验证，《jama》316
    (22) (2016) 2402–2410。'
- en: '[17] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S. Thrun,
    Dermatologist-level classification of skin cancer with deep neural networks, nature
    542 (7639) (2017) 115–118.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S.
    Thrun, 通过深度神经网络进行皮肤癌的皮肤科医生级分类，《自然》542 (7639) (2017) 115–118。'
- en: '[18] D. Gurari, D. Theriault, M. Sameki, B. Isenberg, T. A. Pham, A. Purwada,
    P. Solski, M. Walker, C. Zhang, J. Y. Wong, M. Betke, How to collect segmentations
    for biomedical images? a benchmark evaluating the performance of experts, crowdsourced
    non-experts, and algorithms, in: 2015 IEEE Winter Conference on Applications of
    Computer Vision, 2015, pp. 1169–1176. [doi:10.1109/WACV.2015.160](https://doi.org/10.1109/WACV.2015.160).'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] D. Gurari, D. Theriault, M. Sameki, B. Isenberg, T. A. Pham, A. Purwada,
    P. Solski, M. Walker, C. Zhang, J. Y. Wong, M. Betke, 如何收集生物医学图像的分割？一个评估专家、众包非专家和算法性能的基准，在：2015
    IEEE Winter Conference on Applications of Computer Vision, 2015, pp. 1169–1176.
    [doi:10.1109/WACV.2015.160](https://doi.org/10.1109/WACV.2015.160)。'
- en: '[19] S. Albarqouni, C. Baur, F. Achilles, V. Belagiannis, S. Demirci, N. Navab,
    Aggnet: deep learning from crowds for mitosis detection in breast cancer histology
    images, IEEE transactions on medical imaging 35 (5) (2016) 1313–1321.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Albarqouni, C. Baur, F. Achilles, V. Belagiannis, S. Demirci, N. Navab,
    Aggnet：从人群中学习用于乳腺癌组织学图像的有丝分裂检测，IEEE transactions on medical imaging 35 (5) (2016)
    1313–1321。'
- en: '[20] P. Rajpurkar, E. Chen, O. Banerjee, E. J. Topol, Ai in health and medicine,
    Nature medicine 28 (1) (2022) 31–38.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] P. Rajpurkar, E. Chen, O. Banerjee, E. J. Topol, 健康与医学中的人工智能，Nature medicine
    28 (1) (2022) 31–38。'
- en: '[21] V. Cheplygina, M. de Bruijne, J. P. Pluim, Not-so-supervised: a survey
    of semi-supervised, multi-instance, and transfer learning in medical image analysis,
    Medical image analysis 54 (2019) 280–296.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] V. Cheplygina, M. de Bruijne, J. P. Pluim, 不完全监督：医学图像分析中的半监督、多实例和迁移学习调查，Medical
    image analysis 54 (2019) 280–296。'
- en: '[22] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, X. Ding, Embracing
    imperfect datasets: A review of deep learning solutions for medical image segmentation,
    Medical Image Analysis 63 (2020) 101693.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, X. Ding, 拥抱不完美的数据集：医学图像分割的深度学习解决方案综述，Medical
    Image Analysis 63 (2020) 101693。'
- en: '[23] C. Jin, Z. Guo, Y. Lin, L. Luo, H. Chen, Label-efficient deep learning
    in medical image analysis: Challenges and future directions, arXiv preprint arXiv:2303.12484
    (2023).'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] C. Jin, Z. Guo, Y. Lin, L. Luo, H. Chen, 标签效率深度学习在医学图像分析中的挑战与未来方向，arXiv
    preprint arXiv:2303.12484 (2023)。'
- en: '[24] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto, T. Kobayashi, K.-i.
    Komatsu, M. Matsui, H. Fujita, Y. Kodera, K. Doi, Development of a digital image
    database for chest radiographs with and without a lung nodule: receiver operating
    characteristic analysis of radiologists’ detection of pulmonary nodules, American
    Journal of Roentgenology 174 (1) (2000) 71–74.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto, T. Kobayashi, K.-i.
    Komatsu, M. Matsui, H. Fujita, Y. Kodera, K. Doi, 胸部X光图像数字数据库的开发，包括有和没有肺结节的图像：放射科医生对肺结节的检测的受试者工作特征分析，American
    Journal of Roentgenology 174 (1) (2000) 71–74。'
- en: '[25] C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thompson, G. Alexander,
    D. Harvey, B. Borowski, P. J. Britson, J. L. Whitwell, C. Ward, et al., The alzheimer’s
    disease neuroimaging initiative (adni): Mri methods, Journal of Magnetic Resonance
    Imaging: An Official Journal of the International Society for Magnetic Resonance
    in Medicine 27 (4) (2008) 685–691.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thompson, G. Alexander,
    D. Harvey, B. Borowski, P. J. Britson, J. L. Whitwell, C. Ward, 等，阿尔茨海默病神经影像学计划（ADNI）：MRI方法，Journal
    of Magnetic Resonance Imaging: An Official Journal of the International Society
    for Magnetic Resonance in Medicine 27 (4) (2008) 685–691。'
- en: '[26] M. W. Weiner, D. P. Veitch, P. S. Aisen, L. A. Beckett, N. J. Cairns,
    R. C. Green, D. Harvey, C. R. Jack Jr, W. Jagust, J. C. Morris, et al., The alzheimer’s
    disease neuroimaging initiative 3: Continued innovation for clinical trial improvement,
    Alzheimer’s & Dementia 13 (5) (2017) 561–571.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. W. Weiner, D. P. Veitch, P. S. Aisen, L. A. Beckett, N. J. Cairns,
    R. C. Green, D. Harvey, C. R. Jack Jr, W. Jagust, J. C. Morris, 等，阿尔茨海默病神经影像学计划3：临床试验改进的持续创新，Alzheimer’s
    & Dementia 13 (5) (2017) 561–571。'
- en: '[27] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
    Y. Burren, N. Porz, J. Slotboom, R. Wiest, et al., The multimodal brain tumor
    image segmentation benchmark (brats), IEEE transactions on medical imaging 34 (10)
    (2014) 1993–2024.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
    Y. Burren, N. Porz, J. Slotboom, R. Wiest, 等，多模态脑肿瘤图像分割基准（BRATS），IEEE transactions
    on medical imaging 34 (10) (2014) 1993–2024。'
- en: '[28] A. Makropoulos, E. C. Robinson, A. Schuh, R. Wright, S. Fitzgibbon, J. Bozek,
    S. J. Counsell, J. Steinweg, K. Vecchiato, J. Passerat-Palmbach, et al., The developing
    human connectome project: A minimal processing pipeline for neonatal cortical
    surface reconstruction, Neuroimage 173 (2018) 88–112.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Makropoulos, E. C. Robinson, A. Schuh, R. Wright, S. Fitzgibbon, J.
    Bozek, S. J. Counsell, J. Steinweg, K. Vecchiato, J. Passerat-Palmbach, 等，发展中的人类连接组计划：新生儿皮层表面重建的最简处理流程，Neuroimage
    173 (2018) 88–112。'
- en: '[29] R. Souza, O. Lucena, J. Garrafa, D. Gobbi, M. Saluzzi, S. Appenzeller,
    L. Rittner, R. Frayne, R. Lotufo, An open, multi-vendor, multi-field-strength
    brain mr dataset and analysis of publicly available skull stripping methods agreement,
    NeuroImage 170 (2018) 482–494.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] R. Souza, O. Lucena, J. Garrafa, D. Gobbi, M. Saluzzi, S. Appenzeller,
    L. Rittner, R. Frayne, R. Lotufo, 一个开放的、多供应商的、多场强的脑部MR数据集及公开可用的颅骨剥离方法的一致性分析，《NeuroImage》170
    (2018) 482–494。'
- en: '[30] H. J. Kuijf, J. M. Biesbroek, J. De Bresser, R. Heinen, S. Andermatt,
    M. Bento, M. Berseth, M. Belyaev, M. J. Cardoso, A. Casamitjana, et al., Standardized
    assessment of automatic segmentation of white matter hyperintensities and results
    of the wmh segmentation challenge, IEEE transactions on medical imaging 38 (11)
    (2019) 2556–2568.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. J. Kuijf, J. M. Biesbroek, J. De Bresser, R. Heinen, S. Andermatt,
    M. Bento, M. Berseth, M. Belyaev, M. J. Cardoso, A. Casamitjana, 等，自动分割白质高信号的标准化评估及WMH分割挑战赛结果，《IEEE
    Transactions on Medical Imaging》38 (11) (2019) 2556–2568。'
- en: '[31] C.-G. Yan, X. Chen, L. Li, F. X. Castellanos, T.-J. Bai, Q.-J. Bo, J. Cao,
    G.-M. Chen, N.-X. Chen, W. Chen, et al., Reduced default mode network functional
    connectivity in patients with recurrent major depressive disorder, Proceedings
    of the National Academy of Sciences 116 (18) (2019) 9078–9083.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C.-G. Yan, X. Chen, L. Li, F. X. Castellanos, T.-J. Bai, Q.-J. Bo, J.
    Cao, G.-M. Chen, N.-X. Chen, W. Chen, 等，复发性重度抑郁症患者的默认模式网络功能连接减少，《Proceedings of
    the National Academy of Sciences》116 (18) (2019) 9078–9083。'
- en: '[32] X. Zhuang, J. Shen, Multi-scale patch and multi-modality atlases for whole
    heart segmentation of mri, Medical image analysis 31 (2016) 77–87.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] X. Zhuang, J. Shen, 多尺度补丁和多模态图谱用于MRI的全心分割，《Medical image analysis》31 (2016)
    77–87。'
- en: '[33] X. Zhuang, L. Li, C. Payer, D. vStern, M. Urschler, M. P. Heinrich, J. Oster,
    C. Wang, Ö. Smedby, C. Bian, et al., Evaluation of algorithms for multi-modality
    whole heart segmentation: an open-access grand challenge, Medical image analysis
    58 (2019) 101537.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] X. Zhuang, L. Li, C. Payer, D. vStern, M. Urschler, M. P. Heinrich, J.
    Oster, C. Wang, Ö. Smedby, C. Bian, 等，多模态全心分割算法评估：一个开放访问的大挑战，《Medical image analysis》58
    (2019) 101537。'
- en: '[34] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng,
    I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester, et al., Deep learning techniques
    for automatic mri cardiac multi-structures segmentation and diagnosis: is the
    problem solved?, IEEE transactions on medical imaging 37 (11) (2018) 2514–2525.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng,
    I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester, 等，自动MRI心脏多结构分割和诊断的深度学习技术：问题是否解决？，《IEEE
    Transactions on Medical Imaging》37 (11) (2018) 2514–2525。'
- en: '[35] Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng, S. Vesal, N. Ravikumar,
    A. Maier, X. Yang, et al., A global benchmark of algorithms for segmenting the
    left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging,
    Medical image analysis 67 (2021) 101832.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng, S. Vesal, N. Ravikumar,
    A. Maier, X. Yang, 等，基于晚期钆增强心脏磁共振成像的左心房分割算法的全球基准，《Medical image analysis》67 (2021)
    101832。'
- en: '[36] X. Zhuang, Multivariate mixture model for cardiac segmentation from multi-sequence
    mri, in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2016, pp. 581–588.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] X. Zhuang, 多变量混合模型用于从多序列MRI中进行心脏分割，载于：医学图像计算与计算机辅助干预国际会议，Springer，2016，第581–588页。'
- en: '[37] V. M. Campello, P. Gkontra, C. Izquierdo, C. Martin-Isla, A. Sojoudi,
    P. M. Full, K. Maier-Hein, Y. Zhang, Z. He, J. Ma, et al., Multi-centre, multi-vendor
    and multi-disease cardiac segmentation: the m&ms challenge, IEEE Transactions
    on Medical Imaging 40 (12) (2021) 3543–3554.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] V. M. Campello, P. Gkontra, C. Izquierdo, C. Martin-Isla, A. Sojoudi,
    P. M. Full, K. Maier-Hein, Y. Zhang, Z. He, J. Ma, 等，多中心、多供应商和多疾病的心脏分割：M&MS挑战赛，《IEEE
    Transactions on Medical Imaging》40 (12) (2021) 3543–3554。'
- en: '[38] J. Sivaswamy, S. Krishnadas, G. D. Joshi, M. Jain, A. U. S. Tabish, Drishti-gs:
    Retinal image dataset for optic nerve head (onh) segmentation, in: 2014 IEEE 11th
    international symposium on biomedical imaging (ISBI), IEEE, 2014, pp. 53–56.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Sivaswamy, S. Krishnadas, G. D. Joshi, M. Jain, A. U. S. Tabish, Drishti-gs:
    视网膜图像数据集用于视神经头（ONH）分割，载于：2014 IEEE第11届国际生物医学影像研讨会（ISBI），IEEE，2014，第53–56页。'
- en: '[39] H. Bogunović, F. Venhuizen, S. Klimscha, S. Apostolopoulos, A. Bab-Hadiashar,
    U. Bagci, M. F. Beg, L. Bekalo, Q. Chen, C. Ciller, et al., Retouch: The retinal
    oct fluid detection and segmentation benchmark and challenge, IEEE transactions
    on medical imaging 38 (8) (2019) 1858–1874.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] H. Bogunović, F. Venhuizen, S. Klimscha, S. Apostolopoulos, A. Bab-Hadiashar,
    U. Bagci, M. F. Beg, L. Bekalo, Q. Chen, C. Ciller, 等，Retouch：视网膜OCT液体检测和分割基准与挑战，《IEEE
    Transactions on Medical Imaging》38 (8) (2019) 1858–1874。'
- en: '[40] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim, H. Liang, S. L. Baxter,
    A. McKeown, G. Yang, X. Wu, F. Yan, et al., Identifying medical diagnoses and
    treatable diseases by image-based deep learning, cell 172 (5) (2018) 1122–1131.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim, H. Liang, S. L. Baxter,
    A. McKeown, G. Yang, X. Wu, F. Yan 等人，通过基于图像的深度学习识别医学诊断和可治疗疾病，《细胞》172 (5) (2018)
    1122–1131。'
- en: '[41] H. Fu, F. Li, J. I. Orlando, H. Bogunović, X. Sun, J. Liao, Y. Xu, S. Zhang,
    X. Zhang, [Palm: Pathologic myopia challenge](https://dx.doi.org/10.21227/55pk-8z03)
    (2019). [doi:10.21227/55pk-8z03](https://doi.org/10.21227/55pk-8z03).'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Fu, F. Li, J. I. Orlando, H. Bogunović, X. Sun, J. Liao, Y. Xu, S.
    Zhang, X. Zhang，[Palm：病理性近视挑战](https://dx.doi.org/10.21227/55pk-8z03) (2019)。
    [doi:10.21227/55pk-8z03](https://doi.org/10.21227/55pk-8z03)。'
- en: URL [https://dx.doi.org/10.21227/55pk-8z03](https://dx.doi.org/10.21227/55pk-8z03)
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://dx.doi.org/10.21227/55pk-8z03](https://dx.doi.org/10.21227/55pk-8z03)
- en: '[42] J. I. Orlando, H. Fu, J. B. Breda, K. Van Keer, D. R. Bathula, A. Diaz-Pinto,
    R. Fang, P.-A. Heng, J. Kim, J. Lee, et al., Refuge challenge: A unified framework
    for evaluating automated methods for glaucoma assessment from fundus photographs,
    Medical image analysis 59 (2020) 101570.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. I. Orlando, H. Fu, J. B. Breda, K. Van Keer, D. R. Bathula, A. Diaz-Pinto,
    R. Fang, P.-A. Heng, J. Kim, J. Lee 等人，Refuge挑战：一个统一的框架用于评估自动化的青光眼评估方法，从眼底照片中获取，《医学图像分析》59
    (2020) 101570。'
- en: '[43] H. Fang, F. Li, H. Fu, X. Sun, X. Cao, F. Lin, J. Son, S. Kim, G. Quellec,
    S. Matta, et al., Adam challenge: Detecting age-related macular degeneration from
    fundus images, IEEE Transactions on Medical Imaging 41 (10) (2022) 2828–2847.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H. Fang, F. Li, H. Fu, X. Sun, X. Cao, F. Lin, J. Son, S. Kim, G. Quellec,
    S. Matta 等人，Adam挑战：从眼底图像中检测年龄相关性黄斑变性，《IEEE医学影像学汇刊》41 (10) (2022) 2828–2847。'
- en: '[44] S. Hu, Z. Liao, Y. Xia, Domain specific convolution and high frequency
    reconstruction based unsupervised domain adaptation for medical image segmentation,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 650–659.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Hu, Z. Liao, Y. Xia，基于领域特定卷积和高频重建的无监督领域适应用于医学图像分割，见：国际医学图像计算与计算机辅助干预会议，Springer，2022，第650–659页。'
- en: '[45] R. Ludovic, R. Daniel, L. Nicolas, K. Maria, I. Humayun, K. Jacques, C. Frédérique,
    G. Catherine, et al., Mitosis detection in breast cancer histological images an
    icpr 2012 contest, Journal of pathology informatics 4 (1) (2013) 8.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] R. Ludovic, R. Daniel, L. Nicolas, K. Maria, I. Humayun, K. Jacques, C.
    Frédérique, G. Catherine 等人，乳腺癌组织学图像中的有丝分裂检测：ICPR 2012竞赛，《病理信息学杂志》4 (1) (2013)
    8。'
- en: '[46] M. Veta, Y. J. Heng, N. Stathonikos, B. E. Bejnordi, F. Beca, T. Wollmann,
    K. Rohr, M. A. Shah, D. Wang, M. Rousson, et al., Predicting breast tumor proliferation
    from whole-slide images: the tupac16 challenge, Medical image analysis 54 (2019)
    111–121.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] M. Veta, Y. J. Heng, N. Stathonikos, B. E. Bejnordi, F. Beca, T. Wollmann,
    K. Rohr, M. A. Shah, D. Wang, M. Rousson 等人，从全切片图像中预测乳腺肿瘤增殖：TUPAC16挑战，《医学图像分析》54
    (2019) 111–121。'
- en: '[47] G. Litjens, P. Bandi, B. Ehteshami Bejnordi, O. Geessink, M. Balkenhol,
    P. Bult, A. Halilovic, M. Hermsen, R. van de Loo, R. Vogels, et al., 1399 h&e-stained
    sentinel lymph node sections of breast cancer patients: the camelyon dataset,
    GigaScience 7 (6) (2018) giy065.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] G. Litjens, P. Bandi, B. Ehteshami Bejnordi, O. Geessink, M. Balkenhol,
    P. Bult, A. Halilovic, M. Hermsen, R. van de Loo, R. Vogels 等人，1399份H&E染色乳腺癌患者的哨点淋巴结切片：Camelyon数据集，《GigaScience》7
    (6) (2018) giy065。'
- en: '[48] G. Aresta, T. Araújo, S. Kwok, S. S. Chennamsetty, M. Safwan, V. Alex,
    B. Marami, M. Prastawa, M. Chan, M. Donovan, et al., Bach: Grand challenge on
    breast cancer histology images, Medical image analysis 56 (2019) 122–139.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] G. Aresta, T. Araújo, S. Kwok, S. S. Chennamsetty, M. Safwan, V. Alex,
    B. Marami, M. Prastawa, M. Chan, M. Donovan 等人，Bach：乳腺癌组织学图像的重大挑战，《医学图像分析》56 (2019)
    122–139。'
- en: '[49] A. R. Saikia, K. Bora, L. B. Mahanta, A. K. Das, Comparative assessment
    of cnn architectures for classification of breast fnac images, Tissue and Cell
    57 (2019) 8–14.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. R. Saikia, K. Bora, L. B. Mahanta, A. K. Das，比较不同CNN架构在乳腺FNAC图像分类中的效果，《组织与细胞》57
    (2019) 8–14。'
- en: '[50] N. Petrick, S. Akbar, K. H. Cha, S. Nofech-Mozes, B. Sahiner, M. A. Gavrielides,
    J. Kalpathy-Cramer, K. Drukker, A. L. Martel, f. t. BreastPathQ Challenge Group,
    Spie-aapm-nci breastpathq challenge: an image analysis challenge for quantitative
    tumor cellularity assessment in breast cancer histology images following neoadjuvant
    treatment, Journal of Medical Imaging 8 (3) (2021) 034501–034501.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] N. Petrick, S. Akbar, K. H. Cha, S. Nofech-Mozes, B. Sahiner, M. A. Gavrielides,
    J. Kalpathy-Cramer, K. Drukker, A. L. Martel, f. t. BreastPathQ Challenge Group，SPIE-AAPM-NCI
    BreastPathQ挑战赛：一个图像分析挑战，用于量化肿瘤细胞密度的评估，针对乳腺癌组织学图像在新辅助治疗后的变化，《医学影像学杂志》8 (3) (2021)
    034501–034501。'
- en: '[51] H. A. Phoulady, P. R. Mouton, A new cervical cytology dataset for nucleus
    detection and image classification (cervix93) and methods for cervical nucleus
    detection, arXiv preprint arXiv:1811.09651 (2018).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. A. Phoulady, P. R. Mouton, 一个新的宫颈细胞学数据集用于细胞核检测和图像分类（cervix93）及宫颈细胞核检测的方法，arXiv预印本
    arXiv:1811.09651 (2018)。'
- en: '[52] E. Hussain, L. B. Mahanta, H. Borah, C. R. Das, Liquid based-cytology
    pap smear dataset for automated multi-class diagnosis of pre-cancerous and cervical
    cancer lesions, Data in brief 30 (2020) 105589.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] E. Hussain, L. B. Mahanta, H. Borah, C. R. Das, 基于液体的细胞学宫颈抹片数据集用于自动化多类诊断前癌病变和宫颈癌病变，《简要数据》30
    (2020) 105589。'
- en: '[53] A. E. Kavur, N. S. Gezer, M. Barıcs, S. Aslan, P.-H. Conze, V. Groza,
    D. D. Pham, S. Chatterjee, P. Ernst, S. Özkan, et al., Chaos challenge-combined
    (ct-mr) healthy abdominal organ segmentation, Medical Image Analysis 69 (2021)
    101950.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. E. Kavur, N. S. Gezer, M. Barıcs, S. Aslan, P.-H. Conze, V. Groza,
    D. D. Pham, S. Chatterjee, P. Ernst, S. Özkan, 等, 混合（CT-MR）健康腹部器官分割的混沌挑战，《医学图像分析》69
    (2021) 101950。'
- en: '[54] F. Su, Y. Sun, Y. Hu, P. Yuan, X. Wang, Q. Wang, J. Li, J.-F. Ji, Development
    and validation of a deep learning system for ascites cytopathology interpretation,
    Gastric Cancer 23 (2020) 1041–1050.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] F. Su, Y. Sun, Y. Hu, P. Yuan, X. Wang, Q. Wang, J. Li, J.-F. Ji, 深度学习系统在腹水细胞病理解释中的开发与验证，《胃癌》23
    (2020) 1041–1050。'
- en: '[55] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, A. Sethi, A dataset
    and a technique for generalized nuclear segmentation for computational pathology,
    IEEE transactions on medical imaging 36 (7) (2017) 1550–1560.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, A. Sethi, 一个数据集和用于计算病理学的通用细胞核分割技术，《IEEE医学影像学事务》36
    (7) (2017) 1550–1560。'
- en: '[56] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. Davidson,
    S. P. Pereira, M. J. Clarkson, D. C. Barratt, Automatic multi-organ segmentation
    on abdominal ct with dense v-networks, IEEE transactions on medical imaging 37 (8)
    (2018) 1822–1834.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B.
    Davidson, S. P. Pereira, M. J. Clarkson, D. C. Barratt, 基于密集V网络的腹部CT自动多脏器分割，《IEEE医学影像学事务》37
    (8) (2018) 1822–1834。'
- en: '[57] K. Yan, X. Wang, L. Lu, R. M. Summers, Deeplesion: automated mining of
    large-scale lesion annotations and universal lesion detection with deep learning,
    Journal of medical imaging 5 (3) (2018) 036501–036501.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Yan, X. Wang, L. Lu, R. M. Summers, Deeplesion：基于深度学习的大规模病变标注自动挖掘和通用病变检测，《医学影像学杂志》5
    (3) (2018) 036501–036501。'
- en: '[58] M. Aubreville, N. Stathonikos, C. A. Bertram, R. Klopfleisch, N. Ter Hoeve,
    F. Ciompi, F. Wilm, C. Marzahl, T. A. Donovan, A. Maier, et al., Mitosis domain
    generalization in histopathology images—the midog challenge, Medical Image Analysis
    84 (2023) 102699.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Aubreville, N. Stathonikos, C. A. Bertram, R. Klopfleisch, N. Ter Hoeve,
    F. Ciompi, F. Wilm, C. Marzahl, T. A. Donovan, A. Maier, 等, 组织病理图像中的有丝分裂域泛化——midog挑战，《医学图像分析》84
    (2023) 102699。'
- en: '[59] K. Sirinukunwattana, S. E. A. Raza, Y.-W. Tsang, D. R. Snead, I. A. Cree,
    N. M. Rajpoot, Locality sensitive deep learning for detection and classification
    of nuclei in routine colon cancer histology images, IEEE transactions on medical
    imaging 35 (5) (2016) 1196–1206.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] K. Sirinukunwattana, S. E. A. Raza, Y.-W. Tsang, D. R. Snead, I. A. Cree,
    N. M. Rajpoot, 用于例行结肠癌组织学图像中细胞核检测与分类的局部敏感深度学习，《IEEE医学影像学事务》35 (5) (2016) 1196–1206。'
- en: '[60] J. N. Kather, J. Krisam, P. Charoentong, T. Luedde, E. Herpel, C.-A. Weis,
    T. Gaiser, A. Marx, N. A. Valous, D. Ferber, et al., Predicting survival from
    colorectal cancer histology slides using deep learning: A retrospective multicenter
    study, PLoS medicine 16 (1) (2019) e1002730.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. N. Kather, J. Krisam, P. Charoentong, T. Luedde, E. Herpel, C.-A. Weis,
    T. Gaiser, A. Marx, N. A. Valous, D. Ferber, 等, 使用深度学习从结直肠癌组织切片中预测生存期：一项回顾性多中心研究，《PLoS医学》16
    (1) (2019) e1002730。'
- en: '[61] G. Litjens, R. Toth, W. Van De Ven, C. Hoeks, S. Kerkstra, B. Van Ginneken,
    G. Vincent, G. Guillard, N. Birbeck, J. Zhang, et al., Evaluation of prostate
    segmentation algorithms for mri: the promise12 challenge, Medical image analysis
    18 (2) (2014) 359–373.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] G. Litjens, R. Toth, W. Van De Ven, C. Hoeks, S. Kerkstra, B. Van Ginneken,
    G. Vincent, G. Guillard, N. Birbeck, J. Zhang, 等, 前列腺分割算法评估：Promise12挑战，《医学图像分析》18
    (2) (2014) 359–373。'
- en: '[62] E. Arvaniti, K. S. Fricker, M. Moret, N. Rupp, T. Hermanns, C. Fankhauser,
    N. Wey, P. J. Wild, J. H. Rueschoff, M. Claassen, Automated gleason grading of
    prostate cancer tissue microarrays via deep learning, Scientific reports 8 (1)
    (2018) 12054.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] E. Arvaniti, K. S. Fricker, M. Moret, N. Rupp, T. Hermanns, C. Fankhauser,
    N. Wey, P. J. Wild, J. H. Rueschoff, M. Claassen, 基于深度学习的前列腺癌组织微阵列自动化Gleason分级，《科学报告》8
    (1) (2018) 12054。'
- en: '[63] W. Bulten, K. Kartasalo, P.-H. C. Chen, P. Ström, H. Pinckaers, K. Nagpal,
    Y. Cai, D. F. Steiner, H. van Boven, R. Vink, et al., Artificial intelligence
    for diagnosis and gleason grading of prostate cancer: the panda challenge, Nature
    medicine 28 (1) (2022) 154–163.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] W. Bulten, K. Kartasalo, P.-H. C. Chen, P. Ström, H. Pinckaers, K. Nagpal,
    Y. Cai, D. F. Steiner, H. van Boven, R. Vink, 等，人工智能用于前列腺癌的诊断和格里森评分：PANDA挑战，《自然医学》28(1)
    (2022) 154–163。'
- en: '[64] F. Prados, J. Ashburner, C. Blaiotta, T. Brosch, J. Carballido-Gamio,
    M. J. Cardoso, B. N. Conrad, E. Datta, G. Dávid, B. De Leener, et al., Spinal
    cord grey matter segmentation challenge, Neuroimage 152 (2017) 312–329.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] F. Prados, J. Ashburner, C. Blaiotta, T. Brosch, J. Carballido-Gamio,
    M. J. Cardoso, B. N. Conrad, E. Datta, G. Dávid, B. De Leener, 等，脊髓灰质分割挑战，《神经影像》152
    (2017) 312–329。'
- en: '[65] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. Wáng, P.-X. Lu, G. Thoma,
    Two public chest x-ray datasets for computer-aided screening of pulmonary diseases,
    Quantitative imaging in medicine and surgery 4 (6) (2014) 475.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. Wáng, P.-X. Lu, G. Thoma,
    用于计算机辅助筛查肺部疾病的两个公开胸部X光数据集，《医学与外科定量成像》4(6) (2014) 475。'
- en: '[66] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2017, pp. 2097–2106.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers，ChestX-ray8：医院级胸部X光数据库及常见胸部疾病的弱监督分类和定位基准，见：IEEE计算机视觉与模式识别会议论文集，2017年，页2097–2106。'
- en: '[67] A. E. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Greenbaum, M. P.
    Lungren, C.-y. Deng, R. G. Mark, S. Horng, Mimic-cxr, a de-identified publicly
    available database of chest radiographs with free-text reports, Scientific data
    6 (1) (2019) 317.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] A. E. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Greenbaum, M. P.
    Lungren, C.-y. Deng, R. G. Mark, S. Horng，MIMIC-CXR，一个去标识化的公开胸部X光数据库，包含自由文本报告，《科学数据》6(1)
    (2019) 317。'
- en: '[68] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha, W. Liang, C. Wang,
    K. Wang, et al., Clinically applicable ai system for accurate diagnosis, quantitative
    measurements, and prognosis of covid-19 pneumonia using computed tomography, Cell
    181 (6) (2020) 1423–1433.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha, W. Liang, C.
    Wang, K. Wang, 等，临床适用的AI系统用于新冠肺炎的准确诊断、定量测量和预后分析，使用计算机断层扫描，《细胞》181(6) (2020) 1423–1433。'
- en: '[69] Z. Lambert, C. Petitjean, B. Dubray, S. Kuan, Segthor: Segmentation of
    thoracic organs at risk in ct images, in: 2020 Tenth International Conference
    on Image Processing Theory, Tools and Applications (IPTA), IEEE, 2020, pp. 1–6.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Z. Lambert, C. Petitjean, B. Dubray, S. Kuan，Segthor：CT图像中胸部危险器官的分割，见：2020年第十届国际图像处理理论、工具与应用会议（IPTA），IEEE，2020年，页1–6。'
- en: '[70] H. Q. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q. Tran, D. B. Nguyen,
    D. D. Le, C. M. Pham, H. T. Tong, D. H. Dinh, et al., Vindr-cxr: An open dataset
    of chest x-rays with radiologist’s annotations, Scientific Data 9 (1) (2022) 429.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] H. Q. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q. Tran, D. B. Nguyen,
    D. D. Le, C. M. Pham, H. T. Tong, D. H. Dinh, 等，Vindr-cxr：一个带有放射科医生注释的胸部X光开放数据集，《科学数据》9(1)
    (2022) 429。'
- en: '[71] S. Shurrab, R. Duwairi, Self-supervised learning methods and applications
    in medical imaging analysis: A survey, PeerJ Computer Science 8 (2022) e1045.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] S. Shurrab, R. Duwairi，自监督学习方法及其在医学影像分析中的应用：综述，《PeerJ计算机科学》8 (2022) e1045。'
- en: '[72] C. Doersch, A. Gupta, A. A. Efros, Unsupervised visual representation
    learning by context prediction, in: Proceedings of the IEEE international conference
    on computer vision, 2015, pp. 1422–1430.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] C. Doersch, A. Gupta, A. A. Efros，通过上下文预测进行无监督视觉表示学习，见：IEEE国际计算机视觉会议论文集，2015年，页1422–1430。'
- en: '[73] M. Noroozi, P. Favaro, Unsupervised learning of visual representations
    by solving jigsaw puzzles, in: European conference on computer vision, Springer,
    2016, pp. 69–84.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Noroozi, P. Favaro，通过解决拼图难题进行无监督视觉表示学习，见：欧洲计算机视觉会议，Springer，2016年，页69–84。'
- en: '[74] S. Gidaris, P. Singh, N. Komodakis, Unsupervised representation learning
    by predicting image rotations, in: International Conference on Learning Representations,
    2018.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] S. Gidaris, P. Singh, N. Komodakis，通过预测图像旋转进行无监督表示学习，见：国际学习表示会议，2018年。'
- en: '[75] W. Bai, C. Chen, G. Tarroni, J. Duan, F. Guitton, S. E. Petersen, Y. Guo,
    P. M. Matthews, D. Rueckert, Self-supervised learning for cardiac mr image segmentation
    by anatomical position prediction, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October
    13–17, 2019, Proceedings, Part II 22, Springer, 2019, pp. 541–549.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] W. Bai, C. Chen, G. Tarroni, J. Duan, F. Guitton, S. E. Petersen, Y. Guo,
    P. M. Matthews, D. Rueckert, 自监督学习在心脏 MRI 图像分割中的应用，通过解剖位置预测，在: 医学图像计算与计算机辅助干预–MICCAI
    2019: 第二十二届国际会议，深圳，中国，2019年10月13-17日，会议录，第 II 部分 22，Springer，2019，第541–549页。'
- en: '[76] A. Taleb, C. Lippert, T. Klein, M. Nabi, Multimodal self-supervised learning
    for medical image analysis, in: International conference on information processing
    in medical imaging, Springer, 2021, pp. 661–673.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] A. Taleb, C. Lippert, T. Klein, M. Nabi, 多模态自监督学习在医学图像分析中的应用，在: 国际医学图像信息处理会议，Springer，2021，第661–673页。'
- en: '[77] X. Zhuang, Y. Li, Y. Hu, K. Ma, Y. Yang, Y. Zheng, Self-supervised feature
    learning for 3d medical images by playing a rubik’s cube, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
    Shenzhen, China, October 13–17, 2019, Proceedings, Part IV 22, Springer, 2019,
    pp. 420–428.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Zhuang, Y. Li, Y. Hu, K. Ma, Y. Yang, Y. Zheng, 通过玩魔方的自监督特征学习用于 3D
    医学图像，在: 医学图像计算与计算机辅助干预–MICCAI 2019: 第二十二届国际会议，深圳，中国，2019年10月13-17日，会议录，第 IV 部分
    22，Springer，2019，第420–428页。'
- en: '[78] J. Zhu, Y. Li, Y. Hu, K. Ma, S. K. Zhou, Y. Zheng, Rubik’s cube+: A self-supervised
    feature learning framework for 3d medical image analysis, Medical image analysis
    64 (2020) 101746.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. Zhu, Y. Li, Y. Hu, K. Ma, S. K. Zhou, Y. Zheng, Rubik’s cube+: 一种用于
    3D 医学图像分析的自监督特征学习框架，医学图像分析 64 (2020) 101746。'
- en: '[79] X.-B. Nguyen, G. S. Lee, S. H. Kim, H. J. Yang, Self-supervised learning
    based on spatial awareness for medical image analysis, IEEE Access 8 (2020) 162973–162981.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] X.-B. Nguyen, G. S. Lee, S. H. Kim, H. J. Yang, 基于空间感知的自监督学习用于医学图像分析，IEEE
    Access 8 (2020) 162973–162981。'
- en: '[80] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, D. Rueckert, Self-supervised
    learning for medical image analysis using image context restoration, Medical image
    analysis 58 (2019) 101539.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, D. Rueckert, 使用图像上下文恢复的自监督学习用于医学图像分析，医学图像分析
    58 (2019) 101539。'
- en: '[81] H.-Y. Zhou, C. Lu, C. Chen, S. Yang, Y. Yu, A unified visual information
    preservation framework for self-supervised pre-training in medical image analysis,
    IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H.-Y. Zhou, C. Lu, C. Chen, S. Yang, Y. Yu, 一个统一的视觉信息保留框架用于医学图像分析中的自监督预训练，IEEE
    计算机学会模式分析与机器智能交易 (2023)。'
- en: '[82] T. Ross, D. Zimmerer, A. Vemuri, F. Isensee, M. Wiesenfarth, S. Bodenstedt,
    F. Both, P. Kessler, M. Wagner, B. Müller, et al., Exploiting the potential of
    unlabeled endoscopic video data with self-supervised learning, International journal
    of computer assisted radiology and surgery 13 (2018) 925–933.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] T. Ross, D. Zimmerer, A. Vemuri, F. Isensee, M. Wiesenfarth, S. Bodenstedt,
    F. Both, P. Kessler, M. Wagner, B. Müller, 等, 利用自监督学习挖掘未标记内窥镜视频数据的潜力，国际计算机辅助放射学与外科手术期刊
    13 (2018) 925–933。'
- en: '[83] Z. Zhou, V. Sodha, M. M. Rahman Siddiquee, R. Feng, N. Tajbakhsh, M. B.
    Gotway, J. Liang, Models genesis: Generic autodidactic models for 3d medical image
    analysis, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings,
    Part IV 22, Springer, 2019, pp. 384–393.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Z. Zhou, V. Sodha, M. M. Rahman Siddiquee, R. Feng, N. Tajbakhsh, M. B.
    Gotway, J. Liang, 模型生成：用于 3D 医学图像分析的通用自学模型，在: 医学图像计算与计算机辅助干预–MICCAI 2019: 第二十二届国际会议，深圳，中国，2019年10月13-17日，会议录，第
    IV 部分 22，Springer，2019，第384–393页。'
- en: '[84] O. G. Holmberg, N. D. Köhler, T. Martins, J. Siedlecki, T. Herold, L. Keidel,
    B. Asani, J. Schiefelbein, S. Priglinger, K. U. Kortuem, et al., Self-supervised
    retinal thickness prediction enables deep learning from unlabelled data to boost
    classification of diabetic retinopathy, Nature Machine Intelligence 2 (11) (2020)
    719–726.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] O. G. Holmberg, N. D. Köhler, T. Martins, J. Siedlecki, T. Herold, L.
    Keidel, B. Asani, J. Schiefelbein, S. Priglinger, K. U. Kortuem, 等, 自监督视网膜厚度预测使得从未标记数据中深度学习成为可能，提升糖尿病视网膜病变的分类，Nature
    Machine Intelligence 2 (11) (2020) 719–726。'
- en: '[85] M. Prakash, T.-O. Buchholz, M. Lalit, P. Tomancak, F. Jug, A. Krull, Leveraging
    self-supervised denoising for image segmentation, in: 2020 IEEE 17th international
    symposium on biomedical imaging (ISBI), IEEE, 2020, pp. 428–432.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] M. Prakash, T.-O. Buchholz, M. Lalit, P. Tomancak, F. Jug, A. Krull, 利用自监督去噪进行图像分割，在:
    2020 IEEE 第17届生物医学成像国际研讨会 (ISBI)，IEEE，2020，第428–432页。'
- en: '[86] X. Tao, Y. Li, W. Zhou, K. Ma, Y. Zheng, Revisiting rubik’s cube: self-supervised
    learning with volume-wise transformation for 3d medical image segmentation, in:
    Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International
    Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part IV 23, Springer,
    2020, pp. 238–248.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] X. Tao, Y. Li, W. Zhou, K. Ma, Y. Zheng, 重访魔方：用于3D医学图像分割的自监督学习与体积变换，发表于：医学图像计算与计算机辅助干预–MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4–8日，论文集，第IV部分 23，Springer，2020年，页码238–248。'
- en: '[87] K. Chaitanya, E. Erdil, N. Karani, E. Konukoglu, Contrastive learning
    of global and local features for medical image segmentation with limited annotations,
    Advances in neural information processing systems 33 (2020) 12546–12558.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] K. Chaitanya, E. Erdil, N. Karani, E. Konukoglu, 在有限注释的医学图像分割中对比学习全球和局部特征，神经信息处理系统进展
    33 (2020) 12546–12558。'
- en: '[88] C. Zhang, H. Zheng, Y. Gu, Dive into the details of self-supervised learning
    for medical image analysis, Medical Image Analysis 89 (2023) 102879.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. Zhang, H. Zheng, Y. Gu, 深入探讨自监督学习在医学图像分析中的细节，医学图像分析 89 (2023) 102879。'
- en: '[89] A. Taleb, M. Kirchler, R. Monti, C. Lippert, Contig: Self-supervised multimodal
    contrastive learning for medical imaging with genetics, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 20908–20921.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] A. Taleb, M. Kirchler, R. Monti, C. Lippert, Contig: 自监督的多模态对比学习用于具有遗传学的医学成像，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2022年，页码20908–20921。'
- en: '[90] H. Sowrirajan, J. Yang, A. Y. Ng, P. Rajpurkar, Moco pretraining improves
    representation and transferability of chest x-ray models, in: Medical Imaging
    with Deep Learning, PMLR, 2021, pp. 728–744.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] H. Sowrirajan, J. Yang, A. Y. Ng, P. Rajpurkar, MoCo预训练改善胸部X光模型的表示和迁移能力，发表于：深度学习医学影像会议，PMLR，2021年，页码728–744。'
- en: '[91] K. He, H. Fan, Y. Wu, S. Xie, R. Girshick, Momentum contrast for unsupervised
    visual representation learning, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, 2020, pp. 9729–9738.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] K. He, H. Fan, Y. Wu, S. Xie, R. Girshick, 动量对比用于无监督视觉表示学习，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2020年，页码9729–9738。'
- en: '[92] Y. N. T. Vu, R. Wang, N. Balachandar, C. Liu, A. Y. Ng, P. Rajpurkar,
    Medaug: Contrastive learning leveraging patient metadata improves representations
    for chest x-ray interpretation, in: Machine Learning for Healthcare Conference,
    PMLR, 2021, pp. 755–769.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Y. N. T. Vu, R. Wang, N. Balachandar, C. Liu, A. Y. Ng, P. Rajpurkar,
    Medaug: 利用患者元数据的对比学习改善胸部X光解释的表示，发表于：医疗保健机器学习会议，PMLR，2021年，页码755–769。'
- en: '[93] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A simple framework for contrastive
    learning of visual representations, in: International conference on machine learning,
    PMLR, 2020, pp. 1597–1607.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, 一种简单的视觉表示对比学习框架，发表于：国际机器学习会议，PMLR，2020年，页码1597–1607。'
- en: '[94] O. Ciga, T. Xu, A. L. Martel, Self supervised contrastive learning for
    digital histopathology, Machine Learning with Applications 7 (2022) 100198.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] O. Ciga, T. Xu, A. L. Martel, 自监督对比学习用于数字组织病理学，应用机器学习 7 (2022) 100198。'
- en: '[95] Y. He, G. Yang, R. Ge, Y. Chen, J.-L. Coatrieux, B. Wang, S. Li, Geometric
    visual similarity learning in 3d medical image self-supervised pre-training, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2023, pp. 9538–9547.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. He, G. Yang, R. Ge, Y. Chen, J.-L. Coatrieux, 3D医学图像自监督预训练中的几何视觉相似性学习，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2023年，页码9538–9547。'
- en: '[96] F. Haghighi, M. R. Hosseinzadeh Taher, Z. Zhou, M. B. Gotway, J. Liang,
    Learning semantics-enriched representation via self-discovery, self-classification,
    and self-restoration, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings,
    Part I 23, Springer, 2020, pp. 137–147.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] F. Haghighi, M. R. Hosseinzadeh Taher, Z. Zhou, M. B. Gotway, J. Liang,
    通过自我发现、自我分类和自我恢复学习语义丰富的表示，发表于：医学图像计算与计算机辅助干预–MICCAI 2020：第23届国际会议，秘鲁利马，2020年10月4–8日，论文集，第I部分
    23，Springer，2020年，页码137–147。'
- en: '[97] X. Zhang, S. Feng, Y. Zhou, Y. Zhang, Y. Wang, Sar: Scale-aware restoration
    learning for 3d tumor segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part II 24, Springer, 2021, pp. 124–133.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] X. Zhang, S. Feng, Y. Zhou, Y. Zhang, Y. Wang, SAR: 关注尺度的恢复学习用于3D肿瘤分割，发表于：医学图像计算与计算机辅助干预–MICCAI
    2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日–10月1日，论文集，第II部分 24，Springer，2021年，页码124–133。'
- en: '[98] H.-Y. Zhou, C. Lu, S. Yang, X. Han, Y. Yu, Preservational learning improves
    self-supervised medical image models by reconstructing diverse contexts, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 3499–3509.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] H.-Y. Zhou, C. Lu, S. Yang, X. Han, Y. Yu, 保护性学习通过重构多样化上下文改进自监督医疗图像模型，发表于：IEEE/CVF国际计算机视觉会议论文集，2021年，第3499–3509页。'
- en: '[99] Y. Tang, D. Yang, W. Li, H. R. Roth, B. Landman, D. Xu, V. Nath, A. Hatamizadeh,
    Self-supervised pre-training of swin transformers for 3d medical image analysis,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2022, pp. 20730–20740.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Tang, D. Yang, W. Li, H. R. Roth, B. Landman, D. Xu, V. Nath, A. Hatamizadeh,
    自监督预训练的Swin变换器用于3D医疗图像分析，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第20730–20740页。'
- en: '[100] P. Yang, X. Yin, H. Lu, Z. Hu, X. Zhang, R. Jiang, H. Lv, Cs-co: A hybrid
    self-supervised visual representation learning method for h&e-stained histopathological
    images, Medical Image Analysis 81 (2022) 102539.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] P. Yang, X. Yin, H. Lu, Z. Hu, X. Zhang, R. Jiang, H. Lv, Cs-co：一种用于H&E染色组织病理图像的混合自监督视觉表示学习方法，医学图像分析81
    (2022) 102539。'
- en: '[101] X. Yan, J. Naushad, S. Sun, K. Han, H. Tang, D. Kong, H. Ma, C. You,
    X. Xie, Representation recovering for self-supervised pre-training on medical
    images, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision, 2023, pp. 2685–2695.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Yan, J. Naushad, S. Sun, K. Han, H. Tang, D. Kong, H. Ma, C. You,
    X. Xie, 医疗图像自监督预训练的表示恢复，发表于：IEEE/CVF计算机视觉应用冬季会议论文集，2023年，第2685–2695页。'
- en: '[102] E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, P. Rajpurkar,
    Expert-level detection of pathologies from unannotated chest x-ray images via
    self-supervised learning, Nature Biomedical Engineering 6 (12) (2022) 1399–1406.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, P. Rajpurkar,
    通过自监督学习从未注释的胸部X光图像中进行专家级病理检测，自然生物医学工程6 (12) (2022) 1399–1406。'
- en: '[103] L. Qu, S. Liu, X. Liu, M. Wang, Z. Song, Towards label-efficient automatic
    diagnosis and analysis: a comprehensive survey of advanced deep learning-based
    weakly-supervised, semi-supervised and self-supervised techniques in histopathological
    image analysis, Physics in Medicine & Biology (2022).'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] L. Qu, S. Liu, X. Liu, M. Wang, Z. Song, 迈向标签高效的自动诊断与分析：关于在组织病理图像分析中应用的先进深度学习基础的弱监督、半监督和自监督技术的综合调查，医学与生物物理学（2022）。'
- en: '[104] X. Wang, Y. Yan, P. Tang, X. Bai, W. Liu, Revisiting multiple instance
    neural networks, Pattern Recognition 74 (2018) 15–24.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] X. Wang, Y. Yan, P. Tang, X. Bai, W. Liu, 重新审视多实例神经网络，模式识别74 (2018) 15–24。'
- en: '[105] E. Schwab, A. Gooßen, H. Deshpande, A. Saalbach, Localization of critical
    findings in chest x-ray without local annotations using multi-instance learning,
    in: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE,
    2020, pp. 1879–1882.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] E. Schwab, A. Gooßen, H. Deshpande, A. Saalbach, 在没有局部注释的情况下利用多实例学习定位胸部X光片中的关键发现，发表于：2020
    IEEE第17届生物医学成像国际研讨会（ISBI），IEEE，2020年，第1879–1882页。'
- en: '[106] J. Ramon, L. De Raedt, Multi instance neural networks, in: Proceedings
    of the ICML-2000 workshop on attribute-value and relational learning, 2000, pp.
    53–60.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. Ramon, L. De Raedt, 多实例神经网络，发表于：ICML-2000关于属性值和关系学习的研讨会论文集，2000年，第53–60页。'
- en: '[107] O. Maron, T. Lozano-Pérez, A framework for multiple-instance learning,
    Advances in neural information processing systems 10 (1997).'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] O. Maron, T. Lozano-Pérez, 多实例学习框架，神经信息处理系统进展10 (1997)。'
- en: '[108] O. Z. Kraus, J. L. Ba, B. J. Frey, Classifying and segmenting microscopy
    images with deep multiple instance learning, Bioinformatics 32 (12) (2016) i52–i59.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] O. Z. Kraus, J. L. Ba, B. J. Frey, 使用深度多实例学习对显微镜图像进行分类和分割，生物信息学32 (12)
    (2016) i52–i59。'
- en: '[109] Y. Yan, X. Wang, X. Guo, J. Fang, W. Liu, J. Huang, Deep multi-instance
    learning with dynamic pooling, in: Asian Conference on Machine Learning, PMLR,
    2018, pp. 662–677.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Y. Yan, X. Wang, X. Guo, J. Fang, W. Liu, J. Huang, 深度多实例学习与动态池化，发表于：亚洲机器学习会议，PMLR，2018年，第662–677页。'
- en: '[110] H. D. Couture, J. S. Marron, C. M. Perou, M. A. Troester, M. Niethammer,
    Multiple instance learning for heterogeneous images: Training a cnn for histopathology,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st
    International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part II 11, Springer, 2018, pp. 254–262.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] H. D. Couture, J. S. Marron, C. M. Perou, M. A. Troester, M. Niethammer,
    异质图像的多实例学习：训练用于组织病理学的CNN，发表于：医学图像计算与计算机辅助干预–MICCAI 2018：第21届国际会议，西班牙格拉纳达，2018年9月16-20日，论文集，第11部分，Springer，2018年，第254–262页。'
- en: '[111] J. Qu, X. Wei, X. Qian, Generalized pancreatic cancer diagnosis via multiple
    instance learning and anatomically-guided shape normalization, Medical Image Analysis
    86 (2023) 102774.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] J. Qu, X. Wei, X. Qian, 通过多实例学习和解剖引导的形状归一化实现泛化的胰腺癌诊断, 医学图像分析 86 (2023)
    102774。'
- en: '[112] Y. Zhao, F. Yang, Y. Fang, H. Liu, N. Zhou, J. Zhang, J. Sun, S. Yang,
    B. Menze, X. Fan, et al., Predicting lymph node metastasis using histopathological
    images based on multiple instance learning with deep graph convolution, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.
    4837–4846.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Y. Zhao, F. Yang, Y. Fang, H. Liu, N. Zhou, J. Zhang, J. Sun, S. Yang,
    B. Menze, X. Fan, 等., 基于深度图卷积的多实例学习预测淋巴结转移, 在: IEEE/CVF 计算机视觉与模式识别会议, 2020, 第4837–4846页。'
- en: '[113] M. Y. Lu, R. J. Chen, J. Wang, D. Dillon, F. Mahmood, Semi-supervised
    histology classification using deep multiple instance learning and contrastive
    predictive coding, arXiv preprint arXiv:1910.10825 (2019).'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. Y. Lu, R. J. Chen, J. Wang, D. Dillon, F. Mahmood, 使用深度多实例学习和对比预测编码的半监督组织学分类,
    arXiv 预印本 arXiv:1910.10825 (2019)。'
- en: '[114] A. v. d. Oord, Y. Li, O. Vinyals, Representation learning with contrastive
    predictive coding, arXiv preprint arXiv:1807.03748 (2018).'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] A. v. d. Oord, Y. Li, O. Vinyals, 使用对比预测编码的表示学习, arXiv 预印本 arXiv:1807.03748
    (2018)。'
- en: '[115] B. Li, Y. Li, K. W. Eliceiri, Dual-stream multiple instance learning
    network for whole slide image classification with self-supervised contrastive
    learning, in: Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition, 2021, pp. 14318–14328.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] B. Li, Y. Li, K. W. Eliceiri, 用于全片图像分类的双流多实例学习网络与自监督对比学习, 在: IEEE/CVF
    计算机视觉与模式识别会议论文集, 2021, 第14318–14328页。'
- en: '[116] P. Chikontwe, M. Luna, M. Kang, K. S. Hong, J. H. Ahn, S. H. Park, Dual
    attention multiple instance learning with unsupervised complementary loss for
    covid-19 screening, Medical Image Analysis 72 (2021) 102105.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] P. Chikontwe, M. Luna, M. Kang, K. S. Hong, J. H. Ahn, S. H. Park, 带有无监督互补损失的双重注意力多实例学习用于COVID-19筛查,
    医学图像分析 72 (2021) 102105。'
- en: '[117] A. Raju, J. Yao, M. M. Haq, J. Jonnagaddala, J. Huang, Graph attention
    multi-instance learning for accurate colorectal cancer staging, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part V 23, Springer, 2020, pp. 529–539.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. Raju, J. Yao, M. M. Haq, J. Jonnagaddala, J. Huang, 图注意力多实例学习用于准确的结直肠癌分期,
    在: 医学图像计算与计算机辅助干预–MICCAI 2020: 第23届国际会议, 秘鲁利马, 2020年10月4–8日, 会议录, 第五部分 23, Springer,
    2020, 第529–539页。'
- en: '[118] Z. Su, T. E. Tavolara, G. Carreno-Galeano, S. J. Lee, M. N. Gurcan, M. Niazi,
    Attention2majority: Weak multiple instance learning for regenerative kidney grading
    on whole slide images, Medical Image Analysis 79 (2022) 102462.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Z. Su, T. E. Tavolara, G. Carreno-Galeano, S. J. Lee, M. N. Gurcan, M.
    Niazi, Attention2majority: 用于全片图像的弱多实例学习的再生肾脏分级, 医学图像分析 79 (2022) 102462。'
- en: '[119] M. Adnan, S. Kalra, H. R. Tizhoosh, Representation learning of histopathology
    images using graph neural networks, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshops, 2020, pp. 988–989.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Adnan, S. Kalra, H. R. Tizhoosh, 使用图神经网络的组织病理图像表示学习, 在: IEEE/CVF 计算机视觉与模式识别会议研讨会,
    2020, 第988–989页。'
- en: '[120] Y. Sharma, A. Shrivastava, L. Ehsan, C. A. Moskaluk, S. Syed, D. Brown,
    Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole
    slide image classification, in: Medical Imaging with Deep Learning, PMLR, 2021,
    pp. 682–698.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y. Sharma, A. Shrivastava, L. Ehsan, C. A. Moskaluk, S. Syed, D. Brown,
    Cluster-to-conquer: 一个用于全片图像分类的端到端多实例学习框架, 在: 使用深度学习的医学成像, PMLR, 2021, 第682–698页。'
- en: '[121] M. Y. Lu, D. F. Williamson, T. Y. Chen, R. J. Chen, M. Barbieri, F. Mahmood,
    Data-efficient and weakly supervised computational pathology on whole-slide images,
    Nature biomedical engineering 5 (6) (2021) 555–570.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Y. Lu, D. F. Williamson, T. Y. Chen, R. J. Chen, M. Barbieri, F. Mahmood,
    数据高效且弱监督的计算病理学在全片图像上的应用, Nature 生物医学工程 5 (6) (2021) 555–570。'
- en: '[122] R. Yan, Y. Shen, X. Zhang, P. Xu, J. Wang, J. Li, F. Ren, D. Ye, S. K.
    Zhou, Histopathological bladder cancer gene mutation prediction with hierarchical
    deep multiple-instance learning, Medical Image Analysis 87 (2023) 102824.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] R. Yan, Y. Shen, X. Zhang, P. Xu, J. Wang, J. Li, F. Ren, D. Ye, S. K.
    Zhou, 通过分层深度多实例学习预测膀胱癌基因突变, 医学图像分析 87 (2023) 102824。'
- en: '[123] M. Ilse, J. Tomczak, M. Welling, Attention-based deep multiple instance
    learning, in: International conference on machine learning, PMLR, 2018, pp. 2127–2136.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] M. Ilse, J. Tomczak, M. Welling, 基于注意力的深度多实例学习，在：国际机器学习会议，PMLR，2018年，页2127–2136。'
- en: '[124] Y. Wang, P. Tang, Y. Zhou, W. Shen, E. K. Fishman, A. L. Yuille, Learning
    inductive attention guidance for partially supervised pancreatic ductal adenocarcinoma
    prediction, IEEE transactions on medical imaging 40 (10) (2021) 2723–2735.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Y. Wang, P. Tang, Y. Zhou, W. Shen, E. K. Fishman, A. L. Yuille, 学习用于部分监督胰腺导管腺癌预测的归纳注意力引导，IEEE医学成像学报
    40 (10) (2021) 2723–2735。'
- en: '[125] Z. Li, W. Zhao, F. Shi, L. Qi, X. Xie, Y. Wei, Z. Ding, Y. Gao, S. Wu,
    J. Liu, et al., A novel multiple instance learning framework for covid-19 severity
    assessment via data augmentation and self-supervised learning, Medical Image Analysis
    69 (2021) 101978.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Z. Li, W. Zhao, F. Shi, L. Qi, X. Xie, Y. Wei, Z. Ding, Y. Gao, S. Wu,
    J. Liu, 等，基于数据增强和自监督学习的COVID-19严重程度评估的新型多实例学习框架，医学图像分析 69 (2021) 101978。'
- en: '[126] Z. Wang, L. Yu, X. Ding, X. Liao, L. Wang, Lymph node metastasis prediction
    from whole slide images with transformer-guided multiinstance learning and knowledge
    transfer, IEEE Transactions on Medical Imaging 41 (10) (2022) 2777–2787.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Z. Wang, L. Yu, X. Ding, X. Liao, L. Wang, 从全幅切片图像中预测淋巴结转移，结合变换器引导的多实例学习和知识转移，IEEE医学成像学报
    41 (10) (2022) 2777–2787。'
- en: '[127] T. Zhao, Z. Yin, Weakly supervised cell segmentation by point annotation,
    IEEE Transactions on Medical Imaging 40 (10) (2020) 2736–2747.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] T. Zhao, Z. Yin, 基于点标注的弱监督细胞分割，IEEE医学成像学报 40 (10) (2020) 2736–2747。'
- en: '[128] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang, L. Xie, X. Yang,
    Q. Tian, A survey on label-efficient deep image segmentation: Bridging the gap
    between weak supervision and dense prediction, IEEE Transactions on Pattern Analysis
    and Machine Intelligence (2023).'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang, L. Xie, X. Yang,
    Q. Tian, 关于标签效率深度图像分割的调查：弥合弱监督与密集预测之间的差距，IEEE模式分析与机器智能学报（2023）。'
- en: '[129] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep
    features for discriminative localization, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2016, pp. 2921–2929.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, 学习用于区分定位的深度特征，在：IEEE计算机视觉与模式识别大会论文集，2016年，页2921–2929。'
- en: '[130] Y. Li, Y. Liu, L. Huang, Z. Wang, J. Luo, Deep weakly-supervised breast
    tumor segmentation in ultrasound images with explicit anatomical constraints,
    Medical image analysis 76 (2022) 102315.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. Li, Y. Liu, L. Huang, Z. Wang, J. Luo, 使用显式解剖约束的深度弱监督乳腺肿瘤超声图像分割，医学图像分析
    76 (2022) 102315。'
- en: '[131] Z. Chen, Z. Tian, J. Zhu, C. Li, S. Du, C-cam: Causal cam for weakly
    supervised semantic segmentation on medical image, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2022, pp. 11676–11685.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Z. Chen, Z. Tian, J. Zhu, C. Li, S. Du, C-cam: 用于医学图像弱监督语义分割的因果相机，在：IEEE/CVF计算机视觉与模式识别大会论文集，2022年，页11676–11685。'
- en: '[132] S. Khan, A. H. Shahin, J. Villafruela, J. Shen, L. Shao, Extreme points
    derived confidence map as a cue for class-agnostic interactive segmentation using
    deep neural network, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings,
    Part II 22, Springer, 2019, pp. 66–73.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] S. Khan, A. H. Shahin, J. Villafruela, J. Shen, L. Shao, 从极端点派生的置信度图作为使用深度神经网络的类无关交互分割线索，在：医学图像计算与计算机辅助干预–MICCAI
    2019：第22届国际会议，中国深圳，2019年10月13日至17日，论文集，第II部分22，Springer，2019年，页66–73。'
- en: '[133] H. R. Roth, D. Yang, Z. Xu, X. Wang, D. Xu, Going to extremes: weakly
    supervised medical image segmentation, Machine Learning and Knowledge Extraction
    3 (2) (2021) 507–524.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] H. R. Roth, D. Yang, Z. Xu, X. Wang, D. Xu, 走向极端：弱监督医学图像分割，机器学习与知识提取
    3 (2) (2021) 507–524。'
- en: '[134] R. Dorent, S. Joutard, J. Shapey, A. Kujawa, M. Modat, S. Ourselin, T. Vercauteren,
    Inter extreme points geodesics for end-to-end weakly supervised image segmentation,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part II 24, Springer, 2021, pp. 615–624.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] R. Dorent, S. Joutard, J. Shapey, A. Kujawa, M. Modat, S. Ourselin, T.
    Vercauteren, 用于端到端弱监督图像分割的极端点测地线，在：医学图像计算与计算机辅助干预–MICCAI 2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日至10月1日，论文集，第II部分24，Springer，2021年，页615–624。'
- en: '[135] H. Qu, P. Wu, Q. Huang, J. Yi, Z. Yan, K. Li, G. M. Riedlinger, S. De,
    S. Zhang, D. N. Metaxas, Weakly supervised deep nuclei segmentation using partial
    points annotation in histopathology images, IEEE transactions on medical imaging
    39 (11) (2020) 3655–3666.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] H. Qu, P. Wu, Q. Huang, J. Yi, Z. Yan, K. Li, G. M. Riedlinger, S. De,
    S. Zhang, D. N. Metaxas, 使用部分点注释的弱监督深度核分割在组织病理图像中，《IEEE医学影像学交易》39(11) (2020) 3655–3666。'
- en: '[136] Y. Lin, Z. Qu, H. Chen, Z. Gao, Y. Li, L. Xia, K. Ma, Y. Zheng, K.-T.
    Cheng, Label propagation for annotation-efficient nuclei segmentation from pathology
    images, arXiv preprint arXiv:2202.08195 (2022).'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Y. Lin, Z. Qu, H. Chen, Z. Gao, Y. Li, L. Xia, K. Ma, Y. Zheng, K.-T.
    Cheng, 用于病理图像中注释高效的核分割的标签传播，arXiv预印本arXiv:2202.08195 (2022)。'
- en: '[137] K. Kise, A. Sato, M. Iwata, Segmentation of page images using the area
    voronoi diagram, Computer Vision and Image Understanding 70 (3) (1998) 370–382.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] K. Kise, A. Sato, M. Iwata, 使用区域 Voronoi 图进行页面图像分割，《计算机视觉与图像理解》70(3)
    (1998) 370–382。'
- en: '[138] Y. Lin, Z. Qu, H. Chen, Z. Gao, Y. Li, L. Xia, K. Ma, Y. Zheng, K.-T.
    Cheng, Nuclei segmentation with point annotations from pathology images via self-supervised
    learning and co-training, Medical Image Analysis (2023) 102933.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Lin, Z. Qu, H. Chen, Z. Gao, Y. Li, L. Xia, K. Ma, Y. Zheng, K.-T.
    Cheng, 通过自监督学习和共同训练的点注释进行病理图像的核分割，《医学图像分析》(2023) 102933。'
- en: '[139] H. Zhang, Y. Meng, Y. Zhao, Y. Qiao, X. Yang, S. E. Coupland, Y. Zheng,
    Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology
    whole slide image classification, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2022, pp. 18802–18812.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] H. Zhang, Y. Meng, Y. Zhao, Y. Qiao, X. Yang, S. E. Coupland, Y. Zheng,
    Dtfd-mil：用于组织病理全切片图像分类的双层特征蒸馏多实例学习，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第18802–18812页。'
- en: '[140] Z. Chen, J. Zhang, S. Che, J. Huang, X. Han, Y. Yuan, Diagnose like a
    pathologist: Weakly-supervised pathologist-tree network for slide-level immunohistochemical
    scoring, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35,
    2021, pp. 47–54.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Z. Chen, J. Zhang, S. Che, J. Huang, X. Han, Y. Yuan, 像病理学家一样诊断：用于切片级免疫组化评分的弱监督病理学家树网络，见：AAAI人工智能会议论文集，第35卷，2021年，第47–54页。'
- en: '[141] W. Bai, H. Suzuki, C. Qin, G. Tarroni, O. Oktay, P. M. Matthews, D. Rueckert,
    Recurrent neural networks for aortic image sequence segmentation with sparse annotations,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st
    International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part IV 11, Springer, 2018, pp. 586–594.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] W. Bai, H. Suzuki, C. Qin, G. Tarroni, O. Oktay, P. M. Matthews, D. Rueckert,
    用于主动脉图像序列分割的递归神经网络与稀疏注释，见：医学图像计算与计算机辅助干预–MICCAI 2018：第21届国际会议，西班牙格拉纳达，2018年9月16-20日，会议录，第四部分11，Springer，2018年，第586–594页。'
- en: '[142] Z. Ji, Y. Shen, C. Ma, M. Gao, Scribble-based hierarchical weakly supervised
    learning for brain tumor segmentation, in: Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China,
    October 13–17, 2019, Proceedings, Part III 22, Springer, 2019, pp. 175–183.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Z. Ji, Y. Shen, C. Ma, M. Gao, 基于涂鸦的层次弱监督学习用于脑肿瘤分割，见：医学图像计算与计算机辅助干预–MICCAI
    2019：第22届国际会议，中国深圳，2019年10月13–17日，会议录，第三部分22，Springer，2019年，第175–183页。'
- en: '[143] Q. Chen, Y. Hong, Scribble2d5: Weakly-supervised volumetric image segmentation
    via scribble annotations, in: International Conference on Medical Image Computing
    and Computer-Assisted Intervention, Springer, 2022, pp. 234–243.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Q. Chen, Y. Hong, Scribble2d5：通过涂鸦注释的弱监督体积图像分割，见：国际医学图像计算与计算机辅助干预会议，Springer，2022年，第234–243页。'
- en: '[144] Y. B. Can, K. Chaitanya, B. Mustafa, L. M. Koch, E. Konukoglu, C. F.
    Baumgartner, Learning to segment medical images with scribble-supervision alone,
    in: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical
    Decision Support: 4th International Workshop, DLMIA 2018, and 8th International
    Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September
    20, 2018, Proceedings 4, Springer, 2018, pp. 236–244.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. B. Can, K. Chaitanya, B. Mustafa, L. M. Koch, E. Konukoglu, C. F.
    Baumgartner, 仅通过涂鸦监督学习医学图像分割，见：深度学习在医学图像分析中的应用及临床决策支持的多模态学习：第四届国际研讨会 DLMIA 2018
    和第八届国际研讨会 ML-CDS 2018，与 MICCAI 2018 一同举行，西班牙格拉纳达，2018年9月20日，会议录4，Springer，2018年，第236–244页。'
- en: '[145] M. Tang, F. Perazzi, A. Djelouah, I. Ben Ayed, C. Schroers, Y. Boykov,
    On regularized losses for weakly-supervised cnn segmentation, in: Proceedings
    of the European Conference on Computer Vision (ECCV), 2018, pp. 507–522.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. Tang, F. Perazzi, A. Djelouah, I. Ben Ayed, C. Schroers, Y. Boykov,
    关于弱监督 CNN 分割的正则化损失，载于：2018 年欧洲计算机视觉会议（ECCV）论文集，第 507–522 页。'
- en: '[146] X. Luo, M. Hu, W. Liao, S. Zhai, T. Song, G. Wang, S. Zhang, Scribble-supervised
    medical image segmentation via dual-branch network and dynamically mixed pseudo
    labels supervision, in: International Conference on Medical Image Computing and
    Computer-Assisted Intervention, Springer, 2022, pp. 528–538.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] X. Luo, M. Hu, W. Liao, S. Zhai, T. Song, G. Wang, S. Zhang，基于双分支网络和动态混合伪标签监督的涂鸦监督医学图像分割，载于：医学图像计算与计算机辅助干预国际会议，Springer，2022，第
    528–538 页。'
- en: '[147] G. Valvano, A. Leo, S. A. Tsaftaris, Learning to segment from scribbles
    using multi-scale adversarial attention gates, IEEE Transactions on Medical Imaging
    40 (8) (2021) 1990–2001.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] G. Valvano, A. Leo, S. A. Tsaftaris，通过多尺度对抗注意力门学习从涂鸦中分割，IEEE 医学影像学报 40
    (8) (2021) 1990–2001。'
- en: '[148] P. Zhang, Y. Zhong, X. Li, Accl: Adversarial constrained-cnn loss for
    weakly supervised medical image segmentation, arXiv preprint arXiv:2005.00328
    (2020).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] P. Zhang, Y. Zhong, X. Li，ACCL：用于弱监督医学图像分割的对抗约束 CNN 损失，arXiv 预印本 arXiv:2005.00328
    (2020)。'
- en: '[149] K. Zhang, X. Zhuang, Cyclemix: A holistic strategy for medical image
    segmentation from scribble supervision, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 11656–11665.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] K. Zhang, X. Zhuang，Cyclemix：一种用于医学图像分割的整体策略，基于涂鸦监督，载于：IEEE/CVF 计算机视觉与模式识别会议论文集，2022，第
    11656–11665 页。'
- en: '[150] F. Gao, M. Hu, M.-E. Zhong, S. Feng, X. Tian, X. Meng, Z. Huang, M. Lv,
    T. Song, X. Zhang, et al., Segmentation only uses sparse annotations: Unified
    weakly and semi-supervised learning in medical images, Medical Image Analysis
    80 (2022) 102515.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] F. Gao, M. Hu, M.-E. Zhong, S. Feng, X. Tian, X. Meng, Z. Huang, M. Lv,
    T. Song, X. Zhang 等，分割仅使用稀疏注释：医学图像中的统一弱监督和半监督学习，医学图像分析 80 (2022) 102515。'
- en: '[151] K. Zhang, X. Zhuang, Shapepu: A new pu learning framework regularized
    by global consistency for scribble supervised cardiac segmentation, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    2022, pp. 162–172.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] K. Zhang, X. Zhuang，Shapepu：一种新的 PU 学习框架，通过全局一致性正则化，用于涂鸦监督的心脏分割，载于：医学图像计算与计算机辅助干预国际会议，Springer，2022，第
    162–172 页。'
- en: '[152] H. Lee, W.-K. Jeong, Scribble2label: Scribble-supervised cell segmentation
    via self-generating pseudo-labels with consistency, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, Springer, 2020, pp. 14–23.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] H. Lee, W.-K. Jeong，Scribble2label：通过自生成伪标签与一致性进行涂鸦监督的细胞分割，载于：医学图像计算与计算机辅助干预–MICCAI
    2020：第 23 届国际会议，秘鲁利马，2020 年 10 月 4–8 日，论文集，第 23 部分，Springer，2020，第 14–23 页。'
- en: '[153] M. Rajchl, M. C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W. Bai,
    M. Damodaram, M. A. Rutherford, J. V. Hajnal, B. Kainz, et al., Deepcut: Object
    segmentation from bounding box annotations using convolutional neural networks,
    IEEE transactions on medical imaging 36 (2) (2016) 674–683.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] M. Rajchl, M. C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W.
    Bai, M. Damodaram, M. A. Rutherford, J. V. Hajnal, B. Kainz 等，Deepcut：使用卷积神经网络从边界框注释中进行目标分割，IEEE
    医学影像学报 36 (2) (2016) 674–683。'
- en: '[154] J. Wang, B. Xia, Bounding box tightness prior for weakly supervised image
    segmentation, in: International conference on medical image computing and computer-assisted
    intervention, Springer, 2021, pp. 526–536.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] J. Wang, B. Xia，边界框紧致先验用于弱监督图像分割，载于：医学图像计算与计算机辅助干预国际会议，Springer，2021，第
    526–536 页。'
- en: '[155] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, Z. Li, Boxpolyp: Boost generalized
    polyp segmentation using extra coarse bounding box annotations, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    2022, pp. 67–77.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, Z. Li，Boxpolyp：使用额外粗略边界框注释提升通用息肉分割，载于：医学图像计算与计算机辅助干预国际会议，Springer，2022，第
    67–77 页。'
- en: '[156] R. Jiao, Y. Zhang, L. Ding, R. Cai, J. Zhang, Learning with limited annotations:
    a survey on deep semi-supervised learning for medical image segmentation, arXiv
    preprint arXiv:2207.14191 (2022).'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] R. Jiao, Y. Zhang, L. Ding, R. Cai, J. Zhang，有限注释下的学习：医学图像分割的深度半监督学习综述，arXiv
    预印本 arXiv:2207.14191 (2022)。'
- en: '[157] X. Yang, Z. Song, I. King, Z. Xu, A survey on deep semi-supervised learning,
    IEEE Transactions on Knowledge and Data Engineering 35 (9) (2023) 8934–8954. [doi:10.1109/TKDE.2022.3220219](https://doi.org/10.1109/TKDE.2022.3220219).'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] X. Yang, Z. Song, I. King, Z. Xu, 深度半监督学习的综述，《IEEE知识与数据工程汇刊》35 (9) (2023)
    8934–8954。 [doi:10.1109/TKDE.2022.3220219](https://doi.org/10.1109/TKDE.2022.3220219)。'
- en: '[158] G. Bortsova, F. Dubost, L. Hogeweg, I. Katramados, M. De Bruijne, Semi-supervised
    medical image segmentation via learning consistency under transformations, in:
    Medical Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International
    Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part VI 22, Springer,
    2019, pp. 810–818.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] G. Bortsova, F. Dubost, L. Hogeweg, I. Katramados, M. De Bruijne, 通过学习变换下的一致性进行半监督医学图像分割，发表于：医学图像计算与计算机辅助干预–MICCAI
    2019：第22届国际会议，中国深圳，2019年10月13日至17日，会议录，第VI部分 22，Springer，2019，第810–818页。'
- en: '[159] X. Li, L. Yu, H. Chen, C.-W. Fu, L. Xing, P.-A. Heng, Transformation-consistent
    self-ensembling model for semisupervised medical image segmentation, IEEE Transactions
    on Neural Networks and Learning Systems 32 (2) (2020) 523–534.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] X. Li, L. Yu, H. Chen, C.-W. Fu, L. Xing, P.-A. Heng, 变换一致自集成模型用于半监督医学图像分割，《IEEE神经网络与学习系统汇刊》32
    (2) (2020) 523–534。'
- en: '[160] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, C. A.
    Raffel, Mixmatch: A holistic approach to semi-supervised learning, Advances in
    neural information processing systems 32 (2019).'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, C. A.
    Raffel, Mixmatch：一种整体性半监督学习方法，《神经信息处理系统进展》32 (2019)。'
- en: '[161] H. Basak, R. Bhattacharya, R. Hussain, A. Chatterjee, An embarrassingly
    simple consistency regularization method for semi-supervised medical image segmentation,
    arXiv preprint arXiv:2202.00677 (2022).'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] H. Basak, R. Bhattacharya, R. Hussain, A. Chatterjee, 一种令人尴尬的简单一致性正则化方法用于半监督医学图像分割，arXiv预印本
    arXiv:2202.00677 (2022)。'
- en: '[162] K. Zheng, J. Xu, J. Wei, Double noise mean teacher self-ensembling model
    for semi-supervised tumor segmentation, in: ICASSP 2022-2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp.
    1446–1450.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] K. Zheng, J. Xu, J. Wei, 双噪声均值教师自集成模型用于半监督肿瘤分割，发表于：ICASSP 2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2022，第1446–1450页。'
- en: '[163] Y. Li, L. Luo, H. Lin, H. Chen, P.-A. Heng, Dual-consistency semi-supervised
    learning with uncertainty quantification for covid-19 lesion segmentation from
    ct images, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2021: 24th International Conference, Strasbourg, France, September 27–October
    1, 2021, Proceedings, Part II 24, Springer, 2021, pp. 199–209.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Y. Li, L. Luo, H. Lin, H. Chen, P.-A. Heng, 具有不确定性量化的双一致性半监督学习用于从CT图像中分割COVID-19病变，发表于：医学图像计算与计算机辅助干预–MICCAI
    2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日至10月1日，会议录，第II部分 24，Springer，2021，第199–209页。'
- en: '[164] X. Xu, T. Sanford, B. Turkbey, S. Xu, B. J. Wood, P. Yan, Shadow-consistent
    semi-supervised learning for prostate ultrasound segmentation, IEEE Transactions
    on Medical Imaging 41 (6) (2021) 1331–1345.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] X. Xu, T. Sanford, B. Turkbey, S. Xu, B. J. Wood, P. Yan, 影子一致性半监督学习用于前列腺超声分割，《IEEE医学成像汇刊》41
    (6) (2021) 1331–1345。'
- en: '[165] Y. Shu, H. Li, B. Xiao, X. Bi, W. Li, Cross-mix monitoring for medical
    image segmentation with limited supervision, IEEE Transactions on Multimedia (2022).'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Y. Shu, H. Li, B. Xiao, X. Bi, W. Li, 用于医学图像分割的交叉混合监控，《IEEE多媒体汇刊》（2022）。'
- en: '[166] M. Sajjadi, M. Javanmardi, T. Tasdizen, Regularization with stochastic
    transformations and perturbations for deep semi-supervised learning, Advances
    in neural information processing systems 29 (2016).'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] M. Sajjadi, M. Javanmardi, T. Tasdizen, 通过随机变换和扰动进行正则化的深度半监督学习，《神经信息处理系统进展》29
    (2016)。'
- en: '[167] X. Li, L. Yu, H. Chen, C.-W. Fu, P.-A. Heng, Semi-supervised skin lesion
    segmentation via transformation consistent self-ensembling model, arXiv preprint
    arXiv:1808.03887 (2018).'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] X. Li, L. Yu, H. Chen, C.-W. Fu, P.-A. Heng, 通过变换一致自集成模型进行半监督皮肤病变分割，arXiv预印本
    arXiv:1808.03887 (2018)。'
- en: '[168] S. Laine, T. Aila, Temporal ensembling for semi-supervised learning,
    in: International Conference on Learning Representations, 2016.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] S. Laine, T. Aila, 时间集成用于半监督学习，发表于：国际学习表征会议，2016。'
- en: '[169] X. Cao, H. Chen, Y. Li, Y. Peng, S. Wang, L. Cheng, Uncertainty aware
    temporal-ensembling model for semi-supervised abus mass segmentation, IEEE transactions
    on medical imaging 40 (1) (2020) 431–443.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] X. Cao, H. Chen, Y. Li, Y. Peng, S. Wang, L. Cheng, 具有不确定性意识的时间集成模型用于半监督恶性肿瘤分割，《IEEE医学成像汇刊》40
    (1) (2020) 431–443。'
- en: '[170] L. Luo, L. Yu, H. Chen, Q. Liu, X. Wang, J. Xu, P.-A. Heng, Deep mining
    external imperfect data for chest x-ray disease screening, IEEE transactions on
    medical imaging 39 (11) (2020) 3583–3594.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] L. Luo, L. Yu, H. Chen, Q. Liu, X. Wang, J. Xu, P.-A. Heng, 深度挖掘外部不完美数据用于胸部X光疾病筛查，《IEEE医学成像汇刊》39
    (11) (2020) 3583–3594。'
- en: '[171] A. Tarvainen, H. Valpola, Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results, Advances in
    neural information processing systems 30 (2017).'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] A. Tarvainen, H. Valpola, 均值教师是更好的榜样：加权平均一致性目标提升半监督深度学习结果，《神经信息处理系统进展》30
    (2017)。'
- en: '[172] L. Yu, S. Wang, X. Li, C.-W. Fu, P.-A. Heng, Uncertainty-aware self-ensembling
    model for semi-supervised 3d left atrium segmentation, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
    Shenzhen, China, October 13–17, 2019, Proceedings, Part II 22, Springer, 2019,
    pp. 605–613.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] L. Yu, S. Wang, X. Li, C.-W. Fu, P.-A. Heng, 不确定性感知自集成模型用于半监督3D左心房分割，载于《医学图像计算与计算机辅助干预–MICCAI
    2019：第22届国际会议》，中国深圳，2019年10月13–17日，论文集，第二部分 22，Springer，2019年，页605–613。'
- en: '[173] Z. Xu, Y. Wang, D. Lu, X. Luo, J. Yan, Y. Zheng, R. K.-y. Tong, Ambiguity-selective
    consistency regularization for mean-teacher semi-supervised medical image segmentation,
    Medical Image Analysis 88 (2023) 102880.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Z. Xu, Y. Wang, D. Lu, X. Luo, J. Yan, Y. Zheng, R. K.-y. Tong, 模糊选择性一致性正则化用于均值教师半监督医学图像分割，《医学图像分析》88
    (2023) 102880。'
- en: '[174] K. Wang, B. Zhan, C. Zu, X. Wu, J. Zhou, L. Zhou, Y. Wang, Tripled-uncertainty
    guided mean teacher model for semi-supervised medical image segmentation, in:
    Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
    Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part
    II 24, Springer, 2021, pp. 450–460.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] K. Wang, B. Zhan, C. Zu, X. Wu, J. Zhou, L. Zhou, Y. Wang, 三重不确定性引导的均值教师模型用于半监督医学图像分割，载于《医学图像计算与计算机辅助干预–MICCAI
    2021：第24届国际会议》，法国斯特拉斯堡，2021年9月27日–10月1日，论文集，第二部分 24，Springer，2021年，页450–460。'
- en: '[175] J. Zhu, B. Bolsterlee, B. V. Chow, Y. Song, E. Meijering, Hybrid dual
    mean-teacher network with double-uncertainty guidance for semi-supervised segmentation
    of mri scans, arXiv preprint arXiv:2303.05126 (2023).'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] J. Zhu, B. Bolsterlee, B. V. Chow, Y. Song, E. Meijering, 混合双重均值教师网络与双重不确定性引导用于半监督MRI扫描分割，arXiv预印本
    arXiv:2303.05126 (2023)。'
- en: '[176] C. Xu, Y. Yang, Z. Xia, B. Wang, D. Zhang, Y. Zhang, S. Zhao, Dual uncertainty-guided
    mixing consistency for semi-supervised 3d medical image segmentation, IEEE Transactions
    on Big Data (2023).'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] C. Xu, Y. Yang, Z. Xia, B. Wang, D. Zhang, Y. Zhang, S. Zhao, 双重不确定性引导的混合一致性用于半监督3D医学图像分割，《IEEE大数据汇刊》(2023)。'
- en: '[177] A. Lou, K. Tawfik, X. Yao, Z. Liu, J. Noble, Min-max similarity: A contrastive
    semi-supervised deep learning network for surgical tools segmentation, IEEE Transactions
    on Medical Imaging (2023).'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] A. Lou, K. Tawfik, X. Yao, Z. Liu, J. Noble, 最小-最大相似度：用于外科工具分割的对比半监督深度学习网络，《IEEE医学成像汇刊》(2023)。'
- en: '[178] S. Sedai, D. Mahapatra, S. Hewavitharanage, S. Maetschke, R. Garnavi,
    Semi-supervised segmentation of optic cup in retinal fundus images using variational
    autoencoder, in: Medical Image Computing and Computer-Assisted Intervention- MICCAI
    2017: 20th International Conference, Quebec City, QC, Canada, September 11-13,
    2017, Proceedings, Part II 20, Springer, 2017, pp. 75–82.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] S. Sedai, D. Mahapatra, S. Hewavitharanage, S. Maetschke, R. Garnavi,
    使用变分自编码器进行视神经杯在视网膜眼底图像中的半监督分割，载于《医学图像计算与计算机辅助干预-MICCAI 2017：第20届国际会议》，加拿大魁北克市，2017年9月11-13日，论文集，第二部分
    20，Springer，2017年，页75–82。'
- en: '[179] H. Wu, G. Chen, Z. Wen, J. Qin, Collaborative and adversarial learning
    of focused and dispersive representations for semi-supervised polyp segmentation,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021,
    pp. 3489–3498.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] H. Wu, G. Chen, Z. Wen, J. Qin, 聚焦与分散表示的协作与对抗学习用于半监督息肉分割，载于《IEEE/CVF国际计算机视觉会议论文集》，2021年，页3489–3498。'
- en: '[180] P. Wang, J. Peng, M. Pedersoli, Y. Zhou, C. Zhang, C. Desrosiers, Cat:
    Constrained adversarial training for anatomically-plausible semi-supervised segmentation,
    IEEE Transactions on Medical Imaging (2023).'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] P. Wang, J. Peng, M. Pedersoli, Y. Zhou, C. Zhang, C. Desrosiers, CAT：用于解剖上合理的半监督分割的约束对抗训练，《IEEE医学成像汇刊》(2023)。'
- en: '[181] Y. Zhang, L. Yang, J. Chen, M. Fredericksen, D. P. Hughes, D. Z. Chen,
    Deep adversarial networks for biomedical image segmentation utilizing unannotated
    images, in: Medical Image Computing and Computer Assisted Intervention- MICCAI
    2017: 20th International Conference, Quebec City, QC, Canada, September 11-13,
    2017, Proceedings, Part III 20, Springer, 2017, pp. 408–416.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Y. Zhang, L. Yang, J. Chen, M. Fredericksen, D. P. Hughes, D. Z. Chen,
    深度对抗网络用于利用未标注图像的生物医学图像分割，发表于《医学图像计算与计算机辅助手术-MICCAI 2017：第20届国际会议》，加拿大魁北克市，2017年9月11-13日，论文集，第III部分
    20，施普林格，2017年，页408–416。'
- en: '[182] H. Peiris, Z. Chen, G. Egan, M. Harandi, Duo-segnet: adversarial dual-views
    for semi-supervised medical image segmentation, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg,
    France, September 27–October 1, 2021, Proceedings, Part II 24, Springer, 2021,
    pp. 428–438.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] H. Peiris, Z. Chen, G. Egan, M. Harandi, Duo-segnet：用于半监督医学图像分割的对抗性双视图，发表于《医学图像计算与计算机辅助手术–MICCAI
    2021：第24届国际会议》，法国斯特拉斯堡，2021年9月27日至10月1日，论文集，第II部分 24，施普林格，2021年，页428–438。'
- en: '[183] J. Hou, X. Ding, J. D. Deng, Semi-supervised semantic segmentation of
    vessel images using leaking perturbations, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, 2022, pp. 2625–2634.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] J. Hou, X. Ding, J. D. Deng, 使用泄漏扰动的半监督语义分割船体图像，发表于《IEEE/CVF冬季计算机视觉应用会议论文集》，2022年，页2625–2634。'
- en: '[184] K. Chaitanya, N. Karani, C. F. Baumgartner, E. Erdil, A. Becker, O. Donati,
    E. Konukoglu, Semi-supervised task-driven data augmentation for medical image
    segmentation, Medical Image Analysis 68 (2021) 101934.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] K. Chaitanya, N. Karani, C. F. Baumgartner, E. Erdil, A. Becker, O. Donati,
    E. Konukoglu, 基于任务的半监督数据增强用于医学图像分割，《医学图像分析》 68 (2021) 101934。'
- en: '[185] D. P. Kingma, M. Welling, Auto-encoding variational bayes, arXiv preprint
    arXiv:1312.6114 (2013).'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] D. P. Kingma, M. Welling, 自编码变分贝叶斯，arXiv预印本 arXiv:1312.6114 (2013)。'
- en: '[186] J. Wang, T. Lukasiewicz, Rethinking bayesian deep learning methods for
    semi-supervised volumetric medical image segmentation, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 182–190.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] J. Wang, T. Lukasiewicz, 重新思考贝叶斯深度学习方法用于半监督体积医学图像分割，发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，2022年，页182–190。'
- en: '[187] H. Jiang, Y. Zhou, Y. Lin, R. C. Chan, J. Liu, H. Chen, Deep learning
    for computational cytology: A survey, Medical Image Analysis (2022) 102691.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] H. Jiang, Y. Zhou, Y. Lin, R. C. Chan, J. Liu, H. Chen, 深度学习在计算细胞学中的应用：综述，《医学图像分析》
    (2022) 102691。'
- en: '[188] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, L. Shao,
    Inf-net: Automatic covid-19 lung infection segmentation from ct images, IEEE transactions
    on medical imaging 39 (8) (2020) 2626–2637.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, L. Shao,
    Inf-net：从CT图像中自动分割COVID-19肺部感染，《IEEE医学成像交易》 39 (8) (2020) 2626–2637。'
- en: '[189] D.-H. Lee, et al., Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks, in: Workshop on challenges in representation
    learning, ICML, Vol. 3, Atlanta, 2013, p. 896.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] D.-H. Lee, 等，伪标签：简单高效的深度神经网络半监督学习方法，发表于《ICML代表学习挑战研讨会》，卷3，亚特兰大，2013年，页896。'
- en: '[190] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker,
    A. King, P. M. Matthews, D. Rueckert, Semi-supervised learning for network-based
    cardiac mr image segmentation, in: Medical Image Computing and Computer-Assisted
    Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada,
    September 11-13, 2017, Proceedings, Part II 20, Springer, 2017, pp. 253–260.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker,
    A. King, P. M. Matthews, D. Rueckert, 网络基础心脏MR图像分割的半监督学习，发表于《医学图像计算与计算机辅助手术-MICCAI
    2017：第20届国际会议》，加拿大魁北克市，2017年9月11-13日，论文集，第II部分 20，施普林格，2017年，页253–260。'
- en: '[191] G. Wang, S. Zhai, G. Lasio, B. Zhang, B. Yi, S. Chen, T. J. Macvittie,
    D. Metaxas, J. Zhou, S. Zhang, Semi-supervised segmentation of radiation-induced
    pulmonary fibrosis from lung ct scans with multi-scale guided dense attention,
    IEEE transactions on medical imaging 41 (3) (2021) 531–542.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] G. Wang, S. Zhai, G. Lasio, B. Zhang, B. Yi, S. Chen, T. J. Macvittie,
    D. Metaxas, J. Zhou, S. Zhang, 具有多尺度引导密集注意力的辐射诱导肺纤维化半监督分割，发表于《IEEE医学成像交易》 41 (3)
    (2021) 531–542。'
- en: '[192] R. Ke, A. I. Aviles-Rivero, S. Pandey, S. Reddy, C.-B. Schönlieb, A three-stage
    self-training framework for semi-supervised semantic segmentation, IEEE Transactions
    on Image Processing 31 (2022) 1805–1815.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] R. Ke, A. I. Aviles-Rivero, S. Pandey, S. Reddy, C.-B. Schönlieb, 三阶段自训练框架用于半监督语义分割，《IEEE图像处理交易》
    31 (2022) 1805–1815。'
- en: '[193] F. Liu, Y. Tian, Y. Chen, Y. Liu, V. Belagiannis, G. Carneiro, Acpl:
    Anti-curriculum pseudo-labelling for semi-supervised medical image classification,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2022, pp. 20697–20706.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] F. Liu, Y. Tian, Y. Chen, Y. Liu, V. Belagiannis, G. Carneiro, Acpl:
    反课程伪标记用于半监督医学图像分类，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第20697–20706页。'
- en: '[194] D. Chen, Y. Bai, W. Shen, Q. Li, L. Yu, Y. Wang, Magicnet: Semi-supervised
    multi-organ segmentation via magic-cube partition and recovery, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    23869–23878.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] D. Chen, Y. Bai, W. Shen, Q. Li, L. Yu, Y. Wang, Magicnet: 通过魔方分区和恢复进行半监督多器官分割，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2023年，第23869–23878页。'
- en: '[195] A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training,
    in: Proceedings of the eleventh annual conference on Computational learning theory,
    1998, pp. 92–100.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] A. Blum, T. Mitchell, 结合标记和未标记数据进行共同训练，载于：第十一届计算学习理论年会论文集，1998年，第92–100页。'
- en: '[196] L. Zhu, K. Yang, M. Zhang, L. L. Chan, T. K. Ng, B. C. Ooi, Semi-supervised
    unpaired multi-modal learning for label-efficient medical image segmentation,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part II 24, Springer, 2021, pp. 394–404.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] L. Zhu, K. Yang, M. Zhang, L. L. Chan, T. K. Ng, B. C. Ooi, 半监督未配对多模态学习用于标签高效医学图像分割，载于：医学图像计算与计算机辅助干预–MICCAI
    2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日–10月1日，论文集，第二部分 24，Springer，2021年，第394–404页。'
- en: '[197] X. Chen, H.-Y. Zhou, F. Liu, J. Guo, L. Wang, Y. Yu, Mass: Modality-collaborative
    semi-supervised segmentation by exploiting cross-modal consistency from unpaired
    ct and mri images, Medical Image Analysis 80 (2022) 102506.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] X. Chen, H.-Y. Zhou, F. Liu, J. Guo, L. Wang, Y. Yu, Mass: 通过利用未配对CT和MRI图像的跨模态一致性进行模态协作半监督分割，《医学图像分析》80
    (2022) 102506。'
- en: '[198] X. Luo, M. Hu, T. Song, G. Wang, S. Zhang, Semi-supervised medical image
    segmentation via cross teaching between cnn and transformer, in: International
    Conference on Medical Imaging with Deep Learning, PMLR, 2022, pp. 820–833.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] X. Luo, M. Hu, T. Song, G. Wang, S. Zhang, 通过CNN和Transformer之间的跨教学进行半监督医学图像分割，载于：深度学习医学成像国际会议，PMLR，2022年，第820–833页。'
- en: '[199] J. Peng, G. Estrada, M. Pedersoli, C. Desrosiers, Deep co-training for
    semi-supervised image segmentation, Pattern Recognition 107 (2020) 107269.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Peng, G. Estrada, M. Pedersoli, C. Desrosiers, 深度共同训练用于半监督图像分割，《模式识别》107
    (2020) 107269。'
- en: '[200] Z. Zhao, J. Hu, Z. Zeng, X. Yang, P. Qian, B. Veeravalli, C. Guan, Mmgl:
    Multi-scale multi-view global-local contrastive learning for semi-supervised cardiac
    image segmentation, in: 2022 IEEE international conference on image processing
    (ICIP), IEEE, 2022, pp. 401–405.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Z. Zhao, J. Hu, Z. Zeng, X. Yang, P. Qian, B. Veeravalli, C. Guan, MMGL:
    多尺度多视角全局-局部对比学习用于半监督心脏图像分割，载于：2022年IEEE国际图像处理会议（ICIP），IEEE，2022年，第401–405页。'
- en: '[201] H. Wang, X. Li, Dhc: Dual-debiased heterogeneous co-training framework
    for class-imbalanced semi-supervised medical image segmentation, arXiv preprint
    arXiv:2307.11960 (2023).'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] H. Wang, X. Li, DHC: 双偏倚异质共同训练框架用于类别不平衡的半监督医学图像分割，arXiv预印本 arXiv:2307.11960
    (2023)。'
- en: '[202] X. Wang, H. Chen, H. Xiang, H. Lin, X. Lin, P.-A. Heng, Deep virtual
    adversarial self-training with consistency regularization for semi-supervised
    medical image classification, Medical image analysis 70 (2021) 102010.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] X. Wang, H. Chen, H. Xiang, H. Lin, X. Lin, P.-A. Heng, 具有一致性正则化的深度虚拟对抗自训练用于半监督医学图像分类，《医学图像分析》70
    (2021) 102010。'
- en: '[203] D. Wang, Y. Zhang, K. Zhang, L. Wang, Focalmix: Semi-supervised learning
    for 3d medical image detection, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2020, pp. 3951–3960.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] D. Wang, Y. Zhang, K. Zhang, L. Wang, Focalmix: 半监督学习用于3D医学图像检测，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第3951–3960页。'
- en: '[204] W. Zhang, L. Zhu, J. Hallinan, S. Zhang, A. Makmur, Q. Cai, B. C. Ooi,
    Boostmis: Boosting medical image semi-supervised learning with adaptive pseudo
    labeling and informative active annotation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 20666–20676.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] W. Zhang, L. Zhu, J. Hallinan, S. Zhang, A. Makmur, Q. Cai, B. C. Ooi,
    Boostmis: 通过自适应伪标记和信息性主动标注提升医学图像半监督学习，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第20666–20676页。'
- en: '[205] K. Chaitanya, E. Erdil, N. Karani, E. Konukoglu, Local contrastive loss
    with pseudo-label based self-training for semi-supervised medical image segmentation,
    Medical Image Analysis 87 (2023) 102792.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] K. Chaitanya, E. Erdil, N. Karani, E. Konukoglu, 基于伪标签的自我训练与局部对比损失用于半监督医学图像分割，医学图像分析
    87 (2023) 102792。'
- en: '[206] H. Basak, Z. Yin, Pseudo-label guided contrastive learning for semi-supervised
    medical image segmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2023, pp. 19786–19797.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] H. Basak, Z. Yin, 基于伪标签的对比学习用于半监督医学图像分割，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2023，pp.
    19786–19797。'
- en: '[207] S. Zhang, J. Zhang, B. Tian, T. Lukasiewicz, Z. Xu, Multi-modal contrastive
    mutual learning and pseudo-label re-learning for semi-supervised medical image
    segmentation, Medical Image Analysis 83 (2023) 102656.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] S. Zhang, J. Zhang, B. Tian, T. Lukasiewicz, Z. Xu, 多模态对比互学习与伪标签再学习用于半监督医学图像分割，医学图像分析
    83 (2023) 102656。'
- en: '[208] L.-L. Zeng, K. Gao, D. Hu, Z. Feng, C. Hou, P. Rong, W. Wang, Ss-tbn:
    A semi-supervised tri-branch network for covid-19 screening and lesion segmentation,
    IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] L.-L. Zeng, K. Gao, D. Hu, Z. Feng, C. Hou, P. Rong, W. Wang, Ss-tbn:
    一种用于新冠病毒筛查和病变分割的半监督三分支网络，IEEE模式分析与机器智能交易 (2023)。'
- en: '[209] T. Lei, D. Zhang, X. Du, X. Wang, Y. Wan, A. K. Nandi, Semi-supervised
    medical image segmentation using adversarial consistency learning and dynamic
    convolution network, IEEE Transactions on Medical Imaging (2022).'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] T. Lei, D. Zhang, X. Du, X. Wang, Y. Wan, A. K. Nandi, 使用对抗一致性学习和动态卷积网络的半监督医学图像分割，IEEE医学成像交易
    (2022)。'
- en: '[210] S. Budd, E. C. Robinson, B. Kainz, A survey on active learning and human-in-the-loop
    deep learning for medical image analysis, Medical Image Analysis 71 (2021) 102062.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] S. Budd, E. C. Robinson, B. Kainz, 关于医学图像分析中主动学习与人机协作深度学习的调研，医学图像分析 71
    (2021) 102062。'
- en: '[211] J. Peng, Y. Wang, Medical image segmentation with limited supervision:
    A review of deep network models, IEEE Access 9 (2021) 36827–36851. [doi:10.1109/ACCESS.2021.3062380](https://doi.org/10.1109/ACCESS.2021.3062380).'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] J. Peng, Y. Wang, 有限监督下的医学图像分割：深度网络模型综述，IEEE Access 9 (2021) 36827–36851.
    [doi:10.1109/ACCESS.2021.3062380](https://doi.org/10.1109/ACCESS.2021.3062380)。'
- en: '[212] J. Liu, L. Cao, Y. Tian, Deep active learning for effective pulmonary
    nodule detection, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings,
    Part VI 23, Springer, 2020, pp. 609–618.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] J. Liu, L. Cao, Y. Tian, 深度主动学习用于有效的肺结节检测，发表于：医学图像计算与计算机辅助干预–MICCAI 2020：第23届国际会议，秘鲁利马，2020年10月4–8日，会议录，第六部分
    23，Springer，2020，pp. 609–618。'
- en: '[213] S. Wen, T. M. Kurc, L. Hou, J. H. Saltz, R. R. Gupta, R. Batiste, T. Zhao,
    V. Nguyen, D. Samaras, W. Zhu, Comparison of different classifiers with active
    learning to support quality control in nucleus segmentation in pathology images,
    AMIA Summits on Translational Science Proceedings 2018 (2018) 227.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] S. Wen, T. M. Kurc, L. Hou, J. H. Saltz, R. R. Gupta, R. Batiste, T.
    Zhao, V. Nguyen, D. Samaras, W. Zhu, 比较不同分类器与主动学习在病理图像细胞核分割质量控制中的应用，AMIA翻译科学峰会论文集
    2018 (2018) 227。'
- en: '[214] X. Wu, C. Chen, M. Zhong, J. Wang, J. Shi, Covid-al: The diagnosis of
    covid-19 with deep active learning, Medical Image Analysis 68 (2021) 101913.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] X. Wu, C. Chen, M. Zhong, J. Wang, J. Shi, Covid-al: 利用深度主动学习诊断新冠病毒，医学图像分析
    68 (2021) 101913。'
- en: '[215] Z. Zhou, J. Y. Shin, S. R. Gurudu, M. B. Gotway, J. Liang, Active, continual
    fine tuning of convolutional neural networks for reducing annotation efforts,
    Medical image analysis 71 (2021) 101997.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] Z. Zhou, J. Y. Shin, S. R. Gurudu, M. B. Gotway, J. Liang, 主动的、持续的卷积神经网络微调以减少标注工作量，医学图像分析
    71 (2021) 101997。'
- en: '[216] S. Balaram, C. M. Nguyen, A. Kassim, P. Krishnaswamy, Consistency-based
    semi-supervised evidential active learning for diagnostic radiograph classification,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 675–685.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] S. Balaram, C. M. Nguyen, A. Kassim, P. Krishnaswamy, 基于一致性的半监督证据主动学习用于诊断X光图像分类，发表于：国际医学图像计算与计算机辅助干预会议，Springer，2022，pp.
    675–685。'
- en: '[217] W. Kuo, C. Häne, E. Yuh, P. Mukherjee, J. Malik, Cost-sensitive active
    learning for intracranial hemorrhage detection, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2018: 21st International Conference, Granada,
    Spain, September 16-20, 2018, Proceedings, Part III 11, Springer, 2018, pp. 715–723.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] W. Kuo, C. Hänke, E. Yuh, P. Mukherjee, J. Malik, 针对颅内出血检测的成本敏感主动学习，见：医学图像计算与计算机辅助干预–MICCAI
    2018：第21届国际会议，西班牙格拉纳达，2018年9月16-20日，会议论文集，第三部分11，Springer，2018年，页码715–723。'
- en: '[218] W. H. Beluch, T. Genewein, A. Nürnberger, J. M. Köhler, The power of
    ensembles for active learning in image classification, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2018, pp. 9368–9377.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] W. H. Beluch, T. Genewein, A. Nürnberg, J. M. Köhler, 集成方法在图像分类中的主动学习能力，见：IEEE计算机视觉与模式识别会议论文集，2018年，页码9368–9377。'
- en: '[219] A. Atzeni, L. Peter, E. Robinson, E. Blackburn, J. Althonayan, D. C.
    Alexander, J. E. Iglesias, Deep active learning for suggestive segmentation of
    biomedical image stacks via optimisation of dice scores and traced boundary length,
    Medical Image Analysis 81 (2022) 102549.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] A. Atzeni, L. Peter, E. Robinson, E. Blackburn, J. Althonayan, D. C.
    Alexander, J. E. Iglesias, 通过优化骰子分数和边界长度的深度主动学习用于生物医学图像堆叠的建议分割，《医学图像分析》81 (2022)
    102549。'
- en: '[220] Y. Gal, R. Islam, Z. Ghahramani, Deep bayesian active learning with image
    data, in: International conference on machine learning, PMLR, 2017, pp. 1183–1192.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Y. Gal, R. Islam, Z. Ghahramani, 深度贝叶斯主动学习与图像数据，见：国际机器学习会议，PMLR，2017年，页码1183–1192。'
- en: '[221] D. Mahapatra, B. Bozorgtabar, J.-P. Thiran, M. Reyes, Efficient active
    learning for image classification and segmentation using a sample selection and
    conditional generative adversarial network, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer, 2018, pp. 580–588.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] D. Mahapatra, B. Bozorgtabar, J.-P. Thiran, M. Reyes, 使用样本选择和条件生成对抗网络的高效图像分类和分割主动学习，见：医学图像计算与计算机辅助干预国际会议，Springer，2018年，页码580–588。'
- en: '[222] C. Dai, S. Wang, Y. Mo, K. Zhou, E. Angelini, Y. Guo, W. Bai, Suggestive
    annotation of brain tumour images with gradient-guided sampling, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part IV 23, Springer, 2020, pp. 156–165.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] C. Dai, S. Wang, Y. Mo, K. Zhou, E. Angelini, Y. Guo, W. Bai, 使用梯度引导采样对脑肿瘤图像的建议性标注，见：医学图像计算与计算机辅助干预–MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4-8日，会议论文集，第四部分23，Springer，2020年，页码156–165。'
- en: '[223] V. Nath, D. Yang, H. R. Roth, D. Xu, Warm start active learning with
    proxy labels and selection via semi-supervised fine-tuning, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer,
    2022, pp. 297–308.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] V. Nath, D. Yang, H. R. Roth, D. Xu, 使用代理标签和通过半监督微调进行选择的暖启动主动学习，见：医学图像计算与计算机辅助干预国际会议，Springer，2022年，页码297–308。'
- en: '[224] X. Li, M. Xia, J. Jiao, S. Zhou, C. Chang, Y. Wang, Y. Guo, Hal-ia: A
    hybrid active learning framework using interactive annotation for medical image
    segmentation, Medical Image Analysis (2023) 102862.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] X. Li, M. Xia, J. Jiao, S. Zhou, C. Chang, Y. Wang, Y. Guo, Hal-ia：一种使用交互式标注的混合主动学习框架，用于医学图像分割，《医学图像分析》
    (2023) 102862。'
- en: '[225] L. Yang, Y. Zhang, J. Chen, S. Zhang, D. Z. Chen, Suggestive annotation:
    A deep active learning framework for biomedical image segmentation, in: Medical
    Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International
    Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part
    III 20, Springer, 2017, pp. 399–407.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] L. Yang, Y. Zhang, J. Chen, S. Zhang, D. Z. Chen, 建议性标注：用于生物医学图像分割的深度主动学习框架，见：医学图像计算与计算机辅助干预-MICCAI
    2017：第20届国际会议，加拿大魁北克市，2017年9月11-13日，会议论文集，第三部分20，Springer，2017年，页码399–407。'
- en: '[226] F. Ozdemir, Z. Peng, P. Fuernstahl, C. Tanner, O. Goksel, Active learning
    for segmentation based on bayesian sample queries, Knowledge-Based Systems 214
    (2021) 106531.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] F. Ozdemir, Z. Peng, P. Fuernstahl, C. Tanner, O. Goksel, 基于贝叶斯样本查询的分割主动学习，《知识基础系统》214
    (2021) 106531。'
- en: '[227] S. Zhao, J. Song, S. Ermon, Infovae: Information maximizing variational
    autoencoders, arXiv preprint arXiv:1706.02262 (2017).'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] S. Zhao, J. Song, S. Ermon, Infovae：信息最大化变分自编码器，arXiv预印本 arXiv:1706.02262
    (2017)。'
- en: '[228] W. Li, J. Li, Z. Wang, J. Polson, A. E. Sisk, D. P. Sajed, W. Speier,
    C. W. Arnold, Pathal: An active learning framework for histopathology image analysis,
    IEEE Transactions on Medical Imaging 41 (5) (2021) 1176–1187.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] W. Li, J. Li, Z. Wang, J. Polson, A. E. Sisk, D. P. Sajed, W. Speier,
    C. W. Arnold, Pathal：一个用于组织病理图像分析的主动学习框架，《IEEE医学成像学报》41 (5) (2021) 1176–1187。'
- en: '[229] S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R. Scott, D. Huang,
    Curriculumnet: Weakly supervised learning from large-scale web images, in: Proceedings
    of the European conference on computer vision (ECCV), 2018, pp. 135–150.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R. Scott, D. Huang,
    Curriculumnet：从大规模网页图像中进行弱监督学习，发表于：欧洲计算机视觉会议（ECCV）论文集，2018，第135–150页。'
- en: '[230] Z. Wang, Z. Yin, Annotation-efficient cell counting, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,
    Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VIII 24, Springer,
    2021, pp. 405–414.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Z. Wang, Z. Yin, 注释高效的细胞计数，发表于：医学图像计算与计算机辅助手术–MICCAI 2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日至10月1日，会议录，第VIII部分24，Springer，2021，第405–414页。'
- en: '[231] S. Kumari, P. Singh, Deep learning for unsupervised domain adaptation
    in medical imaging: Recent advancements and future perspectives, arXiv preprint
    arXiv:2308.01265 (2023).'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] S. Kumari, P. Singh, 深度学习在医学成像中的无监督领域适应：近期进展和未来展望，arXiv预印本 arXiv:2308.01265
    (2023)。'
- en: '[232] M. Yu, H. Guan, Y. Fang, L. Yue, M. Liu, Domain-prior-induced structural
    mri adaptation for clinical progression prediction of subjective cognitive decline,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 24–33.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] M. Yu, H. Guan, Y. Fang, L. Yue, M. Liu, 领域优先诱导的结构MRI适应用于主观认知衰退的临床进展预测，发表于：国际医学图像计算与计算机辅助手术会议，Springer，2022，第24–33页。'
- en: '[233] Y. Fang, M. Wang, G. G. Potter, M. Liu, Unsupervised cross-domain functional
    mri adaptation for automated major depressive disorder identification, Medical
    Image Analysis 84 (2023) 102707.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] Y. Fang, M. Wang, G. G. Potter, M. Liu, 无监督跨领域功能MRI适应用于自动化主要抑郁症识别，《医学图像分析》84
    (2023) 102707。'
- en: '[234] Q. Hu, H. Li, J. Zhang, Domain-adaptive 3d medical image synthesis: An
    efficient unsupervised approach, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2022: 25th International Conference, Singapore, September
    18–22, 2022, Proceedings, Part VI, Springer, 2022, pp. 495–504.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Q. Hu, H. Li, J. Zhang, 领域自适应3D医学图像合成：一种高效的无监督方法，发表于：医学图像计算与计算机辅助手术–MICCAI
    2022：第25届国际会议，新加坡，2022年9月18日至22日，会议录，第VI部分，Springer，2022，第495–504页。'
- en: '[235] M. Sahu, R. Strömsdörfer, A. Mukhopadhyay, S. Zachow, Endo-sim2real:
    Consistency learning-based domain adaptation for instrument segmentation, in:
    International Conference on Medical Image Computing and Computer-Assisted Intervention,
    Springer, 2020, pp. 784–794.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] M. Sahu, R. Strömsdörfer, A. Mukhopadhyay, S. Zachow, Endo-sim2real：基于一致性学习的领域适应用于仪器分割，发表于：国际医学图像计算与计算机辅助手术会议，Springer，2020，第784–794页。'
- en: '[236] A. Gomariz, H. Lu, Y. Y. Li, T. Albrecht, A. Maunz, F. Benmansour, A. M.
    Valcarcel, J. Luu, D. Ferrara, O. Goksel, Unsupervised domain adaptation with
    contrastive learning for oct segmentation, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer, 2022, pp. 351–361.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] A. Gomariz, H. Lu, Y. Y. Li, T. Albrecht, A. Maunz, F. Benmansour, A.
    M. Valcarcel, J. Luu, D. Ferrara, O. Goksel, 基于对比学习的无监督领域适应用于OCT分割，发表于：国际医学图像计算与计算机辅助手术会议，Springer，2022，第351–361页。'
- en: '[237] F. Wu, X. Zhuang, Cf distance: a new domain discrepancy metric and application
    to explicit domain adaptation for cross-modality cardiac image segmentation, IEEE
    Transactions on Medical Imaging 39 (12) (2020) 4274–4285.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] F. Wu, X. Zhuang, Cf距离：一种新的领域差异度量及其在跨模态心脏图像分割中的显式领域适应应用，《IEEE医学成像学报》39
    (12) (2020) 4274–4285。'
- en: '[238] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, V. Lempitsky, Domain-adversarial training of neural networks, The
    journal of machine learning research 17 (1) (2016) 2096–2030.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, V. Lempitsky, 神经网络的领域对抗训练，《机器学习研究期刊》17 (1) (2016) 2096–2030。'
- en: '[239] J. Ren, I. Hacihaliloglu, E. A. Singer, D. J. Foran, X. Qi, Adversarial
    domain adaptation for classification of prostate histopathology whole-slide images,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st
    International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part II 11, Springer, 2018, pp. 201–209.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] J. Ren, I. Hacihaliloglu, E. A. Singer, D. J. Foran, X. Qi, 用于前列腺组织病理全图像分类的对抗性领域适应，见于：医学图像计算与计算机辅助手术–MICCAI
    2018: 第21届国际会议，西班牙格拉纳达，2018年9月16-20日，论文集，第二部分 11，Springer，2018年，页码 201–209。'
- en: '[240] Y. Zhang, H. Chen, Y. Wei, P. Zhao, J. Cao, X. Fan, X. Lou, H. Liu, J. Hou,
    X. Han, et al., From whole slide imaging to microscopy: Deep microscopy adaptation
    network for histopathology cancer image classification, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2019,
    pp. 360–368.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Y. Zhang, H. Chen, Y. Wei, P. Zhao, J. Cao, X. Fan, X. Lou, H. Liu, J.
    Hou, X. Han, 等，从全幻灯片成像到显微镜：用于组织病理癌症图像分类的深度显微镜适应网络，见于：医学图像计算与计算机辅助手术国际会议，Springer，2019年，页码
    360–368。'
- en: '[241] Y. Feng, Z. Wang, X. Xu, Y. Wang, H. Fu, S. Li, L. Zhen, X. Lei, Y. Cui,
    J. S. Z. Ting, et al., Contrastive domain adaptation with consistency match for
    automated pneumonia diagnosis, Medical Image Analysis 83 (2023) 102664.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Y. Feng, Z. Wang, X. Xu, Y. Wang, H. Fu, S. Li, L. Zhen, X. Lei, Y. Cui,
    J. S. Z. Ting, 等，具有一致性匹配的对比领域适应用于自动化肺炎诊断，《医学图像分析》83 (2023) 102664。'
- en: '[242] C. Bian, C. Yuan, J. Wang, M. Li, X. Yang, S. Yu, K. Ma, J. Yuan, Y. Zheng,
    Uncertainty-aware domain alignment for anatomical structure segmentation, Medical
    Image Analysis 64 (2020) 101732.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] C. Bian, C. Yuan, J. Wang, M. Li, X. Yang, S. Yu, K. Ma, J. Yuan, Y.
    Zheng, 不确定性感知的领域对齐用于解剖结构分割，《医学图像分析》64 (2020) 101732。'
- en: '[243] L. Liu, Z. Zhang, S. Li, K. Ma, Y. Zheng, S-cuda: Self-cleansing unsupervised
    domain adaptation for medical image segmentation, Medical Image Analysis 74 (2021)
    102214.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] L. Liu, Z. Zhang, S. Li, K. Ma, Y. Zheng, S-cuda：自清洁无监督领域适应用于医学图像分割，《医学图像分析》74
    (2021) 102214。'
- en: '[244] W. Huang, X. Liu, Z. Cheng, Y. Zhang, Z. Xiong, Domain adaptive mitochondria
    segmentation via enforcing inter-section consistency, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 89–98.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] W. Huang, X. Liu, Z. Cheng, Y. Zhang, Z. Xiong, 通过强制交叉段一致性进行领域自适应线粒体分割，见于：医学图像计算与计算机辅助手术国际会议，Springer，2022年，页码
    89–98。'
- en: '[245] M. A. Karaoglu, N. Brasch, M. Stollenga, W. Wein, N. Navab, F. Tombari,
    A. Ladikos, Adversarial domain feature adaptation for bronchoscopic depth estimation,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part IV 24, Springer, 2021, pp. 300–310.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] M. A. Karaoglu, N. Brasch, M. Stollenga, W. Wein, N. Navab, F. Tombari,
    A. Ladikos, 用于支气管镜深度估计的对抗性领域特征适应，见于：医学图像计算与计算机辅助手术–MICCAI 2021: 第24届国际会议，法国斯特拉斯堡，2021年9月27日至10月1日，论文集，第四部分
    24，Springer，2021年，页码 300–310。'
- en: '[246] H. Hao, C. Xu, D. Zhang, Q. Yan, J. Zhang, Y. Liu, Y. Zhao, Sparse-based
    domain adaptation network for octa image super-resolution reconstruction, IEEE
    Journal of Biomedical and Health Informatics 26 (9) (2022) 4402–4413.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] H. Hao, C. Xu, D. Zhang, Q. Yan, J. Zhang, Y. Liu, Y. Zhao, 基于稀疏的领域适应网络用于八角图像超分辨率重建，《IEEE生物医学与健康信息学期刊》26
    (9) (2022) 4402–4413。'
- en: '[247] C. Yoo, H. W. Lee, J.-W. Kang, Transferring structured knowledge in unsupervised
    domain adaptation of a sleep staging network, IEEE journal of biomedical and health
    informatics 26 (3) (2021) 1273–1284.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] C. Yoo, H. W. Lee, J.-W. Kang, 在无监督领域适应下的睡眠分期网络中的结构化知识转移，《IEEE生物医学与健康信息学期刊》26
    (3) (2021) 1273–1284。'
- en: '[248] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image translation
    using cycle-consistent adversarial networks, in: Proceedings of the IEEE international
    conference on computer vision, 2017, pp. 2223–2232.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, 使用循环一致对抗网络的无配对图像到图像翻译，见于：IEEE国际计算机视觉会议论文集，2017年，页码
    2223–2232。'
- en: '[249] Y. Tang, Y. Tang, V. Sandfort, J. Xiao, R. M. Summers, Tuna-net: Task-oriented
    unsupervised adversarial network for disease recognition in cross-domain chest
    x-rays, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings,
    Part VI 22, Springer, 2019, pp. 431–440.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] Y. Tang, Y. Tang, V. Sandfort, J. Xiao, R. M. Summers, Tuna-net：用于跨领域胸部X光病症识别的任务导向无监督对抗网络，见于：医学图像计算与计算机辅助手术–MICCAI
    2019: 第22届国际会议，中国深圳，2019年10月13–17日，论文集，第六部分 22，Springer，2019年，页码 431–440。'
- en: '[250] J. Jiang, Y.-C. Hu, N. Tyagi, P. Zhang, A. Rimner, G. S. Mageras, J. O.
    Deasy, H. Veeraraghavan, Tumor-aware, adversarial domain adaptation from ct to
    mri for lung cancer segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2018: 21st International Conference, Granada, Spain, September
    16-20, 2018, Proceedings, Part II 11, Springer, 2018, pp. 777–785.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] J. Jiang, Y.-C. Hu, N. Tyagi, P. Zhang, A. Rimner, G. S. Mageras, J.
    O. Deasy, H. Veeraraghavan, 肿瘤感知的对抗性领域适应从CT到MRI用于肺癌分割，见：医学图像计算与计算机辅助干预–MICCAI
    2018: 第21届国际会议，西班牙格拉纳达，2018年9月16-20日，论文集，第二部分11，Springer，2018，页777–785。'
- en: '[251] J. Jiang, Y.-C. Hu, N. Tyagi, A. Rimner, N. Lee, J. O. Deasy, S. Berry,
    H. Veeraraghavan, Psigan: Joint probabilistic segmentation and image distribution
    matching for unpaired cross-modality adaptation-based mri segmentation, IEEE transactions
    on medical imaging 39 (12) (2020) 4071–4084.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] J. Jiang, Y.-C. Hu, N. Tyagi, A. Rimner, N. Lee, J. O. Deasy, S. Berry,
    H. Veeraraghavan, Psigan: 联合概率分割与图像分布匹配用于无配对跨模态适应的MRI分割，IEEE医学影像学报 39 (12) (2020)
    4071–4084。'
- en: '[252] D. Tomar, M. Lortkipanidze, G. Vray, B. Bozorgtabar, J.-P. Thiran, Self-attentive
    spatial adaptive normalization for cross-modality domain adaptation, IEEE Transactions
    on Medical Imaging 40 (10) (2021) 2926–2938.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] D. Tomar, M. Lortkipanidze, G. Vray, B. Bozorgtabar, J.-P. Thiran, 自注意力空间自适应归一化用于跨模态领域适应，IEEE医学影像学报
    40 (10) (2021) 2926–2938。'
- en: '[253] A. Kapil, A. Meier, K. Steele, M. Rebelatto, K. Nekolla, A. Haragan,
    A. Silva, A. Zuraw, C. Barker, M. L. Scott, et al., Domain adaptation-based deep
    learning for automated tumor cell (tc) scoring and survival analysis on pd-l1
    stained tissue images, IEEE Transactions on Medical Imaging 40 (9) (2021) 2513–2523.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] A. Kapil, A. Meier, K. Steele, M. Rebelatto, K. Nekolla, A. Haragan,
    A. Silva, A. Zuraw, C. Barker, M. L. Scott, 等，基于领域适应的深度学习用于自动肿瘤细胞（TC）评分和PD-L1染色组织图像的生存分析，IEEE医学影像学报
    40 (9) (2021) 2513–2523。'
- en: '[254] F. Xing, T. Bennett, D. Ghosh, Adversarial domain adaptation and pseudo-labeling
    for cross-modality microscopy image quantification, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
    Shenzhen, China, October 13–17, 2019, Proceedings, Part I 22, Springer, 2019,
    pp. 740–749.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] F. Xing, T. Bennett, D. Ghosh, 对抗性领域适应与伪标签用于跨模态显微镜图像量化，见：医学图像计算与计算机辅助干预–MICCAI
    2019: 第22届国际会议，中国深圳，2019年10月13–17日，论文集，第一部分22，Springer，2019，页740–749。'
- en: '[255] F. Xing, T. C. Cornish, T. D. Bennett, D. Ghosh, Bidirectional mapping-based
    domain adaptation for nucleus detection in cross-modality microscopy images, IEEE
    Transactions on Medical Imaging 40 (10) (2020) 2880–2896.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] F. Xing, T. C. Cornish, T. D. Bennett, D. Ghosh, 基于双向映射的领域适应用于跨模态显微镜图像中的核检测，IEEE医学影像学报
    40 (10) (2020) 2880–2896。'
- en: '[256] F. Xing, T. C. Cornish, Low-resource adversarial domain adaptation for
    cross-modality nucleus detection, in: International Conference on Medical Image
    Computing and Computer-Assisted Intervention, Springer, 2022, pp. 639–649.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] F. Xing, T. C. Cornish, 低资源对抗性领域适应用于跨模态核检测，见：医学图像计算与计算机辅助干预国际会议，Springer，2022，页639–649。'
- en: '[257] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, Domain generalization:
    A survey, IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, 领域泛化：综述，IEEE模式分析与机器智能学报（2022）。'
- en: '[258] J. Yang, N. C. Dvornek, F. Zhang, J. Chapiro, M. Lin, J. S. Duncan, Unsupervised
    domain adaptation via disentangled representations: Application to cross-modality
    liver segmentation, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings,
    Part II 22, Springer, 2019, pp. 255–263.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] J. Yang, N. C. Dvornek, F. Zhang, J. Chapiro, M. Lin, J. S. Duncan, 通过解耦表示的无监督领域适应：应用于跨模态肝脏分割，见：医学图像计算与计算机辅助干预–MICCAI
    2019: 第22届国际会议，中国深圳，2019年10月13–17日，论文集，第二部分22，Springer，2019，页255–263。'
- en: '[259] R. Wang, G. Zheng, Cycmis: Cycle-consistent cross-domain medical image
    segmentation via diverse image augmentation, Medical Image Analysis 76 (2022)
    102328.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] R. Wang, G. Zheng, Cycmis: 通过多样化图像增强实现的循环一致跨域医学图像分割，医学图像分析 76 (2022)
    102328。'
- en: '[260] X. Sun, Z. Liu, S. Zheng, C. Lin, Z. Zhu, Y. Zhao, Attention-enhanced
    disentangled representation learning for unsupervised domain adaptation in cardiac
    segmentation, in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 745–754.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] X. Sun, Z. Liu, S. Zheng, C. Lin, Z. Zhu, Y. Zhao, 基于注意力增强的解缠表示学习用于心脏分割的无监督领域适应，载于《医学图像计算与计算机辅助干预国际会议》，Springer，2022年，页745–754。'
- en: '[261] Q. Xie, Y. Li, N. He, M. Ning, K. Ma, G. Wang, Y. Lian, Y. Zheng, Unsupervised
    domain adaptation for medical image segmentation by disentanglement learning and
    self-training, IEEE Transactions on Medical Imaging (2022).'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] Q. Xie, Y. Li, N. He, M. Ning, K. Ma, G. Wang, Y. Lian, Y. Zheng, 通过解缠学习和自我训练进行医学图像分割的无监督领域适应，《IEEE医学影像学报》
    (2022)。'
- en: '[262] C. Yang, X. Guo, M. Zhu, B. Ibragimov, Y. Yuan, Mutual-prototype adaptation
    for cross-domain polyp segmentation, IEEE Journal of Biomedical and Health Informatics
    25 (10) (2021) 3886–3897.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] C. Yang, X. Guo, M. Zhu, B. Ibragimov, Y. Yuan, 跨领域息肉分割的互原型适应，《IEEE生物医学与健康信息学杂志》25
    (10) (2021) 3886–3897。'
- en: '[263] S. Wu, C. Chen, Z. Xiong, X. Chen, X. Sun, Uncertainty-aware label rectification
    for domain adaptive mitochondria segmentation, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg,
    France, September 27–October 1, 2021, Proceedings, Part III 24, Springer, 2021,
    pp. 191–200.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] S. Wu, C. Chen, Z. Xiong, X. Chen, X. Sun, 不确定性感知标签修正用于领域自适应线粒体分割，载于《医学图像计算与计算机辅助干预–MICCAI
    2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日–10月1日，会议论文集，第三部分24》，Springer，2021年，页191–200。'
- en: '[264] F. Gröger, A.-M. Rickmann, C. Wachinger, Strudel: Self-training with
    uncertainty dependent label refinement across domains, in: Machine Learning in
    Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with
    MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings 12, Springer,
    2021, pp. 306–316.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] F. Gröger, A.-M. Rickmann, C. Wachinger, Strudel: 跨领域的不确定性依赖标签细化自我训练，载于《医学影像中的机器学习：第12届国际研讨会，MLMI
    2021，与MICCAI 2021联合举办，法国斯特拉斯堡，2021年9月27日，会议论文集12》，Springer，2021年，页306–316。'
- en: '[265] Q. Qi, X. Lin, C. Chen, W. Xie, Y. Huang, X. Ding, X. Liu, Y. Yu, Curriculum
    feature alignment domain adaptation for epithelium-stroma classification in histopathological
    images, IEEE Journal of Biomedical and Health Informatics 25 (4) (2020) 1163–1172.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] Q. Qi, X. Lin, C. Chen, W. Xie, Y. Huang, X. Ding, X. Liu, Y. Yu, 针对组织病理图像中上皮-基质分类的课程特征对齐领域适应，《IEEE生物医学与健康信息学杂志》25
    (4) (2020) 1163–1172。'
- en: '[266] H. Cho, K. Nishimura, K. Watanabe, R. Bise, Effective pseudo-labeling
    based on heatmap for unsupervised domain adaptation in cell detection, Medical
    Image Analysis 79 (2022) 102436.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] H. Cho, K. Nishimura, K. Watanabe, R. Bise, 基于热图的有效伪标签用于细胞检测的无监督领域适应，《医学图像分析》79
    (2022) 102436。'
- en: '[267] A. Mottaghi, A. Sharghi, S. Yeung, O. Mohareri, Adaptation of surgical
    activity recognition models across operating rooms, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 530–540.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] A. Mottaghi, A. Sharghi, S. Yeung, O. Mohareri, 跨手术室的手术活动识别模型适应，载于《医学图像计算与计算机辅助干预国际会议》，Springer，2022年，页530–540。'
- en: '[268] Y. Sun, E. Tzeng, T. Darrell, A. A. Efros, Unsupervised domain adaptation
    through self-supervision, arXiv preprint arXiv:1909.11825 (2019).'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] Y. Sun, E. Tzeng, T. Darrell, A. A. Efros, 通过自我监督进行无监督领域适应，arXiv 预印本
    arXiv:1909.11825 (2019)。'
- en: '[269] N. A. Koohbanani, B. Unnikrishnan, S. A. Khurram, P. Krishnaswamy, N. Rajpoot,
    Self-path: Self-supervision for classification of pathology images with limited
    annotations, IEEE Transactions on Medical Imaging 40 (10) (2021) 2845–2856.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] N. A. Koohbanani, B. Unnikrishnan, S. A. Khurram, P. Krishnaswamy, N.
    Rajpoot, Self-path: 用于有限注释病理图像分类的自我监督，《IEEE医学影像学报》40 (10) (2021) 2845–2856。'
- en: '[270] C. Abbet, L. Studer, A. Fischer, H. Dawson, I. Zlobec, B. Bozorgtabar,
    J.-P. Thiran, Self-rule to multi-adapt: Generalized multi-source feature learning
    using unsupervised domain adaptation for colorectal cancer tissue detection, Medical
    image analysis 79 (2022) 102473.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] C. Abbet, L. Studer, A. Fischer, H. Dawson, I. Zlobec, B. Bozorgtabar,
    J.-P. Thiran, 从自我规则到多适应：使用无监督领域适应进行结直肠癌组织检测的广义多源特征学习，《医学图像分析》79 (2022) 102473。'
- en: '[271] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, A. Yuille, Domain
    adaptive relational reasoning for 3d multi-organ segmentation, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, Springer, 2020, pp. 656–666.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, A. Yuille, 用于3D多器官分割的领域自适应关系推理，发表于：医学图像计算与计算机辅助干预–MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4–8日，会议录，第I部分23，Springer，2020，第656–666页。'
- en: '[272] Y. Xue, S. Feng, Y. Zhang, X. Zhang, Y. Wang, Dual-task self-supervision
    for cross-modality domain adaptation, in: Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru,
    October 4–8, 2020, Proceedings, Part I 23, Springer, 2020, pp. 408–417.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] Y. Xue, S. Feng, Y. Zhang, X. Zhang, Y. Wang, 跨模态领域适应的双任务自监督，发表于：医学图像计算与计算机辅助干预–MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4–8日，会议录，第I部分23，Springer，2020，第408–417页。'
- en: '[273] C. Chen, Q. Dou, H. Chen, J. Qin, P. A. Heng, Unsupervised bidirectional
    cross-modality adaptation via deeply synergistic image and feature alignment for
    medical image segmentation, IEEE transactions on medical imaging 39 (7) (2020)
    2494–2505.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] C. Chen, Q. Dou, H. Chen, J. Qin, P. A. Heng, 无监督双向跨模态适应通过深度协同图像和特征对齐用于医学图像分割，《IEEE医学成像事务》39（7）（2020）2494–2505。'
- en: '[274] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    T. Darrell, Cycada: Cycle-consistent adversarial domain adaptation, in: International
    conference on machine learning, Pmlr, 2018, pp. 1989–1998.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    T. Darrell, Cycada：循环一致的对抗性领域适应，发表于：国际机器学习会议，Pmlr，2018，第1989–1998页。'
- en: '[275] X. Jia, S. Wang, X. Liang, A. Balagopal, D. Nguyen, M. Yang, Z. Wang,
    J. X. Ji, X. Qian, S. Jiang, Cone-beam computed tomography (cbct) segmentation
    by adversarial learning domain adaptation, in: Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China,
    October 13–17, 2019, Proceedings, Part VI 22, Springer, 2019, pp. 567–575.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] X. Jia, S. Wang, X. Liang, A. Balagopal, D. Nguyen, M. Yang, Z. Wang,
    J. X. Ji, X. Qian, S. Jiang, 基于对抗学习领域适应的锥束计算机断层扫描（CBCT）分割，发表于：医学图像计算与计算机辅助干预–MICCAI
    2019：第22届国际会议，中国深圳，2019年10月13–17日，会议录，第VI部分22，Springer，2019，第567–575页。'
- en: '[276] D. Liu, D. Zhang, Y. Song, F. Zhang, L. O’Donnell, H. Huang, M. Chen,
    W. Cai, Pdam: A panoptic-level feature alignment framework for unsupervised domain
    adaptive instance segmentation in microscopy images, IEEE Transactions on Medical
    Imaging 40 (1) (2020) 154–165.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] D. Liu, D. Zhang, Y. Song, F. Zhang, L. O’Donnell, H. Huang, M. Chen,
    W. Cai, PDAM：一种全景级特征对齐框架，用于显微镜图像中的无监督领域自适应实例分割，《IEEE医学成像事务》40（1）（2020）154–165。'
- en: '[277] H. Cui, C. Yuwen, L. Jiang, Y. Xia, Y. Zhang, Bidirectional cross-modality
    unsupervised domain adaptation using generative adversarial networks for cardiac
    image segmentation, Computers in Biology and Medicine 136 (2021) 104726.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] H. Cui, C. Yuwen, L. Jiang, Y. Xia, Y. Zhang, 使用生成对抗网络的双向跨模态无监督领域适应用于心脏图像分割，《生物医学计算机》136（2021）104726。'
- en: '[278] X. Chen, T. Kuang, H. Deng, S. H. Fung, J. Gateno, J. J. Xia, P.-T. Yap,
    Dual adversarial attention mechanism for unsupervised domain adaptive medical
    image segmentation, IEEE Transactions on Medical Imaging 41 (11) (2022) 3445–3453.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] X. Chen, T. Kuang, H. Deng, S. H. Fung, J. Gateno, J. J. Xia, P.-T. Yap,
    用于无监督领域自适应医学图像分割的双对抗注意力机制，《IEEE医学成像事务》41（11）（2022）3445–3453。'
- en: '[279] Z. Zhao, F. Zhou, K. Xu, Z. Zeng, C. Guan, S. K. Zhou, Le-uda: Label-efficient
    unsupervised domain adaptation for medical image segmentation, IEEE transactions
    on medical imaging 42 (3) (2023) 633–646.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] Z. Zhao, F. Zhou, K. Xu, Z. Zeng, C. Guan, S. K. Zhou, Le-uda：医学图像分割的标签高效无监督领域适应，《IEEE医学成像事务》42（3）（2023）633–646。'
- en: '[280] X. Li, S. Niu, X. Gao, X. Zhou, J. Dong, H. Zhao, Self-training adversarial
    learning for cross-domain retinal oct fluid segmentation, Computers in Biology
    and Medicine 155 (2023) 106650.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] X. Li, S. Niu, X. Gao, X. Zhou, J. Dong, H. Zhao, 用于跨领域视网膜OCT液体分割的自训练对抗学习，《生物医学计算机》155（2023）106650。'
- en: '[281] D. Karimi, H. Dou, S. K. Warfield, A. Gholipour, Deep learning with noisy
    labels: Exploring techniques and remedies in medical image analysis, Medical image
    analysis 65 (2020) 101759.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] D. Karimi, H. Dou, S. K. Warfield, A. Gholipour, 深度学习与噪声标签：在医学图像分析中探索技术和解决方案，《医学图像分析》65（2020）101759。'
- en: '[282] A. Ghosh, H. Kumar, P. S. Sastry, Robust loss functions under label noise
    for deep neural networks, in: Proceedings of the AAAI conference on artificial
    intelligence, Vol. 31, 2017.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] A. Ghosh, H. Kumar, P. S. Sastry, 针对深度神经网络的标签噪声下的鲁棒损失函数，见：AAAI人工智能会议论文集，第31卷，2017。'
- en: '[283] Z. Zhang, M. Sabuncu, Generalized cross entropy loss for training deep
    neural networks with noisy labels, Advances in neural information processing systems
    31 (2018).'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] Z. Zhang, M. Sabuncu, 用于训练深度神经网络的广义交叉熵损失函数，神经信息处理系统进展31 (2018)。'
- en: '[284] D. J. Matuszewski, I.-M. Sintorn, Minimal annotation training for segmentation
    of microscopy images, in: 2018 IEEE 15th International Symposium on Biomedical
    Imaging (ISBI 2018), IEEE, 2018, pp. 387–390.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] D. J. Matuszewski, I.-M. Sintorn, 微观图像分割的最小注释训练，见：2018 IEEE第15届生物医学成像国际研讨会（ISBI
    2018），IEEE，2018，第387–390页。'
- en: '[285] G. Wang, X. Liu, C. Li, Z. Xu, J. Ruan, H. Zhu, T. Meng, K. Li, N. Huang,
    S. Zhang, A noise-robust framework for automatic segmentation of covid-19 pneumonia
    lesions from ct images, IEEE Transactions on Medical Imaging 39 (8) (2020) 2653–2663.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] G. Wang, X. Liu, C. Li, Z. Xu, J. Ruan, H. Zhu, T. Meng, K. Li, N. Huang,
    S. Zhang, 一种抗噪声的框架用于从CT图像中自动分割COVID-19肺炎病灶，IEEE医学成像事务39 (8) (2020) 2653–2663。'
- en: '[286] H. Chen, W. Tan, J. Li, P. Guan, L. Wu, B. Yan, J. Li, Y. Wang, Adaptive
    cross entropy for ultrasmall object detection in computed tomography with noisy
    labels, Computers in Biology and Medicine 147 (2022) 105763.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] H. Chen, W. Tan, J. Li, P. Guan, L. Wu, B. Yan, J. Li, Y. Wang, 自适应交叉熵用于计算机断层扫描中的超小物体检测，计算机生物医学杂志147
    (2022) 105763。'
- en: '[287] X. Guo, Y. Yuan, Joint class-affinity loss correction for robust medical
    image segmentation with noisy labels, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer, 2022, pp. 588–598.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] X. Guo, Y. Yuan, 针对带噪声标签的鲁棒医学图像分割的联合类别亲和损失修正，见：医学图像计算与计算机辅助干预国际会议，Springer，2022，第588–598页。'
- en: '[288] H. Le, D. Samaras, T. Kurc, R. Gupta, K. Shroyer, J. Saltz, Pancreatic
    cancer detection in whole slide images using noisy label annotations, in: Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International
    Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part I 22, Springer,
    2019, pp. 541–549.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] H. Le, D. Samaras, T. Kurc, R. Gupta, K. Shroyer, J. Saltz, 使用噪声标签注释的全幻灯片图像中的胰腺癌检测，见：医学图像计算与计算机辅助干预–MICCAI
    2019：第22届国际会议，中国深圳，2019年10月13–17日，会议录，第I部分22，Springer，2019，第541–549页。'
- en: '[289] Z. Mirikharaji, Y. Yan, G. Hamarneh, Learning to segment skin lesions
    from noisy annotations, in: Domain Adaptation and Representation Transfer and
    Medical Image Learning with Less Labels and Imperfect Data: First MICCAI Workshop,
    DART 2019, and First International Workshop, MIL3ID 2019, Shenzhen, Held in Conjunction
    with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, Proceedings 1, Springer,
    2019, pp. 207–215.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] Z. Mirikharaji, Y. Yan, G. Hamarneh, 从噪声标注中学习皮肤病变分割，见：领域适应与表示迁移及医学图像学习与较少标签和不完善数据：第一届MICCAI研讨会，DART
    2019，以及第一届国际研讨会，MIL3ID 2019，深圳，与MICCAI 2019同步举行，深圳，中国，2019年10月13日和17日，会议录1，Springer，2019，第207–215页。'
- en: '[290] T. Zhang, L. Yu, N. Hu, S. Lv, S. Gu, Robust medical image segmentation
    from non-expert annotations with tri-network, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima,
    Peru, October 4–8, 2020, Proceedings, Part IV 23, Springer, 2020, pp. 249–258.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] T. Zhang, L. Yu, N. Hu, S. Lv, S. Gu, 基于三网络的鲁棒医学图像分割，见：医学图像计算与计算机辅助干预–MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4–8日，会议录，第IV部分23，Springer，2020，第249–258页。'
- en: '[291] J. Wang, S. Zhou, C. Fang, L. Wang, J. Wang, Meta corrupted pixels mining
    for medical image segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8,
    2020, Proceedings, Part I 23, Springer, 2020, pp. 335–345.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] J. Wang, S. Zhou, C. Fang, L. Wang, J. Wang, 医学图像分割的元腐败像素挖掘，见：医学图像计算与计算机辅助干预–MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4–8日，会议录，第I部分23，Springer，2020，第335–345页。'
- en: '[292] S. Min, X. Chen, Z.-J. Zha, F. Wu, Y. Zhang, A two-stream mutual attention
    network for semi-supervised biomedical segmentation with noisy labels, in: Proceedings
    of the AAAI Conference on Artificial Intelligence, Vol. 33, 2019, pp. 4578–4585.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] S. Min, X. Chen, Z.-J. Zha, F. Wu, Y. Zhang, 一种双流互注意力网络用于带噪声标签的半监督生物医学分割，见：AAAI人工智能会议论文集，第33卷，2019，第4578–4585页。'
- en: '[293] E. Malach, S. Shalev-Shwartz, Decoupling” when to update” from” how to
    update”, Advances in neural information processing systems 30 (2017).'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] E. Malach, S. Shalev-Shwartz, 解耦“何时更新”与“如何更新”，《神经信息处理系统进展》 30 (2017)。'
- en: '[294] C. Xue, L. Yu, P. Chen, Q. Dou, P.-A. Heng, Robust medical image classification
    from noisy labeled data with global and local representation guided co-training,
    IEEE Transactions on Medical Imaging 41 (6) (2022) 1371–1382.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] C. Xue, L. Yu, P. Chen, Q. Dou, P.-A. Heng, 从带噪声标记的数据中进行鲁棒医学图像分类：基于全局和局部表示引导的共同训练，《IEEE医学影像学汇刊》
    41 (6) (2022) 1371–1382。'
- en: '[295] S. Yang, G. Wang, H. Sun, X. Luo, P. Sun, K. Li, Q. Wang, S. Zhang, Learning
    covid-19 pneumonia lesion segmentation from imperfect annotations via divergence-aware
    selective training, IEEE Journal of Biomedical and Health Informatics 26 (8) (2022)
    3673–3684.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] S. Yang, G. Wang, H. Sun, X. Luo, P. Sun, K. Li, Q. Wang, S. Zhang, 从不完美标注中学习COVID-19肺炎病灶分割：通过差异感知选择性训练，《IEEE生物医学与健康信息学期刊》
    26 (8) (2022) 3673–3684。'
- en: '[296] X. Li, Y. Wei, Q. Hu, C. Wang, J. Yang, Learning to segment subcortical
    structures from noisy annotations with a novel uncertainty-reliability aware learning
    framework, Computers in Biology and Medicine 151 (2022) 106326.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] X. Li, Y. Wei, Q. Hu, C. Wang, J. Yang, 从带噪声标注中学习分割皮层下结构：一种新型的不确定性-可靠性感知学习框架，《计算机与生物医学》
    151 (2022) 106326。'
- en: '[297] Z. Xu, D. Lu, Y. Wang, J. Luo, J. Jayender, K. Ma, Y. Zheng, X. Li, Noisy
    labels are treasure: mean-teacher-assisted confident learning for hepatic vessel
    segmentation, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2021: 24th International Conference, Strasbourg, France, September 27–October
    1, 2021, Proceedings, Part I 24, Springer, 2021, pp. 3–13.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] Z. Xu, D. Lu, Y. Wang, J. Luo, J. Jayender, K. Ma, Y. Zheng, X. Li, 噪声标签是宝藏：基于平均教师辅助的自信学习用于肝血管分割，见于：医学图像计算与计算机辅助干预–MICCAI
    2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日–10月1日，会议录，第I部分第24卷，Springer，2021年，第3–13页。'
- en: '[298] Z. Xu, D. Lu, J. Luo, Y. Wang, J. Yan, K. Ma, Y. Zheng, R. K.-Y. Tong,
    Anti-interference from noisy labels: Mean-teacher-assisted confident learning
    for medical image segmentation, IEEE Transactions on Medical Imaging 41 (11) (2022)
    3062–3073.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] Z. Xu, D. Lu, J. Luo, Y. Wang, J. Yan, K. Ma, Y. Zheng, R. K.-Y. Tong,
    反噪声标签干扰：基于平均教师辅助的自信学习用于医学图像分割，《IEEE医学影像学汇刊》 41 (11) (2022) 3062–3073。'
- en: '[299] J. Shi, J. Wu, Distilling effective supervision for robust medical image
    segmentation with noisy labels, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part I 24, Springer, 2021, pp. 668–677.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] J. Shi, J. Wu, 提炼有效监督以增强医学图像分割的鲁棒性：应对噪声标签，见于：医学图像计算与计算机辅助干预–MICCAI 2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日–10月1日，会议录，第I部分第24卷，Springer，2021年，第668–677页。'
- en: '[300] F. Garcea, A. Serra, F. Lamberti, L. Morra, Data augmentation for medical
    imaging: A systematic literature review, Computers in Biology and Medicine (2022)
    106391.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] F. Garcea, A. Serra, F. Lamberti, L. Morra, 医学成像的数据增强：系统文献综述，《计算机与生物医学》
    (2022) 106391。'
- en: '[301] J. Nalepa, M. Marcinkiewicz, M. Kawulok, Data augmentation for brain-tumor
    segmentation: a review, Frontiers in computational neuroscience 13 (2019) 83.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] J. Nalepa, M. Marcinkiewicz, M. Kawulok, 脑肿瘤分割的数据增强：综述，《计算神经科学前沿》 13
    (2019) 83。'
- en: '[302] S. Pereira, A. Pinto, V. Alves, C. A. Silva, Brain tumor segmentation
    using convolutional neural networks in mri images, IEEE transactions on medical
    imaging 35 (5) (2016) 1240–1251.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] S. Pereira, A. Pinto, V. Alves, C. A. Silva, 使用卷积神经网络在MRI图像中进行脑肿瘤分割，《IEEE医学影像学汇刊》
    35 (5) (2016) 1240–1251。'
- en: '[303] Y. Liu, S. Stojadinovic, B. Hrycushko, Z. Wardak, S. Lau, W. Lu, Y. Yan,
    S. B. Jiang, X. Zhen, R. Timmerman, et al., A deep convolutional neural network-based
    automatic delineation strategy for multiple brain metastases stereotactic radiosurgery,
    PloS one 12 (10) (2017) e0185844.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] Y. Liu, S. Stojadinovic, B. Hrycushko, Z. Wardak, S. Lau, W. Lu, Y. Yan,
    S. B. Jiang, X. Zhen, R. Timmerman, 等，基于深度卷积神经网络的多脑转移瘤立体定向放射外科自动轮廓绘制策略，《PloS one》
    12 (10) (2017) e0185844。'
- en: '[304] A. Galdran, A. Alvarez-Gila, M. I. Meyer, C. L. Saratxaga, T. Araújo,
    E. Garrote, G. Aresta, P. Costa, A. M. Mendoncca, A. Campilho, Data-driven color
    augmentation techniques for deep skin image analysis, arXiv preprint arXiv:1703.03702
    (2017).'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] A. Galdran, A. Alvarez-Gila, M. I. Meyer, C. L. Saratxaga, T. Araújo,
    E. Garrote, G. Aresta, P. Costa, A. M. Mendoncca, A. Campilho, 基于数据驱动的颜色增强技术用于深度皮肤图像分析，arXiv
    预印本 arXiv:1703.03702 (2017)。'
- en: '[305] A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin,
    A. A. Kalinin, Albumentations: fast and flexible image augmentations, Information
    11 (2) (2020) 125.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin,
    A. A. Kalinin, Albumentations：快速灵活的图像增强，信息 11（2）（2020）125。'
- en: '[306] F. Pérez-García, R. Sparks, S. Ourselin, Torchio: a python library for
    efficient loading, preprocessing, augmentation and patch-based sampling of medical
    images in deep learning, Computer Methods and Programs in Biomedicine 208 (2021)
    106236.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] F. Pérez-García, R. Sparks, S. Ourselin, Torchio：一个用于深度学习中医学图像高效加载、预处理、增强和基于补丁的采样的Python库，计算机方法与程序在生物医学
    208（2021）106236。'
- en: '[307] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, Y. Bengio, Generative adversarial nets, Advances in neural information
    processing systems 27 (2014).'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, Y. Bengio, 生成对抗网络，神经信息处理系统进展 27（2014）。'
- en: '[308] M. Mirza, S. Osindero, Conditional generative adversarial nets, arXiv
    preprint arXiv:1411.1784 (2014).'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] M. Mirza, S. Osindero, 条件生成对抗网络，arXiv预印本 arXiv:1411.1784（2014）。'
- en: '[309] P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, Image-to-image translation
    with conditional adversarial networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2017, pp. 1125–1134.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, 使用条件对抗网络进行图像到图像转换，见：IEEE计算机视觉与模式识别大会论文集，2017，第1125–1134页。'
- en: '[310] T. Park, M.-Y. Liu, T.-C. Wang, J.-Y. Zhu, Semantic image synthesis with
    spatially-adaptive normalization, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, 2019, pp. 2337–2346.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] T. Park, M.-Y. Liu, T.-C. Wang, J.-Y. Zhu, 使用空间自适应归一化的语义图像合成，见：IEEE/CVF计算机视觉与模式识别大会论文集，2019，第2337–2346页。'
- en: '[311] A. Radford, L. Metz, S. Chintala, Unsupervised representation learning
    with deep convolutional generative adversarial networks, arXiv preprint arXiv:1511.06434
    (2015).'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] A. Radford, L. Metz, S. Chintala, 使用深度卷积生成对抗网络进行无监督表征学习，arXiv预印本 arXiv:1511.06434（2015）。'
- en: '[312] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, T. Aila, Analyzing
    and improving the image quality of stylegan, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2020, pp. 8110–8119.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, T. Aila, 分析和改善StyleGAN图像质量，见：IEEE/CVF计算机视觉与模式识别大会论文集，2020，第8110–8119页。'
- en: '[313] T. Karras, T. Aila, S. Laine, J. Lehtinen, Progressive growing of gans
    for improved quality, stability, and variation, in: International Conference on
    Learning Representations, 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] T. Karras, T. Aila, S. Laine, J. Lehtinen, 为了提高质量、稳定性和变化性，逐步增长的GAN，见：国际表征学习大会，2018。'
- en: '[314] A. Bissoto, E. Valle, S. Avila, Gan-based data augmentation and anonymization
    for skin-lesion analysis: A critical review, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2021, pp. 1847–1856.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] A. Bissoto, E. Valle, S. Avila, 基于GAN的数据增强和匿名化用于皮肤病变分析：一项关键性综述，见：IEEE/CVF计算机视觉与模式识别大会论文集，2021，第1847–1856页。'
- en: '[315] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, H. Greenspan, Synthetic
    data augmentation using gan for improved liver lesion classification, in: 2018
    IEEE 15th international symposium on biomedical imaging (ISBI 2018), IEEE, 2018,
    pp. 289–293.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, H. Greenspan, 使用GAN进行合成数据增强以提高肝脏病变分类精度，见：2018年IEEE第十五届生物医学成像国际研讨会（ISBI
    2018），IEEE，2018，第289–293页。'
- en: '[316] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger, H. Greenspan,
    Gan-based synthetic medical image augmentation for increased cnn performance in
    liver lesion classification, Neurocomputing 321 (2018) 321–331.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger, H. Greenspan,
    基于GAN的合成医学图像增强以提高CNN在肝脏病变分类中的性能，神经计算 321（2018）321–331。'
- en: '[317] H. Rashid, M. A. Tanveer, H. A. Khan, Skin lesion classification using
    gan based data augmentation, in: 2019 41St annual international conference of
    the IEEE engineering in medicine and biology society (EMBC), IEEE, 2019, pp. 916–919.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] H. Rashid, M. A. Tanveer, H. A. Khan, 基于GAN的数据增强用于皮肤病变分类，见：2019年第41届IEEE医学与生物学工程学会年度国际会议（EMBC），IEEE，2019，第916–919页。'
- en: '[318] A. Waheed, M. Goyal, D. Gupta, A. Khanna, F. Al-Turjman, P. R. Pinheiro,
    Covidgan: data augmentation using auxiliary classifier gan for improved covid-19
    detection, Ieee Access 8 (2020) 91916–91923.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] A. Waheed, M. Goyal, D. Gupta, A. Khanna, F. Al-Turjman, CovidGAN：使用辅助分类器GAN进行数据增强以提高COVID-19检测，IEEE
    Access 8（2020）91916–91923。'
- en: '[319] H. Zhao, H. Li, S. Maurer-Stroh, L. Cheng, Synthesizing retinal and neuronal
    images with generative adversarial nets, Medical image analysis 49 (2018) 14–26.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] H. Zhao, H. Li, S. Maurer-Stroh, L. Cheng, 使用生成对抗网络合成视网膜和神经图像，医学图像分析
    49 (2018) 14–26。'
- en: '[320] M. Nishio, C. Muramatsu, S. Noguchi, H. Nakai, K. Fujimoto, R. Sakamoto,
    H. Fujita, Attribute-guided image generation of three-dimensional computed tomography
    images of lung nodules using a generative adversarial network, Computers in Biology
    and Medicine 126 (2020) 104032.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] M. Nishio, C. Muramatsu, S. Noguchi, H. Nakai, K. Fujimoto, R. Sakamoto,
    H. Fujita, 基于属性引导的肺结节三维计算机断层扫描图像生成，使用生成对抗网络，《生物医学计算机》126 (2020) 104032。'
- en: '[321] K. Chen, Y. Guo, C. Yang, Y. Xu, R. Zhang, C. Li, R. Wu, Enhanced breast
    lesion classification via knowledge guided cross-modal and semantic data augmentation,
    in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part V 24, Springer, 2021, pp. 53–63.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] K. Chen, Y. Guo, C. Yang, Y. Xu, R. Zhang, C. Li, R. Wu, 通过知识引导的跨模态和语义数据增强提升乳腺病变分类，见于：医学图像计算与计算机辅助手术–MICCAI
    2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日至10月1日，会议论文集，第V部分24，Springer，2021年，第53–63页。'
- en: '[322] C. Muramatsu, M. Nishio, T. Goto, M. Oiwa, T. Morita, M. Yakami, T. Kubo,
    K. Togashi, H. Fujita, Improving breast mass classification by shared data with
    domain transformation using a generative adversarial network, Computers in biology
    and medicine 119 (2020) 103698.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] C. Muramatsu, M. Nishio, T. Goto, M. Oiwa, T. Morita, M. Yakami, T. Kubo,
    K. Togashi, H. Fujita, 通过共享数据和领域转换使用生成对抗网络提高乳腺肿块分类，生物医学计算机 119 (2020) 103698。'
- en: '[323] P. Guo, P. Wang, J. Zhou, V. M. Patel, S. Jiang, Lesion mask-based simultaneous
    synthesis of anatomic and molecular mr images using a gan, in: Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part II 23, Springer, 2020, pp. 104–113.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] P. Guo, P. Wang, J. Zhou, V. M. Patel, S. Jiang, 基于病变掩模的解剖和分子MRI图像的同时合成，使用GAN，见于：医学图像计算与计算机辅助手术–MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4–8日，会议论文集，第II部分23，Springer，2020年，第104–113页。'
- en: '[324] M. Rezaei, K. Harmuth, W. Gierke, T. Kellermeier, M. Fischer, H. Yang,
    C. Meinel, A conditional adversarial network for semantic segmentation of brain
    tumor, in: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain
    Injuries: Third International Workshop, BrainLes 2017, Held in Conjunction with
    MICCAI 2017, Quebec City, QC, Canada, September 14, 2017, Revised Selected Papers
    3, Springer, 2018, pp. 241–252.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] M. Rezaei, K. Harmuth, W. Gierke, T. Kellermeier, M. Fischer, H. Yang,
    C. Meinel, 用于脑肿瘤语义分割的条件对抗网络，见于：Brainles: 胶质瘤、多个硬化症、中风和创伤性脑损伤：第三届国际研讨会，BrainLes
    2017，与MICCAI 2017同时举行，加拿大魁北克市，2017年9月14日，修订精选论文集3，Springer，2018年，第241–252页。'
- en: '[325] Y. Chen, X.-H. Yang, Z. Wei, A. A. Heidari, N. Zheng, Z. Li, H. Chen,
    H. Hu, Q. Zhou, Q. Guan, Generative adversarial networks in medical image augmentation:
    A review, Computers in Biology and Medicine 144 (2022) 105382.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] Y. Chen, X.-H. Yang, Z. Wei, A. A. Heidari, N. Zheng, Z. Li, H. Chen,
    H. Hu, Q. Zhou, Q. Guan, 医学图像增强中的生成对抗网络：综述，生物医学计算机 144 (2022) 105382。'
- en: '[326] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup: Beyond empirical
    risk minimization, in: International Conference on Learning Representations, 2018.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup: 超越经验风险最小化，见于：国际学习表示会议，2018年。'
- en: '[327] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix: Regularization
    strategy to train strong classifiers with localizable features, in: Proceedings
    of the IEEE/CVF international conference on computer vision, 2019, pp. 6023–6032.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix: 训练具有可定位特征的强分类器的正则化策略，见于：IEEE/CVF国际计算机视觉会议论文集，2019年，第6023–6032页。'
- en: '[328] J. Yang, Y. Zhang, Y. Liang, Y. Zhang, L. He, Z. He, Tumorcp: A simple
    but effective object-level data augmentation for tumor segmentation, in: Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
    Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part
    I 24, Springer, 2021, pp. 579–588.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] J. Yang, Y. Zhang, Y. Liang, Y. Zhang, L. He, Z. He, Tumorcp: 一种简单但有效的对象级数据增强方法，用于肿瘤分割，见于：医学图像计算与计算机辅助手术–MICCAI
    2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日至10月1日，会议论文集，第I部分24，Springer，2021年，第579–588页。'
- en: '[329] Y. Lin, Z. Wang, K.-T. Cheng, H. Chen, Insmix: Towards realistic generative
    data augmentation for nuclei instance segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 140–149.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] Y. Lin, Z. Wang, K.-T. Cheng, H. Chen, Insmix: 面向真实的核实例分割生成数据增强，载于: 医学图像计算与计算机辅助干预国际会议，Springer，2022年，页码
    140–149。'
- en: '[330] Q. Zhu, Y. Wang, L. Yin, J. Yang, F. Liao, S. Li, Selfmix: a self-adaptive
    data augmentation method for lesion segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 683–692.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] Q. Zhu, Y. Wang, L. Yin, J. Yang, F. Liao, S. Li, Selfmix: 一种自适应的数据增强方法用于病灶分割，载于:
    医学图像计算与计算机辅助干预国际会议，Springer，2022年，页码 683–692。'
- en: '[331] X. Zhang, C. Liu, N. Ou, X. Zeng, Z. Zhuo, Y. Duan, X. Xiong, Y. Yu,
    Z. Liu, Y. Liu, et al., Carvemix: a simple data augmentation method for brain
    lesion segmentation, NeuroImage 271 (2023) 120041.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] X. Zhang, C. Liu, N. Ou, X. Zeng, Z. Zhuo, Y. Duan, X. Xiong, Y. Yu,
    Z. Liu, Y. Liu, 等，Carvemix: 一种简单的脑病灶分割数据增强方法，神经影像学 271 (2023) 120041。'
- en: '[332] Y. Wang, Y. Ji, H. Xiao, A data augmentation method for fully automatic
    brain tumor segmentation, Computers in Biology and Medicine 149 (2022) 106039.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] Y. Wang, Y. Ji, H. Xiao, 一种用于全自动脑肿瘤分割的数据增强方法，计算机生物学与医学 149 (2022) 106039。'
- en: '[333] M. Salem, S. Valverde, M. Cabezas, D. Pareto, A. Oliver, J. Salvi, À. Rovira,
    X. Lladó, Multiple sclerosis lesion synthesis in mri using an encoder-decoder
    u-net, IEEE Access 7 (2019) 25171–25184.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] M. Salem, S. Valverde, M. Cabezas, D. Pareto, A. Oliver, J. Salvi, À.
    Rovira, X. Lladó, 使用编码器-解码器U-Net的MRI多发性硬化症病灶合成，IEEE Access 7 (2019) 25171–25184。'
- en: '[334] K. H. Cha, N. Petrick, A. Pezeshk, C. G. Graff, D. Sharma, A. Badal,
    B. Sahiner, Evaluation of data augmentation via synthetic images for improved
    breast mass detection on mammograms using deep learning, Journal of Medical Imaging
    7 (1) (2020) 012703–012703.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] K. H. Cha, N. Petrick, A. Pezeshk, C. G. Graff, D. Sharma, A. Badal,
    B. Sahiner, 通过合成图像评估数据增强以提高乳腺肿块在乳腺X光片上的检测性能，医学成像杂志 7 (1) (2020) 012703–012703。'
- en: '[335] Y. Sun, P. Yuan, Y. Sun, Mm-gan: 3d mri data augmentation for medical
    image segmentation via generative adversarial networks, in: 2020 IEEE International
    conference on knowledge graph (ICKG), IEEE, 2020, pp. 227–234.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] Y. Sun, P. Yuan, Y. Sun, Mm-gan: 基于生成对抗网络的3D MRI数据增强用于医学图像分割，载于: 2020
    IEEE国际知识图谱会议（ICKG），IEEE，2020年，页码 227–234。'
- en: '[336] Y. Onishi, A. Teramoto, M. Tsujimoto, T. Tsukamoto, K. Saito, H. Toyama,
    K. Imaizumi, H. Fujita, Multiplanar analysis for pulmonary nodule classification
    in ct images using deep convolutional neural network and generative adversarial
    networks, International journal of computer assisted radiology and surgery 15
    (2020) 173–178.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] Y. Onishi, A. Teramoto, M. Tsujimoto, T. Tsukamoto, K. Saito, H. Toyama,
    K. Imaizumi, H. Fujita, 利用深度卷积神经网络和生成对抗网络对CT图像中的肺结节进行多平面分析，国际计算机辅助放射学与手术杂志 15
    (2020) 173–178。'
- en: '[337] Y. Chen, D. Ruan, J. Xiao, L. Wang, B. Sun, R. Saouaf, W. Yang, D. Li,
    Z. Fan, Fully automated multiorgan segmentation in abdominal magnetic resonance
    imaging with deep neural networks, Medical physics 47 (10) (2020) 4971–4982.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] Y. Chen, D. Ruan, J. Xiao, L. Wang, B. Sun, R. Saouaf, W. Yang, D. Li,
    Z. Fan, 基于深度神经网络的腹部磁共振成像中的全自动多脏器分割，医学物理学 47 (10) (2020) 4971–4982。'
- en: '[338] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, Q. V. Le, Autoaugment: Learning
    augmentation policies from data, arXiv preprint arXiv:1805.09501 (2018).'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, Q. V. Le, Autoaugment: 从数据中学习增强策略，arXiv预印本
    arXiv:1805.09501 (2018)。'
- en: '[339] C. Chen, C. Qin, C. Ouyang, Z. Li, S. Wang, H. Qiu, L. Chen, G. Tarroni,
    W. Bai, D. Rueckert, Enhancing mr image segmentation with realistic adversarial
    data augmentation, Medical Image Analysis 82 (2022) 102597.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[339] C. Chen, C. Qin, C. Ouyang, Z. Li, S. Wang, H. Qiu, L. Chen, G. Tarroni,
    W. Bai, D. Rueckert, 通过真实的对抗性数据增强提升MR图像分割，医学图像分析 82 (2022) 102597。'
- en: '[340] K. Fujita, M. Kobayashi, T. Nagao, Data augmentation using evolutionary
    image processing, in: 2018 Digital Image Computing: Techniques and Applications
    (DICTA), IEEE, 2018, pp. 1–6.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[340] K. Fujita, M. Kobayashi, T. Nagao, 利用进化图像处理进行数据增强，载于: 2018数字图像计算：技术与应用（DICTA），IEEE，2018年，页码
    1–6。'
- en: '[341] Y. Wang, Q. Yao, J. T. Kwok, L. M. Ni, Generalizing from a few examples:
    A survey on few-shot learning, ACM computing surveys (csur) 53 (3) (2020) 1–34.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[341] Y. Wang, Q. Yao, J. T. Kwok, L. M. Ni, 从少量样本中泛化：少样本学习综述，ACM计算机调查（csur）53
    (3) (2020) 1–34。'
- en: '[342] A. K. Mondal, J. Dolz, C. Desrosiers, Few-shot 3d multi-modal medical
    image segmentation using generative adversarial learning, arXiv preprint arXiv:1810.12241
    (2018).'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[342] A. K. Mondal, J. Dolz, C. Desrosiers, 使用生成对抗学习的少样本3D多模态医学图像分割，arXiv预印本
    arXiv:1810.12241 (2018)。'
- en: '[343] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, A. V. Dalca, Data
    augmentation using learned transformations for one-shot medical image segmentation,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2019, pp. 8543–8553.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[343] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, A. V. Dalca, 使用学习到的变换进行数据增强以实现一次性医学图像分割，见：IEEE/CVF计算机视觉与模式识别会议论文集，2019，页码
    8543–8553。'
- en: '[344] Q. Lu, W. Liu, Z. Zhuo, Y. Li, Y. Duan, P. Yu, L. Qu, C. Ye, Y. Liu,
    A transfer learning approach to few-shot segmentation of novel white matter tracts,
    Medical Image Analysis 79 (2022) 102454.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[344] Q. Lu, W. Liu, Z. Zhuo, Y. Li, Y. Duan, P. Yu, L. Qu, C. Ye, Y. Liu,
    一种迁移学习方法用于少样本新颖白质束分割，《医学图像分析》79 (2022) 102454。'
- en: '[345] W. Liu, Q. Lu, Z. Zhuo, Y. Liu, C. Ye, One-shot segmentation of novel
    white matter tracts via extensive data augmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 133–142.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[345] W. Liu, Q. Lu, Z. Zhuo, Y. Liu, C. Ye, 通过广泛的数据增强进行新颖白质束的一次性分割，见：国际医学图像计算与计算机辅助干预会议，Springer，2022，页码
    133–142。'
- en: '[346] M. Fischer, T. Hepp, S. Gatidis, B. Yang, Self-supervised contrastive
    learning with random walks for medical image segmentation with limited annotations,
    Computerized Medical Imaging and Graphics 104 (2023) 102174.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[346] M. Fischer, T. Hepp, S. Gatidis, B. Yang, 使用随机游走进行自监督对比学习，以解决有限标注的医学图像分割问题，《计算机化医学成像与图形》104
    (2023) 102174。'
- en: '[347] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learning,
    Advances in neural information processing systems 30 (2017).'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[347] J. Snell, K. Swersky, R. Zemel, 少样本学习的原型网络，《神经信息处理系统进展》30 (2017)。'
- en: '[348] S. Ali, B. Bhattarai, T.-K. Kim, J. Rittscher, Additive angular margin
    for few shot learning to classify clinical endoscopy images, in: Machine Learning
    in Medical Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction
    with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings 11, Springer, 2020,
    pp. 494–503.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[348] S. Ali, B. Bhattarai, T.-K. Kim, J. Rittscher, 用于分类临床内窥镜图像的少样本学习的加性角度边际，见：医学成像中的机器学习：第11届国际研讨会，MLMI
    2020，与MICCAI 2020联合举行，秘鲁利马，2020年10月4日，论文集11，Springer，2020，页码 494–503。'
- en: '[349] R. Wang, Q. Zhou, G. Zheng, Few-shot medical image segmentation regularized
    with self-reference and contrastive learning, in: International Conference on
    Medical Image Computing and Computer-Assisted Intervention, Springer, 2022, pp.
    514–523.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[349] R. Wang, Q. Zhou, G. Zheng, 通过自我参考和对比学习正则化的少样本医学图像分割，见：国际医学图像计算与计算机辅助干预会议，Springer，2022，页码
    514–523。'
- en: '[350] C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, D. Rueckert, Self-supervision
    with superpixels: Training few-shot medical image segmentation without annotation,
    in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XXIX 16, Springer, 2020, pp. 762–780.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[350] C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, D. Rueckert, 使用超像素的自监督：在没有注释的情况下训练少样本医学图像分割，见：计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XXIX卷 16，Springer，2020，页码 762–780。'
- en: '[351] C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, D. Rueckert, Self-supervised
    learning for few-shot medical image segmentation, IEEE Transactions on Medical
    Imaging 41 (7) (2022) 1837–1848.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[351] C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, D. Rueckert, 用于少样本医学图像分割的自监督学习，《IEEE医学成像汇刊》41
    (7) (2022) 1837–1848。'
- en: '[352] S. Hansen, S. Gautam, R. Jenssen, M. Kampffmeyer, Anomaly detection-inspired
    few-shot medical image segmentation through self-supervision with supervoxels,
    Medical Image Analysis 78 (2022) 102385.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[352] S. Hansen, S. Gautam, R. Jenssen, M. Kampffmeyer, 通过自监督和超体素进行异常检测启发的少样本医学图像分割，《医学图像分析》78
    (2022) 102385。'
- en: '[353] S. Hansen, S. Gautam, S. A. Salahuddin, M. Kampffmeyer, R. Jenssen, Adnet++:
    A few-shot learning framework for multi-class medical image volume segmentation
    with uncertainty-guided feature refinement, Medical Image Analysis (2023) 102870.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[353] S. Hansen, S. Gautam, S. A. Salahuddin, M. Kampffmeyer, R. Jenssen, Adnet++：一种用于多类医学图像体积分割的少样本学习框架，具有不确定性引导的特征细化，《医学图像分析》
    (2023) 102870。'
- en: '[354] Q. Yu, K. Dang, N. Tajbakhsh, D. Terzopoulos, X. Ding, A location-sensitive
    local prototype network for few-shot medical image segmentation, in: 2021 IEEE
    18th international symposium on biomedical imaging (ISBI), IEEE, 2021, pp. 262–266.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[354] Q. Yu, K. Dang, N. Tajbakhsh, D. Terzopoulos, X. Ding, 一种位置敏感的局部原型网络用于少样本医学图像分割，见：2021
    IEEE第18届国际生物医学成像研讨会（ISBI），IEEE，2021，第262–266页。'
- en: '[355] H. Tang, X. Liu, S. Sun, X. Yan, X. Xie, Recurrent mask refinement for
    few-shot medical image segmentation, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2021, pp. 3918–3928.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[355] H. Tang, X. Liu, S. Sun, X. Yan, X. Xie, 用于少样本医学图像分割的递归掩码优化，见：IEEE/CVF国际计算机视觉会议论文集，2021，第3918–3928页。'
- en: '[356] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for fast
    adaptation of deep networks, in: International conference on machine learning,
    PMLR, 2017, pp. 1126–1135.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[356] C. Finn, P. Abbeel, S. Levine, 模型无关的元学习用于深度网络的快速适应，见：国际机器学习会议，PMLR，2017，第1126–1135页。'
- en: '[357] R. Singh, V. Bharti, V. Purohit, A. Kumar, A. K. Singh, S. K. Singh,
    Metamed: Few-shot medical image classification using gradient-based meta-learning,
    Pattern Recognition 120 (2021) 108111.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[357] R. Singh, V. Bharti, V. Purohit, A. Kumar, A. K. Singh, S. K. Singh,
    Metamed: 使用基于梯度的元学习进行少样本医学图像分类，《模式识别》120 (2021) 108111。'
- en: '[358] S. Chao, D. Belanger, Generalizing few-shot classification of whole-genome
    doubling across cancer types, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2021, pp. 3382–3392.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[358] S. Chao, D. Belanger, 跨癌症类型的全基因组倍增的少样本分类泛化，见：IEEE/CVF国际计算机视觉会议论文集，2021，第3382–3392页。'
- en: '[359] A. Achmamad, F. Ghazouani, S. Ruan, Few-shot learning for brain tumor
    segmentation from mri images, in: 2022 16th IEEE International Conference on Signal
    Processing (ICSP), Vol. 1, IEEE, 2022, pp. 489–494.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[359] A. Achmamad, F. Ghazouani, S. Ruan, 基于MRI图像的脑肿瘤分割的少样本学习，见：2022年第16届IEEE国际信号处理会议（ICSP），第1卷，IEEE，2022，第489–494页。'
- en: '[360] R. Khadka, D. Jha, S. Hicks, V. Thambawita, M. A. Riegler, S. Ali, P. Halvorsen,
    Meta-learning with implicit gradients in a few-shot setting for medical image
    segmentation, Computers in Biology and Medicine 143 (2022) 105227.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[360] R. Khadka, D. Jha, S. Hicks, V. Thambawita, M. A. Riegler, S. Ali, P.
    Halvorsen, 在少样本设置下使用隐式梯度的元学习用于医学图像分割，《计算机生物医学》143 (2022) 105227。'
- en: '[361] A. Rajeswaran, C. Finn, S. M. Kakade, S. Levine, Meta-learning with implicit
    gradients, Advances in neural information processing systems 32 (2019).'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[361] A. Rajeswaran, C. Finn, S. M. Kakade, S. Levine, 基于隐式梯度的元学习，《神经信息处理系统进展》32
    (2019)。'
- en: '[362] Z. Zhao, F. Zhou, Z. Zeng, C. Guan, S. K. Zhou, Meta-hallucinator: Towards
    few-shot cross-modality cardiac image segmentation, in: International Conference
    on Medical Image Computing and Computer-Assisted Intervention, Springer, 2022,
    pp. 128–139.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[362] Z. Zhao, F. Zhou, Z. Zeng, C. Guan, S. K. Zhou, Meta-hallucinator: 面向少样本跨模态心脏图像分割，见：国际医学图像计算与计算机辅助干预会议，Springer，2022，第128–139页。'
- en: '[363] X. Song, J. Li, X. Qian, Diagnosis of glioblastoma multiforme progression
    via interpretable structure-constrained graph neural networks, IEEE Transactions
    on Medical Imaging 42 (2) (2022) 380–390.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[363] X. Song, J. Li, X. Qian, 通过可解释的结构约束图神经网络诊断胶质母细胞瘤进展，《IEEE医学成像汇刊》42 (2)
    (2022) 380–390。'
- en: '[364] M. Gao, H. Jiang, L. Zhu, Z. Jiang, M. Geng, Q. Ren, Y. Lu, Discriminative
    ensemble meta-learning with co-regularization for rare fundus diseases diagnosis,
    Medical Image Analysis 89 (2023) 102884.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[364] M. Gao, H. Jiang, L. Zhu, Z. Jiang, M. Geng, Q. Ren, Y. Lu, 带有共同正则化的判别性集成元学习用于稀有眼底疾病的诊断，《医学图像分析》89
    (2023) 102884。'
- en: '[365] A. G. Roy, S. Siddiqui, S. Pölsterl, N. Navab, C. Wachinger, ‘squeeze
    & excite’guided few-shot segmentation of volumetric images, Medical image analysis
    59 (2020) 101587.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[365] A. G. Roy, S. Siddiqui, S. Pölsterl, N. Navab, C. Wachinger, 基于‘squeeze
    & excite’指导的体积图像少样本分割，《医学图像分析》59 (2020) 101587。'
- en: '[366] R. Feng, X. Zheng, T. Gao, J. Chen, W. Wang, D. Z. Chen, J. Wu, Interactive
    few-shot learning: Limited supervision, better medical image segmentation, IEEE
    Transactions on Medical Imaging 40 (10) (2021) 2575–2588.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[366] R. Feng, X. Zheng, T. Gao, J. Chen, W. Wang, D. Z. Chen, J. Wu, 交互式少样本学习：有限监督，改进医学图像分割，《IEEE医学成像汇刊》40
    (10) (2021) 2575–2588。'
- en: '[367] S. Kim, S. An, P. Chikontwe, S. H. Park, Bidirectional rnn-based few
    shot learning for 3d medical image segmentation, in: Proceedings of the AAAI conference
    on artificial intelligence, Vol. 35, 2021, pp. 1808–1816.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[367] S. Kim, S. An, P. Chikontwe, S. H. Park, 基于双向RNN的少样本学习用于3D医学图像分割，见：AAAI人工智能会议论文集，第35卷，2021，第1808–1816页。'
- en: '[368] Y. Feng, Y. Wang, H. Li, M. Qu, J. Yang, Learning what and where to segment:
    A new perspective on medical image few-shot segmentation, Medical Image Analysis
    87 (2023) 102834.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[368] Y. Feng, Y. Wang, H. Li, M. Qu, J. Yang, 学习什么和在哪里分割：医学图像少量样本分割的新视角，医学图像分析
    87 (2023) 102834。'
- en: '[369] W. Zhu, H. Liao, W. Li, W. Li, J. Luo, Alleviating the incompatibility
    between cross entropy loss and episode training for few-shot skin disease classification,
    in: Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,
    Springer International Publishing, Cham, 2020, pp. 330–339.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[369] W. Zhu, H. Liao, W. Li, W. Li, J. Luo, 缓解交叉熵损失与集训之间的不兼容性，用于少量样本皮肤病分类，见：医学图像计算与计算机辅助干预
    – MICCAI 2020，Springer国际出版公司，Cham，2020，页330–339。'
- en: '[370] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B.
    Gotway, J. Liang, Convolutional neural networks for medical image analysis: Full
    training or fine tuning?, IEEE transactions on medical imaging 35 (5) (2016) 1299–1312.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[370] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M.
    B. Gotway, J. Liang, 卷积神经网络用于医学图像分析：完全训练还是微调？，IEEE医学影像学交易 35 (5) (2016) 1299–1312。'
- en: '[371] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2016, pp. 770–778.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[371] K. He, X. Zhang, S. Ren, J. Sun, 深度残差学习用于图像识别，见：IEEE计算机视觉与模式识别会议录，2016，页770–778。'
- en: '[372] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale
    image recognition, arXiv preprint arXiv:1409.1556 (2014).'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[372] K. Simonyan, A. Zisserman, 非常深的卷积网络用于大规模图像识别，arXiv预印本 arXiv:1409.1556
    (2014)。'
- en: '[373] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
    large-scale hierarchical image database, in: 2009 IEEE conference on computer
    vision and pattern recognition, Ieee, 2009, pp. 248–255.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[373] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet：一个大规模分层图像数据库，见：2009年IEEE计算机视觉与模式识别会议，IEEE，2009，页248–255。'
- en: '[374] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura,
    R. M. Summers, Deep convolutional neural networks for computer-aided detection:
    Cnn architectures, dataset characteristics and transfer learning, IEEE transactions
    on medical imaging 35 (5) (2016) 1285–1298.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[374] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura,
    R. M. Summers, 用于计算机辅助检测的深度卷积神经网络：CNN架构、数据集特征和迁移学习，IEEE医学影像学交易 35 (5) (2016) 1285–1298。'
- en: '[375] B. Q. Huynh, H. Li, M. L. Giger, Digital mammographic tumor classification
    using transfer learning from deep convolutional neural networks, Journal of Medical
    Imaging 3 (3) (2016) 034501–034501.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[375] B. Q. Huynh, H. Li, M. L. Giger, 使用深度卷积神经网络的迁移学习进行数字化乳腺肿瘤分类，医学影像学杂志 3
    (3) (2016) 034501–034501。'
- en: '[376] Y. Yuan, W. Qin, M. Buyyounouski, B. Ibragimov, S. Hancock, B. Han, L. Xing,
    Prostate cancer classification with multiparametric mri transfer learning model,
    Medical physics 46 (2) (2019) 756–765.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[376] Y. Yuan, W. Qin, M. Buyyounouski, B. Ibragimov, S. Hancock, B. Han, L.
    Xing, 前列腺癌分类与多参数MRI迁移学习模型，医学物理 46 (2) (2019) 756–765。'
- en: '[377] S. Minaee, R. Kafieh, M. Sonka, S. Yazdani, G. J. Soufi, Deep-covid:
    Predicting covid-19 from chest x-ray images using deep transfer learning, Medical
    image analysis 65 (2020) 101794.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[377] S. Minaee, R. Kafieh, M. Sonka, S. Yazdani, G. J. Soufi, Deep-covid：利用深度迁移学习从胸部X光图像预测COVID-19，医学图像分析
    65 (2020) 101794。'
- en: '[378] P. Kora, C. P. Ooi, O. Faust, U. Raghavendra, A. Gudigar, W. Y. Chan,
    K. Meenakshi, K. Swaraja, P. Plawiak, U. R. Acharya, Transfer learning techniques
    for medical image analysis: A review, Biocybernetics and Biomedical Engineering
    42 (1) (2022) 79–107.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[378] P. Kora, C. P. Ooi, O. Faust, U. Raghavendra, A. Gudigar, W. Y. Chan,
    K. Meenakshi, K. Swaraja, P. Plawiak, U. R. Acharya, 医学图像分析的迁移学习技术：综述，生物控制论与生物医学工程
    42 (1) (2022) 79–107。'
- en: '[379] C. Ma, Z. Ji, M. Gao, Neural style transfer improves 3d cardiovascular
    mr image segmentation on inconsistent data, in: Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China,
    October 13–17, 2019, Proceedings, Part II 22, Springer, 2019, pp. 128–136.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[379] C. Ma, Z. Ji, M. Gao, 神经风格迁移在不一致数据上改善三维心血管MR图像分割，见：医学图像计算与计算机辅助干预–MICCAI
    2019: 第22届国际会议，深圳，中国，2019年10月13–17日，会议录，第二部分 22，Springer，2019，页128–136。'
- en: '[380] X. Qin, Transfer learning with edge attention for prostate mri segmentation,
    arXiv preprint arXiv:1912.09847 (2019).'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[380] X. Qin, 边缘注意力迁移学习用于前列腺MRI分割，arXiv预印本 arXiv:1912.09847 (2019)。'
- en: '[381] J. Liu, B. Dong, S. Wang, H. Cui, D.-P. Fan, J. Ma, G. Chen, Covid-19
    lung infection segmentation with a novel two-stage cross-domain transfer learning
    framework, Medical image analysis 74 (2021) 102205.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[381] J. Liu, B. Dong, S. Wang, H. Cui, D.-P. Fan, J. Ma, G. Chen, Covid-19肺部感染分割的新型双阶段跨域迁移学习框架，医学图像分析
    74 (2021) 102205。'
- en: '[382] D. M. Nguyen, T. T. Nguyen, H. Vu, Q. Pham, M.-D. Nguyen, B. T. Nguyen,
    D. Sonntag, Tatl: Task agnostic transfer learning for skin attributes detection,
    Medical Image Analysis 78 (2022) 102359.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[382] D. M. Nguyen, T. T. Nguyen, H. Vu, Q. Pham, M.-D. Nguyen, B. T. Nguyen,
    D. Sonntag, Tatl: 任务无关的皮肤属性检测迁移学习，医学图像分析 78 (2022) 102359。'
- en: '[383] Q. Yu, L. Xie, Y. Wang, Y. Zhou, E. K. Fishman, A. L. Yuille, Recurrent
    saliency transformation network: Incorporating multi-stage visual cues for small
    organ segmentation, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2018, pp. 8280–8289.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[383] Q. Yu, L. Xie, Y. Wang, Y. Zhou, E. K. Fishman, A. L. Yuille, 递归显著性变换网络：融合多阶段视觉线索进行小器官分割，在：IEEE计算机视觉与模式识别会议论文集，2018，第8280–8289页。'
- en: '[384] S. Liu, D. Xu, S. K. Zhou, O. Pauly, S. Grbic, T. Mertelmeier, J. Wicklein,
    A. Jerebko, W. Cai, D. Comaniciu, 3d anisotropic hybrid network: Transferring
    convolutional features from 2d images to 3d anisotropic volumes, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2018: 21st International Conference,
    Granada, Spain, September 16-20, 2018, Proceedings, Part II 11, Springer, 2018,
    pp. 851–858.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[384] S. Liu, D. Xu, S. K. Zhou, O. Pauly, S. Grbic, T. Mertelmeier, J. Wicklein,
    A. Jerebko, W. Cai, D. Comaniciu, 3D各向异性混合网络：从2D图像到3D各向异性体积的卷积特征迁移，在：医学图像计算与计算机辅助干预–MICCAI
    2018：第21届国际会议，西班牙格拉纳达，2018年9月16-20日，会议录，第II部分11，Springer，2018，第851–858页。'
- en: '[385] H. Messaoudi, A. Belaid, D. B. Salem, P.-H. Conze, Cross-dimensional
    transfer learning in medical image segmentation with deep learning, Medical Image
    Analysis (2023) 102868.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[385] H. Messaoudi, A. Belaid, D. B. Salem, P.-H. Conze, 在医学图像分割中的跨维度迁移学习与深度学习，医学图像分析
    (2023) 102868。'
- en: '[386] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, A. V. Dalca, An unsupervised
    learning model for deformable medical image registration, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2018, pp. 9252–9260.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[386] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, A. V. Dalca, 一种用于可变形医学图像配准的无监督学习模型，在：IEEE计算机视觉与模式识别会议论文集，2018，第9252–9260页。'
- en: '[387] C. V. Nguyen, Y. Li, T. D. Bui, R. E. Turner, Variational continual learning,
    in: International Conference on Learning Representations, 2018.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[387] C. V. Nguyen, Y. Li, T. D. Bui, R. E. Turner, 变分持续学习，在：国际学习表征会议，2018。'
- en: '[388] E. Zheng, Q. Yu, R. Li, P. Shi, A. Haake, A continual learning framework
    for uncertainty-aware interactive image segmentation, in: Proceedings of the AAAI
    Conference on Artificial Intelligence, Vol. 35, 2021, pp. 6030–6038.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[388] E. Zheng, Q. Yu, R. Li, P. Shi, A. Haake, 一种用于不确定性感知交互图像分割的持续学习框架，在：AAAI人工智能会议论文集，第35卷，2021，第6030–6038页。'
- en: '[389] J. Zhang, R. Gu, G. Wang, L. Gu, Comprehensive importance-based selective
    regularization for continual segmentation across multiple sites, in: Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,
    Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, Springer,
    2021, pp. 389–399.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[389] J. Zhang, R. Gu, G. Wang, L. Gu, 基于重要性的综合选择性正则化用于多站点的持续分割，在：医学图像计算与计算机辅助干预–MICCAI
    2021：第24届国际会议，法国斯特拉斯堡，2021年9月27日–10月1日，会议录，第I部分24，Springer，2021，第389–399页。'
- en: '[390] Z. Li, C. Zhong, R. Wang, W.-S. Zheng, Continual learning of new diseases
    with dual distillation and ensemble strategy, in: Medical Image Computing and
    Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima,
    Peru, October 4–8, 2020, Proceedings, Part I 23, Springer, 2020, pp. 169–178.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[390] Z. Li, C. Zhong, R. Wang, W.-S. Zheng, 使用双重蒸馏和集成策略进行新疾病的持续学习，在：医学图像计算与计算机辅助干预–MICCAI
    2020：第23届国际会议，秘鲁利马，2020年10月4日–8日，会议录，第I部分23，Springer，2020，第169–178页。'
- en: '[391] M. M. Derakhshani, I. Najdenkoska, T. van Sonsbeek, X. Zhen, D. Mahapatra,
    M. Worring, C. G. Snoek, Lifelonger: A benchmark for continual disease classification,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2022, pp. 314–324.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[391] M. M. Derakhshani, I. Najdenkoska, T. van Sonsbeek, X. Zhen, D. Mahapatra,
    M. Worring, C. G. Snoek, Lifelonger: 一种持续疾病分类的基准，在：国际医学图像计算与计算机辅助干预会议，Springer，2022，第314–324页。'
- en: '[392] Y. Yang, Z. Cui, J. Xu, C. Zhong, W.-S. Zheng, R. Wang, Continual learning
    with bayesian model based on a fixed pre-trained feature extractor, Visual Intelligence
    1 (1) (2023) 5.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[392] Y. Yang, Z. Cui, J. Xu, C. Zhong, W.-S. Zheng, R. Wang, 基于固定预训练特征提取器的贝叶斯模型的持续学习，《视觉智能》1
    (1) (2023) 5。'
- en: '[393] N. Bayasi, G. Hamarneh, R. Garbi, Culprit-prune-net: Efficient continual
    sequential multi-domain learning with application to skin lesion classification,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer, 2021, pp. 165–175.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[393] N. Bayasi, G. Hamarneh, R. Garbi, Culprit-prune-net：高效的持续序列多领域学习及其在皮肤病变分类中的应用，见：医学图像计算与计算机辅助手术国际会议，Springer，2021，页
    165–175。'
- en: '[394] M. Lenga, H. Schulz, A. Saalbach, Continual learning for domain adaptation
    in chest x-ray classification, in: Medical Imaging with Deep Learning, PMLR, 2020,
    pp. 413–423.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[394] M. Lenga, H. Schulz, A. Saalbach, 胸部X光分类中的持续学习用于领域适应，见：深度学习医学成像，PMLR，2020，页
    413–423。'
- en: '[395] B. Chen, K. Thandiackal, P. Pati, O. Goksel, Generative appearance replay
    for continual unsupervised domain adaptation, arXiv preprint arXiv:2301.01211
    (2023).'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[395] B. Chen, K. Thandiackal, P. Pati, O. Goksel, 生成式外观重放用于持续无监督领域适应，arXiv
    预印本 arXiv:2301.01211 (2023)。'
- en: '[396] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, C. P. Langlotz, Contrastive
    learning of medical visual representations from paired images and text, in: Machine
    Learning for Healthcare Conference, PMLR, 2022, pp. 2–25.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[396] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, C. P. Langlotz, 从配对图像和文本中对医学视觉表示进行对比学习，见：医疗保健会议的机器学习，PMLR，2022，页
    2–25。'
- en: '[397] X. Xie, J. Niu, X. Liu, Z. Chen, S. Tang, S. Yu, A survey on incorporating
    domain knowledge into deep learning for medical image analysis, Medical Image
    Analysis 69 (2021) 101985.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[397] X. Xie, J. Niu, X. Liu, Z. Chen, S. Tang, S. Yu, 将领域知识融入医学图像分析的深度学习综述，《医学图像分析》69
    (2021) 101985。'
- en: '[398] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information
    processing systems 30 (2017).'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[398] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, I. Polosukhin, 注意力就是你所需要的一切，《神经信息处理系统进展》30 (2017)。'
- en: '[399] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16
    words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929
    (2020).'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[399] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.
    Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, 等，图像价值 16x16 个单词：用于大规模图像识别的变换器，arXiv
    预印本 arXiv:2010.11929 (2020)。'
- en: '[400] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, C. Feichtenhofer,
    Multiscale vision transformers, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2021, pp. 6824–6835.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[400] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, C. Feichtenhofer,
    多尺度视觉变换器，见：IEEE/CVF 国际计算机视觉会议论文集，2021，页 6824–6835。'
- en: '[401] J. Li, J. Chen, Y. Tang, C. Wang, B. A. Landman, S. K. Zhou, Transforming
    medical imaging with transformers? a comparative review of key properties, current
    progresses, and future perspectives, Medical image analysis (2023) 102762.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[401] J. Li, J. Chen, Y. Tang, C. Wang, B. A. Landman, S. K. Zhou, 用变换器改变医学影像？关键属性、当前进展和未来展望的比较综述，《医学图像分析》
    (2023) 102762。'
- en: '[402] Y. Xie, J. Zhang, Y. Xia, Q. Wu, Unified 2d and 3d pre-training for medical
    image classification and segmentation, arXiv preprint arXiv:2112.09356 1 (2021).'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[402] Y. Xie, J. Zhang, Y. Xia, Q. Wu, 统一的二维和三维预训练用于医学图像分类和分割，arXiv 预印本 arXiv:2112.09356
    1 (2021)。'
- en: '[403] B. Zoph, Q. Le, Neural architecture search with reinforcement learning,
    in: International Conference on Learning Representations, 2016.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[403] B. Zoph, Q. Le, 使用强化学习的神经架构搜索，见：学习表征国际会议，2016。'
- en: '[404] T. Elsken, J. H. Metzen, F. Hutter, Neural architecture search: A survey,
    The Journal of Machine Learning Research 20 (1) (2019) 1997–2017.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[404] T. Elsken, J. H. Metzen, F. Hutter, 神经架构搜索：综述，《机器学习研究杂志》20 (1) (2019)
    1997–2017。'
- en: '[405] A. Yuille, C. Liu, Deep nets: What have they ever done for vision?.,
    Int J Comput Vis 129 (2021) 781–802.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[405] A. Yuille, C. Liu, 深度网络：它们对视觉做了什么？，《计算机视觉国际期刊》129 (2021) 781–802。'
- en: '[406] D. B. Larson, D. C. Magnus, M. P. Lungren, N. H. Shah, C. P. Langlotz,
    Ethics of using and sharing clinical imaging data for artificial intelligence:
    a proposed framework, Radiology 295 (3) (2020) 675–682.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[406] D. B. Larson, D. C. Magnus, M. P. Lungren, N. H. Shah, C. P. Langlotz,
    使用和共享临床影像数据以用于人工智能的伦理：一种提出的框架，《放射学》295 (3) (2020) 675–682。'
- en: '[407] N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni,
    S. Bakas, M. N. Galtier, B. A. Landman, K. Maier-Hein, et al., The future of digital
    health with federated learning, NPJ digital medicine 3 (1) (2020) 119.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[407] N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni,
    S. Bakas, M. N. Galtier, B. A. Landman, K. Maier-Hein 等，联邦学习引领数字健康的未来，NPJ 数字医学
    3 (1) (2020) 119。'
- en: '[408] R. S. Antunes, C. André da Costa, A. Küderle, I. A. Yari, B. Eskofier,
    Federated learning for healthcare: Systematic review and architecture proposal,
    ACM Transactions on Intelligent Systems and Technology (TIST) 13 (4) (2022) 1–23.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[408] R. S. Antunes, C. André da Costa, A. Küderle, I. A. Yari, B. Eskofier,
    医疗保健中的联邦学习：系统综述与架构提议，ACM 智能系统与技术事务 (TIST) 13 (4) (2022) 1–23。'
- en: '[409] M. J. Sheller, B. Edwards, G. A. Reina, J. Martin, S. Pati, A. Kotrotsou,
    M. Milchenko, W. Xu, D. Marcus, R. R. Colen, et al., Federated learning in medicine:
    facilitating multi-institutional collaborations without sharing patient data,
    Scientific reports 10 (1) (2020) 12598.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[409] M. J. Sheller, B. Edwards, G. A. Reina, J. Martin, S. Pati, A. Kotrotsou,
    M. Milchenko, W. Xu, D. Marcus, R. R. Colen 等，医学中的联邦学习：在不共享患者数据的情况下促进多机构合作，Scientific
    reports 10 (1) (2020) 12598。'
- en: '[410] Z. Li, Y. Li, Q. Li, P. Wang, D. Guo, L. Lu, D. Jin, Y. Zhang, Q. Hong,
    Lvit: language meets vision transformer in medical image segmentation, IEEE Transactions
    on Medical Imaging (2023).'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[410] Z. Li, Y. Li, Q. Li, P. Wang, D. Guo, L. Lu, D. Jin, Y. Zhang, Q. Hong,
    Lvit: 语言与视觉变换器在医学图像分割中的融合，IEEE 医学影像学事务 (2023)。'
