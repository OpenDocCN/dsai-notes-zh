- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:37:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2307.10275] Survey on Controlable Image Synthesis with Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2307.10275] 深度学习中的可控图像合成调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.10275](https://ar5iv.labs.arxiv.org/html/2307.10275)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2307.10275](https://ar5iv.labs.arxiv.org/html/2307.10275)
- en: Survey on Controlable Image Synthesis with Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的可控图像合成调查
- en: 'Shixiong Zhang Shixiong Zhang is with the School of Automation Engineering,
    University of Electronic Science and Technology of China, Chengdu, Sichuan, China
    (email: 202221060818@std.uestc.edu.cn).    Jiao Li Jiao Li is with the College
    of Information Engineering, Sichuan Agricultural University, Chengdu, Sichuan,
    China (email: 202005852@stu.sicau.edu.cn).    Lu Yang Lu Yang is with the School
    of Automation Engineering, University of Electronic Science and Technology of
    China, Chengdu, Sichuan, China (email: yanglu@uestc.edu.cn). Corresponding Author.
    Member, IEEE.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shixiong Zhang Shixiong Zhang 在中国电子科技大学自动化工程学院工作，位于中国四川成都（电子邮件：202221060818@std.uestc.edu.cn）。
       Jiao Li Jiao Li 在四川农业大学信息工程学院工作，位于中国四川成都（电子邮件：202005852@stu.sicau.edu.cn）。
       Lu Yang Lu Yang 在中国电子科技大学自动化工程学院工作，位于中国四川成都（电子邮件：yanglu@uestc.edu.cn）。 通讯作者。
    IEEE会员。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Image synthesis has attracted emerging research interests in academic and industry
    communities. Deep learning technologies especially the generative models greatly
    inspired controllable image synthesis approaches and applications, which aim to
    generate particular visual contents with latent prompts. In order to further investigate
    low-level controllable image synthesis problem which is crucial for fine image
    rendering and editing tasks, we present a survey of some recent works on 3D controllable
    image synthesis using deep learning. We first introduce the datasets and evaluation
    indicators for 3D controllable image synthesis. Then, we review the state-of-the-art
    research for geometrically controllable image synthesis in two aspects: 1) Viewpoint/pose-controllable
    image synthesis; 2) Structure/shape-controllable image synthesis. Furthermore,
    the photometrically controllable image synthesis approaches are also reviewed
    for 3D re-lighting researches. While the emphasis is on 3D controllable image
    synthesis algorithms, the related applications, products and resources are also
    briefly summarized for practitioners.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图像合成在学术界和工业界引起了新兴的研究兴趣。深度学习技术，特别是生成模型，极大地激发了可控图像合成方法和应用，这些方法旨在通过潜在提示生成特定的视觉内容。为了进一步探讨低级可控图像合成问题，这对精细图像渲染和编辑任务至关重要，我们介绍了一些关于使用深度学习进行3D可控图像合成的最新工作。我们首先介绍了3D可控图像合成的数据集和评估指标。然后，我们从两个方面回顾了几何可控图像合成的最前沿研究：1）视角/姿态可控图像合成；2）结构/形状可控图像合成。此外，还回顾了光度可控图像合成方法，用于3D重光照研究。虽然重点是3D可控图像合成算法，但也简要总结了相关应用、产品和资源，以供从业者参考。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: image synthesis, 3D controlable, NeRF, GAN, diffusion model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像合成，3D可控，NeRF，GAN，扩散模型。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Artificial Intelligence Generated Content (AIGC) is the term for digital media
    produced by machine learning methods, such as ChatGPT and stable diffusion[[1](#bib.bib1)],
    which are currently popular[[2](#bib.bib2)]. AIGC has various applications in
    domains such as entertainment, education, marketing, and research[[3](#bib.bib3)].
    Image synthesis is a subcategory of AIGC that involves generating realistic or
    stylized images from textual inputs, sketches, or other images[[4](#bib.bib4)].
    Image synthesis can also perform various tasks such as inpainting, semantic scene
    synthesis, super-resolution, and unconditional image generation[[1](#bib.bib1),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能生成内容（AIGC）是指由机器学习方法生成的数字媒体，如ChatGPT和稳定扩散[[1](#bib.bib1)]，这些方法目前很受欢迎[[2](#bib.bib2)]。AIGC在娱乐、教育、营销和研究等领域有各种应用[[3](#bib.bib3)]。图像合成是AIGC的一个子类别，涉及从文本输入、草图或其他图像生成逼真或风格化的图像[[4](#bib.bib4)]。图像合成还可以执行各种任务，如修补、语义场景合成、超分辨率和无条件图像生成[[1](#bib.bib1),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]。
- en: 'Image synthesis can be classified into two types based on controllability:
    unconditional and conditional [[8](#bib.bib8)]. Conditional image synthesis can
    be further divided into three levels of control: high, medium, and low. High-level
    control refers to the image content such as category, medium-level control refers
    to the image background and other aspects, and low-level control refers to manipulating
    the image based on the underlying principles of traditional computer vision [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图像合成可以根据可控性分为两种类型：无条件和有条件[[8](#bib.bib8)]。有条件图像合成可以进一步分为三种控制级别：高、中和低。高层次控制指图像内容，如类别，中层次控制指图像背景和其他方面，而低层次控制指基于传统计算机视觉的原理操控图像[[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)]。
- en: 'Conventional 3D image synthesis techniques face challenges in handling intricate
    details and patterns that vary across different objects [[12](#bib.bib12)]. Deep
    learning methods can better model the variations in shape, texture, and illumination
    of 3D objects[[13](#bib.bib13)]. The field of deep learning-based image synthesis
    has made remarkable progress in recent years, aided by the availability of more
    open source datasets [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]. Various
    image synthesis methods have emerged, such as generative adversarial network (GAN)
    [[7](#bib.bib7)], diffusion model (DM) [[6](#bib.bib6)], and neural radiance field
    (NeRF) [[17](#bib.bib17)]. These methods differ in their levels of controllability:
    GAN and DM are suitable for high-level or medium-level controllable image synthesis,
    while NeRF is suitable for low-level controllable image synthesis.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 常规的3D图像合成技术在处理复杂细节和不同对象间的变化模式方面面临挑战[[12](#bib.bib12)]。深度学习方法能够更好地建模3D对象在形状、纹理和光照方面的变化[[13](#bib.bib13)]。近年来，基于深度学习的图像合成领域取得了显著进展，这得益于更多开放源数据集的出现[[14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)]。各种图像合成方法已相继出现，如生成对抗网络（GAN）[[7](#bib.bib7)]、扩散模型（DM）[[6](#bib.bib6)]和神经辐射场（NeRF）[[17](#bib.bib17)]。这些方法在可控性方面有所不同：GAN和DM适用于高层次或中层次可控的图像合成，而NeRF适用于低层次可控的图像合成。
- en: Low-level controllable image synthesis can be categorized into geometric and
    illumination control. Geometric control involves manipulating the pose and structure
    of the scene, where the pose can refer to either the camera or the object, while
    the structure can refer to either the global shape (using depth maps, point clouds,
    or other 3D representations) or the local attributes (such as size, shape, color,
    etc.) of the object. Illumination control involves manipulating the light source
    and the material properties of the object. Refer to Fig. [2](#S2.F2 "Figure 2
    ‣ II Data Sets and Evaluation Indicators for 3D Controlable Image Synthesis ‣
    Survey on Controlable Image Synthesis with Deep Learning"), [3](#S2.F3 "Figure
    3 ‣ II-A Data Sets ‣ II Data Sets and Evaluation Indicators for 3D Controlable
    Image Synthesis ‣ Survey on Controlable Image Synthesis with Deep Learning"),
    and [4](#S2.F4 "Figure 4 ‣ II-B Evaluation Indicators ‣ II Data Sets and Evaluation
    Indicators for 3D Controlable Image Synthesis ‣ Survey on Controlable Image Synthesis
    with Deep Learning") for an example figure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 低层次可控图像合成可以分为几何控制和光照控制。几何控制涉及场景的姿态和结构，其中姿态可以指相机或对象，而结构可以指全球形状（使用深度图、点云或其他3D表示）或对象的局部属性（如大小、形状、颜色等）。光照控制涉及操控光源和对象的材料属性。请参见图[2](#S2.F2
    "Figure 2 ‣ II Data Sets and Evaluation Indicators for 3D Controlable Image Synthesis
    ‣ Survey on Controlable Image Synthesis with Deep Learning")、[3](#S2.F3 "Figure
    3 ‣ II-A Data Sets ‣ II Data Sets and Evaluation Indicators for 3D Controlable
    Image Synthesis ‣ Survey on Controlable Image Synthesis with Deep Learning")和[4](#S2.F4
    "Figure 4 ‣ II-B Evaluation Indicators ‣ II Data Sets and Evaluation Indicators
    for 3D Controlable Image Synthesis ‣ Survey on Controlable Image Synthesis with
    Deep Learning")以获取示例图。
- en: Several surveys have attempted to cover the state-of-the-art techniques and
    applications in image synthesis. However, most of these surveys have become obsolete
    due to the rapid development of the field [[8](#bib.bib8)], or have focused on
    the high-level and medium-level aspects of image synthesis, while ignoring the
    low-level aspects [[18](#bib.bib18)]. Furthermore, most of these surveys have
    adopted a methodological perspective, which is useful for researchers who want
    to understand the underlying principles and algorithms of image synthesis, but
    not for practitioners who want to apply image synthesis techniques to solve specific
    problems in various domains [[18](#bib.bib18), [19](#bib.bib19)]. This paper provides
    a task-oriented review of low-level controllable image synthesis, excluding human
    subjects[[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 几个调研试图涵盖图像合成领域的最先进技术和应用。然而，由于该领域的快速发展，这些调研中的大多数已经变得过时[[8](#bib.bib8)]，或者专注于图像合成的高级和中级方面，而忽视了低级方面[[18](#bib.bib18)]。此外，这些调研大多采取了方法论视角，这对希望理解图像合成的基本原理和算法的研究人员有用，但对希望将图像合成技术应用于解决各种领域中特定问题的实践者则不适用[[18](#bib.bib18),
    [19](#bib.bib19)]。本文提供了面向任务的低级可控图像合成的综述，不包括人类主题[[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)]。
- en: This review offers a comprehensive overview of the state-of-the-art deep learning
    methods for 3D controllable image synthesis. In Section [II](#S2 "II Data Sets
    and Evaluation Indicators for 3D Controlable Image Synthesis ‣ Survey on Controlable
    Image Synthesis with Deep Learning"), we begin by introducing the common data
    sets and evaluation indicators for this task. For the data set part, we divide
    it by its content. In Section [III](#S3 "III Pose Manipulation ‣ Survey on Controlable
    Image Synthesis with Deep Learning") to [V](#S5 "V Illumination Manipulation ‣
    Survey on Controlable Image Synthesis with Deep Learning"), we survey the control
    methods based on pose, structure and illumination, and divided each part into
    global and local controls. In Section [VI](#S6 "VI Application ‣ Survey on Controlable
    Image Synthesis with Deep Learning"), we discuss some of the current applications
    of 3D controllable image synthesis based on deep learning. Finally, Section [VII](#S7
    "VII Conclusion ‣ Survey on Controlable Image Synthesis with Deep Learning") concludes
    this paper. The overview of the surveyed 3D controlable image synthesis is shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Survey on Controlable Image Synthesis
    with Deep Learning").In the following sections, we will review common data sets
    and evaluation indicators in detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述提供了关于最先进的深度学习方法用于 3D 可控图像合成的综合概述。在[II](#S2 "II 数据集和评价指标用于 3D 可控图像合成 ‣ 深度学习下的可控图像合成调研")节中，我们首先介绍了该任务的常用数据集和评价指标。在数据集部分，我们按其内容进行划分。在[III](#S3
    "III 姿态操控 ‣ 深度学习下的可控图像合成调研")至[V](#S5 "V 光照操控 ‣ 深度学习下的可控图像合成调研")节中，我们调研了基于姿态、结构和光照的控制方法，并将每部分分为全局控制和局部控制。在[VI](#S6
    "VI 应用 ‣ 深度学习下的可控图像合成调研")节中，我们讨论了一些基于深度学习的 3D 可控图像合成的当前应用。最后，[VII](#S7 "VII 结论
    ‣ 深度学习下的可控图像合成调研")节总结了本文。调研的 3D 可控图像合成概述见图[1](#S1.F1 "图 1 ‣ I 引言 ‣ 深度学习下的可控图像合成调研")。在接下来的章节中，我们将详细回顾常用的数据集和评价指标。
- en: '![Refer to caption](img/aa275b34839dfb3a020feb4dff1fb13e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aa275b34839dfb3a020feb4dff1fb13e.png)'
- en: 'Figure 1: The structure diagram of this paper'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：本文的结构图
- en: II Data Sets and Evaluation Indicators for 3D Controlable Image Synthesis
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 数据集和评价指标用于 3D 可控图像合成
- en: '![Refer to caption](img/8931656b6f720111418b05f7c9cf0a2e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8931656b6f720111418b05f7c9cf0a2e.png)'
- en: (a) Original image
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始图像
- en: '![Refer to caption](img/23ebaae7f33e057113f1ee9caaa38714.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/23ebaae7f33e057113f1ee9caaa38714.png)'
- en: (b) Image after changing viewpoint
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 视角改变后的图像
- en: '![Refer to caption](img/02da001e8c04ac473989bc27d7375bf1.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02da001e8c04ac473989bc27d7375bf1.png)'
- en: (c) Original image
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 原始图像
- en: '![Refer to caption](img/6df6db2314cd59c437252263ddaadf45.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6df6db2314cd59c437252263ddaadf45.png)'
- en: (d) Image after rotating object
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 物体旋转后的图像
- en: 'Figure 2: Pose manipulation example.Subfigure (a) and (b) correspond to the
    content of Global Pose realization in Section [III](#S3 "III Pose Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning"), which come from
    [[24](#bib.bib24)]. Subfigure (c) and (d) correspond to the content of Local Pose
    realization in Section [III](#S3 "III Pose Manipulation ‣ Survey on Controlable
    Image Synthesis with Deep Learning"), which come from [[25](#bib.bib25)]'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：姿态操控示例。子图 (a) 和 (b) 对应于第 [III](#S3 "III Pose Manipulation ‣ Survey on Controlable
    Image Synthesis with Deep Learning") 节中的全局姿态实现内容，来自 [[24](#bib.bib24)]。子图 (c)
    和 (d) 对应于第 [III](#S3 "III Pose Manipulation ‣ Survey on Controlable Image Synthesis
    with Deep Learning") 节中的局部姿态实现内容，来自 [[25](#bib.bib25)]。
- en: One of the key challenges in 3D controllable image synthesis is to evaluate
    the quality and diversity of the generated images. Different data sets and metrics
    have been proposed to measure various aspects of 3D controllable image synthesis,
    such as realism, consistency, fidelity, and controllability. In this section,
    we will introduce some of the commonly used data sets and metrics for 3D controllable
    image synthesis, and discuss their advantages and limitations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 可控图像合成的一个关键挑战是评估生成图像的质量和多样性。已经提出了不同的数据集和指标来测量 3D 可控图像合成的各个方面，如真实感、一致性、保真度和可控性。在本节中，我们将介绍一些常用的
    3D 可控图像合成数据集和指标，并讨论它们的优缺点。
- en: II-A Data Sets
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 数据集
- en: '3D image synthesis is the task of generating realistic images of 3D objects
    from different viewpoints. This task requires a large amount of training data
    that can capture the shape, texture, lighting and pose variations of 3D objects.
    Several datasets have been proposed for this purpose, each with its own advantages
    and limitations. Some of the data sets are:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 图像合成是从不同视角生成 3D 物体逼真图像的任务。这个任务需要大量的训练数据，以捕捉 3D 物体的形状、纹理、光照和姿态变化。为此，已提出了几个数据集，每个数据集都有其自身的优缺点。一些数据集包括：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ABO is a synthetic data set that contains 3D shapes generated by assembling
    basic objects (ABOs) such as cubes, spheres, cylinders, and cones. It has 10 categories
    and 1000 shapes per category. ABO is useful for tasks such as shape abstraction,
    decomposition, and generation. However, ABO is also limited by its synthetic nature,
    its small number of categories and instances, and its lack of realistic lighting
    and occlusion[[24](#bib.bib24)].
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ABO 是一个合成数据集，包含通过组装基本对象（ABOs）如立方体、球体、圆柱体和圆锥体生成的 3D 形状。它有 10 类，每类 1000 个形状。ABO
    对于形状抽象、分解和生成等任务非常有用。然而，ABO 也受到其合成特性、类别和实例数量较少以及缺乏真实光照和遮挡的限制[[24](#bib.bib24)]。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Clevr3D is a synthetic data set that contains 3D scenes composed of simple geometric
    shapes with various attributes such as color, size, and material. It also provides
    natural language descriptions and questions for each scene. Clevr3D is useful
    for tasks such as scene understanding, reasoning, and captioning. However, Clevr3D
    is also limited by its synthetic nature, its simple scene composition, and its
    lack of realistic textures and backgrounds[[26](#bib.bib26)].
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Clevr3D 是一个合成数据集，包含由简单几何形状组成的 3D 场景，具有颜色、大小和材质等各种属性。它还提供了每个场景的自然语言描述和问题。Clevr3D
    对于场景理解、推理和描述等任务非常有用。然而，Clevr3D 也受到其合成特性、简单场景构成以及缺乏真实纹理和背景的限制[[26](#bib.bib26)]。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ScanNet is an RGB-D video data set that contains 2.5 million views in more than
    1500 scans of indoor scenes. It provides annotations such as camera poses, surface
    reconstructions, and instance-level semantic segmentations. ScanNet is useful
    for tasks such as semantic segmentation, object detection, and pose estimation.ScanNet
    is also limited by its incomplete coverage (due to scanning difficulties), its
    inconsistent labeling (due to human errors), and its lack of fine-grained details
    (such as object parts)[[27](#bib.bib27)].
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ScanNet 是一个 RGB-D 视频数据集，包含超过 1500 次扫描的室内场景，共 250 万个视图。它提供了相机姿态、表面重建和实例级语义分割等注释。ScanNet
    对于语义分割、目标检测和姿态估计等任务非常有用。ScanNet 也受到其覆盖不完全（由于扫描困难）、标签不一致（由于人工错误）以及缺乏细粒度细节（如物体部件）的限制[[27](#bib.bib27)]。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RealEstate10K is a data set for view synthesis that contains camera poses corresponding
    to 10 million frames derived from about 80,000 video clips gathered from YouTube
    videos. The data set also provides links to download the original videos. RealEstate10K
    is a large-scale and diverse data set that covers various types of scenes, such
    as houses, apartments, offices, and landscapes. RealEstate10K is useful for tasks
    such as stereo magnification, light field rendering, and novel view synthesis.
    However, RealEstate10K also has some challenges, such as the low quality of the
    videos, the inconsistency of the camera poses, and the lack of depth information[[28](#bib.bib28)].
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RealEstate10K是一个用于视图合成的数据集，包含与1千万帧相机姿势对应的相机数据，这些帧来自大约80,000个从YouTube视频收集的视频片段。该数据集还提供了下载原始视频的链接。RealEstate10K是一个大规模且多样化的数据集，覆盖了各种类型的场景，如房屋、公寓、办公室和风景。RealEstate10K对立体放大、光场渲染和新视图合成等任务很有用。然而，RealEstate10K也面临一些挑战，如视频质量较低、相机姿势不一致以及缺乏深度信息[[28](#bib.bib28)]。
- en: '![Refer to caption](img/04953b95d6123d887831d87c2e8a3423.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04953b95d6123d887831d87c2e8a3423.png)'
- en: (a) Original image
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始图像
- en: '![Refer to caption](img/3fe22fddaf9c9598c7f3a10374aa0a2d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3fe22fddaf9c9598c7f3a10374aa0a2d.png)'
- en: (b) Image after changing the depth of car
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 更改汽车深度后的图像
- en: '![Refer to caption](img/3e3cc57d22d8c9fdeca9ed2076b18d9f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3e3cc57d22d8c9fdeca9ed2076b18d9f.png)'
- en: (c) Original image
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 原始图像
- en: '![Refer to caption](img/e905e7797514d873de8c898e9eada869.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e905e7797514d873de8c898e9eada869.png)'
- en: (d) Image after changing the color of car
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 更改汽车颜色后的图像
- en: 'Figure 3: Structure manipulation example.Subfigure (a) and (b) correspond to
    the content of Global Structure realization in Section [IV](#S4 "IV Structure
    Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning"), which
    come from [[29](#bib.bib29)]. Subfigure (c) and (d) correspond to the content
    of Local Structure realization in Section [IV](#S4 "IV Structure Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning"), which come from
    [[30](#bib.bib30)]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：结构操作示例。子图（a）和（b）对应于第[IV](#S4 "IV 结构操作 ‣ 基于深度学习的可控图像合成调研")节中的全局结构实现内容，来源于[[29](#bib.bib29)]。子图（c）和（d）对应于第[IV](#S4
    "IV 结构操作 ‣ 基于深度学习的可控图像合成调研")节中的局部结构实现内容，来源于[[30](#bib.bib30)]。
- en: 'Point cloud data sets are collections of points that represent the shape and
    appearance of a 3D object or scene. They are often obtained from sensors such
    as lidar, radar, or cameras. Some of the data sets are:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 点云数据集是表示3D对象或场景形状和外观的点的集合。这些数据通常来自传感器，如激光雷达、雷达或相机。一些数据集包括：
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ShapeNet is a large-scale repository of 3D CAD models that covers 55 common
    object categories and 4 million models. It provides rich annotations such as category
    labels, part labels, alignments, and correspondences. ShapeNet is useful for tasks
    such as shape classification, segmentation, retrieval, and completion.Some of
    the limitations of ShapeNet are that it does not contain realistic textures or
    materials, it does not capture the variability and diversity of natural scenes,
    and it does not provide ground truth poses or camera parameters for rendering[[31](#bib.bib31)].
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ShapeNet是一个大规模的3D CAD模型库，涵盖了55种常见对象类别和400万个模型。它提供了丰富的注释，如类别标签、部件标签、对齐和对应关系。ShapeNet对形状分类、分割、检索和补全等任务非常有用。ShapeNet的一些局限性包括不包含真实的纹理或材料，未能捕捉自然场景的变化和多样性，以及不提供用于渲染的真实姿态或相机参数[[31](#bib.bib31)]。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: KITTI is a data set for autonomous driving that contains 3D point clouds captured
    by a Velodyne HDL-64E LIDAR sensor, along with RGB images, GPS/IMU data, object
    annotations, and semantic labels. KITTI is one of the most popular and challenging
    data sets for 3D object detection and semantic segmentation, as it covers various
    scenarios, weather conditions, and occlusions. However, KITTI also has some limitations,
    such as the limited number of frames per sequence (around 200), the fixed sensor
    configuration, and the lack of dynamic objects[[32](#bib.bib32)].
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: KITTI是一个用于自动驾驶的数据集，包含由Velodyne HDL-64E LIDAR传感器捕捉的3D点云，以及RGB图像、GPS/IMU数据、对象注释和语义标签。KITTI是最受欢迎且具有挑战性的数据集之一，适用于3D对象检测和语义分割，因为它覆盖了各种场景、天气条件和遮挡。然而，KITTI也有一些限制，例如每个序列的帧数有限（约200帧）、传感器配置固定以及缺乏动态对象[[32](#bib.bib32)]。
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: nuScenes is another data set for autonomous driving that contains 3D point clouds
    captured by a 32-beam LIDAR sensor, along with RGB images, radar data, GPS/IMU
    data, object annotations, and semantic labels. nuScenes is more comprehensive
    and diverse than KITTI, as it covers 1000 scenes from six cities in different
    countries, with varying traffic rules and driving behaviors. nuScenes also provides
    more temporal information, with 20 seconds of continuous data per scene. However,
    nuScenes also has some challenges, such as the lower resolution of the point clouds,
    the higher complexity of the scenes, and the need for sensor fusion[[33](#bib.bib33)].
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: nuScenes 是另一个用于自动驾驶的数据集，包含由32光束激光雷达传感器捕捉的3D点云，以及 RGB 图像、雷达数据、GPS/IMU 数据、对象注释和语义标签。nuScenes
    比 KITTI 更全面、多样，因为它覆盖了来自六个国家的1000个场景，具有不同的交通规则和驾驶行为。nuScenes 还提供了更多的时间信息，每个场景提供20秒的连续数据。然而，nuScenes
    也面临一些挑战，如点云分辨率较低、场景复杂度较高，以及需要传感器融合[[33](#bib.bib33)]。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Matterport3D is a data set for indoor scene understanding that contains 3D point
    clouds reconstructed from RGB-D images captured by a Matterport camera. The data
    set also provides surface reconstructions, camera poses, and 2D and 3D semantic
    segmentations. Matterport3D is a large-scale and high-quality data set that covers
    10,800 panoramic views from 194,400 RGB-D images in 90 building types. Matterport3D
    is useful for tasks such as keypoint matching, view overlap prediction, and scene
    completion. However, Matterport3D also has some limitations, such as the lack
    of dynamic objects, the dependence on RGB-D sensors, and the difficulty of obtaining
    ground truth annotations[[34](#bib.bib34)].
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Matterport3D 是一个用于室内场景理解的数据集，包含由 Matterport 相机捕捉的 RGB-D 图像重建的3D点云。数据集还提供了表面重建、相机姿态以及2D和3D语义分割。Matterport3D
    是一个大规模且高质量的数据集，涵盖了来自90种建筑类型的10,800个全景视图，包含194,400张 RGB-D 图像。Matterport3D 对于关键点匹配、视图重叠预测和场景补全等任务非常有用。然而，Matterport3D
    也有一些局限性，如缺乏动态物体、依赖 RGB-D 传感器以及获取真实注释的难度[[34](#bib.bib34)]。
- en: 'Depth map data sets are collections of images and their corresponding depth
    values, which can be used for various computer vision tasks such as depth estimation,
    3D reconstruction, scene understanding, etc.The commonly used depth map data sets
    are as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图数据集是图像及其对应深度值的集合，可用于各种计算机视觉任务，如深度估计、3D重建、场景理解等。常用的深度图数据集如下：
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Middlebury Stereo is a data set of stereo images with ground truth disparity
    maps obtained using structured light or a robot arm. It contains several versions
    of data sets collected from 2001 to 2021, with different scenes, resolutions,
    and levels of difficulty. The data set is widely used for evaluating stereo matching
    algorithms and provides online benchmarks and leaderboards. The strengths of this
    data set are its high accuracy, diversity, and availability. The limitations are
    its relatively small size, indoor scenes only, and lack of semantic labels[[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)].
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Middlebury Stereo 是一个立体图像数据集，带有使用结构光或机器人臂获取的真实视差图。它包含从2001年到2021年收集的几个版本的数据集，具有不同的场景、分辨率和难度等级。该数据集广泛用于评估立体匹配算法，并提供在线基准和排行榜。该数据集的优点是高精度、多样性和可用性，缺点是相对较小的规模、仅限室内场景以及缺乏语义标签[[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)]。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: NYU Depth Data set V2 is a data set of RGB-D images captured by Microsoft Kinect
    in various indoor scenes. It contains 1449 densely labeled pairs of aligned RGB
    and depth images, as well as 407024 unlabeled frames. The data set also provides
    surface normals, 3D point clouds, and semantic labels for each pixel. The data
    set is widely used for evaluating monocular depth estimation algorithms and provides
    online tools for data processing and visualization. The strengths of this data
    set are its large size, rich annotations, and realistic scenes. The limitations
    are its low resolution, noisy depth values, and indoor scenes only[[40](#bib.bib40)].
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NYU Depth Data set V2 是一个由微软 Kinect 捕捉的 RGB-D 图像数据集，包含多个室内场景。它包含1449对对齐的 RGB
    和深度图像的密集标注对，以及407024帧未标注图像。数据集还提供了每个像素的表面法线、3D 点云和语义标签。该数据集广泛用于评估单目深度估计算法，并提供在线数据处理和可视化工具。该数据集的优点是规模大、注释丰富、场景真实，缺点是分辨率低、深度值噪声大且仅限室内场景[[40](#bib.bib40)]。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: KITTI also includes depth maps, but its depth maps are limited by sparse and
    noisy LiDAR depth maps. There is also a lack of real depth maps on the ground
    for certain scenes, as well as limitations on city Settings[[32](#bib.bib32)].
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: KITTI 还包括深度图，但其深度图受限于稀疏和噪声较多的 LiDAR 深度图。某些场景中还缺乏实际的地面深度图，以及城市设置的限制[[32](#bib.bib32)]。
- en: 'Illumination data sets are collections of information about the intensity,
    distribution, and characteristics of artificial or natural light sources. Some
    examples of common illumination data sets are:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 照明数据集是关于人工或自然光源的强度、分布和特征的信息集合。常见的照明数据集的一些例子包括：
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi-PIE is a large-scale data set that contains over 750,000 images of 337
    subjects, captured in 15 view angles and 19 illumination conditions. Each subject
    also performed different facial expressions, such as neutral, smile, surprise,
    and squint. The data set is useful for studying face recognition, face alignment,
    face synthesis, and face editing under varying conditions.However, Multi-PIE only
    contains images of Caucasian subjects, which limits its diversity and generalization[[41](#bib.bib41)].
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Multi-PIE 是一个大规模数据集，包含 337 名受试者的超过 750,000 张图像，这些图像在 15 个视角和 19 种照明条件下拍摄。每个受试者还进行了不同的面部表情，如中性、微笑、惊讶和眯眼。该数据集对于研究人脸识别、人脸对齐、人脸合成和在不同条件下的人脸编辑非常有用。然而，Multi-PIE
    仅包含白种人受试者的图像，这限制了其多样性和泛化能力[[41](#bib.bib41)]。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Relightables is a collection of high-quality 3D scans of human subjects under
    varying lighting conditions. This data set allows for realistic rendering of human
    performances with any lighting and viewpoint, which can be integrated into any
    CG scene. Nevertheless, this data set has some drawbacks, such as the low diversity
    of subjects, poses, and expressions, and the high computational expense of processing
    the data[[42](#bib.bib42)].
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Relightables 是一个高质量的 3D 扫描数据集，涵盖了不同照明条件下的人体受试者。该数据集允许在任何照明和视点下进行真实感渲染，并可以集成到任何计算机图形场景中。然而，这个数据集也存在一些缺点，例如受试者、姿势和表情的多样性较低，以及处理数据的高计算开销[[42](#bib.bib42)]。
- en: In conclusion, data sets are essential for 3D controllable image synthesis based
    on deep learning, as they provide the necessary information for training and evaluating
    deep generative models. These data sets provide rich annotations and variations
    for different type of control, such as viewpoint, lighting, poses, point clouds,
    and depth. However, each data set has its own strengths and weaknesses, and there
    is still room for improvement and innovation in this field.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，数据集对于基于深度学习的 3D 可控图像合成至关重要，因为它们提供了训练和评估深度生成模型所需的信息。这些数据集为不同类型的控制提供了丰富的注释和变化，例如视点、照明、姿势、点云和深度。然而，每个数据集都有其自身的优缺点，并且在这一领域仍有改进和创新的空间。
- en: II-B Evaluation Indicators
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 评估指标
- en: '![Refer to caption](img/82fa2fae2bb4ef55a10c8da3cdbb5873.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/82fa2fae2bb4ef55a10c8da3cdbb5873.png)'
- en: (a) Original image
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始图像
- en: '![Refer to caption](img/3dc98e08b0f09b0ac4abc29e67d75492.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3dc98e08b0f09b0ac4abc29e67d75492.png)'
- en: (b) Image after changing light source
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 更改光源后的图像
- en: '![Refer to caption](img/05eb08214b583bff8d71c49231b2c41d.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/05eb08214b583bff8d71c49231b2c41d.png)'
- en: (c) Original image
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 原始图像
- en: '![Refer to caption](img/a044486e09caca4ecd595eecf7be4c73.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a044486e09caca4ecd595eecf7be4c73.png)'
- en: (d) Image after changing roughness
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 更改粗糙度后的图像
- en: 'Figure 4: Illumination manipulation example.Subfigure (a) and (b) correspond
    to the content of Global Illumination realization in Section [V](#S5 "V Illumination
    Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning"), which
    come from [[41](#bib.bib41)]. Subfigure (c) and (d) correspond to the content
    of Local Illumination realization in Section [V](#S5 "V Illumination Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning"), which come from
    [[43](#bib.bib43)]'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：照明操作示例。子图 (a) 和 (b) 对应于第 [V](#S5 "V Illumination Manipulation ‣ Survey on
    Controlable Image Synthesis with Deep Learning") 节中的全局照明实现内容，来源于 [[41](#bib.bib41)]。子图
    (c) 和 (d) 对应于第 [V](#S5 "V Illumination Manipulation ‣ Survey on Controlable Image
    Synthesis with Deep Learning") 节中的局部照明实现内容，来源于 [[43](#bib.bib43)]。
- en: 'To evaluate the quality and diversity of the synthesized images, several performance
    indicators are commonly used. Some of them are:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估合成图像的质量和多样性，通常使用几个性能指标。其中一些包括：
- en: '- Peak signal-to-noise ratio (PSNR)[[44](#bib.bib44)]: This measures the similarity
    between the synthesized image and a reference image in terms of pixel values.
    It is defined as the ratio of the maximum possible power of a signal to the power
    of noise that affects the fidelity of its representation. A higher PSNR indicates
    a better image quality.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '- 峰值信噪比（PSNR）[[44](#bib.bib44)]：这衡量合成图像和参考图像在像素值上的相似性。它定义为信号最大可能功率与影响其表现的噪声功率的比率。较高的PSNR表示图像质量更好。'
- en: '- Structural similarity index (SSIM)[[45](#bib.bib45)]: This measures the similarity
    between the synthesized image and a reference image in terms of luminance, contrast,
    and structure. It is based on the assumption that the human visual system is highly
    adapted to extract structural information from images. A higher SSIM indicates
    a better image quality.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '- 结构相似性指数（SSIM）[[45](#bib.bib45)]：这衡量合成图像和参考图像在亮度、对比度和结构上的相似性。它基于人类视觉系统高度适应于从图像中提取结构信息的假设。较高的SSIM表示图像质量更好。'
- en: '- Learned Perceptual Image Patch Similarity (LPIPS)[[46](#bib.bib46)]: This
    measures the similarity between the synthesized image and a reference image in
    terms of deep features. It is defined as the distance between the activations
    of two image patches for a pre-trained network. A lower LPIPS indicates a better
    image quality.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '- 学习感知图像块相似度（LPIPS）[[46](#bib.bib46)]：这衡量合成图像和参考图像在深度特征上的相似性。它定义为预训练网络中两个图像块激活之间的距离。较低的LPIPS表示图像质量更好。'
- en: '- Inception score (IS)[[47](#bib.bib47)]: This measures the quality and diversity
    of the synthesized images using a pre-trained classifier, such as Inception-v3\.
    It is based on the idea that good images should have high class diversity (i.e.,
    they can be classified into different categories) and low class ambiguity (i.e.,
    they can be classified with high confidence). A higher IS indicates a better image
    synthesis.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '- Inception分数（IS）[[47](#bib.bib47)]：这衡量合成图像的质量和多样性，使用预训练分类器，如Inception-v3。它基于这样一个理念：优质图像应具有高类别多样性（即它们可以被分类到不同的类别中）和低类别歧义（即它们可以被高度自信地分类）。较高的IS表示图像合成质量更好。'
- en: '- Fréchet inception distance (FID)[[48](#bib.bib48)]: This measures the distance
    between the feature distributions of the synthesized images and the real images
    using a pre-trained classifier, such as Inception-v3\. It is based on the idea
    that good images should have similar feature statistics to real images. A lower
    FID indicates a better image synthesis.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '- Fréchet inception距离（FID）[[48](#bib.bib48)]：这衡量合成图像和真实图像特征分布之间的距离，使用预训练分类器，如Inception-v3。它基于这样一个理念：优质图像应具有与真实图像相似的特征统计。较低的FID表示图像合成质量更好。'
- en: '- Kernel Inception Distance (KID)[[49](#bib.bib49)]: This measures the squared
    maximum mean discrepancy between the feature distributions of the synthesized
    images and the real images using a pre-trained classifier, such as Inception-v3\.
    It is based on the idea that good images should have similar feature statistics
    to real images. A lower KID indicates a better image synthesis.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '- 核Inception距离（KID）[[49](#bib.bib49)]：这衡量合成图像和真实图像特征分布之间的平方最大均值差异，使用预训练分类器，如Inception-v3。它基于这样一个理念：优质图像应具有与真实图像相似的特征统计。较低的KID表示图像合成质量更好。'
- en: III Pose Manipulation
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 姿态操作
- en: III-A Global Pose
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 全球姿态
- en: III-A1 GAN
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 GAN
- en: '![Refer to caption](img/255c0b6c2d3f735527c2b032c9bde22e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/255c0b6c2d3f735527c2b032c9bde22e.png)'
- en: 'Figure 5: Schematic of GAN, which come from [[50](#bib.bib50)]'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：GAN示意图，来自[[50](#bib.bib50)]
- en: 'Generative Adversarial Network (GAN)[[7](#bib.bib7)] can generate realistic
    and diverse data from a latent space. GAN consists of two neural networks: a generator
    and a discriminator. The generator tries to produce data that can fool the discriminator,
    while the discriminator tries to distinguish between real and fake data. The loss
    function of GAN measures how well the generator and the discriminator perform
    their tasks. The loss function is usually composed of two terms: one for the generator($\mathcal{L}_{G}$)
    and one for the discriminator($\mathcal{L}_{D}$). $\mathcal{L}_{G}$ is based on
    how often the discriminator classifies the generated data as real, while $\mathcal{L}_{D}$
    is based on how often it correctly classifies the real and fake data. The goal
    of GAN is to minimize $\mathcal{L}_{G}$ and maximize $\mathcal{L}_{D}$.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）[[7](#bib.bib7)]可以从潜在空间生成真实且多样化的数据。GAN由两个神经网络组成：一个生成器和一个判别器。生成器尝试生成能够欺骗判别器的数据，而判别器则试图区分真实数据和虚假数据。GAN的损失函数衡量生成器和判别器执行任务的效果。损失函数通常由两个部分组成：一个是生成器的($\mathcal{L}_{G}$)，另一个是判别器的($\mathcal{L}_{D}$)。$\mathcal{L}_{G}$基于判别器将生成的数据分类为真实的频率，而$\mathcal{L}_{D}$则基于判别器正确分类真实和虚假数据的频率。GAN的目标是最小化$\mathcal{L}_{G}$并最大化$\mathcal{L}_{D}$。
- en: '|  | $\mathcal{L}_{D}=\mathbb{E}_{\mathbf{x}\sim p_{\mathbf{data}}\mathbf{(x)}}[\log(D(\mathbf{x}))]+\mathbb{E}_{\mathbf{z}\sim
    p_{\mathbf{z}}\mathbf{(z)}}[\log(1-D(G(\mathbf{z})))],$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{D}=\mathbb{E}_{\mathbf{x}\sim p_{\mathbf{data}}\mathbf{(x)}}[\log(D(\mathbf{x}))]+\mathbb{E}_{\mathbf{z}\sim
    p_{\mathbf{z}}\mathbf{(z)}}[\log(1-D(G(\mathbf{z})))],$ |  |'
- en: '|  | $\mathcal{L}_{G}=-\mathbb{E}_{\mathbf{z}\sim p_{\mathbf{z}}\mathbf{(z)}}[\log(D(G(\mathbf{z})))],$
    |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{G}=-\mathbb{E}_{\mathbf{z}\sim p_{\mathbf{z}}\mathbf{(z)}}[\log(D(G(\mathbf{z})))],$
    |  |'
- en: '|  | $\mathcal{L}_{GAN}=\mathcal{L}_{D}+\mathcal{L}_{G}.$ |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{GAN}=\mathcal{L}_{D}+\mathcal{L}_{G}.$ |  |'
- en: 'a)Crossview image synthesis. Viewpoint manipulation refers to the ability to
    manipulate the perspective or orientation of the objects or scenes in the synthetic
    images. The earliest view composites were usually only able to composite a specific
    view, such as a bird’s eye view, a frontal view of a person’s face, etc. Huang
    et al. introduced TP-GAN, a method that integrates global structure and local
    details to generate realistic frontal views of faces [[51](#bib.bib51)]. Similarly,
    Zhao et al. proposed VariGAN, which combines variational inference and generative
    adversarial networks for the progressive refinement of synthesized target images
    [[52](#bib.bib52)]. To address the challenge of generating scenes from different
    viewpoints and resolutions, Krishna and Ali developed two methods: Crossview Fork
    (X-Fork) and Crossview Sequential (X-Seq) [[53](#bib.bib53)]. These methods employ
    semantic segmentation graphs to aid conditional GANs (cGANs) in producing sharper
    images. Furthermore, Krishna and Ali utilized geometry-guided cGANs for image
    synthesis, converting ground images to aerial views [[54](#bib.bib54)].Mokhayeri
    et al. proposed a cross-domain face synthesis approach using a Controllable GAN
    (C-GAN). This method generates realistic face images under various poses by refining
    simulated images from a 3D face model through an adversarial game [[55](#bib.bib55)].
    Zhu et al. developed BridgeGAN, a technique for synthesizing bird’s eye view images
    from single frontal view images. They employed a homography view as an intermediate
    representation to accomplish this task [[56](#bib.bib56)]. Ding et al. addressed
    the problem of cross-view image synthesis by utilizing generative adversarial
    networks (GANs) based on deformable convolution and attention mechanisms [[57](#bib.bib57)].
    Lastly, Ren et al. proposed MLP-Mixer GANs for cross-view image conversion. This
    method comprises two stages to alleviate severe deformation when generating entirely
    different views [[58](#bib.bib58)].'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: a)跨视图图像合成。视点操作指的是操控合成图像中物体或场景的透视或方向的能力。最早的视图合成通常只能合成特定视图，例如鸟瞰图、正面人脸图等。黄等人介绍了TP-GAN，这是一种集成全球结构和局部细节以生成逼真的正面人脸图的技术[[51](#bib.bib51)]。类似地，赵等人提出了VariGAN，它结合了变分推理和生成对抗网络用于逐步优化合成目标图像[[52](#bib.bib52)]。为了应对从不同视点和分辨率生成场景的挑战，克里希纳和阿里开发了两种方法：Crossview
    Fork (X-Fork) 和 Crossview Sequential (X-Seq) [[53](#bib.bib53)]。这些方法利用语义分割图来帮助条件GAN
    (cGANs) 生成更清晰的图像。此外，克里希纳和阿里还利用几何引导的cGANs进行图像合成，将地面图像转换为空中视角[[54](#bib.bib54)]。穆赫耶里等人提出了一种跨领域的人脸合成方法，使用可控GAN
    (C-GAN)。这种方法通过在对抗游戏中优化3D人脸模型生成的模拟图像来生成各种姿态的逼真的人脸图像[[55](#bib.bib55)]。朱等人开发了BridgeGAN，这是一种从单一正面视图图像合成鸟瞰图像的技术。他们使用单应性视图作为中间表示来完成这一任务[[56](#bib.bib56)]。丁等人通过利用基于可变形卷积和注意机制的生成对抗网络
    (GANs) 解决了跨视图图像合成的问题[[57](#bib.bib57)]。最后，任等人提出了MLP-Mixer GANs用于跨视图图像转换。这种方法包含两个阶段，以缓解生成完全不同视图时的严重变形[[58](#bib.bib58)]。
- en: b)Free viewpoint image synthesis. By adding conditional inputs such as camera
    pose or camera manifold to the GAN network, it can output images from any viewpoint.
    Zhu et al. introduced CycleGAN, a method capable of recovering the front face
    from a single profile postural facial image, even when the source domain does
    not match the target domain[[59](#bib.bib59)]. This approach is based on a conditional
    variational autoencoder and generative adversarial network (cVAE-GAN) framework,
    which does not require paired data, making it a versatile method for view translation[[60](#bib.bib60)].Shen
    et al. proposed Pairwise-GAN, employing two parallel U-Nets as generators and
    PatchGAN as a discriminator to synthesize frontal face images[[61](#bib.bib61)].
    Similarly, Chan et al. presented pi-GAN, a method utilizing periodic implicit
    Generative Adversarial Networks for high-quality 3D-aware image synthesis[[62](#bib.bib62)].
    Cai et al. further extended this approach with Pix2NeRF, an unsupervised method
    leveraging pi-GAN to train on single images without relying on 3D or multi-view
    supervision[[63](#bib.bib63)].Leimkuhler et al. introduced FreeStyleGAN, which
    integrates a pre-trained StyleGAN into standard 3D rendering pipelines, enabling
    stereo rendering or consistent insertion of faces in synthetic 3D environments[[64](#bib.bib64)].
    Medin et al. proposed MOST GAN, explicitly incorporating physical facial attributes
    as prior knowledge to achieve realistic portrait image manipulation[[65](#bib.bib65)].On
    the other hand, Or-El et al. developed StyleSDF, a novel method generating images
    based on StyleGAN2 by utilizing Signed Distance Fields (SDFs) to accurately model
    3D surfaces, enabling volumetric rendering with consistent results[[66](#bib.bib66)].
    Additionally, Zheng et al. presented SDF-StyleGAN, a deep learning method for
    generating 3D shapes based on StyleGAN2, employing two new shape discriminators
    operating on global and local levels to compare real and synthetic SDF values
    and gradients, significantly enhancing shape geometry and visual quality[[67](#bib.bib67)].Moreover,
    Deng et al. proposed GRAM, a novel approach regulating point sampling and radiance
    field learning on 2D manifolds, embodied as a set of learned implicit surfaces
    in the 3D volume, leading to improved synthesis results[[68](#bib.bib68)]. Xiang
    et al. built upon this work with GRAM-HD, capable of generating high-resolution
    images with strict 3D consistency, up to a resolution of 1024x1024[[69](#bib.bib69)].In
    another line of research, Chan et al. developed an efficient framework for generating
    realistic 3D shapes from 2D images using generative adversarial networks (GANs),
    comprising a geometry-aware module predicting the 3D shape and its projection
    parameters from the input image, and a refinement module enhancing shape quality
    and details[[70](#bib.bib70)]. Similarly, Zhao et al. proposed a method for generating
    high-quality 3D images from 2D inputs using GAN, achieving consistency across
    different viewpoints and offering rendering with novel lighting effects[[71](#bib.bib71)].Lastly,
    Alhaija et al. introduced XDGAN, a method for synthesizing realistic and diverse
    3D shapes from 2D images, converting 3D shapes into compact 1-channel geometry
    images and utilizing StyleGAN3 and image-to-image translation networks to generate
    3D objects in a 2D space[[72](#bib.bib72)]. These advancements in image synthesis
    techniques have significantly enriched the field of 3D image generation from 2D
    inputs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: b) 自由视角图像合成。通过向GAN网络添加条件输入，例如相机姿态或相机流形，可以从任何视角输出图像。Zhu等人引入了CycleGAN，这是一种能够从单个侧面姿势面部图像中恢复前脸的方法，即使源域与目标域不匹配[[59](#bib.bib59)]。该方法基于条件变分自编码器和生成对抗网络（cVAE-GAN）框架，不需要配对数据，使其成为一种通用的视角转换方法[[60](#bib.bib60)]。Shen等人提出了Pairwise-GAN，采用两个并行的U-Net作为生成器，并使用PatchGAN作为判别器来合成正面面部图像[[61](#bib.bib61)]。类似地，Chan等人提出了pi-GAN，这是一种利用周期性隐式生成对抗网络进行高质量3D感知图像合成的方法[[62](#bib.bib62)]。Cai等人进一步扩展了这一方法，提出了Pix2NeRF，这是一种无监督方法，利用pi-GAN在单张图像上进行训练，而无需依赖3D或多视图监督[[63](#bib.bib63)]。Leimkuhler等人引入了FreeStyleGAN，它将预训练的StyleGAN集成到标准的3D渲染管道中，实现了立体渲染或在合成的3D环境中一致地插入面部[[64](#bib.bib64)]。Medin等人提出了MOST
    GAN，明确地将物理面部特征作为先验知识，以实现逼真的肖像图像操控[[65](#bib.bib65)]。另一方面，Or-El等人开发了StyleSDF，这是一种基于StyleGAN2生成图像的新方法，通过利用签名距离场（SDFs）来准确建模3D表面，实现了体积渲染的一致结果[[66](#bib.bib66)]。此外，Zheng等人提出了SDF-StyleGAN，这是一种深度学习方法，用于基于StyleGAN2生成3D形状，采用两个新的形状判别器在全局和局部层面上比较真实和合成的SDF值及梯度，显著提升了形状几何和视觉质量[[67](#bib.bib67)]。此外，Deng等人提出了GRAM，这是一种调节点采样和辐射场学习的创新方法，体现为3D体积中的一组学习的隐式表面，从而改善了合成结果[[68](#bib.bib68)]。Xiang等人基于此工作开发了GRAM-HD，能够生成具有严格3D一致性的高分辨率图像，分辨率高达1024x1024[[69](#bib.bib69)]。在另一项研究中，Chan等人开发了一个高效的框架，通过生成对抗网络（GANs）从2D图像生成逼真的3D形状，该框架包括一个几何感知模块，用于预测3D形状及其投影参数，以及一个增强形状质量和细节的细化模块[[70](#bib.bib70)]。类似地，Zhao等人提出了一种从2D输入生成高质量3D图像的方法，利用GAN实现不同视角间的一致性，并提供具有新颖光照效果的渲染[[71](#bib.bib71)]。最后，Alhaija等人引入了XDGAN，这是一种从2D图像合成逼真多样的3D形状的方法，将3D形状转换为紧凑的单通道几何图像，并利用StyleGAN3和图像到图像转换网络在2D空间中生成3D物体[[72](#bib.bib72)]。这些图像合成技术的进展显著丰富了从2D输入生成3D图像的领域。
- en: III-A2 NeRF
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 NeRF
- en: '![Refer to caption](img/c38bb91d21cb0f6aa38a9a8bf0119c16.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c38bb91d21cb0f6aa38a9a8bf0119c16.png)'
- en: 'Figure 6: Schematic of NeRF, which come from [[17](#bib.bib17)]'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：NeRF的示意图，来源于[[17](#bib.bib17)]
- en: 'Neural Radiance Fields (NeRF)[[17](#bib.bib17)] are a novel representation
    for complex 3D scenes that can be rendered photorealistically from any viewpoint.
    NeRF models a scene as a continuous function that maps 5D coordinates (3D location
    and 2D viewing direction, expressed as ($x$, $y$, $z$, $\theta$, $\varphi$)) to
    a 4D output (RGB color and opacity). This function is learned from a set of posed
    images of the scene using a deep neural network. Before the nerf passes the ($x$,
    $y$, $z$, $\theta$, $\varphi$) input to the network, it maps the input to a higher
    dimensional space using high-frequency functions to better fit the data containing
    high-frequency variations. The high-frequency coding function is:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 神经辐射场（NeRF）[[17](#bib.bib17)]是一种新颖的复杂3D场景表示方法，可以从任何视角进行逼真渲染。NeRF将场景建模为一个连续的函数，该函数将5D坐标（3D位置和2D视角方向，表示为（$x$，$y$，$z$，$\theta$，$\varphi$））映射到4D输出（RGB颜色和不透明度）。这个函数是通过深度神经网络从一组已定姿态的场景图像中学习到的。在将（$x$，$y$，$z$，$\theta$，$\varphi$）输入传递给网络之前，NeRF使用高频函数将输入映射到更高维的空间，以更好地拟合包含高频变化的数据。高频编码函数是：
- en: '|  | $\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\ldots,\sin\left(2^{L-1}\pi
    p\right),\cos\left(2^{L-1}\pi p\right)\right)$ |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\ldots,\sin\left(2^{L-1}\pi
    p\right),\cos\left(2^{L-1}\pi p\right)\right)$ |  |'
- en: Where $p$ is the input ($x$, $y$, $z$, $\theta$, $\varphi$).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p$是输入（$x$，$y$，$z$，$\theta$，$\varphi$）。
- en: 'TABLE I: Enhancements to NeRF'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：NeRF的改进
- en: '|  | Method | Publication | Image resolution | Dataset |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 发表 | 图像分辨率 | 数据集 |'
- en: '| no camera pose | NeRF–[[73](#bib.bib73)] | arXiv2022 | 756x1008/1080x1920/520x780
    | [[74](#bib.bib74)]/[[28](#bib.bib28)]/[[73](#bib.bib73)] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 无相机姿态 | NeRF–[[73](#bib.bib73)] | arXiv2022 | 756x1008/1080x1920/520x780
    | [[74](#bib.bib74)]/[[28](#bib.bib28)]/[[73](#bib.bib73)] |'
- en: '| GNeRF[[75](#bib.bib75)] | ICCV2021 | 400x400/500x400 | [[17](#bib.bib17)]/[[76](#bib.bib76)]
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GNeRF[[75](#bib.bib75)] | ICCV2021 | 400x400/500x400 | [[17](#bib.bib17)]/[[76](#bib.bib76)]
    |'
- en: '| SCNeRF[[77](#bib.bib77)] | ICCV2021 |  | [[74](#bib.bib74)]/[[78](#bib.bib78)]
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SCNeRF[[77](#bib.bib77)] | ICCV2021 |  | [[74](#bib.bib74)]/[[78](#bib.bib78)]
    |'
- en: '| NoPe-NeRF[[79](#bib.bib79)] | CVPR2023 |  | [[76](#bib.bib76)]/[[74](#bib.bib74)]/[[80](#bib.bib80)]
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| NoPe-NeRF[[79](#bib.bib79)] | CVPR2023 |  | [[76](#bib.bib76)]/[[74](#bib.bib74)]/[[80](#bib.bib80)]
    |'
- en: '| SPARF[[81](#bib.bib81)] | CVPR2023 | 960x540/648x484 | [[78](#bib.bib78)]/[[27](#bib.bib27)]
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SPARF[[81](#bib.bib81)] | CVPR2023 | 960x540/648x484 | [[78](#bib.bib78)]/[[27](#bib.bib27)]
    |'
- en: '| sparse data | NeRS[[82](#bib.bib82)] | NIPS2021 |  | [[82](#bib.bib82)] |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| sparse data | NeRS[[82](#bib.bib82)] | NIPS2021 |  | [[82](#bib.bib82)] |'
- en: '| MixNeRF[[83](#bib.bib83)] | CVPR2023 |  | [[76](#bib.bib76)]/[[74](#bib.bib74)]/[[17](#bib.bib17)]
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MixNeRF[[83](#bib.bib83)] | CVPR2023 |  | [[76](#bib.bib76)]/[[74](#bib.bib74)]/[[17](#bib.bib17)]
    |'
- en: '| SceneRF[[84](#bib.bib84)] | arXiv2023 | 1220x370 | [[85](#bib.bib85)] |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| SceneRF[[84](#bib.bib84)] | arXiv2023 | 1220x370 | [[85](#bib.bib85)] |'
- en: '| GM-NeRF[[86](#bib.bib86)] | CVPR2023 | 224x224 | [[87](#bib.bib87)]/[[88](#bib.bib88)]/[[89](#bib.bib89)]/[[90](#bib.bib90)]
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GM-NeRF[[86](#bib.bib86)] | CVPR2023 | 224x224 | [[87](#bib.bib87)]/[[88](#bib.bib88)]/[[89](#bib.bib89)]/[[90](#bib.bib90)]
    |'
- en: '| SPARF[[81](#bib.bib81)] | CVPR2023 | 960x540/648x484 | [[78](#bib.bib78)]/[[27](#bib.bib27)]
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SPARF[[81](#bib.bib81)] | CVPR2023 | 960x540/648x484 | [[78](#bib.bib78)]/[[27](#bib.bib27)]
    |'
- en: '| noisy data | RawNeRF[[91](#bib.bib91)] | CVPR2022 |  | [[91](#bib.bib91)]
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| noisy data | RawNeRF[[91](#bib.bib91)] | CVPR2022 |  | [[91](#bib.bib91)]
    |'
- en: '| Deblur-NeRF[[92](#bib.bib92)] | CVPR2022 |  | [[92](#bib.bib92)] |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Deblur-NeRF[[92](#bib.bib92)] | CVPR2022 |  | [[92](#bib.bib92)] |'
- en: '| HDR-NeRF[[93](#bib.bib93)] | CVPR2022 | 400x400/804x534 | [[93](#bib.bib93)]
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| HDR-NeRF[[93](#bib.bib93)] | CVPR2022 | 400x400/804x534 | [[93](#bib.bib93)]
    |'
- en: '| NAN[[94](#bib.bib94)] | CVPR2022 |  | [[94](#bib.bib94)] |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| NAN[[94](#bib.bib94)] | CVPR2022 |  | [[94](#bib.bib94)] |'
- en: '| large-scale image synthesis | Mip-NeRF 360[[95](#bib.bib95)] | CVPR2022 |  |
    [[78](#bib.bib78)] |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 大规模图像合成 | Mip-NeRF 360[[95](#bib.bib95)] | CVPR2022 |  | [[78](#bib.bib78)]
    |'
- en: '| BungeeNeRF[[96](#bib.bib96)] | ECCV2022 |  | [[97](#bib.bib97)] |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| BungeeNeRF[[96](#bib.bib96)] | ECCV2022 |  | [[97](#bib.bib97)] |'
- en: '| Block-NeRF[[98](#bib.bib98)] | arXiv2022 |  | [[98](#bib.bib98)] |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Block-NeRF[[98](#bib.bib98)] | arXiv2022 |  | [[98](#bib.bib98)] |'
- en: '| GridNeRF[[99](#bib.bib99)] | CVPR2023 |  | [[100](#bib.bib100)]/[[99](#bib.bib99)]
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| GridNeRF[[99](#bib.bib99)] | CVPR2023 |  | [[100](#bib.bib100)]/[[99](#bib.bib99)]
    |'
- en: '| EgoNeRF[[101](#bib.bib101)] | CVPR2023 |  | [[101](#bib.bib101)] |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| EgoNeRF[[101](#bib.bib101)] | CVPR2023 |  | [[101](#bib.bib101)] |'
- en: '| image synthesis speed | PlenOctrees[[102](#bib.bib102)] | ICCV2021 | 800x800/1920x1080
    | [[17](#bib.bib17)]/[[78](#bib.bib78)] |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 图像合成速度 | PlenOctrees[[102](#bib.bib102)] | ICCV2021 | 800x800/1920x1080 |
    [[17](#bib.bib17)]/[[78](#bib.bib78)] |'
- en: '| DirectVoxGO[[103](#bib.bib103)] | CVPR2022 | 800x800/800x800/768x576/1920x1080/512x512
    | [[17](#bib.bib17)]/[[104](#bib.bib104)]/[[105](#bib.bib105)]/[[78](#bib.bib78)]/[[106](#bib.bib106)]
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| DirectVoxGO[[103](#bib.bib103)] | CVPR2022 | 800x800/800x800/768x576/1920x1080/512x512
    | [[17](#bib.bib17)]/[[104](#bib.bib104)]/[[105](#bib.bib105)]/[[78](#bib.bib78)]/[[106](#bib.bib106)]
    |'
- en: '| R2L[[107](#bib.bib107)] | ECCV2022 |  | [[17](#bib.bib17)]/[[108](#bib.bib108)]
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| R2L[[107](#bib.bib107)] | ECCV2022 |  | [[17](#bib.bib17)]/[[108](#bib.bib108)]
    |'
- en: '| SqueezeNeRF[[109](#bib.bib109)] | CVPR2022 |  | [[17](#bib.bib17)]/[[74](#bib.bib74)]
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeNeRF[[109](#bib.bib109)] | CVPR2022 |  | [[17](#bib.bib17)]/[[74](#bib.bib74)]
    |'
- en: '| MobileNeRF[[110](#bib.bib110)] | CVPR2023 | 800x800/1008x756/1256x828 | [[17](#bib.bib17)]/[[74](#bib.bib74)]/[[95](#bib.bib95)]
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MobileNeRF[[110](#bib.bib110)] | CVPR2023 | 800x800/1008x756/1256x828 | [[17](#bib.bib17)]/[[74](#bib.bib74)]/[[95](#bib.bib95)]
    |'
- en: '| L2G-NeRF[[111](#bib.bib111)] | CVPR2023 |  | [[74](#bib.bib74)] |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| L2G-NeRF[[111](#bib.bib111)] | CVPR2023 |  | [[74](#bib.bib74)] |'
- en: 'Zhang et al. introduced NeRF++ as a framework that enhances NeRF (Neural Radiance
    Fields) through adaptive sampling, hierarchical volume rendering, and multi-scale
    feature encoding techniques [[112](#bib.bib112)]. This approach enables high-quality
    rendering for both static and dynamic scenes while improving efficiency and robustness.
    Rebain et al. proposed a method to enhance the efficiency and quality of neural
    rendering by employing spatial decomposition [[113](#bib.bib113)].Park et al.
    developed a novel technique for capturing and rendering high-quality 3D selfies
    using a single RGB camera. Their method utilizes a deformable neural radiance
    field (NeRF) model capable of representing both the geometry and appearance of
    dynamic scenes [[114](#bib.bib114)]. Li et al. introduced MINE, a method for novel
    view synthesis and depth estimation from a single image. This approach generalizes
    Multiplane Images (MPI) with continuous depth using Neural Radiance Fields (NeRF)
    [[115](#bib.bib115)].Park et al. proposed HyperNeRF, a method for representing
    and rendering complex 3D scenes with varying topology using neural radiance fields
    (NeRFs). Unlike previous NeRF-based approaches that rely on a fixed 3D coordinate
    system, HyperNeRF employs a higher-dimensional continuous embedding space to capture
    arbitrary scene changes [[116](#bib.bib116)]. Chen et al. presented Aug-NeRF,
    a novel method for training neural radiance fields (NeRFs) with physically-grounded
    augmentations at different levels: scene, camera, and pixel [[117](#bib.bib117)].Kaneko
    proposed AR-NeRF, a method for learning 3D representations of natural images without
    supervision. The approach utilizes a neural radiance field (NeRF) model to render
    images with various viewpoints and aperture sizes, capturing both depth and defocus
    effects [[118](#bib.bib118)]. Li et al. introduced SymmNeRF, a framework that
    utilizes neural radiance fields (NeRFs) to synthesize novel views of objects from
    a single image. This method leverages symmetry priors to recover fine appearance
    details, particularly in self-occluded areas [[119](#bib.bib119)].Zhou et al.
    proposed NeRFLiX, a novel framework for improving the quality of novel view synthesis
    using neural radiance fields (NeRF). This approach addresses rendering artifacts
    such as noise and blur by employing an inter-viewpoint aggregation framework that
    fuses high-quality training images to generate more realistic synthetic views
    [[120](#bib.bib120)].'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人介绍了 NeRF++，这是一个通过自适应采样、分层体积渲染和多尺度特征编码技术来增强 NeRF（神经辐射场）的框架 [[112](#bib.bib112)]。这种方法使得在静态和动态场景中都能实现高质量渲染，同时提高了效率和鲁棒性。Rebain
    等人提出了一种通过空间分解来提高神经渲染效率和质量的方法 [[113](#bib.bib113)]。Park 等人开发了一种使用单个 RGB 相机捕捉和渲染高质量
    3D 自拍的新技术。他们的方法利用了一个变形的神经辐射场（NeRF）模型，该模型能够表示动态场景的几何和外观 [[114](#bib.bib114)]。Li
    等人介绍了 MINE，一种从单张图像中进行新视角合成和深度估计的方法。这种方法利用神经辐射场（NeRF）来推广多平面图像（MPI）和连续深度 [[115](#bib.bib115)]。Park
    等人提出了 HyperNeRF，一种使用神经辐射场（NeRF）表示和渲染具有不同拓扑的复杂 3D 场景的方法。与依赖于固定 3D 坐标系统的先前 NeRF
    方法不同，HyperNeRF 采用了更高维的连续嵌入空间来捕捉任意场景变化 [[116](#bib.bib116)]。Chen 等人提出了 Aug-NeRF，一种在场景、相机和像素等不同层次上使用物理基础增强的训练神经辐射场（NeRF）的方法
    [[117](#bib.bib117)]。Kaneko 提出了 AR-NeRF，一种无需监督地学习自然图像 3D 表示的方法。这种方法利用神经辐射场（NeRF）模型渲染具有各种视角和光圈尺寸的图像，捕捉深度和散焦效果
    [[118](#bib.bib118)]。Li 等人介绍了 SymmNeRF，一个利用神经辐射场（NeRF）从单张图像合成新视角的框架。该方法利用对称先验来恢复细微的外观细节，特别是在自遮挡区域
    [[119](#bib.bib119)]。Zhou 等人提出了 NeRFLiX，这是一个使用神经辐射场（NeRF）提高新视角合成质量的新框架。这种方法通过采用一个视角间聚合框架来融合高质量训练图像，从而生成更逼真的合成视角，并解决渲染伪影如噪声和模糊
    [[120](#bib.bib120)]。
- en: Besides, a number of researchers have proposed enhancements to the original
    NeRF model, addressing its limitations in scenarios such as no camera pose, sparse
    data, noisy data, large-scale image synthesis, and image synthesis speed.See Table
    [I](#S3.T1 "TABLE I ‣ III-A2 NeRF ‣ III-A Global Pose ‣ III Pose Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning").
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究人员提出了对原始 NeRF 模型的改进，以解决其在无相机姿态、稀疏数据、噪声数据、大规模图像合成以及图像合成速度等场景中的局限性。见表格
    [I](#S3.T1 "TABLE I ‣ III-A2 NeRF ‣ III-A Global Pose ‣ III Pose Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning")。
- en: III-A3 Diffusion model
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 扩散模型
- en: '![Refer to caption](img/2a1ce14797f7f3b68ca52ebed3e84c20.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2a1ce14797f7f3b68ca52ebed3e84c20.png)'
- en: 'Figure 7: Schematic of diffusion model, which come from [[50](#bib.bib50)]'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：扩散模型示意图，来源于 [[50](#bib.bib50)]
- en: One of the most widely used models in deep learning is the diffusion model,
    which is a generative model that can produce realistic and diverse images from
    random noise. The diffusion model is based on the idea of reversing the process
    of adding Gaussian noise to an image until it becomes completely corrupted. The
    diffusion process starts from a data sample and gradually adds noise until it
    reaches a predefined noise level, i.e. $q\left(\mathbf{x}_{t}\mid\mathbf{x}_{t-1}\right)$
    in the Fig. [7](#S3.F7 "Figure 7 ‣ III-A3 Diffusion model ‣ III-A Global Pose
    ‣ III Pose Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning").
    The generative model then learns to reverse this process by denoising the samples
    at each step i.e. $p_{\mathbf{\theta}}\left(\mathbf{x}_{t-1}\mid\mathbf{x}_{t}\right)$
    in the Fig. [7](#S3.F7 "Figure 7 ‣ III-A3 Diffusion model ‣ III-A Global Pose
    ‣ III Pose Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning").
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中最广泛使用的模型之一是扩散模型，这是一种生成模型，能够从随机噪声中生成现实且多样的图像。扩散模型基于逆转向图像添加高斯噪声直到完全损坏的过程的理念。扩散过程从一个数据样本开始，逐渐添加噪声，直到达到预定义的噪声水平，即图中的
    $q\left(\mathbf{x}_{t}\mid\mathbf{x}_{t-1}\right)$ [7](#S3.F7 "Figure 7 ‣ III-A3
    Diffusion model ‣ III-A Global Pose ‣ III Pose Manipulation ‣ Survey on Controlable
    Image Synthesis with Deep Learning")。然后，生成模型通过在每一步去噪样本来学习逆转这一过程，即图中的 $p_{\mathbf{\theta}}\left(\mathbf{x}_{t-1}\mid\mathbf{x}_{t}\right)$
    [7](#S3.F7 "Figure 7 ‣ III-A3 Diffusion model ‣ III-A Global Pose ‣ III Pose Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning")。
- en: '|  | $q\left(\mathbf{x}_{t}\mid\mathbf{x}_{t-1}\right)=\mathcal{N}\left(\mathbf{x}_{t};\sqrt{1-\beta_{t}}\mathbf{x}_{t-1},\beta_{t}\mathbf{I}\right)$
    |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $q\left(\mathbf{x}_{t}\mid\mathbf{x}_{t-1}\right)=\mathcal{N}\left(\mathbf{x}_{t};\sqrt{1-\beta_{t}}\mathbf{x}_{t-1},\beta_{t}\mathbf{I}\right)$
    |  |'
- en: Sbrolli et al. introduced IC3D, a novel approach addressing various challenges
    in shape generation. This method is capable of reconstructing a 3D shape from
    a single view, synthesizing a 3D shape from multiple views, and completing a 3D
    shape from partial inputs [[121](#bib.bib121)]. Another significant contribution
    in this area is the work by Gu et al., who developed Control3Diff, a generative
    model with 3D-awareness and controllability. By combining diffusion models and
    3D GANs, Control3Diff can synthesize diverse and realistic images without relying
    on 3D ground truth data, and can be trained solely on single-view image datasets
    [[122](#bib.bib122)].Additionally, Anciukevicius et al. proposed RenderDiffusion,
    an innovative diffusion model for 3D generation and inference. Remarkably, this
    model can be trained using only monocular 2D supervision and incorporates an intermediate
    three-dimensional representation of the scene during each denoising step, effectively
    integrating a robust inductive structure into the diffusion process [[123](#bib.bib123)].Xiang
    et al. presented a novel method for generating 3D-aware images using 2D diffusion
    models. Their approach involves a sequential process of generating multiview 2D
    images from different perspectives, ultimately achieving the synthesis of 3D-aware
    images [[124](#bib.bib124)].Furthermore, Liu et al. proposed a framework for changing
    the camera viewpoint of an object using only a single RGB image. Leveraging the
    geometric priors learned by large-scale diffusion models about natural images,
    their framework employs a synthetic dataset to learn the controls for adjusting
    the relative camera viewpoint [[125](#bib.bib125)].Lastly, Chan et al. developed
    a method for generating diverse and realistic novel views of a scene based on
    a single input image. Their approach utilizes a diffusion-based model that incorporates
    3D geometry priors through a latent feature volume. This feature volume captures
    the distribution of potential scene representations and enables the rendering
    of view-consistent images [[126](#bib.bib126)].
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Sbrolli 等人提出了 IC3D，一种新颖的方法来解决形状生成中的各种挑战。该方法能够从单一视角重建 3D 形状、从多个视角合成 3D 形状，以及从部分输入完成
    3D 形状 [[121](#bib.bib121)]。该领域的另一项重要贡献是 Gu 等人的工作，他们开发了 Control3Diff，这是一种具有 3D
    认知和可控性的生成模型。通过将扩散模型和 3D GAN 结合，Control3Diff 可以合成多样且真实的图像，而无需依赖 3D 实测数据，并且可以仅通过单视图图像数据集进行训练
    [[122](#bib.bib122)]。此外，Anciukevicius 等人提出了 RenderDiffusion，这是一种用于 3D 生成和推断的创新扩散模型。值得注意的是，该模型可以仅使用单眼
    2D 监督进行训练，并在每个去噪步骤中引入场景的中间三维表示，有效地将一个强大的归纳结构融入到扩散过程中 [[123](#bib.bib123)]。Xiang
    等人提出了一种使用 2D 扩散模型生成 3D 认知图像的新方法。他们的方法涉及从不同视角生成多视图 2D 图像的顺序过程，最终实现 3D 认知图像的合成 [[124](#bib.bib124)]。此外，Liu
    等人提出了一种框架，通过仅使用单个 RGB 图像来改变物体的相机视角。利用大规模扩散模型对自然图像学到的几何先验，他们的框架采用合成数据集来学习调整相机视角的控制
    [[125](#bib.bib125)]。最后，Chan 等人开发了一种基于单个输入图像生成多样且真实的场景新视图的方法。他们的方法利用了一种基于扩散的模型，该模型通过潜在特征体积结合
    3D 几何先验。这个特征体积捕捉了潜在场景表示的分布，并能渲染视角一致的图像 [[126](#bib.bib126)]。
- en: '![Refer to caption](img/986bf5a7e9e0f90ca4de34a6172e999a.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/986bf5a7e9e0f90ca4de34a6172e999a.png)'
- en: 'Figure 8: Schematic of Transformer, which come from [[128](#bib.bib128)]'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Transformer 的示意图，来源于 [[128](#bib.bib128)]
- en: III-A4 Transformer
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 Transformer
- en: 'Transformers are a type of neural network architecture that have been widely
    used in natural language processing. They are based on the idea of self-attention,
    which allows the network to learn the relationships between different parts of
    the input and output sequences. Transformers is introduced into the field of computer
    vision in the paper ViT[[127](#bib.bib127)]. Its core is the Attention section
    in Fig. [8](#S3.F8 "Figure 8 ‣ III-A3 Diffusion model ‣ III-A Global Pose ‣ III
    Pose Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning"),
    and its formula is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 是一种广泛用于自然语言处理的神经网络架构。它们基于自注意力的思想，允许网络学习输入和输出序列中不同部分之间的关系。Transformers
    被引入计算机视觉领域的论文 ViT [[127](#bib.bib127)]。其核心是图 [8](#S3.F8 "Figure 8 ‣ III-A3 Diffusion
    model ‣ III-A Global Pose ‣ III Pose Manipulation ‣ Survey on Controlable Image
    Synthesis with Deep Learning") 中的 Attention 部分，其公式如下：
- en: '|  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$ |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$ |  |'
- en: Leveraging the Transformer architecture for vision applications, several studies
    have explored its potential for synthesizing 3D views. Nguyen-Ha and colleagues
    presented a pioneering approach to synthesizing new views of a scene using a given
    set of input views. Their method employs a transformer-based architecture that
    effectively captures the long-range dependencies among the input views. By using
    a sequential process, the method generates high-quality novel views. This research
    contribution is documented in [[129](#bib.bib129)]. Similarly, Yang and colleagues
    proposed an innovative method for generating viewpoint-invariant 3D shapes from
    a single image. Their approach is based on disentangling learning and parametric
    NURBS surface generation. The method employs an encoder-decoder network augmented
    with a disentangled transformer module. This configuration enables the independent
    learning of shape semantics and camera viewpoints. The output of this comprehensive
    network includes the geometric parameters of the NURBS surface representing the
    3D shape, as well as the camera-viewpoint parameters involving rotation, translation,
    and scaling. Further details of this method can be found in [[130](#bib.bib130)].
    Additionally, Kulhánek and colleagues proposed ViewFormer, an impressive neural
    rendering method that does not rely on NeRF and instead capitalizes on the power
    of transformers. ViewFormer is designed to learn a latent representation of a
    scene using only a few images, and this learned representation enables the synthesis
    of novel views. Notably, ViewFormer can handle complex scenes with varying illumination
    and geometry without requiring any 3D information or ray marching. The specific
    approach and findings of ViewFormer are detailed in [[131](#bib.bib131)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 Transformer 架构在视觉应用中的潜力，已有几项研究探讨了其合成 3D 视图的可能性。Nguyen-Ha 和同事提出了一种开创性的方法，通过给定的一组输入视图合成场景的新视图。他们的方法采用了基于
    Transformer 的架构，有效捕捉了输入视图之间的长距离依赖关系。通过使用顺序处理，这种方法生成了高质量的新视图。这项研究贡献在 [[129](#bib.bib129)]
    中有所记录。类似地，Yang 和同事提出了一种创新方法，用于从单幅图像生成视角不变的 3D 形状。他们的方法基于解缠学习和参数化 NURBS 表面生成。该方法采用了增强了解缠
    Transformer 模块的编码器-解码器网络。这种配置使得形状语义和相机视角能够独立学习。该综合网络的输出包括代表 3D 形状的 NURBS 表面的几何参数，以及涉及旋转、平移和缩放的相机视角参数。该方法的进一步细节可以在
    [[130](#bib.bib130)] 中找到。此外，Kulhánek 和同事提出了 ViewFormer，这是一种令人印象深刻的神经渲染方法，未依赖于
    NeRF，而是利用了 Transformer 的强大功能。ViewFormer 旨在仅使用少量图像来学习场景的潜在表示，这种学习到的表示能够合成新视图。值得注意的是，ViewFormer
    能够处理具有不同光照和几何特征的复杂场景，而无需任何 3D 信息或光线行进。ViewFormer 的具体方法和发现详见 [[131](#bib.bib131)]。
- en: III-A5 Hybrid NeRF
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A5 混合 NeRF
- en: a)GAN-based NeRF. Neural Radiance Fields (NeRF) are a novel method for rendering
    images from arbitrary viewpoints, but they suffer from high computational cost
    due to their pixel-wise optimization. Generative Adversarial Networks (GANs) can
    synthesize realistic images in a single forward pass, but they may not preserve
    the view consistency across different viewpoints. Hence, there is a growing interest
    in exploring the integration of NeRF and GAN for efficient and consistent image
    synthesis.Meng et al. presented the GNeRF framework, which combines GANs and NeRF
    reconstruction to generate scenes with unknown or random camera poses [[75](#bib.bib75)].
    Similarly, Zhou et al. introduced CIPS-3D, a generative model that utilizes style
    transfer, shallow NeRF networks, and deep INR networks to represent 3D scenes
    and provide precise control over camera poses [[132](#bib.bib132)].Another approach
    by Xu et al. is GRAF, a generative model for radiance fields that enables high-resolution
    image synthesis while being aware of the 3D shape. GRAF disentangles camera and
    scene properties from unposed 2D images, allowing for the synthesis of novel views
    and modifications to shape and appearance [[133](#bib.bib133)]. Lan et al. proposed
    a self-supervised geometry-aware encoder for style-based 3D GAN inversion. Their
    encoder recovers the latent code of a given 3D shape and enables manipulation
    of its style and geometry attributes [[134](#bib.bib134)].Li et al. developed
    a two-step approach for 3D-aware multi-class image-to-image translation using
    NeRFs. They trained a multi-class 3D-aware GAN with a conditional architecture
    and innovative training strategy. Based on this GAN, they constructed a 3D-aware
    image-to-image translation system [[135](#bib.bib135)]. Shahbazi et al. focused
    on knowledge distillation, proposing a method to transfer the knowledge of a GAN
    trained on NeRF representation to a convolutional neural network (CNN). This enables
    efficient 3D-aware image synthesis [[136](#bib.bib136)].Kania et al. introduced
    a generative model for 3D objects based on NeRFs, which are rendered into 2D novel
    views using a hypernetwork. The model is trained adversarially with a 2D discriminator
    [[137](#bib.bib137)]. Lastly, Bhattarai et al. proposed TriPlaneNet, an encoder
    specifically designed for EG3D inversion. The task of EG3D inversion involves
    reconstructing 3D shapes from 2D edge images [[138](#bib.bib138)].
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: a) 基于GAN的NeRF。神经辐射场（NeRF）是一种从任意视角渲染图像的新颖方法，但由于其像素级优化，计算成本较高。生成对抗网络（GANs）可以在一次前向传播中合成逼真的图像，但可能无法保持不同视角之间的视图一致性。因此，越来越多的研究关注NeRF和GAN的集成，以实现高效且一致的图像合成。Meng等人提出了GNeRF框架，它结合了GANs和NeRF重建，用于生成具有未知或随机相机姿态的场景[[75](#bib.bib75)]。类似地，Zhou等人介绍了CIPS-3D，这是一种生成模型，利用风格迁移、浅层NeRF网络和深层INR网络来表示3D场景，并提供对相机姿态的精确控制[[132](#bib.bib132)]。Xu等人的另一种方法是GRAF，这是一种用于辐射场的生成模型，它能够在考虑3D形状的同时实现高分辨率图像合成。GRAF将相机和场景属性从未定位的2D图像中分离，允许合成新的视图并对形状和外观进行修改[[133](#bib.bib133)]。Lan等人提出了一种自监督几何感知编码器，用于基于风格的3D
    GAN反演。他们的编码器恢复给定3D形状的潜在代码，并允许操作其风格和几何属性[[134](#bib.bib134)]。Li等人开发了一种基于NeRF的3D感知多类图像到图像转换的两步方法。他们训练了一种具有条件架构和创新训练策略的多类3D感知GAN。基于该GAN，他们构建了一个3D感知的图像到图像转换系统[[135](#bib.bib135)]。Shahbazi等人专注于知识蒸馏，提出了一种将基于NeRF表示训练的GAN的知识转移到卷积神经网络（CNN）中的方法。这使得高效的3D感知图像合成成为可能[[136](#bib.bib136)]。Kania等人介绍了一种基于NeRF的3D对象生成模型，该模型通过超网络渲染成2D新视图。该模型通过2D鉴别器进行对抗性训练[[137](#bib.bib137)]。最后，Bhattarai等人提出了TriPlaneNet，这是一种专门设计用于EG3D反演的编码器。EG3D反演任务涉及从2D边缘图像中重建3D形状[[138](#bib.bib138)]。
- en: b)Diffusion model-based NeRF. Likewise, the diffusion model alone fails to produce
    images that are consistent across different viewpoints. Therefore, many researchers
    integrate it with NeRF to synthesize high-quality and view-consistent images.Muller
    et al. proposed DiffRF, which directly generates volumetric radiance fields from
    a set of posed images by using a 3D denoising model and a rendering loss [[139](#bib.bib139)].
    Similarly, Xu et al. proposed NeuralLift-360, a framework that generates a 3D
    object with 360° views from a single 2D photo using a depth-aware NeRF and a denoising
    diffusion model [[140](#bib.bib140)]. Chan et al. proposed a 3D-aware image synthesis
    framework using NeRF and diffusion models, which jointly optimizes a NeRF auto-decoder
    and a latent diffusion model to enable simultaneous 3D reconstruction and prior
    learning from multi-view images of diverse objects [[141](#bib.bib141)]. Lastly,
    Gu et al. proposed NerfDiff, a method for generating realistic and 3D-consistent
    novel views from a single input image. This method distills the knowledge of the
    conditional diffusion model (CDM) into the NeRF by synthesizing and refining a
    set of virtual views at test time, using a NeRF-guided distillation algorithm
    [[142](#bib.bib142)]. These approaches demonstrate the potential of using NeRF
    and diffusion models for 3D scene synthesis, and further research in this area
    is expected to yield even more exciting results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: b) 基于扩散模型的 NeRF。同样，扩散模型单独无法生成在不同视角下保持一致的图像。因此，许多研究人员将其与 NeRF 结合以合成高质量和视角一致的图像。Muller
    等人提出了 DiffRF，该方法通过使用 3D 去噪模型和渲染损失从一组已标定的图像中直接生成体积辐射场[[139](#bib.bib139)]。类似地，Xu
    等人提出了 NeuralLift-360，一个框架，它使用深度感知 NeRF 和去噪扩散模型从单张 2D 照片中生成一个具有 360° 视角的 3D 对象[[140](#bib.bib140)]。Chan
    等人提出了一种使用 NeRF 和扩散模型的 3D 感知图像合成框架，该框架联合优化 NeRF 自编码器和潜在扩散模型，以实现从多视角图像中对各种对象进行同时
    3D 重建和先验学习[[141](#bib.bib141)]。最后，Gu 等人提出了 NerfDiff，一种从单张输入图像生成逼真且 3D 一致的新视角的方法。该方法通过在测试时合成和精炼一组虚拟视角，将条件扩散模型（CDM）的知识提炼到
    NeRF 中，使用 NeRF 指导的提炼算法[[142](#bib.bib142)]。这些方法展示了使用 NeRF 和扩散模型进行 3D 场景合成的潜力，预计在这一领域的进一步研究将带来更加激动人心的成果。
- en: c)Transformer-based NeRF. Building on the previous work of integrating generative
    adversarial networks (GANs) and neural radiance fields (NeRFs), some researchers
    have explored the possibility of using Transformer models and NeRFs to generate
    3D images that are consistent across different viewpoints. Wang et al. proposed
    a method that can handle complex scenes with dynamic objects and occlusions, and
    can generalize to unseen scenes without fine-tuning. The key idea is to use a
    transformer to learn a global latent representation of the scene, which is then
    used to condition a NeRF model that renders novel views [[143](#bib.bib143)].
    Similarly, Lin et al. proposed a method for novel view synthesis from a single
    unposed image using NeRF and vision transformer (ViT). The method leverages both
    global and local image features to form a 3D representation of the scene, which
    is then used to render novel views by a multi-layer perceptron (MLP) network [[144](#bib.bib144)].
    Finally, Liu et al. proposed a method for visual localization using a conditional
    NeRF model. The method can estimate the 6-DoF pose of a query image given a sparse
    set of reference images and their poses [[145](#bib.bib145)]. These methods demonstrate
    the potential of NeRFs and transformers in addressing challenging problems in
    computer vision.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: c) 基于 Transformer 的 NeRF。在将生成对抗网络（GANs）和神经辐射场（NeRFs）进行整合的前期工作基础上，一些研究人员探索了使用
    Transformer 模型和 NeRFs 来生成在不同视角下保持一致的 3D 图像的可能性。Wang 等人提出了一种方法，可以处理具有动态物体和遮挡的复杂场景，并且能够在不进行微调的情况下泛化到未见过的场景。关键思想是使用
    Transformer 学习场景的全局潜在表示，然后用这个表示来条件化一个 NeRF 模型，从而渲染新视角[[143](#bib.bib143)]。类似地，Lin
    等人提出了一种从单张未摆姿态图像生成新视角的 NeRF 和视觉 Transformer（ViT）方法。该方法利用全局和局部图像特征形成场景的 3D 表示，然后通过一个多层感知机（MLP）网络渲染新视角[[144](#bib.bib144)]。最后，Liu
    等人提出了一种使用条件 NeRF 模型进行视觉定位的方法。该方法可以根据一组稀疏的参考图像及其姿态来估计查询图像的 6-DoF 姿态[[145](#bib.bib145)]。这些方法展示了
    NeRFs 和 Transformers 在解决计算机视觉中挑战性问题方面的潜力。
- en: III-B Local Pose
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 本地姿态
- en: III-B1 GAN
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 GAN
- en: Liao et al. proposed a novel framework consisting of two components for learning
    generative models that can achieve this goal. The first component is a 3D generator
    that learns to reconstruct the 3D shape and appearance of an object from a single
    image, while the second component is a 2D generator that learns to render the
    3D object into a 2D image. This framework can generate high-quality images with
    controllable factors such as pose, shape, and appearance [[146](#bib.bib146)].Nguyen-Phuoc
    et al. proposed BlockGAN, a novel image generative model that can create realistic
    images of scenes composed of multiple objects. BlockGAN learns to generate 3D
    features for each object and combine them into a 3D scene representation. The
    model then renders the 3D scene into a 2D image, taking into account the occlusion
    and interaction between objects, such as shadows and lighting. BlockGAN can manipulate
    the pose and identity of each object independently while preserving image quality
    [[147](#bib.bib147)].Pan et al. proposed a novel framework that can reconstruct
    3D shapes from 2D image GANs without any supervision or prior knowledge. The method
    can generate realistic and diverse 3D shapes for various object categories, and
    the reconstructed shapes are consistent with the 2D images generated by the GANs.
    The recovered 3D shapes allow high-quality image editing such as relighting and
    object rotation [[148](#bib.bib148)].Tewari et al. proposed a novel 3D generative
    model that can learn to separate the geometry and appearance factors of objects
    from a dataset of monocular images. The model uses a non-rigid deformable scene
    formulation, where each object instance is represented by a deformed canonical
    3D volume. The model can also compute dense correspondences between images and
    embed real images into its latent space, enabling editing of real images [[149](#bib.bib149)].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Liao 等人提出了一个新颖的框架，该框架由两个组件组成，用于学习生成模型以实现这一目标。第一个组件是一个 3D 生成器，它学习从单张图像重建物体的 3D
    形状和外观，而第二个组件是一个 2D 生成器，它学习将 3D 物体渲染成 2D 图像。该框架可以生成高质量的图像，并具有可控的因素，如姿势、形状和外观 [[146](#bib.bib146)]。Nguyen-Phuoc
    等人提出了 BlockGAN，这是一种新颖的图像生成模型，可以创建由多个物体组成的场景的真实图像。BlockGAN 学习为每个物体生成 3D 特征，并将它们组合成
    3D 场景表示。该模型然后将 3D 场景渲染成 2D 图像，考虑物体之间的遮挡和互动，如阴影和光照。BlockGAN 可以独立操控每个物体的姿势和身份，同时保持图像质量
    [[147](#bib.bib147)]。Pan 等人提出了一个新颖的框架，该框架可以从 2D 图像 GANs 中重建 3D 形状，而无需任何监督或先验知识。该方法可以为各种物体类别生成真实且多样的
    3D 形状，且重建的形状与 GANs 生成的 2D 图像一致。恢复的 3D 形状允许高质量的图像编辑，如重新照明和物体旋转 [[148](#bib.bib148)]。Tewari
    等人提出了一个新颖的 3D 生成模型，该模型可以从单目图像的数据集中学习分离物体的几何和外观因素。该模型使用非刚性可变形场景公式，其中每个物体实例由一个变形的标准
    3D 体积表示。该模型还可以计算图像之间的密集对应关系，并将真实图像嵌入其潜在空间，从而实现对真实图像的编辑 [[149](#bib.bib149)]。
- en: III-B2 NeRF
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 NeRF
- en: Niemeyer and Geiger introduced GIRAFFE, a deep generative model that can synthesize
    realistic and controllable images of 3D scenes. The model represents scenes as
    compositional neural feature fields that encode the shape and appearance of individual
    objects as well as the background. The model can disentangle these factors from
    unstructured and unposed image collections without any additional supervision.
    With GIRAFFE, individual objects in the scene can be manipulated by translating,
    rotating, or changing their appearance, as well as changing the camera pose[[29](#bib.bib29)].Yang
    et al. proposed a neural scene rendering system called OC-Nerf that learns an
    object-compositional neural radiance field for editable scene rendering. OC-Nerf
    consists of a scene branch and an object branch, which encode the scene and object
    geometry and appearance, respectively. The object branch is conditioned on learnable
    object activation codes that enable object-level editing such as moving, adding,
    or rotating objects[[25](#bib.bib25)].Kobayashi et al. proposed a method to enable
    semantic editing of 3D scenes represented by neural radiance fields (NeRFs). The
    authors introduced distilled feature fields (DFFs), which are 3D feature descriptors
    learned by transferring the knowledge of pre-trained 2D image feature extractors
    such as CLIP-LSeg or DINO. DFFs allow users to query and select specific regions
    or objects in the 3D space using text, image patches, or point-and-click inputs.
    The selected regions can then be edited in various ways, such as rotation, translation,
    scaling, warping, colorization, or deletion[[150](#bib.bib150)].Zhang et al. introduced
    Nerflets, a new approach to represent 3D scenes from 2D images using local radiance
    fields. Unlike prior approaches that rely on global implicit functions, Nerflets
    partition the scene into a collection of local coordinate frames that encode the
    structure and appearance of the scene. This enables efficient rendering and editing
    of complex scenes with high fidelity and detail. Nerflets can manipulate the object’s
    orientation, position, and size, among other operations[[151](#bib.bib151)].Finally,
    Zheng et al. proposed EditableNeRF, a method that allows users to edit dynamic
    scenes modeled by neural radiance fields (NeRF) with key points. The method can
    handle topological changes and generate novel views from a single camera input.
    The key points are detected and optimized automatically by the network, and users
    can drag them to modify the scene. These approaches provide various means for
    3D scene synthesis and editing, including manipulating objects, changing camera
    pose, selecting and editing specific regions or objects, and handling topological
    changes[[152](#bib.bib152)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Niemeyer 和 Geiger 介绍了 GIRAFFE，这是一种深度生成模型，可以合成逼真且可控的 3D 场景图像。该模型将场景表示为组合神经特征场，这些特征场编码了个体物体的形状和外观以及背景。该模型能够在没有任何额外监督的情况下，从无结构和非姿态图像集合中解开这些因素。使用
    GIRAFFE，可以通过平移、旋转或更改外观来操控场景中的个体物体，还可以更改相机姿态[[29](#bib.bib29)]。Yang 等人提出了一种名为 OC-Nerf
    的神经场景渲染系统，该系统学习了一个对象组合的神经辐射场，用于可编辑的场景渲染。OC-Nerf 由一个场景分支和一个对象分支组成，分别编码场景和对象的几何形状和外观。对象分支基于可学习的对象激活代码，从而实现对象级别的编辑，例如移动、添加或旋转对象[[25](#bib.bib25)]。Kobayashi
    等人提出了一种方法，使得对由神经辐射场（NeRFs）表示的 3D 场景进行语义编辑成为可能。作者引入了精炼特征场（DFFs），这些是通过迁移预训练的 2D
    图像特征提取器（如 CLIP-LSeg 或 DINO）的知识来学习的 3D 特征描述符。DFFs 允许用户使用文本、图像补丁或点击输入在 3D 空间中查询和选择特定区域或物体。选定的区域随后可以以各种方式进行编辑，如旋转、平移、缩放、变形、上色或删除[[150](#bib.bib150)]。Zhang
    等人介绍了 Nerflets，这是一种新的方法，用于从 2D 图像中表示 3D 场景，使用局部辐射场。与依赖于全局隐式函数的先前方法不同，Nerflets
    将场景分割为一组局部坐标系，这些坐标系编码了场景的结构和外观。这使得高保真度和细节的复杂场景的渲染和编辑变得高效。Nerflets 可以操控物体的方向、位置和大小等[[151](#bib.bib151)]。最后，Zheng
    等人提出了 EditableNeRF，这是一种允许用户用关键点编辑由神经辐射场（NeRF）建模的动态场景的方法。该方法可以处理拓扑变化，并从单一相机输入生成新视角。关键点由网络自动检测和优化，用户可以拖动这些关键点以修改场景。这些方法提供了多种
    3D 场景合成和编辑手段，包括操控物体、更改相机姿态、选择和编辑特定区域或物体，以及处理拓扑变化[[152](#bib.bib152)]。
- en: IV Structure Manipulation
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 结构操控
- en: IV-A Global Structure
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 全局结构
- en: IV-A1 Editting point cloud
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 点云编辑
- en: A depth map is a representation of the distance between a scene and a reference
    point, such as a camera. It can be used to create realistic effects such as depth
    of field, occlusion, and parallax[[153](#bib.bib153)]. Chen et al. proposed a
    novel method called SPIDR for representing and manipulating 3D objects using neural
    point fields (NPFs) and signed distance functions (SDFs) [[154](#bib.bib154)].
    The method combines explicit point cloud and implicit neural representations to
    enable high-quality mesh and surface reconstruction for object deformation and
    lighting estimation. With the trained SPIDR model, various geometric edits can
    be applied to the point cloud representation, which can be used for image editing.Zhang
    et al. introduced a new method for rendering point clouds with frequency modulation,
    which enables easy editing of shape and appearance [[155](#bib.bib155)]. The method
    converts point clouds into a set of frequency-modulated signals that can be rendered
    efficiently using Fourier analysis. The signals can also be manipulated in the
    frequency domain to achieve various editing effects, such as deformation, smoothing,
    sharpening, and color adjustment.Chen et al. also proposed NeuralEditor, a novel
    method for editing neural radiance fields (NeRFs) for shape editing tasks [[156](#bib.bib156)].
    The method uses point clouds as the underlying structure to construct NeRFs and
    renders them with a new scheme based on K-D tree-guided voxels. NeuralEditor can
    perform shape deformation and scene morphing by mapping points between point clouds.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图是一种表示场景与参考点（如摄像机）之间距离的图像。它可以用来创建真实感效果，如景深、遮挡和视差[[153](#bib.bib153)]。陈等人提出了一种新颖的方法，称为SPIDR，利用神经点场（NPFs）和有符号距离函数（SDFs）来表示和操作3D对象[[154](#bib.bib154)]。该方法结合了显式点云和隐式神经表示，以实现高质量的网格和表面重建，用于对象变形和光照估计。借助训练好的SPIDR模型，可以对点云表示进行各种几何编辑，这些编辑可用于图像编辑。张等人介绍了一种新的点云渲染方法，采用频率调制技术，便于对形状和外观进行编辑[[155](#bib.bib155)]。该方法将点云转换为一组频率调制信号，通过傅里叶分析可以高效渲染。这些信号还可以在频域中进行操作，以实现各种编辑效果，如变形、平滑、锐化和颜色调整。陈等人还提出了NeuralEditor，这是一种用于形状编辑任务的神经辐射场（NeRFs）编辑新方法[[156](#bib.bib156)]。该方法利用点云作为基础结构来构建NeRFs，并通过基于K-D树指导的体素方案进行渲染。NeuralEditor可以通过在点云之间映射点来实现形状变形和场景变换。
- en: IV-A2 Editting depth map
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 编辑深度图
- en: Chen et al. introduced the Visual Object Networks (VON) framework, which enables
    the disentangled learning of 3D object representations from 2D images. This framework
    comprises three modules, namely a shape generator, an appearance generator, and
    a rendering network. By manipulating the generators, VON can perform a range of
    tasks, including shape manipulation, appearance transfer, and novel view synthesis
    [[157](#bib.bib157)].Mirzaei et al. proposed a reference-guided controllable inpainting
    method for neural radiance fields (NeRFs), which allows for the synthesis of novel
    views of a scene with missing regions. The method employs a reference image to
    guide the inpainting process, and a user interface that enables the user to adjust
    the degree of blending between the reference and the original NeRF [[158](#bib.bib158)].Yin
    et al. introduced OR-NeRF, a novel pipeline that can remove objects from 3D scenes
    using point or text prompts on a single view. This pipeline leverages a points
    projection strategy, a 2D segmentation model, 2D inpainting methods, and depth
    supervision and perceptual loss to achieve better editing quality and efficiency
    than previous works [[159](#bib.bib159)]. Chen et al. proposed a visual comfort
    aware-reinforcement learning (VCARL) method for depth adjustment of stereoscopic
    3D images. This method aims to improve the visual quality and comfort of 3D images
    by learning a depth adjustment policy from human feedback [[160](#bib.bib160)].
    These advancements offer various means of manipulating objects, adjusting depth,
    and generating novel views, ultimately enhancing the quality and realism of 3D
    scene synthesis and editing.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人介绍了Visual Object Networks（VON）框架，它使得从2D图像中解耦学习3D物体表示成为可能。该框架包括三个模块，即形状生成器、外观生成器和渲染网络。通过操作生成器，VON可以执行一系列任务，包括形状操控、外观转移和新视图合成
    [[157](#bib.bib157)]。Mirzaei等人提出了一种参考指导的可控修复方法，用于神经辐射场（NeRFs），该方法允许合成具有缺失区域的场景的新视图。该方法使用参考图像来指导修复过程，并提供一个用户界面，使用户可以调整参考图像与原始NeRF之间的混合程度
    [[158](#bib.bib158)]。Yin等人介绍了OR-NeRF，这是一种新颖的管道，可以使用单视图中的点或文本提示从3D场景中移除对象。该管道利用点投影策略、2D分割模型、2D修复方法以及深度监督和感知损失，实现了比以往工作更好的编辑质量和效率
    [[159](#bib.bib159)]。陈等人提出了一种视觉舒适感增强的强化学习（VCARL）方法，用于立体3D图像的深度调整。该方法通过从人工反馈中学习深度调整策略，旨在提高3D图像的视觉质量和舒适感
    [[160](#bib.bib160)]。这些进展提供了多种操控物体、调整深度和生成新视图的手段，*最终*提高了3D场景合成和编辑的质量与真实感。
- en: IV-B Local Structure
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 本地结构
- en: IV-B1 GAN
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 GAN
- en: In recent years, there have been significant advancements in the field of 3D
    scene inpainting and editing using generative adversarial networks (GANs). Jheng
    et al. proposed a dual-stream GAN for free-form 3D scene inpainting. The network
    comprises two streams, namely a depth stream and a color stream, which are jointly
    trained to inpaint the missing regions of a 3D scene. The depth stream predicts
    the depth map of the scene, while the color stream synthesizes the color image.
    This approach enables the removal of objects using existing 3D editing tools [[161](#bib.bib161)].Another
    recent development in GAN training is the introduction of LinkGAN, a regularizer
    proposed by Zhu et al. that links some latent axes to image regions or semantic
    categories. By resampling partial latent codes, this approach enables local control
    of GAN generation [[30](#bib.bib30)].Wang et al. proposed a novel method for synthesizing
    realistic images of indoor scenes with explicit camera pose control and object-level
    editing capabilities. This method builds on BlobGAN, a 2D GAN that disentangles
    individual objects in the scene using 2D blobs as latent codes. To extend this
    approach to 3D scenes, the authors introduced 3D blobs, which capture the 3D nature
    of objects and allow for flexible manipulation of their location and appearance
    [[162](#bib.bib162)]. These recent advancements in GAN-based 3D scene inpainting
    and editing have the potential to significantly improve the quality and realism
    of synthesized scenes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，使用生成对抗网络（GAN）在3D场景修补和编辑领域取得了显著进展。Jheng等人提出了一种用于自由形式3D场景修补的双流GAN。该网络由深度流和颜色流两个部分组成，这两个部分共同训练，以修补3D场景的缺失区域。深度流预测场景的深度图，而颜色流合成颜色图像。这种方法使得通过现有的3D编辑工具可以移除对象[[161](#bib.bib161)]。另一个最近的GAN训练发展是LinkGAN的引入，这是一种由Zhu等人提出的正则化器，它将一些潜在轴与图像区域或语义类别联系起来。通过重新采样部分潜在编码，这种方法使GAN生成的局部控制成为可能[[30](#bib.bib30)]。Wang等人提出了一种新颖的方法，用于合成具有明确相机姿态控制和对象级编辑能力的室内场景的逼真图像。该方法基于BlobGAN，这是一个2D
    GAN，它使用2D blob作为潜在编码来解开场景中的个体对象。为了将这种方法扩展到3D场景，作者引入了3D blob，它们捕捉了对象的3D特性，并允许灵活地操控其位置和外观[[162](#bib.bib162)]。这些基于GAN的3D场景修补和编辑的最新进展具有显著提升合成场景质量和真实感的潜力。
- en: IV-B2 NeRF
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 NeRF
- en: Liu et al. [[104](#bib.bib104)] introduced Neural Sparse Voxel Fields (NSVF),
    which combines neural implicit functions with sparse voxel octrees to enable high-quality
    novel view synthesis from a sparse set of input images, without requiring explicit
    geometry reconstruction or meshing. Gu et al. [[163](#bib.bib163)] introduced
    StyleNeRF, a method that enables camera pose manipulation for synthesizing high-resolution
    images with strong multi-view coherence and photorealism. Wang et al. [[164](#bib.bib164)]
    introduced CLIP-NeRF, a method for manipulating 3D objects represented by neural
    radiance fields (NeRF) using text or image inputs. Kania et al. [[165](#bib.bib165)]
    proposed a novel method for manipulating neural 3D representations of scenes beyond
    novel view rendering by allowing the user to specify which part of the scene they
    want to control with mask annotations in the training images. Lazova et al. [[166](#bib.bib166)]
    proposed a novel method for performing flexible, 3D-aware image content manipulation
    while enabling high-quality novel view synthesis by combining scene-specific feature
    volumes with a general neural rendering network. Yuan et al. [[167](#bib.bib167)]
    proposed a method for user-controlled shape deformation of scenes represented
    by implicit neural rendering, especially Neural Radiance Field (NeRF). Sun et al.
    [[168](#bib.bib168)] proposed NeRFEditor, a learning framework for 3D scene editing
    that uses a pre-trained StyleGAN model and a NeRF model to generate stylized images
    from a 360-degree video input. Wang et al. [[169](#bib.bib169)] proposed a novel
    method for image synthesis of topology-varying objects using generative deformable
    radiance fields (GDRFs). Tertikas et al. [[170](#bib.bib170)] proposed PartNeRF,
    a novel part-aware generative model for editable 3D shape synthesis that does
    not require any explicit 3D supervision. Bao et al. [[171](#bib.bib171)] proposed
    SINE, a novel approach for editing a neural radiance field (NeRF) with a single
    image or text prompts. Cohen-Bar et al. [[172](#bib.bib172)] proposed a novel
    framework for synthesizing and manipulating 3D scenes from text prompts and object
    proxies. Finally, Mirzaei et al. [[173](#bib.bib173)] proposed a novel method
    for reconstructing 3D scenes from multiview images by leveraging neural radiance
    fields (NeRF) to model the geometry and appearance of the scene, and introducing
    a segmentation network and a perceptual inpainting network to handle occlusions
    and missing regions. These methods represent significant progress towards the
    goal of enabling high-quality, user-driven 3D scene synthesis and editing.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人 [[104](#bib.bib104)] 引入了神经稀疏体素场（NSVF），该方法将神经隐式函数与稀疏体素八叉树相结合，以实现从稀疏输入图像集中生成高质量的新视图合成，无需显式几何重建或网格生成。谷等人
    [[163](#bib.bib163)] 提出了StyleNeRF，这是一种允许相机姿态操作的方法，用于合成具有强多视图一致性和光真实感的高分辨率图像。王等人
    [[164](#bib.bib164)] 提出了CLIP-NeRF，这是一种使用文本或图像输入操作由神经辐射场（NeRF）表示的3D对象的方法。卡尼亚等人
    [[165](#bib.bib165)] 提出了一种新方法，通过允许用户使用训练图像中的掩模注释指定他们想要控制的场景部分，从而在新视图渲染之外操控神经3D场景表示。拉佐娃等人
    [[166](#bib.bib166)] 提出了一种新方法，通过将场景特定的特征体积与通用神经渲染网络相结合，实现灵活的、具有3D感知的图像内容操控，同时实现高质量的新视图合成。袁等人
    [[167](#bib.bib167)] 提出了一种用户控制的场景形状变形方法，特别是神经辐射场（NeRF）表示的场景。孙等人 [[168](#bib.bib168)]
    提出了NeRFEditor，这是一种用于3D场景编辑的学习框架，使用预训练的StyleGAN模型和NeRF模型从360度视频输入中生成风格化图像。王等人 [[169](#bib.bib169)]
    提出了一种用于拓扑变化对象图像合成的新方法，使用生成变形辐射场（GDRFs）。特提卡斯等人 [[170](#bib.bib170)] 提出了PartNeRF，这是一种新型部件感知生成模型，用于可编辑的3D形状合成，无需任何显式的3D监督。包等人
    [[171](#bib.bib171)] 提出了SINE，这是一种使用单张图像或文本提示编辑神经辐射场（NeRF）的新方法。科恩-巴尔等人 [[172](#bib.bib172)]
    提出了一个新框架，用于从文本提示和对象代理合成和操控3D场景。最后，米尔扎埃伊等人 [[173](#bib.bib173)] 提出了一种通过利用神经辐射场（NeRF）建模场景的几何和外观，并引入分割网络和感知修复网络来处理遮挡和缺失区域的多视图图像重建的新方法。这些方法代表了朝着实现高质量、用户驱动的3D场景合成和编辑目标的重要进展。
- en: IV-B3 Diffusion model
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 扩散模型
- en: Avrahami et al. [[174](#bib.bib174)] introduced a method for local image editing
    based on natural language descriptions and region-of-interest masks. The method
    uses a pretrained language-image model (CLIP) and a denoising diffusion probabilistic
    model (DDPM) to produce realistic outcomes that conform to the text input. It
    can perform various editing tasks, such as object addition, removal, replacement,
    or modification, background replacement, and image extrapolation.Nichol et al.
    [[175](#bib.bib175)] proposed GLIDE, a diffusion-based model for text-conditional
    image synthesis and editing. This method uses a guidance technique to trade off
    diversity for fidelity and produces photorealistic images that match the text
    prompts.Couairon et al. [[176](#bib.bib176)] proposed DiffEdit, a method that
    uses text-conditioned diffusion models to edit images based on text queries. It
    can automatically generate a mask that highlights the regions of the image that
    need to be changed according to the text query. It also uses latent inference
    to preserve the content in those regions. DiffEdit can produce realistic and diverse
    semantic image edits for various text prompts and image sources.Sella et al. [[177](#bib.bib177)]
    proposed Vox-E, a novel framework that uses latent diffusion models to edit 3D
    objects based on text prompts. It takes 2D images of a 3D object as input and
    learns a voxel grid representation of it. It then optimizes a score distillation
    loss to align the voxel grid with the text prompt while regularizing it in 3D
    space to preserve the global structure of the original object. Vox-E can create
    diverse and realistic edits.Haque et al. [[178](#bib.bib178)] proposed a novel
    method for editing 3D scenes with natural language instructions. The method leverages
    a neural radiance field (NeRF) representation of the scene and a transformer-based
    model that can parse the instructions and modify the NeRF accordingly. The method
    can perform various editing tasks, such as changing the color, shape, position,
    and orientation of objects, as well as adding and removing objects, with high
    fidelity and realism.Lin et al. [[179](#bib.bib179)] proposed CompoNeRF, a novel
    method for text-guided multi-object compositional NeRF with editable 3D scene
    layout. CompoNeRF can synthesize photorealistic images of complex scenes from
    natural language descriptions and user-specified camera poses. It can also edit
    the 3D layout of the scene by manipulating the objects’ positions, orientations,
    and scales. These methods have shown promising results in advancing the field
    of image and 3D object editing using natural language descriptions, and they have
    the potential to be applied in various applications.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Avrahami 等人 [[174](#bib.bib174)] 提出了一种基于自然语言描述和感兴趣区域掩膜的本地图像编辑方法。该方法使用预训练的语言-图像模型（CLIP）和去噪扩散概率模型（DDPM），生成符合文本输入的真实结果。它可以执行各种编辑任务，如对象添加、移除、替换或修改、背景替换以及图像外推。Nichol
    等人 [[175](#bib.bib175)] 提出了 GLIDE，这是一种基于扩散的文本条件图像合成和编辑模型。该方法使用指导技术在多样性和保真度之间进行权衡，生成与文本提示匹配的照片级真实图像。Couairon
    等人 [[176](#bib.bib176)] 提出了 DiffEdit，这是一种使用文本条件扩散模型根据文本查询编辑图像的方法。它可以自动生成一个掩膜，突出显示需要根据文本查询更改的图像区域。它还使用潜在推断来保留这些区域的内容。DiffEdit
    可以为各种文本提示和图像来源生成真实且多样的语义图像编辑。Sella 等人 [[177](#bib.bib177)] 提出了 Vox-E，这是一种使用潜在扩散模型根据文本提示编辑
    3D 对象的新框架。它将 3D 对象的 2D 图像作为输入，学习其体素网格表示。然后，它优化一个分数蒸馏损失，以使体素网格与文本提示对齐，同时在 3D 空间中对其进行正则化，以保留原始对象的整体结构。Vox-E
    可以创建多样且真实的编辑。Haque 等人 [[178](#bib.bib178)] 提出了用于自然语言指令编辑 3D 场景的新方法。该方法利用场景的神经辐射场（NeRF）表示和一种基于变换器的模型，该模型可以解析指令并相应地修改
    NeRF。该方法可以执行各种编辑任务，如更改对象的颜色、形状、位置和方向，以及添加和移除对象，具有高保真度和现实感。Lin 等人 [[179](#bib.bib179)]
    提出了 CompoNeRF，这是一种新颖的文本引导的多对象组合 NeRF 方法，具有可编辑的 3D 场景布局。CompoNeRF 可以从自然语言描述和用户指定的相机姿势合成复杂场景的照片级真实图像。它还可以通过操纵对象的位置、方向和尺度来编辑场景的
    3D 布局。这些方法在利用自然语言描述推动图像和 3D 对象编辑领域方面显示了有前景的结果，并且具有在各种应用中应用的潜力。
- en: V Illumination Manipulation
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 照明操作
- en: controlable image generation refers to the use of technology to generate images
    and to constrain and adjust the generation process so that the generated images
    meet specific requirements.By guiding external conditions or manipulating and
    adjusting the code, it is possible to trim a certain area or attribute of the
    image while leaving other areas or attributes unchanged.To solve the low-level
    image generation problem, we analyze the image generation for different conditions,
    lighting being one of them, and summarize the algorithms for each solution under
    different lighting conditions.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 可控图像生成指的是利用技术生成图像，并限制和调整生成过程，以使生成的图像符合特定要求。通过引导外部条件或操控和调整代码，可以修整图像的某个区域或属性，同时保持其他区域或属性不变。为了解决低层次的图像生成问题，我们分析了在不同条件下的图像生成，其中光照是其中之一，并总结了在不同光照条件下的算法解决方案。
- en: Inverse rendering.Currently,neural rendering is applied to scene restruction.One
    approach is to capture photometric appearance variations in in-the-wild data,
    decomposing the scene into image-dependent shared components[[180](#bib.bib180)].
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向渲染。目前，神经渲染被应用于场景重建。一种方法是捕捉在自然数据中的光度外观变化，将场景分解为图像依赖的共享组件[[180](#bib.bib180)]。
- en: Another very important type of rendering is inverse rendering.The inverse rendering
    of objects under completely unknown capture conditions is a fundamental challenge
    in computer vision and graphics.This challenge is especially acute when the input
    image is captured in a complex and changing environment.Without using the nerf
    method,Mark Boss et al. proposed a join optimization framework to estimate the
    shape,BRDF,per-image camera pose and illumination[[181](#bib.bib181)].
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的渲染类型是逆向渲染。在完全未知的捕捉条件下进行物体的逆向渲染是计算机视觉和图形学中的一个基本挑战。当输入图像在复杂且变化的环境中捕捉时，这一挑战尤其严峻。在不使用
    nerf 方法的情况下，Mark Boss 等人提出了一种联合优化框架来估计形状、BRDF、每图像相机姿态和光照[[181](#bib.bib181)]。
- en: Changwoon Choi et al. proposed IBL-NeRF also based on rendering .This method’s
    inverse rendering extends the original NeRF formulation to capture the spatial
    variation of lighting within the scene volume, in addition to surface properties.
    Specifically, the scenes of diverse materials are decomposed into intrinsic components
    for image-based ren dering, namely, albedo, roughness, surface normal, irradiance,
    and prefiltered radiance.All of the components are inferred as neural images from
    MLP, and model large-scale general scenes[[182](#bib.bib182)].
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Changwoon Choi 等人提出的 IBL-NeRF 也基于渲染。这种方法的逆向渲染将原始的 NeRF 公式扩展到捕捉场景体积内的空间光照变化，除了表面属性外。具体来说，将不同材料的场景分解为图像基础渲染的内在组件，即反射率、粗糙度、表面法线、辐射度和预过滤辐射。所有这些组件都作为来自
    MLP 的神经图像进行推断，并建模大规模通用场景[[182](#bib.bib182)]。
- en: However, NeRF-based methods encode shape,reflectance and illumination implicitly
    and this makes it challenging for users to manipulate these properties in the
    rendered images explicitly.So a new hybrid SDF-based 3D neural representation
    is generated, capable of rendering scene deformations and lighting more accurately.This
    neural representation also adds a new SDF regularization.The disadvantage of this
    approach is that it sacrifices rendering quality. In reverse rendering, high render
    quality is often at odds with accurate lighting decomposition, as shadows and
    lighting can easily be misinterpreted as textures. Therefore, rendering quality
    still requires a concerted effort of surface reconstruction and reverse rendering[[154](#bib.bib154)].
    While dynamic Neural Radiation Field (NeRF) is a powerful algorithm capable of
    rendering photo-realistic novel view images from a monocular RGB video of a dynamic
    scene. But dynamic NeRF does not model the change of the reflected color during
    the warping.This is one of its drawbacks.To address this problem in rendering,
    Zhiwen Yan et al. allowed specularly reflective surfaces of different poses to
    maintain different reflective colors when mapped to the common canonical space
    by reformulating the neural radiation field function as conditional on the position
    and orientation of the surface in the observation space.This method more accurately
    reconstructs and renders dynamic specular scenes[[183](#bib.bib183)].
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于 NeRF 的方法隐式地编码了形状、反射率和光照，这使得用户在渲染图像中显式操控这些属性变得具有挑战性。因此，生成了一种新的混合 SDF 基于
    3D 神经表示的方法，能够更准确地渲染场景变形和光照。该神经表示还增加了新的 SDF 正则化。这种方法的缺点是牺牲了渲染质量。在反向渲染中，高渲染质量通常与准确的光照分解相矛盾，因为阴影和光照很容易被误解为纹理。因此，渲染质量仍需表面重建和反向渲染的共同努力[[154](#bib.bib154)]。动态神经辐射场（NeRF）是一个强大的算法，能够从动态场景的单目
    RGB 视频中渲染出照片级真实感的新视图图像。但动态 NeRF 并不建模在变形过程中反射颜色的变化。这是其缺点之一。为了解决这一问题，Zhiwen Yan
    等人通过将神经辐射场函数重新公式化为依赖于观测空间中表面的位置信息和方向来解决渲染问题，使不同姿态的镜面反射表面在映射到公共规范空间时保持不同的反射颜色。这种方法更准确地重建和渲染动态镜面场景[[183](#bib.bib183)]。
- en: 'The inverse rendering objective function of this method is as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的逆向渲染目标函数如下：
- en: '|  | $\mathcal{L}=\mathcal{L}_{\text{render }}+\mathcal{L}_{\text{pref }}+\mathcal{L}_{\text{prior
    }}+\lambda_{I,\text{ reg }}\mathcal{L}_{I,\text{ reg }}$ |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{L}_{\text{render }}+\mathcal{L}_{\text{pref }}+\mathcal{L}_{\text{prior
    }}+\lambda_{I,\text{ reg }}\mathcal{L}_{I,\text{ reg }}$ |  |'
- en: $\mathcal{L}_{\text{render }}$ and $\mathcal{L}_{\text{pref }}$ are rendering
    losses to match the rendered images with the input images.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L}_{\text{render }}$ 和 $\mathcal{L}_{\text{pref }}$ 是渲染损失，用于将渲染图像与输入图像匹配。
- en: Next, we will explain each of these parameters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解释这些参数。
- en: '|  | $\mathcal{L}_{\text{render }}=\left\&#124;L_{o}(r)-\hat{L}_{o}(r)\right\&#124;_{2}^{2},$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{render }}=\left\|L_{o}(r)-\hat{L}_{o}(r)\right\|_{2}^{2},$
    |  |'
- en: This is for each pixel of the camera light. $r$ represents a single piexl.where
    $L_{o}$ is our nated radiance and $\hat{L}_{o}$ is ground truth radiance.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相机光的每个像素。 $r$ 代表单个像素，其中 $L_{o}$ 是我们的原始辐射，而 $\hat{L}_{o}$ 是真实辐射。
- en: '|  | $\mathcal{L}_{\text{pref}}=\sum_{j}\left\&#124;L_{\text{pref }}^{j}(r)-L_{\mathrm{G}}^{j}(r)\right\&#124;_{2}^{2}.$
    |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{pref}}=\sum_{j}\left\|L_{\text{pref }}^{j}(r)-L_{\mathrm{G}}^{j}(r)\right\|_{2}^{2}.$
    |  |'
- en: This is the rendering loss of pre-filtered radiation.$L_{\text{pref }}^{j}(r)$
    is inferred prefiltered radiance of $j^{(}th)$ level and $L_{\mathrm{G}}^{j}(r)$
    is the radiance convolved with jth level Gaussian convolution,where $L_{\mathrm{G}}^{0}$
    = L.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预过滤辐射的渲染损失。$L_{\text{pref }}^{j}(r)$ 是第 $j^{(}th)$ 层的推断预过滤辐射，而 $L_{\mathrm{G}}^{j}(r)$
    是与第 $j$ 层高斯卷积卷积后的辐射，其中 $L_{\mathrm{G}}^{0}$ = L。
- en: '|  | $\mathcal{L}_{\text{prior }}=\&#124;a(r)-\hat{a}(r)\&#124;_{2}^{2}.$ |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{prior }}=\|a(r)-\hat{a}(r)\|_{2}^{2}.$ |  |'
- en: The equation encourages our inferred albedo $a$ to match the pseudo albedo.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程鼓励我们推断的反射率 $a$ 与伪反射率匹配。
- en: '|  | $\mathcal{L}_{I,\text{ reg }}=\&#124;I(r)-\mathbb{E}[\hat{I}]\&#124;_{2}^{2},$
    |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{I,\text{ reg }}=\|I(r)-\mathbb{E}[\hat{I}]\|_{2}^{2},$ |  |'
- en: This is the irradiance regularization loss,where $\mathbb{E}[\hat{I}]$ is the
    mean of irradiance (shading) values in training set images.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这是辐射正则化损失，其中 $\mathbb{E}[\hat{I}]$ 是训练集中图像的辐射（阴影）值的均值。
- en: V-A Global Illumination
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 全局光照
- en: The absence of ideal light and the fact that the studied objects are in an unfavorable
    environment such as deflection, movement, darkness, and high interference can
    lead to under-illuminated, single irradiated light source, and complex illumination
    of the acquired images, all of which can degrade the performance of the final
    image generation.Next, we will review the various ways to deal with these aspects.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 理想光的缺失以及所研究对象处于不利环境（如偏转、运动、黑暗和高干扰）可能导致光照不足、单一照射光源以及获取图像的复杂照明，这些都可能降低最终图像生成的性能。接下来，我们将回顾应对这些方面的各种方法。
- en: V-A1 Brightness level
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 亮度水平
- en: The use of illumination normalization generative adversarial network - IN-GAN
    can be well generalized to images with less illumination variations.The method
    combines deep convolutional neural networks and generative adversarial networks
    to normalize the illumination of color or grayscale face images, then train feature
    extractors and classifiers, and process both frontal and non-frontal face images
    illumination.The method can be extended to other areas, not only for face image
    generation.However, it cannot preserve more texture details and has some limitations.Meanwhile,
    the training model is conducted with well-controlled illumination variations,
    which can deal with poorly controlled illumination variation to a certain extent,
    but there are still limitations to the study of other features and geometric structures
    in realistic and complex environments, etc.It can be further investigated whether
    the model can work better if the model is trained under complex lighting changes.[[184](#bib.bib184)].
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用光照归一化生成对抗网络 - IN-GAN可以很好地推广到光照变化较少的图像。这种方法结合了深度卷积神经网络和生成对抗网络来归一化彩色或灰度面部图像的光照，然后训练特征提取器和分类器，并处理正面和非正面面部图像的光照。该方法可以扩展到其他领域，不仅限于面部图像生成。然而，它不能保留更多的纹理细节，并且存在一些限制。同时，训练模型是在控制良好的光照变化下进行的，这可以在一定程度上处理光照变化控制不佳的情况，但在现实和复杂环境中的其他特征和几何结构研究上仍然存在局限性。是否可以在复杂光照变化下训练模型以获得更好的效果，还需进一步探讨[[184](#bib.bib184)]。
- en: When the data set is insufficient, an unsupervised approach can be used for
    this.For example, for low-light scenes, the unsupervised Aleth-NeRF method is
    used to learn directly from dark images.The algorithm is mainly a multi-view synthesis
    method that takes a low-light scene as input and renders a normally illuminated
    scene.However, a model needs to be trained specifically for different scenes,
    and also does not handle non-uniform lighting conditions well[[185](#bib.bib185)].
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集不足时，可以使用无监督的方法。例如，对于低光照场景，使用无监督的Aleth-NeRF方法直接从暗图像中学习。该算法主要是一种多视图合成方法，它以低光照场景为输入，并渲染出正常光照的场景。然而，模型需要专门为不同场景进行训练，并且对于非均匀光照条件的处理效果也不好[[185](#bib.bib185)]。
- en: Also as far as the results are concerned, images taken in low-light scenes are
    affected by distracting factors such as blur and noise.For this type of problem,
    a hybrid architecture based on Retinex theory and Generative Adversarial Network
    (GAN) can be used to deal with it.For image vision tasks in the dark or under
    low light conditions, the image is first decomposed into a light image and a reflection
    image, and then the enhancement part is used to generate a high quality clear
    image, starting from minimizing the effect of blurring or noise generation.The
    method introduces Structural Similarity loss to avoid the side effect of blur.But
    real-life eligible low level and high level images may not be easily acquired
    and have the shortage of input.Also to maximize the performance of the algorithm,
    a sufficient size of data set is required.The data obtained after training also
    has the problem of real-time, which is not enough to meet real-life needs.In general,
    the algorithm is only from the perspective of solving image blurring and noise,
    making the impact of these two minimal, other aspects of the problem still exists
    more, need to further optimize the network structure.[[186](#bib.bib186)]This
    class of problems can also be explored by exploring multiple diffusion spaces
    to estimate the light component, which is used as bright pixels to enhance the
    shimmering image based on the maximum diffusion value.Generates high-fidelity
    images without significant distortion, minimizing the problem of noise amplification[[187](#bib.bib187)].Later,
    the conditional diffusion implicit model is utilized in DiFaReli’s method (DDIM)
    to decode the coding of decomposed light.Puntawat Ponglertnapakorn et al. proposed
    a novel conditioning technique that eases the modeling of the complex interaction
    between light and geometry by using a rendered shading reference to spatially
    modulate the DDIM.This method allows for single-view face re-illumination in the
    wild.However, this method has limitations in eliminating shadows cast by external
    objects and is susceptible to image ambiguity[[188](#bib.bib188)].
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 就结果而言，低光照场景拍摄的图像受到了模糊和噪声等干扰因素的影响。针对这种问题，可以使用基于 Retinex 理论和生成对抗网络（GAN）的混合架构来处理。对于黑暗或低光照条件下的图像视觉任务，首先将图像分解为光照图像和反射图像，然后使用增强部分生成高质量清晰图像，从而最小化模糊或噪声生成的影响。该方法引入了结构相似性损失以避免模糊的副作用。但实际可用的低级和高级图像可能不易获得，并且输入数据存在不足。为了最大限度地提高算法性能，还需要足够大小的数据集。训练后获得的数据还存在实时性问题，不能满足实际需求。总体而言，该算法仅从解决图像模糊和噪声的角度出发，将这两者的影响降到最低，其他方面的问题仍然存在，需要进一步优化网络结构。[[186](#bib.bib186)]这类问题也可以通过探索多个扩散空间来估计光照成分，从而作为亮度像素来增强基于最大扩散值的闪烁图像。生成高保真图像而无显著失真，最小化噪声放大问题[[187](#bib.bib187)]。后来，在
    DiFaReli 的方法（DDIM）中利用了条件扩散隐式模型来解码分解的光照编码。Puntawat Ponglertnapakorn 等人提出了一种新颖的条件技术，通过使用渲染阴影参考来在空间上调制
    DDIM，从而简化了光照与几何之间复杂交互的建模。这种方法允许在野外进行单视角面部重新照明。然而，该方法在消除外部物体投射的阴影方面存在局限性，并且容易受到图像模糊的影响[[188](#bib.bib188)]。
- en: 'In summary,the full objectives of this method are as followed:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，该方法的完整目标如下：
- en: '|  | $\displaystyle L(G,D)=$ | $\displaystyle L_{\text{adversarial }}(G,D)+\lambda_{1}\times
    L_{\text{content }}(\mathrm{G})+\lambda_{2}$ |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(G,D)=$ | $\displaystyle L_{\text{对抗损失}}(G,D)+\lambda_{1}\times
    L_{\text{内容损失}}(\mathrm{G})+\lambda_{2}$ |  |'
- en: '|  |  | $\displaystyle\times L_{l1}(\mathrm{G})$ |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times L_{l1}(\mathrm{G})$ |  |'
- en: where $\lambda_{1},\lambda_{2}$ are weight parameters respectively.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1},\lambda_{2}$ 分别是权重参数。
- en: '$L_{\text{adversarial }}$, $L_{\text{content }}$ and $L_{l1}$ are as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: $L_{\text{对抗损失}}$, $L_{\text{内容损失}}$ 和 $L_{l1}$ 如下：
- en: '|  |  | $\displaystyle L_{\text{adversarial }}(G,D)=E_{x}[\log D(x)]+E_{G(x)}[\log(1-D(G(x)))]$
    |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle L_{\text{对抗损失}}(G,D)=E_{x}[\log D(x)]+E_{G(x)}[\log(1-D(G(x)))]$
    |  |'
- en: '|  |  | $\displaystyle L_{\text{content }}(G)=\&#124;F(y)-F(G(x))\&#124;_{1}$
    |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle L_{\text{内容损失}}(G)=\|F(y)-F(G(x))\|_{1}$ |  |'
- en: '|  |  | $\displaystyle L_{l1}(G)=\&#124;y-G(x)\&#124;_{1}$ |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle L_{l1}(G)=\|y-G(x)\|_{1}$ |  |'
- en: where $\mathrm{x}$ denotes input image, whereas $\mathrm{y}$ is the target image,
    $\mathrm{F}$ means feature extractor.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathrm{x}$ 表示输入图像，而 $\mathrm{y}$ 是目标图像，$\mathrm{F}$ 表示特征提取器。
- en: V-A2 Light source movement
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 光源运动
- en: A method of generating scenes with a sense of reality from captured object images
    can be used when the light is moving.On the basis of neural radiation fields (NeRFs),
    the bulk density of the scene and the radiance of the directional emission are
    simulated.A representation of each object light transmission is implicitly simulated
    using illumination and view-related neural networks.This approach can cope with
    the problem of light movement without retraining the model[[189](#bib.bib189)].
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当光线移动时，可以使用从捕获的物体图像生成具有现实感的场景的方法。在神经辐射场（NeRFs）的基础上，模拟场景的体积密度和定向发射的辐射。使用与光照和视角相关的神经网络隐式模拟每个物体的光传输。这种方法可以应对光线移动的问题，而无需重新训练模型[[189](#bib.bib189)]。
- en: V-A3 Uneven illumination
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A3 不均匀光照
- en: For the characteristics of light inhomogeneity in the environment, it is possible
    to use the light correction network framework, UDoc-GAN, to solve it.The main
    thing is to convert uncertain normal to abnormal image panning to deterministic
    image panning with different levels of ambient light for learning guidance.In
    contrast, Aleth-NeRF cannot handle non-uniform illumination or shadow images.Meanwhile,
    UDoc-GAN algorithm is more computationally efficient in the inference stage and
    closer to realistic requirements[[190](#bib.bib190)].
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于环境中光照不均的特征，可以使用光线校正网络框架 UDoc-GAN 来解决。主要是将不确定的法线图像平移转换为具有不同级别环境光的确定性图像平移作为学习指导。相比之下，Aleth-NeRF
    无法处理非均匀光照或阴影图像。同时，UDoc-GAN 算法在推理阶段的计算效率更高，更接近实际要求[[190](#bib.bib190)]。
- en: V-A4 Shadow ray
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A4 阴影光线
- en: Jingwang Ling et al. monitored the camera illumination between the scene and
    multi-view image planes and noticed shadow rays, which led to a new shadow ray
    supervision scheme. This scheme optimizes the samples and ray positions along
    the rays. By supervising the shadow rays to achieve controllable illumination,
    a neural SDF network for single-view scene reproduction under multi-illumination
    conditions is finally constructed.However,the method is only applicable to point
    and parallel light sources and has obvious requirements for the position of the
    light source. The implementation of the method is also based on a simple environment
    where the scene is not illuminate[[191](#bib.bib191)].
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Jingwang Ling 等人监测了场景与多视角图像平面之间的相机光照，并注意到阴影光线，这引出了一个新的阴影光线监督方案。该方案优化了光线沿线的样本和光线位置。通过监督阴影光线以实现可控的光照，最终构建了一个在多光照条件下用于单视角场景重建的神经
    SDF 网络。然而，该方法仅适用于点光源和平行光源，并且对光源的位置有明显要求。该方法的实现也基于一个简单的环境，其中场景没有被照明[[191](#bib.bib191)]。
- en: V-A5 Complex light variation
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A5 复杂光照变化
- en: Also, for uncontrolled complex environment settings from which images are acquired,
    the NeRF-OSR algorithm enables the generation of new views and new illumination.This
    is a solution for image generation in complex environments.Solving some fuzzy
    performance from the perspective of optimizing this algorithm can be an interesting
    future research direction.For example, resolving inaccuracies in geometric estimation,
    incorporating more priori knowledge of the outdoor scenes, etc[[192](#bib.bib192)].Later,
    for this problem, Higuera C et al. proposed a solution to the complex problem
    of light variation by reducing the perceptual differences in vision and using
    a probabilistic diffusion model to capture light.The method is implemented based
    on simulated data and can address the limitations of large-scale data.Of course,
    the method suffers from the problem of computation time, especially in the denoising
    process which consumes more time[[193](#bib.bib193)].This is especially true for
    reflections in complex environments, for example, with glass and mirrors.YuanChen
    Guo et al. introduced NeRFReN for simulating scenes with reflections, mainly by
    dividing the scene into transmission and reflection components and modeling these
    two components with independent neural radiation fields.This approach has far-reaching
    implications for further research in scene understanding and neural editing.However,
    this method does not consider modeling curved reflective surfaces and multiple
    non-coplanar reflective surfaces[[194](#bib.bib194)].
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于从未控制的复杂环境设置中获取的图像，NeRF-OSR算法能够生成新的视图和新的照明。这是复杂环境中图像生成的一种解决方案。从优化该算法的角度解决一些模糊性能可能是一个有趣的未来研究方向。例如，解决几何估计的不准确性，结合更多的户外场景先验知识等[[192](#bib.bib192)]。之后，Higuera
    C等人提出了一种通过减少视觉感知差异并使用概率扩散模型捕捉光线来解决光线变化复杂问题的方法。该方法基于模拟数据实施，可以解决大规模数据的限制。当然，该方法存在计算时间的问题，尤其是在去噪过程中耗时较多[[193](#bib.bib193)]。对于复杂环境中的反射，特别是玻璃和镜子，这一点尤为明显。YuanChen
    Guo等人引入了NeRFReN用于模拟具有反射的场景，主要通过将场景划分为传输和反射成分，并用独立的神经辐射场对这两个成分进行建模。这种方法对场景理解和神经编辑的进一步研究具有深远的意义。然而，该方法未考虑建模弯曲的反射表面和多个非共面的反射表面[[194](#bib.bib194)]。
- en: V-B Local Illumination
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 局部照明
- en: V-B1 Reflectance
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 反射率
- en: Generally speaking, reflected light can be divided into three components, namely
    ambient reflection, diffuse reflection and specular reflection.The different media
    materials that cause the reflected light will show different lighting cues in
    the exposure.An omnidirectional illumination method trains deep neural networks
    on videos with automatic exposure and white balance to match real images with
    predicted illumination based on image re-illumination and then regression from
    the background[[195](#bib.bib195)].
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，反射光可以分为三种成分，即环境反射、漫反射和镜面反射。引起反射光的不同介质材料在曝光中会显示出不同的光照提示。全方位照明方法通过在具有自动曝光和白平衡的视频上训练深度神经网络，以基于图像重新照明的真实图像与预测照明匹配，然后从背景回归[[195](#bib.bib195)]。
- en: 'The method focuses on minimizing the reconstructed illumination loss function
    and adding an adversarial loss.And the reconstructed illumination loss and the
    adversarial loss are as followed:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法侧重于最小化重建照明损失函数，并添加对抗损失。重建照明损失和对抗损失如下所示：
- en: '|  | $L_{\mathrm{rec}}=\sum_{b=0}^{2}\lambda_{b}\left\&#124;\hat{M}\odot\left(\Lambda\left(\hat{I}_{b}\right)^{\frac{1}{\gamma}}-\Lambda\left(I_{b}\right)\right)\right\&#124;_{1}.$
    |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\mathrm{rec}}=\sum_{b=0}^{2}\lambda_{b}\left\&#124;\hat{M}\odot\left(\Lambda\left(\hat{I}_{b}\right)^{\frac{1}{\gamma}}-\Lambda\left(I_{b}\right)\right)\right\&#124;_{1}.$
    |  |'
- en: In this formulation, the linear rendering of the shear is $\gamma$-encoded with
    $\gamma$ to match $I$.$\hat{M}$ represents a binary mask.$\lambda_{b}$ represents
    an optional weight.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，剪切的线性渲染用$\gamma$-编码与$\gamma$匹配$I$。$\hat{M}$表示二进制掩模。$\lambda_{b}$表示可选的权重。
- en: '|  | $\displaystyle L_{\mathrm{adv}}=$ | $\displaystyle\log D\left(\Lambda\left(I_{c}\right)\right)$
    |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{\mathrm{adv}}=$ | $\displaystyle\log D\left(\Lambda\left(I_{c}\right)\right)$
    |  |'
- en: '|  |  | $\displaystyle+\log\left(1-D\left(\Lambda\left(\sum_{\theta,\phi}R(\theta,\phi)e^{G(x;\theta,\phi)}\right)^{\frac{1}{\gamma}}\right)\right)$
    |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\log\left(1-D\left(\Lambda\left(\sum_{\theta,\phi}R(\theta,\phi)e^{G(x;\theta,\phi)}\right)^{\frac{1}{\gamma}}\right)\right)$
    |  |'
- en: In this formulation, the $D$ represents an auxiliary discriminator network,the
    $G$ represents the generator,the $x$ represents input image.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在此公式中，$D$ 代表辅助判别网络，$G$ 代表生成器，$x$ 代表输入图像。
- en: 'Therefore, combining the two yields the following common objectives:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将两者结合起来会产生以下共同目标：
- en: '|  | $G^{*}=\arg\min_{G}\max_{D}\left(1-\lambda_{\mathrm{rec}}\right)\mathbf{E}\left[L_{\mathrm{adv}}\right]+\lambda_{\mathrm{rec}}\mathbf{E}\left[L_{\mathrm{rec}}\right]$
    |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $G^{*}=\arg\min_{G}\max_{D}\left(1-\lambda_{\mathrm{rec}}\right)\mathbf{E}\left[L_{\mathrm{adv}}\right]+\lambda_{\mathrm{rec}}\mathbf{E}\left[L_{\mathrm{rec}}\right]$
    |  |'
- en: Of course, there are certainly real-life situations where the reflectance is
    similar.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现实生活中确实存在反射率相似的情况。
- en: In illumination variation, there is also a cluster optimization method based
    on neural reflection field using reflection iteration to solve the problem of
    similar reflectance of different instances from the perspective of hierarchical
    clustering.However, there still exists the challenge of facing complex scenarios
    that do not conform to the unsupervised intrinsic prior, and solutions to such
    problems need to be proposed[[196](#bib.bib196)].
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在光照变化方面，还有一种基于神经反射场的聚类优化方法，该方法使用反射迭代从层次聚类的角度解决不同实例的相似反射问题。然而，仍然存在面临不符合无监督固有先验的复杂场景的挑战，需要提出针对这些问题的解决方案[[196](#bib.bib196)]。
- en: V-B2 Radiance
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 辐射
- en: Different mediums have different radiance to light, using a web-based query
    light integration network on which reflection decomposition is performed.The algorithm
    captures changing illumination, enabling more accurate new view compositing and
    re-illumination. Finally, fast and practical distinguishable rendering areas are
    implemented.The algorithm can also estimate the shape and BRDF of the objects
    in the image, which is a point of superiority over other algorithms.However, this
    method has some limitations in the study of mutual reflection.In particular, an
    effective treatment of the interactions between all effects could be a future
    research direction[[197](#bib.bib197)].
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 不同介质对光的辐射不同，使用基于网络的光查询集成网络进行反射分解。该算法捕捉到光照变化，实现了更准确的新视图合成和重新照明。最后，实现了快速且实用的可区分渲染区域。该算法还可以估计图像中物体的形状和
    BRDF，这一点相较于其他算法具有优势。然而，该方法在相互反射的研究中存在一些局限性。特别是，对所有效应之间的相互作用的有效处理可能是未来的研究方向[[197](#bib.bib197)]。
- en: VI Application
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 应用
- en: 3D controllable image synthesis has many potential applications in various domains,
    such as entertainment, industry, and security.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 可控图像合成在娱乐、工业和安全等各个领域具有许多潜在应用。
- en: VI-A Entertainment Application
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 娱乐应用
- en: a)Video games. 3D image synthesis can create immersive and interactive virtual
    worlds for gamers to explore and enjoy. It can also enhance the realism and variety
    of characters, objects and environments in the game[[198](#bib.bib198), [199](#bib.bib199)].
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: a) 视频游戏。3D 图像合成可以为玩家创建身临其境和互动的虚拟世界。它还可以增强游戏中角色、物体和环境的真实感和多样性[[198](#bib.bib198),
    [199](#bib.bib199)]。
- en: b)Movies and TV shows. 3D image synthesis can produce stunning visual effects
    and animations for movies and TV shows. It can also enable the creation of digital
    actors, creatures and scenarios that would be impossible or impractical to film
    in real life[[200](#bib.bib200), [201](#bib.bib201)].
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: b) 电影和电视节目。3D 图像合成可以为电影和电视节目制作令人惊叹的视觉效果和动画。它还可以实现数字演员、生物和场景的创建，这些在现实生活中拍摄是不可能或不切实际的[[200](#bib.bib200),
    [201](#bib.bib201)]。
- en: c)Virtual reality and augmented reality. 3D image synthesis can generate realistic
    and immersive virtual experiences for users who wear VR or AR devices. It can
    also augment the real world with digital information and graphics that enhance
    the user’s perception and interaction[[202](#bib.bib202)].
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: c) 虚拟现实和增强现实。3D 图像合成可以为佩戴 VR 或 AR 设备的用户生成逼真而沉浸的虚拟体验。它还可以通过数字信息和图形增强现实世界，从而提高用户的感知和互动[[202](#bib.bib202)]。
- en: d)Art and design. 3D image synthesis can enable artists and designers to express
    their creativity and vision in new ways. It can also facilitate the creation and
    presentation of 3D artworks, models and prototypes[[203](#bib.bib203)].
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: d) 艺术和设计。3D 图像合成可以使艺术家和设计师以新的方式表达他们的创造力和愿景。它还可以促进 3D 艺术品、模型和原型的创作和展示[[203](#bib.bib203)]。
- en: VI-B Entertainment Industry
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 娱乐行业
- en: a)Product design and prototyping. By using 3D image synthesis, designers can
    visualize and test different aspects of their products, such as shape, color,
    texture, functionality and performance, before manufacturing them. This can save
    time and money, as well as improve the quality and innovation of the products[[204](#bib.bib204)].
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: a)产品设计和原型制作。通过使用3D图像合成，设计师可以在生产前可视化和测试产品的不同方面，如形状、颜色、纹理、功能和性能。这可以节省时间和成本，并提高产品的质量和创新[[204](#bib.bib204)]。
- en: b)Training and simulation. By using 3D image synthesis, trainers can create
    realistic and immersive scenarios for workers to practice their skills and learn
    new procedures. For example, 3D image synthesis can be used to simulate hazardous
    environments, such as oil rigs, mines or nuclear plants, where workers can train
    safely and effectively.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: b)培训和模拟。通过使用3D图像合成，培训师可以创建逼真且沉浸的场景，让工人练习技能并学习新程序。例如，3D图像合成可以用于模拟危险环境，如石油平台、矿井或核电站，工人可以在安全有效的环境中进行培训。
- en: c)Inspection and quality control. By using 3D image synthesis, inspectors can
    detect and analyze defects and errors in products or processes, such as cracks,
    leaks or misalignments. For example, 3D image synthesis can be used to inspect
    complex structures, such as bridges, pipelines or aircrafts, where human inspection
    may be difficult or dangerous[[205](#bib.bib205), [206](#bib.bib206)].
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: c)检查和质量控制。通过使用3D图像合成，检查员可以检测和分析产品或过程中的缺陷和错误，如裂纹、泄漏或错位。例如，3D图像合成可以用于检查复杂结构，如桥梁、管道或飞机，这些地方的人工检查可能困难或危险[[205](#bib.bib205),
    [206](#bib.bib206)]。
- en: VI-C Entertainment Security
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 娱乐安全
- en: a)Biometric authentication. 3D image synthesis can be used to generate realistic
    face images from 3D face scans or facial landmarks, which can be used for identity
    verification or access control. For example, Face ID on iPhone uses 3D image synthesis
    to project infrared dots on the user’s face and match them with the stored 3D
    face model[[207](#bib.bib207), [208](#bib.bib208)].
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: a)生物识别认证。3D图像合成可以用于从3D面部扫描或面部标记生成逼真的面部图像，这些图像可以用于身份验证或访问控制。例如，iPhone上的Face ID使用3D图像合成在用户的面部投射红外点，并将其与存储的3D面部模型进行匹配[[207](#bib.bib207),
    [208](#bib.bib208)]。
- en: b)Forensic analysis. 3D image synthesis can be used to reconstruct crime scenes
    or evidence from partial or noisy data, such as surveillance videos, witness sketches
    or DNA samples. For example, Snapshot DNA Phenotyping uses 3D image synthesis
    to predict the facial appearance of a person from their DNA[[209](#bib.bib209)].
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: b)法医分析。3D图像合成可以用于从部分或噪声数据（如监控视频、证人素描或DNA样本）重建犯罪现场或证据。例如，Snapshot DNA Phenotyping使用3D图像合成根据DNA预测一个人的面部特征[[209](#bib.bib209)]。
- en: c)Counter-terrorism. 3D image synthesis can be used to detect and prevent potential
    threats by generating realistic scenarios or simulations based on intelligence
    data or risk assessment. For example, the US Department of Defense uses 3D image
    synthesis to create virtual environments for training and testing purposes.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: c)反恐。3D图像合成可以通过根据情报数据或风险评估生成逼真的场景或模拟，来检测和预防潜在威胁。例如，美国国防部使用3D图像合成创建虚拟环境用于培训和测试目的。
- en: d)Cybersecurity. 3D image synthesis can be used to protect sensitive data or
    systems from unauthorized access or manipulation by generating fake or distorted
    images that can fool attackers or malware. For example, Adversarial Robustness
    Toolbox uses 3D image synthesis to generate adversarial examples that can evade
    or mislead deep learning models[[210](#bib.bib210)].
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: d)网络安全。3D图像合成可以用于保护敏感数据或系统免受未经授权的访问或操控，通过生成假图像或失真的图像来迷惑攻击者或恶意软件。例如，Adversarial
    Robustness Toolbox使用3D图像合成生成对抗样本，从而规避或误导深度学习模型[[210](#bib.bib210)]。
- en: VII Conclusion
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: In this paper we have given a comprehensive survey of the emerging progress
    on 3D controllable image synthesis. We discussed a variety of 3D controllable
    image synthesis aspects according to their low-level vision cues. The survey reviewed
    important progress made on 3D datasets, geometrically controllable image synthesis,
    photometrically controllable image synthesis and related applications. Moreover,
    the global and local synthesis approaches are separately summarized in each controllable
    mode to further distinguish diverse synthesis tasks. Our ultimate goal is to provide
    a useful guide for the researchers and developers who would be interested to synthesizing
    and editing the image from the low-level 3D prompts. We categorize literatures
    mainly according to controllable 3D cues since they directly decide our synthesis
    tasks and abilities. However, there are still other non-rigid 3D cues such as
    body kinematic joints and elastic shape deformation which are not covered by this
    survey. In the future, we expect that more explainable controllable cues can be
    explored from current diffusion and neural radiance fields models by advanced
    latent decomposition or inverse rendering techniques. Together with the semantic-level
    controllable image synthesis, the low-level 3D controllable image synthesis and
    editing can generate more incredible and reliable images in our lives.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对 3D 可控图像合成的新兴进展进行了全面调查。我们根据其低层次视觉线索讨论了各种 3D 可控图像合成方面。这项调查回顾了在 3D 数据集、几何可控图像合成、光度可控图像合成及相关应用方面取得的重要进展。此外，我们分别总结了全球和局部合成方法，以进一步区分不同的合成任务。我们的最终目标是为那些对从低层次
    3D 提示中合成和编辑图像感兴趣的研究人员和开发者提供有用的指南。我们主要根据可控的 3D 线索对文献进行分类，因为这些线索直接决定了我们的合成任务和能力。然而，仍有其他非刚性
    3D 线索，如身体运动关节和弹性形状变形，这些在本次调查中没有涵盖。未来，我们期望通过先进的潜在分解或逆向渲染技术，从当前的扩散模型和神经辐射场模型中探索更多可解释的可控线索。结合语义层次的可控图像合成，低层次的
    3D 可控图像合成和编辑能够在我们的生活中生成更加惊人和可靠的图像。
- en: References
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution
    image synthesis with latent diffusion models,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp.
    10 684–10 695.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, 和 B. Ommer, “使用潜在扩散模型进行高分辨率图像合成，”
    载于 *IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集*，2022年6月，第 10 684–10 695 页。'
- en: '[2] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, “A comprehensive
    survey of ai-generated content (aigc): A history of generative ai from gan to
    chatgpt,” 2023.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, 和 L. Sun, “关于 AI 生成内容
    (AIGC) 的全面调查：从 GAN 到 ChatGPT 的生成 AI 历史，” 2023年。'
- en: '[3] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch,
    D. Card, R. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel, J. Davis, D. Demszky,
    C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei,
    C. Finn, T. Gale, L. E. Gillespie, K. Goel, N. D. Goodman, S. Grossman, N. Guha,
    T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. F.
    Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani,
    O. Khattab, P. W. Koh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak,
    M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D.
    Manning, S. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan,
    B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr,
    I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan,
    R. Reich, H. Ren, F. Rong, Y. H. Roohani, C. Ruiz, J. Ryan, C. R’e, D. Sadigh,
    S. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan, A. Tamkin, R. Taori, A. W.
    Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga,
    J. You, M. A. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou,
    and P. Liang, “On the opportunities and risks of foundation models,” *ArXiv*,
    2021\. [Online]. Available: https://crfm.stanford.edu/assets/report.pdf'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch,
    D. Card, R. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel, J. Davis, D.
    Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh,
    L. Fei-Fei, C. Finn, T. Gale, L. E. Gillespie, K. Goel, N. D. Goodman, S. Grossman,
    N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J.
    Huang, T. F. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling,
    F. Khani, O. Khattab, P. W. Koh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Kumar,
    F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A.
    Malik, C. D. Manning, S. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A.
    Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. F.
    Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance,
    C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. H. Roohani, C. Ruiz, J.
    Ryan, C. R’e, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan, A.
    Tamkin, R. Taori, A. W. Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu,
    Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. A. Zaharia, M. Zhang, T. Zhang, X. Zhang,
    Y. Zhang, L. Zheng, K. Zhou 和 P. Liang，“基础模型的机会与风险”， *ArXiv*，2021年。[在线] 可用: https://crfm.stanford.edu/assets/report.pdf'
- en: '[4] L. Zhang and M. Agrawala, “Adding conditional control to text-to-image
    diffusion models,” 2023.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] L. Zhang 和 M. Agrawala，“在文本到图像扩散模型中添加条件控制”，2023年。'
- en: '[5] X. Wang, L. Xie, C. Dong, and Y. Shan, “Real-esrgan: Training real-world
    blind super-resolution with pure synthetic data,” in *International Conference
    on Computer Vision Workshops (ICCVW)*.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Wang, L. Xie, C. Dong 和 Y. Shan，“Real-esrgan: 使用纯合成数据训练现实世界的盲超分辨率”，见于
    *国际计算机视觉研讨会（ICCVW）*。'
- en: '[6] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    in *Advances in Neural Information Processing Systems*, H. Larochelle, M. Ranzato,
    R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.   Curran Associates, Inc., 2020,
    pp. 6840–6851. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Ho, A. Jain 和 P. Abbeel，“去噪扩散概率模型”，见于 *神经信息处理系统进展*，H. Larochelle, M.
    Ranzato, R. Hadsell, M. Balcan 和 H. Lin 编，卷33。Curran Associates, Inc.，2020年，第6840–6851页。[在线]
    可用: https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf'
- en: '[7] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. C. Courville, and Y. Bengio, “Generative adversarial nets,” in *NIPS*, 2014.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. C. Courville 和 Y. Bengio，“生成对抗网络”，见于 *NIPS*，2014年。'
- en: '[8] H. Huang, P. S. Yu, and C. Wang, “An introduction to image synthesis with
    generative adversarial nets,” 2018.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Huang, P. S. Yu 和 C. Wang，“生成对抗网络图像合成简介”，2018年。'
- en: '[9] M. Mirza and S. Osindero, “Conditional generative adversarial nets,” 2014.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Mirza 和 S. Osindero，“条件生成对抗网络”，2014年。'
- en: '[10] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using convolutional
    neural networks,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2016, pp. 2414–2423.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] L. A. Gatys, A. S. Ecker 和 M. Bethge，“使用卷积神经网络进行图像风格迁移”，见于 *2016 IEEE计算机视觉与模式识别大会（CVPR）*，2016年，第2414–2423页。'
- en: '[11] S. Agarwal, N. Snavely, I. Simon, S. M. Seitz, and R. Szeliski, “Building
    rome in a day,” in *2009 IEEE 12th International Conference on Computer Vision*,
    2009, pp. 72–79.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Agarwal, N. Snavely, I. Simon, S. M. Seitz 和 R. Szeliski，“一天建罗马”，见于
    *2009 IEEE第12届计算机视觉国际会议*，2009年，第72–79页。'
- en: '[12] L. Yang, T. Yendo, M. Panahpour Tehrani, T. Fujii, and M. Tanimoto, “Probabilistic
    reliability based view synthesis for ftv,” in *2010 IEEE International Conference
    on Image Processing*, 2010, pp. 1785–1788.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L. Yang, T. Yendo, M. Panahpour Tehrani, T. Fujii 和 M. Tanimoto，“基于概率可靠性的视图合成用于ftv”，见于*2010
    IEEE国际图像处理会议*，2010年，第1785–1788页。'
- en: '[13] Y. Zheng, G. Zeng, H. Li, Q. Cai, and J. Du, “Colorful 3d reconstruction
    at high resolution using multi-view representation,” *Journal of Visual Communication
    and Image Representation*, vol. 85, p. 103486, 2022\. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1047320322000402'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Zheng, G. Zeng, H. Li, Q. Cai 和 J. Du，“使用多视图表示的高分辨率彩色3d重建”，*视觉通信与图像表示杂志*，第85卷，第103486页，2022年。[在线]。可用链接：
    https://www.sciencedirect.com/science/article/pii/S1047320322000402'
- en: '[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE Conference on Computer
    Vision and Pattern Recognition*, 2009, pp. 248–255.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li 和 L. Fei-Fei，“Imagenet：一个大规模层次化图像数据库”，见于*2009年IEEE计算机视觉与模式识别会议*，2009年，第248–255页。'
- en: '[15] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti,
    T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson,
    L. Schmidt, R. Kaczmarczyk, and J. Jitsev, “Laion-5b: An open large-scale dataset
    for training next generation image-text models,” *ArXiv*, vol. abs/2210.08402,
    2022.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti,
    T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K.
    Crowson, L. Schmidt, R. Kaczmarczyk 和 J. Jitsev，“Laion-5b：用于训练下一代图像-文本模型的开放大规模数据集”，*ArXiv*，第abs/2210.08402卷，2022年。'
- en: '[16] S. M. Mohammad and S. Kiritchenko, “An annotated dataset of emotions evoked
    by art,” in *Proceedings of the 11th Edition of the Language Resources and Evaluation
    Conference (LREC-2018)*, Miyazaki, Japan, 2018.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. M. Mohammad 和 S. Kiritchenko，“由艺术引发的情感注释数据集”，见于*第11届语言资源与评估会议（LREC-2018）*，日本宫崎，2018年。'
- en: '[17] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    in *ECCV*, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi
    和 R. Ng，“Nerf：将场景表示为神经辐射场以进行视图合成”，见于*ECCV*，2020年。'
- en: '[18] S. Huang, Q. Li, J. Liao, L. Liu, and L. Li, “An overview of controllable
    image synthesis: Current challenges and future trends,” *SSRN Electronic Journal*,
    2022\. [Online]. Available: https://ssrn.com/abstract=4187269'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Huang, Q. Li, J. Liao, L. Liu 和 L. Li，“可控图像合成概述：当前挑战与未来趋势”，*SSRN电子期刊*，2022年。[在线]。可用链接：
    https://ssrn.com/abstract=4187269'
- en: '[19] A. Tsirikoglou, G. Eilertsen, and J. Unger, “A survey of image synthesis
    methods for visual machine learning,” *Computer Graphics Forum*, vol. 39, 2020.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Tsirikoglou, G. Eilertsen 和 J. Unger，“用于视觉机器学习的图像合成方法综述”，*计算机图形论坛*，第39卷，2020年。'
- en: '[20] R. Haas, S. Graßhof, and S. S. Brandt, “Controllable gan synthesis using
    non-rigid structure-from-motion,” 2022.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] R. Haas, S. Graßhof 和 S. S. Brandt，“使用非刚性运动结构进行可控gan合成”，2022年。'
- en: '[21] J. Zhang, A. Siarohin, Y. Liu, H. Tang, N. Sebe, and W. Wang, “Training
    and tuning generative neural radiance fields for attribute-conditional 3d-aware
    face generation,” 2022.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Zhang, A. Siarohin, Y. Liu, H. Tang, N. Sebe 和 W. Wang，“训练和调整生成神经辐射场以进行属性条件的3d感知面部生成”，2022年。'
- en: '[22] J. Ko, K. Cho, D. Choi, K. Ryoo, and S. Kim, “3d gan inversion with pose
    optimization,” *WACV*, 2023.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Ko, K. Cho, D. Choi, K. Ryoo 和 S. Kim，“带姿态优化的3d gan反演”，*WACV*，2023年。'
- en: '[23] S. Yang, W. Wang, B. Peng, and J. Dong, “Designing a 3d-aware stylenerf
    encoder for face editing,” in *ICASSP 2023 - 2023 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2023, pp. 1–5.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Yang, W. Wang, B. Peng 和 J. Dong，“为面部编辑设计一个3d感知stylenerf编码器”，见于*ICASSP
    2023 - 2023 IEEE国际声学、语音和信号处理会议（ICASSP）*，2023年，第1–5页。'
- en: '[24] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang,
    T. F. Y. Vicente, T. Dideriksen, H. Arora, M. Guillaumin, and J. Malik, “Abo:
    Dataset and benchmarks for real-world 3d object understanding,” in *CVPR 2022*,
    2022\. [Online]. Available: https://www.amazon.science/publications/abo-dataset-and-benchmarks-for-real-world-3d-object-understanding'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang,
    T. F. Y. Vicente, T. Dideriksen, H. Arora, M. Guillaumin 和 J. Malik，“Abo：真实世界3d对象理解的数据集和基准”，见于*CVPR
    2022*，2022年。[在线]。可用链接： https://www.amazon.science/publications/abo-dataset-and-benchmarks-for-real-world-3d-object-understanding'
- en: '[25] B. Yang, Y. Zhang, Y. Xu, Y. Li, H. Zhou, H. Bao, G. Zhang, and Z. Cui,
    “Learning object-compositional neural radiance field for editable scene rendering,”
    in *International Conference on Computer Vision (ICCV)*, October 2021.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] B. Yang, Y. Zhang, Y. Xu, Y. Li, H. Zhou, H. Bao, G. Zhang, 和 Z. Cui,
    “学习对象组成神经辐射场用于可编辑场景渲染，”在*国际计算机视觉大会（ICCV）*，2021年10月。'
- en: '[26] X. Yan, Z. Yuan, Y. Du, Y. Liao, Y. Guo, Z. Li, and S. Cui, “Clevr3d:
    Compositional language and elementary visual reasoning for question answering
    in 3d real-world scenes,” 2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] X. Yan, Z. Yuan, Y. Du, Y. Liao, Y. Guo, Z. Li, 和 S. Cui, “Clevr3d: 用于3D真实世界场景问题回答的组成语言和基础视觉推理，”2021年。'
- en: '[27] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proc. Computer
    Vision and Pattern Recognition (CVPR), IEEE*, 2017.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner,
    “Scannet: 丰富标注的室内场景3D重建，”在*计算机视觉与模式识别会议（CVPR）论文集，IEEE*，2017年。'
- en: '[28] Google, “Realestate10k: A large dataset of camera trajectories from video
    clips,” 2018\. [Online]. Available: https://google.github.io/realestate10k/'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Google, “Realestate10k: 来自视频片段的大规模相机轨迹数据集，”2018年。[在线]. 可用链接: https://google.github.io/realestate10k/'
- en: '[29] M. Niemeyer and A. Geiger, “Giraffe: Representing scenes as compositional
    generative neural feature fields,” in *Proc. IEEE Conf. on Computer Vision and
    Pattern Recognition (CVPR)*, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Niemeyer 和 A. Geiger, “Giraffe: 将场景表示为组成生成神经特征场，”在*IEEE计算机视觉与模式识别大会（CVPR）论文集*，2021年。'
- en: '[30] J. Zhu, C. Yang, Y. Shen, Z. Shi, D. Zhao, and Q. Chen, “Linkgan: Linking
    gan latents to pixels for controllable image synthesis,” 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Zhu, C. Yang, Y. Shen, Z. Shi, D. Zhao, 和 Q. Chen, “Linkgan: 将GAN隐变量与像素关联进行可控图像合成，”2023年。'
- en: '[31] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q. Huang, Z. Li,
    S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu, “Shapenet: An
    information-rich 3d model repository,” *ArXiv*, vol. abs/1512.03012, 2015.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q. Huang, Z.
    Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, 和 F. Yu, “Shapenet:
    一个信息丰富的3D模型库，”*ArXiv*，第abs/1512.03012卷，2015年。'
- en: '[32] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *2012 IEEE Conference on Computer Vision
    and Pattern Recognition*, 2012, pp. 3354–3361.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Geiger, P. Lenz, 和 R. Urtasun, “我们准备好自动驾驶了吗？KITTI视觉基准套件，”在*2012 IEEE计算机视觉与模式识别大会*，2012年，第3354–3361页。'
- en: '[33] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, 和 O. Beijbom, “nuscenes: 一个用于自动驾驶的多模态数据集，”在*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，2020年6月。'
- en: '[34] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M.
    Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao,
    and D. Batra, “Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments
    for embodied AI,” in *Thirty-fifth Conference on Neural Information Processing
    Systems Datasets and Benchmarks Track (Round 2)*, 2021\. [Online]. Available:
    https://openreview.net/forum?id=-v4OuqNs5P'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J.
    M. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao,
    和 D. Batra, “Habitat-matterport 3d数据集（HM3d）：1000个大型3D环境用于具身AI，”在*第三十五届神经信息处理系统大会数据集与基准跟踪（第二轮）*，2021年。[在线].
    可用链接: https://openreview.net/forum?id=-v4OuqNs5P'
- en: '[35] D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame
    stereo correspondence algorithms,” *Int. J. Comput. Vision*, vol. 47, no. 1-3,
    pp. 7–42, apr 2002\. [Online]. Available: https://doi.org/10.1023/A:1014573219977'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] D. Scharstein 和 R. Szeliski, “密集双帧立体对应算法的分类与评估，”*国际计算机视觉期刊*，第47卷，第1-3期，第7–42页，2002年4月。[在线].
    可用链接: https://doi.org/10.1023/A:1014573219977'
- en: '[36] ——, “High-accuracy stereo depth maps using structured light,” ser. CVPR’03.   USA:
    IEEE Computer Society, 2003, pp. 195–202.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] ——, “使用结构光进行高精度立体深度图，”系列：CVPR’03。美国：IEEE计算机学会，2003年，第195–202页。'
- en: '[37] D. Scharstein and C. Pal, “Learning conditional random fields for stereo,”
    in *2007 IEEE Conference on Computer Vision and Pattern Recognition*, 2007, pp.
    1–8.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] D. Scharstein 和 C. Pal, “学习条件随机场用于立体匹配，”在*2007 IEEE计算机视觉与模式识别大会*，2007年，第1–8页。'
- en: '[38] H. Hirschmuller and D. Scharstein, “Evaluation of cost functions for stereo
    matching,” in *2007 IEEE Conference on Computer Vision and Pattern Recognition*,
    2007, pp. 1–8.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] H. Hirschmuller 和 D. Scharstein, “立体匹配的代价函数评估，”在*2007 IEEE计算机视觉与模式识别大会*，2007年，第1–8页。'
- en: '[39] D. Scharstein, H. Hirschmüller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang,
    and P. Westling, “High-resolution stereo datasets with subpixel-accurate ground
    truth,” in *German Conference on Pattern Recognition*, 2014.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] D. Scharstein, H. Hirschmüller, Y. Kitajima, G. Krathwohl, N. Nesic, X.
    Wang, 和 P. Westling, “具有亚像素精确地面真值的高分辨率立体数据集，”发表于*德国模式识别大会*，2014年。'
- en: '[40] P. K. Nathan Silberman, Derek Hoiem and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” in *ECCV*, 2012.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] P. K. Nathan Silberman, Derek Hoiem 和 R. Fergus, “基于RGBD图像的室内分割与支持推断，”发表于*ECCV*，2012年。'
- en: '[41] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-pie,”
    in *2008 8th IEEE International Conference on Automatic Face and Gesture Recognition*,
    2008, pp. 1–8.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] R. Gross, I. Matthews, J. Cohn, T. Kanade, 和 S. Baker, “Multi-pie，”发表于*2008年第8届IEEE国际自动面部和姿态识别会议*，2008年，第1–8页。'
- en: '[42] K. Guo, P. Lincoln, P. Davidson, J. Busch, X. Yu, M. Whalen, G. Harvey,
    S. O. Escolano, R. K. Pandey, J. Dourgarian, D. Tang, A. Tkach, A. Kowdle, E. Cooper,
    M. Dou, S. Fanello, G. Fyffe, C. Rhemann, J. Taylor, P. Debevec, and S. Izadi,
    “The relightables: Volumetric performance capture of humans with realistic relighting,”
    2019\. [Online]. Available: https://dl.acm.org/citation.cfm?id=3356571'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Guo, P. Lincoln, P. Davidson, J. Busch, X. Yu, M. Whalen, G. Harvey,
    S. O. Escolano, R. K. Pandey, J. Dourgarian, D. Tang, A. Tkach, A. Kowdle, E.
    Cooper, M. Dou, S. Fanello, G. Fyffe, C. Rhemann, J. Taylor, P. Debevec, 和 S.
    Izadi, “Relightables：具有现实重光照的体积性能捕捉人类，”2019年。[在线]。可用链接：https://dl.acm.org/citation.cfm?id=3356571'
- en: '[43] M. Boss, R. Braun, V. Jampani, J. T. Barron, C. Liu, and H. P. Lensch,
    “Nerd: Neural reflectance decomposition from image collections,” in *IEEE International
    Conference on Computer Vision (ICCV)*, 2021.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. Boss, R. Braun, V. Jampani, J. T. Barron, C. Liu, 和 H. P. Lensch, “Nerd：从图像集合中进行神经反射分解，”发表于*IEEE国际计算机视觉大会（ICCV）*，2021年。'
- en: '[44] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    metrics: Psnr vs. ssim,” in *2010 20th International Conference on Pattern Recognition*.   IEEE,
    2010, p. 2366–2369.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Z. Wang, A. C. Bovik, H. R. Sheikh, 和 E. P. Simoncelli, “图像质量度量：PSNR与SSIM，”发表于*2010年第20届国际模式识别会议*。IEEE，2010年，第2366–2369页。'
- en: '[45] ——, “Image quality assessment: from error visibility to structural similarity,”
    *IEEE transactions on image processing*, vol. 13, no. 4, p. 600–612, 2004.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] ——, “图像质量评估：从误差可见性到结构相似性，”*IEEE图像处理交易*，第13卷，第4期，第600–612页，2004年。'
- en: '[46] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2018, p. 586–595.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, 和 O. Wang, “深度特征作为感知度量的非凡有效性，”发表于*IEEE计算机视觉与模式识别会议论文集*，2018年，第586–595页。'
- en: '[47] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,
    “Improved techniques for training gans,” in *Advances in neural information processing
    systems*, 2016, p. 2234–2242.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, 和 X. Chen,
    “改进的生成对抗网络训练技术，”发表于*神经信息处理系统进展*，2016年，第2234–2242页。'
- en: '[48] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
    “Gans trained by a two time-scale update rule converge to a local nash equilibrium,”
    in *Proceedings of the 31st International Conference on Neural Information Processing
    Systems*, ser. NIPS’17.   Red Hook, NY, USA: Curran Associates Inc., 2017, p.
    6629–6640.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, 和 S. Hochreiter, “通过双时间尺度更新规则训练的生成对抗网络收敛到局部纳什均衡，”发表于*第31届国际神经信息处理系统会议论文集*，NIPS’17系列。红钩，纽约，美国：Curran
    Associates Inc.，2017年，第6629–6640页。'
- en: '[49] M. Bińkowski, D. J. Sutherland, M. Arbel, and A. Gretton, “Demystifying
    MMD GANs,” in *International Conference on Learning Representations*, 2018\. [Online].
    Available: https://openreview.net/forum?id=r1lUOzWCW'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Bińkowski, D. J. Sutherland, M. Arbel, 和 A. Gretton, “揭示MMD GANs的奥秘，”发表于*国际学习表征会议*，2018年。[在线]。可用链接：https://openreview.net/forum?id=r1lUOzWCW'
- en: '[50] Z. Shi, S. Peng, Y. Xu, Y. Liao, and Y. Shen, “Deep generative models
    on 3d representations: A survey,” 2022.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. Shi, S. Peng, Y. Xu, Y. Liao, 和 Y. Shen, “3D表示的深度生成模型：综述，”2022年。'
- en: '[51] R. Huang, S. Zhang, T. Li, and R. He, “Beyond face rotation: Global and
    local perception gan for photorealistic and identity preserving frontal view synthesis,”
    in *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*,
    Oct 2017.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] R. Huang, S. Zhang, T. Li, 和 R. He, “超越面部旋转：用于逼真且保持身份的正面视图合成的全局和局部感知生成对抗网络，”发表于*IEEE国际计算机视觉大会（ICCV）论文集*，2017年10月。'
- en: '[52] B. Zhao, X. Wu, Z.-Q. Cheng, H. Liu, Z. Jie, and J. Feng, “Multi-view
    image generation from a single-view,” in *Proceedings of the 26th ACM International
    Conference on Multimedia*, ser. MM ’18.   New York, NY, USA: Association for Computing
    Machinery, 2018, p. 383–391\. [Online]. Available: https://doi.org/10.1145/3240508.3240536'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] B. Zhao, X. Wu, Z.-Q. Cheng, H. Liu, Z. Jie 和 J. Feng，“从单视图生成多视图图像”，发表于
    *第26届ACM国际多媒体会议论文集*，MM ’18系列。   纽约, NY, USA: 计算机协会, 2018年，第383–391页。 [在线]. 可用:
    https://doi.org/10.1145/3240508.3240536'
- en: '[53] K. Regmi and A. Borji, “Cross-view image synthesis using conditional gans,”
    in *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2018,
    pp. 3501–3510.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] K. Regmi 和 A. Borji，“使用条件GAN进行跨视角图像合成”，发表于 *2018 IEEE/CVF计算机视觉与模式识别大会*，2018年，第3501–3510页。'
- en: '[54] ——, “Cross-view image synthesis using geometry-guided conditional GANs,”
    *Computer Vision and Image Understanding*, vol. 187, p. 102788, oct 2019\. [Online].
    Available: https://doi.org/10.1016%2Fj.cviu.2019.07.008'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] ——，“使用几何引导的条件GAN进行跨视角图像合成”，*计算机视觉与图像理解*，第187卷，第102788页，2019年10月。 [在线].
    可用: https://doi.org/10.1016%2Fj.cviu.2019.07.008'
- en: '[55] F. Mokhayeri, K. Kamali, and E. Granger, “Cross-domain face synthesis
    using a controllable gan,” in *2020 IEEE Winter Conference on Applications of
    Computer Vision (WACV)*, 2020, pp. 241–249.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] F. Mokhayeri, K. Kamali 和 E. Granger，“使用可控GAN进行跨领域面部合成”，发表于 *2020 IEEE冬季计算机视觉应用大会
    (WACV)*，2020年，第241–249页。'
- en: '[56] X. Zhu, Z. Yin, J. Shi, H. Li, and D. Lin, “Generative adversarial frontal
    view to bird view synthesis,” in *2018 International Conference on 3D Vision (3DV)*.   IEEE,
    2018, pp. 454–463.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] X. Zhu, Z. Yin, J. Shi, H. Li 和 D. Lin，“生成对抗的正视图到鸟瞰图合成”，发表于 *2018国际3D视觉大会
    (3DV)*。   IEEE, 2018年，第454–463页。'
- en: '[57] H. Ding, S. Wu, H. Tang, F. Wu, G. Gao, and X.-Y. Jing, “Cross-view image
    synthesis with deformable convolution and attention mechanism,” in *Pattern Recognition
    and Computer Vision: Third Chinese Conference, PRCV 2020, Nanjing, China, October
    16–18, 2020, Proceedings, Part I*.   Berlin, Heidelberg: Springer-Verlag, 2020,
    p. 386–397\. [Online]. Available: https://doi.org/10.1007/978-3-030-60633-6_32'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] H. Ding, S. Wu, H. Tang, F. Wu, G. Gao 和 X.-Y. Jing，“具有可变形卷积和注意力机制的跨视角图像合成”，发表于
    *模式识别与计算机视觉：第三届中国会议, PRCV 2020, 南京, 中国, 2020年10月16–18日, 论文集, 第一部分*。   Berlin,
    Heidelberg: Springer-Verlag, 2020年，第386–397页。 [在线]. 可用: https://doi.org/10.1007/978-3-030-60633-6_32'
- en: '[58] B. Ren, H. Tang, and N. Sebe, “Cascaded cross mlp-mixer gans for cross-view
    image translation,” in *British Machine Vision Conference*, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] B. Ren, H. Tang 和 N. Sebe，“级联交叉MLP-Mixer GANs用于跨视角图像转换”，发表于 *英国机器视觉会议*，2021年。'
- en: '[59] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *2017 IEEE International
    Conference on Computer Vision (ICCV)*, 2017, pp. 2242–2251.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J.-Y. Zhu, T. Park, P. Isola 和 A. A. Efros，“使用循环一致性对抗网络的无配对图像到图像转换”，发表于
    *2017 IEEE国际计算机视觉大会 (ICCV)*，2017年，第2242–2251页。'
- en: '[60] M. Yin, L. Sun, and Q. Li, “Novel view synthesis on unpaired data by conditional
    deformable variational auto-encoder,” in *Computer Vision – ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII*.   Berlin,
    Heidelberg: Springer-Verlag, 2020, p. 87–103\. [Online]. Available: https://doi.org/10.1007/978-3-030-58604-1_6'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] M. Yin, L. Sun 和 Q. Li，“通过条件可变形变分自编码器在无配对数据上进行新视图合成”，发表于 *计算机视觉 – ECCV
    2020: 第16届欧洲会议, 格拉斯哥, 英国, 2020年8月23–28日, 论文集, 第XXVIII部分*。   Berlin, Heidelberg:
    Springer-Verlag, 2020年，第87–103页。 [在线]. 可用: https://doi.org/10.1007/978-3-030-58604-1_6'
- en: '[61] X. Shen, J. Plested, Y. Yao, and T. Gedeon, “Pairwise-gan: Pose-based
    view synthesis through pair-wise training,” in *Neural Information Processing*,
    H. Yang, K. Pasupa, A. C.-S. Leung, J. T. Kwok, J. H. Chan, and I. King, Eds.   Cham:
    Springer International Publishing, 2020, pp. 507–515.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] X. Shen, J. Plested, Y. Yao 和 T. Gedeon，“Pairwise-gan: 通过配对训练的基于姿态的视角合成”，发表于
    *神经信息处理*，H. Yang, K. Pasupa, A. C.-S. Leung, J. T. Kwok, J. H. Chan 和 I. King
    编辑。   Cham: Springer International Publishing, 2020年，第507–515页。'
- en: '[62] E. R. Chan, M. Monteiro, P. Kellnhofer, J. Wu, and G. Wetzstein, “pi-gan:
    Periodic implicit generative adversarial networks for 3d-aware image synthesis,”
    in *2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    2021, pp. 5795–5805.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] E. R. Chan, M. Monteiro, P. Kellnhofer, J. Wu 和 G. Wetzstein，“pi-gan:
    周期性隐式生成对抗网络用于3D感知图像合成”，发表于 *2021 IEEE/CVF计算机视觉与模式识别大会 (CVPR)*，2021年，第5795–5805页。'
- en: '[63] S. Cai, A. Obukhov, D. Dai, and L. Van Gool, “Pix2nerf: Unsupervised conditional
    $\pi$-gan for single image to neural radiance fields translation,” in *2022 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022, pp. 3971–3980.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Cai, A. Obukhov, D. Dai, 和 L. Van Gool，“Pix2nerf: 无监督条件 $\pi$-gan 用于单幅图像到神经辐射场的转换”，在*2022
    IEEE/CVF 计算机视觉与模式识别会议 (CVPR)*，2022年，第 3971-3980 页。'
- en: '[64] T. Leimkühler and G. Drettakis, “FreeStyleGAN,” *ACM Transactions on Graphics*,
    vol. 40, no. 6, pp. 1–15, dec 2021\. [Online]. Available: https://doi.org/10.1145%2F3478513.3480538'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] T. Leimkühler 和 G. Drettakis，“FreeStyleGAN”，*ACM 图形学汇刊*，第 40 卷，第 6 期，第
    1-15 页，2021年 12 月。[在线]. 可用: https://doi.org/10.1145%2F3478513.3480538'
- en: '[65] S. C. Medin, B. Egger, A. Cherian, Y. Wang, J. B. Tenenbaum, X. Liu, and
    T. K. Marks, “Most-gan: 3d morphable stylegan for disentangled face image manipulation,”
    *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 36, no. 2,
    pp. 1962–1971, Jun. 2022\. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/20091'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. C. Medin, B. Egger, A. Cherian, Y. Wang, J. B. Tenenbaum, X. Liu, 和
    T. K. Marks，“Most-gan: 用于解耦面部图像操作的 3D 可变形 StyleGAN”，*AAAI 人工智能会议论文集*，第 36 卷，第
    2 期，第 1962-1971 页，2022年 6 月。[在线]. 可用: https://ojs.aaai.org/index.php/AAAI/article/view/20091'
- en: '[66] R. Or-El, X. Luo, M. Shan, E. Shechtman, J. J. Park, and I. Kemelmacher-Shlizerman,
    “StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2022, pp. 13 503–13 513.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] R. Or-El, X. Luo, M. Shan, E. Shechtman, J. J. Park, 和 I. Kemelmacher-Shlizerman，“StyleSDF:
    高分辨率 3D 一致图像与几何生成”，在*IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集*，2022年 6 月，第 13,503-13,513
    页。'
- en: '[67] X.-Y. Zheng, Y. Liu, P.-S. Wang, and X. Tong, “Sdf-stylegan: Implicit
    sdf-based stylegan for 3d shape generation,” in *Comput. Graph. Forum (SGP)*,
    2022.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] X.-Y. Zheng, Y. Liu, P.-S. Wang, 和 X. Tong，“Sdf-stylegan: 基于隐式 SDF 的 StyleGAN
    用于 3D 形状生成”，在*计算机图形学论坛 (SGP)*，2022年。'
- en: '[68] Y. Deng, J. Yang, J. Xiang, and X. Tong, “Gram: Generative radiance manifolds
    for 3d-aware image generation,” in *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Y. Deng, J. Yang, J. Xiang, 和 X. Tong，“Gram: 用于 3D 生成的生成辐射流形”，在*IEEE/CVF
    计算机视觉与模式识别会议*，2022年。'
- en: '[69] J. Xiang, J. Yang, Y. Deng, and X. Tong, “Gram-hd: 3d-consistent image
    generation at high resolution with generative radiance manifolds,” 2022.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] J. Xiang, J. Yang, Y. Deng, 和 X. Tong，“Gram-hd: 使用生成辐射流形在高分辨率下进行 3D 一致图像生成”，2022年。'
- en: '[70] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. De Mello, O. Gallo,
    L. J. Guibas, J. Tremblay, S. Khamis, T. Karras, and G. Wetzstein, “Efficient
    geometry-aware 3d generative adversarial networks,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp.
    16 123–16 133.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. De Mello, O.
    Gallo, L. J. Guibas, J. Tremblay, S. Khamis, T. Karras, 和 G. Wetzstein，“高效几何感知
    3D 生成对抗网络”，在*IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集*，2022年 6 月，第 16,123-16,133 页。'
- en: '[71] X. Zhao, F. Ma, D. Güera, Z. Ren, A. G. Schwing, and A. Colburn, “Generative
    multiplane images: Making a 2d gan 3d-aware,” in *Proc. ECCV*, 2022.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] X. Zhao, F. Ma, D. Güera, Z. Ren, A. G. Schwing, 和 A. Colburn，“生成的多平面图像：使
    2D GAN 具备 3D 感知”，在*欧洲计算机视觉会议论文集 (ECCV)*，2022年。'
- en: '[72] H. A. Alhaija, A. Dirik, A. Knörig, S. Fidler, and M. Shugrina, “Xdgan:
    Multi-modal 3d shape generation in 2d space,” in *33rd British Machine Vision
    Conference 2022, BMVC 2022, London, UK, November 21-24, 2022*.   BMVA Press, 2022\.
    [Online]. Available: https://bmvc2022.mpi-inf.mpg.de/0782.pdf'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] H. A. Alhaija, A. Dirik, A. Knörig, S. Fidler, 和 M. Shugrina，“Xdgan: 在
    2D 空间中的多模态 3D 形状生成”，在*第 33 届英国机器视觉会议 2022，BMVC 2022，伦敦，英国，2022年 11 月 21-24 日*。BMVA
    Press, 2022。 [在线]. 可用: https://bmvc2022.mpi-inf.mpg.de/0782.pdf'
- en: '[73] Z. Wang, S. Wu, W. Xie, M. Chen, and V. A. Prisacariu, “Nerf–: Neural
    radiance fields without known camera parameters,” 2022.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Z. Wang, S. Wu, W. Xie, M. Chen, 和 V. A. Prisacariu，“Nerf–: 无需已知相机参数的神经辐射场”，2022年。'
- en: '[74] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari, R. Ramamoorthi,
    R. Ng, and A. Kar, “Local light field fusion: Practical view synthesis with prescriptive
    sampling guidelines,” *ACM Trans. Graph.*, vol. 38, no. 4, jul 2019\. [Online].
    Available: https://doi.org/10.1145/3306346.3322980'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari, R. Ramamoorthi,
    R. Ng, 和 A. Kar，“局部光场融合：具有规定采样指南的实用视图合成”，*ACM 图形学汇刊*，第 38 卷，第 4 期，2019 年 7 月。[在线].
    可用: https://doi.org/10.1145/3306346.3322980'
- en: '[75] Q. Meng, A. Chen, H. Luo, M. Wu, H. Su, L. Xu, X. He, and J. Yu, “GNeRF:
    GAN-based Neural Radiance Field without Posed Camera,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision (ICCV)*, 2021.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Q. Meng, A. Chen, H. Luo, M. Wu, H. Su, L. Xu, X. He, 和 J. Yu，“GNeRF：基于GAN的神经辐射场，无需姿态相机，”发表于*IEEE/CVF国际计算机视觉会议（ICCV）*，2021年。'
- en: '[76] R. Jensen, A. Dahl, G. Vogiatzis, E. Tola, and H. Aanæs, “Large scale
    multi-view stereopsis evaluation,” in *Proceedings of the 2014 IEEE Conference
    on Computer Vision and Pattern Recognition*, ser. CVPR ’14.   USA: IEEE Computer
    Society, 2014, pp. 406–413\. [Online]. Available: https://doi.org/10.1109/CVPR.2014.59'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] R. Jensen, A. Dahl, G. Vogiatzis, E. Tola, 和 H. Aanæs，“大规模多视角立体视觉评估，”发表于*2014年IEEE计算机视觉与模式识别会议*，CVPR
    ’14系列。美国：IEEE计算机协会，2014年，第406–413页。[在线] 可用: https://doi.org/10.1109/CVPR.2014.59'
- en: '[77] C. C. A. A. M. C. Yoonwoo Jeong, Seokjun Ahn and J. Park, “Self-calibrating
    neural radiance fields,” in *ICCV*, 2021.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] C. C. A. A. M. C. Yoonwoo Jeong, Seokjun Ahn 和 J. Park，“自校准神经辐射场，”发表于*ICCV*，2021年。'
- en: '[78] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and temples:
    Benchmarking large-scale scene reconstruction,” *ACM Trans. Graph.*, vol. 36,
    no. 4, jul 2017\. [Online]. Available: https://doi.org/10.1145/3072959.3073599'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] A. Knapitsch, J. Park, Q.-Y. Zhou, 和 V. Koltun，“坦克与神庙：大规模场景重建基准测试，”*ACM图形学会会刊*，第36卷，第4期，2017年7月。[在线]
    可用: https://doi.org/10.1145/3072959.3073599'
- en: '[79] W. Bian, Z. Wang, K. Li, J.-W. Bian, and V. A. Prisacariu, “Nope-nerf:
    Optimising neural radiance field with no pose prior,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    4160–4169.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] W. Bian, Z. Wang, K. Li, J.-W. Bian, 和 V. A. Prisacariu，“Nope-nerf: 无姿态先验的神经辐射场优化，”发表于*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2023年6月，第4160–4169页。'
- en: '[80] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel,
    R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan,
    J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira,
    M. Savva, D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove, and
    R. A. Newcombe, “The replica dataset: A digital replica of indoor spaces,” *CoRR*,
    vol. abs/1906.05797, 2019. [Online]. Available: http://arxiv.org/abs/1906.05797'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel,
    R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan,
    J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L.
    Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove,
    和 R. A. Newcombe，“Replica 数据集：室内空间的数字副本，”*CoRR*，第abs/1906.05797卷，2019年。[在线] 可用:
    http://arxiv.org/abs/1906.05797'
- en: '[81] P. Truong, M.-J. Rakotosaona, F. Manhardt, and F. Tombari, “Sparf: Neural
    radiance fields from sparse and noisy poses.”   IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, CVPR, 2023.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] P. Truong, M.-J. Rakotosaona, F. Manhardt, 和 F. Tombari，“Sparf：来自稀疏和噪声姿态的神经辐射场。”
    IEEE/CVF计算机视觉与模式识别会议，CVPR，2023年。'
- en: '[82] J. Y. Zhang, G. Yang, S. Tulsiani, and D. Ramanan, “NeRS: Neural reflectance
    surfaces for sparse-view 3d reconstruction in the wild,” in *Conference on Neural
    Information Processing Systems*, 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Y. Zhang, G. Yang, S. Tulsiani, 和 D. Ramanan，“NeRS：用于稀疏视角3D重建的神经反射表面，”发表于*神经信息处理系统会议*，2021年。'
- en: '[83] S. Seo, D. Han, Y. Chang, and N. Kwak, “Mixnerf: Modeling a ray with mixture
    density for novel view synthesis from sparse inputs,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    20 659–20 668.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Seo, D. Han, Y. Chang, 和 N. Kwak，“Mixnerf: 基于混合密度建模的稀疏输入下的新视角合成，”发表于*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2023年6月，第20,659–20,668页。'
- en: '[84] A.-Q. Cao and R. de Charette, “Scenerf: Self-supervised monocular 3d scene
    reconstruction with radiance fields,” 2023.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A.-Q. Cao 和 R. de Charette，“Scenerf：具有辐射场的自监督单目3D场景重建，”2023年。'
- en: '[85] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    and J. Gall, “SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR
    Sequences,” in *Proc. of the IEEE/CVF International Conf. on Computer Vision (ICCV)*,
    2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    和 J. Gall，“SemanticKITTI：用于LiDAR序列的语义场景理解数据集，”发表于*IEEE/CVF国际计算机视觉会议（ICCV）*，2019年。'
- en: '[86] J. Chen, W. Yi, L. Ma, X. Jia, and H. Lu, “Gm-nerf: Learning generalizable
    model-based neural radiance fields from multi-view images,” 2023.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Chen, W. Yi, L. Ma, X. Jia, 和 H. Lu，“Gm-nerf：从多视角图像中学习可泛化的基于模型的神经辐射场，”2023年。'
- en: '[87] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, and Y. Liu, “Function4d: Real-time
    human volumetric capture from very sparse consumer rgbd sensors,” in *IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR2021)*, June 2021.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, 和 Y. Liu，“Function4d: 从非常稀疏的消费级RGBD传感器实时捕捉人体体积，”在*IEEE计算机视觉与模式识别会议（CVPR2021）*，2021年6月。'
- en: '[88] B. Bhatnagar, G. Tiwari, C. Theobalt, and G. Pons-Moll, “Multi-garment
    net: Learning to dress 3d people from images,” in *2019 IEEE/CVF International
    Conference on Computer Vision (ICCV)*, 2019, pp. 5419–5429.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] B. Bhatnagar, G. Tiwari, C. Theobalt, 和 G. Pons-Moll，“Multi-garment net:
    从图像中学习为3D人物穿衣，”在*2019 IEEE/CVF国际计算机视觉会议（ICCV）*，2019年，第5419–5429页。'
- en: '[89] W. Cheng, S. Xu, J. Piao, C. Qian, W. Wu, K.-Y. Lin, and H. Li, “Generalizable
    neural performer: Learning robust radiance fields for human novel view synthesis,”
    *arXiv preprint arXiv:2204.11798*, 2022.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] W. Cheng, S. Xu, J. Piao, C. Qian, W. Wu, K.-Y. Lin, 和 H. Li，“通用神经表现者:
    学习用于人类新视角合成的鲁棒辐射场，”*arXiv预印本 arXiv:2204.11798*，2022年。'
- en: '[90] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, “Neural
    body: Implicit neural representations with structured latent codes for novel view
    synthesis of dynamic humans,” in *CVPR*, 2021.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, 和 X. Zhou，“Neural
    body: 用于动态人类新视角合成的隐式神经表示与结构化潜变量码，”在*CVPR*，2021年。'
- en: '[91] B. Mildenhall, P. Hedman, R. Martin-Brualla, P. P. Srinivasan, and J. T.
    Barron, “NeRF in the dark: High dynamic range view synthesis from noisy raw images,”
    *CVPR*, 2022.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] B. Mildenhall, P. Hedman, R. Martin-Brualla, P. P. Srinivasan, 和 J. T.
    Barron，“NeRF in the dark: 从噪声原始图像中进行高动态范围视角合成，”*CVPR*，2022年。'
- en: '[92] L. Ma, X. Li, J. Liao, Q. Zhang, X. Wang, J. Wang, and P. V. Sander, “Deblur-nerf:
    Neural radiance fields from blurry images,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp. 12 861–12 870.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] L. Ma, X. Li, J. Liao, Q. Zhang, X. Wang, J. Wang, 和 P. V. Sander，“Deblur-nerf:
    从模糊图像中恢复神经辐射场，”在*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年6月，第12,861–12,870页。'
- en: '[93] X. Huang, Q. Zhang, Y. Feng, H. Li, X. Wang, and Q. Wang, “Hdr-nerf: High
    dynamic range neural radiance fields,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 18 398–18 408.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] X. Huang, Q. Zhang, Y. Feng, H. Li, X. Wang, 和 Q. Wang，“Hdr-nerf: 高动态范围神经辐射场，”在*IEEE/CVF计算机视觉与模式识别会议*，2022年，第18,398–18,408页。'
- en: '[94] N. Pearl, T. Treibitz, and S. Korman, “Nan: Noise-aware nerfs for burst-denoising,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2022, pp. 12 672–12 681.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] N. Pearl, T. Treibitz, 和 S. Korman，“Nan: 噪声感知神经辐射场用于爆炸去噪，”在*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年6月，第12,672–12,681页。'
- en: '[95] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman,
    “Mip-nerf 360: Unbounded anti-aliased neural radiance fields,” *CVPR*, 2022.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, 和 P. Hedman，“Mip-nerf
    360: 无界反走样神经辐射场，”*CVPR*，2022年。'
- en: '[96] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, and D. Lin,
    “Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering,”
    in *The European Conference on Computer Vision (ECCV)*, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, 和 D.
    Lin，“Bungeenerf: 极端多尺度场景渲染的渐进神经辐射场，”在*欧洲计算机视觉会议（ECCV）*，2022年。'
- en: '[97] “Google earth studio,” https://earth.google.com/studio/.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] “Google earth studio，” https://earth.google.com/studio/.'
- en: '[98] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan,
    J. T. Barron, and H. Kretzschmar, “Block-nerf: Scalable large scene neural view
    synthesis,” 2022.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan,
    J. T. Barron, 和 H. Kretzschmar，“Block-nerf: 可扩展的大场景神经视角合成，”2022年。'
- en: '[99] L. Xu, Y. Xiangli, S. Peng, X. Pan, N. Zhao, C. Theobalt, B. Dai, and
    D. Lin, “Grid-guided neural radiance fields for large urban scenes,” 2023.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] L. Xu, Y. Xiangli, S. Peng, X. Pan, N. Zhao, C. Theobalt, B. Dai, 和 D.
    Lin，“用于大型城市场景的网格引导神经辐射场，”2023年。'
- en: '[100] H. Turki, D. Ramanan, and M. Satyanarayanan, “Mega-nerf: Scalable construction
    of large-scale nerfs for virtual fly-throughs,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp.
    12 922–12 931.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. Turki, D. Ramanan, 和 M. Satyanarayanan，“Mega-nerf: 可扩展的大规模神经辐射场构建用于虚拟飞行，”在*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年6月，第12,922–12,931页。'
- en: '[101] C. Choi, S. M. Kim, and Y. M. Kim, “Balanced spherical grid for egocentric
    view synthesis,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2023, pp. 16 590–16 599.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C. Choi, S. M. Kim, 和 Y. M. Kim, “平衡球形网格用于自我中心视图合成,” 见于 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2023年6月，pp.
    16 590–16 599。'
- en: '[102] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, and A. Kanazawa, “PlenOctrees
    for real-time rendering of neural radiance fields,” in *ICCV*, 2021.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, 和 A. Kanazawa, “PlenOctrees 用于实时渲染神经辐射场,”
    见于 *ICCV*，2021年。'
- en: '[103] C. Sun, M. Sun, and H. Chen, “Direct voxel grid optimization: Super-fast
    convergence for radiance fields reconstruction,” in *CVPR*, 2022.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] C. Sun, M. Sun, 和 H. Chen, “直接体素网格优化: 对辐射场重建的超快收敛,” 见于 *CVPR*，2022年。'
- en: '[104] L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, and C. Theobalt, “Neural sparse
    voxel fields,” *NeurIPS*, 2020.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, 和 C. Theobalt, “神经稀疏体素场,” *NeurIPS*，2020年。'
- en: '[105] Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan,
    “Blendedmvs: A large-scale dataset for generalized multi-view stereo networks,”
    *Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, 和 L. Quan,
    “Blendedmvs: 用于广义多视图立体网络的大规模数据集,” *计算机视觉与模式识别会议（CVPR）*，2020年。'
- en: '[106] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, and M. Zollhöfer,
    “Deepvoxels: Learning persistent 3d feature embeddings,” in *Proc. Computer Vision
    and Pattern Recognition (CVPR), IEEE*, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, 和 M. Zollhöfer,
    “Deepvoxels: 学习持久的 3D 特征嵌入,” 见于 *计算机视觉与模式识别会议（CVPR），IEEE*，2019年。'
- en: '[107] H. Wang, J. Ren, Z. Huang, K. Olszewski, M. Chai, Y. Fu, and S. Tulyakov,
    “R2l: Distilling neural radiance field to neural light field for efficient novel
    view synthesis,” in *ECCV*, 2022.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Wang, J. Ren, Z. Huang, K. Olszewski, M. Chai, Y. Fu, 和 S. Tulyakov,
    “R2l: 将神经辐射场蒸馏为神经光场以实现高效的新视图合成,” 见于 *ECCV*，2022年。'
- en: '[108] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller, C. R. A. Chaitanya,
    A. Kaplanyan, and M. Steinberger, “Donerf: Towards real-time rendering of compact
    neural radiance fields using depth oracle networks,” *Computer Graphics Forum*,
    vol. 40, no. 4, pp. 45–59, 2021\. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14340'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller, C. R. A. Chaitanya,
    A. Kaplanyan, 和 M. Steinberger, “Donerf: 通过深度预言网络实现紧凑神经辐射场的实时渲染,” *计算机图形学论坛*,
    vol. 40, no. 4, pp. 45–59, 2021年。 [在线]. 可用: https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14340'
- en: '[109] K. Wadhwani and T. Kojima, “SqueezeNeRF: Further factorized FastNeRF
    for memory-efficient inference,” in *2022 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition Workshops (CVPRW)*.   IEEE, jun 2022\. [Online]. Available:
    https://doi.org/10.1109%2Fcvprw56347.2022.00307'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] K. Wadhwani 和 T. Kojima, “SqueezeNeRF: 进一步因式分解的 FastNeRF 用于内存高效推理,” 见于
    *2022 IEEE/CVF 计算机视觉与模式识别会议研讨会（CVPRW）*，IEEE，2022年6月。 [在线]. 可用: https://doi.org/10.1109%2Fcvprw56347.2022.00307'
- en: '[110] Z. Chen, T. Funkhouser, P. Hedman, and A. Tagliasacchi, “Mobilenerf:
    Exploiting the polygon rasterization pipeline for efficient neural field rendering
    on mobile architectures,” in *The Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2023.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Z. Chen, T. Funkhouser, P. Hedman, 和 A. Tagliasacchi, “Mobilenerf: 利用多边形光栅化管线在移动架构上高效渲染神经场,”
    见于 *计算机视觉与模式识别会议（CVPR）*，2023年。'
- en: '[111] Y. Chen, X. Chen, X. Wang, Q. Zhang, Y. Guo, Y. Shan, and F. Wang, “Local-to-global
    registration for bundle-adjusting neural radiance fields,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023, pp.
    8264–8273.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Y. Chen, X. Chen, X. Wang, Q. Zhang, Y. Guo, Y. Shan, 和 F. Wang, “从局部到全球的配准用于束调整神经辐射场,”
    见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023年，pp. 8264–8273。'
- en: '[112] K. Zhang, G. Riegler, N. Snavely, and V. Koltun, “Nerf++: Analyzing and
    improving neural radiance fields,” 2020.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] K. Zhang, G. Riegler, N. Snavely, 和 V. Koltun, “Nerf++: 分析与改进神经辐射场,”
    2020年。'
- en: '[113] D. Rebain, W. Jiang, S. Yazdani, K. Li, K. M. Yi, and A. Tagliasacchi,
    “Derf: Decomposed radiance fields,” in *2021 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2021, pp. 14 148–14 156.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] D. Rebain, W. Jiang, S. Yazdani, K. Li, K. M. Yi, 和 A. Tagliasacchi,
    “Derf: 分解的辐射场,” 见于 *2021 IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2021年，pp. 14 148–14 156。'
- en: '[114] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz,
    and R. Martin-Brualla, “Nerfies: Deformable neural radiance fields,” in *2021
    IEEE/CVF International Conference on Computer Vision (ICCV)*, 2021, pp. 5845–5854.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz,
    和 R. Martin-Brualla, “Nerfies: 可变形的神经辐射场,” 见于 *2021 IEEE/CVF 国际计算机视觉会议（ICCV）*，2021年，pp.
    5845–5854。'
- en: '[115] J. Li, Z. Feng, Q. She, H. Ding, C. Wang, and G. H. Lee, “Mine: Towards
    continuous depth mpi with nerf for novel view synthesis,” in *2021 IEEE/CVF International
    Conference on Computer Vision (ICCV)*, 2021, pp. 12 558–12 568.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Li, Z. Feng, Q. She, H. Ding, C. Wang, 和 G. H. Lee，“Mine: 迈向使用 NeRF
    的连续深度 MPI 进行新视角合成”，在 *2021 IEEE/CVF 国际计算机视觉会议（ICCV）*，2021，第12 558–12 568页。'
- en: '[116] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B. Goldman,
    R. Martin-Brualla, and S. M. Seitz, “Hypernerf: A higher-dimensional representation
    for topologically varying neural radiance fields,” *ACM Trans. Graph.*, vol. 40,
    no. 6, dec 2021.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B. Goldman,
    R. Martin-Brualla, 和 S. M. Seitz，“Hypernerf：用于拓扑变化的神经辐射场的高维表示”，*ACM Trans. Graph.*，第40卷，第6期，2021年12月。'
- en: '[117] T. Chen, P. Wang, Z. Fan, and Z. Wang, “Aug-nerf: Training stronger neural
    radiance fields with triple-level physically-grounded augmentations,” in *2022
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022,
    pp. 15 170–15 181.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] T. Chen, P. Wang, Z. Fan, 和 Z. Wang，“Aug-nerf：通过三层物理基础增强训练更强大的神经辐射场”，在
    *2022 IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2022年，第15 170–15 181页。'
- en: '[118] T. Kaneko, “Ar-nerf: Unsupervised learning of depth and defocus effects
    from natural images with aperture rendering neural radiance fields,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2022, pp. 18 387–18 397.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] T. Kaneko，“Ar-nerf：从自然图像中无监督学习深度和散焦效果的孔径渲染神经辐射场”，在 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2022年6月，第18 387–18 397页。'
- en: '[119] X. Li, C. Hong, Y. Wang, Z. Cao, K. Xian, and G. Lin, “Symmnerf: Learning
    to explore symmetry prior for single-view view synthesis,” in *Proceedings of
    the Asian Conference on Computer Vision (ACCV)*, December 2022, pp. 1726–1742.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] X. Li, C. Hong, Y. Wang, Z. Cao, K. Xian, 和 G. Lin，“Symmnerf：学习探索对称先验用于单视图视角合成”，在
    *亚洲计算机视觉会议（ACCV）*，2022年12月，第1726–1742页。'
- en: '[120] K. Zhou, W. Li, Y. Wang, T. Hu, N. Jiang, X. Han, and J. Lu, “Nerflix:
    High-quality neural view synthesis by learning a degradation-driven inter-viewpoint
    mixer,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2023, pp. 12 363–12 374.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] K. Zhou, W. Li, Y. Wang, T. Hu, N. Jiang, X. Han, 和 J. Lu，“Nerflix：通过学习退化驱动的视点混合器进行高质量神经视图合成”，在
    *IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2023年6月，第12 363–12 374页。'
- en: '[121] C. Sbrolli, P. Cudrano, M. Frosi, and M. Matteucci, “Ic3d: Image-conditioned
    3d diffusion for shape generation,” 2023.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] C. Sbrolli, P. Cudrano, M. Frosi, 和 M. Matteucci，“Ic3d：用于形状生成的图像条件 3D
    扩散”，2023年。'
- en: '[122] J. Gu, Q. Gao, S. Zhai, B. Chen, L. Liu, and J. Susskind, “Learning controllable
    3d diffusion models from single-view images,” 2023.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] J. Gu, Q. Gao, S. Zhai, B. Chen, L. Liu, 和 J. Susskind，“从单视图图像中学习可控的
    3D 扩散模型”，2023年。'
- en: '[123] T. Anciukevičius, Z. Xu, M. Fisher, P. Henderson, H. Bilen, N. J. Mitra,
    and P. Guerrero, “Renderdiffusion: Image diffusion for 3d reconstruction, inpainting
    and generation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2023, pp. 12 608–12 618.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] T. Anciukevičius, Z. Xu, M. Fisher, P. Henderson, H. Bilen, N. J. Mitra,
    和 P. Guerrero，“Renderdiffusion：用于 3D 重建、修补和生成的图像扩散”，在 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2023年6月，第12 608–12 618页。'
- en: '[124] J. Xiang, J. Yang, B. Huang, and X. Tong, “3d-aware image generation
    using 2d diffusion models,” 2023.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. Xiang, J. Yang, B. Huang, 和 X. Tong，“使用 2D 扩散模型的 3D 感知图像生成”，2023年。'
- en: '[125] R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick,
    “Zero-1-to-3: Zero-shot one image to 3d object,” 2023.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, 和 C. Vondrick，“Zero-1-to-3：零样本一张图像到
    3D 物体”，2023年。'
- en: '[126] E. R. Chan, K. Nagano, M. A. Chan, A. W. Bergman, J. J. Park, A. Levy,
    M. Aittala, S. D. Mello, T. Karras, and G. Wetzstein, “Generative novel view synthesis
    with 3d-aware diffusion models,” 2023.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] E. R. Chan, K. Nagano, M. A. Chan, A. W. Bergman, J. J. Park, A. Levy,
    M. Aittala, S. D. Mello, T. Karras, 和 G. Wetzstein，“使用 3D 感知扩散模型生成新视角合成”，2023年。'
- en: '[127] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    *ICLR*, 2021.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.
    Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, 和 N.
    Houlsby，“一张图像价值 16x16 个词：用于大规模图像识别的 Transformer”，*ICLR*，2021年。'
- en: '[128] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017\. [Online]. Available:
    https://arxiv.org/pdf/1706.03762.pdf'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, 和 I. Polosukhin，“注意力机制是你所需要的一切”，2017年。[在线]. 可用链接：https://arxiv.org/pdf/1706.03762.pdf'
- en: '[129] P. Nguyen-Ha, L. Huynh, E. Rahtu, and J. Heikkila, “Sequential view synthesis
    with transformer,” in *Proceedings of the Asian Conference on Computer Vision
    (ACCV)*, November 2020.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] P. Nguyen-Ha, L. Huynh, E. Rahtu, 和 J. Heikkila，“使用变换器的序列视图合成”，发表于*亚洲计算机视觉会议（ACCV）*，2020年11月。'
- en: '[130] J. Yang, Y. Li, and L. Yang, “Shape transformer nets: Generating viewpoint-invariant
    3d shapes from a single image,” *Journal of Visual Communication and Image Representation*,
    vol. 81, p. 103345, 2021\. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1047320321002285'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] J. Yang, Y. Li, 和 L. Yang，“形状变换器网络：从单张图像生成视角不变的3D形状”，*视觉通信与图像表示期刊*，第81卷，第103345页，2021年。[在线].
    可用链接：https://www.sciencedirect.com/science/article/pii/S1047320321002285'
- en: '[131] J. Kulhánek, E. Derner, T. Sattler, and R. Babuška, “Viewformer: Nerf-free
    neural rendering from few images using transformers,” in *European Conference
    on Computer Vision (ECCV)*, 2022.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Kulhánek, E. Derner, T. Sattler, 和 R. Babuška，“Viewformer：使用变换器从少量图像进行NeRF-free神经渲染”，发表于*欧洲计算机视觉会议（ECCV）*，2022年。'
- en: '[132] P. Zhou, L. Xie, B. Ni, and Q. Tian, “Cips-3d: A 3d-aware generator of
    gans based on conditionally-independent pixel synthesis,” 2021.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] P. Zhou, L. Xie, B. Ni, 和 Q. Tian，“CIPS-3D：基于条件独立像素合成的3D感知GAN生成器”，2021年。'
- en: '[133] X. Xu, X. Pan, D. Lin, and B. Dai, “Generative occupancy fields for 3d
    surface-aware image synthesis,” in *Advances in Neural Information Processing
    Systems(NeurIPS)*, 2021.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] X. Xu, X. Pan, D. Lin, 和 B. Dai，“生成占用场用于3D表面感知图像合成”，发表于*神经信息处理系统进展（NeurIPS）*，2021年。'
- en: '[134] Y. Lan, X. Meng, S. Yang, C. C. Loy, and B. Dai, “Self-supervised geometry-aware
    encoder for style-based 3d gan inversion,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp. 20 940–20 949.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Y. Lan, X. Meng, S. Yang, C. C. Loy, 和 B. Dai，“用于风格基础3D GAN反演的自监督几何感知编码器”，发表于*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2023年6月，页码20,940–20,949。'
- en: '[135] S. Li, J. van de Weijer, Y. Wang, F. S. Khan, M. Liu, and J. Yang, “3d-aware
    multi-class image-to-image translation with nerfs,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    12 652–12 662.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] S. Li, J. van de Weijer, Y. Wang, F. S. Khan, M. Liu, 和 J. Yang，“具有NeRF的3D感知多类别图像到图像翻译”，发表于*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2023年6月，页码12,652–12,662。'
- en: '[136] M. Shahbazi, E. Ntavelis, A. Tonioni, E. Collins, D. P. Paudel, M. Danelljan,
    and L. V. Gool, “Nerf-gan distillation for efficient 3d-aware generation with
    convolutions,” 2023.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] M. Shahbazi, E. Ntavelis, A. Tonioni, E. Collins, D. P. Paudel, M. Danelljan,
    和 L. V. Gool，“用于高效3D感知生成的NeRF-GAN蒸馏”，2023年。'
- en: '[137] A. Kania, A. Kasymov, M. Zięba, and P. Spurek, “Hypernerfgan: Hypernetwork
    approach to 3d nerf gan,” 2023.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. Kania, A. Kasymov, M. Zięba, 和 P. Spurek，“HyperNeRF-GAN：3D NeRF GAN的超网络方法”，2023年。'
- en: '[138] A. R. Bhattarai, M. Nießner, and A. Sevastopolsky, “Triplanenet: An encoder
    for eg3d inversion,” 2023.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. R. Bhattarai, M. Nießner, 和 A. Sevastopolsky，“TriPlaneNet：用于EG3D反演的编码器”，2023年。'
- en: '[139] N. Müller, Y. Siddiqui, L. Porzi, S. R. Bulo, P. Kontschieder, and M. Nießner,
    “Diffrf: Rendering-guided 3d radiance field diffusion,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023, pp. 4328–4338.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] N. Müller, Y. Siddiqui, L. Porzi, S. R. Bulo, P. Kontschieder, 和 M. Nießner，“Diffrf:
    渲染指导的3D辐射场扩散”，发表于*IEEE/CVF计算机视觉与模式识别会议*，2023年，页码4328–4338。'
- en: '[140] D. Xu, Y. Jiang, P. Wang, Z. Fan, Y. Wang, and Z. Wang, “Neurallift-360:
    Lifting an in-the-wild 2d photo to a 3d object with 360deg views,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2023, pp. 4479–4489.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] D. Xu, Y. Jiang, P. Wang, Z. Fan, Y. Wang, 和 Z. Wang，“Neurallift-360：将野外2D照片提升为具有360度视角的3D对象”，发表于*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2023年6月，页码4479–4489。'
- en: '[141] H. Chen, J. Gu, A. Chen, W. Tian, Z. Tu, L. Liu, and H. Su, “Single-stage
    diffusion nerf: A unified approach to 3d generation and reconstruction,” 2023.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] H. Chen, J. Gu, A. Chen, W. Tian, Z. Tu, L. Liu, 和 H. Su，“单阶段扩散NeRF：一种统一的3D生成与重建方法”，2023年。'
- en: '[142] J. Gu, A. Trevithick, K.-E. Lin, J. Susskind, C. Theobalt, L. Liu, and
    R. Ramamoorthi, “Nerfdiff: Single-image view synthesis with nerf-guided distillation
    from 3d-aware diffusion,” in *International Conference on Machine Learning*, 2023.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. Gu, A. Trevithick, K.-E. Lin, J. Susskind, C. Theobalt, L. Liu 和 R.
    Ramamoorthi，“NeRFDiff：通过NeRF引导的蒸馏进行单图像视图合成”，发表于*国际机器学习大会*，2023年。'
- en: '[143] D. Wang, X. Cui, S. Salcudean, and Z. J. Wang, “Generalizable neural
    radiance fields for novel view synthesis with transformer,” 2022.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] D. Wang, X. Cui, S. Salcudean 和 Z. J. Wang，“具有变压器的可泛化神经辐射场用于新视角合成”，2022年。'
- en: '[144] K.-E. Lin, L. Yen-Chen, W.-S. Lai, T.-Y. Lin, Y.-C. Shih, and R. Ramamoorthi,
    “Vision transformer for nerf-based view synthesis from a single input image,”
    in *2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)*,
    2023, pp. 806–815.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] K.-E. Lin, L. Yen-Chen, W.-S. Lai, T.-Y. Lin, Y.-C. Shih 和 R. Ramamoorthi，“用于基于NeRF的视图合成的视觉变换器，从单一输入图像”，发表于*2023
    IEEE/CVF计算机视觉应用冬季会议（WACV）*，2023年，页码806–815。'
- en: '[145] J. Liu, Q. Nie, Y. Liu, and C. Wang, “Nerf-loc: Visual localization with
    conditional neural radiance field,” 2023.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Liu, Q. Nie, Y. Liu 和 C. Wang，“NeRF-LOC：基于条件神经辐射场的视觉定位”，2023年。'
- en: '[146] Y. Liao, K. Schwarz, L. Mescheder, and A. Geiger, “Towards unsupervised
    learning of generative models for 3d controllable image synthesis,” in *Proceedings
    IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Y. Liao, K. Schwarz, L. Mescheder 和 A. Geiger，“朝着无监督学习3D可控图像合成的生成模型迈进”，发表于*IEEE计算机视觉与模式识别会议（CVPR）*，2020年。'
- en: '[147] T. Nguyen-Phuoc, C. Richardt, L. Mai, Y.-L. Yang, and N. Mitra, “Blockgan:
    Learning 3d object-aware scene representations from unlabelled images,” in *Advances
    in Neural Information Processing Systems 33*, Nov 2020.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] T. Nguyen-Phuoc, C. Richardt, L. Mai, Y.-L. Yang 和 N. Mitra，“Blockgan：从无标签图像中学习3D对象感知场景表示”，发表于*神经信息处理系统大会第33届*，2020年11月。'
- en: '[148] X. Pan, B. Dai, Z. Liu, C. C. Loy, and P. Luo, “Do 2d gans know 3d shape?
    unsupervised 3d shape reconstruction from 2d image gans,” in *International Conference
    on Learning Representations*, 2021.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] X. Pan, B. Dai, Z. Liu, C. C. Loy 和 P. Luo，“二维GAN是否了解三维形状？基于二维图像GAN的无监督三维形状重建”，发表于*国际学习表征会议*，2021年。'
- en: '[149] A. Tewari, M. B. R, X. Pan, O. Fried, M. Agrawala, and C. Theobalt, “Disentangled3d:
    Learning a 3d generative model with disentangled geometry and appearance from
    monocular images,” in *2022 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*, 2022, pp. 1506–1515.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] A. Tewari, M. B. R, X. Pan, O. Fried, M. Agrawala 和 C. Theobalt，“Disentangled3d：从单目图像中学习一个具有解耦几何和外观的3D生成模型”，发表于*2022
    IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年，页码1506–1515。'
- en: '[150] S. Kobayashi, E. Matsumoto, and V. Sitzmann, “Decomposing nerf for editing
    via feature field distillation,” in *Advances in Neural Information Processing
    Systems*, vol. 35, 2022\. [Online]. Available: https://arxiv.org/pdf/2205.15585.pdf'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Kobayashi, E. Matsumoto 和 V. Sitzmann，“通过特征场蒸馏分解NeRF以进行编辑”，发表于*神经信息处理系统进展*，第35卷，2022年。[在线].
    可用链接: https://arxiv.org/pdf/2205.15585.pdf'
- en: '[151] X. Zhang, A. Kundu, T. Funkhouser, L. Guibas, H. Su, and K. Genova, “Nerflets:
    Local radiance fields for efficient structure-aware 3d scene representation from
    2d supervision,” *CVPR*, 2023.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] X. Zhang, A. Kundu, T. Funkhouser, L. Guibas, H. Su 和 K. Genova，“NeRFlets：用于高效结构感知的2D监督下的局部辐射场3D场景表示”，*CVPR*，2023年。'
- en: '[152] C. Zheng, W. Lin, and F. Xu, “Editablenerf: Editing topologically varying
    neural radiance fields by key points,” 2023.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] C. Zheng, W. Lin 和 F. Xu，“Editablenerf：通过关键点编辑拓扑变化的神经辐射场”，2023年。'
- en: '[153] J. Zhang and L. Yang, “MonodepthPlus: self-supervised monocular depth
    estimation using soft-attention and learnable outlier-masking,” *Journal of Electronic
    Imaging*, vol. 30, no. 2, p. 023017, 2021. [Online]. Available: https://doi.org/10.1117/1.JEI.30.2.023017'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] J. Zhang 和 L. Yang，“MonodepthPlus：使用软注意力和可学习的异常值掩蔽进行自监督单目深度估计”，*电子成像杂志*，第30卷，第2期，页码023017，2021年。[在线].
    可用链接: https://doi.org/10.1117/1.JEI.30.2.023017'
- en: '[154] R. Liang, J. Zhang, H. Li, C. Yang, Y. Guan, and N. Vijaykumar, “Spidr:
    Sdf-based neural point fields for illumination and deformation,” *arXiv preprint
    arXiv:2210.08398*, 2022.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] R. Liang, J. Zhang, H. Li, C. Yang, Y. Guan 和 N. Vijaykumar，“Spidr：基于SDF的神经点场用于照明和变形”，*arXiv预印本arXiv:2210.08398*，2022年。'
- en: '[155] Y. Zhang, X. Huang, B. Ni, T. Li, and W. Zhang, “Frequency-modulated
    point cloud rendering with easy editing,” *arXiv preprint arXiv:2303.07596*, 2023.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Y. Zhang, X. Huang, B. Ni, T. Li 和 W. Zhang，“频率调制点云渲染及简便编辑”，*arXiv预印本arXiv:2303.07596*，2023年。'
- en: '[156] J.-K. Chen, J. Lyu, and Y.-X. Wang, “NeuralEditor: Editing neural radiance
    fields via manipulating point clouds,” in *CVPR*, 2023.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J.-K. Chen, J. Lyu, 和 Y.-X. Wang， “NeuralEditor: 通过操控点云编辑神经辐射场，” 载于 *CVPR*，2023年。'
- en: '[157] J.-Y. Zhu, Z. Zhang, C. Zhang, J. Wu, A. Torralba, J. B. Tenenbaum, and
    W. T. Freeman, “Visual object networks: Image generation with disentangled 3D
    representations,” in *Advances in Neural Information Processing Systems (NeurIPS)*,
    2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] J.-Y. Zhu, Z. Zhang, C. Zhang, J. Wu, A. Torralba, J. B. Tenenbaum, 和
    W. T. Freeman， “视觉对象网络：通过解耦3D表示生成图像，” 载于 *神经信息处理系统进展（NeurIPS）*，2018年。'
- en: '[158] A. Mirzaei, T. Aumentado-Armstrong, M. A. Brubaker, J. Kelly, A. Levinshtein,
    K. G. Derpanis, and I. Gilitschenski, “Reference-guided controllable inpainting
    of neural radiance fields,” 2023.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] A. Mirzaei, T. Aumentado-Armstrong, M. A. Brubaker, J. Kelly, A. Levinshtein,
    K. G. Derpanis, 和 I. Gilitschenski， “参考引导的可控神经辐射场修补，” 2023年。'
- en: '[159] Y. Yin, Z. Fu, F. Yang, and G. Lin, “Or-nerf: Object removing from 3d
    scenes guided by multiview segmentation with neural radiance fields,” 2023.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Y. Yin, Z. Fu, F. Yang, 和 G. Lin， “Or-nerf: 由多视图分割引导的3D场景对象去除，” 2023年。'
- en: '[160] H. G. Kim, M. Park, S. Lee, S. Kim, and Y. M. Ro, “Visual comfort aware-reinforcement
    learning for depth adjustment of stereoscopic 3d images,” *Proceedings of the
    AAAI Conference on Artificial Intelligence*, vol. 35, no. 2, pp. 1762–1770, May
    2021\. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/16270'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] H. G. Kim, M. Park, S. Lee, S. Kim, 和 Y. M. Ro， “视觉舒适感感知强化学习用于立体3D图像的深度调整，”
    *AAAI人工智能会议论文集*，第35卷，第2期，页1762–1770，2021年5月。 [在线]. 可用: https://ojs.aaai.org/index.php/AAAI/article/view/16270'
- en: '[161] R.-F. Jheng, T.-H. Wu, J.-F. Yeh, and W. H. Hsu, “Free-form 3d scene
    inpainting with dual-stream gan,” in *British Machine Vision Conference*, 2022.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] R.-F. Jheng, T.-H. Wu, J.-F. Yeh, 和 W. H. Hsu， “具有双流GAN的自由形式3D场景修补，”
    载于 *英国机器视觉会议*，2022年。'
- en: '[162] Q. Wang, Y. Wang, M. Birsak, and P. Wonka, “Blobgan-3d: A spatially-disentangled
    3d-aware generative model for indoor scenes,” 2023.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Q. Wang, Y. Wang, M. Birsak, 和 P. Wonka， “Blobgan-3d: 一种空间解耦的3D感知生成模型用于室内场景，”
    2023年。'
- en: '[163] J. Gu, L. Liu, P. Wang, and C. Theobalt, “Stylenerf: A style-based 3d
    aware generator for high-resolution image synthesis,” in *International Conference
    on Learning Representations*, 2022.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] J. Gu, L. Liu, P. Wang, 和 C. Theobalt， “Stylenerf: 一种基于风格的3D感知生成器用于高分辨率图像合成，”
    载于 *国际学习表示会议*，2022年。'
- en: '[164] C. Wang, M. Chai, M. He, D. Chen, and J. Liao, “Clip-nerf: Text-and-image
    driven manipulation of neural radiance fields,” in *2022 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*, 2022, pp. 3825–3834.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] C. Wang, M. Chai, M. He, D. Chen, 和 J. Liao， “Clip-nerf: 由文本和图像驱动的神经辐射场操控，”
    载于 *2022 IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年，页3825–3834。'
- en: '[165] K. Kania, K. M. Yi, M. Kowalski, T. Trzciński, and A. Tagliasacchi, “CoNeRF:
    Controllable Neural Radiance Fields,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2022.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] K. Kania, K. M. Yi, M. Kowalski, T. Trzciński, 和 A. Tagliasacchi， “CoNeRF:
    可控神经辐射场，” 载于 *IEEE计算机视觉与模式识别会议论文集*，2022年。'
- en: '[166] V. Lazova, V. Guzov, K. Olszewski, S. Tulyakov, and G. Pons-Moll, “Control-nerf:
    Editable feature volumes for scene rendering and manipulation,” in *2023 IEEE/CVF
    Winter Conference on Applications of Computer Vision (WACV)*, 2023, pp. 4329–4339.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] V. Lazova, V. Guzov, K. Olszewski, S. Tulyakov, 和 G. Pons-Moll， “Control-nerf:
    可编辑特征体用于场景渲染和操控，” 载于 *2023 IEEE/CVF计算机视觉应用冬季会议（WACV）*，2023年，页4329–4339。'
- en: '[167] Y.-J. Yuan, Y.-T. Sun, Y.-K. Lai, Y. Ma, R. Jia, and L. Gao, “Nerf-editing:
    Geometry editing of neural radiance fields,” in *2022 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2022, pp. 18 332–18 343.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Y.-J. Yuan, Y.-T. Sun, Y.-K. Lai, Y. Ma, R. Jia, 和 L. Gao， “Nerf-editing:
    神经辐射场的几何编辑，” 载于 *2022 IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年，页18 332–18 343。'
- en: '[168] C. Sun, Y. Liu, J. Han, and S. Gould, “Nerfeditor: Differentiable style
    decomposition for full 3d scene editing,” *arXiv preprint arXiv:2212.03848*, 2022.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] C. Sun, Y. Liu, J. Han, 和 S. Gould， “Nerfeditor: 可微分风格分解用于完整3D场景编辑，”
    *arXiv预印本 arXiv:2212.03848*，2022年。'
- en: '[169] Z. Wang, Y. Deng, J. Yang, J. Yu, and X. Tong, “Generative Deformable
    Radiance Fields for Disentangled Image Synthesis of Topology-Varying Objects,”
    *Computer Graphics Forum*, 2022.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Z. Wang, Y. Deng, J. Yang, J. Yu, 和 X. Tong， “生成可变形辐射场用于拓扑变化对象的解耦图像合成，”
    *计算机图形学论坛*，2022年。'
- en: '[170] K. Tertikas, D. Paschalidou, B. Pan, J. J. Park, M. A. Uy, I. Emiris,
    Y. Avrithis, and L. Guibas, “Partnerf: Generating part-aware editable 3d shapes
    without 3d supervision,” in *Proceedings IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR)*, 2023.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] K. Tertikas, D. Paschalidou, B. Pan, J. J. Park, M. A. Uy, I. Emiris,
    Y. Avrithis 和 L. Guibas，“Partnerf：生成无3D监督的部分感知可编辑3D形状，”发表于 *IEEE计算机视觉与模式识别会议（CVPR）*，2023年。'
- en: '[171] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui,
    “Sine: Semantic-driven image-based nerf editing with prior-guided editing field,”
    in *The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)*, 2023.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang 和 Z. Cui，“Sine：基于语义驱动的图像基础NeRF编辑与先验引导的编辑场，”发表于
    *IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2023年。'
- en: '[172] D. Cohen-Bar, E. Richardson, G. Metzer, R. Giryes, and D. Cohen-Or, “Set-the-scene:
    Global-local training for generating controllable nerf scenes,” 2023.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] D. Cohen-Bar, E. Richardson, G. Metzer, R. Giryes 和 D. Cohen-Or，“Set-the-scene：生成可控NeRF场景的全局-局部训练，”2023年。'
- en: '[173] A. Mirzaei, T. Aumentado-Armstrong, K. G. Derpanis, J. Kelly, M. A. Brubaker,
    I. Gilitschenski, and A. Levinshtein, “SPIn-NeRF: Multiview segmentation and perceptual
    inpainting with neural radiance fields,” in *CVPR*, 2023.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] A. Mirzaei, T. Aumentado-Armstrong, K. G. Derpanis, J. Kelly, M. A. Brubaker,
    I. Gilitschenski 和 A. Levinshtein，“SPIn-NeRF：基于神经辐射场的多视角分割与感知修复，”发表于 *CVPR*，2023年。'
- en: '[174] O. Avrahami, D. Lischinski, and O. Fried, “Blended diffusion for text-driven
    editing of natural images,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2022, pp. 18 208–18 218.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] O. Avrahami, D. Lischinski 和 O. Fried，“基于文本驱动的自然图像混合扩散编辑，”发表于 *IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年6月，第18,208–18,218页。'
- en: '[175] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever,
    and M. Chen, “Glide: Towards photorealistic image generation and editing with
    text-guided diffusion models,” 2022.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I.
    Sutskever 和 M. Chen，“Glide：基于文本引导的逼真图像生成与编辑的扩散模型，”2022年。'
- en: '[176] G. Couairon, J. Verbeek, H. Schwenk, and M. Cord, “Diffedit: Diffusion-based
    semantic image editing with mask guidance,” 2022.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] G. Couairon, J. Verbeek, H. Schwenk 和 M. Cord，“Diffedit：基于扩散的语义图像编辑与掩码指导，”2022年。'
- en: '[177] E. Sella, G. Fiebelman, P. Hedman, and H. Averbuch-Elor, “Vox-e: Text-guided
    voxel editing of 3d objects,” 2023.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] E. Sella, G. Fiebelman, P. Hedman 和 H. Averbuch-Elor，“Vox-e：基于文本引导的3D对象体素编辑，”2023年。'
- en: '[178] A. Haque, M. Tancik, A. Efros, A. Holynski, and A. Kanazawa, “Instruct-nerf2nerf:
    Editing 3d scenes with instructions,” 2023.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] A. Haque, M. Tancik, A. Efros, A. Holynski 和 A. Kanazawa，“Instruct-nerf2nerf：用指令编辑3D场景，”2023年。'
- en: '[179] Y. Lin, H. Bai, S. Li, H. Lu, X. Lin, H. Xiong, and L. Wang, “Componerf:
    Text-guided multi-object compositional nerf with editable 3d scene layout,” 2023.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Y. Lin, H. Bai, S. Li, H. Lu, X. Lin, H. Xiong 和 L. Wang，“Componerf：基于文本引导的多对象组成NeRF与可编辑的3D场景布局，”2023年。'
- en: '[180] R. Martin-Brualla, N. Radwan, M. S. M. Sajjadi, J. T. Barron, A. Dosovitskiy,
    and D. Duckworth, “NeRF in the Wild: Neural Radiance Fields for Unconstrained
    Photo Collections,” in *CVPR*, 2021.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] R. Martin-Brualla, N. Radwan, M. S. M. Sajjadi, J. T. Barron, A. Dosovitskiy
    和 D. Duckworth，“NeRF在野外：用于无约束照片集合的神经辐射场，”发表于 *CVPR*，2021年。'
- en: '[181] M. Boss, A. Engelhardt, A. Kar, Y. Li, D. Sun, J. T. Barron, H. P. Lensch,
    and V. Jampani, “SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary
    Image collections,” in *Advances in Neural Information Processing Systems (NeurIPS)*,
    2022.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] M. Boss, A. Engelhardt, A. Kar, Y. Li, D. Sun, J. T. Barron, H. P. Lensch
    和 V. Jampani，“SAMURAI：从不受限的真实世界任意图像集合中提取形状和材料，”发表于 *神经信息处理系统进展（NeurIPS）*，2022年。'
- en: '[182] C. Choi, J. Kim, and Y. M. Kim, “Ibl-nerf: Image-based lighting formulation
    of neural radiance fields,” *arXiv preprint arXiv:2210.08202*, 2022.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] C. Choi, J. Kim 和 Y. M. Kim，“Ibl-nerf：神经辐射场的图像基础光照公式，” *arXiv预印本arXiv:2210.08202*，2022年。'
- en: '[183] Z. Yan, C. Li, and G. H. Lee, “Nerf-ds: Neural radiance fields for dynamic
    specular objects,” *CVPR*, 2023.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Z. Yan, C. Li 和 G. H. Lee，“Nerf-ds：动态高光物体的神经辐射场，” *CVPR*，2023年。'
- en: '[184] D. Guo, L. Zhu, S. Ling, T. Li, G. Zhang, Q. Yang, P. Wang, S. Jiang,
    S. Wu, and J. Liu, “Face illumination normalization based on generative adversarial
    network,” *Natural Computing: An International Journal*, vol. 22, no. 1, pp. 105–117,
    jul 2022\. [Online]. Available: https://doi.org/10.1007/s11047-022-09892-4'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] D. Guo, L. Zhu, S. Ling, T. Li, G. Zhang, Q. Yang, P. Wang, S. Jiang,
    S. Wu 和 J. Liu，“基于生成对抗网络的面部照明归一化，” *自然计算：国际期刊*，第22卷，第1期，第105–117页，2022年7月。[在线].
    可用链接：https://doi.org/10.1007/s11047-022-09892-4'
- en: '[185] Z. Cui, L. Gu, X. Sun, Y. Qiao, and T. Harada, “Aleth-nerf: Low-light
    condition view synthesis with concealing fields,” 2023.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Z. Cui、L. Gu、X. Sun、Y. Qiao 和 T. Harada，"Aleth-nerf: 低光条件视图合成与隐匿场"，2023。'
- en: '[186] N. Abirami R. and D. R. Vincent P. M., “Low-light image enhancement based
    on generative adversarial network,” *Frontiers in Genetics*, vol. 12, 2021\. [Online].
    Available: https://www.frontiersin.org/articles/10.3389/fgene.2021.799777'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] N. Abirami R. 和 D. R. Vincent P. M.，"基于生成对抗网络的低光图像增强"，*遗传学前沿*，第12卷，2021年。[在线].
    可用: https://www.frontiersin.org/articles/10.3389/fgene.2021.799777'
- en: '[187] W. Kim, R. Lee, M. Park, and S.-H. Lee, “Low-light image enhancement
    based on maximal diffusion values,” *IEEE Access*, vol. 7, pp. 129 150–129 163,
    2019.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] W. Kim、R. Lee、M. Park 和 S.-H. Lee，"基于最大扩散值的低光图像增强"，*IEEE Access*，第7卷，第129,150–129,163页，2019。'
- en: '[188] P. Ponglertnapakorn, N. Tritrong, and S. Suwajanakorn, “Difareli: Diffusion
    face relighting,” *arXiv preprint arXiv:2304.09479*, 2023.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] P. Ponglertnapakorn、N. Tritrong 和 S. Suwajanakorn，"Difareli: 扩散面部重光照"，*arXiv
    预印本 arXiv:2304.09479*，2023。'
- en: '[189] M. Guo, A. Fathi, J. Wu, and T. Funkhouser, “Object-centric neural scene
    rendering,” *arXiv preprint arXiv:2012.08503*, 2020.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] M. Guo、A. Fathi、J. Wu 和 T. Funkhouser，"以对象为中心的神经场景渲染"，*arXiv 预印本 arXiv:2012.08503*，2020。'
- en: '[190] Y. Wang, W. gang Zhou, Z. Lu, and H. Li, “Udoc-gan: Unpaired document
    illumination correction with background light prior,” *Proceedings of the 30th
    ACM International Conference on Multimedia*, 2022.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Y. Wang、W. gang Zhou、Z. Lu 和 H. Li，"Udoc-gan: 无配对文档光照校正与背景光先验"，*第30届
    ACM 国际多媒体会议论文集*，2022。'
- en: '[191] J. Ling, Z. Wang, and F. Xu, “Shadowneus: Neural sdf reconstruction by
    shadow ray supervision,” *ArXiv*, vol. abs/2211.14086, 2022.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] J. Ling、Z. Wang 和 F. Xu，"Shadowneus: 通过阴影射线监督的神经 sdf 重建"，*ArXiv*，卷 abs/2211.14086，2022。'
- en: '[192] V. Rudnev, M. Elgharib, W. Smith, L. Liu, V. Golyanik, and C. Theobalt,
    “Nerf for outdoor scene relighting,” in *European Conference on Computer Vision
    (ECCV)*, 2022.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] V. Rudnev、M. Elgharib、W. Smith、L. Liu、V. Golyanik 和 C. Theobalt，"户外场景重光照的
    Nerf"，见于*欧洲计算机视觉会议（ECCV）*，2022。'
- en: '[193] C. Higuera, B. Boots, and M. Mukadam, “Learning to read braille: Bridging
    the tactile reality gap with diffusion models,” *CoRR*, vol. abs/2304.01182, 2023\.
    [Online]. Available: https://doi.org/10.48550/arXiv.2304.01182'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] C. Higuera、B. Boots 和 M. Mukadam，"学习阅读盲文: 通过扩散模型弥合触觉现实差距"，*CoRR*，卷 abs/2304.01182，2023年。[在线].
    可用: https://doi.org/10.48550/arXiv.2304.01182'
- en: '[194] Y.-C. Guo, D. Kang, L. Bao, Y. He, and S.-H. Zhang, “Nerfren: Neural
    radiance fields with reflections,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp. 18 409–18 418.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Y.-C. Guo、D. Kang、L. Bao、Y. He 和 S.-H. Zhang，"Nerfren: 带有反射的神经辐射场"，见于*IEEE/CVF
    计算机视觉与模式识别会议（CVPR）*，2022年6月，第18,409–18,418页。'
- en: '[195] C. LeGendre, W.-C. Ma, G. Fyffe, J. Flynn, L. Charbonnel, J. Busch, and
    P. Debevec, “Deeplight: Learning illumination for unconstrained mobile mixed reality,”
    in *2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    2019, pp. 5911–5921.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] C. LeGendre、W.-C. Ma、G. Fyffe、J. Flynn、L. Charbonnel、J. Busch 和 P. Debevec，"Deeplight:
    用于无约束移动混合现实的光照学习"，见于*2019 IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2019，第5911–5921页。'
- en: '[196] W. Ye, S. Chen, C. Bao, H. Bao, M. Pollefeys, Z. Cui, and G. Zhang, “Intrinsicnerf:
    Learning intrinsic neural radiance fields for editable novel view synthesis,”
    2022.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] W. Ye、S. Chen、C. Bao、H. Bao、M. Pollefeys、Z. Cui 和 G. Zhang，"Intrinsicnerf:
    学习可编辑的新颖视图合成的内在神经辐射场"，2022。'
- en: '[197] M. Boss, V. Jampani, R. Braun, C. Liu, J. T. Barron, and H. P. Lensch,
    “Neural-pil: Neural pre-integrated lighting for reflectance decomposition,” in
    *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] M. Boss、V. Jampani、R. Braun、C. Liu、J. T. Barr 和 H. P. Lensch，"Neural-pil:
    反射分解的神经预积分光照"，见于*神经信息处理系统进展（NeurIPS）*，2021。'
- en: '[198] S. Saito, T. Simon, J. Saragih, and H. Joo, “Pifuhd: Multi-level pixel-aligned
    implicit function for high-resolution 3d human digitization,” in *2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020, pp. 81–90.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] S. Saito、T. Simon、J. Saragih 和 H. Joo，"Pifuhd: 高分辨率 3D 人体数字化的多级像素对齐隐式函数"，见于*2020
    IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2020，第81–90页。'
- en: '[199] H. Tang, S. Bai, L. Zhang, P. H. S. Torr, and N. Sebe, “Xinggan for person
    image generation,” in *Computer Vision – ECCV 2020*, A. Vedaldi, H. Bischof, T. Brox,
    and J.-M. Frahm, Eds.   Cham: Springer International Publishing, 2020, pp. 717–734.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] H. Tang、S. Bai、L. Zhang、P. H. S. Torr 和 N. Sebe，"Xinggan用于人物图像生成"，见于*计算机视觉
    – ECCV 2020*，A. Vedaldi、H. Bischof、T. Brox 和 J.-M. Frahm 编，Cham: Springer International
    Publishing，2020，第717–734页。'
- en: '[200] Y. Ren, X. Yu, J. Chen, T. H. Li, and G. Li, “Deep image spatial transformation
    for person image generation,” in *2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2020, pp. 7687–7696.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Y. Ren, X. Yu, J. Chen, T. H. Li, 和 G. Li，“用于人物图像生成的深度图像空间变换，” 见 *2020
    IEEE/CVF 计算机视觉与模式识别大会 (CVPR)*，2020 年，第 7687–7696 页。'
- en: '[201] Y. Liu, Z. Qin, T. Wan, and Z. Luo, “Auto-painter: Cartoon image generation
    from sketch by using conditional wasserstein generative adversarial networks,”
    *Neurocomputing*, vol. 311, pp. 78–87, 2018\. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0925231218306209'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Y. Liu, Z. Qin, T. Wan, 和 Z. Luo，“自动画家：通过条件 Wasserstein 生成对抗网络从草图生成卡通图像，”
    *神经计算*，第 311 卷，第 78–87 页，2018 年。 [在线]. 可用: https://www.sciencedirect.com/science/article/pii/S0925231218306209'
- en: '[202] H. Li, “Ai synthesis for the metaverse: From avatars to 3d scenes,” 2022.
    [Online]. Available: https://talks.stanford.edu/hao-li-pinscreen-on-ai-synthesis-for-the-metaverse-from-avatars-to-3d-scenes/'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] H. Li，“元宇宙中的 AI 合成：从虚拟角色到 3D 场景，” 2022 年。 [在线]. 可用: https://talks.stanford.edu/hao-li-pinscreen-on-ai-synthesis-for-the-metaverse-from-avatars-to-3d-scenes/'
- en: '[203] “Mapping gothic france,” https://mcid.mcah.columbia.edu/art-atlas/mapping-gothic,
    accessed: 2023-06-03\. [Online]. Available: https://mcid.mcah.columbia.edu/art-atlas/mapping-gothic'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] “哥特式法国地图，” https://mcid.mcah.columbia.edu/art-atlas/mapping-gothic，访问日期：2023-06-03。
    [在线]. 可用: https://mcid.mcah.columbia.edu/art-atlas/mapping-gothic'
- en: '[204] X. Yuejia, L. Chuanhao, L. Qingdazhu, Y. Xiaocui, L. Bo, and J. Meizhi,
    “A creative industry image generation dataset based on captions,” 2022.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] X. Yuejia, L. Chuanhao, L. Qingdazhu, Y. Xiaocui, L. Bo, 和 J. Meizhi，“基于字幕的创意产业图像生成数据集，”
    2022 年。'
- en: '[205] C. Tatsch, J. A. B. Jnr, D. Covell, I. B. Tulu, and Y. Gu, “Rhino: An
    autonomous robot for mapping underground mine environments,” 2023.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] C. Tatsch, J. A. B. Jnr, D. Covell, I. B. Tulu, 和 Y. Gu，“犀牛：一种用于绘制地下矿环境的自主机器人，”
    2023 年。'
- en: '[206] Y. Tian, L. Li, A. Fumagalli, Y. Tadesse, and B. Prabhakaran, “Haptic-enabled
    mixed reality system for mixed-initiative remote robot control,” 2021.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Y. Tian, L. Li, A. Fumagalli, Y. Tadesse, 和 B. Prabhakaran，“具触觉功能的混合现实系统用于混合主动式远程机器人控制，”
    2021 年。'
- en: '[207] G. Pu, Y. Men, Y. Mao, Y. Jiang, W.-Y. Ma, and Z. Lian, “Controllable
    image synthesis with attribute-decomposed gan,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 45, no. 2, pp. 1514–1532, 2023.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] G. Pu, Y. Men, Y. Mao, Y. Jiang, W.-Y. Ma, 和 Z. Lian，“具有属性分解 GAN 的可控图像合成，”
    *IEEE 模式分析与机器智能学报*，第 45 卷，第 2 期，第 1514–1532 页，2023 年。'
- en: '[208] X. Wu, Y. Zhang, Q. Li, Y. Qi, J. Wang, and Y. Guo, “Face aging with
    pixel-level alignment gan,” *Applied Intelligence*, vol. 52, no. 11, p. 14665–14678,
    2022.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] X. Wu, Y. Zhang, Q. Li, Y. Qi, J. Wang, 和 Y. Guo，“通过像素级对齐 GAN 进行面部衰老，”
    *应用智能*，第 52 卷，第 11 期，第 14665–14678 页，2022 年。'
- en: '[209] D. Sero, A. Zaidi, J. Li, J. D. White, T. B. Gonz’alez Zarzar, M. L.
    Marazita, S. M. Weinberg, P. Suetens, D. Vandermeulen, J. K. Wagner *et al.*,
    “Facial recognition from dna using face-to-dna classifiers,” *Nature communications*,
    vol. 10, no. 1, p. 1–12, 2019.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] D. Sero, A. Zaidi, J. Li, J. D. White, T. B. Gonz’alez Zarzar, M. L.
    Marazita, S. M. Weinberg, P. Suetens, D. Vandermeulen, J. K. Wagner *等人*，“通过面部到
    DNA 分类器进行面部识别，” *自然通讯*，第 10 卷，第 1 期，第 1–12 页，2019 年。'
- en: '[210] M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wistuba,
    V. Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig, I. Molloy, and B. Edwards, “Adversarial
    robustness toolbox v1.0.0,” https://github.com/Trusted-AI/adversarial-robustness-toolbox,
    2018.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wistuba,
    V. Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig, I. Molloy, 和 B. Edwards，“对抗鲁棒性工具箱
    v1.0.0，” https://github.com/Trusted-AI/adversarial-robustness-toolbox，2018 年。'
