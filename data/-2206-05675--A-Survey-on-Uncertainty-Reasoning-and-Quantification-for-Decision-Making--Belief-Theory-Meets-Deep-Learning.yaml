- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:45:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:45:41'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2206.05675] A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2206.05675] 关于不确定性推理与量化在决策中的应用调研：信念理论与深度学习的结合'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2206.05675](https://ar5iv.labs.arxiv.org/html/2206.05675)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2206.05675](https://ar5iv.labs.arxiv.org/html/2206.05675)
- en: 'A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于不确定性推理与量化在决策中的应用调研：信念理论与深度学习的结合
- en: Zhen Guo*, Zelin Wan*, Qisheng Zhang [zguo, zelin, qishengz19@vt.edu](mailto:zguo,%20zelin,%20qishengz19@vt.edu)
    [https://orcid.org/0000-0002-6563-5934, https://orcid.org/0000-0001-5293-0363,
    https://orcid.org/0000-0001-8785-8437](https://orcid.org/https://orcid.org/0000-0002-6563-5934,%20https://orcid.org/0000-0001-5293-0363,%20https://orcid.org/0000-0001-8785-8437
    "ORCID identifier") Department of Computer ScienceVirginia Tech7054 Haycock RoadFalls
    ChurchVAUSA22043 ,  Xujiang Zhao*, Feng Chen [xujiang.zhao, feng.chen@utdallas.edu](mailto:xujiang.zhao,%20feng.chen@utdallas.edu%20)
    [https://orcid.org/0000-0003-4950-4018, https://orcid.org/0000-0002-4508-5963](https://orcid.org/https://orcid.org/0000-0003-4950-4018,%20https://orcid.org/0000-0002-4508-5963
    "ORCID identifier") Department of Computer ScienceUniversity of Texas at Dallas800
    W Campbell RdRichardsonTXUSA75080 ,  Jin-Hee Cho, Qi Zhang [jicho, qiz21@vt.edu](mailto:jicho,%20qiz21@vt.edu)
    [https://orcid.org/ 0000-0002-5908-4662, https://orcid.org/0000-0002-3607-3258](https://orcid.org/https://orcid.org/%0A0000-0002-5908-4662,%20https://orcid.org/0000-0002-3607-3258
    "ORCID identifier") Department of Computer ScienceVirginia Tech7054 Haycock RoadFalls
    ChurchVAUSA22043 ,  Lance M. Kaplan [lance.m.kaplan.civ@army.mil](mailto:lance.m.kaplan.civ@army.mil)
    [https://orcid.org/0000-0002-3627-4471, https://orcid.org/](https://orcid.org/https://orcid.org/0000-0002-3627-4471,%20https://orcid.org/
    "ORCID identifier") US Army Research Laboratory2800 Powder Mill Rd.AdelphiMDUSA20783
    ,  Dong H. Jeong [djeong@udc.edu](mailto:djeong@udc.edu) [https://orcid.org/0000-0001-5271-293X](https://orcid.org/https://orcid.org/0000-0001-5271-293X
    "ORCID identifier") Department of Computer Science of Information TechnologyUniversity
    of the District of Columbia4200 Connecticut Ave NWWashington, DCUSA20008  and 
    Audun Jøsang [audun.josang@mn.uio.no](mailto:audun.josang@mn.uio.no) [https://orcid.org/0000-0001-6337-2264](https://orcid.org/https://orcid.org/0000-0001-6337-2264
    "ORCID identifier") Department of InformaticsUniversity of OsloOle-Johan Dahls
    hus GaustadalléenOsloNorway23b 0373(2022)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 祁生张*, 朱凌万*, 郑国 [zguo, zelin, qishengz19@vt.edu](mailto:zguo,%20zelin,%20qishengz19@vt.edu)
    [https://orcid.org/0000-0002-6563-5934, https://orcid.org/0000-0001-5293-0363,
    https://orcid.org/0000-0001-8785-8437](https://orcid.org/https://orcid.org/0000-0002-6563-5934,%20https://orcid.org/0000-0001-5293-0363,%20https://orcid.org/0000-0001-8785-8437
    "ORCID identifier") 计算机科学系，弗吉尼亚理工大学，7054 Haycock Road, Falls Church, VA, USA 22043，徐江赵*,
    冯晨 [xujiang.zhao, feng.chen@utdallas.edu](mailto:xujiang.zhao,%20feng.chen@utdallas.edu%20)
    [https://orcid.org/0000-0003-4950-4018, https://orcid.org/0000-0002-4508-5963](https://orcid.org/https://orcid.org/0000-0003-4950-4018,%20https://orcid.org/0000-0002-4508-5963
    "ORCID identifier") 计算机科学系，德克萨斯大学达拉斯分校，800 W Campbell Rd, Richardson, TX, USA
    75080，珍-希·赵, 齐张 [jicho, qiz21@vt.edu](mailto:jicho,%20qiz21@vt.edu) [https://orcid.org/0000-0002-5908-4662,
    https://orcid.org/0000-0002-3607-3258](https://orcid.org/https://orcid.org/%0A0000-0002-5908-4662,%20https://orcid.org/0000-0002-3607-3258
    "ORCID identifier") 计算机科学系，弗吉尼亚理工大学，7054 Haycock Road, Falls Church, VA, USA 22043，兰斯·M·卡普兰
    [lance.m.kaplan.civ@army.mil](mailto:lance.m.kaplan.civ@army.mil) [https://orcid.org/0000-0002-3627-4471,
    https://orcid.org/](https://orcid.org/https://orcid.org/0000-0002-3627-4471,%20https://orcid.org/
    "ORCID identifier") 美国陆军研究实验室，2800 Powder Mill Rd, Adelphi, MD, USA 20783，董浩中
    [djeong@udc.edu](mailto:djeong@udc.edu) [https://orcid.org/0000-0001-5271-293X](https://orcid.org/https://orcid.org/0000-0001-5271-293X
    "ORCID identifier") 华盛顿特区大学计算机科学与信息技术系，4200 Connecticut Ave NW, Washington, DC,
    USA 20008，奥登·约桑 [audun.josang@mn.uio.no](mailto:audun.josang@mn.uio.no) [https://orcid.org/0000-0001-6337-2264](https://orcid.org/https://orcid.org/0000-0001-6337-2264
    "ORCID identifier") 奥斯陆大学信息学系，Ole-Johan Dahls hus, Gaustadalléen, Oslo, Norway
    23b 0373 (2022)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: An in-depth understanding of uncertainty is the first step to making effective
    decisions under uncertainty. Deep/machine learning (ML/DL) has been hugely leveraged
    to solve complex problems involved with processing high-dimensional data. However,
    reasoning and quantifying different types of uncertainties to achieve effective
    decision-making have been much less explored in ML/DL than in other Artificial
    Intelligence (AI) domains. In particular, belief/evidence theories have been studied
    in KRR since the 1960s to reason and measure uncertainties to enhance decision-making
    effectiveness. We found that only a few studies have leveraged the mature uncertainty
    research in belief/evidence theories in ML/DL to tackle complex problems under
    different types of uncertainty. In this survey paper, we discuss several popular
    belief theories and their core ideas dealing with uncertainty causes and types
    and quantifying them, along with the discussions of their applicability in ML/DL.
    In addition, we discuss three main approaches that leverage belief theories in
    Deep Neural Networks (DNNs), including Evidential DNNs, Fuzzy DNNs, and Rough
    DNNs, in terms of their uncertainty causes, types, and quantification methods
    along with their applicability in diverse problem domains. Based on our in-depth
    survey, we discuss insights, lessons learned, limitations of the current state-of-the-art
    bridging belief theories and ML/DL, and finally, future research directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对不确定性的深入理解是有效决策的第一步。深度/机器学习（ML/DL）在解决处理高维数据的复杂问题方面得到了巨大的应用。然而，在ML/DL中，对不确定性进行推理和量化以实现有效决策的研究远不如其他人工智能（AI）领域那么广泛。特别是，信念/证据理论自1960年代以来在知识表示和推理（KRR）中被研究，用于推理和衡量不确定性，以提升决策有效性。我们发现，只有少数研究利用了信念/证据理论中的成熟不确定性研究，在ML/DL中应对不同类型的不确定性问题。在这篇综述论文中，我们讨论了几种流行的信念理论及其处理不确定性原因和类型的核心思想，并量化这些不确定性，同时讨论它们在ML/DL中的适用性。此外，我们讨论了三种主要的方法，它们在深度神经网络（DNNs）中利用信念理论，包括证据DNNs、模糊DNNs和粗糙DNNs，涉及其不确定性原因、类型和量化方法，以及它们在不同问题领域中的适用性。基于我们的深入调查，我们讨论了当前信念理论与ML/DL结合的见解、经验教训、局限性，以及未来的研究方向。
- en: 'Belief theory, uncertainty reasoning, uncertainty quantification, decision
    making, machine/deep learning^†^†copyright: acmcopyright^†^†journalyear: 2022^†^†ccs:
    Computing methodologies Knowledge representation and reasoning^†^†ccs: Computing
    methodologies Machine learning algorithms'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 信念理论、不确定性推理、不确定性量化、决策制定、机器/深度学习
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 1.1\. Motivation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 动机
- en: In all sorts of business processes and our private life, we are confronted with
    various kinds of decisions involving multiple choices and relative uncertainty.
    A clear understanding of the uncertainty is a prerequisite of sound and effective
    decision making. Although the topic of reasoning and decision making under uncertainty
    has been studied for decades in various artificial intelligence (AI) domains,
    including belief/evidence theory, game theory, and machine/deep learning (ML/DL),
    the different manifestations of uncertainty based on its root causes have not
    been investigated in-depth. The era of the Internet and Big Data has brought a
    flood of information which can be leveraged for decision making. Under such a
    situation, the challenge for timely, accurate decision making is no longer the
    lack of information, but the risk from a lack of understanding and managing inherent
    uncertainty resulting from unreliable, incomplete, deceptive, and conflicting
    information.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种业务流程和我们的私人生活中，我们面临着涉及多个选择和相对不确定性的决策。清晰理解不确定性是做出合理和有效决策的前提。尽管关于不确定性下的推理和决策的主题在包括信念/证据理论、博弈论和机器/深度学习（ML/DL）在内的各种人工智能（AI）领域中已经研究了几十年，但基于其根本原因的不确定性不同表现形式尚未深入研究。互联网和大数据时代带来了大量信息，可以用于决策。在这种情况下，及时、准确的决策挑战不再是信息的缺乏，而是对来自不可靠、不完整、具有误导性和冲突信息的不确定性缺乏理解和管理的风险。
- en: In AI, a series of belief or evidence theories have a long history studying
    reasoning and/or decision making under uncertainty. However, there has been still
    limited understanding since uncertainty is not caused only by a lack of evidence
    or unpredictability. In addition, ML/DL algorithms have considered uncertainty
    (e.g., aleatoric or epistemic uncertainty) to offer solutions for effective decision
    making. However, there has been no common, solid understanding of multidimensional
    uncertainty where each domain has a different and/or limited understanding in
    uncertainty even if they are in the pursuit of a common goal of effective decision
    making.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI领域，一系列信念或证据理论有着长期研究在不确定性下的推理和/或决策的历史。然而，由于不确定性不仅仅由证据不足或不可预测性造成，理解仍然有限。此外，ML/DL算法已经考虑了不确定性（例如，随机不确定性或认识不确定性）以提供有效决策的解决方案。然而，对于多维度的不确定性还没有一个共同而坚实的理解，因为每个领域在不确定性的理解上各有不同和/或有限，即使它们追求一个共同的有效决策目标。
- en: Our survey paper aims to conduct an in-depth survey on a series of belief models
    and introduce a new solution domain that can leverage uncertainty research in
    belief/evidence theory to develop ML/DL solutions for attaining effective decision
    making. In particular, we are interested in quantifying different types of uncertainty
    caused by different root causes. This will help provide solutions for ML/DL that
    can meet explainable AI, the so-called XAI, by providing how uncertainty derives
    from, what is the reason behind, and ultimately how it affects the effectiveness
    of decision making.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查论文旨在深入调查一系列信念模型，并引入一个新的解决方案领域，该领域可以利用信念/证据理论中的不确定性研究来开发ML/DL解决方案，以实现有效决策。特别是，我们感兴趣的是量化由不同根本原因引起的不同类型的不确定性。这将有助于为ML/DL提供满足可解释AI，即所谓的XAI的解决方案，通过提供不确定性的来源、原因以及最终它如何影响决策的有效性。
- en: The state-of-the-art decision making research has been fully recognized the
    significant importance of considering uncertainty for effective decision making.
    However, there has been little study that has extensively surveyed existing belief
    models to study uncertainty and its applicability for decision making in the ML/DL
    domain.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的决策研究已经充分认识到考虑不确定性对于有效决策的重要性。然而，关于现有信念模型以研究不确定性及其在ML/DL领域决策中的适用性的广泛调查研究仍然很少。
- en: 1.2\. Comparison of Our Survey Paper with Other Existing Survey Papers
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 我们的调查论文与其他现有调查论文的比较
- en: In this section, we discuss existing survey papers that have discussed uncertainty
    research. And then, we identify the key differences between the existing survey
    papers and our survey paper.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了现有的调查论文，这些论文讨论了不确定性研究。然后，我们确定了现有调查论文与我们的调查论文之间的关键区别。
- en: 'Li et al. ([2012](#bib.bib53)) discussed the causes of different uncertainties
    and how to process them in belief models for making effective decisions in various
    domains. According to the nature of aleatory and epistemic uncertainty, they classified
    uncertainty types processing in probability theory, fuzzy theory, information-gap
    theory and derived uncertainty theory for their comparison. They focused on how
    different uncertainties can be processed in data management techniques. Kabir
    et al. ([2018](#bib.bib45)) surveyed prediction interval techniques using deep
    neural networks (DNNs). The prediction interval techniques quantify the level
    of uncertainty or randomness and have been widely applied in the medical and electricity
    fields. They discussed aleatoric and epistemic uncertainty (see Section [2](#S2
    "2\. Classification Types, Causes, and Ontology of Uncertainty ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning")
    for their definitions) to explain uncertainty in prediction using DNNs. They also
    discussed how a Bayesian method is used to optimize the weight of an NN during
    training and applied for NN-based prediction intervals in various fields.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'Li 等人 ([2012](#bib.bib53)) 讨论了不同不确定性的原因及如何在信念模型中处理这些不确定性，以在各种领域做出有效决策。根据随机不确定性和认知不确定性的性质，他们对概率理论、模糊理论、信息间隙理论和派生不确定性理论中的不确定性处理类型进行了分类，并进行比较。他们专注于如何在数据管理技术中处理不同的不确定性。Kabir
    等人 ([2018](#bib.bib45)) 调查了使用深度神经网络（DNNs）的预测区间技术。这些预测区间技术量化了不确定性或随机性的水平，并已广泛应用于医学和电力领域。他们讨论了**随机**和**认知**不确定性（见第[2](#S2
    "2\. Classification Types, Causes, and Ontology of Uncertainty ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning")节以了解其定义），以解释使用
    DNNs 进行预测中的不确定性。他们还讨论了如何使用贝叶斯方法在训练过程中优化神经网络的权重，并将其应用于不同领域的基于神经网络的预测区间。'
- en: Hariri et al. ([2019](#bib.bib31)) surveyed various AI techniques, including
    ML, Natural Language Processing (NLP), and computational intelligence, that can
    recognize and reduce uncertainty in Big Data. Abdar et al. ([2021](#bib.bib3))
    reviewed over 700 papers that studied uncertainty quantification in ML/DL. They
    mainly focused on discussing Bayesian and ensemble techniques and their applications
    in various related areas, such as image processing, computer vision, medical applications,
    NLP, and text mining.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Hariri 等人 ([2019](#bib.bib31)) 调查了各种 AI 技术，包括机器学习（ML）、自然语言处理（NLP）和计算智能，这些技术可以识别和减少大数据中的不确定性。Abdar
    等人 ([2021](#bib.bib3)) 回顾了超过 700 篇研究 ML/DL 中不确定性量化的论文。他们主要讨论了贝叶斯方法和集成技术及其在图像处理、计算机视觉、医学应用、NLP
    和文本挖掘等相关领域的应用。
- en: 'We also discuss the contributions of the survey papers focusing on the uncertainty
    mainly in ML/DL in the following papers. Hüllermeier and Waegeman ([2021](#bib.bib38))
    distinguished aleatoric uncertainty from epistemic uncertainty. They explained
    how these two uncertainties are represented in various ML problems or models and
    can contribute to decision making under the assessed uncertainty. Ulmer ([2021](#bib.bib89))
    surveyed the methods of quantifying uncertainty in evidential deep learning model
    based on the conjugate prior and posterior distributions and unknown outlier samples.
    This model estimates uncertainty from the Dirichlet distribution by data (aleatoric)
    uncertainty, model (epistemic) uncertainty, and distributional uncertainty. Gawlikowski
    et al. ([2021](#bib.bib28)) provided a comprehensive survey on the uncertainty
    in DNNs. They discussed two types of uncertainties: reducible uncertainty and
    unreducible uncertainty.’ The concept of reducible uncertainty is aligned with
    that of epistemic uncertainty where the reducible uncertainty can be introduced
    by variability in a real-world, errors in model structure, or in training parameter
    (i.e., batch size, optimizer). Unreducible uncertainty means uncertainty by noises
    in measurement (i.e., sensor noise) and is in line with aleatoric uncertainty.
    The authors classified uncertainty estimation methods based on the cross-combination
    of the nature (i.e., deterministic or stochastic) and number (i.e., single or
    multiple) of DNNs. Since the discussions of uncertainty in (Hüllermeier and Waegeman,
    [2021](#bib.bib38); Ulmer, [2021](#bib.bib89); Gawlikowski et al., [2021](#bib.bib28))
    are very limited in the scope, we did not include them in Table [1](#S1.T1 "Table
    1 ‣ 1.2\. Comparison of Our Survey Paper with Other Existing Survey Papers ‣ 1\.
    Introduction ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还讨论了以下几篇论文中针对机器学习/深度学习中的不确定性的调查论文的贡献。Hüllermeier 和 Waegeman ([2021](#bib.bib38))
    区分了可变不确定性和认知不确定性。他们解释了这两种不确定性如何在各种机器学习问题或模型中表现，并且如何在评估的不确定性下有助于决策。Ulmer ([2021](#bib.bib89))
    调查了基于共轭先验和后验分布及未知离群样本的证据深度学习模型中量化不确定性的方法。该模型通过数据（可变）不确定性、模型（认知）不确定性和分布不确定性从狄利克雷分布中估计不确定性。Gawlikowski
    等人 ([2021](#bib.bib28)) 提供了对深度神经网络中不确定性的综合调查。他们讨论了两种类型的不确定性：可减少的不确定性和不可减少的不确定性。可减少的不确定性与认知不确定性的概念一致，其中可减少的不确定性可以由现实世界的变异性、模型结构中的错误或训练参数（即，批量大小、优化器）引入。不可减少的不确定性指的是测量中的噪声（即，传感器噪声），与可变不确定性一致。作者根据深度神经网络的性质（即，确定性或随机性）和数量（即，单一或多重）的交叉组合对不确定性估计方法进行了分类。由于
    (Hüllermeier 和 Waegeman, [2021](#bib.bib38); Ulmer, [2021](#bib.bib89); Gawlikowski
    等人, [2021](#bib.bib28)) 对不确定性的讨论范围非常有限，我们没有将其包含在表格 [1](#S1.T1 "Table 1 ‣ 1.2\.
    Comparison of Our Survey Paper with Other Existing Survey Papers ‣ 1\. Introduction
    ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning") 中。'
- en: 'Unlike the existing survey papers above (Li et al., [2012](#bib.bib53); Kabir
    et al., [2018](#bib.bib45); Hariri et al., [2019](#bib.bib31); Abdar et al., [2021](#bib.bib3)),
    our paper provides an in-depth survey of eight different belief models with emphasizing
    how to reason and quantify uncertainty based on the root causes and types of the
    uncertainty. In addition, we discussed how a belief model is applicable in the
    DL domain. This will allow researchers to leverage both the solid methodologies
    of uncertainty reasoning/quantification in belief models and DL techniques for
    attaining effective decision making. Finally, in Table [1](#S1.T1 "Table 1 ‣ 1.2\.
    Comparison of Our Survey Paper with Other Existing Survey Papers ‣ 1\. Introduction
    ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning"), we summarize the key differences between our survey
    paper and the existing four survey paper on uncertainty research. We selected
    the key criteria based on the common discussion points covered by the existing
    survey papers considered in this paper as well as the key discussion points made
    in our survey paper.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '与上述现有的调查论文（Li et al., [2012](#bib.bib53); Kabir et al., [2018](#bib.bib45);
    Hariri et al., [2019](#bib.bib31); Abdar et al., [2021](#bib.bib3)）不同，我们的论文提供了对八种不同信念模型的深入调查，重点强调如何根据不确定性的根本原因和类型来推理和量化不确定性。此外，我们讨论了信念模型在深度学习领域的适用性。这将使研究人员能够利用信念模型中的不确定性推理/量化的扎实方法和深度学习技术，从而实现有效的决策。最后，在表[1](#S1.T1
    "Table 1 ‣ 1.2\. Comparison of Our Survey Paper with Other Existing Survey Papers
    ‣ 1\. Introduction ‣ A Survey on Uncertainty Reasoning and Quantification for
    Decision Making: Belief Theory Meets Deep Learning")中，我们总结了我们调查论文与现有四篇不确定性研究调查论文之间的关键差异。我们根据现有调查论文中涉及的共同讨论点以及我们调查论文中的关键讨论点选择了关键标准。'
- en: Table 1. Comparison of Our Survey Paper with the Existing Surveys on Uncertainty
    Research
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 我们的调查论文与现有不确定性研究调查论文的比较
- en: '| Key Criteria | Our Survey (2022) | Li et al. ([2012](#bib.bib53)) (2012)
    | Kabir et al. ([2018](#bib.bib45)) (2018) | Hariri et al. ([2019](#bib.bib31))
    (2019) | Abdar et al. ([2021](#bib.bib3)) (2021) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 关键标准 | 我们的调查（2022） | Li et al. ([2012](#bib.bib53))（2012） | Kabir et al.
    ([2018](#bib.bib45))（2018） | Hariri et al. ([2019](#bib.bib31))（2019） | Abdar
    et al. ([2021](#bib.bib3))（2021） |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Ontology of uncertainty | ✔ | ▲ | ▲ | ▲ | ▲ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 不确定性的本体 | ✔ | ▲ | ▲ | ▲ | ▲ |'
- en: '| Causes of uncertainty | ✔ | ✔ | ✔ | ▲ | ✔ |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 不确定性的原因 | ✔ | ✔ | ✔ | ▲ | ✔ |'
- en: '| Uncertainty reasoning & quantification in DST | ✔ | ✔ | ✘ | ✘ | ✘ |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| DST中的不确定性推理与量化 | ✔ | ✔ | ✘ | ✘ | ✘ |'
- en: '| Uncertainty reasoning & quantification in TBM | ✔ | ✘ | ✘ | ✘ | ✘ |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| TBM中的不确定性推理与量化 | ✔ | ✘ | ✘ | ✘ | ✘ |'
- en: '| Uncertainty reasoning & quantification in DSmT | ✔ | ✘ | ✘ | ✘ | ✘ |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| DSmT中的不确定性推理与量化 | ✔ | ✘ | ✘ | ✘ | ✘ |'
- en: '| Uncertainty reasoning & quantification in IDM | ✔ | ▲ | ✘ | ✘ | ✘ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| IDM中的不确定性推理与量化 | ✔ | ▲ | ✘ | ✘ | ✘ |'
- en: '| Uncertainty reasoning & quantification in TVL | ✔ | ▲ | ✘ | ✘ | ✘ |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| TVL中的不确定性推理与量化 | ✔ | ▲ | ✘ | ✘ | ✘ |'
- en: '| Uncertainty reasoning & quantification in Fuzzy Logic | ✔ | ✔ | ✘ | ▲ | ✘
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 模糊逻辑中的不确定性推理与量化 | ✔ | ✔ | ✘ | ▲ | ✘ |'
- en: '| Uncertainty reasoning & quantification in Bayesian Inference | ✔ | ✔ | ✘
    | ▲ | ▲ |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 贝叶斯推断中的不确定性推理与量化 | ✔ | ✔ | ✘ | ▲ | ▲ |'
- en: '| Uncertainty reasoning & quantification in Subjective Logic | ✔ | ✘ | ✘ |
    ▲ | ✘ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 主观逻辑中的不确定性推理与量化 | ✔ | ✘ | ✘ | ▲ | ✘ |'
- en: '| Uncertainty reasoning & quantification in Bayesian Deep Learning | ✔ | ✘
    | ▲ | ✘ | ✔ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 贝叶斯深度学习中的不确定性推理与量化 | ✔ | ✘ | ▲ | ✘ | ✔ |'
- en: '| Applicability of Belief Models in Deep Learning | ✔ | ✘ | ▲ | ✘ | ✘ |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 信念模型在深度学习中的适用性 | ✔ | ✘ | ▲ | ✘ | ✘ |'
- en: '| Discussions of insights, lessons, and limitations of the existing uncertainty-aware
    approaches | ✔ | ▲ | ▲ | ▲ | ✔ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 对现有不确定性感知方法的见解、经验教训和局限性的讨论 | ✔ | ▲ | ▲ | ▲ | ✔ |'
- en: '| Discussions of future research directions | ✔ | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 未来研究方向的讨论 | ✔ | ✔ | ✔ | ✔ | ✔ |'
- en: '✔: Fully addressed; ▲: Partially addressed; ✘: Not addressed at all; DST: Dempster-Shafer
    Theory; TBM: Transferable Belief Model, DSmT: Dezert-Smarandache Theory; TVL:
    Three-Valued Logic.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '✔: 完全涵盖; ▲: 部分涵盖; ✘: 完全未涵盖; DST: 丹普斯特-谢弗理论; TBM: 可迁移信念模型, DSmT: 德泽特-斯马兰纳赫理论;
    TVL: 三值逻辑。'
- en: 1.3\. Research Questions
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 研究问题
- en: 'In this work, we aim to answer the following research questions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们旨在回答以下研究问题：
- en: 'RQ1.:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ1.:'
- en: What are the key causes and types of uncertainty studied in belief theory and
    deep learning?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 信念理论和深度学习中研究的不确定性的关键原因和类型是什么？
- en: 'RQ2.:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ2.:'
- en: How can the ontology of uncertainty be defined based on the multidimensional
    aspects of uncertainty studied in belief models and deep learning?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如何根据在信念模型和深度学习中研究的不确定性的多维度方面定义不确定性本体？
- en: 'RQ3.:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3.：
- en: How has each belief model considered and measured uncertainty?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个信念模型如何考虑和测量不确定性？
- en: 'RQ4.:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RQ4.：
- en: How has each belief model been applied in deep learning and vice-versa for effective
    decision making under uncertainty?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个信念模型如何在深度学习中应用，反之亦然，以在不确定性下进行有效决策？
- en: 'RQ5.:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: RQ5.：
- en: What are the key differences of uncertainty reasoning and quantification in
    belief theory and deep learning?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 信念理论和深度学习中的不确定性推理和量化的主要差异是什么？
- en: 'RQ6.:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: RQ6.：
- en: How can belief model(s) be applied in deep learning to solve complicated decision
    making problems?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将信念模型应用于深度学习以解决复杂的决策问题？
- en: 'The research questions above will be answered in Section [6](#S6 "6\. Concluding
    Remarks ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making:
    Belief Theory Meets Deep Learning").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '上述研究问题将在第[6](#S6 "6\. Concluding Remarks ‣ A Survey on Uncertainty Reasoning
    and Quantification for Decision Making: Belief Theory Meets Deep Learning")节中回答。'
- en: 1.4\. Scope & Key Contributions
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4\. 研究范围与关键贡献
- en: Although uncertainty has been considered in various domains, we limit the scope
    of our paper to belief models and their applications in DL algorithms. Note that
    when we refer to ‘decision making’, we mean a choice among multiple alternatives.
    For example, it can be a certain class in classification tasks to maximize prediction
    accuracy, an action chosen among multiple actions available to maximize a decision
    utility, or a strategy chosen for optimizing system performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在各种领域中已经考虑了不确定性，但我们将论文的范围限制在信念模型及其在深度学习算法中的应用上。需要注意的是，当我们提到‘决策’时，指的是在多个备选方案中进行选择。例如，这可以是分类任务中为了最大化预测准确性而选择的某个类别，或者是为了最大化决策效用而选择的多个行动中的一个，或是为了优化系统性能而选择的策略。
- en: 'In this paper, we made the following key contributions:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们做出了以下关键贡献：
- en: (1)
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: We are the first conducting an extensive survey on identifying the causes and
    types of uncertainty studied in various belief models and deep learning and provides
    the ontology of uncertainty.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们是第一个进行广泛调查，识别各种信念模型和深度学习中研究的不确定性的原因和类型，并提供不确定性本体的研究。
- en: (2)
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: We first investigate how various belief theories have considered uncertainty
    and quantified it for effective decision making.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首先调查了各种信念理论如何考虑不确定性并将其量化，以便进行有效的决策。
- en: (3)
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: We also first discuss how belief theories can be effectively leveraged for deep
    learning-based solutions for decision making.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还首先讨论了如何有效利用信念理论来提供基于深度学习的决策解决方案。
- en: (4)
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: We identify the key commonalities and differences about how each belief theory
    reasons and quantifies uncertainty and how it is applied in the context of deep
    learning or along with it.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们识别了每种信念理论推理和量化不确定性的方法的关键共性和差异，以及它在深度学习背景下或与之结合的应用方式。
- en: (5)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: We provide the overall perspectives of insights and lessons learned as well
    as the limitations from our extensive survey and suggest promising future research
    directions.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对见解和经验教训的整体观点，以及我们广泛调查中的局限性，并建议了有前景的未来研究方向。
- en: 1.5\. Structure of the Paper
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 论文结构
- en: 'The rest of the paper is organized as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下：
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [2](#S2 "2\. Classification Types, Causes, and Ontology of Uncertainty
    ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning") provides various classification types of uncertainty,
    the causes of different types of uncertainties, and a proposed uncertainty ontology
    based on the surveyed multidimensional concepts of uncertainty.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[2](#S2 "2\. Classification Types, Causes, and Ontology of Uncertainty ‣ A
    Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning")节提供了不确定性的各种分类类型、不同类型不确定性的原因以及基于调查的多维度不确定性概念提出的不确定性本体。'
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [3](#S3 "3\. Decision Making under Uncertainty in Belief Theory ‣ A
    Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning") provides the details of eight belief models and discusses
    belief formation, causes and types of uncertainty, uncertainty quantification,
    and its application as decision making application. The eight belief models include
    Dempster Shafer Theory (DST), Transferable Belief Model (TBM), Dezert-Smarandache
    Theory (DSmT), Imprecise Dirichlet Model (IDM), Kleene’s Three-Valued Logic (TVL),
    Fuzzy Logic (FL), Bayesian Inference (BI), and Subjective Logic (SL).'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[3](#S3 "3\. 信念理论下的不确定性决策 ‣ 不确定性推理与量化调查：信念理论与深度学习的融合")节提供了八种信念模型的详细信息，并讨论了信念形成、不确定性的原因和类型、不确定性量化及其在决策应用中的应用。这八种信念模型包括邓普斯特-沙弗理论（DST）、可转移信念模型（TBM）、德泽特-斯马兰尼亚奇理论（DSmT）、不精确的狄利克雷模型（IDM）、克莱尼的三值逻辑（TVL）、模糊逻辑（FL）、贝叶斯推理（BI）和主观逻辑（SL）。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [4](#S4 "4\. Belief Theory Meets Deep Learning ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning")
    discusses how a belief theory can be applied in the context of DL as decision
    making applications under uncertainty, particularly in terms of evidential neural
    networks, fuzzy deep neural networks, and rough deep neural networks.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[4](#S4 "4\. 信念理论与深度学习的融合 ‣ 不确定性推理与量化调查：信念理论与深度学习的融合")节讨论了信念理论如何在深度学习（DL）背景下应用于不确定性下的决策应用，特别是在证据神经网络、模糊深度神经网络和粗糙深度神经网络方面。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [5](#S5 "5\. Summary of the Key Findings ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning")
    provides the answers to the key research questions raised in Section [1](#S1 "1\.
    Introduction ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning").'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[5](#S5 "5\. 主要发现总结 ‣ 不确定性推理与量化调查：信念理论与深度学习的融合")节提供了第[1](#S1 "1\. 引言 ‣ 不确定性推理与量化调查：信念理论与深度学习的融合")节提出的关键研究问题的答案。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [6](#S6 "6\. Concluding Remarks ‣ A Survey on Uncertainty Reasoning
    and Quantification for Decision Making: Belief Theory Meets Deep Learning") concludes
    our paper by discussing the limitations, insights, and lessons learned from our
    survey. In addition, we suggest promising future research directions in applying
    belief models to solve DL-based decision making problems.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[6](#S6 "6\. 结论性评论 ‣ 不确定性推理与量化调查：信念理论与深度学习的融合")节通过讨论我们调查中的局限性、见解和所学到的经验来总结我们的论文。此外，我们建议在将信念模型应用于解决基于DL的决策问题方面的有前途的未来研究方向。
- en: 'Caveat of Mathematical Notations: In Sections [3](#S3 "3\. Decision Making
    under Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning") and [4](#S4 "4\. Belief
    Theory Meets Deep Learning ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning"), we discuss a set of
    belief theories and deep learning theories leveraging belief models, including
    Subjective Logic, Fuzzy theory, and rough set theory. The discussion of a theory
    needs to use mathematical notations which are only used under the theory, not
    other theories. We keep the mathematical notations of original papers in order
    to deliver the common notations used to discuss each theory in the literature.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数学符号的警示：在第[3](#S3 "3\. 信念理论下的不确定性决策 ‣ 不确定性推理与量化调查：信念理论与深度学习的融合")节和第[4](#S4 "4\.
    信念理论与深度学习的融合 ‣ 不确定性推理与量化调查：信念理论与深度学习的融合")节中，我们讨论了一组信念理论和利用信念模型的深度学习理论，包括主观逻辑、模糊理论和粗糙集理论。对某一理论的讨论需要使用仅在该理论下使用的数学符号，而不是其他理论下的符号。我们保留了原始论文中的数学符号，以便传达文献中讨论每种理论所使用的通用符号。
- en: 2\. Classification Types, Causes, and Ontology of Uncertainty
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 不确定性的分类类型、原因和本体
- en: 'In this work, we deal with uncertainty in data or information. We define an
    uncertainty type as a perceived state of data or information, such as fuzziness,
    discord, non-specificity, ambiguity, and so forth (see Section [2.1](#S2.SS1 "2.1\.
    Classification of Uncertainty Types ‣ 2\. Classification Types, Causes, and Ontology
    of Uncertainty ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning")). We define the causes of uncertainty
    by the reason introducing uncertainty in a decision maker’s judgment. We also
    discuss the ontology of uncertainty where ontology is studied as a branch of philosophy
    which is defined as the “science of what it is” describing “the structures of
    objects, properties, events, processes, and relations in every area of reality (Smith,
    [2012](#bib.bib79)). In this section, we will describe the ontology of uncertainty
    in terms of its types, causes, and outcomes of decision making based on uncertainty
    reasoning and quantification.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们处理数据或信息中的不确定性。我们将不确定性类型定义为对数据或信息的感知状态，如模糊性、争议性、非特指性、歧义等（参见第[2.1](#S2.SS1
    "2.1\. Classification of Uncertainty Types ‣ 2\. Classification Types, Causes,
    and Ontology of Uncertainty ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning")节）。我们通过引入不确定性的原因来定义不确定性的成因。我们还讨论了不确定性的本体论，其中本体论作为哲学的一个分支，定义为“研究事物存在的科学”，描述“现实中每个领域的对象、属性、事件、过程和关系的结构”（Smith，[2012](#bib.bib79)）。在本节中，我们将从不确定性推理和量化的角度，描述不确定性的类型、成因以及决策结果。'
- en: 2.1\. Classification of Uncertainty Types
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 不确定性类型的分类
- en: 'In the probabilistic uncertainty research community (Jøsang, [2016](#bib.bib43);
    Kiureghian and Ditlevsen, [2009](#bib.bib48)), two types of uncertainty natures
    are widely used and commonly discussed by:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率不确定性研究领域（Jøsang，[2016](#bib.bib43)；Kiureghian 和 Ditlevsen，[2009](#bib.bib48)），两种类型的不确定性本质被广泛使用并且常常讨论：
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Aleatoric uncertainty: This refers to statistical uncertainty about the long
    term relative frequencies of possible outcomes (Jøsang, [2016](#bib.bib43)). For
    example, if we do not know whether a dice is loaded – and thereby unfair – then
    we are faced with aleatoric uncertainty. This uncertainty can be reduced to the
    true variance about the loaded dice by throwing the dice sufficiently many times.
    However, every time a dice is thrown, we cannot predict its outcome exactly but
    can have only a long-term probability (Jøsang, [2016](#bib.bib43)). In this sense,
    the long-term probability can reduce epistemic uncertainty by more and more observations.
    Therefore, aleatoric uncertainty is fundamentally related to the nature of randomness
    in which a variable is governed by a frequentist process (Kiureghian and Ditlevsen,
    [2009](#bib.bib48); Jøsang, [2016](#bib.bib43)).'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机不确定性：这指的是对可能结果的长期相对频率的统计不确定性（Jøsang，[2016](#bib.bib43)）。例如，如果我们不知道一个骰子是否是加重的——从而是不公平的——那么我们面临的是随机不确定性。这种不确定性可以通过多次掷骰子来减少到真实的方差。然而，每次掷骰子时，我们无法准确预测其结果，只能获得长期概率（Jøsang，[2016](#bib.bib43)）。因此，长期概率可以通过越来越多的观测来减少认知不确定性。因此，随机不确定性与随机性本质上是相关的，其中一个变量由频率过程控制（Kiureghian
    和 Ditlevsen，[2009](#bib.bib48)；Jøsang，[2016](#bib.bib43)）。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Epistemic uncertainty: This uncertainty is related to a situation that we cannot
    predict exactly an event because of a lack of knowledge. A typical example is
    the assassination of President Kennedy in 1963 (Jøsang, [2016](#bib.bib43)), where
    the uncertainty is about whether he was killed by Lee Harvey Oswald and who organized
    it. The nature of epistemic uncertainty derives from a lack of knowledge or information
    (or data). This type of uncertainty is also called systematic uncertainty or model
    uncertainty. This means that the outcome of a specific future or past event can
    be known, but there is insufficient evidence to support it. The epistemic uncertainty
    can be reduced by more evidence, more advanced technology, and/or scientific principles
    to interpret the evidence (e.g., forensic science) (Kiureghian and Ditlevsen,
    [2009](#bib.bib48)). This epistemic uncertainty follows a non-frequentist process
    representing the likelihood of an event (Jøsang, [2016](#bib.bib43)).'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识不确定性：这种不确定性与由于知识缺乏而无法准确预测事件的情况有关。一个典型的例子是1963年肯尼迪总统的暗杀事件（Jøsang，[2016](#bib.bib43)），其中的不确定性在于他是否被李·哈维·奥斯瓦尔德杀害以及谁组织了这次事件。知识不确定性的本质源于缺乏知识或信息（或数据）。这种类型的不确定性也称为系统性不确定性或模型不确定性。这意味着特定未来或过去事件的结果可能是已知的，但缺乏足够的证据来支持它。知识不确定性可以通过更多的证据、更先进的技术和/或科学原理来解释证据（例如，法医科学）（Kiureghian
    和 Ditlevsen，[2009](#bib.bib48)）来减少。这种知识不确定性遵循一种非频率学过程，表示事件的可能性（Jøsang，[2016](#bib.bib43)）。
- en: Since the above two natures of uncertainty have been most widely discussed as
    the nature of uncertainty, we will discuss how a different type of uncertainty
    in different belief models and DL models is related to these two natures of uncertainty.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述两种不确定性的性质已被广泛讨论，我们将讨论在不同信念模型和深度学习模型中，不同类型的不确定性如何与这两种不确定性的性质相关。
- en: 'Uncertainty reasoning and quantification research has been heavily explored
    by several theories, such as probability theory, fuzzy sets theory, possibility
    theory, evidence theory, and rough sets theory. These theories can be seen as
    complementary as each of them is designed to deal with different types of uncertainty.
    In these theories, Dubois ([1980](#bib.bib24)) identified three uncertainty types
    in terms of fuzziness, discord, and nonspecificity. The latter two terms, discord
    and non-specificity, are combined as the term ambiguity. Each type is represented
    by a brief common-sense characterization and several pertinent synonyms as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性推理和量化研究已被多个理论广泛探索，例如概率论、模糊集理论、可能性理论、证据理论和粗糙集理论。这些理论可以被视为互补的，因为每种理论都旨在处理不同类型的不确定性。在这些理论中，Dubois（[1980](#bib.bib24)）识别了三种不确定性类型，即模糊性、不一致性和非特指性。后两种术语，不一致性和非特指性，被合并为模糊性。每种类型都有一个简短的常识性描述和几个相关的同义词，如下所示：
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fuzziness: This results from a lack of definite or sharp distinctions and has
    pertinent synonyms, such as vagueness, cloudiness, haziness, unclearness, indistinctness,
    or sharplessness.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模糊性：这源于缺乏明确或清晰的区分，并有相关的同义词，如含糊不清、模糊不清、朦胧、不明确、不清晰或不锐利。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ambiguity: In general, there exists ambiguity when an object cannot be specified
    due to a lack of certain distinctions or detected as a single class due to conflicting
    evidence. Hence, these two cases can be further classified into the following
    two subclasses:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模糊性：一般来说，当对象无法由于缺乏某些区分而被明确指定，或者由于冲突的证据被检测为单一类别时，就存在模糊性。因此，这两种情况可以进一步分类为以下两个子类：
- en: –
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Discord: This is associated with disagreement among several alternatives and
    interchangeably used with synonyms including dissonance, incongruity, discrepancy,
    or conflict.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不一致：这与几个选择之间的分歧有关，并且可以互换地使用包括不和谐、矛盾、不一致或冲突等同义词。
- en: –
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Nonspecificity: This refers to a situation in which two or more alternatives
    are left unspecified and is used with pertinent synonyms, such as variety, generality,
    diversity, equivocation, and imprecision.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非特指性：指的是两种或更多的选择被留作不特定情况，并用相关的同义词表示，如多样性、一般性、多样性、模棱两可和不精确。
- en: Based on our understanding, fuzziness introduces vagueness (i.e., failing in
    distinguishing one from another) while ambiguity belongs to epistemic uncertainty.
    In addition to the above, the most common uncertainty is also derived from a lack
    of evidence, called vacuity (or ignorance) (Jøsang, [2016](#bib.bib43)) as we
    do not know how to make a decision because of insufficient evidence, which belongs
    to epistemic uncertainty as more evidence can reduce vacuity.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的理解，模糊性引入了含糊性（即，无法区分彼此），而歧义属于认识论的不确定性。除此之外，最常见的不确定性还源于缺乏证据，这被称为虚无（或无知）（Jøsang,
    [2016](#bib.bib43)），因为我们由于证据不足而不知道如何做决定，这属于认识论的不确定性，因为更多的证据可以减少虚无。
- en: '![Refer to caption](img/18f0a57d5cd30c3718120ea1caaceaa8.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/18f0a57d5cd30c3718120ea1caaceaa8.png)'
- en: Figure 1. Classification of uncertainty types.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 不确定性类型的分类。
- en: 'In the modeling and risk assessment research (Linkov and Burmistrov, [2003](#bib.bib56);
    Walker et al., [2003](#bib.bib93)), uncertainty associated with choices made by
    modelers has been studied, such as differences in problem formulation, model implementation,
    and parameter selection originating from subjective interpretation of the problem
    at hand. We call this ‘modeler uncertainty’ which has been categorized by the
    following three types:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模和风险评估研究中（Linkov 和 Burmistrov, [2003](#bib.bib56); Walker 等, [2003](#bib.bib93)），研究了模型选择中与模型制作者相关的不确定性，例如源于对问题的主观解释的不同问题表述、模型实现和参数选择。我们称之为“模型制作者不确定性”，并将其归类为以下三种类型：
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parameter uncertainty: This refers to uncertainty derived from the values of
    input parameters in a model, such as measurement errors, sampling errors, variability,
    and use of surrogate data. Hence, it is a type of epistemic uncertainty and can
    be reduced by collecting more reliable evidence to more accurately estimate the
    parameters used in the model.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数不确定性：这是指模型中输入参数值的不确定性，例如测量误差、采样误差、变异性以及替代数据的使用。因此，它是一种认识论的不确定性，可以通过收集更可靠的证据来更准确地估计模型中使用的参数，从而减少。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model uncertainty: This indicates uncertainty about a model structure and the
    mathematical relationships of components defined in the model. For example, uncertainty
    can be introduced by making assumptions and simplifying mathematical equations
    in modeling real-world problems. Thus, this uncertainty is introduced by missing
    or incomplete information, which makes hard to fully define the model. This uncertainty
    belongs to epistemic uncertainty and can be reduced by gathering necessary, reliable
    information to accurately define the model.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型不确定性：这表明对模型结构和模型中定义的组件数学关系的不确定性。例如，在建模现实世界问题时，通过做假设和简化数学方程式，可以引入不确定性。因此，这种不确定性是由缺失或不完整的信息引入的，这使得模型的完全定义变得困难。这种不确定性属于认识论的不确定性，可以通过收集必要的、可靠的信息来准确地定义模型，从而减少。
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scenario uncertainty: This represents uncertainty caused by normative choices
    made on constructing scenarios, including the choice of a functional unit, time
    horizon, geographical scale, and other methodological choices. This uncertainty
    arises from uncertain problem formulations and theoretic assumptions, which are
    not statistical in nature. Due to this nature of uncertainty, we understand this
    uncertainty as epistemic uncertainty. This uncertainty can be reduced by collecting
    more evidence, using more advanced technology, and/or considering scientific principles
    to interpret the evidence, such as finding a better choice of a time horizon or
    a geographical scale for crime hotspot detection.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 情景不确定性：这表示由构建情景时做出的规范性选择引起的不确定性，包括功能单元的选择、时间范围、地理尺度和其他方法选择。这种不确定性源于不确定的问题表述和理论假设，本质上不是统计性的。由于这种不确定性的性质，我们将其理解为认识论的不确定性。这种不确定性可以通过收集更多证据、使用更先进的技术和/或考虑科学原理来解释证据来减少，例如，为犯罪热点检测找到更好的时间范围或地理尺度选择。
- en: 2.2\. Causes of Uncertainty
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 不确定性的原因
- en: 'From the perspective of framing research in decision making, where frames are
    heuristic representations of the external world, there are three main causes of
    uncertainty, including unpredictability, incomplete knowledge, and multiple knowledge
    frames (Brugnach et al., [2008](#bib.bib8)). From an engineering perspective,
    these three causes can be introduced by a lack of evidence, limited cognition
    to process a large amount of evidence, conflicting evidence, ambiguity, measurement
    errors, and subjective beliefs (Zimmermann, [2000](#bib.bib124)). The three causes
    are described by:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从决策中的框架研究角度来看，框架是外部世界的启发式表征，主要有三种不确定性的原因，包括不可预测性、不完全知识和多重知识框架（Brugnach et al.,
    [2008](#bib.bib8)）。从工程学的角度，这三种原因可以由缺乏证据、处理大量证据的认知能力有限、证据冲突、模糊性、测量误差和主观信念引入（Zimmermann,
    [2000](#bib.bib124)）。这三种原因的描述如下：
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unpredictability: A system (or entity/data) may exhibit chaotic, variable behavior
    in space/time. In Statistics, confidence intervals have been used as a measure
    of uncertainty (Zimmermann, [2000](#bib.bib124)). Statistical noise is a common
    factor triggering uncertainty, leading to unpredictability. Even if the system
    learns and adapts to dynamic, new conditions, it exhibits highly variable behaviors.
    The variability may be due to unreliability in information, data, or an entity,
    which is caused by system/network dynamics, non-stationary environmental conditions,
    or adversarial attacks. If this is the case, this type of uncertainty can be reduced
    by detecting and excluding the unreliable sources or data in decision making process (Zimmermann,
    [2000](#bib.bib124)).'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不可预测性：一个系统（或实体/数据）可能表现出在空间/时间上混沌和变化的行为。在统计学中，置信区间被用作不确定性的度量（Zimmermann, [2000](#bib.bib124)）。统计噪声是触发不确定性的常见因素，导致不可预测性。即使系统学习并适应动态的新条件，它也会表现出高度变化的行为。这种变化可能是由于信息、数据或实体的不可靠性，这种不可靠性是由系统/网络动态、非平稳环境条件或敌对攻击造成的。如果是这种情况，可以通过在决策过程中检测并排除不可靠的来源或数据来减少这种不确定性（Zimmermann,
    [2000](#bib.bib124)）。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Incomplete Knowledge: This refers to situations where we do not know enough
    about a system to be managed or our knowledge about the system is incomplete (i.e.,
    epistemic uncertainty) (Zimmermann, [2000](#bib.bib124)). This can be due to a
    lack of evidence (e.g., information/data) or a lack of knowledge because we may
    not have sufficient theoretical understanding (e.g., ignorance) or reliable information
    or data. This uncertainty can be reduced by considering more evidence or discarding
    unreliable evidence. In addition, when human decision makers receive a large amount
    of information, which is often highly complex, they cannot process them properly
    because of their limited cognition and processing power. To deal with this, people
    usually transform available data into information with a rougher ‘granularity’
    or focus on important features, neglecting other less important (or noisy) information
    or data. This uncertainty can be reduced by considering relevant information among
    the available information (Zimmermann, [2000](#bib.bib124)).'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不完全知识：这指的是我们对一个系统的了解不足以进行有效管理，或者我们对该系统的知识是不完整的（即认知不确定性）（Zimmermann, [2000](#bib.bib124)）。这可能是由于缺乏证据（例如，信息/数据）或缺乏理论理解（例如，知识盲区）或可靠的信息或数据造成的。通过考虑更多的证据或排除不可靠的证据，可以减少这种不确定性。此外，当人类决策者接收到大量信息时，这些信息通常非常复杂，由于其有限的认知能力和处理能力，他们无法正确处理这些信息。为了解决这个问题，人们通常将可用数据转化为较粗糙的‘粒度’信息或关注重要特征，忽略其他不那么重要（或噪声）的信息或数据。通过在可用信息中考虑相关信息，可以减少这种不确定性（Zimmermann,
    [2000](#bib.bib124)）。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multiple Knowledge Frames: This refers to the case when same information (e.g.,
    evidence or opinions) is interpreted differently, resulting in different, conflicting
    views. Dewulf et al. ([2005](#bib.bib21)) defined ambiguity as the presence of
    multiple, valid beliefs about a certain phenomenon. The ways of understanding
    the system (or the external world) can differ in where to put the boundaries of
    the system or what and who to put as the focus of attention. The differences can
    also emerge from the way in which the information about the system is interpreted.
    Another major cause is conflicting evidence, representing a situation where some
    of the information available may be incorrect, simply irrelevant, or the model
    to observe a system may not be correct at a given time. Further, multiple observers
    may provide different opinions based on their subjective views.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多重知识框架：指的是相同的信息（例如，证据或意见）被不同地解释，导致不同且冲突的观点。Dewulf 等（[2005](#bib.bib21)）将模糊性定义为对某一现象存在多个有效信念的情况。对系统（或外部世界）的理解方式可以在于如何划定系统的边界，或将什么和谁作为关注焦点。差异还可能来源于对系统信息的解释方式。另一个主要原因是相互冲突的证据，这代表了一种情况，其中某些信息可能是不正确的、无关紧要的，或者观察系统的模型在某个时刻可能不正确。此外，多位观察者可能基于其主观观点提供不同的意见。
- en: 'We demonstrated our view about the classifications of uncertainty types based
    on the existing classifications in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1\. Classification
    of Uncertainty Types ‣ 2\. Classification Types, Causes, and Ontology of Uncertainty
    ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了基于现有分类（见图[1](#S2.F1 "Figure 1 ‣ 2.1\. Classification of Uncertainty Types
    ‣ 2\. Classification Types, Causes, and Ontology of Uncertainty ‣ A Survey on
    Uncertainty Reasoning and Quantification for Decision Making: Belief Theory Meets
    Deep Learning")）对不确定性类型分类的观点。'
- en: 2.3\. Ontology of Uncertainty
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 不确定性的本体论
- en: 'Ontology provides “a definitive and exhaustive classification of entities in
    all spheres of being” where the classification should be able to fully describe
    the details of the entities (Smith, [2012](#bib.bib79)). The information fusion
    research community has developed “Uncertainty Representation and Reasoning Framework
    (URREF)” (Costa et al., [2018](#bib.bib16)), which considers uncertainty ontology
    in the information processing systems. However, its scope was limited only to
    the information processing systems and only considered a subset of uncertainty
    types studied in the uncertainty research domain. In order to more extensively
    understand the concepts of uncertainty and its multiple causes, we develop an
    ontology of uncertainty by using W3C Web Ontology Language (OWL) (Stanford Center
    for Biomedical Informatics Research (BMIR), [2019](#bib.bib83)), which is demonstrated
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2.3\. Ontology of Uncertainty ‣ 2\. Classification
    Types, Causes, and Ontology of Uncertainty ‣ A Survey on Uncertainty Reasoning
    and Quantification for Decision Making: Belief Theory Meets Deep Learning"). We
    do not show the subattributes of ambiguity and fuzziness (see Fig. [1](#S2.F1
    "Figure 1 ‣ 2.1\. Classification of Uncertainty Types ‣ 2\. Classification Types,
    Causes, and Ontology of Uncertainty ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning")) in Fig. [2](#S2.F2 "Figure
    2 ‣ 2.3\. Ontology of Uncertainty ‣ 2\. Classification Types, Causes, and Ontology
    of Uncertainty ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning") due to the space constraint. Hence,
    we describe more about the key attributes of uncertainty used above in Appendix
    F of the supplement document.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本体提供了“在所有存在领域中实体的明确和详尽分类”，分类应能完全描述实体的细节（Smith，[2012](#bib.bib79)）。信息融合研究社区已经开发了“**不确定性表示与推理框架（URREF）**”（Costa
    等，[2018](#bib.bib16)），该框架考虑了信息处理系统中的不确定性本体。然而，其范围仅限于信息处理系统，并且仅考虑了不确定性研究领域中研究的子集的不确定性类型。为了更广泛地理解不确定性的概念及其多种原因，我们使用
    W3C 网络本体语言（OWL）（Stanford Biomedical Informatics Research Center (BMIR)，[2019](#bib.bib83)）开发了不确定性本体，如图
    [2](#S2.F2 "图 2 ‣ 2.3\. 不确定性的本体 ‣ 2\. 分类类型、原因和不确定性本体 ‣ 关于决策制定中的不确定性推理与量化的调查：信念理论遇见深度学习")
    所示。由于空间限制，我们在图 [2](#S2.F2 "图 2 ‣ 2.3\. 不确定性的本体 ‣ 2\. 分类类型、原因和不确定性本体 ‣ 关于决策制定中的不确定性推理与量化的调查：信念理论遇见深度学习")
    中没有显示模糊性和不明确性的子属性（参见图 [1](#S2.F1 "图 1 ‣ 2.1\. 不确定性类型的分类 ‣ 2\. 分类类型、原因和不确定性本体 ‣
    关于决策制定中的不确定性推理与量化的调查：信念理论遇见深度学习")）。因此，我们在补充文件的附录 F 中详细描述了上述不确定性的关键属性。
- en: '![Refer to caption](img/32bc7bc597d8a77ff4c72b9293b8349f.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/32bc7bc597d8a77ff4c72b9293b8349f.png)'
- en: Figure 2. An Ontology of Uncertainty.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 不确定性的本体。
- en: 3\. Decision Making under Uncertainty in Belief Theory
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 信念理论中的不确定性下的决策
- en: Different types of uncertainty affect the assessment and analysis of a specific
    situation. Underlying uncertainty comes from how to view and model a given part
    of the world which we call a *domain*. A domain is the abstract representations
    of states of the world, where analysts or decision makers can have beliefs about
    the true states of a domain. Beliefs about domains can be easily biased by an
    analyst or a decision maker, which is often called “framing effect” (Tversky and
    Kahneman, [1985](#bib.bib88); Walker et al., [2003](#bib.bib93)), which can cause
    subjective beliefs about the world to deviate from ground truth of the world (e.g.,
    past or future events) (Walker et al., [2003](#bib.bib93)). The way a situation
    is formally modeled (i.e., elements in a domain) can also affect the types and
    levels of uncertainty perceived by a decision maker.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的不确定性影响特定情况的评估和分析。潜在的不确定性源于如何看待和建模我们称之为*领域*的世界某个部分。领域是对世界状态的抽象表示，分析师或决策者可以对领域的真实状态持有信念。关于领域的信念可能会被分析师或决策者轻易地偏见，这通常被称为“框架效应”（Tversky
    和 Kahneman，[1985](#bib.bib88)；Walker 等，[2003](#bib.bib93)），这可能导致对世界的主观信念偏离世界的客观真相（例如，过去或未来事件）（Walker
    等，[2003](#bib.bib93)）。情况的正式建模方式（即领域中的元素）也可能影响决策者感知到的不确定性类型和水平。
- en: Belief has been based off for decision making process. In 1930s, Kleene ([1938](#bib.bib49))
    proposed Three-Valued Logic (TVL) by defining algebra based on three values including
    false, unknown, and truth. Many other theories defining a belief based on probabilities
    have been proposed since 1960’s, including Fuzzy Logic (Zadeh, [1965](#bib.bib107)),
    Dempster-Shafer Theory (Shafer, [1976](#bib.bib72)), Transferable Belief Model (Smets
    and Kennes, [1994](#bib.bib78)), Subjective Logic (Jøsang, [1999](#bib.bib41),
    [2001](#bib.bib42)), Dezert-Smarandache Theory (DSmT) (Dezert and Smarandache,
    [2004](#bib.bib22)), Bayesian Inference (Fienberg, [2006](#bib.bib26)), Imprecise
    Dirichlet Model (IDM) (Walley, [1996](#bib.bib94)). The Dempster-Shafer Theory
    (DST) (Shafer, [1976](#bib.bib72)) first defined a “frame of discernment,” the
    set of propositions considered. DST generalized Bayesian theory based on subjective
    probability (Shafer, [1976](#bib.bib72)). Transferrable Belief Model (TBM) (Smets
    and Kennes, [1994](#bib.bib78)) has been proposed to deal with more knowledge
    and situations than DST. Zadeh ([1965](#bib.bib107)) introduced fuzzy set theory
    to represent a uncertain, subjective belief based on a membership function (Zadeh,
    [1983](#bib.bib108)) and has been applied in various trust-based systems (Nagy
    et al., [2008](#bib.bib62); Lesani and Bagheri, [2006](#bib.bib52); Chen et al.,
    [2009](#bib.bib12); Liao et al., [2009](#bib.bib54); Luo et al., [2008](#bib.bib59);
    Manchala, [1998](#bib.bib61); Nefti et al., [2005](#bib.bib63)). DSmT (Dezert
    and Smarandache, [2004](#bib.bib22)) extended DST to deal with conflicting evidence
    in trust management systems (Wang and Sun, [2007](#bib.bib96); Deepa and Swamynathan,
    [2014](#bib.bib19)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 信念已经成为决策过程的基础。1930年代，克利尼（[1938](#bib.bib49)）通过定义基于三个值（包括假、未知和真）的代数，提出了三值逻辑（TVL）。自1960年代以来，许多其他基于概率定义信念的理论被提出，包括模糊逻辑（Zadeh，[1965](#bib.bib107)）、邓普斯特-谢弗理论（Shafer，[1976](#bib.bib72)）、可转移信念模型（Smets
    and Kennes，[1994](#bib.bib78)）、主观逻辑（Jøsang，[1999](#bib.bib41)、[2001](#bib.bib42)）、德泽特-斯马兰达切理论（DSmT）（Dezert
    and Smarandache，[2004](#bib.bib22)）、贝叶斯推理（Fienberg，[2006](#bib.bib26)）、不精确的狄利克雷模型（IDM）（Walley，[1996](#bib.bib94)）。邓普斯特-谢弗理论（DST）（Shafer，[1976](#bib.bib72)）首次定义了“辨识框架”，即考虑的命题集合。DST基于主观概率对贝叶斯理论进行了推广（Shafer，[1976](#bib.bib72)）。可转移信念模型（TBM）（Smets
    and Kennes，[1994](#bib.bib78)）被提出以处理比DST更多的知识和情况。Zadeh（[1965](#bib.bib107)）引入了模糊集理论，通过成员函数（Zadeh，[1983](#bib.bib108)）表示不确定的主观信念，并已应用于各种基于信任的系统（Nagy
    et al.，[2008](#bib.bib62)；Lesani and Bagheri，[2006](#bib.bib52)；Chen et al.，[2009](#bib.bib12)；Liao
    et al.，[2009](#bib.bib54)；Luo et al.，[2008](#bib.bib59)；Manchala，[1998](#bib.bib61)；Nefti
    et al.，[2005](#bib.bib63)）。DSmT（Dezert and Smarandache，[2004](#bib.bib22)）扩展了DST，以处理信任管理系统中的冲突证据（Wang
    and Sun，[2007](#bib.bib96)；Deepa and Swamynathan，[2014](#bib.bib19)）。
- en: 3.1\. Kleene’s Three-Valued Logic (TVL)
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 克利尼的三值逻辑（TVL）
- en: 3.1.1\. Belief Formation.
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 信念形成。
- en: 'Kleene ([1938](#bib.bib49)) first proposed TVL in 1938 . Its truth table is
    shown in Table [2](#S3.T2 "Table 2 ‣ 3.1.1\. Belief Formation. ‣ 3.1\. Kleene’s
    Three-Valued Logic (TVL) ‣ 3\. Decision Making under Uncertainty in Belief Theory
    ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning"), where $p_{1}$ and $p_{2}$ are two logical variables.
    TVL’s belief distribution is determined by the logical values of logical variables,
    i.e., $\mathbf{b}(p)_{q}=1$ when $p=q$ and $0$ otherwise. The $p$ is a logical
    variable and $q$ is a logical value from $\{T,U,F\}$ where $T$ is true, $U$ is
    unknown, and $F$ is false.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '克利尼（[1938](#bib.bib49)）于1938年首次提出了TVL。其真值表见表[2](#S3.T2 "Table 2 ‣ 3.1.1\. Belief
    Formation. ‣ 3.1\. Kleene’s Three-Valued Logic (TVL) ‣ 3\. Decision Making under
    Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning")，其中$p_{1}$和$p_{2}$是两个逻辑变量。TVL的信念分布由逻辑变量的逻辑值决定，即当$p=q$时，$\mathbf{b}(p)_{q}=1$，否则为$0$。$p$是一个逻辑变量，$q$是来自$\{T,U,F\}$的逻辑值，其中$T$为真，$U$为未知，$F$为假。'
- en: Table 2. Truth Table of the TVL
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. TVL的真值表
- en: $p_{1}\wedge p_{2}$
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: $p_{1}\wedge p_{2}$
- en: '| <svg version="1.1" height="17.3" width="21.68" overflow="visible"><g transform="translate(0,17.3)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,8.65) scale(1,
    -1)"><foreignobject width="10.84" height="8.65" overflow="visible">$p_{1}$</foreignobject></g></g>
    <g  transform="translate(10.84,8.65)"><g transform="translate(0,8.65) scale(1,
    -1)"><foreignobject width="10.84" height="8.65" overflow="visible">$p_{2}$</foreignobject></g></g></g></svg>
    | $T$ | $U$ | $F$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="17.3" width="21.68" overflow="visible"><g transform="translate(0,17.3)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,8.65) scale(1,
    -1)"><foreignobject width="10.84" height="8.65" overflow="visible">$p_{1}$</foreignobject></g></g>
    <g  transform="translate(10.84,8.65)"><g transform="translate(0,8.65) scale(1,
    -1)"><foreignobject width="10.84" height="8.65" overflow="visible">$p_{2}$</foreignobject></g></g></g></svg>
    | $T$ | $U$ | $F$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $T$ | $T$ | $U$ | $F$ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| $T$ | $T$ | $U$ | $F$ |'
- en: '| $U$ | $U$ | $U$ | $F$ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| $U$ | $U$ | $U$ | $F$ |'
- en: '| $F$ | $F$ | $F$ | $F$ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| $F$ | $F$ | $F$ | $F$ |'
- en: $p_{1}\vee p_{2}$
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $p_{1}\vee p_{2}$
- en: '| <svg version="1.1" height="17.3" width="21.68" overflow="visible"><g transform="translate(0,17.3)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,8.65) scale(1,
    -1)"><foreignobject width="10.84" height="8.65" overflow="visible">$p_{1}$</foreignobject></g></g>
    <g  transform="translate(10.84,8.65)"><g transform="translate(0,8.65) scale(1,
    -1)"><foreignobject width="10.84" height="8.65" overflow="visible">$p_{2}$</foreignobject></g></g></g></svg>
    | $T$ | $U$ | $F$ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="17.3" width="21.68" overflow="visible"><g transform="translate(0,17.3)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,8.65) scale(1,
    -1)"><foreignobject width="10.84" height="8.65" overflow="visible">$p_{1}$</foreignobject></g></g>
    <g  transform="translate(10.84,8.65)"><g transform="translate(0,8.65) scale(1,
    -1)"><foreignobject width="10.84" height="8.65" overflow="visible">$p_{2}$</foreignobject></g></g></g></svg>
    | $T$ | $U$ | $F$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $T$ | $T$ | $T$ | $T$ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| $T$ | $T$ | $T$ | $T$ |'
- en: '| $U$ | $T$ | $U$ | $U$ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| $U$ | $T$ | $U$ | $U$ |'
- en: '| $F$ | $T$ | $U$ | $F$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| $F$ | $T$ | $U$ | $F$ |'
- en: 'Kleene Algebras and TVL. Kleene’s TVL is a special case of Kleene algebras.
    The properties of Kleene algebra, $\mathcal{K}=(K,\vee,\wedge,\sim,F,T)$, are:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 克利尼代数和三值逻辑。克利尼的三值逻辑是克利尼代数的一个特例。克利尼代数 $\mathcal{K}=(K,\vee,\wedge,\sim,F,T)$
    的属性是：
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathcal{K}$ is a bounded distributive lattice; and
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{K}$ 是一个有界分配格；以及
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\forall a,b\in K$, $\sim(a\;\wedge b)=\sim a\;\vee\sim b$, $\sim\sim a=a$,
    and $a\;\wedge\sim a\leq b\;\vee\sim b$.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于任意的 $a,b\in K$，$\sim(a\;\wedge b)=\sim a\;\vee\sim b$，$\sim\sim a=a$，以及 $a\;\wedge\sim
    a\leq b\;\vee\sim b$。
- en: Here we apply the semantics where $\vee$ means logical disjunction, $\wedge$
    means logical conjunction, and $\sim$ means negation. It can be easily derived
    that Kleene’s TVL, in which $K=\{T,U,F\}$, is a Kleene algebra where $T=\sim F$,
    $F=\sim T$, and $U=\sim U$.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应用的语义是 $\vee$ 代表逻辑析取，$\wedge$ 代表逻辑合取，而 $\sim$ 代表否定。可以很容易推导出克利尼的三值逻辑，其中
    $K=\{T,U,F\}$，是一个克利尼代数，其中 $T=\sim F$，$F=\sim T$，和 $U=\sim U$。
- en: 'Rough Sets and Kleene Algebras. Kleene algebras are related to rough sets.
    The relationships between these two concepts are as follows. Given an information
    system, $I=(S,\mathbb{A})$, where $S$ is a set of objects and $\mathbb{A}$ is
    a set of attributes $a:x\mapsto a(x)$ for any $x\in S$, we can define the set
    of equivalence relationships, $IND(I)$: $IND(I)=\{IND(A):A\subseteq\mathbb{A}\}$,
    where $IND(A)=\{(x,y)\in S^{2}:\forall a\in A,a(x)=a(y)\}$. Given any equivalence
    relationship $R\in IND(I)$, a rough set $\mathcal{X}\in(S\times S)/R$ is a pair
    $(\underline{R}X,\overline{R}X)$, where $\underline{R}X$ and $\overline{R}X$ are
    called the $R$-lower and $R$-upper approximation of $X$, respectively. More specifically,'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 粗糙集和克利尼代数。克利尼代数与粗糙集相关。这两个概念之间的关系如下。给定一个信息系统 $I=(S,\mathbb{A})$，其中 $S$ 是对象集合，$\mathbb{A}$
    是属性集合 $a:x\mapsto a(x)$ 对于任意 $x\in S$，我们可以定义等价关系集 $IND(I)$：$IND(I)=\{IND(A):A\subseteq\mathbb{A}\}$，其中
    $IND(A)=\{(x,y)\in S^{2}:\forall a\in A,a(x)=a(y)\}$。给定任意等价关系 $R\in IND(I)$，一个粗糙集
    $\mathcal{X}\in(S\times S)/R$ 是一对 $(\underline{R}X,\overline{R}X)$，其中 $\underline{R}X$
    和 $\overline{R}X$ 分别称为 $R$-下近似和 $R$-上近似。更具体地，
- en: '| (1) |  | $\displaystyle\underline{R}X=\bigcup\{Y\in S/R:Y\subseteq X\},\;\;\overline{R}X=\bigcup\{Y\in
    S/R:Y\cap X\neq\emptyset\},$ |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle\underline{R}X=\bigcup\{Y\in S/R:Y\subseteq X\},\;\;\overline{R}X=\bigcup\{Y\in
    S/R:Y\cap X\neq\emptyset\},$ |  |'
- en: where $S/R$ is the collection of equivalence classes corresponding to $R$.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S/R$ 是与 $R$ 对应的等价类集合。
- en: 'Given any set $S$ with $|S|\geq 2$, universal equivalence relationship $R:=S\times
    S$ and information system $I=(S,\mathbb{A})$, we can induce a three-valued algebra
    on a collection of rough sets, $\mathcal{RS}$, with the Kleene semantics by:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 给定任意集合 $S$ 且 $|S|\geq 2$，普遍等价关系 $R:=S\times S$ 和信息系统 $I=(S,\mathbb{A})$，我们可以在粗糙集的集合
    $\mathcal{RS}$ 上诱导一个三值代数，使用克利尼语义如下：
- en: '| (2) |  | $\mathcal{RS}=\{(\underline{R}A,\overline{R}A):A\subseteq S\}=\{(S,S),(\emptyset,S),(\emptyset,\emptyset)\}.$
    |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\mathcal{RS}=\{(\underline{R}A,\overline{R}A):A\subseteq S\}=\{(S,S),(\emptyset,S),(\emptyset,\emptyset)\}.$
    |  |'
- en: 'Here, if we define $\sim\mathcal{X}:=(\underline{R}X^{c},\overline{R}X^{c})$,
    we have $(S,S)=\sim(\emptyset,\emptyset)$, $(\emptyset,S)=\sim(\emptyset,S)$,
    and $(\emptyset,\emptyset)=\sim(\emptyset,\emptyset)$. This means $K\cong\mathcal{RS}$.
    In general, given the set of all logic functions (propositional formula) denoted
    by $\mathcal{F}$, the set of all Kleene algebras by $\mathcal{A}_{\mathcal{K}}$,
    and the collections of all rough sets over all possible information systems by
    $\mathcal{A}_{\mathcal{RS}}$, the following theorem (Kumar and Banerjee, [2017](#bib.bib51))
    is held:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，如果我们定义$\sim\mathcal{X}:=(\underline{R}X^{c},\overline{R}X^{c})$，则有$(S,S)=\sim(\emptyset,\emptyset)$，$(\emptyset,S)=\sim(\emptyset,S)$，以及$(\emptyset,\emptyset)=\sim(\emptyset,\emptyset)$。这意味着$K\cong\mathcal{RS}$。一般而言，给定所有逻辑函数（命题公式）的集合记为$\mathcal{F}$，所有克利尼代数的集合记为$\mathcal{A}_{\mathcal{K}}$，以及所有可能信息系统上所有粗糙集的集合记为$\mathcal{A}_{\mathcal{RS}}$，以下定理（Kumar和Banerjee，[2017](#bib.bib51)）是成立的：
- en: '| (3) |  | $\forall\alpha,\beta\in\mathcal{F},\alpha\vDash_{\mathcal{A}_{\mathcal{K}}}\beta\Leftrightarrow\alpha\vDash_{\mathcal{A}_{\mathcal{RS}}}\beta.$
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\forall\alpha,\beta\in\mathcal{F},\alpha\vDash_{\mathcal{A}_{\mathcal{K}}}\beta\Leftrightarrow\alpha\vDash_{\mathcal{A}_{\mathcal{RS}}}\beta.$
    |  |'
- en: 'The above can be read by: For any logic functions $\alpha$ and $\beta$ in $\mathcal{F}$,
    if $\beta$ is a semantic consequence of $\alpha$ in $\mathcal{A}_{\mathcal{K}}$,
    then $\beta$ is a semantic consequence of $\alpha$ in $\mathcal{A}_{\mathcal{RS}}$.
    We summarize the uncertainty-aware decision making process using Kleene’s TVL
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.3\. Uncertainty Quantification ‣ 3.1\. Kleene’s
    Three-Valued Logic (TVL) ‣ 3\. Decision Making under Uncertainty in Belief Theory
    ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning").'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容可以这样理解：对于$\mathcal{F}$中的任何逻辑函数$\alpha$和$\beta$，如果$\beta$是$\mathcal{A}_{\mathcal{K}}$中$\alpha$的语义蕴涵，那么$\beta$也是$\mathcal{A}_{\mathcal{RS}}$中$\alpha$的语义蕴涵。我们在图[3](#S3.F3
    "Figure 3 ‣ 3.1.3. 不确定性量化 ‣ 3.1. 克利尼三值逻辑 (TVL) ‣ 3. 不确定性下的决策理论 ‣ 不确定性推理与量化的调查：信念理论与深度学习的结合")中总结了使用克利尼三值逻辑的敏感决策过程。
- en: 3.1.2\. Causes and Types of Uncertainty
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2. 不确定性的原因和类型
- en: 'Uncertainty is formalized as a logical value $U$ (unknown) and its relationship
    with two classical logical values $T$ (true) and $F$ (false) are shown in Table [2](#S3.T2
    "Table 2 ‣ 3.1.1\. Belief Formation. ‣ 3.1\. Kleene’s Three-Valued Logic (TVL)
    ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning").
    The stated uncertainty here refers to unpredictability because of a lack of information
    or knowledge. For example, in rough sets, due to unpredictable noises, sets are
    represented by approximation spaces.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性被形式化为逻辑值$U$（未知），其与两个经典逻辑值$T$（真）和$F$（假）的关系如表[2](#S3.T2 "Table 2 ‣ 3.1.1.
    信念形成 ‣ 3.1. 克利尼三值逻辑 (TVL) ‣ 3. 不确定性下的决策理论 ‣ 不确定性推理与量化的调查：信念理论与深度学习的结合")所示。这里所述的不确定性指的是由于缺乏信息或知识而导致的不可预测性。例如，在粗糙集中，由于不可预测的噪声，集合通过近似空间来表示。
- en: 3.1.3\. Uncertainty Quantification
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3. 不确定性量化
- en: '![Refer to caption](img/32f9a10fd96c77eb28207041b533ed82.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/32f9a10fd96c77eb28207041b533ed82.png)'
- en: Figure 3. Uncertainty-aware decision making process using Kleene’s TVL.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 使用克利尼的三值逻辑（TVL）进行的不确定性感知决策过程。
- en: 'Uncertainty in TVL represents an unknown or unspecified state in the decision
    making using TVL. This is related to vacuity uncertainty caused by a lack of information/knowledge
    or non-specificity. Since uncertainty is regarded as a logical value, uncertainty
    value can be quantified through logical operations of logical variables. In Kleene’s
    TVL, the three values of $T$, $U$ (uncertainty), and $F$ are often defined by
    1, 0, and -1\. As seen in Table [2](#S3.T2 "Table 2 ‣ 3.1.1\. Belief Formation.
    ‣ 3.1\. Kleene’s Three-Valued Logic (TVL) ‣ 3\. Decision Making under Uncertainty
    in Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning"), uncertainty, $U$, can be ignored
    under $\wedge$ to decide $T$ or $F$ while it can be used to support $T$ over $F$.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: TVL 中的不确定性表示决策过程中未知或未指定的状态。这与因缺乏信息/知识或非特异性而导致的虚无不确定性有关。由于不确定性被视为逻辑值，不确定性值可以通过逻辑变量的逻辑运算来量化。在
    Kleene 的 TVL 中，$T$、$U$（不确定性）和 $F$ 的三个值通常由 1、0 和 -1 定义。如表 [2](#S3.T2 "Table 2 ‣
    3.1.1\. 信念形成 ‣ 3.1\. Kleene 的三值逻辑 (TVL) ‣ 3\. 信念理论下的不确定性决策 ‣ 关于决策的不确定性推理与量化的调查：信念理论遇见深度学习")
    所示，不确定性 $U$ 可以在 $\wedge$ 下忽略，以决定 $T$ 或 $F$，同时它可以用于支持 $T$ 超过 $F$。
- en: 3.1.4\. Applications of TVL on Machine/Deep Learning
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. TVL 在机器/深度学习中的应用
- en: 'Kashkevich and Krasnoproshin ([1979](#bib.bib46)) defined a function of TVL
    to solve classification problems in pattern recognition tasks. They viewed accepted
    accurate classification as $T$, accepted incorrect classification as $F$, and
    refused classification as $U$. Dahl ([1979](#bib.bib17)) leveraged TVL to construct
    a database used for natural language consultation. Codd ([1986](#bib.bib14)) applied
    TVL in the area of SQLs, with “Null” value behaving like the uncertain value $U$
    in TVL. TVL is rarely observed for its applications in recent research. As the
    topological generalization of TVL, rough sets are used in ML/DL, as described
    in Section [4.3](#S4.SS3 "4.3\. Rough Deep Neural Networks (RDNNs) ‣ 4\. Belief
    Theory Meets Deep Learning ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning").'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Kashkevich 和 Krasnoproshin ([1979](#bib.bib46)) 定义了 TVL 的一个函数来解决模式识别任务中的分类问题。他们将接受的准确分类视为
    $T$，接受的错误分类视为 $F$，拒绝的分类视为 $U$。Dahl ([1979](#bib.bib17)) 利用 TVL 构建了一个用于自然语言咨询的数据库。Codd
    ([1986](#bib.bib14)) 将 TVL 应用于 SQL 领域，其中“Null”值表现得像 TVL 中的不确定值 $U$。最近的研究中很少观察到
    TVL 的应用。作为 TVL 的拓扑概化，粗糙集被用于机器学习/深度学习，如第 [4.3](#S4.SS3 "4.3\. 粗糙深度神经网络 (RDNNs)
    ‣ 4\. 信念理论遇见深度学习 ‣ 关于决策的不确定性推理与量化的调查：信念理论遇见深度学习") 节所述。
- en: 3.2\. Dempster Shafer Theory (DST)
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 邓普斯特-谢弗理论（DST）
- en: DST is a fusion technique for decision-making based on the belief mass (a.k.a.
    evidence) of various detection systems. Each system can be defined as a set of
    possible conclusions, called proposition (Shafer, [1976](#bib.bib72)). The set
    of all propositions is denoted by $\Theta$ (a.k.a. the frame of discernment (FOD)).
    Given the set $\Theta$, we can generate the power set $P(\Theta)$ (a.k.a. the
    powerset of FOD), where the $P(\Theta)$ represents all possible combination of
    the set $\Theta$, including an empty set $\emptyset$. So, $2^{|\Theta|}$ is the
    size of the $P(\Theta)$. As an example of $P(\Theta)$, if $\Theta=\{W,Z,L\}$,
    then $P(\Theta)=\{\emptyset,\{W\},\{Z\},\{L\},\{W,Z\},\{W,L\},\{Z,L\},\{W,Z,L\}\}$.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: DST 是一种基于各种检测系统信念质量（即证据）的决策融合技术。每个系统可以定义为一组可能的结论，称为命题（Shafer, [1976](#bib.bib72)）。所有命题的集合用
    $\Theta$ 表示（即识别框架（FOD））。给定集合 $\Theta$，我们可以生成幂集 $P(\Theta)$（即 FOD 的幂集），其中 $P(\Theta)$
    表示集合 $\Theta$ 的所有可能组合，包括一个空集 $\emptyset$。因此，$2^{|\Theta|}$ 是 $P(\Theta)$ 的大小。例如，如果
    $\Theta=\{W,Z,L\}$，那么 $P(\Theta)=\{\emptyset,\{W\},\{Z\},\{L\},\{W,Z\},\{W,L\},\{Z,L\},\{W,Z,L\}\}$。
- en: 3.2.1\. Belief Formation
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 信念形成
- en: 'The belief mass is an observed probability based on evidences. For example,
    assuming we have a black ball, a black square, and a red ball. The mass for focal
    element $black$ is $m(black)=\frac{2}{3}$, and the mass for focal element $red$
    is $m(red)=\frac{1}{3}$. For a given system, we assign a belief mass to each element
    in power set $P(\Theta)$, and defined the mass function as $m:P(\Theta)\rightarrow[0,1]$.
    The mass function is also called basic belief assignment (bba), and the sum of
    the mass for each element in set $P(\Theta)$ is equal to one, that is (Shafer,
    [1976](#bib.bib72)):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 信念质量是基于证据观察到的概率。例如，假设我们有一个黑球、一个黑方块和一个红球。焦点元素$black$的质量为$m(black)=\frac{2}{3}$，而焦点元素$red$的质量为$m(red)=\frac{1}{3}$。对于给定的系统，我们将信念质量分配给幂集$P(\Theta)$中的每个元素，并定义质量函数为$m:P(\Theta)\rightarrow[0,1]$。质量函数也称为基本信念分配（bba），而幂集$P(\Theta)$中每个元素的质量之和等于1，即（Shafer,
    [1976](#bib.bib72)）：
- en: '| (4) |  | $m:\Theta\rightarrow[0,1],\sum_{A\in P(\Theta)}m(A)=1,\;\;\textsf{where}\;\;m(\emptyset)=0.$
    |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $m:\Theta\rightarrow[0,1],\sum_{A\in P(\Theta)}m(A)=1,\;\;\textsf{where}\;\;m(\emptyset)=0.$
    |  |'
- en: 'Given the power set $P(\Theta)$ and the corresponding belief mass $m$ for each
    focal element (i.e., a subset) $A$ in $P(\Theta)$, we can calculate the belief
    interval of each focal element $A$, and represent it as $[{Bel}(A),{pl}(A)]$.
    The belief ${Bel}(A)$ is the lower bound and plausibility ${pl}(A)$ indicates
    the upper bound (Smarandache et al., [2012](#bib.bib77)). The ${Bel}(A)$ and ${pl}(A)$
    are obtained by:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 给定幂集$P(\Theta)$和每个焦点元素（即子集）$A$的相应信念质量$m$，我们可以计算每个焦点元素$A$的信念区间，并表示为$[{Bel}(A),{pl}(A)]$。信念${Bel}(A)$是下限，而可能性${pl}(A)$表示上限（Smarandache
    et al., [2012](#bib.bib77)）。${Bel}(A)$和${pl}(A)$的计算方法如下：
- en: '| (5) |  | $\displaystyle{Bel}(A)=\sum_{B&#124;B\subseteq A}m(B),\;\;{pl}(A)=\sum_{B&#124;B\cap
    A\neq\emptyset}m(B),\;\;{Dis}(A)=1-{pl}(A).$ |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle{Bel}(A)=\sum_{B|B\subseteq A}m(B),\;\;{pl}(A)=\sum_{B|B\cap
    A\neq\emptyset}m(B),\;\;{Dis}(A)=1-{pl}(A).$ |  |'
- en: '![Refer to caption](img/8a72f3d9fbe6971b70e650c913886527.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a72f3d9fbe6971b70e650c913886527.png)'
- en: Figure 4. Dempster’s Rule of Combination.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图4. 典普斯特组合规则。
- en: '![Refer to caption](img/027961a93e67ad7e2954800ecfc7e476.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/027961a93e67ad7e2954800ecfc7e476.png)'
- en: Figure 5. Belief, Plausibility, and Disbelief in Dempster Shafer Theory.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 典普斯特-谢弗理论中的信念、可能性和不信任度。
- en: For example, given $\Theta=\{W,Z,L\}$ and the belief mass $m(W)$, $m(Z)$, $m(W\text{
    or }L)$, we can obtain beliefs of focal element $W$ and ($W$ and $Z$) by ${Bel}(W)=m(W)$
    and ${Bel}(W\text{ and }Z)=m(W)\cdot m(Z)$, respectively, and plausibility of
    $W$ by $pl(W)=m(W)+m(W\text{ or }L)$. The belief interval for $W$ is denoted by
    $[m(W),m(W)+m(W\text{ or }L)]$.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定$\Theta=\{W,Z,L\}$和信念质量$m(W)$、$m(Z)$、$m(W\text{ or }L)$，我们可以通过${Bel}(W)=m(W)$和${Bel}(W\text{
    and }Z)=m(W)\cdot m(Z)$分别获得焦点元素$W$和（$W$与$Z$）的信念，以及$W$的可能性$pl(W)=m(W)+m(W\text{
    or }L)$。$W$的信念区间表示为$[m(W),m(W)+m(W\text{ or }L)]$。
- en: 'The disbelief of focal element $A$ is represented as ${Dis}(A)$, which equals
    ${Bel}(\overline{A})$, where $\overline{A}$ means the complement of $A$ (i.e.,
    negation of $A$). The ${Dis}(A)$ is calculated by summing all masses of the focal
    elements that do not intersect with $A$. Another way to calculate the ${Dis}(A)$
    is ${Dis}(A)=1-{pl}(A)$ where ${pl}(A)$ can be considered as an estimated uncertainty
    as a potential credit to increase the given belief. While uncertainty is often
    considered risk (van Asselt, [2000](#bib.bib91)), DST uses uncertainty as a credit
    to support a particular belief. Fig. [5](#S3.F5 "Figure 5 ‣ 3.2.1\. Belief Formation
    ‣ 3.2\. Dempster Shafer Theory (DST) ‣ 3\. Decision Making under Uncertainty in
    Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning") describes the key concept of DST.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '焦点元素$A$的不信任度表示为${Dis}(A)$，其值等于${Bel}(\overline{A})$，其中$\overline{A}$表示$A$的补集（即$A$的否定）。${Dis}(A)$通过求和所有与$A$不相交的焦点元素的质量来计算。另一种计算${Dis}(A)$的方法是${Dis}(A)=1-{pl}(A)$，其中${pl}(A)$可以被视为作为潜在信用来增加给定信念的估计不确定性。尽管不确定性通常被视为风险（van
    Asselt, [2000](#bib.bib91)），DST将不确定性作为支持特定信念的信用。图[5](#S3.F5 "Figure 5 ‣ 3.2.1\.
    Belief Formation ‣ 3.2\. Dempster Shafer Theory (DST) ‣ 3\. Decision Making under
    Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning")描述了DST的关键概念。'
- en: 'Dempster’s Rule of Combination is a belief mass combination function for two
    independent detection systems $i$ and $j$ over the same frame. The joint mass
    committed to focal element $A$ is given by:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 典普斯特组合规则是一个用于两个独立检测系统$i$和$j$在同一框架上的信念质量组合函数。分配给焦点元素$A$的联合质量由下式给出：
- en: '| (6) |  | $m(A)=\kappa\sum_{A_{i}\cap B_{j}=A\neq\emptyset}m_{1}(A_{i})\cdot
    m_{2}(B_{j}),$ |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $m(A)=\kappa\sum_{A_{i}\cap B_{j}=A\neq\emptyset}m_{1}(A_{i})\cdot
    m_{2}(B_{j}),$ |  |'
- en: where $A_{i}$ and $B_{j}$ are values in set $\Theta$ of two different systems
    $i$ and $j$ that contain target value $A$. The $\kappa$ is a renormalization constant,
    defined by $\kappa=(1-\sum_{A_{i}\cap B_{j}=\varnothing}m_{1}(A_{i})m_{2}(B_{j}))^{-1}$ (Shafer,
    [1976](#bib.bib72)). For example, we have $m_{1}(W)$, $m_{1}(Z)$, $m_{1}(W,Z)$,
    $m_{2}(W)$, $m_{2}(Z)$, and $m_{2}(W\text{ or }Z)$. The joint mass for focal element
    $W$ is calculated by $m(W)=m_{1}(W)\cdot m_{2}(W)+m_{1}(W)\cdot m_{2}(W\text{
    or }Z)+m_{1}(W\text{ or }Z)\cdot m_{2}(W)+m_{1}(W\text{ or }Z)\cdot m_{2}(W\text{
    or }Z)$.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$A_{i}$和$B_{j}$是两个不同系统$i$和$j$中包含目标值$A$的集合$\Theta$中的值。$\kappa$是一个重归一化常数，由$\kappa=(1-\sum_{A_{i}\cap
    B_{j}=\varnothing}m_{1}(A_{i})m_{2}(B_{j}))^{-1}$定义 (Shafer, [1976](#bib.bib72))。例如，我们有$m_{1}(W)$、$m_{1}(Z)$、$m_{1}(W,Z)$、$m_{2}(W)$、$m_{2}(Z)$和$m_{2}(W\text{
    or }Z)$。焦点元素$W$的联合质量由$m(W)=m_{1}(W)\cdot m_{2}(W)+m_{1}(W)\cdot m_{2}(W\text{
    or }Z)+m_{1}(W\text{ or }Z)\cdot m_{2}(W)+m_{1}(W\text{ or }Z)\cdot m_{2}(W\text{
    or }Z)$计算。
- en: 'We summarize the key concept of Dempter’s rule of combination in Fig. [4](#S3.F4
    "Figure 4 ‣ 3.2.1\. Belief Formation ‣ 3.2\. Dempster Shafer Theory (DST) ‣ 3\.
    Decision Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning
    and Quantification for Decision Making: Belief Theory Meets Deep Learning") based
    on our discussion above. Many DST variants have been proposed. Due to the space
    constraint, we discuss some variants of DST in Appendix A of the supplement document.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [4](#S3.F4 "图 4 ‣ 3.2.1\. 信念形成 ‣ 3.2\. Dempster Shafer理论 (DST) ‣ 3\. 信念理论中的不确定性决策
    ‣ 不确定性推理与量化的综述：信念理论与深度学习的融合")中总结了Dempter组合规则的关键概念，基于我们以上的讨论。许多DST变体已经被提出。由于空间限制，我们在补充文档的附录A中讨论了一些DST的变体。
- en: 3.2.2\. Causes and Types of Uncertainty
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 不确定性的原因和类型
- en: DST considers uncertainty in plausibility due to a lack of evidence. This implies
    that uncertainty in DST is closely related to epistemic uncertainty or vacuity.
    Hence, DST can quantify an uncertain opinion as a subjective belief in a given
    proposition (Shafer, [1976](#bib.bib72)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: DST考虑由于缺乏证据而产生的可行性不确定性。这意味着DST中的不确定性与认知不确定性或空白密切相关。因此，DST可以将不确定的观点量化为对给定命题的主观信念 (Shafer,
    [1976](#bib.bib72))。
- en: 3.2.3\. Uncertainty Quantification
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 不确定性量化
- en: 'Smarandache et al. ([2012](#bib.bib77)) measured uncertainty in DST based on
    its multiple dimensions, including auto-conflict (i.e., conflict in a belief function
    with conjunctive rule), non-specificity (i.e., a generic form of Hartley entropy
    with base 2), confusion (i.e., uncertainty by a lack of evidence), dissonance
    (i.e., all beliefs are mostly the same), aggregate uncertainty measure (AU) (i.e.,
    generalized Shannon entropy), and ambiguity measure (AM) (i.e., non-specificity
    and discord). We provide the detail of each certainty explained above in Appendix
    A of the supplement document. Blasch et al. ([2013](#bib.bib7)) defined the Interval
    of Uncertainty (IOU) by:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Smarandache等人 ([2012](#bib.bib77))根据DST的多个维度测量不确定性，包括自动冲突（即信念函数中的冲突与联接规则）、非特异性（即以2为底的Hartley熵的通用形式）、混淆（即由于缺乏证据的不确定性）、不和谐（即所有信念基本相同）、汇总不确定性测度（AU）（即广义的Shannon熵），以及模糊测度（AM）（即非特异性和不一致性）。我们在补充文档的附录A中详细介绍了上述每种确定性。Blasch等人
    ([2013](#bib.bib7)) 定义了不确定区间（IOU）：
- en: '| (7) |  | $IOU(A)=pl(A)-Bel(A)=1-Bel(\overline{A})-Bel(A)=1-Dis(A)-Bel(A).$
    |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $IOU(A)=pl(A)-Bel(A)=1-Bel(\overline{A})-Bel(A)=1-Dis(A)-Bel(A).$
    |  |'
- en: 'Klir and Ramer ([1990](#bib.bib50)) measured the total uncertainty in DST,
    denoted by $U^{T}(A)$, by considering two types of uncertainty, non-specificity
    and discord. Both $U^{T}(A)$ and AM consider non-specificity and discord and differently
    capture them. AM captures them in a level of each proposition (i.e., element $\theta\in\Theta$)
    while $U^{T}(A)$ obtains them at the level of sets, $A\subset\Theta$. Hence, $U^{T}(A)$
    is given by (Klir and Ramer, [1990](#bib.bib50)):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Klir和Ramer ([1990](#bib.bib50))通过考虑两种不确定性类型，非特异性和不一致性，测量DST中的总不确定性，用$U^{T}(A)$表示。$U^{T}(A)$和AM都考虑了非特异性和不一致性，但它们的捕捉方式不同。AM在每个命题的层面（即元素$\theta\in\Theta$）捕捉这些，而$U^{T}(A)$则在集合的层面$A\subset\Theta$中获取它们。因此，$U^{T}(A)$由(Klir和Ramer,
    [1990](#bib.bib50))给出：
- en: '| (8) |  | $U^{T}(A)=\sum_{A\subset\Theta}m(A)\log_{2}\Bigg{(}\frac{&#124;A&#124;}{\sum_{B\subset\Theta}m(B)\frac{&#124;A\cap
    B&#124;}{&#124;B&#124;}}\Bigg{)}.$ |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $U^{T}(A)=\sum_{A\subset\Theta}m(A)\log_{2}\Bigg{(}\frac{&#124;A&#124;}{\sum_{B\subset\Theta}m(B)\frac{&#124;A\cap
    B&#124;}{&#124;B&#124;}}\Bigg{)}.$ |  |'
- en: The key merit of DST is to combine an amount of uncertain evidence from multiple
    sources and select elements based on the combined belief mass. However, the combination
    rule of DST fails to balance different sources, especially when sources provide
    conflicting evidence. Although many alternative combination rules have been proposed,
    Dubois and Prade ([1988](#bib.bib23)) argued that no single combination rule could
    be used as a universal solution to all encountered situations.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DST的关键优点在于结合来自多个来源的不确定证据，并基于组合的信念质量选择元素。然而，DST的组合规则未能平衡不同来源，特别是当来源提供冲突证据时。尽管已经提出了许多替代组合规则，Dubois和Prade
    ([1988](#bib.bib23)) 认为没有单一的组合规则可以作为所有遇到情况的通用解决方案。
- en: 3.2.4\. Applications of DST on Machine/Deep Learning
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. DST在机器/深度学习中的应用
- en: DST has been used with deep learning techniques. Soua et al. ([2016](#bib.bib82))
    proposed a framework to predict traffic flow using a Deep Belief Network (DBN),
    a class of deep neural network (DNN), to make an identical prediction based on
    two types of data, including data streams and event data. Then DST was used to
    fuse those two predictions. Tong et al. ([2021](#bib.bib86)) studied a set-valued
    classification (SVC) where a sample can be classified as multiple classes, not
    just a single class to identify outliers not represented in a training dataset.
    The authors proposed a technique of combining DST with Convolutional Neural Networks
    (CNNs) to improve the accuracy of the SVC. Tian et al. ([2020](#bib.bib84)) proposed
    a new intrusion detection system (IDS) using DST with Long Short-Term Memory Recurrent
    Neural Network (LSTM-RNN) to combine the results from different classes. Zhang
    et al. ([2020b](#bib.bib115)) proposed a new fault diagnosis technique using an
    improved DST by fusing data from multiple sensors for fault classification.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: DST已与深度学习技术结合使用。Soua等人 ([2016](#bib.bib82)) 提出了一个框架，通过深度信念网络（DBN），一种深度神经网络（DNN）的类别，来预测交通流量，以基于数据流和事件数据这两种数据进行相同的预测。然后使用DST来融合这两个预测。Tong等人
    ([2021](#bib.bib86)) 研究了一种集值分类（SVC），其中一个样本可以被分类为多个类别，而不仅仅是一个类别，以识别在训练数据集中未表示的异常值。作者提出了一种将DST与卷积神经网络（CNNs）结合的技术，以提高SVC的准确性。Tian等人
    ([2020](#bib.bib84)) 提出了一个新的入侵检测系统（IDS），使用DST与长短期记忆递归神经网络（LSTM-RNN）结合不同类别的结果。Zhang等人
    ([2020b](#bib.bib115)) 提出了使用改进的DST的新故障诊断技术，通过融合来自多个传感器的数据进行故障分类。
- en: 3.3\. Transferable Belief Model (TBM)
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 可转移信念模型（TBM）
- en: 'TBM was developed as a variant of DST to resolve unreasonable results of the
    DST combination rule (see Section [3.2](#S3.SS2 "3.2\. Dempster Shafer Theory
    (DST) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning"))
    when multiple sources provide conflicting evidence (Smets and Kennes, [1994](#bib.bib78)).
    TBM is based on the open-world assumption with two levels of belief reasoning:
    credal level and pignistic level. The credal level quantifies and updates a belief
    through a belief function. The pignistic level transfers a belief into a probability
    using the so-called pignistic probability function for making decision (Smets
    and Kennes, [1994](#bib.bib78)).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: TBM作为DST的一个变体被开发出来，以解决DST组合规则的非合理结果（见第[3.2](#S3.SS2 "3.2\. 邓普斯特-沙费尔理论 (DST)
    ‣ 3\. 不确定性下的决策理论 ‣ 关于不确定性推理与量化的调查：信念理论遇见深度学习")节），当多个来源提供冲突证据时(Smets和Kennes, [1994](#bib.bib78))。TBM基于开放世界假设，具有两个信念推理层次：可信层次和皮格尼斯层次。可信层次通过信念函数量化和更新信念。皮格尼斯层次使用所谓的皮格尼斯概率函数将信念转换为概率，以便做出决策（Smets和Kennes,
    [1994](#bib.bib78)）。
- en: 3.3.1\. Belief Formation
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 信念形成
- en: 'TBM defines basic belief masses the same as DST (Shafer, [1976](#bib.bib72))
    (see Eq. ([4](#S3.E4 "In 3.2.1\. Belief Formation ‣ 3.2\. Dempster Shafer Theory
    (DST) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning"))).
    The credal level belief function updating a belief upon the arrival of new evidence
    is formulated by (Smets and Kennes, [1994](#bib.bib78)):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: TBM定义的基本信念质量与DST相同（Shafer, [1976](#bib.bib72)）（见方程式([4](#S3.E4 "在3.2.1\. 信念形成
    ‣ 3.2\. 邓普斯特-沙费尔理论 (DST) ‣ 3\. 不确定性下的决策理论 ‣ 关于不确定性推理与量化的调查：信念理论遇见深度学习"))）。当新证据到达时，信念的可信度信念函数由(Smets和Kennes,
    [1994](#bib.bib78)) 提出。
- en: '| (9) |  | $m_{B}(A)=\begin{cases}\frac{\sum_{C\subseteq\overline{B}}m(A\cup
    C)}{1-\sum_{C\subseteq\overline{B}}m(C)}\;\;\text{for}\;\;(A\subseteq B)\wedge(A\neq\emptyset);\\
    \;\;\;\;\;\;\;\;0\;\;\;\;\;\;\;\;\;\;\;\text{otherwise.}\end{cases}$ |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $m_{B}(A)=\begin{cases}\frac{\sum_{C\subseteq\overline{B}}m(A\cup
    C)}{1-\sum_{C\subseteq\overline{B}}m(C)}\;\;\text{for}\;\;(A\subseteq B)\wedge(A\neq\emptyset);\\
    \;\;\;\;\;\;\;\;0\;\;\;\;\;\;\;\;\;\;\;\text{otherwise.}\end{cases}$ |  |'
- en: 'Here $m_{B}(A)$ means the belief mass supporting propositions $A$ when conditional
    evidence does not support proposition $B$ as the truth (Smets and Kennes, [1994](#bib.bib78)).
    The $\sum_{C\subseteq\overline{B}}m(C)$ refers the sum of beliefs supporting a
    set not supporting $B$ and $\sum_{C\subseteq\overline{B}}m(A\cup C)$ is the sum
    of beliefs not supporting $B$ or supporting $A$. For $m_{B}(A):\mathbb{R}\rightarrow[0,1]$,
    Eq. ([9](#S3.E9 "In 3.3.1\. Belief Formation ‣ 3.3\. Transferable Belief Model
    (TBM) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning"))
    needs to hold $\sum_{C\subseteq\overline{B}}m(A\cup C)<1-\sum_{C\subseteq\overline{B}}m(C)$.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '这里 $m_{B}(A)$ 表示在条件证据不支持命题 $B$ 作为真理时支持命题 $A$ 的信念量（Smets 和 Kennes，[1994](#bib.bib78)）。$\sum_{C\subseteq\overline{B}}m(C)$
    指支持不支持 $B$ 的集合的信念总和，而 $\sum_{C\subseteq\overline{B}}m(A\cup C)$ 是不支持 $B$ 或支持 $A$
    的信念总和。对于 $m_{B}(A):\mathbb{R}\rightarrow[0,1]$，公式 ([9](#S3.E9 "In 3.3.1\. Belief
    Formation ‣ 3.3\. Transferable Belief Model (TBM) ‣ 3\. Decision Making under
    Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning")) 需要满足 $\sum_{C\subseteq\overline{B}}m(A\cup
    C)<1-\sum_{C\subseteq\overline{B}}m(C)$。'
- en: 'The probability transformed through the pignistic probability function for
    decision making is denoted by $BetP$, which is estimated by:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 pignistic 概率函数进行决策的概率表示为 $BetP$，其估计公式为：
- en: '| (10) |  | $BetP(x)=\sum_{x\in A\subseteq X}\frac{m(A)}{&#124;A&#124;}=\sum_{x\in
    A\subseteq X}m(A)\frac{&#124;x\cap A&#124;}{&#124;A&#124;},$ |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $BetP(x)=\sum_{x\in A\subseteq X}\frac{m(A)}{|A|}=\sum_{x\in A\subseteq
    X}m(A)\frac{|x\cap A|}{|A|},$ |  |'
- en: '![Refer to caption](img/807de00ee746d38a8c96d1c819a02974.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/807de00ee746d38a8c96d1c819a02974.png)'
- en: Figure 6. A belief at the credal level and pignistic level and their relationships
    in TBM.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. Credal 层次和 pignistic 层次的信念及其在 TBM 中的关系。
- en: where $\frac{m(A)}{|A|}$ means belief mass, $m(A)$, is evenly distributed into
    the atoms of $A$, a set of atoms, and $|A|$ means the number of atoms $x$ in set
    $A$ (i.e., $x\in A$). The $X$ is the Boolean algebra of the subset of $\Omega$,
    where $\Omega$ is a set of worlds (truth). The probability distribution calculated
    from the pignistic probability function is used for decision making.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\frac{m(A)}{|A|}$ 表示信念量 $m(A)$ 均匀分布到 $A$ 的原子集合中，而 $|A|$ 表示集合 $A$ 中的原子 $x$
    的数量（即 $x\in A$）。$X$ 是 $\Omega$ 的布尔代数，其中 $\Omega$ 是一个世界（真理）集合。从 pignistic 概率函数计算的概率分布用于决策制定。
- en: 3.3.2\. Causes and Types of Uncertainty
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 不确定性的原因与类型
- en: TBM considers epistemic uncertainty caused by a lack of evidence.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: TBM 考虑了由于缺乏证据而造成的认识不确定性。
- en: 3.3.3\. Uncertainty Quantification
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 不确定性量化
- en: 'In TBM, uncertainty has not been explicitly discussed. Since a belief at the
    pignistic level is for decision making in real-world settings, we can understand
    the pignistic probability function gives a belief mass that considers uncertainty
    in practice while the credal level belief function estimates a belief based on
    observed evidence. We show how a belief is constructed at the credal level based
    on the arrival of evidence and how the credal level belief is transferred to the
    pignistic level belief for decision making in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3.1\.
    Belief Formation ‣ 3.3\. Transferable Belief Model (TBM) ‣ 3\. Decision Making
    under Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '在 TBM 中，不确定性没有被明确讨论。由于 pignistic 层次的信念用于现实世界中的决策，我们可以理解 pignistic 概率函数给出了一个考虑实际不确定性的信念量，而
    credal 层次的信念函数则基于观察到的证据来估计信念。我们展示了基于证据的到达如何在 credal 层次构建信念，以及 credal 层次的信念如何转移到
    pignistic 层次的信念以进行决策，如图 [6](#S3.F6 "Figure 6 ‣ 3.3.1\. Belief Formation ‣ 3.3\.
    Transferable Belief Model (TBM) ‣ 3\. Decision Making under Uncertainty in Belief
    Theory ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making:
    Belief Theory Meets Deep Learning") 所示。'
- en: 3.3.4\. Applications of TBM on Machine/Deep Learning
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. TBM 在机器学习/深度学习中的应用
- en: TBM has been used as a competitive algorithm to solve various types of classification
    problems (Zhang et al., [2020a](#bib.bib113); Guil, [2019](#bib.bib29); Quost
    et al., [2005](#bib.bib67); Honer and Hettmann, [2018](#bib.bib36); Henni et al.,
    [2019](#bib.bib32)). However, to the best of our knowledge, we have not found
    any prior work that uses TBM along with ML/DL.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: TBM 被用作解决各种分类问题的竞争算法 (Zhang 等, [2020a](#bib.bib113); Guil, [2019](#bib.bib29);
    Quost 等, [2005](#bib.bib67); Honer 和 Hettmann, [2018](#bib.bib36); Henni 等, [2019](#bib.bib32))。然而，据我们所知，我们尚未找到任何将
    TBM 与 ML/DL 结合使用的先前研究。
- en: 3.4\. Dezert-Smarandache Theory (DSmT)
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. Dezert-Smarandache 理论 (DSmT)
- en: Dezert and Smarandache ([2004](#bib.bib22)) introduced DSmT theory for data
    and information fusion problems as a general framework that provides new rules
    of handling highly imprecise, vague, and uncertain sources of evidence and making
    decisions under them. The main advantages of DSmT over DST are as follows. First,
    DSmT has a more general fusion space in a hyper-power set (discussed in the Appendix
    B.1 of the supplement document), compared to a power set. Second, DSmT fits free
    and a hybrid model compared to a strict DST model (see Appendix B.1 of the supplement
    document). Third, DSmT also combines complex classes based on subsets or complements
    and introduces better fusion rules, such as proportional conflict redistribution
    rule 5 (PCR5), dynamic fusion by hybrid DSm rule (DSmH), a new probability transformation,
    qualitative operators for data with labels (e.g., linguistic labels in natural
    language), and new belief conditioning rules (BCRs), or new fusion rules for set-valued
    imprecise beliefs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Dezert 和 Smarandache ([2004](#bib.bib22)) 引入了 DSmT 理论，用于数据和信息融合问题，作为一个提供处理高度不精确、模糊和不确定证据源的新规则的一般框架，并在这些情况下做出决策。DSmT
    相对于 DST 的主要优势如下。首先，DSmT 在超幂集中的融合空间更为一般（在补充文档的附录 B.1 中讨论），而不是幂集。其次，DSmT 适用于自由和混合模型，而不是严格的
    DST 模型（见补充文档的附录 B.1）。第三，DSmT 还基于子集或补集组合复杂类别，并引入了更好的融合规则，如比例冲突重新分配规则 5 (PCR5)、通过混合
    DSm 规则 (DSmH) 动态融合、新的概率转换、用于带标签的数据的定性运算符（例如自然语言中的语言标签）以及新的信念条件规则 (BCRs)，或用于集值不精确信念的新融合规则。
- en: 3.4.1\. Belief Formation
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 信念形成
- en: 'A generalized basic belief assignment (gbba) is formulated the same as a belief
    function of DST in Eqs. ([4](#S3.E4 "In 3.2.1\. Belief Formation ‣ 3.2\. Dempster
    Shafer Theory (DST) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣
    A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning")) and ([5](#S3.E5 "In 3.2.1\. Belief Formation ‣ 3.2\.
    Dempster Shafer Theory (DST) ‣ 3\. Decision Making under Uncertainty in Belief
    Theory ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making:
    Belief Theory Meets Deep Learning")), but the domain of DSmT is the hyper-power
    set $D^{\Theta}$, compared to a power set $P(\Theta)$ of DST. Note that $P(\Theta)\overset{\Delta}{=}(\Theta,\cup)$,
    $D^{\Theta}\overset{\Delta}{=}(\Theta,\cup,\cap)$, and $S^{\Theta}\overset{\Delta}{=}(\Theta,\cup,\cap,c(\cdot))$.
    If $\Theta=\{a,b\}$, $P(\Theta)\overset{\Delta}{=}(\emptyset,a,b,a\cup b)$, $D^{\Theta}\overset{\Delta}{=}(\emptyset,a,b,a\cup
    b,a\cap b)$, and $S^{\Theta}=(\emptyset,a,b,a\cup b,a\cap b,c(\emptyset),c(a),c(b),c(a\cup
    b),c(a\cap b))$ where $c(X)$ refers to the complement of $X$. We provide the details
    of other various types of belief mass functions introduced in DSmT in Appendix
    B.1 of the supplement document.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一般化基本信念分配 (gbba) 的公式与 DST 中的信念函数在公式 ([4](#S3.E4 "在 3.2.1\. 信念形成 ‣ 3.2\. 邓普斯特-沙弗理论
    (DST) ‣ 3\. 信念理论中的不确定性决策 ‣ 关于决策制定的不确定性推理和量化的调查：信念理论与深度学习的结合")) 和 ([5](#S3.E5 "在
    3.2.1\. 信念形成 ‣ 3.2\. 邓普斯特-沙弗理论 (DST) ‣ 3\. 信念理论中的不确定性决策 ‣ 关于决策制定的不确定性推理和量化的调查：信念理论与深度学习的结合"))
    中的信念函数相同，但 DSmT 的领域是超幂集 $D^{\Theta}$，而不是 DST 的幂集 $P(\Theta)$。注意到 $P(\Theta)\overset{\Delta}{=}(\Theta,\cup)$,
    $D^{\Theta}\overset{\Delta}{=}(\Theta,\cup,\cap)$ 和 $S^{\Theta}\overset{\Delta}{=}(\Theta,\cup,\cap,c(\cdot))$。如果
    $\Theta=\{a,b\}$，$P(\Theta)\overset{\Delta}{=}(\emptyset,a,b,a\cup b)$, $D^{\Theta}\overset{\Delta}{=}(\emptyset,a,b,a\cup
    b,a\cap b)$，以及 $S^{\Theta}=(\emptyset,a,b,a\cup b,a\cap b,c(\emptyset),c(a),c(b),c(a\cup
    b),c(a\cap b))$，其中 $c(X)$ 指代 $X$ 的补集。我们在补充文档的附录 B.1 中提供了 DSmT 中介绍的其他各种信念质量函数的详细信息。
- en: 3.4.2\. Causes and Types of Uncertainty
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. 不确定性的原因和类型
- en: 'DSmT handles various uncertainties as follows (Smarandache and Dezert, [2009](#bib.bib76)):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: DSmT 处理各种不确定性如下 (Smarandache 和 Dezert, [2009](#bib.bib76))：
- en: •
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Precise, uncertain beliefs from multiple sources: The beliefs from multiple
    sources contribute uncertainty even if the belief of each proposition is a precise
    $m(X)$, where each $m(X)$ is only represented by one real number in $[0,1]$ in
    $D^{\Theta}$. Uncertainty exists when a single source provides beliefs about partial
    elements or multiple sources provide conflicting beliefs. For example, for $\Theta=\{\theta_{1},\theta_{2},\theta_{3}\}$,
    two independent sources provide beliefs $m_{1}(\theta_{1})=0.6,m_{1}(\theta_{3})=0.4$
    and $m_{2}(\theta_{2})=0.8,m_{2}(\theta_{3})=0.2$, respectively.'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多个来源的精确、不确定信念：即使每个命题的信念是精确的 $m(X)$，其中每个 $m(X)$ 仅由 $D^{\Theta}$ 中 $[0,1]$ 的一个实数表示，多来源的信念仍然会引入不确定性。当单个来源提供关于部分元素的信念或多个来源提供冲突信念时，会存在不确定性。例如，对于
    $\Theta=\{\theta_{1},\theta_{2},\theta_{3}\}$，两个独立来源提供的信念分别为 $m_{1}(\theta_{1})=0.6,m_{1}(\theta_{3})=0.4$
    和 $m_{2}(\theta_{2})=0.8,m_{2}(\theta_{3})=0.2$。
- en: •
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Highly conflicting evidence from multiple sources: If $k$ multiple sources
    have conflicting evidences toward a same event, there is uncertainty for which
    source to trust. For example, for $\Theta=\{\theta_{1},\theta_{2},\theta_{3}\}$,
    two sources provide $m_{1}(\theta_{1})=0.2,m_{1}(\theta_{2})=0.1,m_{1}(\theta_{3})=0.7$
    and $m_{2}(\theta_{1})=0.5,m_{2}(\theta_{2})=0.4,m_{2}(\theta_{3})=0.1$. The decision
    is based on those conflicting evidence.'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多个来源的高度冲突证据：如果 $k$ 个来源对同一事件有冲突证据，就存在不确定性，无法确定信任哪个来源。例如，对于 $\Theta=\{\theta_{1},\theta_{2},\theta_{3}\}$，两个来源分别提供
    $m_{1}(\theta_{1})=0.2,m_{1}(\theta_{2})=0.1,m_{1}(\theta_{3})=0.7$ 和 $m_{2}(\theta_{1})=0.5,m_{2}(\theta_{2})=0.4,m_{2}(\theta_{3})=0.1$。决策基于这些冲突的证据。
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Imprecise beliefs: Imprecise beliefs are represented by the admissible imprecise.
    Imprecise beliefs can be either quantitative or qualitative. Quantitative imprecise
    beliefs $m^{I}(\cdot)$ are real subunitary intervals of $[0,1]$ or real subunitary
    sets over $D^{\Theta}$. Qualitative $m^{I}(\cdot)$ is a set of labels $L=\{L_{0},L_{1},L_{2},\ldots,L_{m},L_{m+1}\}$
    in order. Imprecise beliefs are common in fusion problems because it is very hard
    to generate precise sources of evidence. For example, the set of ordered sentiment
    labels are $L=\{L_{0},L_{1},L_{2}\}=\{negative,neutral,positive\}$ and the set
    of elements is $\Theta=\{\theta_{1},\theta_{2}\}$. The two sources can give qualitative
    beliefs by sentiment labels as $qm_{1}(\theta_{1})=L_{1},qm_{1}(\theta_{2})=L_{0}$
    and $qm_{2}(\theta_{1})=L_{2},qm_{2}(\theta_{2})=L_{1}$, respectively.'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不精确信念：不精确信念由可接受的不精确信念表示。不精确信念可以是定量的或定性的。定量不精确信念 $m^{I}(\cdot)$ 是 $[0,1]$ 的实子单元区间或
    $D^{\Theta}$ 上的实子单元集。定性 $m^{I}(\cdot)$ 是有序的标签集 $L=\{L_{0},L_{1},L_{2},\ldots,L_{m},L_{m+1}\}$。由于生成精确的证据源非常困难，不精确信念在融合问题中很常见。例如，有序情感标签的集合是
    $L=\{L_{0},L_{1},L_{2}\}=\{negative,neutral,positive\}$，而元素集合是 $\Theta=\{\theta_{1},\theta_{2}\}$。这两个来源可以通过情感标签提供定性信念，分别为
    $qm_{1}(\theta_{1})=L_{1},qm_{1}(\theta_{2})=L_{0}$ 和 $qm_{2}(\theta_{1})=L_{2},qm_{2}(\theta_{2})=L_{1}$。
- en: •
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Subjective probability (DSm probability or DSmP) transformation, fusion space,
    and fusion rules: The criteria (i.e., frame $\Theta$), the set of elements (i.e.,
    $G^{\Theta}$), the choice of combination rule, the probability function, and controllable
    parameter $\epsilon$ for DSmP, all contribute to the uncertainty that can significantly
    impact decision making.'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 主观概率（DSm 概率或 DSmP）转换、融合空间和融合规则：标准（即框架 $\Theta$）、元素集（即 $G^{\Theta}$）、组合规则的选择、概率函数以及
    DSmP 的可控参数 $\epsilon$ 都会影响不确定性，从而显著影响决策。
- en: 3.4.3\. Uncertainty Quantification
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3\. 不确定性量化
- en: 'DSmT does not provide its own uncertainty measure. It borrows other methods
    and helps decision making using the following uncertainty measures:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: DSmT 不提供其自身的不确定性度量。它借用其他方法，并通过以下不确定性度量帮助决策：
- en: •
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In probability theory, uncertainty in proposition $A$ can be defined as (Smarandache,
    [2012](#bib.bib75)):'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在概率论中，命题 $A$ 的不确定性可以定义为 (Smarandache, [2012](#bib.bib75))：
- en: '| (11) |  | $U(A)=\sum_{\begin{subarray}{c}B\in S^{\Theta}\backslash\{\emptyset\},B\cap
    A\neq\emptyset,B\cap C(A)\neq\emptyset\end{subarray}}m(B),$ |  |'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (11) |  | $U(A)=\sum_{\begin{subarray}{c}B\in S^{\Theta}\backslash\{\emptyset\},B\cap
    A\neq\emptyset,B\cap C(A)\neq\emptyset\end{subarray}}m(B),$ |  |'
- en: 'where $A$, $B$, and $C$ are three different elements (i.e., propositions),
    $\Theta$ is a set of the elements, and $S^{\Theta}$ is a super power set. The
    $C(A)$ is the complement of $A$. Uncertainty and IOU can also be defined in the
    same way as DST in Eq. ([7](#S3.E7 "In 3.2.3\. Uncertainty Quantification ‣ 3.2\.
    Dempster Shafer Theory (DST) ‣ 3\. Decision Making under Uncertainty in Belief
    Theory ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making:
    Belief Theory Meets Deep Learning")).'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '其中 $A$、$B$ 和 $C$ 是三个不同的元素（即命题），$\Theta$ 是这些元素的集合，$S^{\Theta}$ 是超幂集。$C(A)$ 是
    $A$ 的补集。不确定性和 IOU 也可以像 DST 中的 Eq. ([7](#S3.E7 "In 3.2.3\. Uncertainty Quantification
    ‣ 3.2\. Dempster Shafer Theory (DST) ‣ 3\. Decision Making under Uncertainty in
    Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning")) 中那样定义。'
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Degree of uncertainty can be evaluated in the probability transformation. Normalized
    Shannon’s entropy is a measure of uncertainty in probability theory and given
    by:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不确定性的程度可以通过概率变换进行评估。归一化的香农熵是概率论中的不确定性度量，其表达式为：
- en: '| (12) |  | $E_{H}=-\frac{\sum_{i=1}^{n}m(\theta_{i})\log_{2}(m(\theta_{i}))}{H_{max}},$
    |  |'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (12) |  | $E_{H}=-\frac{\sum_{i=1}^{n}m(\theta_{i})\log_{2}(m(\theta_{i}))}{H_{max}},$
    |  |'
- en: where $H_{max}$ is the maximal entropy for the uniform distribution.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $H_{max}$ 是均匀分布的最大熵。
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Probabilistic information content (PIC) score refers to the degree of certainty
    which can be estimated by $PIC=1-E_{H}$. Less uncertainty (or higher certainty)
    can lead to a correct and reliable decision.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 概率信息内容（PIC）评分指的是可以通过 $PIC=1-E_{H}$ 估计的确定性程度。较少的不确定性（或较高的确定性）可以导致正确和可靠的决策。
- en: For decision making, DSmT extends the probability function, called classical
    pignistic transformation (CPT) in DST, into two ways. First, CPT can be generalized
    to the Generalized pignistic transformation (GPT). Second, it can be generalized
    to a subjective probability measure of $m(\cdot)$ by $\epsilon\geq 0$ in the new
    probability transformation, $DSmP_{\epsilon}$, which is a probability transformation
    with a subjective measure, $\epsilon$. Due to the space constraint, we discuss
    these two transformation methods in Appendix B of the supplement document.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策制定，DSmT 将概率函数扩展为两种方式，这在 DST 中称为经典的 pignistic 变换（CPT）。首先，CPT 可以推广到广义 pignistic
    变换（GPT）。其次，它可以推广到新的概率变换 $DSmP_{\epsilon}$，其中 $m(\cdot)$ 的主观概率度量为 $\epsilon\geq
    0$。由于空间限制，我们在补充文档的附录 B 中讨论这两种变换方法。
- en: '![Refer to caption](img/dd7e1384c2af376813507a7d3c6b84b8.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dd7e1384c2af376813507a7d3c6b84b8.png)'
- en: Figure 7. Uncertainty-aware decision making process using DSmT where the generalized
    basic belief assignment ($gbba$) is the formal name of $m(\cdot)$ and BetP refers
    to pignistic transformation in $gbba$ domain.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7. 使用 DSmT 的不确定性感知决策过程，其中广义基本信念分配（$gbba$）是 $m(\cdot)$ 的正式名称，BetP 指的是 $gbba$
    领域中的 pignistic 变换。
- en: 'Fig. [7](#S3.F7 "Figure 7 ‣ 3.4.3\. Uncertainty Quantification ‣ 3.4\. Dezert-Smarandache
    Theory (DSmT) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey
    on Uncertainty Reasoning and Quantification for Decision Making: Belief Theory
    Meets Deep Learning") demonstrates the following steps/criteria to make a decision.
    In Fig. [7](#S3.F7 "Figure 7 ‣ 3.4.3\. Uncertainty Quantification ‣ 3.4\. Dezert-Smarandache
    Theory (DSmT) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey
    on Uncertainty Reasoning and Quantification for Decision Making: Belief Theory
    Meets Deep Learning"), (b) and (c) combines the belief masses from different sources.
    Uncertainty is not combined into input evidence and is used to make final decisions
    in (c).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#S3.F7 "Figure 7 ‣ 3.4.3\. Uncertainty Quantification ‣ 3.4\. Dezert-Smarandache
    Theory (DSmT) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey
    on Uncertainty Reasoning and Quantification for Decision Making: Belief Theory
    Meets Deep Learning") 演示了做决策的以下步骤/标准。在图 [7](#S3.F7 "Figure 7 ‣ 3.4.3\. Uncertainty
    Quantification ‣ 3.4\. Dezert-Smarandache Theory (DSmT) ‣ 3\. Decision Making
    under Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning") 中，(b) 和 (c) 将来自不同来源的信念质量结合在一起。不确定性没有结合到输入证据中，而是在
    (c) 中用于做出最终决策。'
- en: (1)
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Belief functions and models are defined in the proper frame $\Theta$ of a given
    problem. In Appendix B of the supplement document, the closed finite set (i.e.,
    frame), denoted by $\Theta$, has $n$ elements of hypotheses. These steps decide
    the elements in the given problem.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信念函数和模型在给定问题的适当框架 $\Theta$ 中定义。在补充文档的附录 B 中，封闭有限集合（即框架） $\Theta$ 包含 $n$ 个假设元素。这些步骤决定了给定问题中的元素。
- en: (2)
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: The belief functions are defined in the proper set of $G^{\Theta}$ (e.g., power,
    hyper, or super set) where $G^{\Theta}$ means any set of items, including power
    set $P^{\Theta}$, hyper power set $D^{\Theta}$, and super power set $S^{\Theta}$.
    DSmT works on any $G^{\Theta}$, but normally $D^{\Theta}$ is used to distinguish
    from $P^{\Theta}$ in DST. This step means the choice of $P^{\Theta}$, $D^{\Theta}$,
    or $S^{\Theta}$.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信任函数在适当的$G^{\Theta}$集合中定义（例如，幂集、超集或超级集），其中$G^{\Theta}$指任何项目的集合，包括幂集$P^{\Theta}$、超幂集$D^{\Theta}$和超级幂集$S^{\Theta}$。DSmT适用于任何$G^{\Theta}$，但通常使用$D^{\Theta}$来区别于DST中的$P^{\Theta}$。这一步骤意味着选择$P^{\Theta}$、$D^{\Theta}$或$S^{\Theta}$。
- en: (3)
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Choose an efficient rule to combine belief functions (in Appendix B.1 of the
    supplement document) for a given problem.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择一个有效的规则来结合信任函数（见补充文档的附录B.1），以解决给定的问题。
- en: (4)
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: Before making a decision, one must use a probability function (e.g., GPT, see
    Eq. (21) in Appendix B.2) or DSmP with a subjective measure) which is from the
    belief functions. The maximum of the GPT function can be used as a decision criterion
    between two choices.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在做出决策之前，必须使用概率函数（例如，GPT，见附录B.2中的公式（21））或带有主观度量的DSmP，这些函数来源于信任函数。GPT函数的最大值可以作为两种选择之间的决策标准。
- en: Making decisions by DSmP can improve the previous probabilistic transformations
    and increase the strength of a critical decision from the total knowledge.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DSmP进行决策可以改进以前的概率变换，并增强来自总体知识的关键决策的力量。
- en: 3.4.4\. Applications of DSmT on Machine/Deep Learning
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4\. DSmT在机器学习/深度学习中的应用
- en: DSmT covers broad information fusion topics of data and sensors in robotics,
    biometrics, image fusion, trust management, situation analysis, or object tracking.
    Various applications such as DSmH (hybrid), DSmP (probabilistic), and DSmT coordinate
    between machine processing and user coordination (Smarandache and Dezert, [2009](#bib.bib76)).
    DSmT serves as an information fusion tool in binary class and multi-class classification
    problems in conjunction with ML/DL models (Abbas et al., [2015](#bib.bib2); Ji
    et al., [[n.d.]](#bib.bib40)). As an extension to Support Vector Machine One-Against-All
    (SVM OAA) model for multi-class classification, DSmT models partial ignorance
    by combining conflicting evidence from two complementary SVM results through PCR6
    rule (see Appendix B.1 of the supplement document) and reduces focal elements
    in the model. A DSmT-based multi-classifier (Ji et al., [[n.d.]](#bib.bib40))
    integrates PCR6 fusion rules into layered ML model structures, including Convolutional
    Neural Network (CNN), Long Short-Term Memory (LSTM), and Random Forests (RF).
    DSmT is applied to the final decision making process by combining multi-signal
    sources of fault characteristics.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: DSmT涵盖了机器人技术、生物识别、图像融合、信任管理、情境分析或物体跟踪中的广泛信息融合主题。各种应用，如DSmH（混合型）、DSmP（概率型）和DSmT，在机器处理和用户协调之间进行协调（Smarandache和Dezert，[2009](#bib.bib76)）。DSmT作为信息融合工具，应用于与ML/DL模型结合的二分类和多分类问题（Abbas等，[2015](#bib.bib2)；Ji等，[[n.d.]](#bib.bib40)）。作为对多分类的支持向量机一对多（SVM
    OAA）模型的扩展，DSmT通过PCR6规则（见补充文档的附录B.1）结合来自两个互补SVM结果的冲突证据，模型中减少了焦点元素。基于DSmT的多分类器（Ji等，[[n.d.]](#bib.bib40)）将PCR6融合规则集成到分层的ML模型结构中，包括卷积神经网络（CNN）、长短期记忆（LSTM）和随机森林（RF）。DSmT应用于通过结合多信号源的故障特征来进行最终决策过程。
- en: 3.5\. Imprecise Dirichlet Model (IDM)
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 不精确的Dirichlet模型（IDM）
- en: Walley ([1996](#bib.bib94)) proposed IDM to derive beliefs based on objective
    statistical inference from multinomial data with no prior information. The inference
    is expressed in terms of posterior upper and lower probabilities. A typical application
    is predicting the color of the next marble from a bag whose contents are initially
    unknown. Objective Bayesian does not satisfy this principle because predicted
    outcome is unknown and we cannot formulate the sample space. In IDM, the inferences
    are expressed as the posterior upper and lower probabilities, $\overline{P}(A|n)$
    and $\underline{P}(A|n)$, where $A$ refers to an event and $n$ is the number of
    observations towards event $A$. In a multinomial sampling (i.e., $k\geq 2$), the
    sample space, any event of interest can be identified as a subset of $\Omega$.
    IDM generates the lower and upper bounds for each value in Beta/Dirichlet PDFs
    (Probability Density Functions).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Walley ([1996](#bib.bib94)) 提出了 IDM，以便根据多项式数据从无先验信息中推导信念。推断以后验的上限和下限概率表示。一个典型的应用是预测从一个初始内容未知的袋子中取出的下一个弹珠的颜色。客观贝叶斯不符合这一原则，因为预测结果未知，且我们无法形成样本空间。在
    IDM 中，推断以后验的上限和下限概率 $\overline{P}(A|n)$ 和 $\underline{P}(A|n)$ 表示，其中 $A$ 表示一个事件，$n$
    是对事件 $A$ 的观察次数。在多项式采样（即 $k\geq 2$）中，样本空间中的任何感兴趣的事件都可以被识别为 $\Omega$ 的一个子集。IDM 生成
    Beta/Dirichlet PDFs（概率密度函数）中每个值的上下界。
- en: 3.5.1\. Belief Formation
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1\. 信念形成
- en: '![Refer to caption](img/c155d85439202ba33f9a8d8cb057e0db.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c155d85439202ba33f9a8d8cb057e0db.png)'
- en: Figure 8. Derivation of the upper and lower bounds in IDM.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8. IDM 中上下界的推导。
- en: According to Walley ([1996](#bib.bib94)), IDM can be defined as the set of all
    Dirichlet $(s,t)$ distribution, such that $0<t_{j}<1$ for $j=1,2,\dots,k$ and
    $\sum_{j=1}^{k}t_{j}=1$ and $s$ is a specified positive constant that does not
    depend on $\Omega$. Walley suggests $s\leq 2$ where $s$ determines how quickly
    the upper and lower probabilities converge as the observation data accumulate.
    This is a prior set and denoted as $\mu_{0}$ to model the prior ignorance about
    chance $\theta$. Given $\theta=\{\theta_{1},\theta_{2},\dots,\theta_{k}\}$, which
    refers to the identical probability distribution of observations. The corresponding
    set of a posterior distribution, denoted by $\mu_{N}$, is composed of all Dirichlet
    $(N+s,\mathbf{t}^{*})$ distribution (i.e., $\mathbf{t}^{*}=\{t_{1}^{*},t_{2}^{*},\ldots,t_{j}^{*},\ldots,t_{k}^{*}\}$),
    where $t^{*}_{j}=\frac{n_{j}+s\times t_{j}}{N+s}$ and $n_{j}$ is the number of
    observations of category $\omega_{j}$ in $N$ trials.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Walley ([1996](#bib.bib94))，IDM 可以定义为所有 Dirichlet $(s,t)$ 分布的集合，其中 $0<t_{j}<1$
    对于 $j=1,2,\dots,k$ 并且 $\sum_{j=1}^{k}t_{j}=1$，$s$ 是一个指定的正数常量，不依赖于 $\Omega$。Walley
    建议 $s\leq 2$，其中 $s$ 决定了随着观察数据的累积，上下概率的收敛速度。这是一个先验集合，表示为 $\mu_{0}$，用于模拟对机会 $\theta$
    的先验无知。给定 $\theta=\{\theta_{1},\theta_{2},\dots,\theta_{k}\}$，表示观察结果的相同概率分布。对应的后验分布集合，表示为
    $\mu_{N}$，由所有 Dirichlet $(N+s,\mathbf{t}^{*})$ 分布（即 $\mathbf{t}^{*}=\{t_{1}^{*},t_{2}^{*},\ldots,t_{j}^{*},\ldots,t_{k}^{*}\}$）组成，其中
    $t^{*}_{j}=\frac{n_{j}+s\times t_{j}}{N+s}$，$n_{j}$ 是在 $N$ 次试验中类别 $\omega_{j}$
    的观察次数。
- en: 'For example, let $A_{j}$ be the event with outcome $\omega_{j}$ from next trial.
    The predictive probability $P(A_{j}|n)$ under Dirichlet $(N+s,\mathbf{t}^{*})$
    is equal to the posterior mean of $\theta$. By maximizing and minimizing $t^{*}_{j}$
    with respect to $t_{j}$ (i.e., $t_{j}\rightarrow 1$ and $t_{j}\rightarrow 0$),
    the posterior upper and lower probabilities of $A_{j}$ are given by:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，设 $A_{j}$ 为下一个试验中结果为 $\omega_{j}$ 的事件。在 Dirichlet $(N+s,\mathbf{t}^{*})$
    下，预测概率 $P(A_{j}|n)$ 等于 $\theta$ 的后验均值。通过对 $t_{j}$（即 $t_{j}\rightarrow 1$ 和 $t_{j}\rightarrow
    0$）进行最大化和最小化 $t^{*}_{j}$，$A_{j}$ 的后验上限和下限概率为：
- en: '| (13) |  | $\displaystyle\overline{P}(A_{j}&#124;n)=\frac{n_{j}+s}{N+s}\;\;\text{for
    $t_{j}\rightarrow 1$};\;\;\;\;\underline{P}(A_{j}&#124;n)=\frac{n_{j}}{N+s}\;\;\text{for
    $t_{j}\rightarrow 0$.}$ |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\displaystyle\overline{P}(A_{j}|n)=\frac{n_{j}+s}{N+s}\;\;\text{对于
    $t_{j}\rightarrow 1$};\;\;\;\;\underline{P}(A_{j}|n)=\frac{n_{j}}{N+s}\;\;\text{对于
    $t_{j}\rightarrow 0$.}$ |  |'
- en: If $s$ is the hidden observation and $N$ is the number of revealed observations,
    those values can be interpreted as the upper and lower bound of relative frequency
    of $A_{j}$. For example, before making any observation, $n_{j}=N=0$, so that $\overline{P}(A_{j}|n)=\frac{s}{s}=1$
    and $\underline{P}(A_{j}|n)=\frac{0}{s}=0$. However, the interval of an IDM bound
    may be out of range under insufficient evidence ($s$) condition. For example,
    if a bag has nine red balls and one black ball, we randomly pick a ball and obtain
    a black ball. Now we have evidence $r(black)=1$, which gives $\underline{P}(black)=\frac{1}{2+1}=\frac{1}{3}$.
    However, we know that the actual probability of having a black ball is $p(black)=\frac{1}{10}$.
    So $\underline{P}(black)>p(black)$ when the number of trials is not sufficient.
    This case shows that actual probability may be outside the range of IDM under
    a lack of evidence (Jøsang, [2016](#bib.bib43)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$s$是隐藏观察，$N$是揭示的观察数量，这些值可以被解释为$A_{j}$的相对频率的上限和下限。例如，在进行任何观察之前，$n_{j}=N=0$，所以$\overline{P}(A_{j}|n)=\frac{s}{s}=1$和$\underline{P}(A_{j}|n)=\frac{0}{s}=0$。然而，在证据（$s$）不足的情况下，IDM的区间可能会超出范围。例如，如果一个袋子里有九个红球和一个黑球，我们随机抽取一个球并获得一个黑球。现在我们有证据$r(black)=1$，这给出了$\underline{P}(black)=\frac{1}{2+1}=\frac{1}{3}$。然而，我们知道实际的黑球概率是$p(black)=\frac{1}{10}$。所以当试验次数不足时，$\underline{P}(black)>p(black)$。这个案例表明，在证据不足的情况下，实际概率可能会超出IDM的范围
    (Jøsang, [2016](#bib.bib43))。
- en: 3.5.2\. Causes and Types of Uncertainty
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2\. 不确定性的原因和类型
- en: In IDM, uncertainty decreases as a more amount of evidence is received. Hence,
    it is aligned with the concept of epistemic uncertainty, which can be reduced
    by increasing an amount of observations (or evidence).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在IDM中，随着接收到的证据量增加，不确定性会减少。因此，它与知识不确定性的概念一致，后者可以通过增加观察（或证据）的数量来减少。
- en: 3.5.3\. Uncertainty Quantification
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3\. 不确定性量化
- en: 'In IDM, the uncertainty is associated with the imprecision whose degree is
    captured by the difference between the posterior upper and lower probabilities
    as:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在IDM中，不确定性与不精确性相关，其程度通过后验上限和下限概率之间的差异来捕捉，如下所示：
- en: '| (14) |  | $\overline{P}(A_{j}&#124;n)-\underline{P}(A_{j}&#124;n)=\frac{s}{N+s}.$
    |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\overline{P}(A_{j}&#124;n)-\underline{P}(A_{j}&#124;n)=\frac{s}{N+s}.$
    |  |'
- en: From the above, we conclude that the imprecision does not depend on event $A_{j}$.
    That is, uncertainty due to the imprecision is based on the amount of hidden observations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面可以得出，不精确性不依赖于事件$A_{j}$。也就是说，由于不精确性引起的不确定性是基于隐藏观察的数量。
- en: 3.5.4\. Applications of IDM on Machine/Deep Learning
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4\. IDM在机器/深度学习中的应用
- en: Utkin ([2015](#bib.bib90)) proposed an algorithm called IDMBoost (Imprecise
    Dirichlet Model Boost), which is an improved version of AdaBoost, one of the well-known
    machine learning algorithms, particularly to improve the overfitting problem with
    a reduced number of iterations. Serafín et al. ([2020](#bib.bib71)) solved imprecise
    classification problems by proposing an improved algorithm of Credal Decision
    Trees (CDTs). CDTs are Decision Trees using imprecise probabilities using the
    Non-Parametric Predictive Inference Model (NPI-M) and showed its outperformance
    over IDM. Corani and de Campos ([2010](#bib.bib15)) presented a tree-augmented
    naïve classifier, called a TANC, based on imprecise probabilities. TANC considered
    prior near-ignorance using the extreme IDM (or EDM). They proved their TANC provides
    an efficient and sensible approximation of the global IDM via extensive comparative
    performance analysis.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Utkin ([2015](#bib.bib90)) 提出了一个名为IDMBoost（不精确Dirichlet模型提升）的算法，这是AdaBoost的改进版本，AdaBoost是著名的机器学习算法之一，特别是通过减少迭代次数来改进过拟合问题。Serafín等人
    ([2020](#bib.bib71)) 通过提出一种改进的Credal决策树（CDTs）算法解决了不精确分类问题。CDTs是使用不精确概率的决策树，使用非参数预测推断模型（NPI-M），并展示了其优于IDM的表现。Corani和de
    Campos ([2010](#bib.bib15)) 提出了一个基于不精确概率的树增强朴素分类器，称为TANC。TANC考虑了使用极端IDM（或EDM）的先验接近无知。他们证明了他们的TANC通过广泛的比较性能分析提供了对全球IDM的高效和合理的近似。
- en: 3.6\. Fuzzy Logic
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 模糊逻辑
- en: 'Łukasiewicz and Alfred Tarski (Łukasiewicz and Tarski, [1930](#bib.bib57))
    first proposed Łukasiewicz logic, which is the most typical case of many valued
    logic. Our discussion focuses on the real-valued semantics of Łukasiewicz logic
    as the backbone of fuzzy logic. Assume $\alpha$ and $\beta$ are two propositional
    formulas with truth values $v(\alpha)=x$ and $v(\beta)=y$, we adopt these semantics
    in the following context:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Łukasiewicz 和 Alfred Tarski（Łukasiewicz and Tarski, [1930](#bib.bib57)）首次提出了
    Łukasiewicz 逻辑，这是许多值逻辑的最典型案例。我们的讨论集中在 Łukasiewicz 逻辑的实值语义上，作为模糊逻辑的核心。假设 $\alpha$
    和 $\beta$ 是两个具有真值 $v(\alpha)=x$ 和 $v(\beta)=y$ 的命题公式，我们在以下背景中采用这些语义：
- en: '| (15) |  | $\displaystyle v(\alpha\vee\beta)=\max\{x,y\},\;\;v(\alpha\wedge\beta)=\min\{x,y\},\;\;v(\sim\alpha)=1-x,$
    |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\displaystyle v(\alpha\vee\beta)=\max\{x,y\},\;\;v(\alpha\wedge\beta)=\min\{x,y\},\;\;v(\sim\alpha)=1-x,$
    |  |'
- en: where $\vee$ refers to logical disjunction, $\wedge$ is logical conjunction,
    and $\sim$ indicates negation.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\vee$ 代表逻辑析取，$\wedge$ 是逻辑合取，$\sim$ 表示否定。
- en: 'Fuzzy logic (Zadeh, [1975b](#bib.bib106)) is a kind of infinite-valued logic
    defined on type $1$ fuzzy sets (Zadeh, [1965](#bib.bib107)). A fuzzy logic truth
    value set $\mathscr{T}$ is a set of linguistic truth-values, which is a language
    generated from a context-free grammar $G$:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊逻辑（Zadeh, [1975b](#bib.bib106)）是一种定义在类型 $1$ 模糊集上的无限值逻辑（Zadeh, [1965](#bib.bib107)）。模糊逻辑真值集合
    $\mathscr{T}$ 是一组语言真值，它是由上下文无关语法 $G$ 生成的语言：
- en: '| (16) |  | $\mathscr{T}=L(G).$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\mathscr{T}=L(G).$ |  |'
- en: 'For each truth value $\tau\in\mathscr{T}$, $\tau$ is defined as a fuzzy subset
    of a truth-value set $l_{\tau}$ of Łukasiewicz logic, which is given by:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个真值 $\tau\in\mathscr{T}$，$\tau$ 被定义为 Łukasiewicz 逻辑的一个真值集合 $l_{\tau}$ 的模糊子集，定义如下：
- en: '| (17) |  | $\tau=\int_{0}^{1}\frac{\mu_{\tau}(v)}{v},$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\tau=\int_{0}^{1}\frac{\mu_{\tau}(v)}{v},$ |  |'
- en: 'where $\mu_{l_{\tau}}:[0,1]\rightarrow c_{\tau}\in[0,1]$ and $\mu_{\tau}:[0,1]\rightarrow[0,c_{\tau}]$
    are defined as the membership function of $l_{\tau}$ and $\tau$, respectively.
    Suppose $\tau$ has a finite support set $\{v_{1},v_{2},\ldots,v_{n}\}\subset[0,1]$,
    then we can write:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu_{l_{\tau}}:[0,1]\rightarrow c_{\tau}\in[0,1]$ 和 $\mu_{\tau}:[0,1]\rightarrow[0,c_{\tau}]$
    分别定义为 $l_{\tau}$ 和 $\tau$ 的隶属函数。假设 $\tau$ 具有有限支撑集 $\{v_{1},v_{2},\ldots,v_{n}\}\subset[0,1]$，则可以写为：
- en: '| (18) |  | $\displaystyle\tau=\frac{\mu_{1}}{v_{1}}+\frac{\mu_{2}}{v_{2}}+\dots+\frac{\mu_{n}}{v_{n}},$
    |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\displaystyle\tau=\frac{\mu_{1}}{v_{1}}+\frac{\mu_{2}}{v_{2}}+\dots+\frac{\mu_{n}}{v_{n}},$
    |  |'
- en: where $\mu_{i}=\mu_{\tau}(v_{i})$ for $i\in[1,n]$ and ‘$+$’ stands for an union
    operation.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu_{i}=\mu_{\tau}(v_{i})$ 对于 $i\in[1,n]$，‘$+$’ 代表并运算。
- en: 'Since truth values are fuzzy subsets of truth-value sets of Łukasiewicz logic,
    logic operations between them can be similarly defined by:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 由于真值是 Łukasiewicz 逻辑的真值集合的模糊子集，它们之间的逻辑运算也可以类似地定义为：
- en: '| (19) |  | $\displaystyle\mu_{\neg\tau_{0}}=1-\mu_{\tau_{0}},\;\;\mu_{\tau_{0}\vee\tau_{1}}=\max\{\mu_{\tau_{0}},\;\;\mu_{\tau_{1}}\},\mu_{\tau_{0}\wedge\tau_{1}}=\min\{\mu_{\tau_{0}},\mu_{\tau_{1}}\},$
    |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\displaystyle\mu_{\neg\tau_{0}}=1-\mu_{\tau_{0}},\;\;\mu_{\tau_{0}\vee\tau_{1}}=\max\{\mu_{\tau_{0}},\;\;\mu_{\tau_{1}}\},\mu_{\tau_{0}\wedge\tau_{1}}=\min\{\mu_{\tau_{0}},\mu_{\tau_{1}}\},$
    |  |'
- en: where $\tau_{0},\tau_{1}\in\mathscr{T}$. From this, we can then derive $\mu_{\tau_{0}\Rightarrow\tau_{1}}=\mu_{\neg\tau_{0}\vee\tau_{1}}=\max\{1-\mu_{\tau_{0}},\mu_{\tau_{1}}\}$
    based on Kleene-Dienes implication.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau_{0},\tau_{1}\in\mathscr{T}$。由此，我们可以基于 Kleene-Dienes 含义推导出 $\mu_{\tau_{0}\Rightarrow\tau_{1}}=\mu_{\neg\tau_{0}\vee\tau_{1}}=\max\{1-\mu_{\tau_{0}},\mu_{\tau_{1}}\}$。
- en: In general, a type $n$ fuzzy set has a membership function defined based on
    the set of fuzzy sets of type $n-1$, where $n\geq 2$. Fuzzy numbers (Zadeh, [1975a](#bib.bib105))
    can also be formulated as instances of fuzzy sets. In other words, each fuzzy
    number is attached to a membership function, that defines a fuzzy set.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，类型 $n$ 的模糊集具有基于类型 $n-1$ 的模糊集集合定义的隶属函数，其中 $n\geq 2$。模糊数（Zadeh, [1975a](#bib.bib105)）也可以被形式化为模糊集的实例。换句话说，每个模糊数都附带一个定义模糊集的隶属函数。
- en: 'A decision making process under fuzzy logic often consists of three phases:
    fuzzification, inference, and defuzzification. A fuzzifier transforms crispy data
    into fuzzy sets. An inference engine does the logical deduction based on given
    fuzzy rules. A defuzzifier transforms the fuzzy relationships to crispy relationships
    and makes a final decision.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊逻辑下的决策过程通常包括三个阶段：模糊化、推理和去模糊化。模糊化器将清晰数据转化为模糊集合。推理引擎根据给定的模糊规则进行逻辑推导。去模糊化器将模糊关系转化为清晰关系，并做出最终决策。
- en: 3.6.1\. Belief Formation
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1\. 置信形成
- en: 'Zadeh ([1968](#bib.bib104)) defined $P(A)$ as the probability of a fuzzy event
    $A$:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Zadeh ([1968](#bib.bib104)) 将 $P(A)$ 定义为模糊事件 $A$ 的概率：
- en: '| (20) |  | $P(A)=\int_{\mathbb{R}^{n}}\mu_{A}(x)dP=E(\mu_{A}),$ |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $P(A)=\int_{\mathbb{R}^{n}}\mu_{A}(x)dP=E(\mu_{A}),$ |  |'
- en: where $A\subseteq\mathbb{R}^{n}$, $\mu_{A}:\mathbb{R}^{n}\rightarrow[0,1]$ is
    the membership function of $A$, and $P(A)$ represents the belief of a fuzzy event
    $A$.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A\subseteq\mathbb{R}^{n}$，$\mu_{A}:\mathbb{R}^{n}\rightarrow[0,1]$ 是 $A$
    的隶属函数，$P(A)$ 表示模糊事件 $A$ 的信念。
- en: 3.6.2\. Causes and Types of Uncertainty
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2\. 不确定性的原因和类型
- en: Uncertainty in fuzzy logic mostly comes from linguistic imprecision or vagueness,
    leading to generating unpredictability, multiple knowledge frames, and/or incomplete
    knowledge.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊逻辑中的不确定性主要来源于语言上的不精确或模糊，从而导致不可预测性、多种知识框架和/或知识不完整。
- en: 3.6.3\. Uncertainty Quantification
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.3\. 不确定性量化
- en: 'Zadeh ([1968](#bib.bib104)) defined two types of fuzzy sets: Type-1 fuzzy set
    and Type-2 fuzzy set. In Type-1 fuzzy sets, the uncertainty of fuzzy events introduces
    unpredictability and multiple knowledge frames. Zadeh formulated the uncertainty
    of a fuzzy event $A$ based on the entropy of event $A$, $H^{P}(A)$, which is given
    by:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Zadeh ([1968](#bib.bib104)) 定义了两种类型的模糊集：Type-1 模糊集和 Type-2 模糊集。在 Type-1 模糊集中，模糊事件的不确定性引入了不可预测性和多种知识框架。Zadeh
    根据事件 $A$ 的熵 $H^{P}(A)$ 来公式化模糊事件 $A$ 的不确定性，公式如下：
- en: '| (21) |  | $H^{P}(A)=-\sum_{i=1}^{n}\mu_{A}(x_{i})P(x_{i})\log P(x_{i}),$
    |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $H^{P}(A)=-\sum_{i=1}^{n}\mu_{A}(x_{i})P(x_{i})\log P(x_{i}),$
    |  |'
- en: where $A=\{x_{1},x_{2},\dots,x_{n}\}$, $\mu_{A}$ is the membership function
    of $A$, and $P=\{P(x_{1}),P(x_{2}),\dots,P(x_{n})\}$. Here, $P(x_{i})$ refers
    to the probability of occurring event $x_{i}$.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A=\{x_{1},x_{2},\dots,x_{n}\}$，$\mu_{A}$ 是 $A$ 的隶属函数，$P=\{P(x_{1}),P(x_{2}),\dots,P(x_{n})\}$。这里，$P(x_{i})$
    指的是事件 $x_{i}$ 发生的概率。
- en: Fuzzy logic research mainly focused on investigating uncertainty measures on
    Type-2 fuzzy sets, which can provide a way of accurately and effectively measuring
    fuzziness and uncertainty characteristics of fuzzy complex systems with two membership
    functions (Zadeh, [1975a](#bib.bib105)). Wu and Mendel ([2007](#bib.bib98)) proposed
    five novel uncertainty metrics, called centroid, cardinality, fuzziness (entropy),
    variance, and skewness, to measure uncertainty in interval Type-2 fuzzy sets.
    They further evaluated these metrics with inter-uncertainty and intra-uncertainty
    raised in words paradigms (Wu and Mendel, [2009](#bib.bib99)). Zhai and Mendel
    ([2011](#bib.bib109)) extended the five metrics to general Type-2 fuzzy sets.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊逻辑研究主要集中于调查 Type-2 模糊集上的不确定性度量，这可以准确有效地衡量具有两个隶属函数的模糊复杂系统的模糊性和不确定性特征 (Zadeh,
    [1975a](#bib.bib105))。Wu 和 Mendel ([2007](#bib.bib98)) 提出了五种新型不确定性度量，称为质心、基数、模糊性（熵）、方差和偏度，用于衡量区间
    Type-2 模糊集中的不确定性。他们进一步评估了这些度量在词语范式中引发的交互不确定性和内部不确定性 (Wu 和 Mendel, [2009](#bib.bib99))。Zhai
    和 Mendel ([2011](#bib.bib109)) 将这五种度量扩展到了普通 Type-2 模糊集。
- en: 3.6.4\. Applications of Fuzzy Logic on Machine/Deep Learning
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.4\. 模糊逻辑在机器学习/深度学习中的应用
- en: '![Refer to caption](img/feae58cb0b1f73877abc0b8211bbef6e.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/feae58cb0b1f73877abc0b8211bbef6e.png)'
- en: Figure 9. Uncertainty-aware decision making process in Fuzzy Logic.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9. 在模糊逻辑中考虑不确定性的决策过程。
- en: 'Recently, fuzzy deep neural networks (FDNNs) are considered for a system using
    both fuzzy logic and deep neural networks (DNNs) to deal with uncertainty or ambiguity
    in data (Das et al., [2020](#bib.bib18)). The methods using FDNNs fall into two
    categories: integrated models and ensemble models. The integrated models integrate
    fuzzy logic as a part of DL models. In particular, Pythagorean Fuzzy Deep Boltzmann
    Machine (PFDBM) (Zheng et al., [2017](#bib.bib122)) was developed based upon the
    DBM (Holyoak, [1987](#bib.bib35)). PFDBM used the Pythagorean Fuzzy Set (PFS) (Yager,
    [2013](#bib.bib101)) to replace standard real-valued parameters. El Hatri and
    Boumhidi ([2018](#bib.bib25)) developed a DL model in which a network architecture
    was designed based on stacked-auto-encoders (SAE) where multiple hyper parameters,
    such as the learning rate and the momentum, were determined using fuzzy logic
    systems. The ensemble models refer to ensembles of DL and fuzzy logic systems
    with three models: models with fuzzy inputs, models with fuzzy outputs, and parallel
    models. Wang et al. ([2016](#bib.bib97)) proposed a DL model that takes fuzzy
    feature points for input for damaged fingerprint classification. Zhang et al.
    ([2014](#bib.bib114)) proposed a model that uses DL with fuzzy granulation features
    to predict time-series data. Chopade and Narvekar ([2017](#bib.bib13)) proposed
    an ensemble of fuzzy logic and DL to predict fuzzy memberships for document summarization.
    Deng et al. ([2016](#bib.bib20)) proposed a DL architecture with DL layers and
    fuzzy membership functions running in parallel. FDNNs have been applied in various
    application domains, such as traffic control (Chen et al., [2018](#bib.bib11);
    Hernandez-Potiomkin et al., [2018](#bib.bib33)), surveillance and security (Chen
    et al., [2015](#bib.bib10); Zheng et al., [2016](#bib.bib123)), text processing (Shirwandkar
    and Kulkarni, [2018](#bib.bib74); Nguyen et al., [2018](#bib.bib64)), image processing (Ahmed
    et al., [2018](#bib.bib4)), and time-series prediction (Luo et al., [2019](#bib.bib58)).'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，模糊深度神经网络（FDNNs）被考虑用于结合模糊逻辑和深度神经网络（DNNs）的系统，以处理数据中的不确定性或模糊性（Das et al., [2020](#bib.bib18)）。使用FDNNs的方法分为两类：集成模型和集成模型。集成模型将模糊逻辑作为深度学习模型的一部分。特别是，毕达哥拉斯模糊深度玻尔兹曼机（PFDBM）（Zheng
    et al., [2017](#bib.bib122)）是基于玻尔兹曼机（DBM）（Holyoak, [1987](#bib.bib35)）开发的。PFDBM使用毕达哥拉斯模糊集（PFS）（Yager,
    [2013](#bib.bib101)）来替代标准的实值参数。El Hatri和Boumhidi（[2018](#bib.bib25)）开发了一种深度学习模型，其中基于堆叠自编码器（SAE）设计了网络架构，并使用模糊逻辑系统确定了多个超参数，如学习率和动量。集成模型指的是将深度学习和模糊逻辑系统集成的三种模型：具有模糊输入的模型、具有模糊输出的模型和并行模型。Wang
    et al.（[2016](#bib.bib97)）提出了一种深度学习模型，该模型以模糊特征点作为输入用于损坏的指纹分类。Zhang et al.（[2014](#bib.bib114)）提出了一种使用模糊颗粒特征的深度学习模型来预测时间序列数据。Chopade和Narvekar（[2017](#bib.bib13)）提出了一种模糊逻辑和深度学习的集成模型，用于预测文档摘要的模糊隶属度。Deng
    et al.（[2016](#bib.bib20)）提出了一种深度学习架构，其中深度学习层和模糊隶属函数并行运行。FDNNs已被应用于各种应用领域，如交通控制（Chen
    et al., [2018](#bib.bib11); Hernandez-Potiomkin et al., [2018](#bib.bib33)）、监控和安全（Chen
    et al., [2015](#bib.bib10); Zheng et al., [2016](#bib.bib123)）、文本处理（Shirwandkar和Kulkarni,
    [2018](#bib.bib74); Nguyen et al., [2018](#bib.bib64)）、图像处理（Ahmed et al., [2018](#bib.bib4)）和时间序列预测（Luo
    et al., [2019](#bib.bib58)）。
- en: 3.7\. Bayesian Inference (BI)
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7. 贝叶斯推断（BI）
- en: Bayesian theory has been evolved for more than a hundred years (Fienberg, [2006](#bib.bib26)).
    Bayesian inference (BI) is the process of inductive learning using Bayer’s rule (Hoff,
    [2009](#bib.bib34)). Inductive learning is the process of estimating characteristics
    of a population from a subset of members of the entire population (Hoff, [2009](#bib.bib34)).
    Although some literature treats BI as an ML technique due to its statistical nature (Tipping,
    [2003](#bib.bib85)), we treat BI as a belief model because it deals with a subjective
    probability representing a belief (Hoff, [2009](#bib.bib34)).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯理论已经发展了超过一百年（Fienberg, [2006](#bib.bib26)）。贝叶斯推断（BI）是利用贝叶斯规则（Hoff, [2009](#bib.bib34)）进行归纳学习的过程。归纳学习是从整个群体的一个子集估计群体特征的过程（Hoff,
    [2009](#bib.bib34)）。尽管一些文献由于其统计性质将BI视为一种机器学习技术（Tipping, [2003](#bib.bib85)），但我们将BI视为一种信念模型，因为它处理表示信念的主观概率（Hoff,
    [2009](#bib.bib34)）。
- en: 3.7.1\. Belief Formation
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.1. 信念形成
- en: 'Bayes’ rule offers a rational tool of updating beliefs of unknown information,
    which connects probabilities and information (Hoff, [2009](#bib.bib34)). Beliefs
    are the statements that can have overlapping domains, such as two beliefs $A$
    and $B$ and $A\cap B\neq\emptyset$. A higher value returned from a belief function
    indicates the higher degree of a given belief. Bayesian inference estimates population
    characteristics $\theta$ from a single dataset sample $y$. A belief is formed
    via three steps:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯规则提供了一种更新未知信息信念的理性工具，它连接了概率和信息（Hoff, [2009](#bib.bib34)）。信念是可以有重叠领域的陈述，例如两个信念
    $A$ 和 $B$，且 $A\cap B\neq\emptyset$。信念函数返回的值越高，表示给定信念的程度越高。贝叶斯推断通过单个数据集样本 $y$ 估计总体特征
    $\theta$。信念的形成包括三个步骤：
- en: (1)
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Prior distribution $p(\theta)$ describes that the belief of $\theta$ being true
    population characteristics.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 先验分布 $p(\theta)$ 描述了对 $\theta$ 作为真实总体特征的信念。
- en: (2)
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Sampling model $p(y|\theta)$ shows the belief where $y$ means a sample of the
    huge sample space $\mathcal{Y}$ if $\theta$ is true and $y$ needs to be estimated.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采样模型 $p(y|\theta)$ 显示了信念，其中 $y$ 代表如果 $\theta$ 为真且 $y$ 需要估计的大样本空间 $\mathcal{Y}$
    的样本。
- en: (3)
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Posterior distribution $p(\theta|y)$ updates the belief about $\theta$ from
    Bayes’ rule based on observed datasets $y$ (Hoff, [2009](#bib.bib34)), for the
    set of all possible parameter values in the parameter space, $\Theta$:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 后验分布 $p(\theta|y)$ 根据观察到的数据集 $y$ 从贝叶斯规则更新对 $\theta$ 的信念（Hoff, [2009](#bib.bib34)），对于参数空间
    $\Theta$ 中所有可能的参数值集合：
- en: '| (22) |  | $p(\theta&#124;y)=\frac{p(y&#124;\theta)p(\theta)}{\int_{\Theta}p(y&#124;\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}.$
    |  |'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (22) |  | $p(\theta&#124;y)=\frac{p(y&#124;\theta)p(\theta)}{\int_{\Theta}p(y&#124;\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}.$
    |  |'
- en: 'Bayesian inference includes conjugate (i.e., prior and posterior distribution
    are in the same class) prior distributions, posterior inference, predictive distributions,
    and confidence regions. There are variants of quantifying the uncertainty of variables
    depending on different sampling methods. Due to the space constraint, we describe
    them in Appendix C of the supplement document. The probability of an event can
    be obtained via the following steps (Hoff, [2009](#bib.bib34)): (1) determine
    proper parameter $\theta$ and sample spaces; (2) select sampling model $p(y|\theta)$
    and collect samples; (3) observe prior distribution $p(\theta)$ by experience
    or select uninformative prior; (4) calculate posterior distribution $p(\theta|y)$
    based on prior and sampling methods; (5) perform sensitivity analysis for a range
    of parameter values; and (6) finalize general estimation of a population mean.
    The reliable estimate of $\theta$ contains a best guess and degree of its confidence.
    We demonstrate the diagram of uncertainty and belief process of Bayesian inference
    in Fig. [10](#S3.F10 "Figure 10 ‣ 3.7.1\. Belief Formation ‣ 3.7\. Bayesian Inference
    (BI) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning").'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯推断包括共轭（即先验和后验分布属于同一类）先验分布、后验推断、预测分布和置信区域。根据不同的采样方法，有多种量化变量不确定性的变体。由于空间限制，我们在补充文档的附录
    C 中描述了它们。事件的概率可以通过以下步骤获得（Hoff, [2009](#bib.bib34)）：（1）确定适当的参数 $\theta$ 和样本空间；（2）选择采样模型
    $p(y|\theta)$ 并收集样本；（3）通过经验观察先验分布 $p(\theta)$ 或选择无信息先验；（4）基于先验和采样方法计算后验分布 $p(\theta|y)$；（5）对一系列参数值进行敏感性分析；（6）最终确定总体均值的总体估计。
    $\theta$ 的可靠估计包含最佳猜测及其置信度。我们在图 [10](#S3.F10 "图 10 ‣ 3.7.1\. 信念形成 ‣ 3.7\. 贝叶斯推断
    (BI) ‣ 3\. 不确定性下的决策理论 ‣ 决策制定中的不确定性推理和量化：信念理论与深度学习相遇") 中展示了贝叶斯推断的不确定性和信念过程图。
- en: '![Refer to caption](img/13820d93e68c0c6d22670bb18f1bac9e.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/13820d93e68c0c6d22670bb18f1bac9e.png)'
- en: 'Figure 10. Uncertainty-aware decision making process using Bayesian inference
    where $p(\theta)$ refers to the probability estimation of $\theta$ that causes
    uncertainty. $Bel(\cdot)$ is the belief of evidence in Eq. ([5](#S3.E5 "In 3.2.1\.
    Belief Formation ‣ 3.2\. Dempster Shafer Theory (DST) ‣ 3\. Decision Making under
    Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning")). When no model is identified,
    Monte Carlo approximation can be used.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10. 使用贝叶斯推断的带有不确定性的决策过程，其中 $p(\theta)$ 表示引起不确定性的 $\theta$ 的概率估计。$Bel(\cdot)$
    是公式 ([5](#S3.E5 "在 3.2.1. 信念形成 ‣ 3.2. 邓普斯特-沙弗理论 (DST) ‣ 3. 决策理论中的不确定性 ‣ 不确定性推理和量化调查：信念理论与深度学习的结合"))
    中的证据信念。当没有识别模型时，可以使用蒙特卡洛近似。
- en: 3.7.2\. Causes and Types of Uncertainty
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.2. 不确定性的原因和类型
- en: A belief is formed with the unknown values of random variables. In a population,
    the parameter of population characteristics $\theta$ may be unknown. This means
    that the conjugate prior belief $p(\theta)$ is unknown. Before obtaining a dataset
    $y$, the subset of a population is also unknown. A sample of dataset $y$ can help
    to reduce the uncertainty about the population characteristics. This type of uncertainty
    is caused by a lack of evidence.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 信念是在随机变量的未知值的基础上形成的。在一个总体中，总体特征的参数 $\theta$ 可能是未知的。这意味着共轭先验信念 $p(\theta)$ 是未知的。在获得数据集
    $y$ 之前，总体的子集也是未知的。数据集 $y$ 的样本可以帮助减少对总体特征的不确定性。这种类型的不确定性是由于缺乏证据造成的。
- en: 3.7.3\. Uncertainty Quantification
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.3. 不确定性量化
- en: 'In single-parameter sampling models, such as Binomial, Poisson, and Monte Carlo
    approximation, the posterior inference variance of the estimated mean $\theta$
    is a measure of uncertainty from the current belief formation. That is, the uncertainty
    is measured by a variance in Binomial model, Poisson model, and Monte Carlo sampling
    by:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在单参数采样模型中，如二项式、泊松和蒙特卡洛近似，估计均值 $\theta$ 的后验推断方差是从当前信念形成中测量的不确定性。这就是说，不确定性通过在二项式模型、泊松模型和蒙特卡洛采样中的方差来衡量：
- en: '| (23) |  | $\displaystyle\mathrm{Var}^{Bin}[\theta&#124;y]=\frac{\mathrm{E}[\theta&#124;y]\mathrm{E}[1-\theta&#124;y]}{a+b+n+1},\;\mathrm{Var}^{Poiss}[\theta&#124;y]=\frac{a+y}{b+n},\;\mathrm{Var}^{MC}[\theta&#124;y]=\sum^{S}_{s=1}\frac{\theta^{(s)}-\overline{\theta})^{2}}{(S-1)},$
    |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $\displaystyle\mathrm{Var}^{Bin}[\theta\mid y]=\frac{\mathrm{E}[\theta\mid
    y]\mathrm{E}[1-\theta\mid y]}{a+b+n+1},\;\mathrm{Var}^{Poiss}[\theta\mid y]=\frac{a+y}{b+n},\;\mathrm{Var}^{MC}[\theta\mid
    y]=\sum^{S}_{s=1}\frac{(\theta^{(s)}-\overline{\theta})^{2}}{(S-1)},$ |  |'
- en: where $n$ is the number of choices of $y$, $a$ and $b$ are the parameters in
    $\mathrm{Beta}(a,b)$ distribution for a Binomial model, $a$ and $b$ are the parameters
    of $\Gamma(a,b)$ distribution for a Poisson model, and $\theta$ is the estimation
    of parameters and $\overline{\theta}$ is the mean of $\theta$ for Monte Carlo
    sampling.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n$ 是 $y$ 的选择数，$a$ 和 $b$ 是二项式模型中 $\mathrm{Beta}(a,b)$ 分布的参数，$a$ 和 $b$ 是泊松模型中
    $\Gamma(a,b)$ 分布的参数，而 $\theta$ 是参数的估计值，$\overline{\theta}$ 是蒙特卡洛采样中 $\theta$ 的均值。
- en: In the normal model with mean $\theta$ and variance $\sigma^{2}$, a joint distribution
    can be transformed to a conditional probability by Eq. (30) in the supplement
    document. The distribution $p(\theta|\sigma^{2},y_{1},\ldots,y_{n})$ is defined
    by Eq. (29) in the supplement document with variance $\tau_{n}^{2}=1/(\frac{1}{\tau_{0}^{2}}+\frac{n}{\sigma^{2}})$.
    The posterior inverse variance $\frac{1}{\tau_{n}^{2}}=\frac{1}{\tau_{0}^{2}}+\frac{n}{\sigma^{2}}$
    indicates that the posterior inverse variance (a.k.a. precision) $1/\tau_{n}^{2}$
    combines sampling precision $1/\sigma^{2}$ and prior precision $1/\tau_{0}^{2}$.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在均值为 $\theta$ 和方差为 $\sigma^{2}$ 的正态模型中，可以通过补充文档中的公式 (30) 将联合分布转化为条件概率。分布 $p(\theta|\sigma^{2},y_{1},\ldots,y_{n})$
    由补充文档中的公式 (29) 定义，其方差为 $\tau_{n}^{2}=1/(\frac{1}{\tau_{0}^{2}}+\frac{n}{\sigma^{2}})$。后验逆方差
    $\frac{1}{\tau_{n}^{2}}=\frac{1}{\tau_{0}^{2}}+\frac{n}{\sigma^{2}}$ 表明后验逆方差（也称为精度）
    $1/\tau_{n}^{2}$ 结合了采样精度 $1/\sigma^{2}$ 和先验精度 $1/\tau_{0}^{2}$。
- en: 3.7.4\. Applications of Bayesian Inference on Machine/Deep Learning
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.4. 贝叶斯推断在机器/深度学习中的应用
- en: Tipping ([2003](#bib.bib85)) introduced how Bayesian inference (BI) is used
    in ML. BI solves a non-deterministic relationship between dependent ($Y$) and
    independent ($X$) variables. Given $N$ data examples and many parameters w, the
    model of probability of $Y$ given $X$ is computed by $P(Y|X)=f(X;\textbf{w})$ (Tipping,
    [2003](#bib.bib85)). The distribution over parameters w can be inferred from Bayes’
    rule. Approximation techniques are the key points, such as least-square, maximum
    likelihood, and regularization. The common choice of a prior is a zero-mean Gaussian
    prior. The Bayesian way of estimating Maximum A Posteriori (MAP) is for posterior
    inference. Marginalization serves an important role in the Bayesian framework (Tipping,
    [2003](#bib.bib85)). Sofman et al. ([2006](#bib.bib81)) used improved robot navigation
    in a linear Gaussian model to estimate the posterior distribution of the general
    Bayesian features and the locale-specific features. Tripathi and Govindaraju ([2007](#bib.bib87))
    used relevance vector machines to predict uncertainty in hydrology. G. Tian and
    Feng ([2011](#bib.bib27)) analyzed brain image segmentation by applying Gaussian
    mixture model (GMM) with a genetic algorithm (GA) and variational expectation-maximization
    algorithm.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Tipping ([2003](#bib.bib85)) 介绍了贝叶斯推断 (BI) 在机器学习中的应用。BI 解决了依赖变量 ($Y$) 和独立变量
    ($X$) 之间的非确定性关系。给定 $N$ 个数据示例和许多参数 w，给定 $X$ 的 $Y$ 的概率模型通过 $P(Y|X)=f(X;\textbf{w})$
    计算（Tipping，[2003](#bib.bib85)）。参数 w 的分布可以从贝叶斯规则中推断出来。近似技术是关键点，如最小二乘法、最大似然法和正则化。常见的先验选择是零均值高斯先验。贝叶斯方式的最大后验估计
    (MAP) 用于后验推断。边际化在贝叶斯框架中发挥了重要作用（Tipping，[2003](#bib.bib85)）。Sofman 等人 ([2006](#bib.bib81))
    使用改进的线性高斯模型进行机器人导航，以估计一般贝叶斯特征和位置特定特征的后验分布。Tripathi 和 Govindaraju ([2007](#bib.bib87))
    使用相关向量机预测水文学中的不确定性。G. Tian 和 Feng ([2011](#bib.bib27)) 通过应用高斯混合模型 (GMM) 与遗传算法
    (GA) 和变分期望最大化算法分析脑图像分割。
- en: 'In the ML/DL, the amount of parameters are expanded in Bayesian neural network
    (BNN) (Wang and Yeung, [2020](#bib.bib95)). BNN is naturally to capture the uncertainty
    for prediction by putting a prior distribution over its weights, such as Gaussian
    prior distribution: $\bm{\theta}\sim\mathcal{N}(0,I)$, where $\bm{\theta}$ is
    the model weights (parameters). Specifically, given a dataset $D=\{X=\{x_{1},\ldots,x_{N}\},Y=\{y_{1},\ldots,y_{N}\}\}$,
    instead of optimizing the deterministic model weights via maximum likelihood estimation
    (MLE), BNN refers to extending standard networks with posterior inference, which
    learns a posterior over model weights $p(\bm{\theta}|D)$ such that model output
    $f(x,\bm{\theta})$ is stochastic.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习/深度学习中，贝叶斯神经网络 (BNN) 扩展了参数的数量（Wang 和 Yeung，[2020](#bib.bib95)）。BNN 通过对其权重施加先验分布，如高斯先验分布：$\bm{\theta}\sim\mathcal{N}(0,I)$，自然地捕捉预测的不确定性，其中
    $\bm{\theta}$ 是模型权重（参数）。具体而言，给定数据集 $D=\{X=\{x_{1},\ldots,x_{N}\},Y=\{y_{1},\ldots,y_{N}\}\}$，BNN
    通过后验推断扩展标准网络，而不是通过最大似然估计 (MLE) 优化确定性模型权重，从而学习一个模型权重的后验分布 $p(\bm{\theta}|D)$，使得模型输出
    $f(x,\bm{\theta})$ 是随机的。
- en: 3.8\. Subjective Logic (SL)
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8. 主观逻辑 (SL)
- en: As a variant of DST, Jøsang ([2016](#bib.bib43)) proposed a belief model, called
    *Subjective Logic* (SL) that describes subjectivity of an opinion in terms of
    multiple belief masses and uncertainty.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 DST 的一个变体，Jøsang ([2016](#bib.bib43)) 提出了一个信念模型，称为*主观逻辑* (SL)，它通过多个信念质量和不确定性来描述观点的主观性。
- en: 3.8.1\. Belief Formation
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.8.1. 信念形成
- en: Since a binomial opinion is a special case of multinomial opinions where the
    number of belief masses is two, for brevity, we only provide the descriptions
    of multinomial opinions and hyper-opinions.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 由于二项意见是多项意见的特例，其中信念质量的数量为二，为简洁起见，我们仅提供多项意见和超意见的描述。
- en: 'Multinomial Opinions: In SL, a multinomial opinion in a given proposition $x$
    is represented by $\omega_{X}=(\bm{b}_{X},u_{X},\bm{a}_{X})$ where a domain is
    $\mathbb{X}$, a random variable $X\in\mathbb{X}$, $\kappa=|\mathbb{X}|>2$ (for
    a binomial opinion, $\kappa=|\mathbb{X}|=2$), and the additivity requirement of
    $\omega_{x}$ is given as $\sum_{x\in\mathbb{X}}\bm{b}_{X}(x)+u_{X}=1$ where each
    parameter refers to: (1) $\bm{b}_{X}$: belief mass distribution over $\mathbb{X}$;
    (2) $u_{X}$: uncertainty mass representing vacuity of evidence; and (3) $\bm{a}_{X}$:
    base rate distribution over $\mathbb{X}$.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 多项意见：在 SL 中，给定命题 $x$ 的多项意见由 $\omega_{X}=(\bm{b}_{X},u_{X},\bm{a}_{X})$ 表示，其中领域是
    $\mathbb{X}$，随机变量 $X\in\mathbb{X}$，$\kappa=|\mathbb{X}|>2$（对于二项意见，$\kappa=|\mathbb{X}|=2$），而
    $\omega_{x}$ 的加性要求为 $\sum_{x\in\mathbb{X}}\bm{b}_{X}(x)+u_{X}=1$，其中每个参数指：（1）$\bm{b}_{X}$：在
    $\mathbb{X}$ 上的信念质量分布；（2）$u_{X}$：表示证据空白的不确定性质量；（3）$\bm{a}_{X}$：在 $\mathbb{X}$
    上的基准率分布。
- en: 'The projected probability distribution of multinomial opinions is given by:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 多项意见的投影概率分布表示为：
- en: '| (24) |  | $\mathbf{P}_{X}(x)=\bm{b}_{X}(x)+\bm{a}_{X}(x)u_{X},\;\;\;\forall
    x\in\mathbb{X}.$ |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $\mathbf{P}_{X}(x)=\bm{b}_{X}(x)+\bm{a}_{X}(x)u_{X},\;\;\;\forall
    x\in\mathbb{X}.$ |  |'
- en: The probability distribution of a multinomial opinion follows Dirichlet distribution (Jøsang,
    [2016](#bib.bib43)).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 多项意见的概率分布遵循 Dirichlet 分布 (Jøsang, [2016](#bib.bib43))。
- en: 'Hyper-opinions: Hyper-opinions represent multiple choices under a specific
    singleton belief $x$ where belief mass is allowed to be assigned to a composite
    value $x\in\mathscr{C}(\mathbb{X})$ consisting of a set of singleton values $x$’s.
    Belief masses assigned to composite values $x\in\mathscr{C}(\mathbb{X})$ can be
    used to estimate the vagueness of an opinion. Hyperdomain, denoted by $\mathscr{R}(\mathbb{X})$,
    is the reduced powerset of $\mathbb{X}$ which is the set of $\mathscr{P}(\mathbb{X})$
    that excludes $\{\mathbb{X}\}$ and $\{\emptyset\}$. Hyperdomain can be defined
    by:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 超意见：超意见表示在特定单点信念 $x$ 下的多重选择，其中信念质量被分配到由一组单点值 $x$ 组成的复合值 $x\in\mathscr{C}(\mathbb{X})$。分配给复合值
    $x\in\mathscr{C}(\mathbb{X})$ 的信念质量可以用来估计意见的模糊性。超领域，记作 $\mathscr{R}(\mathbb{X})$，是
    $\mathbb{X}$ 的简化幂集，即排除了 $\{\mathbb{X}\}$ 和 $\{\emptyset\}$ 的 $\mathscr{P}(\mathbb{X})$
    集合。超领域可以定义为：
- en: '| (25) |  | $\text{Hyperdomain}:\mathscr{R}(\mathbb{X})=\mathscr{P}(\mathbb{X})\backslash\{\{\mathbb{X}\},\{\emptyset\}\}.\vspace{-1mm}$
    |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $\text{超领域}:\mathscr{R}(\mathbb{X})=\mathscr{P}(\mathbb{X})\backslash\{\{\mathbb{X}\},\{\emptyset\}\}.\vspace{-1mm}$
    |  |'
- en: 'Given $X$ as a hyper variable in $\mathscr{R}(\mathbb{X})$, a hyper-opinion
    on $X$ is represented by $\omega_{X}=(\bm{b}_{X},u_{X},\bm{a}_{X})$ where each
    opinion dimension includes: (1) $\bm{b}_{X}$: belief mass distribution over $\mathscr{R}(\mathbb{X})$;
    (2) $u_{X}$: uncertainty mass representing vacuity of evidence; and (3) $\bm{a}_{X}$:
    base rate distribution over $\mathbb{X}$, where $\sum_{x\in\mathscr{R}(\mathbb{X})}\bm{b}_{X}(x)+u_{X}=1$.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 $X$ 作为 $\mathscr{R}(\mathbb{X})$ 中的超变量，$X$ 上的超意见由 $\omega_{X}=(\bm{b}_{X},u_{X},\bm{a}_{X})$
    表示，其中每个意见维度包括：（1）$\bm{b}_{X}$：在 $\mathscr{R}(\mathbb{X})$ 上的信念质量分布；（2）$u_{X}$：表示证据空白的不确定性质量；（3）$\bm{a}_{X}$：在
    $\mathbb{X}$ 上的基准率分布，其中 $\sum_{x\in\mathscr{R}(\mathbb{X})}\bm{b}_{X}(x)+u_{X}=1$。
- en: 'The projected probability distribution of a hyper-opinion can be given by:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 超意见的投影概率分布可以表示为：
- en: '| (26) |  | $\small\mathbf{P}_{X}(x)=\sum_{x_{i}\in\mathscr{R}(\mathbb{X})}\bm{a}_{X}(x&#124;x_{i})\bm{b}_{X}(x_{i})+\bm{a}_{X}(x)u_{X},\;\;\bm{a}_{X}(x&#124;x_{i})=\frac{\bm{a}_{X}(x\cap
    x_{i})}{\bm{a}_{X}(x_{i})},\forall x,x_{i}\in\mathscr{R}(\mathbb{X}),$ |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $\small\mathbf{P}_{X}(x)=\sum_{x_{i}\in\mathscr{R}(\mathbb{X})}\bm{a}_{X}(x&#124;x_{i})\bm{b}_{X}(x_{i})+\bm{a}_{X}(x)u_{X},\;\;\bm{a}_{X}(x&#124;x_{i})=\frac{\bm{a}_{X}(x\cap
    x_{i})}{\bm{a}_{X}(x_{i})},\forall x,x_{i}\in\mathscr{R}(\mathbb{X}),$ |  |'
- en: where $\bm{a}_{X}(x|x_{j})$ is the relative base rate and $\bm{a}_{X}(x_{i})\neq
    0$. For the binomial or multinomial opinions, the additivity requirement is met
    (i.e., $\sum_{x\in\mathbb{X}}\mathbf{P}_{X}(x)=1$). However, for the hyper-opinion,
    the additivity requirement may not be met, but $\mathbf{P}_{X}(x)$ follows super-additivity
    (i.e., $\sum_{x\in\mathscr{R}(\mathbb{X})}\mathbf{P}_{X}(x)\geq 1$) with a hyperdomain,
    $\mathscr{R}(\mathbb{X})$.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{a}_{X}(x|x_{j})$ 是相对基准率，且 $\bm{a}_{X}(x_{i})\neq 0$。对于二项或多项意见，加性要求是满足的（即
    $\sum_{x\in\mathbb{X}}\mathbf{P}_{X}(x)=1$）。然而，对于超意见，加性要求可能不满足，但 $\mathbf{P}_{X}(x)$
    遵循超加性（即 $\sum_{x\in\mathscr{R}(\mathbb{X})}\mathbf{P}_{X}(x)\geq 1$），在超领域 $\mathscr{R}(\mathbb{X})$
    中。
- en: Hyper-opinions can be represented by Dirichlet PDFs and the hyper-Dirichlet
    distribution (Hankin, [2010](#bib.bib30)). To do so, we can project a hyper-opinion
    into a multinomial opinion based on (Jøsang, [2016](#bib.bib43)). The approximation
    by the projection of hyper-opinions to multinomial opinions removes vague information
    in the representation of opinions. This allows a decision maker to see a particular
    opinion without the veil of vagueness, which facilitates a more direct and intuitive
    interpretation of the opinion.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 超级意见可以通过 Dirichlet 概率密度函数和超 Dirichlet 分布（Hankin, [2010](#bib.bib30)）来表示。为此，我们可以根据
    (Jøsang, [2016](#bib.bib43)) 将超级意见投影到多项式意见中。通过将超级意见投影到多项式意见的近似方法去除了意见表示中的模糊信息。这使得决策者能够在没有模糊面纱的情况下看到特定意见，从而促进了意见的更直接和直观的解释。
- en: 3.8.2\. Causes and Types of Uncertainty
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.8.2\. 不确定性的原因和类型
- en: 'In SL, three types of uncertainties are discussed as follows (Jøsang et al.,
    [2018](#bib.bib44)): vacuity, vagueness, and dissonance. Vacuity uncertainty is
    caused by a lack of evidence or knowledge. Vagueness uncertainty is caused by
    vague observations, leading to failing in identifying a distinctive singleton
    belief. Dissonance uncertainty is introduced due to conflicting evidence, resulting
    in inconclusiveness. Vacuity and dissonance can be understood as epistemic uncertainty,
    which can be reduced with more evidence. Vagueness is related to fuzziness, which
    triggers aleatoric uncertainty in its nature.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SL 中，讨论了三种类型的不确定性，如下所述 (Jøsang et al., [2018](#bib.bib44))：空虚度、模糊度和不一致度。空虚度不确定性是由于缺乏证据或知识引起的。模糊度不确定性是由于模糊的观察导致未能识别出明显的单一信念。不一致度不确定性是由于冲突的证据引起的，导致结论不明确。空虚度和不一致度可以理解为认知不确定性，可以通过更多证据来减少。而模糊度与模糊性相关，这本质上会引发偶然性不确定性。
- en: 3.8.3\. Uncertainty Quantification
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.8.3\. 不确定性量化
- en: 'Uncertainty measures across all belief masses are calculated based on the sum
    of uncertainty masses associated with individual belief masses, as discussed above.
    They include total vacuity (same as $u_{X}$), total vagueness ( $b^{\mathrm{TV}}_{X}$),
    and total dissonance ($\dot{b}_{X}^{\mathrm{Diss}}$):'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 所有信念质量的不确定性度量是基于与单个信念质量相关的不确定性质量的总和计算的，如上所述。它们包括总空虚度（同 $u_{X}$）、总模糊度（$b^{\mathrm{TV}}_{X}$）和总不一致度（$\dot{b}_{X}^{\mathrm{Diss}}$）：
- en: '| (27) |  | $\displaystyle u_{X}=\!\!\!\!\sum\limits_{x\in\mathscr{R}(\mathbb{X})}\mathbf{u}^{F}_{X}(x),\;\;b^{\mathrm{TV}}_{X}=\!\!\!\!\sum\limits_{x\in\mathscr{R}(\mathbb{X})}\!\!\!\!\bm{b}_{X}(x),\;\;\dot{b}_{X}^{\mathrm{Diss}}=\sum\limits_{x_{i}\in\mathbb{X}}\bm{b}^{\mathrm{Diss}}_{X}(x_{i}),$
    |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $\displaystyle u_{X}=\!\!\!\!\sum\limits_{x\in\mathscr{R}(\mathbb{X})}\mathbf{u}^{F}_{X}(x),\;\;b^{\mathrm{TV}}_{X}=\!\!\!\!\sum\limits_{x\in\mathscr{R}(\mathbb{X})}\!\!\!\!\bm{b}_{X}(x),\;\;\dot{b}_{X}^{\mathrm{Diss}}=\sum\limits_{x_{i}\in\mathbb{X}}\bm{b}^{\mathrm{Diss}}_{X}(x_{i}),$
    |  |'
- en: where $\mathbf{u}^{F}_{X}(x)$ refers to a focal uncertainty (vacuity per belief),
    $\bm{b}_{X}(x)$ is a belief mass supporting $x$, and $\bm{b}^{\mathrm{Diss}}_{X}(x_{i})$
    indicates dissonance per singleton belief. All these belief masses are detailed
    in Appendix D.2 of the supplement document.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}^{F}_{X}(x)$ 指的是焦点不确定性（每个信念的空虚度），$\bm{b}_{X}(x)$ 是支持 $x$ 的信念质量，而
    $\bm{b}^{\mathrm{Diss}}_{X}(x_{i})$ 表示每个单一信念的不一致度。所有这些信念质量在补充文档的附录 D.2 中有详细描述。
- en: The uncertainty associated with each belief is elaborated in Appendix D of the
    supplement document. In addition, Jøsang ([2016](#bib.bib43)) proposed a technique
    called uncertainty maximization to offset the amount of evidence received in the
    past to consider additional new evidence because additional new evidence does
    not change the current belief states significantly if uncertainty is very low.
    We provided the uncertainty maximization formulation in Appendix D of the supplement
    document.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 与每个信念相关的不确定性在补充文档的附录 D 中进行了详细阐述。此外，Jøsang ([2016](#bib.bib43)) 提出了一个叫做不确定性最大化的技术，用以抵消过去收到的证据量，以考虑额外的新证据，因为如果不确定性非常低，额外的新证据不会显著改变当前的信念状态。我们在补充文档的附录
    D 中提供了不确定性最大化的公式。
- en: 'When one makes a decision under uncertainty using SL, we can leverage SL’s
    capability to estimate multidimensional uncertainty (i.e., vagueness, vacuity,
    and dissonance) to make effective decisions. As in Fig. [11](#S3.F11 "Figure 11
    ‣ 3.8.3\. Uncertainty Quantification ‣ 3.8\. Subjective Logic (SL) ‣ 3\. Decision
    Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning
    and Quantification for Decision Making: Belief Theory Meets Deep Learning"), after
    estimating multiple dimensions of uncertainty, one can use the vacuity maximization
    technique if more evidence is needed in order to allow considering more evidence
    even under low vacuity, representing high certainty due to a large volume of evidence
    collected. Recall that SL-based opinion cannot be updated or is rarely updated
    significantly if its vacuity is or close to zero. One can also consider other
    opinions by using a variety of fusion operators in SL (Jøsang, [2016](#bib.bib43)),
    which can generate a single opinion with the updates of corresponding belief masses
    and vacuity values. The generated single opinion can be also assessed based on
    which decision has the most utility by normalizing the opinion based on each decision
    (i.e., belief mass)’s utility. Most decision making problems can be solved by
    these processes and allows us to make a decision with minimum uncertainty and
    maximum utility. However, if all decisions have the same uncertainty-aware maximum
    utility, one can select a decision at random, which we want to avoid.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '当使用主观逻辑在不确定性下做出决策时，我们可以利用主观逻辑估计多维不确定性（即模糊性、空白性和不和谐性）的能力来做出有效决策。如图 [11](#S3.F11
    "Figure 11 ‣ 3.8.3\. Uncertainty Quantification ‣ 3.8\. Subjective Logic (SL)
    ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning")所示，在估计了多个不确定性维度之后，如果需要更多证据以便在低空白性下考虑更多证据，可以使用空白性最大化技术，这表示由于收集了大量证据而具有较高的确定性。请记住，如果主观逻辑基础上的意见的空白性为零或接近零，则该意见不能更新或很少显著更新。还可以通过使用主观逻辑中的各种融合运算符（Jøsang,
    [2016](#bib.bib43)）来考虑其他意见，这可以生成一个更新了相应信念质量和空白性值的单一意见。生成的单一意见也可以通过根据每个决策（即信念质量）效用的标准来评估哪个决策具有最大效用。这些过程可以解决大多数决策问题，并允许我们在最小不确定性和最大效用下做出决策。然而，如果所有决策具有相同的不确定性感知最大效用，则可以随机选择一个决策，但我们希望避免这种情况。'
- en: '![Refer to caption](img/27066bc7e59f813bbee397e9095e61d4.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/27066bc7e59f813bbee397e9095e61d4.png)'
- en: Figure 11. Uncertainty-aware decision making process using Subjective Logic.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11. 使用主观逻辑的不确定性感知决策过程
- en: 3.8.4\. Applications of SL on Machine/Deep Learning
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.8.4\. 主观逻辑在机器/深度学习中的应用
- en: Recently SL has been considered along with machine/deep learning. Uncertainty
    reasoning to solve classification tasks has been studied by leveraging SL to consider
    vacuity and dissonance uncertainty dimensions (Sensoy et al., [2018](#bib.bib70);
    Zhao et al., [2019b](#bib.bib120)). In addition, SL-based opinion formulation
    is used to infer subjective opinions along with DL in the presence of adversarial
    attacks (Alim et al., [2019](#bib.bib5); Zhao et al., [2018a](#bib.bib116), [b](#bib.bib117)).
    Further, SL-based opinions are considered along with deep reinforcement learning
    to propose uncertainty-aware decision making (Zhao et al., [2019a](#bib.bib119)).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，主观逻辑（SL）与机器/深度学习一起被考虑。通过利用主观逻辑来考虑空白性和不和谐性不确定性维度，研究了不确定性推理以解决分类任务（Sensoy et
    al., [2018](#bib.bib70); Zhao et al., [2019b](#bib.bib120)）。此外，基于主观逻辑的意见形成被用于在对抗攻击的情况下结合深度学习推断主观意见（Alim
    et al., [2019](#bib.bib5); Zhao et al., [2018a](#bib.bib116), [b](#bib.bib117)）。进一步地，基于主观逻辑的意见与深度强化学习结合，被用来提出不确定性感知决策（Zhao
    et al., [2019a](#bib.bib119)）。
- en: 4\. Belief Theory Meets Deep Learning
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 信念理论与深度学习相遇
- en: In this section, we review several hybrid frameworks that combine belief models
    and neural networks, including evidential (or subjective) neural networks, fuzzy
    neural networks, and rough deep neural networks.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们回顾了几种结合信念模型和神经网络的混合框架，包括证据（或主观）神经网络、模糊神经网络和粗糙深度神经网络。
- en: 4.1\. Evidential Neural Networks (ENNs)
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 证据神经网络（ENNs）
- en: Evidential neural networks (ENNs) (Sensoy et al., [2018](#bib.bib70)) is a hybrid
    framework of subjective belief models and neural networks. They are similar to
    classic neural networks for classification. The main difference is that the softmax
    layer is replaced with an activation function in ENNs, e.g., ReLU, to ensure non-negative
    output (i.e., range of $[0,+\infty]$), which is taken as the evidence vector for
    the predicted Dirichlet distribution, or equivalently, multinomial opinion.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 证据神经网络（ENNs）(Sensoy 等，[2018](#bib.bib70)) 是主观信念模型和神经网络的混合框架。它们类似于经典的分类神经网络。主要区别在于
    ENNs 中的 softmax 层被激活函数（例如 ReLU）替代，以确保非负输出（即 $[0,+\infty]$ 范围），作为预测的 Dirichlet
    分布的证据向量，或者等效地，多项式意见。
- en: 4.1.1\. Key Formulation of ENNs
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. ENNs 的关键公式
- en: 'Given the feature vector ${\bf x}$ of an input sample, let $f({\bf x}|{\bm{\theta}})$
    represent the evidence vector by the network for the classification, where ${\bm{\theta}}$
    is network parameters. Then the corresponding Dirichlet distribution has parameters
    ${\bm{\alpha}}=f({\bf x}_{i}|{\bm{\theta}})+1$, where the $k$-th parameter ${\alpha}_{k}$
    denotes the effective number of observations of the $k$-th class, and the total
    number of classes is $K$. Let $\textbf{p}=(p_{1},\dots,p_{K})^{T}$ be the probabilities
    of the $K$ predefined classes. The Dirichlet PDF (i.e., $\text{Dir}({\bf p};{\bm{\alpha}})$)
    with ${\bf p}$ as a random vector is defined by:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入样本 ${\bf x}$ 的特征向量，令 $f({\bf x}|{\bm{\theta}})$ 表示网络为分类提供的证据向量，其中 ${\bm{\theta}}$
    是网络参数。则相应的 Dirichlet 分布具有参数 ${\bm{\alpha}}=f({\bf x}_{i}|{\bm{\theta}})+1$，其中第
    $k$ 个参数 ${\alpha}_{k}$ 表示第 $k$ 类的有效观测次数，总类别数为 $K$。令 $\textbf{p}=(p_{1},\dots,p_{K})^{T}$
    为 $K$ 个预定义类别的概率。Dirichlet PDF（即 $\text{Dir}({\bf p};{\bm{\alpha}})$）以 ${\bf p}$
    作为随机向量定义为：
- en: '| (28) |  | $\displaystyle\mathrm{Dir}(\bm{p}&#124;{\bm{\alpha}})=\frac{1}{B({\bm{\alpha}})}\prod\nolimits_{k\in\mathbb{Y}}p_{k}^{(\alpha_{k}-1)},$
    |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $\displaystyle\mathrm{Dir}(\bm{p}&#124;{\bm{\alpha}})=\frac{1}{B({\bm{\alpha}})}\prod\nolimits_{k\in\mathbb{Y}}p_{k}^{(\alpha_{k}-1)},$
    |  |'
- en: 'where $\frac{1}{B({\bm{\alpha}})}=\frac{\Gamma(\sum_{k\in\mathbb{Y}}\alpha_{k})}{\prod_{k\in\mathbb{Y}}(\alpha_{k})}$,
    $\alpha_{k}\geq 0$, and $p_{k}\neq 0$, if $\alpha_{k}<1$. The expected value of
    class probabilities $\textbf{p}=(p_{1},\ldots,p_{K})^{T}$ is given by:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\frac{1}{B({\bm{\alpha}})}=\frac{\Gamma(\sum_{k\in\mathbb{Y}}\alpha_{k})}{\prod_{k\in\mathbb{Y}}(\alpha_{k})}$,
    $\alpha_{k}\geq 0$，且当 $\alpha_{k}<1$ 时 $p_{k}\neq 0$。类别概率 $\textbf{p}=(p_{1},\ldots,p_{K})^{T}$
    的期望值为：
- en: '| (29) |  | $\displaystyle\mathbb{E}[p_{k}]=\frac{\alpha_{k}}{\sum_{j=1}^{K}\alpha_{j}}=\frac{e_{k}+a_{k}W}{\sum_{j=1}^{K}e_{j}+W}.$
    |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $\displaystyle\mathbb{E}[p_{k}]=\frac{\alpha_{k}}{\sum_{j=1}^{K}\alpha_{j}}=\frac{e_{k}+a_{k}W}{\sum_{j=1}^{K}e_{j}+W}.$
    |  |'
- en: 'The observed evidence in a Dirichlet PDF $\mathrm{Dir}(\bm{p}|{\bm{\alpha}})$
    can be mapped to a multinomial opinion $(b_{1},\cdots,b_{K},u)$ as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Dirichlet PDF $\mathrm{Dir}(\bm{p}|{\bm{\alpha}})$ 中观察到的证据可以映射到多项式意见 $(b_{1},\cdots,b_{K},u)$
    如下：
- en: '| (30) |  | $\displaystyle b_{k}=\frac{e_{k}}{S},\;\;u=\frac{W}{S},\text{ for
    }k=1,\cdots,K,$ |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $\displaystyle b_{k}=\frac{e_{k}}{S},\;\;u=\frac{W}{S},\text{ for
    }k=1,\cdots,K,$ |  |'
- en: 'where $S=\sum_{k=1}^{K}\alpha_{k}$ refers to the Dirichlet strength. Without
    loss of generality, we set $a_{k}=1/K$ and the non-informative prior weight (i.e.,
    $W=K$), which indicates that $a_{k}\cdot W=1$ for each $k\in\{1,\cdots,K\}$. Therefore,
    the output of an ENN can be applied to measure the subjective uncertainty about
    the predictive class variable $y$ in different types, such as vacuity and dissonance
    as defined based on a multinomial opinion (See Section [3.8](#S3.SS8 "3.8\. Subjective
    Logic (SL) ‣ 3\. Decision Making under Uncertainty in Belief Theory ‣ A Survey
    on Uncertainty Reasoning and Quantification for Decision Making: Belief Theory
    Meets Deep Learning")).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S=\sum_{k=1}^{K}\alpha_{k}$ 表示 Dirichlet 强度。为了不失一般性，我们设定 $a_{k}=1/K$ 和非信息先验权重（即
    $W=K$），这表示 $a_{k}\cdot W=1$ 对每个 $k\in\{1,\cdots,K\}$。因此，ENN 的输出可以用于衡量关于预测类别变量
    $y$ 的主观不确定性，涉及不同类型，如基于多项意见定义的空洞性和不和谐性（参见第 [3.8](#S3.SS8 "3.8\. 主观逻辑 (SL) ‣ 3\.
    不确定性下的决策理论 ‣ 关于不确定性推理和量化的调查：信念理论与深度学习的结合") 节）。
- en: 'The Bayesian framework of ENNs was proposed in (Malinin and Gales, [2018](#bib.bib60))that
    considers a prior distribution on the network parameters ${\bm{\theta}}$, denoted
    by $P({\bm{\theta}})$. Let $P({\bm{\theta}}|\mathcal{D})$ be the posterier PDF,
    where $\mathcal{D}$ refers to the training set. Let $\text{Cat}(y|{\bf p})$ be
    the PDF of the categorical distribution about the predictive variable $y$, where
    the class probabilities ${\bf p}$ are the parameters. We can then show terms associated
    with different uncertainty as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ENNs的贝叶斯框架在（Malinin 和 Gales，[2018](#bib.bib60)）中提出，该框架考虑了网络参数 ${\bm{\theta}}$
    的先验分布，记为 $P({\bm{\theta}})$。令 $P({\bm{\theta}}|\mathcal{D})$ 为后验PDF，其中 $\mathcal{D}$
    指训练集。令 $\text{Cat}(y|{\bf p})$ 为关于预测变量 $y$ 的类别分布的PDF，其中类别概率 ${\bf p}$ 是参数。然后，我们可以如下展示与不同不确定性相关的项：
- en: '| (31) |  | $\displaystyle P(y&#124;{\bf x},\mathcal{D})=\int\int\underbrace{\text{Cat}(y&#124;{\bf
    p})}_{Data\;\;uncertainty}\;\;\underbrace{P({\bf p}&#124;{\bf x},{\bm{\theta}})}_{Subjective\;\;uncertainty}\;\;\underbrace{P({\bm{\theta}}&#124;\mathcal{D})}_{Model\;\;uncertainty}d{\bm{p}}d{\bm{\theta}},$
    |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $\displaystyle P(y|{\bf x},\mathcal{D})=\int\int\underbrace{\text{Cat}(y|{\bf
    p})}_{数据\;\;不确定性}\;\;\underbrace{P({\bf p}|{\bf x},{\bm{\theta}})}_{主观\;\;不确定性}\;\;\underbrace{P({\bm{\theta}}|\mathcal{D})}_{模型\;\;不确定性}d{\bm{p}}d{\bm{\theta}},$
    |  |'
- en: 'where $P({\bf p}|{\bf x},{\bm{\theta}})=\text{Dir}({\bf p}|{\bm{\alpha}})$
    and ${\bm{\alpha}}=f({\bf x},{\bm{\theta}})$. In this expression, data (aleatoric),
    subjective (distributional), and model (epistemic) uncertainty are modeled by
    a separate term within an interpretable probabilistic framework. The data uncertainty
    is described by the point-estimate categorical distribution, $\text{Cat}(y|{\bf
    p})$. The subjective (or distributional) uncertainty is described by the distribution
    over predictive class variables $P({\bf p}|{\bf x},{\bm{\theta}})$. The model
    uncertainty is described by the posterior distribution over the parameters given
    the data. The relationship between uncertainties is made explicit - model uncertainty
    affects estimates of subjective uncertainty, which in turn affects the estimates
    of data uncertainty. This forms a hierarchical model with three layers of uncertainty:
    the posterior over classes, the per-data Dirichlet prior distribution, and the
    global posterior distribution over model parameters. The uncertainty due to the
    mismatch between testing and training distributions can be measured by two methods.
    First, as the Dirichlet distribution $P({\bf p}|{\bf x},{\bm{\theta}})$ is equivalent
    to a subjective multinomial opinion based on the mapping defined in Eq. ([30](#S4.E30
    "In 4.1.1\. Key Formulation of ENNs ‣ 4.1\. Evidential Neural Networks (ENNs)
    ‣ 4\. Belief Theory Meets Deep Learning ‣ A Survey on Uncertainty Reasoning and
    Quantification for Decision Making: Belief Theory Meets Deep Learning")), we can
    quantify subjective uncertainty types directly based on the Dirichlet distribution,
    such as vacuity and dissonance, where vacuity captures elements of distributional
    uncertainty. Second, the distributional uncertainty can be measured based on mutual
    information between the categorical label $y$ and the class probabilities ${\bf
    p}$ as:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $P({\bf p}|{\bf x},{\bm{\theta}})=\text{Dir}({\bf p}|{\bm{\alpha}})$，且 ${\bm{\alpha}}=f({\bf
    x},{\bm{\theta}})$。在这个表达式中，数据（aleatoric）、主观（distributional）和模型（epistemic）不确定性通过一个可解释的概率框架中的单独项进行建模。数据不确定性由点估计的类别分布
    $\text{Cat}(y|{\bf p})$ 描述。主观（或分布）不确定性由预测类别变量 $P({\bf p}|{\bf x},{\bm{\theta}})$
    的分布描述。模型不确定性由给定数据的参数的后验分布描述。不确定性之间的关系被明确化——模型不确定性影响主观不确定性的估计，而主观不确定性反过来影响数据不确定性的估计。这形成了一个具有三层不确定性的层次模型：类别的后验分布、每数据的Dirichlet先验分布以及模型参数的全局后验分布。由于测试和训练分布不匹配引起的不确定性可以通过两种方法来测量。首先，由于Dirichlet分布
    $P({\bf p}|{\bf x},{\bm{\theta}})$ 等价于基于方程（[30](#S4.E30 "In 4.1.1\. Key Formulation
    of ENNs ‣ 4.1\. Evidential Neural Networks (ENNs) ‣ 4\. Belief Theory Meets Deep
    Learning ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making:
    Belief Theory Meets Deep Learning")）中定义的映射的主观多项式意见，我们可以直接基于Dirichlet分布量化主观不确定性类型，例如虚空性和不一致性，其中虚空性捕获了分布不确定性的元素。其次，分布不确定性可以基于类别标签
    $y$ 和类别概率 ${\bf p}$ 之间的互信息来测量，如下所示：'
- en: '| (32) |  | $\displaystyle\underbrace{\mathcal{I}[y,{\bf p}&#124;{\bf x},\mathcal{D})]}_{Epistemic\;\;uncertainty}=\underbrace{\mathcal{H}[\mathbb{E}_{P({\bf
    p}&#124;{\bf x};\mathcal{D})}[\text{Cat}(y&#124;{\bf p})]]}_{Entropy}-\underbrace{\mathbb{E}_{P({\bf
    p}&#124;{\bf x};\mathcal{D})}[\mathcal{H}[\text{Cat}(y&#124;{\bf p})]]}_{Aleatoric\;\;uncertainty}$
    |  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $\displaystyle\underbrace{\mathcal{I}[y,{\bf p}&#124;{\bf x},\mathcal{D})]}_{Epistemic\;\;uncertainty}=\underbrace{\mathcal{H}[\mathbb{E}_{P({\bf
    p}&#124;{\bf x};\mathcal{D})}[\text{Cat}(y&#124;{\bf p})]]}_{Entropy}-\underbrace{\mathbb{E}_{P({\bf
    p}&#124;{\bf x};\mathcal{D})}[\mathcal{H}[\text{Cat}(y&#124;{\bf p})]]}_{Aleatoric\;\;uncertainty}$
    |  |'
- en: We note that distributional uncertainty and vacuity are negatively correlated,
    if the parameters ${\bm{\theta}}$ are deterministic. The former is maximized (and
    the latter is minimized) when all categorical distributions are equiprobable,
    which occurs when the Dirichlet distribution is flat.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，分布不确定性和虚无在参数${\bm{\theta}}$是确定性的情况下是负相关的。当所有类别分布都是等概率的时，前者最大化（而后者最小化），这种情况发生在Dirichlet分布为平坦时。
- en: 4.1.2\. Causes and Types of Uncertainty
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 不确定性的原因与类型
- en: Since ENNs provide a hybrid framework of subjective belief models and neural
    networks, we can estimate evidential uncertainty, such as vacuity (scenario uncertainty)
    and dissonance (discord uncertainty), based on a subjective opinion. Recall that
    vacuity is due to a lack of evidence introducing uncertainty by incomplete knowledge.
    Dissonance is due to conflicting evidence, resulting in multiple knowledge frames.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ENNs提供了一种主观信念模型和神经网络的混合框架，我们可以基于主观意见估计证据的不确定性，例如虚无（情景不确定性）和不一致（冲突不确定性）。请记住，虚无是由于证据的缺乏引起的不确定性，这源于知识的不完整。不一致是由于证据冲突，导致多个知识框架。
- en: 4.1.3\. Uncertainty Quantification
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 不确定性量化
- en: 'ENNs estimate Dirichlet distribution parameters directly, which can be transferred
    to a subjective opinion. After then, we can estimate vacuity ($u$) and dissonance
    ($diss$) based on SL-based subjective opinion where there are $K$ classes and
    $e_{k}$ number of evidence to support each class $k$ by:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ENNs直接估计Dirichlet分布参数，这些参数可以转化为主观意见。之后，我们可以基于SL-based主观意见估计虚无（$u$）和不一致（$diss$），其中有$K$类，每类$k$有$e_{k}$个证据支持，计算方法为：
- en: '| (33) |  | $\displaystyle\small u=\frac{K}{\sum_{k=1}^{K}e_{k}+K},\;diss=\sum_{k=1}^{K}\Bigg{(}\frac{b_{k}\sum_{j\neq
    k}b_{j}\text{Bal}(b_{j},b_{k})}{\sum_{j\neq k}b_{j}}\Bigg{)},$ |  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $\displaystyle\small u=\frac{K}{\sum_{k=1}^{K}e_{k}+K},\;diss=\sum_{k=1}^{K}\Bigg{(}\frac{b_{k}\sum_{j\neq
    k}b_{j}\text{Bal}(b_{j},b_{k})}{\sum_{j\neq k}b_{j}}\Bigg{)},$ |  |'
- en: '| (34) |  | $\displaystyle\small{\text{Bal}(b_{j},b_{k})=\begin{cases}1-\frac{&#124;b_{j}-b_{k}&#124;}{b_{j}+b_{k}}&amp;\text{if
    $b_{i}b_{j}\neq 0$}\\ 0&amp;\text{if $\min(b_{i},b_{j})=0$}\end{cases}}$ |  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $\displaystyle\small{\text{Bal}(b_{j},b_{k})=\begin{cases}1-\frac{&#124;b_{j}-b_{k}&#124;}{b_{j}+b_{k}}&amp;\text{如果
    $b_{i}b_{j}\neq 0$}\\ 0&amp;\text{如果 $\min(b_{i},b_{j})=0$}\end{cases}}$ |  |'
- en: where $b_{k}$ and $b_{j}$ refer to the belief masses supporting $k$ class and
    $j$ class, respectively.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$b_{k}$和$b_{j}$分别指代支持$k$类和$j$类的信念质量。
- en: 4.1.4\. Applications of ENNs
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. ENNs的应用
- en: There is a whole class of evidential neural networks that have the interpretation
    that evidence represents the number of nearby training samples of various classes
    relative to the sample under test. This includes the generative version from (Sensoy
    et al., [2020](#bib.bib69)), posterior networks based on density-based pseudo-counts
    in (Charpentier et al., [2020](#bib.bib9)), and Epistemic Neural networks (Osband
    et al., [2021](#bib.bib65)) that allow a general interface to distinguish epistemic
    from aleatoric uncertainty. The ENNs have been applied on several applications
    in different domains, such as justified true belief models (Virani et al., [2020](#bib.bib92);
    Bhushan et al., [2020](#bib.bib6)), active learning on image data (Shi et al.,
    [2020](#bib.bib73)), misclassification and out-of-distribution detection on graph
    data (Zhao et al., [2020](#bib.bib118); Hu et al., [2020](#bib.bib37)), event
    early detection on time series data (Zhao et al., [2022](#bib.bib121)), and self-training
    on NLP task (Xu et al., [2021](#bib.bib100)).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个完整的证据神经网络类别，其解释是证据表示相对于待测样本的各种类别的邻近训练样本数量。这包括来自（Sensoy et al., [2020](#bib.bib69)）的生成版本、基于密度伪计数的后验网络（Charpentier
    et al., [2020](#bib.bib9)）以及允许一般接口区分知识不确定性和随机不确定性的知识神经网络（Osband et al., [2021](#bib.bib65)）。ENNs已应用于不同领域的若干应用中，如正当真信念模型（Virani
    et al., [2020](#bib.bib92); Bhushan et al., [2020](#bib.bib6)）、图像数据的主动学习（Shi et
    al., [2020](#bib.bib73)）、图数据中的误分类和分布外检测（Zhao et al., [2020](#bib.bib118); Hu et
    al., [2020](#bib.bib37)）、时间序列数据的事件早期检测（Zhao et al., [2022](#bib.bib121)）和自然语言处理任务的自我训练（Xu
    et al., [2021](#bib.bib100)）。
- en: 4.2\. Fuzzy Deep Neural Networks (FDNNs)
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 模糊深度神经网络（FDNNs）
- en: Fuzzy deep neural networks (FDNNs) is a hybrid framework of fuzzy logic systems
    and deep neural networks (Das et al., [2020](#bib.bib18)). FDNNs is designed to
    address the drawback that deep neural networks are sensitive to the uncertainties
    and the ambiguities of real-world data. Multiple approaches are developed to implement
    an FDNN. Some models, such as Fuzzy Restricted Boltzmann Machines (FRBMs) (Chen
    et al., [2015](#bib.bib10)), consider the concept of fuzzy numbers to represent
    network weights. Some models use fuzzy logic units to replace perceptrons in the
    network (Park et al., [2016](#bib.bib66)). Fuzzy systems are also used to train
    the network parameters of a deep neural network (El Hatri and Boumhidi, [2018](#bib.bib25)).
    In this section, we use Pythagorean Fuzzy Deep Boltzmann Machines (PFDBMs) (Zheng
    et al., [2016](#bib.bib123)), a recent extension of FRBMs, to demonstrate how
    fuzzy logic can be integrated as a part of deep neural networks, such as deep
    Boltzmann machines.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊深度神经网络（FDNNs）是模糊逻辑系统和深度神经网络（Das et al., [2020](#bib.bib18)）的混合框架。FDNNs旨在解决深度神经网络对现实世界数据的不确定性和模糊性敏感的缺陷。开发了多种方法来实现FDNN。一些模型，例如模糊限制玻尔兹曼机（FRBMs）（Chen
    et al., [2015](#bib.bib10)），考虑了用模糊数来表示网络权重的概念。一些模型使用模糊逻辑单元来替代网络中的感知机（Park et al.,
    [2016](#bib.bib66)）。模糊系统还用于训练深度神经网络的网络参数（El Hatri and Boumhidi, [2018](#bib.bib25)）。在这一部分，我们使用勾股模糊深度玻尔兹曼机（PFDBMs）（Zheng
    et al., [2016](#bib.bib123)），这是FRBMs的最新扩展，以展示模糊逻辑如何作为深度神经网络的一部分进行集成，例如深度玻尔兹曼机。
- en: 4.2.1\. Key Formulation of PFRBMs
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. PFRBMs的关键公式
- en: We will first introduce the building blocks, including deep Boltzmann machines
    and pythagorean fuzzy set, and then introduce the architecture design of PFRBMs.
    A deep Boltzmann machine (DBM) is an extension of the restricted Boltzmann machine (Zhang
    et al., [2018](#bib.bib112)) and considers multiple hidden layers to capture more
    complex correlations of the activities of the preceding layers (Salakhutdinov
    and Larochelle, [2010](#bib.bib68)). Considering a $L$ hidden layers DBM whose
    set of layers is $\{\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L}\}$ where $x$
    is a set of visible units $\textbf{x}\in\{0,1\}^{D}$, and $\textbf{h}_{l}$ is
    $l$-th hidden layerwith a set of hidden units $\textbf{h}_{l}\in\{0,1\}^{P_{l}}$.
    DBM is an energy-based probabilistic model which defines a joint probability distribution
    over $x$ as
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍基础构建块，包括深度玻尔兹曼机和勾股模糊集，然后介绍PFRBMs的架构设计。深度玻尔兹曼机（DBM）是限制玻尔兹曼机（Zhang et al.,
    [2018](#bib.bib112)）的扩展，考虑了多个隐藏层以捕捉前面层活动的更复杂的关联（Salakhutdinov and Larochelle,
    [2010](#bib.bib68)）。考虑一个$L$隐藏层DBM，其层集合为$\{\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L}\}$，其中$x$是可见单元的集合$\textbf{x}\in\{0,1\}^{D}$，而$\textbf{h}_{l}$是第$l$个隐藏层，其隐藏单元集合为$\textbf{h}_{l}\in\{0,1\}^{P_{l}}$。DBM是一种基于能量的概率模型，定义了$x$的联合概率分布为
- en: '| (35) |  | $P(\textbf{x};\theta)=\frac{1}{Z(\theta)}\sum_{\textbf{h}_{1}}\cdots\sum_{\textbf{h}_{L}}e^{-E\left(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L},\theta\right)}$
    |  |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $P(\textbf{x};\theta)=\frac{1}{Z(\theta)}\sum_{\textbf{h}_{1}}\cdots\sum_{\textbf{h}_{L}}e^{-E\left(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L},\theta\right)}$
    |  |'
- en: where $\theta=[W_{1},\ldots,W_{L}]$ is a vector of the parameters, and $E(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L};\theta)$
    is the energy function (Smolensky, [1986](#bib.bib80)) n defined as
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\theta=[W_{1},\ldots,W_{L}]$ 是参数向量，而$E(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L};\theta)$
    是能量函数（Smolensky，[1986](#bib.bib80)）定义为
- en: '| (36) |  | $E(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L};\theta)=-\textbf{x}^{T}W_{1}\textbf{h}_{1}-\sum_{l=2}^{L}\textbf{h}_{l-1}^{T}W_{l}\textbf{h}_{l},$
    |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| (36) |  | $E(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L};\theta)=-\textbf{x}^{T}W_{1}\textbf{h}_{1}-\sum_{l=2}^{L}\textbf{h}_{l-1}^{T}W_{l}\textbf{h}_{l},$
    |  |'
- en: and $Z(\theta)$ is the partition function defined as
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: $Z(\theta)$ 是分区函数定义为
- en: '| (37) |  | $Z(\theta)=\sum_{\textbf{x}}\sum_{\textbf{h}_{1}}\cdots\sum_{\textbf{h}_{L}}e^{-E\left(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L},\theta\right)}$
    |  |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| (37) |  | $Z(\theta)=\sum_{\textbf{x}}\sum_{\textbf{h}_{1}}\cdots\sum_{\textbf{h}_{L}}e^{-E\left(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L},\theta\right)}$
    |  |'
- en: DBM aims to maximize the joint probability $P(\textbf{x};\theta)$, which has
    the same effect to minimize energy function $E(\cdot)$.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: DBM旨在最大化联合概率$P(\textbf{x};\theta)$，这等同于最小化能量函数$E(\cdot)$。
- en: 'Pythagorean fuzzy sets (PFS) is an extension of the basic fuzzy sets in two
    perspectives. First, it introduces a non-membership degree besides the standard
    membership degree []. Second, it considers the restriction that the sum of the
    squares of the membership degree is between 0 and 1\. PFS is defined by the mathematical
    object:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 皮塔哥拉斯模糊集（PFS）在两个方面扩展了基本模糊集。首先，它引入了除标准隶属度之外的非隶属度。其次，它考虑了隶属度平方和在0和1之间的限制。PFS由数学对象定义：
- en: '| (38) |  | $\mathcal{P}=\left\{\langle x,\mu_{p}(x),v_{p}(x)\rangle\mid x\in
    S\right\},$ |  |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| (38) |  | $\mathcal{P}=\left\{\langle x,\mu_{p}(x),v_{p}(x)\rangle\mid x\in
    S\right\},$ |  |'
- en: 'where $\mu_{p}(x):S\rightarrow[0,1]$ is the membership degree (how much degree
    of $x\in S$) of element $x$ to $S$ in $P$, and $\nu_{p}(x):S\rightarrow[0,1]$
    is the non-membership degree (how much degree of $x\notin S$) as well. In addition,
    we have $\mu^{2}_{p}(x)+\nu^{2}_{p}(x)\leq 1$, and the hesitant degree, neither
    membership nor non-membership degree, may consider as uncertainty degree. The
    hesitation degree (uncertainty degree) is the function that expresses lack of
    knowledge of whether $x\in S$ or $x\notin S$. It can be calculated by:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mu_{p}(x):S\rightarrow[0,1]$ 是元素$x$对$S$的隶属度（$x\in S$的程度），而$\nu_{p}(x):S\rightarrow[0,1]$
    是非隶属度（$x\notin S$的程度）。此外，我们有$\mu^{2}_{p}(x)+\nu^{2}_{p}(x)\leq 1$，并且犹豫度，既不是隶属度也不是非隶属度，可能被视为不确定度。犹豫度（不确定度）是表达对$x\in
    S$还是$x\notin S$的知识缺乏的函数。它可以通过以下方式计算：
- en: '| (39) |  | $\pi_{p}(x)=\sqrt{1-\mu_{p}^{2}(x)-v_{p}^{2}(x)}.$ |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| (39) |  | $\pi_{p}(x)=\sqrt{1-\mu_{p}^{2}(x)-v_{p}^{2}(x)}.$ |  |'
- en: 'Moreover, to simplify it, $\mathcal{P}(\mu_{p}(x),\nu_{p}(x))$ is called a
    Pythagorean fuzzy number (PFN) denoted $\beta=\mathcal{P}(\mu_{\beta},\nu_{\beta})$,
    where $\mu_{\beta},\nu_{\beta}\in[0,1]$ and $\mu_{\beta}^{2}+\nu_{\beta}^{2}\leq
    1$. We can use two metrics to rank a PFN by:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了简化，$\mathcal{P}(\mu_{p}(x),\nu_{p}(x))$ 被称为皮塔哥拉斯模糊数（PFN），记作$\beta=\mathcal{P}(\mu_{\beta},\nu_{\beta})$，其中$\mu_{\beta},\nu_{\beta}\in[0,1]$且$\mu_{\beta}^{2}+\nu_{\beta}^{2}\leq
    1$。我们可以使用两个度量来对PFN进行排序：
- en: '| (40) |  |  | $\displaystyle h(\beta)=\mu_{\beta}^{2}+v_{\beta}^{2},$ | $\displaystyle
    s(\beta)=\mu_{\beta}^{2}-v_{\beta}^{2},$ |  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| (40) |  |  | $\displaystyle h(\beta)=\mu_{\beta}^{2}+v_{\beta}^{2},$ | $\displaystyle
    s(\beta)=\mu_{\beta}^{2}-v_{\beta}^{2},$ |  |'
- en: 'where $h(\beta)$ is the accuracy function of $\beta$ and $s(\beta)$ is the
    score function of $\beta$. The ranking of two PFNs, $\beta_{1}=P(\mu_{\beta_{1}},\nu_{\beta_{1}})$
    and $\beta_{2}=P(\mu_{\beta_{2}},\nu_{\beta_{2}})$, is performed by: (1) If $s\left(\beta_{1}\right)<s\left(\beta_{2}\right)$,
    then $\beta_{1}<\beta_{2}$; and (2) If $s\left(\beta_{1}\right)=s\left(\beta_{2}\right)$,
    then (a) if $h\left(\beta_{1}\right)<h\left(\beta_{2}\right)$, then $\beta_{1}<\beta_{2}$;
    and (b) if $h\left(\beta_{1}\right)=h\left(\beta_{2}\right)$, then $\beta_{1}=\beta_{2}$.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$h(\beta)$是$\beta$的准确度函数，$s(\beta)$是$\beta$的评分函数。两个PFN，$\beta_{1}=P(\mu_{\beta_{1}},\nu_{\beta_{1}})$
    和 $\beta_{2}=P(\mu_{\beta_{2}},\nu_{\beta_{2}})$ 的排序方法是：（1）如果$s\left(\beta_{1}\right)<s\left(\beta_{2}\right)$，则$\beta_{1}<\beta_{2}$；（2）如果$s\left(\beta_{1}\right)=s\left(\beta_{2}\right)$，则（a）如果$h\left(\beta_{1}\right)<h\left(\beta_{2}\right)$，则$\beta_{1}<\beta_{2}$；（b）如果$h\left(\beta_{1}\right)=h\left(\beta_{2}\right)$，则$\beta_{1}=\beta_{2}$。
- en: 'The Pythagorean Fuzzy Restricted Boltzmann Machine (PFRBM) extends the DBM
    model by replacing the standard real-valued parameters with PFNs. PFRBM is able
    to handle fuzzy and/or incomplete data and the fuzzy parameters provide a better
    representation of the data using fuzzy probability. Fig. [12](#S4.F12 "Figure
    12 ‣ 4.2.1\. Key Formulation of PFRBMs ‣ 4.2\. Fuzzy Deep Neural Networks (FDNNs)
    ‣ 4\. Belief Theory Meets Deep Learning ‣ A Survey on Uncertainty Reasoning and
    Quantification for Decision Making: Belief Theory Meets Deep Learning") describes
    the framework of a PFRBM with $L$ layers, denoted as $h_{1},\ldots,h_{L}$. Let
    $\widetilde{\theta}=[\widetilde{W_{1}},\ldots,\widetilde{W_{L}}]$ be the fuzzy
    parameters and $\textbf{x}=(x_{1},\cdots,x_{D})$ be the input feature vector.
    The energy function and probability function of a PFRBM are shown as follows:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '勾股模糊限制玻尔兹曼机（PFRBM）通过用PFN代替标准实值参数来扩展DBM模型。PFRBM能够处理模糊和/或不完整的数据，模糊参数通过模糊概率提供了更好的数据表示。图[12](#S4.F12
    "Figure 12 ‣ 4.2.1\. Key Formulation of PFRBMs ‣ 4.2\. Fuzzy Deep Neural Networks
    (FDNNs) ‣ 4\. Belief Theory Meets Deep Learning ‣ A Survey on Uncertainty Reasoning
    and Quantification for Decision Making: Belief Theory Meets Deep Learning") 描述了具有
    $L$ 层的PFRBM的框架，记作 $h_{1},\ldots,h_{L}$。设 $\widetilde{\theta}=[\widetilde{W_{1}},\ldots,\widetilde{W_{L}}]$
    为模糊参数，$\textbf{x}=(x_{1},\cdots,x_{D})$ 为输入特征向量。PFRBM的能量函数和概率函数如下所示：'
- en: '| (41) |  | $\displaystyle\widetilde{E}\left(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L};\widetilde{\theta}\right)=-\textbf{x}^{T}\widetilde{W_{1}}\textbf{h}_{1}-\sum_{l=2}^{L}\textbf{h}_{l-1}^{T}\widetilde{W_{l}}\textbf{h}_{l},$
    |  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| (41) |  | $\displaystyle\widetilde{E}\left(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L};\widetilde{\theta}\right)=-\textbf{x}^{T}\widetilde{W_{1}}\textbf{h}_{1}-\sum_{l=2}^{L}\textbf{h}_{l-1}^{T}\widetilde{W_{l}}\textbf{h}_{l},$
    |  |'
- en: '| (42) |  | $\displaystyle\widetilde{P}(\textbf{x};\bar{\theta})=\frac{1}{\widetilde{Z}(\widetilde{\theta})}\sum_{\textbf{h}_{1}}\cdots\sum_{\textbf{h}_{L}}e^{-\widetilde{E}\left(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L},\widetilde{\theta}\right)}.$
    |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| (42) |  | $\displaystyle\widetilde{P}(\textbf{x};\bar{\theta})=\frac{1}{\widetilde{Z}(\widetilde{\theta})}\sum_{\textbf{h}_{1}}\cdots\sum_{\textbf{h}_{L}}e^{-\widetilde{E}\left(\textbf{x},\textbf{h}_{1},\ldots,\textbf{h}_{L},\widetilde{\theta}\right)}.$
    |  |'
- en: Therefore, we consider the log likelihood as the objective function,
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将对数似然作为目标函数，
- en: '| (43) |  | $\max_{\tilde{\theta}}\widetilde{\mathcal{L}}(\widetilde{\theta},D)=\sum_{\textbf{x}\in
    D}\log(\widetilde{P}(\textbf{x},\widetilde{\theta})).$ |  |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| (43) |  | $\max_{\tilde{\theta}}\widetilde{\mathcal{L}}(\widetilde{\theta},D)=\sum_{\textbf{x}\in
    D}\log(\widetilde{P}(\textbf{x},\widetilde{\theta})).$ |  |'
- en: As fuzzy optimization problems are intractable, the PFDBM is trained using a
    combination of gradient descent and metaheuristic techniques.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模糊优化问题难以处理，PFDBM使用梯度下降和元启发式技术的组合进行训练。
- en: '![Refer to caption](img/32867e4fc0befc02ccc2c1a247dc967a.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/32867e4fc0befc02ccc2c1a247dc967a.png)'
- en: Figure 12. Pythagorean Fuzzy Deep Belief Network (PFDBN).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图12. 勾股模糊深度置信网络（PFDBN）。
- en: 4.2.2\. Causes and Types of Uncertainty
  id: totrans-399
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2. 不确定性的原因和类型
- en: PFDBMs provides a hybrid framework of fuzzy sets and DNNs where uncertainty
    comes from a fuzzy set due to fuzzy and/or incomplete data, leading to unpredictability.
    The fuzziness has its root nature in aleatoric uncertainty.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: PFDBMs提供了模糊集和DNN的混合框架，其中的不确定性来自模糊集，由于模糊和/或不完整数据，导致不可预测性。模糊性根源于aleatoric不确定性。
- en: 4.2.3\. Uncertainty Quantification
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3. 不确定性量化
- en: Unlike traditional DNNs, PFDBMs with fuzzy parameters provide a better representation
    of data using a fuzzy probability to represent uncertainty. The fuzzy parameters
    can learn new features and allow investigating how much a certain (or uncertain)
    feature influences the output.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的深度神经网络（DNNs）不同，具有模糊参数的PFDBMs通过使用模糊概率来表示不确定性，从而提供了更好的数据表示。模糊参数可以学习新特征，并允许研究某一（或不确定的）特征对输出的影响程度。
- en: 4.2.4\. Applications of FDNNs
  id: totrans-403
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4. FDNNs的应用
- en: PFDBMs was proposed to develop an airline passenger profiling (Zheng et al.,
    [2016](#bib.bib123)) and provide an early warning system for industrial accidents (Zheng
    et al., [2017](#bib.bib122)). Besides PFDBMs, Park et al. ([2016](#bib.bib66))
    developed intra- and inter-fraction FDNNs to tracking lung-cancer tumor motion.
    Similar to (El Hatri and Boumhidi, [2018](#bib.bib25)), fuzzy logic was employed
    to train the learning parameters in FDNNs for traffic incident detection. In addition,
    some models consider fuzzy logic and deep learning in a sequential or parallel
    fashion.  Wang et al. ([2016](#bib.bib97)) proposed a model that uses deep neural
    network with fuzzy feature points for damaged fingerprint classification.  Zhang
    et al. ([2014](#bib.bib114)) proposed a model utilizing fuzzy granulation and
    deep belief network for predicting time-series data.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: PFDBMs被提出用于开发航空乘客画像（Zheng et al., [2016](#bib.bib123)）并提供工业事故的早期预警系统（Zheng et
    al., [2017](#bib.bib122)）。除了PFDBMs，Park et al. ([2016](#bib.bib66)) 开发了用于追踪肺癌肿瘤运动的内分数和跨分数FDNNs。类似于（El
    Hatri and Boumhidi, [2018](#bib.bib25)），模糊逻辑被用于训练FDNNs中的学习参数以进行交通事故检测。此外，一些模型考虑了模糊逻辑和深度学习的顺序或并行应用。Wang
    et al. ([2016](#bib.bib97)) 提出了一个使用带模糊特征点的深度神经网络用于损坏指纹分类的模型。Zhang et al. ([2014](#bib.bib114))
    提出了一个利用模糊粒化和深度信念网络预测时间序列数据的模型。
- en: 4.3\. Rough Deep Neural Networks (RDNNs)
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 粗糙深度神经网络（RDNNs）
- en: 'Rough neural networks (RNNs) have been studied for a decade by combining a
    rough set or rough neuron with DNNs to process the uncertainties and high-dimensional
    data (Lingras, [1996](#bib.bib55)). The methods fall into two main categories:
    rough neural-based and rough set-based. Due to the space constraint, we discuss
    the rough neural-based method while providing the details of the rough set-based
    approaches in Appendix E.2 of the supplement document.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 粗糙神经网络（RNNs）通过将粗糙集或粗糙神经元与DNNs结合来处理不确定性和高维数据，已经研究了十年（Lingras，[1996](#bib.bib55)）。这些方法分为两大类：粗糙神经网络基础的方法和粗糙集基础的方法。由于篇幅限制，我们讨论粗糙神经网络基础的方法，而粗糙集基础的方法的详细信息见补充文档的附录E.2。
- en: '![Refer to caption](img/d9eac0d891f81de65eaab329eec2451b.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d9eac0d891f81de65eaab329eec2451b.png)'
- en: Figure 13. Rough neuron with six tunable parameters
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图13. 具有六个可调参数的粗糙神经元
- en: 4.3.1\. Key Formulation of RDNNs
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. RDNNs的关键表述
- en: Rough neural-based method considers a rough neuron in DNNs to improve the robustness
    of learning. For a traditional neural network, if the input feature is represented
    by a range, such as the temperature of climate (e.g., daily maximum and minimum
    temperature), the neural network cannot learn a good representation and the prediction
    error will be relatively large. The neural network based on a rough neuron can
    address this issue.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 粗糙神经网络基础的方法考虑了在DNNs中使用粗糙神经元以提高学习的鲁棒性。对于传统神经网络，如果输入特征由一个范围表示，例如气候的温度（例如，日最高和最低温度），神经网络不能很好地学习表示，预测误差会相对较大。基于粗糙神经元的神经网络可以解决这个问题。
- en: 'Fig. [13](#S4.F13 "Figure 13 ‣ 4.3\. Rough Deep Neural Networks (RDNNs) ‣ 4\.
    Belief Theory Meets Deep Learning ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning") shows how the rough neuron
    is applied for rough pattern recognition. This neuron consists of an upper bound
    neuron with parameters $\theta_{U}=\{W_{U},b_{U},\alpha\}$, and a lower bound
    neuron with parameters $\theta_{L}=\{W_{L},b_{L},\beta\}$. Here $W_{U}$ and $b_{U}$
    are the weight and bias of the upper bound, respectively, while $W_{L}$ and $b_{L}$
    are those for the lower bound neuron, respectively. Output coefficients, $0\leq\alpha$
    and $\beta\leq 1$, determine the contribution of upper bound output $O_{U}$ and
    lower bound output $O_{L}$ to the overall neuron’s output $O$. A rough extension
    of auto-encode, called rough auto-encoder (RAE), uses rough neurons in its hidden
    layer and output layer. Here $W^{k}_{U}$, $b^{k}_{U}$, and $\alpha^{k}$ are the
    upper bound parameters of layer $k$ and $W^{k}_{L},b^{k}_{L}$, and $\beta^{k}$
    are the lower bound parameters of layer $k$, respectively.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [13](#S4.F13 "图 13 ‣ 4.3\. 粗糙深度神经网络（RDNNs） ‣ 4\. 信念理论与深度学习的融合 ‣ 决策制定的不确定性推理与量化：信念理论与深度学习的融合")
    显示了粗糙神经元在粗糙模式识别中的应用。该神经元包括一个具有参数 $\theta_{U}=\{W_{U},b_{U},\alpha\}$ 的上界神经元和一个具有参数
    $\theta_{L}=\{W_{L},b_{L},\beta\}$ 的下界神经元。这里 $W_{U}$ 和 $b_{U}$ 分别是上界的权重和偏置，而 $W_{L}$
    和 $b_{L}$ 分别是下界神经元的权重和偏置。输出系数 $0\leq\alpha$ 和 $\beta\leq 1$ 决定了上界输出 $O_{U}$ 和下界输出
    $O_{L}$ 对整体神经元输出 $O$ 的贡献。一个粗糙的自编码扩展，称为粗糙自编码器（RAE），在其隐藏层和输出层中使用粗糙神经元。这里 $W^{k}_{U}$、$b^{k}_{U}$
    和 $\alpha^{k}$ 是第 $k$ 层的上界参数，而 $W^{k}_{L}$、$b^{k}_{L}$ 和 $\beta^{k}$ 是第 $k$ 层的下界参数。
- en: 'Given the RAE defined with input vector $h_{0}=X$, the upper bound and lower
    bound outputs of the first hidden layer are shown where $W_{U}^{1}$ and $W_{L}^{1}$
    are the learned parameters and $f^{1}W_{L}^{1}X+b_{L}^{1}$ can be larger than
    $f^{1}W_{U}^{1}X+b_{U}^{1}$. The $h_{U}^{1}(X)$ and $h_{L}^{1}(X)$ are defined
    by:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 给定定义了输入向量 $h_{0}=X$ 的 RAE，第一个隐藏层的上界和下界输出如所示，其中 $W_{U}^{1}$ 和 $W_{L}^{1}$ 是学习到的参数，且
    $f^{1}W_{L}^{1}X+b_{L}^{1}$ 可能大于 $f^{1}W_{U}^{1}X+b_{U}^{1}$。$h_{U}^{1}(X)$ 和
    $h_{L}^{1}(X)$ 定义为：
- en: '| (44) |  |  | $\displaystyle h_{U}^{1}(X)=\max\Big{[}f^{1}\left(W_{U}^{1}X+b_{U}^{1}\right),f^{1}\left(W_{L}^{1}X+b_{L}^{1}\right)\Big{]},$
    |  |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| (44) |  |  | $\displaystyle h_{U}^{1}(X)=\max\Big{[}f^{1}\left(W_{U}^{1}X+b_{U}^{1}\right),f^{1}\left(W_{L}^{1}X+b_{L}^{1}\right)\Big{]},$
    |  |'
- en: '|  |  | $\displaystyle h_{L}^{1}(X)=\min\Big{[}f^{1}\left(W_{U}^{1}X+b_{U}^{1}\right),f^{1}\left(W_{L}^{1}X+b_{L}^{1}\right)\Big{]},$
    |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle h_{L}^{1}(X)=\min\Big{[}f^{1}\left(W_{U}^{1}X+b_{U}^{1}\right),f^{1}\left(W_{L}^{1}X+b_{L}^{1}\right)\Big{]},$
    |  |'
- en: 'where $f^{1}$ is a sigmoid function. The latent representation in the hidden
    layer is computed by:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f^{1}$ 是一个 sigmoid 函数。隐藏层中的潜在表示通过以下方式计算：
- en: '| (45) |  | $h^{1}=\alpha^{1}h^{1}_{U}+\beta^{1}h^{1}_{L}.$ |  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| (45) |  | $h^{1}=\alpha^{1}h^{1}_{U}+\beta^{1}h^{1}_{L}.$ |  |'
- en: 'For the rough decoding process in the output layer, the upper bound and lower
    bound outputs are computed as:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出层中的粗糙解码过程，上界和下界输出计算为：
- en: '| (46) |  | $\displaystyle h_{U}^{2}$ | $\displaystyle=\max\Big{[}f^{2}\left(W_{U}^{2}h^{1}+b_{U}^{2}\right),f^{2}\left(W_{L}^{2}h^{1}+b_{L}^{2}\right)\Big{]},$
    |  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| (46) |  | $\displaystyle h_{U}^{2}$ | $\displaystyle=\max\Big{[}f^{2}\left(W_{U}^{2}h^{1}+b_{U}^{2}\right),f^{2}\left(W_{L}^{2}h^{1}+b_{L}^{2}\right)\Big{]},$
    |  |'
- en: '|  | $\displaystyle h_{L}^{2}$ | $\displaystyle=\min\Big{[}f^{2}\left(W_{U}^{2}h^{1}+b_{U}^{2}\right),f^{2}\left(W_{L}^{2}h^{1}+b_{L}^{2}\right)\Big{]},$
    |  |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{L}^{2}$ | $\displaystyle=\min\Big{[}f^{2}\left(W_{U}^{2}h^{1}+b_{U}^{2}\right),f^{2}\left(W_{L}^{2}h^{1}+b_{L}^{2}\right)\Big{]},$
    |  |'
- en: where $f^{2}$ is considered to be a linear function. Therefore, we have the
    reconstructed input,
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f^{2}$ 被认为是一个线性函数。因此，我们得到了重构输入，
- en: '| (47) |  | $r=\alpha^{2}h^{2}_{U}+\beta^{2}h^{2}_{L}.$ |  |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| (47) |  | $r=\alpha^{2}h^{2}_{U}+\beta^{2}h^{2}_{L}.$ |  |'
- en: 4.3.2\. Causes and Types of Uncertainty
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 不确定性的原因和类型
- en: In RDNNs, uncertainty is considered in a rough set introducing unpredictability
    and rough neuron introducing incomplete knowledge. Hence, the rough set and neuron
    can capture vagueness from model input and parameter uncertainty from model parameters.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在粗糙深度神经网络（RDNNs）中，不确定性被考虑为粗糙集合引入的不可预测性和粗糙神经元引入的不完整知识。因此，粗糙集合和神经元可以捕捉模型输入中的模糊性以及模型参数中的不确定性。
- en: 4.3.3\. Uncertainty Quantification
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 不确定性量化
- en: 'The uncertainty in RDNNs can be estimated based on the rough set theorem introduced
    in Section [3.1](#S3.SS1 "3.1\. Kleene’s Three-Valued Logic (TVL) ‣ 3\. Decision
    Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning
    and Quantification for Decision Making: Belief Theory Meets Deep Learning"). The
    rough set theorem approximates an $M$-boundary region, which contains a set of
    objects that cannot be clearly classified by only employing the set of attributes
    and represents vagueness.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 'RDNN中的不确定性可以基于第[3.1](#S3.SS1 "3.1\. Kleene’s Three-Valued Logic (TVL) ‣ 3\.
    Decision Making under Uncertainty in Belief Theory ‣ A Survey on Uncertainty Reasoning
    and Quantification for Decision Making: Belief Theory Meets Deep Learning")节中介绍的粗糙集定理进行估计。粗糙集定理近似于一个$M$-边界区域，该区域包含一个不能仅通过属性集清晰分类的对象集合，并表示模糊性。'
- en: 4.3.4\. Applications of RDNNs
  id: totrans-426
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4. RDNNs的应用
- en: Most RDNNs are proposed to reduce uncertainty. Zhang and Wang ([2006](#bib.bib111))
    applied the fuzzy-rough neural network in vowels recognition. Khodayar et al.
    ([2017](#bib.bib47)) proposed a rough extension of stacked denoising autoencoder
    (SDAE) for ultrashort-term and short-term wind speed forecasting, incorporating
    a rough neural network into wind uncertainties. Sinusoidal Rough-Neural Network
    (SR-NN) (Jahangir et al., [2020](#bib.bib39)) is proposed to predict wind speed
    by using rough neurons to handle the high intermittent behavior of wind speed.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数RDNN旨在减少不确定性。张和王（[2006](#bib.bib111)）在元音识别中应用了模糊-粗糙神经网络。Khodayar等人（[2017](#bib.bib47)）提出了一种粗糙扩展的堆叠去噪自编码器（SDAE），用于超短期和短期风速预测，将粗糙神经网络融入风速的不确定性中。Sinusoidal
    Rough-Neural Network（SR-NN）（Jahangir等人，[2020](#bib.bib39)）被提议用于通过粗糙神经元处理风速的高间歇性行为来预测风速。
- en: 5\. Summary of the Key Findings
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 关键发现总结
- en: 'We summarize the key findings from our survey by answering the key research
    questions below: RQ1. What are the key causes and types of uncertainty studied
    in belief theory and deep learning?'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过回答以下关键研究问题来总结调查的关键发现：RQ1. 在信念理论和深度学习中，主要的不可确定性原因和类型是什么？
- en: 'Answer: The majority of belief models, such as DST, TBM, IDM, SL, TVL, and
    Bayesian inference, consider uncertainty caused by a lack of evidence, which is
    called vacuity in SL. It is related to aleatoric uncertainty where a long-term
    probability can increase as more evidence is received. The second most common
    uncertainty type is considered in belief models, such as DSmT, SL, or Fuzzy Logic,
    is discord (or dissonance), caused by disagreement or conflicting evidence from
    multiple sources or observers, which generates multiple knowledge frames and results
    in inconclusiveness in decision making. Lastly, unpredictability is introduced
    by unclearness or impreciseness of observations or beliefs, which are considered
    as fuzziness in TVL, vagueness in SL, and imprecise beliefs in DSmT. In DL, two
    types of uncertainty natures are mainly considered: epistemic uncertainty and
    aleatoric uncertainty. Epistemic uncertainty, also called ‘model or systematic
    uncertainty,’ represents the model (parameters) uncertainty due to the limited
    training data. Aleatoric uncertainty indicates data uncertainty introduced by
    the nature of randomness in data.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 回答：大多数信念模型，如DST、TBM、IDM、SL、TVL和贝叶斯推断，考虑了由于证据不足而引起的不确定性，这在SL中称为空缺。它与样本不确定性相关，其中长期概率随着更多证据的获得而增加。信念模型（如DSmT、SL或模糊逻辑）中考虑的第二种最常见的不确定性类型是冲突（或不一致），由来自多个来源或观察者的意见分歧或冲突证据引起，产生多个知识框架，并导致决策的不确定性。最后，不可预测性由观察或信念的模糊或不精确引入，在TVL中被称为模糊性，在SL中称为模糊性，在DSmT中称为不精确信念。在DL中，主要考虑了两种不确定性类型：知识性不确定性和样本不确定性。知识性不确定性，也称为“模型或系统不确定性”，表示由于训练数据有限而产生的模型（参数）不确定性。样本不确定性表示由于数据的随机性而引入的数据不确定性。
- en: RQ2. How can the ontology of uncertainty be defined based on the multidimensional
    aspects of uncertainty studied in belief models and deep learning?
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2. 如何基于信念模型和深度学习中研究的多维不确定性方面定义不确定性的本体？
- en: 'Answer: We demonstrated the ontology of uncertainty in Fig. [2](#S2.F2 "Figure
    2 ‣ 2.3\. Ontology of Uncertainty ‣ 2\. Classification Types, Causes, and Ontology
    of Uncertainty ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning"). The source of uncertainty can be
    from machines, networks, environmental factors, and humans that can generate a
    lot of various types of uncertain data. Uncertainty has a model of reasoning and
    quantifying various types of uncertainties to make effective decision making.
    We limited the models in belief theory and DL. Uncertainty has procedures to collect
    evidence, including both subjective and objective data or information. Uncertainty
    has its multiple types including ambiguity and fuzziness which also have been
    studied under different taxonomies (see Fig. [1](#S2.F1 "Figure 1 ‣ 2.1\. Classification
    of Uncertainty Types ‣ 2\. Classification Types, Causes, and Ontology of Uncertainty
    ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning")), such as vagueness, imprecision, unclearness, and
    so forth. Uncertainty has its root nature in the most popularly used two types
    of uncertainty: aleatoric and epistemic uncertainty.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 答：我们在图 [2](#S2.F2 "图 2 ‣ 2.3. 不确定性本体 ‣ 2. 分类类型、原因和不确定性本体 ‣ 不确定性推理与量化的调查：信念理论与深度学习的结合")
    中展示了不确定性的本体。不确定性的来源可以来自机器、网络、环境因素和人类，这些都可以生成各种类型的不确定数据。不确定性有一个推理和量化各种类型不确定性的模型，以进行有效决策。我们将模型限制在信念理论和深度学习中。不确定性有收集证据的程序，包括主观和客观数据或信息。不确定性有多种类型，包括模糊性和不清晰性，这些也在不同分类下进行了研究（参见图
    [1](#S2.F1 "图 1 ‣ 2.1. 不确定性类型的分类 ‣ 2. 分类类型、原因和不确定性本体 ‣ 不确定性推理与量化的调查：信念理论与深度学习的结合")），例如模糊性、不精确性、不明确性等。不确定性的根本性质在最常用的两种不确定性中：随机不确定性和知识性不确定性。
- en: RQ3. How has each belief model considered and measured uncertainty?
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3. 每种信念模型如何考虑和衡量不确定性？
- en: 'Answer: DST’s combination rule helps the decision by combining beliefs from
    multiple information channels. IDM provides a belief range, rather than a single
    value, allowing a decision maker to be aware of the magnitude of uncertainty.
    In DSmT, Shannon’s entropy and the Probabilistic Information Content (PIC) score
    are used to indicate uncertainty caused by a lack of evidence where a decision
    is made based on GPT and DSmP. Bayesian inference theory uses a variance or co-variance
    to measure uncertainty representing unpredictability. In SL, one can use a projected
    belief that interprets uncertainty (i.e., vacuity) based on its prior belief (i.e.,
    base rate). If there is very low vacuity but high dissonance, one can maximize
    vacuity by offsetting the amount of the smallest belief mass while increasing
    vacuity to have a high effect on new evidence. TVL uses an unknown status to model
    system uncertainty and defines a set of logical operations to decide the system
    status for decision making in system operations. Fuzzy Logic uses fuzzy entropy
    to quantify the unpredictability and multiple knowledge frames of fuzzy events.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 答：DST 的组合规则通过结合来自多个信息渠道的信念来帮助决策。IDM 提供一个信念范围，而不是一个单一值，这使决策者能够了解不确定性的程度。在 DSmT
    中，香农熵和概率信息内容（PIC）分数用于指示由于证据不足而造成的不确定性，其中决策是基于 GPT 和 DSmP 的。贝叶斯推理理论使用方差或协方差来衡量表示不可预测性的
    ​​不确定性。在 SL 中，可以使用投影信念来解释基于其先验信念（即基准率）的不确定性（即空洞性）。如果空洞性非常低但不一致性很高，则可以通过抵消最小信念质量的数量来最大化空洞性，同时增加空洞性，以对新证据产生很大影响。TVL
    使用未知状态来建模系统不确定性，并定义一组逻辑操作来决定系统状态，以便在系统操作中进行决策。模糊逻辑使用模糊熵来量化不可预测性和模糊事件的多个知识框架。
- en: RQ4. How has each belief model been applied in deep learning and vice-versa
    for effective decision making under uncertainty?
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: RQ4. 每种信念模型如何在深度学习中应用，反之亦然，以在不确定性下进行有效决策？
- en: 'Answer: TVL is used to solve classification problems in pattern recognition
    tasks (Kashkevich and Krasnoproshin, [1979](#bib.bib46)) and leveraged to establish
    a database for natural language consultation (Dahl, [1979](#bib.bib17)) in 1970s.
    We rarely found any recent work using TVL in ML/DL applications. DST is mainly
    used to fuse data from multi-sensors before conducting neural network training,
    or fuse predictions from two identically trained models (Soua et al., [2016](#bib.bib82);
    Tong et al., [2021](#bib.bib86); Tian et al., [2020](#bib.bib84)). To our knowledge,
    TBM is also used to solve classification problems but not used with ML/DL. DSmT
    is used along with ML/DL to solve classification problems where it is integrated
    with SVM, CNN, LSTM, and RF (Abbas et al., [2015](#bib.bib2); Ji et al., [[n.d.]](#bib.bib40)).
    IDM is used to improve ML algorithms, such as AdaBoost (Utkin, [2015](#bib.bib90)),
    Decision Tree (Serafín et al., [2020](#bib.bib71)), or näive classifier (Corani
    and de Campos, [2010](#bib.bib15)). Fuzzy Logic is combined with DNNs, named fuzzy
    DNNs, to deal with ambiguity in data (Das et al., [2020](#bib.bib18); Holyoak,
    [1987](#bib.bib35); El Hatri and Boumhidi, [2018](#bib.bib25); Wang et al., [2016](#bib.bib97);
    Zhang et al., [2014](#bib.bib114)). Bayesian inference is mainly used to infer
    the posterior distribution of Bayesian features (Sofman et al., [2006](#bib.bib81);
    Tripathi and Govindaraju, [2007](#bib.bib87); G. Tian and Feng, [2011](#bib.bib27)).
    SL’s vacuity and dissonance uncertainty dimensions are considered in evidential
    neural networks for uncertainty-aware decision making in classification problems (Sensoy
    et al., [2018](#bib.bib70); Zhao et al., [2019b](#bib.bib120)). Vacuity is used
    to detect out-of-distribution (OOD) samples while dissonance is used to detect
    missclassification samples. Rough set theory is combined with DNNs, named RDNNs,
    to deal with imprecise information and uncertainty in data (e.g., ranges as values
    for input and/or output variables) (Zhang, [2007](#bib.bib110); Yasdi, [1995](#bib.bib103);
    Lingras, [1996](#bib.bib55); Khodayar et al., [2017](#bib.bib47)).'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 答：TVL 被用于解决模式识别任务中的分类问题 (Kashkevich 和 Krasnoproshin, [1979](#bib.bib46))，并且在1970年代被用于建立自然语言咨询的数据库 (Dahl,
    [1979](#bib.bib17))。我们很少发现近期有工作在机器学习/深度学习应用中使用 TVL。DST 主要用于在进行神经网络训练之前融合来自多个传感器的数据，或融合两个同样训练的模型的预测 (Soua
    et al., [2016](#bib.bib82); Tong et al., [2021](#bib.bib86); Tian et al., [2020](#bib.bib84))。据我们了解，TBM
    也用于解决分类问题，但未用于机器学习/深度学习。DSmT 与机器学习/深度学习结合使用，以解决分类问题，集成了 SVM、CNN、LSTM 和 RF (Abbas
    et al., [2015](#bib.bib2); Ji et al., [[n.d.]](#bib.bib40))。IDM 用于改进机器学习算法，例如
    AdaBoost (Utkin, [2015](#bib.bib90))、决策树 (Serafín et al., [2020](#bib.bib71))
    或朴素分类器 (Corani 和 de Campos, [2010](#bib.bib15))。模糊逻辑与 DNNs 结合，称为模糊 DNNs，以处理数据中的模糊性 (Das
    et al., [2020](#bib.bib18); Holyoak, [1987](#bib.bib35); El Hatri 和 Boumhidi,
    [2018](#bib.bib25); Wang et al., [2016](#bib.bib97); Zhang et al., [2014](#bib.bib114))。贝叶斯推断主要用于推断贝叶斯特征的后验分布 (Sofman
    et al., [2006](#bib.bib81); Tripathi 和 Govindaraju, [2007](#bib.bib87); G. Tian
    和 Feng, [2011](#bib.bib27))。SL 的虚空和不一致不确定性维度在证据神经网络中被考虑用于分类问题的不确定性感知决策 (Sensoy
    et al., [2018](#bib.bib70); Zhao et al., [2019b](#bib.bib120))。虚空用于检测分布外 (OOD)
    样本，而不一致用于检测分类错误样本。粗糙集理论与 DNNs 结合，称为 RDNNs，以处理数据中的不精确信息和不确定性（例如，作为输入和/或输出变量的值范围） (Zhang,
    [2007](#bib.bib110); Yasdi, [1995](#bib.bib103); Lingras, [1996](#bib.bib55);
    Khodayar et al., [2017](#bib.bib47))。
- en: RQ5. What are the key differences of uncertainty reasoning and quantification
    in belief theory and deep learning?
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: RQ5. 不确定性推理和量化在信念理论与深度学习中的关键区别是什么？
- en: 'Answer: Deep learning (DL) has received high attention because of its powerful
    capability to deal with a large volume of high dimensional data and provide solutions
    to maximize decision performance. However, as DL is limited in dealing with uncertainty
    explicitly, it often faces the issue of unexplainability, a well-known issue of
    explainable AI (XAI), due to its nature of statistical inference. On the other
    hand, belief models provide rigorous mathematical formulation based on a limited
    number of parameters which can offer the capability to easily reason and quantify
    different types of uncertainties. This merit of quantifiable uncertainty in belief
    models can provide reasons to explain a decision made based on mathematical induction.
    However, belief models suffer from dealing with a large volume of data, which
    can be complemented by DL. Therefore, our work discussed how a belief model (e.g.,
    SL) has been bridged with DL to improve decision making capability based on the
    merits of both approaches to achieve XAI.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 答：深度学习（DL）因其强大的处理大量高维数据的能力和提供最大化决策性能的解决方案而受到高度关注。然而，由于深度学习在显式处理不确定性方面存在局限性，它经常面临解释性不足的问题，这也是可解释人工智能（XAI）的一个众所周知的问题，这源于其统计推断的本质。另一方面，信念模型基于有限的参数提供了严格的数学公式，这可以提供轻松推理和量化不同类型不确定性的能力。这种在信念模型中的可量化不确定性的优点可以提供基于数学归纳法解释决策的理由。然而，信念模型在处理大量数据时存在困难，这可以通过深度学习来弥补。因此，我们的工作讨论了信念模型（例如，SL）如何与深度学习结合，以利用两种方法的优点来提高决策能力，实现可解释人工智能（XAI）。
- en: RQ6. How can belief model(s) be applied in DL to solve complicated decision
    making problems?
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: RQ6. 如何在深度学习中应用信念模型来解决复杂的决策问题？
- en: 'Answer: There may be various ways to leverage belief models considered in DL
    research. One example we discussed in Section [4](#S4 "4\. Belief Theory Meets
    Deep Learning ‣ A Survey on Uncertainty Reasoning and Quantification for Decision
    Making: Belief Theory Meets Deep Learning") is combining SL’s opinions with DNNs
    by constructing evidential NNs (ENNs). That is, ENNs can be built to generate
    evidence to formulate a subjective opinion in SL, rather than using a common activation
    function, such as softmax, generating class probabilities. Based on the estimated
    evidence in ENNs, we can calculate vacuity and dissonance uncertainty by leveraging
    the operators in SL. Depending on the degree of the quantified uncertainty values,
    such as vacuity, vagueness, or dissonance, diverse algorithms can be developed
    for effective decision making. Based on our prior work (Zhao et al., [2019b](#bib.bib120)),
    we found vacuity is a promising uncertainty type to detect out-of-distribution
    (OOD) samples while dissonance is an uncertainty type that can effectively detect
    misclassification samples.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '答：在深度学习研究中可能有多种利用信念模型的方法。我们在第[4](#S4 "4\. Belief Theory Meets Deep Learning
    ‣ A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief
    Theory Meets Deep Learning")节中讨论了一个例子，即通过构建证据神经网络（ENNs）将SL的意见与DNN结合起来。也就是说，ENNs可以被构建来生成证据，以在SL中形成主观意见，而不是使用常见的激活函数，如softmax，来生成类别概率。基于ENNs中估计的证据，我们可以通过利用SL中的运算符来计算虚无性和不一致性不确定性。根据量化的不确定性值的程度，例如虚无性、模糊性或不一致性，可以开发多样的算法以实现有效的决策。根据我们之前的工作（Zhao
    et al., [2019b](#bib.bib120)），我们发现虚无性是一种有前景的检测分布外（OOD）样本的不确定性类型，而不一致性是一种可以有效检测错误分类样本的不确定性类型。'
- en: 6\. Concluding Remarks
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: 6.1\. Insights, Lessons Learned, and Limitations
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 见解、经验教训和局限性
- en: •
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recent efforts have been made to estimate different types of uncertainty in
    our prior work (Jøsang et al., [2018](#bib.bib44)) while vacuity and vagueness
    have been mainly considered in the past (Shafer, [1976](#bib.bib72); Jøsang, [2016](#bib.bib43);
    Zadeh, [1965](#bib.bib107)). However, reasoning and quantification of other dimensions
    of uncertainty still remains unaddressed in the literature. Furthermore, the question
    of how different types of uncertainty can be helpful for effective decision making
    has not been addressed in the literature.
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们之前的工作中，已进行了不同类型不确定性的估计（Jøsang et al., [2018](#bib.bib44)），而虚无性和模糊性主要在过去被考虑（Shafer,
    [1976](#bib.bib72); Jøsang, [2016](#bib.bib43); Zadeh, [1965](#bib.bib107)）。然而，其他维度的不确定性的推理和量化在文献中仍未得到解决。此外，不同类型的不确定性如何有助于有效决策的问题在文献中尚未得到解决。
- en: •
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When both all the belief masses and a prior belief supporting each belief are
    the same, decision making becomes more challenging because it leads to inconclusiveness.
    Although utility-based belief masses have been studied (Yang et al., [2009](#bib.bib102);
    Jøsang, [2016](#bib.bib43)), their contributions are limited with the theoretical
    discussions based on simple examples.
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当所有信念质量和支持每个信念的先验信念都相同时，决策变得更加具有挑战性，因为这会导致不确定性。尽管基于效用的信念质量已经被研究过（Yang et al.,
    [2009](#bib.bib102); Jøsang, [2016](#bib.bib43)），但其贡献仅限于基于简单示例的理论讨论。
- en: •
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most belief/evidence theories and their uncertainty reasoning show high maturity
    as they have been explored since the 1960s. However, they are mostly theoretical
    and have not been thoroughly validated based on real datasets and/or applications
    for effective decision making and learning.
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数信念/证据理论及其不确定性推理显示出较高的成熟度，因为它们自1960年代以来已经被探索。然而，它们大多是理论性的，并且尚未基于真实数据集和/或应用程序进行彻底验证，以实现有效的决策制定和学习。
- en: •
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Bayesian theorem and inference methods are the foundations of the advanced machine
    learning and deep learning algorithms. However, the fitting of Bayesian inference
    from an one-parameter model to a deep learning model with a large volume of parameters
    can introduce non-trivial challenges in quantifying uncertainties.
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯定理和推断方法是高级机器学习和深度学习算法的基础。然而，从单参数模型到具有大量参数的深度学习模型的贝叶斯推断拟合可能会在量化不确定性时引入非平凡的挑战。
- en: •
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Although belief models have considered various types of uncertainties, as described
    in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1\. Classification of Uncertainty Types ‣ 2\.
    Classification Types, Causes, and Ontology of Uncertainty ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning"),
    the terminologies of those types are often found very similar but their distinctions
    have not been clarified. Although our survey paper can help readers better understand
    the diverse types of uncertainties, one may want to argue about our clarification
    of uncertainty types in Section [2](#S2 "2\. Classification Types, Causes, and
    Ontology of Uncertainty ‣ A Survey on Uncertainty Reasoning and Quantification
    for Decision Making: Belief Theory Meets Deep Learning"). This implies that much
    more efforts should be made to investigate different types of uncertainties and
    their effect on diverse decision making settings and applications.'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '尽管信念模型已经考虑了各种类型的不确定性，如图[1](#S2.F1 "Figure 1 ‣ 2.1\. Classification of Uncertainty
    Types ‣ 2\. Classification Types, Causes, and Ontology of Uncertainty ‣ A Survey
    on Uncertainty Reasoning and Quantification for Decision Making: Belief Theory
    Meets Deep Learning")所示，这些类型的术语通常非常相似，但它们的区别尚未明确。尽管我们的综述论文可以帮助读者更好地理解各种类型的不确定性，但有人可能会对我们在第[2](#S2
    "2\. Classification Types, Causes, and Ontology of Uncertainty ‣ A Survey on Uncertainty
    Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning")节对不确定性类型的澄清提出异议。这意味着应该付出更多努力来调查不同类型的不确定性及其对多种决策设置和应用的影响。'
- en: •
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Some efforts leveraging both a belief theory and deep learning have been made,
    such as fuzzy deep neural networks (FDNNs) combining fuzzy logic and DNNs (Das
    et al., [2020](#bib.bib18)), rough deep neural networks (RDNNs) combining rough
    logic and DNNs (Lingras, [1996](#bib.bib55)), and evidential neural networks (ENNs)
    combining SL and DNNs (Zhao et al., [2020](#bib.bib118)). However, although belief
    theories have been explored for several decades and their uncertainty research
    has been matured more than any other fields, their applications in reasoning and
    quantifying uncertainty in DL are still in an infant stage.
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已经有一些努力结合了信念理论和深度学习，如将模糊逻辑和DNNs结合的模糊深度神经网络（FDNNs）（Das et al., [2020](#bib.bib18)）、将粗糙逻辑和DNNs结合的粗糙深度神经网络（RDNNs）（Lingras,
    [1996](#bib.bib55)）以及将SL和DNNs结合的证据神经网络（ENNs）（Zhao et al., [2020](#bib.bib118)）。然而，尽管信念理论已被探索数十年，其不确定性研究也比其他领域更为成熟，但其在深度学习中推理和量化不确定性的应用仍处于初级阶段。
- en: •
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Although we can consider misclassification detection and out-of-distribution
    detection tasks to evaluate the accuracy of predictive uncertainty measured, the
    metrics of predictive uncertainty have not been validated as it is hard to determine
    the ground truth of measured uncertainty. To have more valid metrics of predictive
    uncertainty, we need to develop a way to evaluate the data generation process
    by considering the causes of uncertainty (e.g., how to generate data with vagueness).
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管我们可以考虑误分类检测和分布外检测任务来评估预测不确定性的准确性，但预测不确定性的度量尚未得到验证，因为很难确定测量不确定性的真实情况。为了获得更有效的预测不确定性度量，我们需要开发一种方法，通过考虑不确定性的原因（例如，如何生成模糊的数据）来评估数据生成过程。
- en: •
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uncertainty can be easily introduced by intelligent adversarial attacks taking
    highly deceptive poisonous or evasion attacks. Detecting adversarial attacks with
    the intent to increase various types of uncertainty should be the first step to
    reduce noises or false information that can increase uncertainty before estimating
    uncertainty in data for effective decision making.
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不确定性可以通过智能对抗攻击轻易引入，这些攻击包括高度欺骗性的毒药攻击或规避攻击。检测具有增加各种不确定性意图的对抗攻击应是减少噪声或虚假信息的第一步，以减少在有效决策前数据中不确定性的估计。
- en: 6.2\. Future Research Directions
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 未来研究方向
- en: •
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uncertainty quantification research can be explored more for the studies using
    qualitative labels. Since belief theories, such as DST, DSmT, or TBM, can provide
    the capability to fuse qualitative beliefs, their applications in natural language
    processing (NLP) are promising. Other conventional NLP methods can be compared
    to DSmT in handling uncertainty for the qualitative beliefs and its effect on
    application performance.
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不确定性量化研究可以进一步探讨使用定性标签的研究。由于信念理论，如DST、DSmT或TBM，可以融合定性信念，它们在自然语言处理（NLP）中的应用前景广阔。可以将其他传统NLP方法与DSmT在处理定性信念的不确定性及其对应用性能的影响上进行比较。
- en: •
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When a different belief model, that has a different way of estimating different
    types of uncertainty, is combined with deep learning, we can investigate how the
    different ways of measuring uncertainty can impact decision making performance.
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当不同信念模型（具有不同的不确定性估计方式）与深度学习结合时，我们可以调查不同的测量不确定性方式如何影响决策性能。
- en: •
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A belief model, such as IDM, offers the ability to derive a belief without any
    prior knowledge about a given proposition. In the settings that do not allow any
    prior knowledge or information about the proposition, IDM can allow one to make
    decisions under uncertainty.
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种信念模型，如IDM，提供了在没有关于给定命题的先验知识的情况下推导信念的能力。在那些不允许有关于命题的先验知识或信息的设置中，IDM可以让人们在不确定性下做出决策。
- en: •
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To solve sequential decision making problems, belief models can be combined
    with deep reinforcement learning. In particular, as IDM does not require having
    prior knowledge in the decision making process, it can be easily used for an RL
    agent to make decisions in the environment with no prior knowledge and to learn
    an optimal action via trials and errors.
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了解决序列决策问题，信念模型可以与深度强化学习结合。特别是，由于IDM在决策过程中不需要先验知识，它可以被轻松地用于RL代理在没有先验知识的环境中做出决策，并通过试错学习到*最优行动*。
- en: •
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Although many different types of uncertainty have been discussed in belief
    models, we can capture three main uncertainty types: vacuity caused by a lack
    of evidence, vagueness (or fuzziness) by failing in capturing a singleton belief,
    and discord (or dissonance) by conflicting evidence. This can be further examined
    to propose a unified mathematical belief framework to quantify various uncertainty
    types and belief masses for its broader applicability.'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管信念模型中讨论了许多不同类型的不确定性，我们可以捕捉到三种主要的不确定性类型：由于缺乏证据导致的空缺、不捕捉单一信念的模糊（或不确定性），以及由冲突证据引起的分歧（或不一致）。这可以进一步研究，提出一个统一的数学信念框架，以量化各种不确定性类型和信念质量，从而提高其广泛适用性。
- en: •
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most uncertainty measurements approaches are designed for a singleton prediction,
    such as image or node classifications. However, they may not be able to extend
    for time series application because they ignore temporal dependencies in uncertainty
    quantification. It is critical to developing novel uncertainty metrics considering
    temporal dependencies of time series data. For example, we may consider fusion
    operators in SL to fuse the subjective opinions from different time steps.
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数不确定性测量方法是为单一预测设计的，例如图像或节点分类。然而，它们可能无法扩展到时间序列应用，因为它们忽视了不确定性量化中的时间依赖性。开发考虑时间序列数据时间依赖性的新型不确定性度量是至关重要的。例如，我们可以考虑在
    SL 中使用融合操作符来融合来自不同时间步的主观意见。
- en: •
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most uncertainty estimation research focuses on unstructured tasks, such as
    classification and regression tasks. Meanwhile, for the structured prediction
    tasks, such as language modeling (e.g., machine translation and named entity recognition),
    we can further investigate a general, unsupervised, interpretable uncertainty
    framework.
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数不确定性估计研究集中在非结构化任务上，例如分类和回归任务。与此同时，对于结构化预测任务，例如语言建模（例如机器翻译和命名实体识别），我们可以进一步研究一个通用的、无监督的、可解释的不确定性框架。
- en: Acknowledgement
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is partly supported by the Army Research Office under Grant Contract
    Number W91NF-20-2-0140 and NSF under the Grant Number 2107449, 2107450, and 2107451\.
    The views and conclusions contained in this document are those of the authors
    and should not be interpreted as representing the official policies, either expressed
    or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government
    is authorized to reproduce and distribute reprints for Government purposes notwithstanding
    any copyright notation herein.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分由陆军研究办公室资助，资助合同号 W91NF-20-2-0140 和 NSF 资助号 2107449、2107450 和 2107451。本文中包含的观点和结论仅代表作者个人意见，不应被解读为陆军研究实验室或美国政府的官方政策，无论是明示还是暗示。美国政府被授权复制和分发重印件用于政府目的，尽管文中有任何版权声明。
- en: References
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Abbas et al. (2015) N. Abbas, Y. Chibani, A. Martin, and F. Smarandache. 2015.
    The effective use of the DSmT for multi-class classification. *Advances and Applications
    of DSmT for Information Fusion* (2015), 359.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbas 等 (2015) N. Abbas, Y. Chibani, A. Martin, 和 F. Smarandache. 2015. DSmT
    在多类别分类中的有效使用。*DSmT 在信息融合中的进展与应用* (2015), 359。
- en: 'Abdar et al. (2021) M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L.
    Liu, M. Ghavamzadeh, P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya, V. Makarenkov,
    and S. Nahavandi. 2021. A review of uncertainty quantification in deep learning:
    techniques, applications and challenges. *Information Fusion* (2021).'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdar 等 (2021) M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M.
    Ghavamzadeh, P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya, V. Makarenkov, 和
    S. Nahavandi. 2021. 深度学习中的不确定性量化综述：技术、应用和挑战。*信息融合* (2021)。
- en: Ahmed et al. (2018) S. A. Ahmed, D. P. Dogra, S. Kar, and P. P. Roy. 2018. Surveillance
    scene representation and trajectory abnormality detection using aggregation of
    multiple concepts. *Expert Systems with Applications* 101 (2018), 43–55.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed 等 (2018) S. A. Ahmed, D. P. Dogra, S. Kar, 和 P. P. Roy. 2018. 使用多个概念聚合的监控场景表示和轨迹异常检测。*应用专家系统*
    101 (2018), 43–55。
- en: Alim et al. (2019) A. Alim, X. Zhao, J.H. Cho, and F. Chen. 2019. Uncertainty-aware
    opinion inference under adversarial attacks. In *The 2019 IEEE International Conference
    on Big Data (IEEE Big Data 2019)*. 6–15.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alim 等 (2019) A. Alim, X. Zhao, J.H. Cho, 和 F. Chen. 2019. 对抗攻击下的不确定性感知意见推断。在
    *2019 IEEE 国际大数据会议 (IEEE Big Data 2019)*。6–15。
- en: Bhushan et al. (2020) C. Bhushan, Z. Yang, N. Virani, and N. Iyer. 2020. Variational
    encoder-based reliable classification. In *2020 IEEE International Conference
    on Image Processing (ICIP)*. IEEE, 1941–1945.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhushan 等 (2020) C. Bhushan, Z. Yang, N. Virani, 和 N. Iyer. 2020. 基于变分编码器的可靠分类。在
    *2020 IEEE 国际图像处理会议 (ICIP)*。IEEE，1941–1945。
- en: Blasch et al. (2013) E. Blasch, J. Dezert, and B. Pannetier. 2013. Overview
    of Dempster-Shafer and belief function tracking methods. In *Proceedings of SPIE*,
    Vol. 8745\. Baltimore, Maryland, USA.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blasch 等 (2013) E. Blasch, J. Dezert, 和 B. Pannetier. 2013. Dempster-Shafer
    和信任函数跟踪方法概述。在 *SPIE 会议录*，第 8745 卷。马里兰州，巴尔的摩，美国。
- en: 'Brugnach et al. (2008) M. Brugnach, A. Dewulf, C. Pahl-Wostl, and T. Taillieu.
    2008. Toward a relational concept of uncertainty: about knowing too little, knowing
    too differently, and accepting not to know. *Ecology and Society* 13, 2 (2008),
    30.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brugnach 等（2008）M. Brugnach, A. Dewulf, C. Pahl-Wostl 和 T. Taillieu. 2008. 朝向不确定性的关系概念：关于知道得太少、知道得过于不同，以及接受不知道。*生态与社会*
    13, 2（2008），30。
- en: 'Charpentier et al. (2020) B. Charpentier, D. Zügner, and S. Günnemann. 2020.
    Posterior network: Uncertainty estimation without ood samples via density-based
    pseudo-counts. *Advances in Neural Information Processing Systems* 33 (2020),
    1356–1367.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charpentier 等（2020）B. Charpentier, D. Zügner 和 S. Günnemann. 2020. 后验网络：通过基于密度的伪计数进行无
    OOD 样本的不确定性估计。*神经信息处理系统进展* 33（2020），1356–1367。
- en: Chen et al. (2015) CL Philip Chen, Chun-Yang Zhang, Long Chen, and Min Gan.
    2015. Fuzzy restricted Boltzmann machine for the enhancement of deep learning.
    *IEEE Transactions on Fuzzy Systems* 23, 6 (2015), 2163–2173.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2015）CL Philip Chen, Chun-Yang Zhang, Long Chen 和 Min Gan. 2015. 用于深度学习增强的模糊限制玻尔兹曼机。*IEEE
    模糊系统汇刊* 23, 6（2015），2163–2173。
- en: Chen et al. (2018) W. Chen, J. An, R. Li, L. Fu, G. Xie, Md. Z. A. Bhuiyan,
    and K. Li. 2018. A novel fuzzy deep-learning approach to traffic flow prediction
    with uncertain spatial–temporal data features. *Future Generation Computer Systems*
    89 (2018), 78–88.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2018）W. Chen, J. An, R. Li, L. Fu, G. Xie, Md. Z. A. Bhuiyan 和 K. Li.
    2018. 一种新颖的模糊深度学习方法，用于具有不确定空间–时间数据特征的交通流量预测。*未来一代计算机系统* 89（2018），78–88。
- en: Chen et al. (2009) W. Chen, Y. Wang, and S. Yang. 2009. Efficient influence
    maximization in social networks. In *Proceedings of the 15th ACM SIGKDD* (Paris,
    France). New York, NY, USA, 199–208.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2009）W. Chen, Y. Wang 和 S. Yang. 2009. 社交网络中的高效影响力最大化。在 *第 15 届 ACM SIGKDD
    会议论文集*（法国巴黎）。纽约，NY，USA，199–208。
- en: Chopade and Narvekar (2017) H. A. Chopade and M. Narvekar. 2017. Hybrid auto
    text summarization using deep neural network and fuzzy logic system. In *2017
    International Conference on Inventive Computing and Informatics (ICICI)*. 52–56.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chopade 和 Narvekar（2017）H. A. Chopade 和 M. Narvekar. 2017. 使用深度神经网络和模糊逻辑系统的混合自动文本摘要。在
    *2017 国际创新计算与信息学会议（ICICI）*。52–56。
- en: Codd (1986) E. F. Codd. 1986. Missing information (applicable and inapplicable)
    in relational databases. *ACM Sigmod Record* 15, 4 (1986), 53–53.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codd（1986）E. F. Codd. 1986. 关系数据库中的缺失信息（适用和不适用）。*ACM Sigmod Record* 15, 4（1986），53–53。
- en: Corani and de Campos (2010) G. Corani and C. P. de Campos. 2010. A tree augmented
    classifier based on extreme imprecise Dirichlet model. *International Journal
    of Approximate Reasoning* 51, 9 (Nov. 2010), 1053–1068.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Corani 和 de Campos（2010）G. Corani 和 C. P. de Campos. 2010. 基于极端不精确 Dirichlet
    模型的树增强分类器。*近似推理国际期刊* 51, 9（2010年11月），1053–1068。
- en: 'Costa et al. (2018) Paulo Costa, Anne-Laure Jousselme, Kathryn Laskey, Erik
    Blasch, Valentina Dragos, Juergen Ziegler, Pieter de Villiers, and Gregor Pavlin.
    2018. URREF: Uncertainty representation and reasoning evaluation framework for
    information fusion. *Journal of Advances in Information Fusion* 13, 2 (2018),
    137–157.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa 等（2018）Paulo Costa, Anne-Laure Jousselme, Kathryn Laskey, Erik Blasch,
    Valentina Dragos, Juergen Ziegler, Pieter de Villiers 和 Gregor Pavlin. 2018. URREF：用于信息融合的不确定性表示与推理评估框架。*信息融合进展杂志*
    13, 2（2018），137–157。
- en: Dahl (1979) V. Dahl. 1979. Quantification in a three-valued logic for natural
    language question-answering systems. In *Proceedings of the 6th international
    joint conference on Artificial intelligence-Volume 1*. 182–187.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dahl（1979）V. Dahl. 1979. 自然语言问答系统中的三值逻辑量化。在 *第 6 届国际人工智能联合会议论文集-第 1 卷*。182–187。
- en: Das et al. (2020) R. Das, S. Sen, and U. Maulik. 2020. A survey on fuzzy deep
    neural networks. *ACM Computing Surveys (CSUR)* 53, 3 (2020), 1–25.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das 等（2020）R. Das, S. Sen 和 U. Maulik. 2020. 关于模糊深度神经网络的综述。*ACM 计算调查（CSUR）*
    53, 3（2020），1–25。
- en: Deepa and Swamynathan (2014) R. Deepa and S. Swamynathan. 2014. *Recent Trends
    in Computer Networks and Distributed Systems Security Communications in Computer
    and Information Science*. Vol. 420. Springer-Verlag Berlin Heidelberg, Chapter
    A Trust Model for Directory-Based Service Discovery in Mobile Ad Hoc Networks,
    115–126.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deepa 和 Swamynathan（2014）R. Deepa 和 S. Swamynathan. 2014. *计算机网络与分布式系统安全的最新趋势*
    《计算机与信息科学通信》。第 420 卷。Springer-Verlag Berlin Heidelberg，第 A 章 移动自组网中基于目录的服务发现信任模型，115–126。
- en: Deng et al. (2016) Y. Deng, Z. Ren, Y. Kong, F. Bao, and Q. Dai. 2016. A hierarchical
    fused fuzzy deep neural network for data classification. *IEEE Transactions on
    Fuzzy Systems* 25, 4 (2016), 1006–1012.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2016）Y. Deng, Z. Ren, Y. Kong, F. Bao 和 Q. Dai. 2016. 一种用于数据分类的分层融合模糊深度神经网络。*IEEE
    模糊系统汇刊* 25, 4（2016），1006–1012。
- en: 'Dewulf et al. (2005) A. Dewulf, M. Craps, R. Bouwen, T. Taillieu, and C. Pahl-Wostl.
    2005. Integrated management of natural resources: Dealing with ambiguous issues,
    multiple actors and diverging frames. *Water Science and Technology* 52 (Feb.
    2005), 115–24.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dewulf et al. (2005) A. Dewulf, M. Craps, R. Bouwen, T. Taillieu, and C. Pahl-Wostl.
    2005. 自然资源的综合管理：应对模糊问题、多方参与和分歧框架。*水科学与技术* 52 (2005年2月), 115–124。
- en: Dezert and Smarandache (2004) J. Dezert and F. Smarandache. 2004. *Advances
    and Applications of DSmT for Information Fusion*. American Research Press, Rehoboth,
    NM, USA, Chapter Advances on DSmT.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dezert and Smarandache (2004) J. Dezert and F. Smarandache. 2004. *DSmT在信息融合中的进展与应用*。American
    Research Press, Rehoboth, NM, USA, 章节：DSmT的进展。
- en: Dubois and Prade (1988) D. Dubois and H. Prade. 1988. Representation and combination
    of uncertainty with belief functions and possibility measures. *Computational
    Intelligence* 4, 3 (1988), 244–264.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubois and Prade (1988) D. Dubois and H. Prade. 1988. 使用信念函数和可能性度量的不确定性表示与组合。*计算智能*
    4, 3 (1988), 244–264。
- en: 'Dubois (1980) D. J. Dubois. 1980. *Fuzzy sets and systems: Theory and applications*.
    Vol. 144. Academic press.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubois (1980) D. J. Dubois. 1980. *模糊集与系统：理论与应用*。第144卷。学术出版社。
- en: El Hatri and Boumhidi (2018) C. El Hatri and J. Boumhidi. 2018. Fuzzy deep learning
    based urban traffic incident detection. *Cognitive Systems Research* 50 (2018),
    206–213.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: El Hatri and Boumhidi (2018) C. El Hatri and J. Boumhidi. 2018. 基于模糊深度学习的城市交通事件检测。*认知系统研究*
    50 (2018), 206–213。
- en: Fienberg (2006) S. E. Fienberg. 2006. When Did Bayesian Inference Become “Bayesian"?
    *Bayesian Analysis* 1, 1 (2006), 1–40.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fienberg (2006) S. E. Fienberg. 2006. 贝叶斯推断何时成为“贝叶斯”的？ *贝叶斯分析* 1, 1 (2006),
    1–40。
- en: G. Tian and Feng (2011) Y. Zhang G. Tian, Y. Xia and D. Feng. 2011. Hybrid genetic
    and variational expectation-maximization algorithm for Gaussian-mixture-model-based
    brain MR image segmentation. *IEEE transactions on information technology in biomedicine*
    15, 3 (2011), 373–380.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G. Tian and Feng (2011) Y. Zhang G. Tian, Y. Xia and D. Feng. 2011. 基于高斯混合模型的脑MR图像分割的混合遗传与变分期望最大化算法。*IEEE生物医学信息技术期刊*
    15, 3 (2011), 373–380。
- en: Gawlikowski et al. (2021) Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,
    Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph
    Triebel, Peter Jung, Ribana Roscher, et al. 2021. A survey of uncertainty in deep
    neural networks. *arXiv preprint arXiv:2107.03342* (2021).
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gawlikowski et al. (2021) Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,
    Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph
    Triebel, Peter Jung, Ribana Roscher, 等. 2021. 深度神经网络中的不确定性调查。*arXiv预印本arXiv:2107.03342*
    (2021)。
- en: Guil (2019) F. Guil. 2019. Associative classification based on the transferable
    belief model. *Knowledge-Based Systems* 182 (2019), 104800.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guil (2019) F. Guil. 2019. 基于可转移信念模型的关联分类。*基于知识的系统* 182 (2019), 104800。
- en: Hankin (2010) R. K. S. Hankin. 2010. A Generalization of the Dirichlet Distribution.
    *Journal of Statistical Software* 33, 11 (Feb. 2010).
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hankin (2010) R. K. S. Hankin. 2010. Dirichlet分布的一般化。*统计软件期刊* 33, 11 (2010年2月)。
- en: 'Hariri et al. (2019) R. H. Hariri, E. M. Fredericks, and K. M. Bowers. 2019.
    Uncertainty in big data analytics: Survey, opportunities, and challenges. *Journal
    of Big Data* 6 (2019), 1–16.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hariri et al. (2019) R. H. Hariri, E. M. Fredericks, and K. M. Bowers. 2019.
    大数据分析中的不确定性：调查、机会与挑战。*大数据期刊* 6 (2019), 1–16。
- en: Henni et al. (2019) A. H. Henni, R. B. Bachouch, O. Bennis, and N. Ramdani.
    2019. Enhanced multiplex binary PIR localization using the transferable belief
    model. *IEEE Sensors Journal* 19, 18 (2019), 8146–8159.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henni et al. (2019) A. H. Henni, R. B. Bachouch, O. Bennis, and N. Ramdani.
    2019. 使用可转移信念模型的增强多路复用二进制PIR定位。*IEEE传感器期刊* 19, 18 (2019), 8146–8159。
- en: Hernandez-Potiomkin et al. (2018) Y. Hernandez-Potiomkin, M. Saifuzzaman, E.
    Bert, R. Mena-Yedra, T. Djukic, and J. Casas. 2018. Unsupervised incident detection
    model in urban and freeway networks. In *2018 21st International Conference on
    Intelligent Transportation Systems (ITSC)*. IEEE, 1763–1769.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernandez-Potiomkin et al. (2018) Y. Hernandez-Potiomkin, M. Saifuzzaman, E.
    Bert, R. Mena-Yedra, T. Djukic, and J. Casas. 2018. 城市和高速公路网络中的无监督事件检测模型。在 *2018年第21届国际智能交通系统会议（ITSC）*。IEEE,
    1763–1769。
- en: Hoff (2009) P. D Hoff. 2009. *A first course in Bayesian statistical methods*.
    Vol. 580. Springer.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoff (2009) P. D Hoff. 2009. *贝叶斯统计方法入门*。第580卷。Springer。
- en: 'Holyoak (1987) K. J Holyoak. 1987. Parallel distributed processing: explorations
    in the microstructure of cognition. *Science* 236 (1987), 992–997.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holyoak (1987) K. J Holyoak. 1987. 并行分布式处理：认知微观结构的探索。*科学* 236 (1987), 992–997。
- en: Honer and Hettmann (2018) J. Honer and H. Hettmann. 2018. Motion state classification
    for automotive LIDAR based on evidential grid maps and transferable belief model.
    In *2018 21st International Conference on Information Fusion (FUSION)*. IEEE,
    1056–1063.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Honer 和 Hettmann（2018）J. Honer 和 H. Hettmann。2018年。《基于证据网格地图和可转移信念模型的汽车LIDAR运动状态分类》。在*2018年第21届信息融合国际会议*中。IEEE，1056–1063页。
- en: Hu et al. (2020) Y. Hu, Y. Ou, X. Zhao, J.-H. Cho, and F. Chen. 2020. Multidimensional
    uncertainty-aware evidential neural networks. *In Proceeding of the Thirty-fifth
    AAAI Conference on Artificial Intelligence* (2020).
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2020）Y. Hu, Y. Ou, X. Zhao, J.-H. Cho 和 F. Chen。2020年。《多维不确定性感知证据神经网络》。在*第三十五届AAAI人工智能会议论文集*（2020年）。
- en: 'Hüllermeier and Waegeman (2021) Eyke Hüllermeier and Willem Waegeman. 2021.
    Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts
    and methods. *Machine Learning* 110, 3 (2021), 457–506.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hüllermeier 和 Waegeman（2021）Eyke Hüllermeier 和 Willem Waegeman。2021年。《机器学习中的偶然与认知不确定性：概念与方法介绍》。*机器学习*
    110卷，第3期（2021年），457–506页。
- en: Jahangir et al. (2020) H. Jahangir, M. A. Golkar, F. Alhameli, A. Mazouz, A.
    Ahmadian, and A. Elkamel. 2020. Short-term wind speed forecasting framework based
    on stacked denoising auto-encoders with rough ANN. *Sustainable Energy Technologies
    and Assessments* 38 (2020), 100601.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jahangir 等（2020）H. Jahangir, M. A. Golkar, F. Alhameli, A. Mazouz, A. Ahmadian
    和 A. Elkamel。2020年。《基于堆叠去噪自编码器与粗糙人工神经网络的短期风速预测框架》。*可持续能源技术与评估* 38卷（2020年），100601页。
- en: Ji et al. ([n.d.]) X. Ji, Y. Ren, H. Tang, and J. Xiang. [n.d.]. DSmT-based
    three-layer method using multi-classifier to detect faults in hydraulic systems.
    *Mechanical Systems and Signal Processing* 153 ([n. d.]), 107513.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等（[n.d.]）X. Ji, Y. Ren, H. Tang 和 J. Xiang。[未注明日期]。《基于DSmT的三层方法使用多分类器检测液压系统故障》。*机械系统与信号处理*
    153卷（[未注明日期]），107513页。
- en: Jøsang (1999) A. Jøsang. 1999. An algebra for assessing trust in certification
    chains,” Proc. Network and Distributed Systems Security. In *Proceedings of Network
    and Distributed Systems Security (NDSS’99) Symposium*.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jøsang（1999）A. Jøsang。1999年。《用于评估认证链中信任的代数》。在*网络与分布式系统安全会议论文集（NDSS’99）*中。
- en: Jøsang (2001) A. Jøsang. 2001. A logic for uncertain probabilities. *International
    Journal of Uncertainty, Fuzziness and Knowledge-based Systems* 9, 3 (Jun. 2001).
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jøsang（2001）A. Jøsang。2001年。《不确定概率的逻辑》。*不确定性、模糊性与知识系统国际期刊* 9卷，第3期（2001年6月）。
- en: 'Jøsang (2016) A. Jøsang. 2016. *Subjective Logic: A Formalism for Reasoning
    Under Uncertainty*. Springer Publishing Company.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jøsang（2016）A. Jøsang。2016年。*主观逻辑：不确定性下的推理形式*。Springer Publishing Company。
- en: Jøsang et al. (2018) A. Jøsang, J. Cho, and F. Chen. 2018. Uncertainty Characteristics
    of Subjective Opinions. In *2018 21st International Conference on Information
    Fusion (FUSION)*. 1998–2005.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jøsang 等（2018）A. Jøsang, J. Cho 和 F. Chen。2018年。《主观意见的不确定性特征》。在*2018年第21届信息融合国际会议*中。1998–2005页。
- en: 'Kabir et al. (2018) H. D. Kabir, A. Khosravi, M. A. Hosen, and S. Nahavandi.
    2018. Neural network-based uncertainty quantification: A survey of methodologies
    and applications. *IEEE access* 6 (2018), 36218–36234.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kabir 等（2018）H. D. Kabir, A. Khosravi, M. A. Hosen 和 S. Nahavandi。2018年。《基于神经网络的不确定性量化：方法论与应用综述》。*IEEE
    Access* 6卷（2018年），36218–36234页。
- en: Kashkevich and Krasnoproshin (1979) S. Kashkevich and V. V. Krasnoproshin. 1979.
    A two-level automated pattern recognition complex. *U. S. S. R. Comput. Math.
    and Math. Phys.* 19, 6 (1979), 227–239.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kashkevich 和 Krasnoproshin（1979）S. Kashkevich 和 V. V. Krasnoproshin。1979年。《一种两级自动化模式识别系统》。*苏联计算数学与数学物理*
    19卷，第6期（1979年），227–239页。
- en: Khodayar et al. (2017) M. Khodayar, O. Kaynak, and M. Khodayar. 2017. Rough
    deep neural architecture for short-term wind speed forecasting. *Transactions
    on Industrial Informatics* 13, 6 (2017), 2770–2779.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khodayar 等（2017）M. Khodayar, O. Kaynak 和 M. Khodayar。2017年。《用于短期风速预测的粗糙深度神经网络架构》。*工业信息学汇刊*
    13卷，第6期（2017年），2770–2779页。
- en: Kiureghian and Ditlevsen (2009) A. D. Kiureghian and O. Ditlevsen. 2009. Aleatory
    or epistemic? Does it matter? *Structural Safety* 31, 2 (2009), 105–112. Risk
    Acceptance and Risk Communication.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiureghian 和 Ditlevsen（2009）A. D. Kiureghian 和 O. Ditlevsen。2009年。《偶然还是认知？重要吗？》。*结构安全*
    31卷，第2期（2009年），105–112页。风险接受与风险沟通。
- en: Kleene (1938) S. Kleene. 1938. On notation for ordinal numbers. *The Journal
    of Symbolic Logic* 3, 4 (1938), 150–155.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kleene（1938）S. Kleene。1938年。《序数的记号》。*符号逻辑期刊* 3卷，第4期（1938年），150–155页。
- en: 'Klir and Ramer (1990) G. Klir and A. Ramer. 1990. Uncertainty in the dempster-shafer
    theory: a critical re-examination. *International Journal of General System* 18,
    2 (1990), 155–166.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klir 和 Ramer（1990）G. Klir 和 A. Ramer. 1990. Dempster-Shafer理论中的不确定性：一个批判性的重新审视。*国际系统学杂志*
    18, 2 (1990), 155–166。
- en: 'Kumar and Banerjee (2017) A. Kumar and M. Banerjee. 2017. Kleene algebras and
    logic: boolean and rough set representations, 3-valued, rough set and perp semantics.
    *Studia Logica* 105, 3 (2017), 439–469.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 和 Banerjee（2017）A. Kumar 和 M. Banerjee. 2017. Kleene代数与逻辑：布尔与粗糙集表示、3值、粗糙集与极限语义。*逻辑研究*
    105, 3 (2017), 439–469。
- en: Lesani and Bagheri (2006) M. Lesani and S. Bagheri. 2006. Fuzzy trust inference
    in trust graphs and its application in semantic web social networks. In *World
    Automation Congress (WAS 2006)*. 1–6.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lesani 和 Bagheri（2006）M. Lesani 和 S. Bagheri. 2006. 信任图中的模糊信任推断及其在语义网社交网络中的应用。发表于*世界自动化大会（WAS
    2006）*。1–6。
- en: 'Li et al. (2012) Y. Li, J. Chen, and L. Feng. 2012. Dealing with uncertainty:
    A survey of theories and practices. *Transactions on Knowledge and Data Engineering*
    25, 11 (2012), 2463–2482.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2012）Y. Li, J. Chen, 和 L. Feng. 2012. 处理不确定性：理论与实践综述。*知识与数据工程交易* 25, 11
    (2012), 2463–2482。
- en: Liao et al. (2009) H. Liao, Q. Wang, and G. Li. 2009. A fuzzy logic-based trust
    model in grid. In *International Conference on Networks Security, Wireless Communications
    and Trusted Computing (NSWCTC 2009)*. Wuhan, Hubei, China, 608–614.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao 等人（2009）H. Liao, Q. Wang, 和 G. Li. 2009. 基于模糊逻辑的网格信任模型。发表于*国际网络安全、无线通信与可信计算会议（NSWCTC
    2009）*。武汉，湖北，中国，608–614。
- en: Lingras (1996) P. Lingras. 1996. Rough neural networks. In *Proc. of the 6th
    Int. Conf. on Information Processing and Management of Uncertainty in Knowledgebased
    Systems*. 1445–1450.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lingras（1996）P. Lingras. 1996. 粗糙神经网络。发表于*第6届国际信息处理与知识系统不确定性管理会议论文集*。1445–1450。
- en: 'Linkov and Burmistrov (2003) I. Linkov and D. Burmistrov. 2003. Model uncertainty
    and choices made by modelers: Lessons learned from the international atomic energy
    agency model intercomparisons. *Risk Analysis: An International Journal* 23, 6
    (2003), 1297–1308.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkov 和 Burmistrov（2003）I. Linkov 和 D. Burmistrov. 2003. 模型不确定性及模型者所做的选择：从国际原子能机构模型比较中学到的经验。*风险分析：国际期刊*
    23, 6 (2003), 1297–1308。
- en: Łukasiewicz and Tarski (1930) J. Łukasiewicz and A. Tarski. 1930. Untersuchungen
    über den aussagenkalkül. *CR des seances de la Societe des Sciences et des Letters
    de Varsovie, cl. III* 23 (1930).
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Łukasiewicz 和 Tarski（1930）J. Łukasiewicz 和 A. Tarski. 1930. 关于命题演算的研究。*华沙科学与文学学会会议纪要，第三类*
    23 (1930)。
- en: Luo et al. (2019) C. Luo, C. Tan, X. Wang, and Y. Zheng. 2019. An evolving recurrent
    interval type-2 intuitionistic fuzzy neural network for online learning and time
    series prediction. *Applied Soft Computing* 78 (2019), 150–163.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人（2019）C. Luo, C. Tan, X. Wang, 和 Y. Zheng. 2019. 用于在线学习和时间序列预测的演变递归区间2型直觉模糊神经网络。*应用软计算*
    78 (2019), 150–163。
- en: Luo et al. (2008) J. Luo, X. Liu, Y. Zhang, D. Ye, and Z. Xu. 2008. Fuzzy trust
    recommendation based on collaborative filtering for mobile ad-hoc networks. In
    *33rd IEEE Conference on Local Computer Networks (LCN 2008)*. 305–311.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人（2008）J. Luo, X. Liu, Y. Zhang, D. Ye, 和 Z. Xu. 2008. 基于协同过滤的模糊信任推荐用于移动自组网。发表于*第33届IEEE本地计算网络会议（LCN
    2008）*。305–311。
- en: Malinin and Gales (2018) A. Malinin and M. Gales. 2018. Predictive uncertainty
    estimation via prior networks. In *Advances in Neural Information Processing Systems*.
    7047–7058.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin 和 Gales（2018）A. Malinin 和 M. Gales. 2018. 通过先验网络的预测不确定性估计。发表于*神经信息处理系统进展*。7047–7058。
- en: Manchala (1998) D. W. Manchala. 1998. Trust metrics, models and protocols for
    electronic commerce transactions. In *Proceedings of the 18th IEEE Int’l Conf.
    on Distributed Computing Systems*. Amsterdam, Netherlands, 312–321.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manchala（1998）D. W. Manchala. 1998. 电子商务交易中的信任度量、模型与协议。发表于*第18届IEEE国际分布式计算系统会议论文集*。阿姆斯特丹，荷兰，312–321。
- en: Nagy et al. (2008) M. Nagy, M. Vargas-Vera, and E. Motta. 2008. Multi agent
    trust for belief combination on the Semantic Web. In *4th International Conference
    on Intelligent Computer Communication and Processing*. 261–264.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagy 等人（2008）M. Nagy, M. Vargas-Vera, 和 E. Motta. 2008. 语义网中的多智能体信任用于信念组合。发表于*第4届国际智能计算通信与处理会议*。261–264。
- en: Nefti et al. (2005) S. Nefti, F. Meziane, and K. Kasiran. 2005. A fuzzy trust
    model for e-commerce. In *7th IEEE International Conference on E-Commerce Technology*.
    401–404.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nefti 等人（2005）S. Nefti, F. Meziane, 和 K. Kasiran. 2005. 用于电子商务的模糊信任模型。发表于*第7届IEEE国际电子商务技术会议*。401–404。
- en: Nguyen et al. (2018) T. Nguyen, S. Kavuri, and M. Lee. 2018. A fuzzy convolutional
    neural network for text sentiment analysis. *Journal of Intelligent & Fuzzy Systems*
    35, 6 (2018), 6025–6034.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen等人（2018）T. Nguyen, S. Kavuri, 和 M. Lee. 2018. 用于文本情感分析的模糊卷积神经网络。*智能与模糊系统期刊*
    35, 6（2018），6025–6034。
- en: Osband et al. (2021) I. Osband, Z. Wen, M. Asghari, M. Ibrahimi, X. Lu, and
    B. Van Roy. 2021. Epistemic neural networks. *arXiv preprint arXiv:2107.08924*
    (2021).
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband等人（2021）I. Osband, Z. Wen, M. Asghari, M. Ibrahimi, X. Lu, 和 B. Van Roy.
    2021. 认知神经网络。*arXiv预印本 arXiv:2107.08924*（2021）。
- en: Park et al. (2016) Seonyeong Park, Suk Jin Lee, Elisabeth Weiss, and Yuichi
    Motai. 2016. Intra-and inter-fractional variation prediction of lung tumors using
    fuzzy deep learning. *IEEE journal of translational engineering in health and
    medicine* 4 (2016), 1–12.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人（2016）Seonyeong Park, Suk Jin Lee, Elisabeth Weiss, 和 Yuichi Motai. 2016.
    使用模糊深度学习预测肺肿瘤的体内和体间变化。*IEEE医学与健康工程翻译期刊* 4（2016），1–12。
- en: Quost et al. (2005) B. Quost, T T. Denaeux, and M. Masson. 2005. Pairwise classifier
    combination in the transferable belief model. In *2005 7th international conference
    on information fusion*, Vol. 1\. 8–pp.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quost等人（2005）B. Quost, T T. Denaeux, 和 M. Masson. 2005. 可转移信念模型中的成对分类器组合。在*2005年第七届国际信息融合会议*，第1卷。8–pp。
- en: Salakhutdinov and Larochelle (2010) Ruslan Salakhutdinov and Hugo Larochelle.
    2010. Efficient learning of deep Boltzmann machines. In *Proceedings of the thirteenth
    international conference on artificial intelligence and statistics*. JMLR Workshop
    and Conference Proceedings, 693–700.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salakhutdinov 和 Larochelle（2010）Ruslan Salakhutdinov 和 Hugo Larochelle. 2010.
    深度玻尔兹曼机的高效学习。在*第十三届人工智能与统计国际会议论文集*。JMLR研讨会与会议论文集，693–700。
- en: Sensoy et al. (2020) M. Sensoy, L. Kaplan, F. Cerutti, and M. Saleki. 2020.
    Uncertainty-aware deep classifiers using generative models. In *Proceedings of
    the AAAI Conference on Artificial Intelligence*, Vol. 34. 5620–5627.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sensoy等人（2020）M. Sensoy, L. Kaplan, F. Cerutti, 和 M. Saleki. 2020. 使用生成模型的不确定性感知深度分类器。在*AAAI人工智能会议论文集*，第34卷。5620–5627。
- en: Sensoy et al. (2018) M. Sensoy, L. M. Kaplan, and M. Kandemir. 2018. Evidential
    deep learning to quantify classification uncertainty. In *Advances in Neural Information
    Processing Systems*. 3179–3189.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sensoy等人（2018）M. Sensoy, L. M. Kaplan, 和 M. Kandemir. 2018. 证据深度学习以量化分类不确定性。在*神经信息处理系统进展*。3179–3189。
- en: Serafín et al. (2020) M. Serafín, M. Carlos J, C. Javier G, and A. Joaquín.
    2020. Imprecise Classification with Non-parametric Predictive Inference. In *International
    Conference on Information Processing and Management of Uncertainty in Knowledge-Based
    Systems*. 53–66.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serafín等人（2020）M. Serafín, M. Carlos J, C. Javier G, 和 A. Joaquín. 2020. 使用非参数预测推断的模糊分类。在*国际信息处理与不确定性管理会议*。53–66。
- en: Shafer (1976) G. Shafer. 1976. *A Mathematical Theory of Evidence*. Princeton
    University Press, Princeton, NJ.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shafer（1976）G. Shafer. 1976. *证据的数学理论*。普林斯顿大学出版社，普林斯顿，NJ。
- en: Shi et al. (2020) W. Shi, X. Zhao, Feng F. Chen, and Q. Yu. 2020. Multifaceted
    uncertainty estimation for label-efficient deep learning. *Advances in Neural
    Information Processing Systems* 33 (2020), 17247–17257.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等人（2020）W. Shi, X. Zhao, Feng F. Chen, 和 Q. Yu. 2020. 标签高效深度学习的多方面不确定性估计。*神经信息处理系统进展*
    33（2020），17247–17257。
- en: Shirwandkar and Kulkarni (2018) N. Shirwandkar and S. Kulkarni. 2018. Extractive
    text summarization using deep learning. In *2018 Fourth International Conference
    on Computing Communication Control and Automation (ICCUBEA)*. 1–5.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shirwandkar 和 Kulkarni（2018）N. Shirwandkar 和 S. Kulkarni. 2018. 使用深度学习的提取式文本摘要。在*2018年第四届国际计算通信控制与自动化会议（ICCUBEA）*。1–5。
- en: 'Smarandache (2012) F. Smarandache. 2012. Neutrosophic masses & indeterminate
    models: applications to information fusion. In *2012 15th International Conference
    on Information Fusion*. IEEE, 1051–1057.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smarandache（2012）F. Smarandache. 2012. 中性学质量与不确定模型：信息融合的应用。在*2012年第十五届国际信息融合会议*。IEEE，1051–1057。
- en: Smarandache and Dezert (2009) F. Smarandache and J. Dezert. 2009. *Advances
    and applications of DSmT for information fusion-Collected works-volume 3*. American
    Research Press.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smarandache 和 Dezert（2009）F. Smarandache 和 J. Dezert. 2009. *DSmT在信息融合中的进展与应用-收录作品-第3卷*。美国研究出版社。
- en: Smarandache et al. (2012) F. Smarandache, D. Han, and A. Martin. 2012. Comparative
    study of contradiction measures in the theory of belief functions. In *2012 15th
    International Conference on Information Fusion*. 271–277.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smarandache等人（2012）F. Smarandache, D. Han, 和 A. Martin. 2012. 信念函数理论中矛盾度量的比较研究。在*2012年第十五届国际信息融合会议*。271–277。
- en: Smets and Kennes (1994) P. Smets and R. Kennes. 1994. The transferable belief
    model. *Artificial Intelligence* 66 (1994), 191–234.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smets 和 Kennes（1994）P. Smets 和 R. Kennes. 1994. 可转移信念模型。*人工智能* 66（1994），191–234。
- en: Smith (2012) B. Smith. 2012. Ontology. In *The furniture of the world*. Brill,
    47–68.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith（2012）B. Smith. 2012. 本体论。在*世界的家具*。布里尔，47–68。
- en: 'Smolensky (1986) P. Smolensky. 1986. *Information processing in dynamical systems:
    Foundations of harmony theory*. Technical Report. Colorado Univ at Boulder Dept
    of Computer Science.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smolensky（1986）P. Smolensky. 1986. *动态系统中的信息处理：和谐理论的基础*。技术报告。科罗拉多大学博尔德分校计算机科学系。
- en: Sofman et al. (2006) B. Sofman, E. Lin, A. Bagnell, J. Cole, N. Vandapel, and
    A. Stentz. 2006. Improving robot navigation through self-supervised online learning.
    *Journal of Field Robotics* 23, 11-12 (2006), 1059–1075.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sofman 等（2006）B. Sofman, E. Lin, A. Bagnell, J. Cole, N. Vandapel 和 A. Stentz.
    2006. 通过自监督在线学习改善机器人导航。*野外机器人学杂志* 23, 11-12（2006），1059–1075。
- en: Soua et al. (2016) R. Soua, A. Koesdwiady, and F. Karray. 2016. Big-data-generated
    traffic flow prediction using deep learning and dempster-shafer theory. In *2016
    International Joint Conference on Neural Networks (IJCNN)*. IEEE, 3195–3202.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soua 等（2016）R. Soua, A. Koesdwiady 和 F. Karray. 2016. 基于深度学习和 Dempster-Shafer
    理论的海量数据生成交通流预测。在*2016 国际神经网络联合会议（IJCNN）*。IEEE，3195–3202。
- en: Stanford Center for Biomedical Informatics Research (BMIR) (2019) Stanford Center
    for Biomedical Informatics Research (BMIR). 2019. *Protégé*. Standford University.
    [https://www.w3.org/OWL/](https://www.w3.org/OWL/)
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福生物医学信息研究中心（BMIR）（2019）斯坦福生物医学信息研究中心（BMIR）。2019. *Protégé*。斯坦福大学。 [https://www.w3.org/OWL/](https://www.w3.org/OWL/)
- en: Tian et al. (2020) Z. Tian, W. Shi, Z. Tan, J. Qiu, Y. Sun, F. Jiang, and Y.
    Liu. 2020. Deep learning and dempster-shafer theory based insider threat detection.
    *Mobile Networks and Applications* (2020), 1–10.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等（2020）Z. Tian, W. Shi, Z. Tan, J. Qiu, Y. Sun, F. Jiang 和 Y. Liu. 2020.
    基于深度学习和 Dempster-Shafer 理论的内部威胁检测。*移动网络与应用*（2020），1–10。
- en: 'Tipping (2003) M. Tipping. 2003. Bayesian inference: An introduction to principles
    and practice in machine learning. In *Summer School on Machine Learning*. Springer,
    41–62.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tipping（2003）M. Tipping. 2003. 贝叶斯推断：机器学习原理与实践导论。在*机器学习暑期学校*。施普林格，41–62。
- en: Tong et al. (2021) Z. Tong, P. Xu, and T. Denoeux. 2021. An evidential classifier
    based on Dempster-Shafer theory and deep learning. *Neurocomputing* 450 (2021),
    275–293.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tong 等（2021）Z. Tong, P. Xu 和 T. Denoeux. 2021. 基于 Dempster-Shafer 理论和深度学习的证据分类器。*神经计算*
    450（2021），275–293。
- en: Tripathi and Govindaraju (2007) S. Tripathi and R. Govindaraju. 2007. On selection
    of kernel parametes in relevance vector machines for hydrologic applications.
    *Stochastic Environmental Research and Risk Assessment* 21, 6 (2007), 747–764.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tripathi 和 Govindaraju（2007）S. Tripathi 和 R. Govindaraju. 2007. 水文应用中相关向量机的核参数选择。*随机环境研究与风险评估*
    21, 6（2007），747–764。
- en: Tversky and Kahneman (1985) A. Tversky and D. Kahneman. 1985. The framing of
    decisions and the psychology of choice. In *Environmental Impact Assessment, Technology
    Assessment, and Risk Analysis*. 107–129.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tversky 和 Kahneman（1985）A. Tversky 和 D. Kahneman. 1985. 决策的框架和选择心理学。在*环境影响评估、技术评估与风险分析*。107–129。
- en: Ulmer (2021) Dennis Ulmer. 2021. A Survey on Evidential Deep Learning For Single-Pass
    Uncertainty Estimation. *arXiv preprint arXiv:2110.03051* (2021).
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ulmer（2021）Dennis Ulmer. 2021. 关于单次传递不确定性估计的证据深度学习调查。*arXiv 预印本 arXiv:2110.03051*（2021）。
- en: Utkin (2015) L. Utkin. 2015. The imprecise Dirichlet model as a basis for a
    new boosting classification algorithm. *Neurocomputing* 151 (2015), 1374–1383.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Utkin（2015）L. Utkin. 2015. 不精确 Dirichlet 模型作为新型提升分类算法的基础。*神经计算* 151（2015），1374–1383。
- en: van Asselt (2000) M. van Asselt. 2000. *Perspectives on uncertainty and risk*.
    Dordrecht, 407–417. [https://doi.org/10.1007/978-94-017-2583-5_10](https://doi.org/10.1007/978-94-017-2583-5_10)
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Asselt（2000）M. van Asselt. 2000. *不确定性和风险的视角*。多德雷赫特，407–417。 [https://doi.org/10.1007/978-94-017-2583-5_10](https://doi.org/10.1007/978-94-017-2583-5_10)
- en: Virani et al. (2020) N. Virani, N. Iyer, and Z. Yang. 2020. Justification-based
    reliability in machine learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 34. 6078–6085.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Virani 等（2020）N. Virani, N. Iyer 和 Z. Yang. 2020. 机器学习中的基于理由的可靠性。在*AAAI 人工智能会议论文集*，第
    34 卷。6078–6085。
- en: 'Walker et al. (2003) W. Walker, P. Harremoës, J. Rotmans, J. Van Der, M. Van
    Asselt, P. Janssen, and M. Krayer. 2003. Defining uncertainty: a conceptual basis
    for uncertainty management in model-based decision support. *Integrated Assessment*
    4, 1 (2003), 5–17.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walker et al. (2003) W. Walker, P. Harremoës, J. Rotmans, J. Van Der, M. Van
    Asselt, P. Janssen 和 M. Krayer. 2003. 不确定性的定义：基于模型的决策支持中的不确定性管理的概念基础。*综合评估* 4,
    1 (2003), 5–17.
- en: 'Walley (1996) P. Walley. 1996. Inferences from multinomial data: learning about
    a bag of marbles. *Journal of the Royal Statistical Society: Series B (Methodological)*
    58, 1 (1996), 3–34.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walley (1996) P. Walley. 1996. 从多项数据推理：了解一袋弹珠。*皇家统计学会杂志：B系列（方法论）* 58, 1 (1996),
    3–34.
- en: Wang and Yeung (2020) Hao Wang and Dit-Yan Yeung. 2020. A survey on Bayesian
    deep learning. *ACM Computing Surveys (CSUR)* 53, 5 (2020), 1–37.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Yeung (2020) Hao Wang 和 Dit-Yan Yeung. 2020. 关于贝叶斯深度学习的综述。*ACM计算机调查（CSUR）*
    53, 5 (2020), 1–37.
- en: Wang and Sun (2007) J. Wang and H. Sun. 2007. Inverse Problemin DSmT and Its
    Applications in Trust Management. In *The First International Symposium on Data,
    Privacy, and E-Commerce(ISDPE’07)*. Chengdu, China, 424–428.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Sun (2007) J. Wang 和 H. Sun. 2007. DSmT中的逆问题及其在信任管理中的应用。在*第一届数据、隐私与电子商务国际研讨会（ISDPE’07）*。中国成都,
    424–428.
- en: Wang et al. (2016) Y. Wang, Z. Wu, and J. Zhang. 2016. Damaged fingerprint classification
    by Deep Learning with fuzzy feature points. In *2016 9th international congress
    on image and signal processing, BioMedical engineering and informatics (CISP-BMEI)*.
    IEEE, 280–285.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016) Y. Wang, Z. Wu 和 J. Zhang. 2016. 通过深度学习与模糊特征点对损坏指纹进行分类。在*2016年第9届国际图像与信号处理、生物医学工程与信息学大会（CISP-BMEI）*。IEEE,
    280–285.
- en: Wu and Mendel (2007) D. Wu and J. Mendel. 2007. Uncertainty measures for interval
    type-2 fuzzy sets. *Information Sciences* 177, 23 (2007), 5378–5393.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu and Mendel (2007) D. Wu 和 J. Mendel. 2007. 区间类型-2模糊集的不确定性度量。*信息科学* 177, 23
    (2007), 5378–5393.
- en: 'Wu and Mendel (2009) D. Wu and J. Mendel. 2009. A comparative study of ranking
    methods, similarity measures and uncertainty measures for interval type-2 fuzzy
    sets. *Information Sciences—Informatics and Computer Science, Intelligent Systems,
    Applications: An International Journal* 179, 8 (2009), 1169–1192.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu and Mendel (2009) D. Wu 和 J. Mendel. 2009. 针对区间类型-2模糊集的排序方法、相似性度量和不确定性度量的比较研究。*信息科学—信息学与计算机科学，智能系统，应用：国际期刊*
    179, 8 (2009), 1169–1192.
- en: Xu et al. (2021) L. Xu, X. Zhang, X. Zhao, X. Chen, F. Chen, and J. D. Choi.
    2021. Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation.
    *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing
    (EMNLP)* (2021).
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021) L. Xu, X. Zhang, X. Zhao, X. Chen, F. Chen, 和 J. D. Choi. 2021.
    通过自学习与不确定性估计提升跨语言迁移。*2021年自然语言处理经验方法会议论文集（EMNLP）* (2021).
- en: Yager (2013) R. Yager. 2013. Pythagorean membership grades in multicriteria
    decision making. *IEEE Transactions on Fuzzy Systems* 22, 4 (2013), 958–965.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yager (2013) R. Yager. 2013. 在多准则决策中使用的毕达哥拉斯成员等级。*IEEE模糊系统汇刊* 22, 4 (2013),
    958–965.
- en: Yang et al. (2009) S. Yang, S. Ding, and W. Chu. 2009. Trustworthy software
    evaluation using utility based evidence theory. *Jisuanji Yanjiu Yu Fazhan/Computer
    Research and Development* 46 (Jul. 2009), 1152–1159.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2009) S. Yang, S. Ding 和 W. Chu. 2009. 使用基于效用的证据理论进行可信软件评估。*计算机研究与发展*
    46 (2009年7月), 1152–1159.
- en: Yasdi (1995) R. Yasdi. 1995. Combining rough sets learning-and neural learning-method
    to deal with uncertain and imprecise information. *Neurocomputing* 7, 1 (1995),
    61–84.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yasdi (1995) R. Yasdi. 1995. 结合粗糙集学习与神经网络学习方法处理不确定和不精确信息。*神经计算* 7, 1 (1995),
    61–84.
- en: Zadeh (1968) L. Zadeh. 1968. Probability measures of fuzzy events. *Journal
    of mathematical analysis and applications* 23, 2 (1968), 421–427.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zadeh (1968) L. Zadeh. 1968. 模糊事件的概率度量。*数学分析与应用杂志* 23, 2 (1968), 421–427.
- en: Zadeh (1975a) L. Zadeh. 1975a. The concept of a linguistic variable and its
    application to approximate reasoning—I. *Information sciences* 8, 3 (1975), 199–249.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zadeh (1975a) L. Zadeh. 1975a. 语言变量的概念及其在近似推理中的应用—I。*信息科学* 8, 3 (1975), 199–249.
- en: Zadeh (1975b) L. Zadeh. 1975b. Fuzzy logic and approximate reasoning. *Synthese*
    30, 3-4 (1975), 407–428.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zadeh (1975b) L. Zadeh. 1975b. 模糊逻辑与近似推理。*Synthese* 30, 3-4 (1975), 407–428.
- en: Zadeh (1965) L-A. Zadeh. 1965. Fuzzy sets. *Information and Control* 8, 3 (1965),
    338–353.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zadeh (1965) L-A. Zadeh. 1965. 模糊集。*信息与控制* 8, 3 (1965), 338–353.
- en: Zadeh (1983) L. A. Zadeh. 1983. The role of fuzzy logic in the management of
    uncertainty in expert systems. *Fuzzy Sets and Systems* 11, 1-3 (Mar 1983), 197–198.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zadeh（1983）L.A. Zadeh. 1983. 模糊逻辑在专家系统中管理不确定性的作用。*模糊集与系统* 11, 1-3 (1983年3月)，197–198。
- en: Zhai and Mendel (2011) D. Zhai and J. M. Mendel. 2011. Uncertainty measures
    for general type-2 fuzzy sets. *Information Sciences* 181, 3 (2011), 503–518.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 和 Mendel（2011）D. Zhai 和 J.M. Mendel. 2011. 通用类型2模糊集的不确定性度量。*信息科学* 181,
    3 (2011)，503–518。
- en: Zhang (2007) D. Zhang. 2007. Integrated methods of rough sets and neural network
    and their applications in pattern recognition. *Hunan University, Hunan* (2007).
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang（2007）D. Zhang. 2007. 粗糙集与神经网络的综合方法及其在模式识别中的应用。*湖南大学，湖南* (2007)。
- en: Zhang and Wang (2006) D. Zhang and Y. Wang. 2006. Fuzzy-rough neural network
    and its application to vowel recognition. *Control and Decision* 21, 2 (2006),
    221.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Wang（2006）D. Zhang 和 Y. Wang. 2006. 模糊粗糙神经网络及其在元音识别中的应用。*控制与决策* 21,
    2 (2006)，221。
- en: Zhang et al. (2018) Nan Zhang, Shifei Ding, Jian Zhang, and Yu Xue. 2018. An
    overview on restricted Boltzmann machines. *Neurocomputing* 275 (2018), 1186–1199.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2018）Nan Zhang, Shifei Ding, Jian Zhang, 和 Yu Xue. 2018. 受限玻尔兹曼机综述。*神经计算*
    275 (2018)，1186–1199。
- en: Zhang et al. (2020a) Q. Zhang, W. Hu, Z. Liu, and J. Tan. 2020a. TBM performance
    prediction with Bayesian optimization and automated machine learning. *Tunnelling
    and Underground Space Technology* 103 (2020), 103493.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2020a）Q. Zhang, W. Hu, Z. Liu, 和 J. Tan. 2020a. 基于贝叶斯优化和自动化机器学习的TBM性能预测。*隧道与地下空间技术*
    103 (2020)，103493。
- en: Zhang et al. (2014) R. Zhang, F. Shen, and J. Zhao. 2014. A model with fuzzy
    granulation and deep belief networks for exchange rate forecasting. In *2014 International
    Joint Conference on Neural Networks (IJCNN)*. IEEE, 366–373.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2014）R. Zhang, F. Shen, 和 J. Zhao. 2014. 基于模糊粒化和深度信念网络的汇率预测模型。发表于 *2014年国际神经网络联合会议（IJCNN）*。IEEE，366–373。
- en: Zhang et al. (2020b) Z. Zhang, W. Jiang, J. Geng, X. Deng, and X. Li. 2020b.
    Fault diagnosis based on non-negative sparse constrained deep neural networks
    and Dempster–Shafer theory. *IEEE Access* 8 (2020), 18182–18195.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2020b）Z. Zhang, W. Jiang, J. Geng, X. Deng, 和 X. Li. 2020b. 基于非负稀疏约束深度神经网络和Dempster–Shafer理论的故障诊断。*IEEE
    Access* 8 (2020)，18182–18195。
- en: Zhao et al. (2018a) X. Zhao, F. Chen, and J. H. Cho. 2018a. Deep learning based
    scalable inference of uncertain opinions. In *2018 International Conference on
    Data Mining (ICDM)*. 807–816.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2018a）X. Zhao, F. Chen, 和 J.H. Cho. 2018a. 基于深度学习的不确定意见的可扩展推理。发表于 *2018年国际数据挖掘会议（ICDM）*。807–816。
- en: Zhao et al. (2018b) X. Zhao, F. Chen, and J. H. Cho. 2018b. Deep learning for
    predicting dynamic uncertain opinions in network data. In *2018 International
    Conference on Big Data (Big Data)*. 1150–1155.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2018b）X. Zhao, F. Chen, 和 J.H. Cho. 2018b. 用于预测网络数据中动态不确定意见的深度学习。发表于
    *2018年国际大数据会议（Big Data）*。1150–1155。
- en: Zhao et al. (2020) X. Zhao, F. Chen, S. Hu, and J. H. Cho. 2020. Uncertainty
    aware semi-supervised learning on graph data. *Advances in Neural Information
    Processing Systems* 33, 12827–12836.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2020）X. Zhao, F. Chen, S. Hu, 和 J.H. Cho. 2020. 基于不确定性的图数据半监督学习。*神经信息处理系统进展*
    33, 12827–12836。
- en: Zhao et al. (2019a) X. Zhao, S. Hu, J.H. Cho, and F. Chen. 2019a. Uncertainty-based
    decision making using deep reinforcement learning. In *2019 22th International
    Conference on Information Fusion (FUSION)*. IEEE, Ottawa, CA, 1–8.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2019a）X. Zhao, S. Hu, J.H. Cho, 和 F. Chen. 2019a. 基于不确定性的决策制定使用深度强化学习。发表于
    *2019年第22届国际信息融合会议（FUSION）*。IEEE，渥太华，加拿大，1–8。
- en: Zhao et al. (2019b) X. Zhao, Y. Ou, L.M. Kaplan, F. Chen, and J. H. Cho. 2019b.
    Quantifying classification uncertainty using regularized evidential neural networks.
    *AAAI 2019 Fall Symposium Series, Artificial Intelligence in Government and Public
    Sector* (2019).
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2019b）X. Zhao, Y. Ou, L.M. Kaplan, F. Chen, 和 J.H. Cho. 2019b. 使用正则化证据神经网络量化分类不确定性。*AAAI
    2019秋季研讨会系列，政府与公共部门中的人工智能* (2019)。
- en: 'Zhao et al. (2022) X. Zhao, X. Zhang, W. Cheng, W. Yu, Y. Chen, H. Chen, and
    F. Chen. 2022. SEED: Sound Event Early Detection via evidential uncertainty. In
    *2022 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人（2022）X. Zhao, X. Zhang, W. Cheng, W. Yu, Y. Chen, H. Chen, 和 F. Chen.
    2022. SEED: 通过证据不确定性进行声音事件早期检测。发表于 *2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。'
- en: Zheng et al. (2017) YJ. Zheng, SY. Chen, Y. Xue, and JY. Xue. 2017. A Pythagorean-type
    fuzzy deep denoising autoencoder for industrial accident early warning. *IEEE
    Transactions on Fuzzy Systems* 25, 6 (2017), 1561–1575.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2017）YJ. Zheng, SY. Chen, Y. Xue, 和 JY. Xue. 2017. 一种用于工业事故预警的毕达哥拉斯型模糊深度去噪自编码器。*IEEE
    模糊系统汇刊* 25, 6 (2017), 1561–1575。
- en: Zheng et al. (2016) YJ. Zheng, WG. Sheng, XM. Sun, and SY. Chen. 2016. Airline
    passenger profiling based on fuzzy deep machine learning. *IEEE Transactions on
    Neural Networks and Learning Systems* 28, 12 (2016), 2911–2923.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2016）YJ. Zheng, WG. Sheng, XM. Sun, 和 SY. Chen. 2016. 基于模糊深度机器学习的航空乘客画像。*IEEE
    神经网络与学习系统汇刊* 28, 12 (2016), 2911–2923。
- en: Zimmermann (2000) HJ. Zimmermann. 2000. An application-oriented view of modeling
    uncertainty. *European Journal of Operational Research* 122, 2 (2000), 190–198.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zimmermann（2000）HJ. Zimmermann. 2000. 面向应用的建模不确定性视角。*欧洲运筹学杂志* 122, 2 (2000),
    190–198。
