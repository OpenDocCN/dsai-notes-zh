- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:37:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2308.01222] Calibration in Deep Learning: A Survey of the State-of-the-Art'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2308.01222] 深度学习中的校准：前沿技术综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.01222](https://ar5iv.labs.arxiv.org/html/2308.01222)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.01222](https://ar5iv.labs.arxiv.org/html/2308.01222)
- en: 'Calibration in Deep Learning: A Survey of the State-of-the-Art'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的校准：前沿技术综述
- en: \nameCheng Wang \emailcwngam@amazon.com
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \nameCheng Wang \emailcwngam@amazon.com
- en: \addrAmazon
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \addrAmazon
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Calibrating deep neural models plays an important role in building reliable,
    robust AI systems in safety-critical applications. Recent work has shown that
    modern neural networks that possess high predictive capability are poorly calibrated
    and produce unreliable model predictions. Though deep learning models achieve
    remarkable performance on various benchmarks, the study of model calibration and
    reliability is relatively underexplored. Ideal deep models should have not only
    high predictive performance but also be well calibrated. There have been some
    recent advances in calibrating deep models. In this survey, we review the state-of-the-art
    calibration methods and their principles for performing model calibration. First,
    we start with the definition of model calibration and explain the root causes
    of model miscalibration. Then we introduce the key metrics that can measure this
    aspect. It is followed by a summary of calibration methods that we roughly classify
    into four categories: post-hoc calibration, regularization methods, uncertainty
    estimation, and composition methods. We also cover recent advancements in calibrating
    large models, particularly large language models (LLMs). Finally, we discuss some
    open issues, challenges, and potential directions.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 校准深度神经模型在构建可靠、稳健的安全关键应用的人工智能系统中扮演了重要角色。最近的研究表明，具有高预测能力的现代神经网络校准效果不佳，产生不可靠的模型预测。尽管深度学习模型在各种基准测试中表现出色，但对模型校准和可靠性的研究相对较少。理想的深度模型不仅应具备高预测性能，还应进行良好的校准。最近在深度模型校准方面取得了一些进展。在这项综述中，我们回顾了最先进的校准方法及其原理。首先，我们从模型校准的定义开始，解释模型误校准的根本原因。接着介绍可以衡量这一方面的关键指标。然后，我们总结了校准方法，并将其大致分为四类：后处理校准、正则化方法、不确定性估计和组合方法。我们还涵盖了在校准大型模型，特别是大型语言模型（LLMs）方面的最新进展。最后，我们讨论了一些开放问题、挑战和潜在的研究方向。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep Neural Networks (DNNs) have been showing promising predictive power in
    many domains such as computer vision (?), speech recognition (?) and natural language
    processing (?). Nowadays, deep neural network models are frequently being deployed
    into real-world systems. However, recent work (?) pointed out that those highly
    accurate, negative-log-likelihood (NLL) trained deep neural networks are poorly
    calibrated (?), i.e., the model predicted class probabilities do not faithfully
    estimate the true correctness likelihood and lead to overconfident and underconfident
    predictions. Deploying uncalibrated models into real-world systems is at high
    risk, particularly for safety-critical applications such as medical diagnosis (?),
    autonomous driving (?) and finance decision-making.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）在计算机视觉（?）、语音识别（?）和自然语言处理（?）等多个领域显示出有前景的预测能力。如今，深度神经网络模型正频繁应用于现实世界系统。然而，最近的研究（?）指出，这些经过负对数似然（NLL）训练的高精度深度神经网络校准效果不佳（?），即模型预测的类别概率并不能真实估计正确性概率，从而导致过度自信或不自信的预测。将未校准的模型应用于现实世界系统存在高风险，尤其是对于医疗诊断（?）、自动驾驶（?）和金融决策等安全关键应用。
- en: '![Refer to caption](img/ab4617b7a77767dc2c6379979330f87a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ab4617b7a77767dc2c6379979330f87a.png)'
- en: (a)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/0f4d399aafe4b369eaf3360fa51bba93.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0f4d399aafe4b369eaf3360fa51bba93.png)'
- en: (b)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/7675ac46ef71c1a087675f2e3e8fdc2c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7675ac46ef71c1a087675f2e3e8fdc2c.png)'
- en: (c)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: '![Refer to caption](img/da3cc00b8f15552d563c3542b30935c0.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/da3cc00b8f15552d563c3542b30935c0.png)'
- en: (d)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (d)
- en: 'Figure 1: An illustration of model calibration. Uncalibrated model (left) that
    trained with standard cross-entropy loss and calibrated model (right) that trained
    with focal loss ($\gamma=5$), have similar predictive performance on a binary
    classification task (accuracy is 83.8% and 83.4% respectively), but the right
    one is better calibrated. Top: The reliability diagram plots with 10 bins. The
    diagonal dash line presents perfect calibration (on a specific bin, confidence
    is equal to accuracy.), Expected Calibration Error (ECE) and Maximum Calibration
    Error (MCE) are used to measure model calibration performance. Bottom: the posterior
    distribution of the two models on 1000 samples, the one from calibrated model
    is better distributed.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：模型校准的示意图。未经校准的模型（左）使用标准交叉熵损失训练，经过校准的模型（右）使用焦点损失（$\gamma=5$）训练，在二分类任务上预测性能相似（准确率分别为83.8%和83.4%），但右侧模型的校准效果更好。顶部：可靠性图绘制了10个区间。对角虚线表示完美校准（在特定区间，置信度等于准确率），使用期望校准误差（ECE）和最大校准误差（MCE）来衡量模型校准性能。底部：两个模型在1000个样本上的后验分布，校准模型的分布更好。
- en: 'Calibrating deep models is a procedure for preventing the model’s posterior
    distribution from being over- or under-confident. Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Calibration in Deep Learning: A Survey of the State-of-the-Art")
    gives an illustration of calibrating a binary classification model. It is noted
    that (1) a highly predictive model can be poorly calibrated, this is exhibited
    by high calibration errors; (2) The deep models tend to be primarily overconfident,
    this is shown by a spiking posterior distribution.  (?, ?, ?). Model overconfidence
    is usually caused by over-parameterized networks, a lack of appropriate regularization,
    limited data, imbalanced label distributions, etc. In the past years, different
    streams of work have been proposed to calibrate models. In this survey, we review,
    classify, and discuss recent calibration methods and their advantages and limitations.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '校准深度模型是防止模型的后验分布过于自信或不自信的过程。图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Calibration
    in Deep Learning: A Survey of the State-of-the-Art")展示了二分类模型的校准。值得注意的是（1）一个高预测能力的模型可能会校准不佳，这表现在高校准误差上；（2）深度模型往往主要表现为过度自信，这通过后验分布的峰值体现出来。模型的过度自信通常由过度参数化的网络、缺乏适当的正则化、数据有限、标签分布不均等原因造成。近年来，已经提出了不同的工作流来校准模型。在这项调查中，我们回顾、分类并讨论了最近的校准方法及其优缺点。'
- en: 1.0.1 Scope and Focus
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.0.1 范围和重点
- en: 'This survey particularly focuses on calibration methods for classification
    problems. There have been some related surveys on this topic (?) or on the highly
    relevant topic–uncertainty estimation. For example model calibration has been
    briefly discussed in the uncerainty estimation surveys (?, ?). Our survey distinguishes
    itself from those surveys in several aspects:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查特别关注分类问题的校准方法。关于这一主题（?）或高度相关的主题——不确定性估计，已有一些相关的调查。例如，模型校准在不确定性估计调查中有简要讨论（?,
    ?）。我们的调查在多个方面与这些调查有所不同：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey reviews the state-of-the-art calibration methods and focuses mostly
    on the ones proposed in the last five years. This includes such as kernel-based
    methods, differentiable calibration proxy, and meta-learning-based approaches.
    Those are rarely discussed in previous surveys.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查回顾了最先进的校准方法，主要集中于过去五年提出的方法，包括基于核的方法、可微分校准代理和元学习方法。这些方法在之前的调查中很少讨论。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey tries to explain calibration principles of each method via the discussion
    of the conceptual relationships among over-parameterization, over-fitting, and
    over-confidence. We systematically categorize those methods into post-hoc, regularization
    (explicit, implicit and differentiable calibration proxy), uncertainty estimation,
    and composition methods.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查通过讨论过度参数化、过拟合和过度自信之间的概念关系，试图解释每种方法的校准原理。我们系统地将这些方法分类为后处理方法、正则化（显式、隐式和可微分校准代理）、不确定性估计和组合方法。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey also discusses the methods of calibrating large pre-trained models,
    particularly large language models (LLMs), where calibrating LLMs in zero-shot
    inference has been attracting increasing interest from AI communities.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查还讨论了大型预训练模型的校准方法，特别是大型语言模型（LLMs），其中零样本推断中LLMs的校准正受到AI社区越来越多的关注。
- en: 'The rest of this survey is structured as follows. In section [2](#S2 "2 Preliminaries
    and Backgrounds ‣ Calibration in Deep Learning: A Survey of the State-of-the-Art")
    we introduce the definition of model calibration and discuss the reasons cause
    miscalibration; Section [2.3](#S2.SS3 "2.3 Calibration Measurements ‣ 2 Preliminaries
    and Backgrounds ‣ Calibration in Deep Learning: A Survey of the State-of-the-Art")
    lists the mainstream calibration metrics that are used for measuring model calibration.
    Section [3](#S3 "3 Calibration Methods ‣ Calibration in Deep Learning: A Survey
    of the State-of-the-Art") review, classify and discuss recently proposed calibration
    methods. Section [4](#S4 "4 Conclusion and Future Work ‣ Calibration in Deep Learning:
    A Survey of the State-of-the-Art") discusses future directions and concludes this
    survey.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文余下部分结构如下。在第 [2](#S2 "2 前言与背景 ‣ 深度学习中的校准：最先进技术的调查") 节中，我们介绍了模型校准的定义并讨论了导致校准误差的原因；第
    [2.3](#S2.SS3 "2.3 校准测量 ‣ 2 前言与背景 ‣ 深度学习中的校准：最先进技术的调查") 节列出了用于测量模型校准的主流校准指标。第
    [3](#S3 "3 校准方法 ‣ 深度学习中的校准：最先进技术的调查") 节回顾、分类并讨论了最近提出的校准方法。第 [4](#S4 "4 结论与未来工作
    ‣ 深度学习中的校准：最先进技术的调查") 节讨论了未来的方向并总结了本次调查。
- en: 2 Preliminaries and Backgrounds
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言与背景
- en: This section describes the definition of model calibration and the aspects cause
    miscalibration.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了模型校准的定义以及导致校准误差的方面。
- en: 2.1 Definitions
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 定义
- en: 'In classification tasks, for a given input variable $X$ and a categorical variable
    $Y\in\{1,2,...,k\}$, assume we have a neural network model $f$ which maps input
    variable $\mathbf{x}$ to a categorical distribution $p=\{p_{1},...,p_{k}\}$ over
    $k$ classes $\{y_{1},...,y_{k}\}$: $f:D\rightarrow\Delta$, where $\Delta$ is the
    $k-1$ dimensional standard probability simplex and $\Delta=\{p\in[0,1]^{k}|\sum_{i=1}^{k}p_{i}=1\}$.
    Calibration measures the degree of the match between predicted probability $p$
    and the true correctness likelihood. A model $f$ is perfectly calibrated on if
    and only if:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类任务中，对于给定的输入变量 $X$ 和一个分类变量 $Y\in\{1,2,...,k\}$，假设我们有一个神经网络模型 $f$，它将输入变量 $\mathbf{x}$
    映射到 $k$ 类 $\{y_{1},...,y_{k}\}$ 的分类分布 $p=\{p_{1},...,p_{k}\}$ 上：$f:D\rightarrow\Delta$，其中
    $\Delta$ 是 $k-1$ 维标准概率单纯形，且 $\Delta=\{p\in[0,1]^{k}|\sum_{i=1}^{k}p_{i}=1\}$。校准测量预测概率
    $p$ 与真实正确性可能性的匹配程度。模型 $f$ 只有在且仅在以下情况下才被认为是完全校准的：
- en: '|  | $\mathbb{P}(Y=y_{i}&#124;f(X)=p)=p_{i}$ |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{P}(Y=y_{i}&#124;f(X)=p)=p_{i}$ |  | (1) |'
- en: where $p$ is true correctness likelihood. Intuitively, for all input pairs $\{x,y\}\in
    D$, if model predicts $p_{i}=0.8$, we expect that 80% have $y_{i}$ as label.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p$ 是真实的正确性可能性。直观地，对于所有输入对 $\{x,y\}\in D$，如果模型预测 $p_{i}=0.8$，我们期望 80% 的样本标记为
    $y_{i}$。
- en: 'Instead of probability distribution, the argmax calibration (?, ?, ?) takes
    only the maximum probability into consideration:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与概率分布不同，argmax 校准（？，？，？）仅考虑最大概率：
- en: '|  | $\mathbb{P}(Y\in\arg\max(p)&#124;\max(f(X))=p^{*})=p^{*}$ |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{P}(Y\in\arg\max(p)&#124;\max(f(X))=p^{*})=p^{*}$ |  | (2) |'
- en: In reality, it is difficult to obtain perfect calibration, any deviation from
    it represents miscalibration.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，获得完美校准是困难的，任何偏离都表示校准误差。
- en: 2.2 Aspects Impact Model Calibration
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 影响模型校准的方面
- en: It has been obversed that some recent changes in modern neural networks are
    responsible for model miscalibration (?, ?, ?). The underlying general cause is
    that modern neural networks’ high capacity makes them vulnerable to miscalibration,
    which is tightly correlated to the concepts of over-parameter, overfitting and
    over-confidence.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到一些现代神经网络的最近变化导致了模型校准误差（？，？，？）。根本原因是现代神经网络的高容量使其容易出现校准误差，这与过参数化、过拟合和过度自信的概念紧密相关。
- en: 2.2.1 Model Size
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 模型规模
- en: While increasing the depth and width of neural networks helps to obtain highly
    predictive power, it also negatively increases calibration errors. Empirical evidence
    has shown that this poor calibration is linked to overfitting on the negative
    log-likelihood (NLL) (?, ?). The over-parameteriaztion is one of the main causes
    of overfitting. Concretely, along with the standard NLL-based model training,
    when classification error is minimized, keeping training will further push the
    model to minimize NLL on the training data, i.e., push the predicted softmax probability
    distribution as close as possible to the ground-truth distribution (which is usually
    one-hot). Model overfitting starts by exhibiting increased test NLL, and then
    the model becomes overconfident (?). For more recent large models, this trend
    is negligible for in-distribution data and reverses under distribution shift (?).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然增加神经网络的深度和宽度有助于获得高度的预测能力，但它也会负面地增加校准误差。实证证据表明，这种不良校准与对负对数似然 (NLL) (?) 的过拟合有关
    (?,?,?)。过度参数化是过拟合的主要原因之一。具体来说，除了标准的 NLL 基础模型训练外，当最小化分类错误时，持续训练将进一步推动模型最小化训练数据上的
    NLL，即将预测的 softmax 概率分布尽可能接近于真实分布（通常是 one-hot）。模型过拟合首先表现为测试 NLL 的增加，然后模型变得过于自信
    (?)。对于更近期的大模型，这种趋势在分布内数据中几乎可以忽略不计，在分布变化下则会反转 (?). '
- en: 2.2.2 Regularization
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '2.2.2 正则化 '
- en: Regularization can effectively prevent overfitting when model capacity increases.
    Recent trends suggest that explicit L2 regularization may not be necessary to
    achieve a highly accurate model when applying batch normalization (?) or dropout (?),
    but Guo et al. (?) emprically demonstrated model tends to be less calibrated without
    using L2 regularization. There have been more recent regularization techniques
     (?, ?, ?, ?) been proposed to improve model calibration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '正则化可以有效防止模型容量增加时的过拟合。最近的趋势表明，当应用批归一化 (?) 或 dropout (?) 时，显式 L2 正则化可能不是实现高精度模型的必要条件，但
    Guo 等人 (?) 实证展示了在不使用 L2 正则化时模型往往较少被校准。最近已经提出了更多改进模型校准的正则化技术 (?, ?, ?, ?)。 '
- en: 2.2.3 Data Issues
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '2.2.3 数据问题 '
- en: Another important aspect that impacts calibration is data quantity (e.g., scale,
    volume, diversity, etc.) and quality (relevance, consistency, completeness, etc.).
    Training high-capacity (over-parameterized) networks with scarce data can easily
    cause overfitting and an overconfident model. Data augmentation is an effective
    way to alleviate this phenomenon and brings implicit calibration effects (?, ?).
    The recent pretrain-finetune paradigm offers the possibility of reducing overfitting
    caused by limited and noisy data (?). Another challenge is data imbalance, where
    models overfit to the majority classes, thus making overconfident predictions
    for the majority classes. Focal loss (?, ?)  has recently demonstrated promising
    performance in calibrating deep models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个影响校准的重要方面是数据的数量（例如规模、体积、多样性等）和质量（相关性、一致性、完整性等）。用稀缺数据训练高容量（过度参数化）网络很容易导致过拟合和过于自信的模型。数据增强是缓解这一现象的有效方法，并带来隐式的校准效果
    (?, ?)。最近的预训练-微调范式提供了减少由有限和噪声数据引起的过拟合的可能性 (?). 另一个挑战是数据不平衡，在这种情况下，模型对多数类别过拟合，从而对多数类别做出过于自信的预测。焦点损失
    (?, ?) 最近在校准深度模型方面表现出了良好的效果。 '
- en: 2.3 Calibration Measurements
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.3 校准测量 '
- en: Exact calibration measurement with finite data samples is impossible given that
    the confidence $p$ is a continuous variable (?). There are some popular metrics
    that approximate model calibration error by grouping $N$ predictions into $M$
    interval bins $\{b_{1},b_{2},...,b_{M}\}$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '由于置信度 $p$ 是连续变量 (?)，用有限的数据样本进行精确的校准测量是不可能的。有一些流行的指标通过将 $N$ 个预测分组到 $M$ 个区间 $\{b_{1},b_{2},...,b_{M}\}$
    来近似模型校准误差。 '
- en: 2.3.1 Expected Calibration Error (ECE)
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '2.3.1 预期校准误差 (ECE) '
- en: ECE (?) is a scalar summary statistic of calibration. It is a weighted average
    of the difference between model accuracy and confidence across $M$ bins,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'ECE (?) 是校准的标量摘要统计量。它是模型准确度和置信度在 $M$ 个区间的加权平均差异， '
- en: '|  | $\displaystyle\textsc{ECE}=\frac{1}{N}\sum_{m=1}^{M}\left&#124;b_{m}\right&#124;&#124;\textrm{acc}(b_{m})-\textrm{conf}(b_{m})&#124;$
    |  | (3) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textsc{ECE}=\frac{1}{N}\sum_{m=1}^{M}\left|b_{m}\right|\left|\textrm{acc}(b_{m})-\textrm{conf}(b_{m})\right|$
    |  | (3) | '
- en: where $N$ is the total number of samples. $\left|b_{m}\right|$ is the number
    of samples in bin $b_{m}$, and
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是样本总数。$\left|b_{m}\right|$ 是 bin $b_{m}$ 中的样本数量，
- en: '|  | $\displaystyle\textrm{acc}(b_{m})=\frac{1}{\left&#124;b_{m}\right&#124;}\sum_{i\in
    B_{m}}\mathbf{1}(\hat{y}_{i}=y_{i}),~{}\textrm{conf}(b_{m})=\frac{1}{\left&#124;b_{m}\right&#124;}\sum_{i\in
    B_{m}}p_{i}.$ |  | (4) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textrm{acc}(b_{m})=\frac{1}{\left|b_{m}\right|}\sum_{i\in
    B_{m}}\mathbf{1}(\hat{y}_{i}=y_{i}),~{}\textrm{conf}(b_{m})=\frac{1}{\left|b_{m}\right|}\sum_{i\in
    B_{m}}p_{i}.$ |  | (4) |'
- en: 2.3.2 Maximum Calibration Error (MCE)
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 最大校准误差 (MCE)
- en: MCE (?) measures the worst-case deviation between accuracy and confidence,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MCE (?) 测量准确率和置信度之间的最坏情况偏差，
- en: '|  | $\textsc{MCE}=\max_{m\in\{1,\dots,M\}}&#124;\textrm{acc}(b_{m})-\textrm{conf}(b_{m})&#124;.$
    |  | (5) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{MCE}=\max_{m\in\{1,\dots,M\}}|\textrm{acc}(b_{m})-\textrm{conf}(b_{m})|.$
    |  | (5) |'
- en: and is particularly important in high-risk applications where reliable confidence
    measures are absolutely necessary.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在高风险应用中尤其重要，因为可靠的置信度测量绝对必要。
- en: 2.3.3 Classwise ECE (CECE)
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 类别 ECE (CECE)
- en: Classwise ECE (?) can be seen as the macro-averaged ECE. It extends the bin-based
    ECE to measure calibration across all the possible $K$ classes. In practice, predictions
    are binned separately for each class, and the calibration error is computed at
    the level of individual class-bins and then averaged. The metric can be formulated
    as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 ECE (?) 可以看作是宏观平均的 ECE。它扩展了基于分箱的 ECE，用于衡量所有可能的 $K$ 类别的校准。在实际操作中，预测会分别针对每个类别进行分箱，然后在单个类别-分箱层面计算校准误差，并取平均。该度量可以被表述为
- en: '|  | $\textsc{CECE}=\sum_{m=1}^{M}\sum_{c=1}^{K}\frac{&#124;b_{m,c}&#124;}{NK}&#124;\textrm{acc}_{c}(b_{m,c})-\textrm{conf}_{c}(b_{m,c})&#124;$
    |  | (6) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{CECE}=\sum_{m=1}^{M}\sum_{c=1}^{K}\frac{|b_{m,c}|}{NK}|\textrm{acc}_{c}(b_{m,c})-\textrm{conf}_{c}(b_{m,c})|$
    |  | (6) |'
- en: where $b_{m,c}$ represents a single bin for class $c$. In this formulation,
    $\textrm{acc}_{c}(b_{m,c})$ represents average binary accuracy for class $c$ over
    bin $b_{m,c}$ and $\textrm{conf}_{c}(b_{m,c})$ represents average confidence for
    class $c$ over bin $b_{m,c}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{m,c}$ 代表类别 $c$ 的单个分箱。在这种表述中，$\textrm{acc}_{c}(b_{m,c})$ 代表类别 $c$ 在分箱
    $b_{m,c}$ 上的平均二元准确率，$\textrm{conf}_{c}(b_{m,c})$ 代表类别 $c$ 在分箱 $b_{m,c}$ 上的平均置信度。
- en: 2.3.4 Adaptive ECE (AECE)
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 自适应 ECE (AECE)
- en: The binning mechanism in the aforementioned metrics can introduce bias; the
    pre-defined bin size determines the number of samples in each bin. Adaptive ECE (?)
    introduces a new binning strategy to use an adaptive scheme that spaces the bin
    intervals to ensure each bin hosts an equal number of samples.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述度量中的分箱机制可能会引入偏差；预定义的分箱大小决定了每个分箱中的样本数量。自适应 ECE (?) 引入了一种新的分箱策略，使用自适应方案调整分箱间隔，以确保每个分箱中包含相同数量的样本。
- en: '|  | $\textsc{AECE}=\sum_{r=1}^{R}\sum_{c=1}^{K}\frac{1}{RK}&#124;\textrm{acc}_{c}(b_{n,c})-\textrm{conf}_{c}(b_{n,c})&#124;$
    |  | (7) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{AECE}=\sum_{r=1}^{R}\sum_{c=1}^{K}\frac{1}{RK}|\textrm{acc}_{c}(b_{n,c})-\textrm{conf}_{c}(b_{n,c})|$
    |  | (7) |'
- en: Where $r\in[1,R]$ is defined by the $[N/R]$-th index of the sorted and threshold
    predictions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r\in[1,R]$ 由排序后的预测和阈值预测的 $[N/R]$-th 索引定义。
- en: For a perfectly calibrated classifier, those calibration erros should equal
    $0$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个完美校准的分类器，这些校准误差应该等于 $0$。
- en: 2.3.5 Reliability Diagram
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5 可靠性图
- en: 'Besides the metrics that provide a scalar summary on calibration, reliability
    diagrams (as shown in [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Calibration in Deep
    Learning: A Survey of the State-of-the-Art")) visualize whether a model is over-
    or under-confident on bins by grouping predictions into bins according to their
    prediction probability. The diagonal line in Figure 1 presents perfect calibration:
    $\textrm{acc}(b_{m})=\textrm{conf}(b_{m}),\forall m$, the red bar presents the
    gap to perfect calibration.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '除了提供标量总结的校准度量，可靠性图（如[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Calibration in
    Deep Learning: A Survey of the State-of-the-Art")中所示）通过根据预测概率将预测分组到分箱中来可视化模型是否对分箱过于自信或不足。图
    1 中的对角线表示完美的校准：$\textrm{acc}(b_{m})=\textrm{conf}(b_{m}),\forall m$，红色条表示与完美校准的差距。'
- en: '| Methods | Categorization | Measurement |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 分类 | 测量 |'
- en: '| --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Dirichlet Calibration (?) | Post-hoc | CECE, NLL |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Dirichlet 校准 (?) | 后处理 | CECE, NLL |'
- en: '| ATS (?) | Post-hoc | ECE, NLL |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ATS (?) | 后处理 | ECE, NLL |'
- en: '| BTS (?) | Post-hoc | ECE |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| BTS (?) | 后处理 | ECE |'
- en: '| LTS (?) | Post-hoc | ECE,MCE,AECE, SCE |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| LTS (?) | 后处理 | ECE, MCE, AECE, SCE |'
- en: '| Focal Loss  (?) | Reg.(Implicit) | ECE, NLL |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 焦点损失 (?) | 正则化（隐式） | ECE, NLL |'
- en: '| FLSD (?) | Reg.(Implicit) | ECE,MCE,AECE, CECE, NLL |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| FLSD (?) | 正则化（隐式） | ECE, MCE, AECE, CECE, NLL |'
- en: '| MMCE (?) | Reg.(Proxy) | ECE, Brier, NLL |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| MMCE (?) | 正则化（代理） | ECE, Brier, NLL |'
- en: '| Meta-Calibration (?) | Reg.(Proxy) | ECE |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 元校准 (?) | 正则化（代理） | ECE |'
- en: '| Label Smoothing (?) | Reg.(Aug.) | ECE |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 标签平滑 (?) | 正则化（增强） | ECE |'
- en: '| Mix-Up (?) | Reg.(Aug.) | ECE |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Mix-Up (?) | 正则化（增强） | ECE |'
- en: '| Mix-n-Match (?) | Composition | ECE |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Mix-n-Match (?) | 组成 | ECE |'
- en: '| TS+MC dropout (?) | Composition | ECE, UCE |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| TS+MC dropout (?) | 组成 | ECE, UCE |'
- en: 'Table 1: The state-of-the-art calibration methods and their categorization.
    The regularization methods are further divided into explicit, implicit regularization
    and trainable calibration proxies. Uncertainty estimation and quantification methods
    are excluded, please refer to surveys (?, ?).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：最先进的校准方法及其分类。正则化方法进一步分为显式正则化、隐式正则化和可训练的校准代理。不包括不确定性估计和量化方法，请参考调查文献 (？，？)。
- en: 3 Calibration Methods
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 校准方法
- en: 'In this section, we categorize the state-of-the-art calibration methods into
    post-hoc methods, regularization methods, implicit calibration methods, and uncertainty
    estimation methods. Besides, we discuss compositional methods that combine different
    calibration methods. Table [1](#S2.T1 "Table 1 ‣ 2.3.5 Reliability Diagram ‣ 2.3
    Calibration Measurements ‣ 2 Preliminaries and Backgrounds ‣ Calibration in Deep
    Learning: A Survey of the State-of-the-Art") summarizes those methods.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将最先进的校准方法分类为事后方法、正则化方法、隐式校准方法和不确定性估计方法。此外，我们讨论了将不同校准方法结合起来的组合方法。表 [1](#S2.T1
    "表 1 ‣ 2.3.5 可靠性图 ‣ 2.3 校准测量 ‣ 2 前言和背景 ‣ 深度学习中的校准：最先进的综述") 总结了这些方法。
- en: 3.1 Post-hoc Methods
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 事后校准方法
- en: Post-hoc calibration methods aim to calibrate a model after training. Those
    include non-parametric calibration histogram binning(?), isotonic regression (?)
    and parametric methods such as Bayesian binning into quantiles (BBQ) and Platt
    scaling (?). Out of them, Platt scaling (?) based approaches are more popular
    due to their low complexity and efficiency. This includes Temperature Scaling
    (TS), attended TS (?).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 事后校准方法旨在训练后校准模型。这些方法包括非参数校准直方图分箱（？）、同调回归 (?) 和诸如贝叶斯分箱入量化（BBQ）和 Platt 缩放 (?)
    的参数方法。在这些方法中，基于 Platt 缩放 (?) 的方法由于其低复杂度和高效性而更受欢迎。这包括温度缩放（TS）、注意 TS (?)。
- en: 3.1.1 Temperature Scaling
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 温度缩放
- en: 'Temperature scaling (TS) is a single-parameter extension of Platt scaling (?)
    and the most recent addition to the offering of post-hoc methods. It uses a temperature
    parameter $\tau$ to calibrate the softmax probability:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 温度缩放（TS）是 Platt 缩放 (?) 的单参数扩展，也是最近添加到事后方法中的一种。它使用温度参数 $\tau$ 来校准 softmax 概率：
- en: '|  | $p_{i}=\frac{\exp(g_{i}/\tau)}{\sum_{j=1}^{k}\exp(g_{j}/\tau)},~{}~{}i\in[1\dots
    k].$ |  | (8) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}=\frac{\exp(g_{i}/\tau)}{\sum_{j=1}^{k}\exp(g_{j}/\tau)},~{}~{}i\in[1\dots
    k].$ |  | (8) |'
- en: 'where $\tau>0$ for all classes is used as a scaling factor to soften model
    predicted probability, it controls the model’s confidence by adjusting the sharpness
    of distribution so that the model prediction is not too certain (overconfident)
    or too uncertain (underconfident). The optimal temperature value is obtained by
    minimizing negative log likelihood loss (NLL) on the validation dataset.:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau>0$ 对所有类别作为缩放因子来软化模型预测的概率，它通过调整分布的锐利度来控制模型的信心，使模型预测不至于过于确定（过于自信）或过于不确定（不自信）。通过最小化验证数据集上的负对数似然损失（NLL）来获得最佳温度值。：
- en: '|  | $\tau^{\ast}=\arg\min_{\tau}\left(-\sum_{i=1}^{N}\log(\textsc{softmax}(g_{i},\tau))&nbsp;\right)$
    |  | (9) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tau^{\ast}=\arg\min_{\tau}\left(-\sum_{i=1}^{N}\log(\textsc{softmax}(g_{i},\tau))\right)$
    |  | (9) |'
- en: TS simplifies matrix (vector) scaling (?) where class-wise $\tau$ is considered
    as a single parameter, and offers good calibration while maintaining minimum computational
    complexity (?, ?).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: TS 简化了矩阵（向量）缩放 (?)，其中类级别的 $\tau$ 被视为一个单一参数，并在保持最低计算复杂度的同时提供良好的校准 (？，？)。
- en: 3.1.2 Temperature Scaling Extensions
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 温度缩放扩展
- en: 'The goal of post-hoc calibration on a validation dataset is to learn a calibration
    map (also known as the canonical calibration function of probablity (?)) which
    transforms uncalibrated probabilities into calibrated ones. Many TS extensions
    aim to find a proper calibration map.   Kull et al. (?) proposed Dirichlet calibration
    which assumes probability distributions are parameterized Dirichlet distributions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 事后校准的目标是在验证数据集上学习一个校准映射（也称为概率的规范校准函数 (?))，将未经校准的概率转换为校准后的概率。许多 TS 扩展旨在找到一个合适的校准映射。
    Kull 等人 (?) 提出了 Dirichlet 校准，假设概率分布是参数化的 Dirichlet 分布：
- en: '|  | $p(x)&#124;y=j\sim Dirichlet(\alpha^{j})$ |  | (10) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(x)\mid y=j\sim Dirichlet(\alpha^{j})$ |  | (10) |'
- en: 'where $\alpha^{j}=\{\alpha^{j}_{1},\alpha^{j}_{2},...,\alpha^{j}_{k}\}$ are
    Dirichlet parameters for $j$-th class. The proposed Dirichlet calibration map
    family coincides with the Beta calibration family (?). Besides, it provides uniqueness
    and interpretability as compared to generic canonical parametrization.    (?)
    suggested that TS has difficulties in finding optimal $\tau$ when the validation
    set has a limited number of samples. They proposed attended temperature scaling
    (ATS) to alleviate this issue by increasing the number of samples in validation
    set. The key idea is to gather samples from each class distribution. Let’s assume
    $p(y|x)$ is the predicted probability. ATS first divides the validation set into
    $K$ subsets with $p(y=k|x),k\in[1,K]$, which allows to add more $y\neq k$ samples
    using the Bayeisan Theorem (?) as the selection criterion:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha^{j}=\{\alpha^{j}_{1},\alpha^{j}_{2},...,\alpha^{j}_{k}\}$ 是第 $j$
    类的 Dirichlet 参数。提出的 Dirichlet 校准映射族与 Beta 校准族相吻合（?）。此外，与通用标准化参数化相比，它提供了唯一性和可解释性。
    (?) 指出，当验证集样本数量有限时，TS 在寻找最佳 $\tau$ 时存在困难。他们提出了加权温度缩放（ATS），通过增加验证集中样本的数量来缓解这个问题。关键思想是从每个类的分布中收集样本。假设
    $p(y|x)$ 是预测的概率。ATS 首先将验证集划分为 $K$ 个子集，包含 $p(y=k|x),k\in[1,K]$，这使得可以利用 Bayeisan
    定理（?）作为选择标准，添加更多 $y\neq k$ 的样本：
- en: '|  | $p(x,y=k)=\frac{p(y=k&#124;x)}{p(y\neq k&#124;x)}p(x,y\neq k)$ |  | (11)
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(x,y=k)=\frac{p(y=k|x)}{p(y\neq k|x)}p(x,y\neq k)$ |  | (11) |'
- en: It indicates that ATS selects the samples with $y\neq k$ which are more probable
    to belong to $p(x,y=k)$.   Bin-wise TS (BTS) (?) was proposed to extend TS to
    multiple equal size bins by using the confidence interval-based binning method.
    Together with data augmentation, BTS showed superior performance as compared to
    TS. Local TS (LTS) (?) extends TS to multi-label semantic segmentation and makes
    it adaptive to local image changes. A local scaling factor is learned for each
    pixel or voxel.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明 ATS 选择了那些更可能属于 $p(x,y=k)$ 的 $y\neq k$ 样本。 Bin-wise TS（BTS）(?) 被提出以通过使用基于置信区间的分箱方法将
    TS 扩展到多个相等大小的分箱。结合数据增强，BTS 相比于 TS 显示了优越的性能。局部 TS（LTS）(?) 将 TS 扩展到多标签语义分割，并使其适应局部图像变化。为每个像素或体素学习一个局部缩放因子。
- en: '|  | $\mathbb{Q}(x,\tau_{i}(x))=max_{l\in L}\textsc{Softmax}(\frac{g_{i}(x)}{\tau_{i}(x)})^{(l)}$
    |  | (12) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{Q}(x,\tau_{i}(x))=max_{l\in L}\textsc{Softmax}(\frac{g_{i}(x)}{\tau_{i}(x)})^{(l)}$
    |  | (12) |'
- en: where $l$ is the class index, $g_{i}(x)$ is the logit of input at location $x$,
    and The $\tau_{i}(x)$ is the location-dependent temperature.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l$ 是类别索引，$g_{i}(x)$ 是位置 $x$ 的 logit，而 $\tau_{i}(x)$ 是位置依赖的温度。
- en: 'Remarks: Although using a single global hyper-parameter, TS remains a popular
    approach due to its effectiveness and accuracy-preserving (?). Post-hoc calibration
    usually works well without a huge amount of validation data and is thus data efficient.
    The calibration procedure is decoupled from training and does not introduce training
    complexity. On the other hand, post-hoc calibration is less expressive and suffers
    difficulty in approximating the canonical calibration function when data is not
    enough (?).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：尽管使用单一的全局超参数，TS 仍然是一个受欢迎的方法，因为它的有效性和保持准确性的能力（?）。后处理校准通常在没有大量验证数据的情况下效果良好，因此数据效率较高。校准过程与训练解耦，不会引入训练复杂性。另一方面，后处理校准的表达能力较差，并且在数据不足时难以近似标准校准函数（?）。
- en: 3.2 Regularization Method
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 正则化方法
- en: Regularization is important to prevent neural network models from overfitting.
    In this section, we discuss some representative work in this direction that either
    explicitly or implicitly regularizes modern neural networks to have better calibration.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化对于防止神经网络模型过拟合非常重要。在本节中，我们讨论了一些代表性的工作，这些工作显式或隐式地对现代神经网络进行正则化，以实现更好的校准。
- en: 3.2.1 Explicit Regularization
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 显式正则化
- en: 'The typical (explicit) way to add regularization term (or penalty) $L_{reg}$
    to standard loss objective (e.g., negative log likelihood):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 将正则化项（或惩罚项）$L_{reg}$ 添加到标准损失目标（例如，负对数似然）的典型（显式）方法：
- en: '|  | $\mathcal{L}(\theta)=-\sum\log p(y&#124;x)+\alpha L_{reg}.$ |  | (13)
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\theta)=-\sum\log p(y|x)+\alpha L_{reg}.$ |  | (13) |'
- en: 'where $\alpha$ controls the importance of penalty in regularizing weight $\theta$.
    L2 regularization has been widely used in training modern neural networks and
    showed its effectiveness in model calibration ](?). Entropy regularization (?):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 控制正则化权重 $\theta$ 的惩罚重要性。L2 正则化在训练现代神经网络中得到了广泛应用，并在模型校准中显示了其有效性（?）。熵正则化（?）：
- en: '|  | $\mathcal{L}(\theta)=-\sum\log p(y&#124;x)-\alpha H(p(y&#124;x)).$ |  |
    (14) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\theta)=-\sum\log p(y\mid x)-\alpha H(p(y\mid x)).$ |  |
    (14) |'
- en: directly penalizes predictive distributions that have low entropy, prevents
    these peaked distributions, and ensures better model generalization.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直接惩罚熵低的预测分布，防止这些尖峰分布，并确保更好的模型泛化。
- en: '3.2.2 Implicit Regularization: Focal Loss and Its Extensions'
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 隐式正则化：Focal Loss 及其扩展
- en: 'Focal loss (?) was originally proposed to alleviate the class imbalance issue
    in object detection:$\mathcal{L}_{f}=-\sum_{i=1}^{N}(1-p_{i})^{\gamma}\log p_{i}$
    where $\gamma$ is a hyperparameter. It has been recently shown that focal loss
    can be interpreted as a trade-off between minimizing Kullback–Leibler (KL) divergence
    and maximizing the entropy, depending on $\gamma$ (?):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Focal loss（？）最初提出是为了解决目标检测中的类别不平衡问题：$\mathcal{L}_{f}=-\sum_{i=1}^{N}(1-p_{i})^{\gamma}\log
    p_{i}$ 其中 $\gamma$ 是一个超参数。最近有研究表明，focal loss 可以被解释为最小化 Kullback–Leibler（KL）散度和最大化熵之间的权衡，这取决于
    $\gamma$（？）：
- en: '|  | $\mathcal{L}_{f}\geq\textsc{KL}(q\parallel p)+\mathbb{H}(q)-\gamma\mathbb{H}(p)$
    |  | (15) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{f}\geq\textsc{KL}(q\parallel p)+\mathbb{H}(q)-\gamma\mathbb{H}(p)$
    |  | (15) |'
- en: The first term pushes the model to learn a probability $p$ to have a high value
    (confident, as close as possible to ground-truth label distribution, which is
    usually one-hot representation). The second term is constant. The last term regularizes
    probability to not be too high (overconfident).   Mukhoti et al. (?) empirically
    observed that $\gamma$ plays an important role in implicitly regularizing entropy
    and weights. However, finding an appropriate $\gamma$ is challenging for all samples
    in the datasets. Thus, they proposed sample-dependent scheduled $\gamma$ (FLSD)
    based on the Lambert-W function (?). They have shown that scheduling $\gamma$
    values according to different confidence ranges helps to improve model calibration
    on both in-domain and out-of-domain (OOD) data. More recent extensions focus on
    computer vision have been proposed, including in margin-based label smoothing
    (MbLS) (?) and multi-class difference of confidence and accuracy (MDCA) (?).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项推动模型学习一个概率 $p$ 使其具有高值（尽可能接近真实标签分布，即通常的一热编码表示）。第二个项是常数。最后一项对概率进行正则化，以避免过高（过度自信）。Mukhoti
    等人（？）经验观察到 $\gamma$ 在隐式正则化熵和权重中发挥了重要作用。然而，找到一个适当的 $\gamma$ 对于数据集中的所有样本都是具有挑战性的。因此，他们提出了基于
    Lambert-W 函数（？）的样本依赖调度 $\gamma$（FLSD）。他们已经表明，根据不同的置信区间调度 $\gamma$ 值有助于改善模型在领域内和领域外（OOD）数据上的校准。最近的扩展主要集中在计算机视觉中，包括基于边际的标签平滑（MbLS）（？）和多类置信度与准确度差异（MDCA）（？）。
- en: 'Differentiable Calibration Proxy: Recall that the aforementioned methods use
    a penalty term (either explicitly or implicitly) to improve model calibration
    on dataset $D$. There is a rising direction that directly optimizes objective
    function by using calibration errors (CE) as differentiable proxy  (?, ?) to standard
    loss:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可微校准代理：回顾上述方法，利用惩罚项（无论是显式还是隐式）来改善数据集 $D$ 上的模型校准。有一种新兴的方向直接通过使用校准误差（CE）作为可微代理来优化目标函数（？，？）以替代标准损失：
- en: '|  | $\arg\min_{\theta}L_{standard}(D,\theta)+L_{calibration}(D,\theta)$ |  |
    (16) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\arg\min_{\theta}L_{standard}(D,\theta)+L_{calibration}(D,\theta)$ |  |
    (16) |'
- en: 'The focus of this line of work is to find differentiable approximations to
    calibration errors.   Kumar et al.  (?) proposed a kernel-based approach to explicitly
    calibrate models in training phrase called Maximum Mean Calibration Error (MMCE),
    which is differentiable and can be optimized using batch stochastic gradient algorithms.
    They cast the calibration error to be differentiable by defining an integral probability
    measure over functions from a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$
    induced by a universal kernel $k(\cdot,\cdot)$ and cannonical feature map $\phi:[0,1]\rightarrow\mathcal{H}$:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这一研究方向的重点是找到校准误差的可微近似。Kumar 等人（？）提出了一种基于核的方法来显式校准训练阶段的模型，称为最大均值校准误差（MMCE），它是可微的，并且可以使用批量随机梯度算法进行优化。他们通过定义在由通用核
    $k(\cdot,\cdot)$ 和规范特征映射 $\phi:[0,1]\rightarrow\mathcal{H}$ 诱导的再现核希尔伯特空间（RKHS）
    $\mathcal{H}$ 上的积分概率测度，将校准误差转化为可微的：
- en: '|  | $\textsc{MMCE}(P(r,c))=\left\&#124;E_{(r,c)\sim P}[(c-r)\phi(r)]\right\&#124;_{\mathcal{H}}$
    |  | (17) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{MMCE}(P(r,c))=\left\|E_{(r,c)\sim P}[(c-r)\phi(r)]\right\|_{\mathcal{H}}$
    |  | (17) |'
- en: where $r,c$ represent confidence and correctness scores, respectively, and $P(r,c)$
    denotes the distribution over $r,c$ of the predicted probability $P(y|x)$.   An
    approach that combines meta-learning and a differentiable calibration proxy was
    proposed by (?). The authors developed a differentiable ECE(DECE) and used it
    as learning objective for a meta network. The meta network takes representations
    from original backbone network and outputs a unit-wise L2 weight decay coefficient
    $\omega$ for backbone network. The DECE is optimized against calibration metrics
    with validation set but attached to standard cross-entropy (CE) loss.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r,c$ 分别表示置信度和正确性分数，$P(r,c)$ 表示预测概率 $P(y|x)$ 在 $r,c$ 上的分布。一个结合元学习和可微校准代理的方法由 (?)
    提出。作者们开发了一个可微 ECE（DECE）并将其用作元网络的学习目标。元网络从原始骨干网络中获取表示，并为骨干网络输出逐单位 L2 权重衰减系数 $\omega$。DECE
    是针对校准指标与验证集进行优化的，但附加于标准交叉熵（CE）损失。
- en: '|  | $\displaystyle\omega^{*}$ | $\displaystyle=\arg\min_{\omega}\mathcal{L}_{DECE}^{val}(f_{c}^{*}\circ
    f_{\theta}^{*}(\omega))$ |  | (18) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\omega^{*}$ | $\displaystyle=\arg\min_{\omega}\mathcal{L}_{DECE}^{val}(f_{c}^{*}\circ
    f_{\theta}^{*}(\omega))$ |  | (18) |'
- en: '|  | $\displaystyle f_{c}^{*},f_{\theta}^{*}(\omega)$ | $\displaystyle=\arg\min_{f_{c},f_{\theta}(\omega)}(\mathcal{L}_{CE}^{train}(f_{c}\circ
    f_{\theta}(\omega)+\omega\left\&#124;f_{c}\right\&#124;^{2})$ |  | (19) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{c}^{*},f_{\theta}^{*}(\omega)$ | $\displaystyle=\arg\min_{f_{c},f_{\theta}(\omega)}(\mathcal{L}_{CE}^{train}(f_{c}\circ
    f_{\theta}(\omega)+\omega\left\&#124;f_{c}\right\&#124;^{2})$ |  | (19) |'
- en: where $f_{c}$ is classification layer and $f_{\theta}$ is the feature extractor.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f_{c}$ 是分类层，$f_{\theta}$ 是特征提取器。
- en: 'Remarks: Regularization methods, as compared to post-hoc methods, can directly
    output a well-calibrated model without additional steps. The increased complexity
    is different for different methods; for instance, focal loss can be seen as an
    implicit regularization and does not introduce observable additional computational
    complexity. Kernel-based and meta-network regularization add additional computations
    depending on the designed kernel methods and meta-networks.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：与后处理方法相比，正则化方法可以直接输出一个经过良好校准的模型，而无需额外步骤。不同方法的复杂度增加程度不同；例如，焦点损失可以看作是一种隐式正则化，并且没有引入明显的额外计算复杂度。基于核的方法和元网络正则化根据设计的核方法和元网络增加额外的计算。
- en: 3.3 Data Augmentation
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据增强
- en: 'This line of work is highly relevant to regularization methods, instead of
    directly adding penalty terms to optimization objectives. Those studies try to
    augment data or add noise to training samples to mitigate model miscalibration.
    Label smoothing (?) and mixup (?) are popular approaches in this line.   Label
    smoothing (?) soften hard labels with an introduced smoothing parameter $\alpha$
    in the standard loss function (e.g., cross-entropy):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作与正则化方法高度相关，而不是直接将惩罚项添加到优化目标中。这些研究尝试通过扩充数据或向训练样本中添加噪声来减轻模型的误校准。标签平滑 (?) 和
    mixup (?) 是这一领域的热门方法。标签平滑 (?) 通过在标准损失函数（例如交叉熵）中引入平滑参数 $\alpha$ 来软化硬标签：
- en: '|  | $\mathcal{L}_{c}=-\sum_{k=1}^{K}y_{k}^{s}\log p_{i},~{}~{}y_{k}^{s}=y_{k}(1-\alpha)+\alpha/K$
    |  | (20) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{c}=-\sum_{k=1}^{K}y_{k}^{s}\log p_{i},~{}~{}y_{k}^{s}=y_{k}(1-\alpha)+\alpha/K$
    |  | (20) |'
- en: where $y_{k}$ is the soft label for $k$-th category. It is shown that LS encourages
    the differences between the logits of the correct class and the logits of the
    incorrect class to be a constant depending on $\alpha$. The confidence penalty
    can be recovered by assuming the prior label distribution is a uniform distribution
    $u$ and reversing the direction of the KL divergence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{k}$ 是第 $k$ 类的软标签。研究表明，标签平滑鼓励正确类别的对数几率与错误类别的对数几率之间的差异成为一个依赖于 $\alpha$
    的常数。通过假设先验标签分布为均匀分布 $u$ 并反转 KL 散度的方向，可以恢复置信度惩罚。
- en: '|  | $\mathcal{L}(\theta)=-\sum\log p(y&#124;x)-\textsc{KL}(u\parallel p(y&#124;x)).$
    |  | (21) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\theta)=-\sum\log p(y&#124;x)-\textsc{KL}(u\parallel p(y&#124;x)).$
    |  | (21) |'
- en: 'Mixup training (?) is another work in this line of exploration. It studies
    the effectiveness of mixup (?) with respect to model calibration (?). Mixup generates
    synthetic samples during training by convexly combining random pairs of inputs
    and labels as well. To mix up two random samples $(x_{i},y_{i})$ and $(x_{j},y_{j})$,
    the following rules are used:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Mixup 训练 (?) 是这一探索方向中的另一个工作。它研究了 mixup (?) 在模型校准 (?) 方面的有效性。Mixup 在训练过程中通过凸性地组合随机对的输入和标签来生成合成样本。要混合两个随机样本
    $(x_{i},y_{i})$ 和 $(x_{j},y_{j})$，使用以下规则：
- en: '|  | $\displaystyle\bar{x}$ | $\displaystyle=\alpha x_{i}+(1-\alpha)x_{j}$
    |  | (22) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bar{x}$ | $\displaystyle=\alpha x_{i}+(1-\alpha)x_{j}$
    |  | (22) |'
- en: '|  | $\displaystyle\bar{y}$ | $\displaystyle=\alpha y_{i}+(1-\alpha)y_{j}$
    |  | (23) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bar{y}$ | $\displaystyle=\alpha y_{i}+(1-\alpha)y_{j}$
    |  | (23) |'
- en: where $(\bar{x}_{i},\bar{y}_{i})$ is the virtual feature-target of original
    pairs. The authors observed that mixup-trained models are better calibrated and
    less prone to overconfidence in prediction on out-of-distribution and noise data.
    It is pointed out that mixing features alone does not bring calibration benefits;
    label smoothing can significantly improve calibration when used together with
    mixing features.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\bar{x}_{i},\bar{y}_{i})$ 是原始对的虚拟特征-目标。作者观察到，mixup训练的模型在处理分布外和噪声数据的预测时更好地进行了校准，不易过于自信。指出仅混合特征并不会带来校准益处；标签平滑在与混合特征一起使用时可以显著改善校准。
- en: 'Remarks: This line of work combats overfitting by data augmentation in hidden
    space. This improves not only model generalization but also calibration. Those
    methods don’t significantly increase network complexity but usually require more
    training time due to more generated or synthesized training samples.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：这项工作通过在隐藏空间中进行数据增强来对抗过拟合。这不仅提高了模型的泛化能力，还改善了校准。这些方法不会显著增加网络复杂性，但通常由于生成或合成的训练样本更多，因此需要更多的训练时间。
- en: 3.4 Uncertainty Estimation
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 不确定性估计
- en: This line of work aims to alleviate model miscalibration by injecting randomness.The
    popular methods are (1) Bayesian neural networks (?, ?), (2) ensembles (?), (3)
    Monte Carlo(MC) dropout (?) and (4) Gumbel-softmax (?) based approaches (?, ?).
    The former three sub-categorgies have been discussed in recent surveys (?, ?)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作旨在通过引入随机性来缓解模型的不匹配。流行的方法包括（1）贝叶斯神经网络（？，？），（2）集成方法（？），（3）蒙特卡洛（MC）丢弃法（？）和（4）Gumbel-softmax（？）基于的方法（？，？）。前面三种子类别已在最近的综述中讨论过（？，？）。
- en: '![Refer to caption](img/5481e77ba65de2bec45840cb6bfd1433.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5481e77ba65de2bec45840cb6bfd1433.png)'
- en: 'Figure 2: The methods of uncertainty estimation (?). (a) Bayesian neural network;
    (b) MC dropout; (c) Ensembles; (d) Gumbel-Softmax trick.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不确定性估计的方法（？）。(a) 贝叶斯神经网络；(b) MC丢弃法；(c) 集成方法；(d) Gumbel-Softmax技巧。
- en: '![Refer to caption](img/28810b445b30f6f930c69202e9d2094d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/28810b445b30f6f930c69202e9d2094d.png)'
- en: (a)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/00a5a7c0299758d954b7eccf0d6a8c86.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/00a5a7c0299758d954b7eccf0d6a8c86.png)'
- en: (b)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/66d3b04aed5220197bded5939c9b2b78.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66d3b04aed5220197bded5939c9b2b78.png)'
- en: (c)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 3: The reliability diagrams for a model trained on CIFAR100 with different
    bin numbers (left to right: 20, 50, 100). The diagonal dash presents perfect calibration,
    the red bar presents the gap to perfect calibration on each bin.The calibration
    error is sensitive to increasing bin numbers.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在CIFAR100上训练的模型的可靠性图，使用不同的bin数量（从左到右：20、50、100）。对角线虚线表示完美的校准，红色条表示每个bin与完美校准之间的差距。校准误差对bin数量的增加非常敏感。
- en: 3.4.1 Bayesian Neural Network
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 贝叶斯神经网络
- en: 'Given a learning objective is to minimize negative log likelihood, $\mathcal{L}=-\frac{1}{N}\sum_{i}^{N}\log
    p(y_{i}|x_{i},w)$. The probability distribution is obtained by Softmax function
    as:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的学习目标是最小化负对数似然，$\mathcal{L}=-\frac{1}{N}\sum_{i}^{N}\log p(y_{i}\mid x_{i},w)$。概率分布由Softmax函数获得，如下所示：
- en: '|  | $\displaystyle p(y_{i}=m&#124;x_{i},w)=\frac{\exp(f_{m}(x_{i},w))}{\sum_{k\in
    M}\exp(f_{k}(x_{i},w)}.$ |  | (24) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(y_{i}=m\mid x_{i},w)=\frac{\exp(f_{m}(x_{i},w))}{\sum_{k\in
    M}\exp(f_{k}(x_{i},w))}$ |  | (24) |'
- en: 'In the inference phase, given a test sample $x^{*}$, the predictive probability
    $y^{*}$ is computed by:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在推断阶段，给定一个测试样本 $x^{*}$，预测概率 $y^{*}$ 由以下公式计算：
- en: '|  | $\displaystyle p(y^{*}&#124;x^{*},D)=\int p(y^{*}&#124;x^{*},w)p(w&#124;D)dw$
    |  | (25) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(y^{*}\mid x^{*},D)=\int p(y^{*}\mid x^{*},w)p(w\mid D)dw$
    |  | (25) |'
- en: 'As posterior $p(w|D)$ is intractable, we perform approximation by minimizing
    the Kullback-Leilber (KL) distance. This can also be treated as the maximization
    of ELBO:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于后验 $p(w\mid D)$ 是不可处理的，我们通过最小化Kullback-Leibler（KL）距离来进行近似。这也可以视为对ELBO的最大化：
- en: '|  | $\displaystyle\mathcal{L_{\theta}}=\int q_{\theta}(w)p(Y&#124;X,w)dw-\textsc{KL}[q_{\theta}(w)\parallel
    p(w)]$ |  | (26) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L_{\theta}}=\int q_{\theta}(w)p(Y\mid X,w)dw-\textsc{KL}[q_{\theta}(w)\parallel
    p(w)]$ |  | (26) |'
- en: where $\theta$ are the variational parameters. With the re-parametrization trick (?),
    a differentiable mini-batched Monte Carlo (MC) estimator can be obtained.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 是变分参数。通过重新参数化技巧（？），可以获得一个可微分的迷你批量蒙特卡洛（MC）估计器。
- en: 'The uncertainty estimation can be done by performing $T$ inference runs and
    averaging predictions: $p(y*|x*)=\frac{1}{T}\sum_{t=1}^{T}p_{w_{t}}(y^{*}|x^{*},w_{t}).$'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性估计可以通过执行 $T$ 次推断运行并平均预测来完成：$p(y*|x*)=\frac{1}{T}\sum_{t=1}^{T}p_{w_{t}}(y^{*}|x^{*},w_{t})$。
- en: 3.4.2 MC Dropout, Ensembles and Gumbel-Softmax
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 MC Dropout、集合和 Gumbel-Softmax
- en: 'By following the above-mentioned strategy, MC-dropout (?), ensembles (?) and
    Gumbel-softmax sampling (?, ?) introduce randomness in different ways, as illustrated
    in Figure [2](#S3.F2 "Figure 2 ‣ 3.4 Uncertainty Estimation ‣ 3 Calibration Methods
    ‣ Calibration in Deep Learning: A Survey of the State-of-the-Art"). Then the $T$
    in equation ([29](#S3.E29 "In 3.6.2 Large Language Models (LLMs) ‣ 3.6 Calibrating
    Pre-trained Large Models ‣ 3 Calibration Methods ‣ Calibration in Deep Learning:
    A Survey of the State-of-the-Art")) corresponds to the number of sets of mask
    vectors from Bernoulli distribution $\{r^{t}\}_{t=1}^{T}$ in MC-dropout, or the
    number of randomly trained models in Ensembles, which potentially leads to different
    sets of learned parameters $\omega=\{\omega_{1},...,\omega_{t}\}$, or the number
    of sets of sampled attention distribution from Gumbel distribution $\{g^{t}\}_{t=1}^{T}$.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '按照上述策略，MC-dropout（？）、集合（？）和 Gumbel-softmax 采样（？，？）以不同的方式引入随机性，如图 [2](#S3.F2
    "Figure 2 ‣ 3.4 Uncertainty Estimation ‣ 3 Calibration Methods ‣ Calibration in
    Deep Learning: A Survey of the State-of-the-Art") 所示。然后，方程中的 $T$ ([29](#S3.E29
    "In 3.6.2 Large Language Models (LLMs) ‣ 3.6 Calibrating Pre-trained Large Models
    ‣ 3 Calibration Methods ‣ Calibration in Deep Learning: A Survey of the State-of-the-Art"))
    对应于 MC-dropout 中来自伯努利分布的掩码向量集合的数量 $\{r^{t}\}_{t=1}^{T}$，或集合中的随机训练模型的数量，这可能导致不同的学习参数集合
    $\omega=\{\omega_{1},...,\omega_{t}\}$，或来自 Gumbel 分布的采样注意力分布集合的数量 $\{g^{t}\}_{t=1}^{T}$。'
- en: 'Remarks: This line of work requires multiple inference runs to perform approximations.
    This increases computational overhead significantly as compared to previous methods.
    On the other hand, besides model calibration, those methods are primarily proposed
    for uncertainty quantification and estimation. Therefore, network uncertainty
    can be captured and measured as well.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：这类工作需要多次推断运行以进行近似。这与以前的方法相比显著增加了计算开销。另一方面，除了模型校准，这些方法主要是为了不确定性量化和估计。因此，网络不确定性也可以被捕获和测量。
- en: 3.5 Composition Calibration
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 组合校准
- en: Beside applying each method independently, we can always have calibration compositions
    by combining two or more methods. One straightforward way to combine non-post-hoc
    methods with post-hoc methods. For instance, performing Temperature Scaling (TS)
    after employing the regularization method and implicit calibration (?, ?). Thulasidasan
    et al. (?) observed that the combination of label smoothing and mixup training
    significantly improved calibration. While there are several possibilities for
    combining different approaches, we highlight some interesting compositions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除了独立应用每种方法，我们总是可以通过结合两种或多种方法来进行校准组合。一个直接的方法是将非后验方法与后验方法结合。例如，在使用正则化方法和隐式校准（？，？）之后执行温度缩放（TS）。Thulasidasan
    等（？）观察到，标签平滑和混合训练的组合显著提高了校准效果。虽然结合不同方法的可能性有很多，但我们突出一些有趣的组合。
- en: 3.5.1 Ensemble Temperature Scaling (ETS)
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 集合温度缩放（ETS）
- en: 'Zhang et al. (?) gave three important definitions related to calibration properties:
    accuracy-preserving, data-efficient, and expressive. They pointed out that TS
    is an accuracy-preserving and data-efficient approach but is less expressive.
    Ensemble Temperature Scaling (ETS) was proposed to improve TS expressivity while
    maintaining the other two properties:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等（？）给出了与校准属性相关的三个重要定义：保持准确性、数据高效性和表现力。他们指出，TS 是一种保持准确性和数据高效性的方法，但表现力较差。提出了集成温度缩放（ETS）以提高
    TS 的表现力，同时保持其他两个属性。
- en: '|  | $T(z;w,\tau)=w_{1}T(z;\tau)+w_{2}z+w_{3}K^{-1}$ |  | (27) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $T(z;w,\tau)=w_{1}T(z;\tau)+w_{2}z+w_{3}K^{-1}$ |  | (27) |'
- en: 'There are three ensemble components: the original TS $T(z;\tau)=(z_{1}^{\tau^{-1}},...,z_{k}^{\tau^{-1}})/\sum_{k=1}^{K}z_{k}^{\tau^{-1}}$,
    uncalibrated prediction with $\tau=1$ and uniform prediction for each class $z_{k}=K^{-1}$.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个集合组件：原始 TS $T(z;\tau)=(z_{1}^{\tau^{-1}},...,z_{k}^{\tau^{-1}})/\sum_{k=1}^{K}z_{k}^{\tau^{-1}}$，$\tau=1$
    的未经校准预测和每个类别的均匀预测 $z_{k}=K^{-1}$。
- en: 3.5.2 Temperature Scaling with MC Dropout
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 带 MC Dropout 的温度缩放
- en: Laves et al. (?) extended TS to dropout variational inference to calibrate model
    uncertainty. The key idea is to insert $\tau$ before final softmax activation
    and insert TS with $\tau>0$ before softmax activation in MC integration:$\hat{p}=\frac{1}{N}\sum_{i=1}^{N}\textsc{softmax}(\frac{f_{w_{i}}(x)}{\tau})$
    where $N$ forward passes are performed to optimize $\tau$ with respect to NLL
    on the validation set. Then the entropy of the softmax likelihood is used to represent
    the uncertainty of all $C$ classes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Laves 等人 (?) 将 TS 扩展到 dropout 变分推理，以校准模型不确定性。关键思想是在最终的 softmax 激活之前插入 $\tau$，并在
    MC 积分中的 softmax 激活之前插入 $\tau>0$：$\hat{p}=\frac{1}{N}\sum_{i=1}^{N}\textsc{softmax}(\frac{f_{w_{i}}(x)}{\tau})$，其中执行
    $N$ 次前向传递以优化 $\tau$ 相对于验证集上的 NLL。然后，使用 softmax 似然的熵来表示所有 $C$ 类的 ​​不确定性。
- en: '|  | $H(p)=-\frac{1}{\log C}\sum p^{c}\log p^{c},H\in[0,1]$ |  | (28) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(p)=-\frac{1}{\log C}\sum p^{c}\log p^{c},H\in[0,1]$ |  | (28) |'
- en: 'Remarks: Appropriately combining different calibration types to some degree
    can further improve calibration, but, it may also combine their disadvantages,
    for instance, Ensemble Temperature Scaling (ETS) (?) has increased complexity.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：适当组合不同的校准类型可以进一步改善校准，但这也可能结合它们的缺点，例如，Ensemble Temperature Scaling (ETS) (?)
    增加了复杂性。
- en: 3.6 Calibrating Pre-trained Large Models
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 校准预训练的大型模型
- en: Pre-trained large models, including vision, language, or vision-language models,
    have been increasingly used in many safety-critical and customer-facing applications.
    The calibration of those large models has been recently studied and revisited (?, ?).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '预训练的大型模型，包括视觉、语言或视觉-语言模型，已经在许多安全关键和面向客户的应用中得到了广泛使用。这些大型模型的校准最近已经被研究和重新审视 (?,
    ?)。 '
- en: 3.6.1 Large Vision Model Calibration
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1 大型视觉模型校准
- en: Minderer et al. (?) studied recent state-of-the-art vision models that include
    vision transformer (?) and MLP-mixer (?). They found out that the model size and
    amount of pre-training in the recent model generation could not fully explain
    the observed decay of calibration with distribution shift or model size in the
    prior model generation. They also discussed the correlation between in-distribution
    and out-of-distribution (OOD) calibration. They pointed out that the models with
    better in-distribution calibration also gave better calibration on OOD benchmarks.
    LeVine et al. (?) studied the calibration of CLIP (?) as a zero-shot inference
    model and found that CLIP is miscalibrated in zero-shot settings.They showed effectiveness
    of learning a temperature on an auxiliary task and applying it to inference regardless
    of prompt or datasets.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Minderer 等人 (?) 研究了包括视觉变换器 (?) 和 MLP-mixer (?) 在内的最新前沿视觉模型。他们发现，近期模型生成中的模型大小和预训练量无法完全解释先前模型生成中的校准衰退现象与分布转移或模型大小之间的关系。他们还讨论了在分布内和分布外
    (OOD) 校准之间的相关性。他们指出，具有更好分布内校准的模型在 OOD 基准上也表现出更好的校准。LeVine 等人 (?) 研究了 CLIP (?)
    作为零样本推理模型的校准，发现 CLIP 在零样本设置中存在校准误差。他们展示了在辅助任务上学习温度并将其应用于推理的有效性，无论提示或数据集如何。
- en: 3.6.2 Large Language Models (LLMs)
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2 大型语言模型 (LLMs)
- en: The LLMs and prompting engineering have become an efficient learning paradigm
    and can perform numerous natural language tasks without or with only few examples.
    However, the outcome of this learning paradigm can be unstable and introduces
    bias with various prompt templates and training examples (?). The introduced biases
    include majority label bias, recency bias and token bias. To mitigate this bias,
    Zhao et al. (?) proposed contextual calibration procedure to improve the predictive
    power of GPT-3 on few-shot learning tasks with a context-free input such as ”N/A”.
    The contextual calibration is performed by using vector scaling (?).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 和提示工程已经成为一种高效的学习范式，并且可以在没有或仅有少量示例的情况下执行众多自然语言任务。然而，这种学习范式的结果可能不稳定，并且会引入与各种提示模板和训练示例相关的偏差
    (?). 引入的偏差包括主要标签偏差、最近性偏差和 token 偏差。为减少这些偏差，赵等人 (?) 提出了上下文校准程序，以改进 GPT-3 在少量学习任务中的预测能力，例如
    “N/A” 的上下文无关输入。上下文校准是通过使用向量缩放 (?) 完成的。
- en: Similarly, Zheng et al. (?) investaged the selection bais of LLMs in multi-choice
    question (MCQ) task and pinpointed that LLMs’ token bias is an intrinsic cause
    of the selection bias. They proposed
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Zheng 等人 (?) 调查了 LLMs 在多项选择题 (MCQ) 任务中的选择偏差，并指出 LLMs 的 token 偏差是选择偏差的内在原因。他们提出了
- en: 'Park et al. (?) extended mixup (?) training to improve model calibration by
    synthesizing samples based on the Area Under the Margin (AUM) for pre-trained
    language models.   Prototypical calibration (PROCA)  (?) is one of the latest
    studies in calibrating LLMs. It showed the importance of decision boundary in
    few-shot classification settings and suggested learning a better boundary with
    a prototypical cluster. Concretely, it estimates $K$ category-wise clusters with
    the Gaussian mixture model (GMM):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Park等人（?）将mixup（?）训练扩展至通过基于Margin下的面积（AUM）合成样本来改进模型校准，适用于预训练语言模型。原型校准（PROCA）（?）是校准LLMs的最新研究之一。它展示了在少样本分类设置中决策边界的重要性，并建议使用原型簇来学习更好的边界。具体来说，它使用高斯混合模型（GMM）来估计$K$类别的簇：
- en: '|  | $\displaystyle P_{GMM}(x)=\sum_{k=1}^{K}\alpha_{k}p_{G}(x&#124;u_{k},\Sigma_{k})$
    |  | (29) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P_{GMM}(x)=\sum_{k=1}^{K}\alpha_{k}p_{G}(x|u_{k},\Sigma_{k})$
    |  | (29) |'
- en: where $u_{k}$ and $\Sigma_{k}$ are the mean vector and covariance matrix of
    the distribution. The parameters $\{\alpha,u,\Sigma\}_{k=1}^{K}$ are estimated
    by using the Expectation-Maximization (EM) algorithm (?) with a small unlabelled
    dataset.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$u_{k}$和$\Sigma_{k}$是分布的均值向量和协方差矩阵。参数$\{\alpha,u,\Sigma\}_{k=1}^{K}$是通过使用期望最大化（EM）算法（?）在一个小的未标记数据集上估计的。
- en: 4 Conclusion and Future Work
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论与未来工作
- en: We have reviewed the state-of-the-art calibration methods, described with the
    motivations, causes, measurement metrics, and categorizations. Then we discussed
    the details and principles of recent methods as well as their individual advantages
    and disadvantages. Despite recent advances in calibrating deep models, there are
    still some challenges and underexplored aspects and needs further exploration.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了最先进的校准方法，描述了动机、原因、测量指标和分类。接着，我们讨论了近期方法的细节和原理以及它们的各自优缺点。尽管在校准深度模型方面取得了最近的进展，但仍存在一些挑战和未充分探索的方面，需进一步探索。
- en: 4.1 Mitigating Calibration Bias
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 减少校准偏差
- en: 'Accurately and reliably measuring calibration is still challenging due to the
    introduced biases from the binning mechanism and the finite sample numbers (?).
    For the former challenge, it mainly suffers from sensitivity and data inefficiency
    issues. The sensitivity to the binning scheme is presented in [3](#S3.F3 "Figure
    3 ‣ 3.4 Uncertainty Estimation ‣ 3 Calibration Methods ‣ Calibration in Deep Learning:
    A Survey of the State-of-the-Art"). We can see that for a given model, increasing
    bin numbers gives higher ECE and MCE scores. A KDE-based ECE Estimator (?) was
    proposed to replace histograms with non-parametric density estimators, which are
    continuous and more data-efficient. Measuring the bias is then important for having
    a correct calibration evaluation. Roelofs et al. (?) proposed Bias-by-Construction
    (BBC) to model the bias in bin-based ECE as a function of the number of samples
    and bins. It confirms the existence of non-negligible statistical biases. To follow
    this line of work, future efforts will include developing an unbiased calibration
    estimator, exploring the trade-off between calibration bias and variance as mentioned
    in  (?).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 准确而可靠地测量校准仍然具有挑战性，这主要是由于分箱机制引入的偏差和有限的样本数量（?）。对于前者挑战，主要存在灵敏度和数据低效的问题。分箱方案的灵敏度在[3](#S3.F3
    "图 3 ‣ 3.4 不确定性估计 ‣ 3 校准方法 ‣ 深度学习中的校准：前沿综述")中有所展示。我们可以看到，对于给定的模型，增加分箱数量会导致更高的ECE和MCE分数。一种基于KDE的ECE估计器（?）被提出，用非参数密度估计器替代直方图，这些估计器是连续的且数据更高效。因此，测量偏差对于正确的校准评估至关重要。Roelofs等人（?）提出了通过构造偏差（BBC）来将分箱基于的ECE的偏差建模为样本和分箱数量的函数。这确认了存在不可忽视的统计偏差。为了跟进这项工作，未来的努力将包括开发无偏校准估计器，探索如（?）中所述的校准偏差与方差之间的权衡。
- en: 4.2 Calibrating Generative Models
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 校准生成模型
- en: Most recent calibration efforts have focused on classification and regression
    tasks; model calibration for sequence generation is rarely discussed in the literature.
    Kumar et al. (?) pointed out the token-level probability in neural machine translation
    tasks is poorly calibrated, which explains the counter-intuitive BLEU drop with
    increased beam-size (?). The token-level probability miscalibration is further
    confirmed in LLM for few-shot learning (?). The main cause is the softmax bottleneck (?)
    on large vocabulary. In the task of sequence generation, early token probability
    miscalibration can magnify the entire sequence. How to effectively calibrate token-level
    probability in various settings would be an interesting direction, particularly
    in the era of LLMs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的标定工作主要集中在分类和回归任务上；关于序列生成的模型标定在文献中很少讨论。Kumar 等人 (?) 指出神经机器翻译任务中的 token-level
    概率标定不佳，这解释了随着 beam-size 增大而 BLEU 分数直观上下降的原因 (?)。在 LLM 中，token-level 概率的标定不佳在少量样本学习中得到了进一步确认
    (?)。主要原因是大词汇量上的 softmax 瓶颈 (?)。在序列生成任务中，早期的 token 概率标定不佳会放大整个序列。如何在各种设置中有效标定 token-level
    概率将是一个有趣的方向，尤其是在 LLM 时代。
- en: References
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Blundell et al. Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D.
    (2015). Weight uncertainty in neural network. In International conference on machine
    learning, pp. 1613–1622\. PMLR.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blundell 等人 Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015).
    神经网络中的权重不确定性. 在国际机器学习会议论文集, 页 1613–1622。PMLR。
- en: 'Bohdal et al. Bohdal, O., Yang, Y., & Hospedales, T. (2021). Meta-calibration:
    Meta-learning of model calibration using differentiable expected calibration error.
    arXiv preprint arXiv:2106.09613.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bohdal 等人 Bohdal, O., Yang, Y., & Hospedales, T. (2021). 元标定：使用可微分期望标定误差的模型标定元学习.
    arXiv 预印本 arXiv:2106.09613。
- en: Bojarski et al. Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp,
    B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., et al. (2016).
    End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bojarski 等人 Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp,
    B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., 等人 (2016). 端到端自驾车学习.
    arXiv 预印本 arXiv:1604.07316。
- en: 'Caruana et al. Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., & Elhadad,
    N. (2015). Intelligible models for healthcare: Predicting pneumonia risk and hospital
    30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference
    on knowledge discovery and data mining, pp. 1721–1730.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caruana 等人 Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., & Elhadad,
    N. (2015). 面向医疗保健的可解释模型：预测肺炎风险和医院 30 天再入院率. 在第21届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集,
    页 1721–1730。
- en: Corless et al. Corless, R. M., Gonnet, G. H., Hare, D. E., Jeffrey, D. J., & Knuth,
    D. E. (1996). On the lambertw function. Advances in Computational mathematics,
    5(1), 329–359.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Corless 等人 Corless, R. M., Gonnet, G. H., Hare, D. E., Jeffrey, D. J., & Knuth,
    D. E. (1996). 关于 lambertw 函数. 计算数学进展, 5(1), 329–359。
- en: Desai & Durrett Desai, S.,  & Durrett, G. (2020). Calibration of pre-trained
    transformers. In EMNLP, pp. 295–302.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Desai & Durrett Desai, S., & Durrett, G. (2020). 预训练变压器的标定. 在 EMNLP 论文集, 页 295–302。
- en: Ding et al. Ding, Z., Han, X., Liu, P., & Niethammer, M. (2021). Local temperature
    scaling for probability calibration. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 6889–6899.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人 Ding, Z., Han, X., Liu, P., & Niethammer, M. (2021). 用于概率标定的局部温度缩放.
    在 IEEE/CVF 国际计算机视觉会议论文集, 页 6889–6899。
- en: 'Dosovitskiy et al. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al. An image is worth 16x16 words: Transformers for image recognition at
    scale. In International Conference on Learning Representations.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人 Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D.,
    Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
    等人 一张图片值 16x16 个词：大规模图像识别的变压器. 在国际学习表征会议。
- en: Fortunato et al. Fortunato, M., Blundell, C., & Vinyals, O. (2017). Bayesian
    recurrent neural networks. arXiv preprint arXiv:1704.02798.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortunato 等人 Fortunato, M., Blundell, C., & Vinyals, O. (2017). 贝叶斯递归神经网络. arXiv
    预印本 arXiv:1704.02798。
- en: 'Gal & Ghahramani Gal, Y.,  & Ghahramani, Z. (2016). Dropout as a bayesian approximation:
    Representing model uncertainty in deep learning. In ICML’16, pp. 1050–1059\. PMLR.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal & Ghahramani Gal, Y., & Ghahramani, Z. (2016). Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性.
    在 ICML’16 论文集, 页 1050–1059。PMLR。
- en: Gawlikowski et al. Gawlikowski, J., Tassi, C. R. N., Ali, M., Lee, J., Humt,
    M., Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R., et al. (2021). A
    survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gawlikowski等人 Gawlikowski, J., Tassi, C. R. N., Ali, M., Lee, J., Humt, M.,
    Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R., 等人. (2021). 深度神经网络中的不确定性调查。arXiv预印本
    arXiv:2107.03342。
- en: Graves et al. Graves, A., Mohamed, A.-r., & Hinton, G. (2013). Speech recognition
    with deep recurrent neural networks. In 2013 IEEE international conference on
    acoustics, speech and signal processing, pp. 6645–6649\. Ieee.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves等人 Graves, A., Mohamed, A.-r., & Hinton, G. (2013). 使用深度递归神经网络的语音识别。2013
    IEEE国际声学、语音和信号处理会议，pp. 6645–6649. IEEE。
- en: Guo et al. Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration
    of modern neural networks. In ICML, pp. 1321–1330\. PMLR.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人 Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). 现代神经网络的校准。ICML，pp.
    1321–1330. PMLR。
- en: Han et al. Han, Z., Hao, Y., Dong, L., Sun, Y., & Wei, F. (2023). Prototypical
    calibration for few-shot learning of language models. In The Eleventh International
    Conference on Learning Representations.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han等人 Han, Z., Hao, Y., Dong, L., Sun, Y., & Wei, F. (2023). 针对少样本学习的语言模型的原型校准。在第十一届国际学习表征会议上。
- en: 'Hebbalaguppe et al. Hebbalaguppe, R., Prakash, J., Madan, N., & Arora, C. (2022).
    A stitch in time saves nine: A train-time regularizing loss for improved neural
    network calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 16081–16090.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hebbalaguppe等人 Hebbalaguppe, R., Prakash, J., Madan, N., & Arora, C. (2022).
    一针见血：一种改进神经网络校准的训练时间正则化损失。IEEE/CVF计算机视觉与模式识别大会论文集，pp. 16081–16090。
- en: 'Ioffe & Szegedy Ioffe, S.,  & Szegedy, C. (2015). Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In International conference
    on machine learning, pp. 448–456\. pmlr.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe & Szegedy Ioffe, S., & Szegedy, C. (2015). 批量归一化：通过减少内部协变量偏移来加速深度网络训练。在国际机器学习会议，pp.
    448–456. PMLR。
- en: Jang et al. Jang, E., Gu, S., & Poole, B. (2017). Categorical reparameterization
    with gumbel-softmax. In ICLR’17.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang等人 Jang, E., Gu, S., & Poole, B. (2017). 使用gumbel-softmax的分类重新参数化。在ICLR’17。
- en: 'Ji et al. Ji, B., Jung, H., Yoon, J., Kim, K., et al. (2019). Bin-wise temperature
    scaling (bts): Improvement in confidence calibration performance through simple
    scaling techniques. In 2019 IEEE/CVF International Conference on Computer Vision
    Workshop (ICCVW), pp. 4190–4196\. IEEE.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji等人 Ji, B., Jung, H., Yoon, J., Kim, K., 等人. (2019). Bin-wise 温度缩放（bts）：通过简单的缩放技术提高置信度校准性能。2019
    IEEE/CVF 国际计算机视觉会议研讨会（ICCVW），pp. 4190–4196. IEEE。
- en: Jin et al. Jin, L., Lazarow, J., & Tu, Z. (2017). Introspective classification
    with convolutional nets. NeurIPS, 30.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin等人 Jin, L., Lazarow, J., & Tu, Z. (2017). 使用卷积网络的内省分类。NeurIPS, 30。
- en: Kingma et al. Kingma, D. P., Salimans, T., & Welling, M. (2015). Variational
    dropout and the local reparameterization trick. In NeurIPS’15, Vol. 28, pp. 2575–2583.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma等人 Kingma, D. P., Salimans, T., & Welling, M. (2015). 变分丢弃和局部重新参数化技巧。NeurIPS’15,
    第28卷, pp. 2575–2583。
- en: Koehn & Knowles Koehn, P.,  & Knowles, R. (2017). Six challenges for neural
    machine translation. In First Workshop on Neural Machine Translation, pp. 28–39\.
    Association for Computational Linguistics.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koehn & Knowles Koehn, P., & Knowles, R. (2017). 神经机器翻译的六大挑战。第一届神经机器翻译研讨会，pp.
    28–39. 计算语言学协会。
- en: Krizhevsky et al. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet
    classification with deep convolutional neural networks. NeurIPS, 25.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等人 Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). 使用深度卷积神经网络进行Imagenet分类。NeurIPS,
    25。
- en: 'Kull et al. Kull, M., Filho, T. S., & Flach, P. (2017). Beta calibration: a
    well-founded and easily implemented improvement on logistic calibration for binary
    classifiers. In Proceedings of the 20th International Conference on Artificial
    Intelligence and Statistics, Vol. 54, pp. 623–631\. PMLR.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kull等人 Kull, M., Filho, T. S., & Flach, P. (2017). Beta 校准：对二元分类器的逻辑校准的一个有充分基础且易于实现的改进。在第20届国际人工智能与统计会议论文集，
    第54卷，pp. 623–631. PMLR。
- en: 'Kull et al. Kull, M., Perello Nieto, M., Kängsepp, M., Silva Filho, T., Song,
    H., & Flach, P. (2019). Beyond temperature scaling: Obtaining well-calibrated
    multi-class probabilities with dirichlet calibration. NeurIPS, 32.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kull等人 Kull, M., Perello Nieto, M., Kängsepp, M., Silva Filho, T., Song, H.,
    & Flach, P. (2019). 超越温度缩放：通过Dirichlet校准获得良好校准的多类概率。NeurIPS, 32。
- en: Kumar & Sarawagi Kumar, A.,  & Sarawagi, S. (2019). Calibration of encoder decoder
    models for neural machine translation. arXiv preprint arXiv:1903.00802.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar & Sarawagi Kumar, A., & Sarawagi, S. (2019). 对神经机器翻译的编码器-解码器模型进行校准。arXiv
    预印本 arXiv:1903.00802。
- en: Kumar et al. Kumar, A., Sarawagi, S., & Jain, U. (2018). Trainable calibration
    measures for neural networks from kernel mean embeddings. In ICML, pp. 2805–2814\.
    PMLR.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 Kumar, A., Sarawagi, S., & Jain, U. (2018). 从核均值嵌入中获得可训练的神经网络校准度量。在
    ICML 会议上，pp. 2805–2814. PMLR。
- en: Lakshminarayanan et al. Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017).
    Simple and scalable predictive uncertainty estimation using deep ensembles. In
    NeurIPS’17.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakshminarayanan 等人 Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017).
    使用深度集成进行简单且可扩展的预测不确定性估计。在 NeurIPS’17。
- en: Laves et al. Laves, M.-H., Ihler, S., Kortmann, K.-P., & Ortmaier, T. (2019).
    Well-calibrated model uncertainty with temperature scaling for dropout variational
    inference. arXiv preprint arXiv:1909.13550.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laves 等人 Laves, M.-H., Ihler, S., Kortmann, K.-P., & Ortmaier, T. (2019). 使用温度缩放进行
    dropout 变分推断的良好校准模型不确定性。arXiv 预印本 arXiv:1909.13550。
- en: LeVine et al. LeVine, W., Pikus, B., Raj, P., & Gil, F. A. (2023). Enabling
    calibration in the zero-shot inference of large vision-language models. arXiv
    preprint arXiv:2303.12748.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeVine 等人 LeVine, W., Pikus, B., Raj, P., & Gil, F. A. (2023). 在大型视觉语言模型的零样本推断中实现校准。arXiv
    预印本 arXiv:2303.12748。
- en: Lin et al. Lin, T.-Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017).
    Focal loss for dense object detection. In Proceedings of the IEEE international
    conference on computer vision, pp. 2980–2988.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 Lin, T.-Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). 用于密集目标检测的焦点损失。在
    IEEE 国际计算机视觉会议论文集，pp. 2980–2988。
- en: 'Liu et al. Liu, B., Ben Ayed, I., Galdran, A., & Dolz, J. (2022). The devil
    is in the margin: Margin-based label smoothing for network calibration. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 80–88.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 Liu, B., Ben Ayed, I., Galdran, A., & Dolz, J. (2022). 魔鬼在于边缘：基于边缘的标签平滑用于网络校准。在
    IEEE/CVF 计算机视觉与模式识别会议论文集，pp. 80–88。
- en: Mena et al. Mena, J., Pujol, O., & Vitria, J. (2021). A survey on uncertainty
    estimation in deep learning classification systems from a bayesian perspective.
    ACM Computing Surveys (CSUR), 54(9), 1–35.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mena 等人 Mena, J., Pujol, O., & Vitria, J. (2021). 从贝叶斯角度对深度学习分类系统中的不确定性估计进行调查。《ACM计算调查》（CSUR），54(9)，1–35。
- en: Minderer et al. Minderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai,
    X., Houlsby, N., Tran, D., & Lucic, M. (2021a). Revisiting the calibration of
    modern neural networks. NeurIPS, 34.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minderer 等人 Minderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai, X.,
    Houlsby, N., Tran, D., & Lucic, M. (2021a). 重新审视现代神经网络的校准。《NeurIPS》，34。
- en: Minderer et al. Minderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai,
    X., Houlsby, N., Tran, D., & Lucic, M. (2021b). Revisiting the calibration of
    modern neural networks..
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minderer 等人 Minderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai, X.,
    Houlsby, N., Tran, D., & Lucic, M. (2021b). 重新审视现代神经网络的校准。
- en: Moon Moon, T. K. (1996). The expectation-maximization algorithm. IEEE Signal
    processing magazine, 13(6), 47–60.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon Moon, T. (1996). 期望最大化算法。《IEEE信号处理杂志》，13(6)，47–60。
- en: 'Mozafari et al. Mozafari, A. S., Gomes, H. S., Leão, W., Janny, S., & Gagné,
    C. (2018). Attended temperature scaling: a practical approach for calibrating
    deep neural networks. arXiv preprint arXiv:1810.11586.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mozafari 等人 Mozafari, A. S., Gomes, H. S., Leão, W., Janny, S., & Gagné, C.
    (2018). 关注温度缩放：校准深度神经网络的实用方法。arXiv 预印本 arXiv:1810.11586。
- en: Mukhoti et al. Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P.,
    & Dokania, P. (2020). Calibrating deep neural networks using focal loss. NeurIPS,
    33, 15288–15299.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukhoti 等人 Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P., &
    Dokania, P. (2020). 使用焦点损失对深度神经网络进行校准。《NeurIPS》，33，15288–15299。
- en: Müller et al. Müller, R., Kornblith, S., & Hinton, G. E. (2019). When does label
    smoothing help?. NeurIPS, 32.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Müller 等人 Müller, R., Kornblith, S., & Hinton, G. E. (2019). 标签平滑什么时候有帮助？《NeurIPS》，32。
- en: Naeini et al. Naeini, M. P., Cooper, G. F., & Hauskrecht, M. (2015). Obtaining
    well calibrated probabilities using bayesian binning. In Proceedings of the Twenty-Ninth
    AAAI Conference on Artificial Intelligence (AAAI).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naeini 等人 Naeini, M. P., Cooper, G. F., & Hauskrecht, M. (2015). 使用贝叶斯分箱获得良好校准的概率。在第二十九届
    AAAI 人工智能会议论文集。
- en: Niculescu-Mizil & Caruana Niculescu-Mizil, A.,  & Caruana, R. (2005). Predicting
    good probabilities with supervised learning. In ICML, pp. 625–632.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niculescu-Mizil & Caruana Niculescu-Mizil, A., & Caruana, R. (2005). 使用监督学习预测良好的概率。在
    ICML 会议上，pp. 625–632。
- en: Nixon et al. Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., & Tran, D.
    (2019). Measuring calibration in deep learning.. In CVPR Workshops, Vol. 2.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nixon 等人。Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., & Tran, D. (2019)。深度学习中的校准测量。见于
    CVPR 研讨会，第2卷。
- en: 'Park & Caragea Park, S. Y.,  & Caragea, C. (2022). On the calibration of pre-trained
    language models using mixup guided by area under the margin and saliency. In Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pp. 5364–5374.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park & Caragea。Park, S. Y., & Caragea, C. (2022)。基于边界下的面积和显著性的 mixup 指导的预训练语言模型的校准。见于第60届计算语言学协会年会（第1卷：长篇论文），页码5364–5374。
- en: Pei et al. Pei, J., Wang, C., & Szarvas, G. (2022). Transformer uncertainty
    estimation with hierarchical stochastic attention. In AAAI, Vol. 36, pp. 11147–11155.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pei 等人。Pei, J., Wang, C., & Szarvas, G. (2022)。基于层次随机注意力的变换器不确定性估计。见于 AAAI，第36卷，页码11147–11155。
- en: Pereyra et al. Pereyra, G., Tucker, G., Chorowski, J., Kaiser, Ł., & Hinton,
    G. (2017). Regularizing neural networks by penalizing confident output distributions.
    arXiv preprint arXiv:1701.06548.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pereyra 等人。Pereyra, G., Tucker, G., Chorowski, J., Kaiser, Ł., & Hinton, G.
    (2017)。通过惩罚自信的输出分布来正则化神经网络。arXiv 预印本 arXiv:1701.06548。
- en: Platt et al. Platt, J.,  et al. (1999). Probabilistic outputs for support vector
    machines and comparisons to regularized likelihood methods. Advances in large
    margin classifiers, 10(3), 61–74.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Platt 等人。Platt, J., 等人。 (1999)。支持向量机的概率输出及与正则化似然方法的比较。大型边界分类器进展，10(3)，61–74。
- en: Radford et al. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,
    S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable
    visual models from natural language supervision. In International conference on
    machine learning, pp. 8748–8763\. PMLR.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人。Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,
    S., Sastry, G., Askell, A., Mishkin, P., Clark, J., 等人。 (2021)。从自然语言监督中学习可转移的视觉模型。见于国际机器学习会议，页码8748–8763。PMLR。
- en: Roelofs et al. Roelofs, R., Cain, N., Shlens, J., & Mozer, M. C. (2022). Mitigating
    bias in calibration error estimation. In International Conference on Artificial
    Intelligence and Statistics, pp. 4036–4054\. PMLR.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roelofs 等人。Roelofs, R., Cain, N., Shlens, J., & Mozer, M. C. (2022)。减少校准误差估计中的偏差。见于国际人工智能与统计会议，页码4036–4054。PMLR。
- en: 'Silva Filho et al. Silva Filho, T., Song, H., Perello-Nieto, M., Santos-Rodriguez,
    R., Kull, M., & Flach, P. (2023). Classifier calibration: a survey on how to assess
    and improve predicted class probabilities. Machine Learning, 1–50.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva Filho 等人。Silva Filho, T., Song, H., Perello-Nieto, M., Santos-Rodriguez,
    R., Kull, M., & Flach, P. (2023)。分类器校准：评估和改进预测类别概率的综述。机器学习，1–50。
- en: 'Srivastava et al. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
    & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from
    overfitting. The journal of machine learning research, 15(1), 1929–1958.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等人。Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov,
    R. (2014)。Dropout：防止神经网络过拟合的简单方法。机器学习研究期刊，15(1)，1929–1958。
- en: 'Thulasidasan et al. Thulasidasan, S., Chennupati, G., Bilmes, J. A., Bhattacharya,
    T., & Michalak, S. (2019). On mixup training: Improved calibration and predictive
    uncertainty for deep neural networks. NeurIPS, 32.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thulasidasan 等人。Thulasidasan, S., Chennupati, G., Bilmes, J. A., Bhattacharya,
    T., & Michalak, S. (2019)。关于 mixup 训练：改进深度神经网络的校准和预测不确定性。NeurIPS, 32。
- en: 'Tolstikhin et al. Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L.,
    Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J.,
    et al. (2021). Mlp-mixer: An all-mlp architecture for vision. NeurIPS, 34, 24261–24272.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tolstikhin 等人。Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai,
    X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., 等人。 (2021)。Mlp-mixer：一种全
    MLP 架构用于视觉。NeurIPS, 34, 24261–24272。
- en: Vaicenavicius et al. Vaicenavicius, J., Widmann, D., Andersson, C., Lindsten,
    F., Roll, J., & Schön, T. (2019). Evaluating model calibration in classification.
    In The 22nd International Conference on Artificial Intelligence and Statistics,
    pp. 3459–3467\. PMLR.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaicenavicius 等人。Vaicenavicius, J., Widmann, D., Andersson, C., Lindsten, F.,
    Roll, J., & Schön, T. (2019)。分类模型的校准评估。见于第22届国际人工智能与统计会议，页码3459–3467。PMLR。
- en: Vaswani et al. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need.
    NeurIPS, 30.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人。Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, Ł., & Polosukhin, I. (2017)。注意力即你所需。NeurIPS, 30。
- en: Wang et al. Wang, C., Lawrence, C., & Niepert, M. (2021a). Uncertainty estimation
    and calibration with finite-state probabilistic rnns. In ICLR’21.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 Wang, C., Lawrence, C., & Niepert, M. (2021a). 使用有限状态概率 RNNs 的不确定性估计与校准。收录于
    ICLR’21。
- en: 'Wang et al. Wang, D.-B., Feng, L., & Zhang, M.-L. (2021b). Rethinking calibration
    of deep neural networks: Do not be afraid of overconfidence. NeurIPS, 34, 11809–11820.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 Wang, D.-B., Feng, L., & Zhang, M.-L. (2021b). 重新思考深度神经网络的校准：不要害怕过度自信。NeurIPS,
    34, 11809–11820。
- en: 'Yang et al. Yang, Z., Dai, Z., Salakhutdinov, R., & Cohen, W. W. (2018). Breaking
    the softmax bottleneck: A high-rank rnn language model. In International Conference
    on Learning Representations.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 Yang, Z., Dai, Z., Salakhutdinov, R., & Cohen, W. W. (2018). 破除 softmax
    瓶颈：一种高秩 RNN 语言模型。收录于国际学习表征会议。
- en: Zadrozny & Elkan Zadrozny, B.,  & Elkan, C. (2001). Obtaining calibrated probability
    estimates from decision trees and naive bayesian classifiers. In Icml, Vol. 1,
    pp. 609–616\. Citeseer.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zadrozny 和 Elkan Zadrozny, B., & Elkan, C. (2001). 从决策树和朴素贝叶斯分类器中获取校准的概率估计。收录于
    ICML，第 1 卷，页码 609–616。Citeseer。
- en: Zadrozny & Elkan Zadrozny, B.,  & Elkan, C. (2002). Transforming classifier
    scores into accurate multiclass probability estimates. In Proceedings of the eighth
    ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 694–699.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zadrozny 和 Elkan Zadrozny, B., & Elkan, C. (2002). 将分类器得分转化为准确的多类概率估计。收录于第八届
    ACM SIGKDD 国际知识发现与数据挖掘会议论文集，页码 694–699。
- en: 'Zhang et al. Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2018).
    mixup: Beyond empirical risk minimization. In International Conference on Learning
    Representations.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2018). mixup：超越经验风险最小化。收录于国际学习表征会议。
- en: 'Zhang et al. Zhang, J., Kailkhura, B., & Han, T. Y.-J. (2020). Mix-n-match:
    Ensemble and compositional methods for uncertainty calibration in deep learning.
    In ICML, pp. 11117–11128\. PMLR.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 Zhang, J., Kailkhura, B., & Han, T. Y.-J. (2020). Mix-n-match：用于深度学习不确定性校准的集成与组合方法。收录于
    ICML，页码 11117–11128。PMLR。
- en: Zhang et al. Zhang, L., Deng, Z., Kawaguchi, K., & Zou, J. (2022). When and
    how mixup improves calibration. In International Conference on Machine Learning,
    pp. 26135–26160\. PMLR.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 Zhang, L., Deng, Z., Kawaguchi, K., & Zou, J. (2022). mixup 何时以及如何提升校准。收录于国际机器学习会议，页码
    26135–26160。PMLR。
- en: 'Zhao et al. Zhao, Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021).
    Calibrate before use: Improving few-shot performance of language models. In ICML,
    pp. 12697–12706\. PMLR.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 Zhao, Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021). 使用前的校准：提升语言模型的少样本表现。收录于
    ICML，页码 12697–12706。PMLR。
- en: Zheng et al. Zheng, C., Zhou, H., Meng, F., Zhou, J., & Huang, M. (2023). On
    large language models’ selection bias in multi-choice questions. arXiv preprint
    arXiv:2309.03882.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 Zheng, C., Zhou, H., Meng, F., Zhou, J., & Huang, M. (2023). 大型语言模型在多选题中的选择偏差。arXiv
    预印本 arXiv:2309.03882。
