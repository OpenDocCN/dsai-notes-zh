- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:44:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:44:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2208.10658] Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2208.10658] å…³äºè¿›åŒ–æ·±åº¦å­¦ä¹ çš„è°ƒæŸ¥ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2208.10658](https://ar5iv.labs.arxiv.org/html/2208.10658)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2208.10658](https://ar5iv.labs.arxiv.org/html/2208.10658)
- en: \UseRawInputEncoding
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \UseRawInputEncoding
- en: 'Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³äºè¿›åŒ–æ·±åº¦å­¦ä¹ çš„è°ƒæŸ¥ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜
- en: Nan Li Northeastern UniversityNo.195, Chuangxin RoadShenyangLiaoning ProvinceChina
    [2010500@stu.neu.edu.cn](mailto:2010500@stu.neu.edu.cn) ,Â  Lianbo Ma Northeastern
    UniversityNo.195, Chuangxin RoadShenyang CityLiaoning ProvinceChina [malb@swc.neu.edu.cn](mailto:malb@swc.neu.edu.cn)
    ,Â  Guo Yu East China University of Science and TechnologyMeilong Road 130ShanghaiChina
    [guoyu@ecust.edu.cn](mailto:guoyu@ecust.edu.cn) ,Â  Bing Xue Victoria University
    of WellingtonWellingtonNew Zealand [bing.xue@ecs.vuw.ac.nz](mailto:bing.xue@ecs.vuw.ac.nz)
    ,Â  Mengjie Zhang Victoria University of WellingtonWellingtonNew Zealand [mengjie.zhang@ecs.vuw.ac.nz](mailto:mengjie.zhang@ecs.vuw.ac.nz)
    Â andÂ  Yaochu Jin Bielefeld UniversityBielefeldGermany [yaochu.jin@uni-bielefeld.de](mailto:yaochu.jin@uni-bielefeld.de)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Nan Li Northeastern UniversityNo.195, Chuangxin RoadShenyangLiaoning ProvinceChina
    [2010500@stu.neu.edu.cn](mailto:2010500@stu.neu.edu.cn) ,  Lianbo Ma Northeastern
    UniversityNo.195, Chuangxin RoadShenyang CityLiaoning ProvinceChina [malb@swc.neu.edu.cn](mailto:malb@swc.neu.edu.cn)
    ,  Guo Yu East China University of Science and TechnologyMeilong Road 130ShanghaiChina
    [guoyu@ecust.edu.cn](mailto:guoyu@ecust.edu.cn) ,  Bing Xue Victoria University
    of WellingtonWellingtonNew Zealand [bing.xue@ecs.vuw.ac.nz](mailto:bing.xue@ecs.vuw.ac.nz)
    ,  Mengjie Zhang Victoria University of WellingtonWellingtonNew Zealand [mengjie.zhang@ecs.vuw.ac.nz](mailto:mengjie.zhang@ecs.vuw.ac.nz)  and  Yaochu
    Jin Bielefeld UniversityBielefeldGermany [yaochu.jin@uni-bielefeld.de](mailto:yaochu.jin@uni-bielefeld.de)
- en: Abstract.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦ã€‚
- en: Over recent years, there has been a rapid development of deep learning (DL)
    in both industry and academia fields. However, finding the optimal hyperparameters
    of a DL model often needs high computational cost and human expertise. To mitigate
    the above issue, evolutionary computation (EC) as a powerful heuristic search
    approach has shown significant merits in the automated design of DL models, so-called
    evolutionary deep learning (EDL). This paper aims to analyze EDL from the perspective
    of automated machine learning (AutoML). Specifically, we firstly illuminate EDL
    from machine learning and EC and regard EDL as an optimization problem. According
    to the DL pipeline, we systematically introduce EDL methods ranging from feature
    engineering, model generation, to model deployment with a new taxonomy (i.e.,
    what and how to evolve/optimize), and focus on the discussions of solution representation
    and search paradigm in handling the optimization problem by EC. Finally, key applications,
    open issues and potentially promising lines of future research are suggested.
    This survey has reviewed recent developments of EDL and offers insightful guidelines
    for the development of EDL.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨å·¥ä¸šå’Œå­¦æœ¯ç•Œéƒ½å–å¾—äº†å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œæ‰¾åˆ°æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æœ€ä½³è¶…å‚æ•°é€šå¸¸éœ€è¦é«˜è®¡ç®—æˆæœ¬å’Œäººç±»ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†ç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œè¿›åŒ–è®¡ç®—ï¼ˆECï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„å¯å‘å¼æœç´¢æ–¹æ³•ï¼Œåœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è‡ªåŠ¨è®¾è®¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œè¿™è¢«ç§°ä¸ºè¿›åŒ–æ·±åº¦å­¦ä¹ ï¼ˆEDLï¼‰ã€‚æœ¬æ–‡æ—¨åœ¨ä»è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰çš„è§’åº¦åˆ†æEDLã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆä»æœºå™¨å­¦ä¹ å’ŒECçš„è§’åº¦é˜æ˜EDLï¼Œå¹¶å°†EDLè§†ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚æ ¹æ®DLç®¡é“ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ä»‹ç»äº†ä»ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹ç”Ÿæˆåˆ°æ¨¡å‹éƒ¨ç½²çš„EDLæ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ–¹æ³•ï¼ˆå³ï¼Œå¦‚ä½•æ¼”åŒ–/ä¼˜åŒ–ï¼‰ï¼Œé‡ç‚¹è®¨è®ºäº†åœ¨å¤„ç†ä¼˜åŒ–é—®é¢˜æ—¶ECçš„è§£è¡¨ç¤ºå’Œæœç´¢èŒƒå¼ã€‚æœ€åï¼Œæå‡ºäº†å…³é”®åº”ç”¨ã€å¼€æ”¾é—®é¢˜ä»¥åŠæ½œåœ¨çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚æœ¬æ–‡å›é¡¾äº†EDLçš„æœ€æ–°è¿›å±•ï¼Œå¹¶ä¸ºEDLçš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚
- en: 'deep learning, evolutionary computation, feature engineering, model generation,
    model deployment.^â€ ^â€ ccs: General and referenceÂ Surveys and overviews^â€ ^â€ ccs:
    Computing methodologiesÂ Machine learning algorithms^â€ ^â€ ccs: Theory of computationÂ Evolutionary
    algorithms'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ·±åº¦å­¦ä¹ ï¼Œè¿›åŒ–è®¡ç®—ï¼Œç‰¹å¾å·¥ç¨‹ï¼Œæ¨¡å‹ç”Ÿæˆï¼Œæ¨¡å‹éƒ¨ç½²ã€‚^â€ ^â€ ccs: ä¸€èˆ¬å’Œå‚è€ƒ è°ƒæŸ¥ä¸æ¦‚è¿°^â€ ^â€ ccs: è®¡ç®—æ–¹æ³• æœºå™¨å­¦ä¹ ç®—æ³•^â€ ^â€ ccs: è®¡ç®—ç†è®º
    è¿›åŒ–ç®—æ³•'
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. å¼•è¨€
- en: 'Deep learning (DL) as a promising technology has been widely used in a variety
    of challenging tasks, such as image analysis (Krizhevsky etÂ al., [2012](#bib.bib103))
    and pattern recognition (LeCun etÂ al., [2015](#bib.bib105)). However, the practitioners
    of DL struggle to manually design deep models and find appropriate configurations
    by trial and error. An example is given in Fig. [1](#S1.F1 "Figure 1 â€£ 1\. Introduction
    â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues"), where domain knowledge is fed to DL in different stages like feature
    engineering (FE) (Xue etÂ al., [2015](#bib.bib226)), model generation (Zhou etÂ al.,
    [2021a](#bib.bib258)) and model deployment (Choudhary etÂ al., [2020](#bib.bib32);
    Cheng etÂ al., [2017](#bib.bib30)). Unfortunately, the difficulty in the acquisition
    of expert knowledge makes DL undergo a great challenge in its development.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰ä½œä¸ºä¸€ç§å‰æ™¯å¹¿é˜”çš„æŠ€æœ¯ï¼Œå·²ç»å¹¿æ³›åº”ç”¨äºå„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†æï¼ˆKrizhevsky et al., [2012](#bib.bib103)ï¼‰å’Œæ¨¡å¼è¯†åˆ«ï¼ˆLeCun
    et al., [2015](#bib.bib105)ï¼‰ã€‚ç„¶è€Œï¼ŒDL çš„ä»ä¸šè€…åœ¨æ‰‹åŠ¨è®¾è®¡æ·±åº¦æ¨¡å‹å’Œé€šè¿‡è¯•é”™æ‰¾åˆ°åˆé€‚çš„é…ç½®æ—¶é¢ä¸´å›°éš¾ã€‚å›¾ä¸­ [1](#S1.F1
    "Figure 1 â€£ 1\. Introduction â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") å±•ç¤ºäº†ä¸€ä¸ªä¾‹å­ï¼Œå…¶ä¸­é¢†åŸŸçŸ¥è¯†åœ¨ä¸åŒé˜¶æ®µè¢«è¾“å…¥åˆ° DL ä¸­ï¼Œä¾‹å¦‚ç‰¹å¾å·¥ç¨‹ï¼ˆFEï¼‰ï¼ˆXue
    et al., [2015](#bib.bib226)ï¼‰ã€æ¨¡å‹ç”Ÿæˆï¼ˆZhou et al., [2021a](#bib.bib258)ï¼‰å’Œæ¨¡å‹éƒ¨ç½²ï¼ˆChoudhary
    et al., [2020](#bib.bib32); Cheng et al., [2017](#bib.bib30)ï¼‰ã€‚ä¸å¹¸çš„æ˜¯ï¼Œä¸“å®¶çŸ¥è¯†è·å–çš„å›°éš¾ä½¿å¾—
    DL åœ¨å‘å±•è¿‡ç¨‹ä¸­é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚'
- en: In contrast, the automatic design of deep neural networks (DNNs) tends to be
    prevalent in recent decades (He etÂ al., [2021](#bib.bib72); Zhou etÂ al., [2021a](#bib.bib258)).
    The main reason lies in the flexibility and computation efficiency of automated
    machine learning (AutoML) in FE (Xue etÂ al., [2015](#bib.bib226)), parameter optimization
    (PO) (Zhang and Gouza, [2018](#bib.bib243)), hyperparameter optimization (HPO)
    (Stanley and Miikkulainen, [2002](#bib.bib186)), neural architecture search (NAS)
    (Yao etÂ al., [2018](#bib.bib231); He etÂ al., [2021](#bib.bib72); Zhou etÂ al.,
    [2021a](#bib.bib258)), and model compression (MC) (Hu etÂ al., [2021b](#bib.bib79)).
    In this way, AutoML without manual intervention has attracted great attention
    and much progress has been made.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰çš„è‡ªåŠ¨è®¾è®¡åœ¨è¿‘å‡ åå¹´å†…è¶‹äºæµè¡Œï¼ˆHe et al., [2021](#bib.bib72); Zhou et al.,
    [2021a](#bib.bib258)ï¼‰ã€‚ä¸»è¦åŸå› åœ¨äºè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰åœ¨ç‰¹å¾å·¥ç¨‹ï¼ˆFEï¼‰ï¼ˆXue et al., [2015](#bib.bib226)ï¼‰ã€å‚æ•°ä¼˜åŒ–ï¼ˆPOï¼‰ï¼ˆZhang
    å’Œ Gouza, [2018](#bib.bib243)ï¼‰ã€è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰ï¼ˆStanley å’Œ Miikkulainen, [2002](#bib.bib186)ï¼‰ã€ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰ï¼ˆYao
    et al., [2018](#bib.bib231); He et al., [2021](#bib.bib72); Zhou et al., [2021a](#bib.bib258)ï¼‰å’Œæ¨¡å‹å‹ç¼©ï¼ˆMCï¼‰ï¼ˆHu
    et al., [2021b](#bib.bib79)ï¼‰ä¸­çš„çµæ´»æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚å› æ­¤ï¼ŒAutoML åœ¨æ²¡æœ‰äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå¹¶å–å¾—äº†å¾ˆå¤§è¿›å±•ã€‚
- en: '![Refer to caption](img/157981cf4645442887259fefbd7ba762.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/157981cf4645442887259fefbd7ba762.png)'
- en: Figure 1. An overview of DL, driven by domain knowledge or evolutionary computation,
    where the life of DL gets through problem, data collection, feature engineering,
    model generation and model deployment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1. DL çš„æ¦‚è¿°ï¼Œç”±é¢†åŸŸçŸ¥è¯†æˆ–è¿›åŒ–è®¡ç®—é©±åŠ¨ï¼Œå…¶ä¸­ DL çš„ç”Ÿå‘½å‘¨æœŸç»å†äº†é—®é¢˜ã€æ•°æ®æ”¶é›†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹ç”Ÿæˆå’Œæ¨¡å‹éƒ¨ç½²ã€‚
- en: 'Evolutionary computation (EC) has been widely applied to automatic DL, owing
    to its flexibility and automatically evolving mechanism. In EC, a population of
    individuals are driven by the environmental selection to evolve towards the optimal
    solutions or front (Jin, [2006](#bib.bib89)). Nowadays, there are many automatic
    DL methods driven by EC, termed as evolutionary deep learning (EDL) (Telikani
    etÂ al., [2021](#bib.bib197); Evans etÂ al., [2018a](#bib.bib53); Zhang and Cagnoni,
    [2020](#bib.bib248); Zhang, [2018](#bib.bib247)). For example, a number of studies
    on EC have been carried out to the feature engineering (Xue etÂ al., [2015](#bib.bib226)),
    model generation (Yao etÂ al., [2018](#bib.bib231); Zhou etÂ al., [2021a](#bib.bib258)),
    and model deployments (Choudhary etÂ al., [2020](#bib.bib32)), as shown in Fig.
    [1](#S1.F1 "Figure 1 â€£ 1\. Introduction â€£ Survey on Evolutionary Deep Learning:
    Principles, Algorithms, Applications and Open Issues"). Therefore, the integration
    of EC and DL has become a hot research topic in both academic and industrial communities.
    Moreover, in Fig. [2](#S1.F2 "Figure 2 â€£ 1\. Introduction â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), the number
    of publications and citations referring to EC & DL by years from Web of Science
    gradually increases until around 2012, whereas it sharply rises in the following
    decade. Hence, more and more researchers work on the area of EDL.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›åŒ–è®¡ç®—ï¼ˆECï¼‰ç”±äºå…¶çµæ´»æ€§å’Œè‡ªåŠ¨æ¼”åŒ–æœºåˆ¶ï¼Œå·²å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰ã€‚åœ¨ EC ä¸­ï¼Œä¸€ç»„ä¸ªä½“é€šè¿‡ç¯å¢ƒé€‰æ‹©é©±åŠ¨å‘æœ€ä¼˜è§£æˆ–å‰æ²¿æ¼”åŒ–ï¼ˆJinï¼Œ[2006](#bib.bib89)ï¼‰ã€‚ç›®å‰ï¼Œè®¸å¤šè‡ªåŠ¨
    DL æ–¹æ³•æ˜¯ç”± EC é©±åŠ¨çš„ï¼Œç§°ä¸ºè¿›åŒ–æ·±åº¦å­¦ä¹ ï¼ˆEDLï¼‰ï¼ˆTelikani ç­‰ï¼Œ[2021](#bib.bib197)ï¼›Evans ç­‰ï¼Œ[2018a](#bib.bib53)ï¼›Zhang
    å’Œ Cagnoniï¼Œ[2020](#bib.bib248)ï¼›Zhangï¼Œ[2018](#bib.bib247)ï¼‰ã€‚ä¾‹å¦‚ï¼Œå·²ç»æœ‰è®¸å¤šå…³äº EC çš„ç ”ç©¶æ¶‰åŠç‰¹å¾å·¥ç¨‹ï¼ˆXue
    ç­‰ï¼Œ[2015](#bib.bib226)ï¼‰ã€æ¨¡å‹ç”Ÿæˆï¼ˆå§šç­‰ï¼Œ[2018](#bib.bib231)ï¼›å‘¨ç­‰ï¼Œ[2021a](#bib.bib258)ï¼‰å’Œæ¨¡å‹éƒ¨ç½²ï¼ˆChoudhary
    ç­‰ï¼Œ[2020](#bib.bib32)ï¼‰ï¼Œå¦‚å›¾ [1](#S1.F1 "å›¾ 1 â€£ 1\. ä»‹ç» â€£ è¿›åŒ–æ·±åº¦å­¦ä¹ è°ƒæŸ¥ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨å’Œå¼€æ”¾é—®é¢˜") æ‰€ç¤ºã€‚å› æ­¤ï¼ŒEC
    å’Œ DL çš„ç»“åˆå·²æˆä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„çƒ­é—¨ç ”ç©¶ä¸»é¢˜ã€‚æ­¤å¤–ï¼Œåœ¨å›¾ [2](#S1.F2 "å›¾ 2 â€£ 1\. ä»‹ç» â€£ è¿›åŒ–æ·±åº¦å­¦ä¹ è°ƒæŸ¥ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨å’Œå¼€æ”¾é—®é¢˜")
    ä¸­ï¼Œæ¶‰åŠ EC å’Œ DL çš„å‡ºç‰ˆç‰©å’Œå¼•ç”¨æ•°é‡ä» Web of Science èµ·é€å¹´å¢åŠ ï¼Œç›´åˆ°å¤§çº¦ 2012 å¹´ï¼Œè€Œåœ¨æ¥ä¸‹æ¥çš„åå¹´ä¸­æ€¥å‰§ä¸Šå‡ã€‚å› æ­¤ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶äººå‘˜å¼€å§‹ä»äº‹
    EDL é¢†åŸŸçš„ç ”ç©¶ã€‚
- en: '![Refer to caption](img/e00f51b178cb0629a960e5ee4e773716.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§å›¾æ³¨](img/e00f51b178cb0629a960e5ee4e773716.png)'
- en: Figure 2. Total publications and citations referring to EC & DL by years from
    Web of Science until July 2022.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2. ä» Web of Science åˆ° 2022 å¹´ 7 æœˆæ¶‰åŠ EC å’Œ DL çš„æ€»å‡ºç‰ˆç‰©å’Œå¼•ç”¨æ•°é‡ã€‚
- en: Table 1. Comparison between existing surveys and our work, where FE, PO, HPO,
    NAS, and MC indicate feature engineering, parameter optimization, hyperparameter
    optimization, neural architecture search, and model compression, respectively.
    â€œâœ“â€ and â€œ-â€ indicate the content is included or not in the paper, respectively.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1. ç°æœ‰è°ƒæŸ¥ä¸æˆ‘ä»¬å·¥ä½œçš„æ¯”è¾ƒï¼Œå…¶ä¸­ FEã€POã€HPOã€NAS å’Œ MC åˆ†åˆ«è¡¨ç¤ºç‰¹å¾å·¥ç¨‹ã€å‚æ•°ä¼˜åŒ–ã€è¶…å‚æ•°ä¼˜åŒ–ã€ç¥ç»æ¶æ„æœç´¢å’Œæ¨¡å‹å‹ç¼©ã€‚ â€œâœ“â€
    å’Œ â€œ-â€ åˆ†åˆ«è¡¨ç¤ºè®ºæ–‡ä¸­æ˜¯å¦åŒ…å«è¯¥å†…å®¹ã€‚
- en: '| Survey | Type | FE | PO | HPO | NAS | MC \bigstrut |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| è°ƒæŸ¥ | ç±»å‹ | FE | PO | HPO | NAS | MC \bigstrut |'
- en: '| (Yao etÂ al., [2018](#bib.bib231)) | AutoML | âœ“ | - | âœ“ | âœ“ | - \bigstrut[t]
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| (å§šç­‰ï¼Œ[2018](#bib.bib231)) | AutoML | âœ“ | - | âœ“ | âœ“ | - \bigstrut[t] |'
- en: '| (He etÂ al., [2021](#bib.bib72)) | AutoML | âœ“ | - | âœ“ | âœ“ | - |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| (He ç­‰ï¼Œ[2021](#bib.bib72)) | AutoML | âœ“ | - | âœ“ | âœ“ | - |'
- en: '| (Yao, [1993](#bib.bib232)) | NAS | - | âœ“ | âœ“ | âœ“ | - |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| (å§šï¼Œ[1993](#bib.bib232)) | NAS | - | âœ“ | âœ“ | âœ“ | - |'
- en: '| (Ren etÂ al., [2021](#bib.bib165)) | NAS | - | - | âœ“ | âœ“ | - |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| (Ren ç­‰ï¼Œ[2021](#bib.bib165)) | NAS | - | - | âœ“ | âœ“ | - |'
- en: '| (JaÃ¢fra etÂ al., [2019](#bib.bib86)) | NAS | - | - | âœ“ | âœ“ | - |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| (JaÃ¢fra ç­‰ï¼Œ[2019](#bib.bib86)) | NAS | - | - | âœ“ | âœ“ | - |'
- en: '| (Santra etÂ al., [2021](#bib.bib172)) | NAS | - | - | âœ“ | âœ“ | - |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| (Santra ç­‰ï¼Œ[2021](#bib.bib172)) | NAS | - | - | âœ“ | âœ“ | - |'
- en: '| (Telikani etÂ al., [2021](#bib.bib197)) | EDL | - | - | - | âœ“ | - |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| (Telikani ç­‰ï¼Œ[2021](#bib.bib197)) | EDL | - | - | - | âœ“ | - |'
- en: '| (Liu etÂ al., [2021b](#bib.bib117)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| (åˆ˜ç­‰ï¼Œ[2021b](#bib.bib117)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
- en: '| (Zhou etÂ al., [2021a](#bib.bib258)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| (å‘¨ç­‰ï¼Œ[2021a](#bib.bib258)) | EDL | - | âœ“ | âœ“ | - |'
- en: '| (Xue etÂ al., [2015](#bib.bib226)) | EDL | âœ“ | - | - | - | - |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| (Xue ç­‰ï¼Œ[2015](#bib.bib226)) | EDL | âœ“ | - | - | - | - |'
- en: '| (Al-Sahaf etÂ al., [2019](#bib.bib6)) | EDL | âœ“ | âœ“ | - | - | - |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| (Al-Sahaf ç­‰ï¼Œ[2019](#bib.bib6)) | EDL | âœ“ | âœ“ | - | - | - |'
- en: '| (Zhang etÂ al., [2011](#bib.bib244)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang ç­‰ï¼Œ[2011](#bib.bib244)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
- en: '| (Alexandropoulos and Aridas, [2019](#bib.bib8)) | EDL | âœ“ | - | âœ“ | âœ“ | -
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| (Alexandropoulos å’Œ Aridasï¼Œ[2019](#bib.bib8)) | EDL | âœ“ | - | âœ“ | âœ“ | - |'
- en: '| (Mirjalili etÂ al., [2019](#bib.bib138)) | EDL | âœ“ | âœ“ | âœ“ | âœ“ | - |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| (Mirjalili ç­‰ï¼Œ[2019](#bib.bib138)) | EDL | âœ“ | âœ“ | âœ“ | âœ“ | - |'
- en: '| (Darwish etÂ al., [2020](#bib.bib41)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| (Darwish ç­‰ï¼Œ [2020](#bib.bib41)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
- en: '| (Freitas, [2003](#bib.bib61)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| (Freitas, [2003](#bib.bib61)) | EDL | - | âœ“ | âœ“ | âœ“ | - |'
- en: '| Ours | EDL | âœ“ | âœ“ | âœ“ | âœ“ | âœ“\bigstrut[b] |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| æˆ‘ä»¬çš„ | EDL | âœ“ | âœ“ | âœ“ | âœ“ | âœ“\bigstrut[b] |'
- en: 'In Table [1](#S1.T1 "Table 1 â€£ 1\. Introduction â€£ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues"), we have listed
    recent surveys on automatic DL. A large number of surveies concentrate on the
    optimization of DL models (He etÂ al., [2021](#bib.bib72); Telikani etÂ al., [2021](#bib.bib197);
    Zhou etÂ al., [2021a](#bib.bib258); Xue etÂ al., [2015](#bib.bib226)), or NAS (Liu
    etÂ al., [2021b](#bib.bib117); Yao, [1993](#bib.bib232)). Many others focus on
    specific optimization paradigms such as reinforcement learning (RL) (JaÃ¢fra etÂ al.,
    [2019](#bib.bib86)), EC (Sun etÂ al., [2020](#bib.bib192)) and gradient (Santra
    etÂ al., [2021](#bib.bib172)). However, very few of them have systematically analysed
    EDL and runs the gamut of FE, PO, HPO, NAS, and MC. To fill the gap, we aim to
    give a comprehensive review of EDL in detail. The main contributions of this work
    are as follows.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¡¨æ ¼ [1](#S1.T1 "Table 1 â€£ 1\. Introduction â€£ Survey on Evolutionary Deep Learning:
    Principles, Algorithms, Applications and Open Issues") ä¸­ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†å…³äºè‡ªåŠ¨æ·±åº¦å­¦ä¹ çš„è¿‘æœŸè°ƒæŸ¥ã€‚å¤§é‡çš„è°ƒæŸ¥é›†ä¸­åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¼˜åŒ–ï¼ˆHe
    ç­‰ï¼Œ[2021](#bib.bib72)ï¼›Telikani ç­‰ï¼Œ[2021](#bib.bib197)ï¼›Zhou ç­‰ï¼Œ[2021a](#bib.bib258)ï¼›Xue
    ç­‰ï¼Œ[2015](#bib.bib226)ï¼‰ï¼Œæˆ–ç¥ç»æ¶æ„æœç´¢ï¼ˆLiu ç­‰ï¼Œ[2021b](#bib.bib117)ï¼›Yaoï¼Œ[1993](#bib.bib232)ï¼‰ã€‚è®¸å¤šå…¶ä»–ç ”ç©¶å…³æ³¨ç‰¹å®šçš„ä¼˜åŒ–èŒƒå¼ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼ˆJaÃ¢fra
    ç­‰ï¼Œ[2019](#bib.bib86)ï¼‰ï¼Œè¿›åŒ–è®¡ç®—ï¼ˆECï¼‰ï¼ˆSun ç­‰ï¼Œ[2020](#bib.bib192)ï¼‰å’Œæ¢¯åº¦ï¼ˆSantra ç­‰ï¼Œ[2021](#bib.bib172)ï¼‰ã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰ç ”ç©¶ç³»ç»Ÿåœ°åˆ†æäº†è¿›åŒ–æ·±åº¦å­¦ä¹ ï¼ˆEDLï¼‰å¹¶æ¶µç›–äº†ç‰¹å¾å·¥ç¨‹ï¼ˆFEï¼‰ã€è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰ã€ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰å’Œæ¨¡å‹é€‰æ‹©ï¼ˆMCï¼‰ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ—¨åœ¨è¯¦ç»†ç»¼è¿°
    EDLã€‚è¿™é¡¹å·¥ä½œçš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ã€‚'
- en: â€¢
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Existing work on EDL is reviewed from the perspective of DL and EC to facilitate
    the understanding of readers from the communities of both ML and EC, and we also
    formulated EDL into an optimization problem from the perspective of EC.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»æ·±åº¦å­¦ä¹ å’Œè¿›åŒ–è®¡ç®—çš„è§’åº¦å›é¡¾ç°æœ‰çš„ EDL å·¥ä½œï¼Œä»¥ä¿ƒè¿›æœºå™¨å­¦ä¹ å’Œè¿›åŒ–è®¡ç®—ç¤¾åŒºè¯»è€…çš„ç†è§£ï¼Œæˆ‘ä»¬è¿˜ä»è¿›åŒ–è®¡ç®—çš„è§’åº¦å°† EDL å½¢å¼åŒ–ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚
- en: â€¢
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: The survey describes and discusses on EDL in terms of feature engineering, model
    generation, and model deployment from a novel taxonomy, where the solution representation
    and the search paradigms are emphasized and systematically discussed. To the best
    of our knowledge, few survey has investigated the evolutionary model deployment.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯¥è°ƒæŸ¥ä»ä¸€ç§æ–°çš„åˆ†ç±»æ³•è§’åº¦æè¿°å’Œè®¨è®ºäº† EDLï¼Œé‡ç‚¹å¼ºè°ƒå¹¶ç³»ç»Ÿåœ°è®¨è®ºäº†è§£å†³æ–¹æ¡ˆè¡¨ç¤ºå’Œæœç´¢èŒƒå¼ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°‘æœ‰è°ƒæŸ¥ç ”ç©¶äº†è¿›åŒ–æ¨¡å‹éƒ¨ç½²ã€‚
- en: â€¢
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: On the basis of the comprehensive review of EDL approaches, a number of applications,
    open issues and trends of EDL are discussed, which will guide the development
    of EDL.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºäºå¯¹ EDL æ–¹æ³•çš„å…¨é¢ç»¼è¿°ï¼Œè®¨è®ºäº†å¤šä¸ªåº”ç”¨ã€å¼€æ”¾é—®é¢˜å’Œ EDL çš„è¶‹åŠ¿ï¼Œè¿™å°†æŒ‡å¯¼ EDL çš„å‘å±•ã€‚
- en: 'The rest of this paper is organized as follows. Section [2](#S2 "2\. An Overview
    of Evolutionary Deep Learning â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") presents an overview of EDL. In Section
    [3](#S3 "3\. Feature Engineering â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues"), EC-driven feature engineering is presented.
    EC-driven model generation is discussed in Section [4](#S4 "4\. Model Generation
    â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues"). Section [5](#S5 "5\. Model Deployment â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues") reviews
    EC-driven model compressions. After that, relevant applications, open issues and
    the trends of EDL are discussed in Section [6](#S6 "6\. Applications, Open Issues,
    and Trends â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues"). Finally, a conclusion of the paper is drawn in Section [7](#S7
    "7\. Conclusions â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ¬æ–‡å…¶ä½™éƒ¨åˆ†çš„ç»„ç»‡ç»“æ„å¦‚ä¸‹ã€‚ç¬¬[2](#S2 "2\. An Overview of Evolutionary Deep Learning â€£ Survey
    on Evolutionary Deep Learning: Principles, Algorithms, Applications and Open Issues")èŠ‚ä»‹ç»äº†è¿›åŒ–æ·±åº¦å­¦ä¹ çš„æ¦‚è¿°ã€‚åœ¨ç¬¬[3](#S3
    "3\. Feature Engineering â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues")èŠ‚ä¸­ä»‹ç»äº†ECé©±åŠ¨çš„ç‰¹å¾å·¥ç¨‹ã€‚ç¬¬[4](#S4 "4\. Model Generation â€£
    Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues")èŠ‚è®¨è®ºäº†ECé©±åŠ¨çš„æ¨¡å‹ç”Ÿæˆã€‚ç¬¬[5](#S5 "5\. Model Deployment â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")èŠ‚å›é¡¾äº†ECé©±åŠ¨çš„æ¨¡å‹å‹ç¼©ã€‚ä¹‹åï¼Œåœ¨ç¬¬[6](#S6
    "6\. Applications, Open Issues, and Trends â€£ Survey on Evolutionary Deep Learning:
    Principles, Algorithms, Applications and Open Issues")èŠ‚ä¸­è®¨è®ºäº†ç›¸å…³åº”ç”¨ã€å¼€æ”¾é—®é¢˜å’Œè¿›åŒ–æ·±åº¦å­¦ä¹ çš„è¶‹åŠ¿ã€‚æœ€åï¼Œåœ¨ç¬¬[7](#S7
    "7\. Conclusions â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues")èŠ‚ä¸­å¯¹æœ¬æ–‡è¿›è¡Œäº†æ€»ç»“ã€‚'
- en: 2\. An Overview of Evolutionary Deep Learning
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. è¿›åŒ–æ·±åº¦å­¦ä¹ æ¦‚è¿°
- en: 2.1\. Deep Learning
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. æ·±åº¦å­¦ä¹ 
- en: 'DL can be described as a triplet $M$ = ($D$, $T$, $P$) (Yao etÂ al., [2018](#bib.bib231)),
    where $D$ is the dataset used for the training of a deep model ($M$), and $T$
    is the targeted task. $P$ indicates the performance of $M$. The aim of DL is to
    boost its performance over specific task $T$, which is measured by $P$ on dataset
    $D$. In Fig. [1](#S1.F1 "Figure 1 â€£ 1\. Introduction â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), we can
    see there are three fundamental processes of DL, i.e., feature engineering, model
    generation and model deployment.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ·±åº¦å­¦ä¹ å¯ä»¥è¢«æè¿°ä¸ºä¸‰å…ƒç»„$M$ = ($D$, $T$, $P$) (Yao et al., [2018](#bib.bib231))ï¼Œå…¶ä¸­$D$æ˜¯ç”¨äºè®­ç»ƒæ·±åº¦æ¨¡å‹($M$)çš„æ•°æ®é›†ï¼Œ$T$æ˜¯ç›®æ ‡ä»»åŠ¡ã€‚$P$è¡¨ç¤º$M$çš„æ€§èƒ½ã€‚æ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ˜¯æå‡å…¶åœ¨ç‰¹å®šä»»åŠ¡$T$ä¸Šçš„æ€§èƒ½ï¼Œè¿™é€šè¿‡åœ¨æ•°æ®é›†$D$ä¸Šæµ‹é‡$P$æ¥å®ç°ã€‚åœ¨å›¾[1](#S1.F1
    "Figure 1 â€£ 1\. Introduction â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues")ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ·±åº¦å­¦ä¹ çš„ä¸‰ä¸ªåŸºæœ¬è¿‡ç¨‹ï¼Œå³ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹ç”Ÿæˆå’Œæ¨¡å‹éƒ¨ç½²ã€‚'
- en: 'Feature engineering: It aims to find a high-quality $D$ to improve the performance
    ($P$) of the deep model ($M$) on specific tasks ($T$). In practice, the feature
    space of $D$ may include redundant and noisy information, which harms the performance
    ($P$) of the model ($M$). On Prostate dataset, the size of feature subset (65)
    selected in (Tran etÂ al., [2018](#bib.bib200)) is only 1% of the total size of
    features (10509).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å·¥ç¨‹ï¼šå®ƒæ—¨åœ¨å¯»æ‰¾ä¸€ä¸ªé«˜è´¨é‡çš„$D$æ¥æå‡æ·±åº¦æ¨¡å‹($M$)åœ¨ç‰¹å®šä»»åŠ¡($T$)ä¸Šçš„æ€§èƒ½($P$)ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ$D$çš„ç‰¹å¾ç©ºé—´å¯èƒ½åŒ…å«å†—ä½™å’Œå™ªå£°ä¿¡æ¯ï¼Œè¿™ä¼šæŸå®³æ¨¡å‹($M$)çš„æ€§èƒ½($P$)ã€‚åœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šï¼Œ(Tran
    et al., [2018](#bib.bib200))ä¸­é€‰æ‹©çš„ç‰¹å¾å­é›†çš„å¤§å°ï¼ˆ65ï¼‰ä»…å ç‰¹å¾æ€»å¤§å°ï¼ˆ10509ï¼‰çš„1%ã€‚
- en: 'Model generation: It targets at optimizing/generating a model ($M$) with desirable
    performance ($P$) for specific task ($T$) on the given datasets ($D$) (He etÂ al.,
    [2021](#bib.bib72)). Model generation can be further divided into parameter optimization,
    model architecture optimization, and joint optimization (Zhou etÂ al., [2021a](#bib.bib258)).
    Parameter optimization is to search the best parameters (e.g., weights) for a
    predefined model. Architecture optimization is dedicated to finding the optimal
    network topology (e.g., number of layers and types of operations) of a deep model
    ($M$) (Luo etÂ al., [2018](#bib.bib127)). Joint optimization involves in the above
    two optimization issues by automatically searching for a powerful model ($M$)
    on the datasets ($D$) (Miikkulainen etÂ al., [2019](#bib.bib137)).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹ç”Ÿæˆï¼šæ—¨åœ¨ä¼˜åŒ–/ç”Ÿæˆå…·æœ‰ç†æƒ³æ€§èƒ½ï¼ˆ$P$ï¼‰çš„æ¨¡å‹ï¼ˆ$M$ï¼‰ï¼Œä»¥æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼ˆ$T$ï¼‰åœ¨ç»™å®šçš„æ•°æ®é›†ï¼ˆ$D$ï¼‰ä¸Šï¼ˆHe ç­‰ï¼Œ [2021](#bib.bib72)ï¼‰ã€‚æ¨¡å‹ç”Ÿæˆå¯ä»¥è¿›ä¸€æ­¥åˆ†ä¸ºå‚æ•°ä¼˜åŒ–ã€æ¨¡å‹æ¶æ„ä¼˜åŒ–å’Œè”åˆä¼˜åŒ–ï¼ˆZhou
    ç­‰ï¼Œ [2021a](#bib.bib258)ï¼‰ã€‚å‚æ•°ä¼˜åŒ–æ˜¯ä¸ºé¢„å®šä¹‰çš„æ¨¡å‹æœç´¢æœ€ä½³å‚æ•°ï¼ˆä¾‹å¦‚æƒé‡ï¼‰ã€‚æ¶æ„ä¼˜åŒ–è‡´åŠ›äºå¯»æ‰¾æ·±åº¦æ¨¡å‹ï¼ˆ$M$ï¼‰çš„æœ€ä½³ç½‘ç»œæ‹“æ‰‘ï¼ˆä¾‹å¦‚å±‚æ•°å’Œæ“ä½œç±»å‹ï¼‰ï¼ˆLuo
    ç­‰ï¼Œ [2018](#bib.bib127)ï¼‰ã€‚è”åˆä¼˜åŒ–é€šè¿‡åœ¨æ•°æ®é›†ï¼ˆ$D$ï¼‰ä¸Šè‡ªåŠ¨æœç´¢å¼ºå¤§çš„æ¨¡å‹ï¼ˆ$M$ï¼‰æ¥æ¶‰åŠä¸Šè¿°ä¸¤ä¸ªä¼˜åŒ–é—®é¢˜ï¼ˆMiikkulainen
    ç­‰ï¼Œ [2019](#bib.bib137)ï¼‰ã€‚
- en: 'Model deployment: This process aims to deploy a deep model ($M$) to solve a
    deployment task $T$ with acceptable performance ($P$) on input data ($D$) within
    limited computational budgets. The key issue of model deployment is how to reduce
    the latency, storage, and energy consumption when the number of parameters of
    a deep model is large, e.g., Transformer-XL Large has 257M parameters (Dowdell
    and Zhang, [2020](#bib.bib48)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹éƒ¨ç½²ï¼šæ­¤è¿‡ç¨‹æ—¨åœ¨å°†æ·±åº¦æ¨¡å‹ï¼ˆ$M$ï¼‰éƒ¨ç½²ä»¥åœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—å†…è§£å†³ä¸€ä¸ªéƒ¨ç½²ä»»åŠ¡ï¼ˆ$T$ï¼‰ï¼Œå¹¶åœ¨è¾“å…¥æ•°æ®ï¼ˆ$D$ï¼‰ä¸Šè¾¾åˆ°å¯æ¥å—çš„æ€§èƒ½ï¼ˆ$P$ï¼‰ã€‚æ¨¡å‹éƒ¨ç½²çš„å…³é”®é—®é¢˜æ˜¯å½“æ·±åº¦æ¨¡å‹çš„å‚æ•°æ•°é‡å¾ˆå¤§æ—¶å¦‚ä½•å‡å°‘å»¶è¿Ÿã€å­˜å‚¨å’Œèƒ½è€—ï¼Œä¾‹å¦‚
    Transformer-XL Large æœ‰ 257M å‚æ•°ï¼ˆDowdell å’Œ Zhangï¼Œ [2020](#bib.bib48)ï¼‰ã€‚
- en: 2.2\. Evolutionary Computation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. è¿›åŒ–è®¡ç®—
- en: EC is a collection of stochastic population-based search methods inspired by
    evolution mechanisms such as natural selection and genetics, which does not need
    gradient information and is able to handle a black-box optimization problem without
    explicit mathematical formulations (Ma etÂ al., [2022](#bib.bib129); Vargas-HÃ¡kim
    etÂ al., [2022](#bib.bib204)). Owing to the above characteristics, EC has been
    widely employed to the automatic design of DL.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: EC æ˜¯ä¸€ç§åŸºäºéšæœºç§ç¾¤çš„æœç´¢æ–¹æ³•ï¼Œçµæ„Ÿæ¥æºäºè‡ªç„¶é€‰æ‹©å’Œé—ä¼ å­¦ç­‰è¿›åŒ–æœºåˆ¶ï¼Œå®ƒä¸éœ€è¦æ¢¯åº¦ä¿¡æ¯ï¼Œèƒ½å¤Ÿå¤„ç†æ²¡æœ‰æ˜ç¡®æ•°å­¦å…¬å¼çš„é»‘ç®±ä¼˜åŒ–é—®é¢˜ï¼ˆMa ç­‰ï¼Œ [2022](#bib.bib129);
    Vargas-HÃ¡kim ç­‰ï¼Œ [2022](#bib.bib204)ï¼‰ã€‚ç”±äºä¸Šè¿°ç‰¹ç‚¹ï¼ŒEC å·²è¢«å¹¿æ³›åº”ç”¨äºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨è®¾è®¡ã€‚
- en: '![Refer to caption](img/43778ec24fd8eb2d2ec7491b539a713d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/43778ec24fd8eb2d2ec7491b539a713d.png)'
- en: Figure 3. A general framework of EC.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3. EC çš„é€šç”¨æ¡†æ¶ã€‚
- en: 'In principle, we can broadly divide EC methods into two categories: evolutionary
    algorithms (EA) and swarm intelligence (SI) (Zhou etÂ al., [2021a](#bib.bib258)).
    Our work doesnâ€™t make an explicit distinction between EAs and SI since they comply
    with a general framework, as shown in Fig. [3](#S2.F3 "Figure 3 â€£ 2.2\. Evolutionary
    Computation â€£ 2\. An Overview of Evolutionary Deep Learning â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), which consists
    of three main components.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°† EC æ–¹æ³•å¤§è‡´åˆ†ä¸ºä¸¤ç±»ï¼šè¿›åŒ–ç®—æ³•ï¼ˆEAï¼‰å’Œç¾¤ä½“æ™ºèƒ½ï¼ˆSIï¼‰ï¼ˆZhou ç­‰ï¼Œ [2021a](#bib.bib258)ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œæ²¡æœ‰æ˜ç¡®åŒºåˆ†
    EA å’Œ SIï¼Œå› ä¸ºå®ƒä»¬ç¬¦åˆä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¦‚å›¾ [3](#S2.F3 "Figure 3 â€£ 2.2\. Evolutionary Computation
    â€£ 2\. An Overview of Evolutionary Deep Learning â€£ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues") æ‰€ç¤ºï¼Œè¯¥æ¡†æ¶ç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆã€‚'
- en: 'Initialization:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ï¼š
- en: is performed to generate a population of individuals which are encoded according
    to the decision space (or search space and variable space) of the optimization
    problem, such as the feature set, model parameters and topological structure.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›è¡Œä»¥ç”Ÿæˆä¸€ç¾¤ä¸ªä½“ï¼Œè¿™äº›ä¸ªä½“æ ¹æ®ä¼˜åŒ–é—®é¢˜çš„å†³ç­–ç©ºé—´ï¼ˆæˆ–æœç´¢ç©ºé—´å’Œå˜é‡ç©ºé—´ï¼‰è¿›è¡Œç¼–ç ï¼Œå¦‚ç‰¹å¾é›†ã€æ¨¡å‹å‚æ•°å’Œæ‹“æ‰‘ç»“æ„ã€‚
- en: 'Evaluation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°ï¼š
- en: aims to calculate the fitness of individuals. In fact, the evaluation of the
    individuals in EDL is a computationally expensive task (Miahi etÂ al., [2022](#bib.bib136)).
    For example, the work (Real etÂ al., [2017](#bib.bib163)) used 3000 GPU days to
    find a desirable architecture.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¨åœ¨è®¡ç®—ä¸ªä½“çš„é€‚åº”åº¦ã€‚å®é™…ä¸Šï¼Œåœ¨ EDL ä¸­è¯„ä¼°ä¸ªä½“æ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†çš„ä»»åŠ¡ï¼ˆMiahi ç­‰ï¼Œ [2022](#bib.bib136)ï¼‰ã€‚ä¾‹å¦‚ï¼Œå·¥ä½œï¼ˆReal
    ç­‰ï¼Œ [2017](#bib.bib163)ï¼‰ä½¿ç”¨äº† 3000 ä¸ª GPU å¤©æ¥å¯»æ‰¾ç†æƒ³çš„æ¶æ„ã€‚
- en: 'Updating:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–°ï¼š
- en: aims to generates a number of offspring solutions through various reproduction
    operations. For example, a new soultion is generated via velocity and position
    formula in particle swarm optimization (PSO) (Tran etÂ al., [2018](#bib.bib200)).
    In terms of genetic algorithm (GA), some reproduction operators (e.g., crossover
    and mutation) are used to generate new individuals (Vafaie and DeÂ Jong, [1998](#bib.bib203)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¨åœ¨é€šè¿‡å„ç§ç¹æ®–è¿ç®—ç”Ÿæˆè®¸å¤šåä»£è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œé€šè¿‡ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆPSOï¼‰ä¸­çš„é€Ÿåº¦å’Œä½ç½®å…¬å¼ç”ŸæˆåŠŸèƒ½è§£ï¼ˆTranç­‰äººï¼Œ[2018](#bib.bib200)ï¼‰ã€‚å¯¹äºé—ä¼ ç®—æ³•ï¼ˆGAï¼‰ï¼Œä½¿ç”¨ä¸€äº›ç¹æ®–ç®—å­ï¼ˆä¾‹å¦‚äº¤å‰å’Œå˜å¼‚ï¼‰æ¥ç”Ÿæˆæ–°çš„ä¸ªä½“ï¼ˆVafaieå’ŒDeÂ Jongï¼Œ[1998](#bib.bib203)ï¼‰ã€‚
- en: 2.3\. Evolutionary Deep Learning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. è¿›åŒ–æ·±åº¦å­¦ä¹ 
- en: 2.3.1\. EDL from two perspectives
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. EDLçš„ä¸¤ä¸ªè§†è§’
- en: In contrast to traditional DL which heavily relies on expert or domain knowledge
    to build deep model, EDL is to automatically design the deep model through an
    evolutionary process (Sun etÂ al., [2020](#bib.bib192); Yao, [1993](#bib.bib232);
    Ren etÂ al., [2021](#bib.bib165); Zhang, [2018](#bib.bib247)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼ ç»Ÿçš„ä¾èµ–ä¸“å®¶æˆ–é¢†åŸŸçŸ¥è¯†æ„å»ºæ·±åº¦æ¨¡å‹çš„DLç›¸æ¯”ï¼ŒEDLæ˜¯é€šè¿‡è¿›åŒ–è¿‡ç¨‹è‡ªåŠ¨è®¾è®¡æ·±åº¦æ¨¡å‹çš„ã€‚ï¼ˆSunç­‰äººï¼Œ[2020](#bib.bib192); Yaoï¼Œ[1993](#bib.bib232);
    Renç­‰äººï¼Œ[2021](#bib.bib165); Zhangï¼Œ[2018](#bib.bib247)ï¼‰ã€‚
- en: 'From the perspective of DL: Traditional DL needs a lot of expert knowledge
    in inventing and analysing a learning tool to a specific dataset or task. In contrast,
    EDL can be seen as a human-friendly learning tool that can automatically find
    appropriate deep models on given datasets or tasks (Yao etÂ al., [2018](#bib.bib231)).
    In other words, EDL concentrates on how easy a learning tool can be used.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä»DLçš„è§’åº¦æ¥çœ‹ï¼šä¼ ç»Ÿçš„DLéœ€è¦å¾ˆå¤šä¸“å®¶çŸ¥è¯†æ¥å‘æ˜å’Œåˆ†æç‰¹å®šæ•°æ®é›†æˆ–ä»»åŠ¡çš„å­¦ä¹ å·¥å…·ã€‚ç›¸åï¼ŒEDLå¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ä¸ªäººæ€§åŒ–çš„å­¦ä¹ å·¥å…·ï¼Œå¯ä»¥åœ¨ç»™å®šçš„æ•°æ®é›†æˆ–ä»»åŠ¡ä¸Šè‡ªåŠ¨æ‰¾åˆ°åˆé€‚çš„æ·±åº¦æ¨¡å‹ï¼ˆYaoç­‰äººï¼Œ[2018](#bib.bib231)ï¼‰ã€‚æ¢å¥è¯è¯´ï¼ŒEDLå…³æ³¨çš„æ˜¯å­¦ä¹ å·¥å…·çš„æ˜“ç”¨æ€§ã€‚
- en: 'From the perspective of EC: The configurations of a model is represented as
    an individual, and the performance as the objective to be optimized. EC plays
    an important role in the optimization driven by evolutionary mechanisms. Namely,
    EDL can be seen as an evolutionary optimization process to find the optimal configurations
    of the deep model with high performance.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ECçš„è§’åº¦ï¼šæ¨¡å‹çš„é…ç½®è¡¨ç¤ºä¸ºä¸€ä¸ªä¸ªä½“ï¼Œæ€§èƒ½ä½œä¸ºè¦ä¼˜åŒ–çš„ç›®æ ‡ã€‚ ECåœ¨ä»¥è¿›åŒ–æœºåˆ¶é©±åŠ¨çš„ä¼˜åŒ–ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒEDLå¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ä¸ªè¿›åŒ–ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»¥æ‰¾åˆ°å…·æœ‰é«˜æ€§èƒ½çš„æ·±åº¦æ¨¡å‹çš„æœ€ä¼˜é…ç½®ã€‚
- en: From the above analysis, EDL not only aims to increase the adaptability of a
    deep model towards learning tasks via the automatic construction approach (from
    the perspective of DL), but also tries to achieve the optimal model under the
    designed objectives or constraints (from the perspective of EC).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šè¿°åˆ†æå¯çŸ¥ï¼ŒEDLä¸ä»…é€šè¿‡è‡ªåŠ¨æ„å»ºæ–¹æ³•ï¼ˆä»DLçš„è§’åº¦ï¼‰æ—¨åœ¨å¢åŠ æ·±åº¦æ¨¡å‹å¯¹å­¦ä¹ ä»»åŠ¡çš„é€‚åº”æ€§ï¼Œè€Œä¸”è¿˜è¯•å›¾åœ¨è®¾è®¡çš„ç›®æ ‡æˆ–çº¦æŸæ¡ä»¶ä¸‹å®ç°æœ€ä¼˜æ¨¡å‹ï¼ˆä»ECçš„è§’åº¦ï¼‰ã€‚
- en: 2.3.2\. Definition and Framework of EDL
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. EDLçš„å®šä¹‰å’Œæ¡†æ¶
- en: 'According to the above discussion in Subsection [2.3.1](#S2.SS3.SSS1 "2.3.1\.
    EDL from two perspectives â€£ 2.3\. Evolutionary Deep Learning â€£ 2\. An Overview
    of Evolutionary Deep Learning â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") and following (Yao etÂ al., [2018](#bib.bib231)),
    we can define EDL as follows.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ ¹æ®ç¬¬[2.3.1](#S2.SS3.SSS1 "2.3.1\. EDL from two perspectives â€£ 2.3\. Evolutionary
    Deep Learning â€£ 2\. An Overview of Evolutionary Deep Learning â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")å°èŠ‚ä¸­çš„è®¨è®ºå’Œéšåçš„ï¼ˆYaoç­‰äººï¼Œ[2018](#bib.bib231)ï¼‰æˆ‘ä»¬å¯ä»¥å¦‚ä¸‹å®šä¹‰EDLã€‚'
- en: '| (1) |  | <math   alttext="\begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&amp;{{\textrm{Learning
    tools'' performance,}}}\end{array}\\ \begin{array}[]{*{20}{c}}{s.t.}&amp;{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{No
    assistance from humans}}}\\'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '| (1) |  | <math   alttext="\begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&amp;{{\textrm{Learning
    tools'' performance,}}}\end{array}\\ \begin{array}[]{*{20}{c}}{s.t.}&amp;{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{No
    assistance from humans}}}\\'
- en: '{{\textrm{Limited computational budgets.}}}\end{array}}\right.}\end{array}\end{array}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr  ><mtd
    columnalign="left"  ><mtable columnspacing="5pt" displaystyle="true"  ><mtr ><mtd
    ><munder  ><mo movablelimits="false"  >Max</mo><mrow ><mrow ><mi >c</mi><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >n</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >f</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >g</mi></mrow><mo lspace="0em"  >.</mo></mrow></munder></mtd><mtd
    ><mtext >Learning toolsâ€™ performance,</mtext></mtd></mtr></mtable></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mtable columnspacing="5pt" displaystyle="true" ><mtr
    ><mtd  ><mrow ><mrow ><mi >s</mi><mo lspace="0em" rspace="0.167em" >.</mo><mi
    >t</mi></mrow><mo lspace="0em"  >.</mo></mrow></mtd><mtd ><mrow ><mo >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd ><mtext
    >No assistance from humans</mtext></mtd></mtr><mtr ><mtd ><mtext >Limited computational
    budgets.</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><matrix ><matrixrow  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >Max</ci><list ><apply ><ci
    >ğ‘</ci><ci >ğ‘œ</ci><ci >ğ‘›</ci><ci >ğ‘“</ci><ci >ğ‘–</ci><ci >ğ‘”</ci></apply></list></apply><ci
    ><mtext >Learning toolsâ€™ performance,</mtext></ci><cerror  ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow></matrix></matrixrow><matrixrow
    ><matrix ><matrixrow  ><apply ><csymbol cd="ambiguous"  >formulae-sequence</csymbol><ci
    >ğ‘ </ci><ci >ğ‘¡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix
    ><matrixrow ><ci ><mtext >No assistance from humans</mtext></ci><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><ci ><mtext >Limited computational budgets.</mtext></ci><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow></matrix></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&{{\textrm{Learning
    tools'' performance,}}}\end{array}\\ \begin{array}[]{*{20}{c}}{s.t.}&{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{No
    assistance from humans}}}\\ {{\textrm{Limited computational budgets.}}}\end{array}}\right.}\end{array}\end{array}</annotation></semantics></math>
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&{{\textrm{å­¦ä¹ å·¥å…·çš„æ€§èƒ½ï¼Œ}}}\end{array}\\
    \begin{array}[]{*{20}{c}}{s.t.}&{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{æ²¡æœ‰äººå·¥å¸®åŠ©}}}\\
    {{\textrm{æœ‰é™çš„è®¡ç®—é¢„ç®—ã€‚}}}\end{array}}\right.}\end{array}\end{array}
- en: where $config.$ indicates the configurations which form the decision space of
    an optimization problem. The problem is to maximize the objective (i.e., learning
    toolsâ€™ performance $P$) of tasks $T$ on datasets $D$ under the constraints of
    no assistance from humans and limited computational resources. Accordingly, three
    aspects are taken into account in the design of EDL.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $config.$ è¡¨ç¤ºå½¢æˆä¼˜åŒ–é—®é¢˜å†³ç­–ç©ºé—´çš„é…ç½®ã€‚é—®é¢˜æ˜¯æœ€å¤§åŒ–ä»»åŠ¡ $T$ åœ¨æ•°æ®é›† $D$ ä¸Šçš„ç›®æ ‡ï¼ˆå³å­¦ä¹ å·¥å…·çš„æ€§èƒ½ $P$ï¼‰ï¼Œåœ¨æ²¡æœ‰äººç±»å¸®åŠ©å’Œæœ‰é™è®¡ç®—èµ„æºçš„çº¦æŸä¸‹ã€‚å› æ­¤ï¼ŒEDL
    çš„è®¾è®¡è€ƒè™‘äº†ä¸‰ä¸ªæ–¹é¢ã€‚
- en: 'Desirable generalization performance::'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½::'
- en: EDL should have desirable generalization performance across given datasets and
    tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: EDL åº”è¯¥åœ¨ç»™å®šçš„æ•°æ®é›†å’Œä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚
- en: 'High search efficiency::'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'é«˜æœç´¢æ•ˆç‡::'
- en: EDL is able to find optimal or desirable configuration within a limited computational
    budges (e.g., hardware, latency, energy consumption) under different designed
    objectives (e.g., high accuracy, small model size).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: EDL èƒ½å¤Ÿåœ¨ä¸åŒè®¾è®¡ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œé«˜å‡†ç¡®ç‡ã€å°æ¨¡å‹å°ºå¯¸ï¼‰çš„é™åˆ¶ä¸‹ï¼Œåœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—ï¼ˆä¾‹å¦‚ï¼Œç¡¬ä»¶ã€å»¶è¿Ÿã€èƒ½è€—ï¼‰å†…æ‰¾åˆ°æœ€ä½³æˆ–ç†æƒ³çš„é…ç½®ã€‚
- en: 'Without human assistance::'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ— éœ€äººå·¥å¹²é¢„::'
- en: EDL is able to automatically configure without human intervention.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: EDL èƒ½å¤Ÿåœ¨æ²¡æœ‰äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹è‡ªåŠ¨é…ç½®ã€‚
- en: 'Following the EC framework described in Fig. [3](#S2.F3 "Figure 3 â€£ 2.2\. Evolutionary
    Computation â€£ 2\. An Overview of Evolutionary Deep Learning â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), we present
    a general framework of EDL as follows.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'æŒ‰ç…§å›¾ [3](#S2.F3 "Figure 3 â€£ 2.2\. Evolutionary Computation â€£ 2\. An Overview
    of Evolutionary Deep Learning â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") ä¸­æè¿°çš„ EC æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†å¦‚ä¸‹çš„ EDL é€šç”¨æ¡†æ¶ã€‚'
- en: 'Step 1 Initialization::'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¥éª¤ 1 åˆå§‹åŒ–::'
- en: A population of individuals are initialized according to the designed encoding
    scheme.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è®¾è®¡çš„ç¼–ç æ–¹æ¡ˆåˆå§‹åŒ–ä¸€ä¸ªä¸ªä½“ç¾¤ä½“ã€‚
- en: 'Step 2 Evaluation::'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¥éª¤ 2 è¯„ä¼°::'
- en: Each individual is evaluated according to the objectives (e.g., high accuracy,
    small model size) or constraints (e.g., energy consumption).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œé«˜å‡†ç¡®ç‡ã€å°æ¨¡å‹å°ºå¯¸ï¼‰æˆ–çº¦æŸï¼ˆä¾‹å¦‚ï¼Œèƒ½è€—ï¼‰è¯„ä¼°æ¯ä¸ªä¸ªä½“ã€‚
- en: 'Step 3 Updating::'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¥éª¤ 3 æ›´æ–°::'
- en: A required number of new solutions are generated from previous generation via
    various updating operations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šä¸€ä»£ç”Ÿæˆæ‰€éœ€æ•°é‡çš„æ–°è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å„ç§æ›´æ–°æ“ä½œã€‚
- en: 'Step 4 Termination condition::'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¥éª¤ 4 ç»ˆæ­¢æ¡ä»¶::'
- en: Go to Step 2 if the predefined termination condition is unsatisfied; Otherwise,
    go to Step 5.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœªæ»¡è¶³é¢„å®šä¹‰çš„ç»ˆæ­¢æ¡ä»¶ï¼Œåˆ™è¿›å…¥æ­¥éª¤ 2ï¼›å¦åˆ™ï¼Œè¿›å…¥æ­¥éª¤ 5ã€‚
- en: 'Step 5 Output::'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¥éª¤ 5 è¾“å‡º::'
- en: Output the solution with the best performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ€§èƒ½æœ€ä½³çš„è§£å†³æ–¹æ¡ˆã€‚
- en: 2.3.3\. Taxonomy of EDL Approaches
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. EDL æ–¹æ³•çš„åˆ†ç±»
- en: 'In this section, a novel taxonomy of EDL approaches is proposed according to
    â€œwhat to evolve/optimizeâ€ and â€œhow to evolve/optimizeâ€, as shown in Fig. [4](#S2.F4
    "Figure 4 â€£ 2.3.3\. Taxonomy of EDL Approaches â€£ 2.3\. Evolutionary Deep Learning
    â€£ 2\. An Overview of Evolutionary Deep Learning â€£ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ¬èŠ‚æ ¹æ®â€œè¦è¿›åŒ–/ä¼˜åŒ–çš„å†…å®¹â€å’Œâ€œå¦‚ä½•è¿›åŒ–/ä¼˜åŒ–â€æå‡ºäº†ä¸€ç§æ–°çš„ EDL æ–¹æ³•åˆ†ç±»ï¼Œå¦‚å›¾ [4](#S2.F4 "Figure 4 â€£ 2.3.3\.
    Taxonomy of EDL Approaches â€£ 2.3\. Evolutionary Deep Learning â€£ 2\. An Overview
    of Evolutionary Deep Learning â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") æ‰€ç¤ºã€‚'
- en: '![Refer to caption](img/dc42ccf964acca317096d7d4d08a8c05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/dc42ccf964acca317096d7d4d08a8c05.png)'
- en: Figure 4. A taxonomy of EDL approaches.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4. EDL æ–¹æ³•çš„åˆ†ç±»ã€‚
- en: 'â€œWhat to evolve/optimizeâ€: We may be concerned about â€œwhat EDL can doâ€ or â€œwhat
    kinds of problems EDL can tackleâ€. In feature engineering, there are three key
    issues to be resolved, including the feature selection, feature construction and
    feature extraction (Yao etÂ al., [2018](#bib.bib231)). In model generation, parameter
    optimization, architecture optimization, and joint optimization become the critical
    issues (Zhou etÂ al., [2021a](#bib.bib258)), while model deployment is involved
    with the issues of model pruning and other compression technologies.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: â€œè¦è¿›åŒ–/ä¼˜åŒ–çš„å†…å®¹â€ï¼šæˆ‘ä»¬å¯èƒ½å…³æ³¨â€œEDL èƒ½åšä»€ä¹ˆâ€æˆ–â€œEDL å¯ä»¥è§£å†³å“ªäº›é—®é¢˜â€ã€‚åœ¨ç‰¹å¾å·¥ç¨‹ä¸­ï¼Œæœ‰ä¸‰ä¸ªå…³é”®é—®é¢˜éœ€è¦è§£å†³ï¼ŒåŒ…æ‹¬ç‰¹å¾é€‰æ‹©ã€ç‰¹å¾æ„é€ å’Œç‰¹å¾æå–
    (Yao et al., [2018](#bib.bib231))ã€‚åœ¨æ¨¡å‹ç”Ÿæˆä¸­ï¼Œå‚æ•°ä¼˜åŒ–ã€æ¶æ„ä¼˜åŒ–å’Œè”åˆä¼˜åŒ–æˆä¸ºå…³é”®é—®é¢˜ (Zhou et al., [2021a](#bib.bib258))ï¼Œè€Œæ¨¡å‹éƒ¨ç½²æ¶‰åŠæ¨¡å‹å‰ªæå’Œå…¶ä»–å‹ç¼©æŠ€æœ¯çš„é—®é¢˜ã€‚
- en: 'â€œHow to evolve/optimizeâ€: The answer to the question is designing appropriate
    solution representation and search paradigm for EC, and acceleration strategies
    for NAS. The representation schemes are designed for the encoding of individuals,
    search paradigms for the achievement of optimal configurations, acceleration strategies
    for the reduction of time or resources consumption.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå¦‚ä½•è¿›åŒ–/ä¼˜åŒ–â€ï¼šè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆæ˜¯ä¸ºECè®¾è®¡é€‚å½“çš„è§£å†³æ–¹æ¡ˆè¡¨ç¤ºå’Œæœç´¢èŒƒå¼ï¼Œä»¥åŠNASçš„åŠ é€Ÿç­–ç•¥ã€‚è¡¨ç¤ºæ–¹æ¡ˆç”¨äºä¸ªä½“çš„ç¼–ç ï¼Œæœç´¢èŒƒå¼ç”¨äºå®ç°æœ€ä½³é…ç½®ï¼ŒåŠ é€Ÿç­–ç•¥ç”¨äºå‡å°‘æ—¶é—´æˆ–èµ„æºæ¶ˆè€—ã€‚
- en: 'According to the above taxonomy, we will elaborately introduce EDL in feature
    engineering, model generation and model deployment in Sections [3](#S3 "3\. Feature
    Engineering â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues"), [4](#S4 "4\. Model Generation â€£ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues") and [5](#S5 "5\.
    Model Deployment â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues"), respectively.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä¸Šè¿°åˆ†ç±»ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬[3](#S3 "3\. ç‰¹å¾å·¥ç¨‹ â€£ è¿›åŒ–æ·±åº¦å­¦ä¹ ç»¼è¿°ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜")ã€[4](#S4 "4\. æ¨¡å‹ç”Ÿæˆ â€£
    è¿›åŒ–æ·±åº¦å­¦ä¹ ç»¼è¿°ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜")å’Œ[5](#S5 "5\. æ¨¡å‹éƒ¨ç½² â€£ è¿›åŒ–æ·±åº¦å­¦ä¹ ç»¼è¿°ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜")èŠ‚ä¸­è¯¦ç»†ä»‹ç»ç‰¹å¾å·¥ç¨‹ä¸­çš„EDLã€æ¨¡å‹ç”Ÿæˆå’Œæ¨¡å‹éƒ¨ç½²ã€‚
- en: 3\. Feature Engineering
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. ç‰¹å¾å·¥ç¨‹
- en: Feature engineering is adopted to pre-process given raw data by filtering out
    the irrelevant features of the data or creating the new features based on original
    features (Xue etÂ al., [2015](#bib.bib226)). Various EC-based techniques have been
    proposed to reduce data dimensionality, speed up learning process, or improve
    model performance (Xue etÂ al., [2015](#bib.bib226)). The common techniques can
    be categorized into feature selection (Nguyen etÂ al., [2014](#bib.bib147)), feature
    construction (Bhanu and Krawiec, [2002](#bib.bib17)) and feature extraction (Peng
    etÂ al., [2021](#bib.bib153)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å·¥ç¨‹ç”¨äºé€šè¿‡ç­›é€‰æ•°æ®ä¸­çš„æ— å…³ç‰¹å¾æˆ–åŸºäºåŸå§‹ç‰¹å¾åˆ›å»ºæ–°ç‰¹å¾æ¥é¢„å¤„ç†ç»™å®šçš„åŸå§‹æ•°æ®ï¼ˆXue ç­‰ï¼Œ[2015](#bib.bib226)ï¼‰ã€‚å·²ç»æå‡ºäº†å„ç§åŸºäºè¿›åŒ–è®¡ç®—ï¼ˆECï¼‰çš„æŠ€æœ¯ï¼Œä»¥å‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹æˆ–æé«˜æ¨¡å‹æ€§èƒ½ï¼ˆXue
    ç­‰ï¼Œ[2015](#bib.bib226)ï¼‰ã€‚å¸¸è§çš„æŠ€æœ¯å¯ä»¥åˆ†ä¸ºç‰¹å¾é€‰æ‹©ï¼ˆNguyen ç­‰ï¼Œ[2014](#bib.bib147)ï¼‰ã€ç‰¹å¾æ„é€ ï¼ˆBhanu
    å’Œ Krawiecï¼Œ[2002](#bib.bib17)ï¼‰å’Œç‰¹å¾æå–ï¼ˆPeng ç­‰ï¼Œ[2021](#bib.bib153)ï¼‰ã€‚
- en: 3.1\. Feature Selection
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. ç‰¹å¾é€‰æ‹©
- en: 3.1.1\. Problem Formulation
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. é—®é¢˜è¡¨è¿°
- en: Feature selection aims to automatically select a representative subset of features
    where there are no irrelevant or redundant features. However, the search space
    grows exponentially with the increase of features. If a dataset has $n$ features,
    then there are $2^{n}$ solutions in the search space. In addition, the interactions
    between features may seriously impact the feature selection performance (Xue etÂ al.,
    [2015](#bib.bib226)). In the followings, we will review existing work on solution
    representations and search paradigms in EC for feature selection.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾é€‰æ‹©æ—¨åœ¨è‡ªåŠ¨é€‰æ‹©ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„ç‰¹å¾å­é›†ï¼Œå…¶ä¸­æ²¡æœ‰æ— å…³æˆ–å†—ä½™çš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œéšç€ç‰¹å¾æ•°é‡çš„å¢åŠ ï¼Œæœç´¢ç©ºé—´å‘ˆæŒ‡æ•°å¢é•¿ã€‚å¦‚æœä¸€ä¸ªæ•°æ®é›†æœ‰ $n$ ä¸ªç‰¹å¾ï¼Œé‚£ä¹ˆæœç´¢ç©ºé—´ä¸­æœ‰
    $2^{n}$ ä¸ªè§£ã€‚æ­¤å¤–ï¼Œç‰¹å¾ä¹‹é—´çš„ç›¸äº’ä½œç”¨å¯èƒ½ä¼šä¸¥é‡å½±å“ç‰¹å¾é€‰æ‹©æ€§èƒ½ï¼ˆXue ç­‰ï¼Œ[2015](#bib.bib226)ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å›é¡¾åœ¨ECä¸­ç”¨äºç‰¹å¾é€‰æ‹©çš„è§£å†³æ–¹æ¡ˆè¡¨ç¤ºå’Œæœç´¢èŒƒå¼çš„ç°æœ‰å·¥ä½œã€‚
- en: 3.1.2\. Solution Representations
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. è§£å†³æ–¹æ¡ˆè¡¨ç¤º
- en: Generally, there are three different categories of solution representations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæœ‰ä¸‰ç§ä¸åŒçš„è§£å†³æ–¹æ¡ˆè¡¨ç¤ºç±»åˆ«ã€‚
- en: 'Linear encoding: This encoding uses vectors or strings to store feature information.
    For example, in (EstÃ©vez and Caballero, [1998](#bib.bib52)), a fixed-length binary
    vector was used to express whether a feature is selected or not, where â€œ1â€ indicates
    a corresponding feature is selected, and â€œ0â€ is the opposite. In (Hong and Cho,
    [2006](#bib.bib75)), a binary index was used to indicate the corresponding feature.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§ç¼–ç ï¼šè¿™ç§ç¼–ç ä½¿ç”¨å‘é‡æˆ–å­—ç¬¦ä¸²æ¥å­˜å‚¨ç‰¹å¾ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨ï¼ˆEstÃ©vez å’Œ Caballeroï¼Œ[1998](#bib.bib52)ï¼‰ä¸­ï¼Œä½¿ç”¨å›ºå®šé•¿åº¦çš„äºŒè¿›åˆ¶å‘é‡æ¥è¡¨ç¤ºç‰¹å¾æ˜¯å¦è¢«é€‰æ‹©ï¼Œå…¶ä¸­â€œ1â€è¡¨ç¤ºå¯¹åº”çš„ç‰¹å¾è¢«é€‰æ‹©ï¼Œâ€œ0â€åˆ™è¡¨ç¤ºç›¸åã€‚åœ¨ï¼ˆHong
    å’Œ Choï¼Œ[2006](#bib.bib75)ï¼‰ä¸­ï¼Œä½¿ç”¨äºŒè¿›åˆ¶ç´¢å¼•æ¥è¡¨ç¤ºå¯¹åº”çš„ç‰¹å¾ã€‚
- en: 'Tree-based encoding: In canonical genetic programming (GP), all leaf nodes/terminal
    nodes represent the selected features and non-terminal nodes represent functions
    (e.g., arithmetic or logic operators) (Krawiec, [2002](#bib.bib101)). For automatic
    classification on high-dimensional data, Krawiec et al. (Krawiec, [2002](#bib.bib101))
    proposed a tree-based encoding to select a subset of highly discriminative features,
    where each feature consisted of sibling leaf nodes and their paternal function
    node. On the basis of the tree-based encoding, Muni et al. (Muni etÂ al., [2006](#bib.bib141))
    proposed a multi-tree GP mothed for online feature selection.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºäºæ ‘çš„ç¼–ç **ï¼šåœ¨ç»å…¸é—ä¼ ç¼–ç¨‹ï¼ˆGPï¼‰ä¸­ï¼Œæ‰€æœ‰å¶èŠ‚ç‚¹/ç»ˆç«¯èŠ‚ç‚¹ä»£è¡¨æ‰€é€‰ç‰¹å¾ï¼Œè€Œéç»ˆç«¯èŠ‚ç‚¹åˆ™ä»£è¡¨å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œç®—æœ¯æˆ–é€»è¾‘è¿ç®—ç¬¦ï¼‰ï¼ˆKrawiecï¼Œ[2002](#bib.bib101)ï¼‰ã€‚é’ˆå¯¹é«˜ç»´æ•°æ®çš„è‡ªåŠ¨åˆ†ç±»ï¼ŒKrawiec
    ç­‰äººï¼ˆKrawiecï¼Œ[2002](#bib.bib101)ï¼‰æå‡ºäº†ä¸€ç§åŸºäºæ ‘çš„ç¼–ç æ–¹æ³•ï¼Œç”¨äºé€‰æ‹©å…·æœ‰é«˜åº¦åŒºåˆ†æ€§çš„ç‰¹å¾å­é›†ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾ç”±å…„å¼Ÿå¶èŠ‚ç‚¹åŠå…¶çˆ¶åŠŸèƒ½èŠ‚ç‚¹ç»„æˆã€‚åœ¨åŸºäºæ ‘çš„ç¼–ç çš„åŸºç¡€ä¸Šï¼ŒMuni
    ç­‰äººï¼ˆMuni ç­‰ï¼Œ[2006](#bib.bib141)ï¼‰æå‡ºäº†ä¸€ç§ç”¨äºåœ¨çº¿ç‰¹å¾é€‰æ‹©çš„å¤šæ ‘ GP æ–¹æ³•ã€‚'
- en: 'Graph-based encoding: In (Oâ€™Boyle and Palmer, [2008](#bib.bib148)), the feature
    space of the high-dimensional data is represented by a graph and each node of
    the graph represents a feature. A feature subset is composed of visited nodes
    of the graph, i.e., the path of node composition or subgraph. Yu et al. (Yu etÂ al.,
    [2009](#bib.bib236)) converted feature selection to the optimal path problem in
    a directed graph, where the value of the node was â€œ1â€ or â€œ0â€ to indicate whether
    the feature was selected or not.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºäºå›¾çš„ç¼–ç **ï¼šåœ¨ï¼ˆOâ€™Boyle å’Œ Palmerï¼Œ[2008](#bib.bib148)ï¼‰ä¸­ï¼Œé«˜ç»´æ•°æ®çš„ç‰¹å¾ç©ºé—´ç”±ä¸€ä¸ªå›¾è¡¨ç¤ºï¼Œå›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‰¹å¾ã€‚ç‰¹å¾å­é›†ç”±å›¾ä¸­çš„è®¿é—®èŠ‚ç‚¹ç»„æˆï¼Œå³èŠ‚ç‚¹ç»„åˆçš„è·¯å¾„æˆ–å­å›¾ã€‚Yu
    ç­‰äººï¼ˆYu ç­‰ï¼Œ[2009](#bib.bib236)ï¼‰å°†ç‰¹å¾é€‰æ‹©è½¬åŒ–ä¸ºæœ‰å‘å›¾ä¸­çš„æœ€ä¼˜è·¯å¾„é—®é¢˜ï¼Œå…¶ä¸­èŠ‚ç‚¹çš„å€¼ä¸ºâ€œ1â€æˆ–â€œ0â€ä»¥è¡¨ç¤ºç‰¹å¾æ˜¯å¦è¢«é€‰æ‹©ã€‚'
- en: 3.1.3\. Search Paradigms
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**3.1.3 æœç´¢èŒƒå¼**'
- en: In feature selection, representative types of search paradigms are introduced
    as follows.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹å¾é€‰æ‹©ä¸­ï¼Œä»‹ç»äº†ä»¥ä¸‹å‡ ç§ä»£è¡¨æ€§çš„æœç´¢èŒƒå¼ã€‚
- en: 'Basic EC search paradigm: In feature selection, typical evolutionary search
    methods have been widely used, such as GA (DaÂ Silva and Neto, [2011](#bib.bib37);
    EstÃ©vez and Caballero, [1998](#bib.bib52)), GP (Krawiec, [2002](#bib.bib101);
    Nekrasov etÂ al., [2020](#bib.bib143)), PSO (Vieira etÂ al., [2013](#bib.bib205);
    Nguyen etÂ al., [2014](#bib.bib147)), ant colony optimization (ACO) (Khushaba etÂ al.,
    [2008](#bib.bib96); Ma etÂ al., [2021c](#bib.bib131)), and artificial bee colony
    (ABC) (Wang etÂ al., [2020d](#bib.bib211)). Besides, some other studies (Khushaba
    etÂ al., [2008](#bib.bib96)) combined ACO with DE to seek optimal feature subsets,
    where the solutions searched by the ACO were fed into the DE to further explore
    the optimal solution. In (DaÂ Silva and Neto, [2011](#bib.bib37)), a family of
    feature selection methods based on different variants of GA were developed to
    improve the accuracy of content-based image retrieval systems.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºæœ¬è¿›åŒ–è®¡ç®—æœç´¢èŒƒå¼**ï¼šåœ¨ç‰¹å¾é€‰æ‹©ä¸­ï¼Œå·²ç»å¹¿æ³›ä½¿ç”¨äº†å…¸å‹çš„è¿›åŒ–æœç´¢æ–¹æ³•ï¼Œå¦‚ GAï¼ˆDa Silva å’Œ Netoï¼Œ[2011](#bib.bib37)ï¼›EstÃ©vez
    å’Œ Caballeroï¼Œ[1998](#bib.bib52)ï¼‰ï¼ŒGPï¼ˆKrawiecï¼Œ[2002](#bib.bib101)ï¼›Nekrasov ç­‰ï¼Œ[2020](#bib.bib143)ï¼‰ï¼ŒPSOï¼ˆVieira
    ç­‰ï¼Œ[2013](#bib.bib205)ï¼›Nguyen ç­‰ï¼Œ[2014](#bib.bib147)ï¼‰ï¼Œèšç¾¤ä¼˜åŒ–ï¼ˆACOï¼‰ï¼ˆKhushaba ç­‰ï¼Œ[2008](#bib.bib96)ï¼›Ma
    ç­‰ï¼Œ[2021c](#bib.bib131)ï¼‰ï¼Œä»¥åŠäººå·¥èœ‚ç¾¤ï¼ˆABCï¼‰ï¼ˆWang ç­‰ï¼Œ[2020d](#bib.bib211)ï¼‰ã€‚æ­¤å¤–ï¼Œä¸€äº›å…¶ä»–ç ”ç©¶ï¼ˆKhushaba
    ç­‰ï¼Œ[2008](#bib.bib96)ï¼‰å°† ACO ä¸ DE ç»“åˆä½¿ç”¨ä»¥å¯»æ‰¾æœ€ä¼˜ç‰¹å¾å­é›†ï¼Œå…¶ä¸­ ACO æœç´¢åˆ°çš„è§£è¢«è¾“å…¥åˆ° DE ä¸­ä»¥è¿›ä¸€æ­¥æ¢ç´¢æœ€ä¼˜è§£ã€‚åœ¨ï¼ˆDa
    Silva å’Œ Netoï¼Œ[2011](#bib.bib37)ï¼‰ä¸­ï¼Œå¼€å‘äº†ä¸€ç³»åˆ—åŸºäºä¸åŒå˜ç§çš„ GA çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œä»¥æé«˜åŸºäºå†…å®¹çš„å›¾åƒæ£€ç´¢ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚'
- en: 'Co-evolution search paradigm: In co-evolution search paradigm for feature selection,
    at least two populations are simultaneously evolved and interacted toward the
    optimal subset of features (Wen and Xu, [2011](#bib.bib214); Rashid etÂ al., [2020](#bib.bib160)).
    For example, a divide-and-conquer strategy was developed in (Wen and Xu, [2011](#bib.bib214))
    to manage two subpopulations. One subpopulation was to conduct an evolution process
    of classifier design, while the other one was to search for an optimal subset
    of features.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**ååŒè¿›åŒ–æœç´¢èŒƒå¼**ï¼šåœ¨ç‰¹å¾é€‰æ‹©çš„ååŒè¿›åŒ–æœç´¢èŒƒå¼ä¸­ï¼Œè‡³å°‘æœ‰ä¸¤ä¸ªç§ç¾¤åŒæ—¶è¿›è¡Œè¿›åŒ–å¹¶ç›¸äº’ä½œç”¨ï¼Œä»¥æ‰¾åˆ°æœ€ä¼˜çš„ç‰¹å¾å­é›†ï¼ˆWen å’Œ Xuï¼Œ[2011](#bib.bib214)ï¼›Rashid
    ç­‰ï¼Œ[2020](#bib.bib160)ï¼‰ã€‚ä¾‹å¦‚ï¼Œï¼ˆWen å’Œ Xuï¼Œ[2011](#bib.bib214)ï¼‰ä¸­å¼€å‘äº†ä¸€ç§åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥æ¥ç®¡ç†ä¸¤ä¸ªå­ç§ç¾¤ã€‚ä¸€ä¸ªå­ç§ç¾¤è´Ÿè´£è¿›è¡Œåˆ†ç±»å™¨è®¾è®¡çš„è¿›åŒ–è¿‡ç¨‹ï¼Œè€Œå¦ä¸€ä¸ªå­ç§ç¾¤åˆ™è´Ÿè´£æœç´¢æœ€ä¼˜çš„ç‰¹å¾å­é›†ã€‚'
- en: 'Multi-objective search paradigm: This type of search paradigms are driven by
    two or more conflicting objectives (Xue etÂ al., [2012](#bib.bib225); Hancer etÂ al.,
    [2015](#bib.bib71); Cheng etÂ al., [2021](#bib.bib29)), such as the maximization
    of the accuracy of a classifier and minimization of the size of a feature subset.
    On the basis of the above two conflicting objectives, Xue et al. (Xue etÂ al.,
    [2012](#bib.bib225)) designed a multi-objective PSO algorithm for feature selection
    and obtained a set of Pareto non-dominated candidate solutions for feature selection
    after the multi-objective search.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šç›®æ ‡æœç´¢èŒƒå¼ï¼šè¿™ç±»æœç´¢èŒƒå¼ç”±ä¸¤ä¸ªæˆ–æ›´å¤šå†²çªçš„ç›®æ ‡é©±åŠ¨ï¼ˆXue et al., [2012](#bib.bib225); Hancer et al.,
    [2015](#bib.bib71); Cheng et al., [2021](#bib.bib29)ï¼‰ï¼Œä¾‹å¦‚åˆ†ç±»å™¨å‡†ç¡®åº¦çš„æœ€å¤§åŒ–å’Œç‰¹å¾å­é›†å¤§å°çš„æœ€å°åŒ–ã€‚åœ¨è¿™ä¸¤ä¸ªå†²çªç›®æ ‡çš„åŸºç¡€ä¸Šï¼ŒXue
    et al.ï¼ˆXue et al., [2012](#bib.bib225)ï¼‰è®¾è®¡äº†ä¸€ä¸ªå¤šç›®æ ‡PSOç®—æ³•ç”¨äºç‰¹å¾é€‰æ‹©ï¼Œå¹¶åœ¨å¤šç›®æ ‡æœç´¢åè·å¾—äº†ä¸€ç»„å¸•ç´¯æ‰˜éæ”¯é…çš„å€™é€‰è§£ç”¨äºç‰¹å¾é€‰æ‹©ã€‚
- en: 3.1.4\. Summary
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. æ€»ç»“
- en: GA and GP are widely applied to feature selection. GA early serves for low-dimensional
    (i.e., $\leq$1000) datasets (Xue etÂ al., [2015](#bib.bib226); Hosni etÂ al., [2020](#bib.bib77)).
    Recently, many GA-based approaches have been proposed to solve high-dimensional
    feature selection (Cheng etÂ al., [2021](#bib.bib29)). Nevertheless, GP is commonly
    applied to large-scale/high-dimensional feature selection since it is flexible
    in feature representation (Xue etÂ al., [2015](#bib.bib226)). Especially, GP outperforms
    GA on some small but high-dimensional datasets, e.g., Brain Tumor-2 (Chen etÂ al.,
    [2022](#bib.bib24)) with 10367 features but only 50 samples. In addition, PSO
    has been proved with faster convergence rate to an optimal feature subset than
    GAs and GP (Zhang etÂ al., [2017](#bib.bib251)). The graph representation of ACO
    outperforms GA and GP on flexibility, but the challenge of ACO is how to design
    appropriate graph encoding for large-scale scenarios (Telikani etÂ al., [2021](#bib.bib197);
    Xue etÂ al., [2015](#bib.bib226)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: GAå’ŒGPå¹¿æ³›åº”ç”¨äºç‰¹å¾é€‰æ‹©ã€‚GAæ—©æœŸç”¨äºä½ç»´ï¼ˆå³ï¼Œ$\leq$1000ï¼‰æ•°æ®é›†ï¼ˆXue et al., [2015](#bib.bib226); Hosni
    et al., [2020](#bib.bib77)ï¼‰ã€‚æœ€è¿‘ï¼Œè®¸å¤šåŸºäºGAçš„æ–¹æ³•è¢«æå‡ºç”¨äºè§£å†³é«˜ç»´ç‰¹å¾é€‰æ‹©ï¼ˆCheng et al., [2021](#bib.bib29)ï¼‰ã€‚ç„¶è€Œï¼ŒGPé€šå¸¸åº”ç”¨äºå¤§è§„æ¨¡/é«˜ç»´ç‰¹å¾é€‰æ‹©ï¼Œå› ä¸ºå®ƒåœ¨ç‰¹å¾è¡¨ç¤ºä¸Šå…·æœ‰çµæ´»æ€§ï¼ˆXue
    et al., [2015](#bib.bib226)ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼ŒGPåœ¨ä¸€äº›å°ä½†é«˜ç»´çš„æ•°æ®é›†ä¸Šä¼˜äºGAï¼Œä¾‹å¦‚ï¼Œå…·æœ‰10367ä¸ªç‰¹å¾ä½†ä»…50ä¸ªæ ·æœ¬çš„Brain
    Tumor-2ï¼ˆChen et al., [2022](#bib.bib24)ï¼‰ã€‚æ­¤å¤–ï¼ŒPSOå·²è¢«è¯æ˜æ¯”GAå’ŒGPå…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼ˆZhang et al.,
    [2017](#bib.bib251)ï¼‰ã€‚ACOçš„å›¾è¡¨ç¤ºåœ¨çµæ´»æ€§ä¸Šä¼˜äºGAå’ŒGPï¼Œä½†ACOçš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•ä¸ºå¤§è§„æ¨¡åœºæ™¯è®¾è®¡åˆé€‚çš„å›¾ç¼–ç ï¼ˆTelikani et
    al., [2021](#bib.bib197); Xue et al., [2015](#bib.bib226)ï¼‰ã€‚
- en: 3.2\. Feature Construction
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. ç‰¹å¾æ„å»º
- en: 3.2.1\. Problem Formulation
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. é—®é¢˜è¡¨è¿°
- en: Feature construction is to create new high-level features from the original
    features (Tran etÂ al., [2016](#bib.bib202)) via appropriate function operators
    (e.g., conjunction and average) (Xue etÂ al., [2013](#bib.bib227); Neshatian etÂ al.,
    [2012](#bib.bib145)), so that the high-level features are more easily discriminative
    than the original ones. Feature construction is a complicated combinatorial optimization
    problem, where search space increases exponentially along with the total number
    of original features and the function operators. In the following subsections,
    we will describe the EC-based feature construction methods in terms of both solution
    representations and search paradigms.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾æ„å»ºæ˜¯é€šè¿‡é€‚å½“çš„å‡½æ•°æ“ä½œç¬¦ï¼ˆä¾‹å¦‚ï¼Œè¿æ¥å’Œå¹³å‡ï¼‰ï¼ˆXue et al., [2013](#bib.bib227); Neshatian et al.,
    [2012](#bib.bib145)ï¼‰ä»åŸå§‹ç‰¹å¾ï¼ˆTran et al., [2016](#bib.bib202)ï¼‰åˆ›å»ºæ–°çš„é«˜çº§ç‰¹å¾ï¼Œä½¿å¾—é«˜çº§ç‰¹å¾æ¯”åŸå§‹ç‰¹å¾æ›´å®¹æ˜“åŒºåˆ†ã€‚ç‰¹å¾æ„å»ºæ˜¯ä¸€ä¸ªå¤æ‚çš„ç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­æœç´¢ç©ºé—´éšç€åŸå§‹ç‰¹å¾å’Œå‡½æ•°æ“ä½œç¬¦çš„æ€»æ•°å‘ˆæŒ‡æ•°å¢é•¿ã€‚åœ¨ä»¥ä¸‹å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æè¿°åŸºäºECçš„ç‰¹å¾æ„å»ºæ–¹æ³•ï¼ŒåŒ…æ‹¬è§£çš„è¡¨ç¤ºå’Œæœç´¢èŒƒå¼ã€‚
- en: 3.2.2\. Solution Representations
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. è§£çš„è¡¨ç¤º
- en: Existing EC-based approaches for feature construction can be categorized into
    three groups.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ç°æœ‰çš„åŸºäºECçš„æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸‰ç±»ã€‚
- en: 'Linear encoding: The study (Xue etÂ al., [2013](#bib.bib227)) used $n$-bit ($n$
    is the total number of original features) binary vector to represent each particle,
    where â€œ0â€ indicated the corresponding feature not applied to build the new high-level
    feature while â€œ1â€ was in the opposite. On the basis of the encoding, a local search
    was performed to select candidate operators from a predefined function set to
    construct a new high-level feature.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§ç¼–ç ï¼šç ”ç©¶ï¼ˆXue et al., [2013](#bib.bib227)ï¼‰ä½¿ç”¨$n$-bitï¼ˆ$n$æ˜¯åŸå§‹ç‰¹å¾çš„æ€»æ•°ï¼‰äºŒè¿›åˆ¶å‘é‡æ¥è¡¨ç¤ºæ¯ä¸ªç²’å­ï¼Œå…¶ä¸­â€œ0â€è¡¨ç¤ºç›¸åº”çš„ç‰¹å¾æœªè¢«ç”¨äºæ„å»ºæ–°çš„é«˜çº§ç‰¹å¾ï¼Œè€Œâ€œ1â€åˆ™ç›¸åã€‚åœ¨æ­¤ç¼–ç çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œäº†å±€éƒ¨æœç´¢ï¼Œä»é¢„å®šä¹‰çš„å‡½æ•°é›†ä¸­é€‰æ‹©å€™é€‰æ“ä½œç¬¦ä»¥æ„å»ºæ–°çš„é«˜çº§ç‰¹å¾ã€‚
- en: 'Tree-based encoding: Tree-based encoding is natural for feature construction,
    where leaf nodes represent the feature information and internal nodes represent
    operators. Many studies (Tran etÂ al., [2016](#bib.bib202); Bhanu and Krawiec,
    [2002](#bib.bib17)) have demonstrated the effectiveness of tree encoding in feature
    construction. For example, Bhanu et al. (Bhanu and Krawiec, [2002](#bib.bib17))
    designed a GP-based coevolutionary feature construction procedure to improve the
    discriminative ability of classifiers. In (Tran etÂ al., [2016](#bib.bib202)),
    an individual in EC was represented by a multi-tree encoding with multiple high-level
    features.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ ‘çš„ç¼–ç ï¼šåŸºäºæ ‘çš„ç¼–ç å¯¹ç‰¹å¾æ„å»ºæ¥è¯´æ˜¯è‡ªç„¶çš„ï¼Œå…¶ä¸­å¶èŠ‚ç‚¹è¡¨ç¤ºç‰¹å¾ä¿¡æ¯ï¼Œå†…éƒ¨èŠ‚ç‚¹è¡¨ç¤ºæ“ä½œç¬¦ã€‚è®¸å¤šç ”ç©¶ï¼ˆTranç­‰ï¼Œ[2016](#bib.bib202)ï¼›Bhanuå’ŒKrawiecï¼Œ[2002](#bib.bib17)ï¼‰å·²ç»è¯æ˜äº†æ ‘ç¼–ç åœ¨ç‰¹å¾æ„å»ºä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼ŒBhanuç­‰ï¼ˆBhanuå’ŒKrawiecï¼Œ[2002](#bib.bib17)ï¼‰è®¾è®¡äº†ä¸€ç§åŸºäºGPçš„ååŒè¿›åŒ–ç‰¹å¾æ„å»ºç¨‹åºï¼Œä»¥æé«˜åˆ†ç±»å™¨çš„åŒºåˆ†èƒ½åŠ›ã€‚åœ¨ï¼ˆTranç­‰ï¼Œ[2016](#bib.bib202)ï¼‰ä¸­ï¼ŒECä¸­çš„ä¸€ä¸ªä¸ªä½“ç”±å¤šæ£µæ ‘ç¼–ç è¡¨ç¤ºï¼Œå…·æœ‰å¤šä¸ªé«˜çº§ç‰¹å¾ã€‚
- en: 'Graph-based encoding: In this encoding, the nodes and edges represent features
    and operators (e.g., â€œ+â€, â€œ-â€, â€œ*â€, â€œ/â€), respectively. Teller et al. (Teller
    and Veloso, [1996](#bib.bib198)) applied an arbitrary directed graph to represent
    all features and operators, where each possible high-level feature can be represented
    as a subgraph of this directed graph. For linear GP, features and operations form
    a many-to-many directed acyclic graph, in which each feature is loaded into predefined
    registers and registerâ€™s value can be used in multiple operators (Fogelberg and
    Zhang, [2005](#bib.bib58)). However, graph encoding becomes inefficient on high-dimensional
    feature sets since the complexity of graph traversal exacerbates the difficulty
    of feature construction.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå›¾çš„ç¼–ç ï¼šåœ¨è¿™ç§ç¼–ç ä¸­ï¼ŒèŠ‚ç‚¹å’Œè¾¹åˆ†åˆ«è¡¨ç¤ºç‰¹å¾å’Œæ“ä½œç¬¦ï¼ˆä¾‹å¦‚ï¼Œâ€œ+â€ï¼Œâ€œ -â€ï¼Œâ€œ*â€ï¼Œâ€œ/â€ï¼‰ã€‚Tellerç­‰ï¼ˆTellerå’ŒVelosoï¼Œ[1996](#bib.bib198)ï¼‰åº”ç”¨ä»»æ„æœ‰å‘å›¾è¡¨ç¤ºæ‰€æœ‰ç‰¹å¾å’Œæ“ä½œç¬¦ï¼Œå…¶ä¸­æ¯ä¸ªå¯èƒ½çš„é«˜çº§ç‰¹å¾å¯ä»¥è¡¨ç¤ºä¸ºè¿™ä¸ªæœ‰å‘å›¾çš„ä¸€ä¸ªå­å›¾ã€‚å¯¹äºçº¿æ€§GPï¼Œç‰¹å¾å’Œæ“ä½œå½¢æˆä¸€ä¸ªå¤šå¯¹å¤šçš„æœ‰å‘æ— ç¯å›¾ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾è¢«åŠ è½½åˆ°é¢„å®šä¹‰çš„å¯„å­˜å™¨ä¸­ï¼Œå¯„å­˜å™¨çš„å€¼å¯ä»¥åœ¨å¤šä¸ªæ“ä½œç¬¦ä¸­ä½¿ç”¨ï¼ˆFogelbergå’ŒZhangï¼Œ[2005](#bib.bib58)ï¼‰ã€‚ç„¶è€Œï¼Œå›¾ç¼–ç åœ¨é«˜ç»´ç‰¹å¾é›†ä¸Šå˜å¾—ä½æ•ˆï¼Œå› ä¸ºå›¾éå†çš„å¤æ‚æ€§åŠ å‰§äº†ç‰¹å¾æ„å»ºçš„éš¾åº¦ã€‚
- en: 3.2.3\. Search Paradigms
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 æœç´¢èŒƒå¼
- en: There are four categories of search paradigms for feature construction in existing
    work .
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç°æœ‰ç ”ç©¶ä¸­çš„ç‰¹å¾æ„å»ºæœ‰å››ç§æœç´¢èŒƒå¼ã€‚
- en: 'Basic EC search paradigm: Existing studies include but are not limited to GA
    (Vafaie and DeÂ Jong, [1998](#bib.bib203)), and GP (Tran etÂ al., [2016](#bib.bib202);
    Neshatian etÂ al., [2007](#bib.bib146); Tariq etÂ al., [2018](#bib.bib196)). For
    example, the work (Neshatian etÂ al., [2007](#bib.bib146)) designed GP-based feature
    construction to reduce the feature (input) dimensions of a classifier. Especially,
    GP has been also widely used to construct new features, where each individual
    following the form of a GP tree usually represents a constructed high-level feature
    (GarcÃ­a etÂ al., [2011](#bib.bib63)).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€ECæœç´¢èŒƒå¼ï¼šç°æœ‰ç ”ç©¶åŒ…æ‹¬ä½†ä¸é™äºGAï¼ˆVafaieå’ŒDe Jongï¼Œ[1998](#bib.bib203)ï¼‰å’ŒGPï¼ˆTranç­‰ï¼Œ[2016](#bib.bib202)ï¼›Neshatianç­‰ï¼Œ[2007](#bib.bib146)ï¼›Tariqç­‰ï¼Œ[2018](#bib.bib196)ï¼‰ã€‚ä¾‹å¦‚ï¼Œå·¥ä½œï¼ˆNeshatianç­‰ï¼Œ[2007](#bib.bib146)ï¼‰è®¾è®¡äº†åŸºäºGPçš„ç‰¹å¾æ„å»ºä»¥å‡å°‘åˆ†ç±»å™¨çš„ç‰¹å¾ï¼ˆè¾“å…¥ï¼‰ç»´åº¦ã€‚ç‰¹åˆ«æ˜¯ï¼ŒGPä¹Ÿè¢«å¹¿æ³›ç”¨äºæ„å»ºæ–°ç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸ªéµå¾ªGPæ ‘å½¢å¼çš„ä¸ªä½“é€šå¸¸ä»£è¡¨ä¸€ä¸ªæ„å»ºçš„é«˜çº§ç‰¹å¾ï¼ˆGarcÃ­aç­‰ï¼Œ[2011](#bib.bib63)ï¼‰ã€‚
- en: 'Co-evolution search paradigm: It can be decomposed to feature construction
    subproblem and classifier design subproblem, and each subproblem is solved with
    a standalone subpopulation by an EC-based method (Bhanu and Krawiec, [2002](#bib.bib17);
    Roberts and Claridge, [2005](#bib.bib166)). For example, the study (Roberts and
    Claridge, [2005](#bib.bib166)) decomposed feature construction into two subproblems
    (i.e., feature construction, and object detection), where the feature construction
    was solved by evolving a population of pixel (i.e., feature) and the object detection
    was optimized using object detection algorithm (ODA) (Roberts and Claridge, [2005](#bib.bib166)).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ååŒè¿›åŒ–æœç´¢èŒƒå¼ï¼šå®ƒå¯ä»¥è¢«åˆ†è§£ä¸ºç‰¹å¾æ„å»ºå­é—®é¢˜å’Œåˆ†ç±»å™¨è®¾è®¡å­é—®é¢˜ï¼Œæ¯ä¸ªå­é—®é¢˜ç”±ECæ–¹æ³•çš„ç‹¬ç«‹å­ç§ç¾¤è§£å†³ï¼ˆBhanuå’ŒKrawiecï¼Œ[2002](#bib.bib17)ï¼›Robertså’ŒClaridgeï¼Œ[2005](#bib.bib166)ï¼‰ã€‚ä¾‹å¦‚ï¼Œç ”ç©¶ï¼ˆRobertså’ŒClaridgeï¼Œ[2005](#bib.bib166)ï¼‰å°†ç‰¹å¾æ„å»ºåˆ†è§£ä¸ºä¸¤ä¸ªå­é—®é¢˜ï¼ˆå³ç‰¹å¾æ„å»ºå’Œç›®æ ‡æ£€æµ‹ï¼‰ï¼Œå…¶ä¸­ç‰¹å¾æ„å»ºé€šè¿‡è¿›åŒ–åƒç´ ï¼ˆå³ç‰¹å¾ï¼‰ç§ç¾¤æ¥è§£å†³ï¼Œè€Œç›®æ ‡æ£€æµ‹åˆ™é€šè¿‡ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼ˆODAï¼‰ï¼ˆRobertså’ŒClaridgeï¼Œ[2005](#bib.bib166)ï¼‰è¿›è¡Œä¼˜åŒ–ã€‚
- en: 'Multi-features construction search paradigm: Unlike early methods (Tran etÂ al.,
    [2016](#bib.bib202); Vafaie and DeÂ Jong, [1998](#bib.bib203); Shafti and PÃ©rez,
    [2008](#bib.bib176); Tran etÂ al., [2019](#bib.bib201)) only constructing one high-level
    feature in a single search process, this sort of paradigms are able to create
    multiple high-level features. For example, Ahmed et al. (Ahmed etÂ al., [2014](#bib.bib4))
    employed Fisher criterion together with $p$-value measure as the discriminant
    information between classes, based on which multiple features were constructed
    through multiple GP trees.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šç‰¹å¾æ„å»ºæœç´¢èŒƒå¼ï¼šä¸æ—©æœŸæ–¹æ³•ï¼ˆTran et al., [2016](#bib.bib202); Vafaie å’Œ De Jong, [1998](#bib.bib203);
    Shafti å’Œ PÃ©rez, [2008](#bib.bib176); Tran et al., [2019](#bib.bib201)ï¼‰ä»…åœ¨å•æ¬¡æœç´¢è¿‡ç¨‹ä¸­æ„å»ºä¸€ä¸ªé«˜çº§ç‰¹å¾ä¸åŒï¼Œè¿™ç±»èŒƒå¼èƒ½å¤Ÿåˆ›å»ºå¤šä¸ªé«˜çº§ç‰¹å¾ã€‚ä¾‹å¦‚ï¼ŒAhmed
    et al.ï¼ˆAhmed et al., [2014](#bib.bib4)ï¼‰åˆ©ç”¨Fisherå‡†åˆ™å’Œ$p$-å€¼åº¦é‡ä½œä¸ºç±»åˆ«é—´çš„åˆ¤åˆ«ä¿¡æ¯ï¼ŒåŸºäºæ­¤é€šè¿‡å¤šä¸ªGPæ ‘æ„å»ºäº†å¤šä¸ªç‰¹å¾ã€‚
- en: 'Multi-objective evolutionary search paradigm: In this search paradigm, the
    number of features and classification accuracy are commonly taken into account
    as the objective functions for multi-objective evolutionary optimization (Hammami
    etÂ al., [2018](#bib.bib69); Castelli etÂ al., [2011](#bib.bib20)). Especially,
    Hammami et al. (Hammami etÂ al., [2018](#bib.bib69)) constructed a set of high-level
    features by optimizing a multi-objective optimization problem (MOP) with three
    objectives (i.e., the number of features, the mutual information, and classification
    accuracy) with Pareto dominance relationship.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šç›®æ ‡è¿›åŒ–æœç´¢èŒƒå¼ï¼šåœ¨è¿™ä¸€æœç´¢èŒƒå¼ä¸­ï¼Œç‰¹å¾æ•°é‡å’Œåˆ†ç±»å‡†ç¡®ç‡é€šå¸¸ä½œä¸ºå¤šç›®æ ‡è¿›åŒ–ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ï¼ˆHammami et al., [2018](#bib.bib69);
    Castelli et al., [2011](#bib.bib20)ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼ŒHammami et al.ï¼ˆHammami et al., [2018](#bib.bib69)ï¼‰é€šè¿‡ä¼˜åŒ–ä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªç›®æ ‡ï¼ˆå³ç‰¹å¾æ•°é‡ã€äº’ä¿¡æ¯å’Œåˆ†ç±»å‡†ç¡®ç‡ï¼‰çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼ˆMOPï¼‰æ„å»ºäº†ä¸€ç»„é«˜çº§ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨äº†å¸•ç´¯æ‰˜ä¸»å¯¼å…³ç³»ã€‚
- en: 3.2.4\. Summary
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. æ€»ç»“
- en: GP-based approaches are popular in feature construction due to the flexible
    representation of features and operations. In addition, the hybrid of evolutionary
    algorithms also attracts much attention for feature construction. However, there
    is still plenty of room for the improvement of efficiency in constructing features
    in high-dimensional or large-scale scenarios, where a large number of computational
    resources are needed (Tariq etÂ al., [2018](#bib.bib196); Tran etÂ al., [2019](#bib.bib201)).
    Notably, feature construction often requires more computational overhead than
    feature selection, since feature construction commonly performs after the feature
    selection and the quality of the selected features may influence the performance
    of feature construction.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºGPçš„æ–¹æ³•åœ¨ç‰¹å¾æ„å»ºä¸­å¾ˆå—æ¬¢è¿ï¼Œå› ä¸ºå®ƒä»¬å¯¹ç‰¹å¾å’Œæ“ä½œçš„è¡¨ç¤ºçµæ´»ã€‚æ­¤å¤–ï¼Œè¿›åŒ–ç®—æ³•çš„æ··åˆä¹Ÿå—åˆ°ç‰¹å¾æ„å»ºçš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨é«˜ç»´æˆ–å¤§è§„æ¨¡åœºæ™¯ä¸‹æ„å»ºç‰¹å¾çš„æ•ˆç‡ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼Œå› ä¸ºè¿™äº›åœºæ™¯éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼ˆTariq
    et al., [2018](#bib.bib196); Tran et al., [2019](#bib.bib201)ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç‰¹å¾æ„å»ºé€šå¸¸éœ€è¦æ¯”ç‰¹å¾é€‰æ‹©æ›´å¤šçš„è®¡ç®—å¼€é”€ï¼Œå› ä¸ºç‰¹å¾æ„å»ºé€šå¸¸åœ¨ç‰¹å¾é€‰æ‹©ä¹‹åè¿›è¡Œï¼Œæ‰€é€‰ç‰¹å¾çš„è´¨é‡å¯èƒ½ä¼šå½±å“ç‰¹å¾æ„å»ºçš„æ€§èƒ½ã€‚
- en: 3.3\. Feature Extraction
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. ç‰¹å¾æå–
- en: 3.3.1\. Problem Formulation
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. é—®é¢˜å®šä¹‰
- en: Feature extraction is to reduce the feature dimensions by altering the original
    features/data via some transformation functions (He etÂ al., [2021](#bib.bib72)).
    Traditional extractors include principal component analysis (PCA) (Abdi and Williams,
    [2010](#bib.bib2)) and linear discriminant analysis (LDA) (Izenman, [2013](#bib.bib85)).
    However, they cannot keep somewhat important information after the transformation
    (Abdi and Williams, [2010](#bib.bib2)) and it is tedious to tune their hyperparameters
    (e.g., number of retained features) to find the best extraction. Thus, automatically
    finding high-quality map functions by EC-based approaches to achieve informative
    feature set tends to be popular.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾æå–æ˜¯é€šè¿‡ä¸€äº›å˜æ¢å‡½æ•°ï¼ˆHe et al., [2021](#bib.bib72)ï¼‰æ”¹å˜åŸå§‹ç‰¹å¾/æ•°æ®ä»¥å‡å°‘ç‰¹å¾ç»´åº¦ã€‚ä¼ ç»Ÿçš„æå–æ–¹æ³•åŒ…æ‹¬ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼ˆAbdi
    å’Œ Williams, [2010](#bib.bib2)ï¼‰å’Œçº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰ï¼ˆIzenman, [2013](#bib.bib85)ï¼‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å˜æ¢åæ— æ³•ä¿ç•™ä¸€äº›é‡è¦çš„ä¿¡æ¯ï¼ˆAbdi
    å’Œ Williams, [2010](#bib.bib2)ï¼‰ï¼Œå¹¶ä¸”è°ƒæ•´å®ƒä»¬çš„è¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œä¿ç•™ç‰¹å¾çš„æ•°é‡ï¼‰ä»¥æ‰¾åˆ°æœ€ä½³æå–æ–¹æ³•éå¸¸ç¹çã€‚å› æ­¤ï¼Œé€šè¿‡åŸºäºECçš„æ–¹æ³•è‡ªåŠ¨å¯»æ‰¾é«˜è´¨é‡çš„æ˜ å°„å‡½æ•°ä»¥è·å¾—ä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾é›†è¶‹å‘äºæ›´å—æ¬¢è¿ã€‚
- en: 3.3.2\. Solution Representations
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. è§£è¡¨ç¤º
- en: There are two typical ways for solution representation in EC-driven feature
    extraction.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ECé©±åŠ¨çš„ç‰¹å¾æå–ä¸­ï¼Œæœ‰ä¸¤ç§å…¸å‹çš„è§£è¡¨ç¤ºæ–¹å¼ã€‚
- en: 'Linear encoding: In this encoding, map functions (Refahi etÂ al., [2020](#bib.bib164);
    Albukhanajer etÂ al., [2015](#bib.bib7)) or function parameters (Zhao etÂ al., [2006](#bib.bib255))
    are encoded as a linear format. For example, Wissam et al. (Albukhanajer etÂ al.,
    [2015](#bib.bib7)) predefined three sets of track functions (i.e., trace functions,
    diametric functions, and circus functions) for feature extraction, and the optimal
    combination between the functions were obtained by an EC-based method. In (Zhao
    etÂ al., [2006](#bib.bib255)), the hyperparameters of map functions were encoded
    by some linear vectors which were constructed by a number of optimal projection
    basis vectors obtained via EC.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§ç¼–ç ï¼šåœ¨è¿™ç§ç¼–ç ä¸­ï¼Œæ˜ å°„å‡½æ•°ï¼ˆRefahi ç­‰ï¼Œ[2020](#bib.bib164)ï¼›Albukhanajer ç­‰ï¼Œ[2015](#bib.bib7)ï¼‰æˆ–å‡½æ•°å‚æ•°ï¼ˆZhao
    ç­‰ï¼Œ[2006](#bib.bib255)ï¼‰è¢«ç¼–ç ä¸ºçº¿æ€§æ ¼å¼ã€‚ä¾‹å¦‚ï¼ŒWissam ç­‰ï¼ˆAlbukhanajer ç­‰ï¼Œ[2015](#bib.bib7)ï¼‰ä¸ºç‰¹å¾æå–é¢„å®šä¹‰äº†ä¸‰ç»„è½¨è¿¹å‡½æ•°ï¼ˆå³ï¼Œè¿½è¸ªå‡½æ•°ã€ç›´å¾„å‡½æ•°å’Œåœ†å½¢å‡½æ•°ï¼‰ï¼Œå¹¶é€šè¿‡åŸºäºè¿›åŒ–è®¡ç®—çš„æ–¹æ³•è·å¾—äº†å‡½æ•°ä¹‹é—´çš„æœ€ä¼˜ç»„åˆã€‚åœ¨
    (Zhao ç­‰ï¼Œ[2006](#bib.bib255)) ä¸­ï¼Œæ˜ å°„å‡½æ•°çš„è¶…å‚æ•°è¢«ç¼–ç ä¸ºä¸€äº›çº¿æ€§å‘é‡ï¼Œè¿™äº›å‘é‡ç”±é€šè¿‡è¿›åŒ–è®¡ç®—è·å¾—çš„è‹¥å¹²æœ€ä¼˜æŠ•å½±åŸºå‘é‡æ„å»ºè€Œæˆã€‚
- en: 'Tree-based encoding: In tree-based encoding, leaf nodes represent original
    features or constants, while the non-leaf nodes are some operators for feature
    extraction including common arithmetic, logical operators (i.e., â€œ+â€, â€œ/â€, â€œ$\cup$â€)
    or other transformation operators (e.g., uLBP, and SobelY). In EC-driven feature
    extraction, an individual represents a feature extractor or map function (Peng
    etÂ al., [2021](#bib.bib153); Zhang and Rockett, [2011](#bib.bib253)). Especially,
    an EC-based framework was developed in (Zhang and Rockett, [2011](#bib.bib253))
    to search for features and sequences of operations by use of tree-based encoding.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ ‘çš„ç¼–ç ï¼šåœ¨åŸºäºæ ‘çš„ç¼–ç ä¸­ï¼Œå¶å­èŠ‚ç‚¹ä»£è¡¨åŸå§‹ç‰¹å¾æˆ–å¸¸é‡ï¼Œè€Œéå¶å­èŠ‚ç‚¹åˆ™æ˜¯ä¸€äº›ç”¨äºç‰¹å¾æå–çš„è¿ç®—ç¬¦ï¼ŒåŒ…æ‹¬å¸¸è§çš„ç®—æœ¯è¿ç®—ç¬¦ã€é€»è¾‘è¿ç®—ç¬¦ï¼ˆä¾‹å¦‚ï¼Œâ€œ+â€ã€
    â€œ/â€ã€ â€œ$\cup$â€ï¼‰æˆ–å…¶ä»–å˜æ¢è¿ç®—ç¬¦ï¼ˆå¦‚ uLBP å’Œ SobelYï¼‰ã€‚åœ¨åŸºäºè¿›åŒ–è®¡ç®—çš„ç‰¹å¾æå–ä¸­ï¼Œä¸€ä¸ªä¸ªä½“ä»£è¡¨ä¸€ä¸ªç‰¹å¾æå–å™¨æˆ–æ˜ å°„å‡½æ•°ï¼ˆPeng
    ç­‰ï¼Œ[2021](#bib.bib153)ï¼›Zhang å’Œ Rockettï¼Œ[2011](#bib.bib253)ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼ŒåŸºäºè¿›åŒ–è®¡ç®—çš„æ¡†æ¶åœ¨ (Zhang
    å’Œ Rockettï¼Œ[2011](#bib.bib253)) ä¸­è¢«å¼€å‘å‡ºæ¥ï¼Œç”¨äºé€šè¿‡æ ‘å½¢ç¼–ç æ¥æœç´¢ç‰¹å¾å’Œæ“ä½œåºåˆ—ã€‚
- en: 3.3.3\. Search Paradigms
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. æœç´¢èŒƒå¼
- en: In this section, some common search paradigms for feature extraction are introduced.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä»‹ç»äº†ä¸€äº›ç”¨äºç‰¹å¾æå–çš„å¸¸è§æœç´¢èŒƒå¼ã€‚
- en: 'Basic EC search paradigm: EC has been successfully utilized in various feature
    extraction tasks (Z.-Flores etÂ al., [2020](#bib.bib237); Bi etÂ al., [2018](#bib.bib18)).
    For example, Zhao et al. (Zhao etÂ al., [2007](#bib.bib257)) introduced bagging
    concept to an evolutionary algorithm for feature extraction. The work in (Zhao
    etÂ al., [2009](#bib.bib256)) developed an evolutionary discriminant feature extraction
    (EDFE) algorithm by combining GA with subspace analysis, which can reduce the
    complexity of the search space and improve the classification performance.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬çš„è¿›åŒ–è®¡ç®—æœç´¢èŒƒå¼ï¼šè¿›åŒ–è®¡ç®—å·²ç»æˆåŠŸåº”ç”¨äºå„ç§ç‰¹å¾æå–ä»»åŠ¡ï¼ˆZ.-Flores ç­‰ï¼Œ[2020](#bib.bib237)ï¼›Bi ç­‰ï¼Œ[2018](#bib.bib18)ï¼‰ã€‚ä¾‹å¦‚ï¼ŒZhao
    ç­‰ï¼ˆZhao ç­‰ï¼Œ[2007](#bib.bib257)ï¼‰å°†è¢‹è£…æ¦‚å¿µå¼•å…¥äº†ç‰¹å¾æå–çš„è¿›åŒ–ç®—æ³•ä¸­ã€‚å·¥ä½œ (Zhao ç­‰ï¼Œ[2009](#bib.bib256))
    å¼€å‘äº†ä¸€ç§è¿›åŒ–åˆ¤åˆ«ç‰¹å¾æå–ï¼ˆEDFEï¼‰ç®—æ³•ï¼Œé€šè¿‡å°†é—ä¼ ç®—æ³•ä¸å­ç©ºé—´åˆ†æç»“åˆï¼Œå‡å°‘äº†æœç´¢ç©ºé—´çš„å¤æ‚æ€§ï¼Œå¹¶æé«˜äº†åˆ†ç±»æ€§èƒ½ã€‚
- en: 'Co-evolution search paradigm: In feature extraction, finding the optimal extractor
    is an optimization problem, which can be decomposed into a series of subproblems
    (Kotani and Kato, [2004](#bib.bib98); Hajati etÂ al., [2010](#bib.bib68)). For
    example, Hajati et al. (Hajati etÂ al., [2010](#bib.bib68)) proposed a co-evolutionary
    method for feature extraction. Specifically, a subpopulation was evolved to optimize
    the classifier-related subproblem (i.e., classifier construction), and the other
    subpopulation made use of genetic information from the first population for the
    optimization of a feature-related subproblem (i.e., feature extraction).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å…±è¿›åŒ–æœç´¢èŒƒå¼ï¼šåœ¨ç‰¹å¾æå–ä¸­ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„æå–å™¨æ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œè¿™å¯ä»¥è¢«åˆ†è§£æˆä¸€ç³»åˆ—å­é—®é¢˜ï¼ˆKotani å’Œ Katoï¼Œ[2004](#bib.bib98)ï¼›Hajati
    ç­‰ï¼Œ[2010](#bib.bib68)ï¼‰ã€‚ä¾‹å¦‚ï¼ŒHajati ç­‰ï¼ˆHajati ç­‰ï¼Œ[2010](#bib.bib68)ï¼‰æå‡ºäº†ä¸€ç§ç”¨äºç‰¹å¾æå–çš„å…±è¿›åŒ–æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä¸€ä¸ªå­ç¾¤ä½“è¢«è¿›åŒ–ä»¥ä¼˜åŒ–ä¸åˆ†ç±»å™¨ç›¸å…³çš„å­é—®é¢˜ï¼ˆå³ï¼Œåˆ†ç±»å™¨æ„å»ºï¼‰ï¼Œå¦ä¸€ä¸ªå­ç¾¤ä½“åˆ©ç”¨ç¬¬ä¸€ä¸ªç¾¤ä½“çš„é—ä¼ ä¿¡æ¯æ¥ä¼˜åŒ–ä¸ç‰¹å¾ç›¸å…³çš„å­é—®é¢˜ï¼ˆå³ï¼Œç‰¹å¾æå–ï¼‰ã€‚
- en: 'Multi-objective search paradigm: In multi-objective feature extraction, the
    model accuracy, computational time, complexity, and robustness are often taken
    into account as the objectives (Zhang and Rockett, [2011](#bib.bib253); Cano etÂ al.,
    [2017](#bib.bib19)). Cano et al. (Cano etÂ al., [2017](#bib.bib19)) proposed a
    Pareto-based multi-objective GP algorithm for feature extraction and data visualization,
    where the objectives were to minimize the complexity of data transformation (i.e.,
    tree size) and maximize the recognition performance (i.e., accuracy).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šç›®æ ‡æœç´¢èŒƒå¼ï¼šåœ¨å¤šç›®æ ‡ç‰¹å¾æå–ä¸­ï¼Œæ¨¡å‹å‡†ç¡®æ€§ã€è®¡ç®—æ—¶é—´ã€å¤æ‚æ€§å’Œé²æ£’æ€§é€šå¸¸ä½œä¸ºç›®æ ‡ï¼ˆZhang and Rockett, [2011](#bib.bib253);
    Cano et al., [2017](#bib.bib19)ï¼‰ã€‚Cano et al.ï¼ˆCano et al., [2017](#bib.bib19)ï¼‰æå‡ºäº†ä¸€ç§åŸºäºå¸•ç´¯æ‰˜çš„å¤šç›®æ ‡GPç®—æ³•ç”¨äºç‰¹å¾æå–å’Œæ•°æ®å¯è§†åŒ–ï¼Œå…¶ä¸­ç›®æ ‡æ˜¯æœ€å°åŒ–æ•°æ®è½¬æ¢çš„å¤æ‚æ€§ï¼ˆå³æ ‘çš„å¤§å°ï¼‰å¹¶æœ€å¤§åŒ–è¯†åˆ«æ€§èƒ½ï¼ˆå³å‡†ç¡®æ€§ï¼‰ã€‚
- en: 3.3.4\. Summary
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. æ€»ç»“
- en: In existing studies, many efficient searching and balancing strategies, driven
    by EC approaches to achieve satisfactory solutions at significantly-reduced computation
    overheads, have been developed in recent years (Zhang and Rockett, [2011](#bib.bib253);
    Cano etÂ al., [2017](#bib.bib19); Mauceri etÂ al., [2021](#bib.bib133); Shakya etÂ al.,
    [2021](#bib.bib177)). However, the performance of extractors may be limited with
    existing encoding methods and predefined operation sets. Therefore, it is essential
    to develop efficient algorithms, operation control strategies and representation
    for high-dimensional feature extraction.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°æœ‰ç ”ç©¶ä¸­ï¼Œè¿‘å¹´æ¥å·²ç»å¼€å‘äº†è®¸å¤šé«˜æ•ˆçš„æœç´¢å’Œå¹³è¡¡ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥ç”±ECæ–¹æ³•é©±åŠ¨ï¼Œä»¥åœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹å®ç°æ»¡æ„çš„è§£å†³æ–¹æ¡ˆï¼ˆZhang and Rockett,
    [2011](#bib.bib253); Cano et al., [2017](#bib.bib19); Mauceri et al., [2021](#bib.bib133);
    Shakya et al., [2021](#bib.bib177)ï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰ç¼–ç æ–¹æ³•å’Œé¢„å®šä¹‰æ“ä½œé›†å¯èƒ½é™åˆ¶æå–å™¨çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå¼€å‘é«˜æ•ˆçš„ç®—æ³•ã€æ“ä½œæ§åˆ¶ç­–ç•¥å’Œé«˜ç»´ç‰¹å¾æå–çš„è¡¨ç¤ºæ˜¯è‡³å…³é‡è¦çš„ã€‚
- en: 4\. Model Generation
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. æ¨¡å‹ç”Ÿæˆ
- en: Model generation is to search for optimal models with desirable learning capability
    on given tasks (Yao etÂ al., [2018](#bib.bib231); He etÂ al., [2021](#bib.bib72)).
    In this section, we introduce corresponding evolutionary parameter optimization,
    architecture optimization, and joint optimization from solution representation
    to search paradigms. Readers interested in other model generation approaches (e.g.,
    RL-based and gradient-based approaches) can refer to the reviews (Ren etÂ al.,
    [2021](#bib.bib165); JaÃ¢fra etÂ al., [2019](#bib.bib86)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹ç”Ÿæˆæ˜¯æŒ‡åœ¨ç»™å®šä»»åŠ¡ä¸Šæœç´¢å…·æœ‰ç†æƒ³å­¦ä¹ èƒ½åŠ›çš„æœ€ä½³æ¨¡å‹ï¼ˆYao et al., [2018](#bib.bib231); He et al., [2021](#bib.bib72)ï¼‰ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä»è§£çš„è¡¨ç¤ºåˆ°æœç´¢èŒƒå¼çš„å¯¹åº”æ¼”åŒ–å‚æ•°ä¼˜åŒ–ã€æ¶æ„ä¼˜åŒ–å’Œè”åˆä¼˜åŒ–ã€‚å¯¹å…¶ä»–æ¨¡å‹ç”Ÿæˆæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ å’ŒåŸºäºæ¢¯åº¦çš„æ–¹æ³•ï¼‰æ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥å‚è€ƒç›¸å…³ç»¼è¿°ï¼ˆRen
    et al., [2021](#bib.bib165); JaÃ¢fra et al., [2019](#bib.bib86)ï¼‰ã€‚
- en: 4.1\. Model Parameter Optimization
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. æ¨¡å‹å‚æ•°ä¼˜åŒ–
- en: 4.1.1\. Problem Formulation
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. é—®é¢˜å®šä¹‰
- en: 'Model parameter optimization targets at searching for the best parameter set
    (i.e., weights $W^{*}$) for a predefined architecture ($A$). The loss function
    $L$ (e.g., the cross-entropy loss function) measures the performance of the model
    with optimized parameters (i.e., $W$ in Eq. [2](#S4.E2 "In 4.1.1\. Problem Formulation
    â€£ 4.1\. Model Parameter Optimization â€£ 4\. Model Generation â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")) on given
    datasets. The general model parameter optimization can be formulated as'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°ä¼˜åŒ–çš„ç›®æ ‡æ˜¯æœç´¢ä¸€ä¸ªé¢„å®šä¹‰æ¶æ„ï¼ˆ$A$ï¼‰çš„æœ€ä½³å‚æ•°é›†ï¼ˆå³æƒé‡ $W^{*}$ï¼‰ã€‚æŸå¤±å‡½æ•° $L$ï¼ˆä¾‹å¦‚ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼‰è¡¡é‡åœ¨ç»™å®šæ•°æ®é›†ä¸Šä¼˜åŒ–å‚æ•°ï¼ˆå³
    $W$ åœ¨ Eq. [2](#S4.E2 "åœ¨ 4.1.1\. é—®é¢˜å®šä¹‰ â€£ 4.1\. æ¨¡å‹å‚æ•°ä¼˜åŒ– â€£ 4\. æ¨¡å‹ç”Ÿæˆ â€£ æ¼”åŒ–æ·±åº¦å­¦ä¹ ç»¼è¿°ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜")ï¼‰çš„æ¨¡å‹æ€§èƒ½ã€‚ä¸€èˆ¬çš„æ¨¡å‹å‚æ•°ä¼˜åŒ–å¯ä»¥è¡¨ç¤ºä¸º
- en: '| (2) |  | $\displaystyle\begin{matrix}{{W}^{*}}$=$\underset{W}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle\begin{matrix}{{W}^{*}}$=$\underset{W}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
- en: where $W$ is usually large-scale (millions of model parameters) and highly non-convex.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $W$ é€šå¸¸æ˜¯å¤§è§„æ¨¡çš„ï¼ˆæ•°ç™¾ä¸‡ä¸ªæ¨¡å‹å‚æ•°ï¼‰ä¸”é«˜åº¦éå‡¸ã€‚
- en: 4.1.2\. Solution Representations
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. è§£çš„è¡¨ç¤º
- en: There are two typical EC-based representation schemes for model parameter optimization,
    including direct encoding and indirect encoding (He etÂ al., [2021](#bib.bib72)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¼”åŒ–è®¡ç®—ï¼ˆECï¼‰çš„æ¨¡å‹å‚æ•°ä¼˜åŒ–æœ‰ä¸¤ç§å…¸å‹çš„è¡¨ç¤ºæ–¹æ¡ˆï¼ŒåŒ…æ‹¬ç›´æ¥ç¼–ç å’Œé—´æ¥ç¼–ç ï¼ˆHe et al., [2021](#bib.bib72)ï¼‰ã€‚
- en: 'Direct encoding: The model parameters are directly represented via a vector
    or matrix, in which each element represents a specific parameter (Ijjina and Chalavadi,
    [2016](#bib.bib83); Karegowda and Manjunath, [2011](#bib.bib94)). For example,
    a chromosome with 64 real numbers was used to directly represent the network corresponding
    weights, where the first 63 real numbers were used to encode three convolution
    masks of size 1 $\times$ 21\. The last real number was the random seed of a generator
    for the initialization of a fully connected network (Ijjina and Chalavadi, [2016](#bib.bib83)).
    This encoding approach may require a huge computational overhead to represent
    and optimize the large-scale weights.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥ç¼–ç ï¼šæ¨¡å‹å‚æ•°é€šè¿‡å‘é‡æˆ–çŸ©é˜µç›´æ¥è¡¨ç¤ºï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºä¸€ä¸ªç‰¹å®šçš„å‚æ•°ï¼ˆIjjina å’Œ Chalavadiï¼Œ[2016](#bib.bib83)ï¼›Karegowda
    å’Œ Manjunathï¼Œ[2011](#bib.bib94)ï¼‰ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åŒ…å«64ä¸ªå®æ•°çš„æŸ“è‰²ä½“ç›´æ¥è¡¨ç¤ºç½‘ç»œå¯¹åº”çš„æƒé‡ï¼Œå…¶ä¸­å‰63ä¸ªå®æ•°ç”¨äºç¼–ç å¤§å°ä¸º 1
    $\times$ 21 çš„ä¸‰ä¸ªå·ç§¯æ©æ¨¡ã€‚æœ€åä¸€ä¸ªå®æ•°æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨çš„éšæœºç§å­ï¼Œç”¨äºåˆå§‹åŒ–ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œï¼ˆIjjina å’Œ Chalavadiï¼Œ[2016](#bib.bib83)ï¼‰ã€‚è¿™ç§ç¼–ç æ–¹æ³•å¯èƒ½éœ€è¦å·¨å¤§çš„è®¡ç®—å¼€é”€æ¥è¡¨ç¤ºå’Œä¼˜åŒ–å¤§è§„æ¨¡æƒé‡ã€‚
- en: 'Indirect encoding: This encoding approach represents only a subset of the model
    parameters via a deterministic transformation (Li etÂ al., [2019](#bib.bib110);
    KoutnÃ­k etÂ al., [2010](#bib.bib99)). In (KoutnÃ­k etÂ al., [2010](#bib.bib99)),
    the weight information was encoded as a set of Fourier coefficients in the frequency
    domain to reduce dimensionality of representation by ignoring high-frequency coefficients.
    Although this method is able to speed up the search process, the loss of parameter
    the information may occur due to the incomplete information representation, which
    maydegrades the model performance.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: é—´æ¥ç¼–ç ï¼šè¿™ç§ç¼–ç æ–¹æ³•é€šè¿‡ç¡®å®šæ€§å˜æ¢ä»…è¡¨ç¤ºæ¨¡å‹å‚æ•°çš„ä¸€ä¸ªå­é›†ï¼ˆLi ç­‰ï¼Œ[2019](#bib.bib110)ï¼›KoutnÃ­k ç­‰ï¼Œ[2010](#bib.bib99)ï¼‰ã€‚åœ¨
    (KoutnÃ­k ç­‰ï¼Œ[2010](#bib.bib99)) ä¸­ï¼Œæƒé‡ä¿¡æ¯è¢«ç¼–ç ä¸ºé¢‘åŸŸä¸­çš„ä¸€ç»„å‚…é‡Œå¶ç³»æ•°ï¼Œä»¥é€šè¿‡å¿½ç•¥é«˜é¢‘ç³»æ•°æ¥å‡å°‘è¡¨ç¤ºçš„ç»´åº¦ã€‚è™½ç„¶è¿™ç§æ–¹æ³•èƒ½å¤ŸåŠ é€Ÿæœç´¢è¿‡ç¨‹ï¼Œä½†ç”±äºä¿¡æ¯è¡¨ç¤ºä¸å®Œæ•´ï¼Œå¯èƒ½ä¼šä¸¢å¤±å‚æ•°ä¿¡æ¯ï¼Œä»è€Œé™ä½æ¨¡å‹æ€§èƒ½ã€‚
- en: 4.1.3\. Search Paradigms
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. æœç´¢èŒƒå¼
- en: EC-based methods for model parameter optimization can be divided into two categories
    according to whether or not method combines with the gradient approach, i.e.,
    pure EC and gradient-based EC.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºECçš„æ–¹æ³•å¯ä»¥æ ¹æ®æ˜¯å¦ç»“åˆæ¢¯åº¦æ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼Œå³çº¯ECå’ŒåŸºäºæ¢¯åº¦çš„ECã€‚
- en: Pure EC paradigms optimize model parameters only via evolutionary search, including
    the basic EC search paradigm and co-evolution search paradigm.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: çº¯ECèŒƒå¼ä»…é€šè¿‡è¿›åŒ–æœç´¢ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬åŸºæœ¬ECæœç´¢èŒƒå¼å’Œå…±åŒè¿›åŒ–æœç´¢èŒƒå¼ã€‚
- en: â€¢
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Basic EC search paradigm: In addition to GA (Montana and Davis, [1989](#bib.bib140);
    Karegowda and Manjunath, [2011](#bib.bib94)), some heuristic algorithms like PSO
    (Al-kazemi and Mohan, [2002](#bib.bib5)), ABC (Karaboga etÂ al., [2007](#bib.bib93))
    and ACO (Socha and Blum, [2007](#bib.bib183)) are also commonly utilized for model
    parameter optimization. For example, Karaboga et al. (Karaboga etÂ al., [2007](#bib.bib93))
    adopted ABC to find a set of weights for a feed-forward neural network (FNN) on
    targeted tasks.'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºæœ¬ECæœç´¢èŒƒå¼ï¼šé™¤äº†GAï¼ˆMontana å’Œ Davisï¼Œ[1989](#bib.bib140)ï¼›Karegowda å’Œ Manjunathï¼Œ[2011](#bib.bib94)ï¼‰ï¼Œä¸€äº›å¯å‘å¼ç®—æ³•å¦‚PSOï¼ˆAl-kazemi
    å’Œ Mohanï¼Œ[2002](#bib.bib5)ï¼‰ï¼ŒABCï¼ˆKaraboga ç­‰ï¼Œ[2007](#bib.bib93)ï¼‰å’ŒACOï¼ˆSocha å’Œ Blumï¼Œ[2007](#bib.bib183)ï¼‰ä¹Ÿå¸¸ç”¨äºæ¨¡å‹å‚æ•°ä¼˜åŒ–ã€‚ä¾‹å¦‚ï¼ŒKaraboga
    ç­‰ï¼ˆKaraboga ç­‰ï¼Œ[2007](#bib.bib93)ï¼‰é‡‡ç”¨ABCæ¥ä¸ºç›®æ ‡ä»»åŠ¡ä¸Šçš„å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFNNï¼‰æ‰¾åˆ°ä¸€ç»„æƒé‡ã€‚
- en: â€¢
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Co-evolution search paradigm: Co-evolution search is conducted on the subproblems
    of the original optimization problem (e.g., synapse-based and neuron-based problems
    (Chandra and Zhang, [2012](#bib.bib23); Chandra, [2015](#bib.bib22))). For example,
    Chandra et al. (Chandra and Zhang, [2012](#bib.bib23)) regarded a single hidden
    layer as a subcomponent in the initialization phase, which will be merged with
    the individuals with the best fitness from different sub-populations to constitute
    new neural networks during the co-evolution optimization process.'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…±åŒè¿›åŒ–æœç´¢èŒƒå¼ï¼šå…±åŒè¿›åŒ–æœç´¢æ˜¯åœ¨åŸå§‹ä¼˜åŒ–é—®é¢˜çš„å­é—®é¢˜ä¸Šè¿›è¡Œçš„ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºçªè§¦å’ŒåŸºäºç¥ç»å…ƒçš„é—®é¢˜ï¼ˆChandra å’Œ Zhangï¼Œ[2012](#bib.bib23)ï¼›Chandraï¼Œ[2015](#bib.bib22)ï¼‰ï¼‰ã€‚ä¾‹å¦‚ï¼ŒChandra
    ç­‰ï¼ˆChandra å’Œ Zhangï¼Œ[2012](#bib.bib23)ï¼‰åœ¨åˆå§‹åŒ–é˜¶æ®µå°†å•ä¸ªéšè—å±‚è§†ä¸ºä¸€ä¸ªå­ç»„ä»¶ï¼Œè¯¥å­ç»„ä»¶å°†åœ¨å…±åŒè¿›åŒ–ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¸æ¥è‡ªä¸åŒå­ç§ç¾¤çš„æœ€ä½³é€‚åº”ä¸ªä½“åˆå¹¶ï¼Œä»¥æ„æˆæ–°çš„ç¥ç»ç½‘ç»œã€‚
- en: '![Refer to caption](img/6825f4f47b67248cf5a997f1ecfa83e1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/6825f4f47b67248cf5a997f1ecfa83e1.png)'
- en: (a)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/38e7f4b1d3767aae05ade0ace0f876a1.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/38e7f4b1d3767aae05ade0ace0f876a1.png)'
- en: (b)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/2410148fead42dccca01069d6689080e.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/2410148fead42dccca01069d6689080e.png)'
- en: (c)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: Figure 5. Three hybrid ways of gradient-based ECs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5. åŸºäºæ¢¯åº¦çš„ECçš„ä¸‰ç§æ··åˆæ–¹å¼ã€‚
- en: Gradient-based EC combine basic EC with the gradient-based method to enhance
    the exploitation ability in optimizing model parameters. According to the execution
    order, there are three hybrid ways.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¢¯åº¦çš„ECå°†åŸºç¡€ECä¸åŸºäºæ¢¯åº¦çš„æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œä»¥å¢å¼ºåœ¨ä¼˜åŒ–æ¨¡å‹å‚æ•°æ—¶çš„å¼€å‘èƒ½åŠ›ã€‚æ ¹æ®æ‰§è¡Œé¡ºåºï¼Œæœ‰ä¸‰ç§æ··åˆæ–¹å¼ã€‚
- en: â€¢
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'The first hybridization approach is shown in Fig. ([5(a)](#S4.F5.sf1 "In Figure
    5 â€£ 4.1.3\. Search Paradigms â€£ 4.1\. Model Parameter Optimization â€£ 4\. Model
    Generation â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues")), where the EC is used to identify the optimal parameters for
    model, then the parameters are further optimized using gradient-based method to
    find the final optimal solution (Zhang and Smart, [2004](#bib.bib250); Chen etÂ al.,
    [2015](#bib.bib25)). For example, a genetic adaptive momentum estimation algorithm
    (GADAM) was proposed in (Zhang and Gouza, [2018](#bib.bib243)) by incorporating
    Adam and GA into a unified learning scheme, where Adam was an adaptive moment
    estimation method with first-order gradient.'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ··åˆæ–¹æ³•å¦‚å›¾æ‰€ç¤º ([5(a)](#S4.F5.sf1 "å›¾ 5 â€£ 4.1.3\. æœç´¢èŒƒå¼ â€£ 4.1\. æ¨¡å‹å‚æ•°ä¼˜åŒ– â€£ 4\. æ¨¡å‹ç”Ÿæˆ
    â€£ æ¼”åŒ–æ·±åº¦å­¦ä¹ è°ƒæŸ¥ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜"))ï¼Œå…¶ä¸­ECç”¨äºè¯†åˆ«æ¨¡å‹çš„æœ€ä½³å‚æ•°ï¼Œç„¶åä½¿ç”¨åŸºäºæ¢¯åº¦çš„æ–¹æ³•è¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›å‚æ•°ï¼Œä»¥æ‰¾åˆ°æœ€ç»ˆçš„æœ€ä¼˜è§£ (Zhang
    å’Œ Smart, [2004](#bib.bib250); Chen ç­‰, [2015](#bib.bib25))ã€‚ä¾‹å¦‚ï¼Œ(Zhang å’Œ Gouza, [2018](#bib.bib243))
    æå‡ºçš„é—ä¼ è‡ªé€‚åº”åŠ¨é‡ä¼°è®¡ç®—æ³•ï¼ˆGADAMï¼‰é€šè¿‡å°†Adamå’ŒGAèå…¥ç»Ÿä¸€å­¦ä¹ æ–¹æ¡ˆä¸­ï¼Œå…¶ä¸­Adamæ˜¯ä¸€ä¸ªå…·æœ‰ä¸€é˜¶æ¢¯åº¦çš„è‡ªé€‚åº”åŠ¨é‡ä¼°è®¡æ–¹æ³•ã€‚
- en: â€¢
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'The second hybridization approach is given in Fig. ([5(b)](#S4.F5.sf2 "In Figure
    5 â€£ 4.1.3\. Search Paradigms â€£ 4.1\. Model Parameter Optimization â€£ 4\. Model
    Generation â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues")), where the gradient-based method is used to produce a set of
    parameters for the initialization of the population used in EC (Wu etÂ al., [2021b](#bib.bib217)).
    For example, the study (Khadka and Tumer, [2018](#bib.bib95)) firstly trained
    a RL agent through a gradient-based method, then the parameters of the RL were
    used as the initial population to feed the EC. As a result, the parameters will
    be further optimized by the EC.'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§æ··åˆæ–¹æ³•å¦‚å›¾æ‰€ç¤º ([5(b)](#S4.F5.sf2 "å›¾ 5 â€£ 4.1.3\. æœç´¢èŒƒå¼ â€£ 4.1\. æ¨¡å‹å‚æ•°ä¼˜åŒ– â€£ 4\. æ¨¡å‹ç”Ÿæˆ
    â€£ æ¼”åŒ–æ·±åº¦å­¦ä¹ è°ƒæŸ¥ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜"))ï¼Œå…¶ä¸­åŸºäºæ¢¯åº¦çš„æ–¹æ³•ç”¨äºç”Ÿæˆä¸€ç»„å‚æ•°ä»¥åˆå§‹åŒ–ç”¨äºECçš„äººç¾¤ (Wu ç­‰, [2021b](#bib.bib217))ã€‚ä¾‹å¦‚ï¼Œç ”ç©¶
    (Khadka å’Œ Tumer, [2018](#bib.bib95)) é¦–å…ˆé€šè¿‡åŸºäºæ¢¯åº¦çš„æ–¹æ³•è®­ç»ƒä¸€ä¸ªRLä»£ç†ï¼Œç„¶åå°†RLçš„å‚æ•°ä½œä¸ºåˆå§‹ç§ç¾¤è¾“å…¥ECã€‚å› æ­¤ï¼Œå‚æ•°å°†ç”±ECè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
- en: â€¢
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'The third approach is presented in Fig. ([5(c)](#S4.F5.sf3 "In Figure 5 â€£ 4.1.3\.
    Search Paradigms â€£ 4.1\. Model Parameter Optimization â€£ 4\. Model Generation â€£
    Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues")), which iteratively applies EC and gradient-based method during
    the optimization to find the optimal parameters. Following this framework, when
    the method is used and which method is chosen are varying in different studies
    (Yang etÂ al., [2021](#bib.bib229); Cui etÂ al., [2018](#bib.bib36)).'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ç§æ–¹æ³•åœ¨å›¾ä¸­å±•ç¤º ([5(c)](#S4.F5.sf3 "å›¾ 5 â€£ 4.1.3\. æœç´¢èŒƒå¼ â€£ 4.1\. æ¨¡å‹å‚æ•°ä¼˜åŒ– â€£ 4\. æ¨¡å‹ç”Ÿæˆ
    â€£ æ¼”åŒ–æ·±åº¦å­¦ä¹ è°ƒæŸ¥ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜"))ï¼Œå®ƒåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è¿­ä»£åœ°åº”ç”¨ECå’ŒåŸºäºæ¢¯åº¦çš„æ–¹æ³•ä»¥æ‰¾åˆ°æœ€ä¼˜å‚æ•°ã€‚æ ¹æ®è¿™ä¸€æ¡†æ¶ï¼Œä½•æ—¶ä½¿ç”¨è¯¥æ–¹æ³•ä»¥åŠé€‰æ‹©å“ªç§æ–¹æ³•åœ¨ä¸åŒçš„ç ”ç©¶ä¸­æœ‰æ‰€ä¸åŒ
    (Yang ç­‰, [2021](#bib.bib229); Cui ç­‰, [2018](#bib.bib36))ã€‚
- en: 4.1.4\. Summary
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. æ€»ç»“
- en: In model parameter optimization, direct encoding is straightforward and able
    to keep more information than the indirect encoding. Compared to gradient-based
    methods easily trapped into local optima, EC shows more powerful ability in global
    search. Here, several scenarios are introduced as follows, where EC is applied
    to model parameter optimization.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹å‚æ•°ä¼˜åŒ–ä¸­ï¼Œç›´æ¥ç¼–ç ç›´è§‚ä¸”èƒ½å¤Ÿä¿ç•™æ¯”é—´æ¥ç¼–ç æ›´å¤šçš„ä¿¡æ¯ã€‚ä¸å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜çš„åŸºäºæ¢¯åº¦çš„æ–¹æ³•ç›¸æ¯”ï¼ŒECåœ¨å…¨å±€æœç´¢ä¸­å±•ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ã€‚è¿™é‡Œä»‹ç»äº†å‡ ä¸ªåœºæ™¯ï¼Œå…¶ä¸­ECåº”ç”¨äºæ¨¡å‹å‚æ•°ä¼˜åŒ–ã€‚
- en: 'Small-scale scenario: (Montana and Davis, [1989](#bib.bib140)) shows that pure
    EC approaches outperform gradient-based methods in search effectiveness on some
    small-scale problems, where the models are with small numbers of parameters or
    simple architectures (e.g., FNN).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å°è§„æ¨¡åœºæ™¯ï¼š (Montana å’Œ Davis, [1989](#bib.bib140)) æ˜¾ç¤ºçº¯ECæ–¹æ³•åœ¨ä¸€äº›å°è§„æ¨¡é—®é¢˜çš„æœç´¢æœ‰æ•ˆæ€§ä¸Šä¼˜äºåŸºäºæ¢¯åº¦çš„æ–¹æ³•ï¼Œè¿™äº›é—®é¢˜ä¸­çš„æ¨¡å‹å‚æ•°è¾ƒå°‘æˆ–ç»“æ„ç®€å•ï¼ˆä¾‹å¦‚ï¼ŒFNNï¼‰ã€‚
- en: 'Large-scale scenario: The performance of pure EC approaches might not be promising
    in large-scale learning models, while a better way is to utilize the hybridization
    of EC and gradient-based methods. Such hybrid methods can alleviate the issue
    of getting trapped in local optima and increase the effectiveness of subsequent
    exploitation (Yang etÂ al., [2021](#bib.bib229)).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è§„æ¨¡åœºæ™¯ï¼šåœ¨å¤§è§„æ¨¡å­¦ä¹ æ¨¡å‹ä¸­ï¼Œçº¯ EC æ–¹æ³•çš„è¡¨ç°å¯èƒ½ä¸å°½å¦‚äººæ„ï¼Œè€Œæ›´å¥½çš„æ–¹æ³•æ˜¯åˆ©ç”¨ EC å’ŒåŸºäºæ¢¯åº¦çš„æ–¹æ³•çš„æ··åˆã€‚è¿™ç§æ··åˆæ–¹æ³•å¯ä»¥ç¼“è§£é™·å…¥å±€éƒ¨æœ€ä¼˜è§£çš„é—®é¢˜ï¼Œå¹¶æé«˜åç»­å¼€å‘çš„æœ‰æ•ˆæ€§ï¼ˆYang
    et al., [2021](#bib.bib229)ï¼‰ã€‚
- en: In addition to above scenarios, EC-based method can be used to train the DNN,
    when the exact gradient information of the loss function is difficult to be acquired
    (Peng etÂ al., [2018](#bib.bib154)). For example, the rewards of policy network
    are sparse or deceptive in deep reinforcement learning (DRL) so that the gradient
    information is unattainable. The work (Conti etÂ al., [2018](#bib.bib35)) introduced
    the novelty search (NS) and the quality diversity (QD) to the evolution strategies
    (ES) for the policy network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä¸Šè¿°åœºæ™¯ï¼Œå½“éš¾ä»¥è·å¾—æŸå¤±å‡½æ•°çš„ç¡®åˆ‡æ¢¯åº¦ä¿¡æ¯æ—¶ï¼Œå¯ä»¥ä½¿ç”¨åŸºäº EC çš„æ–¹æ³•æ¥è®­ç»ƒ DNNï¼ˆPeng et al., [2018](#bib.bib154)ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ä¸­ï¼Œç­–ç•¥ç½‘ç»œçš„å¥–åŠ±æ˜¯ç¨€ç–æˆ–å…·æœ‰è¯¯å¯¼æ€§çš„ï¼Œä»è€Œä½¿å¾—æ¢¯åº¦ä¿¡æ¯æ— æ³•è·å¾—ã€‚å·¥ä½œï¼ˆConti
    et al., [2018](#bib.bib35)ï¼‰å°†æ–°é¢–æ€§æœç´¢ï¼ˆNSï¼‰å’Œè´¨é‡å¤šæ ·æ€§ï¼ˆQDï¼‰å¼•å…¥äº†ç­–ç•¥ç½‘ç»œçš„è¿›åŒ–ç­–ç•¥ï¼ˆESï¼‰ä¸­ã€‚
- en: 4.2\. Model Architecture Optimization
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. æ¨¡å‹æ¶æ„ä¼˜åŒ–
- en: 4.2.1\. Problem Formulation
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. é—®é¢˜è¡¨è¿°
- en: Model architecture optimization, also termed as NAS, is to search promising
    network architectures with good performance such as model accuracy on given tasks.
    The model architecture optimization can be formulated as follows.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ¶æ„ä¼˜åŒ–ï¼Œä¹Ÿç§°ä¸º NASï¼Œæ˜¯æŒ‡æœç´¢å…·æœ‰è‰¯å¥½æ€§èƒ½çš„ç½‘ç»œæ¶æ„ï¼Œä¾‹å¦‚åœ¨ç»™å®šä»»åŠ¡ä¸Šçš„æ¨¡å‹å‡†ç¡®æ€§ã€‚æ¨¡å‹æ¶æ„ä¼˜åŒ–å¯ä»¥è¡¨è¿°å¦‚ä¸‹ã€‚
- en: '| (3) |  | <math   alttext="\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&amp;A\in\mathcal{A}\\'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '| (3) |  | <math alttext="\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&amp;A\in\mathcal{A}\\'
- en: \end{matrix}\\
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \end{matrix}\\
- en: \end{matrix}\right." display="block"><semantics ><mrow ><mo >{</mo><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mrow ><msup ><mi >A</mi><mo >âˆ—</mo></msup><mo lspace="0em"
    rspace="0em" >â€‹</mo><mtext >=</mtext><mo lspace="0.167em" rspace="0em" >â€‹</mo><munder
    accentunder="true" ><mrow ><mi >arg</mi><mo lspace="0.167em" >â¡</mo><mi >min</mi></mrow><mrow
    ><mi >W</mi><mo >,</mo><mi >A</mi></mrow></munder><mo lspace="0.167em" rspace="0em"
    >â€‹</mo><mi >L</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo >(</mo><mi >W</mi><mo
    >,</mo><mi >A</mi><mo >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mtable columnspacing="5pt"
    displaystyle="true" ><mtr ><mtd ><mrow ><mrow ><mi >s</mi><mo lspace="0em" rspace="0.167em"
    >.</mo><mi >t</mi></mrow><mo lspace="0em" >.</mo></mrow></mtd><mtd ><mrow ><mi
    >A</mi><mo >âˆˆ</mo><mi >ğ’œ</mi></mrow></mtd></mtr></mtable></mtd></mtr></mtable></mrow><annotation
    encoding="application/x-tex" >\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&A\in\mathcal{A}\\ \end{matrix}\\ \end{matrix}\right.</annotation></semantics></math>
    |  |
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \end{matrix}\right." display="block"><semantics ><mrow ><mo >{</mo><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mrow ><msup ><mi >A</mi><mo >âˆ—</mo></msup><mo lspace="0em"
    rspace="0em" >â€‹</mo><mtext >=</mtext><mo lspace="0.167em" rspace="0em" >â€‹</mo><munder
    accentunder="true" ><mrow ><mi >arg</mi><mo lspace="0.167em" >â¡</mo><mi >min</mi></mrow><mrow
    ><mi >W</mi><mo >,</mo><mi >A</mi></mrow></munder><mo lspace="0.167em" rspace="0em"
    >â€‹</mo><mi >L</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo >(</mo><mi >W</mi><mo
    >,</mo><mi >A</mi><mo >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mtable columnspacing="5pt"
    displaystyle="true" ><mtr ><mtd ><mrow ><mrow ><mi >s</mi><mo lspace="0em" rspace="0.167em"
    >.</mo><mi >t</mi></mrow><mo lspace="0em" >.</mo></mrow></mtd><mtd ><mrow ><mi
    >A</mi><mo >âˆˆ</mo><mi >ğ’œ</mi></mrow></mtd></mtr></mtable></mtd></mtr></mtable></mrow><annotation
    encoding="application/x-tex" >\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&A\in\mathcal{A}\\ \end{matrix}\\ \end{matrix}\right.</annotation></semantics></math>
    |  |
- en: where $A^{*}$ indicates the architecture from the search space ($\mathcal{A}$)
    with the best performance under the parameters $W$, and $L$ is used to measure
    the performance of architectures on given tasks. Thereby, this optimization is
    a bi-level optimization problem (Zhou etÂ al., [2021a](#bib.bib258); Liu etÂ al.,
    [2021b](#bib.bib117)), where the model architecture optimization is subject to
    the model parameter optimization (Lu etÂ al., [2021](#bib.bib125)). Since the current
    NAS works are mainly focused on CNN, we will discuss the solution representations,
    the search paradigms, and acceleration strategies of CNN. Due to the page limit,
    the design of the search space of NAS are not introduced here, but interested
    readers can check these surveys (Liu etÂ al., [2021b](#bib.bib117); Ren etÂ al.,
    [2021](#bib.bib165)) which have details about search space design.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $A^{*}$ è¡¨ç¤ºåœ¨å‚æ•° $W$ ä¸‹ï¼Œå…·æœ‰æœ€ä½³æ€§èƒ½çš„æœç´¢ç©ºé—´ï¼ˆ$\mathcal{A}$ï¼‰ä¸­çš„æ¶æ„ï¼Œè€Œ $L$ ç”¨äºè¡¡é‡æ¶æ„åœ¨ç»™å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œè¿™ç§ä¼˜åŒ–æ˜¯ä¸€ä¸ªåŒå±‚æ¬¡ä¼˜åŒ–é—®é¢˜ï¼ˆZhou
    et al., [2021a](#bib.bib258); Liu et al., [2021b](#bib.bib117)ï¼‰ï¼Œå…¶ä¸­æ¨¡å‹æ¶æ„ä¼˜åŒ–å—é™äºæ¨¡å‹å‚æ•°ä¼˜åŒ–ï¼ˆLu
    et al., [2021](#bib.bib125)ï¼‰ã€‚ç”±äºç›®å‰çš„NASå·¥ä½œä¸»è¦é›†ä¸­åœ¨CNNä¸Šï¼Œæˆ‘ä»¬å°†è®¨è®ºCNNçš„è§£å†³æ–¹æ¡ˆè¡¨ç¤ºã€æœç´¢èŒƒå¼å’ŒåŠ é€Ÿç­–ç•¥ã€‚ç”±äºé¡µé¢é™åˆ¶ï¼Œè¿™é‡Œæ²¡æœ‰ä»‹ç»NASçš„æœç´¢ç©ºé—´è®¾è®¡ï¼Œä½†æ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥æŸ¥é˜…è¿™äº›ç»¼è¿°ï¼ˆLiu
    et al., [2021b](#bib.bib117); Ren et al., [2021](#bib.bib165)ï¼‰ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³æœç´¢ç©ºé—´è®¾è®¡çš„è¯¦ç»†ä¿¡æ¯ã€‚
- en: 4.2.2\. Solution Representations
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. è§£å†³æ–¹æ¡ˆè¡¨ç¤º
- en: According to varying lengths of encodings, we can classify the encoding strategies
    into fixed-length and variable-length encodings.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ç¼–ç é•¿åº¦çš„ä¸åŒï¼Œæˆ‘ä»¬å¯ä»¥å°†ç¼–ç ç­–ç•¥åˆ†ä¸ºå›ºå®šé•¿åº¦ç¼–ç å’Œå˜é•¿ç¼–ç ã€‚
- en: 'Fixed-length encoding: The length of each individual is fixed during the evolution.
    For example, a fixed-length vector is designed to represent the model architecture
    of CNN (Xie and Yuille, [2017](#bib.bib222)), where a subset of elements in the
    vector represents an architectural units (e.g., convolutional, pooling or fully-connected
    layer) of a CNN. Such encoding may be easily adapted to evolutionary operations
    (e.g., crossover and mutation) of EC (Xie and Yuille, [2017](#bib.bib222)), but
    it has to specify an appropriate maximal length, which is usually unknown in advance
    and needs to predefine based on domain expertise.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å›ºå®šé•¿åº¦ç¼–ç ï¼šåœ¨è¿›åŒ–è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªä¸ªä½“çš„é•¿åº¦æ˜¯å›ºå®šçš„ã€‚ä¾‹å¦‚ï¼Œè®¾è®¡ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡æ¥è¡¨ç¤ºCNNçš„æ¨¡å‹æ¶æ„ï¼ˆXie and Yuille, [2017](#bib.bib222)ï¼‰ï¼Œå…¶ä¸­å‘é‡ä¸­çš„ä¸€ä¸ªå­é›†è¡¨ç¤ºCNNçš„æ¶æ„å•å…ƒï¼ˆä¾‹å¦‚å·ç§¯å±‚ã€æ± åŒ–å±‚æˆ–å…¨è¿æ¥å±‚ï¼‰ã€‚è¿™ç§ç¼–ç å¯èƒ½å®¹æ˜“é€‚åº”è¿›åŒ–è®¡ç®—ï¼ˆECï¼‰çš„æ“ä½œï¼ˆä¾‹å¦‚äº¤å‰å’Œçªå˜ï¼‰ï¼ˆXie
    and Yuille, [2017](#bib.bib222)ï¼‰ï¼Œä½†å®ƒå¿…é¡»æŒ‡å®šä¸€ä¸ªåˆé€‚çš„æœ€å¤§é•¿åº¦ï¼Œè¿™é€šå¸¸æ˜¯æœªçŸ¥çš„ï¼Œéœ€è¦åŸºäºé¢†åŸŸä¸“ä¸šçŸ¥è¯†é¢„å…ˆå®šä¹‰ã€‚
- en: 'Variable-length encoding: Different from the fixed-length approach, the variable-length
    encoding strategy does not require a prior knowledge about the optimal depth of
    model architecture and actually could be a way to reduce the complexity of the
    search space. The flexible design of this encoding may encode more detailed information
    about the architecture into a solution vector, and the optimal length of the solution
    is automatically found during the search process (Liu etÂ al., [2021b](#bib.bib117)).
    In (Chen etÂ al., [2020](#bib.bib27)), the entire variational autoencoder (VAE)
    was divided into four blocks, including h-block, $\mu$-block, $\sigma$-block and
    t-block, while the variable-length chromosomes consisted of different quantities
    and types of layers. Notably, variable-length encoding it is not straightforward
    to apply standard genetic operators (e.g., crossover).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å˜é•¿ç¼–ç ï¼šä¸å›ºå®šé•¿åº¦æ–¹æ³•ä¸åŒï¼Œå˜é•¿ç¼–ç ç­–ç•¥ä¸éœ€è¦äº‹å…ˆäº†è§£æ¨¡å‹æ¶æ„çš„æœ€ä½³æ·±åº¦ï¼Œå®é™…ä¸Šå¯èƒ½æ˜¯å‡å°‘æœç´¢ç©ºé—´å¤æ‚æ€§çš„ä¸€ç§æ–¹å¼ã€‚è¿™ç§ç¼–ç çš„çµæ´»è®¾è®¡å¯èƒ½å°†æœ‰å…³æ¶æ„çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ç¼–ç åˆ°è§£å†³æ–¹æ¡ˆå‘é‡ä¸­ï¼Œå¹¶ä¸”è§£å†³æ–¹æ¡ˆçš„æœ€ä½³é•¿åº¦åœ¨æœç´¢è¿‡ç¨‹ä¸­è‡ªåŠ¨æ‰¾åˆ°ï¼ˆLiu
    et al., [2021b](#bib.bib117)ï¼‰ã€‚åœ¨ï¼ˆChen et al., [2020](#bib.bib27)ï¼‰ä¸­ï¼Œæ•´ä¸ªå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¢«åˆ†ä¸ºå››ä¸ªå—ï¼ŒåŒ…æ‹¬hå—ã€$\mu$å—ã€$\sigma$å—å’Œtå—ï¼Œè€Œå˜é•¿æŸ“è‰²ä½“åˆ™ç”±ä¸åŒæ•°é‡å’Œç±»å‹çš„å±‚ç»„æˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå˜é•¿ç¼–ç ä¸å®¹æ˜“ç›´æ¥åº”ç”¨æ ‡å‡†é—ä¼ æ“ä½œï¼ˆä¾‹å¦‚äº¤å‰ï¼‰ã€‚
- en: Since the neural network architectures are composed of basic units and connections
    between them, so that both of them are to be encoded, as suggested in (Liu etÂ al.,
    [2021b](#bib.bib117)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç¥ç»ç½‘ç»œæ¶æ„ç”±åŸºæœ¬å•å…ƒåŠå…¶ä¹‹é—´çš„è¿æ¥ç»„æˆï¼Œå› æ­¤è¿™ä¸¤è€…éƒ½éœ€è¦ç¼–ç ï¼Œå¦‚ï¼ˆLiu et al., [2021b](#bib.bib117)ï¼‰æ‰€å»ºè®®çš„ã€‚
- en: 1) Encoding hyperparameters of basic units. In CNNs, there are many hyperparameters
    to be specified for each unit (e.g., layer, block or cell), such as feature map
    size, type of convolution layer, and filter size (Sun etÂ al., [2020](#bib.bib192)).
    In (Sun etÂ al., [2019b](#bib.bib189)), DenseBlock only had to set two hyperparameters
    (e.g., block type and specific parameter of internal unit) to configure the block
    can be seen as a microcosm of a complete CNN model. The parameterization of a
    cell is more flexible than that of a block since it can be configured via a combination
    of different primitive layers (Sun etÂ al., [2019c](#bib.bib190)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 1) åŸºæœ¬å•å…ƒçš„è¶…å‚æ•°ç¼–ç ã€‚åœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­ï¼Œæ¯ä¸ªå•å…ƒï¼ˆä¾‹å¦‚ï¼Œå±‚ã€å—æˆ–ç»†èƒï¼‰éƒ½æœ‰è®¸å¤šéœ€è¦æŒ‡å®šçš„è¶…å‚æ•°ï¼Œå¦‚ç‰¹å¾å›¾å¤§å°ã€å·ç§¯å±‚ç±»å‹å’Œæ»¤æ³¢å™¨å¤§å°ï¼ˆSun
    et al., [2020](#bib.bib192)ï¼‰ã€‚åœ¨ï¼ˆSun et al., [2019b](#bib.bib189)ï¼‰ä¸­ï¼ŒDenseBlock åªéœ€è®¾ç½®ä¸¤ä¸ªè¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œå—ç±»å‹å’Œå†…éƒ¨å•å…ƒçš„å…·ä½“å‚æ•°ï¼‰å³å¯é…ç½®è¯¥å—ï¼Œè¿™å¯ä»¥çœ‹ä½œæ˜¯å®Œæ•´
    CNN æ¨¡å‹çš„ç¼©å½±ã€‚ä¸å—çš„å‚æ•°åŒ–ç›¸æ¯”ï¼Œç»†èƒçš„å‚æ•°åŒ–æ›´åŠ çµæ´»ï¼Œå› ä¸ºå®ƒå¯ä»¥é€šè¿‡ä¸åŒåŸå§‹å±‚çš„ç»„åˆæ¥é…ç½®ï¼ˆSun et al., [2019c](#bib.bib190)ï¼‰ã€‚
- en: '2) Encoding connections between units. In general, there are two kinds of model
    architectures according to the connection patterns of basic units: linear topological
    architectures and non-linear topological architectures (Yang etÂ al., [2020](#bib.bib230)).
    The linear pattern of architecture consists of sequential basic units, and the
    non-linear pattern allows for skip or loop connections in the architecture (Liu
    etÂ al., [2021b](#bib.bib117)).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 2) å•å…ƒä¹‹é—´è¿æ¥çš„ç¼–ç ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ ¹æ®åŸºæœ¬å•å…ƒçš„è¿æ¥æ¨¡å¼ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„æ¨¡å‹æ¶æ„ï¼šçº¿æ€§æ‹“æ‰‘æ¶æ„å’Œéçº¿æ€§æ‹“æ‰‘æ¶æ„ï¼ˆYang et al., [2020](#bib.bib230)ï¼‰ã€‚çº¿æ€§æ¶æ„çš„æ¨¡å¼ç”±è¿ç»­çš„åŸºæœ¬å•å…ƒç»„æˆï¼Œè€Œéçº¿æ€§æ¨¡å¼å…è®¸åœ¨æ¶æ„ä¸­è¿›è¡Œè·³è·ƒæˆ–å¾ªç¯è¿æ¥ï¼ˆLiu
    et al., [2021b](#bib.bib117)ï¼‰ã€‚
- en: â€¢
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Linear topological architecture: The linear topology widely appears in the
    construction of layer-wise and block-wise search spaces. Due to the simplicity
    of linear topology, basic units can be stacked one by one by a linear piecing
    method. In this way, the skeleton of an architecture can be built up effectively
    (Sun etÂ al., [2019b](#bib.bib189); Chen etÂ al., [2020](#bib.bib27)) regardless
    of the complexity of the internal of basic units.'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: çº¿æ€§æ‹“æ‰‘ç»“æ„ï¼šçº¿æ€§æ‹“æ‰‘å¹¿æ³›å‡ºç°åœ¨å±‚æ¬¡åŒ–å’Œæ¨¡å—åŒ–çš„æœç´¢ç©ºé—´æ„å»ºä¸­ã€‚ç”±äºçº¿æ€§æ‹“æ‰‘çš„ç®€å•æ€§ï¼ŒåŸºæœ¬å•å…ƒå¯ä»¥é€šè¿‡çº¿æ€§æ‹¼æ¥æ–¹æ³•ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å †å èµ·æ¥ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¶æ„çš„éª¨æ¶å¯ä»¥æœ‰æ•ˆåœ°å»ºç«‹èµ·æ¥ï¼ˆSun
    et al., [2019b](#bib.bib189); Chen et al., [2020](#bib.bib27)ï¼‰ï¼Œè€Œä¸è€ƒè™‘åŸºæœ¬å•å…ƒå†…éƒ¨çš„å¤æ‚æ€§ã€‚
- en: â€¢
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Non-linear topological architecture: Compared to the linear architecture, the
    non-linear topological architecture receives much more attention due to its flexibility
    to construct well-performing architectures (Wang etÂ al., [2021c](#bib.bib209),
    [2020b](#bib.bib207); Xie and Yuille, [2017](#bib.bib222)), such as macro structures
    composed of basic units, and micro structures within basic units. There are two
    typical encoding approaches for non-linear topological architectures. The one
    is to use adjacent matrix to represent the connections in non-linear architectures,
    where â€œ1â€ of the matrix denotes the existence of the connection between two units
    and â€œ0â€ goes the opposite. In (Lorenzo and Nalepa, [2018](#bib.bib123)), skip
    connections are represented by a matrix where constraints can be set in place
    to guarantee valid encoding and avoid recurrent edges while performing skip connections.
    Note that adjacent matrix has a limitation that the number of basic units needs
    to be fixed in advance (Kitano, [1990](#bib.bib97)). Another one is to utilize
    an ordered pair to represent a directed acyclic graph, and then encode the connections
    between unites. The ordered pair can be formulated as $G$ = ($V$, $E$) where $V$
    is a set of vertices and $E$ is a directed edge in the acyclic graph, and it has
    been applied in (Irwin-Harris etÂ al., [2019](#bib.bib84)) to encode the connections.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: éçº¿æ€§æ‹“æ‰‘ç»“æ„ï¼šä¸çº¿æ€§ç»“æ„ç›¸æ¯”ï¼Œéçº¿æ€§æ‹“æ‰‘ç»“æ„å› å…¶åœ¨æ„å»ºé«˜æ€§èƒ½æ¶æ„æ–¹é¢çš„çµæ´»æ€§è€Œå—åˆ°æ›´å¤šå…³æ³¨ï¼ˆWang et al., [2021c](#bib.bib209),
    [2020b](#bib.bib207); Xie and Yuille, [2017](#bib.bib222)ï¼‰ï¼Œä¾‹å¦‚ç”±åŸºæœ¬å•å…ƒç»„æˆçš„å®è§‚ç»“æ„å’ŒåŸºæœ¬å•å…ƒå†…éƒ¨çš„å¾®è§‚ç»“æ„ã€‚éçº¿æ€§æ‹“æ‰‘ç»“æ„æœ‰ä¸¤ç§å…¸å‹çš„ç¼–ç æ–¹æ³•ã€‚ä¸€ç§æ˜¯ä½¿ç”¨é‚»æ¥çŸ©é˜µè¡¨ç¤ºéçº¿æ€§æ¶æ„ä¸­çš„è¿æ¥ï¼Œå…¶ä¸­çŸ©é˜µä¸­çš„â€œ1â€è¡¨ç¤ºä¸¤ä¸ªå•å…ƒä¹‹é—´çš„è¿æ¥å­˜åœ¨ï¼Œè€Œâ€œ0â€è¡¨ç¤ºè¿æ¥ä¸å­˜åœ¨ã€‚åœ¨ï¼ˆLorenzo
    and Nalepa, [2018](#bib.bib123)ï¼‰ä¸­ï¼Œè·³è·ƒè¿æ¥ç”±ä¸€ä¸ªçŸ©é˜µè¡¨ç¤ºï¼Œå…¶ä¸­å¯ä»¥è®¾ç½®çº¦æŸä»¥ä¿è¯æœ‰æ•ˆç¼–ç ï¼Œå¹¶é¿å…åœ¨æ‰§è¡Œè·³è·ƒè¿æ¥æ—¶å‡ºç°é‡å¤è¾¹ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé‚»æ¥çŸ©é˜µæœ‰ä¸€ä¸ªé™åˆ¶ï¼Œå³åŸºæœ¬å•å…ƒçš„æ•°é‡éœ€è¦é¢„å…ˆå›ºå®šï¼ˆKitano,
    [1990](#bib.bib97)ï¼‰ã€‚å¦ä¸€ç§æ˜¯åˆ©ç”¨æœ‰åºå¯¹è¡¨ç¤ºæœ‰å‘æ— ç¯å›¾ï¼Œç„¶åç¼–ç å•å…ƒä¹‹é—´çš„è¿æ¥ã€‚è¿™ä¸ªæœ‰åºå¯¹å¯ä»¥è¡¨ç¤ºä¸º $G$ = ($V$, $E$)ï¼Œå…¶ä¸­
    $V$ æ˜¯é¡¶ç‚¹çš„é›†åˆï¼Œ$E$ æ˜¯æ— ç¯å›¾ä¸­çš„æœ‰å‘è¾¹ï¼Œå¹¶å·²åœ¨ï¼ˆIrwin-Harris et al., [2019](#bib.bib84)ï¼‰ä¸­åº”ç”¨äºè¿æ¥çš„ç¼–ç ã€‚
- en: 4.2.3\. Search Paradigms
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 æœç´¢èŒƒå¼
- en: In this section, the commonly used EC-based search paradigms for NAS are introduced.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä»‹ç»äº†ç”¨äºNASçš„å¸¸ç”¨ECåŸºç¡€æœç´¢èŒƒå¼ã€‚
- en: 'Basic EC search paradigm: Many basic EC algorithms have been widely applied
    in existing NAS methods, such as GA (Kitano, [1990](#bib.bib97)) and PSO (Sun
    etÂ al., [2019d](#bib.bib191)). A general framework of EC is presented in Fig.
    [3](#S2.F3 "Figure 3 â€£ 2.2\. Evolutionary Computation â€£ 2\. An Overview of Evolutionary
    Deep Learning â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŸºæœ¬ECæœç´¢èŒƒå¼ï¼šè®¸å¤šåŸºæœ¬çš„ECç®—æ³•å·²å¹¿æ³›åº”ç”¨äºç°æœ‰çš„NASæ–¹æ³•ä¸­ï¼Œå¦‚GA (Kitano, [1990](#bib.bib97)) å’ŒPSO (Sun
    et al., [2019d](#bib.bib191))ã€‚ECçš„ä¸€èˆ¬æ¡†æ¶è§å›¾ [3](#S2.F3 "Figure 3 â€£ 2.2\. Evolutionary
    Computation â€£ 2\. An Overview of Evolutionary Deep Learning â€£ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")ã€‚'
- en: 'Incremental search paradigm: A model architecture can be built in an incremental
    way where model elements (e.g., layers and connections) are gradually added to
    the model during the evolutionary process (Liu etÂ al., [2018c](#bib.bib111); Wang
    etÂ al., [2020c](#bib.bib208); Shen etÂ al., [2019](#bib.bib179)). This way allows
    to find parts of architecture at different optimization stages, which reduces
    the computational burden on acquiring a complete model at once (Liu etÂ al., [2018c](#bib.bib111)).
    For example, Wang et al. (Wang etÂ al., [2020c](#bib.bib208)) used an incremental
    approach to stack blocks for building architectures, which improved the capacity
    of the final architecture via a progressive process.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å¢é‡æœç´¢èŒƒå¼ï¼šæ¨¡å‹æ¶æ„å¯ä»¥ä»¥å¢é‡çš„æ–¹å¼æ„å»ºï¼Œå…¶ä¸­æ¨¡å‹å…ƒç´ ï¼ˆä¾‹å¦‚ï¼Œå±‚å’Œè¿æ¥ï¼‰åœ¨è¿›åŒ–è¿‡ç¨‹ä¸­é€æ­¥æ·»åŠ åˆ°æ¨¡å‹ä¸­ï¼ˆLiu et al., [2018c](#bib.bib111);
    Wang et al., [2020c](#bib.bib208); Shen et al., [2019](#bib.bib179)ï¼‰ã€‚è¿™ç§æ–¹å¼å…è®¸åœ¨ä¸åŒä¼˜åŒ–é˜¶æ®µæ‰¾åˆ°æ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œä»è€Œå‡å°‘ä¸€æ¬¡æ€§è·å–å®Œæ•´æ¨¡å‹çš„è®¡ç®—è´Ÿæ‹…ï¼ˆLiu
    et al., [2018c](#bib.bib111)ï¼‰ã€‚ä¾‹å¦‚ï¼ŒWang et al. (Wang et al., [2020c](#bib.bib208))
    ä½¿ç”¨å¢é‡æ–¹æ³•å †å å—æ¥æ„å»ºæ¶æ„ï¼Œé€šè¿‡æ¸è¿›è¿‡ç¨‹æé«˜äº†æœ€ç»ˆæ¶æ„çš„èƒ½åŠ›ã€‚
- en: 'Co-evolution search paradigm: An architecture optimization problem is decomposed
    into the optimizations of a blueprint and its components (Oâ€™Neill etÂ al., [2018](#bib.bib150);
    Zhang etÂ al., [2022b](#bib.bib241)). Specifically, the blueprint plays a role
    in specifying the topological connection patterns of its components, and an optimal
    architecture is acquired by cooperatively optimizing the blueprint and its components.
    For example, Oâ€™Neill et al. (Oâ€™Neill etÂ al., [2018](#bib.bib150)) proposed a co-evolution
    search paradigm for NAS, where the candidate blueprints and components were sampled
    from two populations, and then combined to form new architectures.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ååŒè¿›åŒ–æœç´¢èŒƒå¼ï¼šä¸€ä¸ªæ¶æ„ä¼˜åŒ–é—®é¢˜è¢«åˆ†è§£ä¸ºè“å›¾åŠå…¶ç»„ä»¶çš„ä¼˜åŒ–ï¼ˆOâ€™Neill et al., [2018](#bib.bib150); Zhang et
    al., [2022b](#bib.bib241)ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œè“å›¾åœ¨æŒ‡å®šå…¶ç»„ä»¶çš„æ‹“æ‰‘è¿æ¥æ¨¡å¼æ–¹é¢å‘æŒ¥ä½œç”¨ï¼Œé€šè¿‡ååŒä¼˜åŒ–è“å›¾åŠå…¶ç»„ä»¶æ¥è·å¾—æœ€ä½³æ¶æ„ã€‚ä¾‹å¦‚ï¼ŒOâ€™Neill
    et al. (Oâ€™Neill et al., [2018](#bib.bib150)) æå‡ºäº†ç”¨äºNASçš„ååŒè¿›åŒ–æœç´¢èŒƒå¼ï¼Œå…¶ä¸­å€™é€‰è“å›¾å’Œç»„ä»¶ä»ä¸¤ä¸ªç§ç¾¤ä¸­é‡‡æ ·ï¼Œç„¶åç»„åˆå½¢æˆæ–°çš„æ¶æ„ã€‚
- en: 'Multi-objective search paradigm: This paradigm targets at searching for a set
    of Pareto optimal architectures based on multiple criteria, and finding the final
    solutions according to some practical considerations, such as computational environment
    (Neshat etÂ al., [2020](#bib.bib144); Lu etÂ al., [2019](#bib.bib126)). This paradigm
    becomes popular in practical applications, since many objectives are required
    to be considered such as the accuracy, inference time, model size, and energy
    consumption. In (Neshat etÂ al., [2020](#bib.bib144)), NSGA-II and RL were used
    to explore model architectures with respect to the model accuracy, and model complexity
    (e.g., the number of model parameters and multiply-adds operators).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šç›®æ ‡æœç´¢èŒƒå¼ï¼šè¯¥èŒƒå¼æ—¨åœ¨åŸºäºå¤šä¸ªæ ‡å‡†æœç´¢ä¸€ç»„å¸•ç´¯æ‰˜æœ€ä¼˜æ¶æ„ï¼Œå¹¶æ ¹æ®ä¸€äº›å®é™…è€ƒè™‘å› ç´ ï¼ˆå¦‚è®¡ç®—ç¯å¢ƒï¼‰å¯»æ‰¾æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼ˆNeshat et al., [2020](#bib.bib144);
    Lu et al., [2019](#bib.bib126)ï¼‰ã€‚è¯¥èŒƒå¼åœ¨å®é™…åº”ç”¨ä¸­å˜å¾—æµè¡Œï¼Œå› ä¸ºéœ€è¦è€ƒè™‘è®¸å¤šç›®æ ‡ï¼Œå¦‚å‡†ç¡®æ€§ã€æ¨ç†æ—¶é—´ã€æ¨¡å‹å¤§å°å’Œèƒ½è€—ã€‚åœ¨ (Neshat
    et al., [2020](#bib.bib144)) ä¸­ï¼Œä½¿ç”¨äº†NSGA-IIå’ŒRLæ¥æ¢ç´¢ä¸æ¨¡å‹å‡†ç¡®æ€§å’Œæ¨¡å‹å¤æ‚æ€§ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹å‚æ•°æ•°é‡å’Œä¹˜åŠ æ“ä½œæ•°ï¼‰ç›¸å…³çš„æ¨¡å‹æ¶æ„ã€‚
- en: 4.2.4\. Acceleration Strategies
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. åŠ é€Ÿç­–ç•¥
- en: 'NAS is a high computational overhead task, mainly due to the large search space
    and highly time-consuming evaluation (Yao etÂ al., [2018](#bib.bib231)). To overcome
    this challenge, various acceleration strategies (Sun etÂ al., [2019d](#bib.bib191);
    AssunÃ§Ã£o etÂ al., [2019b](#bib.bib13)) have been developed to accelerate the optimization.
    In this section, we summarize the speed-up strategies from the aspects of algorithm
    design to the hardware implementation, as shown in Fig. [6](#S4.F6 "Figure 6 â€£
    4.2.4\. Acceleration Strategies â€£ 4.2\. Model Architecture Optimization â€£ 4\.
    Model Generation â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'NAS æ˜¯ä¸€ä¸ªé«˜è®¡ç®—å¼€é”€çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå¤§çš„æœç´¢ç©ºé—´å’Œé«˜åº¦è€—æ—¶çš„è¯„ä¼° (Yao et al., [2018](#bib.bib231))ã€‚ä¸ºå…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œå·²ç»å¼€å‘äº†å„ç§åŠ é€Ÿç­–ç•¥
    (Sun et al., [2019d](#bib.bib191); AssunÃ§Ã£o et al., [2019b](#bib.bib13)) ä»¥åŠ é€Ÿä¼˜åŒ–ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»ç®—æ³•è®¾è®¡åˆ°ç¡¬ä»¶å®ç°çš„å„ä¸ªæ–¹é¢æ€»ç»“äº†åŠ é€Ÿç­–ç•¥ï¼Œå¦‚å›¾
    [6](#S4.F6 "Figure 6 â€£ 4.2.4\. Acceleration Strategies â€£ 4.2\. Model Architecture
    Optimization â€£ 4\. Model Generation â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") æ‰€ç¤ºã€‚'
- en: '![Refer to caption](img/f173dbb8c1a13e886eadc7aa6aba2e6a.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/f173dbb8c1a13e886eadc7aa6aba2e6a.png)'
- en: Figure 6. Overview of acceleration strategies.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6. åŠ é€Ÿç­–ç•¥æ¦‚è§ˆã€‚
- en: From the algorithm design point of view, we summarized a number of acceleration
    strategies from population initialization to evaluations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç®—æ³•è®¾è®¡çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬æ€»ç»“äº†ä»ç§ç¾¤åˆå§‹åŒ–åˆ°è¯„ä¼°çš„å¤šç§åŠ é€Ÿç­–ç•¥ã€‚
- en: â€¢
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Initialization:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ï¼š
- en: â€“
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: 'Reduced population: The simplest way of acceleration during the initialization
    stage is to set the population with a small size. In other words, less evaluations
    are required with a smaller size of population since the evaluation of a candidate
    architecture is time-consuming (Yao etÂ al., [2018](#bib.bib231)). As a result,
    some studies (AssunÃ§Ã£o etÂ al., [2019b](#bib.bib13), [2018](#bib.bib12)) use small
    population with fixed size to speed up their evolution, like CARS (size = 32)
    (Yang etÂ al., [2020](#bib.bib230)). In contrast, some other studies use dynamic
    sizes of populations during the optimization. In (Fan etÂ al., [2020](#bib.bib56)),
    the population size is dynamically changed to reach a balance between algorithmic
    efficiency and population diversity.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‡å°‘ç§ç¾¤ï¼šåœ¨åˆå§‹åŒ–é˜¶æ®µï¼ŒåŠ é€Ÿçš„æœ€ç®€å•æ–¹æ³•æ˜¯è®¾å®šä¸€ä¸ªè¾ƒå°çš„ç§ç¾¤ã€‚æ¢å¥è¯è¯´ï¼Œè¾ƒå°çš„ç§ç¾¤éœ€è¦æ›´å°‘çš„è¯„ä¼°ï¼Œå› ä¸ºå€™é€‰æ¶æ„çš„è¯„ä¼°æ˜¯è€—æ—¶çš„ (Yao et al.,
    [2018](#bib.bib231))ã€‚å› æ­¤ï¼Œä¸€äº›ç ”ç©¶ (AssunÃ§Ã£o et al., [2019b](#bib.bib13), [2018](#bib.bib12))
    ä½¿ç”¨å›ºå®šå¤§å°çš„å°ç§ç¾¤æ¥åŠ é€Ÿå…¶æ¼”åŒ–ï¼Œæ¯”å¦‚ CARS (å¤§å° = 32) (Yang et al., [2020](#bib.bib230))ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¶ä»–ä¸€äº›ç ”ç©¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä½¿ç”¨åŠ¨æ€å¤§å°çš„ç§ç¾¤ã€‚åœ¨
    (Fan et al., [2020](#bib.bib56)) ä¸­ï¼Œç§ç¾¤å¤§å°åŠ¨æ€å˜åŒ–ï¼Œä»¥åœ¨ç®—æ³•æ•ˆç‡å’Œç§ç¾¤å¤šæ ·æ€§ä¹‹é—´è¾¾åˆ°å¹³è¡¡ã€‚
- en: â€“
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: 'Efficient search space: Another way is to design efficient search space to
    speed up the search process. For example, an architecture constructed on the basis
    of cell-wise search space (Liu etÂ al., [2021b](#bib.bib117)) is composed of many
    similar structures of cells and only representative cells need to be optimized,
    which contributes to significant computational speed-up.'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: é«˜æ•ˆçš„æœç´¢ç©ºé—´ï¼šå¦ä¸€ç§æ–¹æ³•æ˜¯è®¾è®¡é«˜æ•ˆçš„æœç´¢ç©ºé—´ä»¥åŠ é€Ÿæœç´¢è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼ŒåŸºäºç»†èƒæœç´¢ç©ºé—´ (Liu et al., [2021b](#bib.bib117))
    æ„å»ºçš„æ¶æ„ç”±è®¸å¤šç›¸ä¼¼çš„ç»†èƒç»“æ„ç»„æˆï¼Œä»…éœ€è¦ä¼˜åŒ–ä»£è¡¨æ€§ç»†èƒï¼Œè¿™æœ‰åŠ©äºæ˜¾è‘—åŠ å¿«è®¡ç®—é€Ÿåº¦ã€‚
- en: â€¢
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Evaluations:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯„ä¼°ï¼š
- en: (1)
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Early stopping policy: A relatively small number of training epochs are used
    to reduce the training cost (i.e., early stopping policy) since the training time
    is reduced (Ahmed etÂ al., [2019](#bib.bib3); Tian etÂ al., [2019](#bib.bib199);
    Sun etÂ al., [2019d](#bib.bib191)).'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ—©æœŸåœæ­¢ç­–ç•¥ï¼šä½¿ç”¨ç›¸å¯¹è¾ƒå°‘çš„è®­ç»ƒå‘¨æœŸæ¥å‡å°‘è®­ç»ƒæˆæœ¬ï¼ˆå³æ—©æœŸåœæ­¢ç­–ç•¥ï¼‰ï¼Œå› ä¸ºè®­ç»ƒæ—¶é—´å‡å°‘ (Ahmed et al., [2019](#bib.bib3);
    Tian et al., [2019](#bib.bib199); Sun et al., [2019d](#bib.bib191))ã€‚
- en: (2)
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Reduced training set: Some methods are designed to reduce the size of the training
    set to improve training efficiency at the expense of a little accuracy (Sapra
    and Pimentel, [2020](#bib.bib173); Liu etÂ al., [2019a](#bib.bib114)). Besides,
    low-resolution data (e.g., ImageNet 32) (Chrabaszcz etÂ al., [2017](#bib.bib33))
    is also commonly used as the training set to accelerate the search process for
    the optimal architecture.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‡å°‘è®­ç»ƒé›†ï¼šä¸€äº›æ–¹æ³•æ—¨åœ¨å‡å°‘è®­ç»ƒé›†çš„å¤§å°ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ï¼Œä½†ä¼šç‰ºç‰²ä¸€äº›å‡†ç¡®æ€§ (Sapra and Pimentel, [2020](#bib.bib173);
    Liu et al., [2019a](#bib.bib114))ã€‚æ­¤å¤–ï¼Œä½åˆ†è¾¨ç‡æ•°æ® (ä¾‹å¦‚ï¼ŒImageNet 32) (Chrabaszcz et al.,
    [2017](#bib.bib33)) ä¹Ÿå¸¸ç”¨äºè®­ç»ƒé›†ï¼Œä»¥åŠ é€Ÿæœç´¢æœ€ä½³æ¶æ„çš„è¿‡ç¨‹ã€‚
- en: (3)
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Weight inheritance Â¹Â¹1In (Rumelhart etÂ al., [1985](#bib.bib168), [1986](#bib.bib169);
    LeCun etÂ al., [1989](#bib.bib106)), Rumelhart/Hinton/LeCun used the term â€weight
    sharingâ€ to mean that different network connections/links share the same set of
    weights, and pointed out that â€weight sharingâ€ is the core of shared weight NNs/CNNs.
    More recent (Lu etÂ al., [2021](#bib.bib125); Xie etÂ al., [2022a](#bib.bib221))
    use of this term refers to â€weight/parameter replicationsâ€ or â€weight inheritanceâ€.:'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: æƒé‡ç»§æ‰¿ Â¹Â¹1Inï¼ˆRumelhart et al., [1985](#bib.bib168), [1986](#bib.bib169); LeCun
    et al., [1989](#bib.bib106)ï¼‰ï¼ŒRumelhart/Hinton/LeCun ä½¿ç”¨â€œæƒé‡å…±äº«â€ä¸€è¯æ¥è¡¨ç¤ºä¸åŒçš„ç½‘ç»œè¿æ¥/é“¾æ¥å…±äº«ç›¸åŒçš„æƒé‡é›†ï¼Œå¹¶æŒ‡å‡ºâ€œæƒé‡å…±äº«â€æ˜¯å…±äº«æƒé‡
    NNs/CNNs çš„æ ¸å¿ƒã€‚æœ€è¿‘ï¼ˆLu et al., [2021](#bib.bib125); Xie et al., [2022a](#bib.bib221)ï¼‰å¯¹è¿™ä¸ªæœ¯è¯­çš„ä½¿ç”¨æŒ‡çš„æ˜¯â€œæƒé‡/å‚æ•°å¤åˆ¶â€æˆ–â€œæƒé‡ç»§æ‰¿â€ã€‚
- en: â€“
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: 'Supernet-based inheritance: uses an over-parameterized and pre-trained supernet
    to encode all candidate architectures (i.e., subnets). In other words, the subnets
    share weights of the identical structures from the supernet, and they are directly
    evaluated on the validation dataset to obtain their model accuracy (Rapaport etÂ al.,
    [2019](#bib.bib159); Dahal and Zhan, [2020](#bib.bib38); Elsken etÂ al., [2019](#bib.bib50);
    Sapra and Pimentel, [2020](#bib.bib173)).'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºäºè¶…ç½‘ç»œçš„ç»§æ‰¿ï¼šä½¿ç”¨ä¸€ä¸ªè¿‡åº¦å‚æ•°åŒ–ä¸”é¢„è®­ç»ƒçš„è¶…ç½‘ç»œæ¥ç¼–ç æ‰€æœ‰å€™é€‰æ¶æ„ï¼ˆå³å­ç½‘ç»œï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œå­ç½‘ç»œä»è¶…ç½‘ç»œä¸­å…±äº«ç›¸åŒç»“æ„çš„æƒé‡ï¼Œå¹¶åœ¨éªŒè¯æ•°æ®é›†ä¸Šç›´æ¥è¯„ä¼°ä»¥è·å¾—å…¶æ¨¡å‹å‡†ç¡®æ€§ï¼ˆRapaport
    et al., [2019](#bib.bib159); Dahal and Zhan, [2020](#bib.bib38); Elsken et al.,
    [2019](#bib.bib50); Sapra and Pimentel, [2020](#bib.bib173)ï¼‰ã€‚
- en: â€“
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: 'Parent-based inheritance: inherits weights from previously-trained networks
    (i.e., parental networks) instead of a supernet, since offspring individuals retain
    some identical parts of their parental architectures (Real etÂ al., [2017](#bib.bib163);
    Elsken etÂ al., [2017](#bib.bib49); Kwasigroch etÂ al., [2019](#bib.bib104); Zhu
    etÂ al., [2019](#bib.bib262)) (Real etÂ al., [2017](#bib.bib163)). As a result,
    offspring architectures can inherit the weights of the identical parts and no
    longer need to be trained from scratch.'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºäºçˆ¶ä½“çš„ç»§æ‰¿ï¼šä»å…ˆå‰è®­ç»ƒè¿‡çš„ç½‘ç»œï¼ˆå³çˆ¶ç½‘ç»œï¼‰ç»§æ‰¿æƒé‡ï¼Œè€Œä¸æ˜¯ä»è¶…ç½‘ç»œï¼Œå› ä¸ºåä»£ä¸ªä½“ä¿ç•™äº†çˆ¶æ¯æ¶æ„çš„ä¸€äº›ç›¸åŒéƒ¨åˆ†ï¼ˆReal et al., [2017](#bib.bib163);
    Elsken et al., [2017](#bib.bib49); Kwasigroch et al., [2019](#bib.bib104); Zhu
    et al., [2019](#bib.bib262))ï¼ˆReal et al., [2017](#bib.bib163)ï¼‰ã€‚å› æ­¤ï¼Œåä»£æ¶æ„å¯ä»¥ç»§æ‰¿ç›¸åŒéƒ¨åˆ†çš„æƒé‡ï¼Œè€Œä¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒã€‚
- en: (4)
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Surrogate model: Since the evaluation of an architecture is time-consuming
    (Yao etÂ al., [2018](#bib.bib231); Liu etÂ al., [2021b](#bib.bib117)), cheap surrogate
    models have been introduced in NAS as performance predictors to reduce the computational
    time (Lu etÂ al., [2021](#bib.bib125)).'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ›¿ä»£æ¨¡å‹ï¼šç”±äºæ¶æ„è¯„ä¼°æ˜¯è€—æ—¶çš„ï¼ˆYao et al., [2018](#bib.bib231); Liu et al., [2021b](#bib.bib117)ï¼‰ï¼Œåœ¨
    NAS ä¸­å¼•å…¥äº†å»‰ä»·çš„æ›¿ä»£æ¨¡å‹ä½œä¸ºæ€§èƒ½é¢„æµ‹å™¨ï¼Œä»¥å‡å°‘è®¡ç®—æ—¶é—´ï¼ˆLu et al., [2021](#bib.bib125)ï¼‰ã€‚
- en: â€“
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: 'Online performance predictors: They are trained online on the datasets sampled
    from past several epochs (Liu etÂ al., [2021b](#bib.bib117)), including a sequence
    of data pairs with different training epochs and their corresponding performance
    of these epochs (Lu etÂ al., [2021](#bib.bib125)). After that, they will be used
    for the performance prediction on new architectures. To reduce the true evaluations
    of architectures, some performance predictors directly predict whether a candidate
    architecture can be survived into next iteration through a trained ranking or
    classification method, such as classification-wise NAS (Ma etÂ al., [2021a](#bib.bib130)).'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨çº¿æ€§èƒ½é¢„æµ‹å™¨ï¼šå®ƒä»¬åœ¨çº¿è®­ç»ƒï¼Œä½¿ç”¨æ¥è‡ªè¿‡å»å‡ ä¸ªè®­ç»ƒå‘¨æœŸçš„æ•°æ®é›†ï¼ˆLiu et al., [2021b](#bib.bib117)ï¼‰ï¼ŒåŒ…æ‹¬å…·æœ‰ä¸åŒè®­ç»ƒå‘¨æœŸçš„æ•°æ®å¯¹åŠè¿™äº›å‘¨æœŸå¯¹åº”çš„æ€§èƒ½ï¼ˆLu
    et al., [2021](#bib.bib125)ï¼‰ã€‚ä¹‹åï¼Œå®ƒä»¬å°†ç”¨äºå¯¹æ–°æ¶æ„è¿›è¡Œæ€§èƒ½é¢„æµ‹ã€‚ä¸ºäº†å‡å°‘å¯¹æ¶æ„çš„çœŸå®è¯„ä¼°ï¼Œä¸€äº›æ€§èƒ½é¢„æµ‹å™¨ç›´æ¥é€šè¿‡è®­ç»ƒçš„æ’åºæˆ–åˆ†ç±»æ–¹æ³•é¢„æµ‹å€™é€‰æ¶æ„æ˜¯å¦èƒ½è¿›å…¥ä¸‹ä¸€æ¬¡è¿­ä»£ï¼Œä¾‹å¦‚åˆ†ç±»å¯¼å‘çš„
    NASï¼ˆMa et al., [2021a](#bib.bib130)ï¼‰ã€‚
- en: â€“
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: 'Offline performance predictors: They are essentially a sort of regression models
    mapping the architectures to specific performance. End-to-end predictors can be
    trained in an offline manner, so that they are able to predict the performance
    of architectures during the entire search process. Consequently, they can significantly
    reduce the computational burden (Sun etÂ al., [2019a](#bib.bib188); Liu etÂ al.,
    [2018c](#bib.bib111); Sun etÂ al., [2021](#bib.bib187)).'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¦»çº¿æ€§èƒ½é¢„æµ‹å™¨ï¼šå®ƒä»¬æœ¬è´¨ä¸Šæ˜¯ä¸€ç§å›å½’æ¨¡å‹ï¼Œå°†æ¶æ„æ˜ å°„åˆ°ç‰¹å®šæ€§èƒ½ã€‚ç«¯åˆ°ç«¯é¢„æµ‹å™¨å¯ä»¥ä»¥ç¦»çº¿æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¾¿åœ¨æ•´ä¸ªæœç´¢è¿‡ç¨‹ä¸­é¢„æµ‹æ¶æ„çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå®ƒä»¬å¯ä»¥æ˜¾è‘—å‡å°‘è®¡ç®—è´Ÿæ‹…ï¼ˆSun
    et al., [2019a](#bib.bib188); Liu et al., [2018c](#bib.bib111); Sun et al., [2021](#bib.bib187)ï¼‰ã€‚
- en: (5)
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: 'Population memory: Population memory is used to store elite individuals from
    different generations during the optimization (Sun etÂ al., [2020](#bib.bib192);
    Fujino etÂ al., [2017](#bib.bib62)). When a new individual is generated, it does
    not need to be evaluated again if it is the same as an individual in the memory.
    In other words, the performance of individuals sharing the same architectures
    are the same and can be acquired via the population memory instead of training
    from scratch. This mechanism relies on the fact that similar or same individuals
    may repeatedly appear in different generations.'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: äººå£è®°å¿†ï¼šäººå£è®°å¿†ç”¨äºåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å­˜å‚¨æ¥è‡ªä¸åŒä¸–ä»£çš„ç²¾è‹±ä¸ªä½“ï¼ˆSun et al., [2020](#bib.bib192); Fujino et al.,
    [2017](#bib.bib62)ï¼‰ã€‚å½“ç”Ÿæˆæ–°çš„ä¸ªä½“æ—¶ï¼Œå¦‚æœå®ƒä¸è®°å¿†ä¸­çš„ä¸ªä½“ç›¸åŒï¼Œåˆ™æ— éœ€å†æ¬¡è¯„ä¼°ã€‚æ¢å¥è¯è¯´ï¼Œå…·æœ‰ç›¸åŒæ¶æ„çš„ä¸ªä½“çš„è¡¨ç°æ˜¯ç›¸åŒçš„ï¼Œå¯ä»¥é€šè¿‡äººå£è®°å¿†è·å–ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚è¯¥æœºåˆ¶ä¾èµ–äºç›¸ä¼¼æˆ–ç›¸åŒçš„ä¸ªä½“å¯èƒ½åœ¨ä¸åŒä¸–ä»£ä¸­é‡å¤å‡ºç°è¿™ä¸€äº‹å®ã€‚
- en: According to the above introduction, we can conclude that many of them improve
    the search efficiency at the expense of sub-optimiality. For example, a small
    population cannot well cover a multi-objective optimal front. Parameter sharing
    may lead to the biased search due to much similarities among the individuals.
    Highly accurate surrogates need a large number of training data, which are commonly
    time-consuming. Population memory heavily relies on the random emergence of similar
    or same individuals.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä¸Šè¿°ä»‹ç»ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œè®¸å¤šæ–¹æ³•é€šè¿‡ç‰ºç‰²æ¬¡ä¼˜æ€§æ¥æé«˜æœç´¢æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œå°è§„æ¨¡ç§ç¾¤æ— æ³•å¾ˆå¥½åœ°è¦†ç›–å¤šç›®æ ‡æœ€ä¼˜å‰æ²¿ã€‚å‚æ•°å…±äº«å¯èƒ½ç”±äºä¸ªä½“ä¹‹é—´çš„ç›¸ä¼¼æ€§è¿‡å¤šè€Œå¯¼è‡´åå€šæœç´¢ã€‚é«˜åº¦å‡†ç¡®çš„ä»£ç†éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè¿™é€šå¸¸æ˜¯æ—¶é—´æ¶ˆè€—å¤§çš„ã€‚äººå£è®°å¿†åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºç›¸ä¼¼æˆ–ç›¸åŒä¸ªä½“çš„éšæœºå‡ºç°ã€‚
- en: 'Hardware implementation: Importantly, a powerful hardware platform can significantly
    speed up the search process under the reasonable utilization of computing resources
    (e.g., cloud computing (Chiba etÂ al., [2019](#bib.bib31)) and volunteer computers
    (Atre etÂ al., [2021](#bib.bib14))). Parallel computation is a powerful tool to
    decompose large search problems into small sub-problems, which can be simultaneously
    optimized by several cheaper hardware(Jin etÂ al., [2019](#bib.bib88), [2018](#bib.bib87)).
    For example, Lorenzo et al. (Lorenzo etÂ al., [2017](#bib.bib124)) proposed a parallel
    PSO algorithm to search for optimal architecture of CNN. The security of the computing
    device also becomes an important consideration. For this reason, an emerging decentralized
    privacy-preserving framework is applied to NAS, which unites multiple local clients
    to collaboratively learn a shared global model trained on the parameters or gradients
    of the local models, instead of the raw data. For example, Zhu et al. (Zhu and
    Jin, [2022](#bib.bib263)) firstly proposed a real-time federated NAS that can
    not only optimize the model architecture but also reduce the computational payload.
    Specifically, the decentralized system is able to accelerate the algorithm efficiency
    of federated NAS. Besides, data encryption is employed on the transmitted data
    (parameters or gradients of the local models) between the clients and the server
    to ensure the privacy even though all of the training are performed in local.
    Accordingly, federated NAS is highly efficient and secure, which may become a
    new hot research topic.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶å®ç°ï¼šé‡è¦çš„æ˜¯ï¼Œå¼ºå¤§çš„ç¡¬ä»¶å¹³å°å¯ä»¥åœ¨åˆç†åˆ©ç”¨è®¡ç®—èµ„æºï¼ˆä¾‹å¦‚ï¼Œäº‘è®¡ç®—ï¼ˆChiba et al., [2019](#bib.bib31)ï¼‰å’Œå¿—æ„¿è®¡ç®—æœºï¼ˆAtre
    et al., [2021](#bib.bib14)ï¼‰ï¼‰çš„æƒ…å†µä¸‹æ˜¾è‘—åŠ å¿«æœç´¢è¿‡ç¨‹ã€‚å¹¶è¡Œè®¡ç®—æ˜¯å°†å¤§è§„æ¨¡æœç´¢é—®é¢˜åˆ†è§£ä¸ºå°å‹å­é—®é¢˜çš„æœ‰åŠ›å·¥å…·ï¼Œè¿™äº›å­é—®é¢˜å¯ä»¥ç”±å‡ å°ä¾¿å®œçš„ç¡¬ä»¶åŒæ—¶ä¼˜åŒ–ï¼ˆJin
    et al., [2019](#bib.bib88), [2018](#bib.bib87)ï¼‰ã€‚ä¾‹å¦‚ï¼ŒLorenzo et al.ï¼ˆLorenzo et al.,
    [2017](#bib.bib124)ï¼‰æå‡ºäº†ä¸€ç§å¹¶è¡ŒPSOç®—æ³•æ¥æœç´¢CNNçš„æœ€ä¼˜æ¶æ„ã€‚è®¡ç®—è®¾å¤‡çš„å®‰å…¨æ€§ä¹Ÿæˆä¸ºä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ ã€‚å› æ­¤ï¼Œåº”ç”¨äºNASçš„ä¸€ä¸ªæ–°å…´çš„å»ä¸­å¿ƒåŒ–éšç§ä¿æŠ¤æ¡†æ¶å°†å¤šä¸ªæœ¬åœ°å®¢æˆ·ç«¯è”åˆèµ·æ¥ï¼Œå…±åŒå­¦ä¹ ä¸€ä¸ªåŸºäºæœ¬åœ°æ¨¡å‹å‚æ•°æˆ–æ¢¯åº¦è®­ç»ƒçš„å…±äº«å…¨å±€æ¨¡å‹ï¼Œè€Œä¸æ˜¯åŸå§‹æ•°æ®ã€‚ä¾‹å¦‚ï¼ŒZhu
    et al.ï¼ˆZhu and Jin, [2022](#bib.bib263)ï¼‰é¦–æ¬¡æå‡ºäº†ä¸€ç§å®æ—¶è”é‚¦NASï¼Œä¸ä»…å¯ä»¥ä¼˜åŒ–æ¨¡å‹æ¶æ„ï¼Œè¿˜èƒ½å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚å…·ä½“æ¥è¯´ï¼Œå»ä¸­å¿ƒåŒ–ç³»ç»Ÿèƒ½å¤ŸåŠ é€Ÿè”é‚¦NASçš„ç®—æ³•æ•ˆç‡ã€‚æ­¤å¤–ï¼Œå¯¹å®¢æˆ·ç«¯ä¸æœåŠ¡å™¨ä¹‹é—´ä¼ è¾“çš„æ•°æ®ï¼ˆæœ¬åœ°æ¨¡å‹çš„å‚æ•°æˆ–æ¢¯åº¦ï¼‰è¿›è¡Œæ•°æ®åŠ å¯†ï¼Œä»¥ç¡®ä¿éšç§ï¼Œå³ä½¿æ‰€æœ‰è®­ç»ƒéƒ½åœ¨æœ¬åœ°è¿›è¡Œã€‚å› æ­¤ï¼Œè”é‚¦NASé«˜æ•ˆä¸”å®‰å…¨ï¼Œå¯èƒ½æˆä¸ºæ–°çš„çƒ­é—¨ç ”ç©¶è¯é¢˜ã€‚
- en: Table 2. Different acceleration strategies
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨2. ä¸åŒçš„åŠ é€Ÿç­–ç•¥
- en: '| Algorithm design | Initialization | Reduced population | (AssunÃ§Ã£o etÂ al.,
    [2019b](#bib.bib13)),(Fan etÂ al., [2020](#bib.bib56)),(Liu etÂ al., [2019a](#bib.bib114)),(Yang
    etÂ al., [2020](#bib.bib230))  \bigstrut |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| ç®—æ³•è®¾è®¡ | åˆå§‹åŒ– | å‡å°‘çš„ç§ç¾¤ | (AssunÃ§Ã£o et al., [2019b](#bib.bib13)),(Fan et al.,
    [2020](#bib.bib56)),(Liu et al., [2019a](#bib.bib114)),(Yang et al., [2020](#bib.bib230))
    \bigstrut |'
- en: '| Evaluation | Early stopping policy | (Sun etÂ al., [2019d](#bib.bib191)),(Ahmed
    etÂ al., [2019](#bib.bib3)),(Ortego etÂ al., [2020](#bib.bib152)),(Frachon etÂ al.,
    [2019](#bib.bib60)),(AssunÃ§Ã£o etÂ al., [2019a](#bib.bib11)),(AssunÃ§Ã£o etÂ al., [2019b](#bib.bib13)),(Mo
    etÂ al., [2021](#bib.bib139))  \bigstrut |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| è¯„ä¼° | æ—©åœç­–ç•¥ | (Sun etÂ al., [2019d](#bib.bib191)),(Ahmed etÂ al., [2019](#bib.bib3)),(Ortego
    etÂ al., [2020](#bib.bib152)),(Frachon etÂ al., [2019](#bib.bib60)),(AssunÃ§Ã£o etÂ al.,
    [2019a](#bib.bib11)),(AssunÃ§Ã£o etÂ al., [2019b](#bib.bib13)),(Mo etÂ al., [2021](#bib.bib139))
    \bigstrut |'
- en: '| Reduced training set | (Sapra and Pimentel, [2020](#bib.bib173)),(Liu etÂ al.,
    [2019a](#bib.bib114)),(Wang etÂ al., [2020c](#bib.bib208))  \bigstrut |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| å‡å°‘çš„è®­ç»ƒé›† | (Sapra and Pimentel, [2020](#bib.bib173)),(Liu etÂ al., [2019a](#bib.bib114)),(Wang
    etÂ al., [2020c](#bib.bib208)) \bigstrut |'
- en: '| Weight inheritance | Supernet-based sharing | (Sapra and Pimentel, [2020](#bib.bib173)),(Rapaport
    etÂ al., [2019](#bib.bib159)),(Dahal and Zhan, [2020](#bib.bib38)),(Elsken etÂ al.,
    [2019](#bib.bib50))  \bigstrut |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| æƒé‡ç»§æ‰¿ | åŸºäºè¶…ç½‘ç»œçš„å…±äº« | (Sapra and Pimentel, [2020](#bib.bib173)),(Rapaport etÂ al.,
    [2019](#bib.bib159)),(Dahal and Zhan, [2020](#bib.bib38)),(Elsken etÂ al., [2019](#bib.bib50))
    \bigstrut |'
- en: '| Parent-based sharing | (Elsken etÂ al., [2017](#bib.bib49)),(Kwasigroch etÂ al.,
    [2019](#bib.bib104)),(Zhu etÂ al., [2019](#bib.bib262)),(Ahmed etÂ al., [2019](#bib.bib3)),(Frachon
    etÂ al., [2019](#bib.bib60)),(Schorn etÂ al., [2020](#bib.bib174)),(Chen etÂ al.,
    [2019b](#bib.bib28))  \bigstrut |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäºçˆ¶çº§çš„å…±äº« | (Elsken etÂ al., [2017](#bib.bib49)),(Kwasigroch etÂ al., [2019](#bib.bib104)),(Zhu
    etÂ al., [2019](#bib.bib262)),(Ahmed etÂ al., [2019](#bib.bib3)),(Frachon etÂ al.,
    [2019](#bib.bib60)),(Schorn etÂ al., [2020](#bib.bib174)),(Chen etÂ al., [2019b](#bib.bib28))
    \bigstrut |'
- en: '| Surrogate model | Online performance predictors | (Lu etÂ al., [2021](#bib.bib125)),(Liu
    etÂ al., [2018c](#bib.bib111)), (Ma etÂ al., [2021a](#bib.bib130)), (Lomurno etÂ al.,
    [2021](#bib.bib119))  \bigstrut |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| æ›¿ä»£æ¨¡å‹ | åœ¨çº¿æ€§èƒ½é¢„æµ‹å™¨ | (Lu etÂ al., [2021](#bib.bib125)),(Liu etÂ al., [2018c](#bib.bib111)),
    (Ma etÂ al., [2021a](#bib.bib130)), (Lomurno etÂ al., [2021](#bib.bib119)) \bigstrut
    |'
- en: '| Offline performance predictors | (Sun etÂ al., [2019a](#bib.bib188)),(Liu
    etÂ al., [2021a](#bib.bib112)), (Rawal and Miikkulainen, [2018](#bib.bib161))\bigstrut
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| ç¦»çº¿æ€§èƒ½é¢„æµ‹å™¨ | (Sun etÂ al., [2019a](#bib.bib188)),(Liu etÂ al., [2021a](#bib.bib112)),
    (Rawal and Miikkulainen, [2018](#bib.bib161))\bigstrut |'
- en: '|  | Population memory | (Sun etÂ al., [2020](#bib.bib192)),(Miahi etÂ al., [2022](#bib.bib136)),(Li
    etÂ al., [2022](#bib.bib109)),(Chu etÂ al., [2020](#bib.bib34))  \bigstrut |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | äººå£è®°å¿† | (Sun etÂ al., [2020](#bib.bib192)),(Miahi etÂ al., [2022](#bib.bib136)),(Li
    etÂ al., [2022](#bib.bib109)),(Chu etÂ al., [2020](#bib.bib34)) \bigstrut |'
- en: '| Hardware implementation | (Chiba etÂ al., [2019](#bib.bib31)),(Jin etÂ al.,
    [2019](#bib.bib88)),(Jin etÂ al., [2018](#bib.bib87)),(Zhu and Jin, [2022](#bib.bib263))  \bigstrut
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ç¡¬ä»¶å®ç° | (Chiba etÂ al., [2019](#bib.bib31)),(Jin etÂ al., [2019](#bib.bib88)),(Jin
    etÂ al., [2018](#bib.bib87)),(Zhu and Jin, [2022](#bib.bib263)) \bigstrut |'
- en: 'Table [2](#S4.T2 "Table 2 â€£ 4.2.4\. Acceleration Strategies â€£ 4.2\. Model Architecture
    Optimization â€£ 4\. Model Generation â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") lists the common acceleration strategies
    to improve the algorithm efficiency. It is noted that multiple strategies can
    be utilized together to improve computational speed-up. For example, Lu et al.
    (Lu etÂ al., [2021](#bib.bib125)) employed supernet and learning curve performance
    predictor in NAS, while Liu et al. (Liu etÂ al., [2019a](#bib.bib114)) leveraged
    a small populations size and a small dataset to reduce the time overhead of evaluation.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ [2](#S4.T2 "Table 2 â€£ 4.2.4\. åŠ é€Ÿç­–ç•¥ â€£ 4.2\. æ¨¡å‹æ¶æ„ä¼˜åŒ– â€£ 4\. æ¨¡å‹ç”Ÿæˆ â€£ è¿›åŒ–æ·±åº¦å­¦ä¹ è°ƒæŸ¥ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨ä¸å¼€æ”¾é—®é¢˜")
    åˆ—å‡ºäº†å¸¸è§çš„åŠ é€Ÿç­–ç•¥ä»¥æé«˜ç®—æ³•æ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯ä»¥ç»“åˆå¤šç§ç­–ç•¥ä»¥æé«˜è®¡ç®—é€Ÿåº¦ã€‚ä¾‹å¦‚ï¼ŒLu et al. (Lu etÂ al., [2021](#bib.bib125))
    åœ¨ NAS ä¸­ä½¿ç”¨äº†è¶…ç½‘ç»œå’Œå­¦ä¹ æ›²çº¿æ€§èƒ½é¢„æµ‹å™¨ï¼Œè€Œ Liu et al. (Liu etÂ al., [2019a](#bib.bib114)) åˆ©ç”¨è¾ƒå°çš„ç§ç¾¤è§„æ¨¡å’Œæ•°æ®é›†æ¥å‡å°‘è¯„ä¼°çš„æ—¶é—´å¼€é”€ã€‚
- en: 4.2.5\. Summary
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. æ‘˜è¦
- en: Most NAS methods are based on basic EC search paradigms on an entire-structured
    search space, which are introduced above. However, there are also some other automatic
    search techniques such as RL-based (JaÃ¢fra etÂ al., [2019](#bib.bib86)), Bayesian-based
    (White etÂ al., [2021](#bib.bib215)), and gradient-based (Liu etÂ al., [2018b](#bib.bib113))
    methods, for architecture search.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•° NAS æ–¹æ³•åŸºäºæ•´ä¸ªç»“æ„åŒ–æœç´¢ç©ºé—´ä¸Šçš„åŸºæœ¬ EC æœç´¢èŒƒå¼ï¼Œå¦‚ä¸Šæ‰€è¿°ã€‚ç„¶è€Œï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–è‡ªåŠ¨æœç´¢æŠ€æœ¯ï¼Œä¾‹å¦‚åŸºäº RL çš„ (JaÃ¢fra etÂ al.,
    [2019](#bib.bib86))ã€åŸºäºè´å¶æ–¯çš„ (White etÂ al., [2021](#bib.bib215)) å’ŒåŸºäºæ¢¯åº¦çš„ (Liu etÂ al.,
    [2018b](#bib.bib113)) æ–¹æ³•ï¼Œç”¨äºæ¶æ„æœç´¢ã€‚
- en: RL-based methods can be regarded as an incremental search, where a policy function
    is learned by using a reward-prediction error to drive the generation of incremental
    architecture. Due to the large-scale of state space and action space, RL-based
    methods require immense computational resources. In addition, there are a large
    number of hyper-parameters (e.g., discount factor) in RL-based NAS. Besides, they
    transform a multi-objective optimization problem into a single-objective problem
    via a priori or expert knowledge, so they are unable to find a Pareto optimal
    set to the target tasks.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäº RL çš„æ–¹æ³•å¯ä»¥è¢«è§†ä¸ºä¸€ç§å¢é‡æœç´¢ï¼Œå…¶ä¸­é€šè¿‡ä½¿ç”¨å¥–åŠ±é¢„æµ‹è¯¯å·®æ¥é©±åŠ¨å¢é‡æ¶æ„çš„ç”Ÿæˆï¼Œä»è€Œå­¦ä¹ ä¸€ä¸ªç­–ç•¥å‡½æ•°ã€‚ç”±äºçŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„è§„æ¨¡åºå¤§ï¼ŒåŸºäº RL
    çš„æ–¹æ³•éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚æ­¤å¤–ï¼ŒåŸºäº RL çš„ NAS ä¸­æœ‰å¤§é‡çš„è¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼ŒæŠ˜æ‰£å› å­ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šè¿‡å…ˆéªŒæˆ–ä¸“å®¶çŸ¥è¯†å°†å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºå•ç›®æ ‡é—®é¢˜ï¼Œå› æ­¤æ— æ³•ä¸ºç›®æ ‡ä»»åŠ¡æ‰¾åˆ°ä¸€ä¸ª
    Pareto æœ€ä¼˜é›†ã€‚
- en: Bayesian-based methods are a common tool for hyperparametric optimization problems
    with low dimensions. In comparison to EC-based methods, they are much more efficient
    on the condition that a proper distance function has to be designed to evaluate
    the similarities between two subnets. However, the computational cost of Gaussian
    process grows exponentially and its accuracy decreases, when the dimensionality
    of the problem increases.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè´å¶æ–¯çš„æ–¹æ³•æ˜¯ç”¨äºä½ç»´è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜çš„å¸¸ç”¨å·¥å…·ã€‚ä¸åŸºäº EC çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨è®¾è®¡åˆé€‚çš„è·ç¦»å‡½æ•°ä»¥è¯„ä¼°ä¸¤ä¸ªå­ç½‘ä¹‹é—´çš„ç›¸ä¼¼æ€§æ—¶ï¼Œå®ƒä»¬çš„æ•ˆç‡æ›´é«˜ã€‚ç„¶è€Œï¼Œå½“é—®é¢˜çš„ç»´åº¦å¢åŠ æ—¶ï¼Œé«˜æ–¯è¿‡ç¨‹çš„è®¡ç®—æˆæœ¬å‘ˆæŒ‡æ•°å¢é•¿ï¼Œå…¶å‡†ç¡®æ€§ä¹Ÿä¼šä¸‹é™ã€‚
- en: Gradient-based methods, taking a NAS problem as a continuous differentiable
    problem instead of a discrete one, are able to efficiently search architectures
    with proper weight parameters. Unfortunately, their GPU costs are usually very
    high due to a large number of parameters to be updated in gradient-based algorithms
    (Liu etÂ al., [2018b](#bib.bib113)).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¢¯åº¦çš„æ–¹æ³•å°† NAS é—®é¢˜è§†ä¸ºè¿ç»­å¯å¾®çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯ç¦»æ•£é—®é¢˜ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æœç´¢å…·æœ‰é€‚å½“æƒé‡å‚æ•°çš„æ¶æ„ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç”±äºæ¢¯åº¦æ–¹æ³•ä¸­éœ€è¦æ›´æ–°çš„å‚æ•°æ•°é‡åºå¤§ï¼Œè¿™äº›æ–¹æ³•çš„
    GPU æˆæœ¬é€šå¸¸éå¸¸é«˜ (Liu et al., [2018b](#bib.bib113))ã€‚
- en: In contrast, EC-based methods benefit from less hyperparameters to be optimized
    and no distance functions to be designed. In addition, EC-based methods can be
    applied to NAS with multiple objectives and constraints. Although there many acceleration
    strategies in EC-based methods, they still suffer from high computational overheads.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäº EC çš„æ–¹æ³•å—ç›Šäºéœ€è¦ä¼˜åŒ–çš„è¶…å‚æ•°è¾ƒå°‘ä¸”æ— éœ€è®¾è®¡è·ç¦»å‡½æ•°ã€‚æ­¤å¤–ï¼ŒåŸºäº EC çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºå…·æœ‰å¤šä¸ªç›®æ ‡å’Œçº¦æŸçš„ NASã€‚å°½ç®¡åŸºäº EC
    çš„æ–¹æ³•æœ‰è®¸å¤šåŠ é€Ÿç­–ç•¥ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´é«˜è®¡ç®—å¼€é”€çš„é—®é¢˜ã€‚
- en: 4.3\. Joint Optimization
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. è”åˆä¼˜åŒ–
- en: 4.3.1\. Problem Formulation
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. é—®é¢˜è¡¨è¿°
- en: 'The independent optimization of architecture or parameters is difficult to
    achieve the optimal model on give tasks. Hence, joint optimization methods have
    been developed to search for the optimal configuration of architecture ($A^{*}$),
    and parameters ($W^{*}$, associated weights). The optimization problem can be
    defined in Eq. [4](#S4.E4 "In 4.3.1\. Problem Formulation â€£ 4.3\. Joint Optimization
    â€£ 4\. Model Generation â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: æ¶æ„æˆ–å‚æ•°çš„ç‹¬ç«‹ä¼˜åŒ–éš¾ä»¥åœ¨ç»™å®šä»»åŠ¡ä¸Šå®ç°**æœ€ä½³æ¨¡å‹**ã€‚å› æ­¤ï¼Œå·²ç»å¼€å‘äº†è”åˆä¼˜åŒ–æ–¹æ³•æ¥æœç´¢**æ¶æ„**($A^{*}$)å’Œ**å‚æ•°**($W^{*}$ï¼Œç›¸å…³æƒé‡)çš„æœ€ä½³é…ç½®ã€‚ä¼˜åŒ–é—®é¢˜å¯ä»¥åœ¨
    Eq. [4](#S4.E4 "åœ¨ 4.3.1\. é—®é¢˜è¡¨è¿° â€£ 4.3\. è”åˆä¼˜åŒ– â€£ 4\. æ¨¡å‹ç”Ÿæˆ â€£ è¿›åŒ–æ·±åº¦å­¦ä¹ çš„è°ƒæŸ¥ï¼šåŸåˆ™ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜")ä¸­å®šä¹‰ã€‚
- en: '| (4) |  | $\displaystyle\begin{matrix}\left({{W}^{*}},{{A}^{*}}\right)$=$\underset{W,A}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\begin{matrix}\left({{W}^{*}},{{A}^{*}}\right)$=$\underset{W,A}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
- en: where $L$ is the loss function.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $L$ æ˜¯æŸå¤±å‡½æ•°ã€‚
- en: In the followings, we will introduce the joint optimization regarding the solution
    representations and search paradigms, and then discuss the pros and cons of EC-based
    methods in comparison to others.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»‹ç»æœ‰å…³è§£å†³æ–¹æ¡ˆè¡¨ç¤ºå’Œæœç´¢èŒƒå¼çš„è”åˆä¼˜åŒ–ï¼Œç„¶åè®¨è®ºä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäº EC çš„æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚
- en: 4.3.2\. Solution Representations
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. è§£å†³æ–¹æ¡ˆè¡¨ç¤º
- en: There are three typical classes of encoding schemes used for joint optimization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸‰ç§å…¸å‹çš„ç¼–ç æ–¹æ¡ˆç”¨äºè”åˆä¼˜åŒ–ã€‚
- en: 'Linear encoding: This is a simple but effective encoding strategy, which has
    been widely used in many studies to build architecture with high performance (Aljarah
    etÂ al., [2018](#bib.bib9); Maniezzo, [1994](#bib.bib132)). In (Maniezzo, [1994](#bib.bib132)),
    a variable-length binary vector was used to represent weights and structure of
    neural networks, where the weights utilize direct encoding.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§ç¼–ç ï¼šè¿™æ˜¯ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„ç¼–ç ç­–ç•¥ï¼Œåœ¨è®¸å¤šç ”ç©¶ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä»¥æ„å»ºé«˜æ€§èƒ½çš„æ¶æ„ï¼ˆAljarah ç­‰ï¼Œ[2018](#bib.bib9)ï¼›Maniezzoï¼Œ[1994](#bib.bib132)ï¼‰ã€‚åœ¨
    (Maniezzoï¼Œ[1994](#bib.bib132)) ä¸­ï¼Œä½¿ç”¨äº†ä¸€ä¸ªå˜é•¿çš„äºŒè¿›åˆ¶å‘é‡æ¥è¡¨ç¤ºç¥ç»ç½‘ç»œçš„æƒé‡å’Œç»“æ„ï¼Œå…¶ä¸­æƒé‡åˆ©ç”¨ç›´æ¥ç¼–ç ã€‚
- en: 'Tree-based encoding: In this encoding, the topology and weights of an architecture
    can be represented by a tree structure with a number of nodes and edges (Zhang
    and MÃ¼hlenbein, [1995](#bib.bib240); Golubski and Feuring, [1999](#bib.bib65)).
    In (Zhang etÂ al., [2020b](#bib.bib242)), the mechanism of Reverse Encoding Tree
    (RET) was developed to ensure the robustness of a deep model, where the topological
    information of an architecture was represented by a combination of nodes and the
    weight information was recorded on the edges.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ ‘çš„ç¼–ç ï¼šåœ¨è¿™ç§ç¼–ç ä¸­ï¼Œæ¶æ„çš„æ‹“æ‰‘å’Œæƒé‡å¯ä»¥é€šè¿‡ä¸€ä¸ªå…·æœ‰å¤šä¸ªèŠ‚ç‚¹å’Œè¾¹çš„æ ‘ç»“æ„æ¥è¡¨ç¤ºï¼ˆZhang å’Œ MÃ¼hlenbeinï¼Œ[1995](#bib.bib240)ï¼›Golubski
    å’Œ Feuringï¼Œ[1999](#bib.bib65)ï¼‰ã€‚åœ¨ (Zhang ç­‰ï¼Œ[2020b](#bib.bib242)) ä¸­ï¼Œå¼€å‘äº†åå‘ç¼–ç æ ‘ï¼ˆRETï¼‰æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ·±åº¦æ¨¡å‹çš„é²æ£’æ€§ï¼Œå…¶ä¸­æ¶æ„çš„æ‹“æ‰‘ä¿¡æ¯ç”±èŠ‚ç‚¹ç»„åˆè¡¨ç¤ºï¼Œæƒé‡ä¿¡æ¯è®°å½•åœ¨è¾¹ç¼˜ä¸Šã€‚
- en: 'Graph-based encoding: In this encoding, the nodes of a graph represent neurons
    or other network units, and the edges are used to record the weight information
    (Han and Cho, [2006](#bib.bib70); Chai etÂ al., [2022](#bib.bib21)). For example,
    a graph incidence matrix was developed in (Oong and Isa, [2011](#bib.bib151))
    to encode a neural network. The size of the matrix was set to ($N_{i}$ + $N_{h}$
    + $N_{o}$) $\times$ ($N_{i}$ + $N_{h}$ + $N_{o}$), where $N_{i}$, $N_{h}$ and
    $N_{o}$ indicate the numbers of input, hidden, and output nodes, respectively.
    In the graph incidence matrix, real numbers represented the weight and biases,
    and â€œ0â€ meant that there was no connection between two nodes.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå›¾çš„ç¼–ç ï¼šåœ¨è¿™ç§ç¼–ç ä¸­ï¼Œå›¾çš„èŠ‚ç‚¹ä»£è¡¨ç¥ç»å…ƒæˆ–å…¶ä»–ç½‘ç»œå•å…ƒï¼Œè¾¹ç¼˜ç”¨äºè®°å½•æƒé‡ä¿¡æ¯ï¼ˆHan å’Œ Choï¼Œ[2006](#bib.bib70)ï¼›Chai
    ç­‰ï¼Œ[2022](#bib.bib21)ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ (Oong å’Œ Isaï¼Œ[2011](#bib.bib151)) ä¸­ï¼Œå¼€å‘äº†ä¸€ä¸ªå›¾çš„å‘ç”ŸçŸ©é˜µæ¥ç¼–ç ç¥ç»ç½‘ç»œã€‚çŸ©é˜µçš„å¤§å°è®¾ç½®ä¸º
    ($N_{i}$ + $N_{h}$ + $N_{o}$) $\times$ ($N_{i}$ + $N_{h}$ + $N_{o}$)ï¼Œå…¶ä¸­ $N_{i}$ã€$N_{h}$
    å’Œ $N_{o}$ åˆ†åˆ«è¡¨ç¤ºè¾“å…¥ã€éšè—å’Œè¾“å‡ºèŠ‚ç‚¹çš„æ•°é‡ã€‚åœ¨å›¾çš„å‘ç”ŸçŸ©é˜µä¸­ï¼Œå®æ•°è¡¨ç¤ºæƒé‡å’Œåç½®ï¼Œâ€œ0â€è¡¨ç¤ºä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´æ²¡æœ‰è¿æ¥ã€‚
- en: 4.3.3\. Search Paradigms
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. æœç´¢èŒƒå¼
- en: There are a number of effective search paradigms for joint optimization, and
    the EC-based search paradigms are in the spotlight.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è®¸å¤šæœ‰æ•ˆçš„æœç´¢èŒƒå¼ç”¨äºè”åˆä¼˜åŒ–ï¼Œå…¶ä¸­åŸºäº EC çš„æœç´¢èŒƒå¼å¤‡å—å…³æ³¨ã€‚
- en: 'Basic EC search paradigm: Some basic EC search methods have been employed to
    handle joint optimization problems (Fahrudin etÂ al., [2016](#bib.bib55); Oong
    and Isa, [2011](#bib.bib151); Stanley and Miikkulainen, [2002](#bib.bib186); Barman
    and Kwon, [2020](#bib.bib15); Behjat and Chidambaran, [2019](#bib.bib16)). In
    (Oong and Isa, [2011](#bib.bib151)), an architecture and its corresponding weights
    were simultaneously optimized by an EC-based method using linear and graph encodings.
    In neuro-evolution of augmenting topologies (NEAT) (Stanley and Miikkulainen,
    [2002](#bib.bib186)), the architecture of a small network is evolved by an incremental
    mechanism, while the weights are optimized by an EC-based method. NEAT is able
    to ensure the lowest dimensional search space over all generations. Some representative
    studies on NEAT are presented in (Barman and Kwon, [2020](#bib.bib15); Behjat
    and Chidambaran, [2019](#bib.bib16)).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬çš„ EC æœç´¢èŒƒå¼ï¼šä¸€äº›åŸºæœ¬çš„ EC æœç´¢æ–¹æ³•å·²ç»è¢«ç”¨äºå¤„ç†è”åˆä¼˜åŒ–é—®é¢˜ï¼ˆFahrudin ç­‰ï¼Œ[2016](#bib.bib55)ï¼›Oong å’Œ
    Isaï¼Œ[2011](#bib.bib151)ï¼›Stanley å’Œ Miikkulainenï¼Œ[2002](#bib.bib186)ï¼›Barman å’Œ Kwonï¼Œ[2020](#bib.bib15)ï¼›Behjat
    å’Œ Chidambaranï¼Œ[2019](#bib.bib16)ï¼‰ã€‚åœ¨ (Oong å’Œ Isaï¼Œ[2011](#bib.bib151)) ä¸­ï¼Œé€šè¿‡ä½¿ç”¨çº¿æ€§å’Œå›¾ç¼–ç çš„åŸºäº
    EC çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ä¸€ä¸ªæ¶æ„åŠå…¶å¯¹åº”çš„æƒé‡ã€‚åœ¨å¢å¼ºæ‹“æ‰‘çš„ç¥ç»è¿›åŒ–ï¼ˆNEATï¼‰ï¼ˆStanley å’Œ Miikkulainenï¼Œ[2002](#bib.bib186)ï¼‰ä¸­ï¼Œå°å‹ç½‘ç»œçš„æ¶æ„é€šè¿‡å¢é‡æœºåˆ¶è¿›è¡Œè¿›åŒ–ï¼Œè€Œæƒé‡ç”±åŸºäº
    EC çš„æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚NEAT èƒ½å¤Ÿåœ¨æ‰€æœ‰ä»£ä¸­ç¡®ä¿æœ€ä½ç»´åº¦çš„æœç´¢ç©ºé—´ã€‚åœ¨ (Barman å’Œ Kwonï¼Œ[2020](#bib.bib15)ï¼›Behjat
    å’Œ Chidambaranï¼Œ[2019](#bib.bib16)) ä¸­ä»‹ç»äº†ä¸€äº› NEAT çš„ä»£è¡¨æ€§ç ”ç©¶ã€‚
- en: 'Multi-objective search paradigm: Multi-objective optimization on model design
    has been developed in many studies (e.g., artificial neural network (Rostami and
    Neri, [2016](#bib.bib167)) and recurrent neural network (Smith and Jin, [2014](#bib.bib182))).
    For example, Smith et al. (Smith and Jin, [2014](#bib.bib182)) built a bi-objective
    optimization (i.e., the minimizations of the mean squared error (MSE) on a training
    dataset and the number of connections in the network) to search for optimal weights
    and connections of network architectures. The chromosome of an individual was
    composed of two parts, where the one with Boolean type represented the structure
    of a network, and the other with real values represented the weights.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šç›®æ ‡æœç´¢èŒƒå¼ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šçš„å¤šç›®æ ‡ä¼˜åŒ–å·²ç»åœ¨è®¸å¤šç ”ç©¶ä¸­å¾—åˆ°å‘å±•ï¼ˆä¾‹å¦‚ï¼Œäººå·¥ç¥ç»ç½‘ç»œï¼ˆRostami å’Œ Neri, [2016](#bib.bib167)ï¼‰å’Œé€’å½’ç¥ç»ç½‘ç»œï¼ˆSmith
    å’Œ Jin, [2014](#bib.bib182)ï¼‰ï¼‰ã€‚ä¾‹å¦‚ï¼ŒSmith ç­‰äººï¼ˆSmith å’Œ Jin, [2014](#bib.bib182)ï¼‰å»ºç«‹äº†ä¸€ä¸ªåŒç›®æ ‡ä¼˜åŒ–ï¼ˆå³ï¼Œæœ€å°åŒ–è®­ç»ƒæ•°æ®é›†ä¸Šçš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œç½‘ç»œä¸­çš„è¿æ¥æ•°ï¼‰æ¥æœç´¢ç½‘ç»œæ¶æ„çš„æœ€ä¼˜æƒé‡å’Œè¿æ¥ã€‚ä¸ªä½“çš„æŸ“è‰²ä½“ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼Œå…¶ä¸­
    Boolean ç±»å‹çš„éƒ¨åˆ†ä»£è¡¨ç½‘ç»œçš„ç»“æ„ï¼Œå¦ä¸€ä¸ªå…·æœ‰å®æ•°å€¼çš„éƒ¨åˆ†ä»£è¡¨æƒé‡ã€‚
- en: 4.3.4\. Summary
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4\. æ€»ç»“
- en: Direct encoding is used to be prevalent in the joint optimization of small-scale
    neural networks (Oong and Isa, [2011](#bib.bib151); Yao and Liu, [1996](#bib.bib233)).
    However, with the increase of the scale of neural networks, direct coding of high-dimensional
    vector or matrix of weights is not realistic. Therefore, recent studies are more
    on indirect encoding. For example, a complex mapping with acceptable accuracy
    loss is designed in (KoutnÃ­k etÂ al., [2014](#bib.bib100); Dâ€™Ambrosio and Stanley,
    [2007](#bib.bib40)) to construct weight vectors with arbitrary size.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥ç¼–ç åœ¨å°è§„æ¨¡ç¥ç»ç½‘ç»œçš„è”åˆä¼˜åŒ–ä¸­æ›¾ç»å¾ˆæµè¡Œï¼ˆOong å’Œ Isa, [2011](#bib.bib151)ï¼›Yao å’Œ Liu, [1996](#bib.bib233)ï¼‰ã€‚ç„¶è€Œï¼Œéšç€ç¥ç»ç½‘ç»œè§„æ¨¡çš„å¢åŠ ï¼Œç›´æ¥ç¼–ç é«˜ç»´å‘é‡æˆ–çŸ©é˜µçš„æƒé‡å·²ä¸å†ç°å®ã€‚å› æ­¤ï¼Œè¿‘æœŸçš„ç ”ç©¶æ›´å¤šé›†ä¸­åœ¨é—´æ¥ç¼–ç ä¸Šã€‚ä¾‹å¦‚ï¼Œåœ¨ï¼ˆKoutnÃ­k
    ç­‰, [2014](#bib.bib100)ï¼›Dâ€™Ambrosio å’Œ Stanley, [2007](#bib.bib40)ï¼‰ä¸­è®¾è®¡äº†ä¸€ä¸ªå¤æ‚çš„æ˜ å°„ï¼Œå…·æœ‰å¯æ¥å—çš„å‡†ç¡®åº¦æŸå¤±ï¼Œä»¥æ„é€ ä»»æ„å¤§å°çš„æƒé‡å‘é‡ã€‚
- en: EC-based approaches with capability of searching the optimal solution have been
    developed to configure a DL model for the specific task. However, they often encounter
    a prohibitive computational cost, which is even higher than that of model architecture
    optimization. Hence, designing efficient EC-based approaches for architecture
    and parameter search deserves much investigation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰æœç´¢æœ€ä¼˜è§£èƒ½åŠ›çš„åŸºäºè¿›åŒ–è®¡ç®—çš„æ–¹æ³•å·²è¢«å¼€å‘å‡ºæ¥ï¼Œä»¥é…ç½®ç”¨äºç‰¹å®šä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸ä¼šé‡åˆ°éš¾ä»¥æ‰¿å—çš„è®¡ç®—æˆæœ¬ï¼Œè¿™ç”šè‡³é«˜äºæ¨¡å‹æ¶æ„ä¼˜åŒ–çš„æˆæœ¬ã€‚å› æ­¤ï¼Œè®¾è®¡é«˜æ•ˆçš„åŸºäºè¿›åŒ–è®¡ç®—çš„æ¶æ„å’Œå‚æ•°æœç´¢æ–¹æ³•å€¼å¾—æ·±å…¥ç ”ç©¶ã€‚
- en: 5\. Model Deployment
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. æ¨¡å‹éƒ¨ç½²
- en: The large-scale DNNs are not straightforward to be deployed into devices (e.g.,
    smartphones) with limited computation and storage resources (e.g., battery capacity
    and memory size). To solve this issue, various model compression approaches have
    been proposed to reduce the model size and inference time, such as pruning, model
    distillation, and quantization (Choudhary etÂ al., [2020](#bib.bib32)). However,
    they need much expert knowledge and a lot of efforts on the manual compression
    of neural network models. In contrast, EC-based approaches are automation approaches
    and has been recently introduced to achieve automated model compression. We have
    observed that most of them concentrate on the area of model pruning.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è§„æ¨¡æ·±åº¦ç¥ç»ç½‘ç»œåœ¨éƒ¨ç½²åˆ°è®¡ç®—å’Œå­˜å‚¨èµ„æºæœ‰é™çš„è®¾å¤‡ï¼ˆä¾‹å¦‚ï¼Œæ™ºèƒ½æ‰‹æœºï¼‰ä¸Šå¹¶ä¸ç®€å•ï¼ˆä¾‹å¦‚ï¼Œç”µæ± å®¹é‡å’Œå†…å­˜å¤§å°ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»æå‡ºäº†å„ç§æ¨¡å‹å‹ç¼©æ–¹æ³•æ¥å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶é—´ï¼Œå¦‚å‰ªæã€æ¨¡å‹è’¸é¦å’Œé‡åŒ–ï¼ˆChoudhary
    ç­‰, [2020](#bib.bib32)ï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡çš„ä¸“å®¶çŸ¥è¯†å’Œå¯¹ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ‰‹åŠ¨å‹ç¼©ä»˜å‡ºå¾ˆå¤šåŠªåŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºè¿›åŒ–è®¡ç®—çš„æ–¹æ³•æ˜¯è‡ªåŠ¨åŒ–çš„æ–¹æ³•ï¼Œå¹¶ä¸”æœ€è¿‘è¢«å¼•å…¥ä»¥å®ç°è‡ªåŠ¨åŒ–æ¨¡å‹å‹ç¼©ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå®ƒä»¬ä¸­çš„å¤§å¤šæ•°é›†ä¸­åœ¨æ¨¡å‹å‰ªæé¢†åŸŸã€‚
- en: 5.1\. Model Pruning
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. æ¨¡å‹å‰ªæ
- en: 5.1.1\. Problem Formulation
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. é—®é¢˜æè¿°
- en: DNN is commonly an over-parameterized model, which has redundant and non-informative
    components (e.g., weights, channels and filters). To address this issue, researchers
    have designed various pruning approaches (e.g., channel pruning (He etÂ al., [2017](#bib.bib73)))
    to obtain a lightweight deep network model with high accuracy. Model pruning can
    be formulated as
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ç¥ç»ç½‘ç»œé€šå¸¸æ˜¯ä¸€ä¸ªè¿‡åº¦å‚æ•°åŒ–çš„æ¨¡å‹ï¼Œå…·æœ‰å†—ä½™å’Œéä¿¡æ¯æ€§çš„ç»„ä»¶ï¼ˆä¾‹å¦‚ï¼Œæƒé‡ã€é€šé“å’Œæ»¤æ³¢å™¨ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜è®¾è®¡äº†å„ç§å‰ªææ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œé€šé“å‰ªæï¼ˆHe
    ç­‰, [2017](#bib.bib73)ï¼‰ï¼‰æ¥è·å¾—ä¸€ä¸ªå…·æœ‰é«˜å‡†ç¡®åº¦çš„è½»é‡çº§æ·±åº¦ç½‘ç»œæ¨¡å‹ã€‚æ¨¡å‹å‰ªæå¯ä»¥è¢«è¡¨è¿°ä¸º
- en: '| (5) |  |  | $\displaystyle Los{{s}_{A_{s}^{*}}}\approx Los{{s}_{A}}$ |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  |  | $\displaystyle Los{{s}_{A_{s}^{*}}}\approx Los{{s}_{A}}$ |  |'
- en: '|  |  | $\displaystyle\begin{matrix}\text{s}\text{.t}\text{.}&amp;{A}_{{s}}^{*}$=$\underset{{C}}{\mathop{\text{pruning}}}\,\left({A,C}\right)\\
    \end{matrix}$ |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\begin{matrix}\text{s}\text{.t}\text{.}&amp;{A}_{{s}}^{*}$=$\underset{{C}}{\mathop{\text{pruning}}}\,\left({A,C}\right)\\
    \end{matrix}$ |  |'
- en: where $C$ represents redundant and non-informative units, $A$ and $A{{}_{s}}^{*}$
    represent original model and lightweight model, respectively, $Los{{s}_{A_{s}^{*}}}$
    and $Los{{s}_{A}}$ represent loss of $A{{}_{s}}^{*}$ and $A$.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $C$ è¡¨ç¤ºå†—ä½™å’Œæ— ä¿¡æ¯çš„å•å…ƒï¼Œ$A$ å’Œ $A{{}_{s}}^{*}$ åˆ†åˆ«è¡¨ç¤ºåŸå§‹æ¨¡å‹å’Œè½»é‡çº§æ¨¡å‹ï¼Œ$Los{{s}_{A_{s}^{*}}}$
    å’Œ $Los{{s}_{A}}$ åˆ†åˆ«è¡¨ç¤º $A{{}_{s}}^{*}$ å’Œ $A$ çš„æŸå¤±ã€‚
- en: This study aims to introduce EC-based methods for model pruning, and readers
    interested in traditional pruning methods such as weight-based pruning, neuron-based
    pruning, filter-based pruning, layer-based pruning, and channel-based pruning
    may refer to the surveys (Choudhary etÂ al., [2020](#bib.bib32)) to get more details.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç ”ç©¶æ—¨åœ¨å¼•å…¥åŸºäº EC çš„æ¨¡å‹å‰ªææ–¹æ³•ï¼Œæ„Ÿå…´è¶£äºä¼ ç»Ÿå‰ªææ–¹æ³•å¦‚åŸºäºæƒé‡çš„å‰ªæã€åŸºäºç¥ç»å…ƒçš„å‰ªæã€åŸºäºæ»¤æ³¢å™¨çš„å‰ªæã€åŸºäºå±‚çš„å‰ªæå’ŒåŸºäºé€šé“çš„å‰ªæçš„è¯»è€…å¯ä»¥å‚è€ƒè°ƒæŸ¥ï¼ˆChoudhary
    et al., [2020](#bib.bib32)ï¼‰ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚
- en: 5.1.2\. Solution Representations
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. è§£å†³æ–¹æ¡ˆè¡¨ç¤º
- en: For model pruning, binary encoding is one of the most popular approaches among
    these solution representations, where each element corresponds to the network
    component (e.g., channel). In (Wang etÂ al., [2018](#bib.bib212)), the network
    pruning task was formulated as a binary programming problem, where a binary variable
    was directly associated with each convolution filter to determine whether or not
    the filter took effect. Although binary representation is straightforward, the
    length of the representation becomes large when the model complexity (i.e., the
    number of units) improves, and the overhead of exploration will also increase.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¨¡å‹å‰ªæï¼ŒäºŒè¿›åˆ¶ç¼–ç æ˜¯è¿™äº›è§£å†³æ–¹æ¡ˆè¡¨ç¤ºæ–¹æ³•ä¸­æœ€æµè¡Œçš„æ–¹å¼ä¹‹ä¸€ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”äºç½‘ç»œç»„ä»¶ï¼ˆä¾‹å¦‚ï¼Œé€šé“ï¼‰ã€‚åœ¨ï¼ˆWang et al., [2018](#bib.bib212)ï¼‰ä¸­ï¼Œç½‘ç»œå‰ªæä»»åŠ¡è¢«è¡¨è¿°ä¸ºä¸€ä¸ªäºŒè¿›åˆ¶ç¼–ç¨‹é—®é¢˜ï¼Œå…¶ä¸­ä¸€ä¸ªäºŒè¿›åˆ¶å˜é‡ç›´æ¥ä¸æ¯ä¸ªå·ç§¯æ»¤æ³¢å™¨ç›¸å…³è”ï¼Œä»¥ç¡®å®šè¯¥æ»¤æ³¢å™¨æ˜¯å¦æœ‰æ•ˆã€‚å°½ç®¡äºŒè¿›åˆ¶è¡¨ç¤ºæ–¹å¼ç®€å•æ˜äº†ï¼Œä½†å½“æ¨¡å‹å¤æ‚åº¦ï¼ˆå³å•ä½æ•°é‡ï¼‰å¢åŠ æ—¶ï¼Œè¡¨ç¤ºçš„é•¿åº¦ä¹Ÿä¼šå˜å¾—å¾ˆå¤§ï¼Œæ¢ç´¢çš„å¼€é”€ä¹Ÿä¼šå¢åŠ ã€‚
- en: To address the above issue, some efficient solution representations (i.e., indirect
    encoding) have been developed. For example, Liu et al. (Liu and Guo, [2021](#bib.bib115))
    used $N$ digits to record the number of compressed layers. The first digit represented
    the number of compressed layers, and following digits recorded the selected compression
    operator index of each layer. This way can significantly improve the search efficiency.
    In (Liu etÂ al., [2019b](#bib.bib118)), encoding vectors are used to represent
    the number of channels in each layer for original networks. Then a meta-network
    is constructed to generate the weights according to network encoding vectors.
    By stochastically fed with different structure encoding, the meta-network gradually
    learns to generate weights for various pruned structures.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åº”å¯¹ä¸Šè¿°é—®é¢˜ï¼Œä¸€äº›é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆè¡¨ç¤ºï¼ˆå³é—´æ¥ç¼–ç ï¼‰å·²ç»è¢«å¼€å‘å‡ºæ¥ã€‚ä¾‹å¦‚ï¼ŒLiu et al.ï¼ˆLiu and Guo, [2021](#bib.bib115)ï¼‰ä½¿ç”¨
    $N$ ä½æ¥è®°å½•å‹ç¼©å±‚çš„æ•°é‡ã€‚ç¬¬ä¸€ä½è¡¨ç¤ºå‹ç¼©å±‚çš„æ•°é‡ï¼Œåç»­ä½è®°å½•æ¯å±‚é€‰æ‹©çš„å‹ç¼©æ“ä½œç¬¦ç´¢å¼•ã€‚è¿™ç§æ–¹å¼å¯ä»¥æ˜¾è‘—æé«˜æœç´¢æ•ˆç‡ã€‚åœ¨ï¼ˆLiu et al., [2019b](#bib.bib118)ï¼‰ä¸­ï¼Œç¼–ç å‘é‡ç”¨äºè¡¨ç¤ºåŸå§‹ç½‘ç»œä¸­æ¯å±‚çš„é€šé“æ•°é‡ã€‚ç„¶åæ„å»ºä¸€ä¸ªå…ƒç½‘ç»œï¼Œæ ¹æ®ç½‘ç»œç¼–ç å‘é‡ç”Ÿæˆæƒé‡ã€‚é€šè¿‡ä»¥ä¸åŒçš„ç»“æ„ç¼–ç éšæœºå–‚å…¥ï¼Œå…ƒç½‘ç»œé€æ¸å­¦ä¹ ç”Ÿæˆå„ç§å‰ªæç»“æ„çš„æƒé‡ã€‚
- en: 5.1.3\. Search Paradigms
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. æœç´¢èŒƒå¼
- en: The search paradigms in model pruning studies can be categorized into two main
    groups.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‰ªæç ”ç©¶ä¸­çš„æœç´¢èŒƒå¼å¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ã€‚
- en: 'Basic EC search paradigm: A number of studies introduce single-objective EC
    search paradigm for model pruning (Wu etÂ al., [2021a](#bib.bib218); Tang etÂ al.,
    [2019](#bib.bib195)). For example, Wu et al. (Wu etÂ al., [2021a](#bib.bib218))
    first analysed the pruning sensitivity on weights via differential evolution (DE),
    and then the model was compressed by iteratively performing the weight pruning
    process according to the weight sensitivity. In addition, this method adopted
    a recovery strategy to increase the pruned model performance during the fine-tuning
    phase.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬çš„ EC æœç´¢èŒƒå¼ï¼šä¸€äº›ç ”ç©¶ä»‹ç»äº†å•ç›®æ ‡ EC æœç´¢èŒƒå¼ç”¨äºæ¨¡å‹å‰ªæï¼ˆWu et al., [2021a](#bib.bib218); Tang et
    al., [2019](#bib.bib195)ï¼‰ã€‚ä¾‹å¦‚ï¼ŒWu et al.ï¼ˆWu et al., [2021a](#bib.bib218)ï¼‰é¦–å…ˆé€šè¿‡å·®åˆ†è¿›åŒ–ï¼ˆDEï¼‰åˆ†æäº†æƒé‡çš„å‰ªææ•æ„Ÿæ€§ï¼Œç„¶åé€šè¿‡æ ¹æ®æƒé‡æ•æ„Ÿæ€§è¿­ä»£æ‰§è¡Œæƒé‡å‰ªæè¿‡ç¨‹æ¥å‹ç¼©æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¾®è°ƒé˜¶æ®µé‡‡ç”¨äº†æ¢å¤ç­–ç•¥ï¼Œä»¥æé«˜å‰ªææ¨¡å‹çš„æ€§èƒ½ã€‚
- en: 'Multi-objective search paradigm: Recently, this sort of search paradigm has
    been adopted to model pruning, which is able to provide users with a set of Pareto
    lightweight models. For example, Zhou et al. (Zhou etÂ al., [2020](#bib.bib259))
    considered two objectives (i.e., minimizing convolutional filters and maximizing
    the model performance) for biomedical image segmentation. During the model pruning,
    a classical multi-objective optimization algorithm (NSGA-II (Deb etÂ al., [2002](#bib.bib42)))
    was used find the optimal set of non-dominated solutions, where the optimization
    was based on a binary string encoding (each bit represents a filter).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šç›®æ ‡æœç´¢èŒƒå¼ï¼šæœ€è¿‘ï¼Œè¿™ç§æœç´¢èŒƒå¼å·²è¢«é‡‡ç”¨åˆ°æ¨¡å‹ä¿®å‰ªä¸­ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›ä¸€ç»„å¸•ç´¯æ‰˜è½»é‡çº§æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå‘¨ç­‰äººï¼ˆå‘¨ç­‰ï¼Œ[2020](#bib.bib259)ï¼‰è€ƒè™‘äº†ä¸¤ä¸ªç›®æ ‡ï¼ˆå³æœ€å°åŒ–å·ç§¯æ»¤æ³¢å™¨å’Œæœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ï¼‰ç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚åœ¨æ¨¡å‹ä¿®å‰ªè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†ä¸€ä¸ªç»å…¸çš„å¤šç›®æ ‡ä¼˜åŒ–ç®—æ³•ï¼ˆNSGA-II
    (Debç­‰ï¼Œ[2002](#bib.bib42)ï¼‰ï¼‰ï¼Œæ‰¾åˆ°äº†ä¸€ç»„éæ”¯é…è§£çš„æœ€ä¼˜é›†ï¼Œå…¶ä¸­ä¼˜åŒ–åŸºäºäºŒè¿›åˆ¶å­—ç¬¦ä¸²ç¼–ç ï¼ˆæ¯ä¸ªæ¯”ç‰¹ä»£è¡¨ä¸€ä¸ªæ»¤æ³¢å™¨ï¼‰ã€‚
- en: 'In Table [3](#S5.T3 "Table 3 â€£ 5.1.3\. Search Paradigms â€£ 5.1\. Model Pruning
    â€£ 5\. Model Deployment â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues"), we have summarized these two categories of search
    paradigms as well as their corresponding ways of encoding.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¡¨[3](#S5.T3 "Table 3 â€£ 5.1.3\. Search Paradigms â€£ 5.1\. Model Pruning â€£ 5\.
    Model Deployment â€£ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues")ä¸­ï¼Œæˆ‘ä»¬æ€»ç»“äº†è¿™ä¸¤ç±»æœç´¢èŒƒå¼ä»¥åŠå®ƒä»¬å¯¹åº”çš„ç¼–ç æ–¹å¼ã€‚'
- en: Table 3. Different search paradigms and solutions representations for model
    pruning
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 3. æ¨¡å‹ä¿®å‰ªçš„ä¸åŒæœç´¢èŒƒå¼å’Œè§£å†³æ–¹æ¡ˆè¡¨ç¤º
- en: '|  | Direct encoding | Indirect encoding \bigstrut |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | ç›´æ¥ç¼–ç  | é—´æ¥ç¼–ç  \bigstrut |'
- en: '| Basic EC search paradigm | (Samala etÂ al., [2018](#bib.bib171)),(Junior and
    Yen, [2021a](#bib.bib91)),(Tang etÂ al., [2019](#bib.bib195)),(Wu etÂ al., [2021a](#bib.bib218)),(Junior
    and Yen, [2021b](#bib.bib92)),(Zhou etÂ al., [2021b](#bib.bib260)),(Gerum etÂ al.,
    [2020](#bib.bib64)),(Zemouri etÂ al., [2019](#bib.bib238)),(Chen etÂ al., [2019a](#bib.bib26)),(Poyatos
    etÂ al., [2022](#bib.bib157)),(Fernandes and Yen, [2021](#bib.bib57)),(Li etÂ al.,
    [2020](#bib.bib107)),(Shang etÂ al., [2022](#bib.bib178)) | (Liu etÂ al., [2019b](#bib.bib118))  \bigstrut
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| åŸºæœ¬ EC æœç´¢èŒƒå¼ | (Samalaç­‰ï¼Œ[2018](#bib.bib171)),(Juniorå’ŒYenï¼Œ[2021a](#bib.bib91)),(å”ç­‰ï¼Œ[2019](#bib.bib195)),(å´ç­‰ï¼Œ[2021a](#bib.bib218)),(Juniorå’ŒYenï¼Œ[2021b](#bib.bib92)),(å‘¨ç­‰ï¼Œ[2021b](#bib.bib260)),(Gerumç­‰ï¼Œ[2020](#bib.bib64)),(Zemouriç­‰ï¼Œ[2019](#bib.bib238)),(é™ˆç­‰ï¼Œ[2019a](#bib.bib26)),(Poyatosç­‰ï¼Œ[2022](#bib.bib157)),(Fernandeså’ŒYenï¼Œ[2021](#bib.bib57)),(æç­‰ï¼Œ[2020](#bib.bib107)),(å°šç­‰ï¼Œ[2022](#bib.bib178))
    | (åˆ˜ç­‰ï¼Œ[2019b](#bib.bib118))  \bigstrut |'
- en: '| Multi-objective search paradigm | (Zhou etÂ al., [2020](#bib.bib259)),(Zhou
    etÂ al., [2021c](#bib.bib261)),(Wu etÂ al., [2019](#bib.bib219)),(Hong etÂ al., [2020](#bib.bib76)),(Xu
    etÂ al., [2021](#bib.bib224)),(Yang etÂ al., [2019](#bib.bib228)),(Zhang etÂ al.,
    [2021a](#bib.bib245)) | (Wang etÂ al., [2021a](#bib.bib213)),(Zhang etÂ al., [2021b](#bib.bib254)),(Loni
    etÂ al., [2020](#bib.bib122))  \bigstrut |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| å¤šç›®æ ‡æœç´¢èŒƒå¼ | (å‘¨ç­‰ï¼Œ[2020](#bib.bib259)),(å‘¨ç­‰ï¼Œ[2021c](#bib.bib261)),(å´ç­‰ï¼Œ[2019](#bib.bib219)),(æ´ªç­‰ï¼Œ[2020](#bib.bib76)),(å¾ç­‰ï¼Œ[2021](#bib.bib224)),(æ¨ç­‰ï¼Œ[2019](#bib.bib228)),(å¼ ç­‰ï¼Œ[2021a](#bib.bib245))
    | (ç‹ç­‰ï¼Œ[2021a](#bib.bib213)),(å¼ ç­‰ï¼Œ[2021b](#bib.bib254)),(Loniç­‰ï¼Œ[2020](#bib.bib122))  \bigstrut
    |'
- en: 5.2\. Other EC-based Model Deployment Methods
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. å…¶ä»–åŸºäº EC çš„æ¨¡å‹éƒ¨ç½²æ–¹æ³•
- en: Different from model pruning, there are several other EC-based model compression
    methods for model deployment. In the followings, some typical methods are introduced,
    including knowledge distillation, low-rank factorization, and EC for hybrid techniques.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ¨¡å‹ä¿®å‰ªä¸åŒï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–åŸºäº EC çš„æ¨¡å‹å‹ç¼©æ–¹æ³•ç”¨äºæ¨¡å‹éƒ¨ç½²ã€‚æ¥ä¸‹æ¥ï¼Œä»‹ç»äº†ä¸€äº›å…¸å‹çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬çŸ¥è¯†è’¸é¦ã€ä½ç§©å› å­åˆ†è§£å’Œæ··åˆæŠ€æœ¯çš„ ECã€‚
- en: 5.2.1\. Knowledge Distillation
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. çŸ¥è¯†è’¸é¦
- en: Knowledge distillation (KD) (Gou etÂ al., [2021](#bib.bib66)) aims to get a small
    light network but with good generalization capability. The basic idea is to transfer
    the knowledge learned from a big cumbersome network (or teacher network) with
    good generalization ability to a small but light network (or student network).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ï¼ˆé«˜ç­‰ï¼Œ[2021](#bib.bib66)ï¼‰æ—¨åœ¨è·å¾—ä¸€ä¸ªå°å‹è½»é‡çº§ç½‘ç»œï¼Œä½†å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯å°†ä»ä¸€ä¸ªå¤§å‹ç¹çç½‘ç»œï¼ˆæˆ–æ•™å¸ˆç½‘ç»œï¼‰å­¦åˆ°çš„çŸ¥è¯†ï¼Œè½¬ç§»åˆ°ä¸€ä¸ªå°è€Œè½»çš„ç½‘ç»œï¼ˆæˆ–å­¦ç”Ÿç½‘ç»œï¼‰ã€‚
- en: However, knowledge distillation may be seriously influenced when there is a
    big gap in the learning capability between the teacher and student networks. In
    other words, if the difference is large, the student network may not be able to
    learn knowledge from the teacher network. Recently, several EC-based approaches
    have been proposed to mitigate the above issue of knowledge distillation. For
    example, Wu et al. (Wu etÂ al., [2020](#bib.bib220)) proposed an evolutionary embedding
    learning (EEL) paradigm to learn a fast accurate student network via massive knowledge
    distillation. Their experimental results show that the EEL is able to narrow the
    performance between the teacher and student networks on given tasks. Zhang et
    al. (Zhang etÂ al., [2022a](#bib.bib246)) developed an evolutionary knowledge distillation
    method to improve the effectiveness of knowledge transfer. In this method, an
    evolutionary teacher was learned online and consistently transfers intermediate
    knowledge to the student network to narrow the gap of the learning capability
    between them.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå½“æ•™å¸ˆç½‘ç»œå’Œå­¦ç”Ÿç½‘ç»œä¹‹é—´çš„å­¦ä¹ èƒ½åŠ›å·®è·å¾ˆå¤§æ—¶ï¼ŒçŸ¥è¯†è’¸é¦å¯èƒ½ä¼šå—åˆ°ä¸¥é‡å½±å“ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœå·®è·å¾ˆå¤§ï¼Œå­¦ç”Ÿç½‘ç»œå¯èƒ½æ— æ³•ä»æ•™å¸ˆç½‘ç»œä¸­å­¦ä¹ çŸ¥è¯†ã€‚æœ€è¿‘ï¼Œæå‡ºäº†å‡ ç§åŸºäºECçš„æ–¹æ³•æ¥ç¼“è§£çŸ¥è¯†è’¸é¦ä¸­çš„ä¸Šè¿°é—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒWu
    et al.ï¼ˆWu et al., [2020](#bib.bib220)ï¼‰æå‡ºäº†ä¸€ç§è¿›åŒ–åµŒå…¥å­¦ä¹ ï¼ˆEELï¼‰èŒƒå¼ï¼Œé€šè¿‡å¤§é‡çŸ¥è¯†è’¸é¦æ¥å­¦ä¹ ä¸€ä¸ªå¿«é€Ÿå‡†ç¡®çš„å­¦ç”Ÿç½‘ç»œã€‚ä»–ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEELèƒ½å¤Ÿç¼©å°æ•™å¸ˆå’Œå­¦ç”Ÿç½‘ç»œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®è·ã€‚Zhang
    et al.ï¼ˆZhang et al., [2022a](#bib.bib246)ï¼‰å¼€å‘äº†ä¸€ç§è¿›åŒ–çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œä»¥æé«˜çŸ¥è¯†è½¬ç§»çš„æœ‰æ•ˆæ€§ã€‚åœ¨æ­¤æ–¹æ³•ä¸­ï¼Œè¿›åŒ–æ•™å¸ˆåœ¨çº¿å­¦ä¹ å¹¶æŒç»­å°†ä¸­é—´çŸ¥è¯†ä¼ é€’ç»™å­¦ç”Ÿç½‘ç»œï¼Œä»¥ç¼©å°ä¸¤è€…ä¹‹é—´çš„å­¦ä¹ èƒ½åŠ›å·®è·ã€‚
- en: 5.2.2\. Low-rank Factorization
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. ä½ç§©åˆ†è§£
- en: DNNs often involve in a huge number of weights, which may impact the inference
    speed and seriously increase the storage overhead of the DNN. The weights can
    be viewed as a matrix $W$ with $m$ $\times$ $n$ dimensions. The low-rank approach
    is commonly applied to the weight matrix ($W$) after the DNN is fully trained.
    For example, singular value decomposition (Fortuna and Frasca, [2021](#bib.bib59))
    is a typical low-rank factorization method, where $W$ is decomposed as follows.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰é€šå¸¸æ¶‰åŠå¤§é‡çš„æƒé‡ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨ç†é€Ÿåº¦ï¼Œå¹¶ä¸¥é‡å¢åŠ DNNçš„å­˜å‚¨å¼€é”€ã€‚æƒé‡å¯ä»¥è§†ä¸ºä¸€ä¸ª$W$çš„çŸ©é˜µï¼Œå…·æœ‰$m$ $\times$
    $n$ç»´åº¦ã€‚ä½ç§©æ–¹æ³•é€šå¸¸åº”ç”¨äºDNNå®Œå…¨è®­ç»ƒåçš„æƒé‡çŸ©é˜µï¼ˆ$W$ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆFortuna å’Œ Frascaï¼Œ[2021](#bib.bib59)ï¼‰æ˜¯ä¸€ç§å…¸å‹çš„ä½ç§©åˆ†è§£æ–¹æ³•ï¼Œå…¶ä¸­$W$è¢«åˆ†è§£å¦‚ä¸‹ã€‚
- en: '| (6) |  | $\displaystyle\begin{matrix}W$=$USV^{T}\end{matrix}$ |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle\begin{matrix}W$=$USV^{T}\end{matrix}$ |  |'
- en: where $U$ $\in$ $R^{m\times m}$, $V^{T}$ $\in$ $R^{n\times n}$ are orthogonal
    matrices and $S$ $\in$ $R^{m\times n}$ is a diagonal matrix.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$U$ $\in$ $R^{m\times m}$ï¼Œ$V^{T}$ $\in$ $R^{n\times n}$æ˜¯æ­£äº¤çŸ©é˜µï¼Œ$S$ $\in$ $R^{m\times
    n}$æ˜¯å¯¹è§’çŸ©é˜µã€‚
- en: Notably, most of the existing low-rank factorization methods rely on domain
    expertise and experience for the selection of hyperparameters (e.g., the rank
    and sparsity of weight matrix) to get an appropriate compression results without
    serious performance degradation (Swaminathan etÂ al., [2020](#bib.bib193); Winata
    etÂ al., [2019](#bib.bib216); Hsu etÂ al., [2021](#bib.bib78)).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤§å¤šæ•°ç°æœ‰çš„ä½ç§©åˆ†è§£æ–¹æ³•ä¾èµ–äºé¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œç»éªŒæ¥é€‰æ‹©è¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œæƒé‡çŸ©é˜µçš„ç§©å’Œç¨€ç–æ€§ï¼‰ï¼Œä»¥è·å¾—åˆé€‚çš„å‹ç¼©ç»“æœï¼Œè€Œä¸ä¼šå‡ºç°ä¸¥é‡çš„æ€§èƒ½ä¸‹é™ï¼ˆSwaminathan
    et al., [2020](#bib.bib193)ï¼›Winata et al., [2019](#bib.bib216)ï¼›Hsu et al., [2021](#bib.bib78)ï¼‰ã€‚
- en: Accordingly, EC-based methods have been introduced to solve the above challenge
    (Wang etÂ al., [2018](#bib.bib212); Huang etÂ al., [2020](#bib.bib82)). For example,
    Huang et al. (Huang etÂ al., [2020](#bib.bib82)) presented a multi-objective evolution
    approach to automatically optimize rank and sparsity for weight matrix without
    human intervention, where two objectives were taken into account including the
    minimization of the model classification error rate and maximization of the model
    compression rate. They therefore generated a set of approximately compressed models
    with different compression rates to mitigate the expensive training process.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åº”åœ°ï¼ŒåŸºäºECçš„æ–¹æ³•å·²è¢«å¼•å…¥ä»¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼ˆWang et al., [2018](#bib.bib212)ï¼›Huang et al., [2020](#bib.bib82)ï¼‰ã€‚ä¾‹å¦‚ï¼ŒHuang
    et al.ï¼ˆHuang et al., [2020](#bib.bib82)ï¼‰æå‡ºäº†ä¸€ç§å¤šç›®æ ‡è¿›åŒ–æ–¹æ³•ï¼Œä»¥è‡ªåŠ¨ä¼˜åŒ–æƒé‡çŸ©é˜µçš„ç§©å’Œç¨€ç–æ€§ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼Œå…¶ä¸­è€ƒè™‘äº†ä¸¤ä¸ªç›®æ ‡ï¼ŒåŒ…æ‹¬æœ€å°åŒ–æ¨¡å‹åˆ†ç±»é”™è¯¯ç‡å’Œæœ€å¤§åŒ–æ¨¡å‹å‹ç¼©ç‡ã€‚å› æ­¤ï¼Œä»–ä»¬ç”Ÿæˆäº†ä¸€ç»„å…·æœ‰ä¸åŒå‹ç¼©ç‡çš„è¿‘ä¼¼å‹ç¼©æ¨¡å‹ï¼Œä»¥ç¼“è§£æ˜‚è´µçš„è®­ç»ƒè¿‡ç¨‹ã€‚
- en: 5.2.3\. EC for Jointly Optimization
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. è”åˆä¼˜åŒ–ä¸­çš„EC
- en: Many compression techniques (e.g., quantization) can be easily applied on top
    of other techniques (e.g., pruning and low-rank factorization). For example, pruning
    first and then quantification can obtain a lightweight model with faster inference.
    Similarly, EC can optimize more than one model compression method at the same
    time. In the followings, we will briefly review such works.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šå‹ç¼©æŠ€æœ¯ï¼ˆä¾‹å¦‚ï¼Œé‡åŒ–ï¼‰å¯ä»¥è½»æ¾åœ°åº”ç”¨äºå…¶ä»–æŠ€æœ¯ï¼ˆä¾‹å¦‚ï¼Œå‰ªæå’Œä½ç§©åˆ†è§£ï¼‰ã€‚ä¾‹å¦‚ï¼Œå…ˆå‰ªæå†é‡åŒ–å¯ä»¥è·å¾—ä¸€ä¸ªå…·æœ‰æ›´å¿«æ¨ç†é€Ÿåº¦çš„è½»é‡çº§æ¨¡å‹ã€‚åŒæ ·ï¼ŒECå¯ä»¥åŒæ—¶ä¼˜åŒ–å¤šä¸ªæ¨¡å‹å‹ç¼©æ–¹æ³•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ç®€è¦å›é¡¾è¿™äº›å·¥ä½œã€‚
- en: Phan et al. (Phan etÂ al., [2020](#bib.bib155)) designed an efficient 1-Bit CNNs,
    which combined quantization with a compact model. Specifically, they firstly created
    a number of strong baseline binary networks (BNNs), which had abundant random
    group combinations at each convolutional layer. Then, they adopted evolutionary
    search to seek an optimal group convolution combination with accuracy above threshold.
    Finally, the obtained binary models werr trained from scratch to achieve the final
    lightweight network. Different from (Phan etÂ al., [2020](#bib.bib155)), Polino
    et al. (Polino etÂ al., [2018](#bib.bib156)) jointly utilized weight quantization
    and distillation to compress large networks (i.e., teacher network) into small
    networks (i.e., student network), where the latency and model error were regarded
    as the objectives during the optimization.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Phanç­‰äººï¼ˆPhan et al., [2020](#bib.bib155)ï¼‰è®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„1-Bit CNNsï¼Œå°†é‡åŒ–ä¸ç´§å‡‘æ¨¡å‹ç›¸ç»“åˆã€‚å…·ä½“è€Œè¨€ï¼Œä»–ä»¬é¦–å…ˆåˆ›å»ºäº†ä¸€äº›å¼ºå¤§çš„åŸºçº¿äºŒå€¼ç½‘ç»œï¼ˆBNNsï¼‰ï¼Œè¿™äº›ç½‘ç»œåœ¨æ¯ä¸ªå·ç§¯å±‚æœ‰ä¸°å¯Œçš„éšæœºç»„ç»„åˆã€‚ç„¶åï¼Œä»–ä»¬é‡‡ç”¨è¿›åŒ–æœç´¢æ¥å¯»æ±‚ä¸€ä¸ªå‡†ç¡®åº¦é«˜äºé˜ˆå€¼çš„æœ€ä½³ç»„å·ç§¯ç»„åˆã€‚æœ€åï¼Œè·å¾—çš„äºŒå€¼æ¨¡å‹ä»å¤´å¼€å§‹è®­ç»ƒï¼Œä»¥å®ç°æœ€ç»ˆçš„è½»é‡çº§ç½‘ç»œã€‚ä¸ï¼ˆPhan
    et al., [2020](#bib.bib155)ï¼‰ä¸åŒï¼ŒPolinoç­‰äººï¼ˆPolino et al., [2018](#bib.bib156)ï¼‰è”åˆåˆ©ç”¨æƒé‡é‡åŒ–å’Œè’¸é¦å°†å¤§å‹ç½‘ç»œï¼ˆå³æ•™å¸ˆç½‘ç»œï¼‰å‹ç¼©æˆå°å‹ç½‘ç»œï¼ˆå³å­¦ç”Ÿç½‘ç»œï¼‰ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œå°†å»¶è¿Ÿå’Œæ¨¡å‹è¯¯å·®ä½œä¸ºç›®æ ‡ã€‚
- en: Recently, Zhou et al. (Zhou etÂ al., [2021b](#bib.bib260)) developed an evolutionary
    algorithm-based method for shallowing DNNs at block levels (ESNB). In ESNB, a
    prior knowledge was extracted from the original model to guide the population
    initialization. Then, an evolutionary multi-objective optimization mothed was
    performed to minimize the number of blocks and the accuracy drop (i.e., loss).
    After that, knowledge distillation was employed to compensate for the performance
    degradation via matching output of the pruned model with the softened and hardened
    output of the original model.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒZhouç­‰äººï¼ˆZhou et al., [2021b](#bib.bib260)ï¼‰å¼€å‘äº†ä¸€ç§åŸºäºè¿›åŒ–ç®—æ³•çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å—çº§åˆ«æµ…åŒ–DNNï¼ˆESNBï¼‰ã€‚åœ¨ESNBä¸­ï¼Œä»åŸå§‹æ¨¡å‹ä¸­æå–äº†å…ˆéªŒçŸ¥è¯†ä»¥æŒ‡å¯¼ç§ç¾¤åˆå§‹åŒ–ã€‚ç„¶åï¼Œæ‰§è¡Œäº†ä¸€ç§è¿›åŒ–å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æœ€å°åŒ–å—çš„æ•°é‡å’Œå‡†ç¡®åº¦ä¸‹é™ï¼ˆå³æŸå¤±ï¼‰ã€‚ä¹‹åï¼Œé‡‡ç”¨çŸ¥è¯†è’¸é¦æ¥å¼¥è¡¥é€šè¿‡å°†å‰ªææ¨¡å‹çš„è¾“å‡ºä¸åŸå§‹æ¨¡å‹çš„è½¯åŒ–å’Œç¡¬åŒ–è¾“å‡ºåŒ¹é…æ‰€å¸¦æ¥çš„æ€§èƒ½é€€åŒ–ã€‚
- en: 5.2.4\. Summary
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. æ‘˜è¦
- en: There is still a big room for the improvement on addressing the huge computational
    overhead of evolutionary model deployment. Acceleration strategies may be able
    to alleviate the issue. Besides, there is a high coupling between model deployment
    and model generation since the performance of the compressed network is strongly
    dependent on the performance of the original network. The black-box nature of
    model also hampers deployment in security-critical tasks (e.g., medicine and finance).Consequently,
    it is promising and challenging to take the model compression, NAS, and interpretability
    as a single optimization problem and handle it with acceptable time consumption.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è§£å†³è¿›åŒ–æ¨¡å‹éƒ¨ç½²çš„å·¨å¤§è®¡ç®—å¼€é”€æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚åŠ é€Ÿç­–ç•¥å¯èƒ½æœ‰åŠ©äºç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œç”±äºå‹ç¼©ç½‘ç»œçš„æ€§èƒ½å¼ºçƒˆä¾èµ–äºåŸå§‹ç½‘ç»œçš„æ€§èƒ½ï¼Œæ¨¡å‹éƒ¨ç½²ä¸æ¨¡å‹ç”Ÿæˆä¹‹é—´å­˜åœ¨è¾ƒé«˜çš„è€¦åˆæ€§ã€‚æ¨¡å‹çš„é»‘ç®±æ€§è´¨ä¹Ÿé˜»ç¢äº†åœ¨å®‰å…¨å…³é”®ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼ŒåŒ»å­¦å’Œé‡‘èï¼‰ä¸­çš„éƒ¨ç½²ã€‚å› æ­¤ï¼Œå°†æ¨¡å‹å‹ç¼©ã€NASå’Œå¯è§£é‡Šæ€§ä½œä¸ºä¸€ä¸ªå•ä¸€ä¼˜åŒ–é—®é¢˜æ¥å¤„ç†ï¼Œå¹¶åœ¨å¯æ¥å—çš„æ—¶é—´æ¶ˆè€—å†…å®Œæˆè¿™ä¸€ä»»åŠ¡æ˜¯å……æ»¡å‰æ™¯ä½†åˆå…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚
- en: 6\. Applications, Open Issues, and Trends
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. åº”ç”¨ã€å¼€æ”¾é—®é¢˜ä¸è¶‹åŠ¿
- en: 6.1\. Applications
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. åº”ç”¨
- en: EDL algorithms have been widely used in various real-world applications. In
    practical, great development has been achieved in computer vision (CV), natural
    language processing (NLP) and other practical applications (e.g., crisis prediction
    and disease prediction).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: EDLç®—æ³•å·²å¹¿æ³›åº”ç”¨äºå„ç§ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ã€‚åœ¨å®è·µä¸­ï¼Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åŠå…¶ä»–å®é™…åº”ç”¨ï¼ˆä¾‹å¦‚ï¼Œå±æœºé¢„æµ‹å’Œç–¾ç—…é¢„æµ‹ï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚
- en: 6.1.1\. Computer Vision
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. è®¡ç®—æœºè§†è§‰
- en: CV is an important domain of computer science, playing an important role in
    identifying useful information (e.g., objects and classifications) for specific
    tasks (e.g., image segmentation (Zhou etÂ al., [2020](#bib.bib259)) and object
    detection (Zhang and Rockett, [2005](#bib.bib252))) on images or videos. In the
    early days, manually designed models for computer vision achieved good performance
    on public datasets at the expense of extensive time and labour. With the development
    of EDL, many new structures have been developed by computer programming and they
    show better performance than these manually designed models, especially on the
    widely used benchmark datasets for image classification, such as CIFAR-10, CIFAR-100
    (Krizhevsky etÂ al., [2009](#bib.bib102)), and ImageNet (Deng etÂ al., [2009](#bib.bib44)).
    For example, the state-of-the-art NAT-M4 (Lu etÂ al., [2021](#bib.bib125)) with
    a small model size achieves Top-1 accuracy of 80.5% on ImageNet. Image-to-image
    processing (Liu etÂ al., [2021b](#bib.bib117)) (e.g., super-resolution, image inpainting,
    and image restoration) also received extensive attention from researchers (Rundo
    etÂ al., [2019](#bib.bib170); Song etÂ al., [2020](#bib.bib184); Zhan etÂ al., [2021](#bib.bib239)).
    Ho et al. (Ho etÂ al., [2021](#bib.bib74)) employed NAS techniques to improve image
    denoising, inpainting, and super-resolution on the foundation of deep image prior
    (Ho etÂ al., [2021](#bib.bib74)). In addition to the above applications, EDL also
    has great potential in other areas of CV, such as object detection (Demirkir and
    Sankur, [2006](#bib.bib43)), video/picture understanding (Maniezzo, [1994](#bib.bib132)),
    and image segmentation (Fan etÂ al., [2020](#bib.bib56)).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦ä¸­çš„ä¸€ä¸ªé‡è¦é¢†åŸŸï¼Œåœ¨è¯†åˆ«æœ‰ç”¨ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œå¯¹è±¡å’Œåˆ†ç±»ï¼‰æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒåˆ†å‰²ï¼ˆZhou et al., [2020](#bib.bib259)ï¼‰å’Œå¯¹è±¡æ£€æµ‹ï¼ˆZhang
    and Rockett, [2005](#bib.bib252)ï¼‰ï¼‰ã€‚æ—©æœŸï¼Œæ‰‹åŠ¨è®¾è®¡çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ä»˜å‡ºäº†å¤§é‡æ—¶é—´å’ŒåŠ³åŠ›ã€‚éšç€EDLçš„å‘å±•ï¼Œè®¸å¤šæ–°ç»“æ„ç”±è®¡ç®—æœºç¼–ç¨‹å¼€å‘ï¼Œå¹¶ä¸”å®ƒä»¬åœ¨å›¾åƒåˆ†ç±»ç­‰å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºè¿™äº›æ‰‹åŠ¨è®¾è®¡çš„æ¨¡å‹ï¼Œå¦‚CIFAR-10ã€CIFAR-100ï¼ˆKrizhevsky
    et al., [2009](#bib.bib102)ï¼‰å’ŒImageNetï¼ˆDeng et al., [2009](#bib.bib44)ï¼‰ã€‚ä¾‹å¦‚ï¼Œæœ€å…ˆè¿›çš„NAT-M4ï¼ˆLu
    et al., [2021](#bib.bib125)ï¼‰åœ¨ImageNetä¸Šä»¥è¾ƒå°çš„æ¨¡å‹è§„æ¨¡å®ç°äº†80.5%çš„Top-1å‡†ç¡®ç‡ã€‚å›¾åƒåˆ°å›¾åƒå¤„ç†ï¼ˆLiu et
    al., [2021b](#bib.bib117)ï¼‰ï¼ˆä¾‹å¦‚ï¼Œè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®è¡¥å’Œå›¾åƒæ¢å¤ï¼‰ä¹Ÿå—åˆ°äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ï¼ˆRundo et al., [2019](#bib.bib170);
    Song et al., [2020](#bib.bib184); Zhan et al., [2021](#bib.bib239)ï¼‰ã€‚Ho et al.ï¼ˆHo
    et al., [2021](#bib.bib74)ï¼‰åœ¨æ·±åº¦å›¾åƒå…ˆéªŒçš„åŸºç¡€ä¸Šä½¿ç”¨äº†NASæŠ€æœ¯æ¥æ”¹è¿›å›¾åƒå»å™ªã€ä¿®è¡¥å’Œè¶…åˆ†è¾¨ç‡ã€‚é™¤äº†ä¸Šè¿°åº”ç”¨ï¼ŒEDLåœ¨CVçš„å…¶ä»–é¢†åŸŸä¹Ÿå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä¾‹å¦‚å¯¹è±¡æ£€æµ‹ï¼ˆDemirkir
    and Sankur, [2006](#bib.bib43)ï¼‰ã€è§†é¢‘/å›¾ç‰‡ç†è§£ï¼ˆManiezzo, [1994](#bib.bib132)ï¼‰å’Œå›¾åƒåˆ†å‰²ï¼ˆFan
    et al., [2020](#bib.bib56)ï¼‰ã€‚
- en: 6.1.2\. Natural Language Processing
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. è‡ªç„¶è¯­è¨€å¤„ç†
- en: Natural language processing (NLP) driven by computer science and computational
    linguistics, aims to understand, analyze, and extract knowledge on text and speech
    recoginition (Young etÂ al., [2018](#bib.bib235)). Many effective NLP models (e.g.,
    GPT-2 (Radford etÂ al., [2018](#bib.bib158)) and BERT (Devlin etÂ al., [2018](#bib.bib45)))
    narrow the chasm between human communication and computer understanding using
    sophisticated mechanisms. Recently, EC-inspired NLP models have been proposed
    such as language model (Murray and Chiang, [2015](#bib.bib142)), entity recognition
    (Sikdar etÂ al., [2012](#bib.bib181)), text classification (Tanaka etÂ al., [2016](#bib.bib194);
    Andersen etÂ al., [2021](#bib.bib10); Londt etÂ al., [2021](#bib.bib120), [2020](#bib.bib121)),
    and keyword spotting (Mazzawi etÂ al., [2019](#bib.bib134)). Satapathy et al. (Song,
    [2021](#bib.bib185)) introduced evolutionary multi-objective (i.e., inference
    time and accuracy) optimization in an English translation system. Sikdar et al.
    (Sikdar etÂ al., [2012](#bib.bib181)) employed DE in feature selection for named
    entity recognition (NER).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç”±è®¡ç®—æœºç§‘å­¦å’Œè®¡ç®—è¯­è¨€å­¦æ¨åŠ¨ï¼Œæ—¨åœ¨ç†è§£ã€åˆ†æå’Œæå–æ–‡æœ¬å’Œè¯­éŸ³è¯†åˆ«çš„çŸ¥è¯†ï¼ˆYoung et al., [2018](#bib.bib235)ï¼‰ã€‚è®¸å¤šæœ‰æ•ˆçš„NLPæ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒGPT-2ï¼ˆRadford
    et al., [2018](#bib.bib158)ï¼‰å’ŒBERTï¼ˆDevlin et al., [2018](#bib.bib45)ï¼‰ï¼‰åˆ©ç”¨å¤æ‚æœºåˆ¶ç¼©çŸ­äº†äººç±»äº¤æµä¸è®¡ç®—æœºç†è§£ä¹‹é—´çš„é¸¿æ²Ÿã€‚æœ€è¿‘ï¼Œæå‡ºäº†å—è¿›åŒ–è®¡ç®—å¯å‘çš„NLPæ¨¡å‹ï¼Œå¦‚è¯­è¨€æ¨¡å‹ï¼ˆMurray
    and Chiang, [2015](#bib.bib142)ï¼‰ã€å®ä½“è¯†åˆ«ï¼ˆSikdar et al., [2012](#bib.bib181)ï¼‰ã€æ–‡æœ¬åˆ†ç±»ï¼ˆTanaka
    et al., [2016](#bib.bib194); Andersen et al., [2021](#bib.bib10); Londt et al.,
    [2021](#bib.bib120), [2020](#bib.bib121)ï¼‰å’Œå…³é”®è¯è¯†åˆ«ï¼ˆMazzawi et al., [2019](#bib.bib134)ï¼‰ã€‚Satapathy
    et al.ï¼ˆSong, [2021](#bib.bib185)ï¼‰åœ¨è‹±è¯­ç¿»è¯‘ç³»ç»Ÿä¸­å¼•å…¥äº†è¿›åŒ–å¤šç›®æ ‡ï¼ˆå³æ¨ç†æ—¶é—´å’Œå‡†ç¡®æ€§ï¼‰ä¼˜åŒ–ã€‚Sikdar et al.ï¼ˆSikdar
    et al., [2012](#bib.bib181)ï¼‰åœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰çš„ç‰¹å¾é€‰æ‹©ä¸­åº”ç”¨äº†å·®åˆ†è¿›åŒ–ï¼ˆDEï¼‰ã€‚
- en: 6.1.3\. Other Applications
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. å…¶ä»–åº”ç”¨
- en: In addition to CV and NLP, EDL also shows strong ability on handling other practical
    applications, such as medical analysis (Liu etÂ al., [2018a](#bib.bib116); Zhu
    etÂ al., [2007](#bib.bib264)), financial prediction (Tang etÂ al., [2019](#bib.bib195)),
    signal processing (Erguzel etÂ al., [2014](#bib.bib51); Huang etÂ al., [2012](#bib.bib81)),
    and industrial prediction (Mei etÂ al., [2017](#bib.bib135); Ma etÂ al., [2021b](#bib.bib128)).
    In particular, Zhu et al. (Zhu etÂ al., [2007](#bib.bib264)) presented a Markov
    blanket-embedded genetic algorithm for feature selection to improve gene selection.
    In (Tang etÂ al., [2019](#bib.bib195)), financial bankruptcy analysis was handled
    by an evolutionary pruning neural network. The work in (Erguzel etÂ al., [2014](#bib.bib51))
    designed a feature selection method based on ACO to classify electromyography
    signals. For remote sensing imagery, a suitable model was found by multi-objective
    neural evolution architecture search (Ma etÂ al., [2021b](#bib.bib128)), where
    architecture complexity and performance error of searched network were two conflicting
    objectives.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº† CV å’Œ NLPï¼ŒEDL åœ¨å¤„ç†å…¶ä»–å®é™…åº”ç”¨æ–¹é¢ä¹Ÿå±•ç°äº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä¾‹å¦‚åŒ»å­¦åˆ†æï¼ˆLiu ç­‰ï¼Œ[2018a](#bib.bib116)ï¼›Zhu ç­‰ï¼Œ[2007](#bib.bib264)ï¼‰ã€é‡‘èé¢„æµ‹ï¼ˆTang
    ç­‰ï¼Œ[2019](#bib.bib195)ï¼‰ã€ä¿¡å·å¤„ç†ï¼ˆErguzel ç­‰ï¼Œ[2014](#bib.bib51)ï¼›Huang ç­‰ï¼Œ[2012](#bib.bib81)ï¼‰å’Œå·¥ä¸šé¢„æµ‹ï¼ˆMei
    ç­‰ï¼Œ[2017](#bib.bib135)ï¼›Ma ç­‰ï¼Œ[2021b](#bib.bib128)ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼ŒZhu ç­‰ï¼ˆZhu ç­‰ï¼Œ[2007](#bib.bib264)ï¼‰æå‡ºäº†ä¸€ç§åµŒå…¥é©¬å°”å¯å¤«æ¯¯çš„é—ä¼ ç®—æ³•ï¼Œç”¨äºç‰¹å¾é€‰æ‹©ä»¥æ”¹å–„åŸºå› é€‰æ‹©ã€‚åœ¨ï¼ˆTang
    ç­‰ï¼Œ[2019](#bib.bib195)ï¼‰ä¸­ï¼Œé‡‘èç ´äº§åˆ†æé€šè¿‡è¿›åŒ–ä¿®å‰ªç¥ç»ç½‘ç»œæ¥å¤„ç†ã€‚å·¥ä½œï¼ˆErguzel ç­‰ï¼Œ[2014](#bib.bib51)ï¼‰è®¾è®¡äº†ä¸€ç§åŸºäº
    ACO çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºåˆ†ç±»è‚Œç”µå›¾ä¿¡å·ã€‚å¯¹äºé¥æ„Ÿå›¾åƒï¼Œé€šè¿‡å¤šç›®æ ‡ç¥ç»è¿›åŒ–æ¶æ„æœç´¢ï¼ˆMa ç­‰ï¼Œ[2021b](#bib.bib128)ï¼‰æ‰¾åˆ°äº†é€‚åˆçš„æ¨¡å‹ï¼Œå…¶ä¸­æ¶æ„å¤æ‚æ€§å’Œæœç´¢ç½‘ç»œçš„æ€§èƒ½è¯¯å·®æ˜¯ä¸¤ä¸ªç›¸äº’å†²çªçš„ç›®æ ‡ã€‚
- en: 6.2\. Open Issues
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. æœªè§£å†³çš„é—®é¢˜
- en: EDL is a hot research topic in both fields of machine learning and evolutionary
    computation. There are a large number of publications on various top conferences
    and journals, such as ICCV, CVPR, GECCO, TPAMI, TEVC, TCYB, and TNNLS (see the
    reference list). Yet some challenges remain to be resolved.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: EDL æ˜¯æœºå™¨å­¦ä¹ å’Œè¿›åŒ–è®¡ç®—é¢†åŸŸçš„ä¸€ä¸ªçƒ­é—¨ç ”ç©¶è¯é¢˜ã€‚å…³äºè¿™ä¸€ä¸»é¢˜ï¼Œæœ‰å¤§é‡çš„è®ºæ–‡å‘è¡¨åœ¨å„å¤§é¡¶çº§ä¼šè®®å’ŒæœŸåˆŠä¸Šï¼Œå¦‚ ICCVã€CVPRã€GECCOã€TPAMIã€TEVCã€TCYB
    å’Œ TNNLSï¼ˆè¯·å‚è§å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼‰ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜éœ€è¦è§£å†³ã€‚
- en: 'Acceleration strategies: Many EDL approaches suffer from low efficiency due
    to the expensive evaluations. So various acceleration strategies, such as surrogate
    model (Sun etÂ al., [2019a](#bib.bib188)), supernet (Guo etÂ al., [2020](#bib.bib67)),
    and early stop (Sun etÂ al., [2019d](#bib.bib191)) have been designed. However,
    the improvements of the accuracy are at the expense of sacrifice a bit of model
    accuracy. Taking the supernet-based inheritance as an example (Xie etÂ al., [2022a](#bib.bib221)),
    we cannot guarantee that every subnet receives a reliable evaluation due to the
    catastrophic forgetting (Zhang etÂ al., [2020a](#bib.bib249)) and weight coupling
    (Hu etÂ al., [2021a](#bib.bib80)). Therefore, how to balance the efficiency and
    accuracy needs further investigation.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ é€Ÿç­–ç•¥ï¼šè®¸å¤š EDL æ–¹æ³•ç”±äºè¯„ä¼°æˆæœ¬é«˜è€Œæ•ˆç‡ä½ä¸‹ã€‚å› æ­¤ï¼Œè®¾è®¡äº†å„ç§åŠ é€Ÿç­–ç•¥ï¼Œå¦‚ä»£ç†æ¨¡å‹ï¼ˆSun ç­‰ï¼Œ[2019a](#bib.bib188)ï¼‰ã€è¶…ç½‘ç»œï¼ˆGuo
    ç­‰ï¼Œ[2020](#bib.bib67)ï¼‰å’Œæ—©åœï¼ˆSun ç­‰ï¼Œ[2019d](#bib.bib191)ï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›ç­–ç•¥åœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œä¹Ÿç‰ºç‰²äº†éƒ¨åˆ†æ¨¡å‹ç²¾åº¦ã€‚ä»¥åŸºäºè¶…ç½‘ç»œçš„ç»§æ‰¿ä¸ºä¾‹ï¼ˆXie
    ç­‰ï¼Œ[2022a](#bib.bib221)ï¼‰ï¼Œç”±äºç¾éš¾æ€§é—å¿˜ï¼ˆZhang ç­‰ï¼Œ[2020a](#bib.bib249)ï¼‰å’Œæƒé‡è€¦åˆï¼ˆHu ç­‰ï¼Œ[2021a](#bib.bib80)ï¼‰ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯æ¯ä¸ªå­ç½‘ç»œéƒ½èƒ½å¾—åˆ°å¯é çš„è¯„ä¼°ã€‚å› æ­¤ï¼Œå¦‚ä½•å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚
- en: 'Effectiveness: There is a debate on whether EDL has many advantages over other
    search paradigms (e.g., random search and RL). Some studies argue that many popular
    search paradigms (e.g., EC-based methods and RL-based methods) have no big difference
    from the random search methods in their performance, and some random search methods
    even outperform EC-based methods in some scenarios (Sciuto etÂ al., [2020](#bib.bib175)).
    On the contrary, EC-based approaches have also been proved to be more effective
    than random search methods in many studies (Real etÂ al., [2019](#bib.bib162);
    Guo etÂ al., [2020](#bib.bib67); Jones etÂ al., [2019](#bib.bib90)). Thus, a unified
    platform is essential to measure the effectiveness of different search models,
    under the consistent search space and hyperparameters configuration (Xie etÂ al.,
    [2022b](#bib.bib223)). In addition, elaborate experiments are required to justify
    the effects of different genetic operators (e.g., crossover operator) to the evolutionary
    process of EDL.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: æ•ˆæœæ€§ï¼šå…³äºEDLæ˜¯å¦ç›¸è¾ƒäºå…¶ä»–æœç´¢èŒƒå¼ï¼ˆå¦‚éšæœºæœç´¢å’ŒRLï¼‰å…·æœ‰æ›´å¤šä¼˜åŠ¿å­˜åœ¨äº‰è®ºã€‚ä¸€äº›ç ”ç©¶è®¤ä¸ºï¼Œè®¸å¤šæµè¡Œçš„æœç´¢èŒƒå¼ï¼ˆå¦‚åŸºäºECçš„æ–¹æ³•å’ŒåŸºäºRLçš„æ–¹æ³•ï¼‰åœ¨æ€§èƒ½ä¸Šä¸éšæœºæœç´¢æ–¹æ³•æ²¡æœ‰å¤ªå¤§åŒºåˆ«ï¼Œç”šè‡³åœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œä¸€äº›éšæœºæœç´¢æ–¹æ³•çš„è¡¨ç°ä¼˜äºåŸºäºECçš„æ–¹æ³•ï¼ˆSciuto
    et al., [2020](#bib.bib175)ï¼‰ã€‚ç›¸åï¼ŒåŸºäºECçš„æ–¹æ³•ä¹Ÿåœ¨è®¸å¤šç ”ç©¶ä¸­è¢«è¯æ˜æ¯”éšæœºæœç´¢æ–¹æ³•æ›´æœ‰æ•ˆï¼ˆReal et al., [2019](#bib.bib162)ï¼›Guo
    et al., [2020](#bib.bib67)ï¼›Jones et al., [2019](#bib.bib90)ï¼‰ã€‚å› æ­¤ï¼Œåœ¨ä¸€è‡´çš„æœç´¢ç©ºé—´å’Œè¶…å‚æ•°é…ç½®ä¸‹ï¼Œå»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„å¹³å°æ¥è¡¡é‡ä¸åŒæœç´¢æ¨¡å‹çš„æ•ˆæœæ˜¯è‡³å…³é‡è¦çš„ï¼ˆXie
    et al., [2022b](#bib.bib223)ï¼‰ã€‚æ­¤å¤–ï¼Œéœ€è¦è¯¦ç»†çš„å®éªŒæ¥è¯æ˜ä¸åŒé—ä¼ ç®—å­ï¼ˆå¦‚äº¤å‰ç®—å­ï¼‰å¯¹EDLè¿›åŒ–è¿‡ç¨‹çš„å½±å“ã€‚
- en: 'Large-scale datasets: There is an issue for the studies of EDL on large-scale
    datasets. It is noted that many studies of EDL are tested on small- and medium-scale
    datasets such as CIFAR-10 and CIFAR-100 (including 60000 32$\times$32 images),
    and especially the accuracy on CIFAR-10 reaches up to 98% (Lu etÂ al., [2021](#bib.bib125)).
    Although large-scale datasets are ubiquitous and essential in various domains
    like gene analysis (Yu etÂ al., [2009](#bib.bib236)) and ImageNet (Deng etÂ al.,
    [2009](#bib.bib44)), computational costs are unaffordable for many researchers
    as pointed in some statistical reports (He etÂ al., [2021](#bib.bib72); Liu etÂ al.,
    [2021b](#bib.bib117)). Therefore, the sensitivity of the EDL methods to different
    scales of datasets is necessary (Zhou etÂ al., [2021a](#bib.bib258)) and how to
    economically and efficiently verify EDL methods on large-scale datasets also deserves
    much investigations.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è§„æ¨¡æ•°æ®é›†ï¼šåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šç ”ç©¶EDLå­˜åœ¨é—®é¢˜ã€‚æ³¨æ„åˆ°è®¸å¤šEDLçš„ç ”ç©¶æ˜¯åœ¨å°å‹å’Œä¸­å‹æ•°æ®é›†ä¸Šæµ‹è¯•çš„ï¼Œå¦‚CIFAR-10å’ŒCIFAR-100ï¼ˆåŒ…æ‹¬60000å¼ 32$\times$32çš„å›¾åƒï¼‰ï¼Œå°¤å…¶æ˜¯CIFAR-10ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°98%ï¼ˆLu
    et al., [2021](#bib.bib125)ï¼‰ã€‚å°½ç®¡å¤§è§„æ¨¡æ•°æ®é›†åœ¨åŸºå› åˆ†æï¼ˆYu et al., [2009](#bib.bib236)ï¼‰å’ŒImageNetï¼ˆDeng
    et al., [2009](#bib.bib44)ï¼‰ç­‰å„ç§é¢†åŸŸä¸­éšå¤„å¯è§ä¸”è‡³å…³é‡è¦ï¼Œä½†å¦‚ä¸€äº›ç»Ÿè®¡æŠ¥å‘Šæ‰€æŒ‡å‡ºï¼Œè®¡ç®—æˆæœ¬å¯¹è®¸å¤šç ”ç©¶äººå‘˜æ¥è¯´æ˜¯éš¾ä»¥æ‰¿å—çš„ï¼ˆHe
    et al., [2021](#bib.bib72)ï¼›Liu et al., [2021b](#bib.bib117)ï¼‰ã€‚å› æ­¤ï¼ŒEDLæ–¹æ³•å¯¹ä¸åŒè§„æ¨¡æ•°æ®é›†çš„æ•æ„Ÿæ€§æ˜¯å¿…è¦çš„ï¼ˆZhou
    et al., [2021a](#bib.bib258)ï¼‰ï¼Œå¦‚ä½•ç»æµé«˜æ•ˆåœ°åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸ŠéªŒè¯EDLæ–¹æ³•ä¹Ÿå€¼å¾—æ·±å…¥ç ”ç©¶ã€‚
- en: 'End-to-end EDL: Originally, AutoML aims to simultaneously optimize feature
    engineering, model generation and model deployment as a whole. However, there
    is a strong correlation between them where the performance of next phase heavily
    relies on the results of the previous phase (Liu and Guo, [2021](#bib.bib115)).
    As a result, most studies only focus on parts of the EDL pipeline (Fig. [1](#S1.F1
    "Figure 1 â€£ 1\. Introduction â€£ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues")). For instance, TPOT (Olson and Moore,
    [2016](#bib.bib149)) is designed on top of Pytorch for building classification
    tasks, which however only supports a multi-layer perception machine (i.e., model
    generation). There are many partially accomplished end-to-end for EDL, such as
    ModelArts (model generation), Googleâ€™s Cloud (NAS), and Feature Labs (feature
    engineering) (Yao etÂ al., [2018](#bib.bib231)), to name but a few. The main reason
    is that the optimization of the whole EDL pipeline may need huge computational
    cost not only on the exploration of the large-scale search space but also on handling
    highly-coupled relation between different parts of EDL. Consequently, finding
    an optimal solution of the complete EDL pipeline is essential but challenging.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ç«¯åˆ°ç«¯çš„EDLï¼šæœ€åˆï¼ŒAutoMLæ—¨åœ¨æ•´ä½“ä¸ŠåŒæ—¶ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹ç”Ÿæˆå’Œæ¨¡å‹éƒ¨ç½²ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¹‹é—´æœ‰å¾ˆå¼ºçš„å…³è”ï¼Œä¸‹ä¸€é˜¶æ®µçš„è¡¨ç°ä¸¥é‡ä¾èµ–äºå‰ä¸€é˜¶æ®µçš„ç»“æœï¼ˆLiu
    å’Œ Guo, [2021](#bib.bib115)ï¼‰ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°ç ”ç©¶ä»…å…³æ³¨EDLç®¡é“çš„éƒ¨åˆ†å†…å®¹ï¼ˆå›¾ [1](#S1.F1 "å›¾ 1 â€£ 1\. ä»‹ç» â€£
    è¿›åŒ–æ·±åº¦å­¦ä¹ ï¼šåŸç†ã€ç®—æ³•ã€åº”ç”¨åŠå¼€æ”¾é—®é¢˜çš„è°ƒæŸ¥")ï¼‰ã€‚ä¾‹å¦‚ï¼ŒTPOTï¼ˆOlson å’Œ Moore, [2016](#bib.bib149)ï¼‰æ˜¯åŸºäºPytorchæ„å»ºåˆ†ç±»ä»»åŠ¡çš„ï¼Œä½†åªæ”¯æŒå¤šå±‚æ„ŸçŸ¥æœºï¼ˆå³ï¼Œæ¨¡å‹ç”Ÿæˆï¼‰ã€‚ç›®å‰æœ‰è®¸å¤šéƒ¨åˆ†å®Œæˆçš„ç«¯åˆ°ç«¯EDLï¼Œä¾‹å¦‚ModelArtsï¼ˆæ¨¡å‹ç”Ÿæˆï¼‰ã€Googleçš„Cloudï¼ˆNASï¼‰å’ŒFeature
    Labsï¼ˆç‰¹å¾å·¥ç¨‹ï¼‰ï¼ˆYao ç­‰, [2018](#bib.bib231)ï¼‰ï¼Œä»…ä¸¾å‡ ä¾‹ã€‚ä¸»è¦åŸå› æ˜¯ä¼˜åŒ–æ•´ä¸ªEDLç®¡é“å¯èƒ½éœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ï¼Œä¸ä»…åœ¨æ¢ç´¢å¤§è§„æ¨¡æœç´¢ç©ºé—´æ–¹é¢ï¼Œè¿˜åœ¨å¤„ç†EDLä¸åŒéƒ¨åˆ†ä¹‹é—´çš„é«˜åº¦è€¦åˆå…³ç³»æ–¹é¢ã€‚å› æ­¤ï¼Œæ‰¾åˆ°å®Œæ•´EDLç®¡é“çš„æœ€ä¼˜è§£å†³æ–¹æ¡ˆè‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: 6.3\. Challenges and Future Trends
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. æŒ‘æˆ˜ä¸æœªæ¥è¶‹åŠ¿
- en: Although remarkable progress has been made in EDL, there are still many promising
    lines of research.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡EDLå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»æœ‰è®¸å¤šæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚
- en: 'Fair comparisons: Unfair comparisons of different EDL methods are easily encountered
    with the following reasons. Firstly, uniform benchmarks are essential. In feature
    engineering, no uniform benchmark is for the fair comparison of different algorithms
    due to different downstream prediction models and feature sets. Secondly, there
    is no uniform criterion for different methods in handling NAS and model compression
    by using different tricks (e.g., cutout (DeVries and Taylor, [2017](#bib.bib46))
    and ScheduledDropPath (Zoph etÂ al., [2018](#bib.bib265))), which may influence
    the performance of the final architecture. Thirdly, a fair platform for EDL is
    essential. There are some fair benchmarks but only for specific tasks, such as
    BenchENAS (Xie etÂ al., [2022b](#bib.bib223)), NAS-Bench-101 (Ying etÂ al., [2019](#bib.bib234)),
    NAS-Bench-201 (Dong and Yang, [2020](#bib.bib47)), NAS-Bench-301 (Siems etÂ al.,
    [2020](#bib.bib180)), and HW-NAS-Bench (Li etÂ al., [2021](#bib.bib108)).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: å…¬å¹³æ¯”è¾ƒï¼šä¸åŒEDLæ–¹æ³•çš„éå…¬å¹³æ¯”è¾ƒå¸¸å› ä»¥ä¸‹åŸå› å‡ºç°ã€‚é¦–å…ˆï¼Œç»Ÿä¸€åŸºå‡†æµ‹è¯•æ˜¯å¿…éœ€çš„ã€‚åœ¨ç‰¹å¾å·¥ç¨‹ä¸­ï¼Œç”±äºä¸åŒçš„ä¸‹æ¸¸é¢„æµ‹æ¨¡å‹å’Œç‰¹å¾é›†ï¼Œæ²¡æœ‰ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•æ¥å…¬å¹³æ¯”è¾ƒä¸åŒç®—æ³•ã€‚å…¶æ¬¡ï¼Œå¤„ç†NASå’Œæ¨¡å‹å‹ç¼©çš„ä¸åŒæ–¹æ³•æ²¡æœ‰ç»Ÿä¸€æ ‡å‡†ï¼ˆä¾‹å¦‚ï¼Œcutoutï¼ˆDeVries
    å’Œ Taylor, [2017](#bib.bib46)ï¼‰å’ŒScheduledDropPathï¼ˆZoph ç­‰, [2018](#bib.bib265)ï¼‰ï¼‰ï¼Œè¿™å¯èƒ½å½±å“æœ€ç»ˆæ¶æ„çš„æ€§èƒ½ã€‚ç¬¬ä¸‰ï¼Œå…¬å¹³çš„EDLå¹³å°æ˜¯å¿…éœ€çš„ã€‚æœ‰ä¸€äº›å…¬å¹³çš„åŸºå‡†æµ‹è¯•ï¼Œä½†ä»…é™äºç‰¹å®šä»»åŠ¡ï¼Œä¾‹å¦‚BenchENASï¼ˆXie
    ç­‰, [2022b](#bib.bib223)ï¼‰ã€NAS-Bench-101ï¼ˆYing ç­‰, [2019](#bib.bib234)ï¼‰ã€NAS-Bench-201ï¼ˆDong
    å’Œ Yang, [2020](#bib.bib47)ï¼‰ã€NAS-Bench-301ï¼ˆSiems ç­‰, [2020](#bib.bib180)ï¼‰å’ŒHW-NAS-Benchï¼ˆLi
    ç­‰, [2021](#bib.bib108)ï¼‰ã€‚
- en: 'Interpretability: EDL is known as a black-box optimization, and there is a
    lack of theoretical analysis to explain its superiority (Wang etÂ al., [2021b](#bib.bib206)).
    For example, it is difficult to explain why EC-based method tends to select features
    contribute to the performance of the classification model in feature engineering.
    As a result, the development of EDL in some sensitive domains such as financial
    and medical fields is slow. To overcome this issue, Evans et al. (Evans etÂ al.,
    [2018b](#bib.bib54)) used visualization to expound how the evolved convolution
    filter served and indirectly explained the search process of the model. Nevertheless,
    some studies argue that the explanation for these occurrences is usually post-hoc
    and lacks trustworthy mathematical deduction (Liu etÂ al., [2021b](#bib.bib117);
    Al-Sahaf etÂ al., [2019](#bib.bib6)). Thus, the interpretability of EDL is an interesting
    and promising research direction.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§£é‡Šæ€§ï¼šEDLè¢«ç§°ä¸ºé»‘ç®±ä¼˜åŒ–ï¼Œç›®å‰ç¼ºä¹ç†è®ºåˆ†ææ¥è§£é‡Šå…¶ä¼˜è¶Šæ€§ï¼ˆWang et al., [2021b](#bib.bib206)ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¾ˆéš¾è§£é‡Šä¸ºä»€ä¹ˆåŸºäºECçš„æ–¹æ³•å€¾å‘äºé€‰æ‹©æœ‰åŠ©äºåˆ†ç±»æ¨¡å‹æ€§èƒ½çš„ç‰¹å¾ã€‚åœ¨ç‰¹å¾å·¥ç¨‹ä¸­ã€‚è¿™å¯¼è‡´EDLåœ¨ä¸€äº›æ•æ„Ÿé¢†åŸŸï¼Œå¦‚é‡‘èå’ŒåŒ»ç–—é¢†åŸŸçš„å‘å±•ç¼“æ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒEvans
    et al. (Evans et al., [2018b](#bib.bib54)) ä½¿ç”¨å¯è§†åŒ–æ¥é˜æ˜è¿›åŒ–å·ç§¯æ»¤æ³¢å™¨çš„ä½œç”¨ï¼Œå¹¶é—´æ¥è§£é‡Šæ¨¡å‹çš„æœç´¢è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œä¸€äº›ç ”ç©¶è®¤ä¸ºï¼Œè¿™äº›ç°è±¡çš„è§£é‡Šé€šå¸¸æ˜¯äº‹åè§£é‡Šçš„ï¼Œç¼ºä¹å¯é çš„æ•°å­¦æ¨å¯¼ï¼ˆLiu
    et al., [2021b](#bib.bib117)ï¼›Al-Sahaf et al., [2019](#bib.bib6)ï¼‰ã€‚å› æ­¤ï¼ŒEDLçš„å¯è§£é‡Šæ€§æ˜¯ä¸€ä¸ªæœ‰è¶£ä¸”å……æ»¡å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚
- en: 'Exploring more scenarios: There is still plenty of room for the improvement
    of the performance of EDL on both benchmarks and real-world applications. Although
    EDL methods outperform manually designed models on various image benchmarks (CIFAR-10
    and ImageNet), the state-of-the-art EDL methods (Wang etÂ al., [2020a](#bib.bib210))
    lost their advantages on NLP in comparison with human-designed models (e.g., GPT-2
    (Radford etÂ al., [2018](#bib.bib158)), Transformer-XL (Dai etÂ al., [2019](#bib.bib39))).
    In comparison with the benchmarks, it is more difficult to handle real-world tasks,
    which inevitably contain noise (e.g., mislabeling and inadequate or imbalance
    data) or may have small-scale datasets (leading to overfitting). Hence, some techniques
    such as unsupervised and self-supervised learning may be incorporated into EDL
    to mitigate these types of issues.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢ç´¢æ›´å¤šåœºæ™¯ï¼šåœ¨åŸºå‡†æµ‹è¯•å’Œå®é™…åº”ç”¨ä¸­ï¼ŒEDLçš„æ€§èƒ½ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚è™½ç„¶EDLæ–¹æ³•åœ¨å„ç§å›¾åƒåŸºå‡†æµ‹è¯•ï¼ˆå¦‚CIFAR-10å’ŒImageNetï¼‰ä¸­ä¼˜äºæ‰‹åŠ¨è®¾è®¡çš„æ¨¡å‹ï¼Œä½†æœ€å…ˆè¿›çš„EDLæ–¹æ³•ï¼ˆWang
    et al., [2020a](#bib.bib210)ï¼‰åœ¨NLPé¢†åŸŸç›¸æ¯”äºäººç±»è®¾è®¡çš„æ¨¡å‹ï¼ˆå¦‚GPT-2 (Radford et al., [2018](#bib.bib158))ï¼ŒTransformer-XL
    (Dai et al., [2019](#bib.bib39))ï¼‰å¤±å»äº†å…¶ä¼˜åŠ¿ã€‚ä¸åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œå¤„ç†å®é™…ä»»åŠ¡æ›´ä¸ºå›°éš¾ï¼Œå› ä¸ºå®é™…ä»»åŠ¡ä¸å¯é¿å…åœ°åŒ…å«å™ªå£°ï¼ˆå¦‚é”™è¯¯æ ‡è®°å’Œæ•°æ®ä¸è¶³æˆ–ä¸å¹³è¡¡ï¼‰æˆ–å¯èƒ½å…·æœ‰å°è§„æ¨¡æ•°æ®é›†ï¼ˆå¯¼è‡´è¿‡æ‹Ÿåˆï¼‰ã€‚å› æ­¤ï¼Œå¯ä»¥å°†ä¸€äº›æŠ€æœ¯ï¼Œå¦‚æ— ç›‘ç£å’Œè‡ªç›‘ç£å­¦ä¹ ï¼Œçº³å…¥EDLä¸­ï¼Œä»¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚
- en: 7\. Conclusions
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. ç»“è®º
- en: With the development of machine leaning and evolutionary computation, many EDL
    approaches have been proposed to automatically optimize the parameters or architectures
    of deep models following the EC optimization framework. EDL approaches show competitive
    performance in robust and search capability, in comparison with the manually designed
    approaches. Therefore, EDL has become a hot research topic.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æœºå™¨å­¦ä¹ å’Œè¿›åŒ–è®¡ç®—çš„å‘å±•ï¼Œè®¸å¤šEDLæ–¹æ³•è¢«æå‡ºï¼Œç”¨äºè‡ªåŠ¨ä¼˜åŒ–æ·±åº¦æ¨¡å‹çš„å‚æ•°æˆ–æ¶æ„ï¼Œéµå¾ªECä¼˜åŒ–æ¡†æ¶ã€‚ä¸æ‰‹åŠ¨è®¾è®¡çš„æ–¹æ³•ç›¸æ¯”ï¼ŒEDLæ–¹æ³•åœ¨é²æ£’æ€§å’Œæœç´¢èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚å› æ­¤ï¼ŒEDLå·²æˆä¸ºä¸€ä¸ªçƒ­é—¨ç ”ç©¶è¯¾é¢˜ã€‚
- en: In this survey, we first introduced EDL from the perspective of DL and EC to
    facilitate the understanding of readers from the communities of ML and EC. Then
    we formulated EDL as a complex optimization problem, and provided a comprehensive
    survey of EC techniques in solving EDL optimization problems in terms of feature
    engineering, model generation to model deployment to form a new taxonomy (i.e.,
    what, where and how to evolve/optimize in EDL). Specifiically, we discussed the
    solution representations and search paradigms of EDL at different stages of its
    pipeline in detail. Then the pros and cons of EC-based approaches in comparison
    to non-EC based ones are discussed. Subsequently, various applications are summarized
    to show the potential ability of EDL in handling real-world problems.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»DLå’ŒECçš„è§’åº¦ä»‹ç»äº†EDLï¼Œä»¥ä¾¿MLå’ŒECé¢†åŸŸçš„è¯»è€…èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†EDLå®šä¹‰ä¸ºä¸€ä¸ªå¤æ‚çš„ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æä¾›äº†å…³äºECæŠ€æœ¯åœ¨è§£å†³EDLä¼˜åŒ–é—®é¢˜ä¸­çš„å…¨é¢è°ƒæŸ¥ï¼ŒåŒ…æ‹¬ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹ç”Ÿæˆåˆ°æ¨¡å‹éƒ¨ç½²ï¼Œä»è€Œå½¢æˆäº†ä¸€ä¸ªæ–°çš„åˆ†ç±»æ³•ï¼ˆå³ï¼Œåœ¨EDLä¸­ä»€ä¹ˆã€åœ¨å“ªé‡Œä»¥åŠå¦‚ä½•è¿›åŒ–/ä¼˜åŒ–ï¼‰ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¯¦ç»†è®¨è®ºäº†EDLåœ¨å…¶ç®¡é“ä¸åŒé˜¶æ®µçš„è§£å†³æ–¹æ¡ˆè¡¨ç¤ºå’Œæœç´¢èŒƒå¼ã€‚ç„¶åï¼Œè®¨è®ºäº†åŸºäºECçš„æ–¹æ³•ä¸éECæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚éšåï¼Œæ€»ç»“äº†å„ç§åº”ç”¨ï¼Œä»¥å±•ç¤ºEDLåœ¨å¤„ç†ç°å®ä¸–ç•Œé—®é¢˜ä¸­çš„æ½œåœ¨èƒ½åŠ›ã€‚
- en: Although EDL approaches have achieved great progress in AutoML, there are still
    a number of challenging issues to be resolved. For example, effective acceleration
    strategies are essential to reduce the expensive optimization process. Another
    issue is to handle large-scale datasets and how to perform fair comparisons between
    different EDL approaches or non-EC based methods. More investigations are required
    to theoretically analyse or interpret the search ability of EDL. In addition,
    a lot of efforts are required on the improving the performance of EDL on both
    benchmarks (e.g., large-scale and small-scale data) and real-world applications.
    Lastly, the development of end-to-end EDL is challenging but deserves much efforts.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡EDLæ–¹æ³•åœ¨AutoMLä¸­å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨è®¸å¤šå¾…è§£å†³çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œæœ‰æ•ˆçš„åŠ é€Ÿç­–ç•¥å¯¹äºå‡å°‘æ˜‚è´µçš„ä¼˜åŒ–è¿‡ç¨‹è‡³å…³é‡è¦ã€‚å¦ä¸€ä¸ªé—®é¢˜æ˜¯å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ä»¥åŠå¦‚ä½•åœ¨ä¸åŒçš„EDLæ–¹æ³•æˆ–éECæ–¹æ³•ä¹‹é—´è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚éœ€è¦æ›´å¤šç ”ç©¶æ¥ç†è®ºåˆ†ææˆ–è§£é‡ŠEDLçš„æœç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦å¤§é‡åŠªåŠ›æ¥æé«˜EDLåœ¨åŸºå‡†ï¼ˆä¾‹å¦‚ï¼Œå¤§è§„æ¨¡å’Œå°è§„æ¨¡æ•°æ®ï¼‰å’Œç°å®åº”ç”¨ä¸­çš„è¡¨ç°ã€‚æœ€åï¼Œç«¯åˆ°ç«¯çš„EDLå¼€å‘å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å€¼å¾—ä»˜å‡ºå¤§é‡åŠªåŠ›ã€‚
- en: References
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: (1)
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Abdi and Williams (2010) HervÃ© Abdi and LynneÂ J Williams. 2010. Principal Component
    Analysis. *Comput. Stat.* 2, 4 (2010), 433â€“459.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdiå’ŒWilliamsï¼ˆ2010ï¼‰HervÃ© Abdiå’ŒLynne J Williamsã€‚2010å¹´ã€‚ä¸»æˆåˆ†åˆ†æã€‚*Comput. Stat.* 2,
    4 (2010), 433â€“459ã€‚
- en: Ahmed etÂ al. (2019) Amr Ahmed, SaadÂ Mohamed Darwish, and MohamedÂ M. El-Sherbiny.
    2019. A Novel Automatic CNN Architecture Design Approach Based on Genetic Algorithm.
    In *Int. Conf. Adv. Intell. Syst. Inform.* 473â€“482.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmedç­‰ï¼ˆ2019ï¼‰Amr Ahmedã€Saad Mohamed Darwishå’ŒMohamed M. El-Sherbinyã€‚2019å¹´ã€‚åŸºäºé—ä¼ ç®—æ³•çš„è‡ªåŠ¨CNNæ¶æ„è®¾è®¡æ–°æ–¹æ³•ã€‚åœ¨*Int.
    Conf. Adv. Intell. Syst. Inform.* 473â€“482ã€‚
- en: Ahmed etÂ al. (2014) Soha Ahmed, Mengjie Zhang, Lifeng Peng, and Bing Xue. 2014.
    Multiple Feature Construction for Effective Biomarker Identification and Classification
    Using Genetic Programming. In *Proc. Genetic Evol. Comput. Conf.* 249â€“256.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmedç­‰ï¼ˆ2014ï¼‰Soha Ahmedã€Mengjie Zhangã€Lifeng Pengå’ŒBing Xueã€‚2014å¹´ã€‚ä½¿ç”¨é—ä¼ ç¼–ç¨‹è¿›è¡Œæœ‰æ•ˆç”Ÿç‰©æ ‡å¿—ç‰©è¯†åˆ«å’Œåˆ†ç±»çš„å¤šç‰¹å¾æ„é€ ã€‚åœ¨*Proc.
    Genetic Evol. Comput. Conf.* 249â€“256ã€‚
- en: Al-kazemi and Mohan (2002) Buthainah Al-kazemi and ChilukuriÂ Krishna Mohan.
    2002. Training Feedforward Neural Networks using Nulti-phase Particle Swarm Optimization.
    In *Proc. Int. Conf. Neural Inf. Process.*, Vol.Â 5. 2615â€“2619.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-kazemiå’ŒMohanï¼ˆ2002ï¼‰Buthainah Al-kazemiå’ŒChilukuri Krishna Mohanã€‚2002å¹´ã€‚ä½¿ç”¨å¤šç›¸ç²’å­ç¾¤ä¼˜åŒ–è®­ç»ƒå‰é¦ˆç¥ç»ç½‘ç»œã€‚åœ¨*Proc.
    Int. Conf. Neural Inf. Process.*, Vol. 5. 2615â€“2619ã€‚
- en: Al-Sahaf etÂ al. (2019) Harith Al-Sahaf, Ying Bi, Qi Chen, Andrew Lensen, Yi
    Mei, Yanan Sun, Binh Tran, Bing Xue, and Mengjie Zhang. 2019. A Survey on Evolutionary
    Machine Learning. *J. R. Soc. N. Z.* 49, 2 (2019), 205â€“228.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Sahafç­‰ï¼ˆ2019ï¼‰Harith Al-Sahafã€Ying Biã€Qi Chenã€Andrew Lensenã€Yi Meiã€Yanan Sunã€Binh
    Tranã€Bing Xueå’ŒMengjie Zhangã€‚2019å¹´ã€‚å…³äºè¿›åŒ–æœºå™¨å­¦ä¹ çš„è°ƒæŸ¥ã€‚*J. R. Soc. N. Z.* 49, 2 (2019),
    205â€“228ã€‚
- en: Albukhanajer etÂ al. (2015) WissamÂ A. Albukhanajer, JohannÂ A. Briffa, and Yaochu
    Jin. 2015. Evolutionary Multiobjective Image Feature Extraction in the Presence
    of Noise. *IEEE Trans. Cybern.* 45, 9 (2015), 1757â€“1768.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Albukhanajerç­‰ï¼ˆ2015ï¼‰Wissam A. Albukhanajerã€Johann A. Briffaå’ŒYaochu Jinã€‚2015å¹´ã€‚åœ¨å™ªå£°å­˜åœ¨ä¸‹çš„è¿›åŒ–å¤šç›®æ ‡å›¾åƒç‰¹å¾æå–ã€‚*IEEE
    Trans. Cybern.* 45, 9 (2015), 1757â€“1768ã€‚
- en: 'Alexandropoulos and Aridas (2019) Stamatios-AggelosÂ N Alexandropoulos and ChristosÂ K
    Aridas. 2019. Multi-objective Evolutionary Optimization Algorithms for Machine
    Learning: A Recent Survey. *Approximation and Optimization* (2019), 35â€“55.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexandropoulos and Aridas (2019) Stamatios-Aggelos N Alexandropoulos å’Œ Christos
    K Aridas. 2019. æœºå™¨å­¦ä¹ çš„å¤šç›®æ ‡è¿›åŒ–ä¼˜åŒ–ç®—æ³•ï¼šæœ€æ–°ç»¼è¿°ã€‚*Approximation and Optimization* (2019), 35â€“55.
- en: Aljarah etÂ al. (2018) Ibrahim Aljarah, Hossam Faris, and SeyedÂ Mohammad Mirjalili.
    2018. Optimizing Connection Weights in Neural Networks Using the Whale Optimization
    Algorithm. *Soft Comput.* 22, 1 (2018), 1â€“15.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aljarah et al. (2018) Ibrahim Aljarah, Hossam Faris, å’Œ Seyed Mohammad Mirjalili.
    2018. ä½¿ç”¨é²¸é±¼ä¼˜åŒ–ç®—æ³•ä¼˜åŒ–ç¥ç»ç½‘ç»œä¸­çš„è¿æ¥æƒé‡ã€‚*Soft Comput.* 22, 1 (2018), 1â€“15.
- en: Andersen etÂ al. (2021) Hayden Andersen, Sean Stevenson, Tuan Ha, Xiaoying Gao,
    and Bing Xue. 2021. Evolving Neural Networks for Text Classification Using Genetic
    Algorithm-based Approaches. In *Proc. IEEE Congr. Evol. Comput.* 1241â€“1248.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andersen et al. (2021) Hayden Andersen, Sean Stevenson, Tuan Ha, Xiaoying Gao,
    å’Œ Bing Xue. 2021. åˆ©ç”¨åŸºäºé—ä¼ ç®—æ³•çš„æ–¹æ³•è¿›åŒ–ç¥ç»ç½‘ç»œè¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚è§äº *Proc. IEEE Congr. Evol. Comput.*
    1241â€“1248.
- en: AssunÃ§Ã£o etÂ al. (2019a) Filipe AssunÃ§Ã£o, Joao Correia, and RÃºben ConceiÃ§Ã£o.
    2019a. Automatic Design of Artificial Neural Networks for Gamma-Ray Detection.
    *IEEE Access* 7 (2019), 110531â€“110540.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AssunÃ§Ã£o et al. (2019a) Filipe AssunÃ§Ã£o, Joao Correia, å’Œ RÃºben ConceiÃ§Ã£o. 2019a.
    ç”¨äºä¼½é©¬å°„çº¿æ¢æµ‹çš„äººå·¥ç¥ç»ç½‘ç»œè‡ªåŠ¨è®¾è®¡ã€‚*IEEE Access* 7 (2019), 110531â€“110540.
- en: AssunÃ§Ã£o etÂ al. (2018) Filipe AssunÃ§Ã£o, Nuno LourenÃ§o, P. Machado, and Bernardete
    Ribeiro. 2018. Evolving the Topology of Large Scale Deep Neural Networks. In *Proc.
    Eur. Conf. Genetic Program*. 19â€“34.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AssunÃ§Ã£o et al. (2018) Filipe AssunÃ§Ã£o, Nuno LourenÃ§o, P. Machado, å’Œ Bernardete
    Ribeiro. 2018. å¤§è§„æ¨¡æ·±åº¦ç¥ç»ç½‘ç»œçš„æ‹“æ‰‘è¿›åŒ–ã€‚è§äº *Proc. Eur. Conf. Genetic Program*. 19â€“34.
- en: 'AssunÃ§Ã£o etÂ al. (2019b) Filipe AssunÃ§Ã£o, Nuno LourenÃ§o, Penousal Machado, and
    Bernardete Ribeiro. 2019b. Fast denser: Efficient Deep Neuroevolution. In *Proc.
    Eur. Conf. Genetic Program*. 197â€“212.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AssunÃ§Ã£o et al. (2019b) Filipe AssunÃ§Ã£o, Nuno LourenÃ§o, Penousal Machado, å’Œ
    Bernardete Ribeiro. 2019b. Fast denser: Efficient Deep Neuroevolution. è§äº *Proc.
    Eur. Conf. Genetic Program*. 197â€“212.'
- en: Atre etÂ al. (2021) Medha Atre, Birendra Jha, and Ashwini Rao. 2021. Distributed
    Deep Learning Using Volunteer Computing-Like Paradigm. In *Proc. Int. Parallel
    and Distrib. Process. Symp.* 933â€“942.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atre et al. (2021) Medha Atre, Birendra Jha, å’Œ Ashwini Rao. 2021. ä½¿ç”¨å¿—æ„¿è®¡ç®—ç±»ä¼¼èŒƒå¼çš„åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ã€‚è§äº
    *Proc. Int. Parallel and Distrib. Process. Symp.* 933â€“942.
- en: Barman and Kwon (2020) Shohag Barman and Yung-Keun Kwon. 2020. A Neuro-Evolution
    Approach to Infer A Boolean Network From Time-Series Gene Expressions. *Bioinformatics*
    36, 2 (2020), i762â€“i769.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barman and Kwon (2020) Shohag Barman å’Œ Yung-Keun Kwon. 2020. ä¸€ç§ç¥ç»è¿›åŒ–æ–¹æ³•æ¥æ¨æ–­æ—¶é—´åºåˆ—åŸºå› è¡¨è¾¾çš„å¸ƒå°”ç½‘ç»œã€‚*Bioinformatics*
    36, 2 (2020), i762â€“i769.
- en: Behjat and Chidambaran (2019) Amir Behjat and Sharat Chidambaran. 2019. Adaptive
    Genomic Evolution of Neural Network Topologies (AGENT) for State-to-Action Mapping
    in Autonomous Agents. In *Proc. Int. Conf. Robot. Autom.* 9638â€“9644.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Behjat and Chidambaran (2019) Amir Behjat å’Œ Sharat Chidambaran. 2019. è‡ªé€‚åº”åŸºå› ç»„è¿›åŒ–ç¥ç»ç½‘ç»œæ‹“æ‰‘ï¼ˆAGENTï¼‰ç”¨äºè‡ªä¸»ä½“çš„çŠ¶æ€åˆ°åŠ¨ä½œæ˜ å°„ã€‚è§äº
    *Proc. Int. Conf. Robot. Autom.* 9638â€“9644.
- en: Bhanu and Krawiec (2002) Bir Bhanu and Krzysztof Krawiec. 2002. Coevolutionary
    Construction of Features for Transformation of Representation in Machine Learning.
    In *Proc. Genetic Evol. Comput. Conf.* 249â€“254.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhanu and Krawiec (2002) Bir Bhanu å’Œ Krzysztof Krawiec. 2002. ç”¨äºæœºå™¨å­¦ä¹ ä¸­è¡¨ç¤ºè½¬åŒ–çš„ç‰¹å¾çš„å…±è¿›åŒ–æ„é€ ã€‚è§äº
    *Proc. Genetic Evol. Comput. Conf.* 249â€“254.
- en: Bi etÂ al. (2018) Ying Bi, Bing Xue, and Mengjie Zhang. 2018. An Automatic Feature
    Extraction Approach to Image Classification Using Genetic Programming. In *Proc.
    Int. Conf. Appl. Evol. Comput.* 421â€“438.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi et al. (2018) Ying Bi, Bing Xue, å’Œ Mengjie Zhang. 2018. ä¸€ç§åŸºäºé—ä¼ ç¼–ç¨‹çš„è‡ªåŠ¨ç‰¹å¾æå–æ–¹æ³•ç”¨äºå›¾åƒåˆ†ç±»ã€‚è§äº
    *Proc. Int. Conf. Appl. Evol. Comput.* 421â€“438.
- en: Cano etÂ al. (2017) Alberto Cano, SebastiÃ¡n Ventura, and KrzysztofÂ J. Cios. 2017.
    Multi-Objective Genetic Programming for Feature Extraction and Data Visualization.
    *Soft Comput.* 21, 8 (2017), 2069â€“2089.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cano et al. (2017) Alberto Cano, SebastiÃ¡n Ventura, å’Œ Krzysztof J. Cios. 2017.
    å¤šç›®æ ‡é—ä¼ ç¼–ç¨‹ç”¨äºç‰¹å¾æå–å’Œæ•°æ®å¯è§†åŒ–ã€‚*Soft Comput.* 21, 8 (2017), 2069â€“2089.
- en: Castelli etÂ al. (2011) Mauro Castelli, Luca Manzoni, and Leonardo Vanneschi.
    2011. Multi Objective Genetic Programming for Feature Construction in Classification
    Problems. In *Proc. Int. Conf. Learn. Intell. Optim.* 503â€“506.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castelli et al. (2011) Mauro Castelli, Luca Manzoni, å’Œ Leonardo Vanneschi. 2011.
    å¤šç›®æ ‡é—ä¼ ç¼–ç¨‹ç”¨äºåˆ†ç±»é—®é¢˜ä¸­çš„ç‰¹å¾æ„é€ ã€‚è§äº *Proc. Int. Conf. Learn. Intell. Optim.* 503â€“506.
- en: Chai etÂ al. (2022) Zheng-Yi Chai, ChuanHua Yang, and Ya-Lun Li. 2022. Communication
    Efficiency Optimization in Federated Learning Based on Multi-Objective Evolutionary
    Algorithm. *Evol. Intell.* (2022), 1â€“12.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai et al. (2022) Zheng-Yi Chai, ChuanHua Yang, å’Œ Ya-Lun Li. 2022. åŸºäºå¤šç›®æ ‡è¿›åŒ–ç®—æ³•çš„è”é‚¦å­¦ä¹ é€šä¿¡æ•ˆç‡ä¼˜åŒ–ã€‚*Evol.
    Intell.* (2022), 1â€“12.
- en: Chandra (2015) Rohitash Chandra. 2015. Competition and Collaboration in Cooperative
    Coevolution of Elman Recurrent Neural Networks for Time-Series Prediction. *IEEE
    Trans. Neural Netw. Learn. Syst.* 26, 12 (2015), 3123â€“3136.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandra (2015) Rohitash Chandra. 2015. åˆä½œè¿›åŒ–ä¸­çš„ç«äº‰ä¸åˆä½œï¼šç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„ Elman é€’å½’ç¥ç»ç½‘ç»œã€‚*IEEE
    Trans. Neural Netw. Learn. Syst.* 26, 12 (2015), 3123â€“3136ã€‚
- en: Chandra and Zhang (2012) Rohitash Chandra and Mengjie Zhang. 2012. Cooperative
    Coevolution of Elman Recurrent Neural Networks for Chaotic Time Series Prediction.
    *Neurocomputing* 86 (2012), 116â€“123.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandra and Zhang (2012) Rohitash Chandra å’Œ Mengjie Zhang. 2012. ç”¨äºæ··æ²Œæ—¶é—´åºåˆ—é¢„æµ‹çš„
    Elman é€’å½’ç¥ç»ç½‘ç»œçš„åˆä½œè¿›åŒ–ã€‚*Neurocomputing* 86 (2012), 116â€“123ã€‚
- en: Chen etÂ al. (2022) Ke Chen, Bing Xue, Mengjie Zhang, and Fengyu Zhou. 2022.
    Evolutionary Multitasking for Feature Selection in High-Dimensional Classification
    via Particle Swarm Optimization. *IEEE Trans. Evol. Comput.* 26, 3 (2022), 446â€“460.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2022) Ke Chen, Bing Xue, Mengjie Zhang, å’Œ Fengyu Zhou. 2022. åŸºäºç²’å­ç¾¤ä¼˜åŒ–çš„é«˜ç»´åˆ†ç±»ç‰¹å¾é€‰æ‹©çš„è¿›åŒ–å¤šä»»åŠ¡å¤„ç†ã€‚*IEEE
    Trans. Evol. Comput.* 26, 3 (2022), 446â€“460ã€‚
- en: Chen etÂ al. (2015) Qi Chen, Bing Xue, and Mengjie Zhang. 2015. Generalisation
    and Domain Adaptation in GP with Gradient Descent for Symbolic Regression. In
    *Proc. IEEE Congr. Evol. Comput.* 1137â€“1144.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2015) Qi Chen, Bing Xue, å’Œ Mengjie Zhang. 2015. ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„ GP ä¸­çš„æ³›åŒ–å’Œé¢†åŸŸé€‚åº”ã€‚å‘è¡¨äº
    *Proc. IEEE Congr. Evol. Comput.* 1137â€“1144ã€‚
- en: Chen etÂ al. (2019a) Shuxin Chen, Lin Lin, Zixun Zhang, and Mitsuo Gen. 2019a.
    Evolutionary NetArchitecture Search for Deep Neural Networks Pruning. In *Proc.
    Aust. Conf. Artif. Intell.* 189â€“196.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019a) Shuxin Chen, Lin Lin, Zixun Zhang, å’Œ Mitsuo Gen. 2019a.
    ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œå‰ªæçš„è¿›åŒ–ç½‘ç»œæ¶æ„æœç´¢ã€‚å‘è¡¨äº *Proc. Aust. Conf. Artif. Intell.* 189â€“196ã€‚
- en: Chen etÂ al. (2020) Xiangru Chen, Yanan Sun, Mengjie Zhang, and Dezhong Peng.
    2020. Evolving Deep Convolutional Variational Autoencoders for Image Classification.
    *IEEE Trans. Evol. Comput.* 25, 5 (2020), 815â€“829.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Xiangru Chen, Yanan Sun, Mengjie Zhang, å’Œ Dezhong Peng. 2020.
    ä¸ºå›¾åƒåˆ†ç±»è¿›åŒ–æ·±åº¦å·ç§¯å˜åˆ†è‡ªç¼–ç å™¨ã€‚*IEEE Trans. Evol. Comput.* 25, 5 (2020), 815â€“829ã€‚
- en: 'Chen etÂ al. (2019b) Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, and
    Chang Huang. 2019b. RENAS: Reinforced Evolutionary Neural Architecture Search.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 4787â€“4796.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019b) Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, å’Œ
    Chang Huang. 2019b. RENASï¼šå¼ºåŒ–è¿›åŒ–ç¥ç»æ¶æ„æœç´¢ã€‚å‘è¡¨äº *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.* 4787â€“4796ã€‚
- en: Cheng etÂ al. (2021) Fan Cheng, Feixiang Chu, Yi Xu, and Lei Zhang. 2021. A Steering-Matrix-Based
    Multiobjective Evolutionary Algorithm for High-Dimensional Feature Selection.
    *IEEE Trans. Cybern.* 52, 9 (2021), 9695â€“9708.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2021) Fan Cheng, Feixiang Chu, Yi Xu, å’Œ Lei Zhang. 2021. åŸºäºå¼•å¯¼çŸ©é˜µçš„å¤šç›®æ ‡è¿›åŒ–ç®—æ³•ç”¨äºé«˜ç»´ç‰¹å¾é€‰æ‹©ã€‚*IEEE
    Trans. Cybern.* 52, 9 (2021), 9695â€“9708ã€‚
- en: Cheng etÂ al. (2017) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A Survey
    of Model Compression and Acceleration for Deep Neural networks. *arXiv preprint
    arXiv:1710.09282* (2017).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, å’Œ Tao Zhang. 2017. æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿè°ƒæŸ¥ã€‚*arXiv
    preprint arXiv:1710.09282* (2017)ã€‚
- en: Chiba etÂ al. (2019) Zouhair Chiba, Noreddine Abghour, Khalid Moussaid, AminaÂ El
    Omri, and Mohamed Rida. 2019. Intelligent Approach to Build a Deep Neural Network
    Based IDS for Cloud Environment Using Combination of Machine Learning Algorithms.
    *Comput. & Sec.* 86 (2019), 291â€“317.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiba et al. (2019) Zouhair Chiba, Noreddine Abghour, Khalid Moussaid, Amina
    El Omri, å’Œ Mohamed Rida. 2019. åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„äº‘ç¯å¢ƒå…¥ä¾µæ£€æµ‹ç³»ç»Ÿçš„æ™ºèƒ½æ„å»ºæ–¹æ³•ï¼Œç»“åˆäº†æœºå™¨å­¦ä¹ ç®—æ³•ã€‚*Comput. &
    Sec.* 86 (2019), 291â€“317ã€‚
- en: Choudhary etÂ al. (2020) Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and
    Jagannathan Sarangapani. 2020. A Comprehensive Survey on Model Compression and
    Acceleration. *Artif. Intell. Rev.* 53, 7 (2020), 5113â€“5155.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choudhary et al. (2020) Tejalal Choudhary, Vipul Mishra, Anurag Goswami, å’Œ Jagannathan
    Sarangapani. 2020. å…³äºæ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿçš„å…¨é¢è°ƒæŸ¥ã€‚*Artif. Intell. Rev.* 53, 7 (2020), 5113â€“5155ã€‚
- en: Chrabaszcz etÂ al. (2017) Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
    2017. A Downsampled variant of imageNet as an alternative to the Cifar datasets.
    *arXiv preprint arXiv:1707.08819* (2017).
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chrabaszcz et al. (2017) Patryk Chrabaszcz, Ilya Loshchilov, å’Œ Frank Hutter.
    2017. ä½œä¸º Cifar æ•°æ®é›†æ›¿ä»£æ–¹æ¡ˆçš„ä¸‹é‡‡æ ·ç‰ˆ imageNetã€‚*arXiv preprint arXiv:1707.08819* (2017)ã€‚
- en: Chu etÂ al. (2020) Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Hailong Ma. 2020.
    Multi-Objective Reinforced Evolution in Mobile Neural Architecture Search. In
    *Proc. Eur. Conf. Comput. Vis.* 99â€“113.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu et al. (2020) Xiangxiang Chu, Bo Zhang, Ruijun Xu, å’Œ Hailong Ma. 2020. ç§»åŠ¨ç¥ç»æ¶æ„æœç´¢ä¸­çš„å¤šç›®æ ‡å¼ºåŒ–è¿›åŒ–ã€‚å‘è¡¨äº
    *Proc. Eur. Conf. Comput. Vis.* 99â€“113ã€‚
- en: Conti etÂ al. (2018) Edoardo Conti, Vashisht Madhavan, FelipeÂ Petroski Such,
    Joel Lehman, KennethÂ O. Stanley, and Jeff Clune. 2018. Improving Exploration in
    Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking
    Agents. In *Proc. Adv. Neural Inf. Process. Syst.*, Vol.Â 31\. 5032â€“5043.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conti ç­‰ï¼ˆ2018ï¼‰Edoardo Contiã€Vashisht Madhavanã€Felipe Petroski Suchã€Joel Lehmanã€Kenneth
    O. Stanley å’Œ Jeff Cluneã€‚2018ã€‚é€šè¿‡ä¸€ç¾¤å¯»æ±‚æ–°å¥‡çš„ä»£ç†æ”¹è¿›æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è¿›åŒ–ç­–ç•¥æ¢ç´¢ã€‚åœ¨ *Proc. Adv. Neural Inf.
    Process. Syst.*ï¼Œç¬¬ 31 å·ï¼Œ5032â€“5043ã€‚
- en: Cui etÂ al. (2018) Xiaodong Cui, Wei Zhang, ZoltÃ¡n TÃ¼ske, and Michael Picheny.
    2018. Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural
    Networks. *Proc. Adv. Neural Inf. Process. Syst.* 31 (2018), 6051â€“6061.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui ç­‰ï¼ˆ2018ï¼‰Xiaodong Cuiã€Wei Zhangã€ZoltÃ¡n TÃ¼ske å’Œ Michael Pichenyã€‚2018ã€‚ç”¨äºä¼˜åŒ–æ·±åº¦ç¥ç»ç½‘ç»œçš„è¿›åŒ–éšæœºæ¢¯åº¦ä¸‹é™ã€‚*Proc.
    Adv. Neural Inf. Process. Syst.* 31ï¼ˆ2018ï¼‰ï¼Œ6051â€“6061ã€‚
- en: DaÂ Silva and Neto (2011) SÃ©rgioÂ Francisco DaÂ Silva and JoÃ£o do ESÂ Batista Neto.
    2011. Improving The Ranking Quality of Medical Image Retrieval Using A Genetic
    Feature Selection Method. *Decis. Support. Syst.* 51, 4 (2011), 810â€“820.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Da Silva å’Œ Netoï¼ˆ2011ï¼‰SÃ©rgio Francisco Da Silva å’Œ JoÃ£o do ES Batista Netoã€‚2011ã€‚ä½¿ç”¨é—ä¼ ç‰¹å¾é€‰æ‹©æ–¹æ³•æé«˜åŒ»å­¦å›¾åƒæ£€ç´¢çš„æ’åè´¨é‡ã€‚*Decis.
    Support. Syst.* 51, 4ï¼ˆ2011ï¼‰ï¼Œ810â€“820ã€‚
- en: Dahal and Zhan (2020) Binay Dahal and JustinÂ Zhijun Zhan. 2020. Effective Mutation
    and Recombination for Evolving Convolutional Networks. In *Proc. Adv. Neural Inf.
    Process. Syst.* 1â€“6.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dahal å’Œ Zhanï¼ˆ2020ï¼‰Binay Dahal å’Œ Justin Zhijun Zhanã€‚2020ã€‚æœ‰æ•ˆçš„çªå˜å’Œé‡ç»„ä»¥æ¼”åŒ–å·ç§¯ç½‘ç»œã€‚åœ¨ *Proc.
    Adv. Neural Inf. Process. Syst.* 1â€“6ã€‚
- en: 'Dai etÂ al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, JaimeÂ G Carbonell,
    Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models
    beyond a Fixed-Length Context. In *Proc. Assoc. Comput. Linguist.* 2978â€“2988.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai ç­‰ï¼ˆ2019ï¼‰Zihang Daiã€Zhilin Yangã€Yiming Yangã€Jaime G Carbonellã€Quoc Le å’Œ Ruslan
    Salakhutdinovã€‚2019ã€‚Transformer-XLï¼šè¶…è¶Šå›ºå®šé•¿åº¦ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›è¯­è¨€æ¨¡å‹ã€‚åœ¨ *Proc. Assoc. Comput. Linguist.*
    2978â€“2988ã€‚
- en: Dâ€™Ambrosio and Stanley (2007) DavidÂ B. Dâ€™Ambrosio and KennethÂ O. Stanley. 2007.
    A Novel Generative Encoding for Exploiting Neural Network Sensor and Output Geometry.
    In *Proc. Genetic Evol. Comput. Conf.* 974â€“981.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dâ€™Ambrosio å’Œ Stanleyï¼ˆ2007ï¼‰David B. Dâ€™Ambrosio å’Œ Kenneth O. Stanleyã€‚2007ã€‚ç”¨äºåˆ©ç”¨ç¥ç»ç½‘ç»œä¼ æ„Ÿå™¨å’Œè¾“å‡ºå‡ ä½•çš„æ–°å‹ç”Ÿæˆç¼–ç ã€‚åœ¨
    *Proc. Genetic Evol. Comput. Conf.* 974â€“981ã€‚
- en: Darwish etÂ al. (2020) Ashraf Darwish, AboulÂ Ella Hassanien, and Swagatam Das.
    2020. A Survey of Swarm And Evolutionary Computing Approaches for Deep Learning.
    *Artif. Intell. Rev.* 53, 3 (2020), 1767â€“1812.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Darwish ç­‰ï¼ˆ2020ï¼‰Ashraf Darwishã€Aboul Ella Hassanien å’Œ Swagatam Dasã€‚2020ã€‚æ·±åº¦å­¦ä¹ çš„ç¾¤ä½“å’Œè¿›åŒ–è®¡ç®—æ–¹æ³•è°ƒæŸ¥ã€‚*Artif.
    Intell. Rev.* 53, 3ï¼ˆ2020ï¼‰ï¼Œ1767â€“1812ã€‚
- en: 'Deb etÂ al. (2002) Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and T. Meyarivan.
    2002. A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II. *IEEE Trans.
    Evol. Comput.* 6, 2 (2002), 182â€“197.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deb ç­‰ï¼ˆ2002ï¼‰Kalyanmoy Debã€Samir Agrawalã€Amrit Pratap å’Œ T. Meyarivanã€‚2002ã€‚ä¸€ä¸ªå¿«é€Ÿä¸”ç²¾è‹±çš„å¤šç›®æ ‡é—ä¼ ç®—æ³•ï¼šNSGA-IIã€‚*IEEE
    Trans. Evol. Comput.* 6, 2ï¼ˆ2002ï¼‰ï¼Œ182â€“197ã€‚
- en: Demirkir and Sankur (2006) Cem Demirkir and BÃ¼lent Sankur. 2006. Object Detection
    Using Haar Feature Selection Optimization. In *Proc. IEEE Signal Process. Commun.
    Appl.* 1â€“4.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demirkir å’Œ Sankurï¼ˆ2006ï¼‰Cem Demirkir å’Œ BÃ¼lent Sankurã€‚2006ã€‚ä½¿ç”¨ Haar ç‰¹å¾é€‰æ‹©ä¼˜åŒ–è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚åœ¨
    *Proc. IEEE Signal Process. Commun. Appl.* 1â€“4ã€‚
- en: 'Deng etÂ al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and
    Li Fei-Fei. 2009. ImageNet: A Large-scale Hierarchical Image Database. *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.* (2009), 248â€“255.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng ç­‰ï¼ˆ2009ï¼‰Jia Dengã€Wei Dongã€Richard Socherã€Li-Jia Liã€K. Li å’Œ Li Fei-Feiã€‚2009ã€‚ImageNetï¼šä¸€ä¸ªå¤§è§„æ¨¡å±‚æ¬¡åŒ–å›¾åƒæ•°æ®åº“ã€‚*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*ï¼ˆ2009ï¼‰ï¼Œ248â€“255ã€‚
- en: 'Devlin etÂ al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin ç­‰ï¼ˆ2018ï¼‰Jacob Devlinã€Ming-Wei Changã€Kenton Lee å’Œ Kristina Toutanovaã€‚2018ã€‚Bertï¼šç”¨äºè¯­è¨€ç†è§£çš„æ·±åº¦åŒå‘
    Transformer é¢„è®­ç»ƒã€‚*arXiv preprint arXiv:1810.04805*ï¼ˆ2018ï¼‰ã€‚
- en: DeVries and Taylor (2017) Terrance DeVries and GrahamÂ W Taylor. 2017. Improved
    Regularization of Convolutional Neural Networks with Cutout. *arXiv preprint arXiv:1708.04552*
    (2017).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeVries å’Œ Taylorï¼ˆ2017ï¼‰Terrance DeVries å’Œ Graham W Taylorã€‚2017ã€‚é€šè¿‡ Cutout æ”¹è¿›å·ç§¯ç¥ç»ç½‘ç»œçš„æ­£åˆ™åŒ–ã€‚*arXiv
    preprint arXiv:1708.04552*ï¼ˆ2017ï¼‰ã€‚
- en: 'Dong and Yang (2020) Xuanyi Dong and Yi Yang. 2020. NAS-Bench-201: Extending
    the Scope of Reproducible Neural Architecture Search. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/2001.00326.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong å’Œ Yangï¼ˆ2020ï¼‰Xuanyi Dong å’Œ Yi Yangã€‚2020ã€‚NAS-Bench-201ï¼šæ‰©å±•å¯é‡å¤ç¥ç»æ¶æ„æœç´¢çš„èŒƒå›´ã€‚åœ¨ *Proc.
    Int. Conf. Learn. Represent.* https://arxiv.org/abs/2001.00326ã€‚
- en: Dowdell and Zhang (2020) Thomas Dowdell and Hongyu Zhang. 2020. Language Modelling
    for Source Code with Transformer-XL. *arXiv preprint arXiv:2007.15813* (2020).
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dowdell å’Œ Zhangï¼ˆ2020ï¼‰Thomas Dowdell å’Œ Hongyu Zhangã€‚2020ã€‚ä½¿ç”¨ Transformer-XL çš„æºä»£ç è¯­è¨€å»ºæ¨¡ã€‚*arXiv
    preprint arXiv:2007.15813*ï¼ˆ2020ï¼‰ã€‚
- en: Elsken etÂ al. (2017) Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. 2017.
    Simple and Efficient Architecture Search for Convolutional Neural Networks. *arXiv
    preprint arXiv:1711.04528* (2017).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Elsken etÂ al. (2017)](https://arxiv.org/abs/1711.04528) Thomas Elsken, Jan-Hendrik
    Metzen, and Frank Hutter. 2017. å·ç§¯ç¥ç»ç½‘ç»œçš„ç®€å•é«˜æ•ˆæ¶æ„æœç´¢ã€‚*arXiv é¢„å°æœ¬ arXiv:1711.04528* (2017)ã€‚'
- en: Elsken etÂ al. (2019) Thomas Elsken, JanÂ Hendrik Metzen, and Frank Hutter. 2019.
    Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution.
    In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1804.09081.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Elsken etÂ al. (2019)](https://arxiv.org/abs/1804.09081) Thomas Elsken, JanÂ Hendrik
    Metzen, å’ŒFrank Hutter. 2019. é€šè¿‡æ‹‰é©¬å…‹è¿›åŒ–å®ç°çš„é«˜æ•ˆå¤šç›®æ ‡ç¥ç»æ¶æ„æœç´¢ã€‚åœ¨*å­¦ä¹ ä»£è¡¨å›½é™…ä¼šè®®*ã€‚https://arxiv.org/abs/1804.09081ã€‚'
- en: Erguzel etÂ al. (2014) TurkerÂ Tekin Erguzel, Serhat Ozekes, Selahattin Gultekin,
    and Nevzat Tarhan. 2014. Ant Colony optimization Based Feature Selection Method
    for QEEG data classification. *Psychiatry Investig.* 11, 3 (2014), 243.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Erguzel etÂ al. (2014)](https://pubmed.ncbi.nlm.nih.gov/25071325/) TurkerÂ Tekin
    Erguzel, Serhat Ozekes, Selahattin Gultekin, å’ŒNevzat Tarhan. 2014. ç”¨äº QEEG æ•°æ®åˆ†ç±»çš„èšç¾¤ä¼˜åŒ–çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚*Psychiatry
    Investig.* 11, 3 (2014), 243ã€‚'
- en: EstÃ©vez and Caballero (1998) PabloÂ A EstÃ©vez and RodrigoÂ E Caballero. 1998.
    A Niching Genetic Algorithm for Selecting Features for Neural Network Classifiers.
    In *Proc. Int. Conf. Artif. Neural Netw.* 311â€“316.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[EstÃ©vez and Caballero (1998)](https://link.springer.com/chapter/10.1007/3-540-49435-4_19)
    PabloÂ A EstÃ©vez and RodrigoÂ E Caballero. 1998. ç”¨äºç¥ç»ç½‘ç»œåˆ†ç±»å™¨ç‰¹å¾é€‰æ‹©çš„åˆ†çº§é—ä¼ ç®—æ³•ã€‚åœ¨*å›½é™…äººå·¥ç¥ç»ç½‘ç»œä¼šè®®*ã€‚311â€“316ã€‚'
- en: 'Evans etÂ al. (2018a) Benjamin Evans, Harith Al-Sahaf, Bing Xue, and Mengjie
    Zhang. 2018a. Evolutionary Deep Learning: A Genetic Programming Approach to Image
    Classification. In *Proc. IEEE Congr. Evol. Comput.* 1538â€“1545.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Evans etÂ al. (2018a)](https://ieeexplore.ieee.org/document/8477923) Benjamin
    Evans, Harith Al-Sahaf, Bing Xue, å’ŒMengjie Zhang. 2018a. è¿›åŒ–æ·±åº¦å­¦ä¹ ï¼šä¸€ç§åŸºäºé—ä¼ ç¼–ç¨‹çš„å›¾åƒåˆ†ç±»æ–¹æ³•ã€‚åœ¨*IEEE
    Evol. Comput. å­¦ä¼šè®®*ã€‚1538â€“1545ã€‚'
- en: 'Evans etÂ al. (2018b) BenjaminÂ Patrick Evans, Harith Al-Sahaf, Bing Xue, and
    Mengjie Zhang. 2018b. Evolutionary Deep Learning: A Genetic Programming Approach
    to Image Classification. In *Proc. IEEE Congr. Evol. Comput.* 1â€“6.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Evans etÂ al. (2018b)](https://ieeexplore.ieee.org/document/8479282) BenjaminÂ Patrick
    Evans, Harith Al-Sahaf, Bing Xue, and Mengjie Zhang. 2018b. è¿›åŒ–æ·±åº¦å­¦ä¹ ï¼šä¸€ç§åŸºäºé—ä¼ ç¼–ç¨‹çš„å›¾åƒåˆ†ç±»æ–¹æ³•ã€‚åœ¨*IEEE
    Evol. Comput. å­¦ä¼šè®®*ã€‚1â€“6ã€‚'
- en: Fahrudin etÂ al. (2016) TresnaÂ Maulana Fahrudin, Iwan Syarif, and AliÂ Ridho Barakbah.
    2016. Ant Colony Algorithm for Feature Selection on Microarray Datasets. In *International
    Electronics Symposium*. 351â€“356.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fahrudin etÂ al. (2016)](https://doi.org/10.1109/IESYSD.2016.7852844) TresnaÂ Maulana
    Fahrudin, Iwan Syarif, and AliÂ Ridho Barakbah. 2016. ç‰¹å¾é€‰æ‹©çš„èšç¾¤ç®—æ³•åœ¨å¾®é˜µåˆ—æ•°æ®é›†ä¸Šçš„åº”ç”¨ã€‚åœ¨*å›½é™…ç”µå­å­¦ç ”è®¨ä¼š*ã€‚351â€“356ã€‚'
- en: Fan etÂ al. (2020) Zhun Fan, Jiahong Wei, Guijie Zhu, Jiajie Mo, and Wenji Li.
    2020. Evolutionary Neural Architecture Search for Retinal Vessel Segmentation.
    *arXiv preprint arXiv:2001.06678* (2020).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fan etÂ al. (2020)](https://arxiv.org/abs/2001.06678) Zhun Fan, Jiahong Wei,
    Guijie Zhu, Jiajie Mo, and Wenji Li. 2020. ç”¨äºè§†ç½‘è†œè¡€ç®¡åˆ†å‰²çš„è¿›åŒ–ç¥ç»æ¶æ„æœç´¢ã€‚*arXiv é¢„å°æœ¬ arXiv:2001.06678*
    (2020)ã€‚'
- en: Fernandes and Yen (2021) FranciscoÂ Erivaldo Fernandes and GaryÂ G. Yen. 2021.
    Automatic Searching and Pruning of Deep Neural Networks for Medical Imaging Diagnostic.
    *IEEE Trans. Neural Netw. Learn. Syst.* 32, 12 (2021), 5664â€“5674.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fernandes and Yen (2021)](https://doi.org/10.1109/TNNLS.2021.3090053) FranciscoÂ Erivaldo
    Fernandes and GaryÂ G. Yen. 2021. åŒ»å­¦å½±åƒè¯Šæ–­çš„æ·±åº¦ç¥ç»ç½‘ç»œçš„è‡ªåŠ¨æœç´¢å’Œä¿®å‰ªã€‚*IEEE Trans. Neural Netw.
    Learn. Syst.* 32, 12 (2021), 5664â€“5674ã€‚'
- en: Fogelberg and Zhang (2005) Christopher Fogelberg and Mengjie Zhang. 2005. Linear
    Genetic Programming for Multi-class Object Classification. In *Proc. Aust. Joint
    Conf. Artif.l Intell.* 369â€“379.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fogelberg and Zhang (2005)](https://doi.org/10.1007/11552569_36) Christopher
    Fogelberg and Mengjie Zhang. 2005. å¤šç±»ç‰©ä½“åˆ†ç±»çš„çº¿æ€§é—ä¼ ç¼–ç¨‹ã€‚åœ¨*æ¾³å¤§åˆ©äºšäººå·¥æ™ºèƒ½è”åˆä¼šè®®*ã€‚369â€“379ã€‚'
- en: Fortuna and Frasca (2021) Luigi Fortuna and Mattia Frasca. 2021. Singular Value
    Decomposition. *Optim. Rob. Control* (2021), 51â€“58.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fortuna and Frasca (2021)](https://doi.org/10.1080/01430708.2021.1902983)
    Luigi Fortuna and Mattia Frasca. 2021. å¥‡å¼‚å€¼åˆ†è§£ã€‚*Optim. Rob. æ§åˆ¶* (2021), 51â€“58ã€‚'
- en: 'Frachon etÂ al. (2019) Luc Frachon, Wei Pang, and GeorgeÂ M Coghill. 2019. Immunecs:
    Neural Committee Search by an Artificial Immune System. *arXiv preprint arXiv:1911.07729*
    (2019).'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Frachon etÂ al. (2019)](https://arxiv.org/abs/1911.07729) Luc Frachon, Wei
    Pang, and GeorgeÂ M Coghill. 2019. Immunecs: ç¥ç»å§”å‘˜ä¼šæœç´¢çš„äººå·¥å…ç–«ç³»ç»Ÿã€‚*arXiv é¢„å°æœ¬ arXiv:1911.07729*
    (2019)ã€‚'
- en: Freitas (2003) AlexÂ A Freitas. 2003. A Survey of Evolutionary Algorithms for
    Data Mining and Knowledge Discovery. In *Adv. Evol. Comput.* Springer, 819â€“845.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Freitas (2003)](https://doi.org/10.1007/978-3-540-39695-5_31) AlexÂ A Freitas.
    2003. ç”¨äºæ•°æ®æŒ–æ˜å’ŒçŸ¥è¯†å‘ç°çš„è¿›åŒ–ç®—æ³•ç»¼è¿°ã€‚åœ¨*Adv. Evol. Comput.* Springer, 819â€“845ã€‚'
- en: Fujino etÂ al. (2017) Saya Fujino, Naoki Mori, and Keinosuke Matsumoto. 2017.
    Deep Convolutional Networks for Human Sketches By Means of The Evolutionary Deep
    Learning. In *Proc. Int. Conf. Soft Comput. Intell. Syst.* 1â€“5.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fujino etÂ al. (2017)](https://doi.org/10.1109/SCIS-ISIS.2017.108) Saya Fujino,
    Naoki Mori, and Keinosuke Matsumoto. 2017. é€šè¿‡è¿›åŒ–æ·±åº¦å­¦ä¹ å®ç°äººç±»ç´ æçš„æ·±åº¦å·ç§¯ç½‘ç»œã€‚åœ¨*å›½é™…è½¯è®¡ç®—æ™ºèƒ½ç³»ç»Ÿä¼šè®®*ã€‚1â€“5ã€‚'
- en: GarcÃ­a etÂ al. (2011) David GarcÃ­a, AntonioÂ GonzÃ¡lez MuÃ±oz, and RaÃºl PÃ©rez. 2011.
    A Two-Step Approach of Feature Construction for A Genetic Learning Algorithm.
    *Proc. Int. Conf. Fuzzy Syst.* (2011), 1255â€“1262.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GarcÃ­a ç­‰ (2011) David GarcÃ­a, Antonio GonzÃ¡lez MuÃ±oz, å’Œ RaÃºl PÃ©rez. 2011. ä¸€ç§ç”¨äºé—ä¼ å­¦ä¹ ç®—æ³•çš„ç‰¹å¾æ„é€ çš„ä¸¤æ­¥æ³•ã€‚è§
    *Proc. Int. Conf. Fuzzy Syst.* (2011), 1255â€“1262ã€‚
- en: Gerum etÂ al. (2020) RichardÂ C Gerum, AndrÃ© Erpenbeck, Patrick Krauss, and Achim
    Schilling. 2020. Sparsity Through Evolutionary Pruning Prevents Neuronal Networks
    From Overfitting. *Neural Netw.* 128 (2020), 305â€“312.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerum ç­‰ (2020) Richard C Gerum, AndrÃ© Erpenbeck, Patrick Krauss, å’Œ Achim Schilling.
    2020. é€šè¿‡è¿›åŒ–å‰ªæå®ç°çš„ç¨€ç–æ€§é˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆã€‚*Neural Netw.* 128 (2020), 305â€“312ã€‚
- en: Golubski and Feuring (1999) Wolfgang Golubski and Thomas Feuring. 1999. Evolving
    Neural Network Structures by Means of Genetic Programming. In *Proc. Eur. Conf.
    Genetic Program*. 211â€“220.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golubski å’Œ Feuring (1999) Wolfgang Golubski å’Œ Thomas Feuring. 1999. é€šè¿‡é—ä¼ ç¼–ç¨‹æ¼”åŒ–ç¥ç»ç½‘ç»œç»“æ„ã€‚è§
    *Proc. Eur. Conf. Genetic Program*. 211â€“220ã€‚
- en: 'Gou etÂ al. (2021) Jianping Gou, Baosheng Yu, StephenÂ J Maybank, and Dacheng
    Tao. 2021. Knowledge Distillation: A Survey. *Int. J. Comput. Vis.* 129, 6 (2021),
    1789â€“1819.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou ç­‰ (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, å’Œ Dacheng Tao. 2021.
    çŸ¥è¯†è’¸é¦ï¼šç»¼è¿°ã€‚*Int. J. Comput. Vis.* 129, 6 (2021), 1789â€“1819ã€‚
- en: Guo etÂ al. (2020) Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu,
    Yichen Wei, and Jian Sun. 2020. Single Path One-Shot Neural Architecture Search
    with Uniform Sampling. In *Proc. Eur. Conf. Comput. Vis.* 544â€“560.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo ç­‰ (2020) Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen
    Wei, å’Œ Jian Sun. 2020. å•è·¯å¾„ä¸€æ¬¡æ€§ç¥ç»æ¶æ„æœç´¢ä¸å‡åŒ€é‡‡æ ·ã€‚è§ *Proc. Eur. Conf. Comput. Vis.* 544â€“560ã€‚
- en: 'Hajati etÂ al. (2010) Farshid Hajati, Caro Lucas, and Yongsheng Gao. 2010. Face
    Localization Using an Effective Co-evolutionary Genetic Algorithm. *Proc. Int.
    Conf. Digit. Image Comput.: Tech. and Appl.* (2010), 522â€“527.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hajati ç­‰ (2010) Farshid Hajati, Caro Lucas, å’Œ Yongsheng Gao. 2010. ä½¿ç”¨æœ‰æ•ˆçš„åè¿›åŒ–é—ä¼ ç®—æ³•è¿›è¡Œäººè„¸å®šä½ã€‚è§
    *Proc. Int. Conf. Digit. Image Comput.: Tech. and Appl.* (2010), 522â€“527ã€‚'
- en: Hammami etÂ al. (2018) Marwa Hammami, Slim Bechikh, and Chih-Cheng Hung. 2018.
    A Multi-Objective Hybrid Filter-Wrapper Evolutionary Approach for Feature Construction
    on High-Dimensional Data. In *Proc. IEEE Congr. Evol. Comput.* 1â€“8.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hammami ç­‰ (2018) Marwa Hammami, Slim Bechikh, å’Œ Chih-Cheng Hung. 2018. é¢å‘é«˜ç»´æ•°æ®çš„å¤šç›®æ ‡æ··åˆè¿‡æ»¤-åŒ…è£¹è¿›åŒ–æ–¹æ³•ç”¨äºç‰¹å¾æ„é€ ã€‚è§
    *Proc. IEEE Congr. Evol. Comput.* 1â€“8ã€‚
- en: Han and Cho (2006) Sang-Jun Han and Sung-Bae Cho. 2006. Evolutionary Neural
    Networks for Anomaly Detection Based on the Behavior of a Program. *IEEE Trans.
    Syst. Man Cybern.* 36, 3 (2006), 559â€“570.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han å’Œ Cho (2006) Sang-Jun Han å’Œ Sung-Bae Cho. 2006. åŸºäºç¨‹åºè¡Œä¸ºçš„å¼‚å¸¸æ£€æµ‹çš„è¿›åŒ–ç¥ç»ç½‘ç»œã€‚*IEEE
    Trans. Syst. Man Cybern.* 36, 3 (2006), 559â€“570ã€‚
- en: Hancer etÂ al. (2015) Emrah Hancer, Bing Xue, Mengjie Zhang, and Dervis Karaboga.
    2015. A Multi-objective Artificial Bee Colony Approach to Feature Selection Using
    Fuzzy Mutual Information. In *Proc. IEEE Congr. Evol. Comput.* 2420â€“2427.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hancer ç­‰ (2015) Emrah Hancer, Bing Xue, Mengjie Zhang, å’Œ Dervis Karaboga. 2015.
    ä½¿ç”¨æ¨¡ç³Šäº’ä¿¡æ¯çš„å¤šç›®æ ‡äººå·¥èœ‚ç¾¤ç®—æ³•è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚è§ *Proc. IEEE Congr. Evol. Comput.* 2420â€“2427ã€‚
- en: 'He etÂ al. (2021) Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A Survey
    of the State-of-the-Art. *Knowl-Based Syst* 212 (2021), 106622.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He ç­‰ (2021) Xin He, Kaiyong Zhao, å’Œ Xiaowen Chu. 2021. AutoML: å‰æ²¿æŠ€æœ¯è°ƒæŸ¥ã€‚*Knowl-Based
    Syst* 212 (2021), 106622ã€‚'
- en: He etÂ al. (2017) Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel Pruning
    for Accelerating Very Deep Neural Networks. In *Proc. IEEE Int. Conf. Comput.
    Vis.* 1398â€“1406.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He ç­‰ (2017) Yihui He, Xiangyu Zhang, å’Œ Jian Sun. 2017. åŠ é€Ÿéå¸¸æ·±å±‚ç¥ç»ç½‘ç»œçš„é€šé“å‰ªæã€‚è§ *Proc.
    IEEE Int. Conf. Comput. Vis.* 1398â€“1406ã€‚
- en: Ho etÂ al. (2021) Kary Ho, Andrew Gilbert, Hailin Jin, and JohnÂ P. Collomosse.
    2021. Neural Architecture Search for Deep Image Prior. *Comput. & Graph.* 98 (2021),
    188â€“196.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho ç­‰ (2021) Kary Ho, Andrew Gilbert, Hailin Jin, å’Œ John P. Collomosse. 2021.
    æ·±åº¦å›¾åƒå…ˆéªŒçš„ç¥ç»æ¶æ„æœç´¢ã€‚*Comput. & Graph.* 98 (2021), 188â€“196ã€‚
- en: Hong and Cho (2006) Jin-Hyuk Hong and Sung-Bae Cho. 2006. Efficient Huge-Scale
    Feature Selection With Speciated Genetic Algorithm. *Pattern Recognit. Lett.*
    27, 2 (2006), 143â€“150.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong å’Œ Cho (2006) Jin-Hyuk Hong å’Œ Sung-Bae Cho. 2006. ä½¿ç”¨ä¸“åŒ–é—ä¼ ç®—æ³•è¿›è¡Œé«˜æ•ˆçš„å¤§è§„æ¨¡ç‰¹å¾é€‰æ‹©ã€‚*Pattern
    Recognit. Lett.* 27, 2 (2006), 143â€“150ã€‚
- en: Hong etÂ al. (2020) Wenjing Hong, Peng Yang, Yiwen Wang, and Ke Tang. 2020. Multi-objective
    Magnitude-Based Pruning for Latency-Aware Deep Neural Network Compression. In
    *Proc. Int. Conf. on Parallel Probl. Solving Nat.* 470â€“483.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong ç­‰ (2020) Wenjing Hong, Peng Yang, Yiwen Wang, å’Œ Ke Tang. 2020. é¢å‘å»¶è¿Ÿæ„ŸçŸ¥çš„å¤šç›®æ ‡åŸºäºå¹…åº¦çš„å‰ªæç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œå‹ç¼©ã€‚è§
    *Proc. Int. Conf. on Parallel Probl. Solving Nat.* 470â€“483ã€‚
- en: Hosni etÂ al. (2020) Mohamed Hosni, GinÃ©s GarcÃ­a-Mateos, and Juan Carrillo-de
    Gea. 2020. A Mapping Study of Ensemble Classification Methods in Lung Cancer Decision
    Support Systems. *Med. & Biol. Eng. & Comput.* 58, 10 (2020), 2177â€“2193.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosni ç­‰äºº (2020) Mohamed Hosniã€GinÃ©s GarcÃ­a-Mateos å’Œ Juan Carrillo-de Geaã€‚2020å¹´ã€‚è‚ºç™Œå†³ç­–æ”¯æŒç³»ç»Ÿä¸­é›†æˆåˆ†ç±»æ–¹æ³•çš„æ˜ å°„ç ”ç©¶ã€‚*åŒ»å­¦ä¸ç”Ÿç‰©å·¥ç¨‹ä¸è®¡ç®—*
    58, 10 (2020), 2177â€“2193ã€‚
- en: Hsu etÂ al. (2021) Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen,
    and Hongxia Jin. 2021. Language Model Compression with Weighted Low-rank Factorization.
    In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/2207.00112.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsu ç­‰äºº (2021) Yen-Chang Hsuã€Ting Huaã€Sungen Changã€Qian Louã€Yilin Shen å’Œ Hongxia
    Jinã€‚2021å¹´ã€‚åŸºäºåŠ æƒä½ç§©åˆ†è§£çš„è¯­è¨€æ¨¡å‹å‹ç¼©ã€‚åœ¨ *å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®è®ºæ–‡é›†* https://arxiv.org/abs/2207.00112ã€‚
- en: 'Hu etÂ al. (2021b) Bin Hu, Tianming Zhao, Yucheng Xie, Yan Wang, and Xiaonan
    Guo. 2021b. MIXP: Efficient Deep Neural Networks Pruning for Further FLOPs Compression
    via Neuron Bond. In *Proc. Int. Joint Conf. Neural Netw.* 1â€“8.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu ç­‰äºº (2021b) Bin Huã€Tianming Zhaoã€Yucheng Xieã€Yan Wang å’Œ Xiaonan Guoã€‚2021bã€‚MIXPï¼šé€šè¿‡ç¥ç»å…ƒç»‘å®šè¿›ä¸€æ­¥å‹ç¼©
    FLOPs çš„é«˜æ•ˆæ·±åº¦ç¥ç»ç½‘ç»œå‰ªæã€‚åœ¨ *å›½é™…è”åˆç¥ç»ç½‘ç»œä¼šè®®è®ºæ–‡é›†* 1â€“8ã€‚
- en: Hu etÂ al. (2021a) Yiming Hu, Xingang Wang, Lujun Li, and Qingyi Gu. 2021a. Improving
    One-Shot NAS with Shrinking-and-Expanding Supernet. *Pattern Recognit.* 118 (2021),
    108025.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu ç­‰äºº (2021a) Yiming Huã€Xingang Wangã€Lujun Li å’Œ Qingyi Guã€‚2021aã€‚é€šè¿‡æ”¶ç¼©ä¸æ‰©å±•è¶…çº§ç½‘ç»œæ”¹è¿›å•æ¬¡ç¥ç»æ¶æ„æœç´¢ã€‚*æ¨¡å¼è¯†åˆ«*
    118 (2021), 108025ã€‚
- en: Huang etÂ al. (2012) Hu Huang, Hong-Bo Xie, Jing-Yi Guo, and Hui-Juan Chen. 2012.
    Ant Colony Optimization-based Feature Selection Method for Surface Electromyography
    Signals Classification. *Comput. Biol. Med.* 42, 1 (2012), 30â€“38.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang ç­‰äºº (2012) Hu Huangã€Hong-Bo Xieã€Jing-Yi Guo å’Œ Hui-Juan Chenã€‚2012å¹´ã€‚åŸºäºèšç¾¤ä¼˜åŒ–çš„è¡¨é¢è‚Œç”µä¿¡å·åˆ†ç±»ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚*è®¡ç®—ç”Ÿç‰©åŒ»å­¦*
    42, 1 (2012), 30â€“38ã€‚
- en: Huang etÂ al. (2020) Junhao Huang, Weize Sun, and Lei Huang. 2020. Deep Neural
    Networks Compression Learning Based on Multiobjective Evolutionary Algorithms.
    *Neurocomputing* 378 (2020), 260â€“269.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang ç­‰äºº (2020) Junhao Huangã€Weize Sun å’Œ Lei Huangã€‚2020å¹´ã€‚åŸºäºå¤šç›®æ ‡è¿›åŒ–ç®—æ³•çš„æ·±åº¦ç¥ç»ç½‘ç»œå‹ç¼©å­¦ä¹ ã€‚*ç¥ç»è®¡ç®—*
    378 (2020), 260â€“269ã€‚
- en: Ijjina and Chalavadi (2016) EarnestÂ Paul Ijjina and KrishnaÂ Mohan Chalavadi.
    2016. Human Action Recognition Using Genetic Algorithms and Convolutional Neural
    Networks. *Pattern Recognit.* 59 (2016), 199â€“212.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ijjina å’Œ Chalavadi (2016) Earnest Paul Ijjina å’Œ Krishna Mohan Chalavadiã€‚2016å¹´ã€‚ä½¿ç”¨é—ä¼ ç®—æ³•å’Œå·ç§¯ç¥ç»ç½‘ç»œçš„äººä½“åŠ¨ä½œè¯†åˆ«ã€‚*æ¨¡å¼è¯†åˆ«*
    59 (2016), 199â€“212ã€‚
- en: Irwin-Harris etÂ al. (2019) William Irwin-Harris, Yanan Sun, Bing Xue, and Mengjie
    Zhang. 2019. A Graph-Based Encoding for Evolutionary Convolutional Neural Network
    Architecture Design. In *Proc. IEEE Congr. Evol. Comput.* 546â€“553.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Irwin-Harris ç­‰äºº (2019) William Irwin-Harrisã€Yanan Sunã€Bing Xue å’Œ Mengjie Zhangã€‚2019å¹´ã€‚ç”¨äºæ¼”åŒ–å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡çš„å›¾ç¼–ç ã€‚
    åœ¨ *IEEE è¿›åŒ–è®¡ç®—å¤§ä¼šè®ºæ–‡é›†* 546â€“553ã€‚
- en: Izenman (2013) AlanÂ Julian Izenman. 2013. Linear Discriminant Analysis. In *Modern
    Multivariate Statistical Techniques*. Springer, 237â€“280.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izenman (2013) Alan Julian Izenmanã€‚2013å¹´ã€‚çº¿æ€§åˆ¤åˆ«åˆ†æã€‚åœ¨ *ç°ä»£å¤šå˜é‡ç»Ÿè®¡æŠ€æœ¯*ã€‚Springerï¼Œ237â€“280ã€‚
- en: 'JaÃ¢fra etÂ al. (2019) Yesmina JaÃ¢fra, JeanÂ Luc Laurent, Aline Deruyver, and
    MohamedÂ Saber Naceur. 2019. Reinforcement Learning for Neural Architecture Search:
    A Review. *Image Vis. Comput.* 89 (2019), 57â€“66.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JaÃ¢fra ç­‰äºº (2019) Yesmina JaÃ¢fraã€Jean Luc Laurentã€Aline Deruyver å’Œ Mohamed Saber
    Naceurã€‚2019å¹´ã€‚ç¥ç»æ¶æ„æœç´¢çš„å¼ºåŒ–å­¦ä¹ ï¼šç»¼è¿°ã€‚*å›¾åƒè§†è§‰è®¡ç®—* 89 (2019), 57â€“66ã€‚
- en: 'Jin etÂ al. (2018) Haifeng Jin, Qingquan Song, and Xia Hu. 2018. Auto-keras:
    Efficient Neural Architecture Search with Network Morphism. *arXiv preprint arXiv:1806.10282*
    (2018).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin ç­‰äºº (2018) Haifeng Jinã€Qingquan Song å’Œ Xia Huã€‚2018å¹´ã€‚Auto-kerasï¼šå…·æœ‰ç½‘ç»œå½¢æ€å­¦çš„é«˜æ•ˆç¥ç»æ¶æ„æœç´¢ã€‚*arXiv
    é¢„å°æœ¬ arXiv:1806.10282* (2018)ã€‚
- en: 'Jin etÂ al. (2019) Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-Keras:
    An Efficient Neural Architecture Search System. In *Proc. ACM SIGKDD Int. Conf.
    on Knowl. Discov. & Data Min.* 1946â€“1956.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin ç­‰äºº (2019) Haifeng Jinã€Qingquan Song å’Œ Xia Huã€‚2019å¹´ã€‚Auto-Kerasï¼šé«˜æ•ˆçš„ç¥ç»æ¶æ„æœç´¢ç³»ç»Ÿã€‚åœ¨
    *ACM SIGKDD å›½é™…ä¼šè®®è®ºæ–‡é›†* 1946â€“1956ã€‚
- en: Jin (2006) Yaochu Jin. 2006. *Multi-Objective Machine Learning*. Springer Science.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin (2006) Yaochu Jinã€‚2006å¹´ã€‚*å¤šç›®æ ‡æœºå™¨å­¦ä¹ *ã€‚Springer Scienceã€‚
- en: Jones etÂ al. (2019) DavidÂ T Jones, Anja Schroeder, and GeoffÂ S. Nitschke. 2019.
    Evolutionary Deep Learning to Identify Galaxies in the Zone of Avoidance. *arXiv
    preprint arXiv:1903.07461* (2019).
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jones ç­‰äºº (2019) David T Jonesã€Anja Schroeder å’Œ Geoff S. Nitschkeã€‚2019å¹´ã€‚æ¼”åŒ–æ·±åº¦å­¦ä¹ ç”¨äºè¯†åˆ«å›é¿åŒºçš„æ˜Ÿç³»ã€‚*arXiv
    é¢„å°æœ¬ arXiv:1903.07461* (2019)ã€‚
- en: Junior and Yen (2021a) Francisco ErivaldoÂ Fernandes Junior and GaryÂ G. Yen.
    2021a. Pruning Deep Convolutional Neural Networks Architectures with Evolution
    Strategy. *Inf. Sci.* 552 (2021), 29â€“47.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Junior å’Œ Yen (2021a) Francisco Erivaldo Fernandes Junior å’Œ Gary G. Yenã€‚2021aã€‚ä½¿ç”¨è¿›åŒ–ç­–ç•¥ä¿®å‰ªæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ã€‚*ä¿¡æ¯ç§‘å­¦*
    552 (2021), 29â€“47ã€‚
- en: Junior and Yen (2021b) Francisco ErivaldoÂ Fernandes Junior and GaryÂ G. Yen.
    2021b. Pruning of Generative Adversarial Neural Networks for Medical Imaging Diagnostics
    with Evolution Strategy. *Inf. Sci.* 558 (2021), 91â€“102.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Junior å’Œ Yenï¼ˆ2021bï¼‰Francisco Erivaldo Fernandes Junior å’Œ Gary G. Yenã€‚2021bå¹´ã€‚é€šè¿‡è¿›åŒ–ç­–ç•¥å¯¹ç”Ÿæˆå¯¹æŠ—ç¥ç»ç½‘ç»œè¿›è¡Œå‰ªæï¼Œç”¨äºåŒ»ç–—æˆåƒè¯Šæ–­ã€‚*Inf.
    Sci.* 558ï¼ˆ2021ï¼‰ï¼Œ91â€“102ã€‚
- en: Karaboga etÂ al. (2007) Dervis Karaboga, Bahriye Akay, and Celal Ã–ztÃ¼rk. 2007.
    Artificial Bee Colony (ABC) Optimization Algorithm for Training Feed-Forward Neural
    Networks. In *Proc. Int. Conf. Modeling Decis. Artif. Intell.* 318â€“329.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karaboga ç­‰ï¼ˆ2007ï¼‰Dervis Karabogaï¼ŒBahriye Akay å’Œ Celal Ã–ztÃ¼rkã€‚2007å¹´ã€‚ç”¨äºè®­ç»ƒå‰é¦ˆç¥ç»ç½‘ç»œçš„äººå·¥èœ‚ç¾¤ï¼ˆABCï¼‰ä¼˜åŒ–ç®—æ³•ã€‚å‘è¡¨äº*Proc.
    Int. Conf. Modeling Decis. Artif. Intell.* 318â€“329ã€‚
- en: Karegowda and Manjunath (2011) AshaÂ Gowda Karegowda and A.Â S. Manjunath. 2011.
    Application of Genetic Algorithm Optimized Neural Network Connection Weights for
    Medical Diagnosis of PIMA Indians Diabetes. *Int. J. Soft Comput.* 2, 2 (2011),
    15â€“23.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karegowda å’Œ Manjunathï¼ˆ2011ï¼‰Asha Gowda Karegowda å’Œ A. S. Manjunathã€‚2011å¹´ã€‚åŸºäºé—ä¼ ç®—æ³•ä¼˜åŒ–çš„ç¥ç»ç½‘ç»œè¿æ¥æƒé‡åœ¨PIMAå°ç¬¬å®‰äººç³–å°¿ç—…åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨ã€‚*Int.
    J. Soft Comput.* 2, 2ï¼ˆ2011ï¼‰ï¼Œ15â€“23ã€‚
- en: Khadka and Tumer (2018) Shauharda Khadka and Kagan Tumer. 2018. Evolution-Guided
    Policy Gradient in Reinforcement Learning. In *Proc. Adv. Neural Inf. Process.
    Syst.*, Vol.Â 31\. 1196â€“1208.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khadka å’Œ Tumerï¼ˆ2018ï¼‰Shauharda Khadka å’Œ Kagan Tumerã€‚2018å¹´ã€‚å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿›åŒ–å¼•å¯¼ç­–ç•¥æ¢¯åº¦ã€‚å‘è¡¨äº*Proc.
    Adv. Neural Inf. Process. Syst.*ï¼Œç¬¬31å·ã€‚1196â€“1208ã€‚
- en: Khushaba etÂ al. (2008) RamiÂ N Khushaba, Ahmed Al-Ani, Akram AlSukker, and Adel
    Al-Jumaily. 2008. A Combined Ant Colony and Differential Evolution Feature Selection
    Algorithm. In *Proc. Int. Conf. Ant Colony Optim. Swarm Intell.* 1â€“12.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khushaba ç­‰ï¼ˆ2008ï¼‰Rami N Khushabaï¼ŒAhmed Al-Aniï¼ŒAkram AlSukker å’Œ Adel Al-Jumailyã€‚2008å¹´ã€‚ç»“åˆèšç¾¤ç®—æ³•å’Œå·®åˆ†è¿›åŒ–ç‰¹å¾é€‰æ‹©ç®—æ³•ã€‚å‘è¡¨äº*Proc.
    Int. Conf. Ant Colony Optim. Swarm Intell.* 1â€“12ã€‚
- en: Kitano (1990) Hiroaki Kitano. 1990. Designing Neural Networks Using Genetic
    Algorithms with Graph Generation System. *Complex Syst.* 4, 4 (1990), 225â€“238.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitanoï¼ˆ1990ï¼‰Hiroaki Kitanoã€‚1990å¹´ã€‚ä½¿ç”¨å›¾ç”Ÿæˆç³»ç»Ÿè®¾è®¡ç¥ç»ç½‘ç»œçš„é—ä¼ ç®—æ³•ã€‚*Complex Syst.* 4, 4ï¼ˆ1990ï¼‰ï¼Œ225â€“238ã€‚
- en: Kotani and Kato (2004) Manabu Kotani and Daisuke Kato. 2004. Feature Extraction
    Using Coevolutionary Genetic Programming. In *Proc. IEEE Congr. Evol. Comput.*
    614â€“619.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotani å’Œ Katoï¼ˆ2004ï¼‰Manabu Kotani å’Œ Daisuke Katoã€‚2004å¹´ã€‚ä½¿ç”¨å…±è¿›åŒ–é—ä¼ ç¼–ç¨‹è¿›è¡Œç‰¹å¾æå–ã€‚å‘è¡¨äº*Proc.
    IEEE Congr. Evol. Comput.* 614â€“619ã€‚
- en: KoutnÃ­k etÂ al. (2010) Jan KoutnÃ­k, FaustinoÂ J. Gomez, and JÃ¼rgen Schmidhuber.
    2010. Evolving Neural Networks in Compressed Weight Space. In *Proc. Genetic Evol.
    Comput. Conf.* 619â€“626.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KoutnÃ­k ç­‰ï¼ˆ2010ï¼‰Jan KoutnÃ­kï¼ŒFaustino J. Gomez å’Œ JÃ¼rgen Schmidhuberã€‚2010å¹´ã€‚åœ¨å‹ç¼©æƒé‡ç©ºé—´ä¸­è¿›åŒ–ç¥ç»ç½‘ç»œã€‚å‘è¡¨äº*Proc.
    Genetic Evol. Comput. Conf.* 619â€“626ã€‚
- en: KoutnÃ­k etÂ al. (2014) Jan KoutnÃ­k, JÃ¼rgen Schmidhuber, and FaustinoÂ J. Gomez.
    2014. Evolving Deep Unsupervised Convolutional Networks for Vision-Based Reinforcement
    Learning. In *Proc. Genetic Evol. Comput. Conf.* 541â€“548.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KoutnÃ­k ç­‰ï¼ˆ2014ï¼‰Jan KoutnÃ­kï¼ŒJÃ¼rgen Schmidhuber å’Œ Faustino J. Gomezã€‚2014å¹´ã€‚ä¸ºåŸºäºè§†è§‰çš„å¼ºåŒ–å­¦ä¹ è¿›åŒ–æ·±åº¦æ— ç›‘ç£å·ç§¯ç½‘ç»œã€‚å‘è¡¨äº*Proc.
    Genetic Evol. Comput. Conf.* 541â€“548ã€‚
- en: Krawiec (2002) Krzysztof Krawiec. 2002. Genetic Programming-Based Construction
    of Features for Machine Learning and Knowledge Discovery Tasks. *Genet. Program
    Evolvable Mach.* 3, 4 (2002), 329â€“343.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krawiecï¼ˆ2002ï¼‰Krzysztof Krawiecã€‚2002å¹´ã€‚åŸºäºé—ä¼ ç¼–ç¨‹çš„ç‰¹å¾æ„é€ ç”¨äºæœºå™¨å­¦ä¹ å’ŒçŸ¥è¯†å‘ç°ä»»åŠ¡ã€‚*Genet. Program
    Evolvable Mach.* 3, 4ï¼ˆ2002ï¼‰ï¼Œ329â€“343ã€‚
- en: Krizhevsky etÂ al. (2009) Alex Krizhevsky, Geoffrey Hinton, etÂ al. 2009. Learning
    Multiple Layers of Features From Tiny Images. (2009).
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky ç­‰ï¼ˆ2009ï¼‰Alex Krizhevskyï¼ŒGeoffrey Hinton ç­‰ã€‚2009å¹´ã€‚ä»å¾®å°å›¾åƒä¸­å­¦ä¹ å¤šä¸ªå±‚æ¬¡çš„ç‰¹å¾ã€‚ï¼ˆ2009å¹´ï¼‰ã€‚
- en: Krizhevsky etÂ al. (2012) Alex Krizhevsky, Ilya Sutskever, and GeoffreyÂ E Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. In *Proc.
    Adv. Neural Inf. Process. Syst.*, Vol.Â 25\. 1097â€“1105.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky ç­‰ï¼ˆ2012ï¼‰Alex Krizhevskyï¼ŒIlya Sutskever å’Œ Geoffrey E Hintonã€‚2012å¹´ã€‚ä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„ImageNetåˆ†ç±»ã€‚å‘è¡¨äº*Proc.
    Adv. Neural Inf. Process. Syst.*ï¼Œç¬¬25å·ã€‚1097â€“1105ã€‚
- en: Kwasigroch etÂ al. (2019) Arkadiusz Kwasigroch, MichaÅ‚ Grochowski, and Mateusz
    Mikolajczyk. 2019. Deep Neural Network Architecture Search using Network Morphism.
    In *Proc. Int. Conf. Methods and Models in Autom. and Robot.* 30â€“35.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwasigroch ç­‰ï¼ˆ2019ï¼‰Arkadiusz Kwasigrochï¼ŒMichaÅ‚ Grochowski å’Œ Mateusz Mikolajczykã€‚2019å¹´ã€‚ä½¿ç”¨ç½‘ç»œå½¢æ€å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ã€‚å‘è¡¨äº*Proc.
    Int. Conf. Methods and Models in Autom. and Robot.* 30â€“35ã€‚
- en: LeCun etÂ al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep
    Learning. *Nature* 521, 7553 (2015), 436â€“444.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun ç­‰ï¼ˆ2015ï¼‰Yann LeCunï¼ŒYoshua Bengio å’Œ Geoffrey Hintonã€‚2015å¹´ã€‚æ·±åº¦å­¦ä¹ ã€‚*Nature*
    521, 7553ï¼ˆ2015ï¼‰ï¼Œ436â€“444ã€‚
- en: LeCun etÂ al. (1989) Yann LeCun, Bernhard Boser, JohnÂ S Denker, Donnie Henderson,
    RichardÂ E Howard, and Wayne Hubbard. 1989. Backpropagation Applied to Handwritten
    Zip Code Recognition. *Neural computation* 1, 4 (1989), 541â€“551.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun ç­‰ï¼ˆ1989ï¼‰Yann LeCunï¼ŒBernhard Boserï¼ŒJohn S Denkerï¼ŒDonnie Hendersonï¼ŒRichard
    E Howard å’Œ Wayne Hubbardã€‚1989å¹´ã€‚åå‘ä¼ æ’­åº”ç”¨äºæ‰‹å†™é‚®æ”¿ç¼–ç è¯†åˆ«ã€‚*Neural computation* 1, 4ï¼ˆ1989ï¼‰ï¼Œ541â€“551ã€‚
- en: 'Li etÂ al. (2020) Bailin Li, Bowen Wu, Jiang Su, Guangrun Wang, and Liang Lin.
    2020. EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning.
    In *Proc. Eur. Conf. Comput. Vis.* 639â€“654.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020) Bailin Li, Bowen Wu, Jiang Su, Guangrun Wang, and Liang Lin.
    2020. EagleEyeï¼šç”¨äºé«˜æ•ˆç¥ç»ç½‘ç»œå‰ªæçš„å¿«é€Ÿå­ç½‘ç»œè¯„ä¼°ã€‚åœ¨*Proc. Eur. Conf. Comput. Vis.* 639â€“654.
- en: 'Li etÂ al. (2021) Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao,
    Haoran You, Qixuan Yu, Yue Wang, Cong Hao, and Yingyan Lin. 2021. HW-NAS-Bench:
    Hardware-Aware Neural Architecture Search Benchmark. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/2103.10584.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021) Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao,
    Haoran You, Qixuan Yu, Yue Wang, Cong Hao, and Yingyan Lin. 2021. HW-NAS-Benchï¼šç¡¬ä»¶æ„ŸçŸ¥ç¥ç»æ¶æ„æœç´¢åŸºå‡†ã€‚åœ¨*Proc.
    Int. Conf. Learn. Represent.* https://arxiv.org/abs/2103.10584.
- en: Li etÂ al. (2022) Qing Li, Wei Zhang, Lin Zhao, Xia Wu, and Tianming Liu. 2022.
    Evolutional Neural Architecture Search for Optimization of Spatiotemporal Brain
    Network Decomposition. *IEEE. Trans. Biomed. Eng.* 69, 2 (2022), 624â€“634.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Qing Li, Wei Zhang, Lin Zhao, Xia Wu, and Tianming Liu. 2022.
    ç”¨äºä¼˜åŒ–æ—¶ç©ºè„‘ç½‘ç»œåˆ†è§£çš„è¿›åŒ–ç¥ç»æ¶æ„æœç´¢ã€‚*IEEE. Trans. Biomed. Eng.* 69, 2 (2022), 624â€“634.
- en: 'Li etÂ al. (2019) Youru Li, Zhenfeng Zhu, Deqiang Kong, Hua Han, and Yao Zhao.
    2019. EA-LSTM: Evolutionary Attention-Based LSTM for Time Series Prediction. *Knowl.-Based
    Syst.* 181 (2019), 104785.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019) Youru Li, Zhenfeng Zhu, Deqiang Kong, Hua Han, and Yao Zhao.
    2019. EA-LSTMï¼šåŸºäºè¿›åŒ–æ³¨æ„åŠ›çš„LSTMç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ã€‚*Knowl.-Based Syst.* 181 (2019), 104785.
- en: Liu etÂ al. (2018c) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
    2018c. Progressive Neural Architecture Search. In *Proc. Eur. Conf. Comput. Vis.*
    19â€“34.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018c) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
    2018c. æ¸è¿›ç¥ç»æ¶æ„æœç´¢ã€‚åœ¨*Proc. Eur. Conf. Comput. Vis.* 19â€“34.
- en: 'Liu etÂ al. (2021a) Chia-Hsiang Liu, Yu-Shin Han, Yuan-Yao Sung, Yi Lee, Hung-Yueh
    Chiang, and Kai-Chiang Wu. 2021a. FOX-NAS: Fast, On-device and Explainable Neural
    Architecture Search. In *Proc. IEEE Int. Conf. Comput. Vis.* 789â€“797.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021a) Chia-Hsiang Liu, Yu-Shin Han, Yuan-Yao Sung, Yi Lee, Hung-Yueh
    Chiang, and Kai-Chiang Wu. 2021a. FOX-NASï¼šå¿«é€Ÿã€è®¾å¤‡ç«¯å’Œå¯è§£é‡Šçš„ç¥ç»æ¶æ„æœç´¢ã€‚åœ¨*Proc. IEEE Int.
    Conf. Comput. Vis.* 789â€“797.
- en: 'Liu etÂ al. (2018b) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018b. DARTS:
    Differentiable Architecture Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1806.09055.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018b) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018b. DARTSï¼šå¯å¾®åˆ†æ¶æ„æœç´¢ã€‚åœ¨*Proc.
    Int. Conf. Learn. Represent.* https://arxiv.org/abs/1806.09055.
- en: Liu etÂ al. (2019a) Peng Liu, Mohammad D.Â El Basha, Yangjunyi Li, Yao Xiao, PinaÂ C.
    Sanelli, and Ruogu Fang. 2019a. Deep Evolutionary Networks with Expedited Genetic
    Algorithms for Medical Image Denoising. *Med. Image Anal.* 54 (2019), 306â€“315.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019a) Peng Liu, Mohammad D. El Basha, Yangjunyi Li, Yao Xiao, Pina
    C. Sanelli, and Ruogu Fang. 2019a. ä½¿ç”¨åŠ é€Ÿé—ä¼ ç®—æ³•çš„æ·±åº¦è¿›åŒ–ç½‘ç»œç”¨äºåŒ»å­¦å›¾åƒå»å™ªã€‚*Med. Image Anal.*
    54 (2019), 306â€“315.
- en: 'Liu and Guo (2021) Sicong Liu and Bin Guo. 2021. AdaSpring: Context-adaptive
    and Runtime-evolutionary Deep Model Compression for Mobile Applications. In *Proc.
    ACM Interact., Mobile, Wearable Ubiquitous Tech.*, Vol.Â 5\. ACM, 1â€“22.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu and Guo (2021) Sicong Liu and Bin Guo. 2021. AdaSpringï¼šé€‚åº”ä¸Šä¸‹æ–‡å’Œè¿è¡Œæ—¶æ¼”åŒ–çš„æ·±åº¦æ¨¡å‹å‹ç¼©ç”¨äºç§»åŠ¨åº”ç”¨ã€‚åœ¨*Proc.
    ACM Interact., Mobile, Wearable Ubiquitous Tech.*, Vol. 5. ACM, 1â€“22.
- en: Liu etÂ al. (2018a) Xiao-Ying Liu, Yong Liang, Sai Wang, Zi-Yi Yang, and Han-Shuo
    Ye. 2018a. A Hybrid Genetic Algorithm With Wrapper-Embedded Approaches for Feature
    Selection. *IEEE Access* 6 (2018), 22863â€“22874.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018a) Xiao-Ying Liu, Yong Liang, Sai Wang, Zi-Yi Yang, and Han-Shuo
    Ye. 2018a. ä¸€ç§æ··åˆé—ä¼ ç®—æ³•ä¸åŒ…è£…åµŒå…¥æ–¹æ³•çš„ç‰¹å¾é€‰æ‹©ã€‚*IEEE Access* 6 (2018), 22863â€“22874.
- en: Liu etÂ al. (2021b) Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, GaryÂ G. Yen,
    and KayÂ Chen Tan. 2021b. A Survey on Evolutionary Neural Architecture Search.
    *IEEE Trans. Neural Netw. Learn. Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3100554](https://doi.org/10.1109/TNNLS.2021.3100554)
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021b) Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen,
    and Kay Chen Tan. 2021b. å…³äºè¿›åŒ–ç¥ç»æ¶æ„æœç´¢çš„ç»¼è¿°ã€‚*IEEE Trans. Neural Netw. Learn. Syst.*
    (2021). [https://doi.org/10.1109/TNNLS.2021.3100554](https://doi.org/10.1109/TNNLS.2021.3100554)
- en: 'Liu etÂ al. (2019b) Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang,
    K. Cheng, and Jian Sun. 2019b. MetaPruning: Meta Learning for Automatic Neural
    Network Channel Pruning. In *Proc. IEEE Int. Conf. Comput. Vis.* 3295â€“3304.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019b) Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang,
    K. Cheng, and Jian Sun. 2019b. MetaPruningï¼šç”¨äºè‡ªåŠ¨ç¥ç»ç½‘ç»œé€šé“å‰ªæçš„å…ƒå­¦ä¹ ã€‚åœ¨*Proc. IEEE Int.
    Conf. Comput. Vis.* 3295â€“3304.
- en: Lomurno etÂ al. (2021) Eugenio Lomurno, Stefano Samele, Matteo Matteucci, and
    Danilo Ardagna. 2021. Pareto-optimal Progressive Neural Architecture Search. In
    *Proc. Genetic Evol. Comput. Conf.* 1726â€“1734.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lomurno ç­‰ï¼ˆ2021ï¼‰Eugenio Lomurno, Stefano Samele, Matteo Matteucci å’Œ Danilo Ardagnaã€‚2021å¹´ã€‚Pareto
    æœ€ä¼˜æ¸è¿›ç¥ç»æ¶æ„æœç´¢ã€‚å‘è¡¨äº *Proc. Genetic Evol. Comput. Conf.* 1726â€“1734ã€‚
- en: Londt etÂ al. (2021) Trevor Londt, Xiaoying Gao, and Peter Andreae. 2021. Evolving
    Character-level DenseNet Architectures Using Genetic Programming. In *Proc. Int.
    Conf. Appl. Evol. Comput.* 665â€“680.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Londt ç­‰ï¼ˆ2021ï¼‰Trevor Londt, Xiaoying Gao å’Œ Peter Andreaeã€‚2021å¹´ã€‚ä½¿ç”¨é—ä¼ ç¼–ç¨‹è¿›åŒ–å­—ç¬¦çº§ DenseNet
    æ¶æ„ã€‚å‘è¡¨äº *Proc. Int. Conf. Appl. Evol. Comput.* 665â€“680ã€‚
- en: Londt etÂ al. (2020) Trevor Londt, Xiaoying Gao, Bing Xue, and Peter Andreae.
    2020. Evolving Character-level Convolutional Neural Networks for Text Classification.
    *arXiv preprint arXiv:2012.02223* (2020).
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Londt ç­‰ï¼ˆ2020ï¼‰Trevor Londt, Xiaoying Gao, Bing Xue å’Œ Peter Andreaeã€‚2020å¹´ã€‚é’ˆå¯¹æ–‡æœ¬åˆ†ç±»çš„å­—ç¬¦çº§å·ç§¯ç¥ç»ç½‘ç»œè¿›åŒ–ã€‚*arXiv
    preprint arXiv:2012.02223*ï¼ˆ2020ï¼‰ã€‚
- en: 'Loni etÂ al. (2020) Mohammad Loni, Sima Sinaei, and Ali Zoljodi. 2020. DeepMaker:
    A Multi-Objective Optimization Framework for Deep Neural Networks in Embedded
    Systems. *Microprocess. Microsyst.* 73 (2020), 102989.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loni ç­‰ï¼ˆ2020ï¼‰Mohammad Loni, Sima Sinaei å’Œ Ali Zoljodiã€‚2020å¹´ã€‚DeepMakerï¼šç”¨äºåµŒå…¥å¼ç³»ç»Ÿä¸­æ·±åº¦ç¥ç»ç½‘ç»œçš„å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶ã€‚*Microprocess.
    Microsyst.* 73ï¼ˆ2020ï¼‰ï¼Œ102989ã€‚
- en: Lorenzo and Nalepa (2018) PabloÂ Ribalta Lorenzo and Jakub Nalepa. 2018. Memetic
    Evolution of Deep Neural Networks. In *Proc. Genetic Evol. Comput. Conf.* 505â€“512.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lorenzo å’Œ Nalepaï¼ˆ2018ï¼‰Pablo Ribalta Lorenzo å’Œ Jakub Nalepaã€‚2018å¹´ã€‚æ·±åº¦ç¥ç»ç½‘ç»œçš„è®°å¿†è¿›åŒ–ã€‚å‘è¡¨äº
    *Proc. Genetic Evol. Comput. Conf.* 505â€“512ã€‚
- en: Lorenzo etÂ al. (2017) PabloÂ Ribalta Lorenzo, Jakub Nalepa, LucianoÂ SÃ¡nchez Ramos,
    and JosÃ© Ranilla. 2017. Hyper-parameter Selection in Deep Neural Networks Using
    Parallel Particle Swarm Optimization. In *Proc. Genetic Evol. Comput. Conf.* 1864â€“1871.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lorenzo ç­‰ï¼ˆ2017ï¼‰Pablo Ribalta Lorenzo, Jakub Nalepa, Luciano SÃ¡nchez Ramos å’Œ
    JosÃ© Ranillaã€‚2017å¹´ã€‚ä½¿ç”¨å¹¶è¡Œç²’å­ç¾¤ä¼˜åŒ–è¿›è¡Œæ·±åº¦ç¥ç»ç½‘ç»œçš„è¶…å‚æ•°é€‰æ‹©ã€‚å‘è¡¨äº *Proc. Genetic Evol. Comput. Conf.*
    1864â€“1871ã€‚
- en: Lu etÂ al. (2021) Zhichao Lu, Gautam Sreekumar, Erik Goodman, Wolfgang Banzhaf,
    Kalyanmoy Deb, and VishnuÂ Naresh Boddeti. 2021. Neural Architecture Transfer.
    *IEEE IEEE Trans. Pattern Anal. Mach. Intell.* 43, 9 (2021), 2971â€“2989.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu ç­‰ï¼ˆ2021ï¼‰Zhichao Lu, Gautam Sreekumar, Erik Goodman, Wolfgang Banzhaf, Kalyanmoy
    Deb å’Œ Vishnu Naresh Boddetiã€‚2021å¹´ã€‚ç¥ç»æ¶æ„è¿ç§»ã€‚*IEEE IEEE Trans. Pattern Anal. Mach.
    Intell.* 43, 9ï¼ˆ2021ï¼‰ï¼Œ2971â€“2989ã€‚
- en: 'Lu etÂ al. (2019) Zhichao Lu, Ian Whalen, VishnuÂ Naresh Boddeti, YasheshÂ D.
    Dhebar, and Kalyanmoy Deb. 2019. NSGA-Net: Neural Architecture Search using Multi-objective
    Genetic Algorithm. In *Proc. Genetic Evol. Comput. Conf.* 419â€“427.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu ç­‰ï¼ˆ2019ï¼‰Zhichao Lu, Ian Whalen, Vishnu Naresh Boddeti, Yashesh D. Dhebar å’Œ
    Kalyanmoy Debã€‚2019å¹´ã€‚NSGA-Netï¼šä½¿ç”¨å¤šç›®æ ‡é—ä¼ ç®—æ³•çš„ç¥ç»æ¶æ„æœç´¢ã€‚å‘è¡¨äº *Proc. Genetic Evol. Comput.
    Conf.* 419â€“427ã€‚
- en: Luo etÂ al. (2018) Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
    2018. Neural architecture optimization. In *Proc. Adv. Neural Inf. Process. Syst.*,
    Vol.Â 31\. 7827â€“7838.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo ç­‰ï¼ˆ2018ï¼‰Renqian Luo, Fei Tian, Tao Qin, Enhong Chen å’Œ Tie-Yan Liuã€‚2018å¹´ã€‚ç¥ç»æ¶æ„ä¼˜åŒ–ã€‚å‘è¡¨äº
    *Proc. Adv. Neural Inf. Process. Syst.*, Vol. 31\. 7827â€“7838ã€‚
- en: 'Ma etÂ al. (2021b) Ailong Ma, Yuting Wan, Yanfei Zhong, Junjue Wang, and Liang
    pei Zhang. 2021b. SceneNet: Remote Sensing Scene Classification Deep Learning
    Network Using Multi-Objective Neural Evolution Architecture Search. *ISPRS J.
    Photogramm. Remote Sens.* 172 (2021), 171â€“188.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma ç­‰ï¼ˆ2021bï¼‰Ailong Ma, Yuting Wan, Yanfei Zhong, Junjue Wang å’Œ Liang pei Zhangã€‚2021bå¹´ã€‚SceneNetï¼šä½¿ç”¨å¤šç›®æ ‡ç¥ç»è¿›åŒ–æ¶æ„æœç´¢çš„é¥æ„Ÿåœºæ™¯åˆ†ç±»æ·±åº¦å­¦ä¹ ç½‘ç»œã€‚*ISPRS
    J. Photogramm. Remote Sens.* 172ï¼ˆ2021ï¼‰ï¼Œ171â€“188ã€‚
- en: Ma etÂ al. (2022) Lianbo Ma, Min Huang, Shengxiang Yang, Rui Wang, and Xingwei
    Wang. 2022. An Adaptive Localized Decision Variable Analysis Approach to Large-Scale
    Multiobjective and Many-Objective Optimization. *IEEE Trans. Cybern.* 52, 7 (2022),
    6684â€“6696.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma ç­‰ï¼ˆ2022ï¼‰Lianbo Ma, Min Huang, Shengxiang Yang, Rui Wang å’Œ Xingwei Wangã€‚2022å¹´ã€‚ä¸€ç§è‡ªé€‚åº”å±€éƒ¨å†³ç­–å˜é‡åˆ†ææ–¹æ³•ç”¨äºå¤§è§„æ¨¡å¤šç›®æ ‡å’Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚*IEEE
    Trans. Cybern.* 52, 7ï¼ˆ2022ï¼‰ï¼Œ6684â€“6696ã€‚
- en: 'Ma etÂ al. (2021a) Lianbo Ma, Nan Li, Guo Yu, Xiaoyu Geng, Min Huang, and Xingwei
    Wang. 2021a. How to Simplify Search: Classification-wise Pareto Evolution for
    One-shot Neural Architecture Search. *arXiv preprint arXiv:2109.07582* (2021).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma ç­‰ï¼ˆ2021aï¼‰Lianbo Ma, Nan Li, Guo Yu, Xiaoyu Geng, Min Huang å’Œ Xingwei Wangã€‚2021aå¹´ã€‚å¦‚ä½•ç®€åŒ–æœç´¢ï¼šé¢å‘åˆ†ç±»çš„
    Pareto è¿›åŒ–ç”¨äºä¸€é˜¶æ®µç¥ç»æ¶æ„æœç´¢ã€‚*arXiv preprint arXiv:2109.07582*ï¼ˆ2021ï¼‰ã€‚
- en: Ma etÂ al. (2021c) Wenping Ma, Xiaobo Zhou, Hao Zhu, Longwei Li, and Licheng
    Jiao. 2021c. A Two-stage Hybrid Ant Colony Optimization for High-dimensional Feature
    Selection. *Pattern Recognit.* 116 (2021), 107933.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma ç­‰ï¼ˆ2021cï¼‰Wenping Ma, Xiaobo Zhou, Hao Zhu, Longwei Li å’Œ Licheng Jiaoã€‚2021cå¹´ã€‚ä¸€ç§ä¸¤é˜¶æ®µæ··åˆèšç¾¤ä¼˜åŒ–ç”¨äºé«˜ç»´ç‰¹å¾é€‰æ‹©ã€‚*Pattern
    Recognit.* 116ï¼ˆ2021ï¼‰ï¼Œ107933ã€‚
- en: Maniezzo (1994) V. Maniezzo. 1994. Genetic Evolution of the Topology and Weight
    Distribution of Neural Networks. *IEEE Trans. Neural. Netw.* 5, 1 (1994), 39â€“53.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maniezzo (1994) V. Maniezzoã€‚1994å¹´ã€‚ç¥ç»ç½‘ç»œæ‹“æ‰‘å’Œæƒé‡åˆ†å¸ƒçš„é—ä¼ è¿›åŒ–ã€‚*IEEE Trans. Neural. Netw.*
    5, 1 (1994), 39â€“53ã€‚
- en: Mauceri etÂ al. (2021) Stefano Mauceri, James Sweeney, Miguel Nicolau, and James
    McDermott. 2021. Feature Extraction by Grammatical Evolution for One-class Time
    Series Classification. *Genet. Program. Evolvable Mach.* 22, 3 (2021), 267â€“295.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mauceri ç­‰äºº (2021) Stefano Mauceri, James Sweeney, Miguel Nicolau å’Œ James McDermottã€‚2021å¹´ã€‚é€šè¿‡è¯­æ³•è¿›åŒ–è¿›è¡Œç‰¹å¾æå–ç”¨äºå•ç±»æ—¶é—´åºåˆ—åˆ†ç±»ã€‚*Genet.
    Program. Evolvable Mach.* 22, 3 (2021), 267â€“295ã€‚
- en: Mazzawi etÂ al. (2019) Hanna Mazzawi, Xavi Gonzalvo, Aleksandar Kracun, and Prashant
    Sridhar. 2019. Improving Keyword Spotting and Language Identification via Neural
    Architecture Search at Scale. In *INTERSPEECH*. 1278â€“1282.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mazzawi ç­‰äºº (2019) Hanna Mazzawi, Xavi Gonzalvo, Aleksandar Kracun å’Œ Prashant
    Sridharã€‚2019å¹´ã€‚é€šè¿‡å¤§è§„æ¨¡ç¥ç»æ¶æ„æœç´¢æ”¹è¿›å…³é”®è¯æ£€æµ‹å’Œè¯­è¨€è¯†åˆ«ã€‚åœ¨ *INTERSPEECH*ã€‚1278â€“1282ã€‚
- en: Mei etÂ al. (2017) Yi Mei, Su Nguyen, Bing Xue, and Mengjie Zhang. 2017. An Efficient
    Feature Selection Algorithm for Evolving Job Shop Scheduling Rules With Genetic
    Programming. *IEEE Trans. Emerg. Topics Comput. Intell.* 1, 5 (2017), 339â€“353.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mei ç­‰äºº (2017) Yi Mei, Su Nguyen, Bing Xue å’Œ Mengjie Zhangã€‚2017å¹´ã€‚ç”¨äºè¿›åŒ–ä½œä¸šè½¦é—´è°ƒåº¦è§„åˆ™çš„é«˜æ•ˆç‰¹å¾é€‰æ‹©ç®—æ³•ã€‚*IEEE
    Trans. Emerg. Topics Comput. Intell.* 1, 5 (2017), 339â€“353ã€‚
- en: Miahi etÂ al. (2022) Erfan Miahi, SeyedÂ Abolghasem Mirroshandel, and Alexis Nasr.
    2022. Genetic Neural Architecture Search for Automatic Assessment of Human Sperm
    Images. *Expert Syst. Appl.* 188 (2022), 115937.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miahi ç­‰äºº (2022) Erfan Miahi, Seyed Abolghasem Mirroshandel å’Œ Alexis Nasrã€‚2022å¹´ã€‚ç”¨äºè‡ªåŠ¨è¯„ä¼°äººç±»ç²¾å­å›¾åƒçš„é—ä¼ ç¥ç»æ¶æ„æœç´¢ã€‚*Expert
    Syst. Appl.* 188 (2022), 115937ã€‚
- en: Miikkulainen etÂ al. (2019) Risto Miikkulainen, Jason Liang, Elliot Meyerson,
    Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak
    Navruzyan, Nigel Duffy, etÂ al. 2019. Evolving Deep Neural Networks. In *Artificial
    Intelligence in the Age Of Neural Networks and Brain Computing*. Elsevier, 293â€“312.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miikkulainen ç­‰äºº (2019) Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya
    Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan,
    Nigel Duffy ç­‰ã€‚2019å¹´ã€‚è¿›åŒ–æ·±åº¦ç¥ç»ç½‘ç»œã€‚åœ¨ *Artificial Intelligence in the Age Of Neural Networks
    and Brain Computing*ã€‚Elsevierï¼Œ293â€“312ã€‚
- en: Mirjalili etÂ al. (2019) Seyedali Mirjalili, Hossam Faris, and Ibrahim Aljarah.
    2019. *Evolutionary Machine Learning Techniques*. Springer.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirjalili ç­‰äºº (2019) Seyedali Mirjalili, Hossam Faris å’Œ Ibrahim Aljarahã€‚2019å¹´ã€‚*è¿›åŒ–æœºå™¨å­¦ä¹ æŠ€æœ¯*ã€‚Springerã€‚
- en: Mo etÂ al. (2021) Hyunho Mo, LeonardoÂ Lucio Custode, and Giovanni Iacca. 2021.
    Evolutionary Neural Architecture Search for Remaining Useful Life Prediction.
    *Appl. Soft Comput.* 108 (2021), 107474.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mo ç­‰äºº (2021) Hyunho Mo, Leonardo Lucio Custode å’Œ Giovanni Iaccaã€‚2021å¹´ã€‚ç”¨äºå‰©ä½™æœ‰æ•ˆå¯¿å‘½é¢„æµ‹çš„è¿›åŒ–ç¥ç»æ¶æ„æœç´¢ã€‚*Appl.
    Soft Comput.* 108 (2021), 107474ã€‚
- en: Montana and Davis (1989) DavidÂ J Montana and Lawrence Davis. 1989. Training
    Feedforward Neural Networks Using Genetic Algorithms. In *Proc. of the Int. Joint
    Conf. Artif. Intell.*, Vol.Â 4\. 762â€“767.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montana å’Œ Davis (1989) David J Montana å’Œ Lawrence Davisã€‚1989å¹´ã€‚ä½¿ç”¨é—ä¼ ç®—æ³•è®­ç»ƒå‰é¦ˆç¥ç»ç½‘ç»œã€‚åœ¨
    *Proc. of the Int. Joint Conf. Artif. Intell.*, ç¬¬ 4 å·ã€‚762â€“767ã€‚
- en: Muni etÂ al. (2006) DurgaÂ Prasad Muni, NikhilÂ R Pal, and Jyotirmay Das. 2006.
    Genetic Programming for Simultaneous Feature Selection and Classifier Design.
    *IEEE Trans. Syst. Man. Cybern.* 36, 1 (2006), 106â€“117.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muni ç­‰äºº (2006) Durga Prasad Muni, Nikhil R Pal å’Œ Jyotirmay Dasã€‚2006å¹´ã€‚ç”¨äºç‰¹å¾é€‰æ‹©å’Œåˆ†ç±»å™¨è®¾è®¡çš„é—ä¼ ç¼–ç¨‹ã€‚*IEEE
    Trans. Syst. Man. Cybern.* 36, 1 (2006), 106â€“117ã€‚
- en: 'Murray and Chiang (2015) Kenton Murray and David Chiang. 2015. Auto-Sizing
    Neural Networks: With Applications to N-Gram Language Models. In *Proc. Conf.
    Empir. Methods Nat. Lang. Proc.* 908â€“916.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murray å’Œ Chiang (2015) Kenton Murray å’Œ David Chiangã€‚2015å¹´ã€‚è‡ªåŠ¨è°ƒæ•´ç¥ç»ç½‘ç»œï¼šåº”ç”¨äº N-Gram
    è¯­è¨€æ¨¡å‹ã€‚åœ¨ *Proc. Conf. Empir. Methods Nat. Lang. Proc.* 908â€“916ã€‚
- en: Nekrasov etÂ al. (2020) Vladimir Nekrasov, Chunhua Shen, and Ian Reid. 2020.
    Template-Based Automatic Search of Compact Semantic Segmentation Architectures.
    In *Proc. Winter Conf. Appl. Comput. Vis.* 1980â€“1989.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nekrasov ç­‰äºº (2020) Vladimir Nekrasov, Chunhua Shen å’Œ Ian Reidã€‚2020å¹´ã€‚åŸºäºæ¨¡æ¿çš„ç´§å‡‘è¯­ä¹‰åˆ†å‰²æ¶æ„è‡ªåŠ¨æœç´¢ã€‚åœ¨
    *Proc. Winter Conf. Appl. Comput. Vis.* 1980â€“1989ã€‚
- en: 'Neshat etÂ al. (2020) Mehdi Neshat, MeysamÂ Majidi Nezhad, Ehsan Abbasnejad,
    LinaÂ Bertling Tjernberg, DavideÂ Astiaso Garcia, Bradley Alexander, and Markus
    Wagner. 2020. An Evolutionary Deep Learning Method for Short-term Wind Speed Prediction:
    A Case Study of the Lillgrund Offshore Wind Farm. *arXiv preprint arXiv:abs/2002.09106*
    (2020).'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neshat ç­‰äºº (2020) Mehdi Neshat, Meysam Majidi Nezhad, Ehsan Abbasnejad, Lina
    Bertling Tjernberg, Davide Astiaso Garcia, Bradley Alexander å’Œ Markus Wagnerã€‚2020å¹´ã€‚ç”¨äºçŸ­æœŸé£é€Ÿé¢„æµ‹çš„è¿›åŒ–æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼šä»¥
    Lillgrund æµ·ä¸Šé£ç”µåœºä¸ºä¾‹ã€‚*arXiv é¢„å°æœ¬ arXiv:abs/2002.09106* (2020)ã€‚
- en: Neshatian etÂ al. (2012) Kourosh Neshatian, Mengjie Zhang, and Peter Andreae.
    2012. A Filter Approach to Multiple Feature Construction for Symbolic Learning
    Classifiers Using Genetic Programming. *IEEE Trans. Evol. Comput.* 16, 5 (2012),
    645â€“661.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neshatian ç­‰ï¼ˆ2012ï¼‰åº“ç½—ä»€Â·å†…æ²™è’‚å®‰ã€å¼ æ¢¦æ´ å’Œ å½¼å¾—Â·å®‰å¾·çƒˆã€‚2012å¹´ã€‚ä½¿ç”¨é—ä¼ ç¼–ç¨‹çš„å¤šç‰¹å¾æ„å»ºçš„è¿‡æ»¤æ–¹æ³•ã€‚*IEEE Trans.
    Evol. Comput.* 16, 5 (2012), 645â€“661ã€‚
- en: Neshatian etÂ al. (2007) Kourosh Neshatian, Mengjie Zhang, and Mark Johnston.
    2007. Feature Construction and Dimension Reduction Using Genetic Programming.
    In *Proc. Aust. Conf. Artif. Intell.* 242â€“253.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neshatian ç­‰ï¼ˆ2007ï¼‰åº“ç½—ä»€Â·å†…æ²™è’‚å®‰ã€å¼ æ¢¦æ´ å’Œ é©¬å…‹Â·çº¦ç¿°æ–¯é¡¿ã€‚2007å¹´ã€‚ä½¿ç”¨é—ä¼ ç¼–ç¨‹çš„ç‰¹å¾æ„å»ºä¸ç»´åº¦çº¦ç®€ã€‚è§äº *Proc. Aust.
    Conf. Artif. Intell.* 242â€“253ã€‚
- en: 'Nguyen etÂ al. (2014) HoaiÂ Bach Nguyen, Bing Xue, Ivy Liu, and Mengjie Zhang.
    2014. PSO and Statistical Clustering for Feature Selection: A New Representation.
    In *Proc. Asia-Pacific Conf. Simulated Evol. Learn.* 569â€“581.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é˜®ç­‰ï¼ˆ2014ï¼‰é˜®æ€€åšã€è–›å†°ã€åˆ˜è‰¾ç»´ å’Œ å¼ æ¢¦æ´ã€‚2014å¹´ã€‚ç”¨äºç‰¹å¾é€‰æ‹©çš„ PSO å’Œç»Ÿè®¡èšç±»ï¼šä¸€ç§æ–°è¡¨ç¤ºã€‚è§äº *Proc. Asia-Pacific
    Conf. Simulated Evol. Learn.* 569â€“581ã€‚
- en: 'Oâ€™Boyle and Palmer (2008) NoelÂ M Oâ€™Boyle and DavidÂ S Palmer. 2008. Simultaneous
    Feature Selection and Parameter Optimisation Using An Artificial Ant Colony: Case
    Study of Melting Point Prediction. *Chem. Cent. J.* 2, 1 (2008), 1â€“15.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oâ€™Boyle å’Œ Palmerï¼ˆ2008ï¼‰è¯ºåŸƒå°”Â·MÂ·å¥¥åšä¼Šå°” å’Œ å¤§å«Â·SÂ·å¸•å°”é»˜ã€‚2008å¹´ã€‚ä½¿ç”¨äººå·¥èšç¾¤è¿›è¡Œç‰¹å¾é€‰æ‹©ä¸å‚æ•°ä¼˜åŒ–çš„åŒæ—¶è¿›è¡Œï¼šç†”ç‚¹é¢„æµ‹æ¡ˆä¾‹ç ”ç©¶ã€‚*Chem.
    Cent. J.* 2, 1 (2008), 1â€“15ã€‚
- en: 'Olson and Moore (2016) RandalÂ S. Olson and JasonÂ H. Moore. 2016. TPOT: A Tree-based
    Pipeline Optimization Tool for Automating Machine Learning. In *Proc. Int. Conf.
    Mach. Learn.* 151â€“160.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olson å’Œ Mooreï¼ˆ2016ï¼‰å…°é“å°”Â·SÂ·å¥¥å°”æ£® å’Œ è©¹æ£®Â·HÂ·æ‘©å°”ã€‚2016å¹´ã€‚TPOTï¼šä¸€ç§åŸºäºæ ‘çš„ç®¡é“ä¼˜åŒ–å·¥å…·ï¼Œç”¨äºè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ã€‚è§äº
    *Proc. Int. Conf. Mach. Learn.* 151â€“160ã€‚
- en: 'Oâ€™Neill etÂ al. (2018) Damien Oâ€™Neill, Bing Xue, and Mengjie Zhang. 2018. Co-evolution
    of Novel Tree-Like ANNs and Activation Functions: An Observational Study. In *Proc.
    Aust. Conf. Artif. Intell.* 616â€“629.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oâ€™Neill ç­‰ï¼ˆ2018ï¼‰è¾¾ç±³å®‰Â·å¥¥å°¼å°”ã€è–›å†° å’Œ å¼ æ¢¦æ´ã€‚2018å¹´ã€‚æ–°å‹æ ‘çŠ¶ ANN å’Œæ¿€æ´»å‡½æ•°çš„å…±è¿›åŒ–ï¼šä¸€ç§è§‚å¯Ÿæ€§ç ”ç©¶ã€‚è§äº *Proc. Aust.
    Conf. Artif. Intell.* 616â€“629ã€‚
- en: Oong and Isa (2011) TattÂ Hee Oong and Nor AshidiÂ Mat Isa. 2011. Adaptive Evolutionary
    Artificial Neural Networks for Pattern Classification. *IEEE Trans. Neural Netw.*
    22, 11 (2011), 1823â€“1836.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oong å’Œ Isaï¼ˆ2011ï¼‰é»„è¾¾ç¦§ å’Œ é©¬ç‰¹Â·ä¼Šè¨ã€‚2011å¹´ã€‚ç”¨äºæ¨¡å¼åˆ†ç±»çš„è‡ªé€‚åº”è¿›åŒ–äººå·¥ç¥ç»ç½‘ç»œã€‚*IEEE Trans. Neural Netw.*
    22, 11 (2011), 1823â€“1836ã€‚
- en: Ortego etÂ al. (2020) Patxi Ortego, Alberto Diez-Olivan, JavierÂ Del Ser, and
    Fernando Veiga. 2020. Evolutionary LSTM-FCN Networks for Pattern Classification
    in Industrial Processes. *Swarm Evol. Comput.* 54 (2020), 100650.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ortego ç­‰ï¼ˆ2020ï¼‰å¸•ç‰¹è¥¿Â·å¥¥å°”ç‰¹æˆˆã€é˜¿å°”è´æ‰˜Â·è¿ªè€¶æ–¯-å¥¥åˆ©ä¸‡ã€å“ˆç»´å°”Â·å¾·å°”Â·å¡å°” å’Œ è´¹å°”å—å¤šÂ·ç»´åŠ ã€‚2020å¹´ã€‚ç”¨äºå·¥ä¸šè¿‡ç¨‹æ¨¡å¼åˆ†ç±»çš„è¿›åŒ–
    LSTM-FCN ç½‘ç»œã€‚*Swarm Evol. Comput.* 54 (2020), 100650ã€‚
- en: Peng etÂ al. (2021) Bo Peng, Shuting Wan, Ying Bi, Bing Xue, and Mengjie Zhang.
    2021. Automatic Feature Extraction and Construction Using Genetic Programming
    for Rotating Machinery Fault Diagnosis. *IEEE Trans. Cybern.* 51, 10 (2021), 4909â€“4923.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½­ç­‰ï¼ˆ2021ï¼‰å½­åšã€ä¸‡ä¹¦å©·ã€æ¯•è¹ã€è–›å†°å’Œå¼ æ¢¦æ´ã€‚2021å¹´ã€‚ä½¿ç”¨é—ä¼ ç¼–ç¨‹è¿›è¡Œæ—‹è½¬æœºæ¢°æ•…éšœè¯Šæ–­çš„è‡ªåŠ¨ç‰¹å¾æå–ä¸æ„å»ºã€‚*IEEE Trans. Cybern.*
    51, 10 (2021), 4909â€“4923ã€‚
- en: Peng etÂ al. (2018) Yiming Peng, Gang Chen, Harman Singh, and Mengjie Zhang.
    2018. NEAT for Large-scale Reinforcement Learning Through Evolutionary Feature
    Learning and Policy Gradient Search. In *Proc. Genetic Evol. Comput. Conf.* 490â€“497.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½­ç­‰ï¼ˆ2018ï¼‰å½­é€¸é¸£ã€é™ˆåˆšã€å“ˆæ›¼Â·è¾›æ ¼ å’Œ å¼ æ¢¦æ´ã€‚2018å¹´ã€‚é€šè¿‡è¿›åŒ–ç‰¹å¾å­¦ä¹ å’Œç­–ç•¥æ¢¯åº¦æœç´¢çš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ çš„ NEATã€‚è§äº *Proc. Genetic
    Evol. Comput. Conf.* 490â€“497ã€‚
- en: Phan etÂ al. (2020) HaiÂ T. Phan, Zechun Liu, DangÂ The Huynh, Marios Savvides,
    Kwang-Ting Cheng, and Zhiqiang Shen. 2020. Binarizing MobileNet via Evolution-Based
    Searching. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 13417â€“13426.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phan ç­‰ï¼ˆ2020ï¼‰æ½˜æµ·æ¶›ã€åˆ˜æ³½æ˜¥ã€èƒ¡æ·¡å’Œ é©¬é‡Œå¥¥æ–¯Â·è¨å¤«ç»´å¾·æ–¯ã€éƒ‘å…‰å»·ã€æ²ˆæ™ºå¼ºã€‚2020å¹´ã€‚é€šè¿‡åŸºäºè¿›åŒ–çš„æœç´¢äºŒå€¼åŒ– MobileNetã€‚è§äº *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.* 13417â€“13426ã€‚
- en: Polino etÂ al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
    Model Compression via Distillation and Quantization. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/1802.05668.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polino ç­‰ï¼ˆ2018ï¼‰å®‰ä¸œå°¼å¥¥Â·æ³¢åˆ©è¯ºã€æ‹‰å…¹ä¸‡Â·å¸•æ–¯å¡åŠª å’Œ ä¸¹Â·é˜¿åˆ©æ–¯å¡”èµ«ã€‚2018å¹´ã€‚é€šè¿‡è’¸é¦å’Œé‡åŒ–è¿›è¡Œæ¨¡å‹å‹ç¼©ã€‚è§äº *Proc. Int.
    Conf. Learn. Represent.* https://arxiv.org/abs/1802.05668ã€‚
- en: 'Poyatos etÂ al. (2022) Javier Poyatos, Daniel Molina, Aritz Martinez, etÂ al.
    2022. EvoPruneDeepTL: An Evolutionary Pruning Model for Transfer Learning based
    Deep Neural Networks. *arXiv preprint arXiv:2202.03844* (2022).'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poyatos ç­‰ï¼ˆ2022ï¼‰å“ˆç»´å°”Â·æ³¢äºšæ‰˜æ–¯ã€ä¸¹å°¼å°”Â·è«åˆ©çº³ã€é˜¿é‡ŒèŒ¨Â·é©¬ä¸å†…æ–¯ ç­‰ã€‚2022å¹´ã€‚EvoPruneDeepTLï¼šä¸€ç§ç”¨äºè¿ç§»å­¦ä¹ çš„è¿›åŒ–å‰ªææ¨¡å‹ï¼ŒåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œã€‚*arXiv
    preprint arXiv:2202.03844* (2022)ã€‚
- en: Radford etÂ al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    etÂ al. 2018. Improving Language Understanding by Generative Pre-training. (2018),
    https://www.cs.ubc.ca/Â amuham01/LING530/papers/radford2018improving.pdf.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford ç­‰ (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    ç­‰. 2018. é€šè¿‡ç”Ÿæˆé¢„è®­ç»ƒæé«˜è¯­è¨€ç†è§£ã€‚ (2018), https://www.cs.ubc.ca/amuham01/LING530/papers/radford2018improving.pdf.
- en: 'Rapaport etÂ al. (2019) Elad Rapaport, Oren Shriki, and Rami Puzis. 2019. EEGNAS:
    Neural Architecture Search for Electroencephalography Data Analysis and Decoding.
    In *Proc. Int. Joint Conf. Artif. Intell.* 3â€“20.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rapaport ç­‰ (2019) Elad Rapaport, Oren Shriki, å’Œ Rami Puzis. 2019. EEGNAS: ç”¨äºè„‘ç”µå›¾æ•°æ®åˆ†æå’Œè§£ç çš„ç¥ç»æ¶æ„æœç´¢ã€‚è§
    *å›½é™…äººå·¥æ™ºèƒ½è”åˆä¼šè®®è®ºæ–‡é›†* 3â€“20.'
- en: Rashid etÂ al. (2020) ANMÂ Bazlur Rashid, Mohiuddin Ahmed, LeslieÂ F Sikos, and
    Paul Haskell-Dowland. 2020. Cooperative Co-Evolution for Feature Selection in
    Big Data With Random Feature Grouping. *J. Big Data* 7, 1 (2020), 1â€“42.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashid ç­‰ (2020) ANM Bazlur Rashid, Mohiuddin Ahmed, Leslie F Sikos, å’Œ Paul Haskell-Dowland.
    2020. åœ¨å¤§æ•°æ®ä¸­ä½¿ç”¨éšæœºç‰¹å¾åˆ†ç»„çš„ç‰¹å¾é€‰æ‹©åˆä½œå…±è¿›ã€‚*å¤§æ•°æ®æ‚å¿—* 7, 1 (2020), 1â€“42.
- en: 'Rawal and Miikkulainen (2018) Aditya Rawal and Risto Miikkulainen. 2018. From
    Nodes to Networks: Evolving Recurrent Neural Networks. *arXiv preprint arXiv:1803.04439*
    (2018).'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawal å’Œ Miikkulainen (2018) Aditya Rawal å’Œ Risto Miikkulainen. 2018. ä»èŠ‚ç‚¹åˆ°ç½‘ç»œï¼šè¿›åŒ–é€’å½’ç¥ç»ç½‘ç»œã€‚*arXiv
    é¢„å°æœ¬ arXiv:1803.04439* (2018).
- en: Real etÂ al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and QuocÂ V Le.
    2019. Regularized Evolution for Image Classifier Architecture Search. In *Proc.
    AAAI Conf. Artif. Intell.*, Vol.Â 33\. 4780â€“4789.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real ç­‰ (2019) Esteban Real, Alok Aggarwal, Yanping Huang, å’Œ Quoc V Le. 2019.
    ç”¨äºå›¾åƒåˆ†ç±»å™¨æ¶æ„æœç´¢çš„æ­£åˆ™åŒ–è¿›åŒ–ã€‚è§ *AAAIäººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†* ç¬¬33å· 4780â€“4789.
- en: Real etÂ al. (2017) Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena,
    YutakaÂ Leon Suematsu, Jie Tan, QuocÂ V. Le, and Alexey Kurakin. 2017. Large-Scale
    Evolution of Image Classifiers. In *Proc. Int. Conf. Mach. Learn.* 2902â€“2911.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real ç­‰ (2017) Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka
    Leon Suematsu, Jie Tan, Quoc V. Le, å’Œ Alexey Kurakin. 2017. å¤§è§„æ¨¡å›¾åƒåˆ†ç±»å™¨è¿›åŒ–ã€‚è§ *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†*
    2902â€“2911.
- en: Refahi etÂ al. (2020) MohammadÂ Saleh Refahi, A Mir, and JalalÂ A Nasiri. 2020.
    A Novel Fusion Based on the Evolutionary Features for Protein Fold Recognition
    Using Support Vector Machines. *Sci. Rep.* 10, 1 (2020), 1â€“13.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Refahi ç­‰ (2020) Mohammad Saleh Refahi, A Mir, å’Œ Jalal A Nasiri. 2020. åŸºäºè¿›åŒ–ç‰¹å¾çš„æ–°å‹èåˆæ–¹æ³•ç”¨äºè›‹ç™½è´¨æŠ˜å è¯†åˆ«ï¼Œä½¿ç”¨æ”¯æŒå‘é‡æœºã€‚*ç§‘å­¦æŠ¥å‘Š*
    10, 1 (2020), 1â€“13.
- en: 'Ren etÂ al. (2021) Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, and
    Zhihui Li. 2021. A Comprehensive Survey of Neural Architecture Search: Challenges
    and Solutions. *ACM Comput. Surv.* 54, 4 (2021), 1â€“34.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren ç­‰ (2021) Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, å’Œ Zhihui Li.
    2021. ç¥ç»æ¶æ„æœç´¢çš„ç»¼åˆè°ƒæŸ¥ï¼šæŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆã€‚*ACMè®¡ç®—æœºè°ƒæŸ¥* 54, 4 (2021), 1â€“34.
- en: Roberts and Claridge (2005) MarkÂ E. Roberts and Ela Claridge. 2005. A Multistage
    Approach to Cooperatively Coevolving Feature Construction and Object Detection.
    In *Proc. Appl. Evol. Comput.* 396â€“406.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts å’Œ Claridge (2005) Mark E. Roberts å’Œ Ela Claridge. 2005. åˆä½œå…±è¿›çš„ç‰¹å¾æ„å»ºä¸ç‰©ä½“æ£€æµ‹çš„å¤šé˜¶æ®µæ–¹æ³•ã€‚è§
    *åº”ç”¨è¿›åŒ–è®¡ç®—ä¼šè®®è®ºæ–‡é›†* 396â€“406.
- en: Rostami and Neri (2016) Shahin Rostami and Ferrante Neri. 2016. Covariance Matrix
    Adaptation Pareto Archived Evolution Strategy With Hypervolume-Sorted Adaptive
    Grid Algorithm. *Integr. Comput. Aided. Eng.* 23, 4 (2016), 313â€“329.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rostami å’Œ Neri (2016) Shahin Rostami å’Œ Ferrante Neri. 2016. åæ–¹å·®çŸ©é˜µé€‚åº”å¸•ç´¯æ‰˜å½’æ¡£è¿›åŒ–ç­–ç•¥ä¸è¶…ä½“ç§¯æ’åºè‡ªé€‚åº”ç½‘æ ¼ç®—æ³•ã€‚*é›†æˆè®¡ç®—æœºè¾…åŠ©å·¥ç¨‹*
    23, 4 (2016), 313â€“329.
- en: Rumelhart etÂ al. (1985) DavidÂ E Rumelhart, GeoffreyÂ E Hinton, and RonaldÂ J Williams.
    1985. *Learning Internal Representations by Error Propagation*. Technical Report.
    California Univ San Diego La Jolla Inst for Cognitive Science.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart ç­‰ (1985) David E Rumelhart, Geoffrey E Hinton, å’Œ Ronald J Williams.
    1985. *é€šè¿‡è¯¯å·®ä¼ æ’­å­¦ä¹ å†…éƒ¨è¡¨ç¤º*ã€‚æŠ€æœ¯æŠ¥å‘Šã€‚åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡è®¤çŸ¥ç§‘å­¦ç ”ç©¶æ‰€.
- en: Rumelhart etÂ al. (1986) DavidÂ E Rumelhart, GeoffreyÂ E Hinton, and RonaldÂ J Williams.
    1986. Learning Representations by Back-propagating Errors. *Nature* 323, 6088
    (1986), 533â€“536.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart ç­‰ (1986) David E Rumelhart, Geoffrey E Hinton, å’Œ Ronald J Williams.
    1986. é€šè¿‡è¯¯å·®åå‘ä¼ æ’­å­¦ä¹ è¡¨ç¤ºã€‚*è‡ªç„¶* 323, 6088 (1986), 533â€“536.
- en: 'Rundo etÂ al. (2019) Leonardo Rundo, Andrea Tangherloni, MarcoÂ S Nobile, Carmelo
    Militello, Daniela Besozzi, Giancarlo Mauri, and Paolo Cazzaniga. 2019. MedGA:
    A Novel Evolutionary Method for Image Enhancement in Medical Imaging Systems.
    *Expert Syst. Appl.* 119 (2019), 387â€“399.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rundo ç­‰ (2019) Leonardo Rundo, Andrea Tangherloni, Marco S Nobile, Carmelo
    Militello, Daniela Besozzi, Giancarlo Mauri, å’Œ Paolo Cazzaniga. 2019. MedGA: ä¸€ç§ç”¨äºåŒ»å­¦æˆåƒç³»ç»Ÿä¸­å›¾åƒå¢å¼ºçš„æ–°å‹è¿›åŒ–æ–¹æ³•ã€‚*ä¸“å®¶ç³»ç»Ÿåº”ç”¨*
    119 (2019), 387â€“399.'
- en: Samala etÂ al. (2018) RaviÂ K. Samala, Heang-Ping Chan, LubomirÂ M. Hadjiiski,
    MarkÂ A. Helvie, CalebÂ D. Richter, and KennyÂ H. Cha. 2018. Evolutionary Pruning
    of Transfer Learned Deep Convolutional Neural Network For Breast Cancer Diagnosis
    In Digital Breast Tomosynthesis. *Phys. Med. Biol.* 63, 9 (2018), 095005.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samala et al. (2018) Ravi K. Samala, Heang-Ping Chan, Lubomir M. Hadjiiski,
    Mark A. Helvie, Caleb D. Richter, å’Œ Kenny H. Cha. 2018. ç”¨äºæ•°å­—ä¹³è…ºæ–­å±‚æ‰«æçš„è¿ç§»å­¦ä¹ æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„è¿›åŒ–å‰ªæã€‚*Phys.
    Med. Biol.* 63, 9 (2018), 095005ã€‚
- en: 'Santra etÂ al. (2021) Santanu Santra, Jun-Wei Hsieh, and Chi-Fang Lin. 2021.
    Gradient Descent Effects on Differential Neural Architecture Search: A Survey.
    *IEEE Access* 9 (2021), 89602â€“89618.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santra et al. (2021) Santanu Santra, Jun-Wei Hsieh, å’Œ Chi-Fang Lin. 2021. æ¢¯åº¦ä¸‹é™å¯¹å·®åˆ†ç¥ç»æ¶æ„æœç´¢çš„å½±å“ï¼šä¸€é¡¹è°ƒæŸ¥ã€‚*IEEE
    Access* 9 (2021), 89602â€“89618ã€‚
- en: Sapra and Pimentel (2020) Dolly Sapra and AndyÂ D Pimentel. 2020. Constrained
    Evolutionary Piecemeal Training to Design Convolutional Neural Networks. In *Proc.
    Int. Conf. Industr., Eng. and Other Appl. of App. Intell. Syst.* 709â€“721.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sapra and Pimentel (2020) Dolly Sapra å’Œ Andy D Pimentel. 2020. å—é™è¿›åŒ–åˆ†æ®µè®­ç»ƒè®¾è®¡å·ç§¯ç¥ç»ç½‘ç»œã€‚åœ¨
    *Proc. Int. Conf. Industr., Eng. and Other Appl. of App. Intell. Syst.* ä¸­ã€‚709â€“721ã€‚
- en: Schorn etÂ al. (2020) Christoph Schorn, Thomas Elsken, Sebastian Vogel, and Armin
    Runge. 2020. Automated Design Of Error-Resilient and Hardware-Efficient Deep Neural
    Networks. *Neural. Comput. Appl.* 32, 24 (2020), 18327â€“18345.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schorn et al. (2020) Christoph Schorn, Thomas Elsken, Sebastian Vogel, å’Œ Armin
    Runge. 2020. è‡ªåŠ¨è®¾è®¡é”™è¯¯å¼¹æ€§å’Œç¡¬ä»¶é«˜æ•ˆçš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚*Neural. Comput. Appl.* 32, 24 (2020), 18327â€“18345ã€‚
- en: Sciuto etÂ al. (2020) Christian Sciuto, Kaicheng Yu, Martin Jaggi, ClaudiuÂ Cristian
    Musat, and Mathieu Salzmann. 2020. Evaluating the Search Phase of Neural Architecture
    Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1902.08142.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sciuto et al. (2020) Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Cristian
    Musat, å’Œ Mathieu Salzmann. 2020. è¯„ä¼°ç¥ç»æ¶æ„æœç´¢çš„æœç´¢é˜¶æ®µã€‚åœ¨ *Proc. Int. Conf. Learn. Represent.*
    ä¸­ã€‚https://arxiv.org/abs/1902.08142ã€‚
- en: 'Shafti and PÃ©rez (2008) LeilaÂ Shila Shafti and E.Â Islas PÃ©rez. 2008. Data Reduction
    by Genetic Algorithms and Non-Algebraic Feature Construction: A Case Study. *Proc.
    Int. Conf. Hybri. Intell. Syst.* (2008), 573â€“578.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shafti and PÃ©rez (2008) Leila Shila Shafti å’Œ E. Islas PÃ©rez. 2008. é€šè¿‡é—ä¼ ç®—æ³•å’Œéä»£æ•°ç‰¹å¾æ„é€ çš„æ•°æ®å‡å°‘ï¼šæ¡ˆä¾‹ç ”ç©¶ã€‚åœ¨
    *Proc. Int. Conf. Hybri. Intell. Syst.* ä¸­ã€‚(2008), 573â€“578ã€‚
- en: Shakya etÂ al. (2021) Pratistha Shakya, Eamonn Kennedy, Christopher Rose, and
    JacobÂ K. Rotein. 2021. High-Dimensional Time Series Feature Extraction for Low-Cost
    Machine Olfaction. *IEEE Sens. J.* 21, 3 (2021), 2495â€“2504.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shakya et al. (2021) Pratistha Shakya, Eamonn Kennedy, Christopher Rose, å’Œ Jacob
    K. Rotein. 2021. ä½æˆæœ¬æœºå™¨å—…è§‰çš„é«˜ç»´æ—¶é—´åºåˆ—ç‰¹å¾æå–ã€‚*IEEE Sens. J.* 21, 3 (2021), 2495â€“2504ã€‚
- en: Shang etÂ al. (2022) Haopu Shang, Jia-Liang Wu, Wenjing Hong, and Chao Qian.
    2022. Neural Network Pruning by Cooperative Coevolution. *arXiv preprint arXiv:2204.05639*
    (2022).
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang et al. (2022) Haopu Shang, Jia-Liang Wu, Wenjing Hong, å’Œ Chao Qian. 2022.
    é€šè¿‡åˆä½œå…±è¿›åŒ–è¿›è¡Œç¥ç»ç½‘ç»œå‰ªæã€‚*arXiv preprint arXiv:2204.05639* (2022)ã€‚
- en: Shen etÂ al. (2019) Mingzhu Shen, Kai Han, Chunjing Xu, and Yunhe Wang. 2019.
    Searching for Accurate Binary Neural Architectures. In *Proc. IEEE Int. Conf.
    Comput. Vis.* 2041â€“2044.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2019) Mingzhu Shen, Kai Han, Chunjing Xu, å’Œ Yunhe Wang. 2019. å¯»æ‰¾å‡†ç¡®çš„äºŒè¿›åˆ¶ç¥ç»æ¶æ„ã€‚åœ¨
    *Proc. IEEE Int. Conf. Comput. Vis.* ä¸­ã€‚2041â€“2044ã€‚
- en: Siems etÂ al. (2020) Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik,
    Margret Keuper, and Frank Hutter. 2020. NAS-bench-301 and The Case for Surrogate
    Benchmarks for Neural Architecture Search. *arXiv preprint arXiv:2008.09777* (2020).
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siems et al. (2020) Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik,
    Margret Keuper, å’Œ Frank Hutter. 2020. NAS-bench-301 å’Œç¥ç»æ¶æ„æœç´¢çš„æ›¿ä»£åŸºå‡†çš„æ¡ˆä¾‹ã€‚*arXiv preprint
    arXiv:2008.09777* (2020)ã€‚
- en: Sikdar etÂ al. (2012) UtpalÂ Kumar Sikdar, Asif Ekbal, and Sriparna Saha. 2012.
    Differential Evolution Based Feature Selection and Classifier Ensemble for Named
    Entity Recognition. In *Proc. COLING*. 2475â€“2490.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sikdar et al. (2012) Utpal Kumar Sikdar, Asif Ekbal, å’Œ Sriparna Saha. 2012.
    åŸºäºå·®åˆ†è¿›åŒ–çš„ç‰¹å¾é€‰æ‹©å’Œåˆ†ç±»å™¨é›†æˆç”¨äºå‘½åå®ä½“è¯†åˆ«ã€‚åœ¨ *Proc. COLING* ä¸­ã€‚2475â€“2490ã€‚
- en: Smith and Jin (2014) Christopher Smith and Yaochu Jin. 2014. Evolutionary Multi-objective
    Generation of Recurrent Neural Network Ensembles for Time Series Prediction. *Neurocomputing*
    143 (2014), 302â€“311.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith and Jin (2014) Christopher Smith å’Œ Yaochu Jin. 2014. ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„è¿›åŒ–å¤šç›®æ ‡ç”Ÿæˆé€’å½’ç¥ç»ç½‘ç»œé›†æˆã€‚*Neurocomputing*
    143 (2014), 302â€“311ã€‚
- en: 'Socha and Blum (2007) Krzysztof Socha and Christian Blum. 2007. An Ant Colony
    Optimization Algorithm for Continuous Optimization: Application to Feed-forward
    Neural Network Training. *Neural. Comput. Appl.* 16, 3 (2007), 235â€“247.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socha and Blum (2007) Krzysztof Socha å’Œ Christian Blum. 2007. ä¸€ç§ç”¨äºè¿ç»­ä¼˜åŒ–çš„èšç¾¤ä¼˜åŒ–ç®—æ³•ï¼šåº”ç”¨äºå‰é¦ˆç¥ç»ç½‘ç»œè®­ç»ƒã€‚*Neural.
    Comput. Appl.* 16, 3 (2007), 235â€“247ã€‚
- en: Song etÂ al. (2020) Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu, and
    Yunhe Wang. 2020. Efficient Residual Dense Block Search for Image Super-Resolution.
    In *Proc. AAAI Conf. Artif. Intell.*, Vol.Â 34\. 12007â€“12014.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song ç­‰ï¼ˆ2020ï¼‰Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu å’Œ Yunhe Wang.
    2020. é«˜æ•ˆæ®‹å·®å¯†é›†å—æœç´¢ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚è§ *Proc. AAAI Conf. Artif. Intell.*, ç¬¬ 34 å·. 12007â€“12014.
- en: Song (2021) Xin Song. 2021. Intelligent English Translation System Based on
    Evolutionary Multi-Objective Optimization Algorithm. *J. Intell. Fuzzy Syst.*
    40 (2021), 6327â€“6337.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Songï¼ˆ2021ï¼‰Xin Song. 2021. åŸºäºè¿›åŒ–å¤šç›®æ ‡ä¼˜åŒ–ç®—æ³•çš„æ™ºèƒ½è‹±è¯­ç¿»è¯‘ç³»ç»Ÿã€‚*J. Intell. Fuzzy Syst.* 40 (2021),
    6327â€“6337.
- en: Stanley and Miikkulainen (2002) KennethÂ O Stanley and Risto Miikkulainen. 2002.
    Evolving Neural Networks Through Augmenting Topologies. *Evol. Comput.* 10, 2
    (2002), 99â€“127.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stanley å’Œ Miikkulainenï¼ˆ2002ï¼‰Kenneth O Stanley å’Œ Risto Miikkulainen. 2002. é€šè¿‡æ‰©å±•æ‹“æ‰‘æ¼”åŒ–ç¥ç»ç½‘ç»œã€‚*Evol.
    Comput.* 10, 2 (2002), 99â€“127.
- en: Sun etÂ al. (2021) Yanan Sun, Xian Sun, Yuhan Fang, GaryÂ G. Yen, and Yuqiao Liu.
    2021. A Novel Training Protocol for Performance Predictors of Evolutionary Neural
    Architecture Search Algorithms. *IEEE Trans. Evol. Comput.* 25, 3 (2021), 524â€“536.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun ç­‰ï¼ˆ2021ï¼‰Yanan Sun, Xian Sun, Yuhan Fang, Gary G. Yen å’Œ Yuqiao Liu. 2021.
    ç”¨äºè¿›åŒ–ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ç®—æ³•æ€§èƒ½é¢„æµ‹å™¨çš„æ–°è®­ç»ƒåè®®ã€‚*IEEE Trans. Evol. Comput.* 25, 3 (2021), 524â€“536.
- en: Sun etÂ al. (2019a) Yanan Sun, Handing Wang, Bing Xue, Yaochu Jin, GaryÂ G Yen,
    and Mengjie Zhang. 2019a. Surrogate-Assisted Evolutionary Deep Learning Using
    an End-To-End Random Forest-Based Performance Predictor. *IEEE Trans. Evol. Comput.*
    24, 2 (2019), 350â€“364.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun ç­‰ï¼ˆ2019aï¼‰Yanan Sun, Handing Wang, Bing Xue, Yaochu Jin, Gary G Yen å’Œ Mengjie
    Zhang. 2019a. ä½¿ç”¨ç«¯åˆ°ç«¯éšæœºæ£®æ—æ€§èƒ½é¢„æµ‹å™¨çš„ä»£ç†è¾…åŠ©è¿›åŒ–æ·±åº¦å­¦ä¹ ã€‚*IEEE Trans. Evol. Comput.* 24, 2 (2019),
    350â€“364.
- en: Sun etÂ al. (2019b) Yanan Sun, Bing Xue, Mengjie Zhang, and GaryÂ G Yen. 2019b.
    Completely Automated CNN Architecture Design Based on Blocks. *IEEE Trans. Neural
    Netw. Learn. Syst.* 31, 4 (2019), 1242â€“1254.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun ç­‰ï¼ˆ2019bï¼‰Yanan Sun, Bing Xue, Mengjie Zhang å’Œ Gary G Yen. 2019b. åŸºäºå—çš„å®Œå…¨è‡ªåŠ¨åŒ–
    CNN æ¶æ„è®¾è®¡ã€‚*IEEE Trans. Neural Netw. Learn. Syst.* 31, 4 (2019), 1242â€“1254.
- en: Sun etÂ al. (2019c) Yanan Sun, Bing Xue, Mengjie Zhang, and GaryÂ G Yen. 2019c.
    Evolving Deep Convolutional Neural Networks for Image Classification. *IEEE Trans.
    Evol. Comput.* 24, 2 (2019), 394â€“407.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun ç­‰ï¼ˆ2019cï¼‰Yanan Sun, Bing Xue, Mengjie Zhang å’Œ Gary G Yen. 2019c. ä¸ºå›¾åƒåˆ†ç±»æ¼”åŒ–æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œã€‚*IEEE
    Trans. Evol. Comput.* 24, 2 (2019), 394â€“407.
- en: Sun etÂ al. (2019d) Yanan Sun, Bing Xue, Mengjie Zhang, and GaryÂ G. Yen. 2019d.
    A Particle Swarm Optimization-Based Flexible Convolutional Autoencoder for Image
    Classification. *IEEE Trans. Neural Netw. Learn. Syst.* 30, 8 (2019), 2295â€“2309.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun ç­‰ï¼ˆ2019dï¼‰Yanan Sun, Bing Xue, Mengjie Zhang å’Œ Gary G. Yen. 2019d. åŸºäºç²’å­ç¾¤ä¼˜åŒ–çš„çµæ´»å·ç§¯è‡ªç¼–ç å™¨ç”¨äºå›¾åƒåˆ†ç±»ã€‚*IEEE
    Trans. Neural Netw. Learn. Syst.* 30, 8 (2019), 2295â€“2309.
- en: Sun etÂ al. (2020) Yanan Sun, Bing Xue, Mengjie Zhang, GaryÂ G Yen, and Jiancheng
    Lv. 2020. Automatically Designing CNN Architectures Using the Genetic Algorithm
    for Image Classification. *IEEE Trans. Cybern.* 50, 9 (2020), 3840â€“3854.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun ç­‰ï¼ˆ2020ï¼‰Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen å’Œ Jiancheng Lv. 2020.
    ä½¿ç”¨é—ä¼ ç®—æ³•è‡ªåŠ¨è®¾è®¡ CNN æ¶æ„ç”¨äºå›¾åƒåˆ†ç±»ã€‚*IEEE Trans. Cybern.* 50, 9 (2020), 3840â€“3854.
- en: Swaminathan etÂ al. (2020) Sridhar Swaminathan, Deepak Garg, Rajkumar Kannan,
    and FrÃ©dÃ©ric AndrÃ¨s. 2020. Sparse Low Rank Factorization for Deep Neural Network
    compression. *Neurocomputing* 398 (2020), 185â€“196.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swaminathan ç­‰ï¼ˆ2020ï¼‰Sridhar Swaminathan, Deepak Garg, Rajkumar Kannan å’Œ FrÃ©dÃ©ric
    AndrÃ¨s. 2020. ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œå‹ç¼©çš„ç¨€ç–ä½ç§©åˆ†è§£ã€‚*Neurocomputing* 398 (2020), 185â€“196.
- en: Tanaka etÂ al. (2016) Tomohiro Tanaka, Takafumi Moriya, and Takahiro Shinozaki.
    2016. Evolutionary Optimization of Long Short-Term Memory Neural Network Language
    Model. *J. Acoust. Soc. Am.* 140, 4 (2016), 3062â€“3062.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanaka ç­‰ï¼ˆ2016ï¼‰Tomohiro Tanaka, Takafumi Moriya å’Œ Takahiro Shinozaki. 2016. é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹çš„è¿›åŒ–ä¼˜åŒ–ã€‚*J.
    Acoust. Soc. Am.* 140, 4 (2016), 3062â€“3062.
- en: Tang etÂ al. (2019) Yajiao Tang, Junkai Ji, Yulin Zhu, Shangce Gao, Zheng Tang,
    and Yuki Todo. 2019. A Differential Evolution-Oriented Pruning Neural Network
    Model for Bankruptcy Prediction. In *Complexity*, Vol.Â 2019. 8682124:1â€“8682124:21.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang ç­‰ï¼ˆ2019ï¼‰Yajiao Tang, Junkai Ji, Yulin Zhu, Shangce Gao, Zheng Tang å’Œ Yuki
    Todo. 2019. é¢å‘å·®åˆ†è¿›åŒ–çš„ç ´äº§é¢„æµ‹ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚åœ¨ *Complexity*, ç¬¬ 2019 å·. 8682124:1â€“8682124:21.
- en: Tariq etÂ al. (2018) Hassan Tariq, Elf Eldridge, and Ian Welch. 2018. An Efficient
    Approach for Feature Construction of High-Dimensional Microarray Data By Random
    Projections. *PLoS ONE* 13, 4 (2018), e0196385.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tariq ç­‰ï¼ˆ2018ï¼‰Hassan Tariq, Elf Eldridge å’Œ Ian Welch. 2018. é€šè¿‡éšæœºæŠ•å½±å¯¹é«˜ç»´å¾®é˜µåˆ—æ•°æ®è¿›è¡Œç‰¹å¾æ„å»ºçš„é«˜æ•ˆæ–¹æ³•ã€‚*PLoS
    ONE* 13, 4 (2018), e0196385.
- en: 'Telikani etÂ al. (2021) Akbar Telikani, Amirhessam Tahmassebi, Wolfgang Banzhaf,
    and AmirÂ H Gandomi. 2021. Evolutionary Machine Learning: A Survey. *ACM Comput.
    Surv.* 54, 8 (2021), 1â€“35.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Telikani ç­‰ (2021) Akbar Telikaniã€Amirhessam Tahmassebiã€Wolfgang Banzhaf å’Œ Amir
    H Gandomiã€‚2021å¹´ã€‚è¿›åŒ–æœºå™¨å­¦ä¹ ï¼šç»¼è¿°ã€‚*ACM Comput. Surv.* 54, 8 (2021), 1â€“35ã€‚
- en: 'Teller and Veloso (1996) Astro Teller and Manuela Veloso. 1996. PADO: A New
    Learning Architecture for Object Recognition. *Symbolic visual learn.* (1996),
    81â€“116.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teller å’Œ Veloso (1996) Astro Teller å’Œ Manuela Velosoã€‚1996å¹´ã€‚PADOï¼šä¸€ç§ç”¨äºç‰©ä½“è¯†åˆ«çš„æ–°å­¦ä¹ æ¶æ„ã€‚*Symbolic
    visual learn.* (1996), 81â€“116ã€‚
- en: Tian etÂ al. (2019) Haiman Tian, ShuChing Chen, MeiLing Shyu, and StuartÂ Harvey
    Rubin. 2019. Automated Neural Network Construction with Similarity Sensitive Evolutionary
    Algorithms. In *Proc. IEEE Int. Conf. Inf. Reuse Integr. Data Sci.* 283â€“290.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian ç­‰ (2019) Haiman Tianã€ShuChing Chenã€MeiLing Shyu å’Œ Stuart Harvey Rubinã€‚2019å¹´ã€‚å…·æœ‰ç›¸ä¼¼æ€§æ•æ„Ÿè¿›åŒ–ç®—æ³•çš„è‡ªåŠ¨ç¥ç»ç½‘ç»œæ„å»ºã€‚åœ¨
    *Proc. IEEE Int. Conf. Inf. Reuse Integr. Data Sci.* 283â€“290ã€‚
- en: Tran etÂ al. (2018) Binh Tran, Bing Xue, and Mengjie Zhang. 2018. A New Representation
    in PSO for Discretization-Based Feature Selection. *IEEE Trans. Cybern.* 48, 6
    (2018), 1733â€“1746.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran ç­‰ (2018) Binh Tranã€Bing Xue å’Œ Mengjie Zhangã€‚2018å¹´ã€‚åœ¨ PSO ä¸­çš„æ–°è¡¨ç¤ºæ–¹æ³•ç”¨äºåŸºäºç¦»æ•£åŒ–çš„ç‰¹å¾é€‰æ‹©ã€‚*IEEE
    Trans. Cybern.* 48, 6 (2018), 1733â€“1746ã€‚
- en: Tran etÂ al. (2019) Binh Tran, Bing Xue, and Mengjie Zhang. 2019. Genetic Programming
    for Multiple-Feature Construction on High-Dimensional Classification. *Pattern
    Recognit.* 93 (2019), 404â€“417.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran ç­‰ (2019) Binh Tranã€Bing Xue å’Œ Mengjie Zhangã€‚2019å¹´ã€‚ç”¨äºé«˜ç»´åˆ†ç±»çš„å¤šç‰¹å¾æ„å»ºçš„é—ä¼ ç¼–ç¨‹ã€‚*Pattern
    Recognit.* 93 (2019), 404â€“417ã€‚
- en: Tran etÂ al. (2016) Binh Tran, Mengjie Zhang, and Bing Xue. 2016. Multiple Feature
    Construction in Classification on High-Dimensional Data Using GP. In *IEEE Symposium
    Series on Computational Intelligence*. 1â€“8.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran ç­‰ (2016) Binh Tranã€Mengjie Zhang å’Œ Bing Xueã€‚2016å¹´ã€‚åœ¨é«˜ç»´æ•°æ®åˆ†ç±»ä¸­ä½¿ç”¨ GP è¿›è¡Œå¤šç‰¹å¾æ„å»ºã€‚åœ¨
    *IEEE Symposium Series on Computational Intelligence*ã€‚1â€“8ã€‚
- en: Vafaie and DeÂ Jong (1998) Haleh Vafaie and Kenneth DeÂ Jong. 1998. Feature Space
    Transformation Using Genetic Algorithms. *IEEE Intell. Syst. Appli.* 13, 2 (1998),
    57â€“65.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vafaie å’Œ De Jong (1998) Haleh Vafaie å’Œ Kenneth De Jongã€‚1998å¹´ã€‚åˆ©ç”¨é—ä¼ ç®—æ³•è¿›è¡Œç‰¹å¾ç©ºé—´è½¬æ¢ã€‚*IEEE
    Intell. Syst. Appli.* 13, 2 (1998), 57â€“65ã€‚
- en: Vargas-HÃ¡kim etÂ al. (2022) GustavoÂ A Vargas-HÃ¡kim, EfrÃ©n Mezura-Montes, and
    HÃ©ctor-Gabriel Acosta-Mesa. 2022. A Review on Convolutional Neural Networks Encodings
    for Neuroevolution. *IEEE Trans. Evol. Comput.* 26, 1 (2022), 12â€“27.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vargas-HÃ¡kim ç­‰ (2022) Gustavo A Vargas-HÃ¡kimã€EfrÃ©n Mezura-Montes å’Œ HÃ©ctor-Gabriel
    Acosta-Mesaã€‚2022å¹´ã€‚å…³äºç”¨äºç¥ç»è¿›åŒ–çš„å·ç§¯ç¥ç»ç½‘ç»œç¼–ç çš„ç»¼è¿°ã€‚*IEEE Trans. Evol. Comput.* 26, 1 (2022),
    12â€“27ã€‚
- en: Vieira etÂ al. (2013) SusanaÂ M Vieira, LuÃ­sÂ F MendonÃ§a, GoncaloÂ J Farinha, and
    JoÃ£oÂ MC Sousa. 2013. Modified Binary PSO for Feature Selection Using SVM Applied
    to Mortality Prediction of Septic Patients. *Appl. Soft Comput.* 13, 8 (2013),
    3494â€“3504.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieira ç­‰ (2013) Susana M Vieiraã€LuÃ­s F MendonÃ§aã€Goncalo J Farinha å’Œ JoÃ£o MC
    Sousaã€‚2013å¹´ã€‚ä½¿ç”¨ SVM çš„æ”¹è¿›äºŒè¿›åˆ¶ PSO ç‰¹å¾é€‰æ‹©åº”ç”¨äºè„“æ¯’ç—‡æ‚£è€…çš„æ­»äº¡é¢„æµ‹ã€‚*Appl. Soft Comput.* 13, 8 (2013),
    3494â€“3504ã€‚
- en: Wang etÂ al. (2021b) Bin Wang, Wenbin Pei, Bing Xue, and Mengjie Zhang. 2021b.
    Evolving Local Interpretable Model-Agnostic Explanations for Deep Neural Networks
    in Image Classification. In *Proc. Genetic Evol. Comput. Conf.* 173â€“174.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ (2021b) Bin Wangã€Wenbin Peiã€Bing Xue å’Œ Mengjie Zhangã€‚2021bå¹´ã€‚ä¸ºå›¾åƒåˆ†ç±»ä¸­çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¼”åŒ–å±€éƒ¨å¯è§£é‡Šæ¨¡å‹æ— å…³è§£é‡Šã€‚åœ¨
    *Proc. Genetic Evol. Comput. Conf.* 173â€“174ã€‚
- en: 'Wang etÂ al. (2020b) Bin Wang, Bing Xue, and Mengjie Zhang. 2020b. Particle
    Swarm Optimization for Evolving Deep Convolutional Neural Networks for Image Classification:
    Single-and Multi-objective Approaches. In *Deep Neural Evolution*. Springer, 155â€“184.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ (2020b) Bin Wangã€Bing Xue å’Œ Mengjie Zhangã€‚2020bå¹´ã€‚ç”¨äºå›¾åƒåˆ†ç±»çš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„ç²’å­ç¾¤ä¼˜åŒ–ï¼šå•ç›®æ ‡å’Œå¤šç›®æ ‡æ–¹æ³•ã€‚åœ¨
    *Deep Neural Evolution*ã€‚Springerï¼Œ155â€“184ã€‚
- en: Wang etÂ al. (2020c) Bin Wang, Bing Xue, and Mengjie Zhang. 2020c. Particle Swarm
    Optimization for Evolving Deep Neural Networks for Image Classification By Evolving
    and Stacking Transferable Blocks. In *Proc. IEEE Congr. Evol. Comput.* 1â€“8.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ (2020c) Bin Wangã€Bing Xue å’Œ Mengjie Zhangã€‚2020cå¹´ã€‚é€šè¿‡æ¼”åŒ–å’Œå †å å¯è¿ç§»å—è¿›è¡Œå›¾åƒåˆ†ç±»çš„æ·±åº¦ç¥ç»ç½‘ç»œçš„ç²’å­ç¾¤ä¼˜åŒ–ã€‚åœ¨
    *Proc. IEEE Congr. Evol. Comput.* 1â€“8ã€‚
- en: Wang etÂ al. (2021c) Bin Wang, Bing Xue, and Mengjie Zhang. 2021c. Surrogate-Assisted
    Particle Swarm Optimization for Evolving Variable-Length Transferable Blocks for
    Image Classification. *IEEE Trans. Neural Netw. Learn. Syst.* 33, 8 (2021), 3727â€“3740.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ (2021c) Bin Wangã€Bing Xue å’Œ Mengjie Zhangã€‚2021cå¹´ã€‚ç”¨äºå›¾åƒåˆ†ç±»çš„å¯å˜é•¿åº¦å¯è¿ç§»å—çš„ä»£ç†è¾…åŠ©ç²’å­ç¾¤ä¼˜åŒ–ã€‚*IEEE
    Trans. Neural Netw. Learn. Syst.* 33, 8 (2021), 3727â€“3740ã€‚
- en: 'Wang etÂ al. (2020a) Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng
    Zhu, Chuang Gan, and Song Han. 2020a. HAT: Hardware-Aware Transformers for Efficient
    Natural Language Processing. In *Proc. Assoc. Comput. Linguist.* 7675â€“7688.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020a) Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu,
    Chuang Gan, å’Œ Song Han. 2020a. HATï¼šé’ˆå¯¹é«˜æ•ˆè‡ªç„¶è¯­è¨€å¤„ç†çš„ç¡¬ä»¶æ„ŸçŸ¥å˜æ¢å™¨ã€‚åœ¨*Proc. Assoc. Comput. Linguist.*
    7675â€“7688ã€‚
- en: 'Wang etÂ al. (2020d) Xiao-han Wang, Yong Zhang, and Xiao-yan Sun. 2020d. Multi-Objective
    Feature Selection Based on Artificial Bee Colony: An Acceleration Approach With
    Variable Sample Size. *Appl. Soft Comput.* 88 (2020), 106041.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020d) Xiao-han Wang, Yong Zhang, å’Œ Xiao-yan Sun. 2020d. åŸºäºäººå·¥èœ‚ç¾¤çš„å¤šç›®æ ‡ç‰¹å¾é€‰æ‹©ï¼šä¸€ç§å…·æœ‰å¯å˜æ ·æœ¬é‡çš„åŠ é€Ÿæ–¹æ³•ã€‚*Appl.
    Soft Comput.* 88 (2020), 106041ã€‚
- en: Wang etÂ al. (2018) Yunhe Wang, Chang Xu, Jiayan Qiu, Chao Xu, and Dacheng Tao.
    2018. Towards Evolutionary Compression. In *Proc. ACM SIGKDD Int. Conf. Knowl.
    Discov. & Data Min.* 2476â€“2485.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) Yunhe Wang, Chang Xu, Jiayan Qiu, Chao Xu, å’Œ Dacheng Tao.
    2018. æœç€è¿›åŒ–å‹ç¼©çš„æ–¹å‘ã€‚åœ¨*Proc. ACM SIGKDD Int. Conf. Knowl. Discov. & Data Min.* 2476â€“2485ã€‚
- en: Wang etÂ al. (2021a) Zhehui Wang, Tao Luo, Miqing Li, JoeyÂ Tianyi Zhou, Rick
    SiowÂ Mong Goh, and Liangli Zhen. 2021a. Evolutionary Multi-Objective Model Compression
    for Deep Neural Networks. *IEEE Comput. Intell. Mag.* 16, 3 (2021), 10â€“21.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) Zhehui Wang, Tao Luo, Miqing Li, Joey Tianyi Zhou, Rick
    Siow Mong Goh, å’Œ Liangli Zhen. 2021a. ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œçš„è¿›åŒ–å¤šç›®æ ‡æ¨¡å‹å‹ç¼©ã€‚*IEEE Comput. Intell.
    Mag.* 16, 3 (2021), 10â€“21ã€‚
- en: Wen and Xu (2011) Yun Wen and Hua Xu. 2011. A Cooperative Coevolution-Based
    Pittsburgh Learning Classifier System Embedded With Memetic Feature Selection.
    In *Proc. IEEE Congr. Evol. Comput.* 2415â€“2422.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen å’Œ Xu (2011) Yun Wen å’Œ Hua Xu. 2011. ä¸€ç§åŸºäºåä½œå…±è¿›åŒ–çš„åŒ¹å…¹å ¡å­¦ä¹ åˆ†ç±»å™¨ç³»ç»Ÿï¼ŒåµŒå…¥äº†å¯å‘å¼ç‰¹å¾é€‰æ‹©ã€‚åœ¨*Proc.
    IEEE Congr. Evol. Comput.* 2415â€“2422ã€‚
- en: 'White etÂ al. (2021) Colin White, Willie Neiswanger, and Yash Savani. 2021.
    BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture
    Search. In *Proc. AAAI Conf. Artif. Intell.*, Vol.Â 35\. 10293â€“10301.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White et al. (2021) Colin White, Willie Neiswanger, å’Œ Yash Savani. 2021. BANANASï¼šç”¨äºç¥ç»æ¶æ„æœç´¢çš„è´å¶æ–¯ä¼˜åŒ–ä¸ç¥ç»æ¶æ„ã€‚åœ¨*Proc.
    AAAI Conf. Artif. Intell.*, ç¬¬35å·. 10293â€“10301ã€‚
- en: Winata etÂ al. (2019) GentaÂ Indra Winata, Andrea Madotto, Jamin Shin, ElhamÂ J
    Barezi, and Pascale Fung. 2019. On the Effectiveness of Low-rank Matrix Factorization
    for LSTM Model Compression. *arXiv preprint arXiv:1908.09982* (2019).
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Winata et al. (2019) Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J
    Barezi, å’Œ Pascale Fung. 2019. ä½ç§©çŸ©é˜µåˆ†è§£åœ¨LSTMæ¨¡å‹å‹ç¼©ä¸­çš„æœ‰æ•ˆæ€§ã€‚*arXiv preprint arXiv:1908.09982*
    (2019)ã€‚
- en: Wu etÂ al. (2021b) Min Wu, Wanjuan Su, Luefeng Chen, and Zhentao Liu. 2021b.
    Weight-Adapted Convolution Neural Network for Facial Expression Recognition in
    Human-Robot Interaction. *IEEE Trans. Syst. Man Cybern.* 51, 3 (2021), 1473â€“1484.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021b) Min Wu, Wanjuan Su, Luefeng Chen, å’Œ Zhentao Liu. 2021b. é¢å‘äººæœºäº¤äº’çš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«çš„æƒé‡è‡ªé€‚åº”å·ç§¯ç¥ç»ç½‘ç»œã€‚*IEEE
    Trans. Syst. Man Cybern.* 51, 3 (2021), 1473â€“1484ã€‚
- en: Wu etÂ al. (2021a) Tao Wu, Xiaoyang Li, Deyun Zhou, Na Li, and Jiao Shi. 2021a.
    Differential Evolution Based Layer-Wise Weight Pruning for Compressing Deep Neural
    Networks. *Sens.* 21, 3 (2021), 880.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021a) Tao Wu, Xiaoyang Li, Deyun Zhou, Na Li, å’Œ Jiao Shi. 2021a.
    åŸºäºå·®åˆ†è¿›åŒ–çš„é€å±‚æƒé‡å‰ªæç”¨äºå‹ç¼©æ·±åº¦ç¥ç»ç½‘ç»œã€‚*Sens.* 21, 3 (2021), 880ã€‚
- en: Wu etÂ al. (2019) Tao Wu, Jiao Shi, Deyun Zhou, Yu Lei, and Maoguo Gong. 2019.
    A Multi-objective Particle Swarm Optimization for Neural Networks Pruning. In
    *Proc. IEEE Congr. Evol. Comput.* 570â€“577.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019) Tao Wu, Jiao Shi, Deyun Zhou, Yu Lei, å’Œ Maoguo Gong. 2019.
    ç”¨äºç¥ç»ç½‘ç»œå‰ªæçš„å¤šç›®æ ‡ç²’å­ç¾¤ä¼˜åŒ–ã€‚åœ¨*Proc. IEEE Congr. Evol. Comput.* 570â€“577ã€‚
- en: Wu etÂ al. (2020) Xiang Wu, Ran He, Yibo Hu, and Zhenan Sun. 2020. Learning an
    Evolutionary Embedding via Massive Knowledge Distillation. *Int. J. Comput. Vis.*
    128, 8 (2020), 1â€“18.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2020) Xiang Wu, Ran He, Yibo Hu, å’Œ Zhenan Sun. 2020. é€šè¿‡å¤§è§„æ¨¡çŸ¥è¯†è’¸é¦å­¦ä¹ è¿›åŒ–åµŒå…¥ã€‚*Int.
    J. Comput. Vis.* 128, 8 (2020), 1â€“18ã€‚
- en: 'Xie etÂ al. (2022a) Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu,
    Zhengsu Chen, Lanfei Wang, Anxiang Xiao, Jianlong Chang, Xiaopeng Zhang, and Qi
    Tian. 2022a. Weight-Sharing Neural Architecture Search: A Battle to Shrink the
    Optimization Gap. *ACM Comput. Surv.* 54, 9 (2022), 1â€“37.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2022a) Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu,
    Zhengsu Chen, Lanfei Wang, Anxiang Xiao, Jianlong Chang, Xiaopeng Zhang, å’Œ Qi
    Tian. 2022a. æƒé‡å…±äº«ç¥ç»æ¶æ„æœç´¢ï¼šç¼©å°ä¼˜åŒ–å·®è·çš„æˆ˜æ–—ã€‚*ACM Comput. Surv.* 54, 9 (2022), 1â€“37ã€‚
- en: Xie and Yuille (2017) Lingxi Xie and Alan Yuille. 2017. Genetic CNN. In *Proc.
    IEEE Int. Conf. Comput. Vis.* 1379â€“1388.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie å’Œ Yuille (2017) Lingxi Xie å’Œ Alan Yuille. 2017. é—ä¼ å·ç§¯ç¥ç»ç½‘ç»œã€‚åœ¨*Proc. IEEE Int.
    Conf. Comput. Vis.* 1379â€“1388ã€‚
- en: 'Xie etÂ al. (2022b) Xiangning Xie, Yuqiao Liu, Yanan Sun, GaryÂ G. Yen, Bing
    Xue, and Mengjie Zhang. 2022b. BenchENAS: A Benchmarking Platform for Evolutionary
    Neural Architecture Search. *IEEE Trans. Evol. Comput.* (2022). [https://doi.org/10.1109/TEVC.2022.3147526](https://doi.org/10.1109/TEVC.2022.3147526)'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2022b) Xiangning Xie, Yuqiao Liu, Yanan Sun, Gary G. Yen, Bing Xue,
    å’Œ Mengjie Zhang. 2022b. BenchENASï¼šç”¨äºè¿›åŒ–ç¥ç»æ¶æ„æœç´¢çš„åŸºå‡†å¹³å°ã€‚*IEEE Trans. Evol. Comput.*
    (2022). [https://doi.org/10.1109/TEVC.2022.3147526](https://doi.org/10.1109/TEVC.2022.3147526)
- en: 'Xu etÂ al. (2021) Ke Xu, Dezheng Zhang, Jianjing An, Li Liu, Lingzhi Liu, and
    Dong Wang. 2021. GenExp: Multi-objective Pruning for Deep Neural Network based
    on Genetic Algorithm. *Neurocomputing* 451 (2021), 81â€“94.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021) Ke Xu, Dezheng Zhang, Jianjing An, Li Liu, Lingzhi Liu, å’Œ Dong
    Wang. 2021. GenExpï¼šåŸºäºé—ä¼ ç®—æ³•çš„æ·±åº¦ç¥ç»ç½‘ç»œå¤šç›®æ ‡å‰ªæã€‚*Neurocomputing* 451 (2021), 81â€“94ã€‚
- en: Xue etÂ al. (2012) Bing Xue, Mengjie Zhang, and WillÂ N Browne. 2012. Multi-Objective
    Particle Swarm Optimization (PSO) for Feature Selection. In *Proc. Genetic Evol.
    Comput. Conf.* 81â€“88.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue et al. (2012) Bing Xue, Mengjie Zhang, å’Œ Will N Browne. 2012. ç”¨äºç‰¹å¾é€‰æ‹©çš„å¤šç›®æ ‡ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆPSOï¼‰ã€‚è§äº
    *Proc. Genetic Evol. Comput. Conf.* 81â€“88ã€‚
- en: Xue etÂ al. (2015) Bing Xue, Mengjie Zhang, WillÂ N Browne, and Xin Yao. 2015.
    A Survey on Evolutionary Computation Approaches to Feature Selection. *IEEE Trans.
    Evol. Comput.* 20, 4 (2015), 606â€“626.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue et al. (2015) Bing Xue, Mengjie Zhang, Will N Browne, å’Œ Xin Yao. 2015. å…³äºè¿›åŒ–è®¡ç®—æ–¹æ³•åœ¨ç‰¹å¾é€‰æ‹©ä¸­çš„åº”ç”¨çš„ç»¼è¿°ã€‚*IEEE
    Trans. Evol. Comput.* 20, 4 (2015), 606â€“626ã€‚
- en: Xue etÂ al. (2013) Bing Xue, Mengjie Zhang, Yan Dai, and WillÂ N Browne. 2013.
    PSO for Feature Construction and Binary Classification. In *Proc. Genetic Evol.
    Comput. Conf.* 137â€“144.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue et al. (2013) Bing Xue, Mengjie Zhang, Yan Dai, å’Œ Will N Browne. 2013. ç”¨äºç‰¹å¾æ„å»ºå’ŒäºŒåˆ†ç±»çš„PSOã€‚è§äº
    *Proc. Genetic Evol. Comput. Conf.* 137â€“144ã€‚
- en: Yang etÂ al. (2019) Chuanguang Yang, Zhulin An, Chao Li, Boyu Diao, and Yongjun
    Xu. 2019. Multi-objective Pruning for CNNs Using Genetic Algorithm. In *Proc.
    Int. Conf. Artif. Neural Netw.* 299â€“305.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) Chuanguang Yang, Zhulin An, Chao Li, Boyu Diao, å’Œ Yongjun
    Xu. 2019. ä½¿ç”¨é—ä¼ ç®—æ³•çš„CNNå¤šç›®æ ‡å‰ªæã€‚è§äº *Proc. Int. Conf. Artif. Neural Netw.* 299â€“305ã€‚
- en: Yang etÂ al. (2021) Shangshang Yang, Ye Tian, Cheng He, Xingyi Zhang, KayÂ Chen
    Tan, and Yaochu Jin. 2021. A Gradient-Guided Evolutionary Approach to Training
    Deep Neural Networks. *IEEE Trans. Neural Netw. Learn. Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3061630](https://doi.org/10.1109/TNNLS.2021.3061630)
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2021) Shangshang Yang, Ye Tian, Cheng He, Xingyi Zhang, Kay Chen
    Tan, å’Œ Yaochu Jin. 2021. ä¸€ç§æ¢¯åº¦å¼•å¯¼çš„è¿›åŒ–æ–¹æ³•ç”¨äºè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œã€‚*IEEE Trans. Neural Netw. Learn.
    Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3061630](https://doi.org/10.1109/TNNLS.2021.3061630)
- en: 'Yang etÂ al. (2020) Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, and Chao
    Xu. 2020. CARS: Continuous Evolution for Efficient Neural Architecture Search.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 1826â€“1835.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2020) Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, å’Œ Chao
    Xu. 2020. CARSï¼šç”¨äºé«˜æ•ˆç¥ç»æ¶æ„æœç´¢çš„è¿ç»­è¿›åŒ–ã€‚è§äº *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*
    1826â€“1835ã€‚
- en: 'Yao etÂ al. (2018) Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, and
    Yu-Feng Li. 2018. Taking Human out of Learning Applications: A Survey on Automated
    Machine Learning. *arXiv preprint arXiv:1810.13306* (2018).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2018) Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, å’Œ
    Yu-Feng Li. 2018. ä»å­¦ä¹ åº”ç”¨ä¸­å»é™¤äººä¸ºå› ç´ ï¼šè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ çš„ç»¼è¿°ã€‚*arXiv preprint arXiv:1810.13306* (2018)ã€‚
- en: Yao (1993) Xin Yao. 1993. A Review of Evolutionary Artificial Neural Networks.
    *Int. J. Intell. Syst.* 8, 4 (1993), 539â€“567.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao (1993) Xin Yao. 1993. è¿›åŒ–äººå·¥ç¥ç»ç½‘ç»œçš„ç»¼è¿°ã€‚*Int. J. Intell. Syst.* 8, 4 (1993), 539â€“567ã€‚
- en: Yao and Liu (1996) Xin Yao and Yong Liu. 1996. Ensemble Structure of Evolutionary
    Artificial Neural Networks. In *Proc. Genetic Evol. Comput. Conf.* 659â€“664.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao and Liu (1996) Xin Yao å’Œ Yong Liu. 1996. è¿›åŒ–äººå·¥ç¥ç»ç½‘ç»œçš„é›†æˆç»“æ„ã€‚è§äº *Proc. Genetic
    Evol. Comput. Conf.* 659â€“664ã€‚
- en: 'Ying etÂ al. (2019) Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen,
    KevinÂ P. Murphy, and Frank Hutter. 2019. NAS-Bench-101: Towards Reproducible Neural
    Architecture Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1902.09635.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ying et al. (2019) Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen,
    Kevin P. Murphy, å’Œ Frank Hutter. 2019. NAS-Bench-101ï¼šè¿ˆå‘å¯é‡å¤çš„ç¥ç»æ¶æ„æœç´¢ã€‚è§äº *Proc. Int.
    Conf. Learn. Represent.* https://arxiv.org/abs/1902.09635ã€‚
- en: Young etÂ al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik
    Cambria. 2018. Recent Trends in Deep Learning Based Natural Language Processing
    [Review Article]. *IEEE Comput. Intell. Mag.* 13, 3 (2018), 55â€“75.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, å’Œ Erik Cambria.
    2018. åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†çš„æœ€æ–°è¶‹åŠ¿ [ç»¼è¿°æ–‡ç« ]ã€‚*IEEE Comput. Intell. Mag.* 13, 3 (2018), 55â€“75ã€‚
- en: Yu etÂ al. (2009) Hualong Yu, Guochang Gu, Haibo Liu, Jing Shen, and Jing Zhao.
    2009. A modified Ant Colony Optimization Algorithm for Tumor Marker Gene Selection.
    *Genomics, Proteomics & Bioinformatics* 7, 4 (2009), 200â€“208.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu ç­‰ (2009) Hualong Yu, Guochang Gu, Haibo Liu, Jing Shen, å’Œ Jing Zhao. 2009.
    ä¸€ç§æ”¹è¿›çš„èšç¾¤ä¼˜åŒ–ç®—æ³•ç”¨äºè‚¿ç˜¤æ ‡å¿—åŸºå› é€‰æ‹©ã€‚*åŸºå› ç»„å­¦ã€è›‹ç™½è´¨ç»„å­¦ä¸ç”Ÿç‰©ä¿¡æ¯å­¦* 7, 4 (2009), 200â€“208ã€‚
- en: Z.-Flores etÂ al. (2020) Emigdio Z.-Flores, Leonardo Trujillo, Pierrick Legrand,
    and FrÃ©dÃ©rique FaÃ¯ta-AÃ¯nseba. 2020. EEG Feature Extraction Using Genetic Programming
    for the Classification of Mental States. *Algorithms* 13, 9 (2020), 221.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z.-Flores ç­‰ (2020) Emigdio Z.-Flores, Leonardo Trujillo, Pierrick Legrand, å’Œ
    FrÃ©dÃ©rique FaÃ¯ta-AÃ¯nseba. 2020. ä½¿ç”¨é—ä¼ ç¼–ç¨‹çš„è„‘ç”µå›¾ç‰¹å¾æå–ç”¨äºå¿ƒç†çŠ¶æ€åˆ†ç±»ã€‚*ç®—æ³•* 13, 9 (2020), 221ã€‚
- en: Zemouri etÂ al. (2019) RyadÂ A. Zemouri, N. Omri, Farhat Fnaiech, Noureddine Zerhouni,
    and Nader Fnaiech. 2019. A New Growing Pruning Deep Learning Neural Network Algorithm
    (GP-DLNN). *Neural. Comput. Appl.* 32 (2019), 1â€“17.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zemouri ç­‰ (2019) Ryad A. Zemouri, N. Omri, Farhat Fnaiech, Noureddine Zerhouni,
    å’Œ Nader Fnaiech. 2019. ä¸€ç§æ–°çš„ç”Ÿé•¿å‰ªææ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œç®—æ³• (GP-DLNN)ã€‚*ç¥ç»è®¡ç®—åº”ç”¨* 32 (2019), 1â€“17ã€‚
- en: Zhan etÂ al. (2021) Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu
    Wu, Tianyun Zhang, Malith Jayaweera, DavidÂ R. Kaeli, Bin Ren, Xue Lin, and Yanzhi
    Wang. 2021. Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture
    and Pruning Search. In *Proc. IEEE Int. Conf. Comput. Vis.* 4801â€“4811.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhan ç­‰ (2021) Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu,
    Tianyun Zhang, Malith Jayaweera, David R. Kaeli, Bin Ren, Xue Lin, å’Œ Yanzhi Wang.
    2021. åŸºäºç§»åŠ¨è®¾å¤‡çš„å®æ—¶è¶…åˆ†è¾¨ç‡å®ç°ï¼Œç»“åˆç¥ç»æ¶æ„ä¸å‰ªææœç´¢ã€‚åœ¨ *IEEE å›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†* 4801â€“4811ã€‚
- en: Zhang and MÃ¼hlenbein (1995) Byoung-Tak Zhang and Heinz MÃ¼hlenbein. 1995. Balancing
    Accuracy and Parsimony in Genetic Programming. *Evol. Comput.* 3, 1 (1995), 17â€“38.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang å’Œ MÃ¼hlenbein (1995) Byoung-Tak Zhang å’Œ Heinz MÃ¼hlenbein. 1995. åœ¨é—ä¼ ç¼–ç¨‹ä¸­å¹³è¡¡å‡†ç¡®æ€§å’Œç®€çº¦æ€§ã€‚*è¿›åŒ–è®¡ç®—*
    3, 1 (1995), 17â€“38ã€‚
- en: Zhang etÂ al. (2022b) Di Zhang, Yichen Zhou, Jiaqi Zhao, and Yong Zhou. 2022b.
    Co-evolution-based Parameter Learning for Remote Sensing Scene Classification.
    *Int. J. Wavelets Multiresolut. Inf. Process.* 20, 2 (2022), 2150046.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ (2022b) Di Zhang, Yichen Zhou, Jiaqi Zhao, å’Œ Yong Zhou. 2022b. åŸºäºå…±è¿›åŒ–çš„é¥æ„Ÿåœºæ™¯åˆ†ç±»å‚æ•°å­¦ä¹ ã€‚*å›½é™…å°æ³¢å¤šåˆ†è¾¨ç‡ä¿¡æ¯å¤„ç†æ‚å¿—*
    20, 2 (2022), 2150046ã€‚
- en: Zhang etÂ al. (2020b) Haoling Zhang, Chao-HanÂ Huck Yang, Hector Zenil, NarsisÂ Aftab
    Kiani, Yue Shen, and JesperÂ N. Tegner. 2020b. Evolving Neural Networks through
    a Reverse Encoding Tree. In *Proc. IEEE Congr. Evol. Comput.* 1â€“10.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ (2020b) Haoling Zhang, Chao-Han Huck Yang, Hector Zenil, Narsis Aftab
    Kiani, Yue Shen, å’Œ Jesper N. Tegner. 2020b. é€šè¿‡åå‘ç¼–ç æ ‘è¿›åŒ–ç¥ç»ç½‘ç»œã€‚åœ¨ *IEEE è¿›åŒ–è®¡ç®—å¤§ä¼šè®ºæ–‡é›†* 1â€“10ã€‚
- en: 'Zhang and Gouza (2018) Jiawei Zhang and FisherÂ B Gouza. 2018. GADAM: Genetic-evolutionary
    ADAM for Deep Neural Network optimization. *arXiv preprint arXiv:1805.07500* (2018).'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang å’Œ Gouza (2018) Jiawei Zhang å’Œ Fisher B Gouza. 2018. GADAMï¼šç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œä¼˜åŒ–çš„é—ä¼ è¿›åŒ–ADAMã€‚*arXiv
    é¢„å°æœ¬ arXiv:1805.07500* (2018)ã€‚
- en: 'Zhang etÂ al. (2011) Jun Zhang, Zhi-hui Zhan, Ying Lin, Ni Chen, Yue-jiao Gong,
    Jing-hui Zhong, HenryÂ SH Chung, Yun Li, and Yu-hui Shi. 2011. Evolutionary Computation
    Meets Machine Learning: A Survey. *IEEE Comput. Intell. Mag.* 6, 4 (2011), 68â€“75.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ (2011) Jun Zhang, Zhi-hui Zhan, Ying Lin, Ni Chen, Yue-jiao Gong, Jing-hui
    Zhong, Henry SH Chung, Yun Li, å’Œ Yu-hui Shi. 2011. è¿›åŒ–è®¡ç®—ä¸æœºå™¨å­¦ä¹ çš„äº¤æ±‡ï¼šä¸€é¡¹è°ƒæŸ¥ã€‚*IEEE è®¡ç®—æ™ºèƒ½æ‚å¿—*
    6, 4 (2011), 68â€“75ã€‚
- en: Zhang etÂ al. (2021a) Kaiyu Zhang, Jinglong Chen, Shuilong He, Enyong Xu, Fudong
    Li, and Zitong Zhou. 2021a. Differentiable Neural Architecture Search Augmented
    with Pruning and Multi-objective Optimization for Time-efficient Intelligent Fault
    Diagnosis of Machinery. *Mech. Syst. Signal Process.* 158 (2021), 107773.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ (2021a) Kaiyu Zhang, Jinglong Chen, Shuilong He, Enyong Xu, Fudong Li,
    å’Œ Zitong Zhou. 2021a. å¢å¼ºçš„å¯å¾®ç¥ç»æ¶æ„æœç´¢ä¸å‰ªæå’Œå¤šç›®æ ‡ä¼˜åŒ–ï¼Œç”¨äºæœºæ¢°è®¾å¤‡çš„æ—¶é—´é«˜æ•ˆæ™ºèƒ½æ•…éšœè¯Šæ–­ã€‚*æœºæ¢°ç³»ç»Ÿä¿¡å·å¤„ç†* 158 (2021),
    107773ã€‚
- en: Zhang etÂ al. (2022a) Kangkai Zhang, Chunhui Zhang, Shikun Li, Dan Zeng, and
    Shiming Ge. 2022a. Student Network Learning via Evolutionary Knowledge Distillation.
    *IEEE Trans. Circuits. Syst. Video Technol.* 32, 4 (2022), 2251â€“2263.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ (2022a) Kangkai Zhang, Chunhui Zhang, Shikun Li, Dan Zeng, å’Œ Shiming
    Ge. 2022a. é€šè¿‡è¿›åŒ–çŸ¥è¯†è’¸é¦è¿›è¡Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ ã€‚*IEEE ç”µè·¯ä¸ç³»ç»Ÿè§†é¢‘æŠ€æœ¯æ±‡åˆŠ* 32, 4 (2022), 2251â€“2263ã€‚
- en: Zhang (2018) Mengjie Zhang. 2018. Evolutionary Deep Learning for Image Analysis.
    (2018), https://ieeetv.ieee.org/mengjieâ€“zhangâ€“evolutionaryâ€“deepâ€“learningâ€“forâ€“imageâ€“analysis.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang (2018) Mengjie Zhang. 2018. å›¾åƒåˆ†æçš„è¿›åŒ–æ·±åº¦å­¦ä¹ ã€‚ (2018), https://ieeetv.ieee.org/mengjieâ€“zhangâ€“evolutionaryâ€“deepâ€“learningâ€“forâ€“imageâ€“analysisã€‚
- en: Zhang and Cagnoni (2020) Mengjie Zhang and Stefano Cagnoni. 2020. Evolutionary
    Computation and Evolutionary Deep Learning for Image Analysis, Signal Processing
    and Pattern Recognition. In *Proc. Genetic Evol. Comput. Conf.* 1221â€“1257.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang å’Œ Cagnoni (2020) Mengjie Zhang å’Œ Stefano Cagnoni. 2020. å›¾åƒåˆ†æã€ä¿¡å·å¤„ç†å’Œæ¨¡å¼è¯†åˆ«ä¸­çš„æ¼”åŒ–è®¡ç®—ä¸æ¼”åŒ–æ·±åº¦å­¦ä¹ ã€‚è§äº
    *Proc. Genetic Evol. Comput. Conf.* 1221â€“1257ã€‚
- en: Zhang etÂ al. (2020a) Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, and StevenÂ W.
    Su. 2020a. Overcoming Multi-Model Forgetting in One-Shot NAS With Diversity Maximization.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 7806â€“7815.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ (2020a) Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, å’Œ Steven W.
    Su. 2020a. é€šè¿‡å¤šæ ·æ€§æœ€å¤§åŒ–å…‹æœä¸€-shot NAS ä¸­çš„å¤šæ¨¡å‹é—å¿˜ã€‚è§äº *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.* 7806â€“7815ã€‚
- en: Zhang and Smart (2004) Mengjie Zhang and Will Smart. 2004. Genetic Programming
    with Gradient Descent Search for Multiclass Object Classification. In *Proc. Eur.
    Conf. Genetic Program*. 399â€“408.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang å’Œ Smart (2004) Mengjie Zhang å’Œ Will Smart. 2004. ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœç´¢çš„é—ä¼ ç¼–ç¨‹ç”¨äºå¤šç±»åˆ«å¯¹è±¡åˆ†ç±»ã€‚è§äº
    *Proc. Eur. Conf. Genetic Program*. 399â€“408ã€‚
- en: Zhang etÂ al. (2017) Yong Zhang, Dun-wei Gong, Xiao-yan Sun, and Yi-nan Guo.
    2017. A PSO-Based Multi-Objective Multi-Label Feature Selection Method in Classification.
    *Sci. Rep.* 7, 1 (2017), 1â€“12.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ (2017) Yong Zhang, Dun-wei Gong, Xiao-yan Sun, å’Œ Yi-nan Guo. 2017. ä¸€ç§åŸºäºPSOçš„å¤šç›®æ ‡å¤šæ ‡ç­¾ç‰¹å¾é€‰æ‹©æ–¹æ³•ç”¨äºåˆ†ç±»ã€‚*Sci.
    Rep.* 7, 1 (2017), 1â€“12ã€‚
- en: 'Zhang and Rockett (2005) Yang Zhang and PeterÂ I. Rockett. 2005. Evolving Optimal
    Feature Extraction Using Multi-objective Genetic Programming: A Methodology and
    Preliminary Study on Edge Detection. In *Proc. Genetic Evol. Comput. Conf.* 795â€“802.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang å’Œ Rockett (2005) Yang Zhang å’Œ Peter I. Rockett. 2005. ä½¿ç”¨å¤šç›®æ ‡é—ä¼ ç¼–ç¨‹æ¼”åŒ–æœ€ä¼˜ç‰¹å¾æå–ï¼šè¾¹ç¼˜æ£€æµ‹æ–¹æ³•åŠåˆæ­¥ç ”ç©¶ã€‚è§äº
    *Proc. Genetic Evol. Comput. Conf.* 795â€“802ã€‚
- en: Zhang and Rockett (2011) Yang Zhang and PeterÂ I. Rockett. 2011. A Generic Optimising
    Feature Extraction Method Using Multiobjective Genetic Programming. *Appl. Soft
    Comput.* 11, 1 (2011), 1087â€“1097.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang å’Œ Rockett (2011) Yang Zhang å’Œ Peter I. Rockett. 2011. ä¸€ç§é€šç”¨ä¼˜åŒ–ç‰¹å¾æå–æ–¹æ³•ï¼Œä½¿ç”¨å¤šç›®æ ‡é—ä¼ ç¼–ç¨‹ã€‚*Appl.
    Soft Comput.* 11, 1 (2011), 1087â€“1097ã€‚
- en: Zhang etÂ al. (2021b) Yidan Zhang, Youheng Zhen, Zhenan He, and GrayÂ G. Yen.
    2021b. Improvement of Efficiency in Evolutionary Pruning. In *Proc. Int. Joint
    Conf. Neural Netw.* 1â€“8.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ (2021b) Yidan Zhang, Youheng Zhen, Zhenan He, å’Œ Gray G. Yen. 2021b.
    æ¼”åŒ–ä¿®å‰ªæ•ˆç‡çš„æé«˜ã€‚è§äº *Proc. Int. Joint Conf. Neural Netw.* 1â€“8ã€‚
- en: Zhao etÂ al. (2006) Qijun Zhao, David Zhang, and Hongtao Lu. 2006. A Direct Evolutionary
    Feature Extraction Algorithm for Classifying High Dimensional Data. In *Proc.
    AAAI Conf. Artif. Intell.*, Vol.Â 1. 561â€“566.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao ç­‰ (2006) Qijun Zhao, David Zhang, å’Œ Hongtao Lu. 2006. ä¸€ç§ç›´æ¥æ¼”åŒ–ç‰¹å¾æå–ç®—æ³•ç”¨äºåˆ†ç±»é«˜ç»´æ•°æ®ã€‚è§äº
    *Proc. AAAI Conf. Artif. Intell.*, Vol. 1. 561â€“566ã€‚
- en: Zhao etÂ al. (2009) Qijun Zhao, DavidÂ Dian Zhang, Lei Zhang, and Hongtao Lu.
    2009. Evolutionary Discriminant Feature Extraction with Application to Face Recognition.
    *EURASIP J. Adv. Signal. Process.* 2009 (2009), 1â€“12.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao ç­‰ (2009) Qijun Zhao, David Dian Zhang, Lei Zhang, å’Œ Hongtao Lu. 2009. æ¼”åŒ–åˆ¤åˆ«ç‰¹å¾æå–åŠå…¶åœ¨äººè„¸è¯†åˆ«ä¸­çš„åº”ç”¨ã€‚*EURASIP
    J. Adv. Signal. Process.* 2009 (2009), 1â€“12ã€‚
- en: Zhao etÂ al. (2007) Tianwen Zhao, Qijun Zhao, Hongtao Lu, and DavidÂ Dian Zhang.
    2007. Bagging Evolutionary Feature Extraction Algorithm for Classification. In
    *Proc. Int. Conf. Neural Comput.*, Vol.Â 3\. 540â€“545.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao ç­‰ (2007) Tianwen Zhao, Qijun Zhao, Hongtao Lu, å’Œ David Dian Zhang. 2007.
    ç”¨äºåˆ†ç±»çš„è¢‹è£…æ¼”åŒ–ç‰¹å¾æå–ç®—æ³•ã€‚è§äº *Proc. Int. Conf. Neural Comput.*, Vol. 3. 540â€“545ã€‚
- en: Zhou etÂ al. (2021a) Xun Zhou, A.Â K. Qin, Maoguo Gong, and KayÂ Chen Tan. 2021a.
    A Survey on Evolutionary Construction of Deep Neural Networks. *IEEE Trans. Evol.
    Comput.* 25, 5 (2021), 894â€“912.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou ç­‰ (2021a) Xun Zhou, A. K. Qin, Maoguo Gong, å’Œ Kay Chen Tan. 2021a. å…³äºæ¼”åŒ–æ„å»ºæ·±åº¦ç¥ç»ç½‘ç»œçš„è°ƒæŸ¥ã€‚*IEEE
    Trans. Evol. Comput.* 25, 5 (2021), 894â€“912ã€‚
- en: Zhou etÂ al. (2020) Yao Zhou, GaryÂ G. Yen, and Zhang Yi. 2020. Evolutionary Compression
    of Deep Neural Networks for Biomedical Image Segmentation. *IEEE Trans. Neural
    Netw. Learn. Syst.* 31, 8 (2020), 2916â€“2929.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou ç­‰ (2020) Yao Zhou, Gary G. Yen, å’Œ Zhang Yi. 2020. ç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¼”åŒ–å‹ç¼©ã€‚*IEEE
    Trans. Neural Netw. Learn. Syst.* 31, 8 (2020), 2916â€“2929ã€‚
- en: Zhou etÂ al. (2021b) Yao Zhou, GaryÂ G. Yen, and Zhang Yi. 2021b. Evolutionary
    Shallowing Deep Neural Networks at Block Levels. *IEEE Trans. Neural Netw. Learn.
    Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3059529](https://doi.org/10.1109/TNNLS.2021.3059529)
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou ç­‰ (2021b) Yao Zhou, Gary G. Yen, å’Œ Zhang Yi. 2021b. åœ¨å—çº§åˆ«æ¼”åŒ–æµ…å±‚æ·±åº¦ç¥ç»ç½‘ç»œã€‚*IEEE
    Trans. Neural Netw. Learn. Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3059529](https://doi.org/10.1109/TNNLS.2021.3059529)
- en: Zhou etÂ al. (2021c) Yao Zhou, GaryÂ G. Yen, and Zhang Yi. 2021c. A Knee-Guided
    Evolutionary Algorithm for Compressing Deep Neural Networks. *IEEE Trans. Cybern.*
    51, 3 (2021), 1626â€“1638.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou ç­‰ (2021c) Yao Zhou, Gary G. Yen, å’Œ Zhang Yi. 2021c. ä¸€ç§è†éƒ¨å¼•å¯¼çš„æ¼”åŒ–ç®—æ³•ç”¨äºå‹ç¼©æ·±åº¦ç¥ç»ç½‘ç»œã€‚*IEEE
    Trans. Cybern.* 51, 3 (2021), 1626â€“1638ã€‚
- en: 'Zhu etÂ al. (2019) Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, and Yongjun
    Xu. 2019. EENA: Efficient Evolution of Neural Architecture. In *Proc. IEEE Int.
    Conf. Comput. Vis.* 1891â€“1899.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu ç­‰äºº (2019) Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu å’Œ Yongjun Xu.
    2019. EENA: Efficient Evolution of Neural Architecture. è§äº *Proc. IEEE Int. Conf.
    Comput. Vis.* 1891â€“1899.'
- en: Zhu and Jin (2022) Hangyu Zhu and Yaochu Jin. 2022. Real-Time Federated Evolutionary
    Neural Architecture Search. *IEEE Trans. Evol. Comput.* 26, 2 (2022), 364â€“378.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu å’Œ Jin (2022) Hangyu Zhu å’Œ Yaochu Jin. 2022. Real-Time Federated Evolutionary
    Neural Architecture Search. *IEEE Trans. Evol. Comput.* 26, 2 (2022), 364â€“378.
- en: Zhu etÂ al. (2007) Zexuan Zhu, Y. Ong, and Manoranjan Dash. 2007. Markov Blanket-Embedded
    Genetic Algorithm for Gene Selection. *Pattern Recognit.* 40, 11 (2007), 3236â€“3248.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu ç­‰äºº (2007) Zexuan Zhu, Y. Ong å’Œ Manoranjan Dash. 2007. Markov Blanket-Embedded
    Genetic Algorithm for Gene Selection. *Pattern Recognit.* 40, 11 (2007), 3236â€“3248.
- en: Zoph etÂ al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and QuocÂ V
    Le. 2018. Learning Transferable Architectures for Scalable Image Recognition.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 8697â€“8710.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph ç­‰äºº (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens å’Œ Quoc V Le. 2018.
    Learning Transferable Architectures for Scalable Image Recognition. è§äº *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.* 8697â€“8710.
