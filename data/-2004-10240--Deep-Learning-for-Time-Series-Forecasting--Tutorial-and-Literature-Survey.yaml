- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:01:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:01:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2004.10240] Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2004.10240] 时间序列预测中的深度学习：教程和文献综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2004.10240](https://ar5iv.labs.arxiv.org/html/2004.10240)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2004.10240](https://ar5iv.labs.arxiv.org/html/2004.10240)
- en: 'Deep Learning for Time Series Forecasting: Tutorial and Literature Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测中的深度学习：教程和文献综述
- en: Konstantinos Benidis
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Konstantinos Benidis
- en: Amazon Research
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: kbenidis@amazon.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: kbenidis@amazon.com
- en: '&Syama Sundar Rangapuram'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&Syama Sundar Rangapuram'
- en: Amazon Research
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: rangapur@amazon.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: rangapur@amazon.com
- en: '&Valentin Flunkert'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&Valentin Flunkert'
- en: Amazon Research
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: flunkert@amazon.com
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: flunkert@amazon.com
- en: '&Yuyang Wang'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yuyang Wang'
- en: Amazon Research
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: East Palo Alto, CA, USA
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 美国加州东帕洛阿尔托
- en: yuyawang@amazon.com
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: yuyawang@amazon.com
- en: '&Danielle Maddix'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&Danielle Maddix'
- en: Amazon Research
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: East Palo Alto, CA, USA
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 美国加州东帕洛阿尔托
- en: dmmaddix@amazon.com
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: dmmaddix@amazon.com
- en: '&Caner Turkmen'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&Caner Turkmen'
- en: Amazon Research
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: atturkm@amazon.com
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: atturkm@amazon.com
- en: '&Jan Gasthaus'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '&Jan Gasthaus'
- en: Amazon Research
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: gasthaus@amazon.com
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: gasthaus@amazon.com
- en: '&Michael Bohlke-Schneider'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&Michael Bohlke-Schneider'
- en: Amazon Research
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: bohlkem@amazon.com
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: bohlkem@amazon.com
- en: '&David Salinas'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&David Salinas'
- en: Amazon Research
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: dsalina@amazon.com
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: dsalina@amazon.com
- en: '&Lorenzo Stella'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&Lorenzo Stella'
- en: Amazon Research
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: stellalo@amazon.com
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: stellalo@amazon.com
- en: '&François-Xavier Aubet'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&François-Xavier Aubet'
- en: Amazon Research
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: aubetf@amazon.com
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: aubetf@amazon.com
- en: '&Laurent Callot'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&Laurent Callot'
- en: Amazon Research
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊研究院
- en: Berlin, Germany
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: lcallot@amazon.com
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: lcallot@amazon.com
- en: '&Tim Januschowski^†^†footnotemark:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&Tim Januschowski^†^†脚注：'
- en: Zalando SE
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Zalando SE
- en: Berlin, Germany
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 德国柏林
- en: tim.januschowski@zalando.de Equal contribution.Work done while at AWS.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: tim.januschowski@zalando.de 相同贡献。工作完成时在AWS。
- en: Abstract
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning based forecasting methods have become the methods of choice in
    many applications of time series prediction or *forecasting* often outperforming
    other approaches. Consequently, over the last years, these methods are now ubiquitous
    in large-scale industrial forecasting applications and have consistently ranked
    among the best entries in forecasting competitions (e.g., M4 and M5). This practical
    success has further increased the academic interest to understand and improve
    deep forecasting methods. In this article we provide an introduction and overview
    of the field: We present important building blocks for deep forecasting in some
    depth; using these building blocks, we then survey the breadth of the recent deep
    forecasting literature.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的预测方法在许多时间序列预测或*预报*应用中已成为首选方法，通常优于其他方法。因此，近年来，这些方法现在在大规模工业预测应用中无处不在，并在预测竞赛中（例如，M4和M5）始终排名靠前。这一实际成功进一步激发了学术界对理解和改进深度预测方法的兴趣。本文提供了该领域的介绍和概述：我们深入介绍了深度预测的一些重要构建块；使用这些构建块，我们随后调查了近期深度预测文献的广度。
- en: '*K*eywords Time series $\cdot$ Forecasting  $\cdot$ Deep learning'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*eywords 时间序列 $\cdot$ 预测 $\cdot$ 深度学习'
- en: 1 Introduction
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Forecasting is the task of extrapolating time series into the future. It has
    many important applications [[54](#bib.bib54)] such as forecasting the demand
    for items sold by retailers [[41](#bib.bib41), [190](#bib.bib190), [156](#bib.bib156),
    [136](#bib.bib136), [14](#bib.bib14), [25](#bib.bib25)], the flow of traffic [[111](#bib.bib111),
    [126](#bib.bib126), [118](#bib.bib118)], the demand and supply of energy [[45](#bib.bib45),
    [170](#bib.bib170), [117](#bib.bib117), [157](#bib.bib157)], or the covariance
    matrix, volatility and long-tail distributions in finance [[30](#bib.bib30), [29](#bib.bib29),
    [124](#bib.bib124), [12](#bib.bib12), [197](#bib.bib197)]. As such, it is a well-studied
    area (e.g., see [[84](#bib.bib84)] for an introduction) with its own dedicated
    research community. The machine learning, data science, systems, and operations
    research communities as well as application-specific research communities have
    also studied the problem intensively (e.g., see a series of recent tutorials [[56](#bib.bib56),
    [55](#bib.bib55), [57](#bib.bib57), [58](#bib.bib58)]). In contrast to traditional
    forecasting applications, modern incarnations often exhibit large panels of related
    time series, all of which need to be forecasted simultaneously [[89](#bib.bib89)].
    Although these problem characteristics make them amenable to deep learning or
    neural networks (NNs), as in many other domains over the course of history, NNs
    were not always a standard tool to tackle such problems. Indeed, their effectiveness
    has historically been regarded as mixed (e.g., [[202](#bib.bib202)]).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是将时间序列外推到未来的任务。它有许多重要的应用，例如预测零售商出售商品的需求[[54](#bib.bib54)]，交通流量[[41](#bib.bib41),
    [190](#bib.bib190), [156](#bib.bib156), [136](#bib.bib136), [14](#bib.bib14),
    [25](#bib.bib25)]，能源的需求和供应[[111](#bib.bib111), [126](#bib.bib126), [118](#bib.bib118)]，或金融中的协方差矩阵、波动性和长尾分布[[45](#bib.bib45),
    [170](#bib.bib170), [117](#bib.bib117), [157](#bib.bib157)], [[30](#bib.bib30),
    [29](#bib.bib29), [124](#bib.bib124), [12](#bib.bib12), [197](#bib.bib197)]。因此，这是一个研究较多的领域（例如，参见[[84](#bib.bib84)]以获取介绍），拥有自己专门的研究社区。机器学习、数据科学、系统和运筹学社区以及应用特定的研究社区也对这个问题进行了深入研究（例如，参见一系列近期教程[[56](#bib.bib56),
    [55](#bib.bib55), [57](#bib.bib57), [58](#bib.bib58)]）。与传统的预测应用相比，现代的预测通常涉及大量相关的时间序列，这些序列需要同时进行预测[[89](#bib.bib89)]。尽管这些问题特征使得深度学习或神经网络（NNs）成为一个合适的选择，但正如历史上许多领域一样，NNs
    并不总是解决这些问题的标准工具。实际上，它们的有效性在历史上被认为是混合的（例如，[[202](#bib.bib202)]）。
- en: The history of NNs starts in 1957 [[152](#bib.bib152)] and in 1964 for NNs in
    forecasting [[83](#bib.bib83)]. Since then, interest in NNs has oscillated, with
    upsurges in attention attributable to breakthroughs. The application of NNs in
    time series forecasting has followed the general popularity, typically with a
    lag of a few years. Examples of such breakthroughs include Rumelhart et al. [[153](#bib.bib153),
    [154](#bib.bib154)] that popularized the training of multilayer perceptrons (MLPs)
    using back-propagation. Significant advances were made subsequently such as the
    use of convolutional NNs (CNNs) [[113](#bib.bib113)], and Long Short Term Memory
    (LSTM) [[81](#bib.bib81)] cells that address the issue of recurrent NNs’ (RNNs)
    training, just to name a few. Despite these advances, NNs remained hard to train
    and difficult to work with. Methods such as Support Vector Machines (SVMs) [[26](#bib.bib26)]
    and Random Forests [[79](#bib.bib79)] that were developed in the 1990s proved
    to be highly effective (LeCun et al. [[114](#bib.bib114)] found that SVMs were
    as good as the best designed NNs available at the time) and were supported by
    attractive theory. This shifted the interest of researchers away from NNs. Forecasting
    was no exception and results obtained with NNs were mostly mixed as reflected
    in a highly cited review [[202](#bib.bib202)]. The breakthrough that marked the
    dawn of the deep learning era came in 2006 when Hinton et al. [[78](#bib.bib78)]
    showed that it was possible to train NNs with a large number of layers (deep)
    if the weights are initialized appropriately. Accordingly, deep learning has had
    a sizable impact on forecasting [[110](#bib.bib110)] and NNs have long entered
    the canon of standard techniques for forecasting [[84](#bib.bib84)]. New models
    specifically designed for forecasting tasks have been proposed, taking advantage
    of deep learning to supercharge classical forecasting models or to develop entirely
    novel approaches. This recent burst of attention on *deep forecasting* models
    is the latest twist in a long and rich history.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NNs）的历史始于1957年[[152](#bib.bib152)]，而在预测领域则始于1964年[[83](#bib.bib83)]。自那时以来，对NNs的兴趣经历了波动，关注的上升归因于突破性进展。NNs在时间序列预测中的应用也随着普及度的一般趋势而变化，通常滞后几年。这样的突破性进展的例子包括Rumelhart等人[[153](#bib.bib153),
    [154](#bib.bib154)]，他们推广了使用反向传播训练多层感知机（MLPs）。随后的重大进展包括使用卷积神经网络（CNNs）[[113](#bib.bib113)]，以及解决循环神经网络（RNNs）训练问题的长短期记忆（LSTM）[[81](#bib.bib81)]单元，仅举几例。尽管有这些进展，NNs仍然难以训练和使用。1990年代开发的支持向量机（SVMs）[[26](#bib.bib26)]和随机森林[[79](#bib.bib79)]等方法被证明非常有效（LeCun等人[[114](#bib.bib114)]发现SVMs与当时最佳设计的NNs表现相当），并且得到了有吸引力的理论支持。这使得研究人员的兴趣从NNs转移开来。预测也不例外，NNs获得的结果大多不一如人意，如一项高度引用的综述所反映[[202](#bib.bib202)]。标志着深度学习时代曙光的突破出现在2006年，当时Hinton等人[[78](#bib.bib78)]展示了如果权重初始化得当，可以训练具有大量层（深层）的NNs。因此，深度学习对预测产生了巨大影响[[110](#bib.bib110)]，NNs已长期成为预测的标准技术之一[[84](#bib.bib84)]。专门为预测任务设计的新模型已经被提出，利用深度学习来增强经典预测模型或开发全新的方法。这一最近对*深度预测*模型的关注是悠久而丰富历史中的最新发展。
- en: Driven by the availability of (closed-source) large time series panels, the
    potential of deep forecasting models, i.e., forecasting models based on NNs, has
    been exploited primarily in applied industrial research divisions over the last
    years [[111](#bib.bib111), [64](#bib.bib64), [156](#bib.bib156), [190](#bib.bib190)].¹¹1Forecasting
    is an example of a sub-discipline in the machine learning community where the
    comparatively modest attention it receives in published research is in stark contrast
    to a tremendous business impact. With the overwhelming success of deep forecasting
    methods in the M4 competition [[169](#bib.bib169)], this has convinced also formerly
    skeptical academics [[128](#bib.bib128), [129](#bib.bib129)]. In the most recent
    M5 competition, deep forecasting methods were the second and third placed solutions
    [[130](#bib.bib130)] although the competition was otherwise dominated by tree-based
    forecasting methods such as LightGBM [[99](#bib.bib99)] and XGBoost [[33](#bib.bib33)],
    see e.g., [[92](#bib.bib92)]. Modern software frameworks [[1](#bib.bib1), [143](#bib.bib143),
    [34](#bib.bib34)] have sped up the development of NN models and dedicated forecasting
    packages available [[4](#bib.bib4)].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于（封闭源代码的）大规模时间序列面板的可用性，深度预测模型，即基于神经网络的预测模型，在过去几年里主要在应用工业研究部门中得到了开发[[111](#bib.bib111),
    [64](#bib.bib64), [156](#bib.bib156), [190](#bib.bib190)]。¹¹1预测是机器学习社区中的一个子领域，其在已发表研究中的相对较少关注与其巨大的商业影响形成了鲜明对比。随着深度预测方法在M4竞赛中的巨大成功[[169](#bib.bib169)]，这也使得曾经持怀疑态度的学者[[128](#bib.bib128),
    [129](#bib.bib129)]改变了看法。在最近的M5竞赛中，深度预测方法获得了第二和第三名[[130](#bib.bib130)]，尽管竞赛其他方面则被树模型预测方法如LightGBM[[99](#bib.bib99)]和XGBoost[[33](#bib.bib33)]所主导，参见，例如[[92](#bib.bib92)]。现代软件框架[[1](#bib.bib1),
    [143](#bib.bib143), [34](#bib.bib34)]加速了神经网络模型的开发，并提供了专门的预测软件包[[4](#bib.bib4)]。
- en: 'While the history of NNs for forecasting is rich, the focus of this article
    is on more recent developments in NN for forecasting, roughly since the time that
    the term “deep learning” was coined. As such, we do not attempt to give a complete
    historical overview and sacrifice comprehensiveness for recency. The main objectives
    of this article are to educate on, review and popularize the recent developments
    in forecasting driven by NNs for a general audience. Therefore, we place emphasis
    on an educational aspect via a tutorial of deep forecasting in the first part
    (Section [2](#S2 "2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey")). In the second part, Section [3](#S3
    "3 Literature review ‣ Deep Learning for Time Series Forecasting: Tutorial and
    Literature Survey"), we provide an overview of the state-of-the-art of modern
    deep forecasting models. Our exposition is driven by an attempt to identify the
    main building blocks of modern deep forecasting models which hopefully enables
    the reader to digest the rapidly increasing literature more easily. We do not
    attempt a taxonomy of all existing methods and our selection of the building blocks
    is opinionated, motivated by our experience of innovating in this area with a
    strong focus on practical applicability. Compared with other surveys [[119](#bib.bib119),
    [76](#bib.bib76), [202](#bib.bib202)], we provide a more comprehensive overview
    with a particular focus on recent, advanced topics. Finally, in Section [4](#S4
    "4 Conclusions and Avenues for Future Work ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey"), we conclude and speculate on potentially fruitful
    areas for future research.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络在预测中的历史丰富，但本文的重点是近年来神经网络在预测中的最新发展，大致从“深度学习”这一术语被创造出来的时间开始。因此，我们不尝试给出完整的历史概述，而是以近期性为重。本文的主要目标是对神经网络驱动的最新预测发展进行教育、审查和推广，以便于一般读者。因此，我们在第一部分（第[2](#S2
    "2 深度预测：教程 ‣ 深度学习时间序列预测：教程和文献综述")）中通过深度预测教程强调了教育方面。在第二部分，第[3](#S3 "3 文献综述 ‣ 深度学习时间序列预测：教程和文献综述")，我们提供了现代深度预测模型的最先进概况。我们的阐述旨在识别现代深度预测模型的主要构建块，希望能帮助读者更容易地消化快速增长的文献。我们不尝试对所有现有方法进行分类，我们对构建块的选择是主观的，受我们在这一领域创新经验的驱动，特别关注实际应用。与其他综述[[119](#bib.bib119),
    [76](#bib.bib76), [202](#bib.bib202)]相比，我们提供了更全面的概述，特别关注近期的先进主题。最后，在第[4](#S4 "4
    结论和未来研究方向 ‣ 深度学习时间序列预测：教程和文献综述")部分，我们总结并对未来研究的潜在有益领域进行了一些推测。
- en: '2 Deep Forecasting: A Tutorial'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度预测：教程
- en: In the following, we formalize the forecasting problem, summarize those advances
    in deep learning that we deem as the most relevant for forecasting, expose important
    building blocks for NNs and discuss archetypal models in detail. For general improvements
    that fueled the deep learning renaissance, like weight initialization, optimization
    algorithms or general-purpose components such as activation functions, we refer
    to standard textbooks like [[71](#bib.bib71)]. We are aware to be opinionated
    in both the selection of topics as well as the style of exposition. We attempt
    to take a perspective akin to a deep forecasting model builder who would compose
    a forecasting model out of several building blocks such as NN architectures, input
    transformations and output representations. Although not all models will fit perfectly
    into this exposition, it is our hope that this downside is outweighed by the benefit
    of allowing the inclined reader to invent new models more easily.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将形式化预测问题，总结我们认为与预测最相关的深度学习进展，揭示神经网络的重要构建模块，并详细讨论典型模型。对于推动深度学习复兴的总体改进，如权重初始化、优化算法或通用组件如激活函数，我们参考标准教材如 [[71](#bib.bib71)]。我们意识到在主题选择和表达风格上可能有主观性。我们尝试从一个类似深度预测模型构建者的角度出发，将预测模型从多个构建模块中组合而成，例如神经网络架构、输入变换和输出表示。尽管并非所有模型都能完全契合这种描述，但我们希望这种缺点被允许感兴趣的读者更容易发明新模型的好处所抵消。
- en: 2.1 Notation and Formalization of the Forecasting Problem
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 预测问题的符号和形式化
- en: Matrices, vectors and scalars are denoted by uppercase bold, lowercase bold
    and lowercase normal letters, i.e., $\mathbf{X}$, $\mathbf{x}$ and $x$, respectively.
    Let $\mathcal{Z}=\{\mathbf{z}_{i,1:T_{i}}\}_{i=1}^{N}$ be a set of $N$ univariate
    time series, where $\mathbf{z}_{i,1:T_{i}}=(z_{i,1},\dots,z_{i,T_{i}})$, $z_{i,t}$
    is the value of the $i$-th time series at time $t$ and $\mathbf{Z}_{t_{1}:t_{2}}$
    the values of all $N$ time series at the time slice $[t_{1},t_{2}]$. Typical examples
    for the domain of the time series values include $\mathbb{R},\mathbb{N},\mathbb{Z},[0,1]$.
    The set of time series is associated with a set of covariate vectors denoted by
    $\mathcal{X}=\{\mathbf{X}_{i,1:T_{i}}\}_{i=1}^{N}$, with $\mathbf{x}_{i,t}\in\mathbb{R}^{d_{x}}$.
    Note that each vector $\mathbf{x}_{i,t}$ can include both time-varying or static
    features. We denote by $\alpha$ a general input in a model (that can be any combination
    of covariates and lagged values of the target) and by $\beta$ a general output.
    Since $\alpha$ and $\beta$ refer to a general case, we always represent them with
    lowercase normal letters. We denote by $\theta$ the parameters of a model (e.g.,
    parameters of a distribution) and by $\Phi$ the learnable free parameters of the
    underlying NN (e.g., the weights and biases).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵、向量和标量分别用大写粗体字母、小写粗体字母和小写普通字母表示，即 $\mathbf{X}$、$\mathbf{x}$ 和 $x$。设 $\mathcal{Z}=\{\mathbf{z}_{i,1:T_{i}}\}_{i=1}^{N}$
    为 $N$ 个单变量时间序列的集合，其中 $\mathbf{z}_{i,1:T_{i}}=(z_{i,1},\dots,z_{i,T_{i}})$，$z_{i,t}$
    是第 $i$ 个时间序列在时间 $t$ 的值，$\mathbf{Z}_{t_{1}:t_{2}}$ 是所有 $N$ 个时间序列在时间片段 $[t_{1},t_{2}]$
    的值。时间序列值的典型领域包括 $\mathbb{R},\mathbb{N},\mathbb{Z},[0,1]$。时间序列集合与协变量向量集合 $\mathcal{X}=\{\mathbf{X}_{i,1:T_{i}}\}_{i=1}^{N}$
    相关，其中 $\mathbf{x}_{i,t}\in\mathbb{R}^{d_{x}}$。注意，每个向量 $\mathbf{x}_{i,t}$ 可以包括时间变化的特征或静态特征。我们用
    $\alpha$ 表示模型中的一个一般输入（可以是协变量和目标滞后值的任意组合），用 $\beta$ 表示一个一般输出。由于 $\alpha$ 和 $\beta$
    代表的是一般情况，我们总是用小写普通字母来表示它们。我们用 $\theta$ 表示模型的参数（例如，分布的参数），用 $\Phi$ 表示基础神经网络（NN）的可学习自由参数（例如，权重和偏置）。
- en: In the most general form, the object of interest in forecasting is the conditional
    distribution
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在最一般的形式中，预测中的关注对象是条件分布。
- en: '|  | $p(\mathbf{Z}_{t+1:t+h}&#124;\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta),$
    |  | (1) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathbf{Z}_{t+1:t+h}&#124;\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta),$
    |  | (1) |'
- en: 'where $\theta$ are the parameters of a (probabilistic) model. Eq. ([1](#S2.E1
    "In 2.1 Notation and Formalization of the Forecasting Problem ‣ 2 Deep Forecasting:
    A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey")) is general in the sense that each $\mathbf{z}_{i}\in\mathcal{Z}$ is
    multidimensional (the length of the time series), $\mathcal{Z}$ is multivariate
    (the number of time series $|\mathcal{Z}|=N>1$) and the forecast is multi-step
    ($h$ steps). Varying degrees of simplification of Eq. ([1](#S2.E1 "In 2.1 Notation
    and Formalization of the Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial
    ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey"))
    are considered in the literature, for example by assuming factorizations of $p$
    and different ways of estimating $\theta$. In the following, we present the three
    archetypical models for addressing Eq. ([1](#S2.E1 "In 2.1 Notation and Formalization
    of the Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey")).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 是（概率）模型的参数。Eq. ([1](#S2.E1 "在 2.1 符号和预测问题的形式化 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献调查"))
    是一般的，因为每个 $\mathbf{z}_{i}\in\mathcal{Z}$ 是多维的（时间序列的长度），$\mathcal{Z}$ 是多变量的（时间序列的数量
    $|\mathcal{Z}|=N>1$），且预测是多步的（$h$ 步）。文献中考虑了 Eq. ([1](#S2.E1 "在 2.1 符号和预测问题的形式化
    ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献调查")) 的不同简化程度，例如通过假设 $p$ 的分解和不同的 $\theta$ 估计方法。接下来，我们介绍三种典型模型来解决
    Eq. ([1](#S2.E1 "在 2.1 符号和预测问题的形式化 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献调查"))。
- en: 'Local univariate model: A separate (*local*) model is trained independently
    for each of the $N$ time series, modelling the predictive distribution'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 局部单变量模型：为每个 $N$ 个时间序列独立训练一个单独的 (*局部*) 模型，建模预测分布
- en: '|  | $p(\mathbf{z}_{i,t+1:t+h}&#124;\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h};\theta_{i}),\quad\theta_{i}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h}),$
    |  | (2) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathbf{z}_{i,t+1:t+h} \mid \mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h};\theta_{i}),\quad\theta_{i}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h}),$
    |  | (2) |'
- en: 'where $\Psi$ is a generic function mapping input features to the parameters
    $\theta_{i}$ of the probabilistic model that are local to the $i$-th time series.
    Note that one may use multidimensional covariates $\mathbf{x}_{i,t}$ for each
    of the $N$ models, but they are still solving a univariate problem, i.e., forecasting
    only one time series. The use of covariates common to all $N$ models is possible
    but any pattern that is learned in one model is not used in another (unless provided
    explicitly which prohibits parallel training). Many classical approaches fall
    into this category and traditionally NNs were employed in this local fashion (e.g., [[202](#bib.bib202)]).
    Note that this approach is not suitable for cold start problems: i.e., forecasting
    a time series without historical values.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Psi$ 是一个通用函数，将输入特征映射到概率模型的参数 $\theta_{i}$，这些参数对第 $i$ 个时间序列是局部的。需要注意的是，虽然可以为每个
    $N$ 个模型使用多维协变量 $\mathbf{x}_{i,t}$，但它们仍在解决单变量问题，即仅预测一个时间序列。使用对所有 $N$ 模型都通用的协变量是可能的，但在一个模型中学习到的任何模式不会在另一个模型中使用（除非明确提供，这会禁止并行训练）。许多经典方法都属于这一类，传统上神经网络以这种局部方式使用（例如，[[202](#bib.bib202)]）。需要注意的是，这种方法不适用于冷启动问题：即在没有历史值的情况下预测时间序列。
- en: 'Global univariate model: A single, *global* model [[91](#bib.bib91), [135](#bib.bib135)]
    is trained using available data from all $N$ time series. However, the model is
    still used to predict a univariate target. It does not produce joint forecasts
    of all time series but forecasts of any single time series at a time. This is
    also sometimes referred to as a cross-learning approach, e.g., [[161](#bib.bib161)].
    In a more general form, global univariate models specialize Eq. ([1](#S2.E1 "In
    2.1 Notation and Formalization of the Forecasting Problem ‣ 2 Deep Forecasting:
    A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey")) to'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 全球单变量模型：使用来自所有 $N$ 个时间序列的可用数据训练一个单一的 *全球* 模型 [[91](#bib.bib91), [135](#bib.bib135)]。然而，该模型仍然用于预测单一的目标。它不会生成所有时间序列的联合预测，而是逐个时间序列进行预测。这有时也称为交叉学习方法，例如
    [[161](#bib.bib161)]。更一般地，全球单变量模型专注于 Eq. ([1](#S2.E1 "在 2.1 符号和预测问题的形式化 ‣ 2 深度预测：教程
    ‣ 时间序列预测的深度学习：教程和文献调查")) 来
- en: '|  | $p(\mathbf{z}_{i,t+1:t+h}&#124;\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta_{i}),\quad\theta_{i}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h},\Phi),$
    |  | (3) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathbf{z}_{i,t+1:t+h}\mid\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta_{i}),\quad\theta_{i}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h},\Phi),$
    |  | (3) |'
- en: where $\Phi$ are shared parameters among all $N$ time series.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Phi$ 是所有 $N$ 时间序列之间共享的参数。
- en: In this article, $\Psi$ in global models is usually a NN and $\mathbf{X}_{i}$
    include item-specific features to allow the model to distinguish between the time
    series. Although the parameters $\theta_{i}$ of the probabilistic model for each
    time series are different, they are still predicted using shared parameters (or
    weights) $\Phi$ in $\Psi$. This allows for efficient learning since the model
    pools information from all time series and in particular improves inference for
    shorter time series compared to local univariate models. Such a model is expected
    to learn some advanced features (“embeddings”) exploiting information across time
    series. Once these advanced features are learned via $\Psi$, the global model
    is then used to forecast each time series independently. That is, although during
    training the model sees all the related time series together, the prediction is
    done by looking at each time series individually. Note that the embeddings learned
    in the global model are useful beyond the $N$ time series used in the training.
    This addresses the cold start problem in the sense that the global model can be
    used to provide forecasts for time series without historical values. Global models
    are also referred to as cross-learning or panel models in econometrics and statistics
    and have been the subject of considerable study, e.g., via dynamic factor models [[66](#bib.bib66)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，全球模型中的 $\Psi$ 通常是一个神经网络（NN），而 $\mathbf{X}_{i}$ 包括项目特定特征，以允许模型区分不同的时间序列。虽然每个时间序列的概率模型参数
    $\theta_{i}$ 是不同的，但它们仍然使用共享参数（或权重） $\Phi$ 在 $\Psi$ 中进行预测。这允许模型通过从所有时间序列中汇集信息来高效学习，特别是与局部单变量模型相比，改进了较短时间序列的推断。这样的模型预计会学习一些高级特征（“嵌入”），利用跨时间序列的信息。一旦这些高级特征通过
    $\Psi$ 学会，全球模型就被用来独立预测每个时间序列。也就是说，尽管在训练期间模型会看到所有相关时间序列，预测还是通过单独查看每个时间序列来完成。注意，全球模型中学到的嵌入不仅对训练中使用的
    $N$ 时间序列有用，还能解决冷启动问题，即全球模型可以用来为没有历史值的时间序列提供预测。全球模型在计量经济学和统计学中也被称为交叉学习或面板模型，并且已经成为大量研究的主题，例如，通过动态因子模型
    [[66](#bib.bib66)]。
- en: 'Multivariate model: Here, a single model is learned for all $N$ time series
    using all available data, directly predicting the multivariate target:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量模型：在这里，为所有 $N$ 时间序列使用所有可用数据学习一个单一模型，直接预测多变量目标：
- en: '|  | $p(\mathbf{Z}_{t+1:t+h}&#124;\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta),\quad\theta=\Psi(\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h},\Phi).$
    |  | (4) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathbf{Z}_{t+1:t+h}\mid\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta),\quad\theta=\Psi(\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h},\Phi).$
    |  | (4) |'
- en: 'Note that the model also learns the dependency structure among the time series.
    Technically speaking, Eq. ([4](#S2.E4 "In 2.1 Notation and Formalization of the
    Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time
    Series Forecasting: Tutorial and Literature Survey")) is a global multivariate
    model and a further distinction from local multivariate models, such as VARMA [[125](#bib.bib125)],
    is possible.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，该模型还学习了时间序列之间的依赖结构。从技术上讲，方程式 ([4](#S2.E4 "在 2.1 符号和预测问题的形式化 ‣ 2 深度预测：教程 ‣
    深度学习用于时间序列预测：教程和文献综述")) 是一个全球多变量模型，并且可以与局部多变量模型（如 VARMA [[125](#bib.bib125)]）进行进一步区分。
- en: Remarks
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: 'Note that in Eq. ([1](#S2.E1 "In 2.1 Notation and Formalization of the Forecasting
    Problem ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey")) and in the following model-specific cases we
    have chosen the multi-step ahead predictive distribution. We can always obtain
    a multi-step predictive distribution via a rolling one-step predictive distribution.
    In our discussion so far, we presented probabilistic forecast models that learn
    the entire distribution of the future values. However, it may be desirable to
    model specific values such as the mean, median or some other quantile, instead
    of the whole probability distribution. These are called point-forecast models
    and the optimal choice of the summary statistics to turn a probabilistic forecast
    into a point forecast depends on the metric used to judge the quality of the point
    forecast [[104](#bib.bib104)]. More concretely, a point-forecast global univariate
    model learns a quantity $\hat{\mathbf{z}}_{i,t+1:t+h}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h},\Phi)$,
    where $\hat{\mathbf{z}}_{i,t+1:t+h}$ is some point estimate of the future values
    of the time series. Table [1](#S2.T1 "Table 1 ‣ Remarks ‣ 2.1 Notation and Formalization
    of the Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey") summarizes the various
    modelling option based on the forecast and model types.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在公式 ([1](#S2.E1 "在 2.1 预测问题的符号和形式化 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述"))和以下模型特定的情况中，我们选择了多步预测分布。我们可以通过滚动的一步预测分布始终获得多步预测分布。在到目前为止的讨论中，我们展示了学习未来值整个分布的概率预测模型。然而，可能更希望建模特定的值，例如均值、中位数或其他分位数，而不是整个概率分布。这些被称为点预测模型，将概率预测转换为点预测的总结统计量的最佳选择取决于用于判断点预测质量的度量[[104](#bib.bib104)]。更具体地说，点预测全局单变量模型学习一个量
    $\hat{\mathbf{z}}_{i,t+1:t+h}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h},\Phi)$，其中
    $\hat{\mathbf{z}}_{i,t+1:t+h}$ 是时间序列未来值的某个点估计。表 [1](#S2.T1 "表 1 ‣ 备注 ‣ 2.1 预测问题的符号和形式化
    ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述") 总结了基于预测和模型类型的各种建模选项。
- en: 'Table 1: Summary of deep forecasting models based on forecast and model type.
    For one-step and multi-step forecasting models $h=1$ and $h>1$, respectively.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：基于预测和模型类型的深度预测模型总结。对于一步预测和多步预测模型，分别取$h=1$和$h>1$。
- en: '| Forecast type | Model type | Formulation |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 预测类型 | 模型类型 | 形式化 |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Point | Local univariate | $\hat{\mathbf{z}}_{i,t+1:t+h}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h})$
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 点预测 | 本地单变量 | $\hat{\mathbf{z}}_{i,t+1:t+h}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h})$
    |'
- en: '| Global univariate | $\hat{\mathbf{z}}_{i,t+1:t+h}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h},\Phi)$
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 全球单变量 | $\hat{\mathbf{z}}_{i,t+1:t+h}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h},\Phi)$
    |'
- en: '| Multivariate | $\hat{\mathbf{Z}}_{t+1:t+h}=\Psi(\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h},\Phi)$
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 多变量 | $\hat{\mathbf{Z}}_{t+1:t+h}=\Psi(\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h},\Phi)$
    |'
- en: '| Probabilistic | Local univariate | $P(\mathbf{z}_{i,t+1:t+h}&#124;\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h};\theta_{i}),\quad\theta_{i}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h})$
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 概率 | 本地单变量 | $P(\mathbf{z}_{i,t+1:t+h}&#124;\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h};\theta_{i}),\quad\theta_{i}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h})$
    |'
- en: '| Global univariate | $P(\mathbf{z}_{i,t+1:t+h}&#124;\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta_{i}),\quad\theta_{i}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h},\Phi)$
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 全球单变量 | $P(\mathbf{z}_{i,t+1:t+h}&#124;\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta_{i}),\quad\theta_{i}=\Psi(\mathbf{z}_{i,1:t},\mathbf{X}_{i,1:t+h},\Phi)$
    |'
- en: '| Multivariate | $P(\mathbf{Z}_{t+1:t+h}&#124;\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta),\quad\theta=\Psi(\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h},\Phi)$
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 多变量 | $P(\mathbf{Z}_{t+1:t+h}&#124;\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h};\theta),\quad\theta=\Psi(\mathbf{Z}_{1:t},\mathbf{X}_{1:t+h},\Phi)$
    |'
- en: 2.2 Neural Network Architectures
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 神经网络架构
- en: 'NNs are compositions of differentiable functions formed from simple building
    blocks to learn an approximation of some unknown function from data. An NN is
    commonly represented as a directed acyclic graph consisting of nodes and edges.
    The edges between the nodes contain weights (also called parameters) that are
    learned from the data. The basic unit of every NN is a neuron (illustrated in
    Fig. [1(a)](#S2.F1.sf1 "In Figure 1 ‣ 2.2.1 Multilayer perceptron ‣ 2.2 Neural
    Network Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time
    Series Forecasting: Tutorial and Literature Survey")), consisting of an input,
    an affine transformation with learnable weights and (optionally) a nonlinear activation
    function. Different types of NNs arrange these components in different ways. We
    refer to other reviews [[119](#bib.bib119)] for more details on the main architectures.
    Here, we only offer a high-level summary for completeness, focusing instead on
    forecasting specific ingredients for NNs such as input processing and loss functions.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NNs）是由简单的构建块组成的可微分函数的组合，以从数据中学习某些未知函数的近似。一个NN通常表示为一个由节点和边组成的有向无环图。节点之间的边包含从数据中学习到的权重（也称为参数）。每个NN的基本单元是一个神经元（如图[1(a)](#S2.F1.sf1
    "在图1 ‣ 2.2.1 多层感知机 ‣ 2.2 神经网络架构 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述")所示），包括一个输入，一个具有可学习权重的仿射变换和（可选的）非线性激活函数。不同类型的NN以不同方式安排这些组件。有关主要架构的更多详细信息，请参考其他综述[[119](#bib.bib119)]。在这里，我们仅提供一个高层次的总结，主要关注神经网络在预测中特定的组成部分，如输入处理和损失函数。
- en: 2.2.1 Multilayer perceptron
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 多层感知机
- en: 'In multilayer perceptrons (MLPs) or synonymously feedforward NNs, layers of
    neurons are stacked on top of each other to learn more complex nonlinear representations
    of the data. An MLP consists of an input and an output layer, while the intermediate
    layers are called hidden. The nodes in each layer of the network are fully connected
    to all the nodes in the previous layer. The output of the last hidden layer can
    be seen as some nonlinear feature representation (also called an *embedding*)
    obtained from the inputs of the network. The output layer then learns a mapping
    from these nonlinear features to the actual target. Learning with MLPs, and more
    generally with NNs, can be thought of as the process of learning a nonlinear feature
    map of the inputs and the relationship between this feature map and the actual
    target. Figure [1(b)](#S2.F1.sf2 "In Figure 1 ‣ 2.2.1 Multilayer perceptron ‣
    2.2 Neural Network Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning
    for Time Series Forecasting: Tutorial and Literature Survey") illustrates the
    structure of an MLP with two hidden layers. Modern incarnations of the MLP have
    added important details to alleviate problems like vanishing gradients [[80](#bib.bib80)].
    For example, ResNet [[75](#bib.bib75)], contains direct connections between hidden
    layers $\ell-1$ and $\ell+1$, skipping over the hidden layer $\ell$.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在多层感知机（MLPs）或同义的前馈神经网络中，神经元层被堆叠在一起，以学习数据的更复杂的非线性表示。一个MLP由输入层和输出层组成，而中间的层称为隐藏层。网络中每一层的节点都与上一层的所有节点完全连接。最后一个隐藏层的输出可以看作是从网络输入中获得的一些非线性特征表示（也称为*嵌入*）。输出层随后学习从这些非线性特征到实际目标的映射。使用MLP，或者更普遍地说，使用神经网络的学习可以被认为是学习输入的非线性特征映射以及这个特征映射与实际目标之间关系的过程。图[1(b)](#S2.F1.sf2
    "在图1 ‣ 2.2.1 多层感知机 ‣ 2.2 神经网络架构 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述")展示了一个具有两个隐藏层的MLP的结构。现代MLP变体加入了重要细节，以缓解如梯度消失等问题[[80](#bib.bib80)]。例如，ResNet[[75](#bib.bib75)]包含隐藏层$\ell-1$和$\ell+1$之间的直接连接，跳过隐藏层$\ell$。
- en: One of the main limitations of MLPs is that they do not exploit the structure
    often present in the data in applications such as computer vision, natural language
    processing and forecasting. Moreover, the number of inputs and outputs is fixed
    making them inapplicable to problems with varying input and output sizes as in
    forecasting. Next, we discuss more complex architectures that overcome these limitations,
    for which MLPs are often used as the basic building blocks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的主要局限之一是它们没有利用在计算机视觉、自然语言处理和预测等应用中经常存在的数据结构。此外，输入和输出的数量是固定的，使它们不适用于输入和输出大小变化的问题，如预测。接下来，我们将讨论克服这些局限性的更复杂的架构，其中MLP通常作为基本构建块使用。
- en: <svg   height="167.55" overflow="visible" version="1.1" width="311.85"><g transform="translate(0,167.55)
    matrix(1 0 0 -1 0 0) translate(26.37,0) translate(0,67.89)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -6.36 57.32)" fill="#000000"
    stroke="#000000"><foreignobject width="12.73" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\alpha_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -21.75 75.75)" fill="#000000" stroke="#000000"><foreignobject width="38.9"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Inputs</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.51 57.32)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 36.22 75.75)" fill="#000000" stroke="#000000"><foreignobject
    width="48.51" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Weights</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.51 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 120 -4.73)" fill="#000000" stroke="#000000"><foreignobject
    width="9.99" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle\Sigma$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 180.34 -3.46)" fill="#000000" stroke="#000000"><foreignobject
    width="22.87" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f(\cdot)$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 152.4 24.96)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Activation
    function</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 252.35 -3.46)"
    fill="#000000" stroke="#000000"><foreignobject width="7.83" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\beta$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 231.67 18.62)" fill="#000000" stroke="#000000"><foreignobject width="44.59"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Output</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -60.79)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.51 -60.79)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{3}$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 85.63 77.1)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Bias
    $b$</foreignobject></g></g></svg>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="167.55" overflow="visible" version="1.1" width="311.85"><g transform="translate(0,167.55)
    matrix(1 0 0 -1 0 0) translate(26.37,0) translate(0,67.89)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -6.36 57.32)" fill="#000000"
    stroke="#000000"><foreignobject width="12.73" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\alpha_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -21.75 75.75)" fill="#000000" stroke="#000000"><foreignobject width="38.9"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">输入</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.51 57.32)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 36.22 75.75)" fill="#000000" stroke="#000000"><foreignobject
    width="48.51" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">权重</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.51 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 120 -4.73)" fill="#000000" stroke="#000000"><foreignobject
    width="9.99" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle\Sigma$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 180.34 -3.46)" fill="#000000" stroke="#000000"><foreignobject
    width="22.87" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f(\cdot)$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 152.4 24.96)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">激活函数</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 252.35 -3.46)" fill="#000000" stroke="#000000"><foreignobject
    width="7.83" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 231.67 18.62)" fill="#000000" stroke="#000000"><foreignobject
    width="44.59" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">输出</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -60.79)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.51 -60.79)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{3}$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 85.63 77.1)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">偏置
    $b$</foreignobject></g></g></svg>
- en: (a) Single node
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 单个节点
- en: <svg   height="226.29" overflow="visible" version="1.1" width="252.26"><g transform="translate(0,226.29)
    matrix(1 0 0 -1 0 0) translate(25.37,0) translate(0,187.54)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g stroke="#808080"><g transform="matrix(1.0 0.0 0.0 1.0
    -6.36 -41.1)" fill="#000000" stroke="#000000"><foreignobject width="12.73" height="8.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -80.47)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -119.84)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -159.21)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{4}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 210.58 -82.2)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 210.58 -121.57)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{2}$</foreignobject></g></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 38.3 24.45)" fill="#000000" stroke="#000000"><foreignobject
    width="41.51" height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Hidden
    layer 1 <g transform="matrix(1.0 0.0 0.0 1.0 -20.76 22.63)" fill="#000000" stroke="#000000"><foreignobject
    width="41.51" height="25.25" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Input
    layer</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 97.35 24.45)" fill="#000000"
    stroke="#000000"><foreignobject width="41.51" height="28.9" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Hidden layer 2</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 156.41 24.45)" fill="#000000" stroke="#000000"><foreignobject width="41.51"
    height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Output layer</foreignobject></g>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="226.29" overflow="visible" version="1.1" width="252.26"><g transform="translate(0,226.29)
    matrix(1 0 0 -1 0 0) translate(25.37,0) translate(0,187.54)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g stroke="#808080"><g transform="matrix(1.0 0.0 0.0 1.0
    -6.36 -41.1)" fill="#000000" stroke="#000000"><foreignobject width="12.73" height="8.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -80.47)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -119.84)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.36 -159.21)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{4}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 210.58 -82.2)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 210.58 -121.57)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{2}$</foreignobject></g></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 38.3 24.45)" fill="#000000" stroke="#000000"><foreignobject
    width="41.51" height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">隐藏层
    1 <g transform="matrix(1.0 0.0 0.0 1.0 -20.76 22.63)" fill="#000000" stroke="#000000"><foreignobject
    width="41.51" height="25.25" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">输入层</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 97.35 24.45)" fill="#000000" stroke="#000000"><foreignobject
    width="41.51" height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">隐藏层
    2</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 156.41 24.45)" fill="#000000"
    stroke="#000000"><foreignobject width="41.51" height="28.9" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">输出层</foreignobject></g>
- en: (b) MLP
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 多层感知器
- en: 'Figure 1: (a) Structure of a single node or neuron. An affine transformation
    is applied to the input followed by an activation function, i.e., $\beta=f\left(\sum\alpha_{i}w_{i}+b\right)$.
    The weights and bias parameters are learned during training. (b) Illustration
    of the MLP structure. Each circle in the hidden and output layers is a node, i.e.,
    it applies an affine transformation followed by a nonlinear activation to the
    set of its inputs.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: (a) 单个节点或神经元的结构。对输入应用仿射变换后，接着使用激活函数，即 $\beta=f\left(\sum\alpha_{i}w_{i}+b\right)$。权重和偏置参数在训练过程中学习。
    (b) MLP 结构的示意图。隐藏层和输出层中的每个圆圈都是一个节点，即对其输入集合应用仿射变换后，再进行非线性激活。'
- en: 2.2.2 Convolutional neural networks
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 卷积神经网络
- en: Convolutional neural networks (CNNs) [[112](#bib.bib112)] are a special class
    of NNs that are designed for applications where inputs have a known ordinal structure
    such as images and time series [[71](#bib.bib71)]. CNNs are locally connected
    NNs that use convolutional layers to exploit the structure present in the input
    data by applying a convolution function to smaller neighborhoods of the input
    data. Convolution here refers to the process of computing moving weighted sums
    by sliding the so-called filter or kernel over different parts of the input data.
    The size of the filter as well as how the filter is slid across the input are
    part of the hyperparameters of the model. A nonlinear activation, typically ReLU
    [[68](#bib.bib68)], is then applied to the output of the convolution operation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）[[112](#bib.bib112)] 是一种特殊的 NN 类，设计用于输入具有已知序列结构的应用，如图像和时间序列[[71](#bib.bib71)]。CNN
    是局部连接的 NN，通过应用卷积函数于输入数据的较小邻域，利用输入数据中的结构。卷积在这里指的是通过在输入数据的不同部分滑动所谓的滤波器或核来计算加权移动和。滤波器的大小以及滤波器在输入上滑动的方式是模型的超参数。然后，对卷积操作的输出应用非线性激活，通常是
    ReLU [[68](#bib.bib68)]。
- en: In addition to convolutional layers, CNNs also typically use a pooling layer
    to reduce the size of the feature representation as well as to make the features
    extracted from the convolutional layer more robust. For example, a commonly used
    max-pooling layer, which is applied to the output of convolutional layers, extracts
    the maximum value of the features in a given neighborhood. Similarly to the convolution
    operation, the pooling operation is applied to smaller neighborhoods by sliding
    the corresponding filter over the input. A pooling layer, however, does not have
    any learnable weights and hence both the convolution and the pooling layer are
    counted as one layer in CNNs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了卷积层，CNN 通常还使用池化层来减少特征表示的大小，并使从卷积层提取的特征更具鲁棒性。例如，常用的最大池化层，它应用于卷积层的输出，提取给定邻域中特征的最大值。类似于卷积操作，池化操作通过在输入上滑动相应的滤波器应用于较小的邻域。然而，池化层没有任何可学习的权重，因此卷积层和池化层在
    CNN 中被视为一层。
- en: Of particular importance for forecasting are the so-called *causal* convolutions,
    defined as
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测而言，所谓的*因果*卷积特别重要，定义为
- en: '|  | $h_{j}=\sum_{d\in\mathcal{D}}w_{d}\alpha_{j-d}\,,$ |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{j}=\sum_{d\in\mathcal{D}}w_{d}\alpha_{j-d}\,,$ |  |'
- en: 'where $h_{j}$ is the output of a hidden node, $\mathbf{\alpha}$ denotes the
    input, $\mathcal{D}=\{1,\ldots,n\}$ for some $n$, $|\mathcal{D}|$ is the *width*
    of the causal convolution (or also called the receptive field) and $\mathbf{w}$
    are the learnable parameters. In other words, causal convolutions are weighted
    moving averages which only take inputs into account which are before $j$ hence
    the reference to causality in its name. A variation are *dilated* causal convolutions
    where we vary the index set $\mathcal{D}$, e.g., such that it does not necessarily
    contain consecutive values, but only every $k$-th value. Typically, these dilated
    causal convolutions are stacked on top of each other where the output of one layer
    of dilated causal convolutions is the input of another layer of causal convolution
    and the dilation grows by the depth of the NN. Figure [2(a)](#S2.F2.sf1 "In Figure
    2 ‣ 2.2.2 Convolutional neural networks ‣ 2.2 Neural Network Architectures ‣ 2
    Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey") illustrates the general structure of a CNN with dilated
    causal convolutions.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{j}$ 是隐藏节点的输出，$\mathbf{\alpha}$ 表示输入，$\mathcal{D}=\{1,\ldots,n\}$ 对于某些
    $n$，$|\mathcal{D}|$ 是因果卷积的*宽度*（也称为感受野），$\mathbf{w}$ 是可学习的参数。换句话说，因果卷积是加权移动平均，只考虑在
    $j$ 之前的输入，因此其名称中提到了因果性。一种变体是*dilated* 因果卷积，其中我们改变索引集 $\mathcal{D}$，例如，使其不一定包含连续值，而是每隔
    $k$ 个值。通常，这些扩张的因果卷积会堆叠在一起，其中一层扩张因果卷积的输出是另一层因果卷积的输入，并且扩张随 NN 的深度增长。图 [2(a)](#S2.F2.sf1
    "在图 2 ‣ 2.2.2 卷积神经网络 ‣ 2.2 神经网络架构 ‣ 2 深度预测：教程 ‣ 深度学习在时间序列预测中的应用：教程和文献综述") 展示了带有扩张因果卷积的
    CNN 的一般结构。
- en: <svg   height="211.76" overflow="visible" version="1.1" width="270.81"><g transform="translate(0,211.76)
    matrix(1 0 0 -1 0 0) translate(-41.76,0) translate(0,17.3)" fill="#000000" stroke="#808080"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.69 -1.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.73" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\alpha_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 111.75 -1.73)" fill="#000000" stroke="#000000"><foreignobject width="12.73"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.8 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 229.86 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{4}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 288.91 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{5}$</foreignobject></g></g></svg>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="211.76" overflow="visible" version="1.1" width="270.81"><g transform="translate(0,211.76)
    matrix(1 0 0 -1 0 0) translate(-41.76,0) translate(0,17.3)" fill="#000000" stroke="#808080"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.69 -1.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.73" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\alpha_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 111.75 -1.73)" fill="#000000" stroke="#000000"><foreignobject width="12.73"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.8 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 229.86 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{4}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 288.91 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{5}$</foreignobject></g></g></svg>
- en: (a) CNN
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CNN
- en: <svg   height="145.78" overflow="visible" version="1.1" width="270.81"><g transform="translate(0,145.78)
    matrix(1 0 0 -1 0 0) translate(17.3,0) translate(0,17.3)" fill="#000000" stroke="#808080"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.69 -1.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.73" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\alpha_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 111.75 -1.73)" fill="#000000" stroke="#000000"><foreignobject width="12.73"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.8 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.78 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{0}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 53.27 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 112.33 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 171.38 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 230.44 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{4}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 53.2 114.65)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 112.26 114.65)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 171.31 114.65)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{3}$</foreignobject></g></g></svg>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="145.78" overflow="visible" version="1.1" width="270.81"><g transform="translate(0,145.78)
    matrix(1 0 0 -1 0 0) translate(17.3,0) translate(0,17.3)" fill="#000000" stroke="#808080"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.69 -1.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.73" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\alpha_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 111.75 -1.73)" fill="#000000" stroke="#000000"><foreignobject width="12.73"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.8 -1.73)" fill="#000000" stroke="#000000"><foreignobject
    width="12.73" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.78 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{0}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 53.27 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 112.33 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 171.38 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 230.44 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="11.56" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{4}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 53.2 114.65)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 112.26 114.65)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 171.31 114.65)" fill="#000000" stroke="#000000"><foreignobject
    width="11.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\beta_{3}$</foreignobject></g></g></svg>
- en: (b) RNN
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (b) RNN
- en: 'Figure 2: (a) Structure of a CNN consisting of a stack of three causal convolution
    layer. The input layer (green) is non-dilated and the other two are dilated. (b)
    Structure of an unrolled RNN. At each timestep $t$ the network receives an external
    input $\alpha_{t}$ and the output of the hidden units from the previous time step
    $\mathbf{h}_{t-1}$. The hidden units all share the same weights. The internal
    state of the network is updated to $\mathbf{h}_{t}$ that is going to play the
    role of the previous state in the next timestep $t+1$. Finally, the network outputs
    $\beta_{t}$ which is a function of $\alpha_{t}$ and $\mathbf{h}_{t}$.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：（a）由三层因果卷积层堆叠组成的CNN结构。输入层（绿色）是非扩张的，其余两个是扩张的。（b）展开的RNN结构。在每个时间步$t$，网络接收外部输入$\alpha_{t}$和来自上一个时间步$\mathbf{h}_{t-1}$的隐藏单元的输出。所有隐藏单元共享相同的权重。网络的内部状态更新为$\mathbf{h}_{t}$，将在下一个时间步$t+1$中充当先前状态。最后，网络输出$\beta_{t}$，它是$\alpha_{t}$和$\mathbf{h}_{t}$的函数。
- en: 2.2.3 Recurrent neural networks
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 循环神经网络
- en: 'Recurrent neural networks (RNNs) are NNs specifically designed to handle sequential
    data that arise in applications such as time series, natural language processing
    and speech recognition. The core idea consists of connecting recurrently the NNs’
    hidden units back to themselves with a time delay [[94](#bib.bib94), [95](#bib.bib95)].
    Since hidden units learn some kind of feature representations of the raw input,
    feeding them back to themselves can be interpreted as providing the network with
    a dynamic memory. One crucial detail here is that the same network is used for
    all timesteps, i.e., the weights of the network are shared across timesteps. This
    weight-sharing idea is similar to that in CNNs where the same filter is used across
    different parts of the input. This allows the RNNs to handle sequences of varying
    length during training and, more importantly, generalize to sequence lengths not
    seen during training. Figure [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2.2.2 Convolutional
    neural networks ‣ 2.2 Neural Network Architectures ‣ 2 Deep Forecasting: A Tutorial
    ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey")
    illustrates the general structure of an (unrolled) RNN.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）是专门设计用于处理序列数据的神经网络，这些数据常见于时间序列、自然语言处理和语音识别等应用中。核心思想是将神经网络的隐藏单元循环地连接回自身，带有时间延迟[[94](#bib.bib94),
    [95](#bib.bib95)]。由于隐藏单元学习到原始输入的某种特征表示，将它们反馈到自身可以解释为为网络提供了动态记忆。一个关键细节是，相同的网络用于所有时间步，即网络的权重在时间步之间共享。这种权重共享的想法类似于卷积神经网络（CNNs），其中相同的滤波器用于输入的不同部分。这使得RNN能够在训练期间处理不同长度的序列，并且更重要的是，可以推广到训练期间未见过的序列长度。图[2(b)](#S2.F2.sf2
    "在图2 ‣ 2.2.2 卷积神经网络 ‣ 2.2 神经网络架构 ‣ 2 深度预测：教程 ‣ 深度学习时间序列预测：教程和文献综述")展示了一个（展开的）RNN的基本结构。
- en: Although RNNs have been widely used in practice, training them is difficult
    given that they are typically applied to long sequences of data. A common issue
    while training very deep NNs by gradient-based methods using back-propagation
    is that of vanishing or exploding gradients  [[142](#bib.bib142)] which renders
    learning challenging.  Hochreiter and Schmidhuber [[81](#bib.bib81)] proposed
    Long short-term memory networks (LSTM) to address this problem. Similar to Resnet,
    via the skip-connections, LSTMs (and a simplified version Gated recurrent units
    (GRU) [[36](#bib.bib36)]) always offer a path where the gradient does not vanish
    or explode.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RNN在实际应用中被广泛使用，但由于它们通常应用于长序列数据，训练起来非常困难。使用基于梯度的方法通过反向传播训练非常深的神经网络时，常见的问题是梯度消失或爆炸[[142](#bib.bib142)]，这使得学习变得具有挑战性。Hochreiter和Schmidhuber[[81](#bib.bib81)]提出了长短期记忆网络（LSTM）来解决这个问题。类似于Resnet，通过跳跃连接，LSTM（以及简化版本的门控循环单元（GRU）[[36](#bib.bib36)]）总是提供一种梯度不消失或爆炸的路径。
- en: 2.2.4 Transformer
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 Transformer
- en: A more recent architecture is based on the attention mechanism which has received
    increased interest in other sequence learning tasks [[37](#bib.bib37), [38](#bib.bib38),
    [184](#bib.bib184), [116](#bib.bib116)] for its ability to improve on long sequence
    prediction tasks over RNNs. One natural way to address this issue is to learn
    more than one feature representation (contrary to RNNs), e.g., one for each time
    step of the input sequence and decide which of these representations are useful
    to predict the current element of the target sequence. Bahdanau et al. [[10](#bib.bib10)]
    suggest using a weighted sum of the representations where the weights are jointly
    learned along with the feature representation learning and the prediction. Note
    that at each time step in the prediction, one needs to learn a separate set of
    weights for the representations. This is essentially training the predictor to
    learn to which parts of the input sequence it should pay attention to produce
    a prediction. This attention mechanism has been shown to be instrumental for the
    state of the art in speech recognition and machine translations tasks [[37](#bib.bib37),
    [38](#bib.bib38)]. Inspired by the success of attention models,  Vaswani et al.
    [[184](#bib.bib184)] developed the so-called Transformer model and showed that
    attention alone is sufficient, thus making the training amenable for parallelization
    and large number of parameters [[28](#bib.bib28), [44](#bib.bib44)]. In the literature,
    the term Transformer can refer to both the specific model and to the overall architecture
    as well.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一种较新的架构基于注意力机制，这在其他序列学习任务中引起了越来越多的关注 [[37](#bib.bib37), [38](#bib.bib38), [184](#bib.bib184),
    [116](#bib.bib116)]，因为它能够改进长期序列预测任务，相比于递归神经网络（RNN）。一种自然的解决方案是学习多个特征表示（与 RNN 相反），例如，为输入序列的每个时间步学习一个特征表示，并决定这些表示中哪些对预测目标序列的当前元素有用。Bahdanau
    等人 [[10](#bib.bib10)] 建议使用加权和的表示，其中权重与特征表示学习和预测一起联合学习。请注意，在每个时间步的预测中，需要为表示学习一组单独的权重。这本质上是训练预测器学习应关注输入序列的哪些部分来生成预测。注意力机制已被证明对语音识别和机器翻译任务的最新技术具有重要作用
    [[37](#bib.bib37), [38](#bib.bib38)]。受注意力模型成功的启发，Vaswani 等人 [[184](#bib.bib184)]
    开发了所谓的 Transformer 模型，并展示了仅使用注意力机制就足够，从而使训练适合于并行化和大量参数 [[28](#bib.bib28), [44](#bib.bib44)]。在文献中，术语
    Transformer 可以指特定模型或整体架构。
- en: 2.3 Input Transformations
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 输入变换
- en: 'The careful handling of the input (parameters $\alpha_{t}$ in Fig. [1](#S2.F1
    "Figure 1 ‣ 2.2.1 Multilayer perceptron ‣ 2.2 Neural Network Architectures ‣ 2
    Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey") and [2](#S2.F2 "Figure 2 ‣ 2.2.2 Convolutional neural
    networks ‣ 2.2 Neural Network Architectures ‣ 2 Deep Forecasting: A Tutorial ‣
    Deep Learning for Time Series Forecasting: Tutorial and Literature Survey")) is
    a practically important ingredient for deep learning models in general and deep
    forecasting models in particular. Deep forecasting models are most commonly deployed
    as so-called global models (see Section [2.1](#S2.SS1 "2.1 Notation and Formalization
    of the Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey")), which means that the
    weights of the NN are trained across the panel of time series. Hence, it is important
    that the scale of the input is comparable. Standard techniques such as mean-variance
    scaling carry over to the forecasting setting. In practice, it is important to
    avoid leakage of future values in normalization schemes, so that mean and variance
    are taken over past windows (similar to causal convolutions).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '输入（图 [1](#S2.F1 "Figure 1 ‣ 2.2.1 Multilayer perceptron ‣ 2.2 Neural Network
    Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey") 和 [2](#S2.F2 "Figure 2 ‣ 2.2.2 Convolutional
    neural networks ‣ 2.2 Neural Network Architectures ‣ 2 Deep Forecasting: A Tutorial
    ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey")
    中的参数 $\alpha_{t}$）的精确处理对于深度学习模型，尤其是深度预测模型，是一个实际重要的因素。深度预测模型最常作为所谓的全局模型进行部署（见第
    [2.1](#S2.SS1 "2.1 Notation and Formalization of the Forecasting Problem ‣ 2 Deep
    Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey") 节），这意味着神经网络的权重在时间序列面板上进行训练。因此，输入的尺度必须是可比的。像均值-方差缩放这样的标准技术可以用于预测设置中。在实际操作中，重要的是避免在归一化方案中泄露未来值，以便均值和方差是基于过去窗口计算的（类似于因果卷积）。'
- en: Traditionally, the forecasting literature has used transformations such as the
    Box-Cox, i.e.,
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，预测文献使用了像 Box-Cox 这样的变换，即，
- en: '|  | $h=\frac{z^{\lambda}-1}{\lambda},$ |  | (5) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $h=\frac{z^{\lambda}-1}{\lambda},$ |  | (5) |'
- en: where $z$ is the input of the transformation, $h$ is the output and $\lambda$
    is a free parameter. Box-Cox is a popular heuristic to have the input data more
    closely resemble the Gaussian distribution. A Box-Cox transformation can be readily
    integrated into an NN, with the free parameter $\lambda$ optimized as part of
    the training process jointly with the other parameters of the network. More sophisticated
    approaches based on probability integral transformation (PIT) or Copulas are similarly
    possible, see e.g., [[88](#bib.bib88)] (and references therein) for a recent example.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z$ 是变换的输入，$h$ 是输出，$\lambda$ 是自由参数。Box-Cox 是一种流行的启发式方法，用于使输入数据更接近高斯分布。Box-Cox
    变换可以方便地集成到神经网络中，自由参数 $\lambda$ 在训练过程中与网络的其他参数一起优化。基于概率积分变换（PIT）或 Copulas 的更复杂方法也是可能的，例如
    [[88](#bib.bib88)]（以及其中的参考文献）提供了一个近期的例子。
- en: A further standard technique is the discretization of input into categorical
    values or *bins*, for example by choosing the number and borders of bins such
    that each bins contains equal mass, see e.g., [[145](#bib.bib145)] for an example
    in forecasting.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种标准技术是将输入离散化为分类值或 *区间*，例如通过选择区间的数量和边界，使每个区间包含相等的质量，例如 [[145](#bib.bib145)]
    提供了预测中的一个示例。
- en: We note that any input transformation must be reversed also to obtain values
    in the actual domain of interest. It is a choice for the modeller where/when to
    apply this reversal. Two extreme choices are to have transformation of the input
    and output fully outside the NN or have the input transformations as part of the
    NN and hence be subjected to learning.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，任何输入变换必须被逆转以获得实际关注领域的值。这是模型设计者决定何时/何地应用这种逆转的选择。两种极端的选择是将输入和输出的变换完全置于神经网络之外，或者将输入变换作为神经网络的一部分，从而接受学习。
- en: 2.4 Output Models and Loss Functions
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 输出模型和损失函数
- en: 'Similar to the input, the output ($\beta_{t}$ in Fig. [1](#S2.F1 "Figure 1
    ‣ 2.2.1 Multilayer perceptron ‣ 2.2 Neural Network Architectures ‣ 2 Deep Forecasting:
    A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey") and [2](#S2.F2 "Figure 2 ‣ 2.2.2 Convolutional neural networks ‣ 2.2
    Neural Network Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning
    for Time Series Forecasting: Tutorial and Literature Survey")) deserve a special
    discussion. Closely related is the question on the choice of loss function which
    we use to train a NN. The simplest form of an output is a single value, also referred
    to as a *point* forecast. For this case, the output $\hat{z}_{i,t}$ is the best
    (w.r.t. the chosen loss function) estimate for the true value $z_{i,t}$. Standard
    regression loss functions (like $\ell_{p}$ losses with their regularized modifications)
    can be used or more sophistication accuracy metrics specifically geared towards
    forecasting such as the MASE, sMAPE or others [[85](#bib.bib85)].'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '与输入类似，输出（图 [1](#S2.F1 "Figure 1 ‣ 2.2.1 Multilayer perceptron ‣ 2.2 Neural
    Network Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time
    Series Forecasting: Tutorial and Literature Survey") 和 [2](#S2.F2 "Figure 2 ‣
    2.2.2 Convolutional neural networks ‣ 2.2 Neural Network Architectures ‣ 2 Deep
    Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey") 中的 $\beta_{t}$）值得特别讨论。相关的问题是我们用来训练神经网络的损失函数的选择。最简单的输出形式是单个值，也称为
    *点* 预测。在这种情况下，输出 $\hat{z}_{i,t}$ 是对真实值 $z_{i,t}$ 的最佳（相对于选择的损失函数）估计。可以使用标准回归损失函数（如
    $\ell_{p}$ 损失及其正则化修改），或更复杂的精度度量，专门用于预测，如 MASE、sMAPE 或其他 [[85](#bib.bib85)]。'
- en: 'As remarked in Section [2.1](#S2.SS1 "2.1 Notation and Formalization of the
    Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time
    Series Forecasting: Tutorial and Literature Survey"), a point estimate $\hat{z}_{i,t}$
    can be seen as a particular realization from a probabilistic estimate of $p(z_{i,t})$.
    Depending on the accuracy metric used in forecasting, a different realization
    may be appropriate [[104](#bib.bib104)]. So, even for obtaining point forecasts,
    *probabilistic* forecasts are important. More importantly, forecasts are often
    used in downstream optimization problem where some form of *expected* cost is
    to be minimized and for this, an estimate of the entire probability distribution
    is required. The probability distribution can be represented equivalently by its
    probability density function (PDF), the cumulative density function (CDF) or its
    inverse, the quantile function. Fig. [3](#S2.F3 "Figure 3 ‣ 2.4 Output Models
    and Loss Functions ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey") contains a visualization of the
    different representations for the Gaussian distribution. Across the deep forecasting
    landscape, most approaches (e.g., [[156](#bib.bib156), [64](#bib.bib64), [150](#bib.bib150),
    [147](#bib.bib147), [146](#bib.bib146)]), have chosen the PDF and quantile function
    to represent $p(z_{i,t})$ and we will discuss general recipes next. Since the
    CDF has typically not been chosen to represent $p(z_{i,t})$, we do not discuss
    it further.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[2.1节](#S2.SS1 "2.1 符号和预测问题的形式化 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述")中所述，点估计$\hat{z}_{i,t}$可以看作是$
    p(z_{i,t})$的概率估计的一种特定实现。根据预测中使用的准确度指标，可能需要不同的实现[[104](#bib.bib104)]。因此，即使是获得点预测，*概率性*预测也很重要。更重要的是，预测通常用于下游优化问题，其中需要最小化某种形式的*期望*成本，为此，需要对整个概率分布进行估计。概率分布可以通过其概率密度函数（PDF）、累积分布函数（CDF）或其反函数，即分位数函数等效表示。图[3](#S2.F3
    "图 3 ‣ 2.4 输出模型和损失函数 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述")展示了高斯分布不同表示的可视化。在深度预测领域，大多数方法（例如[[156](#bib.bib156)，[64](#bib.bib64)，[150](#bib.bib150)，[147](#bib.bib147)，[146](#bib.bib146)]）选择了PDF和分位数函数来表示$p(z_{i,t})$，接下来我们将讨论一般方法。由于CDF通常未被选择来表示$p(z_{i,t})$，我们不再进一步讨论。
- en: '![Refer to caption](img/7dc096dba7e62b5aed5f672bbf8380a8.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7dc096dba7e62b5aed5f672bbf8380a8.png)'
- en: 'Figure 3: For a Gaussian distribution, its density function $f$ is on the left-hand
    panel, the corresponding cumulative density function $F$ (the primitive integral
    of $f$) in the central panel and the quantile function $F^{-1}$ on the right-hand
    panel.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：对于高斯分布，其密度函数$f$在左侧面板中，相应的累积分布函数$F$（$f$的原始积分）在中央面板中，分位数函数$F^{-1}$在右侧面板中。
- en: 2.4.1 PDF
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 PDF
- en: 'Arguably the most common way to represent a probability distribution in forecasting
    is via its PDF. The literature contains examples of using the standard parametric
    distribution families to represent probabilistic forecasts. For example, the output
    layer of an NN may produce the mean and variance parameter of a Gaussian distribution.
    So, the parameter $\beta_{t}$ in Fig. [1](#S2.F1 "Figure 1 ‣ 2.2.1 Multilayer
    perceptron ‣ 2.2 Neural Network Architectures ‣ 2 Deep Forecasting: A Tutorial
    ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey")
    and [2](#S2.F2 "Figure 2 ‣ 2.2.2 Convolutional neural networks ‣ 2.2 Neural Network
    Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey") is a two-dimensional vector corresponding
    to $\mu_{t}$ and $\sigma_{t}$ of a Gaussian distribution. We typically achieve
    $\sigma_{t}\geq 0$ by mapping the corresponding parameter through a softplus function.
    For the loss function, a natural choice is the negative log-likelihood (NLL) since
    a PDF allows to readily compute the likelihood of a point under it.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '可以说，预测中最常见的概率分布表示方法是通过其PDF。文献中包含了使用标准参数分布族来表示概率预测的例子。例如，神经网络（NN）的输出层可能会产生高斯分布的均值和方差参数。因此，图[1](#S2.F1
    "Figure 1 ‣ 2.2.1 Multilayer perceptron ‣ 2.2 Neural Network Architectures ‣ 2
    Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey")和[2](#S2.F2 "Figure 2 ‣ 2.2.2 Convolutional neural networks
    ‣ 2.2 Neural Network Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning
    for Time Series Forecasting: Tutorial and Literature Survey")中的参数$\beta_{t}$是一个二维向量，对应于高斯分布的$\mu_{t}$和$\sigma_{t}$。我们通常通过softplus函数将相应的参数映射为$\sigma_{t}\geq
    0$。对于损失函数，自然的选择是负对数似然（NLL），因为PDF允许直接计算点在其下的似然。'
- en: 'Beyond Gaussian likelihood, a number of differentiable parametric distributions
    have been used in the literature depending on the nature of the forecasting problem,
    e.g., the student-t distribution or the Tweedie distribution for continuous data,
    the negative binomial distribution for count data and more flexible approaches
    via mixtures of Gaussian. Although forecasting is most commonly done for domains
    of numerical values (i.e., we assume $z_{i,t}$ to be in $\mathbb{R}$ or $\mathbb{N}$),
    other distributions such as the multinomial have also been employed successfully
    in forecasting even though they have no notion of the order on the domain [[145](#bib.bib145)].
    The deployment of a multinomial distribution requires a binning of the input values
    (see Section [2.3](#S2.SS3 "2.3 Input Transformations ‣ 2 Deep Forecasting: A
    Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey")). An alternative approach is to cut the output space in bins and treat
    each of them as a uniform distribution, while modelling the tails with a parametric
    distribution [[50](#bib.bib50)], this results in a piecewise linear CDF.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '除了高斯似然，文献中还使用了多种可微分的参数分布，这取决于预测问题的性质，例如，对于连续数据使用学生-t分布或Tweedie分布，对于计数数据使用负二项分布，并通过高斯混合模型采用更灵活的方法。尽管预测最常用于数值值领域（即我们假设$z_{i,t}$在$\mathbb{R}$或$\mathbb{N}$中），但其他分布，如多项分布，也在预测中取得了成功，即使它们对域没有排序的概念[[145](#bib.bib145)]。多项分布的部署需要对输入值进行分箱（见第[2.3](#S2.SS3
    "2.3 Input Transformations ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey")节）。另一种方法是将输出空间分割成若干箱，将每一箱视为均匀分布，同时用参数分布建模尾部[[50](#bib.bib50)]，这会导致分段线性累积分布函数（CDF）。'
- en: 2.4.2 Quantile function
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 分位数函数
- en: Another representation of $p(z_{i,t})$ is via the quantile function which has
    a particular importance for forecasting. Often, a particular quantile is of practical
    interest. For example, in a simplified supply chain scenario for inventory control,
    there is a direct correspondence between the chosen quantile and a safety stock
    level in the newsvendor problem [[54](#bib.bib54)].
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: $p(z_{i,t})$的另一种表示方法是通过分位数函数，这在预测中具有特别重要的意义。通常，某个特定的分位数具有实际意义。例如，在简化的供应链库存控制场景中，所选择的分位数与新闻贩问题中的安全库存水平之间存在直接对应关系[[54](#bib.bib54)]。
- en: So naturally, estimating the quantiles directly via quantile regression approaches [[103](#bib.bib103)]
    is a common choice in forecasting either via choosing a single quantile (in a
    point-forecasting approach) or multiple quantiles simultaneously [[51](#bib.bib51),
    [190](#bib.bib190)]. Essentially, this discretizes the quantile function and estimates
    specific points only. A common choice for the loss function is the quantile loss
    or pinball loss. For the $q$-th quantile and $F^{-1}$ the quantile function, the
    quantile loss is defined as
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，自然地，通过分位数回归方法[[103](#bib.bib103)]直接估计分位数是一种常见的预测选择，无论是选择单一分位数（在点预测方法中）还是同时选择多个分位数[[51](#bib.bib51),
    [190](#bib.bib190)]。本质上，这种方法离散化了分位数函数，并仅估计特定的点。常见的损失函数选择是分位数损失或弹球损失。对于$q$-th分位数和$F^{-1}$的分位数函数，分位数损失定义为
- en: '|  | $\mathrm{QS}_{q}\left(\hat{F}_{i,t}^{-1}(q),z_{i,t}\right)\vcentcolon=2\left(\mathbbm{1}_{\{z_{i,t}\leq\hat{F}_{i,t}^{-1}(q)\}}-q\right)\left(\hat{F}_{i,t}^{-1}(q)-z_{i,t}\right),$
    |  | (6) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{QS}_{q}\left(\hat{F}_{i,t}^{-1}(q),z_{i,t}\right)\vcentcolon=2\left(\mathbbm{1}_{\{z_{i,t}\leq\hat{F}_{i,t}^{-1}(q)\}}-q\right)\left(\hat{F}_{i,t}^{-1}(q)-z_{i,t}\right),$
    |  | (6) |'
- en: where $\mathbbm{1}_{\{\text{{cond}}\}}$ is the indicator function that is equal
    to 1 if cond is true and 0 otherwise. The output of the NN is $\hat{F}_{i,t}^{-1}(q)$,
    i.e., the estimated value of the $q$-th quantile. For $q=0.5$ this reduces to
    the median of the forecast distribution and is a common choice of point forecasts.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbbm{1}_{\{\text{{cond}}\}}$是指示函数，当条件为真时等于1，否则为0。NN的输出是$\hat{F}_{i,t}^{-1}(q)$，即$q$-th分位数的估计值。对于$q=0.5$，这等同于预测分布的中位数，是点预测的常见选择。
- en: As an alternative to a quantile regression approach, we can make a parametric
    assumption on the quantile function and estimate it directly. The main requirements
    for modelling a quantile function are that its domain should be constrained to
    $[0,1]$ and the function should be monotonically increasing. This can be achieved
    easily via linear splines for example, so the output of the NN’s last layers are
    the corresponding free parameters. For the loss function, a rich theory around
    the continuous ranked probability score (CRPS) exists [[132](#bib.bib132), [69](#bib.bib69)]
    and CRPS can be used as a loss function directly. CRPS can be defined [[108](#bib.bib108)]
    to summarize all possible quantile losses as
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分位数回归方法的替代方案，我们可以对分位数函数做参数假设，并直接估计它。建模分位数函数的主要要求是其定义域应限制在$[0,1]$，且函数应单调递增。例如，这可以通过线性样条轻松实现，因此NN的最后几层输出即为对应的自由参数。关于损失函数，存在丰富的连续排名概率分数（CRPS）理论[[132](#bib.bib132),
    [69](#bib.bib69)]，并且CRPS可以直接用作损失函数。CRPS可以定义[[108](#bib.bib108)]以总结所有可能的分位数损失为
- en: '|  | $\displaystyle\mathrm{CRPS}(\hat{F}_{i,t},z_{i,t})$ | $\displaystyle\vcentcolon=\int_{0}^{1}\mathrm{QS}_{q}\left(\hat{F}_{i,t}^{-1}(q),z_{i,t}\right)\
    dq.$ |  | (7) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{CRPS}(\hat{F}_{i,t},z_{i,t})$ | $\displaystyle\vcentcolon=\int_{0}^{1}\mathrm{QS}_{q}\left(\hat{F}_{i,t}^{-1}(q),z_{i,t}\right)\
    dq.$ |  | (7) |'
- en: Multivariate extensions such as the energy score [[69](#bib.bib69)] exist.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量扩展，例如能量分数[[69](#bib.bib69)]，是存在的。
- en: 'Interestingly, a popular discretization strategy, adaptive binning, used with
    multinomial distributions corresponds to quantile functions parametrized by piece-wise
    linear splines, see Fig. [4](#S2.F4 "Figure 4 ‣ 2.4.2 Quantile function ‣ 2.4
    Output Models and Loss Functions ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning
    for Time Series Forecasting: Tutorial and Literature Survey").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，一种流行的离散化策略，自适应分箱，用于多项分布对应于由分段线性样条参数化的分位数函数，见图[4](#S2.F4 "图4 ‣ 2.4.2 分位数函数
    ‣ 2.4 输出模型和损失函数 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述")。
- en: '![Refer to caption](img/d2323830a2378001bf8f3eef22c450a2.png)![Refer to caption](img/aba8260a128ebaa46b580269f3e40f4c.png)![Refer
    to caption](img/30160d05aea89eda993431b3d995fd91.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d2323830a2378001bf8f3eef22c450a2.png)![参见说明](img/aba8260a128ebaa46b580269f3e40f4c.png)![参见说明](img/30160d05aea89eda993431b3d995fd91.png)'
- en: 'Figure 4: An illustration how a quantile function parametrized by linear splines
    (left panel) corresponds to a piece-wise linear CDF (middle) which in turn corresponds
    to a piece-wise constant PDF as assumed in an adaptive binning strategy (right
    panel).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：一个说明，如何通过线性样条参数化的分位数函数（左侧面板）对应于分段线性CDF（中间），而分段常数PDF则假设在自适应分箱策略中（右侧面板）。
- en: 2.4.3 Further approaches
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3 进一步的方法
- en: 'The recent deep learning literature contains more advanced examples for density
    estimation, most prominently via Generalized Adversarial Networks (GANs). We discuss
    them in Section [3.8](#S3.SS8 "3.8 Generalized Adversarial Networks ‣ 3 Literature
    review ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey")
    and discuss *normalizing flows* here which have arguably resonated more strongly
    in forecasting. Normalizing flows are invertible NNs that transform a simple distribution
    to a more complex output distribution. Invertibility guarantees the conservation
    of probability mass and allows the evaluation of the associated density function
    everywhere. The key observation is that the probability density of an observation
    $z_{i,t}$ can be computed using the change of variables formula:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的深度学习文献中包含了更多先进的密度估计示例，其中最突出的是通过生成对抗网络（GANs）。我们在第[3.8节](#S3.SS8 "3.8 Generalized
    Adversarial Networks ‣ 3 Literature review ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey")中讨论了这些方法，并在此讨论*归一化流*，它们在预测中显然产生了更强的共鸣。归一化流是可逆的神经网络，它将简单分布转换为更复杂的输出分布。可逆性保证了概率质量的守恒，并允许在所有地方评估相关的密度函数。关键的观察是，观察值
    $z_{i,t}$ 的概率密度可以使用变量变化公式计算：'
- en: '|  | $p(z_{i,t})=p_{y_{i,t}}(f^{-1}(z_{i,t}))&#124;\text{det}[\text{Jac}{f_{i,t}^{-1}}{z_{i,t}}]&#124;,$
    |  | (8) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(z_{i,t})=p_{y_{i,t}}(f^{-1}(z_{i,t}))&#124;\text{det}[\text{Jac}{f_{i,t}^{-1}}{z_{i,t}}]&#124;,$
    |  | (8) |'
- en: where the first term $p_{y_{i,t}}(f^{-1}(z_{i,t}))$ is the (in general simple)
    density of a variable $y_{i,t}$, and the second is the absolute value of the determinant
    of the Jacobian of $f_{i,t}^{-1}$, evaluated at $z_{i,t}$.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首项 $p_{y_{i,t}}(f^{-1}(z_{i,t}))$ 是变量 $y_{i,t}$ 的（通常是简单的）密度，第二项是雅可比矩阵 $f_{i,t}^{-1}$
    在 $z_{i,t}$ 处的行列式的绝对值。
- en: 'The invertible function $f$ is typically parametrized by an NN. A particular
    instantiation is the Box-Cox transformation, Eq. ([5](#S2.E5 "In 2.3 Input Transformations
    ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey")). The field of normalizing flows (e.g., [[46](#bib.bib46),
    [101](#bib.bib101), [137](#bib.bib137)]) studies invertible NNs that typically
    transform isotropic Gaussians to more complex data distributions. The choice of
    a particular instantiation of $f$ can facilitate the computation of the likelihood
    of a given point when the NLL is amenable as a loss function. Alternatively, generating
    samples may be computationally more viable for other instantiations (this is typically
    the cases with generative adversarial networks as well). In this case, the NLL
    can be replaced by other loss functions such as CRPS.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '可逆函数 $f$ 通常由神经网络参数化。一个特定的实例是Box-Cox变换，见公式（[5](#S2.E5 "In 2.3 Input Transformations
    ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey")）。归一化流领域（例如，[[46](#bib.bib46), [101](#bib.bib101),
    [137](#bib.bib137)]）研究通常将各向同性高斯分布转换为更复杂数据分布的可逆神经网络。选择 $f$ 的特定实例可以在负对数似然（NLL）作为损失函数时简化给定点的似然计算。或者，对于其他实例，生成样本可能在计算上更可行（这通常也是生成对抗网络的情况）。在这种情况下，NLL
    可以被其他损失函数如 CRPS 替代。'
- en: A number of extensions are possible. For example, more complex models for $p(z_{i,t})$
    are possible such as Hidden Markov Models or Linear Dynamical Systems. NNs can
    output the free parameters of these models but then need to be combined with the
    learning and inference schemes associated with these models, such as Kalman Filtering/Smoothing
    in the case of Linear Dynamical System [[146](#bib.bib146), [42](#bib.bib42)]
    or the Forward/Backward Algorithm in the case of Hidden Markov Models [[5](#bib.bib5)].
    Another avenue is to relax constraints on the representation of $p(z_{i,t})$ to
    obtain closely related objects with more favorable computational properties. For
    example, *energy based models* (EBMs) approximate the unnormalized log-probability [[115](#bib.bib115),
    [77](#bib.bib77)]. EBMs perform well in learning high dimensional distributions
    at the cost of being difficult to train [[174](#bib.bib174)] and have been employed
    in forecasting [[151](#bib.bib151)].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多扩展的可能性。例如，对于$p(z_{i,t})$，可以使用更复杂的模型，如隐马尔可夫模型或线性动态系统。神经网络可以输出这些模型的自由参数，但随后需要与这些模型相关的学习和推断方案结合使用，例如线性动态系统中的卡尔曼滤波/平滑[[146](#bib.bib146),
    [42](#bib.bib42)]或隐马尔可夫模型中的前向/后向算法[[5](#bib.bib5)]。另一种途径是放宽对$p(z_{i,t})$表示的约束，以获得计算属性更有利的相关对象。例如，*基于能量的模型*（EBMs）近似无归一化对数概率[[115](#bib.bib115),
    [77](#bib.bib77)]。EBMs在学习高维分布方面表现良好，但训练难度较大[[174](#bib.bib174)]，并已应用于预测[[151](#bib.bib151)]。
- en: 2.5 Archetypical Architectures
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 原型架构
- en: '![Refer to caption](img/bd94b3d78847f10089387d34cb6baf65.png)![Refer to caption](img/9d159052b9938b6e27e9570c1ba88071.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd94b3d78847f10089387d34cb6baf65.png)![参见说明](img/9d159052b9938b6e27e9570c1ba88071.png)'
- en: 'Figure 5: Canonical versus sequence-to-sequence models.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：标准模型与序列到序列模型的对比。
- en: With all key components in place, in this section we present in more details
    popular forecasting architectures. In particular we focus on the widely-used RNN-based
    architecture that takes as input its previous hidden state, the currently available
    information and produces an one-step ahead estimate of the target time series.
    There are subtle details on how to handle a multi-step unrolled model during training
    (e.g., [[109](#bib.bib109)]), which we will skip over. We further examine the
    sequence-to-sequence (seq2seq) modelling approach where the model takes an *encoding
    sequence* as input and maps it to a *decoding* sequence (of predetermined length)
    on which the loss is computed against the actual values $\mathbf{z}$ during training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有关键组件到位后，本节将更详细地介绍流行的预测架构。特别地，我们关注广泛使用的基于RNN的架构，该架构以其先前的隐藏状态、当前可用信息为输入，并生成目标时间序列的一步预测。有关如何在训练期间处理多步展开模型的细节（例如，[[109](#bib.bib109)]），我们将略过。我们进一步探讨了序列到序列（seq2seq）建模方法，其中模型将*编码序列*作为输入，并将其映射到*解码*序列（预定长度），在训练过程中计算与实际值$\mathbf{z}$的损失。
- en: <svg   height="194.46" overflow="visible" version="1.1" width="302.31"><g transform="translate(0,194.46)
    matrix(1 0 0 -1 0 0) translate(17.3,0) translate(0,17.3)" fill="#000000" stroke="#808080"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 57.88 -1.73)" fill="#000000"
    stroke="#000000"><foreignobject width="18.11" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\alpha_{t-1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 128.03 -1.79)" fill="#000000" stroke="#000000"><foreignobject width="11.65"
    height="8.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 190.01 -1.41)" fill="#000000" stroke="#000000"><foreignobject
    width="21.55" height="9.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -8.47 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="16.94" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t-2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 58.46 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="16.94" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 128.62 55.44)" fill="#000000" stroke="#000000"><foreignobject
    width="10.49" height="11.99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 190.59 55.82)" fill="#000000" stroke="#000000"><foreignobject
    width="20.39" height="12.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 257.52 55.82)" fill="#000000" stroke="#000000"><foreignobject
    width="20.39" height="12.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t+2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 49.14 144.18)" fill="#000000" stroke="#000000"><foreignobject
    width="35.58" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$(\hat{\mu}_{t},\hat{\sigma}_{t})$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 106.17 144.18)" fill="#000000" stroke="#000000"><foreignobject
    width="55.38" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$(\hat{\mu}_{t+1},\hat{\sigma}_{t+1})$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 173.1 144.18)" fill="#000000" stroke="#000000"><foreignobject
    width="55.38" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$(\hat{\mu}_{t+2},\hat{\sigma}_{t+2})$</foreignobject></g></g></svg>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="194.46" overflow="visible" version="1.1" width="302.31"><g transform="translate(0,194.46)
    matrix(1 0 0 -1 0 0) translate(17.3,0) translate(0,17.3)" fill="#000000" stroke="#808080"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 57.88 -1.73)" fill="#000000"
    stroke="#000000"><foreignobject width="18.11" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\alpha_{t-1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 128.03 -1.79)" fill="#000000" stroke="#000000"><foreignobject width="11.65"
    height="8.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 190.01 -1.41)" fill="#000000" stroke="#000000"><foreignobject
    width="21.55" height="9.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -8.47 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="16.94" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t-2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 58.46 55.5)" fill="#000000" stroke="#000000"><foreignobject
    width="16.94" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 128.62 55.44)" fill="#000000" stroke="#000000"><foreignobject
    width="10.49" height="11.99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 190.59 55.82)" fill="#000000" stroke="#000000"><foreignobject
    width="20.39" height="12.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 257.52 55.82)" fill="#000000" stroke="#000000"><foreignobject
    width="20.39" height="12.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{h}_{t+2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 49.14 144.18)" fill="#000000" stroke="#000000"><foreignobject
    width="35.58" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$(\hat{\mu}_{t},\hat{\sigma}_{t})$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 106.17 144.18)" fill="#000000" stroke="#000000"><foreignobject
    width="55.38" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$(\hat{\mu}_{t+1},\hat{\sigma}_{t+1})$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 173.1 144.18)" fill="#000000" stroke="#000000"><foreignobject
    width="55.38" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$(\hat{\mu}_{t+2},\hat{\sigma}_{t+2})$</foreignobject></g></g></svg>
- en: 'Figure 6: DeepAR: The model outputs parameters of a previously chosen family
    of distributions. Samples from this distribution can be fed back into the model
    during prediction (dotted lines) or in case of $\alpha_{t}$ being missing.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：DeepAR：该模型输出先前选择的分布族的参数。在预测过程中（虚线）或在$\alpha_{t}$缺失的情况下，可以将该分布的样本反馈给模型。
- en: 'A typical instance in the training set in this approach consists of the target
    and covariate values up to a certain point in time $t$ as the encoding sequence
    and the outputs of the NN are a predetermined number of target values after time
    $t$. Figure [5](#S2.F5 "Figure 5 ‣ 2.5 Archetypical Architectures ‣ 2 Deep Forecasting:
    A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey") contrasts both approaches. In the following we present two popular deep
    forecasting models, DeepAR and MQRNN/MQCNN, in some details to illustrate the
    core concepts. They represent the one-step-ahead RNN-based and seq2seq approach,
    respectively.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种方法中，训练集中的典型实例由目标值和协变量值组成，直到时间 $t$ 作为编码序列，NN 的输出是时间 $t$ 之后的预定数量的目标值。图 [5](#S2.F5
    "Figure 5 ‣ 2.5 Archetypical Architectures ‣ 2 Deep Forecasting: A Tutorial ‣
    Deep Learning for Time Series Forecasting: Tutorial and Literature Survey") 对比了这两种方法。接下来，我们将详细介绍两个流行的深度预测模型，DeepAR
    和 MQRNN/MQCNN，以说明核心概念。它们分别代表了基于 RNN 的一步预测和 seq2seq 方法。'
- en: 2.5.1 DeepAR
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.1 DeepAR
- en: 'Among the first of the modern deep forecasting models is DeepAR [[156](#bib.bib156)],
    a global univariate model (see Table [1](#S2.T1 "Table 1 ‣ Remarks ‣ 2.1 Notation
    and Formalization of the Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial
    ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey"))
    that consists of an RNN backbone (typically an LSTM).²²2Hewamalage et al. [[76](#bib.bib76)]
    provide an overview specifically targeted at RNNs for forecasting. The input of
    the model is a combination of lagged target values and relevant covariates. The
    output is either a point forecast with a standard loss function or, in the basic
    variant, a probabilistic forecast via the parameters of a PDF (e.g., $\mu$ and
    $\sigma$ of a Gaussian distribution), where the loss function is then the NLL.
    The output modelling of DeepAR has been the subject of follow-up work, e.g., Jeon
    and Seong [[93](#bib.bib93)] propose a Tweedie loss, Mukherjee et al. [[136](#bib.bib136)]
    propose a mixture of Gaussians as the distribution and domain specific feature
    processing blocks. Figure [6](#S2.F6 "Figure 6 ‣ 2.5 Archetypical Architectures
    ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey") summarizes the architecture. The dotted arrows
    in the picture correspond to drawing a sample that can be used as alternative
    input (as a lagged target) during training (even though $\alpha_{t}$ may be available
    or in the case where an $\alpha_{t}$ is missing) and during prediction to obtain
    multi-step ahead forecasts.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '现代深度预测模型中最早的之一是 DeepAR [[156](#bib.bib156)]，它是一个全球单变量模型（见表 [1](#S2.T1 "Table
    1 ‣ Remarks ‣ 2.1 Notation and Formalization of the Forecasting Problem ‣ 2 Deep
    Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey")），由一个 RNN 主干（通常是 LSTM）组成。²²2Hewamalage 等 [[76](#bib.bib76)]
    提供了专门针对 RNN 进行预测的概述。模型的输入是滞后目标值和相关协变量的组合。输出可以是带有标准损失函数的点预测，或者在基本变体中，通过 PDF 的参数（例如，高斯分布的
    $\mu$ 和 $\sigma$）进行概率预测，其中损失函数为 NLL。DeepAR 的输出建模已经成为后续工作的主题，例如，Jeon 和 Seong [[93](#bib.bib93)]
    提出了 Tweedie 损失，Mukherjee 等 [[136](#bib.bib136)] 提出了高斯混合作为分布以及领域特定特征处理块。图 [6](#S2.F6
    "Figure 6 ‣ 2.5 Archetypical Architectures ‣ 2 Deep Forecasting: A Tutorial ‣
    Deep Learning for Time Series Forecasting: Tutorial and Literature Survey") 总结了架构。图中的虚线箭头对应于绘制一个可以用作替代输入（作为滞后目标）在训练期间（即使
    $\alpha_{t}$ 可能可用或在 $\alpha_{t}$ 缺失的情况下）和在预测期间，以获得多步预测。'
- en: 'It is also possible to change the output of DeepAR to model the quantile function
    and use CRPS, Eq. ([7](#S2.E7 "In 2.4.2 Quantile function ‣ 2.4 Output Models
    and Loss Functions ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey")), as the loss function [[64](#bib.bib64),
    [96](#bib.bib96), [72](#bib.bib72)]. While this in general computationally challenging,
    special cases are amendable for practical computation. For example, we can assume
    a parametrization of the quantile function by linear isotonic regression splines:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '也可以改变 DeepAR 的输出，以建模分位数函数，并使用 CRPS，公式 ([7](#S2.E7 "In 2.4.2 Quantile function
    ‣ 2.4 Output Models and Loss Functions ‣ 2 Deep Forecasting: A Tutorial ‣ Deep
    Learning for Time Series Forecasting: Tutorial and Literature Survey")) 作为损失函数
    [[64](#bib.bib64), [96](#bib.bib96), [72](#bib.bib72)]。虽然这在一般情况下计算上具有挑战性，但特殊情况可以进行实际计算。例如，我们可以假设通过线性等距回归样条对分位数函数进行参数化：'
- en: '|  | $\displaystyle s(q;\gamma,b,d)=\gamma+\sum_{\ell=0}^{L}b_{\ell}(q-d_{\ell})_{+}\;$
    |  | (9) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s(q;\gamma,b,d)=\gamma+\sum_{\ell=0}^{L}b_{\ell}(q-d_{\ell})_{+}\;$
    |  | (9) |'
- en: 'where $q\in[0,1]$ is the quantile level, $\gamma\in\mathbb{R}$ is the intercept
    term, $b\in\mathbb{R}^{L+1}$ are weights describing the slopes of the function
    pieces, $d\in\mathbb{R}^{L+1}$ is a vector of knot positions, $L$ the number of
    pieces of the spline and $(x)_{+}=\max(x,0)$ is the ReLU function. In order for
    $s(\cdot)$ to represent a quantile function we need to guarantee its monotonicity
    and restrict its domain to $[0,1]$. Both of these constraints can readily be achieved
    using standard NN tooling using a reparametrization of Eq. ([9](#S2.E9 "In 2.5.1
    DeepAR ‣ 2.5 Archetypical Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep
    Learning for Time Series Forecasting: Tutorial and Literature Survey")), while
    CRPS can be solved in closed form for linear splines (see [[64](#bib.bib64)]).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $q\in[0,1]$ 是分位数水平，$\gamma\in\mathbb{R}$ 是截距项，$b\in\mathbb{R}^{L+1}$ 是描述函数片段斜率的权重，$d\in\mathbb{R}^{L+1}$
    是节点位置向量，$L$ 是样条的片段数量，$(x)_{+}=\max(x,0)$ 是 ReLU 函数。为了使 $s(\cdot)$ 表示分位数函数，我们需要保证其单调性，并将其定义域限制在
    $[0,1]$。这两个约束可以通过使用标准的神经网络工具并重新参数化方程 ([9](#S2.E9 "In 2.5.1 DeepAR ‣ 2.5 Archetypical
    Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey")) 来轻松实现，而 CRPS 可以为线性样条形式求解（参见 [[64](#bib.bib64)]）。'
- en: Bag of tricks.
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 各种技巧的集合。
- en: While the general setup of DeepAR is straightforward, a number of algorithmic
    optimizations turn it into a robust, general-purpose forecasting model. The handling
    of missing values via sample replacement from the probability distribution is
    one such example. Another one is oversampling of “important” training examples
    during training, where importance typically corresponds to time series with larger
    absolute values. Adding lagged values further help improve predictive accuracy.
    Lags can be chosen heuristically based on the frequency of the time series. For
    example, in a time series with daily frequency, a lag of 7 days often helps. Similarly,
    covariates corresponding to calendar events (e.g., indicator variables for weekends
    or holidays) can help further.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DeepAR 的一般设置很简单，但通过一系列算法优化，它变成了一个强大的通用预测模型。通过从概率分布中替换样本来处理缺失值就是一个这样的例子。另一个例子是在训练过程中对“重要”训练样本进行过采样，其中“重要性”通常对应于绝对值较大的时间序列。添加滞后值进一步有助于提高预测准确性。滞后值可以基于时间序列的频率进行启发式选择。例如，在具有每日频率的时间序列中，7天的滞后通常会有所帮助。类似地，与日历事件相关的协变量（例如，周末或假期的指示变量）也可以进一步提供帮助。
- en: 2.5.2 MQRNN/MQCNN
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.2 MQRNN/MQCNN
- en: 'As an example for another type of deep forecasting model, we discuss the multi-horizon
    quantile recurrent forecaster (MQRNN) [[190](#bib.bib190)] next which was conceived
    concurrently to DeepAR. Contrary to DeepAR, it is most naturally deployed as a
    discriminative, seq2seq model in a quantile regression setting. For each time
    point $t$ in the forecast horizon, MQRNN outputs a chosen number of estimates
    for corresponding quantiles and the loss function in MQRNN is Eq. ([6](#S2.E6
    "In 2.4.2 Quantile function ‣ 2.4 Output Models and Loss Functions ‣ 2 Deep Forecasting:
    A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey")), i.e., the pinball loss summed over all quantiles and time points.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '作为另一种深度预测模型的例子，我们接下来讨论多时间跨度分位数递归预测模型（MQRNN）[[190](#bib.bib190)]，该模型与 DeepAR
    同时构思。与 DeepAR 相对，它最自然地作为一个分位数回归设置中的判别性 seq2seq 模型进行部署。在预测范围内的每个时间点 $t$，MQRNN 输出一个选择的分位数估计值，并且
    MQRNN 的损失函数是方程 ([6](#S2.E6 "In 2.4.2 Quantile function ‣ 2.4 Output Models and
    Loss Functions ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey"))，即针对所有分位数和时间点的钉球损失的总和。'
- en: While MQRNN can use multiple configurations, often a CNN-based architecture
    is chosen in practice in the encoder (MQCNN) for computational efficiency reasons
    over RNN-based methods and two MLPs in the decoder. The first MLP captures all
    inputs during the forecast horizon and the context provided by the encoder. A
    second, local MLP applies only to specific horizons for which it uses the corresponding
    available input and the output of the MLP. A further innovation provided by MQCNN
    is the training scheme via the so-called *forking sequences* where the model forecasts
    by placing a series of decoders with shared parameters at each timestep in the
    encoder. Thus, the model can structurally forecast at each timestep, while the
    optimization process is stabilized by updating the gradients from the sequences
    together. An additional component of MQRNN is a local MLP component that aims
    to model spikes and events specifically.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 MQRNN 可以使用多种配置，但在实践中，为了计算效率，通常选择基于 CNN 的架构（MQCNN）作为编码器，而不是基于 RNN 的方法，并且解码器中使用两个
    MLP。第一个 MLP 捕捉了预测时间范围内的所有输入和编码器提供的上下文。第二个局部 MLP 仅适用于特定的时间范围，它使用对应的可用输入和 MLP 的输出。MQCNN
    还提供了一种通过所谓的*分叉序列*进行训练的创新方法，其中模型通过在编码器的每个时间步放置一系列具有共享参数的解码器来进行预测。因此，模型可以在每个时间步进行结构化预测，而优化过程则通过一起更新来自序列的梯度来稳定。MQRNN
    的另一个组件是一个局部 MLP 组件，旨在专门建模峰值和事件。
- en: 3 Literature review
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 文献综述
- en: In the prior section, we provided an in-depth introduction to selected, basic
    topics. Building on these topics, we survey the literature on modern deep forecasting
    models more broadly in this section. Given the breadth of the literature available,
    our selection is necessarily subjective.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分，我们对选定的基本主题进行了深入介绍。在这一部分，我们在更广泛的范围内审视了现代深度预测模型的文献。鉴于文献的广度，我们的选择必然是主观的。
- en: 'We proceed as follows. In Section [3.1](#S3.SS1 "3.1 Probabilistic Forecast
    Models ‣ 3 Literature review ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey") we present probabilistic forecasting models, both one-step
    and multi-step. Similarly, in Section [3.2](#S3.SS2 "3.2 Point Forecast Models
    ‣ 3 Literature review ‣ Deep Learning for Time Series Forecasting: Tutorial and
    Literature Survey") we summarize point forecast models. We remark that, after
    Section [2](#S2 "2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey"), we have recipes at hand to turn
    an one-step ahead forecasting model into a multi-step forecasting model and a
    point forecasting model into a probabilistic model. We discuss hybrids of deep
    learning with state space models in Section [3.3](#S3.SS3 "3.3 Deep State Space
    Models ‣ 3 Literature review ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey"), multivariate forecasting in Section [3.4](#S3.SS4 "3.4
    Multivariate Forecasting ‣ 3 Literature review ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey"), physics-based model in Section [3.5](#S3.SS5
    "3.5 Physics-based Models ‣ 3 Literature review ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey"), global-local models in Section [3.6](#S3.SS6
    "3.6 Global-local ‣ 3 Literature review ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey"), models for intermittent time series in Section [3.7](#S3.SS7
    "3.7 Intermittent Time Series ‣ 3 Literature review ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey") and generative adversarial networks
    for forecasting in Section [3.8](#S3.SS8 "3.8 Generalized Adversarial Networks
    ‣ 3 Literature review ‣ Deep Learning for Time Series Forecasting: Tutorial and
    Literature Survey"). We close this section with an overview of the large number
    of available models in Section [3.9](#S3.SS9 "3.9 Summary and Practical Guidelines
    ‣ 3 Literature review ‣ Deep Learning for Time Series Forecasting: Tutorial and
    Literature Survey") where we also provide guidelines on where to start the journey
    with deep forecasting models.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的步骤如下。在第[3.1](#S3.SS1 "3.1 Probabilistic Forecast Models ‣ 3 Literature review
    ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey")节中，我们介绍了概率预测模型，包括一步预测和多步预测。同样，在第[3.2](#S3.SS2
    "3.2 Point Forecast Models ‣ 3 Literature review ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey")节中，我们总结了点预测模型。我们指出，在第[2](#S2 "2 Deep
    Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey")节之后，我们有方法可以将一步预测模型转化为多步预测模型，将点预测模型转化为概率模型。我们在第[3.3](#S3.SS3
    "3.3 Deep State Space Models ‣ 3 Literature review ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey")节中讨论了深度学习与状态空间模型的混合，在第[3.4](#S3.SS4
    "3.4 Multivariate Forecasting ‣ 3 Literature review ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey")节中讨论了多变量预测，在第[3.5](#S3.SS5 "3.5 Physics-based
    Models ‣ 3 Literature review ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey")节中讨论了基于物理的模型，在第[3.6](#S3.SS6 "3.6 Global-local ‣ 3 Literature
    review ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey")节中讨论了全球-局部模型，在第[3.7](#S3.SS7
    "3.7 Intermittent Time Series ‣ 3 Literature review ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey")节中讨论了间歇性时间序列模型，在第[3.8](#S3.SS8 "3.8
    Generalized Adversarial Networks ‣ 3 Literature review ‣ Deep Learning for Time
    Series Forecasting: Tutorial and Literature Survey")节中讨论了用于预测的生成对抗网络。我们在第[3.9](#S3.SS9
    "3.9 Summary and Practical Guidelines ‣ 3 Literature review ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey")节中总结了大量可用模型，并提供了开始使用深度预测模型的指导方针。'
- en: 3.1 Probabilistic Forecast Models
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概率预测模型
- en: 3.1.1 One-step forecast
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 一步预测
- en: 'The DeepAR model presented in Sec. [2.5.1](#S2.SS5.SSS1 "2.5.1 DeepAR ‣ 2.5
    Archetypical Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey"), is an example of one-step
    canonical forecasting model. In its base variant, DeepAR is a global univariate
    model which learns a univariate distribution; we discuss multivariate extensions
    in Sec. [3.4](#S3.SS4 "3.4 Multivariate Forecasting ‣ 3 Literature review ‣ Deep
    Learning for Time Series Forecasting: Tutorial and Literature Survey"). DeepAR
    can be equipped with outputs representing a parametrized PDF including Gaussian
    Mixture Distributions Mukherjee et al. [[136](#bib.bib136)] or quantile functions Gasthaus
    et al. [[64](#bib.bib64)].'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2.5.1](#S2.SS5.SSS1 "2.5.1 DeepAR ‣ 2.5 原型架构 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程与文献调研")节中介绍的DeepAR模型，是一种一步经典预测模型的例子。在其基本变体中，DeepAR是一个全局单变量模型，学习单变量分布；我们在第[3.4](#S3.SS4
    "3.4 多变量预测 ‣ 3 文献综述 ‣ 时间序列预测的深度学习：教程与文献调研")节讨论了多变量扩展。DeepAR可以配备表示参数化PDF的输出，包括高斯混合分布Mukherjee等人[[136](#bib.bib136)]或分位数函数Gasthaus等人[[64](#bib.bib64)]。
- en: Rasul et al. [[151](#bib.bib151)] propose TimeGrad which, like DeepAR, is an
    RNN model using LSTM or GRU cells for which samples are drawn from the data distribution
    at each time step, with the difference that in TimeGrad the RNN conditions a diffusion
    probabilistic model [[172](#bib.bib172)] which allows the model to easily scale
    to multivariate time series and accurately use the dependencies between dimensions.
    Replacing the RNN-backbone of DeepAR with dilated causal convolutions has been
    proposed as both point and probabilistic forecasting models [[20](#bib.bib20),
    [4](#bib.bib4), [183](#bib.bib183)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Rasul等人[[151](#bib.bib151)]提出了TimeGrad，它与DeepAR类似，是一种使用LSTM或GRU单元的RNN模型，其中样本在每个时间步骤从数据分布中抽取。不同之处在于，TimeGrad的RNN条件化了一个扩散概率模型[[172](#bib.bib172)]，这使得模型能够轻松扩展到多变量时间序列，并准确地利用维度之间的依赖关系。已经提出用扩张因果卷积替换DeepAR的RNN骨干，无论是点预测还是概率预测模型[[20](#bib.bib20),
    [4](#bib.bib4), [183](#bib.bib183)]。
- en: 3.1.2 Multi-step forecast
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 多步预测
- en: 'Contrary to the some of the models in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 One-step
    forecast ‣ 3.1 Probabilistic Forecast Models ‣ 3 Literature review ‣ Deep Learning
    for Time Series Forecasting: Tutorial and Literature Survey") which produce one-step
    ahead forecasts, multi-step forecasts can be obtained directly with a seq2seq
    architecture. In Section [2.5.2](#S2.SS5.SSS2 "2.5.2 MQRNN/MQCNN ‣ 2.5 Archetypical
    Architectures ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey"), we reviewed the MQRNN/MQCNN architecture [[190](#bib.bib190)]
    as a seq2seq architecture for probabilistic forecasting. The main advantage of
    seq2seq over one-step ahead forecast models is that the decoder architecture can
    be chosen to output all future target values at once. This removes the need to
    unroll over the forecast horizon which can lead to *error accumulation* since
    early forecast errors propagate through the forecast horizon. Thus, the decoder
    of seq2seq forecasting models is typically an MLP while other architectures are
    also used for the encoder [[190](#bib.bib190), [138](#bib.bib138)].'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与第[3.1.1](#S3.SS1.SSS1 "3.1.1 一步预测 ‣ 3.1 概率预测模型 ‣ 3 文献综述 ‣ 时间序列预测的深度学习：教程与文献调研")节中的一些模型不同，这些模型只能产生一步预测，多步预测可以通过seq2seq架构直接获得。在第[2.5.2](#S2.SS5.SSS2
    "2.5.2 MQRNN/MQCNN ‣ 2.5 原型架构 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程与文献调研")节中，我们回顾了MQRNN/MQCNN架构[[190](#bib.bib190)]作为一种用于概率预测的seq2seq架构。seq2seq相对于一步预测模型的主要优点是解码器架构可以一次性输出所有未来的目标值。这避免了在预测视野上展开的需求，因为早期的预测误差会在预测视野中传播，导致*误差累积*。因此，seq2seq预测模型的解码器通常是MLP，而其他架构也用于编码器[[190](#bib.bib190),
    [138](#bib.bib138)]。
- en: Wen and Torkkola [[189](#bib.bib189)] extended the MQCNN model with a generative
    quantile copula. This model learns the conditional quantile function that maps
    the quantile index, which is a uniform random variable conditioned on the covariates,
    to the target. During training, the model draws the quantile index from a uniform
    distribution. This turns MQCNN into a generative, marginal quantile model. The
    authors combine this approach with a Gaussian copula to draw correlated marginal
    quantile index random values. They show that the Gaussian copula component improves
    the forecast at the distribution tails. Chen et al. [[35](#bib.bib35)] proposed
    DeepTCN, another seq2seq model where the encoder is the dilated causal convolution
    with residual blocks, and the decoder is simply an MLP with residual connections.
    Structure-wise, DeepTCN is almost the same as the basic structure of MQCNN [[190](#bib.bib190)],
    i.e., without the local MLP component that aims to model spikes and events.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Wen 和 Torkkola [[189](#bib.bib189)] 使用生成分位数 copula 扩展了 MQCNN 模型。该模型学习将分位数索引（一个基于协变量的均匀随机变量）映射到目标的条件分位数函数。在训练过程中，模型从均匀分布中抽取分位数索引。这将
    MQCNN 转变为生成的边际分位数模型。作者将这种方法与高斯 copula 结合，绘制相关的边际分位数索引随机值。他们展示了高斯 copula 组件在分布尾部改善预测的效果。Chen
    等人 [[35](#bib.bib35)] 提出了 DeepTCN，这是另一个 seq2seq 模型，其中编码器是具有残差块的膨胀因果卷积，解码器则只是一个带有残差连接的
    MLP。从结构上看，DeepTCN 与 MQCNN 的基本结构几乎相同 [[190](#bib.bib190)]，即没有旨在建模尖峰和事件的本地 MLP 组件。
- en: 'Park et al. [[141](#bib.bib141)] propose the incremental quantile functions
    (IQF), a flexible and efficient distribution-free quantile estimation framework
    that resolves quantile crossing with a simple NN layer. A seq2seq encoder-decoder
    structure is used although the method can be readily applied to recurrent models
    with one-step ahead forecasts [[156](#bib.bib156)]. IQF is trained using the CRPS
    loss (Eq. ([7](#S2.E7 "In 2.4.2 Quantile function ‣ 2.4 Output Models and Loss
    Functions ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey"))) similar to [[64](#bib.bib64)].'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Park 等人 [[141](#bib.bib141)] 提出了增量分位数函数（IQF），这是一种灵活且高效的无分布分位数估计框架，通过简单的 NN 层解决了分位数交叉问题。尽管该方法可以直接应用于具有一步预测的递归模型，但使用了
    seq2seq 编码器-解码器结构 [[156](#bib.bib156)]。IQF 使用 CRPS 损失（Eq. ([7](#S2.E7 "在 2.4.2
    分位数函数 ‣ 2.4 输出模型和损失函数 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述"))) 进行训练，类似于 [[64](#bib.bib64)]。
- en: A combination of recurrent and encoder-decoder structures has also been explored.
    In [[205](#bib.bib205)], the authors use an LSTM with Monte Carlo dropout as both
    the encoder and decoder. However, unlike other models that directly use RNNs to
    generate forecasts, the learned embedding at the end of the decoding step is fed
    into an MLP prediction network and is combined with other external features to
    generate the forecast. Along a similar line, Laptev et al. [[111](#bib.bib111)]
    employ an LSTM as a feature extractor (LSTM autoencoder), and use the extracted
    features, combined with external inputs to generate the forecasts with another
    LSTM.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结合递归和编码器-解码器结构也被探索过。在 [[205](#bib.bib205)] 中，作者使用 LSTM 和 Monte Carlo dropout
    作为编码器和解码器。然而，与直接使用 RNN 生成预测的其他模型不同，解码步骤结束时学习到的嵌入被输入到 MLP 预测网络中，并与其他外部特征结合生成预测。类似地，Laptev
    等人 [[111](#bib.bib111)] 使用 LSTM 作为特征提取器（LSTM 自编码器），并将提取的特征与外部输入结合，以另一种 LSTM 生成预测。
- en: Van Den Oord et al. [[183](#bib.bib183)] introduced the WaveNet architecture,
    a generative model for speech synthesis, which uses dilated causal convolutions
    to learn the long range dependencies important for audio signals. Since this architecture
    is based on convolutions, training is very efficient on GPUs – prediction is still
    sequential and further changes are necessary for fast inference. Adaptations of
    WaveNet for forecasting are available [[4](#bib.bib4)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Van Den Oord 等人 [[183](#bib.bib183)] 引入了 WaveNet 架构，这是一种用于语音合成的生成模型，使用膨胀因果卷积来学习对音频信号重要的长期依赖关系。由于该架构基于卷积，因此在
    GPU 上训练非常高效——预测仍然是顺序的，并且需要进一步的更改以实现快速推断。WaveNet 的预测适配版可用 [[4](#bib.bib4)]。
- en: 3.2 Point Forecast Models
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 点预测模型
- en: 'Point forecast models do not model the probability distribution of the future
    values of a time series but rather output directly a point forecast that typically
    corresponds to a summary statistic of the predictive distribution. We have discussed
    generic recipes on how to turn a point forecasting model into a probabilistic
    forecasting model in Section [2.4](#S2.SS4 "2.4 Output Models and Loss Functions
    ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey") and the literature contains further examples
    (see e.g., [[74](#bib.bib74), [175](#bib.bib175)] for recent complementary approaches).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '点预测模型不建模时间序列未来值的概率分布，而是直接输出一个点预测，该预测通常对应于预测分布的总结统计量。我们在第 [2.4](#S2.SS4 "2.4
    Output Models and Loss Functions ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning
    for Time Series Forecasting: Tutorial and Literature Survey") 节讨论了如何将点预测模型转变为概率预测模型的通用方法，文献中还有更多的例子（例如，[[74](#bib.bib74),
    [175](#bib.bib175)] 提供了近期的补充方法）。'
- en: 3.2.1 One-step forecast
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 一步预测
- en: 'A considerable amount of attention of the community is dedicated to one-step
    forecasting. LSTNet [[107](#bib.bib107)] is a model using a combination of a CNN
    and an RNN. Targeting multivariate time series, LSTNet uses a convolution network
    (without pooling) to extract short-term temporal patterns as well as correlations
    among variables. The output of the convolution network is fed into a recurrent
    layer and a temporal attention layer which, combined with the autoregressive component,
    generates the final point forecast. While LSTNet uses a standard point forecast
    loss function, it can readily be turned into a probabilistic forecast model using
    the components described in Sec. [2](#S2 "2 Deep Forecasting: A Tutorial ‣ Deep
    Learning for Time Series Forecasting: Tutorial and Literature Survey"), e.g.,
    by modifying LSTNet to output the parameters of a probability distribution and
    using NLL as a loss function. Qiu et al. [[144](#bib.bib144)] proposed an ensemble
    of deep belief networks for forecasting. The outputs of all the networks is concatenated
    and fed into a support vector regression model (SVR) that gives the final prediction.
    The NNs and the SVR are not trained jointly though. Hsu [[82](#bib.bib82)] proposed
    an augmented LSTM model which combines autoencoders with LSTM cells. The input
    observations are first encoded to latent variables, which is equivalent to feature
    extraction, and are fed into the LSTM cells. The decoder is an MLP which maps
    the LSTM output into the predicted values. For point forecast multivariate forecasting,
    Yoo and Kang [[198](#bib.bib198)] proposed time-invariant attention to learn the
    dependencies between the dimensions of the time series and use them with a convolution
    architecture to model the time series.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '社区中有相当多的关注集中在一步预测上。LSTNet [[107](#bib.bib107)] 是一种结合了 CNN 和 RNN 的模型。针对多变量时间序列，LSTNet
    使用卷积网络（无池化）来提取短期时间模式以及变量间的关联。卷积网络的输出被送入递归层和时间注意力层，这些与自回归组件结合，生成最终的点预测。虽然 LSTNet
    使用标准的点预测损失函数，但可以通过第 [2](#S2 "2 Deep Forecasting: A Tutorial ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey") 节中描述的组件，轻松转变为概率预测模型，例如，通过修改
    LSTNet 输出概率分布的参数，并使用 NLL 作为损失函数。Qiu 等人 [[144](#bib.bib144)] 提出了一个深度信念网络的集成用于预测。所有网络的输出被连接在一起，并送入支持向量回归模型（SVR）以给出最终预测。然而，NNs
    和 SVR 并未联合训练。Hsu [[82](#bib.bib82)] 提出了一个扩展的 LSTM 模型，它将自编码器与 LSTM 单元结合。输入观测首先被编码为潜在变量，相当于特征提取，然后送入
    LSTM 单元。解码器是一个 MLP，它将 LSTM 输出映射到预测值。对于点预测的多变量预测，Yoo 和 Kang [[198](#bib.bib198)]
    提出了时间不变注意力，以学习时间序列维度之间的依赖关系，并将其与卷积架构结合以建模时间序列。'
- en: Building upon the success of CNNs in other application domains, Borovykh et al.
    [[24](#bib.bib24)] proposed an adjustment to WaveNet [[183](#bib.bib183)] that
    makes it applicable to conditional forecasting. They evaluated their model on
    various datasets with mixed results, concluding that it can serve as a strong
    baseline and that various improvements could be made. In a similar vein, inspired
    by the Transformer architecture [[184](#bib.bib184)] Song et al. [[173](#bib.bib173)]
    proposed an adjustment that makes the architecture applicable to time series.
    Their method is applied to both regression and classification tasks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN在其他应用领域的成功，Borovykh等人[[24](#bib.bib24)]提出了对WaveNet的调整[[183](#bib.bib183)]，使其适用于条件预测。他们在各种数据集上评估了他们的模型，结果不一，最终认为该模型可以作为一个强大的基准，并且可以进行各种改进。类似地，受到Transformer架构[[184](#bib.bib184)]的启发，Song等人[[173](#bib.bib173)]提出了一种调整，使得该架构适用于时间序列。他们的方法被应用于回归和分类任务。
- en: 3.2.2 Multi-step forecast
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 多步预测
- en: N-BEATS [[138](#bib.bib138)] is an NN architecture purpose-built for the forecasting
    task that relies on a deep, residual stack of MLP layers to obtain point forecasts.
    The basic building block in this architecture is a forked MLP stack that takes
    the block input and feeds the intermediate representation into separate MLPs to
    learn the parameters of the context (the authors call it backcast) and forecast
    time series models. The residual architecture removes the part of the context
    signal it can explain well before passing to the next block and adds up the forecasts.
    The learned time series model can have free parameters or be constrained to follow
    a particular, functional form. Constraining the model to trend and seasonality
    functional forms does not have a big impact on the error and generates models
    whose stacks are interpretable since the trend and seasonality components of the
    model can be separated and analyzed. N-BEATS has also been interpreted as a meta-learning
    model [[139](#bib.bib139)], where the repeated application of residual blocks
    can be seen as an inner optimization loop. N-BEATS generalizes better than other
    architectures when trained on a source dataset (e.g., M4-monthly) and applied
    to a different target datasets (e.g., M3-monthly).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS[[138](#bib.bib138)]是一种专为预测任务设计的神经网络架构，依赖于深层的、残差堆叠的MLP层来获得点预测。这一架构的基本构建块是一个分叉的MLP堆叠，它接收块输入并将中间表示传递给不同的MLP，以学习上下文的参数（作者称之为回溯）和预测时间序列模型。残差架构在传递给下一个块之前移除其可以很好解释的上下文信号部分，并将预测结果加总。学习到的时间序列模型可以有自由参数，也可以被约束为遵循特定的功能形式。将模型约束为趋势和季节性功能形式不会对误差产生很大影响，并生成可解释的模型堆叠，因为模型的趋势和季节性成分可以被分离和分析。N-BEATS也被解释为一种元学习模型[[139](#bib.bib139)]，其中残差块的重复应用可以被视为一个内部优化循环。N-BEATS在以源数据集（例如，M4-monthly）进行训练并应用于不同的目标数据集（例如，M3-monthly）时，比其他架构具有更好的泛化能力。
- en: Lv et al. [[126](#bib.bib126)] propose a *stacked autoencoder* (SAE) architecture
    to learn features from spatio-temporal traffic flow data. On top of the autoencoder,
    a logistic regression layer is used to output predictions of the traffic flow
    at all locations in a future time window. The resulting architecture is trained
    layer-wise in a greedy manner. The experimental results show that the method significantly
    improves over other shallow architectures, suggesting that the SAE is capable
    of extracting latent features regarding the spatio-temporal correlations of the
    data. In the same context of spatio-temporal forecasting and under the seq2seq
    framework, Li et al. [[118](#bib.bib118)] proposed the Diffusion Convolutional
    Recurrent NN (DCRNN). Diffusion convolution is employed to capture the dependencies
    on the spatial domain, while an RNN is utilized to model the temporal dependencies.
    Finally, Asadi and Regan [[6](#bib.bib6)] proposed a framework where the time
    series are decomposed in an initial preprocessing step to separately feed short-term,
    long-term, and spatial patterns into different components of a NN. Neighbouring
    time series are clustered based on their similarity of the residuals as there
    can be meaningful short-term patterns for spatial time series. Then, in a CNN
    based architecture, each kernel of a multi-kernel convolution layer is applied
    to a cluster of time series to extract short-term features in neighbouring areas.
    The output of the convolution layer is concatenated by trends and is followed
    by a convolution-LSTM layer to capture long-term patterns in larger regional areas.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Lv 等人 [[126](#bib.bib126)] 提出了一个 *堆叠自编码器*（SAE）架构，用于从时空交通流数据中学习特征。在自编码器的基础上，使用逻辑回归层来输出未来时间窗口内所有位置的交通流预测。最终的架构是以贪婪的方式逐层训练的。实验结果表明，该方法相比其他浅层架构有显著的改进，表明
    SAE 能够提取关于数据时空相关性的潜在特征。在时空预测的相同背景下，Li 等人 [[118](#bib.bib118)] 提出了扩散卷积递归神经网络（DCRNN）。扩散卷积用于捕捉空间域的依赖关系，而
    RNN 用于建模时间依赖性。最后，Asadi 和 Regan [[6](#bib.bib6)] 提出了一个框架，其中时间序列在初步预处理步骤中被分解，以将短期、长期和空间模式分别输入到神经网络的不同组件中。相邻时间序列根据残差的相似性进行聚类，因为空间时间序列可能具有有意义的短期模式。然后，在基于
    CNN 的架构中，多核卷积层的每个卷积核应用于一个时间序列集群，以提取邻近区域的短期特征。卷积层的输出通过趋势进行拼接，之后跟随卷积-LSTM 层，以捕捉更大区域内的长期模式。
- en: Bandara et al. [[13](#bib.bib13)] addressed the problem of predicting a set
    of disparate time series, which may not be well captured by a single global model.
    For this reason, the authors propose to cluster the time series according to a
    vector of features extracted using the technique from [[87](#bib.bib87)] and the
    Snob clustering algorithm [[186](#bib.bib186)]. Only then, an RNN is trained per
    cluster, after having decomposed the series into trend, seasonality and residual
    components. The RNN is followed by an affine neural layer to project the cell
    outputs to the dimension of the intended forecast horizon. This approach is applied
    to publicly available datasets from time series competitions, and appears to consistently
    improve against learning a single global model. In subsequent work, Bandara et al.
    [[15](#bib.bib15)] continued to mix heuristics, in this instance seasonality decomposition
    techniques, known from classical forecasting methods with standard NN techniques.
    Their aim is to improve on scenarios with multiple seasonalities such as inter
    and intra daily. The findings are that for panels of somewhat unrelated time series,
    such decomposition techniques help global models whereas for panels of related
    or homogeneous time series this may be harmful. The authors do not attempt to
    integrate these steps into the NN architecture itself, which would allow for end-to-end
    learning.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Bandara 等人 [[13](#bib.bib13)] 解决了预测一组不同时间序列的问题，这些序列可能无法通过单一的全局模型很好地捕捉。因此，作者提出根据使用[[87](#bib.bib87)]中的技术和
    Snob 聚类算法 [[186](#bib.bib186)] 提取的特征向量对时间序列进行聚类。只有在将序列分解为趋势、季节性和残差分量后，才会对每个聚类训练一个
    RNN。RNN 后面跟着一个仿射神经层，以将单元输出映射到目标预测范围的维度。该方法应用于公开可用的时间序列竞赛数据集，似乎始终优于学习单一全局模型。在后续工作中，Bandara
    等人 [[15](#bib.bib15)] 继续将启发式方法（此处为来自经典预测方法的季节性分解技术）与标准 NN 技术结合。他们的目标是在存在多种季节性的场景中改进，如日内和每日内的季节性。研究发现，对于某种程度上不相关的时间序列面板，这些分解技术有助于全局模型，而对于相关或同质的时间序列面板，这可能是有害的。作者没有尝试将这些步骤整合到
    NN 架构中，这将允许端到端学习。
- en: Cinar et al. [[39](#bib.bib39)] proposed a content attention mechanism that
    seats on top of any seq2seq RNN. The idea is to select a combination of the hidden
    states from the history and combine them using a pseudo-period vector of weights
    to the predicted output step.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Cinar 等人 [[39](#bib.bib39)] 提出了一个内容注意机制，置于任何 seq2seq RNN 的顶部。其思想是选择历史中的隐藏状态的组合，并使用伪周期权重向量将它们结合到预测输出步骤。
- en: Li et al. [[116](#bib.bib116)] introduce two modifications to the Transformer
    architecture to improve its performance for forecasting. First, they include causal
    convolutions in the attention to make the key and query context dependent, which
    makes the model more sensitive to local contexts. Second, they introduce a sparse
    attention, meaning the model cannot attend to all points in the history, but only
    to selected points. Through exponentially increasing distances between these points,
    the memory complexity can be reduced from quadratic to $O(T(\log T)^{2})$, where
    $T$ is the sequence length, which is important for long sequences that occur frequently
    in forecasting. Other architectural improvements to the Transformer model have
    also been used more recently to improve accuracy and computational complexity
    in forecasting applications. For example, Lim et al. [[120](#bib.bib120)] introduce
    the Temporal Fusion Transformer (TFT), which incorporates novel model components
    for embedding static covariates, performing “variable selection”, and gating components
    that skip over irrelevant parts of the context. The TFT is trained to predict
    forecast quantiles, and promotes forecast interpretability by modifying self-attention
    and learning input variable importance. Eisenach et al. [[51](#bib.bib51)] propose
    MQ-Transformer, a Transformer architecture that employs novel attention mechanisms
    in the encoder and decoder separately, and consider learning positional embeddings
    from event indicators. The authors discuss the improvements not only on forecast
    accuracy, but also on excess forecast volatility where their model improves over
    the state of the art. Finally, Zhou et al. [[204](#bib.bib204)] recently proposed
    the Informer, a computationally efficient Transformer architecture, that specifically
    targets applications with long forecast horizons. The Informer introduces a ProbSparse
    attention layer and a distilling mechanism to reduce both the time complexity
    and memory usage of learning to $O(T\log T)$, while improving forecast performance
    over deep forecasting benchmark.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人[[116](#bib.bib116)]对Transformer架构进行了两个改进，以提高其预测性能。首先，他们在注意力机制中加入了因果卷积，使得键和值的上下文依赖性更强，从而使模型对局部上下文更加敏感。其次，他们引入了稀疏注意力机制，即模型不能关注历史中的所有点，而只能关注选定的点。通过在这些点之间指数级增加距离，内存复杂度可以从二次方降低到$O(T(\log
    T)^{2})$，其中$T$是序列长度，这对在预测中频繁出现的长序列非常重要。最近，Transformer模型的其他结构改进也被用来提高预测应用中的准确性和计算复杂性。例如，Lim等人[[120](#bib.bib120)]引入了Temporal
    Fusion Transformer (TFT)，它包含用于嵌入静态协变量、执行“变量选择”和跳过无关上下文部分的门控组件的新模型组件。TFT经过训练以预测预测分位数，并通过修改自注意力和学习输入变量重要性来促进预测可解释性。Eisenach等人[[51](#bib.bib51)]提出了MQ-Transformer，这是一种在编码器和解码器中分别采用新型注意力机制的Transformer架构，并考虑从事件指示符中学习位置嵌入。作者讨论了改进不仅在预测准确性上的表现，还有在超额预测波动方面的提升，模型优于现有技术。最后，Zhou等人[[204](#bib.bib204)]最近提出了Informer，这是一种计算高效的Transformer架构，特别针对具有长预测视野的应用。Informer引入了ProbSparse注意力层和蒸馏机制，以将学习的时间复杂度和内存使用减少到$O(T\log
    T)$，同时在深度预测基准上提高了预测性能。
- en: 3.3 Deep State Space Models
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度状态空间模型
- en: 'In contrast to pure deep learning methods for time series forecasting introduced
    in Section [2](#S2 "2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey"), Rangapuram et al. [[146](#bib.bib146)]
    propose to combine classical state space models (SSM) [[49](#bib.bib49), [86](#bib.bib86)]
    with deep learning. The main motivation is to bridge the gap between SSMs that
    provide a principled framework for incorporating structural assumptions but fail
    to learn patterns across a collection of time series, and NNs that are capable
    of extracting higher order features but results in models that are hard to interpret.
    Their method parametrizes a linear Gaussian SSM using an RNN. The parameters of
    the RNN are learned jointly from a dataset of raw time series and associated covariates.
    Instead of learning the SSM parameters $\theta_{i,1:T_{i}}$ for the $i$-th time
    series individually or locally in the terminology of Section [2.1](#S2.SS1 "2.1
    Notation and Formalization of the Forecasting Problem ‣ 2 Deep Forecasting: A
    Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey")), the model is global and learns a shared mapping from the covariates
    associated with each target time series to the parameters of a linear SSM. This
    mapping $\theta_{i,t}=f(\mathbf{x}_{i,1:t};\Phi)$, for $i=1,\ldots,N$ and $t=1,\ldots,T_{i}+h$,
    is implemented by an RNN with weights $\Phi$ which are shared across different
    time series as well as different time steps. Note that $f$ depends on the entire
    covariate time series up to time $t$ as well as the set of shared parameters $\Phi$.
    Since each individual time series $i$ is modelled using an SSM with parameters
    $\Theta_{i}$, assumptions such as temporal smoothness in the forecasts are easily
    enforced. The shared model parameters $\Phi$ are learned by maximizing the likelihood
    given the observations $\mathcal{Z}=\{\mathbf{z}_{i,1:T_{i}}\}_{i=1}^{N}$. The
    likelihood terms for each time series reduce to the standard likelihood computation
    under the linear-Gaussian SSM, which can be carried out efficiently via Kalman
    filtering [[16](#bib.bib16)]. Once the parameters $\Phi$ are learned, it is straightforward
    to obtain the forecast distribution via the SSM parameters $\theta_{i,T_{i}+1:T_{i}+h}$.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '与第[2](#S2 "2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey")节中介绍的纯深度学习时间序列预测方法相比，Rangapuram等人[[146](#bib.bib146)]
    提出了将经典状态空间模型（SSM）[[49](#bib.bib49), [86](#bib.bib86)]与深度学习结合的方案。主要动机是填补SSM的空白，SSM提供了一个有原则的框架来纳入结构性假设，但无法学习跨时间序列的模式，而神经网络（NN）则能提取更高阶的特征，但结果是模型难以解释。他们的方法使用RNN对线性高斯SSM进行参数化。RNN的参数从原始时间序列数据集和相关协变量中共同学习。与第[2.1](#S2.SS1
    "2.1 Notation and Formalization of the Forecasting Problem ‣ 2 Deep Forecasting:
    A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey")节中术语所述的单独或局部学习第$i$个时间序列的SSM参数$\theta_{i,1:T_{i}}$不同，该模型是全局的，并且从与每个目标时间序列相关的协变量中学习到一个线性SSM的共享映射。这个映射$\theta_{i,t}=f(\mathbf{x}_{i,1:t};\Phi)$，对于$i=1,\ldots,N$和$t=1,\ldots,T_{i}+h$，由一个具有共享权重$\Phi$的RNN实现，这些权重在不同时间序列和不同时间步长之间共享。注意，$f$依赖于直到时间$t$的整个协变量时间序列以及共享参数集$\Phi$。由于每个单独的时间序列$i$使用参数$\Theta_{i}$的SSM进行建模，因此可以轻松地强加预测中的时间平滑性等假设。通过最大化给定观察值$\mathcal{Z}=\{\mathbf{z}_{i,1:T_{i}}\}_{i=1}^{N}$的似然来学习共享模型参数$\Phi$。每个时间序列的似然项简化为线性高斯SSM下的标准似然计算，这可以通过卡尔曼滤波[[16](#bib.bib16)]高效地进行。一旦学习到参数$\Phi$，可以通过SSM参数$\theta_{i,T_{i}+1:T_{i}+h}$直接获得预测分布。'
- en: 'There are two major limitations of the method proposed in Rangapuram et al.
    [[146](#bib.bib146)]: first, the observations are assumed to follow a Gaussian
    distribution and second, the underlying latent process that generates observations
    is assumed to evolve linearly. de Bézenac et al. [[42](#bib.bib42)] address the
    first limitation via Normalizing Kalman Filters (NKF) by augmenting SSMs with
    normalizing flows [[46](#bib.bib46), [101](#bib.bib101), [137](#bib.bib137)] thereby
    giving them the flexibility to model non-Gaussian, multimodal data. Their main
    idea is to map the non-Gaussian observations $\{\mathbf{z}_{i,1:T_{i}}\}$ to more
    Gaussian-like observations via a sequence of learnable, nonlinear transformations
    (e.g., a normalizing flow) so that the method in [[146](#bib.bib146)] can then
    be applied on the transformed observations. While being more flexible, their method
    still retains attractive properties of linear Gaussian SSMs, namely, tractability
    of exact inference and likelihood computation, efficient sampling, and robustness
    to noise.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Rangapuram等人[[146](#bib.bib146)]提出的方法有两个主要的局限性：首先，观测值被假设为遵循高斯分布，其次，生成观测值的潜在过程被假设为线性演变。de
    Bézenac等人[[42](#bib.bib42)]通过使用归一化卡尔曼滤波器（NKF）来解决第一个局限性，通过将SSMs与归一化流[[46](#bib.bib46),
    [101](#bib.bib101), [137](#bib.bib137)]结合，赋予它们建模非高斯、多模态数据的灵活性。他们的主要思路是通过一系列可学习的非线性变换（例如，归一化流）将非高斯观测值$\{\mathbf{z}_{i,1:T_{i}}\}$映射到更类似高斯的观测值，以便可以在变换后的观测值上应用[[146](#bib.bib146)]中的方法。尽管更具灵活性，他们的方法仍保留了线性高斯SSMs的吸引性特性，即精确推断和似然计算的可处理性、高效的采样以及对噪声的鲁棒性。
- en: In a concurrent work to [[42](#bib.bib42)],  Kurle et al. [[106](#bib.bib106)]
    improve the method in [[146](#bib.bib146)] by addressing both limitations. In
    particular, to model nonlinear latent dynamics, they propose a recurrent switching
    Gaussian SSM, which uses additional latent variables to switch between different
    linear dynamics. Moreover, to handle non-Gaussian observations, they propose a
    nonlinear emission model via a decoder-type NN [[61](#bib.bib61)]. Although the
    exact inference is no longer tractable with these improvements, they show that
    the approximate inference and likelihood estimation can be Rao-Blackwellised;
    i.e., the inference for the Gaussian latent states can be done exactly while the
    inference for the switch variables needs to be approximated.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[42](#bib.bib42)]的并行工作中，Kurle等人[[106](#bib.bib106)]通过解决两者的局限性改进了[[146](#bib.bib146)]中的方法。特别是，为了建模非线性潜在动态，他们提出了一种递归切换高斯状态空间模型（SSM），该模型使用额外的潜在变量在不同的线性动态之间切换。此外，为了处理非高斯观测，他们通过解码器类型的神经网络（NN）提出了一种非线性发射模型[[61](#bib.bib61)]。尽管这些改进使得精确推断变得不可行，但他们展示了近似推断和似然估计可以进行Rao-Blackwell化；即，高斯潜在状态的推断可以精确完成，而切换变量的推断则需要近似处理。
- en: Finally, Ansari et al. [[5](#bib.bib5)] propose to extend [[146](#bib.bib146)]
    via incorporating switching dynamics. The recurrent explicit duration switching
    dynamical system (RED-SDS) is a flexible model that is capable of identifying
    both state- and time-dependent switching dynamics of a time series. State-dependent
    switching is enabled by a recurrent state-to-switch connection and an explicit
    duration count variable is used to improve the time-dependent switching behavior.
    A hybrid algorithm that approximates the posterior of the continuous states via
    an inference network and performs exact inference for the discrete switches and
    counts provides efficient inference. The method is able to infer meaningful switching
    patterns from the data and extrapolate the learned patterns into the forecast
    horizon.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Ansari等人[[5](#bib.bib5)]提出通过引入切换动态来扩展[[146](#bib.bib146)]。递归显式时长切换动态系统（RED-SDS）是一个灵活的模型，能够识别时间序列中的状态依赖和时间依赖切换动态。状态依赖切换由递归的状态到切换连接实现，显式的时长计数变量用于改善时间依赖切换行为。通过推断网络近似连续状态的后验，并对离散切换和计数进行精确推断的混合算法提供了高效的推断。该方法能够从数据中推断出有意义的切换模式，并将学习到的模式外推到预测范围内。
- en: 3.4 Multivariate Forecasting
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 多变量预测
- en: 'The models presented up to this point are mainly global univariate models,
    i.e., they are trained on all time series but they are still used to predict a
    univariate target. When dealing with multivariate time series, one should be able
    to exploit the dependency structure between the different time series in the panel
    in a generalization of Eq. ([3](#S2.E3 "In 2.1 Notation and Formalization of the
    Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time
    Series Forecasting: Tutorial and Literature Survey")) to Eq. ([4](#S2.E4 "In 2.1
    Notation and Formalization of the Forecasting Problem ‣ 2 Deep Forecasting: A
    Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey")).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止提出的模型主要是全球单变量模型，即它们是在所有时间序列上进行训练，但仍然用于预测单变量目标。在处理多变量时间序列时，应能够利用面板中不同时间序列之间的依赖结构，扩展到方程([3](#S2.E3
    "在2.1符号和预测问题的形式化 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述"))到方程([4](#S2.E4 "在2.1符号和预测问题的形式化
    ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述"))。
- en: Toubeau et al. [[179](#bib.bib179)] and Salinas et al. [[155](#bib.bib155)]
    combined RNN-based models with copulas to model multivariate distributions. The
    model in [[179](#bib.bib179)] uses a nonparametric copula to capture the multivariate
    dependence structure. In contrast, the work in [[155](#bib.bib155)] uses a Gaussian
    copula process approach. Salinas et al. [[155](#bib.bib155)] use a low-rank covariance
    matrix approximation to scale to thousands of dimensions. Additionally, the model
    implements a non-parametric transformation of the marginals to deal with varying
    scales in the dimensions and non-Gaussian data. More recently, Rasul et al. [[150](#bib.bib150)]
    proposed to represent the data distribution with a type of normalizing flows called
    Masked Autoregressive Flows [[140](#bib.bib140)] while using either an RNN or
    a Transformer [[184](#bib.bib184)] to model the multivariate temporal dynamics
    of time series. Normalizing flows were also used to bring deep SSMs [[146](#bib.bib146)]
    to a flexible, multivariate scenario [[42](#bib.bib42)]. Rasul et al. [[151](#bib.bib151)]
    propose TimeGrad which, like DeepAR, is an RNN model for which samples are drawn
    from the data distribution at each time step, with the difference that in TimeGrad
    the RNN conditions a diffusion probabilistic model [[172](#bib.bib172)] which
    allows the model to easily scale to multivariate time series and accurately use
    the dependencies between dimensions.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Toubeau等人[[179](#bib.bib179)]和Salinas等人[[155](#bib.bib155)]将基于RNN的模型与copulas结合，用于建模多变量分布。[[179](#bib.bib179)]中的模型使用非参数copula来捕捉多变量依赖结构。相比之下，[[155](#bib.bib155)]中的研究采用了高斯copula过程方法。Salinas等人[[155](#bib.bib155)]使用低秩协方差矩阵近似来扩展到数千维。此外，该模型实现了边际的非参数变换，以处理维度上的变化尺度和非高斯数据。最近，Rasul等人[[150](#bib.bib150)]提出了一种通过称为Masked
    Autoregressive Flows[[140](#bib.bib140)]的正则化流来表示数据分布的方法，同时使用RNN或Transformer[[184](#bib.bib184)]来建模时间序列的多变量时间动态。正则化流也被用来将深度SSMs[[146](#bib.bib146)]扩展到灵活的多变量场景[[42](#bib.bib42)]。Rasul等人[[151](#bib.bib151)]提出了TimeGrad，这类似于DeepAR，是一种RNN模型，样本在每个时间步从数据分布中提取，不同之处在于TimeGrad中的RNN对一个扩散概率模型[[172](#bib.bib172)]进行条件化，使模型能够轻松扩展到多变量时间序列并准确利用维度之间的依赖关系。
- en: A recent application of global multivariate models is for hierarchical forecasting
    problems [[17](#bib.bib17), [191](#bib.bib191), [176](#bib.bib176), [7](#bib.bib7)].
    Typically, in such problems an aggregation structure is defined (e.g., via a product
    hierarchy) and a trade-off between forecast accuracy and forecast coherency with
    respect to the aggregation structure must be managed. Here, forecast coherency
    or consistency means that the forecasts conforms to the aggregation structure,
    so that aggregated forecasts are the same as forecasts of aggregated time series.
    This aggregation structure is typically encoded via linear constraints where the
    aggregation structure is captured in a matrix $S$. Rangapuram et al. [[147](#bib.bib147)]
    propose to use a multivariate model such as [[155](#bib.bib155)] and enforce consistency
    of forecast samples via incorporation of a projection of the samples with $S$
    into the learning problem. Dedicated work exists for aggregation along the time
    dimension [[178](#bib.bib178), [8](#bib.bib8), [148](#bib.bib148)].
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 全球多变量模型的一个近期应用是用于层次预测问题 [[17](#bib.bib17), [191](#bib.bib191), [176](#bib.bib176),
    [7](#bib.bib7)]。通常，在这类问题中定义了一个聚合结构（例如，通过产品层次结构），并且必须在预测准确性与相对于聚合结构的预测一致性之间进行权衡。在这里，预测一致性或一致性意味着预测符合聚合结构，使得聚合预测与聚合时间序列的预测相同。这种聚合结构通常通过线性约束进行编码，其中聚合结构在矩阵$S$中被捕获。Rangapuram等人
    [[147](#bib.bib147)] 提出使用多变量模型，如 [[155](#bib.bib155)]，并通过将样本的投影与$S$结合到学习问题中来强制预测样本的一致性。专门的工作也存在于时间维度上的聚合
    [[178](#bib.bib178), [8](#bib.bib8), [148](#bib.bib148)]。
- en: 'In some multivariate forecasting settings the different dimensions are tied
    together by some interpretable connections other than a hierarchy and this can
    be modelled as part of the input layer rather than the output as discussed so
    far. One can for example think of forecasting the traffic network of a city where
    the traffic at each of the location in the city is mostly influenced by the traffic
    at the neighboring locations, like in PEMS-BAY and METR-LA [[118](#bib.bib118)].
    Graph Neural Networks (GNN) have been used in this forecasting setting [[163](#bib.bib163),
    [43](#bib.bib43), [193](#bib.bib193), [102](#bib.bib102), [63](#bib.bib63), [206](#bib.bib206)]
    where, in addition to the forecasting task, the challenge is to best use the graph
    information that is provided or even learn the graph if none is available. The
    methods that propose to learn the graph do so by looking for the graph that allows
    to produce the most accurate forecasts. An embedding is learned for each dimension,
    and similarity scores are computed between every two dimension using these embeddings
    from which the adjacency matrix is obtained, either by taking the K-top edges
    [[43](#bib.bib43), [193](#bib.bib193)] or sampling from them [[163](#bib.bib163),
    [102](#bib.bib102)]. As of now, two main strategies have been proposed to learn
    the node embeddings, either simply by gradient descent [[43](#bib.bib43), [193](#bib.bib193)]
    or by taking representation from the time series [[163](#bib.bib163), [102](#bib.bib102)],
    with the latter approach to seemingly yielding better results. While these methods
    were all presented as point forecasting method, one could obtain probabilistic
    forecasts by training these models to parametrize a predictive distribution as
    explained in Section [2.4](#S2.SS4 "2.4 Output Models and Loss Functions ‣ 2 Deep
    Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial
    and Literature Survey").'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些多变量预测设置中，不同的维度通过某些可解释的连接（除了层级结构）联系在一起，这可以被建模为输入层的一部分，而不是像之前讨论的那样作为输出层的一部分。例如，可以考虑预测一个城市的交通网络，其中城市中每个地点的交通状况主要受邻近地点交通的影响，如PEMS-BAY和METR-LA
    [[118](#bib.bib118)]。图神经网络（GNN）已被应用于这种预测设置中 [[163](#bib.bib163), [43](#bib.bib43),
    [193](#bib.bib193), [102](#bib.bib102), [63](#bib.bib63), [206](#bib.bib206)]，在这种设置中，除了预测任务之外，挑战在于如何最佳利用所提供的图信息，甚至在没有图的情况下学习图。提出学习图的方法通过寻找能够产生最准确预测的图来实现。每个维度的嵌入被学习，并且使用这些嵌入计算每两个维度之间的相似性得分，从中获得邻接矩阵，邻接矩阵可以通过取K-top边
    [[43](#bib.bib43), [193](#bib.bib193)] 或从中进行采样 [[163](#bib.bib163), [102](#bib.bib102)]。目前，已经提出了两种主要策略来学习节点嵌入，要么通过简单的梯度下降
    [[43](#bib.bib43), [193](#bib.bib193)]，要么通过从时间序列中获取表示 [[163](#bib.bib163), [102](#bib.bib102)]，后者方法似乎产生了更好的结果。虽然这些方法都被呈现为点预测方法，但通过训练这些模型以参数化预测分布，可以获得概率预测，如第[2.4节](#S2.SS4
    "2.4 输出模型与损失函数 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程和文献综述")中所解释的那样。
- en: 3.5 Physics-based Models
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 基于物理的模型
- en: In *physics-based* models, deep forecasting methods have been proposed that
    model the underlying dynamics in sophisticated ways. Chen et al. [[32](#bib.bib32)]
    proposed the Neural ODE (NODE) model, where an ordinary differential equation
    (ODE) is solved forward in time, and the adjoint equation is solved backwards
    in time using backpropagation. One limitation of the Neural ODE model is that
    the unknown parameters $\theta$ are assumed to be constant in time. Other limitations
    such as computational complexity have been addressed in follow-up work, e.g., [[18](#bib.bib18)].
    Vialard et al. [[185](#bib.bib185)] extends the NODE model to allow the parameters
    $\theta(t)$ to be time-varying by introducing a shooting formulation. In the shooting
    formulation, the optimal $\theta$ is determined by minimizing a regularized loss
    function. Vialard et al. [[185](#bib.bib185)] also shows that a residual network
    (ResNet) can be expressed as the Forward Euler discretization of an ODE with time
    step $\Delta t=1$. Wang et al. [[187](#bib.bib187)] compares successful time series
    deep sequence models, such as [[156](#bib.bib156), [146](#bib.bib146)] to NODE
    and other hybrid deep learning models to model COVID-19 dynamics, as well as the
    population dynamics using the Lotka-Volterra equations. Through their benchmarking
    study, the authors show that distribution shifts can pose problems for deep sequence
    models on these tasks, and propose a hybrid model AutoODE to model the underlying
    dynamics.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在*基于物理的*模型中，已经提出了深度预测方法，这些方法以复杂的方式建模潜在的动态。Chen等人[[32](#bib.bib32)]提出了神经常微分方程（NODE）模型，其中一个常微分方程（ODE）在时间上向前求解，伴随方程通过反向传播在时间上向后求解。神经ODE模型的一个限制是未知参数$\theta$被假设为时间不变。其他限制，如计算复杂性，在后续工作中得到了处理，例如[[18](#bib.bib18)]。Vialard等人[[185](#bib.bib185)]扩展了NODE模型，通过引入发射公式使参数$\theta(t)$能够随时间变化。在发射公式中，通过最小化正则化损失函数来确定最佳$\theta$。Vialard等人[[185](#bib.bib185)]还展示了残差网络（ResNet）可以表示为具有时间步长$\Delta
    t=1$的ODE的向前欧拉离散化。Wang等人[[187](#bib.bib187)]将成功的时间序列深度序列模型，如[[156](#bib.bib156),
    [146](#bib.bib146)]，与NODE及其他混合深度学习模型进行了比较，以建模COVID-19动态，以及使用Lotka-Volterra方程建模人口动态。通过他们的基准研究，作者展示了分布偏移可能对这些任务中的深度序列模型造成问题，并提出了一种混合模型AutoODE来建模潜在的动态。
- en: 3.6 Global-local
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 全球-局部
- en: 'With local models, the free parameters of the model are learned individually
    for each series in a collection, see Section [2.1](#S2.SS1 "2.1 Notation and Formalization
    of the Forecasting Problem ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey"). Classical local time
    series models such as SSMs, ARIMA, and exponential smoothing (ETS) [[84](#bib.bib84)]
    excel at modelling the complex dynamics of individual time series given a sufficiently
    long history. Other local models include Gaussian SSMs, which are computationally
    efficient, e.g., via a Kalman filter, and Gaussian Processes (GPs)  [[149](#bib.bib149),
    [159](#bib.bib159), [67](#bib.bib67), [27](#bib.bib27)]. These methods provide
    uncertainty estimates, which are critical for optimal downstream decision making.
    Since these methods are local, they learn one model per time series and cannot
    effectively extract information across multiple time series. These methods are
    unable to address cold-start problems where there is a need to generate predictions
    for a time series with little or no observed history.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用局部模型时，模型的自由参数是针对集合中的每个序列单独学习的，参见第[2.1](#S2.SS1 "2.1 预测问题的符号和形式化 ‣ 2 深度预测：教程
    ‣ 时间序列预测的深度学习：教程和文献综述")节。经典的局部时间序列模型如SSMs、ARIMA和指数平滑（ETS）[[84](#bib.bib84)]擅长在具有足够长历史的情况下建模个体时间序列的复杂动态。其他局部模型包括高斯SSMs，这些模型计算高效，例如，通过卡尔曼滤波器，以及高斯过程（GPs）[[149](#bib.bib149),
    [159](#bib.bib159), [67](#bib.bib67), [27](#bib.bib27)]。这些方法提供不确定性估计，这对于最佳的下游决策制定至关重要。由于这些方法是局部的，它们为每个时间序列学习一个模型，无法有效地提取跨多个时间序列的信息。这些方法无法解决冷启动问题，即需要为具有很少或没有观察历史的时间序列生成预测。
- en: Conversely, recall that in global models, their free parameters are learned
    jointly on every series in a collection of time series. NNs have proven particularly
    well suited as global models [[156](#bib.bib156), [64](#bib.bib64), [146](#bib.bib146),
    [190](#bib.bib190), [111](#bib.bib111)]. Global methods can extract patterns from
    collections of irregular time series even when these patterns would not be distinguishable
    using a single series.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，回忆一下在全球模型中，它们的自由参数是通过对每个时间序列集合中的所有序列进行联合学习来获得的。NNs 证明特别适合作为全球模型 [[156](#bib.bib156),
    [64](#bib.bib64), [146](#bib.bib146), [190](#bib.bib190), [111](#bib.bib111)]。全球方法可以从不规则时间序列的集合中提取模式，即使这些模式在单个序列中无法区分。
- en: 'Global-local models have been proposed to combine the advantages of both global
    and local models. Examples include mixed effect models  [[40](#bib.bib40)], which
    consist of two kinds of effects: fixed (global) effects that describe the whole
    population, and random (local) effects that capture the idiosyncratic of individuals
    or subgroups. A similar mixed approach is used in hierarchical Bayesian [[65](#bib.bib65)]
    methods, which combine global and local models to jointly model a population of
    related statistical problems. In an early example of hierarchical Bayesian models,
    [[31](#bib.bib31)] combined global and local features for intermittent demand
    forecasting in retail planning. In [[3](#bib.bib3), [123](#bib.bib123)], other
    combined global and local models are detailed.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出全球-本地模型，以结合全球模型和本地模型的优点。例如，混合效应模型 [[40](#bib.bib40)] 包括两种效应：描述整个群体的固定（全球）效应，以及捕捉个体或子群体特异性的随机（本地）效应。类似的混合方法在层次贝叶斯
    [[65](#bib.bib65)] 方法中使用，这些方法结合了全球和本地模型，以联合建模相关统计问题的群体。在早期的层次贝叶斯模型示例中，[[31](#bib.bib31)]
    结合了全球和本地特征，用于零售规划中的间歇性需求预测。在 [[3](#bib.bib3), [123](#bib.bib123)] 中，详细描述了其他组合的全球和本地模型。
- en: A recent global-local family of models, Deep Factors [[188](#bib.bib188)] provide
    an alternative way to combine the expressive power of NNs with the data efficiency
    and uncertainty estimation abilities of classical probabilistic local models.
    Each time series, or its latent function for non-Gaussian data, is represented
    as the weighted sum of a global time series and a local model. The global part
    is given by a linear combination of a set of deep dynamic factors, where the loading
    is temporally determined by attentions. The local model is stochastic. Typical
    choices include white noise processes, linear dynamical systems, GPs [[127](#bib.bib127)]
    or RNNs. The stochastic local component allows for the uncertainty to propagate
    forward in time, while the global NN model is capable of extracting complex nonlinear
    patterns across multiple time series. The global-local structure extracts complex
    nonlinear patterns globally while capturing individual random effects for each
    time series locally.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的全球-本地模型系列，Deep Factors [[188](#bib.bib188)] 提供了一种将神经网络的表达能力与经典概率本地模型的数据效率和不确定性估计能力结合起来的替代方法。每个时间序列，或其对于非高斯数据的潜在函数，都被表示为全球时间序列和本地模型的加权和。全球部分由一组深度动态因子的线性组合给出，其中负载通过注意机制在时间上确定。本地模型是随机的。典型的选择包括白噪声过程、线性动态系统、GPs
    [[127](#bib.bib127)] 或 RNNs。随机本地组件允许不确定性随时间传播，而全球NN模型能够提取多个时间序列中的复杂非线性模式。全球-本地结构在全局范围内提取复杂的非线性模式，同时在本地捕捉每个时间序列的个体随机效应。
- en: The Deep Global Local Forecaster (DeepGLO) [[162](#bib.bib162)] is a method
    that “thinks globally and acts locally” to forecast collections of up to millions
    of time series. It crucially relies on a type of temporal convolution (a so-called
    leveled network), that can be trained across a large amount time series with different
    scales without the need for normalization or rescaling. DeepGLO is a hybrid model
    that uses a global matrix factorization model [[200](#bib.bib200)] regularized
    by a temporal deep leveled network and a local temporal deep level network to
    capture patterns specific to each time series. Each time series is represented
    by a linear combination of $k$ basis time series, where $k\ll N$, with $N$ the
    total number of time series. The global and local models are combined through
    data-driven attention for each time series.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 深度全球局部预测器（DeepGLO）[[162](#bib.bib162)]是一种“全球思考，局部行动”的方法，用于预测高达数百万条时间序列的集合。它主要依赖于一种时间卷积（所谓的分层网络），这种网络可以在不同尺度的大量时间序列上进行训练，无需标准化或重新缩放。DeepGLO
    是一种混合模型，使用一个全球矩阵分解模型[[200](#bib.bib200)]，通过时间深度分层网络进行正则化，并结合局部时间深度层级网络以捕捉每条时间序列特定的模式。每条时间序列通过$k$个基础时间序列的线性组合来表示，其中$k\ll
    N$，$N$是时间序列的总数。全球和局部模型通过数据驱动的注意力机制结合在一起，以处理每条时间序列。
- en: 'A further example in the global-local model class is the ES-RNN model proposed
    by Smyl [[169](#bib.bib169)] that has recently attracted attention by winning
    the M4 competition [[129](#bib.bib129)] by a large margin on both evaluation settings.
    In the ES-RNN model, locally estimated level and trend components are multiplicatively
    combined with an RNN model. Apart from its global-local nature, it also integrates
    aspects of different model classes into a a single model similar to Deep State
    Space models (Section [3.3](#S3.SS3 "3.3 Deep State Space Models ‣ 3 Literature
    review ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature Survey")).
    In particular, the $h$-step ahead prediction $\hat{\mathbf{z}}_{i,t+1:t+h}=l_{i,t}\cdot\mathbf{s}_{i,t+1:t+h}\cdot\exp(\text{RNN}(\mathbf{x}_{i,t}))$
    consists of a level $l_{i,t}$ and a seasonal component $s_{i,t}$ obtained through
    local exponential smoothing, and the output of a global RNN model $\text{RNN}(\mathbf{x}_{i,t})$,
    where $\mathbf{x}_{i,t}$ is a vector of preprocessed data extracted from deseasonalized
    and normalized time series $\mathbf{x}_{i,t}=\log(\mathbf{z}_{i,t-K:t}/(\mathbf{s}_{i,t-K:t}l_{i,t}))$
    cut in a window of length $K+1$. The RNN models are composed of dilated LSTM layers
    with additional residual connections. The M4-winning entry used slightly different
    architectures for the different type of time series in the competition.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '全球-局部模型类的另一个例子是Smyl提出的ES-RNN模型[[169](#bib.bib169)]，该模型因在M4比赛中[[129](#bib.bib129)]在两个评估设置中大幅度获胜而受到关注。在ES-RNN模型中，局部估计的水平和趋势成分与RNN模型乘法组合。除了全球-局部性质外，它还将不同模型类别的方面整合到一个类似于深度状态空间模型的单一模型中（第[3.3](#S3.SS3
    "3.3 Deep State Space Models ‣ 3 Literature review ‣ Deep Learning for Time Series
    Forecasting: Tutorial and Literature Survey")节）。特别地，$h$步预测$\hat{\mathbf{z}}_{i,t+1:t+h}=l_{i,t}\cdot\mathbf{s}_{i,t+1:t+h}\cdot\exp(\text{RNN}(\mathbf{x}_{i,t}))$由通过局部指数平滑获得的水平$l_{i,t}$和季节成分$s_{i,t}$组成，以及一个全球RNN模型的输出$\text{RNN}(\mathbf{x}_{i,t})$，其中$\mathbf{x}_{i,t}$是从去季节化和标准化的时间序列中提取的预处理数据向量$\mathbf{x}_{i,t}=\log(\mathbf{z}_{i,t-K:t}/(\mathbf{s}_{i,t-K:t}l_{i,t}))$，被截取在长度为$K+1$的窗口中。RNN模型由扩张LSTM层和额外的残差连接组成。M4获胜的条目对比赛中不同类型的时间序列使用了略有不同的架构。'
- en: 3.7 Intermittent Time Series
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 间歇性时间序列
- en: We noted in the introduction that deep forecasting models had a major impact
    on operational forecasting problems. In these large-scale problem, intermittent
    time series occur regularly [[25](#bib.bib25)]. Accordingly, research on NNs for
    intermittent time series forecasting has been an active area. Salinas et al. [[156](#bib.bib156)]
    propose a standard RNN architecture with a negative binomial likelihood to handle
    intermittent demand similar to [[171](#bib.bib171)] in classical methods. To the
    best of our knowledge, other likelihoods that have been proposed for intermittent
    time series in classical models, e.g., by [[160](#bib.bib160)], have not yet been
    carried over to NNs. However, some initial work is available via more standard
    likelihoods [[156](#bib.bib156), [93](#bib.bib93)].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在引言中指出，深度预测模型对操作预测问题产生了重大影响。在这些大规模问题中，间歇性时间序列经常出现[[25](#bib.bib25)]。因此，关于间歇性时间序列预测的神经网络研究一直是一个活跃的领域。Salinas
    等人[[156](#bib.bib156)] 提出了一个标准的 RNN 架构，使用负二项分布似然来处理类似于经典方法中[[171](#bib.bib171)]的间歇性需求。根据我们所知，其他在经典模型中为间歇性时间序列提出的似然，例如[[160](#bib.bib160)]，尚未被应用于神经网络。然而，一些初步工作已经通过更标准的似然[[156](#bib.bib156),
    [93](#bib.bib93)]可用。
- en: In the seminal paper on intermittent demand forecasting [[41](#bib.bib41)],
    Croston separates the data in a sequence of observed non-zero demands and a sequence
    of time intervals between positive demand observations, and runs exponential smoothing
    separately on both series. A comparison of NNs to classical models for intermittent
    demand first appeared in Gutierrez et al. [[73](#bib.bib73)], where the authors
    compare the performance of a shallow and narrow MLP with Croston’s method. They
    find NNs to outperform classical methods by a significant margin.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于间歇性需求预测的开创性论文[[41](#bib.bib41)]中，Croston 将数据分为观察到的非零需求序列和正需求观察之间的时间间隔序列，并分别对这两个序列进行指数平滑。对神经网络与经典模型进行的间歇性需求比较首次出现在
    Gutierrez 等人[[73](#bib.bib73)]的研究中，作者将浅层且狭窄的 MLP 与 Croston 方法的性能进行比较。他们发现神经网络在性能上显著优于经典方法。
- en: Kourentzes [[105](#bib.bib105)] proposes two MLP architectures for intermittent
    demand, taking demand sizes and intervals as inputs. As in Gutierrez et al. [[73](#bib.bib73)],
    the networks are shallow and narrow by modern standards, with only a single hidden
    layer and three hidden units. The difference between the two architectures is
    in the output. In one case interval times and non-zero occurrences are output
    separately, while in the other a ratio of the two is computed. The approach proposed
    by Kourentzes [[105](#bib.bib105)] outperforms other approaches primarily with
    respect to inventory metrics, but not forecasting accuracy metrics, challenging
    previous results in [[73](#bib.bib73)]. It is unclear whether the models are used
    as global or local. However, given the concern around overfitting and regularization,
    we assume that these models were primarily used as local models in the experiments.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Kourentzes[[105](#bib.bib105)] 提出了两种间歇性需求的 MLP 架构，将需求大小和时间间隔作为输入。与 Gutierrez
    等人[[73](#bib.bib73)]一样，这些网络在现代标准下被认为是浅层且狭窄的，仅有一个隐藏层和三个隐藏单元。这两种架构的区别在于输出。在一种情况下，间隔时间和非零出现次数分别输出，而另一种情况下则计算二者的比率。Kourentzes
    提出的[[105](#bib.bib105)] 方法主要在库存指标上优于其他方法，但在预测准确性指标上却不如[[73](#bib.bib73)]中的结果。这些模型是否作为全局模型或局部模型使用尚不明确。然而，考虑到过拟合和正则化的问题，我们假设这些模型主要在实验中作为局部模型使用。
- en: Both approaches of [[73](#bib.bib73), [105](#bib.bib105)] only offer point forecasts.
    This shortcoming is addressed by [[180](#bib.bib180), [182](#bib.bib182)], where
    the authors propose renewal processes as natural models for intermittent demand
    forecasting. Specifically, they use RNNs to modulate both discrete time and continuous
    time renewal processes, using the simple analogy that RNNs can replace exponential
    smoothing in [[41](#bib.bib41)].
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[[73](#bib.bib73), [105](#bib.bib105)] 两种方法仅提供点预测。[[180](#bib.bib180), [182](#bib.bib182)]
    通过提出将更新过程作为间歇性需求预测的自然模型来解决这一不足。具体来说，他们使用 RNNs 来调节离散时间和连续时间更新过程，使用简单的类比，将 RNNs
    替代[[41](#bib.bib41)]中的指数平滑。'
- en: Finally, a recent trend in sequence modelling employs NNs in modelling discrete
    event sequences observed in continuous time  [[48](#bib.bib48), [133](#bib.bib133),
    [194](#bib.bib194), [164](#bib.bib164), [181](#bib.bib181), [165](#bib.bib165)]
    and [[166](#bib.bib166)] for an overview. Notably, Xiao et al. [[195](#bib.bib195)]
    use two RNNs to parametrize a probabilistic “point process” model. These networks
    consume data from asynchronous event sequences and uniformly sampled time series
    observations respectively. Their model can be used in forecasting tasks where
    time series data can be enriched with discrete event observations in continuous
    time.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，序列建模中的一个新趋势是使用神经网络建模在连续时间中观察到的离散事件序列 [[48](#bib.bib48), [133](#bib.bib133),
    [194](#bib.bib194), [164](#bib.bib164), [181](#bib.bib181), [165](#bib.bib165)]
    和 [[166](#bib.bib166)] 以获取概述。值得注意的是，Xiao 等人 [[195](#bib.bib195)] 使用两个 RNN 来参数化一个概率“点过程”模型。这些网络分别处理来自异步事件序列的数据和均匀采样的时间序列观察值。他们的模型可以用于预测任务，其中时间序列数据可以通过连续时间中的离散事件观察进行丰富。
- en: 3.8 Generalized Adversarial Networks
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 泛化对抗网络
- en: 'Additionally to the approaches mentioned in Sections [2.4](#S2.SS4 "2.4 Output
    Models and Loss Functions ‣ 2 Deep Forecasting: A Tutorial ‣ Deep Learning for
    Time Series Forecasting: Tutorial and Literature Survey") and [2.4.3](#S2.SS4.SSS3
    "2.4.3 Further approaches ‣ 2.4 Output Models and Loss Functions ‣ 2 Deep Forecasting:
    A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey"), the recent literature contains further examples for density estimation,
    most prominently via Generalized Adversarial Networks (GANs) [[70](#bib.bib70)].
    While GANs have received much attention in the overall deep learning literature [[98](#bib.bib98),
    [199](#bib.bib199), [52](#bib.bib52), [121](#bib.bib121), [53](#bib.bib53)], this
    has not been reflected in forecasting. We speculate that this is because a discriminator
    network can be replaced by metrics such as CRPS which measure the quality of generated
    samples. We therefore only provide a brief overview here and mention that, while
    they rely on the buildings blocks discussed in Section [2](#S2 "2 Deep Forecasting:
    A Tutorial ‣ Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey"), they typically require architectures that are more complex than then
    ones discussed here and lead to involved optimization problems.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除了第 [2.4](#S2.SS4 "2.4 输出模型与损失函数 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程与文献综述") 和第 [2.4.3](#S2.SS4.SSS3
    "2.4.3 进一步方法 ‣ 2.4 输出模型与损失函数 ‣ 2 深度预测：教程 ‣ 时间序列预测的深度学习：教程与文献综述") 节中提到的方法外，最近的文献中还包含了更多的密度估计示例，最突出的是通过泛化对抗网络（GANs）
    [[70](#bib.bib70)]。尽管 GAN 在整体深度学习文献中受到广泛关注 [[98](#bib.bib98), [199](#bib.bib199),
    [52](#bib.bib52), [121](#bib.bib121), [53](#bib.bib53)]，但这种关注并没有反映在预测中。我们推测这是因为判别网络可以被如
    CRPS 这样的度量替代，这些度量衡量生成样本的质量。因此，我们在这里仅提供简要概述，并提到尽管它们依赖于第 [2](#S2 "2 深度预测：教程 ‣ 时间序列预测的深度学习：教程与文献综述")
    节中讨论的基础构件，但它们通常需要比这里讨论的更复杂的架构，并导致复杂的优化问题。
- en: Despite the comparably less attention that GANs have received in forecasting,
    they have been recently applied to the time series domain [[53](#bib.bib53), [199](#bib.bib199)]
    to synthesize data [[177](#bib.bib177), [53](#bib.bib53)] or to employ an adversarial
    loss in forecasting tasks [[192](#bib.bib192)]. Many time series GAN architectures
    use recurrent networks to model temporal dynamics [[134](#bib.bib134), [53](#bib.bib53),
    [199](#bib.bib199)]. Modelling long-range dependencies and scaling recurrent networks
    to higher lengths is inherently difficult and limits the application of time series
    GANs to short sequence lengths [[199](#bib.bib199), [53](#bib.bib53)]. One way
    to achieve longer realistic synthetic time series is by employing convolutional [[183](#bib.bib183),
    [11](#bib.bib11), [62](#bib.bib62)] and self-attention architectures [[184](#bib.bib184)].
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GAN 在预测领域受到的关注相对较少，但它们最近已被应用于时间序列领域 [[53](#bib.bib53), [199](#bib.bib199)]
    来合成数据 [[177](#bib.bib177), [53](#bib.bib53)] 或在预测任务中使用对抗损失 [[192](#bib.bib192)]。许多时间序列
    GAN 架构使用递归网络来建模时间动态 [[134](#bib.bib134), [53](#bib.bib53), [199](#bib.bib199)]。建模长程依赖关系并将递归网络扩展到更长的序列本质上是困难的，并限制了时间序列
    GAN 的应用于短序列长度 [[199](#bib.bib199), [53](#bib.bib53)]。实现更长的真实合成时间序列的一种方法是使用卷积 [[183](#bib.bib183),
    [11](#bib.bib11), [62](#bib.bib62)] 和自注意力架构 [[184](#bib.bib184)]。
- en: Convolutional architectures are able to learn relevant features from the raw
    time series data [[183](#bib.bib183), [11](#bib.bib11), [62](#bib.bib62)], but
    are ultimately limited to local receptive fields and can only capture long-range
    dependencies via many stacks of convolutional layers. Self-attention can bridge
    this gap and allow for modelling long-range dependencies from convolutional feature
    maps, which has been a successful approach in the image [[203](#bib.bib203)] and
    time series forecasting domain [[116](#bib.bib116)]. Another technique to achieve
    long sample sizes is progressive growing, which successively increases the resolution
    by adding layers to a GAN generator and discriminator during training [[97](#bib.bib97)].
    A recent proposal [[22](#bib.bib22)] synthesizes progressive growing with convolutions
    and self-attention into a novel architecture particularly geared towards time
    series.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积架构能够从原始时间序列数据中学习相关特征[[183](#bib.bib183), [11](#bib.bib11), [62](#bib.bib62)]，但最终受限于局部感受野，只能通过多个卷积层堆叠捕获长期依赖关系。自注意力可以弥补这一差距，并允许从卷积特征图中建模长期依赖关系，这在图像[[203](#bib.bib203)]和时间序列预测领域[[116](#bib.bib116)]中取得了成功。另一种实现长样本大小的技术是渐进生长，通过在训练过程中向GAN生成器和判别器添加层来逐步增加分辨率[[97](#bib.bib97)]。最近的一个提案[[22](#bib.bib22)]将渐进生长与卷积和自注意力综合为一种新型架构，特别针对时间序列。
- en: 3.9 Summary and Practical Guidelines
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9 总结和实用指南
- en: 'In Section [2](#S2 "2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time
    Series Forecasting: Tutorial and Literature Survey") and this section, we introduced
    a large number of deep forecasting models. We summarize the main approaches in
    Table LABEL:tab:model_summary. The list below provide keys to reading the table.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[2](#S2 "2 Deep Forecasting: A Tutorial ‣ Deep Learning for Time Series Forecasting:
    Tutorial and Literature Survey)节和本节中，我们介绍了大量的深度预测模型。我们在表 LABEL:tab:model_summary
    中总结了主要方法。以下列表提供了阅读表格的关键。'
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Forecast* distinguishes between probabilistic (*Prob*) and *Point* forecasts.'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*Forecast* 区分概率预测（*Prob*）和*点*预测。'
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Horizon* indicates whether the model does one-step predictions (noted $1$)
    in which case multi-step forecasts are obtained recursively, or if it directly
    predicts a whole sequence ($\geq 1$).'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*Horizon* 表示模型是否进行一步预测（记作 $1$），在这种情况下，多步预测是递归获得的，或者模型是否直接预测整个序列（$\geq 1$）。'
- en: •
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Loss* and *Metrics* specifies the loss used for training and metrics used
    for evaluation. Here, we only provide an explanation of the acronyms and not the
    definition of each metric which can be easily found in the corresponding papers:
    negative log-likelihood (NLL), quantile loss (QL), continuous ranked probability
    score (CRPS), (normalized) (root) mean squared error (NRMSE, RMSE, MSE), root
    relative squared error (RRSE), relative geometric RMSE (RGRMSE), weighted absolute
    percentage error (WAPE), normalized deviation (ND), mean absolute deviation (MAD),
    mean absolute error (MAE), mean relative error (MRE), (weighted) mean absolute
    percentage error (wMAPE, MAPE), mean absolute scaled error (MASE), overall weighted
    average (OWA), mean scaled interval score (MSIS), Kullback-Leibler divergence
    (KL), Value-at-Risk (VaR), expected shortfall (ES), empirical correlation coefficient
    (CORR), area under the receiver operating characteristic (AUROC), percentage best
    (PB).'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*Loss* 和 *Metrics* 指定了用于训练的损失函数和用于评估的指标。在这里，我们只提供缩写的解释，而不是每个指标的定义，具体定义可以在相关论文中找到：负对数似然（NLL）、分位数损失（QL）、连续排名概率得分（CRPS）、（归一化的）（均方根）误差（NRMSE,
    RMSE, MSE）、根相对平方误差（RRSE）、相对几何均方根误差（RGRMSE）、加权绝对百分比误差（WAPE）、归一化偏差（ND）、平均绝对偏差（MAD）、平均绝对误差（MAE）、平均相对误差（MRE）、（加权的）平均绝对百分比误差（wMAPE,
    MAPE）、平均绝对缩放误差（MASE）、整体加权平均（OWA）、平均缩放区间得分（MSIS）、Kullback-Leibler散度（KL）、风险值（VaR）、预期短缺（ES）、经验相关系数（CORR）、接收者操作特征曲线下面积（AUROC）、最佳百分比（PB）。'
- en: While Table LABEL:tab:model_summary serves to illustrate the wealth of deep
    forecasting methods now available, their sheer number may be slightly overwhelming.
    Furthermore, empirical evidence on the effectiveness of the different architectures
    has so far not revealed a clearly superior approach [[4](#bib.bib4)]. In this,
    forecasting differs from other domains, e.g., natural language processing where
    Transformer-based models [[184](#bib.bib184)] dominate overall. Also, deep forecasting
    methods seem to differ from other model families, such as tree-based methods where
    LightGBM [[99](#bib.bib99)] or XGBoost [[33](#bib.bib33)] dominate (as in the
    recent M5 forecasting competition [[92](#bib.bib92)]). We speculate that this
    diffuse picture is in part due to the practical reasons, the relative immaturity
    of the field and the corresponding software implementations and in part due to
    fundamental reason as a natural consequence of the breadth and diversity of forecasting
    problems.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:tab:model_summary 旨在展示目前可用的丰富深度预测方法，其数量之多可能稍显令人不知所措。此外，对不同架构有效性的实证证据迄今为止尚未揭示出明显优越的方法[[4](#bib.bib4)]。在这方面，预测不同于其他领域，例如自然语言处理，其中基于
    Transformer 的模型[[184](#bib.bib184)]总体上占据主导地位。此外，深度预测方法似乎与其他模型家族有所不同，例如树基方法，其中
    LightGBM[[99](#bib.bib99)] 或 XGBoost[[33](#bib.bib33)] 占主导地位（如在最近的 M5 预测比赛中[[92](#bib.bib92)]）。我们推测这种模糊的现象部分是由于实际原因、该领域的相对不成熟以及相应的软件实现，部分是由于预测问题的广泛性和多样性作为自然结果的根本原因。
- en: So, choosing the appropriate architecture for a problem at hand can be a daunting
    task. In the following, we therefore attempt to provide guidelines for a more
    informed deep forecasting model selection. These are largely based on our own
    experience in working with practical forecasting problem and they should primarily
    be taken as a non-exhaustive guidance on where to start model exploration.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为手头的问题选择合适的架构可能是一项艰巨的任务。接下来，我们将尝试提供有关更有根据的深度预测模型选择的指南。这些指南主要基于我们在处理实际预测问题中的经验，应主要作为模型探索的非详尽性指导。
- en: 3.9.1 Baseline methods and standard mode of deployment
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.9.1 基准方法和标准部署模式
- en: At the start of any in-depth model exploration, considering a baseline model
    is commonly accepted best practice. To the best of our knowledge, the most mature
    deep forecasting models are DeepAR [[156](#bib.bib156)] and MQCNN [[190](#bib.bib190)]
    which exist in a number of open-source and commercial implementations.³³3[https://aws.amazon.com/blogs/machine-learning/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/](https://aws.amazon.com/blogs/machine-learning/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/)
    As a practical guideline, we recommend to start model exploration using at least
    these methods as baselines. Other candidates we would consider are N-BEATS [[138](#bib.bib138)],
    WaveNet [[183](#bib.bib183)] and a Transformer-based model. The relative performance
    of these methods compared with other methods should give reasonable, directional
    evidence whether the problem at hand is amenable to deep forecasting methods.
    We note that AutoML approaches for forecasting are available⁴⁴4[http://ai.googleblog.com/2020/12/using-automl-for-time-series-forecasting.html](http://ai.googleblog.com/2020/12/using-automl-for-time-series-forecasting.html)
    but while promising are in their infancy. At least in the M5 competition, they
    are still outperformed by the aforementioned more specialized deep forecasting
    models.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何深入的模型探索开始时，考虑基准模型通常被认为是最佳实践。根据我们的了解，最成熟的深度预测模型是 DeepAR[[156](#bib.bib156)]
    和 MQCNN[[190](#bib.bib190)]，这些模型在许多开源和商业实现中存在。³³3[https://aws.amazon.com/blogs/machine-learning/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/](https://aws.amazon.com/blogs/machine-learning/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/)
    作为实用指南，我们建议至少使用这些方法作为基准来开始模型探索。我们还考虑的其他候选方法有 N-BEATS[[138](#bib.bib138)]、WaveNet[[183](#bib.bib183)]
    和基于 Transformer 的模型。这些方法与其他方法的相对性能应提供合理的方向性证据，表明所处理的问题是否适合深度预测方法。我们注意到，虽然 AutoML
    方法对于预测是可用的⁴⁴4[http://ai.googleblog.com/2020/12/using-automl-for-time-series-forecasting.html](http://ai.googleblog.com/2020/12/using-automl-for-time-series-forecasting.html)，但虽然有前景，但仍处于初期阶段。至少在
    M5 比赛中，它们仍被前述更专业的深度预测模型所超越。
- en: Our typical suggestion is to employ NNs as global models since, given enough
    data, global methods outperform classical local methods when dealing with groups
    of similar time series.⁵⁵5This is a more generally applicable fact beyond NN.
    Montero-Manso and Hyndman [[135](#bib.bib135)] show favorable theoretical and
    empirical properties for global over local models. Interestingly, recent empirical
    evidence have shown that global models can achieve a state-of-the-art performance
    even in heterogeneous groups of time series. This is supported by the M4 [[129](#bib.bib129)]
    and M5 competitions where the top performing models had some form of globality.
    This suggests a more general applicability of global methods with a high impact
    on practical application where a general automated forecasting mechanism is required.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的典型建议是将神经网络作为全局模型，因为在有足够数据的情况下，与处理相似时间序列组的经典局部方法相比，全局方法会表现出色。⁵⁵5这是一个不仅适用于神经网络的更普遍的事实。Montero-Manso和Hyndman[[135](#bib.bib135)]展示了全局模型相对于局部模型具有有利的理论和实证性质。有趣的是，最近的实证证据表明，即使在不同的时间序列组中，全局模型也能取得最先进的性能。这得到了M4[[129](#bib.bib129)]和M5比赛的支持，其中表现最佳的模型具有某种全局性。这表明了全局方法在实际应用中具有更广泛的适用性，对于需要一种通用的自动化预测机制的实际应用产生了很大的影响。
- en: 3.9.2 Data characteristics
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.9.2 数据特征
- en: The amount of data available is among the easiest dimensions in choosing a deep
    forecasting model. First, NNs require a minimum amount of data to be effective
    in comparison to other, more parsimoniously parametrized models. This is perhaps
    the most important factor in successful applications of NNs in forecasting. How
    much data does one need for a given application? Several important points should
    be discussed on this question. First, the amount of data is often misunderstood
    as the *number of time series* but in reality the amount of data typically relates
    to the *number of observations*. For instance, one may have only one time series
    but many thousands of observations, as in the case of a time series from a real-time
    sensor where measurements happen every second for a year, allowing to fit a complex
    NN [[2](#bib.bib2)]. Second, it is probably better to see the amount of data in
    terms of *information quantity*. For instance, in finance the amount of information
    of many millions of hourly transactions is limited given the very low signal-to-noise
    ratio in contrast to a retailer whose products follow clear seasonality and patterns,
    making it easier to apply deep learning methods. The more structured the data
    is (e.g., via strong seasonality or knowledge about the underlying process) the
    better deep forecasting models that incorporate these structures will fare. On
    the contrary, if the time series are more irregular or short, a more data-driven
    approach (e.g., via Transformer-based models) will often be preferable. The importance
    of covariate information for the forecasting problem at hand can further help
    determine the correct method. Some NN architectures need extensions to include
    such information while others readily accept them.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 可用数据量是选择深度预测模型中最容易的维度之一。首先，与其他参数更为简约的模型相比，神经网络需要更少的数据才能发挥有效作用。这也许是神经网络成功应用于预测中最重要的因素。对于特定应用，需要多少数据？这个问题上有几个重要的观点需要讨论。首先，数据量通常被误解为*时间序列的数量*，但实际上，数据量通常与*观测数量*相关联。例如，一个人可能只有一个时间序列，但有成千上万次的观测，就像实时传感器的时间序列，在一年内每秒进行测量，可以适应一个复杂的神经网络[[2](#bib.bib2)]。其次，可能更好地将数据量看作*信息量*。例如，在金融领域，每小时成百万次交易的信息量是有限的，因为信噪比非常低，与之形成鲜明对比的是，零售商的产品具有明显的季节性和规律性，更容易应用深度学习方法。数据结构化程度越高（例如通过强烈的季节性或对基础过程的了解），将结构纳入深层预测模型的效果就越好。相反，如果时间序列更不规则或更短，则更数据驱动的方法（例如基于Transformer的模型）通常更可取。对于手头的预测问题的附加信息的重要性可以进一步帮助确定正确的方法。一些神经网络架构需要扩展以包括这些信息，而其他一些则可以轻易接受它们。
- en: From a practical perspective, NNs have been reported to outperform demand forecasting
    baselines starting from 50000 observations in [[156](#bib.bib156)] and from a
    few hundred observations in load-forecasting [[146](#bib.bib146), [188](#bib.bib188)].
    Understanding better these limitation, both theoretically and empirically, is
    an area of current research and is not yet well understood. See [[131](#bib.bib131)]
    for some current theoretical work on sample complexity of global-local approaches
    for instance and [[23](#bib.bib23)] for empiricial work.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际角度来看，已有报告指出，神经网络（NNs）在[[156](#bib.bib156)]中从50000个观测值开始表现优于需求预测基准，而在负载预测中从几百个观测值开始表现优于[[146](#bib.bib146),
    [188](#bib.bib188)]。更好地理解这些限制，无论是理论上的还是经验上的，是当前研究的一个领域，尚未被完全理解。有关全球-局部方法样本复杂性的当前理论工作可参考[[131](#bib.bib131)]，经验性工作可参考[[23](#bib.bib23)]。
- en: 3.9.3 Problem characteristics
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.9.3 问题特征
- en: The characteristics of the forecasting problem to be solved are natural important
    decision points. We list a few dimensions to consider here.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 需要解决的预测问题的特征是自然重要的决策点。我们列出了一些需要考虑的维度。
- en: 'One important aspect of a model is its forecast nature, i.e., if it produces
    probabilistic or point forecasts. The choice of this is highly dependent on the
    underlying application. To illustrate this we can examine two different forecasting
    use cases: product demand and CPU utilization. In the former use case one wishes
    to forecast the future demand of a product in order to take a more informed decision
    about the stock that is required to have in a warehouse or to optimize the labour
    planning based on the traffic that is expected. In the latter, the forecast of
    CPU utilization could be used to identify in a timely manner if a process will
    fail in order to proactively resolve associated issues, or to detect possible
    anomalous behaviours that could trigger some root cause analysis and system improvements.
    Although in both applications a forecast is required, the end goal is different,
    which changes the requirements of the chosen forecasting model. For example, for
    product demand the whole distribution of the future demand might be important:
    one cannot rely on a single forecast value since the variance in the forecast
    plays an important role to avoid out of stock issues or under/over planning the
    expected required labour. Therefore, in this application it is important to use
    a model that focuses on predicting accurately the whole distribution. On the other
    hand, for CPU utilization one might be interested in the $99$-th percentile, since
    everything below that threshold might not be of particular interest or does not
    produce any actionable alarm. In this case, a model that focuses on a particular
    quantile of importance is of higher interest than a model that predicts the whole
    distribution with possibly worse accuracy on the selected quantile.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的一个重要方面是其预测性质，即它是生成概率预测还是点预测。这一选择高度依赖于基础应用。为了说明这一点，我们可以检查两个不同的预测用例：产品需求和CPU利用率。在前者中，人们希望预测产品的未来需求，以便对仓库中所需库存量做出更为明智的决策，或根据预计的流量优化劳动力计划。在后者中，CPU利用率的预测可以用来及时识别一个进程是否会失败，以便主动解决相关问题，或者检测可能的异常行为，从而触发根本原因分析和系统改进。尽管这两种应用都需要预测，但最终目标不同，这改变了所选择预测模型的要求。例如，对于产品需求，未来需求的整个分布可能是重要的：不能仅依赖于单一的预测值，因为预测中的方差在避免缺货问题或避免劳动力规划不足/过度中扮演了重要角色。因此，在这个应用中，使用一个专注于准确预测整个分布的模型是重要的。另一方面，对于CPU利用率，人们可能更关心第$99$百分位数，因为低于该阈值的内容可能不特别重要或不会产生任何可操作的警报。在这种情况下，专注于特定分位数的模型比预测整个分布但在选定分位数上的准确性可能较差的模型更为重要。
- en: It is observed [[156](#bib.bib156), [146](#bib.bib146), [155](#bib.bib155),
    [42](#bib.bib42), [106](#bib.bib106)] empirically that autoregressive models are
    superior in performance (in terms of forecast accuracy) compared to state space
    models, especially when the data is less noisy and the forecast horizon is not
    too long. This is not surprising given that the autoregressive models directly
    use past observations as input features and treat own predictions as lag inputs
    in the multi-step forecast setting. A general rule of thumb is that if one knows
    details such as the forecast horizon, the quantile to query or the exact goals
    of the forecasting problem in advance and these are unlikely to change, then a
    discriminative model is often a good default choice. Conversely, state space models
    proved to be robust when there are missing and/or noisy observations [[42](#bib.bib42)].
    Moreover, if the application-specific constraints can be incorporated in the latent
    state, then state space models usually perform better even in the low-data regimes [[146](#bib.bib146)].
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到[[156](#bib.bib156), [146](#bib.bib146), [155](#bib.bib155), [42](#bib.bib42),
    [106](#bib.bib106)]实证结果表明，自动回归模型在性能（即预测准确性）上优于状态空间模型，尤其是在数据噪声较小且预测范围不是很长时。这并不奇怪，因为自动回归模型直接使用过去的观察作为输入特征，并在多步预测设置中将自身的预测作为滞后输入。一个一般的经验法则是，如果提前知道预测范围、查询的分位数或预测问题的确切目标，并且这些目标不太可能改变，那么判别模型通常是一个好的默认选择。相反，当存在缺失和/或噪声观察时，状态空间模型证明是鲁棒的[[42](#bib.bib42)]。此外，如果可以将特定应用的约束纳入潜在状态，那么即使在数据较少的情况下，状态空间模型通常表现更好[[146](#bib.bib146)]。
- en: The length of the forecast horizon relative to the history or, more generally
    speaking, the importance of the historic values for future values must further
    be taken into account. For example, very long forecast horizons may require to
    control (e.g., via differential equations) the exponential growth in the target.
    A canonical example for this is forecasting of a pandemic. This example further
    clarifies the importance of being able to produce counterfactuals for what-if
    analysis (e.g., the incorporation of intervention). Not all deep forecasting models
    allow for this.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进一步考虑的是预测范围相对于历史数据的长度，或者更一般地说，历史值对未来值的重要性。例如，非常长的预测范围可能需要控制（例如，通过微分方程）目标中的指数增长。一个经典的例子是疫情预测。这个例子进一步阐明了能够生成反事实进行“假设分析”（例如，干预的纳入）的重要性，并非所有深度预测模型都允许这样做。
- en: 3.9.4 Other Aspects
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.9.4 其他方面
- en: A number of other aspects can further help to narrow the model exploration space.
    For example, computational constraints (how much time/money for training is available,
    are there constraints on the latency during inference) can favor “simpler” NNs,
    see e.g., [[23](#bib.bib23)] for a discussion on multi-objective forecasting model
    selection. Another aspect to consider could be CNN over RNN-based architectures.
    The skill set of the research team available is an important factor. For example,
    probabilistic models often are more sensitive towards parametrization and identifying
    reasonable parameter ranges requires in-depth knowledge. On the other extreme,
    troubleshooting Transformer-based models requires deep learning experience that
    not every research team may possess. The time budget available for model development
    and the willingness to extend existing models are further factors.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他方面可以进一步帮助缩小模型探索的范围。例如，计算约束（例如，训练所需的时间/资金是否有限，推断期间是否存在延迟约束）可以倾向于选择“更简单”的神经网络，详见[[23](#bib.bib23)]关于多目标预测模型选择的讨论。另一个需要考虑的方面可能是卷积神经网络（CNN）与递归神经网络（RNN）架构的对比。可用的研究团队的技能水平也是一个重要因素。例如，概率模型通常对参数化更为敏感，确定合理的参数范围需要深入的知识。另一方面，解决基于Transformer的模型问题需要深度学习经验，而不是所有研究团队都具备这种经验。模型开发的时间预算和愿意扩展现有模型的意愿也是进一步的因素。
- en: 4 Conclusions and Avenues for Future Work
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论与未来工作的方向
- en: This article has attempted to provide an introduction to and an overview of
    NNs for forecasting or deep forecasting. We began by providing a panorama of some
    of the core concepts in the modern literature on NNs chosen by their degree of
    relevance for forecasting. We then reviewed the literature on recent advances
    in deep forecasting models.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 本文尝试介绍和概述了用于预测或深度预测的神经网络。我们首先提供了现代文献中一些核心概念的全景视图，这些概念根据其与预测的相关程度进行选择。然后，我们回顾了深度预测模型的最新进展文献。
- en: Deep forecasting methods have received considerable attention in the literature
    because they excel at addressing forecasting problems with many related time series
    and at extracting weak signals and complex patterns from large amounts of data.
    From a practical perspective, the availability of efficient programming frameworks
    helps to alleviate many of the pain points that practitioners experience with
    other forecasting methods such as manual feature engineering or the need to derive
    gradients. However, NNs are not a silver bullet. For many important classes of
    forecasting problems such as long-range macro-economic forecasts or other problems
    requiring external domain knowledge not learnable from the data, deep forecasting
    methods are not the most appropriate choice and will likely never be. Still, it
    is our firm belief that NNs belong to the toolbox of every forecaster, in industry
    and academia.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 深度预测方法在文献中受到相当多的关注，因为它们在处理具有许多相关时间序列的预测问题和从大量数据中提取微弱信号和复杂模式方面表现优异。从实际角度来看，高效的编程框架的可用性有助于缓解从事其他预测方法的实践者所经历的许多痛点，例如手动特征工程或需要推导梯度。然而，神经网络并不是万能的。对于许多重要的预测问题类别，如长期宏观经济预测或需要从数据中无法学习的外部领域知识的其他问题，深度预测方法可能并不是最合适的选择，也可能永远不会。然而，我们坚信神经网络属于每个预测员的工具箱，无论是在工业界还是学术界。
- en: Building onto the existing promising work in NNs for forecasting, many challenges
    remain to be solved. We expect that the current trends of hybridizing existing
    time series techniques with NNs [[146](#bib.bib146), [169](#bib.bib169), [64](#bib.bib64),
    [180](#bib.bib180)] and bringing innovations from other related areas or general
    purpose techniques to forecasting [[70](#bib.bib70), [183](#bib.bib183), [184](#bib.bib184)]
    will continue organically. Typical general challenges for NNs, such as data effectiveness,
    are important in forecasting and likely need a special treatment (see [[59](#bib.bib59)]
    for an approach in time series classification with transfer learning). Other topics
    of general ML interest such as interpretability, explainability and causality
    (e.g., [[19](#bib.bib19), [122](#bib.bib122), [158](#bib.bib158)]) are of particular
    practical importance in the forecasting setting. It is our hope that original
    methods such as new NN architectures will be pioneered in the time series prediction
    sector (e.g., [[138](#bib.bib138)]) and that those will then feed back into the
    general NN literature to help solve problems in other disciplines.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有神经网络预测的有希望的工作基础上，许多挑战仍待解决。我们预计，将现有时间序列技术与神经网络[[146](#bib.bib146), [169](#bib.bib169),
    [64](#bib.bib64), [180](#bib.bib180)] 混合以及将其他相关领域或通用技术的创新带入预测[[70](#bib.bib70),
    [183](#bib.bib183), [184](#bib.bib184)] 的当前趋势将继续有机发展。神经网络的典型普遍挑战，如数据有效性，在预测中也很重要，可能需要特殊处理（参见[[59](#bib.bib59)]关于转移学习的时间序列分类方法）。其他如可解释性、解释性和因果关系等通用机器学习兴趣话题（例如[[19](#bib.bib19),
    [122](#bib.bib122), [158](#bib.bib158)]）在预测环境中尤为重要。我们希望，诸如新的神经网络架构等原创方法将在时间序列预测领域得到开创，并进而反馈到神经网络的通用文献中，帮助解决其他学科中的问题。
- en: Beyond such organic improvements, we speculate that another area in which NNs
    have had tremendous impact [[167](#bib.bib167), [168](#bib.bib168)] may become
    important for forecasting, namely deep reinforcement learning. In contrast to
    current practice, where forecasting merely serves as input to downstream decision
    problems (often mixed-integer nonlinear stochastic optimization problems), for
    example to address problems such as restocking decisions, reinforcement learning
    allows to directly learn optimal decisions in business context [[90](#bib.bib90)].
    It will be interesting to see whether reinforcement based approaches can improve
    decision making – and how good forecasting models could help improve reinforcement
    approaches.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些有机改进，我们推测神经网络（NNs）在另一个领域也产生了巨大的影响[[167](#bib.bib167), [168](#bib.bib168)]，即深度强化学习。与当前的实践不同，目前的预测仅作为下游决策问题（通常是混合整数非线性随机优化问题）的输入，例如解决补货决策问题，强化学习允许直接在商业环境中学习最佳决策[[90](#bib.bib90)]。有趣的是，强化学习方法是否能改善决策制定，以及优秀的预测模型如何帮助改进强化学习方法。
- en: As methodology advances, so will the applicability. Many potential applications
    of forecasting methods are under-explored. To pick areas that are close to the
    authors’ interests, in database management, cloud computing, and system operations
    a host of applications would greatly benefit from the use of principled forecasting
    methods (see e.g., [[21](#bib.bib21), [9](#bib.bib9), [60](#bib.bib60)]). Forecasting
    can also be used to improve core ML tasks such as hyperparameter optimization
    (e.g., [[47](#bib.bib47)]) and we expect more applications to open up in this
    area.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 随着方法学的进步，其适用性也会提高。许多预测方法的潜在应用尚未充分探索。为了选择与作者兴趣接近的领域，在数据库管理、云计算和系统操作中，大量应用将从使用有原则的预测方法中受益（参见，例如，[21](#bib.bib21),
    [9](#bib.bib9), [60](#bib.bib60)）。预测还可以用来改进核心机器学习任务，如超参数优化（例如，[47](#bib.bib47)），我们预计这一领域会有更多应用的出现。
- en: References
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abadi et al. [2016] Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,
    Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
    Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning.
    In *12th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    16)*, pages 265–283, 2016.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abadi 等 [2016] Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy
    Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,
    Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
    Yu, 和 Xiaoqiang Zheng. Tensorflow: 一个大规模机器学习系统。在*第12届USENIX操作系统设计与实现研讨会 (OSDI
    16)*上，第265–283页, 2016。'
- en: Ahmad et al. [2017] Subutai Ahmad, Alexander Lavin, Scott Purdy, and Zuha Agha.
    Unsupervised real-time anomaly detection for streaming data. *Neurocomputing*,
    262:134–147, 2017.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad 等 [2017] Subutai Ahmad, Alexander Lavin, Scott Purdy, 和 Zuha Agha. 无监督实时异常检测用于流数据。*神经计算*,
    262:134–147, 2017。
- en: Ahmed et al. [2012] Amr Ahmed, Moahmed Aly, Joseph Gonzalez, Shravan Narayanamurthy,
    and Alexander J Smola. Scalable inference in latent variable models. In *Proceedings
    of the fifth ACM International Conference on Web Search and Data Mining*, pages
    123–132\. ACM, 2012.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed 等 [2012] Amr Ahmed, Moahmed Aly, Joseph Gonzalez, Shravan Narayanamurthy,
    和 Alexander J Smola. 潜变量模型中的可扩展推断。在*第五届ACM国际网络搜索与数据挖掘会议*上，第123–132页。ACM, 2012。
- en: 'Alexandrov et al. [2020] Alexander Alexandrov, Konstantinos Benidis, Michael
    Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C
    Maddix, Syama Sundar Rangapuram, David Salinas, Jasper Schulz, et al. Gluonts:
    Probabilistic and neural time series modeling in python. *Journal of Machine Learning
    Research*, 21(116):1–6, 2020.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alexandrov 等 [2020] Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider,
    Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C Maddix, Syama Sundar
    Rangapuram, David Salinas, Jasper Schulz, 等。Gluonts: Python 中的概率性和神经时间序列建模。*机器学习研究杂志*,
    21(116):1–6, 2020。'
- en: Ansari et al. [2021] Abdul Fatir Ansari, Konstantinos Benidis, Richard Kurle,
    Ali Caner Turkmen, Harold Soh, Alexander J Smola, Bernie Wang, and Tim Januschowski.
    Deep explicit duration switching models for time series. *Advances in Neural Information
    Processing Systems*, 34, 2021.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansari 等 [2021] Abdul Fatir Ansari, Konstantinos Benidis, Richard Kurle, Ali
    Caner Turkmen, Harold Soh, Alexander J Smola, Bernie Wang, 和 Tim Januschowski.
    用于时间序列的深度显式持续切换模型。*神经信息处理系统进展*, 34, 2021。
- en: Asadi and Regan [2020] Reza Asadi and Amelia C Regan. A spatial-temporal decomposition
    based deep neural network for time series forecasting. *Applied Soft Computing*,
    87:105963, 2020.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asadi 和 Regan [2020] Reza Asadi 和 Amelia C Regan. 一种基于空间-时间分解的深度神经网络用于时间序列预测。*应用软计算*,
    87:105963, 2020。
- en: Athanasopoulos et al. [2009] George Athanasopoulos, Roman A Ahmed, and Rob J
    Hyndman. Hierarchical forecasts for australian domestic tourism. *International
    Journal of Forecasting*, 25(1):146–166, 2009.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athanasopoulos 等 [2009] George Athanasopoulos, Roman A Ahmed, 和 Rob J Hyndman.
    澳大利亚国内旅游的层次预测。*国际预测杂志*, 25(1):146–166, 2009。
- en: Athanasopoulos et al. [2017] George Athanasopoulos, Rob J Hyndman, Nikolaos
    Kourentzes, and Fotios Petropoulos. Forecasting with temporal hierarchies. *European
    Journal of Operational Research*, 262(1):60–74, 2017.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athanasopoulos 等 [2017] George Athanasopoulos, Rob J Hyndman, Nikolaos Kourentzes,
    和 Fotios Petropoulos. 利用时间层级进行预测。*欧洲运筹学杂志*, 262(1):60–74, 2017。
- en: 'Ayed et al. [2020] Fadhel Ayed, Lorenzo Stella, Tim Januschowski, and Jan Gasthaus.
    Anomaly detection at scale: The case for deep distributional time series models,
    2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ayed 等 [2020] Fadhel Ayed, Lorenzo Stella, Tim Januschowski, 和 Jan Gasthaus.
    大规模异常检测: 深度分布时间序列模型的案例，2020。'
- en: Bahdanau et al. [2014] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural
    machine translation by jointly learning to align and translate. *arXiv preprint
    arXiv:1409.0473*, 2014.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau et al. [2014] Dzmitry Bahdanau, Kyunghyun Cho 和 Yoshua Bengio. 通过联合学习对齐和翻译的神经机器翻译。*arXiv
    预印本 arXiv:1409.0473*，2014年。
- en: Bai et al. [2018] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical
    evaluation of generic convolutional and recurrent networks for sequence modeling.
    *arXiv preprint arXiv:1803.01271*, 2018.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2018] Shaojie Bai, J Zico Kolter 和 Vladlen Koltun. 对通用卷积和递归网络进行序列建模的实证评估。*arXiv
    预印本 arXiv:1803.01271*，2018年。
- en: 'Ballestra et al. [2019] Luca Vincenzo Ballestra, Andrea Guizzardi, and Fabio
    Palladini. Forecasting and trading on the vix futures market: A neural network
    approach based on open to close returns and coincident indicators. *International
    Journal of Forecasting*, 35(4):1250 – 1262, 2019. ISSN 0169-2070.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ballestra et al. [2019] Luca Vincenzo Ballestra, Andrea Guizzardi 和 Fabio Palladini.
    对 VIX 期货市场的预测与交易：一种基于开盘到收盘回报和一致性指标的神经网络方法。*国际预测期刊*，35(4):1250 – 1262, 2019。ISSN
    0169-2070。
- en: Bandara et al. [2017] Kasun Bandara, Christoph Bergmeir, and Slawek Smyl. Forecasting
    across time series databases using long short-term memory networks on groups of
    similar series. *arXiv preprint arXiv:1710.03222*, 8:805–815, 2017.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bandara et al. [2017] Kasun Bandara, Christoph Bergmeir 和 Slawek Smyl. 使用长短期记忆网络在类似系列的时间序列数据库之间进行预测。*arXiv
    预印本 arXiv:1710.03222*，8:805–815，2017年。
- en: Bandara et al. [2019] Kasun Bandara, Peibei Shi, Christoph Bergmeir, Hansika
    Hewamalage, Quoc Tran, and Brian Seaman. Sales demand forecast in e-commerce using
    a long short-term memory neural network methodology. In *International Conference
    on Neural Information Processing*, pages 462–474\. Springer, 2019.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bandara et al. [2019] Kasun Bandara, Peibei Shi, Christoph Bergmeir, Hansika
    Hewamalage, Quoc Tran 和 Brian Seaman. 使用长短期记忆神经网络方法进行电子商务销售需求预测。发表于 *神经信息处理国际会议*，第462–474页。Springer，2019年。
- en: 'Bandara et al. [2020] Kasun Bandara, Christoph Bergmeir, and Hansika Hewamalage.
    Lstm-msnet: Leveraging forecasts on sets of related time series with multiple
    seasonal patterns. *IEEE Transactions on Neural Networks and Learning Systems*,
    2020.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bandara et al. [2020] Kasun Bandara, Christoph Bergmeir 和 Hansika Hewamalage.
    LSTM-MSNet：利用具有多重季节性模式的相关时间序列集合进行预测。*IEEE 神经网络与学习系统汇刊*，2020年。
- en: Barber [2012] David Barber. *Bayesian reasoning and machine learning*. Cambridge
    University Press, 2012.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barber [2012] David Barber. *贝叶斯推理与机器学习*。剑桥大学出版社，2012年。
- en: Ben Taieb et al. [2017] Souhaib Ben Taieb, James W Taylor, and Rob J Hyndman.
    Coherent probabilistic forecasts for hierarchical time series. In *International
    Conference on Machine Learning*, pages 3348–3357, 2017.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben Taieb et al. [2017] Souhaib Ben Taieb, James W Taylor 和 Rob J Hyndman. 层次时间序列的连贯概率预测。发表于
    *国际机器学习会议*，第3348–3357页，2017年。
- en: 'Biloš et al. [2021] Marin Biloš, Johanna Sommer, Syama Sundar Rangapuram, Tim
    Januschowski, and Stephan Günnemann. Neural flows: Efficient alternative to neural
    odes. *Advances in Neural Information Processing Systems*, 34, 2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biloš et al. [2021] Marin Biloš, Johanna Sommer, Syama Sundar Rangapuram, Tim
    Januschowski 和 Stephan Günnemann. 神经流：神经 ODE 的高效替代方案。*神经信息处理系统进展*，34，2021年。
- en: Binder et al. [2016] Alexander Binder, Sebastian Bach, Gregoire Montavon, Klaus-Robert
    Müller, and Wojciech Samek. Layer-wise relevance propagation for deep neural network
    architectures. In *Information Science and Applications (ICISA)*, pages 913–922\.
    Springer, 2016.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Binder et al. [2016] Alexander Binder, Sebastian Bach, Gregoire Montavon, Klaus-Robert
    Müller 和 Wojciech Samek. 深度神经网络架构的层级相关传播。发表于 *信息科学与应用 (ICISA)*，第913–922页。Springer，2016年。
- en: 'Bischoff and Gross [2019] Toby Bischoff and Austin Gross. Wavenet & dropout:
    An efficient setup for competitive forecasts at scale. In *Proceedings of the
    International Symposium on Forecasting*, 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bischoff 和 Gross [2019] Toby Bischoff 和 Austin Gross. Wavenet 与 dropout：一种高效的规模化竞争预测设置。发表于
    *国际预测研讨会论文集*，2019年。
- en: Bohlke-Schneider et al. [2020] Michael Bohlke-Schneider, Shubham Kapoor, and
    Tim Januschowski. Resilient neural forecasting systems. In *Proceedings of the
    Fourth International Workshop on Data Management for End-to-End Machine Learning*,
    DEEM’20, New York, NY, USA, 2020\. Association for Computing Machinery. ISBN 9781450380232.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bohlke-Schneider et al. [2020] Michael Bohlke-Schneider, Shubham Kapoor 和 Tim
    Januschowski. 强韧的神经预测系统。发表于 *第四届端到端机器学习数据管理国际研讨会论文集*，DEEM’20，纽约，NY，美国，2020年。计算机协会。ISBN
    9781450380232。
- en: 'Bohlke-Schneider et al. [2022] Michael Bohlke-Schneider, Paul Jeha, Pedro Mercado,
    Shubham Kapoor, Jan Gasthaus, and Tim Januschowski. PSA-GAN: Progressive self
    attention gans for synthetic time series. *International Conference on Learning
    Representations (ICLR)*, 2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bohlke-Schneider 等 [2022] 迈克尔·博尔克-施奈德、保罗·杰哈、佩德罗·梅尔卡多、舒巴姆·卡普尔、扬·加斯陶斯 和 蒂姆·扬舒夫斯基。PSA-GAN:
    进阶自注意力生成对抗网络用于合成时间序列。*国际学习表征会议 (ICLR)*，2022年。'
- en: Borchert et al. [2022] Oliver Borchert, David Salinas, Valentin Flunkert, Tim
    Januschowski, and Stephan Günnemann. Multi-objective model selection for time
    series forecasting. *arXiv preprint arXiv:2202.08485*, 2022.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borchert 等 [2022] 奥利弗·博尔赫特、戴维·萨利纳斯、瓦伦丁·弗伦克特、蒂姆·扬舒夫斯基 和 斯特凡·君曼。时间序列预测的多目标模型选择。*arXiv
    预印本 arXiv:2202.08485*，2022年。
- en: Borovykh et al. [2017] Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee.
    Conditional time series forecasting with convolutional neural networks. *arXiv
    preprint arXiv:1703.04691*, 2017.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borovykh 等 [2017] 安娜斯塔西亚·博罗维赫、桑德·博赫特 和 科内利斯·W·奥斯特尔。使用卷积神经网络进行条件时间序列预测。*arXiv
    预印本 arXiv:1703.04691*，2017年。
- en: Böse et al. [2017] Joos-Hendrik Böse, Valentin Flunkert, Jan Gasthaus, Tim Januschowski,
    Dustin Lange, David Salinas, Sebastian Schelter, Matthias Seeger, and Yuyang Wang.
    Probabilistic demand forecasting at scale. *Proceedings of the VLDB Endowment*,
    10(12):1694–1705, 2017.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Böse 等 [2017] 乔斯-亨德里克·博泽、瓦伦丁·弗伦克特、扬·加斯陶斯、蒂姆·扬舒夫斯基、达斯汀·朗、戴维·萨利纳斯、塞巴斯蒂安·谢尔特、马蒂亚斯·西格
    和 于洋·王。大规模概率需求预测。*VLDB 资助会会议录*，10(12):1694–1705，2017年。
- en: Boser et al. [1992] Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik.
    A training algorithm for optimal margin classifiers. In *Proceedings of the 5th
    Annual Workshop on Computational Learning Theory*, COLT ’92, pages 144–152, New
    York, NY, USA, 1992\. ACM.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boser 等 [1992] 伯恩哈德·E·博泽、伊莎贝尔·M·古永 和 弗拉基米尔·N·瓦普尼克。最优间隔分类器的训练算法。发表于*第5届年度计算学习理论研讨会论文集*，COLT
    ’92，第144–152页，纽约，NY，美国，1992年。ACM。
- en: Brahim-Belhouari and Bermak [2004] Sofiane Brahim-Belhouari and Amine Bermak.
    Gaussian process for nonstationary time series prediction. *Computational Statistics
    & Data Analysis*, 47(4):705–712, 2004.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brahim-Belhouari 和 Bermak [2004] 索非安·布拉希姆-贝尔胡阿里 和 阿敏·伯马克。用于非平稳时间序列预测的高斯过程。*计算统计与数据分析*，47(4):705–712，2004年。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in Neural Information
    Processing Systems*, 33:1877–1901, 2020.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 [2020] 汤姆·布朗、本杰明·曼、尼克·赖德、梅拉妮·萨比亚、贾雷德·D·卡普兰、普拉弗拉·达里瓦尔、阿尔文·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔
    等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020年。
- en: Callot et al. [2019] Laurent Callot, Mehmet Caner, A Özlem Önder, and Esra Ulaşan.
    A nodewise regression approach to estimating large portfolios. *Journal of Business
    & Economic Statistics*, pages 1–12, 2019.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Callot 等 [2019] 洛朗·卡洛特、梅赫梅特·贾内尔、Özlem Önder 和 埃斯拉·乌拉尚。基于节点的回归方法估计大型投资组合。*商业与经济统计学杂志*，第1–12页，2019年。
- en: Callot et al. [2017] Laurent AF Callot, Anders B Kock, and Marcelo C Medeiros.
    Modeling and forecasting large realized covariance matrices and portfolio choice.
    *Journal of Applied Econometrics*, 32(1):140–158, 2017.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Callot 等 [2017] 洛朗·AF·卡洛特、安德斯·B·科克 和 马塞洛·C·梅德罗斯。建模和预测大型实现协方差矩阵及投资组合选择。*应用计量经济学杂志*，32(1):140–158，2017年。
- en: Chapados [2014] Nicolas Chapados. Effective bayesian modeling of groups of related
    count time series. In *International Conference on Machine Learning*, pages 1395–1403\.
    PMLR, 2014.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapados [2014] 尼古拉斯·查帕多斯。相关计数时间序列组的有效贝叶斯建模。发表于*国际机器学习会议*，第1395–1403页。PMLR，2014年。
- en: Chen et al. [2018] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K
    Duvenaud. Neural ordinary differential equations. *Advances in Neural Information
    Processing Systems*, 31, 2018.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2018] 瑞奇·TQ 陈、尤莉亚·鲁巴诺娃、杰西·贝滕考特 和 大卫·K 杜维诺。神经普通微分方程。*神经信息处理系统进展*，31，2018年。
- en: 'Chen and Guestrin [2016] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable
    tree boosting system. In *Proceedings of the 22nd ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*, KDD ’16, pages 785–794\. ACM, 2016.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 和 Guestrin [2016] 田启陈 和 卡洛斯·古斯特林。XGBoost: 一种可扩展的树提升系统。发表于*第22届 ACM SIGKDD
    国际知识发现与数据挖掘大会论文集*，KDD ’16，第785–794页。ACM，2016年。'
- en: 'Chen et al. [2015] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie
    Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible
    and efficient machine learning library for heterogeneous distributed systems.
    *NeurIPS Workshop on Machine Learning Systems*, 2015.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2015] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie
    Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, 和 Zheng Zhang. Mxnet：用于异构分布式系统的灵活高效的机器学习库。*NeurIPS
    Workshop on Machine Learning Systems*，2015年。
- en: Chen et al. [2020] Yitian Chen, Yanfei Kang, Yixiong Chen, and Zizhuo Wang.
    Probabilistic forecasting with temporal convolutional neural network. *Neurocomputing*,
    399:491–501, 2020.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2020] Yitian Chen, Yanfei Kang, Yixiong Chen, 和 Zizhuo Wang. 基于时间卷积神经网络的概率预测。*Neurocomputing*，399:491–501，2020年。
- en: 'Cho et al. [2014] KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and
    Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder
    approaches. *CoRR*, abs/1409.1259, 2014.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho et al. [2014] KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, 和 Yoshua
    Bengio. 神经机器翻译的属性：编码器-解码器方法。*CoRR*，abs/1409.1259，2014年。
- en: 'Chorowski et al. [2014] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and
    Yoshua Bengio. End-to-end continuous speech recognition using attention-based
    recurrent NN: First results. *arXiv preprint arXiv:1412.1602*, 2014.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chorowski et al. [2014] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, 和 Yoshua
    Bengio. 基于注意力的递归神经网络的端到端连续语音识别：初步结果。*arXiv preprint arXiv:1412.1602*，2014年。
- en: Chorowski et al. [2015] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,
    Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition.
    In *Advances in Neural Information Processing Systems*, pages 577–585, 2015.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chorowski et al. [2015] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,
    Kyunghyun Cho, 和 Yoshua Bengio. 基于注意力的语音识别模型。在*Advances in Neural Information
    Processing Systems*，第577–585页，2015年。
- en: Cinar et al. [2017] Yagmur Gizem Cinar, Hamid Mirisaee, Parantapa Goswami, Eric
    Gaussier, Ali Aït-Bachir, and Vadim Strijov. Position-based content attention
    for time series forecasting with sequence-to-sequence RNNs. In *International
    Conference on Neural Information Processing*, pages 533–544, 2017.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cinar et al. [2017] Yagmur Gizem Cinar, Hamid Mirisaee, Parantapa Goswami, Eric
    Gaussier, Ali Aït-Bachir, 和 Vadim Strijov. 基于位置的内容注意力用于序列到序列RNN的时间序列预测。在*International
    Conference on Neural Information Processing*，第533–544页，2017年。
- en: Crawley [2012] Michael J Crawley. Mixed-effects models. *The R Book, Second
    Edition*, pages 681–714, 2012.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crawley [2012] Michael J Crawley. 混合效应模型。*The R Book, Second Edition*，第681–714页，2012年。
- en: Croston [1972] J. D. Croston. Forecasting and stock control for intermittent
    demands. *Journal of the Operational Research Society*, 23(3):289–303, Sep 1972.
    ISSN 1476-9360.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croston [1972] J. D. Croston. 间歇性需求的预测与库存控制。*Journal of the Operational Research
    Society*，23(3):289–303，1972年9月。ISSN 1476-9360。
- en: de Bézenac et al. [2020] Emmanuel de Bézenac, Syama Sundar Rangapuram, Konstantinos
    Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson,
    Patrick Gallinari, and Tim Januschowski. Normalizing kalman filters for multivariate
    time series analysis. *Advances in Neural Information Processing Systems*, 33,
    2020.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Bézenac et al. [2020] Emmanuel de Bézenac, Syama Sundar Rangapuram, Konstantinos
    Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson,
    Patrick Gallinari, 和 Tim Januschowski. 用于多变量时间序列分析的归一化卡尔曼滤波器。*Advances in Neural
    Information Processing Systems*，33，2020年。
- en: Deng and Hooi [2021] Ailin Deng and Bryan Hooi. Graph neural network-based anomaly
    detection in multivariate time series. In *Proceedings of the 35th AAAI Conference
    on Artificial Intelligence, Vancouver, BC, Canada*, pages 2–9, 2021.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng and Hooi [2021] Ailin Deng 和 Bryan Hooi. 基于图神经网络的多变量时间序列异常检测。在*Proceedings
    of the 35th AAAI Conference on Artificial Intelligence, Vancouver, BC, Canada*，第2–9页，2021年。
- en: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*, pages 4171–4186, Minneapolis, Minnesota, June
    2019\. Association for Computational Linguistics.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    BERT：用于语言理解的深度双向变换器预训练。在*Proceedings of the 2019 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*，第4171–4186页，明尼阿波利斯，明尼苏达州，2019年6月。计算语言学协会。'
- en: Dimoulkas et al. [2019] I. Dimoulkas, P. Mazidi, and L. Herre. Neural networks
    for GEFCom2017 probabilistic load forecasting. *International Journal of Forecasting*,
    35(4):1409 – 1423, 2019.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dimoulkas et al. [2019] I. Dimoulkas, P. Mazidi, 和 L. Herre. 用于GEFCom2017概率负荷预测的神经网络。*International
    Journal of Forecasting*，35(4):1409 – 1423，2019年。
- en: Dinh et al. [2017] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density
    estimation using real NVP. In *5th International Conference on Learning Representations,
    ICLR 2017*, 2017.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinh et al. [2017] Laurent Dinh、Jascha Sohl-Dickstein 和 Samy Bengio。使用真实NVP的密度估计。在*第5届国际学习表征会议，ICLR
    2017*，2017年。
- en: Domhan et al. [2015] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter.
    Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation
    of learning curves. In *Proceedings of the 24th International Conference on Artificial
    Intelligence*, IJCAI’15, pages 3460–3468\. AAAI Press, 2015. ISBN 978-1-57735-738-4.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Domhan et al. [2015] Tobias Domhan、Jost Tobias Springenberg 和 Frank Hutter。通过学习曲线外推加速深度神经网络的自动超参数优化。在*第24届国际人工智能会议论文集*，IJCAI’15，第3460–3468页。AAAI出版社，2015年。ISBN
    978-1-57735-738-4。
- en: 'Du et al. [2016] Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel
    Gomez-Rodriguez, and Le Song. Recurrent marked temporal point processes: Embedding
    event history to vector. In *Proceedings of the 22nd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining*, pages 1555–1564\. ACM, 2016.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. [2016] Nan Du、Hanjun Dai、Rakshit Trivedi、Utkarsh Upadhyay、Manuel Gomez-Rodriguez
    和 Le Song。递归标记时间点过程：将事件历史嵌入向量。在*第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，第1555–1564页。ACM，2016年。
- en: Durbin and Koopman [2012] James Durbin and Siem Jan Koopman. *Time series analysis
    by state space methods*, volume 38. Oxford University Press, 2012.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durbin and Koopman [2012] James Durbin 和 Siem Jan Koopman。*状态空间方法的时间序列分析*，第38卷。牛津大学出版社，2012年。
- en: Ehrlich et al. [2021] Elena Ehrlich, Laurent Callot, and François-Xavier Aubet.
    Spliced binned-pareto distribution for robust modeling of heavy-tailed time series.
    *arXiv preprint arXiv:2106.10952*, 2021.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ehrlich et al. [2021] Elena Ehrlich、Laurent Callot 和 François-Xavier Aubet。用于稳健建模重尾时间序列的拼接型箱型帕累托分布。*arXiv
    预印本 arXiv:2106.10952*，2021年。
- en: 'Eisenach et al. [2020] Carson Eisenach, Yagna Patel, and Dhruv Madeka. Mqtransformer:
    Multi-horizon forecasts with context dependent and feedback-aware attention. *arXiv
    preprint arXiv:2009.14799*, 2020.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eisenach et al. [2020] Carson Eisenach、Yagna Patel 和 Dhruv Madeka。Mqtransformer：具有上下文依赖和反馈感知注意力的多视角预测。*arXiv
    预印本 arXiv:2009.14799*，2020年。
- en: 'Engel et al. [2018] Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani,
    Chris Donahue, and Adam Roberts. GANSynth: Adversarial neural audio synthesis.
    In *International Conference on Learning Representations*, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engel et al. [2018] Jesse Engel、Kumar Krishna Agrawal、Shuo Chen、Ishaan Gulrajani、Chris
    Donahue 和 Adam Roberts。GANSynth：对抗性神经音频合成。在*国际学习表征会议*，2018年。
- en: Esteban et al. [2017] Cristóbal Esteban, Stephanie L Hyland, and Gunnar Rätsch.
    Real-valued (medical) time series generation with recurrent conditional gans.
    *arXiv preprint arXiv:1706.02633*, 2017.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esteban et al. [2017] Cristóbal Esteban、Stephanie L Hyland 和 Gunnar Rätsch。具有递归条件GAN的实值（医学）时间序列生成。*arXiv
    预印本 arXiv:1706.02633*，2017年。
- en: 'et al. [2020] Fotios Petropoulos et al. Forecasting: theory and practice. *International
    Journal of Forecasting*, 2020.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: et al. [2020] Fotios Petropoulos 等人。预测：理论与实践。*国际预测学杂志*，2020年。
- en: 'Faloutsos et al. [2018] Christos Faloutsos, Jan Gasthaus, Tim Januschowski,
    and Yuyang Wang. Forecasting big time series: old and new. *Proceedings of the
    VLDB Endowment*, 11(12):2102–2105, 2018.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faloutsos et al. [2018] Christos Faloutsos、Jan Gasthaus、Tim Januschowski 和 Yuyang
    Wang。大时间序列预测：旧与新。*VLDB基金会会议录*，11(12)：2102–2105，2018年。
- en: 'Faloutsos et al. [2019a] Christos Faloutsos, Valentin Flunkert, Jan Gasthaus,
    Tim Januschowski, and Yuyang Wang. Forecasting big time series: Theory and practice.
    In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019.*, 2019a.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faloutsos et al. [2019a] Christos Faloutsos、Valentin Flunkert、Jan Gasthaus、Tim
    Januschowski 和 Yuyang Wang。大时间序列预测：理论与实践。在*第25届ACM SIGKDD国际知识发现与数据挖掘会议论文集，KDD
    2019，美国安克雷奇，2019年8月4-8日*，2019a。
- en: Faloutsos et al. [2019b] Christos Faloutsos, Jan Gasthaus, Tim Januschowski,
    and Yuyang Wang. Classical and contemporary approaches to big time series forecasting.
    In *Proceedings of the 2019 International Conference on Management of Data*, SIGMOD
    ’19, New York, NY, USA, 2019b. ACM.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faloutsos et al. [2019b] Christos Faloutsos、Jan Gasthaus、Tim Januschowski 和
    Yuyang Wang。大时间序列预测的经典与现代方法。在*2019年数据管理国际会议论文集*，SIGMOD ’19，美国纽约，2019b。ACM。
- en: 'Faloutsos et al. [2020] Christos Faloutsos, Valentin Flunkert, Jan Gasthaus,
    Tim Januschowski, and Yuyang Wang. Forecasting big time series: Theory and practice.
    In *Companion Proceedings of the Web Conference 2020*, WWW ’20, pages 320–321\.
    Association for Computing Machinery, 2020.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faloutsos 等人 [2020] Christos Faloutsos, Valentin Flunkert, Jan Gasthaus, Tim
    Januschowski, 和 Yuyang Wang. 大时间序列预测：理论与实践。在 *Web 会议 2020 附录*，WWW ’20，第 320–321
    页。计算机协会，2020 年。
- en: Fawaz et al. [2018] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber,
    Lhassane Idoumghar, and Pierre-Alain Muller. Transfer learning for time series
    classification. In *IEEE International Conference on Big Data, Big Data 2018,
    Seattle, WA, USA, December 10-13, 2018*, pages 1367–1376, 2018.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fawaz 等人 [2018] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane
    Idoumghar, 和 Pierre-Alain Muller. 时间序列分类的迁移学习。在 *IEEE 国际大数据会议，大数据 2018，华盛顿州西雅图，美国，2018
    年 12 月 10-13 日*，第 1367–1376 页，2018 年。
- en: Flunkert et al. [2020] Valentin Flunkert, Quentin Rebjock, Joel Castellon, Laurent
    Callot, and Tim Januschowski. A simple and effective predictive resource scaling
    heuristic for large-scale cloud applications. *arXiv preprint arXiv:2008.01215*,
    2020.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flunkert 等人 [2020] Valentin Flunkert, Quentin Rebjock, Joel Castellon, Laurent
    Callot, 和 Tim Januschowski. 一种简单有效的大规模云应用预测资源扩展启发式方法。*arXiv 预印本 arXiv:2008.01215*，2020
    年。
- en: Fraccaro et al. [2017] Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole
    Winther. A disentangled recognition and nonlinear dynamics model for unsupervised
    learning. *Advances in Neural Information Processing Systems*, 30, 2017.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fraccaro 等人 [2017] Marco Fraccaro, Simon Kamronn, Ulrich Paquet, 和 Ole Winther.
    一种用于无监督学习的解耦识别与非线性动态模型。*神经信息处理系统进展*，30，2017 年。
- en: Franceschi et al. [2019] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin
    Jaggi. Unsupervised scalable representation learning for multivariate time series.
    *Advances in Neural Information Processing Systems*, 32, 2019.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Franceschi 等人 [2019] Jean-Yves Franceschi, Aymeric Dieuleveut, 和 Martin Jaggi.
    用于多变量时间序列的无监督可扩展表示学习。*神经信息处理系统进展*，32，2019 年。
- en: Garcia Satorras et al. [2022] Victor Garcia Satorras, Syama Sundar Rangapuram,
    and Tim Januschowski. Multivariate time series forecasting with latent graph inference.
    *arXiv preprint*, 2022.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia Satorras 等人 [2022] Victor Garcia Satorras, Syama Sundar Rangapuram, 和
    Tim Januschowski. 带有潜在图推断的多变量时间序列预测。*arXiv 预印本*，2022 年。
- en: Gasthaus et al. [2019] Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar
    Rangapuram, David Salinas, Valentin Flunkert, and Tim Januschowski. Probabilistic
    forecasting with spline quantile function RNNs. In *The 22nd International Conference
    on Artificial Intelligence and Statistics*, pages 1901–1910, 2019.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gasthaus 等人 [2019] Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar
    Rangapuram, David Salinas, Valentin Flunkert, 和 Tim Januschowski. 使用样条分位函数 RNN
    的概率预测。在 *第 22 届人工智能与统计国际会议*，第 1901–1910 页，2019 年。
- en: Gelman et al. [2013] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson,
    Aki Vehtari, and Donald B Rubin. *Bayesian data analysis*. CRC press, 2013.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gelman 等人 [2013] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson,
    Aki Vehtari, 和 Donald B Rubin. *贝叶斯数据分析*。CRC 出版社，2013 年。
- en: Geweke [1977] John Geweke. The dynamic factor analysis of economic time series.
    *Latent variables in socio-economic models*, 1977.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geweke [1977] John Geweke. 经济时间序列的动态因子分析。*社会经济模型中的潜在变量*，1977 年。
- en: Girard et al. [2003] Agathe Girard, Carl Edward Rasmussen, Joaquin Quinonero
    Candela, and Roderick Murray-Smith. Gaussian process priors with uncertain inputs
    application to multiple-step ahead time series forecasting. In *Advances in Neural
    Information Processing Systems*, pages 545–552, 2003.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girard 等人 [2003] Agathe Girard, Carl Edward Rasmussen, Joaquin Quinonero Candela,
    和 Roderick Murray-Smith. 带有不确定输入的高斯过程先验应用于多步预测时间序列。在 *神经信息处理系统进展*，第 545–552 页，2003
    年。
- en: Glorot et al. [2011] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep
    sparse rectifier neural networks. In *Proceedings of the 14th International Conference
    on Artificial Intelligence and Statistics*, pages 315–323, 2011.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glorot 等人 [2011] Xavier Glorot, Antoine Bordes, 和 Yoshua Bengio. 深度稀疏整流神经网络。在
    *第 14 届人工智能与统计国际会议论文集*，第 315–323 页，2011 年。
- en: 'Gneiting et al. [2007] Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery.
    Probabilistic forecasts, calibration and sharpness. *Journal of the Royal Statistical
    Society: Series B (Statistical Methodology)*, 69(2):243–268, 2007.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gneiting 等人 [2007] Tilmann Gneiting, Fadoua Balabdaoui, 和 Adrian E Raftery.
    概率预测、校准与准确性。*皇家统计学会杂志：B 系列（统计方法）*，69(2):243–268，2007 年。
- en: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *Advances in Neural Information Processing Systems*, 27, 2014.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人 [2014] Ian Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David
    Warde-Farley、Sherjil Ozair、Aaron Courville 和 Yoshua Bengio. 生成对抗网络. *神经信息处理系统进展*,
    27, 2014.
- en: Goodfellow et al. [2016] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
    *Deep Learning*. MIT Press, 2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人 [2016] Ian Goodfellow、Yoshua Bengio 和 Aaron Courville. *深度学习*.
    MIT Press, 2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).
- en: Gouttes et al. [2021] Adèle Gouttes, Kashif Rasul, Mateusz Koren, Johannes Stephan,
    and Tofigh Naghibi. Probabilistic time series forecasting with implicit quantile
    networks. *arXiv preprint arXiv:2107.03743*, 2021.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gouttes 等人 [2021] Adèle Gouttes、Kashif Rasul、Mateusz Koren、Johannes Stephan
    和 Tofigh Naghibi. 隐式分位网络的概率时间序列预测. *arXiv 预印本 arXiv:2107.03743*, 2021.
- en: Gutierrez et al. [2008] Rafael S. Gutierrez, Adriano O. Solis, and Somnath Mukhopadhyay.
    Lumpy demand forecasting using neural networks. *International Journal of Production
    Economics*, 111(2):409–420, February 2008.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gutierrez 等人 [2008] Rafael S. Gutierrez、Adriano O. Solis 和 Somnath Mukhopadhyay.
    使用神经网络的波动需求预测. *国际生产经济学期刊*, 111(2):409–420, 2008年2月.
- en: 'Hasson et al. [2021] Hilaf Hasson, Bernie Wang, Tim Januschowski, and Jan Gasthaus.
    Probabilistic forecasting: A level-set approach. In *Advances in Neural Information
    Processing Systems*. Curran Associates, Inc., 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasson 等人 [2021] Hilaf Hasson、Bernie Wang、Tim Januschowski 和 Jan Gasthaus. 概率预测：一种水平集方法.
    载于 *神经信息处理系统进展*. Curran Associates, Inc., 2021.
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pages 770–778, 2016.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2016] Kaiming He、Xiangyu Zhang、Shaoqing Ren 和 Jian Sun. 用于图像识别的深度残差学习.
    载于 *IEEE计算机视觉与模式识别会议论文集*, 页 770–778, 2016.
- en: 'Hewamalage et al. [2021] Hansika Hewamalage, Christoph Bergmeir, and Kasun
    Bandara. Recurrent neural networks for time series forecasting: Current status
    and future directions. *International Journal of Forecasting*, 37(1):388–427,
    2021.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hewamalage 等人 [2021] Hansika Hewamalage、Christoph Bergmeir 和 Kasun Bandara.
    时间序列预测的递归神经网络：现状与未来方向. *国际预测期刊*, 37(1):388–427, 2021.
- en: Hinton [2002] Geoffrey E. Hinton. Training Products of Experts by Minimizing
    Contrastive Divergence. *Neural Computation*, 14(8):1771–1800, 08 2002.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton [2002] Geoffrey E. Hinton. 通过最小化对比散度训练专家产品. *神经计算*, 14(8):1771–1800,
    2002年8月.
- en: Hinton et al. [2006] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A
    fast learning algorithm for deep belief nets. *Neural Computation*, 18(7):1527–1554,
    2006.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等人 [2006] Geoffrey E Hinton、Simon Osindero 和 Yee-Whye Teh. 一种快速学习算法用于深度信念网络.
    *神经计算*, 18(7):1527–1554, 2006.
- en: Ho [1995] Tin Kam Ho. Random decision forests. In *Proceedings of 3rd International
    Conference on Document Analysis and Recognition*, volume 1, pages 278–282\. IEEE,
    1995.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho [1995] Tin Kam Ho. 随机决策森林. 载于 *第三届国际文档分析与识别会议论文集*, 第1卷, 页 278–282. IEEE,
    1995.
- en: Hochreiter [1998] Sepp Hochreiter. The vanishing gradient problem during learning
    recurrent neural nets and problem solutions. *International Journal of Uncertainty,
    Fuzziness and Knowledge-Based Systems*, 6(02):107–116, 1998.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter [1998] Sepp Hochreiter. 在学习递归神经网络过程中消失梯度问题及其解决方案. *不确定性、模糊性与知识系统国际期刊*,
    6(02):107–116, 1998.
- en: Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural Computation*, 9(8):1735–1780, 1997.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997] Sepp Hochreiter 和 Jürgen Schmidhuber. 长短期记忆.
    *神经计算*, 9(8):1735–1780, 1997.
- en: Hsu [2017] Daniel Hsu. Time series forecasting based on augmented long short-term
    memory. *arXiv preprint arXiv:1707.00666*, 2017.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsu [2017] Daniel Hsu. 基于增强长短期记忆的时间序列预测. *arXiv 预印本 arXiv:1707.00666*, 2017.
- en: Hu and Root [1964] M. J. C. Hu and Halbert E. Root. An adapative data processing
    system for weather forecasting. *Journal of Applied Metereology*, 1964.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 和 Root [1964] M. J. C. Hu 和 Halbert E. Root. 用于天气预测的自适应数据处理系统. *应用气象学期刊*,
    1964.
- en: 'Hyndman and Athanasopoulos [2018] Rob J Hyndman and George Athanasopoulos.
    *Forecasting: principles and practice*. OTexts, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyndman 和 Athanasopoulos [2018] Rob J Hyndman 和 George Athanasopoulos. *预测：原理与实践*.
    OTexts, 2018.
- en: Hyndman and Koehler [2006] Rob J Hyndman and Anne B Koehler. Another look at
    measures of forecast accuracy. *International Journal of Forecasting*, pages 679–688,
    2006.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyndman 和 Koehler [2006] Rob J Hyndman 和 Anne B Koehler. 另一种预测准确性度量方法. *国际预测期刊*,
    页 679–688, 2006.
- en: 'Hyndman et al. [2008] Rob J Hyndman, Anne B Koehler, J Keith Ord, and Ralph D
    Snyder. *Forecasting with Exponential Smoothing: the State Space Approach*. Springer,
    2008.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyndman et al. [2008] Rob J Hyndman, Anne B Koehler, J Keith Ord, 和 Ralph D
    Snyder. *使用指数平滑的预测：状态空间方法*。Springer，2008年。
- en: Hyndman et al. [2015] Rob J Hyndman, Earo Wang, and Nikolay Laptev. Large-scale
    unusual time series detection. In *IEEE International Conference on Data Mining
    Workshop (ICDMW)*, pages 1616–1619, 2015.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyndman et al. [2015] Rob J Hyndman, Earo Wang, 和 Nikolay Laptev. 大规模异常时间序列检测。在*IEEE国际数据挖掘会议研讨会（ICDMW）*，第1616–1619页，2015年。
- en: Janke et al. [2021] Tim Janke, Mohamed Ghanmi, and Florian Steinke. Implicit
    generative copulas. *Advances in Neural Information Processing Systems*, 34, 2021.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janke et al. [2021] Tim Janke, Mohamed Ghanmi, 和 Florian Steinke. 隐式生成型协方差函数。*神经信息处理系统进展*，34，2021年。
- en: 'Januschowski and Kolassa [2019] Tim Januschowski and Stephan Kolassa. A classification
    of business forecasting problems. *Foresight: The International Journal of Applied
    Forecasting*, 52:36–43, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Januschowski and Kolassa [2019] Tim Januschowski 和 Stephan Kolassa. 商业预测问题分类。*前瞻：国际应用预测期刊*，52:36–43，2019年。
- en: 'Januschowski et al. [2018] Tim Januschowski, Jan Gasthaus, Yuyang Wang, Syama Sundar
    Rangapuram, and Laurent Callot. Deep learning for forecasting: Current trends
    and challenges. *Foresight: The International Journal of Applied Forecasting*,
    51:42–47, 2018.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Januschowski et al. [2018] Tim Januschowski, Jan Gasthaus, Yuyang Wang, Syama
    Sundar Rangapuram, 和 Laurent Callot. 深度学习在预测中的应用：当前趋势与挑战。*前瞻：国际应用预测期刊*，51:42–47，2018年。
- en: Januschowski et al. [2019] Tim Januschowski, Jan Gasthaus, Yuyang Wang, David
    Salinas, Valentin Flunkert, Michael Bohlke-Schneider, and Laurent Callot. Criteria
    for classifying forecasting methods. *International Journal of Forecasting*, 2019.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Januschowski et al. [2019] Tim Januschowski, Jan Gasthaus, Yuyang Wang, David
    Salinas, Valentin Flunkert, Michael Bohlke-Schneider, 和 Laurent Callot. 预测方法分类标准。*国际预测期刊*，2019年。
- en: Januschowski et al. [2021] Tim Januschowski, Yuyang Wang, Hilaf Hasson, Timo
    Erkkila, Kari Torkkila, and Jan Gasthaus. Forecasting with trees. *International
    Journal of Forecasting*, 2021.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Januschowski et al. [2021] Tim Januschowski, Yuyang Wang, Hilaf Hasson, Timo
    Erkkila, Kari Torkkila, 和 Jan Gasthaus. 使用树进行预测。*国际预测期刊*，2021年。
- en: Jeon and Seong [2021] Yunho Jeon and Sihyeon Seong. Robust recurrent network
    model for intermittent time-series forecasting. *International Journal of Forecasting*,
    2021.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeon and Seong [2021] Yunho Jeon 和 Sihyeon Seong. 针对间歇性时间序列预测的鲁棒递归网络模型。*国际预测期刊*，2021年。
- en: 'Jordan [1986] Michael I. Jordan. Serial order: A parallel, distributed processing
    approach. Technical report, Institute for Cognitive Science, University of California,
    San Diego, 1986.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jordan [1986] Michael I. Jordan. 序列顺序：一种并行分布式处理方法。技术报告，加州大学圣地亚哥分校认知科学研究所，1986年。
- en: 'Jordan [1989] Michael I. Jordan. Serial order: A parallel, distributed processing
    approach. In *Advances in Connectionist Theory: Speech*. Erlbaum, 1989.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jordan [1989] Michael I. Jordan. 序列顺序：一种并行分布式处理方法。在*连接主义理论进展：语音*。Erlbaum，1989年。
- en: Kan et al. [2022] Kelvin Kan, François-Xavier Aubet, Tim Januschowski, Youngsuk
    Park, Konstantinos Benidis, Lars Ruthotto, and Jan Gasthaus. Multivariate quantile
    function forecaster. In *The 25th International Conference on Artificial Intelligence
    and Statistics*, 2022.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kan et al. [2022] Kelvin Kan, François-Xavier Aubet, Tim Januschowski, Youngsuk
    Park, Konstantinos Benidis, Lars Ruthotto, 和 Jan Gasthaus. 多变量分位数函数预测器。在*第25届国际人工智能与统计学会议*，2022年。
- en: Karras et al. [2017] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
    Progressive growing of gans for improved quality, stability, and variation. *arXiv
    preprint arXiv:1710.10196*, 2017.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras et al. [2017] Tero Karras, Timo Aila, Samuli Laine, 和 Jaakko Lehtinen.
    生成对抗网络的渐进式增长以提高质量、稳定性和多样性。*arXiv预印本 arXiv:1710.10196*，2017年。
- en: Karras et al. [2019] Tero Karras, Samuli Laine, and Timo Aila. A style-based
    generator architecture for generative adversarial networks. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June
    2019.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras et al. [2019] Tero Karras, Samuli Laine, 和 Timo Aila. 一种基于风格的生成对抗网络生成器架构。在*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2019年6月。
- en: 'Ke et al. [2017] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen,
    Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: a highly efficient gradient boosting
    decision tree. *Advances in Neural Information Processing Systems*, 30, 2017.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke et al. [2017] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen,
    Weidong Ma, Qiwei Ye, 和 Tie-Yan Liu. LightGBM：一种高效的梯度提升决策树。*神经信息处理系统进展*，30，2017年。
- en: Khashei and Bijari [2011] Mehdi Khashei and Mehdi Bijari. A novel hybridization
    of artificial neural networks and ARIMA models for time series forecasting. *Applied
    Soft Computing*, 11(2):2664–2675, 2011.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khashei 和 Bijari [2011] Mehdi Khashei 和 Mehdi Bijari. 用于时间序列预测的人工神经网络和ARIMA模型的新型混合。*应用软计算*，11(2):2664–2675，2011年。
- en: 'Kingma and Dhariwal [2018] Diederik P. Kingma and Prafulla Dhariwal. Glow:
    Generative flow with invertible 1x1 convolutions. In *Advances in Neural Information
    Processing Systems 31: Annual Conference on Neural Information Processing Systems
    2018*, pages 10236–10245, 2018.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma 和 Dhariwal [2018] Diederik P. Kingma 和 Prafulla Dhariwal. Glow：具有可逆
    1x1 卷积的生成流。在*神经信息处理系统进展 31: 神经信息处理系统年度会议 2018*，页码 10236–10245，2018年。'
- en: Kipf et al. [2018] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling,
    and Richard Zemel. Neural relational inference for interacting systems. In *International
    Conference on Machine Learning*, pages 2688–2697\. PMLR, 2018.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf et al. [2018] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling,
    和 Richard Zemel. 用于交互系统的神经关系推断。在*国际机器学习大会*，页码 2688–2697。PMLR，2018年。
- en: Koenker [2005] Roger Koenker. *Quantile Regression*. Econometric Society Monographs.
    Cambridge University Press, 2005.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koenker [2005] Roger Koenker. *分位数回归*。计量经济学学会专著。剑桥大学出版社，2005年。
- en: Kolassa [2020] Stephan Kolassa. Why the “best” point forecast depends on the
    error or accuracy measure. *International Journal of Forecasting*, 36(1):208–211,
    2020.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolassa [2020] Stephan Kolassa. 为什么“最佳”点预测取决于误差或准确性度量。*国际预测学杂志*，36(1):208–211，2020年。
- en: Kourentzes [2013] Nikolaos Kourentzes. Intermittent demand forecasts with neural
    networks. *International Journal of Production Economics*, 143(1):198–206, 2013.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kourentzes [2013] Nikolaos Kourentzes. 用神经网络进行间歇性需求预测。*国际生产经济学杂志*，143(1):198–206，2013年。
- en: Kurle et al. [2020] Richard Kurle, Syama Sundar Rangapuram, Emmanuel de Bézenac,
    Stephan Günnemann, and Jan Gasthaus. Deep rao-blackwellised particle filters for
    time series forecasting. *Advances in Neural Information Processing Systems*,
    33, 2020.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurle et al. [2020] Richard Kurle, Syama Sundar Rangapuram, Emmanuel de Bézenac,
    Stephan Günnemann, 和 Jan Gasthaus. 用于时间序列预测的深度拉奥-布莱克维尔粒子滤波器。*神经信息处理系统进展*，33，2020年。
- en: Lai et al. [2018] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
    Modeling long- and short-term temporal patterns with deep neural networks. In
    *The 41st International ACM SIGIR Conference on Research & Development in Information
    Retrieval*, pages 95–104\. ACM, 2018.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai et al. [2018] Guokun Lai, Wei-Cheng Chang, Yiming Yang, 和 Hanxiao Liu. 使用深度神经网络建模长期和短期时间模式。在*第41届国际ACM
    SIGIR信息检索研究与发展会议*，页码 95–104。ACM，2018年。
- en: Laio and Tamea [2007] F. Laio and S. Tamea. Verification tools for probabilistic
    forecasts of continuous hydrological variables. *Hydrology and Earth System Sciences*,
    11(4):1267–1277, 2007.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laio 和 Tamea [2007] F. Laio 和 S. Tamea. 连续水文变量概率预测的验证工具。*水文学与地球系统科学*，11(4):1267–1277，2007年。
- en: 'Lamb et al. [2016] Alex M Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang,
    Aaron C Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training
    recurrent networks. *Advances in Neural Information Processing Systems*, 29, 2016.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lamb et al. [2016] Alex M Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron
    C Courville, 和 Yoshua Bengio. 教授强制：一种用于训练递归网络的新算法。*神经信息处理系统进展*，29，2016年。
- en: Längkvist et al. [2014] Martin Längkvist, Lars Karlsson, and Amy Loutfi. A review
    of unsupervised feature learning and deep learning for time-series modeling. *Pattern
    Recognition Letters*, 42:11–24, 2014.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Längkvist et al. [2014] Martin Längkvist, Lars Karlsson, 和 Amy Loutfi. 对时间序列建模的无监督特征学习和深度学习的综述。*模式识别快报*，42:11–24，2014年。
- en: Laptev et al. [2017] Nikolay Laptev, Jason Yosinsk, Li Li Erran, and Slawek
    Smyl. Time-series extreme event forecasting with neural networks at Uber. In *ICML
    Time Series Workshop*. 2017.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laptev et al. [2017] Nikolay Laptev, Jason Yosinsk, Li Li Erran, 和 Slawek Smyl.
    Uber中基于神经网络的时间序列极端事件预测。在*ICML 时间序列研讨会*，2017年。
- en: LeCun [1989] Yann LeCun. Generalization and network design strategies. In *Connectionism
    in perspective*, 1989.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun [1989] Yann LeCun. 泛化与网络设计策略。在*连接主义视角*，1989年。
- en: LeCun and Bengio [1995] Yann LeCun and Yoshua Bengio. Convolutional networks
    for images, speech, and time series. *The handbook of brain theory and neural
    networks*, 3361(10), 1995.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 和 Bengio [1995] Yann LeCun 和 Yoshua Bengio. 图像、语音和时间序列的卷积网络。*大脑理论与神经网络手册*，3361(10)，1995年。
- en: LeCun et al. [1995] Yann LeCun, L.D. Jackel, Leon Bottou, A. Brunot, Corinna
    Cortes, J. S. Denker, Harris Drucker, I. Guyon, U.A. Muller, Eduard Sackinger,
    Patrice Simard, and V. Vapnik. Comparison of learning algorithms for handwritten
    digit recognition. In *International Conference on Artificial Neural Networks*,
    volume 60, pages 53–60\. Perth, Australia, 1995.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 [1995] Yann LeCun, L.D. Jackel, Leon Bottou, A. Brunot, Corinna Cortes,
    J. S. Denker, Harris Drucker, I. Guyon, U.A. Muller, Eduard Sackinger, Patrice
    Simard 和 V. Vapnik. 手写数字识别学习算法的比较。见 *国际人工神经网络会议*，第 60 卷，第 53–60 页。澳大利亚珀斯，1995
    年。
- en: LeCun et al. [2006] Yann LeCun, Sumit Chopra, Raia Hadsell, Fu Jie Huang, and
    et al. A tutorial on energy-based learning. In *Predicitng Structured Data*. MIT
    Press, 2006.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 [2006] Yann LeCun, Sumit Chopra, Raia Hadsell, Fu Jie Huang 等人. 关于能量基础学习的教程。见
    *预测结构化数据*。MIT Press，2006 年。
- en: Li et al. [2019a] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen,
    Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory
    bottleneck of transformer on time series forecasting. *Advances in Neural Information
    Processing Systems*, 32, 2019a.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2019a] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang
    Wang 和 Xifeng Yan. 提升局部性并突破变换器在时间序列预测中的记忆瓶颈。*神经信息处理系统进展*，第 32 卷，2019 年。
- en: 'Li et al. [2019b] Xuerong Li, Wei Shang, and Shouyang Wang. Text-based crude
    oil price forecasting: A deep learning approach. *International Journal of Forecasting*,
    35(4):1548 – 1560, 2019b.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2019b] Xuerong Li, Wei Shang 和 Shouyang Wang. 基于文本的原油价格预测：一种深度学习方法。*国际预测期刊*，35(4):1548–1560，2019
    年。
- en: 'Li et al. [2018] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion
    convolutional recurrent neural network: Data-driven traffic forecasting. In *International
    Conference on Learning Representations*, 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2018] Yaguang Li, Rose Yu, Cyrus Shahabi 和 Yan Liu. 扩散卷积递归神经网络：数据驱动的交通预测。见
    *国际学习表征会议*，2018 年。
- en: 'Lim and Zohren [2021] Bryan Lim and Stefan Zohren. Time-series forecasting
    with deep learning: a survey. *Philosophical Transactions of the Royal Society
    A: Mathematical, Physical and Engineering Sciences*, 379(2194), Feb 2021.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lim 和 Zohren [2021] Bryan Lim 和 Stefan Zohren. 使用深度学习进行时间序列预测：综述。*皇家学会哲学交易 A：数学、物理与工程科学*，379(2194)，2021
    年 2 月。
- en: Lim et al. [2021] Bryan Lim, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister.
    Temporal fusion transformers for interpretable multi-horizon time series forecasting.
    *International Journal of Forecasting*, 37(4):1748–1764, 2021.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lim 等人 [2021] Bryan Lim, Sercan Ö Arık, Nicolas Loeff 和 Tomas Pfister. 用于可解释的多时间段时间序列预测的时间融合变换器。*国际预测期刊*，37(4):1748–1764，2021
    年。
- en: Lin et al. [2017] Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting
    Sun. Adversarial ranking for language generation. *Advances in Neural Information
    Processing Systems*, 30, 2017.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2017] Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang 和 Ming-Ting
    Sun. 语言生成的对抗性排名。*神经信息处理系统进展*，第 30 卷，2017 年。
- en: Lipton [2018] Zachary C. Lipton. The mythos of model interpretability. *Queue*,
    16(3):30:31–30:57, 2018. ISSN 1542-7730.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lipton [2018] Zachary C. Lipton. 模型可解释性的神话。*Queue*，16(3):30:31–30:57，2018 年。ISSN
    1542-7730。
- en: Low et al. [2011] Yucheng Low, Deepak Agarwal, and Alexander J Smola. Multiple
    domain user personalization. In *Proceedings of the 17th ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining*, pages 123–131\. ACM, 2011.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Low 等人 [2011] Yucheng Low, Deepak Agarwal 和 Alexander J Smola. 多领域用户个性化。见 *第
    17 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*，第 123–131 页。ACM，2011 年。
- en: Luo et al. [2018] Rui Luo, Weinan Zhang, Xiaojun Xu, and Jun Wang. A neural
    stochastic volatility model. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 32, 2018.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 [2018] Rui Luo, Weinan Zhang, Xiaojun Xu 和 Jun Wang. 神经随机波动模型。见 *AAAI
    人工智能会议论文集*，第 32 卷，2018 年。
- en: Lütkepohl [2005] Helmut Lütkepohl. Vector autoregressive moving average processes.
    In *New Introduction to Multiple Time Series Analysis*, pages 419–446\. Springer,
    2005.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lütkepohl [2005] Helmut Lütkepohl. 向量自回归移动平均过程。见 *多重时间序列分析新介绍*，第 419–446 页。Springer，2005
    年。
- en: 'Lv et al. [2014] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue
    Wang. Traffic flow prediction with big data: A deep learning approach. *IEEE Transactions
    on Intelligent Transportation Systems*, 16(2):865–873, 2014.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv 等人 [2014] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li 和 Fei-Yue Wang.
    利用大数据进行交通流预测：一种深度学习方法。*IEEE 智能交通系统汇刊*，16(2):865–873，2014 年。
- en: Maddix et al. [2018] Danielle C Maddix, Yuyang Wang, and Alex Smola. Deep factors
    with gaussian processes for forecasting. *arXiv preprint arXiv:1812.00098*, 2018.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maddix 等人 [2018] Danielle C Maddix, Yuyang Wang 和 Alex Smola. 使用高斯过程的深度因子预测。*arXiv
    预印本 arXiv:1812.00098*，2018 年。
- en: 'Makridakis et al. [2018a] Spyros Makridakis, Evangelos Spiliotis, and Vassilios
    Assimakopoulos. Statistical and machine learning forecasting methods: Concerns
    and ways forward. *PLOS ONE*, 13, 03 2018a.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makridakis et al. [2018a] Spyros Makridakis, Evangelos Spiliotis, 和 Vassilios
    Assimakopoulos. 统计和机器学习预测方法：关注点与前进方向。*PLOS ONE*, 13, 03 2018a。
- en: 'Makridakis et al. [2018b] Spyros Makridakis, Evangelos Spiliotis, and Vassilios
    Assimakopoulos. The M4 competition: Results, findings, conclusion and way forward.
    *International Journal of Forecasting*, 34(4):802–808, 2018b.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makridakis et al. [2018b] Spyros Makridakis, Evangelos Spiliotis, 和 Vassilios
    Assimakopoulos. M4 竞赛：结果、发现、结论及前进方向。*国际预测学期刊*, 34(4):802–808, 2018b。
- en: 'Makridakis et al. [2021] Spyros Makridakis, Evangelos Spiliotis, Vassilios
    Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, and Robert L Winkler. The m5
    uncertainty competition: Results, findings and conclusions. *International Journal
    of Forecasting*, 2021.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makridakis et al. [2021] Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos,
    Zhi Chen, Anil Gaba, Ilia Tsetlin, 和 Robert L Winkler. m5 不确定性竞赛：结果、发现与结论。*国际预测学期刊*,
    2021。
- en: Mariet and Kuznetsov [2019] Zelda Mariet and Vitaly Kuznetsov. Foundations of
    sequence-to-sequence modeling for time series. In *The 22nd International Conference
    on Artificial Intelligence and Statistics*, pages 408–417\. PMLR, 2019.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mariet and Kuznetsov [2019] Zelda Mariet 和 Vitaly Kuznetsov. 时间序列的序列到序列建模基础。见于
    *第22届人工智能与统计国际会议*, 页408–417\. PMLR, 2019。
- en: Matheson and Winkler [1976] James E. Matheson and Robert L. Winkler. Scoring
    rules for continuous probability distributions. *Management Science*, 22(10):1087–1096,
    1976.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matheson and Winkler [1976] James E. Matheson 和 Robert L. Winkler. 连续概率分布的评分规则。*管理科学*,
    22(10):1087–1096, 1976。
- en: 'Mei and Eisner [2017] Hongyuan Mei and Jason M. Eisner. The neural hawkes process:
    A neurally self-modulating multivariate point process. In *Advances in Neural
    Information Processing Systems*, pages 6754–6764, 2017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mei and Eisner [2017] Hongyuan Mei 和 Jason M. Eisner. 神经 Hawkes 过程：一种神经自调节的多变量点过程。见于
    *神经信息处理系统进展*, 页6754–6764, 2017。
- en: 'Mogren [2016] Olof Mogren. C-RNN-GAN: Continuous recurrent neural networks
    with adversarial training. *arXiv preprint arXiv:1611.09904*, 2016.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mogren [2016] Olof Mogren. C-RNN-GAN：带有对抗训练的连续递归神经网络。*arXiv 预印本 arXiv:1611.09904*,
    2016。
- en: 'Montero-Manso and Hyndman [2020] Pablo Montero-Manso and Rob J Hyndman. Principles
    and algorithms for forecasting groups of time series: Locality and globality.
    *arXiv preprint arXiv:2008.00444*, 2020.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montero-Manso and Hyndman [2020] Pablo Montero-Manso 和 Rob J Hyndman. 时间序列组预测的原理和算法：局部性与全球性。*arXiv
    预印本 arXiv:2008.00444*, 2020。
- en: 'Mukherjee et al. [2018] Srayanta Mukherjee, Devashish Shankar, Atin Ghosh,
    Nilam Tathawadekar, Pramod Kompalli, Sunita Sarawagi, and Krishnendu Chaudhury.
    ARMDN: Associative and recurrent mixture density networks for eretail demand forecasting.
    *arXiv preprint arXiv:1803.03800*, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukherjee et al. [2018] Srayanta Mukherjee, Devashish Shankar, Atin Ghosh, Nilam
    Tathawadekar, Pramod Kompalli, Sunita Sarawagi, 和 Krishnendu Chaudhury. ARMDN：用于电子零售需求预测的关联与递归混合密度网络。*arXiv
    预印本 arXiv:1803.03800*, 2018。
- en: Oliva et al. [2018] Junier Oliva, Avinava Dubey, Manzil Zaheer, Barnabas Poczos,
    Ruslan Salakhutdinov, Eric Xing, and Jeff Schneider. Transformation autoregressive
    networks. In *International Conference on Machine Learning*, pages 3898–3907\.
    PMLR, 2018.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oliva et al. [2018] Junier Oliva, Avinava Dubey, Manzil Zaheer, Barnabas Poczos,
    Ruslan Salakhutdinov, Eric Xing, 和 Jeff Schneider. 转换自回归网络。见于 *国际机器学习会议*, 页3898–3907\.
    PMLR, 2018。
- en: 'Oreshkin et al. [2019] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and
    Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time
    series forecasting. *arXiv preprint arXiv:1905.10437*, 2019.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oreshkin et al. [2019] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, 和
    Yoshua Bengio. N-beats：用于可解释时间序列预测的神经基扩展分析。*arXiv 预印本 arXiv:1905.10437*, 2019。
- en: Oreshkin et al. [2020] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and
    Yoshua Bengio. Meta-learning framework with applications to zero-shot time-series
    forecasting. *arXiv preprint arXiv:2002.02887*, 2020.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oreshkin et al. [2020] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, 和
    Yoshua Bengio. 元学习框架及其在零样本时间序列预测中的应用。*arXiv 预印本 arXiv:2002.02887*, 2020。
- en: Papamakarios et al. [2017] George Papamakarios, Theo Pavlakou, and Iain Murray.
    Masked autoregressive flow for density estimation. *arXiv preprint arXiv:1705.07057*,
    2017.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papamakarios et al. [2017] George Papamakarios, Theo Pavlakou, 和 Iain Murray.
    用于密度估计的掩码自回归流。*arXiv 预印本 arXiv:1705.07057*, 2017。
- en: Park et al. [2022] Youngsuk Park, Danielle Maddix, François-Xavier Aubet, Kelvin
    Kan, Jan Gasthaus, and Yuyang Wang. Learning quantile functions without quantile
    crossing for distribution-free time series forecasting. In *The 25th International
    Conference on Artificial Intelligence and Statistics*, 2022.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人 [2022] Youngsuk Park, Danielle Maddix, François-Xavier Aubet, Kelvin
    Kan, Jan Gasthaus, 和 Yuyang Wang. 无量纲时间序列预测中的学习分位数函数而不发生分位数交叉. 见于*第25届国际人工智能与统计会议*，2022.
- en: Pascanu et al. [2013] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the
    difficulty of training recurrent neural networks. In *Proceedings of the 30th
    International Conference on Machine Learning*, volume 28, pages 1310–1318, 2013.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascanu等人 [2013] Razvan Pascanu, Tomas Mikolov, 和 Yoshua Bengio. 训练递归神经网络的难度.
    见于*第30届国际机器学习大会会议论文集*，第28卷，页码1310–1318, 2013.
- en: 'Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
    *Advances in Neural Information Processing Systems*, 32, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke等人 [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
    Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga 等人.
    Pytorch：一种命令式风格的高性能深度学习库. *神经信息处理系统进展*，32, 2019.
- en: Qiu et al. [2014] Xueheng Qiu, Le Zhang, Ye Ren, Ponnuthurai N Suganthan, and
    Gehan Amaratunga. Ensemble deep learning for regression and time series forecasting.
    In *Symposium on Computational Intelligence in Ensemble Learning (CIEL)*, pages
    1–6\. IEEE, 2014.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu等人 [2014] Xueheng Qiu, Le Zhang, Ye Ren, Ponnuthurai N Suganthan, 和 Gehan
    Amaratunga. 回归和时间序列预测的集成深度学习. 见于*集成学习计算智能研讨会 (CIEL)*，页码1–6. IEEE, 2014.
- en: 'Rabanser et al. [2020] Stephan Rabanser, Tim Januschowski, Valentin Flunkert,
    David Salinas, and Jan Gasthaus. The effectiveness of discretization in forecasting:
    An empirical study on neural time series models. *arXiv preprint arXiv:2005.10111*,
    2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabanser等人 [2020] Stephan Rabanser, Tim Januschowski, Valentin Flunkert, David
    Salinas, 和 Jan Gasthaus. 离散化在预测中的有效性：对神经时间序列模型的实证研究. *arXiv预印本 arXiv:2005.10111*,
    2020.
- en: Rangapuram et al. [2018] Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus,
    Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for
    time series forecasting. In *Advances in Neural Information Processing Systems*,
    pages 7785–7794, 2018.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rangapuram等人 [2018] Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus,
    Lorenzo Stella, Yuyang Wang, 和 Tim Januschowski. 用于时间序列预测的深度状态空间模型. 见于*神经信息处理系统进展*，页码7785–7794,
    2018.
- en: Rangapuram et al. [2021] Syama Sundar Rangapuram, Lucien D Werner, Konstantinos
    Benidis, Pedro Mercado, Jan Gasthaus, and Tim Januschowski. End-to-end learning
    of coherent probabilistic forecasts for hierarchical time series. In *International
    Conference on Machine Learning*, pages 8832–8843\. PMLR, 2021.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rangapuram等人 [2021] Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis,
    Pedro Mercado, Jan Gasthaus, 和 Tim Januschowski. 对于层次时间序列的连贯概率预测的端到端学习. 见于*国际机器学习大会*，页码8832–8843.
    PMLR, 2021.
- en: Rangapuram et al. [2022] Syama Sundar Rangapuram, Shubham Shubham Kapoor, Rajbir
    Nirwan, Pedro Mercado, Tim Januschowski, Yuyang Wang, and Michael Bohlke-Schneider.
    Coherent probabilistic forecasting for temporal hierarchies, 2022.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rangapuram等人 [2022] Syama Sundar Rangapuram, Shubham Shubham Kapoor, Rajbir
    Nirwan, Pedro Mercado, Tim Januschowski, Yuyang Wang, 和 Michael Bohlke-Schneider.
    时间层次的连贯概率预测, 2022.
- en: Rasmussen and Williams [2006] Carl Edward Rasmussen and Christopher KI Williams.
    *Gaussian process for machine learning*. MIT press, 2006.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasmussen和Williams [2006] Carl Edward Rasmussen 和 Christopher KI Williams. *机器学习的高斯过程*.
    MIT出版社, 2006.
- en: Rasul et al. [2020] Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs
    Bergmann, and Roland Vollgraf. Multi-variate probabilistic time series forecasting
    via conditioned normalizing flows. *arXiv preprint arXiv:2002.06103*, 2020.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasul等人 [2020] Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann,
    和 Roland Vollgraf. 通过条件正则化流进行多变量概率时间序列预测. *arXiv预印本 arXiv:2002.06103*, 2020.
- en: Rasul et al. [2021] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland
    Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic
    time series forecasting. In *International Conference on Machine Learning*, pages
    8857–8868\. PMLR, 2021.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasul等人 [2021] Kashif Rasul, Calvin Seward, Ingmar Schuster, 和 Roland Vollgraf.
    自回归去噪扩散模型用于多变量概率时间序列预测. 见于*国际机器学习大会*，页码8857–8868. PMLR, 2021.
- en: Rosenblatt [1957] Frank Rosenblatt. *The perceptron, a perceiving and recognizing
    automaton Project Para*. Cornell Aeronautical Laboratory, 1957.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenblatt [1957] Frank Rosenblatt. *感知器，一个感知和识别自动化项目*. 康奈尔航空实验室, 1957.
- en: Rumelhart et al. [1985] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    Learning internal representations by error propagation. Technical report, University
    of California San Diego, La Jolla Institute for Cognitive Science, 1985.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 等人 [1985] David E Rumelhart、Geoffrey E Hinton 和 Ronald J Williams。通过误差传播学习内部表征。技术报告，加州大学圣地亚哥分校，拉荷亚认知科学研究所，1985。
- en: Rumelhart et al. [1986] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
    Williams. Learning representations by back-propagating errors. *Nature*, 323(6088):533–536,
    1986.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 等人 [1986] David E. Rumelhart、Geoffrey E. Hinton 和 Ronald J. Williams。通过反向传播误差学习表征。*自然*，323(6088)：533–536，1986。
- en: Salinas et al. [2019] David Salinas, Michael Bohlke-Schneider, Laurent Callot,
    and Jan Gasthaus. High-dimensional multivariate forecasting with low-rank gaussian
    copula processes. In *Advances in Neural Information Processing Systems*, 2019.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salinas 等人 [2019] David Salinas、Michael Bohlke-Schneider、Laurent Callot 和 Jan
    Gasthaus。使用低秩高斯 Copula 过程进行高维多变量预测。在 *神经信息处理系统进展* 中，2019。
- en: 'Salinas et al. [2020] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim
    Januschowski. DeepAR: Probabilistic forecasting with autoregressive recurrent
    networks. *International Journal of Forecasting*, 36(3):1181–1191, 2020.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salinas 等人 [2020] David Salinas、Valentin Flunkert、Jan Gasthaus 和 Tim Januschowski。DeepAR：使用自回归递归网络的概率预测。*国际预测杂志*，36(3)：1181–1191，2020。
- en: Saxena et al. [2019] Harshit Saxena, Omar Aponte, and Katie T. McConky. A hybrid
    machine learning model for forecasting a billing period’s peak electric load days.
    *International Journal of Forecasting*, 35(4):1288 – 1303, 2019.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saxena 等人 [2019] Harshit Saxena、Omar Aponte 和 Katie T. McConky。用于预测计费周期峰值电负荷天数的混合机器学习模型。*国际预测杂志*，35(4)：1288
    – 1303，2019。
- en: Schölkopf [2019] Bernhard Schölkopf. Causality for machine learning. *arXiv
    preprint arXiv:1911.10500*, 2019.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schölkopf [2019] Bernhard Schölkopf。机器学习的因果关系。*arXiv 预印本 arXiv:1911.10500*，2019。
- en: Seeger [2004] Matthias Seeger. Gaussian processes for machine learning. *International
    Journal of Neural Systems*, 14(02):69–106, 2004.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seeger [2004] Matthias Seeger。用于机器学习的高斯过程。*国际神经系统杂志*，14(02)：69–106，2004。
- en: Seeger et al. [2016] Matthias W Seeger, David Salinas, and Valentin Flunkert.
    Bayesian intermittent demand forecasting for large inventories. In *Advances in
    Neural Information Processing Systems*, pages 4646–4654, 2016.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seeger 等人 [2016] Matthias W Seeger、David Salinas 和 Valentin Flunkert。大规模库存的贝叶斯间歇需求预测。在
    *神经信息处理系统进展* 中，页码 4646–4654，2016。
- en: Semenoglou et al. [2021] Artemios-Anargyros Semenoglou, Evangelos Spiliotis,
    Spyros Makridakis, and Vassilios Assimakopoulos. Investigating the accuracy of
    cross-learning time series forecasting methods. *International Journal of Forecasting*,
    37(3):1072–1084, 2021.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Semenoglou 等人 [2021] Artemios-Anargyros Semenoglou、Evangelos Spiliotis、Spyros
    Makridakis 和 Vassilios Assimakopoulos。调查交叉学习时间序列预测方法的准确性。*国际预测杂志*，37(3)：1072–1084，2021。
- en: 'Sen et al. [2019] Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. Think globally,
    act locally: A deep neural network approach to high-dimensional time series forecasting.
    *Advances in Neural Information Processing Systems*, 32, 2019.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sen 等人 [2019] Rajat Sen、Hsiang-Fu Yu 和 Inderjit S Dhillon。全球思考，本地行动：一种深度神经网络方法用于高维时间序列预测。*神经信息处理系统进展*，32，2019。
- en: Shang et al. [2021] Chao Shang, Jie Chen, and Jinbo Bi. Discrete graph structure
    learning for forecasting multiple time series. In *International Conference on
    Learning Representations*, 2021.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang 等人 [2021] Chao Shang、Jie Chen 和 Jinbo Bi。用于预测多时间序列的离散图结构学习。在 *国际学习表征会议*
    中，2021。
- en: Sharma et al. [2018] Anuj Sharma, Robert Johnson, Florian Engert, and Scott
    Linderman. Point process latent variable models of larval zebrafish behavior.
    In *Advances in Neural Information Processing Systems*, pages 10919–10930, 2018.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 [2018] Anuj Sharma、Robert Johnson、Florian Engert 和 Scott Linderman。幼鱼行为的点过程潜变量模型。在
    *神经信息处理系统进展* 中，页码 10919–10930，2018。
- en: Shchur et al. [2021a] Oleksandr Shchur, Ali Caner Turkmen, Tim Januschowski,
    Jan Gasthaus, and Stephan Günnemann. Detecting anomalous event sequences with
    temporal point processes. *Advances in Neural Information Processing Systems*,
    34, 2021a.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shchur 等人 [2021a] Oleksandr Shchur、Ali Caner Turkmen、Tim Januschowski、Jan Gasthaus
    和 Stephan Günnemann。使用时间点过程检测异常事件序列。*神经信息处理系统进展*，34，2021a。
- en: 'Shchur et al. [2021b] Oleksandr Shchur, Ali Caner Türkmen, Tim Januschowski,
    and Stephan Günnemann. Neural temporal point processes: A review. *arXiv preprint
    arXiv:2104.03528*, 2021b.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shchur 等人 [2021b] Oleksandr Shchur、Ali Caner Türkmen、Tim Januschowski 和 Stephan
    Günnemann。神经时间点过程：综述。*arXiv 预印本 arXiv:2104.03528*，2021b。
- en: Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez,
    Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
    Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural
    networks and tree search. *nature*, 529(7587):484–489, 2016.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver 等人 [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent
    Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
    Panneershelvam, Marc Lanctot 等人. 使用深度神经网络和树搜索掌握围棋。*自然*，529(7587)：484–489，2016。
- en: Silver et al. [2018] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis
    Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,
    Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general
    reinforcement learning algorithm that masters chess, shogi, and go through self-play.
    *Science*, 362(6419):1140–1144, 2018.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver 等人 [2018] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis
    Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,
    Thore Graepel, Timothy Lillicrap, Karen Simonyan 和 Demis Hassabis. 一种通用的强化学习算法，通过自我对弈掌握国际象棋、将棋和围棋。*科学*，362(6419)：1140–1144，2018。
- en: Smyl [2020] Slawek Smyl. A hybrid method of exponential smoothing and recurrent
    neural networks for time series forecasting. *International Journal of Forecasting*,
    36(1):75–85, 2020.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smyl [2020] Slawek Smyl. 一种指数平滑和递归神经网络的混合方法用于时间序列预测。*国际预测杂志*，36(1)：75–85，2020。
- en: Smyl and Hua [2019] Slawek Smyl and N. Grace Hua. Machine learning methods for
    GEFCom2017 probabilistic load forecasting. *International Journal of Forecasting*,
    35(4):1424–1431, 2019.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smyl 和 Hua [2019] Slawek Smyl 和 N. Grace Hua. 用于 GEFCom2017 概率负荷预测的机器学习方法。*国际预测杂志*，35(4)：1424–1431，2019。
- en: 'Snyder et al. [2012] Ralph D. Snyder, J. Keith Ord, and Adrian Beaumont. Forecasting
    the intermittent demand for slow-moving inventories: A modelling approach. *International
    Journal of Forecasting*, 28(2):485–496, 2012.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snyder 等人 [2012] Ralph D. Snyder, J. Keith Ord 和 Adrian Beaumont. 预测慢速移动库存的间歇性需求：一种建模方法。*国际预测杂志*，28(2)：485–496，2012。
- en: Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *International Conference on Machine Learning*, pages 2256–2265\. PMLR, 2015.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein 等人 [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan
    和 Surya Ganguli. 使用非平衡热力学的深度无监督学习。发表于 *国际机器学习会议*，第 2256–2265 页。PMLR，2015。
- en: 'Song et al. [2018] Huan Song, Deepta Rajan, Jayaraman J Thiagarajan, and Andreas
    Spanias. Attend and diagnose: Clinical time series analysis using attention models.
    In *Thirty-Second AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2018] Huan Song, Deepta Rajan, Jayaraman J Thiagarajan 和 Andreas Spanias.
    关注与诊断：使用注意力模型的临床时间序列分析。发表于 *第三十二届 AAAI 人工智能会议*，2018。
- en: Song and Kingma [2021] Yang Song and Diederik P Kingma. How to train your energy-based
    models. *arXiv preprint arXiv:2101.03288*, 2021.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 和 Kingma [2021] Yang Song 和 Diederik P Kingma. 如何训练你的基于能量的模型。*arXiv 预印本
    arXiv:2101.03288*，2021。
- en: Stankeviciute et al. [2021] Kamile Stankeviciute, Ahmed M Alaa, and Mihaela
    van der Schaar. Conformal time-series forecasting. *Advances in Neural Information
    Processing Systems*, 34, 2021.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stankeviciute 等人 [2021] Kamile Stankeviciute, Ahmed M Alaa 和 Mihaela van der
    Schaar. 形状一致的时间序列预测。*神经信息处理系统进展*，34，2021。
- en: Taieb et al. [2021] Souhaib Ben Taieb, James W Taylor, and Rob J Hyndman. Hierarchical
    probabilistic forecasting of electricity demand with smart meter data. *Journal
    of the American Statistical Association*, 116(533):27–43, 2021.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taieb 等人 [2021] Souhaib Ben Taieb, James W Taylor 和 Rob J Hyndman. 基于智能电表数据的电力需求分层概率预测。*美国统计协会杂志*，116(533)：27–43，2021。
- en: 'Takahashi et al. [2019] Shuntaro Takahashi, Yu Chen, and Kumiko Tanaka-Ishii.
    Modeling financial time-series with generative adversarial networks. *Physica
    A: Statistical Mechanics and its Applications*, 527:121261, 2019.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takahashi 等人 [2019] Shuntaro Takahashi, Yu Chen 和 Kumiko Tanaka-Ishii. 使用生成对抗网络建模金融时间序列。*物理学
    A：统计力学及其应用*，527：121261，2019。
- en: 'Theodosiou and Kourentzes [2021] Filotas Theodosiou and Nikolaos Kourentzes.
    Forecasting with deep temporal hierarchies. *Available at SSRN: https://ssrn.com/abstract=3918315
    or http://dx.doi.org/10.2139/ssrn.3918315*, 2021.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theodosiou 和 Kourentzes [2021] Filotas Theodosiou 和 Nikolaos Kourentzes. 使用深度时间层次进行预测。*可在
    SSRN 上获取：https://ssrn.com/abstract=3918315 或 http://dx.doi.org/10.2139/ssrn.3918315*，2021。
- en: Toubeau et al. [2018] Jean-François Toubeau, Jérémie Bottieau, François Vallée,
    and Zacharie De Grève. Deep learning-based multivariate probabilistic forecasting
    for short-term scheduling in power markets. *IEEE Transactions on Power Systems*,
    34(2):1203–1215, 2018.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toubeau 等 [2018] Jean-François Toubeau, Jérémie Bottieau, François Vallée 和
    Zacharie De Grève. 基于深度学习的多变量概率预测用于电力市场的短期调度。*IEEE 电力系统学报*，34(2):1203–1215，2018年。
- en: Turkmen et al. [2019] Ali Caner Turkmen, Yuyang Wang, and Tim Januschowski.
    Intermittent demand forecasting with deep renewal processes. *arXiv preprint arXiv:1911.10416*,
    2019.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turkmen 等 [2019] Ali Caner Turkmen, Yuyang Wang 和 Tim Januschowski. 使用深度更新过程的间歇性需求预测。*arXiv
    预印本 arXiv:1911.10416*，2019年。
- en: 'Türkmen et al. [2019] Ali Caner Türkmen, Yuyang Wang, and Alexander J. Smola.
    Fastpoint: Scalable deep point processes. In *Joint European Conference on Machine
    Learning and Knowledge Discovery in Databases*, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Türkmen 等 [2019] Ali Caner Türkmen, Yuyang Wang 和 Alexander J. Smola. Fastpoint:
    可扩展的深度点过程。见于 *欧洲联合机器学习与数据库知识发现会议*，2019年。'
- en: 'Turkmen et al. [2021] Ali Caner Turkmen, Tim Januschowski, Yuyang Wang, and
    Ali Taylan Cemgil. Forecasting intermittent and sparse time series: A unified
    probabilistic framework via deep renewal processes. *PlosOne*, 2021.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turkmen 等 [2021] Ali Caner Turkmen, Tim Januschowski, Yuyang Wang 和 Ali Taylan
    Cemgil. 预测间歇性和稀疏时间序列：通过深度更新过程的统一概率框架。*PlosOne*，2021年。
- en: 'Van Den Oord et al. [2016] Aäron Van Den Oord, Sander Dieleman, Heiga Zen,
    Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W Senior,
    and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. *SSW*, 125,
    2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Van Den Oord 等 [2016] Aäron Van Den Oord, Sander Dieleman, Heiga Zen, Karen
    Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W Senior 和 Koray
    Kavukcuoglu. Wavenet: 一种用于原始音频的生成模型。*SSW*，125，2016年。'
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Advances in Neural Information Processing Systems*, pages 5998–6008,
    2017.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin. 注意力机制是你所需的一切。见于
    *神经信息处理系统进展*，第5998–6008页，2017年。
- en: Vialard et al. [2020] François-Xavier Vialard, Roland Kwitt, Suan Wei, and Marc
    Niethammer. A shooting formulation of deep learning. In *Advances in Neural Information
    Processing Systems*, 2020.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vialard 等 [2020] François-Xavier Vialard, Roland Kwitt, Suan Wei 和 Marc Niethammer.
    深度学习的发射式公式。见于 *神经信息处理系统进展*，2020年。
- en: Wallace and Dowe [2000] Chris S Wallace and David L Dowe. MML clustering of
    multi-state, Poisson, von Mises circular and Gaussian distributions. *Statistics
    and Computing*, 10(1):73–83, 2000.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wallace 和 Dowe [2000] Chris S Wallace 和 David L Dowe. MML 聚类的多状态、泊松、冯·米塞斯圆形和高斯分布。*统计学与计算*，10(1):73–83，2000年。
- en: Wang et al. [2021] Rui Wang, Danielle Maddix, Christos Faloutsos, Yuyang Wang,
    and Rose Yu. Bridging physics-based and data-driven modeling for learning dynamical
    systems. In *Learning for Dynamics and Control*, pages 385–398\. PMLR, 2021.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2021] Rui Wang, Danielle Maddix, Christos Faloutsos, Yuyang Wang 和 Rose
    Yu. 结合基于物理的建模和数据驱动建模来学习动态系统。见于 *动态与控制的学习*，第385–398页，PMLR，2021年。
- en: Wang et al. [2019] Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean
    Foster, and Tim Januschowski. Deep factors for forecasting. In *International
    Conference on Machine Learning*, pages 6607–6617, 2019.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2019] Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean Foster
    和 Tim Januschowski. 用于预测的深度因子。见于 *国际机器学习大会*，第6607–6617页，2019年。
- en: Wen and Torkkola [2019] Ruofeng Wen and Kari Torkkola. Deep generative quantile-copula
    models for probabilistic forecasting. *arXiv preprint arXiv:1907.10697*, 2019.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 和 Torkkola [2019] Ruofeng Wen 和 Kari Torkkola. 用于概率预测的深度生成量化-副本模型。*arXiv
    预印本 arXiv:1907.10697*，2019年。
- en: Wen et al. [2017] Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and
    Dhruv Madeka. A multi-horizon quantile recurrent forecaster. *arXiv preprint arXiv:1711.11053*,
    2017.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等 [2017] Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy 和 Dhruv
    Madeka. 多时间尺度量化递归预测器。*arXiv 预印本 arXiv:1711.11053*，2017年。
- en: Wickramasuriya et al. [2015] Shanika L Wickramasuriya, George Athanasopoulos,
    Rob J Hyndman, et al. Forecasting hierarchical and grouped time series through
    trace minimization. *Department of Econometrics and Business Statistics, Monash
    University*, 2015.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wickramasuriya 等 [2015] Shanika L Wickramasuriya, George Athanasopoulos, Rob
    J Hyndman 等. 通过迹最小化预测层次和分组时间序列。*经济计量学与商业统计系，莫纳什大学*，2015年。
- en: Wu et al. [2020a] Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei,
    and Junzhou Huang. Adversarial sparse transformer for time series forecasting.
    *Advances in Neural Information Processing Systems*, 33:17105–17115, 2020a.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2020a] Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei 和 Junzhou
    Huang. 对抗稀疏变换器用于时间序列预测。*神经信息处理系统进展*，33:17105–17115，2020a年。
- en: 'Wu et al. [2020b] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun
    Chang, and Chengqi Zhang. Connecting the dots: Multivariate time series forecasting
    with graph neural networks. In *Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, pages 753–763, 2020b.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2020b] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang
    和 Chengqi Zhang. 连接点：利用图神经网络进行多变量时间序列预测。在 *第26届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，第753–763页，2020b年。
- en: Xiao et al. [2017a] Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan,
    Le Song, and Hongyuan Zha. Wasserstein learning of deep generative point process
    models. In *Advances in Neural Information Processing Systems*, pages 3247–3257,
    2017a.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等 [2017a] Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song
    和 Hongyuan Zha. Wasserstein 深度生成点过程模型学习。在 *神经信息处理系统进展*，第3247–3257页，2017a年。
- en: Xiao et al. [2017b] Shuai Xiao, Junchi Yan, Mehrdad Farajtabar, Le Song, Xiaokang
    Yang, and Hongyuan Zha. Joint modeling of event sequence and time series with
    attentional twin recurrent neural networks. *arXiv preprint arXiv:1703.08524*,
    2017b.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等 [2017b] Shuai Xiao, Junchi Yan, Mehrdad Farajtabar, Le Song, Xiaokang
    Yang 和 Hongyuan Zha. 带有注意力的双重递归神经网络的事件序列和时间序列联合建模。*arXiv 预印本 arXiv:1703.08524*，2017b年。
- en: Xu et al. [2016] Qifa Xu, Xi Liu, Cuixia Jiang, and Keming Yu. Quantile autoregression
    neural network model with applications to evaluating value at risk. *Applied Soft
    Computing*, 49:1–12, 2016.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 [2016] Qifa Xu, Xi Liu, Cuixia Jiang 和 Keming Yu. 分位自回归神经网络模型及其在评估风险价值中的应用。*应用软计算*，49:1–12，2016年。
- en: Yan et al. [2018] Xing Yan, Weizhong Zhang, Lin Ma, Wei Liu, and Qi Wu. Parsimonious
    quantile regression of financial asset tail dynamics via sequential learning.
    *Advances in Neural Information Processing Systems*, 31, 2018.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等 [2018] Xing Yan, Weizhong Zhang, Lin Ma, Wei Liu 和 Qi Wu. 通过序列学习的金融资产尾部动态的简约分位回归。*神经信息处理系统进展*，31，2018年。
- en: Yoo and Kang [2021] Jaemin Yoo and U Kang. Attention-based autoregression for
    accurate and efficient multivariate time series forecasting. In *Proceedings of
    the 2021 SIAM International Conference on Data Mining (SDM)*, pages 531–539\.
    SIAM, 2021.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoo 和 Kang [2021] Jaemin Yoo 和 U Kang. 基于注意力的自回归用于准确和高效的多变量时间序列预测。在 *2021年SIAM国际数据挖掘会议（SDM）论文集*，第531–539页。SIAM，2021年。
- en: Yoon et al. [2019] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar.
    Time-series generative adversarial networks. *Advances in Neural Information Processing
    Systems*, 32, 2019.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoon 等 [2019] Jinsung Yoon, Daniel Jarrett 和 Mihaela Van der Schaar. 时间序列生成对抗网络。*神经信息处理系统进展*，32，2019年。
- en: Yu et al. [2016] Hsiang-Fu Yu, Rao N., and I.S. Dhillon. Temporal regularized
    matrix factorization for high-dimensional time series prediction. *Advances in
    Neural Information Processing Systems*, pages 847–855, 2016.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 [2016] Hsiang-Fu Yu, Rao N. 和 I.S. Dhillon. 时序正则化矩阵分解用于高维时间序列预测。*神经信息处理系统进展*，第847–855页，2016年。
- en: Zhang [2003] G Peter Zhang. Time series forecasting using a hybrid ARIMA and
    neural network model. *Neurocomputing*, 50:159–175, 2003.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang [2003] G Peter Zhang. 使用混合ARIMA和神经网络模型的时间序列预测。*神经计算*，50:159–175，2003年。
- en: 'Zhang et al. [1998] Guoqiang Zhang, B Eddy Patuwo, and Michael Y Hu. Forecasting
    with artificial neural networks: The state of the art. *International journal
    of forecasting*, 14(1):35–62, 1998.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [1998] Guoqiang Zhang, B Eddy Patuwo 和 Michael Y Hu. 使用人工神经网络的预测：现状。*国际预测期刊*，14(1):35–62，1998年。
- en: Zhang et al. [2019] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus
    Odena. Self-attention generative adversarial networks. In *International Conference
    on Machine Learning*, pages 7354–7363\. PMLR, 2019.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2019] Han Zhang, Ian Goodfellow, Dimitris Metaxas 和 Augustus Odena.
    自注意力生成对抗网络。在 *国际机器学习会议*，第7354–7363页。PMLR，2019年。
- en: 'Zhou et al. [2020] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin
    Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long
    sequence time-series forecasting. *arXiv preprint arXiv:2012.07436*, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 [2020] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin
    Li, Hui Xiong 和 Wancai Zhang. Informer：超越高效的变换器用于长序列时间序列预测。*arXiv 预印本 arXiv:2012.07436*，2020年。
- en: Zhu and Laptev [2017] Lingxue Zhu and Nikolay Laptev. Deep and confident prediction
    for time series at Uber. In *IEEE International Conference on Data Mining Workshops
    (ICDMW)*, pages 103–110, 2017.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱和拉普捷夫 [2017] 朱灵雪和尼古拉·拉普捷夫。Uber 中时间序列的深度和自信预测。发表于 *IEEE 国际数据挖掘会议工作坊（ICDMW）*，页码
    103–110，2017年。
- en: Zügner et al. [2021] Daniel Zügner, François-Xavier Aubet, Victor Garcia Satorras,
    Tim Januschowski, Stephan Günnemann, and Jan Gasthaus. A study of joint graph
    inference and forecasting. *arXiv preprint arXiv:2109.04979*, 2021.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zügner 等人 [2021] 丹尼尔·兹格纳、弗朗索瓦-泽维尔·奥贝、维克多·加西亚·萨托拉斯、蒂姆·贾努什科夫斯基、斯特凡·居内曼和扬·加斯图斯。关于联合图推断和预测的研究。*arXiv
    预印本 arXiv:2109.04979*，2021年。
