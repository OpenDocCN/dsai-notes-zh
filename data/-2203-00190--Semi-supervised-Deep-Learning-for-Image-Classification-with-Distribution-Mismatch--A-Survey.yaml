- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:47:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:47:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2203.00190] Semi-supervised Deep Learning for Image Classification with Distribution
    Mismatch: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2203.00190] 半监督深度学习在图像分类中的分布不匹配：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2203.00190](https://ar5iv.labs.arxiv.org/html/2203.00190)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2203.00190](https://ar5iv.labs.arxiv.org/html/2203.00190)
- en: 'Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督深度学习在图像分类中的分布不匹配：综述
- en: 'Saul Calderon-Ramirez, Shengxiang Yang,  and David Elizondo Manuscript received
    07 February 2022.S. Calderon-Ramirez, S. Yang, and D. Elizondo are with the Institute
    of Artificial Intelligence (IAI), De Montfort University, Leicester LE1 9BH, United
    Kingdom (e-mail: sacalderon@itcr.ac.cr, syang@dmu.ac.uk, elizondo@dmu.ac.uk).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Saul Calderon-Ramirez, Shengxiang Yang 和 David Elizondo 手稿收到日期：2022年2月7日。S.
    Calderon-Ramirez, S. Yang 和 D. Elizondo 现为英国莱斯特德蒙福特大学人工智能研究所（IAI）成员（电子邮件：sacalderon@itcr.ac.cr,
    syang@dmu.ac.uk, elizondo@dmu.ac.uk）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract
- en: Deep learning methodologies have been employed in several different fields,
    with an outstanding success in image recognition applications, such as material
    quality control, medical imaging, autonomous driving, etc. Deep learning models
    rely on the abundance of labelled observations to train a prospective model. These
    models are composed of millions of parameters to estimate, increasing the need
    of more training observations. Frequently it is expensive to gather labelled observations
    of data, making the usage of deep learning models not ideal, as the model might
    over-fit data. In a semi-supervised setting, unlabelled data is used to improve
    the levels of accuracy and generalization of a model with small labelled datasets.
    Nevertheless, in many situations different unlabelled data sources might be available.
    This raises the risk of a significant distribution mismatch between the labelled
    and unlabelled datasets. Such phenomena can cause a considerable performance hit
    to typical semi-supervised deep learning frameworks, which often assume that both
    labelled and unlabelled datasets are drawn from similar distributions. Therefore,
    in this paper we study the latest approaches for semi-supervised deep learning
    for image recognition. Emphasis is made in semi-supervised deep learning models
    designed to deal with a distribution mismatch between the labelled and unlabelled
    datasets. We address open challenges with the aim to encourage the community to
    tackle them, and overcome the high data demand of traditional deep learning pipelines
    under real-world usage settings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法已被应用于多个不同领域，在图像识别应用中取得了显著成功，如材料质量控制、医学成像、自动驾驶等。深度学习模型依赖大量标注观察数据来训练一个潜在模型。这些模型由数百万个参数组成，需要更多的训练观察数据。收集标注数据通常成本较高，使得深度学习模型的使用并不理想，因为模型可能会对数据过拟合。在半监督设置中，未标注的数据用于提高模型在小规模标注数据集上的准确性和泛化能力。然而，在许多情况下，可能会有不同的未标注数据源。这增加了标注数据集与未标注数据集之间存在显著分布不匹配的风险。这种现象可能会对典型的半监督深度学习框架造成显著性能损失，因为这些框架通常假设标注和未标注数据集来自类似的分布。因此，本文研究了用于图像识别的半监督深度学习的最新方法。重点介绍了旨在处理标注和未标注数据集之间分布不匹配的半监督深度学习模型。我们讨论了开放挑战，旨在鼓励社区应对这些挑战，并克服传统深度学习流程在实际使用中的高数据需求。
- en: 'Impact statement: This paper is a deep review of the state of the art semi-supervised
    deep learning methods, focusing on methods dealing with the distribution mismatch
    setting. Under real world usage scenarios, a distribution mismatch might occur
    between the labelled and unlabelled datasets. Recent research has found an important
    performance degradation of the state of the art semi-supervised deep learning
    (SSDL) methods. Therefore, state of the art methodologies aim to increase the
    robustness of semi-supervised deep learning frameworks to this phenomena. In this
    work, we are the first to our knowledge to systematize and study recent approaches
    to robust SSDL under distribution mismatch scenarios. We think this work can add
    value to the literature around this subject, as it identifies the main tendencies
    surrounding it. Also we consider that our work encourages the community to draw
    the attention on this emerging subject, which we think is an important challenge
    to address in order to decrease the lab-to-real-world gap of deep learning methodologies.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 影响声明：本文深入综述了最新的半监督深度学习方法，重点关注处理分布不匹配设置的方法。在实际使用场景中，可能会发生标注数据集与未标注数据集之间的分布不匹配。最近的研究发现，最新的半监督深度学习（SSDL）方法存在显著的性能下降。因此，最新的方法旨在提高半监督深度学习框架对这种现象的鲁棒性。在这项工作中，我们是首个系统化和研究在分布不匹配场景下的鲁棒SSDL方法的研究者。我们认为这项工作可以为该领域的文献增添价值，因为它识别了相关的主要趋势。我们还认为，我们的工作鼓励社区关注这一新兴主题，这是一项重要的挑战，需要解决，以缩小深度学习方法在实验室与实际应用之间的差距。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep learning, image classification, Semi-supervised learning, Distribution
    mismatch
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、图像分类、半监督学习、分布不匹配
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Deep learning based approaches continue to provide more accurate results in
    a wide variety of fields, from medicine to biodiversity conservation [[10](#bib.bib10),
    [33](#bib.bib33), [14](#bib.bib14), [69](#bib.bib69), [97](#bib.bib97), [7](#bib.bib7),
    [62](#bib.bib62), [17](#bib.bib17), [12](#bib.bib12)]. Most of deep learning architectures
    rely on the usage of extensively labelled datasets to train models with millions
    of parameters to estimate [[35](#bib.bib35), [16](#bib.bib16), [13](#bib.bib13)].
    Over-fitting is a frequent issue when implementing a deep learning based solution
    trained with a small, or not representative dataset. Such phenomena often causes
    poor generalization performance during its real world usage. In spite of this
    risk, the acquisition of a sufficiently sized and representative sample, through
    rigorous procedures and standards, is a pending challenge, as argued in [[5](#bib.bib5)].
    Moreover, procedures to determine whether a dataset is large and/or representative
    enough is still an open subject in the literature, as discussed in [[58](#bib.bib58)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的方法在从医学到生物多样性保护等广泛领域继续提供更准确的结果[[10](#bib.bib10), [33](#bib.bib33), [14](#bib.bib14),
    [69](#bib.bib69), [97](#bib.bib97), [7](#bib.bib7), [62](#bib.bib62), [17](#bib.bib17),
    [12](#bib.bib12)]。大多数深度学习架构依赖于使用广泛标注的数据集来训练具有数百万个参数的模型[[35](#bib.bib35), [16](#bib.bib16),
    [13](#bib.bib13)]。当使用小型或不具代表性的数据集训练深度学习解决方案时，过拟合是一个常见问题。这种现象通常会导致在实际使用中表现不佳。尽管存在这种风险，通过严格的程序和标准获取足够大且具代表性的样本仍然是一个悬而未决的挑战，如[[5](#bib.bib5)]所述。此外，如何确定数据集是否足够大和/或具代表性仍然是文献中的一个开放问题，如[[58](#bib.bib58)]所讨论。
- en: Often labels are expensive to generate, especially in fields developed by highly
    trained medical professionals, such as radiologists, pathologists, or psychologists
    [[17](#bib.bib17), [30](#bib.bib30), [10](#bib.bib10), [42](#bib.bib42)]. Examples
    of this include the labelling of hystopathological images, necessary for training
    a deep learning model for its usage in clinical procedures [[17](#bib.bib17)].
    Therefore, there is an increasing interest for dealing with scarce labelled data
    to feed deep learning architectures, stimulated by the success of deep learning
    based models [[63](#bib.bib63)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 标签的生成往往成本高昂，尤其是在由高度训练的医学专业人员，如放射科医生、病理学家或心理学家，开发的领域中[[17](#bib.bib17), [30](#bib.bib30),
    [10](#bib.bib10), [42](#bib.bib42)]。例如，组织病理学图像的标注是训练深度学习模型以用于临床程序所必需的[[17](#bib.bib17)]。因此，对于处理稀缺标注数据以喂养深度学习架构的兴趣日益增加，这一趋势受到基于深度学习模型成功的刺激[[63](#bib.bib63)]。
- en: Among the most popular and simple approaches to deal with limited labelled observations
    and diminish model over-fitting is data augmentation. Data augmentation adds artificial
    observations to the training dataset, using simple transformations of real data
    samples; namely image rotation, flipping, artificial noise addition [[35](#bib.bib35)].
    A description of simple data augmentation procedures for deep learning architectures
    can be found in [[96](#bib.bib96)]. More complex data augmentation techniques
    make use of generative adversarial networks. Generative models approximate the
    data distribution, which can be sampled to create new observations, as seen in
    [[78](#bib.bib78), [102](#bib.bib102), [32](#bib.bib32)] with different applications.
    Data augmentation is implemented in popular deep learning frameworks, such as
    *Pytorch* and *TensorFlow* [[64](#bib.bib64)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 处理有限标注观测数据和减少模型过拟合的最流行和简单的方法之一是数据增强。数据增强通过对真实数据样本进行简单的变换，如图像旋转、翻转和添加人工噪声[[35](#bib.bib35)]，向训练数据集中添加人工观测数据。关于深度学习架构的数据增强的简单程序描述可以参见[[96](#bib.bib96)]。更复杂的数据增强技术利用生成对抗网络。生成模型近似数据分布，可以对其进行采样以创建新观测数据，如[[78](#bib.bib78),
    [102](#bib.bib102), [32](#bib.bib32)] 中不同应用所示。数据增强在流行的深度学习框架中实现，如 *Pytorch* 和
    *TensorFlow* [[64](#bib.bib64)]。
- en: Transfer learning is also a common approach for dealing with the lack of enough
    labels. It first trains a model $f$ with an external or source-labelled dataset,
    hopefully from a similar domain. Secondly, parameters are fine-tuned with the
    intended, or target dataset [[88](#bib.bib88)]. Similar to data augmentation,
    *TensorFlow* and *Pytorch* include the weights of widely used deep learning models
    trained in general purpose datasets as ImageNet [[27](#bib.bib27)], making its
    usage widespread. Its implementation yields better results with more similar source
    and target datasets. A detailed review on deep transfer learning can be found
    in [[84](#bib.bib84)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习也是处理标签不足的常见方法。它首先使用外部或源标签数据集训练模型 $f$，理想情况下该数据集来自类似的领域。其次，用目标数据集对参数进行微调[[88](#bib.bib88)]。类似于数据增强，*TensorFlow*
    和 *Pytorch* 包含了在通用数据集如 ImageNet [[27](#bib.bib27)] 上训练的广泛使用的深度学习模型的权重，使其应用非常广泛。其实现效果更佳时，源数据集和目标数据集越相似。有关深度迁移学习的详细回顾可以参见[[84](#bib.bib84)]。
- en: Another alternative to deal with small labelled datasets is Semi-supervised
    Deep Learning (SSDL) which enables the model to take advantage of unlabelled or
    even noisily-labelled data [[94](#bib.bib94), [47](#bib.bib47)]. As an application
    example, take the problem of training a face based apparent emotion recognition
    model. Unlabelled videos and images of human faces are available on the web, and
    can be fetched with a web crawler. Taking advantage of such unlabelled information
    might yield improved accuracy and generalization for deep learning architectures.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理小规模标注数据集的替代方法是半监督深度学习（SSDL），它使模型能够利用未标注或甚至是噪声标注的数据[[94](#bib.bib94), [47](#bib.bib47)]。以训练基于面部的表情识别模型为例。网络上可以获取到人脸的未标注视频和图像，并可以通过网络爬虫获取。利用这些未标注的信息可能会提高深度学习模型的准确性和泛化能力。
- en: 'One of the first works in the literature regarding semi-supervised learning
    is [[76](#bib.bib76)]; where different methods for using unlabelled data were
    proposed. More recently, with the increasing development and usage of deep learning
    architectures, semi-supervised learning methods are attracting more attention.
    An important number of SSDL frameworks are general enough to allow the usage of
    popular deep learning architectures in different application domains [[63](#bib.bib63)].
    Therefore, we argue that it is necessary to review and study the relationship
    between recent deep learning based semi-supervised techniques, in order to spot
    missing gaps and boost research in the field. Some recent semi-supervised learning
    reviews are already available in the literature. These are detailed in Section
    [I-A](#S1.SS1 "I-A Previous work ‣ I Introduction ‣ Semi-supervised Deep Learning
    for Image Classification with Distribution Mismatch: A Survey"). Moreover, we
    argue that it is important to discuss the open challenges of implementing SSDL
    in real-world settings, to narrow the lab-application gap. One of the remaining
    challenges is the frequent distribution mismatch between the labelled and unlabelled
    data, which can hinder the performance of the SSDL framework.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '文献中关于半监督学习的最早工作之一是[[76](#bib.bib76)]；其中提出了使用未标记数据的不同方法。近年来，随着深度学习架构的不断发展和使用，半监督学习方法正吸引越来越多的关注。许多SSDL框架足够通用，可以在不同应用领域中使用流行的深度学习架构[[63](#bib.bib63)]。因此，我们认为有必要回顾和研究近期基于深度学习的半监督技术之间的关系，以发现缺失的环节并推动该领域的研究。文献中已经有一些最近的半监督学习综述。这些综述在[
    I-A](#S1.SS1 "I-A Previous work ‣ I Introduction ‣ Semi-supervised Deep Learning
    for Image Classification with Distribution Mismatch: A Survey")部分中详细介绍。此外，我们认为讨论在实际环境中实施SSDL的开放挑战也很重要，以缩小实验室与应用之间的差距。一个剩余的挑战是标记数据和未标记数据之间的频繁分布不匹配，这可能会阻碍SSDL框架的性能。'
- en: I-A Previous work
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 之前的工作
- en: 'In [[101](#bib.bib101)], an extensive review on semi-supervised approaches
    for machine learning was developed. The authors defined the following semi-supervised
    approaches: self-training, co-training, and graph based methods. However, no deep
    learning based concepts were popular by the time of the survey, as auto-encoder
    and generative adversarial networks were less used, given its high computational
    cost and its consequent impractical usage.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[101](#bib.bib101)]中，开发了一项关于机器学习的半监督方法的广泛综述。作者定义了以下半监督方法：自我训练、共同训练和基于图的方法。然而，在调查时，深度学习概念并不流行，因为自编码器和生成对抗网络的使用较少，主要由于其高计算成本和随之而来的不实用性。
- en: Later, a review of semi-supervised learning methods was developed in [[66](#bib.bib66)].
    In this work, authors enlist self-training, co-training, transductive support
    vector machines, multi-view learning and generative discriminative approaches.
    Still deep learning architectures were not popular by the time. Thus, semi-supervised
    architectures based on more traditional machine learning methods are reviewed
    in such work.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，在[[66](#bib.bib66)]中开发了一项半监督学习方法的综述。在这项工作中，作者列出了自我训练、共同训练、传导支持向量机、多视角学习和生成判别方法。然而，到那时深度学习架构尚不流行。因此，相关工作回顾了基于更传统机器学习方法的半监督架构。
- en: 'A brief survey in semi-supervised learning for image analysis and natural language
    processing was developed in [[67](#bib.bib67)]. The study defines the following
    semi-supervised learning approaches: generative models, self-training, co-training,
    multi-view learning and graph based models. This review, however, does not focus
    on deep semi-supervised learning approaches.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[67](#bib.bib67)]中开发了一项关于图像分析和自然语言处理的半监督学习简要调查。该研究定义了以下半监督学习方法：生成模型、自我训练、共同训练、多视角学习和基于图的方法。然而，该综述并未关注深度半监督学习方法。
- en: A more recent survey on semi-supervised learning for medical imaging can be
    found in [[21](#bib.bib21)], with different machine learning based approaches
    listed. Authors distinguished self-training, graph based, co-training, and manifold
    regularization approaches for semi-supervised learning. More medical imaging solutions
    based on transfer learning than semi-supervised learning were found by the authors,
    given its simplicity of implementation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于医学影像的半监督学习的最新调查可以在[[21](#bib.bib21)]中找到，其中列出了不同的机器学习方法。作者区分了自我训练、基于图的方法、共同训练和流形正则化方法。由于其实现的简单性，作者发现基于迁移学习的医学影像解决方案比半监督学习更多。
- en: In [[63](#bib.bib63)], authors experimented with some recent SSDL architectures
    and included a short review. The authors argued that typical testing of semi-supervised
    techniques is not enough to measure its performance in real-world applications.
    For instance, common semi-supervised learning benchmarks do not include unlabelled
    datasets with observations from classes not defined in the labelled data. This
    is referred to as distractor classes or collective outliers [[79](#bib.bib79)].
    The authors also highlight the lack of tests around the interaction of semi-supervised
    learning pipelines with other types of learning, namely transfer learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[63](#bib.bib63)]中，作者实验了一些最近的SSDL架构，并进行了简要的综述。作者认为，半监督技术的典型测试不足以衡量其在实际应用中的性能。例如，常见的半监督学习基准不包括来自未标记数据集中定义之外类别的观察。这被称为干扰类别或集体异常值[[79](#bib.bib79)]。作者还强调了半监督学习管道与其他类型学习（即迁移学习）的互动测试缺乏。
- en: 'More recently, in [[92](#bib.bib92)], the authors extensively review different
    semi-supervised learning frameworks, mostly for deep learning architectures. A
    detailed concept framework around the key assumptions of most SSDL (low density/clustering
    and the manifold assumptions) is developed. The taxonomy proposed for semi-supervised
    methods include two major categories: inductive and transductive based methods.
    Inductive methods build a mathematical model or function that can be used for
    new points in the input space, while transductive methods do not. According to
    the authors, significantly more semi-supervised inductive methods can be found
    in the literature. These methods can be further categorized into: unsupervised
    pre-processing, wrapper based, and intrinsically semi-supervised methods [[92](#bib.bib92)].
    The authors mentioned the distribution mismatch challenge for semi-supervised
    learning introduced in [[63](#bib.bib63)]. However, no focus on techniques around
    this subject was done in their review.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在[[92](#bib.bib92)]中，作者对不同的半监督学习框架进行了广泛的回顾，主要针对深度学习架构。开发了一个围绕大多数SSDL（低密度/聚类和流形假设）的关键假设的详细概念框架。提出的半监督方法的分类包括两个主要类别：归纳和推断方法。归纳方法建立一个可以用于输入空间中新点的数学模型或函数，而推断方法则不建立。根据作者的说法，文献中可以找到显著更多的半监督归纳方法。这些方法可以进一步分为：无监督预处理、包装型和内在半监督方法[[92](#bib.bib92)]。作者提到[[63](#bib.bib63)]中提出的半监督学习的分布不匹配挑战。然而，他们的综述中没有关注这一主题的技术。
- en: In [[74](#bib.bib74)], a review of semi-, self- and unsupervised learning methods
    for image classification was developed. The authors focus on the common concepts
    used in these methods (most of them based on deep learning architectures). Concepts
    such as pretext or proxy task learning, data augmentation, contrastive optimization,
    etc., are described as common ideas within the three learning approaches. A useful
    set of tables describing the different semi-supervised learning approaches along
    with the common concepts is included in this work. After reviewing the yielded
    results of recent semi-supervised methods, the authors conclude that few of them
    include benchmarks closer to real-world (high resolution images, with similar
    features for each class). Also, real-world settings, such as class imbalance and
    noisy labels are often missing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[74](#bib.bib74)]中，开发了一个关于图像分类的半监督、自监督和无监督学习方法的综述。作者关注于这些方法中使用的共同概念（大多数基于深度学习架构）。例如，预任务或代理任务学习、数据增强、对比优化等概念被描述为这三种学习方法中的共同理念。文中包含了一组有用的表格，描述了不同的半监督学习方法及其共同概念。在回顾了最近半监督方法的结果后，作者总结道，其中少数方法包含了更接近实际世界的基准（高分辨率图像，每个类别具有相似特征）。此外，实际世界设置，如类别不平衡和标签噪声，通常被忽略。
- en: We argue that a detailed survey in SSDL is still missing, as common short reviews
    included in SSDL papers usually focus on its closest related works. The most recent
    semi-supervised learning surveys we have found are outdated and do not focus on
    deep learning based approaches. We argue that recent SSDL approaches add new perspectives
    to the semi-supervised learning framework. However, more importantly, to narrow
    the lab-to-application gap, it is necessary to fully study the state of the art
    in the efforts to address such challenges. In the context of SSDL, we consider
    that increasing the robustness of SSDL methods to the distribution mismatch between
    the labelled and unlabelled datasets is key. Therefore, this review focuses on
    the distribution mismatch problem between the labelled and the unlabelled datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，关于SSDL的详细调查仍然缺失，因为SSDL论文中包含的常见简短评论通常关注于其最相关的工作。我们发现的最新半监督学习调查已经过时，并且没有关注基于深度学习的方法。我们认为，近期的SSDL方法为半监督学习框架提供了新的视角。然而，更重要的是，为了缩小实验室到应用的差距，有必要充分研究当前在解决这些挑战方面的最新进展。在SSDL的背景下，我们认为，提高SSDL方法对标记数据集和未标记数据集之间分布不匹配的鲁棒性是关键。因此，本综述集中讨论了标记数据集和未标记数据集之间的分布不匹配问题。
- en: 'In Section [I-B](#S1.SS2 "I-B Semi-supervised learning ‣ I Introduction ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey"),
    a review of the main ideas of semi-supervised learning is carried out. Based on
    the concepts of both previous sections, in Section [II](#S2 "II Semi-supervised
    Deep Learning ‣ Semi-supervised Deep Learning for Image Classification with Distribution
    Mismatch: A Survey") we review the main approaches for SSDL. Later we address
    the different methods developed so far in the literature regarding SSDL when facing
    a distribution mismatch between $S^{(u)}$ and $S^{(l)}$. Finally, we discuss the
    pending challenges of SSDL under distribution mismatch conditions in Section [IV](#S4
    "IV Open challenges ‣ Semi-supervised Deep Learning for Image Classification with
    Distribution Mismatch: A Survey").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '在节 [I-B](#S1.SS2 "I-B Semi-supervised learning ‣ I Introduction ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    中，对半监督学习的主要思想进行了回顾。基于前两节的概念，在节 [II](#S2 "II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    中，我们回顾了SSDL的主要方法。随后，我们讨论了文献中已开发的不同方法，这些方法在面对$S^{(u)}$和$S^{(l)}$之间的分布不匹配时的表现。最后，我们在节
    [IV](#S4 "IV Open challenges ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey") 中讨论了SSDL在分布不匹配条件下的待解决挑战。'
- en: I-B Semi-supervised learning
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 半监督学习
- en: In this section we describe the key terminology to analyze SSDL in more detail.
    We based our terminology on the semi-supervised learning analytical framework
    developed in [[4](#bib.bib4)]. This framework extends the learning framework proposed
    in [[90](#bib.bib90)], as a machine learning theoretical framework.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了分析SSDL的关键术语。我们的术语基于在[[4](#bib.bib4)]中开发的半监督学习分析框架。该框架扩展了在[[90](#bib.bib90)]中提出的学习框架，作为一个机器学习理论框架。
- en: A model $f_{\textbf{w}}$ is said to be semi-supervised, if it is trained using
    a set of labelled observations $S^{(l)}$, along a set of unlabelled observations
    $S^{(u)}=\left\{\mathbf{x}_{n_{1}},\mathbf{x}_{n_{2}},\ldots,\mathbf{x}_{n_{u}}\right\},$
    with the total number of observations $n=n_{l}+n_{u}$. Frequently, the number
    of unlabelled observations $n_{u}$ is considerably higher than the number of labelled
    observations. This makes $n_{u}\gg n_{l}$, as labels are expensive to obtain in
    different domains. If the model $f_{\textbf{w}}$ corresponds to a Deep Neural
    Network (DNN), we refer to SSDL. The deep model $f_{\textbf{w}}$ is often referred
    to as a back-bone model. In semi-supervised learning, additional information is
    extracted from an unlabelled dataset $S^{(u)}$. Therefore, training a deep model
    can be extended to $f_{\mathbf{w}}=T\left(S^{(l)},S^{(u)},f_{\mathbf{w}}\right)$.
    The estimated hypothesis should classify test data in $\mathbf{x}\in S^{(t)}$with
    a higher accuracy than just using the labelled data $S^{(l)}$.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型 $f_{\textbf{w}}$ 是半监督的，则它是使用一组标记观察 $S^{(l)}$ 和一组未标记观察 $S^{(u)}=\left\{\mathbf{x}_{n_{1}},\mathbf{x}_{n_{2}},\ldots,\mathbf{x}_{n_{u}}\right\},$
    以及总观察数 $n=n_{l}+n_{u}$ 进行训练的。通常，未标记观察 $n_{u}$ 的数量远高于标记观察的数量。这使得 $n_{u}\gg n_{l}$，因为在不同领域中获取标签的成本很高。如果模型
    $f_{\textbf{w}}$ 对应于深度神经网络（DNN），我们称之为半监督深度学习（SSDL）。深度模型 $f_{\textbf{w}}$ 通常被称为骨干模型。在半监督学习中，从未标记数据集
    $S^{(u)}$ 中提取附加信息。因此，训练深度模型可以扩展到 $f_{\mathbf{w}}=T\left(S^{(l)},S^{(u)},f_{\mathbf{w}}\right)$。估计的假设应该比仅使用标记数据
    $S^{(l)}$ 更准确地对测试数据 $\mathbf{x}\in S^{(t)}$ 进行分类。
- en: 'Figure 1: Semi-supervised setting, circles represent the unlabelled observations
    $S^{(u)}$, the filled shapes correspond to labelled observations $S^{(l)}$ of
    $K=2$ classes, and the yellow circles correspond to unlabelled observations or
    members of the distractor class.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：半监督设置，圆圈代表未标记的观察 $S^{(u)}$，填充形状对应于 $K=2$ 类的标记观察 $S^{(l)}$，黄色圆圈对应于未标记的观察或干扰类的成员。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction
    ‣ Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey") plots a semi-supervised setting with observations in $d=2$ dimensions.
    The label can also correspond to an array, $\mathbf{y}_{i}\in\mathbb{R}^{k}$,
    in case of using a $1-K$ encoding (one-hot vector) for classifying observations
    in $K$ classes, or $y_{i}\in\mathbb{R}$ for regression. More specifically, observations
    of both the labelled and unlabelled dataset belong to the observation space $\mathbf{x}_{i}\in\mathcal{X}$
    and labels lie within the label space $\mathcal{Y}$. For instance, observation
    for binary images of written digits with $d$ pixels would make up for an observation
    space $\mathcal{X}\in\{0,1\}^{d}$, and its label set is given as $\mathcal{Y}=\left\{0,1,\ldots,9\right\}$.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    描绘了一个在 $d=2$ 维度中观察的半监督设置。标签也可以对应于数组 $\mathbf{y}_{i}\in\mathbb{R}^{k}$，如果使用 $1-K$
    编码（独热向量）来对 $K$ 类的观察进行分类，或 $y_{i}\in\mathbb{R}$ 用于回归。更具体地说，标记和未标记数据集的观察都属于观察空间
    $\mathbf{x}_{i}\in\mathcal{X}$，标签位于标签空间 $\mathcal{Y}$ 中。例如，对于具有 $d$ 像素的手写数字的二值图像，其观察空间
    $\mathcal{X}\in\{0,1\}^{d}$，其标签集为 $\mathcal{Y}=\left\{0,1,\ldots,9\right\}$。'
- en: The concept class $\mathcal{C}_{k}$ corresponds to all the valid combinations
    of values in the array $\mathbf{x}_{i}\in\mathbb{R}^{d}$ for a specific class
    $k$. For example, for the digit $1$, a subset of all possible of observations
    that belong to class $k$ belong to the concept $\mathcal{C}_{k}$. The concept
    class models all the possible images of the digit $1$. In such case $\mathbf{x}_{i}\in\mathcal{C}_{k=1}$.
    The concept class $\mathcal{C}=\left\{\mathcal{C}_{1},\ldots,\mathcal{C}_{k}\right\}$
    includes all the possible observations which can be drawn for all the existing
    classes in a given problem domain.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 概念类 $\mathcal{C}_{k}$ 对应于特定类别 $k$ 中数组 $\mathbf{x}_{i}\in\mathbb{R}^{d}$ 中所有有效值的组合。例如，对于数字
    $1$，属于类别 $k$ 的所有可能观察的子集属于概念 $\mathcal{C}_{k}$。概念类模型表示所有可能的数字 $1$ 的图像。在这种情况下 $\mathbf{x}_{i}\in\mathcal{C}_{k=1}$。概念类
    $\mathcal{C}=\left\{\mathcal{C}_{1},\ldots,\mathcal{C}_{k}\right\}$ 包含了给定问题领域中所有现有类别的所有可能观察。
- en: From a data distribution perspective, usually the population density function
    of the concept class $p_{\mathbf{x}\sim\mathcal{C}}\left(\mathbf{x}\right)=p\left(\mathbf{x}|y=1,\ldots,K\right)$
    and the density for each concept $p_{\mathbf{x}\sim\mathcal{C}_{k}}\left(\mathbf{x}\right)=p\left(\mathbf{x}|y=k\right)$
    is unknown. Most semi-supervised methods assume that both $S^{(u)}$ and labelled
    data $S^{(l)}$ sample the concept class density, making $p_{\mathbf{x}\sim S^{(l)}}\left(\mathbf{x}\right)$
    and $p_{\mathbf{x}\sim S^{(u)}}\left(\mathbf{x}\right)$ very similar [[92](#bib.bib92)].
    A labelled and an unlabelled dataset, $S^{(l)}$ and $S^{(u)}$, respectively, are
    said to be identically and independently sampled if the density functions $p_{\mathbf{x}\sim
    S^{(u)}}$ and $p_{\mathbf{x}\sim S^{(l)}}$ are identical and are statistically
    independent.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据分布的角度来看，通常情况下概念类别的总体密度函数 $p_{\mathbf{x}\sim\mathcal{C}}\left(\mathbf{x}\right)=p\left(\mathbf{x}|y=1,\ldots,K\right)$
    以及每个概念的密度函数 $p_{\mathbf{x}\sim\mathcal{C}_{k}}\left(\mathbf{x}\right)=p\left(\mathbf{x}|y=k\right)$
    都是未知的。大多数半监督方法假设 $S^{(u)}$ 和有标签数据 $S^{(l)}$ 都采样自概念类别密度，使得 $p_{\mathbf{x}\sim S^{(l)}}\left(\mathbf{x}\right)$
    和 $p_{\mathbf{x}\sim S^{(u)}}\left(\mathbf{x}\right)$ 非常相似 [[92](#bib.bib92)]。标记的数据集
    $S^{(l)}$ 和未标记的数据集 $S^{(u)}$ 被认为是独立同分布采样的，如果密度函数 $p_{\mathbf{x}\sim S^{(u)}}$
    和 $p_{\mathbf{x}\sim S^{(l)}}$ 是相同且统计独立的。
- en: 'However, in real-world settings different violations to the Independent and
    Identically Distributed (IID) assumption can be faced. For instance, unlabelled
    data is likely to contain observations which might not belong to any of the $K$
    classes. Potentially, this could lead to a different sampled density function
    from $p_{\mathbf{x}\sim\mathcal{C}}\left(\mathbf{x}\right)$. These observations
    belong to a distractor class dataset $\mathbf{x}\in\mathcal{D}$, and are drawn
    from a theoretical distribution of the distractor class $p_{\mathbf{x}\sim\mathcal{D}}\left(\mathbf{x}\right)$.
    Figure [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction ‣
    Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey") shows distractor observations drawn from a distractor distribution
    $p_{\mathbf{x}\sim\mathcal{D}}\left(\mathbf{x}\right)$ in yellow.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，现实世界的设置可能面临不同的违反独立同分布（IID）假设。例如，未标记的数据很可能包含不属于 $K$ 个类别之一的观测。从理论上讲，这可能导致从
    $p_{\mathbf{x}\sim\mathcal{C}}\left(\mathbf{x}\right)$ 不同的采样密度函数。这些观测属于干扰类别数据集
    $\mathbf{x}\in\mathcal{D}$，并且是从干扰类别的理论分布 $p_{\mathbf{x}\sim\mathcal{D}}\left(\mathbf{x}\right)$
    中抽取的。图 [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction ‣
    Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey") 展示了从干扰分布 $p_{\mathbf{x}\sim\mathcal{D}}\left(\mathbf{x}\right)$ 抽取的干扰观测。'
- en: A subset of unlabelled observations from $S^{(u)}$, referred to as ${S^{(u)}}_{D}$
    are said to belong to a distractor class, if they are drawn from a different distribution
    than the observations that belong to the concept classes. The distractor class
    is frequently semantically different than the concept classes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从与概念类别不同的分布中抽取的未标记观测 ${S^{(u)}}_{D}$ 被称为干扰类别，则干扰类别通常在语义上与概念类别有所不同。
- en: 'Different causes for a violation to the IID assumption for $S^{(u)}$ and $S^{(l)}$
    might be faced in real-world settings. These are enlisted as follows, and can
    be found with different degrees [[45](#bib.bib45)]:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的情况下，可能会面临多种导致 $S^{(u)}$ 和 $S^{(l)}$ 违反独立同分布（IID）假设的原因。以下列出了这些原因，并且可以在不同程度上找到
    [[45](#bib.bib45)]：
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prior probability shift: The label distribution in the dataset $S^{(l)}$ might
    differ when compared to $S^{(u)}$. A specific case would be the label imbalance
    of the labelled dataset $S^{(l)}$ and a balanced unlabelled dataset.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 先验概率偏移：数据集 $S^{(l)}$ 的标签分布可能与 $S^{(u)}$ 不同。一个特殊的例子是标记数据集 $S^{(l)}$ 的标签不平衡和一个平衡的未标记数据集。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Covariate shift: A difference in the feature distributions between the datasets
    $S^{(l)}$ with respect to $S^{(u)}$ with the same classes in both, might be sampled,
    leading to a distribution mismatch. In a medical imaging application, for example,
    this can be related to the difference in the distribution of the sampled features
    between $S^{(l)}$ and $S^{(u)}$. This can be caused by the difference of the patients
    sample.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 协变量偏移：数据集 $S^{(l)}$ 和数据集 $S^{(u)}$ 之间的特征分布可能存在差异，尽管两者都具有相同的类别，在采样时可能导致分布不匹配。例如，在医学成像应用中，这可能与
    $S^{(l)}$ 和 $S^{(u)}$ 之间抽样特征分布的差异有关。这可能是由于患者样本的差异引起的。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Concept shift: This is associated to a shift in the labels of $S^{(l)}$ with
    respect to $S^{(u)}$ of data with the same features. For example, in the medical
    imaging domain, different practitioners might categorize the same x-ray image
    into different classes. This is very related to the problem of noisy labelling
    [[31](#bib.bib31)].'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 概念漂移：这与 $S^{(l)}$ 中标签的变化相关，相对于具有相同特征的 $S^{(u)}$ 数据。例如，在医学影像领域，不同的从业者可能将相同的 x
    光图像分类为不同的类别。这与噪声标签的问题密切相关 [[31](#bib.bib31)]。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unseen classes: The dataset $S^{(u)}$ contains observations of unseen or unrepresented
    classes in the dataset $S^{(l)}$. One or more distractor classes are sampled in
    the unlabelled dataset. Therefore, a mismatch in the number of labels exist, along
    with a prior probability shift and a feature distribution mismatch.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未见类别：数据集 $S^{(u)}$ 包含了在数据集 $S^{(l)}$ 中未见或未表示的类别的观察值。未标记数据集中采样了一个或多个干扰类。因此，存在标签数量的不匹配，以及先验概率偏移和特征分布不匹配。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction
    ‣ Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey") illustrates a distribution mismatch setting. The circles correspond
    to unlabelled data and the squares and diamonds to the labelled dataset. The labelled
    and unlabelled data for the two classes are clearly imbalanced and sample different
    feature values. In this case, all the blue unlabelled observations are drawn from
    the concept classes. However, the yellow unlabelled observations, can be considered
    to have different feature value distributions. Many SSDL methods make usage of
    the clustered-data/low-density separation assumption together with the manifold
    hypothesis [[70](#bib.bib70)].'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    说明了一个分布不匹配的设置。圆圈对应于未标记的数据，方块和菱形对应于标记的数据集。这两个类别的标记和未标记数据明显不平衡，并且样本具有不同的特征值。在这种情况下，所有的蓝色未标记观察值都来自概念类别。然而，黄色未标记观察值可以被认为具有不同的特征值分布。许多
    SSDL 方法利用了簇状数据/低密度分离假设以及流形假设 [[70](#bib.bib70)]。'
- en: II Semi-supervised Deep Learning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 半监督深度学习
- en: In this section we study recent semi-supervised deep learning architectures.
    They are divided into different categories. Such categorization is meant to ease
    its analysis. However each category is not mutually exclusive with the rest of
    them, as there are several methods that mix concepts of two or more categories.
    This serves as a background to understand current SSDL approaches to deal with
    the distribution mismatch between $S^{(u)}$ and $S^{(l)}$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们研究了最近的半监督深度学习架构。它们被分为不同的类别。这样的分类旨在简化分析。然而，每个类别并不是彼此排斥的，因为有几个方法混合了两个或更多类别的概念。这为理解当前
    SSDL 方法处理 $S^{(u)}$ 和 $S^{(l)}$ 之间的分布不匹配提供了背景。
- en: II-A Pre-training for semi-supervised deep learning
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 半监督深度学习的预训练
- en: 'A basic approach to leverage the information from an unlabelled dataset $S^{(u)}$,
    is to perform as a first step an unsupervised pre-training of the classifier $f_{\textbf{w}}$.
    In this document we refer to it as Pre-trained Semi-Supervised deep learning (PT-SSDL).
    A straightforward way to implement PT-SSDL, is to pre-train the encoding section
    of the model $h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$
    to optimize a *proxy* or *pretext [[89](#bib.bib89)]* task $\delta$. The proxy
    task does not need the specific labels, allowing the usage of unlabelled data.
    This proxy loss is minimized during training, and enables the usage of unlabelled
    data:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 利用未标记数据集 $S^{(u)}$ 信息的一种基本方法是，首先对分类器 $f_{\textbf{w}}$ 进行无监督预训练。在本文中，我们称之为预训练的半监督深度学习（PT-SSDL）。实施
    PT-SSDL 的一种直接方法是，预训练模型的编码部分 $h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$，以优化
    *代理* 或 *预设 [[89](#bib.bib89)]* 任务 $\delta$。代理任务不需要具体标签，允许使用未标记数据。在训练过程中最小化代理损失，从而实现未标记数据的使用。
- en: '|  | $\mathcal{\>L}_{u}^{\left(p\right)}\left(S^{(u)},\textbf{w}_{\textrm{FE}}\right)=\>\sum_{\textbf{x}_{i}\in
    S^{(u)}}\delta\left(r_{i},f_{\textrm{proxy}}\left(f_{\textbf{w}_{\textrm{FE}}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)\right)\right),$
    |  | (1) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{\>L}_{u}^{\left(p\right)}\left(S^{(u)},\textbf{w}_{\textrm{FE}}\right)=\>\sum_{\textbf{x}_{i}\in
    S^{(u)}}\delta\left(r_{i},f_{\textrm{proxy}}\left(f_{\textbf{w}_{\textrm{FE}}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)\right)\right),$
    |  | (1) |'
- en: where the function $\delta$ compares the proxy label $r_{i}$ with the output
    of the proxy model $f_{\textrm{proxy}}$. The proxy task can be optimized also
    using labelled data. The process of optimizing a proxy task is also known as self-supervision
    [[44](#bib.bib44)]. This can be done in a pre-training step or during training,
    as seen in the models with unsupervised regularization. A simple approach for
    this *proxy* or *auxiliary* loss is to minimize the unsupervised reconstruction
    error. This is similar to the usage of a consistency function $\delta$, where
    the proxy task corresponds to reconstruct the input, making $\delta\left(\textbf{x}_{i},h_{\textbf{w}_{\textrm{DE}}}^{\textrm{(DE)}}\left(h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)\right)\right)$.
    The usage of an auto-encoder based reconstruction means it is usually necessary
    to add a decoder path $h_{\textbf{w}_{\textrm{DE}}}^{\textrm{(DE)}}$, which is
    later discarded at evaluation time. Pre-training can be performed for the whole
    model, or in a per-layer fashion, as initially explored in [[6](#bib.bib6)]. Moreover,
    pre-training can be easily combined with other semi-supervised techniques, as
    seen in [[50](#bib.bib50)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中函数 $\delta$ 比较代理标签 $r_{i}$ 与代理模型 $f_{\textrm{proxy}}$ 的输出。代理任务也可以使用标记数据进行优化。优化代理任务的过程也称为自监督
    [[44](#bib.bib44)]。这可以在预训练步骤或训练过程中完成，如在具有无监督正则化的模型中所见。对于这种*代理*或*辅助*损失，一种简单的方法是最小化无监督重建误差。这类似于一致性函数
    $\delta$ 的使用，其中代理任务对应于重建输入，使得 $\delta\left(\textbf{x}_{i},h_{\textbf{w}_{\textrm{DE}}}^{\textrm{(DE)}}\left(h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)\right)\right)$。使用基于自动编码器的重建意味着通常需要添加一个解码器路径
    $h_{\textbf{w}_{\textrm{DE}}}^{\textrm{(DE)}}$，在评估时将其丢弃。预训练可以针对整个模型进行，或者逐层进行，如
    [[6](#bib.bib6)] 中最初探讨的那样。此外，预训练可以很容易地与其他半监督技术结合，如 [[50](#bib.bib50)] 中所见。
- en: In [[28](#bib.bib28)], a Convolutional Neural Network (CNN) is pre-trained with
    image patches from unlabelled data, with the proxy task of predicting the position
    of a new second patch. The approach was tested in object detection benchmarks.
    In [[18](#bib.bib18)] an unsupervised pre-training approach was proposed. It implements
    a proxy task optimization followed by a clustering step, both using unlabelled
    data. The proxy task consists of the random rotation of the unlabelled data, and
    the prediction of its rotation. The proposed method was tested against other unsupervised
    pre-training methods, using the PASCAL Visual Object Classes 2007 dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[28](#bib.bib28)] 中，一个卷积神经网络（CNN）通过来自未标记数据的图像补丁进行预训练，代理任务是预测新第二个补丁的位置。该方法在物体检测基准中进行了测试。在
    [[18](#bib.bib18)] 中，提出了一种无监督预训练方法。它实现了代理任务优化，随后进行聚类步骤，均使用未标记数据。代理任务包括未标记数据的随机旋转以及其旋转的预测。所提方法在使用
    PASCAL Visual Object Classes 2007 数据集的其他无监督预训练方法中进行了测试。
- en: The proxy or auxiliary task is implemented in different manners in SSDL, as
    it is not exclusive to pre-training methods. This can be seen in consistency based
    regularization techniques, later discussed in this work. For instance in [[98](#bib.bib98)],
    an extensive set of proxy tasks are added as an unsupervised regularization term,
    and compared with some popular regularized SSDL methods. The authors used the
    ImageNet Large Scale Visual Recognition Challenge (ILSVRC) for the executed benchmarks.
    The proposed method showed a slight accuracy gain, with no statistical significance,
    against other two unsupervised regularization based methods.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 代理或辅助任务在 SSDL 中以不同方式实现，因为它并不限于预训练方法。这可以在基于一致性的正则化技术中看到，后者将在本工作中进一步讨论。例如，在 [[98](#bib.bib98)]
    中，添加了一组广泛的代理任务作为无监督正则化项，并与一些流行的正则化 SSDL 方法进行了比较。作者使用了 ImageNet Large Scale Visual
    Recognition Challenge（ILSVRC）进行执行基准测试。所提方法显示出略微的准确性提升，但与其他两种基于无监督正则化的方法相比，统计学上并不显著。
- en: II-B Pseudo-label semi-supervised deep learning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 伪标签半监督深度学习
- en: In Pseudo-label Semi-Supervised deep learning (PLT-SSDL) or also known as self-training,
    self-teaching or bootstrapping, pseudo-labels are estimated for unlabelled data,
    and used for model fine-tuning. A straightforward approach of pseudo-label based
    training consisting in co-training two models can be found in [[4](#bib.bib4)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪标签半监督深度学习（PLT-SSDL），也称为自训练、自教学或引导训练中，伪标签被估计用于未标记数据，并用于模型微调。基于伪标签的训练的直接方法可以在
    [[4](#bib.bib4)] 中找到，其中涉及两个模型的共同训练。
- en: In co-training, two or more different sets of input dimensions or views are
    used to train two or more different models. Such views can be just the result
    of splitting the original input array $\textbf{x}_{i}$. For instance, in two-views
    $v_{1}$ and $v_{2}$ co-training [[4](#bib.bib4)], two labelled datasets $S^{\left(l,v_{1}\right)}$
    and $S^{\left(l,v_{2}\right)}$ are used. In an initial iteration $i=1$, two models
    are trained using the labelled dataset, yielding the two view models $\widetilde{\textbf{w}}_{i}^{\left(v_{1}\right)}=T\left(f_{\textbf{w}},S^{\left(l,v_{1}\right)}\right)$
    and $\widetilde{\textbf{w}}_{i}^{\left(v_{1}\right)}=T\left(f_{\textbf{w}},S^{\left(l,v_{2}\right)}\right)$.
    This can be considered as a pre-training step. The resulting models can be referred
    to as an ensemble of models $\textbf{f}_{\widetilde{\textbf{w}}_{1}}=\left[f_{\widetilde{\textbf{w}}_{1}^{\left(v_{2}\right)}},f_{\widetilde{\textbf{w}}_{2}^{\left(v_{2}\right)}}\right]$.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在协同训练中，使用两个或更多不同的输入维度或视图来训练两个或更多不同的模型。这些视图可以仅仅是拆分原始输入数组$\textbf{x}_{i}$的结果。例如，在两个视图$v_{1}$和$v_{2}$的协同训练[[4](#bib.bib4)]中，使用了两个标记数据集$S^{\left(l,v_{1}\right)}$和$S^{\left(l,v_{2}\right)}$。在初始迭代$i=1$中，使用标记数据集训练两个模型，得到两个视图模型$\widetilde{\textbf{w}}_{i}^{\left(v_{1}\right)}=T\left(f_{\textbf{w}},S^{\left(l,v_{1}\right)}\right)$和$\widetilde{\textbf{w}}_{i}^{\left(v_{1}\right)}=T\left(f_{\textbf{w}},S^{\left(l,v_{2}\right)}\right)$。这可以视为预训练步骤。得到的模型可以被称为模型集成$\textbf{f}_{\widetilde{\textbf{w}}_{1}}=\left[f_{\widetilde{\textbf{w}}_{1}^{\left(v_{2}\right)}},f_{\widetilde{\textbf{w}}_{2}^{\left(v_{2}\right)}}\right]$。
- en: As a second step, the disagreement probability $\mathbf{Pr}_{\textbf{x}_{i}\sim
    S^{(u)}}\left[f_{\widetilde{\textbf{w}}_{1}^{\left(v_{2}\right)}}\left(\textbf{x}_{j}\right)\neq
    f_{\widetilde{\textbf{w}}_{2}^{\left(v_{2}\right)}}\left(\textbf{x}_{j}\right)\right]$
    with $\textbf{x}_{j}\in S^{(u)}$ is used to estimate new labels or pseudo-labels
    $\widehat{y}_{j}^{(i,k)}=f_{\widetilde{\textbf{w}}_{i}^{\left(v_{k}\right)}}\left(\textbf{x}_{j}\right)$.
    The final pseudo-labels for each observation $\textbf{x}_{j}$ can be the result
    of applying a view-wise summarizing operation $\mu$ (like averaging or taking
    the maximum logits) making $\widehat{y}_{j}^{(i)}=\mu\left(\textbf{f}_{\textbf{w}_{i}}\left(\textbf{x}_{j}\right)\right)$.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第二步，使用不一致概率$\mathbf{Pr}_{\textbf{x}_{i}\sim S^{(u)}}\left[f_{\widetilde{\textbf{w}}_{1}^{\left(v_{2}\right)}}\left(\textbf{x}_{j}\right)\neq
    f_{\widetilde{\textbf{w}}_{2}^{\left(v_{2}\right)}}\left(\textbf{x}_{j}\right)\right]$，其中$\textbf{x}_{j}\in
    S^{(u)}$，来估计新的标签或伪标签$\widehat{y}_{j}^{(i,k)}=f_{\widetilde{\textbf{w}}_{i}^{\left(v_{k}\right)}}\left(\textbf{x}_{j}\right)$。每个观察值$\textbf{x}_{j}$的最终伪标签可以是应用视图汇总操作$\mu$（如平均或取最大logits）的结果，使得$\widehat{y}_{j}^{(i)}=\mu\left(\textbf{f}_{\textbf{w}_{i}}\left(\textbf{x}_{j}\right)\right)$。
- en: The set of pseudo labels for the iteration i can be represented as $\widehat{S}_{i}=\mu\left(\textbf{f}_{\textbf{w}_{i}}\left(S^{(u)}\right)\right)$.
    In co-training [[4](#bib.bib4)], the agreed observations for the two models are
    picked in the function $\widetilde{S}_{i}=\varphi\left(\widehat{S}_{i}\right)$,
    as highly confident observations. The pseudo-labelled data with high confidence
    are included in the labelled dataset $S_{i+1}^{\left(r\right)}=S^{(l)}\bigcup\widetilde{S}_{i}$
    as pseudo-labelled observations. Later the model is re-trained for $i=2,..,\vartheta$
    iterations repeating the process of pseudo-labelling, filtering the most confident
    pseudo-labels and re-training the model. In general, we refer to a pseudo-labelling
    to the idea of estimating the hard labels $\widehat{y}_{j}^{(i)}$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代$i$的伪标签集合可以表示为$\widehat{S}_{i}=\mu\left(\textbf{f}_{\textbf{w}_{i}}\left(S^{(u)}\right)\right)$。在协同训练[[4](#bib.bib4)]中，两个模型的同意观察被挑选出来，函数为$\widetilde{S}_{i}=\varphi\left(\widehat{S}_{i}\right)$，作为高置信度观察。具有高置信度的伪标签数据被包含在标记数据集$S_{i+1}^{\left(r\right)}=S^{(l)}\bigcup\widetilde{S}_{i}$中作为伪标签观察。之后，模型会在$i=2,..,\vartheta$迭代中重新训练，重复伪标记过程，筛选出最有信心的伪标签并重新训练模型。一般来说，我们将伪标记指代为估计硬标签$\widehat{y}_{j}^{(i)}$的思想。
- en: In [[29](#bib.bib29)] the Tri-net semi-supervised deep model (Tri-Net) was proposed.
    Here an ensemble $\textbf{f}_{\textbf{w}}$ of Deep Convolutional Neural Network
    (DCNN) s is trained with $k=1,2,3$ different top models, with also $k=1,2,3$ different
    labelled datasets $S_{i}^{(l,k)}$. The output posterior probability is the result
    of the three models voting. This results in the pseudo-labelled for the whole
    evaluated dataset $\widetilde{S}_{i}$, with $i=1$ for the first iteration. The
    pseudo-label filtering operation $\varphi$ includes the observations where at
    least two of the models agreed are included into the labelled dataset. The process
    is repeated for a fixed number of iterations. Also Tri-Net can be combined with
    any regularized SSDL approach. This combination was tested in [[82](#bib.bib82)],
    and is referred to in this document as Tri-net semi-supervised deep model with
    a Pi-Model regularization (TriNet+Pi). In [[82](#bib.bib82)], a similar ensemble-based
    pseudo-labelling approach is found. In such work, a mammogram image classifier
    was implemented, with an ensemble of classifiers that vote for the unlabelled
    observations. The observations with the highest confidence are added to the dataset,
    in an iterative fashion.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[29](#bib.bib29)]中提出了Tri-net半监督深度模型（Tri-Net）。这里训练了一个由深度卷积神经网络（DCNN）组成的集成$\textbf{f}_{\textbf{w}}$，使用$k=1,2,3$不同的顶级模型，以及$k=1,2,3$不同的标记数据集$S_{i}^{(l,k)}$。输出的后验概率是三个模型投票的结果。这导致了对整个评估数据集的伪标记$\widetilde{S}_{i}$，其中$i=1$表示第一次迭代。伪标签过滤操作$\varphi$包括至少两个模型一致的观测结果，并将其包含到标记数据集中。该过程重复进行固定次数的迭代。Tri-Net还可以与任何正则化的SSDL方法结合。这种组合在[[82](#bib.bib82)]中进行了测试，并在本文档中称为具有Pi-Model正则化的Tri-net半监督深度模型（TriNet+Pi）。在[[82](#bib.bib82)]中，发现了类似的基于集成的伪标签方法。在该工作中，实现了一个乳腺X光图像分类器，使用一个投票的分类器集成来处理未标记的观测结果。具有最高置信度的观测结果被迭代地添加到数据集中。
- en: Another recent deep self-training approach can be found in [[23](#bib.bib23)],
    named as Speed as a supervisor for semi-supervised Learning Model (SaaSM). In
    a first step, the pseudo-labels are estimated by measuring the learning speed
    in epochs, optimizing the estimated labels as a probability density function $\widetilde{S}_{1}=f_{\textbf{w}_{1}}\left(S^{(u)}\right)$
    with a stochastic gradient descent approach. The estimated labels are used to
    optimize an unsupervised regularized loss. SaaSM was tested using the Canadian
    Institute for Advanced Research dataset of 10 classes (CIFAR-10) and Street View
    House Numbers dataset (SVHN) datasets. It yielded slightly higher accuracy to
    other consistency regularized methods such as mean teacher, according to the reported
    results. No statistical significance analysis was done.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种近期的深度自训练方法可以在[[23](#bib.bib23)]中找到，称为半监督学习模型的速度监督（SaaSM）。在第一步中，通过测量训练周期中的学习速度来估计伪标签，利用随机梯度下降方法优化估计标签作为概率密度函数$\widetilde{S}_{1}=f_{\textbf{w}_{1}}\left(S^{(u)}\right)$。估计标签用于优化无监督正则化损失。SaaSM使用加拿大高级研究院的10类数据集（CIFAR-10）和街景房屋号码数据集（SVHN）进行了测试。根据报告的结果，其准确率略高于其他一致性正则化方法，如均值教师。没有进行统计显著性分析。
- en: II-C Regularized semi-supervised learning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 正则化半监督学习
- en: 'In Regularized Semi-Supervised deep learning (R-SSDL), or co-regularized learning
    as defined in [[101](#bib.bib101)], the loss function of a deep learning model
    $f_{\textbf{w}}$ includes a regularization term using unlabelled data $S^{(u)}$:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在正则化半监督深度学习（R-SSDL）中，或如[[101](#bib.bib101)]所定义的共同正则化学习中，深度学习模型$f_{\textbf{w}}$的损失函数包括一个使用未标记数据$S^{(u)}$的正则化项：
- en: '|  | $\overset{\underset{\textbf{w}}{\textrm{argmin}}\mathcal{L}\left(S,f_{\textbf{w}}\right)=}{\underset{\textbf{w}}{\textrm{argmin}}\sum_{\left(\textbf{x}_{i},y_{i}\right)\in
    S^{(l)}}\mathcal{L}_{l}\left(f_{\textbf{w}}\left(\textbf{x}_{i}\right),y_{i}\right)+\gamma\sum_{\textbf{x}_{j}\in
    S^{(u)}}\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{i}\right)}.$ |  | (2)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\overset{\underset{\textbf{w}}{\textrm{argmin}}\mathcal{L}\left(S,f_{\textbf{w}}\right)=}{\underset{\textbf{w}}{\textrm{argmin}}\sum_{\left(\textbf{x}_{i},y_{i}\right)\in
    S^{(l)}}\mathcal{L}_{l}\left(f_{\textbf{w}}\left(\textbf{x}_{i}\right),y_{i}\right)+\gamma\sum_{\textbf{x}_{j}\in
    S^{(u)}}\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{i}\right)}.$ |  | (2)
    |'
- en: 'The unsupervised loss $\mathcal{L}_{u}$ regularizes the model $f_{\textbf{w}}$
    using the unlabelled observations $\textbf{x}_{j}$. The unsupervised regularization
    coefficient $\gamma$ controls the unsupervised regularization influence during
    the model training. We consider it an SSDL sub-category, as a wide number of approaches
    have been developed inspired by this idea. In the literature, different approaches
    for implementing the unsupervised loss function $\mathcal{L}_{u}$ can be found.
    Sections [II-C1](#S2.SS3.SSS1 "II-C1 Consistency based regularization ‣ II-C Regularized
    semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey"),
    [II-C2](#S2.SS3.SSS2 "II-C2 Adversarial augmentation based regularization ‣ II-C
    Regularized semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    and [II-C3](#S2.SS3.SSS3 "II-C3 Graph based regularization ‣ II-C Regularized
    semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    group the most common approaches for implementing it.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督损失 $\mathcal{L}_{u}$ 使用未标记的观测 $\textbf{x}_{j}$ 对模型 $f_{\textbf{w}}$ 进行正则化。无监督正则化系数
    $\gamma$ 控制模型训练过程中的无监督正则化影响。我们将其视为 SSDL 的一个子类别，因为有许多方法受到了这一思想的启发。文献中可以找到不同的实现无监督损失函数
    $\mathcal{L}_{u}$ 的方法。章节 [II-C1](#S2.SS3.SSS1 "II-C1 基于一致性的正则化 ‣ II-C 正则化的半监督学习
    ‣ II 半监督深度学习 ‣ 针对分布不匹配的图像分类的半监督深度学习：综述")、[II-C2](#S2.SS3.SSS2 "II-C2 基于对抗增强的正则化
    ‣ II-C 正则化的半监督学习 ‣ II 半监督深度学习 ‣ 针对分布不匹配的图像分类的半监督深度学习：综述") 和 [II-C3](#S2.SS3.SSS3
    "II-C3 基于图的正则化 ‣ II-C 正则化的半监督学习 ‣ II 半监督深度学习 ‣ 针对分布不匹配的图像分类的半监督深度学习：综述") 汇总了实现该方法的最常见的方法。
- en: II-C1 Consistency based regularization
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 基于一致性的正则化
- en: 'A Consistency Regularized Semi-supervised deep learning (RC-SSDL) loss function
    measures how *robust* a model is, when classifying unlabelled observations in
    $S^{(u)}$ with different transformations applied to the unlabelled data. Such
    transformations usually perturb the unlabelled data without changing its semantics
    and class label (label preserving transformations). For instance, in [[4](#bib.bib4)]
    consistency assumption $\chi^{\textrm{(CL)}}$ is enforced for two views in [[4](#bib.bib4)],
    using the Euclidean distance: $\delta\left(\textbf{x}_{j},f_{\textbf{w}}\right)=\left\|f_{\textbf{w}^{\prime}}\left(\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)\right)-f_{\textbf{w}}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)\right\|.$'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性正则化的半监督深度学习 (RC-SSDL) 损失函数测量了模型在对 $S^{(u)}$ 中未标记观测进行分类时的*鲁棒性*，其中对未标记数据应用了不同的变换。这些变换通常会扰动未标记数据而不改变其语义和类别标签（标签保持变换）。例如，在
    [[4](#bib.bib4)] 中，一致性假设 $\chi^{\textrm{(CL)}}$ 是通过使用欧几里得距离在 [[4](#bib.bib4)]
    中对两个视图进行强制的：$\delta\left(\textbf{x}_{j},f_{\textbf{w}}\right)=\left\|f_{\textbf{w}^{\prime}}\left(\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)\right)-f_{\textbf{w}}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)\right\|.$
- en: Where $\delta\left(\textbf{x}_{i},f_{\textbf{w}}\right)$ is the consistency
    function. Consistency can also be measured for labelled observations in $\textbf{x}_{j}\in
    S^{(l)}$. A number of SSDL techniques are based on consistency regularization.
    Therefore we refer to this category as Consistency based Regularized Semi-Supervised
    deep learning (CR-SSDL).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\delta\left(\textbf{x}_{i},f_{\textbf{w}}\right)$ 是一致性函数。一致性也可以在 $\textbf{x}_{j}\in
    S^{(l)}$ 的标记观测上进行测量。一些 SSDL 技术基于一致性正则化。因此，我们将这一类别称为基于一致性的正则化半监督深度学习 (CR-SSDL)。
- en: A simple interpretation of the consistency regularization term is the increase
    of a model’s robustness to noise, by using the data in $S^{(u)}$. A consistent
    model output for corrupted observations implies a more robust model, with better
    generalization. Consistency can be measured between two deep learning models $f_{\textbf{w}^{\prime}}$
    and $f_{\textbf{w}}$ fed with two different views or random modifications of the
    same observation, $\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)$ and $\Psi^{\eta}\left(\textbf{x}_{j}\right)$.
    For this reason, some authors refer to consistency based SSDL approaches as self-ensemble
    learning models [[55](#bib.bib55)]. The consistency of two or more variations
    of the model is evaluated, measuring the overall model robustness.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性正则化项的简单解释是通过使用 $S^{(u)}$ 中的数据来提高模型对噪声的鲁棒性。对于损坏观察的一致模型输出意味着模型更鲁棒，具有更好的泛化能力。一致性可以在两个深度学习模型
    $f_{\textbf{w}^{\prime}}$ 和 $f_{\textbf{w}}$ 之间测量，这两个模型分别接受两个不同视图或相同观察的随机修改 $\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)$
    和 $\Psi^{\eta}\left(\textbf{x}_{j}\right)$。因此，一些作者将基于一致性的SSDL方法称为自集成学习模型[[55](#bib.bib55)]。通过评估模型的两个或更多变体的一致性，来测量整体模型的鲁棒性。
- en: Consistency regularized methods are based on the consistency assumption $\chi^{(\textrm{CL})}$.
    Thus, such methods can be related to other previously SSDL approaches that also
    exploit this assumption. For instance, as previously mentioned, the consistency
    assumption is also implemented in PT-SSDL as the proxy task, where the model is
    pre-trained to minimize a proxy function. The consistency function can be thought
    as a particular case of the aforementioned confidence function for self-training
    $\varphi$, using two or more views of the observations, as developed in [[4](#bib.bib4)].
    However, in this case the different corrupted views of the observation $\textbf{x}_{j}$
    are highly correlated, in spite of the original co-training approach developed
    in [[4](#bib.bib4)]. Nevertheless, recent regularized SSDL models [[3](#bib.bib3),
    [86](#bib.bib86), [49](#bib.bib49)] simplify this assumption. They consider as
    a view of an observation $\textbf{x}_{j}$ its corruption with random noise $\eta$,
    making up a corrupted view $\Psi^{\eta}\left(\textbf{x}_{j}\right)$. The independence
    assumption [[4](#bib.bib4)] between the views of co-training, fits better when
    measuring the consistence between different signal sources, as seen in [[56](#bib.bib56)].
    In such work, different data sources are used for semi-supervised human activity
    recognition.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性正则化方法基于一致性假设 $\chi^{(\textrm{CL})}$。因此，这些方法可以与其他以前的SSDL方法相关联，这些方法也利用了这一假设。例如，如前所述，一致性假设也在PT-SSDL中作为代理任务实现，在该任务中，模型被预训练以最小化代理函数。一致性函数可以被认为是前述自训练置信度函数
    $\varphi$ 的特定情况，使用两个或更多观察视图，如在[[4](#bib.bib4)]中所述。然而，在这种情况下，观察 $\textbf{x}_{j}$
    的不同损坏视图高度相关，尽管存在于[[4](#bib.bib4)]中开发的原始共同训练方法中。尽管如此，最近的正则化SSDL模型[[3](#bib.bib3),
    [86](#bib.bib86), [49](#bib.bib49)]简化了这一假设。它们将观察 $\textbf{x}_{j}$ 的视图视为其与随机噪声
    $\eta$ 的损坏，形成一个损坏视图 $\Psi^{\eta}\left(\textbf{x}_{j}\right)$。当测量不同信号源之间的一致性时，[[4](#bib.bib4)]中对共同训练视图之间独立性假设的拟合更好，如[[56](#bib.bib56)]中所见。在这项工作中，使用了不同的数据源进行半监督的人体活动识别。
- en: 'In [[3](#bib.bib3)] the Pi Model (Pi-M) was proposed. The consistency of the
    deep model with random noise injected to its weights (commonly referred to as
    dropout) is evaluated. The weights $\textbf{w}^{\prime}$ are a corrupted version
    of the parent model with weights w, making up what the authors refer as a pseudo-ensemble.
    The Pi-M model was tested in [[86](#bib.bib86)] using the CIFAR-10 and SVHN datasets.
    Intersected yielded results of Pi-M with the rest of the discussed methods in
    this work can be found in Table [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization
    ‣ II-C Regularized semi-supervised learning ‣ II Semi-supervised Deep Learning
    ‣ Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[3](#bib.bib3)]中提出了Pi模型（Pi-M）。评估了将随机噪声注入其权重（通常称为丢弃）的深度模型的一致性。权重 $\textbf{w}^{\prime}$
    是父模型权重 w 的损坏版本，形成了作者所称的伪集成。Pi-M模型在[[86](#bib.bib86)]中使用CIFAR-10和SVHN数据集进行了测试。交叉的Pi-M结果与本文讨论的其他方法的结果可以在表
    [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey") 中找到。'
- en: 'A consistency evaluation of both unlabelled and labelled datasets can be performed,
    as proposed in [[72](#bib.bib72)], in the Mutual Exclusivity-Transformation Model
    (METM). In such method, an unsupervised loss term for Transformation Supervision
    (TS) was proposed: $\mathcal{L}_{u}^{\left(\textrm{TS}\right)}\left(f_{\textbf{w}},\textbf{x}_{i}\right)=\sum_{j}^{M}\sum_{k}^{M}\left\|f_{\textbf{w}}\left(\Psi_{j}\left(\textbf{x}_{i}\right)\right)-f_{\textbf{w}}\left(\Psi_{k}\left(\textbf{x}_{i}\right)\right)\right\|^{2},$
    where $M$ random transformations $\Psi$ are performed over the observation $\textbf{x}_{i}$.
    This can be used for unsupervised pre-training. Such loss term can be regarded
    as a consistency measurement. Furthermore, a Mutual Exclusivity (ME) based loss
    function is used. It encourages non-overlapping predictions of the model. The
    ME loss term is depicted as $\mathcal{L}_{u}^{\left(\textrm{ME}\right)}=\left\|-\prod_{k}^{K}f\left(\textbf{x}_{i}\right)\prod_{k}^{K}\left(1-f_{\textbf{w}}\left(\textbf{x}_{i}\right)\right)\right\|^{2}$.
    The final unsupervised loss is implemented as $\mathcal{L}_{u}=\lambda_{1}\mathcal{L}_{u}^{\left(\textrm{ME}\right)}+\lambda_{2}\mathcal{L}_{u}^{\left(\textrm{TS}\right)}$,
    with the weighting coefficients $\lambda_{1}$ and $\lambda_{2}$ for each unsupervised
    loss term. METM was tested with the SVHN and CIFAR-10 datasets. Comparable results
    with the rest of reviewed methods in this work are depicted in Table [I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '可以对未标记和标记的数据集进行一致性评估，如[[72](#bib.bib72)] 中提出的，在互斥-变换模型（METM）中。在这种方法中，提出了变换监督（TS）的无监督损失项：$\mathcal{L}_{u}^{\left(\textrm{TS}\right)}\left(f_{\textbf{w}},\textbf{x}_{i}\right)=\sum_{j}^{M}\sum_{k}^{M}\left\|f_{\textbf{w}}\left(\Psi_{j}\left(\textbf{x}_{i}\right)\right)-f_{\textbf{w}}\left(\Psi_{k}\left(\textbf{x}_{i}\right)\right)\right\|^{2},$
    其中 $M$ 次随机变换 $\Psi$ 作用于观测值 $\textbf{x}_{i}$。这可以用于无监督预训练。这种损失项可以视为一致性测量。此外，使用了基于互斥性（ME）的损失函数。它鼓励模型的预测不重叠。ME
    损失项表示为 $\mathcal{L}_{u}^{\left(\textrm{ME}\right)}=\left\|-\prod_{k}^{K}f\left(\textbf{x}_{i}\right)\prod_{k}^{K}\left(1-f_{\textbf{w}}\left(\textbf{x}_{i}\right)\right)\right\|^{2}$。最终的无监督损失实现为
    $\mathcal{L}_{u}=\lambda_{1}\mathcal{L}_{u}^{\left(\textrm{ME}\right)}+\lambda_{2}\mathcal{L}_{u}^{\left(\textrm{TS}\right)}$，其中
    $\lambda_{1}$ 和 $\lambda_{2}$ 为每个无监督损失项的权重系数。METM 在 SVHN 和 CIFAR-10 数据集上进行了测试。表
    [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey") 中显示了与本文所评审的其他方法相当的结果。'
- en: 'Later, authors in [[49](#bib.bib49)] proposed the Temporal Ensemble Model (TEM),
    which calculates the consistency of the trained model with the the moving weighted
    average of the predictions from different models along each training epoch $\tau$:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，[[49](#bib.bib49)] 的作者提出了时间集合模型（TEM），该模型计算经过训练的模型与不同模型在每个训练周期 $\tau$ 的移动加权平均预测的一致性。
- en: '|  | $f_{\textbf{w}^{\prime}_{\tau}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)=(1-\rho)f_{\textbf{w}_{\tau}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)+\rho
    f_{\textbf{w}_{\tau-1}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right),$ |  |
    (3) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{\textbf{w}^{\prime}_{\tau}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)=(1-\rho)f_{\textbf{w}_{\tau}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)+\rho
    f_{\textbf{w}_{\tau-1}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right),$ |  |
    (3) |'
- en: 'with $\rho$ the decay parameter, and $\tau$ the current training epoch. The
    temporal ensemble evaluates the output of a temporally averaged model to a noisy
    observation $\Psi^{\eta}\left(\textbf{x}_{i}\right)$. This enforces temporal consistency
    of the model. Table [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C
    Regularized semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    shows the yielded results by the TEM method for the CIFAR-10 dataset. Based on
    this approach, the authors in [[85](#bib.bib85)], developed an SSDL approach based
    on the Kullback-Leibler cross-entropy to measure model consistency. Different
    transformations $\Psi^{\eta}$ are applied to the input observations $\textbf{x}_{j}$.
    These correspond to image flipping, random contrast adjustment, rotation and cropping.
    The method was evaluated in a real world scenario with ultrasound fetal images
    for anatomy classification.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\rho$ 是衰减参数，$\tau$ 是当前的训练周期。时间集成评估了时间平均模型的输出与嘈杂观测值 $\Psi^{\eta}\left(\textbf{x}_{i}\right)$
    的一致性。这强制模型的时间一致性。表格 [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C
    Regularized semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    显示了 TEM 方法在 CIFAR-10 数据集上的结果。基于这种方法，作者在 [[85](#bib.bib85)] 中开发了一种基于 Kullback-Leibler
    交叉熵来测量模型一致性的 SSDL 方法。对输入观测值 $\textbf{x}_{j}$ 应用不同的转换 $\Psi^{\eta}$。这些转换包括图像翻转、随机对比度调整、旋转和裁剪。该方法在实际场景中使用超声胎儿图像进行解剖分类进行了评估。'
- en: 'An extension of the temporal ensembling idea was presented by the authors of
    [[86](#bib.bib86)], in the popular Mean Teacher Model (MTM). Instead of averaging
    the predictions of models calculated in past epochs, the authors implemented an
    exponential weight average: $\textbf{w}^{\prime}_{\tau}=\rho\textbf{w}^{\prime}_{\tau-1}+\left(1-\rho\right)\textbf{w}^{\prime}_{\tau}$
    for a training epoch $\tau$, with an exponential weighting coefficient $\rho$.
    Such exponentially averaged model with parameters $\textbf{w}^{\prime}$ is referred
    to by the authors as the teacher model. For comparison purposes, the yielded results
    by MTM using the CIFAR-10 dataset are depicted in Table [I](#S2.T1 "Table I ‣
    II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning ‣
    II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[[86](#bib.bib86)] 的作者提出了时间集成思想的扩展，即流行的均值教师模型（MTM）。作者没有对过去周期中计算的模型进行平均预测，而是实现了指数加权平均：$\textbf{w}^{\prime}_{\tau}=\rho\textbf{w}^{\prime}_{\tau-1}+\left(1-\rho\right)\textbf{w}^{\prime}_{\tau}$，其中
    $\tau$ 是训练周期，$\rho$ 是指数加权系数。具有参数 $\textbf{w}^{\prime}$ 的这种指数平均模型被作者称为教师模型。为了比较目的，MTM
    在 CIFAR-10 数据集上的结果如表格 [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣
    II-C Regularized semi-supervised learning ‣ II Semi-supervised Deep Learning ‣
    Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey") 所示。'
- en: 'More recently, authors in [[59](#bib.bib59)], proposed the Virtual Adversarial
    Training Model (VATM). They implemented a generative adversarial network to inject
    adversarial perturbations $\eta$ into the labelled and unlabelled observations.
    Artificially generated observations are compared to the original unlabelled data.
    This results in adversarial noise encouraging a more challenging consistency robustness.
    Furthermore, the authors also added a conditional entropy term, in order to make
    the model more confident when minimizing it. We refer to this variation as Virtual
    Adversarial Training with Entropy Minimization (VATM+EM). Both VATM and VATM+EM
    were tested with the CIFAR-10 dataset, thus we include the comparable results
    with the rest of the reviewed methods in Table [I](#S2.T1 "Table I ‣ II-C3 Graph
    based regularization ‣ II-C Regularized semi-supervised learning ‣ II Semi-supervised
    Deep Learning ‣ Semi-supervised Deep Learning for Image Classification with Distribution
    Mismatch: A Survey").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，作者在 [[59](#bib.bib59)] 中提出了虚拟对抗训练模型（VATM）。他们实现了一个生成对抗网络，将对抗扰动 $\eta$ 注入到标记和未标记的观测数据中。将人工生成的观测数据与原始未标记数据进行比较。这会产生对抗噪声，促使模型在一致性方面变得更加具有挑战性。此外，作者还添加了条件熵项，以使模型在最小化时更具信心。我们将这种变体称为具有熵最小化的虚拟对抗训练（VATM+EM）。VATM
    和 VATM+EM 都在 CIFAR-10 数据集上进行了测试，因此我们在表格 [I](#S2.T1 "Table I ‣ II-C3 Graph based
    regularization ‣ II-C Regularized semi-supervised learning ‣ II Semi-supervised
    Deep Learning ‣ Semi-supervised Deep Learning for Image Classification with Distribution
    Mismatch: A Survey") 中包含了与其他方法的可比结果。'
- en: 'Another variation of the consistency function $\mathcal{L}_{u}$ was developed
    in [[19](#bib.bib19)] in what the authors referred to as a memory loss function.
    We refer to this as the Memory based Model (MeM). This memory loss is based on
    a memory module, consisting of an embedding $m_{i}=\left(\check{\textbf{x}}_{i},\hat{\textbf{y}}_{i}\right)$.
    It is composed of the features extracted $\check{\textbf{x}}_{i}=h_{{}_{\textbf{w}^{\left(\textrm{FE}\right)}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$
    by the deep learning model $f_{\textbf{w}}$, and the corresponding probability
    density function computed $\hat{\textbf{y}}_{i}=f_{\textbf{w}}\left(\textbf{x}_{i}\right)$
    (with $\hat{\textbf{y}}$ the logits output), for a given observation $\textbf{x}_{i}$.
    The memory stores one embedding $k$ per class $m_{k}=\left(\check{\textbf{$\textbf{x}$}}_{k},\hat{\textbf{$\textbf{y}$}}_{k}\right)$,
    corresponding to the average embedding of all the observations within class $k$.
    Previous approaches like the temporal ensemble [[49](#bib.bib49)] needed to store
    the output of past models for each observation. In the memory loss based approach
    of [[19](#bib.bib19)] this is avoided by only storing one average embedding per
    class. In the second step, the memory loss is computed as follows: $\mathcal{L}_{m}=H\left(\textbf{p}_{i}\right)+\max\left(\textbf{p}_{i}\right)\delta_{\textrm{KL}}\left(\textbf{p}_{i},\hat{\textbf{y}}_{i}\right),$
    where $\textbf{p}_{i}$ is they key addressed probability, calculated as the closest
    embedding to $\textbf{x}_{i}$, and $\hat{\textbf{y}}_{i}$ is the model output
    for such observation. The factor $\max\left(\textbf{p}_{i}\right)$ is the highest
    value of the probability distribution $\textbf{p}_{i}$ and $H\left(\textbf{p}_{i}\right)$
    is the entropy of the key addressed output distribution $\textbf{p}_{i}$. The
    factor $\delta_{\textrm{KL}}\left(\textbf{p}_{i},\hat{\textbf{y}}_{i}\right)$
    is the Kullback-Leibler distance of the output for the observation $\textbf{x}_{i}$
    and the recovery key address from the memory mapping. Comparable results to the
    rest of the reviewed methods for the MeM method are shown in Table [I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey") for the CIFAR-10 dataset.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种一致性函数 $\mathcal{L}_{u}$ 的变体是在[[19](#bib.bib19)]中开发的，作者将其称为记忆丧失函数。我们称之为基于记忆的模型（MeM）。这种记忆丧失基于一个记忆模块，由一个嵌入
    $m_{i}=\left(\check{\textbf{x}}_{i},\hat{\textbf{y}}_{i}\right)$ 组成。它由深度学习模型 $f_{\textbf{w}}$
    提取的特征 $\check{\textbf{x}}_{i}=h_{{}_{\textbf{w}^{\left(\textrm{FE}\right)}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$
    和对应的概率密度函数 $\hat{\textbf{y}}_{i}=f_{\textbf{w}}\left(\textbf{x}_{i}\right)$（其中
    $\hat{\textbf{y}}$ 为 logits 输出）组成，用于给定的观测 $\textbf{x}_{i}$。记忆模块每个类别存储一个嵌入 $k$，即
    $m_{k}=\left(\check{\textbf{$\textbf{x}$}}_{k},\hat{\textbf{$\textbf{y}$}}_{k}\right)$，对应于类别
    $k$ 中所有观测的平均嵌入。以前的方法如时间集成 [[49](#bib.bib49)] 需要为每个观测存储过去模型的输出。在[[19](#bib.bib19)]
    的记忆丧失方法中，通过仅存储每个类别的一个平均嵌入来避免这一点。在第二步中，记忆丧失的计算如下：$\mathcal{L}_{m}=H\left(\textbf{p}_{i}\right)+\max\left(\textbf{p}_{i}\right)\delta_{\textrm{KL}}\left(\textbf{p}_{i},\hat{\textbf{y}}_{i}\right),$
    其中 $\textbf{p}_{i}$ 是关键地址概率，计算为与 $\textbf{x}_{i}$ 最接近的嵌入，而 $\hat{\textbf{y}}_{i}$
    是模型对该观测的输出。因子 $\max\left(\textbf{p}_{i}\right)$ 是概率分布 $\textbf{p}_{i}$ 的最高值，$H\left(\textbf{p}_{i}\right)$
    是关键地址输出分布 $\textbf{p}_{i}$ 的熵。因子 $\delta_{\textrm{KL}}\left(\textbf{p}_{i},\hat{\textbf{y}}_{i}\right)$
    是观测 $\textbf{x}_{i}$ 的输出与从记忆映射中恢复的关键地址之间的 Kullback-Leibler 距离。MeM 方法与其余审查方法的可比结果显示在表
    [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey") 中，用于 CIFAR-10 数据集。'
- en: 'More recently, an SSDL approach was proposed in [[77](#bib.bib77)]. This is
    referred to as the Transductive Model (TransM). The authors implement a transductive
    learning approach. This means that the unknown labels $\widetilde{y}$ are also
    treated as variables, thus optimized along with the model parameters w. Therefore,
    the loss function implements a cross-entropy supervised loss: $\mathcal{L}_{l}\left(f_{\textbf{w}},\textbf{x}_{i},y_{i}\right)=r_{i}H_{\textrm{CE}}\left(f_{\textbf{w}}\left(\textbf{x}_{i}\right),y_{i}\right),$
    with $r_{i}$ an element of the set $R=\left\{r_{i}\right\}_{i=1}^{n_{l}+n_{u}}$,
    which indicates the label estimation confidence level for an observation $\textbf{x}_{i}$.
    Such confidence level coefficient makes the model more robust to outliers in both
    the labelled and unlabelled datasets. The confidence coefficient is calculated
    using a k-nearest neighbors approach from the labelled data, making use of the
    observation density assumption $\chi^{(\textrm{CL})}$. This means that the label
    estimated is of high confidence, if the observations lies in a high density space
    for the labelled data within the feature space. As DCNN s are meant to be used
    by the model, the feature space is learned within the training process, making
    necessary to recalculate $R$ at each training step $\tau$. As for the unlabelled
    regularization term: $\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{j}\right)=\lambda_{\textrm{RF}}\mathcal{L}_{\textrm{RF}}\left(f_{\textbf{w}},\textbf{x}_{j}\right)+\lambda_{\textrm{MMF}}\sum_{\textbf{x}_{i}}\mathcal{L}_{\textrm{MMF}}\left(f_{\textbf{w}},\textbf{x}_{j},\textbf{x}_{i}\right),$
    it is composed of a robust feature measurement $\mathcal{L}_{\textrm{RF}}$ and
    a min-max separation term $\mathcal{L}_{\textrm{MMF}}$, where $\lambda_{\textrm{RF}}$
    and $\lambda_{\textrm{MMF}}$ weigh their contribution to the unsupervised signal.
    The first term measures the feature consistency, thus using the output of the
    learned feature extractor of the model $\check{\textbf{x}}_{i}=h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$.
    The consistency of the learned features is measured with the Euclidian distance
    $\mathcal{L}_{\textrm{RF}}\left(\textbf{w},\textbf{x}_{j}\right)=\left\|h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)-h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)\right)\right\|^{2}$.
    Regarding the second term, referred to as the min-max separation function, it
    is meant to maximize the distance between observations of different classes by
    a minimum margin $\rho$, and to minimize the distance from observations within
    the same class. It is implemented as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[[77](#bib.bib77)]中提出了一种SSDL方法。这被称为传导模型（TransM）。作者实施了一种传导学习方法。这意味着未知标签 $\widetilde{y}$
    也被视为变量，从而与模型参数 w 一起进行优化。因此，损失函数实现了一个交叉熵监督损失：$\mathcal{L}_{l}\left(f_{\textbf{w}},\textbf{x}_{i},y_{i}\right)=r_{i}H_{\textrm{CE}}\left(f_{\textbf{w}}\left(\textbf{x}_{i}\right),y_{i}\right),$
    其中 $r_{i}$ 是集合 $R=\left\{r_{i}\right\}_{i=1}^{n_{l}+n_{u}}$ 的一个元素，表示对观察 $\textbf{x}_{i}$
    的标签估计置信度水平。这样的置信度系数使模型对标记和未标记数据集中都能更好地抵御异常值。置信度系数是使用来自标记数据的 k 最近邻方法计算的，利用了观察密度假设
    $\chi^{(\textrm{CL})}$。这意味着如果观察位于特征空间内的标记数据的高密度区域，则标签估计具有高置信度。由于 DCNN 是模型使用的，因此特征空间在训练过程中进行学习，必须在每个训练步骤
    $\tau$ 重新计算 $R$。对于未标记正则化项：$\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{j}\right)=\lambda_{\textrm{RF}}\mathcal{L}_{\textrm{RF}}\left(f_{\textbf{w}},\textbf{x}_{j}\right)+\lambda_{\textrm{MMF}}\sum_{\textbf{x}_{i}}\mathcal{L}_{\textrm{MMF}}\left(f_{\textbf{w}},\textbf{x}_{j},\textbf{x}_{i}\right),$
    它由一个鲁棒特征度量 $\mathcal{L}_{\textrm{RF}}$ 和一个最小-最大分离项 $\mathcal{L}_{\textrm{MMF}}$
    组成，其中 $\lambda_{\textrm{RF}}$ 和 $\lambda_{\textrm{MMF}}$ 权衡它们对无监督信号的贡献。第一个项度量特征一致性，使用模型的学习特征提取器的输出
    $\check{\textbf{x}}_{i}=h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$。学习特征的一致性通过欧氏距离
    $\mathcal{L}_{\textrm{RF}}\left(\textbf{w},\textbf{x}_{j}\right)=\left\|h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)-h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)\right)\right\|^{2}$
    来衡量。至于第二项，被称为最小-最大分离函数，它旨在通过最小边际 $\rho$ 最大化不同类别观察之间的距离，并最小化同一类别观察之间的距离。其实现如下：
- en: '|  | $\mathcal{L}_{\textrm{MMF}}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j}\right)=r_{i}r_{j}\left\&#124;f_{\textbf{w}}\left(\textbf{x}_{i}\right)-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\&#124;^{2}\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)$
    |  | (4) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\textrm{MMF}}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j}\right)=r_{i}r_{j}\left\&#124;f_{\textbf{w}}\left(\textbf{x}_{i}\right)-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\&#124;^{2}\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)$
    |  | (4) |'
- en: '|  | $-\textrm{min}\left(\left\&#124;f_{\textbf{w}}\left(\textbf{x}_{i}\right)-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\&#124;^{2}-\rho,0\right)\left(1-\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)\right).$
    |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $-\textrm{min}\left(\left\&#124;f_{\textbf{w}}\left(\textbf{x}_{i}\right)-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\&#124;^{2}-\rho,0\right)\left(1-\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)\right).$
    |  |'
- en: 'With $\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)=1$ when $\widetilde{y}_{i}=\widetilde{y}_{j}$,
    or cero otherwise. The first term in $\mathcal{L}_{\textrm{MMF}}$ minimizes the
    intra-class distance and the second term maximizes the inter-class observation
    distance. We highlight the theoretical outlier robustness of the method by implementing
    the confidence coefficient $r_{i}$. This coefficient is able to give lower relevance
    to unconfident estimations. However, it is yet to be fully proved, as the experiments
    conducted in [[77](#bib.bib77)] have not tested the model robustness to unlabelled
    single and collective outliers. The approach was also combined and tested against
    other consistency regularization approaches, like the MTM. This is referred to,
    in this work, as Transductive Model with Mean Teacher (TransM+MTM). The comparable
    results with the rest of the reviewed approaches are depicted in Table [I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey").'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '当$\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)=1$时，$\widetilde{y}_{i}=\widetilde{y}_{j}$，否则为零。$\mathcal{L}_{\textrm{MMF}}$中的第一个项最小化类内距离，第二个项最大化类间观察距离。我们通过实现置信系数$r_{i}$来突显该方法的理论异常值鲁棒性。该系数能够对不确定的估计给予较低的相关性。然而，尚未完全证明，因为在[[77](#bib.bib77)]中进行的实验未测试模型对无标签单个和集体异常值的鲁棒性。该方法还与其他一致性正则化方法结合并进行测试，如MTM。在本文中，这被称为均值教师的转导模型（TransM+MTM）。与其他评审方法的可比结果见表[I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey")。'
- en: 'An alternative approach for the consistency function was implemented in [[89](#bib.bib89)],
    with a model named by the authors as Self Supervised network Model (SESEMI). The
    consistency function is fed by what the authors defined as a self-supervised branch.
    This branch aims to learn simple image transformations or pretext tasks, such
    as image rotation. The authors claim that their model is easier to calibrate than
    the MTM, by just using an unsupervised signal weight of $\lambda=1$. The intersected
    results of SESEMI with the rest of the reviewed methods are detailed in Table
    [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey").'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '一种替代一致性函数的方法在[[89](#bib.bib89)]中实施，模型名称为作者命名的自监督网络模型（SESEMI）。一致性函数由作者定义的自监督分支提供输入。该分支旨在学习简单的图像变换或预训练任务，如图像旋转。作者声称他们的模型比MTM更容易校准，只需使用一个无监督信号权重$\lambda=1$。SESEMI与其他评审方法的交集结果详见表[I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey")。'
- en: 'In [[9](#bib.bib9)], the authors proposed a novel SSDL method known as MixMatch.
    This method implements a consistency loss which first calculates a soft pseudo-label
    for each unlabelled observation. Those soft pseudo-labels are the result of averaging
    the model response to a number of transformations of the input $\textbf{x}_{j}$
    $\widehat{\textbf{y}}{}_{j}=\frac{1}{\mathcal{T}}\sum_{\eta=1}^{\mathcal{T}}f_{\textbf{w}}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)$.
    In such equation, $\mathcal{T}$ refers to the number of transformations of the
    input image (i.e., image rotation, cropping, etc.). The specific image transformation
    is represented in $\Psi^{\eta}$. The authors in [[9](#bib.bib9)] recommend to
    use $\mathcal{T}=2$. Later, the obtained soft pseudo-label is sharpened, in order
    to decrease its entropy and under-confidence of the pseudo-label. For this, a
    parameter $\rho$ is used within the softmax of the output $\widehat{\textbf{y}}{}_{j}$:
    $s\left(\widehat{\textbf{y}},\rho\right)_{i}=\frac{\widehat{y}_{i}^{1/\rho}}{\sum_{j}\widehat{y}_{j}^{1/\rho}}$.
    The dataset $\widetilde{S}_{u}=\left(X_{u},\widetilde{Y}\right)$ contains the
    sharpened soft pseudo-labels, where $\widetilde{Y}=\left\{\widetilde{\textbf{y}}_{1},\widetilde{\textbf{y}}_{2},\ldots,\widetilde{\textbf{y}}_{n_{u}}\right\}$.
    The authors of MixMatch found that data augmentation is very important to improve
    its performance. Taking this into account, the authors implemented the MixUp methodology
    to augment both the labelled and unlabelled datasets [[99](#bib.bib99)]. This
    is represented as follows: $\left(S^{\prime}_{l},\widetilde{S}^{\prime}_{u}\right)=\Psi_{\textrm{MixUp}}\left(S_{l},\widetilde{S}_{u},\alpha\right)$.
    The MixUp generates new observations through a linear interpolation between different
    combinations of both the labelled and unlabelled data. The labels for the new
    observations are also lineally interpolated, using both the labels and the pseudo-labels
    (for the unlabelled data).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[9](#bib.bib9)]中，作者提出了一种名为MixMatch的新型SSDL方法。该方法实现了一种一致性损失，首先计算每个无标签观测的软伪标签。这些软伪标签是对输入$\textbf{x}_{j}$的若干次变换模型响应的平均值得到的
    $\widehat{\textbf{y}}{}_{j}=\frac{1}{\mathcal{T}}\sum_{\eta=1}^{\mathcal{T}}f_{\textbf{w}}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)$。在这个方程中，$\mathcal{T}$是指输入图像的变换次数（即图像旋转、裁剪等）。特定的图像变换在$\Psi^{\eta}$中表示。作者在[[9](#bib.bib9)]中建议使用$\mathcal{T}=2$。随后，获得的软伪标签被锐化，以减少其熵和伪标签的低置信度。为此，在输出$\widehat{\textbf{y}}{}_{j}$的softmax中使用参数$\rho$：$s\left(\widehat{\textbf{y}},\rho\right)_{i}=\frac{\widehat{y}_{i}^{1/\rho}}{\sum_{j}\widehat{y}_{j}^{1/\rho}}$。数据集$\widetilde{S}_{u}=\left(X_{u},\widetilde{Y}\right)$包含锐化的软伪标签，其中$\widetilde{Y}=\left\{\widetilde{\textbf{y}}_{1},\widetilde{\textbf{y}}_{2},\ldots,\widetilde{\textbf{y}}_{n_{u}}\right\}$。MixMatch的作者发现数据增强对提高性能非常重要。考虑到这一点，作者实施了MixUp方法来增强标记和未标记的数据集[[99](#bib.bib99)]。表示如下：$\left(S^{\prime}_{l},\widetilde{S}^{\prime}_{u}\right)=\Psi_{\textrm{MixUp}}\left(S_{l},\widetilde{S}_{u},\alpha\right)$。MixUp通过标记数据和未标记数据的不同组合之间的线性插值生成新的观测。新观测的标签也是线性插值的，同时使用标签和伪标签（用于未标记数据）。
- en: 'Mathematically, MixUp takes two pseudo-labelled or labelled data pairs $\left(\textbf{x}_{a},y_{a}\right)$
    and $\left(\textbf{x}_{b},y_{b}\right)$, and generates the augmented datasets
    $\left(S^{\prime}_{l},\widetilde{S}^{\prime}_{u}\right)$. These augmented datasets
    are used by MixMatch, to train neural network with parameters w through the minimization
    of the following loss function:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，MixUp采用两个伪标记或标记的数据对$\left(\textbf{x}_{a},y_{a}\right)$和$\left(\textbf{x}_{b},y_{b}\right)$，并生成增强数据集$\left(S^{\prime}_{l},\widetilde{S}^{\prime}_{u}\right)$。这些增强数据集由MixMatch使用，通过最小化以下损失函数来训练具有参数w的神经网络：
- en: '|  | $\mathcal{L}\left(S,\textbf{w}\right)=\sum_{\left(\textbf{x}_{i},\textbf{y}_{i}\right)\in
    S^{\prime}_{l}}\mathcal{L}_{l}\left(\textbf{w},\textbf{x}_{i},\textbf{y}_{i}\right)+$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}\left(S,\textbf{w}\right)=\sum_{\left(\textbf{x}_{i},\textbf{y}_{i}\right)\in
    S^{\prime}_{l}}\mathcal{L}_{l}\left(\textbf{w},\textbf{x}_{i},\textbf{y}_{i}\right)+$
    |  |'
- en: '|  | $\gamma r(t)\sum_{\left(\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)\in\widetilde{S}^{\prime}_{u}}\mathcal{L}_{u}\left(\textbf{w},\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)$
    |  | (5) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\gamma r(t)\sum_{\left(\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)\in\widetilde{S}^{\prime}_{u}}\mathcal{L}_{u}\left(\textbf{w},\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)$
    |  | (5) |'
- en: 'The labelled loss term $\mathcal{L}_{l}$, can be implemented with a cross-entropy
    function, as recommended in [[9](#bib.bib9)]; $\mathcal{L}_{l}\left(\textbf{w},\textbf{x}_{i},\textbf{y}_{i}\right)=H_{\textrm{CE}}\left(\textbf{y}_{i},f_{\textbf{w}}\left(\textbf{x}_{i}\right)\right)$.
    Regarding the unlabelled loss term, an Euclidean distance was tested by the authors
    in [[9](#bib.bib9)] $\mathcal{L}_{u}\left(\textbf{w},\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)=\left\|\widetilde{\textbf{y}}_{j}-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\|$.
    In the MixMatch loss function, the coefficient $r(t)$ is implemented as a ramp-up
    function which augments the weight of the unlabelled loss term as $t$ increases.
    The parameter $\gamma$ controls the overall influence of the unlabelled loss term.
    In Table [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized
    semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey"),
    the results yielded in [[9](#bib.bib9)] are depicted for the  CIFAR-10 dataset.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '标记损失项 $\mathcal{L}_{l}$ 可以通过交叉熵函数来实现，如[[9](#bib.bib9)]中推荐的那样；$\mathcal{L}_{l}\left(\textbf{w},\textbf{x}_{i},\textbf{y}_{i}\right)=H_{\textrm{CE}}\left(\textbf{y}_{i},f_{\textbf{w}}\left(\textbf{x}_{i}\right)\right)$。关于未标记损失项，作者在[[9](#bib.bib9)]中测试了欧几里得距离，$\mathcal{L}_{u}\left(\textbf{w},\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)=\left\|\widetilde{\textbf{y}}_{j}-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\|$。在MixMatch损失函数中，系数
    $r(t)$ 被实现为一个递增函数，该函数随着 $t$ 的增加而增强未标记损失项的权重。参数 $\gamma$ 控制未标记损失项的整体影响。在表[I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey")中，[[9](#bib.bib9)]
    中得到的结果展示了CIFAR-10数据集的表现。'
- en: 'In [[8](#bib.bib8)] an extension of the MixMatch algorithm was developed, referred
    to as ReMixMatch. Two main modifications were proposed: a distribution alignment
    procedure and a more extensive use of data augmentation. Distribution alignment
    consists of the normalization of each prediction using both the running average
    prediction of each class (in a set of previous model epochs) and the marginal
    label distribution using the labelled dataset. This way, soft pseudo-label estimation
    accounts for the label distribution and previous label predictions, enforcing
    soft pseudo-label consistency with both distributions. The extension of the previous
    simple data-augmentation step implemented in the original MixMatch algorithm (where
    flips and crops were used) consists of two methods. They are referred to as anchor
    augmentation and CTAugment by the authors. The empirical evidence gathered by
    the authors when implementing stronger data augmenting transformations (i.e. gamma
    and brightness modifications, etc.) in MixMatch showed a performance deterioration.
    This is caused by the larger variation in the model output for each type of strong
    transformation, making the pseudo-label less meaningful. To circumvent this, the
    authors proposed an augmentation anchoring approach. It uses the same pseudo-labels
    estimated when using a weak transformation, for the $\mathcal{T}^{\prime}$ strong
    transformations. Such strong transformations are calculated through a modification
    of the auto-augment algorithm. Auto-augment originally uses reinforcement learning
    to find the best resulting augmentation policy (set of transformations used) for
    the specific target problem [[24](#bib.bib24)]. To simplify its implementation
    for small labelled datasets, the authors in [[8](#bib.bib8)] proposed a modification
    referred to as CTAugment. It estimates the likelihood of generating a correctly
    classified image, in order to generate images that are unlikely to result in wrong
    predictions. The performance reported in the executed benchmarks of ReMixMatch,
    showed an accuracy gain ranging from 1% to 6%, when compared to the original MixMatch
    algorithm. No statistical significance tests were reported. Comparable results
    to other methods reviewed in this work for the CIFAR-10 dataset are shown in Table
    [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[8](#bib.bib8)]中，开发了一种 MixMatch 算法的扩展，称为 ReMixMatch。提出了两个主要修改：分布对齐程序和数据增强的更广泛使用。分布对齐包括利用每个类别的运行平均预测（在一组先前模型的训练周期中）和带标签数据集的边际标签分布对每个预测进行标准化。这样，软伪标签估计考虑了标签分布和先前的标签预测，强制软伪标签在两种分布下保持一致。原始
    MixMatch 算法中实现的简单数据增强步骤（使用翻转和裁剪）扩展为两种方法。作者称之为锚点增强（anchor augmentation）和 CT 增强（CTAugment）。作者在
    MixMatch 中实施更强的数据增强变换（即伽玛和亮度修改等）时，收集到的实证证据表明性能下降。这是因为每种强变换的模型输出变化较大，使伪标签的意义降低。为了解决这个问题，作者提出了一种增强锚点方法。该方法使用在使用弱变换时估计的相同伪标签，用于$\mathcal{T}^{\prime}$强变换。这些强变换通过修改自动增强算法来计算。自动增强最初使用强化学习来寻找针对特定目标问题的最佳增强策略（使用的变换集合）[[24](#bib.bib24)]。为了简化在小型标注数据集上的实现，[[8](#bib.bib8)]中的作者提出了一种修改，称为
    CTAugment。它估计生成正确分类图像的可能性，以生成不容易导致错误预测的图像。在 ReMixMatch 执行的基准测试中报告的性能显示，与原始 MixMatch
    算法相比，准确率提高了 1% 到 6%。未报告统计显著性测试。与本文中其他方法在 CIFAR-10 数据集上的结果对比见表 [I](#S2.T1 "Table
    I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning
    ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey")。'
- en: 'More recently, an SSDL method referred to as FixMatch was proposed in [[80](#bib.bib80)].
    The authors argue the proposition of a simplified SSDL method compared to other
    techniques. FixMatch is based upon pseudo-labelling and consistency regularized
    SSDL. The loss function uses a cross-entropy labelled loss term, along with weak
    augmentations for the labelled data. For the unlabelled loss term, the cross-entropy
    is also used, but for the strongly unlabelled observations with its corresponding
    pseudo-label. The soft pseudo-label is calculated using weak transformations,
    taking the maximum logit of the model output. Therefore no model output sharpening
    is done, unlike MixMatch. Strong augmentations are tested using both Random Augmentation
    (RA) [[25](#bib.bib25)] and CTAugmentation (CTA) [[8](#bib.bib8)]. For benchmarking
    FixMatch, the authors used the CIFAR-10 (40, 250, 4000 labels), Canadian Institute
    For Advanced Research dataset with 100 classes (CIFAR-100) (400, 2500 and 10000
    labels), SVHN (40, 250, 1000 labels) and Self-Taught Learning 10 classes (STL-10)
    (1000 labels) datasets. For all the methods, variations of the Wide-ResNet CNN
    backbone were used. The average accuracy for each test configuration was similar
    to the results yielded by ReMixMatch, with no statistical significance tests performed.
    Comparable results yielded by FixMatch are depicted in Table [I](#S2.T1 "Table
    I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning
    ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，提出了一种被称为FixMatch的SSDL方法，参考了[[80](#bib.bib80)]。作者认为，与其他技术相比，这种简化的SSDL方法具有一定优势。FixMatch基于伪标签和一致性正则化的SSDL。损失函数使用交叉熵标签损失项，并对标记数据进行弱增强。对于未标记数据的损失项，也使用交叉熵，但对于强未标记观测值使用其相应的伪标签。软伪标签是通过弱变换计算的，取模型输出的最大logit。因此，没有进行模型输出的锐化，这与MixMatch不同。强增强使用了随机增强（RA）[[25](#bib.bib25)]和CT增强（CTA）[[8](#bib.bib8)]进行测试。为了对FixMatch进行基准测试，作者使用了CIFAR-10（40,
    250, 4000标签）、加拿大高级研究院数据集（CIFAR-100）（400, 2500和10000标签）、SVHN（40, 250, 1000标签）和自学学习10类（STL-10）（1000标签）数据集。所有方法均使用了Wide-ResNet
    CNN骨干网的变体。每个测试配置的平均准确度与ReMixMatch的结果相似，没有进行统计显著性检验。FixMatch所产生的可比结果如表[I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey")所示。'
- en: II-C2 Adversarial augmentation based regularization
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 基于对抗增强的正则化
- en: Recent advances in deep generative networks for learning data distribution has
    encouraged its usage in SSDL architectures [[36](#bib.bib36)]. Regularized techniques
    usually employ basic data augmentation pipelines, in order to evaluate the consistency
    term $\mathcal{L}_{u}$. However, generative neural networks can be used to learn
    the distribution of labelled and unlabelled data and generate entirely new observations.
    These are categorized as Generative adversarial Network based Consistency Regularized
    Semi-supervised deep learning (GaNC-SSDL). Learning a good approximation of data
    distribution $\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}\left(\textbf{x}\right)$
    allows the artificial generation of new observations. The observations can be
    added to the unlabelled dataset $S^{(u)}$, or the very same adversarial training
    might lead to a refined set of model parameters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度生成网络在学习数据分布方面的最新进展推动了其在SSDL架构中的使用[[36](#bib.bib36)]。正则化技术通常采用基本的数据增强管道，以评估一致性项$\mathcal{L}_{u}$。然而，生成对抗网络可以用来学习标记和未标记数据的分布，并生成全新的观测数据。这些被分类为基于生成对抗网络的一致性正则化半监督深度学习（GaNC-SSDL）。学习数据分布$\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}\left(\textbf{x}\right)$的良好近似允许人工生成新观测数据。这些观测数据可以添加到未标记数据集$S^{(u)}$中，或者对抗训练可能会导致模型参数的优化。
- en: 'In [[81](#bib.bib81)], the generative network architecture was extended for
    SSDL, by implementing a discriminator function $f_{\textbf{w}_{d}}^{\left(d\right)}$
    able to estimate not only if an observation $\textbf{x}_{i}$ belongs or not to
    one of the classes to discriminate from, but also to which specific class it belongs
    to. The model was named by the authors as Categorical Generative Adversarial Network
    (CAT-GAN), given the capacity of the discriminator to perform ordinary $1-K$ classification.
    Therefore, $f_{\textbf{w}_{d}}^{\left(d\right)}$ is able to estimate the density
    function of an unlabelled observation $\textbf{x}_{i}\in S^{(u)}$, $\hat{\textbf{y}}_{i}=f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$.
    The discriminator model implements a semi-supervised loss function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[81](#bib.bib81)]中，通过实现一个判别函数$f_{\textbf{w}_{d}}^{\left(d\right)}$，将生成网络架构扩展到SSDL中，该函数不仅能够估计观察$\textbf{x}_{i}$是否属于要区分的类别之一，还能估计其属于哪个具体类别。作者将模型命名为分类生成对抗网络（CAT-GAN），因为判别器能够执行普通的$1-K$分类。因此，$f_{\textbf{w}_{d}}^{\left(d\right)}$能够估计未标记观察$\textbf{x}_{i}\in
    S^{(u)}$的密度函数，$\hat{\textbf{y}}_{i}=f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$。判别器模型实现了一个半监督损失函数：
- en: '|  | $\mathcal{L}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)=\mathcal{L}_{l}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i},\textbf{y}_{i}\right)+\mathcal{L}_{u}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)$
    |  | (6) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)=\mathcal{L}_{l}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i},\textbf{y}_{i}\right)+\mathcal{L}_{u}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)$
    |  | (6) |'
- en: with $\mathcal{L}_{l}^{\left(d\right)}\left(\textbf{w}_{d},\textbf{x}_{i}\right)=H_{\textrm{CE}}\left(\textbf{y}_{i},f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)\right),$
    where $H_{\textrm{CE}}$ is the cross entropy. The unsupervised discriminator term
    $\mathcal{L}_{u}^{\left(d\right)}$ was designed for maximizing the certainty for
    unlabelled observations and minimizing it for artificial observations. The authors
    also included a term for encouraging imbalance correction for the $K$ classes.
    The proposed method in [[81](#bib.bib81)] was tested using the CIFAR-10 and Modified
    National Institute of Standards and Technology dataset (MNIST) datasets for SSDL.
    It was compared only against the Pi-M, with marginally better average results
    and no statistical significance analysis of the results. However, the CAT-GAN
    served as a foundation for posterior work on using generative deep models for
    SSDL.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用$\mathcal{L}_{l}^{\left(d\right)}\left(\textbf{w}_{d},\textbf{x}_{i}\right)=H_{\textrm{CE}}\left(\textbf{y}_{i},f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)\right),$
    其中$H_{\textrm{CE}}$是交叉熵。无监督判别器项$\mathcal{L}_{u}^{\left(d\right)}$旨在最大化未标记观察的确定性，并最小化人工观察的确定性。作者还加入了一个项以鼓励对$K$个类别的不平衡修正。[[81](#bib.bib81)]中提出的方法在CIFAR-10和修改版国家标准与技术研究所数据集（MNIST）上进行了测试。它仅与Pi-M进行比较，结果略有改善，但没有对结果进行统计显著性分析。然而，CAT-GAN为后续利用生成深度模型进行SSDL的工作奠定了基础。
- en: 'A breakthrough improvement in training $f_{\textbf{w}_{d}}^{\left(d\right)}$
    and $f_{\textbf{w}_{g}}^{\left(g\right)}$ models was achieved in [[73](#bib.bib73)],
    aiming to overcome the difficulty of training complementary loss functions with
    a stochastic gradient descent algorithm. This problem is known as the Nash equilibrium
    dilemma. The authors yielded such improvement, through a feature matching loss
    for the generator $f_{\textbf{w}_{g}}^{\left(g\right)}$, which seeks to make the
    generated observations match the statistical moments of a real sample from training
    data $S$. The enhanced trainability of the Feature Matching Generative Adversarial
    Network (FM-GAN) was tested in a semi-supervised learning setting. The semi-supervised
    loss function implements an unsupervised term $\mathcal{L}_{u}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)$
    which aims to maximize the discriminator success rate in discriminating both unlabelled
    real observations $\textbf{x}_{i}\in S^{(u)}$ and artificially generated ones.
    The discriminator model $f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$
    outputs the probability of the observation $\textbf{x}_{i}$ belonging to one of
    the real classes. Also in this work, the authors showed how achieving a good semi-supervised
    learning accuracy (thus a good discriminator), often yields a poor generative
    performance. Authors suggested that a bad generator describes better Out of Distribution
    (OOD) data, improving the overall model robustness. Intersected results with the
    rest of the reviewed SSDL methods are depicted in Table [I](#S2.T1 "Table I ‣
    II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning ‣
    II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[73](#bib.bib73)]中，实现了对训练 $f_{\textbf{w}_{d}}^{\left(d\right)}$ 和 $f_{\textbf{w}_{g}}^{\left(g\right)}$
    模型的突破性改进，旨在克服使用随机梯度下降算法训练互补损失函数的困难。这个问题被称为纳什均衡困境。作者通过对生成器 $f_{\textbf{w}_{g}}^{\left(g\right)}$
    使用特征匹配损失函数取得了这样的改进，该函数旨在使生成的观测值与训练数据 $S$ 中真实样本的统计矩匹配。特征匹配生成对抗网络（FM-GAN）的增强训练能力在半监督学习设置中进行了测试。半监督损失函数实现了一个无监督项
    $\mathcal{L}_{u}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)$，旨在最大化鉴别器在区分未标记的真实观测值
    $\textbf{x}_{i}\in S^{(u)}$ 和人工生成的观测值时的成功率。鉴别器模型 $f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$
    输出观测值 $\textbf{x}_{i}$ 属于真实类别之一的概率。在这项工作中，作者还展示了如何在获得良好的半监督学习准确性（即良好的鉴别器）的情况下，通常会导致生成性能较差。作者建议，一个差的生成器更好地描述了分布外（OOD）数据，从而提高了整体模型的鲁棒性。与其他回顾的SSDL方法的交叉结果见表
    [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey")。'
- en: 'In [[26](#bib.bib26)], the authors further explored the inverse relationship
    between generator and semi-supervised discriminate performance with the Bad Generative
    Adversarial Network (Bad-GAN). The experiments showed how a *bad* generator $f_{\textbf{w}_{g}}^{\left(g\right)}$,
    created observations out of the distribution of the concept class $\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}$
    enhancing the performance of the discriminator $f_{\textbf{w}_{d}}^{\left(d\right)}$
    for semi-supervised learning. More specifically, the generator loss $\mathcal{L}^{\left(g\right)}$
    is encouraged to maximize the Kullback-Leibler distance to the sampled data distribution.
    This enforces the boundaries built by the discriminator $f_{\textbf{w}_{d}}^{\left(d\right)}$
    for distractor observations. In [[52](#bib.bib52)] a comparison between the triple
    generative network proposed in [[22](#bib.bib22)] and the bad generator [[26](#bib.bib26)]
    was done. No conclusive results were reached, leading the authors to suggest a
    combination of the approaches to leverage accuracy. For comparison purposes with
    related work, results with the CIFAR-10 are described in Table [I](#S2.T1 "Table
    I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning
    ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[26](#bib.bib26)]中，作者进一步探讨了生成器与半监督判别性能之间的逆关系，使用了**不良生成对抗网络**（Bad-GAN）。实验展示了一个*不良*生成器
    $f_{\textbf{w}_{g}}^{\left(g\right)}$ 如何从概念类分布 $\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}$
    中产生观察数据，从而提升了半监督学习中判别器 $f_{\textbf{w}_{d}}^{\left(d\right)}$ 的性能。更具体地说，生成器损失 $\mathcal{L}^{\left(g\right)}$
    被鼓励最大化与采样数据分布的 Kullback-Leibler 距离。这强制了判别器 $f_{\textbf{w}_{d}}^{\left(d\right)}$
    为干扰观察建立的边界。在[[52](#bib.bib52)]中，对比了[[22](#bib.bib22)]中提出的三重生成网络和[[26](#bib.bib26)]中的不良生成器。没有得出结论性的结果，导致作者建议结合这些方法以提高准确性。为与相关工作进行比较，CIFAR-10
    的结果描述在表格 [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized
    semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    中。'
- en: Later, in [[68](#bib.bib68)], the authors proposed a co-training adversarial
    regularization approach for the Co-trained Generative Adversarial Network (Co-GAN),
    making use of the consistency assumption of two different models $f_{\textbf{w}_{d_{1}}}^{\left(d_{1}\right)}$
    and $f_{\textbf{w}_{d_{2}}}^{\left(d_{2}\right)}$. Each model is trained with
    a different view from the same observation $\textbf{x}_{i}=\left\langle\textbf{x}_{i}^{\left(v_{1}\right)},\textbf{x}_{i}^{\left(v_{2}\right)}\right\rangle$.
    A general loss function $\mathcal{L}\left(S\right)=\mathcal{L}_{l}\left(S_{l}\right)+\mathcal{L}_{u}\left(S^{(u)}\right)$
    is minimized, with the unsupervised loss function defined as
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，在[[68](#bib.bib68)]中，作者提出了一种用于联合生成对抗网络（Co-GAN）的共同训练对抗正则化方法，利用了两个不同模型 $f_{\textbf{w}_{d_{1}}}^{\left(d_{1}\right)}$
    和 $f_{\textbf{w}_{d_{2}}}^{\left(d_{2}\right)}$ 的一致性假设。每个模型都使用来自相同观察 $\textbf{x}_{i}=\left\langle\textbf{x}_{i}^{\left(v_{1}\right)},\textbf{x}_{i}^{\left(v_{2}\right)}\right\rangle$
    的不同视图进行训练。最小化了一个通用损失函数 $\mathcal{L}\left(S\right)=\mathcal{L}_{l}\left(S_{l}\right)+\mathcal{L}_{u}\left(S^{(u)}\right)$，其中无监督损失函数定义为
- en: '|  | $\mathcal{L}_{u}\left(f_{\textbf{w}},S^{(u)}\right)=\lambda_{\textrm{cot}}\mathcal{L}_{\textrm{cot}}\left(f_{\textbf{w}},S^{(u)}\right)+\lambda_{\textrm{dif}}\mathcal{L}_{\textrm{dif}}\left(f_{\textbf{w}},\textbf{z}\right).$
    |  | (7) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{u}\left(f_{\textbf{w}},S^{(u)}\right)=\lambda_{\textrm{cot}}\mathcal{L}_{\textrm{cot}}\left(f_{\textbf{w}},S^{(u)}\right)+\lambda_{\textrm{dif}}\mathcal{L}_{\textrm{dif}}\left(f_{\textbf{w}},\textbf{z}\right).$
    |  | (7) |'
- en: The term $\mathcal{L}_{\textrm{cot}}$ measures the expected consistency of the
    two models using two views from the same observation, through the Jensen-Shannon
    divergence. In $\mathcal{L}_{\textrm{dif}}\left(\textbf{z}\right)$, each model
    artificially generates observations to deceive the other one. This stimulates
    a view difference between the models, to avoid them collapsing into each other.
    The coefficients $\lambda_{\textrm{cot}}$ and $\lambda_{\textrm{dif}}$ weigh the
    contribution of each term. Therefore, for each view, a generator is trained and
    the models $f_{\textbf{w}_{d_{1}}}^{\left(d_{1}\right)}$ and $f_{\textbf{w}_{d_{2}}}^{\left(d_{2}\right)}$
    play the detective role. The proposed method out performed MTM, TEM and the Bad-GAN
    according to [[68](#bib.bib68)]. Experiments were performed with more than two
    observation views, generalizing the model for a multi-view layout $\textbf{x}_{i}=\left\langle\textbf{x}_{i}^{\left(v_{1}\right)},\textbf{x}_{i}^{\left(v_{2}\right)}\right\rangle$.
    The best performing model implemented 8 views, henceforth referred in this document
    as Co-trained Generative Adversarial Network with 8 views (Co-8-GAN). We include
    results of the benchmarks done in [[68](#bib.bib68)] with the SVHN and CIFAR-10
    datasets. The authors did not report any statistical significance analysis of
    the provided results.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 $\mathcal{L}_{\textrm{cot}}$ 通过 Jensen-Shannon 散度测量两个模型在使用相同观测的两个视图时的期望一致性。在
    $\mathcal{L}_{\textrm{dif}}\left(\textbf{z}\right)$ 中，每个模型人工生成观测以欺骗另一个模型。这刺激了模型之间的视图差异，以避免它们互相崩溃。系数
    $\lambda_{\textrm{cot}}$ 和 $\lambda_{\textrm{dif}}$ 权衡了每个术语的贡献。因此，对于每个视图，训练一个生成器，而模型
    $f_{\textbf{w}_{d_{1}}}^{\left(d_{1}\right)}$ 和 $f_{\textbf{w}_{d_{2}}}^{\left(d_{2}\right)}$
    扮演侦探角色。根据 [[68](#bib.bib68)]，所提出的方法优于 MTM、TEM 和 Bad-GAN。实验使用了两个以上的观测视图，推广了模型到多视图布局
    $\textbf{x}_{i}=\left\langle\textbf{x}_{i}^{\left(v_{1}\right)},\textbf{x}_{i}^{\left(v_{2}\right)}\right\rangle$。表现最佳的模型实现了
    8 个视图，文档中称为 8 视图联合生成对抗网络（Co-8-GAN）。我们包括了 [[68](#bib.bib68)] 中使用 SVHN 和 CIFAR-10
    数据集的基准结果。作者没有报告提供结果的统计显著性分析。
- en: 'The Triple Generative Adversarial Network (Triple-GAN) [[22](#bib.bib22)] addressed
    the aforementioned inverse relationship between generative and semi-supervised
    classification performance, by training three different models, detailed as follows.
    First, a classifier $f_{\textbf{w}_{c}}^{\left(c\right)}\left(\textbf{x}_{i}\right)$
    which learns the data distribution $\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}$ and
    outputs pseudo-labels for artificially generated observations. Secondly, a class-conditional
    generator $f_{\textbf{w}_{g}}^{\left(g\right)}$ able to generate observations
    for each individual class. Thirdly, a discriminator $f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$,
    which rejects observations out of the labelled classes. The architecture uses
    pseudo-labelling, since the discriminator uses the pseudo labels of the classifier
    $\hat{\textbf{y}}_{i}=f_{\textbf{w}_{c}}^{\left(c\right)}\left(\textbf{x}_{i}\right)$.
    Nevertheless, a consistency regularization was implemented in the classifier loss
    $\mathcal{L}^{\left(c\right)}$. The results using CIFAR-10 with the settings also
    tested in the rest of the reviewed works are depicted in tables [I](#S2.T1 "Table
    I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning
    ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '三重生成对抗网络（Triple-GAN）[[22](#bib.bib22)] 通过训练三种不同的模型，解决了生成模型与半监督分类性能之间的逆向关系，具体如下。首先，一个分类器
    $f_{\textbf{w}_{c}}^{\left(c\right)}\left(\textbf{x}_{i}\right)$ 学习数据分布 $\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}$
    并输出人工生成观测值的伪标签。其次，一个条件生成器 $f_{\textbf{w}_{g}}^{\left(g\right)}$ 能够为每个类别生成观测值。第三，一个鉴别器
    $f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$，它会拒绝标签类别之外的观测值。该架构使用伪标签，因为鉴别器使用分类器的伪标签
    $\hat{\textbf{y}}_{i}=f_{\textbf{w}_{c}}^{\left(c\right)}\left(\textbf{x}_{i}\right)$。尽管如此，分类器损失
    $\mathcal{L}^{\left(c\right)}$ 中实现了一个一致性正则化。使用 CIFAR-10 和其他审阅工作的设置测试的结果见表 [I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey")。'
- en: II-C3 Graph based regularization
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 基于图的正则化
- en: Graph based Regularized Semi-Supervised Deep Learning (GR-SSDL) is based on
    previous graph based regularization techniques [[34](#bib.bib34)]. The core idea
    of GR-SSDL is to preserve mutual distance from observations in the dataset $S$
    (for both labelled and unlabelled) in a new feature space. An embedding is built
    through a mapping function $\check{\textbf{x}}_{i}=h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$
    which reduces the input dimensionality $d$ to $\check{d}$. The mutual distance
    of the observations in the original input space $\textbf{x}_{i}\in\mathbb{R}^{d}$
    , represented in the matrix $W\in\mathbb{R}^{n\times n}$, with $W_{i,j}=\delta\left(\textbf{x}_{i},\textbf{x}_{j}\right)$
    is meant to be preserved in the new feature space $\check{\textbf{x}}_{i}\in\mathbb{R}^{\check{d}}$.
    The multidimensional scaling algorithm developed in [[48](#bib.bib48)] is one
    of the first approaches to preserve the mutual distance of the embeddings of the
    observations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的正则化半监督深度学习（GR-SSDL）基于以前的图正则化技术 [[34](#bib.bib34)]。GR-SSDL 的核心思想是将数据集 $S$（包括标记和未标记）中的观察值的相互距离保留在新的特征空间中。通过映射函数
    $\check{\textbf{x}}_{i}=h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$
    构建了一个嵌入，将输入维度 $d$ 减少到 $\check{d}$。在原始输入空间 $\textbf{x}_{i}\in\mathbb{R}^{d}$ 中的观察值的相互距离，以矩阵
    $W\in\mathbb{R}^{n\times n}$ 表示，$W_{i,j}=\delta\left(\textbf{x}_{i},\textbf{x}_{j}\right)$，旨在新的特征空间
    $\check{\textbf{x}}_{i}\in\mathbb{R}^{\check{d}}$ 中保留。[[48](#bib.bib48)] 开发的多维尺度算法是保留观察值嵌入的相互距离的首批方法之一。
- en: 'More recently, in [[55](#bib.bib55)], a graph-based regularization was implemented
    in the Smooth Neighbors on Teacher Graphs Model (SNTGM). The model aims to smooth
    the consistency of the classifier along the observations in a cluster, and not
    only the artificially created observations by the previous consistency-based regularization
    techniques. The proposed approach implements both a consistency based regularization
    $\mathcal{L}_{c}$ with weight $\lambda_{1}$, and a guided embedding $\mathcal{L}_{e}$
    with coefficient $\lambda_{2}$:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在 [[55](#bib.bib55)] 中，在平滑邻域教师图模型（SNTGM）中实施了基于图的正则化。该模型旨在平滑分类器在簇中观察的一致性，而不仅仅是先前一致性基于正则化技术人工创建的观察。所提出的方法同时实现了带有权重
    $\lambda_{1}$ 的一致性基正则化 $\mathcal{L}_{c}$ 和带有系数 $\lambda_{2}$ 的引导嵌入 $\mathcal{L}_{e}$：
- en: '|  | $\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j},W_{i,j}\right)=\lambda_{1}\mathcal{L}_{c}\left(f_{\textbf{w}},\textbf{x}_{i}\right)+\lambda_{2}\mathcal{L}_{e}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j},W\right)$
    |  | (8) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j},W_{i,j}\right)=\lambda_{1}\mathcal{L}_{c}\left(f_{\textbf{w}},\textbf{x}_{i}\right)+\lambda_{2}\mathcal{L}_{e}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j},W\right)$
    |  | (8) |'
- en: 'where $\textbf{x}_{i},\textbf{x}_{j}\in S^{(u)}$. $\mathcal{L}_{c}$ measures
    the prediction consistency, by using previous approaches in consistency based
    regularized techniques. The term $\mathcal{L}_{e}$ implements the observation
    embedding, with a $\gamma$ margin-restricted distance. To build the neighbourhood
    matrix $W$, the authors in [[55](#bib.bib55)] used label information instead of
    computing the distance between the observations. Regarding unlabelled observations
    in $S^{(u)}$, the authors estimated the output of the teacher to be $\hat{y}_{i}=f_{\textbf{w}^{\prime}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)$.
    Thus, the neighbourhood matrix is given as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{x}_{i},\textbf{x}_{j}\in S^{(u)}$。$\mathcal{L}_{c}$ 通过使用基于一致性的正则化技术的先前方法来测量预测的一致性。术语
    $\mathcal{L}_{e}$ 实现了观察嵌入，具有 $\gamma$ 边界限制的距离。为了构建邻域矩阵 $W$，[[55](#bib.bib55)]
    的作者使用了标签信息，而不是计算观察之间的距离。关于 $S^{(u)}$ 中的未标记观察，作者估计教师的输出为 $\hat{y}_{i}=f_{\textbf{w}^{\prime}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)$。因此，邻域矩阵如下所示：
- en: '|  | $W_{i,j}=\begin{cases}1&amp;\textrm{if }\hat{y}_{i}=\hat{y}_{j}\\ 0&amp;\textrm{if
    }\hat{y}_{i}\neq\hat{y}_{j}\end{cases}.$ |  | (9) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $W_{i,j}=\begin{cases}1&amp;\textrm{如果 }\hat{y}_{i}=\hat{y}_{j}\\ 0&amp;\textrm{如果
    }\hat{y}_{i}\neq\hat{y}_{j}\end{cases}.$ |  | (9) |'
- en: The loss term $\mathcal{L}_{e}$ encourages similar representations for observations
    within the same class and higher difference for representations of different classes.
    The algorithm was combined and tested with a Pi-M and VATM consistency functions,
    henceforth Smooth Neighbors on Teacher Graphs Model with Pi-Model regularization
    (SNTGM+Pi-M) and Smooth Neighbors on Teacher Graphs Model with Virtual Adversarial
    Training (SNTGM+VATM), respectively.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 损失项 $\mathcal{L}_{e}$ 鼓励相同类别观察的表示相似，并且不同类别的表示有更高的差异。该算法与 Pi-M 和 VATM 一致性函数结合并测试，分别称为带有
    Pi-Model 正则化的平滑邻居教师图模型 (SNTGM+Pi-M) 和带有虚拟对抗训练的平滑邻居教师图模型 (SNTGM+VATM)。
- en: '| Model | Category | $n_{l}=2000$ | $n_{l}=4000$ | $n_{l}=5000$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Model | Category | $n_{l}=2000$ | $n_{l}=4000$ | $n_{l}=5000$ |'
- en: '| Supervised only | Supervised | 33.94$\pm$0.73[[86](#bib.bib86)] | 20.02$\pm$0.6[[86](#bib.bib86)]
    | 18.02$\pm$0.6[[86](#bib.bib86)] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Supervised only | Supervised | 33.94$\pm$0.73[[86](#bib.bib86)] | 20.02$\pm$0.6[[86](#bib.bib86)]
    | 18.02$\pm$0.6[[86](#bib.bib86)] |'
- en: '| Pi-M |  | 18.02$\pm$0.6[[86](#bib.bib86)] | 13.2$\pm$0.27[[49](#bib.bib49)]
    | 6.06$\pm$0.11[[49](#bib.bib49)] |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Pi-M |  | 18.02$\pm$0.6[[86](#bib.bib86)] | 13.2$\pm$0.27[[49](#bib.bib49)]
    | 6.06$\pm$0.11[[49](#bib.bib49)] |'
- en: '| TEM |  | - | 12.16$\pm$0.24[[49](#bib.bib49)] | 5.6$\pm$0.1 [[49](#bib.bib49)]
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| TEM |  | - | 12.16$\pm$0.24[[49](#bib.bib49)] | 5.6$\pm$0.1 [[49](#bib.bib49)]
    |'
- en: '| VATM+EM |  | - | 13.15$\pm$0.21[[59](#bib.bib59)] | - |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| VATM+EM |  | - | 13.15$\pm$0.21[[59](#bib.bib59)] | - |'
- en: '| VATM |  | - | 14.87$\pm$0.13[[59](#bib.bib59)] | - |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| VATM |  | - | 14.87$\pm$0.13[[59](#bib.bib59)] | - |'
- en: '| MTM |  | 15.73$\pm$0.31[[86](#bib.bib86)] | 12.31$\pm$0.28[[86](#bib.bib86)]
    | 5.94$\pm$0.15[[68](#bib.bib68), [86](#bib.bib86)] |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MTM |  | 15.73$\pm$0.31[[86](#bib.bib86)] | 12.31$\pm$0.28[[86](#bib.bib86)]
    | 5.94$\pm$0.15[[68](#bib.bib68), [86](#bib.bib86)] |'
- en: '| SESEMI |  | 14.22$\pm$0.27[[89](#bib.bib89)] | 11.65$\pm$0.13[[89](#bib.bib89)]
    | - |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SESEMI |  | 14.22$\pm$0.27[[89](#bib.bib89)] | 11.65$\pm$0.13[[89](#bib.bib89)]
    | - |'
- en: '| METM |  | - | 11.29$\pm$0.24[[72](#bib.bib72)] | - |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| METM |  | - | 11.29$\pm$0.24[[72](#bib.bib72)] | - |'
- en: '| TransM | RC-SSDL | 14.65$\pm$0.33[[77](#bib.bib77)] | 10.9$\pm$0.23[[77](#bib.bib77)]
    | 5.2$\pm$0.14[[77](#bib.bib77)] |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| TransM | RC-SSDL | 14.65$\pm$0.33[[77](#bib.bib77)] | 10.9$\pm$0.23[[77](#bib.bib77)]
    | 5.2$\pm$0.14[[77](#bib.bib77)] |'
- en: '| TransM+MTM |  | 13.54$\pm$0.32[[77](#bib.bib77)] | 9.3$\pm$0.55[[77](#bib.bib77)]
    | 5.19$\pm$0.14[[77](#bib.bib77)] |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| TransM+MTM |  | 13.54$\pm$0.32[[77](#bib.bib77)] | 9.3$\pm$0.55[[77](#bib.bib77)]
    | 5.19$\pm$0.14[[77](#bib.bib77)] |'
- en: '| MeM |  | - | 11.91$\pm$0.22[[19](#bib.bib19)] | - |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| MeM |  | - | 11.91$\pm$0.22[[19](#bib.bib19)] | - |'
- en: '| MixMatch |  | - | 6.42$\pm$0.10[[9](#bib.bib9)] | - |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MixMatch |  | - | 6.42$\pm$0.10[[9](#bib.bib9)] | - |'
- en: '| ReMixMatch |  | - | 4.72$\pm$0.13[[8](#bib.bib8)] | - |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ReMixMatch |  | - | 4.72$\pm$0.13[[8](#bib.bib8)] | - |'
- en: '| FixMatch(RA) |  | - | 4.31$\pm$0.15 | - |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| FixMatch(RA) |  | - | 4.31$\pm$0.15 | - |'
- en: '| FixMatch(CTA) |  | - | 4.26$\pm$0.05 | - |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| FixMatch(CTA) |  | - | 4.26$\pm$0.05 | - |'
- en: '| SNTGM+VATM | GR-SSDL | - | 12.49$\pm$0.36[[55](#bib.bib55)] | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| SNTGM+VATM | GR-SSDL | - | 12.49$\pm$0.36[[55](#bib.bib55)] | - |'
- en: '| SNTGM+Pi-M |  | - | 13.62$\pm$0.17[[55](#bib.bib55)] | - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SNTGM+Pi-M |  | - | 13.62$\pm$0.17[[55](#bib.bib55)] | - |'
- en: '| FM-GAN |  | - | 18.63$\pm$2.32[[23](#bib.bib23)] | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| FM-GAN |  | - | 18.63$\pm$2.32[[23](#bib.bib23)] | - |'
- en: '| CAT-GAN |  | - | 19.58$\pm$0.58[[81](#bib.bib81)] | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| CAT-GAN |  | - | 19.58$\pm$0.58[[81](#bib.bib81)] | - |'
- en: '| Co-8-GAN | GaNC-SSDL | - | 8.35$\pm$0.06[[68](#bib.bib68)] | - |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Co-8-GAN | GaNC-SSDL | - | 8.35$\pm$0.06[[68](#bib.bib68)] | - |'
- en: '| Bad-GAN |  | - | 14.41$\pm$0.3[[26](#bib.bib26)] | - |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Bad-GAN |  | - | 14.41$\pm$0.3[[26](#bib.bib26)] | - |'
- en: '| Triple-GAN |  | - | 16.99$\pm$0.36[[22](#bib.bib22)] | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Triple-GAN |  | - | 16.99$\pm$0.36[[22](#bib.bib22)] | - |'
- en: '| Tri-Net |  | - | 8.45$\pm$0.22[[29](#bib.bib29)] | - |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Tri-Net |  | - | 8.45$\pm$0.22[[29](#bib.bib29)] | - |'
- en: '| SaaSM | PLT-SSDL | - | 10.94$\pm$0.07[[23](#bib.bib23)] | - |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| SaaSM | PLT-SSDL | - | 10.94$\pm$0.07[[23](#bib.bib23)] | - |'
- en: '| TriNet+Pi |  | - | 8.3$\pm$0.15[[29](#bib.bib29)] | - |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| TriNet+Pi |  | - | 8.3$\pm$0.15[[29](#bib.bib29)] | - |'
- en: 'TABLE I: SSDL error rates (the lower the better) from literature of state of
    the art methods, using the CIFAR-10 dataset. As number of labels, $n_{l}=2000$,
    $n_{l}=4000$ and $n_{l}=5000$ were the most frequently used in the literature.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 使用 CIFAR-10 数据集的最先进方法的 SSDL 错误率（越低越好）。文献中使用的标签数量 $n_{l}=2000$、$n_{l}=4000$
    和 $n_{l}=5000$ 是最常见的。'
- en: III Dealing with distribution mismatch in SSDL
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 处理 SSDL 中的分布不匹配
- en: In [[63](#bib.bib63)] and [[15](#bib.bib15)], extensive evaluation of the distribution
    mismatch setting is developed. The authors agreed upon its decisive impact in
    the performance of SSDL methods and the consequent importance of increase their
    robustness to such phenomena. SSDL methods designed to deal with the distribution
    mismatch between $S^{(u)}$ and $S^{(l)}$ often use ideas and concepts from OOD
    detection techniques. Most methods for SSDL that are robust to distribution mismatch
    calculate a weight or a coefficient referred to as the function $\mathcal{H}\left(\textbf{x}_{j}^{u}\right)$
    in this article, to score how likely the unlabelled observation $\textbf{x}_{j}^{u}$
    is OOD. The score can be used to either completely discard $\textbf{x}_{j}^{u}$
    from the unlabelled training dataset (referred to as hard thresholding in this
    work) or to weigh it (soft thresholding). Thresholding the unlabelled dataset
    can take place as a data pre-processing step or in an online fashion during training.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[63](#bib.bib63)] 和 [[15](#bib.bib15)] 中，对分布不匹配设置进行了广泛评估。作者一致认为，这对 SSDL 方法的性能具有决定性影响，因此提高其对这种现象的鲁棒性具有重要意义。旨在处理
    $S^{(u)}$ 和 $S^{(l)}$ 之间分布不匹配的 SSDL 方法通常使用来自 OOD 检测技术的理念和概念。大多数对分布不匹配鲁棒的 SSDL
    方法计算一个称为 $\mathcal{H}\left(\textbf{x}_{j}^{u}\right)$ 的函数，用于评估未标记观察 $\textbf{x}_{j}^{u}$
    是否 OOD。该评分可以用于完全丢弃 $\textbf{x}_{j}^{u}$（在本文中称为硬阈值处理），或对其进行加权（软阈值处理）。阈值处理未标记数据集可以作为数据预处理步骤，或在训练过程中在线进行。
- en: 'Therefore, we first review modern approaches for OOD detection using deep learning
    in [Section III-A](#S3.SS1 "III-A OOD Data Detection ‣ III Dealing with distribution
    mismatch in SSDL ‣ Semi-supervised Deep Learning for Image Classification with
    Distribution Mismatch: A Survey"). Later we address state of the art SSDL methods
    that are robust to distribution mismatch in [Section III-B](#S3.SS2 "III-B Semi-supervised
    Deep Learning methods robust to distribution mismatch ‣ III Dealing with distribution
    mismatch in SSDL ‣ Semi-supervised Deep Learning for Image Classification with
    Distribution Mismatch: A Survey").'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们首先在 [Section III-A](#S3.SS1 "III-A OOD Data Detection ‣ III Dealing with
    distribution mismatch in SSDL ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey") 中回顾现代基于深度学习的 OOD 检测方法。随后，我们在 [Section III-B](#S3.SS2
    "III-B Semi-supervised Deep Learning methods robust to distribution mismatch ‣
    III Dealing with distribution mismatch in SSDL ‣ Semi-supervised Deep Learning
    for Image Classification with Distribution Mismatch: A Survey") 中介绍对分布不匹配鲁棒的最先进
    SSDL 方法。'
- en: III-A OOD Data Detection
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A OOD 数据检测
- en: OOD data detection is a classic challenge faced in machine learning applications.
    It corresponds to the detection of data observations which are far from the training
    dataset distribution [[38](#bib.bib38)]. Individual and collective outlier detection
    are particular problems of OOD detection [[79](#bib.bib79)]. Other particular
    OOD detection settings have been tackled in the literature such as novel data
    and anomaly detection [[65](#bib.bib65)] and infrequent event detection [[37](#bib.bib37),
    [1](#bib.bib1)]. Well studied and known concepts have been developed within the
    pattern recognition community related to OOD detection. Some of them are kernel
    representations [[87](#bib.bib87)], density estimation [[57](#bib.bib57)], robust
    moment estimation [[71](#bib.bib71)] and prototyping [[57](#bib.bib57)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: OOD 数据检测是机器学习应用中面临的经典挑战。它涉及到检测远离训练数据集分布的数据观察 [[38](#bib.bib38)]。个体和集体异常值检测是
    OOD 检测的特定问题 [[79](#bib.bib79)]。文献中已经解决了其他特定的 OOD 检测设置，例如新颖数据和异常检测 [[65](#bib.bib65)]
    和不频繁事件检测 [[37](#bib.bib37), [1](#bib.bib1)]。在模式识别社区中已经开发了与 OOD 检测相关的成熟和已知概念。其中一些包括核表示
    [[87](#bib.bib87)]、密度估计 [[57](#bib.bib57)]、鲁棒矩估计 [[71](#bib.bib71)] 和原型 [[57](#bib.bib57)]。
- en: 'The more recent developments in the burgeoning field of deep learning for image
    analysis tasks have boosted the interest in developing OOD detection methods for
    deep learning architectures. According to our literature survey, we found that
    OOD detection methods for deep learning architectures can be classified into the
    following categories: DNN output based and DNN feature space based. In the next
    subsections we proceed to describe the most popular methods within each category.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分析任务中，深度学习领域的最新发展引发了对开发深度学习架构 OOD 检测方法的兴趣。根据我们的文献调查，我们发现深度学习架构的 OOD 检测方法可以分为以下几类：基于
    DNN 输出和基于 DNN 特征空间。接下来的小节中，我们将描述每个类别中最受欢迎的方法。
- en: III-A1 DNN output based
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 基于 DNN 输出
- en: In [[39](#bib.bib39)] the authors proposed a simple method known as Out of DIstribution
    detector for Neural networks (ODIN) to score input observations according to its
    OOD probability. The proposed method by the authors implements a confidence score
    based upon the DNN model’s output which is transformed using a softmax layer.
    The maximum softmax value of all the units is associated with the model’s confidence.
    The authors argued that this scores is able to distinguish in-distribution from
    OOD data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[39](#bib.bib39)]中，作者提出了一种简单的方法，称为**神经网络的分布外检测器（ODIN）**，用于根据其OOD概率对输入观测进行评分。作者提出的方法实现了一个基于DNN模型输出的置信度分数，该分数通过softmax层进行变换。所有单位的最大softmax值与模型的置信度相关联。作者认为，这种分数能够区分分布内数据和OOD数据。
- en: More recently, in [[53](#bib.bib53)], the authors argued that using the softmax
    output of a DNN model to estimate OOD probability can be often a misleading measure
    in non calibrated models. Therefore, the authors in [[53](#bib.bib53)] proposed
    a DNN calibration method. This method implements a temperature coefficient which
    aims to improve the model’s output discriminatory power between OOD and In-Distribution
    (IOD) data. In [[53](#bib.bib53)] the authors tested ODIN against the softmax
    based OOD score proposed in [[39](#bib.bib39)], with significantly better results
    obtained by ODIN.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在[[53](#bib.bib53)]中，作者认为使用DNN模型的softmax输出估计OOD概率在未校准模型中常常会导致误导。因此，[[53](#bib.bib53)]中的作者提出了一种DNN校准方法。该方法实现了一个温度系数，旨在提高模型在OOD和分布内（IOD）数据之间的输出区分能力。在[[53](#bib.bib53)]中，作者对ODIN进行了测试，结果显著优于[[39](#bib.bib39)]中提出的基于softmax的OOD分数。
- en: An alternative approach for OOD detection using the DNN’s output is the popular
    approach known as Monte Carlo Dropout (MCD) [[54](#bib.bib54), [46](#bib.bib46)].
    This approach uses the distribution of $N$ model forward passes (input evaluation),
    using the same input observation with mild transformations (noise, flips, etc.)
    or injecting noise to the model using a parameter drop-out. The output distribution
    is used to calculate distribution moments (variance usually) or other scalar distribution
    descriptors such as the entropy. This idea has been implemented in OOD detection
    settings, as OOD observations might score higher entropy or variance values [[43](#bib.bib43),
    [75](#bib.bib75)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DNN输出进行OOD检测的另一种方法是被称为**蒙特卡罗丢弃（Monte Carlo Dropout，MCD）**的流行方法[[54](#bib.bib54),
    [46](#bib.bib46)]。这种方法利用$N$次模型前向传递（输入评估）的分布，通过对相同的输入观测进行轻微变换（噪声、翻转等）或通过丢弃参数向模型注入噪声。输出分布用于计算分布矩（通常是方差）或其他标量分布描述符，如熵。这个思路已经在OOD检测中得到了应用，因为OOD观测可能会获得更高的熵或方差值[[43](#bib.bib43),
    [75](#bib.bib75)]。
- en: III-A2 DNN’s feature space based
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 基于DNN特征空间的
- en: Recently, as an alternative approach for OOD detection, different methods use
    the feature or latent space for OOD detection. In [[51](#bib.bib51)] the authors
    propose the usage of the Mahalanobis distance in the feature space between the
    training dataset and the input observation. Therefore the covariance matrix and
    a mean observation is calculated from the training data (in-distribution data).
    By using the Mahalanobis distance, the proposed method by the authors assume a
    Gaussian distribution of the training data. Also, the proposed method was tested
    mixed with the ODIN calibration method previously discussed. The authors reported
    a superior performance of their method over the softmax based score proposed in
    [[39](#bib.bib39)] and ODIN [[53](#bib.bib53), [39](#bib.bib39)]. However no statistical
    significance analysis of the results was carried out.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，作为OOD检测的另一种方法，不同的方法使用特征或潜在空间进行OOD检测。在[[51](#bib.bib51)]中，作者提出了在特征空间中使用马氏距离来衡量训练数据集和输入观测之间的距离。因此，从训练数据（分布内数据）中计算协方差矩阵和均值观测。通过使用马氏距离，作者提出的方法假设训练数据服从高斯分布。此外，所提出的方法与之前讨论过的ODIN校准方法混合进行测试。作者报告称，他们的方法在性能上优于[[39](#bib.bib39)]中提出的基于softmax的分数和ODIN[[53](#bib.bib53),
    [39](#bib.bib39)]。然而，结果没有进行统计显著性分析。
- en: In [[91](#bib.bib91)] another feature space based was proposed, referred to
    as Deterministic Uncertainty Quantification (DUQ) by the authors. The proposed
    method was tested for both uncertainty estimation and OOD detection. It consists
    in calculating a centroid for each one of the classes within the training dataset
    (IOD dataset). Later, for each new observation where either uncertainty estimation
    or OOD detection is intended to be used, the method calculates the distance to
    each centroid. The shortest distance is used as either uncertainty or OOD score.
    DUQ performance for OOD detection was compared against a variation of the MCD
    approach, with a an ensemble of networks for OOD detection. The authors claimed
    a better performance of DUQ for OOD detection, however no statistical analysis
    of the results was done. The benchmark consisted in using CIFAR-10 as an IOD dataset
    and SVHN as a OOD dataset. Therefore, as usual in OOD detection benchmarks, the
    unseen classes setting for the IID assumption violation was tested.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[91](#bib.bib91)]中，作者提出了另一种基于特征空间的方法，称为确定性不确定性量化（DUQ）。该方法在不确定性估计和OOD检测方面都进行了测试。它的核心在于为训练数据集中的每个类别计算一个质心。然后，对于每个新的观察数据，不论是用于不确定性估计还是OOD检测，该方法计算到每个质心的距离。最短的距离被用作不确定性或OOD得分。DUQ在OOD检测方面的表现与一种变体的MCD方法进行了比较，后者使用了一个网络集成进行OOD检测。作者声称DUQ在OOD检测方面表现更好，但未进行结果的统计分析。基准测试使用CIFAR-10作为IOD数据集，SVHN作为OOD数据集。因此，像往常一样，OOD检测基准测试中对IID假设违反的未见类别设置进行了测试。
- en: III-B Semi-supervised Deep Learning methods robust to distribution mismatch
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 适应分布不匹配的半监督深度学习方法
- en: In the literature, there are two most commonly studied causes for the violation
    of the IID assumption. The first one is the prior probability shift (different
    distribution of the labels) between $S^{(u)}$ and $S^{(l)}$. Novel methods proposed
    to deal with this challenge are described in this section. The other cause for
    the IID violation assumption is the unseen class setting, which has been more
    widely studied. State of the art methods are also discussed in this section.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中通常研究的IID假设违反的两个主要原因是：第一个是$S^{(u)}$和$S^{(l)}$之间的先验概率偏移（标签分布不同）。本节描述了应对这一挑战的新方法。另一个IID假设违反的原因是未见类别设置，这一方面的研究较为广泛。当前最先进的方法也在本节中讨论。
- en: III-B1 Unseen classes as a cause for the distribution mismatch
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 未见类别作为分布不匹配的原因
- en: Most of the SSDL methods designed to deal with distribution mismatch have been
    tested using a labelled dataset with different classes (usually less) from the
    unlabelled dataset. For example, in this setting, for $S^{(l)}$ the SVHN is used,
    and for $S^{(u)}$ a percentage of the sample is drawn from the CIFAR-10 dataset,
    and the rest from the SVHN dataset. In this context, the dataset CIFAR-10 is often
    referred to as the OOD data contamination source. Benchmarks with varying degrees
    of data contamination for SSDL with distribution mismatch can be found in literature.
    In this section we describe the most recent approaches for SSDL under distribution
    mismatch with unseen classes in the literature.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数旨在处理分布不匹配的SSDL方法都是使用标签数据集（通常是较少的类别）与未标签数据集进行测试的。例如，在这种设置中，对于$S^{(l)}$使用SVHN，而对于$S^{(u)}$，则从CIFAR-10数据集中抽取一定比例的样本，其余则来自SVHN数据集。在这个背景下，CIFAR-10数据集通常被称为OOD数据污染源。文献中可以找到针对分布不匹配的SSDL在不同程度数据污染下的基准测试。在这一部分，我们描述了文献中针对分布不匹配和未见类别的SSDL的最新方法。
- en: In [[61](#bib.bib61)] an SSDL method for dealing with distribution mismatch
    was developed. The authors refer to this method as RealMix. It was proposed as
    an extension of the MixMatch SSDL method. Therefore, it uses the consistency based
    regularization with augmented observations, and the MixUp data augmentation method
    implemented in MixMatch. For distribution mismatch robustness, RealMix uses the
    softmax of the output from the model as a confidence value, to score each unlabelled
    observation. During training, in the loss function, the unlabelled observations
    are masked out using such confidence score. The $\phi$ percent of unlabelled observations
    with the lowest confidence scores are discarded at each training epoch. To test
    their method, the authors deployed a benchmark based upon CIFAR-10 with a disjoint
    set of classes for $S^{(l)}$ and $S^{(u)}$. The reported results showed a slight
    accuracy gain of the proposed method against other SSDL approaches not designed
    for distribution mismatch robustness. A fixed number of labelled observations
    and CNN backbones were used. No statistical significance tests over the results
    were done. RealMix can be categorized as a DNN output based OOD scoring method.
    The thresholding is done during training, several times, using binary or hard
    thresholding (keep or discard). The testing can be considered limited as the OOD
    contamination source causes a hard distribution mismatch.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[61](#bib.bib61)]中，提出了一种处理分布不匹配的SSDL方法。作者将这种方法称为RealMix。它被提出作为MixMatch SSDL方法的扩展。因此，它使用基于一致性的正则化方法，并结合了MixMatch中实现的MixUp数据增强方法。为了提高对分布不匹配的鲁棒性，RealMix使用模型输出的softmax作为置信度值，以对每个未标记的观察进行评分。在训练过程中，损失函数中使用这样的置信度分数来掩蔽未标记的观察。每次训练周期中，置信度分数最低的$\phi$百分比未标记观察会被丢弃。为了测试他们的方法，作者在CIFAR-10的基础上部署了一个基准测试，并为$S^{(l)}$和$S^{(u)}$设置了不重叠的类别。报告的结果显示，与其他未针对分布不匹配鲁棒性设计的SSDL方法相比，所提出的方法略有准确度提升。使用了固定数量的标记观察和CNN骨干网络。结果未进行统计显著性测试。RealMix可以归类为基于DNN输出的OOD评分方法。阈值设置在训练过程中进行，多次使用二进制或硬阈值（保留或丢弃）。由于OOD污染源导致了严重的分布不匹配，测试可以认为是有限的。
- en: More recently, the method known as Uncertainty Aware Self-Distillation (UASD),
    was proposed in [[20](#bib.bib20)] for SSDL distribution mismatch robustness.
    UASD uses an unsupervised regularized loss function. For each unlabelled observation,
    a pseudo-label is estimated as the average label from an ensemble of models. The
    ensemble is composed of past models yielded in previous training epochs. Similar
    to RealMix, UASD uses the output of a DNN model to score each unlabelled observation.
    However, to increase the robustness of such confidence score, UASD uses the ensemble
    of predictions from past models, to estimate the model’s confidence over its prediction
    for each unlabelled observation. The maximum logits of the ensemble prediction
    is used as the confidence score. Therefore we can categorize the UASD method as
    a DNN output based approach. Also in a similar fashion to RealMix, the estimated
    scores are used for hard-thresholding the unlabelled observations. In a resembling
    trend to RealMix, the authors of the UASD method evaluated their approach using
    the CIFAR-10 dataset. $S^{(l)}$ includes 6 classes of animals, and $S^{(u)}$ samples
    other 4 classes from CIFAR-10, with a varying degree of class distribution mismatch.
    Only five runs were performed to approximate the error-rate distribution, and
    no statistical analysis was done for the results. No varying number of labelled
    observations, or different DNN backbones were tested. UASD was compared with SSDL
    methods not designed for distribution mismatch robustness. From the reported results,
    an accuracy gain of up to 6 percent over previous SSDL methods was yielded by
    UASD, when facing heavy distribution mismatch settings.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，名为不确定性感知自蒸馏（UASD）的方法被提出用于SSDL分布不匹配的鲁棒性[[20](#bib.bib20)]。UASD使用一种无监督正则化的损失函数。对于每个未标记的观察，伪标签被估计为来自模型集成的平均标签。这个集成由在之前训练周期中产生的过去模型组成。与RealMix类似，UASD使用DNN模型的输出对每个未标记的观察进行评分。然而，为了提高这种置信度评分的鲁棒性，UASD使用来自过去模型的预测集成，以估计模型对每个未标记观察的预测置信度。集成预测的最大logits被用作置信度评分。因此，我们可以将UASD方法归类为基于DNN输出的方法。与RealMix类似，估计的分数用于对未标记观察进行硬阈值处理。与RealMix相似，UASD方法的作者使用CIFAR-10数据集评估了他们的方法。$S^{(l)}$包括6类动物，而$S^{(u)}$从CIFAR-10中采样其他4类，具有不同程度的类别分布不匹配。仅进行过五次实验以近似误差率分布，且未对结果进行统计分析。未测试不同数量的标记观察或不同的DNN骨干。UASD与未针对分布不匹配鲁棒性设计的SSDL方法进行了比较。从报告的结果来看，UASD在面对严重分布不匹配设置时，相对于以前的SSDL方法，准确率提高了最多6个百分点。
- en: In [[20](#bib.bib20)], an SSDL approach to deal with distribution mismatch was
    introduced. The authors refer to their proposed approach as Deep Safe Semi-Supervised
    Learning (D3SL). It implements an unsupervised regularization, through the mean
    square loss between the prediction of unlabelled observation and its noisy modification.
    An observation-wise weight for each unlabelled observation is implemented, similar
    to RealMix and UASD. However, the weights for the entire unlabelled dataset are
    calculated using an error gradient optimization approach. Both the model’s parameters
    and the observation-wise weights are estimated in two nested optimization steps.
    Therefore, we can categorize this method as a gradient optimized scoring of the
    unlabelled observations. The weights are continuous or non-binary values, therefore
    we can refer to this method as a softly-thresholded one. According to the authors,
    this increases training time up to $3\times$. The testing benchmark uses the CIFAR-10
    and MNIST datasets. For both of them, 6 classes are used to sample $S^{(l)}$,
    and the remaining for $S^{(u)}$. Only a Wide ResNet-28-10 CNN backbone was used
    with a fixed number of labels. A varying degree of OOD contamination was tested.
    The proposed D3SL method was compared with generic SSDL methods, therefore ignoring
    previous SSDL robust methods to distribution mismatch. From the reported results,
    an averaged accuracy gain of around 2% was yielded by the proposed method under
    the heaviest OOD data contamination settings, with no statistical significance
    reported. Only five runs were done to report such averaged error-rates.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[20](#bib.bib20)]中，介绍了一种应对分布不匹配的SSDL方法。作者将他们提出的方法称为深度安全半监督学习（D3SL）。该方法通过未标记观测的预测与其噪声修改之间的均方损失来实现无监督正则化。为每个未标记观测实现了观测级别的权重，类似于RealMix和UASD。然而，整个未标记数据集的权重是通过误差梯度优化方法计算的。模型的参数和观测级别的权重在两个嵌套的优化步骤中估计。因此，我们可以将该方法归类为对未标记观测进行梯度优化评分的方法。权重是连续的或非二进制的，因此我们可以称此方法为软阈值方法。根据作者的说法，这会将训练时间增加最多$3\times$。测试基准使用了CIFAR-10和MNIST数据集。在这两者中，6个类别用于采样$S^{(l)}$，其余用于$S^{(u)}$。仅使用了固定标签数量的Wide
    ResNet-28-10 CNN骨干网。测试了不同程度的OOD污染。提出的D3SL方法与通用SSDL方法进行了比较，因此忽略了先前的SSDL对分布不匹配的鲁棒方法。从报告的结果来看，在最严重的OOD数据污染设置下，提出的方法平均准确率提高了约2%，但没有报告统计显著性。仅进行了五次实验来报告这样的平均错误率。
- en: 'A similar gradient optimization based method to D3SL can be found in [[95](#bib.bib95)].
    The proposed method is referred to as a Multi-Task Curriculum Framework (MTCF)
    by the authors. Similar to previous methods, MTCF defines an OOD score for the
    unlabelled observations, as an extension to the MixMatch algorithm [[9](#bib.bib9)].
    Such scores are alternately optimized together with the DNN parameters, as seen
    in D3SL. However, the optimization problem is perhaps more simple than the D3SL,
    as the OOD scores are not optimized in a gradient descent fashion directly. Instead,
    the DNN output is used as the OOD score. The usage of a loss function that includes
    the OOD scores, enforces a new condition to the optimization of the DNN parameters.
    This is referred to as a curriculum multi-task learning framework by the authors
    in [[95](#bib.bib95)]. The proposed method was tested in what the authors defined
    as an Open-set semi-supervised learning setting (Open-Set-SSLS), where different
    OOD data contamination sources were used. Regarding the specific benchmarking
    settings, the authors only tested a Wide ResNet DNN backbone, to compare a baseline
    MixMatch method to their proposed approach. No comparison with other SSDL methods
    was performed. The authors used two IOD datasets: CIFAR-10 and SVHN. Four different
    OOD datasets were used: Uniform, Gaussian noise, Tiny ImageNet (TIN) and Large-scale
    Scene Understanding dataset (LSUN). The average of the last 10 checkpoints of
    the model training, using the same partitions was reported (no different partitions
    were tested). A fixed OOD data contamination degree was tested. The reported accuracy
    gains went from 1% to 10%. The usage of the same data partitions inhibited an
    appropriate statistical analysis of the results.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与D3SL类似的梯度优化方法可以在[[95](#bib.bib95)]中找到。作者将所提方法称为多任务课程框架（MTCF）。与之前的方法类似，MTCF为未标记的观察定义了一个OOD分数，作为对MixMatch算法[[9](#bib.bib9)]的扩展。这些分数与DNN参数交替优化，类似于D3SL。然而，这个优化问题可能比D3SL更简单，因为OOD分数并不是直接以梯度下降的方式优化的。相反，DNN输出被用作OOD分数。包含OOD分数的损失函数在DNN参数的优化中引入了一个新条件。作者在[[95](#bib.bib95)]中将其称为课程多任务学习框架。所提方法在作者定义的开放集半监督学习设置（Open-Set-SSLS）中进行了测试，其中使用了不同的OOD数据污染源。关于具体的基准设置，作者仅测试了一个宽度ResNet
    DNN骨干，以将基线MixMatch方法与其提出的方法进行比较。没有与其他SSDL方法进行比较。作者使用了两个IOD数据集：CIFAR-10和SVHN。使用了四个不同的OOD数据集：均匀分布、Gaussian噪声、Tiny
    ImageNet (TIN)和大规模场景理解数据集 (LSUN)。报告了使用相同分区的模型训练最后10个检查点的平均值（未测试不同的分区）。测试了固定的OOD数据污染程度。报告的准确率提升从1%到10%不等。使用相同数据分区阻碍了结果的适当统计分析。
- en: 'In the same trend, authors in [[100](#bib.bib100)] proposed a gradient optimization
    based method to calculate the observation-wise weights for data in $S^{(u)}$.
    Two different gradient approximation methods were tested: Implicit Differentiation
    (IF) and Meta Approximation (MetaA). Authors argue that finding the weights for
    each unlabelled observation in a large sample $S^{(u)}$ is an intractable problem.
    Therefore, both tested methods aim to reduce the computational cost of optimizing
    such weights. Moreover, to further reduce the number of weights to find, the method
    performs a clustering in the feature space. This reduces the number of weights
    to find, as one weight is assigned per cluster. Another interesting finding reported
    by the authors, is the impact of OOD data in batch normalization. Even if the
    OOD data lies far to the decision boundary, if batch normalization is carried
    out, a degradation of performance is likely. If no batch normalization is performed,
    OOD data far from the decision boundary might not significantly harm performance.
    Therefore, the weights found are also used to perform a weighted mini-batch normalization
    of the data. Regarding the benchmarking of the proposed method, the authors used
    the CIFAR-10 and FashionMNIST datasets, with different degrees of OOD contamination.
    The OOD data was sampled from a set of classes excluded from the IOD dataset.
    A WRN-28-2 (WideResNet) backbone was used. No statistical analysis of the results,
    with the same number of partitions across the tested methods was performed. The
    average accuracy gains show a positive margin for the proposed method ranging
    from 5% to 20%.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的趋势下，[[100](#bib.bib100)] 的作者提出了一种基于梯度优化的方法来计算数据在 $S^{(u)}$ 中的观察权重。测试了两种不同的梯度近似方法：隐式微分（IF）和元近似（MetaA）。作者认为，寻找大样本
    $S^{(u)}$ 中每个未标记观察的权重是一个难以处理的问题。因此，这两种测试的方法都旨在减少优化这些权重的计算成本。此外，为了进一步减少需要找到的权重数量，该方法在特征空间中进行聚类。这减少了需要找到的权重数量，因为每个聚类分配一个权重。作者报告的另一个有趣发现是OOD数据在批量归一化中的影响。即使OOD数据远离决策边界，如果进行批量归一化，性能也可能会下降。如果不进行批量归一化，远离决策边界的OOD数据可能不会显著损害性能。因此，找到的权重也用于对数据进行加权的小批量归一化。在对所提方法进行基准测试时，作者使用了CIFAR-10和FashionMNIST数据集，具有不同程度的OOD污染。OOD数据从排除在IOD数据集之外的一组类别中抽样。使用了WRN-28-2（WideResNet）骨干网。未对测试方法中相同数量分区的结果进行统计分析。平均准确率的提升显示出所提方法具有5%到20%不等的正向增益。
- en: Another approach for robust SSDL to distribution mismatch was proposed in [[93](#bib.bib93)],
    referred to by the authors as Augmented Distribution Alignment (ADA). Similar
    to MixMatch, ADA uses MixMup [[99](#bib.bib99)] for data augmentation. The method
    includes an unsupervised regularization term which measures the distribution divergence
    between the $S^{(u)}$ and $S^{(l)}$ datasets. This divergence is measured in the
    feature space. In order to diminish the empirical distribution mismatch of $S^{(u)}$
    and $S^{(l)}$, the distribution distance of both datasets is minimized to build
    a feature extractor aiming for a latent space where both feature densities are
    aligned. This is done through adversarial loss optimization. We can categorize
    this method as a feature space based method. As for the reported benchmarks, the
    authors did not test different degrees of OOD data contamination, and only compared
    their method to generic SSDL methods, not designed to handle distribution mismatch.
    No statistical significance tests were done to measure the confidence of their
    accuracy gains. In average, the proposed method seems to improve the error-rate
    from 0.5 to 2%, when compared to other SSDL generic methods. These results do
    not ensure a practical accuracy gain, as no statistical analysis was performed.
    From the baseline model with no distribution alignment, an accuracy gain of around
    5% was reported, again with no statistical meaning analysis performed. The authors
    in [[11](#bib.bib11)] also used the feature space to score unlabelled observations.
    The proposed method was tested in the specific application setting of COVID-19
    detection using chest X-ray images.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 针对分布不匹配的鲁棒性半监督深度学习（SSDL）方法，[[93](#bib.bib93)] 提出了另一种方法，作者称之为增强分布对齐（ADA）。类似于
    MixMatch，ADA 使用 MixMup [[99](#bib.bib99)] 进行数据增强。该方法包括一个无监督的正则化项，用于衡量 $S^{(u)}$
    和 $S^{(l)}$ 数据集之间的分布差异。这种差异在特征空间中进行衡量。为了减少 $S^{(u)}$ 和 $S^{(l)}$ 的经验分布不匹配，最小化两个数据集的分布距离，以构建一个特征提取器，旨在建立一个特征密度对齐的潜在空间。这是通过对抗损失优化来完成的。我们可以将这种方法归类为基于特征空间的方法。至于报告的基准测试，作者没有测试不同程度的
    OOD 数据污染，仅将其方法与通用 SSDL 方法进行比较，而这些方法并未设计用于处理分布不匹配。没有进行统计显著性测试来测量其准确性提升的信心。与其他通用
    SSDL 方法相比，该方法的平均错误率似乎从 0.5% 改善到 2%。这些结果并不能确保实际的准确性提升，因为没有进行统计分析。从没有分布对齐的基准模型中报告的准确性提升约为
    5%，但也没有进行统计意义分析。[[11](#bib.bib11)] 的作者也使用特征空间对未标记的观察进行评分。该方法在使用胸部 X 光图像进行 COVID-19
    检测的特定应用设置中进行了测试。
- en: Recently, in [[40](#bib.bib40)], an SSDL approach to distribution mismatch robustness
    was developed. The method consists of two training steps. The first or warm-up
    step performs a self-training phase, where a pretext task is optimized using the
    DNN backbone. This is implemented as the prediction of the degrees of rotation
    that each image, from a rotationally augmented dataset. This includes observations
    from both data samples $S^{(u)}$ and $S^{(l)}$, (along with the OOD observations).
    In the second step, the model is trained using a consistency based regularization
    approach for the unlabelled data. This consistency regularization also uses the
    rotation consistency loss. In this step, an OOD filtering method is implemented,
    referred by the authors as a cross-modal mechanism. This consists of the prediction
    of a pseudo-label, defined as the softmax of the DNN output. This pseudo-label,
    along its feature embedding, is fed to what the authors refer to as a matching
    head. Such matching head consists of a multi-perceptron model that is trained
    to estimate whether the pseudo-label is accurately matched to its embedding. The
    matching head model is trained with the labelled data, with different matching
    combinations of the labels and the observations. As for the testing benchmark,
    the authors used CIFAR-10, Animals-10 and CIFAR-ID-50 as IOD datasets. For OOD
    data sources, images of Gaussian and Uniform noise, along with the TIN and LSUN
    datasets were used. The average accuracy reported for all the tested methods correspond
    to the last 20 model copies yielded during training. Therefore, no different training
    partitions were tested, preventing an adequate statistical analysis of the results.
    The average results were not significantly better when compared to other generic
    SSDL methods such as FixMatch, with an accuracy gain of around 0.5% to 3%. No
    computational cost figures about the cost of training the additional matching
    head or warm-up training were provided. The authors claim that in their method,
    OOD data is re-used. However other methods like UASD also prevent totally discarding
    OOD data, as dynamic and soft observation-wise weights are calculated every epoch.
    Perhaps, from our point of view, a more appropriate description of the novelty
    of their method could be referred to as the complete usage of OOD data in a pre-training
    stage.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在[[40](#bib.bib40)]中，开发了一种针对分布不匹配鲁棒性的SSDL方法。该方法包括两个训练步骤。第一个或预热步骤执行自我训练阶段，其中通过DNN骨干网络优化一个预任务。这通过预测每个图像的旋转度来实现，该图像来自一个旋转增强的数据集。这包括来自数据样本$S^{(u)}$和$S^{(l)}$的观察结果（以及OOD观察结果）。在第二步中，模型使用基于一致性的正则化方法对未标记数据进行训练。这个一致性正则化方法也使用旋转一致性损失。在这一步中，实施了一种被作者称为跨模态机制的OOD过滤方法。这包括伪标签的预测，定义为DNN输出的softmax。这个伪标签及其特征嵌入被送入作者所称的匹配头。这个匹配头由一个多层感知器模型组成，该模型被训练来估计伪标签是否准确匹配到其嵌入。匹配头模型使用标记数据进行训练，配有不同的标签和观察组合。作为测试基准，作者使用了CIFAR-10、Animals-10和CIFAR-ID-50作为IOD数据集。对于OOD数据源，使用了高斯噪声和均匀噪声图像，以及TIN和LSUN数据集。所有测试方法的平均准确率对应于训练期间最后20个模型副本。因此，没有测试不同的训练分区，阻止了结果的适当统计分析。与其他通用SSDL方法（如FixMatch）相比，平均结果并没有显著提高，准确率提高约0.5%至3%。没有提供训练额外匹配头或预热训练的计算成本数据。作者声称他们的方法中，OOD数据被重新利用。然而，其他方法如UASD也防止完全丢弃OOD数据，因为每个epoch都会计算动态和软观察权重。从我们的角度来看，对其方法新颖性的更合适描述可能是指在预训练阶段完全使用OOD数据。
- en: Prior probability shift
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 先验概率偏移
- en: Data imbalance for supervised approaches has been tackled in the literature
    widely. Different approaches have been proposed, ranging from data transformations
    (over-sampling, data augmentation, etc.) to model architecture focused approaches
    (i. e., modification of thee loss function, etc.) [[2](#bib.bib2), [83](#bib.bib83),
    [60](#bib.bib60)]. Nevertheless, related to the problem of label imbalance or
    label balance mismatch between the labelled and unlabelled datasets, more scarce
    is found in the literature. This setting can be interpreted as a particularisation
    of the distribution mismatch problem described in [[63](#bib.bib63)]. A distribution
    mismatch between $S^{(l)}$ and $S^{(u)}$ might arise when the label or class membership
    distribution of the observations in both datasets meaningfully differ.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督方法中的数据不平衡问题在文献中已经得到了广泛探讨。提出了不同的方法，包括数据转换（过采样、数据增强等）到模型架构相关的方法（即，损失函数的修改等）[[2](#bib.bib2),
    [83](#bib.bib83), [60](#bib.bib60)]。然而，关于标注数据集和未标注数据集之间的标签不平衡或标签平衡不匹配的问题，在文献中却较为稀少。这种情况可以被解释为在[[63](#bib.bib63)]中描述的分布不匹配问题的一个特例。当两个数据集中的观察值的标签或类别分布存在实质性差异时，$S^{(l)}$和$S^{(u)}$之间可能会出现分布不匹配。
- en: In [[41](#bib.bib41)], an assessment of how distribution mismatch impacts a
    SSDL model is carried out. The cause of the distribution mismatch between the
    labelled and unlabelled datasets was the label imbalance difference between them.
    An accuracy decrease between 2% and 10% was measured when the SSDL faced such
    data setting. The authors proposed a simple method to recover such performance
    degradation. The method consists on assigning a specific weight for each unlabelled
    observation in the loss term. To choose the weight, the output unit of the model
    with highest score at the current epoch is used as a label prediction. In this
    work, the mean teacher model was tested as a SSDL approach [[86](#bib.bib86)].
    The authors yielded a superior performance of the SSDL model by using the proposed
    method. An extension to the work in [[41](#bib.bib41)] is found in [[16](#bib.bib16)],
    where in this case the more recent MixMatch SSDL method is modified to improve
    the robustness of the model to heavy imbalance conditions in the labelled dataset.
    The approach was extensively tested in the specific application of COVID-19 detection
    using chest X-ray images.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[41](#bib.bib41)]中，对分布不匹配如何影响SSDL模型进行了评估。标注数据集和未标注数据集之间的分布不匹配原因是它们之间的标签不平衡差异。当SSDL遇到这种数据设置时，测量到2%到10%的准确度下降。作者提出了一种简单的方法来恢复这种性能下降。该方法包括在损失项中为每个未标注观察分配特定的权重。选择权重时，使用当前纪元中得分最高的模型输出单元作为标签预测。在这项工作中，均值教师模型作为SSDL方法进行了测试[[86](#bib.bib86)]。作者通过使用提出的方法获得了SSDL模型的优越性能。在[[41](#bib.bib41)]中的工作的扩展可以在[[16](#bib.bib16)]中找到，其中在这种情况下，最近的MixMatch
    SSDL方法被修改以提高模型在标注数据集中的严重不平衡条件下的鲁棒性。这一方法在使用胸部X射线图像进行COVID-19检测的特定应用中进行了广泛测试。
- en: IV Open challenges
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 开放挑战
- en: Among the most important challenges faced by SSDL under practical usage situations
    is the distribution mismatch between the labelled and unlabelled datasets. However,
    according to our state of the art review, there is significant work pending, mostly
    related to the implementation of standard benchmarks for novel methods. The benchmarks
    found so far in the literature show a significant bias towards the unseen class
    distribution mismatch setting. No testing of other distribution mismatch causes
    such as covariate shift was found in the literature. Real world usage settings
    might include covariate shift and prior probability distribution shift, which
    violate the frequently used IID assumption. Therefore, we urge the community to
    focus on different distribution mismatch causes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际使用情况下，SSDL面临的最重要挑战之一是标注数据集和未标注数据集之间的分布不匹配。然而，根据我们的前沿综述，仍有大量工作待完成，主要涉及对新方法的标准基准的实施。迄今为止，文献中的基准显示出对未见类别分布不匹配设置的显著偏见。文献中未发现其他分布不匹配原因如协变量转移的测试。现实世界的使用设置可能包括协变量转移和先验概率分布转移，这违背了常用的IID假设。因此，我们敦促社区关注不同的分布不匹配原因。
- en: Studying and developing methods for dealing with distribution mismatch settings,
    shifts the focus upon data-oriented (i.e., data transformation, filtering and
    augmentation) methods instead of more popular model-oriented methods. Recently,
    the renowned researcher Andrew Ng, has drawn the attention towards data-oriented
    methods ¹¹1[https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=68b63b2174f5](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=68b63b2174f5).
    In his view, not enough effort has been carried out by the community in studying
    and developing data-oriented methods to face real-world usage settings.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 研究和开发应对分布不匹配设置的方法，将重点转向数据导向的方法（即数据转换、过滤和增强），而不是更流行的模型导向方法。最近，著名研究员**安德鲁·吴**引起了对数据导向方法的关注¹¹1[https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=68b63b2174f5](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=68b63b2174f5)。在他看来，社区在研究和开发应对实际应用设置的数据导向方法方面的努力还不够。
- en: We agree with Andrew Ng’s opinion, and add that besides establishing and testing
    a set of standard benchmarks where different distribution mismatch settings are
    tested, experimental reproducibility must be enforced. Recent technological advances
    not only allow to share the code and the datasets used, but also the testing environments
    through virtualization and container technology. Finally, we argue that the deep
    learning research community must be mindful of not only comparing average accuracies
    from the different state-of-the art methods. Statistical analysis tools must be
    used to test whether the performance difference between one method over another
    is reproducible and is statistically meaningful. Therefore we suggest that the
    results distribution is shared and not only the means and standard deviations
    of the results, in order to enable further statistical analysis.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同意安德鲁·吴的观点，并补充说，除了建立和测试不同分布不匹配设置的标准基准外，还必须强制实验的可重复性。最近的技术进步不仅允许共享代码和使用的数据集，还可以通过虚拟化和容器技术共享测试环境。最后，我们认为深度学习研究社区必须注意不仅仅比较不同最先进方法的平均准确度。必须使用统计分析工具来测试一种方法与另一种方法之间的性能差异是否可重复且具有统计意义。因此，我们建议共享结果的分布，而不仅仅是结果的均值和标准差，以便进行进一步的统计分析。
- en: References
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman,
    and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565,
    2016.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman,
    和 Dan Mané. 人工智能安全中的具体问题. arXiv 预印本 arXiv:1606.06565, 2016.'
- en: '[2] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
    imbalanced data. In Joint European Conference on Machine Learning and Knowledge
    Discovery in Databases, pages 770–785\. Springer, 2017.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Shin Ando 和 Chun Yuan Huang. 用于分类不平衡数据的深度过采样框架. 在联合欧洲机器学习与数据库知识发现会议中，页面
    770–785. Springer, 2017.'
- en: '[3] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles.
    In Advances in Neural Information Processing Systems, pages 3365–3373, 2014.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Philip Bachman, Ouais Alsharif, 和 Doina Precup. 使用伪集成学习. 在神经信息处理系统进展中，页面
    3365–3373, 2014.'
- en: '[4] Maria-Florina Balcan and Avrim Blum. 21 an augmented pac model for semi-supervised
    learning. 2006.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Maria-Florina Balcan 和 Avrim Blum. 21 增强型 PAC 模型用于半监督学习. 2006.'
- en: '[5] Indranil Balki, Afsaneh Amirabadi, Jacob Levman, Anne L Martel, Ziga Emersic,
    Blaz Meden, Angel Garcia-Pedrero, Saul C Ramirez, Dehan Kong, Alan R Moody, et al.
    Sample-size determination methodologies for machine learning in medical imaging
    research: A systematic review. Canadian Association of Radiologists Journal, 2019.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Indranil Balki, Afsaneh Amirabadi, Jacob Levman, Anne L Martel, Ziga Emersic,
    Blaz Meden, Angel Garcia-Pedrero, Saul C Ramirez, Dehan Kong, Alan R Moody, 等等.
    医学影像研究中的样本大小确定方法：系统评审. 加拿大放射学会期刊, 2019.'
- en: '[6] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy
    layer-wise training of deep networks. In Advances in neural information processing
    systems, pages 153–160, 2007.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Yoshua Bengio, Pascal Lamblin, Dan Popovici, 和 Hugo Larochelle. 贪婪逐层训练深度网络.
    在神经信息处理系统进展中，页面 153–160, 2007.'
- en: '[7] Ariana Bermudez, Saul Calderon-Ramirez, Trevor Thang, Pascal Tyrrell, Armaghan
    Moemeni, Shengxiang Yang, and Jordina Torrents-Barrena. Quality assessment of
    dental photostimulable phosphor plates with deep learning. Institute of Electrical
    and Electronics Engineers, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ariana Bermudez、Saul Calderon-Ramirez、Trevor Thang、Pascal Tyrrell、Armaghan
    Moemeni、Shengxiang Yang 和 Jordina Torrents-Barrena。利用深度学习评估牙科光刺激磷光板的质量。《电气与电子工程师协会》，2020年。'
- en: '[8] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn,
    Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution
    alignment and augmentation anchoring. arXiv preprint arXiv:1911.09785, 2019.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] David Berthelot、Nicholas Carlini、Ekin D Cubuk、Alex Kurakin、Kihyuk Sohn、Han
    Zhang 和 Colin Raffel。Remixmatch：具有分布对齐和数据增强锚定的半监督学习。arXiv预印本 arXiv:1911.09785，2019年。'
- en: '[9] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
    Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning.
    In Advances in Neural Information Processing Systems, pages 5050–5060, 2019.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] David Berthelot、Nicholas Carlini、Ian Goodfellow、Nicolas Papernot、Avital
    Oliver 和 Colin A Raffel。Mixmatch：一种整体性的半监督学习方法。发表于《神经信息处理系统进展》，页码5050–5060，2019年。'
- en: '[10] S Calderon, F Fallas, M Zumbado, PN Tyrrell, H Stark, Ziga Emersic, Blaz
    Meden, and M Solis. Assessing the impact of the deceived non local means filter
    as a preprocessing stage in a convolutional neural network based approach for
    age estimation using digital hand x-ray images. In 2018 25th IEEE International
    Conference on Image Processing (ICIP), pages 1752–1756\. IEEE, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S Calderon、F Fallas、M Zumbado、PN Tyrrell、H Stark、Ziga Emersic、Blaz Meden
    和 M Solis。评估在基于卷积神经网络的年龄估计中，欺骗性非局部均值滤波器作为预处理阶段的影响。发表于2018年第25届IEEE国际图像处理会议（ICIP），页码1752–1756。IEEE，2018年。'
- en: '[11] Saul Calderon-Ramirez, Raghvendra Giri, Shengxiang Yang, Armaghan Moemeni,
    Mario Umana, David Elizondo, Jordina Torrents-Barrena, and Miguel A Molina-Cabello.
    Dealing with scarce labelled data: Semi-supervised deep learning with mix match
    for covid-19 detection using chest x-ray images. In 2020 25th International Conference
    on Pattern Recognition (ICPR), pages 5294–5301\. IEEE, Jan. 2021.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Saul Calderon-Ramirez、Raghvendra Giri、Shengxiang Yang、Armaghan Moemeni、Mario
    Umana、David Elizondo、Jordina Torrents-Barrena 和 Miguel A Molina-Cabello。应对稀缺标记数据：利用Mixmatch进行半监督深度学习以检测COVID-19，使用胸部X光图像。发表于2020年第25届国际模式识别会议（ICPR），页码5294–5301。IEEE，2021年1月。'
- en: '[12] Saul Calderon-Ramirez and Erick Mata-Montero. A first glance into reversing
    senescence on herbarium sample images through conditional generative adversarial
    networks. In High Performance Computing: 6th Latin American Conference, CARLA
    2019, Turrialba, Costa Rica, September 25–27, 2019, Revised Selected Papers, volume
    1087, page 438\. Springer Nature, 2020.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Saul Calderon-Ramirez 和 Erick Mata-Montero。通过条件生成对抗网络对标本样本图像进行衰老逆转的初步研究。发表于《高性能计算：第六届拉丁美洲会议，CARLA
    2019》，图尔里亚巴，哥斯达黎加，2019年9月25–27日，修订版论文，卷1087，页438。Springer Nature，2020年。'
- en: '[13] Saul Calderon-Ramirez, Diego Murillo-Hernandez, Kevin Rojas-Salazar, Luis-Alexander
    Calvo-Valverde, Shengxiang Yang, Armaghan Moemeni, David Elizondo, Ezequiel Lopez-Rubio,
    and Miguel Molina-Cabello. Improving uncertainty estimations for mammogram classification
    using semi-supervised learning. In Institute of Electrical and Electronics Engineers,
    2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Saul Calderon-Ramirez、Diego Murillo-Hernandez、Kevin Rojas-Salazar、Luis-Alexander
    Calvo-Valverde、Shengxiang Yang、Armaghan Moemeni、David Elizondo、Ezequiel Lopez-Rubio
    和 Miguel Molina-Cabello。利用半监督学习改进乳腺X光图像分类的不确定性估计。发表于《电气与电子工程师协会》，2021年。'
- en: '[14] Saul Calderon-Ramirez, Diego Murillo-Hernandez, Kevin Rojas-Salazar, David
    Elizondo, Shengxiang Yang, and Miguel Molina-Cabello. A real use case of semi-supervised
    learning for mammogram classification in a local clinic of costa rica. arXiv preprint
    arXiv:2107.11696, 2021.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Saul Calderon-Ramirez、Diego Murillo-Hernandez、Kevin Rojas-Salazar、David
    Elizondo、Shengxiang Yang 和 Miguel Molina-Cabello。在哥斯达黎加一家本地诊所中进行乳腺X光图像分类的半监督学习实际案例。arXiv预印本
    arXiv:2107.11696，2021年。'
- en: '[15] Saul Calderon-Ramirez, Luis Oala, Jordina Torrents-Barrena, Shengxiang
    Yang, Armaghan Moemeni, Wojciech Samek, and Miguel A Molina-Cabello. Mixmood:
    A systematic approach to class distribution mismatch in semi-supervised learning
    using deep dataset dissimilarity measures. arXiv preprint arXiv:2006.07767, 2020.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Saul Calderon-Ramirez、Luis Oala、Jordina Torrents-Barrena、Shengxiang Yang、Armaghan
    Moemeni、Wojciech Samek 和 Miguel A Molina-Cabello。Mixmood：一种系统化的方法来解决半监督学习中的类别分布不匹配问题，使用深度数据集不相似度度量。arXiv预印本
    arXiv:2006.07767，2020年。'
- en: '[16] Saul Calderon-Ramirez, Shengxiang Yang, Armaghan Moemeni, David Elizondo,
    Simon Colreavy-Donnelly, Luis Fernando Chavarría-Estrada, and Miguel A Molina-Cabello.
    Correcting data imbalance for semi-supervised covid-19 detection using x-ray chest
    images. Applied Soft Computing, 111:107692, 2021.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Saul Calderon-Ramirez, Shengxiang Yang, Armaghan Moemeni, David Elizondo,
    Simon Colreavy-Donnelly, Luis Fernando Chavarría-Estrada 和 Miguel A Molina-Cabello.
    使用x射线胸部图像纠正半监督COVID-19检测中的数据不平衡. 应用软计算, 111:107692, 2021.'
- en: '[17] Iván Calvo, Saul Calderon-Ramirez, Jordina Torrents-Barrena, Erick Muñoz,
    and Domenec Puig. Assessing the impact of a preprocessing stage on deep learning
    architectures for breast tumor multi-class classification with histopathological
    images. In Latin American High Performance Computing Conference, pages 262–275\.
    Springer, 2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Iván Calvo, Saul Calderon-Ramirez, Jordina Torrents-Barrena, Erick Muñoz
    和 Domenec Puig. 评估预处理阶段对乳腺肿瘤多类别分类深度学习架构的影响，使用组织病理图像. 拉丁美洲高性能计算会议论文集，页码 262–275.
    Springer, 2019.'
- en: '[18] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised
    pre-training of image features on non-curated data. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pages 2959–2968, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Mathilde Caron, Piotr Bojanowski, Julien Mairal 和 Armand Joulin. 在非精细数据上进行图像特征的无监督预训练.
    见于IEEE/CVF国际计算机视觉会议论文集，页码 2959–2968, 2019.'
- en: '[19] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Semi-supervised deep learning
    with memory. In Proceedings of the European Conference on Computer Vision (ECCV),
    pages 268–283, 2018.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Yanbei Chen, Xiatian Zhu 和 Shaogang Gong. 具有记忆的半监督深度学习. 见于欧洲计算机视觉会议（ECCV）论文集，页码
    268–283, 2018.'
- en: '[20] Yanbei Chen, Xiatian Zhu, Wei Li, and Shaogang Gong. Semi-supervised learning
    under class distribution mismatch. In AAI, pages 3569–3576, 2020.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Yanbei Chen, Xiatian Zhu, Wei Li 和 Shaogang Gong. 类别分布不匹配下的半监督学习. 见于AAI，页码
    3569–3576, 2020.'
- en: '[21] Veronika Cheplygina, Marleen de Bruijne, and Josien PW Pluim. Not-so-supervised:
    a survey of semi-supervised, multi-instance, and transfer learning in medical
    image analysis. Medical image analysis, 54:280–296, 2019.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Veronika Cheplygina, Marleen de Bruijne 和 Josien PW Pluim. 不完全监督: 医学图像分析中半监督、多实例和迁移学习的调查.
    医学图像分析, 54:280–296, 2019.'
- en: '[22] LI Chongxuan, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial
    nets. In Advances in neural information processing systems, pages 4088–4098, 2017.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] LI Chongxuan, Taufik Xu, Jun Zhu 和 Bo Zhang. 三重生成对抗网络. 见于神经信息处理系统进展，页码
    4088–4098, 2017.'
- en: '[23] Safa Cicek, Alhussein Fawzi, and Stefano Soatto. Saas: Speed as a supervisor
    for semi-supervised learning. In Proceedings of the European Conference on Computer
    Vision (ECCV), pages 149–163, 2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Safa Cicek, Alhussein Fawzi 和 Stefano Soatto. Saas: 作为半监督学习监督者的速度. 见于欧洲计算机视觉会议（ECCV）论文集，页码
    149–163, 2018.'
- en: '[24] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V
    Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501,
    2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan 和 Quoc V Le.
    Autoaugment: 从数据中学习增强策略. arXiv 预印本 arXiv:1805.09501, 2018.'
- en: '[25] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment:
    Practical automated data augmentation with a reduced search space. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
    pages 702–703, 2020.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Ekin D Cubuk, Barret Zoph, Jonathon Shlens 和 Quoc V Le. Randaugment: 实用的自动数据增强，减少了搜索空间.
    见于IEEE/CVF计算机视觉与模式识别会议研讨会论文集，页码 702–703, 2020.'
- en: '[26] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov.
    Good semi-supervised learning that requires a bad gan. In Advances in neural information
    processing systems, pages 6510–6520, 2017.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen 和 Ruslan R Salakhutdinov.
    需要一个糟糕的gan的良好半监督学习. 见于神经信息处理系统进展，页码 6510–6520, 2017.'
- en: '[27] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
    Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on
    computer vision and pattern recognition, pages 248–255\. Ieee, 2009.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li 和 Li Fei-Fei. Imagenet:
    大规模层次图像数据库. 见于2009 IEEE计算机视觉与模式识别会议，页码 248–255. IEEE, 2009.'
- en: '[28] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation
    learning by context prediction. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1422–1430, 2015.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Carl Doersch, Abhinav Gupta 和 Alexei A Efros. 通过上下文预测进行无监督视觉表示学习. 见于IEEE国际计算机视觉会议论文集，页码
    1422–1430, 2015.'
- en: '[29] WeiWang Dong-DongChen and Zhi-HuaZhou WeiGao. Tri-net for semi-supervised
    deep learning. IJCAI, 2018.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] WeiWang Dong-DongChen 和 Zhi-HuaZhou WeiGao. Tri-net用于半监督深度学习. IJCAI, 2018.'
- en: '[30] Dominic B Dwyer, Peter Falkai, and Nikolaos Koutsouleris. Machine learning
    approaches for clinical psychology and psychiatry. Annual review of clinical psychology,
    14:91–118, 2018.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Dominic B Dwyer、Peter Falkai 和 Nikolaos Koutsouleris。用于临床心理学和精神病学的机器学习方法。《临床心理学年鉴》，14:91–118，2018年。'
- en: '[31] Benoît Frénay and Michel Verleysen. Classification in the presence of
    label noise: a survey. IEEE transactions on neural networks and learning systems,
    25(5):845–869, 2013.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Benoît Frénay 和 Michel Verleysen。在标签噪声存在下的分类：综述。《IEEE神经网络与学习系统汇刊》，25(5):845–869，2013年。'
- en: '[32] Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit
    Greenspan. Synthetic data augmentation using gan for improved liver lesion classification.
    In 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018), pages
    289–293\. IEEE, 2018.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Maayan Frid-Adar、Eyal Klang、Michal Amitai、Jacob Goldberger 和 Hayit Greenspan。使用GAN进行合成数据增强以提高肝病变分类。载于2018年IEEE第15届生物医学成像国际研讨会（ISBI
    2018），第289–293页。IEEE，2018年。'
- en: '[33] Angel Garcia-Pedrero, Ana I García-Cervigón, José M Olano, Miguel García-Hidalgo,
    Mario Lillo-Saavedra, Consuelo Gonzalo-Martín, Cristina Caetano, and Saúl Calderón-Ramírez.
    Convolutional neural networks for segmenting xylem vessels in stained cross-sectional
    images. Neural Computing and Applications, pages 1–13, 2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Angel Garcia-Pedrero、Ana I García-Cervigón、José M Olano、Miguel García-Hidalgo、Mario
    Lillo-Saavedra、Consuelo Gonzalo-Martín、Cristina Caetano 和 Saúl Calderón-Ramírez。用于分割染色横截面图像中木质部导管的卷积神经网络。《神经计算与应用》，第1–13页，2019年。'
- en: '[34] Andrew B Goldberg, Xiaojin Zhu, and Stephen Wright. Dissimilarity in graph-based
    semi-supervised classification. In Artificial Intelligence and Statistics, pages
    155–162, 2007.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Andrew B Goldberg、Xiaojin Zhu 和 Stephen Wright。图基半监督分类中的差异。载于《人工智能与统计》，第155–162页，2007年。'
- en: '[35] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT
    press, 2016.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ian Goodfellow、Yoshua Bengio 和 Aaron Courville。深度学习。MIT出版社，2016年。'
- en: '[36] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Advances in neural information processing systems, pages 2672–2680, 2014.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Ian Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David Warde-Farley、Sherjil
    Ozair、Aaron Courville 和 Yoshua Bengio。生成对抗网络。载于《神经信息处理系统进展》，第2672–2680页，2014年。'
- en: '[37] Ryuhei Hamaguchi, Ken Sakurada, and Ryosuke Nakamura. Rare event detection
    using disentangled representation learning. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 9327–9335, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Ryuhei Hamaguchi、Ken Sakurada 和 Ryosuke Nakamura。使用解耦表示学习进行稀有事件检测。载于IEEE计算机视觉与模式识别会议论文集，第9327–9335页，2019年。'
- en: '[38] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified
    and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136,
    2016.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Dan Hendrycks 和 Kevin Gimpel。在神经网络中检测误分类和分布外样本的基线。arXiv预印本arXiv:1610.02136，2016年。'
- en: '[39] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified
    and out-of-distribution examples in neural networks. CoRR, abs/1610.02136, 2016.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Dan Hendrycks 和 Kevin Gimpel。在神经网络中检测误分类和分布外样本的基线。CoRR，abs/1610.02136，2016年。'
- en: '[40] Junkai Huang, Chaowei Fang, Weikai Chen, Zhenhua Chai, Xiaolin Wei, Pengxu
    Wei, Liang Lin, and Guanbin Li. Trash to treasure: Harvesting ood data with cross-modal
    matching for open-set semi-supervised learning. arXiv preprint arXiv:2108.05617,
    2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Junkai Huang、Chaowei Fang、Weikai Chen、Zhenhua Chai、Xiaolin Wei、Pengxu
    Wei、Liang Lin 和 Guanbin Li。废物变宝物：通过跨模态匹配进行开放集半监督学习的数据收获。arXiv预印本arXiv:2108.05617，2021年。'
- en: '[41] Minsung Hyun, Jisoo Jeong, and Nojun Kwak. Class-imbalanced semi-supervised
    learning. arXiv preprint arXiv:2002.06815, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Minsung Hyun、Jisoo Jeong 和 Nojun Kwak。类别不平衡的半监督学习。arXiv预印本arXiv:2002.06815，2020年。'
- en: '[42] Vladimir I Iglovikov, Alexander Rakhlin, Alexandr A Kalinin, and Alexey A
    Shvets. Paediatric bone age assessment using deep convolutional neural networks.
    In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical
    Decision Support, pages 300–308\. Springer, 2018.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Vladimir I Iglovikov、Alexander Rakhlin、Alexandr A Kalinin 和 Alexey A Shvets。使用深度卷积神经网络进行儿科骨龄评估。载于《医学图像分析中的深度学习与临床决策支持的多模态学习》，第300–308页。Springer，2018年。'
- en: '[43] Baihong Jin, Yingshui Tan, Yuxin Chen, and Alberto Sangiovanni-Vincentelli.
    Augmenting monte carlo dropout classification models with unsupervised learning
    tasks for detecting and diagnosing out-of-distribution faults. arXiv preprint
    arXiv:1909.04202, 2019.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Baihong Jin、Yingshui Tan、Yuxin Chen 和 Alberto Sangiovanni-Vincentelli。通过无监督学习任务增强蒙特卡罗Dropout分类模型，以检测和诊断分布外故障。arXiv预印本arXiv:1909.04202，2019年。'
- en: '[44] Longlong Jing and Yingli Tian. Self-supervised visual feature learning
    with deep neural networks: A survey. IEEE transactions on pattern analysis and
    machine intelligence, 2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Longlong Jing 和 Yingli Tian. 基于深度神经网络的自监督视觉特征学习：综述。IEEE Transactions on
    Pattern Analysis and Machine Intelligence, 2020。'
- en: '[45] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
    Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
    Rachel Cummings, et al. Advances and open problems in federated learning. arXiv
    preprint arXiv:1912.04977, 2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
    Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
    Rachel Cummings 等。联邦学习的进展与开放问题。arXiv预印本 arXiv:1912.04977, 2019。'
- en: '[46] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian
    deep learning for computer vision? In Proceedings of the 31st International Conference
    on Neural Information Processing Systems, pages 5580–5590, 2017.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Alex Kendall 和 Yarin Gal. 在计算机视觉的贝叶斯深度学习中我们需要什么不确定性？发表于第31届国际神经信息处理系统会议论文集，页面5580–5590,
    2017。'
- en: '[47] Kyeongbo Kong, Junggi Lee, Youngchul Kwak, Minsung Kang, Seong Gyun Kim,
    and Woo-Jin Song. Recycling: Semi-supervised learning with noisy labels in deep
    neural networks. IEEE Access, 7:66998–67005, 2019.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Kyeongbo Kong, Junggi Lee, Youngchul Kwak, Minsung Kang, Seong Gyun Kim
    和 Woo-Jin Song. 回收：在深度神经网络中处理噪声标签的半监督学习。IEEE Access, 7:66998–67005, 2019。'
- en: '[48] Joseph B Kruskal. Multidimensional scaling by optimizing goodness of fit
    to a nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Joseph B Kruskal. 通过优化与非度量假设的拟合优度进行多维尺度分析。Psychometrika, 29(1):1–27, 1964。'
- en: '[49] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning.
    arXiv preprint arXiv:1610.02242, 2016.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Samuli Laine 和 Timo Aila. 半监督学习的时间集成。arXiv预印本 arXiv:1610.02242, 2016。'
- en: '[50] Hye-Woo Lee, Noo-ri Kim, and Jee-Hyong Lee. Deep neural network self-training
    based on unsupervised learning and dropout. International Journal of Fuzzy Logic
    and Intelligent Systems, 17(1):1–9, 2017.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Hye-Woo Lee, Noo-ri Kim 和 Jee-Hyong Lee. 基于无监督学习和Dropout的深度神经网络自训练。International
    Journal of Fuzzy Logic and Intelligent Systems, 17(1):1–9, 2017。'
- en: '[51] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework
    for detecting out-of-distribution samples and adversarial attacks. In Advances
    in Neural Information Processing Systems, pages 7167–7177, 2018.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Kimin Lee, Kibok Lee, Honglak Lee 和 Jinwoo Shin. 检测分布外样本和对抗攻击的简单统一框架。发表于神经信息处理系统进展，页面7167–7177,
    2018。'
- en: '[52] Wenyuan Li, Zichen Wang, Jiayun Li, Jennifer Polson, William Speier, and
    Corey Arnold. Semi-supervised learning based on generative adversarial network:
    a comparison between good gan and bad gan approach. arXiv preprint arXiv:1905.06484,
    2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Wenyuan Li, Zichen Wang, Jiayun Li, Jennifer Polson, William Speier 和
    Corey Arnold. 基于生成对抗网络的半监督学习：Good GAN与Bad GAN方法的比较。arXiv预印本 arXiv:1905.06484,
    2019。'
- en: '[53] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability
    of out-of-distribution image detection in neural networks. In 6th International
    Conference on Learning Representations, ICLR 2018, 2018.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Shiyu Liang, Yixuan Li 和 Rayadurgam Srikant. 增强神经网络中分布外图像检测的可靠性。发表于第6届国际学习表征会议，ICLR
    2018, 2018。'
- en: '[54] Antonio Loquercio, Mattia Segu, and Davide Scaramuzza. A general framework
    for uncertainty estimation in deep learning. IEEE Robotics and Automation Letters,
    5(2):3153–3160, 2020.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Antonio Loquercio, Mattia Segu 和 Davide Scaramuzza. 深度学习中的不确定性估计通用框架。IEEE
    Robotics and Automation Letters, 5(2):3153–3160, 2020。'
- en: '[55] Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors
    on teacher graphs for semi-supervised learning. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 8896–8905, 2018.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren 和 Bo Zhang. 在教师图上的平滑邻居用于半监督学习。发表于IEEE计算机视觉与模式识别会议论文集，页面8896–8905,
    2018。'
- en: '[56] Mingqi Lv, Ling Chen, Tieming Chen, and Gencai Chen. Bi-view semi-supervised
    learning based semantic human activity recognition using accelerometers. IEEE
    Transactions on Mobile Computing, 17(9):1991–2001, 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Mingqi Lv, Ling Chen, Tieming Chen 和 Gencai Chen. 基于双视图的半监督学习用于加速度计的语义人体活动识别。IEEE
    Transactions on Mobile Computing, 17(9):1991–2001, 2018。'
- en: '[57] Markos Markou and Sameer Singh. Novelty detection: a review—part 1: statistical
    approaches. Signal processing, 83(12):2481–2497, 2003.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Markos Markou 和 Sameer Singh. 新奇检测：综述—第1部分：统计方法。Signal Processing, 83(12):2481–2497,
    2003。'
- en: '[58] Mauro Mendez, Saul Calderon-Ramirez, and Pascal Tyrrell. Using cluster
    analysis to assess the impact of dataset heterogeneity on deep convolutional network
    accuracy: A first glance. In Latin America High Performance Computing Conference
    (CARLA) 2019.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Mauro Mendez、Saul Calderon-Ramirez 和 Pascal Tyrrell。使用聚类分析评估数据集异质性对深度卷积网络准确性的影响：初步观察。发表于拉丁美洲高性能计算会议（CARLA）2019年。'
- en: '[59] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual
    adversarial training: a regularization method for supervised and semi-supervised
    learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993,
    2018.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Takeru Miyato、Shin-ichi Maeda、Masanori Koyama 和 Shin Ishii。虚拟对抗训练：一种用于监督和半监督学习的正则化方法。IEEE模式分析与机器智能学报，41(8)：1979–1993，2018年。'
- en: '[60] Sankha Subhra Mullick, Shounak Datta, and Swagatam Das. Generative adversarial
    minority oversampling. In Proceedings of the IEEE International Conference on
    Computer Vision, pages 1695–1704, 2019.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Sankha Subhra Mullick、Shounak Datta 和 Swagatam Das。生成对抗性少数类过采样。发表于IEEE国际计算机视觉会议论文集，第1695–1704页，2019年。'
- en: '[61] Varun Nair, Javier Fuentes Alonso, and Tony Beltramelli. Realmix: Towards
    realistic semi-supervised deep learning algorithms. arXiv preprint arXiv:1912.08766,
    2019.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Varun Nair、Javier Fuentes Alonso 和 Tony Beltramelli。RealMix：面向现实的半监督深度学习算法。arXiv
    预印本 arXiv:1912.08766，2019年。'
- en: '[62] Luis Oala, Jana Fehr, Luca Gilli, Pradeep Balachandran, Alixandro Werneck
    Leite, Saul Calderon-Ramirez, Danny Xie Li, Gabriel Nobis, Erick Alejandro Muñoz
    Alvarado, Giovanna Jaramillo-Gutierrez, et al. Ml4h auditing: From paper to practice.
    In Machine Learning for Health, pages 280–317\. PMLR, 2020.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Luis Oala、Jana Fehr、Luca Gilli、Pradeep Balachandran、Alixandro Werneck
    Leite、Saul Calderon-Ramirez、Danny Xie Li、Gabriel Nobis、Erick Alejandro Muñoz Alvarado、Giovanna
    Jaramillo-Gutierrez 等。Ml4h 审计：从理论到实践。发表于《健康机器学习》，第280–317页。PMLR，2020年。'
- en: '[63] Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian
    Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms.
    In Advances in Neural Information Processing Systems, pages 3235–3246, 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Avital Oliver、Augustus Odena、Colin A Raffel、Ekin Dogus Cubuk 和 Ian Goodfellow。深度半监督学习算法的现实评估。发表于神经信息处理系统进展论文集，第3235–3246页，2018年。'
- en: '[64] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
    Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic
    differentiation in pytorch. 2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Adam Paszke、Sam Gross、Soumith Chintala、Gregory Chanan、Edward Yang、Zachary
    DeVito、Zeming Lin、Alban Desmaison、Luca Antiga 和 Adam Lerer。PyTorch中的自动微分。2017年。'
- en: '[65] Pramuditha Perera and Vishal M Patel. Deep transfer learning for multiple
    class novelty detection. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 11544–11552, 2019.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Pramuditha Perera 和 Vishal M Patel。用于多类别新颖性检测的深度迁移学习。发表于IEEE计算机视觉与模式识别会议论文集，第11544–11552页，2019年。'
- en: '[66] Nitin Namdeo Pise and Parag Kulkarni. A survey of semi-supervised learning
    methods. In 2008 International Conference on Computational Intelligence and Security,
    volume 2, pages 30–34\. IEEE, 2008.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Nitin Namdeo Pise 和 Parag Kulkarni. 半监督学习方法的调查。发表于2008年计算智能与安全国际会议，第二卷，第30–34页。IEEE，2008年。'
- en: '[67] V Jothi Prakash and Dr LM Nithya. A survey on semi-supervised learning
    techniques. arXiv preprint arXiv:1402.4645, 2014.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] V Jothi Prakash 和 Dr LM Nithya。半监督学习技术的调查。arXiv 预印本 arXiv:1402.4645，2014年。'
- en: '[68] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan Yuille. Deep
    co-training for semi-supervised image recognition. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 135–152, 2018.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Siyuan Qiao、Wei Shen、Zhishuai Zhang、Bo Wang 和 Alan Yuille。深度协同训练用于半监督图像识别。发表于欧洲计算机视觉会议（ECCV）论文集，第135–152页，2018年。'
- en: '[69] Saúl Calderón Ramírez, Raghvendra Giri, Shengxiang Yang, Armaghan Moemeni,
    Mario Umaña, David A Elizondo, Jordina Torrents-Barrena, and Miguel A Molina-Cabello.
    Dealing with scarce labelled data: Semi-supervised deep learning with mix match
    for covid-19 detection using chest x-ray images. In ICPR, pages 5294–5301, 2020.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Saúl Calderón Ramírez、Raghvendra Giri、Shengxiang Yang、Armaghan Moemeni、Mario
    Umaña、David A Elizondo、Jordina Torrents-Barrena 和 Miguel A Molina-Cabello。处理稀缺标记数据：使用MixMatch进行半监督深度学习以检测COVID-19并利用胸部X光图像。发表于ICPR，第5294–5301页，2020年。'
- en: '[70] Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier
    Muller. The manifold tangent classifier. In Advances in neural information processing
    systems, pages 2294–2302, 2011.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Salah Rifai、Yann N Dauphin、Pascal Vincent、Yoshua Bengio 和 Xavier Muller。流形切线分类器。发表于神经信息处理系统进展论文集，第2294–2302页，2011年。'
- en: '[71] Peter J. Rousseeuw. Least median of squares regression. Journal of the
    American Statistical Association, 79(388):871–880, 1984.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Peter J. Rousseeuw. 最小中位数平方回归。美国统计协会期刊，79(388):871–880，1984。'
- en: '[72] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with
    stochastic transformations and perturbations for deep semi-supervised learning.
    In Advances in Neural Information Processing Systems, pages 1163–1171, 2016.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Mehdi Sajjadi, Mehran Javanmardi, 和 Tolga Tasdizen. 使用随机变换和扰动的正则化进行深度半监督学习。在神经信息处理系统进展中，页1163–1171，2016。'
- en: '[73] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    and Xi Chen. Improved techniques for training gans. In Advances in neural information
    processing systems, pages 2234–2242, 2016.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    和 Xi Chen. 改进的生成对抗网络训练技术。在神经信息处理系统进展中，页2234–2242，2016。'
- en: '[74] Lars Schmarje, Monty Santarossa, Simon-Martin Schröder, and Reinhard Koch.
    A survey on semi-, self-and unsupervised learning for image classification. IEEE
    Access, 2021.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Lars Schmarje, Monty Santarossa, Simon-Martin Schröder, 和 Reinhard Koch.
    关于图像分类的半监督、自监督和无监督学习的综述。IEEE Access，2021。'
- en: '[75] Andreas Sedlmeier, Thomas Gabor, Thomy Phan, and Lenz Belzner. Uncertainty-based
    out-of-distribution detection in deep reinforcement learning. Digitale Welt, 4(1):74–78,
    2020.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Andreas Sedlmeier, Thomas Gabor, Thomy Phan, 和 Lenz Belzner. 基于不确定性的深度强化学习中的分布外检测。数字世界，4(1):74–78，2020。'
- en: '[76] Behzad M Shahshahani and David A Landgrebe. The effect of unlabeled samples
    in reducing the small sample size problem and mitigating the hughes phenomenon.
    IEEE Transactions on Geoscience and remote sensing, 32(5):1087–1095, 1994.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Behzad M Shahshahani 和 David A Landgrebe. 无标签样本在减少小样本问题和缓解Hughes现象中的作用。IEEE地球科学与遥感学报，32(5):1087–1095，1994。'
- en: '[77] Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaXiaoyu Tao, and Nanning
    Zheng. Transductive semi-supervised deep learning using min-max features. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 299–315, 2018.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaXiaoyu Tao, 和 Nanning Zheng.
    使用最小-最大特征的传导性半监督深度学习。在欧洲计算机视觉会议（ECCV）论文集中，页299–315，2018。'
- en: '[78] Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G Schwarz,
    Matthew L Senjem, Jeffrey L Gunter, Katherine P Andriole, and Mark Michalski.
    Medical image synthesis for data augmentation and anonymization using generative
    adversarial networks. In International Workshop on Simulation and Synthesis in
    Medical Imaging, pages 1–11\. Springer, 2018.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G Schwarz,
    Matthew L Senjem, Jeffrey L Gunter, Katherine P Andriole, 和 Mark Michalski. 使用生成对抗网络进行医学图像合成以实现数据增强和匿名化。在医学影像模拟与合成国际研讨会论文集中，页1–11。Springer，2018。'
- en: '[79] Karanjit Singh and Shuchita Upadhyaya. Outlier detection: applications
    and techniques. International Journal of Computer Science Issues (IJCSI), 9(1):307,
    2012.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Karanjit Singh 和 Shuchita Upadhyaya. 离群点检测：应用与技术。计算机科学问题国际期刊（IJCSI），9(1):307，2012。'
- en: '[80] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang,
    Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch:
    Simplifying semi-supervised learning with consistency and confidence. Advances
    in Neural Information Processing Systems, 33, 2020.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang,
    Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, 和 Chun-Liang Li. Fixmatch: 利用一致性和置信度简化半监督学习。神经信息处理系统进展，33，2020。'
- en: '[81] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with
    categorical generative adversarial networks. arXiv preprint arXiv:1511.06390,
    2015.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Jost Tobias Springenberg. 使用分类生成对抗网络的无监督和半监督学习。arXiv预印本 arXiv:1511.06390，2015。'
- en: '[82] Wenqing Sun, Tzu-Liang Bill Tseng, Jianying Zhang, and Wei Qian. Enhancing
    deep convolutional neural network scheme for breast cancer diagnosis with unlabeled
    data. Computerized Medical Imaging and Graphics, 57:4–9, 2017.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Wenqing Sun, Tzu-Liang Bill Tseng, Jianying Zhang, 和 Wei Qian. 利用无标签数据增强深度卷积神经网络方案用于乳腺癌诊断。计算机化医学影像与图形，57:4–9，2017。'
- en: '[83] Aboozar Taherkhani, Georgina Cosma, and TM McGinnity. Adaboost-cnn: an
    adaptive boosting algorithm for convolutional neural networks to classify multi-class
    imbalanced datasets using transfer learning. Neurocomputing, 2020.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Aboozar Taherkhani, Georgina Cosma, 和 TM McGinnity. Adaboost-cnn: 一种自适应提升算法，用于通过迁移学习分类多类别不平衡数据集的卷积神经网络。神经计算，2020。'
- en: '[84] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang
    Liu. A survey on deep transfer learning. In International Conference on Artificial
    Neural Networks, pages 270–279\. Springer, 2018.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang 和 Chunfang
    Liu。深度迁移学习综述。载于《国际人工神经网络会议》，第270–279页。Springer，2018年。'
- en: '[85] Jeremy Tan, Anselm Au, Qingjie Meng, and Bernhard Kainz. Semi-supervised
    learning of fetal anatomy from ultrasound. In Domain Adaptation and Representation
    Transfer and Medical Image Learning with Less Labels and Imperfect Data, pages
    157–164. Springer, 2019.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Jeremy Tan, Anselm Au, Qingjie Meng 和 Bernhard Kainz。利用超声波进行胎儿解剖的半监督学习。载于《领域适应与表示迁移以及医学图像学习：标签较少和不完美数据》，第157–164页。Springer，2019年。'
- en: '[86] Antti Tarvainen and Harri Valpola. Mean teachers are better role models:
    Weight-averaged consistency targets improve semi-supervised deep learning results.
    In Advances in neural information processing systems, pages 1195–1204, 2017.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Antti Tarvainen 和 Harri Valpola。均值教师是更好的榜样：权重平均一致性目标改善半监督深度学习结果。载于《神经信息处理系统进展》，第1195–1204页，2017年。'
- en: '[87] David M. J. Tax and Robert P. W. Duin. Support vector data description.
    Mach. Learn., 54(1):45–66, 2004.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] David M. J. Tax 和 Robert P. W. Duin。支持向量数据描述。《机器学习》，54(1):45–66，2004年。'
- en: '[88] Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research
    on machine learning applications and trends: algorithms, methods, and techniques,
    pages 242–264\. IGI Global, 2010.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Lisa Torrey 和 Jude Shavlik。迁移学习。载于《机器学习应用与趋势手册：算法、方法与技术》，第242–264页。IGI
    Global，2010年。'
- en: '[89] Phi Vu Tran. Semi-supervised learning with self-supervised networks. arXiv
    preprint arXiv:1906.10343, 2019.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Phi Vu Tran。使用自监督网络的半监督学习。arXiv预印本 arXiv:1906.10343，2019年。'
- en: '[90] Leslie G Valiant. A theory of the learnable. In Proceedings of the sixteenth
    annual ACM symposium on Theory of computing, pages 436–445\. ACM, 1984.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Leslie G Valiant。可学习性的理论。载于《第十六届ACM计算理论年会论文集》，第436–445页。ACM，1984年。'
- en: '[91] Joost van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Simple
    and scalable epistemic uncertainty estimation using a single deep deterministic
    neural network. arXiv e-prints, Jun. 2020.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Joost van Amersfoort, Lewis Smith, Yee Whye Teh 和 Yarin Gal。使用单一深度确定性神经网络进行简单且可扩展的认识不确定性估计。arXiv
    e-prints，2020年6月。'
- en: '[92] Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning.
    Machine Learning, 109(2):373–440, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Jesper E Van Engelen 和 Holger H Hoos。半监督学习综述。《机器学习》，109(2):373–440，2020年。'
- en: '[93] Qin Wang, Wen Li, and Luc Van Gool. Semi-supervised learning by augmented
    distribution alignment. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pages 1466–1475, 2019.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Qin Wang, Wen Li 和 Luc Van Gool。通过增强分布对齐进行半监督学习。载于《IEEE/CVF国际计算机视觉会议论文集》，第1466–1475页，2019年。'
- en: '[94] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning
    from massive noisy labeled data for image classification. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 2691–2699, 2015.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Tong Xiao, Tian Xia, Yi Yang, Chang Huang 和 Xiaogang Wang。利用大量噪声标签数据进行图像分类。载于《IEEE计算机视觉与模式识别会议论文集》，第2691–2699页，2015年。'
- en: '[95] Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-task curriculum
    framework for open-set semi-supervised learning. In European Conference on Computer
    Vision, pages 438–454. Springer, 2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Qing Yu, Daiki Ikami, Go Irie 和 Kiyoharu Aizawa。开放集半监督学习的多任务课程框架。载于《欧洲计算机视觉会议》，第438–454页。Springer，2020年。'
- en: '[96] Xingrui Yu, Xiaomin Wu, Chunbo Luo, and Peng Ren. Deep learning in remote
    sensing scene classification: a data augmentation enhanced convolutional neural
    network framework. GIScience & Remote Sensing, 54(5):741–758, 2017.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Xingrui Yu, Xiaomin Wu, Chunbo Luo 和 Peng Ren。遥感场景分类中的深度学习：数据增强增强的卷积神经网络框架。《地理信息科学与遥感》，54(5):741–758，2017年。'
- en: '[97] Willard Zamora-Cárdenas, Mauro Mendez, Saul Calderon-Ramirez, Martin Vargas,
    Gerardo Monge, Steve Quiros, David Elizondo, Jordina Torrents-Barrena, and Miguel A
    Molina-Cabello. Enforcing morphological information in fully convolutional networks
    to improve cell instance segmentation in fluorescence microscopy images. In International
    Work-Conference on Artificial Neural Networks, pages 36–46\. Springer, 2021.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Willard Zamora-Cárdenas, Mauro Mendez, Saul Calderon-Ramirez, Martin Vargas,
    Gerardo Monge, Steve Quiros, David Elizondo, Jordina Torrents-Barrena 和 Miguel
    A Molina-Cabello。加强完全卷积网络中的形态学信息，以改善荧光显微镜图像中的细胞实例分割。载于《国际人工神经网络工作会议》，第36–46页。Springer，2021年。'
- en: '[98] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l:
    Self-supervised semi-supervised learning. In Proceedings of the IEEE international
    conference on computer vision, pages 1476–1485, 2019.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov 和 Lucas Beyer. S4l:
    自监督半监督学习。发表于IEEE国际计算机视觉会议，页面1476–1485，2019年。'
- en: '[99] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup:
    Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin 和 David Lopez-Paz. mixup:
    超越经验风险最小化。arXiv 预印本 arXiv:1710.09412，2017年。'
- en: '[100] Xujiang Zhao, Killamsetty Krishnateja, Rishabh Iyer, and Feng Chen. Robust
    semi-supervised learning with out of distribution data. arXiv preprint arXiv:2010.03658,
    2020.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Xujiang Zhao, Killamsetty Krishnateja, Rishabh Iyer 和 Feng Chen. 使用分布外数据的鲁棒半监督学习。arXiv
    预印本 arXiv:2010.03658，2020年。'
- en: '[101] Xiaojin Jerry Zhu. Semi-supervised learning literature survey. Technical
    report, University of Wisconsin-Madison Department of Computer Sciences, 2005.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Xiaojin Jerry Zhu. 半监督学习文献综述。技术报告，威斯康星大学麦迪逊分校计算机科学系，2005年。'
- en: '[102] Xinyue Zhu, Yifan Liu, Jiahong Li, Tao Wan, and Zengchang Qin. Emotion
    classification with data augmentation using generative adversarial networks. In
    Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 349–360\.
    Springer, 2018.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Xinyue Zhu, Yifan Liu, Jiahong Li, Tao Wan 和 Zengchang Qin. 使用生成对抗网络的数据增强情感分类。发表于太平洋-亚洲知识发现与数据挖掘会议，页面349–360。Springer，2018年。'
