- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:36:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:36:46'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2310.01154] Modularity in Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2310.01154] 深度学习中的模块化：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.01154](https://ar5iv.labs.arxiv.org/html/2310.01154)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.01154](https://ar5iv.labs.arxiv.org/html/2310.01154)
- en: 'Modularity in Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的模块化：综述
- en: Haozhe Sun ¹    Isabelle Guyon ^(1,2)(¹  LISN/CNRS/INRIA, Université Paris-Saclay,
    France
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Haozhe Sun ¹    Isabelle Guyon ^(1,2)(¹  LISN/CNRS/INRIA, 巴黎萨克雷大学, 法国
- en: ²  ChaLearn, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²  ChaLearn, 美国
- en: 'Email: haozhe.sun@universite-paris-saclay.fr)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '邮箱: haozhe.sun@universite-paris-saclay.fr'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Modularity is a general principle present in many fields. It offers attractive
    advantages, including, among others, ease of conceptualization, interpretability,
    scalability, module combinability, and module reusability. The deep learning community
    has long sought to take inspiration from the modularity principle, either implicitly
    or explicitly. This interest has been increasing over recent years. We review
    the notion of modularity in deep learning around three axes: data, task, and model,
    which characterize the life cycle of deep learning. Data modularity refers to
    the observation or creation of data groups for various purposes. Task modularity
    refers to the decomposition of tasks into sub-tasks. Model modularity means that
    the architecture of a neural network system can be decomposed into identifiable
    modules. We describe different instantiations of the modularity principle, and
    we contextualize their advantages in different deep learning sub-fields. Finally,
    we conclude the paper with a discussion of the definition of modularity and directions
    for future research.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化是许多领域中的一个普遍原则。它提供了许多有吸引力的优势，包括概念化的便捷性、可解释性、可扩展性、模块组合性和模块重用性。深度学习社区长期以来一直寻求从模块化原则中获得灵感，无论是隐式还是显式的。这种兴趣近年来一直在增加。我们围绕数据、任务和模型这三个轴心回顾了深度学习中的模块化概念，这些轴心特征了深度学习的生命周期。数据模块化指的是为了不同目的观察或创建数据组。任务模块化指的是将任务分解成子任务。模型模块化意味着神经网络系统的架构可以分解成可识别的模块。我们描述了模块化原则的不同实例，并将其优势在不同深度学习子领域中进行背景化。最后，我们通过对模块化定义的讨论以及未来研究方向的展望来结束本文。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Modularity is a general principle present in many fields such as biology [[29](#bib.bib29),
    [241](#bib.bib241), [80](#bib.bib80), [195](#bib.bib195), [22](#bib.bib22), [58](#bib.bib58),
    [84](#bib.bib84), [59](#bib.bib59), [81](#bib.bib81), [186](#bib.bib186), [140](#bib.bib140),
    [180](#bib.bib180), [52](#bib.bib52), [108](#bib.bib108)], complex systems [[217](#bib.bib217),
    [218](#bib.bib218)], mathematics [[14](#bib.bib14), [31](#bib.bib31)], system
    design [[177](#bib.bib177), [91](#bib.bib91), [208](#bib.bib208), [167](#bib.bib167),
    [55](#bib.bib55)], computer science [[17](#bib.bib17), [83](#bib.bib83)], graph
    theory [[170](#bib.bib170), [168](#bib.bib168), [182](#bib.bib182), [93](#bib.bib93)].
    While sharing the same name, there is no universally agreed upon definition of
    modularity [[26](#bib.bib26)]. However, we can identify a shared definition [[9](#bib.bib9),
    [207](#bib.bib207)]: in general, modularity is the property of an entity whereby
    it can be broken down into a number of sub-entities (referred to as modules).
    This definition has different instantiations in different fields with their nuances [[205](#bib.bib205)]
    from which various properties may arise. Such field-specific properties include
    autonomy of modules (limited interaction or limited interdependence between modules) [[177](#bib.bib177),
    [17](#bib.bib17), [119](#bib.bib119), [15](#bib.bib15), [170](#bib.bib170), [87](#bib.bib87),
    [14](#bib.bib14), [208](#bib.bib208), [167](#bib.bib167), [52](#bib.bib52), [113](#bib.bib113),
    [96](#bib.bib96), [260](#bib.bib260), [7](#bib.bib7)], functional specialization
    of modules [[80](#bib.bib80), [59](#bib.bib59), [195](#bib.bib195), [84](#bib.bib84),
    [91](#bib.bib91), [140](#bib.bib140), [52](#bib.bib52)], reusability of modules [[14](#bib.bib14),
    [192](#bib.bib192), [12](#bib.bib12), [6](#bib.bib6), [61](#bib.bib61), [207](#bib.bib207),
    [174](#bib.bib174), [175](#bib.bib175), [55](#bib.bib55), [184](#bib.bib184),
    [142](#bib.bib142), [43](#bib.bib43), [176](#bib.bib176), [51](#bib.bib51), [188](#bib.bib188)],
    combinability of modules [[12](#bib.bib12), [6](#bib.bib6), [144](#bib.bib144),
    [177](#bib.bib177), [188](#bib.bib188), [151](#bib.bib151), [184](#bib.bib184),
    [239](#bib.bib239), [165](#bib.bib165)], replaceability of modules [[177](#bib.bib177),
    [174](#bib.bib174), [175](#bib.bib175)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化是一个在许多领域中普遍存在的原则，例如生物学 [[29](#bib.bib29), [241](#bib.bib241), [80](#bib.bib80),
    [195](#bib.bib195), [22](#bib.bib22), [58](#bib.bib58), [84](#bib.bib84), [59](#bib.bib59),
    [81](#bib.bib81), [186](#bib.bib186), [140](#bib.bib140), [180](#bib.bib180),
    [52](#bib.bib52), [108](#bib.bib108)]，复杂系统 [[217](#bib.bib217), [218](#bib.bib218)]，数学 [[14](#bib.bib14),
    [31](#bib.bib31)]，系统设计 [[177](#bib.bib177), [91](#bib.bib91), [208](#bib.bib208),
    [167](#bib.bib167), [55](#bib.bib55)]，计算机科学 [[17](#bib.bib17), [83](#bib.bib83)]，图论 [[170](#bib.bib170),
    [168](#bib.bib168), [182](#bib.bib182), [93](#bib.bib93)]。尽管它们共享相同的名称，但对模块化的定义并没有一个普遍认可的标准 [[26](#bib.bib26)]。然而，我们可以识别出一个共享的定义 [[9](#bib.bib9),
    [207](#bib.bib207)]：一般来说，模块化是一个实体的属性，使其可以被拆分成多个子实体（称为模块）。这个定义在不同领域中有不同的体现，其细微差别 [[205](#bib.bib205)]
    可能产生各种属性。这些领域特定的属性包括模块的自治性（模块之间的互动或相互依赖有限） [[177](#bib.bib177), [17](#bib.bib17),
    [119](#bib.bib119), [15](#bib.bib15), [170](#bib.bib170), [87](#bib.bib87), [14](#bib.bib14),
    [208](#bib.bib208), [167](#bib.bib167), [52](#bib.bib52), [113](#bib.bib113),
    [96](#bib.bib96), [260](#bib.bib260), [7](#bib.bib7)]，模块的功能专门化 [[80](#bib.bib80),
    [59](#bib.bib59), [195](#bib.bib195), [84](#bib.bib84), [91](#bib.bib91), [140](#bib.bib140),
    [52](#bib.bib52)]，模块的可重用性 [[14](#bib.bib14), [192](#bib.bib192), [12](#bib.bib12),
    [6](#bib.bib6), [61](#bib.bib61), [207](#bib.bib207), [174](#bib.bib174), [175](#bib.bib175),
    [55](#bib.bib55), [184](#bib.bib184), [142](#bib.bib142), [43](#bib.bib43), [176](#bib.bib176),
    [51](#bib.bib51), [188](#bib.bib188)]，模块的可组合性 [[12](#bib.bib12), [6](#bib.bib6),
    [144](#bib.bib144), [177](#bib.bib177), [188](#bib.bib188), [151](#bib.bib151),
    [184](#bib.bib184), [239](#bib.bib239), [165](#bib.bib165)]，以及模块的可替换性 [[177](#bib.bib177),
    [174](#bib.bib174), [175](#bib.bib175)]。
- en: As a general principle, modularity is a descriptive property and an organizational
    scheme. It is a means of representing entities (data, tasks, models) to be able
    to manipulate them, conceptually or practically [[92](#bib.bib92), [17](#bib.bib17),
    [177](#bib.bib177), [55](#bib.bib55)]. Though modular entities are not necessarily
    hierarchical [[177](#bib.bib177)], many modular entities have a hierarchical structure [[217](#bib.bib217)]
    in the sense that multiple modules of a lower hierarchy level can form one module
    of a higher hierarchy level. The modules of the lower hierarchy level are of finer
    granularity than those of the higher hierarchy level. At the same level of the
    hierarchy, modules can refer to an exclusive division of the overall entity (hard
    division) or overlapping parts of the overall entity (soft division). The decomposed
    modules can be homogeneous (similar modules) or heterogeneous (dissimilar modules).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种通用原则，模块化是一种描述性属性和组织方案。它是一种表示实体（数据、任务、模型）以便能够进行操作的方式，无论是在概念上还是在实际中[[92](#bib.bib92),
    [17](#bib.bib17), [177](#bib.bib177), [55](#bib.bib55)]。尽管模块化实体不一定是层级的[[177](#bib.bib177)]，许多模块化实体在层级结构上具有层次性[[217](#bib.bib217)]，即较低层级的多个模块可以形成一个较高层级的模块。较低层级的模块粒度比较高层级的模块更细。在同一层级的模块可以指代整体实体的独占划分（硬划分）或整体实体的重叠部分（软划分）。被分解的模块可以是同质的（相似模块）或异质的（不相似模块）。
- en: 'Back to the very beginning of neural network research in the last century,
    the community started to be interested in bringing the notion of modularity to
    neural networks [[13](#bib.bib13), [15](#bib.bib15), [119](#bib.bib119), [192](#bib.bib192)],
    this interest has been revived recently [[12](#bib.bib12), [209](#bib.bib209),
    [77](#bib.bib77), [135](#bib.bib135), [6](#bib.bib6), [9](#bib.bib9), [61](#bib.bib61),
    [78](#bib.bib78), [39](#bib.bib39), [239](#bib.bib239)]. The publication trend
    (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Modularity in Deep Learning: A
    Survey")) shows an increasing interest in the modularity principle within deep
    learning over recent years. This survey investigates the notion of modularity
    in deep learning around three axes: data, task, and model. The organization of
    the survey is shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Modularity
    in Deep Learning: A Survey").'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '回到上世纪神经网络研究的最初阶段，学术界开始对将模块化概念引入神经网络产生兴趣[[13](#bib.bib13), [15](#bib.bib15),
    [119](#bib.bib119), [192](#bib.bib192)]，这一兴趣最近得到了复兴[[12](#bib.bib12), [209](#bib.bib209),
    [77](#bib.bib77), [135](#bib.bib135), [6](#bib.bib6), [9](#bib.bib9), [61](#bib.bib61),
    [78](#bib.bib78), [39](#bib.bib39), [239](#bib.bib239)]。出版趋势（图[1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Modularity in Deep Learning: A Survey)")显示，近年来深度学习中对模块化原则的兴趣逐渐增加。本次调查围绕数据、任务和模型这三条轴探讨了深度学习中的模块化概念。调查的组织结构如图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Modularity in Deep Learning: A Survey")所示。'
- en: '![Refer to caption](img/83471c5163b89e830dcea972f005d56c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/83471c5163b89e830dcea972f005d56c.png)'
- en: 'Figure 1: Publication trend of “modular deep learning” from 1990 to 2021. The
    ratio of the count of publications containing “modular deep learning” and “modular
    neural network” among publications containing “deep learning” and “neural network”,
    indexed by Google Scholar. The horizontal axis is the publication year.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：“模块化深度学习”从1990年到2021年的出版趋势。包含“模块化深度学习”和“模块化神经网络”的出版物与包含“深度学习”和“神经网络”的出版物的比例，由Google
    Scholar索引。横轴为出版年份。
- en: '![Refer to caption](img/5e0ab356eeb04da76c02da515a94fcb1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5e0ab356eeb04da76c02da515a94fcb1.png)'
- en: 'Figure 2: Organization of this survey. The first three sections discuss how
    the modularity principle is instantiated in the three axes: data, task, and model
    architecture. We then cover other modularity notions for completeness. Finally,
    we discuss the definition of modularity and directions for future research. The
    introduction and conclusion are ignored in this figure.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本次调查的组织结构。前三部分讨论了模块化原则在数据、任务和模型架构这三条轴上的具体体现。接着，我们覆盖了其他模块化概念以确保全面性。最后，我们讨论了模块化的定义以及未来研究的方向。引言和结论在此图中被忽略。
- en: 2 Data modularity
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据模块化
- en: Data is an entity used to represent knowledge and information. In the context
    of machine learning and deep learning, it can take various forms e.g., image,
    audio sound, and text. Data samples can be interpreted as points in a high dimensional
    space (fixed-length dense vectors) [[8](#bib.bib8), [136](#bib.bib136), [138](#bib.bib138)].
    A collection of data samples is a dataset. Datasets can be used to train or test
    deep learning models, referred to as training or test datasets. In these scenarios,
    data is the input of deep learning models (neural networks) [[94](#bib.bib94)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是用来表示知识和信息的实体。在机器学习和深度学习的背景下，它可以采取各种形式，如图像、音频和文本。数据样本可以被理解为高维空间中的点（固定长度的密集向量） [[8](#bib.bib8),
    [136](#bib.bib136), [138](#bib.bib138)]。数据样本的集合是数据集。数据集可以用于训练或测试深度学习模型，被称为训练集或测试集。在这些情景中，数据是深度学习模型（神经网络）的输入 [[94](#bib.bib94)]。
- en: Data modularity is the observation or creation of data groups; it refers to
    how a dataset can be divided into different modules for various purposes. The
    division of the dataset into modules facilitates conception and data manipulation.
    Data modularization can influence the training of learning machines [[100](#bib.bib100),
    [71](#bib.bib71), [227](#bib.bib227)]. Some algorithms leverage data modularity
    so that each data module is processed by a different solver [[187](#bib.bib187)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模块化是对数据组进行观察或创建；它指的是数据集如何可以为不同目的划分为不同模块。将数据集划分为模块有助于概念和数据操作。数据的模块化能够影响机器学习的训练 [[100](#bib.bib100),
    [71](#bib.bib71), [227](#bib.bib227)]。一些算法利用数据模块化，使得每个数据模块由不同的求解器处理 [[187](#bib.bib187)]。
- en: 'We identify two types of data modularity: intrinsic data modularity and imposed
    data modularity. Intrinsic data modularity means identifiable dataset divisions
    naturally in data, which a human practitioner does not introduce. Imposed data
    modularity means identifiable dataset divisions that a human practitioner introduces.
    The rationale of this taxonomy is that when the dataset reaches the practitioner
    who analyses it, it already contains some form of intrinsic modularity, including
    that stemming from the class labels. The people who collect the data are not considered
    practitioners.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以鉴别两种数据模块化：内在数据模块化和外加数据模块化。内在数据模块化意味着数据中自然可识别的数据集划分，而人类从业者并未引入。外加数据模块化意味着人类从业者引入可鉴别的数据集划分。这一分类的基本原理在于当数据集送达分析者手中时，它已经包含某种形式的内在模块化，包括从类标签中衍生出的模块化。收集数据的人并未被视为从业者。
- en: '![Refer to caption](img/6e01d951631f1e07b4eed9848d9beb19.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e01d951631f1e07b4eed9848d9beb19.png)'
- en: 'Figure 3: Illustration of modularity in data. (a) intrinsic data modularity
    based on super-classes, images, and class hierarchy in ImageNet [[63](#bib.bib63)];
    (b) intrinsic data modularity based on styles characterized by a set of metadata,
    the upper-left circle contains black-on-white characters, the upper-right circle
    contains white-on-black characters, the lower circle contains characters with
    natural foreground and background, all characters are drawn from the same set
    of classes (small-case Latin characters), these three circles illustrate the division
    of a character dataset based on its metadata; (c) intrinsic manifolds in the form
    of a moon dataset, where each data manifold can be considered as a module; (d)
    few-shot learning episodes, reprinted from [[191](#bib.bib191)]. (a), (b) and
    (c) are examples of intrinsic data modularity, (d) is an example of imposed data
    modularity.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '图3: 数据中的模块化示意图。 (a) 基于ImageNet中的超类、图像和类层次结构的内在数据模块化 [[63](#bib.bib63)]； (b)
    基于一组元数据所表征的风格的内在数据模块化，左上方圆圈包含白底黑字的字符，右上方圆圈包含黑底白字的字符，下方圆圈包含带有天然前景和背景的字符，所有字符来自同一类别集合（小写拉丁字符），这三个圆圈展示了基于其元数据对字符数据集的划分；
    (c) 以月球数据集形式存在的内在曲面，其中每个数据曲面可以被视为一个模块； (d) 来自[[191](#bib.bib191)]的少样本学习案例，重新引用自[[191](#bib.bib191)]。
    (a)、(b)和(c)是内在数据模块化的例子，(d)是外加数据模块化的例子。'
- en: 2.1 Intrinsic data modularity
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 内在数据模块化
- en: Intrinsic data modularity means identifiable dataset divisions naturally in
    data, which are not introduced by a human practitioner.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 内在数据模块化意味着数据中自然可识别的数据集划分，并不是由人类从业者引入的。
- en: 'Any supervised learning datasets can be divided according to the classes (labels);
    data points belonging to the same class are supposed to be close to each other
    in a hidden space, which allows for solutions of classification algorithms. Classes
    sharing common semantics can be further grouped to form super-classes. For example,
    ImageNet [[63](#bib.bib63)] has a class hierarchy (see Figure [3](#S2.F3 "Figure
    3 ‣ 2 Data modularity ‣ Modularity in Deep Learning: A Survey") (a)) which is
    used by Meta-Dataset [[235](#bib.bib235)]. Omniglot dataset [[142](#bib.bib142)]
    and OmniPrint datasets [[227](#bib.bib227)] contain character images organized
    in scripts, each script (super-class) contains several characters (classes); Meta-Album
    dataset [[236](#bib.bib236)] is a meta-dataset including 40 datasets, where each
    dataset can be considered as a super-class. The super-classes provide information
    about class similarity, allowing splitting datasets according to the semantics [[258](#bib.bib258)].'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '任何有监督学习的数据集都可以根据类别（标签）进行划分；属于同一类别的数据点应该在隐藏空间中彼此接近，这为分类算法的解决方案提供了可能。具有共同语义的类别可以进一步分组形成超类别。例如，ImageNet [[63](#bib.bib63)]
    具有一个类别层次结构（见图 [3](#S2.F3 "Figure 3 ‣ 2 Data modularity ‣ Modularity in Deep Learning:
    A Survey") (a)），Meta-Dataset [[235](#bib.bib235)] 使用了这个层次结构。Omniglot 数据集 [[142](#bib.bib142)]
    和 OmniPrint 数据集 [[227](#bib.bib227)] 包含按脚本组织的字符图像，每个脚本（超类别）包含若干个字符（类别）；Meta-Album
    数据集 [[236](#bib.bib236)] 是一个包含40个数据集的元数据集，每个数据集可以视为一个超类别。超类别提供了有关类别相似性的信息，允许根据语义对数据集进行拆分 [[258](#bib.bib258)]。'
- en: 'In addition to the classes or super-classes, data points can also be grouped
    by one or several metadata such as time, location, and gender. Such metadata is
    available with the Exif data of photos. The OmniPrint data synthesizer generates
    data together with a comprehensive set of metadata, including font, background,
    foreground, margin size, shear angle, rotation angle, etc. [[227](#bib.bib227)]
    (see Figure [3](#S2.F3 "Figure 3 ‣ 2 Data modularity ‣ Modularity in Deep Learning:
    A Survey") (b)). The NORB dataset collected stereo image pairs of 50 uniform-colored
    toys under 36 angles, 9 azimuths, and 6 lighting conditions, where the angles,
    azimuths, and lighting conditions serve as the metadata [[145](#bib.bib145)].'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '除了类别或超类别外，数据点还可以按一个或多个元数据进行分组，例如时间、位置和性别。这些元数据可以通过照片的Exif数据获得。OmniPrint 数据合成器生成数据，并附带一整套全面的元数据，包括字体、背景、前景、边距大小、剪切角度、旋转角度等 [[227](#bib.bib227)]（见图 [3](#S2.F3
    "Figure 3 ‣ 2 Data modularity ‣ Modularity in Deep Learning: A Survey") (b)）。NORB
    数据集收集了50种均匀颜色玩具在36个角度、9个方位角和6种光照条件下的立体图像对，其中角度、方位角和光照条件作为元数据 [[145](#bib.bib145)]。'
- en: 'Some datasets contain intrinsic clusters in the high-dimensional feature space.
    Such intrinsic clusters can stem from the underlying data generative process,
    where latent categorical variables determine the natural groups of data. An illustrative
    example is a Gaussian Mixture distribution where data points are assumed to be
    generated from a mixture of a finite number of Gaussian distributions with unknown
    parameters [[101](#bib.bib101)]. Some datasets have intrinsic manifolds; an illustrative
    example is the moons dataset as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2 Data
    modularity ‣ Modularity in Deep Learning: A Survey") (c), where the two manifolds
    interlace while preserving an identifiable division, each manifold can be considered
    as a module. Both of the above examples fall into the category of data clustering.
    When data samples are interconnected in the form of a graph [[154](#bib.bib154),
    [249](#bib.bib249)], this is called graph partitioning. One question which arises
    is how to determine the optimal clustering of a dataset. Luxburg et al. [[240](#bib.bib240)]
    argue that there are no optimal domain-independent clustering algorithms and that
    clustering should always be studied in the context of its end-use.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '一些数据集在高维特征空间中包含内在的簇。这些内在簇可能源自底层的数据生成过程，其中潜在的类别变量决定了数据的自然分组。一个说明性的例子是高斯混合分布，其中数据点被假设为从若干个未知参数的高斯分布的混合中生成 [[101](#bib.bib101)]。一些数据集具有内在的流形；一个说明性的例子是如图 [3](#S2.F3
    "Figure 3 ‣ 2 Data modularity ‣ Modularity in Deep Learning: A Survey") (c) 所示的月亮数据集，其中两个流形交织在一起，同时保持可识别的分界线，每个流形可以视为一个模块。上述两个例子都属于数据聚类的范畴。当数据样本以图形的形式互联时 [[154](#bib.bib154),
    [249](#bib.bib249)]，这被称为图分割。一个问题是如何确定数据集的最佳聚类。Luxburg 等人 [[240](#bib.bib240)]
    认为没有最优的领域独立聚类算法，聚类应该始终在其最终用途的背景下进行研究。'
- en: Multi-modal deep learning aims to build models that can process and relate information
    from multiple modalities. Here the modality refers to the way in which something
    happens or is experienced e.g., data in the form of image, text, audio [[19](#bib.bib19)].
    Multi-modal datasets fall into the category of intrinsic data modularity in the
    sense that the data in each modality can be considered a module. For example,
    VQA v2.0 dataset [[97](#bib.bib97)] consists of open-ended questions about images;
    SpeakingFaces dataset [[3](#bib.bib3)] consists of aligned thermal and visual
    spectra image streams of fully-framed faces synchronized with audio recordings
    of each subject speaking.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态深度学习旨在构建能够处理和关联来自多种模态的信息的模型。这里的模态指的是某事发生或被体验的方式，例如数据以图像、文本、音频的形式呈现[[19](#bib.bib19)]。多模态数据集属于内在数据模块化的范畴，因为每种模态中的数据可以被视为一个模块。例如，VQA
    v2.0数据集[[97](#bib.bib97)]包含关于图像的开放性问题；SpeakingFaces数据集[[3](#bib.bib3)]包含与每个受试者讲话的音频录音同步的热成像和视觉光谱图像流。
- en: 2.2 Imposed data modularity
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 强加的数据模块化
- en: Imposed data modularity means identifiable dataset divisions which are introduced
    by a human practitioner.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 强加的数据模块化意味着由人类从业者引入的可识别的数据集划分。
- en: When training deep learning models [[94](#bib.bib94)], human practitioners usually
    divide the whole training dataset into mini-batches, which can be seen as a kind
    of imposed data modularity. The gradient is computed using one mini-batch of data
    for each parameter update; one training epoch means passing through all the mini-batches.
    This iterative learning regime is called stochastic gradient descent [[199](#bib.bib199)].
    Mini-batches reduce the memory requirement for backpropagation, which makes training
    large deep learning models possible. On the other hand, batch size also influences
    learning behavior. Smith et al. [[221](#bib.bib221)] showed that the benefits
    of decaying the learning rate could be obtained by instead increasing the training
    batch size. Keskar et al. [[131](#bib.bib131)] showed that learning with large
    batch sizes usually gives worse generalization performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度学习模型时[[94](#bib.bib94)]，人类从业者通常将整个训练数据集划分为小批量，这可以看作是一种强加的数据模块化。每次参数更新时，梯度是使用一个小批量的数据计算的；一个训练周期意味着遍历所有小批量。这种迭代学习模式称为随机梯度下降[[199](#bib.bib199)]。小批量减少了反向传播所需的内存，从而使得训练大型深度学习模型成为可能。另一方面，批量大小也会影响学习行为。Smith等人[[221](#bib.bib221)]表明，通过增加训练批量大小可以获得学习率衰减的好处。Keskar等人[[131](#bib.bib131)]表明，大批量大小的学习通常会导致较差的泛化性能。
- en: Instead of using a sequence of mini-batches sampled uniformly at random from
    the entire training dataset, curriculum learning [[100](#bib.bib100)] uses non-uniform
    sampling of mini-batches such that the mini-batch sequence exhibits an increasing
    level of difficulty. A related concept is active learning [[193](#bib.bib193)],
    which assumes that different data points in a dataset have different values for
    the current model update; it tries to select the data points with the highest
    value to construct the actual training set.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习[[100](#bib.bib100)]不使用从整个训练数据集中均匀随机采样的小批量序列，而是使用非均匀采样的小批量，使得小批量序列表现出递增的难度。相关概念是主动学习[[193](#bib.bib193)]，它假设数据集中的不同数据点对当前模型更新有不同的价值；它尝试选择具有最高价值的数据点来构建实际训练集。
- en: 'The model performance is usually tested on few-shot episodes in few-shot learning
    and meta-learning. Few-shot episodes are typically formed by drawing several classes
    $N$ from the class pool and several examples $K$ for each selected class, called
    $N$-way-$K$-shot episodes [[79](#bib.bib79), [223](#bib.bib223)] (Figure [3](#S2.F3
    "Figure 3 ‣ 2 Data modularity ‣ Modularity in Deep Learning: A Survey") (d)).
    For such scenarios, the meta-training phase can employ the same episodic learning
    regime or not [[235](#bib.bib235)], recent studies [[242](#bib.bib242), [244](#bib.bib244),
    [141](#bib.bib141)] and competition results [[71](#bib.bib71)] suggest that episodic
    meta-training is not more effective than vanilla pretraining with access to the
    global class pool.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能通常在少样本学习和元学习中的少样本实验上进行测试。少样本实验通常通过从类别池中抽取若干类别$N$和每个选择的类别的若干样本$K$来形成，称为$N$-way-$K$-shot实验 [[79](#bib.bib79),
    [223](#bib.bib223)]（图 [3](#S2.F3 "图 3 ‣ 2 数据模块化 ‣ 深度学习中的模块化：综述")（d））。对于这种情况，元训练阶段可以采用相同的情节学习机制，也可以不采用 [[235](#bib.bib235)]，最近的研究 [[242](#bib.bib242),
    [244](#bib.bib244), [141](#bib.bib141)]和竞赛结果 [[71](#bib.bib71)] 表明，情节元训练不比在全球类别池中访问的普通预训练更有效。
- en: Data augmentation is a way to generate more training data by applying transformations
    to existing data [[216](#bib.bib216)]. The transformed versions of the same data
    point can be seen as a module. Some transformations, such as rotation and translation,
    form a group structure [[196](#bib.bib196)]. The effect of such data augmentation
    can be understood as averaging over the orbits of the group that keeps the data
    distribution approximately invariant and leads to variance reduction [[40](#bib.bib40)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一种通过对现有数据应用转换来生成更多训练数据的方法 [[216](#bib.bib216)]。相同数据点的转换版本可以视为一个模块。一些转换，如旋转和平移，形成了一个组结构 [[196](#bib.bib196)]。这种数据增强的效果可以理解为在保持数据分布大致不变的情况下，对组的轨道进行平均，从而减少方差 [[40](#bib.bib40)]。
- en: In addition to splitting the dataset into subsets of samples, each data sample
    can be split into subdivisions of features, referred to as feature partitioning.
    A dataset can be represented as a matrix where each row represents one data sample;
    each column represents one feature dimension. It can then be divided along the
    sample and feature dimensions. Schmidt et al. [[207](#bib.bib207)] process each
    feature partition with a different model. For image classification tasks, input
    images can be split into small patches that can be processed in parallel [[121](#bib.bib121),
    [67](#bib.bib67)].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将数据集划分为样本子集外，每个数据样本还可以被划分为特征子集，称为特征分区。数据集可以表示为一个矩阵，其中每一行代表一个数据样本；每一列代表一个特征维度。然后可以沿样本和特征维度进行划分。Schmidt
    等人 [[207](#bib.bib207)] 使用不同的模型处理每个特征分区。对于图像分类任务，输入图像可以被分割成可以并行处理的小块 [[121](#bib.bib121),
    [67](#bib.bib67)]。
- en: 2.3 Conclusion of data modularity
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 数据模块化的结论
- en: We argue that data without structure contains no useful information for learning
    dependencies (e.g., between feature and label). Some dependencies boil down to
    the emergence or the creation of groups. Intrinsic data modularity relates to
    the semantic relationship between samples and how data samples are similar or
    dissimilar. Imposed data modularity, on the other hand, relates to the way that
    practitioners organize data at hand to better train learning machines.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，没有结构的数据对学习依赖关系（例如，特征与标签之间）没有有用的信息。一些依赖关系归结为组的出现或创建。内在的数据模块化涉及样本之间的语义关系以及数据样本的相似或不相似。强加的数据模块化，则涉及从业者如何组织手头的数据，以更好地训练学习机器。
- en: Future research for data-centric deep learning may investigate the relationship
    between intrinsic and imposed data modularity. For example, does intrinsic data
    modularity promote imposed data modularity? How does this interplay affect model
    training?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 未来对数据中心深度学习的研究可能会探讨内在和强加数据模块化之间的关系。例如，内在数据模块化是否促进强加数据模块化？这种相互作用如何影响模型训练？
- en: Data modularity describes how the input of deep learning models can be modularized.
    On the other hand, the end goal (the output) of deep learning models can also
    be modularized, which is the topic of the next section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模块化描述了深度学习模型输入如何被模块化。另一方面，深度学习模型的最终目标（输出）也可以被模块化，这是下一节的主题。
- en: 3 Task modularity
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 任务模块化
- en: '![Refer to caption](img/05919b7fab61cb1aca5aa9cc8bc81371.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/05919b7fab61cb1aca5aa9cc8bc81371.png)'
- en: 'Figure 4: Illustration of sub-task decomposition. The upper figure illustrates
    the parallel decomposition of a task. The lower figure illustrates the sequential
    decomposition of a task.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：子任务分解的示意图。上图展示了任务的并行分解，下图展示了任务的顺序分解。
- en: Deep learning models are tools to solve tasks e.g., from the classification
    of entities to the generation of realistic photos. Solving a task is equal to
    achieving a corresponding objective. In deep learning, we usually model an objective
    by an explicit differentiable objective function (also known as a loss function),
    allowing end-to-end training. This perspective can be generalized to any task,
    even if the objective function is implicit and does not entail a differentiable
    form. For example, the task of “purchasing a cup of tea” can be characterized
    by an indicator function that returns a penalty if no tea can be purchased or
    a bonus otherwise. In deep learning, tasks are often related to data; but they
    are different. Given the same dataset, one can define various tasks on top of
    it. For example, the MNIST dataset can be used either for an image classification
    benchmarking task [[158](#bib.bib158)] or for a pixel sequence classification
    benchmarking task [[139](#bib.bib139), [96](#bib.bib96)], the OmniPrint-meta[1-5]
    datasets [[227](#bib.bib227)] can be used either for a few-shot learning benchmarking
    task or for domain adaptation benchmarking task. Tasks define the objective; they
    are orthogonal to how the end goal should be achieved.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型是解决任务的工具，例如，从实体分类到生成真实的照片。解决一个任务等同于实现一个相应的目标。在深度学习中，我们通常通过明确的可微分目标函数（也称为损失函数）来建模一个目标，从而实现端到端训练。这种观点可以推广到任何任务，即使目标函数是隐式的且不具有可微分形式。例如，“购买一杯茶”的任务可以通过一个指示函数来表征，该函数如果不能购买茶则返回惩罚，否则返回奖励。在深度学习中，任务通常与数据相关，但它们是不同的。给定相同的数据集，可以在其基础上定义各种任务。例如，MNIST数据集可以用于图像分类基准任务[[158](#bib.bib158)]或像素序列分类基准任务[[139](#bib.bib139),
    [96](#bib.bib96)]，OmniPrint-meta[1-5]数据集[[227](#bib.bib227)]可以用于少样本学习基准任务或领域适应基准任务。任务定义了目标；它们与实现最终目标的方式是正交的。
- en: This section presents task modularity i.e., sub-task decomposition. Sub-task
    decomposition means that a task could be factorized or decomposed into sub-tasks.
    Sub-task decomposition facilitates conceptualization and problem-solving. The
    divide-and-conquer principle breaks down a complex problem into easier sub-problems [[57](#bib.bib57),
    [187](#bib.bib187), [15](#bib.bib15), [118](#bib.bib118)]. By solving each individual
    sub-problem and combining the solutions, the complex problem can be solved more
    efficiently. The sub-task decomposition facilitates the integration of expert
    knowledge, and the a priori knowledge can further facilitate problem-solving.
    Sub-task decomposition can also promote reuse if the overall task is compositional;
    the solution to sub-tasks may be reused in other tasks [[173](#bib.bib173), [161](#bib.bib161),
    [64](#bib.bib64), [219](#bib.bib219), [185](#bib.bib185)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍任务模块化，即**子任务分解**。子任务分解意味着一个任务可以被分解为多个子任务。子任务分解有助于概念化和解决问题。分治原则将复杂问题分解为更简单的子问题[[57](#bib.bib57),
    [187](#bib.bib187), [15](#bib.bib15), [118](#bib.bib118)]。通过解决每个子问题并结合解决方案，可以更有效地解决复杂问题。子任务分解有助于整合专家知识，先验知识还可以进一步促进问题解决。如果整体任务是可组合的，子任务分解还可以促进重用；子任务的解决方案可能会在其他任务中被重用[[173](#bib.bib173),
    [161](#bib.bib161), [64](#bib.bib64), [219](#bib.bib219), [185](#bib.bib185)]。
- en: 'The sub-task decomposition can be categorized into two regimes: parallel decomposition
    and sequential decomposition (Figure [4](#S3.F4 "Figure 4 ‣ 3 Task modularity
    ‣ Modularity in Deep Learning: A Survey")). Parallel decomposition means that
    the sub-tasks can be executed in parallel. Sequential decomposition means that
    the sub-tasks need to be executed in order; certain sub-tasks cannot be executed
    before the previous sub-task is finished. In practice, these two regimes can be
    mixed. For example, a sub-task from a sequential decomposition can be further
    decomposed parallelly, which leads to a directed acyclic graph workflow.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '子任务分解可以分为两种模式：并行分解和顺序分解（见图[4](#S3.F4 "Figure 4 ‣ 3 Task modularity ‣ Modularity
    in Deep Learning: A Survey")）。并行分解意味着子任务可以同时执行。顺序分解意味着子任务需要按顺序执行；某些子任务在前一个子任务完成之前不能执行。在实践中，这两种模式可以混合使用。例如，顺序分解中的子任务可以进一步并行分解，这会导致一个有向无环图工作流。'
- en: 3.1 Parallel sub-task decomposition
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 并行子任务分解
- en: A parallel sub-task decomposition is called homogeneous if the decomposed sub-tasks
    are similar. One typical example is dividing a multi-class classification problem
    into multiple smaller classification problems [[92](#bib.bib92)]. Given a neural
    network trained to perform a multi-class classification problem, Csordás et al. [[61](#bib.bib61)]
    use parameter masks to identify subsets of parameters solely responsible for individual
    classes on their own. Kim et al. [[133](#bib.bib133)] learn to split a neural
    network into a tree structure to handle different subsets of classes. They assume
    that different classes use different features, the tree-structured neural network
    ensuring that the later layers do not share features across different subsets
    of classes. Pan et al. [[174](#bib.bib174), [175](#bib.bib175)] and Kingetsu et
    al. [[134](#bib.bib134)] decompose a multi-class classification model into reusable,
    replaceable and combinable modules, where each module is a binary classifier.
    Such modules can be recombined without retraining to obtain a new multi-class
    classifier. These methods can be useful in situations where the classes to be
    classified frequently change. Abbas et al. [[2](#bib.bib2)] use transfer learning
    and class decomposition to improve the performance of medical image classification.
    Such sub-task decomposability is an implicit prerequisite of the model editing
    problem [[220](#bib.bib220), [162](#bib.bib162), [127](#bib.bib127), [163](#bib.bib163),
    [160](#bib.bib160)]. Model editing aims to modify a specific sub-task learned
    by a trained neural network without damaging model performance on other inputs,
    e.g., it aims to patch the mistake of the model for a particular sample. If the
    task cannot be decomposed into disentangled sub-tasks, then model editing cannot
    be achieved.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果并行子任务的分解是同质的，那么被分解的子任务是相似的。一个典型的例子是将一个多类分类问题划分为多个更小的分类问题[[92](#bib.bib92)]。在给定一个训练好的多类分类神经网络的情况下，Csordás等人[[61](#bib.bib61)]使用参数掩码来识别仅对各个类别负责的参数子集。Kim等人[[133](#bib.bib133)]学习将神经网络拆分成树形结构，以处理不同的类别子集。他们假设不同的类别使用不同的特征，树形结构的神经网络确保后面的层不会在不同的类别子集之间共享特征。Pan等人[[174](#bib.bib174),
    [175](#bib.bib175)]和Kingetsu等人[[134](#bib.bib134)]将多类分类模型分解成可重复使用、可替换和可组合的模块，每个模块是一个二分类器。这些模块可以在不重新训练的情况下重新组合，以获得一个新的多类分类器。这些方法在需要频繁更改分类类别的情况下非常有用。Abbas等人[[2](#bib.bib2)]使用迁移学习和类别分解来提高医疗图像分类的性能。这种子任务可分解性是模型编辑问题的隐含前提[[220](#bib.bib220),
    [162](#bib.bib162), [127](#bib.bib127), [163](#bib.bib163), [160](#bib.bib160)]。模型编辑的目标是修改由训练好的神经网络学习的特定子任务，而不影响模型在其他输入上的性能，例如，它旨在修复模型在特定样本上的错误。如果任务不能被分解成解耦的子任务，那么模型编辑就无法实现。
- en: A parallel sub-task decomposition is termed heterogeneous if the decomposed
    sub-tasks are dissimilar; such decomposition is usually problem-dependent and
    requires expert knowledge of the task at hand. Belay et al. [[25](#bib.bib25)]
    decompose the recognition task of Amharic characters into a vowel recognition
    task and a consonant recognition task to reduce overall task complexity. Cao et
    al. [[36](#bib.bib36)] decompose the full self-attention into question-wide and
    passage-wide self-attentions to speed up inference for question answering tasks.
    Ding et al. [[66](#bib.bib66)] decompose the facial recognition task into multiple
    facial component recognition tasks. Zhou et al. [[266](#bib.bib266)] decompose
    the neural network learning task into structure learning and parameter learning
    to learn equivariance from data automatically. Gatys et al. [[89](#bib.bib89)]
    decompose the natural image synthesis task into a content component and a style
    component, which allows recombining the content and the style in a combinatorial
    way to generate new images.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果并行子任务的分解是异质的，那么被分解的子任务是不相似的；这种分解通常依赖于问题本身，并且需要对当前任务有专家知识。Belay等人[[25](#bib.bib25)]将阿姆哈拉字符的识别任务分解为元音识别任务和辅音识别任务，以减少整体任务复杂性。Cao等人[[36](#bib.bib36)]将完整的自注意力机制分解为问题范围和段落范围的自注意力，以加快问答任务的推理速度。Ding等人[[66](#bib.bib66)]将面部识别任务分解为多个面部组件识别任务。Zhou等人[[266](#bib.bib266)]将神经网络学习任务分解为结构学习和参数学习，以自动从数据中学习等变性。Gatys等人[[89](#bib.bib89)]将自然图像合成任务分解为内容组件和风格组件，这允许以组合的方式重新组合内容和风格，从而生成新的图像。
- en: 3.2 Sequential sub-task decomposition
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 顺序子任务分解
- en: Sequential sub-task decomposition reflects the sequential pipeline of the task.
    A simple example is the division of a machine learning task into a preprocessing
    stage (data cleaning and normalization) and a model inference stage [[190](#bib.bib190)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序子任务分解反映了任务的顺序流水线。一个简单的例子是将机器学习任务分为预处理阶段（数据清理和归一化）和模型推断阶段 [[190](#bib.bib190)]。
- en: 'In reinforcement learning, a complex task can usually be decomposed [[219](#bib.bib219)]
    into a sequence of sub-tasks or steps. An illustrative example is to imagine that
    the task of manufacturing an artifact $Z$ requires purchasing the raw material
    $X$, forging $X$ to produce parts $Y$, and then assembling the parts $Y$ into
    the end product $Z$. Both $X$ and $Y$ can take different values independently
    ($X\in\{x_{1},x_{2},x_{3},...\},Y\in\{y_{1},y_{2},y_{3},...\}$). Different values
    of $X$ and $Y$ can be recombined, which forms a combinatorial number of possible
    scenarios to learn. This pipeline can be factorized into three stages: (1) raw
    material purchase, (2) forging to produce parts, and (3) assembling of parts.
    Reinforcement learning agents would learn more efficiently if the learning happens
    at the granularity of the factorized stages instead of the overall task [[56](#bib.bib56)].
    Furthermore, such a factorization enables the independence of credit assignment [[181](#bib.bib181)];
    the failure of the overall task can be traced back to the problematic stages,
    while the other stages can remain untouched. For example, if the raw material
    is of bad quality, then the purchase sub-task needs to be improved; the forging
    sub-task and the assembling sub-task do not need to be changed [[39](#bib.bib39)].'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一个复杂任务通常可以被分解 [[219](#bib.bib219)] 为一系列子任务或步骤。一个说明性的例子是想象制造一个工件 $Z$ 需要购买原材料
    $X$、将 $X$ 锻造为部件 $Y$，然后将部件 $Y$ 组装成最终产品 $Z$。$X$ 和 $Y$ 都可以独立地取不同的值（$X\in\{x_{1},x_{2},x_{3},...\},Y\in\{y_{1},y_{2},y_{3},...\}$）。不同的
    $X$ 和 $Y$ 的值可以重新组合，形成一个组合数量的可能场景进行学习。这个流水线可以分解为三个阶段：（1）原材料采购，（2）锻造生产部件，以及（3）部件组装。如果学习发生在分解阶段的粒度上而不是整体任务上，强化学习代理会更高效 [[56](#bib.bib56)]。此外，这种分解还允许信用分配的独立性 [[181](#bib.bib181)]；整体任务的失败可以追溯到有问题的阶段，而其他阶段可以保持不变。例如，如果原材料质量差，那么需要改进采购子任务；锻造子任务和组装子任务不需要更改 [[39](#bib.bib39)]。
- en: The sequential pipeline is omnipresent in practical applications e.g., optical
    character recognition (OCR), natural language processing (NLP). When facing a
    multi-script (multi-language) recognition task, the pipeline can consist of a
    script identification stage and a script-specific recognition stage [[210](#bib.bib210),
    [112](#bib.bib112)], which decouples the domain classifier and the domain-specific
    solver. The text-in-the-wild recognition task [[41](#bib.bib41)] usually consists
    of decoupled text detector (to localize the bounding box of the text) and recognizer
    (recognize the text from the bounding box) [[41](#bib.bib41)]. Traditional OCR
    methods also decompose the word recognition task into a character segmentation
    task and a character recognition task [[37](#bib.bib37), [204](#bib.bib204), [48](#bib.bib48),
    [128](#bib.bib128)]. Traditional NLP pipeline includes sentence segmentation,
    word tokenization, part-of-speech tagging, lemmatization, filtering stop words,
    and dependency parsing [[125](#bib.bib125)]. In bioinformatics, the scientific
    workflow (data manipulations and transformations) groups similar or strongly coupled
    workflow steps into modules to facilitate understanding and reuse [[55](#bib.bib55)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序流水线在实际应用中无处不在，例如光学字符识别（OCR）、自然语言处理（NLP）。在面对多脚本（多语言）识别任务时，流水线可以包括脚本识别阶段和脚本特定识别阶段 [[210](#bib.bib210),
    [112](#bib.bib112)]，这将领域分类器和领域特定求解器解耦。文本识别任务 [[41](#bib.bib41)] 通常包括解耦的文本检测器（定位文本的边界框）和识别器（从边界框中识别文本） [[41](#bib.bib41)]。传统的OCR方法还将词语识别任务分解为字符分割任务和字符识别任务 [[37](#bib.bib37),
    [204](#bib.bib204), [48](#bib.bib48), [128](#bib.bib128)]。传统的NLP流水线包括句子分割、词语标记、词性标注、词形还原、停用词过滤和依存解析 [[125](#bib.bib125)]。在生物信息学中，科学工作流（数据操作和转换）将相似或强耦合的工作流步骤分组到模块中，以便于理解和重用 [[55](#bib.bib55)]。
- en: 3.3 Conclusion of task modularity
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 任务模块化的结论
- en: The sub-task decomposition can be parallel, sequential, or mixed (directed acyclic
    graph). We provided examples from the literature that leverage sub-task decomposition
    to reduce task complexity or promote the reuse of sub-task solutions. Task modularity
    can help integrate expert knowledge and promote model interpretability when paired
    with model modularity, as will be discussed in the next section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 子任务分解可以是并行的、顺序的或混合的（有向无环图）。我们提供了文献中的例子，利用子任务分解来减少任务复杂性或促进子任务解决方案的重用。任务模块化可以在与模型模块化配对时帮助整合专家知识并促进模型解释性，这将在下一节中讨论。
- en: Future research may focus on how to automate the process of sub-task decomposition
    or make the problem-dependent sub-task decomposition techniques transferable to
    other tasks, which is an important step for AutoML. It would reduce the demand
    for highly qualified deep learning engineers, which can reduce expert bias and
    entry barriers to deep learning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究可能会集中在如何自动化子任务分解过程，或使与问题相关的子任务分解技术可迁移到其他任务，这对AutoML是一个重要的步骤。这将减少对高素质深度学习工程师的需求，从而减少专家偏见和进入深度学习的门槛。
- en: 4 Model modularity
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 模型模块化
- en: This section presents model modularity. It means that the architecture of the
    neural network system (one neural network or a system of neural networks) consists
    of identifiable sub-entities (modules).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了模型模块化。这意味着神经网络系统（一个神经网络或神经网络系统）由可识别的子实体（模块）组成。
- en: Model modularity is different from task modularity. A task define an objective,
    task modularity focuses on decomposing the objective into sub-objectives. Model
    modularity focuses on the architecture of the neural network system, it decomposes
    the solution into sub-solutions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模型模块化与任务模块化不同。任务定义一个目标，任务模块化侧重于将目标分解成子目标。模型模块化关注神经网络系统的架构，它将解决方案分解成子解决方案。
- en: 4.1 Advantages of model modularity
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型模块化的优势
- en: Model modularity provides ease of conceptual design and implementation. For
    example, modern neural networks consist of repeated layer/block patterns (modules).
    Examples include fully-connected neural networks [[94](#bib.bib94)], vanilla convolutional
    neural networks, ResNet [[105](#bib.bib105), [251](#bib.bib251)], Inception [[230](#bib.bib230)]
    and models searched by Neural Architecture Search (NAS) [[270](#bib.bib270), [74](#bib.bib74)].
    The design with homogeneous modules allows for a more concise description of the
    model architecture in the sense of Kolmogorov complexity (short description length) [[149](#bib.bib149),
    [148](#bib.bib148)]. For example, instead of specifying how each primitive operation
    (e.g., sum, product, concatenation) interacts in a computational graph, the model
    can be described as a collection of modules that interact with each other [[92](#bib.bib92)].
    The standardization of such neural network building blocks (fully-connected layers,
    convolutional layers) also enabled the development of highly optimized hardware
    and software ecosystems for fast computation [[178](#bib.bib178), [156](#bib.bib156),
    [1](#bib.bib1), [90](#bib.bib90), [98](#bib.bib98)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 模型模块化提供了概念设计和实施的便利。例如，现代神经网络由重复的层/块模式（模块）组成。示例包括全连接神经网络[[94](#bib.bib94)]、普通卷积神经网络、ResNet[[105](#bib.bib105),
    [251](#bib.bib251)]、Inception[[230](#bib.bib230)]和由神经结构搜索（NAS）搜索出的模型[[270](#bib.bib270),
    [74](#bib.bib74)]。具有同质模块的设计允许在Kolmogorov复杂度（简短描述长度）的意义上对模型架构进行更简洁的描述[[149](#bib.bib149),
    [148](#bib.bib148)]。例如，模型可以描述为一组相互作用的模块，而不是指定每个原始操作（例如，和、积、拼接）在计算图中的交互[[92](#bib.bib92)]。此类神经网络构建块（全连接层、卷积层）的标准化还促进了高度优化的硬件和软件生态系统的发展，以实现快速计算[[178](#bib.bib178),
    [156](#bib.bib156), [1](#bib.bib1), [90](#bib.bib90), [98](#bib.bib98)]。
- en: Together with sub-task decomposition (task modularity), model modularity offers
    ease of expert knowledge integration [[12](#bib.bib12), [95](#bib.bib95), [214](#bib.bib214),
    [211](#bib.bib211), [25](#bib.bib25)] and interpretability [[135](#bib.bib135),
    [184](#bib.bib184), [118](#bib.bib118), [137](#bib.bib137)]. Interpretability
    can have different forms. For example, each neural network module could be assigned
    a specific interpretable sub-task. On the other hand, selective module evaluation
    provides insights on how different samples/tasks are related [[119](#bib.bib119),
    [209](#bib.bib209), [12](#bib.bib12), [6](#bib.bib6)] in the context of conditional
    computation [[28](#bib.bib28)].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与子任务分解（任务模块化）一起，模型模块化提供了专家知识集成[[12](#bib.bib12), [95](#bib.bib95), [214](#bib.bib214),
    [211](#bib.bib211), [25](#bib.bib25)]和可解释性[[135](#bib.bib135), [184](#bib.bib184),
    [118](#bib.bib118), [137](#bib.bib137)]。可解释性可以有不同的形式。例如，每个神经网络模块可以分配一个特定的可解释子任务。另一方面，选择性模块评估提供了关于不同样本/任务如何相关的见解[[119](#bib.bib119),
    [209](#bib.bib209), [12](#bib.bib12), [6](#bib.bib6)]，这在条件计算的背景下[[28](#bib.bib28)]。
- en: The model decomposition into modules promotes reusability and knowledge transfer [[33](#bib.bib33)].
    Though each neural network is typically trained to perform a specific task, its
    (decomposed) modules could be shared across tasks if appropriate mechanisms promote
    such reusability. The simplest example would be the classical fine-tuning paradigm
    of large pretrained models [[106](#bib.bib106), [258](#bib.bib258), [50](#bib.bib50),
    [104](#bib.bib104)]. This paradigm typically freezes the pretrained model and
    only retrains its last classification layer to adapt it to the downstream task.
    Pretrained models are typically pretrained on large datasets [[201](#bib.bib201),
    [225](#bib.bib225), [254](#bib.bib254)]. The large amount and diversity of training
    data make pretrained models’ intermediate features reusable for other downstream
    tasks. More recently, the finer-grained reusability of neural network systems
    has attracted the attention of researchers. Such methods assume that the tasks
    share underlying patterns and keep an inventory of reusable modules (each module
    is a small neural network) [[12](#bib.bib12), [135](#bib.bib135), [6](#bib.bib6),
    [239](#bib.bib239)]. Each module learns different facets (latent factors or atomic
    skills) of the knowledge required to solve each task. The selective/sparse use
    and dynamic reassembling/recombination of these modules can promote sample efficiency [[184](#bib.bib184)]
    and combinatorial generalization [[12](#bib.bib12), [62](#bib.bib62), [6](#bib.bib6),
    [117](#bib.bib117)].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型分解为模块可以促进重用性和知识转移[[33](#bib.bib33)]。尽管每个神经网络通常被训练以执行特定任务，但如果适当的机制促进这种重用性，其（被分解的）模块可以在任务之间共享。最简单的例子就是大规模预训练模型的经典微调范式[[106](#bib.bib106),
    [258](#bib.bib258), [50](#bib.bib50), [104](#bib.bib104)]。该范式通常冻结预训练模型，只重新训练其最后的分类层，以使其适应下游任务。预训练模型通常在大型数据集上进行预训练[[201](#bib.bib201),
    [225](#bib.bib225), [254](#bib.bib254)]。大量和多样化的训练数据使得预训练模型的中间特征可以重用于其他下游任务。最近，神经网络系统的细粒度重用性引起了研究人员的关注。这些方法假设任务共享底层模式，并保持一个可重用模块的清单（每个模块是一个小型神经网络）[[12](#bib.bib12),
    [135](#bib.bib135), [6](#bib.bib6), [239](#bib.bib239)]。每个模块学习解决每个任务所需知识的不同方面（潜在因素或原子技能）。这些模块的选择性/稀疏使用和动态重组/组合可以促进样本效率[[184](#bib.bib184)]和组合泛化[[12](#bib.bib12),
    [62](#bib.bib62), [6](#bib.bib6), [117](#bib.bib117)]。
- en: Combinatorial generalization is also known as compositional generalization,
    “infinite use of finite means” [[47](#bib.bib47)], and systematic generalization.
    It aims to generalize to unseen compositions of known functions/factors/words [[60](#bib.bib60),
    [82](#bib.bib82), [143](#bib.bib143), [132](#bib.bib132), [173](#bib.bib173),
    [38](#bib.bib38), [185](#bib.bib185)], it is the ability to systematically recombine
    previously learned elements to map new inputs made up from these elements to their
    correct output [[206](#bib.bib206)]. For example, new sentences consist of new
    compositions of a known set of words. Combinatorial generalization is argued to
    be important to achieve human-like generalization [[23](#bib.bib23), [184](#bib.bib184),
    [164](#bib.bib164), [146](#bib.bib146), [114](#bib.bib114), [183](#bib.bib183),
    [134](#bib.bib134), [243](#bib.bib243), [142](#bib.bib142), [115](#bib.bib115),
    [237](#bib.bib237), [153](#bib.bib153)]. Learning different facets of knowledge
    with different modules in a reusable way could be one solution to combinatorial
    generalization. Modular systems have been shown effective for combinatorial generalization [[197](#bib.bib197)]
    in various fields e.g., natural language processing [[144](#bib.bib144), [184](#bib.bib184),
    [114](#bib.bib114), [183](#bib.bib183), [169](#bib.bib169)], visual question answering [[12](#bib.bib12),
    [62](#bib.bib62), [16](#bib.bib16)], object recognition [[185](#bib.bib185), [142](#bib.bib142),
    [176](#bib.bib176), [38](#bib.bib38)], and robotics [[6](#bib.bib6), [64](#bib.bib64),
    [179](#bib.bib179), [51](#bib.bib51)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 组合概括也被称为组合概括，“有限手段的无限使用”[[47](#bib.bib47)]，以及系统概括。它旨在对已知函数/因子/词的未见组合进行概括[[60](#bib.bib60),
    [82](#bib.bib82), [143](#bib.bib143), [132](#bib.bib132), [173](#bib.bib173),
    [38](#bib.bib38), [185](#bib.bib185)], 它是有系统地重新组合先前学习的元素以将由这些元素组成的新输入映射到其正确输出的能力[[206](#bib.bib206)]。例如，新句子由已知一组词的新组合构成。据说组合概括对实现类人概括是重要的[[23](#bib.bib23),
    [184](#bib.bib184), [164](#bib.bib164), [146](#bib.bib146), [114](#bib.bib114),
    [183](#bib.bib183), [134](#bib.bib134), [243](#bib.bib243), [142](#bib.bib142),
    [115](#bib.bib115), [237](#bib.bib237), [153](#bib.bib153)]。以可重复利用的方式用不同模块学习知识的不同方面可能是解决组合概括的一种方法。模块化系统已被证明对各个领域的组合概括[[197](#bib.bib197)]
    如自然语言处理[[144](#bib.bib144), [184](#bib.bib184), [114](#bib.bib114), [183](#bib.bib183),
    [169](#bib.bib169)], 视觉问答[[12](#bib.bib12), [62](#bib.bib62), [16](#bib.bib16)],
    目标识别[[185](#bib.bib185), [142](#bib.bib142), [176](#bib.bib176), [38](#bib.bib38)],
    和机器人技术[[6](#bib.bib6), [64](#bib.bib64), [179](#bib.bib179), [51](#bib.bib51)]
    有效。
- en: The modularization of neural network systems promotes knowledge retention. If
    different knowledge is localized into different modules, targeted knowledge updates
    and troubleshooting [[134](#bib.bib134), [174](#bib.bib174), [175](#bib.bib175)]
    will be possible. This can alleviate gradient interference of different tasks [[261](#bib.bib261),
    [126](#bib.bib126), [155](#bib.bib155)] and catastrophic forgetting [[239](#bib.bib239),
    [202](#bib.bib202), [233](#bib.bib233), [10](#bib.bib10), [120](#bib.bib120),
    [85](#bib.bib85), [6](#bib.bib6), [4](#bib.bib4), [72](#bib.bib72), [129](#bib.bib129),
    [172](#bib.bib172)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络系统的模块化有助于知识的保留。如果不同的知识被定位到不同的模块中，就可能进行有针对性的知识更新和故障排除[[134](#bib.bib134),
    [174](#bib.bib174), [175](#bib.bib175)]。这可以减轻不同任务的梯度干扰[[261](#bib.bib261), [126](#bib.bib126),
    [155](#bib.bib155)]和灾难性遗忘[[239](#bib.bib239), [202](#bib.bib202), [233](#bib.bib233),
    [10](#bib.bib10), [120](#bib.bib120), [85](#bib.bib85), [6](#bib.bib6), [4](#bib.bib4),
    [72](#bib.bib72), [129](#bib.bib129), [172](#bib.bib172)]。
- en: Modular neural network systems facilitate model scaling in two ways. (1) Modular
    models like fully-connected models and ResNet can be scaled up (or down) by simply
    stacking more (or less) modules to increase (or decrease) the model capacity to
    fit larger (or smaller) datasets [[105](#bib.bib105)]. (2) Modular methods based
    on sparsely activated Mixture-of-Experts [[209](#bib.bib209)] decouple computation
    cost from model size. They allow drastically increasing the model capacity without
    increasing compute cost because only a small fraction of the model is evaluated
    on each forward pass [[75](#bib.bib75), [209](#bib.bib209), [102](#bib.bib102),
    [68](#bib.bib68), [49](#bib.bib49), [21](#bib.bib21)]. The extreme example of
    these sparsely activated models is Switch Transformer [[76](#bib.bib76)] which
    contains 1.6 trillion parameters, pushing the competition of large model sizes [[35](#bib.bib35),
    [222](#bib.bib222)] to the next level.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化神经网络系统通过两种方式促进模型的扩展。 (1) 模块化模型，如全连接模型和 ResNet，可以通过简单地堆叠更多（或更少）的模块来放大（或缩小），从而增加（或减少）模型的容量以适应更大（或更小）的数据集[[105](#bib.bib105)]。
    (2) 基于稀疏激活专家混合体的模块化方法[[209](#bib.bib209)]将计算成本与模型大小解耦。它们允许在不增加计算成本的情况下大幅度增加模型容量，因为每次前向传播时只有模型的一小部分会被评估[[75](#bib.bib75),
    [209](#bib.bib209), [102](#bib.bib102), [68](#bib.bib68), [49](#bib.bib49), [21](#bib.bib21)]。这些稀疏激活模型的极端例子是
    Switch Transformer[[76](#bib.bib76)]，它包含 1.6 万亿个参数，将大模型尺寸的竞争推向了新的水平[[35](#bib.bib35),
    [222](#bib.bib222)]。
- en: 4.2 Typical modules in deep learning models
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 深度学习模型中的典型模块
- en: This section reviews some typical modules in the deep learning literature.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了深度学习文献中的一些典型模块。
- en: Almost all systems are modular to some degree [[205](#bib.bib205)], neural network
    systems can almost always be decomposed into subsystems (modules) [[18](#bib.bib18)]
    following different points of view. More specifically, they usually consist of
    a hierarchical structure in which a module of a higher hierarchy level is made
    of modules of a lower hierarchy level. The elementary layer of modern neural networks
    (e.g., fully-connected layer, convolutional layer) can be seen as a module on
    its own. On the other hand, any neural network as a whole can also be considered
    as a module e.g., in the context of ensemble [[268](#bib.bib268)], Mixture-of-Experts [[119](#bib.bib119)],
    and Generative Adversarial Networks (GAN) [[95](#bib.bib95)]. Some literature [[61](#bib.bib61),
    [26](#bib.bib26), [226](#bib.bib226), [134](#bib.bib134)] define modules as sub-neural
    networks where part of the parameters are masked out (set to 0). In these cases,
    overlapping modules can be obtained when the masks overlap.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有系统都在某种程度上是模块化的[[205](#bib.bib205)]，神经网络系统几乎总是可以根据不同的观点被分解为子系统（模块）[[18](#bib.bib18)]。更具体地说，它们通常由一个层次结构组成，其中较高层次的模块由较低层次的模块构成。现代神经网络的基本层（例如，全连接层、卷积层）可以被视为一个独立的模块。另一方面，任何神经网络整体也可以被视为一个模块，例如，在集成[[268](#bib.bib268)]、专家混合体[[119](#bib.bib119)]和生成对抗网络（GAN）[[95](#bib.bib95)]的背景下。一些文献[[61](#bib.bib61),
    [26](#bib.bib26), [226](#bib.bib226), [134](#bib.bib134)]将模块定义为子神经网络，其中部分参数被遮蔽（设置为
    0）。在这些情况下，当遮蔽区域重叠时，可以获得重叠的模块。
- en: '![Refer to caption](img/244cac99b7dc0d7ddc74aeee4fcd38ff.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/244cac99b7dc0d7ddc74aeee4fcd38ff.png)'
- en: 'Figure 5: Examples of a module. (a) a fully-connected layer; (b) a basic ResNet
    module, reprinted from [[105](#bib.bib105)]; (c) an LSTM module, reprinted from
    [[44](#bib.bib44)].'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：模块示例。 (a) 一个全连接层； (b) 一个基础 ResNet 模块，转载自 [[105](#bib.bib105)]； (c) 一个 LSTM
    模块，转载自 [[44](#bib.bib44)]。
- en: 4.2.1 Modules for non-sequential data
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.1 非顺序数据的模块
- en: 'Fully-connected layers (Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Typical modules in
    deep learning models ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey")
    (a)) imitate the connections between neurons in biological neural networks but
    connect every input neuron to every output neuron [[94](#bib.bib94)]. In practice,
    a fully-connected layer is implemented as a matrix multiplication between input
    data and learnable parameters. Convolutional layers introduce the inductive bias
    of translation equivariance. Conceptually, a convolutional layer (with a single
    output channel) can be obtained from a fully-connected layer by enforcing local
    connectivity and parameter sharing [[94](#bib.bib94)]. Local connectivity means
    that each neuron only connects to a subset of neurons of the previous layer; parameter
    sharing means that the same learnable parameters are used across receptive fields.
    In practice, a convolutional layer is implemented as a collection of kernels/filters
    shifted over the input data [[178](#bib.bib178), [156](#bib.bib156)]. Each kernel
    performs a dot product between input data and learnable parameters. Depending
    on the number of dimensions over which kernels are shifted, a convolutional layer
    is termed e.g., 1D, 2D, 3D. 2D convolutional layers are widely used in computer
    vision tasks [[147](#bib.bib147), [138](#bib.bib138)]. Locally connected layers
    are similar to convolutional layers except that they remove the constraint of
    parameter sharing (across kernels). It helps if one wants to impose local receptive
    fields while there is no reason to think each local kernel should be the same [[94](#bib.bib94)].
    Low-rank locally connected layers relax spatial equivariance and provide a trade-off
    between locally connected layers and convolutional layers. The kernel applied
    at each position is constructed as a linear combination of a basis set of kernels
    with spatially varying combining weights. Varying the number of basis kernels
    allows controlling the degree of relaxation of spatial equivariance [[73](#bib.bib73)].
    Standard convolutional layers offer translation equivariance; a line of research
    focuses on generalizing this to other equivariances (rotation, reflection), referred
    to as group convolutional layers [[53](#bib.bib53), [65](#bib.bib65), [54](#bib.bib54),
    [248](#bib.bib248), [88](#bib.bib88), [24](#bib.bib24), [247](#bib.bib247), [246](#bib.bib246)].
    On the other hand, depthwise separable convolutional layers [[213](#bib.bib213),
    [46](#bib.bib46), [109](#bib.bib109)] factorize a standard convolutional layer
    into a depthwise convolutional layer and a pointwise convolutional layer, which
    reduces model size and computation.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层（图 [5](#S4.F5 "图 5 ‣ 4.2 深度学习模型中的典型模块 ‣ 4 模型模块化 ‣ 深度学习中的模块化：调查") (a)）模拟了生物神经网络中神经元之间的连接，但将每个输入神经元与每个输出神经元连接[[94](#bib.bib94)]。在实践中，全连接层实现为输入数据与可学习参数之间的矩阵乘法。卷积层引入了平移等变性的归纳偏置。从概念上讲，通过强制局部连接和参数共享，可以从全连接层获得一个卷积层（具有单一输出通道）[[94](#bib.bib94)]。局部连接意味着每个神经元仅连接到前一层的一部分神经元；参数共享意味着在感受野中使用相同的可学习参数。在实践中，卷积层实现为在输入数据上移动的内核/滤波器的集合[[178](#bib.bib178),
    [156](#bib.bib156)]。每个内核执行输入数据与可学习参数之间的点积。根据内核移动的维度数量，卷积层被称为，例如，1D、2D、3D。2D卷积层广泛应用于计算机视觉任务[[147](#bib.bib147),
    [138](#bib.bib138)]。局部连接层与卷积层类似，不同之处在于它们去除了参数共享的约束（跨内核）。这有助于在想要施加局部感受野的情况下，而没有理由认为每个局部内核应该是相同的[[94](#bib.bib94)]。低秩局部连接层放宽了空间等变性，并在局部连接层和卷积层之间提供了一种折中。每个位置应用的内核被构造为一组基础内核的线性组合，具有空间变化的组合权重。改变基础内核的数量允许控制空间等变性的放宽程度[[73](#bib.bib73)]。标准卷积层提供平移等变性；一项研究方向集中在将其推广到其他等变性（旋转、反射），称为组卷积层[[53](#bib.bib53),
    [65](#bib.bib65), [54](#bib.bib54), [248](#bib.bib248), [88](#bib.bib88), [24](#bib.bib24),
    [247](#bib.bib247), [246](#bib.bib246)]。另一方面，深度可分离卷积层[[213](#bib.bib213), [46](#bib.bib46),
    [109](#bib.bib109)] 将标准卷积层分解为深度卷积层和逐点卷积层，从而减少了模型大小和计算量。
- en: 'Multiple layers can be grouped into a building block (a module of a higher
    hierarchy level). Such examples include the building blocks of ResNet [[105](#bib.bib105)],
    Inception [[230](#bib.bib230), [229](#bib.bib229)], ResNeXt [[251](#bib.bib251)],
    Wide ResNet [[262](#bib.bib262)]. Inception [[230](#bib.bib230), [229](#bib.bib229)]
    has parallel kernels of multiple sizes within each block and merge their results
    to extract information at varying scales. Inception also includes several techniques
    to reduce computation cost e.g., factorizing large kernels into smaller kernels
    and using $1\times 1$ convolution to reduce dimensionality. A ResNet block [[105](#bib.bib105)]
    (Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Typical modules in deep learning models ‣ 4
    Model modularity ‣ Modularity in Deep Learning: A Survey") (b)) contains a sequence
    of convolutional layers; it adds a skip-connection (also known as residual connection,
    identity mapping) from the beginning to the end of the block to alleviate vanishing
    gradients. Many variants of the ResNet block have been proposed. For example,
    Wide ResNet [[262](#bib.bib262)] increases the block width; ResNeXt [[251](#bib.bib251)]
    aggregates parallel paths within each block.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '多层可以被组合成一个构建块（更高层级的模块）。例如，ResNet [[105](#bib.bib105)]、Inception [[230](#bib.bib230),
    [229](#bib.bib229)]、ResNeXt [[251](#bib.bib251)]、Wide ResNet [[262](#bib.bib262)]等构建块。在每个块内，Inception [[230](#bib.bib230),
    [229](#bib.bib229)]具有多个尺寸的并行内核，并将它们的结果合并，以提取不同尺度的信息。Inception还包括几种技术来降低计算成本，例如将大内核分解为小内核和使用$1\times
    1$卷积来减少维度。ResNet块 [[105](#bib.bib105)]（图 [5](#S4.F5 "Figure 5 ‣ 4.2 Typical modules
    in deep learning models ‣ 4 Model modularity ‣ Modularity in Deep Learning: A
    Survey") (b)）包含一系列卷积层；它从块的开始到结束添加一个跳跃连接（也称为残差连接、恒等映射），以缓解梯度消失问题。已经提出了许多ResNet块的变体。例如，Wide
    ResNet [[262](#bib.bib262)]增加了块的宽度；ResNeXt [[251](#bib.bib251)]在每个块内聚合并行路径。'
- en: The block design could be automatically searched instead of handcrafted. In
    order to narrow down the model search space, some Neural Architecture Search methods [[74](#bib.bib74),
    [116](#bib.bib116), [270](#bib.bib270), [257](#bib.bib257)] automatically search
    the optimal design pattern for a block (also known as a cell) while fixing the
    block composition scheme (also known as meta-architecture). Once the block design
    patterns are searched, the full model is instantiated by repeating the searched
    blocks following the predefined block composition scheme. For example, NAS-Bench-101 [[257](#bib.bib257)]
    defines the block search space as all possible directed acyclic graphs on V nodes
    ($V\leqslant 7$) while limiting the maximum number of edges to 9.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 块设计可以通过自动搜索而非手工设计。为了缩小模型搜索空间，一些神经架构搜索方法 [[74](#bib.bib74), [116](#bib.bib116),
    [270](#bib.bib270), [257](#bib.bib257)] 自动搜索块的最佳设计模式（也称为单元），同时固定块的组成方案（也称为元架构）。一旦搜索到块设计模式，整个模型将通过重复搜索到的块并遵循预定义的块组成方案来实例化。例如，NAS-Bench-101 [[257](#bib.bib257)]
    将块搜索空间定义为所有可能的有向无环图在V节点（$V\leqslant 7$）上，同时将最大边数限制为9。
- en: McNeely-White et al. [[159](#bib.bib159)] report that the features learned by
    Inception and ResNet are almost linear transformations of each other, even though
    these two architectures have a remarkable difference in the architectural design
    philosophy. This result explains why the two architectures usually perform similarly
    and highlights the importance of training data. This result is corroborated by
    Bouchacourt et al. [[30](#bib.bib30)], who argue that invariance generally stems
    from the data itself rather than from architectural bias.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: McNeely-White等人 [[159](#bib.bib159)] 报告称，Inception和ResNet学习到的特征几乎是彼此的线性变换，尽管这两种架构在设计哲学上有显著差异。这一结果解释了为什么这两种架构通常表现相似，并强调了训练数据的重要性。这一结果得到了Bouchacourt等人 [[30](#bib.bib30)]
    的证实，他们认为不变性通常源自数据本身，而非架构偏差。
- en: 4.2.2 Modules for sequential data
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2.2 顺序数据的模块
- en: When the input data is sequential e.g., time series, text, audio, video, Recurrent
    Neural Networks (RNN) [[200](#bib.bib200)] come into play. The RNN module processes
    the sequential data one at a time; the output (also known as the hidden state)
    of the RNN module at the previous time step is recursively fed back to the RNN
    module, which allows it to aggregate information across different time steps.
    The vanilla RNN module suffers from short-term memory issues; it cannot effectively
    preserve information over long sequences. To overcome this issue, gated recurrent
    unit (GRU) [[45](#bib.bib45)] and long short-term memory (LSTM) [[107](#bib.bib107)]
    module use gates to control which information should be stored or forgotten in
    the memory, which allows better preservation of long-term information. In GRU
    and LSTM modules, gates are neural networks with trainable parameters. While GRU
    modules are faster to train than LSTM modules, their performance comparison varies
    depending on the scenario. GRU surpasses LSTM in long text and small dataset scenarios
    while LSTM outperforms GRU in other scenarios [[255](#bib.bib255)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入数据是序列性的，例如时间序列、文本、音频或视频时，递归神经网络（RNN）[[200](#bib.bib200)] 就会发挥作用。RNN 模块逐个处理序列数据；RNN
    模块在上一个时间步骤的输出（也称为隐藏状态）会递归地反馈给 RNN 模块，这使其能够跨时间步骤汇总信息。普通的 RNN 模块存在短期记忆问题；它无法有效地保留长序列中的信息。为了解决这个问题，门控递归单元（GRU）[[45](#bib.bib45)]
    和长短期记忆（LSTM）[[107](#bib.bib107)] 模块使用门控来控制哪些信息应被存储或遗忘，从而更好地保留长期信息。在 GRU 和 LSTM
    模块中，门控是具有可训练参数的神经网络。虽然 GRU 模块比 LSTM 模块训练速度更快，但它们的性能比较因场景而异。GRU 在长文本和小数据集场景中优于
    LSTM，而 LSTM 在其他场景中则超越 GRU [[255](#bib.bib255)]。
- en: Contrary to RNN, GRU, and LSTM, which process sequential data one at a time,
    self-attention layers [[238](#bib.bib238)] process the data sequence in parallel.
    For each data point in a data sequence (e.g., each time step of a time series),
    a self-attention layer creates three transformed versions, referred to as query
    vector, key vector, and value vector, through linear transformations. Between
    each pair of data points, the dot product between the query vector and the key
    vector of the pair reflects how much those two data points are related within
    the sequence. These dot products are then normalized and combined with the corresponding
    value vectors to get the new representation of each data point in the sequence.
    An enhanced version of self-attention layers is multi-head self-attention layers,
    which extract different versions of query vector, key vector, and value vector
    for each data point. Multi-head self-attention layers improve performance by capturing
    more diverse representations. A transformer block combines multi-head self-attention
    layers, fully-connected layers, normalization layers, and skip-connections. Models
    built upon transformer blocks have achieved state-of-the-art performance in a
    wide range of tasks such as natural language processing [[130](#bib.bib130)] and
    speech synthesis [[150](#bib.bib150)]. Transformer models can be applied to image
    modality by transforming each input image into a sequence of small image patches [[67](#bib.bib67)].
    Despite the lack of image-specific inductive bias (translation equivariance, locality),
    vision transformers can achieve state-of-the-art performance when combined with
    a large amount of training data [[67](#bib.bib67), [103](#bib.bib103), [20](#bib.bib20)].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与逐个处理序列数据的 RNN、GRU 和 LSTM 相反，自注意力层[[238](#bib.bib238)] 并行处理数据序列。对于数据序列中的每个数据点（例如时间序列中的每个时间步），自注意力层通过线性变换创建三个转换版本，分别称为查询向量、键向量和值向量。在每对数据点之间，查询向量和键向量的点积反映了这两个数据点在序列中的相关程度。这些点积随后被归一化，并与对应的值向量组合，以获得每个数据点在序列中的新表示。自注意力层的增强版本是多头自注意力层，它为每个数据点提取不同版本的查询向量、键向量和值向量。多头自注意力层通过捕捉更多样化的表示来提高性能。变换器块结合了多头自注意力层、全连接层、归一化层和跳跃连接。基于变换器块构建的模型在自然语言处理[[130](#bib.bib130)]
    和语音合成[[150](#bib.bib150)] 等广泛任务中实现了最先进的性能。变换器模型可以通过将每个输入图像转换为小的图像补丁序列[[67](#bib.bib67)]
    来应用于图像模态。尽管缺乏图像特定的归纳偏置（平移等变性、局部性），但当与大量训练数据结合时，视觉变换器仍能实现最先进的性能[[67](#bib.bib67),
    [103](#bib.bib103), [20](#bib.bib20)]。
- en: 4.3 Composition of modules
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 模块组成
- en: '![Refer to caption](img/1f90a435558132504f1f4d60e662f384.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1f90a435558132504f1f4d60e662f384.png)'
- en: 'Figure 6: Illustration of module composition. (a) Sequential concatenation.
    (b) Ensembling. (c) Tree-structure composition. (d) General Directed Acyclic Graph.
    (e) Conditional composition. (f) Cooperation composition.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：模块组成示意图。 (a) 顺序连接。 (b) 集成。 (c) 树状结构组成。 (d) 一般有向无环图。 (e) 条件组成。 (f) 协作组成。
- en: 'Section [4.2](#S4.SS2 "4.2 Typical modules in deep learning models ‣ 4 Model
    modularity ‣ Modularity in Deep Learning: A Survey") presents typical modules
    in the literature. Section [4.3](#S4.SS3 "4.3 Composition of modules ‣ 4 Model
    modularity ‣ Modularity in Deep Learning: A Survey") discusses how to organize
    these modules to form a model (or a module of a higher hierarchy level).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '第 [4.2](#S4.SS2 "4.2 Typical modules in deep learning models ‣ 4 Model modularity
    ‣ Modularity in Deep Learning: A Survey") 节介绍了文献中的典型模块。第 [4.3](#S4.SS3 "4.3 Composition
    of modules ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey") 节讨论了如何组织这些模块以形成一个模型（或更高层次的模块）。'
- en: 4.3.1 Static composition of modules
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3.1 模块的静态组成
- en: Static composition means that the composed structure does not vary with input;
    the same structure is used for all input samples or tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 静态组成意味着组成结构不会随输入而变化；相同的结构用于所有输入样本或任务。
- en: 'One straightforward way to compose modules is sequential concatenation (Figure [6](#S4.F6
    "Figure 6 ‣ 4.3 Composition of modules ‣ 4 Model modularity ‣ Modularity in Deep
    Learning: A Survey") (a)). It implies that multiple (typically homogeneous) modules
    are sequentially concatenated into a chain to form a model, where a module’s output
    is the next module’s input. Examples of sequential concatenation include fully-connected
    models [[94](#bib.bib94)] and ResNet models [[105](#bib.bib105)]. This composition
    scheme typically does not assume an explicit sub-task decomposition; the chain
    of concatenated modules can instead be seen as a series of information extraction
    steps [[258](#bib.bib258), [234](#bib.bib234), [5](#bib.bib5)], extracted features
    transition from low-level to high-level.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '组成模块的一个直接方法是顺序连接 (图 [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules ‣ 4 Model
    modularity ‣ Modularity in Deep Learning: A Survey") (a))。这意味着多个（通常是同质的）模块被顺序连接成一个链，以形成一个模型，其中一个模块的输出是下一个模块的输入。顺序连接的例子包括全连接模型 [[94](#bib.bib94)]
    和 ResNet 模型 [[105](#bib.bib105)]。这种组成方案通常不假设显式的子任务分解；连接模块的链可以看作是一系列信息提取步骤 [[258](#bib.bib258),
    [234](#bib.bib234), [5](#bib.bib5)]，提取的特征从低级过渡到高级。'
- en: 'Ensembling composition [[268](#bib.bib268), [124](#bib.bib124), [171](#bib.bib171)],
    on the other hand, organizes modules in a parallel manner (Figure [6](#S4.F6 "Figure
    6 ‣ 4.3 Composition of modules ‣ 4 Model modularity ‣ Modularity in Deep Learning:
    A Survey") (b)). The principle of ensembling is to aggregate (e.g., averaging)
    the results of multiple modules (weaker learners) to obtain a more robust prediction.
    The rationale is that different modules are expected to provide complementary
    and diverse views of input data. Each module’s data is processed independently
    without relying on the other modules at inference time. The regularization method
    Dropout [[224](#bib.bib224)], which randomly deactivates neurons during training,
    can be seen as an implicit ensemble method of overlapping modules.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '集成组成 [[268](#bib.bib268), [124](#bib.bib124), [171](#bib.bib171)]，另一方面，以并行方式组织模块
    (图 [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules ‣ 4 Model modularity ‣ Modularity
    in Deep Learning: A Survey") (b))。集成的原则是聚合（例如，平均）多个模块（较弱学习者）的结果，以获得更稳健的预测。其理由是不同的模块预计会提供输入数据的互补和多样化视图。每个模块的数据在推理时独立处理，不依赖于其他模块。正则化方法
    Dropout [[224](#bib.bib224)]，在训练期间随机停用神经元，可以看作是重叠模块的隐式集成方法。'
- en: 'Sequential composition and parallel composition can be combined, e.g., in the
    form of a tree structure (Figure [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules
    ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey") (c)). A typical
    scenario of tree-structure composition is a model with a shared feature extractor
    and multiple task-specific heads [[265](#bib.bib265), [215](#bib.bib215)]. All
    the above composition schemes are special cases of DAG (Directed Acyclic Graph,
    Figure [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules ‣ 4 Model modularity
    ‣ Modularity in Deep Learning: A Survey") (d)). The general DAG composition scheme
    is typically found in models searched by Neural Architecture Search [[152](#bib.bib152),
    [252](#bib.bib252), [192](#bib.bib192)].'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '顺序组合和并行组合可以结合使用，例如，形成树状结构（图 [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules
    ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey") (c)）。树状结构组合的典型场景是一个具有共享特征提取器和多个任务特定头的模型
    [[265](#bib.bib265), [215](#bib.bib215)]。上述所有组合方案都是有向无环图（DAG，图 [6](#S4.F6 "Figure
    6 ‣ 4.3 Composition of modules ‣ 4 Model modularity ‣ Modularity in Deep Learning:
    A Survey") (d)）的特例。一般的DAG组合方案通常出现在通过神经架构搜索（Neural Architecture Search）找到的模型中 [[152](#bib.bib152),
    [252](#bib.bib252), [192](#bib.bib192)]。'
- en: 'Cooperation composition (Figure [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules
    ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey") (f)) assumes that
    each module is a standalone neural network with specific functionality and that
    these neural networks cooperate during training or inference; it is a neural network
    system that consists of multiple separate neural networks. Different from ensembling
    composition, modules in cooperation composition are typically heterogeneous and
    interact with each other more diversely. For example, siamese networks [[34](#bib.bib34),
    [42](#bib.bib42), [122](#bib.bib122)] consists of two neural networks (module)
    which work together to produce different versions of the input data. Generative
    Adversarial Networks (GAN) [[95](#bib.bib95), [269](#bib.bib269)] trains a generator
    under the guidance of a discriminator. The same spirit applies to teacher and
    student neural networks [[231](#bib.bib231)]. Some deep reinforcement learning
    methods implement the Actor-Critic [[228](#bib.bib228)] with two separate new
    networks, such as AlphaGo [[214](#bib.bib214)], A3C [[166](#bib.bib166)], ACKTR [[250](#bib.bib250)].
    Continual learning with deep replay buffer [[211](#bib.bib211)] consists of a
    continual neural network learner and a generative neural network serving as the
    replay buffer. Some other continual learning methods [[202](#bib.bib202), [233](#bib.bib233),
    [10](#bib.bib10), [239](#bib.bib239)] continuously expanding model capacity for
    new tasks by adding new modules which work in cooperation with old modules.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '协作组合（图 [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules ‣ 4 Model modularity
    ‣ Modularity in Deep Learning: A Survey") (f)）假设每个模块是一个具有特定功能的独立神经网络，这些神经网络在训练或推理过程中相互合作；这是一个由多个独立神经网络组成的神经网络系统。与集成组合不同，协作组合中的模块通常是异构的，彼此之间的互动更加多样。例如，孪生网络
    [[34](#bib.bib34), [42](#bib.bib42), [122](#bib.bib122)] 由两个神经网络（模块）组成，它们协同工作以产生输入数据的不同版本。生成对抗网络（GAN）
    [[95](#bib.bib95), [269](#bib.bib269)] 在鉴别器的指导下训练生成器。同样的思想适用于教师和学生神经网络 [[231](#bib.bib231)]。一些深度强化学习方法实现了具有两个独立网络的Actor-Critic
    [[228](#bib.bib228)]，例如AlphaGo [[214](#bib.bib214)]、A3C [[166](#bib.bib166)]、ACKTR
    [[250](#bib.bib250)]。使用深度重放缓冲区的持续学习 [[211](#bib.bib211)] 包含一个持续的神经网络学习者和一个作为重放缓冲区的生成神经网络。其他一些持续学习方法
    [[202](#bib.bib202), [233](#bib.bib233), [10](#bib.bib10), [239](#bib.bib239)]
    通过添加新的模块来不断扩展模型的能力，这些新模块与旧模块协同工作以应对新任务。'
- en: 4.3.2 Conditional composition of modules
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3.2 模块的条件组合
- en: 'Conditional composition (Figure [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules
    ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey") (e)) is complementary
    to static composition in the sense that the composed modules are selectively (conditionally,
    sparsely, or dynamically) activated (used or evaluated) for each particular input.
    The input conditioning can happen at the granularity of individual sample [[12](#bib.bib12),
    [135](#bib.bib135), [119](#bib.bib119)] as well as task [[184](#bib.bib184), [155](#bib.bib155),
    [226](#bib.bib226), [157](#bib.bib157), [118](#bib.bib118)]. In the literature,
    this paradigm is also termed conditional computation [[28](#bib.bib28), [27](#bib.bib27)].'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '条件组合（图 [6](#S4.F6 "Figure 6 ‣ 4.3 Composition of modules ‣ 4 Model modularity
    ‣ Modularity in Deep Learning: A Survey") (e)）是对静态组合的补充，因为组合的模块会根据每个特定输入进行选择性（条件、稀疏或动态）激活（使用或评估）。输入条件化可以在单个样本[[12](#bib.bib12),
    [135](#bib.bib135), [119](#bib.bib119)]的粒度上进行，也可以在任务[[184](#bib.bib184), [155](#bib.bib155),
    [226](#bib.bib226), [157](#bib.bib157), [118](#bib.bib118)]上进行。在文献中，这种范式也被称为条件计算[[28](#bib.bib28),
    [27](#bib.bib27)]。'
- en: The idea of conditional computation can be traced back to Mixture-of-Experts
    (MoE) introduced in the last century. An MoE is a system composed of multiple
    separate neural networks (modules), each of which learns to handle a sub-task
    of the overall task [[118](#bib.bib118), [267](#bib.bib267)] e.g., a subset of
    the complete training dataset. A gating network computes the probability of assigning
    each example to each module [[119](#bib.bib119), [123](#bib.bib123)] or a sparse
    weighted combination of modules [[209](#bib.bib209)]. Two issues of MoE are module
    collapse [[209](#bib.bib209), [135](#bib.bib135), [164](#bib.bib164)] and shrinking
    batch size [[209](#bib.bib209)], both of which are related to the balance of module
    utilization. Module collapse means under-utilization of modules or lack of module
    diversity. Due to the self-reinforcing behavior of the gating network during training,
    premature modules may be selected and thus trained even more. The gating network
    may end up converging to always selecting a small subset of modules while the
    other modules are never used. Shrinking batch size means the batch size is reduced
    for each conditionally activated module. Large batch sizes are necessary for modern
    hardware to make efficient inferences because they alleviate the cost of data
    transfers [[209](#bib.bib209)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 条件计算的概念可以追溯到上世纪引入的专家混合（MoE）。MoE 是由多个独立的神经网络（模块）组成的系统，每个模块学习处理整体任务的一个子任务[[118](#bib.bib118),
    [267](#bib.bib267)]，例如，完整训练数据集的一个子集。一个门控网络计算将每个示例分配到每个模块的概率[[119](#bib.bib119),
    [123](#bib.bib123)]，或者是模块的稀疏加权组合[[209](#bib.bib209)]。MoE 的两个问题是模块崩溃[[209](#bib.bib209),
    [135](#bib.bib135), [164](#bib.bib164)]和批量大小缩小[[209](#bib.bib209)]，这两个问题都与模块利用的平衡有关。模块崩溃指的是模块的利用不足或模块多样性的缺乏。由于门控网络在训练过程中具有自我强化行为，可能会选择早期的模块并使其得到更多训练。门控网络可能最终会收敛到总是选择一小部分模块，而其他模块则从未被使用。批量大小缩小指的是每个条件激活模块的批量大小减少。对于现代硬件，大批量是必要的，因为它们缓解了数据传输的成本[[209](#bib.bib209)]。
- en: 'MoE can be generalized to e.g., stacked MoE [[70](#bib.bib70), [135](#bib.bib135),
    [198](#bib.bib198), [77](#bib.bib77), [189](#bib.bib189)] or hierarchical MoE [[209](#bib.bib209),
    [256](#bib.bib256)] (Figure [7](#S4.F7 "Figure 7 ‣ 4.3 Composition of modules
    ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey")). Eigen et al. [[70](#bib.bib70)]
    first explored stacked MoE; they introduced the idea of using multiple MoE with
    their own gating networks. In order to train stacked MoE, Kirsch et al. [[135](#bib.bib135)]
    use generalized Viterbi Expectation-Maximization algorithm, Rosenbaum et al. [[198](#bib.bib198)]
    employ a multi-agent reinforcement learning algorithm, Fernando et al. [[77](#bib.bib77)]
    use a genetic algorithm. MoE systems do not always have explicit gating networks;
    for instance, Fernando et al. [[77](#bib.bib77)] rely on the results of the genetic
    algorithm to decide the module routing scheme.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'MoE 可以推广到例如，堆叠 MoE[[70](#bib.bib70), [135](#bib.bib135), [198](#bib.bib198),
    [77](#bib.bib77), [189](#bib.bib189)] 或层次 MoE[[209](#bib.bib209), [256](#bib.bib256)]（图 [7](#S4.F7
    "Figure 7 ‣ 4.3 Composition of modules ‣ 4 Model modularity ‣ Modularity in Deep
    Learning: A Survey")）。Eigen 等人[[70](#bib.bib70)] 首次探索了堆叠 MoE；他们提出了使用多个 MoE 及其自身门控网络的想法。为了训练堆叠
    MoE，Kirsch 等人[[135](#bib.bib135)] 使用了广义 Viterbi 期望最大化算法，Rosenbaum 等人[[198](#bib.bib198)]
    采用了多智能体强化学习算法，Fernando 等人[[77](#bib.bib77)] 使用了遗传算法。MoE 系统并不总是具有明确的门控网络；例如，Fernando
    等人[[77](#bib.bib77)] 依赖于遗传算法的结果来决定模块路由方案。'
- en: '![Refer to caption](img/bff0b0a19ad7c59556c2d19f8514aa09.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bff0b0a19ad7c59556c2d19f8514aa09.png)'
- en: 'Figure 7: Extension of Mixture-of-Experts (MoE). (a) A stacked MoE, which stacks
    multiple MoE layers into a chain. (b) A hierarchical MoE, where a primary gating
    network chooses a sparse weighted combination of “modules”, each of which is an
    MoE with its own gating network.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：混合专家（MoE）的扩展。（a）一个堆叠的MoE，将多个MoE层堆叠成一个链条。（b）一个分层的MoE，其中一个主要的门控网络选择“模块”的稀疏加权组合，每个模块都是一个具有自己门控网络的MoE。
- en: Inspired by MoE, some deep learning methods keep an inventory of reusable specialized
    modules that can be conditionally reassembled for each input. This approach has
    been advocated to promote knowledge transfer, sample efficiency, and generalization.
    For example, in visual question answering, Neural Module Networks [[12](#bib.bib12),
    [111](#bib.bib111), [62](#bib.bib62)] dynamically reassemble modules into a neural
    network to locate the attention (region of interest) on the questioned image.
    The question’s parsing guides the reassembling process so that the reassembled
    model reflects the structure and semantics of the question. For this particular
    task, the compositionality of modules comes from the compositionality of visual
    attention. Following the question’s syntax, the reassembled modules sequentially
    modify the attention onto the questioned image. For example, the module associated
    with the word “cat” locates the image region containing a cat, and the module
    associated with the word “above” shifts up the attention. Zhang et al. [[264](#bib.bib264)]
    investigated adding new abilities to a generic network by directly transplanting
    the module corresponding to the new ability, dubbed network transplanting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 受到MoE的启发，一些深度学习方法保留了可重复使用的专门模块的库存，这些模块可以针对每个输入条件性地重新组装。这种方法被提倡以促进知识转移、样本效率和泛化。例如，在视觉问答中，神经模块网络 [[12](#bib.bib12),
    [111](#bib.bib111), [62](#bib.bib62)] 动态地将模块重新组装成一个神经网络，以定位被询问图像上的注意区域（兴趣区域）。问题的解析引导了重新组装过程，以便重新组装的模型反映出问题的结构和语义。对于这个特定的任务，模块的组合性来自于视觉注意的组合性。按照问题的语法，重新组装的模块依次修改对被询问图像的注意。例如，与“cat”一词相关的模块定位包含猫的图像区域，而与“above”一词相关的模块则向上移动注意力。张等人 [[264](#bib.bib264)]
    研究了通过直接移植与新能力相对应的模块来为通用网络添加新能力的方法，这被称为网络移植。
- en: Some work relies on the hypothesis that the tasks at hand share some commonalities
    i.e., hidden factors are shared across tasks. Each hidden factor can be learned
    by a separate module from the module inventory for transfer learning and meta-learning.
    For example, Alet et al. [[6](#bib.bib6)] use simulated annealing to meta-learn
    an inventory of modules reusable across tasks to achieve combinatorial generalization.
    The parameters of an inventory of modules are optimized during meta-training;
    the trained modules are reassembled during the meta-test with an optional parameter
    fine-tuning process. They demonstrated the utility of their method for robotics
    tasks. Ponti et al. [[184](#bib.bib184)] assume that each task is associated with
    a subset of latent discrete skills from a skill inventory. They try to generalize
    more systematically to new tasks by disentangling and recombining different facets
    of knowledge. More precisely, they jointly learn a skill-specific parameter vector
    for each latent skill and a binary task-skill allocation matrix. For each new
    task, the new model’s parameter vector is created as the average of the skill-specific
    parameter vectors corresponding to the skills present in the new task (in addition
    to a shared base parameter vector).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作依赖于这样的假设：当前的任务之间存在一些共同点，即隐藏因素在任务之间是共享的。每个隐藏因素可以通过从模块库存中分开的模块进行转移学习和元学习。例如，Alet
    等人 [[6](#bib.bib6)] 使用模拟退火来元学习一个可以在任务之间重复使用的模块库存，以实现组合泛化。在元训练期间，模块库存的参数被优化；在元测试期间，训练好的模块在一个可选的参数微调过程中被重新组装。他们展示了他们的方法在机器人任务中的效用。Ponti
    等人 [[184](#bib.bib184)] 假设每个任务与技能库存中的一组潜在离散技能相关联。他们通过解开和重新组合知识的不同方面，尝试更系统地推广到新任务中。更具体地，他们联合学习每个潜在技能的技能特定参数向量和一个二进制任务-技能分配矩阵。对于每个新任务，新模型的参数向量被创建为与新任务中存在的技能对应的技能特定参数向量的平均值（此外还有一个共享的基础参数向量）。
- en: The conditional composition scheme also has other forms. For example, Teerapittayanon et
    al. [[232](#bib.bib232)] save computation on easy input data via early exiting;
    later layers will be skipped if the intermediate feature’s prediction confidence
    passes a predefined threshold. Fuengfusin et al. [[86](#bib.bib86)] train models
    whose layers can be removed at inference time without significantly reducing the
    performance to allow adaptive accuracy-latency trade-off. Similarly, Yu et al. [[259](#bib.bib259)]
    train models which are executable at customizable widths (the number of channels
    in a convolutional layer). Xiong et al. [[253](#bib.bib253)] sparsely activate
    convolutional kernels within each layer for each particular input sample, which
    provides an example of the conditional composition of overlapping modules.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 条件组合方案还有其他形式。例如，Teerapittayanon等人[[232](#bib.bib232)]通过早期退出在简单输入数据上节省计算；如果中间特征的预测置信度超过预定义阈值，则会跳过后续层。Fuengfusin等人[[86](#bib.bib86)]训练可以在推理时移除层而不会显著降低性能的模型，以实现自适应的准确度与延迟权衡。类似地，Yu等人[[259](#bib.bib259)]训练可在可定制宽度（卷积层中的通道数）下执行的模型。Xiong等人[[253](#bib.bib253)]在每个特定输入样本的每层中稀疏激活卷积核，这提供了重叠模块条件组合的一个例子。
- en: 4.4 Conclusion of model modularity
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 模型模块化的结论
- en: 'Section [4](#S4 "4 Model modularity ‣ Modularity in Deep Learning: A Survey")
    presents how the notion of modularity is instantiated in the architecture of neural
    network systems. The structure of neural network modules (Section [4.2](#S4.SS2
    "4.2 Typical modules in deep learning models ‣ 4 Model modularity ‣ Modularity
    in Deep Learning: A Survey")) and the way to organize the modules (Section [4.3](#S4.SS3
    "4.3 Composition of modules ‣ 4 Model modularity ‣ Modularity in Deep Learning:
    A Survey")) provide a complementary view of model modularity.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '第[4](#S4 "4 Model modularity ‣ Modularity in Deep Learning: A Survey")节介绍了模块化概念如何在神经网络系统的架构中体现。神经网络模块的结构（第[4.2](#S4.SS2
    "4.2 Typical modules in deep learning models ‣ 4 Model modularity ‣ Modularity
    in Deep Learning: A Survey")节）和模块组织方式（第[4.3](#S4.SS3 "4.3 Composition of modules
    ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey")节）提供了对模型模块化的补充视角。'
- en: 'While all modern neural networks are modular to some extent, different instantiations
    of the modularity principle offer different advantages (Section [4.1](#S4.SS1
    "4.1 Advantages of model modularity ‣ 4 Model modularity ‣ Modularity in Deep
    Learning: A Survey")). The advantages include ease of conceptual design and implementation,
    ease of expert knowledge integration, better interpretability, ease of knowledge
    transfer and reuse, better generalization and sample efficiency, ease of knowledge
    retention, ease of troubleshooting, and better scalability.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然所有现代神经网络在某种程度上都是模块化的，但不同的模块化原则实现提供了不同的优势（第[4.1](#S4.SS1 "4.1 Advantages of
    model modularity ‣ 4 Model modularity ‣ Modularity in Deep Learning: A Survey")节）。这些优势包括概念设计和实现的简便性、专家知识的集成、较好的可解释性、知识转移和重用的便利性、较好的泛化能力和样本效率、知识保留的便利性、故障排除的简便性以及更好的可扩展性。'
- en: 5 Other notions of modularity
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 其他模块化概念
- en: There remain some other notions of modularity in the deep learning literature.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习文献中仍存在其他模块化概念。
- en: 'In graph theory, the term “modularity” refers to a measure commonly used in
    community detection. It measures the density of connections within a community
    (module) compared to between modules communities [[170](#bib.bib170)]. This measure
    can be applied to graph clustering problems in the form of modularity optimization [[32](#bib.bib32),
    [110](#bib.bib110), [203](#bib.bib203), [212](#bib.bib212)]. Inspired by this
    measure, Filan et al. [[78](#bib.bib78)] investigate the parameter clustering
    pattern that emerged from the training of a neural network. They view a neural
    network as an undirected weighted graph (edge weights are the absolute value of
    network parameters) and apply spectral clustering on the obtained graph. They
    observe that some neural networks trained on image classification tasks have some
    clustering properties of their parameters: edge weights are stronger within one
    cluster than between clusters. Watanabe et al. [[245](#bib.bib245)] have obtained
    similar results. Béna et al. [[26](#bib.bib26)] adapted the graph-theoretic modularity
    measure to define structural modularity and define functional specialization through
    three heuristic measures. The functional specialization can be intuitively understood
    as the extent to which a sub-network can do a sub-task independently. To investigate
    the relationship between structural and functional modularity, they design a scenario
    where a model with two parallel modules (with an adjustable number of interconnections)
    is used to predict whether the parity of the two digits is the same or different.
    They show that enforcing structural modularity via sparse connectivity between
    two communicating modules does lead to functional specialization of the modules.
    However, this phenomenon only happens at extreme levels of sparsity. With even
    a moderate number of interconnections, the modules become functionally entangled.
    Mittal et al. [[164](#bib.bib164)] observed that modular systems (weighted combination
    of parallel modules) with a good module specialization are good in terms of the
    overall system performance, however end-to-end training itself is not enough to
    achieve a good module specialization.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在图论中，“模块性”这一术语指的是在社区检测中常用的一个度量。它测量一个社区（模块）内的连接密度与社区间的连接密度相比[[170](#bib.bib170)]。这一度量可以应用于图聚类问题，通过模块性优化的形式[[32](#bib.bib32),
    [110](#bib.bib110), [203](#bib.bib203), [212](#bib.bib212)]。受这一度量的启发，Filan 等人[[78](#bib.bib78)]研究了从神经网络训练中出现的参数聚类模式。他们将神经网络视为一个无向加权图（边的权重是网络参数的绝对值），并对获得的图进行谱聚类。他们观察到，一些在图像分类任务上训练的神经网络具有其参数的某些聚类属性：边权重在一个聚类内比在聚类之间更强。Watanabe
    等人[[245](#bib.bib245)]也获得了类似的结果。Béna 等人[[26](#bib.bib26)]将图论中的模块性度量调整为定义结构模块性，并通过三种启发式度量定义功能专业化。功能专业化可以直观地理解为子网络在独立执行子任务的能力。为了研究结构模块性与功能模块性之间的关系，他们设计了一个场景，其中一个具有两个并行模块（具有可调节的连接数）的模型用于预测两个数字的奇偶性是否相同。他们展示了通过在两个通信模块之间施加稀疏连接来强制结构模块性确实会导致模块的功能专业化。然而，这种现象只发生在极端的稀疏水平下。即使在中等数量的连接下，模块也会变得功能上纠缠。Mittal
    等人[[164](#bib.bib164)]观察到，具有良好模块专业化的模块化系统（并行模块的加权组合）在整体系统性能方面表现良好，但端到端训练本身不足以实现良好的模块专业化。
- en: '![Refer to caption](img/1c4dbd07c041a90d98ce130b5d83b7a1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1c4dbd07c041a90d98ce130b5d83b7a1.png)'
- en: 'Figure 8: Illustration of a disentangled representation.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：解缠表示的示意图。
- en: 'The term “modularity” is related to the notion of independence in some literature.
    For example, Galanti et al. [[87](#bib.bib87)] use modularity to refer to the
    ability of hypernetworks [[99](#bib.bib99)] to learn a different function for
    each input instance. A line of research has been carried out on learning disentangled
    representation. Intuitively, disentangled representation aims to reverse the underlying
    data generating process and retrieve its latent factors into the learned representation
    (Figure [8](#S5.F8 "Figure 8 ‣ 5 Other notions of modularity ‣ Modularity in Deep
    Learning: A Survey")). One of the desirable properties of a disentangled representation [[194](#bib.bib194),
    [69](#bib.bib69), [263](#bib.bib263)] is “modularity”. In this context, a modular
    representation is a representation where each dimension of the representation
    conveys information about at most one latent generative factor.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '“模块化”这一术语在某些文献中与独立性概念相关。例如，Galanti等人[[87](#bib.bib87)]使用模块化来指代超网络[[99](#bib.bib99)]为每个输入实例学习不同功能的能力。一系列关于学习解缠表示的研究已经开展。从直观上讲，解缠表示旨在逆转数据生成过程，并将其潜在因素提取到学习到的表示中（图[8](#S5.F8
    "Figure 8 ‣ 5 Other notions of modularity ‣ Modularity in Deep Learning: A Survey")）。解缠表示的一个理想属性[[194](#bib.bib194),
    [69](#bib.bib69), [263](#bib.bib263)]是“模块化”。在这种背景下，模块化表示是指每个表示维度传达最多一个潜在生成因素的信息。'
- en: 6 Discussion
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: 'Defining modularity is, in itself, a challenging problem. The notion of modularity
    is present in literature across many different fields [[29](#bib.bib29), [241](#bib.bib241),
    [80](#bib.bib80), [195](#bib.bib195), [22](#bib.bib22), [58](#bib.bib58), [84](#bib.bib84),
    [59](#bib.bib59), [81](#bib.bib81), [186](#bib.bib186), [140](#bib.bib140), [180](#bib.bib180),
    [217](#bib.bib217), [218](#bib.bib218), [14](#bib.bib14), [31](#bib.bib31), [177](#bib.bib177),
    [91](#bib.bib91), [208](#bib.bib208), [167](#bib.bib167), [55](#bib.bib55), [17](#bib.bib17),
    [83](#bib.bib83), [170](#bib.bib170), [168](#bib.bib168), [182](#bib.bib182),
    [93](#bib.bib93)]. While many researchers have a strong intuition about what it
    means for an entity to be modular, there has yet to be a universal agreement on
    what defines modularity. The same is true even within the field of deep learning.
    As rightly said by Béna et al. [[26](#bib.bib26)]: “Modularity of neural networks
    is a bit like the notion of beauty in art: everyone agrees that it’s important,
    but nobody can say exactly what it means”. We argue that the difficulty of defining
    modularity stems from the fact that the notion of modularity usually comes with
    many different properties: replaceability of modules, combinability of modules,
    reusability of modules, autonomy of modules (limited interaction or limited interdependence
    between modules), functional specialization of modules. Authors from different
    fields typically only retain one or two of the above properties to claim an entity
    to be modular.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模块化本身就是一个具有挑战性的问题。模块化的概念在多个不同领域的文献中都有出现[[29](#bib.bib29), [241](#bib.bib241),
    [80](#bib.bib80), [195](#bib.bib195), [22](#bib.bib22), [58](#bib.bib58), [84](#bib.bib84),
    [59](#bib.bib59), [81](#bib.bib81), [186](#bib.bib186), [140](#bib.bib140), [180](#bib.bib180),
    [217](#bib.bib217), [218](#bib.bib218), [14](#bib.bib14), [31](#bib.bib31), [177](#bib.bib177),
    [91](#bib.bib91), [208](#bib.bib208), [167](#bib.bib167), [55](#bib.bib55), [17](#bib.bib17),
    [83](#bib.bib83), [170](#bib.bib170), [168](#bib.bib168), [182](#bib.bib182),
    [93](#bib.bib93)]。虽然许多研究人员对实体的模块化有很强的直觉，但尚未达成关于模块化定义的普遍共识。即便在深度学习领域也是如此。正如Béna等人[[26](#bib.bib26)]所说：“神经网络的模块化有点像艺术中的美学：大家都同意它很重要，但没有人能准确地说清楚它的含义。”我们认为，定义模块化的困难在于，模块化的概念通常伴随着许多不同的属性：模块的可替换性、模块的可组合性、模块的可重用性、模块的自主性（模块间有限的互动或有限的相互依赖）、模块的功能专门化。不同领域的作者通常只保留上述一个或两个属性来声称一个实体是模块化的。
- en: 'In this survey, we define modularity as the property of an entity whereby it
    can be broken down into a number of sub-entities (referred to as modules). This
    definition is the prerequisite of the properties mentioned above; it is the greatest
    common definition of the notion of modularity. By recursively applying the definition
    of modularity, a modular entity is an entity that can be broken down into sub-entities,
    where each sub-entity can be further broken down into sub-sub-entities. This recursion
    can be repeated for discrete entities until the atomic elements (minimum indivisible
    modules) are reached. In that case, a set of atomic elements $\{a\in D\}$ can
    formally characterize a discrete entity $D$; a subset of atomic elements can then
    characterize a module $M\subseteq D$. The above framework applies to data modularity
    (Section [2](#S2 "2 Data modularity ‣ Modularity in Deep Learning: A Survey"))
    and model modularity (Section [4](#S4 "4 Model modularity ‣ Modularity in Deep
    Learning: A Survey")). The reason is that data and models are both discrete: data
    samples and model parameters are stored in physical computers where everything
    is represented quantitatively. On the other hand, we need to use a different framework
    for task modularity because tasks are usually not discrete. As discussed in Section [3](#S3
    "3 Task modularity ‣ Modularity in Deep Learning: A Survey"), each task can be
    characterized by an objective function $F$. In this sense, task modularity can
    be formally characterized by (objective) function compositions. A task is decomposable
    if there exists a set of functions $\{f_{1},f_{2},...\}$ that, when composed together,
    retrieve the form of the original objective function $F$.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项调查中，我们将模块化定义为一个实体的属性，即它可以被分解成多个子实体（称为模块）。这个定义是上述属性的前提条件；它是模块化概念的最通用定义。通过递归应用模块化定义，一个模块化实体是一个可以被分解成子实体的实体，每个子实体还可以进一步分解成子子实体。这种递归可以对离散实体重复进行，直到达到原子元素（最小不可分模块）。在这种情况下，一组原子元素
    $\{a\in D\}$ 可以正式地表征一个离散实体 $D$；然后，原子元素的一个子集可以表征一个模块 $M\subseteq D$。上述框架适用于数据模块化（第[2](#S2
    "2 Data modularity ‣ Modularity in Deep Learning: A Survey")节）和模型模块化（第[4](#S4
    "4 Model modularity ‣ Modularity in Deep Learning: A Survey")节）。原因在于数据和模型都是离散的：数据样本和模型参数存储在物理计算机中，其中一切都以量化形式表示。另一方面，我们需要使用不同的框架来处理任务模块化，因为任务通常不是离散的。如第[3](#S3
    "3 Task modularity ‣ Modularity in Deep Learning: A Survey")节所讨论的，每个任务可以通过目标函数
    $F$ 来表征。从这个意义上讲，任务模块化可以通过（目标）函数组合来正式表征。如果存在一组函数 $\{f_{1},f_{2},...\}$，它们组合在一起能够恢复原始目标函数
    $F$ 的形式，则该任务是可分解的。'
- en: For discrete entities, one needs to choose the atomic elements. Naively, one
    could choose each data sample in a dataset and each neuron in a neural network
    as the atomic elements. However, both choices remain to be discussed because they
    are indeed not the smallest indivisible modules. Regarding data modularity, the
    dataset division can happen both at the sample dimension and the feature dimension,
    which means that each data sample can be divided into smaller elements e.g., feature
    vectors of reduced length or image patches. Regarding model modularity, the modularization
    can happen at the granularity of parameters e.g., modules can be obtained by masking
    out parameters [[61](#bib.bib61), [26](#bib.bib26), [226](#bib.bib226), [134](#bib.bib134)].
    Consequently, one can choose the scalar numbers stored in physical computers (often
    represented by floating-point numbers) as the atomic elements. The atomic elements
    for data are every single dimension of data samples; the atomic elements for models
    are every single scalar parameters in the neural network. It entails that, in
    some cases, there needs to be some relationship $R$ among atomic elements $\{a\in
    D\}$ because any arbitrary subsets of atomic elements do not necessarily form
    a valid module if the relationship $R$ is broken. In the above example, the relationship
    $R$ indicates which scalar numbers should come together to form data samples or
    how to use each scalar parameter along the feedforward computation in the computational
    graph of neural networks. In consequence, an entity can be a set or a system;
    a system is a set equipped with relationships $R$ among atomic elements $\{a\in
    D\}$.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散实体，需要选择原子元素。简单来说，可以选择数据集中的每一个数据样本和神经网络中的每一个神经元作为原子元素。然而，这两种选择仍需讨论，因为它们确实不是最小的不可分割模块。关于数据模块化，数据集的划分可以发生在样本维度和特征维度，这意味着每个数据样本可以被划分为更小的元素，例如，缩短长度的特征向量或图像补丁。关于模型模块化，模块化可以发生在参数的粒度上，例如，通过屏蔽参数可以获得模块[[61](#bib.bib61),
    [26](#bib.bib26), [226](#bib.bib226), [134](#bib.bib134)]。因此，可以选择存储在物理计算机中的标量数（通常表示为浮点数）作为原子元素。数据的原子元素是数据样本的每一个维度；模型的原子元素是神经网络中的每一个标量参数。这意味着，在某些情况下，需要在原子元素$\{a\in
    D\}$之间存在某种关系$R$，因为任何任意的原子元素子集如果关系$R$被破坏，就不一定形成有效的模块。在上述示例中，关系$R$指示了哪些标量数应该组合在一起以形成数据样本，或者如何在神经网络计算图中的前向计算中使用每一个标量参数。因此，一个实体可以是一个集合或系统；一个系统是一个配备了原子元素$\{a\in
    D\}$之间关系$R$的集合。
- en: 7 Future research
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来研究
- en: As modularity is a general principle, this survey covered many elements from
    different sub-fields of deep learning; each sub-field can provide a lot of future
    avenues of research on its own. To name a few, McNeely-White et al. [[159](#bib.bib159)]
    and Bouchacourt et al. [[30](#bib.bib30)] showed that given the same training
    data, learned features exhibit similar properties across models with markedly
    different architectural inductive biases. Is it still worth improving neural network
    architectures if data dominate learning results? Future research may validate
    the results of McNeely-White et al. [[159](#bib.bib159)] and Bouchacourt et al. [[30](#bib.bib30)]
    by extending their research to more kinds of models and training datasets in a
    more systematic way. If these results still hold, one may need to ground these
    results theoretically. On the other hand, whether neural networks can learn and
    behave compositionally is still an open question [[114](#bib.bib114), [11](#bib.bib11)].
    It entails that we need a domain-agnostic way to test the compositionality of
    neural networks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模块化是一个通用原则，本调查涵盖了深度学习不同子领域中的许多元素；每个子领域本身可以提供大量未来研究的途径。举几个例子，McNeely-White等人[[159](#bib.bib159)]和Bouchacourt等人[[30](#bib.bib30)]表明，给定相同的训练数据，学习到的特征在具有明显不同架构归纳偏差的模型中展示出相似的属性。如果数据主导学习结果，那么改进神经网络架构是否仍然值得？未来的研究可以通过以更系统的方式将McNeely-White等人[[159](#bib.bib159)]和Bouchacourt等人[[30](#bib.bib30)]的研究扩展到更多种类的模型和训练数据集来验证这些结果。如果这些结果仍然成立，可能需要从理论上支持这些结果。另一方面，神经网络是否能够以组合的方式进行学习和行为仍然是一个悬而未决的问题[[114](#bib.bib114),
    [11](#bib.bib11)]。这意味着我们需要一种领域无关的方式来测试神经网络的组合性。
- en: Different aspects of the modularity principle can be further investigated to
    improve deep learning models. It boils down to designing new deep learning methods
    that provide e.g., better interpretability, reusability, scalability, and efficiency.
    While model modularity may, to some extent, reflect task modularity, it is still
    unclear whether data modularity directly corresponds with model modularity. One
    research avenue is to automate imposed data modularization regarding specific
    models in the spirit of AutoML. Similarly, automating task modularization can
    facilitate problem-solving and reduce human-introduced bias.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化原则的不同方面可以进一步研究以改进深度学习模型。这归结为设计新的深度学习方法，例如，提供更好的可解释性、可重用性、可扩展性和效率。尽管模型的模块化在某种程度上可能反映了任务的模块化，但尚不清楚数据模块化是否与模型模块化直接对应。一个研究方向是自动化对特定模型的强制性数据模块化，类似于AutoML的精神。类似地，自动化任务模块化可以促进问题解决并减少人为偏差。
- en: 8 Conclusion
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: 'Deep learning is becoming dominant in many applications, such as computer vision
    and natural language processing. It is natural to ask ourselves whether there
    are guidelines for designing deep learning algorithms. Modularity is one guiding
    principle that has been put forward in the literature. This survey reveals that
    modularity is pervasive in three related yet distinct axes of deep learning: data,
    task, and model architecture. We observed that some modularity concepts come in
    the form of a prior, while others come in the form of a posterior.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多应用中正变得主导，例如计算机视觉和自然语言处理。我们自然会问自己是否有设计深度学习算法的指导原则。模块化是文献中提出的一个指导原则。本调查揭示了模块化在深度学习的三个相关但独立的轴心中的普遍存在：数据、任务和模型架构。我们观察到一些模块化概念以先验形式出现，而另一些则以后验形式出现。
- en: The efforts of bringing the modularity principle into deep learning are not
    new; however, reviewing deep learning literature using the point of view of modularity
    is relatively new. This survey provides a step towards clarifying and investigating
    the notion of modularity in deep learning and elsewhere.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 将模块化原则引入深度学习的努力并不新鲜；然而，从模块化的角度审视深度学习文献相对较新。本调查为澄清和研究深度学习及其他领域中的模块化概念迈出了一步。
- en: Acknowledgments
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We gratefully acknowledge constructive feedback and suggestions from Birhanu
    Hailu Belay, Romain Egele, Felix Mohr, Hedi Tabia, and the reviewers. This work
    was supported by ChaLearn and the ANR (Agence Nationale de la Recherche, National
    Agency for Research) under AI chair of excellence HUMANIA, grant number ANR-19-CHIA-0022.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们衷心感谢**Birhanu Hailu Belay**、**Romain Egele**、**Felix Mohr**、**Hedi Tabia**及审稿人提供的建设性反馈和建议。这项工作得到了ChaLearn和ANR（法国国家科研署）在HUMANIA人工智能卓越研究椅项目下的资助，资助编号为ANR-19-CHIA-0022。
- en: References
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Accelerate Fast Math with Intel® oneAPI Math Kernel Library. https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 加速快速数学运算使用 Intel® oneAPI 数学核心库. https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html.'
- en: '[2] Asmaa Abbas, Mohammed Abdelsamea, and Mohamed Gaber. DeTraC: Transfer Learning
    of Class Decomposed Medical Images in Convolutional Neural Networks. IEEE Access,
    PP:1–1, April 2020.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] **Asmaa Abbas**、**Mohammed Abdelsamea** 和 **Mohamed Gaber**. DeTraC: 卷积神经网络中类分解医学图像的迁移学习.
    IEEE Access, PP:1–1, 2020年4月.'
- en: '[3] Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat Khassanov,
    Michael Lewis, and Huseyin Atakan Varol. Speakingfaces: A large-scale multimodal
    dataset of voice commands with visual and thermal video streams. Sensors, 21(10):3465,
    2021.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] **Madina Abdrakhmanova**、**Askat Kuzdeuov**、**Sheikh Jarju**、**Yerbolat
    Khassanov**、**Michael Lewis** 和 **Huseyin Atakan Varol**. Speakingfaces: 一个大规模多模态数据集，包含语音命令的视觉和热成像视频流.
    传感器, 21(10):3465, 2021.'
- en: '[4] Wickliffe C. Abraham and Anthony Robins. Memory retention – the synaptic
    stability versus plasticity dilemma. Trends in Neurosciences, 28(2):73–78, February
    2005.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] **Wickliffe C. Abraham** 和 **Anthony Robins**. 记忆保持——突触稳定性与可塑性困境. 神经科学趋势,
    28(2):73–78, 2005年2月.'
- en: '[5] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using
    linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] **Guillaume Alain** 和 **Yoshua Bengio**. 使用线性分类器探针理解中间层. arXiv 预印本 arXiv:1610.01644,
    2016.'
- en: '[6] Ferran Alet, Tomás Lozano-Pérez, and Leslie P. Kaelbling. Modular meta-learning.
    arXiv:1806.10166 [cs, stat], May 2019.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] **Ferran Alet**、**Tomás Lozano-Pérez** 和 **Leslie P. Kaelbling**. 模块化元学习.
    arXiv:1806.10166 [cs, stat], 2019年5月.'
- en: '[7] Anirudh Goyal ALIAS PARTH GOYAL, Aniket Didolkar, Nan Rosemary Ke, Charles
    Blundell, Philippe Beaudoin, Nicolas Heess, Michael C Mozer, and Yoshua Bengio.
    Neural production systems. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,
    and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems,
    volume 34, pages 25673–25687\. Curran Associates, Inc., 2021.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Anirudh Goyal ALIAS PARTH GOYAL, Aniket Didolkar, Nan Rosemary Ke, Charles
    Blundell, Philippe Beaudoin, Nicolas Heess, Michael C Mozer 和 Yoshua Bengio. 神经生成系统.
    在 M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang 和 J. Wortman Vaughan 编辑的《神经信息处理系统进展》第34卷中,
    页码25673–25687. Curran Associates, Inc., 2021年。'
- en: '[8] Felipe Almeida and Geraldo Xexéo. Word Embeddings: A Survey, January 2019.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Felipe Almeida 和 Geraldo Xexéo. 词嵌入：综述, 2019年1月。'
- en: '[9] Mohammed Amer and Tomas Maul. A Review of Modularization Techniques in
    Artificial Neural Networks. Artificial Intelligence Review, 52, June 2019.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Mohammed Amer 和 Tomas Maul. 人工神经网络模块化技术综述. 人工智能评论, 52, 2019年6月。'
- en: '[10] Ark Anderson, Kyle Shaffer, Artem Yankov, Court D. Corley, and Nathan O.
    Hodas. Beyond Fine Tuning: A Modular Approach to Learning on Small Data, November
    2016.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ark Anderson, Kyle Shaffer, Artem Yankov, Court D. Corley 和 Nathan O.
    Hodas. 超越微调：小数据上的模块化学习方法, 2016年11月。'
- en: '[11] Jacob Andreas. Measuring compositionality in representation learning.
    In International Conference on Learning Representations, 2019.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jacob Andreas. 衡量表征学习中的组合性. 在国际学习表征会议上, 2019年。'
- en: '[12] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural
    Module Networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pages 39–48, Las Vegas, NV, USA, June 2016. IEEE.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Jacob Andreas, Marcus Rohrbach, Trevor Darrell 和 Dan Klein. 神经模块网络. 在2016年IEEE计算机视觉与模式识别会议(CVPR)上,
    页码39–48, 拉斯维加斯, NV, USA, 2016年6月。IEEE。'
- en: '[13] G. Auda and M. Kamel. Modular neural networks a survey. International
    journal of neural systems, 9 2:129–51, 1999.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] G. Auda 和 M. Kamel. 模块化神经网络综述. 国际神经系统学杂志, 9 2:129–51, 1999年。'
- en: '[14] Jeremy Avigad. Modularity in mathematics. The Review of Symbolic Logic,
    13(1):47–79, March 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jeremy Avigad. 数学中的模块化. 符号逻辑评论, 13(1):47–79, 2020年3月。'
- en: '[15] Farooq Azam. Biologically Inspired Modular Neural Networks. PhD thesis,
    Virginia Tech, May 2000.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Farooq Azam. 生物启发的模块化神经网络. 博士论文, 维吉尼亚理工大学, 2000年5月。'
- en: '[16] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen,
    Harm de Vries, and Aaron Courville. Systematic generalization: What is required
    and can it be learned? In International Conference on Learning Representations,
    2019.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen,
    Harm de Vries 和 Aaron Courville. 系统性泛化：需要什么，能否学习？在国际学习表征会议上, 2019年。'
- en: '[17] Carliss Y. Baldwin and Kim B. Clark. Design Rules: The Power of Modularity,
    volume 1. Cambridge, MA: MIT Press, first edition, 1999.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Carliss Y. Baldwin 和 Kim B. Clark. 设计规则：模块化的力量，第1卷. 剑桥, MA: MIT出版社, 第1版,
    1999年。'
- en: '[18] Randall Balestriero and Yann LeCun. POLICE: Provably Optimal Linear Constraint
    Enforcement for Deep Neural Networks, November 2022.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Randall Balestriero 和 Yann LeCun. POLICE: 对深度神经网络的可证明最优线性约束执行, 2022年11月。'
- en: '[19] Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal
    machine learning: A survey and taxonomy. IEEE transactions on pattern analysis
    and machine intelligence, 41(2):423–443, 2018.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Tadas Baltrušaitis, Chaitanya Ahuja 和 Louis-Philippe Morency. 多模态机器学习：综述与分类.
    IEEE模式分析与机器智能学报, 41(2):423–443, 2018年。'
- en: '[20] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT Pre-Training
    of Image Transformers. In International Conference on Learning Representations,
    2022.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Hangbo Bao, Li Dong, Songhao Piao 和 Furu Wei. BEiT: BERT预训练图像变换器. 在国际学习表征会议上,
    2022年。'
- en: '[21] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand,
    Dan Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta,
    Parker Schuh, Ryan Sepassi, Laurent El Shafey, Chandramohan A. Thekkath, and Yonghui
    Wu. Pathways: Asynchronous Distributed Dataflow for ML. arXiv:2203.12533 [cs],
    March 2022.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand,
    Dan Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta,
    Parker Schuh, Ryan Sepassi, Laurent El Shafey, Chandramohan A. Thekkath 和 Yonghui
    Wu. Pathways: 用于机器学习的异步分布式数据流. arXiv:2203.12533 [cs], 2022年3月。'
- en: '[22] H. Clark Barrett and Robert Kurzban. Modularity in cognition: Framing
    the debate. Psychological Review, 113(3):628–647, July 2006.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Clark Barrett 和 Robert Kurzban. 认知中的模块化：框架辩论. 心理学评论, 113(3):628–647,
    2006年7月。'
- en: '[23] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez,
    Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro,
    Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George
    Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer,
    Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia
    Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph
    networks. arXiv:1806.01261 [cs, stat], October 2018.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez,
    Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro,
    Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George
    Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer,
    Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia
    Li 和 Razvan Pascanu. 关系性归纳偏置、深度学习与图网络。arXiv:1806.01261 [cs, stat]，2018年10月。'
- en: '[24] Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen AJ Eppenhof, Josien PW
    Pluim, and Remco Duits. Roto-Translation Covariant Convolutional Networks for
    Medical Image Analysis. arXiv:1804.03393 [cs, math], June 2018.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen AJ Eppenhof, Josien
    PW Pluim 和 Remco Duits. 医学图像分析中的旋转-平移协变卷积网络。arXiv:1804.03393 [cs, math]，2018年6月。'
- en: '[25] Birhanu Belay, Tewodros Habtegebrial, Marcus Liwicki, Gebeyehu Belay,
    and Didier Stricker. Factored Convolutional Neural Network for Amharic Character
    Image Recognition. In 2019 IEEE International Conference on Image Processing (ICIP),
    pages 2906–2910, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Birhanu Belay, Tewodros Habtegebrial, Marcus Liwicki, Gebeyehu Belay 和
    Didier Stricker. 用于阿姆哈拉字符图像识别的分解卷积神经网络。见于2019年IEEE国际图像处理大会（ICIP），第2906–2910页，2019年。'
- en: '[26] Gabriel Béna and Dan F. M. Goodman. Extreme sparsity gives rise to functional
    specialization. arXiv:2106.02626 [cs, q-bio], June 2021.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Gabriel Béna 和 Dan F. M. Goodman. 极端稀疏性引发功能专门化。arXiv:2106.02626 [cs, q-bio]，2021年6月。'
- en: '[27] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional
    Computation in Neural Networks for faster models. arXiv:1511.06297 [cs], January
    2016.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau 和 Doina Precup. 神经网络中的条件计算以加快模型速度。arXiv:1511.06297
    [cs]，2016年1月。'
- en: '[28] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or Propagating
    Gradients Through Stochastic Neurons for Conditional Computation. arXiv:1308.3432
    [cs], August 2013.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yoshua Bengio, Nicholas Léonard 和 Aaron Courville. 通过随机神经元估计或传播梯度以进行条件计算。arXiv:1308.3432
    [cs]，2013年8月。'
- en: '[29] J. Bongard. Evolving modular genetic regulatory networks. In Proceedings
    of the 2002 Congress on Evolutionary Computation. CEC’02 (Cat. No.02TH8600), volume 2,
    pages 1872–1877 vol.2, May 2002.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Bongard. 进化模块化遗传调控网络。见于2002年进化计算大会论文集。CEC’02 (Cat. No.02TH8600)，第2卷，第1872–1877页，2002年5月。'
- en: '[30] Diane Bouchacourt, Mark Ibrahim, and Ari Morcos. Grounding inductive biases
    in natural images: Invariance stems from variations in data. In M. Ranzato, A. Beygelzimer,
    Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information
    Processing Systems, volume 34, pages 19566–19579\. Curran Associates, Inc., 2021.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Diane Bouchacourt, Mark Ibrahim 和 Ari Morcos. 在自然图像中实现归纳偏置：不变性源于数据的变化。见于M.
    Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang 和 J. Wortman Vaughan主编的《神经信息处理系统进展》，第34卷，第19566–19579页。Curran
    Associates, Inc.，2021年。'
- en: '[31] Nicholas Bourbaki. The Architecture of Mathematics. The American Mathematical
    Monthly, 57(4):221–232, April 1950.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Nicholas Bourbaki. 数学的架构。《美国数学月刊》，57(4):221–232，1950年4月。'
- en: '[32] Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer,
    Zoran Nikoloski, and Dorothea Wagner. On modularity clustering. IEEE transactions
    on knowledge and data engineering, 20(2):172–188, 2007.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer,
    Zoran Nikoloski 和 Dorothea Wagner. 关于模块化聚类。IEEE知识与数据工程学报，20(2):172–188，2007年。'
- en: '[33] Alexander Braylan, Mark Hollenbeck, Elliot Meyerson, and Risto Miikkulainen.
    Reuse of neural modules for general video game playing. In Proceedings of the
    AAAI Conference on Artificial Intelligence, volume 30, 2016.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Alexander Braylan, Mark Hollenbeck, Elliot Meyerson 和 Risto Miikkulainen.
    重新使用神经模块进行通用视频游戏玩法。见于AAAI人工智能会议论文集，第30卷，2016年。'
- en: '[34] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak
    Shah. Signature Verification using a ”Siamese” Time Delay Neural Network. In J. Cowan,
    G. Tesauro, and J. Alspector, editors, Advances in Neural Information Processing
    Systems, volume 6\. Morgan-Kaufmann, 1994.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger 和 Roopak Shah.
    使用“Siamese”时间延迟神经网络进行签名验证。见于J. Cowan, G. Tesauro 和 J. Alspector主编的《神经信息处理系统进展》，第6卷。Morgan-Kaufmann，1994年。'
- en: '[35] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
    models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
    and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
    pages 1877–1901\. Curran Associates, Inc., 2020.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario Amodei. 语言模型是少样本学习者.
    在 H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, 和 H. Lin 编辑的《神经信息处理系统进展》中,
    第33卷, 页码 1877–1901. Curran Associates, Inc., 2020年.'
- en: '[36] Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, and Niranjan Balasubramanian.
    DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering.
    In Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pages 4487–4497, Online, July 2020\. Association for Computational
    Linguistics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 曹青青, Harsh Trivedi, Aruna Balasubramanian, 和 Niranjan Balasubramanian.
    DeFormer：分解预训练变换器以加快问答速度. 在第58届计算语言学协会年会上, 页码 4487–4497, 在线, 2020年7月. 计算语言学协会.'
- en: '[37] Richard G Casey and Eric Lecolinet. A survey of methods and strategies
    in character segmentation. IEEE transactions on pattern analysis and machine intelligence,
    18(7):690–706, 1996.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Richard G Casey 和 Eric Lecolinet. 字符分割方法和策略综述. IEEE模式分析与机器智能学报, 18(7):690–706,
    1996年.'
- en: '[38] Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths.
    Automatically composing representation transformations as a means for generalization.
    In International Conference on Learning Representations, 2019.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Michael Chang, Abhishek Gupta, Sergey Levine, 和 Thomas L. Griffiths. 自动组合表示变换以实现泛化.
    在国际表示学习大会上, 2019年.'
- en: '[39] Michael Chang, Sid Kaushik, Sergey Levine, and Tom Griffiths. Modularity
    in Reinforcement Learning via Algorithmic Independence in Credit Assignment. In
    International Conference on Machine Learning, pages 1452–1462\. PMLR, July 2021.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Michael Chang, Sid Kaushik, Sergey Levine, 和 Tom Griffiths. 通过算法独立性在信用分配中的模块化.
    在国际机器学习大会上, 页码 1452–1462. PMLR, 2021年7月.'
- en: '[40] Shuxiao Chen, Edgar Dobriban, and Jane Lee. A group-theoretic framework
    for data augmentation. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
    and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
    pages 21321–21333\. Curran Associates, Inc., 2020.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] 陈书孝, Edgar Dobriban, 和 Jane Lee. 数据增强的群体理论框架. 在 H. Larochelle, M. Ranzato,
    R. Hadsell, M.F. Balcan, 和 H. Lin 编辑的《神经信息处理系统进展》中, 第33卷, 页码 21321–21333. Curran
    Associates, Inc., 2020年.'
- en: '[41] Xiaoxue Chen, Lianwen Jin, Yuanzhi Zhu, Canjie Luo, and Tianwei Wang.
    Text Recognition in the Wild: A Survey. arXiv:2005.03492 [cs], December 2020.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] 陈小雪, 廉文金, 朱元志, 罗灿杰, 和 王天伟. 野外文本识别：综述. arXiv:2005.03492 [cs], 2020年12月.'
- en: '[42] Xinlei Chen and Kaiming He. Exploring Simple Siamese Representation Learning.
    In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
    pages 15745–15753, Nashville, TN, USA, June 2021\. IEEE.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] 陈欣磊 和 何恺明. 探索简单的孪生网络表示学习. 在2021年IEEE/CVF计算机视觉与模式识别大会（CVPR）上, 页码 15745–15753,
    美国田纳西州纳什维尔, 2021年6月. IEEE.'
- en: '[43] Yutian Chen, Abram L Friesen, Feryal Behbahani, Arnaud Doucet, David Budden,
    Matthew Hoffman, and Nando de Freitas. Modular meta-learning with shrinkage. Advances
    in Neural Information Processing Systems, 33:2858–2869, 2020.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 陈玉田, Abram L Friesen, Feryal Behbahani, Arnaud Doucet, David Budden, Matthew
    Hoffman, 和 Nando de Freitas. 模块化元学习与收缩. 神经信息处理系统进展, 33:2858–2869, 2020.'
- en: '[44] Guillaume Chevalier. Long short-term memory (LSTM cell). Wikipedia, September
    2022.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Guillaume Chevalier. 长短期记忆（LSTM单元）. 维基百科, 2022年9月.'
- en: '[45] Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.
    On the properties of neural machine translation: Encoder–Decoder approaches. Syntax,
    Semantics and Structure in Statistical Translation, page 103, 2014.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Cho Kyunghyun, Bart van Merriënboer, Dzmitry Bahdanau, 和 Yoshua Bengio.
    神经机器翻译的性质：编码器-解码器方法. 统计翻译中的句法、语义和结构, 页码 103, 2014年.'
- en: '[46] François Chollet. Xception: Deep learning with depthwise separable convolutions.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1251–1258, 2017.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] François Chollet. Xception: 通过深度可分卷积进行深度学习。在IEEE计算机视觉与模式识别会议论文集中，页码1251–1258，2017年。'
- en: '[47] Noam Chomsky. Aspects of the Theory of Syntax. MIT Press, 1965.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Noam Chomsky. 句法理论的若干方面。MIT出版社，1965年。'
- en: '[48] Amit Choudhary, Rahul Rishi, and Savita Ahlawat. A new character segmentation
    approach for off-line cursive handwritten words. Procedia Computer Science, 17:88–95,
    2013.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Amit Choudhary、Rahul Rishi 和 Savita Ahlawat. 一种用于离线草书手写词的新字符分割方法。Procedia
    Computer Science, 17:88–95, 2013年。'
- en: '[49] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek
    Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif,
    Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,
    Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa
    Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,
    Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling Language Modeling
    with Pathways. arXiv:2204.02311 [cs], April 2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav Mishra、Adam
    Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann、Parker Schuh、Kensen
    Shi、Sasha Tsvyashchenko、Joshua Maynez、Abhishek Rao、Parker Barnes、Yi Tay、Noam Shazeer、Vinodkumar
    Prabhakaran、Emily Reif、Nan Du、Ben Hutchinson、Reiner Pope、James Bradbury、Jacob
    Austin、Michael Isard、Guy Gur-Ari、Pengcheng Yin、Toju Duke、Anselm Levskaya、Sanjay
    Ghemawat、Sunipa Dev、Henryk Michalewski、Xavier Garcia、Vedant Misra、Kevin Robinson、Liam
    Fedus、Denny Zhou、Daphne Ippolito、David Luan、Hyeontaek Lim、Barret Zoph、Alexander
    Spiridonov、Ryan Sepassi、David Dohan、Shivani Agrawal、Mark Omernick、Andrew M. Dai、Thanumalayan
    Sankaranarayana Pillai、Marie Pellat、Aitor Lewkowycz、Erica Moreira、Rewon Child、Oleksandr
    Polozov、Katherine Lee、Zongwei Zhou、Xuezhi Wang、Brennan Saeta、Mark Diaz、Orhan Firat、Michele
    Catasta、Jason Wei、Kathy Meier-Hellstern、Douglas Eck、Jeff Dean、Slav Petrov 和 Noah
    Fiedel. PaLM: 通过路径扩展语言建模。arXiv:2204.02311 [cs]，2022年4月。'
- en: '[50] Brian Chu, Vashisht Madhavan, Oscar Beijbom, Judy Hoffman, and Trevor
    Darrell. Best practices for fine-tuning visual classifiers to new domains. In
    European Conference on Computer Vision, pages 435–442. Springer, 2016.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Brian Chu、Vashisht Madhavan、Oscar Beijbom、Judy Hoffman 和 Trevor Darrell.
    视觉分类器微调到新领域的最佳实践。在欧洲计算机视觉会议上，页码435–442。Springer，2016年。'
- en: '[51] Ignasi Clavera, David Held, and Pieter Abbeel. Policy transfer via modularity
    and reward guiding. In 2017 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), pages 1537–1544\. IEEE, 2017.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Ignasi Clavera、David Held 和 Pieter Abbeel. 通过模块化和奖励引导进行策略迁移。在2017 IEEE/RSJ国际智能机器人与系统会议（IROS）上，页码1537–1544。IEEE，2017年。'
- en: '[52] Jeff Clune, Jean-Baptiste Mouret, and Hod Lipson. The evolutionary origins
    of modularity. Proceedings of the Royal Society b: Biological sciences, 280(1755):20122863,
    2013.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Jeff Clune、Jean-Baptiste Mouret 和 Hod Lipson. 模块化的进化起源。皇家学会B期刊：生物科学，280(1755):20122863，2013年。'
- en: '[53] Taco S. Cohen and Max Welling. Group Equivariant Convolutional Networks.
    arXiv:1602.07576 [cs, stat], June 2016.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Taco S. Cohen 和 Max Welling. 群体等变卷积网络。arXiv:1602.07576 [cs, stat]，2016年6月。'
- en: '[54] Taco S. Cohen and Max Welling. Steerable CNNs. arXiv:1612.08498 [cs, stat],
    December 2016.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Taco S. Cohen 和 Max Welling. 可引导卷积神经网络。arXiv:1612.08498 [cs, stat]，2016年12月。'
- en: '[55] Sarah Cohen-Boulakia, Khalid Belhajjame, Olivier Collin, Jérôme Chopard,
    Christine Froidevaux, Alban Gaignard, Konrad Hinsen, Pierre Larmande, Yvan Le Bras,
    Frédéric Lemoine, et al. Scientific workflows for computational reproducibility
    in the life sciences: Status, challenges and opportunities. Future Generation
    Computer Systems, 75:284–298, 2017.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Sarah Cohen-Boulakia、Khalid Belhajjame、Olivier Collin、Jérôme Chopard、Christine
    Froidevaux、Alban Gaignard、Konrad Hinsen、Pierre Larmande、Yvan Le Bras、Frédéric
    Lemoine 等。生命科学中的科学工作流：计算可重复性的现状、挑战与机遇。Future Generation Computer Systems, 75:284–298,
    2017年。'
- en: '[56] Cédric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and
    Pierre-Yves Oudeyer. Curious: Intrinsically motivated modular multi-goal reinforcement
    learning. In International Conference on Machine Learning, pages 1331–1340\. PMLR,
    2019.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Cédric Colas、Pierre Fournier、Mohamed Chetouani、Olivier Sigaud 和 Pierre-Yves
    Oudeyer. Curious: 内在驱动的模块化多目标强化学习。在国际机器学习会议上，页码1331–1340。PMLR，2019年。'
- en: '[57] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein.
    Introduction to Algorithms. 2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 托马斯·H·科门、查尔斯·E·莱瑟森、罗纳德·L·里维斯特和克利福德·斯坦。算法导论。2022年。'
- en: '[58] Leda Cosmides and John Tooby. Cognitive Adaptations for Social Exchange.
    undefined, pages 163–228, 1992.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 莱达·科斯米迪斯和约翰·图比。社会交换的认知适应。未定义，第163–228页，1992年。'
- en: '[59] Leda Cosmides and John Tooby. Origins of domain specificity: The evolution
    of functional organization. In Lawrence A. Hirschfeld and Susan A. Gelman, editors,
    Mapping the Mind: Domain Specificity in Cognition and Culture, pages 85–116\.
    Cambridge University Press, Cambridge, 1994.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 莱达·科斯米迪斯和约翰·图比。领域特异性的起源：功能组织的进化。见劳伦斯·A·赫希费尔德和苏珊·A·盖尔曼主编，《思维的映射：认知与文化中的领域特异性》，第85–116页，剑桥大学出版社，剑桥，1994年。'
- en: '[60] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. CTL++: Evaluating
    Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility
    of Neural Representations. In Proc. Conf. on Empirical Methods in Natural Language
    Processing (EMNLP), December 2022.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 罗伯特·楚尔达什、伊赖·和泉和于尔根·施密特胡贝尔。CTL++：评估从未见过的已知函数组合模式的泛化性及神经表征的兼容性。见自然语言处理经验方法会议（EMNLP），2022年12月。'
- en: '[61] Róbert Csordás, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Are Neural
    Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks.
    In International Conference on Learning Representations, 2021.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 罗伯特·楚尔达什、斯乔德·范·斯廷基斯特和于尔根·施密特胡贝尔。神经网络是否模块化？通过可微权重掩码检查功能模块化。见国际学习表征会议，2021年。'
- en: '[62] Vanessa D’Amario, Tomotake Sasaki, and Xavier Boix. How Modular should
    Neural Module Networks Be for Systematic Generalization? In Thirty-Fifth Conference
    on Neural Information Processing Systems, 2021.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 瓦妮莎·达马里奥、佐佐木智武和哈维尔·博伊克斯。神经模块网络在系统化泛化中应具备多少模块化？见第35届神经信息处理系统会议，2021年。'
- en: '[63] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
    ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on
    Computer Vision and Pattern Recognition, pages 248–255, 2009.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 姚邓、魏东、理查德·索彻、李佳、凯·李和李飞飞。ImageNet：大规模层次化图像数据库。见2009 IEEE计算机视觉与模式识别会议，第248–255页，2009年。'
- en: '[64] Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey
    Levine. Learning modular neural network policies for multi-task and multi-robot
    transfer. In 2017 IEEE International Conference on Robotics and Automation (ICRA),
    pages 2169–2176\. IEEE, 2017.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 科琳·德文、阿比谢克·古普塔、特雷弗·达雷尔、皮特·阿贝尔和谢尔盖·列文。为多任务和多机器人迁移学习模块化神经网络策略。见2017 IEEE国际机器人与自动化会议（ICRA），第2169–2176页，IEEE，2017年。'
- en: '[65] Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting Cyclic
    Symmetry in Convolutional Neural Networks. arXiv:1602.02660 [cs], May 2016.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 桑德·迪勒曼、杰弗里·德·法乌和科雷·卡武克丘格鲁。利用卷积神经网络中的循环对称性。arXiv:1602.02660 [cs]，2016年5月。'
- en: '[66] Changxing Ding and Dacheng Tao. Trunk-branch ensemble convolutional neural
    networks for video-based face recognition. IEEE transactions on pattern analysis
    and machine intelligence, 40(4):1002–1014, 2017.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 丁常兴和陶大程。用于基于视频的人脸识别的主干-分支集成卷积神经网络。IEEE模式分析与机器智能学报，40(4)：1002–1014，2017年。'
- en: '[67] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words:
    Transformers for Image Recognition at Scale. In International Conference on Learning
    Representations, 2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] 阿列克谢·多索维茨基、卢卡斯·贝耶、亚历山大·科列斯尼科夫、迪尔克·魏森博恩、肖华·翟、托马斯·安特纳、穆斯塔法·德赫加尼、马蒂亚斯·敏德尔、乔治·海戈尔德、西尔万·杰利、雅各布·乌斯科雷特和尼尔·霍尔斯比。图像值16x16词：用于大规模图像识别的变换器。见国际学习表征会议，2021年。'
- en: '[68] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong
    Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient
    scaling of language models with mixture-of-experts. In International Conference
    on Machine Learning, pages 5547–5569\. PMLR, 2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] 杜楠、黄艳平、安德鲁·M·戴、赛门·汤、德米特里·列皮欣、徐元中、马克西姆·克里昆、周艳琪、亚当斯·魏·余、奥尔汉·费拉特等。Glam：混合专家的高效语言模型扩展。见国际机器学习会议，第5547–5569页，PMLR，2022年。'
- en: '[69] Cian Eastwood and Christopher K. I. Williams. A Framework for the Quantitative
    Evaluation of Disentangled Representations. In Sixth International Conference
    on Learning Representations (ICLR 2018), May 2018.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] 西恩·伊斯特伍德和克里斯托弗·K·I·威廉姆斯。解缠表示的定量评估框架。见第六届国际学习表征会议（ICLR 2018），2018年5月。'
- en: '[70] David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored
    representations in a deep mixture of experts. In ICLR Workshop, 2014.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] David Eigen, Marc’Aurelio Ranzato, 和 Ilya Sutskever. 在深度专家混合中学习分解表示。见
    ICLR Workshop，2014年。'
- en: '[71] Adrian El Baz, Ihsan Ullah, Edesio Alcobaça, André C. P. L. F. Carvalho,
    Hong Chen, Fabio Ferreira, Henry Gouk, Chaoyu Guan, Isabelle Guyon, Timothy Hospedales,
    Shell Hu, Mike Huisman, Frank Hutter, Zhengying Liu, Felix Mohr, Ekrem Öztürk,
    Jan N. van Rijn, Haozhe Sun, Xin Wang, and Wenwu Zhu. Lessons learned from the
    NeurIPS 2021 MetaDL challenge: Backbone fine-tuning without episodic meta-learning
    dominates for few-shot learning image classification. In Douwe Kiela, Marco Ciccone,
    and Barbara Caputo, editors, Proceedings of the NeurIPS 2021 Competitions and
    Demonstrations Track, volume 176 of Proceedings of Machine Learning Research,
    pages 80–96. PMLR, December 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Adrian El Baz, Ihsan Ullah, Edesio Alcobaça, André C. P. L. F. Carvalho,
    Hong Chen, Fabio Ferreira, Henry Gouk, Chaoyu Guan, Isabelle Guyon, Timothy Hospedales,
    Shell Hu, Mike Huisman, Frank Hutter, Zhengying Liu, Felix Mohr, Ekrem Öztürk,
    Jan N. van Rijn, Haozhe Sun, Xin Wang, 和 Wenwu Zhu. 从 NeurIPS 2021 MetaDL 挑战中获得的经验教训：背骨微调在少样本学习图像分类中的表现优于情节元学习。见
    Douwe Kiela, Marco Ciccone 和 Barbara Caputo 主编，《NeurIPS 2021 竞赛和演示轨道论文集》，机器学习研究第176卷，第80–96页。PMLR，2022年12月。'
- en: '[72] Kai Olav Ellefsen, Jean-Baptiste Mouret, and Jeff Clune. Neural Modularity
    Helps Organisms Evolve to Learn New Skills without Forgetting Old Skills. PLOS
    Computational Biology, 11(4):e1004128, April 2015.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Kai Olav Ellefsen, Jean-Baptiste Mouret, 和 Jeff Clune. 神经模块化帮助有机体进化以学习新技能而不遗忘旧技能。PLOS
    计算生物学，11(4)：e1004128，2015年4月。'
- en: '[73] Gamaleldin F. Elsayed, Prajit Ramachandran, Jonathon Shlens, and Simon
    Kornblith. Revisiting Spatial Invariance with Low-Rank Local Connectivity. arXiv:2002.02959
    [cs, stat], August 2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Gamaleldin F. Elsayed, Prajit Ramachandran, Jonathon Shlens, 和 Simon Kornblith.
    重新审视低秩局部连接的空间不变性。arXiv:2002.02959 [cs, stat]，2020年8月。'
- en: '[74] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural Architecture
    Search. pages 69–86.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Thomas Elsken, Jan Hendrik Metzen, 和 Frank Hutter. 神经架构搜索。第69–86页。'
- en: '[75] William Fedus, Jeff Dean, and Barret Zoph. A Review of Sparse Expert Models
    in Deep Learning, September 2022.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] William Fedus, Jeff Dean, 和 Barret Zoph. 稀疏专家模型在深度学习中的综述，2022年9月。'
- en: '[76] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling
    to trillion parameter models with simple and efficient sparsity. Journal of Machine
    Learning Research, 23(120):1–39, 2022.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] William Fedus, Barret Zoph, 和 Noam Shazeer. Switch transformers：通过简单高效的稀疏性扩展到万亿参数模型。机器学习研究杂志，23(120)：1–39，2022年。'
- en: '[77] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David
    Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. PathNet: Evolution Channels
    Gradient Descent in Super Neural Networks. arXiv:1701.08734 [cs], January 2017.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David
    Ha, Andrei A. Rusu, Alexander Pritzel, 和 Daan Wierstra. PathNet：在超级神经网络中进化引导梯度下降。arXiv:1701.08734
    [cs]，2017年1月。'
- en: '[78] Daniel Filan, Stephen Casper, Shlomi Hod, Cody Wild, Andrew Critch, and
    Stuart Russell. Clusterability in Neural Networks. arXiv:2103.03386 [cs], March
    2021.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Daniel Filan, Stephen Casper, Shlomi Hod, Cody Wild, Andrew Critch, 和
    Stuart Russell. 神经网络中的聚类能力。arXiv:2103.03386 [cs]，2021年3月。'
- en: '[79] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning
    for Fast Adaptation of Deep Networks. arXiv:1703.03400 [cs], July 2017.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Chelsea Finn, Pieter Abbeel, 和 Sergey Levine. 用于快速适应深度网络的模型无关元学习。arXiv:1703.03400
    [cs]，2017年7月。'
- en: '[80] Jerry A. Fodor. The Modularity of Mind. April 1983.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Jerry A. Fodor. 《心智的模块化》。1983年4月。'
- en: '[81] Jerry A. Fodor. The Mind Doesn’t Work That Way: The Scope and Limits of
    Computational Psychology. MIT Press, 2000.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Jerry A. Fodor. 《心智不是那样运作的：计算心理学的范围与局限》。MIT Press，2000年。'
- en: '[82] Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture:
    A critical analysis. Cognition, 28(1-2):3–71, 1988.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Jerry A Fodor 和 Zenon W Pylyshyn. 连接主义与认知架构：批判性分析。认知，28(1-2)：3–71，1988年。'
- en: '[83] Martin Ford. Architects of Intelligence: The Truth about AI from the People
    Building It. Packt Publishing, Birmingham, UK, first published: november 2018
    edition, 2018.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Martin Ford. 《智能的建筑师：构建 AI 的人的真相》。Packt Publishing，英国伯明翰，首次出版：2018年11月版，2018年。'
- en: '[84] Willem E. Frankenhuis and Annemie Ploeger. Evolutionary Psychology Versus
    Fodor: Arguments For and Against the Massive Modularity Hypothesis. Philosophical
    Psychology, 20(6):687–710, December 2007.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Willem E. Frankenhuis 和 Annemie Ploeger. 进化心理学与 Fodor 的对抗：对大规模模块化假说的支持与反对。哲学心理学，20(6)：687–710，2007年12月。'
- en: '[85] Robert French. Using Semi-Distributed Representations to Overcome Catastrophic
    Forgetting in Connectionist Networks. 1991.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Robert French。使用半分布式表示克服连接主义网络中的灾难性遗忘。1991年。'
- en: '[86] Ninnart Fuengfusin and Hakaru Tamukoh. Network with Sub-networks: Layer-wise
    Detachable Neural Network. Journal of Robotics, Networking and Artificial Life,
    7(4):240–244, 2020.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Ninnart Fuengfusin 和 Hakaru Tamukoh。具有子网络的网络：层次可分离神经网络。《机器人、网络与人工生命期刊》，7(4):240–244，2020年。'
- en: '[87] Tomer Galanti and Lior Wolf. On the Modularity of Hypernetworks. arXiv:2002.10006
    [cs, stat], November 2020.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Tomer Galanti 和 Lior Wolf。关于超网络的模块化。arXiv:2002.10006 [cs, stat]，2020年11月。'
- en: '[88] Hongyang Gao and Shuiwang Ji. Efficient and Invariant Convolutional Neural
    Networks for Dense Prediction. In 2017 IEEE International Conference on Data Mining
    (ICDM), pages 871–876, 2017.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Hongyang Gao 和 Shuiwang Ji。高效且不变的卷积神经网络用于密集预测。收录于2017年IEEE国际数据挖掘会议（ICDM），第871–876页，2017年。'
- en: '[89] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image Style Transfer
    Using Convolutional Neural Networks. In 2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), pages 2414–2423, Las Vegas, NV, USA, June 2016.
    IEEE.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Leon A. Gatys、Alexander S. Ecker 和 Matthias Bethge。利用卷积神经网络进行图像风格迁移。收录于2016年IEEE计算机视觉与模式识别会议（CVPR），第2414–2423页，美国拉斯维加斯，2016年6月。IEEE。'
- en: '[90] Pralhad Gavali and J. Saira Banu. Chapter 6 - deep convolutional neural
    network for image classification on CUDA platform. In Arun Kumar Sangaiah, editor,
    Deep Learning and Parallel Computing Environment for Bioengineering Systems, pages
    99–122\. Academic Press, 2019.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Pralhad Gavali 和 J. Saira Banu。第6章 - 在CUDA平台上进行图像分类的深度卷积神经网络。收录于Arun Kumar
    Sangaiah编辑的《生物工程系统的深度学习与并行计算环境》，第99–122页。学术出版社，2019年。'
- en: '[91] Peter Gentile. Theory of Modularity, a Hypothesis. Procedia Computer Science,
    20, December 2013.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Peter Gentile。模块化理论，一种假设。《计算机科学程序学》，20，2013年12月。'
- en: '[92] Badih Ghazi, Rina Panigrahy, and Joshua Wang. Recursive Sketches for Modular
    Deep Learning. In Proceedings of the 36th International Conference on Machine
    Learning, pages 2211–2220\. PMLR, May 2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Badih Ghazi、Rina Panigrahy 和 Joshua Wang。用于模块化深度学习的递归草图。收录于第36届国际机器学习会议论文集，第2211–2220页。PMLR，2019年5月。'
- en: '[93] Daniel Gómez, J. Tinguaro Rodríguez, Javier Yáñez, and Javier Montero.
    A new modularity measure for Fuzzy Community detection problems based on overlap
    and grouping functions. International Journal of Approximate Reasoning, 74:88–107,
    July 2016.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Daniel Gómez、J. Tinguaro Rodríguez、Javier Yáñez 和 Javier Montero。基于重叠和分组函数的模糊社区检测问题的新模块化测度。《近似推理国际期刊》，74:88–107，2016年7月。'
- en: '[94] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT
    Press, 2016.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Ian Goodfellow、Yoshua Bengio 和 Aaron Courville。《深度学习》。麻省理工学院出版社，2016年。'
- en: '[95] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks.
    arXiv:1406.2661 [cs, stat], June 2014.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Ian J. Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David Warde-Farley、Sherjil
    Ozair、Aaron Courville 和 Yoshua Bengio。生成对抗网络。arXiv:1406.2661 [cs, stat]，2014年6月。'
- en: '[96] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine,
    Yoshua Bengio, and Bernhard Schölkopf. Recurrent independent mechanisms. In International
    Conference on Learning Representations, 2021.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Anirudh Goyal、Alex Lamb、Jordan Hoffmann、Shagun Sodhani、Sergey Levine、Yoshua
    Bengio 和 Bernhard Schölkopf。递归独立机制。收录于国际学习表征会议，2021年。'
- en: '[97] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
    Making the v in vqa matter: Elevating the role of image understanding in visual
    question answering. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 6904–6913, 2017.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Yash Goyal、Tejas Khot、Douglas Summers-Stay、Dhruv Batra 和 Devi Parikh。让vqa中的v变得重要：提升图像理解在视觉问答中的作用。收录于IEEE计算机视觉与模式识别会议论文集，第6904–6913页，2017年。'
- en: '[98] Scott Gray, Alec Radford, and Diederik P Kingma. GPU Kernels for Block-Sparse
    Weights. Technical report.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Scott Gray、Alec Radford 和 Diederik P Kingma。块稀疏权重的GPU内核。技术报告。'
- en: '[99] David Ha, Andrew Dai, and Quoc V. Le. HyperNetworks. arXiv:1609.09106
    [cs], December 2016.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] David Ha、Andrew Dai 和 Quoc V. Le。超网络。arXiv:1609.09106 [cs]，2016年12月。'
- en: '[100] Guy Hacohen and Daphna Weinshall. On The Power of Curriculum Learning
    in Training Deep Networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
    Proceedings of the 36th International Conference on Machine Learning, volume 97
    of Proceedings of Machine Learning Research, pages 2535–2544\. PMLR, June 2019.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Guy Hacohen 和 Daphna Weinshall. 课程学习在训练深度网络中的力量。在 Kamalika Chaudhuri
    和 Ruslan Salakhutdinov 编辑的第36届国际机器学习大会论文集中，第97卷机器学习研究论文集，第2535–2544页。PMLR，2019年6月。'
- en: '[101] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
    Statistical Learning: Data Mining, Inference and Prediction. Springer, second
    edition, 2009.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Trevor Hastie, Robert Tibshirani 和 Jerome Friedman. 统计学习的元素：数据挖掘、推断与预测。Springer，第二版，2009年。'
- en: '[102] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng
    Shi, and Qin Li. FasterMoE: Modeling and optimizing training of large-scale dynamic
    pre-trained models. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles
    and Practice of Parallel Programming, pages 120–134, 2022.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng
    Shi 和 Qin Li. FasterMoE: 大规模动态预训练模型的建模与优化。在第27届 ACM SIGPLAN 并行程序设计原则与实践研讨会论文集中，第120–134页，2022年。'
- en: '[103] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross
    Girshick. Masked autoencoders are scalable vision learners. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000–16009,
    2022.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár 和 Ross
    Girshick. 掩码自编码器是可扩展的视觉学习器。在 IEEE/CVF 计算机视觉与模式识别会议论文集中，第16000–16009页，2022年。'
- en: '[104] Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    4918–4927, 2019.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Kaiming He, Ross Girshick 和 Piotr Dollár. 重新思考 imagenet 预训练。在 IEEE/CVF
    国际计算机视觉会议论文集中，第4918–4927页，2019年。'
- en: '[105] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual
    learning for image recognition. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 770–778, 2016.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun. 用于图像识别的深度残差学习。在 IEEE
    计算机视觉与模式识别会议论文集中，第770–778页，2016年。'
- en: '[106] G. E. Hinton and R. R. Salakhutdinov. Reducing the Dimensionality of
    Data with Neural Networks. Science, 313(5786):504–507, 2006.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] G. E. Hinton 和 R. R. Salakhutdinov. 用神经网络减少数据的维度。科学，313(5786):504–507，2006年。'
- en: '[107] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Sepp Hochreiter 和 Jürgen Schmidhuber. 长短期记忆。神经计算，9(8):1735–1780，1997年。'
- en: '[108] Michel A Hofman. Evolution of the human brain: When bigger is better.
    Frontiers in neuroanatomy, 8:15, 2014.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Michel A Hofman. 人脑的演变：什么时候更大更好。神经解剖学前沿，8:15，2014年。'
- en: '[109] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional
    neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861,
    2017.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto 和 Hartwig Adam. Mobilenets: 高效的卷积神经网络用于移动视觉应用。arXiv
    预印本 arXiv:1704.04861, 2017。'
- en: '[110] Guosheng Hu, Yuxin Hu, Kai Yang, Zehao Yu, Flood Sung, Zhihong Zhang,
    Fei Xie, Jianguo Liu, Neil Robertson, Timothy Hospedales, and Qiangwei Miemie.
    Deep Stock Representation Learning: From Candlestick Charts to Investment Decisions.
    arXiv:1709.03803 [q-fin], February 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Guosheng Hu, Yuxin Hu, Kai Yang, Zehao Yu, Flood Sung, Zhihong Zhang,
    Fei Xie, Jianguo Liu, Neil Robertson, Timothy Hospedales 和 Qiangwei Miemie. 深度股票表示学习：从蜡烛图到投资决策。arXiv:1709.03803
    [q-fin], 2018年2月。'
- en: '[111] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate
    Saenko. Learning to Reason: End-to-End Module Networks for Visual Question Answering.
    In Proceedings of the IEEE International Conference on Computer Vision (ICCV),
    2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell 和 Kate Saenko.
    学习推理：用于视觉问答的端到端模块网络。在 IEEE 国际计算机视觉会议（ICCV）论文集中，2017年。'
- en: '[112] Jing Huang, Guan Pang, Rama Kovvuri, Mandy Toh, Kevin J Liang, Praveen
    Krishnan, Xi Yin, and Tal Hassner. A multiplexed network for end-to-end, multilingual
    OCR. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pages 4547–4557, 2021.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Jing Huang, Guan Pang, Rama Kovvuri, Mandy Toh, Kevin J Liang, Praveen
    Krishnan, Xi Yin 和 Tal Hassner. 一个用于端到端、多语言 OCR 的多路复用网络。在 IEEE/CVF 计算机视觉与模式识别会议论文集中，第4547–4557页，2021年。'
- en: '[113] Joost Huizinga, Jeff Clune, and Jean-Baptiste Mouret. Evolving neural
    networks that are both modular and regular: Hyperneat plus the connection cost
    technique. In Proceedings of the 2014 Annual Conference on Genetic and Evolutionary
    Computation, pages 697–704, 2014.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Joost Huizinga、Jeff Clune和Jean-Baptiste Mouret。进化的既模块化又规律的神经网络：Hyperneat加上连接成本技术。发表于2014年遗传和进化计算年会，页697–704，2014年。'
- en: '[114] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality
    decomposed: How do neural networks generalise? Journal of Artificial Intelligence
    Research, 67:757–795, 2020.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Dieuwke Hupkes、Verna Dankers、Mathijs Mul和Elia Bruni。组合性分解：神经网络如何进行泛化？人工智能研究杂志，67：757–795，2020年。'
- en: '[115] Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai
    Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra,
    Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser
    Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell,
    and Zhijing Jin. State-of-the-art generalisation research in NLP: A taxonomy and
    review, October 2022.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Dieuwke Hupkes、Mario Giulianelli、Verna Dankers、Mikel Artetxe、Yanai Elazar、Tiago
    Pimentel、Christos Christodoulopoulos、Karim Lasri、Naomi Saphra、Arabella Sinclair、Dennis
    Ulmer、Florian Schottmann、Khuyagbaatar Batsuren、Kaiser Sun、Koustuv Sinha、Leila
    Khalatbari、Maria Ryskina、Rita Frieske、Ryan Cotterell和Zhijing Jin。NLP领域的前沿泛化研究：分类和综述，2022年10月。'
- en: '[116] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, editors. Automatic
    Machine Learning: Methods, Systems, Challenges. Springer, 2019.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Frank Hutter、Lars Kotthoff和Joaquin Vanschoren，编辑。自动化机器学习：方法、系统、挑战。Springer，2019年。'
- en: '[117] Riashat Islam, Hongyu Zang, Anirudh Goyal, Alex Lamb, Kenji Kawaguchi,
    Xin Li, Romain Laroche, Yoshua Bengio, and Remi Tachet Des Combes. Discrete Factorial
    Representations as an Abstraction for Goal Conditioned Reinforcement Learning,
    October 2022.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Riashat Islam、洪宇·臧、Anirudh Goyal、Alex Lamb、Kenji Kawaguchi、Xin Li、Romain
    Laroche、Yoshua Bengio和Remi Tachet Des Combes。离散因子表示作为目标条件强化学习的抽象，2022年10月。'
- en: '[118] Robert A Jacobs, Michael I Jordan, and Andrew G Barto. Task decomposition
    through competition in a modular connectionist architecture: The what and where
    vision tasks. Cognitive science, 15(2):219–250, 1991.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Robert A Jacobs、Michael I Jordan和Andrew G Barto。通过模块化连接主义架构中的竞争进行任务分解：什么和哪里视觉任务。认知科学，15(2)：219–250，1991年。'
- en: '[119] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E.
    Hinton. Adaptive Mixtures of Local Experts. Neural Computation, 3(1):79–87, March
    1991.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Robert A. Jacobs、Michael I. Jordan、Steven J. Nowlan和Geoffrey E. Hinton。自适应局部专家混合。神经计算，3(1)：79–87，1991年3月。'
- en: '[120] Khurram Javed and Martha White. Meta-Learning Representations for Continual
    Learning. arXiv:1905.12588 [cs, stat], October 2019.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Khurram Javed和Martha White。元学习表示用于持续学习。arXiv:1905.12588 [cs, stat]，2019年10月。'
- en: '[121] Tian Jin and Seokin Hong. Split-CNN: Splitting Window-Based Operations
    in Convolutional Neural Networks for Memory System Optimization. In Proceedings
    of the Twenty-Fourth International Conference on Architectural Support for Programming
    Languages and Operating Systems, ASPLOS ’19, pages 835–847, New York, NY, USA,
    2019. Association for Computing Machinery.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Tian Jin和Seokin Hong。Split-CNN：在卷积神经网络中拆分基于窗口的操作以优化内存系统。发表于第二十四届国际编程语言和操作系统支持会议（ASPLOS
    ’19），页835–847，美国纽约，2019年。计算机协会。'
- en: '[122] Li Jing, Jiachen Zhu, and Yann LeCun. Masked Siamese ConvNets, June 2022.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] 李靖、贾辰周和杨·勒昆。掩码Siamese卷积网络，2022年6月。'
- en: '[123] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts
    and the EM algorithm. Neural computation, 6(2):181–214, 1994.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Michael I Jordan和Robert A Jacobs。专家的分层混合与EM算法。神经计算，6(2)：181–214，1994年。'
- en: '[124] Cheng Ju, Aurélien Bibaut, and Mark van der Laan. The relative performance
    of ensemble methods with deep convolutional neural networks for image classification.
    Journal of Applied Statistics, 45(15):2800–2818, 2018.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Cheng Ju、Aurélien Bibaut和Mark van der Laan。深度卷积神经网络与集成方法在图像分类中的相对性能。应用统计学杂志，45(15)：2800–2818，2018年。'
- en: '[125] Dan Jurafsky and James H Martin. Speech and language processing (3rd
    draft ed.), 2019.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Dan Jurafsky和James H Martin。语音和语言处理（第3稿），2019年。'
- en: '[126] Menelaos Kanakis, David Bruggemann, Suman Saha, Stamatios Georgoulis,
    Anton Obukhov, and Luc Van Gool. Reparameterizing convolutions for incremental
    multi-task learning without task interference. In European Conference on Computer
    Vision, pages 689–707\. Springer, 2020.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Menelaos Kanakis、David Bruggemann、Suman Saha、Stamatios Georgoulis、Anton
    Obukhov和Luc Van Gool。重新参数化卷积以实现无任务干扰的增量多任务学习。发表于欧洲计算机视觉会议，页689–707。Springer，2020年。'
- en: '[127] Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and Peter Clark. BeliefBank:
    Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief.
    In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing, pages 8849–8861, 2021.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Nora Kassner、Oyvind Tafjord、Hinrich Schütze 和 Peter Clark。BeliefBank：为预训练语言模型添加记忆，以系统化的信念概念。在2021年自然语言处理实证方法会议论文集，页码8849–8861，2021年。'
- en: '[128] Amandeep Kaur, Seema Baghla, and Sunil Kumar. Study of various character
    segmentation techniques for handwritten off-line cursive words: A review. International
    Journal of Advances in Science Engineering and Technology, 3(3):154–158, 2015.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Amandeep Kaur、Seema Baghla 和 Sunil Kumar。手写离线草书字符分割技术的研究：综述。《国际科学工程与技术进展杂志》，3(3):154–158，2015年。'
- en: '[129] Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu. Achieving forgetting
    prevention and knowledge transfer in continual learning. Advances in Neural Information
    Processing Systems, 34:22443–22456, 2021.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Zixuan Ke、Bing Liu、Nianzu Ma、Hu Xu 和 Lei Shu。实现遗忘防止和知识迁移的持续学习。《神经信息处理系统进展》，34:22443–22456，2021年。'
- en: '[130] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    Proceedings of NAACL-HLT, pages 4171–4186, 2019.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Jacob Devlin Ming-Wei Chang Kenton 和 Lee Kristina Toutanova。BERT：用于语言理解的深度双向变换器的预训练。在NAACL-HLT会议论文集，页码4171–4186，2019年。'
- en: '[131] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
    and Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization
    Gap and Sharp Minima. In ICLR, 2017.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Nitish Shirish Keskar、Dheevatsa Mudigere、Jorge Nocedal、Mikhail Smelyanskiy
    和 Ping Tak Peter Tang。大批量训练深度学习：泛化差距与锐利极小值。在ICLR，2017年。'
- en: '[132] Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel
    Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak,
    Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring
    compositional generalization: A comprehensive method on realistic data. In International
    Conference on Learning Representations, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Daniel Keysers、Nathanael Schärli、Nathan Scales、Hylke Buisman、Daniel Furrer、Sergii
    Kashubin、Nikola Momchev、Danila Sinopalnikov、Lukasz Stafiniak、Tibor Tihon、Dmitry
    Tsarkov、Xiao Wang、Marc van Zee 和 Olivier Bousquet。测量组合泛化：基于现实数据的全面方法。在国际学习表征会议，2020年。'
- en: '[133] Juyong Kim, Yookoon Park, Gunhee Kim, and Sung Ju Hwang. SplitNet: Learning
    to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization.
    In Proceedings of the 34th International Conference on Machine Learning, pages
    1866–1874\. PMLR, July 2017.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Juyong Kim、Yookoon Park、Gunhee Kim 和 Sung Ju Hwang。SplitNet：学习语义分割深度网络以实现参数减少和模型并行化。在第34届国际机器学习会议论文集，页码1866–1874。PMLR，2017年7月。'
- en: '[134] Hiroaki Kingetsu, Kenichi Kobayashi, and Taiji Suzuki. Neural Network
    Module Decomposition and Recomposition, December 2021.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Hiroaki Kingetsu、Kenichi Kobayashi 和 Taiji Suzuki。神经网络模块的分解与重组，2021年12月。'
- en: '[135] Louis Kirsch, Julius Kunze, and David Barber. Modular Networks: Learning
    to Decompose Neural Computation. In Advances in Neural Information Processing
    Systems, volume 31\. Curran Associates, Inc., 2018.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Louis Kirsch、Julius Kunze 和 David Barber。模块化网络：学习分解神经计算。在《神经信息处理系统进展》，第31卷。Curran
    Associates, Inc.，2018年。'
- en: '[136] Eunjeong Koh and Shlomo Dubnov. Comparison and Analysis of Deep Audio
    Embeddings for Music Emotion Recognition, April 2021.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Eunjeong Koh 和 Shlomo Dubnov。深度音频嵌入在音乐情感识别中的比较与分析，2021年4月。'
- en: '[137] Yamuna Krishnamurthy and Chris Watkins. Interpretability in gated modular
    neural networks. In eXplainable AI Approaches for Debugging and Diagnosis., 2021.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Yamuna Krishnamurthy 和 Chris Watkins。门控模块神经网络中的可解释性。在eXplainable AI调试与诊断方法，2021年。'
- en: '[138] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification
    with Deep Convolutional Neural Networks. In Advances in Neural Information Processing
    Systems, volume 25, 2012.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E Hinton。使用深度卷积神经网络的ImageNet分类。在《神经信息处理系统进展》，第25卷，2012年。'
- en: '[139] David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas
    Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and Christopher
    Pal. Zoneout: Regularizing RNNs by randomly preserving hidden activations. In
    International Conference on Learning Representations, 2017.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] David Krueger、Tegan Maharaj、Janos Kramar、Mohammad Pezeshki、Nicolas Ballas、Nan
    Rosemary Ke、Anirudh Goyal、Yoshua Bengio、Aaron Courville 和 Christopher Pal。Zoneout：通过随机保留隐藏激活来正则化RNN。在国际学习表征会议，2017年。'
- en: '[140] Ray Kurzweil. How to Create a Mind: The Secret of Human Thought Revealed.
    Penguin Books, USA, 2013.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Ray Kurzweil. 《如何创造大脑：揭示人类思维的秘密》。Penguin Books, USA，2013年。'
- en: '[141] Steinar Laenen and Luca Bertinetto. On episodes, prototypical networks,
    and few-shot learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,
    and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems,
    volume 34, pages 24581–24592\. Curran Associates, Inc., 2021.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Steinar Laenen 和 Luca Bertinetto. 《关于情节、原型网络和少样本学习》。见 M. Ranzato, A.
    Beygelzimer, Y. Dauphin, P.S. Liang, 和 J. Wortman Vaughan 编辑的《神经信息处理系统进展》，第34卷，页24581–24592。Curran
    Associates, Inc.，2021年。'
- en: '[142] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept
    learning through probabilistic program induction. Science, 350(6266):1332–1338,
    December 2015.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] B. M. Lake, R. Salakhutdinov, 和 J. B. Tenenbaum. 《通过概率程序归纳实现人类水平的概念学习》。《科学》，350(6266):1332–1338，2015年12月。'
- en: '[143] Brenden Lake and Marco Baroni. Generalization without systematicity:
    On the compositional skills of sequence-to-sequence recurrent networks. In International
    Conference on Machine Learning, pages 2873–2882\. PMLR, 2018.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Brenden Lake 和 Marco Baroni. 《没有系统性的泛化：序列到序列递归网络的组合技能》。见《国际机器学习会议论文集》，页2873–2882。PMLR，2018年。'
- en: '[144] Brenden M. Lake. Compositional generalization through meta sequence-to-sequence
    learning. arXiv:1906.05381 [cs], October 2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Brenden M. Lake. 《通过元序列到序列学习实现组合泛化》。arXiv:1906.05381 [cs]，2019年10月。'
- en: '[145] Y. LeCun, Fu Jie Huang, and L. Bottou. Learning methods for generic object
    recognition with invariance to pose and lighting. In Proceedings of the 2004 IEEE
    Computer Society Conference on Computer Vision and Pattern Recognition, 2004\.
    CVPR 2004., volume 2, pages II–104 Vol.2, June 2004.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Y. LeCun, Fu Jie Huang, 和 L. Bottou. 《通用物体识别的学习方法：对姿势和光照的不变性》。见《2004
    IEEE计算机视觉与模式识别会议论文集》，2004年。CVPR 2004，第2卷，页II–104，第2卷，2004年6月。'
- en: '[146] Yann LeCun. A path towards autonomous machine intelligence version 0.9\.
    2, 2022-06-27. 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Yann LeCun. 《通往自主机器智能的路径 0.9版》。2022年6月27日。2022年。'
- en: '[147] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based
    learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324,
    1998.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Yann LeCun, Léon Bottou, Yoshua Bengio, 和 Patrick Haffner. 《基于梯度的学习应用于文档识别》。《IEEE会议录》，86(11):2278–2324，1998年。'
- en: '[148] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances
    in neural information processing systems, 2, 1989.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Yann LeCun, John Denker, 和 Sara Solla. 《最佳脑损伤》。神经信息处理系统进展，第2卷，1989年。'
- en: '[149] Ming Li and Paul M.B. Vitnyi. An Introduction to Kolmogorov Complexity
    and Its Applications. Springer Publishing Company, Incorporated, third edition,
    2008.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Ming Li 和 Paul M.B. Vitnyi. 《Kolmogorov复杂性及其应用导论》。Springer Publishing
    Company, Incorporated, 第三版, 2008年。'
- en: '[150] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural
    speech synthesis with transformer network. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 33, pages 6706–6713, 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, 和 Ming Liu. 《基于变换器网络的神经语音合成》。见《AAAI人工智能会议论文集》，第33卷，页6706–6713，2019年。'
- en: '[151] Zhi Li, Bo Wu, Qi Liu, Likang Wu, Hongke Zhao, and Tao Mei. Learning
    the compositional visual coherence for complementary recommendations. In Christian
    Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference
    on Artificial Intelligence, IJCAI-20, pages 3536–3543\. International Joint Conferences
    on Artificial Intelligence Organization, July 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Zhi Li, Bo Wu, Qi Liu, Likang Wu, Hongke Zhao, 和 Tao Mei. 《学习用于补充推荐的组合视觉一致性》。见
    Christian Bessiere 编辑的《第二十九届国际人工智能联合会议论文集》，IJCAI-20，页3536–3543。国际人工智能联合会议组织，2020年7月。'
- en: '[152] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and
    Koray Kavukcuoglu. Hierarchical representations for efficient architecture search.
    In International Conference on Learning Representations, 2018.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, 和 Koray
    Kavukcuoglu. 《用于高效架构搜索的层次表示》。见《国际学习表征会议论文集》，2018年。'
- en: '[153] João Loula, Marco Baroni, and Brenden M. Lake. Rearranging the familiar:
    Testing compositional generalization in recurrent networks. In BlackboxNLP@EMNLP,
    pages 108–114, 2018.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] João Loula, Marco Baroni, 和 Brenden M. Lake. 《重新安排熟悉的事物：在递归网络中测试组合泛化》。见
    BlackboxNLP@EMNLP，页108–114，2018年。'
- en: '[154] Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled
    graph convolutional networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
    editors, Proceedings of the 36th International Conference on Machine Learning,
    volume 97 of Proceedings of Machine Learning Research, pages 4212–4221\. PMLR,
    June 2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang 和 Wenwu Zhu. 解耦图卷积网络。在 Kamalika
    Chaudhuri 和 Ruslan Salakhutdinov 主编的第36届国际机器学习会议论文集中，第97卷，机器学习研究会议论文，页码 4212–4221。PMLR，2019年6月。'
- en: '[155] Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos. Attentive
    Single-Tasking of Multiple Tasks. In 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), pages 1851–1860, Long Beach, CA, USA, June 2019\.
    IEEE.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Kevis-Kokitsi Maninis, Ilija Radosavovic 和 Iasonas Kokkinos. 多任务的专注单任务处理。在2019
    IEEE/CVF计算机视觉与模式识别会议（CVPR），页码 1851–1860，美国加州长滩，2019年6月。IEEE。'
- en: '[156] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
    Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing
    Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion
    Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon
    Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
    Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
    Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning
    on Heterogeneous Systems, 2015.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
    Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing
    Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion
    Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon
    Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
    Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
    Martin Wicke, Yuan Yu 和 Xiaoqiang Zheng. TensorFlow：异构系统上的大规模机器学习，2015年。'
- en: '[157] Nicolas Y. Masse, Gregory D. Grant, and David J. Freedman. Alleviating
    catastrophic forgetting using context-dependent gating and synaptic stabilization.
    Proceedings of the National Academy of Sciences, 115(44):E10467–E10475, 2018.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Nicolas Y. Masse, Gregory D. Grant 和 David J. Freedman. 使用上下文依赖门控和突触稳定化减轻灾难性遗忘。美国国家科学院院刊，115(44):E10467–E10475，2018年。'
- en: '[158] Vittorio Mazzia, Francesco Salvetti, and Marcello Chiaberge. Efficient-capsnet:
    Capsule network with self-attention routing. Scientific reports, 11(1):1–13, 2021.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Vittorio Mazzia, Francesco Salvetti 和 Marcello Chiaberge. 高效的胶囊网络：带有自注意力路由的胶囊网络。科学报告，11(1):1–13，2021年。'
- en: '[159] David McNeely-White, J. Ross Beveridge, and Bruce A. Draper. Inception
    and ResNet features are (almost) equivalent. Cognitive Systems Research, 59:312–318,
    January 2020.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] David McNeely-White, J. Ross Beveridge 和 Bruce A. Draper. Inception 和
    ResNet 特征（几乎）等效。认知系统研究，59:312–318，2020年1月。'
- en: '[160] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating
    and Editing Factual Associations in GPT. February 2022.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Kevin Meng, David Bau, Alex Andonian 和 Yonatan Belinkov. 定位和编辑GPT中的事实关联。2022年2月。'
- en: '[161] Elliot Meyerson and Risto Miikkulainen. Modular universal reparameterization:
    Deep multi-task learning across diverse domains. Advances in Neural Information
    Processing Systems, 32, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Elliot Meyerson 和 Risto Miikkulainen. 模块化通用重参数化：跨多样领域的深度多任务学习。神经信息处理系统进展，32，2019年。'
- en: '[162] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D.
    Manning. Fast Model Editing at Scale. arXiv:2110.11309 [cs], October 2021.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn 和 Christopher
    D. Manning. 大规模快速模型编辑。arXiv:2110.11309 [cs]，2021年10月。'
- en: '[163] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D.
    Manning. Memory-Based Model Editing at Scale. In International Conference on Machine
    Learning, 2022.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn 和 Christopher
    D. Manning. 基于记忆的大规模模型编辑。在国际机器学习会议，2022年。'
- en: '[164] Sarthak Mittal, Yoshua Bengio, and Guillaume Lajoie. Is a Modular Architecture
    Enough?, 2022.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Sarthak Mittal, Yoshua Bengio 和 Guillaume Lajoie. 模块化架构足够吗？2022年。'
- en: '[165] Sarthak Mittal, Sharath Chandra Raparthy, Irina Rish, Yoshua Bengio,
    and Guillaume Lajoie. Compositional attention: Disentangling search and retrieval.
    In International Conference on Learning Representations, 2022.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Sarthak Mittal, Sharath Chandra Raparthy, Irina Rish, Yoshua Bengio 和
    Guillaume Lajoie. 组成性注意力：解开搜索和检索的纠缠。在国际学习表征会议，2022年。'
- en: '[166] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P.
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods
    for Deep Reinforcement Learning. arXiv e-prints, page arXiv:1602.01783, February
    2016.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Volodymyr Mnih、Adrià Puigdomènech Badia、Mehdi Mirza、Alex Graves、Timothy
    P. Lillicrap、Tim Harley、David Silver 和 Koray Kavukcuoglu. 深度强化学习的异步方法。arXiv 电子预印本，页码
    arXiv:1602.01783，2016年2月。'
- en: '[167] Vladimir Modrak and Zuzana Soltysova. Development of the Modularity Measure
    for Assembly Process Structures. Mathematical Problems in Engineering, 2021:e4900748,
    December 2021.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Vladimir Modrak 和 Zuzana Soltysova. 装配过程结构的模块化度量发展。《工程中的数学问题》，2021:e4900748，2021年12月。'
- en: '[168] Stefanie Muff, Francesco Rao, and Amedeo Caflisch. Local modularity measure
    for network clusterizations. Physical Review E, 72(5):056107, November 2005.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Stefanie Muff、Francesco Rao 和 Amedeo Caflisch. 网络聚类的局部模块化度量。《物理评论E》，72(5):056107，2005年11月。'
- en: '[169] Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D. Manning.
    Characterizing Intrinsic Compositionality in Transformers with Tree Projections,
    November 2022.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Shikhar Murty、Pratyusha Sharma、Jacob Andreas 和 Christopher D. Manning.
    使用树投影表征变换器的内在组合性，2022年11月。'
- en: '[170] M. E. J. Newman. Modularity and community structure in networks. Proceedings
    of the National Academy of Sciences, 103(23):8577–8582, 2006.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] M. E. J. Newman. 网络中的模块化和社区结构。《国家科学院院刊》，103(23):8577–8582，2006年。'
- en: '[171] Michael Opitz, Horst Possegger, and Horst Bischof. Efficient model averaging
    for deep neural networks. In Asian Conference on Computer Vision, pages 205–220.
    Springer, 2016.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Michael Opitz、Horst Possegger 和 Horst Bischof. 深度神经网络的高效模型平均。发表于《亚洲计算机视觉会议》，第205–220页。Springer，2016年。'
- en: '[172] Oleksiy Ostapenko, Pau Rodriguez, Massimo Caccia, and Laurent Charlin.
    Continual learning via local module composition. Advances in Neural Information
    Processing Systems, 34:30298–30312, 2021.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Oleksiy Ostapenko、Pau Rodriguez、Massimo Caccia 和 Laurent Charlin. 通过局部模块组合进行持续学习。《神经信息处理系统进展》，34:30298–30312，2021年。'
- en: '[173] Oleksiy Ostapenko, Pau Rodriguez, Alexandre Lacoste, and Laurent Charlin.
    Attention for compositional modularity. In NeurIPS ’22 Workshop on All Things
    Attention: Bridging Different Perspectives on Attention, 2022.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Oleksiy Ostapenko、Pau Rodriguez、Alexandre Lacoste 和 Laurent Charlin.
    组合模块的注意力。发表于《NeurIPS ’22 关注事项研讨会：桥接不同视角的注意力》，2022年。'
- en: '[174] Rangeet Pan and Hridesh Rajan. On Decomposing a Deep Neural Network into
    Modules. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE
    2020, pages 889–900, New York, NY, USA, 2020\. Association for Computing Machinery.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Rangeet Pan 和 Hridesh Rajan. 关于将深度神经网络分解为模块。在第28届ACM联合欧洲软件工程会议及软件工程基础研讨会（ESEC/FSE
    2020）论文集中，第889–900页，美国纽约，2020年。计算机协会。'
- en: '[175] Rangeet Pan and Hridesh Rajan. Decomposing Convolutional Neural Networks
    into Reusable and Replaceable Modules. In Proceedings of The 44th International
    Conference on Software Engineering (ICSE 2022), December 2021.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Rangeet Pan 和 Hridesh Rajan. 将卷积神经网络分解为可重用和可替换模块。发表于《第44届国际软件工程会议（ICSE
    2022）》，2021年12月。'
- en: '[176] Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard
    Schölkopf. Learning independent causal mechanisms. In International Conference
    on Machine Learning, pages 4036–4044\. PMLR, 2018.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Giambattista Parascandolo、Niki Kilbertus、Mateo Rojas-Carulla 和 Bernhard
    Schölkopf. 学习独立的因果机制。发表于《国际机器学习会议》，第4036–4044页。PMLR，2018年。'
- en: '[177] D. L. Parnas. On the criteria to be used in decomposing systems into
    modules. Communications of the ACM, 15(12):1053–1058, December 1972.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] D. L. Parnas. 关于将系统分解为模块的标准。《ACM通讯》，15(12):1053–1058，1972年12月。'
- en: '[178] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
    Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
    Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,
    Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
    PyTorch: An Imperative Style, High-Performance Deep Learning Library. In H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett, editors,
    Advances in Neural Information Processing Systems 32, pages 8024–8035\. Curran
    Associates, Inc., 2019.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Adam Paszke、Sam Gross、Francisco Massa、Adam Lerer、James Bradbury、Gregory
    Chanan、Trevor Killeen、Zeming Lin、Natalia Gimelshein、Luca Antiga、Alban Desmaison、Andreas
    Kopf、Edward Yang、Zachary DeVito、Martin Raison、Alykhan Tejani、Sasank Chilamkurthy、Benoit
    Steiner、Lu Fang、Junjie Bai 和 Soumith Chintala。PyTorch：一种命令式风格的高性能深度学习库。见于 H. Wallach、H.
    Larochelle、A. Beygelzimer、F. d’Alché-Buc、E. Fox 和 R. Garnett 主编的《神经信息处理系统进展 32》，第8024–8035页。Curran
    Associates, Inc.，2019年。'
- en: '[179] Deepak Pathak, Christopher Lu, Trevor Darrell, Phillip Isola, and Alexei A
    Efros. Learning to control self-assembling morphologies: A study of generalization
    via modularity. Advances in Neural Information Processing Systems, 32, 2019.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Deepak Pathak、Christopher Lu、Trevor Darrell、Phillip Isola 和 Alexei A
    Efros。学习控制自组装形态：通过模块化进行泛化的研究。《神经信息处理系统进展》，32卷，2019年。'
- en: '[180] Jose B Pereira-Leal, Emmanuel D Levy, and Sarah A Teichmann. The origins
    and evolution of functional modules: Lessons from protein complexes. Philosophical
    Transactions of the Royal Society B: Biological Sciences, 361(1467):507–517, 2006.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Jose B Pereira-Leal、Emmanuel D Levy 和 Sarah A Teichmann。功能模块的起源与演化：来自蛋白质复合物的教训。《皇家学会B辑：生物科学哲学交易》，361(1467)：507–517，2006年。'
- en: '[181] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of Causal
    Inference: Foundations and Learning Algorithms. Adaptive Computation and Machine
    Learning Series. MIT Press, Cambridge, MA, USA, November 2017.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Jonas Peters、Dominik Janzing 和 Bernhard Schölkopf。因果推断元素：基础与学习算法。《自适应计算与机器学习系列》。麻省理工学院出版社，剑桥，MA，美国，2017年11月。'
- en: '[182] Timothée Poisot. An a posteriori measure of network modularity. F1000Research,
    2:130, December 2013.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Timothée Poisot。网络模块化的后验度量。《F1000Research》，2：130，2013年12月。'
- en: '[183] Edoardo Ponti. Inductive Bias and Modular Design for Sample-Efficient
    Neural Language Learning. PhD thesis, University of Cambridge, 2021.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Edoardo Ponti。样本高效神经语言学习的归纳偏差与模块化设计。博士论文，剑桥大学，2021年。'
- en: '[184] Edoardo M. Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy.
    Combining Modular Skills in Multitask Learning, March 2022.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Edoardo M. Ponti、Alessandro Sordoni、Yoshua Bengio 和 Siva Reddy。在多任务学习中结合模块化技能，2022年3月。'
- en: '[185] Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, and Marc’Aurelio
    Ranzato. Task-driven modular networks for zero-shot compositional learning. In
    Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    3593–3602, 2019.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Senthil Purushwalkam、Maximilian Nickel、Abhinav Gupta 和 Marc’Aurelio Ranzato。任务驱动的模块化网络用于零样本组合学习。见于《IEEE/CVF国际计算机视觉会议论文集》，第3593–3602页，2019年。'
- en: '[186] Zenon Pylyshyn. Is vision continuous with cognition?: The case for cognitive
    impenetrability of visual perception. Behavioral and Brain Sciences, 22(3):341–365,
    June 1999.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Zenon Pylyshyn。视觉是否与认知连续？：视觉感知的认知不可渗透性案例。《行为与脑科学》，22(3)：341–365，1999年6月。'
- en: '[187] Jun-Fei Qiao, Xi Meng, Wen-Jing Li, and Bogdan M. Wilamowski. A novel
    modular RBF neural network based on a brain-like partition method. Neural Computing
    and Applications, 32(3):899–911, February 2020.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Jun-Fei Qiao、Xi Meng、Wen-Jing Li 和 Bogdan M. Wilamowski。基于类似大脑分区方法的新型模块化RBF神经网络。《神经计算与应用》，32(3)：899–911，2020年2月。'
- en: '[188] Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua
    Bengio, Francesco Locatello, and Bernhard Schölkopf. Dynamic inference with neural
    interpreters. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
    Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,
    pages 10985–10998\. Curran Associates, Inc., 2021.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Nasim Rahaman、Muhammad Waleed Gondal、Shruti Joshi、Peter Gehler、Yoshua
    Bengio、Francesco Locatello 和 Bernhard Schölkopf。使用神经解释器进行动态推理。见于 M. Ranzato、A.
    Beygelzimer、Y. Dauphin、P.S. Liang 和 J. Wortman Vaughan 主编的《神经信息处理系统进展》，第34卷，第10985–10998页。Curran
    Associates, Inc.，2021年。'
- en: '[189] Prajit Ramachandran and Quoc V. Le. Diversity and depth in per-example
    routing models. In International Conference on Learning Representations, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Prajit Ramachandran 和 Quoc V. Le。在每个示例路由模型中的多样性和深度。见于《国际学习表示会议》，2019年。'
- en: '[190] G Ranganathan et al. A study to find facts behind preprocessing on deep
    learning algorithms. Journal of Innovative Image Processing (JIIP), 3(01):66–74,
    2021.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] G Ranganathan 等. 研究深度学习算法预处理背后的事实. 创新图像处理期刊（JIIP）, 3(01):66–74, 2021年。'
- en: '[191] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot
    learning. In International Conference on Learning Representations, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Sachin Ravi 和 Hugo Larochelle. 作为少样本学习模型的优化. 发表在国际学习表征会议上, 2017年。'
- en: '[192] J. Reisinger, K. Stanley, and R. Miikkulainen. Evolving Reusable Neural
    Modules. In GECCO, 2004.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] J. Reisinger, K. Stanley 和 R. Miikkulainen. 进化可重用神经模块. 发表在GECCO, 2004年。'
- en: '[193] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B
    Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM computing
    surveys (CSUR), 54(9):1–40, 2021.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij
    B Gupta, Xiaojiang Chen 和 Xin Wang. 深度主动学习综述. ACM计算调查（CSUR）, 54(9):1–40, 2021年。'
- en: '[194] Karl Ridgeway and Michael C Mozer. Learning Deep Disentangled Embeddings
    With the F-Statistic Loss. In Advances in Neural Information Processing Systems,
    volume 31\. Curran Associates, Inc., 2018.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Karl Ridgeway 和 Michael C Mozer. 使用F统计量损失学习深度解缠嵌入. 发表在神经信息处理系统进展中, 第31卷.
    Curran Associates, Inc., 2018年。'
- en: '[195] Philip Robbins. Modularity of Mind. In Edward N. Zalta, editor, The Stanford
    Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, winter
    2017 edition, 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Philip Robbins. 心智的模块化. 见Edward N. Zalta主编的《斯坦福哲学百科全书》。斯坦福大学形而上学研究实验室,
    2017年冬季版, 2017年。'
- en: '[196] John S Rose. A Course on Group Theory. Courier Corporation, 1994.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] John S Rose. 群体理论课程. Courier Corporation, 1994年。'
- en: '[197] Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger. Routing
    Networks and the Challenges of Modular and Compositional Computation, April 2019.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Clemens Rosenbaum, Ignacio Cases, Matthew Riemer 和 Tim Klinger. 路由网络及模块化和组合计算的挑战,
    2019年4月。'
- en: '[198] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing Networks:
    Adaptive Selection of Non-Linear Functions for Multi-Task Learning. In International
    Conference on Learning Representations, 2018.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Clemens Rosenbaum, Tim Klinger 和 Matthew Riemer. 路由网络：多任务学习的非线性函数自适应选择.
    发表在国际学习表征会议上, 2018年。'
- en: '[199] Sebastian Ruder. An overview of gradient descent optimization algorithms.
    arXiv preprint arXiv:1609.04747, 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Sebastian Ruder. 梯度下降优化算法概述. arXiv预印本 arXiv:1609.04747, 2016年。'
- en: '[200] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning
    internal representations by error propagation. Technical report, California Univ
    San Diego La Jolla Inst for Cognitive Science, 1985.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] David E Rumelhart, Geoffrey E Hinton 和 Ronald J Williams. 通过误差传播学习内部表示.
    技术报告, 加州大学圣地亚哥分校认知科学研究所, 1985年。'
- en: '[201] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
    ImageNet Large Scale Visual Recognition Challenge. In International Journal of
    Computer Vision, volume 115, pages 211–252\. Springer, 2015.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein 等. ImageNet
    大规模视觉识别挑战. 见国际计算机视觉期刊, 第115卷, 页211–252. Springer, 2015年。'
- en: '[202] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer,
    James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive
    Neural Networks. arXiv:1606.04671 [cs], September 2016.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer,
    James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu 和 Raia Hadsell. 进阶神经网络. arXiv:1606.04671
    [cs], 2016年9月。'
- en: '[203] Guillaume Salha-Galvan, Johannes F. Lutzeyer, George Dasoulas, Romain
    Hennequin, and Michalis Vazirgiannis. Modularity-Aware Graph Autoencoders for
    Joint Community Detection and Link Prediction, June 2022.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Guillaume Salha-Galvan, Johannes F. Lutzeyer, George Dasoulas, Romain
    Hennequin 和 Michalis Vazirgiannis. 模块化感知图自编码器用于联合社区检测和链接预测, 2022年6月。'
- en: '[204] M. Schenkel, H. Weissman, I. Guyon, C. Nohl, and D. Henderson. Recognition-based
    segmentation of on-line hand-printed words. In S. Hanson, J. Cowan, and C. Giles,
    editors, Advances in Neural Information Processing Systems, volume 5\. Morgan-Kaufmann,
    1992.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] M. Schenkel, H. Weissman, I. Guyon, C. Nohl 和 D. Henderson. 基于识别的在线手写词分割.
    见S. Hanson, J. Cowan 和 C. Giles主编的《神经信息处理系统进展》，第5卷. Morgan-Kaufmann, 1992年。'
- en: '[205] Melissa Schilling. Toward a General Modular Systems Theory and Its Application
    to Interfirm Product Modularity. Academy of Management Review, 25, April 2000.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Melissa Schilling. 朝向一般模块系统理论及其在企业间产品模块化中的应用. 管理学院评论, 25, 2000年4月。'
- en: '[206] Jürgen Schmidhuber. Towards compositional learning in dynamic networks.
    1990.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] 於尔根·施密德胡伯。动态网络中的组合学习。1990年。'
- en: '[207] Albrecht Schmidt and Zuhair Bandar. Modularity - A Concept For New Neural
    Network Architectures. November 2001.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] 阿尔布雷希特·施密特和祖赫尔·班达尔。模块化 - 一种新的神经网络架构概念。2001年11月。'
- en: '[208] Yue Shao and Victor M. Zavala. Modularity measures: Concepts, computation,
    and applications to manufacturing systems. AIChE Journal, 66(6):e16965, 2020.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] 邵岳和维克托·M·扎瓦拉。模块化测量：概念、计算及其在制造系统中的应用。AIChE期刊，66(6):e16965，2020年。'
- en: '[209] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V.
    Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously Large Neural Networks: The
    Sparsely-Gated Mixture-of-Experts Layer. In 5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings. OpenReview.net, 2017.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] 诺亚姆·沙泽尔、阿扎利亚·米尔霍塞尼、克日什托夫·马兹亚尔兹、安迪·戴维斯、阮国伟、杰弗瑞·E·辛顿和杰夫·迪恩。极其庞大的神经网络：稀疏门控专家混合层。在第5届国际学习表征会议ICLR
    2017，法国土伦，2017年4月24-26日，会议论文集。OpenReview.net，2017年。'
- en: '[210] Baoguang Shi, Xiang Bai, and Cong Yao. Script identification in the wild
    via discriminative convolutional neural network. Pattern Recognition, 52:448–458,
    April 2016.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] 石宝光、白翔和姚聪。通过判别卷积神经网络在野外进行脚本识别。模式识别，52:448–458，2016年4月。'
- en: '[211] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual Learning
    with Deep Generative Replay. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
    R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
    Processing Systems, volume 30\. Curran Associates, Inc., 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] 辛赫尔、李正权、金在宏和金智元。利用深度生成重放进行持续学习。在I. Guyon、U. V. Luxburg、S. Bengio、H. Wallach、R.
    Fergus、S. Vishwanathan和R. Garnett主编的《神经信息处理系统进展》卷30。Curran Associates, Inc.，2017年。'
- en: '[212] Hiroaki Shiokawa, Yasuhiro Fujiwara, and Makoto Onizuka. Fast algorithm
    for modularity-based graph clustering. In Proceedings of the AAAI Conference on
    Artificial Intelligence, volume 27, pages 1170–1176, 2013.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] 塩川广明、藤原康宏和大塚诚。基于模块化的图聚类的快速算法。在人工智能AAAI会议论文集，卷27，第1170–1176页，2013年。'
- en: '[213] L Sifre. Rigid-Motion Scattering for Image Classification [PhD Thesis].
    PhD thesis, 2014.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] L·西弗雷。用于图像分类的刚性运动散射 [博士论文]。博士论文，2014年。'
- en: '[214] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre,
    George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam,
    Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya
    Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel,
    and Demis Hassabis. Mastering the game of Go with deep neural networks and tree
    search. Nature, 529(7587):484–489, January 2016.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] 大卫·西尔弗、阿贾·黄、克里斯·J·马迪森、亚瑟·盖兹、洛朗·西弗雷、乔治·范登·德里施、朱利安·施利特维瑟、伊奥尼斯·安东诺格鲁、维达·帕内尔舍尔瓦姆、马克·兰克托、桑德·迪勒曼、多米尼克·格雷韦、约翰·南、纳尔·卡尔赫布伦纳、伊利亚·苏茨凯弗、蒂莫西·利利克拉普、马德琳·利奇、科雷·卡夫克乌格鲁、托尔·格雷佩尔和德米斯·哈萨比斯。通过深度神经网络和树搜索掌握围棋。自然，529(7587):484–489，2016年1月。'
- en: '[215] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou,
    Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
    Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche,
    Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge.
    Nature, 550(7676):354–359, October 2017.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] 大卫·西尔弗、朱利安·施利特维瑟、凯伦·西蒙扬、伊奥尼斯·安东诺格鲁、阿贾·黄、亚瑟·盖兹、托马斯·休伯特、卢卡斯·贝克、马修·莱、阿德里安·波尔顿、于天辰、蒂莫西·利利克拉普、范辉、洛朗·西弗雷、乔治·范登·德里施、托尔·格雷佩尔和德米斯·哈萨比斯。无需人类知识掌握围棋。自然，550(7676):354–359，2017年10月。'
- en: '[216] P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for convolutional
    neural networks applied to visual document analysis. In Seventh International
    Conference on Document Analysis and Recognition, 2003\. Proceedings., pages 958–963,
    August 2003.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] P. Y. 西马尔德、D. 斯坦克劳斯和J. C. 普拉特。应用于视觉文档分析的卷积神经网络最佳实践。在第七届国际文档分析与识别会议，2003年。论文集，第958–963页，2003年8月。'
- en: '[217] Herbert A. Simon. The Architecture of Complexity. Proceedings of the
    American Philosophical Society, 106(6):467–482, 1962.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] 赫伯特·A·西蒙。复杂性的架构。美国哲学学会学报，106(6):467–482，1962年。'
- en: '[218] Herbert A. Simon and Albert Ando. Aggregation of variables in dynamic
    systems. Econometrica, 29(2):111–138, 1961.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] 赫伯特·A·西蒙和阿尔伯特·安多。动态系统中的变量聚合。计量经济学，29(2):111–138，1961年。'
- en: '[219] Christopher Simpkins and Charles Isbell. Composable modular reinforcement
    learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,
    pages 4975–4982, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Christopher Simpkins 和 Charles Isbell。可组合模块化强化学习。载于 AAAI 人工智能会议论文集，第
    33 卷，页码 4975–4982，2019年。'
- en: '[220] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and
    Artem Babenko. Editable Neural Networks. In International Conference on Learning
    Representations, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Anton Sinitsin、Vsevolod Plokhotnyuk、Dmitry Pyrkin、Sergei Popov 和 Artem
    Babenko。可编辑的神经网络。载于国际表示学习会议，2019年。'
- en: '[221] Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t
    decay the learning rate, increase the batch size. In International Conference
    on Learning Representations, 2018.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Samuel L Smith、Pieter-Jan Kindermans、Chris Ying 和 Quoc V Le。不要衰减学习率，增加批量大小。载于国际表示学习会议，2018年。'
- en: '[222] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
    Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay
    Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer,
    Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan
    Catanzaro. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale
    Generative Language Model, February 2022.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Shaden Smith、Mostofa Patwary、Brandon Norick、Patrick LeGresley、Samyam
    Rajbhandari、Jared Casper、Zhun Liu、Shrimai Prabhumoye、George Zerveas、Vijay Korthikanti、Elton
    Zhang、Rewon Child、Reza Yazdani Aminabadi、Julie Bernauer、Xia Song、Mohammad Shoeybi、Yuxiong
    He、Michael Houston、Saurabh Tiwary 和 Bryan Catanzaro。使用 DeepSpeed 和 Megatron 训练
    Megatron-Turing NLG 530B，一个大规模生成语言模型，2022年2月。'
- en: '[223] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical Networks
    for Few-shot Learning. arXiv:1703.05175 [cs, stat], June 2017.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Jake Snell、Kevin Swersky 和 Richard S. Zemel。原型网络用于小样本学习。arXiv:1703.05175
    [cs, stat]，2017年6月。'
- en: '[224] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,
    and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from
    overfitting. Journal of Machine Learning Research, 15(56):1929–1958, 2014.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Nitish Srivastava、Geoffrey Hinton、Alex Krizhevsky、Ilya Sutskever 和 Ruslan
    Salakhutdinov。Dropout：防止神经网络过拟合的简单方法。机器学习研究期刊，15(56)：1929–1958，2014年。'
- en: '[225] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting
    unreasonable effectiveness of data in deep learning era. In Proceedings of the
    IEEE International Conference on Computer Vision, pages 843–852, 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] Chen Sun、Abhinav Shrivastava、Saurabh Singh 和 Abhinav Gupta。重新审视深度学习时代数据的非凡有效性。载于
    IEEE 国际计算机视觉会议论文集，页码 843–852，2017年。'
- en: '[226] Guolei Sun, Thomas Probst, Danda Pani Paudel, Nikola Popovic, Menelaos
    Kanakis, Jagruti Patel, Dengxin Dai, and Luc Van Gool. Task Switching Network
    for Multi-task Learning. In 2021 IEEE/CVF International Conference on Computer
    Vision (ICCV), pages 8271–8280, Montreal, QC, Canada, October 2021. IEEE.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Guolei Sun、Thomas Probst、Danda Pani Paudel、Nikola Popovic、Menelaos Kanakis、Jagruti
    Patel、Dengxin Dai 和 Luc Van Gool。多任务学习的任务切换网络。载于 2021 IEEE/CVF 国际计算机视觉会议（ICCV），页码
    8271–8280，加拿大蒙特利尔，2021年10月。IEEE。'
- en: '[227] Haozhe Sun, Wei-Wei Tu, and Isabelle M. Guyon. OmniPrint: A Configurable
    Printed Character Synthesizer. In Thirty-Fifth Conference on Neural Information
    Processing Systems Datasets and Benchmarks Track (Round 1), 2021.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Haozhe Sun、Wei-Wei Tu 和 Isabelle M. Guyon。OmniPrint：一个可配置的打印字符合成器。载于第35届神经信息处理系统会议数据集和基准轨道（第1轮），2021年。'
- en: '[228] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction.
    The MIT Press, second edition, 2018.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Richard S. Sutton 和 Andrew G. Barto。《强化学习：导论》。麻省理工学院出版社，第2版，2018年。'
- en: '[229] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In Thirty-First AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] Christian Szegedy、Sergey Ioffe、Vincent Vanhoucke 和 Alexander A Alemi。Inception-v4、inception-resnet
    及残差连接对学习的影响。载于第31届 AAAI 人工智能会议，2017年。'
- en: '[230] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
    Wojna. Rethinking the inception architecture for computer vision. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818–2826,
    2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Christian Szegedy、Vincent Vanhoucke、Sergey Ioffe、Jon Shlens 和 Zbigniew
    Wojna。重新思考计算机视觉中的 inception 架构。载于 IEEE 计算机视觉与模式识别会议论文集，页码 2818–2826，2016年。'
- en: '[231] Antti Tarvainen and Harri Valpola. Mean teachers are better role models:
    Weight-averaged consistency targets improve semi-supervised deep learning results.
    arXiv:1703.01780 [cs, stat], April 2018.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Antti Tarvainen 和 Harri Valpola。均值教师是更好的榜样：权重平均一致性目标改善半监督深度学习结果。arXiv:1703.01780
    [cs, stat]，2018年4月。'
- en: '[232] Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. BranchyNet: Fast
    inference via early exiting from deep neural networks. In 2016 23rd International
    Conference on Pattern Recognition (ICPR), pages 2464–2469, 2016.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] Surat Teerapittayanon、Bradley McDanel 和 H.T. Kung。《BranchyNet：通过深度神经网络的早期退出实现快速推理》。发表于2016年第23届国际模式识别大会（ICPR），第2464–2469页，2016年。'
- en: '[233] Alexander Terekhov, Guglielmo Montone, and J. O’Regan. Knowledge Transfer
    in Deep Block-Modular Neural Networks. In Biomimetic and Biohybrid Systems, pages
    268–279. Springer, July 2015.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] Alexander Terekhov、Guglielmo Montone 和 J. O’Regan。《深度块模块神经网络中的知识转移》。发表于《仿生学与生物混合系统》，第268–279页。Springer，2015年7月。'
- en: '[234] Naftali Tishby and Noga Zaslavsky. Deep learning and the information
    bottleneck principle. In 2015 IEEE Information Theory Workshop (ITW), pages 1–5,
    2015.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Naftali Tishby 和 Noga Zaslavsky。《深度学习与信息瓶颈原则》。发表于2015 IEEE信息理论研讨会（ITW），第1–5页，2015年。'
- en: '[235] Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku
    Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol,
    et al. Meta-dataset: A dataset of datasets for learning to learn from few examples.
    In International Conference on Learning Representations, 2019.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Eleni Triantafillou、Tyler Zhu、Vincent Dumoulin、Pascal Lamblin、Utku Evci、Kelvin
    Xu、Ross Goroshin、Carles Gelada、Kevin Swersky、Pierre-Antoine Manzagol 等人。《Meta-dataset：用于从少量样本中学习的元数据集》。发表于国际学习表征大会，2019年。'
- en: '[236] Ihsan Ullah, Dustin Carrion, Sergio Escalera, Isabelle M. Guyon, Mike
    Huisman, Felix Mohr, Jan N. van Rijn, Haozhe Sun, Joaquin Vanschoren, and Phan Anh
    Vu. Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification, 2022.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] Ihsan Ullah、Dustin Carrion、Sergio Escalera、Isabelle M. Guyon、Mike Huisman、Felix
    Mohr、Jan N. van Rijn、Haozhe Sun、Joaquin Vanschoren 和 Phan Anh Vu。《Meta-Album：用于少样本图像分类的多领域Meta数据集》，2022年。'
- en: '[237] Ivan I Vankov and Jeffrey S Bowers. Training neural networks to encode
    symbols enables combinatorial generalization. Philosophical Transactions of the
    Royal Society B, 375(1791):20190309, 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Ivan I Vankov 和 Jeffrey S Bowers。《训练神经网络以编码符号实现组合泛化》。发表于《皇家学会B辑哲学学报》，375(1791):20190309，2020年。'
- en: '[238] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30\.
    Curran Associates, Inc., 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan
    N Gomez、Łukasz Kaiser 和 Illia Polosukhin。《注意力即一切》。发表于I. Guyon、U. Von Luxburg、S.
    Bengio、H. Wallach、R. Fergus、S. Vishwanathan 和 R. Garnett 编辑的《神经信息处理系统进展》，第30卷。Curran
    Associates, Inc.，2017年。'
- en: '[239] Tom Veniat, Ludovic Denoyer, and Marc’Aurelio Ranzato. Efficient Continual
    Learning with Modular Networks and Task-Driven Priors. In 9th International Conference
    on Learning Representations, ICLR 2021, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Tom Veniat、Ludovic Denoyer 和 Marc’Aurelio Ranzato。《通过模块化网络和任务驱动先验进行高效的持续学习》。发表于第九届国际学习表征会议，ICLR
    2021，2021年。'
- en: '[240] Ulrike von Luxburg, Robert C. Williamson, and Isabelle Guyon. Clustering:
    Science or Art? In Proceedings of ICML Workshop on Unsupervised and Transfer Learning,
    pages 65–79\. JMLR Workshop and Conference Proceedings, June 2012.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Ulrike von Luxburg、Robert C. Williamson 和 Isabelle Guyon。《聚类：科学还是艺术？》。发表于ICML无监督和迁移学习研讨会会议记录，第65–79页。JMLR工作坊和会议记录，2012年6月。'
- en: '[241] Gunter P. Wagner and Lee Altenberg. Perspective: Complex Adaptations
    and the Evolution of Evolvability. Evolution, 50(3):967–976, 1996.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Gunter P. Wagner 和 Lee Altenberg。《视角：复杂适应与进化能力的演变》。发表于《进化》期刊，50(3):967–976，1996年。'
- en: '[242] Haoxiang Wang, Han Zhao, and Bo Li. Bridging multi-task learning and
    meta-learning: Towards efficient training and effective adaptation. In International
    Conference on Machine Learning, pages 10991–11002\. PMLR, 2021.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] Haoxiang Wang、Han Zhao 和 Bo Li。《桥接多任务学习与元学习：迈向高效训练和有效适应》。发表于国际机器学习大会，第10991–11002页。PMLR，2021年。'
- en: '[243] Jianan Wang, Eren Sezener, David Budden, Marcus Hutter, and Joel Veness.
    A Combinatorial Perspective on Transfer Learning. In H. Larochelle, M. Ranzato,
    R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
    Processing Systems, volume 33, pages 918–929\. Curran Associates, Inc., 2020.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Jianan Wang、Eren Sezener、David Budden、Marcus Hutter 和 Joel Veness。《从组合的角度看迁移学习》。发表于H.
    Larochelle、M. Ranzato、R. Hadsell、M. F. Balcan 和 H. Lin 编辑的《神经信息处理系统进展》，第33卷，第918–929页。Curran
    Associates, Inc.，2020年。'
- en: '[244] Ruohan Wang, Massimiliano Pontil, and Carlo Ciliberto. The role of global
    labels in few-shot classification and how to infer them. In Advances in Neural
    Information Processing Systems, volume 34, pages 27160–27170, 2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] 汪若涵、马西米利亚诺·庞蒂尔和卡洛·奇利贝尔托。全局标签在少样本分类中的作用及其推断方法。在《神经信息处理系统进展》第34卷论文集中，27160–27170页，2021年。'
- en: '[245] Chihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Modular representation
    of layered neural networks. Neural Networks, 97:62–73, 2018.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] 渡边千寻、平松薰和香野邦男。层次神经网络的模块化表示。神经网络，97:62–73，2018年。'
- en: '[246] Maurice Weiler and Gabriele Cesa. General $E(2)$-Equivariant Steerable
    CNNs. arXiv:1911.08251 [cs, eess], April 2021.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] 莫里斯·维勒和加布里埃尔·塞萨。通用$E(2)$-等变可引导卷积神经网络。arXiv:1911.08251 [cs, eess]，2021年4月。'
- en: '[247] Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning Steerable
    Filters for Rotation Equivariant CNNs. arXiv:1711.07289 [cs], March 2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] 莫里斯·维勒、弗雷德·A·汉普雷希特和马丁·斯托拉斯特。学习可引导滤波器用于旋转等变卷积神经网络。arXiv:1711.07289 [cs]，2018年3月。'
- en: '[248] Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J.
    Brostow. Harmonic Networks: Deep Translation and Rotation Equivariance. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5028–5037,
    2017.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] 丹尼尔·E·沃拉尔、斯蒂芬·J·加宾、达尼亚尔·图尔穆汉别托夫和加布里埃尔·J·布罗斯托。谐波网络：深度平移和旋转等变性。在IEEE计算机视觉与模式识别会议论文集中，5028–5037页，2017年。'
- en: '[249] Likang Wu, Zhi Li, Hongke Zhao, Qi Liu, Jun Wang, Mengdi Zhang, and Enhong
    Chen. Learning the implicit semantic representation on graph-structured data.
    In International Conference on Database Systems for Advanced Applications, pages
    3–19\. Springer, 2021.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] 吴立康、李智、赵宏科、刘奇、王俊、张萌迪和陈恩宏。图结构数据上的隐式语义表示学习。在数据库系统高级应用国际会议论文集中，3–19页。Springer，2021年。'
- en: '[250] Yuhuai Wu, Elman Mansimov, Shun Liao, Roger Grosse, and Jimmy Ba. Scalable
    trust-region method for deep reinforcement learning using Kronecker-factored approximation.
    arXiv e-prints, page arXiv:1708.05144, August 2017.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] 吴玉怀、埃尔曼·曼西莫夫、肖顺、罗杰·格罗斯和吉米·巴。使用克罗内克因子的近似的可扩展信任区域方法用于深度强化学习。arXiv e-prints，页面
    arXiv:1708.05144，2017年8月。'
- en: '[251] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He.
    Aggregated Residual Transformations for Deep Neural Networks. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] 谢赛宁、罗斯·吉尔什克、皮奥特·美元、朱文图和凯明·赫。深度神经网络的聚合残差变换。在IEEE计算机视觉与模式识别会议（CVPR）论文集中，2017年7月。'
- en: '[252] Saining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He. Exploring
    Randomly Wired Neural Networks for Image Recognition. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision (ICCV), October 2019.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] 谢赛宁、亚历山大·基里洛夫、罗斯·吉尔什克和凯明·赫。探索随机连线神经网络用于图像识别。在IEEE/CVF国际计算机视觉大会（ICCV）论文集中，2019年10月。'
- en: '[253] Chao Xiong, Xiaowei Zhao, Danhang Tang, Karlekar Jayashree, Shuicheng
    Yan, and Tae-Kyun Kim. Conditional convolutional neural network for modality-aware
    face recognition. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 3667–3675, 2015.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] 熊超、赵晓伟、汤丹航、贾雅什里·卡尔卡和闫水成。用于模态感知人脸识别的条件卷积神经网络。在IEEE国际计算机视觉大会论文集中，3667–3675页，2015年。'
- en: '[254] I. Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan.
    Billion-scale semi-supervised learning for image classification. CoRR, abs/1905.00546,
    2019.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] I·泽基·亚尔尼兹、厄尔维·热戈、陈侃、马诺哈尔·帕卢里和德鲁夫·马贾恩。亿规模半监督学习用于图像分类。CoRR，abs/1905.00546，2019年。'
- en: '[255] Shudong Yang, Xueying Yu, and Ying Zhou. LSTM and GRU neural network
    performance comparison study: Taking yelp review dataset as an example. In 2020
    International Workshop on Electronic Communication and Artificial Intelligence
    (IWECAI), pages 98–101, 2020.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] 杨曙东、徐颖和周颖。LSTM与GRU神经网络性能比较研究：以Yelp评论数据集为例。在2020年国际电子通信与人工智能研讨会（IWECAI）论文集中，98–101页，2020年。'
- en: '[256] Bangpeng Yao, Dirk Walther, Diane Beck, and Li Fei-fei. Hierarchical
    mixture of classification experts uncovers interactions between brain regions.
    In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, editors,
    Advances in Neural Information Processing Systems, volume 22\. Curran Associates,
    Inc., 2009.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] 杨邦鹏、迪尔克·瓦尔特、黛安·贝克和李飞飞。分类专家的层次混合揭示大脑区域之间的相互作用。在Y·本吉奥、D·舒尔曼斯、J·拉弗提、C·威廉姆斯和A·库洛塔编辑的《神经信息处理系统进展》第22卷。Curran
    Associates, Inc.，2009年。'
- en: '[257] Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy,
    and Frank Hutter. Nas-bench-101: Towards reproducible neural architecture search.
    In International Conference on Machine Learning, pages 7105–7114\. PMLR, 2019.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy
    和 Frank Hutter. Nas-bench-101: 朝着可重复的神经架构搜索迈进。在国际机器学习会议，页面7105–7114。PMLR，2019年。'
- en: '[258] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable
    are features in deep neural networks? arXiv:1411.1792 [cs], November 2014.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] Jason Yosinski, Jeff Clune, Yoshua Bengio 和 Hod Lipson. 深度神经网络中的特征有多可转移？arXiv:1411.1792
    [cs]，2014年11月。'
- en: '[259] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable
    Neural Networks. In International Conference on Learning Representations, 2019.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang 和 Thomas Huang. 可调整的神经网络。在国际学习表示会议，2019年。'
- en: '[260] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal,
    and Tamara L Berg. MAttNet: Modular attention network for referring expression
    comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 1307–1315, 2018.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal 和
    Tamara L Berg. MAttNet: 模块化注意力网络用于引用表达理解。发表于IEEE计算机视觉与模式识别会议论文集，页面1307–1315，2018年。'
- en: '[261] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman,
    and Chelsea Finn. Gradient Surgery for Multi-Task Learning. In Advances in Neural
    Information Processing Systems, volume 33, pages 5824–5836\. Curran Associates,
    Inc., 2020.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman
    和 Chelsea Finn. 多任务学习的梯度手术。在《神经信息处理系统进展》，第33卷，页面5824–5836。Curran Associates, Inc.，2020年。'
- en: '[262] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
    arXiv:1605.07146, 2016.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] Sergey Zagoruyko 和 Nikos Komodakis. 宽残差网络。arXiv预印本 arXiv:1605.07146，2016年。'
- en: '[263] Julian Zaidi, Jonathan Boilard, Ghyslain Gagnon, and Marc-André Carbonneau.
    Measuring Disentanglement: A Review of Metrics. arXiv:2012.09276 [cs], January
    2021.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] Julian Zaidi, Jonathan Boilard, Ghyslain Gagnon 和 Marc-André Carbonneau.
    测量解耦：度量回顾。arXiv:2012.09276 [cs]，2021年1月。'
- en: '[264] Quanshi Zhang, Yu Yang, Qian Yu, and Ying Nian Wu. Network Transplanting.
    arXiv:1804.10272 [cs, stat], December 2018.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] Quanshi Zhang, Yu Yang, Qian Yu 和 Ying Nian Wu. 网络移植。arXiv:1804.10272
    [cs, stat]，2018年12月。'
- en: '[265] Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions
    on Knowledge and Data Engineering, 2021.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] Yu Zhang 和 Qiang Yang. 多任务学习综述。IEEE知识与数据工程学报，2021年。'
- en: '[266] Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-Learning Symmetries by
    Reparameterization. arXiv:2007.02933 [cs, stat], October 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] Allan Zhou, Tom Knowles 和 Chelsea Finn. 通过重参数化进行元学习对称性。arXiv:2007.02933
    [cs, stat]，2020年10月。'
- en: '[267] Tianyi Zhou, Shengjie Wang, and Jeff A Bilmes. Diverse ensemble evolution:
    Curriculum data-model marriage. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
    N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing
    Systems, volume 31\. Curran Associates, Inc., 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] Tianyi Zhou, Shengjie Wang 和 Jeff A Bilmes. 多样化的集成进化：课程数据-模型结合。在S. Bengio,
    H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi 和 R. Garnett编辑的《神经信息处理系统进展》，第31卷，Curran
    Associates, Inc.，2018年。'
- en: '[268] Zhi-Hua Zhou. Ensemble Methods: Foundations and Algorithms. CRC press,
    2012.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] Zhi-Hua Zhou. 集成方法：基础与算法。CRC出版社，2012年。'
- en: '[269] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired
    Image-to-Image Translation using Cycle-Consistent Adversarial Networks. In Computer
    Vision (ICCV), 2017 IEEE International Conference On, 2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] Jun-Yan Zhu, Taesung Park, Phillip Isola 和 Alexei A Efros. 使用循环一致对抗网络进行未配对的图像到图像翻译。在计算机视觉（ICCV），2017
    IEEE国际会议上，2017年。'
- en: '[270] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning
    Transferable Architectures for Scalable Image Recognition. arXiv:1707.07012 [cs,
    stat], April 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] Barret Zoph, Vijay Vasudevan, Jonathon Shlens 和 Quoc V. Le. 学习可转移的架构以实现可扩展的图像识别。arXiv:1707.07012
    [cs, stat]，2018年4月。'
