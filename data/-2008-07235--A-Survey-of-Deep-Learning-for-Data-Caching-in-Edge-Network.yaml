- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:59:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年09月06日 19:59:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2008.07235] A Survey of Deep Learning for Data Caching in Edge Network'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2008.07235] 关于边缘网络数据缓存的深度学习综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2008.07235](https://ar5iv.labs.arxiv.org/html/2008.07235)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2008.07235](https://ar5iv.labs.arxiv.org/html/2008.07235)
- en: A Survey of Deep Learning for Data Caching in Edge Network
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于边缘网络数据缓存的深度学习综述
- en: Yantong Wang ¹ and Vasilis Friderikos ²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yantong Wang ¹ 和 Vasilis Friderikos ²
- en: Center for Telecommunications Research
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 电信研究中心
- en: Department of Engineering, King’s College London
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦国王学院工程系
- en: London, WC2R 2LS, U.K.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦，邮编 WC2R 2LS，英国
- en: ¹ yantong.wang@kcl.ac.uk
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ yantong.wang@kcl.ac.uk
- en: ² vasilis.friderikos@kcl.ac.uk
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ² vasilis.friderikos@kcl.ac.uk
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The concept of edge caching provision in emerging 5G and beyond mobile networks
    is a promising method to deal both with the traffic congestion problem in the
    core network as well as reducing latency to access popular content. In that respect
    end user demand for popular content can be satisfied by proactively caching it
    at the network edge, i.e, at close proximity to the users. In addition to model
    based caching schemes learning-based edge caching optimizations has recently attracted
    significant attention and the aim hereafter is to capture these recent advances
    for both model based and data driven techniques in the area of proactive caching.
    This paper summarizes the utilization of deep learning for data caching in edge
    network. We first outline the typical research topics in content caching and formulate
    a taxonomy based on network hierarchical structure. Then, a number of key types
    of deep learning algorithms are presented, ranging from supervised learning to
    unsupervised learning as well as reinforcement learning. Furthermore, a comparison
    of state-of-the-art literature is provided from the aspects of caching topics
    and deep learning methods. Finally, we discuss research challenges and future
    directions of applying deep learning for caching.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘缓存在新兴的5G及其后续移动网络中的应用是解决核心网络拥塞问题和降低访问热门内容时延的一种有前景的方法。在这方面，通过在网络边缘，即接近用户的地方，主动缓存热门内容来满足终端用户对热门内容的需求。除了基于模型的缓存方案之外，基于学习的边缘缓存优化方案近年来引起了广泛关注。本文总结了利用深度学习进行边缘网络数据缓存的应用。我们首先概述内容缓存的典型研究主题，并基于网络层次结构制定了一个分类系统。然后，我们介绍了一些关键类型的深度学习算法，从监督学习到无监督学习以及强化学习。此外，我们还提供了从缓存主题和深度学习方法两个方面的现有文献的比较。最后，我们讨论了应用深度学习进行缓存的研究挑战和未来方向。
- en: '*K*eywords deep learning  $\cdot$ content caching  $\cdot$ network optimization
     $\cdot$ edge network'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 深度学习  $\cdot$ 内容缓存  $\cdot$ 网络优化  $\cdot$ 边缘网络'
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Undoubtedly, future 5G and beyond mobile communication networks will have to
    address stringent requirements of delivering popular content at ultra high speeds
    and low latency due to the proliferation of advanced mobile devices and data rich
    applications. In that ecosystem, edge-caching has received significant research
    attention over the last decade as an efficient technique to reduce delivery latency
    and network congestion especially during peak-traffic times or during unexpected
    network congestion episodes by bringing popular data closer to the end users.
    One of the main reasons of enabling edge caching in the network is to reduce the
    number of requests that traverse the access and core mobile network as well as
    reducing the load at the origin servers that would have to, otherwise, respond
    to all requests directly in absence of edge caching. In that case popular content
    and objects can be stored and served from edge locations, which are closer to
    the end users. This operation is also beneficial from the end user perspective
    since edge caching can dramatically reduce the overall latency to access the content
    and increase in the sense overall user experience. It is also important to note
    that the notion of popular content means that the requests of top 10% of video
    content on the Internet account for almost 80% of all traffic; which relates to
    multiple requests from different end users of the same content [[1](#bib.bib1)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，未来的5G及更高级的移动通信网络必须满足严格的要求，以超高速和低延迟传输热门内容，这是由于先进移动设备和数据丰富的应用程序的普及。在这种生态系统中，边缘缓存作为一种有效的技术，得到了过去十年的显著研究关注，它可以通过将热门数据更接近终端用户，从而减少交付延迟和网络拥堵，特别是在高峰流量时段或意外网络拥堵时。启用边缘缓存的主要原因之一是减少穿越接入和核心移动网络的请求数量，同时减少原始服务器的负载，否则这些服务器必须直接响应所有请求，如果没有边缘缓存的话。在这种情况下，热门内容和对象可以存储并从接近终端用户的边缘位置提供。这种操作对于终端用户来说也是有益的，因为边缘缓存可以显著减少访问内容的整体延迟，并提高整体用户体验感。还需要注意的是，热门内容的概念意味着，互联网上前10%的视频内容请求占据了几乎80%的所有流量；这涉及到来自不同终端用户对相同内容的多个请求[[1](#bib.bib1)]。
- en: 'Recently, deep learning (DL) has attracted significant attention from both
    academia and industry and has been applied to diverse domains like self-driving,
    medical diagnosis, playing complex games such as Go [[2](#bib.bib2)]. DL has also
    made their way into communication areas [[3](#bib.bib3)]. In this paper, we pay
    attention to the application of DL in caching policy. Though there are some earlier
    surveys related to machine learning applications, they either focus on general
    machine learning techniques for caching [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)],
    or concentrate on overall wireless applications [[7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. The work [[3](#bib.bib3)] provides a big picture of applying
    machine learning in wireless communications. In [[4](#bib.bib4)], the authors
    consider the machine learning on both caching and routing strategy. A comprehensive
    survey on machine learning applications for caching content in edge networks is
    provided in [[5](#bib.bib5)]. The researchers [[6](#bib.bib6)] provide a survey
    about machine learning on mobile edge caching and communication resources. On
    the other hand, [[7](#bib.bib7)] overviews how artificial neural networks can
    be employed for various wireless network problems. The authors in [[8](#bib.bib8)]
    detail a survey on deep reinforcement learning (DRL) for issues in communications
    and networking. [[9](#bib.bib9)] presents a comprehensive on deep learning applications
    and edge computing paradigm. Our work can be distinguished from the aforementioned
    papers based on the fact that we focus on the deep learning techniques on content
    caching and both wired and wireless caching are taken into account. Our main contributions
    are listed as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习（DL）引起了学术界和工业界的广泛关注，并已应用于自动驾驶、医学诊断、围棋等复杂游戏 [[2](#bib.bib2)]。深度学习还进入了通信领域
    [[3](#bib.bib3)]。在本文中，我们关注深度学习在缓存策略中的应用。尽管有一些早期的关于机器学习应用的调查，但它们要么关注于用于缓存的一般机器学习技术
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]，要么集中于整体无线应用 [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]。工作 [[3](#bib.bib3)] 提供了机器学习在无线通信中应用的整体视图。在 [[4](#bib.bib4)]
    中，作者考虑了机器学习在缓存和路由策略中的应用。 [[5](#bib.bib5)] 提供了关于机器学习在边缘网络中缓存内容的应用的全面调查。研究人员 [[6](#bib.bib6)]
    提供了关于移动边缘缓存和通信资源的机器学习调查。另一方面， [[7](#bib.bib7)] 概述了人工神经网络如何用于各种无线网络问题。 [[8](#bib.bib8)]
    详细介绍了深度强化学习（DRL）在通信和网络问题中的应用。 [[9](#bib.bib9)] 提出了深度学习应用和边缘计算范式的全面概述。我们的工作与上述论文不同，因为我们专注于深度学习技术在内容缓存中的应用，并考虑了有线和无线缓存。我们的主要贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We classify the content caching problem into Layer 1 Caching and Layer 2 Caching.
    Each layer caching consists of four tightly coupled subproblems: where to cache,
    what to cache, cache dimensioning and content delivery. Related researches are
    provided accordingly.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将内容缓存问题分为第1层缓存和第2层缓存。每一层缓存包含四个紧密相关的子问题：缓存位置、缓存内容、缓存维度和内容传递。相关的研究也随之提供。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present the fundamentals of DL techniques which are widely used in content
    caching, such as convolutional neural network, recurrent neural network, actor-critic
    model based deep reinforcement learning, etc.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了在内容缓存中广泛使用的深度学习（DL）技术的基础知识，如卷积神经网络、递归神经网络、基于演员-评论家模型的深度强化学习等。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyze a broad range of state-of-the-art literature which use DL to content
    caching. These papers are compared based on the DL structure, layer caching coupled
    subproblems and the objective of DL in each scenarios. Then we discuss research
    challenges and potential directions for the utilization of DL in caching.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了广泛的前沿文献，这些文献使用深度学习进行内容缓存。这些论文基于深度学习结构、层缓存相关子问题和每种场景中深度学习的目标进行比较。然后我们讨论了深度学习在缓存中应用的研究挑战和潜在方向。
- en: '![Refer to caption](img/9493f7dba2702916c3de1d1b5d24b77e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9493f7dba2702916c3de1d1b5d24b77e.png)'
- en: 'Figure 1: Survey Architecture'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：调查架构
- en: The rest of this survey is organized as follows (as illustrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning for Data Caching in Edge
    Network") ). Section [2](#S2 "2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") presents the categories of content caching
    problem. Section [3](#S3 "3 Deep Learning Outline ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") reviews typical deep neural network structures.
    In Section [4](#S4 "4 Deep Learning for Data Caching ‣ A Survey of Deep Learning
    for Data Caching in Edge Network"), we list state-of-the-art DL-based caching
    strategies and their comparison. Section [5](#S5 "5 Research Challenges and Future
    Directions ‣ A Survey of Deep Learning for Data Caching in Edge Network") debates
    challenges as well as potential research directions. In the end, Section [6](#S6
    "6 Conclusions ‣ A Survey of Deep Learning for Data Caching in Edge Network")
    concludes this paper. For better readability, the abbreviations in this paper
    is listed as Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") shows.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分组织如下（如图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 边缘网络数据缓存深度学习调查") 所示）。第 [2](#S2 "2 数据缓存综述
    ‣ 边缘网络数据缓存深度学习调查") 节介绍了内容缓存问题的类别。第 [3](#S3 "3 深度学习概要 ‣ 边缘网络数据缓存深度学习调查") 节回顾了典型的深度神经网络结构。在第
    [4](#S4 "4 数据缓存的深度学习 ‣ 边缘网络数据缓存深度学习调查") 节中，我们列出了最先进的基于深度学习的缓存策略及其比较。第 [5](#S5
    "5 研究挑战与未来方向 ‣ 边缘网络数据缓存深度学习调查") 节讨论了挑战以及潜在的研究方向。最后，第 [6](#S6 "6 结论 ‣ 边缘网络数据缓存深度学习调查")
    节总结了本文。为了更好的可读性，本文中的缩写列表见表 [1](#S1.T1 "表 1 ‣ 1 介绍 ‣ 边缘网络数据缓存深度学习调查")。
- en: 'Table 1: List of Abbreviations.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 缩写列表。'
- en: '| Abbr. | Description | Abbr. | Description |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 缩写 | 描述 | 缩写 | 描述 |'
- en: '| 3C | Computing, Caching and Communication | A3C | Asynchronous Advantage
    Actor-Critic |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 3C | 计算、缓存与通信 | A3C | 异步优势演员-评论家 |'
- en: '| BBU | Baseband Unit | CCN | Content-Centric Network |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| BBU | 基带单元 | CCN | 内容中心网络 |'
- en: '| CNN | Convolutional Neural Network | CoMP-JT | Coordinated Multi Point Joint
    Transmission |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 | CoMP-JT | 协调多点联合传输 |'
- en: '| CR | Content Router | C-RAN | Cloud-Radio Access Network |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| CR | 内容路由器 | C-RAN | 云无线接入网络 |'
- en: '| CSI | Channel State Information | D2D | Device to Device |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| CSI | 信道状态信息 | D2D | 设备到设备 |'
- en: '| DDPG | Deep Deterministic Policy Gradient | DL | Deep Learning |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| DDPG | 深度确定性策略梯度 | DL | 深度学习 |'
- en: '| DNN | Deep Neural Network | DQN | Deep Q Network |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 深度神经网络 | DQN | 深度Q网络 |'
- en: '| DRL | Deep Reinforcement Learning | DT | Digital Twin |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| DRL | 深度强化学习 | DT | 数字双胞胎 |'
- en: '| ED | End Device | ES | Edge Server |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ED | 终端设备 | ES | 边缘服务器 |'
- en: '| ETSI | European Telecommunication Standardization Institute |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ETSI | 欧洲电信标准化协会 |'
- en: '| ESN | Echo-State Network | FIFO | First In First Out |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| ESN | 回声状态网络 | FIFO | 先进先出 |'
- en: '| FNN | Feedforward Neural Network | FBS | Femto Base Station |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| FNN | 前馈神经网络 | FBS | 雌基站 |'
- en: '| ICN | Information-Centric Network | LFU | Least Frequently Used |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ICN | 信息中心网络 | LFU | 最少频繁使用 |'
- en: '| LP | Linear Programming | LRU | Least Recently Used |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| LP | 线性规划 | LRU | 最少使用 |'
- en: '| LSTM | Long Short-Term Memory | MAR | Mobile Augmented Reality |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 长短期记忆 | MAR | 移动增强现实 |'
- en: '| MD | Mobile Device | MILP | Mixed Integer Linear Programming |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| MD | 移动设备 | MILP | 混合整数线性规划 |'
- en: '| MBS | Macro Base Station | NFV | Network Function Virtualization |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| MBS | 宏基站 | NFV | 网络功能虚拟化 |'
- en: '| PNF | Physical Network Function | PPO | Proximal Policy Optimization |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| PNF | 物理网络功能 | PPO | 近端策略优化 |'
- en: '| QoE | Quality of Experience | RL | Reinforcement Learning |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| QoE | 体验质量 | RL | 强化学习 |'
- en: '| RNN | Recurrent Neural Network | RRH | Remote Radio Head |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 循环神经网络 | RRH | 遥控无线头 |'
- en: '| SAE | Sparse Auto Encoder | SDN | Software Defined Network |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| SAE | 稀疏自编码器 | SDN | 软件定义网络 |'
- en: '| seq2seq | Sequence to Sequence | SNM | Shot Noise Model |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| seq2seq | 序列到序列 | SNM | 射线噪声模型 |'
- en: '| TRPO | Trust Region Policy Optimization | TTL | Time to Live |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| TRPO | 信任区域策略优化 | TTL | 生存时间 |'
- en: '| VNF | Virtual Network Function | WSN | Wireless Sensor Network |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| VNF | 虚拟网络功能 | WSN | 无线传感器网络 |'
- en: 2 Data Caching Review
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据缓存综述
- en: 'The paradigm of data caching in edge networks is illustrated in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Data Caching Review ‣ A Survey of Deep Learning for Data Caching
    in Edge Network"). Similarity to [[10](#bib.bib10)], the scope of edge in this
    paper is along the path between end user and data server, which contains Content
    Router (CR), Macro Base Station (MSB), Femto Base Station (FSB) and End Device
    (ED). In the context of Cloud-Radio Access Network (C-RAN)[[11](#bib.bib11)],
    both baseband unit (BBU) and remote radio head (RRH) are considered as potential
    caching candidates to hosting content, where the BBUs are clustered as a BBU pool
    centrally and RRHs are deployed near BS’s antenna distributively. According to
    the hierarchical structure of edge network, the data caching is classified into
    two categories: Layer 1 Caching and Layer 2 Caching. In this section we illustrate
    the typical research topics in these two areas.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘网络中的数据缓存模式在图[2](#S2.F2 "Figure 2 ‣ 2 Data Caching Review ‣ A Survey of Deep
    Learning for Data Caching in Edge Network")中有所说明。与[[10](#bib.bib10)]类似，本文中边缘的范围包括在最终用户和数据服务器之间的路径，其中包含内容路由器（CR）、宏基站（MSB）、微基站（FSB）和终端设备（ED）。在云无线接入网络（C-RAN）[[11](#bib.bib11)]的背景下，基带单元（BBU）和远程无线头（RRH）都被视为潜在的缓存候选者来托管内容，其中BBU集中成一个BBU池，而RRH则分布在基站的天线附近。根据边缘网络的层级结构，数据缓存分为两类：层级
    1 缓存和层级 2 缓存。本节我们将说明这两个领域中的典型研究课题。
- en: '![Refer to caption](img/6f5024e4294f7d10008b934d8a03e5fc.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6f5024e4294f7d10008b934d8a03e5fc.png)'
- en: 'Figure 2: Data Caching in Edge Network'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 边缘网络中的数据缓存'
- en: 2.1 Layer 1 Caching
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 层级 1 缓存
- en: 'In Layer 1 Caching, the popular content is considered to be hosted in CRs.
    In the context of Information-Centric Network (ICN), CR plays dual roles both
    as a typical router (i.e. data flow forwarding) and content store (i.e. local
    area data caching facility). Generally, the CR is connected via wired networks.
    Layer 1 Caching consists of four tightly coupled problems: where to cache, what
    to cache, cache dimensioning and content delivery[[12](#bib.bib12)].'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在层级 1 缓存中，热门内容被认为应托管在CR中。在信息中心网络（ICN）的背景下，CR既作为典型路由器（即数据流转发），又作为内容存储（即本地数据缓存设施）。通常，CR通过有线网络连接。层级
    1 缓存包括四个紧密相关的问题：缓存位置、缓存内容、缓存规模和内容传递[[12](#bib.bib12)]。
- en: 'Where to cache focuses on selecting the proper CRs to host the content. For
    instance, in Figure [2](#S2.F2 "Figure 2 ‣ 2 Data Caching Review ‣ A Survey of
    Deep Learning for Data Caching in Edge Network"), contents replicas can be placed
    in lower hierarchical level CRs, such as router B and C, as the mean of reducing
    transmission cost but with extra cost pay for hosting contents; reversely, consolidating
    caching in CR A can be adopted to saving caching cost at the expense of more transmission
    cost and has the risk of expiring end users’ delay requirement. Here the caching/hosting
    cost is the cost to deploy the content, which could be measured by space utilization,
    energy consumption or other metrics. The transmission cost represents the price
    for delivering the content from cached CR (or data server) to end user and is
    basically estimated via the number of hops. Where to cache problem usually has
    been modelled as a Mixed Integer Linear Programming (MILP):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的位置集中于选择合适的内容路由器（CR）来托管内容。例如，在图[2](#S2.F2 "Figure 2 ‣ 2 Data Caching Review
    ‣ A Survey of Deep Learning for Data Caching in Edge Network")中，内容副本可以放置在较低层次的CR中，如路由器B和C，以减少传输成本，但需要额外的托管成本；相反，集中缓存于CR
    A可以节省缓存成本，但会增加传输成本，并有超出最终用户延迟要求的风险。在这里，缓存/托管成本是部署内容的成本，可以通过空间利用率、能源消耗或其他指标来衡量。传输成本表示将内容从缓存CR（或数据服务器）传递到最终用户的费用，通常通过跳数来估算。缓存位置问题通常被建模为混合整数线性规划（MILP）：
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\mathop{\min}_{\begin{subarray}{c}x\end{subarray}}\;$ |
    $\displaystyle c^{T}x$ |  | (1a) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathop{\min}_{\begin{subarray}{c}x\end{subarray}}\;$ |
    $\displaystyle c^{T}x$ |  | (1a) |'
- en: '|  | s.t. | $\displaystyle Ax\leq b$ |  | (1b) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | 满足 | $\displaystyle Ax\leq b$ |  | (1b) |'
- en: '|  |  | $\displaystyle x\in\{0,1\}$ |  | (1c) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle x\in\{0,1\}$ |  | (1c) |'
- en: '|  | or | $\displaystyle x\geq 0$ |  | (1d) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | 或 | $\displaystyle x\geq 0$ |  | (1d) |'
- en: where $x$ is the decision variable. Normally it is a binary variable indicating
    the CR assignment. In special cases, with the aim of modelling or linearization,
    some non-binary auxiliary variables are introduced as constraint ([1d](#S2.E1.4
    "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network")) shows. If taking caching a part of a file
    not the complete into consideration, the decision variable $x$ is a continuous
    variable representing the segments host in the CR, then constraint ([1c](#S2.E1.3
    "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network")) becomes $x\in[0,1]$ and MILP model turns to
    linear programming (LP). There are many work allocating contents via MILP with
    different objectives and limitations. The authors in [[13](#bib.bib13)] propose
    a model to minimize the user delay and load balancing level of CRs with the satisfaction
    of cache space. The work in [[14](#bib.bib14)] considers a trade-off between caching
    and transmission cost with cache space, link bandwidth and user latency constraints.
    In [[15](#bib.bib15)], an energy efficient optimization model is constructed consisting
    of caching energy and transport energy. [[16](#bib.bib16)] provides more details
    of mathematical model and related heuristic algorithms in caching deployment of
    wired networks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是决策变量。通常它是一个二进制变量，用于指示 CR 分配。在特殊情况下，为了建模或线性化，引入了一些非二进制的辅助变量作为约束，如[1d](#S2.E1.4
    "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network")所示。如果考虑缓存文件的部分而非完整文件，决策变量 $x$ 就是一个连续变量，表示 CR
    中的段，那么约束条件[1c](#S2.E1.3 "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣
    A Survey of Deep Learning for Data Caching in Edge Network") 变为 $x\in[0,1]$，MILP
    模型转化为线性规划 (LP)。许多工作通过 MILP 分配内容，具有不同的目标和限制。[[13](#bib.bib13)] 的作者提出了一种模型，以在满足缓存空间的条件下，最小化用户延迟和
    CR 的负载均衡水平。[[14](#bib.bib14)] 的工作考虑了缓存与传输成本之间的权衡，同时满足缓存空间、链路带宽和用户延迟的约束条件。在[[15](#bib.bib15)]中，构建了一个能源效率优化模型，包括缓存能源和传输能源。[[16](#bib.bib16)]
    提供了有关有线网络缓存部署的数学模型和相关启发式算法的更多细节。
- en: 'What to cache concentrates on selecting the proper contents in CRs for the
    purpose of maximizing the cache hit ratio. Via exploiting the statistical patterns
    of user requests, the popularity of requested information and user preference
    can be forecasted and play a very significant role in determining caching content.
    On the one hand, from the view of aggregated request contents, researchers propose
    many different models and algorithms for popularity estimation. One widely used
    model in web caching is the Zipf model based the assumption that the content popularity
    is static and each users’ request is independent[[17](#bib.bib17)]. However, this
    method fails to reflect the temporal and spatial correlations of the content,
    where the temporal correlation reflects the popularity varies over time and the
    spatial correlation represents the content preference is different on the geographical
    area and social cultural media. A temporal model named the shot noise model (SNM)
    is built in [[18](#bib.bib18)] which enables users to estimate the content popularity
    dynamically. Inspired by SNM, the work in [[19](#bib.bib19)] considers both spatial
    and temporal characteristics during caching decision. On the other hand, from
    the view of a specific end user during a certain period, caching his/her preference
    content (may not be the popular in network) can also help to reduce the traffic
    flow. Many approaches in recommendation systems can be applied in this case[[20](#bib.bib20)].
    Another aspect of what to cache problem is the designing of cache eviction strategies
    when storage space faces the risk of overflow. Depending on the life of caching
    contents, these policies can be divided into two categories roughly: one is like
    first in first out (FIFO), least frequently used (LFU), least recently used (LRU)
    and randomized replacement, the contents would not be removed until no more memory
    is available; the other one is called time to live (TTL) strategy, where the eviction
    happens once the related timer expires. [[21](#bib.bib21)] presents analytic model
    for hit ratio in TTL-based cache requested by independent and identically distributed
    flows. It worth noting that in [[21](#bib.bib21)], the TTL-based cache policy
    is used for the consistency of dynamic contents instead of contents replacement.
    In [[22](#bib.bib22)], the authors introduce a TTL model for cache eviction and
    the timer is reset once related content cache hit happens.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 关于缓存内容的选择集中在选择适当的 CR 内容，以最大化缓存命中率。通过利用用户请求的统计模式，可以预测请求信息的流行程度和用户偏好，这在确定缓存内容中发挥着非常重要的作用。一方面，从汇总请求内容的角度来看，研究人员提出了许多不同的流行度估计模型和算法。在
    Web 缓存中，广泛使用的一种模型是基于内容流行度静态且每个用户请求独立的假设的 Zipf 模型[[17](#bib.bib17)]。然而，这种方法未能反映内容的时间和空间相关性，其中时间相关性反映了流行度随时间变化，而空间相关性表示地理区域和社会文化媒体上的内容偏好不同。[[18](#bib.bib18)]
    中建立了一种称为短噪声模型（SNM）的时间模型，它使用户能够动态估计内容流行度。受到 SNM 启发，[[19](#bib.bib19)] 的工作在缓存决策过程中考虑了空间和时间特征。另一方面，从特定终端用户在某一时间段的角度来看，缓存其/她的偏好内容（可能在网络上不流行）也有助于减少流量。在这种情况下，许多推荐系统中的方法可以应用[[20](#bib.bib20)]。缓存内容面对存储空间溢出风险时，另一个方面是设计缓存驱逐策略。根据缓存内容的生命周期，这些策略大致可以分为两类：一种类似于先进先出（FIFO）、最少使用（LFU）、最近最少使用（LRU）和随机替换，内容不会被移除，直到没有更多内存可用；另一种称为生存时间（TTL）策略，其中一旦相关定时器到期即发生驱逐。[[21](#bib.bib21)]
    提出了 TTL 基于缓存的命中率的分析模型，这些缓存由独立同分布的流请求。值得注意的是，在 [[21](#bib.bib21)] 中，TTL 基于缓存策略用于动态内容的一致性，而不是内容替换。在
    [[22](#bib.bib22)] 中，作者介绍了一个用于缓存驱逐的 TTL 模型，并且一旦相关内容缓存命中，定时器会被重置。
- en: 'Cache dimensioning highlights how much storage space to be allocated. Benefit
    from the softwarization and virtualization technologies, the cache size in each
    CR or edge cloud can be managed in a more flexible and dynamical way, which makes
    the cache dimensioning decisions become an important feature in data caching.
    Technically, the cache hit ratio rises with the increasing of cache memory, and
    consequently eases the traffic congestion in the core network. However, excessive
    space allocation would waste the resource like energy to support the caching function.
    Hence there is a trade-off between cache size cost and network congestion. Economically,
    taking such scenario into consideration: a small content provider wants to rent
    service from a CDN provider such as Akamai or Huawei Cloud, and there is also
    a balance between investment saving and network performance. In [[23](#bib.bib23)],
    the proper cache size of individual CR in Content-Centric Network (CCN) is investigated
    via exploiting the network topology. In [[24](#bib.bib24)], the authors consider
    the effect of network traffic distribution and user behaviours when designing
    cache size.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存维度化突出需要分配多少存储空间。得益于软件化和虚拟化技术，每个CR或边缘云中的缓存大小可以以更灵活和动态的方式进行管理，这使得缓存维度化决策成为数据缓存中的一个重要特征。在技术上，缓存命中率随着缓存内存的增加而上升，从而缓解了核心网络中的流量拥堵。然而，过度分配空间会浪费资源，如能源，以支持缓存功能。因此，缓存大小成本和网络拥堵之间存在权衡。从经济角度来看，考虑到这样的情景：一个小型内容提供商希望从CDN提供商（如Akamai或华为云）租赁服务，在投资节省和网络性能之间也存在平衡。在[[23](#bib.bib23)]中，通过利用网络拓扑调查了内容中心网络（CCN）中每个CR的合适缓存大小。在[[24](#bib.bib24)]中，作者在设计缓存大小时考虑了网络流量分布和用户行为的影响。
- en: Content delivery considers how to transform the caching content to the requested
    user. The delivery traffic embraces single cache file downloading and video content
    steaming and the metrics for these two scenarios vary. Regarding file downloading,
    the content cannot be consumed until the delivery is completed. Therefore the
    downloading time of the entire file is viewed as a metric to reflect the quality
    of experience (QoE). For video steaming, especially for those large video splitted
    into several chunks, the delay limitation only works on the first chunk. In that
    case, delivering the first chunk in time and keep the smooth transmission of the
    rest chunks are the key aims [[25](#bib.bib25)]. Apart from those measuring metrics,
    another problem in content delivery is the routing policy. In CCN [[26](#bib.bib26)],
    one implementation of ICN architecture, employs a flooding-based name routing
    protocol to publish the request among cached CRs. On one hand, flooding strategy
    simplifies the designing complexity and reduce the maintaining cost particularly
    in an unstable scenario; on the other hand, it costly wastes bandwidth resources.
    In [[27](#bib.bib27)], the authors discuss the optimal radius in scoped flooding.
    The deliver route is often considered jointly with where to cache problem, in
    which the objective function ([1a](#S2.E1.1 "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data
    Caching Review ‣ A Survey of Deep Learning for Data Caching in Edge Network"))
    includes both deployment and routing cost.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 内容交付考虑如何将缓存内容转换为用户所请求的内容。交付流量包括单一缓存文件下载和视频内容流媒体，这两种情况的指标有所不同。对于文件下载，内容在交付完成之前无法被使用。因此，整个文件的下载时间被视为反映体验质量（QoE）的指标。对于视频流媒体，特别是那些被分成多个块的大型视频，延迟限制仅对第一个块有效。在这种情况下，按时交付第一个块并保持其余块的顺畅传输是关键目标[[25](#bib.bib25)]。除了这些测量指标，内容交付中的另一个问题是路由策略。在CCN
    [[26](#bib.bib26)]中，ICN架构的一种实现，采用基于泛洪的名称路由协议来在缓存的CR之间发布请求。一方面，泛洪策略简化了设计复杂性，并减少了特别是在不稳定情况下的维护成本；另一方面，它会浪费带宽资源。在[[27](#bib.bib27)]中，作者讨论了范围泛洪中的最佳半径。传递路径通常与缓存问题共同考虑，其中目标函数
    ([1a](#S2.E1.1 "In 1 ‣ 2.1 Layer 1 Caching ‣ 2 Data Caching Review ‣ A Survey
    of Deep Learning for Data Caching in Edge Network")) 包括部署和路由成本。
- en: 2.2 Layer 2 Caching
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 第二层缓存
- en: 'Contrast to Layer 1 caching in wired connection, Layer 2 caching considers
    implementing caching techniques in wireless network. Though both of them need
    solve where to cache, what to cache, cache dimensioning and content delivery problems,
    wireless caching is more challenging and some mature strategies in wired caching
    cannot be migrated directly to wireless case. Some reasons come from the listed
    aspects: the resources in wireless environment, such as caching storage and spectrum,
    are limited compared with CRs in Layer 1 Caching; the mobility of end users and
    dynamic network typologies are also required to be considered during the design
    of caching strategies; moreover, the wireless channels are uncertain since they
    can be effected by fading and interference.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与有线连接中的第1层缓存相比，第2层缓存考虑在无线网络中实施缓存技术。尽管两者都需要解决缓存的位置、缓存的内容、缓存的维度以及内容传输等问题，但无线缓存面临的挑战更大，一些成熟的有线缓存策略不能直接迁移到无线场景中。原因包括以下几个方面：无线环境中的资源，如缓存存储和频谱，相较于第1层缓存中的CR（缓存资源）是有限的；用户的移动性和动态的网络拓扑也需要在缓存策略设计中加以考虑；此外，无线信道是不确定的，因为它们可能受到衰落和干扰的影响。
- en: In wireless caching, where to cache focus on finding the proper candidates among
    MBS, FBS, ED, even BBU pool and RRU in C-RAN to host the content. Caching at MBS
    and FBS can alleviate backhaul congestion since end users obtain the requested
    content from BS directly instead of from CR via backhaul links. Compared with
    FBS, MBS has wider coverage and typically, there is no overlap among different
    MBSs [[28](#bib.bib28)]. As mentioned above, the caching space in BSs is limited
    and it is impractical to cache all popular content. With the aim of improving
    cache-hit ratio, a MILP-modelled collaborative caching strategy among MBSs is
    proposed in [[29](#bib.bib29)]. If the accessed MBS does not host the content,
    the request will be served by a neighbour MBS which cache the file rather than
    by the data server. For FBS caching, a distributed caching method is presented
    in [[30](#bib.bib30)] and the main idea is that the ED locating in the FBS coverage
    overlap is able to obtain contents from multiple hosters. Caching at ED can not
    only ease backhaul congestion but also improve the area spectral efficiency [[28](#bib.bib28)].
    When the end user requests a content, he/she would be severed by the local storage
    if the content is precached in his/her ED or by adjacent ED via D2D communication
    if the content is host accordingly. In [[31](#bib.bib31)], the authors model the
    cache-enabled D2D network as a Poisson cluster process, where end users are grouped
    into several clusters and the collective performance is improved. Individually,
    caching the interested contents for other users affects personal benefit. In [[32](#bib.bib32)],
    a Stackelberg game model is applied to formulate the conflict among end users
    and a related incentive mechanism is designed to encourage content sharing. For
    the case of cache-enabled C-RAN, caching at BBU can ease the traffic congestion
    in the backhaul while caching at RRH can reduce the fronthaul communication cost.
    On the other hand, caching all at BBU raises the signaling overhead of BBU pool
    while at RRH weakens the processing capability. Therefore, where to cache the
    content in C-RAN makes a substantial contribution to balancing the signal processing
    capability at the BBU pool and the backhaul/fronthaul costs [[28](#bib.bib28)].
    The work in [[33](#bib.bib33)] investigates caching at RRHs with jointly considering
    cell outage probability and fronthaul utilization. Due to the end users’ mobility,
    the prediction/awareness of user moving behaviour also influence the proper hoster
    selection. There are some researches exploiting user mobility in cache strategy
    designing like [[34](#bib.bib34)] and [[35](#bib.bib35)].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在无线缓存中，缓存的位置主要集中在选择合适的MBS、FBS、ED，甚至是C-RAN中的BBU池和RRU来托管内容。缓存于MBS和FBS可以缓解回程链路的拥塞，因为终端用户直接从BS获取请求的内容，而不是通过回程链路从CR获取。与FBS相比，MBS的覆盖范围更广，且通常不同的MBS之间没有重叠[[28](#bib.bib28)]。如上所述，BS中的缓存空间有限，缓存所有热门内容是不切实际的。为了提高缓存命中率，在[[29](#bib.bib29)]中提出了一种MILP建模的MBS之间的协作缓存策略。如果访问的MBS没有托管该内容，请求将由缓存了该文件的邻近MBS提供，而不是由数据服务器提供。对于FBS缓存，在[[30](#bib.bib30)]中提出了一种分布式缓存方法，主要思想是FBS覆盖重叠中的ED能够从多个主机获取内容。ED的缓存不仅可以缓解回程链路的拥塞，还能提高区域频谱效率[[28](#bib.bib28)]。当终端用户请求内容时，如果内容已经预缓存于他的ED中，则由本地存储提供；如果内容相应地托管在邻近ED中，则通过D2D通信由邻近ED提供。在[[31](#bib.bib31)]中，作者将缓存启用的D2D网络建模为泊松簇过程，其中终端用户被分组为几个簇，集体性能得到提升。个体上，为其他用户缓存感兴趣的内容会影响个人利益。在[[32](#bib.bib32)]中，应用了Stackelberg博弈模型来制定终端用户之间的冲突，并设计了相关激励机制来鼓励内容共享。对于缓存启用的C-RAN，BBU的缓存可以缓解回程链路的流量拥堵，而RRH的缓存可以减少前程通信成本。另一方面，全部缓存于BBU会增加BBU池的信令开销，而在RRH处缓存则会削弱处理能力。因此，在C-RAN中缓存内容的位置对平衡BBU池的信号处理能力和回程/前程成本具有重要作用[[28](#bib.bib28)]。[[33](#bib.bib33)]中的研究探讨了在RRH缓存时，联合考虑小区故障概率和前程利用率。由于终端用户的移动性，用户移动行为的预测/意识也会影响合适的托管选择。一些研究利用用户移动性设计缓存策略，如[[34](#bib.bib34)]和[[35](#bib.bib35)]。
- en: Similar with Layer 1, what to cache decision as well as eviction policy of layer
    2 depends on the accurate prediction on content popularity or user preference
    in proactive caching method. The content popularity contains the feature of temporal
    and spatial correlations, which has already been described in Layer 1 Caching.
    In Layer 2 caching, the proper spatial granularity in popular contents estimation
    needs to take special attentions [[36](#bib.bib36)]. For example, the coverage
    of MBS and FBS are different, which makes the popularity in MBS and FBS are different
    as well. Because the former based on a large number of users’ behaviors but the
    individual may prefer specific content categories. For small cells, the preference
    estimation requires more accurate information like historical data [[28](#bib.bib28)].
    In order to capture the temporal and spatial dynamics of user preference, many
    different deep learning based algorithms are proposed, which will be illustrated
    in Section [4](#S4 "4 Deep Learning for Data Caching ‣ A Survey of Deep Learning
    for Data Caching in Edge Network").
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与第1层类似，第2层的缓存决策以及驱逐策略依赖于对内容受欢迎程度或用户偏好的准确预测，这些都在前向缓存方法中体现。内容受欢迎程度包含了时间和空间相关性的特征，这在第1层缓存中已经描述过。在第2层缓存中，估算热门内容时需要特别注意适当的空间粒度[[36](#bib.bib36)]。例如，MBS和FBS的覆盖范围不同，这使得MBS和FBS的受欢迎程度也有所不同。前者基于大量用户行为，而个体可能偏好特定内容类别。对于小型蜂窝网络，偏好估算需要更准确的信息，如历史数据[[28](#bib.bib28)]。为了捕捉用户偏好的时间和空间动态，提出了许多不同的基于深度学习的算法，这将在第[4](#S4
    "4 Deep Learning for Data Caching ‣ A Survey of Deep Learning for Data Caching
    in Edge Network")节中说明。
- en: Cache dimensioning in Layer 2 Caching has more complicated factors need to be
    considered, not only including the network topology and content popularity as
    Layer 1 Caching, but also containing backhaul transmission status and wireless
    channel features. The proper cache size assignment is studied in the scenario
    of backhaul limited cellular network [[37](#bib.bib37)]. It also provides the
    closed-form boundary of minimum cache size in one cell case. In the case of dense
    wireless network, the work in [[38](#bib.bib38)] quantifies the minimum required
    cache to achieve the linear capacity scaling of network throughput. The authors
    of [[39](#bib.bib39)] also consider the scenario of dense networks. They derive
    the closed-form of the optimal memory size which can reduce the consumption of
    backhaul capacity as well as guarantee wireless QoS.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第2层缓存的缓存维度设计涉及更多复杂因素，需要考虑的不仅包括网络拓扑和内容受欢迎程度，还包括回传传输状态和无线信道特性。研究了在回传有限的蜂窝网络场景下的适当缓存大小分配[[37](#bib.bib37)]。它还提供了单个小区情况下最小缓存大小的封闭形式边界。在密集无线网络的情况下，[[38](#bib.bib38)]中的工作量化了实现网络吞吐量线性容量扩展所需的最小缓存量。[[39](#bib.bib39)]的作者也考虑了密集网络的场景。他们推导出了最佳内存大小的封闭形式，这可以减少回传容量的消耗，同时保证无线服务质量。
- en: 'According to the number of transmitters and receivers, we divide the content
    delivery in Layer 2 caching into three categories: one candidate serves one end
    user, such as unicast and D2D transmission; one candidate serves multiple users
    like multicast; and coordinated delivery including multiple transmitters serve
    one or more receivers like coordinated multi-point joint transmission (CoMP-JT).
    Once the requested content is cached locally, BS can serve the end user via unicast
    or the adjacent device shares the contents by implementing D2D transmission. Concurrent
    transmission has the risk of co-channel interference in dense deployed networks.
    In D2D network, link scheduling is introduced to select subsets of links to transmit
    simultaneously [[28](#bib.bib28)]. With the aim of improving the spectral efficiency,
    multicast is applied in content delivery when serving multiple requests simultaneously
    with the same content. Therefore there is a trade off between spectral efficiency
    and service delay. For the aim of serving more users in one transmission as well
    as higher spectral efficiency, the BS will wait to collect enough requirement
    for the same content which makes the first request a long waiting time. An optimal
    dynamic multicast scheduling is proposed in [[40](#bib.bib40)] to balance these
    two factors. Multicast can also serve multiple requests with different contents.
    In [[41](#bib.bib41)], the authors provides a coded caching scheme which requires
    the communication link is error free and each user caches a part of its own content
    and partial of other users. Then BS multicasts the coded data to all users. Each
    user can decode his own requested content by XOR operation between the received
    data and the precached other users’ file. However, the coding complexity increases
    exponentially as the quantity of end users grows. The CoMP-JT can improve the
    spectral efficiency as well via sharing channel state information (CSI) and contents
    among BSs but it also needs high-capacity backhaul consumption for exchanging
    data. In C-RAN, the BBUs are centralized in the BBU pool, which makes communication
    among BSs very efficiency. [[42](#bib.bib42)] designs CoMP-JT in C-RAN for the
    purpose of minimizing power consumption with limitations of transmission energy,
    link capacity and requested QoS.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据发射器和接收器的数量，我们将第2层缓存中的内容传输分为三类：一个候选者服务于一个终端用户，例如单播和D2D传输；一个候选者服务于多个用户，如多播；以及协调传输，包括多个发射器服务于一个或多个接收器，如协调多点联合传输（CoMP-JT）。一旦请求的内容被本地缓存，基站（BS）可以通过单播服务终端用户，或通过实现D2D传输，邻近设备共享内容。并发传输在密集部署网络中存在同频干扰的风险。在D2D网络中，引入链路调度以选择同时传输的链路子集
    [[28](#bib.bib28)]。为了提高频谱效率，当同时处理多个请求时，多播被应用于内容传输。因此，频谱效率和服务延迟之间存在权衡。为了在一次传输中服务更多用户以及提高频谱效率，基站将等待收集足够的相同内容的需求，这使得首次请求的等待时间较长。[[40](#bib.bib40)]
    提出了一个优化的动态多播调度方案，以平衡这两个因素。多播还可以服务于不同内容的多个请求。在 [[41](#bib.bib41)] 中，作者提供了一种编码缓存方案，该方案要求通信链路无误差，每个用户缓存自己的一部分内容以及其他用户的部分内容。然后基站将编码数据广播给所有用户。每个用户可以通过对接收的数据和预缓存的其他用户文件进行
    XOR 操作来解码自己请求的内容。然而，随着终端用户数量的增长，编码复杂性呈指数级增加。CoMP-JT 通过在基站之间共享频道状态信息（CSI）和内容，也可以提高频谱效率，但它也需要高容量的回传链路来交换数据。在
    C-RAN 中，BBU 被集中在 BBU 池中，这使得基站之间的通信非常高效。[[42](#bib.bib42)] 在 C-RAN 中设计了 CoMP-JT，旨在最小化功耗，并考虑传输能量、链路容量和请求的
    QoS 限制。
- en: 3 Deep Learning Outline
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习概述
- en: 'As Figure [3](#S3.F3 "Figure 3 ‣ 3 Deep Learning Outline ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") shows, some typical deep neural network
    (DNN) methods are stated. These models are classified into three categories depending
    on the training methods: supervised learning, unsupervised learning and reinforcement
    learning.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S3.F3 "Figure 3 ‣ 3 Deep Learning Outline ‣ A Survey of Deep Learning
    for Data Caching in Edge Network") 所示，列出了几种典型的深度神经网络（DNN）方法。这些模型根据训练方法的不同被分为三类：监督学习、无监督学习和强化学习。
- en: '![Refer to caption](img/436e8d028fa2a7f5ceeb406c762bdd24.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/436e8d028fa2a7f5ceeb406c762bdd24.png)'
- en: 'Figure 3: Typical DNN Structures'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：典型的 DNN 结构
- en: 3.1 Feedforward Neural Network (FNN)
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 前馈神经网络（FNN）
- en: FNN is a kind of DNNs whose information propagation direction is forward and
    there is no cycle in neurons. In this paper, the term FNN is used to represent
    fully connected neural network, which indicates the connection between two adjacent
    layers is filled. According to the Universal Approximation Theorem, FNN has the
    ability to approximate any closed and bounded function with enough neurons in
    hidden layer [[43](#bib.bib43)]. The hidden layer is applied to extract features
    of input vector, and then feed the output layer, which works as a classifier.
    Though FNN is very powerful, it gets into trouble when dealing with real-world
    task such as image recognition due to enormous weight parameters (because of fully
    connected) and lack of data augmentation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: FNN是一种信息传播方向为前向且神经元之间没有循环的DNN。在本文中，FNN一词用于表示全连接神经网络，这表明两个相邻层之间的连接是完全的。根据通用逼近定理，FNN具有用隐藏层中的足够神经元逼近任何封闭且有界函数的能力[[43](#bib.bib43)]。隐藏层用于提取输入向量的特征，然后将其送入输出层，输出层作为分类器工作。尽管FNN非常强大，但在处理如图像识别等现实世界任务时，由于庞大的权重参数（因为全连接）和数据增强的缺乏，常常遇到困难。
- en: 3.2 Convolutional Neural Network (CNN)
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 卷积神经网络 (CNN)
- en: For the aim of overcoming the aforementioned drawback of FNN, CNN employs convolution
    and pooling operations, where the former applies sliding convolutional filters
    to the input vector and the later does down sampling, usually via maximum or mean
    pooling. Generally, CNN tends to contain deeper layers and smaller convolutional
    filters, and the structure becomes fully convolutional network [[44](#bib.bib44)],
    reducing the ratio of pooling layers as well as fully connected layers. Taxonomically,
    CNN belongs to FNN and has been broadly employed in image recognition, video analysis,
    natural language processing, etc. Including CNN, one of the limitations of FNN
    is that the output only depends on current input vectors. So it is hard to deal
    with sequential tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服上述FNN的缺点，CNN采用了卷积和池化操作，其中前者将滑动卷积滤波器应用于输入向量，后者进行下采样，通常通过最大池化或均值池化来实现。通常，CNN倾向于包含更深的层次和更小的卷积滤波器，并且结构变成完全卷积网络[[44](#bib.bib44)]，减少了池化层和全连接层的比例。从分类学上讲，CNN属于FNN，并广泛应用于图像识别、视频分析、自然语言处理等领域。包括CNN在内，FNN的一个限制是输出仅依赖于当前输入向量。因此，很难处理顺序任务。
- en: 3.3 Recurrent Neural Network (RNN)
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 递归神经网络 (RNN)
- en: In order to deal with sequential tasks and using historical information, RNN
    employs neurons with self feedback in hidden layers. Unlike the hidden neuron
    in FNN, the output of recurrent neuron depends on both current output of precious
    layer and last hidden state. Compared with FNN approximates any continues functions,
    RNN with Sigmoid activation function can simulate a universal Turing Machine and
    has the ability to solve all computational problems [[45](#bib.bib45)]. It is
    worth noting that RNN has the risk to suffer from long-term dependencies problem
    [[43](#bib.bib43)] including gradient exploding and vanishing. Additionally, RNN
    has more parameters waiting to be trained due to adding recurrent weights. In
    the following, we introduce some RNN variants as Figure [4](#S3.F4 "Figure 4 ‣
    3.3 Recurrent Neural Network (RNN) ‣ 3 Deep Learning Outline ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") shows.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理顺序任务并利用历史信息，RNN在隐藏层中使用带有自反馈的神经元。与FNN中的隐藏神经元不同，递归神经元的输出依赖于前一层的当前输出和上一个隐藏状态。与FNN可以逼近任何连续函数相比，具有Sigmoid激活函数的RNN可以模拟通用图灵机，并具有解决所有计算问题的能力[[45](#bib.bib45)]。值得注意的是，RNN面临长期依赖问题的风险[[43](#bib.bib43)]，包括梯度爆炸和消失。此外，由于添加了递归权重，RNN有更多的参数需要训练。接下来，我们介绍一些RNN变体，如图[4](#S3.F4
    "图4 ‣ 3.3 递归神经网络 (RNN) ‣ 3 深度学习概述 ‣ 边缘网络数据缓存的深度学习调查")所示。
- en: '![Refer to caption](img/92efa15b66cb14c37adb80f9d051ac85.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/92efa15b66cb14c37adb80f9d051ac85.png)'
- en: 'Figure 4: RNN Variants'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：RNN变体
- en: 3.3.1 Echo-State Network (ESN)
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 回声状态网络 (ESN)
- en: As aforementioned, simple RNN contains more parameters in training step, where
    the recurrent weights and input weights are difficult to learn [[43](#bib.bib43)].
    The basic idea of ESN is fixing these two kinds of weights and only learn the
    output weights (as links highlighted in Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Recurrent
    Neural Network (RNN) ‣ 3 Deep Learning Outline ‣ A Survey of Deep Learning for
    Data Caching in Edge Network")). The hidden layer is renamed as reservoir in ESN,
    where the neurons are sparsely connected and the weights are randomly assigned.
    The recurrent weights keep constant so the information of previous moments is
    stored in the reservoir with constant weight like voice echoing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，简单的循环神经网络在训练步骤中包含更多参数，其中递归权重和输入权重都很难学习[[43](#bib.bib43)]。 ESN的基本思想是固定这两种权重，只学习输出权重（如图[4](#S3.F4
    "图 4 ‣ 3.3 循环神经网络(RNN) ‣ 3 深度学习概要 ‣ 边缘网络数据缓存的深度学习调查中的图 4")中突出显示的链接）。 ESN中的隐藏层被重命名为沉积层，其中神经元是稀疏连接的，权重是随机分配的。
    递归权重保持不变，因此上一时刻的信息以恒定的权重存储在类似回声的沉积层中。
- en: 3.3.2 Long Short-Term Memory (LSTM)
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 长短期记忆（LSTM）
- en: 'Recently, an efficient way to cope with long-term dependencies in practical
    is employing gated RNN, including LSTM [[43](#bib.bib43)]. Then we compare with
    the recurrent neuron in simple RNN: Internally LSTM introduces three gates to
    control signal propagation, where input gate $I$ decides the partition of input
    signal to be stored, forget gate $F$ controls ratio of last moment memory to be
    kept until next period (the name "forget gate" may be a little misleading because
    it actually represents the ratio to be remembered) and output gate $O$ influences
    the proportion of current state to be delivered; Externally LSTM has four inputs
    embracing one input signal and three control signals for three gates. All these
    four signals are derived via the calculation of current network input and last
    moment delivered state.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在实际中应对长期依赖的有效方法是使用门控RNN，包括LSTM[[43](#bib.bib43)]。 然后，我们将简单RNN中的递归神经元与之进行比较：内部LSTM引入了三个门来控制信号传播，其中输入门$I$决定要存储的输入信号的分区，遗忘门$F$控制了上一个时刻记忆保留到下一个周期的比例（"遗忘门"的名称可能有点误导，因为实际上它代表了要记住的比例），输出门$O$影响了当前状态要传递的比例；外部LSTM有四个输入，其中包括一个输入信号和三个用于三个门的控制信号。
    所有这四个信号都是通过当前网络输入和上一时刻传递的状态的计算得出的。
- en: 3.3.3 Pointer Network
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 指针网络
- en: A typical application of RNN is converting one sequence to another sequence
    (seq2seq) such as machine translation. Conventionally, the output of seq2seq architecture
    is a probability distribution of output dictionary. However, it cannot deal with
    the problem that the size of output relies on the length of input due to fixed
    output dictionary. In [[46](#bib.bib46)], the authors modify the output to be
    the distribution of input sequence, which is analogous to pointers in C/C++. Pointer
    network has been widely used in text condensation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的一个典型应用是将一个序列转换为另一个序列（seq2seq），例如机器翻译。 传统上，seq2seq架构的输出是输出词典的概率分布。 但是，由于固定输出词典，它无法处理输出大小依赖于输入长度的问题。
    在[[46](#bib.bib46)]中，作者修改输出为输入序列的分布，这类似于C/C++中的指针。 指针网络已经广泛用于文本压缩。
- en: 3.4 Auto Encoder
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 自动编码器
- en: Auto Encoder is a stack of two NNs named encoder and decoder respectively, where
    the former tries to learn the representative characteristics of input and generate
    a related code, and the later reads the code and reconstructs the original input.
    In order to avoid the auto encoder simply copying the input, some restrictions
    are considered like the dimension of code is smaller than input vector [[43](#bib.bib43)].
    The quality of auto encoder can be measured via reconstruction error, which estimates
    the similarity between input and output. In most cases, the auto encoder is used
    for the proper representation of input vector so the decoder part is removed after
    unsupervised training. The code can be employed as input for further deep learning
    models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是两个称为编码器和解码器的NN堆栈，前者试图学习输入的代表性特征并生成相关的代码，而后者读取代码并重新构造原始输入。 为了避免自动编码器简单地复制输入，考虑了一些限制，例如代码的维度比输入向量小[[43](#bib.bib43)]。
    自动编码器的质量可以通过重建错误来衡量，它估计了输入和输出之间的相似性。 在大多数情况下，自动编码器用于适当表示输入向量，因此在无监督训练后解码器部分被移除。
    代码可以作为进一步深度学习模型的输入。
- en: 3.5 Deep Reinforcement Learning (DRL)
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 深度强化学习（DRL）
- en: Reinforcement Learning (RL) is a Markov Decision Process represented by a quintuple
    $\{\mathcal{S,A,P,R},\gamma\}$, where $\mathcal{S}$ is the state space controlled
    by environment; $\mathcal{A}$ is the action space determined by agent; $\mathcal{P}$
    is the state transition function measuring the probability of moving to a new
    state $s_{t+1}$ given previous state $s_{t}$ and action $a_{t}$; $R$ is reward
    function calculated by environment considering state and action; $\gamma$ is a
    discount factor for estimating total reward. During the interaction between agent
    and environment, agent observes current state $s_{t}$ from environment, and then
    takes action $a_{t}$ following its policy $\pi$. The environment moves to a new
    state $s_{t+1}$ stochastically based on $\mathcal{P}(s_{t},a_{t})$ and returns
    a reward $r_{t}$ to agent. The RL’s aim is finding the policy $\pi$ to maximum
    accumulated reward $\sum_{t}\gamma^{t}r_{t}$. In the early stage, RL focuses on
    scenarios whose $\mathcal{S}$ and $\mathcal{A}$ are discrete and limited. So the
    agent can use a table to record these information. Recently, some tasks have enormous
    discrete states and actions such as playing go and even continuous value such
    as self-driving, which makes table recording impractical. In order to solve this,
    DRL combines RL and DL, where RL defines the problem and optimization object;
    DL models the policy and the reward expectation. Depending on the roles of DNN
    in DRL, we classify the DRL into 3 categories as Figure shows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一个由五元组 $\{\mathcal{S,A,P,R},\gamma\}$ 表示的马尔可夫决策过程，其中 $\mathcal{S}$
    是由环境控制的状态空间；$\mathcal{A}$ 是由代理确定的动作空间；$\mathcal{P}$ 是状态转移函数，测量给定先前状态 $s_{t}$ 和动作
    $a_{t}$ 的情况下，转移到新状态 $s_{t+1}$ 的概率；$R$ 是由环境根据状态和动作计算的奖励函数；$\gamma$ 是用于估计总奖励的折扣因子。在代理和环境的交互过程中，代理从环境中观察当前状态
    $s_{t}$，然后按照其策略 $\pi$ 采取动作 $a_{t}$。环境基于 $\mathcal{P}(s_{t},a_{t})$ 随机转移到新状态 $s_{t+1}$
    并返回奖励 $r_{t}$ 给代理。RL 的目标是找到使累计奖励 $\sum_{t}\gamma^{t}r_{t}$ 最大化的策略 $\pi$。在早期阶段，RL
    关注于 $\mathcal{S}$ 和 $\mathcal{A}$ 是离散且有限的场景。因此，代理可以使用表格记录这些信息。最近，一些任务具有大量离散状态和动作，例如围棋，甚至有连续值任务如自动驾驶，这使得表格记录变得不切实际。为了解决这个问题，DRL
    结合了 RL 和 DL，其中 RL 定义问题和优化目标；DL 则对策略和奖励期望进行建模。根据 DNN 在 DRL 中的角色，我们将 DRL 分为 3 类，如图所示。
- en: 3.5.1 DNN as Critic (Value-Based)
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 DNN 作为批评者（基于价值）
- en: 'In value-based method, DNN does not get involved with policy decision but estimates
    the policy performance. Two functions are introduced for the measurement: $V^{\pi}(s)$
    represents the reward expectation of policy $\pi$ starting from state $s$; $Q^{\pi}(s,a)$
    illustrates the reward expectation of policy $\pi$ starting from state $s$ and
    taking action $a$. In addition, $V^{\pi}(s)$ is the expected value of $Q^{\pi}(s,a)$.
    If we can estimate $Q^{\pi}(s,a)$, the policy $\pi$ can also be improved by choosing
    the action $a^{*}$ hold $Q^{\pi}(s,a^{*})\geq V^{\pi}(s)$. So the DNN employed
    in agent is approximating function $Q^{\pi}(s,a)$, where the inputs are state
    $s$ and action $a$ and output is the estimated value $Q^{\pi}(s,a)$. There are
    some representative critic methods like Deep Q Networks (DQN) [[47](#bib.bib47)]
    and its variants Double DQN [[48](#bib.bib48)], Dueling DQN [[49](#bib.bib49)],
    etc.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，DNN 不涉及策略决策，而是评估策略的性能。为了进行测量，引入了两个函数：$V^{\pi}(s)$ 代表从状态 $s$ 开始的策略
    $\pi$ 的奖励期望；$Q^{\pi}(s,a)$ 说明从状态 $s$ 开始并采取动作 $a$ 的策略 $\pi$ 的奖励期望。此外，$V^{\pi}(s)$
    是 $Q^{\pi}(s,a)$ 的期望值。如果我们能够估计 $Q^{\pi}(s,a)$，则可以通过选择动作 $a^{*}$ 使得 $Q^{\pi}(s,a^{*})\geq
    V^{\pi}(s)$ 来改进策略 $\pi$。因此，代理中使用的 DNN 近似函数 $Q^{\pi}(s,a)$，其中输入是状态 $s$ 和动作 $a$，输出是估计值
    $Q^{\pi}(s,a)$。一些具有代表性的批评方法包括 Deep Q Networks (DQN) [[47](#bib.bib47)] 及其变体 Double
    DQN [[48](#bib.bib48)]、Dueling DQN [[49](#bib.bib49)] 等。
- en: 3.5.2 DNN as Actor (Policy-Based)
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 DNN 作为行动者（基于策略）
- en: In policy-based method, DNN gets involved in the action selection directly instead
    of via $Q^{\pi}(s,a)$. The policy can be viewed as an optimization problem, where
    the objective function is maximizing reward expectation and the search space is
    policy space. The input of DNN is current state and output is the probability
    distribution of potential actions. By employing gradient ascent, we can update
    the DNN to provide better action then maximize total reward. Some popular algorithms
    include Trust Region Policy Optimization (TRPO) [[50](#bib.bib50)], Proximal Policy
    Optimization (PPO) [[51](#bib.bib51)].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的方法中，DNN直接参与动作选择，而不是通过$Q^{\pi}(s,a)$。策略可以视为一个优化问题，其中目标函数是最大化奖励期望，搜索空间是策略空间。DNN的输入是当前状态，输出是潜在动作的概率分布。通过使用梯度上升，我们可以更新DNN以提供更好的动作，从而最大化总奖励。一些流行的算法包括信任区域策略优化（TRPO）[[50](#bib.bib50)]，近端策略优化（PPO）[[51](#bib.bib51)]。
- en: 3.5.3 Actor-Critic Model
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 Actor-Critic模型
- en: Generally, compared with policy-based approach, the value-based method is less
    stable and suffer from poor convergence since the policy is derived based on $Q^{\pi}(s,a)$
    approximation. But value-based method is more sample efficient, while policy-based
    method is easier to fall into local optimal solution because the search space
    is vast. The actor-critic model combines these two approaches, i.e. the agent
    contains two DNNs named actor and critic respectively. In each training iteration,
    the actor considers current state $s$ and policy $\pi$ for deciding action $a$.
    Then the environment changes to state $s^{\prime}$ and returns reward $r$. The
    critic updates its own parameters based on the feedback from environment and output
    a mark for the actor’s action. The actor updates the policy $\pi$ depending on
    critic’s mark. Some typical algorithms are proposed recent years like Deep Deterministic
    Policy Gradient (DDPG) [[52](#bib.bib52)] and Asynchronous Advantage Actor-Critic
    (A3C) [[53](#bib.bib53)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，与基于策略的方法相比，基于价值的方法不够稳定且收敛性差，因为策略是基于$Q^{\pi}(s,a)$近似得出的。但基于价值的方法样本效率更高，而基于策略的方法则更容易陷入局部最优解，因为搜索空间很大。Actor-Critic模型结合了这两种方法，即代理包含两个DNN，分别称为actor和critic。在每次训练迭代中，actor考虑当前状态$s$和策略$\pi$以决定动作$a$。然后环境变化到状态$s^{\prime}$并返回奖励$r$。critic基于来自环境的反馈更新自身参数，并为actor的动作输出评分。actor根据critic的评分更新策略$\pi$。近年来提出了一些典型算法，如深度确定性策略梯度（DDPG）[[52](#bib.bib52)]和异步优势actor-critic（A3C）[[53](#bib.bib53)]。
- en: 4 Deep Learning for Data Caching
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习用于数据缓存
- en: 'We divide the studies regarding deep learning for data caching in edge networks
    into four categories depending on the DL tools employed: FNN and CNN; RNN; Auto
    Encoder; DRL. Recently many works utilize more than one DL techniques for jointly
    considered caching problems. For instance, at the beginning we applies a RNN to
    predict content popularity, and then a DRL to find suboptimal solutions of content
    placement for the purpose of reducing time complexity. In such case, we classify
    the related work into DRL since it represents the caching allocation policy. Unless
    mention the caching location (such as CRs, MBSs, FBSs, EDs and BBUs) otherwise,
    the approaches in this section can be utilized for both Layer 1 and Layer 2 caching.
    Table [2](#S4.T2 "Table 2 ‣ 4 Deep Learning for Data Caching ‣ A Survey of Deep
    Learning for Data Caching in Edge Network") summarize some studies of DL for caching.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将边缘网络中关于数据缓存的深度学习研究分为四类，依据所使用的DL工具：FNN 和 CNN；RNN；自动编码器；DRL。最近许多研究结合了多种DL技术来共同解决缓存问题。例如，开始时我们使用RNN预测内容流行度，然后使用DRL寻找内容放置的次优解，以减少时间复杂度。在这种情况下，我们将相关工作归入DRL，因为它代表了缓存分配策略。除非提及缓存位置（如CRs、MBSs、FBSs、EDs
    和 BBUs），否则本节的方法可用于层1和层2缓存。表[2](#S4.T2 "表 2 ‣ 4 深度学习用于数据缓存 ‣ 边缘网络中深度学习用于数据缓存的调查")总结了部分DL缓存研究。
- en: 'Table 2: Summary of Deep Learning for Data Caching'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：深度学习用于数据缓存的总结
- en: '| method | Study | Caching Problem | DL Objective |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 研究 | 缓存问题 | DL 目标 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| FNN and CNN | [[54](#bib.bib54)] | content delivery | reduce feasible region
    of time slot allocation |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| FNN 和 CNN | [[54](#bib.bib54)] | 内容交付 | 减少时间槽分配的可行区域 |'
- en: '| [[55](#bib.bib55)] | where to cache, content delivery | determine MBSs for
    caching & delivery duration |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | 缓存位置，内容交付 | 确定缓存和交付持续时间的MBSs |'
- en: '| [[56](#bib.bib56)] | where to cache, content delivery | nominate proper CRs
    for caching |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| [[56](#bib.bib56)] | 缓存位置、内容传输 | 指定适当的缓存替代物 |'
- en: '| [[57](#bib.bib57)] | where to cache, content delivery | reduce feasible region
    for caching |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| [[57](#bib.bib57)] | 缓存位置、内容传输 | 减少缓存的可行区域 |'
- en: '| [[58](#bib.bib58)] | what to cache | extract video features |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| [[58](#bib.bib58)] | 缓存内容 | 提取视频特征 |'
- en: '| [[59](#bib.bib59)] | what to cache | predict requested content & frequency
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] | 缓存内容 | 预测请求内容与频率 |'
- en: '| [[60](#bib.bib60)] | what to cache | predict requested content |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| [[60](#bib.bib60)] | 缓存内容 | 预测请求内容 |'
- en: '| [[61](#bib.bib61)] | what to cache | predict content popularity |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bib61)] | 缓存内容 | 预测内容流行度 |'
- en: '| RNN | [[62](#bib.bib62), [63](#bib.bib63)] | what to cache | predict requested
    content & user mobility |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| RNN | [[62](#bib.bib62), [63](#bib.bib63)] | 缓存内容 | 预测请求内容与用户移动性 |'
- en: '| [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] | what to cache | predict content popularity |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] | 缓存内容 | 预测内容流行度 |'
- en: '| [[69](#bib.bib69)] | content delivery | reduce traffic load, select optimal
    BS subset |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)] | 内容传输 | 减少流量负载，选择最佳基站子集 |'
- en: '| Auto Encoder | [[70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73),
    [74](#bib.bib74)] | what to cache | predict content popularity |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 自编码器 | [[70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73),
    [74](#bib.bib74)] | 缓存内容 | 预测内容流行度 |'
- en: '| [[75](#bib.bib75)] | what to cache | predict top popular contents |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] | 缓存内容 | 预测最受欢迎的内容 |'
- en: '| DRL | [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85)] | what to cache | decide cache placement |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| DRL | [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85)] | 缓存内容 | 决定缓存位置 |'
- en: '| [[86](#bib.bib86)] | what to cache | decide cache replacement & power allocation
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| [[86](#bib.bib86)] | 缓存内容 | 决定缓存替换与功率分配 |'
- en: '| [[87](#bib.bib87)] | what to cache | predict popularity & searching best
    NN model |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| [[87](#bib.bib87)] | 缓存内容 | 预测流行度与寻找最佳NN模型 |'
- en: '| [[88](#bib.bib88)] | where to cache | decide cache location |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| [[88](#bib.bib88)] | 缓存位置 | 决定缓存位置 |'
- en: '| [[89](#bib.bib89)] | content delivery | users grouping |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bib89)] | 内容传输 | 用户分组 |'
- en: '| [[90](#bib.bib90)] | where to cache, content delivery | decide BS connection,
    computation offloading & caching location |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| [[90](#bib.bib90)] | 缓存位置、内容传输 | 决定基站连接、计算卸载与缓存位置 |'
- en: '| [[91](#bib.bib91)] | what to cache, content delivery | decide caching & bandwidth
    allocation |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [[91](#bib.bib91)] | 缓存内容、内容传输 | 决定缓存与带宽分配 |'
- en: '| [[92](#bib.bib92)] | what to cache, content delivery | decide caching, computing
    offloading & radio resource allocation |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| [[92](#bib.bib92)] | 缓存内容、内容传输 | 决定缓存、计算卸载与无线资源分配 |'
- en: '| [[93](#bib.bib93)] | what to cache, content delivery | decide multicast scheduling
    & caching replacement |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] | 缓存内容、内容传输 | 决定多播调度与缓存替换 |'
- en: '| [[94](#bib.bib94)] | where & what to cache | predict popularity, decide caching
    & task offloading |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] | 缓存位置与内容 | 预测流行度，决定缓存与任务卸载 |'
- en: '| [[95](#bib.bib95)] | where & what to cache, content delivery | predict user
    mobility & content popularity, determine D2D link |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| [[95](#bib.bib95)] | 缓存位置与内容、内容传输 | 预测用户移动性与内容流行度，确定D2D链接 |'
- en: 4.1 FNN and CNN
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 FNN 和 CNN
- en: In [[54](#bib.bib54)], the content delivery problem in wireless network is formulated
    as two MILP optimization models with the aims of minimum delivery time slot and
    energy consumption respectively. Both models consider the data rate for content
    delivery. Considering the computational complexity of solving MILP, a CNN is introduced
    to reduce the feasible region of decision variables, where the input is channel
    coefficients matrix. The FNN in paper [[55](#bib.bib55)] plays a similar role
    as [[54](#bib.bib54)] to simplify the searching space of the content delivery
    optimization model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [[54](#bib.bib54)] 中，无线网络中的内容传输问题被公式化为两个MILP优化模型，目标分别是最小化传输时间槽和能量消耗。两个模型都考虑了内容传输的数据速率。考虑到求解MILP的计算复杂性，引入CNN来减少决策变量的可行区域，其中输入为信道系数矩阵。文献
    [[55](#bib.bib55)] 中的FNN在简化内容传输优化模型的搜索空间方面发挥了类似的作用。 '
- en: For resource allocation problem, the authors of [[96](#bib.bib96)] model it
    as linear sum assignment problems then utilize CNN and FNN to solve the model.
    The idea is extended in [[56](#bib.bib56)] and [[57](#bib.bib57)], where the authors
    consider where to cache problem among potential CRs and content delivery jointly,
    which is modeled as MILP with the aim of balancing caching and transmission cost
    by considering the user mobility, space utilization and bandwidth limitations.
    The cache allocation is viewed as multi-label classification problem and is decomposed
    into several independent sub-problems, where each one correlates with a CNN to
    predict assignment. The input of CNN is a grey-scale image which combines the
    information of user mobility, space and link utilization level. In [[56](#bib.bib56)],
    a hill climbing local search algorithm is provided to improve the performance
    of CNN while in [[57](#bib.bib57)], the prediction of CNN is used to feed a smaller
    MILP model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于资源分配问题，[[96](#bib.bib96)]的作者将其建模为线性和分配问题，然后利用 CNN 和 FNN 解决模型。这个思路在[[56](#bib.bib56)]和[[57](#bib.bib57)]中得到了扩展，作者考虑了在潜在
    CR 和内容传递中的缓存问题，并将其建模为 MILP，目的是通过考虑用户移动性、空间利用和带宽限制来平衡缓存和传输成本。缓存分配被视为多标签分类问题，并被分解为若干独立的子问题，每个子问题都与
    CNN 相关联以预测分配。CNN 的输入是一个灰度图像，结合了用户移动性、空间和链路利用水平的信息。在[[56](#bib.bib56)]中，提供了一种爬山局部搜索算法来提高
    CNN 的性能，而在[[57](#bib.bib57)]中，CNN 的预测结果用于输入一个更小的 MILP 模型。
- en: For these above works [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [96](#bib.bib96)], the FNN or CNN input is extracted from the
    optimization model. The work in [[97](#bib.bib97)] trains a CNN via original graph
    instead of parameters matrix/image, which makes the process human recognizable
    and interpretable. Though the authors take traveling salesman problem not data
    caching as an example, the method can be viewed as a potential research direction.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述工作[[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [96](#bib.bib96)]，FNN 或 CNN 的输入是从优化模型中提取的。工作[[97](#bib.bib97)]通过原始图而不是参数矩阵/图像训练
    CNN，这使得过程更符合人类的识别和解释。尽管作者以旅行商问题而非数据缓存作为示例，但该方法可以视为一个潜在的研究方向。
- en: In [[58](#bib.bib58)], an ILP model is proposed to minimize the backhaul video-data
    type load by determining the portion of cached content in BSs. Considering the
    fact that the mobile users covered by a BS change frequently, therefore predicting
    user preference is unnecessary. Instead, the authors concentrate on the popular
    content in general. At the beginning, a 3D CNN is introduced to extract spatio-temporal
    features of videos. The popularity of new contents without historical information
    is determined via comparing similar video features. The authors of [[59](#bib.bib59)]
    also considers the spatio-temporal features among visiting contents in a mobile
    bus WiFI environment. By exploiting the previous 9 days collecting data, the content
    that the user may visit on the last day and corresponding visiting frequency can
    be forecast. The social property is taken into account in [[60](#bib.bib60)].
    By observing users interests on tweets during 2016 U.S. election, a CNN based
    predicted model can foresee the content category that is most likely to be requested.
    Such kind of content would be cached in MBSs and FBSs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[58](#bib.bib58)]中，提出了一个 ILP 模型，通过确定缓存内容在 BS 中的比例来最小化回程视频数据类型负荷。考虑到被 BS 覆盖的移动用户频繁变化，因此预测用户偏好是不必要的。相反，作者集中于一般的热门内容。首先，引入了一个
    3D CNN 来提取视频的时空特征。通过比较相似的视频特征来确定没有历史信息的新内容的流行度。[[59](#bib.bib59)]的作者还考虑了移动巴士 WiFi
    环境中访问内容的时空特征。通过利用之前 9 天收集的数据，可以预测用户在最后一天可能访问的内容及其访问频率。在[[60](#bib.bib60)]中考虑了社交属性。通过观察用户在
    2016 年美国选举期间对推文的兴趣，基于 CNN 的预测模型可以预见最有可能被请求的内容类别。这种内容将被缓存到 MBS 和 FBS 中。
- en: The work of [[61](#bib.bib61)] examines the role of DNN in caching from another
    aspect. The authors propose a FNN to predict content popularity as a regression
    problem. The results show that FNN outperforms RNN, though the later is believed
    to be effective to solve sequential predictions. Moreover, replacing the FNN by
    a linear estimator does not devalue the performance significantly. The author
    provides explanation that FNN would work better than linear predictor in the case
    of incomplete information, and RNN has more advantages to model the popularity
    prediction as a classification rather than a regression problem.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 文献[[61](#bib.bib61)]的工作从另一个方面审视了DNN在缓存中的作用。作者提出了一个FNN来将内容流行度预测作为回归问题。结果显示，虽然RNN被认为在解决序列预测方面有效，但FNN表现更优。此外，用线性估计器替代FNN并不会显著降低性能。作者解释说，FNN在信息不完整的情况下会比线性预测器表现更好，而RNN在将流行度预测建模为分类问题时比回归问题更具优势。
- en: 4.2 RNN
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 RNN
- en: 'Considering RNN is superior in dealing with sequential tasks, the work [[64](#bib.bib64)]
    applies a bidirectional RNN for online content popularity prediction in mobile
    edge network. Simple RNN’s output depends on previous and current storage, but
    the bidirectional can also take future information into account. The forecast
    model consists three blocks cascadingly: a CNN reads user requests and extract
    features; bidirectional LTSM learns association of requests over time step; FNN
    is added in the end to improve the prediction performance. Then content eviction
    is based on the popularity prediction.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到RNN在处理序列任务方面的优势，文献[[64](#bib.bib64)]应用了双向RNN用于移动边缘网络中的在线内容流行度预测。简单RNN的输出依赖于之前和当前的存储，但双向RNN也能考虑未来的信息。预测模型由三个模块级联组成：CNN读取用户请求并提取特征；双向LSTM学习请求随时间步的关联；最后加入FNN以提高预测性能。然后根据流行度预测进行内容驱逐。
- en: The authors in [[62](#bib.bib62)] utilize ESN to predict both content request
    distribution and end user mobility pattern. The user’s preference is viewed as
    context which links with personal information combining gender, age, job, location,
    etc.. For the request prediction, the input of ESN is user’s information vector
    and the output represent the probability distribution of content. For mobility
    prediction, the input includes historical and present user’s location and the
    output is the expected position for next time duration. Eventually, the prediction
    influences the caching content decisions in BBUs and RRHs for the purpose of minimizing
    traffic load and delay in CRAN. The authors extend their work in [[63](#bib.bib63)]
    by introducing conceptor-based ESN which can split users’ context into different
    patterns and learn them independently. Therefore a more accurate prediction is
    achieved.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 文献[[62](#bib.bib62)]中的作者利用ESN预测内容请求分布和终端用户移动模式。用户的偏好被视为上下文，这些上下文与个人信息（如性别、年龄、职业、位置等）结合。对于请求预测，ESN的输入是用户的信息向量，输出表示内容的概率分布。对于移动预测，输入包括历史和当前用户的位置，输出是下一个时间段的预期位置。最终，预测会影响BBUs和RRHs中的缓存内容决策，以期减少CRAN中的流量负载和延迟。作者在文献[[63](#bib.bib63)]中扩展了他们的工作，引入了基于概念器的ESN，该方法可以将用户上下文分成不同的模式并独立学习。因此，实现了更准确的预测。
- en: In [[65](#bib.bib65)], a caching decision policy named PA-Cache is proposed
    to predict time-variant video popularity for cache eviction when the space is
    full. The temporal content popularity is exploited by attaching every hidden layer
    representation of RNN to an output regression. In order to improve the accuracy,
    hedge backpropagation is introduced during training process which decides when
    and how to adapt the depth of the DNN in an evolving manner. Similarly, the work
    in [[66](#bib.bib66)] also considers caching replacement of video content. A deep
    LSTM network is utilized for popularity prediction consisting of stacking multiple
    LSTM layers and one softmax layer, where the input of the network is request sequence
    data (device, timestamp, location, title of video) without any prepossessing and
    the output is estimated content popularity. Another work concentrates on prediction
    and interactions between user mobility and content popularity can be found [[67](#bib.bib67)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[65](#bib.bib65)]中，提出了一种名为PA-Cache的缓存决策策略，用于在空间满时预测视频流行度以进行缓存驱逐。通过将RNN的每个隐藏层表示附加到输出回归中，利用了时间内容的流行度。为了提高准确性，在训练过程中引入了Hedge
    Backpropagation来动态决定何时以及如何调整DNN的深度。类似地，[[66](#bib.bib66)]中的工作也考虑了视频内容的缓存替换。使用深度LSTM网络进行流行度预测，包括堆叠多个LSTM层和一个softmax层，网络的输入是请求序列数据（设备、时间戳、位置、视频标题），没有任何预处理，输出是估计的内容流行度。另一项工作集中于用户移动性和内容流行度之间的预测和交互，可以在[[67](#bib.bib67)]中找到。
- en: The work of [[68](#bib.bib68)] recognizes the popularity prediction as a seq2seq
    modeling problem and proposes LSTM Encoder-Decoder model. The input vector consists
    of past probabilities where each vectors are calculated during a predefined time
    window. In [[69](#bib.bib69)], the authors focus on caching content delivery with
    the aim of minimizing BSs to cover all requested users, i.e. set cover problem,
    via coded caching. Unlike [[68](#bib.bib68)], an auto encoder is introduced in
    coded caching stage for file conversion to reduce transmission load. In addition,
    a RNN model is employed to select BSs for broadcasting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 文献[[68](#bib.bib68)]将流行度预测视为seq2seq建模问题，并提出了LSTM编码器-解码器模型。输入向量由过去时间窗口内计算的每个向量的概率组成。在[[69](#bib.bib69)]中，作者关注的是通过编码缓存实现缓存内容交付，目的是通过编码缓存来最小化基站（BS）以覆盖所有请求的用户，即集合覆盖问题。与[[68](#bib.bib68)]不同的是，在编码缓存阶段引入了自动编码器进行文件转换以减轻传输负载。此外，还采用了一个RNN模型来选择用于广播的BS。
- en: The paper [[98](#bib.bib98)] shows the potential of RNN in solving where to
    cache problem. In [[98](#bib.bib98)], a task allocation model is formulated as
    a knapsack problem and the decision variables represent the task is processed
    locally in mobile devices (MDs) or remotely in edge servers (ESs). The authors
    design a multi-pointer network structure of 3 RNNs, where 2 encoders encode MDs
    and ESs respectively, 1 decoder demonstrates ES and MD pairing. Considering the
    similarity of where to cache optimization model and knapsack problem, the multi-pointer
    network can be transferred for caching location decision after according parameter
    modifications.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 文献[[98](#bib.bib98)]展示了RNN在解决缓存位置选择问题方面的潜力。在[[98](#bib.bib98)]中，将任务分配模型表述为一个背包问题，决策变量表示任务是在移动设备（MD）中本地处理还是在边缘服务器（ES）中远程处理。作者设计了一个由3个RNN组成的多指针网络结构，其中2个编码器分别对MD和ES进行编码，1个解码器展示ES和MD的配对。考虑到缓存位置优化模型与背包问题的相似性，可以在相应的参数修改后将多指针网络用于缓存位置决策。
- en: 4.3 Auto Encoder
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 自动编码器
- en: Generally, auto encoder is utilized to learn efficient representation or extract
    features of raw data in an unsupervised manner. The work in [[70](#bib.bib70)]
    considers the cache replacement in wireless sensor network (WSN) based on content
    popularity. Considering sparse auto encoder (SAE) can extract representative expression
    of input data, the authors employ a SAE followed by a classifier where the input
    contains collecting user content requests and the output represents the contents
    popularity level. The authors also think about the implementation in a distributed
    way by SDN/NFV technical, i.e. the input layer is deployed on sink node, while
    the rest layers are implemented on the main controller. A related work applying
    auto encoder in 5G network proactive caching can be found in [[71](#bib.bib71)].
    In [[72](#bib.bib72)], two auto encoders are utilized for extracting the features
    of users and content respectively. Then the extracted information is explored
    to estimate popularity at the core network. Similarly, the auto encoder in [[73](#bib.bib73)]
    is for spatio-temporal popularity features extraction and auto encoders work collaboratively
    in [[75](#bib.bib75)] to predict top K popular videos.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，自动编码器用于以无监督的方式学习有效的表示或提取原始数据的特征。文献[[70](#bib.bib70)]考虑了基于内容流行度的无线传感器网络（WSN）缓存替换。鉴于稀疏自动编码器（SAE）能够提取输入数据的代表性表达，作者使用了一个SAE，随后是一个分类器，其中输入包含用户内容请求的收集，输出表示内容的流行度等级。作者还考虑了通过SDN/NFV技术以分布式方式实施，即输入层部署在汇聚节点上，而其余层则实现于主控制器上。有关自动编码器在5G网络主动缓存中应用的相关工作可以参见[[71](#bib.bib71)]。在[[72](#bib.bib72)]中，两个自动编码器分别用于提取用户和内容的特征。然后，提取的信息被用来估计核心网络中的流行度。类似地，[[73](#bib.bib73)]中的自动编码器用于时空流行度特征的提取，而[[75](#bib.bib75)]中的自动编码器协同工作以预测前K个热门视频。
- en: 4.4 DRL
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 DRL
- en: 'The work in [[78](#bib.bib78)] focuses on the cooperative caching policy at
    FBSs with maximum distance separable coding in ultra dense networks. A value-based
    model is utilized to determine caching categories and the content quantity at
    FBSs during off peak duration. The authors in [[79](#bib.bib79)] study the problem
    of caching 360°videos and virtual viewports in FBSs with unknown content popularity.
    The virtual viewport represents the most popular tiles of a 360°video over users’
    population. A DQN is introduced to decide which tiles of a video to be hosted
    and in which quality. Additionally, [[80](#bib.bib80)] employs DQN for content
    eviction decision offering a satisfactory quality of experience and [[81](#bib.bib81)]
    is for the purpose of minimizing energy consumption. In [[82](#bib.bib82)], the
    authors also apply DQN to decide cache eviction in a single BS. Moreover, the
    critic is generated with stacking LSTM and FNN to evaluate Q value and an external
    memory is added for recording learned knowledge. For the purpose of improving
    the prediction accuracy, the Q value update is determined by the similarity of
    estimated value of critic and recording information in the external memory, instead
    of critic domination. The paper [[84](#bib.bib84)] puts forth DQN a two-level
    network caching, where a parent node links with multiple leaf nodes to cache content
    instead of a single BS. In [[76](#bib.bib76)], a DRL framework with Wolpertinger
    architecture [[99](#bib.bib99)] is presented for content caching at BSs. The Wolpertinger
    architecture is based on actor-critic model and performs efficiently in large
    discrete action space. [[76](#bib.bib76)] employs two FNNs working as actor and
    critic respectively, where the former determines requested content is cached or
    not and the later estimates the reward. The whole framework consists two phases:
    in offline phase, these two FNNs are trained in supervised learning; in online
    phase, the critic and actor update via the interaction with environment. The authors
    extend their work to a multi agent actor-critic model for decentralized cooperative
    caching at multiple BSs [[77](#bib.bib77)]. In [[83](#bib.bib83)], an actor-critic
    model is used for solving cache replacement problem, which balance the data freshness
    and communication cost. The aforementioned papers put attention on the network
    performance while ignore the influence of caching on information processing and
    resource consumption. Therefore, authors of [[85](#bib.bib85)] design cache policy
    considering both network performance during content transmission and processing
    efficiency during data consumption. A DQN is employed to determine the number
    of chunks of the requested file to be updated. The paper [[86](#bib.bib86)] investigates
    a joint cache replacement and power allocation optimization problem to minimize
    latency in a downlink F-RAN. A DQN is proposed for finding a suboptimal solution.
    Though [[87](#bib.bib87)] is regarded as solving what to cache problem like [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86)],
    the reinforcement learning approach plays a different role. In [[87](#bib.bib87)],
    a DNN is utilized for content popularity prediction and then a RL is used for
    DNN hyperparameters tuning instead of determining caching content. Therefore the
    action space consists of choosing model architectures (i.e. CNN, LSTM, etc.),
    number of layers and layer configurations.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[[78](#bib.bib78)]的研究集中在超密集网络中的FBSs的协作缓存策略与最大距离可分编码上。使用基于价值的模型来确定FBSs在非高峰期的缓存类别和内容数量。[[79](#bib.bib79)]的作者研究了在FBSs中缓存360°视频和虚拟视口的问题，内容受欢迎程度未知。虚拟视口代表了用户群体中最受欢迎的360°视频的瓦片。引入了DQN来决定视频中要托管的瓦片及其质量。此外，[[80](#bib.bib80)]利用DQN做出内容驱逐决策，提供令人满意的体验质量，而[[81](#bib.bib81)]则旨在最小化能耗。在[[82](#bib.bib82)]中，作者还应用DQN来决定单一BS中的缓存驱逐。此外，通过堆叠LSTM和FNN生成评价者来评估Q值，并添加了外部内存来记录学习到的知识。为了提高预测准确性，Q值更新由评价者的估计值与外部内存中记录的信息的相似性决定，而不是由评价者主导。论文[[84](#bib.bib84)]提出了DQN两级网络缓存，其中一个父节点与多个叶节点连接以缓存内容，而不是单个BS。在[[76](#bib.bib76)]中，提出了一种带有Wolpertinger架构的DRL框架[[99](#bib.bib99)]用于BSs的内容缓存。Wolpertinger架构基于演员-评论家模型，并在大离散动作空间中表现高效。[[76](#bib.bib76)]使用两个FNN分别作为演员和评论家，其中前者决定请求的内容是否被缓存，后者则估计奖励。整个框架分为两个阶段：离线阶段，这两个FNN在监督学习中进行训练；在线阶段，通过与环境的交互更新评论家和演员。作者将他们的工作扩展到一个多智能体演员-评论家模型，用于多个BSs的去中心化协作缓存[[77](#bib.bib77)]。在[[83](#bib.bib83)]中，使用演员-评论家模型来解决缓存替换问题，平衡数据的新鲜度和通信成本。上述论文关注网络性能，但忽视了缓存对信息处理和资源消耗的影响。因此，[[85](#bib.bib85)]的作者设计了一种缓存策略，考虑到内容传输期间的网络性能和数据消耗期间的处理效率。使用DQN来确定请求文件的更新块数量。论文[[86](#bib.bib86)]研究了一个联合缓存替换和功率分配优化问题，以最小化下行F-RAN的延迟。提出了一种DQN用于寻找次优解。虽然[[87](#bib.bib87)]被认为是解决类似于[[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86)]的缓存问题，但强化学习方法发挥了不同的作用。在[[87](#bib.bib87)]中，利用DNN进行内容受欢迎度预测，然后使用RL进行DNN超参数调优，而不是确定缓存内容。因此，动作空间包括选择模型架构（即CNN，LSTM等）、层数和层配置。'
- en: In [[88](#bib.bib88)], the authors generate an optimization model with the aim
    of maximizing network operator’s utility in mobile social networks under the framework
    of mobile edge computing, in network caching and D2D communications (3C). The
    trust value which if estimated through social relationships among users are also
    considered. Then a DQN model is utilized for solving optimization problem, including
    determine video provider and subscriber association, video transcoding offloading
    and the video cache allocation for video providers.. The DQN employs two CNNs
    for training process, where one generates target Q value and the other is for
    estimated Q value. Unlike the conventional DQN, the authors in [[88](#bib.bib88)]
    introduces a dueling structure, i.e. the Q value is not computed in the final
    fully connected layer, but is decomposed into two components and use the summary
    as estimated Q value, which helps achieve a more robust result. The authors also
    consider utilizing dueling DQN model in different scenarios like cache-enabled
    opportunistic interference alignment [[89](#bib.bib89)] and orchestrating 3C in
    vehicular network [[90](#bib.bib90)]. The work [[91](#bib.bib91)] provides a DDPG
    model to cope with continuous valued control decision for 3C in vehicular edge
    networks, which is combined with the idea of DQN and actor-critic model. The DDPG
    structure can be divided into two parts as DQN, one is for estimated Q value and
    the other for target Q value. Each part consists of two DNNs, which play the role
    of actor and critic respectively. The critic updates its parameters like DQN while
    the actor learns policy via deterministic policy gradient approach. The proposed
    DRL is used for deciding content caching/replacement, vehicle organization and
    bandwidth resource assignment on different duration.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[88](#bib.bib88)]中，作者提出了一种优化模型，旨在在移动边缘计算框架下最大化网络运营商在移动社交网络中的效用，涉及网络缓存和D2D通信（3C）。考虑了通过用户之间的社交关系估计的信任值。然后，使用DQN模型来解决优化问题，包括确定视频提供者和订阅者的关联、视频转码卸载和视频缓存分配。DQN使用两个CNN进行训练，一个生成目标Q值，另一个用于估计Q值。与传统的DQN不同，[[88](#bib.bib88)]中的作者引入了一种对战结构，即Q值不在最终的全连接层中计算，而是分解成两个部分，并使用摘要作为估计的Q值，这有助于实现更稳健的结果。作者还考虑在不同的场景中利用对战DQN模型，如缓存启用的机会干扰对齐[[89](#bib.bib89)]和车载网络中的3C协调[[90](#bib.bib90)]。工作[[91](#bib.bib91)]提供了一个DDPG模型来处理车载边缘网络中3C的连续值控制决策，该模型结合了DQN和演员-评论员模型的思想。DDPG结构可以分为两个部分，类似于DQN，一个用于估计Q值，另一个用于目标Q值。每个部分由两个DNN组成，分别扮演演员和评论员的角色。评论员像DQN一样更新其参数，而演员通过确定性策略梯度方法学习策略。所提出的DRL用于决定内容缓存/替换、车辆组织和不同持续时间的带宽资源分配。
- en: The paper [[92](#bib.bib92)] provides an optimization model which takes what
    to cache and content delivery into consideration in the fog-enabled IoT network
    in order to minimize service latency. Since the wireless signals and user requests
    are stochastic, a actor-critic model is engaged where the actor makes decision
    for requesting contents while critic estimates the reward. Specially, the action
    space $S$ consists of decision variables and reward function is a variant of the
    objective function. A caching replacement strategy and dynamic multicast scheduling
    strategy are studied in [[93](#bib.bib93)]. In order to get a suboptimal result,
    an auto encoder is used to approximate the state. Further, a weighted double DQN
    scheme is utilized for avoiding overestimation of Q value. [[94](#bib.bib94)]
    applies a RNN to predict content popularity by collecting historical requests
    and the output represents the popularity in the near future. Then the prediction
    is employed for cooperative caching and computation offloading among MEC servers,
    which is modelled as a ILP problem. For the purpose of solving it efficiently,
    a multi-agent DQN is applied where each user is viewed as an agent. The action
    space consists of task local computing and offloading decision as well as local
    caching and cooperative caching determination. The reward is measured by accumulated
    latency. The agent choose its own action based on current state without cooperation.
    The where to cache, what to cache and content delivery decision of D2D network
    are jointly modelled in [[95](#bib.bib95)]. Two RNNs, ESN and LSTM, are considered
    to predict mobile users’ location and requested content popularity. Then the prediction
    result is used for determining content categories and cache locations. The content
    delivery is formulated as the actor-critic based DRL framework. The state spaces
    include CSI, transmission distances and communication power between requested
    user and other available candidates. The function of DRL is determining the communication
    link among users with the aim of minimizing power consumption and content delay.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[[92](#bib.bib92)]提供了一个优化模型，该模型在雾计算支持的物联网网络中考虑了缓存内容和内容交付，以最小化服务延迟。由于无线信号和用户请求是随机的，因此采用了演员-评论员模型，其中演员负责请求内容的决策，而评论员估计奖励。特别地，动作空间$S$由决策变量组成，奖励函数是目标函数的变体。在[[93](#bib.bib93)]中研究了缓存替换策略和动态多播调度策略。为了获得次优结果，使用了自动编码器来逼近状态。此外，采用加权双重DQN方案以避免Q值的高估。[[94](#bib.bib94)]通过收集历史请求应用RNN预测内容的流行度，输出表示近未来的流行度。然后，这些预测被用于MEC服务器之间的协作缓存和计算卸载，该问题被建模为ILP问题。为了高效解决它，应用了多智能体DQN，每个用户被视为一个智能体。动作空间包括任务本地计算和卸载决策，以及本地缓存和协作缓存决策。奖励由累积延迟来衡量。智能体根据当前状态选择自己的动作，不进行合作。D2D网络中的缓存位置、缓存内容和内容交付决策在[[95](#bib.bib95)]中进行了联合建模。考虑了两个RNN，ESN和LSTM，用于预测移动用户的位置和请求内容的流行度。然后，预测结果用于确定内容类别和缓存位置。内容交付被表述为基于演员-评论员的DRL框架。状态空间包括CSI、传输距离以及请求用户与其他可用候选者之间的通信功率。DRL的功能是确定用户之间的通信链路，以最小化功耗和内容延迟。
- en: We notice that most papers prefer to use value-based model (critic) and value-policy-based
    (actor-critic) model in DRL framework, but rare paper considers only policy-based
    model to solve data caching problem. One proper reason is that the search space
    of caching problem is enormous so policy-based model is easier to fall into local
    optimal solution, resulting in poor performance. Though the value-based model
    is less stable, some variant structures are utilized like Double DQN in [[93](#bib.bib93)]
    to avoid value overestimation and dueling DQN in [[88](#bib.bib88), [89](#bib.bib89),
    [90](#bib.bib90)] to improve robust.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，大多数论文更倾向于在DRL框架中使用基于价值的模型（评论员）和基于价值-策略的模型（演员-评论员），但很少有论文仅考虑基于策略的模型来解决数据缓存问题。一个合适的原因是缓存问题的搜索空间巨大，因此基于策略的模型更容易陷入局部最优解，导致性能较差。虽然基于价值的模型稳定性较差，但一些变体结构如[[93](#bib.bib93)]中的Double
    DQN被用来避免价值过高估计，[[88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90)]中的dueling
    DQN被用来提高鲁棒性。
- en: 5 Research Challenges and Future Directions
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 研究挑战与未来方向
- en: A serious of open issues on content caching and potential research directions
    are discussed in this section. We first extend the idea of content caching to
    virtual network function chain since caching can be viewed as a specific network
    function. Then we consider the caching for augmented reality applications. Moreover,
    we notice that the cache dimensioning has not been covered yet by DL methods.
    Finally, we debate the addition cost introduced by DL.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了内容缓存和潜在研究方向的一系列开放问题。我们首先将内容缓存的概念扩展到虚拟网络功能链，因为缓存可以被视为一个特定的网络功能。接着，我们考虑增强现实应用的缓存。此外，我们注意到缓存维度尚未被深度学习（DL）方法覆盖。最后，我们讨论了深度学习引入的额外成本。
- en: 5.1 Caching as a Virtual Network Function Chain
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 作为虚拟网络功能链的缓存
- en: The concept of Network Function Virtualization (NFV) has been firstly discussed
    and proposed within the realms of the European Telecommunication Standardization
    Institute (ETSI)¹¹1Network Functions Virtualisation, An Introduction, Benefits,
    Enablers, Challenges and Call for Action, ETSI, 2012 https://portal.etsi.org/NFV/NFV_White_Paper.pdf.
    The rational is to facilitate the dynamic provisioning of network services through
    virtualization technologies to decouple the service creation process form the
    underlying hardware. The framework allows network services to be implemented by
    a specific chaining and ordering of a set of functions which can be implemented
    either on a more traditional dedicated hardware which in this this case are called
    Physical Network Functions (PNFs), or alternatively as Virtual Network Functions
    (VNFs) which is a software running on top of virtualized general-purpose hardware.
    The decoupling between the hardware and the software is one of the important considerations
    the other – equally important – is that a virtualized service lend itself naturally
    to a dynamic programmable service creation where VNF resources can be deployed
    as required. Hence, edge cloud and network resource usage can be adapted to the
    instantaneous user demand whilst avoiding a more static over-provisioned configurations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 网络功能虚拟化（NFV）的概念最早在欧洲电信标准化协会（ETSI）的领域内进行讨论和提出¹¹1网络功能虚拟化，简介、好处、促进因素、挑战和行动呼吁，ETSI，2012
    https://portal.etsi.org/NFV/NFV_White_Paper.pdf。其理据是通过虚拟化技术促进网络服务的动态供应，以将服务创建过程与底层硬件解耦。该框架允许通过特定的功能链和顺序实现网络服务，这些功能可以实现于更传统的专用硬件上（在这种情况下称为物理网络功能（PNFs）），或者作为虚拟网络功能（VNFs）运行在虚拟化的通用硬件上。硬件与软件之间的解耦是一个重要的考虑因素，另一个同样重要的是虚拟化服务自然适合动态可编程的服务创建，其中VNF资源可以根据需要进行部署。因此，边缘云和网络资源的使用可以根据即时用户需求进行调整，同时避免了更静态的过度配置。
- en: Within that framework, the incoming network service requests include the specification
    of the service function chain that need to be created in the form of an ordered
    sequence of VNFs. For example different type of VNFs such as a firewall or a NAT
    mechanism need to be visited in a specific order. In such constructed service
    chain each independent VNF requires specific underlying resources in terms for
    example of CPU cycles and/or memory.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在该框架下，传入的网络服务请求包括需要创建的服务功能链的规格，这种链以有序的VNF序列形式呈现。例如，不同类型的VNF，如防火墙或NAT机制，需要以特定顺序访问。在这种构建的服务链中，每个独立的VNF需要特定的底层资源，例如CPU周期和/或内存。
- en: Under this framework, caching of popular content can be considered as a specialized
    VNF chain function since inevitably delivery of the cached popular content to
    users will require a set of other functions to be supported related to security,
    optimization of the content etc. However, the issue of data caching and VNF chaining
    have evolved rather independently in the literature and the issue on how to optimize
    data caching when seeing it as part of VNF chain is still an interesting open
    ended issue.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在此框架下，流行内容的缓存可以视为一个专门的VNF链功能，因为将缓存的流行内容传递给用户必然需要支持一组其他相关功能，如安全性、内容优化等。然而，数据缓存和VNF链的问题在文献中已经相对独立地发展，而将数据缓存视为VNF链的一部分时如何优化数据缓存仍然是一个有趣的开放性问题。
- en: 5.2 Caching for Mobile Augmented Reality (MAR) applications and Digital Twins
    (DTs)
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 针对移动增强现实（MAR）应用和数字双胞胎（DTs）的缓存
- en: Mobile augmented reality (MAR) applications can be considered as a way to augment
    the physical real-world environment with artificial computer-based generated information
    and is an area that has received significant research attention recently. In order
    to successfully superimpose different digital object in the physical world MAR
    applications include several computationally and storage complex concepts such
    as image recognition, mobile camera calibration, and also the use of advanced
    2D and 3D graphics rendering. These functionalities are highly computationally
    intensive and as such require support from an edge cloud, in addition the virtual
    objects to be embedded in the physical world are expected to be proactively cached
    closer to the end user so that latency is minimized. Ultra low latency in these
    type of applications is of paramount importance so that to provide a photorealistic
    embedding of virtual objects in the video view of the end user. However, since
    computational and augmented reality objects need to be readily available, the
    caching of those objects should be considered in conjunction with the computational
    capabilities of he edge cloud. In addition to the above when MAR is considered
    under the lenses of an NFV environment the application might inherently require
    access to some VNFs and therefore the above discussion on VNF chaining for MAR
    applications is also valid in this case.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 移动增强现实（MAR）应用可以被视为一种通过人工计算机生成的信息来增强物理现实世界环境的方式，并且这是一个最近受到大量研究关注的领域。为了成功地将不同的数字对象叠加到物理世界中，MAR
    应用包括多个计算和存储复杂的概念，如图像识别、移动相机校准以及高级 2D 和 3D 图形渲染。这些功能计算量极大，因此需要边缘云的支持。此外，嵌入物理世界中的虚拟对象需要主动缓存到离终端用户更近的位置，以便将延迟最小化。在这类应用中，超低延迟至关重要，以便在终端用户的视频视图中提供真实感的虚拟对象嵌入。然而，由于计算和增强现实对象需要随时可用，因此这些对象的缓存应与边缘云的计算能力结合考虑。此外，当在
    NFV 环境下考虑 MAR 时，应用可能会固有地需要访问一些 VNFs，因此上述关于 MAR 应用的 VNF 链接的讨论在这种情况下也是有效的。
- en: Recently the concept of Digital Twin (DT) [[100](#bib.bib100)], [[101](#bib.bib101)]
    has received significant research attention due to the plethora of applications
    ranging from industrial manufacturing and health to smart cities. In a nutshell,
    a DT can be defined as an accurate digital replica of a real world object across
    multiple granularity levels; and this real world object could be a machine, a
    robot or an industrial process or (sub) system. By reflecting the physical status
    of the system under consideration in a virtual space open up a plethora of optimization,
    prediction, fault tolerance and automation process that cannot be done using solely
    the physical object. At the core of DT applications is the requirement of stringent
    two–way real time communication between the digital replica and the physical object.
    This requirement inevitably require support from edge clouds to minimize latency
    and efficient storage and computational resources including caching. In that setting,
    the use of the aforementioned deep learning technologies will have a key role
    to play in order to provide high quality real time decision making to avoid misalignment
    between the digital replica of the physical object under consideration. Efficient
    machine-to-DT connectivity would require capabilities similar to the above mentioned
    augmented reality application but due to the continuous real-time control-loop
    operation DTs will require a complete new set of network optimization capabilities
    and in that frontier efficient caching and data-driven techniques will have a
    central role to play. Hence, as the research regarding the inter-play between
    low latency communications and DTs is still in embryonic stage there is significant
    scope in the investigation of suitable data driven deep learning techniques to
    be utilized for distributed allocation of caching and computing resources.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，数字双胞胎（DT）[[100](#bib.bib100)], [[101](#bib.bib101)] 的概念因其在工业制造、健康以及智能城市等领域的广泛应用而受到显著的研究关注。简而言之，DT
    可以定义为在多个粒度级别上真实世界对象的精确数字副本；而这个真实世界的对象可以是机器、机器人或工业过程或（子）系统。通过在虚拟空间中反映待考虑系统的物理状态，可以开启一系列优化、预测、容错和自动化过程，而这些过程仅凭物理对象无法完成。在
    DT 应用的核心是要求数字副本与物理对象之间进行严格的双向实时通信。这一要求不可避免地需要边缘云的支持，以最小化延迟，并提供高效的存储和计算资源，包括缓存。在这种环境下，前述的深度学习技术将发挥关键作用，以提供高质量的实时决策，避免数字副本与考虑中的物理对象之间的错位。高效的机器到
    DT 的连接需要类似于上述增强现实应用的能力，但由于 DT 的连续实时控制循环操作，它们将需要全新的网络优化能力，在这一前沿，效率高的缓存和数据驱动技术将发挥核心作用。因此，鉴于低延迟通信与
    DT 之间的相互作用的研究仍处于初期阶段，探讨适用于分布式缓存和计算资源分配的数据驱动深度学习技术具有重要意义。
- en: 5.3 Deep Learning for Cache Dimensioning
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 深度学习在缓存维度中的应用
- en: As introduced in Section [2](#S2 "2 Data Caching Review ‣ A Survey of Deep Learning
    for Data Caching in Edge Network"), cache dimensioning explores the appropriate
    cache size allocation for content host such as CRs and BSs. Disappointingly, there
    is rare paper applies DL on cache dimensioning decisions. One proper reason is
    lack of training data set in contrast to content popular prediction, where we
    have historical user request log to train a DNN. In addition, the caching size
    allocation affects the network performance and economic investment. Recently,
    network slicing is identified as an important tool to enable 5G to provide multi-services
    with diverse characteristics. The slice is established on physical infrastructure
    including network storage. Therefore it is a very interesting topic to consider
    the allocation of the memory space to support content caching and other storage
    services, which guarantees QoE and satisfies task requirements. Furthermore, for
    the case lack of training data set, DRL can be viewed as a promising technology
    to configure slicing settings as well as cache dimensioning. For the action space
    designing, it can be either discrete by setting storage levels, or continuous
    which is allocate the memory space directly. However, there are requirements to
    design caching-enabled network slicing model especially for dynamic allocation
    as well as associated DRL framework including state space, detailed action space,
    reward function and agent structure.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2](#S2 "2 Data Caching Review ‣ A Survey of Deep Learning for Data Caching
    in Edge Network")节所介绍，缓存维度探讨了为内容主机如CRs和BSs分配合适缓存大小的问题。令人失望的是，目前几乎没有论文将DL应用于缓存维度决策。一个适当的原因是缺乏训练数据集，与内容热门预测相对，我们有历史用户请求日志来训练DNN。此外，缓存大小分配会影响网络性能和经济投资。最近，网络切片被认为是使5G能够提供具有多样特征的多服务的重要工具。切片是基于包括网络存储在内的物理基础设施建立的。因此，考虑分配内存空间以支持内容缓存和其他存储服务是一个非常有趣的话题，这可以保证QoE并满足任务要求。此外，对于缺乏训练数据集的情况，DRL可以被视为一种有前景的技术，用于配置切片设置以及缓存维度。对于动作空间设计，它可以是通过设置存储级别的离散型，或是通过直接分配内存空间的连续型。然而，需要设计支持动态分配的缓存启用网络切片模型及其相关的DRL框架，包括状态空间、详细的动作空间、奖励函数和代理结构。
- en: 5.4 The Cost of Deep Learning
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 深度学习的成本
- en: Though the application of DL brings performance efficiency for caching policy,
    additional cost introduced by DL is unneglected, since training and deploying
    DL model require not only network resources but also time duration. Naturally
    there is a trade-off between the cost which DL-assisted caching policy saved and
    the consumption which supports DL itself running, which indicates the trading
    with DL results in either profit, loss, or break even. Therefore, where and when
    to apply DL should be carefully investigated. In addition, for the purpose of
    reducing resource consumption and accelerating training process, some knowledge
    transfer methods like transfer learning [[102](#bib.bib102)] can be utilized,
    which can transform the knowledge already learnt from the source domain to a relevant
    target domain.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DL的应用为缓存策略带来了性能效率，但DL引入的额外成本不可忽视，因为训练和部署DL模型不仅需要网络资源，还需要时间。因此，DL辅助缓存策略节省的成本与支持DL自身运行的消耗之间存在权衡，这表明DL的应用可能带来利润、亏损或持平。因此，DL的应用时机和地点应该经过仔细研究。此外，为了减少资源消耗和加快训练过程，可以利用一些知识转移方法，如迁移学习[[102](#bib.bib102)]，这些方法可以将从源领域已经学习到的知识转移到相关目标领域。
- en: 6 Conclusions
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This article presents a comprehensive study for the application of deep learning
    methods in the area of content caching. Particularly, the data caching is divided
    into two classifications according to the caching location of edge network. Each
    category contains where to cache, what to cache, cache dimensioning and content
    delivery. Then we introduce typical DNN methods which are categorised via training
    process into supervised learning, unsupervised learning and RL. Further, this
    paper critically compares and analyzes state-of-the-art papers on parameters,
    such as DL methods employed, the caching problems solved and the objective of
    applying DL. The challenges and research directions of DL on caching is also examined
    on the topic of extending caching to VNF chains, the application of caching for
    MAR as well as DTs, DL for cache size allocation and the additional cost of employing
    DL. Undoubtedly, DL is playing a significant role in 5G and beyond. We hope this
    paper will increase discussions and interests on DL for caching policy design
    and relevant applications, which will advance future network communications.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对深度学习方法在内容缓存领域的应用进行了全面研究。特别是，数据缓存根据边缘网络的缓存位置分为两类。每个类别包含缓存的位置、缓存的内容、缓存维度和内容传递。然后我们介绍了典型的
    DNN 方法，这些方法通过训练过程分为监督学习、无监督学习和强化学习（RL）。进一步地，本文对最先进的文献在参数方面进行了批判性比较和分析，例如使用的深度学习方法、解决的缓存问题以及应用深度学习的目标。本文还检查了深度学习在缓存领域的挑战和研究方向，包括将缓存扩展到
    VNF 链、缓存在 MAR 以及 DTs 中的应用、缓存大小分配的深度学习以及使用深度学习的额外成本。毫无疑问，深度学习在 5G 及未来技术中发挥了重要作用。我们希望本文能增加对深度学习在缓存策略设计和相关应用中的讨论和兴趣，从而推动未来网络通信的发展。
- en: References
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] X. Wang, M. Chen, Z. Han, D. O. Wu, and T. T. Kwon. Toss: Traffic offloading
    by social network service-based opportunistic sharing in mobile social networks.
    In IEEE INFOCOM 2014 - IEEE Conference on Computer Communications, pages 2346–2354,
    2014.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] X. Wang, M. Chen, Z. Han, D. O. Wu, 和 T. T. Kwon. Toss: 基于社交网络服务的机会性共享在移动社交网络中的流量卸载。在
    IEEE INFOCOM 2014 - IEEE 计算机通信会议，页码 2346–2354, 2014。'
- en: '[2] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing
    of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329,
    2017.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, 和 Joel S Emer. 深度神经网络的高效处理：教程和调查。IEEE
    会议录, 105(12):2295–2329, 2017。'
- en: '[3] Yaohua Sun, Mugen Peng, Yangcheng Zhou, Yuzhe Huang, and Shiwen Mao. Application
    of machine learning in wireless networks: Key techniques and open issues. IEEE
    Communications Surveys & Tutorials, 21(4):3072–3108, 2019.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Yaohua Sun, Mugen Peng, Yangcheng Zhou, Yuzhe Huang, 和 Shiwen Mao. 机器学习在无线网络中的应用：关键技术和开放问题。IEEE
    Communications Surveys & Tutorials, 21(4):3072–3108, 2019。'
- en: '[4] Adita Kulkarni and Anand Seetharam. Model and machine learning based caching
    and routing algorithms for cache-enabled networks. arXiv preprint arXiv:2004.06787,
    2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Adita Kulkarni 和 Anand Seetharam. 基于模型和机器学习的缓存和路由算法用于缓存启用的网络。arXiv preprint
    arXiv:2004.06787, 2020。'
- en: '[5] Junaid Shuja, Kashif Bilal, Eisa Alanazi, Waleed Alasmary, and Abdulaziz
    Alashaikh. Applying machine learning techniques for caching in edge networks:
    A comprehensive survey. arXiv preprint arXiv:2006.16864, 2020.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Junaid Shuja, Kashif Bilal, Eisa Alanazi, Waleed Alasmary, 和 Abdulaziz
    Alashaikh. 在边缘网络中应用机器学习技术进行缓存：全面调查。arXiv preprint arXiv:2006.16864, 2020。'
- en: '[6] Stephen ANOKYE, SEID Mohammed, and SUN Guolin. A survey on machine learning
    based proactive caching. ZTE Communications, 17(4):46–55, 2020.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Stephen ANOKYE, SEID Mohammed, 和 SUN Guolin. 基于机器学习的主动缓存调查。ZTE Communications,
    17(4):46–55, 2020。'
- en: '[7] Mingzhe Chen, Ursula Challita, Walid Saad, Changchuan Yin, and Mérouane
    Debbah. Artificial neural networks-based machine learning for wireless networks:
    A tutorial. IEEE Communications Surveys & Tutorials, 21(4):3039–3071, 2019.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Mingzhe Chen, Ursula Challita, Walid Saad, Changchuan Yin, 和 Mérouane Debbah.
    基于人工神经网络的无线网络机器学习：教程。IEEE Communications Surveys & Tutorials, 21(4):3039–3071,
    2019。'
- en: '[8] Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang,
    Ying-Chang Liang, and Dong In Kim. Applications of deep reinforcement learning
    in communications and networking: A survey. IEEE Communications Surveys & Tutorials,
    21(4):3133–3174, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang,
    Ying-Chang Liang, 和 Dong In Kim. 深度强化学习在通信和网络中的应用：调查。IEEE Communications Surveys
    & Tutorials, 21(4):3133–3174, 2019。'
- en: '[9] Xiaofei Wang, Yiwen Han, Victor CM Leung, Dusit Niyato, Xueqiang Yan, and
    Xu Chen. Convergence of edge computing and deep learning: A comprehensive survey.
    IEEE Communications Surveys & Tutorials, 22(2):869–904, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Xiaofei Wang, Yiwen Han, Victor CM Leung, Dusit Niyato, Xueqiang Yan, and
    Xu Chen. 边缘计算与深度学习的融合：全面调查。IEEE 通信调查与教程，22(2):869–904, 2020。'
- en: '[10] Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, and Lanyu Xu. Edge computing:
    Vision and challenges. IEEE internet of things journal, 3(5):637–646, 2016.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, and Lanyu Xu. 边缘计算：愿景与挑战。IEEE
    物联网期刊，3(5):637–646, 2016。'
- en: '[11] Mugen Peng, Yaohua Sun, Xuelong Li, Zhendong Mao, and Chonggang Wang.
    Recent advances in cloud radio access networks: System architectures, key techniques,
    and open issues. IEEE Communications Surveys & Tutorials, 18(3):2282–2308, 2016.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Mugen Peng, Yaohua Sun, Xuelong Li, Zhendong Mao, and Chonggang Wang.
    云无线接入网络的最新进展：系统架构、关键技术及开放问题。IEEE 通信调查与教程，18(3):2282–2308, 2016。'
- en: '[12] Georgios S Paschos, George Iosifidis, Meixia Tao, Don Towsley, and Giuseppe
    Caire. The role of caching in future communication systems and networks. IEEE
    Journal on Selected Areas in Communications, 36(6):1111–1125, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Georgios S Paschos, George Iosifidis, Meixia Tao, Don Towsley, and Giuseppe
    Caire. 缓存在未来通信系统和网络中的作用。IEEE 选择通信领域期刊，36(6):1111–1125, 2018。'
- en: '[13] Siyang Shan, Chunyan Feng, Tiankui Zhang, and Jonathan Loo. Proactive
    caching placement for arbitrary topology with multi-hop forwarding in icn. IEEE
    Access, 7:149117–149131, 2019.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Siyang Shan, Chunyan Feng, Tiankui Zhang, and Jonathan Loo. 在ICN中针对任意拓扑的前瞻缓存放置与多跳转发。IEEE
    Access，7:149117–149131, 2019。'
- en: '[14] Yantong Wang, Gao Zheng, and Vasilis Friderikos. Proactive caching in
    mobile networks with delay guarantees. In ICC 2019-2019 IEEE International Conference
    on Communications (ICC), pages 1–6\. IEEE, 2019.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Yantong Wang, Gao Zheng, and Vasilis Friderikos. 带延迟保证的移动网络前瞻缓存。在 ICC
    2019-2019 IEEE国际通信会议 (ICC)，第1–6页\. IEEE，2019。'
- en: '[15] Chao Fang, F Richard Yu, Tao Huang, Jiang Liu, and Yunjie Liu. An energy-efficient
    distributed in-network caching scheme for green content-centric networks. Computer
    Networks, 78:119–129, 2015.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Chao Fang, F Richard Yu, Tao Huang, Jiang Liu, and Yunjie Liu. 一种用于绿色内容中心网络的节能分布式网络内缓存方案。计算机网络，78:119–129,
    2015。'
- en: '[16] Jagruti Sahoo, Mohammad A Salahuddin, Roch Glitho, Halima Elbiaze, and
    Wessam Ajib. A survey on replica server placement algorithms for content delivery
    networks. IEEE Communications Surveys & Tutorials, 19(2):1002–1026, 2016.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Jagruti Sahoo, Mohammad A Salahuddin, Roch Glitho, Halima Elbiaze, and
    Wessam Ajib. 内容交付网络中副本服务器放置算法的调查。IEEE 通信调查与教程，19(2):1002–1026, 2016。'
- en: '[17] Asif Kabir, Gohar Rehman, Syed Mushhad Gilani, Edvin J Kitindi, Zain Ul Abidin Jaffri,
    and Khurrum Mustafa Abbasi. The role of caching in next generation cellular networks:
    A survey and research outlook. Transactions on Emerging Telecommunications Technologies,
    31(2):e3702, 2020.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Asif Kabir, Gohar Rehman, Syed Mushhad Gilani, Edvin J Kitindi, Zain Ul Abidin Jaffri,
    and Khurrum Mustafa Abbasi. 缓存在下一代蜂窝网络中的作用：调查与研究展望。新兴通信技术交易，31(2):e3702, 2020。'
- en: '[18] Stefano Traverso, Mohamed Ahmed, Michele Garetto, Paolo Giaccone, Emilio
    Leonardi, and Saverio Niccolini. Temporal locality in today’s content caching:
    why it matters and how to model it. ACM SIGCOMM Computer Communication Review,
    43(5):5–12, 2013.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Stefano Traverso, Mohamed Ahmed, Michele Garetto, Paolo Giaccone, Emilio
    Leonardi, and Saverio Niccolini. 当今内容缓存中的时间局部性：为何重要及如何建模。ACM SIGCOMM 计算机通信评论，43(5):5–12,
    2013。'
- en: '[19] Ali Dabirmoghaddam, Maziar Mirzazad Barijough, and JJ Garcia-Luna-Aceves.
    Understanding optimal caching and opportunistic caching at" the edge" of information-centric
    networks. In Proceedings of the 1st ACM conference on information-centric networking,
    pages 47–56, 2014.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Ali Dabirmoghaddam, Maziar Mirzazad Barijough, and JJ Garcia-Luna-Aceves.
    理解信息中心网络“边缘”的最佳缓存和机会缓存。在第1届ACM信息中心网络会议论文集中，第47–56页，2014。'
- en: '[20] Yue Shi, Martha Larson, and Alan Hanjalic. Collaborative filtering beyond
    the user-item matrix: A survey of the state of the art and future challenges.
    ACM Computing Surveys (CSUR), 47(1):1–45, 2014.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Yue Shi, Martha Larson, and Alan Hanjalic. 超越用户-项目矩阵的协同过滤：前沿状态与未来挑战的调查。ACM
    计算机调查 (CSUR)，47(1):1–45, 2014。'
- en: '[21] Jaeyeon Jung, Arthur W Berger, and Hari Balakrishnan. Modeling ttl-based
    internet caches. In IEEE INFOCOM 2003\. Twenty-second Annual Joint Conference
    of the IEEE Computer and Communications Societies (IEEE Cat. No. 03CH37428), volume 1,
    pages 417–426\. IEEE, 2003.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jaeyeon Jung, Arthur W Berger, and Hari Balakrishnan. 基于TTL的互联网缓存建模。在
    IEEE INFOCOM 2003\. 第二十二届IEEE计算机与通信学会年会（IEEE Cat. No. 03CH37428），第1卷，第417–426页\.
    IEEE，2003。'
- en: '[22] Nicaise Choungmo Fofack, Philippe Nain, Giovanni Neglia, and Don Towsley.
    Performance evaluation of hierarchical ttl-based cache networks. Computer Networks,
    65:212–231, 2014.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Nicaise Choungmo Fofack, Philippe Nain, Giovanni Neglia, 和 Don Towsley.
    分层 TTL 缓存网络的性能评估. 计算机网络, 65:212–231, 2014年。'
- en: '[23] Dario Rossi and Giuseppe Rossini. On sizing ccn content stores by exploiting
    topological information. In 2012 Proceedings IEEE INFOCOM Workshops, pages 280–285.
    IEEE, 2012.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Dario Rossi 和 Giuseppe Rossini. 利用拓扑信息确定 CCN 内容存储的规模. 载于2012年 IEEE INFOCOM
    研讨会论文集, 页码 280–285. IEEE, 2012年。'
- en: '[24] Yuemei Xu, Yang Li, Tao Lin, Zihou Wang, Wenjia Niu, Hui Tang, and Song
    Ci. A novel cache size optimization scheme based on manifold learning in content
    centric networking. Journal of Network and Computer Applications, 37:273–281,
    2014.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Yuemei Xu, Yang Li, Tao Lin, Zihou Wang, Wenjia Niu, Hui Tang, 和 Song
    Ci. 基于流形学习的内容中心网络中的新型缓存大小优化方案. 网络与计算机应用杂志, 37:273–281, 2014年。'
- en: '[25] Georgios Paschos, George Iosifidis, and Giuseppe Caire. Cache optimization
    models and algorithms. arXiv preprint arXiv:1912.12339, 2019.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Georgios Paschos, George Iosifidis, 和 Giuseppe Caire. 缓存优化模型和算法. arXiv
    预印本 arXiv:1912.12339, 2019年。'
- en: '[26] Van Jacobson, Diana K Smetters, James D Thornton, Michael F Plass, Nicholas H
    Briggs, and Rebecca L Braynard. Networking named content. In Proceedings of the
    5th international conference on Emerging networking experiments and technologies,
    pages 1–12, 2009.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Van Jacobson, Diana K Smetters, James D Thornton, Michael F Plass, Nicholas
    H Briggs, 和 Rebecca L Braynard. 命名内容的网络化. 载于第五届国际新兴网络实验与技术会议论文集，页码 1–12, 2009年。'
- en: '[27] Liang Wang, Suzan Bayhan, Jörg Ott, Jussi Kangasharju, and Jon Crowcroft.
    Understanding scoped-flooding for content discovery and caching in content networks.
    IEEE Journal on Selected Areas in Communications, 36(8):1887–1900, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Liang Wang, Suzan Bayhan, Jörg Ott, Jussi Kangasharju, 和 Jon Crowcroft.
    理解内容网络中的范围洪泛以进行内容发现和缓存. IEEE 选择领域通信学报, 36(8):1887–1900, 2018年。'
- en: '[28] Liying Li, Guodong Zhao, and Rick S Blum. A survey of caching techniques
    in cellular networks: Research issues and challenges in content placement and
    delivery strategies. IEEE Communications Surveys & Tutorials, 20(3):1710–1732,
    2018.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Liying Li, Guodong Zhao, 和 Rick S Blum. 移动网络中缓存技术的综述：内容放置和传递策略的研究问题与挑战.
    IEEE 通信调查与教程, 20(3):1710–1732, 2018年。'
- en: '[29] Ammar Gharaibeh, Abdallah Khreishah, Bo Ji, and Moussa Ayyash. A provably
    efficient online collaborative caching algorithm for multicell-coordinated systems.
    IEEE Transactions on Mobile Computing, 15(8):1863–1876, 2015.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Ammar Gharaibeh, Abdallah Khreishah, Bo Ji, 和 Moussa Ayyash. 具有证明效率的在线协作缓存算法用于多小区协调系统.
    IEEE 移动计算学报, 15(8):1863–1876, 2015年。'
- en: '[30] Negin Golrezaei, Andreas F Molisch, Alexandros G Dimakis, and Giuseppe
    Caire. Femtocaching and device-to-device collaboration: A new architecture for
    wireless video distribution. IEEE Communications Magazine, 51(4):142–149, 2013.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Negin Golrezaei, Andreas F Molisch, Alexandros G Dimakis, 和 Giuseppe Caire.
    小型基站缓存和设备对设备协作：无线视频分发的新架构. IEEE 通信杂志, 51(4):142–149, 2013年。'
- en: '[31] Mehrnaz Afshang, Harpreet S Dhillon, and Peter Han Joo Chong. Fundamentals
    of cluster-centric content placement in cache-enabled device-to-device networks.
    IEEE Transactions on Communications, 64(6):2511–2526, 2016.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Mehrnaz Afshang, Harpreet S Dhillon, 和 Peter Han Joo Chong. 缓存启用的设备对设备网络中的集群中心内容放置基础.
    IEEE 通信学报, 64(6):2511–2526, 2016年。'
- en: '[32] Zhuoqun Chen, Yangyang Liu, Bo Zhou, and Meixia Tao. Caching incentive
    design in wireless d2d networks: A stackelberg game approach. In 2016 IEEE International
    Conference on Communications (ICC), pages 1–6\. IEEE, 2016.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Zhuoqun Chen, Yangyang Liu, Bo Zhou, 和 Meixia Tao. 无线 D2D 网络中的缓存激励设计：一个
    Stackelberg 博弈方法. 载于2016年 IEEE 国际通信会议 (ICC)论文集, 页码 1–6. IEEE, 2016年。'
- en: '[33] Zhun Ye, Cunhua Pan, Huiling Zhu, and Jiangzhou Wang. Tradeoff caching
    strategy of the outage probability and fronthaul usage in a cloud-ran. IEEE Transactions
    on Vehicular Technology, 67(7):6383–6397, 2018.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Zhun Ye, Cunhua Pan, Huiling Zhu, 和 Jiangzhou Wang. 云计算中停机概率和前传使用的权衡缓存策略.
    IEEE 车辆技术学报, 67(7):6383–6397, 2018年。'
- en: '[34] Dewang Ren, Xiaolin Gui, Kaiyuan Zhang, and Jie Wu. Mobility-aware traffic
    offloading via cooperative coded edge caching. IEEE Access, 8:43427–43442, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Dewang Ren, Xiaolin Gui, Kaiyuan Zhang, 和 Jie Wu. 通过协作编码边缘缓存进行面向移动性的流量卸载.
    IEEE Access, 8:43427–43442, 2020年。'
- en: '[35] Jaeyoung Song and Wan Choi. Mobility-aware content placement for device-to-device
    caching systems. IEEE Transactions on Wireless Communications, 18(7):3658–3668,
    2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Jaeyoung Song 和 Wan Choi. 面向移动性的内容放置用于设备对设备缓存系统. IEEE 无线通信学报, 18(7):3658–3668,
    2019年。'
- en: '[36] Dong Liu, Binqiang Chen, Chenyang Yang, and Andreas F Molisch. Caching
    at the wireless edge: design aspects, challenges, and future directions. IEEE
    Communications Magazine, 54(9):22–28, 2016.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Dong Liu、Binqiang Chen、Chenyang Yang 和 Andreas F Molisch。在无线边缘的缓存：设计方面、挑战和未来方向。IEEE
    通讯杂志，54(9):22–28，2016年。'
- en: '[37] Xi Peng, Jun Zhang, SH Song, and Khaled B Letaief. Cache size allocation
    in backhaul limited wireless networks. In 2016 IEEE International Conference on
    Communications (ICC), pages 1–6\. IEEE, 2016.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Xi Peng、Jun Zhang、SH Song 和 Khaled B Letaief。回程限制无线网络中的缓存大小分配。在2016 IEEE
    国际通讯会议（ICC），页面1–6。IEEE，2016年。'
- en: '[38] An Liu and Vincent KN Lau. How much cache is needed to achieve linear
    capacity scaling in backhaul-limited dense wireless networks? IEEE/ACM Transactions
    on Networking, 25(1):179–188, 2016.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] An Liu 和 Vincent KN Lau。实现回程限制的密集无线网络中线性容量扩展所需的缓存大小？IEEE/ACM 网络通讯汇刊，25(1):179–188，2016年。'
- en: '[39] Jaeyoung Song and Wan Choi. Minimum cache size and backhaul capacity for
    cache-enabled small cell networks. IEEE Wireless Communications Letters, 7(4):490–493,
    2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Jaeyoung Song 和 Wan Choi。缓存启用的小型蜂窝网络的最小缓存大小和回程容量。IEEE 无线通讯快报，7(4):490–493，2017年。'
- en: '[40] Bo Zhou, Ying Cui, and Meixia Tao. Optimal dynamic multicast scheduling
    for cache-enabled content-centric wireless networks. IEEE Transactions on Communications,
    65(7):2956–2970, 2017.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Bo Zhou、Ying Cui 和 Meixia Tao。针对缓存启用的内容中心无线网络的最佳动态多播调度。IEEE 通讯汇刊，65(7):2956–2970，2017年。'
- en: '[41] Mohammad Ali Maddah-Ali and Urs Niesen. Fundamental limits of caching.
    IEEE Transactions on Information Theory, 60(5):2856–2867, 2014.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Mohammad Ali Maddah-Ali 和 Urs Niesen。缓存的基本限制。IEEE 信息理论汇刊，60(5):2856–2867，2014年。'
- en: '[42] Vu Nguyen Ha, Long Bao Le, et al. Coordinated multipoint transmission
    design for cloud-rans with limited fronthaul capacity constraints. IEEE Transactions
    on Vehicular Technology, 65(9):7432–7447, 2015.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Vu Nguyen Ha、Long Bao Le 等。具有有限前传容量约束的云无线接入网络的协调多点传输设计。IEEE 车辆技术汇刊，65(9):7432–7447，2015年。'
- en: '[43] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT
    Press, 2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Ian Goodfellow、Yoshua Bengio 和 Aaron Courville。深度学习。麻省理工学院出版社，2016。[http://www.deeplearningbook.org](http://www.deeplearningbook.org)。'
- en: '[44] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Jonathan Long、Evan Shelhamer 和 Trevor Darrell。用于语义分割的全卷积网络。在IEEE 计算机视觉与模式识别会议论文集，页面3431–3440，2015年。'
- en: '[45] Hava T Siegelmann and Eduardo D Sontag. Turing computability with neural
    nets. Applied Mathematics Letters, 4(6):77–80, 1991.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Hava T Siegelmann 和 Eduardo D Sontag。神经网络中的图灵可计算性。应用数学通讯，4(6):77–80，1991年。'
- en: '[46] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.
    In Advances in neural information processing systems, pages 2692–2700, 2015.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Oriol Vinyals、Meire Fortunato 和 Navdeep Jaitly。指针网络。在神经信息处理系统进展，页面2692–2700，2015年。'
- en: '[47] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
    Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,
    et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533,
    2015.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Andrei A Rusu、Joel Veness、Marc
    G Bellemare、Alex Graves、Martin Riedmiller、Andreas K Fidjeland、Georg Ostrovski
    等。通过深度强化学习实现人类级控制。自然，518(7540):529–533，2015年。'
- en: '[48] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning
    with double q-learning. In Thirtieth AAAI conference on artificial intelligence,
    2016.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Hado Van Hasselt、Arthur Guez 和 David Silver。使用双重 Q 学习的深度强化学习。在第三十届 AAAI
    人工智能会议，2016年。'
- en: '[49] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and
    Nando Freitas. Dueling network architectures for deep reinforcement learning.
    In International conference on machine learning, pages 1995–2003, 2016.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Ziyu Wang、Tom Schaul、Matteo Hessel、Hado Hasselt、Marc Lanctot 和 Nando Freitas。用于深度强化学习的对偶网络架构。在国际机器学习会议，页面1995–2003，2016年。'
- en: '[50] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
    Moritz. Trust region policy optimization. In International conference on machine
    learning, pages 1889–1897, 2015.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] John Schulman、Sergey Levine、Pieter Abbeel、Michael Jordan 和 Philipp Moritz。信任域策略优化。在国际机器学习会议，页面1889–1897，2015年。'
- en: '[51] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford 和 Oleg Klimov。邻近策略优化算法。arXiv
    预印本 arXiv:1707.06347，2017年。'
- en: '[52] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 蒂莫西·P·利利克拉普，乔纳森·J·亨特，亚历山大·普里策尔，尼古拉斯·赫斯，汤姆·埃雷兹，尤瓦尔·塔萨，大卫·银和丹·维尔斯特拉。深度强化学习的连续控制。arXiv预印本arXiv:1509.02971，2015年。'
- en: '[53] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937, 2016.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 沃洛德米尔·姆尼赫，阿德里亚·普伊格多门赫·巴迪亚，梅赫迪·米尔扎，亚历克斯·格雷夫斯，蒂莫西·利利克拉普，蒂姆·哈雷，大卫·银和科雷·卡武克丘奥格鲁。深度强化学习的异步方法。发表于国际机器学习会议，第1928–1937页，2016年。'
- en: '[54] Lei Lei, Yaxiong Yuan, Thang X Vu, Symeon Chatzinotas, and Björn Ottersten.
    Learning-based resource allocation: Efficient content delivery enabled by convolutional
    neural network. In 2019 IEEE 20th International Workshop on Signal Processing
    Advances in Wireless Communications (SPAWC), pages 1–5\. IEEE, 2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 雷雷，雅雄·袁，汤辉山，西门·查齐诺塔斯和比约恩·奥特斯滕。基于学习的资源分配：通过卷积神经网络实现高效内容传递。发表于2019年IEEE第20届无线通信信号处理进展国际研讨会（SPAWC），第1–5页。IEEE，2019年。'
- en: '[55] Lei Lei, Lei You, Gaoyang Dai, Thang Xuan Vu, Di Yuan, and Symeon Chatzinotas.
    A deep learning approach for optimizing content delivering in cache-enabled hetnet.
    In 2017 international symposium on wireless communication systems (ISWCS), pages
    449–453\. IEEE, 2017.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 雷雷，雷有，高阳戴，汤辉山，谭元和西门·查齐诺塔斯。用于缓存启用的异构网络中内容传递优化的深度学习方法。发表于2017年国际无线通信系统研讨会（ISWCS），第449–453页。IEEE，2017年。'
- en: '[56] Yantong Wang and Vasilis Friderikos. Caching as an image characterization
    problem using deep convolutional neural networks. arXiv preprint arXiv:1907.07263,
    2019.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 王彦同和瓦西里斯·弗里德里科斯。将缓存视为图像特征化问题，使用深度卷积神经网络。arXiv预印本arXiv:1907.07263，2019年。'
- en: '[57] Yantong Wang and Vasilis Friderikos. Network orchestration in mobile networks
    via a synergy of model-driven and ai-based techniques. arXiv preprint arXiv:2004.00660,
    2020.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 王彦同和瓦西里斯·弗里德里科斯。通过模型驱动和基于AI的技术的协同在移动网络中的网络编排。arXiv预印本arXiv:2004.00660，2020年。'
- en: '[58] Khai Nguyen Doan, Thang Van Nguyen, Tony QS Quek, and Hyundong Shin. Content-aware
    proactive caching for backhaul offloading in cellular network. IEEE Transactions
    on Wireless Communications, 17(5):3128–3140, 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 凯·阮段，汤辉山，托尼·QS·阮和贤东·申。用于蜂窝网络回程卸载的内容感知主动缓存。IEEE无线通信学报，17(5)：3128–3140，2018年。'
- en: '[59] Zhou Qin, Yikun Xian, and Desheng Zhang. A neural networks based caching
    scheme for mobile edge networks. In Proceedings of the 17th Conference on Embedded
    Networked Sensor Systems, pages 408–409, 2019.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 周钦，易坤·谢和德胜·张。基于神经网络的移动边缘网络缓存方案。发表于第17届嵌入式网络传感系统会议论文集，第408–409页，2019年。'
- en: '[60] Kuo Chun Tsai, Li Wang, and Zhu Han. Mobile social media networks caching
    with convolutional neural network. In 2018 IEEE wireless communications and networking
    conference workshops (WCNCW), pages 83–88\. IEEE, 2018.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 蔡国春，李旺和朱汉。移动社交媒体网络的卷积神经网络缓存。发表于2018年IEEE无线通信和网络会议研讨会（WCNCW），第83–88页。IEEE，2018年。'
- en: '[61] Vladyslav Fedchenko, Giovanni Neglia, and Bruno Ribeiro. Feedforward neural
    networks for caching: n enough or too much? ACM SIGMETRICS Performance Evaluation
    Review, 46(3):139–142, 2019.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 弗拉基斯拉夫·费琴科，乔瓦尼·内格利亚和布鲁诺·里贝罗。用于缓存的前馈神经网络：足够还是过多？ACM SIGMETRICS 性能评估评论，46(3)：139–142，2019年。'
- en: '[62] Mingzhe Chen, Walid Saad, Changchuan Yin, and Mérouane Debbah. Echo state
    networks for proactive caching in cloud-based radio access networks with mobile
    users. IEEE Transactions on Wireless Communications, 16(6):3520–3535, 2017.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 陈铭哲，瓦利德·萨阿德，常川·尹和梅鲁安·德巴赫。面向云基无线接入网络中移动用户的主动缓存的回声状态网络。IEEE无线通信学报，16(6)：3520–3535，2017年。'
- en: '[63] Mingzhe Chen, Mohammad Mozaffari, Walid Saad, Changchuan Yin, Mérouane
    Debbah, and Choong Seon Hong. Caching in the sky: Proactive deployment of cache-enabled
    unmanned aerial vehicles for optimized quality-of-experience. IEEE Journal on
    Selected Areas in Communications, 35(5):1046–1061, 2017.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 陈铭哲，穆罕默德·莫扎法里，瓦利德·萨阿德，常川·尹，梅鲁安·德巴赫和洪钟善。天上的缓存：缓存启用的无人机的主动部署以优化用户体验质量。IEEE选定领域通讯学报，35(5)：1046–1061，2017年。'
- en: '[64] Laha Ale, Ning Zhang, Huici Wu, Dajiang Chen, and Tao Han. Online proactive
    caching in mobile edge computing using bidirectional deep recurrent neural network.
    IEEE Internet of Things Journal, 6(3):5520–5530, 2019.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Laha Ale, Ning Zhang, Huici Wu, Dajiang Chen 和 Tao Han. 使用双向深度递归神经网络的移动边缘计算中的在线主动缓存。IEEE物联网学报，6(3)：5520–5530，2019年。'
- en: '[65] Qilin Fan, Jian Li, Xiuhua Li, Qiang He, Shu Fu, and Sen Wang. Pa-cache:
    Learning-based popularity-aware content caching in edge networks. arXiv preprint
    arXiv:2002.08805, 2020.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Qilin Fan, Jian Li, Xiuhua Li, Qiang He, Shu Fu 和 Sen Wang. Pa-cache：基于学习的流行度感知内容缓存，在边缘网络中应用。arXiv预印本
    arXiv:2002.08805，2020年。'
- en: '[66] Cong Zhang, Haitian Pang, Jiangchuan Liu, Shizhi Tang, Ruixiao Zhang,
    Dan Wang, and Lifeng Sun. Toward edge-assisted video content intelligent caching
    with long short-term memory learning. IEEE access, 7:152832–152846, 2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Cong Zhang, Haitian Pang, Jiangchuan Liu, Shizhi Tang, Ruixiao Zhang,
    Dan Wang 和 Lifeng Sun. 朝向边缘辅助视频内容智能缓存，采用长短期记忆学习。IEEE访问，7：152832–152846，2019年。'
- en: '[67] Hanlin Mou, Yuhong Liu, and Li Wang. Lstm for mobility based content popularity
    prediction in wireless caching networks. In 2019 IEEE Globecom Workshops (GC Wkshps),
    pages 1–6\. IEEE, 2019.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Hanlin Mou, Yuhong Liu 和 Li Wang. LSTM用于无线缓存网络中的移动性基础内容流行度预测。在2019年IEEE
    Globecom研讨会（GC Wkshps），页面1–6。IEEE，2019年。'
- en: '[68] Arvind Narayanan, Saurabh Verma, Eman Ramadan, Pariya Babaie, and Zhi-Li
    Zhang. Deepcache: A deep learning based framework for content caching. In Proceedings
    of the 2018 Workshop on Network Meets AI & ML, pages 48–53, 2018.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Arvind Narayanan, Saurabh Verma, Eman Ramadan, Pariya Babaie 和 Zhi-Li
    Zhang. Deepcache：一个基于深度学习的内容缓存框架。在2018年网络遇见人工智能与机器学习研讨会论文集，页面48–53，2018年。'
- en: '[69] Zhengming Zhang, Yaru Zheng, Chunguo Li, Yongming Huang, and Luxi Yang.
    On the cover problem for coded caching in wireless networks via deep neural network.
    In 2019 IEEE Global Communications Conference (GLOBECOM), pages 1–6\. IEEE, 2019.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Zhengming Zhang, Yaru Zheng, Chunguo Li, Yongming Huang 和 Luxi Yang. 基于深度神经网络的无线网络编码缓存中的封面问题。在2019年IEEE全球通信会议（GLOBECOM），页面1–6。IEEE，2019年。'
- en: '[70] Fangyuan Lei, Jun Cai, Qingyun Dai, and Huimin Zhao. Deep learning based
    proactive caching for effective wsn-enabled vision applications. Complexity, 2019,
    2019.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Fangyuan Lei, Jun Cai, Qingyun Dai 和 Huimin Zhao. 基于深度学习的主动缓存，用于有效的WSN启用视觉应用。复杂性，2019年，2019年。'
- en: '[71] FangYuan Lei, QinYun Dai, Jun Cai, HuiMin Zhao, Xun Liu, and Yan Liu.
    A proactive caching strategy based on deep learning in epc of 5g. In International
    Conference on Brain Inspired Cognitive Systems, pages 738–747\. Springer, 2018.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] FangYuan Lei, QinYun Dai, Jun Cai, HuiMin Zhao, Xun Liu 和 Yan Liu. 基于深度学习的5G
    EPC主动缓存策略。在脑启发认知系统国际会议论文集，页面738–747。Springer，2018年。'
- en: '[72] Shailendra Rathore, Jung Hyun Ryu, Pradip Kumar Sharma, and Jong Hyuk
    Park. Deepcachnet: A proactive caching framework based on deep learning in cellular
    networks. IEEE Network, 33(3):130–138, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Shailendra Rathore, Jung Hyun Ryu, Pradip Kumar Sharma 和 Jong Hyuk Park.
    Deepcachnet：一个基于深度学习的蜂窝网络主动缓存框架。IEEE网络，33(3)：130–138，2019年。'
- en: '[73] Wai-Xi Liu, Jie Zhang, Zhong-Wei Liang, Ling-Xi Peng, and Jun Cai. Content
    popularity prediction and caching for icn: A deep learning approach with sdn.
    IEEE access, 6:5075–5089, 2017.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Wai-Xi Liu, Jie Zhang, Zhong-Wei Liang, Ling-Xi Peng 和 Jun Cai. ICN中的内容流行度预测与缓存：一种基于SDN的深度学习方法。IEEE访问，6：5075–5089，2017年。'
- en: '[74] Wei Li, Jun Wang, Guoyong Zhang, Li Li, Ze Dang, and Shaoqian Li. A reinforcement
    learning based smart cache strategy for cache-aided ultra-dense network. IEEE
    Access, 7:39390–39401, 2019.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Wei Li, Jun Wang, Guoyong Zhang, Li Li, Ze Dang 和 Shaoqian Li. 基于强化学习的智能缓存策略，用于缓存辅助的超密集网络。IEEE访问，7：39390–39401，2019年。'
- en: '[75] Yu-Tai Lin, Chia-Cheng Yen, and Jia-Shung Wang. Video popularity prediction:
    An autoencoder approach with clustering. IEEE Access, 8:129285–129299, 2020.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Yu-Tai Lin, Chia-Cheng Yen 和 Jia-Shung Wang. 视频流行度预测：一种带有聚类的自编码器方法。IEEE访问，8：129285–129299，2020年。'
- en: '[76] Chen Zhong, M Cenk Gursoy, and Senem Velipasalar. A deep reinforcement
    learning-based framework for content caching. In 2018 52nd Annual Conference on
    Information Sciences and Systems (CISS), pages 1–6\. IEEE, 2018.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Chen Zhong, M Cenk Gursoy 和 Senem Velipasalar. 一个基于深度强化学习的内容缓存框架。在2018年第52届信息科学与系统年会（CISS），页面1–6。IEEE，2018年。'
- en: '[77] Chen Zhong, M Cenk Gursoy, and Senem Velipasalar. Deep reinforcement learning-based
    edge caching in wireless networks. IEEE Transactions on Cognitive Communications
    and Networking, 6(1):48–61, 2020.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Chen Zhong, M Cenk Gursoy 和 Senem Velipasalar. 基于深度强化学习的无线网络边缘缓存。IEEE认知通信与网络学报，6(1)：48–61，2020年。'
- en: '[78] Shen Gao, Peihao Dong, Zhiwen Pan, and Geoffrey Ye Li. Reinforcement learning
    based cooperative coded caching under dynamic popularities in ultra-dense networks.
    IEEE Transactions on Vehicular Technology, 69(5):5442–5456, 2020.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Shen Gao, Peihao Dong, Zhiwen Pan, 和 Geoffrey Ye Li. 基于强化学习的合作编码缓存于超密集网络中的动态流行度。IEEE
    车辆技术学报，69(5):5442–5456, 2020年。'
- en: '[79] Pantelis Maniotis and Nikolaos Thomos. Viewport-aware deep reinforcement
    learning approach for 360$\^{o}$ video caching. arXiv preprint arXiv:2003.08473,
    2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Pantelis Maniotis 和 Nikolaos Thomos. 面向360°视频缓存的视口感知深度强化学习方法。arXiv 预印本
    arXiv:2003.08473, 2020年。'
- en: '[80] Xiaoming He, Kun Wang, and Wenyao Xu. Qoe-driven content-centric caching
    with deep reinforcement learning in edge-enabled iot. IEEE Computational Intelligence
    Magazine, 14(4):12–20, 2019.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Xiaoming He, Kun Wang, 和 Wenyao Xu. 基于深度强化学习的QoE驱动内容中心缓存于边缘启用物联网。IEEE
    计算智能杂志，14(4):12–20, 2019年。'
- en: '[81] Jie Tang, Hengbin Tang, Xiuyin Zhang, Kanapathippillai Cumanan, Gaojie
    Chen, Kai-Kit Wong, and Jonathon A Chambers. Energy minimization in d2d-assisted
    cache-enabled internet of things: A deep reinforcement learning approach. IEEE
    Transactions on Industrial Informatics, 16(8):5412–5423, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Jie Tang, Hengbin Tang, Xiuyin Zhang, Kanapathippillai Cumanan, Gaojie
    Chen, Kai-Kit Wong, 和 Jonathon A Chambers. 在D2D辅助的缓存启用物联网中能量最小化：一种深度强化学习方法。IEEE
    工业信息学学报，16(8):5412–5423, 2019年。'
- en: '[82] Pingyang Wu, Jun Li, Long Shi, Ming Ding, Kui Cai, and Fuli Yang. Dynamic
    content update for wireless edge caching via deep reinforcement learning. IEEE
    Communications Letters, 23(10):1773–1777, 2019.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Pingyang Wu, Jun Li, Long Shi, Ming Ding, Kui Cai, 和 Fuli Yang. 通过深度强化学习进行无线边缘缓存的动态内容更新。IEEE
    通信快报，23(10):1773–1777, 2019年。'
- en: '[83] Hao Zhu, Yang Cao, Xiao Wei, Wei Wang, Tao Jiang, and Shi Jin. Caching
    transient data for internet of things: A deep reinforcement learning approach.
    IEEE Internet of Things Journal, 6(2):2074–2083, 2018.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Hao Zhu, Yang Cao, Xiao Wei, Wei Wang, Tao Jiang, 和 Shi Jin. 面向物联网的瞬态数据缓存：一种深度强化学习方法。IEEE
    物联网学报，6(2):2074–2083, 2018年。'
- en: '[84] Alireza Sadeghi, Gang Wang, and Georgios B Giannakis. Deep reinforcement
    learning for adaptive caching in hierarchical content delivery networks. IEEE
    Transactions on Cognitive Communications and Networking, 5(4):1024–1033, 2019.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Alireza Sadeghi, Gang Wang, 和 Georgios B Giannakis. 用于层次内容分发网络的自适应缓存的深度强化学习。IEEE
    认知通信与网络学报，5(4):1024–1033, 2019年。'
- en: '[85] Yimeng Wang, Yongbo Li, Tian Lan, and Vaneet Aggarwal. Deepchunk: Deep
    q-learning for chunk-based caching in wireless data processing networks. IEEE
    Transactions on Cognitive Communications and Networking, 5(4):1034–1045, 2019.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Yimeng Wang, Yongbo Li, Tian Lan, 和 Vaneet Aggarwal. Deepchunk：用于无线数据处理网络中基于块的缓存的深度Q学习。IEEE
    认知通信与网络学报，5(4):1034–1045, 2019年。'
- en: '[86] GM Shafiqur Rahman, Mugen Peng, Shi Yan, and Tian Dang. Learning based
    joint cache and power allocation in fog radio access networks. IEEE Transactions
    on Vehicular Technology, 69(4):4401–4411, 2020.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] GM Shafiqur Rahman, Mugen Peng, Shi Yan, 和 Tian Dang. 基于学习的雾无线接入网络中的联合缓存和功率分配。IEEE
    车辆技术学报，69(4):4401–4411, 2020年。'
- en: '[87] Kyi Thar, Thant Zin Oo, Yan Kyaw Tun, Ki Tae Kim, Choong Seon Hong, et al.
    A deep learning model generation framework for virtualized multi-access edge cache
    management. IEEE Access, 7:62734–62749, 2019.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Kyi Thar, Thant Zin Oo, Yan Kyaw Tun, Ki Tae Kim, Choong Seon Hong, 等.
    用于虚拟化多接入边缘缓存管理的深度学习模型生成框架。IEEE Access，7:62734–62749, 2019年。'
- en: '[88] Ying He, Chengchao Liang, Richard Yu, and Zhu Han. Trust-based social
    networks with computing, caching and communications: A deep reinforcement learning
    approach. IEEE Transactions on Network Science and Engineering, 2018.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Ying He, Chengchao Liang, Richard Yu, 和 Zhu Han. 基于信任的社交网络计算、缓存和通信：一种深度强化学习方法。IEEE
    网络科学与工程学报，2018年。'
- en: '[89] Ying He, Zheng Zhang, F Richard Yu, Nan Zhao, Hongxi Yin, Victor CM Leung,
    and Yanhua Zhang. Deep-reinforcement-learning-based optimization for cache-enabled
    opportunistic interference alignment wireless networks. IEEE Transactions on Vehicular
    Technology, 66(11):10433–10445, 2017.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Ying He, Zheng Zhang, F Richard Yu, Nan Zhao, Hongxi Yin, Victor CM Leung,
    和 Yanhua Zhang. 基于深度强化学习的缓存启用机会干扰对齐无线网络优化。IEEE 车辆技术学报，66(11):10433–10445, 2017年。'
- en: '[90] Ying He, Nan Zhao, and Hongxi Yin. Integrated networking, caching, and
    computing for connected vehicles: A deep reinforcement learning approach. IEEE
    Transactions on Vehicular Technology, 67(1):44–55, 2017.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Ying He, Nan Zhao, 和 Hongxi Yin. 面向连接车辆的集成网络、缓存和计算：一种深度强化学习方法。IEEE 车辆技术学报，67(1):44–55,
    2017年。'
- en: '[91] Guanhua Qiao, Supeng Leng, Sabita Maharjan, Yan Zhang, and Nirwan Ansari.
    Deep reinforcement learning for cooperative content caching in vehicular edge
    computing and networks. IEEE Internet of Things Journal, 7(1):247–257, 2019.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Guanhua Qiao, Supeng Leng, Sabita Maharjan, Yan Zhang, 和 Nirwan Ansari.
    车载边缘计算和网络中用于协作内容缓存的深度强化学习。IEEE 物联网期刊, 7(1):247–257, 2019。'
- en: '[92] Yifei Wei, F Richard Yu, Mei Song, and Zhu Han. Joint optimization of
    caching, computing, and radio resources for fog-enabled iot using natural actor–critic
    deep reinforcement learning. IEEE Internet of Things Journal, 6(2):2061–2073,
    2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Yifei Wei, F Richard Yu, Mei Song, 和 Zhu Han. 使用自然演员–评论家深度强化学习进行雾计算物联网的缓存、计算和无线资源的联合优化。IEEE
    物联网期刊, 6(2):2061–2073, 2018。'
- en: '[93] Zhengming Zhang, Hongyang Chen, Meng Hua, Chunguo Li, Yongming Huang,
    and Luxi Yang. Double coded caching in ultra dense networks: Caching and multicast
    scheduling via deep reinforcement learning. IEEE Transactions on Communications,
    68(2):1071–1086, 2019.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Zhengming Zhang, Hongyang Chen, Meng Hua, Chunguo Li, Yongming Huang,
    和 Luxi Yang. 超密集网络中的双重编码缓存：通过深度强化学习进行缓存和组播调度。IEEE 通讯学报, 68(2):1071–1086, 2019。'
- en: '[94] Shilu Li, Baogang Li, and Wei Zhao. Joint optimization of caching and
    computation in multi-server noma-mec system via reinforcement learning. IEEE Access,
    2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Shilu Li, Baogang Li, 和 Wei Zhao. 通过强化学习在多服务器 noma-mec 系统中缓存和计算的联合优化。IEEE
    Access, 2020。'
- en: '[95] Lixin Li, Yang Xu, Jiaying Yin, Wei Liang, Xu Li, Wei Chen, and Zhu Han.
    Deep reinforcement learning approaches for content caching in cache-enabled d2d
    networks. IEEE Internet of Things Journal, 7(1):544–557, 2019.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Lixin Li, Yang Xu, Jiaying Yin, Wei Liang, Xu Li, Wei Chen, 和 Zhu Han.
    内容缓存的深度强化学习方法在缓存支持的 d2d 网络中。IEEE 物联网期刊, 7(1):544–557, 2019。'
- en: '[96] Mengyuan Lee, Yuanhao Xiong, Guanding Yu, and Geoffrey Ye Li. Deep neural
    networks for linear sum assignment problems. IEEE Wireless Communications Letters,
    7(6):962–965, 2018.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Mengyuan Lee, Yuanhao Xiong, Guanding Yu, 和 Geoffrey Ye Li. 线性和分配问题的深度神经网络。IEEE
    无线通讯快报, 7(6):962–965, 2018。'
- en: '[97] Zhengxuan Ling, Xinyu Tao, Yu Zhang, and Xi Chen. Solving optimization
    problems through fully convolutional networks: An application to the traveling
    salesman problem. IEEE Transactions on Systems, Man, and Cybernetics: Systems,
    2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Zhengxuan Ling, Xinyu Tao, Yu Zhang, 和 Xi Chen. 通过全卷积网络解决优化问题：以旅行商问题为应用。IEEE
    系统、人机与控制论学报: 系统, 2020。'
- en: '[98] Qingmiao Jiang, Yuan Zhang, and Jinyao Yan. Neural combinatorial optimization
    for energy-efficient offloading in mobile edge computing. IEEE Access, 8:35077–35089,
    2020.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Qingmiao Jiang, Yuan Zhang, 和 Jinyao Yan. 移动边缘计算中能效卸载的神经组合优化。IEEE Access,
    8:35077–35089, 2020。'
- en: '[99] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag,
    Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris,
    and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv
    preprint arXiv:1512.07679, 2015.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag,
    Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris,
    和 Ben Coppin. 大规模离散动作空间中的深度强化学习。arXiv 预印本 arXiv:1512.07679, 2015。'
- en: '[100] A. El Saddik. Digital twins: The convergence of multimedia technologies.
    IEEE MultiMedia, 25(2):87–92, 2018.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] A. El Saddik. 数字双胞胎：多媒体技术的融合。IEEE 多媒体, 25(2):87–92, 2018。'
- en: '[101] K. M. Alam and A. El Saddik. C2ps: A digital twin architecture reference
    model for the cloud-based cyber-physical systems. IEEE Access, 5:2050–2062, 2017.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] K. M. Alam 和 A. El Saddik. C2ps: 云计算网络物理系统的数字双胞胎架构参考模型。IEEE Access, 5:2050–2062,
    2017。'
- en: '[102] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer
    learning. Journal of Big data, 3(1):9, 2016.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Karl Weiss, Taghi M Khoshgoftaar, 和 DingDing Wang. 转移学习综述。大数据学报, 3(1):9,
    2016。'
