- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:00:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2006.01423] Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2006.01423] 单目人体姿态估计：基于深度学习方法的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2006.01423](https://ar5iv.labs.arxiv.org/html/2006.01423)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2006.01423](https://ar5iv.labs.arxiv.org/html/2006.01423)
- en: 'Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单目人体姿态估计：基于深度学习方法的综述
- en: Yucheng Chen [chenyucheng@mail.nwpu.edu.cn](mailto:chenyucheng@mail.nwpu.edu.cn)
    Yingli Tian [ytian@ccny.cuny.edu](mailto:ytian@ccny.cuny.edu) Mingyi He [myhe@nwpu.edu.cn](mailto:myhe@nwpu.edu.cn)
    Northwestern Polytechnical University, Xi’an, China The City College, City University
    of New York, NY 10031, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 陈宇成 [chenyucheng@mail.nwpu.edu.cn](mailto:chenyucheng@mail.nwpu.edu.cn) 田颖丽
    [ytian@ccny.cuny.edu](mailto:ytian@ccny.cuny.edu) 贺名义 [myhe@nwpu.edu.cn](mailto:myhe@nwpu.edu.cn)
    西安西北工业大学，美国纽约市城市大学的城市学院
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Vision-based monocular human pose estimation, as one of the most fundamental
    and challenging problems in computer vision, aims to obtain posture of the human
    body from input images or video sequences. The recent developments of deep learning
    techniques have been brought significant progress and remarkable breakthroughs
    in the field of human pose estimation. This survey extensively reviews the recent
    deep learning-based 2D and 3D human pose estimation methods published since 2014\.
    This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation
    metrics, performance comparison, and discusses some promising future research
    directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视觉的单目人体姿态估计，作为计算机视觉中最基本且具有挑战性的问题之一，旨在从输入图像或视频序列中获取人体姿态。深度学习技术的最新发展在人体姿态估计领域带来了显著进展和突破。本文广泛回顾了自2014年以来发表的基于深度学习的2D和3D人体姿态估计方法。本文总结了挑战、主要框架、基准数据集、评估指标、性能比较，并讨论了一些有前景的未来研究方向。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'MSC:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: MSC：
- en: '41A05, 41A10, 65D05, 65D17 \KWDKeyword1, Keyword2, Keyword3 deep learning;
    human pose estimation; survey;^†^†journal: Computer Vision and Image Understanding'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 41A05, 41A10, 65D05, 65D17 \KWDKeyword1, Keyword2, Keyword3 深度学习；人体姿态估计；综述；^†^†期刊：计算机视觉与图像理解
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The human pose estimation (HPE) task, which has been developed for decades,
    aims to obtain posture of the human body from given sensor inputs. Vision-based
    approaches are often used to provide such a solution by using cameras. In recent
    years, with deep learning shows good performance on many computer version tasks
    such as image classification (Krizhevsky et al., [2012](#bib.bib78)), object detection
    (Ren et al., [2015](#bib.bib138)), semantic segmentation (Long et al., [2015](#bib.bib95)),
    etc., HPE also achieves rapid progress by employing deep learning technology.
    The main developments include well-designed networks with great estimation capability,
    richer datasets (Lin et al., [2014](#bib.bib93); Joo et al., [2017](#bib.bib70);
    Mehta et al., [2017a](#bib.bib104)) for feeding networks and more practical exploration
    of body models (Loper et al., [2015](#bib.bib96); Kanazawa et al., [2018](#bib.bib73)).
    Although there are some existing reviews for HPE, however, there still lacks a
    survey to summarize the most recent deep learning-based achievements. This paper
    extensively reviews deep learning-based 2D/3D human pose estimation methods from
    monocular images or video footage of humans. Algorithms relied on other sensors
    such as depth (Shotton et al., [2012](#bib.bib147)), infrared light source (Faessler
    et al., [2014](#bib.bib36)), radio frequency signal (Zhao et al., [2018](#bib.bib183)),
    and multi-view inputs (Rhodin et al., [2018b](#bib.bib140)) are not included in
    this survey.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计（HPE）任务，经过数十年的发展，旨在从给定的传感器输入中获取人体姿态。基于视觉的方法通常通过使用摄像头来提供这样的解决方案。近年来，深度学习在图像分类（Krizhevsky
    et al., [2012](#bib.bib78)）、目标检测（Ren et al., [2015](#bib.bib138)）、语义分割（Long et
    al., [2015](#bib.bib95)）等众多计算机视觉任务上表现良好，HPE也通过应用深度学习技术取得了快速进展。主要的发展包括设计良好的具有出色估计能力的网络、更丰富的数据集（Lin
    et al., [2014](#bib.bib93)；Joo et al., [2017](#bib.bib70)；Mehta et al., [2017a](#bib.bib104)）用于网络训练，以及对身体模型的更实际的探索（Loper
    et al., [2015](#bib.bib96)；Kanazawa et al., [2018](#bib.bib73)）。尽管已有一些关于HPE的现有综述，但仍然缺乏总结最新深度学习成果的调查。本文广泛回顾了基于深度学习的2D/3D人体姿态估计方法，基于单目图像或视频资料。依赖其他传感器如深度（Shotton
    et al., [2012](#bib.bib147)）、红外光源（Faessler et al., [2014](#bib.bib36)）、射频信号（Zhao
    et al., [2018](#bib.bib183)）和多视角输入（Rhodin et al., [2018b](#bib.bib140)）的算法未包含在此调查中。
- en: As one of the fundamental computer vision tasks, HPE is a very important research
    field and can be applied to many applications such as action/activity recognition
    (Li et al., [2017b](#bib.bib81); Luvizon et al., [2018](#bib.bib98); Li et al.,
    [2018b](#bib.bib83)), action detection (Li et al., [2017a](#bib.bib80)), human
    tracking (Insafutdinov et al., [2017](#bib.bib57)), Movies and animation, Virtual
    reality, Human-computer interaction, Video surveillance, Medical assistance, Self-driving,
    Sports motion analysis, etc.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算机视觉的基础任务之一，HPE 是一个非常重要的研究领域，可以应用于许多场景，如动作/活动识别（Li et al., [2017b](#bib.bib81);
    Luvizon et al., [2018](#bib.bib98); Li et al., [2018b](#bib.bib83)）、动作检测（Li et
    al., [2017a](#bib.bib80)）、人体跟踪（Insafutdinov et al., [2017](#bib.bib57)）、电影和动画、虚拟现实、人机交互、视频监控、医疗辅助、自动驾驶、运动动作分析等。
- en: 'Movies and animation: The generation of various vivid digital characters is
    inseparable from the capture of human movements. Cheap and accurate human motion
    capture system can better promote the development of the digital entertainment
    industry.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 电影和动画：各种生动数字角色的生成离不开对人体运动的捕捉。便宜且准确的人体动作捕捉系统可以更好地促进数字娱乐产业的发展。
- en: 'Virtual reality: Virtual reality is a very promising technology that can be
    applied in both education and entertainment. Estimation of human posture can further
    clarify the relation between human and virtual reality world and enhance the interactive
    experience.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟现实：虚拟现实是一项非常有前景的技术，既可以应用于教育也可以应用于娱乐。人体姿态估计可以进一步明确人类与虚拟现实世界之间的关系，并增强互动体验。
- en: 'Human–computer interaction (HCI): HPE is very important for computers and robots
    to better understand the identification, location, and action of people. With
    the posture of human (e.g. gesture), computers and robots can execute instructions
    in an easy way and be more intelligent.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人机交互（HCI）：HPE 对于计算机和机器人更好地理解人的识别、定位和动作非常重要。通过人的姿态（例如手势），计算机和机器人可以以更简单的方式执行指令，变得更智能。
- en: 'Video surveillance: Video surveillance is one of the early applications to
    adopt HPE technology in tracking, action recognition, re-identification people
    within a specific range.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 视频监控：视频监控是早期应用 HPE 技术进行跟踪、动作识别、在特定范围内重新识别人员的应用之一。
- en: 'Medical assistance: In the application of medical assistance, HPE can provide
    physicians with quantitative human motion information especially for rehabilitation
    training and physical therapy.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗辅助：在医疗辅助应用中，HPE 可以为医生提供定量的人体运动信息，尤其是用于康复训练和物理治疗。
- en: 'Self-driving: Advanced self-driving has been developed rapidly. With HPE, self-driving
    cars can respond more appropriately to pedestrians and offer more comprehensive
    interaction with traffic coordinators.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶：先进的自动驾驶技术发展迅速。通过 HPE，自动驾驶汽车可以更适当地响应行人，并提供与交通协调员更全面的互动。
- en: 'Sport motion analysis: Estimating players’ posture in sport videos can further
    obtain the statistics of athletes’ indicators (e.g. running distance, number of
    jumps). During training, HPE can provide a quantitative analysis of action details.
    In physical education, instructors can make more objective evaluations of students
    with HPE.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 运动动作分析：估计运动视频中运动员的姿态可以进一步获取运动员指标的统计数据（例如跑步距离、跳跃次数）。在训练中，HPE 可以提供动作细节的定量分析。在体育教育中，教练可以通过
    HPE 对学生进行更客观的评估。
- en: '![Refer to caption](img/43532b9809672c0d91868ce4e0ef3bef.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43532b9809672c0d91868ce4e0ef3bef.png)'
- en: 'Fig. 1: Typical challenges of HPE in monocular images or videos. Example images
    are from Max Planck Institute for Informatics (MPII) dataset (Andriluka et al.,
    [2014](#bib.bib4)).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：单目图像或视频中 HPE 的典型挑战。示例图像来自马克斯·普朗克信息学研究所（MPII）数据集（Andriluka et al., [2014](#bib.bib4)）。
- en: 'Monocular human pose estimation has some unique characteristics and challenges.
    As shown in Fig. [1](#S1.F1 "Fig. 1 ‣ 1 Introduction ‣ Monocular Human Pose Estimation:
    A Survey of Deep Learning-based Methods"), the challenges of human pose estimation
    mainly fall in three aspects:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 单目人体姿态估计具有一些独特的特征和挑战。如图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ 单目人体姿态估计：基于深度学习的方法综述")所示，人体姿态估计的挑战主要体现在三个方面：
- en: '1.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Flexible body configuration indicates complex interdependent joints and high
    degree-of-freedom limbs, which may cause self-occlusions or rare/complex poses.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灵活的身体配置表示复杂的相互依赖的关节和高自由度的肢体，这可能导致自遮挡或稀有/复杂的姿态。
- en: '2.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: Diverse body appearance includes different clothing and self-similar parts.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多样化的身体外观包括不同的服装和自我相似的部分。
- en: '3.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: Complex environment may cause foreground occlusion, occlusion or similar parts
    from nearby persons, various viewing angles, and truncation in the camera view.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复杂的环境可能导致前景遮挡、邻近人物的遮挡或类似部位、各种视角以及相机视野中的截断。
- en: 'The papers of human pose estimation can be categorized in different ways. Based
    on whether to use designed human body models or not, the methods can be categorized
    into generative methods (model-based) and discriminative methods (model-free).
    According to from which level (high-level abstraction or low-level pixel evidence)
    to start the processing, they can be classified into top-down methods and bottom-up
    methods. More details of different category strategies for HPE approaches are
    summarized in Table [2](#S2.T2 "Table 2 ‣ 2 Categories of HPE Methods and Human
    Body Models ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") and described in Section [2.1](#S2.SS1 "2.1 HPE Method Categories ‣
    2 Categories of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation:
    A Survey of Deep Learning-based Methods").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '人体姿态估计的论文可以通过不同的方式进行分类。根据是否使用设计的人体模型，这些方法可以分为生成方法（基于模型）和判别方法（无模型）。根据处理的起始层次（高层次抽象或低层次像素证据），它们可以被分类为自上而下的方法和自下而上的方法。不同类别策略的详细信息汇总在表格[2](#S2.T2
    "Table 2 ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular Human
    Pose Estimation: A Survey of Deep Learning-based Methods")中，并在第[2.1节](#S2.SS1
    "2.1 HPE Method Categories ‣ 2 Categories of HPE Methods and Human Body Models
    ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods")中进行了描述。'
- en: 'As listed in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Monocular Human Pose
    Estimation: A Survey of Deep Learning-based Methods"), with the development of
    human pose estimation in the past decades, several notable surveys summarized
    the research work in this area. The surveys (Aggarwal and Cai, [1999](#bib.bib1);
    Gavrila, [1999](#bib.bib42); Poppe, [2007](#bib.bib134); Ji and Liu, [2010](#bib.bib67);
    Moeslund et al., [2011](#bib.bib111)) reviewed the early work of human motion
    analysis in many aspects (e.g., detection and tracking, pose estimation, recognition)
    and described the relation between human pose estimation and other related tasks.
    While Hu et al. ([2004](#bib.bib54)) summarized the research of human motion analysis
    for video surveillance application, the reviews (Moeslund and Granum, [2001](#bib.bib109);
    Moeslund et al., [2006](#bib.bib110)) focused on the human motion capture systems.
    More recent surveys were mainly focusing on relatively narrow directions, such
    as RGB-D-based action recognition(Chen et al., [2013](#bib.bib18); Wang et al.,
    [2018b](#bib.bib172)), 3D HPE (Sminchisescu, [2008](#bib.bib150); Holte et al.,
    [2012](#bib.bib52); Sarafianos et al., [2016](#bib.bib145)), model-based HPE (Holte
    et al., [2012](#bib.bib52); Perez-Sala et al., [2014](#bib.bib128)), body parts-based
    HPE (Liu et al., [2015](#bib.bib94)), and monocular-based HPE (Sminchisescu, [2008](#bib.bib150);
    Gong et al., [2016](#bib.bib47)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Monocular Human Pose Estimation: A
    Survey of Deep Learning-based Methods")所列，随着过去几十年人体姿态估计的发展，一些显著的综述总结了这一领域的研究工作。这些综述（Aggarwal
    and Cai, [1999](#bib.bib1); Gavrila, [1999](#bib.bib42); Poppe, [2007](#bib.bib134);
    Ji and Liu, [2010](#bib.bib67); Moeslund et al., [2011](#bib.bib111)）从多个方面（例如检测与跟踪、姿态估计、识别）回顾了人体运动分析的早期工作，并描述了人体姿态估计与其他相关任务之间的关系。虽然Hu
    et al. ([2004](#bib.bib54))总结了用于视频监控应用的人体运动分析研究，但综述（Moeslund and Granum, [2001](#bib.bib109);
    Moeslund et al., [2006](#bib.bib110)）则专注于人体运动捕捉系统。最近的综述主要集中于相对狭窄的方向，例如RGB-D基础的动作识别（Chen
    et al., [2013](#bib.bib18); Wang et al., [2018b](#bib.bib172)）、3D HPE（Sminchisescu,
    [2008](#bib.bib150); Holte et al., [2012](#bib.bib52); Sarafianos et al., [2016](#bib.bib145)）、基于模型的HPE（Holte
    et al., [2012](#bib.bib52); Perez-Sala et al., [2014](#bib.bib128)）、基于身体部位的HPE（Liu
    et al., [2015](#bib.bib94)）以及单目HPE（Sminchisescu, [2008](#bib.bib150); Gong et
    al., [2016](#bib.bib47)）。'
- en: Different from existing review papers, this survey extensively summarizes the
    recent milestone work of deep learning-based human pose estimation methods, which
    were mainly published from 2014\. In order to provide a comprehensive summary,
    this survey includes a few research work which has been discussed in some surveys
    (Liu et al., [2015](#bib.bib94); Gong et al., [2016](#bib.bib47); Sarafianos et al.,
    [2016](#bib.bib145)), but most of the recent advances are not been presented in
    any survey before.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有的综述论文不同，这项调查广泛总结了自2014年发布的基于深度学习的人体姿态估计方法的最新里程碑工作。为了提供全面的总结，本调查包括了一些在某些综述中讨论过的研究工作（Liu
    et al., [2015](#bib.bib94); Gong et al., [2016](#bib.bib47); Sarafianos et al.,
    [2016](#bib.bib145)），但大多数最近的进展在之前的任何综述中都没有呈现。
- en: 'The remainder of this paper is organized as follows. Section [2](#S2 "2 Categories
    of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation: A Survey
    of Deep Learning-based Methods") introduces the existing review papers for human
    motion analysis and HPE, different ways to category HPE methods, and the widely
    used human body models. Sections [3](#S3 "3 2D Human Pose Estimation ‣ Monocular
    Human Pose Estimation: A Survey of Deep Learning-based Methods") and [4](#S4 "4
    3D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") describe 2D HPE and 3D HPE approaches respectively. In each section,
    we further describe HPE approaches for both single person pose estimation and
    multi-person pose estimation. Since data are a very important and fundamental
    element for deep learning-based methods, the recent HPE datasets and the evaluation
    metrics are summarized in Section [5](#S5 "5 Datasets and evaluation protocols
    ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods").
    Finally, Section [6](#S6 "6 Conclusion and Future Research Directions ‣ Monocular
    Human Pose Estimation: A Survey of Deep Learning-based Methods") concludes the
    paper and discusses several promising future research directions.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。第[2](#S2 "2 类别的HPE方法和人体模型 ‣ 单目人体姿态估计：基于深度学习的方法综述")节介绍了现有的有关人体运动分析和HPE的综述论文、HPE方法的不同分类方式，以及广泛使用的人体模型。第[3](#S3
    "3 2D 人体姿态估计 ‣ 单目人体姿态估计：基于深度学习的方法综述")节和第[4](#S4 "4 3D 人体姿态估计 ‣ 单目人体姿态估计：基于深度学习的方法综述")节分别描述了2D
    HPE和3D HPE方法。在每一节中，我们进一步描述了单人姿态估计和多人姿态估计的HPE方法。由于数据是深度学习方法中非常重要和基础的元素，第[5](#S5
    "5 数据集和评估协议 ‣ 单目人体姿态估计：基于深度学习的方法综述")节总结了近期的HPE数据集和评估指标。最后，第[6](#S6 "6 结论和未来研究方向
    ‣ 单目人体姿态估计：基于深度学习的方法综述")节总结了论文并讨论了几个有前景的未来研究方向。
- en: 'Table 1: Summary of the related surveys of human motion analysis and HPE.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：人体运动分析和HPE相关综述的总结。
- en: '| No. | Survey $\&amp;$ Reference | Venue | Content |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 序号 | 综述 $\&amp;$ 参考文献 | 发表场所 | 内容 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 1 &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1 &#124;'
- en: '| Human motion analysis: A review (Aggarwal and Cai, [1999](#bib.bib1)) | CVIU
    | A review of human motion analysis including body structure analysis, motion
    tracking and action recognition. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 人体运动分析：综述 (Aggarwal 和 Cai, [1999](#bib.bib1)) | CVIU | 包括身体结构分析、运动跟踪和动作识别的人体运动分析综述。
    |'
- en: '| 2 | The visual analysis of human movement: A survey (Gavrila, [1999](#bib.bib42))
    | CVIU | A survey of whole-body and hand motion analysis. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 人体运动的视觉分析：综述 (Gavrila, [1999](#bib.bib42)) | CVIU | 全身和手部运动分析的综述。 |'
- en: '| 3 | A survey of computer vision-based human motion capture (Moeslund and
    Granum, [2001](#bib.bib109)) | CVIU | An overview based on motion capture system,
    including initialization, tracking, pose estimation, and recognition. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 基于计算机视觉的人体运动捕捉综述 (Moeslund 和 Granum, [2001](#bib.bib109)) | CVIU | 基于运动捕捉系统的概述，包括初始化、跟踪、姿态估计和识别。
    |'
- en: '| 4 | A survey on visual surveillance of object motion and behaviors (Hu et al.,
    [2004](#bib.bib54)) | TSMCS | A summary of human motion analysis based one the
    framework of visual surveillance in dynamic scenes. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 关于物体运动和行为的视觉监控综述 (Hu et al., [2004](#bib.bib54)) | TSMCS | 基于动态场景视觉监控框架的人体运动分析总结。
    |'
- en: '| 5 | A survey of advances in vision-based human motion capture and analysis
    (Moeslund et al., [2006](#bib.bib110)) | CVIU | Further summary of human motion
    capture and analysis from 2000 to 2006, following (Moeslund and Granum, [2001](#bib.bib109)).
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 基于视觉的人体运动捕捉和分析的进展综述 (Moeslund et al., [2006](#bib.bib110)) | CVIU | 从2000年到2006年的人体运动捕捉和分析的进一步总结，继承自(Moeslund
    和 Granum, [2001](#bib.bib109))。 |'
- en: '| 6 | Vision-based human motion analysis: An overview (Poppe, [2007](#bib.bib134))
    | CVIU | A summary of vision-based human motion analysis with markerless data.
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 基于视觉的人体运动分析：概述 (Poppe, [2007](#bib.bib134)) | CVIU | 无标记数据的基于视觉的人体运动分析总结。
    |'
- en: '| 7 | 3D human motion analysis in monocular video: techniques and challenges
    (Sminchisescu, [2008](#bib.bib150)) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 单目视频中的3D人体运动分析：技术和挑战 (Sminchisescu, [2008](#bib.bib150)) |'
- en: '&#124; Book &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 书籍 &#124;'
- en: '&#124; Chapter &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 章节 &#124;'
- en: '| An overview of reconstructing 3D human motion with video sequences from single-view
    camera. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 单视角摄像机视频序列重建三维人体运动的概述。 |'
- en: '| 8 | Advances in view-invariant human motion analysis: A review (Ji and Liu,
    [2010](#bib.bib67)) | TSMCS | A summary of human motion analysis, including human
    detection, view-invariant pose representation and estimation, and behavior understanding.
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 基于视角不变的人体运动分析进展：综述（Ji 和 Liu，[2010](#bib.bib67)） | TSMCS | 人体运动分析的总结，包括人体检测、视角不变的姿势表示与估计，以及行为理解。
    |'
- en: '| 9 | Visual analysis of humans (Moeslund et al., [2011](#bib.bib111)) | Book
    | A comprehensive overview of human analysis, including detection and tracking,
    pose estimation, recognition, and applications with human body and face. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 人体的视觉分析（Moeslund 等，[2011](#bib.bib111)） | 书籍 | 人体分析的综合概述，包括检测与跟踪、姿势估计、识别以及人体和面部的应用。
    |'
- en: '| 10 | Human pose estimation and activity recognition from multi-view videos:
    Comparative explorations of recent developments (Holte et al., [2012](#bib.bib52))
    | JSTSP | A review of model-based 3D HPE and action recognition methods under
    multi-view. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 从多视角视频中估计人体姿势与活动识别：近期发展比较探索（Holte 等，[2012](#bib.bib52)） | JSTSP | 基于模型的3D
    HPE和动作识别方法在多视角下的综述。 |'
- en: '| 11 | A survey of human motion analysis using depth imagery (Chen et al.,
    [2013](#bib.bib18)) | PRL | A survey of traditional RGB-D-based human action recognition
    methods, including description of sensors, corresponding datasets, and approaches.
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 使用深度图像的人体运动分析调查（Chen 等，[2013](#bib.bib18)） | PRL | 传统的基于RGB-D的人体动作识别方法的调查，包括传感器的描述、相关数据集和方法。
    |'
- en: '| 12 | A survey on model based approaches for 2D and 3D visual human pose recovery
    (Perez-Sala et al., [2014](#bib.bib128)) | Sensors | A survey of model-based approaches
    for HPE, grouped in five main modules: appearance, viewpoint, spatial relations,
    temporal consistence, and behavior. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 2D和3D视觉人体姿势恢复的模型基础方法综述（Perez-Sala 等，[2014](#bib.bib128)） | 传感器 | 基于模型的HPE方法的综述，分为五个主要模块：外观、视角、空间关系、时间一致性和行为。
    |'
- en: '| 13 | A survey of human pose estimation: the body parts parsing based methods
    (Liu et al., [2015](#bib.bib94)) | JVCIR | A survey of body parts parsing-based
    HPE methods under both single-view and multiple-view from different input sources(images,
    videos, depth). |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 人体姿势估计的调查：基于身体部位解析的方法（Liu 等，[2015](#bib.bib94)） | JVCIR | 基于身体部位解析的HPE方法的调查，涵盖单视角和多视角的不同输入源（图像、视频、深度）。
    |'
- en: '| 14 | Human pose estimation from monocular images: A comprehensive survey
    (Gong et al., [2016](#bib.bib47)) | Sensors | A survey of monocular-based traditional
    HPE methods with a few deep learning-based methods. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 从单目图像中估计人体姿势：全面综述（Gong 等，[2016](#bib.bib47)） | 传感器 | 单目基础的传统HPE方法的综述，以及少量基于深度学习的方法。
    |'
- en: '| 15 | 3d human pose estimation: A review of the literature and analysis of
    covariates (Sarafianos et al., [2016](#bib.bib145)) | CVIU | A review of 3D HPE
    methods with different type of inputs(e.g., single image or video, monocular or
    multi-view). |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 3D人体姿势估计：文献综述与协变量分析（Sarafianos 等，[2016](#bib.bib145)） | CVIU | 3D HPE方法的综述，涵盖不同类型的输入（例如，单张图像或视频，单目或多视角）。
    |'
- en: '| 16 | RGB-D-based human motion recognition with deep learning: A survey (Wang
    et al., [2018b](#bib.bib172)) | CVIU | A survey of RGB-D-based motion recognition
    in four categories: RGB-based, depth-based, skeleton-based, and RGB-D-based. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 基于RGB-D的深度学习人体运动识别：综述（Wang 等，[2018b](#bib.bib172)） | CVIU | RGB-D基础的运动识别的调查，分为四类：基于RGB的、基于深度的、基于骨架的和基于RGB-D的。
    |'
- en: '| 17 | Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods
    | Ours | A comprehensive survey of deep learning-based monocular HPE research
    and human pose datasets, organized into four groups: 2D single HPE, 2D multi-HPE,
    3D single HPE and 3D multi-HPE |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 17 | 单目人体姿势估计：基于深度学习的方法综述 | 我们的 | 深度学习基础的单目HPE研究及人体姿势数据集的全面综述，分为四组：2D单目HPE、2D多目HPE、3D单目HPE和3D多目HPE。
    |'
- en: 2 Categories of HPE Methods and Human Body Models
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 HPE方法与人体模型的类别
- en: 'Table 2: The Categories of deep learning-based monocular human pose estimation.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于深度学习的单目人体姿势估计的类别。
- en: '| Direction | Sub-direction | Categories | Sub-categories |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 方向 | 子方向 | 类别 | 子类别 |'
- en: '| 2D HPE | 2D Single | Regression-based | (1) Direct prediction: (Krizhevsky
    et al., [2012](#bib.bib78)), on video (Pfister et al., [2014](#bib.bib130)) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 2D HPE | 2D 单目 | 基于回归 | (1) 直接预测：（Krizhevsky 等，[2012](#bib.bib78)），在视频上（Pfister
    等，[2014](#bib.bib130)） |'
- en: '| (2) Supervision improvement: transform heatmaps to joint coordinates (Luvizon
    et al., [2017](#bib.bib99); Nibali et al., [2018](#bib.bib116)), recursive refinement (Carreira
    et al., [2016](#bib.bib14)), bone-based constraint (Sun et al., [2017](#bib.bib152))
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| (2) 监督改进：将热图转换为关节坐标 (Luvizon et al., [2017](#bib.bib99)；Nibali et al., [2018](#bib.bib116))，递归细化 (Carreira
    et al., [2016](#bib.bib14))，基于骨架的约束 (Sun et al., [2017](#bib.bib152)) |'
- en: '| (3) Multi-task: with body part detection (Li et al., [2014](#bib.bib88)),
    with person detection and action classification (Gkioxari et al., [2014a](#bib.bib44)),
    with heatmap-based joint detection (Fan et al., [2015](#bib.bib37)), with action
    recognition on video sequences (Luvizon et al., [2018](#bib.bib98)) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| (3) 多任务：带有身体部位检测 (Li et al., [2014](#bib.bib88))，带有人物检测和动作分类 (Gkioxari et al.,
    [2014a](#bib.bib44))，带有基于热图的关节检测 (Fan et al., [2015](#bib.bib37))，带有视频序列上的动作识别 (Luvizon
    et al., [2018](#bib.bib98)) |'
- en: '| Detection-based | (1) Patch-based: (Jain et al., [2013](#bib.bib63); Chen
    and Yuille, [2014](#bib.bib20); Ramakrishna et al., [2014](#bib.bib137)) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 基于检测 | (1) 基于补丁： (Jain et al., [2013](#bib.bib63)；Chen and Yuille, [2014](#bib.bib20)；Ramakrishna
    et al., [2014](#bib.bib137)) |'
- en: '| (2) Network design: (Tompson et al., [2015](#bib.bib163); Bulat and Tzimiropoulos,
    [2016](#bib.bib12); Xiao et al., [2018](#bib.bib176)), multi-scale inputs (Rafi
    et al., [2016](#bib.bib136)), heatmap-based improvement (Papandreou et al., [2017](#bib.bib123)),
    Hourglass (Newell et al., [2016](#bib.bib115)), CPM (Wei et al., [2016](#bib.bib174)),
    PRM (Yang et al., [2017](#bib.bib177)), feed forward module (Belagiannis and Zisserman,
    [2017](#bib.bib7)), HRNet (Sun et al., [2019](#bib.bib151)), GAN (Chou et al.,
    [2017](#bib.bib23); Chen et al., [2017](#bib.bib21); Peng et al., [2018](#bib.bib127))
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| (2) 网络设计： (Tompson et al., [2015](#bib.bib163)；Bulat and Tzimiropoulos, [2016](#bib.bib12)；Xiao
    et al., [2018](#bib.bib176))，多尺度输入 (Rafi et al., [2016](#bib.bib136))，基于热图的改进 (Papandreou
    et al., [2017](#bib.bib123))，Hourglass (Newell et al., [2016](#bib.bib115))，CPM (Wei
    et al., [2016](#bib.bib174))，PRM (Yang et al., [2017](#bib.bib177))，前馈模块 (Belagiannis
    and Zisserman, [2017](#bib.bib7))，HRNet (Sun et al., [2019](#bib.bib151))，GAN (Chou
    et al., [2017](#bib.bib23)；Chen et al., [2017](#bib.bib21)；Peng et al., [2018](#bib.bib127))
    |'
- en: '| (3) Body structure constraint: (Tompson et al., [2014](#bib.bib164); Lifshitz
    et al., [2016](#bib.bib91); Yang et al., [2016](#bib.bib178); Gkioxari et al.,
    [2016](#bib.bib46); Chu et al., [2016](#bib.bib24), [2017](#bib.bib25); Ning et al.,
    [2018](#bib.bib119); Ke et al., [2018](#bib.bib74); Tang et al., [2018a](#bib.bib157);
    Tang and Wu, [2019](#bib.bib156)) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (3) 身体结构约束： (Tompson et al., [2014](#bib.bib164)；Lifshitz et al., [2016](#bib.bib91)；Yang
    et al., [2016](#bib.bib178)；Gkioxari et al., [2016](#bib.bib46)；Chu et al., [2016](#bib.bib24)，[2017](#bib.bib25)；Ning
    et al., [2018](#bib.bib119)；Ke et al., [2018](#bib.bib74)；Tang et al., [2018a](#bib.bib157)；Tang
    and Wu, [2019](#bib.bib156)) |'
- en: '| (4) Temporal constraint: (Jain et al., [2014](#bib.bib64); Pfister et al.,
    [2015](#bib.bib129); Luo et al., [2018](#bib.bib97)) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| (4) 时间约束： (Jain et al., [2014](#bib.bib64)；Pfister et al., [2015](#bib.bib129)；Luo
    et al., [2018](#bib.bib97)) |'
- en: '| (5) Network compression: (Tang et al., [2018b](#bib.bib158); Debnath et al.,
    [2018](#bib.bib28); Feng et al., [2019](#bib.bib40)) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (5) 网络压缩： (Tang et al., [2018b](#bib.bib158)；Debnath et al., [2018](#bib.bib28)；Feng
    et al., [2019](#bib.bib40)) |'
- en: '| 2D Multiple | Top-down | coarse-to-fine (Iqbal and Gall, [2016](#bib.bib60);
    Huang et al., [2017](#bib.bib55)), bounding box refinement (Fang et al., [2017](#bib.bib38)),
    multi-level feature fusion (Xiao et al., [2018](#bib.bib176); Chen et al., [2018](#bib.bib22)),
    results refinement (Moon et al., [2019](#bib.bib112)) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 2D 多重 | 自上而下 | 粗到细 (Iqbal and Gall, [2016](#bib.bib60)；Huang et al., [2017](#bib.bib55))，边界框细化 (Fang
    et al., [2017](#bib.bib38))，多级特征融合 (Xiao et al., [2018](#bib.bib176)；Chen et al.,
    [2018](#bib.bib22))，结果细化 (Moon et al., [2019](#bib.bib112)) |'
- en: '| Bottom-up | (1) Two-stage: DeepCut (Pishchulin et al., [2016](#bib.bib131)),
    DeeperCut (Insafutdinov et al., [2016](#bib.bib58)), OpenPose (Cao et al., [2016](#bib.bib13)),
    PPN (Nie et al., [2018](#bib.bib118)), PifPafNet (Kreiss et al., [2019](#bib.bib77))
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上 | (1) 两阶段：DeepCut (Pishchulin et al., [2016](#bib.bib131))，DeeperCut (Insafutdinov
    et al., [2016](#bib.bib58))，OpenPose (Cao et al., [2016](#bib.bib13))，PPN (Nie
    et al., [2018](#bib.bib118))，PifPafNet (Kreiss et al., [2019](#bib.bib77)) |'
- en: '| (2) Single-stage: heatmaps and associative embedding maps (Newell et al.,
    [2017](#bib.bib114)) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| (2) 单阶段：热图和关联嵌入图 (Newell et al., [2017](#bib.bib114)) |'
- en: '| (3) Multi-task: instance segmentation (Papandreou et al., [2018](#bib.bib122)),
    keypoint detection and semantic segmentation (Kocabas et al., [2018](#bib.bib76))
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| (3) 多任务：实例分割 (Papandreou et al., [2018](#bib.bib122))，关键点检测和语义分割 (Kocabas
    et al., [2018](#bib.bib76)) |'
- en: '| 3D HPE | 3D Single | Model-free | (1) Single-stage: direct prediction (Li
    and Chan, [2014](#bib.bib87); Pavlakos et al., [2017](#bib.bib125)), body structure
    constraint (Li et al., [2015b](#bib.bib89); Tekin et al., [2016](#bib.bib159);
    Sun et al., [2017](#bib.bib152); Pavlakos et al., [2018a](#bib.bib124)) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 3D 人体姿态估计 | 3D 单人 | 无模型 | (1) 单阶段：直接预测 (Li 和 Chan，[2014](#bib.bib87)；Pavlakos
    等，[2017](#bib.bib125))，身体结构约束 (Li 等，[2015b](#bib.bib89)；Tekin 等，[2016](#bib.bib159)；Sun
    等，[2017](#bib.bib152)；Pavlakos 等，[2018a](#bib.bib124)) |'
- en: '| (2) 2D-to-3D: (Martinez et al., [2017](#bib.bib103); Zhou et al., [2017](#bib.bib184);
    Tekin et al., [2017](#bib.bib160); Li and Lee, [2019](#bib.bib85); Qammaz and
    Argyros, [2019](#bib.bib135); Chen and Ramanan, [2017](#bib.bib17); Moreno-Noguer,
    [2017](#bib.bib113); Wang et al., [2018a](#bib.bib171); Yang et al., [2018](#bib.bib179))
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| (2) 2D转3D： (Martinez 等，[2017](#bib.bib103)；Zhou 等，[2017](#bib.bib184)；Tekin
    等，[2017](#bib.bib160)；Li 和 Lee，[2019](#bib.bib85)；Qammaz 和 Argyros，[2019](#bib.bib135)；Chen
    和 Ramanan，[2017](#bib.bib17)；Moreno-Noguer，[2017](#bib.bib113)；Wang 等，[2018a](#bib.bib171)；Yang
    等，[2018](#bib.bib179)) |'
- en: '| Model-based | (1) SMPL-based: (Bogo et al., [2016](#bib.bib8); Tan et al.,
    [2017](#bib.bib155); Pavlakos et al., [2018b](#bib.bib126); Omran et al., [2018](#bib.bib120);
    Varol et al., [2018](#bib.bib167); Kanazawa et al., [2018](#bib.bib73); Arnab
    et al., [2019](#bib.bib6)) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 基于模型 | (1) 基于SMPL： (Bogo 等，[2016](#bib.bib8)；Tan 等，[2017](#bib.bib155)；Pavlakos
    等，[2018b](#bib.bib126)；Omran 等，[2018](#bib.bib120)；Varol 等，[2018](#bib.bib167)；Kanazawa
    等，[2018](#bib.bib73)；Arnab 等，[2019](#bib.bib6)) |'
- en: '| (2) Kinematic model-based: (Mehta et al., [2017a](#bib.bib104); Nie et al.,
    [2017](#bib.bib117); Zhou et al., [2016](#bib.bib185); Mehta et al., [2017c](#bib.bib107);
    Rhodin et al., [2018a](#bib.bib139)) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| (2) 基于运动学模型： (Mehta 等，[2017a](#bib.bib104)；Nie 等，[2017](#bib.bib117)；Zhou
    等，[2016](#bib.bib185)；Mehta 等，[2017c](#bib.bib107)；Rhodin 等，[2018a](#bib.bib139))
    |'
- en: '|  |  | (3) Other model-based: probabilistic model (Tome et al., [2017](#bib.bib162))
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (3) 其他基于模型：概率模型 (Tome 等，[2017](#bib.bib162)) |'
- en: '|  | 3D Multiple |  | bottom-up (Mehta et al., [2017b](#bib.bib106)), top-down (Rogez
    et al., [2017](#bib.bib141)), SMPL-based (Zanfir et al., [2018](#bib.bib181)),
    real-time (Mehta et al., [2019](#bib.bib105)) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | 3D 多人 |  | 自下而上 (Mehta 等，[2017b](#bib.bib106))，自上而下 (Rogez 等，[2017](#bib.bib141))，基于SMPL
    (Zanfir 等，[2018](#bib.bib181))，实时 (Mehta 等，[2019](#bib.bib105)) |'
- en: 2.1 HPE Method Categories
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 人体姿态估计方法类别
- en: 'This section summarizes the different categories of deep learning-based HPE
    methods based on different characteristics: 1) generative (human body model-based)
    and discriminative (human body model-free); 2) top-down (from high-level abstraction
    to low-level pixel evidence) and bottom-up (from low-level pixel evidence to high-level
    abstraction); 3) regression-based (directly mapping from input images to body
    joint positions) and detection-based (generating intermediate image patches or
    heatmaps of joint locations); and 4) one-stage (end-to-end training) and multi-stage
    (stage-by-stage training).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了基于深度学习的人体姿态估计方法的不同类别，依据不同特征进行分类：1) 生成型（基于人体模型）和判别型（无人体模型）；2) 自上而下（从高层抽象到低层像素证据）和自下而上（从低层像素证据到高层抽象）；3)
    基于回归（直接从输入图像映射到身体关节位置）和基于检测（生成关节位置的中间图像补丁或热图）；4) 单阶段（端到端训练）和多阶段（逐阶段训练）。
- en: 'Generative V.S. Discriminative: The main difference between generative and
    discriminative methods is whether a method uses human body models or not. Based
    on the different representations of human body models, generative methods can
    be processed in different ways such as prior beliefs about the structure of the
    body model, geometrically projection from different views to 2D or 3D space, high-dimensional
    parametric space optimization in regression manners. More details of human body
    model representation can be found in Section [2.2](#S2.SS2 "2.2 Human Body models
    ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation:
    A Survey of Deep Learning-based Methods"). Discriminative methods directly learn
    a mapping from input sources to human pose space (learning-based) or search in
    existing examples (example-based) without using human body models. Discriminative
    methods are usually faster than generative methods but may have less robustness
    for poses never trained with.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '生成式与判别式：生成式方法和判别式方法之间的主要区别在于方法是否使用人体模型。根据人体模型的不同表示，生成式方法可以通过不同的方式处理，例如关于人体模型结构的先验知识、从不同视角到二维或三维空间的几何投影、高维参数空间优化的回归方法。有关人体模型表示的更多细节可以在第[2.2](#S2.SS2
    "2.2 Human Body models ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular
    Human Pose Estimation: A Survey of Deep Learning-based Methods")节中找到。判别式方法直接学习从输入源到人体姿态空间的映射（基于学习）或在现有示例中进行搜索（基于示例），而不使用人体模型。判别式方法通常比生成式方法更快，但在未训练过的姿态上可能鲁棒性较差。'
- en: 'Top-down V.S. Bottom-up: For multi-person pose estimation, HPE methods can
    generally be classified as top-down and bottom-up methods according to the starting
    point of the prediction: high-level abstraction or low-level pixel evidence. Top-down
    methods start from high-level abstraction to first detect persons and generate
    the person locations in bounding boxes. Then pose estimation is conducted for
    each person. In contrast, bottom-up methods first predict all body parts of every
    person in the input image and then group them either by human body model fitting
    or other algorithms. Note that body parts could be joints, limbs, or small template
    patches depending on different methods. With an increased number of people in
    an image, the computation cost of top-down methods significantly increases, while
    keeps stable for bottom-up methods. However, if there are some people with a large
    overlap, bottom-up methods face challenges to group corresponding body parts.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自上而下与自下而上：对于多人姿态估计，HPE 方法通常可以根据预测的起点分为自上而下和自下而上两种方法：高层抽象或低层像素证据。自上而下的方法从高层抽象开始，首先检测人物并生成人物的边界框位置。然后对每个人进行姿态估计。相反，自下而上的方法首先预测输入图像中每个人的所有身体部位，然后通过人体模型拟合或其他算法将它们分组。需要注意的是，身体部位可以是关节、四肢或小模板补丁，具体取决于不同的方法。随着图像中人物数量的增加，自上而下的方法的计算成本显著增加，而自下而上的方法则保持稳定。然而，如果有一些人存在较大的重叠，自下而上的方法在分组相应身体部位时面临挑战。
- en: 'Regression-based V.S. Detection-based: Based on the different problem formulations,
    deep learning-based human pose estimation methods can be split into regression-based
    or detection-based methods. The regression-based methods directly map the input
    image to the coordinates of body joints or the parameters of human body models.
    The detection-based methods treat the body parts as detection targets based on
    two widely used representations: image patches and heatmaps of joint locations.
    Direct mapping from images to joint coordinates is very difficult since it is
    a highly nonlinear problem, while small-region representation provides dense pixel
    information with stronger robustness. Compared to the original image size, the
    detected results of small-region representation limit the accuracy of the final
    joint coordinates.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基于回归与基于检测：根据不同的问题表述，基于深度学习的人体姿态估计方法可以分为基于回归和基于检测的方法。基于回归的方法直接将输入图像映射到身体关节的坐标或人体模型的参数。基于检测的方法将身体部位视为检测目标，基于两种广泛使用的表示：图像块和关节位置的热图。从图像到关节坐标的直接映射非常困难，因为这是一个高度非线性的问题，而小区域表示提供了更密集的像素信息，具有更强的鲁棒性。与原始图像大小相比，小区域表示的检测结果限制了最终关节坐标的准确性。
- en: 'One-stage V.S. Multi-stage: The deep learning-based one-stage methods aim to
    map the input image to human poses by employing end-to-end networks, while multi-stage
    methods usually predict human pose in multiple stages and are accompanied by intermediate
    supervision. For example, some multi-person pose estimation methods first detect
    the locations of people and then estimate the human pose for each detected person.
    Other 3D human pose estimation methods first predict joint locations in the 2D
    surface, then extend them to 3D space. The training of one-stage methods is easier
    than multi-stage methods, but with less intermediate constraints.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶段方法与多阶段方法的对比：基于深度学习的一阶段方法旨在通过使用端到端网络将输入图像映射到人体姿态，而多阶段方法通常在多个阶段预测人体姿态，并伴有中间监督。例如，一些多人体姿态估计方法首先检测人的位置，然后为每个检测到的人估计人体姿态。其他
    3D 人体姿态估计方法首先在 2D 平面上预测关节位置，然后将其扩展到 3D 空间。一阶段方法的训练比多阶段方法更容易，但约束条件较少。
- en: 'This survey reviews the recent work in two main sections: 2D human pose estimation
    (Section 3) and 3D human pose estimation (Section 4). For each section, we further
    divide them into subsections based on their respective characteristics (see a
    summary of all the categories and the corresponding papers in Table [2](#S2.T2
    "Table 2 ‣ 2 Categories of HPE Methods and Human Body Models ‣ Monocular Human
    Pose Estimation: A Survey of Deep Learning-based Methods").)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查综述了最近的工作，分为两个主要部分：2D 人体姿态估计（第 3 节）和 3D 人体姿态估计（第 4 节）。对于每一部分，我们进一步根据其各自的特点将其细分为子部分（有关所有类别及对应论文的总结请参见表格
    [2](#S2.T2 "表 2 ‣ 2 种类的 HPE 方法和人体模型 ‣ 单目人体姿态估计：基于深度学习方法的综述")）。
- en: 2.2 Human Body models
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 人体模型
- en: 'Human body modeling is a key component of HPE. Human body is a flexible and
    complex non-rigid object and has many specific characteristics like kinematic
    structure, body shape, surface texture, the position of body parts or body joints,
    etc. A mature model for human body is not necessary to contain all human body
    attributes but should satisfy the requirements for specific tasks to build and
    describe human body pose. Based on different levels of representations and application
    scenarios, as shown in Fig. [2](#S2.F2 "Fig. 2 ‣ 2.2 Human Body models ‣ 2 Categories
    of HPE Methods and Human Body Models ‣ Monocular Human Pose Estimation: A Survey
    of Deep Learning-based Methods"), there are three types of commonly used human
    body models in HPE: skeleton-based model, contour-based model, and volume-based
    model. For more detailed descriptions of human body models, we refer interested
    readers to two well-summarized papers (Liu et al., [2015](#bib.bib94); Gong et al.,
    [2016](#bib.bib47)).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 人体建模是 HPE 的关键组成部分。人体是一个灵活且复杂的非刚性物体，具有许多特定特征，如运动学结构、体型、表面纹理、身体部位或关节的位置等。一个成熟的人体模型不一定包含所有人体属性，但应满足特定任务的要求，以构建和描述人体姿态。如图
    [2](#S2.F2 "图 2 ‣ 2.2 人体模型 ‣ 2 种类的 HPE 方法和人体模型 ‣ 单目人体姿态估计：基于深度学习方法的综述") 所示，根据不同的表示水平和应用场景，在
    HPE 中常用的人体模型有三种类型：基于骨架的模型、基于轮廓的模型和基于体积的模型。有关人体模型的更详细描述，请参阅两篇总结良好的论文（Liu et al.,
    [2015](#bib.bib94); Gong et al., [2016](#bib.bib47)）。
- en: '![Refer to caption](img/2d46b4bce7ae2b75a364587dd97a50e8.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2d46b4bce7ae2b75a364587dd97a50e8.png)'
- en: 'Fig. 2: Commonly used human body models. (a) skeleton-based model; (b) contour-based
    models; (c) volume-based models.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：常用的人体模型。（a）基于骨架的模型；（b）基于轮廓的模型；（c）基于体积的模型。
- en: 'Skeleton-based Model: Skeleton-based model, also known as stick-figure or kinematic
    model, represents a set of joint (typically between 10 to 30) locations and the
    corresponding limb orientations following the human body skeletal structure. The
    skeleton-based model can also be described as a graph where vertices indicating
    joints and edges encoding constraints or prior connections of joints within the
    skeleton structure (Felzenszwalb and Huttenlocher, [2005](#bib.bib39)). This human
    body topology is very simple and flexible which is widely utilized in both 2D
    and 3D HPE (Cao et al., [2016](#bib.bib13); Mehta et al., [2017c](#bib.bib107))
    and human pose datasets (Andriluka et al., [2014](#bib.bib4); Wu et al., [2017](#bib.bib175)).
    With obvious advantages of simple and flexible representing, it also has many
    shortcomings such as lacking texture information which indicates there is no width
    and contour information of human body.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于骨架的模型：基于骨架的模型，也称为 stick-figure 或运动学模型，表示一组关节（通常在 10 到 30 个之间）位置及其对应的肢体方向，遵循人体骨骼结构。骨架模型也可以描述为一个图，其中顶点表示关节，边表示骨架结构内关节的约束或先验连接（Felzenszwalb
    和 Huttenlocher, [2005](#bib.bib39)）。这种人体拓扑结构非常简单且灵活，广泛用于 2D 和 3D HPE（Cao et al.,
    [2016](#bib.bib13)；Mehta et al., [2017c](#bib.bib107)）以及人体姿势数据集（Andriluka et al.,
    [2014](#bib.bib4)；Wu et al., [2017](#bib.bib175)）。虽然具有简单和灵活表示的明显优势，但也存在许多缺点，如缺乏纹理信息，表示了人体的宽度和轮廓信息的缺失。
- en: 'Contour-based Model: The contour-based model is widely used in earlier HPE
    methods which contains the rough width and contour information of body limbs and
    torso. Human body parts are approximately represented with rectangles or boundaries
    of person silhouette. Widely used contour-based models include cardboard models
    (Ju et al., [1996](#bib.bib72)) and Active Shape Models (ASMs) (Cootes et al.,
    [1995](#bib.bib26)).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于轮廓的模型：轮廓模型在早期 HPE 方法中被广泛使用，包含了身体肢体和躯干的粗略宽度和轮廓信息。人体部位大致用矩形或人影的边界表示。广泛使用的轮廓模型包括纸板模型（Ju
    et al., [1996](#bib.bib72)）和活动形状模型（ASMs）（Cootes et al., [1995](#bib.bib26)）。
- en: 'Volume-based Model: 3D human body shapes and poses are generally represented
    by volume-based models with geometric shapes or meshes. Earlier geometric shapes
    for modeling body parts include cylinders, conics, etc. (Sidenbladh et al., [2000](#bib.bib148)).
    Modern volume-based models are represented in mesh form, normally captured with
    3D scans. Widely used volume-based models includes Shape Completion and Animation
    of People (SCAPE) (Anguelov et al., [2005](#bib.bib5)), Skinned Multi-Person Linear
    model (SMPL) (Loper et al., [2015](#bib.bib96)), and a unified deformation model
    (Joo et al., [2018](#bib.bib71)).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于体积的模型：3D 人体形状和姿势通常由具有几何形状或网格的体积模型表示。早期用于建模身体部位的几何形状包括圆柱体、圆锥体等（Sidenbladh et
    al., [2000](#bib.bib148)）。现代体积模型以网格形式表示，通常通过 3D 扫描捕获。广泛使用的体积模型包括 Shape Completion
    and Animation of People (SCAPE)（Anguelov et al., [2005](#bib.bib5)）、Skinned Multi-Person
    Linear 模型 (SMPL)（Loper et al., [2015](#bib.bib96)）和统一变形模型（Joo et al., [2018](#bib.bib71)）。
- en: 3 2D Human Pose Estimation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2D 人体姿势估计
- en: 2D human pose estimation calculates the locations of human joints from monocular
    images or videos. Before deep learning brings a huge impact on vision-based human
    pose estimation, traditional 2D HPE algorithms adopt hand-craft feature extraction
    and sophisticated body models to obtain local representations and global pose
    structures (Dantone et al., [2013](#bib.bib27); Chen and Yuille, [2014](#bib.bib20);
    Gkioxari et al., [2014b](#bib.bib45)). Here, the recent deep learning-based 2D
    human pose estimation methods are categorized into ”single person pose estimation”
    and ”multi-person pose estimation.”
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 人体姿势估计通过单目图像或视频计算人体关节的位置。在深度学习对基于视觉的人体姿势估计产生巨大影响之前，传统的 2D HPE 算法采用手工特征提取和复杂的身体模型来获得局部表示和全局姿势结构（Dantone
    et al., [2013](#bib.bib27)；Chen 和 Yuille, [2014](#bib.bib20)；Gkioxari et al.,
    [2014b](#bib.bib45)）。在这里，最近的基于深度学习的 2D 人体姿势估计方法被分为“单人姿势估计”和“多人姿势估计”。
- en: 3.1 2D single person pose estimation
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2D 单人姿势估计
- en: 2D single person pose estimation is to localize body joint positions of a single
    person in an input image. For images with more persons, pre-processing is needed
    to crop the original image so that there is only one person in the input image
    such as using an upper-body detector (Eichner and Ferrari, [2012a](#bib.bib30))
    or full-body detector (Ren et al., [2015](#bib.bib138)), and cropping from original
    images based on the annotated person center and body scale (Andriluka et al.,
    [2014](#bib.bib4); Newell et al., [2016](#bib.bib115)). Early work of introducing
    deep learning into human pose estimation mainly extended traditional HPE methods
    by simply replaced some components of frameworks by neural networks (Jain et al.,
    [2013](#bib.bib63); Ouyang et al., [2014](#bib.bib121)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 2D单人姿态估计是定位输入图像中单个人的身体关节位置。对于有多个人的图像，需要预处理以裁剪原始图像，使得输入图像中只有一个人，如使用上半身检测器（Eichner和Ferrari，[2012a](#bib.bib30)）或全身检测器（Ren等，[2015](#bib.bib138)），以及基于标注的人员中心和身体尺度从原始图像中裁剪（Andriluka等，[2014](#bib.bib4)；Newell等，[2016](#bib.bib115)）。将深度学习引入人体姿态估计的早期工作主要是通过简单地用神经网络替换一些框架组件来扩展传统的HPE方法（Jain等，[2013](#bib.bib63)；Ouyang等，[2014](#bib.bib121)）。
- en: 'Based on the different formulations of human pose estimation task, the proposed
    methods using CNNs can be classified into two categories: regression-based methods
    and detection-based methods. Regression-based methods attempt to learn a mapping
    from image to kinematic body joint coordinates by an end-to-end framework and
    generally directly produce joint coordinates (Toshev and Szegedy, [2014](#bib.bib165)).
    Detection-based methods are intended to predict approximate locations of body
    parts (Chen and Yuille, [2014](#bib.bib20)) or joints (Newell et al., [2016](#bib.bib115)),
    usually are supervised by a sequence of rectangular windows (each including a
    specific body part) (Jain et al., [2013](#bib.bib63); Chen and Yuille, [2014](#bib.bib20))
    or heatmaps (each indicating one joint position by a 2D Gaussian distribution
    centered at the joint location) (Newell et al., [2016](#bib.bib115); Wei et al.,
    [2016](#bib.bib174)). Each of these two kinds of methods has its advantages and
    disadvantages. Direct regression learning of only one single point is a difficulty
    since it is a highly nonlinear problem and lacks robustness, while heatmap learning
    is supervised by dense pixel information which results in better robustness. Compared
    to the original image size, heatmap representation has much lower resolution due
    to the pooling operation in CNNs, which limits the accuracy of joint coordinate
    estimation. And obtaining joint coordinates from heatmap is normally a non-differentiable
    process that blocks the network to be trained end-to-end. The recent representative
    work for 2D single person pose estimation are summarized in Table [3](#S3.T3 "Table
    3 ‣ 3.1 2D single person pose estimation ‣ 3 2D Human Pose Estimation ‣ Monocular
    Human Pose Estimation: A Survey of Deep Learning-based Methods"), the last column
    is the comparisons of PCKh@0.5 scores on the MPII testing set. More details of
    datasets and evaluation metrics are described in Section [5](#S5 "5 Datasets and
    evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '根据人体姿态估计任务的不同公式，使用CNN的方法可以分为两类：基于回归的方法和基于检测的方法。基于回归的方法试图通过端到端框架学习从图像到运动体关节坐标的映射，并通常直接产生关节坐标（Toshev和Szegedy，[2014](#bib.bib165)）。基于检测的方法旨在预测身体部位（Chen和Yuille，[2014](#bib.bib20)）或关节（Newell等，[2016](#bib.bib115)）的大致位置，通常通过一系列矩形窗口（每个窗口包含一个特定的身体部位）（Jain等，[2013](#bib.bib63)；Chen和Yuille，[2014](#bib.bib20)）或热图（每个热图通过一个以关节位置为中心的二维高斯分布指示一个关节位置）（Newell等，[2016](#bib.bib115)；Wei等，[2016](#bib.bib174)）来监督。每种方法都有其优缺点。仅学习一个单点的直接回归具有难度，因为这是一个高度非线性的问题，缺乏鲁棒性，而热图学习由密集像素信息进行监督，结果更具鲁棒性。与原始图像大小相比，由于CNN中的池化操作，热图表示具有较低的分辨率，这限制了关节坐标估计的准确性。而从热图中获得关节坐标通常是一个不可微的过程，这阻碍了网络的端到端训练。2D单人姿态估计的近期代表性工作总结在表[3](#S3.T3
    "Table 3 ‣ 3.1 2D single person pose estimation ‣ 3 2D Human Pose Estimation ‣
    Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods")中，最后一列是MPII测试集上的PCKh@0.5分数比较。数据集和评估指标的更多细节在第[5](#S5
    "5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey
    of Deep Learning-based Methods")节中描述。'
- en: 'Table 3: Summary of 2D single person pose estimation methods. Note that the
    last column shows the PCKh@0.5 scores on the Max Planck Institute for Informatics
    (MPII) Human Pose testing set.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：2D 单人姿态估计方法总结。请注意，最后一列显示的是 Max Planck 信息学研究所（MPII）人体姿态测试集上的 PCKh@0.5 分数。
- en: '| Methods | Backbone | Input size | Highlights | PCKh (%) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | 输入尺寸 | 亮点 | PCKh (%) |'
- en: '| Regression-based |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 回归基础 |'
- en: '| (Toshev and Szegedy, [2014](#bib.bib165)) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (Toshev and Szegedy, [2014](#bib.bib165)) |'
- en: '&#124; AlexNet &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AlexNet &#124;'
- en: '| 220$\times$220 | Direct regression, multi-stage refinement | - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 220$\times$220 | 直接回归，多阶段精炼 | - |'
- en: '| (Carreira et al., [2016](#bib.bib14)) | GoogleNet | 224$\times$224 | Iterative
    error feedback refinement from initial pose. | 81.3 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| (Carreira et al., [2016](#bib.bib14)) | GoogleNet | 224$\times$224 | 从初始姿势进行迭代错误反馈精炼
    | 81.3 |'
- en: '| (Sun et al., [2017](#bib.bib152)) | ResNet-50 | 224$\times$224 | Bone based
    representation as additional constraint, general for both 2D/3D HPE | 86.4 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (Sun et al., [2017](#bib.bib152)) | ResNet-50 | 224$\times$224 | 基于骨骼的表示作为附加约束，适用于
    2D/3D HPE | 86.4 |'
- en: '| (Luvizon et al., [2017](#bib.bib99)) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| (Luvizon et al., [2017](#bib.bib99)) |'
- en: '&#124; Inception-v4+ &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Inception-v4+ &#124;'
- en: '&#124; Hourglass &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Hourglass &#124;'
- en: '| 256$\times$256 | Multi-stage architecture, proposed soft-argmax function
    to convert heatmaps into joint locations | 91.2 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 256$\times$256 | 多阶段架构，提出的 soft-argmax 函数将热图转换为关节位置 | 91.2 |'
- en: '| Detection-based |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 检测基础 |'
- en: '| (Tompson et al., [2014](#bib.bib164)) | AlexNet | 320$\times$240 | Heatmap
    representation, multi-scale input, MRF-like Spatial-Model | 79.6 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (Tompson et al., [2014](#bib.bib164)) | AlexNet | 320$\times$240 | 热图表示，多尺度输入，MRF
    类空间模型 | 79.6 |'
- en: '| (Yang et al., [2016](#bib.bib178)) | VGG | 112$\times$112 | Jointly learning
    DCNNs with deformable mixture of parts models | - |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| (Yang et al., [2016](#bib.bib178)) | VGG | 112$\times$112 | 通过可变形部分模型联合学习
    DCNN | - |'
- en: '| (Newell et al., [2016](#bib.bib115)) | Hourglass | 256$\times$256 | Proposed
    stacked Hourglass architecture with intermediate supervision. | 90.9 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| (Newell et al., [2016](#bib.bib115)) | Hourglass | 256$\times$256 | 提出的堆叠
    Hourglass 架构，具有中间监督 | 90.9 |'
- en: '| (Wei et al., [2016](#bib.bib174)) | CPM | 368$\times$368 | Proposed Convolutional
    Pose Machines (CPM) with intermediate input and supervision, learn spatial correlations
    among body parts | 88.5 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| (Wei et al., [2016](#bib.bib174)) | CPM | 368$\times$368 | 提出的卷积姿态机器（CPM）具有中间输入和监督，学习身体部位之间的空间关系
    | 88.5 |'
- en: '| (Chu et al., [2017](#bib.bib25)) | Hourglass | 256$\times$256 | Multi-resolution
    attention maps from multi-scale features, proposed micro hourglass residual units
    to increase the receptive field | 91.5 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| (Chu et al., [2017](#bib.bib25)) | Hourglass | 256$\times$256 | 从多尺度特征中生成多分辨率注意力图，提出微型
    Hourglass 残差单元以增加感受野 | 91.5 |'
- en: '| (Yang et al., [2017](#bib.bib177)) | Hourglass | 256$\times$256 | Proposed
    Pyramid Residual Module (PRM) learns filters for input features with different
    resolutions | 92.0 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| (Yang et al., [2017](#bib.bib177)) | Hourglass | 256$\times$256 | 提出的金字塔残差模块（PRM）为不同分辨率的输入特征学习滤波器
    | 92.0 |'
- en: '| (Chen et al., [2017](#bib.bib21)) | conv-deconv | 256$\times$256 | GAN, stacked
    conv-deconv architecture, multi-task for pose and occlusion, two discriminators
    for distinguishing whether the pose is ’real’ and the confidence is strong | 91.9
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| (Chen et al., [2017](#bib.bib21)) | conv-deconv | 256$\times$256 | GAN，堆叠的
    conv-deconv 架构，多任务用于姿态和遮挡，两个判别器用于区分姿态是否“真实”以及置信度是否强 | 91.9 |'
- en: '| (Peng et al., [2018](#bib.bib127)) | Hourglass | 256$\times$256 | GAN, proposed
    augmentation network to generate data augmentations without looking for more data
    | 91.5 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| (Peng et al., [2018](#bib.bib127)) | Hourglass | 256$\times$256 | GAN，提出的增强网络生成数据增强，无需寻找更多数据
    | 91.5 |'
- en: '| (Ke et al., [2018](#bib.bib74)) | Hourglass | 256$\times$256 | Improved Hourglass
    network with multi-scale intermediate supervision, multi-scale feature combination,
    structure-aware loss and data augmentation of joints masking | 92.1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| (Ke et al., [2018](#bib.bib74)) | Hourglass | 256$\times$256 | 改进的 Hourglass
    网络，具有多尺度中间监督、多尺度特征组合、结构感知损失和关节遮罩的数据增强 | 92.1 |'
- en: '| (Tang et al., [2018a](#bib.bib157)) | Hourglass | 256$\times$256 | Compositional
    model, hierarchical representation of body parts for intermediate supervision
    | 92.3 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (Tang et al., [2018a](#bib.bib157)) | Hourglass | 256$\times$256 | 组合模型，对身体部位进行层次表示以进行中间监督
    | 92.3 |'
- en: '| (Sun et al., [2019](#bib.bib151)) | HRNet | 256$\times$256 | high-resolution
    representations of features across the whole network, multi-scale fusion. | 92.3
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| (Sun et al., [2019](#bib.bib151)) | HRNet | 256$\times$256 | 全网络中的高分辨率特征表示，多尺度融合
    | 92.3 |'
- en: '| (Tang and Wu, [2019](#bib.bib156)) | Hourglass | 256$\times$256 | data-driven
    joint grouping, proposed part-based branching network (PBN) to learn representations
    specific to each part group. | 92.7 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| (Tang and Wu, [2019](#bib.bib156)) | Hourglass | 256$\times$256 | 数据驱动的关节分组，提出了基于部分的分支网络（PBN）来学习特定于每个部分组的表示。
    | 92.7 |'
- en: 3.1.1 Regression-based methods
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 回归方法
- en: 'AlexNet (Krizhevsky et al., [2012](#bib.bib78)) was one of the early networks
    for deep learning-based HPE methods due to its simple architecture and impressive
    performance. Toshev and Szegedy ([2014](#bib.bib165)) firstly attempted to train
    an AlexNet-like deep neural network to learn joint coordinates from full images
    in a very straightforward manner without using any body model or part detectors
    as shown in Fig. [3](#S3.F3 "Fig. 3 ‣ 3.1.1 Regression-based methods ‣ 3.1 2D
    single person pose estimation ‣ 3 2D Human Pose Estimation ‣ Monocular Human Pose
    Estimation: A Survey of Deep Learning-based Methods"). Moreover, a cascade architecture
    of multi-stage refining regressors is employed to refine the cropped images from
    the previous stage and show improved performance. Pfister et al. ([2014](#bib.bib130))
    also applied an AlexNet-like network using a sequence of concatenated frames as
    input to predict the human pose in the videos.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet (Krizhevsky et al., [2012](#bib.bib78)) 是早期的深度学习基础 HPE 方法之一，因其简单的架构和令人印象深刻的性能而广受欢迎。Toshev
    和 Szegedy ([2014](#bib.bib165)) 首次尝试训练类似 AlexNet 的深度神经网络，以非常直接的方式从完整图像中学习关节坐标，而不使用任何身体模型或部分检测器，如图
    [3](#S3.F3 "图 3 ‣ 3.1.1 回归方法 ‣ 3.1 2D 单人姿势估计 ‣ 3 2D 人体姿势估计 ‣ 单目人体姿势估计：基于深度学习的方法综述")
    所示。此外，采用了多阶段精炼回归器的级联架构，以精炼前一阶段的裁剪图像并显示出改进的性能。Pfister 等人 ([2014](#bib.bib130))
    还应用了类似 AlexNet 的网络，使用一系列串联的帧作为输入来预测视频中的人体姿势。
- en: '![Refer to caption](img/4e9399328466b61e3ea5a51f8f9745ad.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e9399328466b61e3ea5a51f8f9745ad.png)'
- en: 'Fig. 3: The framework of DeepPose (Toshev and Szegedy, [2014](#bib.bib165)).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: DeepPose 的框架 (Toshev 和 Szegedy, [2014](#bib.bib165))。'
- en: Only using joints without the surrounding information lacks robustness. Converting
    heatmap supervision to numerical joint positions supervision can retain the advantages
    of both representations. Luvizon et al. ([2017](#bib.bib99)) proposed a Soft-argmax
    function to transform heatmaps to joint coordinates which can convert a detection-based
    network to a differentiable regression-based one. Nibali et al. ([2018](#bib.bib116))
    designed a differentiable spatial to numerical transform (DSNT) layer to calculate
    joint coordinates from heatmaps, which worked well with low-resolution heatmaps.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用关节而没有周围信息缺乏鲁棒性。将热图监督转换为数值关节位置监督可以保留两种表示方法的优点。Luvizon 等人 ([2017](#bib.bib99))
    提出了一个 Soft-argmax 函数来将热图转换为关节坐标，这可以将基于检测的网络转换为可微分的回归网络。Nibali 等人 ([2018](#bib.bib116))
    设计了一个可微分的空间到数值转换（DSNT）层，以从热图计算关节坐标，这在低分辨率热图中效果良好。
- en: Prediction of joint coordinates directly from input images with few constrains
    is very hard, therefore more powerful networks were introduced with a refinement
    or body model structure. Carreira et al. ([2016](#bib.bib14)) proposed an Iterative
    Error Feedback network based on GoogleNet which recursively processes the combination
    of the input image and output results. The final pose is improved from an initial
    mean pose after iterations. Sun et al. ([2017](#bib.bib152)) proposed a structure-aware
    regression approach based on a ResNet-50\. Instead of using joints to represent
    pose, a bone-based representation is designed by involving body structure information
    to achieve more stable results than only using joint positions. The bone-based
    representation also works on 3D HPE.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从输入图像预测关节坐标并且限制条件很少是非常困难的，因此引入了具有改进或身体模型结构的更强大的网络。Carreira 等人 ([2016](#bib.bib14))
    提出了一个基于 GoogleNet 的迭代误差反馈网络，该网络递归地处理输入图像和输出结果的组合。最终姿势通过迭代从初始的平均姿势中改进。Sun 等人 ([2017](#bib.bib152))
    提出了一个基于 ResNet-50 的结构感知回归方法。与使用关节表示姿势不同，该方法通过涉及身体结构信息设计了一个基于骨骼的表示，以实现比仅使用关节位置更稳定的结果。基于骨骼的表示也适用于
    3D HPE。
- en: Networks handling multiple closely related tasks of human body may learn diverse
    features to improve the prediction of joint coordinates. Li et al. ([2014](#bib.bib88))
    employed an AlexNet-like multi-task framework to handle the joint coordinate prediction
    task from full images in a regression way, and the body part detection task from
    image patches obtained by a sliding-window. Gkioxari et al. ([2014a](#bib.bib44))
    used a R-CNN architecture to synchronously detect person, estimate pose, and classify
    action. Fan et al. ([2015](#bib.bib37)) proposed a dual-source deep CNNs which
    take image patches and full images as inputs and output heatmap represented joint
    detection results of sliding windows together with coordinate represented joint
    localization results. The final estimated posture is obtained from the combination
    of the two results. Luvizon et al. ([2018](#bib.bib98)) designed a network that
    can jointly handle 2D/3D pose estimation and action recognition from video sequences.
    The pose estimated in the middle of the network can be used as a reference for
    action recognition.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 处理人类身体多个紧密相关任务的网络可能会学习多样化的特征，以提高关节坐标的预测精度。Li 等人 ([2014](#bib.bib88)) 使用了一种类似于
    AlexNet 的多任务框架，以回归方式处理从完整图像中预测关节坐标的任务，以及从滑动窗口获得的图像补丁中检测身体部位的任务。Gkioxari 等人 ([2014a](#bib.bib44))
    使用了 R-CNN 架构来同步检测人、估计姿势和分类动作。Fan 等人 ([2015](#bib.bib37)) 提出了双源深度 CNNs，输入图像补丁和完整图像，输出热图表示的关节检测结果以及坐标表示的关节定位结果。最终估计的姿势是通过结合这两个结果得到的。Luvizon
    等人 ([2018](#bib.bib98)) 设计了一个可以从视频序列中联合处理 2D/3D 姿势估计和动作识别的网络。网络中间估计的姿势可以用作动作识别的参考。
- en: 3.1.2 Detection-based methods
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 基于检测的方法
- en: Detection-based methods are developed from body part detection methods. In traditional
    part-based HPE methods, body parts are first detected from image patch candidates
    and then are assembled to fit a human body model. The detected body parts in early
    work are relatively big and generally represented by rectangular sliding windows
    or patches. We refer to (Poppe, [2007](#bib.bib134); Gong et al., [2016](#bib.bib47))
    for a more detailed introduction. Some early methods use neural networks as body
    part detectors to distinguish whether a candidate patch is a specific body part (Jain
    et al., [2013](#bib.bib63)), classify a candidate patch among predefined templates (Chen
    and Yuille, [2014](#bib.bib20)) or predict the confidence map belonging to multiple
    classes (Ramakrishna et al., [2014](#bib.bib137)). Body part detection methods
    are usually sensitive to complexity background and body occlusions. Therefore
    the independent image patches with only local appearance may not be sufficiently
    discriminative for body part detection.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检测的方法源自身体部位检测方法。在传统的基于部位的 HPE 方法中，首先从图像补丁候选区域检测身体部位，然后将其组合以适配人体模型。早期工作的检测到的身体部位相对较大，通常由矩形滑动窗口或补丁表示。我们参考了(Poppe,
    [2007](#bib.bib134); Gong 等人, [2016](#bib.bib47)) 获取更详细的介绍。一些早期方法使用神经网络作为身体部位检测器，以区分候选补丁是否为特定的身体部位
    (Jain 等人, [2013](#bib.bib63))，在预定义模板中分类候选补丁 (Chen 和 Yuille, [2014](#bib.bib20))，或预测属于多个类别的置信度图
    (Ramakrishna 等人, [2014](#bib.bib137))。身体部位检测方法通常对复杂背景和身体遮挡较为敏感。因此，只有局部外观的独立图像补丁可能不足以区分身体部位。
- en: 'In order to provide more supervision information than just joint coordinates
    and to facilitate the training of CNNs, more recent work employed heatmap to indicate
    the ground truth of the joint location (Tompson et al., [2014](#bib.bib164); Jain
    et al., [2014](#bib.bib64)). As shown in Fig. [4](#S3.F4 "Fig. 4 ‣ 3.1.2 Detection-based
    methods ‣ 3.1 2D single person pose estimation ‣ 3 2D Human Pose Estimation ‣
    Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods"), each
    joint occupies a heatmap channel with a 2D Gaussian distribution centered at the
    target joint location. Moreover, Papandreou et al. ([2017](#bib.bib123)) proposed
    an improved representation of the joint location, which is a combination of binary
    activation heatmap and corresponding offset. Since heatmap representation is more
    robust than coordinate representation, most of the recent research is based on
    heatmap representation.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提供比仅关节坐标更多的监督信息，并便于CNN的训练，较新的研究采用了热图来表示关节位置的真实情况（Tompson等，[2014](#bib.bib164)；Jain等，[2014](#bib.bib64)）。如图[4](#S3.F4
    "Fig. 4 ‣ 3.1.2 Detection-based methods ‣ 3.1 2D single person pose estimation
    ‣ 3 2D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep
    Learning-based Methods")所示，每个关节占据一个热图通道，该通道中心在目标关节位置的二维高斯分布。此外，Papandreou等人（[2017](#bib.bib123)）提出了一种改进的关节位置表示方法，即二进制激活热图与相应偏移量的组合。由于热图表示比坐标表示更为鲁棒，因此大多数近期研究基于热图表示。'
- en: '![Refer to caption](img/57281c62a790d7d9a3bb9df7a4d64253.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/57281c62a790d7d9a3bb9df7a4d64253.png)'
- en: 'Fig. 4: Heatmap representation of different joints.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不同关节的热图表示。
- en: The neural network architecture is very important to make better use of input
    information. Some approaches are mainly based on classic networks with appropriate
    improvements, such as GoogLeNet-based network with multi-scale inputs (Rafi et al.,
    [2016](#bib.bib136)), ResNet-based network with deconvolutional layers Xiao et al.
    ([2018](#bib.bib176)). In terms of iterative refinement, some work designed networks
    in a multi-stage style to refine results from coarse prediction via end-to-end
    learning (Tompson et al., [2015](#bib.bib163); Bulat and Tzimiropoulos, [2016](#bib.bib12);
    Newell et al., [2016](#bib.bib115); Wei et al., [2016](#bib.bib174); Yang et al.,
    [2017](#bib.bib177); Belagiannis and Zisserman, [2017](#bib.bib7)). Such networks
    generally use intermediate supervision to address vanishing gradients. Newell
    et al. ([2016](#bib.bib115)) proposed a novel stacked hourglass architecture by
    using a residual module as the component unit. Wei et al. ([2016](#bib.bib174))
    proposed a multi-stage prediction framework with input image for each stage. Yang
    et al. ([2017](#bib.bib177)) designed a Pyramid Residual Module (PRMs) to replace
    the residual module of the Hourglass network to enhance the invariance across
    scales of DCNNs by learning features on various scales. Belagiannis and Zisserman
    ([2017](#bib.bib7)) combined a 7 layers feedforward module with a recurrent module
    to iteratively refine the results. This model learns to predict location heatmaps
    for both joints and body limbs. Also, they analyzed keypoint visibility with unbalanced
    ground truth distribution. To keep high-resolution representations of features
    across the whole network, Sun et al. ([2019](#bib.bib151)) proposed a novel High-Resolution
    Net (HRNet) with multi-scale feature fusion.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构对于更好地利用输入信息非常重要。一些方法主要基于经典网络并进行了适当的改进，例如具有多尺度输入的GoogLeNet网络（Rafi等，[2016](#bib.bib136)），具有反卷积层的ResNet网络（Xiao等，[2018](#bib.bib176)）。在迭代精炼方面，一些工作设计了多阶段风格的网络，通过端到端学习从粗略预测中细化结果（Tompson等，[2015](#bib.bib163)；Bulat和Tzimiropoulos，[2016](#bib.bib12)；Newell等，[2016](#bib.bib115)；Wei等，[2016](#bib.bib174)；Yang等，[2017](#bib.bib177)；Belagiannis和Zisserman，[2017](#bib.bib7)）。这些网络通常使用中间监督来解决梯度消失问题。Newell等（[2016](#bib.bib115)）提出了一种新颖的堆叠沙漏结构，使用残差模块作为组件单元。Wei等（[2016](#bib.bib174)）提出了一种多阶段预测框架，每个阶段都有输入图像。Yang等（[2017](#bib.bib177)）设计了金字塔残差模块（PRMs），用来替代沙漏网络的残差模块，通过在不同尺度上学习特征来增强DCNN的尺度不变性。Belagiannis和Zisserman（[2017](#bib.bib7)）将7层前馈模块与递归模块相结合，迭代地细化结果。该模型学习预测关节和身体肢体的位置热图。此外，他们分析了关键点的可见性与不平衡的真实分布。为了在整个网络中保持高分辨率特征表示，Sun等（[2019](#bib.bib151)）提出了一种新型的高分辨率网络（HRNet），实现了多尺度特征融合。
- en: Different from earlier work which attempted to fit detected body parts into
    body models, some recent work tried to encode human body structure information
    into networks. Tompson et al. ([2014](#bib.bib164)) jointly trained a network
    with a MRF-like spatial-model for learning typical spatial relations between joints.
    Lifshitz et al. ([2016](#bib.bib91)) discretized an image into log-polar bins
    centered around each joint and employed a VGG-based network to predict joint category
    confident for each pair-wise joints (binary terms). With all relative confident
    scores, the final heatmap for each joint can be generated by a deconvolutional
    network. Yang et al. ([2016](#bib.bib178)) designed a two-stage network. Stage
    one is a convolutional neural network to predict joint locations in heatmap representation.
    Stage two is a message-passing model connected manually according to the human
    body structure to find optimal joint locations with a max-sum algorithm. Gkioxari
    et al. ([2016](#bib.bib46)) proposed a convolutional Recurrent Neural Network
    to output joint location one by one following a chain model. The output of each
    step depends on both the input image and the previously predicted output. The
    network can handle both images and videos with different connection strategy.
    Chu et al. ([2016](#bib.bib24)) proposed to transform kernels by a bi-directional
    tree to pass information between corresponding joints in a tree body model. Chu
    et al. ([2017](#bib.bib25)) replaced the residual modules of the Hourglass network
    with more sophisticated ones. The Conditional Random Field (CRF) is utilized for
    attention maps as intermediate supervisions for learning body structure information.
    (Ning et al., [2018](#bib.bib119)) designed a fractal network to impose body prior
    knowledge to guide the network. The external knowledge visual features are encoded
    into the basic network by using a learned projection matrix. Ke et al. ([2018](#bib.bib74))
    proposed a multi-scale structure-aware network based on Hourglass network with
    multi-scale supervision, multi-scale feature combination, structure-aware loss,
    and data augmentation of joints masking. On the basic framework of Hourglass network,
    Tang et al. ([2018a](#bib.bib157)) designed a hierarchical representation of body
    parts for intermediate supervision to replace heatmap for each joint. Thus the
    network learns the bottom-up/top-down body structure, rather than only scattered
    joints. Tang and Wu ([2019](#bib.bib156)) proposed a part-based branching network
    (PBN) to learn specific representations of each part group rather than predict
    all joint heatmaps from one branch. The data-driven part groups are then split
    by calculating mutual information of joints.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于早期尝试将检测到的身体部位适配到身体模型中的工作，最近的一些研究尝试将人体结构信息编码到网络中。Tompson 等人 ([2014](#bib.bib164))
    共同训练了一个带有类似 MRF 的空间模型的网络，以学习关节之间的典型空间关系。Lifshitz 等人 ([2016](#bib.bib91)) 将图像离散化为围绕每个关节中心的对数极坐标箱，并采用基于
    VGG 的网络来预测每对关节的类别置信度（即二元项）。利用所有相对置信度分数，可以通过反卷积网络生成每个关节的最终热图。Yang 等人 ([2016](#bib.bib178))
    设计了一个两阶段网络。第一阶段是一个卷积神经网络，用于预测热图表示中的关节位置。第二阶段是一个根据人体结构手动连接的消息传递模型，用于通过最大求和算法找到最佳的关节位置。Gkioxari
    等人 ([2016](#bib.bib46)) 提出了一个卷积递归神经网络，按照链模型一个接一个地输出关节位置。每一步的输出依赖于输入图像和先前预测的输出。该网络可以处理图像和视频，并具有不同的连接策略。Chu
    等人 ([2016](#bib.bib24)) 提出了通过双向树转换卷积核，以在树状身体模型中的对应关节之间传递信息。Chu 等人 ([2017](#bib.bib25))
    用更复杂的残差模块替换了 Hourglass 网络的残差模块。条件随机场 (CRF) 被用于注意力图作为中间监督，以学习身体结构信息。（Ning 等人， [2018](#bib.bib119)）设计了一个分形网络，以施加身体先验知识来指导网络。外部知识的视觉特征通过使用学习到的投影矩阵被编码到基本网络中。Ke
    等人 ([2018](#bib.bib74)) 提出了一个基于 Hourglass 网络的多尺度结构感知网络，具有多尺度监督、多尺度特征组合、结构感知损失和关节遮挡的数据增强。在
    Hourglass 网络的基本框架上，Tang 等人 ([2018a](#bib.bib157)) 设计了一个身体部位的分层表示，用于中间监督，替代每个关节的热图。因此，网络学习的是自底向上/自顶向下的身体结构，而不仅仅是分散的关节。Tang
    和 Wu ([2019](#bib.bib156)) 提出了一个基于部件的分支网络 (PBN)，以学习每个部件组的特定表示，而不是从一个分支预测所有关节热图。然后通过计算关节的互信息来划分数据驱动的部件组。
- en: Generative Adversarial Networks (GANs) are also employed to provide adversarial
    supervision for learning body structure or network training. Chou et al. ([2017](#bib.bib23))
    introduced adversarial learning with two same Hourglass networks as generator
    and discriminator respectively. The generator predicts heatmap location of each
    joint, while the discriminator distinguishes ground truth heatmaps from generated
    heatmaps. Chen et al. ([2017](#bib.bib21)) proposed a structure-aware convolutional
    network with one generator and two discriminators to incorporate priors of human
    body structure. The generator is designed from the Hourglass network to predict
    joint heatmaps as well as occlusion heatmaps. The pose discriminator can discriminate
    against reasonable body configuration from unreasonable body configuration. The
    confidence discriminator shows the confidence score of predictions. Peng et al.
    ([2018](#bib.bib127)) studied how to jointly optimize data augmentation and network
    training without looking for more data. Instead of using random data augmentation,
    they applied augmentations to increase the network loss while the pose network
    learns from the generated augmentations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）也被用来为学习人体结构或网络训练提供对抗性监督。Chou等人（[2017](#bib.bib23)）引入了生成对抗学习，其中两个相同的Hourglass网络分别作为生成器和判别器。生成器预测每个关节的热图位置，而判别器则区分真实的热图和生成的热图。Chen等人（[2017](#bib.bib21)）提出了一种结构感知卷积网络，具有一个生成器和两个判别器，以结合人体结构的先验信息。生成器由Hourglass网络设计，用于预测关节热图以及遮挡热图。姿态判别器可以区分合理的身体配置和不合理的身体配置。置信度判别器显示预测的置信度分数。Peng等人（[2018](#bib.bib127)）研究了如何在不寻求更多数据的情况下共同优化数据增强和网络训练。他们不是使用随机数据增强，而是应用增强来增加网络损失，同时姿态网络从生成的增强数据中学习。
- en: Utilization of temporal information is also very important to estimate 2D human
    poses in monocular video sequences. Jain et al. ([2014](#bib.bib64)) designed
    a framework contains two-branch CNNs taking multi-scale RGB frames and optical-flow
    maps as inputs. The extracted features are concatenated before the last convolutional
    layers. Pfister et al. ([2015](#bib.bib129)) used optical-flow maps as a guide
    to align predicted heatmaps from neighboring frames based on the temporal context
    of videos. Luo et al. ([2018](#bib.bib97)) exploited temporal information with
    a Recurrent Neural Network redesigned from CPM by changing multi-stage architecture
    with LSTM structure.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 时间信息的利用对于在单目视频序列中估计2D人体姿态也非常重要。Jain等人（[2014](#bib.bib64)）设计了一个框架，该框架包含两个分支的CNN，输入为多尺度RGB帧和光流图。提取的特征在最后的卷积层之前被拼接在一起。Pfister等人（[2015](#bib.bib129)）使用光流图作为引导，根据视频的时间上下文对来自邻近帧的预测热图进行对齐。Luo等人（[2018](#bib.bib97)）利用时间信息，使用从CPM重新设计的递归神经网络，通过将多阶段架构改为LSTM结构。
- en: In order to estimate human poses on low-capacity devices, network parameters
    can be reduced while still maintaining competitive performance. Tang et al. ([2018b](#bib.bib158))
    committed to improving the network structure by proposing a densely connected
    U-Nets and efficient usage of memory. This network is similar to the idea of the
    Hourglass network while utilizing U-Net as each component with a more optimized
    global connection across each stage resulting in fewer parameters and small model
    size. Debnath et al. ([2018](#bib.bib28)) adapted MobileNets (Howard et al., [2017](#bib.bib53))
    for pose estimation by designing a split stream architecture at the final two
    layers of the MobileNets. Feng et al. ([2019](#bib.bib40)) designed a lightweight
    variant of Hourglass network and trained it with a full teacher Hourglass network
    by a Fast Pose Distillation (FPD) training strategy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在低容量设备上估计人体姿态，可以减少网络参数，同时仍保持竞争性能。Tang等人（[2018b](#bib.bib158)）致力于通过提出密集连接的U-Net和高效的内存使用来改进网络结构。该网络类似于Hourglass网络的概念，同时利用U-Net作为每个组件，通过跨每个阶段的更优化全局连接，结果是参数更少，模型大小更小。Debnath等人（[2018](#bib.bib28)）通过在MobileNets的最后两层设计分流架构，将MobileNets（Howard等人，[2017](#bib.bib53)）适配用于姿态估计。Feng等人（[2019](#bib.bib40)）设计了一种Hourglass网络的轻量级变体，并通过Fast
    Pose Distillation（FPD）训练策略用完整的教师Hourglass网络对其进行训练。
- en: In summary, the heatmap representation is more suitable for network training
    than coordinate representation from detection-based methods in deep learning-based
    2D single person pose estimation.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，热图表示比基于检测的方法中的坐标表示更适合于深度学习中2D单人姿态估计的网络训练。
- en: 3.2 2D multi-person pose estimation
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 2D多人体姿态估计
- en: Different from single person pose estimation, multi-person pose estimation needs
    to handle both detection and localization tasks since there is no prompt of how
    many persons in the input images. According to from which level (high-level abstraction
    or low-level pixel evidence) to start the calculation, human pose estimation methods
    can be classified into top-down methods and bottom-up methods.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与单人姿态估计不同，多人姿态估计需要同时处理检测和定位任务，因为输入图像中没有提示有多少人。根据从哪个层次（高层次抽象或低层次像素证据）开始计算，人体姿态估计方法可以分为自上而下的方法和自下而上的方法。
- en: 'Top-down methods generally employ person detectors to obtain a set of the bounding
    box of people in the input image and then directly leverage existing single-person
    pose estimators to predict human poses. The predicted poses heavily depend on
    the precision of the person detection. The runtime for the whole system is proportional
    based on the number of persons. While bottom-up methods directly predict all the
    2D joints of all persons and then assemble them into independent skeletons. Correct
    grouping of joint points in a complex environment is a challenging research task.
    Table [4](#S3.T4 "Table 4 ‣ 3.2 2D multi-person pose estimation ‣ 3 2D Human Pose
    Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") summarizes recent deep learning-based work about 2D multi-person pose
    estimation methods in both top-down and bottom-up categories. The last column
    of Table [4](#S3.T4 "Table 4 ‣ 3.2 2D multi-person pose estimation ‣ 3 2D Human
    Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") is the Average Precision (AP) scores on the COCO test-dev dataset. More
    details of datasets and evaluation metrics are described in Section [5](#S5 "5
    Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey
    of Deep Learning-based Methods").'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '自上而下的方法通常采用人物检测器来获取输入图像中人的边界框集合，然后直接利用现有的单人姿态估计器来预测人体姿态。预测的姿态在很大程度上依赖于人物检测的精度。整个系统的运行时间与人数成正比。而自下而上的方法则直接预测所有人的
    2D 关节，然后将它们组装成独立的骨架。在复杂环境中正确分组关节点是一个具有挑战性的研究任务。表 [4](#S3.T4 "Table 4 ‣ 3.2 2D
    multi-person pose estimation ‣ 3 2D Human Pose Estimation ‣ Monocular Human Pose
    Estimation: A Survey of Deep Learning-based Methods") 总结了最近基于深度学习的 2D 多人姿态估计方法，涵盖了自上而下和自下而上的类别。表
    [4](#S3.T4 "Table 4 ‣ 3.2 2D multi-person pose estimation ‣ 3 2D Human Pose Estimation
    ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods") 的最后一列是
    COCO 测试开发数据集上的平均精度（AP）分数。数据集和评估指标的更多细节描述见第 [5](#S5 "5 Datasets and evaluation
    protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods")
    节。'
- en: 'Table 4: Comparison of 2D multi-person pose estimation methods. Note that the
    last column shows the Average Precision (AP) scores on the COCO test-dev set.
    The results with * were obtained with COCO16 training set, while others with COCO17
    training set.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：2D 多人姿态估计方法的比较。请注意，最后一列显示了 COCO 测试开发集上的平均精度（AP）分数。带有 * 的结果是使用 COCO16 训练集获得的，而其他结果是使用
    COCO17 训练集获得的。
- en: '| Methods | Network type | Highlights | AP Score (%) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 网络类型 | 亮点 | AP 分数 (%) |'
- en: '| Top-down |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下 |'
- en: '| (Iqbal and Gall, [2016](#bib.bib60)) | Faster R-CNN + CPM | After person
    detection and single HPE, refines detected local joint candidates with Integer
    Linear Programming (ILP). | - |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| (Iqbal and Gall, [2016](#bib.bib60)) | Faster R-CNN + CPM | 在人物检测和单人 HPE
    之后，使用整数线性规划（ILP）细化检测到的局部关节候选。 | - |'
- en: '| (Fang et al., [2017](#bib.bib38)) | Faster R-CNN + Hourglass | Combines symmetric
    spatial transformer network (SSTN) and Hourglass model to do SPPE on detected
    results; proposes a parametric pose NMS for refining pose proposals; designs a
    pose-guided proposals generator to augment the existing training samples | $63.3^{*}$
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| (Fang et al., [2017](#bib.bib38)) | Faster R-CNN + Hourglass | 将对称空间变换网络（SSTN）和
    Hourglass 模型结合以对检测结果进行 SPPE；提出了一个参数化的姿态 NMS 来细化姿态提案；设计了一个姿态引导提案生成器来扩充现有的训练样本 |
    $63.3^{*}$ |'
- en: '| (Papandreou et al., [2017](#bib.bib123)) | Faster R-CNN + ResNet-101 | Produces
    heatmap and offset map of each joint for SPPE and combines them with an aggregation
    procedure; uses keypoint-based NMS to avoid duplicate poses | $64.9^{*}$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| (Papandreou et al., [2017](#bib.bib123)) | Faster R-CNN + ResNet-101 | 为
    SPPE 生成每个关节的热图和偏移图，并通过聚合过程将它们结合；使用基于关键点的 NMS 避免重复姿态 | $64.9^{*}$ |'
- en: '| (Huang et al., [2017](#bib.bib55)) | Faster R-CNN + Inception-v2 | Produces
    coarse and fine poses for SPPE with multi-level supervisions; multi-scale features
    fusion | $72.2^{*}$ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| (Huang et al., [2017](#bib.bib55)) | Faster R-CNN + Inception-v2 | 为 SPPE
    生成粗略和精细的姿态，具有多级监督；多尺度特征融合 | $72.2^{*}$ |'
- en: '| (He et al., [2017](#bib.bib51)) | Mask R-CNN + ResNet-FPN | An extension
    of Mask R-CNN framework; predicts keypoints and human mask synchronously | $63.1^{*}$
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| (He et al., [2017](#bib.bib51)) | Mask R-CNN + ResNet-FPN | Mask R-CNN框架的扩展；同步预测关键点和人体掩模
    | $63.1^{*}$'
- en: '| (Xiao et al., [2018](#bib.bib176)) | Faster R-CNN + ResNet | Simply adds
    a few deconvolutional layers after ResNet to generate heatmaps from deep and low
    resolution features | 73.7 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| (Xiao et al., [2018](#bib.bib176)) | Faster R-CNN + ResNet | 仅在ResNet之后添加了几个反卷积层，以从深层和低分辨率特征中生成热图
    | 73.7 |'
- en: '| (Chen et al., [2018](#bib.bib22)) | FPN + CPN | Proposes CPN with feature
    pyramid; two-stage network; online hard keypoints mining | 73.0 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| (Chen et al., [2018](#bib.bib22)) | FPN + CPN | 提出了具有特征金字塔的CPN；两阶段网络；在线困难关键点挖掘
    | 73.0 |'
- en: '| (Moon et al., [2019](#bib.bib112)) | ResNet + upsampling | proposes PoseFix
    net to refine estimated pose from any HPE methods based on pose error distributions
    | - |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| (Moon et al., [2019](#bib.bib112)) | ResNet + 上采样 | 提出了PoseFix网络以根据姿态误差分布从任何HPE方法中精细化估计的姿态
    | - |'
- en: '| (Sun et al., [2019](#bib.bib151)) | Faster R-CNN + HRNet | high-resolution
    representations of features across the whole network, multi-scale fusion | 75.5
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| (Sun et al., [2019](#bib.bib151)) | Faster R-CNN + HRNet | 跨整个网络的高分辨率特征表示，多尺度融合
    | 75.5 |'
- en: '| Bottom-up |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上的方法 |'
- en: '| (Pishchulin et al., [2016](#bib.bib131)) | Fast R-CNN | Formulate the distinguishing
    different persons as an ILP problem; cluster detected part candidates; combine
    person clusters and labeled parts to obtain final poses | - |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| (Pishchulin et al., [2016](#bib.bib131)) | Fast R-CNN | 将区分不同人员的问题表述为ILP问题；对检测到的部件候选进行聚类；结合人员簇和标记部件以获得最终姿态
    | - |'
- en: '| (Insafutdinov et al., [2016](#bib.bib58)) | ResNet | Employs image-conditioned
    pairwise terms to assemble the part proposals | - |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (Insafutdinov et al., [2016](#bib.bib58)) | ResNet | 使用图像条件配对项来组装部件提议 | -
    |'
- en: '| (Cao et al., [2016](#bib.bib13)) | VGG-19 + CPM | OpenPose; real-time; Simultaneous
    joints detection and association in a two-branch architecture; propose Part Affinity
    Fields (PAFs) to encode the location and orientation of limbs | $61.8^{*}$ |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| (Cao et al., [2016](#bib.bib13)) | VGG-19 + CPM | OpenPose；实时；在双分支架构中同时进行关节检测和关联；提出了部件亲和场（PAFs）来编码肢体的位置和方向
    | $61.8^{*}$ |'
- en: '| (Newell et al., [2017](#bib.bib114)) | Hourglass | Simultaneous joints detection
    and association in one branch; propose dense associative embedding tags for detected
    joints grouping | 65.5 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| (Newell et al., [2017](#bib.bib114)) | Hourglass | 在一个分支中同时进行关节检测和关联；提出了密集关联嵌入标签以进行检测关节分组
    | 65.5 |'
- en: '| (Nie et al., [2018](#bib.bib118)) | Hourglass | Simultaneous joints detection
    and association in a two-branch architecture; generate partitions in the embedding
    space parameterized by person centroids over joint candidates; estimate pose instances
    by a local greedy inference approach | - |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| (Nie et al., [2018](#bib.bib118)) | Hourglass | 在双分支架构中同时进行关节检测和关联；在嵌入空间中生成由关节候选上的人员质心参数化的分区；通过局部贪婪推理方法估计姿态实例
    | - |'
- en: '| (Papandreou et al., [2018](#bib.bib122)) | ResNet | Multi-task (pose estimation
    and instance segmentation) network; simultaneous joints detection and association
    in a multi-branch architecture; multi-range joint offsets following tree-structured
    kinematic graph to guide joints grouping | 68.7 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| (Papandreou et al., [2018](#bib.bib122)) | ResNet | 多任务（姿态估计和实例分割）网络；在多分支架构中同时进行关节检测和关联；使用树状运动学图指导关节分组的多范围关节偏移
    | 68.7 |'
- en: '| (Kocabas et al., [2018](#bib.bib76)) | ResNet-FPN + RetinaNet | Multi-task
    (pose estimation, person detection and person segmentation) network; simultaneous
    keypoint detection and person detection in a two-branch architecture; proposes
    a Pose Residual Network (PRN) to assign keypoint detection to person instances
    | 69.6 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| (Kocabas et al., [2018](#bib.bib76)) | ResNet-FPN + RetinaNet | 多任务（姿态估计、行人检测和行人分割）网络；在双分支架构中同时进行关键点检测和行人检测；提出了Pose
    Residual Network (PRN)来将关键点检测分配给行人实例 | 69.6 |'
- en: '| (Kreiss et al., [2019](#bib.bib77)) | ResNet-50 | predicts Part Intensity
    Fields (PIF) and Part Association Fields (PAF) to represent body joints location
    and body joints association; works well under low-resolution | 66.7 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| (Kreiss et al., [2019](#bib.bib77)) | ResNet-50 | 预测部件强度场（PIF）和部件关联场（PAF）来表示身体关节的位置和关节关联；在低分辨率下效果良好
    | 66.7 |'
- en: 3.2.1 Top-down methods
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 自上而下的方法
- en: The two most important components of top-down HPE methods are human body region
    candidate detector and a single person pose estimator. Most of the research focused
    on human part estimation based on existing human detectors such as Faster R-CNN (Ren
    et al., [2015](#bib.bib138)), Mask R-CNN (He et al., [2017](#bib.bib51)), FPN (Lin
    et al., [2017](#bib.bib92)). Iqbal and Gall ([2016](#bib.bib60)) utilized a convolutional
    pose machine-based pose estimator to generate initial poses. Then integer linear
    programming (ILP) is applied to obtain the final poses. Fang et al. ([2017](#bib.bib38))
    adopted spatial transformer network (STN) (Jaderberg et al., [2015](#bib.bib62)),
    Non-Maximum-Suppression (NMS), and Hourglass network to facilitate pose estimation
    in the presence of inaccurate human bounding boxes. Huang et al. ([2017](#bib.bib55))
    developed a coarse-fine network (CFN) with Inception-v2 network (Szegedy et al.,
    [2016](#bib.bib154)) as the backbone. The network is supervised in multiple levels
    for learning coarse and fine prediction. Xiao et al. ([2018](#bib.bib176)) added
    several deconvolutional layers over the last convolution layer of ResNet to generate
    heatmaps from deep and low-resolution features. Chen et al. ([2018](#bib.bib22))
    proposed a cascade pyramid network (CPN) by employing multi-scale feature maps
    from different layers to obtain more inference from local and global features
    with an online hard keypoint mining loss for difficulty joints. Based on similar
    pose error distributions of different HPE approaches, Moon et al. ([2019](#bib.bib112))
    designed PoseFix net to refine estimated poses from any methods.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 自顶向下的人体姿态估计方法的两个最重要的组成部分是人体区域候选检测器和单人姿态估计器。大部分研究集中在基于现有人体检测器（如Faster R-CNN (Ren
    et al., [2015](#bib.bib138))、Mask R-CNN (He et al., [2017](#bib.bib51))、FPN (Lin
    et al., [2017](#bib.bib92))等）进行的人体部位估计上。Iqbal 和 Gall ([2016](#bib.bib60)) 利用基于卷积姿态机的姿态估计器生成初始姿态，然后应用整数线性规划(ILP)获得最终姿态。Fang
    等人 ([2017](#bib.bib38)) 采用空间变换网络（STN）(Jaderberg et al., [2015](#bib.bib62))、非最大值抑制（NMS）和Hourglass网络来促进在存在不准确人体边界框的情况下的姿态估计。Huang
    等人 ([2017](#bib.bib55)) 开发了一个由Inception-v2网络(Szegedy et al., [2016](#bib.bib154))作为骨干的粗细网络（CFN）。该网络在多个层面上进行监督学习，学习粗糙和精细预测。Xiao
    等人 ([2018](#bib.bib176)) 在ResNet的最后一个卷积层上添加了几层反卷积层，从深层和低分辨率特征生成热图。Chen 等人 ([2018](#bib.bib22))
    提出了一种级联金字塔网络（CPN），通过使用来自不同层的多尺度特征图来获得更多来自局部和全局特征的推理，并采用在线难关键点挖掘损失进行难关节。基于不同HPE方法的类似姿态误差分布，Moon
    等人 ([2019](#bib.bib112)) 设计了PoseFix网络来改进来自任何方法的估计姿态。
- en: Top-down HPE methods can be easily implemented by combining existing detection
    networks and single HPE networks. Meanwhile, the performance of this kind of methods
    is affected by person detection results and the operation speed is usually not
    real-time.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 自顶向下的人体姿态估计方法可以通过组合现有的检测网络和单一的姿态估计网络轻松实现。与此同时，这类方法的性能受人体检测结果的影响，通常操作速度不是实时的。
- en: 3.2.2 Bottom-up methods
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 自下而上的方法
- en: The main components of bottom-up HPE methods include body joint detection and
    joint candidate grouping. Most algorithms handle these two components separately.
    DeepCut (Pishchulin et al., [2016](#bib.bib131)) employed a Fast R-CNN based body
    part detector to first detect all the body part candidates, then labeled each
    part to its corresponding part category, and assembled these parts with integer
    linear programming to a complete skeleton. DeeperCut (Insafutdinov et al., [2016](#bib.bib58))
    improved the DeepCut by using a stronger part detector based on ResNet and a better
    incremental optimization strategy exploring geometric and appearance constraints
    among joint candidates. OpenPose (Cao et al., [2016](#bib.bib13)) used CPM to
    predict candidates of all body joints with Part Affinity Fields (PAFs). The proposed
    PAFs can encode locations and orientations of limbs to assemble the estimated
    joints into different poses of persons. Nie et al. ([2018](#bib.bib118)) proposed
    a Pose Partition Network (PPN) to conduct both joint detection and dense regression
    for joint partition. Then PPN performs local inference for joint configurations
    with joint partition. Similar to OpenPose, Kreiss et al. ([2019](#bib.bib77))
    designed a PifPaf net to predict a Part Intensity Field (PIF) and a Part Association
    Field (PAF) to represent body joint locations and body joint association. It works
    well on low-resolution images due to the fine-grained PAF and the utilization
    of Laplace loss.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 自下而上的 HPE 方法的主要组成部分包括身体关节检测和关节候选分组。大多数算法分别处理这两个组件。DeepCut (Pishchulin et al.,
    [2016](#bib.bib131)) 使用基于 Fast R-CNN 的身体部位检测器，首先检测所有身体部位候选，然后将每个部位标记为其对应的部位类别，并使用整数线性规划将这些部位组装成完整的骨架。DeeperCut
    (Insafutdinov et al., [2016](#bib.bib58)) 通过使用基于 ResNet 的更强大的部位检测器和探索关节候选的几何和外观约束的更好增量优化策略改进了
    DeepCut。OpenPose (Cao et al., [2016](#bib.bib13)) 使用 CPM 通过 Part Affinity Fields
    (PAFs) 预测所有身体关节的候选。所提出的 PAFs 能够编码肢体的位置和方向，将估计的关节组装成不同的人体姿势。Nie et al. ([2018](#bib.bib118))
    提出了一个 Pose Partition Network (PPN) 来进行关节检测和关节分区的密集回归。然后，PPN 对关节配置进行局部推断。类似于 OpenPose，Kreiss
    et al. ([2019](#bib.bib77)) 设计了一个 PifPaf 网络来预测 Part Intensity Field (PIF) 和 Part
    Association Field (PAF) 以表示身体关节位置和身体关节关联。由于精细的 PAF 和利用了拉普拉斯损失，它在低分辨率图像上表现良好。
- en: The above methods are all following a separation of joint detection and joint
    grouping. Recently, some methods can do the prediction in one stage. Newell et al.
    ([2017](#bib.bib114)) introduced a single-stage deep network architecture to simultaneously
    perform both detection and grouping. This network can produce detection heatmaps
    for each joint, and associative embedding maps that contain the grouping tags
    of each joint.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以上方法都遵循了关节检测和关节分组的分离。最近，一些方法能够在一个阶段完成预测。Newell et al. ([2017](#bib.bib114))
    引入了一个单阶段深度网络架构，以同时执行检测和分组。该网络可以生成每个关节的检测热图以及包含每个关节分组标签的关联嵌入图。
- en: Some methods employed multi-task structures. Papandreou et al. ([2018](#bib.bib122))
    proposed a box-free multi-task network for pose estimation and instance segmentation.
    The ResNet-based network can synchronously predict joint heatmaps of all keypoints
    for every person and their relative displacements. Then the grouping starts from
    the most confident detection with a greedy decoding process based on a tree-structured
    kinematic graph. The network proposed by Kocabas et al. ([2018](#bib.bib76)) combines
    a multi-task model with a novel assignment method to handle human keypoint estimation,
    detection, and semantic segmentation tasks altogether. Its backbone network is
    a combination of ResNet and FPN with shared features for keypoints and person
    detection subnets. The human detection results are used as constraints of the
    spatial position of people.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法采用了多任务结构。Papandreou et al. ([2018](#bib.bib122)) 提出了一个无框多任务网络用于姿势估计和实例分割。基于
    ResNet 的网络可以同步预测每个人的所有关键点的关节热图及其相对位移。然后，分组从最可信的检测开始，并基于树结构运动学图进行贪婪解码过程。Kocabas
    et al. ([2018](#bib.bib76)) 提出的网络将多任务模型与新颖的分配方法相结合，以同时处理人体关键点估计、检测和语义分割任务。其骨干网络是
    ResNet 和 FPN 的组合，具有共享的关键点和人员检测子网络的特征。人体检测结果被用作人物空间位置的约束。
- en: Currently, the processing speed of bottom-up methods is very fast, and some
    (Cao et al., [2016](#bib.bib13); Nie et al., [2018](#bib.bib118)) can run in real-time.
    However, the performance can be very influenced by the complex background and
    human occlusions. The top-down approaches achieved state-of-the-art performance
    in almost all benchmark datasets while the processing speed is limited by the
    number of detected people.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，底层方法的处理速度非常快，一些方法（Cao et al., [2016](#bib.bib13); Nie et al., [2018](#bib.bib118)）可以实时运行。然而，性能可能受到复杂背景和人体遮挡的很大影响。顶层方法在几乎所有基准数据集上都达到了最先进的性能，但处理速度受到检测到的人数的限制。
- en: 4 3D Human Pose Estimation
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 3D 人体姿态估计
- en: 3D human pose estimation is to predict locations of body joints in 3D space
    from images or other input sources. Although commercial products such as Kinect (Kinect,
    [accessed on 2019](#bib.bib75)) with depth sensor, VICON (Vicon, [accessed on
    2019](#bib.bib169)) with optical sensor and TheCaptury (TheCaptury, [accessed
    on 2019](#bib.bib161)) with multiple cameras have been employed for 3D body pose
    estimation, all these systems work in very constrained environments or need special
    markers on human body. Monocular camera, as the most widely used sensor, is very
    important for 3D human pose estimation. Deep neural networks have the capability
    to estimate the dense depth (Li et al., [2015a](#bib.bib84), [2018a](#bib.bib82),
    [2019](#bib.bib90)) and sparse depth points (joints) as well from monocular images.
    Moreover, the progress of 3D human pose estimation from monocular inputs can further
    improve multi-view 3D human pose estimation in constrained environments. Thus,
    this section focuses on the deep learning-based methods that estimate 3D human
    pose from monocular RGB images and videos including 3D single person pose estimation
    and 3D multi-person pose estimation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 人体姿态估计是从图像或其他输入源中预测身体关节在 3D 空间中的位置。虽然像 Kinect（Kinect, [accessed on 2019](#bib.bib75)）、VICON（Vicon,
    [accessed on 2019](#bib.bib169)）和 TheCaptury（TheCaptury, [accessed on 2019](#bib.bib161)）这样的商业产品都用于
    3D 身体姿态估计，但这些系统都在非常受限的环境中工作或需要在人体上标记特殊标记。单目相机作为最广泛使用的传感器，对 3D 人体姿态估计非常重要。深度神经网络有能力从单目图像中估计密集深度（Li
    et al., [2015a](#bib.bib84), [2018a](#bib.bib82), [2019](#bib.bib90)）和稀疏深度点（关节）。此外，单目输入的
    3D 人体姿态估计的进展可以进一步改善受限环境中的多视角 3D 人体姿态估计。因此，本节重点关注基于深度学习的方法，这些方法从单目 RGB 图像和视频中估计
    3D 人体姿态，包括 3D 单人姿态估计和 3D 多人姿态估计。
- en: 4.1 3D single person pose estimation
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 3D 单人姿态估计
- en: 'Compared to 2D HPE, 3D HPE is more challenging since it needs to predict the
    depth information of body joints. In addition, the training data for 3D HPE are
    not easy to obtain as 2D HPE. Most existing datasets are obtained under constrained
    environments with limited generalizability. For single person pose estimation,
    the bounding box of the person in the image is normally provided, and hence it
    is not necessary to combine the process of person detection. In this section,
    we divide the methods of 3D single person pose estimation into model-free and
    model-based categories and summarize the recent work in Table [5](#S4.T5 "Table
    5 ‣ 4.1 3D single person pose estimation ‣ 4 3D Human Pose Estimation ‣ Monocular
    Human Pose Estimation: A Survey of Deep Learning-based Methods"). The last column
    of Table [5](#S4.T5 "Table 5 ‣ 4.1 3D single person pose estimation ‣ 4 3D Human
    Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") is the comparisons of Mean Per Joint Position Error (MPJPE) in millimeter
    on Human3.6M dataset under protocol #1\. More details of datasets and evaluation
    metrics are described in Section [5](#S5 "5 Datasets and evaluation protocols
    ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods").'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '与 2D HPE 相比，3D HPE 更具挑战性，因为它需要预测身体关节的深度信息。此外，3D HPE 的训练数据不像 2D HPE 那样容易获得。现有的大多数数据集是在受限环境下获得的，通用性有限。对于单人姿态估计，通常提供图像中的人物边界框，因此不需要结合人物检测过程。本节将
    3D 单人姿态估计的方法分为无模型和基于模型两类，并在表 [5](#S4.T5 "Table 5 ‣ 4.1 3D single person pose estimation
    ‣ 4 3D Human Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep
    Learning-based Methods") 中总结了近期工作。表 [5](#S4.T5 "Table 5 ‣ 4.1 3D single person
    pose estimation ‣ 4 3D Human Pose Estimation ‣ Monocular Human Pose Estimation:
    A Survey of Deep Learning-based Methods") 的最后一列是 Human3.6M 数据集中在协议 #1 下的每关节位置误差（MPJPE）的比较，以毫米为单位。数据集和评估指标的更多细节在第
    [5](#S5 "5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation:
    A Survey of Deep Learning-based Methods") 节中描述。'
- en: 'Table 5: Comparison of 3D single person pose estimation methods. Here “E.”
    stands for “Extra data” and “T.” indicates “Temporal info”. The last column is
    the Mean Per Joint Position Error (MPJPE) in millimeter on Human3.6M dataset under
    protocol #1\. The results with $*$ were reported from 6 actions in testing set,
    while others from all 17 actions. The results with $\dagger$ were reported with
    2D joint ground truth. The methods with $\#$ report joint rotation as well.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5：3D单人姿势估计方法的比较。这里“E.”代表“额外数据”，“T.”表示“时间信息”。最后一列是Human3.6M数据集中协议 #1 下的每关节位置平均误差（MPJPE），单位为毫米。带有
    $*$ 的结果来自测试集中6个动作，而其他结果来自全部17个动作。带有 $\dagger$ 的结果使用2D关节真实值。带有 $\#$ 的方法也报告了关节旋转。'
- en: '| Methods | Backbone | E. | T. | Highlights | MPJPE (mm) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干 | E. | T. | 亮点 | MPJPE (毫米) |'
- en: '| Model-free |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 无模型 |'
- en: '| (Li and Chan, [2014](#bib.bib87)) | shallow CNNs | ✗ | ✗ | A multi-task network
    to predict of body part detection with sliding windows and 3D pose estimation
    jointly | $132.2^{*}$ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| (Li 和 Chan，[2014](#bib.bib87)) | 浅层CNN | ✗ | ✗ | 一个多任务网络，用于预测身体部位检测与滑动窗口和3D姿势估计联合进行
    | $132.2^{*}$ |'
- en: '| (Li et al., [2015b](#bib.bib89)) | shallow CNNs | ✗ | ✗ | Compute matching
    score of image-pose pairs | $120.2^{*}$ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| (Li 等，[2015b](#bib.bib89)) | 浅层CNN | ✗ | ✗ | 计算图像-姿势对的匹配得分 | $120.2^{*}$
    |'
- en: '| (Tekin et al., [2016](#bib.bib159)) | auto-encoder+ shallow CNNs | ✗ | ✗
    | Employ an auto-encoder to learn a high-dimensional representation of 3D pose;
    use a shallow CNNs network to learn the high-dimensional pose representation |
    $116.8^{*}$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| (Tekin 等，[2016](#bib.bib159)) | 自编码器+浅层CNN | ✗ | ✗ | 使用自编码器学习3D姿势的高维表示；使用浅层CNN网络学习高维姿势表示
    | $116.8^{*}$ |'
- en: '| (Tekin et al., [2017](#bib.bib160)) | Hourglass | ✓ | ✗ | Predict 2D heatmaps
    for joints first; then use a trainable fusion architecture to combine 2D heatmaps
    and extracted features; 2D module is pre-trained with MPII | $69.7$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| (Tekin 等，[2017](#bib.bib160)) | Hourglass | ✓ | ✗ | 首先预测关节的2D热图；然后使用可训练的融合架构将2D热图和提取的特征结合；2D模块在MPII上预训练
    | $69.7$ |'
- en: '| (Chen and Ramanan, [2017](#bib.bib17)) | CPM | ✓ | ✗ | Estimate 2D poses
    from images first; then estimate depth of them by matching to a library of 3D
    poses; 2D module is pre-trained with MPII | $82.7$  $/57.5^{\dagger}$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| (Chen 和 Ramanan，[2017](#bib.bib17)) | CPM | ✓ | ✗ | 首先从图像中估计2D姿势；然后通过匹配到3D姿势库来估计深度；2D模块在MPII上预训练
    | $82.7$  $/57.5^{\dagger}$ |'
- en: '| (Moreno-Noguer, [2017](#bib.bib113)) | CPM | ✓ | ✗ | Use Euclidean Distance
    Matrices (EDMs) to encoding pairwise distances of 2D and 3D body joints; train
    a network to learn 2D-to-3D EDM regression; jointly trained with other 3D (Humaneva-I)
    dataset | $87.3$ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| (Moreno-Noguer，[2017](#bib.bib113)) | CPM | ✓ | ✗ | 使用欧氏距离矩阵（EDM）编码2D和3D身体关节的成对距离；训练网络学习2D到3D
    EDM回归；与其他3D (Humaneva-I) 数据集联合训练 | $87.3$ |'
- en: '| (Pavlakos et al., [2017](#bib.bib125)) | Hourglass | ✓ | ✗ | Volumetric representation
    for 3D human pose; a coarse-to-fine prediction scheme; 2D module is pre-trained
    with MPII | $71.9$ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| (Pavlakos 等，[2017](#bib.bib125)) | Hourglass | ✓ | ✗ | 用于3D人体姿势的体积表示；粗到细的预测方案；2D模块在MPII上预训练
    | $71.9$ |'
- en: '| (Zhou et al., [2017](#bib.bib184)) | Hourglass | ✓ | ✗ | A proposed loss
    induced from a geometric constraint for 2D data; bone-length constraints; jointly
    trained with 2D (MPII) dataset | $64.9$ |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| (Zhou 等，[2017](#bib.bib184)) | Hourglass | ✓ | ✗ | 提出的损失来自2D数据的几何约束；骨长约束；与2D
    (MPII) 数据集联合训练 | $64.9$ |'
- en: '| (Martinez et al., [2017](#bib.bib103)) | Hourglass | ✓ | ✗ | Directly map
    predicted 2D poses to 3D poses with two linear layers; 2D module is pre-trained
    with MPII; process in real-time | $62.9$  $/45.5^{\dagger}$ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| (Martinez 等，[2017](#bib.bib103)) | Hourglass | ✓ | ✗ | 通过两个线性层将预测的2D姿势直接映射到3D姿势；2D模块在MPII上预训练；实时处理
    | $62.9$  $/45.5^{\dagger}$ |'
- en: '| (Sun et al., [2017](#bib.bib152))^# | ResNet | ✓ | ✗ | A bone-based representation
    involving body structure information to enhance robustness; bone-length constraints;
    jointly trained with 2D (MPII) dataset | $48.3$ |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| (Sun 等，[2017](#bib.bib152))^# | ResNet | ✓ | ✗ | 基于骨骼的表示，涉及身体结构信息以增强鲁棒性；骨长约束；与2D
    (MPII) 数据集联合训练 | $48.3$ |'
- en: '| (Yang et al., [2018](#bib.bib179)) | Hourglass | ✓ | ✗ | Adversarial learning
    for domain adaptation of 2D/3D datasets; adopted generator from (Zhou et al.,
    [2017](#bib.bib184)); multi-source discriminator with image, pairwise geometric
    structure and joint location; jointly trained with 2D (MPII) dataset | $58.6$
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| (Yang 等，[2018](#bib.bib179)) | Hourglass | ✓ | ✗ | 用于2D/3D数据集领域适应的对抗学习；采用
    (Zhou 等，[2017](#bib.bib184)) 的生成器；多源判别器包括图像、成对几何结构和关节位置；与2D (MPII) 数据集联合训练 | $58.6$
    |'
- en: '| Pavlakos et al. ([2018a](#bib.bib124)) | Hourglass | ✓ | ✗ | Volumetric representation
    for 3D human pose; additional ordinal depths annotations for human joints; jointly
    trained with 2D (MPII) and 3D (Humaneva-I) datasets | $56.2$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Pavlakos et al. ([2018a](#bib.bib124)) | Hourglass | ✓ | ✗ | 3D 人体姿态的体积表示；为人体关节增加了额外的序数深度注释；与
    2D（MPII）和 3D（Humaneva-I）数据集联合训练 | $56.2$ |'
- en: '| (Sun et al., [2018](#bib.bib153)) | Mask R-CNN | ✓ | ✗ | Volumetric representation
    for 3D human pose; integral operation unifies the heat map representation and
    joint regression; jointly trained with 2D (MPII) dataset | $40.6$ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| (Sun et al., [2018](#bib.bib153)) | Mask R-CNN | ✓ | ✗ | 3D 人体姿态的体积表示；积分操作统一了热图表示和关节回归；与
    2D（MPII）数据集联合训练 | $40.6$ |'
- en: '| (Li and Lee, [2019](#bib.bib85)) | Hourglass | ✓ | ✗ | Multiple hypotheses
    of 3D poses are generated from 2D poses; the best one is chosen by 2D reprojections;
    2D module is pre-trained with MPII | $52.7$ |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| (Li and Lee, [2019](#bib.bib85)) | Hourglass | ✓ | ✗ | 从 2D 姿态生成多个 3D 姿态假设；通过
    2D 重新投影选择最佳一个；2D 模块使用 MPII 进行预训练 | $52.7$ |'
- en: '| Model-based |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 基于模型 |'
- en: '| (Bogo et al., [2016](#bib.bib8))^# | DeepCut | ✗ | ✗ | SMPL model; fit SMPL
    model to 2D joints by minimizing the distance between 2D joints and projected
    3D model joints | $82.3$ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| (Bogo et al., [2016](#bib.bib8))^# | DeepCut | ✗ | ✗ | SMPL 模型；通过最小化 2D 关节和投影的
    3D 模型关节之间的距离来拟合 SMPL 模型 | $82.3$'
- en: '| (Zhou et al., [2016](#bib.bib185))^# | ResNet | ✗ | ✗ | kinematic model;
    embedded a kinematic object model into network for general articulated object
    pose estimation; orientation and rotational constrains | $107.3$ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| (Zhou et al., [2016](#bib.bib185))^# | ResNet | ✗ | ✗ | 运动学模型；将运动学对象模型嵌入网络中，以实现通用的关节对象姿态估计；方向和旋转约束
    | $107.3$ |'
- en: '| (Mehta et al., [2017c](#bib.bib107))^# | ResNet | ✓ | ✓ | A real-time pipeline
    with temporal smooth filter and model-based kinematic skeleton fitting; 2D module
    is pre-trained with MPII and LSP; process in real-time; provide body height |
    $80.5$ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| (Mehta et al., [2017c](#bib.bib107))^# | ResNet | ✓ | ✓ | 一个实时管道，结合了时间平滑滤波器和基于模型的运动学骨架拟合；2D
    模块使用 MPII 和 LSP 进行预训练；实时处理；提供身体高度 | $80.5$ |'
- en: '| (Tan et al., [2017](#bib.bib155)) | shallow CNNs | ✗ | ✗ | SMPL model; first
    train a decoder to predict a 2D body silhouette from parameters of SMPL; then
    train a encoder-decoder network with images and corresponding silhouettes; the
    trained encoder can predict parameters of SMPL from images | - |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| (Tan et al., [2017](#bib.bib155)) | 浅层 CNN | ✗ | ✗ | SMPL 模型；首先训练一个解码器从 SMPL
    参数预测 2D 身体轮廓；然后训练一个编码-解码网络，利用图像和相应的轮廓；训练后的编码器可以从图像预测 SMPL 参数 | - |'
- en: '| (Mehta et al., [2017a](#bib.bib104)) | Resnet | ✓ | ✗ | Kinematic model;
    transfer learning from features learned for 2D pose estimation; 2D pose prediction
    as auxiliary task; predict relative joint locations following the kinematic tree
    body model; jointly trained with 2D (MPII and LSP) datasets | $74.1$ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| (Mehta et al., [2017a](#bib.bib104)) | Resnet | ✓ | ✗ | 运动学模型；从为 2D 姿态估计学习的特征进行迁移学习；将
    2D 姿态预测作为辅助任务；预测相对关节位置，遵循运动学树体模型；与 2D（MPII 和 LSP）数据集联合训练 | $74.1$ |'
- en: '| (Nie et al., [2017](#bib.bib117)) | RMPE + LSTM | ✓ | ✗ | Kinematic model;
    joint depth estimation from global 2D pose with skeleton-LSTM and local body parts
    with patch-LSTM; 2D module is pre-trained with MPII | $79.5$ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| (Nie et al., [2017](#bib.bib117)) | RMPE + LSTM | ✓ | ✗ | 运动学模型；从全局 2D 姿态和骨架-LSTM
    以及局部身体部位和补丁-LSTM 中进行关节深度估计；2D 模块使用 MPII 进行预训练 | $79.5$ |'
- en: '| (Kanazawa et al., [2018](#bib.bib73))^# | ResNet | ✓ | ✗ | SMPL model; adversarial
    learning for domain adaptation of 2D images and 3D human body model; propose a
    framework to learn parameters of SMPL; jointly trained with 2D (LSP, MPII and
    COCO) datasets; process in real-time | $88.0$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| (Kanazawa et al., [2018](#bib.bib73))^# | ResNet | ✓ | ✗ | SMPL 模型；用于 2D
    图像和 3D 人体模型领域适应的对抗学习；提出一个学习 SMPL 参数的框架；与 2D（LSP、MPII 和 COCO）数据集联合训练；实时处理 | $88.0$
    |'
- en: '| (Pavlakos et al., [2018b](#bib.bib126))^# | Hourglass | ✓ | ✗ | SMPL model;
    first predict 2D heatmaps of joint and human silhouette; second generate parameters
    of SMPL; 2D module is trained with MPII and LSP | $75.9$ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| (Pavlakos et al., [2018b](#bib.bib126))^# | Hourglass | ✓ | ✗ | SMPL 模型；首先预测关节和人体轮廓的
    2D 热图；其次生成 SMPL 参数；2D 模块使用 MPII 和 LSP 进行训练 | $75.9$ |'
- en: '| (Omran et al., [2018](#bib.bib120))^# | RefineNet | ✗ | ✗ | SMPL model; first
    predict 2D body parts segmentation from the RGB image; second take this segmentation
    to predict the parameters of SMPL | $59.9$ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| (Omran et al., [2018](#bib.bib120))^# | RefineNet | ✗ | ✗ | SMPL 模型；首先从 RGB
    图像中预测 2D 身体部位分割；其次利用该分割预测 SMPL 参数 | $59.9$ |'
- en: '| (Varol et al., [2018](#bib.bib167)) | Hourglass | ✓ | ✗ | SMPL model; first
    predict 2D pose and 2D body parts segmentation; second predict 3D pose; finally
    predict volumetric shape to fit SMPL model; 2D modules are trained with MPII and
    SURREAL | $49.0$ |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| (Varol et al., [2018](#bib.bib167)) | Hourglass | ✓ | ✗ | SMPL模型；首先预测2D姿态和2D身体部位分割；其次预测3D姿态；最后预测体积形状以适配SMPL模型；2D模块使用MPII和SURREAL进行训练
    | $49.0$ |'
- en: '| (Arnab et al., [2019](#bib.bib6))^# | ResNet | ✓ | ✓ | SMPL model; 2D keypoints,
    SMPL and camera parameters estimation; off-line bundle adjustment with temporal
    constraints; 2D module is trained with COCO | $77.8$  $/63.3^{\dagger}$ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| (Arnab et al., [2019](#bib.bib6))^# | ResNet | ✓ | ✓ | SMPL模型；2D关键点、SMPL和相机参数估计；离线束调整与时间约束；2D模块使用COCO进行训练
    | $77.8$  $/63.3^{\dagger}$ |'
- en: '| (Tome et al., [2017](#bib.bib162)) | CPM | ✓ | ✗ | Pre-trained probabilistic
    3D pose model; 3D lifting and projection by probabilistic model within the CPM-like
    network; 2D module is pre-trained with MPII; process in real-time | $88.4$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| (Tome et al., [2017](#bib.bib162)) | CPM | ✓ | ✗ | 预训练的概率3D姿态模型；在CPM-like网络内通过概率模型进行3D提升和投影；2D模块使用MPII进行预训练；实时处理
    | $88.4$ |'
- en: '| (Rhodin et al., [2018a](#bib.bib139)) | Hourglass | ✗ | ✗ | A latent variable
    body model learned from multi-view images; an encoder-decoder to predict a novel
    view image from a given one; the pre-trained encoder with additional shallow layers
    to predict 3D poses from images | - |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| (Rhodin et al., [2018a](#bib.bib139)) | Hourglass | ✗ | ✗ | 从多视图图像中学习的潜变量身体模型；一个编码器-解码器预测给定图像的全新视图图像；预训练编码器与额外的浅层网络以从图像中预测3D姿态
    | - |'
- en: 4.1.1 Model-free methods
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 无模型方法
- en: 'The model-free methods do not employ human body models as the predicted target
    or intermediate cues. They can be roughly categorized into two types: 1) directly
    map an image to 3D pose, and 2) estimate depth following intermediately predicted
    2D pose from 2D pose estimation methods.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型方法不使用人体模型作为预测目标或中间提示。它们大致可以分为两种类型：1）直接将图像映射到3D姿态，2）根据从2D姿态估计方法中间预测的2D姿态估计深度。
- en: Approaches that directly estimate the 3D pose from image features usually contain
    very few constraints. Li and Chan ([2014](#bib.bib87)) employed a shallow network
    to regress 3D joint coordinates directly with synchronous task of body part detection
    with sliding windows. Pavlakos et al. ([2017](#bib.bib125)) proposed a volumetric
    representation for 3D human pose and employed a coarse-to-fine prediction scheme
    to refine predictions with a multi-stage structure. Some researchers attempted
    to add body structure information or the dependencies between human joints to
    the deep learning networks. Li et al. ([2015b](#bib.bib89)) designed an embedding
    sub-network learning latent pose structure information to guide the 3D joint coordinates
    mapping. The sub-network can assign matching scores for input image-pose pairs
    with a maximum-margin cost function. Tekin et al. ([2016](#bib.bib159)) pre-trained
    an unsupervised auto-encoder to learn a high-dimensional latent pose representation
    of 3D pose for adding implicit constraints about the human body and then used
    a shallow network to learn the high-dimensional pose representation. Sun et al.
    ([2017](#bib.bib152)) proposed a structure-aware regression approach. They designed
    a bone-based representation involving body structure information which is more
    stable than only using joint positions. Pavlakos et al. ([2018a](#bib.bib124))
    trained the network with additional ordinal depths of human joints as constraints,
    by which the 2D human datasets can also be feed in with ordinal depths annotations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从图像特征估计3D姿态的方法通常约束很少。Li和Chan ([2014](#bib.bib87))使用浅层网络直接回归3D关节坐标，并与滑动窗口的身体部位检测同步任务。Pavlakos
    et al. ([2017](#bib.bib125))提出了一种3D人体姿态的体积表示，并采用粗到精的预测方案通过多阶段结构来细化预测。一些研究者尝试将身体结构信息或人体关节之间的依赖关系添加到深度学习网络中。Li
    et al. ([2015b](#bib.bib89))设计了一个嵌入子网络，以学习潜在的姿态结构信息来引导3D关节坐标映射。该子网络可以为输入图像-姿态对分配匹配分数，并使用最大边际成本函数。Tekin
    et al. ([2016](#bib.bib159))预训练了一个无监督自编码器，学习3D姿态的高维潜在姿态表示，以添加关于人体的隐含约束，然后使用浅层网络学习高维姿态表示。Sun
    et al. ([2017](#bib.bib152))提出了一种结构感知回归方法。他们设计了一种基于骨骼的表示，涉及比仅使用关节位置更稳定的身体结构信息。Pavlakos
    et al. ([2018a](#bib.bib124))使用额外的关节排序深度作为约束来训练网络，通过这种方式，2D人体数据集也可以通过排序深度注释进行输入。
- en: The 3D HPE methods which intermediately estimate 2D poses gain the advantages
    of 2D HPE, and can easily utilize images from 2D human datasets. Some of them
    adopt off-the-shelf 2D HPE modules to first estimate 2D poses, then extend to
    3D poses. (Martinez et al., [2017](#bib.bib103)) designed a 2D-to-3D pose predictor
    with only two linear layers. (Zhou et al., [2017](#bib.bib184)) presented a depth
    regression module to predict 3D pose from 2D heatmaps with a proposed geometric
    constraint loss for 2D data. Tekin et al. ([2017](#bib.bib160)) proposed a two-branch
    framework to predict 2D heatmaps and extract features from images. The extracted
    features are fused with 2D heatmaps by a trainable fusion scheme instead of being
    hand-crafted to obtain the final 3D joint coordinates. Li and Lee ([2019](#bib.bib85))
    considered 3D HPE as an inverse problem with multiple feasible solutions. Multiple
    feasible hypotheses of 3D poses are generated from 2D poses and the best one is
    chosen by 2D reprojections. Qammaz and Argyros ([2019](#bib.bib135)) proposed
    MocapNET directly encoding 2D poses into the 3D BVH (Meredith et al., [2001](#bib.bib108))
    format for subsequent rendering. By consolidating OpenPose (Cao et al., [2016](#bib.bib13))
    the architecture estimated and rendered 3D human pose in real-time using only
    CPU processing.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 中间估计2D姿势的3D HPE方法获得了2D HPE的优势，并且可以轻松利用来自2D人类数据集的图像。其中一些采用了现成的2D HPE模块，首先估计2D姿势，然后扩展到3D姿势。（Martinez等人，[2017](#bib.bib103)）设计了一个仅有两层线性层的2D到3D姿势预测器。（Zhou等人，[2017](#bib.bib184)）提出了一个深度回归模块，通过提出的几何约束损失来预测2D热图中的3D姿势。Tekin等人（[2017](#bib.bib160)）提出了一个双分支框架来预测2D热图并从图像中提取特征。提取的特征通过一个可训练的融合方案与2D热图融合，而不是手工制作，以获得最终的3D关节坐标。Li和Lee（[2019](#bib.bib85)）将3D
    HPE视为具有多种可行解的逆问题。生成多个3D姿势的可行假设，从2D姿势中选择最佳的，通过2D重投影进行选择。Qammaz和Argyros（[2019](#bib.bib135)）提出了MocapNET，直接将2D姿势编码为3D
    BVH（Meredith等人，[2001](#bib.bib108)）格式以进行后续渲染。通过整合OpenPose（Cao等人，[2016](#bib.bib13)），该架构使用仅CPU处理来实时估计和渲染3D人体姿势。
- en: When mapping 2D pose to 3D pose, different strategies may be applied. Chen and
    Ramanan ([2017](#bib.bib17)) used a matching strategy for an estimated 2D pose
    and 3D pose from a library. Moreno-Noguer ([2017](#bib.bib113)) encoded pairwise
    distances of 2D and 3D body joints into two Euclidean Distance Matrices (EDMs)
    and trained a regression network to learn the mapping of the two matrices. Wang
    et al. ([2018a](#bib.bib171)) predicted depth rankings of human joints as a cue
    to infer 3D joint positions from a 2D pose. Yang et al. ([2018](#bib.bib179))
    adopted a generator from (Zhou et al., [2017](#bib.bib184)) and designed a multi-source
    discriminator with image, pairwise geometric structure, and joint location information.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 将2D姿势映射到3D姿势时，可能会应用不同的策略。Chen和Ramanan（[2017](#bib.bib17)）使用了一种匹配策略，从库中估计2D姿势和3D姿势。Moreno-Noguer（[2017](#bib.bib113)）将2D和3D身体关节的成对距离编码为两个欧几里得距离矩阵（EDMs），并训练了一个回归网络来学习这两个矩阵的映射。Wang等人（[2018a](#bib.bib171)）预测了人体关节的深度排名作为从2D姿势推断3D关节位置的线索。Yang等人（[2018](#bib.bib179)）采用了（Zhou等人，[2017](#bib.bib184)）的生成器，并设计了一个多源判别器，结合了图像、成对几何结构和关节位置的信息。
- en: 4.1.2 Model-based methods
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 基于模型的方法
- en: Model-based methods generally employ a parametric body model or template to
    estimate human pose and shape from images. Early geometric-based models are not
    included in this paper. More recent models are estimated from multiple scans of
    diverse people (Hasler et al., [2009](#bib.bib50); Loper et al., [2015](#bib.bib96);
    Pons-Moll et al., [2015](#bib.bib132); Zuffi and Black, [2015](#bib.bib186)) or
    combination of different body models (Joo et al., [2018](#bib.bib71)). These models
    are typically parameterized by separate body pose and shape components.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法通常使用参数化的身体模型或模板来从图像中估计人体姿势和形状。早期基于几何的模型未包括在本文中。更现代的模型通常通过对多个人进行扫描（Hasler等人，[2009](#bib.bib50)；Loper等人，[2015](#bib.bib96)；Pons-Moll等人，[2015](#bib.bib132)；Zuffi和Black，[2015](#bib.bib186)）或不同身体模型的组合（Joo等人，[2018](#bib.bib71)）来估计。这些模型通常由单独的身体姿势和形状组件参数化。
- en: Some work employed the body model of SMPL (Loper et al., [2015](#bib.bib96))
    and attempted to estimate the 3D parameters from images. For example, Bogo et al.
    ([2016](#bib.bib8)) fit SMPL model to estimated 2D joints and proposed an optimization-based
    method to recover SMPL parameters from 2D joints. Tan et al. ([2017](#bib.bib155))
    inferred SMPL parameters by first training a decoder to predict silhouettes from
    SMPL parameters with synthetic data, and then learning an image encoder with the
    trained decoder. The trained encoder can predict SMPL parameters from input images.
    Directly learning parameters of SMPL is hard, some work predicted intermediate
    cues as constrains. For example, intermediate 2D pose and human body segmentation
    (Pavlakos et al., [2018b](#bib.bib126)), body parts segmentation (Omran et al.,
    [2018](#bib.bib120)), 2D pose and body parts segmentation (Varol et al., [2018](#bib.bib167)).
    In order to overcome the problem of lacking training data for the human body model,
    (Kanazawa et al., [2018](#bib.bib73)) employed adversarial learning by using a
    generator to predict parameters of SMPL, and a discriminator to distinguish the
    real SMPL model and the predicted ones. (Arnab et al., [2019](#bib.bib6)) reconstructed
    person from video sequences which explored the multiple views information.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究使用了 SMPL 的身体模型（Loper 等人，[2015](#bib.bib96)），并尝试从图像中估计 3D 参数。例如，Bogo 等人（[2016](#bib.bib8)）将
    SMPL 模型拟合到估计的 2D 关节上，并提出了一种基于优化的方法来从 2D 关节恢复 SMPL 参数。Tan 等人（[2017](#bib.bib155)）通过首先训练一个解码器来预测来自
    SMPL 参数的轮廓，再用训练好的解码器学习一个图像编码器，从而推断 SMPL 参数。训练好的编码器可以从输入图像中预测 SMPL 参数。直接学习 SMPL
    参数很困难，一些研究预测了中间线索作为约束。例如，中间 2D 姿态和人体分割（Pavlakos 等人，[2018b](#bib.bib126)），身体部位分割（Omran
    等人，[2018](#bib.bib120)），2D 姿态和身体部位分割（Varol 等人，[2018](#bib.bib167)）。为了克服人体模型训练数据不足的问题，（Kanazawa
    等人，[2018](#bib.bib73)）通过使用生成器预测 SMPL 参数，并使用判别器区分真实的 SMPL 模型和预测模型来采用对抗学习。（Arnab
    等人，[2019](#bib.bib6)）从视频序列中重建人物，探索了多视角信息。
- en: Kinematic model is widely used for 3D HPE. (Mehta et al., [2017a](#bib.bib104))
    predicted relative joint locations from 2D heatmaps following the kinematic tree
    body model. (Nie et al., [2017](#bib.bib117)) employed LSTM to exploit global
    2D joint locations and local body part images following kinematic tree body model
    which are two cues for joint depth estimation. Zhou et al. ([2016](#bib.bib185))
    embedded a kinematic object model into a network for general articulated object
    pose estimation which provides orientation and rotational constrains. Mehta et al.
    ([2017c](#bib.bib107)) proposed a pipeline for 3D single HPE running in real-time.
    The temporal information and kinematic body model are used as a smooth filter
    and skeleton fitting respectively. Rhodin et al. ([2018a](#bib.bib139)) used an
    encoder-decoder network to learn a latent variable body model without 2D or 3D
    annotations under self-supervision, then employed the pre-trained encoder to predict
    3D poses.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 动力学模型在 3D HPE 中得到广泛应用。（Mehta 等人，[2017a](#bib.bib104)）在动力学树体模型下，从 2D 热图中预测相对关节位置。（Nie
    等人，[2017](#bib.bib117)）使用 LSTM 来利用全球 2D 关节位置和局部身体部位图像，这两种线索用于关节深度估计。Zhou 等人（[2016](#bib.bib185)）将一个动力学对象模型嵌入到网络中，用于通用的关节化对象姿态估计，提供了方向和旋转约束。Mehta
    等人（[2017c](#bib.bib107)）提出了一种 3D 单体 HPE 实时运行的管道。时间信息和动力学身体模型分别作为平滑滤波器和骨架拟合使用。Rhodin
    等人（[2018a](#bib.bib139)）使用编码器-解码器网络在自我监督下学习潜在变量身体模型，没有 2D 或 3D 注释，然后使用预训练的编码器来预测
    3D 姿态。
- en: Additional to those typical body models, latent 3D pose model learned from data
    is also used for 3D HPE. Tome et al. ([2017](#bib.bib162)) proposed a multi-stage
    CPM-like network including a pre-trained probabilistic 3D pose model layer which
    can generate 3D pose from 2D heatmaps.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除了那些典型的身体模型外，还使用了从数据中学习到的潜在 3D 姿态模型用于 3D HPE。Tome 等人（[2017](#bib.bib162)）提出了一种多阶段类似
    CPM 的网络，包括一个经过预训练的概率 3D 姿态模型层，该层可以从 2D 热图生成 3D 姿态。
- en: 'Table 6: Summary of 3D multi-person pose estimation methods.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：3D 多人姿态估计方法总结。
- en: '| Methods | Network type | Highlights |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 网络类型 | 亮点 |'
- en: '| --- | --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| (Mehta et al., [2017b](#bib.bib106)) | ResNet | Propose an occlusion-robust
    pose-maps (ORPM) for full-body pose inference even under (self-)occlusions; combine
    2D pose and part affinity fields to infer person instances |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| (Mehta 等人，[2017b](#bib.bib106)) | ResNet | 提出了一个抗遮挡姿态图（ORPM），即使在（自我）遮挡下也能进行全身姿态推断；结合
    2D 姿态和部分亲和场来推断人物实例 |'
- en: '| (Rogez et al., [2017](#bib.bib141)) | Faster R-CNN + VGG-16 | Localize human
    bounding boxes with Faster R-CNN; classify the closest anchor-pose for each proposal;
    regress anchor-pose to get final pose |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| (Rogez 等人，[2017](#bib.bib141)) | Faster R-CNN + VGG-16 | 使用 Faster R-CNN
    定位人类边界框；为每个提议分类最接近的锚点姿态；回归锚点姿态以获得最终姿态 |'
- en: '| (Zanfir et al., [2018](#bib.bib181)) | DMHS | Feed forward process of body
    parts semantic segmentation and 3d pose estimates; feed backward process of refining
    pose and shape parameters of a body model SMPL |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| (Zanfir 等人，[2018](#bib.bib181)) | DMHS | 身体部位语义分割和 3D 姿态估计的前馈过程；对身体模型 SMPL
    的姿态和形状参数进行精细化的反向传播过程 |'
- en: '| Mehta et al. ([2019](#bib.bib105)) | SelecSLS Net | Real-time; a new CNN
    architecture that uses selective long and short range skip connections; 2D and
    3D pose features prediction along with identity assignments for all visible joints
    of all individuals; complete 3D pose reconstruction including occluded joints;
    temporal stability refinement and kinematic skeleton fitting. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Mehta 等人 ([2019](#bib.bib105)) | SelecSLS Net | 实时；一种新的 CNN 架构，使用选择性的长短距离跳跃连接；2D
    和 3D 姿态特征预测以及所有可见关节的身份分配；包括遮挡关节的完整 3D 姿态重建；时间稳定性精细化和运动骨架拟合。 |'
- en: 4.2 3D multi-person pose estimation
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 3D 多人姿态估计
- en: 'The achievements of monocular 3D multi-person pose estimation are based on
    3D single person pose estimation and other deep learning methods. This research
    field is pretty new and only a few methods are proposed. Table [6](#S4.T6 "Table
    6 ‣ 4.1.2 Model-based methods ‣ 4.1 3D single person pose estimation ‣ 4 3D Human
    Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") summarizes these methods.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '单目 3D 多人姿态估计的成就基于 3D 单人姿态估计和其他深度学习方法。该研究领域相对较新，提出的方法也很少。表 [6](#S4.T6 "Table
    6 ‣ 4.1.2 Model-based methods ‣ 4.1 3D single person pose estimation ‣ 4 3D Human
    Pose Estimation ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") 总结了这些方法。'
- en: Mehta et al. ([2017b](#bib.bib106)) proposed a bottom-up method by using 2D
    pose and part affinity fields to infer person instances. An occlusion-robust pose-maps
    (ORPM) is proposed to provide multi-style occlusion information regardless of
    the number of people. Rogez et al. ([2017](#bib.bib141)) proposed a Localization-Classification-Regression
    Network (LCR-Net) following three-stage processing. First, Faster R-CNN is employed
    to detect people locations. Second, each pose proposal is assigned with the closest
    anchor-pose scored by a classifier. The final poses are refined with a regressor
    respectively. Zanfir et al. ([2018](#bib.bib181)) proposed a framework with feed
    forward and feed backward stages for 3D multi-person pose and shape estimation.
    The feed forward process includes semantic segmentation of body parts and 3D pose
    estimates based on DMHS (Popa et al., [2017](#bib.bib133)). Then the feed backward
    process refines the pose and shape parameters of SMPL (Loper et al., [2015](#bib.bib96)).
    Mehta et al. ([2019](#bib.bib105)) estimated multiple poses in real-time with
    three stages. First, SelecSLS Net infers 2D pose and intermediate 3D pose encoding
    for visible body joints. Then based on each detected person, it reconstructs the
    complete 3D pose, including occluded joints. Finally, refinement is provided for
    temporal stability and kinematic skeleton fitting.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Mehta 等人 ([2017b](#bib.bib106)) 提出了一个自下而上的方法，通过使用 2D 姿态和部件亲和场来推断人员实例。提出了一种遮挡鲁棒的姿态映射（ORPM），无论人员数量多少，都能提供多样化的遮挡信息。Rogez
    等人 ([2017](#bib.bib141)) 提出了一个三阶段处理的定位-分类-回归网络（LCR-Net）。首先，使用 Faster R-CNN 检测人员位置。其次，每个姿态提议都由分类器分配到最接近的锚点姿态。最终姿态通过回归器分别进行精细化。Zanfir
    等人 ([2018](#bib.bib181)) 提出了一个框架，该框架包含前馈和反向传播阶段，用于 3D 多人姿态和形状估计。前馈过程包括对身体部位的语义分割和基于
    DMHS 的 3D 姿态估计（Popa 等人，[2017](#bib.bib133)）。然后，反向传播过程对 SMPL（Loper 等人，[2015](#bib.bib96)）的姿态和形状参数进行精细化。Mehta
    等人 ([2019](#bib.bib105)) 通过三个阶段实时估计多个姿态。首先，SelecSLS Net 推断 2D 姿态和可见身体关节的中间 3D
    姿态编码。然后，基于每个检测到的人员，重建完整的 3D 姿态，包括遮挡关节。最后，提供时间稳定性和运动骨架拟合的精细化。
- en: 5 Datasets and evaluation protocols
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 数据集和评估协议
- en: Datasets play an important role in deep learning-baed human pose estimation.
    Datasets not only are essential for fair comparison of different algorithms but
    also bring more challenges and complexity through their expansion and improvement.
    With the maturity of the commercial motion capture systems and crowdsourcing services,
    recent datasets are no longer limited by the data quantity or lab environments.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在基于深度学习的人体姿态估计中扮演着重要角色。数据集不仅对不同算法的公平比较至关重要，而且通过扩展和改进带来了更多挑战和复杂性。随着商业运动捕捉系统和众包服务的成熟，最近的数据集不再受数据量或实验室环境的限制。
- en: This section discusses the popular publicly available human pose datasets for
    2D and 3D human pose estimation, introduces the characteristics and the evaluation
    methods, as well as the performance of recent state-of-the-art work on several
    popular datasets. In addition to these basic datasets, some researchers have extended
    the existing datasets in their own way (Pavlakos et al., [2018a](#bib.bib124);
    Lassner et al., [2017](#bib.bib79)). In addition, some relevant human datasets
    are also within the scope of this section (Güler et al., [2018](#bib.bib49)).
    A brief description of how researchers collected all the annotated images of each
    dataset is also provided to bring inspiration to readers who want to generate
    their own datasets.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了用于2D和3D人体姿态估计的流行公开数据集，介绍了这些数据集的特点、评估方法以及近期在多个流行数据集上最先进工作的表现。除了这些基础数据集外，一些研究人员以自己的方式扩展了现有的数据集（Pavlakos等，[2018a](#bib.bib124)；Lassner等，[2017](#bib.bib79)）。此外，一些相关的人体数据集也包含在本节范围内（Güler等，[2018](#bib.bib49)）。还提供了研究人员如何收集每个数据集的所有标注图像的简要描述，以激发希望生成自己数据集的读者的灵感。
- en: 5.1 Datasets for 2D human pose estimation
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 2D人体姿态估计的数据集
- en: Before deep learning brings significant progress for 2D HPE, there are many
    2D human pose datasets for specific scenarios and tasks. Upper body pose datasets
    include Buffy Stickmen (Ferrari et al., [2008](#bib.bib41)) (frontal-facing view,
    from indoor TV show), ETHZ PASCAL Stickmen (Eichner et al., [2009](#bib.bib32))
    (frontal-facing view, from PASCAL VOC (Everingham et al., [2010](#bib.bib34))),
    We Are Family (Eichner and Ferrari, [2010](#bib.bib29)) (Group photo scenario),
    Video Pose 2 (Sapp et al., [2011](#bib.bib144)) (from indoor TV show), Sync. Activities
    (Eichner and Ferrari, [2012b](#bib.bib31)) (sports, full-body image, upper body
    annotation). full-body pose datasets include PASCAL Person Layout (Everingham
    et al., [2010](#bib.bib34)) (daily scene, from PASCAL VOC (Everingham et al.,
    [2010](#bib.bib34))), Sport (Wang et al., [2011](#bib.bib173)) (sport scenes)
    and UIUC people (Li and Fei-fei, [2007](#bib.bib86)) (sport scenes). For detailed
    description of these datasets, we refer interested readers to several well-summarized
    papers (Andriluka et al., [2014](#bib.bib4)) and (Gong et al., [2016](#bib.bib47)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习为2D HPE带来显著进展之前，已经存在许多用于特定场景和任务的2D人体姿态数据集。上半身姿态数据集包括Buffy Stickmen（Ferrari等，[2008](#bib.bib41)）（正面视图，来自室内电视节目）、ETHZ
    PASCAL Stickmen（Eichner等，[2009](#bib.bib32)）（正面视图，来自PASCAL VOC（Everingham等，[2010](#bib.bib34)））、We
    Are Family（Eichner和Ferrari，[2010](#bib.bib29)）（群体照片场景）、Video Pose 2（Sapp等，[2011](#bib.bib144)）（来自室内电视节目）、Sync.
    Activities（Eichner和Ferrari，[2012b](#bib.bib31)）（体育，全身图像，上半身标注）。全身姿态数据集包括PASCAL
    Person Layout（Everingham等，[2010](#bib.bib34)）（日常场景，来自PASCAL VOC（Everingham等，[2010](#bib.bib34)））、Sport（Wang等，[2011](#bib.bib173)）（运动场景）和UIUC
    people（Li和Fei-fei，[2007](#bib.bib86)）（运动场景）。有关这些数据集的详细描述，我们参考了几篇总结良好的论文（Andriluka等，[2014](#bib.bib4)）和（Gong等，[2016](#bib.bib47)）。
- en: 'Above earlier datasets for 2D human pose estimation have many shortcomings
    such as few scenes, monotonous view angle, lack of diverse activities, and limited
    number of images. The scale is the most important aspect of a dataset for deep
    learning-based methods. Small training sets are insufficient for learning robust
    features, unsuitable for networks with deep layers and complex design, and may
    easily cause overfitting. Thus in this section, we only introduce 2D human pose
    datasets with the number of images for training over 1,000\. The features of these
    selected 2D HPE datasets are summarized in Table [7](#S5.T7 "Table 7 ‣ 5.1 Datasets
    for 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular
    Human Pose Estimation: A Survey of Deep Learning-based Methods") and some sample
    images with annotations are illustrated in Fig. [5](#S5.F5 "Fig. 5 ‣ 5.1 Datasets
    for 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular
    Human Pose Estimation: A Survey of Deep Learning-based Methods").'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '上述早期的2D人体姿态估计数据集存在许多不足之处，例如场景少、视角单一、活动缺乏多样性以及图像数量有限。数据集的规模是深度学习方法中最重要的方面。小型训练集不足以学习到稳健的特征，不适合具有深层和复杂设计的网络，并且可能容易导致过拟合。因此，在本节中，我们仅介绍图像数量超过1,000的2D人体姿态数据集。这些选定的2D
    HPE数据集的特征总结在表[7](#S5.T7 "Table 7 ‣ 5.1 Datasets for 2D human pose estimation ‣
    5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey
    of Deep Learning-based Methods")中，部分带注释的示例图像在图[5](#S5.F5 "Fig. 5 ‣ 5.1 Datasets
    for 2D human pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular
    Human Pose Estimation: A Survey of Deep Learning-based Methods")中展示。'
- en: 'Table 7: Popular 2D databases for human pose estimation. Selected example images
    with annotations are shown in Fig. [5](#S5.F5 "Fig. 5 ‣ 5.1 Datasets for 2D human
    pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation:
    A Survey of Deep Learning-based Methods"). Here Jnt. indicates the number of joints'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '表7：用于人体姿态估计的流行2D数据库。带注释的选定示例图像见图[5](#S5.F5 "Fig. 5 ‣ 5.1 Datasets for 2D human
    pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation:
    A Survey of Deep Learning-based Methods")。这里的Jnt.表示关节的数量。'
- en: '| Dataset | Single/ | Jnt. | Number of images/videos | Evaluation | Highlights
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | Single/ | Jnt. | Number of images/videos | Evaluation | Highlights
    |'
- en: '| name | Multiple | Train | Val | Test | protocol |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| name | Multiple | Train | Val | Test | protocol |'
- en: '| Image-based |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Image-based |'
- en: '| FLIC | single | 10 | $\approx$5k | 0 | $\approx$1k | PCP&PCK | Upper body
    poses; Sampled from movies; FLIC-full is complete version (Sapp and Taskar, [2013](#bib.bib143));
    FLIC-plus is cleaned version (Tompson et al., [2014](#bib.bib164)); FLIC is a
    simple version with no difficult poses. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| FLIC | single | 10 | $\approx$5k | 0 | $\approx$1k | PCP&PCK | 上半身姿态；从电影中抽样；FLIC-full是完整版本（Sapp和Taskar，[2013](#bib.bib143)）；FLIC-plus是清理过的版本（Tompson等，[2014](#bib.bib164)）；FLIC是没有困难姿态的简易版本。
    |'
- en: '| FLIC-full | $\approx$20k | 0 | 0 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| FLIC-full | $\approx$20k | 0 | 0 |'
- en: '| FLIC-plus | $\approx$17k | 0 | 0 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| FLIC-plus | $\approx$17k | 0 | 0 |'
- en: '| LSP | single | 14 | $\approx$1k | 0 | $\approx$1k | PCP | full-body poses;
    From Flickr with 8 sports tags (Johnson and Everingham, [2010](#bib.bib68)); Extended
    by adding most challenging poses lie in 3 tags (Johnson and Everingham, [2011](#bib.bib69)).
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| LSP | single | 14 | $\approx$1k | 0 | $\approx$1k | PCP | 全身姿态；来自Flickr，具有8个运动标签（Johnson和Everingham，[2010](#bib.bib68)）；通过添加最具挑战性的姿态扩展，涵盖了3个标签（Johnson和Everingham，[2011](#bib.bib69)）。
    |'
- en: '| LSP- extended | $\approx$10k | 0 | 0 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| LSP-extended | $\approx$10k | 0 | 0 |'
- en: '| MPII | single | 16 | $\approx$29k | 0 | $\approx$12k | PCPm/PCKh | Various
    body poses; Downloaded videos from YouTube; Multiple annotations (bounding boxes,
    3D viewpoint of the head and torso, position of the eyes and nose, joint locations);
    (Andriluka et al., [2014](#bib.bib4)). |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| MPII | single | 16 | $\approx$29k | 0 | $\approx$12k | PCPm/PCKh | 各种身体姿态；从YouTube下载的视频；多重注释（边界框、头部和躯干的3D视角、眼睛和鼻子的位置、关节位置）；（Andriluka等，[2014](#bib.bib4)）。
    |'
- en: '| multiple | $\approx$3.8k | 0 | $\approx$1.7k | mAP |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| multiple | $\approx$3.8k | 0 | $\approx$1.7k | mAP |'
- en: '| COCO16 | multiple | 17 | $\approx$45k | $\approx$22k | $\approx$80k | AP
    | Various body poses; From Google, Bing and Flickr; Multiple annotations (bounding
    boxes, human body masks, joint locations); With about 120K unlabeled images for
    semi-supervised learning; (Lin et al., [2014](#bib.bib93)) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| COCO16 | multiple | 17 | $\approx$45k | $\approx$22k | $\approx$80k | AP
    | 各种身体姿态；来自Google、Bing和Flickr；多重注释（边界框、人体遮罩、关节位置）；约120K未标记图像用于半监督学习；（Lin等，[2014](#bib.bib93)）
    |'
- en: '| COCO17 | $\approx$64k | $\approx$2.7k | $\approx$40k |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| COCO17 | $\approx$64k | $\approx$2.7k | $\approx$40k |'
- en: '| AIC- HKD | multiple | 14 | $\approx$210k | $\approx$30k | $\approx$60k |
    AP | Various body poses; From Internet search engines; Multiple annotations (bounding
    boxes, joint locations); (Wu et al., [2017](#bib.bib175)) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| AIC- HKD | 多样 | 14 | $\approx$210k | $\approx$30k | $\approx$60k | AP | 各种体态；来自互联网搜索引擎；多重标注（边界框、关节位置）；（Wu
    et al., [2017](#bib.bib175)） |'
- en: '| Video-based |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 基于视频 |'
- en: '| Penn Action | single | 13 | $\approx$1k | 0 | $\approx$1k | - | full-body
    poses; From YouTube; 15 actions; Multiple annotations (joint locations, bounding
    boxes, action classes) (Zhang et al., [2013](#bib.bib182)). |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Penn Action | 单一 | 13 | $\approx$1k | 0 | $\approx$1k | - | 全身姿势；来自 YouTube；15
    种动作；多重标注（关节位置、边界框、动作类别）（Zhang et al., [2013](#bib.bib182)）。 |'
- en: '| J-HMDB | single | 15 | $\approx$0.6k | 0 | $\approx$0.3k | - | full-body
    poses; Generated from action recognition dataset; 21 actions; Multiple annotations
    (joint positions and relations, optical flows, segmentation masks) (Jhuang et al.,
    [2013](#bib.bib65)). |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| J-HMDB | 单一 | 15 | $\approx$0.6k | 0 | $\approx$0.3k | - | 全身姿势；从动作识别数据集生成；21
    种动作；多重标注（关节位置和关系、光流、分割掩码）（Jhuang et al., [2013](#bib.bib65)）。 |'
- en: '| PoseTrack | multiple | 15 | 292 | 50 | 208 | mAP | Various body poses; Extended
    from MPII; Dense annotations (joint locations, head bounding boxes) (Andriluka
    et al., [2018](#bib.bib3)). |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| PoseTrack | 多样 | 15 | 292 | 50 | 208 | mAP | 各种体态；扩展自 MPII；密集标注（关节位置、头部边界框）（Andriluka
    et al., [2018](#bib.bib3)）。 |'
- en: '![Refer to caption](img/4882275d5c51cc62df66e16ce190df6a.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4882275d5c51cc62df66e16ce190df6a.png)'
- en: 'Fig. 5: Some selected example images with annotations from typical 2D human
    pose estimation datasets.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 一些从典型 2D 人体姿势估计数据集中选出的带注释的示例图像。'
- en: Frames Labeled In Cinema (FLIC) Dataset (Sapp and Taskar, [2013](#bib.bib143))
    contains $5,003$ images collected from popular Hollywood movies. For every tenth
    frame of 30 movies, a person detector (Bourdev and Malik, [2009](#bib.bib11))
    was run to obtain about 20K person candidates. Then all candidates are sent to
    Amazon Mechanical Turk to obtain ground truth labeling for 10 upper body joints.
    Finally, images with person occluded or severely non-frontal views are manually
    deleted. The undeleted original set called FLIC-full consisting of occluded, non-frontal,
    or just plain mislabeled examples ($20,928$ examples) is also available. Moreover,
    in (Tompson et al., [2014](#bib.bib164)), the FLIC-full dataset is further cleaned
    to FLIC-plus to make sure that the training subset does not include any images
    from the same scene as the test subset.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Frames Labeled In Cinema (FLIC) 数据集（Sapp 和 Taskar，[2013](#bib.bib143)）包含 $5,003$
    张从流行好莱坞电影中收集的图像。对 30 部电影的每第十帧，运行了一个人检测器（Bourdev 和 Malik，[2009](#bib.bib11)），以获得约
    20K 人物候选。然后将所有候选发送到 Amazon Mechanical Turk 以获取 10 个上半身关节的真实标注。最后，手动删除了人物被遮挡或严重非正面的视图的图像。未删除的原始集合称为
    FLIC-full，其中包含遮挡、非正面或明显标注错误的样本（$20,928$ 个样本）也可用。此外，在 (Tompson et al., [2014](#bib.bib164))
    中，FLIC-full 数据集进一步清理为 FLIC-plus，以确保训练子集不包含与测试子集相同场景的任何图像。
- en: Leeds Sports Pose (LSP) Dataset (Johnson and Everingham, [2010](#bib.bib68))
    contains $2,000$ images of full-body poses collected from Flickr by downloading
    with 8 sports tags (athletics, badminton, baseball, gymnastics, parkour, soccer,
    tennis, and volleyball). Each image is annotated with up to 14 visible joint locations.
    Further, the extension version Leeds Sports Pose Extended (LSP-extended) training
    dataset (Johnson and Everingham, [2011](#bib.bib69)) is gathered to extend the
    LSP dataset only for training. It contains $10,000$ images collected from Flickr
    searches with 3 most challenging tags (parkour, gymnastics, and athletics). The
    annotations were conducted through Amazon Mechanical Turk and the accuracy cannot
    be guaranteed.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Leeds Sports Pose (LSP) 数据集（Johnson 和 Everingham，[2010](#bib.bib68)）包含 $2,000$
    张全身姿势图像，这些图像通过下载 8 个运动标签（田径、羽毛球、棒球、体操、跑酷、足球、网球和排球）从 Flickr 收集而来。每张图像标注了最多 14 个可见的关节位置。此外，扩展版本
    Leeds Sports Pose Extended (LSP-extended) 训练数据集（Johnson 和 Everingham，[2011](#bib.bib69)）是为扩展
    LSP 数据集而专门收集的，仅用于训练。它包含 $10,000$ 张图像，这些图像从 Flickr 搜索中收集，带有 3 个最具挑战性的标签（跑酷、体操和田径）。这些标注通过
    Amazon Mechanical Turk 完成，准确性不能保证。
- en: Max Planck Institute for Informatics (MPII) Human Pose Dataset (Andriluka et al.,
    [2014](#bib.bib4)) is one of current the state-of-the-art benchmarks for evaluation
    of articulated human pose estimation with rich annotations. First, with guidance
    from a two-level hierarchy of human activities from (Ainsworth et al., [2011](#bib.bib2)),
    $3,913$ videos spanning $491$ different activities are downloaded from YouTube.
    Then frames that either contains different people in the video or the same person
    in a very different pose were manually selected which results in $24,920$ frames.
    Rich annotations including 16 body joints, the 3D viewpoint of the head and torso
    and position of the eyes and nose are labeled by in-house workers and on Amazon
    Mechanical Turk. For corresponding joints, visibility and left/right labels are
    also annotated in a person-centric way. Images in MPII have various body poses
    and are suitable for many tasks such as 2D single/multiple human pose estimation,
    action recognition, etc.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 马克斯·普朗克信息学研究所（MPII）人体姿态数据集（Andriluka 等，[2014](#bib.bib4)）是当前用于评估关节人体姿态估计的最先进基准之一，具有丰富的注释。首先，在（Ainsworth
    等，[2011](#bib.bib2)）提出的两级人类活动层次结构的指导下，从 YouTube 下载了涵盖 $491$ 种不同活动的 $3,913$ 个视频。然后，手动选择了包含视频中不同人物或同一人物在非常不同姿态下的帧，共得到
    $24,920$ 帧。由内部工作人员和在亚马逊 Mechanical Turk 上的工人标注了丰富的注释，包括 $16$ 个身体关节、头部和躯干的 3D 视角以及眼睛和鼻子的位置信息。对于对应的关节，还以以人为中心的方式注释了可见性和左右标签。MPII
    中的图像具有各种身体姿态，适用于许多任务，如 2D 单人/多人姿态估计、动作识别等。
- en: 'Microsoft Common Objects in Context (COCO) Dataset (Lin et al., [2014](#bib.bib93))
    is a large-scale dataset that was originally proposed for daily object detection
    and segmentation in natural environments. With improvements and extensions, the
    usage of COCO covers image captioning and keypoint detection. Images are collected
    from Google, Bing, and Flickr image search with isolated or pairwise object categories.
    Annotations were conducted on Amazon Mechanical Turk. The whole set contains more
    than $200,000$ images and $250,000$ labeled person instances. Suitable examples
    are selected for human pose estimation, thus forming two datasets: COCO keypoints
    2016 and COCO keypoints 2017, corresponding to two public keypoint detection challenges
    respectively. The only difference between these two versions is the train/val/test
    splitting strategy based on community feedback (shown in Table [7](#S5.T7 "Table
    7 ‣ 5.1 Datasets for 2D human pose estimation ‣ 5 Datasets and evaluation protocols
    ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods")),
    and cross-year results can be compared directly since the images in the test set
    are same. The COCO Keypoint Detection Challenge aims to localize keypoints of
    people in uncontrolled images. The annotations for each person include 17 body
    joints with visibility and left/right labels, and instance human body segmentation.
    Note that COCO dataset contains about 120K unlabeled images following the same
    class distribution as the labeled images which can be used for unsupervised or
    semi-supervised learning.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '微软常见物体背景（COCO）数据集（Lin 等，[2014](#bib.bib93)）是一个大规模数据集，最初用于自然环境中的日常物体检测和分割。通过改进和扩展，COCO
    的使用范围涵盖了图像字幕生成和关键点检测。图像来自 Google、Bing 和 Flickr 图像搜索，包含孤立或成对的物体类别。注释工作在亚马逊 Mechanical
    Turk 上完成。整个数据集包含超过 $200,000$ 张图像和 $250,000$ 个标注的人体实例。适当的示例被选择用于人体姿态估计，从而形成两个数据集：COCO
    关键点 2016 和 COCO 关键点 2017，分别对应两个公开的关键点检测挑战。这两个版本之间唯一的区别在于基于社区反馈的训练/验证/测试分割策略（见表
    [7](#S5.T7 "Table 7 ‣ 5.1 Datasets for 2D human pose estimation ‣ 5 Datasets and
    evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods")），由于测试集中的图像相同，可以直接比较跨年度的结果。COCO 关键点检测挑战旨在定位在非受控图像中的人物关键点。每个人的注释包括 $17$
    个身体关节的可见性和左右标签，以及实例人体分割。请注意，COCO 数据集包含大约 120K 张未标记图像，具有与标记图像相同的类别分布，可用于无监督或半监督学习。'
- en: AI Challenger Human Keypoint Detection (AIC-HKD) Dataset (Wu et al., [2017](#bib.bib175))
    has the largest number of training examples. It contains $210,000$, $30,000$,
    $30,000$, and $30,000$ images for training, validation, test A, and test B respectively.
    The images, focusing on the daily life of people, were collected from Internet
    search engines. Then, after removing inappropriate examples (e.g. with the political,
    constabulary, violent and sexual contents; too small or too crowded human figures),
    each person in the images were annotated with a bounding box and 14 keypoints.
    Each keypoint has the visibility and left/right labels.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: AI Challenger 人体关键点检测（AIC-HKD）数据集（Wu et al., [2017](#bib.bib175)）拥有最大数量的训练样本。它包含
    $210,000$、$30,000$、$30,000$ 和 $30,000$ 张用于训练、验证、测试 A 和测试 B 的图像。图像专注于人们的日常生活，收集自互联网搜索引擎。然后，经过去除不适当的样本（例如政治、警察、暴力和色情内容；人类形象过小或过于拥挤），每个图像中的每个人都用一个边界框和
    14 个关键点进行标注。每个关键点都有可见性及左右标签。
- en: In addition to the datasets described above which are in static image style,
    datasets with densely annotated video frames are collected in closer to real-life
    application scenarios which offer the possibility to utilize temporal information
    and can be used for action recognition. Some of them focus on single individuals
    (Zhang et al., [2013](#bib.bib182); Jhuang et al., [2013](#bib.bib65); Charles
    et al., [2016](#bib.bib16)) and others have pose annotations for multiple people
    (Insafutdinov et al., [2017](#bib.bib57); Iqbal et al., [2016](#bib.bib61); Andriluka
    et al., [2018](#bib.bib3)).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述静态图像风格的数据集外，密集标注的视频帧数据集被收集于更接近实际应用场景的环境中，这提供了利用时间信息的可能性，并可用于动作识别。其中一些专注于单个个体（Zhang
    et al., [2013](#bib.bib182); Jhuang et al., [2013](#bib.bib65); Charles et al.,
    [2016](#bib.bib16)），而其他的则为多人提供了姿态标注（Insafutdinov et al., [2017](#bib.bib57); Iqbal
    et al., [2016](#bib.bib61); Andriluka et al., [2018](#bib.bib3)）。
- en: 'Penn Action Dataset (Zhang et al., [2013](#bib.bib182)) consists of $2,326$
    videos downloaded from YouTube covering 15 actions: baseball pitch, baseball swing,
    bench press, bowling, clean and jerk, golf swing, jump rope, jumping jacks, pull
    up, push up, sit up, squat, strum guitar, tennis forehand, and tennis serve. Annotations
    for each frame were labeled by VATIC (Vondrick et al., [2013](#bib.bib170)) (an
    annotation tool) on Amazon Mechanical Turk. Each video involves an action class
    label and each video frame contains a bounding box of human and 13 joint locations
    with the visibility and left/right labels.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Penn Action 数据集（Zhang et al., [2013](#bib.bib182)）包含 $2,326$ 个从 YouTube 下载的视频，涵盖
    15 种动作：棒球投球、棒球挥棒、卧推、保龄球、清洁与挺举、高尔夫挥杆、跳绳、开合跳、引体向上、俯卧撑、仰卧起坐、深蹲、弹吉他、网球正手击球和网球发球。每一帧的标注由
    VATIC（Vondrick et al., [2013](#bib.bib170)）（一个标注工具）在 Amazon Mechanical Turk 上完成。每个视频包括一个动作类别标签，每个视频帧包含一个人类的边界框和
    13 个关节位置的可见性及左右标签。
- en: 'Joint-annotated Human Motion Database (J-HMDB) (Jhuang et al., [2013](#bib.bib65))
    is based on the HMDB51 (Jhuang et al., [2011](#bib.bib66)) which is originally
    collected for action recognition. First, 21 action categories with relatively
    large body movements were selected from original 51 actions in HMDB51, including:
    brush hair, catch, clap, climb stairs, golf, jump, kick ball, pick, pour, pull-up,
    push, run, shoot ball, shoot bow, shoot gun, sit, stand, swing baseball, throw,
    walk, and wave. Then, after a selection-and-cleaning process, $928$ clips comprising
    $31,838$ annotated frames are selected. Finally, a 2D articulated human puppet
    model (Zuffi et al., [2012](#bib.bib187)) is employed to generate all the needed
    annotations using Amazon Mechanical Turk. The 2D puppet model is an articulated
    human body model that provides scale, pose, segmentation, coarse viewpoint, and
    dense optical flow for the humans in actions. The annotations include 15 joint
    positions and relations, 2D optical flow corresponding to the human motion, human
    body segmentation mask. The 70$\%$ images are used for training and the 30$\%$
    images for testing. J-HMDB can also be used for action recognition and human detection
    tasks.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 关节标注的人体动作数据库（J-HMDB）（Jhuang 等，[2013](#bib.bib65)）基于 HMDB51（Jhuang 等，[2011](#bib.bib66)），后者最初是为了动作识别而收集的。首先，从
    HMDB51 的 51 种动作中选择了 21 种具有较大身体运动的动作类别，包括：刷头发、接球、拍手、爬楼梯、高尔夫、跳跃、踢球、拾取、倒水、引体向上、推、跑步、射球、射箭、射击、坐、站、挥棒、投掷、走路和挥手。然后，在选择和清理过程中，选择了
    $928$ 个剪辑，共 $31,838$ 帧已标注的帧。最后，使用 2D 关节化人体木偶模型（Zuffi 等，[2012](#bib.bib187)）生成所有所需的标注，采用
    Amazon Mechanical Turk。2D 木偶模型是一个关节化的人体模型，提供尺度、姿势、分割、粗略视角和密集的光流。标注包括 15 个关节位置和关系、对应于人体运动的
    2D 光流、人体分割掩码。70$\%$ 的图像用于训练，30$\%$ 的图像用于测试。J-HMDB 还可以用于动作识别和人体检测任务。
- en: There are several video datasets annotated with human upper body pose. BBC Pose
    (Charles et al., [2014](#bib.bib15)) contains 20 videos (10/5/5 for train/val/test,
    1.5 million frames in total) with 9 sign language signers. $2,000$ frames for
    validation and test are manually annotated and the rest of the frames are annotated
    with a semi-automatic method. Extended BBC Pose dataset (Pfister et al., [2014](#bib.bib130))
    adds $72$ additional training videos for BBC Pose which has about $7$ million
    frames in total. MPII Cooking (Rohrbach et al., [2012](#bib.bib142)) dataset contains
    $1,071$ frames for training and $1,277$ frames for testing with manually annotated
    joint locations for cooking activities. YouTube Pose dataset (Charles et al.,
    [2016](#bib.bib16)) contains $50$ YouTube videos with single person in. The activities
    cover dancing, stand-up comedy, how-to, sports, disk jockeys, performing arts
    and dancing, and sign language signers. $100$ frames of each video are manually
    annotated with joint locations of the upper body. The scenes of these datasets
    are relatively simple, with static views and the characters are normally in a
    small motion range.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个视频数据集标注了人体上半身姿势。BBC Pose（Charles 等，[2014](#bib.bib15)）包含 20 个视频（10/5/5 用于训练/验证/测试，总计
    150 万帧），其中包括 9 位手语使用者。$2,000$ 帧用于验证和测试是手动标注的，其余帧使用半自动方法标注。扩展 BBC Pose 数据集（Pfister
    等，[2014](#bib.bib130)）为 BBC Pose 增加了 $72$ 个额外的训练视频，总共有约 $7$ 百万帧。MPII Cooking（Rohrbach
    等，[2012](#bib.bib142)）数据集包含 $1,071$ 帧用于训练，$1,277$ 帧用于测试，并对烹饪活动的关节位置进行手动标注。YouTube
    Pose 数据集（Charles 等，[2016](#bib.bib16)）包含 $50$ 个 YouTube 视频，每个视频中只有一个人。活动包括舞蹈、单口相声、教程、运动、DJ、表演艺术和手语使用者。每个视频的
    $100$ 帧手动标注了上半身的关节位置。这些数据集的场景相对简单，视角静态，人物通常在较小的运动范围内。
- en: From unlabeled MPII Human Pose (Andriluka et al., [2014](#bib.bib4)) video data,
    there are several extended versions result in dense annotations of video frames.
    The general approach is to extend the original labeled frame with the connected
    frames both forward and backward and annotate unlabeled frames in the same way
    as the labeled frame. MPII Video Pose dataset (Insafutdinov et al., [2017](#bib.bib57))
    provides 28 videos containing 21 frames each by selecting the challenging labeled
    images and unlabeled neighboring +/-10 frames from the MPII dataset. In Multi-Person
    PoseTrack dataset (Iqbal et al., [2016](#bib.bib61)), each selected labeled frame
    is extended with unlabeled clips ranging +/-20 frames, and each person has a unique
    ID. Also, additional videos of more than 41 frames are provided for longer and
    variable-length sequences. In total, it contains 60 videos with additional videos
    with more than 41 frames for longer and variable-length sequences. PoseTrack dataset
    (Andriluka et al., [2018](#bib.bib3)) is the integrated expansion of the above
    two datasets and is the current largest multi-person pose estimation and tracking
    dataset. Each person in the video has a unique track ID with annotations of a
    head bounding box and 15 body joint locations. All pose annotations are labeled
    with VATIC (Vondrick et al., [2013](#bib.bib170)). PoseTrack contains $550$ video
    sequences with the frames mainly ranging between $41$ and $151$ frames in a wide
    variety of everyday human activities and is divided into $292$, $50$, and $208$
    videos for training, validation, and testing, following original MPII split strategy.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从未标记的 MPII 人体姿态（Andriluka 等，[2014](#bib.bib4)）视频数据中，有几个扩展版本产生了密集的视频帧注释。一般方法是将原始标记帧与前后连接的帧进行扩展，并以与标记帧相同的方式注释未标记的帧。MPII
    视频姿态数据集（Insafutdinov 等，[2017](#bib.bib57)）提供了28个视频，每个视频包含21帧，选取了具有挑战性的标记图像及其未标记的相邻
    +/-10 帧。Multi-Person PoseTrack 数据集（Iqbal 等，[2016](#bib.bib61)）中，每个选定的标记帧扩展为包含
    +/-20 帧的未标记片段，每个人都有唯一的ID。此外，还提供了超过41帧的额外视频，以用于更长和可变长度的序列。总共包含60个视频，并附有超过41帧的额外视频用于更长和可变长度的序列。PoseTrack
    数据集（Andriluka 等，[2018](#bib.bib3)）是上述两个数据集的综合扩展，是当前最大的多人姿态估计和跟踪数据集。视频中的每个人都有一个唯一的跟踪ID，并标注了一个头部边界框和15个身体关节位置。所有姿态注释都使用VATIC（Vondrick
    等，[2013](#bib.bib170)）进行标记。PoseTrack包含$550$个视频序列，帧数主要在$41$到$151$帧之间，涉及各种日常活动，按照原始MPII拆分策略分为$292$、$50$和$208$个视频用于训练、验证和测试。
- en: 5.2 Evaluation Metrics of 2D human pose estimation
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 2D 人体姿态估计的评估指标
- en: 'Different datasets have different features (e.g. various range of human body
    sizes, upper/full human body) and different task requirements (single/multiple
    pose estimation), so there are several evaluation metrics for 2D human pose estimation.
    The summary of different evaluation metrics which are commonly used are listed
    in Table [8](#S5.T8 "Table 8 ‣ 5.2 Evaluation Metrics of 2D human pose estimation
    ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey
    of Deep Learning-based Methods").'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '不同数据集具有不同的特征（例如，各种范围的人体尺寸，上半身/全身）以及不同的任务要求（单人/多人姿态估计），因此存在多种2D人体姿态估计的评估指标。常用的不同评估指标的总结列在表[8](#S5.T8
    "Table 8 ‣ 5.2 Evaluation Metrics of 2D human pose estimation ‣ 5 Datasets and
    evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods")中。'
- en: 'Table 8: Summary of commonly used evaluation metrics for 2D HPE.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 常用2D HPE评估指标的总结。'
- en: '| Metric | Meaning | Typical datasets and Description |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 含义 | 典型数据集和描述 |'
- en: '| Single person |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 单人 |'
- en: '| PCP |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| PCP |'
- en: '&#124; Percentage &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 百分比 &#124;'
- en: '&#124; of Correct &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确的 &#124;'
- en: '&#124; Parts &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 部件 &#124;'
- en: '| LSP |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| LSP |'
- en: '&#124; Percentage of correct predicted Parts which their end points fall within
    a threshold &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预测的部件中其端点落在阈值范围内的正确比例 &#124;'
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PCK |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| PCK |'
- en: '&#124; Percentage &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 百分比 &#124;'
- en: '&#124; of Correct &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确的 &#124;'
- en: '&#124; Keypoints &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关键点 &#124;'
- en: '|'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LSP &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LSP &#124;'
- en: '&#124; MPII &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPII &#124;'
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Percentage of correct predicted joints which fall within a threshold
    &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确预测的关节中落在阈值范围内的百分比 &#124;'
- en: '|'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Multiple person |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 多人 |'
- en: '| AP | Average Precision |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| AP | 平均精度 |'
- en: '&#124; MPII &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPII &#124;'
- en: '&#124; PoseTrack &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PoseTrack &#124;'
- en: '|'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; mean AP (mAP) is reported by AP for each body part after assigning predicted
    &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均精度（mAP）通过每个身体部件的AP来报告，分配预测后 &#124;'
- en: '&#124; pose to the ground truth pose by PCKh score. &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过PCKh评分将姿势与真实姿势对齐。 &#124;'
- en: '|'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| COCO | $\bullet$  $AP_{coco}$: at OKS=.50:.05:.95 (primary metric) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| COCO | $\bullet$  $AP_{coco}$: 在 OKS=.50:.05:.95 下（主要指标） |'
- en: '| $\bullet$  $AP^{OKS=.50}_{coco}$: at OKS=.50 (loose metric) |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$  $AP^{OKS=.50}_{coco}$: 在 OKS=.50 下（宽松指标） |'
- en: '| $\bullet$  $AP^{OKS=.75}_{coco}$: at OKS=.75 (strict metric) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$  $AP^{OKS=.75}_{coco}$: 在 OKS=.75 下（严格指标） |'
- en: '| $\bullet$  $AP^{medium}_{coco}$: for medium objects: $32^{2}$  $<$ area $<$  $96^{2}$
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$  $AP^{medium}_{coco}$: 对于中等大小的物体：$32^{2}$  $<$ 面积 $<$  $96^{2}$
    |'
- en: '| $\bullet$  $AP^{large}_{coco}$: for large objects: area $>$  $96^{2}$ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$  $AP^{large}_{coco}$: 对于大物体：面积 $>$  $96^{2}$ |'
- en: '| AR | Average Recall | COCO | $\bullet$  $AR_{coco}$: at OKS=.50:.05:.95 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| AR | 平均召回率 | COCO | $\bullet$  $AR_{coco}$: 在 OKS=.50:.05:.95 下 |'
- en: '| $\bullet$  $AR^{OKS=.50}_{coco}$: at OKS=.50 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$  $AR^{OKS=.50}_{coco}$: 在 OKS=.50 下 |'
- en: '| $\bullet$  $AR^{OKS=.75}_{coco}$: at OKS=.75 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$  $AR^{OKS=.75}_{coco}$: 在 OKS=.75 下 |'
- en: '| $\bullet$  $AR^{medium}_{coco}$: for medium objects: $32^{2}$  $<$ area $<$  $96^{2}$
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$  $AR^{medium}_{coco}$: 对于中等大小的物体：$32^{2}$  $<$ 面积 $<$  $96^{2}$
    |'
- en: '| $\bullet$  $AR^{large}_{coco}$: for large objects: area $>$  $96^{2}$ |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$  $AR^{large}_{coco}$: 对于大物体：面积 $>$  $96^{2}$ |'
- en: '| OKS |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| OKS |'
- en: '&#124; Object &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对象 &#124;'
- en: '&#124; Keypoint &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关键点 &#124;'
- en: '&#124; Similarity &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相似度 &#124;'
- en: '| COCO |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| COCO |'
- en: '&#124; A similar role as the Intersection over Union (IoU) for AP/AR. &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类似于交并比（IoU）在 AP/AR 中的作用。 &#124;'
- en: '|'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Percentage of Correct Parts (PCP) (Ferrari et al., [2008](#bib.bib41)) is widely
    used in early research. It reports the localization accuracy for limbs. A limb
    is correctly localized if its two endpoints are within a threshold from the corresponding
    ground truth endpoints. The threshold can be $50\%$ of the limb length. Besides
    a mean PCP, some limbs PCP (torso, upper legs, lower legs, upper arms, forearms,
    head) normally are also reported (Johnson and Everingham, [2010](#bib.bib68)).
    And percentage curves for each limb can be obtained with the variation of threshold
    in the metric (Gkioxari et al., [2013](#bib.bib43)). The similar metrics PCPm
    from (Andriluka et al., [2014](#bib.bib4)) use $50\%$ of the mean ground-truth
    segment length over the entire test set as a matching threshold.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 正确部件百分比（PCP）（Ferrari 等，[2008](#bib.bib41)）在早期研究中被广泛使用。它报告了肢体的定位准确性。如果肢体的两个端点在一个阈值范围内接近相应的真实端点，则该肢体被认为定位正确。阈值可以是肢体长度的
    $50\%$。除了均值 PCP，一些肢体 PCP（躯干、上腿、下腿、上臂、前臂、头部）通常也会报告（Johnson 和 Everingham，[2010](#bib.bib68)）。每个肢体的百分比曲线可以通过阈值在指标中的变化获得（Gkioxari
    等，[2013](#bib.bib43)）。类似的指标 PCPm 来自（Andriluka 等，[2014](#bib.bib4)），使用整个测试集上平均真实段长度的
    $50\%$ 作为匹配阈值。
- en: Percentage of Correct Keypoints (PCK) (Yang and Ramanan, [2013](#bib.bib180))
    measures the accuracy of the localization of the body joints. A candidate body
    joint is considered as correct if it falls within the threshold pixels of the
    ground-truth joint. The threshold can be a fraction of the person bounding box
    size (Yang and Ramanan, [2013](#bib.bib180)), pixel radius that normalized by
    the torso height of each test sample (Sapp and Taskar, [2013](#bib.bib143)) (denoted
    as Percent of Detected Joints (PDJ) in (Toshev and Szegedy, [2014](#bib.bib165))),
    $50\%$ of the head segment length of each test image (denoted as PCKh@0.5 in (Andriluka
    et al., [2014](#bib.bib4))). Also, with the variation of a threshold, Area Under
    the Curve (AUC) can be generated for further analysis.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正确关键点百分比（PCK）（Yang 和 Ramanan，[2013](#bib.bib180)）衡量身体关节定位的准确性。如果候选身体关节落在真实关节的阈值像素范围内，则认为其正确。阈值可以是人体边框框大小的一部分（Yang
    和 Ramanan，[2013](#bib.bib180)）、每个测试样本的躯干高度标准化的像素半径（Sapp 和 Taskar，[2013](#bib.bib143)）（在（Toshev
    和 Szegedy，[2014](#bib.bib165)）中表示为检测到的关节百分比（PDJ））、每张测试图像头部段长度的 $50\%$（在（Andriluka
    等，[2014](#bib.bib4)）中表示为 PCKh@0.5）。此外，通过阈值的变化，可以生成曲线下面积（AUC）以便进一步分析。
- en: The Average Precision (AP). For systems in which there are only joint locations
    but no annotated bounding boxes for human bodies/heads or number of people in
    the image as ground truth at testing, the detection problem must be addressed
    as well. Similar to object detection, an Average Precision (AP) evaluation method
    is proposed, which is first called Average Precision of Keypoints (APK) in (Yang
    and Ramanan, [2013](#bib.bib180)). In AP measure, if a predicted joint falls within
    a threshold of the ground-truth joint location, it is counted as a true positive.
    Note that correspondence between candidates and ground-truth poses are established
    separately for each keypoint. For multi-person pose evaluation, all predicted
    poses are assigned to the ground truth poses one by one based on the PCKh score
    order, while unassigned predictions are counted as false positives (Pishchulin
    et al., [2016](#bib.bib131)). The mean average precision (mAP) is reported from
    the AP of each body joint.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度（AP）。对于只有关节点但没有注释的边界框用于人类身体/头部或图像中人数作为测试时的真实值的系统，检测问题也必须得到解决。类似于目标检测，提出了一种平均精度（AP）评估方法，在（Yang
    和 Ramanan，[2013](#bib.bib180)）中首次称为关键点的平均精度（APK）。在 AP 测量中，如果预测的关节落在真实关节位置的阈值范围内，则被计为真正例。注意，候选者和真实姿势之间的对应关系是针对每个关键点单独建立的。对于多人姿态评估，所有预测姿态根据
    PCKh 分数顺序逐一分配给真实姿态，而未分配的预测被计为假正例（Pishchulin 等，[2016](#bib.bib131)）。从每个身体关节的 AP
    中报告平均精度（mAP）。
- en: 'Average Precision (AP), Average Recall (AR) and their variants. In (Lin et al.,
    [2014](#bib.bib93)), evaluating multi-person pose estimation results as an object
    detection problem is further designed. AP, AR, and their variants are reported
    based on an analogous similarity measure: object keypoint similarity (OKS) which
    plays the same role as the Intersection over Union (IoU). Additional, AP/AR with
    different human body scales are also reported in COCO dataset. Table [8](#S5.T8
    "Table 8 ‣ 5.2 Evaluation Metrics of 2D human pose estimation ‣ 5 Datasets and
    evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") summarizes all above evaluation metrics.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '平均精度（AP）、平均召回率（AR）及其变体。在（Lin 等，[2014](#bib.bib93)）中，将多人人体姿态估计结果作为目标检测问题进行评估的方案进一步设计。AP、AR
    及其变体是基于类似的相似度度量报告的：目标关键点相似度（OKS），它与交并比（IoU）具有相同的作用。此外，COCO 数据集中也报告了具有不同人体尺度的 AP/AR。表
    [8](#S5.T8 "Table 8 ‣ 5.2 Evaluation Metrics of 2D human pose estimation ‣ 5 Datasets
    and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") 总结了上述所有评估指标。'
- en: Frame Rate, Number of Weights and Giga Floating-point Operations Per Second
    (GFLOPs). The computational performance metrics are also very important for HPE.
    Frame Rate indicates the processing speed of input data, generally expressed by
    Frames Per Second (FPS) or seconds per image (s/image) (Cao et al., [2016](#bib.bib13)).
    Number of Weights and GFLOPs show the efficiency of the network, mainly related
    to the network design and the specific used GPUs/CPUs (Sun et al., [2019](#bib.bib151)).
    These computational performance metrics are suitable for 3D HPE as well.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 帧率、权重数量和每秒吉浮点运算数（GFLOPs）。计算性能指标对于 HPE 也非常重要。帧率表示输入数据的处理速度，通常以每秒帧数（FPS）或每张图像秒数（s/image）表示（Cao
    等，[2016](#bib.bib13)）。权重数量和 GFLOPs 显示网络的效率，主要与网络设计和具体使用的 GPUs/CPUs 相关（Sun 等，[2019](#bib.bib151)）。这些计算性能指标同样适用于
    3D HPE。
- en: 5.3 Datasets for 3D human pose estimation
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3D 人体姿态估计的数据集
- en: 'For a better understanding of the human body in 3D space, there are many kinds
    of body representations with different modern equipment. 3D human body shape scans,
    such as SCAPE (Anguelov et al., [2005](#bib.bib5)), INRIA4D (INRIA4D, [accessed
    on 2019](#bib.bib56)) and FAUST (Bogo et al., [2014](#bib.bib9), [2017](#bib.bib10)),
    3D human body surface cloud points with time of flight (TOF) depth sensors (Shahroudy
    et al., [2016](#bib.bib146)), 3D human body reflective markers capture with motion
    capture systems (MoCap) (Sigal et al., [2010](#bib.bib149); Ionescu et al., [2014](#bib.bib59)),
    orientation and acceleration of 3D human body data with Inertial Measurement Unit
    (IMU) (von Marcard et al., [2016](#bib.bib102), [2018](#bib.bib101)). It is difficult
    to summarize them all, this paper summarizes the datasets that involve RGB images
    and 3D joint coordinates. The details of the selected 3D datasets are summarized
    in Table [9](#S5.T9 "Table 9 ‣ 5.3 Datasets for 3D human pose estimation ‣ 5 Datasets
    and evaluation protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based
    Methods") and some example images with annotations are shown in Fig. [6](#S5.F6
    "Fig. 6 ‣ 5.3 Datasets for 3D human pose estimation ‣ 5 Datasets and evaluation
    protocols ‣ Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods").'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 3D 空间中的人体，有多种不同的现代设备提供了各种身体表示。3D 人体形状扫描，如 SCAPE（Anguelov 等，[2005](#bib.bib5)），INRIA4D（INRIA4D，[2019
    年访问](#bib.bib56)）和 FAUST（Bogo 等，[2014](#bib.bib9)，[2017](#bib.bib10)），使用时间飞行（TOF）深度传感器的
    3D 人体表面点云（Shahroudy 等，[2016](#bib.bib146)），使用运动捕捉系统（MoCap）捕捉的 3D 人体反射标记（Sigal
    等，[2010](#bib.bib149)；Ionescu 等，[2014](#bib.bib59)），使用惯性测量单元（IMU）的 3D 人体数据的方向和加速度（von
    Marcard 等，[2016](#bib.bib102)，[2018](#bib.bib101)）。很难总结所有这些，本文总结了涉及 RGB 图像和 3D
    关节坐标的数据集。选定的 3D 数据集的详细信息总结在表 [9](#S5.T9 "表 9 ‣ 5.3 3D 人体姿态估计的数据集 ‣ 5 数据集和评估协议
    ‣ 单目人体姿态估计：深度学习方法综述")，一些带注释的示例图像显示在图 [6](#S5.F6 "图 6 ‣ 5.3 3D 人体姿态估计的数据集 ‣ 5 数据集和评估协议
    ‣ 单目人体姿态估计：深度学习方法综述")。
- en: 'Table 9: Popular databases for 3D human pose estimation. Selected example images
    with annotations are shown in Fig. [6](#S5.F6 "Fig. 6 ‣ 5.3 Datasets for 3D human
    pose estimation ‣ 5 Datasets and evaluation protocols ‣ Monocular Human Pose Estimation:
    A Survey of Deep Learning-based Methods") (Cams. indicates the number of cameras;
    Jnt. indicates the number of joints)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 3D 人体姿态估计的流行数据库。带注释的示例图像显示在图 [6](#S5.F6 "图 6 ‣ 5.3 3D 人体姿态估计的数据集 ‣ 5 数据集和评估协议
    ‣ 单目人体姿态估计：深度学习方法综述")（Cams. 表示相机数量；Jnt. 表示关节数量）'
- en: '| Dataset | Cams. | Jnt. | Number of frames/videos | Evaluation | Highlights
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 相机数量 | 关节数量 | 帧/视频数量 | 评估 | 亮点 |'
- en: '| name | Train | Val | Test | protocol |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 训练 | 验证 | 测试 | 协议 |'
- en: '| Single person |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 单人 |'
- en: '| HumanEva-I | 7 | 15 | $\approx$6.8k | $\approx$6.8k | $\approx$24k |     MPJPE
    | 4/2 (I/II) subjects, 6/1 (I/II) actions, Vicon data, indoor environment. (Sigal
    et al., [2010](#bib.bib149)) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| HumanEva-I | 7 | 15 | $\approx$6.8k | $\approx$6.8k | $\approx$24k | MPJPE
    | 4/2 (I/II) 受试者，6/1 (I/II) 动作，Vicon 数据，室内环境。（Sigal 等，[2010](#bib.bib149)） |'
- en: '| HumanEva-II | 4 | 0 | 0 | $\approx$2.5k |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| HumanEva-II | 4 | 0 | 0 | $\approx$2.5k |'
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Human3.6M &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Human3.6M &#124;'
- en: '| 4 | 17 | $\approx$1.5M | $\approx$0.6M | $\approx$1.5M | MPJPE | 11 subjects,
    17 actions, Vicon data, multi-annotation (3D joints, person bounding boxes, depth
    data, 3D body scans), indoor environment. (Ionescu et al., [2014](#bib.bib59))
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 17 | $\approx$1.5M | $\approx$0.6M | $\approx$1.5M | MPJPE | 11 个受试者，17
    个动作，Vicon 数据，多重注释（3D 关节、人物边界框、深度数据、3D 身体扫描），室内环境。（Ionescu 等，[2014](#bib.bib59)）
    |'
- en: '|'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TNT15 &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TNT15 &#124;'
- en: '| 8 | 15 | $\approx$13k | HumanEva | 4 subjects, 5 actions, IMU data, 3D body
    scans, indoor environment. (von Marcard et al., [2016](#bib.bib102)) |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 15 | $\approx$13k | HumanEva | 4 个受试者，5 个动作，IMU 数据，3D 身体扫描，室内环境。（von
    Marcard 等，[2016](#bib.bib102)） |'
- en: '|'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MPI-INF-3DHP &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPI-INF-3DHP &#124;'
- en: '| 14 | 15 | $\approx$1.3M | 3DPCK | 8 subjects, 8 actions, commercial markerless
    system, indoor and outdoor scenes. (Mehta et al., [2017a](#bib.bib104)) |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 15 | $\approx$1.3M | 3DPCK | 8 个受试者，8 个动作，商业无标记系统，室内和室外场景。（Mehta 等，[2017a](#bib.bib104)）
    |'
- en: '|'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TotalCapture &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TotalCapture &#124;'
- en: '| 8 | 26 | $\approx$1.9M | MPJPE | 5 subjects, 5 actions, IMU and Vicon data,
    indoors environment. (Trumble et al., [2017](#bib.bib166)) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 26 | $\approx$1.9M | MPJPE | 5 个受试者，5 个动作，IMU 和 Vicon 数据，室内环境。（Trumble
    等，[2017](#bib.bib166)） |'
- en: '| Multiple person |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 多人 |'
- en: '|'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Panoptic &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Panoptic &#124;'
- en: '| 521 | 15 | 65 videos (5.5 hours) | 3DPCK | up to 8 subjects in each video,
    social interactions, markerless studio, multi-annotation (3D joints, cloud points,
    optical flow), indoors environment. (Joo et al., [2017](#bib.bib70)) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 521 | 15 | 65 个视频（5.5 小时） | 3DPCK | 每个视频最多 8 个受试者、社交互动、无标记的工作室、多重标注（3D 关节、点云、光流）、室内环境。（Joo
    等，[2017](#bib.bib70)） |'
- en: '|'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3DPW &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3DPW &#124;'
- en: '| 1 | 18 | 60 videos ($\approx$51k frames) | MPJPE MPJAE | 7 subjects(up to
    2), daily actions, estimated 3D poses from videos and attached IMUs, 3D body scans,
    SMPL model fitting, in the wild. (von Marcard et al., [2018](#bib.bib101)) |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 18 | 60 个视频（约 51k 帧） | MPJPE MPJAE | 7 个受试者（最多 2 个）、日常动作、从视频和附加 IMUs
    估计的 3D 姿势、3D 人体扫描、SMPL 模型拟合，野外环境。（von Marcard 等，[2018](#bib.bib101)） |'
- en: '![Refer to caption](img/baba44e8aafbad2f28302e350efcd36e.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/baba44e8aafbad2f28302e350efcd36e.png)'
- en: 'Fig. 6: Some selected example images with annotations from typical 3D human
    pose estimation datasets.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：来自典型 3D 人体姿势估计数据集的部分选定示例图像及其注释。
- en: HumanEva-I&II Datasets (Sigal et al., [2010](#bib.bib149)). The ground truth
    annotations of both datasets were captured with a commercial MoCap system from
    ViconPeak. The HumanEva-I dataset contains 7-view video sequences (4 grayscales
    and 3 colors) which are synchronized with 3D body poses. There are 4 subjects
    with markers on their bodies performing 6 common actions (e.g. walking, jogging,
    gesturing, throwing and catching a ball, boxing, combo) in an 3m x 2m capture
    area. HumanEva-II is an extension of HumanEva-I dataset for testing, which contains
    2 subjects performing the action combo.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEva-I&II 数据集（Sigal 等，[2010](#bib.bib149)）。两个数据集的地面真实标注是使用 ViconPeak 的商业动作捕捉系统捕获的。HumanEva-I
    数据集包含 7 个视角的视频序列（4 个灰度和 3 个彩色），与 3D 人体姿势同步。数据集包括 4 名身上有标记的受试者在一个 3 米 x 2 米的捕捉区域内执行
    6 项常见动作（例如行走、慢跑、做手势、扔和接球、拳击、组合动作）。HumanEva-II 是 HumanEva-I 数据集的扩展，用于测试，包含 2 名受试者执行组合动作。
- en: 'Human3.6M Dataset (Ionescu et al., [2014](#bib.bib59)) was collected using
    accurate marker-based MoCap systems (Vicon, [accessed on 2019](#bib.bib169)) in
    an indoor laboratory setup with 11 professional actors (5 females and 6 males)
    dressing moderately realistic clothing. It contains 3.6 million 3D human poses
    and corresponding images from 4 different views. The performed $17$ daily activities
    include discussion, smoking, taking photos, talking on the phone, etc. Main capturing
    devices include 4 digital video cameras, 1 time-of-flight sensor, 10 motion cameras
    working synchronously. The capture area is about 4m x 3m. The provided annotations
    include 3D joint positions, joint angles, person bounding boxes, and 3D laser
    scans of each actor. For evaluation, there are three protocols with different
    training and testing data splits (protocol #1, protocol #2 and protocol #3.)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 'Human3.6M 数据集（Ionescu 等，[2014](#bib.bib59)）使用准确的基于标记的动作捕捉系统（Vicon，[访问于 2019](#bib.bib169)）在室内实验室环境中收集，参与者为
    11 名专业演员（5 名女性和 6 名男性），穿着中等真实感的服装。该数据集包含 360 万个 3D 人体姿势和来自 4 个不同视角的对应图像。执行的 $17$
    项日常活动包括讨论、吸烟、拍照、打电话等。主要捕捉设备包括 4 台数字视频摄像机、1 台飞行时间传感器、10 台同步工作的动作摄像机。捕捉区域约为 4 米
    x 3 米。提供的标注包括 3D 关节位置、关节角度、人员边界框以及每个演员的 3D 激光扫描。评估包括三个具有不同训练和测试数据划分的协议（协议 #1、协议
    #2 和协议 #3）。'
- en: TNT15 Dataset (von Marcard et al., [2016](#bib.bib102)) consists of synchronized
    data streams from $8$ RGB-cameras and $10$ IMUs. It has been recorded in an office
    environment. The dataset records 4 actors performing five activities (e.g. walking,
    running on the spot, rotating arms, jumping and skiing exercises, dynamic punching.)
    and contains about $13$k frames including binary segmented images obtained by
    background subtraction, 3D laser scans and registered meshes of each actor.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: TNT15 数据集（von Marcard 等，[2016](#bib.bib102)）包含来自 $8$ 台 RGB 摄像机和 $10$ 台 IMUs
    的同步数据流。它在一个办公环境中记录。该数据集记录了 4 名演员执行五项活动（例如行走、原地跑步、旋转手臂、跳跃和滑雪练习、动态拳击），包含约 $13$k
    帧，包括通过背景减除获得的二值分割图像、3D 激光扫描以及每个演员的配准网格。
- en: MPI-INF-3DHP (Mehta et al., [2017a](#bib.bib104)) was collected with a markerless
    multi-camera MoCap system (TheCaptury, [accessed on 2019](#bib.bib161)) in both
    indoor and outdoor scenes. It contains over $1.3$M frames from $14$ different
    views. Eight subjects (4 females and 4 males) are recorded performing $8$ activities
    (e.g. walking/standing, exercise, sitting, crouch/reach, on the floor, sports,
    miscellaneous.)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: MPI-INF-3DHP（Mehta 等，[2017a](#bib.bib104)）是使用无标记的多摄像头 MoCap 系统（TheCaptury，[访问于
    2019](#bib.bib161)）在室内和室外场景中收集的。数据集包含来自 $14$ 个不同视角的超过 $1.3$M 帧。记录了 $8$ 名受试者（4
    名女性和 4 名男性）执行 $8$ 种活动（例如步行/站立、锻炼、坐着、蹲下/伸手、地板上的动作、运动、杂项）。
- en: 'TotalCapture Dataset (Trumble et al., [2017](#bib.bib166)) was captured in
    indoors with space measuring roughly 8m x 4m with $8$ calibrated HD video cameras
    at a frame rate of 60Hz. There are $4$ male and $1$ female subjects each performing
    four diverse performances, repeated 3 times: Range Of Motion (ROM), Walking, Acting,
    and Freestyle. There is a total of $1,892,176$ frames of synchronized video, IMU
    and Vicon data. The variation and body motions contained in particular within
    the acting and freestyle sequences are very challenging with actions such as yoga,
    giving directions, bending over and crawling performed in both the train and test
    data.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: TotalCapture 数据集（Trumble 等，[2017](#bib.bib166)）是在一个大约 8m x 4m 的室内空间中捕获的，使用了
    $8$ 台校准过的 HD 视频相机，帧率为 60Hz。数据集中包含 $4$ 名男性和 $1$ 名女性受试者，每人表演四种不同的动作，共重复 3 次：运动范围（ROM）、步态、表演和自由风格。数据总量为
    $1,892,176$ 帧同步视频、IMU 和 Vicon 数据。特别是在表演和自由风格序列中，包含的变换和身体动作非常具有挑战性，例如瑜伽、指引方向、弯腰和爬行等动作在训练和测试数据中都有出现。
- en: MARCOnI Dataset (Elhayek et al., [2017](#bib.bib33)) is a test dataset containing
    sequences in a variety of uncontrolled indoor and outdoor scenarios. The sequences
    vary according to different data modalities captured (multiple videos, video +
    marker positions), in the numbers and identities of actors to track, the complexity
    of the motions, the number of cameras used, the existence and number of moving
    objects in the background, and the lighting conditions (i.e. some body parts lit
    and some in shadow). Cameras differ in the types (from cell phones to vision cameras),
    the frame resolutions, and the frame rates.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: MARCOnI 数据集（Elhayek 等，[2017](#bib.bib33)）是一个测试数据集，包含各种未受控的室内和室外场景序列。这些序列根据不同的数据模态（多个视频、视频
    + 标记位置）、跟踪演员的数量和身份、动作的复杂性、使用的相机数量、背景中移动物体的存在和数量以及光照条件（例如某些身体部位被照亮而某些则在阴影中）而有所不同。相机类型各异（从手机到专业相机），帧分辨率和帧率也不同。
- en: Panoptic Dataset (Joo et al., [2017](#bib.bib70)) was captured with a markerless
    motion capturing using multiple view systems which contains $480$ VGA camera views,
    $31$ HD views, $10$ RGB-D sensors and hardware-based synchronized system. It contains
    $65$ sequences (5.5 hours) of social interaction with $1.5$ millions of 3D skeletons.
    The annotations include 3D keypoints, cloud points, optical flow, etc.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Panoptic 数据集（Joo 等，[2017](#bib.bib70)）是通过无标记的运动捕捉技术和多个视角系统捕获的，包含 $480$ 个 VGA
    相机视角、$31$ 个 HD 视角、$10$ 个 RGB-D 传感器和硬件同步系统。它包含 $65$ 个序列（5.5 小时）的社交互动数据，包含 $150$
    万个 3D 骨架点。注释包括 3D 关键点、点云、光流等。
- en: 3DPW Dataset (von Marcard et al., [2018](#bib.bib101)) was captured with a single
    hand-held camera in natural environments. 3D annotations are estimated from IMUs
    attached to subjects’ limbs with proposed method Video Inertial Poser. All subjects
    are provided with 3D scans. The dataset consists of $60$ video sequences (more
    than $51,000$ frames) with daily actions including walking in the city, going
    up-stairs, having coffee or taking the bus.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 3DPW 数据集（von Marcard 等，[2018](#bib.bib101)）是在自然环境中使用单个手持摄像机捕获的。3D 注释是通过附在受试者四肢上的
    IMU 和提议的方法 Video Inertial Poser 估计的。所有受试者都提供了 3D 扫描。数据集包含 $60$ 个视频序列（超过 $51,000$
    帧），包括步行在城市中、上楼梯、喝咖啡或乘坐公交车等日常动作。
- en: In addition to the datasets collected with MoCap systems, there are other approaches
    to create a dataset for 3D human pose estimation. JTA (Joint Track Auto) (Fabbri
    et al., [2018](#bib.bib35)) is a fully synthetic dataset generated from highly
    photorealistic video game Grand Theft Auto V. It contains almost $10M$ annotated
    body poses and over $460,800$ densely annotated frames. In Human3D+ (Chen et al.,
    [2016](#bib.bib19)), the training images are obtained by integrating real background
    images and 3D textured models which generated from SCAPE model (Anguelov et al.,
    [2005](#bib.bib5)) with different texture deformation. The parameters for generating
    basic SCAPE models are captured from a MoCap system, or inferred from human-annotated
    2D poses. SURREAL (Synthetic hUmans foR REAL) (Varol et al., [2017](#bib.bib168))
    contains videos of single synthetic people with real unchanged background. It
    contains annotations of body parts segmentation, depth, optical flow, and surface
    normals. The dataset employs the SMPL body model for generating body poses and
    shapes. LSP-MPII-Ordinal (Pavlakos et al., [2018a](#bib.bib124)) is an extension
    of two 2D human pose datasets (LSP (Johnson and Everingham, [2010](#bib.bib68))
    and MPII (Andriluka et al., [2014](#bib.bib4))) by adding the ordinal depth relation
    for each pair of joints. UP-3D (Lassner et al., [2017](#bib.bib79)) is a combination
    of color images from 2D human pose benchmarks like LSP (Johnson and Everingham,
    [2010](#bib.bib68)) and MPII (Andriluka et al., [2014](#bib.bib4)) and human body
    model SMPL Bogo et al. ([2016](#bib.bib8)). The 3D human shape candidates are
    fit to color images by human annotators. DensePose (Güler et al., [2018](#bib.bib49))
    is an extension on 50K COCO images with people. All RGB images are manually annotated
    with surface-based representations of the human body. AMASS Dataset (Mahmood et al.,
    [2019](#bib.bib100)) unifies 15 different optical marker-based human motion capture
    datasets with SMPL Loper et al. ([2015](#bib.bib96)) body model as a standard
    fitting representation for human skeleton and surface mesh. Each body joint in
    this rich dataset has 3 rotational Degrees of Freedom (DoF) which are parametrized
    with exponential coordinates.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用 MoCap 系统收集的数据集外，还有其他方法可以创建用于 3D 人体姿态估计的数据集。JTA（Joint Track Auto）（Fabbri
    et al., [2018](#bib.bib35)）是一个完全合成的数据集，生成自高度真实感的电子游戏《侠盗猎车手 V》。它包含了近 $10M$ 个标注的身体姿态和超过
    $460,800$ 个密集标注的帧。在 Human3D+（Chen et al., [2016](#bib.bib19)）中，训练图像是通过将真实背景图像与从
    SCAPE 模型（Anguelov et al., [2005](#bib.bib5)）生成的不同纹理变形的 3D 纹理模型整合而来的。生成基本 SCAPE
    模型的参数是从 MoCap 系统中捕获的，或者从人工标注的 2D 姿态中推断出来的。SURREAL（Synthetic hUmans foR REAL）（Varol
    et al., [2017](#bib.bib168)）包含了带有真实未改变背景的单一合成人物的视频。它包含了身体部位分割、深度、光流和表面法线的注释。该数据集使用
    SMPL 身体模型来生成身体姿态和形状。LSP-MPII-Ordinal（Pavlakos et al., [2018a](#bib.bib124)）是对两个
    2D 人体姿态数据集（LSP（Johnson and Everingham, [2010](#bib.bib68)）和 MPII（Andriluka et
    al., [2014](#bib.bib4)））的扩展，通过为每对关节添加了序数深度关系。UP-3D（Lassner et al., [2017](#bib.bib79)）是将来自
    2D 人体姿态基准数据集（如 LSP（Johnson and Everingham, [2010](#bib.bib68)）和 MPII（Andriluka
    et al., [2014](#bib.bib4)））的彩色图像与人体模型 SMPL（Bogo et al., [2016](#bib.bib8)）相结合的结果。这些
    3D 人体形状候选者通过人工标注者与彩色图像相匹配。DensePose（Güler et al., [2018](#bib.bib49)）是对 50K COCO
    图像中的人物的扩展。所有 RGB 图像都手动标注了基于表面的身体表示。AMASS 数据集（Mahmood et al., [2019](#bib.bib100)）统一了
    15 个不同的光学标记基础的人体运动捕捉数据集，并以 SMPL（Loper et al., [2015](#bib.bib96)）身体模型作为人体骨架和表面网格的标准拟合表示。该丰富数据集中的每个身体关节具有
    3 个旋转自由度（DoF），其参数化使用了指数坐标。
- en: 5.4 Evaluation Metrics of 3D human pose estimation
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 3D 人体姿态估计的评估指标
- en: There are several evaluation metrics for 3D human pose estimation with different
    limitation factors. Note that we only list widely used evaluation metrics as below.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 3D 人体姿态估计，有几种评估指标，每种指标都有不同的限制因素。请注意，我们仅列出了以下广泛使用的评估指标。
- en: 'Mean Per Joint Position Error (MPJPE) is the most widely used measures to evaluate
    the performance of 3D pose estimation. It calculates the Euclidean distance from
    the estimated 3D joints to the ground truth in millimeters, averaged over all
    joints in one image. In the case of a set of frames, the mean error is averaged
    over all frames. For different datasets and different protocols, there are different
    data post-processing of estimated joints before computing the MPJPE. For example,
    in the protocol #1 of Human3.6M, the MPJPE is calculated after aligning the depths
    of the root joints (generally pelvis joint) (Tome et al., [2017](#bib.bib162);
    Yang et al., [2018](#bib.bib179)), which is also called N-MPJPE (Rhodin et al.,
    [2018a](#bib.bib139)). The MPJPE in HumanEva-I and the protocol #2 & #3 of Human3.6M
    is calculated after the alignment of predictions and ground truth with a rigid
    transformation using Procrustes Analysis (Gower, [1975](#bib.bib48)), which is
    also called reconstruction error (Kanazawa et al., [2018](#bib.bib73); Pavlakos
    et al., [2018b](#bib.bib126)), P-MPJPE (Rhodin et al., [2018a](#bib.bib139)) or
    PA-MPJPE (Sun et al., [2018](#bib.bib153)).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '平均每关节位置误差（MPJPE）是评估 3D 姿态估计性能的最广泛使用的指标。它计算从估计的 3D 关节到实际值的欧几里得距离（以毫米为单位），并在一张图片中的所有关节上取平均。在一组帧的情况下，平均误差是在所有帧上取平均的。对于不同的数据集和不同的协议，在计算
    MPJPE 之前，对估计的关节进行不同的数据后处理。例如，在 Human3.6M 的协议 #1 中，MPJPE 在对根关节（通常是盆骨关节）的深度对齐后计算（Tome
    et al., [2017](#bib.bib162); Yang et al., [2018](#bib.bib179)），这也称为 N-MPJPE（Rhodin
    et al., [2018a](#bib.bib139)）。在 HumanEva-I 和 Human3.6M 的协议 #2 & #3 中，MPJPE 是在使用
    Procrustes 分析（Gower, [1975](#bib.bib48)）对预测和实际值进行刚性变换对齐后计算的，这也称为重建误差（Kanazawa
    et al., [2018](#bib.bib73); Pavlakos et al., [2018b](#bib.bib126)）、P-MPJPE（Rhodin
    et al., [2018a](#bib.bib139)）或 PA-MPJPE（Sun et al., [2018](#bib.bib153)）。'
- en: Percentage of Correct Keypoints (PCK) and Area Under the Curve (AUC) are suggested
    by (Mehta et al., [2017a](#bib.bib104)) for 3D pose evaluation similar to PCK
    and AUC in MPII for 2D pose evaluation. PCK counts the percentage of points that
    fall in a threshold also called 3DPCK, and AUC is computed by a range of PCK thresholds.
    The general threshold in 3D space is 150mm, corresponding to roughly half of the
    head size.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 正确关键点的百分比（PCK）和曲线下面积（AUC）由（Mehta et al., [2017a](#bib.bib104)）建议用于 3D 姿态评估，类似于
    MPII 中用于 2D 姿态评估的 PCK 和 AUC。PCK 计算落在某个阈值范围内的点的百分比，也称为 3DPCK，而 AUC 是通过一系列 PCK 阈值计算的。3D
    空间中的一般阈值为 150 毫米，约为头部大小的一半。
- en: In addition to the evaluation metrics for 3D joint coordinates, there is another
    evaluation measurement Mean Per-vertex Error to report the results of 3D body
    shape which report the error between predicted and ground truth meshes (Varol
    et al., [2018](#bib.bib167); Pavlakos et al., [2018b](#bib.bib126)).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 3D 关节坐标的评估指标，还有另一种评估测量：平均每顶点误差，用于报告 3D 身体形状的结果，这报告了预测网格与实际网格之间的误差（Varol et
    al., [2018](#bib.bib167); Pavlakos et al., [2018b](#bib.bib126)）。
- en: 6 Conclusion and Future Research Directions
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来研究方向
- en: 'Human pose estimation is a hot research area in computer vision that evolved
    recently along with the blooming of deep learning. Due to limitations in hardware
    device capability and the quantity and quality of training data, early networks
    are relatively shallow, used in a very straightforward way and can only handle
    small images or patches (Toshev and Szegedy, [2014](#bib.bib165); Tompson et al.,
    [2015](#bib.bib163); Li and Chan, [2014](#bib.bib87)). More recent networks are
    more powerful, deeper and efficient (Newell et al., [2016](#bib.bib115); Cao et al.,
    [2016](#bib.bib13); He et al., [2017](#bib.bib51); Sun et al., [2019](#bib.bib151)).
    In this paper, we have reviewed the recent deep learning-based research addressing
    the 2D/3D human pose estimation problem from monocular images or video footage
    and organize approaches into four categories based on specific tasks: (1) 2D single
    person pose estimation, (2) 2D multi-person pose estimation, (3) 3D single person
    pose estimation, and (4) 3D multi-person pose estimation. Further, we have summarized
    the popular human pose datasets and evaluation protocols.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计是计算机视觉领域的热门研究方向，随着深度学习的蓬勃发展而不断演变。由于硬件设备能力的限制以及训练数据的数量和质量，早期的网络相对较浅，使用方式非常直接，只能处理小图像或图像块（Toshev
    和 Szegedy，[2014](#bib.bib165)；Tompson 等，[2015](#bib.bib163)；Li 和 Chan，[2014](#bib.bib87)）。更近期的网络更强大、更深且更高效（Newell
    等，[2016](#bib.bib115)；Cao 等，[2016](#bib.bib13)；He 等，[2017](#bib.bib51)；Sun 等，[2019](#bib.bib151)）。在本文中，我们回顾了针对单目图像或视频镜头中的2D/3D人体姿态估计问题的最新深度学习研究，并根据具体任务将方法整理为四类：（1）2D单人姿态估计，（2）2D多人姿态估计，（3）3D单人姿态估计，以及（4）3D多人姿态估计。此外，我们总结了流行的人体姿态数据集和评估协议。
- en: Despite the great development of monocular human pose estimation with deep learning,
    there still remain some unresolved challenges and gap between research and practical
    applications, such as the influence of body part occlusion and crowded people.
    Efficient networks and adequate training data are the most important requirements
    for deep learning-based approaches.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管单目人体姿态估计在深度学习的推动下取得了重大进展，但仍存在一些未解决的挑战和研究与实际应用之间的差距，例如身体部位遮挡和拥挤人群的影响。高效的网络和充足的训练数据是基于深度学习方法的最重要要求。
- en: Future networks should explore both global and local contexts for more discriminative
    features of the human body while exploiting human body structures into the network
    for prior constraints. Current networks have validated some effective network
    design tricks such as multi-stage structure, intermediate supervision, multi-scale
    feature fusion, multi-task learning, body structure constrains. Network efficiency
    is also a very important factor to apply algorithms in real-life applications.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的网络应探索全球和局部上下文，以获取人体的更多辨别特征，同时将人体结构融入网络以进行先验约束。目前的网络已经验证了一些有效的网络设计技巧，如多阶段结构、中间监督、多尺度特征融合、多任务学习、身体结构约束。网络效率也是将算法应用于实际应用中的一个非常重要的因素。
- en: Diversity data can improve the robustness of networks to handle complex scenes
    with irregular poses, occluded body limbs and crowded people. Data collection
    for specific complex scenarios is an option and there are other ways to extend
    existing datasets. Synthetic technology can theoretically generate unlimited data
    while there is a domain gap between synthetic data and real data. Cross-dataset
    supplementation, especially to supplement 3D datasets with 2D datasets can mitigate
    the problem of insufficient diversity of training data.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 多样化的数据可以提高网络处理复杂场景的鲁棒性，尤其是对于不规则姿态、遮挡的身体肢体和拥挤人群。针对特定复杂场景的数据收集是一种选择，此外还有其他方法可以扩展现有的数据集。合成技术理论上可以生成无限的数据，但合成数据与真实数据之间存在领域差距。跨数据集补充，特别是用2D数据集补充3D数据集，可以缓解训练数据多样性不足的问题。
- en: Acknowledgments
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This manuscript is based upon the work supported by National Science Foundation
    (NSF) under award number IIS-1400802 and National Natural Science Foundation of
    China (61420106007, 61671387). Yucheng Chen’s contribution was made when he was
    a visiting student at the City University of New York, sponsored by the Chinese
    Scholarship Council.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 本手稿基于国家科学基金会（NSF）资助的工作，奖号为IIS-1400802，以及中国国家自然科学基金（61420106007，61671387）。陈玉成的贡献是在他作为纽约城市大学的访问学生时进行的，由中国国家留学基金资助。
- en: References
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aggarwal and Cai (1999) Aggarwal, J.K., Cai, Q., 1999. Human motion analysis:
    A review. Computer Vision and Image Understanding 73, 428–440.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aggarwal 和 Cai (1999) Aggarwal, J.K., Cai, Q., 1999. 人体运动分析：综述。计算机视觉与图像理解 73,
    428–440。
- en: 'Ainsworth et al. (2011) Ainsworth, B.E., Haskell, W.L., Herrmann, S.D., Meckes,
    N., Bassett Jr, D.R., Tudor-Locke, C., Greer, J.L., Vezina, J., Whitt-Glover,
    M.C., Leon, A.S., 2011. 2011 compendium of physical activities: a second update
    of codes and met values. Medicine & science in sports & exercise 43, 1575–1581.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ainsworth 等 (2011) Ainsworth, B.E., Haskell, W.L., Herrmann, S.D., Meckes, N.,
    Bassett Jr, D.R., Tudor-Locke, C., Greer, J.L., Vezina, J., Whitt-Glover, M.C.,
    Leon, A.S., 2011. 2011 年体力活动汇编：代码和 MET 值的第二次更新。医学与运动科学 43, 1575–1581。
- en: 'Andriluka et al. (2018) Andriluka, M., Iqbal, U., Milan, A., Insafutdinov,
    E., Pishchulin, L., Gall, J., Schiele, B., 2018. Posetrack: A benchmark for human
    pose estimation and tracking, in: Proc. IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 5167–5176.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andriluka 等 (2018) Andriluka, M., Iqbal, U., Milan, A., Insafutdinov, E., Pishchulin,
    L., Gall, J., Schiele, B., 2018. Posetrack：人体姿态估计和跟踪基准，发表于：IEEE 计算机视觉与模式识别会议论文集，第5167–5176页。
- en: 'Andriluka et al. (2014) Andriluka, M., Pishchulin, L., Gehler, P., Schiele,
    B., 2014. 2d human pose estimation: New benchmark and state of the art analysis,
    in: Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 3686–3693.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andriluka 等 (2014) Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B., 2014.
    2D 人体姿态估计：新的基准和最新分析，发表于：IEEE 计算机视觉与模式识别会议论文集，第3686–3693页。
- en: 'Anguelov et al. (2005) Anguelov, D., Srinivasan, P., Koller, D., Thrun, S.,
    Rodgers, J., Davis, J., 2005. Scape: shape completion and animation of people,
    in: ACM transactions on graphics, ACM. pp. 408–416.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anguelov 等 (2005) Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers,
    J., Davis, J., 2005. Scape：人类形状补全和动画，发表于：ACM 图形学交易，ACM. 第408–416页。
- en: 'Arnab et al. (2019) Arnab, A., Doersch, C., Zisserman, A., 2019. Exploiting
    temporal context for 3d human pose estimation in the wild, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 3395–3404.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arnab 等 (2019) Arnab, A., Doersch, C., Zisserman, A., 2019. 利用时间上下文进行野外 3D 人体姿态估计，发表于：IEEE
    计算机视觉与模式识别会议论文集，第3395–3404页。
- en: 'Belagiannis and Zisserman (2017) Belagiannis, V., Zisserman, A., 2017. Recurrent
    human pose estimation, in: Proc. IEEE Conference on Automatic Face and Gesture
    Recognition, IEEE. pp. 468–475.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belagiannis 和 Zisserman (2017) Belagiannis, V., Zisserman, A., 2017. 循环人类姿态估计，发表于：IEEE
    自动面部和姿态识别会议论文集，IEEE. 第468–475页。
- en: 'Bogo et al. (2016) Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero,
    J., Black, M.J., 2016. Keep it smpl: Automatic estimation of 3d human pose and
    shape from a single image, in: Proc. European Conference on Computer Vision, Springer.
    pp. 561–578.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogo 等 (2016) Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black,
    M.J., 2016. 保持简单：从单张图像自动估计 3D 人体姿态和形状，发表于：欧洲计算机视觉会议论文集，Springer. 第561–578页。
- en: 'Bogo et al. (2014) Bogo, F., Romero, J., Loper, M., Black, M.J., 2014. FAUST:
    Dataset and evaluation for 3D mesh registration, in: Proc. IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 3794–3801.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogo 等 (2014) Bogo, F., Romero, J., Loper, M., Black, M.J., 2014. FAUST：3D 网格注册数据集和评估，发表于：IEEE
    计算机视觉与模式识别会议论文集，第3794–3801页。
- en: 'Bogo et al. (2017) Bogo, F., Romero, J., Pons-Moll, G., Black, M.J., 2017.
    Dynamic FAUST: Registering human bodies in motion, in: Proc. IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 6233–6242.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogo 等 (2017) Bogo, F., Romero, J., Pons-Moll, G., Black, M.J., 2017. 动态 FAUST：注册运动中的人类身体，发表于：IEEE
    计算机视觉与模式识别会议论文集，第6233–6242页。
- en: 'Bourdev and Malik (2009) Bourdev, L., Malik, J., 2009. Poselets: Body part
    detectors trained using 3d human pose annotations, in: Proc. IEEE International
    Conference on Computer Vision, IEEE. pp. 1365–1372.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bourdev 和 Malik (2009) Bourdev, L., Malik, J., 2009. Poselets：使用 3D 人体姿态注释训练的身体部位检测器，发表于：IEEE
    国际计算机视觉会议论文集，IEEE. 第1365–1372页。
- en: 'Bulat and Tzimiropoulos (2016) Bulat, A., Tzimiropoulos, G., 2016. Human pose
    estimation via convolutional part heatmap regression, in: Proc. European Conference
    on Computer Vision, Springer. pp. 717–732.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulat 和 Tzimiropoulos (2016) Bulat, A., Tzimiropoulos, G., 2016. 通过卷积部分热图回归进行人体姿态估计，发表于：欧洲计算机视觉会议论文集，Springer.
    第717–732页。
- en: Cao et al. (2016) Cao, Z., Simon, T., Wei, S.E., Sheikh, Y., 2016. Realtime
    multi-person 2d pose estimation using part affinity fields. arXiv preprint arXiv:1611.08050
    .
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等 (2016) Cao, Z., Simon, T., Wei, S.E., Sheikh, Y., 2016. 实时多人 2D 姿态估计使用部分亲和场。arXiv
    预印本 arXiv:1611.08050。
- en: 'Carreira et al. (2016) Carreira, J., Agrawal, P., Fragkiadaki, K., Malik, J.,
    2016. Human pose estimation with iterative error feedback, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4733–4742.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡雷拉等（2016）卡雷拉，J.，阿格拉瓦尔，P.，弗拉基亚达基，K.，马利克，J.，2016。具有迭代误差反馈的人体姿态估计，载于：IEEE计算机视觉与模式识别会议论文集，第4733–4742页。
- en: Charles et al. (2014) Charles, J., Pfister, T., Everingham, M., Zisserman, A.,
    2014. Automatic and efficient human pose estimation for sign language videos.
    International Journal of Computer Vision 110, 70–90.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查尔斯等（2014）查尔斯，J.，费斯特，T.，埃弗宁汉，M.，齐泽曼，A.，2014。用于手语视频的自动高效人体姿态估计。计算机视觉国际期刊 110，70–90。
- en: 'Charles et al. (2016) Charles, J., Pfister, T., Magee, D., Hogg, D., Zisserman,
    A., 2016. Personalizing human video pose estimation, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 3063–3072.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查尔斯等（2016）查尔斯，J.，费斯特，T.，梅吉，D.，霍格，D.，齐泽曼，A.，2016。个性化的人体视频姿态估计，载于：IEEE计算机视觉与模式识别会议论文集，第3063–3072页。
- en: 'Chen and Ramanan (2017) Chen, C.H., Ramanan, D., 2017. 3d human pose estimation=
    2d pose estimation+ matching, in: Proc. IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 7035–7043.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈和拉马南（2017）陈，C.H.，拉马南，D.，2017。三维人体姿态估计 = 二维姿态估计 + 匹配，载于：IEEE计算机视觉与模式识别会议论文集，第7035–7043页。
- en: Chen et al. (2013) Chen, L., Wei, H., Ferryman, J., 2013. A survey of human
    motion analysis using depth imagery. Pattern Recognition Letters 34, 1995–2006.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2013）陈，L.，魏，H.，费瑞曼，J.，2013。基于深度图像的人体运动分析综述。模式识别快报 34，1995–2006。
- en: 'Chen et al. (2016) Chen, W., Wang, H., Li, Y., Su, H., Wang, Z., Tu, C., Lischinski,
    D., Cohen-Or, D., Chen, B., 2016. Synthesizing training images for boosting human
    3d pose estimation, in: Proc. IEEE International Conference on 3D Vision, IEEE.
    pp. 479–488.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2016）陈，W.，王，H.，李，Y.，苏，H.，王，Z.，屠，C.，利施金斯基，D.，科恩-奥尔，D.，陈，B.，2016。合成训练图像以提升三维人体姿态估计，载于：IEEE国际三维视觉会议论文集，第479–488页。
- en: 'Chen and Yuille (2014) Chen, X., Yuille, A.L., 2014. Articulated pose estimation
    by a graphical model with image dependent pairwise relations, in: Advances in
    neural information processing systems, pp. 1736–1744.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈和尤伊尔（2014）陈，X.，尤伊尔，A.L.，2014。通过具有图像依赖配对关系的图形模型进行关节姿态估计，载于：神经信息处理系统进展，第1736–1744页。
- en: 'Chen et al. (2017) Chen, Y., Shen, C., Wei, X.S., Liu, L., Yang, J., 2017.
    Adversarial posenet: A structure-aware convolutional network for human pose estimation.
    CoRR, abs/1705.00389 2.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2017）陈，Y.，沈，C.，魏，X.S.，刘，L.，杨，J.，2017。对抗性姿态网络：一种结构感知卷积网络用于人体姿态估计。CoRR，abs/1705.00389
    2。
- en: 'Chen et al. (2018) Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.,
    2018. Cascaded pyramid network for multi-person pose estimation, in: Proc. IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 7103–7112.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2018）陈，Y.，王，Z.，彭，Y.，张，Z.，余，G.，孙，J.，2018。用于多人体姿态估计的级联金字塔网络，载于：IEEE计算机视觉与模式识别会议论文集，第7103–7112页。
- en: Chou et al. (2017) Chou, C.J., Chien, J.T., Chen, H.T., 2017. Self adversarial
    training for human pose estimation. arXiv preprint arXiv:1707.02439 .
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2017）周，C.J.，钱，J.T.，陈，H.T.，2017。用于人体姿态估计的自我对抗训练。arXiv预印本 arXiv:1707.02439。
- en: 'Chu et al. (2016) Chu, X., Ouyang, W., Li, H., Wang, X., 2016. Structured feature
    learning for pose estimation, in: Proc. IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 4715–4723.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 傅等（2016）傅，X.，欧阳，W.，李，H.，王，X.，2016。用于姿态估计的结构化特征学习，载于：IEEE计算机视觉与模式识别会议论文集，第4715–4723页。
- en: Chu et al. (2017) Chu, X., Yang, W., Ouyang, W., Ma, C., Yuille, A.L., Wang,
    X., 2017. Multi-context attention for human pose estimation. arXiv preprint arXiv:1702.07432
    1.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 傅等（2017）傅，X.，杨，W.，欧阳，W.，马，C.，尤伊尔，A.L.，王，X.，2017。用于人体姿态估计的多上下文注意力。arXiv预印本 arXiv:1702.07432
    1。
- en: Cootes et al. (1995) Cootes, T.F., Taylor, C.J., Cooper, D.H., Graham, J., 1995.
    Active shape models-their training and application. Computer Vision and Image
    Understanding 61, 38–59.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库特斯等（1995）库特斯，T.F.，泰勒，C.J.，库珀，D.H.，格雷厄姆，J.，1995。主动形状模型——其训练和应用。计算机视觉与图像理解 61，38–59。
- en: 'Dantone et al. (2013) Dantone, M., Gall, J., Leistner, C., Van Gool, L., 2013.
    Human pose estimation using body parts dependent joint regressors, in: Proc. IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 3041–3048.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丹顿等（2013）丹顿，M.，盖尔，J.，莱斯特纳，C.，范·古尔，L.，2013。使用依赖于身体部位的关节回归器的人体姿态估计，载于：IEEE计算机视觉与模式识别会议论文集，第3041–3048页。
- en: 'Debnath et al. (2018) Debnath, B., O’Brien, M., Yamaguchi, M., Behera, A.,
    2018. Adapting mobilenets for mobile based upper body pose estimation, in: Proc.
    IEEE Conference on Advanced Video and Signal Based Surveillance, pp. 1–6.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debnath 等人（2018）Debnath, B., O’Brien, M., Yamaguchi, M., Behera, A., 2018. 适配
    MobileNets 进行基于移动设备的上半身姿态估计，见：Proc. IEEE Conference on Advanced Video and Signal
    Based Surveillance，页码 1–6。
- en: 'Eichner and Ferrari (2010) Eichner, M., Ferrari, V., 2010. We are family: Joint
    pose estimation of multiple persons, in: Proc. European Conference on Computer
    Vision, Springer. pp. 228–242.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eichner 和 Ferrari（2010）Eichner, M., Ferrari, V., 2010. 我们是一家人：多人联合姿态估计，见：Proc.
    European Conference on Computer Vision，Springer，页码 228–242。
- en: 'Eichner and Ferrari (2012a) Eichner, M., Ferrari, V., 2012a. Calvin upper-body
    detector v1.04. URL: [http://groups.inf.ed.ac.uk/calvin/calvin_upperbody_detector/](http://groups.inf.ed.ac.uk/calvin/calvin_upperbody_detector/).'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eichner 和 Ferrari（2012a）Eichner, M., Ferrari, V., 2012a. Calvin 上半身检测器 v1.04。网址：[http://groups.inf.ed.ac.uk/calvin/calvin_upperbody_detector/](http://groups.inf.ed.ac.uk/calvin/calvin_upperbody_detector/)。
- en: Eichner and Ferrari (2012b) Eichner, M., Ferrari, V., 2012b. Human pose co-estimation
    and applications. IEEE transactions on pattern analysis and machine intelligence
    34, 2282–2288.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eichner 和 Ferrari（2012b）Eichner, M., Ferrari, V., 2012b. 人体姿态联合估计及其应用。IEEE transactions
    on pattern analysis and machine intelligence 34, 2282–2288。
- en: 'Eichner et al. (2009) Eichner, M., Ferrari, V., Zurich, S., 2009. Better appearance
    models for pictorial structures, in: Proc. British Machine Vision Conference,
    p. 5.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eichner 等人（2009）Eichner, M., Ferrari, V., Zurich, S., 2009. 更好的外观模型用于图像结构，见：Proc.
    British Machine Vision Conference，页码 5。
- en: Elhayek et al. (2017) Elhayek, A., de Aguiar, E., Jain, A., Thompson, J., Pishchulin,
    L., Andriluka, M., Bregler, C., Schiele, B., Theobalt, C., 2017. Marconi—convnet-based
    marker-less motion capture in outdoor and indoor scenes. IEEE transactions on
    pattern analysis and machine intelligence 39, 501–514.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhayek 等人（2017）Elhayek, A., de Aguiar, E., Jain, A., Thompson, J., Pishchulin,
    L., Andriluka, M., Bregler, C., Schiele, B., Theobalt, C., 2017. Marconi—基于 ConvNet
    的无标记动作捕捉，适用于室内和室外场景。IEEE transactions on pattern analysis and machine intelligence
    39, 501–514。
- en: Everingham et al. (2010) Everingham, M., Van Gool, L., Williams, C.K., Winn,
    J., Zisserman, A., 2010. The pascal visual object classes (voc) challenge. International
    journal of computer vision 88, 303–338.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Everingham 等人（2010）Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman,
    A., 2010. Pascal 视觉对象类别（VOC）挑战。International journal of computer vision 88, 303–338。
- en: 'Fabbri et al. (2018) Fabbri, M., Lanzi, F., Calderara, S., Palazzi, A., Vezzani,
    R., Cucchiara, R., 2018. Learning to detect and track visible and occluded body
    joints in a virtual world, in: Proc. European Conference on Computer Vision, pp.
    430–446.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fabbri 等人（2018）Fabbri, M., Lanzi, F., Calderara, S., Palazzi, A., Vezzani, R.,
    Cucchiara, R., 2018. 在虚拟世界中学习检测和跟踪可见和遮挡的身体关节，见：Proc. European Conference on Computer
    Vision，页码 430–446。
- en: 'Faessler et al. (2014) Faessler, M., Mueggler, E., Schwabe, K., Scaramuzza,
    D., 2014. A monocular pose estimation system based on infrared leds, in: Proc.
    IEEE International Conference on Robotics and Automation, IEEE. pp. 907–913.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faessler 等人（2014）Faessler, M., Mueggler, E., Schwabe, K., Scaramuzza, D., 2014.
    基于红外 LED 的单目姿态估计系统，见：Proc. IEEE International Conference on Robotics and Automation，IEEE，页码
    907–913。
- en: 'Fan et al. (2015) Fan, X., Zheng, K., Lin, Y., Wang, S., 2015. Combining local
    appearance and holistic view: Dual-source deep neural networks for human pose
    estimation. arXiv preprint arXiv:1504.07159 .'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人（2015）Fan, X., Zheng, K., Lin, Y., Wang, S., 2015. 结合局部外观和整体视图：用于人体姿态估计的双源深度神经网络。arXiv
    预印本 arXiv:1504.07159。
- en: 'Fang et al. (2017) Fang, H., Xie, S., Tai, Y.W., Lu, C., 2017. Rmpe: Regional
    multi-person pose estimation, in: Proc. IEEE International Conference on Computer
    Vision, pp. 2334–2343.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等人（2017）Fang, H., Xie, S., Tai, Y.W., Lu, C., 2017. RMPE：区域多人姿态估计，见：Proc.
    IEEE International Conference on Computer Vision，页码 2334–2343。
- en: Felzenszwalb and Huttenlocher (2005) Felzenszwalb, P.F., Huttenlocher, D.P.,
    2005. Pictorial structures for object recognition. International journal of computer
    vision 61, 55–79.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felzenszwalb 和 Huttenlocher（2005）Felzenszwalb, P.F., Huttenlocher, D.P., 2005.
    用于对象识别的图像结构。International journal of computer vision 61, 55–79。
- en: 'Feng et al. (2019) Feng, Z., Xiatian, Z., Mao, Y., 2019. Fast human pose estimation,
    in: Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人（2019）Feng, Z., Xiatian, Z., Mao, Y., 2019. 快速人体姿态估计，见：Proc. IEEE Conference
    on Computer Vision and Pattern Recognition，页码 1–8。
- en: 'Ferrari et al. (2008) Ferrari, V., Marin-Jimenez, M., Zisserman, A., 2008.
    Progressive search space reduction for human pose estimation, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 1–8.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrari 等人（2008）Ferrari, V., Marin-Jimenez, M., Zisserman, A., 2008. 人体姿态估计的渐进搜索空间减少，见：Proc.
    IEEE Conference on Computer Vision and Pattern Recognition，页码 1–8。
- en: 'Gavrila (1999) Gavrila, D.M., 1999. The visual analysis of human movement:
    A survey. Computer Vision and Image Understanding 73, 82–98.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gavrila (1999) Gavrila, D.M., 1999. 人体运动的视觉分析：一项调查。计算机视觉与图像理解 73，82–98。
- en: 'Gkioxari et al. (2013) Gkioxari, G., Arbelaez, P., Bourdev, L., Malik, J.,
    2013. Articulated pose estimation using discriminative armlet classifiers, in:
    Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 3342–3349.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gkioxari 等 (2013) Gkioxari, G., Arbelaez, P., Bourdev, L., Malik, J., 2013.
    使用判别式臂套分类器进行关节姿态估计，见：IEEE 计算机视觉与模式识别会议论文集，第 3342–3349 页。
- en: Gkioxari et al. (2014a) Gkioxari, G., Hariharan, B., Girshick, R., Malik, J.,
    2014a. R-cnns for pose estimation and action detection. arXiv preprint arXiv:1406.5212
    .
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gkioxari 等 (2014a) Gkioxari, G., Hariharan, B., Girshick, R., Malik, J., 2014a.
    用于姿态估计和动作检测的 R-CNNs。arXiv 预印本 arXiv:1406.5212。
- en: 'Gkioxari et al. (2014b) Gkioxari, G., Hariharan, B., Girshick, R., Malik, J.,
    2014b. Using k-poselets for detecting people and localizing their keypoints, in:
    Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 3582–3589.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gkioxari 等 (2014b) Gkioxari, G., Hariharan, B., Girshick, R., Malik, J., 2014b.
    使用 k-poselets 检测人物并定位其关键点，见：IEEE 计算机视觉与模式识别会议论文集，第 3582–3589 页。
- en: 'Gkioxari et al. (2016) Gkioxari, G., Toshev, A., Jaitly, N., 2016. Chained
    predictions using convolutional neural networks, in: Proc. European Conference
    on Computer Vision, Springer. pp. 728–743.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gkioxari 等 (2016) Gkioxari, G., Toshev, A., Jaitly, N., 2016. 使用卷积神经网络进行链式预测，见：欧洲计算机视觉会议论文集，Springer，第
    728–743 页。
- en: 'Gong et al. (2016) Gong, W., Zhang, X., Gonzàlez, J., Sobral, A., Bouwmans,
    T., Tu, C., Zahzah, E.h., 2016. Human pose estimation from monocular images: A
    comprehensive survey. Sensors 16, 1966.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等 (2016) Gong, W., Zhang, X., Gonzàlez, J., Sobral, A., Bouwmans, T., Tu,
    C., Zahzah, E.h., 2016. 从单目图像中进行人体姿态估计：一项全面的调查。传感器 16，1966。
- en: Gower (1975) Gower, J.C., 1975. Generalized procrustes analysis. Psychometrika
    40, 33–51.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gower (1975) Gower, J.C., 1975. 广义 Procrustes 分析。心理计量学 40，33–51。
- en: 'Güler et al. (2018) Güler, R.A., Neverova, N., Kokkinos, I., 2018. Densepose:
    Dense human pose estimation in the wild, in: Proc. IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 7297–7306.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Güler 等 (2018) Güler, R.A., Neverova, N., Kokkinos, I., 2018. Densepose：野外的密集人体姿态估计，见：IEEE
    计算机视觉与模式识别会议论文集，第 7297–7306 页。
- en: 'Hasler et al. (2009) Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., Seidel,
    H.P., 2009. A statistical model of human pose and body shape, in: Computer Graphics
    Forum, Wiley Online Library. pp. 337–346.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasler 等 (2009) Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., Seidel, H.P.,
    2009. 人体姿态和体型的统计模型，见：计算机图形学论坛，Wiley 在线图书馆，第 337–346 页。
- en: 'He et al. (2017) He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask
    r-cnn, in: Proc. IEEE International Conference on Computer Vision, IEEE. pp. 2980–2988.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2017) He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask R-CNN，见：IEEE
    国际计算机视觉会议论文集，IEEE，第 2980–2988 页。
- en: 'Holte et al. (2012) Holte, M.B., Tran, C., Trivedi, M.M., Moeslund, T.B., 2012.
    Human pose estimation and activity recognition from multi-view videos: Comparative
    explorations of recent developments. IEEE Journal of selected topics in signal
    processing 6, 538–552.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holte 等 (2012) Holte, M.B., Tran, C., Trivedi, M.M., Moeslund, T.B., 2012. 从多视角视频中进行人体姿态估计和活动识别：对近期发展的比较探索。IEEE
    选择信号处理学报 6，538–552。
- en: 'Howard et al. (2017) Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang,
    W., Weyand, T., Andreetto, M., Adam, H., 2017. Mobilenets: Efficient convolutional
    neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861
    .'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 等 (2017) Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W.,
    Weyand, T., Andreetto, M., Adam, H., 2017. Mobilenets：用于移动视觉应用的高效卷积神经网络。arXiv
    预印本 arXiv:1704.04861。
- en: Hu et al. (2004) Hu, W., Tan, T., Wang, L., Maybank, S., 2004. A survey on visual
    surveillance of object motion and behaviors. IEEE Transactions on Systems, Man,
    and Cybernetics, Part C (Applications and Reviews) 34, 334–352.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2004) Hu, W., Tan, T., Wang, L., Maybank, S., 2004. 关于物体运动和行为的视觉监控调查。IEEE
    系统、人和控制论学报，C 部分（应用与评论）34，334–352。
- en: 'Huang et al. (2017) Huang, S., Gong, M., Tao, D., 2017. A coarse-fine network
    for keypoint localization, in: Proc. IEEE International Conference on Computer
    Vision, pp. 3028–3037.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2017) Huang, S., Gong, M., Tao, D., 2017. 用于关键点定位的粗细网络，见：IEEE 国际计算机视觉会议论文集，第
    3028–3037 页。
- en: 'INRIA4D (accessed on 2019) INRIA4D, accessed on 2019. URL: [http://4drepository.inrialpes.fr](http://4drepository.inrialpes.fr).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: INRIA4D (访问于 2019) INRIA4D，访问于 2019。网址：[http://4drepository.inrialpes.fr](http://4drepository.inrialpes.fr)。
- en: 'Insafutdinov et al. (2017) Insafutdinov, E., Andriluka, M., Pishchulin, L.,
    Tang, S., Levinkov, E., Andres, B., Schiele, B., 2017. Arttrack: Articulated multi-person
    tracking in the wild, in: Proc. IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 6457–6465.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Insafutdinov等（2017）Insafutdinov, E., Andriluka, M., Pishchulin, L., Tang, S.,
    Levinkov, E., Andres, B., Schiele, B., 2017。Arttrack：野外的多人体关节跟踪，见：IEEE计算机视觉与模式识别会议论文集，pp.
    6457–6465。
- en: 'Insafutdinov et al. (2016) Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
    M., Schiele, B., 2016. Deepercut: A deeper, stronger, and faster multi-person
    pose estimation model, in: Proc. European Conference on Computer Vision, Springer.
    pp. 34–50.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Insafutdinov等（2016）Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
    M., Schiele, B., 2016。Deepercut：一个更深、更强、更快的多人姿态估计模型，见：欧洲计算机视觉会议论文集，Springer。pp.
    34–50。
- en: 'Ionescu et al. (2014) Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.,
    2014. Human3.6m: Large scale datasets and predictive methods for 3d human sensing
    in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence
    36, 1325–1339.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ionescu等（2014）Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C., 2014。Human3.6m：用于自然环境中的3D人体感知的大规模数据集和预测方法。IEEE模式分析与机器智能学报
    36，1325–1339。
- en: 'Iqbal and Gall (2016) Iqbal, U., Gall, J., 2016. Multi-person pose estimation
    with local joint-to-person associations, in: Proc. European Conference on Computer
    Vision, Springer. pp. 627–642.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iqbal和Gall（2016）Iqbal, U., Gall, J., 2016。带有局部关节到人的关联的多人姿态估计，见：欧洲计算机视觉会议论文集，Springer。pp.
    627–642。
- en: 'Iqbal et al. (2016) Iqbal, U., Milan, A., Gall, J., 2016. Posetrack: Joint
    multi-person pose estimation and tracking. arXiv:1611.07727 .'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iqbal等（2016）Iqbal, U., Milan, A., Gall, J., 2016。Posetrack：联合多人的姿态估计与跟踪。arXiv:1611.07727。
- en: 'Jaderberg et al. (2015) Jaderberg, M., Simonyan, K., Zisserman, A., et al.,
    2015. Spatial transformer networks, in: Advances in neural information processing
    systems, pp. 2017–2025.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg等（2015）Jaderberg, M., Simonyan, K., Zisserman, A., 等，2015。空间变换网络，见：神经信息处理系统进展，pp.
    2017–2025。
- en: Jain et al. (2013) Jain, A., Tompson, J., Andriluka, M., Taylor, G.W., Bregler,
    C., 2013. Learning human pose estimation features with convolutional networks.
    arXiv preprint arXiv:1312.7302 .
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain等（2013）Jain, A., Tompson, J., Andriluka, M., Taylor, G.W., Bregler, C.,
    2013。利用卷积网络学习人体姿态估计特征。arXiv预印本 arXiv:1312.7302。
- en: 'Jain et al. (2014) Jain, A., Tompson, J., LeCun, Y., Bregler, C., 2014. Modeep:
    A deep learning framework using motion features for human pose estimation, in:
    Proc. Asian conference on computer vision, Springer. pp. 302–315.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain等（2014）Jain, A., Tompson, J., LeCun, Y., Bregler, C., 2014。Modeep：一个使用运动特征进行人体姿态估计的深度学习框架，见：亚洲计算机视觉会议论文集，Springer。pp.
    302–315。
- en: 'Jhuang et al. (2013) Jhuang, H., Gall, J., Zuffi, S., Schmid, C., Black, M.J.,
    2013. Towards understanding action recognition, in: Proc. IEEE International Conference
    on Computer Vision, pp. 3192–3199.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jhuang等（2013）Jhuang, H., Gall, J., Zuffi, S., Schmid, C., Black, M.J., 2013。为了理解动作识别，见：IEEE国际计算机视觉会议论文集，pp.
    3192–3199。
- en: 'Jhuang et al. (2011) Jhuang, H., Garrote, H., Poggio, E., Serre, T., Hmdb,
    T., 2011. A large video database for human motion recognition, in: Proc. IEEE
    International Conference on Computer Vision, p. 6.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jhuang等（2011）Jhuang, H., Garrote, H., Poggio, E., Serre, T., Hmdb, T., 2011。一个大型视频数据库用于人体动作识别，见：IEEE国际计算机视觉会议论文集，第6页。
- en: 'Ji and Liu (2010) Ji, X., Liu, H., 2010. Advances in view-invariant human motion
    analysis: A review. IEEE Transactions on Systems, Man, and Cybernetics, Part C
    (Applications and Reviews) 40, 13–24.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji和Liu（2010）Ji, X., Liu, H., 2010。在视角不变的人体动作分析方面的进展：综述。IEEE系统、人类与控制论学报，第C部分（应用与综述）40，13–24。
- en: 'Johnson and Everingham (2010) Johnson, S., Everingham, M., 2010. Clustered
    pose and nonlinear appearance models for human pose estimation, in: Proc. British
    Machine Vision Conference, p. 5.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson和Everingham（2010）Johnson, S., Everingham, M., 2010。用于人体姿态估计的聚类姿态与非线性外观模型，见：英国机器视觉会议论文集，第5页。
- en: 'Johnson and Everingham (2011) Johnson, S., Everingham, M., 2011. Learning effective
    human pose estimation from inaccurate annotation, in: Proc. IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 1465–1472.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson和Everingham（2011）Johnson, S., Everingham, M., 2011。从不准确的标注中学习有效的人体姿态估计，见：IEEE计算机视觉与模式识别会议论文集，pp.
    1465–1472。
- en: 'Joo et al. (2017) Joo, H., Simon, T., Li, X., Liu, H., Tan, L., Gui, L., Banerjee,
    S., Godisart, T.S., Nabbe, B., Matthews, I., Kanade, T., Nobuhara, S., Sheikh,
    Y., 2017. Panoptic studio: A massively multiview system for social interaction
    capture. IEEE Transactions on Pattern Analysis and Machine Intelligence 41, 190–204.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joo et al. (2017) Joo, H., Simon, T., Li, X., Liu, H., Tan, L., Gui, L., Banerjee,
    S., Godisart, T.S., Nabbe, B., Matthews, I., Kanade, T., Nobuhara, S., Sheikh,
    Y., 2017. Panoptic studio：一个大规模多视角系统用于社会互动捕捉。IEEE模式分析与机器智能学报 41, 190–204。
- en: 'Joo et al. (2018) Joo, H., Simon, T., Sheikh, Y., 2018. Total capture: A 3d
    deformation model for tracking faces, hands, and bodies, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 8320–8329.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joo et al. (2018) Joo, H., Simon, T., Sheikh, Y., 2018. 全捕捉：用于跟踪面部、手部和身体的3D形变模型，见：IEEE计算机视觉与模式识别会议论文集，页码8320–8329。
- en: 'Ju et al. (1996) Ju, S.X., Black, M.J., Yacoob, Y., 1996. Cardboard people:
    A parameterized model of articulated image motion, in: Proc. IEEE Conference on
    Automatic Face and Gesture Recognition, pp. 38–44.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ju et al. (1996) Ju, S.X., Black, M.J., Yacoob, Y., 1996. 纸板人：一个参数化的关节图像运动模型，见：IEEE自动面部和姿态识别会议论文集，页码38–44。
- en: 'Kanazawa et al. (2018) Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.,
    2018. End-to-end recovery of human shape and pose, in: Proc. IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 7122–7131.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanazawa et al. (2018) Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J., 2018.
    端到端的人体形状与姿态恢复，见：IEEE计算机视觉与模式识别会议论文集，页码7122–7131。
- en: Ke et al. (2018) Ke, L., Chang, M.C., Qi, H., Lyu, S., 2018. Multi-scale structure-aware
    network for human pose estimation. arXiv preprint arXiv:1803.09894 .
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke et al. (2018) Ke, L., Chang, M.C., Qi, H., Lyu, S., 2018. 多尺度结构感知网络用于人体姿态估计。arXiv预印本arXiv:1803.09894。
- en: 'Kinect (accessed on 2019) Kinect, accessed on 2019. URL: [https://developer.microsoft.com/en-us/windows/kinect](https://developer.microsoft.com/en-us/windows/kinect).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinect (accessed on 2019) Kinect，访问于2019年。网址：[https://developer.microsoft.com/en-us/windows/kinect](https://developer.microsoft.com/en-us/windows/kinect)。
- en: 'Kocabas et al. (2018) Kocabas, M., Karagoz, S., Akbas, E., 2018. Multiposenet:
    Fast multi-person pose estimation using pose residual network, in: Proc. European
    Conference on Computer Vision, Springer. pp. 437–453.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocabas et al. (2018) Kocabas, M., Karagoz, S., Akbas, E., 2018. Multiposenet：使用姿态残差网络的快速多人体姿态估计，见：欧洲计算机视觉会议，Springer。页码437–453。
- en: 'Kreiss et al. (2019) Kreiss, S., Bertoni, L., Alahi, A., 2019. Pifpaf: Composite
    fields for human pose estimation, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 11977–11986.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kreiss et al. (2019) Kreiss, S., Bertoni, L., Alahi, A., 2019. Pifpaf: 人体姿态估计的复合场，见：IEEE计算机视觉与模式识别会议论文集，页码11977–11986。'
- en: 'Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    Imagenet classification with deep convolutional neural networks, in: Advances
    in neural information processing systems, pp. 1097–1105.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    使用深度卷积神经网络进行Imagenet分类，见：神经信息处理系统的进展，页码1097–1105。
- en: 'Lassner et al. (2017) Lassner, C., Romero, J., Kiefel, M., Bogo, F., Black,
    M.J., Gehler, P.V., 2017. Unite the people: Closing the loop between 3d and 2d
    human representations, in: Proc. IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 4704–4713.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lassner et al. (2017) Lassner, C., Romero, J., Kiefel, M., Bogo, F., Black,
    M.J., Gehler, P.V., 2017. 统一人群：3D与2D人体表示之间的闭环，见：IEEE计算机视觉与模式识别会议论文集，页码4704–4713。
- en: 'Li et al. (2017a) Li, B., Chen, H., Chen, Y., Dai, Y., He, M., 2017a. Skeleton
    boxes: Solving skeleton based action detection with a single deep convolutional
    neural network, in: Proc. IEEE International Conference on Multimedia and Expo
    Workshops, pp. 613–616.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2017a) Li, B., Chen, H., Chen, Y., Dai, Y., He, M., 2017a. 骨架框：使用单个深度卷积神经网络解决基于骨架的动作检测，见：IEEE多媒体与展会研讨会国际会议论文集，页码613–616。
- en: 'Li et al. (2017b) Li, B., Dai, Y., Cheng, X., Chen, H., Lin, Y., He, M., 2017b.
    Skeleton based action recognition using translation-scale invariant image mapping
    and multi-scale deep cnn, in: Proc. IEEE International Conference on Multimedia
    and Expo Workshops, pp. 601–604.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2017b) Li, B., Dai, Y., Cheng, X., Chen, H., Lin, Y., He, M., 2017b.
    基于骨架的动作识别：使用平移-尺度不变图像映射和多尺度深度CNN，见：IEEE多媒体与展会研讨会国际会议论文集，页码601–604。
- en: Li et al. (2018a) Li, B., Dai, Y., He, M., 2018a. Monocular depth estimation
    with hierarchical fusion of dilated cnns and soft-weighted-sum inference. Pattern
    Recognition 83, 328–339.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2018a) Li, B., Dai, Y., He, M., 2018a. 单目深度估计：通过扩张CNN和软加权和推理的层次融合。模式识别
    83, 328–339。
- en: Li et al. (2018b) Li, B., He, M., Dai, Y., Cheng, X., Chen, Y., 2018b. 3d skeleton
    based action recognition by video-domain translation-scale invariant mapping and
    multi-scale dilated cnn. Multimedia Tools and Applications 77, 22901–22921.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2018b) Li, B., He, M., Dai, Y., Cheng, X., Chen, Y., 2018b. 基于3D骨架的视频领域转换-尺度不变映射和多尺度扩张CNN进行动作识别。多媒体工具与应用
    77, 22901–22921。
- en: 'Li et al. (2015a) Li, B., Shen, C., Dai, Y., Hengel, A., He, M., 2015a. Depth
    and surface normal estimation from monocular images using regression on deep features
    and hierarchical crfs, in: Proc. IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 1119–1127.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2015a) Li, B., Shen, C., Dai, Y., Hengel, A., He, M., 2015a. 使用深度特征回归和层次化CRFs从单目图像中估计深度和表面法线,
    见于：IEEE计算机视觉与模式识别会议论文集, 页1119–1127。
- en: 'Li and Lee (2019) Li, C., Lee, G.H., 2019. Generating multiple hypotheses for
    3d human pose estimation with mixture density network, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 9887–9895.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Lee (2019) Li, C., Lee, G.H., 2019. 利用混合密度网络生成3D人体姿态估计的多种假设, 见于：IEEE计算机视觉与模式识别会议论文集,
    页9887–9895。
- en: 'Li and Fei-fei (2007) Li, L., Fei-fei, L., 2007. What, where and who? classifying
    events by scene and object recognition, in: Proc. IEEE International Conference
    on Computer Vision, p. 6.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Fei-fei (2007) Li, L., Fei-fei, L., 2007. 什么、哪里和谁？通过场景和物体识别分类事件, 见于：IEEE国际计算机视觉会议论文集,
    页6。
- en: 'Li and Chan (2014) Li, S., Chan, A.B., 2014. 3d human pose estimation from
    monocular images with deep convolutional neural network, in: Proc. Asian Conference
    on Computer Vision, Springer. pp. 332–347.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Chan (2014) Li, S., Chan, A.B., 2014. 利用深度卷积神经网络从单目图像中进行3D人体姿态估计, 见于：亚洲计算机视觉会议论文集,
    Springer. 页332–347。
- en: 'Li et al. (2014) Li, S., Liu, Z.Q., Chan, A.B., 2014. Heterogeneous multi-task
    learning for human pose estimation with deep convolutional neural network, in:
    Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp.
    482–489.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2014) Li, S., Liu, Z.Q., Chan, A.B., 2014. 利用深度卷积神经网络进行人体姿态估计的异质多任务学习,
    见于：IEEE计算机视觉与模式识别会议研讨会论文集, 页482–489。
- en: 'Li et al. (2015b) Li, S., Zhang, W., Chan, A.B., 2015b. Maximum-margin structured
    learning with deep networks for 3d human pose estimation, in: Proc. IEEE International
    Conference on Computer Vision, pp. 2848–2856.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2015b) Li, S., Zhang, W., Chan, A.B., 2015b. 使用深度网络进行3D人体姿态估计的最大边际结构学习,
    见于：IEEE国际计算机视觉会议论文集, 页2848–2856。
- en: Li et al. (2019) Li, Z., Dekel, T., Cole, F., Tucker, R., Snavely, N., Liu,
    C., Freeman, W., 2019. Learning the depths of moving people by watching frozen
    , 4521–4530.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019) Li, Z., Dekel, T., Cole, F., Tucker, R., Snavely, N., Liu,
    C., Freeman, W., 2019. 通过观看静态图像学习移动人群的深度, 4521–4530。
- en: 'Lifshitz et al. (2016) Lifshitz, I., Fetaya, E., Ullman, S., 2016. Human pose
    estimation using deep consensus voting, in: Proc. European Conference on Computer
    Vision, Springer. pp. 246–260.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lifshitz et al. (2016) Lifshitz, I., Fetaya, E., Ullman, S., 2016. 使用深度共识投票进行人体姿态估计,
    见于：欧洲计算机视觉会议论文集, Springer. 页246–260。
- en: 'Lin et al. (2017) Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B.,
    Belongie, S., 2017. Feature pyramid networks for object detection, in: Proc. IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 2117–2125.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2017) Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B.,
    Belongie, S., 2017. 用于物体检测的特征金字塔网络, 见于：IEEE计算机视觉与模式识别会议论文集, 页2117–2125。
- en: 'Lin et al. (2014) Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., Zitnick, C.L., 2014. Microsoft coco: Common objects in
    context, in: Proc. European Conference on Computer Vision, Springer. pp. 740–755.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2014) Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., Zitnick, C.L., 2014. Microsoft coco: 上下文中的常见物体, 见于：欧洲计算机视觉会议论文集,
    Springer. 页740–755。'
- en: 'Liu et al. (2015) Liu, Z., Zhu, J., Bu, J., Chen, C., 2015. A survey of human
    pose estimation: the body parts parsing based methods. Journal of Visual Communication
    and Image Representation 32, 10–19.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2015) Liu, Z., Zhu, J., Bu, J., Chen, C., 2015. 人体姿态估计的综述：基于身体部位解析的方法。视觉通信与图像表示杂志
    32, 10–19。
- en: 'Long et al. (2015) Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional
    networks for semantic segmentation, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 3431–3440.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2015) Long, J., Shelhamer, E., Darrell, T., 2015. 用于语义分割的全卷积网络,
    见于：IEEE计算机视觉与模式识别会议论文集, 页3431–3440。
- en: 'Loper et al. (2015) Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black,
    M.J., 2015. Smpl: A skinned multi-person linear model. ACM Transactions on Graphics
    34, 248.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Loper et al. (2015) Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black,
    M.J., 2015. SMPL: 一个皮肤化的多人物线性模型。ACM图形学学报 34, 248。'
- en: 'Luo et al. (2018) Luo, Y., Ren, J., Wang, Z., Sun, W., Pan, J., Liu, J., Pang,
    J., Lin, L., 2018. Lstm pose machines, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 5207–5215.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2018) Luo, Y., Ren, J., Wang, Z., Sun, W., Pan, J., Liu, J., Pang,
    J., Lin, L., 2018. LSTM 姿态机器，见：IEEE 计算机视觉与模式识别会议论文集，第 5207–5215 页。
- en: 'Luvizon et al. (2018) Luvizon, D.C., Picard, D., Tabia, H., 2018. 2d/3d pose
    estimation and action recognition using multitask deep learning, in: Proc. IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 5137–5146.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luvizon et al. (2018) Luvizon, D.C., Picard, D., Tabia, H., 2018. 使用多任务深度学习进行
    2D/3D 姿态估计和动作识别，见：IEEE 计算机视觉与模式识别会议论文集，第 5137–5146 页。
- en: Luvizon et al. (2017) Luvizon, D.C., Tabia, H., Picard, D., 2017. Human pose
    regression by combining indirect part detection and contextual information. arXiv
    preprint arXiv:1710.02322 .
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luvizon et al. (2017) Luvizon, D.C., Tabia, H., Picard, D., 2017. 通过结合间接部件检测和上下文信息进行人体姿态回归。arXiv
    预印本 arXiv:1710.02322。
- en: 'Mahmood et al. (2019) Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G.,
    Black, M.J., 2019. Amass: Archive of motion capture as surface shapes. arXiv preprint
    arXiv:1904.03278 .'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahmood et al. (2019) Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G.,
    Black, M.J., 2019. AMASS：作为表面形状的动作捕捉档案。arXiv 预印本 arXiv:1904.03278。
- en: 'von Marcard et al. (2018) von Marcard, T., Henschel, R., Black, M.J., Rosenhahn,
    B., Pons-Moll, G., 2018. Recovering accurate 3d human pose in the wild using imus
    and a moving camera, in: Proc. European Conference on Computer Vision, pp. 601–617.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: von Marcard et al. (2018) von Marcard, T., Henschel, R., Black, M.J., Rosenhahn,
    B., Pons-Moll, G., 2018. 使用 IMU 和移动相机在野外恢复准确的 3D 人体姿态，见：欧洲计算机视觉会议论文集，第 601–617
    页。
- en: von Marcard et al. (2016) von Marcard, T., Pons-Moll, G., Rosenhahn, B., 2016.
    Human pose estimation from video and imus. IEEE transactions on pattern analysis
    and machine intelligence 38, 1533–1547.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: von Marcard et al. (2016) von Marcard, T., Pons-Moll, G., Rosenhahn, B., 2016.
    从视频和 IMU 中进行人体姿态估计。IEEE 模式分析与机器智能学报 38, 1533–1547。
- en: 'Martinez et al. (2017) Martinez, J., Hossain, R., Romero, J., Little, J.J.,
    2017. A simple yet effective baseline for 3d human pose estimation, in: Proc.
    IEEE International Conference on Computer Vision, pp. 2640–2649.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martinez et al. (2017) Martinez, J., Hossain, R., Romero, J., Little, J.J.,
    2017. 一种简单而有效的 3D 人体姿态估计基准，见：IEEE 国际计算机视觉会议论文集，第 2640–2649 页。
- en: 'Mehta et al. (2017a) Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko,
    O., Xu, W., Theobalt, C., 2017a. Monocular 3d human pose estimation in the wild
    using improved cnn supervision, in: Proc. IEEE International Conference on 3D
    Vision, IEEE. pp. 506–516.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2017a) Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko,
    O., Xu, W., Theobalt, C., 2017a. 使用改进的 CNN 监督进行野外单目 3D 人体姿态估计，见：IEEE 国际 3D 视觉会议论文集，IEEE，第
    506–516 页。
- en: 'Mehta et al. (2019) Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Elgharib,
    M., Fua, P., Seidel, H.P., Rhodin, H., Pons-Moll, G., Theobalt, C., 2019. Xnect:
    Real-time multi-person 3d human pose estimation with a single rgb camera. arXiv:1907.00837
    .'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2019) Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Elgharib,
    M., Fua, P., Seidel, H.P., Rhodin, H., Pons-Moll, G., Theobalt, C., 2019. Xnect：使用单个
    RGB 相机进行实时多人 3D 人体姿态估计。arXiv:1907.00837。
- en: Mehta et al. (2017b) Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar,
    S., Pons-Moll, G., Theobalt, C., 2017b. Single-shot multi-person 3d body pose
    estimation from monocular rgb input. arXiv preprint arXiv:1712.03453 .
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2017b) Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar,
    S., Pons-Moll, G., Theobalt, C., 2017b. 从单目 RGB 输入中进行单次多人人体 3D 姿态估计。arXiv 预印本
    arXiv:1712.03453。
- en: 'Mehta et al. (2017c) Mehta, D., Sridhar, S., Sotnychenko, O., Rhodin, H., Shafiei,
    M., Seidel, H.P., Xu, W., Casas, D., Theobalt, C., 2017c. Vnect: Real-time 3d
    human pose estimation with a single rgb camera. ACM Transactions on Graphics 36,
    44.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2017c) Mehta, D., Sridhar, S., Sotnychenko, O., Rhodin, H., Shafiei,
    M., Seidel, H.P., Xu, W., Casas, D., Theobalt, C., 2017c. Vnect：使用单个 RGB 相机进行实时
    3D 人体姿态估计。ACM 图形学报 36, 44。
- en: Meredith et al. (2001) Meredith, M., Maddock, S., et al., 2001. Motion capture
    file formats explained. Department of Computer Science, University of Sheffield
    211, 241–244.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meredith et al. (2001) Meredith, M., Maddock, S., et al., 2001. 动作捕捉文件格式说明。谢菲尔德大学计算机科学系
    211, 241–244。
- en: Moeslund and Granum (2001) Moeslund, T.B., Granum, E., 2001. A survey of computer
    vision-based human motion capture. Computer Vision and Image Understanding 81,
    231–268.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moeslund and Granum (2001) Moeslund, T.B., Granum, E., 2001. 基于计算机视觉的人体动作捕捉综述。计算机视觉与图像理解
    81, 231–268。
- en: Moeslund et al. (2006) Moeslund, T.B., Hilton, A., Krüger, V., 2006. A survey
    of advances in vision-based human motion capture and analysis. Computer Vision
    and Image Understanding 104, 90–126.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moeslund et al. (2006) Moeslund, T.B., Hilton, A., Krüger, V., 2006. 基于视觉的人体动作捕捉和分析进展综述。计算机视觉与图像理解
    104, 90–126。
- en: Moeslund et al. (2011) Moeslund, T.B., Hilton, A., Krüger, V., Sigal, L., 2011.
    Visual analysis of humans. Springer.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moeslund等人（2011）Moeslund，T.B.，Hilton，A.，Krüger，V.，Sigal，L.，2011. “Visual analysis
    of humans”，Springer。
- en: 'Moon et al. (2019) Moon, G., Chang, J.Y., Lee, K.M., 2019. Posefix: Model-agnostic
    general human pose refinement network, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 7773–7781.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moon等人（2019）Moon，G.，Chang，J.Y.，Lee，K.M.，2019. "Posefix: Model-agnostic general
    human pose refinement network",IEEE计算机视觉与模式识别会议论文集，pp. 7773–7781。'
- en: 'Moreno-Noguer (2017) Moreno-Noguer, F., 2017. 3d human pose estimation from
    a single image via distance matrix regression, in: Proc. IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 1561–1570.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moreno-Noguer（2017）Moreno-Noguer，F.，2017. “3d human pose estimation from a single
    image via distance matrix regression”，IEEE计算机视觉与模式识别会议论文集，pp. 1561–1570。
- en: 'Newell et al. (2017) Newell, A., Huang, Z., Deng, J., 2017. Associative embedding:
    End-to-end learning for joint detection and grouping, in: Advances in Neural Information
    Processing Systems, pp. 2277–2287.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Newell等人（2017）Newell，A.，Huang，Z.，Deng，J.，2017. “Associative embedding: End-to-end
    learning for joint detection and grouping”，神经信息处理系统的进展，pp. 2277–2287。'
- en: 'Newell et al. (2016) Newell, A., Yang, K., Deng, J., 2016. Stacked hourglass
    networks for human pose estimation, in: Proc. European Conference on Computer
    Vision, Springer. pp. 483–499.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell等人（2016）Newell，A.，Yang，K.，Deng，J.，2016. “Stacked hourglass networks for
    human pose estimation”，欧洲计算机视觉会议论文集，Springer，pp. 483–499。
- en: Nibali et al. (2018) Nibali, A., He, Z., Morgan, S., Prendergast, L., 2018.
    Numerical coordinate regression with convolutional neural networks. arXiv preprint
    arXiv:1801.07372 .
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nibali等人（2018）Nibali，A.，He，Z.，Morgan，S.，Prendergast，L.，2018. “Numerical coordinate
    regression with convolutional neural networks”，arXiv预印本arXiv:1801.07372。
- en: 'Nie et al. (2017) Nie, B.X., Wei, P., Zhu, S.C., 2017. Monocular 3d human pose
    estimation by predicting depth on joints, in: Proc. IEEE International Conference
    on Computer Vision, pp. 3447–3455.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie等人（2017）Nie，B.X.，Wei，P.，Zhu，S.C.，2017. "Monocular 3d human pose estimation
    by predicting depth on joints"，IEEE国际计算机视觉会议论文集，pp. 3447–3455。
- en: 'Nie et al. (2018) Nie, X., Feng, J., Xing, J., Yan, S., 2018. Pose partition
    networks for multi-person pose estimation, in: Proc. European Conference on Computer
    Vision, pp. 684–699.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie等人（2018）Nie，X.，Feng，J.，Xing，J.，Yan，S.，2018. "Pose partition networks for
    multi-person pose estimation"，欧洲计算机视觉会议论文集，pp. 684–699。
- en: Ning et al. (2018) Ning, G., Zhang, Z., He, Z., 2018. Knowledge-guided deep
    fractal neural networks for human pose estimation. IEEE Transactions on Multimedia
    20, 1246–1259.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ning等人（2018）Ning，G.，Zhang，Z.，He，Z.，2018. “Knowledge-guided deep fractal neural
    networks for human pose estimation”，IEEE多媒体交易，20，1246–1259。
- en: 'Omran et al. (2018) Omran, M., Lassner, C., Pons-Moll, G., Gehler, P., Schiele,
    B., 2018. Neural body fitting: Unifying deep learning and model based human pose
    and shape estimation, in: Proc. IEEE International Conference on 3D Vision, IEEE.
    pp. 484–494.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Omran等人（2018）Omran，M.，Lassner，C.，Pons-Moll，G.，Gehler，P.，Schiele，B.，2018. "Neural
    body fitting: Unifying deep learning and model based human pose and shape estimation",IEEE
    3D Vision国际会议，IEEE，pp. 484–494。'
- en: 'Ouyang et al. (2014) Ouyang, W., Chu, X., Wang, X., 2014. Multi-source deep
    learning for human pose estimation, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 2329–2336.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2014）Ouyang，W.，Chu，X.，Wang，X.，2014. “Multi-source deep learning for
    human pose estimation”，IEEE计算机视觉与模式识别会议论文集，pp. 2329–2336。
- en: 'Papandreou et al. (2018) Papandreou, G., Zhu, T., Chen, L.C., Gidaris, S.,
    Tompson, J., Murphy, K., 2018. Personlab: Person pose estimation and instance
    segmentation with a bottom-up, part-based, geometric embedding model. arXiv preprint
    arXiv:1803.08225 .'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papandreou等人（2018）Papandreou，G.，Zhu，T.，Chen，L.C.，Gidaris，S.，Tompson，J.，Murphy，K.，2018.
    “Personlab: Person pose estimation and instance segmentation with a bottom-up,
    part-based, geometric embedding model”，arXiv预印本arXiv:1803.08225。'
- en: 'Papandreou et al. (2017) Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A.,
    Tompson, J., Bregler, C., Murphy, K., 2017. Towards accurate multi-person pose
    estimation in the wild, in: Proc. IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 4903–4911.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papandreou等人（2017）Papandreou，G.，Zhu，T.，Kanazawa，N.，Toshev，A.，Tompson，J.，Bregler，C.，Murphy，K.，2017.
    “Towards accurate multi-person pose estimation in the wild”，IEEE计算机视觉与模式识别会议论文集，pp.
    4903–4911。
- en: Pavlakos et al. (2018a) Pavlakos, G., Zhou, X., Daniilidis, K., 2018a. Ordinal
    depth supervision for 3d human pose estimation. arXiv preprint arXiv:1805.04095
    .
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlakos等人（2018a）Pavlakos，G.，Zhou，X.，Daniilidis，K.，2018a. “Ordinal depth supervision
    for 3d human pose estimation”，arXiv预印本arXiv:1805.04095。
- en: 'Pavlakos et al. (2017) Pavlakos, G., Zhou, X., Derpanis, K.G., Daniilidis,
    K., 2017. Coarse-to-fine volumetric prediction for single-image 3d human pose,
    in: Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 1263–1272.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlakos 等人（2017）Pavlakos, G., Zhou, X., Derpanis, K.G., Daniilidis, K., 2017.
    单图像 3D 人体姿态的粗到细体积预测，发表于：IEEE 计算机视觉与模式识别会议论文集，页码 1263–1272。
- en: Pavlakos et al. (2018b) Pavlakos, G., Zhu, L., Zhou, X., Daniilidis, K., 2018b.
    Learning to estimate 3d human pose and shape from a single color image. arXiv
    preprint arXiv:1805.04092 .
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlakos 等人（2018b）Pavlakos, G., Zhu, L., Zhou, X., Daniilidis, K., 2018b. 学习从单张彩色图像中估计
    3D 人体姿态和形状。arXiv 预印本 arXiv:1805.04092。
- en: 'Peng et al. (2018) Peng, X., Tang, Z., Yang, F., Feris, R.S., Metaxas, D.,
    2018. Jointly optimize data augmentation and network training: Adversarial data
    augmentation in human pose estimation, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 2226–2234.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人（2018）Peng, X., Tang, Z., Yang, F., Feris, R.S., Metaxas, D., 2018. 联合优化数据增强和网络训练：在人体姿态估计中的对抗性数据增强，发表于：IEEE
    计算机视觉与模式识别会议论文集，页码 2226–2234。
- en: Perez-Sala et al. (2014) Perez-Sala, X., Escalera, S., Angulo, C., Gonzalez,
    J., 2014. A survey on model based approaches for 2d and 3d visual human pose recovery.
    Sensors 14, 4189–4210.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez-Sala 等人（2014）Perez-Sala, X., Escalera, S., Angulo, C., Gonzalez, J., 2014.
    基于模型的二维和三维视觉人体姿态恢复的调查。传感器 14, 4189–4210。
- en: 'Pfister et al. (2015) Pfister, T., Charles, J., Zisserman, A., 2015. Flowing
    convnets for human pose estimation in videos, in: Proc. IEEE International Conference
    on Computer Vision, pp. 1913–1921.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfister 等人（2015）Pfister, T., Charles, J., Zisserman, A., 2015. 视频中用于人体姿态估计的流动卷积网络，发表于：IEEE
    国际计算机视觉会议论文集，页码 1913–1921。
- en: 'Pfister et al. (2014) Pfister, T., Simonyan, K., Charles, J., Zisserman, A.,
    2014. Deep convolutional neural networks for efficient pose estimation in gesture
    videos, in: Proc. Asian Conference on Computer Vision, Springer. pp. 538–552.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfister 等人（2014）Pfister, T., Simonyan, K., Charles, J., Zisserman, A., 2014.
    用于手势视频中高效姿态估计的深度卷积神经网络，发表于：亚洲计算机视觉会议论文集，Springer. 页码 538–552。
- en: 'Pishchulin et al. (2016) Pishchulin, L., Insafutdinov, E., Tang, S., Andres,
    B., Andriluka, M., Gehler, P.V., Schiele, B., 2016. Deepcut: Joint subset partition
    and labeling for multi person pose estimation, in: Proc. IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 4929–4937.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pishchulin 等人（2016）Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka,
    M., Gehler, P.V., Schiele, B., 2016. Deepcut：用于多人姿态估计的联合子集划分和标记，发表于：IEEE 计算机视觉与模式识别会议论文集，页码
    4929–4937。
- en: 'Pons-Moll et al. (2015) Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.,
    2015. Dyna: A model of dynamic human shape in motion. ACM Transactions on Graphics
    34, 120.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pons-Moll 等人（2015）Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J., 2015.
    Dyna：一个动态人体形状模型。ACM 图形学汇刊 34, 120。
- en: 'Popa et al. (2017) Popa, A.I., Zanfir, M., Sminchisescu, C., 2017. Deep multitask
    architecture for integrated 2d and 3d human sensing, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4714–4723.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Popa 等人（2017）Popa, A.I., Zanfir, M., Sminchisescu, C., 2017. 用于集成 2D 和 3D 人体感知的深度多任务架构，发表于：IEEE
    计算机视觉与模式识别会议论文集，页码 4714–4723。
- en: 'Poppe (2007) Poppe, R., 2007. Vision-based human motion analysis: An overview.
    Computer Vision and Image Understanding 108, 4–18.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poppe（2007）Poppe, R., 2007. 基于视觉的人体运动分析：概述。计算机视觉与图像理解 108, 4–18。
- en: 'Qammaz and Argyros (2019) Qammaz, A., Argyros, A., 2019. Mocapnet: Ensemble
    of snn encoders for 3d human pose estimation in rgb images, in: Proc. British
    Machine VIsion Conference.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qammaz 和 Argyros（2019）Qammaz, A., Argyros, A., 2019. Mocapnet：用于 RGB 图像中的 3D
    人体姿态估计的 SNN 编码器集成，发表于：英国机器视觉会议论文集。
- en: 'Rafi et al. (2016) Rafi, U., Leibe, B., Gall, J., Kostrikov, I., 2016. An efficient
    convolutional network for human pose estimation, in: Proc. British Machine Vision
    Conference, p. 2.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafi 等人（2016）Rafi, U., Leibe, B., Gall, J., Kostrikov, I., 2016. 一种高效的卷积网络用于人体姿态估计，发表于：英国机器视觉会议论文集，页码
    2。
- en: 'Ramakrishna et al. (2014) Ramakrishna, V., Munoz, D., Hebert, M., Bagnell,
    J.A., Sheikh, Y., 2014. Pose machines: Articulated pose estimation via inference
    machines, in: Proc. European Conference on Computer Vision, Springer. pp. 33–47.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramakrishna 等人（2014）Ramakrishna, V., Munoz, D., Hebert, M., Bagnell, J.A., Sheikh,
    Y., 2014. 姿态机器：通过推理机器进行关节姿态估计，发表于：欧洲计算机视觉会议论文集，Springer. 页码 33–47。
- en: 'Ren et al. (2015) Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn:
    Towards real-time object detection with region proposal networks, in: Advances
    in neural information processing systems, pp. 91–99.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人（2015）Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster R-CNN：通过区域建议网络实现实时目标检测，发表于：神经信息处理系统进展，页码
    91–99。
- en: Rhodin et al. (2018a) Rhodin, H., Salzmann, M., Fua, P., 2018a. Unsupervised
    geometry-aware representation for 3d human pose estimation. arXiv:1804.01110 .
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rhodin 等 (2018a) Rhodin, H., Salzmann, M., Fua, P., 2018a. 无监督几何感知表示用于 3D 人体姿态估计。arXiv:1804.01110。
- en: 'Rhodin et al. (2018b) Rhodin, H., Spörri, J., Katircioglu, I., Constantin,
    V., Meyer, F., Müller, E., Salzmann, M., Fua, P., 2018b. Learning monocular 3d
    human pose estimation from multi-view images, in: Proc. IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 8437–8446.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rhodin 等 (2018b) Rhodin, H., Spörri, J., Katircioglu, I., Constantin, V., Meyer,
    F., Müller, E., Salzmann, M., Fua, P., 2018b. 从多视角图像中学习单目 3D 人体姿态估计，发表于：IEEE 计算机视觉与模式识别会议论文集，页码8437–8446。
- en: 'Rogez et al. (2017) Rogez, G., Weinzaepfel, P., Schmid, C., 2017. Lcr-net:
    Localization-classification-regression for human pose, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 3433–3441.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rogez 等 (2017) Rogez, G., Weinzaepfel, P., Schmid, C., 2017. Lcr-net: 人体姿态的定位-分类-回归方法，发表于：IEEE
    计算机视觉与模式识别会议论文集，页码3433–3441。'
- en: 'Rohrbach et al. (2012) Rohrbach, M., Amin, S., Andriluka, M., Schiele, B.,
    2012. A database for fine grained activity detection of cooking activities, in:
    Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 1194–1201.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rohrbach 等 (2012) Rohrbach, M., Amin, S., Andriluka, M., Schiele, B., 2012.
    一种用于烹饪活动精细检测的数据库，发表于：IEEE 计算机视觉与模式识别会议论文集，页码1194–1201。
- en: 'Sapp and Taskar (2013) Sapp, B., Taskar, B., 2013. Modec: Multimodal decomposable
    models for human pose estimation, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 3674–3681.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sapp 和 Taskar (2013) Sapp, B., Taskar, B., 2013. Modec: 人体姿态估计的多模态可分解模型，发表于：IEEE
    计算机视觉与模式识别会议论文集，页码3674–3681。'
- en: 'Sapp et al. (2011) Sapp, B., Weiss, D., Taskar, B., 2011. Parsing human motion
    with stretchable models, in: Proc. IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 1281–1288.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sapp 等 (2011) Sapp, B., Weiss, D., Taskar, B., 2011. 使用可拉伸模型解析人体运动，发表于：IEEE
    计算机视觉与模式识别会议论文集，页码1281–1288。
- en: 'Sarafianos et al. (2016) Sarafianos, N., Boteanu, B., Ionescu, B., Kakadiaris,
    I.A., 2016. 3d human pose estimation: A review of the literature and analysis
    of covariates. Computer Vision and Image Understanding 152, 1–20.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarafianos 等 (2016) Sarafianos, N., Boteanu, B., Ionescu, B., Kakadiaris, I.A.,
    2016. 3D 人体姿态估计：文献综述与协变量分析。计算机视觉与图像理解 152，1–20。
- en: 'Shahroudy et al. (2016) Shahroudy, A., Liu, J., Ng, T.T., Wang, G., 2016. Ntu
    rgb+ d: A large scale dataset for 3d human activity analysis, in: Proc. IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 1010–1019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shahroudy 等 (2016) Shahroudy, A., Liu, J., Ng, T.T., Wang, G., 2016. NTU RGB+D:
    大规模 3D 人体活动分析数据集，发表于：IEEE 计算机视觉与模式识别会议论文集，页码1010–1019。'
- en: Shotton et al. (2012) Shotton, J., Girshick, R., Fitzgibbon, A., Sharp, T.,
    Cook, M., Finocchio, M., Moore, R., Kohli, P., Criminisi, A., Kipman, A., et al.,
    2012. Efficient human pose estimation from single depth images. IEEE transactions
    on pattern analysis and machine intelligence 35, 2821–2840.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shotton 等 (2012) Shotton, J., Girshick, R., Fitzgibbon, A., Sharp, T., Cook,
    M., Finocchio, M., Moore, R., Kohli, P., Criminisi, A., Kipman, A., 等，2012. 从单一深度图像中高效地进行人体姿态估计。IEEE
    模式分析与机器智能汇刊 35，2821–2840。
- en: 'Sidenbladh et al. (2000) Sidenbladh, H., De la Torre, F., Black, M.J., 2000.
    A framework for modeling the appearance of 3d articulated figures, in: Proc. IEEE
    Conference on Automatic Face and Gesture Recognition, IEEE. pp. 368–375.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sidenbladh 等 (2000) Sidenbladh, H., De la Torre, F., Black, M.J., 2000. 建模 3D
    人物外观的框架，发表于：IEEE 自动面部和手势识别会议论文集，IEEE。页码368–375。
- en: 'Sigal et al. (2010) Sigal, L., Balan, A.O., Black, M.J., 2010. Humaneva: Synchronized
    video and motion capture dataset and baseline algorithm for evaluation of articulated
    human motion. International journal of computer vision 87, 4.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sigal 等 (2010) Sigal, L., Balan, A.O., Black, M.J., 2010. Humaneva: 同步视频和动作捕捉数据集以及用于评估关节人体运动的基线算法。计算机视觉国际期刊
    87，4。'
- en: 'Sminchisescu (2008) Sminchisescu, C., 2008. 3d human motion analysis in monocular
    video: techniques and challenges, in: Human Motion. Springer, pp. 185–211.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sminchisescu (2008) Sminchisescu, C., 2008. 单目视频中的 3D 人体运动分析：技术与挑战，发表于：人类运动。Springer，页码185–211。
- en: 'Sun et al. (2019) Sun, K., Xiao, B., Liu, D., Wang, J., 2019. Deep high-resolution
    representation learning for human pose estimation, in: Proc. IEEE Conference on
    Computer Vision and Pattern Recognition.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2019) Sun, K., Xiao, B., Liu, D., Wang, J., 2019. 深度高分辨率表示学习用于人体姿态估计，发表于：IEEE
    计算机视觉与模式识别会议论文集。
- en: 'Sun et al. (2017) Sun, X., Shang, J., Liang, S., Wei, Y., 2017. Compositional
    human pose regression, in: Proc. IEEE International Conference on Computer Vision,
    p. 7.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2017）Sun, X., Shang, J., Liang, S., Wei, Y., 2017. 组合人体姿态回归，见于：IEEE国际计算机视觉会议论文集，页7。
- en: 'Sun et al. (2018) Sun, X., Xiao, B., Wei, F., Liang, S., Wei, Y., 2018. Integral
    human pose regression, in: Proc. European Conference on Computer Vision, pp. 529–545.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2018）Sun, X., Xiao, B., Wei, F., Liang, S., Wei, Y., 2018. 整体人体姿态回归，见于：欧洲计算机视觉会议论文集，页529–545。
- en: 'Szegedy et al. (2016) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna,
    Z., 2016. Rethinking the inception architecture for computer vision, in: Proc.
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818–2826.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy等（2016）Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.,
    2016. 重新思考计算机视觉中的Inception架构，见于：IEEE计算机视觉与模式识别会议论文集，页2818–2826。
- en: 'Tan et al. (2017) Tan, J., Budvytis, I., Cipolla, R., 2017. Indirect deep structured
    learning for 3d human body shape and pose prediction, in: Proc. British Machine
    Vision Conference.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan等（2017）Tan, J., Budvytis, I., Cipolla, R., 2017. 间接深度结构学习用于3d人体形状和姿态预测，见于：英国机器视觉会议论文集。
- en: 'Tang and Wu (2019) Tang, W., Wu, Y., 2019. Does learning specific features
    for related parts help human pose estimation?, in: Proc. IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 1107–1116.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang和Wu（2019）Tang, W., Wu, Y., 2019. 针对相关部分学习特定特征是否有助于人体姿态估计？，见于：IEEE计算机视觉与模式识别会议论文集，页1107–1116。
- en: 'Tang et al. (2018a) Tang, W., Yu, P., Wu, Y., 2018a. Deeply learned compositional
    models for human pose estimation, in: Proc. European Conference on Computer Vision,
    pp. 190–206.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等（2018a）Tang, W., Yu, P., Wu, Y., 2018a. 深度学习的组合模型用于人体姿态估计，见于：欧洲计算机视觉会议论文集，页190–206。
- en: 'Tang et al. (2018b) Tang, Z., Peng, X., Geng, S., Wu, L., Zhang, S., Metaxas,
    D., 2018b. Quantized densely connected u-nets for efficient landmark localization,
    in: Proc. European Conference on Computer Vision, pp. 339–354.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等（2018b）Tang, Z., Peng, X., Geng, S., Wu, L., Zhang, S., Metaxas, D., 2018b.
    量化密集连接u-net用于高效的标志点定位，见于：欧洲计算机视觉会议论文集，页339–354。
- en: Tekin et al. (2016) Tekin, B., Katircioglu, I., Salzmann, M., Lepetit, V., Fua,
    P., 2016. Structured prediction of 3d human pose with deep neural networks. arXiv
    preprint arXiv:1605.05180 .
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tekin等（2016）Tekin, B., Katircioglu, I., Salzmann, M., Lepetit, V., Fua, P.,
    2016. 使用深度神经网络进行3d人体姿态的结构化预测。arXiv预印本arXiv:1605.05180。
- en: 'Tekin et al. (2017) Tekin, B., Marquez Neila, P., Salzmann, M., Fua, P., 2017.
    Learning to fuse 2d and 3d image cues for monocular body pose estimation, in:
    Proc. IEEE International Conference on Computer Vision, pp. 3941–3950.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tekin等（2017）Tekin, B., Marquez Neila, P., Salzmann, M., Fua, P., 2017. 学习融合2d和3d图像线索进行单目身体姿态估计，见于：IEEE国际计算机视觉会议论文集，页3941–3950。
- en: 'TheCaptury (accessed on 2019) TheCaptury, accessed on 2019. URL: [https://thecaptury.com/](https://thecaptury.com/).'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TheCaptury（访问于2019）TheCaptury, 访问于2019。网址：[https://thecaptury.com/](https://thecaptury.com/)。
- en: 'Tome et al. (2017) Tome, D., Russell, C., Agapito, L., 2017. Lifting from the
    deep: Convolutional 3d pose estimation from a single image, pp. 2500–2509.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tome等（2017）Tome, D., Russell, C., Agapito, L., 2017. 从深度学习提升：从单张图像进行卷积3d姿态估计，页2500–2509。
- en: 'Tompson et al. (2015) Tompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler,
    C., 2015. Efficient object localization using convolutional networks, in: Proc.
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 648–656.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tompson等（2015）Tompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler, C., 2015.
    使用卷积网络进行高效物体定位，见于：IEEE计算机视觉与模式识别会议论文集，页648–656。
- en: 'Tompson et al. (2014) Tompson, J.J., Jain, A., LeCun, Y., Bregler, C., 2014.
    Joint training of a convolutional network and a graphical model for human pose
    estimation, in: Advances in neural information processing systems, pp. 1799–1807.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tompson等（2014）Tompson, J.J., Jain, A., LeCun, Y., Bregler, C., 2014. 联合训练卷积网络和图形模型用于人体姿态估计，见于：神经信息处理系统进展，页1799–1807。
- en: 'Toshev and Szegedy (2014) Toshev, A., Szegedy, C., 2014. Deeppose: Human pose
    estimation via deep neural networks, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 1653–1660.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toshev和Szegedy（2014）Toshev, A., Szegedy, C., 2014. DeepPose：通过深度神经网络进行人体姿态估计，见于：IEEE计算机视觉与模式识别会议论文集，页1653–1660。
- en: 'Trumble et al. (2017) Trumble, M., Gilbert, A., Malleson, C., Hilton, A., Collomosse,
    J., 2017. Total capture: 3d human pose estimation fusing video and inertial sensors,
    in: Proc. British Machine Vision Conference, pp. 1–13.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trumble等（2017）Trumble, M., Gilbert, A., Malleson, C., Hilton, A., Collomosse,
    J., 2017. 全捕获：融合视频和惯性传感器的3d人体姿态估计，见于：英国机器视觉会议论文集，页1–13。
- en: 'Varol et al. (2018) Varol, G., Ceylan, D., Russell, B., Yang, J., Yumer, E.,
    Laptev, I., Schmid, C., 2018. Bodynet: Volumetric inference of 3d human body shapes.
    arXiv preprint arXiv:1804.04875 .'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Varol等（2018年）Varol, G., Ceylan, D., Russell, B., Yang, J., Yumer, E., Laptev,
    I., Schmid, C., 2018年。Bodynet：3D人体形状的体积推断。arXiv预印本 arXiv:1804.04875。
- en: 'Varol et al. (2017) Varol, G., Romero, J., Martin, X., Mahmood, N., Black,
    M.J., Laptev, I., Schmid, C., 2017. Learning from synthetic humans, in: Proc.
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 4627–4635.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Varol等（2017年）Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M.J., Laptev,
    I., Schmid, C., 2017年。从合成人体中学习，见：IEEE计算机视觉与模式识别会议论文集，页码4627–4635。
- en: 'Vicon (accessed on 2019) Vicon, accessed on 2019. URL: [https://www.vicon.com/](https://www.vicon.com/).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vicon（访问于2019年）Vicon，访问于2019年。网址：[https://www.vicon.com/](https://www.vicon.com/)。
- en: Vondrick et al. (2013) Vondrick, C., Patterson, D., Ramanan, D., 2013. Efficiently
    scaling up crowdsourced video annotation. International Journal of Computer Vision
    101, 184–204.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vondrick等（2013年）Vondrick, C., Patterson, D., Ramanan, D., 2013年。高效扩展众包视频注释。计算机视觉国际期刊101,
    184–204。
- en: 'Wang et al. (2018a) Wang, M., Chen, X., Liu, W., Qian, C., Lin, L., Ma, L.,
    2018a. Drpose3d: Depth ranking in 3d human pose estimation. arXiv preprint arXiv:1805.08973
    .'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2018a年）Wang, M., Chen, X., Liu, W., Qian, C., Lin, L., Ma, L., 2018a年。Drpose3d：3D人体姿势估计中的深度排名。arXiv预印本
    arXiv:1805.08973。
- en: 'Wang et al. (2018b) Wang, P., Li, W., Ogunbona, P., Wan, J., Escalera, S.,
    2018b. Rgb-d-based human motion recognition with deep learning: A survey. Computer
    Vision and Image Understanding 171, 118–139.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2018b年）Wang, P., Li, W., Ogunbona, P., Wan, J., Escalera, S., 2018b年。基于RGB-D的人体运动识别与深度学习：综述。计算机视觉与图像理解171,
    118–139。
- en: 'Wang et al. (2011) Wang, Y., Tran, D., Liao, Z., 2011. Learning hierarchical
    poselets for human parsing, in: Proc. IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 1705–1712.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2011年）Wang, Y., Tran, D., Liao, Z., 2011年。学习层次化姿势模型用于人体解析，见：IEEE计算机视觉与模式识别会议论文集，页码1705–1712。
- en: 'Wei et al. (2016) Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y., 2016.
    Convolutional pose machines, in: Proc. IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 4724–4732.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2016年）Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y., 2016年。卷积姿态网络，见：IEEE计算机视觉与模式识别会议论文集，页码4724–4732。
- en: 'Wu et al. (2017) Wu, J., Zheng, H., Zhao, B., Li, Y., Yan, B., Liang, R., Wang,
    W., Zhou, S., Lin, G., Fu, Y., et al., 2017. Ai challenger: A large-scale dataset
    for going deeper in image understanding. arXiv preprint arXiv:1711.06475 .'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2017年）Wu, J., Zheng, H., Zhao, B., Li, Y., Yan, B., Liang, R., Wang, W.,
    Zhou, S., Lin, G., Fu, Y., 等，2017年。Ai挑战者：一个用于深入图像理解的大规模数据集。arXiv预印本 arXiv:1711.06475。
- en: 'Xiao et al. (2018) Xiao, B., Wu, H., Wei, Y., 2018. Simple baselines for human
    pose estimation and tracking, in: Proc. European Conference on Computer Vision,
    pp. 466–481.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等（2018年）Xiao, B., Wu, H., Wei, Y., 2018年。用于人体姿势估计和跟踪的简单基线，见：欧洲计算机视觉会议论文集，页码466–481。
- en: 'Yang et al. (2017) Yang, W., Li, S., Ouyang, W., Li, H., Wang, X., 2017. Learning
    feature pyramids for human pose estimation, in: Proc. IEEE International Conference
    on Computer Vision, pp. 1281–1290.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2017年）Yang, W., Li, S., Ouyang, W., Li, H., Wang, X., 2017年。学习特征金字塔用于人体姿势估计，见：IEEE国际计算机视觉会议论文集，页码1281–1290。
- en: 'Yang et al. (2016) Yang, W., Ouyang, W., Li, H., Wang, X., 2016. End-to-end
    learning of deformable mixture of parts and deep convolutional neural networks
    for human pose estimation, in: Proc. IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 3073–3082.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2016年）Yang, W., Ouyang, W., Li, H., Wang, X., 2016年。端到端学习变形部件混合和深度卷积神经网络用于人体姿势估计，见：IEEE计算机视觉与模式识别会议论文集，页码3073–3082。
- en: 'Yang et al. (2018) Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., Wang, X.,
    2018. 3d human pose estimation in the wild by adversarial learning, in: Proc.
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 5255–5264.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2018年）Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., Wang, X., 2018年。通过对抗学习进行野外3D人体姿势估计，见：IEEE计算机视觉与模式识别会议论文集，页码5255–5264。
- en: Yang and Ramanan (2013) Yang, Y., Ramanan, D., 2013. Articulated human detection
    with flexible mixtures of parts. IEEE transactions on pattern analysis and machine
    intelligence 35, 2878–2890.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang和Ramanan（2013年）Yang, Y., Ramanan, D., 2013年。灵活部件混合的关节化人体检测。IEEE模式分析与机器智能汇刊35,
    2878–2890。
- en: 'Zanfir et al. (2018) Zanfir, A., Marinoiu, E., Sminchisescu, C., 2018. Monocular
    3d pose and shape estimation of multiple people in natural scenes-the importance
    of multiple scene constraints, in: Proc. IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 2148–2157.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanfir 等人（2018）Zanfir, A., Marinoiu, E., Sminchisescu, C., 2018. 自然场景中多人的单目3D姿态与形状估计——多场景约束的重要性，见：IEEE计算机视觉与模式识别大会论文集，第2148–2157页。
- en: 'Zhang et al. (2013) Zhang, W., Zhu, M., Derpanis, K.G., 2013. From actemes
    to action: A strongly-supervised representation for detailed action understanding,
    in: Proc. IEEE International Conference on Computer Vision, pp. 2248–2255.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2013）Zhang, W., Zhu, M., Derpanis, K.G., 2013. 从动作单位到动作：一种用于详细动作理解的强监督表示，见：IEEE国际计算机视觉大会论文集，第2248–2255页。
- en: 'Zhao et al. (2018) Zhao, M., Li, T., Abu Alsheikh, M., Tian, Y., Zhao, H.,
    Torralba, A., Katabi, D., 2018. Through-wall human pose estimation using radio
    signals, in: Proc. IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 7356–7365.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2018）Zhao, M., Li, T., Abu Alsheikh, M., Tian, Y., Zhao, H., Torralba,
    A., Katabi, D., 2018. 基于无线电信号的穿墙人体姿态估计，见：IEEE计算机视觉与模式识别大会论文集，第7356–7365页。
- en: 'Zhou et al. (2017) Zhou, X., Huang, Q., Sun, X., Xue, X., Wei, Y., 2017. Towards
    3d human pose estimation in the wild: a weakly-supervised approach, in: Proc.
    IEEE International Conference on Computer Vision, pp. 398–407.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2017）Zhou, X., Huang, Q., Sun, X., Xue, X., Wei, Y., 2017. 面向真实环境的3D人体姿态估计：一种弱监督方法，见：IEEE国际计算机视觉大会论文集，第398–407页。
- en: 'Zhou et al. (2016) Zhou, X., Sun, X., Zhang, W., Liang, S., Wei, Y., 2016.
    Deep kinematic pose regression, in: Proc. European Conference on Computer Vision,
    Springer. pp. 186–201.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2016）Zhou, X., Sun, X., Zhang, W., Liang, S., Wei, Y., 2016. 深度运动学姿态回归，见：欧洲计算机视觉大会论文集，Springer，第186–201页。
- en: 'Zuffi and Black (2015) Zuffi, S., Black, M.J., 2015. The stitched puppet: A
    graphical model of 3d human shape and pose, in: Proc. IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 3537–3546.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zuffi 和 Black（2015）Zuffi, S., Black, M.J., 2015. 缝合木偶：3D人体形状和姿态的图形模型，见：IEEE计算机视觉与模式识别大会论文集，第3537–3546页。
- en: 'Zuffi et al. (2012) Zuffi, S., Freifeld, O., Black, M.J., 2012. From pictorial
    structures to deformable structures, in: Proc. IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 3546–3553.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zuffi 等人（2012）Zuffi, S., Freifeld, O., Black, M.J., 2012. 从图像结构到可变结构，见：IEEE计算机视觉与模式识别大会论文集，第3546–3553页。
